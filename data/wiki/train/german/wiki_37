<doc id="4104" url="https://de.wikipedia.org/wiki?curid=4104" title="Panamakanal">
Panamakanal

Der Panamakanal ist eine künstliche, rund 82 Kilometer lange Wasserstraße, die die Landenge von Panama in Mittelamerika durchschneidet, den Atlantik mit dem Pazifik für die Schifffahrt verbindet und ihr damit die Fahrt um das Kap Hoorn oder durch die Magellanstraße an der Südspitze Südamerikas erspart. Der 1914 eröffnete Kanal ist eine der wichtigsten Wasserstraßen der Welt; etwa 14.300 Schiffe (Mittel der Jahre 2011 bis 2013) durchfahren ihn pro Jahr.

Die Abmessungen der Schleusen und Fahrrinnen des ursprünglichen Kanals genügten bis Mitte 2016 nur für Schiffe bis zum Panamax-Maß, also beispielsweise Containerschiffe mit 4.600 Standardcontainern (TEU), die nach den Maßstäben dieser Zeit lediglich mittelgroße Schiffe darstellten. Seit der Eröffnung der 2007 begonnenen Erweiterung am 26. Juni 2016 können ihn Schiffe mit 14.000 TEU passieren.

Die durch den Panamakanal jährlich transportierte Warenmenge entspricht etwa fünf Prozent des weltweiten Seefrachtverkehrs. Der Kanal generiert fast acht Prozent des Bruttoinlandsprodukts von Panama (Stand 2013).

Der Kanal ist einschließlich der Zufahrtskanäle ungefähr 82 Kilometer lang. Er verläuft zwischen den Städten Colón an der Atlantik- und Balboa, einem Vorort von Panama-Stadt an der Pazifikküste. Er ist durchgehend zweispurig für Gegenverkehr ausgelegt. Die dieser Beschreibung zugrundegelegte Richtung vom Atlantik zum Pazifik gilt in der umgekehrten Richtung sinngemäß ebenso. Die Schiffe werden bei Colón durch die Gatún-Schleusen zu dem auf 26 Metern über dem Meeresspiegel aufgestauten Gatúnsee gehoben, fahren in ausgebaggerten Rinnen durch den Gatunsee und den Río Chagres, durchqueren im Gaillard-Kanal (auch Culebra Cut genannt) einen Bergrücken und werden mit den dicht aufeinanderfolgenden Pedro-Miguel- und Miraflores-Schleusen wieder zum Pazifik hinabgelassen.

Der Kanal ist seit seiner Übergabe durch die Vereinigten Staaten an Panama am 31. Dezember 1999 "unveräußerliches Eigentum des panamaischen Volkes" und wird von der Panamakanal-Behörde (spanisch: Autoridad del Canal de Panamá, „ACP“) verwaltet und betrieben. Die ACP ist eine selbstständige panamaische Behörde mit rund 9000 Mitarbeitern. Sie ist an das "Abkommen über die Neutralität des Kanals" gebunden, das ein Bestandteil der am 7. September 1977 zwischen den Vereinigten Staaten und Panama abgeschlossenen Torrijos-Carter-Verträge ist. Deshalb ist die ACP verpflichtet, die Benutzung allen Schiffen, auch Kriegsschiffen, aller Nationen ohne Diskriminierung und zu gleichen Bedingungen gegen Bezahlung der Transitgebühren zu gewähren.

Durch die Schleusen und die Puente de las Américas war die Größe der Schiffe bis Juni 2016 auf die Panamax-Maße beschränkt. Schiffe, die dementsprechend maximal 294,3 Meter lang und exakt 32,3 Meter breit sind, hatten in den Schleusen auf beiden Seiten noch 61 Zentimeter Abstand zu den Wänden der Schleusenkammer.
Seit der Eröffnung der 2007 begonnenen Erweiterung am 26. Juni 2016 kann ein Teil der Postpanamax-Schiffe den Panamakanal passieren. Die größten dann erlaubten Schiffsabmessungen sind seither 366 Meter Länge, 49 Meter Breite und 15,2 Meter Tiefgang (New Panamax oder Neopanamax).

Die für die Durchfahrt benötigte Zeit wird vom Andrang und von dem nicht so seltenen Nebel beeinflusst. Für gebuchte Passagen betrug sie 2011 im Durchschnitt 15,2 Stunden für die gesamte Strecke und knapp 11 Stunden von der Einfahrt in die erste Schleuse bis zur Ausfahrt aus der letzten Schleuse.

Im Jahr 2011 fuhren 14.684 Schiffe durch den Kanal, davon 6.918 Schiffe der Panamax-Klasse. Das Transportvolumen betrug 322,1 Mio. PCUMS (Einheiten des Panama Canal Universal Measurement System); die Gebührenerlöse beliefen sich auf 1,73 Milliarden Balboa (= US-Dollar). Am 4. September 2010 fuhr der chinesische Schüttgut-Frachter mit dem Namen „Fortune Plum“ (Glückspflaume) als einmillionstes Schiff seit der Eröffnung durch den Panamakanal.

Bedeutend ist der Kanal insbesondere für Transporte zwischen der Ost- und der Westküste der Vereinigten Staaten sowie für die Importe aus Asien, soweit sie zur Ostküste der Vereinigten Staaten transportiert werden. Die wichtigsten Nutzer des Kanals sind daher die Vereinigten Staaten und China. Die transportierte Warenmenge entspricht 68 % aller Waren, die in US-Häfen be- oder entladen werden; für China 23 % und für Japan 16 %.

Der Preis der Passage wird nach Art und Größe des Schiffes berechnet. Seit 2011 gilt eine revidierte, komplexe Gebührentabelle, die eine Vielzahl von Grund- und Nebengebühren enthält, wie beispielsweise Lotsen-, Schlepper- und Lokomotivgebühren. Als Anhaltspunkt für die anfallenden Gebühren können die Basisgebühren von 72 US-Dollar pro Standardcontainer und von 134 US-Dollar pro Passagier-Bett dienen.

Vor dem Bau des Kanals führte die kürzeste nutzbare Seeverbindung von der Ostküste zur Westküste Nordamerikas durch die Magellanstraße. Durch den Kanal wurde die Seestrecke New York–San Francisco von etwa 25.000 auf zirka 10.000 Kilometer verkürzt. Die Einsparung von 15.000 Kilometern (8.100 Seemeilen) entspricht bei einer angenommenen Schiffsgeschwindigkeit von 15 Knoten einer Verkürzung der Reisezeit um rund drei Wochen. Für anders liegende Relationen, etwa von/nach Europa oder von/nach China ist die Abkürzung naturgemäß geringer.

Der Kanal wurde 1984 von der American Society of Civil Engineers in die List of Historic Civil Engineering Landmarks aufgenommen.

Der Staat Panama erstreckt sich etwa 700 Kilometer entlang des neunten Breitengrades, so dass der Atlantik beziehungsweise die Karibik nördlich und der Pazifik südlich von Panama liegen. Da der Kanal von der Einfahrt auf der atlantischen Seite aus in südöstlicher Richtung verläuft, liegt diese atlantische Einfahrt westlicher als die pazifische Ausfahrt bei Panama-Stadt. Mit anderen Worten: Der Panamakanal führt auf Grund dieser geographischen Besonderheit in West-Ost-Richtung von dem östlicher gelegenen Atlantischen Ozean in den westlich gelegenen Pazifischen Ozean.

Auf der atlantischen Seite gilt die Einfahrt in die Limón Bay (Bahía Limón) zwischen den Wellenbrechern von Colón und Fort Sherman (Fuerte Sherman) als Beginn des Kanals. Die Limón Bay dient als Reede für wartende Schiffe. Ihr 8,7 Kilometer von der Einfahrt entferntes südliches Ende ist gleichzeitig der Beginn des 3,1 Kilometer langen Zufahrtskanals zu den Gatún-Schleusen.

In den Gatún-Schleusen werden die Schiffe zu dem im Mittel 26 Meter über dem Meeresspiegel liegenden, künstlich aufgestauten Gatúnsee gehoben.

Von den Gatún-Schleusen aus ist in südwestlicher Richtung der Gatún-Damm zu sehen, ein 2.300 Meter langer Erddamm, in dessen Mitte eine bogenförmige Betonmauer mit 14 großen Stahltoren den Abfluss aus dem See steuert. Hinter dem Damm liegt ein Wasserkraftwerk, das den Schleusenbetrieb und die Steuerung des Damms mit Strom versorgt. Mit dem Damm wurde der Río Chagres so weit aufgestaut, dass die Schiffe große Strecken im Gatúnsee fahren können und außerdem der Durchstich des Kanals durch die Berge nicht bis unter den Meeresspiegel ausgehoben werden musste.

Im Gatúnsee folgt der Kanal in einer ausgebaggerten und gut betonnten, etwa 29 Kilometer langen Fahrrinne dem früheren Bett des Río Chagres. Die Fahrrinne umgeht dabei verschiedene Inseln, unter anderem die größte und bekannteste Insel, das Barro Colorado Island (BCI) mit der gleichnamigen Forschungsstation der Smithsonian Institution. Auf Grund der seit 1946 durchgeführten intensiven Beobachtung durch internationale Forscher gilt der tropische Regenwald auf BCI als der besterforschte der Erde. Kleinere Boote, wie die Dienstboote der Panama Canal Authority und private Segeljachten, können im Gatunsee eine "Banana Cut" genannte Abkürzung durch eine enge Passage zwischen zwei kleineren Inseln nehmen.

Der Gatúnsee geht in den immer noch aufgestauten Teil des Río Chagres über, dem die Schiffe über 8 Kilometer lang bis zu dem Ort "Gamboa" folgen. Dort beginnt der rund 13 Kilometer lange "Culebra Cut" oder "Gaillard-Durchstich" durch die Berge der kontinentalen Wasserscheide. Der Kanal war auf dieser Strecke ursprünglich nur 152 Meter breit, so dass für große Schiffe kein Begegnungsverkehr möglich war. 2002 wurde der Abschnitt auf 192 Meter in den Geraden und auf 222 Meter in Kurven erweitert. Seither kann ständig in beide Richtungen gefahren werden, was zu einer Zeitersparnis von mehr als 16 % führte.

Kurz vor dem Ende des Gaillard-Durchstichs überquert die Puente Centenario (Centennial Bridge, Jahrhundertbrücke), eine sechsspurige Schrägseilbrücke, den Kanal in 80 Metern Höhe.

Der Gaillard-Durchstich endet an der Pedro-Miguel-Schleuse, die den Abstieg zum Pazifik einleitet und in den ebenfalls künstlich aufgestauten, nur rund zwei Kilometer langen Miraflores-See führt.

Auf der anderen Seite des 16,5 Meter über dem mittleren Wasserspiegel des Pazifik liegenden Sees befindet sich die Einfahrt in die Miraflores-Schleusen. Unmittelbar neben der Schleuseneinfahrt beginnt der Miraflores-Damm, eine 133 Meter lange Gewichtsstaumauer aus Beton mit acht anhebbaren Stahltoren zur Steuerung des Abflusses aus der Pedro-Miguel-Schleuse und aus dem in den See mündenden Río Cocoli.

Nach den Miraflores-Schleusen folgt ein natürlicher Zufahrtskanal vorbei an den Hafenanlagen von Balboa und unter der Puente de las Américas (Brücke der Amerikas) hindurch in den Golf von Panama. Die Puente de las Américas hat eine lichte Höhe von 61,3 m über MHW – Mittleres Hochwasser, weshalb die Höhe der Schiffe auf 57,91 m (190 Fuß) über der Wasserlinie begrenzt ist. Der Anfang beziehungsweise das Ende des Kanals auf der pazifischen Seite wird durch eine Boje bei der Isla Perico am Ende eines langen Dammes markiert. Die Strecke von den Schleusen bis zu der Boje ist gut 12 Kilometer lang. Der Damm dient hier weniger dem Schutz vor anbrandenden Wellen. Er soll vielmehr den Eintrag von Schlamm durch eine entlang der Küste verlaufende Strömung verhindern.

Der Kanal kann seit der Erweiterung des Gaillard-Durchstichs auch von großen Schiffen durchgehend in beiden Richtungen im Gegenverkehr befahren werden.

Um bei den zahlreichen Kurven und Richtungsänderungen keine Missverständnisse aufkommen zu lassen, werden alle vom Atlantik zum Pazifik fahrenden Schiffe als "southbound (südwärts fahrend)," die in der Gegenrichtung zum Atlantik fahrenden Schiffe als "northbound (nordwärts fahrend)" bezeichnet.

Der Panamakanal hat drei Schleusenanlagen, die für den Begegnungsverkehr und als Vorsorge für Wartungsunterbrechungen jeweils paarweise angelegt sind:

Es gibt somit auf beiden Seiten je drei Schleusenkammern pro Richtung, insgesamt also zwölf Schleusenkammern, die folgende Maße haben:

Die zulässigen Schiffsmaße sind deshalb:

Ein Schiff mit genau der zulässigen Breite und Länge hat somit an beiden Seiten noch 0,61 Meter Abstand zur Kanalwand und an Bug und Heck noch 5,33 Meter Abstand zu den Schleusentoren der kürzeren Kammern. Die Schiffsklasse, die an diese zulässigen Schiffsmaße angepasst ist, wird Panamax genannt. Schiffe, die diese Maße überschreiten, werden Post-Panamax-Schiffe genannt.

Das Luxuskreuzfahrtschiff Queen Elizabeth 2 wurde so konstruiert, dass es gerade noch in die Schleusen des Panamakanals passt (Panamax-Klasse). Die Nachfolgerin Queen Mary 2 ist deutlich größer und kann den Panamakanal erst seit Abschluss der Erweiterungen bei Niedrigwasser passieren.

Die beiden obersten Kammern der Gatún-Schleusen, die Pedro-Miguel-Schleuse und die oberste Kammer der Miraflores-Schleusen haben an beiden Enden aus Sicherheitsgründen doppelte Schleusentore im Abstand von ungefähr 24 m (80 Fuß). Die Gatún-Schleusen haben somit sechs Tore pro Richtung, die Pedro-Miguel-Schleuse vier Tore und die Miraflores-Schleusen fünf Tore pro Richtung, zusammen somit 15 Tore beziehungsweise in beiden Richtungen insgesamt 30 Tore. Jedes Tor hat zwei V-förmig zum Oberwasser schließende Torflügel, die beim Absenken des Unterwassers vom steigenden Wasserdruck gegeneinandergepresst und abgedichtet werden. Jeder Torflügel ist 19,81 m (65 Fuß) breit und 2,13 m (7 Fuß) stark. Seine Oberkante ist als Gehweg für das Schleusenpersonal ausgebildet. Der untere Teil der Torflügel enthält Hohlkörper, die ihm so viel Auftrieb geben, dass sie sich relativ leicht bewegen lassen. Die Höhe der Torflügel variiert je nach ihrem Einsatzort zwischen 14,33 m im oberen Tor der Gatún-Schleusen und 25 m in den Miraflores-Schleusen (zwischen 47 und 82 Fuß) und ihr Gewicht zwischen 320,96 Tonnen und 600,73 Tonnen (353,8 und 662,2 Short tons). Ursprünglich wurden die Tore von einem stählernen Arm bewegt, der an einem horizontalen Rad mit 6 Meter Durchmesser in den Wänden der Schleusenkammern angebracht war, ähnlich der Pleuelstange am Treibrad einer Dampflokomotive. Dabei genügten zwei 19-kW-Elektromotoren für die Bewegung eines Torflügels, der im Notfall auch von einem der Motoren bewegt werden konnte. Seit einigen Jahren werden die Torflügel hydraulisch bewegt.

Die parallelen Schleusenkammern sind durch eine 18,29 m (60 Fuß) dicke Betonwand getrennt, in deren oberem Teil sich ein Gang für das Personal, ein Kabelkanal und eine Abwasserleitung befinden. Die Seitenwände sind an der Basis 15,24 m stark und verjüngen sich nach oben auf rund 2 Meter. Im unteren Teil der drei Wände verlaufen die Zu- und Abflussleitungen der Schleusenkammern, die am Einlass zunächst einen Durchmesser von 6,7 m (22 Fuß) haben, der nach 97,5 m (320 Fuß) reduziert wird auf 5,49 m (18 Fuß). Je zehn elliptische Querverbindungen unter dem Boden der Kammern sind abwechselnd mit dem Seiten- und dem Mittelrohr verbunden. Sie haben jeweils fünf Öffnungen im Boden der Schleusenkammer, so dass das Wasser durch 100 Öffnungen im Boden mit geringen Turbulenzen ein- und ausfließen kann.

Die Schleusenkammern werden allein durch die Schwerkraft des Wassers gefüllt und geleert, es gibt dafür keine Pumpen. Es dauert ungefähr acht Minuten, um eine Kammer zu füllen oder zu leeren, das heißt, den Wasserspiegel vom Niveau des Unterwassers auf das des Oberwassers anzuheben und umgekehrt. Der Schleusungsvorgang mit Ein- und Ausfahrt der Schiffe dauert wesentlich länger als die genannten 8 Minuten. Dabei laufen typischerweise 101.000 m³ Wasser aus der Kammer oder in sie. Die Schleusen haben keine Sparbecken. Um Wasser zu sparen, kann jedoch die Verbindung zwischen den parallelen Schleusen genutzt werden. Eine Schleusung im Sinne einer Durchfahrt durch alle Schleusen soll 197.000 m³ Süßwasser verbrauchen (52 Millionen US-Gallonen).

Eine zügige und sichere Durchfahrt durch die Schleusenanlagen gewährleisten die beidseitig angebrachten Zahnradbahnen. Je nach Größe des Schiffes schleppen vier bis acht Zahnradlokomotiven ("Treidelloks," „Mulis“, nach den Lasttieren benannt) die Schiffe durch die Schleusen und stabilisieren sie gegen die Strömungen in der Schleusenkammer beim Wasserein- und -auslass. Dabei können sie auf bis zu 45 Grad steilen Rampen von einer Schleusenkammer zur nächsten fahren. Das Zahnradbahnsystem und die Lokomotiven wurden von Edward Schildhauer konstruiert, dem für die Planung der elektrischen Ausstattung der Schleusen zuständigen Ingenieur, und von Schenectady, New York, zu einem Preis von 13.000 US-Dollar pro Stück gebaut und geliefert. Um eine bessere Traktion zu erreichen, werden spezielle Zahnstangen verwendet, die auf dem System Riggenbach beruhen. Die 2012 im Einsatz befindlichen neuen Lokomotiven wurden 1998 bei der Mitsubishi Corporation bestellt. Sie wiegen rund 45 Tonnen (50 tons) und haben zwei Antriebseinheiten mit jeweils 290 PS, gegenüber den 170 PS des Vorgängermodells. Kleine Schiffe werden dagegen in traditioneller Weise mit von Hand gehaltenen Leinen geführt.

Der Wasserverlust durch die Schleusungen wird durch den Río Chagres ausgeglichen, wobei der Gatúnsee als Pufferspeicher wirkt. Da zunehmender Verkehr zwangsläufig zu einem größeren Wasserverlust führt, wurde bereits 1935 der Oberlauf des Río Chagres durch den Madden-Damm zum Alajuelasee aufgestaut, um die großen Wassermassen der Regenzeit für die Trockenzeit speichern zu können. Sollte der Wasserspiegel des Gatunsees dennoch unter die für den maximalen Tiefgang der Schiffe ausschlaggebende Niedrigwassermarke von 24,84 Meter (81,5 Fuß) fallen, wird der zulässige Tiefgang schrittweise um jeweils 15 cm reduziert, nach Möglichkeit mit dreiwöchiger Ankündigung.

Extrem hoher Zulauf kann weitgehend durch die großen Tore des Gatún-Dammes und durch die großen Rohre in den Schleusen abgeführt werden, um den Wasserspiegel im Gatúnsee nicht über 26,5 Meter (87 Fuß) ansteigen zu lassen. In großen zeitlichen Abständen führt der Río Chagres allerdings so viel Wasser, dass sowohl der Alajuela- als auch der Gatúnsee ihren maximalen Füllstand erreichen. So musste der Schiffsverkehr auf dem Panamakanal vom 8. zum 9. Dezember 2010 für 17 Stunden unterbrochen werden – zum dritten Mal in der bis dahin 96-jährigen Geschichte des Kanals.

Nach den traditionellen Regeln der Autoridad del Canal de Panamá (ACP) erfolgt die Durchfahrt in der Reihenfolge der Ankunft. Dabei können Wartezeiten von mehreren Stunden entstehen, im Fall der Schließung einer "Fahrspur" wegen Reparaturarbeiten an den Schleusenkammern auch von mehreren Tagen. Deshalb können pro Tag 24 Passagen (von insgesamt etwa 40 möglichen Durchfahrten) in einem komplexen Verfahren im Voraus gebucht werden, eine 25. Passage wird von der ACP versteigert.

Die Autoridad del Canal de Panamá (ACP) hat für den Schiffsverkehr 291 Lotsen und 36 Schlepper zur Verfügung, während in den Schleusen 100 Lokomotiven im Einsatz sind (Stand Mai 2012). Die Schiffe müssen einer Reihe von technischen Vorschriften entsprechen, die vor der ersten Passage nachgewiesen werden. Im Kanal besteht Lotsenpflicht, eine Passage wird von bis zu drei Lotsen gleichzeitig begleitet. Vor den Schleusen wird eine Gruppe von "Pasacables" (in etwa: Schiffsbefestiger) an Bord genommen, die die Leinenverbindung zu den Lokomotiven herstellen und bis zur Ausfahrt aus der Schleuse überwachen. Im engen und kurvigen Culebra Cut erhalten große Schiffe meist die Unterstützung von ein oder zwei Schleppern. Die Höchstgeschwindigkeit wird von der ACP für die verschiedenen Kanalabschnitte je nach den Umständen festgelegt, meist wird mit 6 kn (11 Kilometer/h) gefahren.

Die Gebühren für eine Passage werden nach einer seit 2011 geltenden, komplexen Gebührentabelle berechnet, bei der unterschieden wird zwischen Art und Größe des Schiffes, ob es beladen oder unter Ballast fährt und die eine Vielzahl von Grund- und Nebengebühren enthält, wie beispielsweise die Gebühren für Lotsen, "Pasacables," Lokomotiven und Schlepper. Als Anhaltspunkt für die anfallenden Gebühren können die Basisgebühren von 72 US-Dollar pro Standardcontainer und von 134 US-Dollar pro Passagier-Bett dienen. Für ein Containerschiff mit den maximal möglichen 4.600 TEU würden daher Grundgebühren von 340.400 US-Dollar anfallen zuzüglich der genannten Nebengebühren. Für ein auf die Panamax-Maße hin konstruiertes Kreuzfahrtschiff wie die Coral Princess oder ihr Schwesterschiff Island Princess mit jeweils 1.970 Passagierbetten wird deshalb eine Grundgebühr von 263.980 US-Dollar zuzüglich der Buchungsgebühr für die Passage und den sonstigen Nebengebühren bezahlt, so dass insgesamt knapp 400.000 US-Dollar anfallen (Stand April 2012). Zum Vergleich: Die Queen Elizabeth 2 musste für ihre Panamakanal-Passage im Jahre 2003 vor der Umstellung der Gebühren auf die Anzahl der Passagierbetten nur 99.000 US-Dollar bezahlen.

Ebenfalls vor der Einführung der neuen Gebührentabelle betrug der höchste Preis für eine versteigerte Passage 220.300 US-Dollar für einen Tanker, der im August 2006 dadurch eine Warteschlange von 90 Schiffen passieren und eine siebentägige Wartezeit wegen der Schließung einer Schleuse vermeiden konnte. Die normale Gebühr hätte nur 13.430 US-Dollar betragen.

Die niedrigste Gebühr fiel im Jahre 1928 für den US-Amerikaner Richard Halliburton (1900–1939) an, den ersten Menschen, der den Panamakanal durchschwamm. Der weltreisende Journalist und Abenteurer war im April 1928 von New Orleans aus nach Mexiko aufgebrochen. Dort wandelte er auf den Spuren von Hernán Cortés und bestieg den Popocatépetl. Krönender Abschluss seiner Tour durch Mittelamerika sollte die Benutzung des Panamakanals als Schwimmer werden. Die US-Kanalbehörde war einverstanden unter dem Vorbehalt, dass sie nur einem „Wasserfahrzeug“ die „Durchfahrt“ gestatten könne. Halliburton wurde daher nach dem Schiffsmaß Tonnage vermessen und eingestuft und durfte gegen eine Gebühr von 36 US-Cent, das niedrigste jemals entrichtete Entgelt, seinen Plan umsetzen. In acht Tagesetappen bewältigte er die Strecke inklusive aller Schleusen.

Die Autoridad del Canal de Panamá (ACP) unterhält eine Verkehrslenkungszentrale, die Wasserkraftwerke am Gatún- und am Madden-Damm, das thermoelektrische Kraftwerk von Miraflores, eigene Leitungsnetze für Strom, Wasser und Telefon sowie verschiedene Schiffe zur Instandhaltung des Kanals, darunter einen großen Greiferbagger und ein Saugbaggerschiff sowie eine Bohrplattform für die Vorbereitung von Sprengungen. Vier Schwimmkräne mit einer Kapazität von 75 bis 386 Tonnen dienen unter anderem der Wartung der Schleusentore. Zwei Trinkwasseraufbereitungsanlagen an den beiden Enden des Kanals versorgen nicht nur die ACP, sondern auch große Teile der umliegenden Städte. Eine Schiffswerft dient der Wartung und Reparatur der Wasserfahrzeuge der ACP.

Die wichtigsten Querungen des Panamakanals sind

Seit 1942 gibt es die Drehbrücke in den Miraflores-Schleusen, die heute wegen der nahegelegenen festen Straßenverbindungen kaum noch genutzt wird.

Bei den Gatún-Schleusen gibt es quer zu den unteren Einfahrten eine einspurige bewegliche Brücke für kleinere Kraftfahrzeuge. So wie die Torflügel der Schleusentore hat sie zwei Brückenteile pro Einfahrt, die sich bei Schiffsverkehr ebenso wie die offenen Torflügel in Nischen entlang den Schleusenwänden befinden, sonst aber wie ein sich schließendes Tor in die Einfahrt gedreht werden können.

Im Rahmen des Ausbauprogramms wird an der atlantischen Seite die 1.050 Meter lange "Atlantic Bridge" gebaut, eine Schrägseilbrücke mit zwei Pylonen.

Nahe der Puente Centenario queren zwei Hochspannungsleitungen den Kanal mit einer Spannweite von je 1,8 Kilometer.

Nach jahrelanger Diskussion wurden im April 2006 die Pläne für den Ausbau verkündet. Ein verfassungsgemäßes Referendum wurde am 22. Oktober 2006 durchgeführt, wobei 78 Prozent der Wähler bei 43 Prozent Wahlbeteiligung für den Ausbau des Kanals stimmten. Das Ergebnis ist aufgrund der Torrijos-Carter-Verträge von 1978 für die Regierung bindend.

Der Panamakanal wurde seit 2007 ausgebaut und erweitert. Dazu wurde auf beiden Seiten, der atlantischen und der pazifischen Seite, neben den existierenden je eine neue, dreistufige Schleusenanlage gebaut, deren Kammern 55 m breit und 427 m lang sind. Die neue pazifische Schleusenanlage umgeht dabei mit verlängerten Zufahrtskanälen sowohl die Miraflores- als auch die Pedro-Miguel-Schleusen. Die beiden neuen Schleusen wurden als Sparschleusen mit je drei Sparbecken pro Kammer ausgelegt. Außerdem sollen die Fahrrinnen im Gatúnsee erweitert und vertieft und der Culebra Cut vertieft. Die alten Schleusen sind weiterhin in Betrieb. Auf der atlantischen Seite wird eine weitere Straßenverbindung durch eine Schrägseilbrücke mit zwei Pylonen geschaffen, die Anfang 2015 eröffnet werden soll.

Mit einer feierlichen Eröffnung am 3. September 2007, dem 30. Jahrestag des Abkommens über die Rückgabe der Kontrolle des Kanals von den Vereinigten Staaten an Panama, wurde der Ausbau begonnen. Im Beisein des früheren US-Präsidenten Jimmy Carter, zahlreicher lateinamerikanischer Staatschefs sowie rund 40.000 Schaulustiger ließ Präsident Martín Torrijos, der Sohn von Omar Torrijos, mehr als 13 Tonnen Sprengstoff explodieren. In der ersten Bauphase mussten rund 47 Millionen Kubikmeter Erde und Gestein abtransportiert werden. 40.000 Arbeiter waren beteiligt. Die neuen Schleusen wurden am 11. Juni 2015 probeweise geflutet.

Der finanzielle Aufwand war schwer abzusehen. Kritiker bezifferten ihn auf rund 8 Milliarden US-Dollar, was rund der Hälfte des jährlichen panamaischen Bruttoinlandsprodukts entspricht. Alberto Alemán Zubieta, der Chef der ACP, bezifferte die Ausbaukosten auf 5,24 Milliarden Dollar. Die Finanzierung sollte durch vorgezogene höhere Kanalgebühren und Kredite sichergestellt werden. Die Kanal-Behörde rechnete bis 2025 mit Einkünften von 4,8 Milliarden Euro. Die Höhe der Kosten war vor allem deshalb umstritten, weil unter anderem zugleich eine Kürzung der Renten diskutiert wurde. Durch die Verzögerung des Ausbaus wurden zusätzliche Kosten von 2,39 Milliarden US-Dollar angemeldet, wovon 227 Millionen Dollar genehmigt wurden.

Gegen den Ausbau des Kanals wurden ökologische Bedenken ins Feld geführt. Der Wasserbedarf des vergrößerten Kanals mit eventuell vergrößerten Stauseen wäre größer und könnte einen erheblichen Eingriff in die Landschaft bedeuten. Diese befindet sich zwar nicht mehr in einem Naturzustand, sondern ist schon seit präkolumbischer Zeit und insbesondere infolge des Kanalbaus bis 1914 eine ausgeprägte Kulturlandschaft. Sie ist aber nicht zuletzt deswegen wichtiger ökologischer Lebensraum und auch wichtige Süßwasserquelle für die Stadt Panama. Die neuen Kanalschleusen sollen deshalb als Sparschleusen mit mehreren Kammern ausgeführt werden, um den Gesamtwasserbedarf zu minimieren. Angeblich sollen keine vergrößerten Wasserreservoirs nötig werden.

Letzterem widersprach der US-Wissenschaftler und Ingenieur Bert G. Shelton in einem am 5. Januar 2011 veröffentlichten Artikel. Darin wies er darauf hin, dass mehrere Verfahren bekannt seien, die benötigten Wassermengen zu reduzieren, von denen nur zwei in das Projekt eingeflossen seien. Shelton gibt an, dass bei Anwendung weiterer Kriterien mit dem gleichen oder geringfügig höheren Kostenaufwand die Kapazität des Kanals erheblich erweitert und damit seine Rentabilität erhöht werden könnte, die Versalzung des Gatun-Sees verhindert, die Instandhaltungskosten verringert und der Bau zusätzlicher Vorhaltebecken vermieden werden könnte. Unter anderem kritisierte er den Bau eines Dammes über einer bekannten tektonischen Bruchlinie, der bei einem Bruch die wichtige Verbindung zwischen Atlantischem und Pazifischem Ozean gefährden und ein ökologisches Desaster auslösen könnte. Shelton äußerte die Vermutung, dass in erster Linie große finanzielle Interessen für das Festhalten an einem vorgefassten und in seiner Sicht mangelhaften Entwurf bei gleichzeitiger Fehlinformation der Öffentlichkeit verantwortlich seien.

Die Erweiterung des Kanals wurde am 26. Juni 2016 feierlich in Betrieb genommen. Das erste Schiff, welches sie passieren durfte, war das unter der Flagge der Marshallinseln fahrende Containerschiff "Cosco Shipping Panama."

1513 überquerte der Spanier Vasco Núñez de Balboa mit einer Gruppe von Gefolgsleuten als erster die Landenge. Die Idee einer Verbindung zwischen Atlantik und Pazifik durch einen Kanal in der mittelamerikanischen Provinz Darién wurde 1523 von Kaiser Karl V. angeregt. In seinem Auftrag suchte Hernando de la Serna 1527 nach einem geeigneten Weg für den Bau eines Kanals.

Das erste Projekt wurde 1529 vom Spanier Alvarado de Saavedra Colon ausgearbeitet. In den folgenden Jahrhunderten beschäftigte sich eine Reihe von Politikern und Wissenschaftlern mit der Frage eines Kanalbaus. Anfang des 19. Jahrhunderts galt das insbesondere für Alexander von Humboldt, der von 1799 bis 1804 Lateinamerika erforschte. Johann Wolfgang von Goethe prophezeite 1827, dass es der „jugendliche Staat“ der Vereinigten Staaten sein werde, der einen Kanal bauen würde. Nach Goldfunden in Kalifornien wurde 1848 eine Lizenz für eine Eisenbahnverbindung vergeben. 1849–1853 nutzten Goldgräber eine Fluss-Land-Route durch das Isthmusgebiet von Panama.

Nach dem finanziellen Erfolg des 1869 eröffneten Sueskanals in Ägypten wurde in Frankreich davon ausgegangen, dass ein Kanal, der Atlantik und Pazifik miteinander verbindet, ebenso einfach zu bauen wäre. Diese Gedanken nahmen Gestalt an, als 1876 in Paris die "Société Civile Internationale du Canal Interocéanique" geschaffen wurde, der 1879 durch französisches Gesetz die Panamakanal-Gesellschaft folgte, zu deren Präsidenten der 73-jährige Graf Ferdinand de Lesseps, der Erbauer des Sueskanals, ernannt wurde. Die Panamakanal-Gesellschaft übernahm eine 1878 von der Société Civile Internationale du Canal Interocéanique erworbene Konzession der kolumbianischen Regierung, die sogenannte Wyse-Konzession, und begann 1881 mit den Arbeiten, die bis 1889 andauerten.

Gebaut werden sollte ein schleusenloser Kanal über den Isthmus von Panama mit einer Länge von 73 Kilometern. Der Aushub sollte 120 Millionen Kubikmeter nicht übersteigen. Eine Aktiengesellschaft, die "Compagnie Universelle du Canal Interocéanique," wurde zur Finanzierung gegründet und versprach genauso hohe Rentabilität wie die Sueskanal-Aktien. 287 Millionen US-Dollar wurden investiert.

Während der Bauarbeiten emittierte die "Compagnie Universelle du Canal Interocéanique" mit großem publizistischen Aufwand wiederholt neue Schuldverschreibungen. Der ständig steigende Finanzbedarf konnte damit nicht befriedigt werden. Ähnlich wie beim Bau des Sueskanals sollte eine Lotterie das notwendige Kapital einbringen. In der französischen Nationalversammlung war die erforderliche gesetzliche Ermächtigung anfangs hoch umstritten. Die Widerstände überwand die "Compagnie Universelle du Canal Interocéanique" 1888 durch Bestechung in Form von Geldzahlungen an mehr als einhundert Abgeordnete. Eine objektive Berichterstattung über die technischen und finanziellen Unzulänglichkeiten des Projekts wurde durch Journalistenbestechung und Einflussnahme auf die Zeitungsverlage durch die "Compagnie Universelle du Canal Interocéanique" hintertrieben, Öffentlichkeit und Anleger wurden planmäßig und systematisch belogen. 1887 revidierte Ferdinand de Lesseps unter dem Druck der schlechten Finanzlage die Pläne und schloss mit dem Ingenieur Gustave Eiffel einen Vertrag ab, um einen Schleusenkanal bis 1890 herzustellen. Die Kosten für den Schleusenkanal wurden auf 1,6 Milliarden Goldfranken geschätzt. Wegen Planungsmängeln, falscher geologischer Untersuchungen, schlechter Organisation, Bestechung, unzähliger technischer Schwierigkeiten und Pannen gaben die Franzosen schließlich aus finanziellen und politischen Überlegungen auf und stellten die Arbeiten 1889 ein.

Am 15. Dezember 1888 trat die Zahlungsunfähigkeit der "Compagnie Universelle du Canal Interocéanique" ein. Wegen des nichtkommerziellen Unternehmensgegenstandes der Gesellschaft, des Baus eines Kanals, wurde diese nach damaliger gesetzlicher Regelung als nichtkonkursfähige Gesellschaft behandelt, was ein vermögensauseinandersetzendes Konkursverfahren ausschloss. Der Zusammenbruch löste den Panamaskandal, einen der größten Finanzskandale des 19. Jahrhunderts in Frankreich, aus und zog heftige öffentliche Auseinandersetzungen und Politikerrücktritte nach sich.

Die Arbeiten am Bau bewirkten die Migration von über 100.000 Arbeitern in die Region, wovon die meisten niemals heimkehrten (Die Originalberichte zu dieser Migration sind inzwischen von der UNESCO als Weltdokumentenerbe eingestuft). In der Bauzeit von 1881 bis 1889 starben bei dem Bau 22.000 Arbeiter (7,5 Menschenleben pro Tag) in der Sumpflandschaft an Gelbfieber und Malaria, deren Erreger noch unbekannt waren. Auf Anraten französischer Ärzte wurde 1883 beim Bau des Kanals angeordnet, zum Schutz vor Malaria die Pfosten der Betten der Arbeiter in Wassereimer zu stellen. Die Eimer wurden allerdings zu Brutstätten der Malariamücken, die Krankheit breitete sich rasend schnell aus, der Bau musste nicht zuletzt auch deshalb abgebrochen werden. Rund um die Baustelle standen Kreuze; Leichen wurden in Essigfässern nach Europa verschifft, damit nicht noch mehr Kreuze aufgestellt werden mussten. 

1894 übernahm eine Auffanggesellschaft, die "Compagnie Nouvelle du Canal de Panama," die Fortführung der theoretischen Arbeiten und verkaufte 1902 den Gesamtkomplex für 40 Millionen US-Dollar an die Vereinigten Staaten, die etwa 40 % der bis dahin geleisteten praktischen Arbeiten verwenden konnten. Senator John Coit Spooner hatte Präsident Theodore Roosevelt überzeugt, dass nun die Gelegenheit gekommen war, beim im Panama steckengebliebenen Projekt einzusteigen. Mit dem 1902 verabschiedeten „Spooner Act“ machte der Kongress den Weg dafür frei.

Die Vereinigten Staaten hatten sich bereits vorher mit verschiedenen Kanalprojekten einer Querung Mittelamerikas beschäftigt, waren jedoch bis dahin zu keinem praktikablen Ergebnis gelangt. Die Planungen des Panamakanals konkurrierten mit Planungen zum Nicaragua-Kanal. Dieser wurde jedoch nicht realisiert, da dessen Investoren den Panamakanal favorisierten. Kolumbien weigerte sich, es entstand der Panamakonflikt. Nach dem Ankauf der Wyse-Konzession verlangten die Vereinigten Staaten von Kolumbien die Abtretung des Panamakanalgebiets.

Im November 1903 landeten US-Truppen, besetzten das Gebiet und riefen den unabhängigen Staat Panama aus. Die US-Regierung glaubte, so schneller den Bau des aus strategischen Gründen für absolut notwendig erachteten Kanals zu erreichen. Am 18. November 1903 vereinbarten der damalige US-Außenminister, John Hay, und ein ehemaliger Mitarbeiter von Ferdinand de Lesseps, der französische Ingenieur Philippe Bunau-Varilla einen Staatsvertrag – den sogenannten Hay-Bunau-Varilla-Vertrag, nach dem Ende der Ratifikation am 26. Februar 1904 in Washington verkündet und auf unbestimmte Zeit gültig – über die Nutzung einer Kanalzone in der Breite von zehn Meilen (16 Kilometern), jeweils fünf Meilen beiderseits der Kanaltrasse, ihre Besetzung und ihre unbeschränkte Kontrolle. Die Vereinigten Staaten kontrollierten damit eine Kanalzone in einer Größe von 84.000 Hektar. Sie mussten aber die territoriale Souveränität Panamas zusichern. Der Vertrag sah zudem die Zahlung von 10 Millionen US-Dollar vor sowie eine ab 1913 beginnende jährliche Zahlung von 250.000 US-Dollar in Gold.

Am 6. Mai 1904 ernannte Präsident Roosevelt John Findley Wallace zum leitenden Ingenieur, der nach einem Jahr aufgab. Die Feldbahn konnte die anfallenden Erdmengen nicht abtransportieren. Außerdem wurde der Bau durch großen bürokratischen Aufwand – jede Entscheidung von Wallace musste durch die Isthmus-Kanal-Kommission (ICC) bestätigt werden – erschwert.

Im April 1905 wurde der Kanalbau dem Ingenieur John Frank Stevens anvertraut. Er erkannte, dass die größten Schwierigkeiten die Krankheiten darstellten und er daher zuerst die Lebensumstände der Arbeiter verbessern musste. Mit seinem Einsatz erwarb er sich ihren Respekt, so dass er sich mit der Planung der Logistik und dem Aufbau der Organisation der eigentlichen Herausforderung stellen konnte. Als er damit fertig war, kündigte er überraschend und zum Unmut Roosevelts mit der Erklärung, er habe seinen Vertrag aufs Wort erfüllt. In diesem hieß es, er solle solange daran arbeiten, bis er selbst mit Sicherheit sagen könne, dass es gelingen oder scheitern wird. Die Gründe für seine Kündigung sind umstritten. Vermutet wird, dass er begriff, der beste Mann für die Planung, aber nicht für die Ausführung zu sein. Eine andere Anekdote besagt, er habe einfach den für ihn interessanten Teil und die eigentliche Herausforderung bewältigt, die Ausführung habe ihn nur noch gelangweilt. In einem Brief seines Nachfolgers an dessen Sohn wird davon gesprochen, Mr. Stevens habe den Bau so perfekt organisiert, dass es für ihn eigentlich nichts zu tun gäbe, außer die Organisation zu erhalten.

Im April 1907 verließ Stevens den Kanal, und die Arbeit wurde von Generalmajor George Washington Goethals fortgesetzt, der besonders von US-Präsident Theodore Roosevelt unterstützt wurde. Roosevelt hatte ihn unter anderem deshalb ausgesucht, weil er – anders als sein Vorgänger – als Militärangehöriger nicht kündigen konnte. Die Kosten des nun mit Schleusen und Stauseen erbauten Kanals beliefen sich auf 386 Millionen US-Dollar. Während der Bauarbeiten 1906–1914 starben 5.609 Arbeiter an Unfällen und Krankheiten (noch etwa 1,9 Todesfälle pro Tag). Insgesamt forderte der Bau somit circa 28.000 Menschenleben.

Am 3. August 1914 passierte der kleine Frachter "Cristobal," ein Doppelschraubendampfer, als erstes Wasserfahrzeug den Panamakanal in voller Länge. Wegen des Ausbruchs des Ersten Weltkriegs – noch am selben Tag – wurden die Eröffnungsfeierlichkeiten abgesagt und erst 1920 nachgeholt. Am 12. Juli 1920 gab US-Präsident Woodrow Wilson die Wasserstraße offiziell für den Schiffsverkehr frei.

Die Tatsache, dass die Vereinigten Staaten die Hoheit über den Kanal und die Panamakanalzone, einen Landstreifen entlang des Kanals, behielten, sorgte wiederholt für Spannungen zwischen der US-Regierung und Panama. 1935 und 1936 erfolgten die ersten Revisionen der entsprechenden Abkommen, die unter anderem eine Erhöhung der jährlichen Zahlungen der Vereinigten Staaten an Panama auf 430.000 US-Dollar und die Gewährung eines Landkorridors durch die Kanalzone an Panama vorsahen und den Vereinigten Staaten das Interventionsrecht absprachen.

1955 erfolgte eine weitere Revision, in der die Jahrespacht auf 1,93 Millionen US-Dollar erhöht wurde. Zudem gab es eine Erhöhung der Löhne der panamaischen Arbeiter der Panamakanalverwaltung. Panama erhielt auch das Recht, die in der Kanalzone tätigen Arbeiter mit Ausnahme der US-Bürger zu besteuern.

1960 gab US-Präsident Dwight D. Eisenhower auf Grund von Beschwerden der Regierung von Panama ein 9-Punkte-Programm bekannt, das insbesondere verbesserte Arbeits- und Wohnbedingungen in der Kanalzone und den Bau einer Wasserleitung zur Versorgung der Hauptstadt Panama-Stadt vorsah.

Am 17. September 1960 gab US-Präsident Eisenhower bekannt, dass ab sofort die Flagge Panamas gemeinsam mit der der Vereinigten Staaten in der Kanalzone als Ausdruck der nominellen Souveränität Panamas über dieses Gebiet gehisst werden solle. Die erste Hissung der panamaischen Flagge fand am 21. September 1960 auf dem Shaler Triangle statt.

1964 kam es zu gewaltsamen Auseinandersetzungen zwischen Zivilisten und der US-Armee im Flaggenstreit zwischen den Vereinigten Staaten und Panama.

1977 handelte US-Präsident Jimmy Carter mit General Omar Torrijos die Torrijos-Carter-Verträge aus, nach denen der Kanal bis zum Jahre 2000 an Panama zurückzugeben sei, was dann am 31. Dezember 1999 um 12 Uhr geschah. Seitdem wird der Kanal von der Panamakanal-Behörde (span. "Autoridad del Canal de Panamá – ACP") verwaltet. Die Behörde ist autonom, allerdings wird ihr Vorstand vom panamaischen Präsidenten ernannt.

Am 15. Juli 2013 stoppte die Behörde ein Schiff, das – verborgen unter Zuckersäcken – zwei Container mit (vermutlich) Raketenausrüstung von Kuba nach Nordkorea transportieren sollte. Panamas Präsident Martinelli erklärte dazu: „Man kann nicht einfach undeklarierte Waffen durch den Panamakanal transportieren. Der Panamakanal ist ein Kanal des Friedens und nicht des Krieges“.

Im Zusammenhang mit den Auseinandersetzungen um den Panamakanal 1964 wurden erneut Pläne diskutiert, Mittelamerika an einer anderen Stelle zu durchstechen, da einem Ausbau des Panamakanals Grenzen gesetzt waren und seine Verkehrskapazität Ende der 1960er Jahre erschöpft war. Erwogen wurden Durchstiche in Mexiko, Nicaragua/Costa Rica, Panama und Kolumbien. In allen Fällen sollte der neu zu bauende Kanal ohne Schleusen angelegt werden, da der Niveau-Unterschied zwischen Pazifik und Atlantik für minimal gehalten wurde.

Alle unten kurz aufgeführten Projekte würden eine Bauzeit von fünf bis zehn Jahren erfordern und mehrere Milliarden US-Dollar an Baukosten verschlingen.

Die Mexiko-Planung sah einen Durchstich am Isthmus von Tehuantepec von der Mündung des Flusses Coatzacoalcos am Atlantik entlang des Flusses zum Golf von Tehuantepec vor. Bei konventionellen Arbeitsmethoden sollte der Kanal 266 Kilometer lang sein und damit mehr als dreimal so lang wie der Panamakanal. Die Vereinigten Staaten erwogen zudem im Rahmen der Operation Plowshare Sprengungen mit Hilfe von Nuklearladungen, die dann eine Kanallänge von 241 Kilometer ergeben hätte.

Schon Mitte des 16. Jahrhunderts hatten spanische Kolonialisten einen Kanal entlang des San-Juan-Flusses zwischen dem heutigen Nicaragua und Costa Rica im Süden angedacht.
Der amerikanische Eisenbahnmagnat Cornelius Vanderbilt (1794–1877) verdiente einen Teil seines Vermögens durch den Goldrausch, indem er in den 1840er und 1850er Jahren Abenteurer und Goldsucher von der Ostküste Nordamerikas – via Nicaragua – nach Kalifornien brachte. Damals gab es noch keine Eisenbahnverbindung durch Nordamerika. Vanderbilt sicherte sich die Rechte für einen Kanalbau und eine Route durch Nicaragua für seine "Accessory Transit Company."

Einen Kanal durch Nicaragua zu bauen erwog auch Napoleon III. Frankreichs letzter Kaiser (1852–1870). Seine Französische Intervention in Mexiko von Januar 1862 bis März 1867 endete mit einer Niederlage und der Hinrichtung des von den Franzosen 1864 als Kaiser von Mexiko eingesetzten Maximilian I.

Der Durchstich durch Nicaragua war parallel zur Grenze mit Costa Rica projektiert und sollte den südlichen Teil des Sees von Nicaragua durchschneiden, die konventionelle Trasse hätte 270 Kilometer und eine Trasse mit Hilfe von Nuklearsprengungen 225 Kilometer betragen. Der Nicaragua-Kanal hätte aber die Grenze mit Costa Rica geschnitten und würde den Pazifik auch auf dem Gebiet von Costa Rica erreichen. Nicaragua will die Pläne mit einem Gesetz vom 3. Juli 2012 wiederbeleben.
Dieses Gesetz überträgt einer "Hong Kong Nicaragua Canal Development Investment Co. (HKND)" die Rechte für zunächst 50 Jahre (verlängerbar um weitere 50 Jahre) für den Bau und den Betrieb des Kanals. HKND bzw. seinem Besitzer, Investor Wang Jing, wird 49 % und Nicaragua 51 % der Anteile gehören.

Im Sommer 2014 gab HKND die Streckenführung des El Gran Canal, von der Mündung des Río Punta Gorda an der Karibikküste zur Mündung des Río Brito auf der Pazifikseite, bekannt und verkündete, dass mit den Baumaßnahmen noch im Jahr 2014 begonnen werden soll. Der Kanal wird mit der geplanten Streckenführung eine Länge von 278 Kilometer sowie eine Breite zwischen 230 und 530 Metern haben.

Für Panama gab es zwei mögliche neue Kanalrouten, die ausgemessen wurden:

Das Kanal-Projekt in Kolumbien sah einen Durchstich unter Nutzung der Flussbetten des Atrato und des Truando vor und sollte eine Länge von 153 Kilometer haben. Seit Anfang 2011 wird zwischen der Volksrepublik China und Kolumbien über den Bau einer Eisenbahnlinie als „trockener Konkurrenz“ zum Panamakanal verhandelt. Eine andere Alternative wäre über den Fluss San Juan, der in den Pazifik mündet. Dies erfordert aber die Teilung eines Berges und würde dann nur kleineren Schiffen von Nutzen sein.







</doc>
<doc id="4130" url="https://de.wikipedia.org/wiki?curid=4130" title="Phidias">
Phidias

Phidias (; * um 500/490 v. Chr. in Athen; † um 430/420 v. Chr.) war ein antiker Bildhauer und Toreut. Er gilt als einer der größten Bildhauer der Antike und als prominenter Vertreter der griechischen Hochklassik. Seine Arbeiten sind vollständig vernichtet worden und heute nur in Kopien greifbar. Die bekanntesten Werke sind die 12 Meter hohe Zeusstatue in Olympia, die zu den Sieben Weltwundern der Antike zählte, und die Athena Parthenos in Athen.

Phidias wurde um 500/490 v. Chr. als Sohn des Charmides in Athen geboren (nicht zu verwechseln mit dem Athener Politiker Charmides). Über sein Leben ist sonst sehr wenig bekannt. Allein durch die Beschreibung seines Werkes kann ein ungefährer Schattenriss des Lebens sichtbar gemacht werden.

Mit großer Wahrscheinlichkeit stand Phidias in engem persönlichen Kontakt zu den Radikaldemokraten, die 462/461 v. Chr. in Athen an die Macht gelangten und deren bekannteste Vertreter Ephialtes und Perikles waren.

Zum letzten Lebensabschnitt gibt es zwei widersprüchliche Überlieferungsstränge. Gemäß der einen Überlieferung wurde Phidias, nachdem er die Statue des olympischen Zeus fertiggestellt hatte, von den Gegnern des Perikles angeklagt, Gold von der bereits zuvor geschaffenen Statue der Athena Parthenos gestohlen zu haben. Dieser Beschuldigung konnte er sich jedoch entziehen. Daraufhin warfen ihm Perikles’ Gegner Gotteslästerung vor, da er sich selbst sowie Perikles auf dem Schild der Athena Parthenos dargestellt hatte, und er wurde eingekerkert. Angeblich starb er kurz darauf an einer Vergiftung. Nach einer anderen Fassung konnte er nach Elis fliehen und schuf die Statue des olympischen Zeus erst nach Anklage und Flucht in der Verbannung. Das Todesdatum, selbst das Ende seiner Schaffenszeit gilt heutzutage allgemein als ungeklärt und wird eher um 420 v. Chr. als um 430 v. Chr. angenommen. Es gibt keinen gesicherten Grund, in der Statue des Zeus sein letztes Werk zu sehen.

Phidias erhielt eine erstklassige Ausbildung bei den Bildhauern Hegias und Ageladas von Argos, der auch Lehrer von Myron und Polyklet gewesen sein soll. Die Ausbildung umfasste nicht nur die Bildhauerei in Stein, sondern auch den Bronzehohlguss in verlorener Form. Aufbauend auf diese Verfahren entwickelte Phidias später die Kunst weiter, in seinen Bildwerken unterschiedliche Materialien wie Marmor, Bronze, Glasfluss, Gold und Elfenbein zu vereinigen.

Das früheste Werk, mit dem Phidias sein Können unter Beweis stellte, war ein Athenastandbild aus Gold und Elfenbein für den Tempel in Pellene in der nördlichen Peloponnes. Es dürfte um 465 v. Chr. entstanden sein.

Phidias’ erstes großes Werk war eine Bronzegruppe, das aus dem traditionell den Göttern vorbehaltenen Zehnten aus der Schlacht bei Marathon (490 v. Chr.) finanziert wurde. Es umfasste die zehn Bronzestatuen der attischen Phylenheroen: Aias, Aigeus, Akamas, Antioches, Erechtheus, Hippothoon, Kekrops, Leos, Oineus und Pandion. Darüber hinaus waren vielleicht auch Athene als Schutzgöttin Athens und Apollon als Schutzgott Delphis dargestellt. Sie standen auf einer 16 m langen Basis vor dem Schatzhaus der Athener in Delphi. Die 10 Phylenheroen waren Ausdruck des Sieges der jungen Demokratie über alle inneren und äußeren Feinde. Gleichzeitig setzte man sich mit dem Monument unter Verweis auf die kleisthenischen Reformen bewusst von dem politischen System der Tyrannis ab. Weitere Heroen sowie ein Standbild des Miltiades, über die uns Pausanias unterrichtet, müssen als spätere, propagandistische Ausschmückung und Nutzung des Monumentes angesehen werden wie auch die Anfügung von drei ptolemäischen Königen, die sich im Hellenismus gerne als Phylenheroen in die Nähe des Marathonsieges rücken ließen.

Wenn auch das Werk selbst verloren ist, so liefert uns seine Ausformung doch sehr wichtige Informationen bezüglich der Marathonschlacht. Als Weihgeschenk aus der Beute des Sieges sollen die Heroen gegossen worden sein. Damit wurde das Überleben der Demokratie gefeiert, mit der es im Falle eines persischen Sieges vorbei gewesen wäre. Unmittelbar nach der Schlacht kam Marathon also eine lokal auf Attika beschränkte Bedeutung zu. Wenn Herodot Athen im fortschreitenden 5. Jahrhundert v. Chr. als „Vorkämpfer der Griechen“ bezeichnet, greift er damit eine gezielt gestreute Propaganda auf, die so effizient war, dass sie bis heute wirksam ist. Das Phylenmonument des Phidias jedoch liefert uns den chronologisch ersten und entscheidenden Beweis, dass Marathon für Griechenland längst nicht die Bedeutung hatte, die ihm später zugemessen wurde.

Die etwa 3,50 m hohe Statue der Athena Areia stand im Tempel, den die Athener auf dem Schlachtfeld bei Plataiai zu Ehren des Sieges über die Perser errichtet hatten. Gewand und Waffen waren aus Gold, die nackten Hautpartien der Göttin aus Marmor.

Das Standbild ist möglicherweise in römischen Kopien im Typus der Athena Medici überliefert. An der Basis befand sich das Bildnis des Arimnestos, der in den Schlachten von Marathon und Plataiai das Kontingent der Plataier befehligt hatte.

Phidias’ Standbild war Zentrum eines Ensembles, zu dem die Maler Polygnotos und Onasias mit Wandgemälden beigetragen hatten. Auf Polygnots Gemälde war die Tötung der Freier durch Odysseus dargestellt, auf dem von Onasias der erste Feldzug der Sieben gegen Theben.

Das bildliche Programm dieses Ensembles richtete sich vor allem gegen die inneren Feinde Athens. Athena, die Schutzgöttin der Stadt, usurpierte die Insignien des Ares, des Schutzgottes von Theben, das während der Perserkriege auf der Seite des Gegners gestanden hatte und ein Verbündeter Spartas war. Onasias’ Gemälde zeigte, wohin Bruderzwist führt, nämlich zum Untergang der Herrscher Thebens; Polygnotos verdeutlichte die Bestrafung jener, die göttliche Gesetze übertreten.

Phidias begann wahrscheinlich um 460 v. Chr. an der neun Meter hohen Bronzestatue zu arbeiten; etwa um 450 v. Chr. dürfte das Werk vollendet gewesen sein. Es schmückte die Akropolis in Athen. Die 5 × 5 m große Basis für die Statue ist heute noch zu erkennen. Reste der Inschrift, die berichten, dass die Statue aus Beutegeldern finanziert wurde, sind erhalten.

Die Athena Promachos ist sehr schlecht überliefert. Auf Münzbildern ist zu erkennen, dass sie den rechten Arm ausgestreckt hielt, auf dem Nike, die Siegesgöttin, stand. Gegen das linke Bein der Göttin lehnte ein Schild, gegen ihre linke Schulter die Lanze. Auf dem Schild war der Kampf der Griechen gegen die Kentauren dargestellt. Gearbeitet hatten ihn der Toreut Mys und der Maler Parrhasios. Nach den Perserkriegen verstand man den Kampf gegen die Kentauren als Metapher für den Kampf gegen die Perser.

In zahlreichen verkleinerten römischen Kopien ist die Athena Parthenos überliefert, die im Parthenon auf der Akropolis von Athen stand. Die Kolossalstatue war 11,5 m groß. Bei der Erschaffung der Athena Parthenos benutzte Phidias Elfenbein, und für die Kleidung, Sandalen und den Helm wurden ca. 1000 kg Gold verbraucht.

Auf der Außenseite des 4,80 m messenden Rundschildes war der Kampf der Athener und Amazonen im Relief dargestellt, auf der Innenseite die Schlacht der Götter und Giganten als Malerei zu sehen. Von der Malerei ist nichts überliefert, das Relief kann aber mit großer Sicherheit rekonstruiert werden:

In Nashville (Tennessee) steht eine maßstabsgetreue Nachbildung des Parthenon mitsamt der 13 m hohen Kolossalstatue der Athena Parthenos.

Die Frage, inwieweit Phidias an der Planung und Ausführung des Skulpturenschmucks am Parthenon beteiligt war, hat in der früheren Phidias-Forschung eine große Rolle gespielt. Plutarch berichtet, Phidias habe aufgrund seiner Freundschaft mit Perikles die Leitung oder Aufsicht über alle Arbeiten auf der Akropolis innegehabt. Einige Forscher haben daraus abgeleitet, dass Phidias das Programm der Bilder entworfen habe, das dann – daran lässt die Forschung keinen Zweifel – von mehreren Meistern ausgeführt worden sei, Phidias in gewissem Sinne also der Schöpfer des Skulpturenschmucks sei.

Heute ist die vorherrschende Meinung, dass Phidias eher eine Vermittlerfunktion zwischen den verschiedenen Bildhauern besaß. Da die Stadt Athen Auftraggeber des Parthenon war, wurden alle damit in Zusammenhang stehenden Fragen (auch die künstlerischen) der Volksversammlung vorgelegt, die auf demokratische Weise darüber befand. Man kann sich vorstellen, dass ein so hervorragender Künstler und einflussreicher Politiker wie Phidias ein von ihm entwickeltes Programm der Volksversammlung zur Entscheidung vorgelegt und sich damit durchgesetzt hat. Phidias war also nicht das große, alles überragende Genie, das nur seinem eigenen Gesetz gehorcht und in unüberwindlichem Gegensatz zu einer es kaum oder gar nicht verstehenden Volksmasse Kunst geschaffen habe, sondern eingebunden in Athens demokratische Strukturen; nicht das eigene Kunstgesetz war ihm oberstes Prinzip, sondern der Wille, Athen und dessen Demokratie zu dienen. Insofern muss man den Skulpturenschmuck am Parthenon als das Gemeinschaftswerk aller Athener ansehen.

Die helmlose, überlebensgroße Statue der Athena Lemnia aus Bronze dürfte um 450 v. Chr. entstanden und auf der Akropolis in Athen aufgestellt worden sein. In der Inschrift wurden die Kleruchen der Insel Lemnos als Stifter genannt. Kleruchen waren Athener, die in die verbündeten Territorien entsandt wurden und dort Land erhielten und Athens Interessen wahrnahmen.

Die Göttin ist nicht behelmt, also nicht bereit zum Kampf. Sie betrachtet den Helm, den sie in der rechten Hand hält, in ruhiger Haltung. Der Speer lehnt ihr jedoch gegen die linke Schulter; er verdeutlicht ihre Kampffähigkeit.

Wohl wegen ihrer wenig kampfbegierigen Erscheinung galt die Statue in der Antike als Phidias’ schönste. Mit großer Wahrscheinlichkeit haben sich Kopien erhalten. Eine Rekonstruktion, die auf Adolf Furtwängler zurückgeht, ist in Dresden zu sehen.

Die 12 m hohe Statue im Zeustempel von Olympia ruhte auf einem inneren Gerüst aus Eisen, Gips und Holz, war außen mit Goldblech, Elfenbein und Ebenholz verkleidet und wurde in der sogenannten Chryselephantin-Technik errichtet. Dabei werden vor allem bei Götterbildern Gesicht, Hände und Füße der hölzernen oder marmornen Figuren mit Goldblech und Elfenbein verkleidet und mit Edelsteinen und gegossenem farbigem Glas verziert.

Bei Ausgrabungsarbeiten in Olympia in der Nähe des Zeustempels fand man die Überreste der Werkstatt des Phidias und darin Materialreste, Werkzeug etc., außerdem einen Keramikbecher, in dessen Boden die Worte zu lesen waren: ( "Pheidiou eimi" „des Pheidias [Eigentum] bin ich“).

Antike Schriftsteller berichten über ein Amazonenmonument im Artemistempel in Ephesos. Phidias, Polyklet, Kresilas, Kydon und Phradmon fertigten je eine Amazonenstatue. Nach dem Stil der Bronzestatuen zu urteilen, dürfte das Monument um 430 v. Chr. errichtet worden sein. Die Statuen des Polyklet, des Phidias und des Kresilas sind in Kopien fassbar, die erhaltenen Statuentypen „Sosikles“, „Mattei“ und „Sciarra“ gehen auf den Wettbewerb zurück. Umstritten ist bis heute die Zuweisung der Amazonentypen an einzelne Künstler.

Die Amazonen waren der Legende nach aus Osten kommend in Attika eingefallen und von den Griechen besiegt worden. Die Überlebenden hatten in Ephesos Zuflucht gefunden. Dort waren aus den „männermordenden“ Kriegerinnen Frauen geworden, die Ehen mit Griechen eingingen und hinfort ein häusliches Leben führten. Die Amazonen wurden von der damaligen Öffentlichkeit als Sinnbild der besiegten Eindringlinge verstanden. Sie standen für das Schicksal derer, die sich gegen Athen zu stellen wagten.

Pausanias berichtet von einer Aphrodite Urania aus Gold und Elfenbein, die im Aphroditetempel in Elis aufgestellt war. Der linke Fuß der Statue ruhte auf einer kleinen Schildkröte. Ein römischer Marmortorso im Pergamonmuseum in Berlin (Inventarnummer Sk 1459) überliefert vielleicht die Statue. Stilistisch kann man das Werk den 430er Jahren zuordnen.

Pausanias berichtet von einer Apollonstatue in Athen, die Phidias zugeschrieben werde (vergleiche 1, 24, 8). Die Statue hielt einen Bogen in der einen, eine Heuschrecke in der anderen Hand. Damit begründet wird der so genannte Kasseler Apollon, die besterhaltene Kopie in einer ganzen Reihe von Marmorkopien einer Bronzeskulptur, als Apollon Parnopios identifiziert und Phidias zugeordnet. Ein Bogen und Pfeil ist in Resten in einer Hand der Kassler Skulptur nachgewiesen.



</doc>
<doc id="4133" url="https://de.wikipedia.org/wiki?curid=4133" title="Persien">
Persien

Persien steht für:

Siehe auch:


</doc>
<doc id="4134" url="https://de.wikipedia.org/wiki?curid=4134" title="Procynosuchus">
Procynosuchus

"Procynosuchus" war etwa 60 Zentimeter lang und wies im Gegensatz zu anderen Therapsiden Anpassungen an ein Leben im Wasser auf, insbesondere einen außergewöhnlich beweglichen Hinterkörper mit seitlich abgeplattetem Schwanz, der wahrscheinlich ein Schwimmen ähnlich dem von Krokodilen oder Ottern erlaubte. Es wird daher angenommen, dass "Procynosuchus" ähnlich wie diese Tiere semi-aquatisch lebte und sich von Fischen oder anderen Wassertieren ernährte.



</doc>
<doc id="4136" url="https://de.wikipedia.org/wiki?curid=4136" title="PLZ">
PLZ

PLZ steht für:



</doc>
<doc id="4137" url="https://de.wikipedia.org/wiki?curid=4137" title="Postleitzahl">
Postleitzahl

Eine Postleitzahl (Abk. "PLZ") ist eine Ziffern- oder Buchstaben-/Ziffern-Kombination innerhalb von Postadressen auf Briefen, Paketen oder Päckchen, die den Zustellort eingrenzt.

Postleitzahlen sollten nicht als Zahlen angesehen werden, da mit ihnen nicht gerechnet wird, sondern als Zeichenketten, weil sie nicht unbedingt nur aus Ziffern bestehen. Bei folgenden Ländern können die Postleitzahlen auch Buchstaben und/oder Sonderzeichen enthalten: Argentinien, Bermuda, Brunei, Kanada, Jamaika, Malta, Niederlande, Vereinigtes Königreich, Venezuela. Außerdem gibt es Postleitzahlen (z. B. Deutschland) mit führenden Nullen, die dann verlorengehen würden. Im englischsprachigen Umfeld werden die Postleitzahlen korrekter als "Code" bezeichnet. Da Postleitzahlen bis zu zehn Stellen haben können (z. B. Vereinigte Staaten von Amerika: fünf Ziffern, Bindestrich, vier Ziffern), wird für die Speicherung von internationalen Adressen in EDV-Systemen eine Länge von zehn Zeichen für die Postleitzahl empfohlen.

Obwohl Postleitzahlen originär als post-interne Zustellsystematik fungieren, haben sie in den allermeisten Ländern der Erde einen mindestens quasi-offiziellen Charakter. Denn nicht nur konkurrierende Zustellunternehmen bedienen sich desselben Systems, auch Unternehmen und Organisationen planen ihre räumlichen Aktivitäten mit Hilfe der Postleitzahl. Auf dieser Basis werden in der Privatwirtschaft beispielsweise Lieferzonen, Geschäftsstellenbereiche oder Außendienstgebiete abgegrenzt. Auch dienen sie manchmal der Marktforschung, wie z. B. zur Erhebung des Kunden-Einzugsgebietes eines Ladengeschäfts (z. B. Baumarkt) an der Kasse mit der Frage nach der Postleitzahl des Kundenwohnortes. 

Der zentrale Vorteil besteht dabei in der Möglichkeit, organisationsrelevante Informationen unterschiedlichster Art räumlich leicht zuzuordnen. Denn nicht nur Unternehmensdaten, sondern auch externe Steuerungskennziffern (z. B. Einwohnerdaten) sind einer Postleitzahl normalerweise leicht und eindeutig zuzuordnen. Die Postleitzahl hat sich aufgrund einer ganzen Reihe von weiteren Eigenschaften als elementare Analyse- und Planungseinheit bewährt:


Des Weiteren dienen Postleitzahlen der örtlichen Zuordnung bei der Onlinesuche in Branchenverzeichnissen, bei Online-Tarifvergleichen von Strom- und Gaspreisen bei Preisvergleichsportalen und Anderen. 

Bei Kfz-Navigationssystemen verkürzt und erleichtert die Verwendung der Postleitzahl statt der namentlichen Ortseingabe die Handhabung meist beträchtlich.

Nachdem die Ukrainische SSR in den 1930er-Jahren für einige Jahre ein landesweites Postleitzahlensystem namens "Index" verwendete, führte das Deutsche Reich als erster Staat weltweit im Jahr 1941 die Postleitzahlen ein, danach folgten die Vereinigten Staaten (1963) sowie die Schweiz (1964) als drittes Land. 2003 hatten nach Angaben des Weltpostvereins 117 Staaten ein Postleitsystem eingeführt.



</doc>
<doc id="4138" url="https://de.wikipedia.org/wiki?curid=4138" title="Pranger">
Pranger

Der Pranger, Schandpfahl oder Kaak war ein Strafwerkzeug in Form einer Säule, eines Holzpfostens oder einer Plattform, an denen ein Verurteilter gefesselt und öffentlich vorgeführt wurde. Zunächst Folter-Werkzeug und Stätte der Prügelstrafe (Stäupen), erlangten Pranger ab dem 13. Jahrhundert weite Verbreitung zur Vollstreckung von Ehrenstrafen. Der Pranger diente den Städten auch als äußeres Zeichen der Gerichtsbarkeit.

Die Strafe bestand vor allem in der öffentlichen Schande, welche der Verurteilte zu erdulden hatte und die vielfach ein „normales“ Weiterleben in der Gemeinschaft unmöglich machte oder sehr erschwerte. Auch war der Bestrafte den Schmähungen der Passanten ausgesetzt, die für ihn nicht ungefährlich waren. Auch das Bewerfen der betroffenen Person mit Gegenständen und das Prügeln (niederdeutsch „kaakstreeken“, Streek = ‚Streich‘ und entsprechend dänisch „kagstryge“) waren üblich. In vielen Städten (z. B. Lübeck) war es jedoch untersagt, mit festen Gegenständen nach der Person im Pranger (hier als Kaak bezeichnet) zu werfen.

Einer der letzten Fälle dürfte im Jahr 1853 in Berlin zu verzeichnen sein: Auf dem Höhepunkt der "Reaktion" in Preußen wurde auf dem Hausvogteiplatz noch eine Frau wegen Meineids an den Pranger gestellt: 

"„… als es uns auffiel, dass sich vor der Hausvogtei eine neugierige Menschenschar unruhig vor etwas herumdrängte. Wir beschleunigten unsere Schritte und erblickten nun eine schon ziemlich bejahrte, korpulente Frau, mit den Händen rücklings an einen Pfahl gebunden, über dem zu lesen war: ‚Wegen Meineid‘. Man schrieb damals 1853. Es war also ein auf der Höhe der Reaktion gemachter Versuch, die mittelalterliche Strafe des Prangers wieder einzuführen. Als wir um zwölf Uhr auf dem Rückwege an derselben Stelle standen, war das uns Jungen natürlich sehr interessierende Schauspiel bereits von der Bildfläche verschwunden. Die Regierung hatte wohl eingesehen, dass sie nach 1848 so etwas den Berlinern nicht mehr bieten durfte.“"

Es gibt verschiedene Typen des Prangers: 

Der Block als wohl verbreitetste Form des Prangers bestand in der Regel aus zwei parallel angeordneten Brettern, die durch ein Scharnier miteinander verbunden und am Ende eines starken Pfahles angebracht waren. In beiden Brettern waren Aussparungen für den Hals und, links und rechts davon, für die Handgelenke. Die geschlossenen Bretter fesselten nun den Straftäter um Hals und Hände. Derart ausgestattet wurde er dann auf öffentlichen Plätzen ausgestellt.

Eine der prominentesten Personen am Pranger war der englische Schriftsteller Daniel Defoe, der 1703 in London für seine Satiren an den Pranger gestellt wurde. Sein Gedicht "Hymn to the Pillory" (engl. Pranger) sprach dem Publikum jedoch derart aus dem Herzen, dass es ihn mit Blumen bewarf, statt mit dem üblichen Fallobst und Steinen, und auf seine Gesundheit trank.

Im übertragenen Sinn bedeutet „An den Pranger stellen“, jemanden quasi-institutionell öffentlich bloßzustellen. Die Nationalsozialisten nutzten den Begriff zum Beispiel in ihrer hetzerischen Radioprogrammzeitschrift Der Deutsche Sender. Die Rubrik, in der sich die Redaktion kritisch mit vergangenen Hörfunksendungen auseinandersetzte, hieß „Funk-Pranger“.

Trotz der modernen Ächtung des Prangers existieren ähnliche Formen der öffentlichen Vorführung nach wie vor: In den Medien werden tatsächliche oder vermeintliche Straftäter (oft mit Bild oder Angabe des Namens) zur Schau gestellt. In den USA werden inzwischen offiziell von Behördenseite Listen von Straftätern (z. B. Vergewaltigern) mit vollem Namen, Anschrift und Foto veröffentlicht. Im Rahmen des sogenannten „Creative Sentencing“ mehren sich vor allem in den Vereinigten Staaten alternative Schuldsprüche, die unter anderem auch das öffentliche Anprangern der Verurteilten vorsehen. Mediale Aufmerksamkeit erhielt zudem ein Fall im US-Bundesstaat Ohio, bei dem eine Frau vom Gericht dazu verurteilt wurde, zweimal eine Stunde lang ein Schild mit der Aufschrift „Nur ein Idiot würde auf dem Gehsteig einen Schulbus überholen“ hochzuhalten. 
Die Rechtsgrundlage von „Online-Prangern“, wie sie u. a. von Seiten des Bayerischen Landesamtes für Gesundheit und Lebensmittelsicherheit und des Landesamtes für Natur, Umwelt und Verbraucherschutz Nordrhein-Westfalen betrieben wurden, um Verstöße gegen das Lebensmittelrecht zu veröffentlichen, ist umstritten.

Des Weiteren werden Pranger und artverwandte Konstrukte im Bereich des BDSM verwendet.





</doc>
<doc id="4142" url="https://de.wikipedia.org/wiki?curid=4142" title="Politisches Spektrum">
Politisches Spektrum

Ein Politisches Spektrum dient zur Klassifikation der politischen Ideologie einer Person oder einer politischen Partei. Traditionell wird das politische Spektrum mit einer eindimensionalen geometrischen Achse beschrieben, wobei für die beiden Hälften der Achse die Attributnamen "links" und "rechts" verwendet werden. Zur genaueren Klassifikation politischer Ideologien werden heute auch unterschiedliche mehrdimensionale Klassifikationssysteme gebraucht.

Das Aufkommen der Unterscheidung „links“ – „rechts“ im Sinne politischer Richtungsbegriffe wird auf den Ursprung der Französischen Nationalversammlung in der verfassunggebenden Nationalversammlung von 1789 zurückgeführt. Dadurch blieb die Sitzordnung nicht länger das Spiegelbild festgefügter gesellschaftlicher Hierarchien wie in der Versammlung der feudalen Generalstände, sondern brachte bald die Dynamik politischer-ideologischer Auseinandersetzungen zum Ausdruck. Es entstand eine Auffächerung der politischen Orientierungen in der Nationalversammlung in ein Meinungsspektrum zwischen zwei Extremen: Die linke Seite „le côté gauche“ kennzeichnete eine revolutionäre, republikanische Stoßrichtung, während „le côté droit“ mehr zurückhaltende, der Monarchie freundlich gesinnte Vorstellungen vertrat. Bald wurden die räumlichen Adjektive „links“ und „rechts“ substantiviert und man sprach nun einfach von „la gauche“ und „la droite“. Innerhalb dieser Lager bildeten sich wiederum sehr rasch Flügelgruppen: „l’extrémité gauche“ und „l’extrémité droite“. Die mit der Verfassung von 1791 eingerichtete gesetzgebende Versammlung setzte sich dann bereits aus mehreren institutionalisierteren Gruppen zusammen, die allerdings nicht wie heutige Fraktionen parlamentarischer Parteien aufzufassen sind, sondern die Organisation der politischen Landschaft der französischen Revolution in Klubs abbildeten. Auch schwankte die Zahl der sympathisierenden Abgeordneten eines Klubs stark und eine knappe Hälfte der 745 Abgeordneten ordnete sich keinem der Klubs zu. Das Spektrum bewegte sich zwischen dem rechten, monarchistischen Klub der Feuillants und den linken Girondisten und Montagnards, zu denen sich vor allem der Klub der Jakobiner und Cordeliers zählten.

Die sich allmählich ausbildenden Sprachkonventionen konnten sich jedoch aufgrund der turbulenten Entwicklung der Revolution nicht fest verwurzeln. Die Machtübernahme der Jakobiner hatte eine rigorose Beschneidung des als legitim geltenden politischen Spektrums zur Folge. Zu Beginn der Restaurationsphase wirkte die Erlahmung noch fort. Nach den Wirren der ersten hundert Tage erneuerte sich das politische Leben im Jahre 1814 rasch. Erst jetzt konnte sich die bereits im ersten Jahr der Großen Revolution entfaltete, an der parlamentarischen Sitzordnung anknüpfende Geographie revitalisieren. Dies geschah aber in etwas veränderten Formen: Zwischen die Lager der „Rechten“ und der „Linken“ trat eine auf Ausgleich setzende, gemäßigt-monarchisch orientierte Mitte („centre“). Man sprach nach wie vor von den „extrémités“, nun aber auch von „extrême gauche“ und „extrême droite“. Bereits vor 1820 gehörte das Kontinuum extrême droite – droite modérée – centre droite – centre gauche – gauche modérée – extrême gauche (Ultraroyalisten – gemäßigte Konservative – Liberale – Radikale/Demokraten – Sozialisten) zum festen politischen Sprachgebrauch.

Von Frankreich aus breitete sich die Links-rechts-Unterscheidung in ganz Europa aus. In Deutschland konstituierte sich das Paulskirchenparlament von 1848 nach ihrem Muster. Hier saßen die republikanischen Abgeordneten, die einen sofortigen Sturz der damaligen Monarchie forderten, links und die Befürworter einer konstitutionellen Monarchie rechts.

Im klassischen, eindimensionalen Modell kann der Gegensatz zwischen "links" und "rechts" stellvertretend für verschiedene, nachfolgend beschriebenen Gegensätze stehen.

Ausgehend vom Gleichheitspostulat (Egalité) der französischen Revolution sind egalitäre politische Ansätze zentral für das Selbstverständnis der „Linken“. Diese richtete sich gegen Benachteiligungen bestimmter Bevölkerungsgruppen. Dies betraf zunächst die materiell schlechter gestellten Schichten (Arbeiterklasse), wurde später aber auch auf religiöse oder ethnische Minderheiten, Frauen, ältere Menschen, Behinderte, Homosexuelle und andere Bevölkerungsgruppen angewandt. Der Kampf für politische und gesellschaftliche Gleichberechtigung galt den Linken als Teil eines fortschrittlichen Strebens nicht nur nach Gleichheit, sondern auch nach Freiheit. Daher ist der Begriff der Emanzipation als Bezeichnung für die Befreiung und Selbstbestimmung benachteiligter Gruppen für das Selbstverständnis linker Gruppen und Organisationen ein wichtiger Bezugspunkt.

Die „Rechte“ rechtfertigt die Notwendigkeit einer mehr oder weniger stark ausgeprägten Ungleichheit. Entweder werden die Gründe dafür in der Natur des Menschen (Begabung, Befähigung) gesehen, oder die Ungleichheit wird auf gesellschaftliche Nützlichkeitserwägungen (Leistungsanreiz) zurückgeführt. In diesem Zusammenhang wird die Herausbildung von Eliten befürwortet, aus denen sich das Führungspersonal gesellschaftlich bedeutsamer (politischer, kultureller, wissenschaftlicher und wirtschaftlicher) Einrichtungen rekrutiert. Dagegen gelten linke/egalitäre Konzepte als „Gleichmacherei“ und werden als Eingriffe in individuelle Freiheitsrechte und Entfaltungsmöglichkeiten oder in die hergebrachte Gesellschaftsordnung abgelehnt.

Im demokratischen Rechtsstaat steht nach erfolgter politischer Gleichberechtigung die Verteilung gesellschaftlichen Reichtums im Zentrum der Auseinandersetzung über egalitäre bzw. antiegalitäre Ansätze. Differenzierungen beim Verdienst (Primärverteilung) werden mit unterschiedlicher „Begabung“ und „Leistung“ des Individuums begründet. Die Frage nach einer „angemessenen“ einkommensabhängigen Steuerbelastung (Sekundärverteilung) ist ein bedeutenderer Streitpunkt in der politischen Auseinandersetzung, da die Besteuerung im unmittelbaren Zugriff der Gesetzgebung liegt.

Willkürliche Ungleichbehandlung (Diskriminierung) aufgrund von Sprache, Geschlecht, „Rasse“, Herkunft, Religion, politischer Anschauungen oder Behinderungen sind in demokratischen Rechtsstaaten geächtet. Umstritten ist jedoch, ob und in welchem Umfang der Staat Maßnahmen zum Ausgleich von Benachteiligungen ergreifen soll und inwiefern der Staat Diskriminierung im gesellschaftlichen Bereich entgegentreten soll. Dabei wird zwischen Gleichstellung und Gleichbehandlung unterschieden. So werden von Teilen der heutigen Linken zur Durchsetzung gesellschaftlicher Gleichstellung Maßnahmen gerechtfertigt, die als Ungleichbehandlung im Sinne einer Besserstellung gesellschaftlich benachteiligter Gruppen konzipiert sind („umgekehrte Diskriminierung“).

In der Anfangszeit der westlichen Demokratien, insbesondere im 19. Jahrhundert, bemühten sich die Linken vor allem um die Verbesserung der Lebensbedingungen der unteren Schichten, insbesondere der Arbeiter, um die Durchsetzung der Menschenrechte und damit um eine kontinuierliche Erneuerung der Gesellschaft. Die Linke propagierte dies als gesellschaftlichen Fortschritt (Progressivität). Die Rechten traten hingegen für die Wahrung des Status quo in Bezug auf politische und ökonomische Verhältnisse ein und verwiesen auf „hergebrachte“ gesellschaftliche Normen, wodurch sie auch die Bezeichnung „konservativ“ („bewahrend“) erwarben.

Mehrere Entwicklungen erschweren heute die Einteilung nach den Begriffen konservativ/progressiv: In den westlichen Demokratien nach 1945 haben auch eher rechts stehende Parteien eigenständige programmatische Fortschrittskonzepte entwickelt und eine eigene Politik der technischen wie auch gesellschaftlichen Modernisierung vertreten. Unterdessen ist es innerhalb und zwischen Organisationen mit linkem Selbstverständnis äußerst umstritten, welche Auffassungen und Maßnahmen als „progressiv“ anzusehen sind. Zudem entwickelte sich die Ideologiefigur der „Verteidigung fortschrittlicher Errungenschaften“, die als eine linke Variante konservativer Denkansätze angesehen werden kann.

Der egalitären Grundidee entsprechend verfolgte die Linke lange Zeit einen internationalistischen Ansatz, begriff sich als weltweite Bewegung und organisierte sich international. Nach 1945 begriffen allerdings viele linke Gruppierungen ihre Aufgabe als „nationalen Befreiungskampf“ und stützten sich dabei auf anti-imperialistische Ideologien. Zur Befriedigung patriotischer Emotionen in der Bevölkerung, zur Durchsetzung territorialer Machtansprüche oder als Ausdruck eines anti-imperialistischen Weltbildes wurden auch von Regierungen mit linkem Selbstverständnis nationalistische Ansätze vertreten. Im Zusammenhang einer globalisierungskritischen Vorstellungswelt wird heute von Teilen der „Linken“ die Souveränität der Nationalstaaten als Voraussetzung für die Absicherungen sozialer Errungenschaften angesehen und gegen eine Internationalität des Kapitalismus gedanklich in Stellung gebracht.

Bis in die Mitte des 20. Jahrhunderts verfolgte das rechte Lager eine nationalistische Politik und vertritt eine entsprechende Ideologie noch heute. Zugleich versteht sich das „bürgerliche Lager“ in Westeuropa – inklusive der Liberalen – aber als treibende Kraft der wirtschaftlichen Globalisierung und verweist auf seinen Beitrag zur europäischen Einigung.

Während die oben genannten Gegensätze zumindest ursprünglich auf das Links-rechts-Spektrum abgebildet werden konnten, ist dies bei weiteren Gegensätzen nicht oder nur im Einzelfall möglich. Dies ist auch die Basis für alternative Modelle.

Ein typisches Beispiel hierfür ist der Gegensatz "Zentralistisch – Separatistisch". Bei einigen Staaten mit starken Autonomie-Bewegungen, z. B. Spanien, gibt es sowohl im linken als auch rechten Teil des politischen Spektrums zentralistische und separatistische Parteien.

Heutige demoskopische Untersuchungen zeigen, dass sich die Wähler der einzelnen parlamentarisch vertretenen Parteien in ihrem Selbstverständnis jeweils über weite Bereiche des politischen Spektrums verteilen. So sehen sich in einer 2007 von Emnid durchgeführten Umfrage bei den Wählern von Bündnis 90/Die Grünen 76 Prozent als „links“, bei denen der SPD 39 Prozent, bei denen der CDU 25 Prozent und bei denen der FDP 23 Prozent. Insgesamt sagten 34 Prozent der Bundesbürger, sie seien im politischen Spektrum „links“ zu verorten, 52 Prozent ordnen sich der „politischen Mitte“ zu und 11 Prozent der politischen Rechten.

Das „konservativ-bürgerliche Lager“ betont in der Selbstdarstellung meist den konservativen und seltener den elitären Aspekt der eigenen Politik. Gerade aus der Opposition heraus wird häufig mit egalitären Ideen geworben, zum Teil auch zur Abgrenzung zu liberalen Positionen.

Der Begriff "rechts" für die eigene Position wird von den Konservativen vermieden, der Begriff "links" – wenn überhaupt – meist nur abwertend für politische Gegner benutzt. Ebenso wie im sozialdemokratischen und liberalen Lager wird von einigen konservativen Volksparteien zunehmend der Begriff „"Politische Mitte"“ proklamiert.

Viele sozialdemokratische Parteien distanzieren sich zunehmend von der Klassifizierung als „"linke" Partei“, um eine breitere Akzeptanz zu erreichen.

Im Godesberger Programm der deutschen SPD von 1959 wurde der Begriff "links" nicht explizit verwendet, im Berliner Programm heißt es lediglich rückblickend: „Die Sozialdemokratische Partei stellte sich in Godesberg als das dar, was sie seit langem war: die "linke" Volkspartei.“ Im Bundestagswahlkampf 1998 warb die SPD mit dem Schlagwort der „Neuen "Mitte"“. Im Oktober 2007 verabschiedeten Hamburger Programm definiert sie sich als „"linke" Volkspartei“. Im vorangegangenen "Bremer Entwurf" vom Januar 2007 wurde die SPD noch zusätzlich als „Partei der solidarischen "Mitte"“ definiert.

Der Liberalismus lässt sich anhand dieser Sichtweise kaum einer bestimmten politischen Orientierung im Rechts-links-Schema zuordnen, weil er einerseits sehr stark die rechtliche Gleichstellung propagiert, leistungsbedingte soziale Unterschiede jedoch als Anreiz für persönliches Engagement befürwortet. Eine Einordnung ist mittels Nolan-Diagramm möglich. Oftmals wird von den Liberalen dem Gegensatz elitär-egalitär der Gegensatz liberal-regulativ entgegengesetzt. Liberale streben sowohl in Bereichen des persönlichen als auch im Bereich des wirtschaftlichen Lebens nach der größtmöglichen Selbstbestimmung und Eigenverantwortung des Individuums. Sozialliberale wollen gesellschaftlich bedingte Chancenungleichheiten kompensatorisch korrigieren. Sie wollen die soziale Frage durch Qualifikation, eine staatlich geförderte Bildungspolitik und Soziale Marktwirtschaft beantworten.

In Deutschland und anderen europäischen Staaten wird der parlamentarische Liberalismus aufgrund seiner Wirtschaftsnähe („Leistungsgerechtigkeit“) teilweise als politisch „rechts“ oder „bürgerlich“ eingestuft.

In den Vereinigten Staaten wird „liberalism“ aufgrund der Betonung auf gesellschaftliche Gleichstellung und Individualrechte eher als politisch „links“ angesehen (vergleichbar mit der europäischen Sozialdemokratie), während Liberale nach europäischem Verständnis eher als „fiscal conservative“ oder „libertarian“ (vgl. Libertarian Party) bezeichnet werden.

Viele Europäische Sozialisten definieren sich mittlerweile direkt über das Attribut "links". Dies kommt am deutlichsten darin zum Ausdruck, dass sich viele Parteien direkt als Linkspartei bezeichnen.

In Deutschland gab sich 2005 die Partei des Demokratischen Sozialismus den neuen Namen "Die Linkspartei.PDS", durch Fusion mit der WASG entstand daraus 2007 die Partei Die Linke.

In Österreich wurde 2000 von Trotzkisten die Sozialistische Linkspartei gegründet, die neben der älteren, größeren und bei Wahlen erfolgreicheren KPÖ (Kommunistische Partei Österreichs) als weitere Partei links der Sozialdemokratie agiert. Im Zuge der Vorbereitungen zur Nationalratswahl 2008 konstituierte sich ein "Linksprojekt", das nach dem Vorbild der deutschen Linkspartei linke sozialdemokratische und gewerkschaftliche sowie weitere links der SPÖ stehende Kräfte vereinigen soll.

Die Grünen sind in den westlichen Ländern aus der Friedens- und Umweltbewegung entstanden und gelten deshalb teilweise als links bzw. links-liberal. Allerdings hatte sich die grüne Partei in Deutschland auch mit rechten Strömungen in den eigenen Reihen (z. B. Herbert Gruhl) auseinanderzusetzen. Sie schlug dann einen durch linke Strömungen (z. B. Ökosozialisten) beeinflussten Kurs ein. Mit den zunehmenden Regierungsbeteiligungen in den 1990er Jahren haben sich die Grünen jedoch in einigen Ländern von radikal-pazifistischen Positionen verabschiedet und in Wirtschaftsfragen neoliberalen Konzepten angenähert, was diese Einstufung in Frage stellt. Zuletzt vollzog sich bei der grünen Partei in Deutschland innerparteilich ein Prozess der Klärung von Positionen in wirtschafts- und sozialpolitischen Fragen, worauf wirtschaftsliberale Kräfte (z. B. Oswald Metzger) die Partei verließen.

Ökologische Positionen sind nicht notwendigerweise mit traditionell „linken“ Positionen verknüpft. So gelten zum Beispiel die Grünen in Lettland eher als konservativ, ebenso die ÖDP in Deutschland. Die Bürgerrechtler des Bündnis 90, welches 1993 mit den gesamtdeutschen Grünen fusionierte, sahen sich zwar eher „links“, grenzten sich aber radikal von der PDS ab. Einige Mitglieder vertraten sogar konservative Positionen, wie zum Beispiel Vera Lengsfeld und Günter Nooke, die sich später in der CDU organisierten. In der Schweiz grenzen sich die Grünliberalen von den Grünen durch eine liberale Wirtschaftspolitik und eine eher restriktive Finanz- und Sozialpolitik ab.

Aufgrund der Vielzahl an Parteien erfolgt im deutschsprachigen Raum an den Enden des politischen Spektrums eine zusätzliche Abstufung mittels der Attribute "radikal" und "extrem", wobei Letzteres als weiter von der Mitte „entfernt“ betrachtet wird.

Hieraus ergibt sich folgende Skala:

Nach Seymour Martin Lipset und Earl Raab bedeutet Extremismus „Antipluralismus“ und die „Schließung des politischen Marktes“. Hiernach ist für Lipset auch ein Extremismus der Mitte denkbar.

"Siehe auch:" Radikalismus, Extremismus, Rechtsextremismus, Linksradikalismus, Linksextremismus

Da das Modell des politischen Spektrums nur bedingt dazu geeignet ist, die Parteienlandschaft umfassend zu beschreiben, werden zusätzliche Attribute für die Einordnung herangezogen. Dabei werden die Parteien und Strömungen allgemein oder in Bezug auf ein bestimmtes Themenfeld Ideologien und politischen Konzepten zugeordnet.


Durch weitere Zusätze wird versucht, zusätzliche Nuancen zu setzen:


Ein Hauptkritikpunkt ist die extreme Vereinfachung der politischen Landschaft durch die Projektion verschiedener programmatischer Unterschiede auf eine einzige Achse. Darüber hinaus wird kritisiert, dass der Begriff "Spektrum" eine Kontinuität suggeriert (wie z. B. bei den Farbschattierungen des Lichtspektrums), obwohl auch ideologisch „benachbarte“ politische Strömungen klare Bruchlinien aufweisen können und die einzelnen politisch-ideologischen Ausrichtungen keineswegs immer bruchlos ineinander übergehen.

Die Verwendung dieser Attribute stellt indirekt eine positive Korrelation zwischen der Radikalität von Ideen (d. h. wie sehr sie vom Status quo abweichen) und der Vehemenz, mit der sie vertreten werden (latente oder offene Gewalt gegen Andersdenkende oder den Staat), her. Obwohl diese Korrelation naturgemäß in gewissem Maße gegeben ist (die Parteien der Mitte haben in der Regel die Unterstützung von Exekutive, Justiz und Medien und bedürfen selbst keiner extremen Maßnahmen), ist sie jedoch keineswegs zwingend. So gibt es moderate Gruppierungen mit radikalen Ideen und aggressive Verfechter allgemein akzeptierter Ansichten. Diesen Umstand versuchen linke und rechte Gruppierungen in jüngster Vergangenheit durch den Begriff "Mitt-Extremismus" (Extremismus der Mitte) zu verdeutlichen.

Ein alternativer, aber ebenfalls stark vereinfachender Ansatz besteht darin, die politische Landschaft nicht als horizontale Gerade, sondern als offenen Kreis („Hufeisenschema“) zu sehen. Durch diese Darstellung soll zum Ausdruck gebracht werden, dass sich die beiden Ränder in manchen Punkten näher sind, als es der Rand zur Mitte ist. Hierbei werden jedoch partielle Übereinstimmungen in den Methoden über grundsätzliche Unterschiede bei den Zielen sowie beim Welt- und Menschenbild gestellt.

Speziell von den Liberalen wird das sogenannte „politische Wertedreieck“ als Modell angewandt. Hier gibt es nicht wie beim linearen Spektrum ein "links" und "rechts", sondern ein Dreieck mit folgenden Werten als Eckpunkte:
Der Vorteil dieses Modells liegt darin, dass man die Parteien innerhalb dieses Dreiecks genauer platzieren kann. Das "Extreme" ist hierbei nicht nur auf die drei Spitzen beschränkt, sondern auch auf den gesamten Rand der drei Dreiecksseiten.

Ein ähnliches Konzept ist der politische Kompass. Er besitzt ebenfalls zwei Achsen, nämlich "links" – "rechts" in Bezug auf die Wirtschaft und die Achse "autoritär" – "libertär", die sich auf das soziale Zusammenleben bezieht. Dabei steht "links" für eine Kontrolle oder gar Steuerung der Wirtschaft durch den Staat oder internationale Instanzen, "rechts" für Wirtschaftsliberalismus. "autoritär" steht für den Autoritarismus, "libertär" für Libertärismus, allerdings auch auf soziale und nicht nur auf ökonomische Fragen bezogen. Der politische Kompass ordnet zum Beispiel den im politischen Spektrum als links eingeordneten Stalin als linken Autoritären (im Bild links oben), den rechts eingeordneten Hitler als Autoritären ohne einen besonderen Hang zu links/rechts (im Bild oben, in der Mitte) ein. Die tatsächlichen Parallelen zwischen Stalin und Hitler sind also nach Ansicht der Ersteller auch im politischen Kompass vorhanden.

Ein weiterer Ansatz, politische Orientierungen zweidimensional über das Maß an ökonomischer und gesellschaftlicher Freiheit darzustellen, ist das Nolan-Diagramm.

Statt der zweidimensionalen Polarisation eines politischen Spektrums bemüht sich die Erforschung von (politischen) Milieus um die Einteilung gesellschaftlicher Gruppen gemäß ähnlicher Werteorientierungen, Alltagseinstellungen und dem sozialen Status. Die bedeutendsten Studien zu (politischen) Milieus erstellt das Sinus-Institut (Sinus-Milieu). An die Stelle der Links-rechts-Achse tritt die Orientierung an traditionellen Werten (Ordnung, Pflichterfüllung), Modernisierung (Individualisierung, Selbstverwirklichung) sowie Neuorientierung (Experimentierfreude, Leben in Paradoxien). Obwohl manche Milieus ihrer Bezeichnung nach mit bestimmten politischen Einstellungen in Verbindung zu bringen sind (z. B. „Konservative“), zielt die Sinus-Milieukartierung nicht vorrangig auf die Abgrenzung von in ihren politischen Haltungen möglichst homogenen Gruppen.

Beim Deutschen Bundestag wird die Sitzordnung vom Ältestenrat auf der Basis von Vorschlägen der Bundestagsverwaltung festgelegt. Der Vor-Ältestenrat besteht aus dem noch amtierenden Bundestagspräsidenten und den parlamentarischen Geschäftsführern der Fraktionen des scheidenden Bundestages und Vertretern aller Fraktionen des neuen Bundestages.

Bei der Sitzordnung orientiert man sich traditionell grob am politischen Spektrum.

Aktuelle Sitzordnung des Deutschen Bundestages:

Die Linke – SPD – Bündnis 90/Die Grünen – CDU/CSU – FDP – AfD

Die FDP wurde 1949 rechts von den Unionsparteien platziert, da sie damals allgemein als rechtsliberal galt. Später wollte keine der beiden Seiten tauschen. Rechts von der FDP saß in den ersten drei Bundestagen die DP. Im ersten Bundestag waren einige Abgeordnete kleinerer Parteien und fraktionslose Abgeordnete noch rechts von der DP platziert. Die zur GB/BHE gewechselten Abgeordneten saßen im ersten Bundestag in den hinteren Reihen, eingeschlossen von Unionsabgeordneten. Im zweiten Bundestag saß der GB/BHE zwischen Union und SPD.

Im ersten Bundestag saß die KPD ganz links, ab dem zweiten Bundestag war dies die SPD. Bis 1983 bestand die SPD darauf, dass links von ihr keine Fraktion sitzen darf. Daher sitzt die Grünen-Fraktion rechts von ihr, obwohl sie in ihrer Anfangszeit als deutlich „linker“ betrachtet wurde. Beim Einzug der damaligen PDS 1990 bestand die SPD nicht weiter auf ihrem äußeren Platz.

In Österreich hat die Sitzordnung des Parlamentes nichts mit der politischen Richtung der Parteien zu tun. Die sozialdemokratische SPÖ sitzt links, die konservative ÖVP rechts, wohingegen die nationalistische FPÖ traditionell den Platz in der Mitte einnimmt, wo auch sonstige im Nationalrat vertretene Parteien platziert werden.

SPÖ – NEOS – PILZ – FPÖ – ÖVP

Bei der Sitzordnung orientiert man sich seit 1995 grob am politischen Spektrum (zuvor orientierte man sich primär an den Sprachgruppen).
Links befindet sich die SP – GPS vorn, CVP hinten – diverse Kleinparteien vorn, FDP.Die Liberalen hinten – und die SVP rechts.



</doc>
<doc id="4143" url="https://de.wikipedia.org/wiki?curid=4143" title="Planung">
Planung

Die Planung beschreibt die menschliche Fähigkeit oder Tätigkeit zur gedanklichen Vorwegnahme von Handlungsschritten, die zur Erreichung eines Zieles notwendig scheinen. Dabei entsteht ein "Plan", gemeinhin als eine zeitlich geordnete Menge von Daten.

Jürgen Wild versteht die Planung als „ ein systematisches zukunftsbezogenes Durchdenken von Zielen, Maßnahmen, Mitteln und Wegen zur zukünftigen Zielerreichung“. Günter Wöhe beschreibt Planung als „die gedankliche Vorwegnahme zukünftigen Handelns durch Abwägen verschiedener Handlungsalternativen und Entscheidungen für den günstigsten Weg “. Planung bedeutet also das Treffen von Entscheidungen, die zukunftsgerichtet sind. Planung ist somit zukunftsbezogen, denn sie soll zukünftige Ereignisse gestalten und künftiges Handeln vorwegnehmen. Sie geht dabei von einem Gestaltungswunsch aus, durch den der Objektbereich festgelegt wird unter Beachtung der mit Hilfe der Planung zu erreichenden Ziele. Eine "flexible Planung" berücksichtigt Handlungsalternativen („Plan B“), falls die ursprünglichen Pläne nicht durchgesetzt werden können und erwartete Ereignisse nicht oder anders eintreten. Dabei sind Szenarioanalysen einzubeziehen. 

Bei der Planung wird berücksichtigt, mit welchen Mitteln das Ziel erreicht werden kann, wie diese Mittel angewendet werden können, um das Ziel überhaupt zu erreichen (Vorgehensmodell), und wie man das Erreichte kontrollieren kann (Steuerung). Als Planungsergebnis erzeugen im Idealfall kurz-, mittel- oder langfristige Pläne Handlungssicherheit.

Ein Plan hat in Bezug auf Management und Organisationen die Bedeutung einer zumindest in schriftliche Form (oder in die einer Zeichnung) gebrachten Vorstellung von den Modalitäten, wie ein erstrebenswertes Ziel erreicht werden kann. Die geistige und handwerkliche Tätigkeit zur Erstellung eines Plans wird Planung genannt. Planung ist die Phase bis zur Genehmigung eines Plans vor Beginn der Realisierung. Der Zweck von Planung besteht darin, über eine realistische Vorgehensweise zu verfügen, wie ein Ziel auf möglichst direktem Weg erreicht werden kann.

Das "abstrakte Planen" ist eine kognitive Fähigkeit, die in der Allgemeinen Psychologie und der Kognitionswissenschaft untersucht wird. Planmäßiges Vorgehen zählt dort zu den Kriterien für Rationalität und Intelligenz. Bei der "konkreten Planung" wird zusätzlich auf externe Informations- und Erfahrungsquellen zurückgegriffen.

In der Neuropsychologie werden das Organisieren und Planen einer Handlung den exekutiven Funktionen zugerechnet. Man versucht dort, diese Fähigkeiten objektiv zu messen, z. B. durch die Turm von London-Aufgabe.

Im Hinblick auf die Planwirtschaft in sozialistischen Staaten galt Planung in Westdeutschland lange Zeit als negativ konnotiert. Im Jahre 2011 schrieb Hans-Werner Frohn rückblickend zum Begriff "Planung":

Neben der Zukunft­sbezogenheit hat Planung vier weitere wesentliche Merkmale:

Die mitunter vielschichtigen Wirkungen der Planung lassen sich auf vier entscheidende Funktionen reduzieren:

Die Gültigkeit einer Planung (Validität) kann nicht als gegeben und selbstverständlich angenommen werden. Eine Planung ist lediglich eine abstrakte Abbildung oder ein Modell der in aufeinanderfolgenden Schritten zu erwarteten Realität, die zudem im weiteren Zeitablauf noch zu gestalten ist. Daher gilt wie für alle Modelle, dass die Planung ein vereinfachendes Abbild der erwarteten künftigen Realität wiedergibt. Die zugrunde liegenden Vereinfachungen bergen die Gefahr, dass solches Modell entscheidende Merkmale nicht enthält und dann die Prüfung der Planung hinsichtlich der
zu Fehleinschätzungen führt. Zudem kann der Planer in Unkenntnis der Prozesse bei der Abbildung in den Plan übersehen, dass es weitere
gibt, die für die Umsetzung bedeutsam sind. Automatische Prüfverfahren können lediglich formale Fehler, wie Schleifen oder unmögliche Terminsetzungen erkennen. Eine weitergehende semantische Prüfung erfordert dagegen die Einbindung von Prozesskenntnissen, um Fehler zu erkennen.

Jede Planung altert mit der Umsetzung. Ein besonderes Problem entsteht aus dem Erkenntnisfortschritt im Zuge der Umsetzung über die finanziellen, sozialen und technischen Einzelheiten. Dabei ist zu erwarten, dass sowohl
mit dem Zeitablauf auf der Grundlage der formulierten Anforderungen (Spezifikationen) verändert eingeschätzt wird. Im Allgemeinen wird eine überarbeitete Planung ein Wachstum in allen diesen Merkmalen aufweisen. Es gehört eine gemeinsame Disziplin dazu, die unvermeidlichen und die wünschenswerten Änderungen zu trennen, um dem Plan weiter folgen zu können. Insgesamt muss die Planung durch fortlaufende Aktualisierung dem im Konsens angenommenen Änderungsbedarf folgen.

Genauso, wie der Planer und die ausführenden Instanzen einen Erkenntnisfortschritt erlangen, wird auch von den Erstellern und von den Empfängern der geplanten Lieferung oder Leistung oder eines entstehenden Werkes eine stets zunehmende Vielzahl von Widrigkeiten erkannt. Um zu vermeiden, dass der Plan schließlich als nicht durchführbar erkannt wird, muss er in jedem Schritt gegen solche Widrigkeiten durchgesetzt (engl. enforcement) werden. Dazu dienen in der Regel Hilfsmittel, die frühe Zeichen besser wahrnehmen lassen. Solche Zeichen werden von den ausführenden Instanzen gegeben, es sind meist
Die Durchsetzung einer Planung wird im Allgemeinen unter Aspekten der Steuerung behandelt. Es ist allerdings zu beachten, dass durch Feedback aus dem Prozess der Umsetzung eher eine Regelung im geschlossenen Kreis erforderlich ist.

Alle Wirtschaftssubjekte (Unternehmen, Privathaushalte, der Staat mit seinen Untergliederungen wie öffentliche Verwaltung oder Staatsunternehmen) befassen sich mit Planungsfragen.

Wichtiger Planungsbereich ist die Unternehmensplanung. Sie gehört seit 1951 Erich Gutenberg zufolge zu den betrieblichen Produktionsfaktoren, und zwar zu den so genannten dispositiven Faktoren. Wichtige Merkmale der Planung sind in der Betriebswirtschaftslehre "Planungsgegenstand", "Planungssubjekt", "Planungsdaten" und "Planungszeitraum". Planungsgegenstand können etwa die künftig bestehenden Arbeitsplätze in einem Unternehmen sein, Planungssubjekt ist der Entscheidungsträger, der die Arbeitplatzplanung zu verantworten hat, Planungsdaten sind insbesondere die künftig erforderliche Personalkapazität. Der Planungshorizont bestimmt die zeitliche Reichweite der Planung und ist ein wesentliches Gestaltungsmerkmal jeder Planung. Je nach der betrieblichen Funktion gibt es Beschaffungsplanung, Produktionsplanung, Finanzierungsplanung, Personalplanung oder Vertriebsplanung (Absatzplanung, Marketingplan).

Das Planverfahren bezeichnet im Rahmen eines Planungsprozesses alle rechtlich normierten Planungs- und Entscheidungsabläufe. Für den Bereich der städtebaulichen Planung oder Stadtplanung werden die Verfahren zur Aufstellung, Änderung oder Aufhebung von Bebauungsplänen und Flächennutzungsplänen in Deutschland im Baugesetzbuch geregelt. In übergeordneten räumlichen Planungen wie der Raumordnung und der Landesplanung erfolgt dies ebenfalls in den einschlägigen Gesetzen der Länder. Auch Fachplanungen wie beispielsweise die Landschaftsplanung oder die überörtliche Verkehrsplanung haben gesetzlich geregelte Verfahrensschritte zur Aufstellung von Planwerken. Die Verfahrensschritte der Planfeststellung, mit denen Großprojekte wie Bundesstraßen oder Flughäfen geplant werden, sind im Verwaltungsverfahrensgesetz festgelegt.

In der Regel enthalten förmliche Planverfahren Beschlüsse politischer Gremien zur Aufstellung und zur Feststellung der jeweiligen Pläne sowie Beteiligungen der Öffentlichkeit und fachlich relevanter Behörden bzw. anderer Träger öffentlicher Belange.

Neben den rechtlich normierten Planverfahren existieren zahlreiche informelle Planungen. Im Bereich der Stadtplanung sind dies beispielsweise der Stadtentwicklungsplan, der Masterplan oder der städtebauliche Rahmenplan.

"Planungsprozess" steht in der Stadtplanung als Oberbegriff für alle formellen und informellen Planungs- und Entscheidungsabläufe. Er vereinigt somit die rechtlich verbindliche Ebene mit ergänzenden, nicht vorgeschriebenen Planungen und Plänen. Er kam erst in den 1990er Jahren auf, als sich das Planungsverständnis zunehmend wandelte. Die Einbeziehung informeller Pläne, wie z. B. Stadtentwicklungspläne und informeller Verfahren, wie z. B. Citymanagement und Charrette-Verfahren, die rechtlich nicht vorgeschrieben und normiert sind, wurde als geeignetes Mittel angesehen, Planungen durchsichtiger und verständlicher zu machen. Die erhöhte Transparenz kommt Behörden und Bürgern gleichermaßen zugute. Eine Ergänzung der formellen Pläne erhöht zudem die Qualität einer Planung, da sie nicht mehr starr immer demselben Muster folgt, sondern Flexibilität erlangt.

Beispiel einer gesetzlich normierten kommunalen Planung ist die Jugendhilfeplanung, in deren Rahmen die Träger der öffentlichen Jugendhilfe den Bestand an Einrichtungen und Diensten festzustellen, den Bedarf unter Berücksichtigung der Wünsche, Bedürfnisse und Interessen der jungen Menschen und der Personensorgeberechtigten für einen mittelfristigen Zeitraum zu ermitteln und die zur Befriedigung des Bedarfs notwendigen Vorhaben rechtzeitig und ausreichend zu planen haben. Dabei soll darauf hingewirkt werden, dass die Jugendhilfeplanung und andere örtliche und überörtliche Planungen aufeinander abgestimmt werden und die Planungen insgesamt den Bedürfnissen und Interessen junger Menschen und ihrer Familien Rechnung tragen ( SGB VIII).

Die Bauleitplanung ist das wichtigste Planungsinstrumentarium zur Lenkung und Ordnung der städtebaulichen Entwicklung. Innerhalb der Bauleitplanung werden alle relevanten Belange im Rahmen einer Abwägung berücksichtigt (§, Baugesetzbuch). Ergänzend zum Städtebaurecht wirkt im planungsrechtlichen Außenbereich das Planungsinstrument der Landschaftsplanung.

Im Bauwesen existiert der Begriff der Bauplanung, in der Architektur auch der Begriff des Entwerfens für Teilbereiche der Planung eines Architekten, deren Gesamtheit in Deutschland in Leistungsphasen nach HOAI gegliedert ist.

Projektplanung ist eine der Hauptaufgaben des Projektmanagements.

Der Begriff der Familienplanung wird meist auf Maßnahmen von Paaren reduziert, die Zahl und den Zeitpunkt der Geburt von Kindern zu planen. 





</doc>
<doc id="4144" url="https://de.wikipedia.org/wiki?curid=4144" title="Punkt">
Punkt

Punkt (lat. "punctum" ‚Einstich‘, Mehrzahl: [die] "Punkte") steht für:

sowie:

Siehe auch:



</doc>
<doc id="4145" url="https://de.wikipedia.org/wiki?curid=4145" title="Pflanzenwuchsform">
Pflanzenwuchsform

Bei Pflanzen ist die Wuchsform die morphologische Ausgestaltung einer Pflanze in Bezug auf ihre Lebensdauer, auf die Dauer ihrer einzelnen Teile, ihre Verzweigung usw. Der Begriff überschneidet sich stark mit Lebensform und wird häufig synonym dazu verwendet. 

Nach einer anderen Definition (von W. Rauh) ist die Wuchsform das Organisationsprinzip, der Bauplan. Die Lebensform ist demnach die im konkreten Fall realisierte Form dessen, was im Lebensraum innerhalb der Bandbreite des Bauplans möglich ist. Beispielsweise werden unter der Zuchtform (Obstgehölze) die Ergebnisse, welche Gärtner für die Wuchsform "Obstbaum" erzielen, zusammengefasst.

In der Literatur wird jedoch zwischen Lebensform und Wuchsform nicht unterschieden, beide Begriffe synonym verwendet, wobei im englischen Sprachraum "growth form" dominiert, im deutschen Lebensform.

Für einen Überblick über die Wuchsformen, siehe Lebensform (Botanik). Für die Wuchsformen von Algen, siehe Organisationsstufe.

Grob sind die Wuchsformen in verholzt und krautig gegliedert:


</doc>
<doc id="4149" url="https://de.wikipedia.org/wiki?curid=4149" title="Peano-Axiome">
Peano-Axiome

Die Peano-Axiome (auch Dedekind-Peano-Axiome oder Peano-Postulate) sind fünf Axiome, welche die natürlichen Zahlen und ihre Eigenschaften charakterisieren. Sie wurden 1889 vom italienischen Mathematiker Giuseppe Peano formuliert und dienen bis heute als Standardformalisierung der Arithmetik für metamathematische Untersuchungen. Während die ursprüngliche Version von Peano in Prädikatenlogik zweiter Stufe formalisiert werden kann, wird heute meist eine schwächere Variante in Prädikatenlogik erster Stufe verwendet, die als Peano-Arithmetik bezeichnet wird. Mit Ausnahme von Vertretern des Ultrafinitismus wird die Peano-Arithmetik in der Mathematik allgemein als korrekte und konsistente Charakterisierung der natürlichen Zahlen anerkannt. Andere Formalisierungen der natürlichen Zahlen, die mit der Peano-Arithmetik verwandt sind, sind die Robinson-Arithmetik und die Primitiv rekursive Arithmetik.

Richard Dedekind bewies bereits 1888 den sogenannten Isomorphiesatz von Dedekind, dass alle Modelle der Peano-Arithmetik mit Induktionsaxiom zweiter Stufe isomorph zum Standardmodell formula_1 sind, d. h. dass die Struktur der natürlichen Zahlen so bis auf Benennung eindeutig charakterisiert wird. Dies gilt dagegen nicht für die erststufige Formalisierung, aus dem Satz von Löwenheim-Skolem folgt die Existenz von paarweise nicht isomorphen Modellen (u. a. Modellen jeder unendlichen Kardinalität), die die Peano-Axiome erfüllen.

Peano betrachtete ursprünglich 1 als kleinste natürliche Zahl. In seiner späteren Version der Axiome, die im Folgenden modern notiert sind, ersetzte er 1 durch 0. Die Axiome haben dann folgende Form:


Diese Axiome lassen sich folgendermaßen verbalisieren, wobei formula_7 als „Nachfolger von formula_8“ gelesen wird: 

Das letzte Axiom heißt "Induktionsaxiom", da auf ihm die Beweismethode der vollständigen Induktion beruht. Es ist äquivalent zur Aussage, dass jede nichtleere Menge natürlicher Zahlen ein kleinstes Element hat. Auch garantiert es, dass Peanos rekursive Definitionen der Addition und Multiplikation auf formula_1 überhaupt wohldefiniert sind: 

Die Eins definierte Peano als Nachfolger der Null:
Aus dieser Definition folgt mit der Additionsdefinition für den Nachfolger formula_21.

Peano setzte als Rahmen eine Klassenlogik voraus. Sein Axiomensystem ist auch in der Mengenlehre interpretierbar oder auch in der Prädikatenlogik zweiter Stufe, da neben Zahlenvariablen im Induktionsaxiom auch die Mengenvariable formula_11 vorkommt.

Die ursprüngliche Formalisierung enthält im Induktionsaxiom eine Quantifikation über Mengen von Objekten (siehe oben). Da aber in der Prädikatenlogik erster Stufe nicht über Mengen von Objekten quantifiziert werden kann, wird für die Formalisierung in der Logik der ersten Stufe das Induktionsaxiom durch ein schwächeres "Axiomenschema" in der Prädikatenlogik erster Stufe ersetzt. Dieses hat die folgende Form:


Für jede Formel formula_24 muss das entsprechende Induktionsaxiom hinzugefügt werden; die erststufige Version der Peano-Arithmetik enthält also eine unendliche Menge von Axiomen.



</doc>
<doc id="4150" url="https://de.wikipedia.org/wiki?curid=4150" title="Phobos">
Phobos

Phobos (von griech.: φόβος "phobos" bzw. φοβία "phobia" = „Furcht“, „Angst“) kann folgende Bedeutungen haben:



</doc>
<doc id="4151" url="https://de.wikipedia.org/wiki?curid=4151" title="Peroxide">
Peroxide

Peroxide, veraltet auch Peroxyde geschrieben, sind eine chemische Stoffgruppe, die das Peroxidanion O beziehungsweise eine Peroxygruppe –O–O– enthalten. Im Gegensatz zum Oxidion, dem häufigsten Anion des Sauerstoffs, besitzen die Sauerstoffatome im Peroxidion die Oxidationsstufe −1.

Die Gruppe peroxidischer Verbindungen kann zunächst grob in anorganische und organische Peroxide unterteilt werden, wobei die anorganischen oftmals salzartigen Charakter besitzen, die organischen hingegen kovalent gebundene Peroxygruppen tragen. Die Sauerstoff-Sauerstoff-Bindung von Peroxiden ist labil und neigt zur homolytischen Spaltung unter Bildung reaktiver Radikale. Aus diesem Grund sind Peroxide in der Natur nur in geringen Mengen anzutreffen, unter anderem im Wasser und der Erdatmosphäre sowie in geringen Mengen in pflanzlichen, tierischen und menschlichen Organismen.

Industrielle Bedeutung erlangen Peroxide durch ihre Bleichwirkung gegenüber organischen Substanzen. So werden sie Waschmitteln beigemengt, unter anderem unter der Werbebezeichnung "Aktivsauerstoff", oder in Haarfärbeprodukten eingesetzt. Zu den weiteren Anwendungsbereichen gehören die chemische Industrie, in der Peroxide zur Synthese eingesetzt werden oder als Zwischenprodukte auftreten, sowie die Medizin. Wasserstoffperoxid ist mit einem jährlichen Produktionsvolumen von über 2.000.000 Tonnen die wirtschaftlich bedeutendste peroxidische Verbindung.

Oft sind die Peroxide instabil, somit nicht lagerfähig und bisweilen gefährliche Substanzen. Deshalb werden sie oft vor Ort ("in situ") hergestellt und sogleich umgesetzt.

Als wahrscheinlich erste synthetisch hergestellte Peroxoverbindung erhielt Alexander v. Humboldt im Jahre 1799 bei "Versuchen zur Zerlegung der Luft" Bariumperoxid. Erst 19 Jahre später erkannte Thénard, dass dieses Salz zur Herstellung einer bisher unbekannten Verbindung genutzt werden kann, die er als "oxidiertes Wasser" bezeichnete - heute als Wasserstoffperoxid bekannt. Im Jahre 1811 stellten Thénard und Gay-Lussac erstmals Natriumperoxid her. In anhaltenden Forschungsbemühungen der folgenden Jahrzehnte wurden Wasserstoffperoxid und seine Salze untersucht. Auf der Suche nach einer Verwendung wurde schon früh die bleichende Wirkung der Verbindung auf natürliche Farbstoffe erkannt. Ein industrieller Einsatz scheiterte jedoch zunächst daran, dass lediglich schwach konzentrierte und verunreinigte Bariumperoxidlösungen hergestellt werden konnten. Die erste industrielle Anlage zur Synthese von Wasserstoffperoxid entstand 1873 in Berlin. Erst nach der Entdeckung der Synthese von Wasserstoffperoxid durch Elektrolyse von Schwefelsäure konnten verbesserte Verfahren auf elektrochemischer Basis entwickelt werden. Die erste Fabrik, die nach diesem Verfahren arbeitete, wurde 1908 in Weißenstein in Kärnten errichtet. Das heute noch verwendete Anthrachinon-Verfahren wurde während der 1930er Jahre bei den I.G. Farben in Ludwigshafen entwickelt. Durch modernere Syntheseverfahren und die Erweiterung des Anwendungsgebiets stieg die Jahresproduktion von Wasserstoffperoxid stark von 35.000 t im Jahr 1950 über 100.000 t 1960 zu 300.000 t 1970 an. 1998 bestand eine weltweite Produktionskapazität von 2.700.000 t pro Jahr.

Peroxide sind meist sehr reaktiv, weshalb es nur geringe natürliche Vorkommen gibt. Hierzu gehören neben Wasserstoffperoxid wenige pflanzliche Naturstoffe wie beispielsweise ein peroxidisches Derivat des Prostaglandins sowie Ascaridol.
Wasserstoffperoxid tritt natürlich in Oberflächenwasser, Grundwasser sowie in der Erdatmosphäre auf. Die Bildung erfolgt hier durch die Einwirkung von Licht oder natürlichen katalytisch wirksamen Substanzen aus Wasser. In Meerwasser sind 0,5–14 μg/l, in Süßwasser 1–30 μg/l, in Luft 0,1–1 ppb enthalten.

Es sind zwei peroxidhaltige Minerale bekannt, Studtit und Metastudtit. Es handelt sich um Uranylperoxide mit unterschiedlichen Mengen Kristallwasser in der Struktur. Das instabile Peroxid entsteht dabei bei der Radiolyse von Wasser durch die Alphastrahlung des Urans. Außer in natürlichen Uranlagerstätten bilden sich diese Verbindungen auch auf der Oberfläche von radioaktiven Abfällen und könnten daher in ihrer Stabilität für die Endlagerung von Uranabfällen von Bedeutung sein.

Auch in menschlichen und tierischen Organismen wird Wasserstoffperoxid gebildet. Es tritt als kurzlebiges Produkt in biochemischen Prozessen auf und gehört zu den Zellgiften. Die Toxizität beruht auf Veränderungen von Proteinen, Membranlipiden und DNA durch oxidative Reaktionen der Peroxidionen. Das Enzym Superoxiddismutase, das der Beseitigung gebildeter Superoxidionen dient, produziert durch Disproportionierung Wasserstoffperoxid "in vivo". Diese wird dann durch das Enzym Katalase rasch zu Sauerstoff und Wasser zersetzt.

Peroxisomen sind Zellorganellen in eukaryotischen Zellen, die dem oxidativen Fettsäureabbau dienen. Während des Abbaus wird in ihnen Wasserstoffperoxid gemäß folgender Gleichung produziert:

Entstandenes Wasserstoffperoxid wird anschließend wiederum durch Katalase umgesetzt.

Ein weiterer Entstehungsort von Wasserstoffperoxid ist der Abbau von Adenosinmonophosphat. In einer Abfolge biochemischer Reaktionen wird dieses zunächst zu Hypoxanthin umgesetzt. Dieses wird dann oxidativ zunächst zu Xanthin und anschließend zu Harnsäure katabolisiert. Diese Reaktion wird durch das Enzym Xanthinoxidase katalysiert und es entsteht ein Äquivalent Wasserstoffperoxid je Formelumsatz.

Auch der Abbau von Guanosinmonophosphat läuft über Xanthin als Zwischenprodukt, welches dann auf gleichem Wege unter Bildung von Wasserstoffperoxid zu Harnsäure umgesetzt wird.

In Eizellen von Seeigeln wird kurz nach der Befruchtung durch ein Spermium kurzzeitig Wasserstoffperoxid produziert. Dieses wird zu OH-Radikalen dissoziiert und dient als Initiator einer radikalischen Polymerisation, welche die Eizelle mit einer dichtenden polymeren Schutzschicht umgibt.

Die Bombardierkäfer besitzen am Hinterleib einen Explosionsapparat, mit welchem sie zur eigenen Verteidigung ätzende und übelriechende Blasen auf Feinde zu schießen vermögen. Zum Hinausblasen einer Blase werden kurz zuvor Wasserstoffperoxid und Hydrochinon miteinander vermischt, die heftig miteinander reagieren und zum Abschuss des Chemikaliengemischs führen.

Des Weiteren stellt Wasserstoffperoxid ein Signalmolekül bei der pflanzlichen Abwehr von Pathogenen dar.

In Leuchtkäfern werden durch Oxidation von Luciferinen 1,2-Dioxetane gebildet. Diese Reaktion wird von Luciferasen katalysiert. Das entstandene Dioxetan ist labil und zerfällt spontan zu Kohlenstoffdioxid und einem angeregten Keton. Dieses relaxiert unter Abgabe eines Photons in den Grundzustand, wodurch das Leuchten dieser Spezies hervorgerufen wird.

Das Peroxidion ist aus zwei Sauerstoffatomen aufgebaut, welche über eine Einfachbindung miteinander verknüpft sind. Dies steht im Einklang mit dem MO-Diagramm des Peroxidions, das auf Grund der doppelt besetzten "antibindenden" π*-Molekülorbitale eine Bindungsordnung von 1 vorhersagt. Die Bindungslänge beträgt 149 pm und ist im Vergleich zum Sauerstoffmolekül (Triplett-Sauerstoff (O): 121 pm) durch die zunehmende Besetzung antibindender Orbitale verlängert. Dies drückt sich auch in der kleineren Kraftkonstante der Bindung (2,8 N/cm, O: 11,4 N/cm) und der kleineren Frequenz der Molekülschwingung (770 cm, O: 1555 cm) aus.

Das Peroxidion ist im Vergleich zu den weiteren Sauerstoffmolekülionen (Hyperoxidion: O und Ozonidion O) sowie dem Sauerstoffmolekül als einziges kein Radikal und nicht paramagnetisch.

Aufgrund der schwachen Bindung zwischen den Sauerstoffatomen kann leicht eine homolytische Spaltung induziert werden. Hierbei werden zwei radikalische Fragmente gebildet, die in den meisten Fällen eine hohe Reaktivität aufweisen.

Auslöser für die Spaltung können sowohl Temperatur (Thermolyse) als auch Lichtwellen (Photolyse) oder chemische Reaktionen sein.

Die Gruppe der anorganischen Peroxide gliedert sich in die Klassen der ionischen Peroxidsalze und die der anorganischen Säureperoxide, die kovalent gebundene Peroxideinheiten tragen. Während von der ersten Klasse im Wesentlichen die Peroxide der Alkali- und Erdalkalimetalle von technischer Bedeutung sind, ist bei den kovalenten Wasserstoffperoxid der prominenteste Vertreter. Des Weiteren sind noch eine Reihe von Peroxiden von Mineralsäuren wie Carosche Säure und Perkohlensäure von Bedeutung. Im Gegensatz zum rein ionischen Charakter der Alkalimetallperoxide besitzen die Peroxide der Übergangsmetalle einen stärker kovalenten Charakter, was auf ihre höhere Elektronegativität zurückzuführen ist.

Alkalimetallperoxide, mit Ausnahme von Lithiumperoxid, können direkt durch Oxidation der Elemente mit Sauerstoff unter Normaldruck dargestellt werden.

Lithiumperoxid hingegen kann durch Umsetzung von Lithiumhydroxid mit Wasserstoffperoxid synthetisiert werden.

Das historisch zur Wasserstoffperoxidherstellung genutzte Bariumperoxid kann durch Oxidation von Bariumoxid bei erhöhter Temperatur und erhöhtem Druck erhalten werden.

Das am weitesten verbreitete Verfahren zur Synthese von Wasserstoffperoxid ist das Anthrachinon-Verfahren. In diesem wird zunächst eingesetztes Anthrachinon katalysiert durch Palladium mit molekularem Wasserstoff hydriert. Aus dem entstandenen Anthrahydrochinon wird in einem zweiten Schritt durch Oxidation mit molekularem Sauerstoff Wasserstoffperoxid unter Rückbildung von Anthrachinon freigesetzt. Die Bruttoreaktionsgleichung lautet somit:

Die direkte Synthese von Wasserstoffperoxid aus den Elementen ist derzeit im industriellen Maßstab nicht möglich, da es nur in geringem Maße gebildet wird.
Viele peroxidische Mineralsäuren lassen sich durch anodische Oxidation der zugrundeliegenden Säuren synthetisieren. So sind beispielsweise Peroxodisulfate und Percarbonate auf diesem Wege zugänglich.

Peroxodischwefelsäure diente historisch zur Herstellung von Wasserstoffperoxid in einem zu Beginn des 20. Jahrhunderts entwickelten Verfahren. Die oxidativ erhaltene Peroxodischwefelsäure wurde hierbei durch Hydrolyse in Wasserstoffperoxid und Schwefelsäure überführt.

Für die Reaktionen von Peroxidsalzen sind wenige Reaktionen allgemein formulierbar. Bei allen Peroxidsalzen tritt bei Versatz mit einem Überschuss verdünnter Säuren die Freisetzung von Wasserstoffperoxid auf.

Die Reaktion mit Wasser, ebenso die Thermolyse des Salzes, führt zur Bildung naszierenden Sauerstoffs.

Peroxide sind starke Oxidationsmittel und können zur Oxidation weiterer Verbindungen eingesetzt werden.

Alkalimetallperoxide können zur Synthese organischer Peroxide verwendet werden. Ein Beispiel ist die Umsetzung von Natriumperoxid mit Benzoylchlorid zu Dibenzoylperoxid.

Viele anorganische Peroxide finden Verwendung als Bleichmittel in der Textilindustrie sowie in der Papierindustrie. Durch die zunehmende Entwicklung umweltschonender Bleichverfahren ist die Verwendung peroxidhaltiger Verbindungen stark angestiegen und hat die älteren auf der Bleichwirkung chlorhaltiger Verbindungen basierende Herangehensweise in weiten Teilen verdrängt. Zu den Hauptanwendungsgebieten im Haushalt gehören der Einsatz in Waschmitteln. In der Vergangenheit wurden hierzu Perborate eingesetzt, welche auf Grund der ansteigenden Borkonzentrationen in der Umwelt weitgehend durch Percarbonate ersetzt wurden. Die Verwendung peroxidischer Verbindungen in Waschmitteln schlägt sich auch in deren Handelsbezeichnungen nieder. So setzt sich der Name des Waschmittels Persil aus den Worten "Perborat" und "Silicat" zusammen.

Durch Reaktion mit Kohlenstoffdioxid kann aus einigen peroxidischen Salzen Sauerstoff unter Bildung von Carbonatsalzen freigesetzt werden. Dies wird in Sauerstoffgeneratoren beispielsweise in Atemgeräten, U-Booten oder Raumkapseln ausgenutzt. Zur Generation von Sauerstoff wird hierbei das in der Atemluft unerwünschte Kohlenstoffdioxid, das als Stoffwechselprodukt in die Umgebung abgegeben wird, umgesetzt und benötigter Sauerstoff freigesetzt. In den meisten Fällen wird hierzu Natriumperoxid verwendet, in Raumkapseln jedoch Lithiumperoxid, da es eine geringere molare Masse besitzt und pro Gewichtseinheit verhältnismäßig ein größeres Volumen an Sauerstoff zu erzeugen vermag.

Bariumperoxid wurde historisch zur Herstellung reinen Sauerstoffs aus Luft verwendet. Ausgenutzt wurde hierzu das temperaturabhängige Gleichgewicht zwischen Bariumoxid und -peroxid. Durch die Umsetzung von Bariumoxid mit Luft wurde zunächst bei 500 °C Bariumperoxid gebildet. Dieses setzt bei Temperaturen über 700 °C Sauerstoff unter Rückbildung von Bariumoxid frei.

Die Gruppe der organischen Peroxide teilt sich im Wesentlichen in zwei Verbindungsklassen auf, die Peroxycarbonsäuren und die "organischen Peroxide" beziehungsweise Hydroperoxide. Die erste Klasse leitet sich von den Carbonsäuren ab, die zweite formal von den Ethern beziehungsweise Alkoholen.

Zur Synthese aliphatischer Peroxide sind mehrere Wege bekannt. So kann die Reaktion von Dialkylsulfaten mit alkalischer Wasserstoffperoxidlösung hierzu benutzt werden. Das Alkylsulfat tritt hierbei als Donor der Alkylgruppe auf und das Sulfation reagiert im Sinne einer Abgangsgruppe.

Cyclische Peroxide hingegen können aus Alkylendisulfaten unter gleichen Bedingungen erhalten werden. Die viergliedrigen Dioxetane lassen sich durch [2+2]-Cycloaddition von Sauerstoff an Alkene darstellen.
Die gezielte Synthese von Hydroperoxiden kann durch radikalische Oxidation von Alkanen mit Sauerstoff durchgeführt werden. Das durch einen Radikalstarter gebildete "Primärradikal" reagiert hierbei mit Sauerstoff zu einem Hydroperoxylradikal. Dieses ist ein wenig reaktives Radikal, das mit hoher Selektivität aktivierte Wasserstoffatome zu abstrahieren vermag, wobei das Hydroperoxid unter Bildung eines neuen Radikals freigesetzt wird.
Diese Selektivität wird beispielsweise in der Hockschen Phenolsynthese großtechnisch ausgenutzt.

Diese Reaktion läuft auch langsam durch Luftsauerstoff induziert bei Stoffen ab, die stabile Radikale bilden können. Beispiele hierfür sind die Autoxidation häufig eingesetzter Lösungsmitteln aus der Gruppe der Ether, wie beispielsweise Diethylether, Diisopropylether, Tetrahydrofuran oder 1,4-Dioxan. Hierbei entstehen explosive "Etherhydroperoxide", die beim Erhitzen oder Konzentrieren schwere Explosionen verursachen können.
Peroxide können in Organismen durch eine En-Reaktion oder Diels-Alder-Reaktion von Alkenen mit Sauerstoff gebildet werden. Als olefinische Substrate können hierbei ungesättigte Fettsäuren zur En-Reaktion oder ungesättigte Aminosäuren wie Histidin zur Diels-Alder-Cyclisierung dienen. Auch das Ranzigwerden von Fetten wird teils durch die Bildung von Peroxiden verursacht. Hierbei laufen Oxidationen mit Luftsauerstoff wie die En-Reaktion oder Radikalreaktionen ab und bilden neben Peroxiden auch Alkohole, Aldehyde und Carbonsäure. Die Verdorbenheit eines Fettes oder Öls kann somit anhand der Peroxidzahl bestimmt werden, welche die Menge an Peroxiden pro Kilogramm Substanz angibt.

Die meisten Peroxycarbonsäuren sind durch die Reaktion von Wasserstoffperoxid mit der entsprechenden Carbonsäure zugänglich:

Einen weiteren Syntheseweg stellt die Verwendung von Carbonsäurechloriden anstelle der freien Carbonsäure dar. Dieser Weg wird vor allem bei aromatischen Carbonsäuren gewählt und im basischen Milieu durchgeführt, um den entstehenden Chlorwasserstoff zu neutralisieren.

Auch aromatische Aldehyde gehen eine Autooxidation zu Peroxycarbonsäuren ein:

Diese reagieren jedoch mit weiterem Aldehyd zur Carbonsäure:

Peroxycarbonsäuren sind im Allgemeinen schwächere Säuren als die ihnen zu Grunde liegenden Carbonsäuren. Wie die meisten peroxidischen Verbindungen neigen sie in hoher Konzentration und bei höheren Temperaturen zu explosivem Verhalten.

In Gegenwart oxidierbarer Verbindungen können sie als Oxidationsmittel fungieren.

Organische Persäuren können zur Synthese von Epoxiden dienen. Dies geschieht in der Prileschajew-Reaktion.
Die Baeyer-Villiger-Umlagerung zur Synthese von Lactonen aus cyclischen Ketonen stellt ein weiteres Anwendungsgebiet für organische Peroxide dar. In beiden Fällen eignen sich besonders elektronenarme Peroxycarbonsäure zur Durchführung der Reaktion. Eine häufig verwendete Persäure stellt "meta"-Chlorperbenzoesäure ("m"CPBA) dar.

Das Hydroperoxid "tert"-Butylhydroperoxid dient in der Sharpless-Epoxidierung zur enantioselektiven Synthese von Epoxiden, für welche unter anderem 2001 der Nobelpreis für Chemie an Barry Sharpless verliehen wurde, als Oxidationsmittel.

Peressigsäure findet als Desinfektionsmittel im medizinischen Bereich und der Lebensmittelindustrie Anwendung. Es sind des Weiteren peroxidhaltige Lösungen zur Reinigung von Kontaktlinsen im Handel erhältlich.

Sowohl im Labor- als auch im industriellen Einsatz wird Dibenzoylperoxid als Radikalstarter verwendet. Die schwache peroxidische Bindung kann leicht homolytisch gespalten werden und bildet somit reaktive Benzoylradikale. Diese können im großtechnischen Maßstab zur Polymerisation zu Kunststoffen wie Polyethylen eingesetzt werden.

Von industrieller Bedeutung ist die Synthese des zur Kunststoffherstellung benötigten ε-Caprolactams. Hierzu wird ε-Caprolacton durch eine Baeyer-Villiger-Umlagerung von Cyclohexanon mit Peressigsäure gebildet, welches anschließend mit Ammoniak zum Lactam umgesetzt wird.

Industrielle Kunstharze auf Basis von Acryl- und/oder Metacrylsäureestern werden ausnahmslos durch radikalische Polymerisation mit organischen Peroxiden bei erhöhten Temperaturen hergestellt. Durch geeignete Wahl von Temperatur und Typ des Peroxids lässt sich die Polymerisationsrate einstellen.

Es konnten wenige peroxidische Wirkstoffe hergestellt werden. Zu ihnen gehören beispielsweise Artesunat und Artemisinin. Ihre Wirkungsweise basiert auf der Bildung von Radikalen an gewünschten Stellen im Organismus.

Auf Grund der explosiven Wirkung vieler Peroxide wurden auch Initialsprengstoffe auf Peroxidbasis entwickelt und eingesetzt. Zu den bekanntesten hiervon zählen Acetonperoxid (APEX) und Hexamethylentriperoxiddiamin (HMTD).

Zur qualitativen und quantitativen Bestimmung von Peroxiden sind viele Analysemethoden in der Literatur beschrieben. Als einfacher qualitativer Nachweis für Peroxide gilt die Iod-Stärke-Reaktion. Hierbei oxidieren anwesende Peroxide, Hydroperoxide oder Persäuren das zugesetzte Kaliumiodid zu Iod, welches in Anwesenheit von Stärke tiefblaue Einlagerungskomplexe bildet. Diese Methode eignet sich auch zur quantitativen Bestimmung, unterscheidet jedoch nicht zwischen den verschiedenen Arten peroxidischer Verbindungen. Zur Unterscheidung zwischen Peroxiden und Persäuren kann die Entfärbung einer Indigolösung untersucht werden, die bei Anwesenheit von Persäuren sofort eintritt. Die Blaufärbung, die bei Anwesenheit von Peroxiden mit Leuko-Methylenblau auftritt, ist hingegen spezifisch für Peroxide. Im Handel sind Iod-Stärke-Papiere erhältlich, die als schneller Indikator auf die Anwesenheit von Peroxiden verwendet werden können.

Zur quantitativen Bestimmung von Hydroperoxiden eignet sich die potentiometrische Titration mit Lithiumaluminiumhydrid. Eine Möglichkeit, den Gehalt an Persäuren und Peroxiden quantitativ zu bestimmen, ist die volumetrische Titration mit Alkoholaten wie Natriumethanolat.

Peroxide sind labile Verbindungen und können besonders in höheren Konzentrationen und bei erhöhter Temperatur explosives Verhalten zeigen. Des Weiteren sind sie brandfördernd und können mit oxidierbaren Stoffen (auch Watte und Zellstoff) unter Feuerentwicklung reagieren. Aus diesem Grund sind bei Arbeiten mit Peroxiden eine Reihe von Sicherheitsmaßnahmen zu beachten:





Die sichere industrielle Handhabung von Peroxiden erfordert umfangreiche Sicherheitsmaßnahmen. Organische Peroxide werden industriell in Zellenbauten hergestellt (Explosionsschutz). Der apparative Aufbau befindet sich meist in Betonzellen mit Folienfenstern zur Druckentlastung bei etwaiger Explosion. Weitere Sicherheitsmaßnahmen sind mit Wasser gefüllte Gräben vor den Produktionszellen, die mit der Reaktionslösung im Falle von thermischem Durchgehen geflutet werden können. Nach der Herstellung und Abfüllung in Kleingebinde müssen peroxidische Verbindungen zügig in gekühlte Lager verbracht werden.

Ausführliche Unfallverhütungsvorschriften sind im Rahmen der berufsgenossenschaftlichen Vorschrift DGUV Vorschrift 13 verankert.



</doc>
<doc id="4152" url="https://de.wikipedia.org/wiki?curid=4152" title="Philosophie der Mathematik">
Philosophie der Mathematik

Die Philosophie der Mathematik ist ein Bereich der theoretischen Philosophie, der anstrebt, Voraussetzungen, Gegenstand, Methode und Natur der Mathematik zu verstehen und zu erklären.

Systematisch grundlegend sind dabei Fragen nach

Ausgangspunkt ist fast durchgehend die Auffassung, dass mathematische Sätze apodiktisch gewiss, zeitlos und exakt sind und ihre Richtigkeit weder von empirischen Ergebnissen noch von persönlichen Ansichten abhängt. Aufgabe ist es, die Bedingungen der Möglichkeit solcher Erkenntnis zu ermitteln, wie auch diesen Ausgangspunkt zu hinterfragen.

Eine unter Mathematikern verbreitete Position ist der Realismus, vertreten u. a. durch Kurt Gödel und Paul Erdős. Mathematische Gegenstände (Zahlen, geometrische Figuren, Strukturen) und Gesetze sind keine Konzepte, die im Kopf des Mathematikers entstehen, sondern es wird ihnen eine vom menschlichen Denken unabhängige Existenz zugesprochen, wie Friedrich Engels im Anti-Dühring betont. Mathematik wird folglich nicht erfunden, sondern entdeckt. Durch diese Auffassung wird dem objektiven, also interpersonellen Charakter der Mathematik entsprochen. Dieser ontologische Realismus ist materialistische Philosophie.

Die klassische Form des Realismus ist der Platonismus, dem zufolge die mathematischen Gegenstände und Sätze losgelöst von der materiellen Welt und unabhängig von Raum und Zeit existieren, zusammen mit den anderen Ideen wie dem „Guten“, dem „Schönen“, oder dem „Göttlichen“. Das Hauptproblem des Platonismus in der Philosophie der Mathematik ist die Frage, auf welche Weise wir als begrenzte Wesen die mathematischen Objekte und Wahrheiten erkennen können, wenn sie in diesem „Ideenhimmel“ beheimatet sind.
Laut Gödel leistet dies eine mathematische Intuition, die, ähnlich einem Sinnesorgan, uns Menschen Teile dieser anderen Welt wahrnehmen lässt. Derartige rationale Intuitionen werden auch von den meisten Klassikern des Rationalismus und in jüngeren Debatten um Rechtfertigung oder Wissen a priori u. a. von Laurence Bonjour verteidigt.

Aristoteles behandelt seine Philosophie der Mathematik in den Büchern XIII und XIV der "Metaphysik". Er kritisiert hier und vielerorts den Platonismus.

Der Logizismus wurde unter anderem von Gottlob Frege, Bertrand Russell und Rudolf Carnap begründet. Nach dieser These lässt sich die Mathematik vollständig auf die formale Logik zurückführen und ist folglich auch als ein Teil der Logik zu verstehen. Logizisten vertreten die Ansicht, dass mathematische Erkenntnis a priori gültig ist. Mathematische Konzepte sind abgeleitet von logischen Konzepten, mathematische Sätze folgen direkt aus den Axiomen der reinen Logik.

Gottlob Frege, der als einer der großen Denker des 20. Jahrhunderts gilt, führte in seinen "Grundgesetzen der Arithmetik" das Gesetzesgebäude des Zahlenrechnens auf logische Prinzipien zurück. Freges Konstruktion erwies sich aber noch vor seiner vollständigen Veröffentlichung als brüchig, nachdem Russell mit seiner berühmten Antinomie zeigte, dass Zirkelschlüsse und Widersprüche seinem auf formale Logik gegründeten mathematischen Gebäude das Fundament nahmen. Russell teilte dies Frege in einem Brief mit, worauf dieser in eine tiefe persönliche Krise geriet. Später konnten mit komplizierteren Axiomensystemen die Widersprüche vermieden werden, so dass die Mengenlehre und insbesondere die Theorie der Natürlichen Zahlen widerspruchslos begründet werden konnten.

Kritisiert wird am Logizismus vor allem, dass er die Grundprobleme der Mathematik nicht löst, sondern lediglich auf Grundlagenprobleme der Logik schiebt und somit keine befriedigenden Antworten gibt.

Der Formalismus versteht die Mathematik ähnlich einem Spiel, das auf einem gewissen Regelwerk beruht, mit dem Zeichenketten "(engl. strings)" manipuliert werden. Zum Beispiel wird in dem Spiel „Euklidische Geometrie“ der Satz des Pythagoras gewonnen, indem gewisse Zeichenfolgen (die "Axiome") mit gewissen Regeln (denen des logischen Schlussfolgerns) wie Bausteine zusammengefügt werden. Mathematische Aussagen verlieren damit den Charakter von Wahrheiten (etwa über geometrische Figuren oder Zahlen), sie sind letztlich gar keine Aussagen mehr „über irgendetwas“.

Als "Deduktivismus" wird oft eine Variante des Formalismus bezeichnet, in der z. B. der Satz des Pythagoras keine absolute Wahrheit mehr darstellt, sondern nur eine "relative": Wenn man den Zeichenfolgen in einer Weise Bedeutungen zuweist, so dass die Axiome und die Schlussregeln wahr sind, dann muss man die Folgerungen, z. B. den Satz des Pythagoras, als wahr ansehen. So gesehen muss der Formalismus kein bedeutungsloses symbolisches Spiel bleiben. Der Mathematiker darf vielmehr hoffen, dass es eine Interpretation der Zeichenfolgen gibt, die ihm z. B. die Physik oder andere Naturwissenschaften vorgeben, so dass die Regeln zu wahren Aussagen führen. Ein deduktivistischer Mathematiker kann sich also sowohl von der Verantwortung für die Interpretationen als auch von den ontologischen Schwierigkeiten der Philosophen freihalten.

David Hilbert gilt als bedeutender früher Vertreter des Formalismus. Er strebt einen konsistenten axiomatischen Aufbau der gesamten Mathematik an, wobei er wiederum die natürlichen Zahlen als Ausgangspunkt wählt, in der Annahme, damit ein vollständiges und widerspruchsfreies System zu besitzen. Dieser Auffassung hat kurze Zeit später Kurt Gödel mit seinem Unvollständigkeitssatz den Boden entzogen. Damit war für jedes Axiomensystem, das die natürlichen Zahlen umfasst, bewiesen, dass es entweder unvollständig oder in sich widersprüchlich ist.

Der Strukturalismus betrachtet die Mathematik in erster Linie als eine Wissenschaft, die sich mit allgemeinen Strukturen beschäftigt, d. h. mit den Relationen von Elementen innerhalb eines Systems. Um dies zu illustrieren, kann man als Beispiel-System etwa die Verwaltung eines Sportvereins betrachten. Die verschiedenen Ämter (etwa Vorstand, Kassenprüfer, Kassenwart usw.) lassen sich unterscheiden von den Personen, die diese Aufgaben übernehmen. Wenn man nur das Gerüst der Ämter betrachtet (und somit die konkreten Personen, die sie ausfüllen, weglässt), dann erhält man die allgemeine Struktur eines Vereins. Der Verein selbst mit den Personen, die die Ämter übernommen haben, exemplifiziert diese Struktur.

Ebenso exemplifiziert jedes System, dessen Elemente einen eindeutigen Nachfolger haben, die Struktur der natürlichen Zahlen: Analoges gilt für andere mathematische Objekte.
Da der Strukturalismus Objekte wie Zahlen nicht losgelöst von ihrer Gesamtheit oder Struktur betrachtet, sondern sie mehr als Plätze in einer Struktur sieht, weicht er der Frage nach der Existenz von mathematischen Objekten aus bzw. klärt sie als Kategorienfehler. So ist etwa die Zwei als natürliche Zahl nicht mehr losgelöst von der Struktur der natürlichen Zahlen zu betrachten, sondern ein Bezeichner für den zweiten Platz in der Struktur der natürlichen Zahlen: weder hat sie interne Eigenschaften noch eine eigene Struktur. Dementsprechend gibt es sowohl Varianten des Strukturalismus, die mathematische Objekte als existent annehmen, als auch solche, die ihre Existenz ablehnen.

Probleme ergeben sich bei dieser Strömung insbesondere aus der Frage nach den Eigenschaften und dem Sein der Strukturen. Ähnlich wie im Universalienstreit handelt es sich bei Strukturen offenbar um etwas, das gleichzeitig vielen Systemen zukommen kann. So wird die Struktur einer Fußballmannschaft sicher von Tausenden Mannschaften exemplifiziert. Es stellt sich also die Frage, ob und wie Strukturen existieren, ob sie etwa unabhängig von Systemen existieren. Andere offene Fragen betreffen den Zugang zu Strukturen; wie können wir etwas über Strukturen lernen?

Aktuelle Vertreter des Strukturalismus sind Stewart Shapiro, Michael Resnik und Geoffrey Hellman.

Der von Luitzen Brouwer begründete Intuitionismus verneint die Existenz mathematischer Begriffe außerhalb des menschlichen Geistes, verwendet deshalb konstruktive Beweise und nicht solche, die Existenzaussagen ohne Angabe einer Konstruktion machen, weshalb in der verwendeten intuitionistischen formalen Logik der Satz vom ausgeschlossenen Dritten nicht verwendet wird. Eine Verallgemeinerung des Intuitionismus ist der Konstruktivismus.

Der Konventionalismus wurde von Henri Poincaré entwickelt und teilweise von logischen Empiristen (Rudolf Carnap, Alfred Jules Ayer, Carl Hempel) weiterentwickelt.

Von der Perspektive des Mathematikers ausgehend und zugleich auf die Erkenntniskritik Immanuel Kants zurückgreifend, ergibt sich die Frage nach der "kategorialen Verfassung" des Menschen, aus welcher sich die mathematischen Disziplinen ableiten lassen (vgl. Ernst Kleinert).

Auch in populärwissenschaftlicher Literatur werden Fragen der Philosophie der Mathematik vorgestellt. So wird u. a. von John D. Barrow und Roger Penrose diskutiert, wieso die Mathematik überhaupt nützlich ist und warum sie so gut auf die Welt passt.








</doc>
<doc id="4153" url="https://de.wikipedia.org/wiki?curid=4153" title="Pneumologie">
Pneumologie

Die Pneumologie (auch Pneumonologie; griech. πνεύμων "pneumōn" „Geist“, „Hauch“, „Atem“, metonymisch für „Lunge“) oder Pulmologie (auch Pulmonologie; lat. "pulmo" „Lunge“) ist ein Teilgebiet der Inneren Medizin, das sich mit Lungenerkrankungen beschäftigt. Die deutsche Bezeichnung lautet Lungenheilkunde.

Die Pneumologie umfasst die Prophylaxe, Erkennung und konservative Behandlung der Krankheiten der Lunge, der Bronchien, des Mittelfells (Mediastinums) und der Pleura.

Die Thoraxchirurgie als Teil der Chirurgie, die Bestrahlung von Lungentumoren als Teil der Strahlentherapie und die Chemotherapie von Lungentumoren als Teil der Onkologie gehören nicht direkt zur Pneumologie.



seltener
internistische Thorakoskopie (eine Spiegelung des Rippenfells)






</doc>
<doc id="4154" url="https://de.wikipedia.org/wiki?curid=4154" title="Pleura">
Pleura

Die Pleura (von griechisch πλευρά „Flanke“, „Rippe“), deutsch Brustfell, ist eine dünne seröse Haut in der Brusthöhle. Sie überzieht die Lungen und kleidet die Brusthöhle von innen aus.

Als Lungenfell (fachsprachlich "Pleura visceralis" oder "Pleura pulmonalis") überzieht die Pleura die Lungen.

Als Brustfell im engeren Sinne (fachsprachlich "Pleura parietalis") kleidet die Pleura die Brusthöhle von innen aus. Man unterscheidet hier nochmals vier Bereiche:

Pleura besteht histologisch aus einschichtigem Plattenepithel, welches ontogenetisch vom Mesothel abstammt, und einer Lamina propria. Die beiden Pleurablätter schlagen am Hilus der Lunge und am Ligamentum pulmonale ineinander um. Sie sind durch die Pleurahöhle ("Cavitas pleuralis") voneinander getrennt, einen dünnen, druckdichten Spalt, der mit seröser Flüssigkeit gefüllt ist. In der Pleurahöhle können sich Ergüsse sammeln oder Metastasen bilden.

Fußwärts (an den Zwerchfellrändern) und zur Brustmitte hin (im Mediastinum) hat die Pleura Reserveausbuchtungen (Recessus), um die Lungenausweitung beim Atemvorgang zu gewährleisten. Diese Reserveräume werden aber auch bei tiefer Inspiration nie ganz ausgefüllt. Die Pleura parietalis bildet vier tiefe Falten:

Die sensible Nervenversorgung des Brustfells übernehmen Äste des zehnten Hirnnerven, des Nervus vagus sowie des Nervus phrenicus (Pars mediastinalis und diaphragmatica) und der Interkostalnerven (Pars costalis). Das Lungenfell dagegen hat wahrscheinlich keine Schmerzempfindung.

Das Lungenfell ist eine gleitende Verschiebeschicht für die Lungenbewegungen. Sie ist ein unbedingt notwendiges, Sog-vermittelndes Medium für die Atmung: Durch relativen Unterdruck im Pleurazwischenraum und die kapillare Adhärenz der Pleurablätter müssen die Lungen beim Einatmen der aktiven Ausdehnung von Brustwandmuskeln und dem Zwerchfell folgen. 

Wird der relative Unterdruck zwischen beiden Pleurablättern aufgehoben (wie etwa durch einströmende Luft bei Stichverletzung), so folgt die Lunge dem sich ausdehnenden Brustkorb beim Einatmen nicht mehr, was schließlich zum Zusammenfallen des auf Entfaltung angewiesenen Lungenflügels führt (Pneumothorax).

Abgesehen von der Befragung des Patienten (Anamnese) kommen verschiedene Untersuchungsmethoden in Frage:




</doc>
<doc id="4157" url="https://de.wikipedia.org/wiki?curid=4157" title="Passiva">
Passiva

Unter Passiva (Plural von "Passivum") versteht man die Summe des einem Unternehmen zur Verfügung gestellten Kapitals, das auf der rechten Seite einer Bilanz zu finden ist. Gegensatz sind die Aktiva. 

Passiva (lat. "pati", „untätig sein“, „leiden“) stehen auf der Passivseite der Bilanz und stellen die Kapitalquellen eines Unternehmens dar. Bilanz- und gesellschaftsrechtlich ist also Kapital die Summe aller Passiva eines Unternehmens. Die Passivseite lässt erkennen, aus welchen Kapitalquellen die auf der Aktivseite der Bilanz stehenden Vermögenswerte finanziert wurden. Von Passivierung spricht man, wenn ein Bilanzposten auf der Passivseite verbucht wird. Dabei ist zu unterscheiden, ob die Passiva einer Passivierungspflicht, einem Passivierungswahlrecht oder einem Passivierungsverbot unterliegen. Grundsätzlich besteht Passivierungspflicht, also die gesetzlich vorgeschriebene Verpflichtung eines bilanzierenden Unternehmens, alle Eigenkapital- und Fremdkapitalposten in die Passivseite der Bilanz aufzunehmen. Ausnahmsweise bestehen Wahlrechte (für bestimmte Pensionsrückstellungen) und Passivierungsverbote (für alle nicht in HGB aufgezählten Rückstellungen). 

Die Buchhaltung führt die Endbestände der Passiv- und Aktivkonten zusammen, die Gegenüberstellung der Passiva mit den Aktiva zu einer kontenmäßigen Einheit heißt Bilanz. Hierin sind die Summen der Aktiva und der Passiva (Bilanzsumme) formal identisch, dies ist ein wesentliches Merkmal einer Bilanz. Der so gefasste Bilanzbegriff unterscheidet sich vom Kontobegriff nur darin, dass man beim Konto von Soll und Haben spricht. Dass Aktiva und Passiva summenmäßig identisch sind, liegt am systematischen Wertausgleich durch den Saldoposten des Erfolges. Daraus ergibt sich die Gleichung

Die drei Bilanzprinzipien der Bilanzwahrheit, Bilanzklarheit und Bilanzkontinuität gelten sowohl für Aktiv- als auch Passivseite. Aus Gründen des Vorsichtsprinzips und des damit einhergehenden Gläubigerschutzes können bestimmte Teile der Passiva (insbesondere Rückstellungen) im Rahmen des Niederstwertprinzips überbewertet werden, Aktiva werden hingegen unterbewertet. Überbewertung bedeutet, dass den als Verbindlichkeiten anzusehenden Rückstellungen in den Grenzen der vernünftigen kaufmännischen Beurteilung ein höherer Rückzahlungsbetrag beigemessen werden kann, der der wahrscheinlichen Inanspruchnahme des Unternehmens durch einen Dritten entspricht.

Der Begriff Passivseite ist ein bestimmter Rechtsbegriff, der im Gliederungsschema des Abs. 3 HGB erwähnt wird. Danach besteht die Passivseite auf der ersten Gliederungsebene abschließend aus Eigenkapital und Verbindlichkeiten, zu denen Rückstellungen, Verbindlichkeiten, Rechnungsabgrenzungsposten und passive latente Steuern gehören. 

Das Eigenkapital ergibt sich als Saldo zwischen den Wertansätzen auf der Aktivseite und denen auf der Passivseite. Es stellt das erbrachte und in der Unternehmung belassene Kapital dar, auf das die Unternehmensinhaber Residualansprüche haben. Bei Einzelunternehmen erfolgt der Ausweis des Eigenkapitals in der Bilanz als Gesamtposten, bei Personengesellschaften kann nach Haftungscharakter unterschieden werden. Personengesellschaften ohne persönlich haftende Gesellschafter (siehe HGB) und Kapitalgesellschaften gliedern das Eigenkapital wie folgt:

Oftmals werden die Positionen IV. und V. auch zu einer Position mit der Bezeichnung "Bilanzgewinn/Bilanzverlust" zusammengefasst.

Unter bestimmten Umständen kann oder muss die Passivseite um weitere Posten ergänzt werden. Neue Posten dürfen hinzugefügt werden, wenn ihr Inhalt nicht von einem vorgeschriebenen Posten abgedeckt wird. Gliederung und Bezeichnung bestimmter Posten der Bilanz sind zu ändern, wenn dies wegen Besonderheiten (der Kapitalgesellschaft) zur Aufstellung eines klaren und übersichtlichen Jahresabschlusses erforderlich ist ( Abs. 5 und 6 HGB).

Sonderposten mit Rücklagenanteil hatten sowohl Fremd- als auch Eigenkapitalcharakter und durften nur bis Dezember 2009 gebildet werden.

Die Bilanzanalyse interessiert sich für die Zusammensetzung der Passiva, deren Verhältnis zu anderen Bilanzpositionen und ermittelt betriebswirtschaftliche Kennzahlen, die sich mit der vertikalen Kapitalstruktur der Passivseite befassen. Hierzu gehören Eigenkapitalquote und Fremdkapitalquote, während sich die horizontale Kapitalstruktur mit dem Verhältnis von Aktiv- zu Passivseite einer Bilanz im Rahmen der Anlagendeckung befasst. Die Eigen- und Fremdkapitalrentabilität spiegeln die Verzinsung des eingesetzten Eigen- oder Fremdkapitals wider, der Verschuldungsgrad macht Aussagen über das Verhältnis von Fremdkapital zum Cashflow.

Ferner ist die Aufteilung des Fremdkapitals in kurzfristig und langfristig fällige Verpflichtungen von Bedeutung. Hierzu haben beispielsweise Kapitalgesellschaften entsprechende Angaben über die Fristigkeiten ihrer Verbindlichkeiten im Anhang zu machen ( Satz 1 Nr. 1 HGB). Hierbei ist insbesondere relevant, ob das Unternehmen mit den vorhandenen oder kurzfristig verfügbaren liquiden Mitteln seinen kurzfristigen Verbindlichkeiten nachkommen kann. Konzernabschlüsse müssen daher u. a. eine Kapitalflussrechnung enthalten ( Abs. 1 HGB).

In den Vermögensbilanzen der Volkswirtschaftlichen Gesamtrechnung bestehen die Passiva aus den Verbindlichkeiten. Die Aktiva saldieren sich mit den Passiva zum Reinvermögen.




</doc>
<doc id="4158" url="https://de.wikipedia.org/wiki?curid=4158" title="Liste bedeutender Psychologen">
Liste bedeutender Psychologen

In der Wissenschaft der Psychologie anerkannte und wirklich bedeutende Psychologen und Psychologinnen mit Artikeln in der deutschsprachigen Wikipedia werden hier aufgeführt. Bedeutung wird z. B. an der Begründung einer Schule oder Theorie mit namhaften Anhängern, dem Verfassen eines epochalen Werkes und/oder der Leitung eines innerhalb der Psychologie herausragenden Institutes festgemacht.
Zum Auffinden aller (relevanten) Psychologen siehe ""

Vorwiegend als Psychotherapeuten bekannte Personen sind in der "Liste bedeutender Psychotherapeuten" enthalten.








</doc>
<doc id="4159" url="https://de.wikipedia.org/wiki?curid=4159" title="Cordwainer Smith">
Cordwainer Smith

Cordwainer Smith (eigentlich "Paul Myron Anthony Linebarger"; * 11. Juli 1913 in Milwaukee, Wisconsin, USA; † 6. August 1966 in Baltimore) war ein US-amerikanischer Science-Fiction-Autor. Von Beruf war er Politikwissenschaftler und politischer Berater. Unter seinem wirklichen Namen veröffentlichte er eine Reihe von Büchern über fernöstliche Politik und psychologische Kriegführung. Weitere von ihm verwendete Pseudonyme waren "Felix C. Forrest", "Carmichael Smith" und "Anthony Bearden".

Linebarger war der Sohn eines pensionierten Richters und wuchs in Japan, China, Frankreich und Deutschland auf. Sein Taufpate war der chinesische Politiker Sun Yat-sen, der ihm den chinesischen Namen Lin-Bai Lo gab (‚Wald des strahlenden Glücks‘, von Linebarger selbst später in seinem Pseudonym „Felix C. Forrest“ wiederverwendet). Er promovierte als Dreiundzwanzigjähriger an der Johns Hopkins University in Politikwissenschaft und arbeitete für den Geheimdienst der US-Armee. Im Zweiten Weltkrieg arbeitete er in der psychologischen Kriegführung in Chongqing, später war er Mitarbeiter des CIA, beriet die britischen Truppen in Malaya und die US-amerikanischen Streitkräfte in Korea. Dem Beraterstab des US-Präsidenten John F. Kennedy gehörte er als Asienexperte an.

Paul Linebarger war ein Experte in südostasiatischer Politik; er hat mit "Psychological Warfare" ein Buch geschrieben, das auch heute noch als Standardwerk der psychologischen Kriegsführung gilt.

Linebarger benutzte das Pseudonym "Cordwainer Smith" für Science-Fiction-Arbeiten. Viele von Cordwainer Smiths' Werken sind im sog. Universum der „Instrumentalität der Menschheit“ angesiedelt: In einer fernen Zukunft hat die Menschheit das Weltall erobert, und sowohl die Erde als auch alle anderen von Menschen bewohnten Planeten werden von einer allumfassenden, wohlmeinenden Regierung, der „Instrumentalität“, regiert. Diese versucht zunächst in utilitaristischer Weise einen stabilen Frieden zu wahren, der durch Eugenik, Unterdrückung aller Leidenschaften und die Versklavung der „Untermenschen“ (aus genmanipulierten Tieren gezüchteter Humanoiden, s.u.) erkauft ist. Erst mit der ausgerechnet von den „Untermenschen“ ausgehenden „Wiederentdeckung des Menschen“ findet die Instrumentalität zu dem neuen Ziel, die Gesellschaft zu ihren Wurzeln zurückzubringen, Unvollkommenheit nicht nur zuzulassen, sondern als substantielles Merkmal der Menschlichkeit zu würdigen und die alten Fehler wie Sklaverei oder Ausbeutung zu vermeiden. Die Erzählungen sind keiner klassischen Richtung der Science-Fiction zuzuordnen. Der an der christlichen Philosophie interessierte Leser kann viele Ideen und Themen wiedererkennen; gleichzeitig erinnert der Erzählstil an die chinesische Literatur.

Einige der bekannteren „Erfindungen“ Cordwainer Smiths sind der Planet "Norstrilia", in dessen Halbwüsten gigantische (über 100 Tonnen schwere) mutierte Schafe ein Unsterblichkeitsserum liefern, der Strafplanet "Shayol", auf dem verurteilten Verbrechern ständig Organe nachwachsen, die für Transplantationen geerntet werden, und die "Untermenschen", die in Wirklichkeit Tiere sind, deren Wachstum bei der Befruchtung so manipuliert wurde, dass sie menschliche Gestalt annehmen. Diese zum Bedienen der Menschen gezogenen, rechtlosen Wesen kämpfen um ihre Menschenrechte.

2001 wurde die "Cordwainer Smith Foundation" gegründet. Sie vergibt jährlich bei der Readercon in Burlington, Massachusetts den Cordwainer Smith Rediscovery Award, der erneute Aufmerksamkeit auf das Werk zu Unrecht vergessener oder nicht angemessen gewürdigter Science-Fiction-Autoren lenken soll.











</doc>
<doc id="4160" url="https://de.wikipedia.org/wiki?curid=4160" title="Peter Plichta">
Peter Plichta

Peter Plichta (* 21. Oktober 1939 in Remscheid) ist ein deutscher Chemiker, Apotheker und Autor.

Peter Plichta studierte Chemie in Köln, legte 1966 seine Diplomprüfung ab und wurde 1970 mit einer Dissertation über Silane ("„Präparative und spektroskopische Untersuchungen zur Darstellung von Disilanyl- und Digermanylverbindungen und Germaniumwasserstoffen“") unter Franz Fehér am Institut für Anorganische Chemie der Universität Köln promoviert. 1977 erlangte er die Approbation als Apotheker. 

Zu Plichtas technischen Konzepten gehört ein Fluggerät, das in seiner Form einem Diskus ähnelt. Dieses Konzept soll nach Plichta das heute übliche Mehrstufenantriebs-Prinzip in der Raumfahrt ablösen, welches nur zur Beförderung sehr geringer Nutzlasten (etwa 4 Prozent bei Ariane 5) in der Lage ist. Seinem Buch "Benzin aus Sand" zufolge ist das „einstufige“ Erreichen der erdnahen Umlaufbahn möglich, weil der von Plichta entdeckte Treibstoff (siehe unten) nicht nur in Sauerstoff, sondern auch in Stickstoff brennt, und deshalb in der Erdatmosphäre kein Oxidationsmittel mitgeführt werden muss.

Peter Plichta hat in mehreren Ländern Patente auf einige seiner Entwicklungen angemeldet, darunter in den USA. Sein Entwurf wird von der Fachwelt abgelehnt. Bis heute wurde kein ernstzunehmender Versuch einer Realisierung unternommen.

In den 1970er-Jahren begann Plichta, sich mit der synthetischen Treibstoffgewinnung aus Silicium, insbesondere aus Sand, zu beschäftigen. Plichta gibt an, als erster stabile, längerkettige Silane synthetisiert zu haben. Trotz Plichtas Veröffentlichungstätigkeit zu diesem Thema (Hauptwerk: „Benzin aus Sand. Die Silan-Revolution“) blieben seine Forschungen bisher ohne nachhaltige Resonanz in der Fachwelt und bei Automobilherstellern. In einer der wenigen Rezensionen des Werks in einer Fachzeitschrift wird Plichtas Vorschlag der Silan-Revolution als „origineller Vorschlag“ bezeichnet und die Frage gestellt, ob dieser „… belastbar oder gar seriös?“ sein könne. Das Buch informiere den Leser „… in einer eigenartigen Mischung aus Selbstbewusstsein, verkanntem Genie, Besserwisserei und Weinerlichkeit über selbstgewählte Höhepunkte …“ und folge in seinen Deutungen einer dem Rezensenten nicht zugänglichen Logik. Auch wenn nicht klar würde, was das Werk solle, gehe ein gewisser Reiz davon aus.

Seit 1991 veröffentlicht Peter Plichta seine Überlegungen zu seinem Weltbild. Er möchte dabei die physikalisch-chemische Realität aufbauend auf zahlentheoretischen und zahlenmystischen Überlegungen beschreiben, wobei insbesondere Primzahlen eine wichtige Rolle spielen. Er behauptet, mit seiner Arbeit die Quantenmechanik obsolet gemacht zu haben:
Plichta stellt den Anspruch, seine Theorie mathematisch „bewiesen“ zu haben. Er schließt Indeterminismus damit kategorisch aus:

Die Urknalltheorie lehnt Plichta ab. Er vertritt eine alternative Erklärung für die Bindung des Sauerstoffs ans Hämoglobin im Blut.

Die Chemiker Jan C. A. Boeyens und Demetrius C. Levendis haben Plichta in ihrem Werk "Number Theory and the Periodicity of Matter" referenziert. Ebenso wie Plichta versuchen sie die moderne Quantenphysik durch elementare zahlentheoretische Überlegungen zu ersetzen, im Gegensatz zu Plichta stellen sie jedoch nicht die allgemeine Relativitätstheorie in Frage, in der der Raum nicht euklidisch ist, und sehen keine ausgezeichnete Rolle des Dezimalsystems ("The specification of common numbers in decimal notation is almost certainly a remnant of counting practice using a ten-finger base"). Peter Plichta trat seit 2011 mehrfach als Gesprächspartner beim "Alpenparlament.tv" sowie 2013 beim "Alpenparlament Kongress" auf.




</doc>
<doc id="4162" url="https://de.wikipedia.org/wiki?curid=4162" title="Pythagoreer">
Pythagoreer

Als Pythagoreer (auch Pythagoräer, altgriechisch Πυθαγόρειοι "Pythagóreioi" oder Πυθαγορικοί "Pythagorikoí") bezeichnet man im engeren Sinne die Angehörigen einer religiös-philosophischen, auch politisch aktiven Schule, die Pythagoras von Samos in den zwanziger Jahren des 6. Jahrhunderts v. Chr. in Süditalien gründete und die nach seinem Tod noch einige Jahrzehnte fortbestand. Im weiteren Sinn sind damit alle gemeint, die seither Ideen des Pythagoras oder ihm zugeschriebene Ideen aufgegriffen und zu einem wesentlichen Bestandteil ihres Weltbildes gemacht haben.

Wegen der verworrenen Quellenlage sind viele Einzelheiten der philosophischen Überzeugungen und politischen Ziele der Pythagoreer unklar, zahlreiche Fragen sind in der Forschung stark umstritten. Sicher ist, dass in einer Reihe von griechischen Städten Süditaliens Gemeinschaften von Pythagoreern bestanden, die sich als soziale und politische Reformbewegung betrachteten und mit Berufung auf die Lehren des Schulgründers massiv in die Politik eingriffen. Dabei kam es zu schweren, gewaltsamen Auseinandersetzungen, die im 5. Jahrhundert v. Chr. mit wechselndem Erfolg ausgetragen wurden und schließlich mit Niederlagen der Pythagoreer endeten. In den meisten Städten wurden die Pythagoreer getötet oder vertrieben.

Für die Pythagoreer charakteristisch ist die Überzeugung, dass der Kosmos eine nach bestimmten Zahlenverhältnissen aufgebaute harmonische Einheit bildet, deren einzelne Bestandteile ebenfalls harmonisch strukturiert sind oder, soweit es sich um menschliche Lebensverhältnisse handelt, harmonisch gestaltet werden sollten. Sie nahmen an, dass in allen Bereichen – in der Natur, im Staat, in der Familie und im einzelnen Menschen – dieselben zahlenmäßig ausdrückbaren Gesetzmäßigkeiten gelten, dass überall Ausgewogenheit und harmonischer Einklang anzustreben sind und dass die Kenntnis der maßgeblichen Zahlenverhältnisse eine weise, naturgemäße Lebensführung ermöglicht. Das Streben nach Eintracht beschränkten sie nicht auf die menschliche Gesellschaft, sondern dehnten es auf die Gesamtheit der Lebewesen aus, was sich in der Forderung nach Rücksichtnahme auf die Tierwelt zeigte.

Von Pythagoras sind keine authentischen Schriften überliefert, nur einige ihm zugeschriebene Verse sind möglicherweise echt. Schon in der Antike gab es unterschiedliche Meinungen darüber, welche der als pythagoreisch geltenden Lehren tatsächlich auf ihn zurückgehen. Die Unterscheidung zwischen frühpythagoreischem und späterem Gedankengut gehört bis heute zu den schwierigsten und umstrittensten Fragen der antiken Philosophiegeschichte. In der Forschung ist sogar strittig, ob es sich bei der Lehre des Pythagoras tatsächlich um Philosophie und um wissenschaftliche Bestrebungen handelte oder um eine rein mythisch-religiöse Kosmologie. Zu diesen Schwierigkeiten trägt das frühe Einsetzen einer üppigen Legendenbildung bei.

Die politische Geschichte der Schule bis zu ihrem Untergang im 5. Jahrhundert v. Chr. ist in Umrissen bekannt. Hinsichtlich ihres Zwecks und ihrer Arbeitsweise und Organisation gehen die Meinungen der Historiker jedoch weit auseinander.

Pythagoras stammte von der griechischen Insel Samos. Er emigrierte zwischen 532 und 529 v. Chr in ein damals von Griechen besiedeltes Gebiet Süditaliens, wo er sich zunächst in Kroton (heute Crotone in Kalabrien) niederließ. Dort gründete er die Schule, die von Anfang an neben den Studien auch politische Ziele verfolgte. Am Krieg zwischen Kroton und der ebenfalls griechischen Stadt Sybaris, der von Sybaris ausging und mit großer Brutalität ausgetragen wurde, nahmen die Pythagoreer aktiv teil. Der Befehlshaber des Heeres der Krotoniaten, der berühmte Athlet Milon, war Pythagoreer.
Nach dem Sieg über Sybaris, das erobert und geplündert wurde (510), wurden die Pythagoreer in heftige Auseinandersetzungen innerhalb der Bürgerschaft von Kroton verwickelt. Dabei ging es um die Verteilung des eroberten Landes und um eine Verfassungsänderung. Wegen dieser Unruhen verlegte Pythagoras seinen Wohnsitz nach Metapontion (heute Metaponto in der Basilikata). Dort setzte er seine Lehrtätigkeit fort, während in Kroton seine hart bedrängten Anhänger unterlagen und vorübergehend die Stadt verlassen mussten. Als Anführer der gegnerischen Partei trat ein vornehmer Bürger namens Kylon hervor (daher „kylonische Unruhen“), und auch ein Volksredner namens Ninon hetzte gegen die Pythagoreer. Berichte einzelner Quellen, wonach es damals bereits zu einer blutigen Verfolgung kam, beruhen aber anscheinend auf Verwechslung mit späteren Vorgängen.

Da die Schule erhebliche Ausstrahlungskraft hatte, bildeten sich auch in anderen griechischen Städten Süditaliens Pythagoreergemeinschaften, die wohl auch dort in die Politik eingriffen. Eine institutionalisierte Herrschaft der Pythagoreer hat es aber weder in Kroton noch in Metapontion oder anderswo gegeben, sondern nur eine mehr oder weniger erfolgreiche Einflussnahme auf den jeweiligen Rat der Stadt und auf die Bürgerversammlung. Mehrere Quellen berichten, dass der Pythagoreismus sich auch in der nichtgriechischen Bevölkerung verbreitete. Genannt werden u. a. die Stämme der Lukanier und Messapier.

Pythagoras starb in den letzten Jahren des 6. Jahrhunderts oder im frühen 5. Jahrhundert. Nach seinem Tod setzten seine Schüler ihre Aktivitäten in den Städten fort. Es bestand nun aber wohl keine zentrale Lenkung der Schule mehr, denn Pythagoras hatte anscheinend keinen Nachfolger als allgemein anerkanntes Schuloberhaupt. Die Pythagoreer waren – gemäß ihrer insgesamt auf Harmonie und Stabilität ausgerichteten Weltanschauung – politisch konservativ. Dadurch wurden sie zu Verbündeten der traditionell in den Stadträten dominierenden Geschlechter. Sie stießen aber, wie das Beispiel Kylons zeigt, in den einflussreichen Familien auch auf Opposition. Ihre natürlichen Feinde waren überall die Agitatoren, die für einen Umsturz und für die Einführung der Demokratie eintraten und nur auf diesem Wege Macht erlangen konnten.

Um die Mitte des 5. Jahrhunderts oder etwas später kamen in einer Reihe von Städten demokratisch gesinnte Volksredner an die Macht. Sie gingen, damaliger Sitte folgend, mit großer Härte gegen die Anhänger der unterlegenen Partei vor. Daher kam es zu blutigen Verfolgungen der Pythagoreer, die entweder getötet wurden oder aus den Städten fliehen mussten. Die politischen Wirren hielten anscheinend längere Zeit an. Dabei konnten die Pythagoreer sich zeitweilig wieder durchsetzen; schließlich unterlagen sie jedoch überall außer in Tarent, wo sie noch bis um die Mitte des 4. Jahrhunderts eine starke Stellung hatten. Viele von ihnen emigrierten nach Griechenland. Die Schule hörte als Organisation zu bestehen auf.

Die Pythagoreer zerfielen laut einigen Quellenberichten in zwei Gruppen oder Richtungen, die „Mathematiker“ und die „Akusmatiker“. „Mathematiker“ waren diejenigen, welche sich mit „Mathemata“ befassten, das heißt mit schriftlich festgehaltenen Lerngegenständen und Empirie (auch, aber nicht nur Mathematik im heutigen Wortsinn). Dies kann als eine frühe Form von mathematisch-naturwissenschaftlicher Forschung betrachtet werden. Akusmatiker wurden Pythagoreer genannt, die sich auf „Akusmata“ (Gehörtes) beriefen, also auf die nur mündlich mitgeteilten Lehren des Pythagoras; dabei ging es hauptsächlich um Verhaltensregeln und das religiöse Weltbild.

Unklar ist, ob schon Pythagoras seine Schüler gemäß ihren Neigungen und Fähigkeiten in zwei Gruppen mit verschiedenen Aufgaben aufteilte oder die Abgrenzung zwischen den beiden Richtungen erst nach seinem Tod deutlich wurde. Jedenfalls kam es nach einem Bericht, den manche Forscher auf Aristoteles zurückführen, zu einem unbekannten Zeitpunkt nach dem Tod des Schulgründers zu einer Spaltung zwischen den zwei Richtungen. Dabei nahm jede von ihnen für sich in Anspruch, die authentische Tradition des Pythagoras fortzusetzen.

Der völlig verschiedenartige Charakter dieser beiden Richtungen gibt bis heute Rätsel auf. So ist unklar, welche der beiden Gruppen älter, welche größer und welche für den Pythagoreismus wichtiger war, welche den Kern der Schule ausmachte und damit als fortgeschrittener und höherrangig galt. Darüber gehen die Ansichten in der Forschung weit auseinander.

Manche Gelehrte (besonders Walter Burkert) meinen, dass zu Lebzeiten des Pythagoras alle Pythagoreer Akusmatiker waren und dass die griechische Wissenschaft außerhalb des Pythagoreismus entstanden ist. Demnach waren die „Mathematiker“ einzelne Pythagoreer, die sich erst nach dem Tod des Schulgründers mit wissenschaftlichen Anliegen zu befassen begannen; ihr Wirken fiel größtenteils in die Zeit nach dem Untergang der Schule. Diese „Mathematiker“ waren aber diejenigen Repräsentanten des Pythagoreismus, mit denen sich Platon im 4. Jahrhundert auseinandersetzte. Sie prägten das später (und bis heute) in der Öffentlichkeit vorherrschende, nach Burkerts Ansicht falsche Bild vom anfänglichen Pythagoreismus, indem sie ihn als eine Pflanzstätte wissenschaftlicher Forschung erscheinen ließen. Hinzu kam, dass schon die Schüler Platons und des Aristoteles platonische Gedanken für pythagoreisch hielten. In Wirklichkeit war die Schule nach Burkerts Deutung ein Bund mit religiösen und politischen Zielen, der seine esoterischen Lehren geheim hielt und kein Interesse an Wissenschaft hatte. Burkert vergleicht die Pythagoreergemeinschaft mit den Mysterienkulten.

Der Hauptvertreter der gegenteiligen Auffassung ist gegenwärtig Leonid Zhmud. Sie besagt, es habe weder eine Geheimlehre der frühen Pythagoreer noch eine für alle verbindliche religiöse Doktrin gegeben. Die Schule sei eine „Hetairie“ gewesen, ein lockerer Zusammenschluss autonom forschender Personen. Diese hätten sich gemeinschaftlich – aber ohne Fixierung auf vorgegebene Dogmen – ihren wissenschaftlichen und philosophischen Studien gewidmet. Außerdem seien sie durch bestimmte politische Ziele verbunden gewesen. Die Berichte über die Akusmatiker seien späte Erfindungen. Die Akusmata – ursprünglich „Symbola“ genannt – seien nur Sprüche und nicht konkrete, verbindliche Regeln für den Alltag gewesen. Sie seien zwar sehr alt, wie auch Burkert meint, aber großenteils nicht pythagoreischen Ursprungs. Vielmehr seien es teils Weisheitssprüche unbestimmter Herkunft, teils handle es sich um uralten Volksaberglauben, der in Pythagoreerkreisen in symbolischem Sinn umgedeutet wurde.

Burkert bemerkt: „Die modernen Kontroversen um Pythagoras und den Pythagoreismus sind im Grunde nur die Fortsetzung des alten Streits zwischen ‚Akusmatikern‘ und ‚Mathematikern‘.“

Andere Gelehrte wie B. L. van der Waerden vertreten eine Mittelposition. Sie weisen weder der einen noch der anderen Gruppe die Priorität und alleinige Authentizität zu, sondern meinen, die Unterscheidung zwischen Mathematikern und Akusmatikern gehe auf unterschiedliche Bestrebungen zurück, die schon zu Pythagoras’ Lebzeiten in der Schule bestanden. Nach dem Tod des Schulgründers habe sich daraus ein Gegensatz entwickelt, der zur Spaltung der Schule führte.

Späte Quellen schildern die Pythagoreer – gemeint sind offenbar Akusmatiker – als eine verschworene Gemeinschaft von Jüngern, die ihren Meister als göttliches oder jedenfalls übermenschliches Wesen verehrten und blind an seine Unfehlbarkeit glaubten. Dieser Glaube soll dazu geführt haben, dass sie jede Frage durch Berufung auf eine (angebliche) mündliche Äußerung des Pythagoras entschieden. Allein der „Autoritätsbeweis“ durch die Versicherung „Er selbst [Pythagoras] hat es gesagt“ habe bei ihnen gegolten. In diesen Zusammenhang gehören auch Berichte, wonach Pythagoras Bewerber, die in seine Schule eintreten wollten, zunächst physiognomisch prüfte und ihnen dann eine lange (nach manchen Angaben fünfjährige) Schweigezeit auferlegte, nach deren erfolgreicher Absolvierung sie in die Gemeinschaft aufgenommen wurden.

Der prominenteste unter den forschenden Pythagoreern der Frühzeit war der Mathematiker und Musiktheoretiker Hippasos von Metapont. Er soll Klangexperimente durchgeführt haben, um das Verhältnis der Konsonanzen zu messbaren physikalischen Größen zu bestimmen. Bekannt ist er vor allem durch die früher herrschende Ansicht, er habe eine „Grundlagenkrise“ des Pythagoreismus ausgelöst, indem er die Inkommensurabilität entdeckte und damit die Behauptung widerlegte, alle Phänomene seien als Erscheinungsformen ganzzahliger Zahlverhältnisse erklärbar. Angeblich schlossen die Pythagoreer Hippasos daraufhin aus und betrachteten seinen Tod durch Ertrinken im Meer als göttliche Strafe für den „Geheimnisverrat“. Die Entdeckung der Inkommensurabilität mag eine historische Tatsache sein, aber die Vermutung, dies habe zu einer Grundlagenkrise geführt, wird in der neueren Forschung abgelehnt.

Zu den frühen Pythagoreern gehörten ferner:

In der Frühzeit sollen auch Frauen in der Bewegung aktiv gewesen sein. Insbesondere wird in den Quellen oft der Name von Pythagoras’ Gattin Theano genannt. Ihr wurden später zahlreiche Aussprüche und Schriften zugeschrieben, die vor allem von Tugend und Frömmigkeit handelten, sowie sieben Briefe, die erhalten sind.

Der Philosoph Parmenides soll Schüler eines Pythagoreers namens Ameinias gewesen sein; pythagoreischer Einfluss auf ihn wird von der heutigen Forschung angenommen, das Ausmaß ist aber unklar. Der Philosoph Empedokles, der Pythagoras bewunderte, war zwar kein Pythagoreer im engeren Sinne, stand aber der pythagoreischen Gedankenwelt sehr nahe.

Trotz der ungeheueren persönlichen Autorität des Pythagoras war der frühe ebenso wie auch der spätere Pythagoreismus kein verbindlich fixiertes, in sich geschlossenes und detailliert ausgearbeitetes dogmatisches Lehrgebäude. Eher handelte es sich um eine bestimmte Art der Weltbetrachtung, die für unterschiedliche Ansätze Spielraum ließ. Alle Pythagoreer teilten die Grundüberzeugung, die gesamte erkennbare Welt sei eine auf der Basis bestimmter Zahlen und Zahlenverhältnisse aufgebaute, prinzipiell harmonisch gestaltete Einheit. Diese Gesetzmäßigkeit bestimme alle Bereiche der Wirklichkeit gleichermaßen. Die Kenntnis der maßgeblichen Zahlenverhältnisse betrachteten sie daher als den Schlüssel zum Verständnis von allem und als Voraussetzung für eine gute, naturgemäße Lebensführung. Ihr Ziel war es, die unterschiedlichen und gegensätzlichen Kräfte durch Ausgewogenheit zu einem harmonischen Einklang zu bringen, sowohl im menschlichen Körper als auch in der Familie und im Staat. Dabei wollten sie das, was sie als Maß, Ordnung und Harmonie zu erkennen meinten, überall in der Natur finden und in ihrem eigenen Leben wahren. Sie gingen also von einer ganzheitlichen Deutung des Kosmos aus. Das, was in ihm in Unordnung geraten war, wollten sie in die natürliche Ordnung zurückbringen. Im Sinne dieses Weltbildes hielten sie alle beseelten Wesen für miteinander verwandt und leiteten daraus ein Gebot der Rücksichtnahme ab. Hinsichtlich der Einzelheiten gingen ihre Meinungen aber oft weit auseinander.

Die Lehre von der Unsterblichkeit der Seele gehört zum ältesten Bestand der frühpythagoreischen Philosophie. Sie ist eine der wichtigsten Gemeinsamkeiten von Pythagoreismus und Platonismus, die sich im Lauf ihrer Entwicklung gegenseitig beeinflussten und bei manchen Philosophen miteinander verschmolzen. Die Pythagoreer waren ebenso wie die Platoniker von der Seelenwanderung überzeugt. Dabei nahmen sie keinen Wesensunterschied zwischen menschlichen und tierischen Seelen an. Diese Idee setzte die Unsterblichkeit der Seele voraus. Da jedoch die Pythagoreer in den harmonischen Zahlenverhältnissen das Fundament der Weltordnung sahen, gab es bei ihnen auch die Vorstellung, dass die Seele eine Harmonie sei, nämlich das harmonische Gleichgewicht der den Körper bestimmenden Kräfte. Das ist mit der Unsterblichkeitsidee schwer zu vereinbaren. In diesem Widerspruch zeigt sich die Unfertigkeit des in Entwicklung begriffenen pythagoreischen Philosophierens. Platon setzte sich in seinem Dialog "Phaidon" mit der Deutung der Seele als Harmonie auseinander und bemühte sich sie zu widerlegen.

Ein weiterer Bereich, in dem innerhalb der pythagoreischen Bewegung offenbar disparate Ideen vertreten und nicht zu einem stimmigen Ganzen verbunden wurden, war die Frage nach der Bestimmung und Zukunft der Seele. Ein wesentlicher, allerdings für die sehr quellenarme Frühzeit nicht eindeutig bezeugter Bestandteil des Pythagoreismus war die religiöse Überzeugung, dass die menschliche Seele göttlicher Herkunft und Natur sei. Daraus folgte (wie bei den Orphikern und den Platonikern), dass es die Aufgabe und Bestimmung der Seele sei, aus dem Diesseits in ihre jenseitige Heimat zurückzukehren. Darauf sollte sie sich durch Schulung und rechte Lebensführung vorbereiten. Ihr wurde zugetraut, ihre göttlichen Fähigkeiten und Möglichkeiten zurückzuerlangen. Der Umstand, dass Pythagoras von vielen seiner Anhänger als gottähnliches Wesen betrachtet wurde, zeigt, dass ein solches Ziel grundsätzlich erreichbar schien. Mit diesem Erlösungsstreben schwer vereinbar war jedoch ein anderes Konzept, welches von einem ewigen, unabänderlichen Kreislauf des Weltgeschehens ausging. Die Annahme, dass ein einheitlicher Kosmos immer und überall von den gleichen mathematischen Gegebenheiten bestimmt sei, und die zyklische Natur der gleichmäßigen Bewegungen der Himmelskörper führten dazu, auch das Schicksal der Menschheit als vorbestimmt und zyklisch aufzufassen. Daher herrschte zumindest bei einem Teil der Pythagoreer ein astrologischer Fatalismus, also die Vorstellung von der zwangsläufigen ewigen Wiederkunft aller irdischen Verhältnisse entsprechend den Gestirnbewegungen. Dieser Idee zufolge beginnt die Weltgeschichte von neuem als exakte Wiederholung, sobald alle Planeten nach Ablauf einer langen kosmischen Periode, des „Großen Jahres“, ihre Ausgangsstellung wieder erreicht haben.

Als religiöse Erlösungslehre präsentierte sich der Pythagoreismus insbesondere in einem sehr populären antiken Gedicht eines unbekannten Autors, den „Goldenen Versen“. Dort wird dem Menschen, der sich an die philosophischen Lebensregeln hält und zur Erkenntnis der Weltgesetze vorgedrungen ist, in Aussicht gestellt, dass seine Seele dem Leiden und der Sterblichkeit entrinnen und in die Daseinsweise unsterblicher Götter überwechseln könne. Dies hatte schon im 5. Jahrhundert Empedokles als Ziel formuliert.

Ebenso wie viele andere philosophische Richtungen traten die Pythagoreer für Beherrschung der Begierden und damit auch für eine einfache Lebensweise und frugale Ernährung ein. Dass sie jeden Luxus – insbesondere den Kleiderluxus – verwarfen, ergab sich aus ihrer allgemeinen Forderung, das rechte Maß zu wahren und so die Harmonie zu verwirklichen.

Ein Kernbestandteil des ursprünglichen Pythagoreismus war der Vegetarismus. Er wurde als „Enthaltung vom Beseelten“ bezeichnet. Diese Bezeichnung weist auf die ethische und religiöse Wurzel des pythagoreischen Vegetarismus hin. Er hing mit der Überzeugung zusammen, dass die Seelen der Menschen und diejenigen der Tiere nicht essentiell verschieden seien und man den Tieren somit Rücksichtnahme schulde. Verschiedene Legenden, nach denen Pythagoras sich Tieren verständlich machen konnte, zeugen von einer besonderen Nähe der Pythagoreer zur Tierwelt. Daher wurden neben der Fleischnahrung auch die Tieropfer verworfen. Damit waren aber soziale Probleme verbunden, denn die Teilnahme an den traditionellen Opfern und den anschließenden Opfermahlzeiten gehörte zu den wichtigsten gemeinschaftsstiftenden Bräuchen, und die politisch aktiven Pythagoreer mussten auf ihr Ansehen in der Bürgerschaft Wert legen. Daher gab es anscheinend kein für alle verbindliches Gebot, und nur ein Teil der Pythagoreer lebte vegetarisch.

Ein strenges Tabu richtete sich gegen den Verzehr von Bohnen. Der ursprüngliche Grund des Bohnenverbots war schon in der Antike unbekannt, es wurde darüber gerätselt. Gelegentlich wurde ein gesundheitlicher Grund angedeutet, aber meist ging man davon aus, dass es ein religiöses Tabu war. Es wurde sogar angenommen, das Verbot sei so umfassend gewesen, dass es auch bloße Berührung einer Bohnenpflanze absolut untersagte. Daher entstanden Legenden, wonach vor Verfolgern fliehende Pythagoreer (bzw. Pythagoras selbst) eher den Tod in Kauf nahmen, als ein Bohnenfeld zu durchqueren. Der tatsächliche Grund für das Bohnentabu ist bis heute nicht geklärt. Die Möglichkeit eines Zusammenhangs mit dem Favismus, einer erblichen Enzymkrankheit, bei welcher der Genuss von Ackerbohnen "(Vicia faba)" gesundheitsgefährlich ist, wurde mehrfach als Erklärung erwogen. Diese Hypothese findet in den Quellen keine konkrete Stütze und ist daher spekulativ.

Eine wichtige Rolle spielte im Pythagoreismus das Konzept der Freundschaft "(philía)". Dieser Begriff wurde gegenüber seiner normalen Bedeutung stark ausgeweitet. Da die Pythagoreer den Kosmos als Einheit von zusammengehörigen und harmonisch zusammenwirkenden Bestandteilen auffassten, gingen sie von einer naturgegebenen Freundschaft sämtlicher Lebewesen (einschließlich der Götter) untereinander aus. Dieses Ideal universaler Freundschaft und Harmonie in der Welt erinnert an den Mythos vom paradiesischen Goldenen Zeitalter. Das Ziel war, die so verstandene Verbundenheit aller zu erkennen und im eigenen Leben umzusetzen. Damit war aber – wie die Beteiligung am Krieg gegen Sybaris schon zu Lebzeiten des Pythagoras zeigt – kein absoluter Gewaltverzicht im Sinne des Pazifismus verbunden.

Insbesondere praktizierten die Pythagoreer die Freundschaft untereinander. Manche von ihnen verstanden darunter eine unbedingte Loyalität nicht nur zu ihren persönlichen Freunden, sondern zu jedem Pythagoreer. Über die Freundestreue sind einige Anekdoten überliefert. Die berühmteste ist die Geschichte von Damon und Phintias, die von Friedrich Schiller für seine Ballade "Die Bürgschaft" verwertet wurde. Es wird erzählt, dass der Pythagoreer Phintias wegen eines Komplotts gegen den Tyrannen Dionysios zum Tode verurteilt wurde, aber die Erlaubnis erhielt, vor der Hinrichtung seine persönlichen Angelegenheiten in Freiheit zu regeln, da sich sein Freund Damon als Geisel für seine Rückkehr verbürgte. Phintias kehrte rechtzeitig zurück; anderenfalls wäre Damon an seiner Stelle hingerichtet worden. Dies beeindruckte den Tyrannen stark, worauf er Phintias begnadigte und selbst vergeblich um Aufnahme in den Freundschaftsbund bat. Nach einer Version hatte Dionysios den ganzen Vorfall nur zum Schein arrangiert, um die legendäre Treue der Pythagoreer auf die Probe zu stellen, nach einer anderen Version handelte es sich um eine wirkliche Verschwörung.

Bekannt war in der Antike der Grundsatz der Pythagoreer, dass der Besitz der Freunde gemeinsam sei "(koiná ta tōn phílōn)". Dies ist aber nicht im Sinne einer „kommunistischen“ Gütergemeinschaft zu verstehen; eine solche wurde – wenn überhaupt – nur von wenigen praktiziert. Gemeint war, dass die Pythagoreer einander in materiellen Notlagen spontan und großzügig unterstützten.

Zahlen und Zahlenverhältnisse haben in der pythagoreischen Lehre von Anfang an eine zentrale Rolle gespielt. Dies ist ein Merkmal, das den Pythagoreismus von anderen Ansätzen unterscheidet. Ob das aber bedeutet, dass Pythagoras schon Mathematik getrieben hat, ist strittig. Manche Forscher (insbesondere Walter Burkert) haben die Ansicht vertreten, er habe sich nur mit Zahlensymbolik befasst, wissenschaftliches Denken sei ihm fremd gewesen, und erst um die Mitte des 5. Jahrhunderts habe sich Hippasos als erster Pythagoreer mathematischen Studien zugewandt. Die Gegenposition von Leonid Zhmud lautet, die frühen Pythagoreer seien Mathematiker gewesen und die Zahlenspekulation sei erst spät hinzugekommen und nur von vereinzelten Pythagoreern betrieben worden.

Der Grundgedanke der Zahlenspekulation wird oft in dem Kernsatz „Alles ist Zahl“ zusammengefasst. In damaliger Ausdrucksweise besagt das, dass die Zahl für die Pythagoreer die archē, das konstituierende Urprinzip der Welt gewesen sei. Damit fiele der Zahl diejenige Rolle zu, die Thales dem Wasser und Anaximenes der Luft zugewiesen hatte. Diese Auffassung ist aber im frühen Pythagoreismus nicht belegt. Aristoteles schreibt sie „den Pythagoreern“ zu, ohne Namen zu nennen. Er kritisiert sie und unterstellt dabei, die Pythagoreer hätten unter den Zahlen etwas Stoffliches verstanden.

In der zweiten Hälfte des 5. Jahrhunderts schrieb der Pythagoreer Philolaos, alles für uns Erkennbare sei notwendigerweise mit einer Zahl verknüpft, denn das sei eine Voraussetzung für gedankliches Erfassen. Seine Feststellung bezieht sich aber nur auf den menschlichen Erkenntnisprozess. Sie besagt nicht im Sinne einer Ontologie der Zahl, dass alle Dinge aus Zahlen bestehen oder hervorgehen. Die Auffassung, dass die Zahlen selbst die Dinge seien, wird oft als typisch pythagoreisch bezeichnet. Dies ist jedoch nur Aristoteles’ möglicherweise irriges Verständnis der pythagoreischen Lehre. Das für Philolaos Wesentliche war der Unterschied zwischen dem nach Zahl, Größe und Form Begrenzten und dem Unbegrenzten, das er für prinzipiell unerforschbar hielt, und das Zusammenspiel dieser beiden Faktoren.

Ausgangspunkt der konkreten Zahlenspekulation war der Gegensatz von geraden und ungeraden Zahlen, wobei die ungeraden als begrenzt (und damit höherrangig) und – wie im chinesischen Yin und Yang – als männlich bezeichnet wurden und die geraden als unbegrenzt und weiblich. Die als Prinzip der Einheit aufgefasste Eins galt als der Ursprung, aus dem alle Zahlen hervorgehen (und infolgedessen die ganze Natur); so gesehen war sie selbst eigentlich keine Zahl, sondern stand jenseits der Zahlenwelt, obwohl sie rechnerisch als Zahl wie alle anderen erscheint. So konnte die Eins paradoxerweise als gerade und ungerade zugleich bezeichnet werden, was rechnerisch nicht zutrifft. Die Zahlen stellte man mit Zählsteinen dar, und mit den ebenen geometrischen Figuren, die mit solchen Steinen gelegt werden können (beispielsweise einem gleichseitigen Dreieck), wurden die den Zahlen zugewiesenen Eigenschaften demonstriert. Große Bedeutung legte man dabei der Tetraktys („Vierheit“) bei, der Gesamtheit der Zahlen 1, 2, 3 und 4, deren Summe die 10 ergibt, die bei Griechen und „Barbaren“ (Nichtgriechen) gleichermaßen als Grundzahl des Dezimalsystems diente. Die Tetraktys und die „vollkommene“ Zehn betrachtete man als für die Weltordnung grundlegend.

Einzelne mathematische Erkenntnisse wurden in der Antike – zu Recht oder zu Unrecht – den Pythagoreern oder einem bestimmten Pythagoreer zugeschrieben. Pythagoras soll einen Beweis für den nach ihm benannten Satz des Pythagoras über das rechtwinklige Dreieck gefunden haben. Hippasos von Metapont schrieb man die Konstruktion des einer Kugel einbeschriebenen Dodekaeders und die Entdeckung der Inkommensurabilität zu. Eine nicht genau bekannte Rolle spielten Pythagoreer bei der Entwicklung der Lehre von den drei Mitteln (arithmetisches, geometrisches und harmonisches Mittel). Ferner sollen sie unter anderem den Satz über die Winkelsumme im Dreieck bewiesen haben. Womöglich stammen große Teile von Euklids "Elementen" – sowohl der arithmetischen als auch der geometrischen Bücher – aus verlorener pythagoreischer Literatur; dazu gehörte die Theorie der Flächenanlegung.

In der Astronomie vertraten die Pythagoreer keinen einheitlichen Standpunkt. Das älteste Modell, das wir kennen, ist dasjenige des Philolaos aus der zweiten Hälfte des 5. Jahrhunderts. Es nimmt ein Zentralfeuer an, das den Mittelpunkt des Universums bildet und um das die Himmelskörper einschließlich der Erde kreisen. Für uns ist es unsichtbar, da die bewohnten Gegenden der Erde auf der ihm stets abgewandten Seite liegen. Um das Zentralfeuer kreist auf der innersten Bahn die Gegenerde, die für uns ebenfalls unsichtbar ist, da sie vom Zentralfeuer verdeckt wird. Darauf folgen (von innen nach außen) die Erdbahn und die Bahnen von Mond, Sonne und fünf Planeten (Merkur, Venus, Mars, Jupiter und Saturn). Umschlossen ist das Ganze von einer kugelförmigen Schale, auf der sich die Fixsterne befinden. Aristoteles kritisierte dieses System, da es nicht von den Erscheinungen, sondern von vorgefassten Ansichten ausgehe; die Gegenerde sei nur eingeführt worden, um die Zahl der bewegten Körper am Himmel auf zehn zu bringen, da diese Zahl als vollkommene galt.

Aristoteles erwähnt, dass „einige“ Pythagoreer einen Kometen zu den Planeten zählten. Das widerspricht der Zehnzahl bei Philolaos. Auch über die Milchstraße hatten die Pythagoreer keine einheitliche Meinung. Daraus ist zu ersehen, dass die frühen Pythagoreer kein gemeinsames, für alle verbindliches Kosmosmodell hatten. Manche Forscher nehmen an, dass es vor Philolaos ein völlig anderes, nämlich geozentrisches pythagoreisches Modell gab. Es sah vor, dass sich die kugelförmige Erde im Zentrum des Kosmos befindet und vom Mond, der Sonne und den damals bekannten fünf Planeten umkreist wird.

Zu den wichtigsten Annahmen der Pythagoreer gehörte die Idee der Sphärenharmonie oder – wie die Bezeichnung in den ältesten Quellen lautet – „Himmelsharmonie“. Man ging davon aus, dass bei der Kreisbewegung der Himmelskörper ebenso wie bei Bewegungen irdischer Objekte Geräusche entstehen. Wegen der Gleichförmigkeit der Bewegung konnte dies für jeden Himmelskörper immer nur ein konstanter Ton sein. Die Gesamtheit dieser Töne, deren Höhe von den unterschiedlichen Geschwindigkeiten und den Abständen der Himmelskörper abhing, sollte einen kosmischen Klang ergeben. Diesen betrachtete man als für uns unhörbar, da er ununterbrochen erklinge und uns nur durch sein Gegenteil, durch einen Gegensatz zwischen Klang und Stille zu Bewusstsein käme. Allerdings soll Pythagoras laut einer Legende als einziger Mensch imstande gewesen sein, die Himmelsharmonie zu hören.

Da die Töne der Himmelskörper nur als gleichzeitig, nicht als nacheinander erklingend gedacht werden konnten, musste als Ergebnis ihres Zusammenklingens ein ebenfalls stets unveränderter Klang angenommen werden. Daher ist der populäre Begriff „Sphärenmusik“ sicher unpassend. Dass der Zusammenklang harmonisch ist, ergibt sich in diesem Modell aus der Annahme, dass die Entfernungen der kreisenden Himmelskörper vom Zentrum und ihre bei größerer Entfernung entsprechend höheren Geschwindigkeiten eine bestimmte arithmetische Proportion aufweisen, die dies ermöglicht.

Die Musik war derjenige Bereich, in dem die Grundidee einer auf Zahlenverhältnissen beruhenden Harmonie am einfachsten demonstrierbar war. Den musikalischen Gesetzmäßigkeiten galt die besondere Aufmerksamkeit der Pythagoreer. Auf diesem Gebiet haben sie offenbar auch experimentiert. Pythagoras wurde in der Antike allgemein als Begründer der mathematischen Analyse der Musik angesehen. Platon bezeichnete die Pythagoreer als Urheber der musikalischen Zahlenlehre, sein Schüler Xenokrates schrieb die entscheidende Entdeckung Pythagoras selbst zu. Dabei ging es um die Darstellung der harmonischen Intervalle durch einfache Zahlenverhältnisse. Das konnte durch Streckenmessung veranschaulicht werden, da die Tonhöhe von der Länge einer schwingenden Saite abhängt. Für solche Versuche eignete sich das Monochord mit verstellbarem Steg. Einen anderen, ebenfalls tauglichen Weg zur Quantifizierung fand Hippasos, der die Töne bronzener Scheiben von unterschiedlicher Dicke bei gleichem Durchmesser untersuchte.

Sicher unhistorisch ist allerdings die Legende von Pythagoras in der Schmiede, der zufolge Pythagoras zufällig an einer Schmiede vorbeiging und, als er die unterschiedlichen Klänge der verschieden schweren Hämmer hörte, sich von dieser Beobachtung dazu anregen ließ, mit an Saiten aufgehängten Metallgewichten zu experimentieren.

Platon, der eine rein spekulative, aus allgemeinen Prinzipien abgeleitete Musiktheorie forderte und die Sinneserfahrung durch das Gehör für unzureichend hielt, kritisierte die Pythagoreer wegen ihres empirischen Vorgehens.

Die Musik eignete sich zur Abstützung der These einer universalen Harmonie und der Verflochtenheit aller Teile des Kosmos. Durch die Idee der klingenden Himmelsharmonie war sie mit der Astronomie verbunden, durch die Messbarkeit der Tonhöhen mit der Mathematik, durch ihre Wirkung auf das Gemüt mit der Seelenkunde, der ethischen Erziehung und der Heilkunst. Die Pythagoreer befassten sich mit den unterschiedlichen Wirkungen verschiedener Instrumente und Tonarten auf das menschliche Gemüt. Den Legenden zufolge setzte Pythagoras ausgewählte Musik gezielt zur Beeinflussung unerwünschter Affekte und zu Heilzwecken ein, betrieb also eine Art Musiktherapie.

Von den Pythagoreern der zweiten Hälfte des 5. Jahrhunderts scheint der Naturphilosoph Philolaos einer der prominentesten gewesen zu sein. Er gehörte anscheinend zu denjenigen, die wegen der politischen Verfolgung in Italien nach Griechenland gingen. Jedenfalls lehrte er zumindest zeitweilig in Theben. Seine Kosmologie mit der Annahme eines Zentralfeuers in der Mitte des Universums unterschied sich stark von der zuvor dominierenden. Den Mond hielt er für bewohnt, die Sonne für glasartig (also kein eigenes Licht ausstrahlend, sondern fremdes Licht wie eine Linse sammelnd). Seine Ansichten sind nur aus Fragmenten seines Buchs bekannt, deren Echtheit zum Teil umstritten ist.

Im 4. Jahrhundert v. Chr. war der bedeutendste Pythagoreer der mit Platon befreundete Archytas von Tarent. Er war sowohl ein erfolgreicher Staatsmann und Heerführer seiner Heimatstadt als auch Philosoph, Mathematiker, Physiker, Musiktheoretiker und ein hervorragender Ingenieur. Das pythagoreische Konzept einer mathematisch fassbaren Harmonie wandte er auf die Politik an, indem er für einen kalkulierten Ausgleich zwischen den sozialen Schichten eintrat. Die Eintracht der Bürger führte er auf eine angemessene, von allen als gerecht empfundene Besitzverteilung zurück.

Platon befasste sich intensiv mit der pythagoreischen Philosophie. Umstritten ist die Frage, inwieweit die Ansichten des Philolaos und des Archytas sein Bild von ihr prägten. Nach seinem Tod dauerte in der Platonischen Akademie das Interesse am Pythagoreismus an, und unter den Platonikern bestand die Neigung, Anregungen aus dieser Tradition aufzunehmen und Platon in entsprechendem Sinne zu deuten.

Aristoteles verfasste eine Schrift über die Pythagoreer, von der nur Fragmente erhalten geblieben sind, und setzte sich auch sonst kritisch mit dem Pythagoreismus auseinander. Unter anderem argumentierte er gegen die Himmelsharmonie (Sphärenharmonie).

Im 4. Jahrhundert lebten in Griechenland zahlreiche aus Italien geflüchtete Anhänger des Pythagoras. Man unterschied nun zwischen „Pythagoreern“ und „Pythagoristen“ (von "Pythagoristḗs" „die pythagoreische Lebensweise Befolgender“). Die letzteren waren ein beliebtes Ziel des Spotts der Komödiendichter, da sie bettelten und asketisch lebten. Besonders ihre äußerst genügsame Ernährungsweise wurde in Komödien aufs Korn genommen. Sie wurden als schmutzige Sonderlinge dargestellt.

Daneben gab es aber unter den aus Italien emigrierten Pythagoreern auch Gelehrte, die sich Respekt zu verschaffen wussten. Zu ihnen gehörte Lysis. Er wurde in Theben Lehrer des später berühmten Staatsmanns und Feldherrn Epameinondas; auf diesem Weg hat möglicherweise der Pythagoreismus ein letztes Mal auf die Politik Einfluss genommen.

Zu den im späten 5. und im 4. Jahrhundert tätigen Pythagoreern gehörten ferner:

Bei den Römern stand Pythagoras in hohem Ansehen. Er wurde als Lehrer des zweiten Königs von Rom, Numa Pompilius, bezeichnet, was allerdings chronologisch unmöglich ist. Im 1. Jahrhundert v. Chr. bemühte sich anscheinend der mit Cicero befreundete Gelehrte und Senator Nigidius Figulus um eine Erneuerung des Pythagoreismus. Da eine kontinuierliche Tradition nicht mehr bestand, war dies ein Neuanfang. Daher pflegt man Nigidius als den ersten Neupythagoreer zu bezeichnen; allerdings ist nicht klar, ob seine tatsächlichen Ansichten und Aktivitäten diese Bezeichnung rechtfertigen. Der Neupythagoreismus dauerte bis in die Spätantike fort, doch gab es keinen kontinuierlichen Schulbetrieb, sondern nur einzelne pythagoreisch gesinnte Philosophen und Gelehrte. Es handelte sich beim Neupythagoreismus nicht um eine in sich geschlossene neue, von älteren Richtungen inhaltlich klar abgrenzbare Lehre.

Starkes Interesse an pythagoreischen Ideen zeigte Marcus Terentius Varro, der berühmteste römische Universalgelehrte. Er wurde gemäß seiner testamentarischen Verfügung „nach pythagoreischer Sitte“ beigesetzt.

In der von Quintus Sextius im 1. Jahrhundert v. Chr. in Rom gegründeten Philosophenschule der Sextier wurden neben stoischen auch neupythagoreische Lehren, darunter der Vegetarismus, vertreten. Dieser (allerdings kurzlebigen) Schule gehörte Sotion, der Lehrer Senecas, an. Von den Sextiern übernahm Seneca die pythagoreische Übung der Rekapitulation des Tages am Abend, mit der man für sich Bilanz zog. Dazu gehörte eine Selbstbefragung mit Fragen wie: „Welches deiner (charakterlichen) Übel hast du heute geheilt? Welchem Laster hast du widerstanden? In welcher Hinsicht bist du besser (geworden)?“

Der Dichter Ovid gab im 15. Buch seiner "Metamorphosen" einem fiktiven Lehrvortrag des Pythagoras breiten Raum und trug damit zur Verbreitung von pythagoreischem Gedankengut bei, doch gibt es keinen Beleg für die Annahme, dass er selbst Neupythagoreer war.

1917 wurde in Rom in der Nähe der Porta Maggiore ein unterirdisches Bauwerk in Form einer Basilika aus der Zeit des Kaisers Claudius (41–54) entdeckt. Es sollte offenbar als Versammlungsraum für einen religiösen Zweck dienen, wurde aber schon bald nach dem Ende der Bauarbeiten geschlossen. Der Historiker und Archäologe Jérôme Carcopino hat eine Reihe von Indizien gesammelt, die dafür sprechen, dass die Erbauer Neupythagoreer waren. Dazu gehört unter anderem die Ausschmückung von Decken und Wänden mit Darstellungen von Szenen aus der Mythologie, die dem Betrachter den als Erlösung aufgefassten Tod und das nachtodliche Schicksal der Seele vor Augen führen.

Der bekannteste Neupythagoreer der römischen Kaiserzeit war Apollonios von Tyana (1. Jahrhundert n. Chr.). Von seiner Philosophie ist wenig Zuverlässiges überliefert. Er orientierte sich offenbar in seiner philosophischen Lebensführung stark am Vorbild des Pythagoras (bzw. an dem damals dominierenden Pythagorasbild) und beeindruckte damit seine Zeitgenossen und die Nachwelt nachhaltig.

Die übrigen Neupythagoreer waren zugleich Platoniker bzw. Neuplatoniker. Im Neupythagoreismus waren frühpythagoreische Ideen mit Legenden aus der späteren pythagoreischen Tradition und (neu)platonischen Lehren verschmolzen. Moderatos von Gades (1. Jahrhundert n. Chr.) betrachtete die Zahlenlehre als didaktisches Mittel zur Veranschaulichung von Erkenntnisgegenständen der geistigen Welt. Von Nikomachos von Gerasa (2. Jahrhundert) stammen eine Einführung in die Arithmetik (d. h. in die pythagoreische Zahlenlehre), die Schulbuch wurde und im Mittelalter in der lateinischen Fassung des Boethius sehr verbreitet war, und ein Handbuch der musikalischen Harmonik. Boethius ging in seiner für das Mittelalter maßgeblichen lateinischen Darstellung der Musiktheorie "(De institutione musica)" von den musikalischen Lehren des Nikomachos aus und behandelte auch die Sphärenharmonie. Außerdem verfasste Nikomachos eine Biographie des Pythagoras, die verloren ist. Im 2. Jahrhundert lebte auch der Platoniker Numenios von Apameia, der den Pythagoreismus mit der authentischen Lehre Platons gleichsetzte und auch aus Sokrates einen Pythagoreer machte; den späteren Platonikern warf er vor, von Platons pythagoreischer Philosophie abgewichen zu sein.

Der Neuplatoniker Porphyrios schrieb im 3. Jahrhundert eine Lebensbeschreibung des Pythagoras und zeigte sich besonders in seinem Eintreten für den Vegetarismus pythagoreisch beeinflusst. Weit stärker trat pythagoreisches Gedankengut bei dem etwas jüngeren Neuplatoniker Iamblichos von Chalkis in den Vordergrund. Er verfasste ein zehnbändiges Werk über die pythagoreische Lehre, von dem Teile erhalten sind, darunter insbesondere die Abhandlung „Vom pythagoreischen Leben“. Sein Pythagorasbild war von einer Fülle von legendenhaftem Stoff geprägt, den er zusammentrug. Sein Anliegen war insbesondere, die metaphysisch-religiöse und die ethische Seite des Pythagoreismus mit der Mathematik (worunter er primär die arithmetische und geometrische Symbolik verstand) zu verbinden und dieses Ganze als göttliche Weisheit darzustellen, die den Menschen durch Pythagoras geschenkt sei. Wie Numenios betrachtete er Platons Lehre nur als Ausgestaltung der pythagoreischen Philosophie.

Im 5. Jahrhundert schrieb der Neuplatoniker Hierokles von Alexandria einen Kommentar zu den „Goldenen Versen“. Er betrachtete dieses Gedicht als allgemeine Einführung in die Philosophie. Unter Philosophie verstand er einen Platonismus, den er mit Pythagoreismus gleichsetzte. Auch der Neuplatoniker Syrianos, ein Zeitgenosse des Hierokles, war überzeugt, dass Platonismus nichts anderes als Pythagoreismus sei.

Seit der Renaissance haben einzelne Naturphilosophen so stark pythagoreisches Gedankengut rezipiert und sich so nachdrücklich zur pythagoreischen Tradition bekannt, dass man sie als Pythagoreer bezeichnen kann. Ihnen ging und geht es darum, das Universum als einen nach mathematischen Regeln sinnvoll und ästhetisch durchstrukturierten Kosmos zu erweisen. Diese harmonische Ordnung soll in den Planetenbahnen ebenso wie in musikalischen Proportionen und in der Zahlensymbolik erkennbar sein. Die Gesetze der Harmonie werden als grundlegende Prinzipien betrachtet, die in der gesamten Natur auffindbar seien. Zu dieser Denkweise bekannten sich bedeutende Humanisten wie Giovanni Pico della Mirandola (1463–1494), der sich ausdrücklich als Pythagoreer bezeichnete, und Johannes Reuchlin (1455–1522). Einen Vorläufer hatten sie in dem spätmittelalterlichen Gelehrten Pietro d’Abano. Besonders eifrig bemühte sich der Astronom und Naturphilosoph Johannes Kepler (1571–1630), die Planetenbewegungen als Ausdruck einer vollkommenen Weltharmonie zu erweisen und astronomische Proportionen mit musikalischen zu verbinden.

Im 20. Jahrhundert knüpfte der Musikwissenschaftler Hans Kayser mit seiner „harmonikalen Grundlagenforschung“ an die pythagoreische Tradition an. Sein Schüler Rudolf Haase setzte seine Arbeit fort. Diese Bemühungen finden insbesondere in Kreisen der Esoterik Anklang. Da die Grundannahme einer kosmischen Harmonie, von der die modernen Pythagoreer ausgehen, den Charakter einer religiösen Überzeugung hat, finden ihre Forschungen in der Wissenschaft kaum Beachtung.

Werner Heisenberg wies in seinem erstmals 1937 veröffentlichten Aufsatz „Gedanken der antiken Naturphilosophie in der modernen Physik“ den Pythagoreern eine Pionierrolle bei der Entstehung der naturwissenschaftlichen Denkweise zu, welche darauf abzielt, die Ordnung in der Natur mathematisch zu fassen. Heisenberg schrieb, die „Entdeckung der mathematischen Bedingtheit der Harmonie“ durch die Pythagoreer beruhe auf „dem Gedanken an die sinngebende Kraft mathematischer Strukturen“, einem „Grundgedanken, den die exakte Naturwissenschaft unserer Zeit aus der Antike übernommen hat“; die moderne Naturwissenschaft sei „eine konsequente Durchführung des Programms der Pythagoreer“. Die Entdeckung der rationalen Zahlenverhältnisse, die der musikalischen Harmonie zugrunde liegen, gehört für Heisenberg „zu den stärksten Impulsen menschlicher Wissenschaft überhaupt“.

Die spanische Philosophin María Zambrano (1904–1991) sah im Pythagoreismus eine Ausrichtung des Denkens, welche die Wirklichkeit in Zahlenverhältnissen sucht und damit das Universum als „ein Gewebe aus Rhythmen, eine körperlose Harmonie“ betrachtet, worin die Dinge nicht in sich selbst bestehen, sondern nur durch ihre mathematischen und zeitlichen Beziehungen zueinander Phänomene in Erscheinung treten lassen. Den Gegenpol dazu bilde der Aristotelismus, für den die einzelnen Dinge als Substanzen in sich ruhen und damit eine eigene innere Wirklichkeit aufweisen. Der Aristotelismus habe zwar gesiegt, da er zunächst eine überlegene Erklärung der Natur und des Lebens anbieten konnte, aber die pythagoreische Haltung existiere als Alternative weiter und die moderne Physik der Relativität sei eine Rückkehr zu ihr.



Übersichtsdarstellung
Gesamtdarstellungen, Untersuchungen


Aufsatzsammlungen

Bibliographie


</doc>
<doc id="4164" url="https://de.wikipedia.org/wiki?curid=4164" title="Phasenübergang">
Phasenübergang

Ein Phasenübergang bzw. eine Phasenumwandlung oder Phasentransformation ist in der Thermodynamik die Umwandlung einer oder mehrerer Phasen eines Stoffes in andere Phasen. Eine grafische Darstellung der Stabilitätsbereiche der Phasen in Abhängigkeit von den Zustandsvariablen wie Druck, Temperatur, chemischer Zusammensetzung und magnetischer Feldstärke liefern Phasendiagramme. In diesen Diagrammen sind die Stabilitätsbereiche durch Phasengrenzlinien begrenzt, an denen die Phasenübergänge ablaufen.

Phasenübergänge können u. a. zwischen festen, flüssigen und gasförmigen Phasen auftreten. Für Phasenübergänge zwischen bestimmten Aggregatzuständen (also sogenannten Aggregatzustandsänderungen) gibt es spezielle Bezeichnungen:


In einigen Stoffsystemen verschwinden oberhalb eines kritischen Punktes, der durch eine kritische Temperatur und einen kritischen Druck gekennzeichnet ist, die Phasengrenzflächen zwischen flüssiger und gasförmiger Phase. Damit sind Flüssigkeit und Gas unter diesen Bedingungen nur noch eine Phase, die „überkritisch“ genannt wird. Somit kann es dort auch kein Verdampfen und Kondensieren mehr geben. Ebenso kann es in einigen Stoffsystemen einen Tripelpunkt geben, an dem sowohl eine feste, als auch eine flüssige und eine gasförmige Phase im Gleichgewicht miteinander stehen und dementsprechend alle sechs erstgenannten Formen des Phasenübergangs gleichzeitig ablaufen.

Andere Typen von Phasenübergängen werden im Weiteren erwähnt.

Grundsätzlich werden Phasenübergänge nach der "Ehrenfest-Klassifikation" (nach Paul Ehrenfest) in unterschiedliche Ordnungen eingeteilt. Dazu betrachtet man thermodynamische Größen wie Volumen, Enthalpie oder Entropie in Abhängigkeit von einer (oder mehreren) Variablen, meist der Temperatur. Das System wird durch ein thermodynamisches Potential G (Gibbs-Energie, freie Enthalpie) beschrieben. Bei einem Phasenübergang n-ter Ordnung ist G als Funktion zum Beispiel der Temperatur (oder des Drucks) betrachtet samt seinen ersten n-1 Ableitungen stetig, erst die n-te Ableitung ist unstetig. Genauer werden Phasenübergänge in der Landau-Theorie beschrieben, wo die Phasen durch Symmetriebrüche von geordneten zu ungeordneten Phasen und begleitenden Sprüngen makroskopischer Größen wie der Magnetisierung oder der Deformation eines Kristallgitters gekennzeichnet sind, den Ordnungsparametern.

Von besonderer Bedeutung ist die Unterscheidung in Phasenübergänge 1. Ordnung und solche höherer Ordnung ("Kontinuierliche Phasenübergänge"), speziell 2. Ordnung. In der "modernen Klassifikation" wird in der Tat nur diese Unterscheidung vorgenommen. Ein Beispiel für einen Phasenübergang erster Ordnung ist der Übergang einer Flüssigkeit wie Wasser zu einem Feststoff am Gefrierpunkt. Zur Überführung vom festen in den flüssigen Zustand muss zusätzlich Wärmeenergie (in Form von latenter Wärme) zugeführt werden, ohne dass es zu einer tatsächlichen Temperaturerhöhung kommt. Da es dabei zu einer Unstetigkeit in der Entropie (der 1. Ableitung der freien Enthalpie G nach der Temperatur) kommt, ist das Schmelzen von Eis ein Phasenübergang erster Ordnung. Ebenso macht das Volumen (1. Ableitung von G nach dem Druck p) einen Sprung an der Phasengrenze. Der Dichteunterschied der Phasen entspricht hier dem Ordnungsparameter in der Landautheorie. Ein Sprung im Ordnungsparameter ist typisch für Phasenübergänge erster Art.

Ein Beispiel für einen Phasenübergang 2. Ordnung ist der Übergang von der ferromagnetischen zur paramagnetischen Phase bei der Curie-Temperatur in einem Ferromagneten. Der Ordnungsparameter ist die Magnetisierung, die am Phasenübergang stetig gegen Null geht, ohne dass dabei zusätzlich latente Wärme auftritt. Es tritt aber ein Sprung in der zweiten Ableitung der freien Enthalpie nach der Temperatur auf (Wärmekapazität). Dieses Verhalten kennzeichnet einen kontinuierlichen Phasenübergang oder auch einen Phasenübergang 2. Ordnung. Typisch ist hier ein stetiger Übergang im Ordnungsparameter. Ein weiteres Beispiel für einen Phasenübergang zweiter Ordnung ist der Übergang von Normalmetall zu Supraleiter.

In der folgenden Abbildung sind die erwähnten Phasenübergänge 1. und 2. Ordnung nach Ehrenfest dargestellt. Die Abbildung zeigt bei konstantem Druck die freie Enthalpie G, das Volumen V, die Enthalpie H, die Entropie S und die Wärmekapazität C in Abhängigkeit von der Temperatur. In der obersten Reihe sind die Parameter ohne Phasenübergang gezeigt, in der Mitte ein Phasenübergang erster Ordnung und in der unteren Reihe ein Phasenübergang zweiter Ordnung. Die Phasenumwandlung findet jeweils bei der kritischen Temperatur T statt. In der Abbildung sind die Kurven der freien Enthalpie der jeweiligen Phase eingezeichnet und grau in der jeweils anderen Phase fortgesetzt; sie können dort Unterkühlungseffekte oder Metastabilität vorhersagen.

Neben dieser grundsätzlichen Einteilung gibt es noch eine Reihe weiterer Unterscheidungen in speziellen Anwendungsgebieten.

Nach der "strukturellen Klassifikation" unterscheidet man in der Mineralogie zwischen diskontinuierlichen (= rekonstruktiven), martensitischen und kontinuierlichen Phasenübergängen. Diskontinuierliche Phasenübergänge sind durch den Bruch chemischer Bindungen charakterisiert. Ein Beispiel ist die Umwandlung von Graphit in Diamant. Bei martensitischen Phasenübergängen wird das Kristallgitter geschert. Ein Beispiel ist die Umwandlung von γ- zu α-Eisen. Martensitische Phasenübergänge werden nochmals in athermale und isothermale Phasenübergänge gegliedert. Im Unterschied zu ersteren ist der Umwandlungsgrad bei letzteren zeitabhängig. Kontinuierliche Phasenübergänge sind nur mit einer Ordnung der Kristallstruktur verbunden. Man unterscheidet zwei Subtypen: Displazive und Ordnungs-Unordnungs-Phasenübergänge. Bei ersterem kommt es zu einer Verschiebung oder Rotation der Atompositionen (zum Beispiel bei der Umwandlung von Hochquarz in Tiefquarz), bei letzteren zu einer Ordnung mehrerer auf verschiedene Atompositionen statistisch verteilter Atome, sodass jede Position nur noch mit einer Atomsorte besetzt ist. In beiden Fällen kann es zum Auftreten großräumiger Periodizitäten kommen, welche die Gitterstruktur überlagern. Man bezeichnet diese als inkommensurable Strukturen.

Die "kinetische Klassifikation" unterteilt Phasenübergänge nach ihrer Reaktionsgeschwindigkeit in Phasenübergänge nullter Ordnung, bei denen die Reaktionsgeschwindigkeit konstant ist, Phasenübergänge erster Ordnung, bei denen sie von der Konzentration der Ausgangsphase abhängt und Phasenübergängen zweiter (dritter) Ordnung, bei denen sie von den Konzentrationen von zwei (drei) Ausgangssubstanzen abhängt.

"Strömungsdynamisch" wird unterschieden bei Geschwindigkeits-Übergängen, wo sich Strömungseigenschaften schlagartig und massiv ändern, zum Beispiel die Änderung wichtiger Werte wie Widerstand und Auftrieb bei Gasen und Flüssigkeiten. Ein wichtiger Bereich ist der kritische Übergang von "unterkritisch" zu "überkritisch".

Phasenübergänge sind oft mit der Änderung bestimmter Materialeigenschaften verbunden, zum Beispiel:


Die Theorie kontinuierlicher Phasenübergänge geht von einem "Ordnungsparameter" aus (zum Beispiel der Magnetisierung bei der Umwandlung eines Ferromagneten in einen Paramagneten). Bei kontinuierlichen Phasenübergängen geht der Ordnungsparameter bei Annäherung an den Umwandlungspunkt kontinuierlich gegen Null (dagegen springt er an einem Phasenübergang 1. Ordnung) und die "Korrelationslänge" divergiert (bei einer Umwandlung 1. Ordnung bleibt sie endlich). Es lassen sich sehr unterschiedliche Arten von kontinuierlichen Phasenübergängen in "Universalitätsklassen" zusammenfassen, was letztlich erneut auf die Divergenz der Korrelationslänge zurückzuführen ist. Diese Klassen können durch einige wenige Parameter charakterisiert werden. Beispielsweise verschwindet der Ordnungsparameter in der Nähe des kritischen Punktes, z. B. als Funktion des Temperaturabstandes zum Übergangspunkt, in der Form eines Potenzgesetzes. Der zugehörige Exponent, der "kritische Exponent", ist ein solcher Parameter.

Der Zusammenhang zwischen grundlegenden Symmetrien der jeweiligen Phasen und den Werten dieser Parameter ist im Rahmen der Statistischen Physik in den letzten Dekaden ausführlich theoretisch untersucht und auch in einer Vielzahl von Experimenten sowie in Computersimulationen überprüft worden. Bei theoretischen Beschreibungen von Phasenübergängen wird mitunter die Landau- oder Mean-Field-Theorie benutzt. Dabei werden jedoch kritische thermische Fluktuationen vernachlässigt, die in der Umgebung des Übergangs eine wesentliche Rolle spielen können (und beispielsweise in der kritischen Opaleszenz beobachtet werden). Die Landau-Theorie kann trotzdem als Ausgangspunkt genauerer Theorien (von der "Skalentheorie" von Pokrowski und Patashinski bis hin zur "epsilon-Entwicklung" von K.G. Wilson und M.E. Fisher) wertvolle erste Einsichten vermitteln. Dies ist insbesondere von Kenneth G. Wilson erkannt worden, der 1982 den Nobelpreis für bahnbrechende Arbeiten über kontinuierliche Phasenübergänge erhielt. Wilson ist einer der entscheidenden Pioniere der Renormierungsgruppentheorie, die berücksichtigt, dass bei kontinuierlichen Phasenübergängen die kritischen Fluktuationen auf vielen Längenskalen in "selbstähnlicher" Form stattfinden. Analoge Theorien finden heute in vielen Bereichen der Physik und Mathematik Anwendung.

Das Wissen über die physikochemischen Bedingungen, bei denen Phasenübergänge ablaufen, erlaubt Mineralogen Rückschlüsse über die Entstehungsgeschichte von Gesteinen. Wenn ein Gestein unter hohe Drücke und Temperaturen gerät, kommt es in vielen Fällen zu einer Phasenumwandlung. Unter der Voraussetzung, dass die anschließende Abkühlung so rasch erfolgt, dass die Umkehrreaktion aufgrund der bei tiefen Temperaturen kaum noch möglichen Diffusion nicht mehr stattfindet, kann man davon ausgehen, dass die bei hohen Temperaturen und Drücken stabilen Minerale „eingefroren“ werden und so an der Erdoberfläche erhalten bleiben. So sind Aussagen darüber möglich, welche Temperaturen und Drücke ein Gestein im Laufe seiner Genese „gesehen“ hat. Beispiele hierfür sind die Phasenübergänge zwischen Andalusit, Sillimanit und Disthen im Bereich der Aluminosilikate, die Umwandlung von Graphit in Diamant und von Quarz in Coesit oder Stishovit. Das durch experimentelle Mineralogie erworbene Wissen über Phasenübergänge erklärt auch das rheologische Verhalten des Erdmantels: Das Eisen-Magnesiumsilikat Olivin wandelt sich in 410 km Tiefe in den in der β-Spinell-Struktur kristallisierenden Wadsleyit um, der sich seinerseits in 520 km Tiefe weiter in den in der γ-Spinell-Struktur auftretenden Ringwoodit umwandelt (siehe auch die Artikel 410-km-Diskontinuität und 520-km-Diskontinuität). Dabei kommt es zu keinerlei chemischen Veränderungen, sondern nur zu einer Änderung der Kristallstruktur. Am Beispiel der Umwandlung von Coesit in Stishovit kann man gut erklären, warum es zu einer Phasenumwandlung kommt: Unter normalen Bedingungen ist Silizium von vier Sauerstoffatomen umgeben, unter hohen Drücken rücken die Atome jedoch dichter zusammen, so dass die Koordination durch sechs Sauerstoffatome energetisch günstiger ist.

Während des keramischen Brandes wandelt sich bei einer Temperatur von 573 °C Quarz in Hochquarz um. Dabei ändert sich das Volumen. Bei einer zu großen Heizrate kann dies zum Zerspringen der Keramik führen. Deshalb wird die Heizrate in diesem Temperaturbereich gedrosselt.

Im Bereich der Konservierung von Kunstobjekten werden die Gegenstände oft kühl und trocken gelagert und auch ausgestellt. Bei Objekten aus Zinn ist dies nicht richtig, weil dieses unterhalb von 15 °C in eine andere Modifikation übergeht, deren äußeres Erscheinungsbild wenig attraktiv ist und die als Zinnpest bezeichnet wird.

Für die Kunstgeschichte ist es interessant zu wissen, dass früher oft das Blaupigment Azurit für die Darstellung des Himmels verwendet wurde. Im Lauf der Jahrhunderte ist dieses jedoch in die thermodynamisch stabile Form Malachit umgewandelt worden, welche grün ist. Dadurch ist der Himmel auf alten Bildern manchmal grün.

Bei der Stahlerzeugung sind mit der Umwandlung der Eisenmodifikation Ferrit in Martensit Veränderungen des Gefüges verbunden, die für die Eigenschaften des Stahls von großer Bedeutung sind.

In zweidimensionalen Materialien, z. B. in dünnen magnetischen Schichten, kann es nur unter eingeschränkten Bedingungen langreichweitige Ordnung und damit einen Phasenübergang geben. Dieser interessante Aspekt wird im Mermin-Wagner-Theorem (nach N. David Mermin und Herbert Wagner) behandelt und ist auch experimentell untersucht worden.

Paraffine besitzen eine besonders große Volumenänderung um etwa 30 % beim Phasenübergang von fest nach flüssig. Dieser Hub kann für die Konstruktion von Aktoren genutzt werden.





</doc>
<doc id="4165" url="https://de.wikipedia.org/wiki?curid=4165" title="PHP">
PHP

PHP (rekursives Akronym und Backronym für „PHP: Hypertext Preprocessor“, ursprünglich „Personal Home Page Tools“) ist eine Skriptsprache mit einer an C und Perl angelehnten Syntax, die hauptsächlich zur Erstellung dynamischer Webseiten oder Webanwendungen verwendet wird. PHP wird als freie Software unter der PHP-Lizenz verbreitet. PHP zeichnet sich durch breite Datenbankunterstützung und Internet-Protokolleinbindung sowie die Verfügbarkeit zahlreicher Funktionsbibliotheken aus.

Eine von mehreren möglichen Versionen des Hallo-Welt-Programms in PHP:
<?php

Seit der Veröffentlichung im Jahr 1995 nahmen die Entwickler umfangreiche Änderungen innerhalb der Programmiersprache vor. Der Kern der Sprache wurde mit PHP 3 komplett neu erstellt. Deutliche Anpassungen wurden mit PHP 5 bzw. PHP 5.3 vorgenommen, die den Fokus der Programmiersprache auf die objektorientierte Programmierung legt. Klassen waren jedoch bereits mit der vierten Version nutzbar.

PHP wurde 1995 von Rasmus Lerdorf entwickelt. Der Begriff stand damals noch für "Personal Home Page Tools" und war ursprünglich als Ersatz für eine Sammlung von Perl-Skripten gedacht, die Lerdorf zur Protokollierung der Zugriffe auf seinen Online-Lebenslauf geschrieben hatte.

Bald schuf Lerdorf eine umfangreichere Version in der Programmiersprache C, in der PHP bis heute entwickelt wird. Das schließlich veröffentlichte PHP/FI (FI stand für "Form Interpreter") war Perl sehr ähnlich, wenn auch eingeschränkter.

PHP 3 wurde von Andi Gutmans und Zeev Suraski 1997 neu geschrieben, da das inzwischen erschienene PHP/FI 2 ihrer Meinung nach für E-Commerce unzureichend war. Lerdorf kooperierte nun mit Gutmans und Suraski, und so wurde die Entwicklung von PHP/FI eingestellt. Die Version PHP3 brachte die Verbreitung der Web-Skriptsprache PHP bedeutend voran. Die neue Sprache wurde einfach unter dem Namen "PHP" veröffentlicht, ein rekursives Akronym für "PHP: Hypertext Preprocessor", um die in den Vorversionen PHP und PHP/FI vorhandene Implizierung einer persönlichen Nutzung zu beseitigen.

Das von Gutmans und Suraski gegründete Unternehmen Zend Technologies Ltd. entwickelte in der Folge die Zend Engine 1, die den Kern der PHP-4-Standardimplementierung bildet. Mit PHP 4 wurden die Ausführungsgeschwindigkeit komplexer Applikationen und die Sicherheit bei Verwendung globaler Variablen verbessert. Eingeführt wurden die Unterstützung für viele weitere Webserver, das Sessionmanagement, die Ausgabepufferung sowie eine Anzahl neuer Sprachkonstrukte.

Da das World Wide Web Ende der 1990er Jahre stark wuchs, bestand großer Bedarf an Skriptsprachen, mit denen sich dynamische Webseiten realisieren lassen. PHP wurde mit der Zeit für die Webentwicklung populärer als der vorherige De-facto-Standard Perl, weil es durch seine Spezialisierung als einfacher erlernbar galt.

Seit 2008 sind die Weiterentwicklung und der Support eingestellt.

Im Sommer 2004 wurde mit Version 5.0 eine weitere Entwicklungsstufe veröffentlicht. Wesentlicher Unterschied zum Vorgänger ist die Zend Engine II, die vor allem ein verbessertes Objektmodell nutzt, somit objektorientierte Anwendungen effizienter ausführt und Sprachkonstrukte wie Überladung ermöglicht. Dazu kommen Exceptions, Reflection, die Integration der Datenbank SQLite sowie Erweiterungen bei XML- und DOM-Handhabung.

Die Weiterentwicklung von PHP 5.5 wurde am 21. Juli 2016 eingestellt. Der aktive Support wurde für PHP 5.6 zum 1. Januar 2017 eingestellt. Sicherheitsupdates für diese Versionen werden nur noch im Sonderfall bereitgestellt. Die Weiterentwicklung von PHP 5.4 wurde im Herbst 2015 eingestellt. Auch Sicherheitsupdates werden seit der Veröffentlichung von PHP 5.4.45 am 3. September 2015 nicht mehr zur Verfügung gestellt. Weitere Möglichkeiten im Rahmen der objektorientierten Programmierung wurden mit PHP 5.3 (Namespaces, Late Static Bindings), 5.4 (Traits, Array- und Konstruktor-Dereferenzierung) und 5.5 (Generatoren) vorangetrieben. Die Version 5.6 führte variadische Funktionen als natives Sprachfeature und "Argument Unpacking" ein, um die Verwendung einer variablen Anzahl von Parametern zu ermöglichen.

Mitte 2011 entschieden die Entwickler, den laut eigenen Angaben chaotischen Release-Prozess zu vereinheitlichen. Demnach sollen regelmäßig neue Versionen erscheinen und der Support für ältere Versionen mit einer festen Zeitspanne garantiert werden.

Ursprünglich als früher Nachfolger von PHP 5 geplant, wurde die Entwicklung von PHP 6 eingestellt. Geplant war unter anderem die Unterstützung verschiedener Unicode-Standards. Einige Verbesserungen sind in aktuellen PHP-Versionen ab 5.3 implementiert.

Nach einer Diskussion darüber, ob die nächste PHP-Version den Namen des ehemals gescheiterten PHP 6 wiederaufnehmen soll, oder stattdessen eine Versionsnummer überspringen und PHP 7 heißen soll, wurde am 29. Juli 2014 bekanntgegeben, dass die Entwickler sich mit 58 zu 24 Stimmen für die Hauptversionsnummer 7 entschieden haben. Der Nachfolger von PHP 5 heißt somit offiziell "PHP 7".

PHP 7 hat eine um bis zu 30 Prozent geringere Ausführungszeit als PHP 5, da unter anderem Hashtabellen neu implementiert wurden. Durch diverse Optimierungen am PHP-Kern beansprucht die neue Version zudem weniger Speicherplatz als der Vorgänger. Auch Komponenten wie Parser, Lexer und Bytecode-Generator wurden einigen Veränderungen unterzogen. Die erste Vorabversion (Alpha 1) erschien am 11. Juni 2015, die fertige Version am 3. Dezember 2015. Bei der Entwicklung von PHP 7 wurde teilweise die Abwärtskompatibilität aufgegeben. Neue Features beinhalten Engine-Exceptions, anonyme Klassen und CSPRNG-Funktionen.

Anfang Dezember 2016 wurde PHP 7.1 veröffentlicht.

Am 30. November 2017 folgte schließlich nach einer ca. viermonatigen Betaphase PHP 7.2. Die Neuerungen der aktuellen Version sollen in erster Linie für mehr Klarheit sorgen.

PHP wurde Anfang 2013 auf etwa 244 Millionen Websites eingesetzt und Ende 2017 von 83 % aller Websites als serverseitige Programmiersprache verwendet. PHP ist die am häufigsten serverseitig verwendete Programmiersprache zum Erstellen von Websites. Zudem ist sie bei den meisten Webhostern vorinstalliert.

PHP ist ein System, das PHP-Code serverseitig verarbeitet. Das bedeutet, dass der Quelltext nicht an den Webbrowser übermittelt wird, sondern an einen Interpreter auf dem Webserver. Erst die Ausgabe des PHP-Interpreters wird an den Browser geschickt. In den meisten Fällen ist das ein HTML-Dokument, wobei es mit PHP aber auch möglich ist, andere Dateitypen, wie Bilder oder PDF-Dateien, zu generieren.

Um eine PHP-Datei im Rahmen einer Webanwendung ausführen zu können, benötigt man ein System, das mit den in der Datei enthaltenen Anweisungen umgehen kann. Aus diesem Grund wird durch eine Schnittstelle, wie ISAPI oder CGI, der Interpreter von einem Server-Daemon oder Server-Dienst, wie Apache oder IIS, ausgeführt. Die Kombination von Linux/Windows/macOS als Betriebssystem, Apache als Webserver, MySQL als Datenbanksystem und PHP wird LAMP (für Linux), WAMP (für Windows) oder MAMP (für Mac OS X) genannt. Fertige LAMP-, MAMP- und WAMP-Pakete, die das einzelne Laden und Konfigurieren von Paketen aus dem Internet unnötig machen, werden etwa im Projekt XAMPP entwickelt. Hier gibt es Versionen für Linux, Solaris, Windows und Mac OS X, die jedoch nur für Test- und Entwicklungsumgebungen genutzt werden sollen.

Da PHP normalerweise in einer Webserver-Umgebung läuft, unterliegt es auch dem zustandslosen HTTP. Jede PHP-Seite belastet den Webserver durch den Interpreter, zudem arbeitet der Interpreter den Quelltext bei jedem Aufruf erneut ab. Das mindert die Reaktionsgeschwindigkeit des Servers und erhöht die Last. Um dem entgegenzuwirken, stehen verschiedene Bytecode-Caches zur Verfügung, die eine zur Ausführung vorbereitete Version des Programmes zwischenspeichern und somit den Zugriff auf diese Datei beim nächsten Aufruf beschleunigen (siehe auch Abschnitt Bytecode-Caching und Artikel PHP-Beschleuniger).

Mit PHP lassen sich auch kommandozeilenorientierte Skripte schreiben, die vom Internet unabhängig sind. Die Qt-Erweiterung und die GTK-Erweiterung stellen sogar eine Programmierschnittstelle für eine grafische Oberfläche zur Verfügung, für die weder ein Webserver noch ein Browser benötigt werden. Die ersten Versionen der Schnittstellen zur grafischen Oberfläche und zu anderen Betriebssystemfunktionen waren spärlich und wurden kaum verwendet. Die aktuell entwickelte PHP-GTK-Version 2 strebt hingegen eine Abdeckung der GTK-API von 95 % an. Gegenwärtig wird PHP jedoch vor allem auf Webservern genutzt.

In PHP sind drei verschiedene Kommentararten möglich.
// einzeiliger Kommentar


/* mehrzeiliger
Ein PHP-Skript lässt sich in HTML integrieren.
<!DOCTYPE HTML>
<html>
</html>
In ein PHP-Skript können externe Dateien eingebunden werden. Dazu gibt es vier verschiedene Anweisungen.
require 'pfad/datei.php'; // wenn die Datei nicht eingebunden werden kann, folgt ein Fehler
include 'pfad/datei.php'; // wenn die Datei nicht eingebunden werden kann, folgt eine Warnung
require_once 'pfad/datei.php'; // bindet die Datei nur ein, wenn sie zuvor noch nicht eingebunden wurde
include_once 'pfad/datei.php'; // bindet die Datei nur ein, wenn sie zuvor noch nicht eingebunden wurde
Variablen müssen in PHP nicht deklariert werden. Variablennamen müssen mit dem Dollarzeichen beginnen.
$variable = 'wert';
Der Datentyp einer Variablen wird üblicherweise nicht vom Programmierer im Quellcode bestimmt, sondern automatisch während der Laufzeit. Er ergibt sich aus dem Kontext, in dem eine Variable benutzt wird. Explizite Typumwandlungen sind möglich. Seit Version 7 können Funktionen auch einfache Typen für ihre Parameter und ihren Rückgabewert vorschreiben – für Argumente, die Arrays oder Klassen beinhalten, war dies bereits vorher möglich.
// Skalare Datentypen
$wahrheitswert = true; // boolean
$ganzzahl = 5; // integer
$kommazahl = 4.71; // float (oder double)
$zeichenkette = 'foo'; // string

// Zusammengesetzte Datentypen
$feld = array('foo', 'bar'); // array
$objekt = new stdClass(); // object
$funktion = function () { // callable

// Spezielle Datentypen
$resource = fopen("foo", "w"); // resource
$variable = NULL; // NULL
PHP setzt man üblicherweise als Servermodul, also als Teil der Webserver-Prozesse oder über FastCGI ein. Setzt man PHP als CGI-Programm ein, so kann sich das negativ auf die Ausführungsgeschwindigkeit auswirken, da für jede HTTP-Anfrage eine neue PHP-Interpreter-Instanz gestartet wird.

Mit der Zend-Engine wird ein PHP-Skript zunächst zu einem (plattformunabhängigen) Bytecode (Zend-Opcode) übersetzt, aus dem zur Ausführung noch Maschinencode erzeugt werden muss. Durch Verwendung eines Bytecode-Cache kann die redundante Generierung von Opcode vermieden werden. Bis zur Version 5.4 besaß PHP keinen integrierten Bytecode-Cache, was dazu führte, dass ein Skript bei jedem Aufruf neu übersetzt werden musste. Um dem entgegenzuwirken, wurden einige Erweiterungen entwickelt, die diese Funktionalität nachrüsten, wie beispielsweise den eAccelerator, den Alternative PHP Cache, XCache sowie den kommerziellen Zend Optimizer. Ab der PHP-Version 5.5 wurde der Zend Optimizer+ nun in die Skriptsprache integriert.

Besonders bei umfangreichen Skripten kann durch einen Bytecode-Cache eine deutliche Steigerung der Ausführungsgeschwindigkeit erreicht werden.

Der PHP-Compiler (PHC) ist ein OpenSource-Compiler von Paul Biggar, welcher PHP-Scripte in optimierten C-Code übersetzt und dann als eigenen Webserver ausführt. Die Grundlage für PHC ist Biggars Doktorarbeit aus dem Jahr 2009 und dessen mehrjährige Forschung an Konzepten zur Konvertierung von Scriptsprachen in kompilierte Sprachen. Mit HipHop existiert seit 2010 eine weitere freie Software zum Übersetzen von PHP-Code in optimierten C++-Code, der kompiliert werden kann, was nach Herstellerangaben dort die Leistung im Schnitt auf etwa das Doppelte steigert. HipHop wurde von Facebook entwickelt, um die Serverlast zu senken (laut Facebook um etwa 40 %). Es ist als Open-Source-Software auf GitHub zu finden.

"Suhosin" ist ein von Stefan Esser, einem ehemaligen Mitglied des „PHP Security Response Team“ und Mitentwickler von PHP, entwickeltes Schutzsystem für PHP-Installationen. Es wurde entworfen, um den Server und die Benutzer vor bekannten und unbekannten Fehlern in PHP-Anwendungen und im PHP-Kern zu schützen.

Eine Auswahl einiger weit verbreiteter Applikationen, die in PHP entwickelt wurden:


PHP 3 wurde unter der GNU General Public License (GPL) vertrieben. Ab Version 4 wird PHP unter der PHP License vertrieben, da der neue Parser, die Zend Engine, vom Hersteller Zend unter einer nicht GPL-kompatiblen Lizenz veröffentlicht wird. Die PHP-Lizenz ist eine Softwarelizenz, die die freie Verwendung und Veränderung der Quelltexte erlaubt. Die Software kann kostenlos aus dem Internet geladen werden; daneben ist PHP auch im Lieferumfang einiger Betriebssysteme (so bei einigen Linux-Distributionen oder macOS) enthalten.

Ab Version 5 unterstützt PHP durch Kapselung der Daten, Destruktoren und Ausnahmebehandlung per Exceptions verbesserte und erweiterte Möglichkeiten der objektorientierten Programmierung. Objekt-Variablen sind in PHP 5 nur noch Referenzen auf Objekte und nicht wie in PHP 4 die Objekte selbst.
Aktuelle Versionen setzen weitere Konzepte objektorientierter Programmierung ein, so werden ab Version 5.3 Namensräume und ab Version 5.4 Traits unterstützt.

Mit der objektorientierten Datenbankabstraktionsebene PDO wurde dem oft bemängelten uneinheitlichen Zugriff auf verschiedene Datenbanken in der Version 5.1 entgegengewirkt.

In PHP 5.3 wurde eine deutliche Verbesserung der Speicheranforderung eines PHP-Skripts zur Laufzeit durch einen internen "Garbage Collector" erreicht. Seitdem ist eine Zusatzinstallation von Zend Optimizer nicht mehr notwendig.

Ab Version 5.4 ist ein einfacher Webserver integriert, der über die Kommandozeile konfigurierbar ist. Er wird nicht für den Produktiveinsatz empfohlen.

Ab Version 5.4 ist der Standardzeichensatz von ISO 8859-1 auf Unicode geändert worden. Ziel ist außerdem die vollständige Unicode-Umsetzung aller Funktionen in PHP, die in PHP 5.4 noch nicht abgeschlossen ist. Damit werden vor allem interne Probleme mit Sprachkonstrukten bei der Verarbeitung von Unicode gelöst. Um Standard-Zeichenkettenfunktionen zu nutzen, welche keine Parameter für den Zeichensatz besitzen, können sie durch das Überladen der entsprechenden Multibyte-Funktionen auf Unicode-Fähigkeit umgestellt werden.

Ab Version 5.6 wurde der Standard-Zeichensatz von ISO-8859-1 auf UTF-8 geändert.

In gewöhnlichen Variablen gespeicherte Daten sind nur innerhalb eines HTTP-Requests verfügbar (im sogenannten "request scope") und werden am Ende des Seitenaufrufs aus dem Speicher gelöscht. Ein Besuch eines Nutzers besteht allerdings meist aus mehreren Seitenaufrufen. Um Daten wie etwa die Produkte in einem Einkaufswagen für eine ganze Sitzung verfügbar zu machen (im "session scope"), verwendet man Sessionvariablen. In der Standardkonfiguration speichert PHP die Sessiondaten im Dateisystem des Webservers. Daten, die sessionübergreifend verfügbar sein sollen (im "application scope"), müssen auf anderen Wegen auf dem Webserver abgespeichert werden.

Einige PHP-Module sind nicht threadsicher.

Die schwache Typisierung von PHP ist Teil des Konzepts, aber auch eine mögliche Fehlerquelle. Ein Fehler tritt etwa auf, wenn numerische Ausdrücke mit Zeichenkettenausdrücken verglichen werden sollen (Operator: "=="), wobei es durch die implizite Typumwandlung zu unerwarteten Ergebnissen kommen kann, sofern kein Operator für typsichere Vergleiche (Operator: ===) verwendet wurde. Zudem ist es in PHP nicht möglich, eine Variablendeklaration zu erzwingen. Variablen werden vielmehr durch erstmalige Verwendung implizit deklariert. Dieser Komfort hat zur Folge, dass etwa Tippfehler im Variablennamen zu schwer auffindbaren Programmfehlern führen können. Um solche Probleme zu finden, kann das LogLevel E_NOTICE aktiviert werden. Dadurch wird beim Verwenden einer nicht initialisierten Variable eine Fehlermeldung ausgegeben.

Es kann vorkommen, dass Funktionen im Falle eines Fehlers keine Ausnahme auslösen. Dadurch ist keine standardisierte Fehlerbehandlung möglich. Mit Hilfe von „set_error_handler“ kann für diese Fälle eine individuelle Fehlerbehandlung definiert werden. Seit PHP 7 können viele fatale Fehler mittels Ausnahmebehandlung ähnlich wie Exceptions abgefangen werden.

Allgemeine Einführungen

Spezielle Themen



</doc>
<doc id="4166" url="https://de.wikipedia.org/wiki?curid=4166" title="Prion">
Prion

Prionen sind Proteine, die im tierischen Organismus sowohl in physiologischen (normalen) als auch in pathogenen (gesundheitsschädigenden) Konformationen (Strukturen) vorliegen können. Die englische Bezeichnung "prion" wurde 1982 von Stanley Prusiner vorgeschlagen, der für die Entdeckung der Prionen 1997 den Nobelpreis erhielt. Sie ist abgeleitet von den Wörtern protein" und "infection und bezieht sich auf die Fähigkeit von Prionen, ihre Konformation auf andere Prionen zu übertragen. Es handelt sich also nicht um Lebewesen, sondern um organische Toxine (Gifte) mit virusähnlichen Eigenschaften.

Körpereigene Prionen kommen vermehrt im Hirngewebe vor, so dass pathologische Veränderungen schwerwiegende Folgen für den Organismus haben können. Die pathogenen Prionen sind mit großer Wahrscheinlichkeit für die Creutzfeldt-Jakob-Krankheit beim Menschen, BSE („Rinderwahn“) beim Rind oder Scrapie (Traberkrankheit) bei Schafen verantwortlich. Sie gelangen am wahrscheinlichsten durch kontaminierte Nahrung in den Körper (z. B. bei BSE, Chronic Wasting Disease oder Kuru). Andere Infektionswege wie etwa die Schmierinfektion konnten noch nicht ausgeschlossen werden. Pathogene Prionen können aber auch durch die spontane Umfaltung körpereigener Prionen entstehen (z. B. familiäre Variante der Creutzfeldt-Jakob-Krankheit, familiäre Schlaflosigkeit).

Grundsätzlich sind pathogene Prionen von anderen Krankheitserregern wie Viren, Bakterien oder Pilzen zu unterscheiden, da sie keine DNA oder RNA enthalten. Sie sind nicht nur von großem wissenschaftlichem und medizinischem Interesse, sondern hatten durch die „BSE-Krise“ auch starke Auswirkungen auf Gebiete wie Landwirtschaft, Verbraucherschutz und Politik.

Eines der zahlreichen im tierischen Körper vorkommenden Eiweiße heißt PrP (Prion Protein cellular = zelluläres Prion-Protein). Es findet sich vor allem im Nervensystem, speziell im Gehirn. Zwischen den verschiedenen Tierarten und gegebenenfalls auch innerhalb einer Tierart unterscheiden sich die Prionen mehr oder weniger geringfügig. PrP kommt vor allem an der Zelloberfläche vor und schützt die Zellen vor zweiwertigen Kupfer-Ionen, HO und freien Radikalen. Des Weiteren wird vermutet, dass es einer der ersten Sensoren in der zellulären Abwehr von reaktivem Sauerstoff und freien Radikalen ist und Auswirkungen auf den enzymatischen Abbau von freien Radikalen hat.

Gerät dieses normale Eiweiß PrP in Kontakt mit einem PrP genannten Eiweiß (Prion Protein Scrapie; pathogene Form des Prion-Proteins, das in der Form zuerst bei an Scrapie erkrankten Tieren gefunden wurde), nimmt PrP die Form von PrP an, „es klappt um“, es ändert seine Konformation. Es entwickelt sich eine Kettenreaktion, in der immer mehr PrP in PrP umgewandelt werden. Große Mengen an PrP wirken zerstörerisch auf das Gehirn, da sie unlöslich sind und sich in den Zellen ablagern. Infolgedessen sterben diese Zellen ab; es entstehen Löcher im Gehirn, eine schwammartige Struktur entsteht. Daher auch der Name dieser Krankheit: spongiforme Enzephalopathie, schwammartige Gehirnerkrankung. Prionerkrankungen enden stets tödlich.

Die Initiation der Krankheit kann auf drei Weisen erfolgen, was unter allen Krankheiten einmalig ist:


Auch die familiären Formen von Prionkrankheiten lassen sich im Experiment übertragen, so kann beispielsweise das in einem Menschen aufgrund der genetischen Disposition entstandene PrP bei Mäusen die Krankheit auslösen, wenn es zuvor ins Gehirn gespritzt wurde.

Prionen sind sehr widerstandsfähig gegen übliche Desinfektions- bzw. Sterilisationsverfahren, was auch ein Grund für die iCJD-Fälle und die BSE-Krise war. Heute gibt es auf die erschwerte Inaktivierung von Prionen abgestimmte strenge Vorschriften für die Sterilisierung von Material, das mit möglicherweise prionhaltigem Gewebe in Kontakt gekommen ist.

Die Prionhypothese gilt heute als relativ gesichert. Dass außer dem PrP noch ein weiterer Faktor eine Rolle spielt, kann jedoch noch nicht endgültig ausgeschlossen werden. Nachdem auch die intensive Suche nach Viren, Viroiden oder Nukleinsäure überhaupt erfolglos blieb, gibt es kaum noch Wissenschaftler, die diesen Weg weiter verfolgen. In der Öffentlichkeit kursieren gelegentlich Außenseitermeinungen wie etwa die Organophosphattheorie, nach der BSE im Zusammenhang mit Insektengiften steht, wofür es jedoch keine wissenschaftlichen Hinweise gibt.

Einzelne Prionkrankheiten wurden schon vor langer Zeit beschrieben (Scrapie, die Prionkrankheit des Schafes, 1759 von Johann George Leopoldt; CJD 1920 von Hans-Gerhard Creutzfeldt), ohne dass man etwas über die Ursache dieser Krankheiten wusste oder sie in eine Gruppe einordnen konnte. Nachdem 1932 die Übertragbarkeit von Scrapie nachgewiesen und 1957 erstmals Kuru beschrieben worden war, wurde Ende der 1950er Jahre von William J. Hadlow die Ähnlichkeit dieser Krankheiten festgestellt und Kuru ebenfalls experimentell auf Affen übertragen. Tikvah Alper und Mitarbeiter stellten 1966 fest, dass der Erreger zu klein war um ein Virus zu sein, und offenbar keine Nukleinsäure enthielt. Daher kam es zu einer "nur Protein"-Hypothese für den Erreger, wobei allerdings unklar blieb, wie sich so ein Protein vermehren könnte. Viele gingen daher am ehesten von Lentiviren als Ursache aus.

Die 1982 von Stanley Prusiner, der an Arbeiten von Daniel C. Gajdusek, welcher bereits unbewusst ein pathogenes Prion entdeckt hatte, anknüpfte, veröffentlichte „Prionhypothese“ wurde zunächst in der Wissenschaft kritisch aufgenommen, da ein nukleinsäurefreies infektiöses Agens bis dahin nicht vorstellbar war. Im Nachhinein erwies sich diese Hypothese jedoch als bahnbrechend und 1997 wurde Prusiner für seine Arbeiten auf dem Gebiet der Prionforschung mit dem Nobelpreis geehrt. In den Jahren nach der Aufstellung dieser Hypothese konnten in zahlreichen Experimenten Hinweise für die Richtigkeit dieser Hypothese gewonnen werden, allerdings kein endgültiger Beweis. 1986 begann die BSE-Epidemie in Großbritannien, 1996 wurden die ersten Fälle von vCJD beschrieben.

Der Nachweis, dass rekombinantes Prion-Protein Krankheiten auslösen kann (womit das Koch'sche Postulat erfüllt war) gelang 2010.

Politiker versuchten, ihre Versäumnisse in Prävention und Verbraucherschutz unter anderem durch großzügige Ausgaben im Bereich der Prionforschung wettzumachen. Zahlreiche Arbeitsgruppen wurden neu eingerichtet, Zentren erbaut und Verbünde gegründet. Die Prionforschung wurde intensiviert und beschleunigt.

2007 ergaben sich Zweifel, ob der Gehalt eines Gewebes an pathogenen Prionen in jedem Fall mit dessen Infektiosität korreliert.

Neue Forschungsergebnisse um die US-amerikanische Forscherin Susan Lindquist zeigen, dass Prionen eine wichtige Rolle bei der Neurogenese (Entwicklung neuer Nervenzellen im Gehirn) spielen.

Physiologische (d. h. normale oder apathogene) Prionen haben zu 43 % die Struktur von Alpha-Helices. Die pathogenen Formen bestehen nur zu 30 % aus Alpha-Helices, zu 43 % bestehen sie aus Beta-Faltblatt-Strukturen. Die Gefahr der pathogenen Prionen besteht darin, dass sie in der Lage sind, die physiologischen, nicht pathogenen Prionen in pathogene umzuwandeln.

Das Prion-Protein ist ein beim Menschen aus 253 Aminosäuren (AS) bestehendes Glykoprotein, das im Prion-Protein-Gen (PRNP) codiert wird. Die AS-Homologie zu anderen Säugetieren beträgt 85 % oder mehr, zwischen Rind und Mensch gibt es z. B. 13 AS-Unterschiede. Es sind jeweils eine oder mehrere Mutationen bekannt, die zu fCJD, GSS oder FFI führen. Am Codon 129 besteht ein Methionin/Valin-Polymorphismus, der für Krankheitsausbruch und -verlauf mitentscheidend ist. PrP enthält zum großen Anteil alpha-Helices, PrP mehr beta-Faltblattstrukturen, aber beide enthalten die gleiche Aminosäure-Primärsequenz.

Der genaue Vorgang der „Umfaltung“ von PrP in PrP ist noch unbekannt. Diese verändert die Eigenschaften des Prion-Proteins, das PrP ist schlechter wasserlöslich, weil die hydrophoben Ketten nicht, wie bei der α-Helix üblich, zur Innenseite der Protein-Tertiärstruktur zeigen. Außerdem ist PrP weitestgehend resistent gegenüber vielen Desinfektionsmitteln, ionisierender und UV-Strahlung und hitzestabil. Feuchte Hitze (131 °C) in zur Sterilisation in der Medizin eingesetzten Autoklaven zerstört das PrP erst nach zwei Stunden, so dass medizinische Instrumente viermal hintereinander autoklaviert werden müssen, bei trockener Hitze wird das Prion bei 200 °C erst nach 60 Minuten inaktiviert und durch Proteasen nur schwer verdaulich (Proteasen können ein Protein am besten im entfalteten Zustand „zerschneiden“, die Denaturierung im Körper ist jedoch durch die veränderte Sekundärstruktur schlechter möglich). PrP ist vor allem an Synapsen lokalisiert.

Die PrP spielen laut neuster Erkenntnisse eine Rolle bei der Bildung von Blut-bildenden Stammzellen (s. u.). Prionprotein-Knockoutmäuse zeigen nach einem Schlaganfall eine verlangsamte Genesung, zudem neigen die Mäuse zu Fettleibigkeit. Es gibt jedoch Hinweise auf eine Rolle als kupferbindendes Protein an der Synapse.
Laut einer Veröffentlichung von US-Forschern in der Fachzeitschrift "PNAS" erhalten normale Prion-Proteine die Regenerationsfähigkeit von blutbildenden Stammzellen. In diesen Zellen treten Prionen in der Zellmembran auf und erfüllen offenbar keine wichtigen Aufgaben – zumindest solange der Körper gesund ist.

Prionkrankheiten sind vor allem durch motorische Störungen wie Ataxie und (am auffallendsten beim Menschen) kognitive Probleme bis zur Demenz gekennzeichnet. Nach einer Inkubationszeit von Jahren bis Jahrzehnten enden die Krankheiten stets tödlich. Im Gehirn finden sich bei der neuropathologischen Begutachtung unter dem Lichtmikroskop spongiöse (schwammartige) Veränderungen und, je nach Krankheit, unterschiedlich ausgeprägte Ablagerungen wie Amyloide, Kuru-Plaques und floride Plaques.


Es wird angenommen, dass die Ansteckung mit der Prionenkrankheit durch Prion-assoziierte Proteine erfolgt und nicht durch das eigentliche Prionprotein. Von besonderem Interesse ist deshalb die Erforschung der Prion-assoziierten Proteine.




</doc>
<doc id="4167" url="https://de.wikipedia.org/wiki?curid=4167" title="Pils">
Pils

Pils steht für:

Pils ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="4168" url="https://de.wikipedia.org/wiki?curid=4168" title="Pelit">
Pelit

Pelit (griechisch "pelos" „Ton, Schlamm“) ist die Bezeichnung für feinklastische Sedimentgesteine mit Korngrößen unter 0,02 mm (Ton bis Mittelschluff). Der Begriff kann zwar prinzipiell materialunabhängig verwendet werden, in der Praxis wird er jedoch fast ausschließlich auf klastische Silikatgesteine angewendet. Er ist daher weitgehend gleichbedeutend mit dem Begriff Tonstein.

Im Fall einer Metamorphose erhalten Pelite die Vorsilbe "Meta"-. Beispiele für solche Metapelite sind Phyllit und Glimmerschiefer.

Von den feinklastischen Peliten werden die mittelklastischen (Psammite) und die grobklastischen (Psephite) unterschieden. Auch die Begriffe "Psammit" und "Psephit" werden in der Regel fast ausschließlich für klastische Silikatgesteine verwendet. In der Karbonatpetrographie werden sehr viel häufiger die Begriffe Lutit/Siltit/Arenit/Rudit zur Beschreibung oder Benennung von Gesteinen nach Korngrößen benutzt. Alternativ zum Schema Pelit/Psammit/Psephit zur Benennung von siliziklastischen Sedimentgesteinen nach Korngrößen werden die Begriffe Tonstein/Siltstein/Sandstein/Konglomerat oder Brekzie angewendet.



</doc>
<doc id="4169" url="https://de.wikipedia.org/wiki?curid=4169" title="Pilsner Bier">
Pilsner Bier

Pils(e)ner Bier, auch Pils oder Bier nach Pils(e)ner Brauart, in der Schweiz Spezialbier, ist ein nach der böhmischen Stadt Pilsen () benanntes, untergäriges Bier mit im Vergleich zu anderen Biersorten erhöhtem Hopfengehalt (und auch starkem Hopfenaroma) und höchstens 12,5 °P Stammwürzegehalt.
Nach Pilsner Brauart hergestellte Biere bilden heute den Großteil der in Deutschland produzierten und verkauften Biere.

Das Bier wurde als Lagerbier und Exportbier sehr beliebt und auch außerhalb Böhmens verbreitet. Bald nannten sich viele Biere nicht nur in Deutschland "Pilsner", "Pilsener" oder auch nur "Pils". Dabei ist die "Pilsner Brauart" aus der schon damals berühmten "Bayerischen Brauart" entstanden, die vor allem auf dem schonend gedarrten und daher sehr hellen Malz beruhte. Dieses heute als Pilsner Malz bezeichnete Malz, eine langsame, kalte Gärung und lange Lagerung in kalten Höhlen und tiefen Kellern sind typische Merkmale des Biers nach Pilsner Brauweise.

Da das vorher in Pilsen gebraute Bier – ein dunkles, trübes, warm vergorenes Bier – einen so schlechten Ruf hatte, dass sogar mehrere Fässer Bier aus Protest öffentlich auf dem Rathausplatz ausgeschüttet wurden, berief der Pilsner Braumeister des "Bürgerlichen Brauhauses" Martin Stelzer in Pilsen 1842 den bayerischen Braumeister Josef Groll aus Vilshofen nach Pilsen, um den Böhmen in Pilsen ein gutes Bier zu brauen. Josef Groll braute am 5. Oktober 1842 den ersten Sud nach Pilsner Brauart. Dieser wurde erstmals am 11. November 1842 öffentlich ausgeschenkt. Heute wird dieses Bier unter der Marke Pilsner Urquell vertrieben.
Erst die Verfügbarkeit von wirtschaftlich arbeitenden Kältemaschinen ab den 1870er-Jahren machte das Brauen nach Pilsener Methode flächendeckend auch dort möglich, wo keine natürlichen Höhlensysteme zur Kühlung vorhanden waren.

Zunächst setzte sich sogar für die untergärigen Biere der Begriff "Nach Bayerischer Brauart" durch, der erst später zu "Pilsner Brauart" geändert wurde. So existieren Etiketten der nach Pilsner Art brauenden Brauerei Heineken mit der Bezeichnung „Nach Bayerischer Brauart“. Die Fürther Brauerei Geismann, die als erste Brauerei im heutigen Bayern Pils braute, taufte dieses „Bayrisch Pilsener“.

Der wesentliche Unterschied eines nach Pilsner Brauart gebrauten Bieres zu anderen Vollbieren – wie etwa dem Hellen – ist, dass es stärker gehopft und somit bitterer ist. Dabei wird insbesondere im Pilsner Urquell Hopfen aus der berühmten Hopfenanbauregion um die nordböhmische Stadt Saaz verwendet.

In der tschechischen Stadt Pilsen können sich Besucher im städtischen Brauereimuseum ausführlich über die Geschichte des Pils informieren. Es beherbergt unter anderem frühere Mälzertrachten, antike Korkmaschinen und seltene Bierkrüge.

Aufgrund eines 1976 in Kraft getretenen Abkommens „über den Schutz von Herkunftsangaben, Ursprungsbezeichnungen und anderen geographischen Bezeichnungen“ mit der damaligen Tschechoslowakei dürfen Bezeichnungen wie „Pils“ oder „Pilsner Bier“ in der Schweiz nur verwendet werden, wenn das Bier aus Tschechien stammt. Im Gegenzug dazu verzichtet Tschechien beispielsweise auf die Verwendung der Bezeichnung Emmentaler für Käse, der nicht in der Schweiz hergestellt wurde. Biere nach Pilsner Art heißen in der Schweiz daher „Spezialbier“ (kurz „Spezial“ oder „Spezli“) oder erhalten frei erfundene Namen.



</doc>
<doc id="4170" url="https://de.wikipedia.org/wiki?curid=4170" title="Pschyrembel (Medizinisches Wörterbuch)">
Pschyrembel (Medizinisches Wörterbuch)

Pschyrembel ist eine Marke für medizinische Nachschlagewerke des Verlags Walter de Gruyter in Berlin. Der Name bezieht sich auf den Frauenarzt Willibald Pschyrembel (1901–1987), der lange das "Klinische Wörterbuch" des Verlags betreut hat. Dieses Wörterbuch ist ein alphabetisches Verzeichnis der gebräuchlichsten und wichtigsten Begriffe der Medizin. Es wurde insbesondere für die medizinischen Berufe konzipiert. Im deutschen Sprachraum erlangte „der Pschyrembel“ eine Bedeutung für den medizinischen Sprachgebrauch, wie sie der Duden für die allgemeine deutsche Sprache hat.

Die Geschichte des Pschyrembel begann mit dem "Wörterbuch der klinischen Kunstausdrücke" des Mediziners Otto Dornblüth, das 1894 im Verlag Veit & Comp., Leipzig, erstmals erschien. Dieser Verlag gehörte zu den fünf Gründungsverlagen des Verlags Walter de Gruyter, bei dem der Pschyrembel seither erscheint. Von 1931 bis 1982 übernahm der Berliner Frauenarzt und Universitätsprofessor Willibald Pschyrembel die Herausgeberschaft. An der 185. bis 250. Auflage wirkten 17 namentlich genannte Fachärzte mit. Die aktuelle Auflage 267 erschien am 1. September 2017. 
Neben der gedruckten Buchversion sind auch eine (kostenpflichtige) Online-Version sowie Versionen für Windows Mobile und iOS erhältlich.

Eine Kuriosität des Pschyrembel ist der fingierte Artikel über die von Loriot erdachte "Steinlaus": Anfangs ein Scherz der Redaktion in der 255. und 256. Auflage, der dann in der 257. Auflage wieder entfernt, aber infolge vieler Lesernachfragen ab der 258. Auflage erneut eingesetzt wurde, findet der Eintrag heute eine große Fangemeinde und bekam ein eigenes Weblog.


Seit Ende der 1990er Jahre veröffentlicht der Verlag de Gruyter weitere medizinische Wörterbücher zu den Themen Therapie, Pflege, Diabetologie, Naturheilkunde und Sozialmedizin. Für diese wurde in Anlehnung an das gut im Markt eingeführte "Klinische Wörterbuch " der Markenname "Pschyrembel" gewählt.

Titel:





</doc>
<doc id="4171" url="https://de.wikipedia.org/wiki?curid=4171" title="Paläontologie">
Paläontologie

Die Paläontologie ( "palaiós" „alt“; "ōn", Genitiv "óntos", „Seiendes“; -logie) ist die Wissenschaft von den Lebewesen und Lebewelten der geologischen Vergangenheit. Gegenstand paläontologischer Forschung sind Fossilien (lat. "fossilis" „ausgegraben“), das heißt in Sedimentgesteinen vorkommende körperliche Überreste sowie sonstige Hinterlassenschaften und Zeugnisse von Lebewesen, die älter sind als 10.000 Jahre.

Der französische Zoologe und Anatom Henri de Blainville führte 1825 den Begriff "Paläontologie" ein, der allmählich die älteren Bezeichnungen Oryktologie (gr. "oryktós" „ausgegraben“) und Petrefaktenkunde (lat. "petrefactum" „versteinert“) ersetzte.

Als Begründer der modernen, nach wissenschaftlichen Kriterien arbeitenden Paläontologie gilt der französische Naturforscher Georges Cuvier (1769–1832). Seine Ansicht, dass Katastrophen das Leben auf der Erde jeweils komplett auslöschten und der Mensch erst nach der letzten Eiszeit erschaffen wurde, widerlegte bereits der britische Geologe Charles Lyell (1797–1875), der die Eiszeittheorie beisteuerte. Parallel dazu erkannte der französische Amateurarchäologe Jacques Boucher de Perthes (1788–1868) als erster in den Steinartefakten menschliche Schöpfungen.

Der Franzose Marcellin Boule (1881–1942) schuf mit seinem Eolithen-Experiment von 1905 die Möglichkeit, menschliche Werkzeuge von natürlich entstandenen Formen zu unterscheiden. Der Schweizer Arzt Otto Hauser (1874–1932) machte in Frankreich (Le Moustier) den professionellen Einstieg in die Höhlen- und Abriforschung. Er stieß dort auf den Widerstand der einheimischen Forschung.

Der erste deutsche Paläontologe, der Darwins Abstammungslehre vertrat, war Ernst Haeckel (1834–1919). Er war Zoologe und brachte die Entwicklung zum Menschen über die Hominiden in die Forschung ein. Er hatte Rudolf Virchow zum Gegner, der ihn den „Affenprofessor“ nannte. Haeckels Anregungen wurden von dem niederländischen Anatom, Geologen und Militärarzt Eugène Dubois (1858–1940) und dem deutschen Paläontologen Gustav Heinrich Ralph von Koenigswald (1902–1982) aufgenommen.

Seit 1997 wurden in Deutschland 21 Paläontologie-Professuren aufgegeben, acht von 27 Hochschulstandorten wurden ganz gestrichen.

Analog zur Biologie rezenter Lebewesen, der Neonatologie („Lehre vom neuen Seienden“), kann die Paläontologie folgendermaßen aufgegliedert werden:

Hinzu kommt die Palichnologie, die verschiedenste fossile Lebensspuren (u. a. Trittsiegel und Fährten, Grabgänge, Fraßspuren) erforscht.

Die Paläontologie der Makrofossilien unterscheidet sich in ihrer Methodik von der Mikropaläontologie, die unter Zuhilfenahme verschiedener Mikroskopie-Techniken Mikrofossilien und die noch kleineren Nannofossilien untersucht. Mikrofossilien können sowohl Überreste von Mikroorganismen als auch mikroskopisch kleine Zeugnisse größerer Lebewesen sein.

Paläontologen untersuchen Fossilien und fossile Organismengruppen unter einer Vielzahl von Gesichtspunkten und Fragestellungen. Eine Einteilung in geologisch und biologisch orientierte Teilgebiete wird vorgenommen:




Der gezielten Suche nach Fossilien in einer paläontologischen Grabung geht die geologische Kartierung der (mutmaßlich) fossilführenden Sedimentgesteine voraus. Ziel ist es, neue Fundpunkte zu finden, die Lage der bereits bekannten Fundhorizonte zu benachbarten Schichten und Gesteinseinheiten aufzuklären und den Ablagerungsraum sedimentologisch näher zu charakterisieren, zum Beispiel ob Sedimente in einem See oder in einem Meer gebildet wurden. Eine derartige Übersichtskartierung entfällt, falls das Alter, die stratigraphische Einordnung und die Lithologie der fossilführenden Gesteine bereits hinreichend bekannt sind.

Eine systematische paläontologische Grabung erfolgt Schicht für Schicht vom Hangenden, das heißt beginnend mit der oben aufliegenden jüngsten Schicht, zum Liegenden, das heißt in Richtung der darunter liegenden älteren Schichten. Begleitend zur Fossiliensuche ist die Geologie der abgetragenen Schichten genau zu beschreiben. Die Horizonte werden durchnummeriert. Die Nummerierung wird auf die Fossilfundstücke übertragen, so dass sie exakt den Horizonten zugeordnet werden können.

Falls größere Organismenreste (wie beispielsweise Dinosaurier-Skelette) Ziel der Grabung sind, ist die Lage einzelner Knochen und Skelettteile innerhalb einer Schicht mit Hilfe eines darüber gelegten Rasters exakt zu dokumentieren. Das ist wichtig, um z. B. Sterbehaltungen oder Ablagerungs- und Transportprozesse zu rekonstruieren und Knochen unterschiedlicher Individuen auseinanderzuhalten.

Für die Gewinnung von Mikrofossilien werden Gesteinsproben der einzelnen Horizonte genommen und später im Labor aufbereitet.

Noch vor Ort werden bröcklige Fossilreste geklebt bzw. mit alkohollöslichen Chemikalien für die spätere Präparation fixiert. Zum Schutz von Knochenfunden kann auch die Ummantelung mit Gips erforderlich sein. Falls Fossilien auf mehrere Gesteinsplatten verteilt sind, werden diese oft an der Bruchstelle wieder zusammengeklebt.

Die spätere Präparation der Fossilien im Labor erfolgt meistens mechanisch, das heißt mit Skalpell und Präpariernadeln (Druckluftmeißel/Airtool) unter der Lupe oder unter Verwendung eines Stereomikroskops. Mit Hilfe von Röntgenstrahlung können vom Gestein verdeckte Fossilienteile lokalisiert werden. Schädigungen bei der Präparation werden auf diese Weise vermieden.

Oftmals lassen sich Mikrofossilien mit Hilfe von Säureätzung oder anderen nasschemischen Verfahren aus dem Gestein herauslösen (siehe Mikropaläontologie).

Wichtig für die weitere Analyse der Fossilien ist die Darstellung mit verschiedenen Methoden, d. h. zeichnerisch, fotografisch und gegebenenfalls zur Sichtbarmachung filigraner Strukturen mit dem Rasterelektronenmikroskop.

Die fotografische und/oder zeichnerische Dokumentation bildet die Grundlage für die Beschreibung und Interpretation eines Fossilfunds und dessen systematische Einordnung. In diesem Rahmen kann auch die Benennung eines neuen Taxons erfolgen.

Aus der Fossilzeichnung kann unter Berücksichtigung bereits bekannter Exemplare und/oder Vertreter verwandter Gruppen der ursprüngliche Skelettzusammenhang (bei Tieren) oder Organzusammenhang (z. B. bei Pflanzen) rekonstruiert werden. Eine Rekonstruktion des Lebensbildes kann im Anschluss erfolgen. Dabei fließen Interpretationen zur Funktion, Lebens- und Fortbewegungsweise des fossilen Lebewesens mit ein. Gegebenenfalls wird auch der Todesvorgang des Tieres rekonstruiert.

Da die Fossilinhalte aller Fundschichten genau dokumentiert sind, kann in dem Fall, dass die jeweiligen Organismenreste nicht von verschiedenen Ursprungsorten antransportiert wurden, sondern aus demselben Ökosystem stammen, eine Analyse der Faunen- und Florenzusammensetzung und im Anschluss eine Rekonstruktion des Nahrungsnetzes erfolgen. Die sedimentologische Beschreibung liefert ergänzende Hinweise zu Transport- und Ablagerungsprozessen, die zur Bildung des fossilführenden Gesteins führten.

Umgekehrt liefern Fossilien den Geologen Aussagen zur Natur des Sedimentationsraums, zum Beispiel, wenn die vorherrschenden Fossiliengruppen nur unter ganz bestimmten Umweltbedingungen (z. B. am Meeresboden in ungetrübtem Wasser bei Temperaturen zwischen 18 und 20 °C und einer Salinität < 2,5 %) vorkamen.

Der vertikalen Abfolge von Horizonten entspricht eine zeitliche: Durch den Vergleich der Lebensgemeinschaften verschiedener Horizonte kann auf die Entwicklungsgeschichte eines vorzeitlichen Ökosystems geschlossen werden.

Falls die Stichproben groß genug sind, das heißt von einer Art genügend Individuen in einem Horizont gefunden und dokumentiert wurden, können diese als Äquivalent zu einer natürlichen Population in Hinsicht auf die Variabilität von Körpermerkmalen untersucht werden. Auch die Zusammensetzung des Ökosystems kann gegebenenfalls quantitativ erfasst werden (z. B. Räuber-Beute-Zahlenverhältnisse).

Der Chemismus von Gewässern kann Einfluss auf die Zusammensetzung von Skeletten und Gehäusen haben. Oftmals sind in akkretionär wachsenden Hartteilen jahres- und tageszeitliche Schwankungen der chemischen und Isotopen-Zusammensetzung zu verzeichnen. Diese lassen sich zum Teil klimatisch interpretieren (siehe auch Paläoklimatologie).

Die chemische Zusammensetzung von Skeletten lässt sich z. B. mit Hilfe von Mikrosondenanalysen aufklären. Die Analyse der Isotopenzusammensetzung erfordert massenspektroskopische Verfahren.

Die mikroskopische Analyse von Dünnschliffen, die von Knochen oder Gehäusen angefertigt wurden, liefert Aussagen zum Wachstum und zur früheren Gewebebeschaffenheit der jeweiligen Hartteile. Sie enthalten mitunter wichtige Anhaltspunkte zur Physiologie und Ontogenese des Hartteilbildners.

Bei vollständiger Erhaltung von Skeletten können Bewegungsabläufe fossiler Tiere in Form von Computermodellen simuliert werden. Auf diese Weise ist es möglich, bestimmte Verhaltens- und Lebensweisen auszuschließen oder als wahrscheinlich anzunehmen.

Verwandtschaftsverhältnisse und Stammbäume fossiler Organismengruppen werden heute im Wesentlichen durch Methoden der rechnergestützten Kladistik ermittelt. Dabei werden Merkmalskombinationen der zu untersuchenden fossilen Arten miteinander verglichen und Stammbäume in Form von Verzweigungsschemata (Kladogrammen) nach dem Prinzip der Sparsamkeit errechnet. Dementsprechend repräsentieren die Ergebnisse dieser Analysen den mutmaßlichen Verlauf der Evolution unter der Annahme möglichst weniger Evolutionsschritte.

Alle fossilen Arten, die eine Fundstätte hervorbringt, kommen in einem bestimmten relativ engen geologischen Zeitraum vor. Falls diese Arten auch von anderen Fundorten bekannt sind, folgt daraus ein möglicher gemeinsamer Bildungszeitraum der verschiedenen Fundschichten.

Der Vergleich mehrerer Sedimentgesteinsabfolgen, die bestimmte Fossilien sowie durch geochronologische Methoden datierbare Vulkanite (wie z. B. Tuffe) enthalten, ermöglicht die Zuweisung genauerer Alter (das heißt solcher mit geringeren Fehlerspannen).

Besonders gut entwickelt ist die biostratigraphische Untergliederung von überwiegend terrestrischen Sedimenten des Känozoikums in Europa mit Hilfe von Landsäugetierresten.





</doc>
<doc id="4174" url="https://de.wikipedia.org/wiki?curid=4174" title="Qualität">
Qualität

Qualität () hat drei Bedeutungen:
Qualität ist bezüglich der Punkte a) und b) die Bezeichnung einer wahrnehmbaren Zustandsform von Systemen und ihrer Merkmale, welche in einem bestimmten Zeitraum anhand bestimmter Eigenschaften des Systems in diesem Zustand definiert wird. Qualität könnte sowohl ein "Produkt" wie Wein und dessen chemische Bestandteile und den daraus resultierenden subjektiv bewertbaren Geschmack beschreiben als auch die "Prozesse" der Reifung der Traube, der Produktion und des Vertriebs des Weines, oder den Prozess des Managements der Winzerei. In der Bedeutung b) spricht man von "Qualitätswein" oder "Wein mit Prädikat" bzw. von "Exzellentem Management". 

Qualität ist mit Bezug auf c) die Summe individueller, (Wert-)Haltungen (Eigenschaften) eines zielgerichtet agierenden Individuums. 

Qualität wird laut der Norm DIN EN ISO 9000:2015-11 (der gültigen Norm zum Qualitätsmanagement) als "„Grad, in dem ein Satz inhärenter Merkmale eines Objekts Anforderungen erfüllt“" definiert. Die Qualität gibt damit an, in welchem Maße ein Produkt (Ware oder Dienstleistung) den bestehenden Anforderungen entspricht. Die Benennung der Qualität kann zusammen mit Adjektiven wie schlecht, gut oder ausgezeichnet verwendet werden. "Inhärent" bedeutet im Gegensatz zu „zugeordnet“ "einer Einheit innewohnend", insbesondere als ständiges Merkmal. Damit sind objektiv messbare Merkmale wie z. B. Länge, Breite, Gewicht, Materialspezifikationen gemeint.

Nicht inhärent sind subjektiv zugeordnete Beschreibungen wie „schön“ oder auch der Preis, weil diese eben nicht objektiv messbar sind. Der Preis oder ein persönliches Urteil sind also nicht Bestandteil der Qualität. Durch die Definition einer Zielgruppe und Meinungsumfragen kann das subjektive Empfinden dieser Zielgruppe ermittelt, ein inhärentes Merkmal definiert und damit „messbar“ und Bestandteil der Qualität werden.

Diese Definition löste die Formulierung des DIN EN ISO 8402:1995-08, des früheren Standards zum Qualitätsmanagement, ab. Nach dieser ist Qualität "„die Gesamtheit von Merkmalen einer Einheit bezüglich ihrer Eignung, festgelegte und vorausgesetzte Erfordernisse zu erfüllen.“" Einheiten sind dabei Produkte, Dienstleistungen, Konzepte, Entwürfe, Software, Arbeitsabläufe, Verfahren und Prozesse; Qualität ist eine Funktion der Anspruchsklasse.

Nach der IEC 2371 ist Qualität die "Übereinstimmung zwischen den festgestellten Eigenschaften und den vorher festgelegten Forderungen einer Betrachtungseinheit".

Während Qualität früher traditionell als eine Eigenschaft von Produkten oder Dienstleistungen verstanden wurde, also die Erfordernisse der Kunden im Vordergrund standen, erstreckt sich der Qualitätsbegriff im Rahmen von Total-Quality-Konzepten, wie dem Total-Quality-Management als umfassender Variante des Qualitätsmanagements, über ganze Unternehmen. Neben die Kundenanforderungen treten die Anforderungen von Mitarbeitern, Kapitalgebern und Öffentlichkeit (rechtliche Anforderungen), an deren Erfüllung sich die umfassende Qualität eines Unternehmens („Total Quality“) misst.

In der praktischen Anwendung des Qualitätsbegriffes kann nach der Auffassung von David A. Garvin zwischen fünf verschiedenen Sichtweisen unterschieden werden:

Philip B. Crosby definierte die vier Eckpfeiler der Qualität wie folgt:

Qualität wird als "Mutter aller Haltungen" dem jeder Handlung vorgelagerten individuellen Erkenntnisprozess eines zielgerichtet handelnden Individuums zugeordnet. Sie ist erkennbar an der Kommunikation des jeweiligen Individuums, an der Wesenheit des Menschentyps (z.B. extrinsischer oder intrinsischer Motivations-Grundhaltung) und ist an der Übereinstimmung des Seins, des Sagens und Tuns (Kommunikation) vom Handlungsergebnis ableitbar. Diese Definition widerspricht den bekannten Definitionen nicht - sie ist vielmehr eine erweiterte Sichtweise, aus der sich insbesondere produkt- und systemtechnisch fokussierte Definitionen ableiten lassen. 

Das unternehmerische Qualitätsverständnis geht über das Qualitätsverständnis der EN ISO 9000:2005 hinaus. Letztere versteht Qualität als Überdeckungsgrad zwischen expliziten und impliziten Forderungen des Kunden „Soll“ und den gelieferten Eigenschaften „Ist“. Eine alleinige Ausrichtung des Unternehmens auf Kundenwünsche ist jedoch nicht zwangsläufig unternehmerisch. Beim unternehmerischen Qualitätsverständnis stellt erst der Überdeckungsgrad der drei Zielgrößen „Kundenforderungen“ (Sollen), „Unternehmensausrichtung“ (Wollen) und „Unternehmensfähigkeit“ (Können) unternehmerische Qualität dar. Kundenforderungen sind z. B. Forderungen nach spezifischen Funktionalitäten oder Eigenschaften, die z. B. ein Produkt (Produktqualität) oder eine Dienstleistung (Dienstleistungsqualität) aus Sicht des Kunden erfüllen soll. In der Unternehmensausrichtung spiegeln sich die Ziele und die strategische Ausrichtung des Unternehmens wider. Die Unternehmensausrichtung definiert, wie ein Unternehmen den Markt und damit die Kunden bearbeiten möchte. Dabei orientiert sich das Unternehmen an den gesellschaftlichen sowie selbst auferlegten Werten. Unter der Unternehmensfähigkeit werden die Kompetenzen verstanden, die das Unternehmen im Rahmen des Qualitätsmanagements besitzt, um die gesetzten Ziele zu erreichen und die Kundenforderungen umzusetzen.

Obgleich die Bezeichnung „Qualität“ an sich keine Bewertung beinhaltet, wird der Begriff im Alltag oft wertend gebraucht. So wird Qualität etwa als Gegenstück zu Quantität verstanden ("Quantität ist nicht gleich Qualität"). „Quantität“ bezeichnet in Wahrheit lediglich die Menge von qualitativen Eigenschaften und drückt sich daher in Mengen- oder Messwerten aus. Die Redewendung bezieht sich jedoch darauf, dass in der Alltagssprache Qualität oft ein Synonym für Güte ist, oft ist daher von „guter“ oder „schlechter“ Qualität die Rede. Kauft ein Kunde ein Produkt oder eine Dienstleistung und erfüllen diese ihre Zwecke für den Kunden, so haben sie im allgemeinen Sprachgebrauch eine „gute Qualität“. Dieses subjektive, kundenbezogene Qualitätsverständnis lässt sich nur sehr schwer insbesondere durch Marktforschung erfassen, da es sich individuell stark unterscheiden kann. "Qualität ist, wenn der Kunde wieder kommt und nicht die Ware."

Tatsächlich hat sich der Begriff „Qualität“ im wirtschaftlichen Alltag als ein allgemeiner Wertmaßstab etabliert, der die Zweckangemessenheit eines Produkts (Produktqualität), einer Dienstleistung (Servicequalität) oder eines Prozesses (Prozessqualität) zum Ausdruck bringen soll. Dieses Verständnis zeigt sich etwa im Ausdruck „Qualitätsarbeit“. Sie findet häufig in einem bereichsübergreifenden, die Qualität der einzelnen Ergebnisse sichernden System statt. Die Planung, Steuerung und Kontrolle aller hierzu nötigen Tätigkeiten wird als Qualitätsmanagement bezeichnet. Als Ergebnis entsteht das „Qualitätsprodukt“.

Der Unterschied zwischen Produkt- und Prozessqualität kann in der Praxis bei Fragen der Haftung zum Tragen kommen. Beispielsweise berücksichtigte das Oberlandesgericht Zweibrücken 2014 bei seiner Zurückweisung von Haftungsansprüchen, welche im Zusammenhang mit den fehlerhaften Brustimplantaten des Herstellers PIP gegen den TÜV Rheinland erhoben worden waren, dass der TÜV Rheinland zwar das Qualitätssicherungssystem von PIP zu prüfen hatte, nicht aber die Beschaffenheit und Qualität der hergestellten Produkte selbst.

Wo sich Produktqualität mit quantitativen Größen messen lässt, wird sie häufig als "technische Qualität" bezeichnet. Das betrifft beispielsweise Eigenschaften wie Bruchfestigkeit, Belastbarkeit, Langlebigkeit, Farbechtheit usw. Als eine der einfachsten Definitionen für Qualität gilt hier die Regel: "Qualität ist die Übereinstimmung von Ist und Soll.", also die Erfüllung von Spezifikationen oder Vorgaben (Fulfilment of a specification) im Gegensatz zu der Erfüllung von Erwartungen und Zielen als dem übergreifenden Qualitätsanspruch (Fitness for Purpose). In der Produktion werden hierbei heute Kennzahlen zur Qualität über rechnergestützte Systeme bestimmt. Diese Systeme zur Qualitätssicherung werden CAQ-Systeme (CAQ von engl. "Computer Aided Quality assurance") genannt.

Oben aufgeführte Sichtweisen sind im Gesundheitswesen gemäß Qualitätsmodell nach Donabedian unter den Stichworten Struktur-, Prozess- und Ergebnisqualität wiederzufinden. Die transzendente Betrachtungsweise, die schon in der Renaissance hinterfragt wurde, hat sich überall da erhalten, wo Einzelfälle und nicht objektivierbare Erfolge bei Patienten und Behandlern eine Rolle spielen. Die Wege zu einer qualitativ hochwertigen und somit erfolgreichen Behandlung werden über „Expertenäußerung“, Medizinische Leitlinien der Fachverbände bis hin zu justitiablen Richtlinien im Rahmen der medizinischen Qualitätssicherung festgelegt. In Bezug auf neuartige Behandlungstechniken und ihre Relevanz für therapeutische Minimal- oder Maximalstandards gelten zum Teil unterschiedliche Bewertungskriterien. Wissenschaftliche und berufspolitische Organisationen versuchen im Konsens bei den verschiedenen miteinander konkurrierenden Therapien den Grad der Evidenz zu bestimmen.

Bei Krankenkassen und Patienten, die eine Kostentransparenz besitzen (Zuzahler), hat sich ein „materialistischer Qualitätsbegriff“ durchgesetzt. D. h. ein gutes Produkt für einen angemessenen Preis, was natürlich auch eine (zahn)ärztliche Dienstleistung sein kann.


Die Deutsche Gesellschaft für Qualität bietet Bildungsveranstaltungen in diesem Umfeld an.




</doc>
<doc id="4175" url="https://de.wikipedia.org/wiki?curid=4175" title="Quentin Tarantino">
Quentin Tarantino

Quentin Jerome Tarantino (/ˌtærənˈtiːnoʊ/; * 27. März 1963 in Knoxville, Tennessee) ist ein US-amerikanischer Filmregisseur, Produzent, Drehbuchautor, Kameramann und Schauspieler. Er ist zweifacher Oscar- und Golden-Globe-Preisträger sowie Gewinner der Goldenen Palme der Internationalen Filmfestspiele von Cannes.

Tarantino kam als Sohn der damals erst 16-jährigen Connie Tarantino und des 21-jährigen Italo-Amerikaners Tony Tarantino in Knoxville, Tennessee, zur Welt. Seinen Namen verdankt er Quint Asper, einer Figur aus der Westernserie "Rauchende Colts". Diese wurde zur Zeit seiner Geburt von Burt Reynolds gespielt. Die Figur in der Serie war Halbblutindianer wie Tarantinos Mutter (jeweils zur Hälfte irischer und Cherokee-Abstammung).

Als Tarantino zwei Jahre alt war, zog seine Mutter mit ihm nach Los Angeles, wo sie ihn alleine großzog. Als Einzelkind verbrachte er seine Freizeit besonders gern in kleinen Vorstadtkinos, die hauptsächlich Martial-Arts- und B-Movies (Grindhousefilme) zeigten.

Mit 15 Jahren brach Tarantino die High School ab und begann eine Schauspielausbildung. Tarantino ist Legastheniker. Fünf Jahre später bekam er wegen seines umfassenden Filmdetailwissens einen Job in der "Video Archives"-Videothek in Manhattan Beach. Er schrieb zusammen mit seinen Freunden Roger Avary und Jerry Martinez die Drehbücher "My Best Friend’s Birthday" (1987, die letzten beiden Akte des Films sind nach ihrer Fertigstellung im Schneideraum verbrannt) und "The Open Road". Das zuletzt genannte wurde wegen seiner Länge (über 500 Seiten) jedoch von sämtlichen Studios abgelehnt und später in "True Romance" und "Natural Born Killers" aufgeteilt. Tarantinos Filmkarriere begann dann mit einer gezielten Lüge: Er behauptete, er habe eine Rolle in Godards Verfilmung von "König Lear", „die ohnehin niemals jemand anschauen würde“ (Zitat Tarantino), gespielt.

In Sundance besuchte Tarantino den Regie-Workshop von Robert Redford, bei dem er Terry Gilliam traf. Dieser habe ihn bezüglich der Umsetzung filmischer Ideen ermutigt; denn es gebe genügend Spezialisten. Als Regisseur müsse man, so Tarantino in einem Interview mit der "Woche", nur wissen, was man wolle – „so wurde ich Regisseur“.

Von der vergeblichen Suche nach Investoren frustriert, verfasste er Anfang der 1990er das Skript zu "Reservoir Dogs – Wilde Hunde", das er ursprünglich mit bescheidenen Mitteln selbst verfilmen wollte. Auf Initiative des Produzenten Lawrence Bender wurde jedoch der Schauspieler Harvey Keitel auf das Projekt aufmerksam, der seine finanzielle Unterstützung zusicherte. Der Film, in dem neben Keitel und Tarantino auch Michael Madsen, Steve Buscemi, Chris Penn, Tim Roth und Lawrence Tierney mitwirkten, wurde ein großer Erfolg und dann auch auf dem Sundance Film Festival gezeigt. Tarantino galt als neuer Hoffnungsträger des unabhängigen Films und fand nun auch Käufer für seine weiteren Drehbücher. 1993 wurde "True Romance" von Tony Scott verfilmt und Oliver Stone drehte ein Jahr später die kontroverse Mediensatire "Natural Born Killers", von der sich Tarantino jedoch distanzierte. Zusammen mit Lawrence Bender gründete er schließlich die Produktionsfirma A Band Apart.

Tarantino begann 1993 mit den Dreharbeiten an seinem zweiten Spielfilm, "Pulp Fiction". Auch dieser entstand mit relativ bescheidenen Mitteln, wurde jedoch ein sensationeller Erfolg an den Kinokassen. Viele Kritiker lobten die Erzählstruktur und die clevere Handlung, auch wenn einzelne die extreme und übertriebene Darstellung von Gewalt bemängelten. Der Film verhalf Schauspielern wie John Travolta, Samuel L. Jackson und Uma Thurman zu einem Karriereschub und erhielt zahlreiche Preise (unter anderem die Goldene Palme von Cannes, den Oscar für das beste Drehbuch sowie sechs weitere Nominierungen).

Nach dem kommerziellen Durchbruch legte Tarantino als Regisseur eine dreijährige Pause ein. 1995 schrieb er Teile der Drehbücher zu dem Episodenfilm "Four Rooms" und zu "From Dusk Till Dawn", das von seinem Freund Robert Rodriguez verfilmt wurde. Für seine nächste Regiearbeit "Jackie Brown" (1997) besetzte Tarantino unter anderem Pam Grier, Robert Forster (zwei seiner Jugendidole), Robert De Niro, Samuel L. Jackson, Bridget Fonda und Michael Keaton. Der Film war für einen Oscar nominiert, erhielt einen Golden Globe Award und eine weitere Golden-Globe-Nominierung.

Nach einer weiteren Pause von fünf Jahren kündigte Tarantino sein nächstes Projekt an – das Racheepos "Kill Bill". Während der Dreharbeiten entschied er, den Film in zwei Teilen zu veröffentlichen, die im Oktober 2003 und im April 2004 ins Kino kamen. Im Jahr 2005 unterstützte er seinen Freund Robert Rodriguez bei den Dreharbeiten zu "Sin City" und führte Gast-Regie für die symbolische Gage von einem Dollar, da Rodriguez, ebenfalls für einen Dollar, den Soundtrack von Kill Bill – Volume 2 zusammengestellt hatte.

Das nächste Projekt, das im April 2007 in die amerikanischen Kinos kam, war "Grindhouse", ein Double Feature, bei dem Tarantino erneut zusammen mit Robert Rodriguez Regie führte. Tarantinos Teil war "Death Proof – Todsicher", eine Art Slasher-Film mit einem Auto als Waffe. 2007 lief Tarantinos "Death Proof" im Wettbewerb der 60. Filmfestspiele von Cannes.

Tarantino drehte 2009 "Inglourious Basterds"; die Dreharbeiten fanden unter anderem in den Babelsberger Filmstudios, in Potsdam und in Görlitz statt. Mit diesem Film war Tarantino erneut im Wettbewerb der 62. Internationalen Filmfestspiele von Cannes vertreten; Christoph Waltz wurde für seine Darstellung des Antagonisten Hans Landa mit dem Darstellerpreis ausgezeichnet. Auch bei der Oscarverleihung 2010 wurde Waltz als „Bester Nebendarsteller“ ausgezeichnet; in weiteren sieben Kategorien kam "Inglourious Basterds" zudem auf eine Nominierung.

Anfang Mai 2011 wurde bekannt, dass Tarantinos nächstes Projekt ein Western mit dem Titel "Django Unchained" sein würde. Christoph Waltz, Jamie Foxx, Leonardo DiCaprio, Kerry Washington und Samuel L. Jackson übernahmen Rollen. Christoph Waltz wurde als „Bester Nebendarsteller“ mit einem Golden Globe ausgezeichnet und bekam zum zweiten Mal nach 2010 den Academy Award (Oscar).

Ende November 2013 kündigte Tarantino in der "The Tonight Show" einen weiteren Western, "The Hateful Eight", an. Nachdem sein Drehbuch, das er seinen Angaben zufolge nur sechs Personen ausgehändigt hatte, im Januar 2014 vermutlich von einer Schauspieleragentur an Dritte weitergereicht worden war, kündigte er einen Stopp der Planungen für den Film an. Am 21. April 2014 veranstaltete er in Los Angeles mit den ursprünglich angedachten Schauspielern eine Lesung des Drehbuchs. "The Hateful Eight" spielt während eines Schneesturms in einem Kurzwarenladen und wird von zwei Kopfgeldjägern dominiert. Die festsitzende Notgemeinschaft wird, Stück für Stück, von einem geheimnisvollen Mörder dezimiert. Tarantino verkündete nach der Lesung, dass es sich noch nicht um eine finale Fassung handle und dass er das Drehbuch noch mehrmals umschreiben wolle. Am 25. Dezember 2015 kam der Film in die Kinos.

Am 11. Juli 2017 wurde bekannt, dass sich Tarantinos nächstes Filmprojekt um die Morde der Manson Family drehen wird. Er selbst habe bereits ein Drehbuch für den Film verfasst und wird auch die Regie übernehmen. Für die zentrale Rolle der ermordeten Schauspielerin Sharon Tate ist die Australierin Margot Robbie vorgesehen. Leonardo DiCaprio wird nach "Django Unchained" erneut mit Tarantino zusammenarbeiten und im Film einen alternden Western-Schauspieler verkörpern. Brad Pitt schlüpft in die Rolle eines Stunt-Doubles, das ebenso große Probleme hat, sich in einem veränderten Hollywood über Wasser zu halten. Als Kandidaten auf die weiteren Rollen werden bislang Namen wie Samuel L. Jackson, Jennifer Lawrence und Tom Cruise gehandelt. Tarantino bat erneut Ennio Morricone um das Komponieren der Filmmusik. Der Film würde das erste Werk des Regisseurs markieren, das auf wahren Begebenheiten beruht; es trägt den Titel "Once Upon a Time in Hollywood". Es ist allerdings nicht auszuschließen, dass Tarantino auch einigen fiktionalen Stoff verarbeiten könnte. Die Drehaufnahmen sollen im Sommer 2018 stattfinden. Als Veröffentlichungstermin ist der 9. August 2019 vorgesehen, welcher den 50. Todestag von Sharon Tate darstellt. Die Wahl dieses Termins stieß nicht überall auf ein positives Echo.

Darüber hinaus wurde im November 2017 bekannt, dass Tarantino, welcher selbst großer Star-Trek-Fan ist, die Veröffentlichung eines Films in diesem Universum plane. Tarantino wird den Film gemeinsam mit J. J. Abrams produzieren, der an einem Drehbuchentwurf arbeiten wird und bei Zustimmung Tarantinos Regie führen wird. Der Film soll ein "R-Rated" bekommen, was bedeutet, dass der Film in den USA von niemandem unter 17 ohne Begleitung eines Erwachsenen gesehen werden darf. Dies würde ein Novum für das Filmuniversum bedeuten.

Tarantino wird der Riege der Autorenfilmer zugeordnet, da er alle wichtigen Aspekte seiner Filme selbst bestimmt, Regie führt und oft auch selbst auftritt ("Pulp Fiction", "Reservoir Dogs", "Four Rooms", "Death Proof", "Inglourious Basterds", "Django Unchained"), sich kurz im Hintergrund zeigt ("Kill Bill") oder auch nur etwas spricht ("Jackie Brown", "The Hateful Eight"). Ferner sind seine Filme oft über bestimmte Dialoge, Markenzeichen oder Anspielungen miteinander verknüpft. So wird schon in "Reservoir Dogs" über Pam Grier gesprochen, die erst fünf Jahre später in "Jackie Brown" die Hauptrolle spielte. Vic Vega in "Reservoir Dogs" ist der Bruder von Vincent Vega aus "Pulp Fiction". Der Klingelton von Abbeys Mobiltelefon, der in "Death Proof" zu hören ist, während sie in einer Tankstelle einkauft, ist die Titelmelodie von "Kill Bill". Auch das Schwert aus "Pulp Fiction", mit dem Butch Coolidge Zed und seinen Kollegen verwundet bzw. tötet, ist ein Hattori-Hanzō-Schwert, das in "Kill Bill" eine tragende Rolle hat. Des Weiteren ist der „Bärenjude“ Donny Donowitz aus "Inglourious Basterds" der Vater des Lee Donowitz aus "True Romance".

Bisher drehte Tarantino all seine Werke mit Ausnahme der Gastregie in "Sin City" auf analogem Film mit der Begründung: „Ich werde nie im Leben digital drehen, das hasse ich! Diese Filme sehen doch grauenvoll aus. Falls eines Tages Filme ausschließlich digital gedreht werden sollten, werde ich Romane schreiben.“

Tarantinos Filme enthalten oft Referenzen an ältere Filme, besonders an skurrile B-Filme und Kampfkunst-Filme, die er nach eigenen Angaben durch seine Arbeit in einer Videothek schätzen gelernt hat. Außerdem bezeichnet er sich als Fan von Sergio Leones Italowestern, deren Stilmittel er häufig übernimmt, so zum Beispiel in "Kill Bill – Volume 2". Als weitere Vorbilder nennt Tarantino Sam Peckinpah, Stanley Kubrick, Brian De Palma, Martin Scorsese, Alfred Hitchcock und James Best. In einem Interview mit der New York Times sagte Tarantino, dass er auch William Witney zu seinen Lieblings-Regisseuren zähle, insbesondere wegen "The Golden Stallion" (1949), einem Film, der auf Roy Rogers zugeschnitten war. Um Witney zu ehren, widmete Tarantino ihm den Film "Kill Bill – Volume 2". Tarantino besetzt oft ehemals bekannte, ältere Schauspieler wie John Travolta, Michael Keaton und Kurt Russell, oder er setzt Schauspieler ein, die in Filmen mitgewirkt haben, auf die angespielt wird. Das sind zum Beispiel Pam Grier in "Jackie Brown", Sonny Chiba und David Carradine in "Kill Bill" und Franco Nero in "Django Unchained". Einigen früheren Stars verhalf Tarantino so zu einem neuen Popularitätsschub.

Einen hohen Stellenwert in Tarantinos Filmen hat die Musik. Häufig lässt er die Musik nicht extra komponieren (Filmmusik oder auch "Score Music)", sondern wählt sie aus bereits existierenden, auch Vokalmusik (sonst eher selten im Film) einschließenden Stücken bekannter Künstler aus. Die gewählten Stücke sind geprägt von klaren Gitarrenklängen und einer Mischung aus südkalifornischen, mexikanischen und texanischen Klängen. Auch klassischer Rock ’n’ Roll, Surfrock, Tex-Mex, Texas Blues und Filmmusik, die stark von Ennio Morricone beeinflusst ist, tauchen in seinen Filmen immer wieder auf.

Mit einigen prominenten Schauspielern hat Tarantino wiederholt zusammengearbeitet. Besonders bekannt:

Neben den wiederkehrenden Darstellern existieren zahlreiche Markenzeichen, die in vielen Tarantino-Filmen auftauchen. Dazu zählen Gegenstände wie Chevrolets, „Red Apple“-Zigaretten bzw. Tabak, „Big Kahuna“- Burger, silberfarbenes Klebeband und diverse Rollennamen; aber auch Stilmittel wie der „Mexican standoff“, der obligatorische „Trunk Shot“ (eine Kamera-Einstellung aus dem Inneren eines Kofferraums), unkonventionelle Erzählstrukturen, die Gliederung in Kapitel sowie lange Kamerafahrten und Großaufnahmen.

Ein besonderes Verhältnis hat Tarantino zu der Schauspielerin Uma Thurman, die sowohl in "Pulp Fiction" als auch in "Kill Bill" Hauptrollen übernahm und die er als seine „Muse“ oder als seine „Marlene Dietrich“ bezeichnet. Von ihr sind in diesen Filmen Großaufnahmen der Füße zu sehen. Generell kommen in Tarantinos Filmen, er bezeichnet sich selbst als Fußfetischisten, häufig Nahaufnahmen von mehr oder weniger bekleideten Füßen zum Einsatz, wie z. B. Bridget Fondas Füße in "Jackie Brown" oder die Füße von Juliette Lewis und Salma Hayek in "From Dusk Till Dawn", die Tarantino selbst in der Rolle des Richard Gecko anstarrt und ableckt, was er in "Death Proof" weiter variiert.
Auch im Film "Inglourious Basterds" spielt ein (verletzter) Fuß eine entscheidende Rolle, es ist der Fuß von Bridget von Hammersmark, gespielt von Diane Kruger, ebenso wie die anfangs noch gelähmten Füße von Uma Thurman in "Kill Bill – Volume 1". Auch in Pulp Fiction gibt es Anspielungen auf Füße und sogar eine kontroverse Diskussion über die Bedeutung einer Fußmassage bei einer Frau.

Als Lieblingsfilme hat Tarantino häufig die beiden Sleaze-Klassiker "Der Tollwütige" (wird in "Jackie Brown" sogar ausschnittweise gezeigt) und "Der Killer von Wien" sowie den Italowestern "Zwei glorreiche Halunken" genannt.

Des Öfteren hat er in seinen Filmen kleine Nebenrollen mit Komikern besetzt: Steven Wright als Radio-DJ in der Originalfassung von "Reservoir Dogs"; Kathy Griffin als Unfallzeugin und Julia Sweeney als die Tochter des Schrottplatzbesitzers in "Pulp Fiction"; Chris Tucker als Beaumont in "Jackie Brown"; Volker Michalowski und Mike Myers in "Inglourious Basterds" und Jonah Hill in "Django Unchained".

Tarantinos Arbeit besticht durch die vielen Zitate, die er den Filmen von Sergio Corbucci, Enzo G. Castellari, Sergio Grieco und dem asiatischen Kino entnommen hat. Dabei ist Tarantino so weit gegangen, dass er ganze Szenen inklusive der Dialoge aus Filmen wie "Django" kopiert. Bei Puristen hat ihm dies Kritik eingebracht, doch den Protagonisten von damals scheint dies nicht viel auszumachen.

In einigen Filmen, bei denen Tarantino mit Robert Rodriguez zusammengearbeitet hat ("From Dusk Till Dawn", "Kill Bill", "Grindhouse"), spielt Michael Parks den Ranger Earl McGraw. Der Sohn von Michael Parks, James Parks, spielt auch seinen fiktiven Nachfahren Edgar McGraw.

Tarantino arbeitet oft mit dem befreundeten Regisseur Robert Rodriguez zusammen, für dessen Film "From Dusk Till Dawn" er das Drehbuch schrieb, mitproduzierte und selbst eine der Hauptrollen spielte. Zudem hatte er einen kurzen Gastauftritt in Rodriguez’ Film "Desperado". Weitere gemeinsame Projekte waren "Four Rooms", die Produktion der "From Dusk Till Dawn"-Fortsetzungen und Tarantinos Auftritt als Gastregisseur für die Comic-Verfilmung "Sin City". Gemeinsam gedreht haben die beiden außerdem "Grindhouse", eine Hommage an die schäbigen B-Movies aus den Sechzigern und Siebzigern, der außerhalb der USA weitgehend getrennt als die zwei Filme Quentin Tarantinos "Death Proof – Todsicher" und Robert Rodriguez’ "Planet Terror" in den Kinos lief.

Roger Avary assistierte ihm bei den Drehbüchern zu "Reservoir Dogs – Wilde Hunde" und "Pulp Fiction". Für Letzteren teilen sich die beiden den Oscar für das beste Original-Drehbuch, den sie 1995 verliehen bekamen. Allerdings kam es bei Tarantino und Avary zum Streit, weil Tarantino ihn im Vorspann von "Pulp Fiction" nicht als Drehbuchautor nennt, sondern lediglich als Mitentwickler der Story.

Von dem in den Jahren 1985 bis 1987 entstandenen Tarantino-Film "My Best Friend’s Birthday" sind nur noch wenige Ausschnitte erhalten, da der Film beim Entwickeln verbrannte, deshalb erschien er auch nie auf DVD.
















Bei der Deutschlandpremiere von "Inglourious Basterds" in Berlin wurde Tarantino mit einer nach ihm benannten Straße auf dem Filmgelände Babelsberg geehrt. Das Straßenschild wurde am 27. Juli 2009 in seiner Anwesenheit enthüllt.

2010 leitete Tarantino die Wettbewerbsjury der 67. Internationalen Filmfestspiele von Venedig.

Tarantino ist seit 2016 mit der israelischen Sängerin Daniela Pick liiert, mit der er schon 2009 eine Beziehung hatte. Das Paar lernte sich 2009 kennen, als Tarantino Inglourious Basterds in Israel vorstellte. Im Juli 2017 gaben sie ihre Verlobung bekannt.

Als im Oktober 2017 bekannt wurde, dass sein langjähriger Freund und Geschäftspartner Harvey Weinstein wohl über Jahrzehnte hinweg Frauen sexuell belästigt und vergewaltigt hatte, schwieg Tarantino zunächst. Über den Twitter-Account der Schauspielerin Amber Tamblyn ließ er am 12. Oktober bekanntgeben, noch einige Tage zu brauchen, bevor er sich zu dem Fall äußern könne. Am 19. Oktober publizierte die New York Times schließlich ein Interview, in dem er zugab, von einigen der Fälle, die an die Öffentlichkeit gelangt waren, gewusst zu haben. Zugleich forderte er alle anderen auf, die genauso Mitwisser waren wie er, sich ebenfalls zu äußern.




</doc>
<doc id="4176" url="https://de.wikipedia.org/wiki?curid=4176" title="Quecksilber">
Quecksilber

Quecksilber ( "Hydrargyros" ,flüssiges Silber‘, davon abgeleitet lat. "hydrargyrum" (Hg), so benannt von Dioskurides; lateinisch "argentum vivum" und "mercurius"; englisch "mercury" und "quicksilver") ist ein chemisches Element mit dem Symbol Hg und der Ordnungszahl 80. Obwohl es eine abgeschlossene d-Schale besitzt, wird es häufig zu den Übergangsmetallen gezählt. Im Periodensystem steht es in der 2. Nebengruppe, bzw. der 12. IUPAC-Gruppe, die auch Zinkgruppe genannt wird. Es ist das einzige Metall und neben Brom das einzige Element, das bei Normalbedingungen flüssig ist. Aufgrund seiner hohen Oberflächenspannung benetzt Quecksilber seine inerte Unterlage nicht, sondern bildet wegen seiner starken Kohäsion linsenförmige Tropfen. Es ist wie jedes andere Metall elektrisch leitfähig.

Quecksilber bedeutet ursprünglich „keckes Silber“, d. h. schnelles – vgl. englisch "quick" – bzw. bewegtes oder lebendiges Silber (von ahd. "quëcsilabar", "quëchsilper", mhd. "quëcsilber", "këcsilber" zu germanisch "kwikw" ,[quick]lebendig‘) als Übersetzung von gleichbedeutend lateinisch "argentum vivum", „lebendiges Silber“, z. B. bei Plinius

Schwefelalkohole werden als "Merkaptane" („Quecksilber-Fänger“) bezeichnet, weil sie mit Quecksilber unter Bildung von Quecksilbersulfiden reagieren können.

Quecksilber ist mindestens seit der Antike bekannt. So wird es schon in den Werken von Aristoteles, Theophrastos von Eresos, Plinius dem Älteren und anderen Schriftstellern der Antike erwähnt. Im Altertum wurde es als "Heilmittel" verwendet (aufgrund seiner Toxizität, die zuerst von dem Arzt und Empiriker Herakleides von Tarent berichtet wurde, jedoch mit entsprechend negativen Folgen). Quecksilber wurde damals durch Verreiben von Zinnober mit Essig oder durch Erhitzen von Zinnober über ein Sublimationsverfahren gewonnen. Vitruv war bereits die Legierung des Quecksilbers mit Gold bekannt. Diese wurde zum Feuervergolden von Gegenständen benutzt, wobei das Quecksilber verdampfte.
Im 5. Jahrhundert n. Chr. kannte man als Quecksilberverbindung das Sublimat (Quecksilber(II)-chlorid). Paracelsus war der erste Arzt, der Präzipitate und basische Quecksilbersalze herstellte und als Heilmittel verwendete. Ab dem 16. Jahrhundert wurde Quecksilber wirtschaftlich bedeutungsvoll, weil es zur Gewinnung von Silber aus Silbererzen über Amalgambildung benötigt wurde.

Am Quecksilber wurde vom niederländischen Physiker Heike Kamerlingh Onnes im Jahre 1911 das erste Mal das Phänomen der Supraleitung entdeckt. Unterhalb von 4,183 Kelvin (−268,9 Grad Celsius) verschwindet dabei der elektrische Widerstand vollständig. Die Nähe zum Siedepunkt von Helium trug dabei zwar zur Entdeckung bei, ist jedoch rein zufällig.

In der griechischen Antike symbolisierte das Quecksilber sowohl den Gott Hermes als auch den zugehörigen Planeten. Dies wurde von den Römern und den Alchemisten für den gleichgesetzten Gott Mercurius übernommen. Daher ist im Lateinischen "mercurius" und im Englischen "mercury" sowohl die Bezeichnung für das Quecksilber als auch für den Planeten und den Gott. Als alternative Bezeichnung für das Metall wird aber auch "quicksilver" verwendet.

Quecksilber wurde in der Alchemie verwendet, um Metalle zu veredeln. So sollte durch Quecksilberzusatz aus Kupfer Silber entstehen. Angestrebt wurde auch eine Verfestigung des Quecksilbers, die "fixatio mercurii", beispielsweise (beschrieben im 15. Jahrhundert durch Hans Kluge) durch physikalisch-chemische Behandlung einer Mischung von Quecksilber mit Kupfervitriol, der weitere Zusätze wie Weinstein, Salpeter und Glaspulver beigefügt wurden.

Für die mittelalterlichen Alchemisten waren Quecksilber, Schwefel und Salz die drei grundlegenden Elemente. Das Einhorn symbolisierte das Quecksilber.

Quecksilber kommt in reiner Form in der Natur vor und ist als einzige flüssige Substanz aus Tradition als Mineral von der IMA anerkannt. Quecksilber ist auch ein Begleitmineral in Steinkohle.

Quecksilbervorkommen gibt es unter anderem in Serbien, Italien, China, Algerien, Russland und Spanien. Meist findet man es als Mineral in Form von Zinnober (HgS) in Gebieten mit ehemaliger vulkanischer Aktivität. Seltener kommt Quecksilber auch gediegen vor. In der Nähe des spanischen Ortes Almadén befinden sich die größten Zinnober-Vorkommen der Erde. Die Förderung wurde im Jahr 2003 beendet und die Mine von Almadén zu einem Besucherbergwerk umgewandelt. Weit seltenere Quecksilberminerale sind Montroydit (HgO), Paraschachnerit, Schachnerit, Eugenit, Luanheit und Moschellandsbergit (alle AgHg). Ein anderes Mineral ist Belendorffit (CuHg).

Große Mengen Quecksilber sind zudem in der gefrorenen Biomasse der Permafrostböden der nördlichen Hemisphäre gebunden. In diesen ist etwa doppelt so viel Quecksilber gespeichert wie in allen anderen Böden, der Atmosphäre sowie den Ozeanen zusammen. Bei einem verstärkten Abtauen des Permafrostes, wie es durch die menschengemachte globale Erwärmung erwartet wird, würden biologische Abbauprozesse einsetzen, durch die das Quecksilber möglicherweise in die Umwelt abgegeben wird, wo es u. a. den arktischen Ökosystemen, Wasserlebewesen in den Ozeanen und der menschlichen Gesundheit schaden könnte.

Quecksilber wird traditionell in Metalltonnen engl. „flask“ von 76-pound (34,473 kg) gehandelt und an der Rohstoffbörse in der Einheit „FL“ = flask notiert.

Reines Quecksilber wird gewonnen, indem man das Quecksilbererz Zinnober (HgS) mit Sauerstoff reagieren lässt "(Röstverfahren)". Die Reaktionsprodukte sind elementares Quecksilber und Schwefeldioxid:

Ein weiterer Nachweis für Quecksilber ist die Glührohrprobe. Dabei wird die zu analysierende Substanz mit etwa der gleichen Menge Natriumcarbonat (Soda) vermengt und im Abzug geglüht. Elementares Quecksilber scheidet sich als metallischer Spiegel an der Reagenzglaswand ab.

Im qualitativen Trennungsgang kann Quecksilber sowohl in der HCl-Gruppe als auch in der HS-Gruppe nachgewiesen werden.
Nach Zugabe von HCl bildet sich Kalomel, HgCl, welches nach Zugabe von Ammoniaklösung zu fein verteiltem Quecksilber und Quecksilber(II)-amidochlorid reagiert.
Nach Einleiten von HS fällt zweiwertiges Quecksilber in Form von schwarzem Zinnober, HgS, aus und kann mit Hilfe der Amalgamprobe nachgewiesen werden.

Für die Spurenanalytik des Quecksilbers und seiner Organoderivate stehen eine Reihe von Methoden zur Verfügung. Allerdings werden in der Literatur laufend neue bzw. verbesserte Verfahren vorgestellt. Ein nicht zu unterschätzendes Problem ist die Probenaufarbeitung.

Unter den verschiedenen Techniken der AAS liefern die Quarzrohr- und die Graphitrohrtechnik die besten Ergebnisse für anorganische und metallorganische Quecksilberverbindungen. Dabei wird eine Quarzküvette elektrisch auf über 900 °C erhitzt und die Probe dabei atomisiert. Anschließend wird die Absorption bei 253,7 nm gemessen. Als Beispiel sei eine Nachweisgrenze für CHHgCl von 100 µg/L genannt. Eine weitere beliebte Technik zum Nachweis von elementarem Quecksilber oder Quecksilberorganylen ist die Kaltdampferzeugung in Verbindung mit der AAS. Bei sehr geringen Konzentrationen werden die volatilen Analytspezies zunächst unter Bildung von Amalgamen auf Gold- oder Silberoberflächen, welche in einer Graphitküvette platziert wurden, angereichert. Anschließend wird bei 1400 °C atomisiert und die Absorption gemessen. Auf diesem Wege wurde eine Nachweisgrenze von 0,03 ng erreicht.

In der AES haben sich das mikrowelleninduzierte Plasma (MIP) und das induktiv gekoppelte Plasma (ICP) zur Atomisierung bewährt. Die Detektion findet auch bei dieser Methode bei 253,65 nm und 247,85 nm statt. Mit Hilfe der MIP-AES wurden absolute Nachweisgrenzen von 4,4 ng/g Probe gefunden. Die ICP-AES weist eine Nachweisgrenze von 20 bis 50 ng/mL auf.

Quecksilber weist insgesamt sieben stabile Isotope unterschiedlicher Häufigkeit auf. Für die Massenspektrometrie sind jedoch häufig nur Hg (13,22 %) und Hg (29,80 %) relevant. Mit Hilfe der ICP-MS können anorganische Quecksilberverbindungen und Quecksilberorganyle wie Methylquecksilber, CHHg, mit Nachweisgrenzen von bis zu 2,6 ng/g bestimmt werden.

Die NAA basiert auf der Kernreaktion Hg(n,γ)Hg (Bestrahlung von Quecksilber mit Neutronen). Dadurch entstehen radioaktive Quecksilbernuklide. Die Intensität resultierenden charakteristischen Gammastrahlung wird mit einem hochreinen Germaniumdetektor bestimmt. Sie ist proportional der Anzahl an vorhandenen aktivierten Kernen und es können durch interne Kalibrierung quantitative Aussagen getroffen werden. Häufig wird Hg mit einer Halbwertszeit von 2,7 Tagen bei 77,3 keV detektiert.

Für die elektrochemische Bestimmung von Hg-Spuren eignet sich am besten die anodische Stripping-Voltammetrie (ASV). Dabei geht der voltammetrischen Messung eine reduktive Anreicherungsperiode auf der Gold-Messelektrode voraus. Es folgt die eigentliche Bestimmung durch Messung des Oxidationsstroms beim Scannen eines Spannungsfensters von 0 V bis 600 mV. Die Höhe des Oxidationspeaks bei 500 mV korreliert mit der Menge an vorhandenem Quecksilber. Es wurden Nachweisgrenzen von 12 pM (2,4 ng/l) Quecksilber im Meerwasser nach 2-minütiger Anreicherungszeit erzielt. Daneben kommt die Invers-Voltammetrie an Gold-, Platin- oder Kohleelektroden in Frage.

Für die Routineanalytik von Quecksilber gibt es mittlerweile automatisierte Analysatoren. Sie beruhen üblicherweise auf dem Prinzip der thermischen Zersetzung, gefolgt von einer Amalgamierung und anschließender Messung der Atomabsorption (siehe AAS). Mit derartigen Analysengeräten können feste und flüssige Proben innerhalb von wenigen Minuten auf ihren Quecksilbergehalt untersucht werden. Diese kommerziell erhältlichen Geräte sind sehr empfindlich und genügen den Anforderungen von nationalen Qualitäts-sicherungsstandards wie der US-EPA-Methode 7473 und der ASTM-Methode D-6722-01.

Quecksilber wird in großen Mengen durch menschliche Aktivitäten freigesetzt. Es wird geschätzt, dass jährlich etwa 2.200 Tonnen als gasförmiges Quecksilber in die Atmosphäre abgegeben werden, zudem noch erhebliche Mengen in Böden und Gewässer.

Bedeutende Emissionsquellen sind:

Bei den luftseitigen Quecksilberemissionen aus Deutschland (10257 kg im Jahr 2013) hatte die Energiewirtschaft aufgrund der Kohlekraftwerke einen Anteil von 68 % (6961 kg), die Metallverhüttung 11 % (1080 kg) und die Zement- u. a. Mineralindustrie 6 % (609 kg). Mit rund 10 Tonnen Quecksilberemission ist Deutschland zusammen mit Polen und Griechenland Spitzenreiter in Europa.

Im Januar 2016 zeigte eine im Auftrag der Grünen erstellte Studie, dass die seit April 2015 in den USA für 1100 Kohlekraftwerke geltenden Quecksilber-Grenzwerte in Deutschland von keinem Kohlekraftwerk eingehalten werden, da entsprechend strenge gesetzliche Anforderungen fehlen. Würden die gleichen Grenzwerte für Quecksilber-Emissionen wie in den USA gelten (im Monatsmittel umgerechnet etwa 1,5 µg/m³ für Steinkohlekraftwerke und 4,4 µg/m³ für Braunkohlekraftwerke), könnte von den 53 meldepflichtigen Kohlekraftwerken in Deutschland lediglich das inzwischen stillgelegte Kraftwerk Datteln (Block 1-3) am Netz bleiben. Das Umweltbundesamt empfiehlt seit mehreren Jahren die Absenkung des Grenzwertes im Abgas von Kohlekraftwerken auf 3 µg/m³ im Tagesmittel und 1 µg/m³ im Jahresmittel. Bei der Umsetzung der europäischen Industrieemissionsrichtlinie haben Bundesregierung und Bundestagsmehrheit Ende Oktober 2012 für Kohlekraftwerke Grenzwerte von 30 µg/m³ im Tagesmittel und (für bestehende Kraftwerke ab 2019) 10 µg/m³ im Jahresmittel beschlossen. Auf der Expertenanhörung im Umweltausschuss des Bundestags am 15. Oktober 2012 war eine Angleichung an die US-amerikanischen Grenzwerte empfohlen worden. Im Juni 2015 hat eine von der Europäischen Kommission geleitete Arbeitsgruppe mit Vertretern aus Mitgliedstaaten, Industrie- und Umweltverbänden festgestellt, dass in Kohlekraftwerken mit quecksilberspezifischen Techniken Quecksilber-Emissionswerte unter 1 µg/m³ im Jahresmittel erreichbar sind. Niedrige Quecksilberemissionen lassen sich durch die Zugabe von Aktivkohle, durch Fällungsmittel im Rauchgaswäscher oder Spezialfiltermodule erreichen. Katalysatoren und die Zugabe von Bromsalzen können die Quecksilberausschleusung verbessern, weil sie elementares in ionisches Quecksilber umwandeln. Die mit diesen Verfahren verbundene Erhöhung der Stromerzeugungskosten wird auf unter 1 Prozent geschätzt.

Niedrige Quecksilber-Konzentrationswerte im Bereich von 1 Mikrogramm pro Normkubikmeter und darunter erreichen beispielsweise das Steinkohle-Kraftwerk in Lünen-Stummhafen, das Steinkohle-Kraftwerk in Wilhelmshaven, das Steinkohle-Kraftwerk in Werne, das Steinkohle-Kraftwerk in Hamm-Uentrop, das Steinkohle-Kraftwerk in Großkrotzenburg bei Hanau sowie das Braunkohlekraftwerk in Oak Grove (Texas/USA).

Die beiden folgenden Tabellen nennen meldepflichtige Betriebe in Deutschland, die im Jahr 2010 mehr als 100 Kilogramm Quecksilber pro Jahr an die Luft gemeldet hatten bzw. mehr als 5 Kilogramm Quecksilbereinleitung pro Jahr in ein Gewässer meldeten. Fett gedruckte Zahlen markieren gegenüber dem Vorjahr gestiegene Emissionen.

In Norwegen sind quecksilberhaltige Produkte seit 2008, in Schweden seit 2009 verboten.

Aufgrund der bekannten Gefahren durch freigesetztes Quecksilber erarbeitete das UN-Umweltprogramm (UNEP) ein internationales Abkommen („Minamata-Übereinkommen“), das im Oktober 2013 von 140 Staaten unterschrieben wurde. Ziel ist die weltweite Minderung von Quecksilberemissionen aus Bergbau, Produktionsverfahren, Produkten und Abfällen. Das Abkommen wurde durch die Ratifizierung des 50. Unterzeichnerstaates am 18. Mai 2017 verbindlich und trat am 16. August 2017 in Kraft.

Das amerikanische Blacksmith Institute ermittel seit 2006 die "Top 10 der am stärksten verseuchten Orte der Erde". Quecksilber gehört hier häufig zu den Schadstoffen der „nominierten“ Orte.

Der Export von Quecksilber bzw. von quecksilberhaltigen Stoffen mit einer Konzentration von über 95 % Quecksilber in Nicht-EU-Staaten ist verboten.

Quecksilber ist ein giftiges Schwermetall, das bereits bei Zimmertemperatur Dämpfe abgibt. Bei der Aufnahme über den Verdauungstrakt ist reines metallisches Quecksilber vergleichsweise ungefährlich, eingeatmete Dämpfe wirken aber stark toxisch.

Besonders toxisch sind vor allem organische Quecksilberverbindungen, wenn sie mit der Nahrung aufgenommen werden. Sie entstehen in der Nahrungskette durch Biomethylierung zu Methylquecksilber. Vergiftungen durch organische Quecksilberverbindungen wurden weltweit Mitte der 1950er Jahre durch die Berichterstattung über die Minamata-Krankheit bekannt.

Je nach Aufnahme sind sowohl eine akute als auch eine chronische Vergiftung möglich. Als Beispiel kann der Fall des englischen Schiffes "Triumph" im Jahre 1810 dienen, auf dem sich mehr als 200 Menschen vergifteten, als ein Fass mit Quecksilber auslief.
In den Jahren 2007 und 2015 sind ayurvedische Mittel mit hohen Quecksilbergehalten aufgefallen.






</doc>
<doc id="4177" url="https://de.wikipedia.org/wiki?curid=4177" title="Quart">
Quart

Quart ( ‚der Vierte‘) bezeichnet

"in der Musik"
"in der Rechtswissenschaft"
"im Sport und Spiel"
"in der Geographie":
"eine Messgröße" 

Quart ist der Name von

QUART ist die Abkürzung für

Siehe auch:



</doc>
<doc id="4178" url="https://de.wikipedia.org/wiki?curid=4178" title="Quarter">
Quarter

Quarter (en. "quarter" „Viertel“, von lat. "quartus" „der vierte“) steht für:

Siehe auch:


</doc>
<doc id="4180" url="https://de.wikipedia.org/wiki?curid=4180" title="Quantenmechanik">
Quantenmechanik

Die Quantenmechanik ist eine Theorie der Physik, die Eigenschaften und Gesetzmäßigkeiten von Materie beschreibt. Im Gegensatz zu den Theorien der klassischen Physik erlaubt sie die Berechnung physikalischer Eigenschaften von Materie im Größenbereich der Atome und darunter. Die Quantenmechanik ist eine der Hauptsäulen der modernen Physik. Sie bildet die Grundlage zur Beschreibung der Phänomene der Atomphysik, der Festkörperphysik und der Kern- und Elementarteilchenphysik, aber auch verwandter Wissenschaften wie der Quantenchemie.

Die Grundlagen der Quantenmechanik wurden zwischen 1925 und 1932 von Werner Heisenberg, Erwin Schrödinger, Max Born, Pascual Jordan, Wolfgang Pauli, Paul Dirac, John von Neumann und weiteren Physikern erarbeitet, nachdem erst die klassische Physik und dann die älteren Quantentheorien bei der systematischen Beschreibung der Vorgänge in den Atomen versagt hatten. Die Quantenmechanik erhielt ihren Namen sowohl in Anlehnung an die Klassische Mechanik als auch in Abgrenzung von ihr. Wie diese bleibt die Quantenmechanik einerseits auf die Bewegung von massenbehafteten Teilchen unter der Wirkung von Kräften beschränkt und behandelt z. B. noch keine Entstehungs- und Vernichtungsprozesse. Andererseits werden einige zentrale Begriffe der klassischen Mechanik, unter anderem „Ort“ und „Bahn“ eines Teilchens, durch grundlegend andere, der Quantenphysik besser angepasste Konzepte ersetzt.

Die Quantenmechanik bezieht sich auf materielle Objekte und modelliert diese als einzelne Teilchen oder als Systeme, die aus einer bestimmten Anzahl von einzelnen Teilchen bestehen. Mit diesen Modellen können Elementarteilchen, Atome, Moleküle oder die makroskopische Materie detailliert beschrieben werden. Zur Berechnung von deren möglichen Zuständen mit ihren jeweiligen physikalischen Eigenschaften und Reaktionsweisen wird ein der Quantenmechanik eigener mathematischer Formalismus genutzt.

Die Quantenmechanik unterscheidet sich nicht nur in ihrer mathematischen Struktur grundlegend von der klassischen Physik. Sie verwendet Begriffe und Konzepte, die sich der Anschaulichkeit entziehen und auch einigen Prinzipien widersprechen, die in der klassischen Physik als fundamental und selbstverständlich angesehen werden. Durch Anwendung von Korrespondenzregeln und Konzepten der Dekohärenztheorie können viele Gesetzmäßigkeiten der klassischen Physik, insbesondere die gesamte klassische Mechanik, als Grenzfälle der Quantenmechanik beschrieben werden. Allerdings gibt es auch zahlreiche Quanteneffekte ohne klassischen Grenzfall. Zur "Deutung" der Theorie wurde eine Reihe verschiedener Interpretationen der Quantenmechanik entwickelt, die sich insbesondere in ihrer Konzeption des Messprozesses und in ihren metaphysischen Prämissen unterscheiden.

Auf der Quantenmechanik und ihren Begriffen bauen die weiterführenden Quantenfeldtheorien auf, angefangen mit der Quantenelektrodynamik ab ca. 1930, mit denen auch die Prozesse der Erzeugung und Vernichtung von Teilchen analysiert werden können.

Genauere Informationen zum mathematischen Formalismus finden sich im Artikel "Mathematische Struktur der Quantenmechanik".

Anfang des 20. Jahrhunderts begann die Entwicklung der Quantenphysik zunächst mit den sogenannten alten Quantentheorien. Max Planck stellte 1900 zur Herleitung des nach ihm benannten Strahlungsgesetzes die Hypothese auf, dass ein Oszillator Energie nur in ganzzahligen Vielfachen des "Energiequantums" formula_1 aufnehmen oder abgeben kann (h ist das Plancksche Wirkungsquantum, f ist die Frequenz des Oszillators). 1905 erklärte Albert Einstein den photoelektrischen Effekt durch die Lichtquantenhypothese. Demnach besteht Licht aus diskreten Partikeln gleicher Energie formula_2, denen mit der Frequenz formula_3 auch eine Welleneigenschaft zukommt.

Im Zeitraum ab 1913 entwickelte Bohr das nach ihm benannte Atommodell. Dieses basiert auf der Annahme, dass Elektronen im Atom nur Zustände von ganz bestimmten Energien einnehmen können und dass die Elektronen bei der Emission oder Absorption von Licht von einem Energieniveau auf ein anderes „springen“ (siehe Elektronischer Übergang). Bei der Formulierung seiner Theorie nutzte Bohr das Korrespondenzprinzip, dem zufolge sich das quantentheoretisch berechnete optische Spektrum von Atomen im Grenzfall großer Quantenzahlen dem klassisch berechneten Spektrum annähern muss. Mit dem Bohrschen Atommodell und seinen Erweiterungen, dem Schalenmodell und dem Bohr-Sommerfeld-Modell, gelangen einige Erfolge, darunter die Erklärung des Wasserstoffspektrums, der Röntgenlinien und des Stark-Effekts, sowie die Erklärung des Aufbaus des Periodensystems der Elemente.

Schnell erwiesen sich diese frühen Atommodelle jedoch als unzureichend. So versagten sie bereits bei der Anwendung auf das Anregungsspektrum von Helium, beim Wert des Bahndrehimpulses des elektronischen Grundzustandes von Wasserstoff und bei der Beschreibung verschiedener spektroskopischer Beobachtungen, wie z. B. des anomalen Zeeman-Effekts oder der Feinstruktur.

Im Jahr 1924 veröffentlichte Louis de Broglie seine Theorie der Materiewellen, wonach jegliche Materie einen Wellencharakter aufweisen kann und umgekehrt Wellen auch einen Teilchencharakter aufweisen können. Diese Arbeit führte die Quantenphänomene auf eine gemeinsame Erklärung zurück, die jedoch wieder heuristischer Natur war und auch keine Berechnung der Spektren von Atomen ermöglichte. Daher wird sie als letzte den alten Quantentheorien zugeordnet, war jedoch richtungsweisend für die Entwicklung der Quantenmechanik.

Die moderne Quantenmechanik fand ihren Beginn im Jahr 1925 mit der Formulierung der Matrizenmechanik durch Werner Heisenberg, Max Born und Pascual Jordan. Während Heisenberg im ersten dieser Aufsätze noch von "„quantentheoretischer Mechanik“" gesprochen hatte, wurde in den beiden späteren Aufsätzen die noch heute gebräuchliche Bezeichnung "„Quantenmechanik“" geprägt. Wenige Monate später stellte Erwin Schrödinger über einen völlig anderen Ansatz – ausgehend von De Broglies Theorie der Materiewellen – die Wellenmechanik bzw. die Schrödingergleichung auf. Kurz darauf konnte Schrödinger nachweisen, dass die Wellenmechanik mit der Matrizenmechanik mathematisch äquivalent ist. Schon 1926 brachte J. H. Van Vleck in den USA unter dem Titel "Quantum Principles and Line Spectra" das erste Lehrbuch zur neuen Quantenmechanik heraus. Das erste deutschsprachige Lehrbuch, "Gruppentheorie und Quantenmechanik" von dem Mathematiker Hermann Weyl, folgte 1928.

Heisenberg entdeckte die nach ihm benannte Unschärferelation im Jahr 1927; im gleichen Jahr wurde auch die bis heute vorherrschende Kopenhagener Interpretation der Quantenmechanik formuliert. In den Jahren ab etwa 1927 vereinigte Paul Dirac die Quantenmechanik mit der speziellen Relativitätstheorie. Er führte auch erstmals die Verwendung der Operator-Theorie inklusive der Bra-Ket-Notation ein und beschrieb diesen mathematischen Kalkül 1930 in seinem Buch "Principles of Quantum Mechanics". Zur gleichen Zeit formulierte John von Neumann eine strenge mathematische Basis für die Quantenmechanik im Rahmen der Theorie linearer Operatoren auf Hilberträumen, die er 1932 in seinem Buch "Mathematische Grundlagen der Quantenmechanik" beschrieb. Die in dieser Aufbauphase formulierten Ergebnisse haben bis heute Bestand und werden allgemein zur Beschreibung quantenmechanischer Aufgabenstellungen verwendet.

Diese Darstellung geht von der Kopenhagener Interpretation der Quantenmechanik aus, die ab 1927 vor allem von Niels Bohr und Werner Heisenberg erarbeitet wurde. Trotz ihrer begrifflichen und logischen Schwierigkeiten hat sie gegenüber anderen Interpretationen bis heute eine vorherrschende Stellung inne. Auf Formeln wird im Folgenden weitgehend verzichtet, Genaueres siehe unter "Mathematische Struktur der Quantenmechanik".

Im Rahmen der klassischen Mechanik lässt sich aus dem Ort und der Geschwindigkeit eines (punktförmigen) Teilchens bei Kenntnis der wirkenden Kräfte dessen Bahnkurve vollständig vorausberechnen. Der Zustand des Teilchens lässt sich also eindeutig durch zwei Größen beschreiben, die (immer in idealen Messungen) mit eindeutigem Ergebnis gemessen werden können. Eine gesonderte Behandlung des Zustandes und der Messgrößen (oder „Observablen“) ist damit in der klassischen Mechanik nicht nötig, weil der Zustand die Messwerte festlegt und umgekehrt.

Die Natur zeigt jedoch Quantenphänomene, die sich mit diesen Begriffen nicht beschreiben lassen. Es ist im Allgemeinen nicht mehr vorhersagbar, an welchem Ort und mit welcher Geschwindigkeit ein Teilchen nachgewiesen wird. Wenn beispielsweise ein Streuexperiment mit einem Teilchen unter exakt gleichen Ausgangsbedingungen wiederholt wird, muss man für das Teilchen nach dem Streuvorgang immer denselben Zustand ansetzen (siehe "Deterministische Zeitentwicklung"), gleichwohl kann es an verschiedenen Orten des Schirms auftreffen. Der Zustand des Teilchens nach dem Streuprozess legt also seine Flugrichtung nicht fest. Allgemein gilt: In der Quantenmechanik gibt es Zustände, die auch dann nicht die Vorhersage eines einzelnen Messergebnisses ermöglichen, wenn der Zustand exakt bekannt ist. Es lässt sich dann jedem der möglichen Messwerte nur noch eine Wahrscheinlichkeit zuordnen. Daher werden in der Quantenmechanik Messgrößen und Zustände getrennt behandelt und es werden für diese Größen andere Konzepte verwendet als in der klassischen Mechanik.

Allen messbaren Eigenschaften eines physikalischen Systems werden in der Quantenmechanik mathematische Objekte zugeordnet, die sogenannten Observablen. Beispiele sind der Ort eines Teilchens, sein Impuls, sein Drehimpuls oder seine Energie. Es gibt zu jeder Observablen einen Satz von speziellen Zuständen, bei denen das Ergebnis einer Messung nicht streuen kann, sondern eindeutig festliegt. Ein solcher Zustand wird „Eigenzustand“ der betreffenden Observablen genannt, und das zugehörige Messergebnis ist einer der „Eigenwerte“ der Observablen. In allen anderen Zuständen, die nicht Eigenzustand zu dieser Observablen sind, sind verschiedene Messergebnisse möglich. Sicher ist aber, dass bei dieser Messung einer der Eigenwerte festgestellt wird und dass das System anschließend im entsprechenden Eigenzustand dieser Observablen ist. Zu der Frage, welcher der Eigenwerte für die zweite Observable zu erwarten ist, oder gleichbedeutend: in welchem Zustand sich das System nach dieser Messung befinden wird, lässt sich nur eine Wahrscheinlichkeitsverteilung angeben, die aus dem Anfangszustand zu ermitteln ist.

Verschiedene Observablen haben im Allgemeinen auch verschiedene Eigenzustände. Dann ist für ein System, das sich als Anfangszustand im Eigenzustand einer Observablen befindet, das Messergebnis einer zweiten Observablen unbestimmt. Der Anfangszustand selbst wird dazu als Überlagerung (Superposition) aller möglichen Eigenzustände der zweiten Observablen interpretiert. Den Anteil eines bestimmten Eigenzustands bezeichnet man als dessen "Wahrscheinlichkeitsamplitude". Das Betragsquadrat einer Wahrscheinlichkeitsamplitude gibt die Wahrscheinlichkeit an, bei einer Messung am Anfangszustand den entsprechenden Eigenwert der zweiten Observablen zu erhalten (Bornsche Regel oder Bornsche Wahrscheinlichkeitsinterpretation). Allgemein lässt sich jeder beliebige quantenmechanische Zustand als Überlagerung von verschiedenen Eigenzuständen einer Observablen darstellen. Verschiedene Zustände unterscheiden sich nur dadurch, welche dieser Eigenzustände mit welchem Anteil zu der Überlagerung beitragen.

Bei manchen Observablen, zum Beispiel beim Drehimpuls, sind nur diskrete Eigenwerte erlaubt. Beim Teilchenort hingegen bilden die Eigenwerte ein Kontinuum. Die Wahrscheinlichkeitsamplitude dafür, das Teilchen an einem bestimmten Ort zu finden, wird deshalb in Form einer ortsabhängigen Funktion, der so genannten Wellenfunktion angegeben. Das Betragsquadrat der Wellenfunktion an einem bestimmten Ort gibt die räumliche Dichte der Aufenthaltswahrscheinlichkeit an, das Teilchen dort zu finden.

Nicht alle quantenmechanischen Observablen haben einen klassischen Gegenpart. Ein Beispiel ist der Spin, der nicht auf aus der klassischen Physik bekannte Eigenschaften wie Ladung, Masse, Ort oder Impuls zurückgeführt werden kann.

Für die mathematische Behandlung physikalischer Vorgänge soll der Zustand des betrachteten Systems zum betrachteten Zeitpunkt alle Angaben enthalten, die – bei bekannten äußeren Kräften – zur Berechnung seines zukünftigen Verhaltens erforderlich sind. Daher ist der Zustand eines Massenpunktes zu einem bestimmten Zeitpunkt "t" in der klassischen Physik schon durch die Angabe von Ort formula_4 und Impuls formula_5 gegeben, zusammen also durch einen "Punkt" in einem 6-dimensionalen Raum, der Zustandsraum oder Phasenraum genannt wird. Genau in dieser Definition liegt begründet, dass die Quantenphänomene in der klassischen Physik keine Erklärung finden können. Dies zeigt sich beispielsweise in der unten beschriebenen Heisenbergschen Unschärferelation, der zufolge Ort und Impuls eines Quantenobjekts prinzipiell nicht gleichzeitig eindeutig bestimmt sein können.

In der Quantenmechanik wird der Zustand durch einen Vektor im Hilbertraum wiedergegeben, die übliche Notation ist formula_6 vereinfacht wird auch oft nur formula_7 geschrieben. Dabei ist zu berücksichtigen, dass zwei verschiedene Vektoren genau dann denselben physikalischen Zustand bezeichnen, wenn sie sich nur um einen konstanten Zahlenfaktor unterscheiden. Eine unter vielen Möglichkeiten, formula_8 zu repräsentieren, ist die Wellenfunktion formula_9 (die ganze Funktion, nicht nur ihr Wert an einem Ort formula_10), oft ebenfalls einfach als formula_7 geschrieben. Betrachtet man die zeitliche Entwicklung des Zustands, schreibt man formula_12 beziehungsweise formula_13 Zwei Wellenfunktionen, die sich nur durch einen konstanten Faktor unterscheiden, geben denselben Zustand wieder.

Eine Observable wird allgemein durch einen linearen Operator formula_14 dargestellt, der mathematisch auf einen Zustandsvektor wirkt und als Ergebnis einen neuen Vektor des Zustandsraums erzeugt: formula_15 Falls formula_8 ein Eigenzustand dieser Observablen ist, gilt die Eigenwertgleichung formula_17 Darin ist der Faktor formula_18 der Eigenwert, also der für diesen Zustand eindeutig festgelegte Messwert der Observablen formula_19 Meist wird der Zustandsvektor formula_8 dann durch einen unteren Index gekennzeichnet, z. B. formula_21 oder formula_22 worin formula_23 der Eigenwert selber ist bzw. n (die „Quantenzahl“) seine laufende Nummer in der Liste aller Eigenwerte (sofern eine solche Liste existiert, also nicht für kontinuierliche Eigenwerte).

Die Beschreibung der zeitlichen Entwicklung eines isolierten Systems erfolgt in der Quantenmechanik analog zur klassischen Mechanik durch eine Bewegungsgleichung, die Schrödingergleichung. Durch Lösen dieser Differentialgleichung lässt sich berechnen, wie sich die Wellenfunktion des Systems entwickelt:

mit dem Hamilton-Operator formula_25, der die Gesamtenergie des quantenmechanischen Systems beschreibt. Der Hamilton-Operator setzt sich zusammen aus einem Term für die kinetische Energie der Teilchen des Systems und einem zweiten Term, der im Falle mehrerer Teilchen die Wechselwirkungen zwischen ihnen beschreibt sowie im Fall externer Felder die potentielle Energie, wobei die externen Felder auch zeitabhängig sein können. Wechselwirkungen zwischen verschiedenen Teilchen werden also – anders als in der newtonschen Mechanik – nicht als Kräfte, sondern ähnlich zur Methodik der klassischen hamiltonschen Mechanik als Energieterme beschrieben. Hierbei ist in den typischen Anwendungen auf Atome, Moleküle, Festkörper insbesondere die elektromagnetische Wechselwirkung relevant.

Die Schrödingergleichung ist eine partielle Differentialgleichung erster Ordnung in der Zeitkoordinate, die Zeitentwicklung des quantenmechanischen Zustands eines geschlossenen Systems ist also vollständig deterministisch.

Wenn der Hamilton-Operator formula_26 eines Systems nicht selbst von der Zeit abhängt, gibt es für dieses System stationäre Zustände, also solche, die sich im Zeitverlauf nicht ändern. Es sind die Eigenzustände zum Hamilton-Operator formula_26. Nur in ihnen hat das System eine wohldefinierte Energie formula_28, eben den jeweiligen Eigenwert:

Die Schrödingergleichung reduziert sich in diesem Fall auf

und hat die Lösung

Die zeitliche Entwicklung drückt sich also einzig in einem zusätzlichen Exponentialfaktor aus, einem "Phasenfaktor". Das bedeutet, dass der durch formula_32 beschriebene Zustand derselbe ist wie formula_33 − ein stationärer Zustand eben. Nur die "quantenmechanische Phase" ändert sich, und zwar mit der Kreisfrequenz formula_34. Auch für andere Observable als die Energie ist in stationären Zuständen die Wahrscheinlichkeit, einen bestimmten Wert zu messen, von der Zeit unabhängig.

Eine weitere wesentliche Eigenschaft des quantenmechanischen Zustandes ist die Möglichkeit zur Interferenz. Wenn z. B. formula_35 und formula_36 Lösungen derselben Schrödingergleichung sind, ist es auch ihre Summe formula_37. In dieser Eigenschaft drückt sich das bei Wellen aller Art geltende Superpositionsprinzip aus. Mathematisch ergibt sie sich hier aus der Linearität der Schrödingergleichung. Die entsprechende räumliche Wahrscheinlichkeitsverteilung für ein Teilchen im Zustand formula_38 ist (bis auf einen konstanten Normierungsfaktor) durch das Betragsquadrat formula_39 gegeben. Im Zustand formula_40 ist die Aufenthaltswahrscheinlichkeit daher nicht die Summe der beiden einzelnen Aufenthaltswahrscheinlichkeiten formula_41 und formula_42, wie man es für klassische Teilchen erwarten würde. Vielmehr ist sie Null an jedem Ort, wo formula_43 gilt (destruktive Interferenz), während sie an Orten mit formula_44 doppelt so groß ist wie die Summe der beiden einzelnen Aufenthaltswahrscheinlichkeiten (konstruktive Interferenz). Diese Eigenschaft weist auch Licht auf, das zum Beispiel hinter einem Doppelspalt ein Interferenzmuster entstehen lässt. Die Quantenmechanik sagt dementsprechend für Teilchen ähnliche Interferenzerscheinungen wie für Licht voraus.

Das Doppelspaltexperiment zeigt sowohl die statistische Natur der Quantenmechanik als auch den Interferenzeffekt und ist damit ein gutes Beispiel für den Welle-Teilchen-Dualismus. Dabei werden mikroskopische „Teilchen“, zum Beispiel Elektronen, in einem breiten Strahl auf ein Hindernis mit zwei eng beieinander liegenden Spalten gesendet und weiter hinten auf einem Leuchtschirm aufgefangen. In der Verteilung der Elektronen auf dem Schirm würde man unter Annahme des klassischen Teilchenmodells zwei klar voneinander abgrenzbare Häufungen erwarten. Das kann man sich so vorstellen, als ob man kleine Kugeln von oben durch zwei Schlitze fallen ließe; diese werden unter jedem Schlitz je einen Haufen bilden. Die mit Elektronen tatsächlich beobachteten Messergebnisse sind anders (siehe Abbildung rechts). Mit der klassischen Teilchenvorstellung stimmen sie nur insoweit überein, als jedes einzelne Elektron auf dem Schirm genau einen einzigen Leuchtpunkt verursacht. Bei der Ausführung des Experiments mit vielen Elektronen (gleich, ob gleichzeitig oder nacheinander auf die Spalte gesendet) wird die Wahrscheinlichkeitsverteilung der Ortsmesswerte sichtbar, die nicht den klassisch erwarteten zwei Häufungen entspricht. Sie weist stattdessen wie beim Licht ausgeprägte Interferenzstreifen auf, in denen sich die destruktive und konstruktive Interferenz abwechseln.

Eine Messung an einem physikalischen Objekt bestimmt den augenblicklichen Wert einer physikalischen Größe. Im Formalismus der Quantenmechanik wird die gemessene Größe durch einen Operator beschrieben, und der Messwert ist ein Eigenwert dieses Operators. Im Allgemeinen sind die Zustände des Systems Überlagerungen von Eigenzuständen zu verschiedenen Eigenwerten, trotzdem wird bei einer einzelnen Messung kein verwaschenes Bild mehrerer Werte gemessen, sondern stets ein eindeutiger Wert. Mit der Messung wird auch festgestellt, dass das Objekt zu diesem Zeitpunkt einen zu diesem Eigenwert gehörenden Eigenzustand des Operators einnimmt. Sofern es sich um eine Messung handelt, die das Objekt intakt lässt, muss eine sofortige Wiederholung der Messung nämlich mit Sicherheit dasselbe Ergebnis liefern, denn jede Änderung des Zustands gemäß der Schrödingergleichung würde eine gewisse Zeit brauchen. 

Das quantenmechanische Messproblem entsteht daraus, dass der Übergang von dem Zustand vor der Messung zu dem durch die Messung festgestellten Zustand nicht als eine zeitliche Entwicklung gemäß der Schrödingergleichung verstanden werden kann. Dieser Übergang wird als "Kollaps der Wellenfunktion" oder als "Zustandsreduktion" bezeichnet. Von den Komponenten, die die Wellenfunktion vor der Messung hat, verschwinden im Kollaps alle diejenigen, die zu anderen Eigenwerten als dem festgestellten Messwert gehören. In den entsprechenden Formulierungen der Quantenmechanik erfolgt dieser Kollaps "beim Vorgang des Messens". Doch dies ist nur eine ungenaue und unbefriedigende Umschreibung in der Alltagssprache. Die Vorgänge in der Messapparatur sind ausnahmslos physikalische Vorgänge. Wenn aber die Quantenmechanik die zutreffende grundlegende Theorie aller physikalischen Vorgänge ist, müsste sie "alle" physikalischen Systeme – inklusive der Messvorrichtung selbst - und deren wechselseitige Wirkung aufeinander beschreiben können. Der Quantenmechanik zufolge überführt der Messvorgang das untersuchte System und die Messvorrichtung in einen Zustand, in dem sie miteinander verschränkt sind. Wenn dann - spätestens durch das Ablesen an der Messvorrichtung - "das" Messergebnis festgestellt wird, stellt sich wieder das Problem der Zustandsreduktion. Offenbar mangelt es an einer Definition in physikalischen Begriffen, was genau den Unterschied einer „Messung“ zu allen anderen physikalischen Prozessen ausmacht, so dass sie den Kollaps der Wellenfunktion verursachen kann. Insbesondere bleibt offen, wo man die Grenze zwischen dem zu beschreibenden Quantensystem und der klassischen „Messapparatur“ festlegen soll. Dies wird als "Demarkationsproblem" bezeichnet. Für die konkrete Vorhersage der Wahrscheinlichkeitsverteilung der Messergebnisse am untersuchten System ist es allerdings unerheblich, wo man diese Grenze zieht, also welche Teile der Messapparatur man mit in die quantenmechanische Betrachtung einbezieht. Fest steht nur, dass zwischen dem Beginn der Messung und dem Registrieren des einzelnen eindeutigen Ergebnisses die Zustandsreduktion erfolgen muss.

Die Kopenhagener Interpretation erklärt den Kollaps und die Fragen zur Demarkation nicht weiter: Eine Messung wird schlicht beschrieben als Interaktion eines "Quantensystems" mit einem "Messgerät", das selber als klassisches physikalisches System aufgefasst wird. Die oben gegebene Beschreibung von Observablen und Zuständen ist an dieser Interpretation orientiert. Davon stark unterschieden ist die Interpretation nach der Viele-Welten-Theorie. Sie betrachtet die im Kollaps verschwundenen Komponenten nicht wirklich als verschwunden, sondern nimmt an, dass in der Messung für jede einzelne Komponente ein Universum neu erschaffen wird, in dem sie als einzige weiterexistiert. Zu diesen und weiteren Sichtweisen siehe Interpretationen der Quantenmechanik.

Eine weiterer wichtiger Unterschied zwischen der quantenmechanischen und der klassischen Messung zeigt sich bei aufeinanderfolgenden Messungen von zwei verschiedenen Größen. Da die (ideale) klassische Messung das gemessene System gar nicht verändert, bleibt hier die Reihenfolge der beiden Messungen ohne Wirkung auf die Ergebnisse. Nach der Quantenmechanik aber wird der anfängliche Zustand durch eine Messung im Allgemeinen verändert, außer es handelt sich schon um einen Eigenzustand der betreffenden Observablen. Bei zwei aufeinanderfolgenden Messungen ist die Reihenfolge daher nur dann unerheblich, wenn sich das System in einem gemeinsamen Eigenzustand beider Observablen befindet. Andernfalls tritt bei mindestens einer der Messungen eine Zustandsreduktion auf, und das betreffende Messergebnis ist nur noch mit Wahrscheinlichkeit vorherzusagen. Für bestimmte Paare von Observablen trifft dies immer zu, denn sie haben überhaupt keinen gemeinsamen Eigenzustand. Solche Observablen werden komplementäre Observablen genannt. Ein Beispiel für ein Paar komplementärer Observablen sind Ort und Impuls. Hat z. B. ein Teilchen einen bestimmten Impuls, so wird eine Messung des Impulses genau diesen Wert ergeben. Eine nachfolgende Ortsmessung ergibt dann einen Wert aus einer unendlich breiten Wahrscheinlichkeitsverteilung, denn bei feststehendem Impuls ist der Ort völlig unbestimmt. Wird aber die Reihenfolge vertauscht, also die Ortsmessung zuerst ausgeführt, ist danach der Impuls unbestimmt, und damit auch das Ergebnis der nachfolgenden Impulsmessung.

Das Unschärfeprinzip der Quantenmechanik, das in Form der Heisenbergschen Unschärferelation bekannt ist, setzt die kleinstmöglichen theoretisch erreichbaren Unsicherheitsbereiche zweier Messgrößen in Beziehung. Es gilt für jedes Paar von komplementären Observablen, insbesondere für Paare von Observablen, die wie Ort und Impuls oder Drehwinkel und Drehimpuls physikalische Messgrößen beschreiben, die in der klassischen Mechanik als kanonisch konjugiert bezeichnet werden und kontinuierliche Werte annehmen können.

Hat für das betrachtete System eine dieser Größen einen exakt bestimmten Wert (Unsicherheitsbereich Null), dann ist der Wert der anderen völlig unbestimmt (Unsicherheitsbereich unendlich). Dieser Extremfall ist allerdings nur theoretisch von Interesse, denn keine reale Messung kann völlig exakt sein. Tatsächlich ist der Endzustand der Messung der Observablen "A" daher kein reiner Eigenzustand der Observablen "A", sondern eine Überlagerung mehrerer dieser Zustände zu einem gewissen Bereich von Eigenwerten zu "A". Bezeichnet man mit formula_45 den Unsicherheitsbereich von "A", mathematisch definiert durch die sog. Standardabweichung, dann gilt für den ebenso definierten Unsicherheitsbereich formula_46 der kanonisch konjugierten Observablen "B" die Ungleichung

Darin ist formula_48 das Plancksche Wirkungsquantum und formula_49.

Selbst wenn beide Messgeräte beliebig genau messen können, wird die Schärfe der Messung von "B" durch die der Messung von "A" beschränkt. Es gibt keinen Zustand, in dem die Messwerte von zwei kanonisch konjugierten Observablen mit kleinerer Unschärfe streuen. Für das Beispiel von Ort und Impuls bedeutet das, dass in der Quantenmechanik die Beschreibung der Bewegung eines Teilchens durch eine Bahnkurve nur mit begrenzter Genauigkeit sinnvoll und insbesondere im Innern eines Atoms unmöglich ist.

Eine ähnliche Unschärferelation gilt zwischen Energie und Zeit. Diese nimmt aber hier eine Sonderrolle ein, da in der Quantenmechanik aus formalen Gründen der Zeit keine Observable zugeordnet ist.

Der Tunneleffekt ist einer der bekannteren Quanteneffekte, die im Gegensatz zur klassischen Physik und zur Alltagserfahrung stehen. Er beschreibt das Verhalten eines Teilchens an einer Potentialbarriere. Im Rahmen der klassischen Mechanik kann ein Teilchen eine solche Barriere nur überwinden, wenn seine Energie höher als der höchste Punkt der Barriere ist, andernfalls prallt es ab. Nach der Quantenmechanik kann das Teilchen hingegen mit einer gewissen Wahrscheinlichkeit die Barriere auch im klassisch verbotenen Fall überwinden. Andererseits wird das Teilchen auch dann mit einer gewissen Wahrscheinlichkeit an der Barriere reflektiert, wenn seine Energie höher als die Barriere ist. Die Wahrscheinlichkeiten für das Tunneln beziehungsweise für die Reflexion können bei bekannter Form der Potentialbarriere präzise berechnet werden.

Der Tunneleffekt hat eine große Bedeutung in verschiedenen Bereichen der Physik wie zum Beispiel bei der Beschreibung des Alpha-Zerfalls, der Kernfusion, der Funktionsweise der Feldemissions- und Rastertunnelmikroskopie oder bei der Erklärung des Zustandekommens der chemischen Bindung.

Wenn zwei Quantensysteme miteinander in Wechselwirkung treten, müssen sie als ein Gesamtsystem betrachtet werden. Selbst wenn vor der Wechselwirkung der quantenmechanische Zustand dieses Gesamtsystems einfach aus den beiden wohldefinierten Anfangszuständen der beiden Teilsysteme zusammengesetzt ist, entwickelt er sich durch die Wechselwirkung zu einer Superposition von Zuständen, die jeweils aus solchen Paaren von Zuständen der Teilsysteme gebildet sind. Es sind mit verschiedener Wahrscheinlichkeit verschiedene Paarungen möglich (z. B. beim Stoß der elastische oder der inelastische Stoß, oder Ablenkung um verschiedene Winkel etc.). In jedem dieser Paare sind die Endzustände der Teilsysteme so aufeinander abgestimmt, dass die Erhaltungssätze (Energie, Impuls, Drehimpuls, Ladung etc.) erfüllt sind. Der Zustand des Gesamtsystems liegt eindeutig fest und ist eine Superposition aller möglichen Paarungen. Er kann nicht – wie der Anfangszustand vor der Wechselwirkung – einfach aus je einem bestimmten Zustand beider Teilsysteme gebildet werden. Dann ist mit einer Messung, die nur an einem Teilsystem ausgeführt wird und dieses in einem bestimmten seiner möglichen Endzustände findet, auch eindeutig festgestellt, dass das andere Teilsystem sich im dazu passenden Endzustand befindet. Es besteht nun eine Korrelation zwischen den physikalischen Eigenschaften der Teilsysteme. Daher bezeichnet man den Zustand des Gesamtsystems als verschränkt. Die Verschränkung bleibt auch dann erhalten, wenn der Zeitpunkt der Wechselwirkung schon weit in der Vergangenheit liegt und die zwei Teilsysteme sich inzwischen weit voneinander entfernt haben. Es ist zum Beispiel möglich, ein Paar von Elektronen so zu präparieren, dass sie sich räumlich entfernen und für keins der Elektronen einzeln die Richtung des Spins vorhersagbar ist, während es feststeht, dass das eine Elektron den Spin „down“ aufweist, wenn das andere Elektron mit dem Spin „up“ beobachtet wurde, und umgekehrt. Diese Korrelationen sind auch beobachtbar, wenn erst nach der Wechselwirkung entschieden wird, welche beliebige Richtung im Raum als Up- bzw. Down-Achse definiert wird.

Folge der Verschränkung ist, dass die Durchführung einer Messung an einem Ort die Messergebnisse an einem (im Prinzip beliebig weit entfernten) anderen Ort beeinflusst, und das ohne jede Zeitverzögerung, also mit Überlichtgeschwindigkeit. Dieses Phänomen war einer der Gründe, weshalb Albert Einstein die Quantenmechanik ablehnte. Er betrachtete die "Separierbarkeit" oder „Lokalität“ physikalischer Systeme (d. h. die Existenz wohlbestimmter lokaler physikalischer Eigenschaften) als ein fundamentales Prinzip der Physik, und versuchte nachzuweisen, dass die Quantenmechanik unvollständig ist. Dazu entwickelte er 1935 gemeinsam mit Boris Podolsky und Nathan Rosen ein Gedankenexperiment, das als Einstein-Podolsky-Rosen-Paradoxon (EPR-Paradoxon) bekannt wurde. Sie zeigten damit, dass aus dem Prinzip der Lokalität das Vorhandensein zusätzlicher Eigenschaften der Systeme folgt, die von der Quantenmechanik nicht beschrieben werden (sogenannte verborgene Variablen); somit sei die Theorie unvollständig. Es blieb jedoch unklar, ob das aus der klassischen Physik bekannte Lokalitätsprinzip tatsächlich auch in der Quantenmechanik gilt. Erst im Jahr 1964 gelang es John Stewart Bell, das EPR-Gedankenexperiment um die experimentell überprüfbare Bellsche Ungleichung zu erweitern und damit die Lokalitätsannahme auf die Probe zu stellen. Alle seitdem durchgeführten Experimente haben die von der Quantenmechanik vorhergesagte Verletzung der Bellschen Ungleichung gezeigt und damit Einsteins Lokalitätsannahme widerlegt.

Weiterhin zeigt die genaue theoretische Analyse des EPR-Effektes, dass dieser nicht im Widerspruch zur speziellen Relativitätstheorie steht, da auf diese Weise keine Information übertragen werden kann: Die einzelne Messung ergibt – unabhängig davon, ob das andere Teilchen bereits gemessen wurde – stets ein am Ort und zum Zeitpunkt der Messung unvorhersagbares Ergebnis. Erst, wenn das Ergebnis der anderen Messung – frühestens durch Kommunikation mit Lichtgeschwindigkeit – bekannt wird, kann man die Korrelation feststellen oder ausnutzen.

Durch die prinzipielle Unmöglichkeit, den Zustand eines quantenphysikalischen Systems nach klassischen Maßstäben „vollständig“ zu bestimmen, verliert eine Unterscheidung zwischen mehreren Teilchen mit gänzlich identischen intrinsischen Eigenschaften (wie beispielsweise Masse oder Ladung, nicht aber zustandsabhängigen Größen wie Energie oder Impuls) in der Quantenmechanik ihren Sinn. Nach den Vorstellungen der klassischen Mechanik können beliebig genaue Orts- und Impulsmessungen simultan an mehreren Teilchen durchgeführt werden – ob identisch oder nicht –, woraus (zumindest prinzipiell) die zukünftige Bahn jedes Teilchens genau vorhergesagt werden kann. Findet man später ein Teilchen an einem bestimmten Ort, kann man ihm eindeutig seinen Ausgangspunkt zuordnen und mit Sicherheit sagen, an beiden Orten habe es sich um "dasselbe" Teilchen gehandelt. Eine quantenmechanische Betrachtung lässt eine solche „Durchnummerierung“ von identischen Teilchen nicht zu. Das ist deshalb wichtig, weil z. B. alle Elektronen in diesem Sinne identische Teilchen sind. Es ist also beispielsweise unmöglich die Frage zu beantworten, ob bei zwei aufeinander folgenden Messungen an einzelnen Elektronen „dasselbe“ oder ein „anderes“ Elektron beobachtet wurde. Hier sind die Worte „dasselbe“ und „anderes“ in Anführungszeichen gesetzt, weil sie zwar umgangssprachlich klar erscheinen mögen, für identische Teilchen aber gar keinen Sinn ergeben. Es ist nicht nur "unmöglich", die gestellte Frage zu beantworten, sie lässt sich schon gar nicht physikalisch sinnvoll stellen.

Da das Vertauschen zweier identischer Teilchen keine der physikalischen Eigenschaften des Zustands eines Vielteilchensystems ändert, muss der Zustandsvektor gleich bleiben oder kann höchstens sein Vorzeichen wechseln. Identische Teilchen bezeichnet man als Bosonen, wenn bei deren Vertauschung der Zustandsvektor gleich bleibt, als Fermionen, wenn er das Vorzeichen wechselt. Das Spin-Statistik-Theorem besagt, dass alle Teilchen mit ganzzahligem Spin Bosonen sind (z. B. die Photonen) und alle Teilchen mit halbzahligem Spin Fermionen. Dies lässt sich nicht im Rahmen der Quantenmechanik, sondern erst aus der Quantenfeldtheorie ableiten.

Eine wichtige Konsequenz ist die als „Pauli-Prinzip“ bekannte Regel, dass zwei identische Fermionen nicht die gleichen Einteilchenzustände einnehmen können. Es schließt bei den Atomen die Mehrfachbesetzung elektronischer Zustände aus und erzwingt deren „Auffüllung“ bis zur Fermienergie. Das ist von großer praktischer Bedeutung, denn es ermöglicht den Atomen, vielgestaltige chemische Verbindungen einzugehen. Das Spin-Statistik-Theorem bewirkt außerdem erhebliche Unterschiede im thermodynamischen Verhalten zwischen Systemen mit vielen identischen Teilchen. Bosonen gehorchen der Bose-Einstein-Statistik, die z. B. die Wärmestrahlung beschreibt, Fermionen der Fermi-Dirac-Statistik, die z. B. die elektronischen Eigenschaften von Leitern und Halbleitern erklärt.

Die Dekohärenz ist ein modernes Konzept der Quantenmechanik, das bei makroskopischen Systemen die äußerst effiziente Unterdrückung der Folgen der Kohärenz beschreibt. Damit kann im Rahmen der Quantenmechanik erklärt werden, dass makroskopische Systeme keine Superpositionseffekte zeigen, sich also (von Ausnahmen abgesehen) „klassisch“ verhalten. Dekohärenz ist damit heute ein wichtiger Bestandteil des Korrespondenzprinzips der Quantenmechanik.

Zur Veranschaulichung dieses Effektes sei das Beispiel eines makroskopischen Objekts betrachtet, das dem Einfluss einer isotropen Lichtstrahlung – im Folgenden auch als "Umgebung" bezeichnet – ausgesetzt ist. Im Rahmen der klassischen Physik ist der Einfluss des einfallenden Lichts auf die Bewegung des Objekts vernachlässigbar, da der mit dem Stoß eines Photons verbundene Impulsübertrag sehr gering ist und sich die Stöße aus verschiedenen Richtungen im Mittel kompensieren. Bei quantenmechanischer Betrachtung findet bei jedem Stoß eine Verschränkung des Objekts mit einem Photon statt (siehe oben), sodass das Objekt und das Photon nun als ein erweitertes Gesamtsystem betrachtet werden müssen. Die für Interferenzeffekte entscheidenden festen Phasenbeziehungen des quantenmechanischen Zustands erstrecken sich nun also über zwei Teilsysteme, das Objekt und das Photon, man spricht auch von einer Delokalisierung der Kohärenz.

Bei isolierter Betrachtung des (Teil)zustands des Objekts äußert sich jeder Stoß in einer Verschiebung seiner quantenmechanischen Phasenbeziehungen und damit in einer Verringerung seiner Interferenzfähigkeit. Hierbei handelt es sich um einen reinen Quanteneffekt, der unabhängig von einem mit dem Stoß verbundenen Impuls- oder Energieübertrag ist. Die praktisch unvermeidlichen, zahlreich auftretenden Wechselwirkungen makroskopischer Objekte mit ihrer Umgebung führen so zu einer effektiven Ausmittelung aller quantenmechanischen Interferenzeffekte. Die für die Dekohärenz charakteristische Zeitskala, die Dekohärenzzeit "τ", ist im Allgemeinen unter Normalbedingungen äußerst kurz (z. B. etwa 10 s), die Dekohärenz gilt daher als der effizienteste bekannte physikalische Effekt. Bei makroskopischen („klassischen“) Objekten sind daher nur noch solche Zustände anzutreffen, die den Prozess der Dekohärenz schon abgeschlossen haben und ihm nicht weiter unterworfen sind. Die verbleibende inkohärente Überlagerung quantenmechanischer Zustände entspricht demnach genau den Zuständen der makroskopischen bzw. klassischen Physik. Die Dekohärenz liefert so eine quantenmechanische Erklärung für das klassische Verhalten von makroskopischen Systemen.

Die Quantenmechanik wurde zuerst noch ohne Berücksichtigung der speziellen Relativitätstheorie entwickelt. Die Schrödingergleichung ist eine Differentialgleichung erster Ordnung in der Zeit, aber zweiter Ordnung in der Raumkoordinate, sie ist also nicht relativistisch kovariant. In der relativistischen Quantenmechanik muss sie durch eine kovariante Gleichung ersetzt werden. Nach der Klein-Gordon-Gleichung, die eine partielle Differentialgleichung zweiter Ordnung in Raum und Zeit ist, setzte sich vor allem die Dirac-Gleichung durch, welche als Pendant in erster Ordnung in Raum und Zeit verstanden werden kann.

Mit der Dirac-Gleichung konnten wichtige am Elektron beobachtete physikalische Phänomene erstmals erklärt oder sogar vorhergesagt werden. Während der halbzahlige Spin in der nichtrelativistischen Quantenmechanik ad hoc als zusätzliches Konstrukt und entgegen den Regeln der Drehimpulsquantelung eingeführt werden muss, ergibt sich seine Existenz zwanglos aus der mathematischen Struktur der Dirac-Gleichung. Auch folgt aus der Dirac-Gleichung richtig, dass das magnetische Moment des Elektrons im Verhältnis zum Spin, der gyromagnetische Faktor, fast genau doppelt so groß ist wie das für eine kreisende Ladung. Auch die Feinstruktur des Wasserstoffspektrums erweist sich als ein relativistischer Effekt, der mit der Dirac-Gleichung berechnet werden kann. Eine weitere erfolgreiche Anwendung der Dirac-Gleichung ist die Beschreibung der Winkelverteilung bei der Streuung von Photonen an Elektronen, also des Compton-Effekts, durch die Klein-Nishina-Formel. Eine weitere zutreffende Folge der Dirac-Gleichung war die zu ihrer Zeit ungeheuerliche Vorhersage der Existenz eines Antiteilchens zum Elektron, des Positrons.

Trotz dieser Erfolge sind diese Theorien jedoch insofern lückenhaft, als sie die Erzeugung und Vernichtung von Teilchen nicht beschreiben können, einen bei hochrelativistischen Energien allgegenwärtigen Effekt. Als sehr fruchtbar erwies sich hier die Entwicklung der Quantenfeldtheorie. In dieser Theorie werden sowohl materielle Objekte als auch deren Wechselwirkungen durch Felder beschrieben, die gemäß bestimmten Quantisierungsregeln, wie z. B. der zweiten Quantisierung, quantisiert werden. Die Quantenfeldtheorie beschreibt nicht nur die Entstehung und Vernichtung von Elementarteilchen (Paarerzeugung, Annihilation), sondern liefert auch eine tiefere Erklärung für deren Ununterscheidbarkeit, für den Zusammenhang zwischen Spin und Statistik von Quantenobjekten sowie für die Existenz von Antiteilchen.

Die klassischen physikalischen Theorien, zum Beispiel die klassische Mechanik oder die Elektrodynamik, haben eine klare Interpretation, das heißt, den Symbolen der Theorie (Ort, Geschwindigkeit, Kraft beziehungsweise Spannungen und Felder) ist eine intuitive, klare Entsprechung in Experimenten (also eine messbare Größe) zugeordnet. Da die Quantenmechanik in ihrer mathematischen Formulierung auf sehr abstrakten Objekten, wie etwa Wellenfunktionen, basiert, ist eine Interpretation nicht mehr intuitiv möglich. Daher wurden seit dem Zeitpunkt der Entstehung der Theorie eine Reihe verschiedener Interpretationen vorgeschlagen. Sie unterscheiden sich in ihren Aussagen über die Existenz von Quantenobjekten und ihren Eigenschaften.

Die Standpunkte der meisten Interpretationen der Quantenmechanik können grob in zwei Gruppen aufgeteilt werden, die "instrumentalistische" Position und die "realistische" Position. Gemäß der instrumentalistischen Position stellt die Quantenmechanik, beziehungsweise ein auf ihrer Basis ausgearbeitetes Modell, keine Abbildung der „Realität“ dar. Vielmehr handele es sich bei dieser Theorie lediglich um einen nützlichen mathematischen Formalismus, der sich als Werkzeug zur Berechnung von Messergebnissen bewährt hat. Diese ursprünglich insbesondere von Bohr im Rahmen der Kopenhagener Interpretation vertretene pragmatische Sicht dominierte bis in die 1960er Jahre die Diskussion um die Interpretation der Quantenmechanik und prägt bis heute viele gängige Lehrbuchdarstellungen.

Neben dieser pragmatischen Variante der Kopenhagener Interpretation existiert heute eine Vielzahl alternativer Interpretationen, die bis auf wenige Ausnahmen das Ziel einer realistischen Deutung der Quantenmechanik verfolgen. In der Wissenschaftstheorie wird eine Interpretation als wissenschaftlich-realistisch bezeichnet, wenn sie davon ausgeht, dass die Objekte und Strukturen der Theorie treue Abbildungen der Realität darstellen und dass sowohl ihre Aussagen über beobachtbare Phänomene als auch ihre Aussagen über nicht beobachtbare Entitäten als (näherungsweise) wahr angenommen werden können.

In vielen Arbeiten zur Quantenphysik wird Realismus gleichgesetzt mit dem Prinzip der Wertdefiniertheit. Dieses Prinzip basiert auf der Annahme, dass einem physikalischen Objekt physikalische Eigenschaften zugeordnet werden können, die es mit einem bestimmten Wert eindeutig entweder "hat" oder "nicht hat". Beispielsweise spricht man bei der Beschreibung der Schwingung eines Pendels davon, dass das Pendel (zu einem bestimmten Zeitpunkt und innerhalb einer gegebenen Genauigkeit) eine Auslenkung "x" "hat".

In der Kopenhagener Interpretation wird die Annahme der Wertdefiniertheit aufgegeben. Ein Quantenobjekt hat demnach im Allgemeinen keine solchen Eigenschaften, vielmehr entstehen Eigenschaften erst im Moment und im speziellen Kontext der Durchführung einer Messung. Die Schlussfolgerung, dass die Wertdefiniertheit aufgegeben werden muss, ist allerdings weder aus logischer noch aus empirischer Sicht zwingend. So geht beispielsweise die (im Experiment von der Kopenhagener Interpretation nicht unterscheidbare) De-Broglie-Bohm-Theorie davon aus, dass Quantenobjekte Teilchen sind, die sich entlang wohldefinierter Bahnkurven bewegen, wobei diese Bahnen selbst aber der Beobachtung entzogen sind.

Niels Bohr formulierte 1923 das sogenannte Korrespondenzprinzip, wonach die Eigenschaften von Quantensystemen im Grenzwert großer Quantenzahlen mit hoher Genauigkeit den Gesetzen der klassischen Physik entsprechen. Dieser Grenzwert bei großen Systemen wird als „klassischer Grenzfall“ oder „Korrespondenz-Limit“ bezeichnet. Hintergrund dieses Prinzips ist, dass klassische Theorien wie die klassische Mechanik oder die klassische Elektrodynamik an makroskopischen Systemen (Federn, Kondensatoren etc.) entwickelt wurden und diese daher sehr genau beschreiben können. Daraus resultiert die Erwartung, dass die Quantenmechanik im Falle „großer“ Systeme diese klassischen Eigenschaften reproduziert beziehungsweise ihnen nicht widerspricht.

Ein wichtiges Beispiel für diesen Zusammenhang zwischen der klassischen Mechanik und der Quantenmechanik ist das Ehrenfestsche Theorem. Es besagt, dass die Mittelwerte der quantenmechanischen Orts- und Impulsobservablen eines Teilchens in guter Näherung der klassischen Bewegungsgleichung folgen, sofern die Kräfte, die auf das Teilchen wirken, nicht zu stark mit dem Ort variieren.

Das Korrespondenzprinzip ist daher ein wichtiges Hilfsmittel bei der Konstruktion und Verifikation quantenmechanischer Modellsysteme: Zum einen liefern „klassische“ Modelle mikroskopischer Systeme wertvolle heuristische Anhaltspunkte zur quantenmechanischen Beschreibung des Systems. Zum anderen kann die Berechnung des klassischen Grenzfalls zur Plausibilisierung der quantenmechanischen Modellrechnungen herangezogen werden. Sofern sich im klassischen Grenzfall physikalisch unsinnige Resultate ergeben, kann das entsprechende Modell verworfen werden.

Umgekehrt bedeutet diese Korrespondenz aber auch, dass die korrekte quantenmechanische Beschreibung eines Systems, inklusive einiger nicht-klassischer Effekte wie etwa des Tunneleffekts, oft näherungsweise mittels klassischer Begriffe möglich ist; solche Näherungen erlauben oft ein tieferes Verständnis der quantenmechanischen Systeme. Man spricht hier auch von semiklassischer Physik. Beispiele für semiklassische Beschreibungen sind die WKB-Näherung und die Gutzwillersche Spurformel.

Allerdings besitzen die oben beschriebenen Korrespondenzregeln keine universale Gültigkeit, da sie nur unter bestimmten einschränkenden Randbedingungen gelten und die Dekohärenz (siehe oben) nicht berücksichtigen. Weiterhin nähern sich nicht alle Quanteneffekte bei Anwendung der Korrespondenzregeln einem klassischen Grenzfall. Wie bereits das Schrödingers-Katze-Gedankenexperiment veranschaulicht, können „kleine“ Quanteneffekte wie z. B. der Zerfall eines radioaktiven Atoms durch Verstärker prinzipiell beliebig vergrößert werden. Zwar bewirken Dekohärenzeffekte bei makroskopischen Systemen in der Regel eine sehr effiziente Ausmittelung von Interferenzeffekten, jedoch weist auch der Zustand makroskopischer Systeme noch quantenmechanische Korrelationen auf, die z. B. in Form der sogenannten Leggett-Garg-Ungleichungen in experimentell überprüfbarer Form beschrieben werden können. Ein weiteres Beispiel für Quanteneffekte, für die keine Korrespondenzregel gilt, sind die Folgen der Ununterscheidbarkeit gleicher Teilchen, etwa die Verdoppelung der Wahrscheinlichkeit einer Ablenkung um 90° beim Stoß (neben weiteren Interferenzerscheinungen in der Winkelverteilung), ganz gleich, wie gering die Energie der Teilchen ist und wie weit entfernt voneinander sie bleiben, wenn es sich nur um zwei "gleiche" Bosonen (z. B. α-Teilchen) handelt.

Da die Gravitationskraft im Vergleich zu den anderen Grundkräften der Physik sehr schwach ist, treten allgemein-relativistische Effekte hauptsächlich bei massiven Objekten, wie z. B. Sternen oder schwarzen Löchern auf, während Quanteneffekte überwiegend bei mikroskopischen Systemen beobachtet werden. Daher gibt es nur wenige empirische Daten zu Quanteneffekten, die durch die Gravitation verursacht sind. Zu den wenigen verfügbaren experimentellen Ergebnissen gehören das Pound-Rebka-Experiment und der Nachweis diskreter gebundener Zustände von Neutronen im Gravitationsfeld.

Die oben genannten Experimente können im Rahmen der nicht-relativistischen Quantenmechanik beschrieben werden, indem für den Potentialterm der Schrödingergleichung das Gravitationspotential verwendet wird. Die Gravitation wird hier als klassisches (also nicht quantisiertes) Feld betrachtet. Eine Vereinheitlichung der Gravitation mit den übrigen drei Grundkräften der Physik, die in ihrer allgemeinsten Form als Quantenfeldtheorien formuliert sind, lässt sich auf diesem Weg also nicht erreichen. Die Vereinheitlichung der Quantentheorie mit der allgemeinen Relativitätstheorie ist ein aktuelles Forschungsthema; der aktuelle Stand ist im Artikel Quantengravitation beschrieben.

Quantenphysikalische Effekte spielen bei zahlreichen Anwendungsfällen der modernen Technik eine wesentliche Rolle. Beispiele sind der Laser, das Elektronenmikroskop, die Atomuhr oder in der Medizin die bildgebenden Verfahren auf Basis von Röntgenstrahlung bzw. Kernspinresonanz. Die Untersuchung von Halbleitern führte zur Erfindung der Diode und des Transistors, ohne die es die moderne Elektronik nicht gäbe. Auch bei der Entwicklung von Kernwaffen spielen die Konzepte der Quantenmechanik eine wesentliche Rolle.

Bei der Erfindung beziehungsweise Entwicklung dieser und zahlreicher weiterer Anwendungen kommen die Konzepte und der mathematische Formalismus der Quantenmechanik jedoch nur selten direkt zum Einsatz (eine bemerkenswerte Ausnahme sind die aktuellen Arbeiten zur Entwicklung eines Quantencomputers). In der Regel sind hierfür die anwendungsnäheren Konzepte, Begriffe und Regeln der Festkörperphysik, der Chemie, der Materialwissenschaften oder der Kernphysik von größerer praktischer Bedeutung. Die Relevanz der Quantenmechanik ergibt sich hingegen aus der überragenden Bedeutung, die diese Theorie bei der Formulierung des theoretischen Fundamentes vieler wissenschaftlicher Disziplinen hat.

Im Folgenden sind einige Beispiele für Anwendungen der Quantenmechanik beschrieben:

Die chemischen Eigenschaften aller Stoffe sind ein Ergebnis der elektronischen Struktur der Atome und Moleküle, aus denen sie aufgebaut sind. Grundsätzlich lässt sich diese elektronische Struktur durch Lösung der Schrödingergleichung für alle involvierten Atomkerne und Elektronen quantitativ berechnen. Eine exakte analytische Lösung ist jedoch nur für den Spezialfall der wasserstoffähnlichen Systeme – also Systeme mit einem Atomkern und einem Elektron – möglich. Bei komplexeren Systemen – also in praktisch allen realen Anwendungen in der Chemie oder der Biologie – kann die Vielteilchen-Schrödingergleichung daher nur unter Verwendung von numerischen Methoden gelöst werden. Diese Berechnungen sind bereits für einfache Systeme sehr aufwändig. Beispielsweise dauerte die Ab-initio-Berechnung der Struktur und des Infrarot-Spektrums von Propan mit einem marktgängigen PC im Jahr 2010 einige Minuten, die entsprechende Berechnung für ein Steroid bereits mehrere Tage. Daher spielen in der theoretischen Chemie Modellvereinfachungen und numerische Verfahren zur effizienten Lösung der Schrödingergleichung eine große Rolle, und die Entwicklung entsprechender Verfahren hat sich zu einer eigenen umfangreichen Disziplin entwickelt.

Ein in der Chemie besonders häufig verwendetes, stark vereinfachtes Modell ist das Orbitalmodell. Bei diesem Modell wird der Vielteilchenzustand der Elektronen der betrachteten Atome durch eine Summe der Einteilchenzustände der Elektronen gebildet. Das Modell beinhaltet verschiedene Näherungen (unter anderem: Vernachlässigung der Coulomb-Abstoßung der Elektronen untereinander, Entkopplung der Bewegung der Elektronen von der Kernbewegung), erlaubt jedoch eine näherungsweise korrekte Beschreibung der Energieniveaus des Atoms. Der Vorteil dieses Modells liegt neben der vergleichsweise einfachen Berechenbarkeit insbesondere in der anschaulichen Aussagekraft sowohl der Quantenzahlen als auch der grafischen Darstellung der Orbitale.

Das Orbitalmodell erlaubt die Klassifizierung von Elektronenkonfigurationen nach einfachen Aufbauregeln (Hundsche Regeln). Auch die Regeln zur chemischen Stabilität (Oktettregel bzw. Edelgasregel, Magische Zahlen) und die Systematik des Periodensystems der Elemente lassen sich durch dieses quantenmechanische Modell rechtfertigen.

Durch Linearkombination mehrerer Atom-Orbitale lässt sich die Methode auf sogenannte Molekülorbitale erweitern, wobei Rechnungen in diesem Fall wesentlich aufwändiger werden, da Moleküle keine Kugelsymmetrie aufweisen. Die Berechnung der Struktur und der chemischen Eigenschaften komplexer Moleküle auf Basis von Näherungslösungen der Schrödingergleichung ist der Gegenstand der Molekularphysik. Dieses Gebiet legte den Grundstein für die Etablierung der Quantenchemie beziehungsweise der Computerchemie als Teildisziplinen der theoretischen Chemie.

Die Kernphysik ist ein weiteres großes Anwendungsgebiet der Quantentheorie. Atomkerne sind aus Nukleonen zusammengesetzte Quantensysteme mit einer sehr komplexen Struktur. Bei ihrer theoretischen Beschreibung kommen – abhängig von der konkreten Fragestellung – eine Reihe konzeptionell sehr unterschiedlicher Kernmodelle zur Anwendung, die in der Regel auf der Quantenmechanik oder der Quantenfeldtheorie basieren.
Im Folgenden sind einige wichtige Anwendungsfälle der Quantenmechanik in der Kernphysik aufgeführt:


Die Vielzahl prinzipiell möglicher chemischer Zusammensetzungen von kondensierter Materie – also von makroskopischer Materie im festen oder flüssigen Zustand – und die große Anzahl an Atomen, aus welchen kondensierte Materie besteht, spiegelt sich in einer großen Vielfalt von Materialeigenschaften wider (siehe Hauptartikel Materie). Die meisten dieser Eigenschaften lassen sich nicht im Rahmen der klassischen Physik beschreiben, während sich quantenmechanische Modelle kondensierter Materie als überaus erfolgreich erwiesen haben.

Aufgrund der großen Anzahl beteiligter Teilchen ist eine direkte Lösung der Schrödingergleichung für alle mikroskopischen Komponenten eines makroskopischen Stückes Materie unpraktikabel. Stattdessen werden Modelle und Lösungsverfahren angewendet, die an die zugrundeliegende Materiegattung (Metall, Halbleiter, Ionenkristall etc.) und an die zu untersuchenden Eigenschaften angepasst sind. In den gängigen Modellen kondensierter Materie sind Atomkerne und Elektronen die relevanten Grundbausteine kondensierter Materie. Hierbei werden in der Regel Atomkerne und innere Elektronen zu einem Ionenrumpf zusammengefasst, wodurch sich die Anzahl der im Modell zu berücksichtigenden Komponenten und Wechselwirkungen stark reduziert. Von den 4 Grundkräften der Physik wird lediglich die elektromagnetische Wechselwirkung berücksichtigt, die Gravitation und die Kernkräfte sind hingegen für die in der Physik kondensierter Materie betrachteten Effekte und Energieskalen irrelevant.

Trotz dieser Vereinfachungen handelt es sich bei Modellen kondensierter Materie um komplexe quantenmechanische Vielteilchenprobleme, wobei insbesondere die Berücksichtigung der Elektron-Elektron-Wechselwirkung eine Herausforderung darstellt. Für viele Anwendungszwecke, wie z. B. die Berechnung der Ladungsverteilung, des Phononenspektrums oder der strukturellen Eigenschaften, ist die Berechnung des elektronischen Grundzustandes ausreichend. In diesem Fall kann das elektronische Vielteilchenproblem unter Anwendung der Dichtefunktionaltheorie oder anderer Verfahren als ein effektives Einteilchenproblem umformuliert werden, welches heute routinemäßig auch für komplexe Systeme berechnet werden kann.

Häufig sind neben den Grundzustandseigenschaften auch die elementaren Anregungen kondensierter Materie von Interesse. Beispielsweise basieren alle experimentellen Methoden der Festkörperspektroskopie auf dem Prinzip, dass durch einen externen Stimulus (z. B. Licht oder Neutronen) bestimmte Freiheitsgrade einer Probe angeregt bzw. abgeregt werden. Bei den elementaren Anregungen handelt es sich um kollektive quantenmechanische Effekte, denen – ähnlich einem freien Quantenobjekt – eine Energie und eine Wellenlänge bzw. ein Wellenvektor zugeordnet werden kann, weshalb sie auch als Quasiteilchen bezeichnet werden. Beispiele sind das Phonon (Energiequant der Gitterschwingung), oder das Exciton (Elektron-Loch-Paar). Quasiteilchen verschiedener Typen können miteinander wechselwirken und so aneinander streuen oder sich verbinden und neue Quantenobjekte mit Eigenschaften bilden, die sich drastisch von den Eigenschaften freier Elektronen unterscheiden. Ein bekanntes Beispiel sind die Cooper-Paare, die gemäß der BCS-Theorie die Supraleitung von Metallen ermöglichen.

Von aktuellem Interesse ist die Suche nach robusten Methoden zur direkten Manipulation von Quantenzuständen. Es werden zurzeit größere Anstrengungen unternommen, einen Quantencomputer zu entwickeln, welcher durch Ausnutzung der verschiedenen Eigenzustände und der Wahrscheinlichkeitsnatur eines quantenmechanischen Systems hochparallel arbeiten würde. Einsatzgebiet eines solchen Quantenrechners wäre beispielsweise das Knacken moderner Verschlüsselungsmethoden. Im Gegenzug hat man mit der Quantenkryptographie ein System zum theoretisch absolut sicheren Schlüsselaustausch gefunden, in der Praxis ist diese Methode häufig etwas abgewandelt und unsicherer, da es hier auch auf die Übertragungsgeschwindigkeit ankommt.
Ein weiteres aktuelles Forschungsgebiet ist die Quantenteleportation, die sich mit Möglichkeiten zur Übertragung von Quantenzuständen über beliebige Entfernungen beschäftigt.

Zwei Jahre nach den ersten Veröffentlichungen hatte sich die Quantenmechanik in der "Kopenhagener Interpretation" durchgesetzt. Als wichtiger Meilenstein gilt die fünfte Solvay-Konferenz im Jahr 1927. Rasch erlangte die Theorie den Status einer zentralen Säule im Theoriengebäude der Physik. Im Hinblick auf ihre Leistungsfähigkeit bei konkreten Anwendungen (jedoch nicht im Hinblick auf ihre Interpretation, siehe oben) ist die Quantenmechanik bis heute praktisch unumstritten. Zwar existieren eine Reihe alternativer, empirisch nicht-äquivalenter Theorien, wie die Familie der Dynamischer-Kollaps-Theorien oder die Nichtgleichgewichts-Versionen der De-Broglie-Bohm-Theorie, jedoch haben diese Theorien gegenüber der Quantenmechanik nur eine marginale Bedeutung.

Für die Entwicklung der Quantenmechanik wurden mehrere Nobelpreise der Physik vergeben:

Hinzu kam eine Reihe weiterer Nobelpreise für Weiterentwicklungen und Anwendungen der Quantenmechanik sowie für die Entdeckung von Effekten, die nur im Rahmen der Quantenmechanik erklärt werden können (siehe Liste der Nobelpreisträger für Physik). Auch einige Nobelpreise für Chemie wurden für erfolgreiche Anwendungen der Quantenmechanik vergeben, darunter die Preise an Robert Mulliken (1929, „für seine grundlegenden Arbeiten über die chemischen Bindungen und die Elektronenstruktur der Moleküle mit Hilfe der Orbital-Methode“), an Walter Kohn (1998, „für seine Entwicklung quantenchemischer Methoden“) oder an John Anthony Pople (1998, „für die Entwicklung von Methoden, mit denen die Eigenschaften von Molekülen und deren Zusammenwirken in chemischen Prozessen theoretisch erforscht werden können“).

Bereits kurz nach Begründung der Quantenmechanik veröffentlichten verschiedene Quantenphysiker, z. B. Born, de Broglie, Heisenberg oder Bohr, eine Reihe semi-populärwissenschaftlicher Bücher, die sich insbesondere mit philosophischen Aspekten der Theorie befassten. Der Physiker G. Gamov veranschaulichte in seinem Buch "Mr. Tompkins Explores the Atom" die Eigenschaften von Quantenobjekten, indem er seinen Protagonisten verschiedene Abenteuer in einer fiktiven Quantenwelt erleben lässt. Auch die 1964 veröffentlichten "Feynman-Vorlesungen über Physik", echte Lehrbücher, aber für die damalige Zeit sensationell anregend geschrieben, wurden in hohen Stückzahlen verkauft. Allerdings erreichten Publikationen über die Quantenmechanik bis in die 1970er Jahre bei weitem nicht das Maß an öffentlicher Wahrnehmung, das beispielsweise der Relativitätstheorie und der Kosmologie zuteilwurde. Weiterhin prägten die praktischen Auswirkungen der Kernphysik, insbesondere die Risiken von Kernwaffen und Kernenergie, die öffentliche Diskussion über die moderne Physik.

Auch in Film und Fernsehen wurde die Quantenmechanik gelegentlich in populärwissenschaftlicher Form dargestellt, z. B. in Sendungen des Physikers Harald Lesch.

Mit dem Aufkommen der New-Age-Gegenkultur ab Anfang der 1970er Jahre entstand ein verstärktes Interesse an Literatur mit aus der Wissenschaft entlehnten Ausdrücken, in der Verbindungen zwischen der Quantenmechanik, dem menschlichen Bewusstsein und fernöstlicher Religion hergestellt wurden. Bücher wie F. Capras "Tao der Physik" oder G. Zukavs "Dancing Wu Li Masters" wurden Bestseller. Die Quantenmechanik – so eine Kernaussage dieser Bücher – enthalte holistische und mystische Implikationen, die eine Verbindung von Spiritualität, Bewusstsein und Physik zu einem „organischen“ Weltbild nahelegten.

Ab den 1980er Jahren erlebte der Markt für quantenmechanisch inspirierte Literatur einen weiteren kräftigen Aufschwung, und das Wort „Quanten“ entwickelte sich zu einem in vielen Komposita verwendeten Modewort. Die veröffentlichten Bücher umfassten ein breites Themenspektrum, welches von allgemeinverständlichen Darstellungen über weitere Bücher zu dem Themenkomplex „Quantenmechanik und Bewusstsein“ bis hin zu Themen wie dem „Quantum Learning“, „Quantum Golf“ oder den „Quantum Carrots“ reichte. Ein bekanntes Beispiel für die Erweiterung quantenmechanischer Konzepte auf Bereiche jenseits ihrer Anwendbarkeit ist der Film "What the Bleep do we (k)now!?".

Die Literaturwissenschaftlerin Elizabeth Leane kommt zu einer zwiespältigen Bewertung des Genres. Einerseits misst sie ihm pädagogische Bedeutung bei der allgemeinverständlichen Darstellung von Wissenschaft zu. Andererseits weist sie auf das Problem von Bedeutungsverschiebungen hin, die durch die Verwendung von Metaphern und „fiktionalen Techniken“ erzeugt werden. Am Beispiel von Zukavs "Dancing Wu Li Masters", einem der meistverkauften und am häufigsten zitierten Bücher, die Quantenmechanik und Esoterik verquicken, zeigt sie eine rhetorische Umdeutung der Quantenmechanik zur Unterstützung eines anthropozentrischen Weltbildes auf. Der Soziologe S. Restivo weist auf prinzipielle linguistische und konzeptionelle Probleme bei Versuchen hin, Quantenmechanik umgangssprachlich zu beschreiben und mit Mystik zu verbinden. Viele Physiker, etwa J. S. Bell, M. Gell-Mann oder V. Stenger, lehnen Hypothesen, die Verbindungen zwischen Quantenmechanik und Bewusstsein herstellen, als spekulativ ab. Einen neuen Anlauf hierzu legte im Jahr 2015 der Politikwissenschaftler Alexander Wendt mit dem Buch "Quantum Mind and Social Science" vor.

Die Quantenmechanik wurde und wird in der Kunst, insbesondere in der Belletristik, aber auch in der bildenden Kunst und punktuell im Theater, wahrgenommen und künstlerisch verarbeitet.

Die Literaturwissenschaftlerin E. Emter weist Rezeptionsspuren der Quantentheorie in Texten von R. Musil "(Der Mann ohne Eigenschaften)," H. Broch, E. Jünger, G. Benn, Carl Einstein und B. Brecht nach, wobei sich ihre Studie auf den deutschen Sprachraum und die Jahre 1925 bis 1970 beschränkt.

In den letzten Jahren erlangten Arbeiten von Bildhauern Aufmerksamkeit, die Quantenobjekte als Skulpturen darstellen. Der Bildhauer J. Voss-Andreae geht davon aus, dass Kunst, die nicht an die Textform gebunden ist, Möglichkeiten zur Darstellung von Realität hat, die der Wissenschaft nicht zur Verfügung stehen. Ein Beispiel ist seine Skulptur "Quantum Man" (siehe Abbildung rechts), die von Kommentatoren als Symbolisierung des Welle-Teilchen-Dualismus und der Beobachterperspektive interpretiert wird. Weitere bekannte Beispiele für künstlerische Darstellungen von Quantenobjekten sind die Skulpturen "Quantum Corral" und die "Spin Family" desselben Künstlers sowie die "Quantum Cloud" von A. Gormley.

Auch einige Theaterstücke thematisieren die Quantenmechanik, so z. B. Tom Stoppards Bühnenstück "Hapgood" oder das Stück "QED" des US-amerikanischen Dramatikers P. Parnell. In seinem Bühnenstück "Kopenhagen" überträgt der Schriftsteller M. Frayn das Heisenbergsche Unschärfeprinzip in ein Unschärfeprinzip des menschlichen Verhaltens.



Atomphysik und theoretische Chemie:

Kernphysik:

Physik kondensierter Materie:

Quanteninformatik:






</doc>
<doc id="4181" url="https://de.wikipedia.org/wiki?curid=4181" title="Quark">
Quark

Quark steht für:

Siehe auch:


</doc>
<doc id="4183" url="https://de.wikipedia.org/wiki?curid=4183" title="Quadratur des Kreises">
Quadratur des Kreises

Die Quadratur des Kreises ist ein klassisches Problem der Geometrie. Die Aufgabe besteht darin, aus einem gegebenen Kreis in endlich vielen Schritten ein Quadrat mit demselben Flächeninhalt zu konstruieren. Sie ist äquivalent zur sogenannten Rektifikation des Kreises, also der Konstruktion einer geraden Strecke, die dem Kreisumfang entspricht. Das wiederum entspricht der Konstruktion der Kreiszahl formula_1 aus der Strecke 1. Beschränkt man die Konstruktionsmittel auf Lineal und Zirkel, so ist die Aufgabe aufgrund der Transzendenz von formula_1 unlösbar. Das konnte im Jahr 1882 vom deutschen Mathematiker Ferdinand von Lindemann bewiesen werden.

Die Quadratur des Kreises gehört zu den populärsten Problemen der Mathematik. Jahrhundertelang suchten neben Mathematikern auch immer wieder Laien vergeblich nach einer Lösung. Der Begriff "Quadratur des Kreises" ist in vielen Sprachen zu einer Metapher für eine unlösbare Aufgabe geworden.

Bereits in den altorientalischen Hochkulturen gab es Verfahren zur Berechnung von Kreisflächen. Beispielsweise wird in einer Problemstellung des Papyrus Rhind (um 1650 v. Chr.) die Fläche eines Kreises vom Durchmesser 9 über ein (unregelmäßiges) Achteck angenähert – indem beide einem Quadrat der Seitenlänge 9 einbeschrieben werden, das in 9 kleinere Quadrate zerlegt wird – und darüber der Fläche eines Quadrats der Seitenlänge 8 gleichgesetzt, was einem recht genauen Wert für die Kreiszahl π von 3 + / + / + / = 3,16… entspricht. Derartige Musterlösungen waren aus der Praxis gewonnen und für die Praxis bestimmt, es gab keine weitergehenden theoretischen Überlegungen, insbesondere wurde kein Unterschied zwischen exakter Lösung und Näherung gemacht.

Eine deduktive Vorgehensweise in der Mathematik, bei der durch Beweise gestützte Sätze die Musteraufgaben ersetzen, entwickelte sich ab dem 6. Jahrhundert v. Chr. in Griechenland. Ansatzweise ist sie schon bei Thales von Milet, deutlicher in der von Pythagoras von Samos gegründeten Schule der Pythagoreer zu erkennen. Mit der gemeinhin dem Pythagoreer Hippasos von Metapont zugeschriebenen Entdeckung inkommensurabler Strecken im späten 6. oder frühen 5. Jahrhundert stellte sich heraus, dass es konstruierbare Objekte gibt (beispielsweise die Diagonale eines Quadrats), die nicht als ganzzahliges Verhältnis darstellbar sind. Als Folge dieser Entdeckung trat die Arithmetik zugunsten der Geometrie in den Hintergrund, Gleichungen mussten jetzt geometrisch gelöst werden, etwa durch Aneinanderlegung von Figuren und Überführung verschiedener Figuren in Rechtecke oder Quadrate. Aus dem späten 5. Jahrhundert stammen die drei klassischen Konstruktionsprobleme der antiken Mathematik, neben der Quadratur des Kreises noch die Aufgabe der Dreiteilung des Winkels und das Delische Problem der Verdoppelung des Würfels.

Eine Beschränkung der Konstruktionsmittel auf Zirkel und Lineal wurde dabei nicht generell gefordert. Während der Beschäftigung mit den klassischen Problemen wurden schon früh Lösungen gefunden, die auf weitergehenden Hilfsmitteln basieren. Allerdings kristallisierte sich im Lauf der Zeit eine Haltung heraus, die eine möglichst weitgehende Beschränkung verlangt. Spätestens bei Pappos war diese weitestgehende Beschränkung zur Maßregel geworden.

Als einer der ersten soll dem griechischen Schriftsteller Plutarch zufolge der Philosoph Anaxagoras „im Gefängnis die Quadratur des Kreises aufgeschrieben (oder: gezeichnet, "")“ haben, nähere Angaben zu Anaxagoras’ Konstruktion macht Plutarch nicht. Ein Gefängnisaufenthalt des Anaxagoras wäre auf etwa 430 v. Chr. zu datieren, als der Philosoph in Athen wegen Gottlosigkeit angeklagt war und schließlich fliehen musste. Ausführlichere Quellen zu den Anfängen der Forschung sind hauptsächlich spätantike Kommentare zu Werken des Aristoteles, Schriften also, die mit einer zeitlichen Distanz von rund 900 Jahren entstanden sind. Dementsprechend unsicher sind zeitliche Reihenfolge und genaue Gedankengänge der ersten Ansätze. Die wichtigsten Arbeiten des 5. Jahrhunderts v. Chr. stammen von Hippokrates von Chios, Antiphon, Bryson von Herakleia und Hippias von Elis.
Die Überführung von Dreiecken in Rechtecke, von Rechtecken in Quadrate (Quadratur des Rechtecks) oder die Addition zweier Quadrate (Satz des Pythagoras) war mit den bekannten geometrischen Sätzen bereits damals elementar zu bewältigen. Die grundlegende Frage, ob auch krummlinig begrenzte Flächen exakt in Quadrate überführt werden können, konnte um 440 v. Chr. von Hippokrates von Chios positiv beantwortet werden. Ausgehend von dem bei ihm noch als Axiom benutzten Satz, dass sich die Flächen ähnlicher Kreissegmente wie die Quadrate über ihren Sehnen verhalten, gelang es Hippokrates, von Kreisbögen begrenzte Flächen, die sogenannten „Möndchen des Hippokrates“, zu quadrieren. Die Quadratur des Kreises ist auf diese Weise jedoch nicht zu erreichen, da nur bestimmte Möndchen – zum Beispiel die über der Seite des Quadrats, nicht jedoch die über der Seite eines regelmäßigen Sechsecks – quadrierbar sind.

Da Dreiecke (und damit beliebige Vielecke) in ein Quadrat übergeführt werden konnten, war ein zweiter Ansatz, ein dem Kreis flächengleiches Polygon zu konstruieren. Antiphon hatte die Idee, den Kreis durch einbeschriebene Vielecke anzunähern. Bryson von Herakleia verfeinerte dieses Vorgehen, indem er den Kreis zusätzlich durch umbeschriebene Vielecke näherte und einen Zwischenwert bildete.

Hippias von Elis entwickelte etwa 425 v. Chr. zur Lösung der Winkeldreiteilung eine Kurve, die mechanisch durch die Überlagerung einer kreisförmigen mit einer linearen Bewegung erzeugt wurde. Gut hundert Jahre später entdeckte Deinostratos, dass mithilfe dieser Kurve, der sogenannten Quadratrix, die Strecke der Länge formula_3 – und damit mithilfe weiterer elementarer Konstruktionen ein Quadrat der Fläche π – konstruiert werden kann. Da die Quadratrix selbst jedoch eine sogenannte "transzendente Kurve" ist, also nicht mit Zirkel und Lineal zu erzeugen, war die Lösung im strengen Sinne damit nicht erreicht.

Eine ausführliche Abhandlung mit dem Titel "Kreismessung" ist von Archimedes überliefert. Archimedes beweist in dieser Arbeit drei grundlegende Sätze:


Mit dem ersten Satz ist das Problem der Quadratur des Kreises auf die Frage nach der Konstruierbarkeit des Umfangs eines Kreises aus dem vorgegebenen Radius und damit auf die Konstruierbarkeit von π zurückgeführt. Im dritten Satz gibt Archimedes gleich eine ebenso einfache wie genaue Näherung dieser Zahl an, nämlich /, ein Wert (≈ 3,143), der für praktische Zwecke noch heute Verwendung findet. Der zweite Satz ist ein einfaches Korollar aus den beiden anderen; dass sich die Fläche eines Kreises proportional zum Quadrat seines Durchmessers verhält, war bereits Euklid bekannt, Archimedes gibt hier den Wert der Proportionalitätskonstanten an.

Zum Beweis seiner Aussagen greift Archimedes die Brysonsche Idee der beliebigen Annäherung des Kreises durch ein- und umbeschriebene regelmäßige Polygone auf. Ausgehend vom einbeschriebenen Sechseck und umbeschriebenen Dreieck gelangt Archimedes durch sukzessive Verdoppelung der Seitenzahl jeweils beim 96-Eck an. Eine geschickte Abschätzung der in den einzelnen Rechenschritten auftretenden Quadratwurzeln ergibt die in Satz 3 genannten Schranken.

In einer weiteren Arbeit "Über Spiralen" beschreibt Archimedes die Konstruktion der später nach ihm benannten archimedischen Spirale, die ähnlich wie Hippias’ Quadratrix durch die Überlagerung einer kreisförmigen mit einer linearen Bewegung gewonnen wird. Er zeigt, dass durch das Anlegen der Tangente an diese Spirale der Umfang eines Kreises auf einer Geraden abgetragen werden kann. Auf die damit geleistete Quadratur des Kreises weisen spätere Kommentatoren hin, Archimedes selbst macht dazu keine Aussage. Wie bei der Quadratrix sind weder die Spirale selbst noch ihre Tangente mit Zirkel und Lineal konstruierbar.

Infolge eines verstärkten Interesses für die antike Mathematik im christlichen Europa ab etwa dem 11. Jahrhundert entstanden etliche Abhandlungen über die Quadratur des Kreises, jedoch ohne dass dabei wesentliche Beiträge zur eigentlichen Lösung geleistet wurden. Als Rückschritt zu betrachten ist, dass im Mittelalter der Archimedische Näherungswert von / für die Kreiszahl lange Zeit als exakt galt.

Einer der ersten Autoren des Mittelalters, der das Problem der Kreisquadratur wiederaufnahm, war Franco von Lüttich. Um 1050 entstand sein Werk "De quadratura circuli." Franco stellt darin zunächst drei Quadraturen vor, die er verwirft. Die ersten beiden geben für die Seitenlänge des Quadrates / beziehungsweise für die Diagonale / des Kreisdurchmessers an, was relativ schlechten Näherungen von 3/ und 3/ für π entspricht. Der dritte Vorschlag wiederum setzt den Umfang des Quadrates dem Kreisumfang gleich, verlangt also die Rektifikation des letzteren.

Francos eigene Lösung geht von einem Kreis mit Durchmesser 14 aus. Dessen Fläche beträgt aus seiner Sicht genau 7² × / = 154. Rechnerisch lässt sich nach Francos Argumentation kein flächengleiches Quadrat finden, da die Quadratwurzel aus / irrational ist, konstruktiv jedoch schon. Dazu zerlegt er den Kreis in 44 gleiche Sektoren, die er zu einem Rechteck der Seitenlängen 11 und 14 zusammenfügt. Den nötigen Kunstgriff, bei dem er die Kreissektoren durch rechtwinklige Dreiecke mit Katheten der Länge 1 und 7 ersetzt, erläutert Franco allerdings nicht. Problematisch ist auch sein nicht ganz geglückter Versuch, das Rechteck anschließend durch eine geeignete Zerlegung in ein Quadrat zu überführen. Offensichtlich war Franco das althergebrachte griechische Verfahren nicht geläufig.

Spätere Abhandlungen der Scholastik erschöpfen sich mehr oder minder in einer Abwägung der Argumente der bekannten Klassiker. Erst mit der Verbreitung lateinischer Übersetzungen der archimedischen Schriften im Spätmittelalter wurde der Wert / wieder als Näherung erkannt und nach neuen Lösungen des Problems gesucht, so beispielsweise von Nikolaus von Kues. Dieser griff die Idee, den Kreis durch eine Folge regelmäßiger Vielecke mit wachsender Seitenzahl anzunähern, wieder auf, suchte im Gegensatz zu Archimedes jedoch nicht den Kreisumfang, sondern den Kreisradius bei vorgegebenem gleichbleibendem Umfang der Polygone zu bestimmen. In einem Brief gab von Kues eine solche Lösung, die er für genau hielt, an. Der daraus ermittelte Wert für die Kreiszahl liegt auch immerhin zwischen den von Archimedes gegebenen Grenzen. Die eigentlichen cusanischen Arbeiten zum Thema liefern deutlich schlechtere Näherungen und wurden damit zum Ziel einer Streitschrift des Regiomontanus, der die Ungenauigkeit der Berechnungen nachwies und die Beweise „als philosophische, aber nicht als mathematische“ bezeichnete.

Fortschritte in der Kreisberechnung brachten ab dem 16. Jahrhundert die Weiterentwicklung des archimedischen Näherungsverfahrens sowie das Aufkommen moderner analytischer Methoden.

Bei der ursprünglichen Methode des Archimedes wird der Kreisumfang durch den Umfang eines dem Kreis einbeschriebenen und den eines dem Kreis umbeschriebenen Vielecks abgeschätzt. Genauere Schranken ergeben sich durch eine Erhöhung der Eckenzahl. Der niederländische Mathematiker Willebrord van Roijen Snell (Snellius) fand heraus, dass auch, ohne die Seitenzahl zu vergrößern, feinere Schranken für die Länge eines Bogenstückes als nur die Sehnen der Polygone angegeben werden können. Er konnte dieses Ergebnis allerdings nicht streng beweisen. Die Ausarbeitung und Verbesserung des snelliusschen Ansatzes leistete Christiaan Huygens in seiner Arbeit "De circuli magnitudine inventa", in der er auch den Beweis der von Snellius aufgestellten Sätze erbrachte. Auf rein elementargeometrischem Weg gelang Huygens eine so gute Eingrenzung der zwischen Vieleck und Kreis liegenden Fläche, dass er bei entsprechender Seitenzahl der Polygone die Kreiszahl auf mindestens dreimal so viel Stellen genau erhielt wie Archimedes mit seinem Verfahren.

Der rein geometrische Ansatz zur Bestimmung der Kreiskonstanten war mit Huygens Arbeit im Wesentlichen ausgeschöpft. Bessere Näherungen ergaben sich mithilfe von unendlichen Reihen, speziell der Reihenentwicklung trigonometrischer Funktionen. Zwar hatte François Viète schon Ende des 16. Jahrhunderts durch die Betrachtung bestimmter Streckenverhältnisse aufeinanderfolgender Polygone eine erste exakte Darstellung von π durch ein unendliches Produkt gefunden, doch erwies sich diese Formel als unhandlich. Eine einfachere Reihe, die darüber hinaus nur mit rationalen Operationen auskommt, stammt von John Wallis, eine weitere Darstellung der Kreiszahl als Kettenbruch von William Brouncker. Wichtiger für die Praxis war die von James Gregory und davon unabhängig von Gottfried Wilhelm Leibniz gefundene Reihe für den Arcustangens. Obwohl diese Reihe selbst nur langsam konvergiert, kann man aus ihr andere Reihen ableiten, die sich wiederum sehr gut zur Berechnung der Kreiszahl eignen. Anfang des 18. Jahrhunderts waren mithilfe solcher Reihen über 100 Stellen von π berechnet, neue Erkenntnisse über das Problem der Kreisquadratur konnten dadurch allerdings nicht gewonnen werden.

Zur Lösung des Problems bedurfte es zum einen der Möglichkeit, dem geometrischen Begriff „konstruierbar“ eine algebraische Bedeutung zu geben, zum anderen genauerer Einsicht der Eigenschaften der Kreiszahl.

Eine geometrische Konstruktion mit Zirkel und Lineal geht von einer endlichen Anzahl vorgegebener Punkte aus und ermittelt in einer endlichen Anzahl von Schritten neue Punkte durch das Schneiden zweier Geraden, zweier Kreise oder einer Geraden mit einem Kreis. Die Übersetzung dieser Vorgehensweise in die Sprache der Algebra gelang durch die Einführung von Koordinatensystemen im Rahmen der im 17. Jahrhundert hauptsächlich von Pierre de Fermat und René Descartes entwickelten analytischen Geometrie. Geraden und Kreise konnten mit den neuen Mitteln durch Gleichungen beschrieben, Schnittpunkte durch das Lösen von Gleichungssystemen bestimmt werden. Es stellte sich heraus, dass die mit Zirkel und Lineal konstruierbaren Streckenlängen genau die sind, die sich durch eine endliche Zahl von rationalen Operationen (Addition, Subtraktion, Multiplikation und Division) sowie einer endlichen Anzahl von Quadratwurzeln aus einer vorgegebenen Länge ableiten lassen. Insbesondere sind diese Längen algebraische Zahlen, also eine Teilmenge der Zahlen, die eine Lösung einer algebraischen Gleichung beliebigen Grades mit rationalen Koeffizienten sind. Zahlen, die nicht algebraisch sind, heißen transzendent und sind nicht konstruierbar.

Ausgangspunkt für die weiteren Untersuchungen der Kreiszahl waren einige grundlegende Erkenntnisse Leonhard Eulers, die dieser 1748 in seinem Werk "Introductio in analysin infinitorum" veröffentlicht hatte. Euler stellte unter anderem mit der bekannten Formel
erstmals einen Zusammenhang zwischen trigonometrischen Funktionen und der Exponentialfunktion her und lieferte darüber hinaus einige Kettenbruch- und Reihendarstellungen von π und der später nach ihm benannten eulerschen Zahl "e".

Diese Vorarbeit machte sich Johann Heinrich Lambert zunutze, der mithilfe einer der eulerschen Kettenbruchentwicklungen 1766 erstmals zeigen konnte, dass "e" und π irrationale, also nicht durch einen ganzzahligen Bruch darstellbare Zahlen sind. Eine kleine Lücke in Lamberts Beweisführung wurde 1806 von Adrien-Marie Legendre geschlossen, der gleichzeitig den Irrationalitätsbeweis für formula_5 erbrachte.

Die Vermutung, dass π nicht algebraisch sein könne, stand jetzt im Raum, wurde zumindest von Euler, Lambert und Legendre ausgesprochen. Dabei war bis zur Mitte des 19. Jahrhunderts noch nicht klar, dass es überhaupt transzendente Zahlen geben musste. Dieser Nachweis gelang 1844/1851 Joseph Liouville durch explizite Konstruktion von transzendenten liouvilleschen Zahlen.

Ferdinand von Lindemann konnte 1882 schließlich beweisen, dass π nicht algebraisch, sondern transzendent ist. Deshalb ist π in gerader Linie nicht konstruierbar und die Quadratur des Kreises unmöglich.

Lindemann griff in seiner Arbeit auf ein Ergebnis des französischen Mathematikers Charles Hermite zurück. Dieser hatte 1873 gezeigt, dass die eulersche Zahl "e" transzendent ist. Darauf aufbauend konnte Lindemann den sogenannten Satz von Lindemann-Weierstraß beweisen, der besagt, dass für beliebige, voneinander verschiedene algebraische Zahlen formula_6 und für beliebige algebraische Zahlen formula_7 die Gleichung
nur dann gelten kann, wenn alle formula_9 den Wert Null haben. Insbesondere kann für keine von Null verschiedene algebraische Zahl "z" der Ausdruck formula_10 eine rationale Zahl ergeben. Nach dieser Vorbereitung konnte Lindemann die Annahme, π sei algebraisch, mithilfe der eulerschen Identität formula_11 zum Widerspruch führen; π musste somit transzendent sein.

Lindemanns Beweis für die Transzendenz von π wurde in den folgenden Jahren und Jahrzehnten noch wesentlich vereinfacht, so etwa durch David Hilbert im Jahre 1893.

Die Quadratur des Kreises erreichte wie nur wenige andere Fragestellungen auch außerhalb der Mathematik eine große Popularität. Als Folge versuchten sich viele mathematische Laien an der Lösung des einfach erscheinenden Problems; etliche glaubten, sie gefunden zu haben.

Als frühester Beleg für das Auftauchen eines sogenannten „Kreisquadrierers“ oder „Quadrators“ wird gelegentlich eine Stelle in Aristophanes’ Komödie "Die Vögel" zitiert, in der Meton als Vermesser auftritt und den Grundriss einer neuen Stadt mit geometrischen Hilfsmitteln so festlegen will, dass „der Kreis ein Viereck werde“. Gemeint ist damit jedoch nicht die Quadratur eines Kreises, sondern das Anlegen zweier rechtwinklig aufeinandertreffender Straßen, auch wenn der Ausdruck wie eine Anspielung auf die Kreisquadratur erscheint.

Berichte über ein wachsendes Aufkommen an Amateurarbeiten ab dem 18. und 19. Jahrhundert und Beispiele zum Thema finden sich bei Jean-Étienne Montucla, Johann Heinrich Lambert und Augustus de Morgan. In der Regel handelte es sich um Verfahren, bei denen das Problem mechanisch, numerisch oder durch eine geometrische Näherungskonstruktion „exakt“ gelöst wurde. Derartige Arbeiten wurden in einer derart großen Zahl an Mathematiker oder wissenschaftliche Institutionen herangetragen, dass sich zum Beispiel die Pariser Akademie der Wissenschaften 1775 genötigt sah, die weitere Untersuchung von vorgeblichen Lösungen der Kreisquadratur offiziell abzulehnen:

Auch nach dem lindemannschen Unmöglichkeitsbeweis wurden immer noch vermeintliche Quadraturen veröffentlicht. In jüngerer Zeit sind die vergeblichen Versuche der Amateurmathematiker Stoff der Unterhaltungsmathematik geworden.

Ein Hauptgrund für die gerade für mathematische Laien hohe Attraktivität ist wohl die sehr elementare Problemstellung, die auch ohne tiefergehendes mathematisches Wissen verstanden werden kann oder zumindest verständlich zu sein scheint. Zusammen mit den zahlreichen vergeblichen Lösungsversuchen etablierter Wissenschaftler erlangte die Kreisquadratur einen regelrechten Nimbus.

Ein weiterer, nicht zu unterschätzender Grund für die zahlreichen Bemühungen um die Quadratur des Kreises war die verbreitete Meinung, auf die Lösung des Problems sei ein hoher Preis ausgesetzt – ein Irrglaube, der möglicherweise auf die irrige Vermutung zurückgeht, die Kreisquadratur stünde in direkter Verbindung mit dem ebenfalls lange ungelösten Problem der exakten Bestimmung der geographischen Länge zur See, auf dessen Lösung in der Tat Preise ausgesetzt waren. Die Sage von den Preisausschreiben hielt sich so hartnäckig, dass selbst 1891 in Meyers Konversations-Lexikon noch zu lesen war, dass "„Karl V. 100.000 Thaler und die holländischen Generalstaaten eine noch höhere Summe“" ausgesetzt hätten.

Prominentes Beispiel für einen Amateurmathematiker, der die Quadratur des Kreises gefunden zu haben glaubte, war der englische Philosoph Thomas Hobbes. Seine 1665 in seinem Werk "De corpore" veröffentlichte Lösung – in Wirklichkeit eine Näherungskonstruktion – wurde von John Wallis noch im selben Jahr widerlegt. In der Folgezeit entspann sich zwischen den beiden eine in scharfem Tonfall geführte Auseinandersetzung, die erst mit Hobbes’ Tod im Jahr 1679 ein Ende fand.

Lambert berichtet von drei Kreisquadraturen mittels eines bestimmten rationalen Wertes. Die in der Mitte des 18. Jahrhunderts erschienenen Arbeiten beruhen auf der Näherung / für das Verhältnis von Kreisdurchmesser zur Seite des flächengleichen Quadrates. Für die Kreiszahl erhält man daraus die Näherung
Einem der drei Autoren, dem Prediger Merkel aus Ravensburg, widmete Gotthold Ephraim Lessing das Gedicht „Auf den Herrn M** den Erfinder der Quadratur des Zirkels“.

Die Kreisquadratur des amerikanischen Arztes Edward J. Goodwin erschien 1894 sogar im ersten Band des "American Mathematical Monthly", wenn auch nur als Annonce des Autors. Die Arbeit selbst ist in sich widersprüchlich und lässt je nach Lesart mehrere Werte für π zu. Sie war Grundlage für einen 1897 dem Parlament von Indiana vorgelegten Gesetzentwurf, der sogenannten Indiana Pi Bill, durch den die Erkenntnisse Goodwins zum Gesetz erhoben werden sollten.

Obwohl eine exakte Lösung mit Zirkel und Lineal nicht möglich ist, gibt es Näherungskonstruktionen für die Kreisquadratur, die für viele Zwecke exakt genug sind. Einfache, schon in der Antike bekannte Verfahren geben ein ganzzahliges Verhältnis von Durchmesser oder Radius des Kreises zur Seite oder Diagonalen des Quadrates an. Neben der im Papyrus Rhind erwähnten Gleichsetzung des Kreises vom Durchmesser 9 mit dem Quadrat der Seitenlänge 8 war auch die des Kreises vom Durchmesser 8 mit dem Quadrat der Diagonalen 10 bekannt. Diese Konstruktion findet sich bei den Babyloniern und eventuell beim römischen Feldmesser Vitruv. Sie liefert den Wert 3/ für π. Um ein bequemes zeichnerisches Verfahren anzugeben, nimmt Albrecht Dürer diese Konstruktion im Jahr 1525 in seinem Werk "Vnderweysung der messung mit dem zirckel und richtscheyt" wieder auf. Dürer ist sich dabei bewusst, dass es sich um eine reine Näherungslösung handelt, er schreibt explizit, dass eine exakte Lösung noch nicht gefunden sei:

Eine klassische Näherungslösung für den halben Kreisumfang ist die Näherungskonstruktion von Kochański, die der polnische Mathematiker Adam Adamandy Kochański im Jahr 1685 entdeckt hat. Sie kommt mit nur einer Zirkelöffnung aus. Die eigentliche Konstruktion besteht aus einer Rektifikation des Halbkreises, Kochanski konstruierte aus dem vorgegebenen Radius "r" näherungsweise eine gerade Strecke der Länge formula_13. Die Quadratur folgt daraus elementar mithilfe des Kathetensatzes. Die Kreiszahl wird bei Kochański auf vier Nachkommastellen genau angenähert:
Im Jahr 1913 erschien eine Konstruktion des indischen Mathematikers S. Ramanujan, die auf der Näherung

beruht, einem Wert, der bereits auf sechs Nachkommastellen genau ist und in Europa jedenfalls seit dem 17. Jahrhundert, in China schon seit dem 5. Jahrhundert als Zu Chongzhi-Bruch bekannt war. Ramanujan merkte bezüglich der Genauigkeit seines Verfahrens an, dass bei einer Kreisfläche von 140.000 Quadratmeilen die konstruierte Quadratseite nur um etwa einen Zoll vom wahren Wert abweiche.

Freie Übersetzung der Beschreibung ; nachvollziehbar auch mit Hilfe des nebenstehenden Fotos der "Skizze aus "Manuscript book 1 of Srinivasa Ramanujan" S. 54":
In einer Arbeit aus dem Folgejahr lieferte Ramanujan neben anderen Näherungsverfahren eine weitere Quadratur mit Zirkel und Lineal. Dieser liegt der Wert
zugrunde, der π sogar auf acht Stellen nahekommt.
Ramanujan konstruierte in dieser Quadratur nicht die Seitenlänge des gesuchten Quadrates, es genügte ihm die Strecke OS darzustellen. In der nebenstehenden Weiterführung der Konstruktion, wird die Strecke OS zusammen mit der Strecke OB zur Darstellung der "mittleren Proportionalen" (rote Strecke OE) herangezogen. 

Freie Übersetzung der Beschreibung: 

Weiterführung der Konstruktion bis zur gesuchten Seitenlänge formula_31 des Quadrates:
Verlängere über A hinaus und schlage den Kreisbogen b um O mit Radius , es ergibt sich S'. Halbiere in D und ziehe den Thaleskreis b über D. Zeichne eine gerade Linie ab O durch C bis zum Thaleskreis b, sie schneidet b in E. Die Strecke ist die oben beschriebene "mittlere Proportionale zwischen OS und OB" auch genannt "geometrisches Mittel", sie ergibt sich aus dem Höhensatz des Euklid. Verlängere die Strecke über O hinaus und übertrage darauf noch zweimal, es ergeben sich F und A und somit die Länge der Strecke mit dem oben beschriebenen Näherungswert von formula_1, den halben Kreisumfang. Halbiere die Strecke in G und zeichne den Thaleskreis b über G. Übertrage die Strecke ab A auf die Strecke , es ergibt sich H. Errichte auf eine Senkrechte ab H bis zum Thaleskreis b, es ergibt sich B. Verbinde A mit B, somit ist die gesuchte Seitenlänge formula_31 für ein nahezu flächengleiches Quadrat ABCD konstruiert.

Beispiele zur Veranschaulichung der Fehlers:

Eine einfachere Methode veröffentlichte Louis Loynes 1961. Sie beruht auf der Feststellung, dass der Flächeninhalt des Umkreises eines rechtwinkligen Dreiecks gleich dem Quadrat über der größeren Kathete ist, wenn der Tangens des kleineren Winkels, also das Verhältnis von kleinerer zu größerer Kathete,
beträgt, ein Wert, der sehr nahe an dem Bruch
liegt.
Daraus ergibt sich eine einfache Näherung, indem man das (konstruierbare) rechtwinklige Dreieck mit dem Katheten-Verhältnis 23:44 zur Quadratur benutzt. Der angenäherte Wert für die Kreiszahl von
ist etwas besser als bei Kochańskis Konstruktion.

Wird auf einem Strahl ein Bruch, dessen Wert annähernd der Kreiszahl formula_1 entspricht, mithilfe des dritten Strahlensatzes konstruiert, ist es mit mehr oder weniger konstruktivem Aufwand möglich, jede gewünschte Anzahl Nachkommastellen von formula_1 darzustellen. Für die Ermittlung der Seitenlänge des Quadrates kann z. B. der Bruch 
herangezogen werden. Er stammt von Johann Heinrich Lambert, der ihn u. a. m. bereits 1770 in seinem Buch "Beyträge zum Gebrauche der Mathematik und deren Anwendung" veröffentlichte.
Der Wert dieses Bruches zeigt fünfzehn Nachkommastellen von formula_1.

Lockert man die Beschränkung auf Zirkel und Lineal und lässt weitere Konstruktionsmittel zu, so erhält man eine Vielzahl von Möglichkeiten, den Kreis zu quadrieren beziehungsweise formula_41 exakt zu konstruieren.

Mithilfe spezieller transzendenter Kurven, den sogenannten Quadratrizes, als einzigem zusätzlichem Hilfsmittel ist es möglich, einen Kreis exakt zu quadrieren. Dabei wird im mathematischen Modell die Existenz beziehungsweise Verfügbarkeit einer solchen Quadratrix einfach vorausgesetzt. Zum praktischen Zeichnen auf Papier steht sie zum Beispiel in Form einer Schablone oder eines Plotterausdrucks zur Verfügung, zudem existieren einige spezielle mechanische Zeichengeräte, mit denen sich solche Kurven erzeugen lassen. Zu den ältesten bereits seit der Antike bekannten Beispielen gehören die Quadratrix des Dinostratos, in der nebenstehenden Darstellung alternativ Quadratrix des Hippias genannt, und die Spirale des Archimedes. Wie man sie darüber hinaus zur Kreisquadratur verwendet, ist in den zugehörigen Artikeln erläutert.

In der nebenstehenden Darstellung verhalten sich nach dem ersten Strahlensatz je zwei Abschnitte zueinander auf folgende Art:
Umgeformt und die entsprechenden Werte eingesetzt ergibt sich

Man füllt einen Zylinder mit einem Radius von einer Längeneinheit so mit Wasser, dass die Höhe des Wasserstandes ebenfalls eine Längeneinheit beträgt. Dann füllt man die so erhaltene Wassermenge von dem Zylinder in ein Prisma um, dessen Grundfläche ein rechtwinkliges Dreieck ist und dessen Höhe vier Längeneinheiten beträgt. Dabei wird das Prisma jedoch nicht auf seiner Grundfläche aufgestellt, sondern auf seiner rechtwinkligen Dreiecksspitze (siehe Zeichnung). Die Wasserfläche im Prisma bildet dann ein Rechteck mit den Seitenlängen von vier und formula_41 Längeneinheiten.

Nachrechnung der Wassermenge im Prisma, formula_47:

Wassermenge im Zylinder formula_54 in Volumeneinheiten [VE]:

Wassermenge im Prisma formula_47 in Volumeneinheiten [VE]:
Das Ergebnis zeigt, die "umgefüllte" Wassermenge im Prisma ist gleich der "eingefüllten" Wassermenge im Zylinder.

Alfred Tarski stellte 1925 die Aufgabe, einen Kreis in beliebig viele Teile zu stückeln und diese dann durch reine Bewegung (also ohne Streckung) so zu verschieben, dass ein Quadrat entsteht.

Miklós Laczkovich gelang 1989 die Lösung: Er bewies, dass es möglich ist, einen Kreis in endlich viele Teile zu zerlegen und diese nur durch Bewegung so zu verschieben, dass ein Quadrat entsteht. Er zerteilte den Kreis in 10 Stücke. Für den Beweis benötigt er jedoch das Auswahlaxiom, das von den meisten Wissenschaftlern heute zwar akzeptiert wird, aber nicht selbstverständlich ist. Der Beweis ähnelt stark dem Banach-Tarski-Paradoxon.

Laczkovich hat zwar bewiesen, dass (unter Annahme des Auswahlaxioms) so eine Zerlegung existiert, diese Zerlegung lässt sich jedoch nicht explizit angeben.

Im Gegensatz zum Kreis ist es möglich, eine Lemniskate (∞) durch zwei Quadrate darzustellen, deren Seitenlänge dem größten Lemniskatenradius "a" entspricht.







</doc>
<doc id="4184" url="https://de.wikipedia.org/wiki?curid=4184" title="Quarz">
Quarz

Quarz, auch Tiefquarz oder α-Quarz genannt, ist ein Mineral mit der chemischen Zusammensetzung SiO und trigonaler Symmetrie. Er ist die auf der Erdoberfläche stabile Form (Modifikation) des Siliciumdioxids und nach den Feldspaten das zweithäufigste Mineral der Erdkruste. Bei einer Temperatur von über 573 °C (unter einem Druck von 1 bar) geht Tiefquarz durch Änderung der Kristallstruktur in Hochquarz über.

Mit einer Mohshärte von 7 gehört Quarz zu den harten Mineralen und dient als Bezugsgröße auf der bis 10 (Diamant) reichenden Skala nach Friedrich Mohs. Er bildet oft gut entwickelte Kristalle von großer Formen- und Farbenvielfalt, deren Kristallflächen Glasglanz aufweisen. Quarz besitzt keine Spaltbarkeit, bricht muschelig wie Glas und zeigt auf den Bruchflächen einen fettigen Glanz.

In der Industrie ist Quarz eines der wichtigsten Minerale und hat gleichermaßen als Baustoff wie als Rohstoff für die Keramik-, Glas- und Zementindustrie weltweite Bedeutung. Quarzkies und gebrochener Quarz sind Rohstoff zur Gewinnung von Silicium.

Darüber hinaus werden Quarz und seine farbigen Varietäten seit alters her als Schmuckstein geschätzt (siehe Verwendung).

Quarzkristalle werden auch künstlich hergestellt: Daraus geschnittene Schwingquarze dienen als Taktgeber in elektronischen Schaltungen und Quarzuhren.

Gelegentlich wird Quarz mit dem Calcit verwechselt, kann jedoch durch seine größere Härte, die niedrigere Doppelbrechung und die Reaktion des Calcits mit verdünnter Salzsäure leicht von diesem unterschieden werden.

Der Ausdruck ist erstmals in der 1. Hälfte des 14. Jahrhunderts im Ostmitteldeutschen als Fachwort des böhmischen Bergbaus bezeugt ("quarz"). Zu seiner Herkunft gibt es verschiedene Hypothesen. Nach einer dieser Hypothesen stammt er von mhd. "querch" (‚Zwerg‘) in Anlehnung an den Aberglauben, dass Berggeister das bergbaulich wertlose Mineral unterschöben, nachdem sie ein an gleicher Stelle ursprünglich vorhandenes wertvolles Erz „geraubt“ hätten (vgl. auch die Etymologie von Kobalt). Eine andere Hypothese besagt, dass "Quarz" von "kwardy", einem Ausdruck aus einem polnischen Dialekt, stammt, welches der tschechischen Vokabel "tvrdý" (‚hart‘) entspricht. Einer Hypothese von Sergei Iwanowitsch Tomkeieff aus dem Jahr 1942 zufolge soll das Wort eine Zusammenziehung des sächsischen Bergmannsausdruckes „Querklufterz“ sein. Tomkeieff setzt dabei die Form "quertz, querz" als Grundform voraus, die sich in Schriften aus dem 16. Jahrhundert wie "Ein nützlich Bergbüchlin" und Agricolas "De re metallica" mit Bezug auf den Sprachgebrauch sächsischer Bergleute findet. Allerdings werden schon in dem 1889 erschienenen siebenten Band des Grimmschen Wörterbuchs ähnliche, seit dem 16. Jahrhundert existierende Herleitungen, die von einer Zusammenziehung aus "quaterz, quaderz" (‚böses Erz‘) oder aus "gewarz, gewärze" (Sammelbegriff zu "Warze") ausgehen, als „"spielende versuche, den ursprung des wortes zu ergründen"“ bezeichnet.

"Quarz" hat sich als Mineralbezeichnung international durchgesetzt, mit leichten, sprachspezifischen Abwandlungen wie beispielsweise "quartz" im Englischen und Französischen, "kvarts" im Schwedischen, "quarzo" im Italienischen oder кварц (kwarz) im Russischen.

Nach der 8. und 9. Auflage der strunzschen Systematik der Minerale gehört Quarz aufgrund seiner chemischen Zusammensetzung zur Mineralklasse der Oxide mit einem Metall-Sauerstoff-Verhältnis von 1:2.

In der ist er zudem Namensgeber für eine Gruppe chemisch ähnlicher oder gleicher Minerale, der "Quarzgruppe", deren weitere Mitglieder Coesit, Cristobalit, Melanophlogit, Mogánit, Opal, Stishovit und Tridymit sind.

Die untergliedert die Oxide allerdings feiner. Quarz sowie die ihm verwandten Minerale Beta-Quarz (Existenz bisher nur als Synthese bekannt) Coesit, Cristobalit, Melanophlogit, Mogánit, Seifertit, Opal, Stishovit und Tridymit werden nun der Unterabteilung "(Chemische Verbindungen) Mit kleinen Kationen: Kieselsäure-Familie" zugerechnet. Das in der Systematik ebenfalls mit aufgeführte Lechatelierit (Kieselglas) hat allerdings nach wie vor einen fraglichen Mineralstatus und ist daher von der International Mineralogical Association (IMA) auch nicht als eigenständiges Mineral anerkannt.

Die Systematik von James Dana ordnet die Minerale nach ihrer Kristallstruktur. Im Quarz ist Silicium tetraedrisch von vier Sauerstoffatomen umgeben. Diese SiO-Tetraeder sind über ihre Ecken zu einem dreidimensionalen Gerüst verknüpft, und Quarz wird daher in der Systematik von Dana den Gerüstsilikaten zugeordnet.

Quarz ist eine sehr reine Verbindung und baut andere Elemente nur in Spuren ins Kristallgitter ein. Natürliche Quarze können zwischen 13 und 15.000 ppm (meist aber nur einige 100 ppm) Al, zwischen 9 und 1400 ppm Na, zwischen 3 und 300 ppm K, sowie geringere Mengen an Fe, Ti, P, H und Li enthalten.

Der Einbau dieser Ionen erfolgt zumeist über einen gekoppelten Ersatz (Substitution) eines Si-Ions durch ein dreiwertiges und ein einwertiges Ion, so etwa Al und Na. Die Fremdionen werden sowohl auf den Si-Positionen im Gitter eingebaut wie auch auf ansonsten leeren Zwischengitterplätzen. Der Einbau von Eisen und Aluminium ist zusammen mit der Einwirkung von ionisierender Strahlung verantwortlich für die verschiedenen Farben der Quarzvarietäten.

Tiefquarz ist trigonal-trapezoedrisch (Kristallklasse 32) und kristallisiert in den enantiomorphen Raumgruppen und . Die Maße der Elementarzelle sind a = a = 4,9124 Å und c = 5,4039 Å. Eine Elementarzelle enthält drei Formeleinheiten SiO. Silicium (Si) und Sauerstoff (O) besetzen kristallographisch unterscheidbare Atompositionen:

Jedes Sauerstoffion ist von zwei Siliciumionen im Abstand von 1,6054 Å und 1,6109 Å umgeben und sechs Sauerstoffionen im Abstand von ca. 2,62 Å. Die Si-O-Bindungen haben einen großen kovalenten Anteil, was die Ursache für die große Härte von Quarz ist. Der Si-O-Si-Bindungswinkel beträgt 143,61°.
Entsprechend ist jedes Siliciumion tetraedrisch von vier Sauerstoffionen umgeben, zwei im Abstand von 1,6054 Å und zwei im Abstand von 1,6109 Å.

"SiO-Gerüst": Die SiO-Tetraeder sind untereinander über die Tetraederecken verknüpft, jeder Tetraeder mit vier benachbarten Tetraedern. In Richtung der c-Achse sind sie zu Paaren von spiralförmigen Ketten verknüpft. Diese SiO-Tetraederhelixpaare, die untereinander nicht verbunden sind, bilden sechsseitige, offene Kanäle in Richtung der c-Achse.

α-Quarzkristalle der beiden enantiomorphen Raumgruppen unterscheiden sich im Drehsinn der Tetraederschrauben. Linkshändischer α-Quarz kristallisiert in der Raumgruppe und die Tetraederschrauben winden sich im Uhrzeigersinn um die c-Achse dem Betrachter entgegen, wenn man von oben auf die c-Achse schaut. Entsprechend winden sich die Tetraederschrauben des rechtshändigen α-Quarzes (Raumgruppe ) entgegen dem Uhrzeigersinn dem Betrachter entgegen. Die spiralförmigen Tetraederketten sind mit sechs benachbarten Tetraederspiralen so verknüpft, dass jeder SiO-Tetraeder zu zwei benachbarten Tetraederketten gehört und an zwei der sechsseitigen Kanäle grenzt.

Quarz ist nur bei niedriger Temperatur in der trigonalen α-Quarz-Phase stabil. Bei 573 °C findet eine Phasenumwandlung in die hexagonale β-Quarz-Phase statt. Die höhere Symmetrie des β-Quarzes führt unter anderem zum Verlust der piezoelektrischen Eigenschaften. Den Übergang von der β-Quarz-Phase zum α-Quarz kann man sich leicht vereinfacht durch Kippen robuster Tetraeder um die <100>-Achse veranschaulichen. Die Kipprichtung entscheidet über die Orientierung des α-Quarzes.

Gut ausgebildete Kristalle sind verbreitet und ihre Form kann je nach Wachstumsbedingungen recht unterschiedlich sein. Die nebenstehende Abbildung illustriert die typische prismatische Kristallform von Linksquarz und wie sich diese Form aus den Grundkörpern der trigonal-trapezoedrischen Klasse (Klasse 32) zusammensetzt. Die in Klammern gesetzten Zahlen im Text und auf der Abbildung sind die Millerschen Indizes. Sie werden in der Kristallographie für die Bezeichnung von Kristallflächen verwendet. Indizes von Kristallflächen werden in runde Klammern gesetzt, Indizes von einer Flächengruppe, die einen Grundkörper bilden, in geschweifte Klammern und Indizes von Richtungen (Kristallachsen) in eckige Klammern.

Dominiert wird die Kristallform vom hexagonalen Prisma I. Stellung ({100}). Die Prismenflächen liegen parallel zur kristallographischen c-Achse. Begrenzt wird das Prisma an den Enden vom positiven und negativen Rhomboeder ({101} und {011}), wobei das positive Hauptrhomboeder mit größeren Flächen auftritt.

Untergeordnet, d. h. kleiner ausgebildet, treten verschiedene trigonale Trapezoeder, meist {511}, und trigonale Bipyramiden, meist {111}, auf. Von diesen Polyedern gibt es in der Kristallklasse 32 jeweils zwei enantiomorphe (linke und rechte), ansonsten aber identische Formen. An einem unverzwillingten Quarzkristall treten entweder nur rechte oder nur linke Trapezoeder und Bipyramiden auf, am Linksquarz (Raumgruppe ) linke Formen und am Rechtsquarz (Raumgruppe ) rechte Formen. Unterschieden werden können Rechts- und Linksquarze anhand der Anordnung der Trapezoeder- und Bipyramidenflächen. Beim Linksquarz treten diese links von den Hauptrhomboederflächen {101} auf und beim Rechtsquarz rechts von den Hauptrhomboederflächen.

Für auffällige Wachstumsformen von Quarz haben sich eigene Namen etabliert:

Weitere Namen sind für bestimmte Verwachsungen mehrerer Kristalle gebräuchlich:

Die beiden chiralen Formen des Quarzes, Rechtsquarz und Linksquarz, treten zuweilen auch orientiert miteinander verwachsen auf.



Quarz zeigt einen starken piezoelektrischen Effekt senkrecht zur Prismenachse entlang der a-Achsen. Auf Druck oder Zug reagiert ein Quarzkristall mit einer elektrischen Polarisierung entlang der Kraftrichtung. Umgekehrt führt das Anlegen einer elektrischen Gleichspannung zu einer Dehnung oder Stauchung des Kristalls. Wird eine Wechselspannung mit geeigneter Frequenz angelegt, so kann der Kristall zu Resonanzschwingungen angeregt werden. Die Resonanzfrequenz ist dabei von der Geometrie (Form und Größe) des Kristalls abhängig. Aufgrund der Regelmäßigkeit und Genauigkeit dieser Schwingungen werden "Schwingquarze" in Quarzoszillatoren als Zeitbasis und Taktgeber für elektronische Schaltungen eingesetzt, zum Beispiel in Uhren, Computern, Geräten der Digitaltechnik und der Funktechnik.

Durch die Kristallisation des Quarzes in einer enantiomorphen Struktur wird die Schwingungsebene des Lichtes, das einen Tiefquarz in Richtung der c-Achse durchquert, gedreht. Die Angabe exakter Messergebnisse dieser Drehung erweist sich als schwierig, da Messergebnisse aufgrund verschiedener Störfaktoren wie unerkannter Verzwillingungen von Rechts- und Linksquarz oder kleinster Verunreinigungen stark streuen. Zusätzlich erschweren Fertigungstoleranzen die Herstellung exakt orientierter Quarzschnitte. Weiterhin ist die Stärke der Drehung der Schwingungsebene des Lichtes abhängig von der Wellenlänge des Lichtes (Beispiel: Natrium-D-Linie: 589,3 nm, Grünfilter für Quecksilberdampflampen: 546 nm). So schwankt die Angabe des optischen Drehvermögens bei Quarz je nach Quelle und Wellenlänge zwischen 21 und 28 °/mm. Andererseits eignet sich bearbeiteter Quarz in Form von Quarzplatten hervorragend zur Überprüfung von Polarimetern.

Quarz ist ein sehr häufiges Mineral und in zahlreichen Vertretern aller drei Gesteinsklassen zu finden.

So kristallisiert er bei der Abkühlung SiO-reicher Schmelzen und ist primärer Bestandteil von SiO-reichen Plutoniten (Quarzreiche Granitoide, Granite, Granodiorite, Tonalite, Quarz-Syenite, Quarz-Monzonite, Quarz-Diorite) sowie der entsprechenden Vulkanite (Rhyolithe, Dacite, Andesite, Quarz-Trachyte, Quarz-Latite). Die Quarzgehalte dieser Gesteine sind eines der Hauptkriterien für ihre Klassifikation nach dem Streckeisendiagramm.

Quarz ist auch in vielen metamorphen Gesteinen enthalten (z. B. in Hornfelsen, Phylliten und Gneisen). Dort ist er entweder vom Ausgangsgestein ererbt oder er wird über zahllose Mineralreaktionen während der Gesteinsmetamorphose gebildet. So markiert zum Beispiel die Reaktion von Chloritoid und Alumosilikat zu Staurolith und Quarz die Grenze zwischen Grünschieferfazies und Amphibolithfazies bei Metapeliten.

Wegen seiner Verwitterungsbeständigkeit ist Quarz überdies ein häufiger Konstituent feinkörniger (aber nicht der feinkörnigsten) klastischer Sedimentgesteine (in erster Linie zu nennen: Sandsteine) sowie von Böden, die sich auf einem quarzreichen Gestein entwickelt haben.

Gut ausgebildete Quarzkristalle mit Sammlerwert entstehen hingegen bevorzugt in Klüften, hydrothermalen Gängen (als sogenannte "Gangart") und als Auskleidung natürlicher Höhlungen, sogenannter Geoden.

Quarz ist die auf der Erdoberfläche stabile Form (Modifikation) des kristallinen Siliciumdioxids. Zahlreiche weitere Modifikationen treten bei höheren Drücken und Temperaturen auf. Einige können metastabil an der Erdoberfläche erhalten bleiben.

Bei niedrigen Temperaturen (70–200 °C) kristallisiert aus SiO-Gel ein Gemisch aus Quarz und Mogánit, einem charakteristischen Bestandteil von Quarzin und Chalcedon.

Bei Temperaturen oberhalb von 573 °C (bei 1013,2 hPa) wandelt sich Quarz in Hochquarz um. Die Phasenumwandlung erfolgt sehr schnell, und Hochquarz bleibt auch bei rascher Abkühlung nie metastabil erhalten. Zwar finden sich in einigen Magmatiten Quarzkristalle mit der Kristallform von Hochquarz (Paramorphose), strukturell handelt es sich jedoch um Quarz.

Bei höheren Temperaturen wandelt sich Hochquarz erst in Tridymit um (ab 867 °C), dann in Cristobalit um (ab 1470 °C). Cristobalit schmilzt bei 1727 °C (Temperaturen jeweils bezogen auf 1013,2 hPa).

Die Umwandlungstemperaturen sind abhängig vom Druck. Allgemein nehmen sie mit steigenden Drücken zu.

Bei hohem Druck, wie er im Erdmantel herrscht oder bei Meteoriteneinschlägen auftritt, bilden sich besonders dichte SiO-Phasen. Ab 2 GPa bildet sich Coesit (3,01 g/cm³), ab 7,5 GPa Stishovit (4,3 g/cm³) und ab ca. 78 GPa Seifertit (4,12 g/cm³).

Reiner Quarz ist vollkommen transparent und farblos und wird, wenn er gut ausgebildete Kristalle entwickelt, als Bergkristall bezeichnet. Quarze sind meist durch mikroskopische Einschlüsse von Flüssigkeiten und Gasen milchig trüb ("Milchquarz") und erscheinen im Gestein eingewachsen grau. Unter der Bezeichnung "Rheinkiesel" sind zudem durchsichtige bis milchig trübe Rollstücke aus Bergkristall bekannt, die vorwiegend aus dem Alpenraum stammen und im Rheinkies gefunden werden.

Durch den Einbau färbender Ionen (im Allgemeinen Fe oder Fe), Einschluss farbiger Minerale oder Einwirkung von ionisierender Strahlung können Quarze unterschiedlich gefärbt sein. Anhand der Farbe und deren Ursache werden folgende Varietäten unterschieden:



Unter mikrokristallinem Quarz versteht man massige Aggregate von sehr feinkristallinem Quarz mit Kristallgrößen im Mikrometerbereich. Hier unterscheidet man drei Formen:

Amethystquarz ist eine undurchsichtige, gebänderte Verwachsung von Amethyst und Milchquarz.

Alle Formen von mikrokristallinem Quarz weisen eine große Dichte an Gitterbaufehlern und Verzwillingungen auf.

Hornstein und Flint (Feuerstein) sind Verwachsungen von mikrokristallinem Quarz mit Mogánit in einem regellosen, granularen Gefüge. Hierbei handelt es sich strenggenommen nicht um Minerale und Mineralvarietäten, sondern um Gesteine, die auch unter dem Oberbegriff "Chert" zusammengefasst werden. Hierunter werden bisweilen auch Chalcedon und seine Erscheinungsformen sowie amorphes SiO (Opal) subsumiert.


Der oft im Handel zu findende "Aqua Aura" ist keine Varietät, sondern meistens Bergkristall (oder ein anderer Quarz), der mit Metall (vorwiegend Gold) bedampft wurde. Resultat ist ein transparenter, blau gefärbter Kristall, zum Teil mit vielfarbigem Schimmer.

"Brasilit" ist dagegen die Handelsbezeichnung für eine durch Brennen grünlich-gelb bis blassgelb gefärbten Quarz.
Im Safiental (Graubünden, Schweiz) wurden die weltweit ersten Funde des Mantelquarzes gefunden, dessen Spitze ein wenig im Prisma versenkt ist.

Quarz besitzt eine relativ große Härte und die Eigenschaft, bei kurzzeitiger starker mechanischer Beanspruchung scharfkantig zu brechen. Daher wurde dieses Mineral in seinen verschiedenen Erscheinungsformen, einschließlich Hornfels, Quarzit und insbesondere Flint, schon in der Altsteinzeit von den Vertretern der Gattung Mensch als Rohstoff für vielerlei Werkzeuge und Waffen verwendet. Obwohl er schwieriger zu bearbeiten ist als Flint, dominieren in einzelnen Fundstätten Steinartefakte aus makrokristallinem Quarz (Gangquarz, sogar Bergkristall) sofern dieser in unmittelbarer Nähe verfügbar ist, speziell in Afrika südlich der Sahara.

Bevorzugt unbearbeitete Quarzitknollen oder rohes Felsgestein wurden als Schlagsteine zur Steinbearbeitung verwendet. Größere Brocken aus makrokristallinem Quarz wurden zudem als Kochsteine bevorzugt, da sie infolge rascher Temperaturwechsel weniger leicht platzen können.


Quarz und Quarzglas reagieren nur mit wenigen Chemikalien. Flusssäure ist die einzige Säure, die Quarz aufzulösen vermag, dabei bilden sich Siliciumtetrafluorid beziehungsweise Hexafluorokieselsäure. Diese Eigenschaft ist förderlich für eine Vielzahl von Anwendungsgebieten:


Künstlich gezogene Quarz-Einkristalle werden u. a. für piezoelektrische Anwendungen eingesetzt.
Die piezoelektrischen Eigenschaften des Quarzes werden bei Schwingquarzen ausgenutzt, die bei geeigneter Erregung durch eine elektrische Spannung mit einer festen Frequenz mechanisch schwingen. Der Bau sehr genau gehender Quarzuhren wurde so möglich. Heute finden sich in praktisch allen elektronischen Geräten Schwingquarze als Taktgeber. Daneben ist Quarz auch geeignet für Druckmessungen, in der Hochfrequenztechnik sowie als akustooptischer Güteschalter in Lasern.

Die beiden chiralen Formen des Quarzes, Rechtsquarz und Linksquarz, zeigen einen gegensätzlichen piezoelektrischen Effekt. In solchen Zwillingen heben sich daher die piezoelektrischen Effekte im Gesamtkristall auf, weshalb sie für piezoelektrische Anwendungen unbrauchbar sind und gegenüber synthetischen Quarzen seltener eingesetzt werden. Für technische Anwendungen werden die Zwillinge häufig parallel zur (01-1)-Ebene (AT-Schnitt) oder (023)-Ebene (BT-Schnitt) geschnitten, da der piezoelektrische Effekt senkrecht zu diesen Ebenen nahezu unabhängig von der Temperatur ist.

Quarzvarietäten wie der Achat, der violette Amethyst, der zitronengelbe Citrin, der blutrote Jaspis oder der schwarz-weiß gestreifte Onyx werden wegen der großen Härte und der guten Schneid- und Polierbarkeit des Minerals in der Schmuckindustrie zu Schmucksteinen verarbeitet.

Dringt kieselsäurereiches Grundwasser in das Gewebe abgestorbener, holziger Pflanzen ein, so können diese durch das Auskristallisieren von mikrokristallinem Quarz (Si(OH) → SiO + 2 HO) fossilisieren, wobei das holzige Gewebe zwar durch Quarz ersetzt wird, die ursprüngliche Zellstruktur jedoch oft erhalten bleibt. Paläobotaniker können daraus heute zum Beispiel Schlüsse zu den einstigen Wachstumsbedingungen der Pflanze ziehen. Bekannt sind auch versteinerte Araukarien-Zapfen aus Patagonien.

Verkieselungen gibt es auch von Tieren. Dabei wird oft ein vormals aus Calciumcarbonat (CaCO) bestehendes Außenskelett bzw. Gehäuse durch mikrokristallinen Quarz ersetzt. Bekannt sind beispielsweise verkieselte Korallen aus dem Miozän von Florida und der Trias von British Columbia und Alaska, opaleszierende Schnecken, Muscheln und Wirbeltierreste aus der Unterkreide des Lightning Ridge in Australien sowie verkieselte Schnecken vom Dekkan-Trapp (Oberkreide) in Indien. Wenn das Gehäuseinnere dieser Schnecken nach der weitgehenden Zersetzung der Weichteile nicht vollständig mit Sediment verfüllt worden war, können darin auch achatartige Drusen ausgebildet sein.

Beim Abbau und der Verarbeitung von Quarz können erhebliche Mengen von Quarzfeinstaub entstehen, der, über Monate oder Jahre hinweg täglich eingeatmet, zu der unter Bergleuten gefürchteten Silikose und im Extremfall sogar zu Lungenkrebs führen kann.

Jedoch kommt es beim Schleifen der Edelsteine nie zur Staubbildung, da der Schleifvorgang immer mit Wasser, Emulsion, Petroleum oder einem speziellen Schleiföl ausreichend gekühlt wird. Ein Trockenschliff würde auch die meisten Edelsteine beschädigen oder zerstören.

In der Esoterik gilt reiner Quarz (Bergkristall) als Heilstein, der vor schädlichen Strahlen bewahren, Kopfschmerzen und verschiedene Entzündungen lindern, Leber und Niere reinigen und die Durchblutung (Krampfadern) stärken soll. Quarz ist dem Tierkreiszeichen Löwe, den Planeten Saturn und Neptun und dem Monat April zugeordnet. Den verschiedenen Varietäten wie dem gelben Citrin oder dem violetten Amethyst werden zudem überwiegend Eigenschaften zugeschrieben, die sich aus der Mythologie ihrer Farbe ableiten lassen, zum Beispiel gelb für Energie und violett für Spiritualität. In der biologisch-dynamischen Landwirtschaft wird das Präparat Hornkiesel eingesetzt. Wissenschaftliche Belege für die Wirksamkeit liegen nicht vor.





</doc>
<doc id="4185" url="https://de.wikipedia.org/wiki?curid=4185" title="Quinoa">
Quinoa

Quinoa (gesprochen , ursprünglich von Quechua: "kinwa", Aussprache: ) ("Chenopodium quinoa") ist eine Pflanzenart aus der Gattung der Gänsefüße in der Familie der Fuchsschwanzgewächse (Amaranthaceae). In den Anden ist sie seit etwa 5000 Jahren als Kulturpflanze bekannt. Die Pflanzen sind anspruchslos und gedeihen bis in Höhen von 4200 m. Der nah verwandte und ähnlich verwendete "Chenopodium pallidicaule" (Cañihua) wird bis in 4550 m Höhe angebaut.

Die kleinen, durchschnittlich ca. 1,3–2 mm großen und 2–6 mg schweren einsamigen Nüsschen dieser Pflanzen sind in diesen Hochregionen ein wichtiges Grundnahrungsmittel der Bergvölker, da Mais in diesen Höhen nicht mehr angebaut werden kann. Das Tausendkorngewicht beträgt ca. 1–4 g, es können durchschnittlich vier Tonnen pro Hektar geerntet werden.

UN-Generalsekretär Ban Ki-moon erklärte das Jahr 2013 zum "Jahr der Quinoa". Die Pflanze soll aufgrund ihrer spezifischen Vorteile helfen, den Hunger auf der Welt, gerade in Zeiten des Klimawandels, zu bekämpfen.

Quinoa ist eine einjährige, meistens gynomonözische (das bedeutet: es sind neben rein weiblichen auch zwittrige Blüten vorhanden), krautige Pflanze mit einer Wuchshöhe von 50–300 cm. Sie bildet reich verzweigte Pfahlwurzeln aus, normalerweise ca. 30–50 cm lang, diese können aber bis 1,5 m tief reichen. Der aufrechte, anfangs innen massive und weiche, aber später hohle und schwammige, außen kompakte und feste, unten zylindrische, dann rippige Stängel ist grün, gelb, violett oder dunkelrot gefärbt, oder auch gestreift, mit einem Durchmesser bis 5 cm. Er ist normalerweise verzweigt, gelegentlich aber auch unverzweigt. 

Die Blätter sind vierseitig-wechselständig, mit einem an der Oberseite geriffelten Stiel, einfach, dicklich, bis 15 cm lang und bis 8 cm breit, anfänglich grün, später gelb, violett oder rot. Die polymorphen Blätter sind im unteren Teil der Pflanze kellenförmig (trullate), dreinervig, selten ganzrandig, meistens buchtig gezähnt, im oberen Teil schmalelliptisch bis lanzettlich, fiedernervig, ganzrandig oder auch buchtig gezähnt, mit kürzeren Stielen. Die Blätter erscheinen mit ebener oder welliger Oberfläche und sind ohne Nebenblätter. Die jungen Blätter sind an der Unterseite oft fein behaart. 

Die endständigen, aufrechten, gelben, roten, aber auch rosa-orange und violetten Blütenstände erscheinen in verschiedenen Formen, sie können 15–70 cm lang werden und sind etwa 5–30 cm im Durchmesser, sie bestehen aus knäueligen Teilblütenständen mit jeweils einer Länge von 1–5 cm. 

Die in Knäueln erscheinenden Blüten sind unscheinbar, weiblich (2–5 mm) oder hermaphroditisch (3 mm), bei Hybriden können auch sterile männliche auftreten, sie sind stiellos und fünfteilig, ohne Kronblätter. Die fünf Tepalen sind bootförmig, außen gekielt und mehr oder weniger spitz. Die zwittrigen Blüten besitzen fünf Staubblätter und 2–3 federige Narben, sowie einen Fruchtknoten, die weiblichen sind ohne Staubblätter. Der oberständige Fruchtknoten entwickelt sich nach der Selbst- oder Fremdbefruchtung zu einer linsenartigen (lentikularen), elliptoiden, rund- bis scharfkantigen, etwa 1–4 mm großen, harten, gelben, orange, roten, rosa, weißen, cremefarbigen, braunen, grauen oder schwarzen (Perikarp), zweikeimblättrigen, mehrschichtigen Achäne. Der Embryo ist krummläufig (kampylotrop) angelegt und umringt das stärkereiche Perisperm median.

Sie ist frost- und trockenheitsresistent und wächst auf durchlässigen, stickstoff- und kalziumhaltigen Böden, mit einem pH-Wert von 4,5 bis 9, am besten sandige bis sandig-lehmige Böden, sie akzeptiert gut verschiedene Klimata, der Temperaturbereich liegt bei −8 °C bis +38 °C, ideal ist 15–20 °C. Quinoa ist je nach Genotyp eine Langtagpflanze (LTP) oder Kurztagpflanze (KTP), sowie auch neutral.

Quinoa ist eine fakultative, C3, recreto-endo-Halophyte, welche überschüssiges Salz über epidermale Blasenzellen (EBCs) (Papilla) ausscheiden kann. Oft sind Teile der Pflanze mit diesen bläschenförmigen, farblosen, durchsichtigen oder violett pigmentierten Ausscheidungen bedeckt, häufig auf jungen Blättern.

Quinoa ist allotetraploid mit einer Chromosomenzahl von 2n  =  4x =  36. Als Allotetraploide ist sie hervorgegangen aus der Hybridisierung zweier diploider Elternarten, deren Genom jeweils als A und B bezeichnet wird. Quelle des A-Genoms ist "Chenopodium pallidicaule" (Cañahua, oder Kañiwa genannt), eine einjährige Art der Anden, die ebenfalls kultiviert wird. Quelle des B-Genoms ist der schwedische Gänsefuß "Chenopodium suecicum" Bei Quinoa ist das gesamte Genom gegenüber dem kombinierten Genom der Elternsippen vermutlich nochmals einmal komplett verdoppelt worden, zusätzlich kam es zu umfangreichen Umbau des gesamten Genoms mit großflächigem Austausch homologer Abschnitte der beiden Elternarten zwischen verschiedenen Chromosomen. Auf die Bildung der (heute hypothetisch erschlossenen) allotetraploiden Stammsippe geht eine Reihe wild wachsender, spontan verwilderter oder kultivierter Sippen hervor, darunter vor allem der Berlandiers Gänsefuß "Chenopodium berlandieri" und der Bocks-Gänsefuß "Chenopodium hircinum" . Nach den genetischen Daten ist der Bocks-Gänsefuß am nächsten verwandt. Da verschiedene genetische Linien von Quinoa und Bocks-Gänsefuß gegeneinander nicht monophyletisch sind, ist es möglich, dass die Tiefland- und die Hochland-Form von Quinoa unabhängig voneinander aus Ahnen aus dieser Art hervorgegangen wären. Dies deutet auf eine zweimalige, unabhängige Domestikation von Quinoa, einmal im chilenischen Küstenland und einmal im Altiplano, hin.

Die meisten Quinoa-Kultivare, Landrassen und Sippen sind genetisch relativ uniform, was auf vorherrschende Selbstbestäubung zurückgeführt wird. 

Die Erstbeschreibung von "Chenopodium quinoa" verfasste 1797 Carl Ludwig von Willdenow.

Synonyme von "Chenopodium quinoa" sind "Chenopodium album" subsp. "quinoa" , "Chenopodium album" var. "quinoa" , "Chenopodium canihua" , "Chenopodium ccoyto" Toro Torrico, "Chenopodium ccuchi-huila" Toro Torrico, "Chenopodium chilense" , "Chenopodium guinoa" , "Chenopodium hircinum" var. "quinoa" und "Chenopodium nuttalliae" Im Deutschen sind auch folgende Begriffe für die Pflanze üblich: "Inkareis", "Reismelde", "Inkakorn", "Reisspinat", "Andenhirse" oder "Perureis".

Quinoa gehört in eine Gruppe äußerst ähnlicher, untereinander teilweise kreuzbarer Arten, die systematisch in Genus "Chenopodium", subgenus "Chenopodium", Sektion "Chenopodium" gehören. Nach der Morphologie des Perikarp und der Blütenhülle (Perianth) wird die Gruppe um Quinoa als Subsektion "Cellulata" gefasst, die morphologisch durch blasige (alveolate) Fruchtwand und gekielte Kelchblätter gekennzeichnet ist. Innerhalb der Subsektion wird eine Serie "Foveosa" unterschieden, die neben dem Feigenblättrigen Gänsefuß "Chenpopodium ficifolium" vor allem Arten aus Nord- und Mittelamerika enthält. Dazu gehören eine Reihe von tetraploiden, wild wachsenden und kultivierten Arten mit Verbreitung in Nord-, Mittel- und Südamerika. Die Verwandtschaft und Abgrenzung der morphologisch definierten Sippen ist problematisch, auch genetisch sind die Sippen nicht immer klar definierbar, wobei Hybride zwischen den Arten das Bild weiter verkomplizieren. Für eine genaue Analyse sind weitere Studien mit breiterer Taxonabdeckung erforderlich. Dies betrifft vor allem die Abgrenzung von Quinoa und des nahe verwandten Bocks-Gänsefußes mitsamt der jeweiligen Varietäten und Unterarten zueinander.

Innerhalb der Art "Chenopodium quinoa" kommen als Unkräuter in Kulturland wild wachsende Sippen vor, die „Ajara“ (oder auch „Ayaras“) genannt wird und nach morphologischen Kriterien als Unterart "Chenopodium quinoa" subsp. "milleanum" oder Varietät "Chenopodium quinoa" var. "melanospermum" gefasst wurden. Diese zeichnen sich u. a. durch die schwarz gefärbten Samenhüllen aus. Tatsächlich erweist sich aber die Verwandtschaft und Ähnlichkeit eher geographisch als zwischen Kultur- und Wildpflanzen determiniert. Die Wild- und die Kulturpflanzen der chilenischen Küstenebene sind untereinander ähnlicher als beide zu den jeweiligen Unkraut- und Kultursippen der Anden. Diese Zweiteilung der Art Quinoa in eine Hochland- und eine Tieflandform wurde durch die genetischen Analysen klar bestätigt. Die chilenische Tieflandsippe, regional als „Quingua“ bezeichnet, ist durch die Morphologie des Blütenstands und die Form des Blattrands von derjenigen des Altiplano abgrenzbar.

Sowohl die genetischen wie die morphologischen Daten erweisen überraschenderweise einen Ursprung des gesamten kultivierten Artenaggregats um Quinoa in Nordamerika, mit anschließender Ausbreitung nach Südamerika. In der (erschlossenen) Ursprungsheimat des Aggregats wurde früher eine weitere, heute ausgestorbene Sippe kultiviert, die als "Chenopodium berlandieri" subsp. "jonesianum" bezeichnet wird. Die wild wachsenden, als "Chenopodium berlandieri" bezeichneten Pflanzen Südamerikas sind Quinoa noch ähnlicher, so dass bei einigen Unkrautsippen die Zugehörigkeit zur jeweiligen Art unsicher ist. Die Daten deuten darauf hin, dass sich zunächst "Chenopodium berlandieri", oder eine sehr ähnliche Stammsippe, von Nord- nach Südamerika ausgebreitet hat und diese hier zunächst wild besammelt und später domestiziert worden ist., vermutlich mindestens zweimal, unabhängig im Hoch- und im Tiefland. Die Art wurde als Kulturpflanze von den präkolumbianischen indianischen Kulturen weiter verbreitet. Diese Ausbreitung wird vor allem mit dem Reich der Inka in Verbindung gebracht, ging aber weit darüber hinaus, so etwa durch das Volk der Mapuche auf die Insel Chiloé. Nach der Eroberung durch die Spanier wurde die Kultur allerdings stark zurückgedrängt und regional sogar ganz aufgegeben, so etwa in der Gegend um Bogotá in Kolumbien oder der Provinz Córdoba in Argentinien.

Als Ursprungsregion im Altiplano, mit der höchsten Vielfalt an Sorten und Landrassen, gilt die Region um den Titicacasee Die tatsächliche Domestikation der Art nachzuweisen ist allerdings schwierig, da die Samen vorher vermutlich lange Zeit von Wildpflanzen gesammelt wurden und Wild- und Kulturpflanzen von Quinoa (und noch mehr der nahe verwandten Cañihua) sich bis heute recht ähnlich sind. Als Hinweis auf domestizierte Pflanzen dienen vor allem größere Samen, in Verbindung mit einer bimodalen (d. h. zweigipfligen) Kurve der Größenfrequenz, die auf eine Differenzierung von Kultur- und Wildpflanzen hindeutet. Weitere Merkmale wie die Dicke der Samenschale werden ergänzend hinzugezogen. Auch nach diesen Daten ist die Gegend um den Titicacasee diejenige mit den frühesten Nachweisen (bei einem Brand verkohlten Körnern aus Chiripa), deren Alter auf etwa 3500 Jahre abgeschätzt wird.

Die mineralstoffreichen Blätter werden als Gemüse oder Salat verzehrt. Die senfkorngroßen Samen haben eine getreideähnliche Zusammensetzung, daher wird Quinoa − ebenso wie Amarant − als glutenfreies Pseudogetreide bezeichnet, wobei aber mindestens zwei Quinoasorten dennoch Glutenabschnitte enthalten. Botanisch zählt Quinoa aber zu den Fuchsschwanzgewächsen, und es ist folglich eher mit dem Spinat oder den Rüben verwandt. Es lässt sich gut anstelle von Reis verwenden.

Der Naturkosthandel führt Quinoa pur oder als Zutat in Müslimischungen. Für die Inkas war es ein Mittel gegen Halsentzündungen. Besonders für Menschen, die unter Zöliakie (Glutenunverträglichkeit) leiden, bildet es bei den meisten Sorten einen vollwertigen Getreideersatz. Aufgrund dieser Eigenschaften ist es für Allergiker geeignet und in der vegetarischen sowie veganen Küche sehr beliebt. Quinoa eignet sich auch für die Herstellung von glutenfreiem Bier.

Quinoa wird seit etwa 5000 Jahren gemeinsam mit Amarant (lokale Bezeichnung "Kiwicha") als Hauptnahrungsmittel verwendet. Es wurde besonders in den Hochebenen der Anden oberhalb einer Höhe von 4000 m angebaut. Dort waren die beiden Pflanzen für die Menschen unentbehrlich, da Mais in diesen Höhen nicht angebaut werden kann. Während der spanischen Eroberungszüge und Kriege gegen die Inkas und Azteken im 16. Jahrhundert (siehe Francisco Pizarro und Hernán Cortés) wurde der Anbau von Quinoa und Amarant verboten und sogar unter Todesstrafe gestellt. Damit sollten die Völker geschwächt werden. Das als „unchristlich“ eingestufte Nahrungsmittel blieb dadurch in Europa bis in das 20. Jahrhundert hinein nahezu unbekannt.

1993 machte ein Bericht der NASA Quinoa als „neues“ Getreide, das sich durch seine hohen Eiweißwerte und einzigartige Aminosäurestruktur besonders für die Nutzung in "Controlled Ecological Life Support Systems" (z. B. Raumstationen oder Kolonien) eignen würde, international bekannt. Die Nachfrage stieg in den kommenden Jahren in Europa und Nordamerika sprunghaft an. Die steigende Nachfrage führte zu einem erhöhten Weltmarktpreis und steigenden Einkünften der Quinoa-Bauern. Andererseits konnten sich nun immer weniger Bolivianer und Peruaner das stark verteuerte Lebensmittel leisten und mussten auf billigere, industriell verarbeitete Lebensmittel ausweichen.

Laut FAO wurden 2016 weltweit ca. 148.720 Tonnen Quinoa geerntet. Hauptanbauländer sind Peru, Bolivien und Ecuador. In Deutschland werden nur geringe Mengen – meist zu Versuchszwecken – angebaut. Wird Quinoa in Mitteleuropa angebaut, so erfolgt die Aussaat von Anfang bis Mitte April. Die Ernte erfolgt ab Mitte September mit Mähdreschern. Da die Körner in den großen Fruchtständen ungleichmäßig reifen, ist nach der Ernte die Trocknung der Körner erforderlich.

Heute wird der Anbau dieses Pseudogetreides im Rahmen von Entwicklungsprojekten in Peru und Bolivien gefördert, da die Pflanzen geringe Ansprüche an Boden und Wasser stellen und als ein gesundes alternatives Nahrungsmittel erkannt wurden. Ein Anbau außerhalb Südamerikas ist bisher unüblich, er wird für die Himalaya-Region in Indien aber erwogen.

Je 100 g essbarem Anteil:
Quinoa ist verhältnismäßig reich an Eiweiß sowie an den Mineralstoffen Kalium, Magnesium und Phosphor. Der Gehalt an Eiweiß und einigen Mineralien (besonders Magnesium und Eisen) übertrifft den Gehalt bei gängigen Getreidearten. Es enthält reichlich Vitamin B1, während die anderen B-Vitamine einschließlich Folsäure nur relativ gering vorhanden sind. Die fettlöslichen Vitamine A und E fehlen nahezu vollständig, und Vitamin C ist nur in geringen Spuren enthalten. Das Aminosäurespektrum umfasst alle essentiellen Aminosäuren, darunter auch Lysin. Die Fettsäuren sind zu über 50 Prozent ungesättigt. Vitamingehalt und Nährwert sind ungefähr dem von Reis vergleichbar. Quinoa enthält jedoch deutlich mehr Mineralien, mehr Eiweiß und eine größere Menge mehrfach ungesättigte Fettsäuren (dafür weniger Kohlenhydrate). Der Verzehr von 100 g Quinoa deckt etwa ein Drittel des empfohlenen Tagesbedarfs an Eisen und Magnesium.

Den Schutz vor Schädlingen erreicht Quinoa durch bitter schmeckende Saponine, die auf der Samenschale (Perikarp) liegen. In ungeschältem Zustand ist Quinoa daher ungenießbar. Handelsübliche Quinoa ist daher geschält oder gewaschen und dadurch vom Saponin befreit und entbittert.
Der Saponingehalt wird durch dieses Verfahren erheblich reduziert. Durch ein Erhitzen/Kochen kann etwa ein Drittel der eventuell verbliebenen Saponine unschädlich gemacht werden. Der mögliche Restgehalt an Saponinen ist für den Menschen nicht schädlich, da sie kaum vom Darm aufgenommen werden.





</doc>
<doc id="4186" url="https://de.wikipedia.org/wiki?curid=4186" title="Quarantäne">
Quarantäne

Die Quarantäne [] (, , „vierzig Tage“) ist eine zum Schutz der Gesellschaft vor ansteckenden Krankheiten befristete Isolation von Personen oder von Tieren, die verdächtig sind, an bestimmten Infektionskrankheiten erkrankt oder Überträger dieser Krankheiten zu sein. Bei Import von Tieren ist die Quarantäne überdies zum Ausschluss der Krankheitsverbreitung in den Einfuhrbestimmungen des jeweiligen Landes präventiv vorgesehen. Die Zeitdauer der Quarantäne richtet sich nach der Inkubationszeit der vermuteten Krankheit.

Die Quarantäne ist eine sehr aufwendige, aber auch sehr wirksame seuchenhygienische Maßnahme, die insbesondere bei hochansteckenden Krankheiten mit hoher Sterblichkeit angewendet werden muss.

In Anlehnung an diese Analogie wird der Begriff auch in der IT-Branche verwendet, um Schadsoftware (wie etwa Trojaner-, Viren- und Wurmprogramme) in einem extra isolierten, meist verschlüsselten Bereich aufzubewahren. 
Im 19. Jahrhundert war ein ebenfalls gängiges Wort für Quarantäne "Kontumaz" (lat. "contumacia", dt. Trotz, Stolz, Eigensinn, Unbeugsamkeit).

In Deutschland sind gefahrenabwehrrechtliche Quarantänebestimmungen in Infektionsschutzgesetz, geregelt. Die dortigen Bestimmungen besagen, dass bei der Bekämpfung bestimmter Erkrankungen besondere Absonderungsmaßnahmen ergriffen werden können und müssen. Dies kann auch gegen den Willen der Erkrankten geschehen. Zurzeit sind nur die Pest und das hämorrhagische Fieber quarantänepflichtig, aber auch bei Erkrankungen wie der Cholera oder bei multiresistenten Keimen kommen Isolationsmaßnahmen zum Einsatz. Nur die behandelnden Ärzte, Pflegekräfte sowie Seelsorger müssen Zutritt zum Erkrankten haben, anderen Personen kann ärztlicherseits der Zutritt erlaubt oder verwehrt werden ( Abs. 4 IfsG).

Im Pflanzenschutz stuft die europäische Kommission sogenannte Quarantäneschaderreger ein.

Bereits 1374 ist eine, allerdings nur zehn Tage dauernde, Quarantäne in Reggio nell’Emilia durchgeführt worden.

Um ihre Stadt vor Pestepidemien zu schützen, beschloss im Juli 1377 die Regierung der Republik Ragusa, dass sich vor dem Betreten der Stadt alle ankommenden Reisenden und Kaufleute dreißig, später dann vierzig (d. h. "quaranta") Tage lang isoliert in eigens dafür errichteten Lazaretten aufhalten müssen, darunter in Lazareti bei Dubrovnik. Von dieser Regelung leitete sich der Begriff "Quarantäne" in der heutigen Bedeutung ab.

1383 wurde zum ersten Mal in Marseille die Quarantäne über ankommende Schiffe verhängt, um sich auch vor der Pest zu schützen, die damals in Europa wütete. Eine andere Quelle spricht davon, dass Beamte aus Venedig 1374 die Quarantäne einführten. Besatzung und Waren wurden zunächst auf der Insel Lazzaretto Nuovo isoliert und durften erst nach dreißig, später nach vierzig Tagen an Land.

Quarantänemaßnahmen haben 1918 Australien vor dem Übertritt der Spanischen Grippe geschützt.

Bei Pockenausbrüchen wurden in Deutschland noch in den 1960er Jahren drastische Isolierungsmaßnahmen ergriffen. Die betroffenen Personen wurden teilweise ohne ärztliche Versorgung in Schullandheimen isoliert und mussten sich selbst versorgen.




</doc>
<doc id="4187" url="https://de.wikipedia.org/wiki?curid=4187" title="Quedlinburg">
Quedlinburg

Quedlinburg ([], plattdeutsch "Queddelnborg", offiziell auch Welterbestadt Quedlinburg) ist eine Stadt an der Bode nördlich des Harzes im Landkreis Harz (Sachsen-Anhalt). 994 mit dem Stadtrecht versehen, war die Stadt vom 10. bis zum 12. Jahrhundert Sitz der zu Ostern besuchten Königspfalz weltlicher Herrscher und fast 900 Jahre lang eines (zunächst geistlichen, nach der Reformation freiweltlichen) Damenstifts.

Quedlinburgs architektonisches Erbe steht seit 1994 auf der UNESCO-Liste des Weltkulturerbes und macht die Stadt zu einem der größten Flächendenkmale in Deutschland.

In der historischen Altstadt mit ihren kopfsteingepflasterten Straßen, verwinkelten Gassen und kleinen Plätzen befinden sich gut 2000 Fachwerkhäuser aus acht Jahrhunderten. Am Markt liegt das Renaissance-Rathaus mit der Roland-Statue, südlich davon der Schlossberg mit der romanischen Stiftskirche und dem Domschatz als Zeugnisse des Quedlinburger Damenstifts. Auch der Münzenberg mit der romanischen Klosterkirche St. Marien und im Tal dazwischen die romanische St. Wiperti, der sich anschließende Abteigarten und der Brühl-Park gehören zum Weltkulturerbe.
Die Stadt liegt im nördlichen Harzvorland durchschnittlich , 50 km südwestlich der Landeshauptstadt Magdeburg. Die unmittelbar angrenzenden Höhen erreichen . Die Stadt liegt im Flussbett der Bode, mit dem größeren Teil westlich des Flusses. Das Stadtgebiet hat eine Fläche von 78,14 Quadratkilometern.

Quedlinburg liegt inmitten des Quedlinburger Sattels, einem Schmalsattel, der das Stadtgebiet von Nordwesten nach Südosten durchquert. Dazu gehört der Quedlinburger Schlossberg mit seiner Verlängerung über den Münzenberg-Strohberg, die nördlich gelegene Hamwarte und die südlicher gelegene Altenburg.

Weiter im Süden liegt die Harznordrandstörung. Parallel zum Nordrand des herausgehobenen Harzes sind die mesozoischen Gesteinsschichten daran aufgebogen und teilweise abgebrochen. Die wechselnden Lagen von unterschiedlich widerständigen mesozoischen Gesteinen (Jura, Kreide, Muschelkalk) bilden teilweise freipräparierte Schichtrippen, die als markante Höhenzüge von der Bode quer durchschnitten werden. Der markanteste Höhenzug ist die Teufelsmauer.

Während der Elster- und der Weichsel-Kaltzeit hatte das Eis den Harzrand erreicht, während die Region in der letzten Kaltzeit (Saale-Kaltzeit) nicht mit Eis bedeckt war. Während der Hochglazialphasen bildeten sich äolische Decken. Diese großflächig aufgewehten Lössschichten überlagerten die älteren Fest- und Lockergesteine und wurden später zu Schwarzerdeböden hoher Güte umgewandelt. Es sind dies die südlichen Ausläufer der fruchtbaren Magdeburger Börde.

Die Stadt befindet sich in der gemäßigten Klimazone. Die durchschnittliche Jahrestemperatur in Quedlinburg beträgt 8,8 °C. Die wärmsten Monate sind Juli und August mit durchschnittlich 17,8 beziehungsweise 17,2 °C und die kältesten Januar und Februar mit 0,1 beziehungsweise 0,4 °C im Mittel. Der meiste Niederschlag fällt im Juni mit durchschnittlich 57 Millimeter, der geringste im Februar mit durchschnittlich 23 Millimeter.

Der Harz liegt als Hindernis in der von Südwesten kommenden Westwinddrift. Durch die Höhe (Brocken mit ) werden die Luftmassen zum Aufsteigen gezwungen und regnen sich dabei ab. Die nordöstliche Seite liegt im Regenschatten des Harzes. In diesem Gebiet befindet sich Quedlinburg mit einem der geringsten Jahresniederschläge in Deutschland von nur 438 Millimetern (zum Vergleich: Köln annähernd 798 Millimeter). Da die Monate Dezember, Januar und Februar absolut die niedrigsten Niederschlagswerte besitzen und die stark abnehmende Tendenz bereits im Spätherbst beginnt, kann von einer Quedlinburger „Wintertrockenheit“ gesprochen werden. Bei der 2010 erstmals durchgeführten Gesamtauswertung der 2100 Messstationen des Deutschen Wetterdienstes wurde festgestellt, dass Quedlinburg im August 2010 mit 72,4 Liter je Quadratmeter (= mm) der trockenste Ort in Deutschland war. Frostfreie Tage gibt es pro Jahr 177, während an 30 Tagen Dauerfrost herrscht. Eine geschlossene Schneedecke ist an weniger als 50 Tagen vorhanden und die Sonnenscheindauer liegt bei 1.422 Stunden jährlich.
Die historische Kernstadt gliedert sich in den ehemaligen Königsbesitz mit dem Westendorf, dem Burgberg, der St.-Wiperti-Kirche sowie dem Münzenberg. Nördlich davon liegt die 994 gegründete Altstadt und östlich die im 12. Jahrhundert gegründete Neustadt. Dazwischen wurde im 13./14. Jahrhundert die Steinbrücke angelegt und die Word trockengelegt. Nördlich der Altstadt befindet sich das mittelalterliche Vorstadtviertel Gröpern.

Um diesen mittelalterlichen Kern wurde am Übergang vom 19. zum 20. Jahrhundert ein Gürtel aus Villen im Jugendstil gebaut. Im Zuge der Industrialisierung entstanden außerhalb dieses Gürtels neue Ortsteile, so die Kleysiedlung, das Neubaugebiet in der Süderstadt (19./20. Jahrhundert) und das auf dem Kleers (1980er Jahre).

Neben dieser Kernstadt gehören zu Quedlinburg noch die Ortsteile Münchenhof (vier Kilometer nördlich), Gersdorfer Burg (drei Kilometer südöstlich), Morgenrot (vier Kilometer östlich) und Quarmbeck (vier Kilometer südlich) sowie seit dem 1. Januar 2014 wieder Gernrode und Bad Suderode.

Am 1. Juli 2014 ist das neue Kommunalverfassungsgesetz des Landes Sachsen-Anhalt in Kraft getreten. In dessen §14 (2) wird den Gemeinden die Möglichkeit gegeben, den Ortsteilen, die vor der Eingemeindung Städte waren, diese Bezeichnung zuzuerkennen. Die Stadt Quedlinburg hat von dieser Regelung Gebrauch gemacht. Ihre geänderte Hauptsatzung stammt vom 12. März 2015. Im §1 (3) werden die Ortsteile und Ortschaften mit ihren amtlichen Namen aufgeführt.

Quedlinburg ist eine Stadt im Landkreis Harz und grenzt an acht sachsen-anhaltische Städte und Gemeinden (im Uhrzeigersinn, im Nordosten beginnend): Gemeinde Harsleben, Stadt Wegeleben, Gemeinden Ditfurt und Selke-Aue, Städte Ballenstedt und Thale.

→ "Hauptartikel: Geschichte der Stadt Quedlinburg"

Die ersten Siedlungsspuren reichen bis in die Altsteinzeit zurück. Die Gegend war fast durchgehend besiedelt. Die ertragreichen Böden machten die Gegend für Siedler während des Neolithikums besonders interessant, was sich durch über 55 Siedlungsreste dieser Epoche allein in der Stadt und der näheren Umgebung nachweisen lässt. So befinden sich auf den markanten Bergspitzen wie dem Moorberg, der Bockshornschanze oder dem Brüggeberg, die an den Seitenwänden des Bodetals aufgereiht wie auf einer Kette aufragen, neolithische Begräbnishügel. Etwa zwei Kilometer nordwestlich von Quedlinburg, westlich der Wüstung Marsleben, konnte 2005 eine Kreisgrabenanlage der Stichbandkeramik untersucht werden, die der Kreisgrabenanlage von Goseck in Alter, Ausdehnung und Form nicht nachsteht.

Am Ende des 8. Jahrhunderts häufen sich urkundliche Nachrichten über Ortschaften in der Umgebung Quedlinburgs: Marsleben, Groß Orden, Ballersleben (alle wüst), Ditfurt und Weddersleben. Die Wipertikirche als Filiale der Abtei Hersfeld ist wahrscheinlich um 835/63 gegründet worden.

Bedeutung erlangte Quedlinburg, als es im 10. Jahrhundert die Königspfalz wurde, in der die ottonischen Herrscher das Osterfest feierten. Erstmals wurde es als "villa quae dicitur Quitilingaburg" in einer Urkunde König Heinrichs I. vom 22. April 922 erwähnt.

Später bestimmte Heinrich den Ort zu seiner Grablege. Nach seinem Tod im Jahr 936 in Memleben wurde sein Leichnam nach Quedlinburg überführt und in der Pfalzkapelle auf dem Schlossberg bestattet.
Seine Witwe Königin Mathilde ließ sich von Heinrichs Sohn und Nachfolger Otto I. die Gründung eines Damenstiftes mit der Aufgabe der Totenmemorie bestätigen. Dreißig Jahre lang stand sie ihrer Stiftsgründung selbst als Leiterin vor, ohne Äbtissin geworden zu sein. Otto I. besuchte Quedlinburg in unregelmäßigen Abständen zur Feier des Osterfestes und zu den Gedenktagen seines Vaters. Im Jahr 941 entging er dabei nur knapp einem Mordanschlag durch seinen jüngeren Bruder Heinrich. Auf dem Oster-Hoftag 966 wurde Ottos Tochter Mathilde als Äbtissin mit der Leitung des Damenstiftes betraut. Zwei Jahre später, am 14. März 968, starb ihre Großmutter und wurde an der Seite ihres Gemahls bestattet. Ihr Grab und ihr steinerner Sarkophag sind erhalten geblieben, während Heinrichs Grablege leer ist.

Der größte und glanzvollste Hoftag Ottos des Großen fand 973 statt. Unter den internationalen Teilnehmern befanden sich Boleslav I., Herzog von Böhmen, und Mieszko I., Herzog der Polanen, die dem Kaiser den Treueeid leisteten. Kurz darauf starb Otto I. Sein Sohn Otto II. besuchte in seiner zehnjährigen Regentschaft nur zweimal Quedlinburg.

Nach dessen Tod 983 war Otto III. erst drei Jahre alt. Sein Onkel Heinrich der Zänker wollte sich in Quedlinburg selbst zum König erheben und entführte den jungen König. Vor allem das Eingreifen von Ottos Großmutter Adelheid, der zweiten Gemahlin Ottos I., und seiner Mutter Theophanu, der Gemahlin Ottos II., zwang Heinrich zwei Jahre später, dem jungen Otto III. in Quedlinburg zu huldigen. Otto III. verlieh 994 dem Stift das Markt-, Münz- und Zollrecht, noch unter dem Vorstand seiner Tante, der Äbtissin Mathilde. Damit war eine wichtige Bedingung für die weitere städtische Entwicklung Quedlinburgs geschaffen.

Von der weiteren reichspolitischen Bedeutung Quedlinburgs im 11. und 12. Jahrhundert zeugen die vor Ort verfassten, später so genannten Quedlinburger Annalen. Diese verzeichnen im Jahre 1009 erstmals in schriftlichen Quellen "Litua", den Namen Litauens. Für die Zeit vom 10. bis zum 12. Jahrhundert, als Quedlinburg die Osterpfalz der ostfränkisch/deutschen Herrscherhäuser war, sind 69 urkundlich nachweisbare Aufenthalte eines Königs oder Kaisers gezählt worden.

In den ersten Jahrzehnten nach seiner Gründung erhielt das Damenstift auch weit entfernte Orte, wie das 170 km entfernte Soltau, die Kirche St. Michael des Volkmarskellers (956), Duderstadt (974), Potsdam (993) und Gera (999), aber auch andere Schätze. Zu den 48 von Otto I. geschenkten Orten kamen unter Otto II. elf, unter Otto III. zehn und unter späteren Herrschern noch weitere 150 Orte hinzu.

1326 schloss sich die Stadt mit Halberstadt und Aschersleben zum Halberstädter Dreistädtebund zusammen, der 150 Jahre andauern sollte.

In den folgenden vier Jahrhunderten nahm Quedlinburg einen wirtschaftlichen Aufschwung. Wie in anderen Städten (Braunschweig, Halberstadt) der Region waren das Gewandschneider- und das Kaufmannswesen besonders intensiv. Um 1330 wurde die Altstadt mit der im 12. Jahrhundert gegründeten Neustadt belehnt, die fortan immer geschlossen als Stadt Quedlinburg agierten.

Zum wirtschaftlichen Erfolg gesellte sich 1336 ein politischer, als die Stadt in einem regionalen Konflikt zwischen dem Halberstädter Bischof und dem Grafen von Regenstein Letzteren gefangen setzen konnte. Die Stadt erlangte größere Unabhängigkeit von der Stadtherrin, der Äbtissin des Damenstiftes, und durfte in der Folge ihre Verteidigungsanlagen massiv ausbauen. Das neue Selbstbewusstsein wurde in Form von vielen Städtebündnissen nach außen hin demonstriert. Als Krönung dieser Entwicklung trat die Stadt 1384 dem Niedersächsischen Städtebund und 1426 dem Hansebund bei.

Der Plan des Stadtrates, sich von den Befugnissen der Äbtissin Hedwig von Sachsen zu befreien, mündete 1477 in einen gewaltsamen Konflikt. Die Quedlinburger versuchten, Hedwig mit Waffengewalt aus der Stadt zu vertreiben. Daraufhin bat diese ihre Brüder, die Wettiner Herzöge Ernst und Albrecht um Hilfe. Die entsandten Truppen stürmten die Stadt ohne eigene Verluste, während 80 Quedlinburger fielen. Die Bürgerschaft unterwarf sich daraufhin und schied aus sämtlichen Bündnissen aus. Der um 1435 aufgestellte Roland, Symbol der Marktfreiheit und Zeichen städtischer Unabhängigkeit, wurde gestürzt und zerschlagen. Erst 1869 wurden die Bruchstücke dieser Rolandstatue wieder aufgefunden, zusammengesetzt und neu aufgestellt. 2013 wurde die Figur gesäubert und komplettiert.

Während des Bauernkriegs wurden vier Klöster der Stadt, das Prämonstratenserkloster St. Wiperti, das Benediktinerinnenkloster St. Marien, das Franziskanerkloster in der Altstadt und das Augustinerkloster in der Neustadt zerstört. Die Reformation wurde in Quedlinburg im Jahr 1539 durchgesetzt und das Stift in ein evangelisches "Freies weltliches Stift" umgewandelt.

Den größten städtebaulichen Aufschwung nahm die Stadt ab dem Dreißigjährigen Krieg. Die meisten der 1200 erhaltenen Fachwerkhäuser sind in dieser Zeit entstanden. Zwei Stadtbrände verwüsteten 1676 und 1797 große Teile der Stadt.

1698 besetzten brandenburgische Truppen die Stadt, womit fortan Preußen Schutzmacht war.
1802 wurde das seit 936 bestehende Damenstift aufgelöst. Die Stiftsgebäude auf dem Schlossberg gingen in den Besitz des preußischen Staates über.

Im Laufe des 18. und besonders des 19. Jahrhunderts entwickelte sich durch die Pflanzenzucht und Saatgutvermehrung ein beachtlicher Wohlstand, der städtebaulich auch in einer Reihe von Jugendstil-Villen seinen Ausdruck fand. Als die erste Zuckerfabrik des Regierungsbezirks Magdeburg 1834 von G. Chr. Hanewald in Quedlinburg eingerichtet wurde, führte dies zur raschen Entwicklung landwirtschaftlicher Zuliefer- und Großbetriebe. Die Entwicklung von Zuchtverfahren, der Anschluss an das Eisenbahnnetz und die Separation (1834–1858) sind Stationen zu einer weltwirtschaftlichen Bedeutung im Saatzuchtbereich. Neben der Zucht von Zier- und landwirtschaftlichen Pflanzen wuchs seit Beginn des 20. Jahrhunderts die Bedeutung der Gemüsezucht.

Von 1815 bis 1938 war Quedlinburg eine Garnisonsstadt.

Von 1865 bis 1888 wurden Fragmente der ältesten bekannten illustrierten biblischen Handschrift ("Quedlinburger Itala") aus dem 5. Jahrhundert in Quedlinburg gefunden.

Im beginnenden 20. Jahrhundert waren die Saatzuchtfirmen die größten Arbeitgeber. 1907 sprach Rosa Luxemburg vor 800 Quedlinburger Saatzucht-Arbeitern. 1911 wurde Quedlinburg, das bis dahin Sitz des Kreises Quedlinburg war, kreisfreie Stadt.

Während des Ersten Weltkrieges wurden viele landwirtschaftliche Arbeiten mit Hilfe von bis zu 17.000 Kriegsgefangenen durchgeführt, die in einem Kriegsgefangenenlager auf dem so genannten Ritteranger nordöstlich der Stadt untergebracht waren. Dieses Lager wurde seit September 1914 eingerichtet und bestand über den Krieg hinaus als Notunterkunft zaristischer Soldaten, bis es im Juni 1922 niedergebrannt wurde. Im selben Jahr fand in Quedlinburg eine Feier zum tausendsten Jahrestag der ersten urkundlichen Erwähnung (922) statt.

Ein verheerendes Hochwasser der Bode zerstörte 1926 alle Brücken und legte die Infrastruktur lahm. Immer wieder behinderten spätere Hochwasser die Wiederaufbauarbeiten.

Zur Zeit des „Dritten Reiches“ wurde die Tausendjahrfeier (936–1936) des Todestages König Heinrichs I. von den Nationalsozialisten in Gestalt der SS als ein "propagandistisches Geschenk" angesehen. Heinrich Himmler entwickelte ab 1936 einen Kult um den König und wurde selbst als eine Reinkarnation Heinrichs angesehen, was ihm geschmeichelt haben soll, wie sein Leibarzt Felix Kersten berichtet. In Quedlinburg wurden die Wipertikrypta und die Kirche St. Servatii beschlagnahmt und zu Weihestätten der SS umfunktioniert. Himmlers persönliches Erscheinen (bis 1939) zu den jährlichen Feierlichkeiten am 2. Juli, die bis 1944 stattfanden, wurde zum Beispiel 1937 propagandistisch mit Nachrichten über das Auffinden der verlorenen Gebeine Heinrichs I. aufgewertet. Nach dem Krieg wurden bei einer Öffnung des (neuen) Sarkophags die von der SS vorgezeigten „Funde“ als plumpe Fälschungen entlarvt.

Am Morgen nach den Zerstörungen der „Reichspogromnacht“ legte der Ladenbesitzer Sommerfeld seine Eisernen Kreuze aus dem Ersten Weltkrieg (EK 1 und 2) in sein zerstörtes Schaufenster und ein Schild: „Der Dank des Vaterlandes ist Dir gewiss.“ Bald darauf begann die Verschleppung jüdischer Bewohner. Im Stadtgebiet befanden sich drei Außenstellen von Konzentrationslagern: das Kreisgerichtsgefängnis und je ein Gefangenenlager in der Kleersturnhalle und im Fliegerhorst in Quarmbeck.

Seit 1943/1944 wurden in Quedlinburg über 8.000 Verwundete in den Sporthallen und Notlazaretten versorgt. In der Woche, bevor am 19. April 1945 amerikanische Truppenverbände (RCT 18) die Stadt fast kampflos einnehmen konnten, gelang es, Teile der V 2, die auf dem Quedlinburger Bahnhof auf Waggons lagerten, aus der Stadt zu bringen. Dies verhinderte eine Bombardierung und so beschränkten sich die Kriegszerstörungen auf Artillerietreffer.
Nach dem Krieg war Quedlinburg Teil des 1945 gegründeten Landes Sachsen-Anhalt, seit 1952 des Bezirkes Halle in der DDR.

Die Demonstrationen vom 17. Juni 1953 konnten in Quedlinburg und Thale nur durch den Einsatz von Streitkräften der Sowjetarmee unterbunden werden.

Obwohl es kaum nennenswerte Kriegszerstörungen gab, reichten die Bemühungen durch die DDR bei weitem nicht aus, den drohenden natürlichen Verfall der Altstadt zu stoppen. Durch den Einsatz erfahrener polnischer Restauratoren aus Toruń "()" konnten nur punktuell Häuser wiederhergestellt werden. Seit 1957 wurde St. Wiperti restauriert und 1959 neugeweiht. Die ursprünglichen Planungen der DDR in den 1960er-Jahren, die historische Altstadt vollständig niederzureißen und durch einen zentralen Platz und sozialistische Plattenbauten zu ersetzen, scheiterten an Geldmangel. Versuche, die Plattenbauweise den historischen Verhältnissen anzupassen, sind im Bereich des Marschlinger Hofes, in Neuendorf und in der Schmalen Straße nördlich des Marktes zu sehen. Dafür wurde die sogenannte Hallesche Monolithbauweise (HMB) modifiziert und als Hallesche Monolithbauweise Typ Quedlinburg (HMBQ) umgesetzt. Erst nach der Wiedervereinigung 1990 wurden zielstrebig Fachwerkbauwerke restauriert.
Im Herbst 1989 demonstrierten in kaum einer anderen Stadt, gemessen an der Einwohnerzahl, so viele Menschen wie in Quedlinburg. Gewaltlose Demonstrationen während der „Wende“ fanden in Quedlinburg immer am Donnerstag statt. Die Demonstration am 2. November 1989 mit 15.000 Teilnehmern war trotz provozierenden Verhaltens der SED-Größen vor Ort ein Beispiel der Gewaltlosigkeit. Die größte Demonstration mit über 30.000 Teilnehmern fand am 9. November 1989 statt. Keiner der Teilnehmer ahnte, dass zur gleichen Zeit die Mauer geöffnet wurde. Die Kreisdienststelle des Ministeriums für Staatssicherheit wurde am 12. Dezember 1989 aufgelöst, nachdem die Klarnamendatei und die brisantesten Akten (beispielsweise zu Kirchenangelegenheiten) in den Tagen vorher vernichtet worden waren.

Am 6. Januar 1990 fand zum Dank für den überwältigenden Empfang beim Überschreiten der Grenze ein großes Stadtfest statt. Zu diesem Fest kamen Würdenträger und 50.000 Gäste.

Bei einem Spontanbesuch sagte Helmut Kohl im Januar 1990 der Stadt Hilfsgelder zur Sicherung der extrem gefährdeten Bausubstanz zu und das Bundesland Niedersachsen spendete im Frühjahr 100.000 Dachziegel für Sofortmaßnahmen.
Ein gesellschaftlicher Tiefpunkt waren im Herbst 1992 ausländerfeindliche Übergriffe in der Quedlinburger Neustadt. Eine Antwort von Quedlinburger Einwohnern war die Gründung der bis heute aktiven Präventionsmaßnahme „Altstadtprojekt“. Die geplante NPD-Demonstration 15 Jahre später wurde durch eine betont bunte Demonstration engagierter Quedlinburger verhindert.

Von den 1945 geraubten zwölf Teilen des Domschatzes kehrten 1993 zehn aus den USA zurück in die Quedlinburger Domschatzkammer. Zwei Beutestücke bleiben weiterhin in Amerika verschollen.

Zur Tausendjahrfeier der Verleihung des Markt-, Münz- und Zollrechtes wurden große Teile der Quedlinburger Altstadt und der Königshofkomplex am 17. Dezember 1994 auf Antrag Deutschlands auf die Liste der Welterbestätten der UNESCO gesetzt, als ein Ensemble, das die Ansprüche gemäß dem Kriterium IV. erfüllt, „ein herausragendes Beispiel eines Typus von Gebäuden oder architektonischen Ensembles oder einer Landschaft, die bedeutsame Abschnitte in der menschlichen Geschichte darstellen“. (IV).
Gerhard Schröder besuchte 1999 mit dem französischen Premierminister Lionel Jospin und 2001 mit dem spanischen Ministerpräsidenten José María Aznar die Stadt.

Das schwedische Königspaar, Carl XVI. Gustaf und seine Frau Silvia, besuchte 2005 die Quedlinburger Stiftskirche. Seit dem Jahr 2006 ist der Bahnhof Quedlinburg an das Netz der Selketalbahn angeschlossen. Nach mehrjährigen Restaurierungen ist die Krypta der Stiftskirche seit März 2009 wieder für die Öffentlichkeit zugänglich. Mit Alles Klara spielt seit 2011 erstmals eine der Vorabendserie der ARD in Quedlinburg und Umgebung. Von 2011 bis 2014 wurden umfassende Neugestaltungsarbeiten am Marktplatz, im Bereich der Breiten Straße und der Steinbrücke vorgenommen. Im Vorfeld dieser Arbeiten wurden bei archäologischen Grabungen Pflasterreste eines Marktes entdeckt, die in das 10. Jahrhundert datiert werden. Im Jahr 2014 wurde seitens des Stadtrates beschlossen, die Bezeichnung "Welterbestadt" voranzustellen. Nach Genehmigung durch den zuständigen Landkreis und die UNESCO Deutschland gilt seit 29. März 2015 die Bezeichnung "Welterbestadt Quedlinburg". Seit dem Frühjahr 2015 ist die ehemalige Krypta der St. Marienkirche auf dem Münzenberg nach fast 500 Jahren wieder zugänglich.

Erstmals wurden am 26. Mai 2017 vor dem Haus Steinweg 81 Stolpersteine für Berta und Bruno Sommerfeld verlegt, die hier zeitweise lebten und 1943 deportiert und dann ermordet wurden.

Da Quedlinburg lange Zeit nicht über seine mittelalterliche (Stadtmauer-)Grenzen hinauswuchs, blieb die Einwohnerzahl vom Mittelalter bis in das 19. Jahrhundert bei maximal 8.000 bis 10.000 Personen. Erst mit der Industrialisierung begann die Zahl zu wachsen und erreichte den höchsten Wert 1950 mit 35.426/35.555 Einwohnern Danach sank sie von 1950 bis 1990 um 21 Prozent (7459) kontinuierlich ab und lag bereits 1975 wieder unter 30.000. Seit der gewaltlosen Revolution und der Grenzöffnung 1989/1990 verlor die Stadt wegen hoher Arbeitslosigkeit, des Wegzugs vieler Einwohner in das Umland und des Geburtenrückgangs erneut 20 Prozent ihrer Bewohner (5.500 Personen). Am 30. Juni 2006 betrug die amtliche Einwohnerzahl für Quedlinburg nach Fortschreibung des Statistischen Landesamtes Sachsen-Anhalt 22.481 (nur Hauptwohnsitze und nach Abgleich mit den anderen Landesämtern). Zum 1. Januar 2011 vergrößerte sich die Stadt zunächst durch die Eingemeindung der Stadt Gernrode sowie der Gemeinden Bad Suderode und Rieder von 78,14 km² auf 141,82 km²; die Bevölkerungszahl stieg von etwas über 21.000 auf über 28.000. Diese Eingliederung musste jedoch wegen eines Formfehlers am 19. Februar 2013 aufgrund einer Gerichtsentscheidung wieder rückgängig gemacht werden. Bad Suderode und Gernrode gehören seit dem 1. Januar 2014 wieder zu Quedlinburg.

Die Bertelsmann-Stiftung, Wegweiser Demographischer Wandel, liefert Daten zur Entwicklung der Einwohnerzahl von 2959 Kommunen in Deutschland (Publikation Januar 2006). Für Quedlinburg wird ein Absinken der Bevölkerung zwischen 2003 und 2020 um 14,1 Prozent (3.281 Personen) vorausgesagt.

Prognose der absoluten Bevölkerungsentwicklung von 2003 bis 2020 für Quedlinburg (Hauptwohnsitze):

Im Rahmen der Fortschreibung des WelterbeManagementPlans wurde 2011 eine eigene Prognose aufgestellt. Zum Stichtag 31. Dezember 2010 wohnten 21.016 Einwohner mit Hauptwohnsitz in Quedlinburg (mit Gebietsstand zu diesem Stichtag). Im Jahressaldo verlor die Stadt im Laufe 2011 insgesamt 69 Einwohner. Unter Einschluss der Zu- und Abwanderungen wurde damit seit 2001 ein durchschnittlicher Negativsaldo von 150 bis 180 Einwohnern jährlich konstatiert. Die Prognose für 2025 liegt nach dieser Erhebung bei 16.200 bis 17.300 Einwohner (in den Grenzen von 2010).
Die folgende Übersicht zeigt die Altersstruktur vom 31. Dezember 2007. Einige Zahlen spiegeln sechs, andere über 20 Jahrgänge wider.

Der überwiegende Teil der Quedlinburger Bevölkerung gehört keiner Religionsgemeinschaft an. Die ehemals fünf protestantischen Gemeinden umfassen rund 16 Prozent der Stadtbevölkerung und haben sich in der "Evangelischen Kirchengemeinde Quedlinburg" zusammengeschlossen, die ein Teil der Evangelischen Kirche in Mitteldeutschland ist. Etwa vier Prozent der Stadtbevölkerung gehören zur katholischen St.-Mathildis-Gemeinde, einer Pfarrei im Bistum Magdeburg.
Weitere christliche Gemeinden gehören zu den Siebenten-Tags-Adventisten, der Evangelisch-Freikirchlichen Gemeinde (Baptisten) oder anderen evangelischen Freikirchen sowie zur Neuapostolischen Kirche. Darüber hinaus leben in der Stadt Mitglieder der Blankenburger Gemeinde der alt-katholischen Kirche.

Bereits im 11./12. Jahrhundert sollen sich jüdische Kaufleute in Quedlinburg angesiedelt haben. Seit dem frühen 13. Jahrhundert sind sie urkundlich fassbar. Sie fungierten vor allem als unabhängige Kreditgeber der Quedlinburger Äbtissin und anderer lokaler Magnaten. Im Jahr 1514 mussten alle Juden Quedlinburg verlassen. Zwar waren im 18. Jahrhundert drei sogenannte Schutzjuden erlaubt, aber erst nach der Auflösung des Stiftes 1802 siedelten sie sich wieder in Quedlinburg an. Von 1933 bis 1945 lebten weniger als 100 „Nichtarier“ in Quedlinburg. Von diesen kamen mindestens 13 gewaltsam zu Tode, 14 gelang die Emigration und 34, überwiegend „Halbjuden“, überlebten und starben eines natürlichen Todes. Die anderen Schicksale sind unbekannt. Eine jüdische Gemeinde gibt es seit der NS-Zeit nicht mehr in Quedlinburg.

An der Spitze der Stadt stand seit dem 13. Jahrhundert der Rat mit zunächst zwölf, später dreimal zwölf Ratsherren (abwechselnd zwölf pro Jahr). Den Vorsitz hatte ein Bürgermeisterpaar, bestehend aus einem Bürgermeister der Alt- und einem der Neustadt. Bis zum 19. Jahrhundert gab es also drei Altstädter und drei Neustädter Bürgermeister, die sich abwechselten. Dann wurde das Amt auf eine Person beschränkt. Von 1890 bis 2000 trugen die Bürgermeister den Titel Oberbürgermeister.

Als Vertretung der Bürger gibt es eine Stadtvertretung, die in Quedlinburg die Bezeichnung "Stadtrat" trägt. Die Mitglieder der Bürgerschaft werden von den Bürgern der Stadt auf fünf Jahre gewählt. Die Mehrheitsverhältnisse in der Quedlinburger Bürgerschaft sind sehr unübersichtlich.

2009 wurde der Stadtrat noch für die Kernstadt Quedlinburg allein gewählt. Durch die Eingemeindung der Ortschaften Rieder (2011–2013) sowie Gernrode und Bad Suderode (2011–2013 sowie seit 2014) vergrößerte sich der Stadtrat zeitweilig auf bis zu 45 Sitze. Bei der Kommunalwahl am 25. Mai 2014 wurde der erste Stadtrat gewählt, bei dem Kandidaten auch aus den neuen Ortsteilen direkt antraten. Zusätzlich wurde für Gernrode und Bad Suderode ein Ortschaftsrat gewählt. Insgesamt bewarben sich 126 Männer und Frauen um die 36 Sitze im Stadtrat.

Das Wahlergebnis sortiert nach prozentualer Reihenfolge der letzten Wahl:
Die Wahlbeteiligung lag 2014 bei 40,0 % (35,7 %; 34,7 %; 51,3 %). Sie zählt damit zwar zu den niedrigsten Werten in Deutschland, erhöht sich aber wieder seit 2004.

Quedlinburg führte seit Jahrhunderten ein Wappen, doch liegen keine Zeugnisse dafür vor, dass dieses Hoheitszeichen rechtmäßig verliehen wurde. Das Wappenbuch des Heraldikers Johann Siebmacher führt im Jahr 1605 die Wappen der Reichstätte und Städte vor; ein Quedlinburger Wappen nennt er nicht. Auch finden sich in den Archiven keine historiografischen Hinweise auf eine Wappenverleihung.

Es ist darum anzunehmen, dass Quedlinburg im Laufe seiner Stadtgeschichte aus dem ursprünglichen Siegelbild ein in Gewohnheitsrecht getragenes Wappen entwickelte. Das erklärt auch die Tatsache, dass das Wappenbild im Laufe der Jahrhunderte häufig wechselte und von einem verbindlichen Erscheinungsbild nicht die Rede sein kann.

Das bis 1998 gebräuchliche Wappenbild fand bei der Landesregierung keine Zustimmung und wurde deshalb in seiner Gestaltung verändert. Diese Änderungen betrafen allerdings lediglich Details und kaum die heraldische Erscheinung des Wappens. Begründet wurde die gestalterische Modifizierung damit, dass es gerade die veränderten Details seien, die aus einem Bild ein korrektes Wappenbild machen würden.

Vorbild des Adlers war das 1882 von Adolf Matthias Hildebrandt gestaltete Wappenbild aus dem „Urkundenbuch der Stadt Quedlinburg“. Der innere Schild wurde in seiner Grafik den heraldischen Gepflogenheiten und überlieferten stilistischen Formen angepasst. Die grafische Ausführung und Dokumentation erfolgte durch den Heraldiker Jörg Mantzsch.

Blasonierung: „In Gold einen rotbewehrten schwarzen Adler mit goldkonturiertem roten Brustschild, darin eine silberne Burg mit schwarz gefugter Zinnenmauer und gezinntem Torturm mit offenem Rundbogenfenster im Spitzdach, geöffneten Torflügeln und emporgezogenem Fallgitter, der Torturm flankiert von zwei spitzbedachten Zinnentürmen mit je einem offenen Rundbogenfenster, im Tor ein sitzender silberner Hund mit schwarzem Halsband.“

Die Farben der Stadt sind Schwarz-Gelb.

Die Flagge der Stadt besteht aus den Farben der Stadt in Streifen mit einem aufgesetzten Stadtwappen.

Quedlinburg hat seit 1961 eine Städtepartnerschaft mit dem kleinen Ort Aulnoye-Aymeries in Nordostfrankreich und seit 1991 eine Städteunion mit den vier historisch bedeutsamen Städten Herford in Nordrhein-Westfalen sowie Celle, Hann. Münden und Hameln in Niedersachsen. Gemeinsam mit diesen wurde ein sogenanntes Städteunionshaus (Hohe Straße 8) eingerichtet, in dem regelmäßig Treffen stattfinden. Seit 2000 gibt es einen Städtekontakt mit Torbay in Großbritannien.

Die Ausstellung des Schlossmuseums zeigt die Entwicklung des Burgberges mit dem Damenstift und Facetten der Stadtgeschichte. Herausragende Exponate sind der bronzezeitliche Hortfund vom Lehof, die Goldscheibenfibel aus Groß Orden (wüst), der sogenannte "Raubgrafenkasten" und eine mittelalterliche Balliste. Seit 2002 wird im sogenannten Ottonenkeller eine Ausstellung zur Rezeption der ottonischen Zeit während des Nationalsozialismus gezeigt.

Im 1570 erbauten Klopstockhaus wurde 1724 der Dichter Friedrich Gottlieb Klopstock geboren. Klopstock wurde durch sein Wirken zu einem Begründer der klassischen deutschen Literatur und war weit über die Grenzen Deutschlands hinaus berühmt. An das Museum im Klopstockhaus angeschlossen sind eine Bibliothek und ein Archiv.
Das Fachwerkmuseum "Ständerbau" zählt zu den ältesten Fachwerkhäusern in Quedlinburg. Neuere Untersuchungen ergaben eine Datierung von 1346/1347. Älter sind Gebäudeteile von Klink 6/7 von 1289 (d), Hölle 11 von 1301 (d), Breite Str. 12/13 1330 (d). Die Ausstellung zeigt die Geschichte des Ständer- und Fachwerkbaus vom 14. bis zum 20. Jahrhundert und einzelne Stile des Quedlinburger Fachwerkbaus anhand von Modellen.

Die 1986 eröffnete Lyonel-Feininger-Galerie zeigt Werke des New Yorker Bauhaus-Künstlers Lyonel Feininger (1871–1956), die vom Quedlinburger Hermann Klumpp, einem Mitschüler des Bauhauses, vor der Vernichtung durch die Nationalsozialisten bewahrt worden waren. Die Sammlung, eine der umfangreichsten geschlossenen Bestände von Grafiken, Radierungen, Lithographien und Holzschnitten des Künstlers, dokumentiert seine Schaffensperioden von 1906 bis 1937.

Daneben befinden sich drei weitere Galerien in der Stadt: Galerie "Weißer Engel", Galerie im Kunsthoken und die „Galerie im kleinen Kunsthaus“.

Im Mitteldeutschen Eisenbahn- und Spielzeugmuseum befinden sich über 3.000 Ausstellungsobjekte zum Thema "Historisches Spielzeug aus der Zeit um 1900" und eine Sammlung historischer Modelleisenbahnen der Spuren I, 0, S und H0, vor allem von Märklin, aber auch ausländische Modelleisenbahnen.

Das „Museum für Glasmalerei und Kunsthandwerk“, untergebracht im restaurierten Wordspeicher, einem Speichergebäude des 17. Jahrhunderts, bietet eine Ausstellung zur Bedeutung und Geschichte der Quedlinburger Glasmalerei sowie eine Schauwerkstatt und einen interaktiven Erlebnisraum.

Das „Münzenbergmuseum“ zeigt die Geschichte des mittelalterlichen Marienklosters auf dem Münzenberg und die Siedlungs- und Sozialgeschichte dieses Viertels in der Frühen Neuzeit.

Die Stiftskirche St. Servatii thront weithin sichtbar auf dem Schlossberg über der Stadt. Der jetzige, vierte Kirchenbau an gleicher Stelle wurde nach einem Brand im Jahr 1070 begonnen und im Jahr 1129 geweiht. Der romanische Kirchenraum ist durch den niedersächsischen Stützenwechsel und einen imposanten, innen und außen verlaufenden Relieffries gekennzeichnet. Der Hohe Chor wurde unter der Äbtissin Jutta von Kranichfeld bis 1320 im gotischen Stil umgebaut. Bei der umfassenden Restaurierung unter Ferdinand von Quast 1863 bis 1882 erhielt die Kirche zwei romanische Türme mit stilwidrigen rheinischen Helmen. In der Zeit von 1936 bis 1945 war die Kirche durch die SS unter dem Reichsführer SS Heinrich Himmler besetzt und profaniert. In den beiden Schatzkammern ist der Quedlinburger Domschatz mit den 1945 gestohlenen und 1992 aus Texas zurückgekehrten Teilen zu sehen. Gezeigt werden unter anderem das Servatiusreliquiar, das Katharinenreliquiar, Fragmente der Quedlinburger Itala, der mit Goldblech beschlagene Servatius- oder Äbtissinnenstab und der Knüpfteppich aus dem 12. Jahrhundert.
St.-Wiperti-Kirche wurde als katholische Filialkirche 1959 neugeweiht. Reste des Altarraums reichen bis zur Mitte des 10. Jahrhunderts zurück. In diesen Bau wurde in der Zeit um das Jahr 1020 die romanische Krypta eingefügt. 1146 wurde der gesamte Kanonikerkonvent (seit 961/964) in einen Prämonstratenserkonvent umgewandelt. Dieses Kloster überstand in vier Jahrhunderten mehrere Zerstörungen (1336, 1525), bevor es im Zuge der Reformation spätestens 1546 aufgehoben wurde. Die Kirche wurde als evangelische Pfarrkirche der Münzenberg- und Westendorfgemeinde genutzt. Mit der Auflösung des Damenstiftes 1802 wurde die Wipertikirche zunächst verpachtet, später verkauft und als Scheune genutzt. Von 1936 bis 1945 wurde sie ebenfalls als nationalsozialistische Weihestätte profaniert. In den Jahren 1954 bis 1958 wiederhergerichtet, wird sie seit 1959 in den Sommermonaten für das sonntägliche Hochamt genutzt. 1995 wurde ein Förderverein gegründet, der die bauliche und historische Substanz betreut.

Die Stiftskirche und die Wipertikirche sind die Quedlinburger Stationen (Nr. 36) auf der südlichen Route der Straße der Romanik.
Die Reste der St.-Marien-Kirche auf dem Münzenberg werden nicht als Sakralraum genutzt. Sie sind aber durch private Initiativen wieder zugänglich gemacht worden. Die 1525 aufgegebene romanische Kirche ist 986 auf Intervention der Äbtissin Mathilde als Klosterkirche eines Benediktinerinnenklosters gegründet worden. 1017 wurde sie nach einem Brand in Gegenwart Heinrichs II. neu geweiht. Nach den Zerstörungen im Bauernkrieg war das Kloster verlassen worden und seit den 1550er-Jahren siedelten sich einfache Leute (Musikanten etc.) auf dem Münzenberg an. Diese zersiedelten das ehemalige Klostergelände mit vielen kleinen Häusern, sodass der Kirchenraum in 17 einzelne Gebäude aufgeteilt war. Ein Großteil der Kirche wurde wieder in der ursprünglichen Form zugänglich gemacht.

St. Aegidii im Norden der Altstadt, eine spätgotische dreischiffige Kirche mit ihren wuchtigen, festungsartigen Türmen, wurde erstmals 1179 erwähnt. Die evangelische Kirchengemeinde Quedlinburg nutzt sie aus denkmaltechnischen Gründen zurzeit nur selten. Aus dem gleichen Grund sind die Besuchsmöglichkeiten eingeschränkt.
Die Marktkirche St. Benedikti mit der angeschlossenen Kalandskapelle ist auf romanischen Resten errichtet und wurde 1233 erstmals erwähnt. Sie wird von der evangelischen Kirchengemeinde als Pfarrkirche genutzt. Der Bau ist eine Hallenkirche mit achteckigen Pfeilern, einem spätgotischen Chor aus dem 14. Jahrhundert und einem Taufstein aus dem Jahre 1648. Dach und Dachstuhl der Kirche sind als Fauna-Flora-Habitat (FFH) für die Große-Mausohr-Fledermäuse ausgewiesen.
St. Nikolai in der Neustadt wurde 1222 erstmals erwähnt und ist mit ihren 72 Meter hohen Türmen und ihrem hohen dreischiffigen Bau ein imposantes Beispiel für einen frühgotischen Kirchenraum. Ob der romanische Vorgängerbau auf eingerammten "Ellernpfählen" errichtet wurde, um in dem morastigen Untergrund Halt zu finden, konnten archäologische Untersuchungen bisher weder bestätigen noch widerlegen. Nach chronikalischen Nachrichten des 13. Jahrhunderts haben zwei Schäfer auf der so genannten Pfannenwiese ihre Herden gehütet und dabei einen Schatz gefunden, den sie zum Bau der Kirche stifteten. Deshalb sind zwei Ecken des Turmes mit Figuren eines Schäfers und seines Hundes geschmückt. Die Hallenkirche besitzt verschiedenartig gegliederte Pfeiler, einen einschiffigen Chor und Doppeltürme.

St. Blasii in der Altstadt, von der nur noch die gotischen Türme (mit Spolien eines romanischen Vorgängerbaus) stehen, während das Kirchenschiff aus dem Barock stammt, wurde wegen fehlender Nutzung durch eine eigene Kirchengemeinde der Stadt übergeben und wird heute vor allem als Konzert- und Ausstellungsraum genutzt. Komplett erhalten sind die hölzernen Bankeinbauten des 16./17. Jahrhunderts.

St. Mathilde im Neuendorf wurde von 1856 bis 1858 nach Plänen des Mitarbeiters der Kölner Dombauhütte Friedrich von Schmidt errichtet. 1858 von Bischof Konrad Martin (Paderborn) konsekriert und Mathilde, der Ehefrau König Heinrich I. geweiht, ist sie die Pfarrkirche der katholischen Gemeinde. Der Turm des neugotischen einschiffigen Baus wurde 1984 aus baustatischen Gründen heruntergenommen.

In der Süderstadt wurde 1906 St. Johannis errichtet, die sich auf dem Gebiet des ehemaligen Hospitals mit der alten St.-Johannes-Kapelle befindet. Die bereits im 13. Jahrhundert erwähnte St.-Johannis-Kapelle ist in den Jakobsweg eingebunden. Sie war einst die Kirche eines weit vor der Stadt Quedlinburg gelegenen Hospitals.

→ "Liste der Kulturdenkmale in Quedlinburg"
Der größte Teil des Hausbestandes im historischen Stadtkern sind Fachwerkhäuser, die in besonderer Weise dem städtebaulichen Denkmalschutz unterstehen. Sie wurden aufgrund ihrer Formen in fünf große Bereiche unterteilt: Danach wurden mindestens elf (1 Prozent) Fachwerkhäuser vor 1530 errichtet, weitere 70 (5 Prozent) zwischen 1531 und 1620, mehr als 439 (33 Prozent) zwischen 1621 und 1700, mehr als 552 (42 Prozent) zwischen 1700 und 1800 und 255 (19 Prozent) die im 19. und 20. Jahrhundert erbaut wurden. Insgesamt sind das mehr als 1327 Fachwerkhäuser in Quedlinburg. Zum Vergleich: Es haben sich in Wernigerode 624, in Stolberg 354 und in Osterwieck 353 Fachwerkbauten erhalten.

In den vergangenen Jahren konnte die Bauforschung mit Hilfe von Dendrochronologie über zwanzig bisher bauzeitlich unbekannte Häuser und Dachstühle aus der Zeit zwischen dem 13. und 15. Jahrhundert identifizieren.

Von 1989 bis 2005 gelang durch verschiedene Förderprogramme die Sanierung von etwa 650 der insgesamt 1200 denkmalgeschützten Quedlinburger Fachwerkhäuser. Besonders um die Förderung verdient gemacht hat sich die Deutsche Stiftung Denkmalschutz. Ein Denkmalpflegeplan, der 2012 veröffentlicht wurde, spricht von 2119 Fachwerkbauten, von denen 1689 als Baudenkmale eingestuft sind. Insgesamt gelten 2050 der 3562 Gebäude als ortsbildprägend.

Von 1990 bis 2010 hat Quedlinburg über 120 Millionen Euro Fördermittel aus Landes-, Bundes- und EU-Töpfen erhalten. Die Finanzlage der Stadt gilt als angespannt.

Das 1989 veröffentlichte Denkmalverzeichnis der Stadt Quedlinburg führt über 1200 Einzeldenkmale auf. Bei den folgenden besonders markanten Bauwerken handelt es sich infolgedessen nur um eine geringe Auswahl:




Der Ring der mittelalterlichen Stadtmauer mit seinen Stadttürmen ist in weiten Teilen noch zu sehen. Von den mittelalterlichen Stadttoren, dem Hohen Tor, dem Gröperntor, dem Öringertor und dem Pölkentor hat sich dagegen keines erhalten. Der Schreckensturm ist der größte erhaltene Turm. Der durch sein grünes Dach leicht erkennbare Lindenbeinsche Turm ist mit einer Galerie versehen und für Besucher geöffnet. Zwei Türme sind heute zu Wohnungen ausgebaut, darunter der Kaiserturm. Einige Türme sind in Privathand, zum Teil in schlechtem baulichen Zustand. Dazu zählen unter anderen der Gänsehirtenturm, der Kuhhirtenturm, der Schweinehirtenturm, der Kruschitzkyturm, der Pulverturm, der Mertensturm und der Spiegelsturm.

Von den im Felde um die Stadt befindlichen ehemals elf Wachtürmen, die entlang des Landgrabens oder der Landwehr an wichtigen strategischen Positionen erbaut waren, sind sechs, hier Feldwarten genannte Türme erhalten: die Bicklingswarte, die Lethwarte, die Altenburgwarte, die Gaterslebener Warte, die Steinholzwarte und die Seweckenwarte. Durch Steinraub weitgehend verschwunden sind die Warte auf dem Lehof, die Aholzwarte, die Heidbergwarte, die Anamberger Warte und die Sültenwarte. Sie waren umgeben von befestigten Höfen, die den auf den Feldern arbeitenden Bauern und Hirten als Fliehburg dienten. Die Warttürme wurden auf Bergen an der Gemarkungsgrenze als Frühwarnsystem errichtet und meldeten Gefahren mittels Rauch- und Feuerzeichen an die Stadt.

Der größte Park ist der Brühl, ein altes Waldstück, das bereits um 1179 als "broil" genannt und im 16./17. Jahrhundert planmäßig angelegt wurde. Der Brühlpark ist Bestandteil des 40 Gartenanlagen umfassenden Projektes Gartenträume Sachsen-Anhalt. Zwischen Brühl und Schlossberg wurde 2006 der Historische Abteigarten wieder neu gestaltet und mit einem Demeter-Garten versehen. Als weiterer Park steht der Worthgarten im unmittelbaren Stadtbereich Spaziergängern offen. In der Süderstadt wurde der ehemalige Johannisfriedhof im 19. Jahrhundert zur Parkanlage Johannishain umgestaltet. Als Ausflugsziele in der Nähe sind die Altenburg, der Lehof, das Steinholz, der 1913 erworbene Eselstall und die Hamwarte zu nennen. Die dort im 19. Jahrhundert befindlichen Ausflugslokale sind vollständig verschwunden.

Das Nordharzer Städtebundtheater ist mit je zwei Spielstätten in Halberstadt und in den Städtischen Bühnen Quedlinburg sowie mit Sommerbespielung im Bergtheater Thale aktiv. Weitere Theaterbesuche sind in der Waldbühne Altenbrak, der Seebühne Magdeburg und der Schlossbühne Wolfenbüttel möglich.

Der 1981 von Kirchenmusikdirektor Gottfried Biller gegründete Quedlinburger Musiksommer bietet in den Sommermonaten wöchentlich ein Konzert innerhalb einer thematischen Konzertreihe in der Stiftskirche St. Servatii in Quedlinburg an.

Von den verschiedenen Chören seien genannt: der Fritz-Prieß-Chor, der Quedlinburger Oratorienchor und der Ökumenische Jugendchor.

Mittlerweile weist Quedlinburg ein zunehmend angenommenes Veranstaltungsprogramm auf. Als größtes Ereignis kristallisiert sich zur Zeit der Advent in den Höfen heraus, bei dem 2006 an jedem Wochenende über 50.000 Besucher und 2007 zum Besuch von Gotthilf Fischer 75.000 in die Stadt kamen. Traditionell am zweiten und dritten Adventswochenende laden bis zu 24 sonst größtenteils geschlossene Höfe zum Geschenkekaufen, Essen, Glühweintrinken und Verweilen ein.

Die Reihe der Veranstaltungen beginnt im Frühjahr mit dem sogenannten "Kaiserfrühling" zu Ostern und Pfingsten, einem mittelalterlichen Spektakel in der historischen Altstadt. Mitte Mai folgt die deutschlandweit verbreitete Lange Nacht der Museen. Das Programm "Zauber der Bäume", eine Kunst- und Musikinstallationen im Brühlpark findet am ersten Samstag des Monats Juli statt. Über den Sommer verteilt, von Juni bis September, finden die verschiedenen Aufführungen des Quedlinburger Musiksommers statt. Meist im August findet das "Gildefest" der Quedlinburger Kaufmannsschaft statt. Am zweiten Wochenende im September wird der Tag des offenen Denkmals für Deutschland in Quedlinburg eröffnet. In der Stadt sind über 70 Quedlinburger Denkmäler für Besucher kostenlos geöffnet, die sonst meist verschlossen sind, und es wird eine "Quedlinburger Blumenmesse" am Mathildenbrunnen in der Neustadt veranstaltet.
Daneben laden alle drei Monate die Quedlinburger Dixieland- und Swingtage ein, bei denen von einem Konzertort zu nächsten gefahren wird, um die Musik zu hören; weiterhin findet monatlich eine so genannte Milonga, ein Tanzabend mit argentinischem Tango statt, der von Braunschweiger "Milongueras" ausgerichtet wird. Im Sommer 2009 fand erstmals das weltweit ausgetragene kostenlose Musikfestival Fête de la Musique statt.

Die Stadt liegt am Knotenpunkt der Bundesstraßen 79 und 6 sowie der neugebauten vierspurigen B 6n. Der nördliche Anschluss (Quedlinburg-Zentrum) zur B 6n über der mittelalterlichen Siedlung Marsleben (wüst) ist seit 2006 unter Verkehr, der Lückenschluss zwischen Quedlinburg-Zentrum und Quedlinburg-Ost wurde am 1. Dezember 2007 unter Verkehr gestellt. Die Autobahn A 14 ist 40 Kilometer in östlicher, die A 395 44 Kilometer in westlicher, die A 2 50 Kilometer in nördlicher und die A 7 75 Kilometer in westlicher Richtung von der Stadt entfernt.

Die 1863 als Durchgangsbahnhof gebaute Station Quedlinburg hat seit 2006 eine herausragende Bedeutung als Verkehrsknotenpunkt
erhalten: Hier besteht eine Umstiegsmöglichkeit von der Eisenbahnstrecke Halberstadt–Thale zu den Harzer Schmalspurbahnen über die Trasse der umgespurten Bahnstrecke Quedlinburg-Frose bis Gernrode und weiter über die Selketalbahn und Harzquerbahn zur Brockenbahn.
Quedlinburg war seit 1863 Durchgangsbahnhof des Nordharzer Eisenbahnnetzes an der Verbindung von Halberstadt zum Harzrand bei Thale. Auf dieser Verbindung verkehrt stündlich der sogenannte Harz-Elbe-Express im Nahverkehr von Magdeburg über Halberstadt nach Thale. Freitags, samstags und sonntags fährt der private Fernverkehrszug Harz-Berlin-Express von Berlin über Potsdam, Magdeburg, Halberstadt und Quedlinburg nach Thale und zurück.

Der frühere Verkehr über die Nebenstrecke Quarmbeck, Gernrode und Ballenstedt nach Ermsleben, der ältesten regelspurigen Nebenbahn des Harzes, des sogenannten "Balkans" wurde 2004 eingestellt. Hier verkehrt heute der Landesbus 318. 2006 wurde der Abschnitt Gernrode-Quedlinburg als Schmalspurbahn reaktiviert. Diese Stichstrecke Frose–Ballenstedt war 1868 von den Magdeburg-Halberstädter Eisenbahnen (MHE) auf Drängen des Herzogs von Anhalt errichtet worden, der sein Schloss in Ballenstedt erreichen wollte.
Nachdem die "Deutsche Bahn AG" den normalspurigen Streckenabschnitt nach Aschersleben über Gernrode stillgelegt hatte, wurde am 18. April 2005 mit den Arbeiten zur Verlängerung der Selketalbahn von Gernrode nach Quedlinburg begonnen. Dafür wurde zunächst der Endbahnhof Gernrode zu einem Durchgangsbahnhof umgebaut. Die Selketalbahn der HSB wurde bis Ende Dezember 2005 um 8,5 Kilometer von Gernrode nach Quedlinburg verlängert. Die bestehende Normalspurstrecke – auf der Trasse der stillgelegten Bahnstrecke Frose–Quedlinburg – wurde dazu auf Meterspur umgespurt. Am 4. März 2006 fuhr der erste Schmalspurzug der Harzer Schmalspurbahnen in den Bahnhof Quedlinburg ein, und seit dem 26. Juni 2006 gibt es einen planmäßigen Zugbetrieb der Harzer Schmalspurbahnen bis Quedlinburg mit mindestens zwei Dampfzugpaaren am Tag.

Der Busverkehr wird von den Harzer Verkehrsbetrieben im Landkreis Harz durchgeführt. Der Bahnhofsvorplatz ist die zentrale Haltestelle für Fernbuslinien des Unternehmens Flixbus.

In den 1920er Jahren wurde im zwei Kilometer südlich gelegenen Quarmbeck ein Regionalflughafen eröffnet, der in den 1930er Jahren zum Militärflugplatz ausgebaut und in "Römergraben" umbenannt wurde. Während der DDR-Zeit war dort ein sowjetischer Truppenstützpunkt untergebracht. Der Flugbetrieb ist heute eingestellt.

Südwestlich in vier Kilometer Entfernung befindet sich der Verkehrslandeplatz Ballenstedt–Quedlinburg, der über eine 800 Meter lange Asphaltbahn verfügt und zum Nachtflugbetrieb zugelassen ist. Als kleiner Sonderlandeplatz (für Flugzeuge bis 5700 Kilogramm zugelassen) befindet sich drei Kilometer nördlich von Aschersleben der Flugplatz Aschersleben. Etwa 22 Kilometer nordöstlich von Quedlinburg befindet sich der seit dem 1. September 2006 wieder aktivierte Flughafen Magdeburg-Cochstedt.

Die nächstgelegenen internationalen Flughäfen sind 90 Kilometer südöstlich der Flughafen Leipzig/Halle und 120 Kilometer nordwestlich der Flughafen Hannover.

Die ersten Nachweise einer Lateinschule der Benediktikirche und der Nikolaikirche reichen bis 1303 zurück. Seit den 1530er-Jahren sind die Rektoren bekannt. Die Lateinschule der Altstadt führte seit 1623 den Namen "Gymnasium illustre" und seit 1776 die Bezeichnung "Fürstliches Gymnasium". Daneben gab es bis 1787 auch acht sogenannte "deutsche Schulen", die Elementarkenntnisse in Lesen, Schreiben und Rechnen vermittelten. Auch eine Mädchenschule wurde bereits 1539 genannt.

Im 19. Jahrhundert wurden eine katholische Privatschule, mehrere höhere Mädchenschulen und eine jüdische Privatschule gegründet. Neben dem altsprachlichen Gymnasium und der Oberrealschule entwickelte sich ein neusprachliches Lyceum.

Zu DDR-Zeiten wurden die Schulen zu zehn sogenannten Polytechnischen Oberschulen vereinheitlicht, die in zehn Klassen die mittlere Reife vermittelten. Das Abitur konnte in zwei weiteren Jahren auf der Erweiterten Oberschule (EOS) im Konvent (heute GutsMuths-Gymnasium) erworben werden.

Derzeit gibt es in Quedlinburg fünf Grundschulen, zwei Förderschulen (Sine Cura Schule und Pestalozzischule), zwei Sekundarschulen (Bosse- und Bansischule), ein Gymnasium und die Kreismusikschule.

Die "Kleersgrundschule" (ab dem Schuljahr 2008/2009: "Integrationsschule Am Kleers") ist im Rahmen der Errichtung des Neubaugebietes Kleers in den 1980er-Jahren entstanden und führt seit 1991 ihren Namen. Seit 2004 ist sie eine integrative Schule mit Kooperationsklassen, integrativen Klassen und einer umfangreichen Nachmittagsbetreuung, die bei mehreren Landeswettbewerben in den Bereichen Schülerzeitung und Schülertheater siegte.

Die "Bosseschule" (von 1983 bis 1991: "Maxim-Gorki-Oberschule") liegt als Sekundarschule inmitten der Altstadt und ist seit 1955 nach dem deutschen Politiker Robert Bosse benannt. Die Schule nimmt seit 2005 an einem Modellversuch "Produktives Lernen" teil, der eine Verknüpfung von Unterricht und betrieblicher Praxis erreichen soll. Durch die Schließung der "Carl-Ritter-Sekundarschule" im Jahr 2004 musste die Bosseschule räumlich umgebaut werden, um einen Teil der zusätzlichen Schüler aufnehmen zu können.
Das "GutsMuths-Gymnasium" besteht aus zwei Gebäuden: dem 1903 gebauten denkmalgeschützten Hauptgebäude im Konvent und dem "Erxleben-Haus" in der Süderstadt, welches von 1991 bis 1998 als "Süderstadt-Gymnasium" und bis 2004 als "Dorothea-Erxleben-Gymnasium" bezeichnet wurde. Beide Schulen fusionierten im Jahre 2004. In der Süderstadt sind die Klassen 5 bis 9 und im Konvent die Oberstufenklassen 10 bis 12 untergebracht. Die Schule zeichnet sich durch ein breit gefächertes Angebot von Freizeitangeboten aus, darunter Projekte wie "Das Lernen lernen" oder Musik am Computer. Seit 2006 trägt die Schule den Titel "Schule ohne Rassismus, Schule mit Courage". Seit 2007 ist sie eine Ganztagsreferenzschule in Sachsen-Anhalt.

Die "Musikschule Johann Heinrich Rolle", Außenstelle der Kreismusikschule Harz und Mitglied im Verband deutscher Musikschulen (VdM), ist 1952 aus dem seit 1945 bestehenden Landeskonservatorium hervorgegangen. Die musikalische Ausbildung von Kindern und Jugendlichen ist ihr Hauptziel. Dafür werden in Quedlinburg und an den betreuten Außenstellen Thale, Ballenstedt und Harzgerode ungefähr 560 Schüler in 30 Fächern instrumental und vokal unterrichtet.

Weiterführende Bildung ermöglichen die Berufsbildende Schule, die Volkshochschule, die Landesfachschule für Gartenbau, das Deutsche Fachwerkzentrum und eine Reihe von Bildungswerken, wie das Regionale Kompetenzzentrum Harz des "Europäischen Bildungswerkes für Beruf und Gesellschaft e. V.", das Bildungszentrum für das "Hotel- und Gaststättengewerbe Ostharz GmbH", das "Bildungswerk der Wirtschaft Sachsen-Anhalt e. V." und die Kreishandwerkerschaft Harzland-Staßfurt. Die Berufsbildende Schule führt seit 2007 den Namen des Quedlinburger Firmengründers und Saatzüchters "Johann Peter Christian Heinrich Mette" (1735–1806). Die amerikanische Texas Tech University bietet in Quedlinburg (Deutsch-)Kurse für ihre Studenten an.

Die "Landesfachschule für Landwirtschaft, Forsten und Gartenbau, Fachbereich Gartenbau" des Ministeriums für Landwirtschaft und Umwelt befindet sich in Quedlinburg. Sie bietet ein- und zweijährige Fachschulausbildungen (staatlich geprüfter Techniker, Wirtschafter, hauswirtschaftlicher Betriebsleiter) in den Bereichen Garten- und Landschaftsbau und Hauswirtschaft sowie Vorbereitungskurse auf die Meisterprüfung in den genannten Bereichen an. Wegen zu geringer Schülerzahlen ist eine Schließung zum Juni 2013 geplant.

Seit 1999 bildet das IBB – Institut für Berufliche Bildung, A. Gesche an der Berufsfachschule für Kosmetik staatlich geprüfte Kosmetiker/innen aus. Außerdem bietet das IBB pflegerische, kosmetische und kaufmännische Weiterbildungen sowie Berufsausbildungen an der Berufsfachschule Altenpflege und Altenpflegehilfe und der staatlich anerkannten Schule für Podologie (Podologe/Podologin) an.

Das "Deutsche Fachwerkzentrum Quedlinburg" wurde 2002 als Trägerverein der Deutschen Stiftung Denkmalschutz, des Landes Sachsen-Anhalt und der Stadt Quedlinburg unter Mithilfe der Deutschen Bundesstiftung Umwelt gegründet. Das Zentrum betreut ökologische Sanierungen und Bauforschungen und ermöglicht Jugendlichen ein Freiwilliges Jahr in der Denkmalpflege in einer "Jugendbauhütte".

In der Kreisbibliothek Quedlinburg stehen 67.500 Medien zur Ausleihe.

In der Stadt gibt es ein 1903 eröffnetes Hallenbad und eine 2004 eröffnete moderne Dreifelderhalle. Für den Schulsport stehen eine Reihe von Sporthallen zur Verfügung, die zum Teil schon älter sind, so wurde die Kleersturnhalle 1910 erbaut. Die größten öffentlichen Sportplätze befinden sich am Moorberg südlich der Stadt und an der Lindenstraße, nordöstlich der Stadt. Im Jahr 2001 wurde das in den 1950er Jahren gebaute Freibad unweit des letztgenannten Sportplatzes geschlossen und eingeebnet. Die Judo-Halle auf dem Gelände der Polizei ist teilweise für den Breitensport zugänglich.

Das Harzklinikum Dorothea Christiane Erxleben befindet sich am östlichen Stadtrand. Das 1907 eingeweihte Krankenhaus wurde in den 1990er-Jahren zum Haus der Schwerpunktversorgung ausgebaut, es ist zudem Lehrkrankenhaus des Universitätsklinikums der Otto-von-Guericke-Universität Magdeburg. Das Klinikum mit 481 stationären und 50 teilstationären Betten besitzt zwölf stationäre Fachbereiche und drei tagesklinische Einrichtungen. Das Hauttumorzentrum des Klinikums ist neben dem Dessauer das einzige zertifizierte in Sachsen-Anhalt. Jährlich werden ca. 20.000 stationäre und 20.000 ambulante Patienten betreut.

Größter kommunaler Friedhof ist der 1906 eingerichtete Städtische Zentralfriedhof am Badeborner Weg. Er befindet sich im Südosten der Stadt und sein Wegenetz ist sternförmig auf die Kapelle ausgerichtet. Während des Ersten Weltkrieges wurden hier über 700 verstorbene kriegsgefangene Soldaten und ein Großteil der über 160 gefallenen Quedlinburger begraben. Das Gleiche geschah im Zweiten Weltkrieg mit mindestens 110 Kriegsgefangenen und einer unbekannten Zahl Quedlinburger. In dieser Zeit wurde das Krematorium (gebaut 1928) auch zur Verbrennung von mindestens 912 Opfern des KZ Langenstein-Zwieberge benutzt.

Die historischen kirchlichen Friedhöfe befanden sich im unmittelbaren Umfeld der jeweiligen Kirche. Sie lagen innerhalb der Stadtmauern an folgenden Stellen: St.-Aegidii-Friedhof nordöstlich der Kirche, er ist bis auf einzelne späte Grabsteine fast vollständig verschwunden; der St.-Benedikti-Kirchhof liegt unter der neuzeitlichen Pflasterung und wird zum Teil als Parkplatz genutzt (ein Mausoleum ist erhalten); der St.-Nikolai-Kirchhof ist heute eine Grünanlage; ein weiterer Friedhof der St.-Nikolai-Gemeinde lag zwischen der östlichen Bebauung (im nördlichen Teil) der Ballstraße und der Stadtmauer (diese Grünanlage ist als privates Gartengelände erhalten). Alle innerhalb der Stadtmauern gelegenen Friedhöfe wurden zu Beginn des 19. Jahrhunderts aufgelassen. Die Gemeinden legten in der Folge neue Friedhöfe vor den Toren der Stadt an: den Friedhof der Marktkirchgemeinde in der Weststraße (seit 1843, Kapelle 1915), den Friedhof der Blasiikirchgemeinde an der Zwergkuhle (neuerrichtet 1841 bis 1843), den Friedhof der Aegidiigemeinde am Ziegelhohlweg (Mitte 19. Jahrhundert), den Friedhof der Katholischen Gemeinde Weststraße (seit 1868) und den Wiperti- und Servatiikirchhof links und rechts der Wipertistraße (Kapelle 1934/1935). An dieser Stelle befindet sich eine Quedlinburger Besonderheit: die in den Felsen des Kapellenberges eingehauene dreistöckige terrassenförmige Gruftanlage mit über zwanzig Grüften auf jeder Etage und Seite des Berges.
Darüber hinaus wurde der Friedhof der jüdischen Gemeinde Quedlinburgs im 19. Jahrhundert von der Straße Weingarten an die Stelle der heutigen Anlage des Jüdischen Friedhof Quedlinburgs an der Zwergkuhle verlegt.

Zu Zeiten der Industrialisierung wuchs auch in Quedlinburg die wirtschaftliche Kraft. Im Süden der Stadt siedelten sich zahlreiche Betriebe, Unternehmen und Firmen an, die besonders in den Bereichen Metallverarbeitung oder landwirtschaftliche Samenzucht zu Hause waren. Der Zuwachs der Beschäftigten in dieser Zeit kam in dem neu gebauten Wohngebiet der "Süderstadt" unter. Nach dem Zweiten Weltkrieg wurden alle diese Werke zwangsenteignet und in staatliche Formen wie Volkseigener Betrieb oder Landwirtschaftliche Produktionsgenossenschaft überführt. Größter Arbeitgeber wurde das Werk "Mertik", der Nachfolgebetrieb von "Hartmann & Söhne", in dem zwischenzeitlich mehr als 3000 Menschen beschäftigt waren. Als weiterer ehemaliger Betrieb ist der VEB Union zu nennen, in dem Schnellkochtöpfe (auch für den Export) und Essgeschirre für die Nationale Volksarmee hergestellt wurden. Das ehemalige volkseigene Gut "August Bebel" hat Saatgut für den landwirtschaftlichen Bedarf und Spezial-Kulturen erzeugt. Die 1874 gegründete Farbenfabrik Wilhelm Brauns, seit 1959 "VEB Farb-Chemie Quedlinburg", produzierte bis 2004 Farb- und Klebstoffe. Viele dieser Betriebe, deren Produktion fast ausschließlich auf die Mitgliedsstaaten des sozialistischen Rates für gegenseitige Wirtschaftshilfe ausgerichtet war, gingen nach der Wiedervereinigung 1990 in die Insolvenz. Die leeren Betriebs- und Lagerhallen stehen zum Teil bis heute.

Eines der wenigen Unternehmen, das die Marktanpassung geschafft hat, ist die Walzengießerei & Hartgusswerk Quedlinburg GmbH, die 1865 gegründet wurde und eine der wenigen Gießereien in Sachsen-Anhalt ist.

Die Nachfolgeeinrichtungen der 1945 enteigneten Saatzuchtbetriebe wurden nach 1990 zu Teilinstituten der Bundesanstalt für Züchtungsforschung an Kulturpflanzen (BAZ), einer dem Bundesministerium für Ernährung, Landwirtschaft und Verbraucherschutz zugeordneten Forschungseinrichtung, umgewandelt. Von den neun Teilinstituten der BAZ befinden sich fünf in Quedlinburg. Es sind dies das Institut für gartenbauliche Kulturen, das Institut für Epidemiologie und Resistenzressourcen, das Institut für Resistenzforschung und Pathogendiagnostik, das Institut für Pflanzenanalytik und das Forschungs- und Koordinierungszentrum für pflanzengenetische Ressourcen. Seit Jahresbeginn 2008 hat das neu gegründete Julius Kühn-Institut – Bundesforschungsinstitut für Kulturpflanzen (JKI), entstanden aus der Biologische Bundesanstalt für Land- und Forstwirtschaft (BBA), der BAZ und der Bundesforschungsanstalt für Landwirtschaft (FAL), seinen Hauptsitz in Quedlinburg. Neben der Funktion des Hauptsitzes dieser Forschungseinrichtung, sind nun sechs Forschungsschwerpunkte: Epidemiologie und Pathogendiagnostik, Ökologische Chemie, Pflanzenanalytik und Vorratsschutz, Resistenzforschung und Stresstoleranz, Sicherheit biotechnologischer Verfahren bei Pflanzen, Züchtungsforschung an gartenbaulichen Kulturen und Obst und Züchtungsforschung an landwirtschaftlichen Kulturen in Quedlinburg angesiedelt. Auch privatwirtschaftliche Unternehmen wie "satimex Quedlinburg", "Quedlinburger Saatgut" oder "International Seeds Processing" (ISP) konnten sich etablieren.

Die Wirtschaftsbereiche unterteilen sich in: 2 Prozent Landwirtschaft, 19,29 Prozent Industrie und 78,71 Prozent Dienstleistungsbereich. Die Landwirtschaft ist spezialisiert auf Saatzucht, die Industrie auf Baugewerbe mit Spezialleistungen für Restaurierung und Sanierung, Bauelementefertigung, Holzverarbeitung, Metallverarbeitung und Pharmazie sowie Druckerei, der Dienstleistungssektor vornehmlich auf Tourismus.

Der Tourismus stellt für Quedlinburg eine der wichtigsten wirtschaftlichen Größen dar, und so zählt die Schaffung einer modernen touristischen Infrastruktur zu den Hauptvorhaben. An Übernachtungskapazitäten in Quedlinburg stehen den auswärtigen Gästen 20 Pensionen, 18 Hotels und eine Jugendherberge zur Verfügung. Die Anzahl der Übernachtungen ist stark saisonabhängig, mit Spitzenwerten um Ostern, von Mai bis Juli, von September bis Oktober und zum Advent/Jahreswechsel. Größte Schwächezeit ist von Januar bis März. In den Spitzenzeiten sind die Kapazitäten in Quedlinburg und im ganzen Vorharz sehr stark ausgelastet. Die meisten Hotels wurden nach 1994 neu gebaut oder vollständig saniert.

Seit 1994 ist Quedlinburg Teil der südlichen Route der Straße der Romanik, einer touristischen Straße zu den romanischen Denkmälern Sachsen-Anhalts. Die St.-Johannes-Kapelle ist seit 2003 eine Station der deutschen Verlängerung des Jakobsweges. Ganz in der Nähe verlaufen die Deutsche Fachwerkstraße und die Deutsche Alleenstraße.

Seit dem 12. November 2008 ist die Stadt staatlich anerkannter Erholungsort.

Der Reiseführer „1000 places to see before you die“ nennt Quedlinburg: „Ein Märchen aus Fachwerk“, der Reiseführer Lonely Planet spricht von einem „ungeschliffenen Juwel“, und die Stadt selbst hat sich 2006 den Leitspruch „Quedlinburg – Wiege Deutschlands“ gegeben (bis 1990 „Blumenstadt Quedlinburg“, bis 2006 „Neugierig auf …?“).

Um den Tourismus zu fördern wird seit 2015 in der Stadt ein WLAN-Netz installiert, welches vor allem in den Einkaufsstraßen empfangbar ist. Es wird durch das Projekt Freifunk Harz realisiert.

Zu den bekannten Persönlichkeiten, die in Quedlinburg geboren wurden, zählen unter anderen
der Rechtswissenschaftler Johann Salomon Brunnquell (1693–1735), Dorothea Erxleben (* 1715), die als erste deutsche Frau in Medizin promovierte, Friedrich Gottlieb Klopstock (* 1724), der Begründer der Erlebnisdichtung und des deutschen Irrationalismus, Johann Christoph Friedrich GutsMuths (* 1759), der als Begründer des modernen Sportunterrichts und Vater der Gymnastik gilt, und auch der Begründer der wissenschaftlichen Erdkunde, Carl Ritter (* 1779). Aus neuerer Zeit sind zu nennen: der Dichter und Maler Fritz Graßhoff (1913–1997), der Schriftsteller Volker von Törne (1934–1980), die ehemalige Präsidentin des Bundesrechnungshofes (1993–2001) Hedda von Wedel (* 1942) und Leander Haußmann (* 1959), der u. a. bei den Filmen Sonnenallee, Herr Lehmann und NVA Regie geführt hat.

Zahlreiche Persönlichkeiten wurden zu Ehrenbürgern der Stadt Quedlinburg ernannt, zum Teil abhängig von den politischen Verhältnissen. So wurden in der Zeit des Nationalsozialismus am 20. April 1933 Adolf Hitler (1889–1945) und am 1. Juni 1937 Heinrich Himmler (1900–1945) die Ehrenbürgerwürde verliehen und nach dem Ende des Zweiten Weltkrieges sofort wieder aberkannt.

Zu den bekanntesten Personen, die durch die Stadt Quedlinburg das Ehrenbürgerrecht erhielten, zählen: 1895 Otto von Bismarck (1815–1898), der erste deutsche Kanzler, 1910 Julius Wolff (1834–1910), ein Dichter und Schriftsteller, und 1998 Gottfried Kiesow (1931–2011), Vorstandsvorsitzender der Deutschen Stiftung Denkmalschutz.

Die "Mitteldeutsche Zeitung" ist mit einer Lokalredaktion in Quedlinburg vertreten. Weiterhin die lokal erscheinenden Blätter "SuperSonntag", "Wochenspiegel" und "Harzer Kreisblatt".

Regionalprogramm des Öffentlich-rechtlichen Rundfunks ist der Mitteldeutsche Rundfunk (MDR) mit Regionalbüro in Halberstadt.

Der Sender des Regionalfernsehens Harz (RFH) kann über das örtliche Kabelnetz hauptsächlich im Harzkreis empfangen werden.

Die Handlung einiger Romane ist in Quedlinburg und Umgebung angesiedelt. So handelt Wilhelm Raabes "Der Schüdderump" (1869) auf der fruchtbaren Erde des geschichtsträchtigen Quedlinburger Landes. Weiterhin spielt der erste Teil von Theodor Fontanes Roman "Cécile" (1887) in Quedlinburg und Thale, ebenso die verschiedenen Romane zu Dorothea Christiane Erxleben und Julius Wolffs Roman "Der Raubgraf. Eine Geschichte aus dem Harzgau" (1884). Weiterhin von Gerhard Beutel "Der Stadthauptmann von Quedlinburg" (Berlin 1972), von Helga Glaesener "Du süße sanfte Mörderin" (München 2000), von Christian Amling "Quitilinga History Land" (2005) und "Odins Fluch. Ein neuer Fall für Irenäus Moll" (2006).

Aufgrund der historischen Bausubstanz bietet sich Quedlinburg als Hintergrund für verschiedene Film- und Fernsehprojekte an. Seit 2012 wird die ARD-Vorabendserie "Heiter bis tödlich: Alles Klara" in der Stadt und ihrer Umgebung gedreht, mit bisher 48 Folgen in drei Staffeln. Die folgende Liste zeigt eine Auswahl von teilweise in Quedlinburg gedrehten Filmen:

An lokal produzierten kulinarischen Spezialitäten sind Imkererzeugnisse, wie reiner Rapshonig, Senfprodukte und Edelbrand aus regionalen Früchten, und das einzige noch in Quedlinburg gebraute Bier "Pubarschknall" der Brauerei Lüdde zu nennen.

Das Hochseeschiff (Typ XD) MS "Quedlinburg" war im August 1967 auf der Warnow-Werft in Rostock vom Stapel gelaufen und fuhr bis Februar 1991 für den VEB Deutsche Seereederei Rostock.

Am 4. Mai 2004 wurde im Hauptbahnhof Magdeburg der ICE Nr. 242 (Baureihe 402/ICE 2) auf den Namen "Quedlinburg" getauft und am 24. September 2008 auf dem Flughafen Frankfurt am Main ein Flugzeug (Bombardier CRJ700) der Lufthansa CityLine. Eine 126 Tonnen schwere Diesellok (Bauart Voith Maxima 40 CC) erhielt am 27. Mai 2011 den Namen "Quedlinburg", da sie für den Transport vom neugebauten Verladebahnhof bei Quedlinburg aus vorgesehen ist.


Zu einer ausführlichen Bibliographie vgl. Brigitte Schröder, Heinz Stoob: "Bibliografie zur deutschen historischen Städteforschung Band 1." Köln 1986, S. 352–354, Nr. 4359–4381.





</doc>
<doc id="4188" url="https://de.wikipedia.org/wiki?curid=4188" title="Quagga">
Quagga

Das Quagga ("Equus quagga quagga") ist eine ausgestorbene Zebra-Form. Sie gilt als südliche Unterart des Steppenzebras ("Equus quagga"), die sich durch eine, verglichen mit anderen Zebraformen, weitestgehende Reduktion der Bestreifung an Rumpf und Beinen auszeichnet. Das Quagga wurde Ende des 19. Jahrhunderts vom Menschen ausgerottet. 

Die ursprüngliche Beschreibung des Quaggas nannte das Tier eine „Mischform aus Pferd und Zebra“, was die äußere Erscheinung gut wiedergibt, aber nichts mit den tatsächlichen Verwandtschaftsverhältnissen zu tun hat. Kräftig schwarz-weiß gestreift waren lediglich Kopf und Hals. Zum Rumpf hin wurden die Streifen blasser und verschmolzen zu einem einfarbigen Rotbraun. Es gab aber auch Tiere mit einer deutlichen Streifenbildung bis zum hinteren Teil des Körpers. Die Beine waren nicht gestreift. Es wurde bis zu 1,30 Meter groß (Stockmaß).

Das Quagga war in Südafrika weit verbreitet und soll bis ins 17. Jahrhundert einer der häufigsten Großsäuger gewesen sein. Dann begann die exzessive Jagd, bei der Tausende von Quaggas geschossen wurden. Farmer betrachteten die Wildtiere als Nahrungskonkurrenten ihrer Rinder, die Tiere wurden aber auch wegen Fleisch und Leder gejagt. Südafrika galt zudem unter Mitgliedern der finanziell gutgestellten Oberschicht als Jagdparadies, wodurch sehr viele Tiere aus „sportlichen“ Motiven getötet wurden. 

Um 1850 war das Quagga südlich des Oranje ausgestorben. Die letzten wilden Quaggas grasten im Oranje-Freistaat, doch auch diese Bestände wurden ausgerottet. Das letzte wilde Quagga wurde schätzungsweise in den späten 1870ern geschossen; nach der Dürre von 1877, der letzte kleinere Populationen zum Opfer fielen, galt das wildlebende Quagga als ausgestorben. Das letzte Tier in Gefangenschaft starb am 12. August 1883 im Artis-Zoo in Amsterdam.
Allerdings berichtet der deutsche Schutztruppenoffizier Victor Franke in seinen Tagebuchaufzeichnungen 1901 noch von kleineren Quaggaherden, die er im damaligen Deutsch-Südwestafrika beobachtet haben will. Franke berichtet an mehreren Stellen, dass er es nicht über sich brächte, auf die „wunderschönen und herrlichen“ Quaggas zu schießen.

Das Quagga bewohnte die trockeneren Grasland-Gebiete in Südafrika. Die nördliche Verbreitungsgrenze scheinen der Oranje, im Osten der Vaal gewesen zu sein, als Südgrenze wird der Great Kei River vermutet.

Die systematische Stellung des Quaggas war und ist umstritten. Die Lehrmeinung, das Quagga als Unterart einzustufen, wurde durch genetische Untersuchungen an Museumsmaterial in den 1980er Jahren gestützt. Die vor allem im angelsächsischen Raum verbreitete Lehrmeinung, das Quagga als eigene Art einzustufen, stützen die genetischen Daten nicht, jedoch werden diese aufgrund der beschränkten Datenbasis von Befürwortern des Artstatus für das Quagga sehr kritisch betrachtet.

Schon in den 1960er Jahren wurde es gelegentlich als Unterart des Steppenzebras eingeordnet. Aber 1980 versuchte sich D.K. Bennett an einer Analyse der Schädelform und kam zu dem Schluss, dass das Quagga mit dem Hauspferd wesentlich näher verwandt sein müsse als mit jeder Zebra-Art.

Um die Frage endgültig zu klären, wurden 1984 vier alten Museumsexemplaren Gewebeproben entnommen, aus denen DNA-Fragmente extrahiert werden konnten. Diese waren mit Proben des Steppenzebras nahezu identisch, so dass der Schluss gezogen wurde, das Quagga sei eine Unterart des Steppenzebras gewesen.

Dieser Schluss ist allerdings immer noch nicht gänzlich unumstritten. So kritisieren Mace A. Hack, Rod East und Dan I. Rubenstein in ihrer Analyse für die IUCN ursprünglich ("Status and Action Plan for the Plains Zebra"), dass eine genetische Analyse auf Basis so fragmentarischer Proben kaum aussagekräftig sei und dass dem Quagga wegen seiner farblichen und formlichen Andersartigkeit der Status einer eigenen Art eingeräumt werden müsse. Untersuchungen aus dem Jahr 2004 ergaben aber auch unter morphologischen und anatomischen Gesichtspunkten eine Stellung des Quaggas als Unterart des Steppenzebras, so dass dies auch heute von der IUCN anerkannt wird. 

In einer neueren Studie von Jennifer Leonhard (Smithsonian Institution) an Genmaterial von acht Museums-Exponaten wurde darauf geschlossen, dass sich das Quagga vor etwa 120.000 bis 290.000 Jahren vom Steppenzebra abgezweigt habe. Da in dieser Zeit die Eisbedeckung der Erde besonders hoch war, erörtert die Wissenschaftlerin, dass ein Klimawechsel die Ursache für diese Aufzweigung gewesen sein könnte.

Mehrere genetische Untersuchungen bestätigten die nahe Verwandtschaft mit dem Steppenzebra, die 1984 veröffentlichten DNA-Analysen waren darüber hinaus Anlass, ein Abbildzüchtungsprojekt des Quaggas aus südlichen Steppenzebras zu starten. Vor allem einzelne Vertreter der südlichen Unterart des Steppenzebras ("E.q. burchelli") zeigen eine deutliche Reduktion der Streifen, was an das Quagga erinnert. Mittlerweile wird in dem von Präparator Reinhold Rau (1932–2006) 1986 initiierten "Quagga Project" versucht, durch selektive Zucht eine Steppenzebra-Zuchtlinie aufzubauen, die äußerlich an die Quaggas erinnert. Mehrere Exemplare ab der dritten Generation zeigen mittlerweile bereits eine erhebliche Streifenreduktion. Ziel des Projekts ist ein Tier, das dem Quagga sehr ähnlich sieht und möglicherweise in den einstigen Lebensräumen ausgewildert werden kann.

Die einzigen erhaltenen 23 Quagga-Präparate finden sich in Museen weltweit, davon ist der Großteil im Besitz deutscher Museen und deren wissenschaftlicher Sammlungen:

Deutschland

Übriges Europa

Der Name „Quagga“ wurde einer Sprache der Khoikhoi entlehnt. Das Doppel-G wurde ursprünglich als kehliges ch ausgesprochen, was heute aber nicht mehr getan wird. Angeblich leitet sich der kehlige Laut vom Ruf des Quaggas ab. Da es aber keine Tonaufzeichnungen der Stimmen der Tiere gibt, ist dies heute nicht mehr nachzuvollziehen.



</doc>
<doc id="4189" url="https://de.wikipedia.org/wiki?curid=4189" title="Quantenfeldtheorie">
Quantenfeldtheorie

Die Quantenfeldtheorie (QFT) ist ein Gebiet der theoretischen Physik, in dem Prinzipien klassischer Feldtheorien (zum Beispiel der klassischen Elektrodynamik) und der Quantenmechanik zur Bildung einer erweiterten Theorie kombiniert werden. Sie geht über die Quantenmechanik hinaus, indem sie Teilchen und Felder einheitlich beschreibt. Dabei werden nicht nur sog. Observablen (also beobachtbare Größen wie Energie oder Impuls) quantisiert, sondern auch die wechselwirkenden (Teilchen-)Felder selbst; Felder und Observable werden also analog behandelt. Die Quantisierung der Felder bezeichnet man auch als Zweite Quantisierung. Diese berücksichtigt explizit die Entstehung und Vernichtung von Elementarteilchen (Paarerzeugung, Annihilation).

Die Methoden der Quantenfeldtheorie kommen vor allem in der Elementarteilchenphysik und in der statistischen Mechanik zur Anwendung. Man unterscheidet dabei zwischen "relativistischen Quantenfeldtheorien", die die spezielle Relativitätstheorie berücksichtigen und häufig in der Elementarteilchenphysik Anwendung finden, und "nicht-relativistischen Quantenfeldtheorien", die beispielsweise in der Festkörperphysik relevant sind.

Die Objekte und Methoden der QFT sind physikalisch motiviert, auch wenn viele Teilbereiche der Mathematik zum Einsatz kommen. Die Axiomatische Quantenfeldtheorie versucht dabei, Grundlagen und Konzepte in einen mathematisch rigorosen Rahmen zu fassen.

Die Quantenfeldtheorie ist eine Weiterentwicklung der Quantenphysik über die Quantenmechanik hinaus. Die vorher existierenden Quantentheorien waren ihrem Aufbau nach Theorien für Systeme mit wenigen Teilchen. Um Systeme mit vielen Teilchen zu beschreiben, ist zwar prinzipiell keine neue Theorie nötig, doch die Beschreibung von bspw. 10 Teilchen in einem Festkörper ist mit den Methoden der Quantenmechanik ohne Näherungen aufgrund des hohen Rechenaufwands rein technisch unmöglich.

Ein fundamentales Problem der Quantenmechanik ist ihre Unfähigkeit, Systeme mit variierender Teilchenzahl zu beschreiben. Die ersten Versuche einer Quantisierung des elektromagnetischen Feldes zielten darauf ab, die Emission von Photonen durch ein Atom zu beschreiben. Außerdem gibt es nach der relativistischen Klein-Gordon-Gleichung und der Dirac-Gleichung die oben erwähnten Antiteilchen-Lösungen. Bei ausreichender Energie ist es dann möglich, Teilchen-Antiteilchen-Paare zu erzeugen, was ein System mit konstanter Teilchenzahl unmöglich macht.

Zur Lösung dieser Probleme behandelt man das Objekt, das in der Quantenmechanik als Wellenfunktion eines Teilchens interpretiert wurde, als "Quantenfeld". Das heißt, dass man es ähnlich behandelt wie eine Observable der Quantenmechanik. Dies löst nicht nur die zuvor genannten Probleme, sondern beseitigt auch Inkonsistenzen der klassischen Elektrodynamik, wie sie z. B. in der Abraham-Lorentz-Gleichung auftreten. Außerdem erhält man Begründungen für das Pauli-Prinzip und das allgemeinere Spin-Statistik-Theorem.

Die Quantenfeldtheorien sind ursprünglich als relativistische Streutheorien entwickelt worden. In gebundenen Systemen sind die Teilchenenergien im Allgemeinen deutlich kleiner als die Massenenergien "mc". Daher ist es in solchen Fällen meist ausreichend genau, in der nichtrelativistischen Quantenmechanik mit der Störungstheorie zu arbeiten. Bei Kollisionen zwischen kleinen Teilchen können jedoch sehr viel höhere Energien auftreten, so dass relativistische Effekte berücksichtigt werden müssen.

Im folgenden Abschnitt wird erklärt, welche Schritte zur Entwicklung einer relativistischen Streutheorie nötig sind. Zunächst wird dazu die Lagrangedichte aufgestellt, dann werden die Felder quantisiert. Zuletzt wird mit den quantisierten Feldern eine Streutheorie beschrieben und ein dabei auftretendes Problem durch die Renormierung gelöst.

Der erste Schritt zu einer Quantenfeldtheorie besteht darin, Lagrangedichten für die Quantenfelder zu finden. Diese Lagrangedichten müssen als Euler-Lagrange-Gleichung die im Allgemeinen bekannte Differentialgleichung für das Feld liefern. Das sind für ein Skalarfeld die Klein-Gordon-Gleichung, für ein Spinorfeld die Dirac-Gleichung und für das Photon die Maxwellgleichungen.

Im Folgenden wird immer 4er-(Raumzeit)-Vektoren-Schreibweise verwendet. Dabei werden die üblichen Kurzschreibweisen benutzt, nämlich die Kurzschreibweise formula_1 für Differentiale und die Einsteinsche Summenkonvention, die besagt, dass über einen oben und einen unten stehenden Index (von 0 bis 3) summiert wird. Im verwendeten Einheitensystem gilt: formula_2.
Dabei bezeichnet formula_3 die Dirac-Matrizen. formula_4 ist der sogenannte adjungierte Spinor. formula_5 sind die Komponenten des Feldstärketensors. Dabei wurden hier die Maxwellgleichungen in kovarianter Formulierung ohne die Quellenterme (Ladungs- und Stromdichte) benutzt.

Die oben aufgeführten Lagrangedichten beschreiben freie Felder, die nicht wechselwirken. Sie ergeben die Bewegungsgleichungen für freie Felder. Für Wechselwirkungen der Felder untereinander müssen den Lagrangedichten zusätzliche Terme hinzugefügt werden. Dabei ist auf folgende Punkte zu achten:

Erlaubte Terme sind zum Beispiel formula_6 wobei "m" und "n" natürliche Zahlen sind (einschließlich Null) und "k" die Kopplungskonstante ist. Wechselwirkungen mit dem Photon werden meist durch die kovariante Ableitung (formula_7) in der Lagrangedichte für das freie Feld realisiert. Dabei ist die elektrische Ladung "e" des Elektrons hier zugleich die Kopplungskonstante des elektromagnetischen Feldes.

Bisher wurde noch keine Aussage über die Eigenschaften der Felder gemacht. Bei starken Feldern mit einer großen Zahl von Bosonen-Anregungen können diese halbklassisch behandelt werden, im Allgemeinen muss man aber zunächst einen Mechanismus entwickeln, um die Auswirkungen der Quantennatur der Felder zu beschreiben. Die Entwicklung eines solchen Mechanismus bezeichnet man als "Feldquantisierung" und sie ist der erste Schritt, um das Verhalten der Felder berechenbar zu machen. Es gibt dabei zwei verschiedene Formalismen, die unterschiedliches Vorgehen beinhalten.


Im Folgenden werden die Grundlagen der Feldquantisierung für freie Felder in beiden Formalismen erklärt.

Für die Feldquantisierung im kanonischen Formalismus benutzt man den Hamilton-Formalismus der klassischen Mechanik. Man ordnet dabei jedem Feld (formula_8 bzw. formula_9) ein "kanonisch konjugiertes Feld" formula_10 analog dem kanonischen Impuls zu. Das Feld und sein kanonisch konjugiertes Feld sind dann im Sinne der Quantenmechanik konjugierte Operatoren, sogenannte Feldoperatoren, und erfüllen eine Unschärferelation, wie Ort und Impuls in der Quantenmechanik. Die Unschärferelation kann entweder durch eine Kommutatorrelation (für Bosonen nach dem Spin-Statistik-Theorem) oder eine Antikommutatorrelation (für Fermionen) analog zum Kommutator von Ort und Impuls realisiert werden. Den Hamilton-Operator, der die Energie des Systems charakterisiert, erhält man, indem man die Hamilton-Funktion bildet und darin die Felder durch die Feldoperatoren ersetzt. Er ist in der Regel positiv definit oder darf zumindest keine unbeschränkt negativen Eigenwerte haben, da ein solches System unter beliebig großer Energieabgabe an die Umgebung in immer tiefere Energieeigenzustände fallen würde.

Für skalare Felder erhält man formula_11 als kanonisch konjugiertes Feld zu formula_12 und formula_13 als kanonisch konjugiertes Feld zu formula_14. Die geforderte Kommutatorrelation lautet
Es ist in Quantenfeldtheorien üblich, im Impulsraum zu rechnen. Dazu betrachtet man die Fourier-Darstellung des Feldoperators, die für das Skalarfeld lautet
Dabei sind formula_17 der Impuls und formula_18 die Stufenfunktion, die bei negativem Argument 0 und sonst 1 ist.
Da formula_19 und formula_20 Operatoren sind, trifft dies auch auf formula_21, formula_22, formula_23 und formula_24 zu. Ihre Kommutatoren folgen aus dem Kommutator der Feldoperatoren. Der Operator formula_22 kann als Operator interpretiert werden, der ein Teilchen mit Impuls formula_17 erzeugt, während formula_24 ein Antiteilchen mit Impuls formula_17 erzeugt. Entsprechend können formula_21 und formula_23 als Operatoren interpretiert werden, die ein Teilchen oder Antiteilchen mit Impuls formula_17 vernichten.
Die Verwendung der Kommutatorrelationen führt wie gewünscht zu einem positiv definiten Hamilton-Operator. Es können beliebig viele Skalarfelder im selben Zustand sein (Bose-Einstein-Statistik).

Wenn man für ein Spinorfeld analog vorgeht, erhält man formula_32 als kanonisch konjugiertes Feld zu formula_33 und formula_34 als kanonisch konjugiertes Feld zu formula_35. Damit ergeben sich die geforderten (Anti-)Kommutatorrelationen zu
Dabei sind formula_37 und formula_38 Spinorindizes. Man betrachtet dann wieder analog die Fourier-Darstellung des Feldoperators und berechnet den Hamilton-Operator. Einen positiven Hamilton-Operator erhält man beim Spinorfeld jedoch nur, wenn man Antikommutatoren benutzt. Diese werden mit geschweiften Klammern geschrieben, was in den obigen Formeln bereits vorweggenommen wurde. Aufgrund dieser Antikommutatoren ergibt die zweimalige Anwendung desselben Erzeugungsoperators auf einen Zustand den Nullzustand. Das bedeutet, dass nie zwei Spin-1/2-Teilchen im selben Zustand sein können (Pauli-Prinzip). Spinorfelder gehorchen daher der Fermi-Dirac-Statistik.

Für Eichfelder lauten die geforderten Kommutatorrelationen
wobei formula_40 die Komponenten der Minkowski-Metrik bezeichnet. Allerdings erhält man aus der Lagrangedichte formula_41, was die geforderte Kommutatorrelation nicht erfüllen kann. Die Quantisierung von Eichfeldern ist daher nur bei Festlegung einer Eichbedingung möglich. Die Festlegung einer geeigneten Eichbedingung, die den Zugang über Kommutatorrelationen von Feldern ermöglicht und gleichzeitig die Lorentzinvarianz der Lagrangedichte erhält, ist kompliziert.

Man verwendet meist eine Abwandlung der Lorenz-Eichung, um sinnvoll ein kanonisch konjugiertes Feld definieren zu können. Der Formalismus wird nach seinen Entwicklern Suraj N. Gupta und Konrad Bleuler als Gupta-Bleuler-Formalismus bezeichnet.

Eine Alternative stellt eine physikalische Eichung wie z. B. die temporale plus eine weitere Eichbedingung dar. Hier werden zwei der vier Polarisationen des Eichfeldes als physikalische Freiheitsgrade direkt durch die Wahl der Eichung formula_42 sowie durch die anschließende Implementierung des Gaußschen Gesetzes formula_43 als Bedingung an die physikalischen Zustände eliminiert. Der wesentliche Vorteil ist die Reduzierung des Hilbertraumes auf ausschließlich physikalische, transversale Freiheitsgrade. Dem steht als Nachteil der Verlust einer manifest kovarianten Formulierung gegenüber.

Im Pfadintegralformalismus werden die Felder nicht als Operatoren, sondern als einfache Funktionen behandelt. Das Pfadintegral stellt im Wesentlichen eine Übergangsamplitude von einem Vakuumzustand zum Zeitpunkt formula_44 zu einem Vakuumzustand zum Zeitpunkt formula_45 dar, wobei über alle dazwischen möglichen Feldkonfigurationen ("Pfade") integriert wird, mit einem Phasenfaktor, der durch die Wirkung festgelegt wird. Es hat für das Skalarfeld die Form

Um allerdings überhaupt Wechselwirkungen bei einem Übergang vom Vakuum zum Vakuum zu erhalten, müssen Felder erzeugt und vernichtet werden können. Dies wird im Pfadintegralformalismus nicht mithilfe von Erzeugungs- und Vernichtungsoperatoren, sondern durch Quellenfelder erzielt. Es wird also zur Lagrangedichte ein Quellenterm der Form formula_47 hinzugefügt. Das Quellenfeld "J(x)" soll nur in einem endlichen Intervall auf der Zeitachse von Null verschieden sein. Das bedeutet, dass die wechselwirkenden Felder genau innerhalb dieses Zeitintervalls existieren. Das volle Pfadintegral für ein freies Skalarfeld hat damit die Form

Das lässt sich wegen der Integration über formula_12 mit einem Analogon des gaußschen Fehlerintegrals in eine Form bringen, die in bestimmter Weise nur noch vom Quellenfeld "J(x)" abhängt, und zwar:

Dabei ist formula_51 gegeben durch formula_52 also gewissermaßen als das Inverse des Klein-Gordon-Operators (formula_53 ist der D’Alembert-Operator). Dieses Objekt wird als zeitgeordnete Greensche Funktion oder Feynman-Propagator bezeichnet. Man bezeichnet das Pfadintegral daher auch als "Erzeugendenfunktional des Propagators", da die Ableitungen nach formula_54 und formula_55 effektiv einer Multiplikation mit dem Propagator entsprechen.

Das Verhalten des freien Feldes in Anwesenheit von Quellen wird nur durch den Propagator und das Quellenfeld bestimmt. Dieses Ergebnis entspricht der Erwartung, denn das Verhalten eines Feldes, das nicht wechselwirkt, ist offenbar nur durch seine Eigenschaften bei Erzeugung und Vernichtung und seine freie Bewegung bestimmt. Erstere stecken im Quellenfeld und das Bewegungsverhalten wird durch den Klein-Gordon-Operator bestimmt, dessen Informationsgehalt hier durch sein Inverses gegeben ist.

Bei der Quantisierung des Spinorfeldes im Pfadintegral-Formalismus tritt das Problem auf, dass die Felder einerseits wie normale zahlenwertige Funktionen behandelt werden, auf der anderen Seite jedoch antikommutieren. Normale Zahlen kommutieren jedoch. Diese Schwierigkeit lässt sich lösen, indem man die Fermionfelder als Elemente einer Graßmann-Algebra, sogenannte Graßmann-Zahlen, auffasst. Rechnerisch bedeutet das nur, dass man sie wie antikommutierende Zahlen behandelt. Durch die Graßmann-Algebra ist diese Vorgehensweise theoretisch abgesichert. Das Pfadintegral mit Quellenfeldern formula_56 und formula_57 hat dann die Form

Daraus lässt sich, wie beim skalaren Feld, eine Form ableiten, die in bestimmter Weise nur noch von formula_59 und formula_60 abhängt. Dabei lässt sich erneut ein Analogon des gaußschen Integrals anwenden, das allerdings nicht dem gewohnten Formalismus entspricht, sondern in gewisser Weise dazu „invers“ ist. Zunächst ist es jedenfalls nötig, einen Integralbegriff für Graßmann-Zahlen zu entwickeln. Dann lässt sich das Pfadintegral in die folgende Form bringen:

Dabei ist formula_62 das Inverse des Dirac-Operators, das auch als Dirac-Propagator bezeichnet wird. Analog zum skalaren Feld ergibt sich auch hier eine Form, die erwartungsgemäß nur von den Quellenfeldern und der Dynamik der Felder bestimmt ist.

Das Pfadintegral für ein Eichfeld ist von der Form

Der Operator formula_64 hat jedoch kein Inverses. Das erkennt man daran, dass er bei Anwendung auf Vektoren des Typs formula_65 Null ergibt. Mindestens einer seiner Eigenwerte ist also Null, was analog einer Matrix dafür sorgt, dass der Operator nicht invertierbar ist.

Daher lässt sich hier nicht dieselbe Vorgehensweise anwenden, wie beim skalaren Feld und beim Spinorfeld. Man muss der Lagrangedichte einen zusätzlichen Term hinzufügen, so dass man einen Operator erhält, zu dem es ein Inverses gibt. Dies ist äquivalent dazu, eine Eichung festzulegen. Daher bezeichnet man den neuen Term als "eichfixierenden Term". Er ist allgemein von der Form formula_66. Die dazu korrespondierende Eichbedingung lautet formula_67.

Das führt jedoch dazu, dass die Lagrangedichte von der Wahl des Eichterms "f" abhängt. Dieses Problem lässt sich durch das Einführen von sogenannten Faddejew-Popow-Geistern beheben. Diese Geister sind antikommutierende skalare Felder und widersprechen damit dem Spin-Statistik-Theorem. Sie können daher nicht als freie Felder auftreten, sondern nur als sogenannte virtuelle Teilchen. Durch die Wahl der sogenannten Axial-Eichung lässt sich das Auftreten dieser Felder vermeiden, was ihre Interpretation als mathematische Artefakte naheliegend erscheinen lässt. Ihr Auftreten in anderen Eichungen ist jedoch aus tieferliegenden theoretischen Gründen (Unitarität der S-Matrix) zwingend notwendig für die Konsistenz der Theorie.

Die vollständige Lagrangedichte mit eichfixierendem Term und Geistfeldern ist von der Eichbedingung abhängig. Für die Lorenz-Eichung lautet sie bei nichtabelschen Eichtheorien
Dabei ist formula_57 das Geistfeld und formula_70 das Anti-Geistfeld.

Für abelsche Eichtheorien wie den Elektromagnetismus nimmt der letzte Term unabhängig von der Eichung die Form formula_71 an. Daher kann dieser Teil des Pfadintegrals einfach integriert werden und trägt nicht zur Dynamik bei.

Das Pfadintegral liefert auch einen Zusammenhang mit den Verteilungsfunktionen der statistischen Mechanik. Dazu wird die "imaginäre" Zeitkoordinate im Minkowskiraum analytisch in den euklidischen Raum fortgesetzt und statt komplexer Phasenfaktoren im Wegintegral erhält man reelle ähnlich den Boltzmann-Faktoren der statistischen Mechanik. In dieser Form ist diese Formulierung auch Ausgangspunkt von numerischen Simulationen der Feldkonfigurationen (meist zufällig im Monte-Carlo-Verfahren mit einer Wichtung über diese "Boltzmannfaktoren" ausgewählt) in Gitter-Rechnungen. Sie liefern die bisher genauesten Methoden z. B. für die Berechnung von Hadronmassen in der Quantenchromodynamik.

Wie oben schon ausgeführt, ist das Ziel der vorangegangenen Verfahren die Beschreibung einer relativistischen Streutheorie. Obwohl die Methoden der Quantenfeldtheorien heute auch in anderen Zusammenhängen genutzt werden, ist die Streutheorie noch heute eines ihrer Hauptanwendungsgebiete. Daher werden die Grundlagen derselben an dieser Stelle erläutert.

Das zentrale Objekt der Streutheorie ist die sogenannte "S-Matrix" oder "Streumatrix", deren Elemente die Übergangswahrscheinlichkeit von einem Anfangszustand formula_72 in einen Ausgangszustand formula_73 beschreiben. Die Elemente der S-Matrix bezeichnet man als Streuamplituden. Auf der Ebene der Felder ist die S-Matrix also bestimmt durch die Gleichung

Die S-Matrix lässt sich im Wesentlichen als Summe von Vakuumerwartungswerten von zeitgeordneten Feldoperatorprodukten (auch n-Punkt-Funktionen, Korrelatoren oder Greensche Funktionen genannt) schreiben. Ein Beweis dieser sogenannten LSZ-Zerlegung ist einer der ersten großen Erfolge der axiomatischen Quantenfeldtheorie. Im Beispiel einer Quantenfeldtheorie, in der es nur ein Skalarfeld gibt, hat die Zerlegung die Form
Dabei ist "K" der Klein-Gordon-Operator und T der Zeitordnungsoperator, der die Felder aufsteigend nach dem Wert der Zeit formula_76 ordnet. Falls noch andere Felder als das Skalarfeld vorkommen, müssen jeweils die entsprechenden Hamilton-Operatoren verwendet werden. Für ein Spinorfeld muss z. B. der Dirac-Operator statt des Klein-Gordon-Operators verwendet werden.

Zur Berechnung der S-Matrix genügt es also, die zeitgeordneten n-Punkt-Funktionen formula_77 berechnen zu können.

Als nützliches Werkzeug zur Vereinfachung der Berechnungen der n-Punkt-Funktionen haben sich die Feynman-Diagramme erwiesen. Diese Kurzschreibweise wurde 1950 von Richard Feynman entwickelt und nutzt aus, dass sich die Terme, die bei der Berechnung der n-Punkt-Funktionen auftreten, in eine kleine Anzahl elementarer Bausteine zerlegen lassen. Diesen Term-Bausteinen werden dann Bildelemente zugeordnet. Diese Regeln, nach denen diese Zuordnung geschieht, bezeichnet man als Feynman-Regeln. Die Feynman-Diagramme ermöglichen es damit, komplizierte Terme in Form kleiner Bilder darzustellen.

Dabei gibt es zu jedem Term in der Lagrangedichte ein entsprechendes Bildelement. Der Massenterm wird dabei zusammen mit dem Ableitungsterm als ein Term behandelt, der das freie Feld beschreibt. Diesen Termen werden für verschiedene Felder meist verschiedene Linien zugeordnet. Den Wechselwirkungstermen entsprechen dagegen Knotenpunkte, sogenannte "Vertices", an denen für jedes Feld, das im Wechselwirkungsterm steht, eine entsprechende Linie endet. Linien, die nur an einem Ende mit dem Diagramm verbunden sind, werden als reale Teilchen interpretiert, während Linien, die zwei Vertices verbinden als virtuelle Teilchen interpretiert werden. Es lässt sich auch eine Zeitrichtung im Diagramm festlegen, so dass es als eine Art Veranschaulichung des Streuprozesses interpretiert werden kann. Dabei muss man jedoch zur vollständigen Berechnung einer bestimmten Streuamplitude alle Diagramme mit den entsprechenden Anfangs- und Endteilchen berücksichtigen. Wenn die Lagrangedichte der Quantenfeldtheorie Wechselwirkungsterme enthält, sind dies im Allgemeinen unendlich viele Diagramme.

Wenn die Kopplungskonstante kleiner ist als eins, werden die Terme mit höheren Potenzen der Kopplungskonstante immer kleiner. Da nach den Feynmanregeln jeder Vertex für die Multiplikation mit der entsprechenden Kopplungskonstante steht, werden die Beiträge von Diagrammen mit vielen Vertices sehr klein. Die einfachsten Diagramme liefern also den größten Beitrag zur Streuamplitude, während die Diagramme mit zunehmender Kompliziertheit gleichzeitig immer kleinere Beiträge liefern. Auf diese Weise lassen sich die Prinzipien der Störungstheorie unter Erzielung guter Ergebnisse für die Streuamplituden anwenden, indem nur die Diagramme niedriger Ordnung in der Kopplungskonstanten berechnet werden.

Die Feynman-Diagramme mit geschlossenen inneren Linien, die sogenannten Schleifendiagramme (z. B. Wechselwirkung eines Elektrons mit „virtuellen“ Photonen aus dem Vakuum, Wechselwirkung eines Photons mit virtuell erzeugten Teilchen-Antiteilchen Paaren aus dem Vakuum), sind meist divergent, da über alle Energien/Impulse (Frequenz/Wellenzahl) integriert wird. Das hat zur Folge, dass sich kompliziertere Feynman-Diagramme zunächst nicht berechnen lassen. Dieses Problem lässt sich jedoch häufig durch ein sogenanntes Renormierungsverfahren beheben, nach einer falschen Rückübersetzung aus dem Englischen auch manchmal als „Renormalisierung“ bezeichnet.

Es gibt grundsätzlich zwei verschiedene Sichtweisen auf diese Prozedur. Die erste traditionelle Sichtweise ordnet die Beiträge der divergierenden Schleifendiagramme so an, dass sie wenigen Parametern in der Lagrangefunktion wie Massen und Kopplungskonstanten entsprechen. Dann führt man Gegenterme (counter terms) in der Lagrangefunktion ein, die als unendliche „nackte“ Werte dieser Parameter diese Divergenzen aufheben. Das ist in der Quantenelektrodynamik möglich, ebenso in der Quantenchromodynamik und anderen solchen Eichtheorien, bei anderen Theorien wie der Gravitation dagegen nicht. Dort wären unendlich viele Gegenterme nötig, die Theorie ist „nicht renormierbar“.

Eine zweite neuere Sichtweise aus dem Umfeld der Renormierungsgruppe beschreibt die Physik je nach Energiebereich durch verschiedene „effektive“ Feldtheorien. Beispielsweise ist die Kopplungskonstante in der Quantenchromodynamik energieabhängig, für kleine Energien geht sie gegen Unendlich (confinement), für hohe Energien gegen Null (Asymptotische Freiheit). Während in der QED die „nackten“ Ladungen durch die Vakuumpolarisation (Paarerzeugung und -vernichtung) wirksam abgeschirmt werden, liegt der Fall bei Yang-Mills-Theorien wie der QCD wegen der Selbstwechselwirkung der geladenen Eichbosonen komplizierter.

Man vermutet, dass sich alle Kopplungskonstanten physikalischer Theorien bei genügend hohen Energien annähern, und dort wird die Physik dann durch eine große vereinheitlichte Theorie der Grundkräfte beschrieben. Das Verhalten von Kopplungskonstanten und die Möglichkeit von Phasenübergängen mit der Energie wird durch die Theorie der Renormierungsgruppe beschrieben. Aus solchen theoretischen Extrapolationen hat es in den 1990er Jahren erste Hinweise auf die Existenz supersymmetrischer Theorien gegeben, für die sich die Kopplungskonstanten am besten in einem Punkt treffen.

Die technische Vorgehensweise ist jedoch unabhängig von der Sichtweise. Es wird zunächst eine Regularisierung vorgenommen, indem ein zusätzlicher Parameter in die Rechnung eingeführt wird. Dieser Parameter muss zuletzt wieder gegen null oder unendlich laufen (je nach Wahl) um die ursprünglichen Terme wieder zu erhalten. Solange der Regularisierungsparameter jedoch als endlich angenommen wird, bleiben die Terme endlich. Man formt dann die Terme so um, dass die Unendlichkeiten nur noch in Termen auftreten, die reine Funktionen des Regularisierungsparameters sind. Diese Terme werden dann weggelassen. Danach setzt man den Regulierungsparameter null bzw. unendlich, wobei das Ergebnis nun endlich bleibt.

Diese Vorgehensweise wirkt auf den ersten Blick willkürlich, doch das „Weglassen“ muss nach bestimmten Regeln erfolgen. Dadurch wird sichergestellt, dass die renormierten Kopplungskonstanten bei niedrigen Energien den gemessenen Konstanten entsprechen.

Ein spezielles Gebiet der relativistischen Quantenmechanik betrifft Lösungen der relativistischen Klein-Gordon-Gleichung und der Dirac-Gleichung mit negativer Energie. Dies würde es Teilchen erlauben, zu unendlicher negativer Energie abzusteigen, was in der Realität nicht beobachtet wird. In der Quantenmechanik löst man dieses Problem, indem man die entsprechenden Lösungen willkürlich als Entitäten mit positiver Energie interpretiert, die sich rückwärts in der Zeit bewegen; man überträgt also in der Wellenfunktion das negative Vorzeichen von der Energie "E" auf die Zeit "t", was wegen der Beziehung formula_78 naheliegend ist ( "h"  ist die Plancksche Konstante und formula_79 das der Energiedifferenz formula_80 zugeordnete Frequenzintervall).

Paul Dirac interpretierte diese rückwärts bewegten Lösungen als Antiteilchen.

Durch Kombination des elektroschwachen Modells mit der Quantenchromodynamik entsteht eine vereinte Quantenfeldtheorie, das so genannte Standardmodell der Elementarteilchenphysik. Es enthält alle bekannten Teilchen und kann die meisten bekannten Vorgänge erklären.

Gleichzeitig ist aber bekannt, dass das Standardmodell nicht die endgültige Theorie sein kann. Zum einen ist die Gravitation nicht enthalten, zum anderen gibt es eine Reihe von Beobachtungen (Neutrinooszillationen, Dunkle Materie), nach denen eine Erweiterung des Standardmodells notwendig scheint. Außerdem enthält das Standardmodell viele willkürliche Parameter und erklärt z. B. das sehr unterschiedliche Massenspektrum der Elementarteilchenfamilien nicht.

Die im Folgenden erläuterten Quantenfeldtheorien sind alle im Standardmodell enthalten.

Die Lagrangedichte der formula_81-Theorie lautet
Diese Quantenfeldtheorie besitzt große theoretische Bedeutung, da sie die einfachste denkbare Quantenfeldtheorie mit einer Wechselwirkung ist und hier im Gegensatz zu realistischeren Modellen einige exakte mathematische Aussagen über ihre Eigenschaften gemacht werden können. Sie beschreibt ein selbstwechselwirkendes reelles oder komplexes Skalarfeld.

In der statistischen Physik spielt sie eine Rolle als einfachstes Kontinuumsmodell für die (sehr allgemeine) Landau-Theorie der Phasenübergänge zweiter Ordnung und der kritischen Phänomene. Von der statistischen Interpretation aus bekommt man zugleich einen neuen und konstruktiven Zugang zum Renormierungsproblem, indem gezeigt wird, dass die Renormierung der Massen, Ladungen und Vertex-Funktionen durch Eliminierung kurzwelliger Wellenphänomene aus der sog. Zustandssumme formula_83 (englisch: „Partition Function“) erreicht werden kann. Auch das Higgsfeld des Standardmodells hat eine formula_81-Selbstwechselwirkung, die allerdings noch um Wechselwirkungen mit den anderen Feldern des Standardmodells ergänzt wird. In diesen Fällen ist die Kopplungskonstante "m" negativ, was einer imaginären Masse entspräche. Diese Felder werden daher als tachyonische Felder bezeichnet. Diese Bezeichnung bezieht sich jedoch auf das "Higgsfeld" und nicht auf das "Higgs-Teilchen", das sogenannte Higgs-Boson, welches kein Tachyon, sondern ein gewöhnliches Teilchen mit reeller Masse ist. Das Higgsteilchen wird auch nicht durch das Higgsfeld beschrieben, sondern nur durch einen bestimmten Anteil dieses Feldes.

Die Lagrangedichte der Quantenelektrodynamik (QED) lautet
Die QED ist die erste physikalisch erfolgreiche Quantenfeldtheorie. Sie beschreibt die Wechselwirkung eines Spinorfeldes mit Ladung "-e", das das Elektron beschreibt, mit einem Eichfeld, das das Photon beschreibt. Man erhält ihre Bewegungsgleichungen aus der Elektrodynamik durch Quantisierung der maxwellschen Gleichungen. Die Quantenelektrodynamik erklärt mit hoher Genauigkeit die elektromagnetische Wechselwirkung zwischen geladenen Teilchen (zum Beispiel Elektronen, Myonen, Quarks) mittels Austausch von virtuellen Photonen sowie die Eigenschaften von elektromagnetischer Strahlung.

Dadurch lassen sich etwa die chemischen Elemente, ihre Eigenschaften und Bindungen und das Periodensystem der Elemente verstehen. Auch die Festkörperphysik mit der wirtschaftlich bedeutsamen Halbleiterphysik leiten sich letztendlich von der QED ab. Konkrete Rechnungen werden allerdings in der Regel im vereinfachten, aber ausreichenden Formalismus der Quantenmechanik durchgeführt.

Die schwache Wechselwirkung, deren bekanntester Effekt der Betazerfall ist, nimmt eine physikalisch geschlossene Formulierung nach Vereinheitlichung mit der QED im elektroschwachen Standardmodell an.
Die Wechselwirkung wird hier durch Photonen, W- und Z-Bosonen vermittelt.

Ein anderes Beispiel einer QFT ist die Quantenchromodynamik (QCD), welche die Starke Wechselwirkung beschreibt.
In ihr wird ein Teil der im Atomkern auftretenden Wechselwirkungen zwischen Protonen und Neutronen auf die subnukleare Wechselwirkung zwischen Quarks und Gluonen reduziert.

Interessant ist in der QCD, dass die Gluonen, welche die Wechselwirkung vermitteln, selbst miteinander wechselwirken. (Das wäre am Beispiel der QED etwa so, als ob sich zwei durchdringende Lichtstrahlen direkt beeinflussen würden.) Eine Konsequenz dieser gluonischen Selbstwechselwirkung ist, dass die elementaren Quarks nicht einzeln beobachtet werden können, sondern immer in Form von Quark-Antiquark-Zuständen oder Zuständen dreier Quarks (oder Antiquarks) auftreten (Confinement). Auf der anderen Seite folgt daraus, dass die Kopplungskonstante bei hohen Energien nicht zunimmt, sondern abnimmt. Dieses Verhalten wird als "asymptotische Freiheit" bezeichnet.

Wie oben schon angesprochen, eignet sich die formula_81-Theorie zur Beschreibung von Systemen mit spontaner Symmetriebrechung oder kritischen Punkten. Der Massenterm wird dazu als Teil des Potentials verstanden. Für eine reelle Masse hat dieses Potential dann nur ein Minimum, während bei imaginärer Masse das Potential eine w-förmige Parabel vierten Grades beschreibt. Wenn das Feld mehr als eine reelle Komponente hat, erhält man noch mehr Minima. Bei einem komplexen Feld (mit zwei reellen Komponenten) erhält man zum Beispiel die Rotationsfigur der w-förmigen Parabel mit einem Minimakreis. Diese Form wird auch als "Mexican Hat Potential" bezeichnet, da das Potential an die Form eines Sombrero erinnert.

Jedes Minimum entspricht nun einem Zustand niedrigster Energie, die vom Feld alle mit gleicher Wahrscheinlichkeit angenommen werden. In jedem dieser Zustände hat das Feld jedoch ein geringeres Maß an Symmetrie, da die Symmetrie der Minima untereinander durch Auswahl eines Minimums verloren geht. Diese Eigenschaft der klassischen Feldtheorie überträgt sich auf die Quantenfeldtheorie, so dass sich die Möglichkeit ergibt, Quantensysteme mit gebrochener Symmetrie zu beschreiben. Beispiele für solche Systeme sind das Ising-Modell aus der Thermodynamik, das die spontane Magnetisierung eines Ferromagneten erklärt, und der Higgs-Mechanismus, der die Massen der Eichbosonen in der schwachen Wechselwirkung erklärt. Durch die erhaltenen Massenterme der Eichbosonen wird nämlich die Eichsymmetrie reduziert.

Die Axiomatische Quantenfeldtheorie versucht, ausgehend von einem Satz möglichst weniger, als mathematisch oder physikalisch unumgänglich angesehener Axiome, eine konsistente Beschreibung der Quantenfeldtheorie zu erzielen.

Die axiomatische Quantenfeldtheorie wurde u. a. aus den Wightman-Axiomen, entstanden im Jahr 1956, begründet. Ein weiterer Zugang ist die von Haag und Araki 1962 formulierte algebraische Quantenfeldtheorie, die durch die Haag-Kastler-Axiome charakterisiert wird. Die Osterwalder-Schrader-Axiome stellen einen dritten axiomatischen Zugang zur Quantenfeldtheorie dar.

Etliche konkrete Ergebnisse konnten mit dieser Herangehensweise erzielt werden, zum Beispiel die Herleitung des Spin-Statistik-Theorems und des CPT-Theorems alleine aus den Axiomen, d. h. unabhängig von einer speziellen Quantenfeldtheorie. Ein früher Erfolg war die 1955 von Lehmann, Symanzik und Zimmermann entwickelte LSZ-Reduktionsformel für die S-Matrix. Außerdem existiert ein von Bogoliubov, Medvedev und Polianov begründeter funktionalanalytischer Zugang zur S-Matrix-Theorie (auch BMP-Theorie genannt).

Weitere Anwendungen im Bereich der klassischen Statistik und der Quantenstatistik sind schon sehr weit fortgeschritten. Sie reichen von der allgemeinen Ableitung der Existenz thermodynamischer Größen, Satz von Gibbs, Zustandsgrößen wie Druck, innerer Energie und Entropie bis zum Beweis der Existenz von Phasenübergängen und der exakten Behandlung wichtiger Vielteilchensysteme:

Versuche, diese Quantenfeldtheorien mit der allgemeinen Relativitätstheorie (Gravitation) zur Quantengravitation zu vereinen, sind bisher ohne Erfolg geblieben. Nach Ansicht vieler Forscher erfordert die Quantisierung der Gravitation neue, über die Quantenfeldtheorie hinausgehende Konzepte, da hier der Raum-Zeit Hintergrund selbst dynamisch wird. Beispiele aus der aktuellen Forschung sind die Stringtheorie, die M-Theorie und die Loop-Quantengravitation. Weiter liefern die Supersymmetrie, die Twistor-Theorie, die Finite Quantenfeldtheorie und die Topologische Quantenfeldtheorie wichtige konzeptionelle Ideen, die zurzeit in der Fachwelt diskutiert werden.

Auch in der Festkörpertheorie finden sich Anwendungen der (nicht-relativistischen) Quantenfeldtheorie, und zwar hauptsächlich in der Vielteilchentheorie.

Allgemeine Einführungen in das Thema (jeweils in alphabetischer Reihenfolge der (Erst-)Autoren)

Deutsch:

Englisch:

Speziellere und verwandte Themen und



</doc>
<doc id="4190" url="https://de.wikipedia.org/wiki?curid=4190" title="Quintenzirkel">
Quintenzirkel

Als Quintenzirkel bezeichnet man in der Musiktheorie eine Reihe von zwölf im Abstand temperierter Quinten angeordneten Tönen, deren letzter Ton die gleiche Tonigkeit wie der erste hat und demzufolge mit ihm gleichgesetzt werden kann. Diese Gleichsetzung ist jedoch nur möglich aufgrund einer enharmonischen Verwechslung, die an beliebiger Stelle erfolgen kann. Durch die Rückkehr zum Anfang ergibt sich ein „Rundgang“, der grafisch als Kreis (lat.:" circulus" „Kreis“) dargestellt wird.

Der Quintenzirkel leistet dreierlei:

Die älteste Darstellung des Quintenzirkels findet sich in einem 1711 gedruckten Generalbasstraktat von Johann David Heinichen.
Ein Quartenzirkel entsteht, wenn man statt von Quinten von temperierten Quarten ausgeht, was jedoch für die Theorie keinen Unterschied zum Quintenzirkel ausmacht. Arnold Schönberg begründet dies folgendermaßen: „Geht man in der einen Richtung des Kreises (C, G, D, A usw.), so ist das der Quintenzirkel, oder wie ich lieber sage: Quintenzirkel aufwärts, weil es die über dem Ausgangspunkt sich aufbauenden Quinten sind. Geht man in der entgegengesetzten Richtung des Kreises, so erhält man C, F, B, Es usw., was manche den Quartenzirkel nennen, was aber wenig Sinn hat, denn C, G ist Quint nach oben oder Quart nach unten und C, F Quint nach unten oder Quart nach oben. Deshalb nenne ich die entgegengesetzte Richtung lieber Quintenzirkel abwärts.“
Da man Quintschritte nach oben auch durch Quartschritte nach unten ersetzen kann, ist es möglich, einen Rundgang durch den Quintenzirkel innerhalb einer Oktave durchzuführen: 
Dies ist auch die gebräuchlichste Reihenfolge der Bassknöpfe bei einem Akkordeon, Stradella-Bass genannt.

Der Quintenzirkel ist kein von der Natur vorgegebenes Ordnungssystem wie etwa die Obertonreihe, sondern eine künstliche Konstruktion, die nur aufgrund von temperierten Stimmungen und ihrer Möglichkeit zur enharmonischen Umdeutung von Tönen denkbar ist. In einer Stimmung mit theoretisch reinen Quinten, wie der pythagoreischen Stimmung, kann kein geschlossener Quintenzirkel, sondern nur eine Quintenspirale verwirklicht werden: Zwölf Quinten ("c g d a e h fis cis gis dis ais eis his") umschließen zwar ein Intervall von circa sieben Oktaven; nimmt man jedoch die Quinten und Oktaven mit ihren reinen Schwingungszahlverhältnissen 3:2 und 2:1 an, so zeigt sich rechnerisch, dass der Abschlusston "his" der Quintenreihe geringfügig um 23,5 Cent (um fast einen Viertel Halbton) höher ist als das gleichzeitig erreichte letzte "c" der Oktavenreihe. Dieser Unterschied, pythagoreisches Komma genannt, verhindert die Gleichsetzung von "his" und "c" sowie aller anderen, ebenfalls um das pythagoreische Komma verschiedenen, enharmonischen Tonpaare ("fis/ges, dis/es, ais/b" usw.). Die Quintenfolge schließt sich somit nicht zum Kreis, sondern lässt sich nur als Spirale darstellen. Würde man dessen ungeachtet versuchen, den Kreis „mit Gewalt“ zu schließen, so ginge das nur über eine sogenannte Wolfsquinte.

Das Problem, die Quintenspirale ohne solche misstönenden Wolfsquinten zum Kreis zu schließen, wird bei der heute verbreiteten gleichstufig temperierten Stimmung dadurch gelöst, dass man jede der zwölf Quinten um 1/12 des pythagoreischen Kommas verkleinert, so dass man mit dem zwölften Quintschritt exakt zur siebten Oktave des Ausgangstons gelangt. Durch die gleichmäßige Verteilung des Kommas wird erreicht, dass zwar außer der Oktave kein Intervall mehr ganz rein klingt, die Unsauberkeiten jedoch so gering bleiben, dass sie kaum noch stören.

Obwohl der Quintenzirkel für sich weder sachlich noch historisch zwingend an das Dur-Moll-System gebunden ist, wird er heute in erster Linie verwendet, um die Verhältnisse im Bereich der Dur- und Molltonarten zu veranschaulichen.

Die nebenstehende Grafik zeigt im Äußeren des Kreises die Dur-Tonarten, die im Deutschen mit Großbuchstaben bezeichnet werden.

Im Inneren des Kreises stehen die parallelen Moll-Tonarten, die mit kleinen Buchstaben benannt werden. Sie haben jeweils die gleichen Vorzeichen wie die zugehörigen Dur-Tonarten.

Der Grafik ist zu entnehmen, dass bei einem Fortschreiten im Quintenzirkel mit jeder Tonart ein Vorzeichen hinzutritt oder verschwindet.



Die notwendige enharmonische Verwechslung wird zumeist zwischen Fis- und Ges-Dur vorgenommen, um die Zahl der benötigten Vorzeichen klein zu halten. Man könnte die Umdeutung jedoch auch an jeder anderen Stelle des Quintenzirkels vornehmen. Im Prinzip könnte man jede b-Tonart auch als Kreuz-Tonart und jede Kreuz-Tonart auch als b-Tonart notieren. Zwischen enharmonischen Tonarten besteht hinsichtlich ihrer Vorzeichen ein einfacher Zusammenhang:
Die Vorzeichen treten sukzessive im Quintabstand hinzu, bei den Kreuz-Tonarten "aufwärts" (im Uhrzeigersinn) fortschreitend. Die Reihenfolge der mit einem zu versehenden Noten (F, C, G, D, A, E, H...) ist aufgrund des Quintabstandes ebenfalls im Quintenzirkel ablesbar, wenn man bei F beginnt und im Uhrzeigersinn weiter geht. (Die Kreuze () erhöhen Töne um jeweils einen Halbtonschritt):


Als Merkhilfe für die Reihenfolge der Dur-Tonarten mit Kreuz-Vorzeichen im Quintenzirkel wurden zahlreiche Sprüche erfunden, zum Beispiel:

Die Bewegung der Tonarten ebenso wie das Hinzutreten der Vorzeichen im Quintabstand, die eine Erniedrigung der Töne erzwingen, erfolgt bei den -Tonarten "abwärts":

Auch hier lässt sich die Reihenfolge der mit einem zu versehenden Noten (H, E, A, D, G, C, F, …) aufgrund des Quintabstandes im Quintenzirkel ablesen, wenn man bei H beginnt und gegen den Uhrzeigersinn weitergeht.


Als Merkhilfe für die Reihenfolge der Dur-Tonarten mit b-Vorzeichen im Quintenzirkel wurden zahlreiche Sprüche erfunden, zum Beispiel:

Die Molltonarten sind im Quintenzirkel als Paralleltonarten den Durtonarten zugeordnet. (Paralleltonarten haben die gleichen Vorzeichen.) Bei Kenntnis der Vorzeichen für die Durtonarten kann man die Vorzeichen einer bestimmten Molltonart leicht ermitteln, wenn man weiß, dass die parallele Durtonart stets eine kleine Terz (= drei Halbtonschritte) höher liegt. So hat beispielsweise g-Moll die gleichen Vorzeichen wie das um eine kleine Terz höhere B-Dur.

Die Reihe der Kreuz- und b-Tonarten ließe sich beliebig fortführen, bei den Kreuztonarten etwa: Cis-Dur mit sieben , Gis-Dur mit acht  bis hin zu Eis-Dur mit elf  usw. Aber das macht nicht nur die Notation sehr unübersichtlich; weitere Erhöhungen bereits erhöhter Töne führen auch im Klangbild nur zu schon dagewesenen Tönen.

So würde bei His-Dur (zwölf ) neben den aus Fis-Dur schon bekannten 6  ein „His“ (klingend wie C), ein „Fisis“ (doppelte Erhöhung von F, klingend wie G), ein „Cisis“ (klingend wie D), ein „Gisis“ (klingend wie A), ein „Disis“ (klingend wie E) und ein „Aisis“ (klingend wie H) hinzutreten. Da aber His-Dur sich (bei gleichstufiger Stimmung) nicht von C-Dur unterscheidet, wäre eine solche Notation wenig sinnvoll.

Deshalb nutzt man das Phänomen der klanglichen Gleichheit unterschiedlich benannter Töne zur "enharmonischen Verwechslung": Statt der immer komplizierter werdenden Kreuztonarten verwendet man die entsprechenden, gleich klingenden -Tonarten, etwa statt Gis-Dur (8 ) As-Dur (4 ). Oder, um beim Beispiel der Tonart „Eis-Dur“ zu bleiben: Statt elf  braucht F-Dur nur ein einziges .

Tonarten mit mehr als sechs Vorzeichen werden so gut wie nie eingesetzt, wenn es um die Wahl der Grundtonart eines Musikstücks geht. Bachs Verwendung von Cis-Dur (7 ) im Wohltemperierten Klavier hat Seltenheitswert.
Im Verlauf eines Musikstücks kann es jedoch sinnvoll sein, auch Tonarten mit vielen Vorzeichen zu verwenden. So würde man z. B. bei einer kurzzeitigen Modulation von E-Dur (4 ) in die Obermediante Gis-Dur (8 ) die Notierung mit Kreuzen beibehalten, um den harmonischen Zusammenhang zu verdeutlichen. Erst wenn ein längeres Verweilen in der neuen Tonart angestrebt wird, ist es üblich, die alten Vorzeichen aufzulösen und durch die einfachere Schreibweise der enharmonischen Tonart (in diesem Falle As-Dur mit 4 ) zu ersetzen.

Die aus den Kirchentönen (Modi) hervorgegangenen modalen Tonleitern, zu denen auch die zu Dur (Ionisch) und dem natürlichen Moll (Äolisch) gehörigen Tonleitern zu rechnen sind, lassen sich aus dem Quintenzirkel ableiten. Dazu wählt man jeweils sieben im Quintenzirkel benachbarte Töne aus und sortiert diese anschließend so um, dass sie im Sekundabstand aufeinanderfolgen. Um beispielsweise alle Tonleitern mit dem Grundton C zu erhalten, wendet man dieses Verfahren auf alle siebentönigen Quintenzirkel-Ausschnitte an, die das C enthalten. Das Ergebnis zeigt folgende Tabelle:
In der Tabelle sind „gleichnamige“, also auf dem gleichen Grundton basierende Tonarten aufgeführt, wobei die durgeschlechtlichen Tonarten mit einem großen C, die mollgeschlechtlichen mit einem kleinen c gekennzeichnet sind. Die Zirkeldarstellung rechts zeigt parallele Tonarten: Die jeweils auf einem bestimmten Radius liegenden Tonarten haben gleiche Vorzeichen.

Auch bei der Bestimmung der Größe der Quintenbreite kann der Quintenzirkel verwendet werden. Ordnet man jedem Ton einer Tongruppe seine Position im Quintenzirkel zu, lässt sich der größte Abstand zweier Töne in Quintsprüngen leicht ablesen.

Die Verwirklichung des Quintenzirkels mit Hilfe temperierter Stimmungen und die dadurch geschaffene Möglichkeit des uneingeschränkten Gebrauchs aller Tonarten fand ihren Niederschlag in Kompositionen, welche die Möglichkeiten der Transposition und Modulation in besonderer Weise ausschöpfen.


Die folgenden Kompositionen modulieren durch alle Tonarten des Quintenzirkels.

Beispiele aus der Rock-/Popmusik: Der französische Perkussionist Pierre Moerlen spielte Ende der 1970er Jahre einige Stücke, die sich durch den Quintenzirkel bewegten, u. a. mit seinem Bruder Benoît und den Oldfield-Geschwistern.

Von Synästhetikern wurden Zusammenhänge zwischen Synästhesie und Musik hergestellt. Entsprechend wurden die Stufen des Quintenzirkels vielfältig mit Farbvorstellungen assoziiert, wobei die farblichen Zuordnungen von Fall zu Fall durchaus differieren können. Hier einige Beispiele:




</doc>
<doc id="4192" url="https://de.wikipedia.org/wiki?curid=4192" title="Quelltext">
Quelltext

Quelltext, auch Quellcode () oder unscharf "Programmcode" genannt, ist in der Informatik der für Menschen lesbare, in einer Programmiersprache geschriebene Text eines Computerprogrammes. Abstrakt betrachtet kann der Quelltext für ein Programm auch als "Software-Dokument" bezeichnet werden, welches das Programm formal so exakt und vollständig beschreibt, dass dieses aus ihm vollständig automatisch von einem Computer in Maschinensprache übersetzt werden kann.

Quelltext kann auch (teilweise oder komplett) nicht-textueller Form sein, zum Beispiel als grafische Verknüpfung von logischen Funktionsblöcken (beispielsweise in Simulink oder als UML-Diagramm). Der Quelltext eines Programms kann mehrteilig sein, zum Beispiel auf mehrere Dateien (evtl. unterschiedlicher Formate) aufgeteilt sein oder teilweise grafisch, teilweise textuell vorliegen (z. B. UML, angereichert mit Methodenimplementierungen in der verwendeten Programmiersprache).

Quelltext wird meist manuell mit Hilfe einer integrierten Entwicklungsumgebung oder eines Texteditors erstellt. Es gibt aber auch Codegeneratoren, die den Code aus strukturierten Entwurfsdokumenten, z. B. Struktogrammen oder UML-Entwürfen, automatisch generieren. Eine weitere Erstellungsmöglichkeit sind Entwicklungswerkzeuge, die aus Entwürfen Code und umgekehrt auch aus Code wieder Entwürfe erzeugen können. Dabei werden manuelle Änderungen am Code nachträglich wieder in den Entwurf übernommen. Auf diese Weise ist ein „Round-Trip-Engineering“ möglich, bei dem an jeder Stelle manuelle Veränderungen in den gesamten Entwicklungsprozess eingebracht werden können.

Zum Erstellen des Quelltextes ist meist ein einfacher Texteditor ausreichend. Mithilfe sprachspezifischer Editoren können gewisse Arbeitsschritte vereinfacht werden: Durch Syntaxhervorhebung werden Teile des Quelltextes entsprechend ihrer Funktionalität farblich hervorgehoben, was die Lesbarkeit verbessert. Ebenfalls zur Verbesserung der Lesbarkeit halten sich Programmierer meist an eine bestimmte Quelltextformatierung (z. B. Einrückung von Unterabschnitten, Groß-/Kleinschreibung …). In manchen Sprachen ist eine bestimmte Quelltextformatierung vorgegeben (z. B. Fortran-77, Python), manche integrierte Entwicklungsumgebung kann den Quelltext automatisch formatieren (sog. "Beautifier").

Bei größeren Programmen, die aus vielen einzelnen Quelldateien bestehen, werden mitunter Informationen verwaltet, welche die Abhängigkeiten der Quelltextdateien untereinander beschreiben. Dies erlaubt beim Kompilieren mittels eines einzigen Aufrufes, alle Arbeitsschritte zur Erstellung des fertigen Programms auszuführen und nur jene Bestandteile erneut zu übersetzen, welche zwischenzeitlich geändert wurden oder von geänderten Komponenten abhängen. Beispiel hierfür sind Makefiles.

Zur besseren Dokumentation der Änderungen oder der gezielten Synchronisierung von mehreren gleichzeitig arbeitenden Programmierern wird der Quelltext häufig mit einer Software-Versionsverwaltung gespeichert, wodurch Änderungen später einsehbar sind und erforderlichenfalls rückgängig gemacht werden können.

Bevor das Programm, das der Programmierer schreibt, von einem Computer ausgeführt werden kann, muss es in Maschinensprache, also in eine vom Computer verständliche Folge von Bits, umgesetzt werden. Dies kann entweder vorab durch einen Compiler oder – zur Laufzeit – durch einen Interpreter oder JIT-Compiler geschehen. In vielen Fällen wird mittlerweile eine Kombination aus beiden Varianten gewählt, wobei zuerst der Quelltext der jeweiligen Programmiersprache – meist vom Programmierer veranlasst – in einen abstrakten Zwischencode übersetzt wird, welcher dann zur Laufzeit von einer Laufzeitumgebung durch einen Interpreter oder JIT-Compiler in den Maschinencode überführt wird. Dieses Prinzip hat den Vorteil, dass ein und derselbe Zwischencode auf sehr vielen verschiedenen Plattformen ausführbar ist und somit nicht für jedes auf dem Markt übliche System eine eigene Version der Software erscheinen muss. Typische Beispiele für einen solchen Zwischencode sind der Java-Bytecode sowie die Common Intermediate Language. Mittels eines Debuggers kann die Funktionsweise des Programmes zur Laufzeit verfolgt werden.

Kommen bei der Programmierung Precompiler zum Einsatz, so kann der vom Programmierer erzeugte "originäre Quelltext" u. a. Ausdrücke/Anweisungen enthalten, die die eigentliche Programmiersprache nicht ‚versteht‘. Ein Beispiel dafür sind SQL-Anweisungen. Solche Anweisungen werden vom Precompiler in Befehle der Programmiersprache übersetzt; in den dabei oft nur "temporär erzeugten Quelltextdaten" werden die Originalanweisungen meist als Kommentare eingefügt, um sichtbar zu bleiben. Ähnlich wird auch beim Einsatz von Makroprozessoren verfahren: Diese generieren aus den als Makro formulierten Anweisungen Befehle der jeweiligen Programmiersprache.
Die bei der Kompilierung erzeugten Maschinenbefehle (bei höheren Programmiersprachen meist mehrere je Quellcodeanweisung) repräsentieren in der Regel die programmierte Funktion selbst (bei einfachen Befehlen wie z. B. Addieren von Variableninhalten); dazu gehören auch Aufrufe von Betriebssystemfunktionen. Alternativ kann der Compiler eine der programmierten Anweisung entsprechende Routine, z. B. aus einer Programmbibliothek, im Zielcode direkt einfügen – oder nur einen Aufrufbefehl für eine solche Routine (z. B. für einen Datenbankaufruf oder für komplexe Algorithmen). So aufgerufene Routinen führen im Quelltext codierte Anweisungen als Unterprogramm und somit ‚gekapselt‘ aus.

Software und der dazugehörige Quelltext unterliegen dem Urheberrecht. Sie können in zwei Kategorien unterteilt werden: Proprietäre Software und quelloffene-Software.

"Quelloffene" Programme, also solche, die unter einer „Open-Source“-Lizenz stehen, werden in der Regel direkt mit ihrem Quelltext ausgeliefert, dessen Änderung und Weitergabe durch die Lizenz gestattet ist. Dies ermöglicht deren Studium, Prüfung und Anpassung für spezifische Bedürfnisse. Verfechter des Open-Source-Prinzips sind der Meinung, dass dies zudem die Qualität verbessere, da Fachleute die Fehler besser lokalisieren könnten und sie entweder direkt beheben oder den ursprünglichen Programmierern qualitativ bessere Fehlermeldungen liefern könnten. Die Möglichkeit, Open-Source-Software anhand ihres Quelltextes zu überprüfen, erhöhe das Vertrauen des Anwenders in ihre Korrektheit und Funktionalität im Sinne des Anwenders. Freie Software ist in diesem Sinne mit Open-Source-Software identisch.

"Proprietäre" Software wird in der Regel ohne Quelltext (Closed Source) oder nur unter besonders restriktiven Lizenzen ausgeliefert. Dem Schutz des Abnehmers/Anwenders dienen dann manchmal Vereinbarungen zur Quelltexthinterlegung "(Source Code Escrow Agreements)". Bei individuell beauftragter Erstellung von Individualsoftware durch Dritte wird in der Regel die Übergabe des Quelltexts ebenfalls vereinbart. Bei Software, die ein Anwender, zum Beispiel ein Unternehmen, für eigene Zwecke erstellt, sind Lizenzvereinbarungen überflüssig.

Im weiteren Sinne versteht man unter Quelltext auch die menschenlesbare Beschreibung eines gerenderten Mediums.



</doc>
<doc id="4194" url="https://de.wikipedia.org/wiki?curid=4194" title="Quam">
Quam

Quam war ein Mobilfunkanbieter mit Geschäftstätigkeit in Deutschland, Österreich und der Schweiz. Er gehörte der "Group 3G", einem Konsortium aus der finnischen Sonera (mittlerweile Telia Company) und der spanischen Telefónica Móviles.

Das Konsortium unter dem Group 3G bestand aus der spanischen Telefónica Móviles (57,2%iger Anteil) und der finnischen Sonera (heute Telia Company) (42,8%iger Anteil), sowie zunächst (bis zu dessen Übernahme durch France Télécom) die britische Orange. Es hatte seinen Sitz in München.

Das Gemeinschaftsunternehmen ersteigerte im August 2000 eine der sechs deutschen UMTS-Lizenzen für 16,45 Milliarden DM. 
Quam startete Anfang des dritten Quartals 2001 seinen Netzbetrieb.

Da Quam kein eigenes Mobilfunknetz besaß, kooperierte es mit dem Mobilfunkbetreiber E-Plus, indem das Mobilfunknetz von E-Plus unter eigenem Namen und mit eigenen Rufnummern genutzt wurde (MVNO). Im Dezember 2001 gab es Probleme mit der Zusammenschaltung der Netze von Quam mit den Netzen der anderen Mobilfunkbetreiber. Unter anderem konnten Quam-Kunden aus vielen Netzen nicht direkt erreicht werden, da Quam nach Aussagen anderer Netzbetreiber notwendige Verbindungsleistungen nicht rechtzeitig bestellt hatte. Anrufer mussten zunächst eine kostenlose Quam-Vermittlung anwählen und sich dann von dieser zum gewünschten Gesprächspartner verbinden lassen. Nachdem sich abzeichnete, dass dieser Zustand noch längere Zeit andauern würde, stellte das Unternehmen mitten im Weihnachtsgeschäft 2001 den Verkauf ein. Die meist in teuren Innenstadtlagen befindlichen Quam-Ladengeschäfte servierten lediglich Kaffee, was den Quam-Geschäften den zweifelhaften Spitznamen „Deutschlands exklusivste Cappuccino-Bars“ einbrachte.

Am 4. Februar 2002 erhielt Quam die ironische Auszeichnung „Goldener Marketingflop 2001“, der von der Universität Duisburg verliehen wird. Begründet wurde die Auszeichnung mit dem riesigen Marketing-Budget von 50 Millionen Euro, während die technischen Voraussetzungen für den Telefoniebetrieb noch nicht gegeben waren.

Im Juli 2002 stellte Quam seine GSM-Dienste ein. Zu diesem Zeitpunkt hatte der Anbieter etwa 200.000 Kunden. Weiterhin war eine spätere Nutzung der UMTS-Lizenz in Planung. Am 15. Oktober 2002 wurde das endgültige Ende des Angebots von GSM-Diensten zum 15. November 2002 bekannt gegeben. Quam empfahl seinen Kunden einen Wechsel zum Konkurrenten T-Mobile, der dem Unternehmen dafür eine Prämie zahlte. Die UMTS-Lizenz verblieb beim Anbieter, da die Lizenzbedingungen der Regulierungsbehörde für Telekommunikation und Post es nicht zuließen, diese zu verkaufen. Der Lizenznehmer musste sich der Bedingung unterwerfen, bis Ende 2003 einen Versorgungsgrad von 25 % zu erreichen. Da bei einer Prüfung durch die Bundesnetzagentur keine Aktivität festgestellt werden konnte, wurde die Lizenz- und Frequenzzuteilung im Jahr 2004 wieder entzogen (Az.: 13 A 2969/07). Das Oberverwaltungsgericht Münster wies die Klage auf Erstattung der rd. 8,5 Mrd. Euro im Jahr 2009 ab. Eine Revision wurde durch das Oberverwaltungsgericht nicht zugelassen, wohl aber durch das Bundesverwaltungsgericht. Letzteres wies die zugelassene Revision allerdings im August 2011 zurück: Der Widerruf der Frequenznutzungsrechte sei als einschränkende Konkretisierung von Art. 14 GG zulässig gewesen; auch eine Erstattung könne nicht beansprucht werden. Das Bundesverfassungsgericht hat diese Entscheidung durch Beschluss vom 25. Juni 2015 bestätigt und eine eingelegte Verfassungsbeschwerde nicht zur Entscheidung angenommen.

Die entzogenen UMTS-Frequenzblöcke wurden im Frühjahr 2010 erneut versteigert.

Im April 2006 wurde die Domain quam.eu von der Muttergesellschaft Telefónica reserviert, bevor diese später wieder zum Verkauf angeboten wurde.




</doc>
<doc id="4195" url="https://de.wikipedia.org/wiki?curid=4195" title="Querruder">
Querruder

Querruder () sorgen bei fast allen 3-Achs-gesteuerten Flugzeugen für die Flugsteuerung um die Längsachse.

Querruder sind im Allgemeinen bewegliche Klappen an den Tragflächen-Hinterkanten, die beim Betätigen der Rollsteuerung gleichzeitig und entgegengesetzt bewegt werden. Das Querruder, das nach unten bewegt wird, erhöht auf seiner Seite den Auftrieb, wodurch sich diese Tragfläche hebt. Das andere Querruder bewegt sich nach oben, verringert somit den Auftrieb und die Tragfläche senkt sich. So entsteht eine Rollbewegung um die Längsachse, welche wiederum beispielsweise die erforderliche Schräglage als Ausgangssituation für einen koordinierten Kurvenflug herbeiführt. 

Teilen sich mehrere aerodynamisch wirksame Flächen die Hinterkante einer Tragfläche, so sind die Querruder meist die am weitesten außen angeordneten Steuerflächen, um eine gute Hebelwirkung bei der Erzeugung des Rollmomentes zu erreichen, Landeklappen hingegen sind meist im inneren Bereich der Flügel zu finden. Bei Düsenverkehrsflugzeugen existieren oft mehrere Querruderklappen pro Tragflächenseite, die sich in Länge, Tiefe und Position unterscheiden und die für verschiedene Geschwindigkeitsbereiche ("Low-, High-Speed") ausgelegt sind.

Die Steuerklappen werden durch Seilzüge, Schubstangen, Hydraulikaktuatoren, elektrohydraulisch oder elektrisch betätigt, bei größeren Flugzeugen werden meist mehrere Möglichkeiten zur redundanten Absicherung vorgesehen. Die Querruder-Trimmung erfolgt durch so genannte Trimmruder.

Im modernen Flugzeugbau werden oft die ursprünglichen Funktionen einzelner Steuerflächen mit anderen Steuerfunktionen gemischt und überlagert, um die Steuerbarkeit zu verbessern, die Flugsicherheit zu erhöhen und die Leistungsdaten des Flugzeuges zu verbessern. Die Einführung von Flugkontrollrechnern und Fly-by-wire-Flugsteuerungen unterstützt diese neuen Möglichkeiten.

Bei Verkehrsflugzeugen wird zur Unterstützung der Querruder üblicherweise ein Teil der Störklappen-Segmente - meist die am Flügel am weitesten außen gelegenen - eingesetzt. Bei einer Steuereingabe wird proportional zusätzlich zum nach oben gehenden Querruder des kurveninneren Flügels der Spoiler ein Stück weit mit betätigt (ausgefahren), während die Spoilerklappen auf der gegenüberliegenden Tragfläche in ihrer eingefahrenen Ruheposition verharren. Dies wirkt neben einer Unterstützung des Rollens durch einseitige Auftriebsverminderung an der abzusenkenden Tragfläche dem durch die Querruder erzeugten "Negativen Wendemoment" entgegen, da an der kurveninneren Tragfläche der Widerstand erhöht wird, was wiederum ein sinnrichtiges unterstützendes Giermoment in Kurvenrichtung zur Folge hat. Beim Airbus A380 werden beispielsweise sechs der auf jeder Seite vorhandenen acht Spoilersegmente zur Unterstützung der Querruder im Außenflügel eingesetzt.

Bei vielen modernen Kampfflugzeugen in üblicher Tragenflächen-/Leitwerkauslegung unterstützen die meist als Pendelruder ohne Dämpfungsfläche ausgeführten Höhenruder die Querruder, indem sie zusätzlich – überlagert zu den für die Höhensteuerung erforderlichen Ruderausschlägen – wie Querruder gegensinnig bewegt werden. Beim Tornado ersetzten diese gegensinnig ausschlagenden Höhenruder herkömmliche Querruder gänzlich. Diese Bauart wird im englischen Fachbegriff auch Taileron ("combined tailplane and aileron") genannt. Der Vorteil der damit möglichen extrem großen, fast über die ganze Flügellänge gehenden Landeklappen (und somit hohe Zuladung bzw. hohe Auftriebserzeugung für engen Kurvenflug im Luftkampf) wird durch eine leicht verminderte Rollrate erkauft. Um diese wichtige Kenngröße eines Kampfflugzeug auf die gefordert hohen Werte zu bringen, wurden die Flügel des Tornados relativ weit außen – und somit sehr wirksam – mit je einer Störklappe ("Rollspoiler") pro Flügel ausgestattet. 

Bei Nurflügel- und Deltakonfigurationen kommen oft Steuerflächen an der Hinterkante der Tragflächen zum Einsatz, die gleichzeitig überlagerte Steuerausschläge für Höhen- und Rollsteuerung ausführen. Diese werden mit dem Kunstwort "Elevon" (aus den englischen Begriffen "elevator" für Höhenruder und "aileron" für Querruder) bezeichnet.

Nimmt die Querruderklappe durch gleichsinnige Verstellbarkeit eine weitere Funktionen als Wölbklappe zur Auftriebssteigerung (Landehilfe) oder zur Profiloptimierung wahr, spricht man von einem Flaperon (aus den englischen Begriffen "Flap" für Klappe und "aileron" für Querruder). Ein Beispiel hierfür ist das Segelflugzeug LS3.

Erstmals wurde ein funktionierendes und bekanntes Querruder von dem Franzosen Robert Esnault-Pelterie entworfen und getestet, wie es auch heute in der Luftfahrt noch verwendet wird. Zuvor im 19. Jahrhundert beschäftigten sich bereits theoretisch Clément Ader, Charles Renard, Edson Gallaudet, Alphonse Pénaud und John Joseph Montgomery mit der Möglichkeit und dem Bau eines Querruders.

In der Frühzeit der Fliegerei wurde die Rollsteuerung oft ohne eigentliche Querruder, sondern durch Tragflächenverwindung erzielt. Dabei wurden die gesamten Tragflächen über Seilzüge gegensinnig etwas in sich verdreht, wodurch sich durch die unterschiedlichen Anstellwinkel ebenfalls eine Auftriebsdifferenz zwischen beiden Flügeln und damit ein Rollen des Flugzeuges ergab.

Die Brüder Wright, deren Flugzeuge durch Flügelverwindung gesteuert wurden, waren der Meinung, dass ihr weitgefasstes Patent auch die von Glenn Hammond Curtiss erfundene Methode der Rollsteuerung über Querruder abdeckte und prozessierten deswegen gegen Curtiss.

Erstmals angewendet wurden Querruder durch Robert Esnault-Pelterie an einem antriebslosen Doppeldecker-Gleitflugzeug im Jahre 1904. Die Flügelverwindung verwendeten neben dem Wright Flyer auch weitere frühe Motorflugzeuge, wie die Blériot XI (1908), die Etrich Taube und ihre Nachbauten (1909), die Morane-Saulnier N (1915) und die zahlreichen Varianten des Fokker-Eindeckers (1915), sowie einige Segelflugzeuge wie die Harth-Messerschmitt S7 (1918).

Im Vergleich zur Verwendung von Querrudern hat die Flügelverwindung den Vorteil, dass man keine durch die Ruderklappen bedingten Spalten in der Tragfläche hat und dass die Profiltreue erhalten bleibt. Somit kann der aerodynamische Widerstand leicht reduziert werden. Die Verwindung ist jedoch konstruktiv schwer mit der nötigen Torsionsfestigkeit von moderneren, üblicherweise gepfeilten Tragflächen vereinbar: Beim Flug mit hohen Geschwindigkeiten, d. h. kleinem Anstellwinkel, entstehen starke Torsionskräfte (Auftrieb wird fast nur an der Hinterkante produziert, so wird der Flügel nach vorne tordiert), denen mit der sehr verdrehsteif ausgebildeten "Torsionsnase" („D-Box“) entgegengewirkt wird.

Ein nachteiliger Sekundär-Effekt des Querrudereinsatzes (auch Querruder-Gier-Moment genannt) entsteht dadurch, dass aufgrund der unterschiedlichen Druckverhältnisse auf Tragflächenober- und Unterseite das – kurveninnere – nach oben ausschlagende Querruder den Strömungswiderstand der abzusenkenden Tragfläche vermindert, das nach unten ausschlagende Querruder auf der Gegenseite durch die Erhöhung der Auftriebserzeugung und der damit einher gehenden Erhöhung des Widerstandes jedoch die Tragfläche nicht nur anhebt, sondern auch abbremst: folglich giert das Flugzeug zusätzlich zur beabsichtigten Rollbewegung, jedoch zur "entgegengesetzten" Seite (= negativ). Aus diesem Grund führt die Betätigung des Querruders zu einem negativen Wendemoment (also: ein Bewegen der Flugzeugnase gegen die gewollte Steuerrichtung), weshalb saubere Kurven im Flugzeug koordiniert mit Quer- und Seitenruder (in die Kurvenrichtung zum Ausgleich des negativen Wendemomentes) eingeleitet und durchflogen werden müssen. 

Dies ist aufgrund der Hebelverhältnisse (große Spannweite im Verhältnis zur Gesamtlänge, damit ein großer Hebel für das angreifende Wendemoment sowie ein relativ kurzer Leitwerkshebelarm) besonders in der Segelfliegerei von großer Bedeutung, um widerstandsarme Kurvenflüge ohne Schieben oder Hängen (Schmieren) zu erzielen; als Hilfsmittel für die Koordination der Ruder dient hierzu der an jedem Segelflugzeug vorhandene Faden.

Das negative Wendemoment kann durch konstruktive Maßnahmen abgeschwächt oder weitgehend neutralisiert werden. Eine mechanisch einfach zu realisierende Möglichkeit der Abschwächung ist eine differenzierte Ansteuerung mit unterschiedlichen Ausschlägen, bei der die Querruderklappen nach unten weniger weit ausschlagen als nach oben. So wird der Ausschlag den unterschiedlichen Druckverhältnissen auf Ober- und Unterseite des Flügels angepasst und die Widerstandmomente werden gleichmäßiger. Eine weitere Möglichkeit ist die Gestaltung der Ruder selbst als "Friese-Querruder", bei der der Scharnier-Drehpunkt der Ruderklappe so gelegt wird, dass bei einem Ausschlag nach oben auch der Widerstand an der Unterseite des Klappenspaltes stark erhöht wird, um ein ausgleichendes Widerstandsmoment zu schaffen. Weiterhin hilft bei größeren Flugzeugen der Einsatz von Störklappen als Rollspoiler (siehe oben), das Wendemoment zu neutralisieren; solche Flugzeuge besitzen oft auch einen sog. Gierdämpfer (engl. yaw damper), der automatisch den Seitenruderausschlag für verschiedene Flugzustände anpasst; ein bewusstes Mitsteuern des Seitenruders durch den Piloten erübrigt sich in diesem Fall. Ebenso kann mechanisch oder über den Flugkontrollrechner ein gewisser, sinnrichtiger Seitenruderausschlag automatisch zur Querruder-Steuereingabe hinzu gemischt werden.

Wird das Querruder betätigt, so erhöht sich an jenem Flügel, bei dem das Querruder nach unten ausschlägt, durch die stärkere geometrische Wölbung der Profil-Skelettlinie der Anstellwinkel. Zusätzlich verursacht das nach unten in den Bereich höheren Druckes ausschlagende Querruder auch eine Widerstandserhöhung.

In extremem Langsamflug kann dies zu einseitigem Strömungsabriss an diesem Flügel führen, wodurch er, anstatt sich nach oben zu bewegen, nach unten „fällt“. Da gleichzeitig durch den erhöhten Widerstand auch eine Drehbewegung um die Hochachse verursacht wird, kann dies das Flugzeug ins Trudeln bringen. Dieser Effekt wird, technisch etwas ungenau, „Querruderumkehr“ genannt.

Die meisten modernen Flugzeuge weisen durch geeignete konstruktive Maßnahmen kaum noch eine Querruderumkehr auf.




</doc>
<doc id="4196" url="https://de.wikipedia.org/wiki?curid=4196" title="Quersumme">
Quersumme

Als Quersumme (oder Ziffernsumme) bezeichnet man üblicherweise die Summe der Ziffernwerte einer natürlichen Zahl. So ist für eine Zahl n = 36036 die dezimale Quersumme q(n) = 3 + 6 + 0 + 3 + 6 = 18. Die Quersumme ist (ebenso wie das Querprodukt) abhängig vom verwendeten Zahlensystem.

Neben der Quersumme als Summe der Ziffernwerte gibt es

Der Graph der Quersummenfunktion formula_1 besitzt einen charakteristischen Verlauf. Im Dezimalsystem steigt er für jeweils zehn aufeinanderfolgende formula_2 mit den Endziffern formula_3 bis formula_4 stetig – pro Schritt um formula_5 – an, um danach einen Zahlenschritt lang zu fallen. Niedrigster und höchster Wert der Anstiegsspanne verschieben sich dabei allerdings von Mal zu Mal um formula_5 nach oben.

Dieses Verhalten wiederholt sich in jeder Zehnerpotenz. Bei formula_7 usw. fällt formula_1 stets wieder auf formula_5. Daraus ergibt sich eine Selbstähnlichkeit des Graphen.

Einzig für formula_10 gilt formula_11, für alle größeren Zahlen ist formula_12. Nach oben hin ist formula_1 nicht beschränkt.

Bei jedem Eingeben und Übertragen von Zahlen können technische oder menschliche Fehler auftreten. Deshalb existieren Prüfverfahren, um die Datenintegrität zu gewährleisten. Eine simple Prüfsummen-Maßnahme ist das Bilden der Quersumme.

Die mit den Faktoren formula_14 gewichtete Quersumme einer ISBN10 (veraltete Version) ist modulo formula_15 immer formula_3 (die Ziffer 'X' hat dabei den Zahlenwert von formula_17 und kann in der letzten Ziffer auftreten). Dies wird erreicht, indem die ersten formula_4 Ziffern das Produkt beschreiben und eine zehnte Ziffer (Prüfziffer) so angehängt wird, dass obige Forderung erfüllt ist.

"Beispiel:" Für die ISBN <nowiki></nowiki> formula_19 ist

Also ist dies eine (formal) gültige ISBN.


Beispielsweise ist im Dezimalsystem die Grundzahl 10, also "n"=9. Damit ist "t" ∈ {1,3,9}. Folglich kann man die Quersummenregelung zur Überprüfung der Teilbarkeit durch 3 und durch 9 anwenden.

Im Hexadezimalsystem ist "n"=15. Damit ist "t" ∈ {1,3,5,15}. Somit kann man die Quersummenregelung im Hexadezimalsystem zur Überprüfung der Teilbarkeit durch 3, durch 5 und durch 15 anwenden.

Allgemein gilt, dass die Quersumme formula_24 der Darstellung einer Zahl formula_25 im Stellenwertsystem mit der Basis formula_2 den Rest modulo formula_27 unverändert lässt, also
und die alternierende Quersumme formula_29 der Darstellung im Stellenwertsystem mit der Basis formula_2 den Rest modulo formula_31 unverändert lässt, also

Für die Teilbarkeit einer Zahl durch 3 oder 9 kann stellvertretend ihre Quersumme herangezogen werden: Eine dezimal dargestellte Zahl "n" ist genau dann durch 3 bzw. 9 teilbar, wenn ihre Quersumme "q(n)" ohne Rest durch 3 bzw. 9 teilbar ist. Generell lässt "n" bei der Division durch 3 oder 9 denselben Rest wie die Quersumme "q"("n"):

Von der einfachen Quersumme wird weiter so lange die Quersumme gebildet, bis nur noch eine einstellige Zahl übrig bleibt.

Beispiel:

Ist die Quersumme einer Zahl "k" eine mehrstellige Zahl, lässt sich der Vorgang so oft wiederholen, bis das Ergebnis nur noch eine Stelle im jeweiligen Zahlensystem hat. Für die so erzeugten (stets einstelligen) iterierten Quersummen formula_36 gilt ("t" sei wie oben wieder die Basis des Zahlensystems - 1):

Beispiel im Dezimalsystem:
und es ist

Insbesondere ist also eine positive natürliche Zahl genau dann durch 9 teilbar, wenn ihre iterierte Quersumme im Dezimalsystem 9 ist.

"Siehe auch:" Hash-Funktion und die dort genannten Verfahren.
Die alternierende Quersumme (auch Querdifferenz, Paarquersumme oder Wechselsumme genannt) erhält man, indem man die Ziffern einer Zahl abwechselnd subtrahiert und addiert. Dabei kann links oder rechts begonnen werden. Im Folgenden wird von rechts begonnen. So ist für die Zahl n = 36036 die alternierende Quersumme aqs(n) = 6 - 3 + 0 - 6 + 3 = 0.

Gleichwertig dazu ist das folgende Verfahren (die Zählung der Ziffern soll wieder rechts beginnen):


Für die Teilbarkeit einer Zahl n durch 11 kann stellvertretend ihre alternierende Quersumme aqs(n) herangezogen werden: Eine dezimal dargestellte Zahl "n" ist genau dann durch 11 teilbar, wenn ihre alternierende Quersumme "aqs(n)" ohne Rest durch 11 teilbar ist.

Wiederholte Anwendung der alternierenden Quersumme liefert den Rest der Zahl bei Division durch 11, wobei negative Werte durch Addition von 11 zu normalisieren sind. Eine aqs von 11 zieht eine weitere Bildung einer aqs nach sich, die 0 liefert (also den Rest der Division von 11 durch 11).

Beispiel:

daraus folgt: Die Zahl 2536874 lässt bei Division durch 11 den Rest 10, ist also nicht durch 11 teilbar.

Die nichtalternierende 2er-Quersumme von n = 36036 ist q = 3+60+36 = 99. Für alle Teiler von 99, also für 3, 9, 11, 33 und 99, ist sie ein Teilbarkeitskriterium: Die nichtalternierende 2er-Quersumme "q" einer dezimalen Zahl "n" ist genau dann durch diese Zahlen teilbar, wenn "n" durch diese teilbar ist.

Die nichtalternierende 3er-Quersumme von n = 36036 ist q = 36+036 = 72. Für alle Teiler von 999, also für 3, 9, 27, 37, 111, 333 und 999, ist sie ein Teilbarkeitskriterium: Die nichtalternierende 3er-Quersumme "q" einer dezimalen Zahl "n" ist genau dann durch diese Zahlen teilbar, wenn "n" durch diese teilbar ist.

"Bemerkung": Die nichtalternierende k-Quersumme ist identisch mit der nichtalternierenden Quersumme zur Basis formula_40. Sie liefert ein Teilbarkeitskriterium für alle Teiler von formula_41.

Die alternierende 2er-Quersumme von n = 36036 ist q = 3-60+36 = -21. Für 101 ist sie ein Teilbarkeitskriterium: Die alternierende 2er-Quersumme "q" einer dezimalen Zahl "n" ist genau dann durch 101 teilbar, wenn "n" durch 101 teilbar ist.

Die alternierende 3er-Quersumme von n = 36036 ist q = -36+036 = 0. Für alle Teiler von 1001, also für 7, 11, 13, 77, 91, 143 und 1001, ist sie ein Teilbarkeitskriterium: Die alternierende 3er-Quersumme "q" einer dezimalen Zahl "n" ist genau dann durch diese Zahlen teilbar, wenn "n" durch die Zahlen teilbar ist.

"Bemerkung": Die alternierende k-Quersumme ist identisch mit der alternierenden Quersumme zur Basis formula_40. Sie liefert ein Teilbarkeitskriterium für alle Teiler von formula_43.

Eine Verallgemeinerung sind "gewichtete Quersummen", bei denen die Ziffern erst mit den Werten einer Zahlenfolge multipliziert und diese Ergebnisse dann addiert werden. Es wird dabei mit der niederwertigsten Ziffer begonnen (bei der einfachen Quersumme ist die Reihenfolge egal). Die Wichtungsfolge kann dabei periodisch
oder nichtperiodisch sein.
Ein Beispiel ist die Periodische Folge
1, 3, 2, −1, −3, −2, … Die gewichtete Quersumme der Zahl 422625 ist (bei der niedrigsten Stelle angefangen):

Die so gewichtete Quersumme liefert eine Teilbarkeitsregel für die Zahl 7. Auch für andere natürliche Zahlen kann man solche periodischen Folgen finden, z. B.


Für die meisten Teiler ist es jedoch nicht praktikabel, die Teilbarkeit mittels Quersummenbildung zu überprüfen, weil es nur wenige gut merkbare periodische Wichtungsfolgen gibt.

Möchte man eine entsprechende Teilbarkeitsregel für die natürliche Zahl m finden, so betrachtet man die Reste der 10er-Potenzen bei der Division mit m. Die Reste entsprechen den gesuchten Gewichten.

Beispiel: m = 7

Die Wichtungsfolge lautet also 1, 3, 2, −1, −3, −2, …



</doc>
<doc id="4197" url="https://de.wikipedia.org/wiki?curid=4197" title="Quaternion">
Quaternion

Die Quaternionen ("Singular:" die Quaternion, von lat. "quaternio, -ionis f." „Vierheit“) sind ein Zahlbereich, der den Zahlbereich der reellen Zahlen erweitert – ähnlich den komplexen Zahlen und über diese hinaus. Beschrieben (und systematisch fortentwickelt) wurden sie ab 1843 von Sir William Rowan Hamilton; sie werden deshalb auch "hamiltonsche Quaternionen" oder Hamilton-Zahlen genannt. Olinde Rodrigues entdeckte sie bereits 1840 unabhängig von Hamilton. Trotzdem wird die Menge der Quaternionen meistens mit formula_1 bezeichnet.

Die Quaternionen bilden einen Schiefkörper (oder Divisionsring), bei dem die Multiplikation "nicht" kommutativ ist. Das heißt, es gibt Quaternionen formula_2 und formula_3, bei denen die beiden Produkte formula_4 und formula_5 von der Reihenfolge der Faktoren abhängen, also
ist.
Einige aus dem Reellen bekannte Rechenregeln gelten deshalb für Quaternionen nicht, jedoch gelten Assoziativ- und Distributivgesetz sowie multiplikative Invertierbarkeit, d. h. die Existenz des Inversen formula_7 zu jedem formula_8.

Die Quaternionen waren der erste derartige Gegenstand in der Geschichte der Mathematik.

Quaternionen erlauben in vielen Fällen eine rechnerisch elegante Beschreibung des dreidimensionalen euklidischen Raumes und anderer Räume, insbesondere im Kontext von Drehungen. Daher verwendet man sie unter anderem in Berechnungs- und Darstellungsalgorithmen für Simulationen sowie zur Auswertung kristallographischer Texturen. Sie sind aber auch als eigenständiges mathematisches Objekt von Interesse und dienen so zum Beispiel im Beweis des Vier-Quadrate-Satzes.

Die Quaternionen entstehen aus den reellen Zahlen durch Hinzufügen (Adjunktion) dreier neuer Zahlen, denen in Anlehnung an die komplex-imaginäre Einheit die Namen formula_9, formula_10 und formula_11 gegeben werden. So ergibt sich ein vierdimensionales Zahlensystem (mathematisch: ein Vektorraum) mit einem "Realteil", der aus "einer" reellen Komponente besteht, und einem "Imaginärteil" aus "drei" Komponenten, der auch Vektorteil genannt wird.

Jede Quaternion lässt sich eindeutig in der Form
mit reellen Zahlen formula_13, formula_14, formula_15, formula_16 schreiben. Damit sind die Elemente formula_17 eine Basis, die Standardbasis der Quaternionen über formula_18. Die Addition ist komponentenweise und wird vom Vektorraum geerbt. Multiplikativ werden die neuen Zahlen formula_9, formula_10, formula_11 gemäß den "Hamilton-Regeln"

verknüpft. Die Skalarmultiplikation formula_23, die ebenfalls vom Vektorraum geerbt wird und bei der die Skalare als mit jedem Element vertauschbar angesehen werden, zusammen mit der Addition, dem Rechtsdistributivgesetz und den Hamilton-Regeln erlauben es, die Multiplikation von der Basis auf alle Quaternionen zu erweitern. Da so auch jeder Skalar formula_24 als formula_25 in formula_1 eingebettet wird, kann formula_18 als Unterring von formula_1 aufgefasst werden.

Die so definierte Multiplikation ist assoziativ, erfüllt die beiden Distributivgesetze und macht so die Quaternionen zu einem Ring. Sie ist allerdings "nicht" kommutativ, d. h. für zwei Quaternionen formula_2 und formula_3 sind die beiden Produkte formula_4 und formula_32 im Allgemeinen verschieden (s. u.). Das Zentrum von formula_33, also die Menge derjenigen Elemente der multiplikativen Gruppe von formula_1, die mit allen Elementen kommutieren, ist formula_35.

Die Quaternionen bilden einen Schiefkörper (Divisionsring), da es zu jeder Quaternion formula_36 eine inverse Quaternion formula_7 gibt mit
Wegen der fehlenden Kommutativität werden Notationen mit Bruchstrich, wie z. B. formula_39, vermieden.

Des Weiteren sind die Quaternionen eine vierdimensionale Divisionsalgebra über formula_18 – und bis auf Isomorphie die einzige.

Im weiteren Text werden folgende Schreibweisen benutzt:

Ist formula_2 eine Quaternion, dann werden ihre (reellen) Komponenten mit formula_42 bezeichnet, und diese sind folgendermaßen zugeordnet
Gelegentlich wird eine vektorielle Schreibweise benötigt. Dabei werden bspw. die Komponenten formula_44 zu einem 3-dimensionalen Vektor formula_45 zusammengefasst, so dass man formula_2 mit dem 4-dimensionalen Vektor formula_47 identifizieren kann.

Analoge Abmachungen sollen für andere Buchstaben wie formula_3 etc. gelten.

In mancher älteren Literatur wurden Quaternionen mit großen Frakturbuchstaben und die imaginären Einheiten als "Einheitsvektoren" mit kleinen formula_49 in Fraktur bezeichnet, z. B. so:
mit formula_51 .

Die Konstruktion der Quaternionen ist der der komplexen Zahlen analog, allerdings wird nicht nur eine neue Zahl hinzugefügt, sondern deren drei, die mit formula_9, formula_10 und formula_11 bezeichnet werden.

Die Linearkombinationen
über der Basis formula_56 spannen mit reellen Komponenten formula_57 den 4-dimensionalen Vektorraum der Quaternionen formula_1 auf. (Das Basiselement formula_59, das zugleich das neutrale Element der Multiplikation darstellt und welches die reellen Zahlen injektiv einbettet, wird in der Linearkombination meist weggelassen.) Die Addition und Subtraktion geschieht komponentenweise wie in jedem Vektorraum. Vom Vektorraum wird auch die Skalarmultiplikation übernommen, also die linke und rechte Multiplikation mit einer reellen Zahl, die distributiv zu jeder Komponente multipliziert wird.

Diese Skalarmultiplikation ist eine Einschränkung der Hamilton-Multiplikation, die auf ganz formula_1 definiert ist. Die Hamilton-Multiplikation der Basiselemente untereinander oder etwas umfassender innerhalb der Menge
geschieht nach den Hamilton-Regeln

die zusammen mit der Vertauschbarkeit von formula_62 mit jedem anderen Element eine vollständige Tafel für eine Verknüpfung ausmachen, die sich als assoziativ erweist und formula_63 zu einer Gruppe macht – der Quaternionengruppe.

Unter Voraussetzung der Regel formula_64 (und der Gruppenaxiome) sind die anderen beiden formula_65 und formula_66, in denen sich u. a. das "zyklische" bzw. "anti-zyklische" Verhalten der drei nicht-reellen Quaternionen-Einheiten ausdrückt, äquivalent zu der Kurzform

Mithilfe dieser Ersetzungsregeln, dem Assoziativgesetz und (linkem wie rechtem) Distributivgesetz lässt sich die Multiplikation auf ganz formula_1 fortsetzen. Die formula_68 kann man wie "anti-kommutierende" Variablen behandeln. Treten Produkte von zweien von ihnen auf, so darf man sie nach den Hamilton-Regeln ersetzen.

Die ausgearbeiteten Formeln für die 2 Verknüpfungen von zwei Quaternionen
lauten

Hiermit sind die für einen Ring erforderlichen 2 Verknüpfungen definiert. Es ist leicht nachgerechnet, dass alle Ring-Axiome erfüllt sind.

Das additive Inverse ist (wie in jedem Vektorraum) das Produkt mit dem Skalar –1. Die "Subtraktion" ist die Addition dieses Inversen.

Die für einen Schiefkörper erforderliche "Division" muss wegen der fehlenden Kommutativität durch eine Multiplikation mit dem (multiplikativen) Inversen ersetzt werden (siehe Inverses und Division).

Aufgrund der besonderen Stellung der Komponente formula_13 einer Quaternion
bezeichnet man sie – wie bei den komplexen Zahlen – als "Realteil" oder "Skalarteil"
während die Komponenten formula_74 zusammen den "Imaginärteil" oder "Vektorteil"
bilden. Häufig identifiziert man den Vektorteil auch mit dem Vektor formula_76.

Zu jeder Quaternion
ist die "konjugierte Quaternion" definiert als
Da hier der Imaginärteil mit seinen Einheitsvektoren verknüpft bleibt und der Realteil als reelle Zahl eindeutig in die Quaternionen einzubetten ist, ergeben sich die einfachen Beziehungen
und
aus denen sich unmittelbar
und
ausrechnet.

Ist eine Quaternion gleich ihrer Konjugierten, so ist sie reell, d. h. der Vektorteil ist null. Ist eine Quaternion gleich dem Negativen ihrer Konjugierten, so ist sie eine reine Quaternion, d. h. der Skalarteil ist null.

Weitere wichtige Eigenschaften der Konjugation sind:
Das Skalarprodukt formula_83 zweier Quaternionen, aufgefasst als Vektoren im formula_84, ist definiert durch:
Es ist eine positiv definite symmetrische Bilinearform, über die sich Norm und Betrag definieren lassen und mit der Winkel und Orthogonalität bestimmt werden können.

Ferner kann man damit die einzelnen Komponenten einer Quaternion isolieren:

Im Folgenden sei das Skalarprodukt, und zwar sowohl das 4- wie das 3-dimensionale – wie in der Physik üblich – mit dem Mittepunkt formula_87 notiert.

Das Kreuzprodukt zweier Quaternionen formula_88 ist das Kreuzprodukt (Vektorprodukt) ihrer Vektorteile und bis auf den Faktor 2 ihr Kommutator. Ist formula_89 und formula_90, so ist

Identifiziert man Quaternionen

mit Paaren aus einem Skalar formula_92 und einem Vektor formula_93

so lässt sich die Multiplikation mithilfe des (dreidimensionalen) Skalarprodukts und Kreuzprodukts beschreiben:
Zwei Quaternionen sind demnach genau dann miteinander "vertauschbar", wenn ihr Kreuzprodukt 0 ist, wenn also ihre Vektorteile als reelle Vektoren linear abhängig sind (s. a. Einbettung der komplexen Zahlen).

Das Skalarprodukt einer Quaternion formula_2 mit sich selbst, welches gleich dem Quaternionenprodukt mit der Konjugierten ist, wird "Norm" genannt:
Insbesondere ist dieser Wert reell und nichtnegativ.
Die Quadratwurzel hieraus
Sie ist ein dreidimensionaler reeller Vektorraum mit Basis formula_98.

Für "reine Quaternionen" nimmt die Multiplikation eine besonders einfache Form an:

Eine "Einheitsquaternion" (auch: "normierte Quaternion", Quaternion "der Länge 1") ist eine Quaternion, deren Betrag gleich 1 ist. Für sie gilt (analog zu den komplexen Zahlen)
Für eine beliebige Quaternion formula_36 ist
eine Einheitsquaternion, die man manchmal auch als das Signum oder den Versor von formula_2 bezeichnet.

Das Produkt zweier Einheitsquaternionen und die Inverse einer Einheitsquaternion sind wieder Einheitsquaternionen. Die Einheitsquaternionen bilden also eine Gruppe.

Geometrisch kann man die Menge der Einheitsquaternionen als die Einheits-3-Sphäre formula_104 im vierdimensionalen euklidischen Raum und damit als Lie-Gruppe interpretieren, mit dem Raum der reinen Quaternionen als zugehöriger Lie-Algebra. Die Darstellung als komplexe Matrizen verdeutlicht die umkehrbar eindeutige Entsprechung der Einheitsquaternionen mit der speziellen unitären Gruppe formula_105.

Die einzigen reellen Einheitsquaternionen sind formula_106. Sie machen auch das Zentrum von formula_104 aus.

Einheitsquaternionen, die auch reine Quaternionen sind, lassen sich als diejenigen Quaternionen charakterisieren, deren Quadrate formula_108 ergeben:

Sie liegen in der Äquatorhyperebene der 3-Sphäre formula_104 und machen die Einheits-2-Sphäre formula_111 des dreidimensionalen Raums formula_112 aus.

Jede Quaternion formula_113 mit Quadrat formula_108 definiert einen Einbettungsisomorphismus formula_115 der komplexen Zahlen in die Quaternionen
mit formula_117 und formula_118 als imaginärer Einheit der komplexen Zahlen. Dabei sind die Bildmengen der formula_113 und formula_120 entsprechenden Einbettungen identisch: formula_121.

Eine jede solche Quaternion darf formula_9 genannt werden, eine senkrechte dazu formula_10 und ihr Produkt formula_11.
Jede nicht-reelle Quaternion liegt in genau einer solchen Einbettung von formula_125 . Zwei Quaternionen sind genau dann vertauschbar, wenn es eine gemeinsame Einbettung gibt.

Zwei verschiedene Bilder haben die reelle Achse zum Durchschnitt.

So betrachtet, sind die Quaternionen eine Vereinigung komplexer Ebenen.

Jede Einheitsquaternion formula_126 kann auf eindeutige Weise in der Form
dargestellt werden.

Mit der verallgemeinerten Exponentialfunktion lässt sich dies wegen formula_131 auch schreiben als
mit der reinen Quaternion formula_133. Will man also eine reine Quaternion formula_134 exponentiieren, so ist formula_135 und die reine Einheitsquaternion formula_136 zu bilden, und es ergibt sich die Einheitsquaternion
Der Fall   formula_138   lässt sich stetig ergänzen. Damit ist die Exponentialabbildung formula_139 surjektiv – und bijektiv   formula_140   bei Einschränkung auf formula_141, denn es ist   formula_142   für unendlich viele formula_143 mit formula_144. Sie ist stetig, wegen der Nicht-Kommutativität der Multiplikation aber kein Homomorphismus.
Allgemein lässt sich jede nicht-reelle Quaternion eindeutig in der Form

kann man diese Gleichung nach



</doc>
<doc id="4199" url="https://de.wikipedia.org/wiki?curid=4199" title="QM">
QM

QM steht für:

Qm steht für:

qm steht für:
Siehe auch:


</doc>
<doc id="4200" url="https://de.wikipedia.org/wiki?curid=4200" title="Quadratmeter">
Quadratmeter

Quadratmeter ist eine Maßeinheit für den Flächeninhalt (→ Flächenmaßeinheit) und eine kohärente SI-Einheit der Fläche. 1 Quadratmeter ist der Flächeninhalt eines Quadrats der Seitenlänge 1 Meter.

Das Einheitenzeichen für Quadratmeter ist "m²". Die oft zu lesende Abkürzung "qm" ist ein veraltetes, im derzeit geltenden SI-Einheitensystem nicht mehr zulässiges Einheitenzeichen.

Umgangssprachlich wird der Quadratmeter auch "Meter im Quadrat" oder (veraltet) "Geviertmeter" genannt. Standardsprachlich bezeichnen "6 Meter im Quadrat" jedoch nicht 6 Quadratmeter, sondern 6 × 6 Quadratmeter.


"Barn", "Ar" und "Hektar" sind in Deutschland, Österreich und der Schweiz gesetzliche Einheiten im Messwesen mit "beschränktem Anwendungsbereich". Das Barn darf nur in Kern- und Atomphysik, Ar und Hektar dürfen nur bei der Flächenangabe für Grund- und Flurstücke benutzt werden.

Zur Angabe von Wohnflächen gelten besondere Regeln (prozentuale Abschläge für Nebenräume oder unter Dachschrägen usw.), sodass eine rechtlich korrekte Angabe von der gemessenen Grundfläche abweichen kann.

Indem man zwischen "Quadrat-" und "-meter" ein entsprechendes SI-Präfix setzt, kann man auch andere Maße für Flächen definieren. Der Vorsatz wird dabei gemäß dem internationalen Einheitensystem (SI) direkt vor die Basiseinheit gesetzt.

Der Quadratmillimeter (Einheitenzeichen: mm²) ist eine SI-Einheit der Fläche. Ein Quadratmillimeter oder „Millimeter zum Quadrat“ ist die Fläche eines Quadrates mit 1 Millimeter = 0,1 Zentimeter Seitenlänge. Es gilt: 100 mm² = 1 cm².

Der Quadratzentimeter (Einheitenzeichen: cm²) ist eine SI-Einheit für die Fläche. Ein Quadratzentimeter oder „Zentimeter zum Quadrat“ ist die Fläche eines Quadrates mit 1 Zentimeter = 10 Millimeter Seitenlänge. Mit der Verbreitung der Computer trat das Problem auf, dass die hochgestellte „2“ anfangs nicht dargestellt werden konnte, daher wurden und werden teilweise noch die Behelfsschreibungen „cm2“, „cm^2“ und „cm**2“ verwendet.
Es gilt: 100 cm² = 1 dm².

Der Quadratdezimeter (Einheitenzeichen: dm²) ist ein Flächenmaß. Ein Quadratdezimeter oder „Dezimeter zum Quadrat“ ist die Fläche eines Quadrates mit 1 Dezimeter Seitenlänge. Auch die Umschreibungen „qdm“, „dm^2“ und „dm**2“ werden benutzt, sind aber unzulässig.
Es gilt: 10.000 cm² = 100 dm² = 1 m².

Ein Quadratkilometer ist die Fläche eines Quadrats von 1 Kilometer Seitenlänge. Ein Quadratkilometer umfasst 100 Hektar. Das Einheitenzeichen für Quadratkilometer ist "km²". Es gilt: 1.000.000 m² = 1 km².

Die immer noch oft verwendete Abkürzung "qkm" entspricht nicht dem Internationalen Einheitensystem, den in Normen festgelegten Empfehlungen des Deutschen Instituts für Normung (DIN) und den deutschen Rechtsvorschriften über gesetzliche Einheiten im Messwesen.



</doc>
<doc id="4202" url="https://de.wikipedia.org/wiki?curid=4202" title="Recht">
Recht

Recht bezeichnet die Gesamtheit der generellen Verhaltensregeln, die von der Gemeinschaft gewährleistet sind. Solche Verhaltensnormen entstehen entweder als Gewohnheitsrecht durch fortdauerndes Befolgen von Regeln, die von der Gemeinschaft als verbindlich akzeptiert werden, oder als gesetztes ("positives") Recht, das von staatlichen oder überstaatlichen Gesetzgebungsorganen oder von satzungsgebenden Körperschaften geschaffen wurde.

Diese generellen Regeln werden als "objektives Recht" bezeichnet. Es ist zu unterscheiden von dem konkreten Recht des Einzelnen, etwas zu tun, zu unterlassen oder von einem anderen zu verlangen (subjektives Recht). Das subjektive Recht kann sich entweder unmittelbar aus dem allgemeinen objektiven Recht ergeben oder in ihm seine (Ermächtigungs-)Grundlage haben. Zu solchen subjektiven Rechten gehören insbesondere die individuellen Freiheitsrechte, die sich aus den generellen Grundrechtsgarantien ergeben (z. B. das Recht, seinen Beruf zu wählen), ferner Ermächtigungen zu rechtswirksamen Handlungen (z. B. ein Kündigungsrecht, durch das ein Mietvertrag beendet werden kann) und schließlich Ansprüche, von einem anderen etwas zu verlangen. Die rechtliche Gewährleistung für solches „Verlangenkönnen“ liegt darin, dass der Berechtigte vor Gericht klagen und dieses dadurch verpflichten kann, ihm zur Durchsetzung seines Rechts zu verhelfen "(ubi actio ibi ius)." Eine wesentliche Komponente eines subjektiven Rechts ist also eine rechtlich gewährleistete Durchsetzungsinitiative. Objektives Recht und subjektives Recht gelten als „zwei verschiedene Seiten derselben Medaille“.

Das Wort „Recht“ ist aus der indogermanischen Wurzel *"h₃reĝ"-, „aufrichten, gerade richten“ entstanden und somit aus etymologischer Sicht moralisch konnotiert. Der etymologische Hintergrund des deutschen Wortes ist der gleiche wie in vielen europäischen Sprachen (, , , , ); auch in außereuropäischen Sprachen finden sich Entsprechungen.

Inhaltlich ist der deutsche Begriff stark von der Bedeutung des lateinischen "ius" beeinflusst, das ursprünglich die menschliche Ordnung gegenüber der überirdischen Ordnung ("fas") bezeichnete. Diese mit "ius" bezeichnete Ordnung wurde durch "leges" konkretisiert, die zunächst Riten darstellten, später aber in die Form staatlicher Gesetze überführt wurden. Mit dieser Begriffswende der "lex" vom Ritus zum staatlichen Gesetz veränderte sich der Begriff des lateinischen "ius" und über die spätscholastische Philosophie und die Rezeption des römischen Rechts auch die Bedeutung des deutschen Worts „Recht“.

In diesem etymologischen Dreiklang aus moralischem Anspruch, herkömmlich-ritueller Lebensordnung und staatlicher Gesetzgebung finden sich bereits drei wesentliche Eckpunkte des (modernen) Diskurses über den Rechtsbegriff, die noch um die geschichtliche Bedingtheit des Rechts ergänzt werden können.

Vor dem Hintergrund dieses sprachgeschichtlich vielschichtigen Begriffs wird die Frage, was Recht ist, wie also „Recht“ von der Gesamtheit gesellschaftlicher Normen abzugrenzen ist, unterschiedlich beantwortet. Staatlich institutionalisiertes Recht ist im Gegensatz zu anderen Normenordnungen wie Moral, Sitte oder Brauch kodifiziert und unterliegt eigenen Entscheidungs-, Änderungs- und Anerkennensregeln.
Essentialistische Ansätze versuchen, diese Frage allgemein verbindlich zu beantworten und beanspruchen damit den „wahren“ Begriff des Rechts. Demgegenüber versuchen nominalistische Ansätze lediglich, eine praktikable Definition aufzustellen, die den Rechtsbegriff für den jeweiligen Untersuchungsbereich zweckmäßig erfasst, während er „für andere Zwecke ganz anders abgegrenzt werden mag“. Umstritten ist, ob Recht inhaltlich frei gesetzt werden kann. Bei Vertretern des Rechtspositivismus, die nur gesetztes Recht als Recht anerkennen, ist häufig unklar, inwieweit die Autoren eine essentialistische oder nominalistische Definition beabsichtigen.

Das wohl gängigste Verständnis des Rechtsbegriffs in der Rechtsdogmatik geht von einer engen Verknüpfung des Rechts mit dem Staat aus. Danach gehören zum Recht die staatlich erlassenen Rechtssätze (Gesetze, Verordnungen, völkerrechtliche Verträge, Richterrecht etc.) sowie staatlich anerkannte Rechtssätze (Kirchenrecht, Handelsgewohnheitsrecht, in begrenztem Umfang auch Naturrecht). Eine solche Definition vermeidet insbesondere Abgrenzungsschwierigkeiten gegenüber moralischen und sittlichen Normen; diese können durch staatliche Akte zu Rechtsnormen werden.

In der Rechtsethnologie (oder: Rechtsanthropologie) wurde gerade in den Anfängen ethnologischer Forschung darüber diskutiert, ob Konfliktlösungsmechanismen nichtstaatlicher Gesellschaften als Recht bezeichnet werden können. Die Frage, ob nichtstaatliche Gesellschaften über Recht (und nicht nur über Moral und Sitte) verfügen, wird inzwischen mehrheitlich bejaht.

Die Rechtsphilosophie umfasst unter allen Disziplinen die wohl größte Spannbreite an unterschiedlich verstandenen Rechtsbegriffen. Während der Rechtspositivismus allein auf staatliche Normen abstellt, sehen Naturrechtslehren die staatlichen Gesetze – wenn überhaupt – als Teil des Rechts an. Eine Zwischenstellung nimmt die Historische Schule der Rechtswissenschaft ein, die das positive Recht gerade als geschichtlich gewachsene Ausprägung des Naturrechts ansieht.

Die Rechtssoziologie kennt drei Wege, Recht als gesellschaftlichen Teilbereich zu erfassen: zum Ersten durch die Feststellung von Normen, die im Zusammenleben der Gruppe für verbindlich gehalten werden und an denen sich aus diesem Grunde die Normadressaten bei ihrem Verhalten orientieren; zum Zweiten durch die Feststellung von Verhaltensmustern, nach denen das Gruppenleben tatsächlich abläuft; und zum Dritten durch die Feststellung von Verhaltensmustern, nach denen der Rechtstab in bestimmten sozialen Situationen reagiert.

Das Recht ist definiert als eine im Menschen innerlich wirkende geistige Macht, die ihn antreibt, bestimmte Dinge zu tun oder zu unterlassen, die aber durch eine äußere Macht unterstützt werden muss, um ein gedeihliches Zusammenleben der Menschen zu erzielen. Alle alten Völker schreiben dem Recht einen überirdischen Ursprung zu.

In modernen Gesellschaften bildet das Recht eine Rechtsordnung. Grundbausteine dieser Rechtsordnung sind Rechtsvorschriften, d. h. Verhaltensgebote, die zu einem bestimmten Tun oder Unterlassen verpflichten, und Rechtsentstehungsnormen, d. h. Regeln darüber, wer auf welche Weise generelle Vorschriften und konkrete Pflichten begründen kann. Rechtsentstehungsnormen waren in der Frühzeit des Rechts hauptsächlich Regeln über die Entstehung von Gewohnheitsrecht. Heute sind es vor allem gesetzliche Ermächtigungen, die bestimmen, wer auf welche Weise allgemeine Vorschriften erlassen oder konkrete Rechtspflichten begründen kann (letzteres z. B. durch eine staatliche Anordnung oder „privatautonom“, insbesondere durch Abschluss eines Vertrages).

Je nach Gesellschaftsordnung und politischer Auffassung überschneiden sich Recht, Moral und Sitte unterschiedlich stark. Recht kann moralischen Bewertungen entspringen. So ist beispielsweise Bigamie in Deutschland verboten BGB und strafbar. Es gibt allerdings auch moralisch neutrale Rechtssätze, zum Beispiel das Links- oder Rechtsfahrgebot im Straßenverkehr. Recht und Moral decken sich also häufig, jedoch nicht immer.

Recht bezieht sich vornehmlich auf das äußere Verhalten des Menschen, während sich die Moral an die Gesinnung des Menschen wendet. Das Recht unterscheidet sich von der Moral auch durch die Art, wie es Geltung fordert und in einem normierten Verfahren durch von der Gemeinschaft autorisierte Organe (Justiz, Sicherheitsbehörden) zwangsweise durchgesetzt wird. Moralisches Verhalten kann durch staatliche Organe nur erzwungen werden, soweit es durch das Recht gefordert wird. 

Eine Sitte wie eine Kleiderordnung kann rechtlich verbindlich sein, muss es aber nicht. So sind Richter und Rechtsanwälte häufig gesetzlich verpflichtet, eine Robe zu tragen. Frauen aus Ländern des islamischen Rechtskreises sind in ihrer Heimat gesetzlich verpflichtet, ein Kopftuch zu tragen, müssen aber in Europa bisweilen aus den gleichen Gründen darauf verzichten.

Das Recht erfüllt mehrere Funktionen: Es soll das soziale Miteinander ordnen, Konflikte geordnet und verbindlich lösen und den Einzelnen vor Übergriffen anderer Personen oder des Staates schützen. Durch das Recht gestaltet der Staat die Gesellschaft mit.

Rechtswissenschaftlich sind folgende Funktionen des Rechts anerkannt:









Die funktionelle Analyse wird meist aus rechtssoziologischer Perspektive vorgenommen. Uwe Wesel unterscheidet darüber hinaus aus historischer Perspektive zwischen vorstaatlichem und staatlichem Recht, wobei er dem vorstaatlichen Recht allein eine Ordnungs- und Gerechtigkeitsfunktion zuschreibt, während er das staatliche Recht zusätzlich durch eine Herrschafts- und (historisch später entstandene) Herrschaftskontrollfunktion gekennzeichnet sieht. Ähnlich unterscheidet der Rechtswissenschaftler Bernd Rüthers zwischen politischen, d. h. an Herrschaft gebundenen, und gesellschaftlichen Funktionen des Rechts, die er noch um Funktionen für das Individuum ergänzt.

Die moderne Rechtsordnung (auch „Rechtssystem“ genannt) besteht aus der Gesamtheit der Normen, die nach ihrem nationalen oder internationalen Geltungsbereich in Rechtsordnungen und das global geltende Völkerrecht eingeteilt sind. Die Jurisprudenz, besonders die Rechtstheorie, unterteilt diese Rechtsordnungen des objektiven Rechts wiederum in Rechtsgebiete, die nach methodischen Gesichtspunkten in die drei großen Bereiche des öffentlichen Rechts, Privatrechts und Strafrechts, nach sachlichen oder inhaltlichen Gesichtspunkten in methodenübergreifende Rechtsgebiete wie das Verkehrsrecht, das Wirtschaftsrecht oder das Baurecht gegliedert werden. Aus den genannten Normsystemen ergibt sich für die Normadressaten im Einzelfall eine Berechtigung (subjektives Recht), wie etwa das Recht auf freie Meinungsäußerung (z. B. in Deutschland: Art. 5 Abs. 1 Satz 1 GG), das Eigentums­recht, ein Anspruch (zum Beispiel eines Verkäufers auf den Kaufpreis) oder das Recht, von einem Vertrag zurückzutreten.

Rechtssystematisch wird das ursprüngliche apodiktische Recht etwa der Zehn Gebote (Du sollst/sollst nicht …) vom konditionalen Recht (wenn – dann) unterschieden, das die moderne Gesetzgebung prägt.

Normbefehle (Rechtsnormen) werden im Voraus, vor dem Zeitpunkt ihrer Anwendung formuliert. Es muss daher zugleich geregelt werden, für welchen Fall sie gelten. So entsteht der Aufbau einer modernen Rechtsnorm: „Wenn die Voraussetzungen A, B und C erfüllt sind, dann soll die Rechtsfolge R eintreten.“ Die Gesamtheit der erforderlichen Voraussetzungen nennt man Tatbestand, die einzelne erforderliche Voraussetzung nennt man Tatbestandsmerkmal. Normen bestehen somit aus Tatbestand und Rechtsfolge.

Rechtsfolge ist das Entstehen von Rechten und Pflichten. Es gibt auch Normen, die als negative Rechtsfolge anordnen, dass Rechte und Pflichten gerade nicht entstehen (zum Beispiel: Wegen Verstoßes gegen die guten Sitten ist ein Rechtsgeschäft nichtig).

Für die Durchsetzung des Rechts ist durch Gerichte vorgesorgt. Die Einzelnen müssen in der Regel ihr Recht vor den staatlichen Gerichten und nicht durch Selbsthilfe suchen. Soweit Rechte strafrechtlich bewehrt sind, ist dem verletzten Bürger ebenfalls die Bestrafung des Täters in Selbstjustiz untersagt. Hier besteht zur Verwirklichung ein staatlicher Strafanspruch, für dessen Durchsetzungen die Strafjustiz, nämlich Staatsanwaltschaften und Strafgerichte sorgen sollen. Was Gerichtsentscheidungen anordnen, ist – wiederum mit staatlicher Hilfe – zwangsweise durchsetzbar durch Organe des Justizvollzugs (in der Strafvollstreckung) beziehungsweise der Zwangsvollstreckung (zur Durchsetzung von Urteilen der Zivilgerichte) bzw. der Vollziehung (zur Durchsetzung von Titeln der Steuerbehörden und Finanzgerichte sowie der allgemeinen und besonderen Verwaltungsgerichte).

Wenn sich ein Richter bei der Rechtsgewährung nicht an das Recht hält, sieht das deutsche Recht für den Fall der Rechtsbeugung strafrechtliche (§ 339 StGB), im übrigen dienstrechtliche Sanktionen gegen ihn (etwa seine Entfernung aus dem Dienst) vor (vgl. §§ 62 und 78 des Deutschen Richtergesetzes).

Ein auf die richtige richterliche Entscheidung gerichteter Zwang ist nicht vorgesehen, denn der Richter soll unabhängig von Weisungen (in richterlicher Unabhängigkeit) nach bestem Wissen und Gewissen urteilen.

Das geltende Recht lässt sich nach (ideen-)geschichtlicher Herkunft in verschiedene Rechtskreise einteilen. Die größten Rechtskreise sind der kontinentaleuropäische, der angelsächsische, der chinesische und der islamische Rechtskreis. Rechtskreise unterscheiden sich bei der Setzung der Normen (Gesetzesrecht, Gewohnheits- und Richterrecht, göttliches Recht), aber auch der Rechtsanwendung, z. B. was die Rolle des Richters angeht.

Den Begriff Rechtsquelle kann man in einem weiten und einem engen Sinn verstehen.

In einem weiten Sinn betrifft er alle Faktoren, die das objektive Recht prägen bzw. aus denen Recht ermittelt werden kann. Diesem Begriff nach fallen etwa die rechtswissenschaftliche Lehre, die Praxis der Verwaltung und das Rechtsempfinden der Bürger und Rechtsanwender darunter. Die faktisch wichtigste Quelle des objektiven Rechts ist heute das Gesetz. Selbst das Präjudiz aus dem Case Law (Richterrecht) des anglo-amerikanischen Rechtskreises wird dort immer mehr vom förmlichen Gesetzesrecht ("Statutory Law") abgelöst. Das auch im Völkerrecht geltende Gewohnheitsrecht füllt als ungeschriebene Rechtsquelle Lücken in den gesetzlichen Regelungen. Ob es über dieses positive Recht hinaus weitere Rechtsquellen gibt, ist in der Rechtswissenschaft umstritten. Die Naturrechts­lehre stellt dem positiven Recht ein überpositives Recht gegenüber, ein ewig gültiges, dem menschlichen Einfluss entzogenes Recht, das seine Gültigkeit von der Natur des Menschen oder einer höheren Macht (Vernunft, Natur oder Gott) ableitet und nicht legitim durch staatliche Gesetzgebung geändert werden kann.

Dem engeren Begriff der Rechtsquelle nach ist all das Recht, was für den Rechtsanwender verbindliche Rechtssätze erzeugt. Die Frage nach den Rechtsquellen ist besonders vor dem Hintergrund des Gewaltenteilungs­prinzips relevant, denn danach entscheidet sich, wer verbindliche Rechtssätze schaffen darf. Besonders wichtig ist als Rechtsquelle daher das geschriebene, in einem verfassungskonformen Verfahren geschaffene Recht sowie die Verfassung selbst. Daneben gibt es als Rechtsquelle auch das Gewohnheitsrecht, welches insbesondere im Bereich des Völkerrechts noch eine große Rolle spielt.

Unter die genannten Kategorien fallen im Einzelnen die folgenden Rechtsquellen:

Die Rechtsquellen des Völkerrechts sind in Artikel 38 Abs. 1 des Statuts des Internationalen Gerichtshofs (IGH-Statut) aufgezählt. Diese Vorschrift legt fest, welche Quellen der Internationale Gerichtshof (IGH) seinen Entscheidungen zu Grunde zu legen hat. Dies sind im Einzelnen:


Lediglich Rechtserkenntnisquellen (Hilfsmittel zur Feststellung von Rechtsnormen, Art. 38 Abs. 1 Buchst. d IGH-Statut) sind richterliche Entscheidungen (Richterrecht) und die anerkannten Lehrmeinungen der Wissenschaft.

Die Rechtsquellen des Europarechts lassen sich folgendermaßen unterteilen:



Neben dem Recht, das von öffentlich-rechtlichen Rechtsetzungsorganen gesetzt wird, sind Rechtsquellen für einzelne Rechte und Pflichten auch:

Nach dem Geltungsbereich unterscheidet man nationales (innerstaatliches) Recht, das innerhalb jedes einzelnen Staates gilt, Gemeinschaftsrecht einer Staatengemeinschaft und das Völkerrecht.

Das "nationale Recht" lässt sich nach dem Rechtsetzungsorgan noch weiter untergliedern. In einem Bundesstaat wie Deutschland gibt es Bundesrecht und Landesrecht. Unterhalb der staatlichen Ebene gibt es öffentlichrechtliche Gebietskörperschaften (Gemeinden, Landkreise) und berufsständische Körperschaften des öffentlichen Rechts (Beispiel: Rechtsanwaltskammer), die für ihren Bereich ebenfalls Recht setzen können.

Das "Völkerrecht" wirkt über das Gebiet eines Staates hinaus. Es besteht aus Normen, die Rechte und Pflichten von Völkerrechtssubjekten regeln. Dabei handelt es sich in erster Linie um Staaten, aber auch um internationale Organisationen wie zum Beispiel die Vereinten Nationen. Völkerrecht entsteht durch Staatsverträge zwischen zwei oder mehr Staaten oder durch Gewohnheit. Ferner gibt es allgemeine Rechtsgrundsätze des Völkerrechts.

Beim "Recht der Europäischen Union" ist umstritten, ob es sich um Völkerrecht oder – so die herrschende Meinung in der deutschen Rechtslehre – um ein Recht eigener Art handelt.

Recht und Rechte lassen sich nach verschiedenen Aspekten unterteilen.

Die Rechtsnormen, die Rechte und Pflichten regeln, bezeichnet man als "materielles Recht", beispielsweise die Regelungen des Strafrechts Deutschlands, wann ein Mord vorliegt und wie er zu bestrafen ist, oder dass wegen einer schuldhaften Pflichtverletzung in einem Vertrags­verhältnis der Gläubiger Schadensersatz verlangen kann.

Als "formelles Recht" werden dagegen diejenigen Regelungen bezeichnet, die dem Vorgang der Feststellung und Durchsetzung des materiellen Rechts dienen, also insbesondere die Verfahrens- und Prozessordnungen der einzelnen Gerichtszweige. Sie regeln, meist nach den Rechtsgebieten unterschieden, die Zuständigkeit des Gerichts, das gerichtliche Verfahren und die Form der gerichtlichen Entscheidung. Dabei wird meist unterschieden zwischen einem Verfahren, in dem die grundlegenden Feststellungen getroffen werden (die meist mit einem Urteil enden), und einem Vollstreckungsverfahren, das der Durchsetzung der Gerichtsentscheidung dient.

Die Rechtsordnung unterscheidet zwischen öffentlichem Recht und Privatrecht. Das öffentliche Recht regelt die Angelegenheiten der Allgemeinheit. Das sind zum einen die Rechtsbeziehungen, in denen sich die Hoheitsträger und der Einzelne (Hoheitsbetroffene) in einem Über- und Unterordnungsverhältnis befinden. Zum anderen sind es die Rechtsbeziehungen der Hoheitsträger untereinander. Hoheitsträger sind die juristischen Personen des öffentlichen Rechts (Körperschaften, Anstalten, Stiftungen des öffentlichen Rechts) und die Beliehenen (natürliche und juristische Personen des Privatrechts, denen durch Rechtsvorschrift eng begrenzte öffentliche Aufgaben und hoheitliche Befugnisse übertragen worden sind). Körperschaften des öffentlichen Rechts sind in erster Linie die staatlichen Gebietskörperschaften Bund und Land und die nichtstaatlichen Gebietskörperschaften, vor allem Landkreis und Gemeinde oder Europäische Unionsrecht (EU), aber auch die (Personal)Körperschaften wie etwa die Universitäten oder die berufsständischen Kammern (Ärztekammer, Rechtsanwaltskammer, Handwerkskammer, Handelskammer usw.). Das Privatrecht regelt demgegenüber die Rechtsbeziehungen, in denen sich die Beteiligten auf der Ebene der Gleichordnung gegenüberstehen. Das sind zum einen die Rechtsbeziehungen der natürlichen Personen, der juristischen Personen des Privatrechts (Körperschaften und Stiftungen des Privatrechts) und der teilrechtsfähigen Vereinigungen des Privatrechts, wie etwa die rechtsfähige Personengesellschaft (§ 124 Abs. 1 HGB), die Gesellschaft des bürgerlichen Rechts (§ 705 BGB) oder die Wohnungseigentümergemeinschaft. Das sind zum anderen die Rechtsbeziehungen, in denen juristische Personen des öffentlichen Rechts nicht als Hoheitsträger, sondern als Privatrechtssubjekte verwaltungsprivatrechtlich (Erfüllung von öffentlichen Aufgaben in privatrechtlichen Handlungsformen) oder fiskalisch (etwa als Grundstückseigentümer) beteiligt sind.

Zum öffentlichen Recht gehören das Völkerrecht, das Europarecht (Unionsrecht), das Staatsrecht (Bund, Land), das Verwaltungsrecht, das Strafrecht (Ordnungswidrigkeitenrecht, Kriminalstrafrecht), das Kirchenrecht (Staatskirchenrecht, innerkirchliches Recht der Kirchen mit dem Status einer Körperschaft des öffentlichen Rechts), das öffentliche Organisationsrecht (juristische Personen des öffentlichen Rechts, Beliehene; Behördenorganisation, Gerichtsverfassung) sowie das Verfahrens- und Prozessrecht (auch: Zivilprozessrecht, Recht der freiwilligen Gerichtsbarkeit, arbeitsgerichtliches Prozess- und Verfahrensrecht).

Das "Privatrecht" gliedert sich in das allgemeine Privatrecht (bürgerliches Recht) und in die Sonderprivatrechte. Zu den Sonderprivatrechten gehören vor allem das Handelsrecht, das Gesellschaftsrecht, das Wertpapierrecht, das Wettbewerbsrecht, das Privatversicherungsrecht und – mit einem hohen Anteil an öffentlich-rechtlichen Regelungen – das Arbeitsrecht. Das bürgerliche Recht (Allgemeiner Teil, Schuldrecht, Sachenrecht, Familienrecht und Erbrecht) ist in Deutschland hauptsächlich im Bürgerlichen Gesetzbuch, in Österreich hauptsächlich im Allgemeinen bürgerlichen Gesetzbuch geregelt. Das sind zum anderen die Rechtsbeziehungen der Hoheitsträger untereinander. Hoheitsträger sind die juristischen Personen des öffentlichen Rechts (Körperschaften, Anstalten, Stiftungen des öffentlichen Rechts) und die Beliehenen (natürliche und juristische Personen des Privatrechts, denen durch Rechtsvorschrift eng begrenzte öffentliche Aufgaben und hoheitliche Befugnisse übertragen worden sind). Körperschaften des öffentlichen Rechts sind in erster Linie die staatlichen Gebietskörperschaften "Bund" und "Land" und die nicht staatlichen Gebietskörperschaften, vor allem Landkreis und Gemeinde oder Europäische Union (EU), aber auch die (Personal)Körperschaften wie etwa die Universitäten oder die berufsständischen Kammern (Ärztekammer, Rechtsanwaltskammer, Handwerkskammer, Handelskammer usw.). Das Privatrecht regelt demgegenüber die Rechtsbeziehungen, in denen sich die Beteiligten auf der Ebene der Gleichordnung gegenüberstehen. Das sind zum einen die Rechtsbeziehungen der natürlichen Personen, der juristischen Personen des Privatrechts (Körperschaften und Stiftungen des Privatrechts) und der teilrechtsfähigen Vereinigungen des Privatrechts, wie etwa die "rechtsfähige Personengesellschaft" (§ 124 Abs. 1 HGB), die Gesellschaft des bürgerlichen Rechts (§ 705 BGB) oder die Wohnungseigentümergemeinschaft. Das sind zum anderen die Rechtsbeziehungen, in denen juristische Personen des öffentlichen Rechts nicht als Hoheitsträger, sondern als Privatrechtssubjekte verwaltungsprivatrechtlich (Erfüllung von öffentlichen Aufgaben in privatrechtlichen Handlungsformen) oder fiskalisch (etwa als Grundstückseigentümer) beteiligt sind.

Historisch bedeutsam ist die Gestaltung eines einheitlichen Arbeitsrechts in den sozialistischen Staaten, z. B. das Arbeitsrecht in der DDR.

Ähnlich den Kategorien von privatem Recht unterscheiden sich Subordinations- und Koordinationsrecht dadurch, dass die Rechtssubjekte in einem Subordinationsrechtsverhältnis in einem Überunterordnungsverhältnis zueinanderstehen, während Koordinationsrecht aus einem Rechtsverhältnis resultiert, in dem die Rechtssubjekte rechtlich gleichgestellt sind.

Das Subordinationsrecht deckt sich mit dem Begriff des öffentlichen Rechts; zum Koordinationsrecht zählt neben dem Privatrecht auch das Völkerrecht.

Die Subjektiven Rechte werden ihrerseits unterschieden in absolute Rechte und relative Rechte.

Absolute Rechte bezeichnen Rechte, die absolut gelten, die mithin von jedermann zu beachten sind. Sie lassen sich wiederum unterteilen in Herrschaftsrechte, wie etwa das Eigentum oder das Urheberrecht und Persönlichkeitsrechte, wie etwa das Recht auf körperliche Unversehrtheit oder das allgemeine Persönlichkeitsrecht. Das Eigentum an einer Sache gibt dem Eigentümer die Befugnis, nach Belieben mit der Sache zu verfahren und andere von jeder Einwirkung auszuschließen. Man spricht deshalb auch von einem „Herrschaftsrecht“, einem dinglichen Recht oder einem Recht „an einer Sache“. Neben dem Eigentum als grundsätzlich umfassendem Herrschaftsrecht gibt es beschränkte dingliche Rechte, die den Gebrauch nur in bestimmten Beziehungen gestatten, wie den Nießbrauch. Auch das ist ein absolutes Recht.

Relative Rechte sind Rechte, die sich gegen bestimmte Personen richten. Unter den relativen Rechten ist von zentraler Bedeutung der Anspruch, also das Recht, von einem anderen ein Tun oder Unterlassen verlangen zu können (vgl. Bürgerliches Gesetzbuch). Dazu gehören typischerweise die Rechte aus Verträgen, beispielsweise beim Kaufvertrag der Anspruch des Käufers auf Eigentumsverschaffung und umgekehrt des Verkäufers auf Zahlung des Kaufpreises, aber auch viele andere, beispielsweise der Schadensersatz­anspruch aus Delikt wegen der Verletzung des Körpers oder von Sachen anderer. Eine besondere Art von subjektiven Rechten sind "Gestaltungsrechte", welche die Befugnis geben, subjektive Rechte zu begründen, zu verändern oder aufzuheben – typischerweise etwa Kündigungserklärungen, die Anfechtung von Willenserklärungen oder der Rücktritt vom Vertrag.

Aus den vorstehenden Ausführungen ergibt sich, dass sich die Komplexität des menschlichen Zusammenlebens in der Rechtsordnung widerspiegelt. Die dadurch bedingte Stofffülle führt ihrerseits dazu, dass sich das Recht in etliche Teilgebiete untergliedern lässt, was vor allem im Rahmen der juristischen Ausbildung unverzichtbar ist.

Die traditionelle Aufteilung des Stoffs in der an den Hochschulen gelehrten Rechtswissenschaft nimmt dabei primär auf die bereits geschilderte Aufteilung in das Privatrecht einerseits und das öffentliche Recht andererseits Bezug. Daneben treten das Strafrecht und das Prozessrecht. Beide sind streng genommen Bestandteil des öffentlichen Rechts, da sie ebenfalls das Verhältnis zwischen Staat und Bürger regeln. Die spezifischen Eigenheiten beider Rechtsgebiete lassen jedoch ihre separate Behandlung in der Praxis sachgerecht erscheinen.

Das Privatrecht lässt sich weiter untergliedern in die einzelnen bürgerlichen Rechtsgebiete, also das Schuldrecht, das Sachenrecht, das Familienrecht und das Erbrecht, in das Handelsrecht als Sonderprivatrecht der Kaufleute, das Gesellschaftsrecht u. a.

Das öffentliche Recht unterteilt sich weiter in die großen Bereiche des Verwaltungsrechts, des Verfassungsrechts und des Staatskirchenrechts. Das Steuerrecht, das begrifflich nur ein Teilgebiet des besonderen Verwaltungsrechts ist, wird wegen seiner Bedeutung und seines Umfangs ebenso wie wegen seiner starken Bezüge zum Wirtschaftsrecht heute regelmäßig als eigenständiges Untergebiet des öffentlichen Rechts begriffen.

Eine schematische Übersicht über die Stoffgliederung des deutschen Rechts bietet der Artikel Bundesdeutsches Recht.

„Alles Recht entwickelt sich.“ Diese Wandelbarkeit des positiven Rechts wurde von Montesquieu erstmals artikuliert und ist heute unbestritten. Über lange Zeiten der Geschichte scheint es aber nicht im gesellschaftlichen Bewusstsein verankert gewesen zu sein, dass positives Recht evolutionären Charakter hat und somit geändert werden kann. So erklärt sich beispielsweise, dass große Kodifikationen sich zumeist auf älteres, bestehendes Recht berufen oder dass einige Rechtsänderungen im Mittelalter mittels Urkundenfälschungen, die ein schon bestehendes Recht vortäuschten, vorgenommen wurden.

Der Geschichtlichkeit des Rechts widersprechen auch solche Theorien nicht, die bestimmte Funktionen des Rechts zur Bestimmung von Normgefügen als Recht heranziehen. Denn diese Funktionen sind nicht rechtsimmanent, sondern werden ihm zur besseren Analyse zugeschrieben.

Mit dem geschichtlichen Ursprung des Rechts befasst sich die Rechtsethnologie; er spielt aber auch zu Bekräftigung rechtsphilosophischer und -soziologischer Hypothesen eine Rolle.

Als erste schriftliche Kodifikationen des Rechts gelten der Codex Ur-Nammu und der Codex Hammurapi. Wie bei allen frühen schriftlichen Quellen (z. B. auch beim Zwölftafelgesetz) war der Inhalt dieser Codices jedoch keine genuine Rechtsetzung, sondern – zumindest zum Teil – eine Sammlung und Zusammenfassung bestehender, ungeschriebener Rechtsnormen.

Über die Entstehung dieses frühgeschichtlichen ungeschriebenen Rechts als soziales Teilsystem gibt es keine Gewissheit; nach der ganz überwiegenden Ansicht jedoch waren Recht, Religion und Moral in vorgeschichtlichen Gesellschaften nicht abgrenzbare Teile einer umfassenden Sittlichkeit, die sich erst in einer späteren Phase der gesellschaftlichen Entwicklung als eigenständige Teilsysteme ausdifferenziert haben.

Nach einer anderen Hypothese ist das Recht eine Hervorbringung der Religion. In diesem Sinne sollen Rechtsnormen aus religiösen Normen umgewandelte Handlungsvorschriften sein. In der Tat berufen sich noch heute einige Rechtssysteme auf ihre Entstehung aus göttlicher Offenbarung, so das jüdische Recht, die Scharia und zum Teil das kanonische Recht. Aufgrund mehrerer Argumente wird diese Hypothese heute allerdings nicht mehr ausdrücklich vertreten: Wesel hält ihr entgegen, dass in den Gesellschaften der Jäger und Sammler die Verbote des Ehebruchs, des Totschlags und des Diebstahls niemals religiöse Bedeutung gehabt hätten. Zudem weist Malinowski darauf hin, dass religiöse Gebote archaischer Gesellschaften „absolut festgelegt, strikt zu befolgen und umfassend sind“, während ihre Rechtsregeln „dem Wesen nach elastisch und anpassungsfähig“ sind und es sich – in scheinbarem Widerspruch – gleichwohl „zweifellos um Regeln bindenden Rechts“ handeln kann. Diese Argumente sagen freilich nichts aus über die religiöse Legitimierung des Rechts in späteren Stadien der gesellschaftlichen Entwicklung; nur betrifft dies einen (späteren) Entwicklungsschritt des Rechts, hingegen nicht seinen geschichtlichen Ursprung.





</doc>
<doc id="4203" url="https://de.wikipedia.org/wiki?curid=4203" title="Robert Rodriguez">
Robert Rodriguez

Robert Anthony Rodriguez (* 20. Juni 1968 in San Antonio, Texas) ist ein US-amerikanischer Filmregisseur, Drehbuchautor, Filmkomponist und Filmproduzent mexikanischer Herkunft. Er hat bei einigen seiner Filme zudem als Kameramann und Filmeditor mitgewirkt. Seit 2013 hat er mit El Rey Network seinen eigenen Fernsehsender.

Wegen zu schlechter Noten wurde Rodriguez nach dem Grundstudium an der University of Texas at Austin nicht in die Filmklasse aufgenommen. Erst nachdem er mit einem Videokurzfilm ein lokales Filmfestival gewonnen hatte, wurde er nachträglich für das Filmstudium zugelassen. Nach unzähligen Videofilmen, die er als Jugendlicher gedreht hatte, konnte er zum ersten Mal auf richtigem Film drehen. Sein erstes Projekt war 1991 der 16-mm-Kurzfilm "Bedhead", mit dem er über ein Dutzend Filmpreise gewann. Während der Sommerferien 1991 drehte er, inspiriert durch die Filme von John Woo, den Action-Film "El Mariachi". Die 16-mm-Filmkamera für diesen Dreh lieh er sich und als Schauspieler verpflichtete er Freunde und Bekannte. Da er auf eine Crew verzichtete und alles am Set selbst machte, reichte ihm ein Budget von nur 7000 $. Einen Teil des Geldes, 3000 $, verdiente er durch das Testen von Medikamenten. Während der 30 Tage im medizinischen Institut schrieb er nicht nur das Drehbuch, sondern lernte auch einen seiner Hauptdarsteller kennen. Nachdem er den Film innerhalb von zweieinhalb Wochen in der mexikanischen Stadt Ciudad Acuña gedreht hatte, versuchte Rodriguez ihn anfangs erfolglos in Hollywood an spanischsprachige Videoverleiher zu verkaufen. Er fand jedoch nach einiger Zeit eine große Agentur, die das Potential des Filmes erkannte und ihm einen Vertrag mit Columbia Pictures verschaffte. Das Studio steckte noch einmal etwa 200.000 $ in den Film, um ihn für den Kinostart vorzubereiten, und reichte ihn zu verschiedenen Festivals ein. Im Januar 1993 gewann Rodriguez beim Sundance Film Festival als größten Erfolg den Publikumspreis.

Rodriguez berichtet über seine Erfahrungen in dem Buch "Rebel Without a Crew", das jungen Filmemachern zeigen soll, dass Filme auch ohne großes Budget möglich sind. Rodriguez gründete 1991 zusammen mit der Produzentin Elizabeth Avellan in Texas die Produktionsfirma Los Hooligans Productions, um fortan darüber Filme zu produzieren. Im Jahr 2001 kam die Firma Troublemaker Studios hinzu.

Sein nächster Film war "Desperado", die Fortsetzung des Stoffes von "El Mariachi". In dem Film hatte auch Salma Hayek ihren ersten Auftritt in einem größeren amerikanischen Film. Rodriguez arbeitete zusammen mit Quentin Tarantino, mit dem ihn auch heute noch eine enge Freundschaft verbindet, an dem Vampir-Thriller "From Dusk Till Dawn". 1998 drehte Robert dann zusammen mit Kevin Williamson den Horrorfilm "The Faculty".

Rodriguez hatte 2001 seinen bis zu diesem Zeitpunkt größten Erfolg mit der Komödie "Spy Kids", zu der es später noch drei Fortsetzungen gab. Ende 2003 schloss er die "Mariachi"-Trilogie mit dem Film "Irgendwann in Mexico" ab. Rodriguez drehte "Spy Kids" und "Irgendwann in Mexiko" auf dem HDCAM-System und zählt somit zu den Pionieren des Einsatzes von digitalen Kinokameras. Auch seine weiteren Filme nahm er durchgängig nicht mehr auf Film, sondern digital auf. Auch auf dem Gebiet der „3D-Filme“ ist Rodriguez vorne mit dabei. So wurde bereits der Film "Mission 3D" und später auch der Film "Die Abenteuer von Sharkboy und Lavagirl in 3-D" im 3D-Verfahren verfilmt und veröffentlicht.

Im Jahr 2004, während der Dreharbeiten zu "Sin City", bestand Rodriguez darauf, dass der Autor der Comicvorlage, Frank Miller, ebenfalls als Regisseur aufgeführt wird, da er dessen visuellen Stil für den Film als prägend empfindet. Auf Grund dessen trat er aus der Directors Guild of America aus, da diese solche Nennungen ablehnt. 2007 erschien Rodriguez' Beitrag zum Double-Feature "Grindhouse" mit dem Namen "Planet Terror". Der andere Teil wurde von Quentin Tarantino gedreht und heißt "Death Proof – Todsicher". Im Juli 2010 erschien der von Rodriguez produzierte Science-Fiction-Film "Predators", eine Fortsetzung des Films "Predator" von 1987. Am 4. November 2010 ist "Machete" in die deutschen Kinos gekommen.

Am 13. Dezember 2013 startete sein Fernsehsender El Rey Network in den Vereinigten Staaten, der von 40 Millionen Haushalten per Kabel oder Satellit empfangen werden kann. Dort erschien auch die von ihm entwickelte Fernsehserie "". Derzeit angekündigt sind die Filme "The Jetsons", und "Nervewreckers". Bei der Fortsetzung von "Machete" schrieb er gemeinsam mit seinem Bruder Marcel und Kyle Ward das Drehbuch für "Machete Kills".

Für Rémy Martin und deren Cognac "Louis XIII" drehte er 2015 mit John Malkovich unter dem Titel "100 Years" einen Kurzfilm, der bis zum Jahr 2115 unter Verschluss steht.

Rodriguez ist Gitarrist der Band Chingon, die von ihm ursprünglich nur für den Soundtrack zu "Irgendwann in Mexiko" gegründet wurde. Außerdem stellte er für Quentin Tarantino den Soundtrack für "Kill Bill 2" zusammen – für eine Gage von einem Dollar. Im Gegenzug führte Tarantino Gastregie in Rodriguez "Sin City", ebenfalls für einen Dollar.

Von 1990 bis 2007 war er mit der Filmproduzentin Elizabeth Avellan verheiratet, gemeinsam haben sie vier Söhne, welche die ungewöhnlichen Namen Rocket Valentino, Racer Maximilliano, Rebel Antonio, Rogue Joaquin tragen, sowie eine Tochter mit dem Namen Rhiannon Elizabeth. Avellan war und ist als seine Produzentin tätig. Gemeinsam betreiben sie die Produktionsfirma Troublemaker Studios. Ihre Zusammenarbeit begann bereits 1990 bei der Inszenierung des Kurzfilms "Bedhead", bei der seine Ex-Frau noch als Animator beteiligt war.

Rodriguez war bis 2009 mit der Schauspielerin Rose McGowan verlobt, die ihrer Beziehung mit ihm das Kapitel "Destruction" ihrer Autobiografie "Brave" widmet. Dort beschreibt sie ihn als herrschsüchtigen und krankhaft eifersüchtigen Tyrannen, der ihr unter anderem einen Lügendetektortest abverlangt hätte, um zu klären, ob sie ein Verhältnis mit Quentin Tarantino habe. Rodriguez ist ein Cousin des Schauspielers Danny Trejo. Trejo wird regelmäßig in seinen Filmen als Darsteller eingesetzt, wie auch Rodriguez' Schwestern, die Schauspielerin Angela Lanza und die Musikerin und Schauspielerin Patricia Vonne.

Im Zuge der im Oktober 2017 veröffentlichten zahlreichen Vorwürfe wegen sexueller Belästigung und Vergewaltigung durch den Produzenten Harvey Weinstein, behauptete Rodriguez in einem Statement gegenüber dem Branchenmagazin Variety, er habe Rose McGowan bewusst gegen Weinsteins Willen in seinem Film "Planet Terror" besetzt, aufgrund einer Verschwiegenheitserklärung aber nicht über seine wahren Gründe sprechen können. McGowan habe ihm berichtet, von Weinstein vergewaltigt worden zu sein und auf einer schwarzen Liste zu stehen. Rodriguez' Statement, das auf eine frühere Stellungnahme bei Deadline.com folgte, ist aufgrund einiger widersprüchlicher Aussagen kritisch aufgenommen worden, zumal der Regisseur auch später weiter mit Weinstein zusammenarbeitete. McGowan selber hatte bereits 2016 einen Tweet veröffentlicht, der Rodriguez' spätere Darstellung in einem anderen Licht erscheinen lässt.

In "Scream 4" gibt es eine Anspielung auf Robert Rodriguez: Er wird im Vorspann des fiktiven Films „Stab“ als Regisseur aufgelistet.




</doc>
<doc id="4204" url="https://de.wikipedia.org/wiki?curid=4204" title="Roland Emmerich">
Roland Emmerich

Roland Emmerich (* 10. November 1955 in Stuttgart) ist ein deutscher Filmproduzent, Regisseur und Drehbuchautor. Er wurde bekannt durch Katastrophenfilme wie "Independence Day", "Godzilla", "The Day After Tomorrow" und "2012" sowie mit dem Science-Fiction-Film "Stargate", auf dem auch mehrere Serien und Filme basieren.

Roland Emmerich wurde in Stuttgart-Obertürkheim geboren und wuchs in Maichingen auf. Er besuchte das Gymnasium in den Pfarrwiesen Sindelfingen. Seine Schwester Ute Emmerich wirkt als Chefin der 1985 gegründeten gemeinsamen Produktionsfirma Centropolis Entertainment (früher Centropolis Film) bei den meisten seiner Filme wie er selbst als Executive Producer mit.

Im Juli 2017 hat Emmerich seinen langjährigen Partner geheiratet. Er wohnt in Los Angeles, London und Berlin. Emmerich engagiert sich für Menschenrechte. So saß er in der Jury, die bei der Auswahl eines universellen Logos für Menschenrechte half.

1977 begann er an der Hochschule für Fernsehen und Film München Szenenbild zu studieren. Nachdem er "Star Wars" gesehen hatte, wechselte er ins Regiefach. Sein Abschlussfilm "Das Arche Noah Prinzip" sprengte dabei in jeder Hinsicht den Rahmen. Größtenteils fremdfinanziert, kostete er etwa eine Million DM – das Budget für einen Abschlussfilm lag damals bei 20.000 DM. Auch das Genre irritierte, denn opulent ausgestattete Science-Fiction-Filme aus Deutschland waren eher unüblich. Letztendlich wurde der Film, der 1984 auf den Internationalen Filmfestspielen in Berlin uraufgeführt wurde, ein Erfolg.

Mit den in Deutschland produzierten, aber in englischer Sprache gedrehten Filmen wie "Joey", "Hollywood Monster" und "Moon 44" konnte Emmerich schließlich in Hollywood Aufmerksamkeit erregen. Ein weiteres Resultat des Films "Moon 44" war die langjährige Zusammenarbeit mit Dean Devlin, der fortan als Drehbuchautor und Produzent bei Emmerichs Filmen tätig war, sowie Volker Engel, der als VFX-Supervisor für die Spezial-Effekte mehrerer seiner Filme verantwortlich zeichnete. 1992 drehte er den Science Fiction-Film "Universal Soldier" mit Jean-Claude Van Damme und Dolph Lundgren, mit dem ihm der endgültige Durchbruch in Hollywood gelang. Sein langjähriges Interesse an Prä-Astronautik und den Theorien Erich von Dänikens setzte er 1994 mit dem Film "Stargate" um.

Den vorläufigen Höhepunkt seiner Karriere erreichte er 1996 mit "Independence Day", einem Blockbuster, der auch heute einen der vordersten Plätze in den Ranglisten der erfolgreichsten Filme belegt. 1998 folgte mit "Godzilla" eine Neuinterpretation der japanischen Monsterfilme. 1994 war er bei dem Film "High Crusade – Frikassee im Weltraum" sowie 1999 bei dem Film "The Thirteenth Floor" als Produzent tätig. 2000 begab sich Emmerich dann auf ernsteres Terrain, als er bei "Der Patriot" Regie führte, der 1776 zur Zeit der Amerikanischen Revolution spielt. Die Hauptrollen übernahmen Mel Gibson und Heath Ledger. In der Zeit danach beschränkte sich Emmerich hauptsächlich auf das Produzieren, bevor er im Mai 2004 mit "The Day After Tomorrow" als Regisseur auf die Leinwand zurückkehrte. In einem Interview mit der Zeitschrift "Merian" sagte Emmerich im November 2009, dass "2012" sein letztes Desaster-Movie sei. „Es ist die Mutter aller Zerstörungsfilme, mit Effekten, wie man sie noch nie gesehen hat. Ich wüsste wirklich nicht, was ich danach noch zerstören sollte.“

Am 26. November 2009 erhielt er zusammen mit Florian Henckel von Donnersmarck, Michael Ballhaus, Oliver Hirschbiegel und Caroline Link einen Ehren-Bambi "Deutsche in Hollywood". Die Trophäen überreichte Jürgen Prochnow. Am 17. März 2017 erhielt Emmerich den Carl-Laemmle-Produzentenpreis.

Bei der Berlinale 2005 war Emmerich Präsident der Internationalen Jury. 2006 spendete er 150.000 US-Dollar für das "Legacy Project", ein gemeinsames Projekt des Film- und Fernseharchivs der UCLA und des größten Filmfestivals von Los Angeles, "Outfest", zur Bewahrung und Restaurierung von Filmen lesbischer und schwuler Künstler. 2007 war Emmerich Gastredner bei der fmx/07 in Stuttgart, einer der wichtigsten internationalen Konferenzen für Animation, Effekte, Spiele und digitale Medien. Im März 2008 kam sein Projekt "10.000 B.C." (Drehbuch: Roland Emmerich und Harald Kloser) weltweit in die Kinos; ein Film der, nachdem das Sony-Studio Columbia Pictures zwischenzeitlich sein Interesse daran verloren hatte, von Warner Bros. produziert wurde. Der ca. 110 Millionen US-Dollar teure Streifen hatte schon nach vier Wochen mehr als 200 Mio. US-Dollar eingespielt.

Emmerichs Film "2012" mit John Cusack, Woody Harrelson und Danny Glover in den Hauptrollen startete am 12. November 2009 weltweit in den Kinos und erreichte bereits nach acht Wochen ein weltweites Einspielergebnis von 730 Mio. US-Dollar. "2012" ist der zweiterfolgreichste Film seiner Karriere. Ab März 2010 fanden in den Filmstudios Babelsberg in Potsdam die Dreharbeiten für seinen Film "Anonymus" (Arbeitstitel: „Soul of the Age“) statt. Das Drama um die Urheberschaft der Werke von William Shakespeare nach einem Script von John Orloff (unter anderem Drehbuchautor von "Band of Brothers") startete in Deutschland im November 2011. Auf der Frankfurter Buchmesse wurde Anonymus den Presse- und Medienvertretern vorgestellt. Bei einer anschließenden Podiumsdiskussion diskutierte Emmerich mit dem Publizisten Hellmuth Karasek, dem Präsidenten der Deutschen Shakespeare-Gesellschaft und Uni-Dozent für Anglistik Tobias Döring, dem Shakespeare-Übersetzer Frank Günther und dem Autor des Buches "Der Mann, der Shakespeare erfand", Kurt Kreiler. Das Thema der Diskussion war die langjährige Debatte um die William-Shakespeare-Urheberschaft.

Ende 2012 drehte Emmerich den Film "White House Down", der im Juni 2013 Premiere feierte. Darin geht es um einen Secret-Service-Mitarbeiter (Channing Tatum), der das Leben des amerikanischen Präsidenten (Jamie Foxx) retten soll, der im Weißen Haus von einer paramilitärischen Gruppe gefangen genommen worden ist. Bis auf Weiteres verschoben hat Roland Emmerich ein Remake des Kinofilms "Die fantastische Reise", "One Nation" (einen politischen Thriller, geschrieben von "The-Day-After-Tomorrow"-Drehbuchautor Jeffrey Nachmanoff) und "King Tut", einen Film über den ägyptischen Pharao Tutanchamun.

2014 bereitete Emmerich die Verfilmung des Historienepos "Maya Lord" vor, das auf der Romanvorlage des Schriftstellers John Cue Robbins beruht. Es handelt sich um die Verfilmung der wahren Geschichte des Gonzalo Guerrero, der 1511 nach einem Schiffbruch von Maya gefangen genommen wurde und schließlich zu ihrem Verbündeten wurde. Im Sommer 2014 drehte er das Drama "Stonewall" nach einem Drehbuch von Jon Robin Baitz, der Film kam in den USA im September 2015 in die Kinos.

Nachdem Roland Emmerich bereits 1999 für seinen Film "Godzilla" mit vier „Goldenen Himbeeren“ ausgezeichnet worden war, wurde 2017 sein Sequel-Film "Independence Day 2: Wiederkehr" für fünf weitere „Goldene Himbeeren“ nominiert, darunter für die schlechteste Regie, das schlechteste Drehbuch sowie den schlechtesten Film.




</doc>
<doc id="4205" url="https://de.wikipedia.org/wiki?curid=4205" title="Rainer Werner Fassbinder">
Rainer Werner Fassbinder

Rainer Werner Fassbinder (* 31. Mai 1945 in Bad Wörishofen, Bayern; † 10. Juni 1982 in München, Pseudonym: Franz Walsch) war ein deutscher Regisseur, Filmproduzent, Schauspieler und Autor. Er gilt als einer der wichtigsten Vertreter des Neuen Deutschen Films der 1970er und 1980er Jahre.

Das einzige Kind des Arztes Helmuth Fassbinder (1918–2010) und der Übersetzerin Liselotte Eder (1922–1993) wuchs nach der Scheidung seiner Eltern 1951 bei seiner Mutter auf. Sein Geburtshaus, das denkmalgeschützte „Haus Hubertus“, steht im Garten des Hotels Sonnengarten Adolf-Scholz-Allee 5. Im Alter von 16 Jahren brach Fassbinder die Schule (das Gymnasium bei St. Anna in Augsburg) ab und zog nach Köln zu seinem Vater. In dieser Zeit verfasste Fassbinder erste Theaterstücke, Gedichte, Kurzgeschichten und Filmtreatments. Er galt als belesen und eignete sich durch das Studium philosophischer, gesellschaftskritischer und psychoanalytischer Schriften eine Bildung auf hohem Niveau an. Bereits im Jugendalter interessierte sich Fassbinder für das Filmemachen. Doch sollte sein Wunsch, an einer Filmhochschule zu studieren, nicht in Erfüllung gehen. Nach einer zweijährigen privaten Schauspielausbildung versuchte er vergeblich, in München die staatliche Schauspielprüfung abzulegen. Auch bei der Aufnahmeprüfung an der damals neu gegründeten Deutschen Film- und Fernsehakademie Berlin scheiterte er.

Dem Film näherte er sich als Autodidakt und scherte sich zunächst wenig um Konventionen. 1966 und Anfang 1967 produzierte Fassbinders Partner Christoph Roser dessen erste Kurzfilme "Der Stadtstreicher" und "Das kleine Chaos". 1967 stieß Fassbinder auf das Action-Theater und wurde von der jungen Gruppe um Ursula Strätz, Peer Raben, Kurt Raab, u. a. als Ensemblemitglied aufgenommen, woraus dann kurz danach das antiteater hervorging. Fassbinder, der bald auch die Regie übernahm, brachte Hanna Schygulla mit, die er 1963 auf der Schauspielschule kennengelernt hatte, sowie Irm Hermann, mit der er und Roser zusammenlebten. Etwas später stießen Harry Baer, Ingrid Caven und Günther Kaufmann sowie 1970 Margit Carstensen hinzu, die er als Star neben Hanna Schygulla in die Gruppe integrierte. Er schrieb für sie unter anderem die Theaterstücke "Bremer Freiheit" und "Die bitteren Tränen der Petra von Kant".

1968 löste sich das Action-Theater auf, und Fassbinder gründete mit Peer Raben, Hanna Schygulla und Kurt Raab das antiteater. Das Ensemble hatte Aufführungen im Büchner-Theater in der Kunstakademie und schließlich im Hinterraum der Schwabinger Kneipe Witwe Bolte.
Beeinflusst von Jean-Luc Godard und der Nouvelle Vague sowie den US-amerikanischen Kriminalfilmen von John Huston, Raoul Walsh und Howard Hawks, vor allem aber von den Melodramen von Douglas Sirk, begann Fassbinder mit den Schauspielern des antiteaters die ersten Spielfilmprojekte zu realisieren. 1969 entstanden der Krimi "Liebe ist kälter als der Tod" und "Katzelmacher". In beiden Filmen verband Fassbinder die Theaterarbeit mit der des Filmemachens. 1969 spielte er fürs Fernsehen die Hauptrolle in Volker Schlöndorffs Theaterverfilmung "Baal". Zwischen 1969 und 1971 entstanden nicht nur zahlreiche Theaterstücke, sondern auch in sehr kurzer Zeit von Fassbinder produzierte „alternative“ Filme unter dem Unternehmensnamen (Firma) "antiteater-X-Film" (z. B. "Götter der Pest", "Rio das Mortes", "Whity", "Der amerikanische Soldat", "Warnung vor einer heiligen Nutte"). Als das "antiteater" Mitte 1971 aufgrund einer desaströsen Finanzsituation auseinanderbrach (es war keine im Handelsregister eingetragene Firma), übernahm Fassbinder die alleinige Verantwortung und in den kommenden Jahren die Abzahlung der Schulden, die sich auf ca. 200.000 DM beliefen. Seine Mutter Liselotte Eder übernahm die Aufarbeitung der Steuer- und Sachleistungsschulden und die Filmgeschäftsführung der von Fassbinder neu gegründeten Produktionsfirma Tango-Film, mit der er im August 1971 als erstes Werk den Film "Händler der vier Jahreszeiten" produzierte.

Fassbinders wachsender künstlerischer Erfolg ließ auch die Verantwortlichen des Fernsehens auf ihn aufmerksam werden, und so begann ab 1971 eine sehr produktive Zusammenarbeit mit dem Westdeutschen Rundfunk (WDR). Peter Märthesheimer, der für ihn später, gemeinsam mit Pea Fröhlich, die Drehbücher seiner BRD-Trilogie schrieb, war sein wichtigster Unterstützer und zeichnete als verantwortlicher Redakteur des WDR bei den Fernsehfilmen "Niklashauser Fart" (1971), der Arbeiterserie "Acht Stunden sind kein Tag" (1972) sowie bei "Martha" (1974), "Welt am Draht" (1973), "Angst vor der Angst", "Ich will doch nur, daß ihr mich liebt" (1975) und "Wie ein Vogel auf dem Draht" (1976).

Das ZDF erteilte 1970 den Auftrag für "Pioniere in Ingolstadt" (ausgestrahlt im Mai 1971) und sendete im März 1972 den "Händler der vier Jahreszeiten". " Wildwechsel" entstand 1972 im Auftrag des SFB, "Bremer Freiheit" 1972 und "Nora Helmer" 1973 im Auftrag des Saarländischen Rundfunks.

In den Jahren 1971 bis 1974 erreichte Fassbinder mit den von Tango-Film produzierten Spielfilmen "Die bitteren Tränen der Petra von Kant" (1972), "Angst essen Seele auf" (1973), "Faustrecht der Freiheit" (1974) und zusätzlich mit Theaterregie-Einsätzen in Bremen, Bochum und Frankfurt am Main ein Optimum an öffentlicher Aufmerksamkeit. Fassbinder war in der Spielzeit 1974/75 Mit-Intendant am Theater am Turm in Frankfurt und schrieb für das Ensemble das Stück "Der Müll, die Stadt und der Tod" (1974), das aufgrund der Verwendung antisemitischer Klischees eine heftige Kontroverse auslöste. Kritik kam z. B. von Joachim Fest, Ignatz Bubis, Salomon Korn und Friedrich Uttitz. Eine der Figuren ist ein jüdischer Immobilienspekulant, dem Ähnlichkeiten mit Ignatz Bubis nachgesagt wurden. Im Jahr 1975 verfilmte der Schweizer Regisseur Daniel Schmid das noch nicht aufgeführte Stück unter dem Titel "Schatten der Engel" (mit Fassbinder selbst in einer Hauptrolle und als Co-Drehbuchautor sowie Ingrid Caven, Klaus Löwitsch, Annemarie Düringer, Boy Gobert und Irm Hermann). Eine erste Inszenierung am Schauspiel Frankfurt wurde in den 1980er Jahren von Demonstranten verhindert, die in dem Stück das antisemitische Klischee vom „reichen Juden“ propagiert sahen und ihren Protest zum Ausdruck brachten, indem sie die Bühne besetzten. Weitere Pläne, das Stück in Deutschland zu zeigen, wurden nach Protesten zurückgezogen. Dagegen wurde das Stück 1999 in Israel inszeniert und ohne Proteste aufgeführt.

Fassbinder entwickelte seine Filmsprache konsequent weiter und die Filme wurden größer und professioneller. 1977 realisierte er für sechs Millionen DM den Film "Despair – Eine Reise ins Licht" mit der Bavaria Film in München, seine bis dahin teuerste Produktion, gedreht in englischer Sprache nach einem Drehbuch des britischen Dramatikers Tom Stoppard, basierend auf einer Novelle von Vladimir Nabokov. Obwohl hochkarätig besetzt (der englische Weltstar Dirk Bogarde spielte die Hauptrolle) und als Wettbewerbsfilm 1978 in Cannes eingeladen, hatte der Film an den Kinokassen keinen Erfolg. Als Teilnehmer der Berlinale und vielen Festivals im Ausland (1974 widmete ihm die "Cinémathèque française" eine Gesamtretrospektive und das "New York Film Festival" zeigte seit 1971 jährlich seine aktuellen Filme) wurde er zwar von der internationalen Kritik in höchsten Tönen gelobt, in Deutschland aber wegen seiner direkten Themen häufig angegriffen. Erst mit seinem vorletzten Film, "Die Sehnsucht der Veronika Voss" (in der Hauptrolle Rosel Zech, die seit den frühen 1970er Jahren ein großer Theaterstar war), gewann er den Goldenen Bären der Berlinale.

Fassbinder schuf weitere wichtige Frauenfiguren der Nachkriegsfilmgeschichte, so mit "Fontane Effi Briest" (1974), "Die Ehe der Maria Braun" (1979) und "Lili Marleen" (1981), dargestellt von Hanna Schygulla, oder mit dem Film "Lola" (1981), in dem Barbara Sukowa die Hauptrolle spielte. Diese Frauenfiguren gingen in die Filmgeschichte ein und ihre Darstellerinnen erlangten eine Bekanntheit, die ihnen eine von Fassbinder unabhängige Karriere ermöglichte.

Fernsehgeschichte schrieb er 1980 mit dem Mehrteiler "Berlin Alexanderplatz" nach dem Roman von Alfred Döblin (mit Günter Lamprecht, Gottfried John, Hanna Schygulla und Barbara Sukowa), der später im Kino, meist bei Festivals und Retrospektiven, als 15½-Stunden-Marathon gezeigt wurde. Fassbinder war auch berühmt wegen seines atemberaubenden Arbeitstempos (im Jahr 1970 entstanden sieben Filme). So habe er sein Lebensziel darauf eingerichtet, am Ende mit der Zahl seiner Filme die Zahl seiner Lebensjahre zu erreichen.

1982 war Fassbinder Hauptdarsteller in dem Film "Kamikaze 1989" des Regisseurs Wolf Gremm. Er starb am 10. Juni 1982 in München, wo er lebte, während der Arbeit an der Endfertigung seines letzten Projekts "Querelle" (nach einem Roman von Jean Genet) im Alter von 37 Jahren. Als Todesursache wurde Herzstillstand diagnostiziert, vermutlich ausgelöst durch Überarbeitung und eine Vergiftung mit einer Mischung aus Kokain, Schlaftabletten und Alkohol.

Nach Fassbinders Tod begann seine Mutter Liselotte Eder, die ihn mit seinem Vater Helmuth Fassbinder beerbte, zusammen mit seiner letzten Partnerin bzw. Mitbewohnerin Juliane Lorenz sein Werk zu ordnen und zu erschließen. 1986 gründete sie die "Rainer Werner Fassbinder Foundation" (RWFF), in die sie ihren Erbteil einbrachte. 1988 wurde der Vater ausbezahlt und sein Erbteil ebenfalls in die RWFF eingebracht. 1991 übertrug Eder die gesamten Anteile an der RWFF an Juliane Lorenz, die diese seit 1992 leitet. Die Stiftung besitzt heute alle Rechte an Fassbinders Nachlass, einschließlich aller nachträglich erworbenen Rechte.
Rainer Werner Fassbinder wurde in einem Urnengrab auf dem Bogenhausener Friedhof (Grab Nr. 1-4-2) in München beigesetzt.

Fassbinder lebte in einer häufig als „Clan“ bezeichneten Gruppe, die ihm als Familienersatz diente. Als „Eckpfeiler und in gewisser Weise auch als Motor vielleicht“ war (nach eigenem Bekunden) Hanna Schygulla von Anfang an Antrieb und Inspiration seines filmischen Schaffens. Er hatte sie vom Zeitpunkt ihrer ersten Begegnung (1963) an als Star seiner künftigen Filme gesehen. Ihre absolute Leinwandpräsenz und ihr ursprünglich völliges „Anti-Star“-Bewusstsein bescherten beiden erste gemeinsame filmische Erfolge. Fassbinder löste sich auch zunehmend von den Abhängigkeiten innerhalb seines „Clans“ und integrierte auch sogenannte „Altstars“ in seine Bühnen- und Filmarbeiten, allen voran Karlheinz Böhm, mit dem er unter anderem "Martha" (1974) drehte, sowie Brigitte Mira und Barbara Valentin, mit denen er "Angst essen Seele auf" (1974) produzierte. Fassbinder traf bei seinem siebten Spielfilm "Whity" 1970 auf den Kameramann Michael Ballhaus, mit dem er in neun gemeinsamen Jahren 15 Filme machte. Von den ehemaligen Gruppenmitgliedern begleiteten ihn noch einige bis Mitte der 1970er Jahre, doch es kam auch zu Trennungen; zum Beispiel 1976 von Kurt Raab, von Hanna Schygulla (für drei Jahre), Irm Hermann und Ingrid Caven. Mit Peer Raben verband ihn jedoch seit seiner frühen Theaterarbeit eine fruchtbare Zusammenarbeit; ebenso mit Harry Baer, der später weitere Aufgaben als Produktionsleiter, Regieassistent und künstlerischer Mitarbeiter in der Filmproduktion übernahm. 1974 trat Fassbinder dem 1971 gegründeten Filmverlag der Autoren bei und wurde eines seiner tragenden Mitglieder.

Ebenfalls 1974 lernte Fassbinder im Hotel Deutsche Eiche seinen Gefährten Armin Meier kennen, der in seiner Episode "Deutschland im Herbst" zu sehen ist. (Er spielt sich selbst.) Anfang 1978 trennte sich Fassbinder von ihm, und kurz darauf wurde Meier tot in der gemeinsamen Wohnung aufgefunden. Vermutlich starb er an einer Überdosis Schlaftabletten. Um den Tod seines Freundes zu verkraften, drehte er den sehr persönlichen Film "In einem Jahr mit 13 Monden", in dem Volker Spengler die Hauptrolle übernahm.

Der bisexuelle Fassbinder integrierte in seinen Clan auch seine Partner. Von 1970 bis 1972 war er mit der Schauspielerin Ingrid Caven verheiratet, für die er auch einige Chanson-Texte schrieb (z. B. "Alles aus Leder", "Freitag im Hotel", "Nietzsche", "Die Straßen stinken)" und der er eine spätere Karriere als Sängerin ermöglichte (erstes öffentliches Konzert 1976 im Münchner Rationaltheater).

Zwischen 1971 und 1974 lebte er mit dem aus Algerien stammenden El Hedi ben Salem zusammen, der Fassbinder von Paris nach Westdeutschland gefolgt war und durch ihn zum bekannten Schauspieler seiner Filme wurde. Es folgte 1974 bis 1978 eine Beziehung mit Armin Meier, der auch in einigen Filmen auftrat.

Bis zu seinem Tod im Juni 1982 lebte Fassbinder dann mit seiner Schnittmeisterin Juliane Lorenz, mit der er seit 1976 zusammenarbeitete, in einer gemeinsamen Wohnung in der Clemensstraße 76 in München. Juliane Lorenz behauptet, dass sie und Fassbinder in Fort Lauderdale (Florida, USA) geheiratet hätten; dies ist sehr umstritten und es gibt dafür keine Beweise. Lorenz gibt an, den Trauschein vor lauter Freude aus dem fahrenden Auto geworfen zu haben.

2007 stellte eine Gruppe ehemaliger Fassbinder-Mitarbeiter seiner frühen „Clan“-Periode drei Monate nach der deutschen Uraufführung des restaurierten Films "Berlin Alexanderplatz" die Arbeit von Juliane Lorenz und der RWFF in Frage. Der Hauptvorwurf war, Lorenz habe den Film bei der digitalen Abtastung aufgehellt. Nach anhaltender, intensiver Diskussion in der deutschen Presse und einer Stellungnahme des künstlerischen Leiters der Restaurierung, Xaver Schwarzenberger (ursprünglicher Kameramann), sowie des Kameramanns Michael Ballhaus konnten die Vorwürfe entkräftet werden. Hinzu kamen sorgfältige Recherchen des Autors Tilman Jens für einen Filmbeitrag in der Fernsehsendung "Kulturzeit", die nahelegten, dass die Behauptungen gegen Lorenz und die RWFF nicht zutreffend waren.

Der US-amerikanische Musikproduzent und Gitarrist Omar Rodriguez Lopez widmete Fassbinder 2009 ein ganzes Album. Nicht nur der Titel des Albums "Despair", sondern auch sämtliche Lieder sind nach Filmen von Fassbinder benannt: „Liebe ist kälter als der Tod“, „Angst essen Seele auf“ oder „Warnung vor einer heiligen Nutte“. Es handelt sich durchweg um reine Instrumentalstücke ohne explizite Bezugnahme.

2015 widmet das Berliner Theatertreffen unter dem Titel "Focus Fassbinder" mehrere Inszenierungen der Erinnerung an den Filmemacher. Auf dem Programm steht auch ein Symposium "Das Private ist politisch! – Rainer Werner Fassbinder im Theater heute". Auch ein Lieder- und Erinnerungsabend mit Hanna Schygulla ist geplant.

2009 und erneut 2015 fanden in München "Fassbindertage" mit Veranstaltungen unter anderem im Filmmuseum, im Residenztheater, den Kammerspielen und dem Teamtheater statt.












</doc>
<doc id="4206" url="https://de.wikipedia.org/wiki?curid=4206" title="Resilienz">
Resilienz

Resilienz (lat. "resilire" ‚zurückspringen‘, ‚abprallen‘) steht für:

Siehe auch:


</doc>
<doc id="4208" url="https://de.wikipedia.org/wiki?curid=4208" title="Resonanz (Begriffsklärung)">
Resonanz (Begriffsklärung)

Resonanz (von lateinisch "resonare" „widerhallen, mitschwingen“) steht für:
Siehe auch:


</doc>
<doc id="4209" url="https://de.wikipedia.org/wiki?curid=4209" title="Robustheit">
Robustheit

Der Begriff Robustheit (lat. "robustus", von "robur" Hart-, Eichenholz) bezeichnet die Fähigkeit eines Systems, Veränderungen ohne Anpassung seiner anfänglich stabilen Struktur standzuhalten. Meist ist es sinnvoll anzugeben, wogegen das System robust ist (z. B. gegen Änderung der Umgebungstemperatur oder gegen Fehlbedienung).

In der Informatik und Softwareentwicklung bedeutet der Begriff „Robustheit“ die Eigenschaft eines Verfahrens, auch unter ungünstigen Bedingungen noch zuverlässig zu funktionieren. Robustheit, auch als „Fehlertoleranz“ bezeichnet, zählt zu den Qualitätskriterien für Software. 

Beispiele für entsprechende Vorkehrungen sind das Verhindern undefinierter Zustände und „Systemabstürze“ (z. B. durch vollständiges und detailliertes Auswerten von Antwortcodes nach der Ausführung von Unterprogrammen oder Systemaufrufen) und insbesondere das Abfangen fehlerhafter Benutzer- oder Dateneingaben (wie ungültiger Kommandos/Funktionscodes, falscher Formate in Datenfeldern usw.). 

So weit wie möglich sollte ein „Spektrum sinnvoller Reaktionsmöglichkeiten, abhängig von der Situation“ definiert und angewendet werden, was je nach Erwartungshaltung hohen Implementierungsaufwand bedeuten kann. 

Trotzdem wird eine 100-prozentige Robustheit nicht erreichbar sein, etwa, wenn erforderliche Komponenten der Systemsoftware fehlen oder nicht korrekt arbeiten. Doch selbst in solchen Fällen kann ein Computerprogramm u. U. noch eine möglichst aussagefähige Fehlermeldung erzeugen und sich kontrolliert selbst beenden.

In der Industrie wird der Begriff „Robuster Produktionsprozess“ verwendet. Für die Automobilindustrie gibt es hierzu einen VDA-Band in der Reihe „Das gemeinsame Qualitätsmanagement in der Lieferkette“ mit dem Titel „Produktherstellung und -lieferung, Robuster Produktionsprozess“. Danach zeichnet sich ein robuster Produktionsprozess dadurch aus, dass er gegen unerwünschte Einflussgrößen unempfindlich ist und eine termin- und abrufgerechte Produktion mit ausgezeichneter Qualität unter Einhaltung des geplanten wirtschaftlichen Aufwandes sicherstellt. Die Definition für den „Robusten Produktionsprozess“ ist gleichzeitig die Festlegung der „Leitplanken“, die den Weg der Realisierung beschreiben.

In der Analytik oder Diagnostik erlaubt die Robustheit eines analytischen Systems eine bestimmte Variabilität der zu analysierenden Probe (z.B. Probenvorbehandlung nicht erforderlich) und/oder anderer definierter physikalischer Parameter während des Messvorganges und liefert trotzdem reproduzierbare und standardisierbare Ergebnisse.

In der Inferenzstatistik bedeutet Robustheit, dass z.B. ein Test selbst bei verletzten Voraussetzungen (z.B. keine Normalverteilung, zu kleine Stichprobe) verlässlich arbeitet und sich der Fehler 1. und 2. Art nur geringfügig ändert. Bei mangelnder Robustheit entstehen durch Voraussetzungsverletzungen vermehrt Fehler 1. Art oder 2. Art und führen entweder zu progressiven (fälschliche Verwerfung der Nullhypothese) oder konservativen Entscheidungen (fälschliches Beibehalten der Nullhypothese). Für weitere Informationen siehe Robuste Schätzverfahren, M-Schätzer, Bruchpunkt, Ausreißer.

Robustheit eines biologischen Systems ist die evolutionäre Beständigkeit eines bestimmten Merkmals oder einer Eigenschaft in einem System unter Störungen oder Unsicherheitsbedingungen. Robustheit in der embryonalen Entwicklung ist Kanalisierung.



</doc>
<doc id="4211" url="https://de.wikipedia.org/wiki?curid=4211" title="Rückkopplung">
Rückkopplung

Eine Rückkopplung, auch Rückkoppelung, Rückmeldung oder Feedback [] (engl.), ist ein Mechanismus in signalverstärkenden oder informationsverarbeitenden Systemen, bei dem ein Teil der Ausgangsgröße direkt oder in modifizierter Form auf den Eingang des Systems zurückgeführt wird.

Rückkopplungen kommen in vielen technischen, biologischen, geologischen, wirtschaftlichen und sozialen Systemen vor. Je nach Art und Richtung der rückgeführten Größe kommt es zur Selbstverstärkung des durch das System bedingten Prozesses oder zu dessen Abschwächung oder Selbstbegrenzung. Im ersten Fall spricht man von positiver Rückkopplung oder Mitkopplung, im letzteren Fall von Gegenkopplung oder negativer Rückkopplung.

In technischen Systemen wird häufig angestrebt, Rückkopplungsvorgänge nicht nur durch die Kapazität der beteiligten Energiespeicher zu begrenzen, sondern auch eine Struktur zu schaffen, die durch passive und aktive Gegenkopplung Überlastungen verhindert. Dabei handelt es sich oft um Regelungsvorgänge.

In der Natur können Rückkopplungen in komplexen Strukturen vorkommen, in denen Elemente über andere, zum Teil entfernt gelegene Systeme, wieder auf sich selbst zurückwirken.

In psychologisch determiniertem Verhalten ist die Richtung der Rückkopplung nicht von vornherein festgelegt. So kann zum Beispiel im System "Lernen in der Schule" eine schlechte Notenwertung je nach Motivation sowohl zu erhöhtem Fleiß (Gegenkopplung) oder im Gegenzug auch zur Resignation führen (Mitkopplung, bzw. verstärkende Wirkung).

Bei der Mitkopplung kommt eine vorzeichen- oder phasenrichtige Rückführung der Ausgangsgröße im Zusammenspiel mit verstärkenden Elementen des Systems zum Tragen. Dies kann nützlich sein, beispielsweise um Reibungsverluste auszugleichen, es kann aber auch eine Gefahr darstellen, denn die beteiligten Größen können hierbei gefährlich anwachsen, solange dazu Energie bereitgestellt wird, und es kann, wenn nicht zusätzliche, auf den Prozess dämpfend einwirkende Größen wirksam werden, zu einer Zerstörung kommen. Hierbei wird der Vorgang nur noch durch die Begrenzung der (Energie-) Ressourcen limitiert.

In technischen Systemen spricht man von einer ungedämpften periodischen Schwingung in Resonanz oder einer aperiodischen Schwingung. Je nachdem, ob es zu einem lawinenartigen Anschwellen im System kommen kann oder nicht, unterscheidet man zwischen unterkritischer, kritischer oder überkritischer Mitkopplung.

Positive Rückkopplung findet man oft bei Wachstumsprozessen. Man spricht in nichttechnischen Systemen auch von einem "circulus virtuosus", oder, wenn das Ergebnis als solches nicht gewünscht wird, auch von einem "Teufelskreis" oder "Circulus vitiosus".

Beispiele: Schuldenfalle, Kettenreaktion, Autokatalyse, Akustische Rückkopplung, Börsencrash, elektronische Schaltungen wie z. B. Schmitt-Trigger oder Oszillatoren (Meißner-Schaltung zur Realisierung eines ungedämpften Schwingkreises), Benjamin-Franklin-Effekt der Sozialpsychologie

Bei der Gegenkopplung ist eine Rückführung des Ausgangssignals mit negativem Vorzeichen bzw. gegenphasiger Polarität wirksam. Diese negative Rückführung wirkt der äußeren Anregung entgegen und führt zu einer sich verringernden Zustandsänderung. Solche Systeme neigen entweder stark zum Einnehmen einer stabilen Lage oder zu mehr oder weniger abklingenden Schwingungsverhalten um einen stabilen Mittelwert.

Die Gegenkopplung ist ein fundamentales Prinzip in der gesamten belebten Natur und verbreitet in technischen Systemen. Insbesondere die Regelungstechnik hat das Ziel, die Gegenkopplung so einzurichten, dass ein stabiles Systemverhalten erreicht wird. Dazu werden in technischen Systemen automatische Regler verwendet.

Unerwünschte Schwingneigung in einem Regelkreis kann beispielsweise durch variable Dimensionierung der Gegenkopplung vermieden werden, indem die Verstärkung mit zunehmender Frequenz verringert wird.

Das kennzeichnende Merkmal in natürlichen Systemen sind Rückkopplungsschleifen mit Selbstregulationseigenschaften. In biologischen Systemen von Organismen ist dieses Prinzip entscheidend bei der Homöostase. Gegenkopplung spielt eine Rolle in allen (Selbst-)Erhaltungsprozessen.

Selbsterregung (d. h. das initiale Entstehen von Schwingungen) tritt ein, wenn zunächst eine positive Kopplung und dann verzögert eine negative Kopplung einsetzt. Die Systemparameter pendeln dann zwischen Ruhe und typischen Höchstwerten. Dieser Fall kann insbesondere in elektronischen Regelkreisen bei hohen Frequenzen eintreten, wenn die Phasenbedingung für eine Gegenkopplung aufgrund zufälliger Phasenverschiebungen aller Komponenten nicht mehr zuverlässig gegeben ist.

Beispiele: Fliehkraftregler, das Füllen eines Eimers mit einem Loch im Boden, selbstregelnde Vorgänge in Ökosystemen, Gegenkopplung in Reglern und Verstärkern, Aussetzen von Nützlingen, Drehstrommotoren, Boiler mit Thermostat, Kühlschrank, Körperwärme-Regulation bei Säugetieren, Preisbildung durch Angebot und Nachfrage

Beispiele mit variablem Schwingungsverhalten: Räuber-Beute-Beziehung, Rhythmus der Cortison-Sekretion, zirkadiane biologische Rhythmen, Menstruationszyklus, Konjunkturzyklen, Regelschwingungen (Instabilität) von Reglern, Selbsterregung von Verstärkern (aufgrund ungeeigneter Gegenkopplung, unzureichender Abschirmung zwischen Ein- und Ausgang oder ungeeigneten Lasten)

Die Bedingungen für die Stabilität eines technischen Systems können formal behandelt werden. Anschaulich ist es das Ziel, dass die Kreisverstärkung noch vor einer kritischen Phasendrehung unter eins sinkt.

Details sind zu finden unter

Im Jahr 1912 patentierte Siegmund Strauss (* 1875, † 1942) eine Rückkopplungsschaltung mit einer Lieben-Röhre. Im Jahr 1913 entwickelten unabhängig voneinander Alexander Meißner (* 1883 in Wien, † 1958 in Berlin) bei Telefunken in Berlin und Edwin Howard Armstrong (* 1890, † 1954) in New York das System einer Sender- und Empfängerschaltung mittels Röhren-Verstärker mit Rückkopplung. Im Jahr 1914 patentierte Lee De Forest (* 1873, † 1961) seine Rückkopplungsschaltung. De Forest patentierte als Letzter, hatte aber wahrscheinlich die Rückkopplung als Erster im Labor festgestellt, siehe Supreme Court Entscheidung von 1934.

1927 entdeckte der Telefoningenieur Harold Stephen Black, dass man die Qualität eines Signalverstärkers erheblich verbessern kann, indem man einen Teil des Ausgangssignals vom Eingangssignal subtrahiert, was im Prinzip eine Gegenkopplung darstellt.

Weitere elektrotechnische Beispiele:

In der Tontechnik bzw. der Elektroakustik ist Rückkopplung (kurz Kopplung oder „Koppeln“ oder „Feedback“ genannt) eine in aller Regel unerwünschte, als unangenehmes Pfeifen wahrgenommene Selbsterregung des Systems Schallaufnehmer – Verstärker – Lautsprecher. Hauptsächlich entsteht sie zwischen den vom Verstärker gespeisten Lautsprechern und den Mikrofonen. Aber auch jeder andere elektroakustische Schallaufnehmer, wie beispielsweise bei E-Gitarren oder Schallplattenspielern, kann betroffen sein. Eine Rückkopplung kann sich ausbilden, wenn ein Lautsprecher das Signal eines Mikrofons wiedergibt und gleichzeitig das Mikrofon dieses Signal erneut aufnimmt, wenn es zu nahe am Lautsprecher steht, wie es typischerweise etwa bei Hörgeräten vorkommt. Das Signal wird erneut verstärkt, über den Lautsprecher wiedergegeben und es entsteht eine elektroakustische Schleife, die sich bis zur Selbsterregung aufschaukelt. Die Frequenz der Selbsterregung hängt von den frequenzselektiven Eigenschaften und der Phasenverschiebung der Übertragungsstrecke (Luftstrecke, Equalizer, Lautsprecher, Mikrofoneigenschaften, reflektierende Raumwände) ab.

Im extremen Fall einer Rückkopplung ist neben der Belästigung der Zuhörer auch die Zerstörung der Lautsprecher möglich, insbesondere die Hochtöner können dadurch beschädigt werden. Tieftonlautsprecher und Subwoofer sind gegen Feedback unempfindlicher und verkraften auch ein stärkeres Feedback als Hochtöner. In Lautsprecherboxen befindliche Frequenzweichen können beschädigt werden.

Um Koppeln im Bühnenbetrieb zu vermeiden, können folgende Maßnahmen helfen:

Oft tritt Kopplung bei leerem Zuhörersaal eher ein, als bei gefülltem, da die Zuhörer den Schall und dessen Reflexion im Raum dämpfen.

Das Feedback wird in verschiedenen modernen Musikstilen, vor allem aber in der Rockmusik, insbesondere beim Heavy Metal ganz bewusst als Sounddesign zur Klangbearbeitung eingesetzt.

Des Weiteren gibt es vereinzelt DJs, die Feedback in ihre Performance mit einbauen. Dazu wird das Signal am Kopfhörerausgang des Mischpults auf einen Eingang gelegt. Unter extrem vorsichtiger Verwendung der verschiedenen Regler (Höhen, Mitten, Tiefen usw.) können somit Geräusche von verzerrt über pfeifend bis hin zu rhythmisch schlagend und weiteren Variationen erzeugt werden. Jedes Mischpult reagiert auf diese Manipulation anders. Selbst bei geringen Veränderungen an den Reglern können schlagartig wechselnde Geräusche entstehen, die bei unvorsichtiger Handhabung, wie oben erwähnt, die Boxen zerstören können.

In der Computertechnik kann Feedback dazu dienen, dem Nutzer Information über den aktuellen Zustand einer Anwendung zu geben. Dies kann mit optischen Mitteln geschehen, etwa durch farblich hervor gehobene Details in einer Graphischen Benutzeroberfläche. Oder es werden akustische Signale genutzt, indem ein Geräusch über einen Lautsprecher abgespielt wird. Manche Eingabegeräte besitzen die Fähigkeit, dem Nutzer mit mechanischen Mitteln eine Rückmeldung zu geben. Dieses Force-Feedback dient unter anderem, den Eindruck einer Simulation realistischer zu gestalten.

Die Hardware mancher Computerspiele ist in der Lage, dem Spieler mit mechanischen Mitteln Rückmeldungen über Ereignisse im Spiel zu geben. Dies wird häufig für eine direkte Reaktion auf Aktionen des Spielers genutzt. Ein Beispiel dafür sind Vibrationen eines Lenkrads.

Eine geschlossene Regelschleife mit dem Nutzer als Systemblock wird beim Verfahren User in the loop genutzt, um z. B. im Mobilfunk oder Smart Grid eine Nachfrageregelung zu erreichen.

In der Klimatologie sind viele Rückkopplungen zu beobachten. Bei der Eis-Albedo-Rückkopplung (positive Rückkopplung) wird beispielsweise durch Vereisung mehr Sonnenlicht reflektiert, so dass es kälter wird. Damit können größere Flächen vereisen und es wird noch kälter. Umgekehrt funktioniert der Prozess ebenfalls. 

Weitere Rückkopplungen beinhalten Wolken-Wasserdampf-Kopplungen („Der Wasserdampf-Effekt in der oberen Troposphäre ist der stärkste bekannte Rückkopplungsprozess.“) oder die Kohlendioxid-Aufnahmefähigkeit von Meeren. Viele dieser Prozesse sind noch nicht genau genug erforscht und erschweren eine genaue Klimavorhersage mit Klimamodellen. Dabei ist während der letzten 35 Jahren die Luftfeuchtigkeit am oberen Rand der Wetterschicht um durchschnittlich ca. zehn Prozent gestiegen.

In der Biologie und Medizin bewirkt negative Rückkopplung die Aufrechterhaltung der Homöostase (Gleichgewichtszustand innerhalb zulässiger Grenzen) eines Systems. Solche Regulationsvorgänge laufen z. B. beim Aufrechterhalten der Körpertemperatur von Warmblütern oder bei der Regulation der Genaktivität ab.

Dagegen bedeutet Biofeedback, dass z. B. ein Signalton oder eine Lampe eine Werteüberschreitung einer gemessenen Größe (z. B. Hauttemperatur, Muskelspannung/Tonus oder EEG-Wellenamplitude einer vorgewählten Frequenz) an die untersuchte Person zurückgemeldet wird, die ansonsten nicht oder nicht hinreichend wahrgenommen werden kann. Sie wird auf diese Weise erfahrbar gemacht und kann z. B. zum Erlernen einer Selbstkontrolle dienen.

Der menschliche Körper ist ein komplexes System, welches von einem sehr effektiven Feedback-Kontroll-System gesteuert und kontrolliert wird – dem Zentralnervensystem. Die aktuellen Sinneseindrücke werden dabei immer mit den dazu passenden Informationen (Erfahrungen) aus dem Gedächtnis kombiniert, um dem Individuum eine passende Zukunftsvorhersage als Handlungsvorschlag für die aktuelle Situation zu liefern. Damit kann man auf jede Situation sofort angemessen reagieren. Der Vorschlag des Gehirns wird immer an die aktuelle Situation angepasst. Z. B. steuert das Gehirn beim Gehen/Laufen nach der gesehenen optischen Information über die Beschaffenheit des Weges, wie man den Fuß aufsetzen muss. Das Feedback-System kann z. B. durch Drogen (Alkoholkonsum) gestört werden; dann torkelt man oder stürzt.




</doc>
<doc id="4212" url="https://de.wikipedia.org/wiki?curid=4212" title="Ridley Scott">
Ridley Scott

Sir Ridley Scott (* 30. November 1937 in South Shields, England) ist ein britischer Filmregisseur und Produzent. Er gilt heute als einer der renommiertesten und einflussreichsten Regisseure und hat die Erzählweisen mehrerer Filmgenres geprägt. Seine Filme "Alien", "Blade Runner", "Thelma & Louise" und "Gladiator" bildeten eigene Stile in der Filmgeschichte aus.

Sein Bruder Tony Scott war ebenfalls Hollywood-Regisseur. Ridley und Tony Scott betrieben seit den 1970er Jahren die Produktionsfirma für Werbefilme "Ridley Scott Associates (RSA)". 1995 gründeten dann beide in Los Angeles die Filmproduktionsfirma "Scott Free Productions". 2003 wurde Scott von der britischen Königin aufgrund seiner Verdienste um die Kunst zum Ritter geschlagen. Seine Söhne Luke und Jake Scott und seine Tochter Jordan Scott sind ebenfalls im Filmgeschäft tätig.

Scott wurde als Sohn eines Berufssoldaten geboren (South Shields, Tyne and Wear). Sein Vater, den er selten zu sehen bekam, diente bei den Royal Engineers (Kampfunterstützungstruppen der britischen Armee). Nach Aufenthalten in Cumbria, Wales und Deutschland ließ sich die Familie in Stockton-on-Tees im Norden Englands nieder (die industriell geprägte Landschaft inspirierte später Szenen in "Blade Runner").

Scott erlernte 1954 bis 1958 Grafikdesign und Malerei am "West Hartlepool College of Art" und erlangte das Diplom mit Auszeichnung. Er studierte daraufhin Grafikdesign (M.A., 1960 bis 1962) am Royal College of Art in London, wo David Hockney einer seiner Mitstudenten war. Er schloss 1963 mit Auszeichnung ab. Scott erhielt ein einjähriges Reisestipendium in die USA und wurde bei Time Life, Inc. beschäftigt, wo er mit den Dokumentaristen Richard Leacock und D. A. Pennebaker arbeitete.

Nach seiner Rückkehr nahm er 1965 eine Lehrstelle bei der BBC als Szenenbildner an. Diese Position führte ihn zur Mitarbeit an beliebten Fernsehproduktionen wie der Polizei-Serie "Z-Cars" oder der Science-Fiction-Serie "Out of the Unknown". Nach kurzer Zeit wurde er ins Trainingsprogramm für Regisseure aufgenommen und inszenierte einige Episoden selbst.

1968 verließ Scott die BBC, um "Ridley Scott Associates" (RSA) zu gründen. An dem Projekt arbeiteten neben seinem Bruder Tony Regisseure wie Alan Parker, Hugh Hudson und Hugh Johnson mit. RSA wurde zu einem der erfolgreichsten Werbefilm-Häuser in Europa, in dessen Auftrag Scott für über 2000 Werbespots verantwortlich zeichnet; viele davon wurden auf den Festspielen von Cannes und Venedig ausgezeichnet.

Ridley Scott gilt in der Branche als ökonomischer Regisseur, da er in der Regel mit einem Drittel der Drehtage seiner Kollegen auskommt. Eigenen Worten zufolge verdankt er dies seiner Vergangenheit als Werbe- und Videospotregisseur sowie der Tatsache, dass er manche Szenen mit bis zu 15 Kameras gleichzeitig drehe.

Ab dem Jahr 2000, als sie in "Gladiator" eine Nebenrolle erhielt, war Scott mit der costa-ricanischen Schauspielerin Giannina Facio, Tochter des Diplomaten und Politikers Gonzalo Facio (1918–2018), liiert. Im Juni 2015 heiratete das Paar.

Scotts Markenzeichen ist ein ausgeprägt ästhetischer und malerischer visueller Stil, der sich durch seine jahrelange Erfahrung als Production Designer und Regisseur von Werbespots entwickelt hat. Sein erster Themenfilm "Die Duellisten" (1977) war zwar kommerziell kein großer Erfolg, fand aber bei der Kritik genug Beachtung, um Scott die Realisierung des Science-Fiction-Films "Alien – Das unheimliche Wesen aus einer fremden Welt" (1979) zu ermöglichen.

Sein nächster Film "Blade Runner" (1982), basierend auf dem Roman "Träumen Androiden von elektrischen Schafen?" von Philip K. Dick, spielt in einem düster-futuristischen Los Angeles. Das Werk war visuell derart beeindruckend, dass es für eine ganze Generation Cyberpunk-Literatur, -Musik und -Kunst als Inspiration diente.

In der Folge drehte Scott "Legende" (1985), "Der Mann im Hintergrund" (1987) und "Black Rain" (1989), die alle nicht an die Bedeutung und den Erfolg der vorigen Werke anknüpfen konnten. "Legende" setzte sich jedoch im Lauf der Zeit als Fantasy-Kultfilm durch und wurde 2002 mit einem restaurierten Director’s Cut ergänzt.

Die von der Kritik stetig vorgebrachte Beschuldigung, visuellen Stil vor Inhalt und Charakterzeichnung zu stellen, wurde mit "Thelma & Louise" (1991) entkräftet. Neben guten Kritiken erhielt Scott seine erste Oscar-Nominierung für die beste Regie.

Danach folgten mit dem Kolumbus-Film "1492 – Die Eroberung des Paradieses" (1992), "White Squall – Reißende Strömung" (1996) und "Die Akte Jane" (1997) erneut Filme, die künstlerisch und kommerziell durchfielen. Insbesondere der Militärfilm "Die Akte Jane", in dem Demi Moore eine Frau spielt, die als erste Mitglied bei den Navy Seals werden will, wurde wegen einer nach Ansicht vieler Kritiker undifferenzierten Pro-Militär-Haltung angegriffen.

Mit "Gladiator" feierte Scott 2000 ein triumphales Comeback. Der Film war beim Publikum sehr erfolgreich und gewann neben dem Oscar für den besten Film im Jahr 2000 auch den Golden Globe 2001. Die Regie-Leistung wurde ebenfalls nominiert, den Preis erhielt Scott jedoch nicht.

Eine weitere Oscar-Nominierung erhielt er für den kontroversen Kriegsfilm "Black Hawk Down" (2001), der einen verunglückten US-amerikanischen Militäreinsatz in Somalia thematisiert und in eindrucksvolle Bilder umsetzt. "Black Hawk Down" prägte die neuere "Action"-Darstellung und verhalf der dokumentaristischen Kameraführung zum Durchbruch in der Filmkunst.

Scott übernahm die Regie bei dem Film "Hannibal" (2001), der Fortsetzung zu "Das Schweigen der Lämmer" (1991, Regie: Jonathan Demme).

2005/06 folgte in zwei Versionen der Film "Königreich der Himmel". 2006 erschien "Ein gutes Jahr" nach dem Roman "Ein guter Jahrgang" seines Landsmannes Peter Mayle. Er handelt von einem Bankmanager, der von seinem Onkel ein Weingut in der Provence erbt und daraufhin beschließt, sein Leben umzukrempeln. Die Hauptrolle spielt der australische Schauspieler Russell Crowe.

Gemeinsam mit seinem Bruder Tony produzierte er für den amerikanischen Kabelsender TNT die Miniserie "The Company – Im Auftrag der CIA", die im August 2007 ausgestrahlt wurde. "The Company" erzählt die Geschichte dreier Yale-Absolventen, die in der Nachkriegszeit auf Seiten der CIA bzw. des KGB in den Kalten Krieg verwickelt werden. In den Hauptrollen sind u. a. Chris O’Donnell, Michael Keaton und Alfred Molina zu sehen.

Im Oktober 2008 bestätigte Ridley Scott, dass er 25 Jahre warten musste, bis die Rechte an dem Buch "Der Ewige Krieg" von Joe Haldeman für eine Verfilmung zur Verfügung standen. Scott plane, dieses Buch in 3D zu verfilmen.

Für den US-Fernsehsender CBS produzierte er seit 2009 die Serie "Good Wife". Die Ausstrahlung begann in den USA im September 2009, in Deutschland bei ProSieben Ende März 2010. Auch hier arbeitete Ridley Scott mit seinem Bruder Tony zusammen.

Mit der 2009 abgedrehten Produktion "Robin Hood" legte Scott erneut einen Historienfilm vor. Mit seinem 22. Spielfilm, realisiert nach einem Drehbuch von Brian Helgeland mit Russell Crowe in der Titelrolle, wurden am 12. Mai 2010 die 63. Filmfestspiele von Cannes eröffnet.

Ridley Scott arbeitet an der ersten Verfilmung von Aldous Huxleys Roman "Schöne neue Welt" für das Kino. Der Film wird von ihm und Leonardo DiCaprio produziert, Drehbuchautor ist Farhad Safinia. Scott wird voraussichtlich auch Regie führen.

Der Film "Prometheus" war ursprünglich als Prequel zu seinem ersten großen Erfolg "Alien" geplant. Das Drehbuch stammt von Jon Spaihts; Damon Lindelof überarbeitete das Drehbuch für 20th Century Fox. In den USA erfolgte der Kinostart am 8. Juni 2012. 2017 folgte die Fortsetzung "". Im selben Jahr verfilmte er mit "Alles Geld der Welt" den Entführungsfall um John Paul Getty III.





Emmy







</doc>
<doc id="4214" url="https://de.wikipedia.org/wiki?curid=4214" title="Redewendung">
Redewendung

Eine Redewendung, auch Phraseologismus, Idiom oder idiomatische Wendung, ist eine feste Verbindung mehrerer Wörter („feste Wortverbindung“) zu einer Einheit, deren Gesamtbedeutung sich nicht unmittelbar aus der Bedeutung der Einzelelemente ergibt. Es handelt sich um ein rhetorisches Stilmittel und um den Spezialfall einer Kollokation.

Die Vielfalt der alten wie der neuen Begriffe ergibt ein terminologisches Chaos. Im Englischen wird sogar der übergeordnete Begriff des rhetorisches Stilmittels („figure of speech“) als weiteres Synonym benutzt. Im Deutschen werden nebeneinander folgende Termini gebraucht:

Der Duden definiert "Redewendung" als eine „feste Verbindung von Wörtern, die zusammen eine bestimmte, meist bildliche Bedeutung haben“.

Das Wort Phraseologie bezeichnet dabei sowohl die Gesamtheit der in einer Sprache auftretenden Redewendungen als auch die sich damit befassende Wissenschaft. Eine "phraseologische Einheit" ist die Verbindung zweier oder mehrerer Wörter, die keine allein aus ihnen selbst erklärbare Einheit bilden. Als Beispiel die Redensart "ins Gras beißen": Sie hat nichts mit "in den Apfel beißen" oder "ins Gras fallen" zu tun. Die Wendung hat die Bedeutung „sterben“ und kann nicht durch Wendungen wie "in die Wiese beißen" oder "ins Gras schnappen" ersetzt werden.

Der Sprachforscher Lutz Röhrich weist darauf hin, dass wörtliche und übertragene Bedeutung oft nebeneinander bestehen.

Der Satz „Der Ofen ist aus“ kann beispielsweise zweierlei bedeuten:
Das Verstehen setzt zunächst die Kenntnis der Hintergründe voraus. Die Aneignung der Sprachbildlichkeit ist ein Prozess, der sich über einen großen Teil der Kindheit hinzieht und am Ende über ein ganzes Leben erstrecken kann. So vermag z. B. ein Nicht-Muttersprachler durchaus zu lernen, was die deutschen Worte "grün" und "Zweig" bedeuten; um aber die Bedeutung von "auf einen grünen Zweig kommen" zu kennen („den richtigen Weg einschlagen“ / „erfolgreich werden“ / „zu Wohlstand kommen“), bedarf es einer größeren Vertrautheit mit dem Deutschen.

Phraseologismen bestimmen die Spezifik einer Sprache stärker als der Wortschatz. Die Idiomatizität einer Wortverbindung zeigt sich daran, dass

Diese Wendungen werden unterschieden von den Gruppen der freien (unfesten) Wortverbindungen und den losen Wortverbindungen. In ungenauer Redeweise werden unter Redewendungen auch Sprichwörter, Redensarten, Funktionsverbgefüge und Zwillingsformeln subsumiert.

Oft enthalten sie ehemalige rhetorische Figuren, vor allem Metaphern. Fast immer sind sie aus sprachhistorisch älteren unidiomatischen („wortwörtlich gebrauchten“) Syntagmen entstanden. Die Unanalysierbarkeit der Bedeutung löst sich somit fast immer auf, wenn die Geschichte einer Redewendung nur weit genug zurückverfolgt werden kann. Redewendungen können (wie alle Wortschatz-Elemente) eine eingeschränkte regionale Verbreitung haben.

Mit dem Sprichwort gemeinsam hat die Redewendung das einprägsame Bild, dessen Wortlaut unveränderlich ist. So heißt es "Maulaffen feilhalten" und nicht "Maulaffen verkaufen".

Literarische Zitate, die als Redewendungen Eingang in den allgemeinen Sprachgebrauch gefunden haben, werden als geflügelte Worte bezeichnet.





</doc>
<doc id="4215" url="https://de.wikipedia.org/wiki?curid=4215" title="Robert Zemeckis">
Robert Zemeckis

Robert Lee Zemeckis [] (* 14. Mai 1952 in Chicago, Illinois) ist ein US-amerikanischer Regisseur und Filmproduzent. Seine Regiearbeit an "Forrest Gump" verschaffte ihm 1995 einen Oscar.

Zemeckis wurde in Chicago als Sohn eines litauisch-amerikanischen Vaters und einer italo-amerikanischen Mutter geboren und wuchs in einer katholischen Arbeiterfamilie auf.

Bereits als Kind liebte Zemeckis Film und Fernsehen und war von der 8-mm-Filmkamera seines Vaters begeistert. Nachdem er damit bereits Familienfeste und Urlaubserlebnisse gefilmt hatte, begann er, kleine Dokumentarfilme mit Freunden zu drehen, in denen er auch Spezialeffekte einzubauen versuchte. Der Film "Bonnie and Clyde" faszinierte ihn dermaßen, dass er beschloss, eine Filmschule zu besuchen.

Zemeckis opferte eigenen Aussagen zufolge viel Lebenszeit, nur um im Filmgeschäft Karriere zu machen. „Mit 44 Jahren gewann ich einen Academy Award, aber ich bezahlte dafür mit meiner Lebenszeit der 20er. Diese Dekade meines Lebens von der Filmschule bis 30 war nichts als Arbeit, nichts als absolutes, getriebenes Arbeiten. Ich hatte kein Geld und kein Leben.“

1980 heiratete Zemeckis die Schauspielerin Mary Ellen Trainor, mit der er einen Sohn namens Alexander hat. Diese Beziehung stand immer im Spannungsverhältnis mit seinem filmischen Schaffen und endete 2000 mit Scheidung. 2001 heiratete er die Schauspielerin Leslie Harter.

Politisch betätigte sich Zemeckis wiederholt als Wahlkampfsponsor für die Demokratische Partei.

Seine Abenteuerkomödie "Auf der Jagd nach dem grünen Diamanten" war Zemeckis’ erster großer Erfolg, welcher ihm die Realisierung des 1985 erschienenen, ebenfalls sehr erfolgreichen Filmklassikers "Zurück in die Zukunft" ermöglichte, für den er zusammen mit Bob Gale auch das Drehbuch verfasste und dem 1989/90 zwei Fortsetzungen zur Trilogie folgten. Zemeckis erhielt 1995 den "„Oscar“" in der Kategorie „Beste Regie“ in dem Film "Forrest Gump" mit Tom Hanks in der Hauptrolle. Er führte auch Regie bei "Falsches Spiel mit Roger Rabbit", "Contact" und "Cast Away – Verschollen" (erneute Zusammenarbeit mit Hanks).

Robert Zemeckis hat bis jetzt acht Filme mit einem Einspielergebnis von über 100 Millionen Dollar in den Vereinigten Staaten geschaffen. Er gehört zu den treuesten Weggefährten Steven Spielbergs. Spielberg produzierte fast alle seine Filme, und Robert Zemeckis schrieb auch das Drehbuch für den Steven-Spielberg-Film "1941 – Wo bitte geht's nach Hollywood".

Robert Zemeckis gründete 1998 mit Steve Starkey und Jack Rapke die Produktionsfirma ImageMovers, welche jedoch 2007 geschlossen und nach einiger Zeit von der Walt Disney Company als "ImageMovers Digital" wieder eröffnet wurde. Zudem gründete er zusammen mit Joel Silver und Gilbert Adler die auf Horrorfilme spezialisierte Produktionsfirma Dark Castle Entertainment.

Zemeckis’ Film "Der Polarexpress" wurde per Motion Capture komplett computeranimiert hergestellt und stellt die dritte Zusammenarbeit mit Tom Hanks dar. Nach "Die Legende von Beowulf" entstammt Charles Dickens’ "Disneys Eine Weihnachtsgeschichte" ebenfalls reiner Computerarbeit mit gleicher Technik. In diesem fungierte Jim Carrey als menschliches Vorbild für die künstlich generierte Computerfigur.

Seit "Auf der Jagd nach dem grünen Diamanten" sind die Regiearbeiten von Robert Zemeckis durch die Filmmusik des Komponisten Alan Silvestri geprägt (Ausnahme: "Drei Wege in den Tod"). 2012 drehte er mit seiner Frau Leslie Zemeckis die Dokumentationen "Bound by Flesh" über die siamesischen Zwillinge Daisy und Violet Hilton.

Nominierung

Nominierung


</doc>
<doc id="4216" url="https://de.wikipedia.org/wiki?curid=4216" title="Robert Benton">
Robert Benton

Robert Benton (* 29. September 1932 in Waxahachie, Texas, USA) ist ein US-amerikanischer Regisseur, Drehbuchautor und Filmproduzent. Seine Bekanntheit begründete er zunächst als Drehbuchautor, um dann als Regisseur internationale Erfolge zu erzielen.

Robert Benton wuchs in seinem kleinen Geburtsort Waxahachie auf, machte an der University of Texas seinen Bachelor of Fine Arts und ging nach New York, wo er an der Columbia University den Master Degree erwarb. Sein Berufswunsch war, Maler zu werden, seine über Jahre anhaltende Arbeit für den Esquire, zuletzt als Art Director, brachte ihn aber mehr zum Schreiben. Zusammen mit Harvey Schmidt veröffentlichte er 1962 "The In and out Book". Zugleich begann er, mit David Newman das Drehbuch für "Bonnie und Clyde" zu entwickeln. Nach großen Anfangsschwierigkeiten wurde es 1966 verfilmt. Nach weiteren Erfolgen mit "Zwei dreckige Halunken" (1969) und "Is’ was, Doc?", für das der Filmemacher Peter Bogdanovich den Plot lieferte, beendete das Autorenteam 1972 mit "In schlechter Gesellschaft" seine Zusammenarbeit. Der Film war für Robert Benton zugleich das Debüt als Regisseur. Für seinen Welterfolg "Kramer gegen Kramer", wie auch für seine späteren Regiearbeiten, war sein eigenes Drehbuch häufig Vorlage.








</doc>
<doc id="4217" url="https://de.wikipedia.org/wiki?curid=4217" title="Richard Quine">
Richard Quine

Richard Quine (* 12. November 1920 in Detroit, Michigan, USA; † 10. Juni 1989 in Los Angeles, Kalifornien) war ein US-amerikanischer Schauspieler, Drehbuchautor, Komponist, Produzent und Filmregisseur. Quine diente im Zweiten Weltkrieg bei der US Coast Guard. 

Er war mit der Schauspielerin Susan Peters von 1943 bis 1948 verheiratet, gemeinsam hatten sie ein Kind. Während der Dreharbeiten zu "Fremde, wenn wir uns begegnen" im Jahr 1959 wurden Kim Novak und Quine ein Paar. Das im Film errichtete futuristische Haus (von Carl Anderson und Ross Bellah) sollte nach Beendigung der Dreharbeiten Novak und Quine als Hochzeitsgeschenk überreicht werden. Da sich aber die Heiratspläne zerschlugen, bezog Quine das Haus später allein.

1989 beging er durch einen Gewehrschuss Suizid.

Als Schauspieler
Als Regisseur


</doc>
<doc id="4218" url="https://de.wikipedia.org/wiki?curid=4218" title="Robert Siodmak">
Robert Siodmak

Robert Siodmak (* 8. August 1900 in Dresden; † 10. März 1973 in Locarno) war ein deutscher Filmregisseur, Drehbuchautor und Filmproduzent. Mit dem Film "Menschen am Sonntag" drehte er 1929 einen der wichtigsten Vertreter der "Neuen Sachlichkeit". Wie viele Filmschaffende seiner Zeit floh er vor der nationalsozialistischen Diktatur aus Deutschland. In Hollywood machte er sich in den 1940er Jahren einen Namen als Regisseur von Thrillern und Films noirs, wie "Die Wendeltreppe" und "Rächer der Unterwelt", die als Klassiker ihrer Genres gelten.

Siodmak entstammte einer jüdischen Familie. Seine Eltern waren der Kaufmann Ignatz Siodmak und dessen Ehefrau Rosa Philippine, geborene Blum. Ignatz Siodmak stammte ursprünglich aus Schlesien, war nach Amerika ausgewandert und hatte sich dann als US-amerikanischer Staatsbürger 1899 in Deutschland niedergelassen, wo er heiratete. Im Jahr 1902 wurde hier auch Robert Siodmaks jüngerer Bruder Curt Siodmak geboren. Robert Siodmaks Mutter starb noch vor ihrem 40. Geburtstag an Krebs, sein jüngster Bruder Rolf beging mit gerade einmal 20 Jahren Selbstmord. Bereits in seinen Jugendjahren brach er mit seinem Vater.

Siodmak besuchte das Gymnasium in Dresden und nahm Schauspielunterricht bei Erich Ponto. Im Jahr 1918 schloss er sich einer Wanderbühne an, 1921 arbeitete er als Buchhalter bei den Banken Mattersdorf und Schermer in Dresden, 1924 gründete er den „Verlag Robert Siodmak“ und gab kurzzeitig die Illustrierte "Das Magazin" heraus.

Einer von Siodmaks ersten Filmaufträgen war die Übersetzung der Zwischentitel für Lewis Milestones "Die Schlachtenbummler" (1927). Bei der von seinem Onkel Heinrich Nebenzahl geleiteten Nero-Film in Berlin etablierte er sich als Schnittmeister und Regieassistent für Filme von Harry Piel und Kurt Bernhardt. Schließlich konnte Siodmak Nebenzahl überreden, ihm das Startkapital für seinen Debütfilm "Menschen am Sonntag" zur Verfügung zu stellen.

Bei "Menschen am Sonntag" (gedreht 1929, uraufgeführt 1930) führte Robert Siodmak, gemeinsam mit Edgar G. Ulmer, erstmals selbst Regie. Durch den Erfolg dieses ausschließlich mit Laien besetzten halbdokumentarischen Films erhielt er einen Vertrag bei der Universum Film (UFA), für die er Filmdramen, Kriminalfilme und Filmkomödien inszenierte. Zu den Darstellern, mit denen Siodmak arbeitete, gehörten schnell Filmgrößen wie Emil Jannings und Hans Albers. Als er 1932 bei "F.P.1 antwortet nicht", einer Verfilmung des Romans von Curt Siodmak, wegen seiner jüdischen Wurzeln übergangen wurde, verließ er die UFA. 1933 erschien "Das brennende Geheimnis", Siodmaks Leinwandversion von Stefan Zweigs Roman "Brennendes Geheimnis". Die Aufführung des Films wurde von Joseph Goebbels’ kurz zuvor eingerichtetem Reichsministerium für Volksaufklärung und Propaganda, das Anspielungen auf den Reichstagsbrand vom 27. Februar 1933 mutmaßte, verboten.

Nach der „Machtergreifung“ der Nationalsozialisten verließ Siodmak Deutschland und ging nach Frankreich. In Paris arbeitete er unter anderem für die Néro-Films seines Cousins Seymour Nebenzahl, der wie Siodmak hatte emigrieren müssen. Größter Publikumserfolg dieser Schaffensperiode wurde 1939 der Film "Mädchenhändler (Pièges)" mit Maurice Chevalier, Marie Déa und Erich von Stroheim. Siodmaks Vorhaben, Ödön von Horváths Roman "Jugend ohne Gott" zu verfilmen, scheiterte am plötzlichen Unfalltod des Autors.

Mit Beginn des Zweiten Weltkrieges wanderte Siodmak in die USA aus. Er arbeitete zunächst für Paramount Pictures, 20th Century Fox und Republic Pictures. Seine Bewerbung bei Mark Hellinger, damals Produzent bei Warner Brothers, war dagegen erfolglos, obwohl Hellinger ihn später für "Rächer der Unterwelt" engagierte. 1943 drehte er mit "Draculas Sohn" seinen ersten Film für die Universal Studios, bei denen er bis 1950 unter Vertrag blieb. Siodmak äußerte sich rückblickend abfällig über seine vor "Zeuge gesucht" (1944) entstandenen amerikanischen Filme, die er als reine Brotarbeiten betrachtete.

"Zeuge gesucht" läutete eine Reihe von Thrillern und Films noirs ein, von denen einige heute als Klassiker ihrer Genres gelten. "Zeuge gesucht", "Rächer der Unterwelt" (1946) und "Gewagtes Alibi" (1949) förderten die Karriere von Burt Lancaster, Ava Gardner, Ella Raines und Tony Curtis. „Wenn man mit einer Sorte Film Erfolg hat, bekommt man den Auftrag, mehr von dieser Sorte zu drehen“, erklärte Siodmak 1959 die Gewichtung seiner Filme jener Jahre. Filmhistoriker entdeckten in diesen Arbeiten als inhaltliche und stilistische Gemeinsamkeiten die Untersuchung von „krankhafter Psychologie“ (Charles Higham) und „obsessiver Liebe und Hass“ (Colin McArthur) sowie „deutsche Lichtsetzung und expressionistische Umwandlung der äußeren Wirklichkeit“ (Foster Hirsch). David Thomson dagegen bezeichnete Siodmak als reinen „Auftragsregisseur“ und bezweifelte, ob man in seinem Fall von einer künstlerischen Autorenschaft sprechen könne. Universal „lieh“ Siodmak auch wiederholt an andere Studios aus, so an RKO Pictures für "Die Wendeltreppe" (1945), 20th Century Fox für "Schrei der Großstadt" (1948) und MGM für die Dostojewski-Verfilmung "Der Spieler" (1949).

Der für Paramount inszenierte "Strafsache Thelma Jordon" (1950) beendete Siodmaks Noir-Reihe. Ein neuerliches Sieben-Jahres-Vertragsangebot von Universal lehnte er ab. 1952 drehte Siodmak mit dem Abenteuerfilm "Der rote Korsar" seinen letzten amerikanischen Film vor Verlassen seiner Wahlheimat. Die Dreharbeiten waren gezeichnet von anhaltenden Auseinandersetzungen zwischen ihm und Hauptdarsteller Burt Lancaster, der inzwischen zum Star avanciert war. Siodmak zog das Ende der 1940er Jahre aussterbende, alte Hollywood-Studiosystem den neuen Studioverhältnissen vor, in denen in seinen Worten „Anarchie“ herrsche und „egomanische“ Stars das Sagen hätten, und ging zurück nach Europa.

Curt Siodmak und Filmhistoriker Hervé Dumont gaben als weiteren Grund für Siodmaks Abkehr von Hollywood an, er sei unter anderem wegen seiner persönlichen Bekanntschaft zu Charlie Chaplin ins Visier des House Committee on Un-American Activities (HUAC) geraten, das in den 1940er und 1950er Jahren Filmschaffende auf ihre politische Gesinnung überprüfte. Diese Behauptung ist jedoch nicht belegt.

Mitte der 1950er Jahre ließ Siodmak sich in Ascona am Schweizer Ufer des Lago Maggiore nieder und führte auch wieder in Deutschland Regie. Erneut betätigte er sich auf so unterschiedlichen Gebieten wie dem Kriminalfilm, dem Filmdrama, dem Western und dem Historienfilm. „Wenn ich, wie Hitchcock, mein ganzes Leben nur Kriminalfilme gemacht hätte, wäre mein Name bestimmt ebenso bekannt. Aber das langweilte mich und ich versuchte es auf verschiedenen Gebieten.“ (Siodmak)

Für "Die Ratten" (1955), seine Verfilmung von Gerhart Hauptmanns gleichnamigem Theaterstück, wurde er auf den Internationalen Filmfestspielen Berlin mit dem Goldenen Bären ausgezeichnet. Der 1957 von ihm inszenierte "Nachts, wenn der Teufel kam" erhielt zahlreiche nationale und internationale Auszeichnungen, darunter auch eine Oscar-Nominierung als Bester fremdsprachiger Film. Der Film behandelt den Fall des angeblichen Serienmörders Bruno Lüdke vor dem zeithistorischen Hintergrund des Nationalsozialismus. In seiner Autobiografie bemerkte Siodmak, dass er von seinen nach der Rückkehr aus den USA gedrehten Filmen nur auf diese zwei stolz sei.

Siodmak drehte mit namhaften Darstellern wie Maria Schell, Curd Jürgens, Heinz Rühmann und Romy Schneider, und die meisten Filme waren an der Kinokasse profitabel, wenn auch nicht immer erfolgreich bei der Kritik. "Mein Schulfreund" (1960) und "Affäre Nina B." (1961) hatten nach "Nachts, wenn der Teufel kam" erneut den Nationalsozialismus als thematischen Hintergrund. In den Jahren 1964 und 1965 führte Siodmak Regie in drei von Artur Brauner produzierten Karl-May-Verfilmungen mit Lex Barker: "Der Schut", "Der Schatz der Azteken" und "Die Pyramide des Sonnengottes". In Spanien drehte Siodmak für US-Produzent Philip Yordan den Western "Ein Tag zum Kämpfen". 1968 und 1969 erschien der wieder von Brauner produzierte zweiteilige Monumentalfilm "Kampf um Rom" nach Felix Dahns 1876 erschienenem historischen Roman "Ein Kampf um Rom".

"Kampf um Rom" blieb Siodmaks letzte Regiearbeit, das geplante Projekt „Atrox“ mit James Mason wurde nicht mehr realisiert. Dessen ungeachtet zeigte er bis zuletzt ein großes Interesse am internationalen filmischen Geschehen, äußerte sich beeindruckt von der französischen Nouvelle Vague, vom italienischen Kino und von Francis Ford Coppolas "Der Pate". In Thomas Schamonis Film "Ein großer graublauer Vogel" (1970) absolvierte Siodmak einen kurzen Auftritt. Seine Frau Bertha, die er am 16. November 1933 in Paris geheiratet hatte, starb am 20. Januar 1973. Zwei Monate später, am 10. März 1973, erlag Robert Siodmak im Alter von 72 Jahren einem Herzinfarkt.

Die Nachrufe im Spiegel und in der New York Times präsentierten sich beide mit fehlerhaften Angaben zu Leben und Werk des Regisseurs: Während der Spiegel Siodmaks Geburtsort in die USA verlegte (eine Legende, die Siodmak in den 1930er Jahren selbst geschaffen hatte, um ein Auslandsvisum zu erhalten), verlieh ihm die New York Times den Titel „Meister der Low-key-Thriller der 1950er“. Zur Erstausgabe von Siodmaks (postum erschienener) Autobiografie schrieb Herausgeber Hans-Christoph Blumenberg: „Er drehte Filme über mörderische Obsessionen, über labile, heillos in krankhafte Familien-Bande verstrickte Schwächlinge, die sich durch Verbrechen zu befreien versuchen […] über einsame, unglückliche Männer im Dickicht der Städte […] aber es dürfte unmöglich sein, einen definitiven Siodmak-Touch zu entdecken: stilistische und thematische Konstanten, die das ganze, über genau vierzig Jahre und viele Länder verstreute Werk zusammenhalten.“ 1965, als Siodmaks Karriere sich bereits ihrem Ende zuneigte, konstatierte Pauline Kael in "I Lost It at the Movies": „Ich vermute, dass jeder, der sich für Filme interessiert, den Namen eines Regisseurs als Leitfaden benutzt […] in den 40er Jahren hielten meine Freunde und ich Ausschau nach den Filmen von Robert Siodmak […] [diese wurden] oft fälschlicherweise anderen Regisseuren oder medienpräsenten Produzenten zugeschrieben.“

1980 zeigte das Londoner National Film Theatre eine Werkschau mit Robert Siodmaks Filmen. 1998 widmeten die Internationalen Filmfestspiele Berlin den Gebrüdern Siodmak eine Retrospektive, der Curt Siodmak, der selbst zwei Jahre später starb, beiwohnte.







</doc>
<doc id="4219" url="https://de.wikipedia.org/wiki?curid=4219" title="Richard Attenborough">
Richard Attenborough

Richard Samuel Attenborough, Baron Attenborough CBE (* 29. August 1923 in Cambridge, England; † 24. August 2014 in London) war ein britischer Schauspieler, Regisseur sowie mehrfacher Oscar- und Golden-Globe-Preisträger.

Attenborough wuchs als Sohn einer sozial sehr engagierten Mutter auf; sein Vater leitete später die University of Leicester. Im Alter von 17 Jahren ging Richard nach London und besuchte dort die "Royal Academy of Dramatic Art". Sein Debüt gab er 1942 am West End Theatre. Im selben Jahr wirkte er außerdem in einer kleineren Rolle in dem Film "In Which We Serve" mit. 

Nach drei Jahren als Pilot bei der Royal Air Force wandte sich Attenborough nach dem Zweiten Weltkrieg wieder der Schauspielerei zu. Den Durchbruch schaffte er mit der Rolle des psychophatischen Jung-Gangsters Pinkie Brown aus Graham Greenes Roman "Brighton Rock", eine Rolle, die er sowohl am Londoner West End als auch in der gleichnamigen Verfilmung von 1947 verkörperte. Im Jahr 1959 gründete er mit Bryan Forbes die Produktionsfirma "Beaver Films". Insgesamt drehte er in den 1940er- und 1950er-Jahren rund 30 Filme. Der große Durchbruch gelang ihm als Schauspieler jedoch erst 1963 mit dem Klassiker "Gesprengte Ketten", in dem er neben Steve McQueen, Charles Bronson, James Coburn und James Garner zu sehen war.

Ab Ende der 1960er-Jahre war Attenborough verstärkt auch als Regisseur und Produzent tätig. Seinen größten Erfolg hinter der Kamera hatte er 1982 mit "Gandhi", der acht Oscars (u. a. Beste Regie und Bester Film) sowie zahlreiche andere Preise erhielt. Attenborough hatte 20 Jahre um die Finanzierung gekämpft. Im Jahr 1993 war Attenborough erstmals seit 1979 wieder als Schauspieler tätig. Steven Spielberg besetzte ihn in "Jurassic Park" und erfüllte sich damit „einen lebenslangen Traum“.

Attenborough war zudem in zahlreichen Ämtern tätig und saß unter anderem in den Vorständen des Fernsehsenders "Channel 4", der Royal Academy of Dramatic Art, deren Präsident er von 2004 bis zu seinem Tod war, des "British Film Institute" und der "British Academy of Screen and Television Arts". Seit 1997 war er Präsident der National Film and Television School und Dozent der Universität von Oxford. Seine zahlreichen Vorstandsposten brachten ihm den Spitznamen „Chairman of London“ ein.

Nach einem Sturz Ende 2008 in seinem Haus in London verschlechterte sich Attenboroughs gesundheitliche Verfassung zunehmend. Er starb im August 2014, wenige Tage vor seinem 91. Geburtstag, in London. 

Zahlreiche bekannte Synchronsprecher liehen Attenborough ihre Stimme. Friedrich W. Bauschulte tat dies in den neueren Filmen, so unter anderem in den ersten beiden Teilen von "Jurassic Park". Weitere namhafte Sprecher des Briten waren Michael Chevalier, Paul Klinger und Joachim Nottke.

Der Tierfilmer und Naturforscher Sir David Attenborough ist sein jüngerer Bruder.

Richard Attenborough war seit 1945 mit der Schauspielerin Sheila Sim (1922–2016) verheiratet, mit der er Vater dreier Kinder wurde. Mit seiner Frau spielte er als Sergeant Trotter in den ersten Londoner Aufführungen von Agatha Christies "Die Mausefalle".

Sein Sohn Michael Attenborough ist Theaterregisseur und früherer Leiter des Londoner Almeida Theatre. Er ist mit der Schauspielerin Karen Lewis verheiratet und Vater zweier Söhne. Attenboroughs Tochter Charlotte ist ebenfalls Schauspielerin. Aus ihrer Ehe gingen drei Kinder hervor.

Richard Attenboroughs 1955 geborene Tochter Jane gilt seit der Flutkatastrophe in Südasien 2004 gemeinsam mit ihrer Schwiegermutter als vermisst. Seine 1989 geborene Enkeltochter kam bei der Katastrophe ums Leben. Sein Enkel, der Schauspieler Samuel Holland, und eine weitere Enkeltochter überlebten das Unglück.

Seine Beliebtheit und sein hohes Ansehen in der Öffentlichkeit nutzte Attenborough, um sich für die Schwachen und Benachteiligten in der Welt stark zu machen. Für sein Engagement erhielt er 1983 den Martin-Luther-King-Friedenspreis. Im Laufe der Zeit wurden ihm zahlreiche weitere Ehrentitel zugesprochen. So war er unter anderem UNICEF-Botschafter, Ehrenbürger der Stadt Leicester, Ehrendoktor und Kanzler der Universität von Sussex, Kommandeur des Ordre des Arts et des Lettres, Ritter der Ehrenlegion und Träger des Padma-Bhushan-Ordens.

Im Jahr 1967 wurde er Commander of the Order of the British Empire (CBE), 1976 schlug ihn Königin Elisabeth II. zum Knight Bachelor, womit die Anrede Sir einherging. Am 30. Juli 1993 erfolgte aus Anlass seines 70. Geburtstages seine Erhebung zum Life Peer mit dem Titel "Baron Attenborough", of Richmond upon Thames im London Borough of Richmond upon Thames, wodurch sich seine Anrede in "Lord" änderte. Er hatte einen Sitz als Labour-Mitglied im House of Lords.

Richard Attenborough bekam 1998 das Praemium Imperiale verliehen.







</doc>
<doc id="4220" url="https://de.wikipedia.org/wiki?curid=4220" title="Robert Redford">
Robert Redford

Charles Robert Redford, Jr. () (* 18. August 1936 in Santa Monica, Kalifornien) ist ein US-amerikanischer Schauspieler, Filmregisseur und Filmproduzent. Redford zählt seit den späten 1960er Jahren zu den populärsten Hollywoodschauspielern und war zwischen 1973 und 1976 der kassenträchtigste Filmstar. Er hat neun Spielfilme inszeniert und für sein Regiedebüt "Eine ganz normale Familie" 1981 den Oscar erhalten. Redford ist auch als Umweltschützer aktiv und unterstützt unter anderem den Natural Resources Defense Council.

Robert Redford ist der Sohn von "Martha" und "Charles Robert Redford" und wuchs mit seinem Halbbruder "William" in einfachen Verhältnissen auf. Er besuchte die Van Nuys High School in Los Angeles. 1955 erhielt er aufgrund seines Talents im Baseballspiel ein Stipendium der University of Colorado Boulder. Im selben Jahr starb seine Mutter, woraufhin seine sportlichen und schulischen Leistungen stark zurückgingen, was schließlich – Alkoholmissbrauch kam hinzu – zum Verlust seines Stipendiums führte. Redford hielt sich zunächst mit kleineren Jobs auf den Ölfeldern um Los Angeles über Wasser, ging dann nach Europa, wo er in Paris und Florenz kurzzeitig an mehreren Kunstakademien studierte. In Florenz versuchte er sich in der Kunst der Malerei und war zeitweise auch als Straßenmaler tätig.

1958 zog es Redford wieder zurück in die USA. Er studierte nun Theaterdesign am New Yorker Pratt Institute und besuchte, dem Rat eines Lehrers folgend, ab 1959 außerdem die "American Academy of Dramatic Arts", wo er seine Leidenschaft für die Schauspielerei entdeckte. Seine ersten Rollen hatte er in den Broadway-Produktionen "Tall Story" und "Little Moon of Alban". Das erste Mal vor der Kamera stand er für eine Episode der Fernsehserie Maverick, wo James Garner, Jack Kelly und Roger Moore die Hauptrollen spielten. 1962 gab er in Hinter feindlichen Linien sein Leinwanddebüt. Während er am Theater große Erfolge feierte, kam seine Filmkarriere nur schwer in Gang – bis 1966 landete er einen Flop nach dem anderen. Joyce Selznick, eine Cousine des Produzenten David O. Selznick und Chefin der Nachwuchs-Scouts der Columbia versuchte, Redford unterzubringen – doch zu dem Zeitpunkt wollten ihn die Studios nicht. Immer häufiger zog er sich mit seiner Familie nach Spanien oder Kreta zurück, um dem Stress zu entkommen, den Hollywood für ihn bedeutete.

1967 machte Redford in Barfuß im Park erstmals ein größeres Kinopublikum auf sich aufmerksam. Zwei Jahre später wurde er mit Butch Cassidy und Sundance Kid zum Superstar. Seit 1976 arbeitet Robert Redford auch als Filmproduzent, seit 1980 zudem als Filmregisseur. Später gründete er gemeinsam mit Richard Gregson die Produktionsfirma Wildwood Enterprises, mit der er fortan Filme produziert.

Der Schauspieler steht dem Film-Establishment in Hollywood von jeher skeptisch gegenüber und führt ein zurückgezogenes Privatleben. Er lebt vorwiegend im Napa Valley bei San Francisco und in Utah und gehörte nie zur Hollywood-Schickeria. Der linksliberale Redford ist als Naturliebhaber bekannt und hat oft den Niedergang des alten amerikanischen Westens beklagt. Er engagiert sich für den Umweltschutz und die Rechte der indigenen Bevölkerung.

Am 21. September 1958 heirateten Redford und Lola Van Wagenen, 1985 ließ sich das Paar scheiden. Aus der Ehe gingen vier Kinder hervor: Scott (*/† 1959), Shauna Redford (* 1960), David James Redford (* 1962) und Amy Heart Redford (* 1970). Seit 1996 ist Robert Redford mit der deutschen Malerin Sibylle Szaggars (* 1957) liiert, am 11. Juli 2009 heiratete das Paar im Hamburger Hotel „Louis C. Jacob“. Redford hat vier Enkelkinder.

Robert Redford stand ab 1960 als TV-Schauspieler vor der Kamera und spielte bis 1964 zahlreiche Rollen in Serien wie "Maverick", "Perry Mason", "Alfred Hitchcock Presents" oder "Die Leute von der Shiloh Ranch". 1962 startete er seine Filmkarriere mit dem künstlerisch ambitionierten (Anti-)Kriegsfilm "Hinter feindlichen Linien", in dem er einen Soldaten des Korea-Krieges spielte. Dieser Film konnte sich an den Kinokassen nicht durchsetzen. Bei den Dreharbeiten freundete sich Redford mit dem späteren Regisseur Sydney Pollack an, der hier ebenfalls als Filmschauspieler debütierte.

Nachdem sich einige Kino-Projekte zerschlagen hatten, drehte Redford 1965 die Kriegskomödie "Lage hoffnungslos, aber nicht ernst", in der er einen amerikanischen Soldaten spielte, der während des Zweiten Weltkriegs von einem deutschen Sonderling (Alec Guinness) im Keller versteckt wird. Dieser in Deutschland gedrehte Film kam weder bei der Kritik noch beim Publikum gut an. Auch das Filmdrama "Ein Mann wird gejagt" (1966), in dem Redford als flüchtiger Sträfling zu sehen war, blieb trotz profilierter Besetzung (u.a. Marlon Brando und Jane Fonda) weit hinter den kommerziellen Erwartungen zurück.

"Verdammte, süße Welt" (1965) war der erste von zwei Filmen, die Redford mit der damals sehr populären Natalie Wood, einer ehemaligen Schulkameradin von der Van Nuys High School, drehte. Sie stellt mit 27 Jahren eine 15-jährige dar, die von einer Hollywood-Karriere träumt. 1966 standen die beiden Darsteller in "Dieses Mädchen ist für alle", der Adaption eines Tennessee Williams-Dramas, erneut zusammen vor der Kamera. Mit diesem Film begann Redford seine jahrzehntelange Zusammenarbeit mit Regisseur Sydney Pollack. Mit Natalie Wood blieb er bis zu deren Tod im Jahr 1981 eng befreundet. Keiner der beiden Filme konnte sich an der Kasse durchsetzen.

Nachdem Redfords erste Filme gefloppt hatten und er sich zunächst nicht als Filmschauspieler hatte etablieren können, gelang ihm 1967 mit der Adaption des Theaterstücks "Barfuß im Park" der Durchbruch zum Kino-Star. In dieser populären Neil-Simon-Komödie hatte der Darsteller bereits als Broadway-Schauspieler einen großen Erfolg gefeiert. In der Hollywood-Adaption war der 31-jährige Darsteller an der Seite von Jane Fonda zu sehen und konnte sich durch seinen Auftritt als spießiger Anwalt, der „barfuß im Park“ spazieren geht, in Hollywood etablieren.

Redford war 1967 auch im Gespräch für die Hauptrolle in "Die Reifeprüfung". Regisseur Mike Nichols glaubte jedoch, dass er der Falsche sei, um einen verklemmten jungen Mann darzustellen, der Probleme mit dem anderen Geschlecht hat. In dieser Rolle gelang schließlich Dustin Hoffman der Durchbruch. 1969 drehte Redford drei Filme, von denen "Blutige Spur" und "Schußfahrt", zwei inhaltlich ambitionierte Dramen, an den Kinokassen floppten. Sein Auftritt in der Westernkomödie "Zwei Banditen" jedoch machte Redford endgültig zu einem der führenden Hollywood-Stars. Seine Gage war mit diesem Film auf 150.000 Dollar gestiegen.

Unter der Regie von George Roy Hill verkörperte er in "Zwei Banditen" den schweigsamen Revolverhelden Sundance Kid an der Seite von Paul Newman, der die eigentliche Hauptrolle des Bandenchefs Butch Cassidy spielte. Zunächst war ein etablierter Star wie Steve McQueen, Warren Beatty oder Jack Lemmon für die Rolle des Sundance Kid vorgesehen. "Butch Cassidy und Sundance Kid" traf den liberalen Zeitgeist der späten 1960er Jahre und zeigte die beiden Zug- und Bankräuber Butch und Sundance als sympathische, fast hippiehafte Antihelden, die schließlich von einem riesigen Polizeiaufgebot zusammengeschossen werden. Dieser Film avancierte zu einem der größten Kassenerfolge der späten 1960er Jahre und verfestigte bei einem weltweiten Publikum Redfords Image als Frauenschwarm und romantischer Held. Mit Paul Newman blieb er zeitlebens eng befreundet.

1973 drehten die beiden Darsteller, wieder unter der Regie von George Roy Hill, die Gaunerkomödie "Der Clou", die zu einem der erfolgreichsten Filme der 1970er Jahre avancierte. Redford erhielt für die Darstellung des Trickbetrügers Hooker seine bislang einzige Oscar-Nominierung als Schauspieler. Redford und Newman wollten nach "Der Clou" ihre erfolgreiche Zusammenarbeit fortsetzen, fanden aber kein geeignetes Projekt mehr.

1972 spielte Robert Redford in "Bill McKay – Der Kandidat" einen idealistischen jungen Anwalt, der zum Politiker wird. Ab den frühen 1970er Jahren arbeitete Redford verstärkt mit Regisseur Sydney Pollack zusammen, mit dem er mehrere große Kinoerfolge feiern konnte. In dem von Pollack produzierten Western "Jeremiah Johnson" (geschätztes Budget: 3,5 Mio. Dollar) war er 1972 als zivilisationsmüder Trapper zu sehen, der die Schönheit und Grausamkeit der Rocky Mountains kennenlernt. Der Film, der unter anderem auf Redfords Anwesen in Utah gedreht wurde, war kein Kassenschlager, avancierte aber zu einem Klassiker seines Genres. 1973 konnten Pollack, Redford und Barbra Streisand mit dem epischen Liebesfilm "So wie wir waren" Kasse machen. Auch Pollacks Politthriller "Die drei Tage des Condor" von 1975 wurde zum Kassenhit.

"Der große Gatsby" (1974, Regie: Jack Clayton) und "Tollkühne Flieger" (1975, Regie: George Roy Hill) konnten die großen Erwartungen in kommerzieller Hinsicht nicht erfüllen. Trotzdem war Redford nach mehreren Top-Hits in kurzer Folge auf dem Höhepunkt seiner Popularität und zwischen 1973 und 1976 der kassenträchtigste Filmstar. Der rotblonde, blauäugige Schauspieler mit dem strahlenden Lächeln galt weltweit als Sexsymbol. 1976 betätigte er sich bei "Die Unbestechlichen" erstmals als Filmproduzent. Der Politthriller beschreibt die Arbeit der Zeitungsreporter Bob Woodward (Redford) und Carl Bernstein (Dustin Hoffman), die in den frühen 1970er Jahren den Watergate-Skandal aufdeckten. Der von Alan J. Pakula inszenierte Film war bei Kritik und Publikum ein großer Erfolg.

Redford spielte 1977 in dem mit Stars besetzten Kriegsfilm "Die Brücke von Arnheim" und trat danach zwei Jahre lang nicht mehr vor eine Filmkamera. 1979 inszenierte Sydney Pollack mit ihm "Der elektrische Reiter", in dem er als abgewrackter Rodeoreiter ein wertvolles Pferd aus Las Vegas entführt. 1980 debütierte Redford als Filmregisseur und inszenierte das Sozialdrama "Eine ganz normale Familie", in dem er allerdings nicht als Schauspieler auftrat. Für sein Regiedebüt erhielt Redford seinen ersten Oscar.

1980 gründete Robert Redford in seiner Heimat Utah das Sundance Institut, benannt nach seiner Rolle in "Butch Cassidy und Sundance Kid". Ziel des Instituts ist die Förderung unabhängiger Filmemacher und ihrer Werke. Seit 1984 findet das jährliche Sundance Film Festival statt, das im Lauf der Jahre zum wichtigsten Treffpunkt der amerikanischen "Independent-Regisseure" (Filmemacher, die nicht den Massengeschmack bedienen wollen, sondern an künstlerisch anspruchsvollen Filmen interessiert sind) wurde.

In den 1980er Jahren trat Redford nur in vier Filmen auf. In "Brubaker" spielte er 1980 einen idealistischen Gefängnisdirektor. 1984 war er in "Der Unbeugsame" als Baseballspieler zu sehen. 1985 konnte er, erneut unter der Regie von Sydney Pollack, in dem epischen Liebesfilm "Jenseits von Afrika" an der Seite von Meryl Streep einen großen Erfolg verbuchen. 1986 war er in dem romantischen Film "Staatsanwälte küßt man nicht" zu sehen. 1990 kam es zur sechsten und letzten Zusammenarbeit mit Pollack. Doch "Havanna", eine Liebesgeschichte vor dem Hintergrund der kubanischen Revolution, war kommerziell ein Flop.

Auch in den 1990er Jahren trat Redford nur sporadisch vor die Kamera und spielte lediglich in fünf Filmen mit. In "Sneakers" (1992) war er der Chef einer Bande von High-Tech-Einbruchsspezialisten. 1993 machte er Demi Moore das titelgebende "Unmoralische Angebot". 1996 spielte er als Fernsehjournalist mit Michelle Pfeiffer in "Aus nächster Nähe" und 1998 in der erfolgreichen Bestsellerverfilmung "Der Pferdeflüsterer" erstmals unter eigener Regie. Seit 2000 trat er unter anderem in den Filmen "Die letzte Festung" (2001) und in dem Spionagefilm "Spy Game" (2001) auf, in dem er als Lehrmeister von Brad Pitt zu sehen war.

Robert Redford hat bislang neun Spielfilme inszeniert: "Eine ganz normale Familie" (1980), "Milagro – Der Krieg im Bohnenfeld" (1988), "Aus der Mitte entspringt ein Fluß" (1992), "Quiz Show" (1994), "Der Pferdeflüsterer" (1998), "Die Legende von Bagger Vance" (2000), "Von Löwen und Lämmern" (2007), "Die Lincoln Verschwörung" (2010) und "The Company You Keep – Die Akte Grant" (2012).

2002 erhielt Redford einen Ehren-Oscar für sein Lebenswerk als Schauspieler, Regisseur und Produzent sowie als Gründer des Sundance Instituts.

Im Jahr 2013 beteiligte er sich als Darsteller von Fernsehspots der Umweltschutzorganisation Natural Resources Defense Council, in denen US-Präsident Barack Obama dazu aufgefordert wurde, Maßnahmen zur Reduktion von Treibhausgasemissionen zu ergreifen. Auch in seinem Blog bei der Huffington Post engagiert er sich für Maßnahmen gegen die globale Erwärmung.

Redford kaufte 1969 ein Tal am Fuße des Timpanogos Bergmassivs in den Rocky Mountains. Dort betreibt er eine Luxusherberge namens Sundance Resort.

Redford sprach sich 2013 gegen die kontroverse Versteigerung heiliger Objekte der Hopi aus. Er bezeichnete die Versteigerung, die die Hopi und die Menschenrechtsorganisation Survival International versucht hatten zu verhindern, als „Sakrileg“ und als „kriminelle Handlung“.

Seit 1969 wurde Robert Redford in fast all seinen Filmen von Rolf Schult (1927–2013) synchronisiert. In den 1970er Jahren war auch Christian Brückner als deutscher Sprecher des Schauspielers tätig. Obwohl sich Rolf Schult aus Altersgründen von der Synchronarbeit weitgehend zurückgezogen hatte, war er noch bis in die 2000er Jahre als Sprecher von Redford tätig (ab 1979 exklusiv).

Im Jahr 2007 hielt ein US-amerikanischer Supervisor, nachdem Schult den Trailer zu Redfords neuestem Film bereits synchronisiert hatte, dessen Stimme für zu tief, sodass Rolf Schult in "Von Löwen und Lämmern" durch Kaspar Eichel ersetzt wurde. Schult ist daher nur noch im ersten Trailer zu hören.
In seinem Film The Company You Keep – Die Akte Grant aus dem Jahr 2013 wird Redford von Jürgen Heinrich synchronisiert; 2014 und 2015 wurde er erneut von Kaspar Eichel in den Filmen The Return of the First Avenger und Picknick mit Bären gesprochen, auch 2016 und 2017 lieh er ihm erneut seine Stimme und ist jetzt wohl als der neue Stammsprecher Redfords anzusehen.

Als Schauspieler
Als Sprecher

Als Produzent

Als Regisseur






</doc>
<doc id="4223" url="https://de.wikipedia.org/wiki?curid=4223" title="Robert De Niro">
Robert De Niro

Robert Anthony De Niro, Jr. (* 17. August 1943 in New York City, New York) ist ein US-amerikanischer Schauspieler, zweifacher Oscarpreisträger, Filmregisseur und Produzent. Er zählt seit Mitte der 1970er Jahre zu den führenden Charakterdarstellern des US-amerikanischen Films und ist in zahlreichen Filmklassikern zu sehen.

Robert De Niro wuchs als Einzelkind einer Künstlerfamilie auf. Sein Vater, Robert De Niro, Sr. (1922–1993), zählt heute zu den führenden abstrakt-expressionistischen Malern der USA. Seine Bilder wurden in zahlreichen Museen und Galerien ausgestellt. Er war mit Schriftstellern wie Henry Miller und Tennessee Williams befreundet und starb mit 71 Jahren an Krebs. Die Mutter Virginia Admiral (1915–2000) war Poetin und Malerin und arbeitete auch als Typistin für Anaïs Nin. Ihre Werke werden unter anderem im Metropolitan Museum of Art ausgestellt.

Als sich seine Eltern 1945 scheiden ließen, lebte De Niro bei seiner Mutter im New Yorker Stadtteil Little Italy in Manhattan. Er verbrachte aber auch viel Zeit mit seinem Vater, der nicht weit entfernt wohnte und nach der Scheidung homosexuelle Beziehungen unterhielt.

Seine spätere Mentorin Shelley Winters erzählte, dass De Niro in Armut aufgewachsen sei, da seine Eltern mit ihrer Kunst kaum Einnahmen erzielt hätten. Die Mutter betrieb unter anderem eine kleine Druckerei. Robert, der wegen seiner Blässe „Bobby Milk“ genannt wurde und als scheu galt, stand mit zehn Jahren erstmals auf einer Bühne – als ängstlicher Löwe in einer Schulaufführung von Der Zauberer von Oz. „Ich wollte schon mit zehn Jahren Schauspieler werden“, erzählte er später.

Er verließ mit 16 Jahren die Schule und strebte mit dem Segen seiner Eltern, die seine künstlerischen Ambitionen unterstützten, eine Karriere als Schauspieler an. Bei einer High-School-Tournee mit Tschechows Stück "Der Bär," die ihn durch New England und New York führte, erhielt er als 16-Jähriger seine erste Schauspielergage.

In seiner Jugend bewunderte De Niro besonders die Schauspieler Montgomery Clift, Robert Mitchum und Marlon Brando. Mit Mitchum und Brando arbeitete er später auch zusammen, Clift starb bereits 1966.

Er tourte als Schauspieler durch die Südstaaten und spielte größere Rollen in Stücken wie "Cyrano de Bergerac" oder "Eines langen Tages Reise in die Nacht". Er trat mit Komödien in Dinner-Theatern auf und nahm ab 1960 professionellen Schauspielunterricht, unter anderem bei Schauspiellehrerin Stella Adler, die im Laufe ihrer Karriere bekannte Darsteller wie Marlon Brando, Harvey Keitel, Martin Sheen, Warren Beatty oder Christoph Waltz unterrichtete.

1963 debütierte De Niro in "The Wedding Party" in einer Nebenrolle als Filmschauspieler. Die mit nur 43.000 Dollar budgetierte Komödie markierte das Filmdebüt des späteren Star-Regisseurs Brian De Palma und orientierte sich formal an den Stilmitteln der französischen Nouvelle Vague. "The Wedding Party" kam erst 1969 in die Kinos und fiel bei Kritik und Publikum gleichermaßen durch.

Nachdem er in zwei weiteren Filmen kleine Rollen gespielt hatte, trat de Niro 1968 in De Palmas zweitem Spielfilm "Greetings" auf, einem zeittypischen Werk, das in satirischer Weise mit Elementen des Underground-Films um Themen wie Sex, Gewalt und Vietnam kreist. "Greetings" erhielt wegen seiner freizügigen Szenen ein X-Rating. Auch dieser Film fiel an der Kinokasse durch.

1968 trat De Niro in dem Off-Broadway-Stück "Glamour, Glory and Gold" auf und wurde von der Theaterkritik, die dem Stück ansonsten skeptisch gegenüberstand, positiv erwähnt. Dem 25-Jährigen wurde großes Talent bescheinigt. Seine Schauspielpartnerin Sally Kirkland erklärte, sie habe nie eine derart brillante Darstellung gesehen.

Um das Jahr 1970 herum wurde die bekannte Charakterdarstellerin und Oscar-Preisträgerin Shelley Winters auf De Niro aufmerksam und besetzte ihn als ihren Schauspielpartner in dem Stück "One Night Stands of a Noisy Passenger", das nach nur sieben Vorstellungen abgesetzt wurde. Da er in dem Stück ein Bett mit einem Karateschlag zerstören musste, trainierte De Niro monatelang diese Kampfsportart. Obwohl auch dieses Theaterstück bei der Kritik auf wenig Gegenliebe stieß, wurde De Niros Darstellung erneut gelobt.

Shelley Winters verglich Robert De Niro mit dem jungen Brando der 1940er Jahre und schlug ihren neuen Protegé dem Regisseur und Produzenten Roger Corman vor, der 1970 den Low-Budget-Gangsterfilm "Bloody Mama" vorbereitete. Hier spielte Winters die Rolle der berüchtigten Ma Barker, die zusammen mit ihren vier Söhnen in der ersten Hälfte der 1930er Jahre zahlreiche Verbrechen verübt hatte. De Niro trat in der Rolle des Lloyd Barker auf und fühlte sich schon damals in extremer Weise in seine Rolle ein. Später sagte er dazu: 

1970 arbeitete er erneut mit Regisseur De Palma zusammen, der ihm die Hauptrolle in seinem Film "Hi, Mom!" übertrug. In dem komödiantischen Low-Budget-Streifen war De Niro als Vietnam-Veteran zu sehen, der die Bewohner eines gegenüberliegenden Appartementblocks beobachtet. In den frühen 1970er Jahren spielte der Darsteller weitere Rollen in Filmen wie "Wo Gangster um die Ecke knallen" (1971) oder "Pforte zur Hölle" (1971), von denen aber keiner seine Karriere voranbrachte. Für eine Rolle in "Der Pate" (1972) sprach er vergeblich vor. 1973 trat er zum vorläufig letzten Mal im Theater auf.

Das Jahr 1973 wurde für den 30-jährigen Darsteller zum entscheidenden Wendepunkt. In "Das letzte Spiel" stellte er den sterbenden Baseballspieler "Bruce Person" dar und gewann den „New York Film Critics Award“ als bester Schauspieler. Er traf Martin Scorsese, der wie er in Little Italy aufgewachsen war und seit den späten 1960er Jahren als Filmregisseur arbeitete. Scorsese gab De Niro die wichtige Rolle des neurotischen Johnny Boy in seinem Film "Hexenkessel," einem biographisch geprägten Porträt einiger New Yorker Kleinganoven, die sich im Italienerviertel durchschlagen.

Der Film wurde von der Kritik nahezu einhellig gelobt, war an den Kinokassen erfolgreich (3 Millionen Dollar Einspielergebnis bei einem Budget von 500.000 Dollar) und begründete die Jahrzehnte dauernden Karrieren von Scorsese, De Niro sowie Harvey Keitel, der hier noch die Hauptrolle spielte. Ab 1975 avancierte De Niro zu Scorseses bevorzugtem Darsteller; sie arbeiteten von 1973 bis 1995 acht Mal zusammen.

Nachdem De Niro in "Hexenkessel" überzeugt hatte, gab ihm Regisseur Francis Ford Coppola die begehrte Rolle des „Paten“ Vito Corleone in "Der Pate – Teil 2" (1974). De Niro übernahm hier den Part, den Marlon Brando zwei Jahre zuvor im ersten Teil des Welterfolgs gespielt hatte, stellte den Mafia-Chef aber als jungen Mann am Anfang seiner „Karriere“ dar. Auf der zweiten Zeitebene des epischen Films war Al Pacino erneut als Vitos Sohn Michael Corleone zu sehen. De Niro hatte sich akribisch darauf vorbereitet, die speziellen Manierismen der berühmten von Brando gespielten Figur zu übernehmen.

"Der Pate – Teil 2" war ein weltweiter Erfolg bei Kritik und Publikum und etablierte De Niro endgültig als neuen Star. Nahezu einhellig wurde ihm bescheinigt, er habe die schwierige Herausforderung als Brandos Nachfolger bravourös gemeistert. 1975 war De Niro als Nebendarsteller erstmals für einen Oscar nominiert und gewann den Preis für seine Darstellung des Vito Corleone. Er erhielt den Preis damit für die Darstellung derselben Figur, für deren Interpretation bereits Marlon Brando (im ersten Teil) einen Oscar erhalten hatte. "Der Pate – Teil 2" war elf Mal nominiert und gewann sechs Preise, darunter auch den in der Kategorie Bester Film des Jahres. De Niro war bislang sechs Mal für den Oscar nominiert und gewann ihn zwei Mal. Seinen Oscar für "Der Pate – Teil 2" konnte er nicht persönlich entgegennehmen, weil er in Italien den Film "1900" drehte.

In Bernardo Bertoluccis großangelegtem Epos "1900", das 1976 in die Kinos kam, stellte De Niro einen italienischen Großgrundbesitzer dar und sorgte wegen einer expliziten Sexszene für Aufsehen (er wird von einer Prostituierten mit der Hand befriedigt). Der von US-Studios finanzierte Fünf-Stunden-Film konnte seine enormen Kosten (20 Millionen Mark) nicht einspielen, obwohl er mehrfach gekürzt und umgeschnitten wurde. Auch De Niro, der mit Bertoluccis Regiestil und seinem französischen Co-Star Gérard Depardieu unzufrieden war, hatte nach allgemeinem Tenor schauspielerisch nicht voll überzeugt.

1976 spielte er dann die Rolle, die ihn endgültig als einen der führenden Charakterdarsteller etablierte. Unter der Regie von Scorsese war er in der titelgebenden Rolle als "Taxi Driver" Travis Bickle zu sehen. Der entwurzelte Vietnam-Veteran („Gottes einsamster Mann“) arbeitet in New York als Taxifahrer und ist angewidert von dem menschlichen Abschaum, mit dem er in den nächtlichen Straßenschluchten konfrontiert wird. Er stilisiert sich zum Beschützer einer jugendlichen Prostituierten (Jodie Foster) und richtet ein Blutbad an.

Mit "Taxi Driver" gelang Scorsese ein durchschlagender Erfolg bei Kritik und Publikum (28 Millionen Dollar in den USA bei einem Budget von 1,3 Millionen Dollar). Das düstere Drama avancierte zu einem der großen Filmklassiker der 1970er Jahre und erhielt 1977 vier Oscar-Nominierungen, darunter eine für den Hauptdarsteller De Niro. Einige improvisierte Dialogzeilen De Niros („You talkin' to me?“) gingen in die Umgangssprache ein. Der Schauspieler hatte sich wie immer mit großem Engagement auf die Rolle vorbereitet und war unter anderem selbst in New York Taxi gefahren.

Nach diesem Erfolg spielte De Niro in zwei Filmen, die bei Kritik und Publikum durchfielen. In Elia Kazans "Der letzte Tycoon" (1976) stellte er einen Filmproduzenten der 1930er Jahre dar (angelehnt an den früh verstorbenen MGM-Chef Irving Thalberg). Trotz eindrucksvoller Starbesetzung (Jack Nicholson, Tony Curtis, Robert Mitchum, Jeanne Moreau) wurde der Film zu einem Flop; auch De Niros Darstellung wurde nicht als herausragend bewertet. "Der letzte Tycoon" markierte für den 67-jährigen Kazan das Ende einer mehr als 30-jährigen Regiekarriere.

1977 drehte Martin Scorsese mit großem Budget und aufwendigen Kulissen das nostalgische Musical "New York, New York," das De Niro im Jahr 1945 als Saxophonspieler zeigt, der sich in eine Sängerin (Liza Minnelli) verliebt. Der Film, für den De Niro das Saxophonspielen erlernte, konnte die Erwartungen nicht erfüllen und wurde zurückhaltend aufgenommen.

Sein nächster Film sah ihn in der Rolle eines russischstämmigen Stahlarbeiters, der zusammen mit seinen Freunden nach Vietnam eingezogen wird. Michael Ciminos epischer Kriegsfilm "Die durch die Hölle gehen" (1978) porträtiert zunächst ausführlich das Leben der Stahlarbeiter und blendet dann in einem abrupten Schnitt auf das barbarische Geschehen in Vietnam über, das die Protagonisten psychisch schwer zeichnet. Mit Darstellern wie John Cazale, Christopher Walken oder Meryl Streep war der Film hochkarätig besetzt. De Niro führte für den Film zahlreiche Stunts selbst aus – er ließ sich unter anderem 15-mal aus 10 Metern Höhe in einen Fluss fallen und hängte sich an die Kufen eines Hubschraubers, der hoch in die Luft stieg. Ciminos Film wurde zu einem kommerziellen und künstlerischen Erfolg, rief aber Kontroversen hervor. Als er bei der Berlinale gezeigt wurde, verließen die Delegationen der kommunistischen Länder das Festival, weil sie die nordvietnamesischen Soldaten durch den Film verunglimpft sahen. Mit fünf Auszeichnungen, darunter eine für den besten Film, war Ciminos Vietnam-Epos der große Gewinner der Oscar-Verleihung des Jahres 1979.

1980 spielte Robert De Niro in "Wie ein wilder Stier" seine nach allgemeinem Tenor bedeutendste Rolle. Das von Martin Scorsese in Schwarzweiß inszenierte Boxer-Drama zeichnet das Leben und die Karriere von Jake LaMotta (1921–2017) nach, der 1949 Weltmeister im Mittelgewicht wurde. Der Regisseur und sein Hauptdarsteller zeigen hier konsequent das Porträt eines unsympathischen Schlägers, mit dessen Leben es immer weiter bergab geht. LaMotta verkommt nach seiner Boxkarriere zu einem aufgeschwemmten Entertainer, der in Nachtclubs schlechte Witze erzählt.

Um sich auf seine Rolle vorzubereiten, absolvierte De Niro ein einjähriges Box-Training und wurde dabei unter anderem von Jake LaMotta persönlich angeleitet, der De Niro bescheinigte, er habe das Zeug zum Profikämpfer. Der Darsteller absolvierte drei Boxkämpfe und gewann davon zwei. Als die aufwendig choreographierten Kampfszenen abgedreht waren (die später von der Kritik für ihre inszenatorische, darstellerische und schnitttechnische Brillanz besonders gelobt wurden), aß sich De Niro innerhalb kurzer Zeit 30 Kilo Übergewicht an, um den späten, fetten Jake LaMotta adäquat spielen zu können. Der Film beginnt und endet mit Szenen, in denen LaMottas traurige Existenz als drittklassiger Nachtclub-Entertainer gezeigt wird. LaMotta verklagte die Produktionsgesellschaft United Artists wegen der Art und Weise, wie er in dem Film porträtiert wird.

Scorseses düsteres Boxerdrama war kein großer Kassenerfolg, avancierte aber schnell zum Klassiker, der vielfach als eines der besten Werke der Filmgeschichte bezeichnet wird. De Niro hatte sich mit einer bis dahin beispiellosen Intensität auf seine Rolle vorbereitet und hielt jahrelang den Rekord für die stärkste Gewichtszunahme, der sich ein Schauspieler für eine Rolle unterzogen hatte. 1987 wurde er von Vincent D’Onofrio geschlagen, der sich für "Full Metal Jacket" 35 Kilo anfraß. De Niro wurde 1981 mit einem Oscar für die beste Hauptrolle ausgezeichnet (er setzte sich gegen Robert Duvall, Jack Lemmon, Peter O’Toole und John Hurt durch). "Wie ein wilder Stier" war auch als bester Film des Jahres nominiert, verlor aber gegen Robert Redfords Regiedebüt "Eine ganz normale Familie," was häufig als Fehlentscheidung gewertet wurde. De Niro schloss mit "Wie ein wilder Stier" seine in künstlerischer Hinsicht wohl bedeutendste Phase als Schauspieler mit einem Triumph ab und befand sich auf dem Höhepunkt seiner Karriere.

Martin Scorsese und Robert De Niro konnten zunächst nicht mehr an ihre Erfolge anknüpfen. 1981 spielte De Niro (der hier noch deutlich übergewichtig aussah) in Ulu Grosbards "Fesseln der Macht" einen Priester in Gewissensnöten, der in den 1940er Jahren indirekt in einen Mordfall verwickelt wird. Der eher ruhige und unspektakuläre, mit Robert Duvall, Charles Durning und Cyril Cusack hoch besetzte Film erhielt gemischte Kritiken und wurde in Deutschland nie im Kino gezeigt.

Mit "King of Comedy" setzten De Niro und Scorsese 1983 ihre Zusammenarbeit fort. De Niro agierte in der Rolle des erfolglosen Stand-Up-Comedians Rupert Pupkin, der einen berühmten TV-Entertainer (Jerry Lewis) entführt. Von der Kritik zwar gelobt, wurde die pessimistische Tragikomödie zu einem katastrophalen Flop an der Kinokasse (2,5 Millionen Dollar in den USA bei einem Budget von 20 Millionen Dollar) und einem Rückschlag in Scorseses Karriere. In den 1980er Jahren war De Niro auch für die Rolle des "Jesus Christus" in Scorseses Film "Die letzte Versuchung Christi" vorgesehen. Das Projekt wurde mehrfach verschoben und 1988 schließlich mit Willem Dafoe realisiert.

Auch Sergio Leones fast vierstündige Gangster-Saga "Es war einmal in Amerika" wurde 1984 zu einem Flop. Der Film erzählt auf drei Zeitebenen (1922/1932/1968) das Leben des jüdischen Gangsters "Noodles" (De Niro), der während der Prohibition an der Seite seines Freundes "Max" (James Woods) Karriere macht, diesen dann aber an die Polizei verrät. Das aufwendige Epos (Budget: 30 Millionen Dollar) fand mit seiner komplexen Erzählstruktur im Kino kaum Publikum und wurde vergeblich umgeschnitten und gekürzt.

"Es war einmal in Amerika" wurde von der Kritik rehabilitiert und gilt seit langem als großer Klassiker der 1980er Jahre. Sergio Leone konnte allerdings nie die von ihm geplante Schnittfassung des Films erstellen, er starb 1989. Unter Scorseses Leitung wurde der Film restauriert und 2012 bei den Filmfestspielen von Cannes in einer um 25 Minuten längeren Fassung präsentiert.

Mit "Der Liebe verfallen" (1984), einer eher konventionell angelegten Liebesgeschichte, hatte De Niro an der Seite von Meryl Streep moderaten kommerziellen Erfolg. 1985 war er in Terry Gilliams "Brazil" in der zentralen, wenn auch kurzen Rolle des "Harry Tuttle" zu sehen, der in einem kafkaesken Überwachungsstaat als Terrorist gesucht wird. Der bizarre und düstere Streifen fand zwar kein großes Publikum, gilt aber allgemein als Meisterwerk und Kultfilm.

In Roland Joffés epischem Historienfilm "The Mission" (1986) war De Niro (langhaarig und mit Vollbart) als Missionar in Südamerika zu sehen. Der mit der Goldenen Palme von Cannes ausgezeichnete Streifen erhielt gemischte Kritiken und konnte sich im Kino nicht durchsetzen. 1987 spielte De Niro (erneut langhaarig und mit Vollbart) in Alan Parkers Horrorthriller "Angel Heart" die zentrale Nebenrolle des ominösen "Louis Cyphre," der in den 1950er Jahren einen heruntergekommenen Privatdetektiv (Mickey Rourke) engagiert. Kritiker merkten an, De Niro habe bei seiner Darstellung des dämonischen Cyphre überzogen. Auch "Angel Heart" wurde kein Kassenerfolg, gilt aber als Genreklassiker.

1987 war De Niro nach Jahren wieder in einem großen Kassenhit zu sehen. Unter der Regie von Brian De Palma, für den er bereits in den 1960er Jahren gespielt hatte, stellte er in dem großangelegten Gangsterepos "The Untouchables – Die Unbestechlichen" Al Capone dar. Kevin Costner (in der Hauptrolle des FBI-Agenten "Eliot Ness)" und Sean Connery (Oscar-gekrönt als erfahrener Polizist) agierten als dessen Widersacher. Um den legendären Mafia-Boss adäquat darstellen zu können, hatte De Niro an Gewicht zugelegt und sich eine Halbglatze rasieren lassen. Er trug außerdem die Sorte Unterwäsche, die früher Capone getragen hatte.

Martin Brests Actionkomödie "Midnight Run – Fünf Tage bis Mitternacht" (1988) zählt zum Genre der damals besonders beliebten „Buddy Movies“ und zeigt De Niro in der Rolle eines heruntergekommenen Kopfgeldjägers, der einen Mafia-Buchhalter (Charles Grodin) quer durch die USA befördern muss. Der Film fand bei der Kritik zwar positive Resonanz, war aber weniger erfolgreich als andere „Buddy Movies“ dieser Zeit (wie zum Beispiel "Lethal Weapon – Zwei stahlharte Profis)".

Für De Niro endeten die 1980er Jahre mit drei Filmen, die an der Kinokasse durchfielen. In "Stanley und Iris" (1989) agierte er als analphabetischer Koch, der sich in eine Kollegin (Jane Fonda) verliebt, in "Jacknife" (1989) als traumatisierter Vietnam-Veteran. Auch die Komödie "Wir sind keine Engel" (1989), in der er und Sean Penn als Gefängnisausbrecher, die sich als Priester verkleiden, zu sehen sind, floppte.

Ab den frühen 1990er Jahren konnte De Niro seine Filmkarriere mit einigen Erfolgen wieder stabilisieren. In Scorseses epischem Gangsterdrama "Good Fellas – Drei Jahrzehnte in der Mafia" agierte er in der zentralen Nebenrolle des „Mobsters“ "Jimmy Conway", der zum Mentor des Nachwuchsgangsters "Henry Hill" (Ray Liotta) wird. Der teils mit schonungsloser Brutalität inszenierte Film avancierte zum Kassenhit und Genreklassiker. Für die Darstellung eines Komapatienten in "Zeit des Erwachens" (1990) war De Niro für einen Oscar nominiert.

In dem eher unspektakulären Drama "Schuldig bei Verdacht" (1991) agierte er als Filmregisseur, der während der McCarthy-Ära unter Druck gerät. Der erfolgreiche Actionfilm "Backdraft – Männer, die durchs Feuer gehen" (1991) zeigte ihn in einer zentralen Nebenrolle als erfahrenen Brandermittler.

Mit einem weltweiten Einspielergebnis von über 180 Millionen Dollar wurde der Psychothriller "Kap der Angst" zum größten Erfolg für Martin Scorsese und Robert De Niro – und zu einem Kassenhit des Jahres 1991. Das freie Remake von "Ein Köder für die Bestie" aus dem Jahr 1962 war deutlich kommerzieller angelegt als frühere Werke des eingespielten Duos und zeigte De Niro als tätowierten Psychopathen "Max Cady", der mit manischer Energie einen Anwalt (Nick Nolte) und seine Familie terrorisiert. (De Niro hatte einen Zahnarzt beauftragt, sein Gebiss für die Cady-Rolle in einen möglichst schlimmen Zustand zu versetzen.) Die brutalen Gewaltdarstellungen des Films wurden kontrovers diskutiert. De Niro, der sich für die Rolle einen besonders drahtig-muskulösen Körper antrainiert hatte, erhielt für seine Darstellung des Max Cady 1992 eine Oscarnominierung.

Mit Streifen wie "Die Nacht von Soho", "Mistress – Die Geliebte von Hollywood" (beide 1992) oder "Sein Name ist Mad Dog" (1993) sorgte er für weniger Aufsehen. 1993 debütierte er mit "In den Straßen der Bronx" als Regisseur und spielte einen Busfahrer in der Bronx, der im Jahr 1960 verhindern will, dass sein Sohn eine Mafia-Karriere einschlägt. De Niro widmete den Film, der beim Publikum kaum Resonanz fand, seinem Vater, der im selben Jahr verstorben war.

Kenneth Branaghs Horrordrama "Mary Shelley’s Frankenstein" wurde 1994 zum Kassenhit, es zeigt De Niro als künstliche Kreatur aus dem Labor "Dr. Frankensteins" (Branagh). Die Kritik reagierte eher ablehnend auf den Film, in dem De Niro unter einer perfekt konstruierten Horrormaske quasi nicht zu erkennen war.

1995 drehte De Niro zwei fast dreistündige Gangsterepen, die zu Klassikern wurden. Unter der Regie von Scorsese agierte er in "Casino" als Manager eines Las-Vegas-Casinos. Der in den 1970er Jahren angesiedelte Film zeigt exemplarisch Aufstieg und Fall von "Sam Rothstein" (De Niro), der als perfektionistischer Kontrollfanatiker höchst effektiv ein Casino führt und seinen Auftraggebern (die Chicagoer Mafia) zu spektakulären Gewinnen verhilft. Als er sich unglücklich in eine schöne Edelprostituierte (Sharon Stone) verliebt und mit mächtigen Lokalpolitikern anlegt, beginnt sein Niedergang. Publikum und Kritik reagierten positiv auf den Film, der thematisch und durch seinen fiebrigen Inszenierungsstil sehr an "Good Fellas – Drei Jahrzehnte in der Mafia" erinnert.

Auch in Michael Manns epischem Gangsterfilm "Heat" (1995) agierte De Niro als Kontrollfanatiker auf der falschen Seite des Gesetzes. "Neil McCauley" (De Niro) ist als erfahrener Profigangster Anführer einer Gang, die in Los Angeles perfekt geplante Einbrüche und Banküberfälle durchführt. Die Bande gerät zunehmend unter Druck, als Polizei-Lieutenant "Vincent Hanna" gegen sie ermittelt. Regisseur Mann sicherte seinem Film große Aufmerksamkeit, indem es ihm gelang, neben De Niro den nicht minder profilierten Al Pacino für die Rolle des "Vincent Hanna" zu engagieren. In der wohl bekanntesten Szene des Films führen die beiden legendären Charakterdarsteller in einem Coffee Shop ein längeres Gespräch über ihr Privatleben und ihre Profession. "Heat" spielte weltweit 174 Millionen Dollar ein, "Casino" 116 Millionen. 1995 wurde damit für de Niro zum bis dahin erfolgreichsten Jahr. Bis 1998 verteuerte sich seine Gage auf 14 Millionen Dollar.

Mit Barry Levinsons Drama "Sleepers," das in den 1960er Jahren angesiedelt ist und um Themen wie Jugendkriminalität und sexuellen Missbrauch kreist, konnte De Niro 1996 erneut einen großen Kassenerfolg verbuchen (Einspielergebnis: 166 Millionen Dollar). Er agierte darin als engagierter Pfarrer und Teil eines profilierten Ensembles (Kevin Bacon, Brad Pitt, Dustin Hoffman, Vittorio Gassman).

Als deutlich weniger erfolgreich erwies sich Tony Scotts Psychothriller "Der Fan" (1996), in dem er einen psychopathischen Baseball-Fan darstellte. Die Filmkritik bezeichnete Scotts Inszenierungsstil überwiegend als zu glatt und oberflächlich. Das Drama "Marvins Töchter" (1996) floppte trotz prominenter Besetzung (Meryl Streep, Leonardo DiCaprio, Diane Keaton); De Niro trat in einer Nebenrolle als Arzt auf.

Bis in die späten 1990er Jahre war De Niro in mehreren Filmen zu sehen, die bescheidenen Erfolg hatten. Er spielte neben Sylvester Stallone und Harvey Keitel in Cop Land (1997), einem Drama, das im Polizei-Milieu angesiedelt ist, und agierte im selben Jahr neben Dustin Hoffman in der Polit-Satire "Wag the Dog – Wenn der Schwanz mit dem Hund wedelt". De Niro spielte profilierte Nebenrollen in Quentin Tarantinos komödiantischem Gangsterfilm "Jackie Brown" (1997) und der Charles-Dickens-Adaption "Große Erwartungen (1998)", die den klassischen Stoff in die Gegenwart verlegte.

John Frankenheimers Actionthriller "Ronin" (1998) zeigte De Niro – mit Jean Reno als Co-Star – in der Hauptrolle des Söldners "Sam", der den Auftrag bekommt, zusammen mit einem Team von Spezialisten einen schwer bewachten Koffer zu erbeuten. Den Film, der in Frankreich spielt und im Kino überraschenderweise floppte, dominieren aufwendig inszenierte Verfolgungsjagden, die unter anderem mit dem ehemaligen Formel-1-Fahrer Jean-Pierre Jarier durchgeführt wurden.

1999 drehte er mit Billy Crystal die Mafia-Komödie "Reine Nervensache", die mit einem weltweiten Einspielergebnis von 141 Millionen Dollar ein Kassenschlager wurde. In seinen Komödien entstammt die Komik in der Regel dem Stilmittel der Parodie.

Noch erfolgreicher als "Reine Nervensache" war im Jahr 2000 "Meine Braut, ihr Vater und ich". Als knochenharter Ex-CIA-Agent Jack Byrnes hegt De Niro starke Vorbehalte gegen seinen zukünftigen Schwiegersohn. Mit einem weltweiten Einspielergebnis von 330 Millionen Dollar wurde die Komödie zu seinem erfolgreichsten Film. Er selbst avancierte im Alter von fast 60 Jahren zu einem der Topverdiener Hollywoods; er erhielt nun Gagen, wie sie für etablierte Actionhelden wie Harrison Ford oder Komödien-Stars wie Jim Carrey üblich waren (20 Millionen Dollar). In den Fortsetzungen "Reine Nervensache 2" (2002), "Meine Frau, ihre Schwiegereltern und ich" (2004) sowie "Meine Frau, unsere Kinder und ich" (2010) wurde das Erfolgsrezept mehrfach variiert.

Kein anderer Film De Niros konnte indessen an die Einspielergebnisse seiner Erfolgskomödien heranreichen, einige fielen bei Kritik und Publikum komplett durch. Filme wie "Makellos" (1999), "Die Abenteuer von Rocky & Bullwinkle" (2000), "15 Minuten Ruhm" (2001), "Showtime" (2002), "City by the Sea" (2002), "Godsend" (2004), "Inside Hollywood" (2008), "Kurzer Prozess – Righteous Kill" (2008), "Everybody’s Fine" (2009), "Stone" (2010) oder "Killer Elite" (2011) wurden allesamt Flops. Mit "The Score" (2001), in dem er neben Marlon Brando und Edward Norton als Meisterdieb auftrat, dem Militärdrama "Men of Honor" (2000), dem historischen Drama "Die Brücke von San Luis Rey" (2004) und dem Actionfilm "Machete" (2010) verbuchte er moderate Erfolge.

Zu einem großen Kinohit avancierte der Animationsfilm "Große Haie – Kleine Fische" (2004), in dem De Niro den Hai-Mafiaboss Don Lino sprach, der friedliche Meeresbewohner terrorisiert. Ein Kassenhit wurde auch der Horrorthriller "Hide and Seek – Du kannst dich nicht verstecken" (2005). De Niros zweite Regiearbeit, "Der gute Hirte", erhielt 2006 gemischte Kritiken. In epischer Breite wird die Karriere des Geheimdienstoffiziers Edward Wilson (Matt Damon) nachgezeichnet, der von den 1940er bis zu den 60er Jahren CIA-Operationen steuerte. Der Film konnte sein hohes Produktionsbudget von 90 Millionen Dollar nicht einspielen.

In dem erfolgreichen Fantasyfilm "Der Sternwanderer" trat De Niro 2007 in einer Nebenrolle auf. Zu einem Publikumserfolg wurde auch der Thriller "Ohne Limit" (2011), in dem er einen Geschäftsmann spielte (weltweites Einspielergebnis: 162 Millionen Dollar). In der erfolgreichen Liebeskomödie "Happy New Year" war er 2011 in einer Nebenrolle zu sehen. 2012 spielte De Niro in der US-amerikanisch-spanischen Mystery-Produktion "Red Lights" unter der Regie von Rodrigo Cortés an der Seite von Sigourney Weaver und Toby Jones einen (vorgeblichen) Parapsychologen, der es zu legendärer Berühmtheit gebracht hat.

De Niro lebt zurückgezogen, gilt als medienscheu und hält sich von der Glamourwelt Hollywoods fern. Sein Lebensmittelpunkt ist seit jeher New York. Er gab zu Beginn seiner Karriere noch Interviews, dann längere Zeit nicht mehr. Auch heute tut er es eher sporadisch und häufig fällt er dabei durch einsilbige Antworten auf. Auskünfte zu seiner Schauspieltechnik oder Rollenvorbereitung gibt er kaum.

1976 heiratete er die Schauspielerin Diahnne Abbott (* 1945), von der er sich 1988 scheiden ließ. Aus der Beziehung ging ein Sohn (* 1976) hervor. Abbotts Tochter Drena (* 1971) wurde von De Niro adoptiert, sie arbeitet ebenfalls als Schauspielerin. De Niro hatte eine langjährige Beziehung mit dem Model Toukie Smith, mit der er 1995 Zwillinge bekam. 1997 heiratete er Grace Hightower (* 1955). Aus dieser Verbindung, ging 1998 ein Sohn hervor. Im November 2004 heiratete er seine Frau – nach einer Scheidung, die nie rechtskräftig wurde – auf seiner Farm in den Catskill Mountains ein zweites Mal. Im Dezember 2011 wurde De Niro mit Hilfe einer Leihmutter erneut Vater.

Zu seinen besten Freunden zählt Joe Pesci, dessen Karriere er förderte und der seit "Wie ein wilder Stier" in mehreren Filmen sein Co-Star war. Auch mit seinem Kollegen Harvey Keitel ist De Niro befreundet, seit er mit ihm 1973 "Hexenkessel" drehte. Ein gutes Verhältnis pflegt er auch zu seinem Schauspielkollegen Al Pacino. Beide spielten oft die Rolle des Gangsters oder Polizisten und gelten als Inbegriff des italo-amerikanischen Charakterdarstellers. In ihrem ersten gemeinsamen Film, "Der Pate – Teil II," hatten sie keine gemeinsamen Szenen. Knapp 20 Jahre später standen sie für "Heat" wieder gemeinsam vor der Kamera; darin hatten sie zwei längere Sequenzen, in denen sie aufeinandertrafen. Erst in dem Film "Kurzer Prozess – Righteous Kill" (2008) waren De Niro und Pacino, die dort ein Polizistenduo spielen, permanent zusammen zu sehen.

1989 gründete De Niro zusammen mit Jane Rosenthal in New York die Produktionsfirma Tribeca Productions, von der seither überwiegend Filme produziert werden, an denen der Star als Schauspieler oder Regisseur beteiligt ist. Die erste Tribeca-Produktion war der Thriller "Kap der Angst", der deutlich kommerzieller ausgerichtet war als die meisten vorherigen Filme des Schauspielers.

Offenbar, um das Unternehmen finanziell zu stützen, orientiert sich De Niro seit den späten 1990er Jahren bei seiner Filmauswahl vor allem am Mainstream und spielt regelmäßig in eingängigen Krimis und Komödien. Seither dreht er außerdem sehr viel mehr Filme als früher. War er in den 1980er Jahren noch in 13 Filmen aufgetreten, waren es in den 1990er Jahren bereits 25 und in den 2000er Jahren 18. Allein für die Jahre 2010–2013 vermerkt die Internet Movie Database 18 Filme mit De Niro, die bereits produziert wurden oder sich in der Vorproduktion befinden (Stand: Juni 2012). De Niro ist damit schon seit Jahren der wohl produktivste Hollywood-Star überhaupt, wobei ein Großteil der Filme (siehe oben) bei Kritik und Publikum auf wenig Resonanz stößt und der Darsteller, wohl auch altersbedingt, zunehmend in Nebenrollen auftritt.

Nach allgemeiner Einschätzung reichen De Niros spätere Filme nicht an seine Klassiker wie "Taxi Driver" oder "Wie ein wilder Stier", mit denen er Filmgeschichte schrieb, heran. Dies zeigt sich auch darin, dass kaum einer von ihnen mit einem wichtigen Filmpreis ausgezeichnet wurde. Die Internet Movie Database listet (Stand: Juni 2012) unter den 250 am besten bewerteten Filmen acht Werke mit Robert De Niro auf: "Der Pate – Teil 2" (Platz 3), "Goodfellas" (16), "Taxi Driver" (47), "Es war einmal in Amerika" (80), "Wie ein wilder Stier" (90), "Heat" (122), "Die durch die Hölle gehen" (132) und "Casino" (164). Diese Filme sind zwischen 1974 und 1995 entstanden. De Niro spielte bislang in zwei Filmen, die mit einem Oscar für den besten Film ausgezeichnet wurden: "Der Pate – Teil 2", 1975, und "Die durch die Hölle gehen", 1979). Sein kommerziell erfolgreichster Film ist (Stand: 2012) "Meine Frau, ihre Schwiegereltern und ich" aus dem Jahr 2004, der weltweit 516 Millionen Dollar einspielte.

Gleichwohl wird De Niro weiterhin als zuverlässiger Charakterdarsteller mit starker Präsenz wahrgenommen. Er selbst räumte ein, dass er sich nicht mehr mit derselben Energie und Besessenheit auf seine Rollen vorbereite, wie er dies in seinen Anfangsjahren getan habe. 2013 war der Schauspieler – nach 21 Jahren – wieder für einen Oscar nominiert (Nebenrolle in "Silver Linings)". Bis 2013 wurde De Niro also zweimal mit dem bekannten Filmpreis ausgezeichnet und weitere fünf Mal nominiert.

Seit 2003 erhielt De Niro mehrere Auszeichnungen für sein Lebenswerk, darunter den renommierten Life Achievement Award vom American Film Institute. Bei der Golden-Globe-Verleihung 2011 wurde er mit dem Cecil B. DeMille Award für sein Lebenswerk geehrt. Darüber hinaus hat er mehrere Dutzend internationale Auszeichnungen erhalten. 2011 leitete De Niro die Wettbewerbsjury der 64. Filmfestspiele von Cannes, die den US-amerikanischen Beitrag "The Tree of Life" von Terrence Malick mit der Goldenen Palme auszeichnete.

De Niro ist als Unterstützer der Demokratischen Partei bekannt, er setzte sich mehrfach für deren Präsidentschaftskandidaten ein.

2011 unterstützte De Niro zusammen mit anderen renommierten Persönlichkeiten und Menschenrechtsverteidigern die Initiative Ein Logo für Menschenrechte.

Die britische Popband Bananarama hatte 1984 einen Hitparadenerfolg mit dem Song "Robert De Niro’s Waiting". Der Sänger Finley Quaye platzierte in seinem Song "Sunday Shining" die Textzeile „I’m a hero like Robert De Niro“. "Gun Love," ein Song von ZZ Top, enthält die Textzeile „Runnin’ with the Wild Bunch, makin’ like Robert De Niro“.

De Niro war mit dem Komiker John Belushi befreundet. Er besuchte ihn noch am frühen Morgen des 5. März 1982, dem Tag, an dem Belushi an einer Überdosis starb.

Glaubt man De Niros eigenen Worten, mag er sich seine Filme nicht ansehen („Ich schlafe dabei ein“).

1998 setzte sich De Niro gegen das Impeachment des Präsidenten Bill Clinton ein.

2006 war De Niro als Gast bei der Show "Hardball with Chris Matthews" eingeladen. Auf die Frage, wen er sich als Präsidenten vorstellen könnte, nannte er Hillary Clinton und Obama." Am 4. Februar 2008 trat er auf einer Veranstaltung Obamas in New Jersey auf.

2012 trat De Niro der Kampagne "Artists Against Fracking" bei.

Während des US-Präsidentschaftswahlkampfs 2016, äußerte sich De Niro deutlich und entschieden gegen den republikanischen Kandidaten Donald Trump, er nannte ihn „immens dumm“ und sagte, dass er ihm „ins Gesicht schlagen wolle“.

Seit "Der Pate – Teil 2" wird Robert De Niro vom gleichaltrigen Christian Brückner synchronisiert. In dem Film "Hexenkessel" (1973) wurde De Niro noch von Rolf Zacher gesprochen. In den 1980er Jahren wurde der Darsteller dann einige Male von Joachim Kerzel synchronisiert (u. a. "Angel Heart" oder "The Untouchables – Die Unbestechlichen)". Seit 1988 ist Brückner der einzige Sprecher De Niros, er übernahm auch die Neu-Synchronisation von "Es war einmal in Amerika", die 2003 für die DVD-Veröffentlichung des Films erstellt wurde. In der TV-Serie (1977) sprach Brückner nicht nur De Niro in der Rolle des jungen, sondern auch Marlon Brando in der Rolle des älteren Vito Corleone.















</doc>
<doc id="4224" url="https://de.wikipedia.org/wiki?curid=4224" title="Religion">
Religion

Religion (von ‚gewissenhafte Berücksichtigung‘, ‚Sorgfalt‘, zu ‚bedenken‘, ‚achtgeben‘, ursprünglich gemeint ist „die gewissenhafte Sorgfalt in der Beachtung von Vorzeichen und Vorschriften.“) ist ein Sammelbegriff für eine Vielzahl unterschiedlicher Weltanschauungen, deren Grundlage der jeweilige Glaube an bestimmte transzendente (überirdische, übernatürliche, übersinnliche) Kräfte ist, sowie häufig auch an heilige Objekte. Das Heilige und Transzendente ist nicht beweisbar im Sinne der Wissenschaftstheorie, sondern beruht auf intuitiven und individuellen Erfahrungen bestimmter Vermittler (Religionsstifter, Propheten, Schamanen). Deren spirituelle Erfahrungen werden in vielen Religionen als Offenbarung bezeichnet. Spiritualität und Religiosität sind geistig-geistliche Anschauungen. Skeptiker und Religionskritiker suchen demgegenüber nach rationalen Erklärungen.

Religion kann Wertvorstellungen normativ beeinflussen, menschliches Verhalten, Handeln, Denken und Fühlen prägen, und in diesem Zusammenhang eine Reihe von ökonomischen, politischen und psychologischen Funktionen erfüllen. Diese umfassenden Eigenschaften von Religion bergen in sich das Risiko der Bildung religiöser Ideologien.

Im deutschen Sprachraum wird der Begriff Religion zumeist sowohl für die "individuelle" Religiosität als auch für die "kollektive" Religionstradition verwendet. Obwohl beide Bereiche im menschlichen Denken eine enorme Vielfalt aufweisen, lassen sich einige universale Elemente formulieren, die in allen Kulturen der Welt anzutreffen sind. Zusammenfassend sind dies die individuellen Wünsche nach Sinnfindung, moralischer Orientierung und Welterklärung, sowie der kollektive Glaube an übernatürliche Mächte, die in irgendeiner Weise das Leben des Menschen beeinflussen; auch das Streben nach der Wiedervereinigung der diesseitigen Existenz mit seinem jenseitigen Ursprung. Diese Standarderklärungen werden jedoch zum Teil kritisiert.

Die weltweit größten Religionen (auch bekannt als Weltreligionen) sind Christentum, Islam, Hinduismus, Buddhismus, Daoismus, Sikhismus, Jüdische Religion, Bahaitum und Konfuzianismus (siehe auch: Liste von Religionen und Weltanschauungen). Die Anzahl und der Formenreichtum der historischen und gegenwärtigen Religionen übersteigt Anzahl und Formenreichtum der Weltreligionen bei weitem.

Vormoderne Kulturen hatten ausnahmslos eine Religion. Religiöse Weltanschauungen und Sinngebungssysteme stehen oft in langen Traditionen. Mehrere Religionen weisen verwandte Elemente auf, wie die Kommunikation mit transzendenten Wesen im Rahmen von Heilslehren, Symbolsystemen, Kulten und Ritualen oder bauen aufeinander auf, wie zum Beispiel Judentum und Christentum. Die Erstellung einer fundierten Systematik der Religionen, die aus den Verwandtschaftsbeziehungen zwischen den Religionen und ihrer Entstehungsgeschichte abgeleitet wird, ist eine noch nicht erfüllte Forderung der Religionswissenschaften.

Einige Religionen beruhen auf philosophischen Systemen im weitesten Sinne oder haben solche rezipiert. Andere sind stärker politisch, teils sogar theokratisch orientiert; wieder andere gründen in der Hauptsache auf spirituelle Aspekte. Überschneidungen finden sich in nahezu allen Religionen, und insbesondere bei deren Rezeption und Ausübung durch den einzelnen Menschen. Zahlreiche Religionen sind als Institutionen organisiert; dabei kann in vielen Fällen von einer "Religionsgemeinschaft" gesprochen werden.

Mit der wissenschaftlichen Erforschung von Religionen und (z. T.) Religiosität befassen sich besonders die Religionswissenschaft, Religionsgeschichte, Religionssoziologie, Religionsethnologie, Religionsphänomenologie, Religionspsychologie, Religionsphilosophie sowie in vielen Fällen Teilgebiete der jeweiligen Theologie. Konzepte, Institutionen und Erscheinungsformen von Religion werden durch Formen der Religionskritik punktuell oder grundsätzlich in Frage gestellt.

Das Adjektiv „religiös“ muss im jeweiligen Kontext gesehen werden: Es bezeichnet entweder „den Bezug zu (einer bestimmten) Religion“ oder „den Bezug zur "Religiosität" eines Menschen“.

Es gibt keine eindeutige Definition von Religion, sondern nur verschiedene Versuche der Definition. Grob lassen sich substantialistische und funktionalistische Ansätze unterscheiden. Substantialistische Definitionen versuchen, das Wesen der Religion etwa in ihrem Bezug zum Heiligen, Transzendenten oder Absoluten zu bestimmen; nach Rüdiger Vaas und Scott Atran etwa stellt der Bezug zum Transzendenten den zentralen Unterschied zum Nichtreligiösen dar.

Funktionalistische Religionsbegriffe versuchen Religion anhand ihrer gemeinschaftsstiftenden gesellschaftlichen Rolle zu bestimmen. Eine der berühmtesten und oft zitierten Definitionen von (christlicher) Religion stammt von Friedrich Schleiermacher und lautet: Religion ist „das Gefühl der schlechthinnigen Abhängigkeit von Gott“.

Eine substantialistische Definition etwa nach Gustav Mensching lautet: Nach dem Religionswissenschaftler Peter Antes werden mit Religion 

Das Wort "religio" hatte im Lateinischen verschiedene Bedeutungen, die von „Bedenken, Zweifel, Besorgnis, Gewissensskrupel“ über „Gewissenhaftigkeit, Religiosität, Gottesfurcht, Frömmigkeit, Gottesdienst“ bis zu „Heiligkeit (z. B. eines Ortes)“ und „Aberglaube“ reichten. Die Etymologie des Begriffs lässt sich nicht mit Sicherheit bis zu seinem Ursprung zurückverfolgen. "Religio" ist kein Terminus altrömischer Religion. Die frühesten Belege für die Verwendung dieses Ausdrucks finden sich in den Komödien des Plautus (ca. 250–184 v. Chr.) und in den politischen Reden des Cato (234–149 v. Chr.).

Nach Cicero (1. Jh. v. Chr.) geht "religio" auf "relegere" zurück, was wörtlich „wieder lesen, wieder auflesen, wieder zusammennehmen“, im übertragenen Sinn „bedenken, beachten“ bedeutet. Cicero dachte dabei an den Tempelkult, den es sorgsam zu beachten galt. Dieser "religio" (als der gewissenhaften Einhaltung überlieferter Regeln) stellte er die "superstitio" (nach der ursprünglichen Bedeutung Ekstase) als eine übertriebene Form von Spiritualität mit tagelangem Beten und Opfern gegenüber. Im Sinn einer „berufsmäßigen“ Gottesverehrung wurden entsprechend im Mittelalter Ordensleute als "religiosi" bezeichnet. Diese Bedeutung hat der Begriff bis heute im römisch-katholischen Kirchenrecht. Auch bei der Entlehnung ins Deutsche im 16. Jahrhundert wird "Religion" zunächst in diesem Sinne verwandt, nämlich zur Bezeichnung amtskirchlicher Bibelauslegung und Kultpraxis und ihrer Abgrenzung gegenüber sogenanntem Aberglauben (siehe Superstitio). Bis heute heißt die römische Kongregation für die Ordensleute „Religiosenkongregation“.

Zu Beginn des 4. Jahrhunderts führte der christliche Apologet Lactantius dagegen das Wort "religio" auf "religare" „zurück-, an-, festbinden“ zurück, wobei er sich polemisch mit Ciceros Auffassung über den Unterschied von "religio" und "superstitio" auseinandersetzte. Er meinte, es handle sich um ein „Band der Frömmigkeit“, das den Gläubigen an Gott binde. Diese Herkunft ist bei Sprachwissenschaftlern jedoch umstritten, da es keine vergleichbaren Wörter gibt, die aus einem Verb der lateinischen a-Konjugation entstammen, bei denen sich das Suffix "-are" ohne Anzeichen zu "-ion" entwickelt hat.

Im Mittelalter und in der frühen Neuzeit waren zur Bezeichnung der Gesamtheit des Religiösen die Ausdrücke "fides" („Glaube“), "lex" („Gesetz“) und "secta" (von "sequi" „folgen“, also „Gefolgschaft, Richtung, Partei“) gebräuchlich. "Religio" bezeichnete zunächst Lehren, die je nach Auffassung für richtig oder falsch gehalten wurden. Erst nach der Reformation, vor allem im Zeitalter der Aufklärung wurde ein abstrakterer Religionsbegriff geprägt, auf den die gegenwärtigen Definitionsansätze zurückgehen.

In den meisten außereuropäischen Sprachen fanden sich bis zum 19. Jahrhundert keine genauen Übersetzungen des Wortes "Religion". Häufig wurde das Phänomen mit mehreren Begriffen umschrieben. Eigene Begriffsprägungen erfolgten relativ spät. Dies trifft beispielsweise auf den Ausdruck Hinduismus zu, dessen Bedeutung zudem einem mehrmaligen Wandel unterlag.

Neuerdings hat der Religions- und Sprachwissenschaftler Axel Bergmann eine andere Etymologie vorgeschlagen. Demnach sei das Wort nicht mit dem Präfix "re-" („zurück, wieder“) gebildet, es gehe vielmehr auf das altlateinische "rem ligere" „eine Sache (oder ein Vorhaben) binden“, d. h. „mit (religiösen) Skrupeln betrachten“ und folglich „in Ehrfurcht scheuen“, zurück. Dieser Ausdruck der Alltagssprache wurde laut Bergmann zunächst speziell auf religiöse Skrupel bezogen und später auf den gesamten Bereich des Religiösen ausgedehnt.

Der österreichische Ethnologe, Kultur- und Sozialanthropologe Karl R. Wernhart hat die grundlegenden Strukturen des „Religiösen an sich“ "(Religious Beliefs per se)", die in sämtlichen Religionen und Kulturen – unabhängig von ihrem steten Wandel – übereinstimmen, wie folgt klassifiziert:


Antworten auf die metaphysischen „Kardinalfragen des Lebens“:
"(Kursiv = Wernhart zitiert die „Erklärung über das Verhältnis der Kirche zu den nichtchristlichen Religionen“ des 2. Vatikanischen Konzils von 1962–1965)"

Viele Wissenschaftler (etwa A. Giddens, L. A. Kirkpatrick, A. Newberg und E. d’Aquili) heben in Bezug auf die Orientierung den Aspekt der Sicherheit für den Einzelnen oder seine Nächsten besonders hervor. Nach ihrer Auffassung befriedigen Religionen das Bedürfnis nach Beistand und Stabilität bei existentiellen Ängsten; sie bieten Trost, Schutz und Sinnerklärung angesichts von Leiden, Krankheit, Tod, Armut, Elend und Ungerechtigkeit. Nach der "Bindungstheorie" von Kirkpatrick sei Gott (bezogen auf das Christentum) eine Ersatz-Bezugsperson, wenn menschliche Bezugspersonen (Eltern, Lehrer u. ä.) fehlen oder unzureichend sind. Diese Annahme konnte von R. K. Ullmann empirisch untermauert werden.

Boyer und Atran kritisieren die Funktionen Sicherheit und Orientierung. Sie halten dem entgegen, dass Religionen häufig mehr Fragen aufwerfen, als sie beantworten, dass der Erlösungsgedanke häufig nicht vorkommt, dass trotz der offiziellen Religionen häufig der Glaube an böse Geister und Hexen vorkomme und dass selbst viele Konfessionen nicht nur Ängste reduzieren, sondern auch neue schaffen. Die beiden Wissenschaftler reduzieren die Universalien im Gegensatz zu Wernhart auf zwei menschliche Bedürfnisse: Kooperation und Information.

Jeder Gläubige hat Erwartungen, Hoffnungen und Sehnsüchte, die vor dem Hintergrund des Glaubens und der religiösen Orientierung ihren Ausdruck in verschiedenen Praktiken finden:

Seit den Anfängen der Religionswissenschaften wurden viele Versuche unternommen, die postulierten historischen Verwandtschaften zwischen den verschiedenen Glaubenssystemen zu rekonstruieren und daraus eine Typologie und Systematik zu erstellen. Während dies bei den Weltreligionen aufgrund der Schriftzeugnisse recht einfach ist, ist es für die ethnischen Religionen – beziehungsweise für die "Gesamtheit" aller Religionen – nach heutigen Maßstäben noch nicht überzeugend gelungen.

Nachdem ältere Theorien – wie die eines prähistorischen Bärenkultes oder einer einheitlichen „Urreligion“ "(siehe etwa: Animistische Urreligion, Core Schamanismus nach Harner oder Urmonotheismus nach Wilhelm Schmidt)" – heute als widerlegt gelten, andererseits aber die lange bezweifelten Datierungen jungpaläolithischer Höhlenmalereien und Musikinstrumente wesentlich erweitert und bestätigt wurden, hat sich ein wissenschaftlicher Konsens über den Beginn menschlicher Religionsgeschichte herausgebildet. Demnach werden Bestattungen und (später) Grabbeigaben als frühe archäologische Zeichen religiösen Ausdrucks anerkannt, die sich ab etwa 120.000 Jahren v. Chr. im Mittelpaläolithikum sowohl bei Homo sapiens als auch beim Neandertaler nachweisen lassen. Der "Homo sapiens" entwickelt im späten Mittelpaläolithikum (mittlerer Abschnitt der Altsteinzeit) und beginnendem Mesolithikum (Mittelsteinzeit) komplexere Ausdrucksformen in frühen Kleinkunstwerken, Höhlenmalereien, später mit aufwändigen Grabstätten und zum Beginn des Neolithikums (Jungsteinzeit) im Nahen Osten herausgehobene Bauwerke, wie das als Tempelanlage interpretierte Göbekli Tepe.

Ab ca. 40.000 v. Chr. – mit dem Auftreten künstlerischer Skulpturen, Malereien und Musikinstrumente – werden die Hinweise deutlicher. Welche religiösen Inhalte und Konzepte diesen Artefakten zuzuschreiben sind, ist allerdings unklar bzw. spekulativ.

Viele der heute noch praktizierten Religionen haben ihre Wurzeln in vorgeschichtlicher Zeit. Andere frühe Religionen existieren heute nicht mehr und sind in ihren Inhalten oft schwer fassbar, da fehlende oder lückenhafte Überlieferung das Verständnis erschwert und religiöse Konzepte sich in den von der Archäologie gefundenen materiellen Artefakten nur mittelbar abbilden. Das führt zu sehr unterschiedlichen Interpretationen der archäologischen Funde.

Auch dort, wo sehr umfangreiche schriftliche Quellen vorliegen (etwa bei den antiken Religionen der Griechen und Römer) sind die Kenntnisse über Kultpraxis und individuelle Religionsübung dennoch sehr lückenhaft. Häufig hat man Kenntnisse hauptsächlich von der Mythologie, so weiß man einiges über die Mythologie der Kelten und der Germanen, über die Kultpraxis jedoch kaum etwas.

Zu den historischen Religionen zählen (räumlich und zeitlich geordnet):
Die Geschichte verschiedener Religionen wird in den Geschichtsabschnitten ihrer jeweiligen Artikel bzw. in gesonderten geschichtlichen Artikeln dargestellt:

Im Gegensatz zu den mittelalterlichen christlichen Gesellschaften, in denen fast die gesamte Lebenswirklichkeit unter der Autorität der Religion stand, verlor die institutionalisierte Religion in der Neuzeit zunehmend an Machtfülle. Anstelle der Theologie errangen die Natur- und Geisteswissenschaften Autorität, beispielsweise in Fragen zu Evolution oder Ethik/Recht, Bereichen, die zuvor der Religion unterstanden. Die Tendenzen hin zu einer Trennung von Kirche und Staat werden als Säkularisierung bezeichnet. Erklärungsversuche für dieses Phänomen beziehen sich oft auf die Ideen des Humanismus und der Aufklärung, Einflüsse der Industrialisierung, die allmähliche bzw. durch eine Revolution hervorgerufene Überwindung des feudalen Ständestaates und den damit verbundenen ökonomischen, sozialen, kulturellen und rechtlichen Wandel. Der Wandel umfasst alle gesellschaftlichen Felder, so auch das der verfassten Religion, welche sich ebenfalls ausdifferenzierte, einerseits Gewaltpotenzial und Unduldsamkeit zeigte, andererseits pluralistischer auftrat und vielfach mehr Toleranz aufbrachte.

In Europa verlor das Christentum seit dem späten 19. Jahrhundert hinsichtlich seiner Reputation, seines gesellschaftlichen und politischen Einflusses und seiner Verbreitung beschleunigt an Bedeutung. Einige traditionell christliche westliche Länder verzeichneten sinkenden Klerikernachwuchs, Verkleinerung der Klöster und ein Anwachsen von Kirchenaustritten oder andere Formen von Distanzierung.

Besonders in Frankreich, wo durch die Revolution 1789, den Code civil 1804 und Anfang des 20. Jahrhunderts durch das Gesetz zur Trennung von Kirche und Staat ein strikter Laizismus umgesetzt worden ist, ging der gesellschaftliche Einfluss der katholischen Kirche zurück.

Die abnehmende materielle und geistige Macht der großen christlichen Kirchen, die Friedrich Nietzsche Ende des 19. Jahrhunderts mit den Worten „"Gott ist tot"“ kommentierte, wurde und wird von einigen religiösen Denkern bemängelt. Sie argumentieren, durch das Schwinden des Einflusses der Religion würden ethische Standards reduziert und der Mensch zum Maß aller Dinge gemacht. Unter der Devise „Ohne Gott ist alles erlaubt“, könnten destruktive Handlungen und nihilistisches Denken gefördert werden. Für solche Folgen gibt es allerdings keine eindeutigen Hinweise.

In der Sowjetunion, insbesondere während der Terrorherrschaft Stalins, im nationalsozialistischen Deutschland und weniger ausgeprägt in den Ostblockstaaten nach 1945 konnte eine öffentliche religiöse Betätigung zu gesellschaftlichen Benachteiligungen führen, bis hin zu Todesurteilen und Verschleppung. Daher war der Anteil der sichtlich praktizierenden Mitglieder von Religionsgemeinschaften vergleichsweise gering. Nach dem Zusammenbruch der Sowjetunion ist eine ambivalente Entwicklung festzustellen. Während in den neuen Bundesländern die organisierte Religion weiterhin nur eine marginale Rolle spielt, ist sie beispielsweise in Polen tief verwurzelt.

Zahlreiche Studien belegen rückläufige Besucherzahlen in Kirchen, Synagogen und anderen religiösen Einrichtungen, z. B. in Großbritannien, Deutschland und Frankreich, obwohl die Kirchen hier Umfragen zufolge weiterhin zu den anerkannten öffentlichen Einrichtungen zählen. In den meisten europäischen Staaten waren 2005 jedoch noch mehr als 50 % der Einwohner Mitglieder einer christlichen Kirche. In Polen, Irland, Spanien und Italien gilt die katholische Kirche, der jeweils mehr als 80 % der Bewohner angehören, als politisch einflussreich. In vielen europäischen Ländern ist es nach wie vor üblich, zumindest formell, einer Religionsgemeinschaft anzugehören. Seit einigen Jahrzehnten, verstärkt seit dem Ende des letzten Jahrtausends, wenden sich vor allem junge Menschen weltweit häufiger wieder institutionalisierten oder anderen religiösen Ausdrucksformen zu.

Im Gegenzug zur Säkularisierung in Europa gewinnen insbesondere Islam und Christentum, aber auch der Buddhismus, in der übrigen Welt an Bedeutung. In den USA und Lateinamerika etwa stellt die Religion nach wie vor einen wichtigen Faktor dar. Im 20. Jahrhundert ist in Afrika der Einfluss des Christentums und des Islam erheblich gewachsen. In der arabischen Welt ist der Islam zunehmend das prägende Element der Gesellschaft. Auch in Teilen Asiens hat der Islam an Einfluss gewonnen, so in Indonesien, Pakistan, Indien und Bangladesch. In der Volksrepublik China erleben religiöse Gemeinschaften seit der Lockerung entsprechender Verbote wieder einen moderaten Aufschwung. Von den christlichen Kirchen und Religionsgemeinschaften erzielen die sich zum Protestantismus zählenden evangelikalen Missionare weltweit die meisten „Bekehrungserfolge“.

Neuere Forschungen verweisen darauf, dass in zeitgenössischen Gesellschaften statistisch nachweisbar ein Zusammenhang zwischen Demografie und Religion besteht. Die Kinderzahl in religiösen Gemeinschaften ist zum Teil erheblich höher als die in den eher säkular geprägten Gesellschaften. Beispiele hierfür sind die Geburtenraten der türkischstämmigen Familien in Deutschland, die zumeist dem sunnitischen Islam angehören, evangelikaler christlicher Gruppen in den USA und zunehmend auch in Europa und Angehöriger des orthodoxen Judentums in Israel. Dieses Phänomen wird gegenwärtig auf dem Hintergrund der Probleme einer wachsenden Weltbevölkerung nicht nur positiv, sondern auch negativ bewertet.

In den meisten Mitgliedsstaaten der Vereinten Nationen wurde das Recht auf Religionsfreiheit inzwischen gesetzlich verankert, aber nicht unbedingt im Alltagsleben verwirklicht. Allerdings gibt es noch zahlreiche Länder, in denen keinerlei Recht auf freie Wahl der Religion besteht, so z. B. Saudi-Arabien und Nordkorea, oder in denen der Handlungsspielraum religiöser Individuen und Gruppen eingeschränkt ist. Demgegenüber gewähren die USA praktisch jeder Gemeinschaft, die sich selbst als religiös bezeichnet, den Status einer "religious community" mit entsprechenden Rechten.

Seit der zunehmenden Anerkennung indigener Völker kommt es zum Teil zu einer Revitalisierung ethnischer Religionen (etwa bei den Tuwinern in China und Russland, bei vielen Indianern Nordamerikas oder bei den Samen Skandinaviens). Aufgrund des vielfach bereits verlorenen Wissens, der langjährigen Einflüsse anderer Religionen oder auch der Bezugnahme auf (zum Teil falsche) Interpretationen westlicher Autoren aus Wissenschaft und Esoterik kann man diese Religionsformen in den meisten Fällen jedoch "nicht" mit den traditionellen Vorläufern gleichsetzen.

Die Quellenlage für präzise Aussagen über die Religionszugehörigkeit weltweit ist äußerst fraglich. Nicht nur die Forschungsmethoden unterscheiden sich erheblich, vor allem ist die Ausgangssituation in den Staaten sehr unterschiedlich. Lediglich über Staaten, in denen Religionsfreiheit besteht, können relativ exakte Aussagen gemacht werden. Aber auch dort gibt es eine hohe Varianz, schon hinsichtlich der Datenerhebung. Unterschiedliche Ergebnisse sind beispielsweise zu erwarten, je nachdem ob die Aussagen auf behördlich erfasster Zugehörigkeit zu einer Religionsgemeinschaft oder auf Befragungen beruhen. Regimes, die keine Religionsfreiheit gewährleisten oder Staaten, die sich offiziell als atheistisch betrachten, machen ein realistisches Bild fast unmöglich. Hinzu kommt, dass auch die Weltreligionen sehr heterogen sind: so unterscheidet sich beispielsweise das Christentum in afrikanischen Ländern von dem in skandinavischen in vielen Merkmalen. Zum Judentum werden zumeist auch nichtreligiöse Juden gerechnet, zum Christentum in Deutschland alle Kirchensteuerzahler, auch wenn sie nicht gläubig sind, zum Islam alle Bürger Saudi-Arabiens. Unschärfen entstehen u. a. auch, weil Kinder und Jugendliche der Religion ihrer Eltern zugerechnet werden, jedoch sich selbst nicht unbedingt dieser Religion angehörig fühlen. Für einzelne Staaten mit ausgewiesenen statistischen Systemen lassen sich genauere Angaben machen, die aber nicht ohne Weiteres miteinander vergleichbar sind.

Zur Verteilung in Deutschland, Österreich und der Schweiz siehe auch Religionen in Deutschland, Anerkannte Religionen in Österreich und Religionen in der Schweiz

Laut einer repräsentativen Umfrage des Eurobarometers glaubten im Jahr 2005 52 % der Menschen in der damaligen Europäischen Union an Gott, weitere 27 % glaubten etwas vager an eine spirituelle Kraft bzw. höhere Macht. 18 % Prozent der Befragten glaubten weder an einen Gott noch an eine andere spirituelle Kraft, 3 % der Unionsbürger waren unentschlossen.

Nach der 15. Shell-Jugendstudie aus dem Jahr 2006 glauben 30 % der befragten deutschen Jugendlichen im Alter zwischen 12 und 24 Jahren an einen persönlichen Gott, weitere 19 % an eine höhere Macht, während 23 % eher agnostische Angaben machten und 28 % weder an einen Gott noch eine höhere Macht glauben.

Bereits beim Versuch, einen wissenschaftlichen Zugang zum "Begriff" „Religion“ zu finden, sehen sich die Wissenschaften vor große Schwierigkeiten gestellt: Wie kann man eine alle Religionen umfassende, über-historische Definition von Religion finden, mit der sich wissenschaftlich arbeiten lässt? Oft sind die Definitionen entweder zu eng, so dass wichtige religiöse Strömungen nicht mit erfasst werden; oder aber der Begriff „Religion“ verliert seine Präzision und wird zu beliebig, als dass sich vergleichbare Untersuchungen noch erlauben würden. Sich wissenschaftlich mit einem Untersuchungsobjekt zu befassen, dessen Definition keine klare Abgrenzung erlaubt, erweist sich als schwierig.

Dennoch ist Religion aber „eine soziale Realität, ein spezifischer Kommunikationsprozess, der Wirklichkeiten schafft und durch soziale Handlungen selbst reale Gestalt gewinnt“ und daher notwendigerweise Gegenstand wissenschaftlicher Neugierde. Die im Folgenden dargestellten Konzepte sollten immer vor dem Hintergrund der Schwierigkeiten in der Begriffsfindung wahrgenommen werden, die im Kontrast zur wissenschaftlichen Notwendigkeit stehen, sich mit dem realen Phänomen „Religion“ zu beschäftigen.

Der Vordenker der Aufklärung Jean-Jacques Rousseau kritisierte in seinem 1762 in Paris erschienenen einflussreichen Werk „Vom Gesellschaftsvertrag oder Prinzipien des Staatsrechtes“ die Religion grundlegend als Quelle von Krieg und Machtmissbrauch, konstatierte aber religiöse Gefühle der Menschen. Er entwickelte das Modell einer Zivilreligion, die den politischen Erfordernissen einer „freien“ aufgeklärten Gesellschaft gerecht werde. Dazu gehörte die Anerkennung der Existenz Gottes, eines Lebens nach dem Tod, die Vergeltung von Gerechtigkeit und Ungerechtigkeit, die Unantastbarkeit (Heiligkeit) des Gesellschaftsvertrages und der Gesetze und schließlich die Toleranz. Diese neue, für alle Bürger gleichermaßen gültige Zivilreligion sollte zur Stabilität der Gemeinschaft beitragen.

Sein ebenfalls aufgeklärter Gegenspieler Voltaire, welcher die Dogmen und die Machtfülle der katholischen Kirche noch schärfer ablehnte, setzte sich für einen vernunftgeleiteten, toleranten Deismus unabhängig von den bis dahin existierenden Religionen ein und betonte die moralische Nützlichkeit des Glaubens an Gott. Er war von der Gesetzmäßigkeit des Kosmos und der Existenz einer höchsten Intelligenz überzeugt, ging von der Unsterblichkeit der Seele und einem freien menschlichen Willen aus, Positionen, die er jedoch auch in jeder Hinsicht bezweifelte. Den Glauben an heilige Schriften oder an Jesus Christus als Sohn Gottes teilte er nicht.

Immanuel Kant formulierte 1793 in seiner religionsphilosophischen Schrift „Die Religion innerhalb der Grenzen der bloßen Vernunft“ seine Auffassung über eine Vernunftreligion. Er entwickelte eine philosophische Religionslehre, die das Prinzip des Bösen postuliert. Das Böse sei dem menschlichen Wesen innewohnend. Er geht von der Existenz Gottes und von der Unsterblichkeit der Seele aus. Gott lasse sich allerdings nicht beweisen. Laut Kant verfügt lediglich das Christentum im Gegensatz zu anderen, seiner Auffassung nach „veralteten“ und „ritualisierten“ Religionen wie Judentum und Islam, über eine Lehre und Moral, die die Philosophie anerkennen kann. Konsequentes moralisches Handeln ist demnach nicht möglich ohne den Glauben an die Freiheit, die Unsterblichkeit der Seele und Gott. Daher ist die Moral das Ursprüngliche. Die Religion indes erklärt die moralischen Pflichten als göttliche Gebote. Also folge die Religion dem bereits vorhandenen Moralgesetz. Um die eigentlichen menschlichen Pflichten zu finden, müsse man das Richtige aus den verschiedenen Religionslehren herausfiltern. Rituelle Praktiken der Religionen lehnte Kant als „Pfaffentum“ ab. Erkenntnistheoretisch nahm er eine agnostische Haltung ein.

Der Religionskritiker Ludwig Feuerbach erklärte 1841 Religion als „das "erste" und "zwar indirekte Selbstbewusstsein" des Menschen. […] der Mensch vergegenständlicht in der Religion sein eignes geheimes Wesen.“ Demnach betrachtet der religiöse Mensch alles, was er für wahr, richtig und gut hält, als selbstständige Erscheinungen außerhalb seiner selbst. Diese selbstständigen Erscheinungen kann sich der Mensch als Person in Einzahl oder Mehrzahl mit begrenztem oder unbegrenztem Wirkungsbereich vorstellen und demzufolge seine Begriffe vom Wahren, Richtigen und Guten als Bereichsgötter oder einzigen Gott benennen oder ohne Personifikation als Kräfte, Mächte, Wirkungen, gesetzmäßige Abläufe oder ähnlich bestimmen. Wie er das tut, richtet sich nach regionaler Entwicklung und Überlieferung. Folgerichtig betrachtet Feuerbach Religion nicht mehr als weltdeutendes, menschenverpflichtendes System, sondern als völkerkundliches Forschungsgebiet.

Karl Marx bezeichnete 1844 im Anschluss an Feuerbach in seiner "Einleitung zur Kritik der Hegelschen Rechtsphilosophie" Religion als „das Opium des Volkes“, ein Ausspruch, der zum geflügelten Wort geworden ist. Ein für Marx zentraler Gedanke ist, dass die Geschöpfe ihre Schöpfer beherrschen: „Wie der Mensch in der Religion vom Machwerk seines eigenen Kopfes, so wird er in der kapitalistischen Produktion vom Machwerk seiner eigenen Hand beherrscht.“

Aus Feuerbachs Forderung an den Menschen, die „Illusion über seinen Zustand aufzugeben“ zieht Marx die Konsequenz, „einen Zustand aufzugeben, der der Illusionen bedarf“. Nach Marx wird Religion als ein verkehrtes Weltbewusstsein von Staat und Gesellschaft produziert, weil in bisherigen Gesellschaftsordnungen der Mensch von sich selbst entfremdet war. „Die Aufhebung der Religion als des "illusorischen" Glücks des Volkes“ ist für ihn daher „die Forderung seines "wirklichen" Glücks“.

Von Friedrich Nietzsche stammt der als Gedanke der Moderne immer wieder zitierte Ausspruch „Gott ist tot!“ Er fährt fort, und dies ist weniger bekannt: „Gott bleibt tot! Und wir haben ihn getötet! Wie trösten wir uns, die Mörder aller Mörder?“ Der Philosoph zählte die wachsende Bedeutung der Naturwissenschaften und der Geschichtswissenschaft zusammen mit der radikalen Religionskritik zu den Ursachen für den Verfall der (christlichen) Moral.

In der Tradition Feuerbachs und Nietzsches stehend, stellte der Begründer der Psychoanalyse Sigmund Freud Religion als Zwangsneurose und infantiles Abwehrverhalten dar. Der Urmensch habe die Naturkräfte personalisiert und zu schützenden Mächten erhoben, damit sie ihn in seiner Hilflosigkeit stützen. Das zugrunde liegende Verhaltensmuster knüpft demnach an die frühkindliche Erfahrung des schützenden, aber auch strafenden Vaters an. Daraus resultiere ein zwiespältiges Verhältnis zum Vater, das im Erwachsenenalter zum „Glauben“ führe. Der Mensch fürchte die Gottheiten und suche gleichzeitig ihren Schutz. Auf die Evolutionstheorie Charles Darwins Bezug nehmend, sah Freud die „Urhorde“ mit einem despotischen „Stammesvater“ als Anführer, der über alle Frauen des Stammes verfügen konnte. Seine Söhne verehrten ihn, fürchteten ihn aber auch. Aus Eifersucht brachten sie gemeinsam den Urvater um. Daraus sei der „Ödipuskomplex“ hervorgegangen. Das Schuldbewusstsein der gesamten Menschheit (Vorstellung von der „Erbsünde“) sei somit der kulturbewahrende Anfang sozialer Organisation, der Religion sowie – damit zusammenhängend – sexueller Einschränkung.

Die argentinische Religionspsychologin Ana-Maria Rizzuto geht – anders als Freud – davon aus, dass die Gottesvorstellung einen notwendigen Teil der Ichbildung darstellt. Demnach entwickeln Kinder aus der breiten Fülle von Fantasien zu Helden und magischen Wesen ihr jeweiliges Gottesbild – im Rahmen des Bezugsystems ihrer Eltern und der Umwelt.

Erich Fromm prägte eine weite, sozialpsychologische Definition. Als Religion betrachtete er jedes von einer Gruppe geteilte System des Denkens und Handelns, das dem Einzelnen einen Rahmen der Orientierung und ein Objekt der Hingabe bietet.

Der zeitgenössische postmoderne deutsche Philosoph Peter Sloterdijk schreibt der Religion die Wirkung eines "psychosemantischen Immunsystems" zu. Im Zuge der kulturellen Entwicklung sei der Mensch offener, aber auch verletzbarer geworden. Religion befähige den Menschen, „Verletzungen, Invasionen und Kränkungen“ selbst zu heilen. Sloterdijk bezeichnet nicht Gott, sondern „das Wissen um Heilung als Realität, von der biologischen bis zu einer spirituellen Stufe“ als "die Perle in der Muschel der Theologie".

Jürgen Habermas, der prominenteste Vertreter der Kritischen Theorie in der Gegenwart, betont seit Ende der 1990er Jahre den positiven Einfluss der (christlichen) Religion auf demokratische Wertsysteme, während Theodor W. Adorno in Marxscher Tradition die Religion als „gesellschaftliche Projektion“ begreift und die Durkheimsche Religionssoziologie pointiert in der Aussage zusammenfasst, dass „in der Religion die Gesellschaft sich selbst anbete“.

Die vorrangige Fragestellung der Religionsgeschichte lautet: „Unterliegt die Religionsentwicklung einer direkten soziokulturellen Evolution oder ist sie nur ein Nebenprodukt anderer kognitiver Entwicklungen?“ Ein evolutionärer Prozess setzt immer selektive Faktoren voraus, so dass die Frage nur beantwortet werden kann, wenn zweifelsfreie Faktoren ermittelt werden können, die gläubigen Menschen irgendwelche Überlebensvorteile verschaffen.

In der Frühzeit der wissenschaftlichen Behandlung von Religion waren evolutionistische Entwürfe vorherrschend, in denen die einzelnen Ereignisse als bloße Etappen vergleichsweise einfacher, globaler, quasi naturgesetzlicher Entwicklungen gesehen wurden, etwa bei James Frazer als eine Entwicklung von der Magie über die Religion zur Wissenschaft. Diese teleologischen Positionen krankten oft an unzureichenden empirischen Grundlagen, enthielten meist explizite oder implizite Wertungen und waren vielfach auf den Einzelfall konkreter religionsgeschichtlicher Ereignisse nicht anwendbar. In der modernen Religionswissenschaft spielen sie nur noch als Materiallieferanten und als Teil der Fachgeschichte eine Rolle.

In einer geschichtsphilosophischen Betrachtung machte Karl Jaspers eine von ihm sogenannte Achsenzeit zwischen 800 und 200 v. Chr. aus, in der wesentliche geistesgeschichtliche Innovationen die Philosophie- und Religionsgeschichte Chinas, Indiens, des Iran und in Griechenland prägten. Jaspers deutete diese als eine umfassende Epoche der „Vergeistigung“ des Menschen, die sich in Philosophie und Religion, sekundär auch in Recht und Technologie ausgewirkt habe. Mit dieser pluralistischen Interpretation wandte Jaspers sich vor allem gegen eine christlich motivierte Konzeption einer Universalgeschichte. Im Gegensatz zu den Offenbarungsreligionen, die er ablehnte, konzipierte er in seinem religionsphilosophischen Werk "Der philosophische Glaube angesichts der Offenbarung" eine philosophische Annäherung an eine Transzendenz angesichts menschlicher Allmachtsvorstellungen.

Die religiös-spirituellen Vorstellungen schriftloser Kulturen, häufig als „Naturreligionen“, wissenschaftlich korrekter als ethnische Religionen oder (veraltet) als Animismus bezeichnet, wurden aufgrund ihrer angeblichen „Primitivität“ lange für die ältesten Formen von Religion gehalten. Doch auch sie unterliegen einem historischen Wandel und werden daher heute von einigen Autoren nicht mehr im Sinne unveränderter Traditionen verstanden. Aufgrund der nicht vorhandenen Dogmen und ihrer großen Anpassungsfähigkeit an veränderte Bedingungen sind sie ganz im Gegenteil sämtlich jünger als die bekannten Hochreligionen. Dennoch halten etliche Prähistoriker (etwa Marcel Otte) an der Vorstellung fest, die Religionen der Vorzeit ließen sich aus Vergleichen mit heutigen „primitiven Religionen“ rekonstruieren. Dabei wird außer Acht gelassen, dass auch diese Glaubenssysteme irgendwann einmal einen Anfang gehabt haben müssen, der erheblich einfacher gedacht werden muss als die komplexen Weltbilder heutiger Indigener.

Allgemein wird heute eine direkte Evolution der Religionen in engem Zusammenhang mit dem Wandel der Sozialstrukturen postuliert, weil sie offenbar gewisse Aspekte des Zusammenlebens positiv beeinflusst. Allerdings ist man sich über die konkreten Selektionsvorteile nach wie vor uneinig. Weder die Förderung altruistischen Verhaltens noch ein konkreter Einfluss auf die Reproduktionsrate ist zweifelsfrei belegt. Überdies kritisiert die Religionswissenschaftlerin Ina Wunn, dass viele Modelle nach wie vor eine Höherentwicklung voraussetzen, womit sie ethnische oder polytheistische Religionen degradieren. Damit würden Repressalien bestimmter Staaten gegen ihre religiösen Minderheiten im Sinne des Fortschritts gerechtfertigt.

In neuerer Zeit tritt die Religionsgeschichte als Universalgeschichte gegenüber dem Studium der Geschichte einzelner Religionen oder Kulturräume zurück. Jedoch finden religionsgeschichtliche Theoriekonzepte wie Säkularisierung und Pluralisierung wieder verstärkt Beachtung.

Religionssoziologische Gedankengänge finden sich bereits in der griechischen Antike, zumal bei Xenophanes ("Wenn die Pferde Götter hätten, sähen sie wie Pferde aus.").

Nach Ferdinand Tönnies (1887), einem der Mitbegründer der Soziologie, ist die Religion in der „Gemeinschaft“ das Äquivalent zur „öffentlichen Meinung“ in der „Gesellschaft“. Diese Abgrenzung versteht Tönnies als normaltypisch. Religion und öffentliche Meinung sind die jeweilige "mentale" Ausbildung von Gemeinschaft bzw. Gesellschaft (neben der politischen und der wirtschaftlichen). Da sich die Menschen in der Gemeinschaft als „Mittel zum Zweck übergeordneter Kollektive“ verstehen, sind sie zu großen Opfern zugunsten einer angenommenen höheren Instanz fähig – anders als „gesellschaftlich“ verbundene Menschen, die alle Kollektive als Mittel für ihre je individuellen Zwecke ansehen, und die jeweilige Gesellschaft utilitaristisch unterstützen oder bekämpfen. Religion und öffentliche Meinung haben, so Tönnies, starke Gemeinsamkeiten, etwa heftige Unduldsamkeit gegen Abweichler.

Laut Émile Durkheim (1912), einem anderen Mitbegründer der Soziologie, trägt Religion zur Festigung sozialer Strukturen, aber auch zur Stabilisierung des Einzelnen bei. Sein Religionsbegriff ist somit ein funktionalistischer. Gemäß Durkheim ist die Religion ein solidarisches System, das sich auf Überzeugungen und Praktiken bezieht, die als heilig erachtete Dinge umfassen und in einer moralischen Gemeinschaft, wie beispielsweise der Kirche, alle Mitglieder miteinander verbindet. Daraus ergeben sich drei Aspekte von Religion, die Glaubensüberzeugungen (Mythen), die Praktiken (Riten) und die Gemeinschaft, auf die diese Überzeugungen und Praktiken bezogen sind. Durkheim bezeichnet unter anderen Faktoren den Glauben als ein Element der Macht, die die Gesellschaft über ihre Mitglieder ausübt. Zu den bemerkenswerten Aspekten seines Religionsbegriffs gehört die Unterscheidung zwischen dem Sakralen und dem Profanen, die es erlaubt, Religion ohne den Bezug auf Gott, Götter oder übernatürliche Wesenheiten (Gottheiten) zu definieren. Sie wird auch außerhalb der Soziologie verwendet und liegt ebenfalls dem Begriff „säkulare Religion“ (bei Max Weber: „Diesseitigkeitsreligion“) zugrunde, mit dem Weltanschauungen bezeichnet werden, die diesseitige Phänomene wie z. B. den Staat, eine Partei oder einen politischen Führer zum Gegenstand einer religionsähnlichen Verehrung machen.

Max Weber, der sich am Anfang des 20. Jahrhunderts ausführlich mit dem Phänomen „Religion“ aus soziologischer Sicht befasste, unterscheidet zwischen Religion und Magie. Unter „Religion“ versteht er ein dauerhaftes, ethisch fundiertes System mit hauptamtlichen Funktionären, die eine geregelte Lehre vertreten, einer organisierten Gemeinschaft vorstehen und gesellschaftlichen Einfluss anstreben. „Magie“ dagegen ist nach Weber lediglich kurzfristig wirksam, gebunden an einzelne Magier oder Zauberer, die als charismatische Persönlichkeiten vermeintlich Naturgewalten bezwingen und eigene moralische Vorstellungen entwickeln. Diese Abgrenzung versteht Weber als idealtypisch. Reinformen sind selten, Überschneidungen und Übergänge werden konstatiert. Weber erarbeitete umfangreiche theoretische Abhandlungen über die verschiedenen Religionen, insbesondere über die protestantische Ethik und führte empirische Studien zu der unterschiedlichen wirtschaftlichen Entwicklung in protestantischen und katholischen Ländern durch.

In der zweiten Hälfte des 20. Jahrhunderts unterscheidet Niklas Luhmann in seiner Systemtheorie zwischen „System“ und „Umwelt“. Die Umwelt bietet demnach Möglichkeiten, die vom System durch Ausgrenzung und Auswahl genutzt werden können. Durch diesen Selektionsprozess wird die Umwelt in ihrer Komplexität eingeschränkt. Da jedoch sowohl das System als auch die Umwelt nach wie vor von hoher Komplexität geprägt sind, sind Vereinfachungen notwendig, die der Orientierung dienen. Als ein soziales Funktionssystem moderner Gesellschaften unter anderen kommt der Religion solch eine orientierende Funktion zu. Sie begrenzt ein Übermaß an Möglichkeiten und verhindert die beliebige Veränderung der Auswahl.

Die Theorie der rationalen Entscheidung der Religionen entstand in den 1980er Jahren. Als Hauptvertreter gelten Rodney Stark, Laurence R. Iannaccone und Roger Finke. Diese Theorie besagt, dass Akteure ihre Handlungen nutzenorientiert wählen. Annahmen dieser Theorie sind: Der Akteur handelt rational durch Abwägen von Kosten und Nutzen; es gibt stabile Präferenzen, die sich weder von Akteur zu Akteur noch zeitlich stark unterscheiden; soziale Ereignisse sind Ergebnisse von sozialen Interaktionen zwischen den Akteuren. Nicht nur der Akteur in Form des Gläubigen handelt nach dieser Theorie nutzenmaximierend, sondern auch religiöse Organisationen. Sie spezialisieren ihr Angebot von religiösen Gütern, so dass sie möglichst viele Gläubige anziehen. Diese Theorie wird von verschiedenen anderen Religionssoziologen kritisiert, da beispielsweise zentrale Begriffe der Theorie nicht genau definiert seien („Kosten“, „Nutzen“), und es ist strittig, ob kostentheoretisch ausgefeilte Begriffe aus der Betriebswirtschaftslehre auf religiöses Handeln übertragen werden können.

Die Religionswissenschaft, die eine Vielzahl von Disziplinen wie Religionssoziologie, Religionsphilosophie, Religionsphilologie, Religionsgeschichte u. a. umfasst, untersucht auf empirischer und theoretischer Grundlage Religionen als gesellschaftliche Phänomene. Religionswissenschaftliche Theorien müssen unabhängig von Glaubensannahmen nachvollziehbar und falsifizierbar sein. Seit etwa 100 Jahren als eigenständige Disziplin etabliert, geht sie auf Vorläufer sowohl innerhalb Europas wie auch darüber hinaus (religionsvergleichende Studien in China und der islamischen Welt) zurück. In Abgrenzung zur Theologie gehört zur Religionswissenschaft einerseits die Möglichkeit des Dialoges, aber auch die Option der Religionskritik.

Nach Clifford Geertz (1973) ist Religion ein kulturell-geschaffenes Symbolsystem, das versucht, dauerhafte Stimmungen und Motivationen im Menschen zu schaffen, indem es eine allgemeine Seinsordnung formuliert. Diese geschaffenen Vorstellungen werden mit einer überzeugenden Wirkung („Aura von Faktizität“) umgeben, dass diese Stimmungen und Motivationen real erscheinen. Solche „heiligen“ Symbolsysteme haben die Funktion, das Ethos – das heißt das moralische Selbstbewusstsein einer Kultur – mit dem Bild, das diese Kultur von der Realität hat, mit ihren Ordnungsvorstellungen zu verbinden. Die Vorstellung von der Welt wird zum Abbild der tatsächlichen Gegebenheiten einer Lebensform. Die religiösen Symbolsysteme bewirken eine Übereinstimmung zwischen einem bestimmten Lebensstil und einer bestimmten Metaphysik, die einander stützen. Religion stimmt demnach menschliche Handlungen auf eine vorgestellte kosmische Ordnung ab. Die ethischen und ästhetischen Präferenzen der Kultur werden dadurch objektiviert und erscheinen als Notwendigkeit, die von einer bestimmten Struktur der Welt erzeugt wird. Die Glaubensvorstellungen der Religionen bleiben demgemäß nicht auf ihre metaphysischen Zusammenhänge beschränkt, sondern erzeugen Systeme allgemeiner Ideen, mit denen intellektuelle, emotionale oder moralische Erfahrungen sinnvoll ausgedrückt werden können. Da somit eine Übertragbarkeit von Symbolsystem und Kulturprozess vorliegt, bieten Religionen nicht nur Welterklärungsmodelle, sondern gestalten auch soziale und psychologische Prozesse. Durch die unterschiedlichen Religionen wird eine Vielfalt unterschiedlicher Stimmungen und Motivationen erzeugt, so dass es nicht möglich ist, die Bedeutsamkeit von Religion in ethischer oder funktionaler Hinsicht festzulegen.

Nach Rüdiger Vaas bieten Religionen die „ultimative Bezogenheit“: das Gefühl der Verbundenheit, Abhängigkeit, Verpflichtung sowie den Glauben an Sinngebung und Bestimmung.

Jacques Waardenburg bezeichnet die Definition von Religion als ‚Glauben‘ als ein Produkt westlicher Tradition. Dieser Begriff treffe daher nicht auf die Vorstellungen anderer Kulturen zu und sei für die Beschreibung von Religionen eher ungeeignet. Religionen können nach seiner Auffassung als Bedeutungsgefüge mit darunterliegenden Grundintentionen für Menschen angesehen werden.

Ein in der Religionswissenschaft gängiger Umgang mit dem Religionsbegriff ist, Religion als „Offenes Konzept“ zu betrachten, also auf eine Definition des Religionsbegriffes gänzlich zu verzichten. Diese Auffassung wurde besonders von dem Bremer Religionswissenschaftler Hans G. Kippenberg vertreten.

Ein kulturwissenschaftlicher Ansatz stammt von Michael Bergunder. Bergunder betrachtet den historischen Begriffswandel und stellt fest, dass "Religion" lange Zeit eurozentrisch belegt war. Das „konsensfähige Alltagsverständnis“, auf das sich die Religionswissenschaft bezieht, müsse hingegen heute auf einem globalen Religionsbegriff beruhen.

Andere Religionswissenschaftler entwickelten das Modell der verschiedenen „Dimensionen“ von Religion. Hier sind vor allem Rodney Stark und Charles Glock zu nennen. Sie unterscheiden die ideologische, die ritualisierte, die intellektuelle Dimension sowie die Dimension der Erfahrung und die handlungspraktische Dimension. Einen ähnlichen Ansatz vertrat der irisch-britische Religionswissenschaftler Ninian Smart: auch er entwarf ein multidimensionales Modell von Religion und unterscheidet dabei sieben Dimensionen: 1. die praktische und rituelle, 2. die erfahrungsmäßige und emotionale, 3. die narrative oder mythische, 4. die doktrinale und philosophische, 5. die ethische und rechtliche, 6. die soziale und institutionale und 7. die materielle Dimension (z. B. sakrale Bauwerke).

In jüngster Zeit entwickelt sich ein Dialog zwischen einigen Hirnforschern und Religionswissenschaftlern sowie Theologen, der mitunter als Neurotheologie bezeichnet wird und sich zunehmend auch mit der Suche von Biologen nach einer schlüssigen Theorie zur Evolution der Religionen verschränkt.

Verschiedene Hirnforscher suchen seit 1970 nach neurologischen Erklärungen für verschiedene Typen religiöser Erfahrungen. Entsprechende Studien wurden etwa publiziert von David M. Wulff, Eugene d’Aquili, C. Daniel Batson, Patricia Schoenrade, W. Larry Ventis, Michael A. Persinger, K. Dewhurst, A. W. Beard, James J. Austin und Andrew Newberg.

Evolutionsforscher wie der Biologe Richard Dawkins und die Psychologin Susan Blackmore stellten die Theorie der Meme auf, und versuchten damit das Phänomen Religion zu erfassen. Dawkins bezeichnet 1991 eine Religion als Gruppe von Ideen und Denkmustern, die sich gegenseitig bestärken und gemeinsam auf ihre Verbreitung hinwirken (Memplex). Grundlage dieser Einordnung bildet die Beobachtung, dass durch Religionen Handlungen und Überzeugungen erfolgreich verbreitet werden können, die außerhalb ihres religiösen Kontexts sinnlos scheinen oder im Gegensatz zur objektiven Realität stehen. Voraussetzung zur Verbreitung von religiösen Gedanken ist laut Dawkins die Bereitschaft zur wörtlichen Weitergabe von Glaubenssätzen und zur Befolgung der in ihnen codierten Anweisungen. Er vergleicht diese Vorgänge mit den Mechanismen, durch die Viren einen befallenen Organismus zur Weiterverbreitung ihres eigenen Erbguts anregen. In Analogie zu Computerviren spricht er auch von „Viren des Geistes“.

Einige Autoren ziehen aus den in unterschiedlichen Kulturen beobachteten Vorstellungen von übernatürlichen Akteuren auch empirische Rückschlüsse auf zugrunde liegende Verarbeitungsprozesse im menschlichen Gehirn. Nach einer aus völkerkundlichen Untersuchungen abgeleiteten Hypothese postuliert z. B. Pascal Boyer, dass das Gehirn Sinneseindrücke mit Hilfe verschiedener Module verarbeite. Eines dieser Module sei darauf spezialisiert, aus Veränderungen in der Umwelt auf die Anwesenheit von Lebewesen zu schließen. Ein solches „Lebewesenerkennungsmodul“ sollte überempfindlich arbeiten, da es dem Überleben meist dienlicher sei, z. B. einen Windhauch irrtümlich als Raubtier zu interpretieren, als ein tatsächlich vorhandenes zu übersehen. Dadurch könnten im Gehirn aus unklaren Wahrnehmungen leicht Vorstellungen von übernatürlich erscheinenden Akteuren, wie etwa Geistern oder Göttern, entstehen.

Auch die Forschungen zur Willensfreiheit bzw. die Annahme einer absoluten Determination des menschlichen Geistes haben Einfluss auf die Erklärungsversuche hinsichtlich religiöser Vorstellungen und Praktiken.

Speziell die Religionspsychologie bearbeitet die Frage, ob allgemein eine Korrelation zwischen Religion und Gesundheit bzw. Lebensdauer eines Individuums besteht. Forschungen in den USA belegen mehrheitlich diese These, während europäische Studien eine solche Verknüpfung häufig nicht finden.

Die amerikanischen Studien von Newberg und d’Aquili belegen etwa, dass religiöse Menschen gesünder und glücklicher seien, länger lebten und sich schneller von Krankheiten und Operationen erholen würden. Als mögliche Ursache geben sie die sicherheitsstiftende und damit stressmindernde Wirkung der Religion, sowie das Zurechtfinden in einer furchteinflössenden Welt an. B. Clark und R. Lelkes führen zudem eine größere Lebenszufriedenheit an, die durch geringer Aggressionsneigung und höherer Sozialkompetenz entstünde.

Verschiedene Kriterien und Begriffe zur Beschreibung religiöser Phänomene liegen vor. Viele von ihnen sind allerdings selbst Produkte religiöser Sichtweisen und damit für das Beschreiben religiöser Phänomene auf wissenschaftlicher Grundlage von umstrittenem Wert. So beschreibt beispielsweise „Synkretismus“ die Vermischung religiöser Ideen, bezeichnet jedoch ursprünglich auch das Übersehen logischer Widersprüche und ist als Kampfbegriff verwendet worden. Dennoch gelten sie (vor allem in der Religionsphänomenologie) als zu vergleichenden Zwecken wertvoll.

Auf die Philosophen der griechischen Antike Platon und Aristoteles gehen Vorstellungen im Zeitalter der Aufklärung über Natürliche Religion (bzw. Natürliche Theologie) zurück, die als Ursprung der geschichtlichen, mit Fehlern behafteten, Religionen gesehen wurde. Dagegen vertrat u. a. Friedrich Schleiermacher die These, dass es sich dabei um Abstraktionen vorhandener Religionen handelt. Diese Auffassung hat sich in der modernen Religionswissenschaft durchgesetzt.

Besonders in der christlich-protestantischen Theologie wird im 20. Jahrhundert nach Karl Barth oft Glaube gegen Religion abgegrenzt. Barth sah Religion als eigenmächtigen Weg des Menschen zu Gott an und betonte, eine Erkenntnis des Willens Gottes gebe es nur im Glauben an Jesus Christus. Das Hören auf das Evangelium sprenge alle menschlichen Begriffe von Gott, alle ethischen Irrwege.

Dietrich Bonhoeffer übernahm die Unterscheidung und radikalisierte sie in seiner Frage nach einem Christentum ohne Religion, grenzte sich jedoch von einem „Offenbarungspositivismus“ Barths ab. Gerhard Ebeling betonte ebenfalls die kritische Kraft des Glaubens gegen religiöse Festlegungen und Sicherheiten, sah aber Religion als Lebensbedingung des Glaubens an.

Die weitgefächerten Bandbreiten des Theismus schließen den Deismus, den Polytheismus, den Pantheismus bzw. Pandeismus und den Panentheismus bzw. Panendeismus mit ein, es werden gleichzeitig Überschneidungen und Abgrenzungen zum Agnostizismus und Atheismus beschrieben.

Religionen, deren Anhänger mehrheitlich an die eigene Verpflichtung, nur einem einzigen höchsten Gott ihre Verehrung zu erweisen, glauben, werden als monotheistisch bezeichnet. Damit ist nicht zwingend eine Annahme der Nichtexistenz anderer Götter verbunden, sondern eventuell auch ein Werturteil, eine Unterscheidung zwischen dem einen wahren Gott und den verschiedenen falschen Göttern ("siehe auch:" Schirk im Islam).

Solche, die von der Existenz mehrerer Götter ausgehen und ihnen eine Bedeutung für bzw. einen Einfluss auf ihr Leben zugestehen, werden polytheistisch genannt.

Vorstellungen, denen zufolge das Göttliche bzw. Gott mit der Gesamtheit der Welt (dem Universum) identisch (und in der Regel nicht persönlich) ist, werden als pantheistisch bezeichnet.

Für einige Forscher, z. B. Ray Billington, gelten Religionen wie der Buddhismus, deren tradierte Vorstellungen und Riten im Kern nicht auf ein oder mehrere Götter ausgerichtet sind, in gewissem Sinn als atheistisch. Als Beispiele werden der Jainismus und der Buddhismus angeführt. Die meisten lehnen es jedoch ab, diesen Begriff auf Weltanschauungen anzuwenden, in denen die Frage nach Gott keine Rolle spielt.

Häufig vermitteln Religionen eine Vorstellung, wie die Welt entstanden ist (eine Schöpfungsgeschichte oder Kosmogonie) und ein Bild der letzten Dinge, eine Eschatologie.

Dazu gehören auch Antworten auf die Frage, was mit dem Menschen nach dem Tod geschieht. (Siehe auch: Seele.) Viele Religionen postulieren ein Dasein nach dem Tod und machen Aussagen über die Zukunft der Welt. Themen wie Reinkarnation, Nirwana, Ewigkeit, Jenseits, Himmel oder Hölle, und was mit der Welt geschehen wird (Weltuntergang, Apokalypse, Ragnarök, Reich Gottes), sind in vielen Religionen zentral.

Die meisten Religionen kennen Personengruppen, die die Religion überliefern, lehren, ihre Rituale ausführen und zwischen Mensch und Gottheit vermitteln. Beispiele sind Seher oder Propheten, Priester, Prediger, Geistliche, Mönche, Nonnen, Magier, Druiden, Medizinmänner oder Schamanen. Manche Religionen sprechen einzelnen dieser Menschen übernatürliche Eigenschaften zu.

Der Status dieser Personen variiert stark. Sie können innerhalb einer formellen Organisation tätig oder unabhängig sein, bezahlt oder unentgeltlich, können auf verschiedene Weise legitimiert sein und unterschiedlichsten Verhaltenskodizes unterliegen.

In einigen Religionen werden die religiösen Rituale vom Familienoberhaupt durchgeführt oder geleitet. Es gibt auch Religionen ohne spezifisch autorisierten Vermittler zwischen dem Übernatürlichen und dem Menschen.

Häufig pflegen Religionen und Konfessionen eine eigene Art von Spiritualität. Spiritualität – ursprünglich ein christlicher Begriff – bezeichnet das geistliche Erleben und den bewussten Bezug zum jeweiligen Glauben im Gegensatz zur Dogmatik, die die festgesetzte Lehre einer Religion darstellt. Im heutigen westlichen Sprachgebrauch wird Spiritualität häufig als seelische Suche nach Gott oder einem anderen transzendenten Bezug betrachtet, ob im Rahmen von spezifischen Religionen oder jenseits davon. Häufig synonym verwendet wird der Begriff der Frömmigkeit, der jedoch heute eher im kirchlichen Kontext verwendet wird und zudem oft eine negative Konnotation im Sinne einer übertrieben bedingungslosen Hinwendung zur Religion hat. In einigen Religionen finden sich Strömungen, deren Anhänger die Begegnung mit der Transzendenz oder dem Göttlichen in mystischen Erfahrungen finden.

Zu den religiösen Riten im weiteren Sinne gehören unter anderem Gebet, Meditation, Gottesdienst, religiöse Ekstase, Opfer, Liturgie, Prozessionen und Wallfahrten. Darüber hinaus zählen dazu beispielsweise auch im Alltag gelebte Frömmigkeit wie das Geben von Almosen, Barmherzigkeit oder Askese.

Auch einige atheistisch-säkulare Weltanschauungen bedienen sich religiös anmutender Rituale. Beispiele sind die aufwändig inszenierten Aufmärsche und Feiern in sozialistischen oder faschistischen Staaten wie auch die zumindest zeitweilig in ihnen praktizierten (An-)Führerkulte. Die These, dass scheinbar nichtreligiöse Systeme sich religiöser Formen bedienen, wird wissenschaftlich diskutiert ("siehe auch:" Politische Religion, Zivilreligion, Staatsreligion bzw. Religio Athletae.).

Vielfach ist es in der Geschichte der Religionen zu Schismen (Spaltungen) gekommen. Neue Religionen entstehen in der Regel durch die Abtrennung einer Gruppe aus einer älteren Religionsgemeinschaft.

Der Begriff Synkretismus beschreibt das Vermischen von Praktiken verschiedener Religionen. Es kann sich hierbei um den Versuch handeln, ähnliche Religionen (wieder) zu vereinen oder die Schaffung einer neuen Religion aus unterschiedlichen Vorgängern zu initiieren.

Insbesondere im Deutschen wird zwischen „Religion(en)“ und „Religiosität“ unterschieden. Während "eine Religion" die religiöse Lehre "(→ Dogma)" und die zugehörige Institution bezeichnet, bezieht sich "Religiosität" auf das subjektive religiöse Empfinden (Ehrfurcht vor dem „Großen Ganzen“, transzendente Welterklärung) und Wünschen (Erleuchtung, Religionszugehörigkeit) des Einzelnen. Für Johann Gottfried Herder war Religiosität der Ausdruck für das echte religiöse Gefühl. Im christlichen Kontext wir Religiosität häufig mit Glaube gleichgesetzt.

In der deutschen Religionsgeschichte betonten vor allem die Romantik und der Pietismus die innere Haltung des Gläubigen. Der protestantische Theologe Friedrich Schleiermacher etwa schrieb in seiner Schrift "Über die Religion" (1799): „Religion ist nicht Metaphysik und Moral, sondern Anschauen und Gefühl.“

Die Betonung des Gefühls ist auch für die mehr als 100 Jahre später vorgelegte Religionsauffassung des nordamerikanischen Philosophen und Psychologen William James kennzeichnend: In seinem Werk "The varieties of religious experience" (1902) vermeidet der Pragmatiker eine allgemeine Definition des Begriffs Religion. Nach James ist religiöse Wahrheit nichts Übergeordnetes, dem Menschen Entzogenes. Sie zeigt sich vielmehr im Erleben des religiösen Menschen und wird durch das religiöse Gefühl erfahren, das sich durch die Verbindung mit einem religiösen Objekt bildet. Aufgrund der unterschiedlichen Gefühle, die von der Gewissheit der transzendentalen Bedeutung von Wörtern und Wahrnehmungen bis hin zum mystischen Gefühl der Verbundenheit mit dem Kosmos reichen, lassen sich verschiedene Formen von Religiosität beschreiben. Das tiefe religiöse Erleben übersteige einfache Moralvorstellungen. Für James sind erkenntnistheoretische Fragen und solche der Methodik sekundär. Er stützt seine Arbeiten allein auf Beschreibungen und Systematisierung religiöser Gefühle.

James’ Werk war bedeutsam für die Entwicklung der frühen Religionswissenschaft Anfang des 20. Jahrhunderts. Während etwa Ernst Troeltsch versuchte, James' Beschreibungen für seine Theorie nutzbar zu machen, wurde James Ansatz von Religionspsychologen wie Wilhelm Wundt und Karl Girgensohn heftig kritisiert.

Der evangelische Theologe und Religionsphilosoph Rudolf Otto geht in seinem 1917 veröffentlichten Hauptwerk "Das Heilige" davon aus, dass es eine besondere Anlage (sensus numinis) für das religiöse Gefühl gibt. Habe ein Mensch diese Anlage nur schwach ausgeprägt oder gar nicht, sei dieser als Religionskundler kaum geeignet. Das religiöse Gefühl sei von anderen Empfindungen zu unterscheiden, gleichwohl sei es möglich, Parallelen zum Erleben anderer Gefühle zu ziehen (z. B. ästhetische Gefühle). Otto nennt vier Momente, die für das Erleben des religiösen Gefühls typisch sind: Das Tremendum = Das Schauervolle, Das Majestas = Das Übermächtige, Das Energische = Die Kraft, Der Wille, Das Mysterium = Das „Ganz Andere“. Diese Momente ziehen sich durch die gesamte Religionsgeschichte. Ferner geht er davon aus, dass das religiöse Erleben, das zunächst affekthaft („irrational“) erfahren, nie „begriffen“, wohl aber durch kognitive Prozesse („Rationalisierung“) „versittlicht“ wird.

Auch Rudolf Ottos auf Gefühlen beruhendes Konzept des Heiligen hat damals eine lebhafte Diskussion ausgelöst. Während sich der Religionswissenschaftler Gustav Mensching in seiner Toleranzidee der Religionen von Otto anregen ließ, wurde Ottos Theorie von dem Wundt-Schüler Willy Hellpach als parapsychologisch verworfen. Heute spielt Otto in der Religionswissenschaft praktisch keine Rolle mehr. Vorstellungen vom Numinosen werden aber nach wie vor u. a. bedingt durch die tiefenpsychologischen Konzepte von Erik H. Erikson und C. G. Jung in alternativen Richtungen der Psychologie (z. B. transpersonale Psychologie) aufgenommen.

Seit der Aufklärung wird – vor allem im westlichen Kulturkreis – zwischen institutionalisierter Religion und persönlicher Haltung zum Transzendenten unterschieden. Hierdurch wird die individuelle Ausformung der Religiosität des Einzelnen begünstigt. Daneben gibt es zunehmend Formen von Religion, die sich nur wenig auf den Lebensstil der Anhänger auswirken, weil diese nur zu bestimmten Gelegenheiten religiöse ‚Dienstleistungen‘ in Anspruch nehmen. Hierzu gehören auch Ansätze, nach denen Gruppen oder Individuen Ideen, Rituale usw. aus Religionen und anderen Weltanschauungen, u. a. esoterischen neu zusammenstellen und auf ihre Bedürfnisse zuschneiden. Dieses eklektizistische Vorgehen wird von Vertretern traditioneller Religionen zuweilen „Patchwork-Religion“ oder „Supermarkt der Weltanschauungen“ genannt.

Zahlreiche alte Religionen hatten den Anspruch, menschliches Zusammenleben durch Gesetze zu regeln. Die meisten Religionen der Gegenwart haben ein ethisches Wertesystem, dessen Einhaltung sie fordern.

Dieses System von Wertvorstellungen umfasst Ansichten darüber, was richtig und falsch und was gut und böse ist, wie ein Angehöriger der jeweiligen Religion zu handeln und teilweise, wie er zu denken hat. Dem liegt zumeist eine bestimmte Auffassung über die Welt, die Natur und die Stellung des Menschen zugrunde. Obgleich sich diese Anschauungen historisch wandeln, stehen hinter solchen religiösen Pflichten in fast allen Religionen ähnliche ethische Prinzipien. Diese sollen das konfliktarme Miteinander der Mitglieder der Religionsgemeinschaft regeln, die Gesellschaft und zum Teil die Politik im Sinne der Religion beeinflussen und die Menschen individuell dem jeweiligen religiösen Ziel näher bringen. Zudem bieten sie für den Einzelnen einen moralischen Rahmen, der ihn psychisch und physisch stabilisieren kann, zu individueller und kollektiver Hilfsbereitschaft anhalten oder sogar zu gesellschaftlichen Verbesserungen beitragen kann.

Alle Weltreligionen und die meisten kleineren Religionen fordern Barmherzigkeit von ihren Mitgliedern. So ist im Islam z. B. vorgeschrieben, dass jeder einen festen Anteil seines Einkommens für soziale Zwecke spenden soll (Zakat). Im christlich geprägten Mittelalter hat die römisch-katholische Kirche Universitäten und Schulen gegründet, Hospitäler und Waisenhäuser unterhalten und für die Armenspeisung gesorgt. Ein Aspekt von Religion kann der Frieden stiftende sein, der in den meisten Religionen durch besondere Vorschriften über Mitgefühl, Vergebung oder sogar Feindesliebe Ausdruck findet.

In einigen Religionen sollen diese moralischen Gesetze der jeweiligen Überlieferung nach direkt dem Religionsstifter von der entsprechenden Gottheit überbracht worden sein und somit höchste Autorität besitzen. (Offenbarungsreligionen). Nach dieser Vorstellung sollen sich auch weltliche Herrscher den jeweiligen ethischen Anforderungen beugen. Gehorsam wird jeweils unter Androhung von diesseitigen oder jenseitigen Strafen gefordert oder als einziger Weg zum Heil dargestellt. Auch Apostasie kann je nach Auslegung der Religion bestraft werden.

Häufig existieren noch weitere Regeln, die nicht direkt vom Stifter der Religion stammen, sondern aus den heiligen Schriften und anderen Tradierungen der jeweiligen Religion abgeleitet werden (z. B. Talmud, Sunna). Einige dieser Normen verloren im Laufe der historischen Entwicklung für viele Gläubige ihren Sinn und wurden in einigen Fällen den sehr unterschiedlichen Wertesystemen der entsprechenden Zeit angepasst. (Vgl. Reformjudentum.)

Wie in allen Weltanschauungen gibt es auch in den Religionen einen Widerspruch zwischen theoretischem Anspruch und praktischer Umsetzung. Während Machtmissbrauch und andere Missstände im Mittelalter und der frühen Neuzeit häufig zu Schismen und religiösen Erneuerungsbewegungen führten, haben sie gegenwärtig vielfach eine Abkehr von der Religion insgesamt zur Folge. Parallel zu Reformbestrebungen kommt es aber auch zu fundamentalistischen religiösen Interpretationen und Praktiken, die bis hin zu terroristischen Aktivitäten mit pseudoreligiöser Begründung reichen.

Die stärkste Form des Versagens ethischer religiöser Normen stellen Religionskriege und andere Gewalttaten dar, die mit religiösen Auffassungen begründet werden. Dies werten Gläubige zumeist als Missbrauch ihrer Religion, während Religionskritiker von einer allen Religionen immanenten Tendenz zu Fanatismus und Grausamkeit ausgehen. Überdies ist umstritten, ob diese Geschehnisse notwendige Folge von Religionen sind.

Die Römisch-Katholische Kirche war für die Inquisition verantwortlich. Verbrechen im Namen der christlichen Religion waren die Kreuzzüge, die Hexenverfolgung, die Judenverfolgung, gewalttätige Formen der Missionierung oder religiös verbrämte, eigentlich politische Gräueltaten, wie die Tötung zahlreicher sogenannter Indios, Angehöriger indigener Völker Südamerikas während der Eroberung und in der Neuzeit teilweise die Unterstützung von Diktaturen und die ambivalente Rolle der Kirchen in der Zeit des Nationalsozialismus. Allerdings fanden sich bei all diesen Ereignissen auch immer wieder Kritiker aus den eigenen Reihen. Der Kirchen- und Religionskritiker Karlheinz Deschner hat in seinem auf zehn Bände angelegten Werk Kriminalgeschichte des Christentums ab 1986 eine Fülle historischen Materials zu diesem Thema ausgewertet und kommentiert.

Auch in jüngerer Zeit sind Gewalttaten partiell mit Religion verbunden: So werden seit der Begründung eines Gottesstaates, der "Islamischen Republik Iran", Tausende von Menschen wegen sogenannter „Verbrechen gegen die Religion“ im Rahmen eines Rechtssystems, das auf einer speziellen Interpretation der Scharia beruht, inhaftiert, gefoltert und häufig sogar (medienwirksam öffentlich) hingerichtet. Frauen werden schon wegen einer Nichteinhaltung von Bekleidungsvorschriften bestraft, wegen „moralischer Vergehen“ in seltenen Fällen gesteinigt. Auch die Religionsgemeinschaft der Baha’i (und z. B. Homosexuelle) werden strafrechtlich und von den „Religionswächtern“ verfolgt. In Indien gibt es zunehmend Ausschreitungen von radikalen Hindus, vor allem gegenüber Muslimen.

Die praktizierte Ethik im Judentum, Christentum und im Islam unterscheidet sich unter anderem dadurch, ob die jeweilige Religion mit einem weiten individuellen Denk- und Handlungsspielraum, traditionell oder fundamentalistisch ausgelegt wird. Auch innerhalb der einzelnen Religionen gibt es unterschiedliche Schulen, welche die jeweilige Morallehre verschieden interpretieren und anwenden. So gibt es z. B. im Christentum Strömungen, die das Alte Testament aufgrund der darin sehr gewalttätig wirkenden Gottheit gering schätzen.

Die drei bedeutendsten Offenbarungsreligionen verbindet in ihren ethischen Systemen der Gedanke an eine Endzeit, allerdings ist das Judentum weniger jenseitsbezogen als die beiden anderen Religionen. Dieses lineare Verständnis von Zeit bedeutet, dass die Gläubigen im Diesseits nach den von ihrer Gottheit geforderten Regeln leben, um den Lohn dafür in einer späteren Zeit zu erhalten, wobei Gott auch im Diesseits schon wirken kann. Allerdings wird im Protestantismus zumeist die göttliche Gnade für ausschlaggebend gehalten, auch unabhängig von der Befolgung moralischer Postulate. Judentum und Islam haben mehr Rechtscharakter und ein umfassenderes System von rituellen Ge- und Verboten als das Christentum, was sich z. B. im hebräischen Wort für Religion, Tora (Gesetz), widerspiegelt. Ähnlich wie im Hinduismus gibt es genaue Anweisungen, wie die Handlungsweisen in der Gruppe sein sollen. In den christlichen Religionen sind heute, anders als im Römisch-Katholischen Mittelalter, u. a. durch Interpretationen biblischer Überlieferungen von Aussagen ihres Stifters, neuplatonische Einflüsse und Auswirkungen der Aufklärung in vielen Strömungen weniger rituelle Ge- und Verbote vorgegeben.

Grundlegend für die jüdische Ethik sind die Thora, der Hauptteil der hebräischen Bibel, der Talmud – besonders die in ihm enthaltenen Pirkej Avot sowie die Halacha, ein seit 1500 Jahren stetig weiterentwickeltes Korpus von rabbinischen Aussagen. Auch heute noch wird die jüdische Ethik durch Äußerungen von Rabbinern der verschiedenen Richtungen des Judentums weiterentwickelt.

Zentral für die jüdische Ethik ist eine Stelle über die Nächstenliebe aus Levitikus (3. Buch Mose) 19, 18, die in deutscher Übersetzung etwa lautet: „Liebe deinen Nächsten, denn er ist wie du“. Weite Teile des Talmuds und auch vieles in der Tora sind Erläuterungen zur konkreten Umsetzung dieser Nächstenliebe.

Die jüdische Ethik ist ein zentraler Teil der jüdischen Philosophie. Insgesamt lässt sich keine allgemeine „jüdische Auffassung“ zu zeitgebundenen ethischen Fragen erkennen. Sehr unterschiedliche Antworten auf solche Fragen finden sich im Ultraorthodoxen Judentum, Orthodoxen Judentum, Konservativen Judentum, Jüdischen Rekonstruktivismus und Liberalen Judentum.

Die christlichen Hauptrichtungen (Orthodoxe, Römisch-katholische und Protestantische Kirche) – wie auch andere christliche Gemeinschaften – fordern, dass der christliche Glaube mit einer moralischen Lebensführung verbunden wird. Dabei ist die Bandbreite dessen, was jeweils darunter zu fassen ist, auch innerhalb einer christlichen Religionsgemeinschaft bzw. Kirche häufig sehr groß. In der Theologie wird zwischen theoretischer Ethik und ihrer Umsetzung unterschieden. Es gibt gewisse Überschneidungen mit der biblischen Ethik, jedoch ist das Feld der christlichen Ethik weiter gefasst.

In der christlichen Ethik existieren vorrangig zwei theoretische Positionen: der christlich teleologische Ansatz und der deontologische, d. h. die Pflichtenlehre, wobei häufig beide mit unterschiedlicher Gewichtung miteinander verbunden werden. Die Teleologie erörtert die Frage nach dem Sinn und Zweck, z. B. nach dem „Guten“, „Wahren“ oder nach dem „Ende“, das Christen erstreben sollen (in einigen christlichen Konzepten ist dies die „Vereinigung mit Gott“), während die christliche Deontologie Moral als Pflicht begreift, Gesetze oder andere religiöse Verordnungen zu erfüllen, vor allem die aus dem alttestamentlich-jüdischen Glauben übernommenen Zehn Gebote.

Die Ethik im Islam ist ähnlich wie im Judentum sehr stark an Gebote für fast alle Lebensbereiche gebunden. Der Koran gibt genaue Anweisungen für die Handlungen des Einzelnen in der Gruppe. Wichtig für den Islam ist eine kollektive Verantwortung für Gut und Böse. Dies wird beispielsweise in der Anweisung "Das Rechte gebieten und das Verwerfliche verbieten" deutlich. In Folge besteht die Möglichkeit einer unumschränkten Befehlsgewalt der Gemeinschaft (siehe auch Hisbah). Der Islam geht in seinen Hauptrichtungen Sunniten und Schiiten von der Prädestination (Vorherbestimmung) aus, die dem Individuum nur begrenzten Handlungsspielraum zugesteht. In fundamentalistisch ausgerichteten Staaten hat die Scharia als islamisches Recht eine wesentliche Bedeutung.

In den Religionen indischen Ursprungs wie dem Buddhismus, Hinduismus, Sikhismus und Jainismus besteht eine direkte Verbindung zwischen dem ethischen bzw. unethischen Verhalten einer Person und dessen Rückwirkungen im gegenwärtigen Leben und in künftigen Leben (Reinkarnation) bzw. in einer künftigen jenseitigen Existenz. Dieser Zusammenhang ergibt sich nicht indirekt durch das Eingreifen einer richtenden, belohnenden und strafenden göttlichen Instanz, sondern wird als naturgesetzlich aufgefasst. Eine Tat gilt als unweigerlich mit ihrer positiven oder negativen Auswirkung auf den Handelnden verknüpft (Karma-Konzept). Daher werden die ethischen Regeln an einer angenommenen universalen Gesetzmäßigkeit bzw. einem Weltprinzip ausgerichtet, das im Hinduismus, Buddhismus und Jainismus Dharma genannt wird. Aus diesem Prinzip werden detaillierte ethische Anweisungen abgeleitet.

Von den Anhängern der Religionsgemeinschaften wird erwartet, die Gesetzmäßigkeiten des Daseins zu erkennen und entsprechend zu handeln. In manchen Fällen sanktioniert die Gemeinschaft Verstöße gegen die Regeln, doch weit wichtiger sind für das Individuum die angenommenen negativen Konsequenzen von Übeltaten in einer künftigen diesseitigen oder jenseitigen Existenzform.

Gemeinsam ist diesen Religionen der Ansatz, den mentalen Ursachen unerwünschter Handlungen konsequent nachzuforschen, um sie in einem möglichst frühen Stadium beeinflussen zu können.

Eine zentrale Rolle spielt in der Ethik dieser Religionen die Auseinandersetzung mit der Gewaltfrage. Gemeinsam ist ihnen ein grundsätzliches Bekenntnis zum Ideal der Gewaltlosigkeit (Ahimsa, „Nicht-Gewalt“). Da kein prinzipieller Unterschied zwischen dem Menschen und anderen Lebensformen gemacht wird, erstreckt sich die Forderung der Gewaltlosigkeit auch auf den Umgang mit Tieren und zumindest theoretisch sogar mit Pflanzen. Eine konsequente Umsetzung des Gewaltlosigkeitsideals scheitert jedoch an dem Erfordernis, das eigene Überleben auf Kosten anderer Lebensformen zu sichern und gegen Angriffe zu verteidigen. Daraus ergibt sich die Notwendigkeit von Konzessionen und Kompromissen, die in den einzelnen Religionen bzw. Strömungen unterschiedlich ausfallen. Die Frage nach der Zulässigkeit defensiver Gewalt und nach ihrer Legitimierung im Einzelfall wurde und wird kontrovers diskutiert.

Sehr unterschiedlich ist auch die Umsetzung in der Praxis der Religionsanhänger. Entgegen einem im Westen verbreiteten Irrtum verbietet weder der Hinduismus noch der Buddhismus oder der Jainismus die Anwendung militärischer Gewalt unter allen Umständen. Daher sind Kriege, die von Anhängern dieser Religionen geführt wurden und werden, nicht notwendigerweise Verstöße gegen religiöse Pflichten. Außerdem gelten in allen Gemeinschaften für Mönche und Nonnen weit strengere Ethikmaßstäbe als für Laienanhänger.

Eine andersartige Position nimmt der in China entstandene Daoismus ein. Er nimmt mit dem Dao zwar auch ein universales Weltprinzip an, aber das heilige Buch Daodejing, betont, dass die Weltordnung hinsichtlich des ethischen oder unethischen Verhaltens der Individuen indifferent sei. „Himmel und Erde“ belohnen weder gute Taten noch bestrafen sie schlechte, das Dao richtet sich nicht nach menschlichen Vorstellungen von Gut und Böse. Ethisches Verhalten ergibt sich aus der Sicht des Daoismus nicht aus Tugendlehren, wie sie der Konfuzianismus vertritt, sondern unmittelbar aus einem spontanen Impuls des autonomen Individuums, das seiner eigenen Natur folgt, soweit es nicht von außen daran gehindert wird. Trotzdem gibt es im späteren religiösen Daoismus auch ethische Lehren. Der Alchemist Ge Hong beispielsweise vertrat konfuzianische Tugenden und der Quanzhen-Daoismus übernahm eine buddhistische Ethik für Mönche und Nonnen. Desgleichen sind die Priester des Daoismus, die Daoshi, angehalten, die Reinheit von Denken und Glauben zu praktizieren und ein integres Leben zu führen. Der Daoismus missbilligt Kriegführung, verwirft sie aber nicht absolut im Sinne eines radikalen Pazifismus.

Näheres zu den einzelnen Religionen findet sich in den folgenden "Hauptartikeln":

Auch indigene Kulturen weisen verschiedenste Moralsysteme auf, welche die Gemeinschaft schützen sollen. Da nur durch ein funktionierendes Sozialbewusstsein das Überleben der Gruppe gesichert werden kann, bildet die Übung prosozialen Verhaltens den Schwerpunkt mündlich weitergegebener Überlieferungen. Häufiger als in komplexeren Kulturen werden unmittelbare übernatürliche Konsequenzen von Regelverstößen erwartet.

Zahlreiche Kunstformen hatten zunächst eine religiöse Hauptbestimmung und verselbstständigten sich erst danach. Klassisches Beispiel ist das abendländische Theater, das aus dem antiken athenischen Dionysos-Kult hervorgegangen ist.

Religionen haben oftmals einen zentralen Einfluss auf die Kulturproduktion einer Gesellschaft und die Entwicklung der bildenden, darstellenden und angewandten Künste ausgeübt. Wie weit dieser Einfluss sich tatsächlich auf die künstlerische Arbeit auswirkt, hängt stark von dem Wertesystem und bestimmten theologischen Konzepten der jeweiligen Religion ab. In der Bildverehrung wird das oftmals künstlerisch ausgestaltete Abbild religiöser Szenen oder Motive zu einem wesentlichen Bestandteil des religiösen Ritus und führt mitunter zu der Ausbildung spezifischer künstlerischer Traditionen. Ebenso wirkt die Ausgestaltung von Sakralbauten und Kultgegenständen (wie Reliquiaren oder sakralem Gerät) auf die Entwicklung architektonischer und handwerklicher Traditionen ein, die später mitunter in die säkulare Kunst übernommen werden.

In vielen Religionen waren und werden den Künsten zugleich thematische oder formale Restriktionen auferlegt. Dies kann daraus abgeleitet werden, dass bestimmte Kunstformen – vor allem der bildenden und der darstellenden Kunst – als zu weltlich empfunden werden, aber auch aus spezifischen theologischen Konzepten, am strengsten im Bilderverbot mancher Religionen; dies kam nicht selten der Ausbildung des Ornaments und der Kalligrafie zugute. Kunstwerke können sogar als blasphemisch verteufelt werden. Die Ablehnung bekämpfter religiöser Anschauungen führte zum Beispiel zum byzantinischen Ikonoklasmus, zum Reformatorischen Bildersturm oder zur Sprengung der Buddha-Statuen von Bamiyan durch die Taliban.

Seit in der Neuzeit in vielen Gesellschaften die Kunst weitgehend als autonom betrachtet wird und Kunstwerke gemäß der Kunstfreiheit behandelt werden, kommt es immer wieder zu Fällen, in denen Werke durch die Vertreter einer Religion als anstößig empfunden und zu symbolischen Auslösern gesellschaftlicher oder sogar politischer Konflikte werden.

So wie Religion ethische Urteile beeinflusst, so beeinflussen religiöse Überzeugungen auch das Rechtssystem. Beispielsweise stellte der Bundesgerichtshof in einem Gutachten von 1953 fest, dass die Familie „von Gott gestiftet“ sei. Im gleichen Gutachten erklärte das Gericht den Mann kraft Schöpfung zum Oberhaupt der Familie und die Frau zuständig für die Kinder. Die Rechtsprechung wurde dadurch direkt durch religiöse Überzeugungen bestimmt. Auch wenn der Einfluss der Religion auf das Recht durch die Säkularisierung in den meisten westlichen Ländern weitgehend reduziert wurde, so ist die Rechtsprechung in vielen traditionelleren Gesellschaften immer noch massiv von religiösen Werten bestimmt. Ein prominentes Beispiel ist die Scharia in muslimischen Ländern.

Das Institut für Trend- und Wirtschaftsforschung (ITZ) behauptet: „Religion wirft ihre archaische Verwurzelung in der Kirche und im Glauben ab und wird als spiritueller Erlebnismarkt wiedergeboren. Religion im postmodernen Gewand des irgendwie Spirituellen wird deshalb tendenziell zu einer Frage der persönlichen Lebensstilentscheidungen. Religion wird individualisiert, personalisiert und nach innen gekehrt: My personal Jesus, mein Gott für bestimmte Lebenslagen. Durch diesen sozialgeschichtlich dramatischen Wandel wird Spiritualität zu einem Wachstumsmarkt, auf dem sich nicht mehr nur Amtskirche und Prediger tummeln, sondern die gesamte medialisierte Erlebnisgesellschaft des 21. Jahrhunderts.“







A. Karl R. Wernhart: "Ethnische Religionen – Universale Elemente des Religiösen." Topos, Kevelaer 2004, ISBN 3-7867-8545-7.


</doc>
<doc id="4225" url="https://de.wikipedia.org/wiki?curid=4225" title="Rhodium">
Rhodium

Rhodium ist ein chemisches Element mit dem Elementsymbol Rh und der Ordnungszahl 45. Es ist ein silberweißes, hartes, unreaktives Übergangsmetall. Im Periodensystem zählt es zusammen mit Cobalt, Iridium und Meitnerium zur 9. Gruppe oder Cobaltgruppe. Rhodium besitzt große Ähnlichkeit zu anderen Platinmetallen wie Platin oder Palladium. Dies betrifft beispielsweise die für Edelmetalle charakteristische geringe Reaktivität und eine hohe katalytische Aktivität.

Rhodium wird daher, oft in Form von Legierungen, vorwiegend als Katalysator eingesetzt. Als wichtiger Bestandteil von Fahrzeugkatalysatoren wird es zur Reduktion von Stickoxiden eingesetzt. Auch in industriellen Prozessen zur Herstellung einiger chemischer Grundstoffe, wie dem Ostwald-Verfahren zur Salpetersäure-Produktion, werden Rhodiumkatalysatoren genutzt. Da das Metall in der Natur sehr selten vorkommt und gleichzeitig eine breite Anwendung findet, zählt es zu den teuersten Metallen überhaupt.

Im menschlichen Körper kommt Rhodium normalerweise nicht vor, eine biologische Bedeutung ist nicht bekannt.

Rhodium wurde 1803 von William Hyde Wollaston in einem aus Südamerika stammenden Rohplatinerz entdeckt. Im gleichen Erz wurden von Wollaston und Smithson Tennant drei weitere Platinmetalle, Palladium, Iridium und Osmium, entdeckt. Dazu lösten sie das Erz zunächst in Königswasser. Es bildete sich eine lösliche Fraktion und ein schwarzer Rückstand, in dem Tennant Osmium und Iridium fand. Wollaston fällte aus der Königswasserlösung Rhodium und einige weitere Bestandteile mit Zinkpulver. Nach der Abtrennung von Kupfer und Blei mit verdünnter Salpetersäure, erneutem Lösen in Königswasser und Zugabe von Natriumchlorid bildete sich Na<nowiki>[</nowiki>RhCl<nowiki>]</nowiki> · n HO, das beim Verdunsten der Flüssigkeit als rosarotes Salz zurückblieb. Aus diesem konnte Wollaston durch Extraktion mit Ethanol und Reduktion mit Zink das elementare Rhodium gewinnen. Der Name wurde von Wollaston nach dem griechischen "rhodeos" („rosenrot“) gewählt, da viele Rhodiumverbindungen diese Farbe zeigen.

Die erste Anwendung des neuen Metalls war ab 1820 Spitzen von Schreibfedern, für die Rhodium-Zinn-Legierungen eingesetzt wurden. Diese wurden später jedoch durch härtere Osmium-Iridium-Legierungen abgelöst.

Rhodium ist nach Rhenium zusammen mit Ruthenium und Iridium eines der seltensten nicht radioaktiven Metalle in der kontinentalen Erdkruste. Sein Anteil beträgt nur 1 ppb. Rhodium kommt in der Natur gediegen vor und ist daher als eigenständiges Mineral anerkannt. Fundorte sind unter anderem die Typlokalität Stillwater in Montana und Goodnews Bay in Alaska. Rhodium ist unter anderem mit anderen Platinmetallen und Gold vergesellschaftet.

Neben dem elementaren Rhodium sind auch einige Rhodiumminerale wie Bowieit, Genkinit oder Miassit bekannt. Diese sind jedoch wie das elementare Rhodium sehr selten und spielen für die Gewinnung keine Rolle. Die wichtigsten Vorkommen des Elements liegen in sulfidischen Nickel-Kupfer-Erzen, die vor allem in Südafrika, Sudbury (Kanada) und Sibirien vorkommen. Auch in mexikanischen Goldlagerstätten kommt Rhodium in nennenswerter Menge vor. Rhodium fällt zusammen mit den anderen Platinmetallen beim Verarbeiten dieser Erze an und muss anschließend von diesen getrennt werden.

Die Gewinnung von Rhodium ist wie die der anderen Platinmetalle sehr aufwändig. Dies liegt vor allem an der Ähnlichkeit und geringen Reaktivität der Platinmetalle, wodurch sie sich schwer trennen lassen. Ausgangsstoff für die Gewinnung von Rhodium ist Anodenschlamm, der bei der Kupfer- und Nickelproduktion als Nebenprodukt bei der Elektrolyse anfällt. Dieser wird zunächst in Königswasser gelöst. Dabei gehen Gold, Platin und Palladium in Lösung, während Ruthenium, Osmium, Rhodium und Iridium sowie Silber als Silberchlorid ungelöst zurückbleiben. Das Silberchlorid wird durch Erhitzen mit Bleicarbonat und Salpetersäure in lösliches Silbernitrat umgewandelt und so abgetrennt.

Um das Rhodium von den anderen Elementen abzutrennen, wird der Rückstand mit Natriumhydrogensulfat geschmolzen. Dabei bildet sich wasserlösliches Rhodium(III)-sulfat Rh(SO), das mit Wasser ausgelaugt werden kann. Das gelöste Rhodium wird zunächst mit Natriumhydroxid als Rhodiumhydroxid Rh(OH) gefällt. Die folgenden Reaktionsschritte sind das Lösen in Salzsäure als H<nowiki>[</nowiki>RhCl<nowiki>]</nowiki> und die Fällung mit Natriumnitrit und Ammoniumchlorid als (NH)<nowiki>[</nowiki>Rh(NO)<nowiki>]</nowiki>. Um elementares Rhodium zu erhalten, wird aus dem Rückstand durch Digerieren mit Salzsäure der lösliche (NH)<nowiki>[</nowiki>RhCl<nowiki>]</nowiki>-Komplex gebildet. Nachdem das Wasser durch Verdampfen entfernt wurde, kann das Rhodium mithilfe von Wasserstoff zum Metallpulver reduziert werden.

Rhodiumisotope entstehen als Nebenprodukte bei der Kernspaltung von U und können aus abgebrannten Brennelementen extrahiert werden. Aufgrund der Radioaktivität gibt es jedoch noch keine kommerzielle Anwendung des so erhaltenen Rhodiums.

Rhodium wird nur in geringem Umfang gewonnen, 2005 betrug die Produktion 23,5 Tonnen. 83,2 % der Gesamtproduktion fanden in Südafrika statt. Das zweitgrößte Produktionsland war Russland (11,9 %), gefolgt von Kanada und Simbabwe.

Rhodium ist ein silberweißes, hochschmelzendes, hartes Edelmetall. Es ist härter als Gold oder Platin, ist jedoch zäh und dehnbar und lässt sich durch Hämmern bearbeiten. In den meisten Eigenschaften ist es mit den anderen Platinmetallen vergleichbar. So liegt der Schmelzpunkt des Rhodiums von 1966 °C zwischen demjenigen von Platin (1772 °C) und Ruthenium (2334 °C). Die Dichte des Elements von 12,41 g/cm ist vergleichbar mit denen der benachbarten Elemente Ruthenium und Palladium. Rhodium besitzt die höchste Wärme- und elektrische Leitfähigkeit aller Platinmetalle. Unterhalb von 0,9 Kelvin wird Rhodium zum Supraleiter.

Rhodium kristallisiert wie Cobalt und Iridium in einer kubisch-dichtesten Kugelpackung (Kupfer-Typ) in der mit dem Gitterparameter a = 380,4 pm sowie vier Formeleinheiten pro Elementarzelle.

Als typisches Edelmetall ist Rhodium sehr reaktionsträge. Nach Iridium ist es das am wenigsten reaktive Platinmetall. Es reagiert mit Sauerstoff und Chlor erst bei Temperaturen von 600 bis 700 °C zu Rhodium(III)-oxid beziehungsweise Rhodium(III)-chlorid. Auch das reaktivste Halogen Fluor reagiert nur in der Hitze zu Rhodium(VI)-fluorid. Von Mineralsäuren wird das Metall nicht angegriffen. Eine Ausnahme ist feinstverteiltes Rhodium, das sich sehr langsam in Königswasser und konzentrierter Schwefelsäure löst.

Das Metall reagiert mit einigen Salzschmelzen und lässt sich so aufschließen. Salze, die dies vermögen, sind Natriumhydrogensulfat, Kaliumdisulfat, Cyanide und Natriumcarbonat.

Sauerstoff löst sich in flüssigem Rhodium (>2000 °C). Beim Erkalten der Schmelze wird dieser unter Spratzen wieder abgegeben. Eine Reaktion erfolgt nicht, da die Oxide oberhalb von ~1100 °C instabil sind.

Es sind insgesamt 33 Isotope sowie weitere 20 Kernisomere des Rhodiums bekannt. Natürliches Rhodium besteht zu 100 % aus dem Isotop Rh, das Element ist somit eines von 22 Reinelementen. Die langlebigsten künstlichen Isotope sind Rh, das mit einer Halbwertszeit von 3,3 Jahren unter Elektroneneinfang zu Ru zerfällt, sowie Rh, das mit einer Halbwertszeit von 3,742 Jahren überwiegend unter Aussendung von Positronen zu Ru zerfällt. Zu einem geringen Teil geht der metastabile Kern auch unter Isomerieübergang in Rh über.

Eine Anwendung als Tracer hat der mit einer Halbwertszeit von 35,88 Stunden kurzlebige Kern Rh gefunden.

"→ Liste der Rhodium-Isotope"

Wie andere Platinmetalle wirkt Rhodium in vielen Prozessen katalytisch. Sowohl das Metall, als auch seine Verbindungen und Legierungen mit anderen Platinmetallen werden daher dementsprechend eingesetzt. Daneben existieren weitere rhodiumspezifische Anwendungsmöglichkeiten, die Verwendung ist jedoch durch den hohen Preis begrenzt.

Die wichtigsten Anwendungsbereiche des Rhodiums sind Fahrzeugkatalysatoren. Es dient darin als Katalysator zur Reduktion von Stickstoffmonoxid zu elementarem Stickstoff. Würde stattdessen Platin oder Palladium eingesetzt, würden verstärkt Ammoniak und Distickstoffmonoxid entstehen.

Ein Teil des Rhodiums wird in Katalysatoren zur Salpetersäureherstellung verwendet. Im sogenannten Ostwald-Verfahren werden zur katalytischen Ammoniakverbrennung zu Stickstoffmonoxid Netze eingesetzt, die aus einer Platin-Rhodiumlegierung mit etwa 10 % Rhodium bestehen. Durch den Einsatz von Rhodium erhöht sich die Haltbarkeit und Ausbeute im Vergleich zu reinem Platin. Auch im Andrussow-Verfahren zur Blausäure-Herstellung wird eine Rhodium-Platin-Legierung als Katalysator eingesetzt.

Metallisches Rhodium kann als Beschichtung eingesetzt werden. Mit Rhodium beschichtete Flächen besitzen ein hohes Reflexionsvermögen und sind daher als hochwertige Spiegel geeignet. Gleichzeitig sind diese Beschichtungen sehr hart und chemisch stabil. Auch als Überzug für Schmuck, Brillengestelle oder Uhren wird Rhodium verwendet. Es verhindert das Anlaufen des verwendeten Metalls. Dies ist vor allem bei Schmuck aus Silber oder Weißgold wichtig. Der Vorgang des Überziehens wird Rhodinieren genannt.

Weitere mögliche Anwendungen sind hochbeanspruchte Laborgeräte, Heizspiralen oder Thermoelemente, die aus Platin-Rhodium-Legierungen gefertigt werden.
Seit Edelmetalle wieder international in den Fokus von Finanzanlegern gekommen sind, gibt es auch physische Rhodium-Anlageprodukte. Aufgrund des späteren Einsatzes in der Industrie nach dem Rückkauf wird Rhodium meist in Pulverform angeboten. Seit 2012 ist "Anlage-Rhodium" auch in Barren-Form erhältlich.

Die Internationale Wertpapierkennnummer (ISIN) im Börsenhandel lautet XY0101622766.

Da der Verbrauch durch erhöhte Nachfrage in der Schmuckindustrie gestiegen ist und 2005 mit 25,3 Tonnen über der Produktion lag, ist der Preis stark gestiegen. So lag der Rhodiumpreis 2003 noch bei etwa 475 Dollar (entsprach 2003 etwa 420 Euro) pro Feinunze (etwa 31,1 Gramm), im Juni 2008 zählte es mit einem Preis von über 9700 US-Dollar (etwa 6230 Euro) pro Feinunze zu den teuersten Metallen überhaupt, fiel danach jedoch schnell auf einen Preis von unter 1000 Dollar im Dezember 2008. Ende März 2017 kostete Rhodium 811 Dollar.

Kompaktes Rhodium ist auf Grund der geringen Reaktivität ungefährlich, als feinverteiltes Pulver dagegen ist es leicht entzündlich und brennbar. Da brennendes Rhodium mit Wasser reagiert, dürfen zur Löschung nur Metallbrandlöscher (Klasse D) eingesetzt werden. Wegen einiger Hinweise auf eine karzinogene Wirkung werden Rhodium und seine Verbindungen in die Kanzerogenitäts-Kategorie 3b eingeordnet.

Wie andere Schwermetallionen sind gelöste Rhodiumionen in hohen Konzentrationen toxisch. In einer Untersuchung mit Lungenepithelzellen wurde ein LC-Wert von 1,2 mmol · l für Rhodium(III)-ionen ermittelt.

Rhodium bildet Verbindungen in den Oxidationsstufen von −I bis +VI. Die stabilste Stufe ist +III, höhere kommen vor allem in Verbindungen mit Fluor, niedrigere in Komplexen mit Liganden wie Kohlenstoffmonoxid, Cyanid oder Phosphanen vor.

Einige Rhodium-Verbindungen, beispielsweise Rhodium(II)-carboxykomplexe, werden untersucht, ob sie sich zur Behandlung von Krebs eignen. Die Verbindungen sind dabei, wie die des Platins auch, sehr oft nierentoxisch.

Einige Rhodiumkomplexe werden in technisch wichtigen Synthesen organischer Chemikalien als Katalysator eingesetzt. Dazu zählt der Wilkinson-Katalysator, ein quadratisch-planarer Rhodiumkomplex mit drei Triphenylphosphan (PPh)- und einem Chlorid- Liganden. Eine Reaktion, die dieser Komplex katalysiert, ist die Hydrierung von Alkenen mit Wasserstoff. Es ist auch möglich, die Liganden durch chirale Gruppen zu ersetzen und so eine asymmetrische Hydrierung zu erreichen. Dies wird unter anderem für die Synthese der Aminosäure -DOPA genutzt. Eine weitere wichtige Reaktion, bei der der Wilkinson-Katalysator eingesetzt wird, ist die Hydroformylierung. Dabei werden aus Alkenen, Kohlenstoffmonoxid und Wasserstoff Aldehyde dargestellt.

Ein weiterer Rhodiumkomplex wird zur Herstellung von Essigsäure eingesetzt. Im Monsanto-Prozess wird "cis"-<nowiki>[</nowiki>(CO)RhI<nowiki>]</nowiki>, ein quadratisch-planarer Komplex mit zwei Kohlenstoffmonoxid- und zwei Iodid-Liganden eingesetzt.

Mit den Halogenen Fluor, Chlor, Brom und Iod sind eine Reihe Verbindungen bekannt. Während Chlor, Brom und Iod nur Verbindungen in der Oxidationsstufe +III bilden, sind die Fluoride Rhodium(IV)-fluorid, Rhodium(V)-fluorid und Rhodium(VI)-fluorid bekannt. Die wichtigste Rhodium-Halogenverbindung ist Rhodium(III)-chlorid, die als Katalysator bei Reduktionen, Polymerisationen oder Isomerisierungen eingesetzt werden kann.

Es sind insgesamt drei Rhodiumoxide, Rhodium(III)-oxid RhO, Rhodium(IV)-oxid RhO und Rhodium(VI)-oxid RhO bekannt. Letzteres ist allerdings nur in der Gasphase zwischen 850 °C und 1050 °C stabil. Rhodium(III)-oxid entsteht in wasserfreier Form durch Verbrennung aus den Elementen bei 600 °C. Erhitzt man dieses unter erhöhtem Sauerstoffdruck weiter, entsteht Rhodium(IV)-oxid.

Rhodium(III)-sulfat Rh(SO) ist ein Zwischenprodukt bei der Rhodiumproduktion. Daneben wird es als Rohstoff für die galvanische Beschichtung von Oberflächen, beispielsweise bei Schmuckwaren, verwendet.

Rhodium(II)-acetat wird in der Organischen Chemie als Katalysator verwendet. Es bildet mit Diazoverbindungen, die eine benachbarte Carbonylgruppe besitzen, Carbene. Aus den Carbenen können unter anderem Cyclopropane dargestellt werden. Auch für die Gewinnung von Yliden und für Insertionsreaktionen können Rhodium-Carbene eingesetzt werden.




</doc>
<doc id="4226" url="https://de.wikipedia.org/wiki?curid=4226" title="Ruthenium">
Ruthenium

Ruthenium (v. „Ruthenien“, „Russland“) ist ein chemisches Element mit dem Elementsymbol Ru und der Ordnungszahl 44. Es zählt zu den Übergangsmetallen, im Periodensystem steht es in der 5. Periode und der Gruppe 8 (früher Teil der 8. Nebengruppe) oder auch Eisengruppe. Es ist ein silberweißes, hartes und sprödes Platinmetall.

Ruthenium wurde 1844 vom deutsch-baltischen Chemiker Karl Ernst Claus in sibirischen Platinerzen entdeckt. Es ist sehr selten und wird nur in geringen Mengen genutzt. Die Hauptanwendungsgebiete des Metalls liegen in der Elektronikindustrie beim Perpendicular Recording, einem Datenspeicherverfahren für Festplatten, und als Katalysator in verschiedenen chemischen Verfahren wie Hydrierungen, Methanisierung oder bei der Ammoniaksynthese. Einige Rutheniumverbindungen, z. B. die Grubbs-Katalysatoren, spielen ebenfalls eine Rolle in chemischen Synthesen.

Ruthenium besitzt keine bekannten biologischen Funktionen, jedoch werden einige Komplexe des Metalls hinsichtlich ihrer Wirkung als Mittel gegen Krebs erforscht.

Nachdem zwischen 1803 und 1804 kurz hintereinander die vier Platinmetalle Palladium, Rhodium, Iridium und Osmium von William Hyde Wollaston und Smithson Tennant in Platinerzen entdeckt wurden, versuchten andere Chemiker ebenfalls, aus derartigen Erzen bislang unbekannte Elemente zu isolieren.

Zunächst meldete der polnische Chemiker Jędrzej Śniadecki 1808, dass er im Jahr zuvor in seltenen südamerikanischen Platinerzen ein neues Element entdeckt habe. Er nannte dieses nach dem kurz zuvor entdeckten Asteroiden Vesta Vestium. Nachdem diese Entdeckung von anderen Chemikern jedoch nicht verifiziert werden konnte, wurde die Entdeckung wieder verworfen.

Nach der Entdeckung großer Platinerzlagerstätten im Ural 1819 begannen Jöns Jakob Berzelius in Stockholm und Gottfried Osann in Tartu, diese zu untersuchen. Dabei erhielt Osann 1828 zunächst ein unbekanntes weißes Oxid, dessen Eigenschaften zu keinem anderen Oxid passten, und nach Reduktion ein unbekanntes goldgelbes Metall. Dieses nannte er nach dem Herkunftsland des Erzes Russland "Ruthenium". Nachdem jedoch Berzelius diese Entdeckung nicht bestätigen konnte, wiederholte Osann seine Arbeiten, konnte aber die Isolierung des Rutheniums nicht wiederholen und zog daraufhin seine Entdeckung zurück.

Der deutsch-baltische Chemiker Karl Ernst Claus versuchte seit 1841 an der Universität Kasan, Osanns Experimente zu wiederholen und unbekannte Elemente aus Platinerzen zu extrahieren. Dies gelang ihm schließlich 1844, als er sechs Gramm eines unbekannten hellgrauen Metalls gewinnen konnte. Er nannte das neue Element wie Osann ebenfalls Ruthenium. Ebenso wie Osann bat Claus Berzelius, die Experimente zu überprüfen und das neue Element zu bestätigen. Da dieser 1845 die Ergebnisse bestätigen konnte, gilt seitdem Claus als Entdecker des Rutheniums.

Ruthenium zählt zu den seltensten nicht-radioaktiven Elementen auf der Erde. Seine Häufigkeit beträgt etwa 1 ppb und ist vergleichbar mit der von Rhodium, Iridium oder Rhenium. Es ist meist mit anderen Platinmetallen vergesellschaftet, so beträgt der Anteil des Rutheniums in der wichtigsten Platinmetalllagerstätte, dem südafrikanischen Bushveld-Komplex, zwischen acht und zwölf Prozent.

Wie andere Platinmetalle kommt es gediegen in der Natur vor und ist darum von der IMA als Mineral mit der System-Nr. 1.AF.05 (Klasse: "Elemente", Abteilung: "Metalle und intermetallische Verbindungen", Unterabteilung: "Platingruppen-Elemente") anerkannt.

Seine Typlokalität, in der das Mineral 1974 erstmals von Y. Urashima, T. Wakabayashi, T. Masaki und Y. Terasaki gefunden wurde, liegt am Fluss Uryū auf der japanischen Insel Hokkaidō. Neben dieser sind weitere 21 Fundorte elementaren Rutheniums bekannt. Zu diesen zählen unter anderem Nischni Tagil und der Miass-Fluss in Russland, der Yuba River in Kalifornien oder der Bushveld-Complex in Südafrika.

Neben elementarem Ruthenium sind auch verschiedene rutheniumhaltige Minerale bekannt. Bei den 13 zurzeit bekannten (Stand: 2010) handelt es sich um Legierungen mit anderen Platinmetallen wie Rutheniridosmin, Sulfide wie Laurit (RuS) oder Arsenide wie Ruthenarsenit (Ru,Ni)As.

Durch die Ähnlichkeiten und geringe Reaktivität der Platinmetalle ist eine Trennung dieser Elemente kompliziert. Es existieren mehrere Möglichkeiten, Ruthenium zu isolieren. Enthält ein Erz eine hohe Rutheniumkonzentration, erfolgt die Abtrennung des Rutheniums am besten zuerst und wird durch Destillation erreicht. Dazu wird eine drei- oder sechswertiges Ruthenium enthaltende Lösung mit Oxidationsmitteln wie Chlor, Chloraten oder Kaliumpermanganat versetzt. Durch dieses wird das Ruthenium zum leicht flüchtigen Ruthenium(VIII)-oxid oxidiert. Dieses kann in verdünnter Salzsäure aufgefangen und zu wasserlöslichen Chlororuthenat-Komplexen reduziert werden. Der Grund für dieses Vorgehen sind die Gefahren, die durch die Bildung von Ruthenium(VIII)-oxid während der Trennung bestehen. So können durch Reaktion von Ruthenium(VIII)-oxid mit Ammoniumsalzen explosive Stickstoff-Chlor-Verbindungen entstehen.

Sind nur geringe Mengen Ruthenium im Ausgangsmaterial enthalten, werden zunächst die übrigen Platinmetalle abgetrennt. Dazu gibt es für die verschiedenen Metalle unterschiedliche Verfahren, insbesondere die Extraktion mit geeigneten Lösungsmitteln oder das Ausfällen der schwerlöslichen Salze. Schließlich bleibt das gelöste Ruthenium übrig. Die Lösung wird von vorhandenem Ammonium befreit, das Ruthenium zu Ruthenium(VIII)-oxid oxidiert und durch Destillation abgetrennt.

Um metallisches Ruthenium zu erhalten, wird es entweder als Ammoniumhexachlororuthenat oder als Ruthenium(IV)-oxid ausgefällt und bei 800 °C in einer Wasserstoffatmosphäre reduziert.

Neben Platinerzen ist auch der Anodenschlamm, der bei der Nickelproduktion anfällt, ein wichtiger Rohstoff für die Gewinnung von Ruthenium und den anderen Platinmetallen. Eine weitere mögliche Quelle für Ruthenium sind abgebrannte Brennelemente, da bei der Kernspaltung auch Ruthenium und andere Platinmetalle entstehen. Eine Tonne dieser Brennelemente enthält über zwei Kilogramm Ruthenium, aber auch andere Platinmetalle wie Rhodium oder Palladium. Die Weltproduktion an Ruthenium liegt im Bereich von ca. 20 t pro Jahr (Stand 2008).

Ruthenium ist ein silberweißes, hartes und sprödes Metall. Mit einer Dichte von 12,37 g/cm ist es nach Palladium das zweitleichteste Platinmetall. Ruthenium schmilzt bei 2606 K und siedet bei etwa 4423 K. Unterhalb von 0,49 K wird das Element zum Supraleiter.

Ebenso wie Osmium kristallisiert Ruthenium in einer hexagonal-dichtesten Kugelpackung in der mit den Gitterparametern "a" = 270,6 pm und "c" = 428,1 pm sowie zwei Formeleinheiten pro Elementarzelle. Mitunter werden vier verschiedene polymorphe Formen des Rutheniums angegeben, in die sich das Metall beim Erhitzen auf Temperaturen von 1308, 1473 und 1770 K umwandelt. Diese beruhen jedoch auf kalorimetrischen Messungen aus dem Jahr 1931, die in der Folgezeit nicht bestätigt werden konnten. Daher ist es wahrscheinlich, dass das Element bis zum Schmelzpunkt lediglich eine Modifikation besitzt.

Innerhalb der Eisengruppe besitzt Ruthenium ähnliche Eigenschaften wie das Osmium, während es sich von denen des Eisens deutlich unterscheidet. Es ist wie andere Platinmetalle und im Gegensatz zum Eisen ein reaktionsträges Edelmetall. Mit dem Sauerstoff der Luft reagiert es erst bei Temperaturen über 700 °C und bildet dabei Ruthenium(VIII)-oxid. Hierbei unterscheidet es sich auch vom Osmium, das schon bei Raumtemperatur beim Kontakt mit Sauerstoff in Spuren das entsprechende Osmium(VIII)-oxid bildet. Auch mit Fluor und Chlor reagiert Ruthenium erst in der Hitze und bildet dabei Ruthenium(VI)-fluorid beziehungsweise Ruthenium(III)-chlorid.

Das Metall löst sich nicht in Säuren wie z. B. Flusssäure, Schwefelsäure, Salpetersäure oder auch Königswasser. Angegriffen wird es dagegen langsam von wässrigen Chlor- und Bromlösungen, schnell von Cyanidlösungen und von Quecksilber(II)-chlorid. Starke Oxidationsmittel wie Kaliumhydroxid-Kaliumnitrat- oder Natriumhydroxid-Natriumperoxid-Schmelzen oxidieren Ruthenium schnell.

Es sind insgesamt 33 Isotope und weitere sechs Kernisomere des Rutheniums zwischen Ru und Ru bekannt. Von diesen sind sieben stabil und kommen auch in der Natur vor. Am häufigsten ist dabei das Isotop Ru mit einem Anteil von 31,6 % an der natürlichen Isotopenzusammensetzung. Vier Isotope, Ru, Ru, Ru und Ru sind mit Anteilen zwischen 12 und 19 % ähnlich häufig. Die seltensten der stabilen Isotope sind Ru und Ru mit Anteilen von 5,52 beziehungsweise 1,88 %. Von den instabilen Isotopen besitzen lediglich Ru (2,9 Tage), Ru (39,26 Tage) und Ru (373,59 Tage) Halbwertszeiten von einigen Tagen; die der anderen liegen im Bereich von Millisekunden (Ru: 1,69 ms) bis Stunden (Ru: 4,44 h).

Rutheniumisotope, vor allem Ru, Ru und Ru entstehen bei der Kernspaltung und sind daher in abgebrannten Brennelementen vorhanden. Eine Tonne bei der Kernspaltung eingesetztes Uran enthält als Spaltprodukt etwa 1,9 Kilogramm Ruthenium. Dieses kann bei der Wiederaufarbeitung durch Oxidation zu flüchtigem Ruthenium(VIII)-oxid aus dem in Salpetersäure gelösten Gemisch abgetrennt werden. Da dieses Ruthenium aber auch einen Anteil des radioaktiven und mit einer Halbwertszeit von 373 Tagen relativ langlebigen Isotopes Ru enthält, kann es nicht direkt für andere Zwecke eingesetzt werden.

Ruthenium wird nur in geringem Maß genutzt. Der größte Teil des Metalls wird dabei in der Elektronikindustrie verwendet. Hier spielt seit dem Jahr 2006 vor allem das Perpendicular Recording eine Rolle, ein Verfahren zur Speicherung von Daten auf Festplatten, bei dem eine dünne Schicht Ruthenium die Speicherschicht aus einer Cobalt-Chrom-Platin-Legierung von einer weichmagnetischen Unterschicht trennt. Der Grund dafür, dass Ruthenium genutzt wird, liegt in seiner hexagonalen Kristallstruktur, die eine ähnliche Gitterkonstante wie die verwendete Speicherschicht-Legierung besitzt. Dünne Rutheniumschichten werden in elektrischen Kontakten wie Schleifringen oder Reed-Relais eingesetzt. Sie sind im Vergleich zu anderen einsetzbaren Metallen wie cobalt-gehärtetem Gold härter und damit beständiger gegen Abrieb.

Wie andere Platinmetalle wirkt auch Ruthenium katalytisch. So kann es etwa zur Hydrierung von Aromaten, Säuren und Ketonen verwendet werden. Ruthenium wirkt auch katalytisch bei der Methanisierung, der Herstellung von Methan aus Wasserstoff und Kohlenstoffmonoxid bzw. Kohlenstoffdioxid. Ruthenium hat bislang jedoch nur geringe Anwendungen für die Methanisierung gefunden, meist werden Nickel-Katalysatoren genutzt. Die zur Methanisierung mit Ruthenium benötigten niedrigeren Temperaturen könnten für Langzeit-Weltraummissionen interessant sein, da das von den Astronauten ausgeatmete Kohlenstoffdioxid umgesetzt und so der Sauerstoffkreislauf geschlossen werden könnte.

Analog zu Eisen und Osmium katalysiert auch Ruthenium die Ammoniak-Synthese aus Stickstoff und Wasserstoff. Es besitzt eine höhere Katalysatoraktivität als Eisen und ermöglicht so eine höhere Ausbeute bei niedrigeren Drücken. Der Einsatz des Metalls ist vor allem durch den Preis limitiert. Industriell eingesetzt wird ein Rutheniumkatalysator, der auf einer Kohlenstoffmatrix geträgert und durch Barium und Caesium als Promotoren verbessert wurde, seit 1998 in zwei Produktionsanlagen von KBR auf Trinidad. Da die langsame Methanisierung des Kohlenstoffträgers beim Prozessablauf stört, wird an kohlenstofffreien Rutheniumkatalysatoren für die Ammoniaksynthese geforscht.

In kleinen Mengen wird Ruthenium in Legierungen von Palladium oder Platin zur Erhöhung der Härte eingesetzt. Rutheniumhaltige Legierungen werden unter anderem für Federspitzen von Füllfederhaltern oder für Zahnfüllungen gebraucht. Titanlegierungen werden durch geringe Mengen an Ruthenium (0,1 %) korrosionsbeständiger, was für Anwendungen in der chemischen Industrie oder der Ölförderung wichtig ist. Es ist dabei eine mögliche Alternative zu Palladium. Auch in Superlegierungen auf Nickelbasis, die für Turbinenschaufeln verwendet werden, kann Ruthenium ein Legierungsbestandteil sein, es bewirkt hier eine erhöhte Phasenstabilität.

Ein großer Teil des Rutheniums wird nicht in Form des Metalls, sondern als Verbindung, vor allem als Ruthenium(IV)-oxid eingesetzt, das unter anderem als Material für Widerstände und Elektroden, beispielsweise für die Beschichtung von Titananoden in der Chloralkali-Elektrolyse.

Der Betastrahler Ru wird zur Strahlentherapie des Aderhautmelanoms eingesetzt.

Wie andere Platinmetalle besitzt Ruthenium keine biologische Bedeutung und kommt im Körper normalerweise nicht vor. Verschiedene Rutheniumkomplexe haben ein pharmakologisches Potenzial. Es werden verschiedene Anwendungen als Wirkstoff erforscht. Einige Verbindungen werden schon in klinischen Studien erprobt. Am wichtigsten hierfür ist die Wirkung als Zytostatikum, also als Mittel zur Therapie von Krebs. Hier gelten Rutheniumkomplexe als mögliche Alternativen zu Cisplatin beziehungsweise Carboplatin. Neben der tumorhemmenden Wirkung, die die Verbindungen mehrerer Platinmetalle besitzen, begründet sich dies vor allem auf drei Eigenschaften von Rutheniumkomplexen:




Da dreiwertiges Ruthenium relativ inaktiv ist, während zweiwertiges eine starke tumorhemmende Wirkung zeigt, sollte es möglich sein, dreiwertiges Ruthenium in einem Tumor zum zweiwertigen zu reduzieren und so zu aktivieren. Damit wäre eine selektivere Wirkung als bei anderen Zytostatika möglich. Bisher ist noch kein Arzneimittel auf der Basis von Ruthenium zugelassen.

Neben der Verwendung in der antineoplastischen Chemotherapie werden auch Anwendungen von Rutheniumverbindungen als Immunsuppressivum, Antibiotikum und antimikrobielle Substanz, beispielsweise zur Bekämpfung von Malaria oder der Chagas-Krankheit, untersucht.

Ruthenium ist als Metall ungiftig. Im Gegensatz zu Osmium bildet sich auch das giftige und leicht flüchtige Tetraoxid nicht durch Reaktion mit Sauerstoff bei Raumtemperatur, sondern nur bei der Reaktion mit starken Oxidationsmitteln. In Pulverform ist Ruthenium brennbar, bei Bränden darf nicht mit Wasser, sondern nur mit Löschpulver oder Metallbrandlöschern gelöscht werden.

Ruthenium bildet Verbindungen in den Oxidationsstufen −2 bis +8, die stabilsten und häufigsten sind dabei +3 und +4. Es zählt damit zusammen mit Osmium und Xenon zu den Elementen, bei denen die höchste Oxidationsstufe +8 chemisch erreicht werden kann.

Ruthenium bildet mit Sauerstoff drei binäre Oxide, Ruthenium(VIII)-oxid, Ruthenium(VI)-oxid und Ruthenium(IV)-oxid. Dazu sind noch Ruthenium(III)-oxid, dieses jedoch nur als Hydrat, und verschiedene Ruthenate, Salze deren Anion eine Ruthenium-Sauerstoffverbindung ist, bekannt. Ruthenium(VIII)-oxid ist, wie Osmium(VIII)-oxid, eine gelbe, leicht flüchtige und giftige Verbindung, die durch Reaktion von Ruthenium oder dessen Verbindungen mit starken Oxidationsmitteln gewonnen wird und die als starkes Oxidationsmittel und für die Abtrennung von Ruthenium von anderen Platinmetallen von Bedeutung ist. Während Ruthenium(VI)-oxid nur in der Gasphase bekannt ist, ist Ruthenium(IV)-oxid ein stabiles, in der Rutilstruktur kristallisierendes Salz, das unter anderem in Widerständen verwendet wird und zur Beschichtung von Elektroden dient.

Im Gegensatz zu Osmium ist vom Ruthenium kein achtwertiges Ruthenat bekannt, in wässrigen Lösungen entsteht bei Reaktion mit starken Oxidationsmitteln ein siebenwertiges, dem Permanganat entsprechendes Perruthenat. Dieses wirkt ebenfalls als Oxidationsmittel, ist jedoch milder und damit selektiver als Ruthenium(VIII)-oxid oder Osmium(VIII)-oxid. So werden primäre Alkohole durch Perruthenate nicht zu Carbonsäuren, sondern nur zu Aldehyden oxidiert. Häufig wird es in organischen Synthesen in Form von Tetrapropylammoniumperruthenat (TPAP) eingesetzt. Es wird dabei zu vierwertigem Ruthenium reduziert.

Vom Ruthenium sind zahlreiche Komplexverbindungen sowohl mit anorganischen als auch organischen Liganden bekannt. Diese können in sehr unterschiedlichen Oxidationsstufen von −2 bis +8 vorliegen. In mittleren Stufen, wie +2, +3 und +4, sind auch nichtklassische Komplexe synthetisiert worden, die Metallcluster mit Ruthenium-Ruthenium-Bindungen enthalten.

Einige Rutheniumkomplexe haben Anwendung als Katalysatoren in verschiedenen organischen Synthesen gefunden. So ist Ruthenium das Zentralmetall in den Komplexen der Grubbs-Katalysatoren, die zu den wichtigsten Katalysatoren für die Olefinmetathese zählen. Ein weiterer, in der organischen Synthese bedeutender Komplex ist der Noyori-Katalysator, ein Ruthenium-Chlor-BINAP-Komplex, der eine effiziente asymmetrische Hydrierung von β-Keto-Estern ermöglicht.
Rutheniumkomplexe sind in der Lage, Polymerisationen zu katalysieren. Neben der auf der Metathese beruhenden Ringöffnungspolymerisation (ROMP) können auch lebende freie radikalische Polymerisationen durch Rutheniumkomplexe ermöglicht werden. Ein Beispiel hierfür ist die Polymerisation von Methylmethacrylat mit RuCl(PPh) als Katalysator.

Zu den bekanntesten Rutheniumkomplexen zählt der Ammin-Komplex Rutheniumrot, der in der Histologie als Färbemittel sowie als Redoxindikator und zur Untersuchung von Textilfasern verwendet wird. Als erster Rutheniumkomplex wurde (1,5-Cyclooctadien)(1,3,5-cyclooctatrien)ruthenium 1963 von Ernst Otto Fischer synthetisiert.

Mit den Halogenen Fluor, Chlor, Brom und Iod bildet Ruthenium eine Reihe von Verbindungen. Am stabilsten sind dabei die dreiwertigen Rutheniumhalogenide, diese sind auch von allen Halogenen bekannt. In höheren Oxidationsstufen sind lediglich die Fluoride bis zum Ruthenium(VI)-fluorid sowie das instabile Ruthenium(IV)-chlorid bekannt. Die wichtigste dieser Verbindungen ist Ruthenium(III)-chlorid, die ein Ausgangsstoff für die Synthese vieler anderer Rutheniumverbindungen ist.

Einen Überblick über Rutheniumverbindungen bietet die .

Im Oktober 2017 wurden in mehreren Ländern Europas erhöhte atmosphärische Ru-Konzentrationen in der Größenordnung 10 mBq/m gemessen, die allerdings weit unterhalb normaler Luftaktivitäten lagen. Die Analyse der Luftströmungen lässt auf eine Quelle südlich des Urals, in Russland oder Kasachstan schließen. Da kein Absturz eines Satelliten verzeichnet wurde (selten basieren Isotopenbatterien auf diesem Nuklid), wurde das Entweichen aus einer nuklearmedizinischen Einrichtung, einer Anlage zur Herstellung von Radiopharmaka oder einem Unfall in einer Wiederaufarbeitungsanlage, möglicherweise in der in Majak, vermutet; 30 Kilometer entfernt von Majak in der Gemeinde Argajasch wurden hohe Luftkonzentrationen von Ru gemessen. Die freigesetzte Aktivität wurde mit 100–300 Terabecquerel abgeschätzt, eine für die lokale Bevölkerung riskante Menge. Im Januar 2018 hatte ein internationaler Bericht die Quelle nicht bestimmen können, jedoch die russischen Versionen einer medizinischen oder weltraumtechnischen Freisetzung abgelehnt. Die französische Zeitung Le Figaro schrieb im Februar 2018, die Ursache hätte ein Auftrag französischer und italienischer Forscher sein können, den die Wiederaufbereitungsanlage Majak für sie ausführte. Majak hatte im Dezember 2017 mitgeteilt, das bestellte Cerium-144 nicht zu liefern, weil der Prozess das erforderliche Niveau nicht erreicht hätte.




</doc>
<doc id="4227" url="https://de.wikipedia.org/wiki?curid=4227" title="Rhenium">
Rhenium

Rhenium ist ein chemisches Element mit dem Elementsymbol Re und der Ordnungszahl 75. Im Periodensystem der Elemente steht es in der 7. Nebengruppe (Gruppe 7) oder Mangangruppe. Es ist ein sehr seltenes, silberweiß glänzendes, schweres Übergangsmetall. Legierungen mit Rheniumanteilen finden Verwendung in Flugzeugtriebwerken, beim Herstellen von bleifreiem Benzin und in Thermoelementen.

Biologische Funktionen des Rheniums sind nicht bekannt, es kommt normalerweise nicht im menschlichen Organismus vor. Ebenso sind keine toxischen Effekte des Metalls bekannt, es gilt als arbeitshygienisch unbedenklich.

Die Existenz des späteren Rheniums wurde erstmals 1871 von Dmitri Iwanowitsch Mendelejew als Dwi-Mangan vorhergesagt. Er schloss aus den Gesetzmäßigkeiten des von ihm entworfenen Periodensystems, dass unterhalb des Mangans zwei noch unbekannte Elemente, die späteren Technetium und Rhenium, stehen müssten.
Entdeckt wurde Rhenium erst 1925 von Walter Noddack, Ida Tacke und Otto Berg. Sie untersuchten Columbit, um die gesuchten Elemente Eka- und Dwi-Mangan zu finden. Da die gesuchten Elemente in den Proben nur in sehr geringem Maße enthalten waren, mussten sie durch Abtrennen der anderen Bestandteile angereichert werden. Schließlich konnte das spätere Rhenium durch Röntgenspektroskopie nachgewiesen werden. Noddack und Tacke behaupteten auch, sehr geringe Mengen des Eka-Mangans (später Technetium) gefunden zu haben, jedoch konnte dies nicht durch Darstellung des Elements bestätigt werden. Sie nannten die Elemente nach ihren Heimatgegenden Rhenium (lat. "Rhenus" für Rhein) und "Masurium" (von Masuren). Letzterer setze sich jedoch nach der Entdeckung des Technetiums 1937 nicht durch.

1928 konnten Noddack und Tacke erstmals ein Gramm Rhenium aus 660 Kilogramm Molybdänerz extrahieren. Wegen der hohen Kosten begann die Herstellung nennenswerter Mengen erst ab 1950, als ein größerer Bedarf für neuentwickelte Wolfram-Rhenium- und Molybdän-Rhenium-Legierungen bestand.

Rhenium ist mit einem Anteil von nur 0,7 ppb in der kontinentalen Erdkruste seltener als Rhodium, Ruthenium und Iridium. Es kommt nicht gediegen, sondern ausschließlich gebunden in einigen Erzen vor. Da Rhenium ähnliche Eigenschaften wie Molybdän besitzt, wird es vor allem in Molybdänerzen wie Molybdänglanz MoS gefunden. In diesen kann bis zu 0,2 % Rhenium enthalten sein. Weitere rheniumhaltige Minerale sind Columbit (Fe,Mn)<nowiki>[</nowiki>NbO<nowiki>]</nowiki>, Gadolinit YFeBe<nowiki>[O|</nowiki>SiO<nowiki>]</nowiki> und Alvit ZrSiO. Auch im Mansfelder Kupferschiefer ist in geringen Mengen Rhenium enthalten. Die größten Vorkommen an rheniumhaltigen Erzen liegen in den Vereinigten Staaten, Kanada und Chile.

Bisher wurde erst ein Rheniummineral, der Rheniit (Rhenium(IV)-sulfid, ReS), entdeckt. Der Fundort lag in einer Fumarole am Gipfelkrater des Vulkans Kudrjawy (russisch: Кудрявый) auf der Insel Iturup, die zu den Kurilen (Russland) gehört.

Der Grundstoff für die Gewinnung von Rhenium sind Molybdänerze, insbesondere Molybdänit. Werden diese im Zuge der Molybdängewinnung geröstet, reichert sich Rhenium als flüchtiges Rhenium(VII)-oxid in der Flugasche an. Dieses kann mit ammoniakhaltigem Wasser zu Ammoniumperrhenat (NHReO) umgesetzt werden.

Das Ammoniumperrhenat wird anschließend bei hohen Temperaturen mit Wasserstoff zu elementarem Rhenium reduziert.

Die Hauptproduzenten waren 2006 Chile, Kasachstan und die Vereinigten Staaten, die Gesamtmenge an produziertem Rhenium belief sich auf etwa 45 Tonnen. 2013 betrug die Gesamtmenge des produzierten Rheniums 48,9 Tonnen; Hauptproduzenten waren Chile (25 t), Polen (7,5 t), die Vereinigten Staaten (7,1 t) sowie Usbekistan (5,5 t). Der USGS gibt als Preis für Rhenium 4.720 USD je kg im Jahre 2010 und 3.160 USD je kg für 2013 an.

Rhenium ist ein weißglänzendes hartes Schwermetall, das äußerlich Palladium und Platin ähnelt. Es kristallisiert in einer hexagonal-dichtesten Kugelpackung in der mit den Gitterparametern "a" = 276,1 pm und "c" = 445,8 pm sowie zwei Formeleinheiten pro Elementarzelle. Die mit 21,03 g/cm sehr hohe Dichte des Rheniums wird nur von den drei Platinmetallen Osmium, Iridium und Platin übertroffen.

Rhenium hat mit 3186 °C einen der höchsten Schmelzpunkte aller Elemente. Es wird nur noch von dem höchstschmelzenden Metall Wolfram (3422 °C) und Kohlenstoff übertroffen. Auch der Siedepunkt des Rheniums ist mit 5630 °C der zweithöchste nach dem des Wolframs von 5930 °C. Unterhalb von 1,7 K wird Rhenium zum Supraleiter.

Rhenium lässt sich gut durch Schmieden und Verschweißen verarbeiten, da es duktil ist und dies im Gegensatz zu Wolfram oder Molybdän auch nach Rekristallisation bleibt. Beim Schweißen von Rhenium tritt keine Versprödung auf, die zu einer höheren Sprödigkeit und damit schlechteren Materialeigenschaften führen würde.

Die Aktivität von Rhenium ist 1,0 MBq/kg.

Obwohl Rhenium mit einem negativen Standardpotential nicht zu den Edelmetallen zählt, ist es bei Raumtemperatur unreaktiv und gegenüber Luft stabil. Erst beim Erhitzen reagiert es ab 400 °C mit Sauerstoff zu Rhenium(VII)-oxid. Auch mit den Nichtmetallen Fluor, Chlor und Schwefel reagiert es beim Erhitzen.

In nichtoxidierenden Säuren, wie Salzsäure oder Flusssäure, ist Rhenium nicht löslich. Dagegen lösen die oxidierenden Schwefel- und Salpetersäure Rhenium leicht auf. Mit Oxidationsschmelzen bilden sich leicht farblose Perrhenate(VII) der Form ReO oder grüne Rhenate(VI) des Typs ReO.

Rhenium ist als Pulver ein entzündbarer Feststoff. Er kann durch kurzzeitige Einwirkung einer Zündquelle leicht entzündet werden und brennt nach deren Entfernung weiter. Die Entzündungsgefahr ist umso größer, je feiner der Stoff verteilt ist. Das feuchte Pulver wird schon bei Raumtemperatur allmählich unter Bildung von Perrheniumsäure oxidiert.

Es sind insgesamt 34 Isotope und weiter 20 Kernisomere des Rheniums bekannt. Von diesen kommen zwei, die Isotope Re und Re, natürlich vor. Re, das mit einem Anteil von 37,40 % an der natürlichen Isotopenverteilung vorkommt, ist das einzige stabile Isotop. Das mit einem Anteil von 62,60 % häufigere Re ist schwach radioaktiv. Es zerfällt unter Betazerfall mit einer Halbwertszeit von 4,12 · 10 Jahren zu Os, was zu einer spezifischen Aktivität von 1020 Becquerel/Gramm führt. Rhenium ist damit neben Indium eines der wenigen Elemente, die zwar ein stabiles Isotop haben, in der Natur jedoch am häufigsten in ihrer radioaktiven Form vorkommen. Beide Isotope sind mit Hilfe der Kernspinresonanzspektroskopie nachweisbar. Von den künstlichen Isotopen werden Re und Re als Tracer verwendet. Als hauptsächlicher Beta-Strahler wird Re in der Nuklearmedizin zur Therapie bei der Radiosynoviorthese eingesetzt. Re dient als radioaktives Arzneimittel in der Tumortherapie.

Der Zerfall von Re zu Os wird als "Rhenium-Osmium-Methode" in der Geologie zur isotopischen Altersbestimmung von Gesteinen oder Mineralen benutzt. Dabei wird zur Korrektur des schon vorher vorhandenen Osmiums die Isochronenmethode verwendet.

"→ Liste der Rhenium-Isotope"

Rhenium wird meist nicht elementar verwendet, sondern als Beimischung in einer Vielzahl von Legierungen eingesetzt. Etwa 70 % des Rheniums werden als Zusatz in Nickel-Superlegierungen genutzt. Ein Zusatz von 4 bis 6 % Rhenium bewirkt eine Verbesserung des Kriech- und Ermüdungsverhaltens bei hohen Temperaturen. Diese Legierungen werden als Turbinenschaufeln für Flugzeugtriebwerke eingesetzt.

Weitere 20 % der produzierten Rheniummenge werden für Platin-Rhenium-Katalysatoren verwendet. Diese spielen eine große Rolle bei der Erhöhung der Oktanzahl von bleifreiem Benzin durch Reformieren („Rheniforming“). Der Vorteil des Rheniums liegt darin, dass es im Vergleich mit reinem Platin nicht so schnell durch Kohlenstoffablagerungen auf der Oberfläche des Katalysators („Coking“) deaktiviert wird. Dadurch ist es möglich, die Produktion bei niedrigeren Temperaturen und Drücken durchzuführen und so wirtschaftlicher zu produzieren. Auch andere Kohlenwasserstoffe, wie Benzol, Toluol und Xylol, lassen sich mit Platin-Rhenium-Katalysatoren herstellen.

Thermoelemente für die Temperaturmessung bei hohen Temperaturen (bis 2200 °C) werden aus Platin-Rhenium-Legierungen gefertigt. Auch als Legierung mit anderen Metallen, wie Eisen, Cobalt, Wolfram, Molybdän oder Edelmetallen, verbessert Rhenium die Beständigkeit gegenüber Hitze und chemischen Einflüssen. Die Anwendung ist jedoch durch die Seltenheit und den hohen Preis des Rheniums beschränkt.

In einigen Spezialanwendungen wird ebenfalls Rhenium verwendet, beispielsweise für Glühkathoden in Massenspektrometern oder Kontakte in elektrischen Schaltern.

Es gibt mehrere Möglichkeiten, Rhenium nachzuweisen. Eine Möglichkeit sind spektroskopische Methoden. Rhenium besitzt eine fahlgrüne Flammenfärbung mit charakteristischen Spektrallinien bei 346 und 488,9 nm. Gravimetrisch ist Rhenium über die charakteristisch kristallisierende Perrheniumsäure oder verschiedene Perrhenat-Salze, etwa Tetraphenylarsonium-perrhenat, nachweisbar. Auch moderne analytische Methoden wie Massenspektrometrie oder Kernresonanzspektroskopie sind für den Nachweis des Elements geeignet.

Wie viele Metalle ist Rhenium in Pulverform leichtentzündlich und brennbar. Zum Löschen darf wegen des entstehenden Wasserstoffes kein Wasser verwendet werden. Stattdessen sind als Löschmittel Löschpulver oder Metallbrandlöscher zu verwenden. Kompaktes Rhenium ist dagegen nicht brennbar und ungefährlich. Rhenium hat keine bekannte biologische Bedeutung für den menschlichen Organismus. Obwohl über die Toxizität von Rhenium nichts Genaueres bekannt ist und keine Toxizitätswerte existieren, gilt Rhenium arbeitshygienisch als unbedenklich.

Rhenium bildet eine große Zahl an Verbindungen; wie bei Mangan und Technetium sind Verbindungen in den Oxidationsstufen von −III bis +VII bekannt. Im Gegensatz zu Mangan sind jedoch Verbindungen in den hohen Oxidationsstufen beständiger als in den niedrigeren.

Es sind insgesamt fünf Oxide des Rheniums bekannt, das gelbe ReO, rotes ReO, ReO, braunschwarzes ReO und ReO. Rhenium(VII)-oxid ReO ist das stabilste Rheniumoxid. Es ist ein Zwischenprodukt bei der Rheniumgewinnung und kann als Ausgangsverbindung für die Synthese anderer Rheniumverbindungen wie Methyltrioxorhenium genutzt werden. In Wasser löst es sich unter Bildung der stabilen Perrheniumsäure HReO. Rhenium(VI)-oxid ReO hat eine charakteristische Kristallstruktur, die als Kristallstrukturtyp (Rheniumtrioxid-Typ) dient.

Es sind insgesamt 13 Verbindungen des Rheniums mit den Halogenen Fluor, Chlor, Brom und Iod bekannt. Dabei reagiert Rhenium bevorzugt zu Hexahalogeniden des Typs ReX. So entstehen blassgelbes Rhenium(VI)-fluorid ReF und grünes Rhenium(VI)-chlorid ReCl direkt aus den Elementen bei 125 °C bzw. 600 °C. Reaktion von Rhenium mit Fluor unter leichtem Druck bei 400 °C führt zu hellgelbem Rhenium(VII)-fluorid, neben Osmium(VII)-fluorid und Iod(VII)-fluorid das einzige bekannte Halogenid in der Oxidationsstufe +VII. Rotbraunes Rhenium(V)-chlorid (ReCl) besitzt eine dimere, oktaedrische Struktur. Chlorierung von ReO mit Thionylchlorid liefert ein schwarzes, polymeres Chlorid ReCl, das aus Ketten von dimeren Re-Cl-Clustern besteht, die über Chloratome verbrückt sind. Werden höhere Rheniumchloride thermisch bei über 550 °C zersetzt, bildet sich dunkelrotes, trimeres Rhenium(III)-chlorid ReCl. Strukturell bestehen dessen Moleküle aus Dreiecksmetallclustern, wobei die Re-Re-Abstände von 248 pm Doppelbindungscharakter der Metall-Metall-Bindungen beweisen. Die Halogenide sind wasserempfindlich und reagieren mit Wasser zu Halogenoxiden oder Oxiden.

Das schwarze Rhenium(VII)-sulfid ReS entsteht aus Perrhenatlösungen durch Einleiten von Schwefelwasserstoff. Thermische Zersetzung ergibt ebenfalls schwarzes Rhenium(IV)-sulfid ReS, das auch direkt aus den Elementen zugänglich ist.

Rhenium bildet eine Vielzahl von Komplexen. Es sind sowohl klassische Komplexe mit einzelnen Metallzentren, als auch Metallcluster bekannt. Bei diesen liegen Rhenium-Rhenium-Mehrfachbindungen teilweise auch in Form von Dreifach- oder Vierfachbindungen vor. Eine Vierfachbindung existiert etwa im ReX-Komplexion (X ist dabei ein Halogenatom oder eine Methylgruppe).

Auch metallorganische Verbindungen des Rheniums sind bekannt. Eine wichtige organische Rheniumverbindung ist Methylrheniumtrioxid (MTO), die als Katalysator für Metathesereaktionen, zur Epoxidierung von Olefinen sowie zur Olefinierung von Aldehyden eingesetzt werden kann. MTO und andere Rheniumkatalysatoren für die Metathese sind besonders beständig gegenüber Katalysatorgiften.




</doc>
<doc id="4228" url="https://de.wikipedia.org/wiki?curid=4228" title="Radium">
Radium

Radium ( ‚Strahl‘, wegen seiner Radioaktivität, wie auch Radon) ist ein chemisches Element mit dem Elementsymbol Ra und der Ordnungszahl 88. Im Periodensystem steht es in der 2. Hauptgruppe, bzw. der 2. IUPAC-Gruppe und zählt damit zu den Erdalkalimetallen.

Radium wurde am 21. Dezember 1898 in Frankreich von der polnischen Physikerin Marie Curie und ihrem Ehemann, dem französischen Physiker Pierre Curie, in der Pechblende aus dem böhmischen St. Joachimsthal entdeckt. Wegweisend war dabei der Befund, dass gereinigtes Uran (als Metallsalz) nur einen geringen Bruchteil der Radioaktivität des ursprünglichen Uranerzes aufwies. Stattdessen fand sich der größte Teil der Radioaktivität des Erzes in der Bariumsulfat-Fällung wieder. Für das abgetrennte Element wurde dann die ausgeprägte Strahlungseigenschaft zur Namensgebung herangezogen.

Radiumverbindungen galten zunächst als relativ harmlos oder gar gesundheitsfördernd und wurden in den Vereinigten Staaten und Europa als Medikament gegen eine Vielzahl von Leiden beworben (z. B. als Krebsmittel) oder als Zusatz in Produkten verarbeitet, die im Dunkeln leuchteten. Die Verarbeitung geschah ohne jegliche Schutzvorkehrungen. Noch bis Mitte der 1930er Jahre wurden Kosmetika und Genussmittel beworben, die Radium enthielten.

Nach der Gründung des Radiumbades Sankt Joachimsthal in Böhmen 1906 kam es unmittelbar vor dem Ersten Weltkrieg aufgrund einer vermuteten Heilwirkung von Radium zu einem Aufblühen der Radiumbäder in Deutschland. Während bereits vor dem Krieg Bad Kreuznach damit warb, stärkstes Radiumsolbad zu sein, waren es nach dem Krieg neben St. Joachimsthal, Oberschlema vor allem Bad Brambach. Letztere beiden Orte behaupteten von sich, stärkstes Radium- bzw. Radiummineralbad der Welt zu sein. Wobei zu beachten ist, dass in den Heilquellen vor allem Radon, Radium hingegen nur in geringen Spuren vorkam. Korrekterweise hätten sich diese Bäder „Radonbad“ nennen müssen.

In den 1920er Jahren erkannte man die gesundheitsschädliche Wirkung von Radium, als sehr viele der als "Radium Girls" bezeichneten Zifferblattmalerinnen in Orange (New Jersey) durch die radioaktive Strahlung der selbstleuchtenden Zifferblatt-Farbe Krebstumoren an Zunge und Lippen bekamen, weil sie mit dem Mund ihre Pinsel spitzten. Der New Yorker Zahnarzt Theodor Blum veröffentlichte 1924 einen Artikel über das Krankheitsbild des "Radiumkiefers" (engl. "radium jaw"). Er schrieb die Erkrankung zunächst der Giftigkeit des Phosphors zu. Harrison Martland, Pathologe in New Jersey, war es schließlich, der 1925 eine Studie begann, in deren Ergebnis die Ursache richtigerweise dem Radium zugeschrieben wurde.

Bis 1931 wurde mit Radium versetztes Wasser namens Radithor in kleinen Flaschen zum Trinken verkauft. Spätestens mit dem Tod des Stahlmagnaten Eben Byers im Jahre 1932, der von 1928 bis 1930 täglich zwei Flaschen "Radithor" zu sich nahm, stand unumstritten fest, dass Radium schwerste Gesundheitsschäden hervorrufen kann.

Radium ist eines der seltensten natürlichen Elemente; sein Anteil an der Erdkruste beträgt etwa 7 · 10 %. Es steht in einem natürlichen Zerfallsgleichgewicht mit Uran. Damit ist der Radiumgehalt des jeweiligen Gesteines proportional zu dessen Urangehalt (unter der Voraussetzung des Nicht-Stattfindens von Transportprozessen). Der massebezogene Faktor beträgt etwa 1/3.000.000 (ca. 0,3 g/t Schwermetall). Im radioaktiven Zerfall, dem es selbst unterliegt, ist es das Mutternuklid von Radon-222.

Als Metall ist es ein typisches Erdalkali-Element. Es ist weich und silberglänzend. Radium ist dem leichteren Gruppenhomologen Barium sehr ähnlich, jedoch noch unedler als dieses. Bei Kontakt mit Sauerstoff oxidiert es sehr rasch und reagiert heftig mit Wasser.

In wässriger Lösung liegt es stets positiv zweiwertig vor. Das zweiwertige Kation ist farblos. Wie Barium bildet es einige schwerlösliche Salze, so das Carbonat, Sulfat und Chromat. Andere Salze wie die Halogenide (das Fluorid ist nur mäßig löslich), Nitrat und Acetat sind leicht löslich. Die Salze geben der Bunsenflamme eine karminrote Färbung.

Die Massenzahlen seiner Isotope reichen von 213 bis 230, ihre Halbwertszeiten liegen zwischen etwa 182 Nanosekunden für Ra und 1602 Jahren für Ra. Da das Radium-Isotop Ra in wägbaren Mengen gewonnen werden kann, ist es möglich, seine chemischen Eigenschaften recht gut zu studieren.

Die Anwendung von geschlossenen Radiumkapseln war eine frühe Form der Brachytherapie bei Krebserkrankungen, z. B. des Gebärmutterhalses. 2013 brachte der Pharmahersteller Bayer HealthCare mit Radium-223-dichlorid (Xofigo®) ein Radiopharmakon auf Basis von Ra, einem alpha-Strahler mit einer Halbwertszeit von 11,43 Tagen, zur intravenösen Anwendung bei symptomatischen Knochenmetastasen des kastrationsresistenten Prostatakrebses auf den Markt.

Zur Darstellung der Alphastrahlung sind Radiumpräparate im Handel, die unter Wahrung der Sicherheitsvorschriften in Nebelkammern eingesetzt werden können. Es stehen zwei Intensitäten (3,7 kBq und 60 kBq) zur Verfügung.

Da Radium über das Zerfallsgleichgewicht an das Uran gekoppelt ist, begleitet es dieses zwangsläufig in seinen Erzen und wird bei den bergbaulichen Aktivitäten mit umgewälzt, also aus dem geologischen Einschluss herausgelöst. Bei der Erzaufbereitung ist im Wesentlichen nur das Uran von Interesse (Yellowcake), das Radium wird zum Bestandteil der Rückstandsfraktion und als Abraum deponiert. Damit ist nicht im „verkauften“ Uran der größte Teil der Radioaktivität des ursprünglich geförderten Uranerzes enthalten, sondern in den Schlammdeponien der Erzaufbereitung.

Eine Beeinflussung der belebten Erdoberfläche („Umwelt“) ergibt sich einerseits über die vom Radium selbst ausgehende Strahlung (insbesondere Alphastrahlung), andererseits über seine Wirkung als Radonquelle. Auswirkungen dieser Art einzudämmen ist das Ziel von Sanierungsanstrengungen in Bergbaufolgelandschaften (siehe Wismut).

Überall, wo große Mengen natürlicher heterogen zusammengesetzter Stoffgemische umgesetzt werden, wird über deren Spurengehalt von Uran und Radium auch „natürliche“ Radioaktivität mit verfrachtet. Dies trifft insbesondere für die Kohlen-Verfeuerung in Kraftwerken zu (Kohlelagerstätten als hydrogeologische Uran-Senken). Nicht zurückgehaltene Stäube verfrachten das Radium der Kohle anteilsweise in die Atmosphäre. Bei greifenden Rauchgasreinigungsmaßnahmen erscheint das Radium dann auch in den festen Rückständen, die zum Teil marktfähig sind.

Radiumverbindungen liegen fast ausschließlich in der Oxidationsstufe +II vor. Diese sind meist farblose, salzartige Feststoffe, die sich infolge Radiolyse der eigenen Alphastrahlung mit der Zeit gelb färben.

Eine Übersicht über Radiumverbindungen gibt die .

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.




</doc>
<doc id="4229" url="https://de.wikipedia.org/wiki?curid=4229" title="Rubidium">
Rubidium

Rubidium (von ‚tiefrot‘; wegen zweier charakteristischer roter Spektrallinien) ist ein chemisches Element mit dem Elementsymbol Rb und der Ordnungszahl 37. Im Periodensystem steht es in der 1. Hauptgruppe, bzw. der 1. IUPAC-Gruppe und zählt zu den Alkalimetallen. Das weiche, silbrigweiß glänzende Metall entzündet sich spontan bei Luftzutritt.

Rubidium wurde 1861 von Robert Wilhelm Bunsen und Gustav Kirchhoff spektroskopisch als Verunreinigung im Lepidolith entdeckt, als sie das Mineralwasser der neu erschlossenen Maxquelle in Bad Dürkheim analysierten. Bunsen gelang es, Rubidiumsalze aus dem Quellwasser zu fällen und diese von anderen Alkalimetallsalzen zu isolieren. Bunsen musste 44200 Liter Dürkheimer Quellwasser verarbeiten, um 9 g RbCl zu isolieren.

Rubidium gehört zur Gruppe der inkompatiblen Elemente und tritt in der Regel zusammen mit diesen in erhöhten Konzentrationen auf. Das Element kommt in kleiner Konzentration in einigen Mineralien wie Leucit, Pollucit und Zinnwaldit vor. Lepidolith enthält bis zu 1,5 % Rubidium. Erst in den letzten Jahren wurden auch eigenständige Rubidium-Mineralien entdeckt, wie Rubiklin (ein Rubidium-Aluminiumsilikat), sowie Voloshinit und Rubidium-Ramanit (Rubidiumpentaborat Tetrahydrat).

Wie die anderen Alkalimetalle ist Rubidium an der Luft unbeständig und oxidiert. Mit Wasser reagiert es äußerst heftig unter Bildung von Rubidiumhydroxid und Wasserstoff, der sich in der Luft in der Regel entzündet. Mit Quecksilber bildet es ein Amalgam, mit den Metallen Gold, Caesium, Natrium und Kalium ist es legierbar. Rubidiumverbindungen färben Flammen dunkelrot (daher der Name des Elements).
Rubidium ist ein starkes Reduktionsmittel. Metallisches Rubidium kann durch Reduktion von Rubidiumchlorid mit Calcium im Vakuum hergestellt werden.

Von den beiden natürlich vorkommenden Isotopen ist nur Rb stabil, Rb ist ein Betastrahler und zerfällt zu Sr. Mit einer extrem langen Halbwertszeit von etwa 48 Milliarden Jahren ist seine Radioaktivität sehr gering. Das Verhältnis von Rb- und Sr-Isotopen in Gesteinen wird zur radiometrischen Datierung herangezogen.

Rubidium und seine Verbindungen besitzen ein nur kleines Anwendungsspektrum und werden hauptsächlich in der Forschung und Entwicklung eingesetzt. Verwendungsmöglichkeiten bestehen als:

Zum Nachweis von Rubidium kann man seine rotviolette Flammenfärbung nutzen. Im Spektroskop zeigt sich eine deutliche Emissionslinie bei 780,0 nm. Quantitativ lässt sich dies in der Flammenphotometrie zur Bestimmung von Rubidiumspuren nutzen.
In der Polarographie zeigt Rubidium eine reversible kathodische Stufe bei −2,118 V (gegen SCE). Dabei müssen als Grundelektrolyt quartäre Ammoniumverbindungen (hier beispielsweise 0,1 M Tetramethylammoniumhydroxid) verwendet werden, weil andere Alkali- oder Erdalkalimetallionen sehr ähnliche Halbstufenpotentiale besitzen.

Für Pflanzen ist Rubidium vermutlich nicht essentiell, bei Tieren scheint es für den normalen Verlauf der Trächtigkeit notwendig zu sein. Der Rubidiumbedarf des Menschen dürfte bei weniger als 100 µg pro Tag liegen. Mit der üblichen Mischkost kommt er auf etwa 1,7 mg am Tag. Ein Rubidiummangel ist bei diesem Angebot ebenso wenig zu erwarten wie eine nutritive Rubidiumbelastung.
Tee und Kaffee – Arabica-Kaffee hat den höchsten Rubidium-Gehalt, der in Lebensmitteln festgestellt wurde (Arabica-Bohne: 25,5–182 mg/kg Trockensubstanz) – liefern Erwachsenen im Mittel 40 % der verzehrten Rubidiummenge. Rubidium wirkt im zentralen Nervensystem und beeinflusst dort die Konzentration von Neurotransmittern, ein Einsatz von Rubidium als antidepressiver Wirkstoff wird diskutiert. Ein Rubidiummangel kann bei Dialysepatienten vorliegen.

Rubidium ist selbstentzündlich und reagiert äußerst heftig mit Wasser. Aus Sicherheitsgründen ist Rubidium in trockenem Mineralöl, im Vakuum oder in einer Inertgasatmosphäre aufzubewahren.








</doc>
<doc id="4231" url="https://de.wikipedia.org/wiki?curid=4231" title="Römische Zahlschrift">
Römische Zahlschrift

Als römische Zahlen werden die Zahlzeichen einer in der römischen Antike entstandenen und noch für Nummern und besondere Zwecke gebräuchlichen Zahlschrift bezeichnet. In der heutigen Normalform werden darin die lateinischen Buchstaben I (1), V (5), X (10), L (50), C (100), D (500) und M (1000) als Zahlzeichen für die Schreibung der natürlichen Zahlen verwendet.

Es handelt sich um eine additive Zahlschrift, mit ergänzender Regel für die subtraktive Schreibung bestimmter Zahlen, aber ohne Stellenwertsystem und ohne Zeichen für Null. Zugrunde liegt ein kombiniert quinär-dezimales oder "biquinäres" Zahlensystem mit den Basiszahlen 5 und 10.

Die in einer römischen Zahl verwendeten Zeichen haben einen festen Wert. Dabei gibt es die Zehnerpotenzen als Basiswerte (die „Einer“) und die fünffachen Hilfsbasiswerte (die „Fünfer“). Abgesehen von der subtraktiven Schreibung ist der Wert unabhängig von der Position.
In der Neuzeit ist die Darstellung mit Großbuchstaben (Majuskeln) üblich. Schreibweisen mit Kleinbuchstaben werden seit dem Mittelalter verwendet und bedeuten für den Zahlenwert keinen Unterschied, allerdings kann es zu Verwechslungen von i und l kommen. Für Seitennummerierungen (beispielsweise im Vorwort, zur Abgrenzung vom eigentlichen, mit indischen Ziffern nummerierten Hauptteil eines Buches) und alphanumerische Gliederungen werden sie aber noch verwendet.

Bisweilen, insbesondere bei handschriftlichen Aufzeichnungen, werden römische Zahlen zur Unterscheidung von normalen Buchstaben durch einen Überstrich oder Über- und Unterstrich gekennzeichnet (so bei IX = 9, nicht [ɪks]).

Als sich im Mittelalter und der frühen Neuzeit aus den Buchstaben I und V die Abwandlungen J und U entwickelten, wurden diese oft für den jeweils gleichen Zahlenwert benutzt. Vor allem bei Minuskeln wurde schließendes i durch ein j wiedergegeben: j = 1; ij = 2; iij = 3 usw. Diese Schreibweise ist nicht mehr üblich.

In Ländern mit nichtlateinischer Schrift werden die römischen Zahlen zum Teil mit anderen, graphisch passenden, einheimischen Zeichen geschrieben. So werden beispielsweise in Russland noch immer die Ziffer 1, die auf Schreibmaschinen immer die Form eines I hatte, für die römische Eins, die Buchstaben П für II, Ш für III und У für V genutzt:
Vor der Zeitenwende wurden auch andere als die üblichen Buchstaben zur Darstellung von Zahlen verwendet:
Zur Umrechnung in eine römische Zahl ohne die weiter unten beschriebene Subtraktionsregel genügt es, mit den großen römischen Ziffern beginnend, deren Wert so häufig wie möglich von der umzurechnenden Zahl abzuziehen und die römischen Ziffern dabei zu notieren. Dadurch werden die Ziffern automatisch der Größe nach sortiert:
Um eine solche römische Zahl wieder zurückzurechnen, sind nur die Werte der einzelnen Zahlenzeichen zu addieren.

Die Subtraktionsregel ist eine übliche, verkürzende Schreibweise, mit der es vermieden werden soll, vier gleiche Zahlzeichen in direkter Aufeinanderfolge zu schreiben. Sie wurde bereits in römischer Zeit gelegentlich angewandt, ihre konsequente Anwendung erscheint jedoch erst seit dem späteren Mittelalter, dabei häufig noch in vermischter Anwendung mit Schreibung einzelner Zahlen ohne Subtraktionsprinzip, und ist seither lediglich eine weithin vorherrschende Konvention geblieben, von der besonders in der Epigraphik vielfach kein Gebrauch gemacht wird.

Die Subtraktionsregel in ihrer Normalform besagt, dass die Zahlzeichen I, X und C einem ihrer beiden jeweils nächstgrößeren Zahlzeichen vorangestellt werden dürfen und dann in ihrem Zahlwert von dessen Wert abzuziehen sind:

Zahlzeichen der Fünferbündelung (V, L, D) werden generell nicht in subtraktiver Stellung einem größeren Zeichen vorangestellt.

Beispiel:
Es gibt zwei Abweichungen von dieser Normalform, die schon seit der Antike vereinzelt zu belegen sind und auch in jüngerer Zeit auftreten:


Beide Abweichungen treten mitunter kombiniert auf, also IIL statt XLVIII für 48, IIC statt XCVIII für 98.

Die subtraktive Schreibung wird zuweilen mit den subtraktiven lateinischen Zahlwörtern in Verbindung gebracht, stimmt aber mit diesen nicht überein. Bei den lateinischen Zahlwörtern werden die Wörter für 1 "und" 2, aber nicht die für 10 und 100 subtraktiv verwendet und hierbei dann nur den Vielfachen der 10 ab 20 ("duodeviginti" = 18, "undeviginti" = 19) sowie vereinzelt einmal der 100 ("undecentum" = 99) vorangestellt.

Eine additive oder kombiniert additiv-subtraktive Zahlschrift wie die römische benötigt kein Zeichen für die Null, wie es dagegen in einem Stellenwertsystem wie dem Dezimalsystem und dessen üblicher indo-arabischer Schreibung als Platzhalter eine grundlegende Rolle spielt. Die Römer kannten zwar sprachliche Ausdrücke für „nicht etwas“ "(nullum)" und „nichts“ "(nihil)", aber kein Zahlzeichen und keinen eigenen mathematischen Begriff für einen Zahlwert „Null“. Bei der Darstellung von Zahlen auf dem Rechenbrett wird das Nichtvorhandensein eines Stellenwertes durch Freilassen der entsprechenden Spalte angezeigt; in Tabellenwerken ist das Fehlen einer Zahl zuweilen durch einen waagerechten Strich, manchmal kombiniert mit einem kleinen Kreis, markiert. Zur Bezeichnung der Null hat Beda Venerabilis um 725 n. Chr. das Zeichen N verwendet.

Für große Zahlen (ab 1000) gibt es mehrere Möglichkeiten der Darstellung:

Die Schreibweisen wurden auch gemischt, wie die Schreibweise mit Apostrophus und die Multiplikationsschreibweise.

Der römische Apostrophus, ein Zeichen, das aussieht wie eine schließende Klammer oder ein an der Vertikalen gespiegeltes C (Ↄ), leitet sich, wie auch andere römische Ziffern, aus chalkidisch-griechischen Zahlzeichen ab.

Das ursprüngliche Zeichen für 1000, das Phi (Φ, auch geschrieben ↀ oder CIↃ) kann als eine Zusammensetzung von einem C, einem I und einem Apostrophus angesehen werden: CIↃ. Durch das Hinzufügen weiterer Bögen, oder C und Apostrophi wurde der Wert jeweils verzehnfacht: ↂ oder CCIↃↃ für 10.000, CCCIↃↃↃ für 100.000.

Die römische 500, die Hälfte von 1000, entsteht auch durch die Halbierung des Zeichens: ↀ → D. Die Bildung von 5000, 50.000 und den folgenden verläuft analog: ↁ oder IↃↃ, IↃↃↃ.

Da die Apostrophus-Schreibweise für sehr große Zahlen unhandlich war, wurde ein Rahmen um eine Ziffer oder Zifferngruppe gezeichnet, um deren Wert mit 100.000 zu multiplizieren. Der Rahmen war üblicherweise unten offen: X, es kommen aber auch vollkommen geschlossene: X, sowie Schreibweisen, die die Zahlzeichen nur links und rechts mit vertikalen Linien einrahmen: X vor.

Die Verwendung der 100.000 als Multiplikationszahl entspricht den römischen Zahlwörtern für große Zahlen, wie "decies centena milia" (buchstäblich „zehnmal je hundert Tausender“ = eine Million), "quadringenties milies centena milia" („vierhundertmal tausendmal je hundert Tausender“ = 40 Milliarden, die Staatsschulden Vespasians). Dabei wurde "centena milia" oft weggelassen, wobei durch die Verwendung der Multiplikativzahl statt der Kardinalzahl "(decies" statt "decem)" klar war, dass sie mit 100.000 multipliziert werden musste.

Ein Vinculum (auch Titulus) ist ein Querstrich über den Ziffern, um eine Multiplikation mit 1000 anzuzeigen: , der Querstrich konnte über mehrere Ziffern gleichzeitig gezogen werden. Möglich waren mehrere Querstriche für höhere Tausenderpotenzen.

Diese Schreibweise darf nicht mit der Kennzeichnung römischer Zahlen durch einen Überstrich (zum Beispiel VI für 6) zur Unterscheidung von normalen Buchstaben verwechselt werden.

Mit größeren Zehnerpotenzen ab 1000 wurde manchmal eine stellenwertbezogene Multiplikationsschreibweise verwendet. Dazu wurde links von dem Zeichen ein Multiplikationsfaktor geschrieben, zum Beispiel V•M für Fünftausend (5 × 1000).

Im 16. Jahrhundert kamen Schreibweisen von Zahlen auf, die spezielle Tausendertrennzeichen verwendeten, um große Zahlen zu gliedern. In Portugal wurde der Cifrão verwendet, ein Symbol ähnlich dem $, in Spanien der Calderón, ein U-ähnliches Zeichen (⊍). Diese Zeichen wurden sowohl mit indischen als auch mit römischen Zahlen benutzt. Die Zahl 18.642 wurde also 18 $ 642 beziehungsweise XVIII $ DCXLII geschrieben.

Die gelegentliche Verwendung eines größeren, längeren I anstelle von zwei aufeinanderfolgenden i in lateinischen Texten ist selten in der Darstellung römischer Zahlen anzutreffen. So steht bei Verwendung dieser Schreibweise MDCLXXI nicht etwa für 1671, sondern für 1672.

Im Zimmerhandwerk wird für Abbundzeichen generell die additive Schreibweise verwendet: 4 = IIII, 9 = VIIII, 14 = XIIII und so weiter. Dies ist zum einen unkomplizierter und verhindert zum anderen die Verwechslung von zum Beispiel IX und XI. Eine weitere Besonderheit ist die häufig verwendete Schreibweise X/ für XV.

Auf Uhrenzifferblättern wird die Zahl 9 immer nach der Subtraktionsregel als IX geschrieben, die Zahl 4 aber oft als IIII.

Die Römer nutzten Brüche mit der Basis 12. Die Nutzung der 12 lag nahe, weil sich die am häufigsten benötigten Brüche „eine Hälfte“, „ein Drittel“ und „ein Viertel“ durch Vielfache von formula_1 darstellen lassen. Der römische Name für ein Zwölftel ist "Uncia," ein Wort, das später zum Gewichtsmaß „Unze“ wurde. Für Brüche, deren Zähler um 1 kleiner als der Nenner ist, wurde teilweise eine subtraktive Bezeichnung verwendet wie bei "Dodrans" (de quadrans, formula_2). Brüche wurden ausgeschrieben oder durch stark variierende Zeichen dargestellt. In einigen Fällen wurden sie einer römischen Zahl als eine den Zwölfteln entsprechende Anzahl von Punkten oder kleinen Querstrichen angehängt. Als Zeichen für formula_3(semis) oder für formula_4 (semuncia) wurde vielfach S oder Σ, für formula_5 (sicilicus) ein seitenverkehrtes C und für formula_6 (duae sextulae) ein Zeichen ähnlich dem Z oder der indischen 2 gebraucht.

Der Unicodeblock Zahlzeichen enthält an den Positionen U+2160 bis U+2188 eigene Codes für die römischen Ziffern 1–12, 50, 100, 500, 1000, 5000 und 10.000 als Groß- und überwiegend auch Kleinbuchstaben sowie einige ungebräuchlich gewordene altrömische Zahlzeichen. Diese Zeichen werden von den Schriftarten, die sie enthalten, in der Regel dicktengleich dargestellt, so dass sie sich als Tabellenziffern sowie für den ostasiatischen (horizontalen oder vertikalen) Satz eignen. Bei vertikaler Schreibrichtung werden sie zudem – im Gegensatz zu gewöhnlichen lateinischen Buchstaben – aufrecht dargestellt. Für die meisten Anwendungen wird im Unicode-Standard dagegen von der Verwendung dieser Zeichen abgeraten, stattdessen sollen römische Zahlen mit den gewöhnlichen lateinischen Buchstaben dargestellt werden.

Weder für die Darstellung eines Rahmens noch für Zeichen mit (mehrfachem) Vinculum sieht Unicode eigene Sonderzeichen vor. Hierzu bedarf es einer sogenannten Smartfonttechnik wie OpenType, um eine an die Buchstabenbreite und -höhe angepasste Variante der Zeichen U+0305 „Combining Overline“ und U+033F „Combining Double Overline“ auswählen können, sofern die benutzte Schrift dieses vorsieht. Zur Darstellung des Rahmens kann eine Variante von U+007C „Vertical Line“ benutzt werden. Derzeit sind diese Lösungen nur sehr wenig verbreitet.

Bei der Verwendung von gebrochenen Schriften und Schreibschriften werden römische Ziffern in Antiqua gesetzt. Sofern vorhanden, ist dies über die erwähnten Unicode-Zeichen anstelle von Großbuchstaben möglich. Ersatzweise wird für römische Ziffern eine zur Textschrift passende Antiqua verwendet.

Wie die meisten Kerbschriften und einfachen Zahlensysteme wurden die römischen Ziffern additiv nach dem Prinzip der kombinierten Zehner- und Fünferbündelung gereiht, so dass nie mehr als vier gleiche Zeichen aufeinanderfolgen. Nach dem Vorbild der Zahlschrift der Etrusker wurde ergänzend auch eine subtraktive Schreibweise praktiziert, bei der die Voranstellung eines Zeichens vor einem der beiden in der Zehnerbündelung nächsthöheren anzeigt, dass sein Wert von diesem abzuziehen ist. In diesem Fall folgen nie mehr als drei gleiche Zeichen einander.

Die ersten drei römischen Zahlzeichen I (1), V oder gerundet U (5) sowie X (10) haben ihre Schreibform im Verlauf der Geschichte im Wesentlichen unverändert beibehalten, davon abgesehen, dass V bzw. U in älteren römischen Inschriften zum Teil noch in kopfständiger Schreibung – mit nach oben weisendem Winkel bzw. Rundbogen – erscheint. Sie finden sich in gleicher oder rotierter Schreibung (V und U regelmäßig kopfständig Λ, statt X manchmal ein aufrecht stehendes Kreuz +) und mit den gleichen Zahlwerten bei den Etruskern. Weitgehend ähnlich – dabei V in einigen Fällen abweichend als einfacher Schrägstrich / oder rückwärts geneigt \ – können sie in älteren italischen Kulturen nachgewiesen werden, dort als Beschriftung von Kerbhölzern. Nach dem Ergebnis der Forschungen von Lucien Gerschel aus den 1960er-Jahren kann damit als sicher gelten, dass die Römer und Etrusker diese ersten drei Zahlzeichen aus der Kerbschrift älterer italischer Völker übernommen haben.

Das ursprüngliche römische Zahlzeichen für 50, bei dem das Zeichen V bzw. U für 5 durch einen senkrechten Abstrich geteilt und so im Wert auf 50 verzehnfacht wurde (ungefähr Ψ), findet sich mit gleicher Schreibform (nur kopfständig) und mit gleichem Zahlwert bei den Etruskern und ähnlich in Kerbschriften anderer Kulturen. Es gleicht außerdem – in der römischen Schreibung – dem Buchstaben Chi des chalkidischen Alphabets, eines westgriechischen Alphabets, das in den griechischen Kolonien Siziliens in Gebrauch war, bzw. dem Psi der ostgriechischen Alphabete. Im Griechischen steht Chi als Zahlzeichen allerdings für den Wert 1000 (als Anfangsbuchstabe des Zahlwortes für 1000: χιλιοι) oder in der dezimal gegliederten griechischen Zahlschrift für den Wert 600, desgleichen Psi dort für den Wert 700. Ihre zahlschriftliche Verwendung im Griechischen kam außerdem wahrscheinlich erst später in Gebrauch als das entsprechende römische und etruskische Zahlzeichen. Entgegen der Vermutung älterer Forschung ist darum nach Gerschel anzunehmen, dass die Römer und Etrusker dieses Zahlzeichen nicht aus dem chalkidischen Alphabet, sondern ebenfalls aus der Kerbschrift älterer italischer Völker übernommen haben. Bei den Römern wurde es dann durch Abflachung des Winkels oder Rundbogens zu einem waagerechten Strich ⊥ und Verkürzung seiner linksseitigen Hälfte an den lateinischen Buchstaben L angeglichen. In dieser Form ist es erstmals 44 v. Chr. belegt.

Die Zahl 100 schrieben die Etrusker nach einem ähnlichen Prinzip wie die 50, indem das Zeichen X für 10 durch einen senkrechten Strich geteilt (ungefähr Ж) und so auf 100 verzehnfacht wurde. Nach den von Gerschel nachgewiesenen Parallelen wurde dieses Zeichen von den Etruskern ebenfalls aus der älteren italischen Kerbschrift übernommen. Die Römer und andere Völker Italiens schrieben die 100 demgegenüber als ein rechts- oder linksseitig offenes C. In der Forschung wurde dieses herkömmlich als Ableitung aus dem griechischen Buchstaben Theta (Zahlwert 9) gedeutet. Gerschel und Georges Ifrah dagegen vermuten, dass hier ebenfalls eine Abwandlung des kerbschriftlichen und etruskischen Zeichens für 100 vorliegt, bei der von einer – etruskisch belegten – gerundeten Schreibvariante des Ж unter dem Einfluss des lateinischen Zahlwortes "centum" („hundert“) nur der eine Rundbogen dieses Zeichens beibehalten wurde.

Die Zahl 500 schrieben die Römer ursprünglich als eine Art waagerecht geteiltes D, also ungefähr D, und die Zahl 1000 als durch senkrechten Abstrich geteilten Kreis Φ oder Halbkreis (d. h. als eine Art kopfständiges Ψ) oder als eine Art liegendes S oder liegende 8 (∞). In einigen Fällen wurde diese liegende 8 (∞) durch einen senkrechten Abstrich geteilt. Nach herkömmlicher Auffassung ist das römische Zeichen für 1000 aus dem griechischen Phi (Zahlwert 500) und das römische Zeichen für 500 durch dessen Halbierung entstanden. Gerschel und Ifrah dagegen vermuten, dass das römische Tausenderzeichen ursprünglich ein kerbschriftlicher senkrecht geteilter Kreis oder ein eingekreistes X oder Kreuz ⊕ war und das D durch dessen Halbierung entstand. Das römische Tausenderzeichen wurde seit etwa dem ersten Jahrhundert v. Chr. zunehmend durch den Buchstaben M (für "mille": „tausend“) ersetzt. Die Zahlzeichen M und D sind inschriftlich erstmals 89 v. Chr. belegt.

Die römische Zahlschrift ist für die epigraphische oder dekorative Schreibung von Zahlen (insbesondere Jahreszahlen), für die Zählung von Herrschern, Päpsten und anderen Trägern gleichen Namens, für die Band-, Buch-, Kapitel- und Abschnittzählung in Texten und für die Bezifferung von Messinstrumenten wie dem Zifferblatt der Uhr noch immer in Gebrauch.

Die römischen Zahlen haben hauptsächlich bei der Schreibung von Zahlwörtern, aber kaum in schriftlichen Rechenoperationen eine Rolle gespielt. Hierfür wurden Hilfsmittel wie die Fingerzahlen, das Rechenbrett und der Abakus herangezogen. Hierbei werden den römischen Zahlen wieder in einem Stellenwertsystem Werte (Anzahl Finger, Rechenmünzen, Kugeln) zugeordnet und mit diesen die Rechenoperation durchgeführt.

493 stellte Victorius von Aquitanien ein Tafelwerk mit 98 Spalten zusammen, in denen er die Produkte der Zahlen von den Brüchen bis zum Wert 1000 mit den Zahlen von 2 bis 50 in römischen Zahlen angab zur Erleichterung der Multiplikation und Division, der sogenannte "Calculus Victorii".

Mathematiker, die die indischen Ziffern kennengelernt hatten, erkannten deren Überlegenheit. Bereits im Jahr 1202 veröffentlichte der italienische Mathematiker Leonardo Fibonacci sein Buch Liber abaci, mit dem er die indischen Zahlen, die er in Bejaja in Nordafrika kennengelernt hatte, in Europa bekannt machen wollte.

Der deutsche Rechenmeister Adam Ries hat nach Untersuchung der existierenden Zahlensysteme ebenfalls den indischen Ziffern den Vorzug gegeben. Ries erkannte, dass durch die Null eine tabellarische Addition und Subtraktion gegenüber den römischen Ziffern wesentlich vereinfacht wurde. Mit der Etablierung von neuzeitlichem Rechnen auf Basis der indischen Ziffern läutete er Anfang des 16. Jahrhunderts zugleich das Ende der Nutzung von römischen Ziffern im Alltagsleben ein.


</doc>
<doc id="4233" url="https://de.wikipedia.org/wiki?curid=4233" title="Rood">
Rood

Rood ist eine anglo-amerikanische Flächeneinheit. Das Einheitenzeichen ist ro.

1 rood = ¼ acre = 1210 sq.yd = 10.890 sq.ft. = 1011,7141056 m² 

1 square mile = 640 acre = 2.560 rood

Im Deutschen wird die Einheit auch als Viertelmorgen bezeichnet.


</doc>
<doc id="4234" url="https://de.wikipedia.org/wiki?curid=4234" title="Rem (Einheit)">
Rem (Einheit)

Das Rem, Einheitenzeichen rem (für ) ist die veraltete Einheit für die Äquivalentdosis, abgeleitet von der Einheit für die Ionendosis Röntgen. 

Sie wurde am 1. Januar 1978 von der SI-Einheit Sievert (Sv) abgelöst und soll nach Ablauf der Übergangszeit seit 1. Januar 1985 nicht mehr verwendet werden. Allerdings sind noch immer ältere Messgeräte im Umlauf, die diese Einheit aufweisen.

100 Rem entsprechen 1 Sievert. 1 Rem entspricht 10 Millisievert.


</doc>
<doc id="4236" url="https://de.wikipedia.org/wiki?curid=4236" title="Reggae">
Reggae

Reggae [] ist eine der bedeutendsten Stilrichtungen der populären Musik, die ursprünglich aus Jamaika stammt. Sie entstand dort Ende der 1960er-Jahre und verbreitete sich, beginnend in Großbritannien, bald international, spätestens ab 1968 nach und nach auch weltweit.

Reggae entstand Ende der 1960er-Jahre unter dem Einfluss US-amerikanischer Musikrichtungen wie Soul, R&B, Blues, Country und Jazz, die in Jamaika über das Radio empfangen wurden, aus seinen unmittelbaren Vorläufern Mento, Ska und Rocksteady. Er entwickelte sich seitdem zu einer der bedeutendsten Richtungen der populären Musik. Die ursprüngliche Bedeutung des Wortes „Reggae“ ist unklar. Manche – darunter Bob Marley – leiten es von dem lateinischen Wort "rex" (König) ab und behaupten, es heiße so viel wie „Musik des Königs“ (siehe auch Rastafari). Andere erklären es wesentlich weniger majestätisch als Ableitung von "streggae", einem jamaikanischen Slang-Wort für ein leichtes Mädchen.
Der erste Reggae-Titel, in welchem dieses Wort vorkommt, war "Do the Reggay" (1968) von Toots & the Maytals, der erste als eigentlicher Reggae-Song geltende ist "People funny boy" (1968) von Lee „Scratch“ Perry.

Der typische Grundrhythmus des Reggae entstand, als bei ersten Eigenproduktionen des Ska US-amerikanische R&B-Stücke gecovert und mit einer starken Betonung des zweiten und vierten Taktteils unterlegt wurden. Im Übergang von Ska zu Rocksteady und dann zu Reggae wurde dieser minimalistische Grundrhythmus jeweils verlangsamt. Reggae und seine Vorläufer entstanden vornehmlich als Tanzmusik, deren Verbreitung hauptsächlich durch sogenannte "Sound Systems", mobile Diskotheken, vorangetrieben wurden. Betreiber dieser Soundsystems wie beispielsweise Clement „Sir Coxsone“ Dodd, Arthur „Duke“ Reid oder Cecil „Prince Buster“ Campbell gehörten zu den ersten Produzenten eigenständiger jamaikanischer Tanzmusik. Als erster großer und durchschlagender internationaler Erfolg – v. a. in den britischen Charts – gilt das 1968 von Desmond Dekker eingespielte "The Israelites", mit Nummer-eins-Platzierungen u. a. in Deutschland und Großbritannien; der erste eigentliche Welthit des Reggaes.

Gegen Ende der 1960er-Jahre war Reggae die bevorzugte Musik der britischen Skinheads, weswegen frühe britische Reggaeproduktionen auch unter dem Begriff "Skinhead-Reggae" vermarktet werden und viele Reggae-Künstler, wie "The Charmers", "The Corporation", "Joe the Boss" oder "Symarip", mit Titeln wie zum Beispiel "Skinhead Moonstomp" oder "Skinheads A Bash Dem" gezielt auch an die Skinhead-Community richteten. Mit der stärker werdenden Fokussierung auf den britischen Popmusik-Markt (zum Beispiel durch den Einsatz von Streichern) und spätestens mit der inhaltlichen Betonung speziell schwarzer Thematiken im Roots-Reggae, verlor die Musik für Teile der Skinhead-Szene an Bedeutung. Hatten frühe jamaikanische und britische Reggae-Produktionen und -Künstler noch Wert auf gerade diese Szene als den Konsumenten gelegt und obwohl eine sich an den ursprünglichen Werten der unrassistischen Skinheadbewegung orientierende Szene bis heute existiert, wurde diese Szene für den Reggaemarkt weitgehend uninteressant; nicht zuletzt auch wegen des ökonomischen Erfolgs der neueren Spielarten. Diese sogenannten "Traditional Skins" oder auch "Trojan Skins" – benannt nach dem Plattenlabel "Trojan Records" – organisieren bis heute traditionelle Reggae-Events, betreiben kleine Label und bringen mit eigenen Bands auch neue Reggae-Musik hervor, die sich stark am Hammondsound und dem Uptempo früher Reggaenummern orientieren.
Angeregt durch den bedeutendsten Reggaemusiker und jamaikanischen Nationalhelden Bob Marley verknüpften zahlreiche Musiker die Musik mit der zu dieser Zeit zwar bereits existierenden, wenngleich noch nicht überaus weit verbreiteten Religion der Rastafari. Der klassische Reggae der 1970er-Jahre wird heutzutage oft als "Roots-Reggae" bezeichnet. Er entstand aus verschiedenen Einflüssen, wie Ska und Rocksteady oder dem frühen Pop-Reggae, der zunächst hauptsächlich aus Großbritannien kam. Auch afrikanische Einflüsse spielten eine Rolle. Parallel dazu entwickelte sich in Großbritannien eine eigene Form des Reggae, die Einflüsse aus anderen Musikformen wie Punk, New Wave oder Pop integrierte und säkularer war als der jamaikanische Stil (dazu gehören Gruppen wie The Police, The Clash, The Jam oder The Specials).

Mittlerweile haben sich sehr viele verschiedene Formen des Reggae entwickelt: Bedeutend ist Dub, eine oft sehr minimalistische Variante, die sich durch starken Gebrauch von elektronischen Studioeffekten und fast völligem Verzicht auf Gesang auszeichnet. Dubeffekte sind heute fester Bestandteil der meisten Reggae-Varianten, auch Bob Marleys Lieder wurden davon beeinflusst.

Schon früh wurden auf den B-Seiten bekannter Reggaesingles Dubversionen der Lieder mitgeliefert. Die Deejays begannen, diese „Versions“ live mit Sprechgesang-Texten zu versehen (Toasting). Das Toasting stammt vom Scat, einer Gesangsart des Jazz, ab.

Aus dem Toasting wurde im Lauf der Zeit eine neue Variante des Reggae: Raggamuffin bzw. Ragga. Parallel dazu entwickelte sich aus dem Toasting im New York der 1970er-Jahre eine weitere neue Musikrichtung, der Hip-Hop.

Moderne Reggaevarianten wie Ragga werden oft als Dancehall-Reggae bezeichnet. Dieser Begriff ist etwas problematisch, denn er bezeichnet streng genommen keine bestimmte Stilrichtung, sondern fasst diejenigen Stile zusammen, die momentan in den jamaikanischen "Dancehalls" (Ort größerer Tanzpartys) populär sind. „Dancehall“ bezeichnet jedoch auch einen spezifischen, stark synkopierten Rhythmus im modernen Reggae.

Gesungen wird im jamaikanischen Reggae meist auf "Patois", einer auf dem Englischen basierenden Kreolsprache mit zahlreichen Wortneuschöpfungen (z. B. "I and I" = "we" ‚wir (Einheit zweier Personen [und mit "Jah"])‘, "to overstand", Neologismus statt "to understand" ‚verstehen‘). Die Texte des Roots-Reggae sind oft sozialkritisch, machen auf Missstände aufmerksam oder wollen den Jamaikanern ihre afrikanischen Wurzeln bewusst machen. Ein weiteres oft vorkommendes Themengebiet lässt sich mit "love, peace & unity" („Liebe, Frieden und Einigkeit“) zusammenfassen, auch der Genuss von Marihuana ist ein beliebtes Motiv.

Die Texte des modernen Dancehall-Reggae sind dagegen – vergleichbar mit Hip-Hop-Texten – oft explizit sexuelle und gewaltverherrlichende Themen ("Slackness") sowie häufig homophobe Texte. Als Gegenbewegung dazu etablierten sich der Conscious Reggae, der soziale, politische und religiöse Themen in den Vordergrund rückt.

Charakteristisch für den Reggae ist die Offbeat-Phrasierung, bei der entweder die Gitarre oder das Keyboard, hin und wieder auch die Bläser, in der in den meisten anderen Musikrichtungen unbetonten zweiten und vierten Taktzeit spielen. Im Gegensatz zum Ska, wo statt der 2/4-Betonung eher Offbeats eingesetzt werden, die die „und“-Zählzeiten betonen, ist der Reggae in der Regel langsamer und weniger durch Bläser dominiert.

Die Instrumentierung der meisten (klassischen) Reggaebands besteht aus Drumset, E-Bass, E-Gitarre, Keyboard und Gesang. Oft kommen Blechbläser und Perkussion hinzu. Bei den neueren Stilrichtungen des Reggae (Dancehall, Ragga, Reggaeton) kommt häufiger die Elektronik in Form von Computern und Samplern zum Einsatz. Dabei ist das Schlagzeug meist stark betont, synthetisch erzeugt und mehr am Rock- und Diskosound orientiert.

Besonders wichtig im Klangbild eines Reggae-Songs ist stets der Basslauf, der das Gegengewicht zu den Offbeats bildet und oft das eigentliche Thema des Stückes beschreibt. Als herausragende Reggae-Bassisten gelten Aston Barrett und Robbie Shakespeare.

Das Schlagzeug zeichnet sich hauptsächlich durch zwei besondere Stile aus:

Das Schlagzeug wird oft von einem Perkussionisten unterstützt. Als herausragende Schlagzeuger gelten Carlton Barrett und Sly Dunbar.

Gitarristen spielen beim Reggae hauptsächlich rhythmische Begleitung, wobei auch hier die Betonung des Offbeats im Vordergrund steht. Hin und wieder wird einfach der Basslauf gedoppelt. „Austoben“ dürfen sich die Gitarristen dann in zum Teil ausgedehnten Soli. Diese Soli geben der Musik auch eine besondere Note.

Erwähnenswerte Gitarristen in der Welt des Reggae sind zum Beispiel Al Anderson, Peter Tosh, Earl „Chinna“ Smith, Ben Harper, Junior Marvin, Ernest Ranglin, Donald Kinsey.

Verwendet werden hauptsächlich Piano- und Orgel-Sounds, hin und wieder auch synthetische Klänge. Die Keyboarder begleiten zusammen mit der Gitarre vorwiegend rhythmisch, natürlich ebenfalls mit Betonung des Offbeats. Zwei Variationen sind sehr gängig: Zum einen die Betonung der zweiten und vierten Zählzeit, zum anderen diese Betonung ergänzt um die folgenden „und“-Zählzeiten. Keyboard-Soli sind relativ selten, Orgel-Soli kommen häufiger vor. Es ist nicht unüblich, dass in größeren Reggaebands zwei Keyboarder spielen.

Berühmte Tastenmänner (oder gar Tastenfrauen) gibt es in diesem Musikstil wenig. Wichtige Vertreter sind aber Earl „Wya“ Lindo, Tyrone Downie, Ian Wynter und Bernard „Tooter“ Harvey, sowie Jackie Mittoo, einer der wichtigsten Orgelspieler bei Studio One.

Außerdem erwähnenswert: Augustus Pablo, berühmt geworden durch sein Melodicaspiel sowie Monty Alexander. Letzterer ist eigentlich Jazzmusiker, spielt aber als geborener Jamaikaner auch Reggae und verbindet diesen dann mit Jazz.

Die Bläser einer klassischen Reggaeband treten meist zu dritt auf. Die gespielten Instrumente sind hierbei in der Regel Posaune und Trompete, oft auch das Saxophon.

Seit Ende der 1970er-Jahre wurden Reggae-Tracks von deutschen Musikern veröffentlicht, so zum Beispiel von Spliff (unter eigenem Namen sowie als Backing-Band von Nina Hagen). Der erste Künstler, der mit deutschsprachigem (bzw. bairischem) Reggae Erfolge erzielte, war Hans Söllner in den 1980er-Jahren. Zuvor veröffentlichte die Aachener Gruppe Taugenixe 1984 die erste Reggae-LP auf deutsch über das Label Ariwa. Ihr Album "Reggae Ron", welches im Studio des Londoner Produzenten Mad Professor aufgenommen wurde, bekam in Magazinen wie Spax aber sehr negative Kritiken. Das Album drang daraufhin nicht zu den Reggae-Fans in Deutschland durch und der kommerzielle Erfolg blieb aus.

Ab den 1990ern wuchs die Zahl der originär deutschen, zum Teil deutschsprachigen Reggae-Bands und -Artists, und etwa seit der Jahrtausendwende erzielen deutsche Reggaekünstler auch Charterfolge, allen voran Gentleman, Culcha Candela und Seeed (Peter Fox und Boundzound auch als Solokünstler), sowie z. B. auch Jan Delay (Album "Searching for the Jan Soul Rebels").

Instrumentalmusik von Kevin MacLeod im Reggaestil – 

Siehe auch , und .




</doc>
<doc id="4237" url="https://de.wikipedia.org/wiki?curid=4237" title="Orthographie">
Orthographie

Die Orthographie (auch Orthografie; von lateinisch , "orthós" „aufrecht“, „richtig“ und -graphie) oder Rechtschreibung ist die allgemein übliche Schreibweise der Wörter einer Sprache in der verwendeten Schrift. Eine davon abweichende Schreibung wird allgemein als Rechtschreibfehler bezeichnet.

Bei der Rechtschreibung in Alphabetschriften unterscheidet man zwei grundlegend unterschiedliche Ansätze:

Der phonemische Ansatz bezieht sich gewöhnlich auf nur eine Standardvarietät der jeweiligen Sprache. In diesem Sinne überwiegend phonemisch ist die Orthographie zum Beispiel des Bulgarischen, Finnischen, Georgischen, Italienischen, Serbischen, Spanischen und Türkischen. Die Orthographie des Spanischen etwa ist für das kastilische Spanisch eher phonemisch als beispielsweise für das argentinische oder das kubanische (die sich beide freilich keineswegs als nachrangige Dialekte, sondern eben als die argentinische bzw. kubanische Hochsprache begreifen).

Besonders fällt die stark etymologisch geprägte morphophonemische Orthographie des Englischen auf. Im Englischen kann eine Buchstabenfolge (z. B. "ough") vier oder mehr verschiedene Aussprachen haben; umgekehrt kann eine bestimmte Lautfolge viele verschiedene Schreibweisen haben, je nachdem, in welchem Wort sie vorkommt, z. B. der Laut (stimmloser postalveolarer Frikativ, „sch“) als ocean, fish, action, sure usf. "Siehe auch:" ghoti.

Auch das Französische schreibt sich entschieden etymologisch. Stellte Frankreich seine Orthographie auf eine rein phonemische Grundlage, wäre die Familienähnlichkeit des Französischen mit den übrigen romanischen Sprachen kaum mehr zu erkennen. Im Französischen kann ein Laut zahlreiche verschiedene Schreibweisen haben (z. B. die Graphemfolgen "au", "aud", "auds", "ault", "aulx", "aut", "auts", "aux", "eau", "eaud", "eaux", "haut", "hauts", "ho", "o", "ô", "od", "ods", "oh", "os", "ot", "ots").

Die Orthographie des Deutschen hat sowohl phonemische als auch morphophonemische Elemente (nicht dargestellte Auslautverhärtung, e/ä-Schreibweise u. a.), allerdings mit nur relativ wenigen etymologischen Schreibweisen (eine Ausnahme bilden viele neuere Fremdwörter und einige Homophone). Insbesondere bei Entlehnungen aus dem Englischen wird die Schreibweise nur selten an das deutsche Lautbild angepasst ("Keks", "Streik", aber nicht "(Korn-)Fleks", "Kompjuter", "Marschmelloh" u. ä.). Allerdings wurden mit der Rechtschreibreform von 1996 auf diesem Gebiet einige Eindeutschungen eingeführt (z. B. "Ketschup", "Portmonee"), die aber nicht konsequent fortgeführt wurden.




</doc>
<doc id="4239" url="https://de.wikipedia.org/wiki?curid=4239" title="Roman">
Roman

Der Roman ist eine literarische Gattung, und zwar die Langform der schriftlichen Erzählung. Das Wort "Roman" ist ein Lehnwort aus dem Französischen und bedeutet „Erzählung in Versen oder Prosa“. Es löste im 17. Jahrhundert das Wort Historie ab, das bis dahin die unter diese Gattung fallenden Werke bezeichnet hatte. Auch verstand man ab dem 17. Jahrhundert nur noch in Prosa abgefasste Schriften als „Romans“. Der Plural „Romane“ wurde erst im 18. Jahrhundert gebräuchlich.

Bis heute ist es schwierig, den Roman eindeutig zu definieren, da er zum einen in der antiken Diskussion über literarische Formen nicht erwähnt wird und zum anderen viele unterschiedliche Einflüsse auf die Romanproduktion einwirkten und bis heute weiter einwirken. Daraus resultieren zwei für die Definition des Romans wichtige Grundannahmen. In der Romantheorie wird der Roman erstens als Synthese verschiedener Gattungen aufgefasst, da es außer dem Prosakriterium kaum formale Vorgaben gibt und daher andere ästhetische Muster leicht zu integrieren sind. Und zweitens ist der Roman aufgrund seiner Offenheit gegenüber anderen Formen und Genres einem stetigen Wandel unterworfen, der bis in die Gegenwart andauert und noch nicht abgeschlossen ist.

Von anderen Gattungen ist der Roman vor allem negativ abzugrenzen. Das Kriterium der Fiktionalität unterscheidet den Roman von faktualen Erzählungen – etwa denen der Geschichtsschreibung –, die ein getreues Abbild eines Geschehens darbieten wollen. Oft weist schon das Wort "Roman" auf dem Bucheinband oder Titelblatt einen Roman als künstlerisches und damit fiktionales Werk aus. Vor 1700 wurden die Romane allerdings oft als Schlüsselromane mit durchaus historiographischem Anspruch gelesen, weswegen sie oft das Wort „Historia“ im Titel trugen. Erst infolge der vertieften poetologischen Diskussionen im 18. Jahrhundert werden Romane als eigenständige Kunstform und als fiktionale Werke im eigentlichen Sinne wahrgenommen. Die Abwertung des Romans als geschichtliche Quelle geht mit einer Aufwertung im literarischen Gattungssystem einher.

Im Unterschied zu historischen Werken behandelt der Roman in der Regel vergleichsweise private Stoffe aus subjektiven Erzählpositionen. Liebesgeschichten waren bis in das 18. Jahrhundert hinein als privates Sujet gattungsbestimmend. Andere, jedoch in der Regel nicht minder private Stoffe breiteten sich in den Untergattungen des Schelmenromans und des satirischen Romans aus.

Novelle, Märchen, Legende sowie Kurzgeschichte sind als Formen epischer Prosa zwar mit dem Roman verwandt, aber als epische Kleinformen meist deutlich kürzer. Außer durch die Länge unterscheidet sich der Roman von diesen Gattungen vor allem aber dadurch, dass er die Darstellung einer kontingenten Welt ins Zentrum rückt. Literaturhistoriker des 19. und 20. Jahrhunderts sprachen wie Georg Lukács vom „epischen Zugriff des Romans auf das Leben in seiner ganzen Totalität“. Im Unterschied zum Epos setzt der Roman jedoch keinen für alle Leser gültigen Sinnhorizont voraus, sondern gestaltet die Welt aus dem Erfahrungshorizont des Einzelmenschen. Dieser kann brüchig, inkohärent oder gar chaotisch sein, was in einem Epos undenkbar wäre. Für Georg Lukács ist die Form des Romans daher „Ausdruck der transzendentalen Obdachlosigkeit“ der Moderne.

Romanhaftes Erzählen war in vielen Hochkulturen bereits früh verbreitet, so in Japan, China, Indien und im arabisch-orientalischen Kulturkreis. Die außereuropäischen Romane und der Roman der Antike waren im Vergleich zum antiken Epos für die Herausbildung des modernen Romans jedoch nur von untergeordneter Bedeutung. Einige der wichtigsten außereuropäischen Romane sind die "Genji Monogatari" (11. Jhdt.) von Murasaki Shikibu, der arabische "Hayy ibn Yaqdhan" von Ibn Tufail (vor 1185) und die "Geschichte der Drei Reiche" Luo Guanzhongs. Die Rezeption der europäischen antiken Romane setzte bereits im Mittelalter ein, intensivierte sich aber erst im 17. Jahrhundert. Den barocken Romanciers wurde vor allem Heliodors "Aithiopika" (3. Jhdt. n. Chr.) mit seinem Medias-in-res-Einstieg und den zahlreichen nachgeholten Vorgeschichten zum unerreichten Gattungsmuster. Weitere wichtige Vorbilder für den höfisch-historischen Roman waren auch Achilleus Tatios’ "Leukippe und Kleitophon" (1. Jhdt. n. Chr.) und Xenophons von Ephesos "Ephesiaka" (2. Jhdt. n. Chr.). Ein vielgelesener Vorläufer des bukolischen Genres war Longos’ "Daphnis und Chloe". Autoren des satirischen Romans konnten an die "Metamorphosen" des Apuleius (2. Jhdt. n. Chr.) sowie an Petrons "Satyricon" (1. Jhdt. n. Chr.) anknüpfen.

Das altfranzösische Wort „romanz“ kam im 12. Jahrhundert für Erzählungen in „romanischer“ (altfranzösischer und anglonormannischer) Volkssprache in Gebrauch, die allerdings im Unterschied zu späteren Romanen nicht in Prosa, sondern in Versen abgefasst waren. Nordfrankreich etablierte sich mit Werken wie dem Roman de Thèbes (ca. 1160), dem Roman d’Énéas (kurz nach 1160), dem Roman de Troie (ca. 1165) und dem Roman d’Alexandre (von dem Fragmente und Redaktionsstufen aus der Zeit zwischen 1120 und 1180 überliefert sind) als neues, bereits humanistisch (im Sinne einer „Renaissance des 12. Jahrhunderts“) geprägtes, literarisches Zentrum im mittelalterlichen Europa. Von der überregionalen Bedeutung dieser Werke zeugen Romane wie Heinrich von Veldekes Eneasroman (ca. 1170–1187) oder Geoffrey Chaucers "Troilus and Criseyde" (1380–1387). Alle diese Werke sind im höfischen Milieu angesiedelt und beziehen ihre Stoffe aus der Antike, weswegen sie als Antikenromane bezeichnet werden.

Nordeuropäische und christliche Stoffe (etwa der Tristan, der Artus-, und der Grals-Stoff) verbreiteten sich seit der Mitte des 12. Jahrhunderts in der Romanliteratur. Im höfischen Milieu angesiedelte Liebesbeziehungen dominieren als Sujet. Der Standardplot der mittelhochdeutschen Artusromane eines Hartmann von Aue oder Wolfram von Eschenbach, in denen der Held von der Geliebten getrennt wird und zahlreiche Abenteuer (Aventiuren) zu bestehen hat, zeigt formale Anleihen zum antiken Roman.

Prosavarianten der neuen Romane – der Prosa-Lancelot, der Prosa-Perceval, der Prosa-Tristan – entstanden ab dem frühen 13. Jahrhundert. Zur selben Zeit wurden auch die ersten Satiren auf diese Romane verfasst.

Seit Beginn der schriftlichen Fixierung von Epen (in Nordeuropa etwa ab dem Jahr 1000, um diese Zeit wurde das altenglische Beowulf-Epos niedergeschrieben) entwickelten sich hier rasch Kunstformen, die auf die Möglichkeit, den Text schrittweise zu komponieren, essentiell angewiesen waren. Der soziale Ort der Epik legte sich im selben Moment fest: Handschriften benötigten bis zum Aufkommen des Papiers finanzstarke Auftraggeber. Ab dem 13. Jahrhundert traten neben den adligen Auftraggebern auch reiche städtische Handelsherren als neue Interessenten auf. Sie erwarben mit Liederhandschriften von Rittern eine Kultur, an der sie selbst vom Stand keinen Anteil haben konnten. Die bedeutendste mittelhochdeutsche Liederhandschrift, der Codex Manesse, weist als Auftragswerk diese bürgerliche Herkunft auf.

Eine Vielzahl von Erzählformen blieb gegenüber der Epik in der Tradition mündlicher Überlieferung: Bis heute bewahrte sich von ihnen allen der Witz seinen Ort mitsamt festgelegten Erzählmustern von Fragen und kuriosen Antworten oder dem Dreierschritt, bei dem der letzte Schritt die Pointe birgt. Der Witz selbst zirkulierte bis in das 19. Jahrhundert neben Langformen mündlichen Erzählens wie dem Schwank, der Fabel, dem Märchen, dem Exempel (mittelhochdeutsch bîspel), das in Büchern wie auf der Kanzel Einschub finden konnte, um eine jeweilige moralische Sentenz zu illustrieren.

Zur Kunst stiegen die kurzen Erzählformen mit den berühmten Erzählzyklen auf – in Europa wie im islamischen Raum. Nezāmis "Die sieben Bildnisse" (1198), die "Erzählungen aus Tausendundeiner Nacht" (älteste überlieferte Handschrift c. 1450), Boccaccios "Decamerone" (zwischen 1349 und 1353) und Chaucers "Canterbury Tales" (zwischen 1386 und 1400) teilen das Muster der Präsentation: In einem Zyklus werden die einzelnen Erzählungen von Erzählern und Erzählerinnen angeboten, die selbst in einer Rahmenhandlung agieren. Die narrative Brechung erlaubt es dem Autor Boccaccio oder Chaucer, sich von den Erzählern seiner Sammlung beliebig zu distanzieren. Er gibt im Zweifelsfall wieder, was diese Binnenerzähler von sich gaben: Erzählungen, die nicht ihn, sondern diese selbst und die Situation, in der erzählt wurde, charakterisieren. Verschiedene Stoffe und divergierende Sentenzen ließen sich so nebeneinander platzieren. Die einzelnen Erzählungen gewannen in den Zyklen eine eigene Erzählkunst:

Mit dem ausgehenden 13. Jahrhundert behauptete sich die Novelle, so der übergeordnete Gattungsbegriff, der sich in der frühen Neuzeit herauskristallisierte, als ernstzunehmende Alternative zur Versromanze. Kritik am Heldentum antiquierter Heldenlieder, an der Erzählform des Epos, seiner Reihung von Kampfesproben und seinem Mangel an klug geplanten Interaktionen wurde erstmals in Novellenzyklen formuliert und von ihnen aus im 16. und 17. Jahrhundert zum Standard weiterführender Romankritik wie zum Plädoyer für die Novelle als einzige realistische Alternative zum antiquierten Roman.

Die populäreren der frühmodernen Historien bereiten der Literaturwissenschaft im Rückblick klare Einordnungsprobleme: Jehan de Mandevilles Bericht seiner Orientreise aus den 1370ern, in billigen gedruckten Ausgaben unverändert bis in das späte 18. Jahrhundert hinein verkauft, ist vollgefüllt mit spektakulären Fiktionen, wie etwa der einfüßigen Äthiopier. Einer Einstufung als Roman steht hier mehreres entgegen: es fehlt die romanhafte Handlung, der Held, der ein Leben erlebt. Gravierender ist: das einfache Publikum, das die "wundernswürdige Historie" las, scheint an der Wahrheit oder Unwahrheit des Gelesenen uninteressiert gewesen zu sein. In eine andere Grauzone führt Thomas Malorys "Le Morte Darthur" (1471): Der Leser erhält hier alle Erzählungen von König Artus samt magischen Details von Zaubereien, mit denen Helden etwa die Gestalt anderer annahmen, um in deren Körpern zu agieren. Die erste gedruckte Ausgabe beginnt dessen ungeachtet 1485 mit einem Vorwort, in dem der Herausgeber William Caxton den Text als "wahre Historie" einstuft. Der moderne Leser wird der kompilativen Historie Qualitäten eines Prosaromans zuerkennen: Hier wird im fiktionalen Raum erzählt. Zeitgenössische Urteile gehen in eine andere Richtung: Man las Historien und ließ Fiktionen dabei zu. Historiker sahen sich regelmäßig dazu aufgerufen, Lücken zu füllen. Beliebt waren die Reden, die auch bei fehlender Überlieferung ausgeschrieben wurden. Sowohl Historiker als auch Romanautoren schrieben mit dem Ziel, zu unterrichten. Uns trennt hier unsere Organisation der Debattenfelder vom 15. Jahrhundert.

Zwischen 1400 und 1700 gerät die Geschichtsschreibung in eine Krise. Sie kulminiert im 17. Jahrhundert in der Pyrrhonismusdebatte mit ihrer letztlich unbeantwortbaren Frage, wann wir einen historischen Bericht für erwiesen erachten können. Eine Quellendiskussion muss Indikatoren dazu liefern, meinten Pierre Bayle und die Autoren um ihn. Die Geschichtsschreibung verzichtet im Verlauf dieser Kontroverse auf die Absicht, mit einer wahren Erzählung zu belehren und präsentiert stattdessen die Materiallage in kritischer Diskussion. Geschichtsbücher sehen von nun an Romanen nicht länger ähnlich.

Historiker können in der Folge von der Mitte des 17. Jahrhunderts davon abrücken, sich vom Roman mit demselben Nachdruck zu distanzieren, mit dem sie sich von jeder geschichtlichen Unwahrheit distanzieren müssen: Unwahre Historien werden durch Quellenkritik disqualifiziert; Romane können dagegen als eigene Kunstform der Erzählung die Anerkennung von Kunstliebhabern finden.

Als kritische Diskussion gewinnt die Geschichtswissenschaft ab dem 17. Jahrhundert in Westeuropa Bedeutung als Verhandlungsfeld partei- und konfessionsübergreifender Kontroversen. Sie ist im 19. und 20. Jahrhundert ein Diskurs, der die Einsetzung historischer Kommissionen rechtfertigt, die politische Fehlentscheidungen einer (zumindest wissenschaftlich) konsensuellen Bewertung unterziehen.

Während sich die Geschichtsschreibung vom Roman wegbewegt, gewinnt dieser in der frühen Neuzeit eigene Anerkennung als Kunst und am Ende als Literatur, das Wort muss zu diesem Zweck im 19. Jahrhundert neu definiert werden. Hierfür sind bis 1700 mehrere Entwicklungsschritte verantwortlich, die aus einer erheblichen Legitimationskrise des Romans des 16. Jahrhunderts resultieren. Der Frühdruck schuf zunächst Raum für billigere Historien. Mit dem 16. Jahrhundert finden diese ein zunehmend breites Publikum. Eine Differenzierung setzt ein, in der sich ein neuer Markt eleganter Bücher vom entstandenen niederen wie vom akademischen Markt wissenschaftlicher Bücher absetzt. Der Erfolg des "Amadis", der ab den 1530ern eine erste internationale Lesemode auslöst, zieht am Ende die erste kritische Debatte im neuen Feld der eleganten Lektüre nach sich. Die Debatte um den "Amadis" wird im 17. Jahrhundert als offener Wettstreit der modernen Genres des Romans ausgetragen. Neue heroische Romane, alternative satirische sowie an Novellensammlungen orientierte Romane bringen sich in eine Gattungsdiskussion ein, in deren Verlauf unter der wachsenden Liebhaberschaft der belletristischen Produktion der kunstvolle Roman gegenüber kunstloser Produktion gerechtfertigt wird.

Mit Huets "Traitté de l’origine des romans" setzt 1670 die Würdigung dieser Reform und die moderne Romaninterpretation ein. Romane werden in den nächsten Jahrzehnten neu bewertet. Man liest sie mit Bildung, um fremde Kulturen und vergangene Epochen zu verstehen. In der zweiten Hälfte des 18. Jahrhunderts verändert die neue Romaninterpretation den Umgang mit Poesie: Man beginnt, Dramen und Gedichte in einem neuen Verbund der Gattungen, die sich angeblich gemeinsam entwickelten, zusammen mit Romanen als fiktionale, künstlerisch gestaltete, kulturelle Produktion zu analysieren. Die neue Wissenschaft wird im 19. Jahrhundert zum zentralen Bereich der Literaturwissenschaft. In den westlichen Gesellschaften organisiert diese Mitte des 19. Jahrhunderts gegenüber der Historik eine eigene Auseinandersetzung, in der es um die Überlieferung künstlerischer Fiktionen als kulturellen Indikator geht.

Bücher wie Malorys "Morte Darthur" oder Mandevilles "Reisen" entstammen letztlich einer Kultur, in der Geschichte nicht die heutige Bedeutung als Austragungsort kritischer Diskussionen hatte – einer Kultur auch, in der die Literaturwissenschaft keinen eigenen Bereich der Fiktionen zu nationaler Kunst zusammenstellte. Beide Bereiche sind Teil einer säkularen Debattenlandschaft, die im 19. Jahrhundert in Westeuropa gegenüber der bis dahin politisch-religiös definierten aufgebaut wurde. Die Entwicklung der Mentalitätsgeschichte erweckt den Eindruck, als hätten die Menschen zu Beginn der Neuzeit noch in Überresten mittelalterlichen Aberglaubens Wirklichkeit und Fiktionen durcheinandergebracht.

Der Buchdruck schuf unverzüglich neue Marktfelder: Flugblätter, Frühformen der Zeitung, religiöse Streitschriften − hierfür hatte es bislang keine vergleichbaren Medien gegeben. Die Wissenschaften erlangten mit dem Druck die Möglichkeit, Standardausgaben von Texten zu erstellen, die am Ende in identischen Ausgaben in Fachbibliotheken zu Referenzwerken würden − sie zogen wenig später ein Besprechungswesen nach sich, das die Qualität der neuen Fachbücher observierte.

Geschichtliche Darstellungen hatten in diesem Spektrum keinen klaren sozialen Status. Fürstenhäuser gaben sie in Auftrag, um sich mit ihnen zu schmücken, reiche Städter eiferten als Handschriftenbesitzer der Aristokratie nach, Bürgerinnen sammelten Beschreibungen von Heiligen und erbauliche Marienleben. Der Druck schuf in diesem Feld am ehesten die Möglichkeit der verbilligten Produktion. Die Verleger suchten beliebte historische Handschriften zusammen und überführten sie mit nachlassender Sorgfalt in gedruckte Fassungen, von denen sich weiter nachdrucken ließ. Mitte des 16. Jahrhunderts lag auf diesem Feld mit Rittererzählungen, Schelmengeschichten wie "Till Eulenspiegel", Heiligenlegenden, und erbaulichen allegorischen Fiktionen wie den "Sieben weisen Meistern" ein breites Segment billiger Bücher vor, das über die nächsten drei Jahrhunderte hinweg kaum noch Veränderungen erfahren sollte. Die Titel wurden mit groben Holzschnitten ausgestattet, in den Textfassungen nur geringfügig modernisiert, gekürzt und verstümmelt, wo sich der Profit maximieren ließ − eine Ware, die bevorzugt ohne Jahres- und Druckerangaben in den Handel kam; sie unterlag keinen Moden und verkaufte sich am besten als zeitlos bekannt an ein Publikum, das gerade das lesen wollte, was man angeblich schon immer gelesen hatte.

Hier entstand ab 1530 erstmals eine Ware, die in den Städten die Handwerksschicht und die Lehrlinge erreichte und auch auf das Land verkauft wurde, − für viele jugendliche Leser ein Durchgangsstadium, das zu besseren Romanen führen konnte. Vorsicht ist jedoch vor dem Urteil angebracht, der Markt der Volksbücher sei Vorgänger der heutigen Trivialliteratur. Die Volksbücher bildeten ein geschlossenes Segment von Titeln ähnlichen Designs, das neben historischen Erzählungen religiöse und pseudowissenschaftliche Titel sowie moralische und medizinische Ratgeber enthielt. Nach aufklärerischen Bildungsinitiativen im Lauf des 18. Jahrhunderts geht dieses Marktsegment mit der Wende ins 19. Jahrhundert weitgehend verloren. In den 1840ern setzt die romantische Neuentdeckung ein. Das Wort „Volksbücher“ wird nun festgelegt, um zu unterstellen, dass hier ungekünstelte, dem Volk nahe Bücher entstanden. Der heutige Markt der Trivialliteratur (dazu weiter ) entwickelt sich im selben Geschehen nicht aus den Volksbüchern heraus, sondern aus den eleganten belles lettres des 17. und 18. Jahrhunderts, als sich von diesem die hohe Produktion nationaler literarischer Werke im späten 18. Jahrhundert abzuheben beginnt.

Die Untergattung des höfisch-historischen Romans entsteht in kritischer Auseinandersetzung mit dem vielgelesenen Amadis-Roman. Dabei wurden insbesondere die Unwahrscheinlichkeit der Handlungsführung sowie die frivolen Passagen kritisiert. Allerdings wurde dessen „grausame“ Länge übernommen, denn die höfisch-historischen Romane gehören bis heute zu den umfangreichsten Werken der Literatur überhaupt. Die Autoren versuchten jedoch durch die Orientierung an historischen Ereignissen den Grad der Wahrscheinlichkeit des Erzählten zu erhöhen.

Zu den wichtigsten nichtdeutschen Romanen dieser Subgattung gehören:

Für den deutschen Sprachraum sind vor allem die folgenden Romane zu nennen:

Die Romane Barclays, der Scudéry wie Anton Ulrichs enthalten als entscheidendes Merkmal Bezüge zur Gegenwart und zu den Moden, die sich mit den neuen Romanen verbreiteten. Die galante Conduite und der galante Stil werden im Lauf des 17. Jahrhunderts die entscheidenden Verkaufsargumente der neuen Ware: Man liest sie, um Muster für Komplimente zu erhalten, Briefe, Reden, übernahmefertige Dialoge des höfischen Umgangs zwischen den Geschlechtern.

Das trifft für die Großromane zu, die sich als Spiel neuer europäischer Hofkultur behaupten, in ganz anderem Maße jedoch auch für die mit 300 bis 500 Seiten sehr viel handhabbarere Ware, die im 17. Jahrhundert einen Kundenstamm von Bildung und Geschmack unter jungen städtischen Lesern erobert. Hier entsteht in der zweiten Hälfte des 17. Jahrhunderts ein besonderer Markt für junge, wohl überwiegend noch unverheiratete Frauen, die galante Romane dieses kürzeren Formats goutieren – angesprochen nicht zuletzt von ihren gleichaltrigen Heldinnen, die als Prinzessinnen Indiens oder des antiken Persiens unter Lebensgefahr Identitäten wechseln, in Männerkleidung und weit unter ihrem Stand fliehen und Schutz suchen, bis sich ihre Lage bessert.

Das Etikett "satirischer Roman" wird im Lauf des 17. Jahrhunderts auf ein breites Spektrum von Titeln mit unterschiedlichen historischen Wurzeln angewendet: Römische und spätantike Satire, insbesondere die Romane Petrons und Lucians, mittelalterliche Schwankgeschichten mit Helden wie Till Eulenspiegel sowie die Romansatire, die bereits im Mittelalter in erheblicher Spannbreite existierte. Der Rosenroman umfasst satirische Passagen. Heinrich Wittenwilers "Ring" ist selbst eine satirische Schlacht im Persiflagen auf das höfische Heldentum.

Eine Reihe von Entwicklungen kennzeichnet die Produktion, die hier mit dem Druck aufkommt. Helden gewinnen Konsistenz: Wo Till Eulenspiegel zwar einen typischen Charakter hat, jedoch Held einer Sammlung überlieferter Episoden bleibt, gewinnen vergleichbare Helden des 16. und 17. Jahrhunderts Lebensgeschichten. Der anonyme "Lazarillo de Tormes" (1554), Richard Heads "English Rogue" (1665) und Grimmelshausens "Abenteuerlicher Simplicissimus" (1666/1668) stehen in dieser Entwicklungstendenz.

Gegenüber den galanten Helden des heroischen Romans, die sich spanischer und dann zunehmend der französischen Conduite bedienen, internationalen Verhaltensmustern, zeichnen sich die satirischen Helden als Leute ihres Volkes aus. Die Titel kennzeichnen die Volkszugehörigkeit: Grimmelshausens Held ist "Simplicissimus Teutsch", Heads Held der "English Rogue", der auf dem Markt mit einem Französischen Schelmen konkurriert. Rozelli: "Der wundernswürdige Neapolitaner" ergänzt das Spektrum mit einem Italiener (französischer Verfasserschaft) zu Beginn des 18. Jahrhunderts. Hier entstand unter Lesern europäischen Geschmacks und deutlicher Distanz zum einfachen Volk eine Lust am nationaltypischen, die im Lauf des 17. Jahrhunderts in die Ausgestaltung nationaler Charaktere und Identitäten mündete. Die national identifizierten Helden wurden gleichzeitig zunehmend zu Akteuren der internationalen Geschichte, nicht als deren Lenker, sondern als Menschen, die von Europas wechselvollen Geschicken betroffen waren und zumeist kurios überlebten. Der Produktion haftet wiederholt eine Nostalgie des Rückblicks auf gar nicht so lustige Zeiten an. Berufsstände wurden Gegenstand der Satiren, zum Teil mit Romanen, deren Helden gezielt mit Berufen ausgestattet sind, öfter noch dadurch, dass diese Helden zum Überleben fortwährend neue Überlebensstrategien und mit ihnen neue Identitäten und Berufe benötigten.

Zu den Helden, die mit ihren Streichen, Geschicken und Missgeschicken die grassierenden Laster entlarven, kam eine eigene Entwicklungslinie von Romanhelden, die vor allem als Persiflagen auf Helden des hohen Romans agieren. Rabelais’ "La vie très horrifique du grand Gargantua" (1532–1554) bietet hier eine Welt zu Riesen übersteigerter Bauerntölpel, von exzessiver Körperlichkeit. Das derbdreiste Heldenepos begeisterte intellektuelle Leser mit Lust an der Zerstörung eleganter populärer Lesestoffe. Als weiterreichende Satire auf den "Amadis" erschien 1605 und 1615 Cervantes' "Don Quixote", der Roman des Mannes, den die Lektüre von Ritterromanen mit kauzigen Fehlwahrnehmungen der realen Welt gegentreten ließ. Scarrons "Roman Comique" nutze eine Truppe reisender Schauspieler zur dezidierten französischen Reflexion über die Welt und die Dichtung nach Cervantes und seiner Romansatire. Lesages "Gil Blas" (1715–1735), Fieldings "Joseph Andrews" (1742) und "" (1749), Diderots "Jacques le Fataliste" (1773, gedruckt 1796) sind Ausläufer der Traditionslinie im 18. Jahrhundert; Jaroslav Hašeks der "Der brave Soldat Schwejk" (1921–1923) und Günter Grass "Die Blechtrommel" (1959) griffen im 20. Jahrhundert deutlich auf Heldentum und Erzählmuster dieses Feldes zurück.

Dass im Englischen und im Spanischen heute von "Novels" bzw. "Novelas" statt von Romanen "(Romances)" gesprochen wird, ist eine späte Folge des Debakels, das der "Amadis" als Roman seinem Nachfolger bereitete. Dabei wurde der "Amadis" zu diesem Zeitpunkt nirgends in Europa länger gelesen. Kritikern war gerade noch bekannt, dass es in ihm um Ritter ging, die Prinzessinnen aus den Händen grausamer Riesen befreiten, dass er auf zahllose Bände anschwoll, und den Verstand kostete mit seinen Erfindungen von Heldentum. Die "Amadis"-Kritik aus dem "Don Quixote" (1615) war zum Standard der Romankritik geworden, obgleich der Roman der Gegenwart sich selbst gerade einem Spektrum gegenüber dem "Amadis" auffächerte.

Der moderne heroische Roman und sein satirisches Gegenstück teilten mit dem "Amadis" Länge und Erfindungsreichtum. Die Alternative wurde an dieser Stelle die Novelle. Sie etablierte im Verlauf in den 1670ern das Wort "Novel" auf dem englischen Buchmarkt. Im Deutschen zeigt sich der Einfluss in Romanen von „curieusen Begebenheiten“. Ist die Novelle in den 1660ern und 1670ern noch vor allem die aktuelle, neue Geschichte, Teil des skandalösen Marktes, so erhält sie im frühen 18. Jahrhundert den Status eines Klassikers. Wegbereitend ist hier im Englischen die "Select Collection of Novels" (1722–1722), die unten noch eingehender erwähnt wird. In den 1720ern werden zunehmend längere "Novels" interessant: lange Liebesgeschichten, wie sie Eliza Haywood vorlegt, sorgen dafür, dass das Wort seine Beschränkung auf kurze pointierte Geschichten verliert. Mitte des 18. Jahrhunderts ist "Romance" im Englischen zwar immer noch der Dachbegriff für den Roman als Gattung. Die "Novel" ist jedoch zu diesem Zeitpunkt das gegenwärtige Genre geworden, das sich von abenteuerlicher Romankunst verabschiedet. Die heutige begriffliche Fixierung richtet sich im Englischen in den 1790ern ein, als die Romantik die "Romance" mit all ihren Schauern des Erfundenen für sich neu entdeckt. Das Wort "Novel" wird im selben Prozess im Englischen zum neutralen Gattungsbegriff, eine Situation in der man ein weiteres Wort für die ursprüngliche Novelle benötigt: „Novella“ wird hier aktiviert gegenüber „Novel“, von nun an das Wort für den langen Roman.

Als kurzer Roman von Neuigkeiten übt die Novelle im 17. Jahrhundert entscheidenden Einfluss auf das gesamte Bild vom Roman aus. Die Gattung hat Wurzeln in die Novellensammlungen Boccaccios und Chaucers. Mit Cervantes’ "Novelas exemplares" (1613) erfolgt die offene Behauptung, sie könnte als Gattungsalternative im Zentrum eines neuen Spektrums stehen.

Die meisten Gattungsdefinitionen – Du Sieurs "Sentimens sur l’histoire" (1680) gewinnen hier Bellegardes Plagiat der Überlegungen und deren Übersetzungen größeren Einfluss – leiten die einzelnen Gattungsmerkmale weitgehend von der Kürze ab. Mit der Kürze geht der Anspruch darauf verloren, den Leser in eine angenehme Dauerlektüre in eine Welt eigener Ideale zu entführen. Die kurze Erzählung muss sich keine Mühe mehr geben, lange Passagen von Reden und Beschreibungen zu entwickeln. „Bombast“ ist, so die Vertreter der Novelle, das Hauptmerkmal der heroischen Romane. Gezielte Kunstlosigkeit reklamieren die neuen Autoren für sich. Ihre Erzählungen sind unverziert, kurz, pointiert. Im Idealfall stellt der Autor klar, warum er erzählt: die folgende Geschichte soll ein Exempel geben für eine Tugend oder Untugend, einen Umstand modernen Lebens, eine erstaunliche und konsequenzenreiche intime Entscheidung. Verlauf, Stil, die gesamte Konzeption der Novelle müssen sich von da an an die Erzählintention, das zu gebende Beispiel zurückbinden lassen. Es gibt in der Novelle kein weiteres Einverständnis mehr darüber, was hoher Stil ist, stattdessen wird konzise auf die zumeist überraschende Schlusswendung, die Pointe, hinerzählt. Die Novelle ordnet sich mit diesen Vorgaben zwischen den heroischen und den satirischen Roman ein. Ersterer wollte durch Helden unterrichten, die man nachahmen will, letzterer durch Helden, deren Lächerlichkeiten man bei Bewusstsein eigener Reputation nicht nacheifern wird. Im neuen Kurzroman geht es dagegen nicht darum, durch eine Identifikation mit den Helden sich zu verbessern. Das Exempel selbst unterrichtet: Wenn man Dinge so tut wie hier in der Geschichte, kann einem eine solche Überraschung passieren. Die Helden werden zu Menschen mit Stärken und Schwächen. Regelmäßig gewinnen die Intriganten. Mitgefühl mit unterlegenen Opfern gewinnt in den neuen Romanen die Rolle, die moralische Balance auch zugunsten unterliegender Protagonisten herstellen zu können.

Gegenüber dem heroischen Roman, der ewige Ideale in Verkörperungen zeigen will, zeigt die Novelle in Tradition der mittelalterlichen Novellistik Spezifik der Orte und der Zeiten. Beliebt sind Geschichten, die sich angeblich tatsächlich so zugetragen haben sollen. Die Novelle verbreitet sich auf dem internationalen Buchmarkt mit dem Angebot lokaler Perspektiven, und sie beschleunigt im selben Moment den Aufbau nationaler Gattungstraditionen: Die Helden von Scarrons "Roman Comique" diskutieren Mitte des 17. Jahrhunderts über die Vorteile der neuen Gattung in nationaler Perspektive: Frankreich müsse Geschichten vorlegen, wie die von den Spaniern als "Novelas" bezeichneten. Marie-Madeleine de La Fayette verfasst in der Folge noch mit ihrer "Zayde" (1670) eine Geschichte im neuen spanischen Stil und mit ihrer "Princesse de Clèves" (1678) deren französisches Pendant: eine exemplarische Geschichte nach höfischer französischer Mode. Deutsche Studenten liefern ab den 1690ern „einheimische Geschichten“. Londoner Leserinnen riskieren sich um 1700 als Autorinnen im gezielt kunstlosen Genre. Die neue Gattung ist unter diesen Vorgaben gezielt skandalös. Angeblich wird hier nur für die lehrreiche Exempel erzählt, ein Prätext, unter dem sich beliebig indiskret in Privataffären vordringen lässt. Die kurzen Geschichten ziehen in den 1670ern in die skandalöse Publizistik ein. Journale und seriöse Historien bieten zur Auflockerung kleine Erzählungen in größeren Zusammenhängen.

Die Novelle öffnet im selben Moment Europas Blick auf außereuropäische Erzähltraditionen: Die "Geschichten aus Tausendundeiner Nacht" werden zum europäischen Markterfolg des frühen 18. Jahrhunderts und sind eine Novellensammlung, kein heroischer und kein satirischer Roman.

Für die Geschichte hat die vorübergehende Verschmelzung der Gattungen Romans mit der Novelle bis heute andauernde Nachwirkungen. Die vor den 1660ern bestehende Romandiskussion fragt nach Idealen und hoher Sprache. Mit der Novelle werden beliebige Erzählintentionen diskutierbar. Novellen werfen Schlaglichter auf das Leben, auf Menschen in überraschenden Situationen. Zu diskutieren ist hier der Realismus der Erzählung und die Moral des jeweiligen Beispiels. Die neue Diskussion hat Einfluss bis in die heutige Romanbetrachtung. Erzählsituationen sind in der Novelle interessant: Der Großroman verabschiedet sich von der Abenteuerserie. Das Abenteuer weicht der Intrige, dem Plan, der zumeist anders ausging als angedacht. Das Spiel mit der Erzählung dringt über die Novellistik im 17. Jahrhundert in den modernen Roman ein.

Mitte des 18. Jahrhunderts ist der große Roman dank der Novellistik wieder legitim. Samuel Richardsons "Pamela or Virtue Rewarded" ist ein langer Roman und im Titel eine exemplarische Novelle: Hier wird ein Beispiel dafür gegeben, dass Tugend sich lohnt. Deutsche Romanautoren schreiben von Hunolds "Satyrischem Roman" (1706/1710) bis zu Schnabels "Wunderliche Fata einiger See-Fahrer" (1731–1743) und Gellerts "Schwedischer Gräfin G***" (1747/48) im Genre der aktuellen exemplarischen Geschichten. Die kurze Novelle steht zu diesem Zeitpunkt bereits als skandalöse Gattung in Misskredit. Heutige Literaturgeschichten scheiden sie in der Regel aus der Romangeschichte gänzlich aus: Es gibt für sie den Schritt der vom "Barockroman" in den Roman der Aufklärung. Häufig ist zu lesen, dass sich die Novelle zwischen dem Mittelalter und Goethes "Novelle" von 1828 gänzlich vom Markt verabschiedete. Die Aussage ist vor allem das Ergebnis einer rückgreifenden Bereinigung der Romangeschichte.

Ende des 17. Jahrhunderts warf der Roman neuen Reformbedarf auf. Konservative Kritiker fuhren fort, von den Verführungen „amadisischer“ Phantasterei zu sprechen, andere monierten, dass der Roman nach wie vor in der Historie eingebettet blieb – die Messkataloge nahmen diese Einordnung vor. Modernere Kritiker verteidigten im selben Moment den Roman mit seinen Ausgriffen in die Realhistorie. Sie taten dies aus zwei Gründen: Die Novellistik wie der heroische Roman, der sich auf das Gebiet der Schlüsselromane wagte, hatten sich reformiert. Die Beobachtung wirklicher Charaktere war sein neues Markenzeichen; die Gefahren des "Amadis" gingen von modernen Romanen nicht mehr aus. Gleichzeitig hatte sich mit den Schlüsselromanen der Scudéry, mit romanhaften Briefsammlungen der d’Aulnoy und ersten Briefromanen wie Aphra Behns "Love-Letters" (1684–1687), mit novellistischer Journalistik wie sie DuNoyer anbot, mit „curieusen“ romanhaften Memoires anonymer französischer Autoren, mit populären Journalen wie dem "Mercure Galant" und mit den aktuellen "nouvelles historiques", wie sie der Abbé de Saint Réal schrieb, ein neuer romanhafter Raum inmitten der Geschichtsschreibung eröffnet, auf dem Autoren elegant und bei Bedarf regimekritisch agierten. Nachhaltigen Einfluss übten hier die romanhaften Aufarbeitungen der Zeitgeschichte aus, die heute Gatien de Courtilz de Sandras zugeschrieben werden (wie die Geschichte des "Mannes mit der eisernen Maske", die unterstellte, dass Ludwig XIV. einen geheimen Bruder verbarg und damit seine Machtansprüche sicherte – die Geschichte, die Alexandre Dumas, mit den "Drei Musketieren" im 19. Jahrhundert berühmt machte). Aufgeschlossene Marktbeobachter wie Pierre Bayle sahen erheblichen Grund, die potentielle Kritikfähigkeit dieses Marktes zu verteidigen, und eher eine kritische Lektüre als die Zensur bzw. Abschaffung solcher Werke zu fordern.

Die zwei Optionen, unter denen Romane in die Historie ausgreifen konnten, schufen ein übersichtliches Schema der Genres, auf die Titelblätter und Vorreden anspielten: Neue Titel konnten vorgeben, Romane zu sein, sich jedoch bei der Lektüre als Schlüsselromane erweisen. Sie konnten auf der anderen Seite wie Defoes "Robinson Crusoe" (1719) als wahre Historie in Form eines Romans angeboten werden. Hier wie dort unterschied der Markt zwischen privaten und öffentlichen Angeboten:

Die Aufteilung bot bei Bedarf Autoren skandalöser öffentlicher wie privater Offenbarungen die Sicherheit, behaupten zu können, sie hätten doch lediglich einen Roman geschrieben. Wer etwas anderes behaupten wolle, müsse erst einmal in einem Gerichtsprozess nachweisen, dass ihre Veröffentlichungen unliebsame Wahrheiten publik machten.

Das erwünschte Zwielicht herzustellen erforderte Kreativität. "Robinson Crusoe" zeigt das: Das Titelblatt von 1719 behauptet, die Abenteuer ("Adventures") eines Seemanns zu veröffentlichen (siehe Abbildung rechts). "Adventures" stand auch auf dem Titelblatt der englischen Ausgabe von François Fénelons berühmtem fiktionalem Roman "Telemach" (siehe Abbildung links). Das Titelblatt von "Robinson Crusoe" überbietet den Rivalen mit spektakulären Adjektiven. In der Vorrede will der Verleger vorgeblich bestätigen, dass es sich um einen wahren Bericht handelt, und streut eben dadurch neue Zweifel.

Weder in der Grenzüberschreitung gegenüber der wahren Geschichte, noch im Realismus, den Defoe riskierte, war sein Buch 1719 ein Novum. Weit realistischer war soeben Constantin de Rennevilles Bericht seiner Gefangenschaft in der Bastille "Inquisition Françoise" (1715) – es ist bis heute unklar, was hier wahr und was erfunden war.

Dass Defoe sich mit "Robinson Crusoe" von der französischen „Romance“ abwandte, ist eine problematische Behauptung. Die besagte „Romance“ hatte bereits in den 1670ern der novellistischen „Novel“ Platz gemacht. Der neue große Roman erweiterte die aktuelle Novellistik mit einem Angebot außergewöhnlich abenteuerlicher Fiktionalität. In die Entwicklung des 18. Jahrhunderts passte sich "Robinson Crusoe" am Ende perfekt ein: Sie führte zu einer Literatur gezielt fiktionaler Werke, die sich mit künstlerischen Mitteln mit der Realität auseinandersetzen. Jean-Jacques Rousseau nahm diese Neueinordnung Defoes in seinem Roman "Émile, ou De l’éducation" (1762) wegbereitend vor.

Die anglistische Forschung verband das 18. Jahrhundert mit Theorien vom Aufstieg des Romans. "The Rise of the Novel" titelt die maßgebliche Untersuchung, die Ian Watt im Blick auf die Romane Defoes, Richardsons und Fieldings 1957 vorlegte. Die Geschichte der englischen Literatur gewinnt an dieser Stelle maßgebliche Bedeutung. Mit ihr verknüpft sich die These eines Einflusswechsels. Französische Autoren bestimmen bis in das frühe 18. Jahrhundert hinein den europäischen Markt, englische gewinnen mit dem Veröffentlichung "Robinson Crusoes" (1719) Bedeutung. Nachweisen lässt sich für die englische wie für die deutsche und die französische Produktion fiktionaler Prosa für das 18. Jahrhundert ein nach relativ stabilen Zahlen für das 17. Jahrhundert beschleunigtes Wachstum.

Die gesamte Buchproduktion lag im Zeitraum 1600 bis 1800 in Sprachen wie Deutsch und Englisch bei 1500 bis 3000 Titeln. Bis in die 1750er war dieses Angebot dominiert von der wissenschaftlichen Produktion an Literatur, von Theologie und von einer tagespolitischen Produktion, zu der Pamphlete, Journale und Zeitungen gehörten. Literatur im heutigen Wortsinn, Romane, Dramen und Gedichte, hatten an der gesamten Buchproduktion bis in die 1750er hinein einen marginalen Anteil von 2–5 %.

Der Anteil der Romane an der Gesamtproduktion lag entsprechend niedrig. Bis in die 1730er erschienen in Sprachen wie Deutsch und Englisch pro Jahr 20 bis 60 Romane. Die Romanproduktion in französischer Sprache lag etwas höher. Dies beruhte vor allem auf der Aufspaltung französischer Publikationen in einen innerfranzösischen und einen niederländischen Markt. Niederländische Verleger druckten, was in Frankreich der Zensur unterlag und verdoppelten so den Markt.

Mitte des 18. Jahrhunderts stieg die Gesamtproduktion an. Fiktionen trugen dazu maßgeblich bei. Das hat vor allem damit zu tun, dass das englischsprachige Verlagswesen sich nach 1750 dezentrierte und zunehmend für den Markt jeweils vor Ort produziert wurde. Nicht minder beginnt hier die Phase der Rückkoppelung des allgemeinen Marktes mit der Literaturkritik, die auf dem Gebiet der Fiktion in der zweiten Hälfte des 18. Jahrhunderts einen Kanon klassischer Werke der Romankunst etablierte, an denen sich neue Romane von nun an messen mussten.

Deutlich veränderte sich vom Mittelalter in das 18. Jahrhundert der Ort des Romans in der gesellschaftlichen Wahrnehmung. Die Epik des Mittelalters hatte sich mit aristokratischen Sitten und der Würdigung des Rittertums befasst. Romane wie der "Amadis" hatten die Ideale der Ritterlichkeit trivialisiert und zu gängiger Münze gemacht. Nationale Hofkultur bestimmte die Titel des 16. und 17. Jahrhunderts, diejenige Spaniens bis in die 1630er, die des französischen Hofes ab den 1640ern. Autoren wie die Scudéry hatten Mitte des 17. Jahrhunderts vor allem mit dem Versprechen verkauft, dass man ihnen die aktuellen Verhaltensformen französischer Etikette entnahm.

In den 1660ern ergab sich eine Aufspaltung französischer Publikationen in einen einheimischen und einen niederländischen Markt. Raubdrucker in Den Haag und Amsterdam betrieben die Zweitvermarktung der Pariser Verleger und sie agierten als Anlaufpunkte für Autoren, die in Frankreich nur noch unter Behinderung durch die Zensur verlegen konnten. Es entstand damals ein politisch brisanter, tagesaktueller von französischer Mode geprägter internationaler Markt, der Moden aller Art verkaufte. Arcangelo Corelli und Antonio Vivaldi veröffentlichten von Italien aus bei Étienne Roger in Amsterdam, demselben Verleger, der 1715 Rennevilles "L’inquisition Françoise" herausbrachte – einen der vielen Titel, die noch selben Jahres in englischer und französischer Übersetzung vorlagen. Der politische Markt war Drehscheibe der internationalen, modisch ausgerichteten Tagesaktualitäten geworden.

Von dieser internationalen Warte aus ließ sich eine dezidiert lokale Produktion inspirieren. Europäische Skandalromane finden im ausgehenden 18. Jahrhundert eine parallele lokale Produktion mit privaten Perspektiven in London wie in Leipzig, Halle und Jena. Entscheidend ist für die neue privatere Produktion, dass ihre Autoren bei den skandalösen intimeren Offenbarungen Anonymität wahren können. Frauen können dies in London, Studenten agieren ähnlich aus einer anonymen Masse heraus in den mitteldeutschen Universitätsstädten. Eine Blüte von Frauenromanen bestimmt das frühe 18. Jahrhundert in England, eine parallele Blüte von Studentenromanen skandalisiert die deutschen Staaten bis in die 1720er Jahre. Zu Beginn des 18. Jahrhunderts gilt es als einfach, Romane zu publizieren – sie suchen keine Bedeutung als Kunst zu erlangen, und bleiben, weitgehend unbesprochen, die Materie derer, die Romane lieben. Die Zensur beachtete religiöse und politische Schriften, kaum jedoch die privatere Romanproduktion.

Der Roman ist damit zu Beginn des 18. Jahrhunderts bei den Lesern angekommen, eine internationale Ware mit besonderen lokalen Zusatzangeboten. Sein eigentlicher Aufstieg beginnt mit der Verbreitung von Huets "Traitté de l’origine des romans" (1670), worin erstmals die Karriere der Gattung öffentlich manifest wird, sowie mit der Romankritik der Moralischen Wochenschriften, die am gesamten Ideal des Galanten Anstoß nehmen. Sie führen nicht zu einem Niedergang der Gattung, sondern zu einer Aufspaltung der Produktion und zu einer Rivalität unter Autoren, die sich Mitte des 18. Jahrhunderts dezidiert der Reform des Romans verschreiben. Es entsteht ein neuer "höherer" Markt, der sich auf die Romankritik einlässt. Mitte des 18. Jahrhunderts übernehmen gelehrte Literaturzeitschriften die mittlerweile öffentliche Aufgabe der Romankritik.

Der Roman kommt mit der neuen öffentlichen Beachtung in den 1780ern im Zentrum öffentlicher Wahrnehmung an, die sich in Zeitungen und Zeitschriften artikuliert. Es dauert von hieran noch ein weiteres halbes Jahrhundert, bis ihn die nationalen Bildungssysteme als literarische Gattung kanonisieren. Der Ort des Romans wird von Beginn des 19. Jahrhunderts an primär durch die Medien zugewiesen.

Knapp formuliert kann gesagt werden, dass der Roman im Lauf des 18. Jahrhunderts vom skandalösen Seitenast der historischen Produktion zum Medium einer Reform der öffentlichen Sitten aufsteigt. Seine Ausrichtung auf das Private und den privaten Leser machen ihn hier sowohl bedrohlich wie zum idealen Medium der Reform: Mit keiner anderen Gattung erreicht man den Leser so klar im Privatraum, mit keiner anderen gibt man ihm so tiefe Einblicke in geheime Gedanken von Helden. Das Ergebnis ist dabei weniger die Reform des Romans als die Aufteilung in eine sich auf die Diskussion einlassende und eine sich ihr entziehende Produktion. Sexszenen etwa gab es in Romanen des 17. Jahrhunderts, eine eigene pornographische Produktion kommt dagegen Mitte des 18. Jahrhunderts gegenüber der moralisch reformfreudigen auf.

Der Aufbau eines Kanons der Weltliteratur geht im Wesentlichen auf Huets "Traitté de l’origine des romans" (1670) zurück. Huet hatte die bald in eigenständigen Ausgaben erscheinende Arbeit – noch gab es keine Literaturhistorik, die sich um sie kümmern konnte – als Vorrede zur "Zayde" Marie de La Fayettes auf dem Romanmarkt selbst publiziert. Mit ihr veränderte sich vor allem die Rechtfertigung der Romanlektüre unter Liebhabern der Belletristik. Musste der Romanleser sich bis dahin dem Vorwurf stellen, in eine prekäre Pseudowirklichkeit zu entfliehen, so demonstrierte Huet, dass man Romane verschiedener Epochen und Kulturen mit einer neuen Interpretationspraxis voneinander differenzieren könnte: Zeiten und Kulturen der Weltgeschichte hatten aus ganz unterschiedlichen Gründen das Fiktionale als Bereich ausgestaltet. Man würde unter dieser Prämisse Romane gezielt lesen können, um mehr über die Sitten, Denkweisen und Konsumbedürfnisse anderer Zeiten und Kulturräume zu erfahren. Die neue Romanlektüre setzte wissenschaftliche Expertise voraus, sobald man die weltweiten Traditionslinien rekonstruierte, in denen sich Fiktionalität verbreitete.

Mit dem späten 17. Jahrhundert mehren sich Ausgaben fiktionaler Literatur mit Verweisen darauf, dass Huet diese Bücher in seiner Weltgeschichte der Fiktionen notierte. Die Romane Heliodors, die "Geschichten aus Tausendundeiner Nacht", Petron und Lucian wurden antike und internationale Klassiker. Gleichzeitig erschien mit Fenélons "Telemach" der Roman, der in den nächsten Jahrzehnten als Beleg dafür diskutiert wurde, dass in der Moderne nicht das heroische Versepos neu aufleben würde, sondern der Roman als dessen modernes Pendant und Ersatz.

Die moderne Romanproduktion erhielt einen eigenen Markt klassischer Moderne: bahnbrechend ist hier die in London zwischen 1720 und 1722 erschienene "Select Collection of Novels"; sie umfasste die aktuelle Novellistik mit Autoren von Machiavelli über Cervantes bis zu La Fayette (ohne bereits ihren Namen zu kennen).

Autoren der Gegenwart versuchten auf diesem Markt Fuß zu fassen, nachdem es Fénelon gelungen war, mit einer einzigen Publikation unverzüglich Klassiker zu werden. Unter bürgerlichen Namen zu veröffentlichen, wird in London in den 1720ern modern.

Dem Aufbau eines internationalen Klassikerfeldes der "belles lettres" folgen in der zweiten Hälfte des 18. Jahrhunderts Versuche, nationale in eigenen Reihen Klassiker zu etablieren.

Die Reform des Romans, die das 18. Jahrhundert anstrebte, zielte vor allen Dingen auf die Sitten, insbesondere die des privaten Zugriffs auf die Presse und die Öffentlichkeit. Christian Friedrich Hunold alias Menantes hatte zu Beginn des 18. Jahrhunderts noch seine Verleger mit den privaten Nutzungsmöglichkeiten der Presse und des Mediums Roman verblüfft. Skandalautorinnen und hatten sich in Romanen in London profiliert. Überwiegend hatten sie genauso wie ihr Publikum und ihre Romanhelden der jungen Generation angehört. Der Roman des 17. Jahrhunderts lebte von Idealen einer privaten Klugheit, die sich der Gesellschaft geschickt entzog (siehe eingehender dazu den Artikel Galante Conduite): Die Helden und Heldinnen müssen in der Regel zu geheimen Aktionen, Intrigen, greifen, um ihr privates Glück zu finden. Der Umgang mit Geheimnissen bestimmt den Roman bis weit ins 18. Jahrhundert hinein, sowohl in den Romanhandlungen wie im Spiel, das diese Titel auf dem Buchmarkt spielen: Romane publizieren private Geheimnisse. Dem steht bis weit ins 18. Jahrhundert hinein eine spezifische Selbstdefinition der Helden wie der Autoren zur Seite: Man definiert sich nicht über psychologische Identität, sondern über Reputation, den Ruf, den man verteidigt. Das Duell ist eine legitime Form, den Ruf gewaltsam zu verteidigen. Die öffentliche Darlegung der eigenen Sicht, das Waschen schmutziger Wäsche, die Diffamierung von anderen Behauptungen sind die Sache von Romanheldinnen und -autorinnen bis in die 1740er. Wer anderes behaupten will, muss sich entscheiden, gegen Offenbarungen aufzutreten und damit die Reputation des Autors herauszufordern. Dieses harte Reglement, in dem Romanhelden und -autoren sich durchgängig als öffentliche Akteure begreifen, wird im 18. Jahrhundert eingetauscht gegen ein weiches der Sensibilität und Empfindsamkeit.

Das empfindsame Verhaltensmodell geht von einem Individuum aus, das von Natur aus nur ungern öffentlich agiert. Scham, zu erröten, wenn von einem gesprochen wird, zeichnet das empfindsame Individuum aus. In Gesellschaft ist es nicht Spieler, der die eigene Reputation kalkuliert nutzt, sondern hilfsbedürftig. Wo Heldinnen des 17. Jahrhunderts ihren Eltern verheimlichen, wen sie lieben, um so den Freiraum zu gewinnen, ihre Ziele zu realisieren, fühlt sich das empfindsame Individuum hilflos. Es muss auf andere zutreten, Vertrauen wagen, die Eltern für das eigene Glück gewinnen. Liebe gegenüber allen Mitmenschen und Transparenz ihnen gegenüber zeichnen empfindsame Helden aus (Verhaltensratgeber des frühen Jahrhunderts definierten die Umwelt dagegen als feindlich und rieten zur Intransparenz). Den neuen Helden und Heldinnen steht im selben Moment eine neue Umwelt gegenüber, in der es gleichgeartete gute Menschen gegenüber intriganten Feinden gibt.

Samuel Richardsons "Pamela or Virtue Rewarded" spielt 1740/41 exemplarisch den neuen Grundkonflikt durch zwischen einer unschuldigen moralischen Heldin niederen Standes und einem ihr als Verführer gegenübertretenden Dienstherrn. Der Konflikt endet weder mit dem Ruin der Heldin wie in den Romanen Delarivier Manleys, noch mit einem gewitzten Siegeszug der vermeintlichen Unschuld wie in vielen Novellen; er mündet stattdessen neuartig in der Reform des im Status überlegenen Mannes. Die neuen Helden erleben sich selbst als von ihren Tugenden geleitet, kaum fähig Geheimnisse zu haben. Der Verlust des Gefühls, in „natürlicher“ Harmonie mit ihrer Umwelt zu leben, macht sie unglücklich. Sie entwickeln eigene empfindsame psychische Dispositionen, mit denen sie sich und anderen ihre Handlungsweise erklären können – offen und auf Mitgefühl und Unterstützung angewiesen, wo ihre Vorgänger gewitzt und auch bei tugendhaftem Charakter „verschlagen“ agierten. Vorläufer hat die neue Produktion in der französischen Novellistik. Insbesondere Marie de LaFayettes "Princesse de Cleves" (1678) eroberte hier dem neuen Verhalten Terrain.

Deutlich handhaben die Romanautoren des mittleren 18. Jahrhunderts ihre Werke als Proben. Die neuen Helden werden der Öffentlichkeit mit didaktischen Intentionen zur Verfügung gestellt: “Now first published in order to cultivate the Principles of Virtue and Religion in the Minds of the Youth of Both Sexes, A Narrative which has the Foundation in Truth and Nature; and at the same time that it agreeably entertains…”, so der Untertitel zu Richardsons "Pamela". Der neue Roman setzt Fiktionalität ein, um zu unterrichten und entwirft Menschen mit der Absicht, darüber zu diskutieren, ob hier nicht erstmals die menschliche Natur korrekt erkannt sei.

Die neuen Charaktere benötigen damit eine Wissenschaft von der geheimen Natur des Menschen, die bislang von Kultur deformiert wurde. Mit der Psychologie entsteht diese Wissenschaft im Parallelprozess. Gleichzeitig entwickeln die neuen Romane ein spezielles Interesse an Entwicklungen (auch dieses Wort ist Mitte des 18. Jahrhunderts neu, „Veränderungen“ machten Protagonisten von Romanen bis in die 1720er durch). Bildungs- und Entwicklungsromane kommen auf. Kindheit und Jugend werden Sujets des modernen Romans. Als Vorbilder dienen hier allenfalls die satirischen Romane des 17. Jahrhunderts, die lustige Schwächen ihrer Helden in ihrer Kindheit darlegten. In den Romanen Jean-Jacques Rousseaus werden in den 1760ern Entwicklungs- und Reifungsprozesse zum Gegenstand philosophisch experimenteller Fiktionen. Die Romane Laurence Sternes und Henry Mackenzies kosten in den 1760ern und 1770ern Entwicklungen ihrer Helden satirisch liebevoll zu „empfindsamen“ Charakterskizzen aus. Bildungsromane deutscher Autoren des 18. und 19. Jahrhunderts verknüpfen das neue Sujet mit einer Spannbreite von Gesellschaftskritik zu individueller historischer Reflexion.

Die neuen Verhaltensnormen werden Mitte des 18. Jahrhunderts zuerst an weiblichen Helden vorgestellt. Männliche Helden verhalten sich wenig später „empfindsam“, oft mit deutlicher Selbstironie. In den 1770ern werden entgegen den konsensorientierten Modellen Romanhelden interessant, die mit der Gesellschaft brechen, an ihrem Glück aus eigenen Dispositionen heraus keinen Anteil haben können. Johann Wolfgang von Goethes "Werther" (1774) setzt hier einen europäischen Maßstab. Von ihm geht Ende des 18. Jahrhunderts eine eigene Mode tragischer Helden aus, denen die Integration in die empfindsamen beengenden Verhältnisse misslingt.

Die gesamte Entwicklung ist an öffentliche Diskussionen gekoppelt, die jedoch keine Gleichschaltung der Romanvielfalt bewirken: Die Kritik ist dissonant, sie fördert Konkurrenz verschiedener Modelle. Sie teilt mehr noch den Markt in einen Bereich, dessen Reformbestrebungen Rezensenten ansprechen können und ein größeres Feld, das sich an den Kritikern vorbei auf Kundenschichten ausrichtet. Die Differenzierung zeigt sich ab Mitte des 18. Jahrhunderts deutlich im Aufbau eines eigenen Bereichs der Pornographie: Für ihn wird nicht öffentlich geworben, eine Subkultur muss Wissen über diese Titel verbreiten. So ergibt sich ein Romanangebot mit breiter unbesprochener trivialer Produktion und geheimen Nischen, in denen Grenzen des moralischen Konsenses aufgehoben werden.

Der neue Status, den der Roman als Gattung öffentlicher Diskussion im 18. Jahrhundert erringt, zeigt sich besonders in den philosophischen und experimentellen Werken.

Philosophische Fiktionen sind dabei keine Neuheit. Platons "Dialoge" kamen in philosophischen Erzählungen heraus. Die klassischen Utopien nutzten von Thomas Morus’ "Utopia" (1516) zu Tommaso Campanellas "La città del Sole" (1602) romanhafte Rahmenhandlungen. Sie blieben als Philosophie diskutiert, da Liebeshandlungen und Intrigen in ihnen keine weitere Rolle spielten. Mit den 1740ern ändert sich das. Morus’ "Utopia" lässt sich nun als „Roman“ herausgeben. Voltaire schreibt philosophische Romane: "Zadig" (1747) und "Candide" (1759) werden zentrale Texte der französischen Aufklärung und Meilensteine der Romangeschichte. Jean-Jacques Rousseau erweitert hier die Optionen mit dem deutlich didaktischen "Emile oder über die Erziehung" (1762) und der wesentlich romanhafteren "Julie oder Die neue Heloise" (1761).

Die genannten Titel belegen, dass philosophische Fragen nun eher im Roman als in fachinternen Abhandlungen gestellt wurden. Voltaire und Rousseau konnten mittlerweile darauf vertrauen, dass ihre Werke auch als Romane publiziert von der Fachdiskussion zur Kenntnis genommen würden. Die gesamte Literaturdiskussion wandte sich von den Wissenschaften aus Fiktionen zu.

Mit dem neuen Selbstverständnis des Romans gingen Experimente mit den Gattungsgrenzen einher. Laurence Sternes "The Life and Opinions of Tristram Shandy, Gentleman" bricht 1759–1767 spielerisch und bahnbrechend mit der fortlaufenden Erzählung als der Grundlage des Romans. Die Autor-Leser Kommunikation, die bislang Vorreden beherrschte, ersetzt die Handlung. Ihr Thema ist der nicht zustande kommende Lebensbericht, ein Scheitern der Erzählung. Visuelle Momente ersetzen Text: eine marmorierte Seite gestaltet die Kommunikation wie eine demonstrativ schwarzer Block; auf einer anderen Seite gibt der Erzähler verschiedene Linien als Verlaufsskizzen der wirren Handlung. Jonathan Swifts satirische Erzählung "A Tale of a Tub" (1704 veröffentlicht) ging hier mit ähnlicher Experimentierfreude voran, experimentierte dabei jedoch nicht mit dem Roman, sondern mit der Gattung des Traktats.

Bis Anfang des 19. Jahrhunderts hatte sich der Roman als primär westeuropäische Produktion entwickelt. Als unwissenschaftliche Gattung, die vor allem mit Lesegenuss und Neugier rezipiert sein will, florierte er in einem breiten Spektrum von Büchern für einfache Leser bis zu brisanten Titeln, die Indiskretionen europäischer Politik vermarkteten. Ohne weitere Lokalisierung in der Religion oder der Politik war er gleichzeitig bis in die 1780er am ehesten Gegenstand kommerzieller Verwertung mehr oder minder eleganter Privatlektüre.

In der zweiten Hälfte des 18. Jahrhunderts rückte der Roman schrittweise in den Brennpunkt öffentlicher Wahrnehmung: Literarische Rezensionen nahmen ihn in den Blick. In der ersten Hälfte des 19. Jahrhunderts festigt sich diese Tendenz: Es etablieren sich in Literaturgeschichten und literarischen Zeitschriften neue Rezensions- bzw. Besprechungsformen. „Literatur“ ist von nun an der Bereich der fiktionalen und poetischen Schriften. Die Wissenschaften, bislang Literatur im Sinne des Wortes, professionalisieren sich und heben sich von der allgemeinen Literaturbesprechung ab bzw. blendet sie aus. Mit Fiktionen und Poesie gewinnt die Literaturkritik in den Nationen des Westens einen eingeschränkten, säkularen, sehr frei handhabbaren Besprechungsgegenstand von großem öffentlichem Interesse.

Mit den 1830ern erfolgt der nächste Schritt: die Nationen Westeuropas etablieren die Literatur im neuen Wortsinn als Unterrichtsgegenstand. Die Nationalliteraturen werden in einem Wettstreit unter den Kulturnationen Europas in Literaturgeschichten kanonisiert und mit Entwicklungsgeschichten versehen. Insbesondere die Nationen Nord-, Ost- und Südeuropas geraten im selben Moment in einen Entwicklungsdruck: Ihre kulturellen Eliten konsumierten in den letzten Jahrhunderten den westeuropäischen Roman. Nun ist es nötig, eigene Werke der großen Nationalliteratur vorzulegen. Der Roman gewinnt in diesem Wettstreit große Bedeutung in einer Produktion epochaler, in die nationale Geschichte greifender Großromane, die nationale Identität definieren.

Bei der Verbreitung der Nationalliteratur spielte Deutschland in der Frühphase eine Vorreiterrolle. Zu Beginn des 18. Jahrhunderts war Deutschland territorial in Kleinstaaten zersplittert und durchgehend auf die Moden Frankreichs ausgerichtet gewesen, der Nation, mit der eine jahrhundertealte Erbfeindschaft bestand, während die politischen Romane ihrer regimekritischen Intellektuellen gelesen wurden. Mitte des 18. Jahrhunderts hatten deutsche Intellektuelle mit Blick auf den englischen Roman einen Entwicklungsrückstand Deutschlands festgehalten. Poesie und Prosafiktion zu Besprechungsgegenständen zu erheben, hatte in Deutschland national einigende Bedeutung – Religion und Politik boten keine vergleichbar überregional besprechbaren Gegenstände. Der Aufbau der deutschen Nationalliteratur schafft Anfang des 19. Jahrhunderts einen Bildungsgegenstand und eine nationale Diskussion, die sich exportieren lassen. Der englische Sprachraum hat zwar den entschieden tragfähigeren kommerziellen Roman aufgebaut, hinkt aber bei der Institutionalisierung nach, mit der im 19. Jahrhundert die Nationalliteratur Unterrichtsgegenstand in den Schulen und öffentlicher Debattengegenstand in den Medien wird. Die "Geschichte der englischen Literatur" des Franzosen Hippolyte Taine holt hier 1863 den institutionellen Entwicklungsschritt nach.

Der Schritt des Romans in den Schulunterricht ist begleitet von einer Neugliederung der Wissenschaften, die für die neuen Bildungsgegenstände zuständig werden. Theologie, Jurisprudenz, Medizin und Philosophie waren die vier Grundwissenschaften bis in das mittlere 18. Jahrhundert hinein gewesen. Mit dem 19. Jahrhundert setzt sich eine neue Teilung in Naturwissenschaften, technische Wissenschaften, Sozialwissenschaften und Geisteswissenschaften durch. Die Geisteswissenschaften werden in dieser Entwicklung die institutionelle Dachstruktur von Geschichte und Kultur, deren Experten auf die Kunst und Literaturbesprechung entscheidenden Einfluss haben.

Mit der neuen gesellschaftlichen Bedeutung verändern sich im 19. Jahrhundert die Modalitäten literarischen Lebens. Das Gesamtangebot wächst und fächert sich auf in einen Bereich diskutierter Werke hohen literarischen Anspruchs und den Massenmarkt der Belletristik, der sich in der zweiten Hälfte des 18. Jahrhunderts entwickelte und der eine weitere Differenzierung mit modischer Trivialliteratur gewinnt. Ein breiter Austausch in den Medien erfasst nun den Roman.

Die Stellung des Autors wird neu definiert. Bis in die Mitte des 18. Jahrhunderts war die anonyme Publikation die Regel. Ihr entsprach die weitgehende Abtrennung des Autors vom Geschäft, das sich mit seinem Buch machen ließ: der Autor lieferte sein Manuskript ab und erhielt ein Honorar nach Anzahl der Druckbögen im publizierten Buch. Im Ernstfall konnten Verleger behaupten, zu den Autoren brisanter Romane nie intensiveren Kontakt gehabt zu haben, sie erwarben Manuskripte, ohne deren Brisanz sogleich zu erkennen (so die Entschuldigung, die Verleger im 17. Jahrhundert wiederholt gegenüber der Zensur hervorbrachten). Mit dem 19. Jahrhundert ändert sich das Urheberrecht. Der Autor wird am weiteren Gewinn beteiligt, den sein Titel macht. Die neue Regelung gibt dem Autor Identität (die ein neues Presserecht mit neuen bürgerlichen Freiheiten schützen muss). Sie erlaubt zudem ein neues Kalkül mit der literarischen Karriere. Ein literarisch anspruchsvolles Buch kann erst einmal in einer kleinen Auflage gedruckt werden. Werden die Kritiker darauf aufmerksam, dann folgt der Durchbruch mit der kritischen Beachtung, die den Titel zum Bildungsgegenstand macht. Der Autor profitiert dann vom Ruhm, den er als anerkannter Dichter findet.

Dichterlesungen sind symptomatisch für die neuen Formen literarischen Lebens wie Kontroversen mit Literaturkritiker oder öffentliche Stellungnahmen großer Autoren zu allen Fragen gesellschaftsweiter Bedeutung. Große Romanautoren des 19. Jahrhunderts wie Charles Dickens und Émile Zola, Lew Tolstoi, Fjodor Dostojewski, sind Produkte des neuen literarischen Lebens mit seiner klaren Ausrichtung auf die Nation und ihre Öffentlichkeit. Die Nation gewinnt mit dem Romanautor im Idealfall eine unabhängige Stimme, ein Gewissen, das nicht in die Politik oder in die Religion eingebunden ist, im Ernstfall aber allein der Kunst gegenüber verantwortlich handeln soll, so die Theorie.

Der neuen Verantwortung, die der Autor großer Literatur für die Nation hat, trägt der literarische Streit in seiner zentralen Thematik Rechnung: Das Dauerthema der Literaturdebatte des 19. Jahrhunderts wird die Frage, wie weit der Autor sich auf die Schilderung der Wirklichkeit einlassen kann (ohne die Kunst mit dem Niederen der Wirklichkeit zu beschmutzen), wie weit er sich im selben Moment in öffentlichen Diskussionen instrumentalisieren lassen darf (oder ob er als Künstler nicht außen stehen muss). Die Gegenposition gegenüber dem Realismus erhebt die Forderung nach l’art pour l’art, Kunst um der Kunst willen. Sie ist nicht minder der weiteren Frage nach der gesellschaftlichen Verantwortung des Künstlers ausgesetzt. Der Künstler entzieht sich hier Ansprüchen der Gesellschaft mit dem Hinweis auf seine alleinige Verantwortung gegenüber der Kunst. Die Frage der moralischen Integrität des Romanautors gewinnt im 19. Jahrhundert Gewicht. Dazu passt, dass Romanautoren sich im Extremfall öffentlichen Verfahren ausgesetzt sehen. Die Karrieren von Émile Zola und Oscar Wilde liefern den Biographien von Autoren wie Alexander Solschenizyn und Salman Rushdie Vorbilder.

Geraten große Autoren im 19. Jahrhundert in das Zentrum eines neuen nationalen literarischen und kulturellen Lebens, so wird das gesamte Buchangebot zunehmend von einem neuen und breiten Massenmarkt fiktionaler Literatur getragen, der die Genres des 18. Jahrhunderts weiterentwickelt. Eine Trennung hoher, auf Diskussionen Anspruch erhebender, und niederer Konsumware bildet sich im 19. Jahrhundert aus. Sie ist das Fundament der gesamten Entwicklung, da sie ein Marktgeschehen ohne Verlierer garantiert: Der Markt wächst insgesamt. Allenfalls die relative Bedeutung der Bereiche verschiebt sich. Unter den Debattengegenständen gewinnt Literatur gegenüber der Theologie gesellschaftlichen Rang.

Das Wort Romantik bindet mit der Wende ins 19. Jahrhundert eine ganze Generation von Künstlern an den Roman als interessanteste Kunstgattung. „Written in a romantick vein“, bedeutete im 17. Jahrhundert, dass der Autor sich eines romanhaften Stils bediente. Das deutsche Äquivalent war im 17. Jahrhundert die „romanische“ Schreibweise (das "t" in „romantisch“ setzt sich mit der anglophonen Epochendefinition durch). Die Romantiker riskieren alle Tugenden und Untugenden der alten Gattung, um erschreckende Welten aufzubauen (statt die reale "realistisch" und "natürlich" abzubilden) und Fiktionen phantastisch wuchern zu lassen (statt der Kunst realistischen Novellistik zu folgen). Die Novelle wird dabei neu entdeckt als Gattung, die sich in ihrer Geschlossenheit der Intrigenhandlung zerstören lässt, und die in ihrer Kürze neue Offenheit erlangen kann, um über sich hinaus auf eine jenseitige Realität zu verweisen.

Anders als die Heldenromane des 17. Jahrhunderts, die sich vom "Amadis" als Abgrund der Verworrenheit abzusetzen suchten, zelebrieren die Romantiker das gefahrvoll Verworrene des Romans. Gegenüber der Romankritik der Aufklärung, die klare didaktische Intentionen lobte, wird offene Subversion geübt. Sie gehen gleichzeitig auf die neue Marktdifferenzierung ein, die den Roman soeben erfasst: Mitten im Prozess, in dem die aktuelle Literaturkritik die Belletristik weitgehend trivialisiert zugunsten eines kleinen Bereichs klassisch apostrophierter Literatur, die von nun an Gegenstand der Literaturdiskussion sein soll, spielt die junge Generation von Autoren mit den Stoffen des niederen Markts, auf dem sich Schauer und Emotion verkaufen. Das Groteske, schonungslos Spannende und atemberaubend Erfundene ist ein willkommenes Reservoir des Fiktionalen, das offenkundig als Trivial unterdrückt werden soll. Der Romantik gelingt im selben Moment der erste Angriff auf die Literaturkritik, die die Kunst durchaus auf Fiktionalität verpflichtet haben wollte.

Die Frage danach, was Kunst eigentlich sein soll, bestimmt die Romane und Novellen der Romantiker offen. Künstlerromane und -Novellen widmeten sich kunsttheoretischen Diskussionen. Allegorische Geschichten erschienen, in denen es wie in E. T. A. Hoffmanns "Der Sandmann" (1817) explizit um Kunstkenntnis und romantische Verklärung der trivialeren und bedrohlicheren Wirklichkeit geht. Das Reale und das Künstliche sind hier ein so interessantes Objekt wie in Mary Shelleys "Frankenstein" (1818), dem noch entschieden deutlicher die Grenzen zum Trivialen überschreitenden Schauerroman über die Erschaffung und die Seele eines neuen künstlichen Menschen.

Die in der Kunst offen diskutierte Frage danach, was Kunst eigentlich sein soll, hat ihren interessantesten Adressaten in der Literaturwissenschaft, die die Kunst insgesamt auf Fiktionalität und damit auf Interpretierbarkeit verpflichtet. Die Frage der Interpretation ist im selben Moment eine in den Romanen der Romantiker offen diskutierte Frage. Fiktionen mit oberflächlich trivialen Realitäten, unter denen tiefere Wahrheiten mit Kunstverständnis zu bergen sind, greifen um sich. Allegorie und der Abgrund der Bedeutungstiefe sind so beliebt wie der Trug des Oberflächlichen, das nur vorgibt, tiefere Bedeutung zu haben und allein im kunstbeflissenen, verklärenden Romantiker solche Bedeutung gewinnt.

Das Fiktionale selbst wird von den Romantikern als der eigentliche Bereich der Kunst akzeptiert. Die neue Frage ist, woher das Fiktionale seine tieferen Wahrheiten bezieht. Dass es Ausdruck sexueller Antriebe, tieferer Ängste und Sehnsüchte ist, gehört ins Antwortenspektrum der Romantiker wie das Programm, die Kunst und die Fiktionalität von allen Regelzwängen zu befreien. Die ungenierte und unbezwingbare Fiktionalität, die sich im Albtraum und in der Vision Bahn bricht, sind beliebte Erzählstoffe, wie die Realität von Wahnvorstellungen, die sich im Verbrechen herstellt. Das Fragment wird zur Kunstform, die von sich aus Gattungsregeln bricht und über sich hinaus auf das Unvollendete verweist.

Das Interesse an psychologischen Abgründen, wie sie im Traum, in der Phantasie, in Wahnvorstellung oder in künstlerischen Fiktionen zu Trage treten, schafft am Ende ein Stoffspektrum, das in der Trivialliteratur wie in der modernen Kunst fortwirkt und das gleichzeitig die gesamte wissenschaftliche Interpretationspraxis beeinflusst. Der Horrorfilm, Fantasy, die Rollenspielszene sind heute durchdrungen von romantischen Entdeckungen düsterer Materialbereiche, insbesondere des „finsteren“ Mittelalters, der Nacht, des Gewaltexzesses, der Isolation des Individuums und der sich dem öffentlichen Zugriffs entziehenden Machtausübung in Geheimengesellschaften. Autoren wie De Sade, Poe, Shelley oder E. T. A. Hoffmann schufen im selben Moment die Vorstellungswelten, die in der Psychoanalyse wie im Surrealismus wieder auftauchten. Freuds Psychoanalyse bezieht zu Beginn des 20. Jahrhunderts einen großen Teil ihrer Plausibilität aus Interpretationen der Literatur, die hier zu Beginn des 19. Jahrhunderts zustande kommt.

In die Geschichte hinabtauchende Romane gibt es seit der Antike. Die Geschichte der Gegenwart ist seit dem 17. Jahrhundert ein ausgiebig genutztes Romanthema – satirische Romane wie Grimmelshausens "Simplicissimus" (1666/1668) beuteten es aus wie historische Enthüllungsromane von der Art des anonymen "La guerre d’Espagne, de Baviere, et de Flandre, ou Memoires du Marquis d***" (Cologne: Pierre Marteau, 1707), das im frühen 18. Jahrhundert einen französischen Agenten in James Bond-Manier Hintergründe des Spanischen Erbfolgekriegs aufdecken ließ.

Die gesellschaftliche Stellung Historischer Romane des 19. Jahrhunderts markiert einen Umbruch: Der satirische Roman des 17. Jahrhunderts blieb unbesprochenes Abenteuer. Den Skandalroman des frühen 18. Jahrhunderts beuteten Journalisten als indiskrete und ungedeckte Informationsquelle aus. Der historische Roman, der mit dem frühen 19. Jahrhundert mit Werken wie Walter Scotts "Waverley" (1814) aufkommt, versucht den Titel in den Bildungskanon aufzunehmen und die Fiktion ins kollektive Geschichtsbewusstsein zu übernehmen, um auf diese Weise nationale Identität zu erzeugen.

Die Stoffe der meisten historischen Romane sind unterhaltend, eskapistisch, leicht bildend und erbaulich. Brisant wurde die Popularisierung der Geschichte jedoch nicht durch den Schulunterricht und die Universitäten, die in den säkularen westlichen Nationen historische Diskussionen veranlassen und damit Identität stiften, sondern aufgrund individueller Lektüreerfahrung. Der historische Roman entwickelt bei seinen Ausgriffen auf die Vergangenheit wie die Gegenwart ein eigenes Potential durch seine Konzentration auf individuelles Erleben und Einzelschicksale. Er stattet die Leben von Romanhelden mit tieferer historischer Bedeutung aus. Historisches Leid und historische Triumphe werden im Roman nacherlebbar.

Romane schaffen eine andere Geschichtswahrnehmung als die Berichterstattung in den öffentlichen Medien oder Geschichtsbücher. Klassenkonflikte, Rassismus, wirtschaftliche Ausbeutung oder Kriege bleiben in Sachtexten weitgehend entindividualisiert; hier ist allenfalls interessant, wer Verantwortung trug. Anders im Roman: Charles Dickens machte die Kinderarbeit in den Arbeitshäusern des 19. Jahrhunderts als individuelles Leid erlebbar, Émile Zola schilderte die Proletarisierung der Großstädte. Mit Lew Tolstois "Krieg und Frieden" (1868/69) wurden die militärischen Auseinandersetzungen Russlands zu persönlichen Schicksalen von Männern und Frauen. Harriet Beecher-Stowes "Uncle Tom’s Cabin" (1851–1852) wurde der meistverkaufte Roman des 19. Jahrhunderts und trug maßgeblich dazu bei, dass die Sklaverei in den Vereinigten Staaten öffentlich debattiert wurde. Die Romane George Eliots sorgten dafür, dass die Bildung der Frauen und ihre Rolle in der Gesellschaft diskutiert wurden. Der Roman wurde somit vom curieusen Gegenstand privater Lektüre zum Gegenstand der allgemeinen Debatte, an der auch die Literaturkritik und die Medien teilnahmen.

Der Aufstieg des Romans zum Gegenstand nationaler Auseinandersetzungen schlug sich im 19. Jahrhundert in der Institutionalisierung der Literaturgeschichtsschreibung nieder, die nun der verankerten französischen, englischen und deutschen Nationalliteratur folgte. Romane mit nationalgeschichtlicher Thematik initiierten hier Entwicklungen in den skandinavischen und ost- und südeuropäischen Sprachen.

Zukunft wurde in einem Seitenzweig der Entwicklung ein neues Thema. Die Entwicklung läuft hier von Samuel Maddens satirische "Memoirs of the Twentieth Century" (1733) über Louis-Sébastien Merciers Fortschrittsutopie "L’An 2440" (1771) und Mary Shelleys autobiographischen wie politischen Roman "The Last Man" (1826) zu den dezidierten Auseinandersetzungen mit aktuellen Entwicklungsprozessen, wie sie Edward Bellamy mit "Looking Backward" (1887) und H. G. Wells mit "The Time Machine" (1895) am Ende des 19. Jahrhunderts vorlegten. Die Auseinandersetzungen mit Zukunft gewann mit diesen Büchern neue Qualität: Entwicklungen, Veränderungen, Evolution auf sozialem und kulturellem Gebiet wurde mit ihnen öffentlicher Diskussionsgegenstand und ein Raum populärer Phantasie, der eine eigene kommerzielle Produktion an Science Fiction inspirierte.

Individualismus hatte den Roman des Mittelalters wie den der frühen Neuzeit bestimmt: In der Regel standen einzelne Helden im Vordergrund. Ob sie als Heldinnen der Antike in Männerkleidern in heidnische Sklaverei gerieten, wie Robinson Crusoe allein auf einer Südseeinsel überleben mussten oder wie Constantin de Renneville am Rande der fiktionalen Durchdringung ihrer Existenz sich der Lebensumgebung der Bastille anpassten: Sie alle erlebten, anders als die Romanhelden des 19. und 20. Jahrhunderts, ihr eigenes Erleben nicht als vollkommen persönlich und unvermittelbar. Das hat sehr verschiedene Gründe.

Mustergültig war bis Mitte des 18. Jahrhunderts das Individuum, das sich in bewusster Conduite über sein öffentliches Renommee definierte und behauptete. Entwicklungen spielten für dieses Individuum keine Rolle, wohl aber Situationsveränderungen, die es klug zu nutzen galt. Der Held, die Heldin, definierten sich über persönliche Stärke gegenüber anderen. Hiervon wichen die satirischen Helden ab, die persönliche Schwächen entwickelten, aus denen der Leser zu lernen hatte und von denen er sich mit Belustigung distanzierte.

Solange der Roman in der Geschichtsschreibung angesiedelt blieb, bestand zudem kaum ein Grund, über persönliche Perspektiven und einen schmerzlichen Bruch zwischen diesen und dem kollektiven Erleben nachzudenken. Die Geschichte war Gegenstand der Vermittlung, für die der Autor mit Schreibkunst zuständig war. Brüche zwischen der eigenen Sicht und der der Umwelt bestimmten vor 1750 nicht den Roman, sondern die religiöse Literatur: die Frauenmystik des Mittelalters und die protestantische spirituelle Autobiographie der frühen Neuzeit.

Im Moment, als der Roman aus der Geschichtsschreibung ausgegliedert und in das Feld der literarischen Kunst eingegliedert wurde, gewann er einen Entwicklungsspielraum mit der Stilisierung, die nun der Romanautor als der Öffentlichkeit gegenübertretendes Individuum fand. Seine Lebensrealität war zunehmend die der Kunst, die sich vom Leben entfernte, wenn sie es in dieser Entfernung nicht neu und intensiver fand – hier wurde unverzüglich ein Spannungsfeld aufgemacht, das zuvor, abseitiger definiert, für den Poeten bereits seit der Antike bestand. Romane wie Laurence Sternes "Sentimental Journey through France and Italy" (1768) kosteten das Spiel mit belustigendem, individuellem Leben noch in Anlehnung an satirische Romane des 17. und frühen 18. Jahrhunderts aus. Eine eigene Auseinandersetzung des Künstlers mit seinem Leben und Erleben kam mit den umliegenden "empfindsamen" Romanen auf. In der Romantik radikalisierten sich diese Erkundungen des einsamen Erlebens im spektakulären Auskosten der Bedrohungsszenarien von Wahn und Sehnsucht.

Als sich in den ersten Jahrzehnten des 19. Jahrhunderts der Roman der Geschichte und der Gegenwart den aktuellen gesellschaftlichen Anliegen neu stellte, kam ein weiteres Spannungsfeld zwischen Individuum und Gesellschaft hinzu: Das Individuum als Beobachter, der Unabhängigkeit von Vereinnahmungen durch die kollektive Realität sucht, wurde ein neues Thema. Der Weg des Künstlers in eine solche Position blieb ein großes Sujet in diesem Feld mit Bildungsromanen von Goethes "Wilhelm Meister" (1777–1795) zu Gottfried Kellers "Grünem Heinrich" (1854, in zweiter Fassung 1879/80). Erkundungen von anderen Beobachtungspositionen kamen mit mehr oder minder fiktionalen Lebensskizzen hinzu, insbesondere von Frauen und in der öffentlichen Berichterstattung vergleichbar randständigen Gruppen, deren Sichtweisen nun gerade als unentdeckt isolierte interessierten.

Der Roman bot sich hier als experimentelle Gattung an, da er zur langen Erzählung ausholte, ohne dass klar war, was eine Erzählung formal definierte, und (anders als das öffentlich inszenierte Drama) seinen Leser selbst im intimen, individuellen Erleben erreichte.

Die Erkundung persönlichen Erlebens revolutionierte hier Schreibweisen des Romans. Die Suche nach einem individuellen, subjektiven Stil stand hier im Vordergrund des Wettbewerbs zwischen den Autoren. Das Experiment mit gänzlich neuen Erzählmustern, die sich wie der stream of consciousness dem Erleben effektiv anglichen, führte bei Autorinnen und Autoren wie Virginia Woolf und James Joyce in der ersten Hälfte des 20. Jahrhunderts zu den klarer definierten Experimenten im Verlauf einer kritischen Distanzierung vom Roman des 19. Jahrhunderts und seinen noch deutlich auktorial organisierten Erzählmustern.

Kulturpessimisten sahen in den letzten 100 Jahren mehrfach das Ende der Literatur gekommen. Das Fernsehen, das Kino, das Internet, Videospiele verdrängten das gute Buch. In einer größeren Sicht wird man das Gegenteil feststellen. Der Roman floriert auf dem Buchmarkt wie in der öffentlichen Wahrnehmung.

Romane waren unter den ersten Büchern, die Nationalsozialisten in den Bücherverbrennungen 1933 demonstrativ vernichteten. Man verbrannte hier in Schauveranstaltungen einzelne Exemplare. Die Romane, die in privaten Regalen standen, ließen sich allenfalls bei Wohnungsdurchsuchungen erfassen. Der Roman war dabei nicht als Gattung verfolgt: Romane wurden mit den letzten Papierkontingenten gedruckt, die die Nationalsozialisten 1944/45 noch zum Druck freigaben. Während das Reich im Bombardement unterging, sollten die Soldaten an den Fronten, für die allein noch Lesestoff hergestellt wurde, fortfahren, in Heimatromanen von der Liebe zu träumen. Romane wurde von den US-Soldaten in Vietnam gelesen und von der Bewegung gegen den Vietnamkrieg. Hermann Hesse und Carlos Castaneda gehörten hier zum Reisegepäck. Während es schwierig war, innerhalb der Sowjetunion mehr über die sibirischen Konzentrationslager zu erfahren, blieb es einem Roman vorbehalten, die interne wie die Weltöffentlichkeit herzustellen – Alexander Solschenizyns "Ein Tag im Leben des Iwan Denissowitsch" (1962), ihm folgte der narrativ historischer angelegte "Archipel Gulag" (1973), der dem Grauen am Ende den Namen gab. Die Kundenschicht, der die Medien gerne die Abkehr vom Buch attestierten, sorgte in den letzten Jahren mit dem Absatz der "Harry Potter" Bände für die größten Bestseller des Jahrhunderts. Die gewichtigste politische Konfrontation der letzten dreißig Jahre, diejenige zwischen dem „freien Westen“ und der „Islamischen Welt“ fand ihre erste Runde mit der weltweiten Verfolgung eines Romans – Salman Rushdies "Satanischen Versen" (1988). Tatsächlich bot sich der Roman hier wie gar kein anderes Medium der Kulturkonfrontation an.

Das Erfolgsrezept des Romans im 20. Jahrhundert liegt zum Teil in seinem antiquierten Trägermedium. Romane zirkulieren in Einzelausgaben als bequemer Lektüregegenstand, mit dem man sich jederzeit in der Öffentlichkeit einen ungestörten Privatraum verschafft. Der Druck kann im politisch brisanten Fall im sicheren Ausland oder in einheimischen Samisdat-Pressen erfolgen. Exemplare können unter der Hand weitergegeben werden. Man erwirbt die "Satanischen Verse" auf Persisch in obskuren Drucken unter schwarzen einfachen Buchdeckeln, ohne Verlagsangabe und mit Angabe einer offenkundig pseudonymen Übersetzerin. Eine grundsätzlich andere Öffentlichkeit haben Theateraufführungen, Fernsehsendungen, Internetangebote. Der Reiz der Theateraufführung liegt gerade darin, dass man Teil einer von Publikum simultan bezeugten Vorführung wird. Fernsehen und Internet werden ähnlich wie das Buch überwiegend privat konsumiert, bleiben jedoch über Sender und Provider simultane Veranstaltungen von Öffentlichkeit, auf die staatliche Organe zugreifen können. Was Romane anbetrifft, so können Zensurbehörden deren Exemplare, einmal gedruckt, nur noch im privaten Besitz verfolgen. Der Roman selbst bleibt dabei öffentlich. Das Buch ist eine industriell verfertigte Massenware, im Medium der offenbare Beleg dafür, dass es so andere Leser wie einen selbst erreichte. Es bleibt dem Romanleser dabei auf spannende Weise unklar, wer las, was er soeben liest, und sich im brisanten Fall als Leser zu erkennen geben wird.

Brisanz gewinnt der Roman als privates Medium im 20. Jahrhundert vor allem durch die ihm angebotene öffentliche institutionelle Deckung. Sie erfasst die private Lektüre, droht sie zu prägen, kann mit ihr in neue Formen des Streits geraten. Das „literarische Leben“, das die „freien Gesellschaften des Westens“ heute verteidigen mitsamt seinen modernen Institutionen von den städtischen Literaturhäusern, den öffentlich diskutierten Literaturpreisen, den in Buchhandlungen veranstalteten Literaturlesungen, den Buchmessen mit ihrem Presseaufgebot, geht kaum vor das mittlere 19. Jahrhundert zurück. Mit der öffentlichen Würdigung geht seitdem eine Kanondebatte einher, die ihren Niederschlag in den Medien, im Zeitungsfeuilleton, wie im Fernsehen und im Internet findet, und die den Bildungsbetrieb in seiner ganzen Hierarchisierung von den universitären Seminaren bis hinab in den täglichen Schulunterricht berührt, der zur Würdigung von primär nationaler Literatur anleitet. Autoren und Verlage können sich bei der Vermarktung anspruchsvoller Romane der Öffentlichkeit nicht verweigern. Die große Masse der Literaturpreise unterhalb des Nobelpreises für Literatur, die Preise, die mit dem Booker Prize und dem Pulitzer Prize beginnen, schafft Vermarktungsplattformen. In den Medien besprochen zu werden, ist für anspruchsvolle Titel, für Titel, die Anspruch auf öffentliche Würdigung erheben, die zentrale Verkaufsvoraussetzung. Die Literaturwissenschaft und die Literaturkritik nehmen im modernen literarischen Leben nur scheinbar den Rang von Beobachtern ein. Tatsächlich fungieren sie im Zentrum des Austauschs: Sie verteilen öffentliche Beachtung und trennen mit ihrer Aufmerksamkeit die gehobene Literatur von den primär kommerziell vertriebenen Büchern.

Was sich das 20. Jahrhundert hier vor allem vom 19. unterscheidet, ist die Rolle der Weltöffentlichkeit, die sich nationalen Öffentlichkeiten gegenüber artikuliert. Im 20. Jahrhundert entwickelt sich eine zunehmende Globalisierung des Romans, welche die Globalisierung der Konflikte (Weltkriege) widerspiegelt. Waren es im 19. Jahrhundert die Staaten Osteuropas, die sich mit der Übernahme des westlichen Literaturbegriffs als Kulturnationen zu bezeichnen begannen und erste Romane in ihren Sprachen vorlegten, so wurde im 20. Jahrhundert die globale Ausbreitung fortgesetzt. Um 1900 wurde das Spektrum der aktuellen Romanproduktion durch südamerikanische und indische Autoren erweitert, worauf arabischsprachige Schriftsteller und ab der zweiten Hälfte des 20. Jahrhunderts schwarzafrikanische Romanschriftsteller folgten. Postkolonialismus-Studien gelten gegenwärtig der Dezentrierung des Literaturbegriffes, die hier geographisch markant wird. Die Liste der Nobelpreisträger für Literatur dokumentiert, wie sich die Standards westlichen literarischen Lebens im 20. Jahrhundert verbreiteten: Rabindranath Tagore erhielt die Auszeichnung als erster Inder 1913. Der erste Japaner, der den Preis zugesprochen erhielt, war Yasunari Kawabata 1968, der erste Südamerikaner Gabriel García Márquez 1982; der erste Nigerianer Wole Soyinka 1986, der erste Autor arabischer Sprache, Naguib Mahfouz 1988. Der Preisvergabe haftet in aller Regel ein politisches Moment an: der Westen unterstützt verfolgte Autoren und Autoren, die sich in ihrer Heimat zum „Gewissen ihrer Nation“ machen. Orhan Pamuk erhielt den Preis 2006 nicht zuletzt, da er als Autor der Türkei kritisch den Umgang mit dem Völkermord an den Armeniern ansprach. Mahfouz kritisierte zwar Rushdies "Satanische Verse" (1988) als „Beleidigung für den Islam“, verteidigte jedoch ihren Autor gegen Chomeinis Todesurteil und bezahlte sein Engagement beinahe mit dem Leben.

Die weltweite Ausbreitung des literarischen Lebens als Teil des internationalen pluralistischen Austauschs geriet im 20. Jahrhundert zur Konfrontation. Das hat stark damit zu tun, dass Roman und die Literatur im Westen im Prozess der Säkularisierung die Bildungssysteme eroberten: Als Teil nationaler Auseinandersetzungen mit Texten, in der das Bibelstudium durch Lektüre der größten Kunstwerke abgelöst wird, die die eigene Nation schuf. Der Roman – vordem allein in der Belletristik beheimatet, eine Produktion aus der Hand primär privater Autoren – bot sich dem staatlichen Unterricht zu maximaler Freiheit der Behandlung an: Man kann Romane beliebig interpretieren, sie textkritisch analysieren, sie kanonisieren. Traditionell in der Theologie beheimatete Formen eines Umgangs mit Texten erfassten damit eine bislang eher säkulare der Moral entzogene Textgattung. In den Buchläden, in Universitätsseminaren, im Schulunterricht des Westens nimmt der Roman effektiv seit der Säkularisation einen Platz ein, den bis dahin religiöse Texten reklamierten. Der Konflikt, der mit der Veröffentlichung von Salman Rushdies "Satanischen Versen" (1988) entbrannte, verschärfte sich in der Folge sofort als Konflikt globaler Dimension zwischen dem säkularen Westen und dem postsäkularen Islam. Aus westlicher Perspektive ging es darum, Rushdie die Freiheit des Künstlers zuzugestehen, der im Roman, einem Medium subjektiven künstlerischen Ausdrucks, eine subjektive Weltsicht bieten muss.

Die Islamische Republik befand sich im selben Moment in einer misslichen Lage: Wenn sie die Religion als Garant unverletzlicher Wahrheit verteidigt, kann sie nicht gleichzeitig der Kunst das Recht einräumen, alle anderen Diskurse nach belieben relativieren zu dürfen. Das geht im Westen, wo Literaturkritiker jede Provokation auf dem Feld der Kunst entschärfen können, indem sie sie bei Bedarf zu einer reinen Frage des Kunstgeschmacks erklären. Die Kunst ist dabei letztlich nur frei, da sie gleichzeitig beliebig skandalisierbar und beliebig entskandalisiert ist: beliebig bedeutungsvoll, da sie als freie Meinungsäußerung verteidigt wird und beliebig bedeutungslos, da sie als jederzeit als pure subjektive Äußerung interpretiert wird. Die Nationalstaaten des Westens schützen individuelle Freiheiten auf Kosten eines Systems das Jahrhunderte lang die Religion schütze. Der Iran konnte in derselben Konfrontation aufzeigen, dass die westlichen Gesellschaften gegenüber der freien künstlerischen Äußerung eigene neue unfreie Bereiche einrichteten. Mit Holocaustleugnungen, die Zeichen der Freiheit sein sollten, die der Iran Wissenschaftlern gewährt, gelang hier der zweischneidige publizistische Gegenschlag.

Hinter der Ausweitung einer Gesellschaftsform, die mit dem Roman eine eigene Öffentlichkeit der privaten und subjektiven Meinungsäußerung verteidigt, steht ein Buch- und Informationsmarkt mit Medienanbindung, der vor dem 19. Jahrhundert kein Pendant hat.

Gut 20 bis 60 Romane erschienen auf Englisch (die deutschen Zahlen differierten hier nicht) im frühen 18. Jahrhundert, bei einer Gesamtzahl von jährlichen 2.000 Titeln aller Textsorten. 2001 kamen in Großbritannien 119.001 Titel auf den Buchmarkt. Der Roman war mit 11 % an diesem Angebot beteiligt. Der Prozentanteil hält sich seit Jahrzehnten stabil, auch wenn sich der gesamte Titelausstoß seit 1986 verdoppelt hat. 5.992 Romane erschienen in Großbritannien 1986, 13.076 waren es 2001. Interessanter als die Titelzahlen ist das quantitative Volumen, die Menge der Bücher, die hier die Druckerpressen verlassen. Wenn wir rechnen, dass Verleger im frühen 18. Jahrhundert bei gutgängiger Ware mit Auflagen von um die 1.000 Exemplaren arbeiteten und lieber mehrfach neu auflegten, als zu viel zu drucken, dann müssten 20.000–60.000 Romane vor 300 Jahren an die englisch- (wie an die deutschsprachige) Kundschaft gegangen sein. Die Auflagenzahlen stiegen gerade in der Belletristik seit dem 18. Jahrhundert. Nach den Nielsen BookScan-Statistiken von 2009 brachten britische Verleger 2008 geschätzte 236,8 Millionen Bücher in den Druck. Romane für Erwachsene machten daran mit geschätzten 75,3 Million Büchern 32 Prozent aus. Der Jugend- und Schulbuchbereich, der Bestseller wie "Harry Potter" umfasst, kam mit 63,4 Millionen Exemplaren, weiteren 27 Prozent, hinzu. Der Gesamtwert der britischen Buchproduktion belief sich 2008 auf um die 1,773 Milliarden Pfund. Die Produktion an Romanen für Erwachsene hatte daran einen Anteil von 454 Millionen Pfund.

Die weltweiten Zahlen differieren mit Größe der nationalen Märkte, die Zahlenverhältnisse dürften in Westeuropa ähnlich liegen.

Das Buchmarketing, das sich mit dieser Produktion herausbildete, optimierte die Kommunikation mit allen Instanzen des modernen literarischen Lebens. Es ist Teil des Geschäftes, dass die Verlage die „hohe“ literarische Produktion mitsamt ersten Diskussionsangeboten auf die Tische der Literaturkritiker kommen lassen. Die Diskussionen entstehen nicht, sie werden vorbereitet, sind Teil des Geschäfts der pluralistischen Gesellschaften.

Die Romanproduktion des 20. Jahrhunderts kann unter dieser Perspektive grob in drei Bereiche eingeteilt werden, die unterschiedlich mit den Diskussionsfeldern umgehen.

Entscheidende Entwicklungen der Techniken des Romans kann man aus der inspirierenden Auseinandersetzung mit konkurrierenden, modernen Medien erklären: Film, Zeitung und Comic entwickelten Einfluss auf den Roman. Schnitt und Montage wurden etablierte Techniken moderner Erzählformen.

Die in der Form experimentellen Romane des 20. Jahrhunderts nahmen im selben Moment gemeinsame Schritte mit der Literaturtheorie, die im 20. Jahrhundert als interdisziplinäre Methodendiskussion die Literaturwissenschaft erfasste. Über die Methodik hatte in der Literaturkritik des 19. Jahrhunderts wenig Streit bestanden: Literatur wurde von Autoren unter dem Einfluss ihrer Epochen geschrieben. Die gesellschaftlichen Bedingungen spiegelten sich in den Werken wider. Man stritt nicht über die Literaturtheorie, sondern darüber, welchem Titel welche Bedeutung im nationalen Kanon zugesprochen wurde.

Die Literaturtheorie des 20. Jahrhunderts vollzog an dieser Stelle eine Bewegung, die mit dem linguistic turn in der philosophischen Erkenntnistheorie einherging: Die Bedeutung eines Textes liegt, so der theoretische Ausgangspunkt, in seiner linguistisch erfassbaren Struktur. Große Kunstwerke bleiben in der Diskussion, da sie komplexere, wenn nicht unerschöpflich komplexe Strukturen aufweisen. Der Leser entschlüsselt und kontextualisiert Texte. Informationen über Autor, Epoche und Werk sind unter dieser Perspektive nicht wirklich Erklärungen des Textes, sondern von der Literaturwissenschaft produzierte Zusatztexte, und die Literaturwissenschaft des 19. Jahrhunderts hatte diese Zusatztexte produziert, ohne sich auch nur um die literarischen Kunstwerke mit Mitteln der Textanalyse zu kümmern. Die Theoriediskussion, die unter den Schlagworten Formalismus (1900–1920), New Criticism (1920–1965), Strukturalismus (1950–1980) und Poststrukturalismus (ab 1980) voranschritt, suchte nach den Aspekten der Kunst, analysierte strukturelle Komplexität und fragte nach den Assoziationen, dank derer sprachliche Bedeutung im Bewusstsein des Lesers entstand. Sie entmachtete den Autor. Für ihn war der Text so sehr Text wie für den Leser, und sie machte sich selbst zur eigentlich angesprochenen Instanz, über die der Text Bedeutung gewann.

Romanautoren reagierten auf dieses Interaktionsangebot mit Texten, die genau das boten: Sprachkompositionen, die sich für den Autor so faszinierend lasen wie für den Leser. James Joyces "Ulysses" (1922 zensierte Erstveröffentlichung in Paris) kennt keinen übergeordneten Erzähler mehr. Der Erzähltext gerät zu einem Bewusstseinsstrom an Gedanken und Empfindungen. Es ist müßig, zu fragen, was der Autor hier sagen will, wenn der Text, den er produziert, ein Gewebe an textlichem Material der gesamten Literaturgeschichte wird. Der Autor erzeugt hier eher einen Assoziationsraum, als dass er Aussagen zu Politik und Gesellschaft macht.

Alfred Döblin öffnete seine Romane "Berlin Alexanderplatz" (1929) und "Babylonische Wanderung" (1934) vergleichbar nicht-literarischem Material, Sätzen aus der Werbung und den Zeitungen. Realität durchdrang hier unvermittelt den fiktionalen Roman.

Autoren der 1960er radikalisierten Konzepte der Erzählung, indem sie diese fragmentierten und Zeit und narrative Sequentialität als Ordnungsformen aufgaben.

Die Postmoderne knüpfte spielerisch an diese Entwicklungen an mit Romanen, die Brücken in die Trivialliteratur schlugen. Ihr wurde genau dies immer vorgeworfen: Nicht originell zu sein, lediglich bestehendes Material neu zusammenzusetzen, und darum minderwertig zu sein. Poststrukturalisten wie Roland Barthes fragten, ob Kreativität überhaupt etwas anderes sein konnte als fortwährende Neukombination vorhandenen sprachlichen Materials. Autoren wie Thomas Pynchon griffen im selben Moment in die Textwelten, aus denen Trivialliteratur und populäre Verschwörungstheorien gemacht wurden, und machten aus ihnen neue Kunst. Das Verhältnis zwischen dem Roman und Literaturtheorie war spannend, da kaum zu erklären ist, was Fiktionalität ist; ein Satz aus einer Zeitung ist genau das, warum wird er Fiktion, wenn er im Roman auftaucht? Was macht eigentlich eine Erzählung aus, wenn man den Erzähler, oder die Chronologie streichen kann? Romanautoren bezogen Fragen aus der Theoriedebatte; sie inspirierten die Literaturtheorie auf der anderen Seite mit Werken, die Fragen dazu aufwarfen, was ein Text eigentlich ist; sie griffen zudem mit eigenen Texten in die Theoriediskussion ein. Raymond Federman formulierte die Theorie dahinter in einer Zusammenführung von fiktionaler Literatur und Literaturkritik unter dem Wort „Critifiction“.

Die Diskussion, die in der zweiten Hälfte des 20. Jahrhunderts zwischen Autoren experimenteller Romane und Literaturtheoretikern geführt wurde, fand sich wiederholt als reines Gedankenspiel kritisiert. Die Experimente würden allenfalls Intellektuelle befriedigen. Filmkunst der letzten Jahrzehnte eignete sich die Diskussion dann jedoch vergleichsweise massenwirksam an. Apocalypse Now (1979) macht unvermittelt einen Roman des 19. Jahrhunderts zur Schablone aktueller Zeitkritik und reflektiert dabei die Sicht des Films auf die Realität als filmisch erzeugte, statt als Sicht, die von der Welt ausgeht. Pulp Fiction (1994) inszeniert sich als filmisches Stückwerk aus Groschenheften, Memento (2000) inszeniert eine Erzählung, die in Einzelszenen zerrissen ist, rückwärts neu zusammengesetzt zum Krimi, Matrix (1999–2003) erweist sich in literaturwissenschaftlicher Interpretation als Konglomerat und Stückwerk religiöser, literarischer und bildlicher Textualität. Die Zuschauer konsumierten die virtuellen Welten als funktionierende in Bestätigung der Theorie die bislang vor allem am Roman gebildet wurde.

Romane entfalten sich letzten Endes in einer privaten Lektüre – eine Privatsache ist das individuelle Erleben im selben Moment nicht mehr. Man kann in der Folge im Roman des 20. und 21. Jahrhunderts den weitesten Bogen spannen vom individuellen Erleben in die öffentliche Politik und über sie hinaus in primär fiktionale Weltentwürfe.

Persönliche Ängste, Tagträume, halluzinatorische Wahrnehmungen breiten sich im Roman des 20. und 21. Jahrhunderts in Experimenten aus. Was privat geäußert die Offenlegung einer psychotischen Störung wäre, etwa der Bericht Gregor Samsas, der in Franz Kafkas "Die Verwandlung" als gewaltiges Ungeziefer aufwacht, erhält in der literarischen Fiktion unverzüglich heterogene Bedeutung, die in der öffentlichen Debatte als Metapher, als Bild für die moderne Lebenserfahrung von Instabilität und Verlust persönlicher Konsistenz interpretiert wird. Das Wort Kafkaesk wurde vom Roman auf die Erfahrung übertragen, die seither kollektiven Rang genießt.

Die Generationen des 20. Jahrhunderts fanden ihre eigenen Romane. Deutschlands Veteranen des Ersten Weltkriegs identifizierten sich mit dem Helden von Erich Maria Remarques "Im Westen nichts Neues" (1928) (und ein Jahrzehnt später mit dem protoexistentialistischen faschistischen Gegenentwurf Thor Gootes). Der französische Existentialismus manifestierte sich in Romanen. Weltruhm erlangten Jean-Paul Sartres "Der Ekel" (1938) und Albert Camus’ "Der Fremde" (1942). Den Kalten Krieg überschattete George Orwells "1984" (1949). Die Subkultur der 1960er entdeckte sich in Hermann Hesses "Der Steppenwolf" (1927) wieder. Ken Keseys "Einer flog über das Kuckucksnest", Thomas Pynchons "Gravity’s Rainbow" und Chuck Palahniuks "Fight Club" (1996) gewannen ähnlich Kultstatus bei den nachfolgenden Generationen (letzterer nicht ohne die Hilfe des Kinofilms, der den Stoff aufnahm).

Den spezifisch auf das männliche Geschlecht ausgerichteten Identifikationsangeboten stehen mit dem 20. Jahrhundert epochale Entwürfe weiblicher Autoren gegenüber: Virginia Woolf, Simone de Beauvoir, Doris Lessing, Elfriede Jelinek machten feministische Geschlechterpolitik. Das Feld ist dabei entschieden komplexer als das einer einfachen Konfrontation. Aneignungen sexistisch männlicher Weiblichkeitsmodelle erweiterten in den letzten Jahren das Spektrum der Optionen. Provokant starke Heldinnen aus der Trivialliteratur erlangten Kultstatus unter weiblichen Lesern.

Die wichtigsten gesellschaftliche Prozesse des 20. Jahrhunderts wurden von und in Romanen reflektiert, zum Beispiel die sexuelle Revolution. Die Zensur riskierte D. H. Lawrence mit "Lady Chatterley’s Lover", in Italien 1928 publiziert, in Großbritannien erst 1960 freigegeben. Einen gleichwertigen Skandal erzeugte Henry Miller mit "Wendekreis des Krebses" (1934) in den USA. Von hier über die "Geschichte der O" von Anne Desclos (1954) und Vladimir Nabokovs "Lolita" (1955) bis zu Michel Houellebecqs "Elementarteilchen" (1998) verläuft eine Geschichte der Grenzüberschreitungen, mit denen der Roman die Öffentlichkeit gegenüber der Sexualität neu positioniert.

Verbrechen und Kriminalität beherrschen den Roman des 20. und 21. Jahrhunderts in einer eigenen Gattung des Kriminalromans. Kriminalität entfaltet sich als Thema in dem Maße, wie aus privater Perspektive die Realität der modernen, hoch organisierten Gesellschaften hinterfragt wird. Der Straftäter steht als Individuum vor Gericht, die Opfer sind in erster Linie privat betroffen. Die Instanz des Detektivs oder Kommissars bildet die Schnittstelle zwischen öffentlicher Institutionalisierung (und Entmachtung, der Kommissar ermittelt, er richtet allenfalls als Privatmann) und privater Gegensicht auf die Realität. Patricia Highsmiths Thriller machten den Krimi zum neuen Ort psychologischer Beobachtung. Paul Austers "New-York-Trilogie" (1985–1986) erschloss das im Ansatz triviale Feld der experimentellen Postmoderne.

Die wichtigsten politischen Kontroversen involvierten Romanautoren des 20. und 21. Jahrhunderts. Günter Grass "Die Blechtrommel" (1959) und Joseph Hellers "Catch-22" (1961) widmeten sich dem Zweiten Weltkrieg. Der Kalte Krieg bestimmte den modernen Spionageromane. Der politische Aufbruch Lateinamerikas manifestierte sich mit einer Riege lateinamerikanischer Autoren der Avantgarde von Julio Cortázar, Mario Vargas Llosa zu Gabriel García Márquez. Magischer Realismus wurde ein Markenzeichen der von lateinamerikanischen Autoren ausgehenden Auseinandersetzung mit der Realität. Die Terroranschläge des 11. Septembers 2001 fanden breiten literarischen Niederschlag. Art Spiegelmans "In the Shadow of No Towers" (2004) ist hier einer der interessantesten Grenzgänge mit den Fragen, denen hier der Autor des graphischen Romans die kollektive Realität der Bilder entgegensetzt.

Jenseits der Realität und ihrer kunstvollen Umformung beginnen die Alternativwelten der Fantasy und der Science-Fiction. Mit der Fantasy verschwimmen zudem die Grenzen zwischen Roman, Rollenspiel und esoterischer Mythenbildung. Im Zentrum dieser Produktion etablierte sich als literarischer Klassiker J. R. R. Tolkiens "Der Herr der Ringe" (1954/55), ein Roman, in dem sich ursprünglich primär jugendliche Leser in einem mythischen Konflikt zwischen archaischen Kulturen der germanischen Vorzeit mit Entlehnungen aus Beowulf und der skandinavischen Edda mit Weltentwürfen der Artusepik identifizieren konnten.

Die Science-Fiction entwickelte gegenüber der Fantasy Spielräume, die mit der Welt ihrer Leser gezielt historisch und räumlich in Verbindung gebracht werden, ohne erreichbar zu werden. Jules Verne schuf die Klassiker der mit Technik und den Wissenschaften spielenden Produktion des 19. Jahrhunderts. Aldous Huxleys "Brave New World" (1932) und George Orwells "1984" gestalteten die Perspektiven auf das technisch Machbare zu warnenden politischen Szenarien aus. Stanisław Lem, Isaac Asimov und Arthur C. Clarke wurden die klassischen Autoren einer eher experimentellen Science-Fiction, die die Interaktion von Menschen und Maschinen besonders thematisierte. (Post-)apokalyptische Phantasien kamen mit der Ost-West-Konfrontation und der atomaren Bedrohung in die Gattung. Virtual realities wurden in den letzten Jahren beliebt – das Internet und der kollektive Umgang mit medienvermittelter Erfahrung sorgten hier für neue Themen. William Gibsons "Neuromancer" (1984) ist heute einer der Klassiker dieses Feldes.

Niedere, aus heutiger Sicht triviale Literatur hatte es seit dem frühen Druck gegeben: Bücher, die man wieder und wieder las und die man kaufte, da sie allgemein beliebt waren. Manche Titel waren europaweit bekannt. Das Segment umfasste Romane ohne klare Trennung zwischen Fiktion und Geschichte. Dieses Marktsegment starb Ende des 18. Jahrhunderts aus, als mit dem Erstarken der Nationalstaaten die Bildungssysteme ausgebaut wurden. Lektüre wurde zur allgemeinen Pflicht erklärt. Das Ziel war die Heranführung des Volkes an wertvolle Fiktionen. Dies führte zu einer Aufspaltung in unterrichtstaugliche Titel und einen Massenmarkt, der unter Verachtung der Kritik seine Kundschaft fand. Die Trivialliteratur, die sich in dieser Umwälzung im 19. Jahrhundert entwickelte, ist aus diesem Grund keine Fortsetzung des alten (in der der Romantik nostalgisch zu Volksbüchern erklärten) Bereichs, sondern weitaus mehr eine Fortsetzung des Feldes eleganter Fiktionalität, das sich seit dem 17. Jahrhundert entwickelt hatte und das bis in die 1780er weitgehend ohne kritische Beachtung verkäuflich war. Die Genres der neuen Trivialliteratur setzten frühere Typen des Romans fort, die bereits um 1700 den Markt beherrscht hatten: Liebesromane, historisch eskapistische Romane, Abenteuerromane (siehe auch Belletristik).

Trivialliteratur umfasst historische Namen wie Karl May, Hedwig Courths-Mahler, Raymond Chandler, Margaret Mitchell, Barbara Cartland, Ian Fleming, Johannes Mario Simmel. In längeren Zeiträumen verändert sich die Auffassung über die Zugehörigkeit zur Trivialliteratur. Paul Heyse war 1910 Nobelpreisträger für Literatur, heute wird er in der Nähe der Trivialliteratur gesehen. Der Bereich ist in sich heterogen und bietet heute im gehobenen Segment international gefeierte Bestsellerautoren wie Rosamunde Pilcher, Ken Follett, Stephen King, Patricia Cornwell, Dan Brown und Joanne K. Rowling. Die kollektive, quasi-industrielle Textproduktion sichert über Jahrzehnte die Kontinuität beliebter Serien.

Trivialliteratur des gehobenen Feldes lebt von Berührungen aktueller Debatten. Politische Konfrontationen, Verschwörungen oder Abgründe des Verbrechens sind verbreitete Stoffe. Populäre Autoren geben zu, dass sie ihre Themen primär als Garanten von Spannung ausschöpfen. Sie artikulieren aber auch den Konsens, der gegen Unrecht besteht. Künstlerische Experimente sind in diesem Marktsegment unüblich.

Renommierte Autoren von Trivialliteratur verfügen über eine Stammkundschaft. Fans verfolgen jeden neuen Titel, der Autor erfüllt ihre Erwartungen. In den niedrigeren Marktsegmenten wird die Bindung an den bekannten Autor durch die Stabilität von Pseudonymen und Serientiteln ersetzt, wie Perry Rhodan, Der Bergdoktor, Captain Future oder Jerry Cotton. Im gesamten Feld schaffen strukturierte Genres (wie Arzt-, Heimat-, Berg-, Abenteuer-, Spionage, Liebes-, Kriminalroman) Klarheit über die zu erwartende Lektüre. Der größte Umsatz wird mit Liebesromanen gemacht.

Vor allem die Rezensionen der Literaturkritik halten die Unterscheidung zwischen Trivialliteratur und gehobener Literatur aufrecht und sorgen für die entsprechende Marktdifferenzierung. Hohe Literatur entsteht somit in der Interaktion mit der Literaturkritik. Trivialliteratur und ihr Publikum werden hingegen von der Literaturkritik und der Literaturwissenschaft hauptsächlich verachtet. Das Verhältnis ist durch gegenseitige Nichtbeachtung geprägt. Die strenge Unterscheidung zwischen anspruchsvoller und trivialer Literatur ist typisch für die kontinentaleuropäischen Nationen und im englischsprachigen Raum weniger ausgeprägt. Deutsche und französische Kritiker verbieten sich noch heute eindeutige Würdigungen des Trivialen.




</doc>
<doc id="4240" url="https://de.wikipedia.org/wiki?curid=4240" title="Distale Radiusfraktur">
Distale Radiusfraktur

Die distale Radiusfraktur ist die häufigste Fraktur des Menschen und macht bis zu 25 % aller Knochenbrüche aus, besonders oft bei Kindern und im Alter. Je nach Unfallhergang kommt es bei Bruch der Speiche nahe dem Handgelenk am häufigsten zu einer Abknickung (Achsenabweichung) zur Streckseite ("Colles-Fraktur"), seltener zur Beugeseite ("Smith-Fraktur"). Gelegentlich treten auch andere Bruchformen auf, oft ist die benachbarte Elle auch gebrochen.

Der Radius weist 2 cm proximal des Handgelenks mit der Metaphyse eine Zone geringerer Stabilität auf (Locus minoris resistentiae). Die Stabilität dieser Zone ist durch Systemerkrankungen maßgeblich beeinflusst. So kommt es durch Osteoporose und der dünner werdenden Kortikalis (Knochenrinde) zu einer verminderten Stabilität und einem damit verbundenen deutlichen Anstieg der distalen Radiusfrakturen im Alter über 60. 

Bei übermässiger Krafteinwirkung durch Sturz oder Aufprall wird die maximale Belastungsgrenze überschritten. Es kommt zur sogenannten "Fractura radii in loco typico". Der übliche Unfallshergang ist ein Sturz, der mit der Hand abgefangen wird. Ist die Hand dabei überstreckt (dorsal extendiert, übliche Haltung beim Abstützen) kommt es zum Extensionstyp, der häufigeren Colles-Fraktur. Erfolgt der Sturz auf die palmar flektierte (Richtung Handfläche gebeugte) Hand, kommt es zu einer Fraktur vom Flexionstyp, der Smith-Fraktur.
Erfolgt eine Dislokation des distalen Radiusfragments nach dorsal (Richtung Handrücken) ergibt sich eine Fourchette-Stellung (frz.: Gabel), bei einer Dislokation nach palmar und radial (Richtung Daumenseite) ergibt sich eine Bajonett-Stellung.

Leitsymptome sind Schmerzen und Funktionseinschränkung. Typischerweise findet sich eine schmerzhafte Schwellung, eine Verbreiterung des Handgelenks, ein Hämatom und eine schmerzhafte Einschränkung der Beweglichkeit. Gelegentlich besteht ein Instabilitätsgefühl. 

Die Diagnose erfolgt mit Röntgenaufnahmen in zwei Ebenen. Hierbei ist auf Begleitverletzungen wie eine Fraktur der Ulna, eine Abrissfraktur des Processus styloideus ulnae und Bandverletzungen, vor allem des scapholunären (SL-) Bandes zu achten. 

Bei der klinischen Untersuchung muss gezielt nach Nervenläsionen, Gefäßverletzungen und Hautverletzungen gesucht werden, wozu eine Palpation der Pulse der Arteria radialis und ulnaris, eine Überprüfung der Kapillarfüllung, der Fingerfunktion, der Daumenstrecksehenen und der Sensibiliät gehören. Bei Verdacht auf Gefäßläsionen können Dopplersonografie und Angiografie eingesetzt werden. 

Für die Therapieentscheidung ist insbesondere die Frage der Stabilität des Knochenbruchs wichtig. Dafür haben sich radiologische Instabilitätskriterien etabliert:

Den Empfehlungen der Norwegischen Gesellschaft für Orthopädie entsprechend sind lediglich die folgenden Kriterien mit ausreichend Evidenz belegt:
Trifft eine oder mehrere der genannten Kriterien bei einem primär durchgeführten Röntgen zu, ist bei Erwachsenen ≥ 18 Jahren eine operative Versorgung in Erwägung zu ziehen.

Eine gängige Klassifikation der distalen Radiusfrakturen ist, wie bei allen Knochenbrüchen, die AO-Klassifikation der Arbeitsgemeinschaft für Osteosynthesefragen.
Die A-Frakturen befinden sich außerhalb des Gelenks (extraartikulär) und lassen sich in die Typen A1, A2 und A3 weiter unterteilen. B-Frakturen sind nach der AO-Klassifikation partiell intraartikulär, d. h., sie betreffen zum Teil die Gelenkfläche. Typ-C-Frakturen befinden sich im Gelenk selbst (intraartikulär). Verschiedene Schweregrade werden hinter den Buchstaben A, B und C aufsteigend mit den Zahlen 1 bis 3 belegt.

Die Behandlung einer einfachen Radiusfraktur ohne Gelenkbeteiligung hängt vom Ausmaß der Verschiebung und Instabilität im Bruchbereich ab. Ohne Verschiebung der Bruchenden erfolgt eine einfache Ruhigstellung (Retention) mit einem Gipsverband für ca. sechs Wochen, bei einfachen Verschiebungen (Dislokation) erfolgt zuerst eine Einrichtung (Reposition) und anschließend die Ruhigstellung im Gipsverband.

Ist der Bruch instabil, neigt er also dazu, sich neuerlich zu verschieben (sekundäre Dislokation; häufig bei glatten und Querbrüchen), oder ist das Einrichten nicht erfolgreich beziehungsweise aufgrund des Ausmaßes nicht möglich, erfolgt die Behandlung operativ.

Lange Jahre war die Kirschnerdraht-Osteosynthese das Standardverfahren mit Einbringen von Drähten in Bruchspaltanästhesie, Plexus- oder Allgemeinanästhesie in der Technik nach Willenegger oder Kapandji. Zuletzt hat sich zunehmend die Plattenosteosynthese als Standard durchgesetzt. 

Dabei ist das Verfahren mit Bohrdrähten ein geschlossenes Verfahren, der Bruch muss also nicht durch einen Schnitt freigelegt werden. Bei der Plattenosteosynthese wird zumeist von beugeseitig (volar) die Haut eröffnet und der Bruch dann offen wieder eingerichtet und mit der Platte und den Schrauben fixiert. Nach geschlossener Behandlung mit Bohrdrähten wird in der Regel zudem ein Gips für vier bis sechs Wochen angelegt. Bei offener Bruchbehandlung mit Plattenosteosynthese kann in der Regel auf einen zusätzlichen Gips alsbald verzichtet werden, um dann rasch mit der Beübung des Handgelenkes beginnen zu können.

Das Verfahren der beugeseitigen T-Platten-Osteosynthese ist in den letzten Jahren immer beliebter geworden und wird bei instabilen Brüchen, komplexen Brüchen oder solchen mit Gelenkbeteiligung immer häufiger eingesetzt. Dies liegt vor allem daran, dass eine sichere und exakte Einrichtung der verschobenen Bruchfragmente möglich ist, auch wenn vergleichende Studien nur ein gering besseres Ergebnis im Vergleich zur Bohrdraht-Behandlung zeigen und diese deutlich kosteneffizienter ist. In einer englischen Studie wurden Gesamtkosten von mittleren 3.800 £ für die Spickdraht-Behandlung und 4.400 £ für die Plattenosteosynthese ermittelt, woraus sich aufgrund der nur minimal besseren Ergebnisse Kosten von 89.000 £ pro gewonnenem qualitätskorrigiertem Lebensjahr (QALY) ergaben.

Zusammenfassend kann also, je nach Art der Fraktur entweder konservativ im Gips, operativ mit Gips und Bohrdrähten oder aber operativ mit Platte, dann aber ohne Gips, behandelt werden.

Offene Radiusfrakturen machen meist die Anlage eines "Fixateur externe" erforderlich. Die Behandlung der Smith-Fraktur erfolgt durch beugeseitige Verplattung mit einer T-förmigen Platte.




</doc>

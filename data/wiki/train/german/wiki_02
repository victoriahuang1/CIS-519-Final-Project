<doc id="201" url="https://de.wikipedia.org/wiki?curid=201" title="Avitus">
Avitus

Eparchius Avitus (* um 385 in der Auvergne; † Anfang 457) war von 455 bis 456 weströmischer Kaiser.

Avitus wurde in der Auvergne um das Jahr 385 als Spross einer vornehmen, traditionsreichen gallorömisch-senatorischen Familie geboren. Sein Vater hieß Agricola, war zweimal gallischer Prätorianerpräfekt gewesen und bekleidete 421 den Konsulat. Avitus hatte eine Tochter, Papianilla, und zwei Söhne, Ecdicius und Agricola.

Um 419/20 diente Avitus dem Heermeister und kurzzeitigen Kaiser Constantius III. und knüpfte erste Kontakte mit dem westgotischen Hof in Toulouse; sein Schwiegersohn war Sidonius Apollinaris, dem wir auch wichtige Nachrichten für diese Zeit und bezüglich des Avitus verdanken, so, dass die Vorbildung des Avitus juristisch geprägt war ("civilia iura secutus"). 430 und 435 war Avitus unter dem „Reichsfeldherrn“ ("patricius") Flavius Aëtius tätig, bevor er 437 selbst das prestigeträchtige und einflussreiche Amt des Heermeisters für Gallien übernahm. Nach dieser Tätigkeit wurde er zwei Jahre später ungewöhnlicherweise Prätorianerpräfekt für Gallien - ungewöhnlich deshalb, weil die zivile und militärische Laufbahn in der Spätantike eigentlich strikt voneinander getrennt waren.

Obgleich Gallien zu dieser Zeit aufgrund der so genannten Völkerwanderung ständiges Krisen- und Kriegsgebiet war, war es für die weströmischen Kaiser nach dem Wegfall Britanniens und Nordafrikas sowie der weitgehenden Verwüstung Hispaniens das wichtigste Reichsgebiet neben Italien. Vor allem die gallischen Steuerzahlungen, die den gesamten weströmischen Staatshaushalt sicherten, waren extrem wichtig. Darum war die Präfektur, die Avitus besetzte und die für die Steuererhebung zuständig war, ein politisch besonders bedeutendes Amt. Avitus half dem faktischen Machthaber Aëtius, als dessen Parteigänger er gelten darf, dabei, die Kontrolle der Regierung in Ravenna über Gallien und Hispanien zu sichern.

Avitus erzielte als Präfekt seinen ersten großen Erfolg, als er durch geschickte Verhandlungen 439 die marodierenden westgotischen Foederaten unter ihrem "rex" Theoderich I. zu einem neuen Bündnisvertrag überreden konnte. Nach deren Rückzug aus Gallien auf die iberische Halbinsel zog sich Avitus, mittlerweile ein Freund der westgotischen Herrscherfamilie, ins Privatleben zurück (um 440).

Erst über zehn Jahre später sind uns wieder politische Aktivitäten seinerseits überliefert, als er 451 auf Bitten des weströmischen Kaisers Valentinian III. und des Aetius hin seine Kontakte nutzte und als Diplomat die Westgoten dazu brachte, sich einer militärischen Allianz gegen Atilla anzuschließen, der in die innerrömischen Machtkämpfe eingegriffen hatte und in Gallien einmarschiert war. Eigentlich bestanden zwischen Aetius und den Westgoten Spannungen. Avitus aber meisterte diese Aufgabe, und Theoderich I. verbündete sich mit den ravennatischen Truppen unter Führung des Aetius. Der Westgote führte selbst seine Armee in die berühmte Schlacht auf den Katalaunischen Feldern, in der er den rechten Flügel befehligte, aber dabei umkam.

Im Herbst 454 erschlug Kaiser Valentinian III. eigenhändig den Aetius, um sich von dessen Dominanz zu befreien, und im März 455 wurde der Kaiser selbst von Anhängern des Aetius ermordet. Mit ihrer Hilfe wurde anschließend Petronius Maximus nach dem Tod von Valentinian Kaiser. Er versuchte, seine instabile Herrschaft abzusichern, und holte deshalb erfahrene Senatoren, die zu den Anhängern des Aetius gezählt hatten, in die Politik zurück, darunter Avitus. Dieser wurde in den Rang eines "patricius" erhoben und sollte sich erneut diplomatisch mit den Westgoten beschäftigen, die das neue Regime militärisch stützen sollten. Zudem fühlten sich die Westgoten nach dem Tod Valentinians, mit dem sie ihre Bündnis geschlossen hatten, offenbar nicht mehr an den Vertrag mit den Römern gebunden. Avitus' Schwiegersohn, der Dichter Sidonius Apollinaris, berichtet sogar, nur durch die Vermittlung des Avitus habe ein Krieg mit den Goten verhindert werden können.

Doch nur kurz darauf überschlugen sich die Ereignisse: Petronius Maximus versuchte, vor den bei Rom gelandeten Vandalen sowie vor der Stadtbevölkerung, die ihn für den Mord an Valentinian III. verantwortlich machte und verachtete, zu fliehen, doch scheiterte dies; er wurde erkannt und am 31. Mai 455 getötet. Da Geiserich, der Anführer der Vandalen, keinen eigenen Kaiser erhob, folgte eine Thronvakanz. Der junge westgotische "rex" Theoderich II., der einst vielleicht sogar Latein bei Avitus gelernt hatte, drängte diesen nun angeblich dazu, sich zum Kaiser erheben zu lassen, und versprach die Unterstützung durch seine Krieger.

Avitus zögerte nicht lange und nahm das Angebot an; denkbar ist durchaus, dass die Initiative ohnehin von ihm selbst ausgegangen war. Dieser Schritt war jedenfalls auch im Sinne einflussreicher Kreise der gallorömischen Senatsaristokratie, die so wieder stärkeren Einfluss auf die Reichspolitik gewinnen wollte. Auf die Aufforderung des westgotischen Königs hin wurde nun – was die Schwäche des Weströmischen Reichsteils verdeutlichte – in Beaucaire eine außerordentliche Sitzung des gallischen Landtages aus den Notabeln der gesamten Präfektur einberufen, die Avitus am 9. Juli 455 ihr Einverständnis gab. Daraufhin wurde er von den römischen Truppen vor Ort zum Kaiser ausgerufen. Zustimmung kam auch aus der mittlerweile fast völlig verwüsteten Region Pannonien. Der oströmische Kaiser Markian verweigerte Avitus allerdings die formelle Anerkennung.

Zunächst schien die Herrschaft des Avitus abgesichert zu sein: Von Ostrom zumindest stillschweigend toleriert und von den Westgoten gestützt, glaubte er ausreichend Rückhalt zu haben. Er ernannte den Westgoten Remistus zum ersten Heermeister und "patricius", also zum faktischen Regierungschef. Doch als er sich nach Italien begab, erkannte er, dass die Gegner seiner Parteiung heftig gegen ihn opponierten und ihn mit Verleumdungen angriffen. Die italischen Senatoren, die im 4. Jahrhundert ganz im Schatten der gallo-römischen Aristokratie gestanden hatten, waren seit etwa 30 Jahren wieder in den Vordergrund gerückt und waren nicht bereit, diese Rolle nun wieder an ihre gallischen Rivalen abzutreten. Die stadtrömische Bevölkerung stand ihm ebenso wie zuvor Petronius Maximus feindselig gegenüber.

Zunächst jedoch drohte eine viel direktere Bedrohung durch die Vandalen, deren Kriegsflotte von rund 60 Schiffen das Tyrrhenische Meer unsicher machte und die Küsten der italischen Halbinsel angriff. Ihr König Geiserich forderte die Einsetzung des Olybrius als neuen Westkaiser. Gleichzeitig wüteten die Sueben in Hispanien, und Pannonien fürchtete eine weitere Verwüstung. Avitus suchte unter dem Eindruck all dieser Gefahren eine arbeitsteilige Lösung zu erreichen: Er bat seinen Verbündeten Theoderich II. um Unterstützung in Spanien, der dort in kaiserlichem Auftrag 456 die Sueben vernichtend schlug, die in die Provinz "Tarraconensis" eingefallen waren, und wollte sich persönlich den Verhältnissen in Pannonien widmen.

Um der maritimen Bedrohung durch Geiserich Herr zu werden, ernannte er einen im römischen Heer tätigen germanischen Offizier zum zweiten Heermeister für Italien: Flavius Ricimer. Als dieser bei Agrigent in Sizilien einen Seesieg über eine vandalische Flottenabteilung errang, konnte diese Bedrohung kurzzeitig eingedämmt werden.

Gleichzeitig war jedoch in Rom eine schwere Hungersnot ausgebrochen, da Geiserich die wichtigen Getreidelieferungen aus Nordafrika unterbrochen hatte. Avitus erkannte, dass die kostenlosen Getreidespenden des Staates ("annona civica") unter diesen Bedingungen nicht mehr leistbar waren, zumal auch die bei Rom stationierten "foederati" im römischen Heer diese Leistungen beanspruchten. Der Kaiser entschied offenbar, diese Truppen zu entlassen, beging damit allerdings einen schwerwiegenden Fehler: Zur Finanzierung der Entlassungen ließ er zahlreiche Bronzestatuen in und um Rom einschmelzen, um aus ihnen Münzen auszuprägen, was die hungernden Bürger noch mehr gegen ihn aufbrachte. Die Senatoren glaubten zudem, Italien werde zugunsten Galliens ausgeplündert. Gleichzeitig verlor er mit der Entlassung der vorwiegend gotischen Soldaten ein wichtiges Druckmittel gegenüber der Stadtbevölkerung, die sich nun Ricimer zuwandte.

Dieser war ohnehin nicht damit einverstanden, dass Avitus den größeren Teil jener Armee, die Italien zu schützen hatte, auflösen wollte. Er nutzte daher die Situation und schloss ein politisches Bündnis mit einigen Senatoren und dem "comes domesticorum" (Gardepräfekten) Iulius Valerius Maiorianus (Majorian), um Avitus zu entmachten. Remistus wurde von ihnen besiegt und getötet. Avitus versuchte daraufhin, sich nach Gallien zu begeben, wo sich seine Machtbasis befand. Nun stellte es sich aber als verhängnisvoll heraus, dass seine westgotischen Verbündeten gerade in Spanien kämpften und ihm daher nicht zur Hilfe kommen konnten: Am 26. Oktober 456 wurde Avitus mitsamt seinen verbliebenen Anhängern bei Piacenza gestellt und zur Abdankung gezwungen. Die sich anschließende Weihe zum Bischof dieser Stadt vermochte Avitus aber nicht zu retten, denn er kam spätestens im Januar 457 gewaltsam ums Leben. Ob er auf Anstiften Ricimers hin getötet wurde, bleibt offen; sein Nachfolger auf dem Thron, Majorian, wird in einigen Quellen ebenfalls dafür verantwortlich gemacht.

Nach Gregor von Tours soll Avitus seine letzte Ruhestätte in Brioude gefunden haben. Die Familie des Avitus blieb in ihrer Heimat zunächst einflussreich. Avitus’ Sohn Ecdicius stieg später unter Kaiser Iulius Nepos 474 zum Heermeister in Gallien auf und kämpfte dort bis zu seiner Absetzung gegen die Goten.



</doc>
<doc id="202" url="https://de.wikipedia.org/wiki?curid=202" title="LEG – Adler und Pfeil">
LEG – Adler und Pfeil

Der Adler war die erste Lokomotive, die kommerziell erfolgreich im Personenverkehr und später auch im Güterverkehr in Deutschland fuhr. Er und seine Schwestermaschine Pfeil wurden als "Dampfwagen" geführt. Das Eisenbahnfahrzeug wurde 1835 von der 1823 gegründeten Firma "Robert Stephenson and Company" im englischen Newcastle konstruiert und gebaut und an die Königlich privilegierte Ludwigs-Eisenbahn-Gesellschaft in Nürnberg (LEG) für den Betrieb auf ihrer Strecke zwischen Nürnberg und Fürth geliefert. Die offizielle Eröffnungsfahrt der Bahn fand, nach mehrmaliger Terminverschiebung (als erster Termin war der Geburtstag Ludwigs I. am 25. August geplant, ein weiterer am 24. November), schließlich am 7. Dezember 1835 statt. Der reguläre Betrieb wurde am 8. Dezember 1835 aufgenommen. Der Adler war eine Dampflokomotive der Bauart Patentee mit der Achsfolge 1A1 (Whyte-Notation: 2-2-2) und war mit einem Schlepptender der Bauart 2 T 2 ausgestattet.

Der "Adler" gilt häufig als die erste Lokomotive einer Eisenbahn auf deutschem Boden. Allerdings wurde bereits 1816 von der Königlich Preußischen Eisengießerei zu Berlin ein betriebsfähiger Dampfwagen konstruiert. Die sogenannte Krigar-Lokomotive zog bei einer Probefahrt einen mit 8000 Pfund beladenen Wagen. Das Fahrzeug kam jedoch nie zu einem kommerziellen Einsatz. Der "Adler" war zweifellos die erste erfolgreiche Lokomotive in Deutschland, die regelmäßig eingesetzt wurde.

Als während der Konstruktion der von Georg Zacharias Platner gegründeten "Ludwigsbahn" eine geeignete Lokomotive gesucht wurde, ging die erste Anfrage über die Londoner Firma Suse und Libeth an die Lokomotivfabrik von Stephenson und an Braithwaite & Ericsson in England. Die Lokomotive sollte in der Lage sein, ein Gewicht von zehn Tonnen zu ziehen, außerdem die Strecke zwischen Nürnberg und Fürth in acht bis zehn Minuten zurückzulegen und mit Holzkohle beheizbar sein. Stephenson antwortete, dass eine Lokomotive derselben Bauart wie die der Liverpool and Manchester Railway mit vier Rädern und einem Gewicht von 7,5 bis 8 Tonnen geliefert werden könne. Eine leichtere Maschine würde nicht bei jeder Wetterlage die nötige Adhäsionskraft besitzen und wäre teurer als eine schwerere Maschine. Johannes Scharrer bat trotzdem am 16. Juni 1833 um einen Kostenvoranschlag für zwei Lokomotiven mit einem Gewicht von 6,5 Tonnen und nötigem Zubehör. Der Kostenvoranschlag von Stephenson vom 4. Juli 1833 hatte eine Höhe von 1.800 Pfund Sterling. Die deutsche Firma Holmes und Rolandson in Unterkochen bei Aalen machte ein Angebot für einen Dampfwagen mit zwei bis sechs PS für 4.500 Gulden. Diese Verhandlungen verzögerten sich jedoch und führten zu keinem brauchbaren Ergebnis, worauf sie abgebrochen wurden. Ein weiteres Angebot kam von Josef Reaullaux aus Eschweiler bei Aachen. Ende April hielten sich Platner und Mainberger aus Nürnberg in Neuwied bei Köln auf, um dort den Auftrag für die Schienen an die Firma "Remy & Co." in Rasselstein zu erteilen. Remy & Co. war damals das einzige deutsche Werk, das seinerzeit Schienen in der geforderten Güte bezüglich Länge, gerade ausgerichtet, aus gewalztem Stahl und schräg abgeschnitten liefern konnte. Die nur 15 Fuß langen Schienen wurden dann aber gerade abgeschnitten geliefert, was von dem deutschen Konstrukteur Denis massiv bemängelt wurde und mussten teilweise noch gerichtet werden. Außerdem spielte ein möglicher hoher Einfuhrzoll und die nicht unerheblichen Frachtkosten bei einer eventuellen Bestellung in England mit, hier diesen Schritt zu wagen. Am 28. April reisten sie nach Köln weiter, um sich mit Platners Freund Konsul Bartls zu treffen. Dieser empfahl ihnen die belgische Maschinenbaufabrik Cockerill. Platner und Mainberger reisten dorthin nach Lüttich, mussten aber feststellen, dass Cockerill bis zu diesem Zeitpunkt noch keine einzige Lokomotive gebaut hatte. Sie erfuhren allerdings, dass Stephenson sich in Brüssel aufhalten würde. Sie erreichten Brüssel am 1. Mai und quartierten sich in dem Gasthof von Flandern ein. Dort wohnte auch Stephenson, der zu der Eröffnung der Eisenbahn von Brüssel nach Antwerpen am 5. Mai angereist war, mit mehreren Ingenieuren. Am 3. Mai 1835 kam es zu dem Abschluss eines Vorvertrages mit Stephenson, der eine Lokomotive der Bauart Patentee mit sechs Rädern mit einem Gewicht von sechs Tonnen für 750 bis 800 Pfund Sterling liefern sollte. Am 15. Mai 1835 wurde die neue Lokomotive bei der Lokomotivfabrik von Stephenson in Newcastle zu diesen Bedingungen bestellt. Darüber hinaus wurden ein Schlepptender und je ein Rahmen für einen Personen- und einen Güterwagen bestellt. Später stellte sich jedoch heraus, dass die Lokomotive entgegen den Vereinbarungen in Brüssel 900 Pfund Sterling kosten würde. Stephenson versprach ursprünglich in Brüssel eine Lieferung der Lokomotive bis Ende Juli nach Rotterdam.

In Nürnberg und England wurde mit abweichenden Maßeinheiten gearbeitet. Das bayerische Fuß und das englische Fuß waren unterschiedlich. Die Spurweite wurde auf die der Stockton and Darlington Railway festgelegt, da Stephenson auf dem Maß von 4 englischen Fuß und 8,5 Zoll (1435 mm) beharrte. Die in Nürnberg bereits verlegten Gleise waren um 5/8 Zoll zu schmal. Der Abstand der Schienen musste entsprechend angepasst werden. Die Lieferung des Fahrzeugs mit allen Zubehörteilen nach Nürnberg zu einem Preis lt. Rechnung von 1140 Pfund Sterling, 19 Schilling und 3 Pence (entspricht Pfund in heutiger Kaufkraft) bestand aus über 100 Einzelteilen in 19 Kisten von 177 Zentnern Gewicht. Die Kisten wurden am 3. September 1835 verspätet auf dem Schiff "Zoar" von London nach Rotterdam verschifft. Der Frachtlohn von Rotterdam nach Köln betrug 700 Francs, von Köln bis Offenbach 507 Gulden und 9 Kreuzer und von Offenbach bis Nürnberg 653 Gulden und 11 Kreuzer. Das Direktorium der Bayerischen Ludwigsbahn suchte am 23. April 1835 um eine Befreiung vom Einfuhrzoll nach. Die Lokomotive wurde als ein Muster für ein bisher unbekanntes Produkt für die Fabriken möglicher Hersteller im Inland deklariert. Nach verschiedenen Schwierigkeiten genehmigte am 26. September 1835 das Finanzministerium die beantragte zollfreie Einfuhr mit dem Fabrikanten Johann Wilhelm Spaeth als Empfänger der Lieferung.

Mit dem Schleppkahn "van Hees" des Schiffers van Hees, der von dem Dampfboot "Hercules" stromaufwärts geschleppt wurde, wurden die Transportkisten am 23. September 1835 von Rotterdam auf dem Rhein nach Köln transportiert. Wegen Niedrigwasser des Rheins musste er sich ab Emmerich für den Weitertransport statt des Dampfbootes des Treidelns mit Zugpferden bedienen. Am 7. Oktober traf der Schleppzug in Köln ein. Der Rest der Strecke musste ab dem 13. Oktober 1835 mit Pferdefuhrwerken zurückgelegt werden, da der Main wegen Niedrigwasser nicht schiffbar war. Diese Reise an Land wurde durch einen Streik der Fuhrleute in Offenbach unterbrochen, der einen Wechsel des Spediteurs zur Folge hatte. Am 26. Oktober 1835 erreichte der Transport Nürnberg. In den Werkstätten der Maschinenfabrik von Johann Wilhelm Spaeth wurde die Dampflokomotive zusammengesetzt. Der Aufbau erfolgte unter Aufsicht des mitgereisten Ingenieurs und Lokomotivführers William Wilson und des Fachlehrers Bauer mit der Hilfe von örtlichen Zimmerleuten.

Am 10. November 1835 äußerte das Direktorium der Bayerischen Ludwigsbahn die Hoffnung auf eine baldige Betriebsfähigkeit der Lokomotive. Die Lokomotive stand als Sinnbild für Kraft, Wagemut und Schnelligkeit.
Die beiden von Stephenson gelieferten Wagengestelle stellten sich als zu schwer für die Verhältnisse in Nürnberg heraus. Denis ging dagegen bei seinen Planungen davon aus, dass die Wagen sowohl von Pferden als auch von der Dampflokomotive gezogen würden und hielt aus diesem Grund eine leichtere Bauart für erforderlich. Der Bau der Wagen wurde von mehreren Firmen durchgeführt, die Untergestelle wurden von Späth, Gemeiner und Manhard hergestellt. Die Aufbauten aus Holz lieferte der Wagnermeister Stahl aus Nürnberg. Wegen der starken Auslastung der genannten Firmen mit anderen Aufträgen wurden drei Wagengestelle und 16 Räder bei der Firma Stein in Lohr bei Aschaffenburg hergestellt. Denis drohte den beteiligten Firmen mit einer künftigen Auftragsvergabe nach England, wenn diese die Arbeiten nicht beschleunigten. Ende August 1835 wurde der erste Wagen fertiggestellt. In der zweiten Oktoberhälfte war die Fertigstellung der restlichen Wagen absehbar. Bis zur Eröffnung der Bayerischen Ludwigsbahn wurden neun Wagen hergestellt: zwei Wagen der dritten Wagenklasse, vier der zweiten und drei Wagen der ersten Klasse. Am 21. Oktober 1835 fand der erste öffentliche Fahrversuch mit einem von einem Pferd gezogenen Personenwagen und 23 Personen statt. Der Konstrukteur Denis hatte eine Bremse für die Wagen entwickelt, die bei dieser Gelegenheit getestet wurde. Der Wagen konnte in jeder Situation sicher zum Stehen gebracht werden.

Am 16. November 1835 wurde die erste Probefahrt mit der Dampflokomotive von Nürnberg nach Fürth und zurück unter großer Anteilnahme der Bevölkerung durchgeführt. Wegen der herrschenden Kälte wurde mit gemäßigter Geschwindigkeit gefahren. Drei Tage später wurden bei einer weiteren Testfahrt fünf vollbesetzte Wagen in 12 bis 13 Minuten über die Strecke befördert. Auf der Rückfahrt erfolgten Bremsproben, und das Ein- und Aussteigen der Passagiere wurde geprobt. Darauf folgende tägliche Versuche zeigten unter anderem, dass bei der Verwendung von Holz als Heizmaterial durch Funkenflug die Kleider von mehreren Passagieren versengt wurden. Die Teilnahme an einer Probefahrt kostete 36 Kreuzer, der Erlös wurde der Fürsorge für Arme zugeführt.

Der Adler war auf einem mit Blech beschlagenen Rahmen aus Holz aufgebaut. Die beiden innenliegenden, mit Nassdampf betriebenen waagerechten Zylinder trieben die sich in der Mitte befindende Treibachse an. Die Treibräder besaßen keinen Spurkranz, um enge Kurvenradien befahren zu können. Die geschmiedeten Radspeichen waren mit dem Radkranz vernietet. Die ursprünglichen Räder bestanden aus Gusseisen und waren mit einem geschmiedeten Radreifen umgeben. Sie wurden später durch stabilere Räder aus Schmiedeeisen ersetzt. Die hohlen Speichen enthielten einen Kern aus Holz, um Unebenheiten besser abzufedern. Alle Räder der Lokomotive waren ungebremst. Eine Spindelbremse wirkte auf die beiden auf der rechten Seite des Heizers liegenden Räder des Tenders. Die Verbindung zwischen Lokomotive und Tender war starr. Die Puffer waren aus Holz. Der hufeisenförmige Wasserkasten umfasste den Kohlenvorrat des Tenders. Als Brennmaterial wurde zunächst Koks und später Steinkohle benutzt.

Die Personenwagen hatten beim Kutschenbau verwendete Kutschenkästen, die auf ein Fahrgestell aus Eisen montiert waren. Der zweiachsige Coupé-Wagen mit drei hintereinandergesetzten einzelnen voneinander getrennten Abteilen bildete das Grundprinzip der ersten deutschen Eisenbahnwagen. Spezielle Fahrgestelle für Personenwagen wurden erst 1842 bei der Great Western Railway entwickelt. Sämtliche Wagen waren in der Farbe Gelb der Postkutschen lackiert. Die Wagen der dritten Wagenklasse besaßen ursprünglich kein Dach, drei Abteile mit acht bis zehn Sitzplätzen, die Einstiege hatten keine Türen. Die Wagen der zweiten Wagenklasse waren demgegenüber mit einem Segeltuchdach ausgestattet und hatten Türen, vor den unverglasten Fenstern waren Vorhänge aus Seide, später aus Leder angebracht. Bei gleicher Breite der Wagen war die Anzahl der Sitzplätze bei den teureren Klassen pro Reihe jeweils um einen reduziert. Die Wagen der ersten Wagenklasse waren mit einem kostbaren blauen Tuch ausgeschlagen, mit Fensterscheiben aus Glas versehen, die Türgriffe waren vergoldet und alle Beschläge aus Messing gefertigt. Der heute noch erhaltene Wagen Nr. 8 der 2. Wagenklasse ist im Verkehrsmuseum Nürnberg ausgestellt.

Am 7. Dezember 1835 fuhr der "Adler" erstmals offiziell die Strecke von 6,05 Kilometern in neun Minuten – mit 200 Ehrengästen sowie dem 26-jährigen Engländer William Wilson auf dem Führerstand. Im Abstand von jeweils zwei Stunden wurden zwei weitere Probefahrten durchgeführt. Die Lokomotive verkehrte mit bis zu neun Wagen mit maximal 192 Fahrgästen. Im normalen Betrieb wurden die Fahrten mit maximal 28 km/h durchgeführt, um die Lok zu schonen. Die normale Fahrzeit betrug etwa 14 Minuten. Demonstrationsfahrten ohne Wagen durften mit bis zu 65 km/h durchgeführt werden. In den meisten Fällen ersetzten allerdings noch Pferde als Zugtiere die Dampfmaschine. Wegen des noch hohen Preises der Kohle wurde der überwiegende Teil der Fahrten auf der Ludwigsbahn mit Pferdebahnen durchgeführt. Gütertransporte wurden zusätzlich zum Personenverkehr erst ab 1839 durchgeführt. Zu den ersten Transportgütern zählten Bierfässer und Vieh. 1845 fand bereits ein reger Güterverkehr statt.

Der "Adler" musste nach elf Betriebsjahren grundlegend überholt und instand gesetzt werden, ebenso wurde der "Pfeil" saniert. Wie im Bericht des Direktoriums vermerkt legten diese Maschinen in 11 resp. 10 Jahren 32.168 Fahrten mit mehr als 2 1/2 Millionen Personen zurück, folglich eine Strecke von ca. 64.000 Meilen (103.000 km), ohne einen Nachlass ihrer Kräfte zu zeigen.

Nach 22 Betriebsjahren wurde die Lokomotive ausgemustert. Sie war mit der Schwesterlokomotive "Pfeil" (er wurde schon 1853 verkauft) inzwischen die kleinste und schwächste Lokomotive auf dem europäischen Kontinent. Darüber hinaus war der Kohleverbrauch neuerer Dampflokomotiven inzwischen deutlich geringer geworden. Die Lokomotive wurde anschließend in Nürnberg als stationäre Dampfmaschine genutzt. 1857 verkaufte die Bahngesellschaft die Lokomotive mit dem Tender, aber ohne Räder und andere Anbauteile, an den Augsburger Ludwig August Riedinger.
Der Geschäftsbericht der Ludwigseisenbahn-Gesellschaft (LEG) im Jahr 1857 vermeldet hierzu:

Die vermutlich einzige Fotografie, die den Adler im Jahr 1851 oder 1856 zeigt, befindet sich im Besitz des Nürnberger Stadtarchivs. Allerdings sind weder das Alter der Fotografie eindeutig belegt, noch ob das Bild die Original-Lokomotive oder nur ein Modell zeigt.

Im Zusammenhang mit der Errichtung des Verkehrsmuseums Nürnberg wurde 1925 geplant, den Adler zu rekonstruieren. Exakte Pläne existierten jedoch nicht mehr. Lediglich ein Stich aus der Zeit der Original-Lokomotive gab Informationen. 1929 beendete die Weltwirtschaftskrise dieses Vorhaben.

Zum 100-Jahr-Jubiläum der Eisenbahn in Deutschland 1935 wurde ab 1933 von der Deutschen Reichsbahn im Ausbesserungswerk Kaiserslautern ein weitgehend originalgetreuer Nachbau erstellt. Die ursprüngliche Überlegung des Generaldirektors der Reichsbahn Dorpmüller und seines Stabs war, den Adler als Propagandainstrument der so genannten „"neuen Zeit"“ in der Stadt der Reichsparteitage (Nürnberg) zu benutzen. Geplant wurde, den Adler den Lokomotivgiganten wie der Baureihe 05 gegenüberzustellen. Für die Verwirklichung des Nachbaus konnte auf die Planungen von 1925 zurückgegriffen werden. Neben abweichenden technischen Daten unterschied sich der Nachbau vom Original vor allem durch dickere Kesselwände, zusätzliche Querverstrebungen und Speichenräder aus Stahl.
Der Nachbau erreichte bei Probefahrten auf einer 81 Kilometer langen Strecke eine Durchschnittsgeschwindigkeit von 33,7 km/h. Die Strecke wies Neigungen zwischen 1:110 und 1:140 auf. Vom 14. Juli bis zum 13. Oktober 1935 konnten Besucher mit dem rekonstruierten Adler-Zug auf einer Strecke von 2 km um das Gelände der Jubiläumsausstellung in Nürnberg fahren. Auf dem Führerstand fuhren unter anderem Reichsbahn-Generaldirektor Julius Dorpmüller und der Gauleiter Frankens Julius Streicher mit. Der Adler-Nachbau fuhr danach noch 1936 beim Cannstatter Wasen in Stuttgart und bei den Olympischen Spielen in Berlin. Beim 100-Jahr-Jubiläum der ersten preußischen Eisenbahn 1938 verkehrte der Adler-Zug zwischen Berlin und Potsdam. Danach kam er in das Verkehrsmuseum Nürnberg. Der Nachbau wurde genau wie das Original als "Dampfwagen" geführt. Durch diese Einstufung war er nicht von Dampflokverbot betroffen (er stand zu dieser Zeit allerdings und danach nicht betriebsfähig im "Adlersaal" im Verkehrsmuseum Nürnberg) und seinem Einsatz zum 150 jährigen Jubiläum 1985 stand somit nichts "Bürokratisches" im Wege.

1950 wurde der Adler-Zug bei einem Festzug zur 900-Jahr-Feier von Nürnberg auf einem Straßenroller durch die Stadt gefahren.

Zur 125-Jahr-Feier der Deutschen Eisenbahnen 1960 wurde der Zug auf der Strecke der Nürnberg-Fürther Straßenbahn zwischen dem Plärrer in Nürnberg und dem Hauptbahnhof Fürth eingesetzt. Die Innenseiten der Räder mussten für die Fahrt auf Straßenbahngleisen abgedreht werden.

1984 wurde er zur 150-Jahr-Feier der Deutschen Eisenbahnen von der Deutschen Bundesbahn im Ausbesserungswerk in Offenburg instand gesetzt. Dabei mussten unter anderem die 1960 für die Fahrt auf Straßenbahngleisen abgedrehten Radinnenseiten wieder aufgeschweißt werden. Der Dampfkessel wurde nach den aktuellen Sicherheitsbestimmungen überprüft. Der Adler nahm an der großen Jubiläumsausstellung in Nürnberg und an zahlreichen Veranstaltungen im damaligen Bundesgebiet wie zum Beispiel in Hamburg, Konstanz und München teil. Am 22. Mai 1984 wurden Publikumsfahrten zwischen dem Hauptbahnhof Nürnberg und Nürnberg Ost angeboten.

Die Lokomotive wurde zwischen Ende 1985 und 1999 nicht betrieben. Für die 1999 geplanten Fahrten waren mehrmonatige Restaurierungsarbeiten erforderlich. Am 16. September 1999 erteilte das Eisenbahn-Bundesamt die Betriebsgenehmigung. 1999 fuhr zur 100-Jahrfeier des ehem. "Königlich Bayerischen Eisenbahnmuseums" und des "Verkehrsmuseums Nürnberg" als dessen Nachfolger der Adler-Zug an drei Sonntagen im Oktober und nahm an der "Großen Fahrzeugparade" im Rangierbahnhof Nürnberg teil. In den Folgejahren wurde der Adler-Nachbau bei mehreren Nostalgiefahrten in Deutschland eingesetzt. Er stand bis 2005 im Verkehrsmuseum Nürnberg.

Bei einem Brand im Depot des Verkehrsmuseums (Ringlokschuppen des Bahnbetriebswerks Nürnberg West) am 17. Oktober 2005, in dem sich 24 Lokomotiven befanden, wurde u. a. der – bis zuletzt fahrtüchtige – Nachbau des "Adlers" schwer beschädigt. Der Vorstand der DB beschloss, ihn wieder instand setzen zu lassen. Das Wrack wurde am 7. November in vierstündiger Arbeit von einem Bergungstrupp der Preßnitztalbahn mit einem Autokran aus den Trümmern des Lokschuppens gehoben und anschließend mit einem Spezialtieflader zum Dampflokwerk Meiningen gebracht. Es zeigte sich, dass zumindest der Kessel dank der Befüllung mit Wasser relativ unbeschädigt geblieben war, obwohl seine komplette Holzverkleidung verbrannt und viele Bleche geschmolzen waren. Er konnte daher für den Wiederaufbau von 2007 verwendet werden.

Die Rekonstruktion des 2005 beschädigten Adlers lief Mitte April 2007 an und war im Oktober 2007 abgeschlossen. Der mit Metall verkleidete Holzrahmen war so stark beschädigt, dass er komplett neu gebaut werden musste. Ein Wagen der dritten Klasse, der an einem anderen Ort ausgestellt war und dadurch den Brand unbeschadet überstand, diente als Vorlage für die neuen kutschenähnlichen Waggons, die von einer Schreinerei in Meiningen gefertigt wurden. Die Kosten beliefen sich auf etwa eine Million Euro, davon konnten 200.000 Euro aus Spenden der Bevölkerung aufgebracht werden. Der Direktor des "DB-Museums Nürnberg" stellte vor Beginn der Rekonstruktion klar, dass die Wiederaufarbeitung mit allen verbrannten Details ausgeführt werden würde, und erklärte, es würden keine Kompromisse gemacht. Es wurde sogar noch präziser nach historischen Zeichnungen gearbeitet, so wurde beispielsweise der ebenfalls beim Brand beschädigte Schornstein nicht in der beim Nachbau von 1935 abweichenden, sondern in der ursprünglichen Form angefertigt. Ein Problem stellte die als Kurbelwelle ausgebildete, einteilige Treibachse der Lok dar, sie konnte nicht im Dampflokwerk Meiningen geschmiedet werden. Mit dieser Arbeit wurden die "Sächsischen Schmiedewerke" in Gröditz beauftragt, die die Schmiedearbeiten an der Kurbelwelle und den Radreifen durchführen konnten. Das anschließende Abdrehen wurde von der "Gröditzer Kurbelwelle Wildau GmbH" durchgeführt. Für den Rahmen der Lokomotive wurde acht bis zwölf Jahre abgelagertes Eschen-Holz verwendet, das elastisch genug ist, die Erschütterungen durch die Kraftübertragung während der Fahrt auszuhalten. Der Unterbau des Tenders wurde aus hartem Eichenholz gefertigt.

Seit dem 23. November 2007 befindet sich der wiederhergestellte „alte“ Adler mit einem alten (1935) und zwei neuen (2007) dazugehörigen Personenwaggons der dritten Wagenklasse wieder im DB Museum in Nürnberg. Im dortigen Stammhaus haben der nur rollfähige Adler von 1950 sowie der originale, 1835 gebaute und 1838 und 1846 umgebaute, Personenwagen der zweiten Wagenklasse Nr. 8 der Ludwigsbahn, der der Konservierung halber nicht mehr auf die Schienen gestellt wird, ihren Platz.
Am 26. April 2008 fuhr der Nachbau erstmals wieder zwischen Nürnberg und Fürth. Im Mai folgten Sonderfahrten in Nürnberg, Koblenz und in Halle an der Saale. Im April 2010 wurden im Rahmen des 175-Jahr-Jubiläums der Eisenbahn in Deutschland auf dem Gelände des DB Museums in Koblenz-Lützel Fahrten mit Besuchern durchgeführt. Im Mai und Juni 2010 fanden Fahrten zwischen Nürnberg Hauptbahnhof und Fürth Hauptbahnhof statt.
Seit 2013 ist auch der betriebsfähige Adler im Nürnberger DB-Museum zu besichtigen, er steht in der Fahrzeughalle II abgestellt.

Der Schlepptender und die drei Personenwagen des Adler-Zuges gehören zu den letzten in Deutschland noch per Handkurbel gebremsten Eisenbahnfahrzeugen.
Ein weiteres Exemplar, das im Gegensatz zum Nachbau von 1935 nicht betriebsfähig ist, wurde in den 1950er Jahren im Auftrag des Werbeamts der Deutschen Bundesbahn im Ausbesserungswerk München-Freimann erstellt. Dieser Nachbau diente der Öffentlichkeitsarbeit auf Ausstellungen und Messen. Er steht ebenfalls im DB Museum Nürnberg.

Seit 1964 fährt auf der Tiergartenbahn Nürnberg ein motorbetriebener Nachbau im Maßstab 1:2. Er wurde 1963/1964 von der Lehrlingswerkstatt der MAN gefertigt. Der „Mini-Adler“ startete in der Nähe des Eingangs und pendelte zum Kinderzoo. Im Zuge des Neubaus der Delfinbecken musste diese Strecke 2008 stillgelegt werden. Zwischenzeitlich wurde eine Erweiterung bzw. Verlegung der Strecke realisiert. Sie führt entlang des Giraffengeheges unterhalb der Delfinlagune vorbei bis hin zum Kinderzoo und hat eine Länge von gut einem Kilometer. Nach einer gut dreijährigen Pause steht die Bahn seit 31. März 2012 wieder der Öffentlichkeit zur Verfügung.

Bei der Görlitzer Parkeisenbahn verkehrt ein Nachbau mit einer Spurweite von 600 mm. Bei diesem Nachbau handelt es sich um eine Diesellokomotive.

Für die TV-Miniserie Der eiserne Weg anlässlich der 150-Jahre-Deutsche-Eisenbahnen-Feier im Jahr 1985 entstand für die Dreharbeiten ein fahrbarer Nachbau. Der Dampf wurde dabei chemisch erzeugt, für den Antrieb sorgte der im Tender verkleidete Vorderwagen eines Renault 5.

Im Rahmen des Stadtjubiläums „1000 Jahre Fürth“ wurde ein Bus mit dem Adler verziert, er machte Werbung für eine Ausstellung, bei der Spenden für den Wiederaufbau gesammelt wurden.

In den Briefmarken-Jahrgängen der Reichspost 1935, der Bundespost 1960 und der Deutschen Post 1960 sowie der Bundespost 1985 wurde der "Adler" zu den Jubiläen „100“, „125“ und „150 Jahre Deutsche Eisenbahn“ gewürdigt. Zum 175. Jahrestag erschien am 11. November 2010 erneut eine Sonderbriefmarke der Deutschen Post AG im Wert von 55 Eurocent mit diesem Motiv und auch eine 10-Euro-Silber-Gedenkmünze der Prägestätte München (D) mit der Randinschrift: "Auf Vereinten Gleisen 1835 – 2010".





</doc>
<doc id="203" url="https://de.wikipedia.org/wiki?curid=203" title="Agatha Christie">
Agatha Christie

Dame Agatha Mary Clarissa Christie, Lady Mallowan, DBE [] (* 15. September 1890 in Torquay, Grafschaft Devon; † 12. Januar 1976 in Wallingford, gebürtig "Agatha Mary Clarissa Miller") war eine britische Schriftstellerin. Die verkaufte Weltauflage ihrer Bücher soll über zwei Milliarden betragen, womit sie zu den erfolgreichsten Autorinnen der Literaturgeschichte zählt.

Bekannt wurde sie vor allem durch eine große Anzahl von Kriminalromanen und Kurzgeschichten, die auch mehrfach mit großem Erfolg für Kino und Fernsehen verfilmt sowie für die Bühne adaptiert wurden. Ihre berühmtesten Schöpfungen sind der belgische Detektiv Hercule Poirot mit seinem Freund Arthur Hastings sowie die altjüngferliche Miss Marple. Daneben gibt es andere wiederkehrende Figuren wie das Ehepaar Tommy und Tuppence Beresford oder Inspektor Battle, Sir Henry Clithering oder die Krimi-Autorin Mrs. Ariadne Oliver. In mehreren Miss-Marple-Romanen treten deren Neffe Raymond West, Schriftsteller, sowie dessen Verlobte und spätere Ehefrau Joan auf. Neben ihrer schriftstellerischen Tätigkeit unterstützte Christie ihren zweiten Ehemann, den Archäologen Max Mallowan, bei seinen Ausgrabungen im Nordirak und in Syrien, insbesondere bei der Restaurierung prähistorischer Keramiken und der Fotodokumentation der Funde. Sie trug maßgeblich zur Finanzierung dieser Expeditionen bei.

Agatha Mary Clarissa Miller kam als jüngstes Kind des Amerikaners Frederick Alvah Miller und dessen englischer Ehefrau Clarissa Margaret Boehmer zur Welt. Sie hatte eine Schwester, Margaret Frary Miller (1879–1950), genannt „Madge“ und einen Bruder, Louis Montant Miller (1880–1929), genannt „Monty“.

Agatha Christie wuchs in der viktorianischen Villa "Ashfield" in Torquay auf und wurde bis zu ihrem 16. Lebensjahr nicht in einer Schule, sondern von ihren Eltern (bzw. der Mutter) unterrichtet, die früh ihr schriftstellerisches Talent erkannten. Mit elf Jahren veröffentlichte sie ein erstes Gedicht in einem Lokalblatt.

Ihr Vater erzielte sein Einkommen aus Geschäften in Übersee, über die nichts Näheres bekannt ist, die der Familie aber ein Leben in Wohlstand ermöglichten. Agatha Christie selber erwähnt in ihrer Autobiographie andeutungsweise Immobilien in New York und in Trusts angelegtes Vermögen, aus dessen Zinseinkünften die Familie Miller lebte. Dabei kam es jedoch zu Veruntreuungen durch die amerikanischen Vermögensverwalter, wodurch die Familie Miller in finanzielle Schieflage geriet. Wie damals allgemein üblich, wurde das eigene Haus für den Sommer an Gäste vermietet, während die Familie Miller die Zeit in Pau und Cauterets bzw. auf den Kanalinseln verbrachte. Frederick Alvah Miller starb 1901, Agatha war damals elf Jahre alt. Clarissa Margaret Miller zog ihre Kinder nun alleine groß und bemühte sich, sie die durch den Tod des Vaters noch weiter verschärfte finanzielle Situation so wenig wie möglich spüren zu lassen.

Ihr zunächst begonnenes Musikstudium in Paris gab Agatha Miller mit Beginn des Ersten Weltkriegs auf und arbeitete als Krankenschwester ("Voluntary Aid Detachment") beim Britischen Roten Kreuz im örtlichen Krankenhaus, später in einer Apotheke. In dieser Zeit sammelte sie viele Erfahrungen mit giftigen Substanzen, die später in ihren Werken eine Rolle spielten.

1914 heiratete sie Oberst Archibald Christie, einen Flieger der königlichen Luftwaffe. Mit ihm hatte sie eine Tochter, Rosalind Margaret Clarissa Christie, die am 5. August 1919 geboren wurde.

1920 erschien ihr erster Kriminalroman: "Das fehlende Glied in der Kette" () mit dem belgischen Detektiv Hercule Poirot zunächst in den USA, dann in England. Schlagartig berühmt wurde Christie jedoch erst mit dem 1926 veröffentlichten Werk "Alibi" (engl. "The Murder of Roger Ackroyd").

In ihrer schriftstellerischen Tätigkeit hatte Agatha Christie schnell Erfolg, privat jedoch verliefen die 1920er Jahre eher unglücklich: Ihr Mann ließ sie berufsbedingt häufig allein, 1926 starb ihre Mutter – ein Ereignis, das sie stark mitnahm; außerdem musste Ashfield geräumt werden. Christie erschöpfte diese Situation. Im August 1926 gestand ihr Mann ihr außerdem die Affäre mit seiner Golfpartnerin Nancy Neele. Trotz mehrerer Versöhnungsversuche entzweite sich das Ehepaar danach immer mehr. Nach einem heftigen Streit am 3. Dezember 1926 verließ Agatha Christie das Haus. Ihr Auto wurde wenige Tage später verlassen an einem See gefunden. Die Suchmeldung der Polizei von Berkshire vom 9. Dezember 1926 zeigte ein Foto der Vermissten und lautete (aus dem Englischen übersetzt):

Nach einer spektakulären Suchaktion, über die sogar die New York Times berichtete und an der sich auch Arthur Conan Doyle beteiligte, fand man die Schriftstellerin zehn Tage nach ihrem Verschwinden in einem Hotel in Harrogate, wo sie unter dem Namen der Geliebten ihres Mannes als Mrs. Neele abgestiegen war. In der Folge beschäftigte die Frage der Kosten der Suchaktion sogar das britische Parlament. Ihre Familie verbreitete die Darstellung, dass sie einen fast vollständigen Gedächtnisverlust für diese Tage erlitten habe. Agatha Christie selbst äußerte sich nie über ihre Beweggründe, auch nicht in ihren Memoiren. 1928 wurde ihre Ehe mit Archibald Christie geschieden.

Die Geschichte um das Verschwinden von Agatha Christie wurde 1979 von Regisseur Michael Apted filmisch umgesetzt in "Das Geheimnis der Agatha Christie" (engl. "Agatha"), mit Vanessa Redgrave in der Hauptrolle.

Um sich von den Strapazen der vergangenen Jahre zu erholen, entschied sie sich relativ spontan im Herbst des Jahres 1928 zu einer ausgedehnten Reise in den Nahen Osten und reiste mit dem Orient-Express nach Bagdad. Diese Spontanentscheidung (eigentlich hatte sie an die Karibik als Reiseziel gedacht) sollte das Leben Agatha Christies maßgeblich verändern und großen Einfluss auf ihr schriftstellerisches Werk ausüben.

Es war allerdings nicht ihre erste Begegnung mit dem Nahen Osten, denn bereits als junge Frau war sie mit ihrer Mutter in Kairo gewesen. Von Bagdad aus reiste sie weiter nach Ur, wo der Archäologe Leonard Woolley mit Ausgrabungen beschäftigt war, die seinerzeit in England starkes Aufsehen erregten. Er und seine Frau Katharine Woolley empfingen die Berühmtheit Agatha Christie hocherfreut; sie blieb längere Zeit beim Grabungsteam und freundete sich mit den Woolleys an. Später widmete sie ihnen die Kurzgeschichtensammlung "Der Dienstagabend-Klub". Das Ehepaar Woolley stand auch Modell für die Hauptfiguren des Romans "Mord in Mesopotamien", wobei Agatha Christie den Woolleys jedoch einige sehr unsympathische Charakterzüge hinzufügte. 

Als sie nach London zurückkehrte, tat sie dies mit einer Einladung von Katharine Woolley im Gepäck, im Frühjahr 1930 nach Mesopotamien zurückzukehren. Während dieses zweiten Aufenthalts in Ur lernte sie auch den 14 Jahre jüngeren Archäologen Max Mallowan kennen, der als Grabungsassistent bei Woolley arbeitete und bei ihrem ersten Besuch wegen einer Blinddarmentzündung abwesend gewesen war. Mallowan wurde nunmehr von den Woolleys „abkommandiert“, Christie die Ausgrabungen und die Gegend zu zeigen. Bei dieser Gelegenheit verliebten sich die beiden. Agatha Christie musste sehr bald (noch im Frühjahr 1930) wegen einer Erkrankung ihrer Tochter nach England zurückkehren, Max Mallowan begleitete sie auf dieser Rückfahrt bereits. Zögerlich nahm Agatha schließlich den Heiratsantrag des viel jüngeren Mallowan an und sie heirateten am 11. September 1930 in Edinburgh.

1930 hatte im Roman "Mord im Pfarrhaus" (engl. "The Murder at the Vicarage") eine neue Detektivin ihren ersten Auftritt: Die altjüngferliche Miss Marple, die noch in zwölf weiteren Kriminalromanen und einigen Kurzgeschichten Christies die Hauptrolle übernehmen sollte. Viele der zahlreichen Romane, die in den Jahren bis 1958 entstanden, schrieb Christie während der archäologischen Expeditionen mit ihrem Mann im Nordirak und in Nordsyrien. Ihre Erlebnisse auf einer der Expeditionen schildert sie in "Erinnerung an glückliche Tage" (engl. "Come, tell me how you live").

Von den existenzbedrohenden Ereignissen nach der Trennung von ihrem ersten Mann geprägt, schrieb Christie in den 1940er-Jahren zwei Kriminalromane, die sie für die spätere Veröffentlichung zurückhielt. "Vorhang", Hercule Poirots letzten Fall, bereitete sie zur Veröffentlichung vor, als sich abzeichnete, dass sie keinen weiteren Roman mehr würde schreiben können. Er erschien kurz vor ihrem Tod, und es ist in der Tat Poirots letzter Fall, der am Ende der Ermittlungen stirbt. Poirot war aber Agatha Christies Haupteinnahmequelle, und so war es nötig, dass er bis zum Erscheinen von "Vorhang" noch einige andere Fälle löste. "Ruhe unsanft" (engl. "Sleeping murder"), mit Miss Marple als Detektivin, war der zweite von Christie zurückgehaltene Roman und erschien erst nach ihrem Tod.

1970 erschien zu ihrem 80. Geburtstag der für Christie atypische Roman "Passenger to Frankfurt", in dem es um eine Weltverschwörung von Neonazis geht. Das umstrittene Buch wurde erst 2008 ins Deutsche übersetzt. 1971 wurde Agatha Christie von Königin Elisabeth II. als Dame Commander in den Orden des britischen Empire aufgenommen und dadurch in den persönlichen Adelsstand erhoben. Ihren letzten Roman "Alter schützt vor Scharfsinn nicht" schrieb sie zwischen 1973 und 1974.

Am 12. Januar 1976 starb Agatha Christie in Winterbrook House im Ort Wallingford, Grafschaft Oxfordshire an einem Schlaganfall. Ihr Grab befindet sich auf dem nahegelegenen Friedhof St Mary’s in Cholsey. 1977 erschien posthum Christies Autobiographie "Meine gute alte Zeit" (engl. "An Autobiography"), die größtenteils in den Jahren 1950 bis 1965 entstanden war, eine Erinnerung an Dinge, die Agatha Christie wichtig gewesen sind, mit Schwerpunkt auf ihrer Kindheit. Ergänzend zu ihrer Autobiographie kann die Biographie von Janet Morgan herangezogen werden. Agatha Christies Tochter Rosalind bat Morgan, eine autorisierte Biografie ihrer Mutter zu verfassen. Durch umfangreiches Quellenstudium und Befragung von Agathas Freunden entstand eine detaillierte Schilderung ihres Lebens.

Insgesamt schrieb Agatha Christie 66 Kriminalromane, aber auch Kurzgeschichten und Bühnenstücke. Gängige Schätzungen, nach Angaben der Erben und der Verlage, gehen von einer verkauften Gesamtauflage von über zwei Milliarden Büchern weltweit aus. Dem Index Translationum der UNESCO zufolge belegt sie mit großem Abstand Platz 1 auf der Liste der meistübersetzten Autoren. Sie gilt als die erfolgreichste Kriminalschriftstellerin der Welt. Wegen dieses Erfolges nennt man sie auch die "Queen of Crime" (dt. Die Königin des Krimis). 

Ihre berühmtesten Schöpfungen sind der belgische Detektiv Hercule Poirot und die altjüngferliche Hobbydetektivin Miss Marple. Weniger bekannt ist das Ermittlerduo Tommy und Tuppence Beresford, denen sie vier Romane und eine Kurzgeschichtensammlung widmete. Unter dem Pseudonym "Mary Westmacott" schrieb sie außerdem sechs romantische Erzählungen. 

Agatha Christie machte auch im Theater Karriere, denn aufgrund schlechter Erfahrungen beschloss sie, ihre Stücke nur noch selbst für die Bühne zu bearbeiten, und war mit Begeisterung bei der Produktion dabei. Eines ihrer Bühnenstücke ist "Die Mausefalle", das am längsten ununterbrochen aufgeführte Theaterstück weltweit.

Agatha Christie ließ zahlreiche Geschichten an realen Schauplätzen stattfinden. Am berühmtesten innerhalb dieser Gruppe ist ihr Roman "Mord im Orient-Express". Auch der Roman "Der blaue Express" spielt in einem historischen Zug. "Tod in den Wolken" spielt im ersten Teil, in dem der Mord geschieht, in einem Passagierflugzeug auf einem Flug von Paris nach London. Zwei der Romane Christies spielen in wesentlichen Passagen auf einem Passagierschiff: "Der Mann im braunen Anzug" auf einem Passagierdampfer von Southampton nach Südafrika, "Der Tod auf dem Nil" auf einem Nil-Dampfschiff für Touristen.

Für gleich drei Romane diente Agatha Christies eigener Landsitz Greenway als Kulisse: Sowohl "Kurz vor Mitternacht" als auch "Das unvollendete Bildnis" und "Wiedersehen mit Mrs. Oliver" machen sich die besondere Geographie von Greenway mit Bootsanleger, Gewächshaus, Tennisplatz, ehemaligem Geschützstand, Nähe zum Ufer des Dart zu eigen. Für ihren Roman "Alter schützt vor Scharfsinn nicht" war Agatha Christies Elternhaus Ashfield die Vorlage für den Schauplatz, wobei sie auch auf Besonderheiten aus ihrer eigenen Kindheit zurückgriff, u. a. der „KK“ (gesprochen: „Kai-Kai“) genannte Geräteschuppen, die Spielzeugpferde Truelove und Mathilde sowie eine Chilenische Araukarie.
Einige der Romane wie "Dreizehn bei Tisch" und "Bertrams Hotel" spielen in London. Auch die Figur Hercule Poirot lebt in London. Die Romane "Und dann gabs keines mehr" und "Das Böse unter der Sonne" spielen auf einer kleinen Insel in Devon: Burgh Island. Dagegen lebt die Amateur-Detektivin Miss Marple in dem fiktiven typisch englischen Dorf St. Mary Mead. Auch weitere zahlreiche Christie-Krimis spielen in englischen Dörfern oder Kleinstädten, zum Beispiel "Der ballspielende Hund" oder "Das Sterben in Wychwood". Als einziger Roman in Devon, ihrer Heimat, spielt "Das Geheimnis von Sittaford"; die unheimliche Landschaft des Dartmoor spielt hier eine besondere Rolle, auch die Stadt Exeter. "Ein Schritt ins Leere" spielt teilweise in Wales und in Hampshire, "Das Haus an der Düne" an der Küste von Cornwall.

Einige der Romane spielen im Nahen Osten, wo sich Christie häufig aufhielt, zum Beispiel "Sie kamen nach Bagdad" oder "Mord in Mesopotamien". "Der Tod wartet" spielt in Jerusalem und Transjordanien. Ägypten ist in drei Geschichten Schauplatz der Ereignisse: In "Der Tod auf dem Nil", der Kurzgeschichte "Das Abenteuer des ägyptischen Grabes" und dem Roman "Rächende Geister". Letzterer nimmt eine Sonderstellung ein, da er im alten Ägypten zur Zeit der Pharaonen spielt und nicht, wie andere Werke Christies, in ihren Zeiten des Schreibens stattfindet.

Der Roman "Karibische Affäre" ist auf der fiktiven Insel St. Honoré in der Karibik angesiedelt, wofür jedoch die Insel Barbados als Vorlage diente.

"Mord auf dem Golfplatz" ist Christies einziger Roman, der komplett in Frankreich, und zwar an der französischen Kanalküste und in Paris, spielt. In anderen Romanen wird der Schauplatz teilweise für Reisen der Ermittler nach Frankreich verlegt, so in "Die Memoiren des Grafen", der in einigen Kapiteln in Paris und Dinard spielt. Große Teile des Romans "Der blaue Express" spielen ebenfalls in Frankreich, vor allem an der Côte d’Azur.

Von Kritikern ihres Werks wird Christie des Öfteren Antisemitismus vorgeworfen. Vor allem in ihrem Frühwerk gibt es darauf Hinweise. So wird das Mordopfer Carlotta Adams in "Dreizehn bei Tisch" (1933) als geldgierige Jüdin, die selber an ihrem Tod schuld sei, porträtiert.


Agatha Christie schrieb 66 Romane, zahlreiche Kurzgeschichten, zwei Autobiographien, mehrere Lyriksammlungen und 23 Bühnenstücke. Diese wurden in fünf Hörspielen, 22 Kinofilmen, 76 Fernsehfilmen, 19 Zeichentrickfilmen sowie in einigen Computerspielen adaptiert. Vier Dokumentationen wurden über sie gedreht.





</doc>
<doc id="207" url="https://de.wikipedia.org/wiki?curid=207" title="Apollo-Programm">
Apollo-Programm

Das Apollo-Programm war ein Raumfahrt-Projekt der USA. Es brachte zum ersten und bislang einzigen Mal Menschen auf den Mond. Das Programm wurde von der National Aeronautics and Space Administration (NASA) zwischen 1961 und 1972 betrieben.

In mehreren Schritten erprobte die NASA Techniken, die für eine Mondlandung wichtig sein würden, wie z. B. das Navigieren und Koppeln von Raumschiffen im All oder das Verlassen eines Raumschiffs im Raumanzug. Viele wichtige Tests wurden in der Vorbereitung im Gemini-Programm durchgeführt. Die erste bemannte Mondlandung selbst fand dann am 20. Juli 1969 statt. Nach fünf weiteren Landungen wurde das Programm 1972, auch aus Kostengründen, eingestellt. Seitdem hat kein Mensch wieder den Mond betreten.

Im Juli 1960, noch bevor das Mercury-Programm erste Erfolge aufzuweisen hatte, fand in Washington eine Konferenz statt, auf der die NASA und verschiedene Industriebetriebe einen Langzeitplan für die Weltraumfahrt erarbeiteten. Geplant war eine bemannte Mondumrundung, von einer Landung war zu diesem Zeitpunkt noch nicht die Rede. Abe Silverstein, der Leiter der Raumfahrt-Entwicklung bei der NASA, schlug für dieses Projekt den Namen "Apollo" vor, um so die NASA-Mission mit der Sonnenfahrt des griechisch-römischen Gottes gleichzusetzen.

Die Konfiguration des Mondfluges war zunächst unklar. Die ersten Planungen der 1960er Jahre sahen ein einziges Raumschiff für die Landung auf dem Mond und die Rückkehr zur Erde vor, da unklar war, ob ein Rendezvousmanöver und die Kopplung zweier Raumfahrzeuge möglich wären. Genauere Studien gingen von vier möglichen Strategien aus:
Das letzte Konzept wurde als erstes verworfen. Es zeigte sich auch bald, dass die Pläne für einen Direktflug unrealistisch waren, da das dafür nötige Trägersystem noch um ein Vielfaches größer als die Saturn V hätte sein müssen. Auch das EOR-Konzept, das eine Vielzahl von Raketen erfordert hätte (man sprach von bis zu 15 Starts pro Mondflug), war mit Mehraufwand und Kosten verbunden. Insbesondere auf Betreiben von John C. Houbolt, der die anfängliche Minderheitsmeinung LOR hartnäckig und ohne Rücksicht auf Hierarchien vertrat, ging man daher Ende 1961 zu einer komplexeren, aber optimierten Konfiguration aus getrennten Raumfahrzeugen über. Dies ermöglichte nicht nur, mit einer einzigen Rakete auszukommen, sondern erlaubte auch die Optimierung der einzelnen Komponenten auf ihren genauen Zweck.

Der eigentliche NASA-Plan sah sieben Missionen bis zur ersten bemannten Mondlandung vor. Dies waren die Missionen A bis G:


Die mit Apollo 8 durchgeführte erste Mondumkreisung, Weihnachten 1968, war von der NASA eigentlich "nicht" vorgesehen und wurde mit der Bezeichnung Mission C' zwischen die Missionen C und D eingeschoben.

Zusätzlich wurden die Missionen H, I und J geplant:


Das Apollo-Programm kostete 23,9 Milliarden Dollar, etwa 120 Milliarden nach heutigen Maßstäben (2009), und beschäftigte bis zu 400.000 Menschen.

Durch den Start von Sputnik 1 im Jahre 1957, die erste unbemannte harte Mondlandung 1959 durch Lunik-2 und den ersten bemannten Raumflug von Juri Gagarin 1961 war die Sowjetunion zu Beginn des Raumfahrtzeitalters zur führenden Raumfahrtnation aufgestiegen. Die US-Amerikaner suchten nach einem Gebiet der Raumfahrt, auf dem sie die Sowjetunion schlagen könnten. Die bemannte Mondlandung wurde dafür als geeignet angesehen.

Am 25. Mai 1961, nur eineinhalb Monate nach dem Start von Juri Gagarin, hielt Präsident John F. Kennedy vor dem amerikanischen Kongress eine berühmte Rede, in der er das Ziel vorgab, noch im selben Jahrzehnt einen Menschen zum Mond und wieder zurückbringen zu lassen. Mit den folgenden Worten fiel der Startschuss für das Apollo-Programm:

Obwohl ursprünglich noch weitere Starts geplant waren, wurde das Apollo-Programm nach der sechsten erfolgreichen Mondlandung von Apollo 17 beendet.

Für den bemannten Mondflug wurde die bis heute größte Rakete entwickelt. Sie erhielt den Namen Saturn V. Maßgeblichen Anteil an ihrer Entwicklung hatte der deutschstämmige Raketenbauer Wernher von Braun, dessen Team die erste Stufe mit den gewaltigen F-1-Triebwerken entwickelte. Alle Starts dieser Rakete waren trotz ihrer großen Leistung und Komplexität erfolgreich, was durchaus beachtenswert ist, da die meisten übrigen Raketensysteme auch Fehlstarts zu verzeichnen hatten.

Als Vorbereitung auf die Mondlandung lief parallel zum Apollo-Programm das Gemini-Programm, mit dem Erfahrungen zu Rendezvous-Manövern, Navigation und Arbeiten im Weltall gesammelt werden sollten. Technologien für die Hitzeschilde der Apollo-Kapseln wurden im Rahmen des FIRE-Projekts entwickelt und getestet.

Am 27. Januar 1967 erlitt das Apollo-Programm einen schweren Rückschlag. Bei Bodentests verbrannten die drei Astronauten Virgil Grissom, Edward H. White und Roger B. Chaffee in ihrer Kommandokapsel. Die Rakete war während dieser Tests nicht betankt. Die Kommandokapsel war aber nicht mit gewöhnlicher Luft, sondern mit reinem Sauerstoff bei atmosphärischem Überdruck gefüllt. Dadurch wurde binnen weniger als einer Minute aus einem kleinen elektrischen Funken ein Feuer, das die Astronauten tötete. Umfangreiche Änderungen an der Kommandokapsel waren die Folge. Dem Test wurde nachträglich die Bezeichnung "Apollo 1" verliehen.

Trotzdem konnte mit der erfolgreichen Mondlandung von Apollo 11 am 20. Juli 1969 das Ziel Kennedys der Landung auf und der sicheren Rückkehr vom Mond erreicht werden.

Gleichzeitig zu dem Apollo-Programm arbeitete auch die Sowjetunion an einem ähnlichen Programm, das ebenfalls die Landung von Menschen auf dem Mond zum Ziel hatte. Mit den Zond-Sonden wurden modifizierte Sojus-Raumschiffe unbemannt zum Mond gestartet und nach einem Mondumlauf wieder zur Erde gebracht. Dies diente dem Test des Raumschiffs, das für einen folgenden bemannten Mondflug gedacht war. Zond-5 umkreiste im September 1968 den Mond, kam jedoch bei der Rückkehr vom Kurs ab und musste aus dem Indischen Ozean geborgen werden, die Landung war eigentlich für das sowjetische Territorium geplant. Im Oktober 1970 wurde das Testprogramm mit Zond-8 beendet; von allen Missionen war nur Zond-7 eine Mission, bei der Menschen an Bord des Raumschiffs überlebt hätten.

Parallel arbeitete die Sowjetunion auch an einer Rakete für eine Mondlandemission, die ähnlich wie bei Apollo mit einer einzigen Rakete gestartet werden sollte. Dafür wurde die N1-Rakete entwickelt. Diese ist jedoch bei allen vier Teststarts, die zwischen 1969 und 1972 erfolgten, vor dem Erreichen einer Erdumlaufbahn explodiert. Daraufhin und angesichts der Tatsache, dass die US-Amerikaner bereits erfolgreich auf dem Mond gelandet waren, gab die Sowjetunion das bemannte Mondprogramm auf. Erst Anfang der 1990er-Jahre, nach dem Ende der Sowjetunion, kamen detaillierte Informationen über dieses Programm und die N1-Rakete an die Öffentlichkeit.

Die ersten beiden Menschen landeten im Rahmen der Mission Apollo 11 am 20. Juli 1969 um 21:17 Uhr (MEZ) auf dem Mond: Neil Armstrong und Edwin Aldrin. Sechs Stunden später, am 21. Juli um 03:56:20 Uhr MEZ, betrat Neil Armstrong im Mare Tranquillitatis als erster Mensch den Mond. Dabei sprach er den berühmt gewordenen Satz:

Das „a“ vor „man“ wurde in späteren Texten hinzugefügt, um den Sinn zu erhalten. Im Funkverkehr war es nicht zu hören gewesen. Armstrong wurde später danach befragt, ob er es tatsächlich nicht gesagt habe, aber er konnte sich nicht mehr daran erinnern. Daher bleibt es ungeklärt, ob es durch Störungen im Funkverkehr verloren gegangen ist oder ob Armstrong dies tatsächlich so gesagt hat.

Der dritte Astronaut, Michael Collins, umkreiste im Apollo-Mutterschiff den Erdtrabanten bis zur Rückkehr der Landeeinheit Eagle.

Im Rahmen des Apollo-Programms wurden insgesamt sechs Mondlandungen durchgeführt. Damit haben bis heute 12 Menschen, allesamt US-Amerikaner, den Mond betreten. Harrison H. Schmitt – Mondfährenpilot von Apollo 17 – setzte als bislang letzter Mensch am 12. Dezember 1972 seinen Fuß auf den Mondboden. Eugene Cernan – Kommandant von Apollo 17 – ist bislang der letzte Mensch, der auf dem Mond war, indem er als letzter in die Mondfähre einstieg. Im Juli 2009 übermittelte die Mondsonde Lunar Reconnaissance Orbiter Aufnahmen der Landestellen von Apollo 11, 14, 15, 16 und 17.

Als Routineflug gestartet und von der Öffentlichkeit kaum wahrgenommen, starteten mit der Mission Apollo 13 am 11. April 1970 die Astronauten James A. Lovell, John L. Swigert und Fred W. Haise. Erst als auf dem Weg zum Mond ein Tank mit flüssigem Sauerstoff explodierte und damit das Leben der drei Insassen des Apollo-Raumschiffs stark gefährdet war, wurde die gesamte Weltöffentlichkeit auf die Mission aufmerksam. Die Astronauten konnten sich nur dadurch retten, dass sie das Lunar Module als „Rettungsboot“ zweckentfremdeten. Damit war an eine Mondlandung nicht mehr zu denken.

Da das Raumschiff zum Zeitpunkt des Unfalls schon die Erdumlaufbahn in Richtung Mond verlassen hatte und für eine sofortige direkte Umkehr der Treibstoff bei weitem nicht ausgereicht hätte, führte der einzige Weg zurück zur Erde um den Mond herum, wobei das Raumschiff durch ein Swing-by-Manöver mit Hilfe der Mondanziehung wieder in Richtung Erde beschleunigt wurde. Nach dem Absprengen des Servicemoduls kurz vor dem Wiedereintritt in die Erdatmosphäre wurde erst das gesamte Ausmaß der Havarie ersichtlich; man geht davon aus, dass der Sauerstofftank der Brennstoffzellen explodiert war. Trotz der gescheiterten Mondlandung wird Apollo 13 dennoch als Erfolg gewertet, weil es erstmals gelungen war, Astronauten aus einer katastrophalen Raumnotlage lebend zur Erde zurückzubringen. Nach fünf Tagen, die für die Astronauten und die Bodenmannschaften sehr anstrengend waren, gelang (nach einer Mondumrundung ohne Landung) am 17. April 1970 die Landung im Pazifik. Kurioserweise waren die Mitglieder der Besatzung von Apollo 13 durch diese Mondumrundung ohne Landung diejenigen Menschen, die bislang am weitesten von der Erde entfernt waren, wenngleich ungeplant.

Davon handelt der 1995 gedrehte Film "Apollo 13" mit Tom Hanks in der Rolle von James Lovell.


Kurz nach der erfolgreichen Mondlandung von Apollo 11 veröffentlichte die NASA die weitere Planung, die bis Ende 1972 neun weitere Apolloflüge vorsah. Doch bereits im Januar 1970, noch vor der Verzögerung durch die Panne von Apollo 13, wurde Apollo 20 aus Kostengründen gestrichen. Im September 1970 wurden auch die ursprüngliche Apollo-15-Mission sowie Apollo 19 eingespart. Die nicht aus dem Programm gestrichenen Missionen Apollo 16, Apollo 17 und Apollo 18 wurden danach in Apollo 15, Apollo 16 und Apollo 17 umbenannt.

Die nach dem Abschluss der Mondflüge noch vorhandenen Apollo-Raumschiffe und Saturnraketen wurden für das Skylab-Projekt 1973/74 und das Apollo-Sojus-Test-Projekt 1975 verwendet.

Dem Apolloprogramm wird vielfach ein zu geringer wissenschaftlicher Nutzen vorgeworfen. Das Ex-Missionmitglied William Anders meint, Apollo sei „kein wissenschaftliches Programm“ gewesen, in Wahrheit habe es sich um eine „Schlacht im Kalten Krieg“ gehandelt. „Sicherlich, wir haben ein paar Gesteinsbrocken gesammelt und ein paar Fotos gemacht, aber wäre da nicht dieser Wettlauf mit den Russen gewesen, hätten wir niemals die Unterstützung der Steuerzahler gehabt.“ Nach dem Erfolg von Apollo 11 kündigten einige Forscher bei der NASA, darunter der damalige NASA-Chefgeologe Eugene Shoemaker. Er vertrat den Standpunkt, dass der wissenschaftliche Ertrag durch unbemannte Sonden zu einem Fünftel der Kosten und bereits 3 bis 4 Jahre früher hätte erbracht werden können.

Wie bei vielen Ereignissen von so großer politischer Tragweite wurden auch die Mondlandungen zum Objekt zahlreicher Verschwörungstheorien. Diese gehen davon aus, dass die Mondlandungen in den Jahren 1969 bis 1972 nicht stattgefunden haben (oft geht es auch nur um die erste bemannte Mondlandung), sondern von der NASA und der US-amerikanischen Regierung vorgetäuscht worden sind. Die Verschwörungstheorien haben seit den 1970ern, durch den Autor Bill Kaysing, jedoch verstärkt wieder seit 2001, Verbreitung gefunden. Keine der Verschwörungstheorien liefert einen nachvollziehbaren, wissenschaftlich haltbaren Zweifel an den erfolgten Mondlandungen.






</doc>
<doc id="208" url="https://de.wikipedia.org/wiki?curid=208" title="April">
April

Der April (von lateinisch Aprilis) ist der vierte Monat des Jahres im gregorianischen Kalender. Er hat 30 Tage und beginnt mit demselben Wochentag wie der Juli und in Schaltjahren auch wie der Januar. Im römischen Kalender war der Aprilis ursprünglich der zweite Monat, weil mit dem Ende des Winters im März das neue landwirtschaftliche (aber auch militärische) Jahr begann.

Es gibt keine gesicherte Herleitung des Namens. Da die Namen der ersten Jahreshälfte Götter wiedergeben, könnte es von Aphrodite stammen, die als Göttin für Liebe zu April passen würde, auch wenn der römische Name Venus gewesen wäre. Der Name bezieht sich möglicherweise auch auf die sich öffnenden Knospen im Frühling und wäre dann, ebenso wie die auf Vegetation bezogene Deutung des Aprils als „der die Erde öffnende Monat“, vom Lateinischen "aperire" („öffnen“) herzuleiten. Eine andere Etymologie sieht "apricus" („sonnig“) als Ursprung des Wortes. Zur Regierungszeit Kaiser Neros wurde der Monat ihm zu Ehren in "Neroneus" umbenannt, was sich allerdings nicht durchsetzte. Unter Kaiser Commodus hieß der Monat dann "Pius", einer der Namen des Kaisers, auch diese Umbenennung wurde nach seinem Tod wieder rückgängig gemacht. Der alte deutsche Name, der durch Karl den Großen im 8. Jahrhundert eingeführt wurde, ist "Ostermond", später auch "Ostermonat" genannt, weil Ostern meist im April liegt. Andere, heute kaum mehr gebräuchliche Bezeichnungen sind "Wandelmonat", "Grasmond" oder auch "Launing". 

Der Legende nach wurde Luzifer am 1. April aus dem Himmel verstoßen.

Seit dem 16. Jahrhundert ist in Europa der Brauch belegt, am 1. April einen Aprilscherz zu begehen, indem man seine Mitmenschen mit einem mehr oder weniger derben Scherz oder einer Lügengeschichte „in den April schickt“. Daher stammen auch die folgenden beiden Sprichwörter:

Aprilwetter steht bildlich für wechselhaftes Wetter, auch wenn es in anderen Monaten stattfindet:



</doc>
<doc id="209" url="https://de.wikipedia.org/wiki?curid=209" title="August">
August

Der August "(Erntemonat, Ährenmonat, Sichelmonat, Ernting;" ")" ist der achte Monat des Jahres im gregorianischen Kalender.

Der August hat 31 Tage und wurde im Jahre 8 v. Chr. nach dem römischen Kaiser Augustus benannt, da er in diesem Monat sein erstes Konsulat angetreten hat. Unter Kaiser Commodus wurde der Name des Monats ihm zu Ehren in "Commodus" geändert, nach dem Tod des Kaisers erhielt der Monat seinen alten Namen zurück.

Im römischen Kalender war der Augustus ursprünglich der sechste Monat und hatte vor seiner Umbenennung den Namen "Sextilis" (lat. ‚der sechste‘). Im Jahr 153 v. Chr. wurde der Jahresbeginn allerdings auf den 1. Januar verlegt.

Der Sextil hatte ursprünglich 29 Tage und bekam durch Julius Caesars Reform 31 Tage. Die Reihenfolge der Tagesanzahl der folgenden Monate September, Oktober, November und Dezember (31 und 30 Tage) wurde umgekehrt, da andernfalls drei Monate (Juli bis September) mit je 31 Tagen unmittelbar aufeinander gefolgt wären. Unter Kaiser Augustus wurde der Monat Sextilis dann zu Ehren des Kaisers in Augustus umbenannt. Die oft zu hörende Behauptung, der Monat August wäre in Caesars ursprünglichem Reformkalender nur 30 Tage lang gewesen und wäre nur deshalb auf 31 Tage verlängert worden, um dem nach Julius Caesar benannten Monat Juli nicht nachzustehen, hat sich als Legende erwiesen.

Der August beginnt in Schaltjahren mit dem gleichen Wochentag wie der Februar. In Gemeinjahren beginnt jedoch kein anderer Monat mit demselben Wochentag wie der August. Der männliche Vorname August wird im Gegensatz zum Monatsnamen auf der ersten Silbe betont.

Der Bundesfeiertag am 1. August ist als Schweizer Nationalfeiertag ein gesetzlicher Feiertag.

Maria Himmelfahrt am 15. August ist in ganz Österreich, in einigen Kantonen der Schweiz, im Saarland und in den überwiegend katholischen Gemeinden Bayerns ein gesetzlicher Feiertag. Er ist auch als Staatsfeiertag der Nationalfeiertag von Liechtenstein.

Zudem ist das Friedensfest am 8. August in der Stadt Augsburg ein gesetzlicher Feiertag. Im übrigen deutschsprachigen Raum ist der August ohne Feiertage.



</doc>
<doc id="210" url="https://de.wikipedia.org/wiki?curid=210" title="Apollo">
Apollo

Apollo steht für:

Raumfahrt:

Technik:

Kraftfahrzeuge:

Unternehmen und Marken:

Geographische Objekte:

Orte in Südafrika:

Orte in den Vereinigten Staaten:
APOLLO steht als Abkürzung für:
Siehe auch:


</doc>
<doc id="211" url="https://de.wikipedia.org/wiki?curid=211" title="Archimedisches Prinzip">
Archimedisches Prinzip

Das archimedische Prinzip wurde vor über 2000 Jahren von dem griechischen Gelehrten Archimedes formuliert. Es lautet:

"Der statische Auftrieb eines Körpers in einem Medium ist genauso groß wie die Gewichtskraft des vom Körper verdrängten Mediums."
Das archimedische Prinzip gilt in allen Fluiden, d. h. in Flüssigkeiten und Gasen. Schiffe verdrängen Wasser und erhalten dadurch Auftrieb. Da die mittlere Dichte eines Schiffes geringer als die Dichte von Wasser ist, schwimmt es an der Oberfläche. Auch Ballone und Luftschiffe machen sich diese Eigenschaft zunutze, um fliegen zu können. Dazu werden sie mit einem Gas befüllt, dessen Dichte geringer ist als die der umgebenden Luft. Diese Gase (z. B. Helium oder Wasserstoff) sind bei vielen Luftschiffen und Ballonen von Natur aus weniger dicht als Luft; in Heißluftballons und Heißluft-Luftschiffen wird die Luftfüllung mit Hilfe von Gasbrennern erwärmt, wodurch ihre Dichte abnimmt.

Die Ursache für die Auftriebskraft liegt darin, dass der hydrostatische Druck an der Oberseite bzw. der Unterseite eines eingetauchten Körpers unterschiedlich ist. Aus diesem Druckunterschied resultieren unterschiedlich große Kräfte auf Unter- und Oberseite des eingetauchten Körpers, auf die Unterseite wirkt eine größere Kraft als auf die weiter oben befindlichen Teile der Oberfläche. 

Im Beispiel (Bild 1) gehen wir von einem Würfel mit 20 cm Kantenlänge aus. Er ist 10 cm tief unter die Wasseroberfläche eingetaucht. 

Der Druck, den 1 m Wassersäule erzeugt, beträgt formula_1.

Nach Archimedes gilt Folgendes:
formula_2.
Bezogen auf das Beispiel (Bild 1) können wir schreiben:

Dabei wurde die Dichte formula_4 des Fluids, die Beziehung formula_5 zur Masse formula_6 und zum Volumen formula_7, und der Ortsfaktor formula_8 verwendet. Wir sehen, dass beide Methoden zum selben Ergebnis führen.

Folgendes Gedankenexperiment veranschaulicht die Richtigkeit des archimedischen Prinzips. Dazu stelle man sich ein ruhendes Fluid vor. Innerhalb des Fluids sei ein beliebiger Teil des Fluids markiert. Die Markierung kann man sich wie eine Art Wasserballon in einem Behälter Wasser vorstellen, nur dass die Haut dieses Wasserballons unendlich dünn und massenlos ist und eine beliebige Form annehmen kann.

Man stellt nun fest, dass der so markierte Teil des Fluids innerhalb des Fluids weder steigt noch sinkt, da sich das gesamte Fluid in Ruhe befindet – der markierte Teil schwebt sozusagen schwerelos im ihn umgebenden Fluid. Das bedeutet, dass die Auftriebskraft des markierten Fluidteils exakt sein Gewicht kompensiert. Daraus kann gefolgert werden, dass die Auftriebskraft des markierten Fluidteils genau seiner Gewichtskraft entspricht. Da die Markierung innerhalb des Fluids beliebig ist, ist somit die Richtigkeit des archimedischen Prinzips für homogene Fluide gezeigt.

Damit der Körper die in der Grafik beschriebene Position beibehält, muss seine Gewichtskraft gleich der Gewichtskraft des verdrängten Wassers (78,48 N) sein. Dann heben sich alle auf den Körper wirkenden Kräfte auf und dieser kommt zum Stillstand.
Nach der Formel formula_9 muss der Körper 8.000 g schwer sein. Des Weiteren hätte er nach formula_5 eine Dichte von 1 kg/dm, also die Dichte von Wasser.

Wir können also folgende Regel formulieren:
Die Körper steigen oder sinken, bis der Gewichtskraft eine betragsmäßig gleich große Kraft entgegenwirkt. Dies kann beim Sinken eine sich ändernde Dichte des Fluids oder auch der Boden des Bechers bewirken. Ein Körper steigt oft so lange, bis er die Oberfläche durchbricht. In diesem Fall gilt: formula_14.

Archimedes war von König Hieron II. von Syrakus beauftragt worden, herauszufinden, ob dessen Krone wie bestellt aus reinem Gold wäre oder ob das Material durch billigeres Metall gestreckt worden sei. Diese Aufgabe stellte Archimedes zunächst vor Probleme, da die Krone natürlich nicht zerstört werden durfte.

Der Überlieferung nach hatte Archimedes schließlich den rettenden Einfall, als er zum Baden in eine bis zum Rand gefüllte Wanne stieg und dabei das Wasser überlief. Er erkannte, dass die Menge Wasser, die übergelaufen war, genau seinem Körpervolumen entsprach. Angeblich lief er dann, nackt wie er war, durch die Straßen und rief "„Heureka!“ („Ich habe es gefunden“)".

Um die gestellte Aufgabe zu lösen, tauchte er einmal die Krone und dann einen Goldbarren, der genauso viel wog wie die Krone, in einen bis zum Rand gefüllten Wasserbehälter und maß die Menge des überlaufenden Wassers. Da die Krone mehr Wasser verdrängte als der Goldbarren und somit bei gleichem Gewicht voluminöser war, musste sie aus einem Material geringerer Dichte, also nicht aus reinem Gold, gefertigt worden sein.

Diese Geschichte wurde vom römischen Architekten Vitruv überliefert.

Obwohl der Legende nach auf dieser Geschichte die Entdeckung des archimedischen Prinzips beruht, würde der Versuch von Archimedes auch mit jeder anderen Flüssigkeit funktionieren. Das Interessanteste am archimedischen Prinzip, nämlich die Entstehung des Auftriebs und damit die Berechnung der Dichte des Fluids, spielt in dieser Entdeckungsgeschichte gar keine Rolle.

Ein Körper wird vom Druck formula_15 belastet, welchen das umgebende Medium (Flüssigkeit oder Gas) auf seine Oberfläche ausübt. Ein betrachtetes Teilstück der Oberfläche mit dem Inhalt formula_16 sei so klein gewählt, dass es praktisch eben ist und dass in seinem Bereich der Druck formula_17 konstant ist. Der Einheitsvektor der äußeren Flächennormale der Teilfläche sei formula_18. Das Medium übt dann die Kraft

auf das Teilstück aus. Eine Summierung dieser Kräfte über alle Teilstücke liefert die gesamte Auftriebskraft.

Das archimedische Prinzip gilt nur genau dann streng, wenn das verdrängte Medium inkompressibel (nicht zusammendrückbar) ist. Für Flüssigkeiten wie z. B. Wasser ist dies gut erfüllt, daher soll im Folgenden von einem Körper ausgegangen werden, der in eine Flüssigkeit der (genau genommen von der Temperatur abhängigen) Dichte formula_20 eintaucht.

In der Flüssigkeit lastet auf einer waagerechten Fläche der Größe formula_16 in der Tiefe formula_22 das Gewicht einer Flüssigkeitssäule der Masse formula_23. Der Druck in dieser Tiefe ist deshalb

Ein entsprechender Druckverlauf gilt bei nicht zu großen Höhendifferenzen formula_22 auch in der Luft oder anderen Gasen (d. h. die Kompressibilität fällt nicht ins Gewicht; bei großen Höhenunterschieden müsste eine veränderliche Dichte berücksichtigt werden). Deshalb gelten die folgenden Überlegungen auch für realistisch große Luftschiffe oder Ballone.

Für einfache geometrische Formen kann man die Gültigkeit des archimedischen Prinzips mit einfachen Mitteln von Hand nachrechnen. Für einen Quader mit Grundfläche formula_16 und Höhe formula_27, der senkrecht in die Flüssigkeit eintaucht, erhält man beispielsweise:

Dabei ist formula_7 das verdrängte Volumen, also formula_34 die verdrängte Masse und formula_35 ihre Gewichtskraft. Das archimedische Prinzip ist also erfüllt. Das negative Vorzeichen entfällt, wenn die formula_22-Achse nach oben gewählt wird.

Für einen beliebig geformten Körper formula_37 erhält man die gesamte Auftriebskraft durch das Oberflächenintegral

Mit dem Integralsatz 
und formula_40 folgt daraus



</doc>
<doc id="212" url="https://de.wikipedia.org/wiki?curid=212" title="Ergotismus">
Ergotismus

Ergotismus oder Mutterkornvergiftung (früher auch Ignis sacer „heiliges Feuer“; von lateinisch "sacer": ‚heilig‘, auch ‚verflucht‘, ‚abscheulich‘) ist eine Mykotoxikose und bezeichnet die Symptomatik einer Vergiftung durch Mutterkornalkaloide wie zum Beispiel Ergotamin oder Ergometrin.

Im Mittelalter trat Ergotismus als Folge des Verzehrs von Nahrungsmitteln auf, die mit Mutterkorn – einer länglichen, kornähnlichen Dauerform (Sklerotium) des Mutterkornpilzes "Claviceps purpurea" – verunreinigt waren. Da die Gefahr, die von Mutterkorn ausgeht, heute bekannt ist, werden verschiedene Maßnahmen ergriffen, um einer Verunreinigung von Getreideprodukten entgegenzuwirken. Der Ergotismus entsteht in der heutigen Zeit daher meist durch die Einnahme von Medikamenten, die Mutterkornalkaloide und deren Derivate enthalten. Diese Medikamente finden noch in der Therapie und Prophylaxe der Migräne (z. B. Ergotamin und Dihydroergotamin), in der Geburtsmedizin (Methyl- und Ergometrin) und in der Behandlung der Parkinson-Krankheit (z. B. Bromocriptin, Pergolid, Cabergolin oder Dihydroergocryptin) Anwendung. Eine unkontrollierte Dosissteigerung kann dabei zu Ergotismus führen.

Durch eine Vergiftung mit Ergotamin kommt es zu einer massiven Verengung der Blutgefäße und in der Folge zu einer Durchblutungsstörung von Herzmuskel, Nieren und Gliedmaßen. Die Gliedmaßen sind kalt und blass, der Puls ist meist kaum nachweisbar. Zudem bestehen Hautkribbeln (Parästhesie), Empfindungsstörungen (Hypästhesie) und eventuell Lähmungserscheinungen (Parese). Eine häufige Folge ist das sekundäre (induzierte) Raynaud-Syndrom oder die Steigerung in Form eines schmerzhaften Absterbens von Fingern und Zehen (Gangrän und Nekrosen bei "Ergotismus gangraenosus", dem „Mutterkornbrand“). Zusätzlich bestehen in der Regel Allgemeinsymptome wie Erbrechen, Verwirrtheit, Wahnvorstellungen, Kopfschmerzen, Ohrensausen und Durchfall. Akute Vergiftungen können durch Atem- oder Herzstillstand zum Tod führen, chronische Vergiftungen zum Verlust der mangelhaft durchbluteten Gliedmaßen, Sekundärinfektionen und darauffolgende Sepsis.

Wichtigstes diagnostisches Kriterium ist das Erkennen der Ergotamineinnahme. Die Anamnese und dabei insbesondere die Medikamentenanamnese ist daher meistens entscheidend. Apparative Untersuchungen können bei Bedarf ergänzend hinzugezogen werden, beispielsweise die Doppler-Sonographie der Extremitätengefäße.

Auslösende Medikamente sind als Erstmaßnahme sofort abzusetzen. Ist dies allein nicht ausreichend, können die Blutgefäße durch die Gabe von Nitraten, Calciumantagonisten und/oder Prostaglandininfusionen weitgestellt werden (Vasodilatation).

Der Ergotismus besitzt eine ganze Reihe zumeist regionaler Bezeichnungen, wie "Antoniusfeuer", "Sankt-Antonius-Rache", "(St.-)Antonius-Plage", "Kriebelkrankheit.", "Magdalenenflechte" (Spanien), "Muttergottesbrand" (Westfalen), "Mutterkornbrand", "St. Antonius-Feuer", "St. Johannis-Fäule" (Böhmen) oder "St. Martialis-Feuer".

In der Antike wurde vorwiegend Weizen angebaut, so dass keine Vergiftungen durch Ergotalkaloide bekannt sind, da die Erkrankung nur durch den Konsum von mit Mutterkorn-Pilz "(Claviceps purpurea)" befallenem Roggen verursacht wird. Der erste belegte, epidemieartige Fall von Ergotismus trat im Jahr 857 bei Xanten auf. 943 sollen europaweit – vorwiegend in Frankreich und Spanien – etwa 40.000 Menschen einer Mutterkornepidemie zum Opfer gefallen sein. Man bezeichnete die Erkrankung als "Antoniusfeuer" (benannt nach dem heiligen Antonius) oder auch "ignis sacer" „heiliges Feuer“, wobei unter diesen und ähnlichen Bezeichnungen auch andere, vor allem mit geschwürigem Gewebszerfall der Extremitäten verbundene Erkrankungen oder Symptome (Phlegmone, Erysipel, Herpes zoster) verstanden wurden.

Vor allem der Antoniter-Orden hatte es sich zur Aufgabe gemacht, am Antoniusfeuer Erkrankte zu behandeln und zu pflegen. Die Antoniter unterhielten im 15. Jahrhundert in ganz Europa etwa 370 Spitäler, in denen rund 4000 Erkrankte versorgt wurden. Die Krankheit war derart gefürchtet, dass Prozessionen und Zeremonien zu ihrer Abwehr zelebriert wurden. Noch heute wird auf Sardinien alljährlich im Januar das „Focolare di Sant’ Antonio“ (Antoniusfeuer) zur Abwehr von Krankheiten und anderen Übeln gefeiert.

Trotz des bereits in der Antike bekannten Zusammenhangs von mit Pilzen oder Fäulnis befallenem Getreide und epidemisch auftretenden Krankheiten sowie deutlicher Hinweise auf einen Zusammenhang zwischen der Verwendung von mutterkornhaltigem Mehl und dem Auftreten von Ergotismus im Mittelalter wurden erst nach neuerlichen Epidemien 1716–1717 in Dresden sowie in den Jahren 1770 und 1777 in ganz Europa gesetzgeberische Maßnahmen ergriffen. Nachdem um 1853 durch den Mykologen L. R. Tulasne der Entwicklungszyklus des Mutterkornpilzes "Claviceps purpurea" aufgeklärt und beschrieben worden war, extrahierte Charles Tanret 1875 aus Mutterkorn eine – allerdings ziemlich verunreinigte – Substanz, die er „Ergotinin“ nannte. Ebenso wie das „Ergotoxin“, das 1907 entdeckt wurde, ist es ein Gemisch verschiedener Ergotalkaloide. Erst Arthur Stoll isolierte 1918 mit Ergotamin das erste reine Mutterkornalkaloid.

Im 19. Jahrhundert gehörten Mutterkorn-Massenvergiftungen größtenteils der Vergangenheit an und seitdem in Europa nur noch hinreichend gereinigtes Getreide verzehrt wird, stellt Mutterkorn dort im Allgemeinen keine Gefahr mehr für die Gesundheit der Menschen dar. Es gab aber vereinzelt auch noch im 20. Jahrhundert Fälle von Vergiftungen. In den Jahren 1926 und 1927 kam es in der Sowjetunion zu Massenvergiftungen; offiziell gab es über 11.000 Tote durch mutterkornhaltiges Brot. Der letzte – allerdings umstrittene – Vergiftungsvorfall, mit 200 Erkrankten und sieben Toten, soll 1951 in Pont-Saint-Esprit (Frankreich) aufgetreten sein.

Da heute zunehmend ungemahlenes Getreide konsumiert wird, das direkt vom Landwirt kommt, kann es z. B. bei ungereinigtem Roggen aus Direktverkäufen zu Vergiftungen kommen. In Deutschland konnte 1985 eine Vergiftung auf mutterkornhaltiges Müsli zurückgeführt werden. Die Untersuchungsämter der Bundesländer stellten auch bei Stichproben von 2004 bis 2011 bisweilen gesundheitsschädliche Alkaloidgehalte in Getreideprodukten fest.




</doc>
<doc id="214" url="https://de.wikipedia.org/wiki?curid=214" title="Alfred Nobel">
Alfred Nobel

Alfred Bernhard Nobel [] (* 21. Oktober 1833 in Stockholm; † 10. Dezember 1896 in Sanremo, Italien) war ein schwedischer Chemiker und Erfinder. Ihm wurden insgesamt 355 Patente erteilt.

Nobel ist der Erfinder des Dynamits sowie Stifter und Namensgeber des Nobelpreises. Das chemische Element Nobelium wurde nach Nobel benannt.

Alfred Nobel war der dritte Sohn des schwedischen Ingenieurs und Industriellen Immanuel Nobel. Er hatte zwei ältere Brüder, Robert (1829–1896) und Ludvig (1831–1888), und den jüngeren Bruder Emil Oskar Nobel (1843–1864). Letzterer starb zusammen mit vier weiteren Personen am 3. September 1864 bei einem durch Experimentieren mit Nitroglyzerin verursachten Unfall in Helenenborg. Alfred Nobel war nicht im Unternehmen anwesend, als der Unglücksfall geschah.

Einer seiner Neffen war der schwedisch-russische Ölmagnat Emanuel Nobel (1859–1932), der Erbauer des ersten Dieselmotorschiffes, der "Vandal". Nobel war Ur-Ur-Urenkel des Universalgelehrten Olof Rudbeck der Ältere.

In den Jahren 1841 und 1842 besuchte Alfred Nobel eine Schule in Stockholm. 1842 kam er nach Sankt Petersburg, wo sein Vater mit Hilfe der norwegischen Regierung einige Hüttenwerke gegründet hatte und die russische Armee belieferte. Dank des Wohlstands des Vaters genoss Alfred eine erstklassige Ausbildung durch Privatlehrer. Bereits im Alter von 17 Jahren beherrschte er fünf Sprachen (Schwedisch, Russisch, Deutsch, Englisch und Französisch).

Neben seinen Chemie- und Physikstudien interessierte er sich besonders für englische Literatur. Das missfiel seinem Vater, der ihn für introvertiert hielt, weshalb er ihn ins Ausland schickte. Nobel besuchte in rascher Folge Schweden, Deutschland, Frankreich und die Vereinigten Staaten. In Paris lernte er dabei Ascanio Sobrero kennen, der drei Jahre zuvor das Nitroglycerin entdeckt hatte, es jedoch aufgrund seiner Gefährlichkeit für nicht praxistauglich hielt. 1859 kehrte er wieder mit seinem Vater nach Stockholm zurück.

Nobel zeigte sich an der Erfindung des Nitroglycerins sehr interessiert und richtete seit 1859 seine Bemühungen darauf, es als Sprengstoff in die Technik einzuführen. Zwischen 1860 und 1864 experimentierte er unter anderem im Ruhrgebiet in Dortmund-Dorstfeld auf der dortigen Zeche Dorstfeld mit Sprengstoffen im Bergbau. Um Nitroglycerin mit größerer Sicherheit sprengen zu können, entwickelte er 1863 die Initialzündung.

Bei Nobels Experimenten mit Nitroglycerin kam es zu mehreren Explosionen; bei einer Explosion 1864, bei der sein Laboratorium zerstört wurde, kamen sein Bruder Emil und vier weitere Personen ums Leben. Aufgrund der Gefährlichkeit verboten die schwedischen Behörden ihm weitere Experimente mit Nitroglycerin innerhalb Stockholms, so dass Nobel im Jahre 1865 ein Labor und Fabriken an den Vinterviken am Mälaren im Westen Stockholms verlegte. Eine ähnliche Anlage baute er in Deutschland bei Krümmel (Schleswig-Holstein) nahe Hamburg. Noch im gleichen Jahr gelang ihm die Massenproduktion von Nitroglycerin, bei der es jedoch ebenfalls zu einer Reihe schwerer Unfälle kam.

Um die Gefährlichkeit des Nitroglycerins bei gleich bleibender Sprengkraft zu verringern, experimentierte Nobel erfolglos mit verschiedenen Zusatzstoffen. Der Legende nach half schließlich der Zufall: 1866 kam es bei einem der zahlreichen Transporte von Nitroglycerin zu einem Zwischenfall, bei dem eines der Transportgefäße undicht wurde und reines Nitroglycerin auf die mit Kieselgur ausgepolsterte Ladefläche des Transportwagens tropfte. Die entstandene breiige Masse erregte die Aufmerksamkeit der Arbeiter, so dass sie diesen Vorfall später an Nobel meldeten. Diesem gelang hierdurch endlich die ersehnte Herstellung eines handhabungssichereren Detonationssprengstoffes. Nobel selbst bestritt immer, es habe sich um eine Zufallsentdeckung gehandelt. Er ließ sich das im Mischungsverhältnis von 3:1 optimierte Verfahren 1867 patentieren und nannte sein Produkt Dynamit.

Da der Bedarf an einem sichereren und trotzdem wirkungsvollen Sprengstoff zu dieser Zeit auch infolge der Blütezeit des Diamantenfiebers groß war, konnte Nobel durch seine Erfindung schnell ein Vermögen aufbauen. Seine Firmen lieferten Nitroglycerin-Produkte nach Europa, Amerika und Australien. Nobel selbst reiste ständig, um seine Produkte zu verkaufen. Er besaß über 90 Dynamit-Fabriken in aller Welt.

Neben seinen Reisen forschte Nobel auch weiterhin mit Sprengstoffen. 1875 entwickelte er die Sprenggelatine, 1887 ließ er sich das Ballistit (rauchschwaches Pulver) patentieren. Nobel bot die Erfindung erst der französischen Regierung an, die jedoch ablehnte, da sie Aussicht auf ein bereits in der Entwicklung befindliches fast rauchfreies Pulver hatte. Daraufhin bot Nobel die Erfindung den Italienern an, die diese sofort kauften.

In Frankreich wurde Nobel daraufhin in der Presse mit Spionage in Verbindung gebracht, er wurde verhaftet und seine Erlaubnis, Experimente durchzuführen, wurde ihm entzogen. Infolge dieser Ereignisse zog Nobel 1891 nach Sanremo, kaufte dort eine 1870 erbaute Villa und verbrachte hier den Rest seines Lebens.

Schon Nobels Vater war als Rüstungsunternehmer zu Wohlstand gekommen, unter anderem durch die Produktion von Seeminen, die das Russische Reich im Krimkrieg einsetzte. Alfred Nobels wichtigste Erfindungen, Dynamit und Sprenggelatine, waren entgegen weit verbreiteter Ansicht nicht zur Kriegsführung geeignet. Das rauchschwache Pulver Ballistit war allerdings eine Ausnahme. Es revolutionierte die gesamte Schusstechnik, von der Pistole bis zur Kanone. Der neue Sprengstoff war ein waffentechnischer Fortschritt im Vergleich zum Schwarzpulver.

Als Nobels Bruder Ludvig 1888 starb, druckte eine französische Zeitung versehentlich einen Nachruf auf Alfred Nobel. Die Überschrift lautete: "Le marchand de la mort est mort" („Der Kaufmann des Todes ist tot“). Nobels Reichtum wurde damit erklärt, dass er das Mittel gefunden habe, „mehr Menschen schneller als jemals zuvor zu töten“. Alfred Nobel war über diese Darstellung entsetzt und begann sich obsessiv mit der Frage zu beschäftigen, wie ihn die Nachwelt sehen würde.

Über Krieg und Frieden diskutierte er intensiv mit Bertha von Suttner. 1876 hatte sie auf eine Stellenanzeige in der Wiener Zeitung "Neue Freie Presse" geantwortet und die Stelle einer Privatsekretärin bei Nobel angenommen, sie jedoch bereits eine Woche später wieder aufgegeben. Nach einem jahrelangen Exil im heutigen Georgien wurde sie eine bedeutende Friedensaktivistin und tauschte sich mit Nobel in einem umfangreichen Briefwechsel aus. Nobel war ihrem Anliegen von vornherein gewogen und bewunderte ihr Engagement, hielt es aber für aussichtsreicher, auf Regierungen einzuwirken, statt wie die Friedensbewegung vor allem die öffentliche Meinung zu mobilisieren. Der freundschaftliche Briefwechsel beeindruckte Nobel und regte ihn zur Stiftung des Friedensnobelpreises an, mit dem 1905 auch Bertha von Suttner ausgezeichnet wurde.

1894 kaufte Nobel sogar den schwedischen Rüstungsbetrieb Bofors – obwohl er den Krieg eigentlich verabscheute. Er verband mit der Rüstungsproduktion die Hoffnung, dass die Armeen eines Tages vom Krieg Abstand nehmen würden, sobald die abschreckende Wirkung ihrer Waffenarsenale groß genug geworden sei.

Da Nobel kinderlos blieb, veranlasste er, dass mit seinem Vermögen von etwa 31,2 Millionen Kronen eine Stiftung gegründet werden sollte.
Ein Jahr vor seinem Tod setzte er in Anwesenheit einiger Freunde, aber ohne Anwalt, am 27. November 1895 sein Testament auf. Den größten Teil seines Vermögens, ungefähr 94 % des Gesamtvermögens, führte er der Stiftung zu.

Nobel bestimmte, dass die Zinsen aus dem Fonds jährlich als Preis an diejenigen ausgeteilt werden sollten, „die im vergangenen Jahr der Menschheit den größten Nutzen erbracht haben“, und zwar zu gleichen Teilen an Preisträger auf fünf Gebieten: Physik, Chemie, Physiologie oder Medizin, Literatur und Frieden („ein Teil an denjenigen, der am meisten oder am besten auf die Verbrüderung der Völker und die Abschaffung oder Verminderung stehender Heere sowie das Abhalten oder die Förderung von Friedenskongressen hingewirkt hat“). Nobel betonte, dass die Nationalität keine Rolle spielen dürfe, vielmehr solle der Würdigste den Preis erhalten.

Nobel legte hier auch fest, wer für die Vergabe der Preise zuständig sein sollte: Die Königlich Schwedische Akademie der Wissenschaften (Nobel war seit 1884 deren Mitglied) vergibt die Auszeichnungen für Physik und Chemie, das Karolinska-Institut den Nobelpreis für Physiologie oder Medizin und die Schwedische Akademie den Nobelpreis für Literatur. Während es sich bei diesen Institutionen um wissenschaftliche handelt, ist für die Vergabe des Friedensnobelpreises das Norwegische Nobelpreiskomitee zuständig, eine vom norwegischen Parlament bestimmte Kommission.

Die Gründung der "Nobel-Stiftung" erfolgte 1900. Im Jahr darauf, an Nobels fünftem Todestag, wurden die Nobelpreise erstmals verliehen.

In seinem letzten Lebensjahr verfasste Alfred Nobel das Theaterstück "Nemesis", eine Tragödie in vier Akten über Beatrice Cenci, in Anlehnung an die von Percy Bysshe Shelley in Versform verfasste Tragödie "The Cenci". Es wurde gedruckt, als er bereits im Sterben lag. Der gesamte Bestand wurde jedoch gleich nach seinem Tod bis auf drei Exemplare vernichtet, da man es als skandalös und blasphemisch empfand. Erst 2003 wurde das Buch veröffentlicht, und zwar in einer zweisprachigen Ausgabe auf Schwedisch und Esperanto. Mittlerweile liegen Übersetzungen ins Slowenische (2004), Italienische (2005), Französische (2008) und Spanische (2008) vor.








</doc>
<doc id="216" url="https://de.wikipedia.org/wiki?curid=216" title="Arithmetisches Kodieren">
Arithmetisches Kodieren

Das arithmetische Kodieren ist eine Form der Entropiekodierung, die unter anderem zur verlustfreien Datenkompression eingesetzt wird.

Dieser Artikel beschreibt nur, wie man mit einem gegebenen Satz von Zeichen-Wahrscheinlichkeits-Paaren einzelne Zeichen so kodieren kann, dass man eine möglichst kleine mittlere Wortlänge benötigt. Dabei ist durch die Entropie (mittlerer Informationsgehalt) eine untere Schranke gegeben (Quellencodierungstheorem).

Das immer zu einem Entropiekodierer gehörende Modell der Zeichen-Wahrscheinlichkeiten ist unter Entropiekodierung#Modell beschrieben.

Zu den Begründern zählt Jorma Rissanen Ende der 1970er und Anfang der 1980er Jahre.
Das Verfahren funktioniert theoretisch mit unendlich genauen reellen Zahlen, auch wenn in den eigentlichen Implementierungen dann wieder auf endlich genaue Integer-, Festkomma- oder Gleitkommazahlen zurückgegriffen werden muss. Dies führt aber immer dazu, dass gerundet werden muss und somit das Ergebnis nicht mehr optimal ist.

Die Eingaben für den arithmetischen Kodierer werden im Folgenden als Symbol, ein Zeichen, bezeichnet. Die Ausgabe für den Kodierer ist eine reelle Zahl (hier mit "x" bezeichnet).

Zuerst müssen sich Kodierer und Dekodierer auf ein Intervall einigen, in dem sich die Zahl x befinden soll. Normalerweise wird hier der Bereich zwischen 0 und 1 (exklusive) benutzt, also das halboffene Intervall formula_1 (siehe ).

Außerdem müssen Kodierer und Dekodierer bei der De- bzw. Kodierung eines Zeichens immer identische Tabellen mit den Wahrscheinlichkeiten aller möglichen dekodierbaren Zeichen zur Verfügung haben. Für die Bereitstellung dieser Wahrscheinlichkeiten ist das Modell verantwortlich.

Eine Möglichkeit ist, vor dem Kodieren speziell für die Eingabedaten eine Häufigkeitsanalyse zu erstellen und diese dem Dekodierer, zusätzlich zur eigentlichen Nachricht, mitzuteilen. Kodierer und Dekodierer verwenden dann für alle Zeichen diese Tabelle.


Der Dekodierer kann nun ein Zeichen nach dem anderen entschlüsseln, indem er folgende Schritte ausführt:

Bei diesem Algorithmus fällt auf, dass er nicht terminiert: Es ist allein an der Zahl x nicht erkennbar, wann das letzte Zeichen dekodiert wurde. Es muss dem Dekodierer also immer durch eine zusätzliche Information mitgeteilt werden, wann er seine Arbeit beendet hat. Dies wird üblicherweise in Form einer Längenangabe realisiert, kann aber auch (bspw. wenn bei der Kodierung nur ein einziger Datendurchlauf erwünscht ist) durch ein Sonderzeichen mit der Bedeutung „Ende“ geschehen.

Die Subintervalle müssen so gewählt werden, dass Kodierer und Dekodierer die Größe und Position gleich bestimmen. Wie oben schon erwähnt, ergibt sich die Größe der Subintervalle aus den Wahrscheinlichkeiten der Zeichen.

Die Anordnung (Reihenfolge) der Intervalle dagegen ist für die Qualität des Algorithmus nicht von Bedeutung, sodass man hier eine beliebige Reihenfolge fest vorgeben kann. Diese ist Gegenstand einer Vereinbarung (z. B. alphabetische Ordnung).

Eine Möglichkeit für die Berechnung der Intervalle ist folgende:

formula_5 und formula_6 sind die Grenzen des Intervalls. formula_7 ist die Länge des Intervalls, also formula_8. Die beiden Werte formula_9 und formula_10 sind die Summe der Wahrscheinlichkeiten aller Zeichen mit einem Code kleiner, bzw. kleiner gleich dem zu kodierenden Zeichen formula_11.

In diesem Beispiel wird die Zeichenkette „AAABAAAC“ komprimiert. Zuerst wird für die Größe der Subintervalle die Häufigkeiten aller Zeichen benötigt. Der Einfachheit halber wird eine statische Wahrscheinlichkeit für alle Zeichen verwendet.

Die optimale Bitzahl ergibt sich aus der Formel für die Entropie. Mit diesem Wert lässt sich errechnen, dass der Informationsgehalt der Zeichenkette 8,49 Bits entspricht.

formula_13

Nun zum Ablauf. Die folgende Tabelle zeigt die genauen Werte für die Subintervalle nach dem Codieren der einzelnen Zeichen. Die Grafik rechts veranschaulicht die Auswahl der Subintervalle noch einmal.

Gespeichert wird eine beliebige, möglichst kurze Zahl aus dem letzten Intervall, also z. B. 0,336.

Das entspricht zwischen 8 und 9 Bits. Die Huffman-Kodierung hätte für die gegebene Zeichenfolge dagegen 10 Bit benötigt (1 für jedes A und je 2 für B und C)

Der Unterschied beträgt in diesem Beispiel 10 %. Der Gewinn wird größer, wenn die tatsächlich von der Huffman-Kodierung verwendete Bitzahl mehr von der optimalen abweicht, also wenn ein Zeichen extrem häufig vorkommt.

Der Dekodierer nimmt diese Zahl zum Dekodieren. Dabei läuft Folgendes ab:

Arithmetisches Kodieren ist asymptotisch optimal:

Nachdem das letzte Symbol verarbeitet wurde erhält man ein Intervall formula_14 mit

Das entspricht der Wahrscheinlichkeit, bei gegebenen Symbolwahrscheinlichkeiten formula_16, genau solch eine Sequenz zu erhalten.

Um nun binär einen Wert im Intervall formula_14 anzugeben, benötigt man


Da

und

können wir die Länge formula_23 der arithmetisch kodierten Sequenz abschätzen:

Das bedeutet, man benötigt mindestens so viele Bits wie die Entropie, höchstens jedoch zwei Bits mehr.

Die mittlere Länge formula_25 eines kodierten Symbols ist beschränkt auf

Für lange Sequenzen verteilen sich diese (höchstens zwei) zusätzlichen Bits gleichmäßig auf alle Symbole, so dass die mittlere Länge eines kodierten Symbols dann asymptotisch gegen die wahre Entropie geht:

Wenn sich alle Symbolwahrscheinlichkeiten formula_28 in der Form formula_29 darstellen lassen,
dann erzeugen arithmetische Kodierung und Huffman-Kodierung einen identisch langen Datenstrom und sind gleich (d.h. optimal) effizient.
In der Praxis ist dies aber so gut wie nie der Fall.

Da man bei einer konkreten Implementierung nicht mit unendlich genauen reellen Zahlen arbeiten kann, muss die konkrete Umsetzung des Algorithmus etwas anders erfolgen. Die maximale Genauigkeit der Zahlen ist im Allgemeinen fest vorgegeben (z. B. 32 Bits) und kann diesen Wert nicht überschreiten. Deshalb kann man einen Arithmetischen Kodierer nicht auf einem realen Computer umsetzen.

Um das Problem der begrenzten Genauigkeit zu umgehen, werden zwei Schritte unternommen:

Punkt 1 führt eigentlich dazu, dass der Algorithmus kein Arithmetischer Kodierer mehr ist, sondern nur ähnlich.
Es gibt aber einige eigenständige Algorithmen, die vom Arithmetischen Kodierer abstammen; diese sind:

Trotz dieser Verfahren bleiben verschiedene Probleme mit der Arithmetischen Kodierung:



</doc>
<doc id="217" url="https://de.wikipedia.org/wiki?curid=217" title="Abbildung">
Abbildung

Abbildung (abgekürzt Abb.) steht für:

Siehe auch:



</doc>
<doc id="219" url="https://de.wikipedia.org/wiki?curid=219" title="Automobil">
Automobil

Ein Automobil, kurz Auto (auch Kraftwagen, in der Schweiz amtlich Motorwagen), ist ein mehrspuriges Kraftfahrzeug (also ein von einem Motor angetriebenes Straßenfahrzeug), das zur Beförderung von Personen (Personenkraftwagen „Pkw“ und Bus) oder Frachtgütern (Lastkraftwagen „Lkw“) dient. Umgangssprachlich – und auch in diesem Artikel – werden mit "Auto" meist Fahrzeuge bezeichnet, deren Bauart überwiegend zur Personenbeförderung bestimmt ist und die mit einem Automobil-Führerschein geführt werden dürfen.

Der weltweite Fahrzeugbestand steigt kontinuierlich an und lag im Jahr 2010 bei über 1,015 Milliarden Automobilen. 2011 wurden weltweit über 80 Millionen Automobile gebaut. In Deutschland waren im Jahr 2012 etwa 51,7 Millionen Kraftfahrzeuge zugelassen, davon sind knapp 43 Millionen Personenkraftwagen.

"Automobil" ist ein substantiviertes Adjektiv. Es entstand Ende des 19. Jahrhunderts aus dem französischen Begriff für eine mit Pressluft betriebene Straßenbahn: "voiture automobile", selbstbewegender Wagen. Der Begriff ist abgeleitet aus griechisch "autós" ‚selbst‘ und lateinisch "mobilis" ‚beweglich‘ und diente zur Unterscheidung von den üblichen Landfahrzeugen, die damals von Pferden gezogen wurden.

Die Definition „selbstbewegendes Fahrzeug“ würde auch motorisierte Zweiräder und Schienenfahrzeuge einschließen. In der Regel wird unter einem Automobil jedoch ein mehrspuriges und nicht schienengebundenes Kraftfahrzeug verstanden, also ein Pkw, Bus oder Lkw. In der Alltagssprache ist meist nur der Pkw gemeint. Der Darmstädter Dozent für Kraftwagen, Freiherr Löw von und zu Steinfurth versuchte, sich in seinem Standardwerk "Das Automobil – sein Bau und sein Betrieb" über alle Ausgaben ab 1909 hinweg an möglichst exakten Definitionen von „Automobil“. In der 5. Auflage von 1924 schreibt er:

Um diese strenge Klassifizierung zu beleuchten, lässt er beispielsweise Forderung 2 weg und kommt damit „zu den sogenannten gleislosen Bahnen, die aus elektrischen Wagen bestehen, denen durch eine Oberleitung die Energie zugeführt wird.“

Im Englischen wird mit einem "automobile" bzw. "car" nur ein Pkw beschrieben. Eine Übersetzung im Sinne des zitierten von und zu Steinfurth gibt es im Englischen nicht; das in diesem Zusammenhang oft erwähnte Wort "motor vehicle" schließt auch Krafträder mit ein und ist demzufolge dem deutschen „Kraftfahrzeug“ gleichzusetzen.

Der Franzose Nicholas Cugnot erbaute 1769 einen Dampfwagen - das erste bezeugte und tatsächlich erbaute Fahrzeug, das nicht auf Muskelkraft oder einer anderen äußeren Kraft (wie z. B. Wind) basierte (und kein Spielzeug war). Im Jahr 1863 machte Étienne Lenoir mit seinem „Hippomobile“ eine 18 km lange Fahrt; es war das erste Fahrzeug mit einem Motor mit interner Verbrennung. Jedoch gilt das Jahr 1886 mit dem Motordreirad „Benz Patent-Motorwagen Nummer 1“ vom deutschen Erfinder Carl Benz als das Geburtsjahr des modernen Automobils mit Verbrennungsmotor, da es große mediale Aufmerksamkeit erregte und zu einer Serienproduktion führte. Zuvor bauten auch andere Erfinder motorisierte Gefährte mit ähnlichen oder gänzlich anderen Motorkonzepten.

Motorisierte Wagen lösten in nahezu allen Bereichen die von Zugtieren gezogenen Fuhrwerke ab, da sie deutlich schneller und weiter fahren und eine höhere Leistung erbringen können. Durch diesen Vorteil steigerte sich seit dem Geburtsjahr des Automobils die Weite der zurückgelegten Strecken, u. a. deshalb wurde dem motorisierten Straßenverkehr immer mehr Raum zugestanden.

Zu den wesentlichen Bestandteilen des Automobils gehören das Fahrwerk mit Fahrgestell und anderen Teilen, ferner Karosserie, Motor, Getriebe und Innenraum. Europäische Pkw bestehen zu über 54 % aus Stahl, die Hälfte davon hochfeste Stahlgüten. Die Technik der Fahrzeuge müssen Ingenieure und Designer in eine funktionale, ergonomische und ästhetische Form bringen, die die Markenwerte des Herstellers vermittelt und Emotionen weckt. Beim Kauf eines Autos ist das Fahrzeugdesign heute eines der wichtigsten Entscheidungskriterien.

Nach Zahlen der WHO sterben 1,2 Millionen Menschen jährlich an den direkten Folgen von Verkehrsunfällen.

Die Sicherheit von Insassen und potenziellen Unfallgegnern von Kraftfahrzeugen ist unter anderem abhängig von organisatorischen und konstruktiven Maßnahmen sowie dem persönlichen Verhalten der Verkehrsteilnehmer. Zu den organisatorischen Maßnahmen zählen zum Beispiel Verkehrslenkung (Straßenverkehrsordnung mit Verkehrsschildern oder etwas moderner durch Verkehrsleitsysteme), gesetzliche Regelungen (Gurtpflicht, Telefonierverbot), Verkehrsüberwachung und straßenbauliche Maßnahmen.

Die konstruktiven Sicherheitseinrichtungen moderner Automobile lassen sich grundsätzlich in zwei verschiedene Bereiche gliedern. Passive Sicherheitseinrichtungen sollen, wenn ein Unfall nicht zu vermeiden ist, die Folgen abmildern. Dazu zählen beispielsweise der Sicherheitsgurt, die Sicherheitskopfstütze, der Gurtstraffer, der Airbag, der Überrollbügel, deformierbare Lenkräder mit ausklinkbaren Lenksäulen, die Knautschzone, der Seitenaufprallschutz sowie konstruktive Maßnahmen zum Unfallgegnerschutz. Aktive Sicherheitseinrichtungen sollen einen Unfall verhindern oder in seiner Schwere herabsetzen. Beispiele hierfür sind das Antiblockiersystem (ABS) sowie das elektronische Stabilitätsprogramm (ESP).

Zu den persönlichen Maßnahmen zählen Verhaltensweisen wie eine defensive Fahrweise, das Einhalten der Verkehrsvorschriften oder Training der Fahrzeugbeherrschung, beispielsweise bei einem Fahrsicherheitstraining. Diese sowie die Verkehrserziehung speziell für Kinder helfen das persönliche Unfallrisiko zu vermindern.

Alle Maßnahmen zur Erhöhung der Verkehrssicherheit zusammen können dazu beitragen, dass die Zahl der bei einem Verkehrsunfall getöteten Personen reduziert wird. In den meisten Industrienationen sind die Opferzahlen seit Jahren rückläufig. In Europa spielen Verkehrsunfälle als Todesursache heute eine geringere Rolle als vor einigen Jahrzehnten, die Zahl der Todesopfer liegt unter den Zahlen der Drogentoten oder Suizidenten. So fielen in Deutschland, Österreich, den Niederlanden oder der Schweiz die Opferzahlen seit den 1970er-Jahren, trotz kaum rückläufiger Zahlen der Verkehrsunfälle, auf ein Drittel. 2011 ist in Deutschland die Zahl der Verkehrstoten zum ersten Mal seit 20 Jahren wieder gestiegen, in Österreich und der Schweiz allerdings auf dem historisch tiefsten Stand.

Nach längerer freiwilliger Aktion wurde das Fahren mit eingeschaltetem Licht am Tag in Österreich am 15. November 2005 verpflichtend eingeführt und 2007 auch per Strafe eingefordert. Zum 1. Januar 2008 wurde die Lichtpflicht allerdings wieder abgeschafft. Ziel dieser Kampagne war es, die menschlichen Sinneseindrücke auf die Gefahrenquellen zu fokussieren und damit die Zahl der Verkehrstoten zu verringern. Schätzungen des Bundesministeriums zufolge wurden jährlich 15 Verkehrstote weniger erwartet. Allerdings zeigte sich nicht der erwartete Effekt, da vermehrt die Aufmerksamkeit von unbeleuchteten Gefahrenquellen (Hindernisse oder andere Verkehrsteilnehmer etwa Fußgänger) weg zu den bewegten und beleuchteten Fahrzeugen gelenkt wurde. Auch in Norwegen wurden in den Jahren nach der Einführung der Lichtpflicht 1985 deutlich mehr Verkehrstote gezählt als in den Jahren davor. Trotzdem wird in einigen Ländern (etwa Deutschland) weiterhin die Einführung einer solchen Maßnahme in Erwägung gezogen.

Sowohl Automobilbauer und Zulieferbetriebe als auch Unternehmen aus der IT-Branche (insbesondere Google) forschen und entwickeln am autonom fahrenden Kraftfahrzeug (meist Pkw). . Erfahrungen amerikanischer Autoversicherungen würden nahelegen, dass bereits die Anzeigen der Assistenz-Sensorik das Unfallrisiko senken können. Auch wird die Ansicht vertreten, dass ein gewisses Maß an Unsicherheit den Erfolg autonomer Automobile nicht verhindern wird.

Das „Wiener Übereinkommen über den Straßenverkehr“ von 1968 verbot lange Zeit autonome Automobile, wurde jedoch Mitte Mai 2014 von der UN geändert, so dass Davor schrieb es unter anderem vor, dass jedes in Bewegung befindliche Fahrzeug einen Fahrer haben und dieser das Fahrzeug auch beherrschen muss. Zu klären sind insbesondere Fragen bezüglich des Haftungsrechts bei Unfällen, wenn technische Assistenzsysteme das Fahren übernehmen. Im bisher dem Fortschritt zugeneigten Kalifornien, das lange Zeit liberale Regelungen für autonome Automobile hatte, wurde 2014 die gesetzliche Situation jedoch verschärft – jetzt muss immer ein Mensch am Steuer sitzen, der „jederzeit eingreifen kann“. Einer Studie des Bundesministeriums für Wirtschaft und Energie zufolge rechnet man damit, dass zumindest die Automatisierung einiger Fahrfunktionen bis spätestens 2020 technisch realisierbar sein werden, während fahrerlose Fahrzeuge auf öffentlichen Straßen erst weit später zu erwarten seien.

Auch Fahrzeuge ohne Lenkrad, Bremse und Gaspedal werden erprobt. In diesem Zusammenhang werden Verkehrskonzepte wie ein erweitertes Car Sharing diskutiert: Man bucht das Auto übers Internet und steigt bei Bedarf zu. Keiner der Insassen benötigt eine Fahrerlaubnis.

Die Gesamtbetriebskosten eines Autos setzen sich zusammen aus Fixkosten (auch „Unterhaltskosten“ genannt) und variablen Kosten (auch „Betriebskosten“ genannt), hinzu kommt der Wertverlust des Autos. Die Kosten werden von vielen Menschen unterschätzt.

Die Fixkosten fallen unabhängig von der jährlichen Kilometerleistung an. Sie setzen sich im Wesentlichen zusammen aus der Kraftfahrzeugsteuer, den obligatorischen Kraftfahrzeug-Haftpflichtversicherungen, in vielen Ländern eines zwangsweisen Mautbeitrags sowie sporadisch vorgeschriebenen Technischen Prüfungen.

Daneben können freiwillige Zusatzversicherungen abgeschlossen werden, wie eine Kaskoversicherung sowie weitere Versicherungen oder zusätzliche versicherungsähnliche Leistungen, welche die Automobilclubs bei einer Mitgliedschaft anbieten.

Die Betriebskosten hängen weitgehend von der jährlichen Kilometerleistung ab. Es entstehen Aufwände für den Energieverbrauch (bei Verbrennungsmotoren ist das der Kraftstoffverbrauch), den Ersatz von Verschleißteilen (insbesondere Autoreifen), sowie für weitere Wartung und ggf. außerplanmäßige Reparaturen. Die Wartung ist je nach Zeit und Kilometern erforderlich. Typische Zeitintervalle liegen bei 1 bis 2 Jahren, typische Kilometerintervalle bei 10.000 km bis 30.000 km. Werden die Wartungsintervalle nicht eingehalten, kann das zu Schwierigkeiten mit Garantieansprüchen bei Defekten führen.
Je nach individuellem Wunsch entstehen Kosten für die Fahrzeugreinigung.

Nicht direkt kilometerabhängig sind Park- und Mautgebühren.

Der Kaufpreis verringert sich sofort als Wertverlust auf den jeweiligen, zeitabhängigen Verkehrswert, während beim Leasing ein ähnlicher Verlust durch Zinszahlungen entsteht.

Statistisches Bundesamt und ADAC veröffentlichen vierteljährlich einen Autokosten-Index. Dieser gibt an, um wie viel Prozent sich verschiedene Kostenbestandteile verteuert oder verbilligt haben.

Unabhängig von der Fahrleistung fallen Fixkosten und Wertverlust bzw. Reparaturkosten von – in Abhängigkeit von der Fahrzeugklasse – etwa 200 Euro im Monat an. Bei einer jährlichen Fahrleistung von 15.000 bzw. 30.000 Kilometern ist mit monatlichen Gesamt-Betriebskosten von über 320 Euro bzw. über 430 Euro zu rechnen.

Der Pkw-Verkehr bringt externe Kosten, insbesondere im Bereich Umweltverschmutzung und Unfallfolgekosten, mit sich. Viele der dabei betrachteten Größen sind kaum bzw. nur sehr ungefähr zu quantifizieren, weshalb verschiedene Publikationen zum Thema unterschiedlich hohe externe Kosten benennen.

Gemäß Umweltbundesamt betrugen die externen Kosten im Straßenverkehr in Deutschland im Jahr 2005 insgesamt 76,946 Mrd. Euro, wovon 61,2 Mrd. auf den Personen- und 15,8 Mrd. auf den Güterverkehr entfielen. Die Unfallkosten machten dabei 52 % (entspricht 41,7 Mrd. Euro) der externen Kosten aus. Das Umweltbundesamt berechnete 2007, dass Pkw in Deutschland durchschnittlich etwa 3 Cent pro Kilometer an Kosten für Umwelt und Gesundheit verursachen, die hauptsächlich durch Luftverschmutzung entstehen. Das ergibt rechnerisch Kosten von 3000 Euro für einen Pkw mit 100.000 Kilometern Laufleistung. Für Lkw betragen diese Kosten sogar 17 Cent pro Kilometer. Diese externen Kosten werden nicht oder nur teilweise durch den Straßenverkehr getragen, sondern u. a. durch Steuern sowie Krankenkassen- und Sozialversicherungsbeiträge finanziert. Die Kostenunterdeckung des Straßenverkehrs (also alle durch den Straßenverkehr direkt und indirekt verursachten Kosten abzüglich aller im Zusammenhang mit dem Straßenverkehr geleisteten Steuern und Abgaben) beziffert das Umweltbundesamt für das Jahr 2005 auf rund 60 Mrd. Euro.

Der österreichische Pkw-Verkehr trug im Jahr 2000 nur einen Teil der von ihm verursachten Kosten: Ein großer Teil der Kosten für die Errichtung und Erhaltung der Straßen sowie der Sekundärkosten wie Unfall- und Umweltkosten (Lärm, Luftschadstoffe) aller Verkehrsteilnehmer werden von der Allgemeinheit übernommen. Während der Pkw-Verkehr für 38 % der durch ihn verursachten Kosten aufkam, trugen Busse die eigenen Kosten zu 10 % und Lkw zu 21 %.

Der Pkw-Verkehr ist Forschungsgegenstand der Volkswirtschaft, namentlich der Verkehrswissenschaft. Das Automobil als industrielles Massenprodukt hat den Alltag der Menschheit verändert. Seit dem Beginn des 20. Jahrhunderts hat es mehr als 2.500 Unternehmen gegeben, die Automobile produzierten. Viele Unternehmen, die im 19. Jahrhundert Eisenwaren oder Stahl produzierten, fingen Mitte des Jahrhunderts mit der Fertigung von Waffen oder Fahrrädern an und entwickelten so die Kenntnisse, die Jahrzehnte später im Automobilbau benötigt wurden.

Heute gibt es neben den großen Herstellern viele kleine Betriebe, die als Automanufaktur zumeist exklusive Fahrzeuge produzieren, beispielsweise Morgan (GB).

Die Bedeutung des Automobils basiert neben der vergleichsweise hohen physischen Leistungsfähigkeit des Systems auch auf der hohen Freizügigkeit in den Nutzungsmöglichkeiten bezüglich der Transportaufgaben und der Erschließung räumlicher bzw. geografischer Bereiche. Bis ins 19. Jahrhundert gab es nur wenige Fortbewegungsmittel, zum Beispiel die Kutsche oder das Pferd. Die Verbreitung der Eisenbahn steigerte zwar die Reisegeschwindigkeit, aber man war an Fahrpläne und bestimmte Haltepunkte gebunden. Mit dem Fahrrad stand ab Ende des 19. Jahrhunderts erstmals ein massentaugliches Individualverkehrsmittel zur Verfügung, allerdings ermöglichte erst das Automobil individuelle motorisierte Fortbewegung sowie den flexiblen und schnellen Transport auch größerer Lasten. In den 1960er Jahren herrschte eine regelrechte Euphorie, woraus eine vorherrschende Meinung entstand, der gesamte Lebensraum müsse der Mobilität untergeordnet werden („Autogerechte Stadt“). Schon in den 1970er Jahren wurden einige solche Projekte jedoch gestoppt. Die Emissionen aus dem Verkehr steigen auch im Jahre 2011 immer noch und im Gegensatz zu den Brennstoffen können die vereinbarten Ziele zum Klimaschutz bei den Treibstoffen (in der Schweiz) nicht erfüllt werden.

Zum 1. Januar 2004 waren in Deutschland 49.648.043 Automobile zugelassen. Im Vergleich mit Fußgängern und Fahrrädern, aber auch mit Bussen und Bahnen hat das Auto einen höheren Platzbedarf. Insbesondere in Ballungsgebieten führt dies zu Problemen durch Staus und Bedarf an öffentlichen Flächen, wodurch sich einige der Vorteile des Automobils auflösen.

Der Güterverkehr auf der Straße ist ein elementarer Bestandteil der heutigen Wirtschaft. So erlaubt es die Flexibilität der Nutzfahrzeuge, leicht verderbliche Waren direkt zum Einzelhandel oder zum Endverbraucher zu bringen. Mobile Baumaschinen übernehmen heute einen großen Teil der Bauleistungen. Die Just-in-time-Produktion ermöglicht einen schnelleren Bauablauf. Beton wird in Betonwerken gemischt und anschließend mit Fahrmischern zur Baustelle gebracht, mobile Betonpumpen ersparen den Gerüst- oder Kranbau.

Der massenhafte Betrieb von Verbrennungsmotoren in Autos führt zu Umweltproblemen, einerseits lokal durch Schadstoffemissionen, die je nach Stand der Technik vielfach vermeidbar sind, andererseits global durch den systembedingten CO-Ausstoß, der zur Klimaerwärmung beiträgt.

Die Luftverschmutzung durch die Abgase der Verbrennungsmotoren nimmt, gerade in Ballungsräumen, oft gesundheitsschädigende Ausmaße an (Smog, Feinstaub). Die Kraftstoffe der Motoren beinhalten giftige Substanzen wie Xylol, Toluol, Benzol sowie Aldehyde. Noch giftigere Bleizusätze sind zumindest in Europa und den USA nicht mehr üblich.

Auch der überwiegend vom Automobil verursachte Straßenlärm schädigt die Gesundheit. Hinzu kommt, dass das Autofahren, besonders über längere Zeit, teilweise mit Bewegungsmangel verbunden sein kann.

Der Verbrauch von Mineralöl, einem fossilen Energieträger zum Betrieb konventioneller Automobile erzeugt einen CO-Ausstoß und trägt damit zum Treibhauseffekt bei.

Nach Planungen der EU-Kommission sollen bis zum Jahr 2050 Autos mit Verbrennungskraftmaschinenantrieb aus den Innenstädten Europas gänzlich verbannt werden.

Der Flächenverbrauch für Fahrzeuge und Verkehrswege verringert den Lebensraum für Menschen, Tiere und Pflanzen.
Das Platz- und Parkplatzproblem der Ballungsgebiete zeigte sich bereits in den 1920er Jahren und schon 1929 verfolgte der deutsche Ingenieur und Erfinder Engelbert Zaschka in Berlin den Ansatz des zerlegbaren (Faltauto). Dieses Stadtauto-Konzept hatte das Ziel, kostengünstig und raumsparend zu sein, indem sich das Fahrzeug nach Gebrauch zusammenklappen ließ.

Die Fertigung von Automobilen verbraucht darüber hinaus erhebliche Mengen an Rohstoffen, Wasser und Energie. Greenpeace geht von einem Wasserverbrauch von 20.000 l für einen Mittelklassewagen aus. Die Zeitschrift Der Spiegel berechnete 1998 für die Herstellung eines Pkw der oberen Mittelklasse (etwa Mercedes E-Klasse) gar 226.000 l Wasser. Die Wasserwirtschaft sieht branchenpositive 380.000 l für ein Fahrzeug als notwendig an.
Das Automobil wird derzeit (2013) zu 85 Prozent recycelt und zu 95 Prozent verwertet. Bei metallischen Bestandteilen beträgt die Recyclingquote 97 Prozent.

Einen Überblick zur Umweltfreundlichkeit von jeweils aktuellen Pkw-Modellen veröffentlicht der Verkehrsclub Deutschland (VCD) jährlich in der Auto-Umweltliste.

Zu den Gefahren des Kraftfahrzeugverkehrs beziehungsweise zu den durch dessen Umwelteinwirkungen verursachten Kosten siehe die Kapitel "Sicherheit" bzw. "Externe Kosten".

Die verbreitete Verwendung des Autos soll die sozialen Räume verändern – u. a. wurden folgende Auswirkungen in der Schweiz beklagt:
Die gesamte kindliche Entwicklung wird beeinflusst.

Seit 1. Dezember 2011 müssen in Deutschland Neuwagen mit einer Energieverbrauchskennzeichnung versehen werden. Die Klassen reichen von A+ bis G. Der Verbrauch wird auf das Fahrzeuggewicht bezogen, womit Vergleiche nur innerhalb einer Gewichtsklasse möglich sind. Dass ein leichterer Wagen bei gleicher Benotung weniger Energie für einen Transport benötigt als ein schwererer Wagen, ist an dem Label nicht erkennbar.

In Deutschland sind eine Reihe von Verbänden entstanden, die anfangs Dienstleistungen für Autofahrer auf Gegenseitigkeit organisierten, vor allem Pannenhilfe. Heute arbeiten sie zunehmend auch als Lobby-Verbände und vertreten die Interessen der Autofahrer und der Automobilindustrie gegenüber Politik, Industrie und Medien.

Bereits 1899 wurde der Automobilclub von Deutschland (AvD) gegründet, der ein Jahr später die erste Internationale Automobilausstellung organisierte. 1911 war der "Allgemeine Deutsche Automobil-Club", der ADAC, aus der 1903 gegründeten "Deutschen Motorradfahrer-Vereinigung" entstanden. Er ist heute mit 15 Millionen Mitgliedern Europas größter Club. Weitere Verbände in Deutschland sind der Auto Club Europa (ACE), der 1965 von Gewerkschaften gegründet wurde, sowie seit 1986 der ökologisch orientierte Verkehrsclub Deutschland (VCD), der zusätzlich auch die Interessen der anderen Verkehrsteilnehmer (Radfahrer, Fußgänger, ÖPNV-Benutzer) vertritt.

Die Interessen der Automobilhersteller und deren Zulieferunternehmen vertritt der Verband der Automobilindustrie (VDA).


Zu den neuen Entwicklungen gehören alternative Antriebe wie das Elektroauto (Elektrofahrzeug). Eine weitere Entwicklung ist das autonome Fahren (Autonomes Landfahrzeug). Durch Carsharing wechselt ein Auto vom Privatbesitz in einen Gemeinschaftsbesitz. Experimentell entwickelt werden zudem Prototypen von Flugautos.




</doc>
<doc id="221" url="https://de.wikipedia.org/wiki?curid=221" title="Adam Ries">
Adam Ries

Adam Ries (oft auch Adam Riese; * 1492 oder 1493 in Staffelstein, Fürstbistum Bamberg; † 30. März oder 2. April 1559 vermutlich in Annaberg oder Wiesa) war ein deutscher Rechenmeister. Bekannt wurde er durch sein Lehrbuch "Rechnung auff der Linihen und Federn [...]", das bis ins 17. Jahrhundert mindestens 120-mal aufgelegt wurde. Bemerkenswert ist, dass Adam Ries seine Werke nicht – wie damals üblich – in lateinischer, sondern in deutscher Sprache schrieb. Dadurch erreichte er einen großen Leserkreis und konnte darüber hinaus auch zur Vereinheitlichung der deutschen Sprache beitragen.

Adam Ries gilt als der „Vater des modernen Rechnens“. Er hat mit seinen Werken entscheidend dazu beigetragen, dass die römische Zahlendarstellung als unhandlich erkannt und weitgehend durch die nach dem Stellenwertsystem strukturierten indisch-arabischen Zahlzeichen ersetzt wurde. Sein Name ist aus der Redewendung „nach Adam Riese“ allgemein bekannt.

Adam Ries stammte aus Staffelstein, in der Vorrede zu seiner „Coß“ gibt er selbst darüber Auskunft. Sein Vater Contz Ries war der Besitzer der dortigen Stockmühle, seine Mutter dessen zweite Frau Eva Kittler.

Das Geburtsjahr ist nicht eindeutig zu bestimmen. Die Umschrift auf dem einzig bekannten zeitgenössischen Porträt des Rechenmeisters lautet: "ANNO 1550 ADAM RIES SEINS ALTERS IM LVIII". Wenn er demnach im Jahr 1550 im 58. Lebensjahr war, muss er 1492 oder 1493 geboren worden sein, je nachdem, wann er das 58. Lebensjahr vollendet hat.

Die ersten Jahrzehnte nach der Geburt Ries’ sind nicht dokumentiert, sodass nicht bekannt ist, welche Schule er besucht hat. Auch findet sich in den Matrikeln der damals bereits bestehenden Universitäten kein Hinweis auf ein Studium des späteren Rechenmeisters. 1509 hielt er sich mit seinem jüngeren Bruder Conrad in Zwickau auf, der dort die Lateinschule besuchte.

Die älteste bekannte urkundliche Erwähnung Adam Ries’ stammt vom 22. April 1517, als er vor dem Staffelsteiner Rat wegen einer Erbstreitigkeit erschien.

1518 ging Ries nach Erfurt, wo er eine Rechenschule leitete. In Erfurt verfasste er zwei seiner Rechenbücher und ließ sie drucken.

1522 zog es ihn in die junge, vom Silbererzbergbau geprägte Stadt Annaberg, in der er den Rest seines Lebens verbrachte. In der Johannisgasse eröffnete er eine private Rechenschule. Das Haus beherbergt heute das Adam-Ries-Museum.

1524 beendete Ries die Arbeiten am Manuskript der "Coß", einem mehr als 500 Seiten umfassenden Lehrbuch der Algebra ("Coß" ist der im Mittelalter übliche Name für die Variable bzw. Unbekannte). Die "Coß" ist ein Bindeglied zwischen der mittelalterlichen und der heutigen Algebra. Das Manuskript war sowohl seinen Söhnen und Schülern als auch anderen mathematisch interessierten Personen aus seinem Umfeld zugänglich, es wurde seinerzeit aber nicht publiziert. Der Druck hätte enorme Kosten verursacht und andere deutsche Mathematiker publizierten vergleichbare Darstellungen zwischen 1520 und 1550. So wurde das vollständige Manuskript erstmals 1992 gedruckt.

Im Traubuch der Annaberger St.-Annen-Kirche wurde 1525 die Vermählung mit Anna Leuber vermerkt, einer Tochter des Freiberger Schlossermeisters Andreas Leuber: „Adam Reyeß Anna Filia Anders lewbers vo Freybergk“. Im selben Jahr legte er den Bürgereid ab. Sein Brot verdiente er zunächst als Rezessschreiber mit Abrechnungen für die einzelnen Erzgruben, später prüfte er als Gegenschreiber diese Abrechnungen und sorgte als Zehntner dafür, dass der Landesherr seinen Anteil am Gewinn erhielt.

Ries übernahm verantwortliche Tätigkeiten in der sächsischen Bergverwaltung. Besondere Bedeutung hatte in der Zeit des aufblühenden Bergbaus die Versorgung der rasch zunehmenden Bevölkerung mit Lebensmitteln, insbesondere mit Brot. Brot hatte Festpreise: Es wurden Groschenbrote, Zweigroschenbrote und Pfennigsemmeln verkauft. Die Schwankungen der Getreidepreise wurden mit unterschiedlich großen Brotlaiben berücksichtigt. Im Auftrag der Stadt Annaberg erarbeitete Ries die sogenannte „Annaberger Brotordnung“ zum Schutz der Bevölkerung.

1539 erwarb er die nach ihm benannte „Riesenburg“, ein Vorwerk außerhalb der Stadt, dessen Gebäude den Namen noch heute tragen. Im Jahr 1550 erschien sein letztes Werk im Druck. 

Adam Ries starb 1559, die überlieferten Angaben differieren zwischen dem 30. März und 2. April. Auch der genaue Sterbeort ist nicht bekannt, Annaberg oder Wiesa.

Da die Schreibweise von Namen damals nicht so festgelegt war wie heute, sind als zeitgenössische Schreibweisen neben "Ries" auch "Ris", "Rise", "Ryse" und sogar "Reyeß" bekannt.

Im heutigen Sprachgebrauch finden sich die beiden Namensformen "Ries" und "Riese". Die Form "Riese" wird auch in der Redewendung „nach Adam Riese“ verwendet.

Der Ausspruch „Das macht nach Adam Riese“ wird heute noch gebraucht, um die Richtigkeit eines Rechenergebnisses zu unterstreichen. Bereits im 18. Jahrhundert war er geläufig. Abraham Gotthelf Kästner etwa schreibt in seiner "Geschichte der Mathematik" 1796: Eine 1785 erschienene Schrift erklärt: 

Adam Ries verfasste drei Rechenbücher für den Unterricht in Rechenschulen und für die Ausbildung von Kaufleuten und Handwerkern:
Ries schrieb seine Bücher in deutscher Sprache. Das förderte ihre Verbreitung im deutschsprachigen Raum und lieferte einen Beitrag zur Vereinheitlichung der deutschen Sprache.

Ries entwarf die Annaberger Brotordnung, diese regelte mit einer Sammlung von Tabellen die zulässigen Gewichtsabweichungen. Später erstellte Adam Ries ähnliche Brotordnungen auch für Joachimsthal, Zwickau, Hof und Leipzig.:

1524 beendete Ries die Arbeiten am Manuskript der "Coß", einem mehr als 500 Seiten umfassenden Lehrbuch der Algebra.

Mit seiner Frau Anna zeugte er mindestens acht Kinder. Drei der fünf Söhne, Adam, Abraham und Jacob, waren zeitweilig als Rechenmeister in Annaberg tätig. Während Abraham und Jacob 1604 in ihrer Heimat starben, soll Adam sich im Harz niedergelassen haben. Den vierten Sohn, Isaac, zog es nach Leipzig, wo er unter anderem als Visierer (Eichmeister) tätig war. Paul, der fünfte Sohn, wurde Gutsbesitzer und Richter in Wiesa. Die drei Töchter Eva, Anna und Sybilla heirateten jeweils in Annaberg. 

Die Nachkommen von Adam Ries sind Gegenstand ständiger, ausführlicher genealogischer Forschung. Noch heute lebt eine Vielzahl von Adam-Ries-Nachfahren im Obererzgebirge. Der "Adam-Ries-Bund" hat es sich zur Aufgabe gemacht, sämtliche Nachkommen von Adam Ries zu ermitteln, und weist in seiner ständig aktualisierten Datenbank bislang mehr als 20.000 direkte Nachkommen auf.




Am 18. August 1997 wurde der Asteroid (7655) Adamries nach ihm benannt.





</doc>
<doc id="222" url="https://de.wikipedia.org/wiki?curid=222" title="Archaeopteryx">
Archaeopteryx

Archaeopteryx (aus ' „uralt“ und ' „Flügel, Schwingen, Feder“, Aussprache: Archeo-ptéryx; sinngemäß „uralte Schwinge“ oder „Urschwinge“) ist eine Gattung der Archosaurier, deren Fossilien in der Fränkischen Alb in den Solnhofener Plattenkalken aus dem Oberjura entdeckt wurden. "Archaeopteryx" gilt als Übergangsform, die zwischen theropoden Dinosauriern und den Vögeln vermittelt. Da der etwa taubengroße "Archaeopteryx" in der Regel den Vögeln als ursprungsnahe Form zugerechnet wird, bezeichnet man die Gattungsmitglieder auch als Urvögel.

"Archaeopteryx" wurde im Jahr 1861 von Hermann von Meyer auf der Grundlage eines isolierten Federabdrucks erstmals beschrieben. Das erste Skelettexemplar (das sogenannte Londoner Exemplar) wurde schon im selben Jahr entdeckt und ist in der Erstveröffentlichung erwähnt. Bis heute folgten mindestens 11 weitere, unterschiedlich vollständige Skelettfunde.

Die Gattung "Archaeopteryx" zeigt ein Mosaik aus (für Vögel) urtümlichen, das heißt reptilienhaften Merkmalen, die später von den modernen Vögeln (Neornithes) abgelegt wurden, und abgeleiteten, das heißt vogeltypischen Merkmalen (die jedoch nach gegenwärtigem Kenntnisstand nur noch zum Teil als allein charakteristisch für Vögel gelten).

Urtümlich sind unter anderem das Vorhandensein von Zähnen und Bauchrippen (Gastralia), eine lange Schwanzwirbelsäule, eine relativ geringe Zahl unverschmolzener Beckenwirbel (Sakralia), unverschmolzene Mittelhand-, Mittelfuß- und Fußwurzel- und Beckenknochen, die drei Fingerklauen sowie das Fehlen eines knöchernen Brustbeins.

Zu den vogeltypischen Merkmalen kann man die modern anmutenden asymmetrischen Schwungfedern zählen, außerdem die zu einem Gabelbein verschmolzenen Schlüsselbeine und die rückwärts oder seitlich-rückwärts orientierte erste Zehe (Hallux) des Fußes (anisodactyler Vogelfuß).

Die Vogel-Charakteristika des Urvogels sind auch für manche gefiederten Dinosaurier belegt oder, wie im Fall der revertierten ersten Zehe des "Archaeopteryx", nicht unangefochten. Deshalb sehen manche Paläontologen den Urvogel nicht als wesentlich vogelähnlicher an als manche theropoden Dinosaurier, die nicht den Vögeln zugerechnet werden (z. B. "Microraptor").

Innerhalb der letzten 20 Jahre sind eine Vielzahl von Fossilien urtümlicher Vögel und vogelähnlicher Dinosaurier entdeckt worden, besonders in Sedimentgesteinen der Unterkreide von Nordostchina (der Jehol-Gruppe). Somit steht "Archaeopteryx" als Mosaikform nicht allein, sondern lässt sich in eine (morphologische, nicht zeitliche) Abfolge von Dinosauriern einordnen, die den Vögeln sukzessive ähnlicher werden.

Vertreter der Hypothese, der Schlagflug der Vögel sei aus dem Gleitflug von einem erhöhten Punkt herab entstanden, interpretieren die Krallen des "Archaeopteryx" als die eines Baumkletterers, der von den Ästen herabglitt. Bei palökologischen Untersuchungen der Fundhorizonte kamen manche Forscher jedoch zu dem Schluss, dass am Bildungsort der Solnhofener Plattenkalke ein heißes und trockenes Klima geherrscht haben muss und wahrscheinlich keine Bäume vorkamen. Im Gegenzug wiesen sie aber auf hohe Klippen an der Küste des Jurameeres hin, die als Startpunkt für erste Flugversuche in Frage kommen. Burgers und Chiappe zeigten, dass "Archaeopteryx" möglicherweise auch vom Boden starten konnte.

Neueste Forschungen, die das elfte bisher bekannte Fossil mit dem bislang besterhaltenen Federkleid untersuchten, gehen davon aus, dass das Federkleid des Vogels nicht vordringlich zum Fliegen, sondern nur zum Wärmen des Tieres diente. Diese Erkenntnis kam auch durch den Vergleich mit anderen Federformen bei Dinosauriern. Zusätzlich dienten die Armschwingen beim schnellen Laufen zum Halten der Balance, waren nützlich bei der Brut und dienten auch der Tarnung und als Schmuck.

Charles Darwin hatte in der von ihm entwickelten Evolutionstheorie 1859 vorhergesagt, dass es bei der Entwicklung neuer Arten Übergangsformen geben sollte, die noch Merkmale der alten, aber auch schon Merkmale der neuen Gruppe besitzen müssten. Als Darwin seine Theorie veröffentlichte, waren noch keine solchen Fossilien bekannt, sie wurden deshalb als fehlende Glieder ("missing links") bezeichnet. Nur zwei Jahre später wurde das erste Skelettexemplar des "Archaeopteryx" gefunden.

Die "Archaeopteryx"-Funde waren der erdgeschichtlich früheste Beleg für Federn eines Wirbeltiers. Dass sie bereits deutliche Merkmale von Vögeln, aber auch noch solche von Reptilien bzw. Sauriern besaßen, machte "Archaeopteryx" zu einem wichtigen Indiz für die Richtigkeit der Darwinschen Evolutionstheorie.
Der Streit über die Evolutionstheorie wurde damit auch zu einer Auseinandersetzung um "Archaeopteryx".

Richard Owen verfasste die erste Beschreibung des Londoner Exemplars (siehe Abbildung oben). Er lehnte aufgrund seiner religiösen Überzeugung die Evolutionstheorie ab und vermied peinlich genau jeden Hinweis auf die mögliche Deutung als vermittelndes Bindeglied zwischen Reptilien und Vögeln. Seine Beschreibung enthielt grobe Fehler. Owen, damals erster Superintendant der naturgeschichtlichen Sammlung des Britischen Museums, kaufte das Fossil an, entgegen der ausdrücklichen Weisung des Aufsichtsrates. Owen wollte mit dem Ankauf verhindern, dass Darwins Evolutionstheorie durch den Urvogel gestützt wird. Erst Thomas Henry Huxley lieferte eine systematische Beschreibung des Londoner Exemplars und interpretierte es als Beleg für die Evolutionstheorie. So wurde der Weg frei für die Evolutionstheorie und ihre Anhänger – in einer der bedeutendsten naturwissenschaftlichen Institutionen der Welt.

Bisher wurden zwölf mehr oder weniger gut erhaltene Skelette der Gattung "Archaeopteryx" sowie eine einzelne Feder gefunden. Sämtliche Exemplare stammten aus den Schichten des oberen weißen Jura in den Steinbrüchen bei Eichstätt, Solnhofen, Langenaltheim, Jachenhausen bei Riedenburg und Schamhaupten. Der Abdruck der einzelnen Feder wurde 1860 entdeckt, das erste Skelett 1855 („Haarlemer Exemplar“) und das bisher letzte Exemplar 2011. Dabei handelt es sich um folgende Stücke (geordnet nach dem Zeitpunkt, an dem der jeweilige Fund erstmals als "Archaeopteryx" erkannt wurde):

1. „Die Feder“, entdeckt 1860 im Gemeindesteinbruch Solnhofen und 1861 beschrieben von dem Frankfurter Wirbeltier-Paläontologen Hermann von Meyer (1801–1869), der den heute noch gültigen Gattungsnamen "Archaeopteryx" prägte, war der erste bekannt gewordene Fund. Der eine Teil des Abdrucks befindet sich im Museum für Naturkunde in Berlin, die andere Seite im Paläontologischen Museum in München. Ob die isolierte Feder tatsächlich von "Archaeopteryx" stammt, ist nicht bekannt. Lange Zeit war dieses Exemplar jedoch problematischerweise der Holotypus.

2. Das „Londoner Exemplar“, gefunden 1861 auf der Langenaltheimer Haardt bei Solnhofen, gehört zu den drei bedeutendsten Exemplaren. Es war der erste vollständige Fund eines Skeletts und ist das Typus-Exemplar der Art "Archaeopteryx lithographica". Wenige Monate nach dem Fund erwarb das Londoner Natural History Museum (damals noch zum British Museum gehörend) das Exemplar von seinem Besitzer, dem Pappenheimer Kreisarzt Carl Friedrich Häberlein (1787–1871). Treibende Kraft des Ankaufs war dabei der britische Naturforscher Richard Owen, damals Leiter der naturhistorischen Sammlung des Britischen Museums und erklärter Gegner von Darwins Theorien. Owen wollte mit dem Ankauf verhindern, dass Darwins Evolutionstheorie durch den Urvogel gestützt wird. Das Fossil blieb lange Zeit unter Verschluss und Untersuchungsergebnisse wurden nur nach und nach veröffentlicht.
3. Das „Berliner Exemplar“ (gefunden zwischen 1874 und 1876 auf dem Blumenberg bei Eichstätt), gilt mit seinen deutlichen Federabdrücken und einem erhaltenen Schädel als das wahrscheinlich schönste und vollständigste Stück. Der Finder Jakob Niemeyer tauschte den Fund für eine Kuh im Wert von 150 bis 180 Mark ein. Der neue Besitzer Johann Dörr, ein Steinbruchbesitzer, veräußerte es für 2.000 Mark an Ernst Otto Häberlein (1819–1886) aus Pappenheim, dem Sohn des Verkäufers des Londoner Exemplars, der den Fund auch präparierte. Zunächst interessierten sich die Bayrische Staatssammlung und die Yale-Universität für das Fundstück, doch konnten beide den hohen Kaufpreis nicht aufbringen. Selbst eine Bitte deutscher Zoologen an Kaiser Wilhelm I. war erfolglos. Schließlich erwarb Werner von Siemens das Exemplar 1879 für 20.000 Mark und übergab es als Dauerleihgabe dem Mineralogischen Museum der Humboldt-Universität zu Berlin. Zwei Jahre später erstattete die Universität dem Leihgeber Siemens den Kaufpreis. Das Exemplar gehört seitdem dem Museum für Naturkunde in Berlin und ist dort seit 2007 dauerhaft ausgestellt.

4. Das „Maxberger Exemplar“ (1956 auf der Langenaltheimer Haardt bei Solnhofen), ist ein Torso mit einigen Federabdrücken. Es befand sich bis zum Tod seines Entdeckers Eduard Opitsch 1991 in dessen Privatbesitz und gilt seitdem als verschollen.

5. Das „Haarlemer Exemplar“ (1855 in Jachenhausen bei Riedenburg) wurde schon 1855, also fünf Jahre vor der Feder gefunden, aber erst 1970 durch John Ostrom "Archaeopteryx" zugeordnet. Dieses Exemplar war durch Hermann von Meyer 1860 als "Pterodactylus crassipes" klassifiziert worden, daher hätte sein Artname "crassipes" gemäß den Prioritätsregeln der Benennung von Fossilien den Namen "litographica" ersetzen müssen. Dies wurde durch energischen Einsatz von Ostrom verhindert. Das Fragment ist in Besitz des Teylers Museum, Haarlem. Eine Ende 2017 publizierte Neuuntersuchung des Fossils ergab, das es sich nicht um "Archaeopteryx" handelt, sondern zu einem nah mit "Anchiornis", einem kleinen, vogelähnlichen, aber nicht flugfähigen Dinosaurier, verwandten Theropoden. Er wurde unter dem wissenschaftlichen Namen "Ostromia" beschrieben.
6. Das „Eichstätter Exemplar“ (1951 in Workerszell bei Eichstätt) galt zunächst als kleiner Raubdinosaurier "Compsognathus", wurde 1973 wiederentdeckt und 1974 von Peter Wellnhofer beschrieben. Das Stück befindet sich im Besitz des Jura-Museums in Eichstätt.

7. Das „Solnhofener Exemplar“ wurde in den 1960er-Jahren von einem türkischen Gastarbeiter in der Nähe von Eichstätt entdeckt und zunächst ebenfalls als "Compsognathus" fehlbestimmt, 1988 aber durch Peter Wellnhofer beschrieben. Es befindet sich im Bürgermeister-Müller-Museum zu Solnhofen. Dazu entschied 2001 das Oberlandesgericht Nürnberg, dass das Fossil nicht an einen Steinbruchbesitzer herausgegeben werden muss, der behauptet hatte, es sei 1985 aus seinem Besitz entwendet worden. Die Abweisung der Klage ist zwar mittlerweile rechtskräftig, doch ist die tatsächliche Herkunft immer noch nicht restlos geklärt.

8. Das „Exemplar des Solnhofener Aktienvereins“ (gefunden im Sommer 1992 in einem Steinbruch der "Solnhofer Aktien-Verein AG" auf der Langenaltheimer Haardt bei Solnhofen) kann im Paläontologischen Museum in München besichtigt werden. 1993 wurde der Fund von Peter Wellnhofer als neue Art "Archaeopteryx bavarica" in die Wissenschaft eingeführt. Die schönen Federabdrücke und das sehr gut erhaltene Skelett ermöglichten zahlreiche neue Erkenntnisse. Die von Wellnhofer als Brustbein beschriebene Struktur hat sich allerdings nach neueren Untersuchungen als Teil des Rabenbeins erwiesen. Die möglicherweise doch recht guten Flugfähigkeiten bleiben dabei allerdings erhalten, da das Brustbein wohl als verknorpelte Struktur vorhanden war. Auch zahlreiche Details des Schädels, des Kiefers und des Schwanzes des elstergroßen Urvogels öffneten neue Blickwinkel auf die Evolution der Vögel. Durch diese Besonderheiten gehört dieser letzte große Fund zweifellos zu den drei bedeutendsten, manche halten ihn sogar für schöner als das Berliner Exemplar.
9. Ein sehr fragmentarischer, neunter Fund (Exemplar Nr. 8) war seit 1997 nur durch einen Abguss dokumentiert, und lange Zeit waren Besitzer und Aufbewahrungsort unbekannt. 2009 präsentierte der Fossilienhändler Raimund Albersdörfer das von ihm erworbene Exemplar auf den Münchner Mineralientagen erstmals der Öffentlichkeit. Das Fossil wurde Ende der 1980er Jahre auf dem Gemeindegebiet von Daiting gefunden.

10. Im Jahr 2004 wurde über einen weiteren, ebenfalls fragmentarischen Fund berichtet, der sich jetzt im Bürgermeister-Müller-Museum Solnhofen befindet.

11. Das „Thermopolis-Exemplar“ wurde 2005 vom Besitzer des Wyoming Dinosaur Center in Thermopolis gekauft und unter anderem von Gerald Mayr untersucht, die Ergebnisse wurden in der Science-Ausgabe vom Dezember 2005 veröffentlicht. Herausragend an dem neuen Exemplar ist neben seinem äußerst guten Erhaltungszustand die Tatsache, dass erstmals der Kopf von oben zu sehen ist und der Mittelfußknochen einen nach oben gerichteten Fortsatz aufweist.

12. Im Jahr 2011 wurde erstmals das 11. Exemplar der Öffentlichkeit präsentiert, das bereits vor mehreren Jahrzehnten im Raum Eichstätt entdeckt worden war. Es zeichnet sich durch eine besonders gute Federerhaltung – ähnlich dem „Berliner Exemplar“ – aus. Ein Münchner Paläontologen-Team der Ludwig-Maximilians-Universität/Bayer. Staatssammlung für Paläontologie und Geologie stellte die weitreichenden Ergebnisse in der Nature-Ausgabe vom Juli 2014 vor (mit Archaeopteryx als Titelbild).
13. Das bislang letzte (12.) Exemplar wurde 2010 von einem Finder, dessen Name geheim gehalten wird, in einem Erlebnissteinbruch in Schamhaupten im Landkreis Eichstätt am Ostrand des Köschinger Forstes entdeckt. Mit einem Alter von 153 Millionen Jahren ist es vermutlich das bisher älteste Archaeopteryx-Exemplar. Noch im Besitz des Finders, wird das Fossil von den Eigentümern des Steinbruchs beansprucht, um es musealen Zwecken zuzuführen. Es wird im Museum des im August 2016 eröffneten Dinosaurier-Park Altmühltal, etwa 10 km vom Fundort entfernt, ausgestellt.

Die meisten der "Archaeopteryx"-Exemplare zeigen eine verkrümmte Halswirbelsäule. Diese typische Rückwärtskrümmung bildete sich erst nach dem Tode der Tiere im Wassergrab heraus.

"Archaeopteryx" wird in der Regel als der urtümlichste Vogel definiert, das heißt, alle Arten, die mit den Modernen Vögeln (Neornithes) entfernter verwandt sind als "Archaeopteryx", gelten als den Vögeln nicht zugehörig. Unter den mesozoischen Vögeln gibt es manche, die nur wenig näher mit den heutigen Vögeln verwandt sind als "Archaeopteryx": "Rahonavis" und "Jeholornis" haben mit "Archaeopteryx" unter anderem den langen knöchernen Schwanz gemein, den höher entwickelte Vögel (Pygostylia) abgelegt haben.

Innerhalb der theropoden Dinosaurier gelten die Deinonychosaurier als nahe Verwandte der Vögel – beide Gruppen werden in der übergeordneten Gruppe Paraves (Eumaniraptora) zusammengefasst. Ein Merkmal, das die Deinonychosaurier mit "Archaeopteryx" und "Rahonavis" verbindet, ist die überdehnbare („hyperextensible“) zweite Klaue des Fußes. Federn mit einer geschlossenen Federfahne waren vermutlich bereits bei den Vorfahren der Paraves ausgeprägt.

Nach Benton (2005) liegen innerhalb der Paraves folgende Verwandtschaftsverhältnisse vor:
Es gibt eine Anzahl stammesgeschichtlicher Analysen, die zu abweichenden Ergebnissen bezüglich der Verwandtschaftsverhältnisse vogelähnlicher Deinonychosaurier und urtümlicher Vögel gelangen. Nach der Hypothese von Mayr u. a. waren manche gefiederten Dromaeosauriden wie "Microraptor" näher mit den Pygostylia verwandt als "Archaeopteryx"; in der Gruppe der Vögel wäre demnach der Vogelflug womöglich mehrfach unabhängig voneinander entstanden.

Kritiker der Dinosaurier-Abstammung der Vögel meinen, die Ähnlichkeit der Urvögel zu theropoden Dinosauriern sei auf massive konvergente Evolution zurückzuführen. Nach Feduccia u. a. (2005) seien die Vögel aus einer noch zu bestimmenden Gruppe urtümlicher Archosaurier hervorgegangen, während "Microraptor" und andere gefiederte Dinosaurier in Wirklichkeit keine Dinosaurier, sondern frühe Formen flugunfähiger Vögel darstellten.

Der Einbezug eines 2011 aufgrund neuer Fossilfunde benannten "Archaeopteryx"-ähnlichen Dinosauriers der spätjurazeitlichen Tiaojishan-Formation (Liaoning, China) in eine stammesgeschichtliche Analyse führte dazu, dass die bisher angenommenen Verwandtschaftsverhältnisse von "Archaeopteryx" angezweifelt wurden. Die Analyse ergab, dass "Archaeopteryx" zusammen mit der neuen Gattung "Xiaotingia", die in manchen Merkmalen Deinonychosauriern gleicht, näher mit den Deinonychosauriern als mit späteren Vögeln verwandt gewesen sein könnte:
Dieser Hypothese zufolge wäre "Archaeopteryx", je nach Definition der Gruppe der Vögel, entweder nicht den Vögeln zugehörig, oder die Vögel schlössen auch die Deinonychosaurier mit ein.

Ob die "Archaeopteryx"-Funde einer oder mehreren Arten angehören, ist seit der Erstbeschreibung des Berliner Exemplars kontrovers diskutiert worden, was sich in der Häufung synonymer Gattungs- und Artnamen, die für dieselben Exemplare vergeben und wieder revidiert wurden, widerspiegelt.

Elzanowski ordnete in seinen Revisionen die damals bekannten Exemplare vier verschiedenen Arten zu, wobei er für das Solnhofener Exemplar, das seiner Ansicht nach anatomisch besonders deutlich von den anderen abweicht, die Zugehörigkeit zu einer eigenen Gattung vorschlug:

Der Befund, dass die bisher bekannten "Archaeopteryx"-Exemplare nicht einer einzigen Art angehören, wird von Mayr u. a. (2007) bestätigt. In ihrer Beschreibung des Thermopolis-Exemplars vergleichen sie den bislang letzten Fund mit vormals bekanntem Fossilmaterial von "Archaeopteryx". Entgegen der Analyse von Elzanowski kommen sie zu der Schlussfolgerung, dass die bestehenden Unterschiede der zehn Skelettfunde nicht mehr als zwei Arten rechtfertigen:




</doc>
<doc id="223" url="https://de.wikipedia.org/wiki?curid=223" title="Absoluter Nullpunkt">
Absoluter Nullpunkt

Der absolute Nullpunkt bezeichnet den unteren Grenzwert für die Temperatur. Dieser definiert den Ursprung der absoluten Temperaturskala und wird als 0 K festgelegt, das entspricht −273,15 °C. Die Existenz und der extrapolierte Wert des absoluten Nullpunkts können aus dem ersten Gesetz von Gay-Lussac abgeleitet werden. 

Nach dem dritten Hauptsatz der Thermodynamik ist der absolute Nullpunkt eine ideale Messgröße, welche nicht erreichbar ist, jedoch können reale Temperaturen beliebig nahe dem absoluten Nullpunkt realisiert werden. Mit Laserkühlung konnten Proben schon bis auf wenige Milliardstel Kelvin abgekühlt werden.

Guillaume Amontons fand 1699 heraus, dass sich das Volumen einer Gasmenge linear mit ihrer Temperatur verändert. Da das Volumen eines Gases aber nicht negativ werden sollte, folgerte er, dass es einen absoluten Nullpunkt geben müsse, bei dem das Volumen der Gasmenge gleich null wäre. Durch Extrapolation seiner Messwerte schätzte er die Lage dieses Nullpunkts ab und kam auf einen Wert von minus 248 Grad Celsius. Diese Methode muss jedoch sehr kritisch betrachtet werden, da die Gesetzmäßigkeit der Volumenverkleinerung nur für ideale Gase gilt, nicht aber für Stoffe, die ihren Aggregatzustand ändern, beispielsweise flüssig werden.

William Thomson, 1. Baron Kelvin, entdeckte 1848, dass nicht die Volumenverkleinerung für diese Frage entscheidend ist, sondern der Energieverlust. Hierbei ist es unerheblich, ob es sich um Gase oder feste Stoffe handelt. Thomson schlug daraufhin vor, eine neue, "absolute" Temperaturskala zu definieren, zu der die Volumenänderung proportional ist. Diese neue Temperaturskala hat keine negativen Werte mehr, beginnt bei null (dies entspricht minus 273,15 Grad Celsius, siehe dazu Eigenschaften der Kelvinskala) und steigt so an, dass ein Temperaturunterschied von einem Kelvin jeweils einem Temperaturunterschied von einem Grad Celsius entspricht. Diese gleiche Schrittweite wurde erreicht durch die Festlegung, dass das Kelvin der 273,16-te Teil der thermodynamischen Temperatur des Tripelpunktes des Wassers – dieser liegt bei 0,01 °C – ist. Die Einheit für diese Temperaturskala wurde zunächst "Grad A" (A für absolut) genannt, später "K" (K für Kelvin). Das Kelvin wird seit 1967 per definitionem "nicht" mehr mit Grad (°) ergänzt.

Physikalische Systeme mit Temperaturen nahe am absoluten Nullpunkt weisen einige besondere Verhaltensweisen wie Suprafluidität und Bose-Einstein-Kondensation auf. Diese Temperaturgebiete der Tieftemperaturphysik können nur noch mit besonderen Methoden erreicht werden.

Bei Normaldruck sind am Nullpunkt alle Elemente fest, abgesehen von Helium, das sich dort in einer flüssigen bzw. suprafluiden Phase befindet.

Thermodynamische Aussagen über den Nullpunkt im Zusammenhang mit der Entropie macht das Theorem von Nernst.
Perfekte Kristalle erreichen beim Nullpunkt für die Entropie formula_1 einen konstanten Wert formula_2, da die Entropie gemäß der statistischen Definition als der mit der Boltzmannkonstanten formula_3 multiplizierte Logarithmus der Anzahl der möglichen Mikrozustände definiert ist und es nur eine mögliche Realisierung des beobachteten Makrozustands gibt. Bei (amorphen) Gläsern gibt es mehrere gleichenergetische Realisierungen eines Zustands mit formula_4, so dass die Entropie von null verschieden ist.



</doc>
<doc id="225" url="https://de.wikipedia.org/wiki?curid=225" title="Antonym">
Antonym

Antonyme (von und ) sind in der Sprachwissenschaft Wörter mit gegensätzlicher Bedeutung. In gleicher Bedeutung werden auch die Ausdrücke Gegensatzwort (oder (kürzer) Gegenwort) und Oppositionswort verwandt. Zwei Wörter, die füreinander Gegensatzwörter sind, heißen "Gegensatzpaar". Die zwischen ihnen bestehende Relation heißt Antonymie, insbesondere von Wörtern, aber auch von Sätzen und Phrasen. Die entsprechende rhetorische Figur ist das Oxymoron.

Der Begriff der Antonymie kann dabei nach der Ebene und Art des Gegensatzes unterschiedliche Ausprägungen erfahren. Die Art der Antonymie hängt inhaltlich davon ab, wie der Gegensatz im logischen Sinn zu verstehen ist, ob er etwa innerhalb eines Oberbegriffes gesucht wird oder ob ein konträres oder kontradiktorisches Verhältnis der mit dem Gegensatzpaar bezeichneten Begriffe vorliegt. Ein Ausdruck, der für beide Begriffe eines Gegensatzpaares stehen kann, heißt Oppositionswort.

In der deutschen Sprache werden in vielen Fällen Antonyme auch durch das Voranstellen der Vorsilbe "un-" gebildet: etwa Ruhe – Unruhe; klar – unklar usw. Jedoch gibt es nicht automatisch derartige Antonympaare, beispielsweise hat "ungefähr" kein Gegenüber "gefähr"; ebenso ist für "unausbleiblich" kein "ausbleiblich" in Benutzung. Darüber hinaus gibt es Wörter mit "un-", die aber zum Stammwort kein Antonym bilden, zum Beispiel Mut und Unmut; ziemlich und unziemlich.

Ein weiterer Aspekt ist, dass verschiedene Oppositionen von Wortpaaren nicht automatisch auf andere übertragbar sind. So sind zwar Überführung und Unterführung (Verkehrswege) Antonyme, aber Übergang und Untergang haben keinen vergleichbaren Sinn und Gegensinn, sondern bedeuten etwas völlig anderes, nichts direkt Gegensätzliches.

Es lassen sich verschiedene Arten von Antonymie unterscheiden:
Ursprünglich sprach man von Antonymie nur im Sinne von gradueller oder auch konträrer Antonymie und bezeichnete damit Adjektivpaare wie beispielsweise "schön"/"hässlich". Teilweise spricht man auch heute noch von Antonymie nur dann, um einen „Bedeutungsgegensatz(.) zwischen skalierbaren lexikalischen Ausdrücke(n)“ zu bezeichnen. In logischer Perspektive ist die graduelle Antonymie bei einem konträren Gegensatz gegeben (Beispiel: "kalt"/"heiß").

Ein Sonderfall der graduellen oder konträren Antonymie ist der polar-konträre Gegensatz, bei dem die gegensätzlichen Bedeutungen am Ende einer Skala sind (Beispiel: "neu"/"alt"). „Nicht-polare Antonyme bezeichnen den gleichen Ausprägungsgrad auf entgegengesetzten Skalen; die Bildung konverser Komparative ist ausgeschlossen“.

Der Ausdruck der Antonymie wird häufig auch in einem weiteren Sinn verwandt, bezeichnet dann allgemein einen Oberbegriff für „semantische Gegensatzrelationen“ und erfasst dann auch den Fall des kontradiktorischen Gegensatzes, der in der Semantik auch als komplementärer Gegensatz "(siehe unten)" bezeichnet wird (Beispiel: "tot"/"lebendig"; "sinnvoll"/"sinnlos").

Man bezeichnet die kontradiktorische Antonymie auch als „Antonymie im strengen Sinne“, während die konträre Antonymie auch als „Antonymie im eigentlichen Sinn“ oder „gelegentlich“ auch als Antonymie im engeren Sinn bezeichnet wird. Die Terminologie ist also alles andere als klar.

Antonymie als Fall (konträrer oder kontradiktorischer) gegensätzlicher Bedeutung ist ein „Sonderfall“ unvereinbarer Bedeutung, das heißt einer "Inkompatibilität" (von Wörtern etc.).

Wird wie hier auch die Unvereinbarkeit im Fall der Kohyponymie als Antonymie angesehen, wird die Antonymie – losgelöst von der Wortbedeutung – mit jedweder Inkompatibilität gleichgesetzt und gleichzeitig der Ausdruck "Inkompatibilität" in einem engeren als üblichen Sinn verwendet.

Auto-Antonyme (Antagonyme) sind Wörter, die gleichzeitig in gegenteiliger Bedeutung verwendet werden können. Das Wort verfügt also über zwei Bedeutungen (Homonymie, Polysemie oder Homophonie), die eine antonymische Opposition bilden. Im Deutschen tritt dies beispielsweise bei dem Ausdruck "Untiefe" auf, das als "sehr geringe Tiefe" oder in der Umgangssprache auch als "sehr große Tiefe" gedeutet werden kann. Gleiches gilt für "Unkosten". Das Englische "overlook" kann sowohl ‚überwachen‘ als auch ‚nicht beachten‘ bedeuten. (Vergleiche: "übersehen" hat die beiden Bedeutungen "überblicken" und "nicht beachten": Ich übersehe die Lage noch nicht. Ich habe den Brief übersehen.) Ein weiteres Beispiel: "transparent" wird sowohl für "durchsichtig, offengelegt" wie auch für "unsichtbar, verbergend" verwendet, insbesondere wenn man keine Materialeigenschaft beschreibt.

Viele Wörter sind Homonyme, d. h., sie haben mehrere Bedeutungen. Homonyme Wörter stellen keine Bedeutungsbeziehung dar. Lediglich die Ausdrucksseite ist identisch, die Inhaltsseite hat nichts miteinander zu tun, auch nicht historisch.

Da die Antonymierelation von der Bedeutung abhängt, gibt es in diesen Fällen auch mehrere Gruppen von Antonymen. Zum Beispiel hat das Wort "abnehmen" Antonyme in den Bedeutungsgruppen übergeben (Ware), aufhängen (Bild), aufdecken (Tischtuch), auflegen (Telefonhörer), anlegen (Schmuck), aufhängen (Gardine), aufsetzen (Hut), wachsen bzw. stehen lassen (Bart), zunehmen (Mond), zunehmen (Gewicht) und weiteren. Man spricht auch von "Antonymengabel". Deren Auftreten kann auch helfen, verdeckte Mehrdeutigkeiten festzustellen.




</doc>
<doc id="226" url="https://de.wikipedia.org/wiki?curid=226" title="Akkumulator">
Akkumulator

Ein Akkumulator oder Akku ist ein wiederaufladbarer Speicher für elektrische Energie auf elektrochemischer Basis. Das lateinische Wort "accumulator" bedeutet „Sammler“ ( ‚Haufen‘, ‚anhäufen‘). Eine frühere Bezeichnung für Akkumulatoren war "Sammler".

Ein einzelnes wiederaufladbares Speicherelement wird Sekundärelement oder Sekundärzelle genannt, im Gegensatz zur nicht (oder nur sehr begrenzt) wiederaufladbaren Primärzelle. Sekundärzellen lassen sich – wie Primärzellen und alle elektrische Energiequellen – zusammenschalten, entweder in Reihenschaltung (zur Steigerung der nutzbaren elektrischen Spannung) oder aber in Parallelschaltung (zur Steigerung der nutzbaren Kapazität beziehungsweise wegen der Eignung für höhere Stromstärken). Beide Schaltungsvarianten führen zur entsprechenden Erhöhung des Gesamt-Energiegehalts (Produkt aus Kapazität und Spannung, angegebenen in Wattstunden (Wh)) der Anordnung.

Bei jedem Akkumulatortyp ist die Nennspannung der Akkumulatorzelle durch die verwendeten Materialien festgelegt; da jene für die meisten Anwendungen zu gering ist, wird häufig die Reihenschaltung angewandt, um die Spannung zu erhöhen (siehe Bild Starterbatterie). Die Kapazität und die mögliche Stromstärke hängen dagegen von der Baugröße ab. Deshalb ist eine Parallelschaltung mehrerer Zellen in der Regel nicht nötig; stattdessen verwendet man einen Akku mit entsprechend groß dimensionierten Zellen.

Ursprünglich war mit "Akkumulator" ein einzelnes wiederaufladbares Speicherelement gemeint (Sekundärzelle). Heute bezeichnet der Begriff – zumindest in der Allgemeinsprache – auch wiederaufladbare Speicher, die aus zusammengeschalteten Sekundärzellen bestehen. Wenn es auf den Unterschied ankommt, sollte man genauere Bezeichnungen verwenden:

Im technischen Sinn ist eine Batterie eine Kombination mehrerer gleichartiger galvanischer Zellen bzw. Elemente, die in Reihe zusammengeschaltet sind. Es gibt Batterien aus Primärzellen (nicht wiederaufladbar) und solche aus Sekundärzellen (wiederaufladbar). Ursprünglich waren mit "Batterien" nur solche aus Primärzellen gemeint. Seit der Ausbreitung der wiederaufladbaren Speicher ist diese einschränkende Definition veraltet.

In der Umgangssprache dient "Batterie" jedoch als Oberbegriff für (echte) Batterien, Primärzellen und Sekundärzellen. Es wird deshalb oft von „Batterien“ gesprochen, wenn eigentlich nur einzelne Primärzellen oder Sekundärzellen (Akkumulatorzellen) gemeint sind.

Beide Zellentypen sind in untereinander austauschbaren Baugrößen auf dem Markt, und beide werden im Englischen genannt, was zur Verwirrung beitragen dürfte. Akkuzellen sind im Englischen (dt. „wiederaufladbare Batterien“) oder .

Elektrische Verbraucher, die sowohl mit Primär- als auch mit Sekundärzellen betrieben werden können, werden deshalb oft einfach "batteriebetrieben" genannt. Nur dann, wenn im täglichen Umgang mit dem Gerät die Wiederaufladbarkeit eine besondere Rolle spielt, bevorzugt man die Bezeichnung "akkubetrieben". Im technisch-wissenschaftlichen Rahmen spricht man wegen der Dominanz des Englischen zunehmend von „wiederaufladbaren Batterien“ oder „sekundären Batterien“.

Kondensatoren speichern ebenfalls elektrische Energie und geben diese wieder ab, allerdings speichern sie die Energie nicht in chemischer Form, sondern in einem elektrischen Feld zwischen den Kondensatorplatten. Kondensatoren sind daher keine Akkumulatoren.

Die erste Vorform eines Akkumulators, der – im Gegensatz zu den Zellen von Alessandro Volta – nach der Entladung wiederaufladbar war, wurde 1803 von Johann Wilhelm Ritter gebaut. Den bekanntesten Akkutyp, den Bleiakkumulator, entwickelte 1854 der Mediziner und Physiker Wilhelm Josef Sinsteden. 1859 entwickelte Gaston Planté Sinstedens Erfindung durch eine spiralförmige Anordnung der Bleiplatten erheblich weiter. Um die Wende zum 20. Jahrhundert speisten von Holz umfasste Bleiakkumulatoren Elektroantriebe für Automobile. Die Akkutechnik nahm in der Zeit eine rasante Entwicklung. Folgender vom Telegraphentechnischen Reichsamt 1924 veröffentlichter Text zeigt das am Beispiel der damals etablierten Telegrafie und noch jungen Telefonie. Akkumulatoren werden hier „Sammler“ genannt, und „Batterien“ waren Ansammlungen galvanischer Elemente:

In einem Akkumulator wird beim Aufladen elektrische Energie in chemische Energie umgewandelt. Wird ein Verbraucher angeschlossen, so wird die chemische Energie wieder in elektrische Energie zurückgewandelt ("siehe dazu: Galvanische Zelle"). Die für eine elektrochemische Zelle typische elektrische Nennspannung, der Wirkungsgrad und die Energiedichte hängen von der Art der verwendeten Materialien ab.

Die Akkumulatortypen werden nach den verwendeten Materialien bezeichnet:


Für viele Anwendungen, insbesondere für mobile Geräte im Bereich der Unterhaltungselektronik, Hörgeräte oder auch Fahrzeuge, ist die Energiedichte von Bedeutung. Je höher diese ist, desto mehr Energie kann in einem Akku je Volumen- bzw. Masseneinheit gespeichert werden. Die auf die Masse bezogene Energiedichte wird oft auch als spezifische Energie bezeichnet. Bezogen auf marktübliche Typen, haben Akkumulatoren (Sekundärzellen) meist eine (oftmals deutliche) geringere Energiedichte als Primärzellen.

Oft sind Akkus mit besonders hoher Energiedichte überproportional teuer oder haben andere Nachteile, insbesondere eine beschränkte Lebensdauer. So kosten Bleiakkumulatoren typischerweise 100 €/kWh; Li-Ion-Akkus hingegen derzeit (2012) typischerweise 350 €/kWh (200 €/kWh 2013), Tendenz fallend. Ursachen sind die anlaufende Massenproduktion, welche die Stückkosten durch bessere Technik und Skaleneffekte deutlich verringern. Allerdings werden die sinkenden Produktionskosten nur verzögert an die Kunden weitergegeben, da auf diesem Markt, speziell in Deutschland, durch die wenigen Angebote nur ein geringer Preisdruck besteht.

Beim Aufladen und Entladen von Akkumulatoren wird durch den inneren Widerstand der Zellen Wärme freigesetzt, wodurch ein Teil der zum Aufladen aufgewandten Energie verloren geht. Das Verhältnis der entnehmbaren zu der beim Laden aufzuwendenden Energie wird als Ladewirkungsgrad bezeichnet. Generell sinkt der Ladewirkungsgrad sowohl durch Schnellladung mit sehr hohen Strömen als auch durch schnelle Entladung (Peukert-Effekt), da die Verluste am Innenwiderstand zunehmen. Das optimale Nutzungsfenster ist dabei je nach Zellchemie stark unterschiedlich.

Ein Vergleich zur Speicherung elektrischer Energie zeigt die Vor- und Nachteile von Akkus gegenüber anderen Speicherverfahren.

Die Ladungsmenge, die ein Akkumulator speichern kann, wird in Amperestunden (Ah) angegeben und als Kapazität (Nennkapazität) bezeichnet. Diese darf nicht verwechselt werden mit der Kapazität eines Kondensators, die in Amperesekunde pro Volt (As/V) definiert ist und in der Einheit Farad (F) angegeben wird. Die angegebene Nennkapazität beim Akku bezieht sich immer auf einen bestimmten Entladestrom und nimmt - je nach Akkutyp unterschiedlich stark - mit höheren Entladeströmen ab.

Ein wichtiger Kennwert von mit Sekundärbatterien betriebenen Geräten ist der "Ladezustand" von Akkumulatoren (, SoC bzw. SOC). Er wird üblicherweise in Prozentwerten angegeben, wobei 100 % einen vollständig geladenen Akkumulator repräsentieren. 100 % minus den Wert des Ladezustands ergibt den Entladungsgrad (DoD bzw. DOD).

Zur Bestimmung sind verschiedene Methoden gebräuchlich: chemische, spannungsabhängige, Strom-integrative (Ladungsbilanzierung), druckabhängige sowie die Messung der Akkumulator-Impedanz.

Wird ein Akku nicht verwendet, so verliert er über die Zeit einen Teil seiner gespeicherten Energie. Diesen Vorgang nennt man Selbstentladung. Das Maß der Selbstentladung hängt von Typ und Alter des Akkumulators sowie von der Lagertemperatur ab.

Für die Lagerung von Akkus wird meistens folgendes empfohlen:


Sanyo hat 2005 (Markteinführung in Europa August 2006) einen modifizierten NiMH-Akku namens Eneloop auf den Markt gebracht, der einer Selbstentladung von lediglich 15 % pro Jahr unterliegt. Es handelt sich hierbei um sogenannte LSD-Akkus (Low Self Discharge), die aufgrund ihrer geringen Selbstentladung als bereits vorgeladene Akkus verkauft werden und daher im Gegensatz zu herkömmlichen Akkus vor der ersten Benutzung durch den Käufer nicht aufgeladen werden müssen.

Alle Angaben zur Selbstentladung beziehen sich auf eine Raumtemperatur von ca. 20 °C.

Lithium-Eisenphosphat-Akkumulatoren erreichen nach Herstellerangaben mehr als 5000 Zyklen bei jeweiliger Entladetiefe von 70 %. Als weltgrößter Hersteller von Lithium-Eisenphosphat-Akkumulatoren gilt BYD, der durch präzise Fertigung eine große Auswahl an Zellen für zyklenfeste Anwendungen, wie zum Beispiel im Einsatz bei stationären Speichersystemen entwickelt hat. Nach 7500 Zyklen mit einem Entladungsgrad von 85 % haben diese noch eine Restkapazität von mindestens 80 % bei einer Rate von 1 C; das entspricht bei einem Vollzyklus pro Tag einer Lebensdauer von mind. 20,5 Jahren.

Der Lithium-Eisenphosphat-Akkumulator Sony Fortelion hat nach 10.000 Zyklen mit 100 % Entladungsgrad noch eine Restkapazität von 71 %. Dieser Akkumulator ist seit 2009 auf dem Markt.

In Solarbatterien eingesetzte Lithium-Ionen-Akkumulatoren weisen teilweise eine sehr hohe Zyklenfestigkeit von mehr als 10000 Lade- und Entladezyklen und eine lange Lebensdauer von bis zu 20 Jahren auf.

Die Organisation "Plug in America" führte unter Fahrern des Tesla Roadster eine Umfrage durch bezüglich der Lebensdauer der verbauten Akkus. Dabei ergab sich, dass nach 100.000 Meilen = 160.000 km die Akkus noch eine Restkapazität von 80 bis 85 Prozent angegeben wurde (Keine Angaben über die Messung in der Quelle). Dies war unabhängig davon, in welcher Klimazone das Fahrzeug bewegt wurde. Der Tesla Roadster wurde zwischen 2008 und 2012 gebaut und verkauft. Für seine 85-kWh-Akkus im Tesla Model S gibt Tesla 8 Jahre Garantie mit unbegrenzter Laufleistung.

Varta Storage gibt auf seine Produktfamilie Engion Family und engion home eine Garantie von 14.000 Vollzyklen und einer Lebensdauer von 10 Jahren.

Das meistverkaufte Elektroauto ist der Nissan Leaf, welcher seit 2010 produziert wird. Nissan gab 2015 an, dass bis dahin nur 0,01 Prozent der Akkus wegen Defekt oder Problemen ausgetauscht werden mussten und das auch nur aufgrund extern zugefügter Schäden. Dabei gibt es vereinzelt Fahrzeuge, die bereits mehr als 200.000 km gefahren sind. Auch diese hätten keine Probleme mit dem Akku.

Die Ladezeit eines Akkus bzw. einer Batterie aus Akkumulatorzellen ist abhängig von verschiedenen Faktoren. Dazu zählen Parameter wie der Innenwiderstand, der direkten Einfluss auf den Ladestrom hat und wiederum von der Temperatur beeinflusst wird. Kürzere Ladezeiten bedeuten höhere Strombelastung und höheren Verschleiß, stehen also im Zielkonflikt zur Lebensdauer des Akkumulators. Je nach Anwendung, Zellchemie und technischer Umsetzung (Klimatisierung, Überwachung) sind daher die praktisch erreichbaren Ladezeiten sehr unterschiedlich.

Der vom Hersteller empfohlene /zulässige Ladestrom wird dabei über den C-Faktor beschrieben und ist u. a. auch abhängig vom Ladezustand. Die Ladespannung ist durch die Zellchemie und den Batterieaufbau bestimmt. Aus diesen beiden Parametern ergibt sich eine obere Grenze der maximalen Ladeleistung, die zugunsten einer höheren Lebensdauer oft noch reduziert wird. Die praktisch erreichbaren Ladezeiten sind daher meist höher als die technisch möglichen Ladezeiten. Als äußere Faktoren sind neben der Temperatur die zur Verfügung stehende Spannungs- und Stromquelle und das angewendete Ladeverfahren zu nennen. Die Akkuzellhersteller geben die einzuhaltenden Parameter und Nutzungsfenster in ihren Datenblättern vor, die von den Herstellern der Endprodukte beachtet werden müssen.

Für klassische Akkus, wie Blei, NiCd und NiMH sind Laderaten bei Normalladung von 0,1 C bis 0,2 C üblich. Das entspricht Ladezeiten von 5-10 Stunden. Bei modernen Lithiumakkus ist in den Datenblättern der Hersteller meist mit 0,5 C die Normalladung spezifiziert, was 2 Stunden Ladedauer entspricht. Zusätzlich wird ein maximal erlaubter, höherer Ladestrom angegeben, beispielsweise 3 C, was eine Aufladung in 20 Minuten ermöglichen würde. Praktisch sind Ladezeiten von 1,5 bis 4 Stunden im Mobilgerätebereich üblich. Elektroautos wie Tesla Model S, Renault Zoe, BMW i3, Nissan Leaf usw. können ihre Akkus an derzeitigen (2017) Schnellladestationen innerhalb von etwa 30 Minuten zu 80 Prozent aufladen.
Allerdings können heutige Lithium-Akkus oft auch deutlich schneller geladen werden. Im Modellbaubereich sind Ladezeiten von 10 bis 15 Minuten bei Schnellladung üblich. Die Obergrenze der Ladeleistung wird gerade bei größeren Batterien von Elektrofahrzeugen in der Praxis nicht mehr von den Akkumulatorzellen, sondern vom Aufbau der Traktionsbatterie (Klimatisierung) und von der verfügbaren Ladetechnik bestimmt. So können neue Schnellladesysteme Elektroautos mit entsprechend konstruierten Traktionsbatterien innerhalb von etwa 15 min zu 80 Prozent aufladen.

Forscher der Justus-Liebig-Universität Gießen haben zusammen mit Wissenschaftlern der BASF SE eine neue reversibel arbeitende Zelle auf Basis von Natrium und Sauerstoff entwickelt. Als Reaktionsprodukt tritt hierbei Natriumsuperoxid auf.

Wissenschaftler der Universität Oslo aus Norwegen haben einen Akkumulator entwickelt, der unterhalb einer Sekunde wieder aufgeladen werden kann. Nach Meinung der Wissenschaftler wäre dieser Akkumulator interessant u. a. für Stadtbusse, die so an jeder Haltestelle geladen werden könnten und somit nur eine relativ kleine Batterie benötigen würden. Ein Nachteil ist, so die Forscher, dass je größer die Batterie ist, je größer muss auch der Ladestrom sein. Somit kann der Akku nicht sehr groß sein. Nach Angaben der Forscher könnte der neuartige Akku auch als Puffer in Sportwagen eingesetzt werden, um kurzfristig Leistung bereitzustellen. Vorerst denken die Forscher aber an Einsatzbereiche in Klein- und Kleinstgeräten.

In Laboratorien der Firma StoreDot aus Israel können Berichten zufolge erste Labormuster von nicht näher spezifizierten Akkus in Mobiltelefonen (Akkukapazität im Bereich um 1 Ah) mit Stand April 2014 in 30 Sekunden geladen werden.

Forscher aus Singapur haben 2014 einen Akku entwickelt, der nach 2 Minuten zu 70 Prozent aufgeladen werden kann. Die Akkus setzen auf die Lithium-Ionen-Technik. Jedoch besteht die Anode, der negative Pol in der Batterie, nicht mehr aus Graphit, sondern einem Titandioxid-Gel. Das Gel beschleunigt die chemische Reaktion deutlich und sorgt so für ein schnelleres Aufladen. Insbesondere sollen diese Akkus in Elektroautos verwendet werden. Bereits im Jahr 2012 haben Forscher der Ludwig-Maximilian-Universität in München das Grundprinzip entdeckt.

Festkörperakkumulatoren sind eine spezielle Bauform, bei welchem beide Elektroden und auch der Elektrolyt aus verschiedenen, festen Materialien bestehen. Da keine Flüssigkeiten vorhanden sind, gibt es kein Problem mit Undichtigkeiten, sollte der Akkumulator beschädigt werden.

Auch wird an Akkumulatoren aus organischem Material gearbeitet.

Wissenschaftler der Stanford-Universität in Kalifornien haben einen neuartigen Akku mit sehr günstigen Eigenschaften entwickelt. Bei dem Aluminium-Ionen-Akkumulator besteht die Anode aus Aluminium und die Kathode aus Grafit. Der Akku schafft mehr als 7500 Ladezyklen ohne Qualitätseinbußen. Die zur Fertigung des Akkus notwendigen Materialien sind sehr kostengünstig und zudem sehr leicht. Der Akku kann nicht in Brand geraten, selbst wenn man den Akku durchbohrt. Der Ladevorgang beträgt eine Minute. Zudem ist der Akku biegsam und kann somit in eine gewünschte Form gebogen und gefaltet werden. Der Akku ist noch nicht marktreif, da die Spannung und die Energiedichte noch zu gering sind.

Nach Schätzungen werden bis 2025 bzw. spätestens 2030 die Lithium-Schwefel wie auch die Lithium-Luft-Akkutechnologie im Automobilbereich einsetzbar sein. Beide haben eine höhere Energiedichte als die im Jahr 2015 eingesetzte Lithium-Ion-Technologie und versprechen höhere Reichweiten in der Elektromobilität.

Ein Team, angeführt von "Yan Yu", an der "Chinesische Universität für Wissenschaft und Technik" in Hefei haben einen Akku entwickelt, der eine hohe Kapazität und Spannung aufweist, auch wenn er 2.000 Mal ge- und entladen wurde (96% Kapazität blieben erhalten). Er basiert auf Tri-Natrium-Di-Vanadium-Triphosphat (NaV(PO)) im Innern eines Graphenemischmaterials.

Bleiakkumulatoren kosten typischerweise 355 €/kWh. Die Preise für Li-Ion-Akkus sind in den letzten Jahren deutlich gefallen: 2007 lagen die Kosten noch bei mehr als 1000 US-Dollar/kWh, 2014 noch 300 Dollar/kWh, Tendenz weiter fallend. So gab z. B. die Chefin von General Motors, Mary Barra bekannt, dass die Akkukosten des Chevrolet Bolt, dessen Serienfertigung Ende 2016 anläuft, bei ca. 145 Dollar/kWh liegen sollen. Für 2022 rechnet sie mit Akkukosten von 100 Dollar/kWh. Eric Feunteun, Leiter der Sparte Elektromobile bei Renault, teilte im Juli 2017 mit, dass Renault eine kWh Akku 80 Dollar kostet. Auch die Marktpreise für Li-Ion-Akkus einschließlich Gewinnmarge sollen bis 2030 unter 100 $/kWh sinken.

Für 2015 gab das Energieministerium der Vereinigten Staaten die Kosten von Lithium-Ionen-Akkumulatoren für Elektroautos mit ca. 250 $/kWh an; angestrebt wird ein Wert von 125 $/kWh Im Jahr 2022. Ursachen für den Preisrückgang sind die zunehmende Massenproduktion, welche die Stückkosten durch bessere Technologien und Skaleneffekte verringert hat.

Nach einer Studie von McKinsey sind die Akkupreise zwischen 2010 und 2016 um 80 Prozent gefallen.

Akkumulatoren werden oft verwendet, wenn ein elektrisches oder elektronisches Gerät ohne dauerhafte Verbindung zum festen Stromnetz oder zu einem Generator betrieben werden soll. Da sie teurer sind als nicht wiederaufladbare Primärbatterien, kommen sie vor allem in solchen Geräten zum Einsatz, die regelmäßig benutzt werden und einen nicht vernachlässigbaren Strombedarf haben, wie in Mobiltelefonen, Laptops oder Akkuwerkzeugen.

Auch in Kraftfahrzeugen dient ein Akku in Form der Starterbatterie dazu, Strom für Licht, Bordelektronik und vor allem den Anlasser zum Starten des Verbrennungsmotors zu liefern. Läuft der Motor, wird der Akkumulator über die als Generator arbeitende Lichtmaschine wieder aufgeladen. Ähnliches gilt für Schiffe und Flugzeuge.

Beim elektrischen Antrieb von Elektrofahrzeugen werden deren Akkus zur Unterscheidung von bloßen Starterbatterien dann als Traktions-Akkumulatoren bezeichnet und zu Traktionsbatterien zusammengeschaltet. Genutzt werden Traktionsbatterien in Elektroautos, Elektromotorrädern, Elektromotorrollern, Batteriebussen und Elektrolastkraftwagen. Immer beliebter werden Pedelecs, ein spezielles Elektrofahrrad. In der Entwicklung befinden sich zudem Elektroflugzeuge für Kurzstreckenflüge.

Akkus kommen in Form von Batterie-Speicherkraftwerken oder Solarbatterien auch zum Einsatz, um Schwankungen bei der regenerativen Erzeugung von Strom mit Wind und/oder Sonne auszugleichen. Batterie-Speicherkraftwerke werden u. a. eingesetzt zur Abdeckung von Spitzenlasten im Stromnetz und auch zur Netzstabilisierung in Stromnetzen. Möglich ist auch der Betrieb als Inselanlage in einem Inselnetz, wenn sich eine abgelegene Verbrauchsstelle nicht oder nur zu unverhältnismäßig hohen Kosten an das Stromnetz anschließen lässt. Oft sind solche Verbrauchsstellen zusätzlich noch mit einem Notstromaggregat ausgerüstet, das einspringt, bevor die Ladung der Akkus z. B. nach mehrtägiger Windstille nicht mehr ausreicht. Beispiele für solche Installationen sind nicht nur abgelegene Hütten, Mobilfunk-Basisstationen in wenig erschlossenen Regionen oder Weltraumsatelliten, sondern auch viele Parkscheinautomaten, bei denen ein Anschluss an das Stromnetz teurer wäre als die Installation einer Solarzelle und eines Akkumulators.

Konventionelle U-Boot-Antriebe bestehen aus Dieselmotoren mit Generatoren (Fahren und Laden der Akkumulatoren bei nicht getauchter Fahrt/Schnorcheln) und mit Akkumulatoren betriebenen Elektromotoren (Tauchfahrten).

Akkumulatoren dienen in Systemen zur unterbrechungsfreien Stromversorgung (USV) auch zur kurz- bis mittelfristigen Überbrückung von Ausfällen der stationären Energieversorgung. Wichtige Bereiche, die es mit einer Notstromversorgung abzusichern gilt, sind z. B. Rechenzentren, Alarmsysteme und lebenserhaltende Systeme in Krankenhäusern. Werden hohe Leistungen benötigt oder sind längere Zeiträume zu überbrücken, wird noch ein Dieselgenerator zusätzlich installiert; die Akkus übernehmen dann die Versorgung nur so lange, wie der Dieselgenerator zum Anspringen und Erreichen der Nenndrehzahl benötigt. Falls die so zu überbrückende Zeit nur kurz ist, können dafür auch andere Systeme als Akkumulatoren eingesetzt werden, insbesondere auf der Basis von Schwungmassen oder gar Kondensatoren.

Kriterien für die Auswahl eines Akkumulatortyps für eine bestimmte Anwendung sind unter anderem:


Aus der Anwendung der oben genannten Kriterien ergeben sich für jeden Akkutyp einige typische Anwendungsgebiete, wobei insbesondere bei NiCd-, NiMH- und Li-Ion-Akkus die Übergänge fließend sind:


Als Alternative zu Akkumulatoren werden Brennstoffzellen-Systeme diskutiert und auch schon verwendet, die elektrische Energie mit Hilfe von Wasserstoff oder Methanol aus chemischer Energie erzeugen. Brennstoffzellen erzeugen die elektrische Energie ohne exotherme Verbrennung und zusätzliche Umwandlungen. Zu beachten ist dabei, dass die Energieabgabe der Brennstoffzelle kaum variiert werden kann. In Systemen mit schwankendem Leistungsbedarf (Bsp.: Hybridelektrokraftfahrzeug) müssen deshalb immer zusätzlich auch Akkumulatoren verwendet werden, die aber in Vergleichen oft unberücksichtigt bleiben.

Bei Vergleichen mit ausschließlichem Akkumulatorbetrieb muss also korrekterweise neben der eigentlichen Brennstoffzelle auch der Raumbedarf und das Gewicht des Treibstoffbehälters (Wasserstoff-Flaschen, Methanol-Tank) sowie der notwendigen Puffer-Akkus berücksichtigt werden.

Konkurrierende Energiespeicher sind auch Hydraulikspeicher sowie elektrochemische Zellen wie die Redox-Flow-Zelle.




</doc>
<doc id="228" url="https://de.wikipedia.org/wiki?curid=228" title="André-Marie Ampère">
André-Marie Ampère

André-Marie Ampère (* 20. Januar 1775 in Lyon, Frankreich; † 10. Juni 1836 in Marseille) war ein französischer Physiker und Mathematiker. Er war der herausragende Experimentator und Theoretiker der frühen Elektrodynamik. Nach ihm ist die internationale Einheit der Stromstärke Ampere benannt.

Ampère war der Sohn von Jean-Jacques Ampère und dessen Ehefrau Jeanne-Antoinette de Sarcey. Er fiel schon früh als wissbegieriger Knabe und durch sein gutes Gedächtnis auf. Sein Vater war ein Verehrer von Jean-Jacques Rousseau und erzog Ampère nach dessen "Emile", seine Mutter sorgte für seine religiöse Verwurzelung im Katholizismus. Ampère las als Jugendlicher Buffons Naturgeschichte und systematisch die 35 Bände der Enzyklopädie von Denis Diderot und Jean d'Alembert und lernte Griechisch, Latein und Italienisch. Sein Vater wurde 1793 nach dem Fall von Lyon (während der Französischen Revolution) als Girondist hingerichtet (als Friedensrichter hatte er zuvor einen führenden Jakobiner in Lyon, Joseph Chalier, verhaften und hinrichten lassen), was bei Ampère eine tiefe Krise auslöste. Als Achtzehnjähriger befasste er sich mit den Lehrbüchern des Schweizer Mathematikers Leonhard Euler und der klassischen Mechanik von Joseph-Louis Lagrange. Im gleichen Alter entwickelte er eine Plansprache, die er als friedensförderndes Werkzeug ansah. Er wandte sich ebenfalls der Botanik, der Metaphysik und der Psychologie zu, ehe er Mathematik und Physik studierte. Nachdem das elterliche Vermögen zusammengeschmolzen war, gab er Privatunterricht besonders in Mathematik. Seine Kontakte zur Außenwelt waren aber gering.

Im Jahre 1796 lernte er Julie Carron kennen, die er 1799 heiratete. Sie war etwas älter und stammte aus einer angesehenen bürgerlichen Familie in einem Nachbarort von Ampère. Obwohl sie aus ähnlichem sozialem Hintergrund kamen, war Ampère keine gute Partie und er warb lange und hartnäckig um sie, was in seinem Tagebuch dokumentiert ist. 1800 wurde ihr Sohn Jean-Jacques Ampère geboren, der ein bekannter Historiker, Philologe und Schriftsteller wurde. 1802 wurde er Lehrer für Physik und Chemie an der École centrale in Bourg-en-Bresse. Im selben Jahr verfasste Ampère ein mathematisches Werk zu einem wahrscheinlichkeitstheoretischen Aspekt von Glücksspielen, und zwar der Frage der Wahrscheinlichkeit des Ruins des Spielers bei stetigem Einsatz eines festen Bruchteils seines Kapitals. Die Arbeit machte ihn unter Wissenschaftlern in Paris bekannt. Bald darauf verfasste er eine Arbeit zur theoretischen Mechanik und eine Abhandlung über partielle Differentialgleichungen, die ihm 1814 die Mitgliedschaft in der französischen Akademie der Wissenschaften (damals Institut Impèrial) einbrachte.

Die vier Jahre seiner ersten Ehe waren die glücklichsten seines Lebens. Im Jahr 1803 starb nach vierjähriger Ehe seine Frau, die sich von der Geburt des Sohnes nie völlig erholt hatte. Ampère war tief getroffen und zog Jahr 1804 nach Paris. Sein Interesse für Mathematik erlahmte, und er befasste sich zunehmend mit den Schriften von Kant, allgemeiner Wissenschaftstheorie und mit der Chemie. Ampère war Repetitor für Mathematik an der Pariser École polytechnique, was ihn aber bald langweilte. Im Jahre 1808 wurde er Generalinspektor der Universitäten, was er bis auf ein paar Jahre in den 1820er Jahren bis zu seinem Tod blieb. Ab 1819 lehrte er außerdem Philosophie an der Historisch-Philosophischen Fakultät der Sorbonne und 1820 wurde er Assistenzprofessor in Astronomie. 1824 erhielt er den Lehrstuhl für Experimentalphysik am Collège de France.

Im August 1806 heiratete er in Paris Jeanne-Françoise Potot (1778–1866), die Ehe war aber unglücklich und wurde bald geschieden. Aus dieser Ehe stammt die Tochter Albine (1807–1842). Er musste nun allein für die zwei Kinder aus den beiden Ehen sorgen. Beide bereiteten ihm später Sorgen, seine Tochter war mit einem jähzornigen und oft betrunkenen Armeeoffizier verheiratet und sein Sohn verfiel dem Einfluss von Madame Recamier.

1836 starb Ampère in Marseille auf einer Inspektionstour im Alter von 61 Jahren an einer Lungenentzündung. Er ist in Paris auf dem Cimetière de Montmartre beigesetzt.

Ampère stellte drei Jahre nach Amedeo Avogadro unabhängig von diesem das Avogadrosche Gesetz auf. Er war auch offen für die Arbeiten von Humphry Davy, die die Grundfesten der französischen Schule der Chemie (Antoine Laurent de Lavoisier) erschütterten: für Lavoisier war Sauerstoff der Träger des Säureprinzips, nach Davys Entdeckung von Natrium und Kalium fand sich dieser aber in starken Basen. Damit löste sich auch das Rätsel des grünen Gases (Chlorgas) bei der Zersetzung von Salzsäure; Ampère wie Davy vermuteten, dass es ein neues Element (Chlor) sein könnte (während man nach der Lavoisier-Theorie Sauerstoff als Bestandteil vermutete). Da Ampère aber weder Zeit noch Mittel hatte, dem weiter nachzugehen, gilt Davy als dessen Entdecker. Später (1813) erkannte Ampère die Verwandtschaft des gerade in Seetang entdeckten Jods mit Chlor, in der öffentlichen Anerkennung als Entdeckung eines neuen Elements kamen ihm aber wieder andere zuvor. Er versuchte die chemische Affinität von Molekülen, die aus punktförmigen Atomen bestehen, aus der Geometrie von geometrischen Körpern (zum Beispiel Tetraeder, Oktaeder oder Würfel) abzuleiten. Beispielsweise bildeten bei Sauerstoff, Stickstoff und Wasserstoff vier Moleküle ein Tetraeder, bei Chlor acht Moleküle ein Oktaeder (nach Ampère); Verbindungen aus Elementen konnten nur bestehen, falls sie reguläre Polyeder bildeten (was bei Tetraeder und Oktaeder nicht möglich war, wohl aber zwei Tetraeder mit einem Oktaeder zu einem Dodekaeder). Ampères spekulativere Arbeiten zur Chemie fanden jedoch bei anderen Gelehrten seiner Zeit kaum Interesse. 

Seine bedeutendsten Arbeiten entstanden ab 1820 und machten ihn zum Begründer der Elektrodynamik. Im Jahr 1827 verschlechterte sich Ampères Gesundheitszustand und er wandte sich von der Elektrodynamik anderen Gebieten zu (Philosophie, Logik, Anatomie, Kristalloptik, Botanik). In der Philosophie war er von Kant beeinflusst und war sogar einer der Ersten in Frankreich, die dessen Werk ernsthaft rezipierten. Für Ampère war dies eine Alternative zu der damals in Frankreich vorherrschenden sensualistischen Erkenntnistheorie von Étienne Bonnot de Condillac. Ampère lehnte aber gleichzeitig die Lehre von Raum und Zeit als A-priori-Anschauungen nach Kant ab, behielt aber dessen Unterscheidung von Phänomenen und Noumenon. Er folgte teilweise der Lehre seines Freundes Maine de Biran im Nachweis der Existenz einer unabhängigen materiellen Welt, von Gott und Seele. Ampère vertrat ein hypothetisch-deduktives Verfahren des wissenschaftlichen Erkenntnisgewinns: Der Naturforscher stellt eine Hypothese auf und fragt sich, welche Experimente unternommen werden müssen, um die Theorie zu stützen oder zu falsifizieren. Dabei ging er pragmatisch vor: Hypothesen konnten frei eingeführt werden, wichtig war nur, wie erfolgreich sie in der Naturerklärung waren. Später beschäftigte er sich mit der Naturphilosophie und der prästabilierten Harmonie von Gottfried Wilhelm Leibniz. Da das Denken des Menschen ein Bild des Denkens Gottes sei und Gott das Universum geschaffen habe, sollte nach Leibniz des Menschen Geist imstande sein, das Universum in reinen Denkakten zu verstehen: Sein und Denkgesetze sollten also einander entsprechen. Einheit der Wissenschaft sollte die Widerspiegelung des göttlichen Geistes sein. Ampère strebte danach, alle Wissenschaften zu klassifizieren, und veröffentlichte darüber 1834 ein Buch. Unter den 64 Disziplinen waren auch einige neu von ihm eingeführt worden, wie die technische Kinematik und Kybernetik.

In der Mathematik ist die Monge-Ampèresche Gleichung nach ihm benannt, eine nichtlineare partielle Differentialgleichung zweiter Ordnung, die in der Differentialgeometrie und bei Transportproblemen Anwendung findet und mit der sich Ampère um 1820 befasste (und davor Gaspard Monge).

Im Frühherbst 1820 wurde Ampère, der nun schon 45 Jahre alt war und dessen bisherige wissenschaftliche Arbeiten höchstens als Fußnoten in Lehrbüchern erschienen wären, durch François Arago auf die Versuche Hans Christian Ørsteds zur Ablenkung einer Magnetnadel durch den elektrischen Strom aufmerksam. Ampère wiederholte den Versuch und erkannte, dass Ørsted die Ablenkung des Magneten durch das Erdmagnetfeld nicht beachtet hatte. Mit einer verbesserten Versuchsanordnung konnte Ampère nun feststellen, dass sich die Magnetnadel immer senkrecht zum stromdurchflossenen Leiter stellte. Ampère nahm nun als Modellhypothese an, dass jeder Magnetismus seine Ursache in elektrischen Strömen habe und Ströme Magnetfelder erzeugen. Er überprüfte seine Hypothese – hypothetisch-deduktiv – zwischen dem 18. September und dem 2. November 1820 und konnte in aufeinanderfolgenden Versuchen nachweisen, dass zwei stromdurchflossene Leiter eine Anziehungskraft aufeinander ausüben, wenn in beiden Leitern die Elektrische Stromrichtung gleich ist, und dass sie eine Abstoßungskraft aufeinander ausüben, wenn die Stromrichtung entgegengesetzt ist. Ampère konstruierte ein Gerät zur Messung des Stroms, das er Galvanometer nannte (unabhängig von Ampère tat dies Johann Schweigger in Deutschland). Ampère verfeinerte seine Hypothese, indem er annahm, dass jeder Magnet viele Moleküle enthält, die jeweils einen kleinen Kreisstrom erzeugen (sog. Ampèresche Molekularströme zur Erklärung des Magnetismus). Er erkannte, dass die fließende Elektrizität die eigentliche Ursache des Magnetismus ist.

Im Jahr 1822 beschäftigte sich Ampère mit der Kraft zwischen zwei nahe beieinander liegenden stromdurchflossenen Leitern. Er konnte zeigen, dass diese Kraft zu dem Kehrwert des Abstands proportional ist. Bei der mathematischen Behandlung dieser Phänomene nahm er sich das Gravitationsgesetz (als Punkt-Kraft-Gesetz) von Isaac Newton zum Vorbild. Da der Strom jedoch als gerichtete Größe behandelt werden muss und die Stromstärke die Zeit als neue Größe enthält, hat das ampèresche Modell nur eine beschränkte Gültigkeit.

Ampère erklärte den Begriff der elektrischen Spannung und des elektrischen Stromes und setzte die Stromrichtung fest.

Neben der Begründung der Elektrodynamik erkannte Ampère das Prinzip der elektrischen Telegrafie (Vorschlag eines elektromagnetischen Telegraphen mit Jacques Babinet 1822), der aber über größere Entfernungen wenig praktikabel war. Erstmals realisiert wurde ein elektrischer Telegraph 1833 von Carl Friedrich Gauß und Wilhelm Eduard Weber in Göttingen.

Ampère glaubte, dass das Erdmagnetfeld durch starke elektrische Ströme ausgelöst wird, die in der Erdrinde von Osten nach Westen fließen.

Ampères Charakter war von großer Liebenswürdigkeit und Sensibilität geprägt. Er neigte aber auch zu Überschwang und zur Melancholie, verstärkt durch mehrere Schicksalsschläge, zur Unentschlossenheit und einer gewissen Hilflosigkeit in Alltagsdingen und seine Zerstreutheit war sprichwörtlich. In seiner wissenschaftlichen Arbeit war er von großer Beharrlichkeit, folgte aber im Allgemeinen keinem systematischen Plan, sondern folgte einem Geistesblitz fieberhaft bis zu dessen Ausarbeitung. Ampère hatte eine Neigung zu metaphysischen Spekulationen und war tief religiös.

Zu Ehren Ampères ist die SI-Einheit des elektrischen Stromes „Ampere“ (Einheitenzeichen A) benannt worden. Er wurde durch Namensnennung auf dem Eiffelturm geehrt. Nach ihm ist seit 1935 ein Mondberg, der Mons Ampère, benannt. Ab 1827 war er korrespondierendes Mitglied der Preußischen Akademie der Wissenschaften.


Briefe




</doc>
<doc id="230" url="https://de.wikipedia.org/wiki?curid=230" title="Astronomische Einheit">
Astronomische Einheit

Die Astronomische Einheit (abgekürzt AE, international au für ) ist ein Längenmaß in der Astronomie: Laut Definition misst eine AE exakt 149 597 870 700 Meter. Das ist ungefähr der mittlere Abstand zwischen Erde und Sonne.

Die Astronomische Einheit ist neben dem Lichtjahr und dem Parsec die wichtigste Einheit unter den astronomischen Maßeinheiten. Sie gehört nicht zum Internationalen Einheitensystem (SI), ist aber zum Gebrauch mit dem SI zugelassen. Sie ist keine gesetzliche Maßeinheit.

Die Astronomische Einheit war historisch von großer Bedeutung für die Astronomie, da die meisten Entfernungsbestimmungen aufgrund der verwendeten Methoden das Ergebnis unmittelbar in AE und nicht in Metern lieferten. Mittlerweile ist jedoch der Umrechenfaktor zwischen AE und Metern so genau bekannt, dass die Verwendung der AE keine Genauigkeitsvorteile mehr bietet. Im Jahre 2012 wurde daher die frühere, von der Gravitationskonstante der Sonne abgeleitete Definition aufgegeben und die AE einfach als eine bestimmte Anzahl von Metern neu definiert. Damit hat die AE ihre ursprüngliche astrophysikalische Bedeutung verloren und ist nur noch eine konventionelle Längeneinheit. Entfernungen innerhalb des Sonnensystems werden jedoch immer noch meist in AE angegeben, da sich so bequeme Zahlenwerte ergeben.

Das Internationale Büro für Maß und Gewicht empfiehlt seit 2014 für die Astronomische Einheit ebenso wie die Internationale Astronomische Union (IAU) das Einheitenzeichen au. Im Gegensatz dazu hat sich in der deutschsprachigen Literatur die Verwendung von AE und AU durchgesetzt.

Ausgedrückt in anderen interstellaren Längenmaßen ergibt sich für die Astronomische Einheit folgende Größenrelation:
Die AE war ursprünglich als die Länge der großen Halbachse der Erdbahn definiert, später als Radius einer Kreisbahn, auf der ein hypothetischer masseloser Körper die Sonne in einem vorgegebenen Zeitraum umrundete (nähere Details werden im Abschnitt Geschichte erläutert).

Am 30. August 2012 beschloss die in Peking tagende 28. Generalversammlung der Internationalen Astronomischen Union in „Resolution B2“,

Gemäß dieser Neudefinition ist die AE nun keine durch Messung zu ermittelnde Eigenschaft des Sonnensystems mehr, sie ist einfach eine Strecke mit einer per definitionem exakt festgelegten Länge in Metern. Der gewählte Zahlenwert entspricht dem bis dahin besten Messwert von 149 597 870 700 m ± 3 m.

Die vorherige Definition der AE beruhte auf der Gaußschen Gravitationskonstanten, welche, wenn sie unter Verwendung der Längeneinheit „1 AE“, der Zeiteinheit „1 Tag“ und der Masseneinheit „1 Sonnenmasse“ ausgedrückt wurde, einen per Konvention fix vorgegebenen Zahlenwert hatte (siehe →Abschnitt Definition von 1976). Welchen Zahlenwert die so definierte Längeneinheit „1 AE“ annahm, wenn sie in SI-Einheiten (also in Metern) ausgedrückt werden sollte, musste durch Beobachtung der Planetenbewegungen ermittelt werden. Infolge der Neudefinition ist die Länge der AE in Metern nun festgelegt; die Gaußsche Gravitationskonstante wird nicht mehr benötigt und ist künftig nicht mehr Bestandteil der astronomischen Konstantensysteme.

Der Zahlenwert der in astronomischen Maßeinheiten ausgedrückten Heliozentrischen Gravitationskonstanten formula_1 war gemäß der vorherigen Definition als Konstante festgelegt. In die Berechnung ihres Zahlenwertes in SI-Einheiten ging jedoch der jeweils aktuelle durch Beobachtung bestimmte Zahlenwert für die Länge der Astronomischen Einheit ein, so dass eine Neuvermessung der AE auch ein verändertes formula_1 nach sich ziehen konnte. Die aufgrund moderner Messungen möglich gewordene direkte Bestimmung von formula_1 in SI-Einheiten macht diesen Umweg über die AE überflüssig. Außerdem ist denkbar, dass eine mögliche zeitliche Änderung von formula_1 in absehbarer Zeit in den Bereich der Messbarkeit rücken könnte. Dies hätte nach der vorherigen Definition die Einführung einer zeitlich veränderlichen AE erfordert, was sich nach der neuen Definition aber erübrigt. Neuere Messungen (2011) deuten bereits eine geringfügige Abnahme von formula_1 an.

Die Genauigkeit moderner Positionsmessungen im Sonnensystem ist so hoch, dass relativistische Korrekturen berücksichtigt werden müssen. Die Übertragung der vorherigen Definition in einen relativistischen Begriffsrahmen hätte zusätzliche Konventionen erfordert und eine vom Bezugssystem abhängige Länge der AE ergeben. Die neu definierte AE hingegen hat in allen relativistischen Bezugssystemen dieselbe Länge. Die Resolution legt explizit fest, dass dieselbe Definition für alle relativistischen Zeitskalen (z. B. TCB, TDB, TCG, TT usw.) verwendet werden soll.

Die Umlaufzeiten der Planeten sind leicht zu beobachten und waren schon frühen Astronomen sehr genau bekannt. Mit Hilfe des Dritten Keplerschen Gesetzes ließ sich aus dem Verhältnis der Umlaufzeiten zweier Planeten mit praktisch derselben Genauigkeit auf das Verhältnis ihrer Bahnradien schließen. Die damaligen Ephemeriden konnten daher mit hoher Genauigkeit berechnen, wievielmal z. B. Mars zu einem gegebenen Zeitpunkt weiter von der Sonne entfernt war als die Erde. Man wählte die große Halbachse der Erdbahn als Längenmaß, nannte sie „Astronomische Einheit“ und konnte anstelle der umständlichen Ausdrucksweise „Mars ist heute 1,438mal so weit von der Sonne entfernt wie die große Halbachse der Erdbahn lang ist“ gleichbedeutend einfach sagen „Mars ist heute 1,438 AE von der Sonne entfernt“. Die in dieser Form als AE ausgedrückten Entfernungen (eigentlich die Verhältnisse zweier Entfernungen zueinander) waren recht genau bestimmbar, in irdischen Längenmaßen wie z. B. Meilen oder Metern waren die Entfernungen jedoch nur recht ungenau bekannt. Für wissenschaftliche Zwecke bot sich daher die Verwendung der AE als Längeneinheit an, wofür sie jedoch auch einer hinreichend genauen Definition bedurfte.

Gemäß dem Dritten Keplerschen Gesetz gilt für die Umlaufdauer formula_6 eines Planeten der Masse formula_7, welcher die Sonne (Masse formula_8) auf einer Bahn mit der großen Halbachse formula_9 umläuft:

Für zwei Planeten P1 und P2 folgt daraus:

Dieses Gesetz enthält nur "Verhältnisse" der Umlaufzeiten, der Massen und der großen Halbachsen. Das Zweite Keplersche Gesetz enthält in ähnlicher Weise nur eine Aussage über die "Verhältnisse" der vom Fahrstrahl in bestimmten Zeitintervallen überstrichenen Flächen. Diese Gesetze liefern die Positionen der Planeten daher zunächst in einem noch unbestimmten Maßstab. Man kann deshalb die Einheiten der vorkommenden Längen, Zeitintervalle und Massen so wählen, dass sie die Rechnungen möglichst einfach gestalten. In der klassischen Astronomie wählte man üblicherweise als "astronomische Längeneinheit" die Länge der großen Halbachse der Erdbahn (1 AE), als "astronomische Masseneinheit" die Masse der Sonne "1 M" und als "astronomische Zeiteinheit" den Tag 1 d.

Da die Positionen der Himmelskörper an der scheinbaren Himmelskugel (also die Richtungswinkel, unter denen sie dem Beobachter erscheinen) von absoluten Maßstäben unabhängig sind, konnten die Astronomen mit diesen relativen Maßstäben bereits hochpräzise Positionsastronomie betreiben. Die Entfernung eines Planeten konnte außerdem für einen gewünschten Zeitpunkt mit hoher Genauigkeit in Astronomischen Einheiten angegeben werden, die Entfernung in Metern hingegen weit weniger genau, da die Länge der Astronomischen Einheit in Metern nur mäßig genau bekannt war. Ähnlich konnten die Massen der Planeten recht genau in Sonnenmassen angegeben werden, deutlich weniger genau in Kilogramm.

Erst in den letzten Jahrzehnten wurde es möglich, auch Entfernungen mit hoher Genauigkeit zu messen (z. B. mittels Laser-Entfernungsmessung zum Mond, mittels Radar-Entfernungsmessung zu Merkur, Venus und Mars, oder mittels Messung der Signallaufzeiten zu Raumsonden).

Der Zahlenwert der Gravitationskonstanten formula_10 in der Gleichung hängt von der Wahl der Einheiten für die vorkommenden physikalischen Größen ab. Für die Umlaufdauer des Planeten formula_11 folgt aus jener Gleichung durch Umstellen:

Mit den Abkürzungen

ergibt sich:

C. F. Gauß bestimmte 1809 den Wert der Gravitationskonstanten formula_12 in astronomischen Maßeinheiten (große Halbachse der Erdbahn als Längeneinheit AE, mittlerer Sonnentag als Zeiteinheit d, Sonnenmasse als Masseneinheit formula_8), indem er die Formel auf die Erde formula_14 als Planet formula_11 anwandte

und die damals besten Zahlenwerte für formula_16 und formula_17 einsetzte:

Dieser Zahlenwert der Gravitationskonstanten in astronomischen Maßeinheiten wurde in der Folge als Standardwert für zahlreiche astronomische Berechnungen verwendet.

Mit stets verbesserter Kenntnis von formula_16 und formula_17 hätte auch der Zahlenwert von formula_12 ständig verbessert werden können. Der gaußsche Wert lag jedoch bald zahlreichen fundamentalen Tabellen zugrunde, welche bei jeder Veränderung von formula_12 hätten neu berechnet werden müssen. Eine Alternative bestand darin, in der Gleichung

den Zahlenwert von formula_12 beizubehalten und stattdessen die Längeneinheit, in der formula_23 gemessen wird, so anzupassen, dass der in der neuen Längeneinheit gemessene neue Zahlenwert von formula_23 die Gleichung auch für die neuen Werte von formula_16 und formula_17 wieder erfüllt (ein Beispiel folgt im nächsten Abschnitt). Die große Halbachse formula_23 der Erdbahn verlor damit ihren definierenden Status: Sie hatte in astronomischen Maßeinheiten nicht mehr strikt die Länge 1 AE. Die Längeneinheit, bezüglich welcher formula_23 den die Gleichung erfüllenden Zahlenwert annahm, war die neue AE. Damit lautete die Definition von 1976:

Da die Definition der AE damit aber ohnehin nicht mehr unmittelbar durch die Erdbahn gegeben war, lösten sich die Astronomen auch von der Erdmasse formula_17 und bezogen die neue Definition auf einen fiktiven Körper formula_30 mit vernachlässigbar kleiner Masse:

Denkt man sich einen solchen fiktiven Körper auf einer ungestörten Bahn, welche dem Gesetz gehorcht und deren große Halbachse gleich der zu bestimmenden neuen Längeneinheit ist

so gilt für ihn

Dieser definierende Körper hat also eine Umlaufdauer von
Die fiktive Bahn lässt sich ohne Beschränkung der Allgemeingültigkeit als kreisförmig annehmen. Die Definition der AE lässt sich daher auch gleichbedeutend formulieren als

Die Praxis, den Zahlenwert von formula_12 festzuhalten und durch ihn die AE zu definieren, war inoffiziell seit dem 19. Jahrhundert üblich. Sie wurde 1938 offiziell von der IAU übernommen, als sie auf der 6. Generalversammlung den gaußschen Zahlenwert für formula_12 per Resolution festschrieb. 1976 erfolgte auf der 28. Generalversammlung erstmals eine explizite textliche Definition.

Für die Umlaufzeiten der Erde formula_14 und des definierenden fiktiven Körpers formula_30 liefert das Dritte Keplergesetz :

Auflösen nach formula_23 und Einsetzen der aktuellen Zahlenwerte
und
ergibt
Aus dem Verhältnis der Umlaufzeiten beider Körper folgt also das Verhältnis ihrer großen Halbachsen . Die eine davon definiert aber gerade die Astronomische Einheit; das Ergebnis ist also die in AE ausgedrückte große Halbachse der Erdbahn, welche nun etwas größer ist als 1 AE.

Setzt man diese neuen Zahlenwerte für formula_23, formula_16 und formula_17 anstelle der alten gaußschen Werte in die gaußsche Formel ein, so erhält man nach wie vor den gaußschen Zahlenwert für formula_12. Wenn der in Tagen gemessenen Umlaufzeit der genannte Zahlenwert formula_16 und der in Sonnenmassen gemessenen Erdmasse der genannte Zahlenwert formula_17 beigelegt werden, dann sind die Voraussetzungen der IAU-Definition also erfüllt, wenn die in AE gemessene große Halbachse der Erdbahn den Zahlenwert 1,000000036 erhält. Jene Längeneinheit, in der die große Halbachse gemessen werden muss, um diesen Zahlenwert anzunehmen, ist die mit den aktuellen Werten von formula_16 und formula_17 kompatible aktuelle AE. Gelingt es, die Länge der großen Halbachse in Metern zu ermitteln, so ist über diesen Zusammenhang auch die Länge der AE in Metern bekannt.

Rechnet man Umlaufzeit formula_44 und große Halbachse formula_45 des fiktiven masselosen Körpers von astronomischen Maßeinheiten wieder nach SI-Einheiten um

und setzt das Ergebnis in Gleichung ein, so ergibt sich:

wobei formula_46 der noch zu bestimmende Umrechnungsfaktor von Astronomischen Einheiten in Meter ist. Einsetzen von

und Auflösen nach formula_1 liefert:

Die eben genannte Formel stellt nichts anderes dar als die Umrechnung von "k²" (in astronomischen Maßeinheiten) nach formula_10 bzw. formula_1 (in SI-Einheiten). In astronomischen Maßeinheiten hat formula_12 stets denselben von der Definition der AE festgelegten Zahlenwert. In SI-Einheiten hängt der Zahlenwert von formula_1 ab von dem jeweils aktuellen durch Beobachtung bestimmten Zahlenwert für die Länge formula_46 der Astronomischen Einheit.

Nicht vorgesehen ist in der 1976er Definition eine eventuelle physikalisch reale Veränderlichkeit von formula_1, etwa durch eine kosmologische Veränderlichkeit von formula_10 oder den Masseverlust der Sonne. Sollte es infolge gesteigerter Messgenauigkeit notwendig werden, ein zeitlich veränderliches formula_1 zu beschreiben, so könnte dies (da formula_12 ja laut Definition auf seinem gegebenen Zahlenwert fixiert ist) nur durch die sehr unbefriedigende Verwendung einer zeitlich veränderlichen AE geschehen.

Die Neudefinition der AE von 2012 entkoppelt "GM" und AE und eröffnet so den Weg zur direkten Messung von formula_1 (und seiner eventuellen Veränderlichkeit) in SI-Einheiten. Der Umweg über die AE ist nicht mehr nötig. Eine Änderung des Zahlenwertes formula_46 der AE infolge einer Neubestimmung hat keine Änderung des Zahlenwertes von formula_1 mehr zur Folge.

Um die Länge der AE in Metern zu ermitteln, war es notwendig, die in AE bekannten Entfernungen zu den Planeten oder zur Sonne in Metern zu messen. Dies konnte bis etwa zur Mitte des 20. Jahrhunderts nur durch Triangulationen mit optischen Mitteln geschehen. Die AE wurde hauptsächlich aus hochgenauen Winkelmessungen (Parallaxen) abgeleitet, die von möglichst weit voneinander entfernten Sternwarten aus zu den Planeten Venus und Mars sowie zu erdnahen Asteroiden durchgeführt wurden. Ein kurzer Überblick über diese Bestimmungen der AE bis ins frühe 20. Jahrhundert findet sich im →Artikel Venustransit.

Seit einigen Jahrzehnten können Entfernungen im Sonnensystem direkt gemessen werden. Der moderne Wert der AE wurde mittels Radar- und anderen Distanzmessungen von der Erde zu den Nachbarplaneten und zu Raumsonden bestimmt. Aus der Vermessung der „mittleren Bewegungen“ (d. h. der mittleren Geschwindigkeiten) oder der Umlaufperioden der Planeten, welche sich sehr genau bestimmen lassen, folgen über das Dritte Keplergesetz (in der newtonschen Fassung inklusive relativistischer Korrekturen) mit derselben Genauigkeit die großen Halbachsen der Planeten in AE. Die Abstandsmessungen zu den Planeten mittels Radar bestimmen deren Bahngeometrie und damit die großen Halbachsen ihrer Bahnen in Metern; das Verhältnis zur Länge der großen Halbachsen in AE liefert die Länge der AE in Metern sowie den Zahlenwert von formula_1 in formula_64.

Die folgende Tabelle listet unter anderem einige moderne Ephemeriden auf, die durch Anpassung der physikalischen Bewegungsgleichungen an umfangreiches Beobachtungsmaterial gewonnen wurden. Jede solche Anpassung liefert unter anderem wie eben beschrieben einen Zahlenwert für den "Skalenfaktor" des Sonnensystems, welcher die Länge der AE in Metern angibt (die jeweils genannten Unsicherheiten sind in der Regel "formale" Unsicherheiten, die im Zuge der Anpassung aus der Konsistenz der Messdaten untereinander abgeschätzt werden und die meist zu optimistisch ausfallen. Ein realistischeres Bild der Unsicherheiten gewinnt man durch Vergleich der Ergebnisse untereinander):

Die Ephemeride DE405 des JPL liegt derzeit zahlreichen Jahrbüchern und sonstigen Ephemeridenwerken zugrunde. Der aus ihr abgeleitete Zahlenwert von 149 597 870 691 m für die AE war daher für mehrere Jahre der gebräuchlichste Standardwert. Er wurde auch vom IERS empfohlen.

Streng genommen ist der genannte Zahlenwert nicht der SI-Wert, da den Berechnungen der Planetenbewegungen die auf den Schwerpunkt des Sonnensystems bezogene Zeitskala TDB zugrunde gelegt wird, während die SI-Sekunde sich definitionsgemäß auf die Erdoberfläche (genauer: das Geoid) bezieht und aus relativistischen Gründen etwas schneller läuft. Rechnet man den TDB-Wert auf strikte SI-Einheiten um, so ergibt sich beispielsweise:

Die 27. Generalversammlung der Internationalen Astronomischen Union beschloss im Jahre 2009, im Rahmen des „IAU 2009 System of Astronomical Constants“ den aus damaligen besten Messungen abgeleiteten Mittelwert von 149 597 870 700 m ± 3 m zur allgemeinen Verwendung zu empfehlen.

Die 28. Generalversammlung der Internationalen Astronomischen Union beschloss im Jahre 2012, von der bisherigen Definition abzugehen (nach welcher die Länge der Astronomischen Einheit in Metern stets das Ergebnis einer "Messung" gewesen war) und die Astronomische Einheit einfach als eine Strecke der Länge 149 597 870 700 m (exakt) neu zu definieren.

Die im Jahre 2012 neu definierte AE ist durch einen festen Zahlenwert festgelegt und damit per Definition unveränderlich. Die über die Gaußsche Konstante definierte frühere AE jedoch ist ein durch Messung zu bestimmender Skalenfaktor des Sonnensystems, der möglicherweise auch Veränderungen des Sonnensystems widerspiegelt. Messungen zur Bestimmung der AE im früheren Sinne können daher durchaus zur Aufdeckung solcher eventueller Veränderungen noch nützlich sein.

Auswertungen von Radarmessungen scheinen anzudeuten, dass der Skalenfaktor des Sonnensystems langsam zunimmt. Es werden Änderungsraten von (15 ± 4) Meter/Jahrhundert, (7 ± 2) Meter/Jahrhundert und (1,2 ± 1,1) Meter/Jahrhundert genannt; die Ursache ist bislang unbekannt.


Bislang lässt sich nicht ausschließen, dass es sich lediglich um systematische Fehler in den Beobachtungen handelt. Bei der Berechnung der Planetenbahnen oder der Signalausbreitung unberücksichtigt gebliebene Effekte werden für weniger wahrscheinlich gehalten. Erklärungsversuche im Rahmen exotischerer Gravitationstheorien wie zum Beispiel der Stringtheorie werden derzeit als „hoch spekulativ“ angesehen.



</doc>
<doc id="231" url="https://de.wikipedia.org/wiki?curid=231" title="AE">
AE

Ae steht für:
ae steht als Abkürzung für:
Æ steht für:
AE steht als Abkürzung für:


æ steht als Transliteration des Buchstabens ä, siehe Æ



</doc>
<doc id="233" url="https://de.wikipedia.org/wiki?curid=233" title="Anders Celsius">
Anders Celsius

Anders Celsius (, * 27. November / 7. Dezember 1701 in Uppsala; † ebenda) war ein schwedischer Astronom, Mathematiker und Physiker.

Anders Celsius wurde 1701 in Uppsala geboren und entstammt einer Adelsfamilie vom Gut Doma in Ovanåker. Er studierte an der Universität Uppsala und wurde dort 1730 Professor. Im Jahr 1733 wurde er zum Mitglied der Leopoldina gewählt. Ab 1734 war er auswärtiges Mitglied der Preußischen Akademie der Wissenschaften. 1739 und 1743 amtierte er als Rektor der Universität Uppsala.

1736 nahm Celsius an einer Expedition zur Vermessung der Form der Erde teil. 1741 stellte er das erste schwedische Observatorium in Uppsala fertig. Er war außerdem auch Poet und Autor populärwissenschaftlicher Literatur. 

Celsius war der Erste, der die Helligkeit von Sternen messtechnisch untersuchte; auch fand er heraus, dass Polarlichter das Magnetfeld der Erde stören.

Celsius starb im Alter von 42 Jahren an Tuberkulose und wurde in der Kirche von Gamla Uppsala begraben.

Anders Celsius definierte 1742 die nach ihm benannte Temperatureinteilung Grad Celsius. Im Gegensatz zur heute verwendeten Celsius-Skala legte er den Siedepunkt von Wasser mit 0° und den Gefrierpunkt mit 100° fest. Erst später wurden die Fixpunkte der Skala vertauscht; heutzutage wird sie ausschließlich in letzterer Form verwendet. Das Revolutionäre war, dass Celsius vorgeschlagen hatte, sie als universelle Skala zu benutzen, um Temperaturen in der ganzen Welt zu vergleichen: Im Gegensatz zu anderen Forschern notierte er bei der genauen Bestimmung der Fixpunkte auch den herrschenden Luftdruck (760 mm auf der Quecksilbersäule) und legte so genaue Messbedingungen fest.

Im Jahr 1948 wurde die Temperaturskala von der 9. Generalkonferenz für Maß und Gewicht in Gedenken an Anders Celsius in Celsius-Skala umbenannt. Das Originalthermometer kann heute im Museum der Universität Uppsala, dem Gustavianum, besichtigt werden. Es besteht, genau wie ein heutiges Thermometer, aus einem auf ein Holzbrett mit Skala montierten Quecksilberreservoir mit angesetzter Kapillare.

Nach ihm sind der Asteroid (4169) Celsius und der Mondkrater Celsius benannt.




</doc>
<doc id="236" url="https://de.wikipedia.org/wiki?curid=236" title="Analysis">
Analysis

Die Analysis [] (, "analýein" ‚auflösen‘) ist ein Teilgebiet der Mathematik, dessen Grundlagen von Gottfried Wilhelm Leibniz und Isaac Newton als Infinitesimalrechnung unabhängig voneinander entwickelt wurden. Als eigenständiges Teilgebiet der Mathematik neben den klassischen Teilgebieten der Geometrie und der Algebra existiert die Analysis seit Leonhard Euler.

Grundlegend für die gesamte Analysis sind die beiden Körper formula_1 (der Körper der reellen Zahlen) und formula_2 (der Körper der komplexen Zahlen) mitsamt deren geometrischen, arithmetischen, algebraischen und topologischen Eigenschaften. Zentrale Begriffe der Analysis sind die des Grenzwerts, der Folge, der Reihe sowie in besonderem Maße der Begriff der Funktion. Die Untersuchung von reellen und komplexen Funktionen hinsichtlich Stetigkeit, Differenzierbarkeit und Integrierbarkeit zählt zu den Hauptgegenständen der Analysis. Die hierzu entwickelten Methoden sind in allen Natur- und Ingenieurwissenschaften von großer Bedeutung.

Die Analysis hat sich zu einem sehr allgemeinen, nicht klar abgrenzbaren Oberbegriff für vielfältige Gebiete entwickelt. Neben der Differential- und Integralrechnung umfasst die Analysis weitere Gebiete, welche darauf aufbauen. Dazu gehören die Theorie der gewöhnlichen und partiellen Differentialgleichungen, die Variationsrechnung, die Vektoranalysis, die Maß- und Integrationstheorie und die Funktionalanalysis.

Eine ihrer Wurzeln hat auch die Funktionentheorie in der Analysis. So kann die Fragestellung, welche Funktionen die Cauchy-Riemannschen-Differentialgleichungen erfüllen, als Fragestellung der Theorie partieller Differentialgleichungen verstanden werden.

Je nach Auffassung können auch die Gebiete der harmonischen Analysis, der Differentialgeometrie mit den Teilgebieten Differentialtopologie und Globale Analysis, der analytischen Zahlentheorie, der Nichtstandardanalysis, der Distributionentheorie und der mikrolokalen Analysis ganz oder in Teilen dazu gezählt werden.

Bei einer linearen Funktion bzw. einer Geraden

heißt "m" die Steigung und "c" der y-Achsen-Abschnitt oder Ordinatenabschnitt der Geraden. Hat man nur 2 Punkte formula_4 und formula_5 auf einer Geraden, so kann die Steigung berechnet werden durch

Bei nicht linearen Funktionen wie z. B. formula_7 kann die Steigung so nicht mehr berechnet werden, da diese Kurven beschreiben und somit keine Geraden sind. Jedoch kann man an einen Punkt formula_8 eine Tangente legen, die wieder eine Gerade darstellt. Die Frage ist nun, wie man die Steigung einer solchen Tangente an einer Stelle formula_9 berechnen kann. Wählt man eine Stelle formula_10 ganz nahe bei formula_9 und legt eine Gerade durch die Punkte formula_8 und formula_13, so ist die Steigung dieser Sekante nahezu die Steigung der Tangente. Die Steigung der Sekante ist (s. o.)

Diesen Quotienten nennt man den Differenzenquotienten oder mittlere Änderungsrate. Wenn wir nun die Stelle formula_10 immer weiter an formula_9 annähern, so erhalten wir per Differenzenquotient die Steigung der Tangente. Wir schreiben

und nennen dies die Ableitung oder den Differentialquotienten von "f" in formula_9.
Der Ausdruck formula_19 bedeutet, dass "x" immer weiter an formula_9 angenähert wird, bzw. dass der Abstand zwischen "x" und formula_9 beliebig klein wird. Wir sagen auch: „"x" geht gegen formula_9“. Die Bezeichnung formula_23 steht für Limes.

Es gibt auch Fälle, in denen dieser Grenzwert nicht existiert. Deswegen hat man den Begriff Differenzierbarkeit eingeführt. Eine Funktion "f" heißt differenzierbar an der Stelle formula_9, wenn der Grenzwert formula_26 existiert.

Die Integralrechnung befasst sich anschaulich mit der Berechnung von Flächen unter Funktionsgraphen. Diese Fläche kann durch eine Summe von Teilflächen approximiert werden und geht im Grenzwert in das Integral über.

Die obige Folge konvergiert, falls "f" gewisse Bedingungen (wie z. B. Stetigkeit) erfüllt. Diese anschauliche Darstellung (Approximation mittels Ober- und Untersummen) entspricht dem sogenannten Riemann-Integral, das in der Schule gelehrt wird.

In der sogenannten "Höheren Analysis" werden darüber hinaus weitere Integralbegriffe, wie z. B. das Lebesgue-Integral betrachtet.

Differentialrechnung und Integralrechnung verhalten sich nach dem Hauptsatz der Analysis in folgender Weise „invers“ zueinander.

Wenn f eine auf einem kompakten Intervall formula_28 stetige reelle Funktion ist, so gilt für formula_29:

und, falls f zusätzlich auf formula_31 gleichmäßig stetig differenzierbar ist,

Deshalb wird die Menge aller Stammfunktionen einer Funktion formula_33 auch als unbestimmtes Integral bezeichnet und durch
formula_34 symbolisiert.

Viele Lehrbücher unterscheiden zwischen Analysis in einer und Analysis in mehreren Dimensionen. Diese Differenzierung berührt die grundlegenden Konzepte nicht, allerdings gibt es in mehreren Dimensionen eine größere mathematische Vielfalt. Die mehrdimensionale Analysis betrachtet Funktionen formula_35 mehrerer reeller Variablen, die oft als ein Vektor beziehungsweise n-Tupel dargestellt werden.

Die Begriffe der Norm (als Verallgemeinerung des Betrags), der Konvergenz, der Stetigkeit und der Grenzwerte lassen sich einfach von einer in mehrere Dimensionen verallgemeinern.

Die Differentiation von Funktionen mehrerer Variablen unterscheidet sich von der eindimensionalen Differentiation.
Wichtige Konzepte sind die Richtungs- und die partielle Ableitung, die Ableitungen in einer Richtung beziehungsweise in einer Variable sind. Der Satz von Schwarz stellt fest, wann partielle beziehungsweise Richtungsableitungen unterschiedlicher Richtungen vertauscht werden dürfen. Außerdem ist der Begriff der totalen Differentiation von Bedeutung. Dieser kann interpretiert werden als die lokale Anpassung einer linearen Abbildung an den Verlauf der mehrdimensionalen Funktion und ist das mehrdimensionale Analogon der (ein-dimensionalen) Ableitung. Der Satz von der impliziten Funktion über die lokale, eindeutige Auflösung impliziter Gleichungen ist eine wichtige Aussage der mehrdimensionalen Analysis und kann als eine Grundlage der Differentialgeometrie verstanden werden.

In der mehrdimensionalen Analysis gibt es unterschiedliche Integralbegriffe wie das Kurvenintegral, das Oberflächenintegral und das Raumintegral. Jedoch von einem abstrakteren Standpunkt aus der Vektoranalysis unterscheiden sich diese Begriffe nicht. Zum Lösen dieser Integrale sind der Transformationssatz als Verallgemeinerung der Substitutionsregel und der Satz von Fubini, welcher es erlaubt, Integrale über n-dimensionale Mengen in iterierte Integrale umzuwandeln, von besonderer Bedeutung. Auch die Integralsätze aus der Vektoranalysis von Gauß, Green und Stokes sind in der mehrdimensionalen Analysis von Bedeutung. Sie können als Verallgemeinerung des Hauptsatzes der Integral- und Differentialrechnung verstanden werden.

Die Funktionalanalysis ist eines der wichtigsten Teilgebiete der Analysis. Die entscheidende Idee in der Entwicklung der Funktionalanalysis war die Entwicklung einer koordinaten- und dimensionsfreien Theorie. Dies brachte nicht nur einen formalen Gewinn, sondern ermöglichte auch die Untersuchung von Funktionen auf unendlichdimensionalen topologischen Vektorräumen. Hierbei werden nicht nur die reelle Analysis und die Topologie miteinander verknüpft, sondern auch Methoden der Algebra spielen eine wichtige Rolle.
Aus wichtigen Resultaten der Funktionalanalysis wie es beispielsweise der Satz von Fréchet-Riesz ist, lassen sich zentrale Methoden für die Theorie partieller Differentialgleichungen ableiten. Zudem ist die Funktionalanalysis, insbesondere mit der Spektraltheorie, der geeignete Rahmen zur mathematischen Formulierung der Quantenmechanik und auf ihr aufbauender Theorien.

Eine Differentialgleichung ist eine Gleichung, die eine unbekannte Funktion und Ableitungen von dieser enthält. Treten in der Gleichung nur gewöhnliche Ableitungen auf, so heißt die Differentialgleichung gewöhnlich. Ein Beispiel ist die Differentialgleichung
des harmonischen Oszillators. Von einer partiellen Differentialgleichung spricht man, wenn in der Differentialgleichung partielle Ableitungen auftreten. Ein Beispiel dieser Klasse ist die Laplace-Gleichung
Ziel der Theorie der Differentialgleichungen ist es, Lösungen, Lösungsmethoden und andere Eigenschaften solcher Gleichungen zu finden. Für gewöhnliche Differentialgleichungen wurde eine umfassende Theorie entwickelt, mit der es möglich ist, zu gegebenen Gleichungen Lösungen anzugeben, insofern diese existieren. Da partielle Differentialgleichungen in ihrer Struktur komplizierter sind, gibt es wenige Theorien, die auf eine große Klasse von partiellen Differentialgleichungen angewandt werden kann. Daher untersucht man im Bereich der partiellen Differentialgleichungen meist nur einzelne oder kleinere Klassen von Gleichungen. Um Lösungen und Eigenschaften solcher Gleichungen zu finden werden vor allem Methoden aus der Funktionalanalysis und auch aus der Distributionentheorie und der mikrolokalen Analysis eingesetzt. Allerdings gibt es viele partielle Differentialgleichungen, bei denen mit Hilfe dieser analytischen Methoden erst wenige Informationen über die Lösungsstruktur in Erfahrung gebracht werden konnten. Ein in der Physik wichtiges Beispiel einer solch komplexen partiellen Differentialgleichung ist das System der Navier-Stokes-Gleichungen. Für diese und für andere partielle Differentialgleichungen versucht man in der numerischen Mathematik näherungsweise Lösungen zu finden.

Im Gegensatz zur reellen Analysis, die sich nur mit Funktionen reeller Variablen befasst, werden in der Funktionentheorie (auch komplexe Analysis genannt) Funktionen komplexer Variablen untersucht. Die Funktionentheorie hat sich von der reellen Analysis mit eigenständigen Methoden und andersartigen Fragestellungen abgesetzt. Jedoch werden einige Phänomene der reellen Analysis erst mit Hilfe der Funktionentheorie richtig verständlich. Das Übertragen von Fragestellungen der reellen Analysis in die Funktionentheorie kann daher zu Vereinfachungen führen.




</doc>
<doc id="239" url="https://de.wikipedia.org/wiki?curid=239" title="Annette von Droste-Hülshoff">
Annette von Droste-Hülshoff

Annette von Droste-Hülshoff (* 12. Januar 1797, nach anderen Quellen 10. Januar 1797, auf Burg Hülshoff bei Münster als "Anna Elisabeth Franzisca Adolphina Wilhelmina Ludovica Freiin von Droste zu Hülshoff"; † 24. Mai 1848 auf der Burg Meersburg in Meersburg) war eine deutsche Schriftstellerin und Komponistin. Sie gilt als eine der bedeutendsten deutschen Dichterinnen.

Annette von Droste-Hülshoff stammte aus dem altwestfälischen, katholischen Adel. Sie wurde als zweites von vier Kindern von Clemens-August II. von Droste zu Hülshoff (1760–1826) und Therese von Haxthausen (1772–1853) am 12. Januar 1797 auf der westfälischen Burg Hülshoff zwischen Havixbeck und Roxel bei Münster geboren. Annette von Droste-Hülshoff führte ein zurückgezogenes und eingeengtes Leben. In ihrer Kindheit und Jugend war sie kränklich, bedingt durch ihre frühe Geburt. Außerdem war sie extrem kurzsichtig. Sie wurde in den Jahren 1812 bis 1819 von Professor Anton Matthias Sprickmann unterrichtet und gefördert. Nach dem Tod ihres Vaters 1826 wurde der Familienbesitz von ihrem Bruder Werner übernommen, sodass sie und ihre ältere Schwester Jenny von Droste zu Hülshoff mit ihrer Mutter auf deren Witwensitz übersiedelten, das Haus Rüschhaus bei Gievenbeck.

Eine erste größere Reise führte sie 1825, ein Jahr vor dem Tod ihres Vaters, an den Rhein nach Köln, Bonn und Koblenz. In Bonn, wo ihr Vetter Clemens-August von Droste zu Hülshoff lebte, verband sie eine Freundschaft mit Sibylle Mertens-Schaaffhausen; zu deren Freundeskreis zählten außer Annette von Droste-Hülshoff Johanna und Adele Schopenhauer sowie Goethes Schwiegertochter Ottilie. In Bonn, das sie bis 1842 mehrfach besuchte, begegnete Annette von Droste-Hülshoff außerdem August Wilhelm Schlegel. Zwar stand Annette von Droste-Hülshoff in brieflichem Kontakt mit intellektuellen Zeitgenossen wie den Brüdern Grimm, sie entzog sich aber niemals den Anforderungen ihrer Familie, etwa wenn sie immer wieder als Krankenpflegerin herangezogen wurde. Da sie ständig selbst kränkelte, standen für sie ein Bruch mit der Familie oder der Versuch, durch ihre Schriftstellerei ihren Lebensunterhalt zu verdienen, nie zur Debatte. Wohl aber sah sie ihre Berufung als Dichterin. Auch ihre Mutter erkannte dies und unterstützte ihre Tochter, indem sie beispielsweise versuchte, den Kontakt mit Christoph Bernhard Schlüter herzustellen, was aber zunächst misslang, da dieser die zugesandten Manuskripte für nicht ausreichend erachtete.

Annette von Droste-Hülshoff nahm ihre literarische Arbeit sehr ernst und war sich bewusst, große Kunst zu schaffen. Ihre Balladen wurden berühmt "(Der Knabe im Moor)", wie auch ihre Novelle "Die Judenbuche". Ein wichtiges Dokument tiefer Religiosität ist ihr Gedichtzyklus "Das geistliche Jahr", in dem aber – typisch für die Zeit – auch die Zerrissenheit des Menschen zwischen aufgeklärtem Bewusstsein und religiöser Suche gestaltet wird. Die Ausführungen in diesem Werk werden heute als autobiographisch erachtet, da sie über 20 Jahre an dem gesamten Zyklus arbeitete.

Bedeutend für ihr literarisches Wirken waren ihre Reisen an den Bodensee, wo sie zunächst zusammen mit der Mutter ihre Schwester Jenny besuchte, die den Freiherrn Joseph von Laßberg („Sepp von Eppishusen“) geheiratet hatte, der sich mit mittelalterlicher Literatur beschäftigte.

Ab 1841 wohnte sie vorwiegend bei ihrem Schwager auf Schloss Meersburg am Bodensee, sah ihr Zuhause aber weiterhin im Rüschhaus bei Nienberge, wo unter anderem ihre Amme, die sie bis zu deren Tode pflegte, und ihre Mutter wohnten. Mit Levin Schücking verband sie seit 1837 eine Dichterfreundschaft. Er war der Sohn einer Freundin, die verstarb, als Schücking ca. 17 Jahre alt war. Durch Annette von Droste-Hülshoffs Vermittlung wurde er 1841 auf Schloss Meersburg Bibliothekar. Insbesondere unter dessen Inspiration entstand in Meersburg ein Großteil der „weltlichen“ Gedichte. Annette erwarb am 17. November 1843 ein Haus, das Fürstenhäusle, am Stadtrand inmitten der Weinberge in Meersburg. Am Nachmittag des 24. Mai 1848 verstarb Annette von Droste-Hülshoff auf Schloss Meersburg am Bodensee, vermutlich an einer schweren Lungenentzündung. Ihr Grab befindet sich auf dem Friedhof Meersburg in Meersburg nahe der alten Friedhofskapelle.

Annettes Werdegang zu einer der bedeutendsten Schriftstellerinnen ging zunächst mit dem einer Musikerin und Komponistin einher. Ihr Wirken als Komponistin wurde lange Zeit verdrängt oder vergessen. Dabei standen ihre Musik und ihr Dichten zunächst in Wechselwirkung zueinander.

Annettes Eltern waren offen für Musik, ihr Vater war selbst passionierter Violinist. Im Stammsitz der Droste-Hülshoffs auf Burg Hülshoff befindet sich noch heute eine ansehnliche Noten- und Musikmaterialien-Sammlung, die für das häusliche Musizieren im Familienkreis unerlässlich war. Die Kinder der Familie wurden oft in Konzert- und Musiktheaterveranstaltungen mitgenommen und mit der zeitgenössischen Musik vertraut gemacht. Annettes Onkel Maximilian-Friedrich von Droste zu Hülshoff war selbst Komponist und Freund Joseph Haydns. Ab 1809 erhielt Annette Klavierunterricht. Sie wurde oft gebeten, vorzuspielen oder andere am Klavier zu begleiten – so perfektionierte sie nach und nach ihr Können. 1812 schrieb ihre Mutter Therese begeistert, dass sich die Tochter „mit aller Heftigkeit ihres Charakters auf’s Componieren geworfen“ habe.

1820 gab Annette ihr erstes öffentliches gesangliches Konzert in Höxter. Erst spät, zwischen 1824 und 1831, erhielt Annette auch Gesangsunterricht. Über ihre Stimme wurde berichtet, sie sei „voll, aber oft zu stark u. grell, geht aber sehr tief, u. ist dann am angenehmsten“. Aus Köln wird berichtet, dass sie eine bessere Stimme als Angelica Catalani (1780–1849) gehabt habe, die als eine der besten Sopranistinnen ihrer Zeit galt. Annette gab auch anderen Familienmitgliedern Unterricht in Gesang und am Klavier.

1821 bekam Annette von ihrem Onkel Maximilian eine Ausgabe seiner Kompositionslehre "Einige Erklärungen über den General=Baß" geschenkt, worüber sie freudig schreibt: „Was folgt daraus? Dass ich aus Dankbarkeit das ganze Werk von Anfang bis Ende durchstudiere und auswendig lerne!“ Optimal vorbereitet – auch durch das Studium zeitgenössischer Musikschriften und Kompositionen – begann Annette zu komponieren. Zu vier Opernprojekten entstanden mehr oder weniger ausgeführte Libretti und Musik. 1836 wurde sie während eines Aufenthaltes im Schweizerischen Eppishausen auf das Lochamer Liederbuch aufmerksam gemacht und angeregt, die darin enthaltenen Lieder für Singstimme und Klavier zu bearbeiten. So haben sich rund 74 Lieder aus ihrer Feder erhalten, die sich ganz auf die Gebote der damaligen Liederschulen berufen und sich durch ihre leichte und eingängige Sangbarkeit auszeichnen.

Mit Clara Schumann und Robert Schumann stand Annette in brieflichem Kontakt: 1845 bat die berühmte Pianistin und Komponistin Annette um ein Libretto, damit es ihr Mann vertonen könne. Robert selbst hatte bereits ein Gedicht von Annette ("Das Hirtenfeuer", op. 59,5) in Musik gesetzt, das 1844 in einer Gedichtsammlung erschienen war, die er sehr schätzte.

Annette spielte ihre eigenen Werke nie öffentlich. Erst 1877 kam ihr Wirken als Komponistin ans Licht, als Christoph Bernhard Schlüter (1801–1884) einige Werke aus dem Nachlass der Dichterin veröffentlichen ließ ("Lieder mit Pianoforte-Begleitung. Componirt von Annette von Droste-Hülshoff"). Er setzte ihr auch im Nekrolog von 1848 ein Denkmal, indem er „ihr großes Talent für Gesang und Musik“ hervorhob und auch, dass sie die „seltenste Gabe“ besaß, „Poesie in Musik und Musik in Poesie zu übersetzen“. Erst im 20. Jahrhundert wurde ihr Nachlass komplett gesichtet und somit auch ihre Musik eingehender untersucht.

Annette von Droste-Hülshoff verknüpfte ihre musikalische Begabung mit einem hohen Anspruch, was aber auch zu einem Konflikt mit ihren literarischen Ambitionen führte: „das Operntextschreiben ist etwas gar zu Klägliches und Handwerksmäßiges“. Letztlich hat sich Annette für die Poesie entschieden – die Musik trat in den Hintergrund. Ihr (musikalischer) Nachlass befindet sich heute als Dauerleihgabe in der Universitäts- und Landesbibliothek Münster.
Ihre Schwester Jenny malte mehrere Porträts der Dichterin. Eine Miniatur, die 1820 von Jenny geschaffen worden war, diente später als Vorlage für die Gestaltung der vierten Serie der 20-DM-Banknote mit ihrer berühmten Schwester.
Als Jugendliche war Annette von Droste-Hülshoff um 1818 auch von C. H. N. Oppermann gemalt worden.

Neben einer Zeichnung von Adele Schopenhauer aus dem Jahr 1840 existieren viele Gemälde von Johann Joseph Sprick (1808–1842), den sie häufiger finanziell unterstützte.

Fotografisch porträtiert wurde sie von Friedrich Hundt, durch den Daguerreotypien von Annette von Droste-Hülshoff der Nachwelt erhalten blieben.

Neben der bereits erwähnten 20-DM-Banknote war Annette von Droste-Hülshoff auch als Motiv auf zwei deutschen Briefmarken-Dauerserien zu sehen: ab 1961 im Rahmen der Serie "Bedeutende Deutsche" sowie ab 2002 im Rahmen der Serie "Frauen der deutschen Geschichte".

Der Annette-von-Droste-Hülshoff-Preis, der Asteroid (12240) Droste-Hülshoff sowie der Droste-Preis der Stadt Meersburg wurden nach ihr benannt. Mehrere Schulen führen ihren Namen.

Im Garten von Burg Hülshoff befindet sich ein Denkmal von Anton Rüller und Heinrich Fleige aus dem Jahr 1896. Das Denkmal diente als Vorlage für eine Büste, die sich heute in der Nähe der Burg Meersburg befindet. Sie wurde kurz danach von Emil Stadelhofer gefertigt. Auch für das Brustbild auf einen Teil der Notmünzen der Provinz Westfalen und der dazugehörigen Zwittermedaille wurde dieses Denkmal als Vorlage verwendet.

Der Droste-Stein im Königslau einem Wald in der Nähe von Bökendorf wurde im Jahr 1964 zur Erinnerung an Annette von Droste-Hülshoff errichtet. Der Hinweis auf den Standort der "Judenbuche" beruht auf einem Irrtum. Der Mord an dem Juden Soistmann Berend aus Ovenhausen am 10. Februar 1783, der die Droste zu ihrer Novelle "Die Judenbuche" anregte, geschah am Südhang des Berges auf dem Waldweg von Bökendorf nach Ovenhausen.

Im Münsteraner "Tatort" bringen die Autoren immer wieder eine Hommage an Droste-Hülshoff unter, etwa in Folge 511 (, 2002), an deren Anfang die Ballade "Der Knabe im Moor" gebracht wird, oder in Folge 659 (, 2007), in der in einer nächtlichen Friedhofsszene "Die tote Lerche" rezitiert wird. Der Konstanzer Tatort widmete 2015 die 935. Folge "" dem Hochzeitswein der Annette von Droste-Hülshoff.

In Roxel wurde eine Straße nach Annette von Droste-Hülshoff benannt. Zudem wird der Stadtteil Münsters am Ortseingangsschild als Geburtsort der Dichterin beworben. Im Juni 2012 wurde bekannt, dass die Gemeinde Havixbeck, der nach der kommunalen Neuordnung im Jahre 1975 die Burg Hülshoff zugeschlagen wurde, plane, ihre Ortseingangsschilder mit dem Zusatz „Havixbeck – Geburtsort der Annette von Droste-Hülshoff“ zu versehen, was in Roxel als „Geschichtsverfälschung“ kritisiert wurde.

Sarah Kirsch drückt in ihrem Gedicht "Der Droste würde ich gerne Wasser reichen" ihre Bewunderung für die Kollegin aus, mit der sie, die „Spätgeborene“, „glucksend übers Moor“ geht, und interpretiert die Beziehung Droste-Hülshoffs zu Levin Schücking "(Ihr Lewin, Beide lieben wir den Kühnen)".

Am 28. September 2012 wurde die Annette von Droste zu Hülshoff-Stiftung offiziell anerkannt. Sie will das Geburtshaus der Dichterin auf Burg Hülshoff bei Havixbeck dauerhaft für die öffentliche Nutzung erhalten. Darüber hinaus werden literarische Veranstaltungen, Ausstellungen und Forschungsvorhaben gefördert.











</doc>
<doc id="240" url="https://de.wikipedia.org/wiki?curid=240" title="Aktualismus (Geologie)">
Aktualismus (Geologie)

Der Aktualismus (lat. ' „wirklich“), auch Aktualitätsprinzip"', Uniformitäts- oder Gleichförmigkeitsprinzip, englisch "Uniformitarianism", ist die grundlegende wissenschaftliche Methode in der Geologie.

Das Prinzip der "Gleichförmigkeit der Prozesse" besagt, dass die geologischen Vorgänge der Gegenwart sich nicht von denen der erdgeschichtlichen Vergangenheit unterscheiden. Diese Annahme bildet die theoretische Grundlage, um mithilfe vergleichender Ontologie von aktuellen geologischen Bildungsprozessen direkte Rückschlüsse auf solche in der Vergangenheit ziehen zu können. Finden sich beispielsweise in Sedimentgestein Strukturen, die denen in Sedimenten von heute gleichen (z. B. Rippelmarken), lässt sich daraus ableiten, dass sich die fossilen Strukturen im Gestein auf dieselbe Weise gebildet haben wie die rezenten im Sediment und es sich somit um dieselbe Art von Sedimentstruktur handelt.

Damit stellt der Aktualismus einen Sonderfall einer allgemeinen wissenschaftlichen Regel dar, des Einfachheitsprinzips. Es besagt, dass man keine zusätzlichen oder unbekannten Ursachen zur Erklärung eines Phänomens heranziehen soll, solange bekannte Ursachen dafür ausreichen. Das Gegenteil der aktualistischen Methode ist der Exzeptionalismus.

Noch allgemeiner gefasst ist das Axiom der "Gleichförmigkeit der Gesetze". Hierbei nimmt man an, dass überall und zu jeder Zeit dieselben Naturgesetze herrschen und geherrscht haben. Wie jedes Axiom ist es prinzipiell nicht beweisbar, aber ohne diese Grundannahme wäre wissenschaftliches Arbeiten von vornherein unmöglich.

Der Aktualismus wurde als Hypothese mehrfach angefochten, zuletzt durch die Vertreter der "Historizität". Dieser Begriff besagt, dass die Naturgesetze sich mit den Bedingungen in geologischen Zeiträumen geändert haben können. Wir würden also heute mit einem Maßstab messen, der sich zeitabhängig geändert hätte.

Die theoretische Physik geht hingegen von einer Permanenz und Konstanz der Naturgesetze aus. Die Veränderlichkeit von Anfangs- und Randbedingungen geologischer Prozesse im Verlauf der Erdgeschichte reicht demnach aus, um die in der geologischen Überlieferung auftretenden Phänomene, die kein gegenwärtiges Pendant haben, zu erklären.

Von einigen Vorläufern abgesehen (Georg Christian Füchsel, Georges-Louis Leclerc de Buffon) wurde der Aktualismus zuerst 1785 von James Hutton (1726–1797) in seinem Werk "Theory of the Earth" formuliert und danach von Charles Lyell in seinem Hauptwerk "Principles of Geology" (1830) weiterentwickelt. Allerdings vermengte Lyell diese methodologischen Ansätze, die selbst von seinen Gegnern nie bezweifelt wurden, in geschickter (aber unzulässiger) Weise mit seiner Theorie von der Gleichförmigkeit der Veränderungen (Gradualismus). Im Gegensatz zum damals noch herrschenden Erklärungsmodell des Katastrophismus glaubte Lyell, dass es in der Erdgeschichte niemals zu Phasen erhöhter geologischer Aktivität gekommen sei, wie etwa verstärkter Vulkanismus, besondere Gebirgsbildungsphasen oder eine schubweise beschleunigte Entwicklung der Lebewesen. Selbst umfassende Umwälzungen der Erde seien ausschließlich durch die langsame Summierung von unzähligen kleinen Ereignissen zu erklären, die sich nach und nach, im Laufe riesiger Zeiträume, akkumuliert hätten. Ebenso vertrat Lyell die Gleichförmigkeit der Zustände. Zum Beispiel widersprach er der damals (und heute) gängigen Ansicht, die Erde müsse aus einem einstmals glutflüssigen Zustand erstarrt sein, und behauptete ein immer gleichbleibendes Verhältnis zwischen kontinentaler Kruste und Ozeanbecken.

Auf Grund von Lyells rhetorischem Geschick wurden seine gradualistischen Ansichten rasch von breiten Kreisen übernommen, und besonders Georges Cuviers katastrophistische Kataklysmentheorie wurde schon um 1850 fast vollständig zurückgedrängt. Obwohl Charles Darwin in seiner Evolutionstheorie ebenfalls eine sehr langsame Entwicklung der Lebewesen in unmerklich kleinen Schritten annahm und damit ganz erheblich zur allgemeinen Akzeptanz des Gradualismus beitrug, kostete es Lyell große Mühe, Darwins Theorie zu akzeptieren. Seiner Meinung nach implizierte die Entstehung völlig neuer biologischer Arten eine viel zu große (weil unumkehrbare) Veränderung im Laufe der Erdgeschichte. Erst gegen Ende seines Lebens gab Lyell unter der erdrückenden Last der Belege nach und schloss sich Darwins Theorie über die gerichtete Weiterentwicklung der Lebewesen an. Ebenso sah sich Lyell später gezwungen, die Eiszeittheorie von Louis Agassiz anzuerkennen. Dieser wandte bei seiner Erforschung der Gletscher zwar ebenfalls klare aktualistische Methoden an, blieb aber als Schüler Cuviers Anhänger einer Katastrophenlehre.

Weitere frühe Vertreter des Aktualismus waren Constant Prévost und Karl Ernst Adolf von Hoff.

Schon Lyells Zeitgenosse Karl Ernst Adolf von Hoff erkannte in seinem Werk "Geschichte der durch Überlieferung nachgewiesenen natürlichen Veränderungen der Erdoberfläche" (1822–1834), dass der Aktualismus als wissenschaftliche Methode zwar unumgänglich ist, dass er aber als Theorie gelegentlich an seine Grenzen stößt. Hoff war der Meinung, dass zuweilen besondere Hypothesen zur Erklärung früherer Vorgänge herangezogen werden dürften, jedoch nur, wenn die Beobachtungen gegenwärtiger Vorgänge und Kräfte dazu nicht ausreichten. Wegen des Erfolges des Lyell’schen Gradualismus gerieten solche Ansätze jedoch weitgehend in Vergessenheit.

Erst im Laufe der 1960er und 70er Jahre setzten sich verschiedene Autoren, wie Reijer Hooykaas, Stephen Jay Gould, Martin Rudwick und Roy Porter, wieder kritisch mit dem mehrdeutigen Gleichförmigkeitsbegriff auseinander, wobei sie besonders die Trennung der (axiomatischen) Methode von der (widerlegbaren) Theorie herausarbeiteten. Dies erleichterte in der Folge die „Renaissance“ von katastrophistischen Theorien über den Verlauf der Erdgeschichte, wie zum Beispiel die Deutung der weltweiten Iridium-Anomalie als Resultat eines Meteoriteneinschlags an der Grenze Kreide-Tertiär.

Ein Beispiel für die Grenzen der eigentlichen aktualistischen Methode liefert die Interpretation archaischer Tektonik. Hierfür ist es nötig, auf Laborexperimente zurückzugreifen, denn im Archaikum hatte sich das Gestein der Erdkruste noch nicht in die heute zu beobachtenden kontinentalen und ozeanischen Krusten getrennt, konnte sich somit auch noch nicht nach den Gesetzen der heute wirkenden Tektonik verhalten. Ebenso war der Sauerstoffgehalt in der archaischen Atmosphäre so gering, dass sich als Folge riesige Lagerstätten von Eisenmineralen formen konnten, die Banded Iron Formations, deren Bildung heute völlig ausgeschlossen wäre. Generell gilt, dass mit zunehmendem Abstand von der Gegenwart eine aktualistische Deutung geowissenschaftlicher Befunde immer unsicherer wird.

"Siehe auch": Geschichte der Geologie




</doc>
<doc id="242" url="https://de.wikipedia.org/wiki?curid=242" title="Albrecht (Preußen)">
Albrecht (Preußen)

Albrecht von Preußen (* 17. Mai 1490 in Ansbach; † 20. März 1568 auf Burg Tapiau) war ein Prinz von Ansbach aus der fränkischen Linie der Hohenzollern und ab 1511 der letzte Hochmeister des Deutschen Ordens in Preußen. Er trat 1525 zur Reformation über, säkularisierte den Deutschen Orden in Preußen in seiner Eigenschaft als eine Ordensgemeinschaft und verwandelte als 1. Herzog von Preußen die katholisch dominierte weltliche Herrschaft des Deutschordensstaates in Preußen in das erbliche lutherische Herzogtum Preußen, das er bis zu seinem Tod als Herzog regierte.

Geboren wurde Albrecht am 17. Mai 1490 in Ansbach. Sein Vater war Friedrich V., Markgraf von Brandenburg-Ansbach. Seine Mutter Sofia Jagiellonica war eine Tochter des polnischen Königs Kasimir IV. Jagiello und der Elisabeth von Habsburg, einer Tochter des deutschen Königs Albrecht II. und Enkelin Kaiser Sigismunds. Seine Eltern bestimmten Albrecht im Sinne der Dispositio Achillea zur geistlichen Laufbahn.

In seinem 21. Lebensjahr wählte ihn der Deutsche Orden 1511 zum 37. Hochmeister. Der Orden beabsichtigte, die 1466 im Zweiten Frieden von Thorn gegenüber dem König von Polen eingegangene Heeresfolge abzuschütteln. Voraussetzung war, dass der neu gewählte Hochmeister den Lehnseid gegenüber dem König Sigismund I. verweigert. Daher erschien Albrecht, der Sohn eines regierenden Fürsten des Heiligen Römischen Reichs und Neffe Sigismunds, dem Ordenskapitel für das Hochmeisteramt als besonders geeignet. Im Vertrauen auf die Beistandspflicht des Deutschmeisters und des Landmeisters von Livland verweigerte Albrecht dem polnischen König den Lehnseid. Sigismund erreichte jedoch 1513 eine Mahnung des Papstes an Albrecht und 1515 von Kaiser Maximilian die Anerkennung des Friedens von 1466, wofür er im Gegenzug dessen Königtum in Böhmen und Ungarn unterstützte.

Nachdem Maximilians Nachfolger Karl V. bei seiner Thronbesteigung 1519 Albrecht zum Lehnseid aufgefordert hatte und klar geworden war, dass weder aus dem Reich noch aus Livland Unterstützung für Albrecht zu erwarten war, fielen polnische Truppen im Winter 1519/1520 in den Ordensstaat ein, um den Orden zu unterwerfen. Wider Erwarten kam es zu keiner Entscheidung. Dänische Unterstützung, ein Söldnerheer aus dem Reich und vor allem die Angst vor dem mit Albrecht verbündeten Russland veranlassten Sigismund mit Albrecht, dessen Söldner immer aufsässiger wurden, im April 1521 durch Vermittlung des Papstes und des Kaisers einen vierjährigen Waffenstillstand zu schließen. 

In den folgenden zwei Jahren verlief Albrechts Suche nach Unterstützung im Reich unglücklich, während Sigismund sich mit Moskau arrangierte. Jedoch wurde Albrecht 1522 während der Religionskämpfe in Nürnberg von Andreas Osiander für die Reformation gewonnen. Auf Luthers Rat entschloss er sich im November 1523, bestätigt durch Sigismunds Gesandten Achatius von Zehmen, das Amt des Hochmeisters niederzulegen, den Deutschordensstaat in ein weltliches Herzogtum umzuwandeln und dort die Reformation einzuführen. Vor Sigismund legte Albrecht am 8. April 1525 in Krakau den Huldigungseid ab, in dem Albrecht Preußen als ein in gerader, männlicher Linie forterbendes Herzogtum zu Lehen nahm. Mitbelehnt wurden seine Brüder Kasimir und Georg. Auf dem Landtag, der kurz darauf in Königsberg gehalten wurde, erklärten sich alle Stände mit dem Bischof von Samland, Georg von Polenz, an der Spitze für die Anerkennung des Herzogtums und für die Annahme der Reformation.

Albrecht setzte an die Durchführung seines Werkes alle Kraft. Sofort erschien eine neue Kirchenordnung, und die Versuche des Deutschen Ordens, Albrecht wieder zu verdrängen, sowie die beim Kammergericht in Deutschland 1531 gegen den Herzog erwirkte und am 18. Januar 1532 verhängte Reichsacht hatten keine andere Wirkung, als dass dieser die Einführung der evangelischen Lehre und die Befestigung seiner Herrschaft umso eifriger betrieb. Das bedeutete das Ende des Ordensstaates in Preußen.

Ganz besonders förderte Albrecht das Schulwesen: In den Städten legte er Lateinschulen an, gründete 1540 das Gymnasium in Königsberg und 1544 die Albertus-Universität Königsberg. Deutsche Schulbücher (Katechismen etc.) ließ er auf eigene Kosten drucken, und Leibeigenen, welche sich dem Lehrgeschäft widmen wollten, gab er die Freiheit. Von ihm stammt der Text der ersten drei Strophen des Kirchenliedes „Was mein Gott will, gescheh allzeit“ ("Evangelisches Gesangbuch" Nr. 364). Albrecht legte auch den Grundstock zur königlichen Bibliothek, dessen 20 prächtigste Bände er für seine zweite Gattin Anna Maria von Braunschweig in reinem Silber beschlagen ließ. Sie erhielt daher den Namen Silberbibliothek.

Seine letzten Regierungsjahre wurden ihm durch kirchliche und politische Zerwürfnisse vielfach verbittert. Der Streit des Königsberger Professors Andreas Osiander, der Melanchthon heftig anfeindete, mit seinen Kollegen, namentlich mit Joachim Mörlin, gab Anlass zu ernsten Verwicklungen. Der Herzog stand auf Seiten Osianders, der größte Teil der Geistlichkeit, auf das Volk gestützt, hielt es mit dem des Landes verwiesenen Mörlin, ebenso die Städte und der Adel, weil jene so die Anerkennung ihrer ehemaligen Vorrechte, dieser dagegen die Beschränkung der herzoglichen Gewalt auf das Verhältnis des ehemaligen Hochmeisters zu seinem Orden zu erreichen hofften. Fast das ganze Land stand dem Fürsten feindselig gegenüber, der angeklagt wurde, die Ausländer zu sehr zu begünstigen, in der Tat viele Jahre sich vom kroatischen Abenteurer und Universalgelehrten Stanislav Pavao Skalić hatte beherrschen lassen und überdies sehr verschuldet war. Die Stände suchten Hilfe in Polen. Daraufhin sandte Polen 1566 eine Kommission nach Königsberg, die gegen den Herzog entschied. Des Herzogs Beichtvater Johann Funck, der Schwiegersohn Osianders, und zwei Verbündete wurden als Hochverräter zum Tode verurteilt, Mörlin wurde zurückberufen und zum Bischof von Samland ernannt. Als solcher schrieb er zur Verdammung der Osianderschen Lehren das symbolische Buch Preußens: "Repetitio corporis doctrinae Prutenicae". Neue Räte wurden dem Herzog von der polnischen Kommission und den Ständen aufgenötigt. Von ihnen abhängig, verlebte Albrecht seine letzten Tage in tiefem Kummer.

Albrecht starb am 20. März 1568 auf der Burg Tapiau an der Pest, 16 Stunden nach ihm auch seine zweite Gemahlin Anna Maria.

Der kroatische Humanist, Priester, Universalgelehrter und Verfasser des ersten Werkes, in dessen Titel das Wort "Enzyklopädie" in der heutigen Bedeutung vorkommt, Pavao Skalić, war Albrechts Berater.

Herzog Albrecht heiratete 1526 in erster Ehe Dorothea von Dänemark und Norwegen (1504–1547), Tochter von Friedrich I. (Dänemark und Norwegen). Aus dieser Ehe stammen sechs Kinder:


In zweiter Ehe heiratete er 1550 Anna Maria von Braunschweig, Tochter von Herzog Erich I. (Braunschweig-Calenberg-Göttingen). Aus dieser Ehe stammen zwei Kinder:


Ein Bildnisrelief Herzog Albrechts befand sich seit 1553 am Collegium Albertinum (Königsberg).

Im Evangelischen Namenkalender wird seiner am 20. März gedacht.

Die 1913 in Königsberg errichtete Herzog-Albrecht-Gedächtniskirche wurde 1972 abgerissen.

Ansbach ehrt den Gründer der ersten Evangelisch-Lutherischen Landeskirche mit einem Denkmal des Bildhauers Friedrich Schelle.





</doc>
<doc id="243" url="https://de.wikipedia.org/wiki?curid=243" title="Amarant">
Amarant

Amarant steht für:

Siehe auch:


</doc>
<doc id="245" url="https://de.wikipedia.org/wiki?curid=245" title="Ammoniak">
Ammoniak

Ammoniak [], auch: [], österr.: [] ist eine chemische Verbindung von Stickstoff und Wasserstoff mit der Summenformel NH. Es ist ein stark stechend riechendes, farbloses, wasserlösliches und giftiges Gas, das zu Tränen reizt und erstickend wirkt. Ammoniak ist ein amphoterer Stoff: Unter wässrigen Bedingungen wirkt es als Base. Es bildet mehrere Reihen von Salzen: die kationischen Ammonium<nowiki>salze</nowiki> sowie die anionischen Amide, Imide und Nitride, bei denen ein (Amide), zwei (Imide) oder alle (Nitride) Protonen (Wasserstoffionen) durch Metallionen ersetzt sind.

Ammoniak ist eine der meistproduzierten Chemikalien und Grundstoff für die Produktion aller weiteren Stickstoffverbindungen. Der größte Teil des Ammoniaks wird zu Düngemitteln, insbesondere Harnstoff und Ammoniumsalzen, weiterverarbeitet. Die Herstellung erfolgt fast ausschließlich über das Haber-Bosch-Verfahren aus den Elementen Wasserstoff und Stickstoff.

Biologisch hat Ammoniak eine wichtige Funktion als Zwischenprodukt beim Auf- und Abbau von Aminosäuren. Aufgrund der Giftigkeit größerer Ammoniakmengen wird es zur Ausscheidung im Körper in den ungiftigen Harnstoff oder, beispielsweise bei Vögeln, in Harnsäure umgewandelt.

Natürlich vorkommende Ammoniumverbindungen sind schon seit langer Zeit bekannt. So wurde Ammoniumchlorid (Salmiak) schon in der Antike in Ägypten durch Erhitzen von Kamelmist gewonnen. Beim Erhitzen bildet sich Ammoniak, das durch Reaktion mit Chlorwasserstoff Ammoniumchlorid als weißen Rauch bildet. Sowohl Salmiak als auch Ammoniak leiten sich vom lateinischen "sal ammoniacum" ab, das wiederum auf den antiken Namen der Oase Siwa (Oase des Ammon oder Amun) zurückgeht. In der Nähe der Oase befanden sich große Salzvorkommen, allerdings handelte es sich dabei wohl um Natriumchlorid und nicht um natürlich vorkommendes Ammoniumchlorid.

Gasförmiges Ammoniak wurde erstmals 1716 von Johannes Kunckel erwähnt, der Gärvorgänge beobachtete. Isoliert wurde das Gas erstmals 1774 von Joseph Priestley. Weitere Forschungen erfolgten durch Carl Wilhelm Scheele und Claude-Louis Berthollet, die die Zusammensetzung des Ammoniaks aus Stickstoff und Wasserstoff erkannten, sowie William Henry, der das exakte Verhältnis der beiden Elemente von 1:3 und damit die chemische Formel NH bestimmte.

In größerer Menge wurde Ammoniak ab 1840 benötigt, nachdem Justus von Liebig die Stickstoffdüngung zur Verbesserung der Erträge in der Landwirtschaft entwickelt hatte. Zunächst wurde Ammoniak als Nebenprodukt bei der Destillation von Kohle gewonnen, dies war jedoch nach kurzer Zeit nicht mehr ausreichend, um die Nachfrage nach Düngemittel zu decken. Ein erstes technisches Verfahren, um größere Mengen Ammoniak zu gewinnen, war 1898 das Frank-Caro-Verfahren, bei dem Calciumcarbid und Stickstoff zu Calciumcyanamid und dieses anschließend mit Wasser zu Ammoniak umgesetzt wurden.

Ab etwa 1900 begann Fritz Haber, aber auch Walther Nernst, mit der Erforschung der direkten Reaktion von Stickstoff und Wasserstoff zu Ammoniak. Sie erkannten bald, dass diese Reaktion bei Normalbedingungen nur in sehr geringem Umfang stattfindet und dass für hohe Ausbeuten hohe Temperaturen, ein hoher Druck sowie ein geeigneter Katalysator nötig sind. 1909 gelang es Haber erstmals, mit Hilfe eines Osmiumkatalysators Ammoniak im Labormaßstab durch Direktsynthese herzustellen. Daraufhin versuchte er mit Hilfe von Carl Bosch dieses Verfahren, das spätere Haber-Bosch-Verfahren auch im industriellen Maß anzuwenden. Dies gelang nach Überwindung der durch das Arbeiten unter hohem Druck verursachten technischen Probleme 1910 im Versuchsbetrieb. 1913 wurde bei der BASF in Ludwigshafen die erste kommerzielle Fabrik zur Ammoniaksynthese in Betrieb genommen. Dabei wurde ein inzwischen von Alwin Mittasch entwickelter Eisen-Mischkatalysator anstatt des teuren Osmiums genutzt. Dieses Verfahren wurde schon nach kurzer Zeit in großem Maßstab angewendet und wird bis heute zur Ammoniakproduktion genutzt. 1918 erhielt Fritz Haber für die Entwicklung der Ammoniaksynthese den Chemie-Nobelpreis, 1931 zusammen mit Friedrich Bergius auch Carl Bosch für die Entwicklung von Hochdruckverfahren in der Chemie.

Über die genauen Abläufe der Reaktion am Katalysator war dagegen lange Zeit nichts Genaues bekannt. Da es sich hierbei um Oberflächenreaktionen handelt, konnten sie erst nach der Entwicklung geeigneter Techniken wie dem Ultrahochvakuum oder dem Rastertunnelmikroskop untersucht werden. Die einzelnen Teilreaktionen der Ammoniaksynthese wurden dabei von Gerhard Ertl entdeckt, der hierfür auch den Nobelpreis für Chemie 2007 erhielt.

Die Reaktion von Ammoniak zu Salpetersäure wurde erstmals ab 1825 von Frédéric Kuhlmann untersucht. Ein technisch anwendbares Verfahren für die Salpetersäuresynthese aus Ammoniak wurde mit dem heutigen Ostwald-Verfahren Anfang des 20. Jahrhunderts von Wilhelm Ostwald entwickelt. Dieses wurde nach Entwicklung des Haber-Bosch-Verfahrens auch technisch wichtig und löste bald weitgehend das bisherige Produktionsverfahren aus teurem Chilesalpeter ab.

Da Ammoniak leicht mit sauren Verbindungen reagiert, kommt freies Ammoniakgas nur in geringen Mengen auf der Erde vor. Es entsteht bei der Zersetzung von abgestorbenen Pflanzen und tierischen Exkrementen. Bei der sogenannten Humifizierung werden stickstoffhaltige Bestandteile der Biomasse durch Mikroorganismen so abgebaut, dass unter anderem Ammoniak entsteht. Dieses gelangt als Gas in die Luft, reagiert dort jedoch mit Säuren wie Schwefel- oder Salpetersäure und bildet die entsprechenden Salze. Diese können auch über größere Strecken transportiert werden und gelangen leicht in den Boden. Wichtige Quellen für die Ammoniakemission sind Vulkanausbrüche, die Viehhaltung wie die Rindermast und auch der Verkehr.

Ammoniumsalze sind dagegen auf der Erde weit verbreitet. Das häufigste Ammoniumsalz ist Salmiak (Ammoniumchlorid), aber auch Diammoniumhydrogenphosphat (Phosphammit), Ammoniumsulfat (Mascagnin) und eine Anzahl komplizierter aufgebauter Ammoniumsalze mit weiteren Kationen sind aus der Natur bekannt. Diese findet man vor allem in der Umgebung von Vulkanen oder brennenden Kohleflözen, in denen organische Substanzen unter anderem zu Ammoniak zersetzt werden. So wird Salmiak vorwiegend als Sublimationsprodukt um Fumarolen gefunden, wo sich die im heißen Dampf enthaltenen Chlorwasserstoff- und Ammoniak-Gase als Ammoniumchlorid niederschlagen.

Auch viele Gesteine und Sedimente, vor allem Muskovit, Biotit und Feldspat-Minerale, enthalten Ammonium. Dagegen enthalten Quarzgesteine nur geringe Mengen Ammonium. Für die Verteilung spielt neben dem Ursprung des Ammoniums auch das Entweichen von Ammoniak bei der Metamorphose eine Rolle.

Ammoniak kommt auch im Weltall vor. Es war 1968 das erste Molekül, das durch sein Mikrowellenspektrum im interstellaren Raum gefunden wurde. Auch auf den Gasplaneten des Sonnensystems kommt Ammoniak vor.

Ammoniak ist eine Grundchemikalie und wird in großem Maßstab produziert. Im Jahr 2011 wurden weltweit 136 Millionen Tonnen hergestellt. Die Hauptproduzenten sind die Volksrepublik China, Indien, Russland und die Vereinigten Staaten. Für die Ammoniakproduktion werden große Mengen fossiler Energieträger benötigt. Der Anteil der Ammoniakproduktion am weltweiten Verbrauch fossiler Energieträger beträgt etwa 1,4 %. Pro Tonne produziertem Ammoniak werden etwa 1,87 Tonnen Kohlenstoffdioxid freigesetzt.<ref name="DOI10.1038/srep01145">Rong Lan, John T. S. Irvine, Shanwen Tao: "Synthesis of ammonia directly from air and water at ambient temperature and pressure." In: "Scientific Reports." 3, 2013, .</ref>

Über 90 % des produzierten Ammoniaks wird in der Direktsynthese über das Haber-Bosch-Verfahren produziert. Dabei reagieren die Gase Stickstoff und Wasserstoff in einer heterogenen Katalysereaktion in großen Reaktoren miteinander.

Vor der eigentlichen Reaktion müssen zunächst die Ausgangsstoffe gewonnen werden. Während Stickstoff als Luftbestandteil in großen Mengen zu Verfügung steht und durch Luftverflüssigung gewonnen wird, muss Wasserstoff zunächst aus geeigneten Quellen hergestellt werden. Das wichtigste Verfahren stellt dabei die Dampfreformierung dar, bei dem vor allem Erdgas, aber auch Kohle und Naphtha in zwei Schritten mit Wasser und Sauerstoff zu Wasserstoff und Kohlenstoffdioxid umgesetzt werden. Nach Abtrennung des Kohlenstoffdioxides wird der Wasserstoff im richtigen Verhältnis mit Stickstoff gemischt und je nach Verfahren auf 80–400 bar, typischerweise auf 150–250 bar, verdichtet.

Das Gasgemisch wird in den Reaktionskreislauf eingespeist. Dort wird es zunächst zur Entfernung von Wasserspuren gekühlt und anschließend an Wärmetauschern auf 400–500 °C erhitzt. Das heiße Gasgemisch kann nun im eigentlichen Reaktor an Eisenkatalysatoren, die mit verschiedenen Promotoren wie Aluminiumoxid oder Calciumoxid vermischt sind, zu Ammoniak reagieren. Aus wirtschaftlichen Gründen werden die Gase im technischen Betrieb nur eine kurze Zeit den Katalysatoren ausgesetzt, so dass sich das Gleichgewicht nicht einstellen kann und die Reaktion nur unvollständig abläuft. Das Gasgemisch, das nun einen Ammoniakgehalt von etwa 16,4 % hat, wird in mehreren Stufen abgekühlt, so dass das Ammoniak flüssig wird und abgetrennt werden kann. Das verbleibende Gemisch aus Stickstoff, Wasserstoff und einem kleinen Restanteil Ammoniak wird zusammen mit frischem Gas wieder in den Kreislauf eingespeist.

Eine mögliche Katalysator-Alternative wäre Ruthenium, das eine deutlich höhere Katalysatoraktivität besitzt und damit höhere Ausbeuten bei niedrigen Drücken ermöglicht. Aufgrund des hohen Preises für das seltene Edelmetall Ruthenium findet die industrielle Anwendung eines solchen Katalysators aber bislang nur in geringem Umfang statt.

2014 wurde eine alternative Synthese zum Haber-Bosch-Verfahren mit deutlich geringerem Energieverbrauch vorgestellt. Hierbei dienen Nanopartikel von Eisen(III)-oxid, die einem äquimolaren Gemisch aus Kaliumhydroxid und Natriumhydroxid zugesetzt werden, als Katalysator. Das Gemisch wird auf 200 °C erhitzt und unter Spannung gesetzt (1,2 V). Wasserdampf und Luft werden zugefügt, worauf sich Ammoniak bildet. Der Wirkungsgrad bezogen auf die eingesetzte elektrische Ladung (Faradayscher Wirkungsgrad) beträgt 35 %.

Mehrere Arbeitsgruppen arbeiten an einer CO-neutralen Ammoniakproduktion, auf der Basis von elektrochemischen Verfahren („elektrochemische Ammoniaksynthese“). Der durch Elektrolyse von Wasser erzeugte Wasserstoff soll dabei in Gegenwart bestimmter Katalysatoren und Membranen direkt mit Stickstoff zu Ammoniak reagieren. Der Strom soll dabei künftig im Wesentlichen aus regenerativen Quellen stammen.<ref name="DOI10.1038/srep01145">Rong Lan, John T. S. Irvine, Shanwen Tao: "Synthesis of ammonia directly from air and water at ambient temperature and pressure." In: "Scientific Reports." 3, 2013, .</ref>

Ammoniak ist bei Raumtemperatur ein farbloses, diamagnetisches, stechend riechendes Gas. Unterhalb von −33 °C wird es flüssig. Die Flüssigkeit ist farblos und stark lichtbrechend. Auch durch Druckerhöhung lässt sich das Gas leicht verflüssigen; bei 20 °C ist schon ein Druck von 900 kPa ausreichend. Die kritische Temperatur ist 132,4 °C, der kritische Druck beträgt 113 bar, die kritische Dichte ist 0,236 g/cm.
Innerhalb des Bereichs von 15,4 bis 33,6 Vol-% (108–336 g/m) ist Ammoniak explosionsgefährlich. Seine Zündtemperatur liegt bei 630 °C.

In der flüssigen Phase bildet Ammoniak Wasserstoffbrückenbindungen aus, was den verhältnismäßig hohen Siedepunkt und eine hohe Verdampfungsenthalpie von 23,35 kJ/mol begründet. Um diese Bindungen beim Verdampfen aufzubrechen, wird viel Energie gebraucht, die aus der Umgebung zugeführt werden muss. Deshalb eignet sich flüssiges Ammoniak zur Kühlung. Vor der Verwendung der Halogenkohlenwasserstoffe war Ammoniak ein häufig benutztes Kältemittel in Kühlschränken.

Unterhalb von −77,7 °C erstarrt Ammoniak in Form von farblosen Kristallen. Es kristallisiert dabei im kubischen Kristallsystem mit einem Gitterparameter a = 508,4 pm (−196 °C). Bei −102 °C beträgt der Gitterparameter a =  513,8 pm. Die Struktur lässt sich von einem kubisch-flächenzentrierten Gitter ableiten, wobei sechs der zwölf Nachbarmoleküle näher zum Zentralmolekül gelegen sind, als die übrigen sechs. Jedes freie Elektronenpaar ist dabei mit jeweils drei Wasserstoffatomen koordiniert.

Ammoniak ist aus einzelnen Molekülen aufgebaut, die jeweils aus einem Stickstoff- und drei Wasserstoffatomen bestehen. Die Atome sind dabei nicht in einer Ebene, sondern in Form einer dreiseitigen Pyramide angeordnet. Das Stickstoffatom bildet die Spitze, die Wasserstoffatome die Grundfläche der Pyramide. Für diese Form verantwortlich ist ein freies Elektronenpaar des Stickstoffs. Wird dieses berücksichtigt, entspricht die Struktur der eines verzerrten Tetraeders. Gemäß dem VSEPR-Modell ergibt sich durch das freie Elektronenpaar eine Abweichung vom idealen Tetraederwinkel (109,5°) zu einem Wasserstoff-Stickstoff-Wasserstoff-Winkel von 107,3°. Dieser liegt damit zwischen den Bindungswinkeln im Methan (idealer Tetraederwinkel von 109,5°) und Wasser (größere Verzerrung durch zwei freie Elektronenpaare, Winkel 104,5°). Die Bindungslänge der Stickstoff-Wasserstoff-Bindung im Ammoniak liegt bei 101,4 pm, was wiederum zwischen den Bindungslängen im Methan von 108,7 pm und Wasser (95,7 pm) liegt. Dies lässt sich durch die zunehmende Elektronegativitätsdifferenz von Kohlenstoff über Stickstoff zu Sauerstoff und damit einer stärkeren polaren Bindung erklären.

Das Ammoniakmolekül ist nicht starr, die Wasserstoffatome können über einen planaren Übergangszustand auf die andere Seite der Pyramide klappen. Die Energiebarriere für die pyramidale Inversion ist mit 24,2 kJ/mol so klein, dass sich bei Raumtemperatur von Ammoniak und davon ableitbaren Aminen NR (R: organische Reste) keine Enantiomere isolieren lassen. Ammoniakmoleküle besitzen eine sehr exakte und konstante Schwingungsfrequenz von 23,786 GHz, die zur Zeitmessung verwendet werden kann. Unter anderem wurde die erste Atomuhr mit Hilfe der Ammoniak-Schwingungsfrequenz konstruiert.

Flüssiges Ammoniak ist ein gutes Lösungsmittel und zeigt ähnliche Eigenschaften wie Wasser. Es löst viele organische Verbindungen, wie Alkohole, Phenole, Aldehyde und Ester und viele Salze unter Solvatisierung der sich bildenden Ionen.

Die Flüssigkeit unterliegt einer Autoprotolyse in Analogie zu Wasser mit dem Ionenprodukt von nur 10 mol/l und einem Neutralpunkt von 14,5:

Flüssiges Ammoniak reagiert mit elementaren Alkalimetallen, sowie elementarem Calcium, Strontium und Barium, unter Bildung von tiefblauen Lösungen. Es bilden sich solvatisierte Metallionen und solvatisierte Elektronen. Die Farbe wird durch solvatisierte Elektronen verursacht, die ohne Bindung zu einem bestimmten Atom in der Lösung vorhanden sind. Diese verursachen auch eine gute elektrische Leitfähigkeit der Lösungen.

Solche Lösungen werden zur Reduktion von Aromaten verwendet, siehe Birch-Reduktion. Die Lösung ist über längere Zeit stabil, in einer Redoxreaktion bildet sich unter Freisetzung von elementaren Wasserstoff langsam ein Metallamid M'NH, in Gegenwart eines Katalysators, wie Eisen(II)-chlorid rasch:

In Wasser ist Ammoniak sehr gut löslich. Bei 0 °C lösen sich 1176 Liter Ammoniak in einem Liter Wasser. Die Lösungen werden Ammoniumhydroxid, "Salmiakgeist" oder "Ammoniakwasser" genannt und reagieren basisch.

Als Base mit einer Basenkonstante p"K" von 4,76 reagiert Ammoniak mit Wasser unter Bildung von Hydroxidionen (OH):

Das Gleichgewicht der Reaktion liegt jedoch weitgehend auf der Seite von Ammoniak und Wasser. Ammoniak liegt daher weitgehend als molekular gelöste Verbindung vor. In wässrigen Lösungen bilden sich keine Amid-Ionen (NH), da diese in Wasser eine sehr starke Base mit p"K" = −9 wären und Ammoniak somit hier nicht als Säure (Protonendonator) reagiert. Mit einer starken Säure setzt sich Ammoniak zu Ammoniumionen (NH) um:

Im klassischen Sinne einer Neutralisation von Ammoniak bildet sich eine Lösung von Ammoniumsalz. Mit Salzsäure bildet sich Ammoniumchlorid:

Ammoniak kann mit Sauerstoff reagieren und zu Stickstoff und Wasser verbrennen. An der Luft lässt sich Ammoniak zwar entzünden, die freiwerdende Energie reicht aber nicht für eine kontinuierliche Verbrennung aus; die Flamme erlischt. In reinem Sauerstoff verbrennt Ammoniak dagegen gut, bei höherem Druck kann diese Reaktion auch explosionsartig erfolgen. Eine entsprechende Reaktion erfolgt auch mit starken Oxidationsmitteln wie Halogenen, Wasserstoffperoxid oder Kaliumpermanganat.

In Gegenwart von Platin- oder Rhodium-Katalysatoren reagiert Ammoniak und Sauerstoff nicht zu Stickstoff und Wasser, sondern zu Stickoxiden, wie etwa Stickstoffmonoxid. Diese Reaktion wird bei der Produktion von Salpetersäure im Ostwald-Verfahren genutzt.

Mit besonders reaktionsfähigen Metallen wie Alkali- oder Erdalkalimetallen und in Abwesenheit von Wasser bilden sich in einer Redoxreaktion Amide der allgemeinen Form MNH (M: einwertiges Metallatom), wie z. B. Natriumamid.

Verbindungen der Form MNH, bei denen zwei der drei Wasserstoffatome ersetzt sind, heißen Imide und sind keine Wasserstoffatome vorhanden, spricht man von Nitriden. Sie lassen sich durch Erhitzen von Amiden gewinnen.

Die Alkali- und Erdalkalisalze setzen sich mit Wasser zu Metallhydroxiden und Ammoniak um.

Ammoniak neigt zur Komplexbildung mit vielen Übergangsmetallen. Beständige Komplexe sind besonders von Cr, Co, Pd, Pt, Ni, Cu bekannt. Bei einem reinen Amminkomplex liegt ein Kation vor, das die Ladung des Metalls trägt und die Ammoniakmoleküle als einzähnige Liganden um ein zentrales Metallatom herum gruppiert sind. Der Ligand bindet sich über sein freies Elektronenpaar an das Zentralatom. Die Bildungsreaktionen der Komplexe lassen sich mit dem Lewis-Säure-Base-Konzept beschreiben. Die Amminkomplexe haben die allgemeine Struktur
Ein bekannter Amminkomplex ist der Kupfertetramminkomplex [Cu(NH)], der eine typische blaue Farbe besitzt und als Nachweis für Kupfer genutzt werden kann. Stabile Komplexe lassen sich in Form von Salzen, z. B. als Sulfate gewinnen und werden Ammin-Salze oder Ammoniakate genannt.

Amminkomplexe können neben Ammoniak auch andere Liganden tragen. Neben dem reinen Chromhexamminkomplex formula_13 sind auch Komplexe mit der allgemeinen Struktur

bekannt. Die Komplexe können durch die Ladungskompensation durch die ionischen Liganden daher auch Anionen oder eine molekulare (ungeladene) Struktur aufweisen. Ein Beispiel dafür ist Cisplatin, [Pt(Cl)(NH)], ein quadratisch-planarer Platin(II)-Komplex mit zwei Amminliganden und zwei Chlorid-Ionen, der ein wichtiges Zytostatikum darstellt.

An Ammoniak-Chlor-Komplexen des Cobalts wurde 1893 von Alfred Werner erstmals eine Theorie zur Beschreibung von Komplexen aufgestellt.

Ammoniak ist der Grundstoff für die Herstellung aller anderen industriell hergestellten stickstoffhaltigen Verbindungen. Mit einem Anteil von 40 % im Jahr 1995 ist dabei Harnstoff die wichtigste aus Ammoniak hergestellte Verbindung, die vorwiegend als Düngemittel und für die Produktion von Harnstoffharzen eingesetzt wird; Harnstoff wird durch Reaktion von Ammoniak mit Kohlenstoffdioxid gewonnen.

Neben Harnstoff werden auch weitere Stickstoffdünger aus Ammoniak hergestellt. Zu den wichtigsten zählen die Ammoniumsalze Ammoniumnitrat, -phosphat und -sulfat. Insgesamt lag der Anteil von Düngemitteln am Gesamtammoniakverbrauch im Jahr 2003 bei 83 %.

Ein weiterer wichtiger aus Ammoniak hergestellter Stoff ist die Salpetersäure, die wiederum Ausgangsmaterial für eine Vielzahl weiterer Verbindungen ist. Im Ostwald-Verfahren reagiert Ammoniak an Platinnetzen mit Sauerstoff und bildet so Stickoxide, die mit Wasser weiter zu Salpetersäure reagieren. Zu den aus Salpetersäure hergestellten Verbindungen zählen unter anderem Sprengstoffe wie Nitroglycerin oder TNT. Weitere aus Ammoniak synthetisierte Stoffe sind Amine, Amide, Cyanide, Nitrate und Hydrazin.

Ammoniak spielt eine Rolle in verschiedenen organischen Synthesen. Primäre Carbonsäureamide können aus Ammoniak und geeigneten Carbonsäurederivaten wie Carbonsäurechloriden oder -estern gewonnen werden. Die direkte Reaktion von Carbonsäure und Ammoniak zum entsprechenden Amid erfolgt dagegen nur bei erhöhten Temperaturen, wenn sich das zuvor gebildete Ammoniumsalz zersetzt. Eine technisch wichtige Reaktion ist die von Adipinsäure und Ammoniak zu Adipinsäuredinitril. Dieses wird weiter zu Hexamethylendiamin hydriert und ist damit ein Zwischenprodukt für die Herstellung von Nylon.

Es ist möglich, Anilin durch die Reaktion von Phenol und Ammoniak an einem Aluminium-Silikat-Katalysator herzustellen. Diese Syntheseroute erfordert jedoch mehr Energie und ergibt eine geringere Ausbeute als die Synthese und Reduktion von Nitrobenzol und wird daher nur in geringem Maß angewendet, wenn Phenol preiswert zur Verfügung steht.

Die Reaktion von Ammoniak mit Säuren wird in der Rauchgasreinigung ausgenutzt. Es ist in der Lage, mit Schwefel- und Salpetersäure zu reagieren und entzieht so dem Rauchgas unerwünschte, umweltschädliche Schwefel- und Stickoxide.

Aufgrund seiner großen spezifischen Verdampfungsenthalpie von 1368 kJ/kg wird Ammoniak auch als Kältemittel eingesetzt. Vorteile sind eine geringe Entflammbarkeit, der nicht vorhandene Beitrag zum Treibhauseffekt oder zur Zerstörung der Ozonschicht sowie der Verwendungsbereich von −60 bis +100 °C. Nachteilig ist die Toxizität der Verbindung.

Ammoniaklösung wird auch in Riechampullen verwendet. Der extreme Geruchsreiz dient als antidissoziative Strategie, etwa im Rahmen einer Dialektisch-Behavioralen Therapie.

Ammoniak reagiert mit der in Hölzern vorkommenden Gerbsäure und färbt das Holz je nach Konzentration der Gerbsäure dunkelbraun. So wird beispielsweise Eichenholz mit Ammoniak oder Salmiak zu der dunkelbraun erscheinenden Räuchereiche verwandelt.

Ammoniak wird des Weiteren experimentell dazu verwendet, Brennstoffzellen zu entwickeln, in denen der benötigte Wasserstoff zum Zeitpunkt des Bedarfs dadurch gewonnen wird, dass das Ammoniak durch chemische Verfahren gespalten wird.

Bei der Herstellung von Lichtpausen (Diazotypien) wird Ammoniak zur Färbung verwendet.

Nur wenige Mikroorganismen sind in der Lage, Ammoniak in der sogenannten Stickstofffixierung direkt aus dem Stickstoff der Luft zu gewinnen. Beispiele hierfür sind Cyanobakterien oder Proteobacterien wie Azotobacter. Aus diesem über das Enzym Nitrogenase gewonnenen Ammoniak werden von den Bakterien Aminosäuren synthetisiert, die von allen Lebewesen benötigt werden. Die meisten Hülsenfrüchtler, wie Bohnen, Klee und Lupinen sind für eine bessere Versorgung mit Aminosäuren auch Symbiosen mit bestimmten Bakterienarten eingegangen.

Im Stoffwechsel beim Auf- und Abbau von Aminosäuren spielt Ammoniak, das unter biochemischen Bedingungen als Ammonium vorliegt, eine wichtige Rolle. Aus Ammonium und α-Ketoglutarat entsteht durch reduktive Aminierung Glutamat, aus dem wiederum durch Transaminierung weitere Aminosäuren synthetisiert werden können. Während Mikroorganismen und Pflanzen auf diese Art alle Aminosäuren synthetisieren, beschränkt sich dies bei Mensch und Tieren auf die nicht-essentiellen Aminosäuren.

Ebenso erfolgt der Abbau von Aminosäuren zunächst über eine Transaminierung zu Glutamat, das durch das Enzym Glutamatdehydrogenase wieder in α-Ketoglutarat und Ammoniak gespalten wird. Da größere Mengen Ammoniak toxisch wirken und auch nicht vollständig für den Aufbau neuer Aminosäuren verwendet werden können, muss es eine Abbaumöglichkeit geben. Der Weg, das überschüssige Ammoniak aus dem Körper zu entfernen, entscheidet sich je nach Tierart und Lebensraum. Wasserbewohnende Lebewesen können Ammonium direkt an das umgebende Wasser abgeben und benötigen keinen ungiftigen Zwischenspeicher. Lebewesen, die auf dem Land leben, müssen das Ammoniak hingegen vor dem Ausscheiden in ungiftige Zwischenprodukte umwandeln. Dabei gibt es im Wesentlichen zwei Stoffe, die genutzt werden. Insekten, Reptilien und Vögel verwenden die wasserunlösliche Harnsäure, die als Feststoff ausgeschieden wird (Uricotelie). Dies ist in wasserarmen Gebieten und bei der Einsparung von Gewicht bei Vögeln vorteilhaft. Säugetiere sind dagegen in der Lage, Ammonium in der Leber über den Harnstoffzyklus in ungiftigen und wasserlöslichen Harnstoff umzuwandeln (Ureotelie). Dieser kann dann über den Urin ausgeschieden werden.

Harnstoff kann durch das Enzym Urease, das in manchen Pflanzen wie der Sojabohne oder der Schwertbohne, in bestimmten Bakterien und wirbellosen Tieren vorkommt, in Ammoniak und Kohlenstoffdioxid gespalten werden. Diese Bakterien finden sich unter anderem im Pansen von Wiederkäuern und bewirken, dass auch Jauche und Mist dieser Tiere ammoniakhaltig ist. Dies stellt auch die größte anthropogene Ammoniak-Quelle in der Umwelt dar.

Eine besondere Bedeutung hat Ammoniak in der Ökologie der Gewässer. Abhängig von pH-Wert verschiebt sich hier das Verhältnis der gelösten Ammonium-Ionen und dem ungelösten Ammoniak im Wasser, wobei die Konzentration des Ammoniaks bei zunehmenden pH-Werten zunimmt, während die der Ammonium-Ionen entsprechend abnimmt. Bei Werten bis etwa pH 8 liegen fast ausschließlich Ammonium-Ionen vor, bei einer Überschreitung eines pH-Wertes von 10,5 fast ausschließlich Ammoniak. Eine Steigerung des pH-Werts kann vor allem durch starke Steigerung der Photosyntheseaktivität, etwa bei Algenblüten, in schwach abgepufferten und abwasserbelasteten Gewässern auftreten. Da Ammoniak für die meisten Organismen der Gewässer toxisch ist, kann bei einer Überschreitung des kritischen pH-Wertes plötzliches Fischsterben auftreten.

Durch den unangenehmen Geruch, der schon bei niedrigen Konzentrationen wahrnehmbar ist, existiert eine Warnung, so dass Vergiftungsfälle mit Ammoniak selten sind. Gasförmiges Ammoniak kann vor allem über die Lungen aufgenommen werden. Dabei wirkt es durch Reaktion mit Feuchtigkeit stark ätzend auf die Schleimhäute. Auch die Augen werden durch die Einwirkung von Ammoniak stark geschädigt. Beim Einatmen hoher Konzentrationen ab etwa 1700 ppm besteht Lebensgefahr durch Schäden in den Atemwegen (Kehlkopfödem, Stimmritzenkrampf, Lungenödeme, Pneumonitis) und Atemstillstand. Beim Übergang substantieller Ammoniakmengen ins Blut steigt der Blutspiegel von NH über 35 µmol/l, was zentralnervöse Erscheinungen wie Tremor der Hände, Sprach- und Sehstörungen und Verwirrung bis hin zum Koma und Tod verursachen kann. Die pathophysiologischen Mechanismen sind noch nicht eindeutig geklärt, Ammoniak scheint vor allem die Astrozyten im Gehirn zu schädigen. Akute Ammoniakvergiftungen können außer durch Einatmung auch infolge von Leberversagen (→ Hepatische Enzephalopathie) oder bei Enzymdefekten auftreten, da dann im Stoffwechsel anfallende N-Verbindungen nicht zu Harnstoff umgebaut und ausgeschieden werden können („endogene Ammoniakvergiftung“). Eine mögliche Erklärung für die nerventoxische Wirkung von Ammoniak ist die Ähnlichkeit von Ammonium mit Kalium. Durch den Austausch von Kalium durch Ammonium kommt es zu Störungen der Aktivität des NMDA-Rezeptors und in Folge davon zu einem erhöhten Calcium-Zufluss in die Nervenzellen, was deren Zelltod bewirkt. Das Zellgift Ammoniak wirkt vorwiegend auf Nerven- und Muskelzellen. Nahezu alle biologischen Membranen sind aufgrund der geringen Größe des Moleküls sowie seiner Lipidlöslichkeit für Ammoniak durchlässig. Die Cytotoxizität beruht dabei auch auf der Störung des Citratzyklus, indem der wichtige Metabolit α-Ketoglutarsäure zu Glutaminsäure aminiert wird, sowie auf der Störung des pH-Werts der Zellen. Die encephalotoxische Wirkung wird auch mit einem erhöhten Glutaminspiegel im Gehirn sowie der Bildung von reaktiven Sauerstoffspezies in Verbindung gebracht.
Auch chronische Auswirkungen bei längerer Einwirkung von Ammoniak sind vorhanden. Durch Schädigung der Atemwege kann es zu Bronchialasthma, Husten oder Atemnot kommen. Wässrige Ammoniaklösungen können auch über Haut und Magen aufgenommen werden und diese verätzen. Ammoniak kommt durch Düngung und Massentierhaltung in die Atemluft. Dort wandelt er sich in Ammoniumsulfat und -nitrat um, was maßgeblich dazu beiträgt, dass Feinstaubpartikeln entstehen. Zudem fördert Ammoniak zusammen mit Stickstoffoxiden die Bildung von gesundheitsschädlichem, bodennahem Ozon. Es wird geschätzt, dass die Landwirtschaft dadurch im Jahr 2010 Ursache für etwa 45 % aller Todesfälle durch Luftverschmutzung in Deutschland war. Ammoniak ist dabei der einzige Luftschadstoff bei dem es seit 1990 zu keiner nennenswerten Reduktion gekommen ist. Mit einer Reduzierung der Ammoniakemissionen um 50 % könnten daher weltweit etwa 250.000 Todesfälle durch Luftverschmutzung vermieden werden, bei einer kompletten Abschaffung dieser Emissionen sogar 800.000 Todesfälle.

Bei Hausrindern kommen akute Ammoniakvergiftungen vor allem bei Fütterung von Nicht-Protein-Stickstoffverbindungen (NPN) vor. Bei einem Harnstoffanteil von über 1,5 % im Futter treten zentralnervöse Vergiftungserscheinungen auf, da der Harnstoff nicht mehr vollständig von der Pansenflora zur Proteinsynthese verarbeitet werden kann. Die chronische Exposition mit Ammoniak in der Stallhaltung bei Nutz- und Labortieren, vor allem bei strohlosen Haltungsformen und höheren Temperaturen bei unzureichender Belüftung, führt zu Schädigungen der Atemwege und damit zu vermehrtem Auftreten von Atemwegsinfektionen, zu verminderter Futteraufnahme und Leistungseinbußen.

Von der Gefahr einer Vergiftung durch Ammoniak sind wegen der guten Wasserlöslichkeit des Ammoniaks insbesondere Fische und andere Wasserlebewesen betroffen. Während viele Fischarten nur geringe Ammoniakkonzentrationen vertragen, haben einige Arten spezielle Strategien entwickelt, auch höhere Konzentrationen zu tolerieren. Dazu zählt die Umwandlung des Ammoniak in ungiftigere Verbindungen wie Harnstoff, oder sogar Pumpen, um Ammoniak aus dem Körper aktiv zu entfernen, die bei Schlammspringern beobachtet wurden. Ammoniakvergiftungen kommen in der Teichwirtschaft und Aquaristik vor. Ursachen können die Verunreinigung des Wassers mit Gülle oder Düngemitteln sowie der Anstieg des pH-Wertes mit Verschiebung des Dissoziationsgleichgewichts in Richtung Ammoniak sein. Betroffene Fische zeigen eine vermehrte Blutfülle (Hyperämie) und Blutungen in den Kiemen und inneren Organen sowie eine vermehrte Schleimproduktion der Haut. Bei höheren Konzentrationen kann es zum Absterben von Flossenteilen, Hautarealen oder Kiemen, zu zentralnervösen Erscheinungen oder zum Tod kommen.

Es gibt mehrere Möglichkeiten, Ammoniak nachzuweisen. Einfache Nachweise, die aber häufig nicht eindeutig sind, sind der typische Geruch, die Verfärbung von Indikatoren durch das basische Ammoniak oder der typische weiße Rauch von Ammoniumchlorid, der bei der Reaktion mit Salzsäure entsteht. Charakteristisch ist auch die Reaktion von Ammoniaklösungen mit Kupfersalzlösungen, bei denen der dunkelblaue Kupfertetramminkomplex [Cu(NH)] entsteht.

Eine genaue – in der Spurenanalytik durch die Störung mit Schwefelwasserstoff jedoch häufig nicht einsetzbare – Reaktion zur Ammoniak-Bestimmung ist die Neßler-Reaktion, bei der Kaliumtetraiodomercurat(II) mit Ammoniak zu einem typischen braunen Niederschlag von (HgN)I reagiert. Ein weiterer Nachteil ist auch die Verwendung des giftigen Quecksilbers.
Stattdessen wird die Berthelot-Reaktion genutzt, bei der Ammoniak mit Hypochlorit Chloramine bildet. Diese sind in der Lage mit Phenolen zu Indophenolen zu reagieren, die an ihrer tiefblauen Farbe erkannt werden können. Für geringe Mengen kann auch die Kjeldahlsche Stickstoffbestimmung genutzt werden. Mit dieser Methode sind auch quantitative Bestimmungen möglich.

Aufgrund seiner physikalischen Eigenschaften ist eine optische Erfassung von Ammoniak in der Luft problematisch. Es werden fast ausschließlich nasschemische Verfahren eingesetzt, um eine gleichzeitige Erfassung von Ammonium in Feinstäuben zu verhindern.

Belastungen der Außenluft mit Ammoniak können mit beschichteten Diffusionsabscheidern, sogenannten Denudern, quantitativ erfasst werden. Als sorbierende Beschichtung dient eine Säure (z. B. Oxalsäure), die nach Abschluss der Probenahme analysiert wird.

Alternativ können Passivsammler eingesetzt werden. Im Gegensatz zu den aktiv sammelnden Denudern wird bei diesen Geräten auf eine gezielte Strömungsführung verzichtet. Das zu detektierende Ammoniak gelangt ausschließlich durch Diffusion zum Sorbens.

Weitere Verfahren sind das Indophenol-Verfahren und das Neßler-Verfahren. Beim Indophenol-Verfahren wird die Luft durch eine mit verdünnter Schwefelsäure befüllten Waschflasche geleitet und als Ammoniumsulfat gebunden. Nach Umsetzung zu Indophenol wird dessen Konzentration photometrisch bestimmt. Beim Nessler-Verfahren wird das gewonnene Ammoniumsulfat mit Neßlers Reagenz umgesetzt und die Färbungsintensität des gewonnenen Kolloids photometrisch bestimmt. Beiden Verfahren ist gemein, dass sie nicht selektiv gegenüber Ammoniak sind.



</doc>
<doc id="246" url="https://de.wikipedia.org/wiki?curid=246" title="Liste der Minerale">
Liste der Minerale

Die Liste der Minerale ist eine alphabetisch geordnete Übersicht von Mineralen, Synonymen und bergmännischen Bezeichnungen. Ebenfalls aufgeführt werden hier Mineral-Varietäten, Mineralgruppen und Mischkristallreihen, zu denen teilweise bereits eigene Artikel bestehen.

Eine systematische Liste nur der Minerale findet sich dagegen unter Systematik der Minerale nach Strunz (8. Auflage), Systematik der Minerale nach Strunz (9. Auflage) und Systematik der Minerale nach Dana.

Davon unabhängig existiert eine ebenfalls alphabetisch geordnete Liste der Gesteine. Für fiktive Minerale siehe Liste erfundener Elemente, Materialien, Isotope und Elementarteilchen.




</doc>
<doc id="247" url="https://de.wikipedia.org/wiki?curid=247" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming (* 6. August 1881 in Darvel, East Ayrshire; † 11. März 1955 in London) war ein schottischer Mediziner und Bakteriologe. Er erhielt 1945 als einer der Entdecker des Antibiotikums "Penicillin" den Nobelpreis. Außerdem entdeckte er das Lysozym, ein Enzym, das starke antibakterielle Eigenschaften aufweist und in verschiedenen Körpersekreten wie Tränen und Speichel vorkommt. 

Alexander Fleming wurde 1881 auf dem Bauernhof "Lochfield" (Gemeinde Darvel) geboren. Er studierte ab 1902 Medizin an der St Mary’s Hospital Medical School in Paddington. 1906 schloss er sein Studium ab, blieb aber weiterhin am Institut. Ab 1921 war er stellvertretender Leiter und ab 1946 Direktor des Instituts, das 1948 in Wright-Fleming-Institut umbenannt wurde. Von 1928 bis 1948 hatte er an der Londoner Universität den Lehrstuhl für Bakteriologie inne.

In seinen jungen Jahren beschäftigte sich Fleming mit Autovaccinen. 1921 isolierte er das Enzym Lysozym, das im Eiweiß des Hühnereis sowie in zahlreichen menschlichen Körpersekreten vorkommt und in der Lage ist, Bakterien zu zerstören.

Er bemerkte zufällig am 28. September 1928 im Labor, wie Schimmelpilze der Gattung "Penicillium", die in eine seiner Staphylokokken-Kulturen hineingeraten waren, eine wachstumshemmende Wirkung auf Bakterien hatten. Weitere Untersuchungen führten später zum Antibiotikum Penicillin. 

Für seine Entdeckung wurde Fleming vielfach geehrt. 1944 wurde er geadelt, und 1945 bekam er zusammen mit Howard Walter Florey und Ernst Boris Chain, die seine Untersuchungen weitergeführt hatten, „für die Entdeckung des Penicillins und seiner heilenden Wirkung bei verschiedenen Infektionskrankheiten“ den Nobelpreis für Physiologie oder Medizin. Weiterhin war er Ehrendoktor von zwölf amerikanischen und europäischen Universitäten, Kommandeur der französischen Ehrenlegion und Ehrendirektor der Universität Edinburgh.

Fleming war in erster Ehe von 1915 bis zu ihrem Tod mit Sarah Marion McElroy (1881–1949) verheiratet. Ihr einziges Kind war Robert Fleming (1924–2015), der Arzt wurde. Nach dem Tod seiner ersten Frau heiratete Fleming 1953 in zweiter Ehe die griechische Ärztin Amalia Koutsouri-Vourekas (1912–1986). Alexander Fleming starb am 11. März 1955 in London an einem Herzinfarkt und wurde in der Londoner St Paul’s Cathedral begraben.

Der Fleming Point, ein Kap auf der antarktischen Brabant-Insel, trägt seinen Namen. Nach ihm wurde 2007 auch der Asteroid (91006) Fleming benannt. Im Jahr 2013 wurde in Stuttgart die neu gegründete „Berufliche Schule für Gesundheit und Pflege“ nach ihm benannt. Der Alexander Fleming Award der Infectious Diseases Society of America ist nach ihm benannt.

Fleming war Freimaurer, ab 1925 mehrfach Meister vom Stuhl der "Santa Maria Freimaurer Nummer 2692" und ab 1936 der "Misericordia Lodge No. 3286". 1942 wurde er "Erster Großschaffner" der Vereinigten Großloge von England und ab 1948 deren Großaufseher. Ebenso war er Mitglied der "London Scottish Rifles Lodge No. 2319" und erreichte den 30. Grad des "Alten und Angenommenen Schottischen Ritus".




</doc>
<doc id="249" url="https://de.wikipedia.org/wiki?curid=249" title="Adzukibohne">
Adzukibohne

Die Adzukibohne ("Vigna angularis") ist eine Pflanzenart aus der Gattung "Vigna" in die Unterfamilie der Schmetterlingsblütler (Faboideae) innerhalb der Familie der Hülsenfrüchtler (Fabaceae, Leguminosae). Diese Nutzpflanze ist nahe verwandt mit einer Reihe anderer „Bohnen“ genannter Feldfrüchte wie der Mungbohne, aus der die sogenannten Sojasprossen hergestellt werden.

Wildformen der Adzukibohne stammen aus Mittelchina, Taiwan, Korea und Japan. Die Adzukibohne wird seit 2000 Jahren in China, Korea und Japan angebaut. Sie wächst am besten in den Subtropen. In China wird sie 赤豆 chìdòu genannt, dies bedeutet übersetzt Rote Bohne.

Die Beschreibungen in den Quellen unterscheiden sich teilweise etwas, das liegt auch daran, dass Autoren die Wildformen oder die Kulturformen beschreiben.

Die Adzukibohne ist eine einjährige, selten zweijährige krautige Pflanze und erreicht Wuchshöhen von selten 20 bis, meist 30 bis 90 Zentimetern; sie können auch bis zu 3 Meter hochklettern. Sie wächst selbstständig aufrecht oder sich gegen den Uhrzeigersinn empor windend; es gibt auch kriechende Formen. Die meist grünen, bei manchen Sorten auf purpurfarbigen, kantigen Stängel sind fein behaart.

An den Knoten (Nodi) werden Wurzeln gebildet. Die Pfahlwurzeln sind 40 bis 50 Zentimeter lang.

Die ersten Blätter des Sämlings sind gegenständig, lang gestielt, einfach und herzförmig. Die wechselständig am Stängel verteilt angeordneten Laubblätter sind in Blattstiel und Blattspreite gegliedert. Der Blattstiel ist relativ lang. Die unpaarig gefiederte Blattspreite enthält drei Fiederblätter. Die Fiederblätter sind bei einer Länge von 5 bis 10 Zentimetern sowie einer Breite von 5 bis 8 Zentimeter eiförmig oder rhomboid-eiförmig mit breit-dreieckigem oder fast gerundtem oberen Ende. Die seitlichen Fiederblätter sind ungleichseitig, einfach bis leicht dreilappig mit zugespitztem oberen Ende. Beide Seiten der Fiederblätter sind spärlich fein behaart. Die zwei mit etwa 8 Millimetern relativ kleinen Nebenblätter sind schildförmig, oft zweispaltig mit Anghängseln an ihrer Basis, untereinander frei und nicht mit dem Blattstiel verwachsen. Die Nebenblättchen der Fiederblätter sind lanzettlich. Die Laubblätter sind meist bis zur Fruchtreife haltbar.

Die Blütezeit reicht in China von Juni bis Juli. In den Blattachseln stehen auf im unteren Stängelbereich langen und im oberen relativ kurzen Blütenstandsschäften die traubigen Blütenstände mit je fünf bis sechs bis zehn (zwei bis zwanzig) Blüten. Es sind Tragblätter vorhanden. Die Deckblätter sind länger als der Blütenkelch. Der relativ kurze Blütenstiel besitzt an seiner Basis ein extraflorales Nektarium.

Die zwittrigen Blüten sind zygomorph mit doppelter Blütenhülle. Die ungleichen, 3 bis 4 Millimeter Kelchblätter sind glockenförmig verwachsen mit zwei Kelchlippen und fünf relativ kurzen Kelchzähnen. Die obere Kelchlippe besteht aus zwei vollkommen oder teilweise verwachsen Kelchzähnen; die untere Kelchlippe besteht aus drei Kelchlappen. Die Blütenkronblätter sind meist leuchtend gelb oder seltener lilafarben. Die 15 bis 18 Millimeter langen Blütenkronen besitzen den typischen Aufbau der Schmetterlingsblüten. Die fünf Kronblätter sind genagelt. Die normal entwickelte Fahne ist kreisförmig mit ausgerandetem oberen Ende. Die länglichen Flügel sind nicht gespornt aber geöhrt und kurz genagelt. Die Flügel sind länger als das Schiffchen. Das Schiffchen ist nach rechts fast halbkreisförmig gebogen und besitzt einen hornförmigen Sporn an seiner linken Seite und ist an der Basis genagelt. Die zehn fertilen Staubblätter sind nicht mit den Kronblättern verwachsen. Neun Staubfäden sind zu einer Röhre verwachsen. Es sind Nektardrüsen auf dem Diskus vorhanden. Das einzige linealische, oberständige Fruchtblatt ist kurz behaart und krümmt abrupt in den oberen Abschnitt, der an einer Seite nahe dem oberen Ende behaart ist. Der gekrümmte, von der Seite betrachtet scheibenförmige Griffel ist behaart und an einer Seite bärtig.

An jedem Fruchtstand hängen zwei bis sechs Hülsenfrüchte und je Pflanzenexemplar sind es 5 bis 40. Die hängende, fast kahle, relativ dünnwandige Hülsenfrucht ist bei einer Länge von 5 bis 8, seltener bis zu 13 Zentimetern sowie einem Durchmesser von 0,5 bis 0,6 Zentimetern schlank, zylindrisch und zwischen den 2 bis 14 (5 bis 12) Samen etwas eingeschnürt, also sind die Samen als deutliche Wölbungen sichtbar. Die Hülsenfrucht ist bei Reife strohfarben gelb, schwärzlich oder braun. Die Früchte reifen in China von September bis Oktober. Bei Reife zerbrechen die Hülsenfrüchte.

Die mit einer Länge von 5 bis 7,5, selten bis zu 9,1 Millimetern sowie einem Durchmesser von 4 bis 5,5, seltener 6,3 Millimetern relativ kleinen Samen sind länglich und rund, selten abgeflacht, mehr oder weniger zylindrisch mit gerundeten oberen Enden. Die glatten Samenschalen sind meist weinrot oder kastanienrot, manchmal strohgelb, lederfarben, cremefarben, schwarz oder gefleckt. Das weiße Hilum ist bei einer Länge von 2,4 bis 3,3 Millimetern sowie einer Breite von 0,6 bis 0,8 Millimeter lang und schmal. Die Samen wiegen jeweils 50 bis 250 mg. Der Embryo ist fast weiß. Das Tausendkorngewicht beträgt 50 bis 200 g, bei den meisten japanischen sorten liegt es bei 130 bis 150 g und bei den Dainagon-Adzukibohnen bei 180 bis 200 g.

Die Chromosomengrundzahl beträgt x = 11; es liegt Diploidie vor mit einer Chromosomenzahl von 2n = 22.

Bei der Adzukibohne handelt es sich um einen Therophyten.

Das Wurzelsystem, das von der Pfahlwurzel ausgeht, breitet sich auf einer Fläche mit einem Durchmesser von 40 bis 50 Zentimetern aus. Der Luftstickstoff wird durch die Symbiose der Wurzelknöllchen der Adzukibohne mit den Knöllchenbakterien "Bradyrhizobium bacteria" genutzt (Stickstofffixierung). Die Wurzelknöllchen weisen einen Durchmesser von 4 bis 10 Millimetern auf und beginnen ihre Entwicklung wenn die ersten Laubblätter sich entfalten.

Die Anthese beginnt meist morgens und kann bis zu 40 Tage dauern. Die Blüten beginnen im unteren Bereich des Stängels zu blühen und das Aufblühen setzt sich dann nach oben hin fort. Es erfolgt meist Selbstbefruchtung oder manchmal Fremdbefruchtung. Die Ausbreitungseinheit (Diaspore) ist der Same.

Die Keimung erfolgt hypogäisch.

Über die ursprüngliche Verbreitung der Wildform liegen sehr unterschiedliche Einschätzungen vor. Einig ist man sich nur bei Japan, Korea und der Mandschurei.

Die Erstveröffentlichung erfolgte 1802 unter dem Namen (Basionym) "Dolichos angularis" durch Carl Ludwig Willdenow in "Species Plantarum" 4. Auflage, Band 3, 2, S. 1051. Die Neukombination zu "Vigna angularis" wurde 1969 durch Jisaburō Ōi und Hiroyoshi Ōhashi in "Azuki beans of Asia" in "Journal of Japanese Botany", Band 44, Nr. 1, S. 29 veröffentlicht. Weitere Synonyme für "Vigna angularis" sind: "Azukia angularis" , "Phaseolus angularis" , "Phaseolus nipponensis" und je nach Autor auch "Vigna angularis" var. "nipponensis" .

"Vigna angularis" gehört zur Untergattung "Ceratotropis" aus der Gattung "Vigna".

Manche Autoren unterschieden zwei Varietäten. In der "Flora of China" 2010 gilt die Varietät "Vigna angularis" var. "nipponensis" als Synonym. Kang et al. 2015 gehen davon aus, dass "Vigna angularis" var. "nipponensis" die Wildform darstellt und unter "Vigna angularis" var. "angularis" die Kulturformen zusammengefasst werden. Archäologische Befunden lassen vermuten, dass die Wildformen im nordöstlichen Asien an verschieden Orten in Kultur genommen wurden, Kulturformen als mehrmals unabhängig voneinander entstanden sind. Früher vermutete man, dass das Gebiet der Entstehung der Kulturformen auf Teile Chinas (Mandschurei), Japan und Korea beschränkt ist, die archäologischen Befunde zeigen, dass auch im Himalaja (Tibet, Bhutan, Nepal) Kulturformen entstanden sind.

Die Bohnen (= Samen) und das daraus gewonnene Mehl sind bedeutende Handelsgüter in ostasiatischen Märkten. Die meisten Sorten dienen der Kornnutzung.

Die ältesten schriftlichen Belege für die Kultur der Adzukibohne in Japan stammen aus dem 8. Jahrhundert. Auch im nördlichen Korea und China ist die Adzukibohne eine alte Kulturpflanze. Sie wird in Ostasien seit 2000 Jahren angebaut.

In Japan, Korea, Taiwan und China existieren Samenbanken. Alleine in Japan sind über 300 Sorten, Landrassen und Zuchtlinien registriert.

Hauptsächlich durch die Anbauintensität bedingt liegen Erträge der an Bohnen (= Samen) im weiten Bereich von 4 bis 8 dt/ha; für Japan und China gibt es sogar Ertragsangaben von 20 und 30 dt/ha. Das jährliche Anbaugebiet der Adzukibohne wurde 1997 in China, Japan, der Koreanischen Halbinsel sowie Taiwan auf 670000, 120000, 30000 sowie 20000  ha geschätzt. Die Adzukibohne wird in über 30 Ländern angebaut, außerhalb Asiens gibt es Anbaugebiete in Südamerika, in die Südstaaten der USA, Neuseeland sowie in Afrika beispielsweise Kongo sowie Angola. Die Adzukibohne kann bis zu 48°N angebaut werden, aber die Hauptanbaugebiete liegen zwischen 40 und 45°N.

Sie wird in tropischen, subtropischen bis gemäßigten Gebieten angebaut; die Temperaturansprüche liegen nicht sehr hoch. Die Jahresdurchschnittstemperaturen für optimales Wachstum liegen bei 15 bis 30 °C. Sie tolerieren hohe Temperaturen, sind aber frostempfindlich. In den Tropen sind größere Höhenlagen für den Anbau besser. Abhängig von der Temperatur und Bodenart sollten am besten Jahresniederschlagsmengen von 1000 bis 1500 mm zur Verfügung stehen; bei Wassermangel im Anbaugebiet erfolgt manchmal eine Bewässerung, die Anbaugrenzen liegen bei 500 bis 1750 mm. Für die Keimung im Frühjahr sowie die Jugendentwicklung sind mittlere und sich gut erwärmende Böden am besten geeignet. Schlechte Kulturbedingungen ergeben sich besonders durch hohen Grundwasserstand. Boden-pH-Werte von 5 bis 7,5 sind geeignet, das Optimum liegt bei 5,5 bis 6,5. Der Anbau erfolgt fast nur in Reinkultur; die Adzukibohne ist allerdings auch konkurrenzstark genug im Mischanbau.

Die Samen bleiben mindestens fünf Jahre keimfähig, wenn sie bei einer relativen Luftfeuchtigkeit von etwa 15 % gelagert werden. Zwischen 8 und 30 kg Saatgut pro Hektar sind erforderlich. Meist beträgt der Reihenabstand 60 bis 90 Zentimeter bei einem Abstand von 30 Zentimeter in der Reihe. Zur Keimung sind Bodentemperaturen oberhalb von 6 bis 10 °C erforderlich und die besten Keimtemperaturen liegen bei 30 bis 34 °C. Die Keimdauer beträgt 7 bis 20 Tage. Das Wachstum ist vergleichsweise langsam. Von der Aussaat bis zur Kornreife dauert es je nach Sorte und Klima meist 80 bis 120 (60 bis 190) Tage, nach Angaben aus dem Kongo kann je nach Anbauzeit es auch bis zu 9 Monate dauern. Die Adzukibohne ist eine quantitative Kurztagspflanze, aber es existieren tagneutrale Sorten. Eine Düngung erfolgt in Japan und Korea vergleichbar zu Sojabohnen. Die Blütezeit dauert 30 bis 40 Tage, kann allerdings bis zu dreimal so lang dauern, bei sehr frühen Aussaaten.

Man kann die frischen Hülsenfrüchte, die frischen Bohnen (= Samen) oder die getrockneten Bohnen verwenden. Reife Samen und grüne Hülsenfrüchte werden als Gemüse und in Suppen oder in Salaten gegessen. Getrocknete Bohnen können gemahlen werden und aus diesem Mehl können Suppen, Gebäck oder süße Getränke hergestellt werden. Der süße, nussige Geschmack der Adzukibohne führt dazu, dass sie in Asien zur Herstellung traditioneller Süßspeisen dient. Beispielsweise werden Süßigkeiten und Eiscreme hergestellt. Aus Adzukibohnen wird Rote Bohnenpaste hergestellt. Ursprünglich aus Japan stammt die Erzeugung von Adzukibohnen-Sprossen, also einem Sprossengemüse. Die Bohnen werden gepoppt, wie Popcorn. Die Bohnen werden kandiert. Die Adzukibohne ist ein Kaffeesurrogat.

In der japanischen Küche wird die Adzukibohne vielseitig verwendet. In Japan ist die Adzukibohne zusammen mit Reisbrei wichtiger Bestandteil von „azuki-gayu“, bei traditionalen Zeremonien und Feiern. Adzukibohne werden gekocht in Japan zu „An“, einer Adzukibohnen-Marmelade, oder „Shiruko“, einer süßen Suppe mit Klebreiskuchen. Aus Adzukibohnenmehl wird in Japan beispielsweise Yōkan hergestellt.

Die Adzukibohne soll leichter verdaulich als viele andere „Bohnen“-Arten sein.

Die Adzukibohne wird als Heilpflanze verwendet. Die medizinischen Wirkungen wurden untersucht. Eine große Bedeutung in bezüglich des yin und yang der bipolaren Nahrung hat China die Adzukibohne, da sie den yang Charakter stärken. So können sie bei Nierenproblemen, Abszessen, bestimmten Tumoren, in der Geburtshilfe und anderen Beschwerden sowie zur Erhöhung des Milchflusses helfen. Die yin der Laubblätter der Adzukibohne verringern Fieber. Bei der Behandlung nach Aborten werden Keimlinge angewendet.

Adzukibohnenmehl wird auch zur Kosmetikherstellung verwendet.

Die Samen werden auch als Viehfutter verwendet.

Besonders die spätreifenden Sorten dienen zur Erzeugung von Grünfutter und zur Gründüngung. Damit wird beispielsweise die Bodenerosion verringert. Es wurden Werte der Stickstofffixierung von bis zu 100 kg N/ha beobachtet, die Mengen sind von der Bodenfeuchtigkeit und dem Boden-pH-Wert abhängig.

Die Adzukibohne hat einen niedrigen Physiologischen Brennwert und einen geringen Gehalt an Fetten. Sie ist eine exzellente Quelle für Proteine, Faserstoff, Vitamin B, Folsäure, Eisen sowie Kalium. Neben den Kohlehydraten ist der Rohproteingehalt ziemlich hoch.

Bei den Adzukibohne werden die Inhaltsstoffe der Samen mit: 20 bis 21 % Rohprotein, 1,4 % Rohfett, 56 bis 64 % Kohlehydrate, 7 bis 8 % Rohfaser und 2 bis 4 % Asche angegeben. Die Aminosäurezusammensetzung des Proteins der Adzukibohne beträgt je 16 g N: Alanin 4,0 g, Histidin 3,3 g, Prolin 4,7 g, Arginin 6,3 g, Isoleucin 3,9 g, Serin 4,2 g, Asparaginsäure 9,8 g, Leucin 7,2 g, Threonin 3,4 g, Cystin 0,9 g, Lysin 7,3 g, Tryptophan 1,7 g, Glutaminsäure 17,2 g, Methionin 1,3 g, Tyrosin 3,4 g, Glycin 3,4 g, Phenylalanin 5,4 g, Valin 4,4 g.

In deutscher Sprache gibt es unterschiedliche Schreibweise: Adzukibohne, Adsukibohne (von , andere Transkription: Aduki)

Trivialnamen in anderen Sprachen sind beispielsweise:



</doc>
<doc id="250" url="https://de.wikipedia.org/wiki?curid=250" title="Augenbohne">
Augenbohne

Die Augenbohne ("Vigna unguiculata"), auch Kuhbohne, Schwarzaugenbohne oder Schlangenbohne genannt, ist eine Nutzpflanze aus der Familie der Hülsenfrüchtler (Fabaceae). Vier Unterarten sind anerkannt:


Die Augenbohne ist eine einjährige Pflanze. Ihr Habitus ist recht variabel und reicht von aufrecht über halb-aufrecht über niederliegend bis kletternd. Der Wuchstyp reicht von undeterminiert bis determiniert, wobei vor allem die nicht-rankenden Formen determiniertes Wachstum zeigen. Sie haben eine ausgeprägte Pfahlwurzel, die 8 Wochen nach der Aussaat 2,4 m tief reichen kann.

Die Blätter sind dreiteilig. Sie sind glatt, stumpf bis glänzend und selten behaart. Das endständige Blättchen ist häufig länger und größer als die beiden seitlichen. Blattgröße und -form sind sehr variabel. Ab Mittag richten sich die Blätter parallel zur einfallenden Sonnenstrahlung aus und entgehen so dem größten Teil der einfallenden Strahlungsintensität.

Die Blüten stehen in mehrfachen Rispen an 20 bis 50 cm langen Blütenstandsstielen, die in den Blattachseln entspringen. Pro Blütenstand werden zwei bis drei Hülsen gebildet, auch vier oder mehr kommen vor. Die Blüten stehen deutlich über den Blättern und enthalten auch Nektarien, um Insekten anzulocken. Dennoch herrscht Selbstbestäubung vor.

Die Hülsen sind glatt und 15 bis 25 cm lang. Sie sind zylindrisch und etwas gekrümmt. Zur Nutzungsreife als Gemüse sind die Hülsen grün, gelb oder purpurn, zur Trockenreife werden die zunächst grünen und gelben Formen braun. Die Samen sind nierenförmig. Die Samenoberfläche ist glatt oder runzelig, die Farbe kann weiß, cremefarben, grün, rot, braun oder schwarz sein. Häufig haben sie ein „Auge“, das heißt, der weiße Nabel (Hilum) ist von einer anderen Farbe umrandet.

Die Chromosomenzahl beträgt 2n = 22 und zwar für die Unterarten "Vigna unguiculata" subsp. "unguiculata", "Vigna unguiculata" subsp. "cylindrica" und "Vigna unguiculata" subsp. "sesquipedalis".

100 g reife Samen enthalten im Durchschnitt 24,8 g Protein, 1,9 g Fett, 6,3 g Fasern und 63,6 g Kohlenhydrate. An Vitaminen sind 0,74 mg Thiamin, 0,42 mg Riboflavin und 2,8 mg Niacin enthalten. Das Protein ist relativ reich an den Aminosäuren Lysin und Tryptophan, aber relativ arm an Methionin und Cystin.

Die Augenbohne ist in Afrika beheimatet. Sie wird heute in Afrika, Lateinamerika, Südostasien sowie im Süden der USA angebaut. Der Anbau in Westafrika reicht mindestens 4000 Jahre zurück.

Sie wird vor allem in den feuchten Tropen, aber auch in temperierten Gebieten angebaut. Hitze und Trockenheit werden gut vertragen, Frost wird nicht toleriert, kühle Temperaturen verlangsamen das Wachstum deutlich. Der Ertrag steigt deutlich mit der Wasserversorgung, etwa Bewässerung. Auch unter trockenen Bedingungen liefert die Augenbohne noch gute Erträge, worauf ihre große Bedeutung in vielen Gebieten zurückgeht.

An den Boden stellt die Augenbohne keine besonderen Anforderungen, am besten gedeiht sie auf gut drainierten sandigen bis sandig-lehmigen Böden bei pH-Werten zwischen 5,5 und 6,5.

Die Augenbohne wird in allen Wachstumsstadien als Gemüsepflanze genutzt. Junge grüne Blätter werden in Afrika wie Spinat als Blattgemüse zubereitet. Unreife Hülsen werden ebenfalls als Gemüse zubereitet. Grüne Samen werden gekocht als Frischgemüse genutzt oder in Konserven verpackt oder tiefgefroren. Reife, trockene Samen werden gekocht oder konserviert. Die Samen lassen sich keimen und können ähnlich wie Mungbohnensprossen roh verzehrt werden.

Das Laub wird an Nutztiere verfüttert und ist vielfach das einzige verfügbare hochwertige Tierfutter. In der Qualität kommt es der Luzerne gleich. Es wird frisch oder trocken verfüttert.

In der bahianischen Küche werden die schwarzen Augen der Bohne entfernt, die Haut der Bohne abgelöst, gemahlen und so zu Acarajé verarbeitet.

Innerhalb der Art "Vigna unguiculata" werden drei Taxa unterschieden, die entweder als Unterarten oder als Sortengruppen eingestuft werden:

Die Augenbohne wird auf Hebräisch "Rubiya" (arabisch: "lubiya") genannt und ist, wie Honig und Apfel, ein Bestandteil des Seders zu Rosch ha-Schana, dem jüdischen spirituellen Neujahr.



</doc>
<doc id="254" url="https://de.wikipedia.org/wiki?curid=254" title="Admiral">
Admiral

Der Admiral ist ein Dienstgrad der Marinestreitkräfte in den meisten Staaten.

Die Bezeichnung leitet sich ab vom arabischen .

Im deutschsprachigen Raum ist der Begriff Admiral erstmals anlässlich der Thronbesteigung Kaiser Ottos III. im Jahr 983 überliefert, wo auf der Liste hoher Würdenträger auch ein „Oberst Admiral“ genannt wird. Im 10. und 11. Jahrhundert führten Flottenführer in Griechenland (Byzantinisches Reich) die Bezeichnung "Amiralios" (entsprach etwa dem Grad des Admirals), während die Heeresführer "Amiras" (etwa General) hießen; beide Begriffe sind von derselben arabischen Wortwurzel "amir" abgeleitet. Im 12. Jahrhundert erhielten zunächst die Befehlshaber der Flotten von Genua und Sizilien die Bezeichnung, im 13. Jahrhundert dann auch die von England und Frankreich, denen die anderen europäischen Staaten später folgten.

Dabei ist die Bezeichnung „Admiral“, als letzte Instanz in einer Flotte, nicht nur auf militärische Verbände beschränkt. Bis ins 17. Jahrhundert konnten in Konvoi aus Handelsschiffen in einer Kapitäns- oder Schifferversammlung einer der Mitglieder als Admiral gekürt werden. Dieser Verband segelte dann in einer Admiralschaft. Ebenso konnte ein Admiral auch der Befehlshaber des Konvoischiffes werden. Dieser Kapitän konnte der Befehlshaber eines städtischen, eines landesherrlichen oder auch privaten Kriegsschiffes sein. Wurde er von den Handelskapitänen zu ihrem Schutz angestellt oder angenommen, zahlten sie also entsprechende Abgaben – das so genannte „Convoigeld“ –, wurde er Admiral dieses geschützten Konvois. Auch hier segelte man dann in einer Admiralschaft.

In der Kaiserlichen Marine wurden von 1872 bis 1918 die Dienstgrade "Konteradmiral" (bis 1898 Contreadmiral), "Vizeadmiral" und "Admiral" sowie "Großadmiral" (Hans von Koester (1905), Heinrich von Preußen (1909), Alfred von Tirpitz (1911) und Henning von Holtzendorff (1918)) vergeben. Der "Großadmiral" entsprach dem "Generalfeldmarschall" im Heer.

In der Reichsmarine von 1922 bis 1935 wurden die Ränge Konteradmiral, Vizeadmiral und Admiral vergeben.

In der deutschen Kriegsmarine wurden von 1935 bis 1945 die Ränge Kommodore, Konteradmiral, Vizeadmiral, Admiral und Generaladmiral sowie Großadmiral Erich Raeder (1939), Karl Dönitz (1943) für die Oberbefehlshaber vergeben.

Zum Dienstgrad Admiral der Kriegsmarine war das Äquivalent der General der Waffengattung bei Heer und Luftwaffe.

In den Seestreitkräften bzw. der Volksmarine (ab 1960) der DDR gab es bis 1982 die drei Admiralsdienstgrade Konteradmiral, Vizeadmiral und Admiral. Mit Beschluss des Staatsrates der DDR vom 25. März 1982 wurde zudem der Dienstgrad des Flottenadmirals, äquivalent zum Armeegeneral, geschaffen, jedoch nie verliehen.

Der Admiral war in der Volksmarine der DDR der zweithöchste Dienstgrad im Admiralsrang und vergleichbar dem Generalleutnantsrang. Er entsprach dem Generaloberst der NVA. Das Dienstgradabzeichen bestand aus Schulterstücken mit marineblauem Untergrund und darauf einer geflochtenen gold-silbernen Schnur, auf der drei fünfeckige silberfarbene Generalssterne („Pickel“) angebracht waren. Schulterstücke wurden zu allen Uniformteilen getragen. Das Ärmelabzeichen bestand aus einem breiten gelbfarbigen Streifen und drei weiteren einfachen Streifen. Darüber war ein fünfzackiger Stern angebracht, in dessen Innerem sich das Wappen der DDR befand. Im Unterschied zu allen übrigen deutschen Marinestreitkräften bedeckten die Ärmelabzeichen nur zu ca. 40 % den Ärmelumfang. Die Admiralskragenspiegel zeigten eine goldfarbene Ranke, die am unteren Ende einen Winkel von 90° aufwies. Das Mützenabzeichen entsprach dem des Admirals der Deutschen Marine.

Waldemar Verner, Wilhelm Ehm und Theodor Hoffmann waren die einzigen Admirale in der Volksmarine. In diesen Dienstgrad wurde man nur in Verbindung der Dienststellung des Ministers für Nationale Verteidigung oder seines Stellvertreters befördert. Bis 1989 war der Chef der Volksmarine gleichzeitig stellvertretender Minister. Theodor Hoffmann wurde anlässlich seiner Berufung zum Minister für Nationale Verteidigung zum Admiral befördert und behielt den Dienstgrad auch als "Chef der Nationalen Volksarmee" unter dem Minister für Abrüstung und Verteidigung Rainer Eppelmann. Waldemar Verner wurde während seiner Zeit als Chef der Politischen Hauptverwaltung, ebenfalls Stellvertreter des Ministers, zum Admiral befördert.

Der Admiral ist einer der Dienstgrade der Bundeswehr für Marineuniformträger. Gesetzliche Grundlage ist die Anordnung des Bundespräsidenten über die Dienstgradbezeichnungen und die Uniform der Soldaten und das Soldatengesetz.

Innerhalb der Kommandostruktur der Teilstreitkraft Marine sind keine Dienststellungen für Admirale ausgeplant. Beispielsweise kann aber der Generalinspekteur der Bundeswehr ein Admiral sein. Denkbar ist auch eine Verwendung in höheren Führungsstäben der NATO. Bisher gab es erst fünf Offiziere im Dienstgrad Admiral.

Die Dienstgradabzeichen des Admirals zeigen einen handbreiten, darüber drei mittelbreite Ärmelstreifen auf beiden Unterärmeln.

Die Dienstgradbezeichnung ranggleicher Luftwaffen- und Heeresuniformträger lautet General. Hinsichtlich Befehlsgewalt, Ernennung, Sold, Dienststellungen, den nach- und übergeordneten Dienstgraden sind Admirale und Generale gleichgestellt. Beide Dienstgrade wurden durch die Anordnung des Bundespräsidenten über die Dienstgradbezeichnungen und die Uniform der Soldaten vom 7. Mai 1956 neu geschaffen.

Gemäß Duden lautet der Plural Admirale oder Admiräle, wobei in der Umgangssprache (meist auch im Sprachgebrauch der Bundeswehr) die Pluralform „Admirale“ vorherrscht.

Gemäß der Zentralen Dienstvorschrift (ZDv) A-1420/24 „Dienstgrade und Dienstgradgruppen“ existiert keine Dienstgradgruppe „Admirale“ oder „Admiräle“. Vielmehr zählen die höheren Marineuniformträger zur Dienstgradgruppe der Generale. Dennoch wird der Plural neben einer Zusammenfassung mehrerer Soldaten im Dienstgrad Admiral auch als Sammelbezeichnung für mehrere Dienstgrade verwendet. Aufgrund des inoffiziellen Charakters ist nicht immer klar, ob Admirale nur alle Soldaten meint, die mit Herr Admiral angeredet werden (also nur die Dienstgrade Flottillenadmiral, Konteradmiral, Vizeadmiral und Admiral<ref name="ZDV10/8"></ref><ref name="ZDV10/4">vgl. auch </ref>) oder darüber hinaus alle weiteren Marineuniformträger (also einschließlich entsprechender Sanitätsoffiziere) der Dienstgradgruppe der Generale mit einschließt.

Der erste englische Admiral der Royal Navy war William of Leybourne, der 1297 von König Edward I. zum "Admiral of the sea of the King of England" ernannt wurde. Der Admiral als Marineoffizier darf nicht verwechselt werden mit dem Amt des "Admiral of England" oder Lord High Admiral, dessen Inhaber die Verantwortung für die gesamte Marine hatte, also ein Marineminister im heutigen Sinne war.

In der Royal Navy gab es seit dem 16. Jahrhundert die Funktion der Vize- und Konteradmirale ("Vice-" beziehungsweise "Rear-Admirals"), die ursprünglich Stellvertreter des kommandierenden Admirals waren. Ein kommandierender Admiral konnte seine Flotte von der Spitze oder von der Mitte aus führen. Befand er sich auf einem Schiff in der Mitte der Flotte, hatte er in der Spitze einen Stellvertreter, den Vizeadmiral. Einen weiteren Stellvertreter hatte er im hinteren, der Spitze entgegengesetzten Bereich, den Konter- oder Rear-Admiral (von lateinisch "contra", gegen, beziehungsweise englisch "rear" für hinten).

Im elisabethanischen Zeitalter wurde die Flotte so groß, dass sie in Geschwader ("squadrons") unterteilt werden musste. Das Geschwader des Admirals führte einen roten Stander, das des Vizeadmirals einen weißen und das des Konteradmirals einen blauen. Nachdem auch diese Geschwader immer mehr angewachsen waren, wurde jedes davon von einem Admiral mit jeweils einem Vize- und Konteradmiral geführt. Die Bezeichnung für die Befehlshaber lautete dann "Admiral of the White", "Admiral of the Blue" usw.

Die Rangfolge der Flotten und damit auch ihrer Admirale war in absteigender Folge: Rot, Weiß, Blau. Die Beförderung zum Admiral erfolgte in Abhängigkeit vom Dienstalter als Kapitän und galt auf Lebenszeit. Man konnte demnach erst dann weiterbefördert werden, wenn der Inhaber des höheren Ranges gestorben war oder seinen Abschied genommen hatte. Eine andere Möglichkeit war, einen unfähigen Admiral oder einen, der den Unwillen der Lords der Admiralität erregt hatte, ohne Kommando zu befördern. Man bezeichnete diese Praxis als "Yellowing" und den auf diese Weise aus dem Weg Geräumten als "Yellow Admiral".

Die Rangfolge der Flaggoffiziere / Admirale (absteigend)


Als Lord Nelson starb, war er "Vice Admiral of the White".

Im 18. Jahrhundert begann man damit, die ursprünglich neun Dienststellungen mit mehreren Inhabern zu besetzen.

1864 wurde die Unterteilung der Flotte in verschiedenfarbige "Divisions" ganz aufgegeben. Die rote Flagge wurde der Handelsmarine zugewiesen, die weiße der Kriegsmarine und die blaue der Reserve und den Hilfsschiffen.

Heute sind die Dienstgrade der Flaggoffiziere der "Royal Navy" der "Rear Admiral", "Vice Admiral", "Admiral" und "Admiral of the Fleet". Seit 1996 wird der Dienstgrad "Admiral of the Fleet" in Friedenszeiten nicht mehr vergeben. Ausnahmen von dieser Regel werden nur für Mitglieder der königlichen Familie gemacht. Die vor diesem Termin ernannten Flottenadmirale behalten aber ihren Rang auf Lebenszeit.

Der Rang des "Commodore" (deutsch bis 1945 Kommodore, heute in etwa Flottillenadmiral) war bis 1996 in der Royal Navy kein Admiralsdienstgrad, sondern eine an den Dienstposten gebundene Bezeichnung für einen dienstälteren Captain, die nach Verlassen des Dienstpostens wieder entfiel. Seit 1996 ist der Dienstgrad “Commodore” ein offizieller Dienstgrad in der Royal Navy. Er ist dem Captain übergeordnet und dem Rear Admiral untergeordnet (NATO-Code: OF 6).

Die Russische Seekriegsflotte hat seit 1992 folgende Admiralsränge.

In der Französischen Marine werden die vier Admiralsränge "Contre-amiral" (zwei Sterne), "Vice-amiral" (drei Sterne), "Vice-amiral d’escadre" (vier Sterne) und "Amiral" (fünf Sterne) vergeben.

Der Titel "Amiral de France" (Admiral von Frankreich) – manchmal auch "Amiral de la flotte" – wurde von 1302 bis 1870 an 28 Marineoffiziere verliehen. Er entsprach dem Rang eines "Maréchal de France" (Marschall von Frankreich).

Nur einmal wurde dann wieder der Titel "Amiral de la flotte" (Admiral der Flotte) 1939 an François Darlan verliehen.

Heute ("Gesetz von 1972") ist der Titel Admiral von Frankreich eine staatliche Würde, die bisher nicht verliehen wurde.

Die United States Navy hatte bis 1862 überhaupt keine Admirale, obwohl die Einrichtung dieses Dienstgrades immer wieder gefordert wurde, unter anderem auch von John Paul Jones, der die Meinung vertrat, dass die kommandierenden Marineoffiziere mit den Armeegeneralen auf einer Stufe stehen sollten. Außerdem hielt er höherrangige Offiziere für nötig, um Streitigkeiten zwischen den rangälteren Kapitänen zu vermeiden oder zu schlichten.

Die verschiedenen Marineminister schlugen dem Kongress wiederholt vor, den Rang eines Admirals zu schaffen, um eine Gleichstellung mit den Marinen anderer Staaten herzustellen, weil die höheren Offiziere der "US Navy" immer wieder in protokollarische Schwierigkeiten mit Offizieren anderer Nationen gerieten. Schließlich stimmte der Kongress am 16. Juli 1862 zu, neun Konteradmirale ("Rear Admirals") zu ernennen, was aber wohl weniger mit der Anpassung an internationale Erfordernisse zu tun hatte, als vielmehr mit der schnell anwachsenden Stärke der Marine im Amerikanischen Bürgerkrieg. Zwei Jahre später erlaubte der Kongress, einen der neuen Konteradmirale, David Farragut, zum Vizeadmiral zu ernennen. Im Juli 1866 autorisierte er US-Präsident Johnson, Farragut zum Admiral und David Dixon Porter zum Vizeadmiral zu ernennen. Als Farragut 1870 starb, wurden Porter Admiral und Stephen C. Rowan Vizeadmiral. Nach dem Tod der beiden ranghöchsten Admirale wurden keine weiteren Beförderungen mehr bewilligt, so dass es bis 1915 keinen Admiral oder Vizeadmiral mehr gab, bis der Kongress zustimmte, je einen Admiral und Vizeadmiral für die Atlantikflotte, die Pazifikflotte und die Asiatische Flotte zu ernennen.

Trotzdem gab es in der Zwischenzeit einen höherrangigen Admiral. 1899 würdigte der Kongress George Deweys Verdienste im Spanisch-Amerikanischen Krieg, indem er Präsident McKinley ermächtigte, ihn zum Admiral of the Navy zu ernennen, was er bis zu seinem Tode 1917 blieb. Dewey war bis heute der einzige US-amerikanische Marineoffizier mit diesem Rang.

1944 genehmigte der Kongress den Rang des Flottenadmirals ("Admiral of the Fleet"). Die ersten und bisher einzigen Inhaber dieses Dienstgrads waren Ernest J. King, William D. Leahy, Chester W. Nimitz (alle im Dezember 1944) und William F. Halsey, der seinen fünften Stern im Dezember 1945 erhielt.

In der Österreichischen (k. k.) Kriegsmarine (ab 1868 k. u. k. Kriegsmarine) wurden von 1849 bis 1918 die Ränge Kontreadmiral (im 20. Jh. auch Konteradmiral), Viceadmiral, und Admiral sowie Großadmiral vergeben.

Bekannte österreichische bzw. österreichisch-ungarische Admirale waren:



</doc>
<doc id="255" url="https://de.wikipedia.org/wiki?curid=255" title="Andine Knollenbohne">
Andine Knollenbohne

Die Andine Knollenbohne, "Ahipa" ("Pachyrhizus ahipa", Syn.: "Dolichos ahipa" ) ist eine Pflanzenart aus der Gattung der Yambohnen ("Pachyrhizus") in der Familie der Hülsenfrüchtler (Fabaceae).

Die Andine Knollenbohne hat ihren Ursprung im gebirgigen Südamerika (von Anden der deutsch Trivialname), wo sie von den Inka genutzt wurde. Sie gedeiht in Höhenlagen von 1000 bis über 2500 Meter und damit bei gemäßigten Temperaturen. Es werden hauptsächlich die Knollen gegessen, aber auch, obwohl sie giftig sind, werden die Hülsen und Samen genutzt. Heute wird sie nur noch selten angebaut.

"Pachyrhizus ahipa" ist eine ausdauernde krautige Pflanze, die Wuchshöhen von 1 bis 2 Meter erreicht: Sie ist, anders als die restlichen Arten der Gattung keine Kletterpflanze. Sie bilden Knollen als Überdauerungsorgane und als Wasserspeicher. Sie wird aber als einjährige Pflanze angebaut, weil die ganze Pflanze wegen der Knollen abgeerntet wird und im Folgejahr eine Neuaussaat erforderlich ist. Die kleine, regelmäßig geformte Knolle besitzt eine gelbe Schale, weißes, faserdurchzogenes „Fleisch“ und wiegt zwischen 500 und 800 g. Die Laubblätter sind dreiteilig gefiedert. Sowohl die zwei Seitenfiederblätter als auch das Terminalfiederblatt sind extrem asymmetrisch und breiter als lang. 

Der kurze Blütenstand weist eine Länge von etwa 4 cm auf und enthält nur wenige Blüten. Die zwittrigen Blüten sind zygomorph. Die Krone ist weiß oder violett. Es wird etwa 8 bis 11 cm lange Hülsenfrucht gebildet, die braune bis schwarze, runde Samen mit etwa 1 cm Durchmesser enthält.

Die Knollen werden sehr vielseitig zubereitet: roh in Salaten, gekocht und frittiert.
Die Hülsenfrüchte können nur gegart gegessen werden, weil dadurch das giftige Rotenon entfernt wird.

Aus den Samen kann Öl gewonnen werden.

Auf Grund ihrer Rotenon-Gehalte finden Hülsenfrüchte und Samen Verwendung als Insektizid, Akarizid und ihre Saponine wirken als Fischgift.




</doc>
<doc id="256" url="https://de.wikipedia.org/wiki?curid=256" title="Apollo 11">
Apollo 11

Apollo 11 war eine Raumfahrtmission im Rahmen des Apollo-Programms der US-amerikanischen Raumfahrtbehörde NASA und der erste bemannte Flug zum Mond, der eine Landung und eine sichere Rückkehr auf die Erde zum Ziel hatte. Die Mission verlief erfolgreich und erfüllte die 1961 von US-Präsident John F. Kennedy erteilte Aufgabe an die Nation, noch vor Ende des Jahrzehnts einen Menschen zum Mond und wieder sicher zurück zur Erde zu bringen.

Die drei Astronauten Neil Armstrong, Edwin „Buzz“ Aldrin und Michael Collins starteten am 16. Juli 1969 mit einer Saturn-V-Rakete von Launch Complex 39A des Kennedy Space Center in Florida und erreichten am 19. Juli eine Mondumlaufbahn. Während Collins im Kommandomodul des Raumschiffs "Columbia" zurückblieb, setzten Armstrong und Aldrin am nächsten Tag mit der Mondlandefähre "Eagle" auf dem Erdtrabanten auf. Wenige Stunden später betrat Armstrong als erster Mensch den Mond, kurz danach auch Aldrin. Nach einem knapp 22-stündigen Aufenthalt startete die Landefähre wieder von der Mondoberfläche und kehrte zum Mutterschiff zurück. Nach Rückkehr zur Erde wasserte die "Columbia" mit den drei Astronauten am 24. Juli rund 25 Kilometer vom Bergungsschiff USS Hornet entfernt im Pazifik südlich des Johnston-Atolls.

Bei der Fernsehübertragung der Mondlandung 1969 verfolgten weltweit rund 600 Millionen Menschen das Ereignis.

Kommandant der Apollo-11-Mission war Neil Armstrong. Er war zunächst ein Kampfflugzeugpilot der US Navy, bevor er bei der NASA als Testpilot zahlreicher Hochgeschwindigkeits-Flugzeuge wie der X-15 tätig war. Als er 1962 in die 2. NASA-Astronautengruppe aufgenommen wurde, gehörte er neben Elliott See zu den ersten beiden Zivilisten der US-Astronauten. Seinen ersten Weltraumflug absolvierte Armstrong mit Gemini 8 im März 1966, bei dem die erste Kopplung zwischen zwei Raumfahrzeugen durchgeführt wurde.

Pilot der Mondlandefähre war Edwin „Buzz“ Aldrin, ein Oberst der US Air Force. Er wurde 1963 ein Mitglied der 3. NASA-Astronautengruppe, nachdem er zuvor mit der Air Force am Koreakrieg teilgenommen und eine Promotion in Raumfahrttechnik erhalten hatte. Im November 1966 war Aldrin mit Gemini 12 erstmals zu einem Raumflug gestartet, in dessen Verlauf er dreimal einen Raumausstieg absolvierte.

Pilot des Kommandomoduls war Michael Collins, ein Oberstleutnant und früherer Kampfjetpilot der US Air Force. Wie Aldrin gehörte er der 3. NASA-Astronautengruppe von 1963 an. Erste Weltraumerfahrung sammelte Collins im Juli 1966, als er mit Gemini 10 im All war. Während dieser Mission verließ er das Raumschiff für zwei Außenbordeinsätze.

Armstrong und Aldrin hatten zusammen mit Fred Haise bereits die Ersatzmannschaft des Fluges Apollo 8 gebildet. Collins war ursprünglich für die dritte bemannte Apollo-Mission (die später auf Apollo 8 vorverlegt wurde) vorgesehen gewesen, aufgrund von Bandscheibenproblemen, die eine Operation erforderlich machten, aber aus dem Team ausgeschieden. Nach seiner Genesung wurde er direkt für die Hauptmannschaft von Apollo 11 nominiert, wodurch Fred Haise in die Ersatzmannschaft rückte.

Für die Ersatzmannschaft, die ein ausgefallenes Mitglied der Hauptbesatzung im Bedarfsfall ersetzt hätte, waren Jim Lovell als Kommandant, William Anders als Pilot des Kommandomoduls und Fred Haise als Pilot der Mondfähre eingeteilt.

Die Unterstützungsmannschaft (Support Crew), die der eigentlichen Besatzung beim Training assistierte, bestand aus Ken Mattingly, Ron Evans, Bill Pogue und Jack Swigert.

Die Mitglieder der Ersatzmannschaft sowie Mattingly und Evans aus der Unterstützungscrew waren außerdem zusammen mit Charles Duke, Bruce McCandless, Don Lind, Owen Garriott und Harrison Schmitt als Verbindungssprecher (CAPCOM) tätig. In dieser Funktion waren sie normalerweise die Einzigen, die mit den Astronauten im Weltraum sprachen.

Flugleiter ("Flight Director") im Kontrollzentrum in Houston waren Cliff Charlesworth (während des Starts und des Mondspaziergangs), Gene Kranz (bei der Mondlandung), Glynn S. Lunney (für den Rückstart zur Erde) und Gerald D. Griffin. Sie trafen die für den Erfolg der Mission relevanten Entscheidungen und waren für die Sicherheit der Besatzung verantwortlich.

Das Abzeichen von Apollo 11 zeigt das Wappentier der Vereinigten Staaten, den Weißkopfseeadler, kurz vor der Landung auf dem Mond. In seinen Krallen trägt er einen Olivenzweig, der die friedvollen Absichten der ersten Mondlandung unterstreichen soll. Die Erde – Start- und Endpunkt der Mission – ist vor einem schwarzen Hintergrund, der das Unbekannte des Weltraums symbolisieren soll, zu erkennen. Auf die Aufnahme der Namen der Astronauten wurde bewusst verzichtet, um den Beitrag jedes Einzelnen, der für das Apollo-Programm gearbeitet hat, hervorzuheben. Stattdessen trägt das Abzeichen den Schriftzug „APOLLO 11“ an der Spitze.

Bei Auswahl der Rufnamen der Raumschiffe wurde der Besatzung vom NASA-Management wegen der historischen Bedeutung der Mission dazu geraten, ehrwürdige Bezeichnungen zu verwenden – beim vorangegangenen Flug Apollo 10 hießen die beiden Raumfahrzeuge nach Figuren aus der Comicserie Die Peanuts "Charlie Brown" und "Snoopy". Die Apollo-11-Astronauten entschieden sich schließlich dazu, die Mondlandefähre – vom im Abzeichen verwendeten Motiv herrührend – "Eagle" (Adler) zu nennen, während die Kommandokapsel das Rufzeichen "Columbia" erhielt. Die Wahl von "Columbia" wurde mit der großen Bedeutung des Wortes in der US-amerikanischen Geschichte begründet.

Apollo 11 war in der Flugsequenz des Apollo-Programms die sogenannte "G-Mission", deren Ziel die erste bemannte Landung auf dem Mond war. Die Planungsphase für Apollo 11 begann im Jahr 1965, nachdem die Entwicklung der beiden Raumfahrzeuge abgeschlossen war, und stand unter der Leitung von Christopher Kraft, dem Flugbetriebsleiter am Manned Spacecraft Center in Houston. Seinem Team oblag die Ausarbeitung der Checklisten für die Besatzung sowie des Flugplans, dessen endgültige Version zwei Wochen vor dem Start am 1. Juli 1969 erschien. Für den Fall einer Verschiebung der Mission erarbeitete Krafts Abteilung darüber hinaus Flugszenarien für Startfenster im August und September 1969.

Die Pläne für den Mondausstieg wurden im Lauf der Vorbereitungen mehrfach überarbeitet. Das ursprüngliche Konzept aus dem Jahr 1964 sah vor, dass nur der Pilot der Landefähre für zwei Stunden die Mondoberfläche betritt, während der Kommandant der Mission zur Überwachung der Systeme in der Mondfähre bleibt. Eine Studie der Grumman Aerospace Corporation aus demselben Jahr deutete jedoch darauf hin, dass die Teilnahme beider Astronauten am Mondspaziergang technisch möglich sei. Anfang Januar 1967 schlug Flugbetriebsleiter Christopher Kraft vor, die Zeit nach der Landung auf der Mondoberfläche für zwei Ausstiege zu verwenden. Der erste Mondspaziergang sollte demnach nur zur Eingewöhnung der Astronauten an die Umgebung dienen, während der zweite Ausstieg zum Aufstellen von wissenschaftlichen Experimenten und der Entnahme von Mondgestein-Proben genutzt werden sollte.

Im September 1968 entschied sich die NASA jedoch, dass Apollo 11 nur eine 2,5-stündige Mondexkursion der beiden Raumfahrer beinhalten und die wissenschaftlichen Instrumente wegen des hohen Gewichts nicht zum Mond mitgeführt werden sollten. Wilmot N. Hess, der Leiter der wissenschaftlichen Abteilung der NASA in Houston, drängte indessen darauf, dennoch ein kleines Paket von wissenschaftlichen Messgeräten zum Mond mitzuführen. Der Planungsstab bewilligte daraufhin am 9. Oktober 1968 die Entwicklung von drei vergleichsweise leichten Experimenten für Apollo 11, dem Early Apollo Surface Experiments Package (EASEP).

Für die Zusammenstellung der Besatzungsmitglieder für Apollo 11 war Deke Slayton, der Chef des NASA-Astronautenbüros, verantwortlich. Bei der Auswahl der einzelnen Crews ging er nach dem Rotationsprinzip vor, wonach eine Ersatzmannschaft zwei Flüge aussetzt, bevor sie selbst für einen Flug nominiert wird. Demnach sollte die Reservemannschaft für Apollo 8, die aus den Astronauten Neil Armstrong, Buzz Aldrin und Fred Haise bestand, die Besatzung für Apollo 11 bilden. Armstrong setzte sich allerdings dafür ein, dass Michael Collins, der wegen einer Operation seinen Platz in der Crew von Apollo 8 verloren hatte, in seine Mannschaft aufrückt, um den Platz von Haise einzunehmen. Das obere NASA-Management hatte keine Einwände gegen diese Besetzung, so dass Armstrong, Aldrin und Collins der Öffentlichkeit am 10. Januar 1969 als Besatzung für Apollo 11 vorgestellt wurden. Zum Zeitpunkt ihrer Auswahl war die Mannschaft noch nicht davon überzeugt, die erste bemannte Mondlandung zu absolvieren, da die Mondlandefähre bis dahin noch nicht bemannt im Weltraum getestet worden war.

Bei der Auswahl des Landeplatzes für die Mondlandefähre war die Sicherheit der Astronauten der Hauptgesichtspunkt. Die Mondlandung musste beispielsweise bei direkter Sonneneinstrahlung und optimalen Sichtverhältnissen durchgeführt werden; der Rückstart zur Erde musste ebenfalls bei Tageslicht erfolgen. Die Vorgabe, möglichst wenig Treibstoff zu verbrauchen, um entsprechend möglichst hohe Treibstoffreserven mitführen zu können, begrenzte den Landeplatz für Apollo 11 zudem auf Gebiete in Nähe des Mondäquators. Eine ebenso wichtige Rolle bei der Auswahl spielte schließlich die Beschaffenheit der Mondoberfläche im Landegebiet. So bestimmten die Kriterien etwa, dass das Gewicht der Landefähre ausreichend getragen werden müsse und die Zahl der Krater und Felsbrocken so klein wie möglich sein sollte.

Zur Bereitstellung von Bildern und anderen Daten von möglichen Landestellen schickte die NASA im Laufe der 1960er Jahre mehrere Raumsonden aus den Ranger- und Surveyor-Programmen zum Mond. Während Ranger zur Übermittlung von hochauflösenden Bildern diente, absolvierten die Surveyor-Sonden eine weiche Landung auf der Mondoberfläche, um wissenschaftliche Daten und Fernsehbilder zur Erde zu senden. Die Qualität der Aufnahmen reichte allerdings für eine detaillierte Analyse der denkbaren Landeplätze nicht aus, weshalb die NASA mit Lunar Orbiter eine weitere Reihe von Raumsonden entwickelte. Diese mit zwei Kameras ausgestatteten Mondsatelliten dokumentierten 99 Prozent der Mondoberfläche und übermittelten Bilder von 20 potenziellen Landestellen für das Apollo-Programm.

Mitte 1965 gründete die NASA das "Apollo Site Selection Board", dessen Aufgabe es war, nach der Abwägung von wissenschaftlichen und flugbetrieblichen Aspekten Vorschläge für mögliche Landeplätze zu machen. Alle Kandidaten befanden sich in Äquatornähe und erschienen auf den Bildern der Mondsonden relativ eben. Darüber hinaus achteten die Missionsplaner bei der Wahl der Landestelle darauf, dass das umliegende Terrain keine Hänge und sonstige Unregelmäßigkeiten aufwies, da sonst das Radar der Mondlandefähre beim Anflug gestört werden könnte. Am 15. Dezember 1968 einigte sich das Auswahlgremium auf eine Liste von fünf denkbaren Landeplätzen, den "Apollo Landing Sites" (ALS).

Für den Flug mit Apollo 11 wählte die NASA mit ALS-2 schließlich die westliche der beiden Landestellen im Meer der Ruhe. Dort war etwa 20 Stunden vor der Landung die Sonne aufgegangen. Da ein vollständiger Mondtag 29,53 Erdtage dauert, stand die Sonne bei der Landung etwa 10° über dem östlichen Horizont. In dem flach einfallenden Morgenlicht waren Unebenheiten der Mondoberfläche gut zu erkennen. Die elliptische Landezone entsprach mit 18,5 Kilometern ungefähr der Insel von Manhattan. Zwei andere, weiter westlich liegende Landeplätze (ALS-3 und ALS-5) dienten im Fall einer Startverschiebung als Ausweichstandorte, um eine bestmögliche Beleuchtung beim Endanflug der Landefähre zu gewährleisten.

Die Startvorbereitungen für Apollo 11 begannen Anfang Januar 1969 mit der Ankunft der Mondlandefähre (Lunar Module, LM) im Kennedy Space Center (KSC) in Florida. Das Apollo-Raumschiff, in dem sich die Besatzung für den Großteil des Flugs aufhielt, traf am 23. Januar an Bord eines Super-Guppy-Transportflugzeugs im Raumfahrtzentrum ein. Beide Raumfahrzeuge wurden in das Manned Spacecraft Operations Building gebracht, wo die einzelnen Komponenten der Raumschiffe integriert und umfangreichen Funktionstests unterzogen wurden. Darüber hinaus absolvierten sowohl die Mondlandefähre als auch das Apollo-Raumschiff mehrere Probeläufe in einer Höhenkammer, um die Belastungen der Systeme im Vakuum des Weltalls zu simulieren. Nach dem Ende der Abschlusskontrollen wurde die Mondlandefähre schließlich von einem 8,5 Meter hohen Kegelstumpf umgeben, der zum Schutz während der Startphase diente. Das Apollo-Raumschiff wurde auf die Spitze dieser Verschalung gesetzt.

Parallel zu den Arbeiten an den beiden Raumfahrzeugen erfolgte im sieben Kilometer entfernten Vehicle Assembly Building (VAB) die Montage der Trägerrakete Saturn V. Nach der Anlieferung aus den Herstellerwerken wurden die drei Stufen der Rakete auf der 5.715 Tonnen schweren Startplattform miteinander verbunden. Die eingekapselte Mondlandefähre und das Apollo-Raumschiff wurden am 14. April hinzugefügt, womit der Aufbau der 110 Meter hohen Trägerrakete abgeschlossen war. Am 14. Mai durchlief die als "space vehicle" bezeichnete Startkonfiguration der Rakete einen simulierten Countdown, der die Kompatibilität der einzelnen Systeme testete.

Am 20. Mai 1969 brachte der sogenannte "Crawler", ein von zwei Dieselmotoren angetriebenes Raupenfahrzeug, die Saturn V zur Startrampe 39A, die zum fünften Mal für einen bemannten Start benutzt wurde. Die 5,5 Kilometer lange Fahrt auf einer speziell präparierten Piste dauerte sechs Stunden. Nach dem Erreichen der Rampe wurde unter der Startplattform ein Flammenlenkblech in Stellung gebracht, das die beim Abheben der Rakete entstehenden Triebwerksgase ableiten sollte. Des Weiteren wurde eine Wartungsplattform vor der Saturn V platziert, um die Arbeiten an der Rakete zu erleichtern. Der Flugbereitschaftstest der Trägerrakete, bei dem auch die Besatzung von Apollo 11 teilnahm, wurde am 6. Juni abgeschlossen.

Am 27. Juni begann mit dem "Countdown Demonstration Test" die letzte wichtige Erprobung der Trägerrakete. Während des mehrtägigen Tests wurden die Tanks der Saturn V mit Treibstoff befüllt und der Countdown bis zum Start der Zündungssequenz simuliert. In einer anschließenden zweiten Phase entleerte man die Brennstofftanks wieder und der Versuchscountdown wurde mit der Besatzung an Bord wiederholt.

Apollo 11 startete am 16. Juli 1969 um 13:32:00 UTC an der Spitze der 2940 Tonnen schweren Saturn V von Cape Canaveral, Florida und erreichte zwölf Minuten später planmäßig die Erdumlaufbahn. Nach anderthalb Erdumkreisungen wurde die dritte Raketenstufe erneut gezündet. Sie brannte etwa sechs Minuten lang und brachte das Apollo-Raumschiff auf Mondkurs. Kurze Zeit später wurde das Kommando/Servicemodul (CSM) an die Landefähre angekoppelt. Der gesamte Hinflug verlief ohne besondere Vorkommnisse. Drei Tage später erreichten sie den Mond und schwenkten um 17:22:00 UTC durch ein Bremsmanöver über der Rückseite des Mondes in eine Mondumlaufbahn ein.

Im Mondorbit stiegen erst Aldrin und eine Stunde später (nach Hochfahren der Systeme) Armstrong in die Mondlandefähre um. Nach Prüfung der Systeme und Ausklappen der Landebeine der Fähre trennten sie diese vom Mutterschiff, in dem Collins verblieb, und leiteten die Abstiegssequenz ein. Heikel war dann der Anflug auf das Zielgebiet im Mare Tranquillitatis. Durch geringe unbeabsichtigte Bahnänderungen beim Abkoppeln zielte der Bordcomputer auf eine Stelle etwa 4,5 Kilometer hinter dem geplanten Landegebiet. Während des Anfluges wurde die Aufmerksamkeit der Besatzung außerdem etwa 1,5 Kilometer über dem Boden mehrfach durch Alarmmeldungen des Navigationscomputers in Anspruch genommen, so dass Armstrong nicht in dem Maße auf charakteristische Merkmale der Mondlandschaft achten konnte, wie es vom Flugplan vorgesehen war. Zu diesen Alarmmeldungen kam es, da entgegen dem Flugplan das Rendezvousradar zusätzlich zum Landeradar eingeschaltet worden war. Das Rendezvousradar überflutete den Computer mit seinen zusätzlichen für diese Phase der Mission nicht vorgesehenen Daten, wodurch der Computer überlastet wurde. Dank dem von Hal Lanning vom M.I.T. Instrumentation Laboratory entwickelten Betriebssystem mit einer Priorisierung der einzelnen Aufgaben (die Aktualisierung eines Displays hat eine niedrige Priorität, die Lage-Steuerung der Landefähre die höchste Priorität) wurde den Daten vom Rendezvousradar eine etwas niedrigere Priorität zugewiesen und der Computer meldete diese Probleme als Fehler 1201 und 1202. Das Problem erwies sich jedoch als unkritisch und konnte ignoriert werden.

Beim Endanflug führte der Autopilot die Fähre auf ein Geröllfeld zu, das einen großen Krater umgab und mit großen Felsen übersät war. Wie sich später herausstellte, handelte es sich dabei um den so genannten "West"-Krater. Armstrong übernahm daraufhin die Handsteuerung der "Eagle", überflog den Krater und landete auf einer ebenen Stelle ca. 500 m weiter westlich (knapp 60 m jenseits des "Little West"-Kraters). Das Kontaktlicht signalisierte den unmittelbar bevorstehenden Bodenkontakt (bei circa 75 cm Höhe) am 20. Juli um 20:17:39 UTC. Der Mondlandepilot Aldrin meldete das („Contact light“) um 20:17:40 UTC. Unmittelbar darauf erfolgte der Kontakt aller vier Landefüße mit dem Mondboden. Circa drei bis vier Sekunden nach den Kontaktsignalen schaltete Armstrong das Triebwerk ab. Zu diesem Zeitpunkt hatte die Fähre „Eagle“ bereits sehr sanft (mit etwa 0,52 m/s) auf dem Mond aufgesetzt.

Die zusätzlichen Manöver hatten das ohnehin knapp kalkulierte Treibstoffbudget so strapaziert, dass die Astronauten nur noch etwa 20 Sekunden Zeit gehabt hätten, eine Entscheidung zu treffen: entweder innerhalb der nächsten 20 Sekunden zu landen oder den Anflug sofort abzubrechen. Spätere Analysen zeigten, dass der in den Tanks schwappende Treibstoff zu ungenauen Anzeigen geführt hatte und noch mehr Reserve vorhanden war.

Armstrong und Aldrin bereiteten sofort einen möglichen Alarmstart vor, für den Fall, dass ein Leck im Tank der Aufstiegsstufe oder ein Einsinken eines der Landebeine einen längeren Aufenthalt unmöglich machen würde. Die Landung war zeitlich so geplant, dass nach dem ursprünglich vorgesehenen Bodenkontakt (geplant bei circa 20:17:00 UTC) ein Zeitfenster von etwa einer Minute für einen sofortigen Rückstart verblieb. Andernfalls hätte man die Umlaufbahn des Mutterschiffs verfehlt, und Collins hätte das Annäherungsmanöver durchführen müssen. Etwa 30 bis 40 Sekunden davon waren durch die zusätzlichen Manöver beim Endanflug verflossen. Letztlich blieb damit nach dem Abschluss dieser Prozeduren eine Zeitreserve von fünf bis zehn Sekunden.

Am 20. Juli 1969 um 20:17:58 Uhr UTC vermeldete Armstrong:

Das primäre Ziel war erreicht. Ab diesem Moment benutzten Armstrong und Aldrin das Rufzeichen "Tranquility Base".

In den folgenden zwei Stunden waren die Astronauten damit beschäftigt, Vorbereitungen für den Rückflug zu treffen, der alle zwei Stunden erfolgen konnte. Unter anderem musste der Bordcomputer mit der genauen Ausrichtung der Mondfähre programmiert werden. Die genaue Position war zu diesem Zeitpunkt jedoch nicht bekannt, weil Armstrong beim Anflug keine bekannten Geländeformationen identifiziert hatte. Bei seinen fünf Überflügen mit der "Columbia" versuchte Collins die Mondfähre zu sichten. Da aber auch ihm keine genaue Position zur Verfügung stand, blieb das erfolglos.

Weiterhin fotografierten Armstrong und Aldrin die Mondoberfläche aus ihren Fenstern. Die ursprünglich geplante Ruhepause von 5 Stunden und 40 Minuten wurde auf Anregung der Astronauten auf 45 Minuten verkürzt und der Ausstieg vorgezogen. Die Vorbereitungen hierzu benötigten etwa drei Stunden.

Am 21. Juli 1969 um 02:56:20 UTC (in den USA war es noch der 20. Juli) betrat Neil Armstrong als erster Mensch den Mond und sprach die berühmten Worte:
Dieses Ereignis wurde sowohl von Aldrin aus dem Fenster der Mondfähre als auch von einer Fernsehkamera am Fuß der Landefähre gefilmt. Etwa 600 Millionen Fernsehzuschauer auf der Erde erlebten die Live-Übertragung.
20 Minuten später verließ auch Buzz Aldrin die Mondfähre. Zur Messung der Zusammensetzung des Sonnenwindes auf dem Mond wurde eine Aluminiumfolie (SWC) aufgehängt, die kurz vor Ende des Ausfluges wieder mitgenommen wurde. Nachdem die US-Flagge gehisst worden war, bauten die beiden Astronauten einige kleine Forschungsgeräte des EASEP ("Early Apollo Scientific Experiment Package"), des Vorläufers des ALSEP, auf dem Mond auf. So sollten mittels eines Seismometers (PSEP) Daten über die seismischen Aktivitäten des Mondes erfasst werden. Das Gerät überstand die erste Mondnacht jedoch nicht. Ein Laserreflektor (LRRR) auf der Oberfläche ermöglichte es, präzise die Entfernung zwischen Mond und Erde zu messen. Außerdem wurden Bodenproben entnommen und 21,6 kg Gestein gesammelt. Der erste Aufenthalt auf der Mondoberfläche endete nach zwei Stunden und 31 Minuten.

Noch vor der Ruhephase stellte Aldrin fest, dass der Hebel eines Schalters abgebrochen war, ein anderer war nicht in der vorgesehenen Position. Offenbar hatte Aldrin bei der Vorbereitung der EVA mit dem Rucksack die Schalter berührt. Diese Schalter wurden erst eine Stunde vor dem Start benötigt. Aldrin verwendete später einen Filzstift, um den Schalter zu betätigen.

Der Start der Landefähre gelang problemlos, die Fähre schwenkte in eine Mondumlaufbahn ein und koppelte knapp vier Stunden später wieder an der Kommandokapsel an. Nachdem Armstrong und Aldrin zu Collins umgestiegen waren, wurde die Mondfähre abgestoßen und das Apollo-Raumschiff wieder auf Erdkurs gebracht. Am 24. Juli 1969 um 16:50 UTC wasserte die Kapsel im Pazifik und wurde vom Bergungsschiff USS Hornet an Bord genommen.

Aus Furcht vor unbekannten Mikroorganismen mussten die drei Astronauten beim Verlassen der Apollo-Landekapsel nach außen vollkommen geschlossene Anzüge zur Isolierung tragen und sich in eine Quarantäne von siebzehn Tagen begeben, bis alle Bedenken ausgeräumt waren. Das mobile Quarantänemodul kann heute an Bord der USS Hornet in Alameda besichtigt werden.

Das Kommandomodul Columbia von Apollo 11 ist nun im National Air and Space Museum in Washington, D.C. ausgestellt.

Wie bei vielen Ereignissen von solch großer Tragweite wurden auch die Mondlandungen zum Objekt zahlreicher Verschwörungstheorien. Diese Theorien gehen davon aus, dass die Landungen in den Jahren 1969 bis 1972 nicht stattgefunden haben (oft geht es auch nur um die erste bemannte Mondlandung), sondern von der NASA und der US-amerikanischen Regierung vorgetäuscht worden sind. Die Verschwörungstheorien haben seit den 1970ern durch den Autor und ehemaligen Mitarbeiter der NASA-Zulieferfirma Rocketdyne Bill Kaysing Verbreitung gefunden. Die NASA veröffentlichte 2012 hochauflösende Bilder des Lunar Reconnaissance Orbiter (LRO) von der Apollo-11-Landestelle.

Der Maler Peter Hecker verewigte das Ereignis 1969 in einem Kirchenfenster der St.-Martinus-Kirche in Solingen-Burg.

Bei der Behauptung, Armstrong habe vor Verlassen des Mondes noch den Satz "Good Luck, Mr. Gorsky" gesprochen, handelt es sich lediglich um eine populäre moderne Sage.

Die Apollo-11-Höhle im Süden Namibias wurde zu Ehren der ersten bemannten Mondmission benannt.

Ein in den Gesteinsproben von Apollo 11 festgestelltes, auf der Erde damals noch nicht bekanntes Mineral wurde nach den drei Astronauten "Armalcolit" getauft.

Aldrin feierte vor dem Ausstieg auf dem Mond das Abendmahl. Da dies sein privater Akt war, schaltete er während der Zeremonie das Mikrofon ab.

2014 übergab die Witwe von Neil Armstrong dem National Air and Space Museum eine Tasche mit Apollo-11-Ausrüstungsgegenständen. Darunter die 16mm Data Acquisition Camera (eine Filmkamera der Firma Maurer), die auf dem Mond verwendet wurde.




</doc>
<doc id="257" url="https://de.wikipedia.org/wiki?curid=257" title="Afrikanische Yambohne">
Afrikanische Yambohne

Die Afrikanische Yambohne ("Sphenostylis stenocarpa") ist eine Pflanzenart in Unterfamilie Schmetterlingsblütler (Faboideae) innerhalb der Familie der Hülsenfrüchtler (Fabaceae oder Leguminosae). Sie ist im tropischen Afrika beheimatet.

Diese Nutzpflanze ist nahe verwandt zu einer Reihe anderer „Bohnen“ genannter Feldfrüchte. Im deutschen Sprachraum sind auch die Bezeichnungen Rübenbohne und 'Knollenbohne' gebräuchlich. Die hier beschriebene Art gehört jedoch nicht zur Gattung der Yambohnen ("Pachyrhizus") und sollte auch nicht mit der Knollenbohne ("Pachyrhizus tuberosus") verwechselt werden. Ihre genaue Abstammung ist nicht geklärt.

Die Afrikanische Yambohne ist eine rankende krautige Pflanze. Die Sprossachsen erreichen Längen von 2 bis 3 Meter. Die Blüten sind zygomorph. Die 25 bis 30 Zentimeter lange, leicht holzige Hülsenfrüchte enthält 20 bis 30 Samen. Die Samen sind bei einer Länge von etwa 5 Millimeter und einem Durchmesser von etwa 1 Millimeter linsenförmig oder oval. 

Die Afrikanische Yambohne wächst im gesamten tropischen Afrika wild und wird in Zentralafrika und Westafrika (Elfenbeinküste, Ghana, Togo, Gabun, Demokratische Republik und Republik Kongo) sowie Teilen Ostafrikas (etwa Äthiopien) kultiviert, insbesondere aber im südlichen Nigeria, wo sie girigiri heißt. Sie verträgt saure und sandige Böden und kommt in Höhenlagen von 0 bis 1800 Meter vor. Die Afrikanische Yambohne ist auf feuchtwarmes Klima und nichtstauende Böden angewiesen.

Sowohl Samen als auch Wurzelknollen dienen als Nahrungsmittel.

Die Afrikanische Yambohne entwickelt an den Wurzeln 5 bis 7,5 Zentimeter lange und 50 bis 300 Gramm schwere Knollen, die wie längliche Süßkartoffeln aussehen, aber doppelt so viel Protein wie diese enthalten. Die Wurzelknollen haben einen Proteingehalt von 11 bis 19 Prozent und einen Kohlenhydratanteil von 63 bis 73 Prozent, mit 3 Prozent Faserstoffen. Pro Pflanze ist eine Ernte von einem halben Kilogramm Wurzeln möglich.

Die Samen sind wohlschmeckend und werden in Westafrika oft allen anderen verfügbaren Samen und Gemüse vorgezogen. Sie enthalten 21 bis 29 Prozent Protein, welches im Vergleich zu dem der Sojabohne ähnliche oder höhere Gehalte an Lysin und Methionin aufweist. Die Samen enthalten weiterhin etwa 50 Prozent Kohlenhydrate und 5 bis 6 Prozent Ballaststoffe. Bis zu 2000 kg Bohnen können pro Hektar geerntet werden.

Die getrockneten Bohnen werden üblicherweise in Wasser eingeweicht und mehrere Stunden gekocht, und dann ohne Beilage oder mit Yams, Reis, Mais oder in Suppen gegessen. Das Fleisch der Wurzelknollen verwendet man roh oder wie Kartoffeln gekocht, auch der Geschmack ist ähnlich.

Mit ihren großen, je nach „Varietät“ rosafarbenen, violetten oder grünlich-weißen Blüten eignet sich die Afrikanische Yambohne auch als Zierpflanze.





</doc>
<doc id="258" url="https://de.wikipedia.org/wiki?curid=258" title="Allergie">
Allergie

Als Allergie ( ‚die Fremdreaktion‘, aus "állos" ‚anders‘, ‚fremd‘, ‚eigenartig‘ und "to érgon" ‚das Werk‘, ‚die Arbeit‘, ‚die Reaktion‘) wird eine überschießende krankhafte Abwehrreaktion des Immunsystems auf bestimmte normalerweise harmlose Umweltstoffe (Allergene) bezeichnet.

Die allergische Reaktion richtet sich gegen von außen kommende Stoffe. Autoimmunreaktionen, also krankhafte Reaktionen des Immunsystems gegen Bestandteile des eigenen Körpers, werden nur dann zu den Allergien gezählt, wenn sie durch von außen in den Körper gelangende Stoffe und Partikel ausgelöst werden.

Neben den Allergien gibt es weitere Unverträglichkeitsreaktionen, z. B. die Pseudoallergie oder die Intoleranz, die mit einem ähnlichen Krankheitsbild wie eine Allergie einhergehen können. Da eine Allergie, eine Pseudoallergie oder eine Intoleranz ähnliche Symptome verursachen können, werden diese Begriffe im allgemeinen Sprachgebrauch undifferenziert und fälschlicherweise oft synonym verwendet.

Allergische Erkrankungen sind wahrscheinlich älter als die Menschheit. Schon aus dem alten Ägypten und aus dem alten Rom sind Krankheitsbeschreibungen bekannt, die man heute als Allergie bezeichnen würde.

Seine Beobachtung, dass manche Menschen Schnupfen und Atemwegsverengungen zeigen, wenn sie sich in der Nähe blühender Rosen aufhalten, bezeichnete der italienische Chirurg Leonardo Botallo 1565 als „Rosenerkältung“. Eine von der Jahreszeit abhängige Nasenerkrankung, beschrieb 1819 der Londoner Arzt John Bostock. Dass Gräserpollen die auslösende Ursache für diesen „Heuschnupfen“ sind, erkannten 1870 Charles Blackley in England und unabhängig von diesem Morrill Wyman an der Harvard University. 1903 hatte der Deutsche Wilhelm P. Dunbar bei Versuchspersonen Heuschnupfensymptome durch mit Pollen versetzte Salzlösungen ausgelöst.

Der Begriff "Allergie" wurde 1906 von Clemens von Pirquet, einem Wiener Kinderarzt, in Analogie zu "Energie" geprägt in der Hinsicht, dass . Pirquet definierte Allergie weit gefasst als . In dieser Definition sind sowohl verstärkte (Hyperergie), verminderte (Hypoergie) wie auch fehlende (Anergie) Reaktivitäten einbezogen. Pirquet erkannte als erster, dass Antikörper nicht nur schützende Immunantworten vermitteln, sondern auch Überempfindlichkeitsreaktionen auslösen können.

Allergien sind häufige Erkrankungen. Hierbei nehmen die Inhalationsallergien wie Heuschnupfen eine besonders prominente Stellung ein.

In Deutschland, zu Beginn der 1990er Jahre, gaben 9,6 % der Befragten beim Nationalen Untersuchungssurvey an, dass sie schon einmal Heuschnupfen hatten. Es gab in den alten Bundesländern einen deutlich höheren Anteil Betroffener (10,6 %) als in den neuen Bundesländern (5,8 %). Zwischen Männern und Frauen war jeweils kaum ein Unterschied zu verzeichnen.

Ende der 1990er Jahre beim Bundes-Gesundheitssurvey (BGS98) waren 14,5 % der Bevölkerung (15,4 % der Frauen und 13,5 % der Männer) betroffen. Die Verbreitung war sowohl in den alten als auch in den neuen Bundesländern deutlich gewachsen. Bei den Frauen fiel diese Zunahme jeweils größer aus, sodass sich bis 1998 ein geschlechtsspezifischer Unterschied herausgebildet hatte.

Weitere 10 Jahre später, beim Untersuchungs- und Befragungssurvey DEGS1, der von 2008 bis 2011 durchgeführt wurde, hatten sich die Zahlen auf diesem hohen Niveau stabilisiert (14,8 % gesamt, 16,5 % der Frauen und 13,0 % der Männer).

Dass sich zwischen Anfang und Ende der 1990er Jahre nicht lediglich das Antwortverhalten der Befragten verändert hat, sondern es sich um einen tatsächlichen Anstieg der Heuschnupfenhäufigkeit handelte, konnte durch vergleichende Analysen und durch Laboruntersuchungen herausgefunden werden. Auf der Basis von allergen-spezifischen IgE-Tests wurde stichprobenartig bei den Gesundheitssurveys die Sensibilisierung auf Inhalationsallergene überprüft.

Im Nationalen Untersuchungssurvey 1990–1992 lag die Rate der Sensibilisierungen auf Inhalationsallergene – genau wie die Heuschnupfenprävalenz – in den alten Bundesländern (27,4 %) höher als in den neuen Bundesländern (24,1 %). Die Gesamtrate betrug 26,7 %. Bis zum Ende der 1990er Jahre kam es gemäß Bundes-Gesundheitssurvey (BGS98) zu einem deutschlandweiten Anstieg der Sensibilisierungsrate auf 31,2 %. Diese Zunahme war etwas weniger ausgeprägt als die beim selbst berichteten Heuschnupfen. Der Anstieg in West (auf 31,9 %) und Ost (auf 28,5 %) verlief ähnlich.

Eine befriedigende Erklärung für die Zunahme allergischer Erkrankungen in den letzten Jahrzehnten gibt es – wie auch für die Zunahme der Autoimmunerkrankungen – bisher nicht, wohl aber einige Thesen:

Einige Forscher führen den beobachteten Anstieg allergischer Erkrankungen in westlichen Industrieländern auf die sogenannte „Dreck- und Urwaldhypothese“ zurück. Diese geht von einer mangelnden Aktivierung („Unterforderung“) des Immunsystems – vor allem in der Kindheit und frühen Jugend – durch übertriebene Hygienemaßnahmen aus. Es wird vermutet, dass der Kontakt mit bestimmten Bakterien insbesondere in den ersten Lebensmonaten wichtig ist, um das Immunsystem, das während der Schwangerschaft eher Typ2-T-Helferzellen-lastig ist, wieder in Richtung einer Typ1-T-Helferzellen-Antwort zu lenken, die weniger mit allergischen Reaktionen assoziiert ist. Eine prominente Studie zum Thema ist die ALEX-Studie.

Die physiologische Funktion von IgE-Antikörpern, die bei Allergien eine wesentliche Rolle spielen, ist die Abwehr von Wurm- und anderem Parasitenbefall. Der Rückgang parasitärer Erkrankungen könnte zu einer Umlenkung des Immunsystems auf andere, harmlose Strukturen führen. Hierfür spricht das geringere Aufkommen von Allergien in Ländern mit geringeren Hygienestandards. Da in den westlichen Industrienationen Parasitenbefall so gut wie nicht mehr vorkommt, bei allergischen Reaktionen aber eine verstärkte IgE-Antikörper-Bildung vorliegt, wird geprüft, ob hier ein Zusammenhang bestehen könnte. Eine Studie an 1600 Kindern in Vietnam zeigte, dass Kinder mit intestinalem Wurmbefall im Vergleich zu Kindern ohne Wurmbefall eine um sechzig Prozent verringerte Chance einer Allergie gegen Hausstaubmilben hatten. Jedoch gibt es derzeit widersprüchliche Forschungsergebnisse, so dass diese Hypothese noch nicht abschließend beurteilt werden kann.

Allergene wie das Hauptallergen der Birke, Bet v 1, können sich an Dieselrußpartikel (auch Feinstaub) anheften und so beim Einatmen unter Umständen in tiefere Lungenabschnitte gelangen. Es ist möglich, dass die Dieselrußpartikel als „Träger“ der Allergene auch eine adjuvante (unterstützende) Wirkung haben und somit eine Sensibilisierung fördern.

Die Umweltverschmutzung sorgt auch bei Haselsträuchern für Stress und verändert die Eiweißbildung derart, dass die betroffenen Menschen immer heftiger darauf reagieren.

Wissenschaftler des Helmholtz-Zentrums in München haben herausgefunden, dass sich die allergischen Reaktionen des Beifußblättrigen Traubenkrauts (Ambrosia artemisiifolia) verstärken, wenn sie mit Stickstoffdioxid in Verbindung treten. Dadurch erhöht sich die Anzahl der Allergene und macht sie aggressiver. Die Pollen der Ambrosia zählen zu den stärksten Allergieauslösern.

Ein Zusammenhang zwischen Allergien und Impfungen konnte nicht nachgewiesen werden. In der DDR war die Durchimpfungsrate deutlich höher (nahe 100 %), die Allergieraten hingegen niedriger als in der BRD (bis 1989). Neu in der Diskussion sind Studien zur frühen Vitamin-D-Prophylaxe
, zu Paracetamol und zur Antibiotikatherapie.

Diese Überlegung bezieht sich darauf, dass aufgrund einer erhöhten Allergenexposition vermehrt Sensibilisierungen stattfinden könnten. Ursachen für eine erhöhte Exposition könnten sein: die Zunahme des Pollenflugs infolge einer Stressreaktion von Bäumen auf die Erderwärmung oder Schadstoffbelastung, die Zunahme der Milbenexposition durch verbesserte Isolierung der Häuser, der vermehrte Konsum exotischer Lebensmittel wie Kiwi.

Veränderungen in der kommensalen Flora könnten ebenfalls das Immunsystem beeinflussen und im Zusammenhang mit dem vermehrten Auftreten von Allergien stehen. Veränderungen in der Darmflora können durch den Einsatz von Antibiotika und durch moderne Ernährungsgewohnheiten ausgelöst werden. Die Bakterienflora der Haut könnte durch die Einführung von Windeln verändert worden sein.
Es wird diskutiert, ob Probiotika einen günstigen Effekt auf die Entwicklung von Allergien haben könnten.

Es gibt etliche weitere Faktoren, von denen ebenfalls vermutet wird, dass sie die Entstehung allergischer Erkrankungen begünstigen können. Dies sind Rauchen, Autoabgase, Stress, kleinere Familien, veränderte Ernährung, aber auch ein veränderter individueller Lebensstil, der sich auf die Entwicklung von Atopie und Allergien auswirken könnte, wie die kürzere Stillzeit junger Mütter und ein dadurch bedingtes höheres Allergierisiko des Kindes. Kinder von Frauen, die während der Schwangerschaft Kontakt zu Tieren, Getreide oder Heu hatten, bekommen im späteren Leben seltener allergische Atemwegs- und Hauterkrankungen. Für einen optimalen Schutz ist aber ein anhaltender Kontakt zu Nutztieren oder Getreide nötig.

Die Ursachen von Allergien kann man in genetische und nicht genetische Faktoren unterteilen.

Zu den genetischen Faktoren gehören: Disposition zur überschießenden Bildung von Gesamt-IgE und allergenspezifischen IgE-Antikörpern, sowie deren Fixierung besonders an Mastzellen und basophilen Granulozyten von Haut und Schleimhäuten (Atopie). Zu den genetischen Faktoren gehört auch eine verminderte Aktivität von Regulatorischen T-Zellen, deren Aufgabe es ist, die Aktivierung des Immunsystems zu begrenzen und dadurch die Selbsttoleranz des Immunsystems zu regulieren. Die allergische Reaktionsbereitschaft ist mit den HLA-Genen assoziiert.

Eindeutig belegt ist ein erhöhtes Allergierisiko für Kinder, bei denen entweder ein oder beide Elternteile Allergiker sind. Offensichtlich spielen aber mehrere genetische Faktoren zusammen, es gibt also nicht das eine „Allergie-Gen“. Es gibt eine Vielzahl von Kandidatengenen, die möglicherweise oder wahrscheinlich an der Entstehung allergischer Erkrankungen beteiligt sind. Auch scheinen unterschiedliche allergische Veranlagungen (z. B. Allergisches Asthma, Atopische Dermatitis) unterschiedlich genetisch determiniert zu sein.

Ursache einer Allergie kann auch eine gestörte Barrierefunktion und eine damit verbundene erhöhte Durchlässigkeit von Haut und Schleimhaut sein, z. B. durch bakterielle oder virale Infekte oder durch chemische Irritation.

Auch eine verstärkte Allergenexposition kann bei entsprechender Veranlagung zu Allergien führen. Diese Form der Allergie spielt besonders bei berufsbedingten Allergien eine Rolle.

Körperlicher oder psycho-sozialer Stress ist nicht Ursache einer Allergie. Stress beeinflusst aber das Immunsystem. Körperlicher und/oder psycho-sozialer Stress kann deshalb eine bestehende Allergie verstärken oder aber bei einer bestehenden Sensibilisierung Auslöser für die allergische Erkrankung sein.

Auslöser von Allergien sind Allergene. Allergene sind Antigene, also Substanzen, die vom Körper als fremd erkannt werden und eine spezifische Immunantwort auslösen. Diese normale körperliche Reaktion ist bei der Allergie fehlgeleitet, sodass eigentlich harmlose Antigene zu allergieauslösenden Allergenen werden. Es gibt eine Vielzahl von Allergenen. Meistens sind Allergene Polypeptide oder Proteine.

Allergene können nach unterschiedlichen Gesichtspunkten eingeteilt werden:

Allergene können vom Körper durch Inhalation, durch Ingestion, durch Hautkontakt oder durch Injektion (darunter fallen auch Insektenstiche), aufgenommen werden.

Allergien gegen Wasser und Zucker sind per Definition nicht möglich, da einer Allergie eine unangemessene Immunantwort auf ein Allergen zu Grunde liegt. Wasser und Zucker sind aber nicht immunogen und daher auch nicht „allergisierend“. Eine Erkrankung, die gelegentlich als Wasserallergie bezeichnet wird, ist die extrem seltene "aquagene Urtikaria" (Wassernesselsucht). Als Wasserallergie wird hin und wieder auch eine Immunantwort auf im Leitungswasser gelöste Stoffe bezeichnet.

Eine Allergie setzt eine Sensibilisierung voraus. Unter Sensibilisierung versteht man den 1. Kontakt mit dem Allergen und der für dieses Allergen spezifischen Immunantwort des Körpers. Diese Sensibilisierung verursacht keine Krankheitssymptome, kann aber im Blut nachgewiesen werden. Erst bei einem erneuten Kontakt mit dem Allergen nach Abschluss der Sensibilisierungsphase (5 Tage bis mehrere Jahre) treten bei Allergikern die allergischen Krankheitssymptome auf.

Die beste Prophylaxe gegen eine Allergie ist die Vermeidung der Sensibilisierung. Das vollständige Vermeiden von sämtlichen Allergenen ist unmöglich. Jedoch ist in bestimmten Fällen die Vermeidung bzw. Verringerung der Belastung mit potentiellen Allergenen möglich und sinnvoll:

Kinder, die mit offenem Rücken (Spina bifida) geboren werden, haben ein sehr hohes Risiko einer Sensibilisierung gegen Latex. Es ist daher heute klinischer Standard, diese Kinder von Geburt an vor jedem Kontakt mit Latex (beispielsweise bei Latex-OP-Handschuhen) zu schützen.

Die optimale Ernährung für Neugeborene ist das ausschließliche Stillen während mindestens der ersten 4 Lebensmonate. Es gibt retrospektive Studien, die beobachtet haben, dass gestillte Kinder seltener an Allergien leiden als nicht gestillte.

Es gibt auch Studien dazu, dass Haushunde und auch Hauskatzen vor Allergien schützen können.<ref name="DOI10.1016/j.jaci.2013.04.009">C. Pelucchi, C. Galeone u. a.: "Pet exposure and risk of atopic dermatitis at the pediatric age: a meta-analysis of birth cohort studies." In: "The Journal of allergy and clinical immunology." Band 132, Nummer 3, September 2013, S. 616–622.e7, . . PMID 23711545.</ref><ref name="DOI10.1097/NNE.0b013e31826f283d">R. E. Pattillo: "Keep the family dog." In: "Nurse educator." Band 37, Nummer 6, 2012 Nov-Dec, S. 227, . . PMID 23086057.</ref><ref name="DOI10.1007/s11882-012-0277-0">J. Smallwood, D. Ownby: "Exposure to dog allergens and subsequent allergic sensitization: an updated review." In: "Current allergy and asthma reports." Band 12, Nummer 5, Oktober 2012, S. 424–428, . . PMID 22684981. (Review).</ref>
<ref name="DOI10.1007/s11882-012-0288-x">S. C. Dharmage, C. L. Lodge u. a.: "Exposure to cats: update on risks for sensitization and allergic diseases." In: "Current allergy and asthma reports." Band 12, Nummer 5, Oktober 2012, S. 413–423, . . PMID 22878928. (Review).</ref> Diese sammeln im Freien Allergene ein, die dann später zu Hause an das Kind abgegeben werden. Dessen Immunsystem wird dann dazu trainiert, die Fremdkörper zwar zu erkennen, diese aber als harmlos einzustufen. Zumindest in einer tierexperimentellen Studie an Mäusen hat dies funktioniert.<ref name="DOI10.1073/pnas.1310750111">K. E. Fujimura, T. Demoor u. a.: "House dust exposure mediates gut microbiome Lactobacillus enrichment and airway immune defense against allergens and virus infection." In: "Proceedings of the National Academy of Sciences of the United States of America." Band 111, Nummer 2, Januar 2014, S. 805–810, . . PMID 24344318. .</ref>

Die exogen-allergische Alveolitis ist meist eine Berufskrankheit, die durch die Inhalation von bestimmten Stäuben (z. B. Mehl bei der Bäcker-Lunge) verursacht wird. Durch entsprechende Arbeitsschutz-Maßnahmen, wie das Tragen von Feinstaubmasken oder auch die Verwendung von Abzugshauben, kann der Allergenkontakt vermindert und die Mitarbeiter somit vor einer Sensibilisierung geschützt werden.

Das Risiko an einer Allergie zu erkranken, wird durch genetisch fixierte Prädisposition, durch die aktuelle Abwehrlage der Körpergrenzflächen, durch Häufigkeit und Intensität der Allergenexposition und durch die allergene Potenz der betreffenden Substanz bestimmt. Die Symptome einer Allergie können mild bis schwerwiegend und in einigen Fällen sogar akut lebensbedrohlich sein. Expositionsbedingt kann es sein, dass die Symptome nur saisonal auftreten, etwa zur Zeit des entsprechenden Pollenflugs, oder dass die Symptome ganzjährig auftreten, wie bei einer Allergie gegen Hausstaubmilbenkot.

Je nachdem, mit welchem Organ Allergene durch den Körper aufgenommen werden, entstehen bei der Allergie unterschiedliche Krankheitssymptome. Allergiker können an einer Krankheitsform leiden, aber auch an Mischformen.

Inhalationsallergien gehören zu den Typ-1-Allergien vom Soforttyp. Inhalationsallergene werden über die Atmungsorgane aufgenommen und/oder gelangen über die Schleimhäute von Nase und Augen in den Körper. Zu den Inhalationsallergenen gehören z. B. Allergene aus Pollen, Pilzsporen, tierischen Epithelien, Federstaub, Speichel, Schweiß, Urin und Kot, Milbenkot, Insektenschüppchen, Holz- und Mehlstaub, Formaldehyd und Harzen.

Inhalationsallerge lösen primär Atemwegssymptome aus, können sekundär aber auch Haut- und Darmsymptome sowie Kreislauf- und Nervenreaktionen auslösen. Typische allergische Erkrankungen durch Inhalationsallergene sind Allergische Rhinitis (Heuschnupfen), Konjunktivitis (Bindehautentzündung), Hustenreiz, bronchiale Hyperreaktivität, Asthma bronchiale.

Ingestionsallergene werden durch den Mund bzw. den Verdauungstrakt aufgenommen. Manche Ingestionsallergene werden erst im Laufe des Verdauungsprozesses freigesetzt und vom Körper aufgenommen. Die Symptome einer Allergie gegen Nahrungsmittel oder gegen oral aufgenommene Medikamente kann deshalb innerhalb weniger Minuten oder auch erst mehrere Stunden nach der Nahrungsaufnahme/ Medikamenteneinnahme auftreten, obwohl es sich bei der Nahrungsmittelallergie um eine Typ-I-Soforttyp-Allergie handelt. Die Arzneimittelallergie kann in Form eines Arzneimittelexanthems auch als Typ-IV-Spätreaktion auftreten.

Ingestionsallergene können bei entsprechend veranlagten und sensibilisierten Menschen primär Verstopfung, Brechdurchfall oder abdominale Koliken verursachen, über die Aufnahme der Allergene durch das Blut auch Haut- und/oder Atemwegssymptome.

Kontaktallergene werden über die Haut aufgenommen. Sie überwinden die Barrierefunktion der Haut. Kontaktallergene können sowohl eine Sofortreaktion der Haut auslösen z. B. Kontakturtikaria oder auch eine Spätreaktion (Typ-IV-Spättyp-Allergie), die erst nach 12 bis 72 Stunden eintritt, z. B. das allergische Kontaktekzem.

Injektionsallergene werden durch Injektion oder Infusion in den Körper eingebracht. Die Barrierefunktion von Haut und Schleimhaut wird dadurch umgangen. Zu den Injektionsallergenen gehören tierische Gifte (z. B. von Bienen, Wespen, Feuerameisen, Quallen, Seeanemonen, Feuerkorallen) und Medikamente (z. B. Penicillin).

Zu den typischen allergischen Reaktionen durch Injektionnsallergene gehören eine gesteigerte örtliche Reaktion und/oder anaphylaktische Reaktionen.

Unabhängig davon, mit welchem Organ Allergene vom Körper aufgenommen werden, kann eine Allergie auch systemische Reaktionen verursachen, die den gesamten Körper betreffen, z. B. Urtikaria und anaphylaktische Reaktionen.

Unter einer Kreuzallergie versteht man eine Sensibilisierung gegenüber biologisch oder chemisch verwandten Substanzen. Die Struktur dieser Substanzen ist teilweise identisch, so dass vom Immunsystem mehrere unterschiedliche Substanzen als Allergen erkannt werden können, obwohl eine Sensibilisierung nur gegen eine der Substanzen vorliegt. Beispielsweise können Allergiker gegen Birkenpollen auch auf Äpfel allergisch reagieren. Die allergische Reaktion kann bei der Kreuzallergie bereits beim Erstkontakt erfolgen, wenn es vorher eine Sensibilisierung mit einer ähnlichen Substanz gab.

Coombs und Gell haben 1963 als Erste Allergien nach ihrem pathophysiologischen Mechanismen in vier Typen eingeteilt, die sich überlappen können:

Die Typ-I- bis Typ-III-Allergien werden durch Antikörper vermittelt (humorale Allergie).

Die Typ-I-Allergie ist die häufigste Allergieform.

Bei der Typ-I-Allergie liegt eine Fehlfunktion der Regelung der IgE-Antikörper vor. IgE-Antikörper bewirken durch mehrere Mediatoren schon in geringen Mengen eine Erweiterung der Blutgefäße und steigern deren Durchlässigkeit für weiße Blutkörperchen. T-Zellen, die normalerweise die IgE-Aktivität auf ein vernünftiges Maß einschränken, fehlen bei der Typ-I-Allergie oder sind zu wenig aktiv. Bei der Typ-I-Allergie werden durch die Vermittlung von IgE-Antikörpern Entzündungsmediatoren, z. B. Histamin, Leukotriene, Prostaglandine, Kallikrein, aus Basophilen Granulozyten und Mastzellen freigesetzt. Dadurch wird eine Entzündung von Haut, Schleimhaut oder eine systemische Entzündung hervorgerufen.

Die allergische Reaktion bei der der Typ-I-Allergie erfolgt innerhalb von Sekunden bis Minuten. Eventuell ist eine zweite Reaktion nach 4 bis 6 Stunden möglich (verzögerte Sofortreaktion). Diese zweite Reaktion darf nicht mit der Spättypreaktion der Typ-IV-Allergie verwechselt werden.

Typische Krankheiten der Typ-I-Allergie:

Bei der Typ-II-Allergie kommt es zur Bildung von Immunkomplexen aus membranständigen Antigenen (z. B. Medikamenten, Blutgruppenantigenen) mit zirkulierenden IgG- oder IgM-Antikörpern. Dadurch werden das Komplementsystem oder zytotoxische Killerzellen aktiviert und es kommt zur Zytolyse (Zerstörung) körpereigener Zellen.

Die allergische Reaktion bei der Typ-II-Allergie erfolgt nach 6 bis 12 Stunden.

Typische Krankheiten für die Typ-II-Allergie:

Bei der Typ-III-Allergie werden Immunkomplexe aus präzipitierenden IgG- und IgM-Antikörpern und Allergenen gebildet. Dadurch werden Komplementfaktoren aktiviert, insbesondere C3a und C5a. Diese speziellen Teile des Komplementsystems führen zur Phagozytose (aktiven Aufnahme) der Immunkomplexe durch Granulozyten unter Freisetzung gewebeschädigender Enzyme, z. B. Elastase, Kollagenase, Myeloperoxidase.

Die allergische Reaktion bei der Typ-III-Allergie erfolgt nach 6 bis 12 Stunden.

Typische Krankheiten für die Typ-III-Allergie:

Die Typ-IV-Allergie wird durch spezifisch sensibilisierte T-Zellen vermittelt (zellvermittelte Allergie).

Die Typ-IV-Allergie ist nach der Typ-I-Allergie die häufigste Allergieform.

Bei der Typ-IV-Allergie werden Lymphokine aus spezifisch sensibilisierten T-Lymphozyten freigesetzt. Diese Lymphokine bewirken die Aktivierung bzw. Vermehrung von Makrophagen und mononukleären Zellen sowie deren Wanderung an den Ort der Allergenbelastung. Dadurch erfolgt eine lokale Infiltration und Entzündung.

Die allergische Reaktion bei der Typ-IV-Allergie erfolgt nach 12 bis 72 Stunden.

Typische Krankheiten der Typ-IV-Allergie:

Auch ein positiver Allergietest ist allein kein Nachweis für eine Allergie. Die Diagnose Allergie kann nur im Zusammenhang mit dem Allergietest und den klinischen Beschwerden gestellt werden. Durch den Hauttest und den Bluttest wird lediglich die Sensibilisierung gegen eine bestimmte Substanz nachgewiesen. Diese Testungen sagen wenig darüber aus, ob überhaupt Beschwerden bestehen oder über die Art oder Schwere der Beschwerden. Mit den Provokationstests werden eine Unverträglichkeit und das Beschwerdebild dieser Unverträglichkeit nachgewiesen, aber nicht, ob es sich bei dieser Unverträglichkeit tatsächlich um eine Allergie handelt.

Hauttests werden als Standarduntersuchungen vorgenommen, wenn der Verdacht besteht, dass ein Patient allergisch auf eine Substanz reagiert. Beim Hauttest werden Allergenextrakte bzw. allergenhaltiges Material auf verschiedene Weisen mit der Haut in Kontakt gebracht. Sensibilisierte Betroffene zeigen nach definierten Zeiten lokale Reaktionen vom Sofort-Typ oder Spät-Typ. An ihnen kann abgelesen werden, gegen welche Allergene oder Allergenquellen der Patient sensibilisiert ist. Dieser Test kann unter Umständen auch Hinweise auf den Schweregrad der allergischen Reaktion geben.


Bei Provokationstests wird das vermutete Allergen dem Patienten nicht über die Haut, sondern in anderer Form zugeführt. Der wesentliche Vorteil der Provokationstests liegt darin, dass eine Beschwerde-Auslösung nachgewiesen werden kann und nicht nur wie beim Bluttest mittels Nachweis von IgE-Antikörpern eine Sensibilisierung. Da bei Provokationstests unerwartet heftige Krankheitszeichen bis zum lebensbedrohlichen anaphylaktischen Schock auftreten können, sollten sie nur von einem allergologisch erfahrenen Arzt durchgeführt werden, der erforderlichenfalls auch die entsprechenden Notfallmaßnahmen durchführen kann.

Bei allergischer Rhinoconjunctivitis (Heuschnupfen) kann zur Provokation ein Allergenextrakt in die Nase gesprüht werden und anschließend die allergische Reaktion gemessen werden, indem die Schwellung der Nasenschleimhaut mittels einer sogenannten Rhinomanometrie oder der Tryptase-Spiegel im Blut gemessen wird.

Bei allergischem Asthma erfolgt die Provokation durch die Inhalation eines Allergenextrakts mit anschließender Erfassung der allergischen Reaktion mit einer Lungenfunktionsprüfung. Da Asthma meist mit einer Bronchialen Hyperregibilität einhergeht, kann auch unspezifisch mit ansteigenden Konzentrationen einer Methacholin-Lösung provoziert werden (Methacholintest).

Bei schweren Nahrungsmittelallergien kann die "double blind placebo controlled food challenge" (Doppelblinde plazebokontrollierte orale Nahrungsmittelprovokation (DBPCFC)) angewendet werden. Dabei werden einer hypoallergenen Grundnahrung nach und nach verschiedene Nahrungsmittel so zugefügt, dass weder der Patient noch der Arzt das Nahrungsmittel erkennen kann. Dabei wird die Verträglichkeit beobachtet. So kann festgestellt werden, welche Nahrungsmittel allergische Reaktionen auslösen, und es können andersherum auch Nahrungsmittel identifiziert werden, die gefahrlos konsumiert werden können. Dieses Verfahren ist allerdings sehr zeitaufwändig und kann i. d. R. nur stationär durchgeführt werden.

In Blutproben können IgE-Antikörper gemessen werden. Zum einen kann der Gesamt-IgE-Spiegel gemessen werden, der alle freien IgE-Antikörper erfasst. Dieser Wert ermöglicht eine Aussage darüber, ob generell vermehrt IgE-Antikörper gebildet werden. Erhöhte Gesamt-IgE-Werte kommen aber nicht nur bei allergischen Erkrankungen vor, sondern auch bei Parasitenbefall und bestimmten hämatologischen Erkrankungen.

Zum anderen können auch allergenspezifische IgE-Antikörper nachgewiesen werden. Hierbei werden also die IgE-Spiegel ermittelt, die sich konkret gegen eine Allergenquelle richten.

Die quantitative Messung von IgE-Antikörpern im Blut korreliert jedoch nur schlecht mit dem klinischen Bild. Das heißt, die Messung von IgE-Antikörpern im Blut erlaubt eine Aussage über die Sensibilisierungen eines Allergikers, aber nur bedingt eine Einschätzung der Schwere der Symptome und gar keine Aussage über die Art der Symptome. Es kann auch sein, dass Allergen-spezifische IgE-Antikörper trotz Sensibilisierung nicht nachgewiesen werden können.

Ein weiterer Parameter, der in Blutproben gemessen werden kann, ist das eosinophile kationische Protein (ECP). ECP wird von aktivierten Eosinophilen ausgeschüttet. ECP ist ein Entzündungsparameter und wird zur Verlaufskontrolle bei allergischem Asthma oder bei atopischer Dermatitis bestimmt.

"Tryptase" kann ebenfalls in Blutproben nachgewiesen werden. Tryptase wird von aktivierten Mastzellen ausgeschüttet und ist ein für aktivierte Mastzellen hochspezifischer Parameter. Der Tryptase-Spiegel wird auch bestimmt zur Diagnostik beim anaphylaktischen Schock, zur postmortalen Diagnose beim Asthmatod, zur Diagnostik der Mastozytose und bei der Provokationstestung bei allergischer Rhinitis.

Durch einen Lymphozytentransformationstest (LTT) kann die Bestimmung sensibilisierter Lymphozyten nachgewiesen und quantifiziert werden. Dies kann bei bestimmten Typ-IV-(Spät-)Allergien sinnvoll sein.

Die Allergenkarenz, d. h. die Allergenvermeidung, ist bei sensibilisierten Personen die optimale Therapie, um eine Allergie zu vermeiden, da eine Allergie nur bei einem Kontakt mit dem entsprechenden Allergen auftreten kann. Eine fortgesetzte Allergenbelastung steigert die Immunantwort auf das Allergen, während eine dauerhafte Allergenkarenz die Sensibilisierung zwar nicht aufhebt, die spezifische Immunantwort aber abschwächt. Wenn die strikte Vermeidung eines Allergens nicht möglich ist, sollte eine möglichst weitgehende Verringerung der Allergenbelastung erfolgen, da eine Allergie auch von der Intensität der Allergenbelastung abhängt.

Bestimmte Produkte, wie milbendichte Matratzenbezüge bei der Hausstaubmilbenallergie oder Pollenfilter in Klimaanlagen bei der Pollenallergie, helfen, den Allergenkontakt zu reduzieren. Auch wenn bei der Tierhaarallergie ein Verzicht auf Haustiere den Allergenkontakt stark reduziert, so sind Tierhaarallergene sehr stabil, werden verschleppt und können auch an Orten wie Schulen nachgewiesen werden, an denen normalerweise keine Tiere gehalten werden. Nahrungsmittelallergene hingegen können meistens sehr gut vermieden werden.

Die meisten Allergien werden mit Medikamenten behandelt, die das Auftreten von allergischen Symptomen mildern oder verhindern, aber keine Heilung von der allergischen Erkrankung bewirken können. Diese Antiallergika werden je nach Krankheitsform und Schwere der Erkrankung in unterschiedlichen Darreichungsformen (Tabletten, Nasensprays, Asthmasprays, Augentropfen, Cremes, Salben und Injektionen) und in unterschiedlichen Intervallen (bei akutem Bedarf, prophylaktisch, dauerhaft) angewendet.

Eingesetzte Wirkstoffe sind

Patienten, bei denen bekannt ist, dass sie Gefahr laufen, einen anaphylaktischen Schock zu erleiden (z. B. bei Insektengiftallergien), wird ein Notfallset mit Antihistaminikum, Glukokortikoid, eventuell einem Inhalationspräparat und einem Autoinjektor mit Adrenalin verschrieben (Adrenalin-Pen), welches sie stets bei sich tragen sollten.

Die Hyposensibilisierung, auch "Spezifische Immuntherapie (SIT)" oder "Desensibilisierung", ist bislang die einzige verfügbare kausale Therapie bei Typ-I-Allergien. Bei der Hyposensibilisierung wird die allergenspezifische IgE-vermittelte Reaktionsbereitschaft des Immunsystems (Allergie vom Soforttyp, Typ-I-Allergie) herabgesetzt durch regelmäßige Zufuhr des Allergens über einen längeren Zeitraum in unterschwelligen, langsam ansteigenden Konzentrationen. Das Allergen oder das modifizierte Allergen (Allergoid) werden entweder unter die Haut gespritzt (subkutane Immuntherapie (SCIT)) oder als Tropfen oder Tabletten sublingual (sublinguale Immuntherapie (SLIT)) aufgenommen.

Voraussetzung für eine erfolgreiche Hyposensibilisierung ist die Bereitschaft und Fähigkeit des Allergikers, die Therapie über einen Zeitraum von drei Jahren, sowie die anschließende Erhaltungstherapie, regelmäßig durchzuführen. Die Indikation für eine Hyposensibilisierung besteht für Menschen ab 5 Jahre, wenn das verursachende Allergen nicht gemieden werden kann, die Wirkung der Hyposensibilisierung für die behandelnde Erkrankung belegt ist und ein geeigneter Allergenextrakt verfügbar ist. Die Wirksamkeit der Hyposensibilisierung ist durch mehrere Studien für Rhinokonjunktivitis bei Pollenallergie, für das allergische Asthma bronchiale, für die Hausstaubmilbenallergie, für die Schimmelpilzallergie, für die Tierhaarallergie und für die Insektengiftallergie belegt.

Auch konnte durch entsprechende Studien für einige Produkte nachgewiesen werden, dass durch die Hyposensibilisierung das Asthmarisiko verringert und die Neusensibilisierung auf weitere Allergene reduziert wird. Aus diesem Grund sollte die Hyposensibilisierung bei Kindern und Jugendlichen frühzeitig erfolgen und solche Produkte gewählt werden, für die dieser Effekt nachgewiesen wurde.

Das Immunsystem von Kindern ist noch nicht voll ausgereift. Kinder haben deshalb eine erhöhte Allergieneigung. Auch und gerade bei Kindern kann deshalb im Verlauf der Erkrankung eine Allergieform durch eine andere ersetzt werden oder zu einer Allergie eine weitere hinzutreten. Bei konsequenter Meidung des auslösenden Allergens verschwindet eine Nahrungsmittelallergie mit Reifung des Immunsystems meistens bis zum 5. Lebensjahr, vor allem die Kuhmilch- und die Hühnereiallergie. Andere Nahrungsmittelallergien, z. B. die Erdnussallergie, haben allerdings nur eine geringe Besserungstendenz.

Besonders bei chronischem Verlauf der Typ-I-Allergie erhöht sich die Reaktionsbereitschaft von Mastzellen, Monozyten, sowie von basophilen und eosinophilen Granulozyten. Dadurch können die Symptome einer bestehenden Allergie verstärkt werden und/oder neue Allergien hinzutreten.

Unter einem Etagenwechsel versteht man bei der Inhalationsallergie das Übergreifen IgE-vermittelter Allergiesymptome (Typ-1-Sofort-Allergie) von den Konjunktiven (Bindehaut des Auges) auf die Nasen- und Bronchialschleimhaut oder von den oberen Atemwegen auf die unteren Atemwege, ein Heuschnupfen wird zum allergischen Asthma. Auch das Hinzutreten weiterer Inhalationsallergien und/oder das Auftreten von Kreuzallergien wird als Etagenwechsel bezeichnet.

Unbehandelt führen 30-40 % aller Allergien gegen Inhalationsallergene zu einem Etagenwechsel.

Es gibt Krankheiten, deren Symptome einer Allergie gleichen, die jedoch nicht immunologisch bedingt sind. Diese Krankheiten werden als Pseudoallergie oder Intoleranz bezeichnet.

Bei der Pseudoallergie werden die allergieähnlichen Symptome ausgelöst, indem Mastzellen unspezifisch aktiviert werden. Wenn Mastzellen aktiviert werden und degranulieren, dann setzen sie eine Reihe von Entzündungsmediatoren (z. B. Histamin) frei. Es entsteht eine Entzündungsreaktion, die sich in allergieähnlichen Symptomen äußert.

Während bei Allergien die Aktivierung der Mastzellen spezifisch erfolgt, nämlich dadurch, dass bestimmte Allergene an oberflächlich gebundene Antikörper binden können, so erfolgt die Mastzell-Aktivierung bei Pseudoallergien unspezifisch, also ohne Beteiligung der oberflächlich gebundenen Antikörper.

Abzugrenzen von der Pseudoallergie und der Allergie ist die Intoleranz, die ebenfalls allergieähnliche Symptome verursachen kann. Bei der Intoleranz handelt es sich um eine Stoffwechselstörung. Der Körper kann bestimmte Substanzen nicht oder nicht ausreichend verstoffwechseln, meistens aufgrund eines Enzymdefektes.

Die Ausbildung zum Allergologen ist eine Zusatzausbildung für Fachärzte. Dieser Facharzt ist also nur für Allergien in seinem Fachbereich zuständig. Für die Hauttestungen ist aber immer der Dermatologe mit der Zusatzausbildung Allergologie zuständig. Für die mit ähnlichen Symptomen auftretenden Pseudoallergien und Intoleranzen gibt es keine speziellen Fachärzte.

Da sich beim Allergiker aber die Symptome nur in den seltensten Fällen auf ein Organ beschränken, der Kranke selbst gar nicht erkennen kann, ob seine Symptome von einer Allergie, einer Pseudoallergie oder einer Intoleranz herrühren und welche spezielle Diagnostik er benötigt, ist die Diagnose von Unverträglichkeiten oft langwierig und schwierig, da man für die Diagnose oft mehrere Ärzte aufsuchen muss.




</doc>
<doc id="259" url="https://de.wikipedia.org/wiki?curid=259" title="Aminosäuren">
Aminosäuren

Aminosäuren (AS), unüblich auch "Aminocarbonsäuren", veraltet "Amidosäuren" genannt, sind chemische Verbindungen mit einer Aminogruppe und einer Carbonsäuregruppe. Sie sind die Bausteine von Proteinen und dienen vor allem dem Aufbau von Körpergewebe. Essentielle Aminosäuren kann ein Organismus nicht selber herstellen, sie müssen daher mit der Nahrung aufgenommen werden. Aminosäuren kommen in allen Lebewesen vor.

Zur Klasse der Aminosäuren zählen organische Verbindungen, die zumindest eine Aminogruppe (–NH bzw. substituiert –NR) und eine Carboxygruppe (–COOH) als funktionelle Gruppen enthalten, also Strukturmerkmale der Amine und der Carbonsäuren aufweisen. Chemisch lassen sie sich nach der Stellung ihrer Aminogruppe zur Carboxygruppe unterscheiden – steht die Aminogruppe am C-Atom unmittelbar benachbart zur endständigen Carboxygruppe, nennt man dies "α-ständig" und spricht von "α-Aminosäuren".

Ausgewählte α-Aminosäuren sind die natürlichen Bausteine von Proteinen. Sie werden miteinander zu Ketten verknüpft, indem die Carboxygruppe der einen Aminosäure mit der Aminogruppe der nächsten eine Peptidbindung eingeht. Die auf diese Weise zu einem Polymer verketteten Aminosäuren unterscheiden sich in ihren Seitenketten und bestimmen zusammen die Form, mit der das Polypeptid im wässrigen Milieu dann zum nativen Protein auffaltet. Diese Biosynthese von Proteinen findet in allen Zellen an den Ribosomen statt nach Vorgabe genetischer Information, die in Form von mRNA vorliegt. Deren Basensequenz gibt in bestimmten Abschnitten die Aminosäurensequenz an, wobei jeweils ein Basentriplett ein Codon darstellt, das für eine bestimmte Aminosäure stehen kann. Die hiermit als Bausteine für die Bildung von Proteinen codierten Aminosäuren werden auch als proteinogene Aminosäuren bezeichnet, beim Menschen sind es 21 verschiedene. Nach der Translation können die Seitenketten einiger im Protein eingebauter Aminosäuren noch modifiziert werden.

Das Spektrum der Aminosäuren geht allerdings über diese rund zwanzig proteinogenen weit hinaus. So sind bisher über 400 "nichtproteinogene" natürlich vorkommende Aminosäuren bekannt, die biologische Funktionen haben. Die vergleichsweise seltenen -Aminosäuren stellen hierbei eine spezielle Gruppe dar. Die Anzahl der synthetisch erzeugten und die der theoretisch möglichen Aminosäuren ist noch erheblich größer.

Einige Aminosäuren spielen als Neurotransmitter eine besondere Rolle, ebenso verschiedene Abbauprodukte von Aminosäuren; biogene Amine treten nicht nur als Botenstoffe im Nervensystem auf, sondern entfalten auch als Hormone und Gewebsmediatoren vielfältige physiologische Wirkungen im Organismus.

Die einfachste Aminosäure, Glycin, konnte nicht nur auf der Erde, sondern auch auf Kometen, Meteoriten und in Gaswolken im interstellaren Raum nachgewiesen werden.

Die erste Aminosäure wurde 1805 im Pariser Labor von Louis-Nicolas Vauquelin und dessen Schüler Pierre Jean Robiquet aus dem Saft von Spargel ("Asparagus officinalis") isoliert und danach Asparagin genannt. Als letzte der üblichen proteinaufbauenden Aminosäuren wurde das Threonin 1931 im Fibrin entdeckt sowie 1935 seiner Struktur nach geklärt von William Rose. Rose hatte durch Experimente mit verschiedenen Futtermitteln herausgefunden, dass die bis dato entdeckten 19 Aminosäuren als Zusatz nicht ausreichten. Er stellte auch die Essentialität anderer Aminosäuren fest und ermittelte je die für ein optimales Wachstum mindestens erforderliche Tagesdosis.

In der Zeit zwischen 1805 und 1935 waren viele der damals bekannten Chemiker und Pharmazeuten daran beteiligt, Aminosäuren erstmals zu isolieren sowie deren Struktur aufzuklären. So gelang Emil Fischer, auf den auch die Fischer-Projektion zurückgeht, die finale Aufklärung der Struktur von Serin (1901), Lysin (1902), Valin (1906) und Cystein (1908). Auch Albrecht Kossel (1896 Histidin aus Störsperma), Richard Willstätter (1900 Prolin via Synthese) und Frederick Hopkins (1901 Tryptophan aus Casein) wurden später Nobelpreisträger. Der deutsche Chemiker Ernst Schulze isolierte drei Aminosäuren erstmals – 1877 Glutamin aus Rüben, 1881 Phenylalanin und 1886 Arginin aus Lupinen – und war an der Strukturaufklärung weiterer Aminosäuren beteiligt. Zuvor hatte Heinrich Ritthausen 1866 Glutaminsäure aus Getreideeiweiß, dem Gluten, kristallin gewonnen. Wilhelm Dittmar klärte 1872 die Struktur von Glutamin und Glutaminsäure, deren Salze Glutamate sind.

Bereits 1810 entdeckte William Hyde Wollaston das schwefelhaltige Cystin als „cystic oxide“ in Blasensteinen, doch erst 1884 Eugen Baumann das monomere Cystein. 1819 trennte Henri Braconnot das Glycin aus Leim ab und Joseph Louis Proust das Leucin aus Getreide. Eugen von Gorup-Besánez isolierte 1856 das Valin aus Pankreassaft. Schon 1846 hatte Justus von Liebig aus Casein erstmals das Tyrosin abtrennen können, dessen Struktur 1869 Ludwig von Barth klärte. Im Hydrolysat des Casein entdeckte Edmund Drechsel 1889 auch das Lysin und später John Howard Mueller 1922 das schwefelhaltige Methionin als 19. Aminosäure, deren Strukturformel George Barger und Philip Coine 1928 angaben. In Melasse hatte Felix Ehrlich schon 1903 als 18. das Isoleucin gefunden, ein Strukturisomer des Leucin.

Friedrich Wöhler, dessen Synthesen in den 1820er Jahren das Gebiet der Biochemie eröffneten, entdeckte keine Aminosäure, doch waren drei seiner Schüler daran beteiligt, neben den erwähnten Gorup-Besánez und Schulze auch Georg Städeler (1863 Serin aus Rohseide). 18 der 20 entdeckten Aminosäuren wurden aus pflanzlichem oder tierischem Material isoliert, nur die beiden Aminosäuren Alanin (1850 Adolph Strecker) und Prolin (Willstätter) durch organische Synthese erhalten. Während die Analyse der stofflichen Zusammensetzung bis hin zur Summenformel mit den damaligen Methoden gut zu bewerkstelligen war, konnte die Strukturformel vieler Aminosäuren oftmals nur durch Teilschritte der Synthese endgültig aufgeklärt werden, was manchmal erst Jahre später gelang. Die Struktur des Asparagins und die von Asparaginsäure klärte Hermann Kolbe erst 1862 auf, 57 Jahre nach der ersten Beschreibung.

Den Gattungsnamen verdanken Aminosäuren zwei funktionellen Gruppen, ihre Einzelnamen mal einem hellen Aussehen (z. B. Arginin, Leucin), einem süßen Geschmack (z. B. Glycin) oder dem Material, in dem sie gefunden wurden (z. B. Asparagin, Cystein, Serin, Tyrosin), Merkmalen der chemischen Struktur (z. B. Prolin, Valin, Isoleucin) bzw. beidem (z. B. Glutamin, Glutaminsäure) und mal auch den Edukten ihrer Synthese (z. B. Alanin).

Aminosäuren bestehen aus mindestens zwei Kohlenstoffatomen. Die instabile Carbamidsäure besitzt lediglich ein Kohlenstoffatom und ist damit keine Aminosäure, sondern ein Kohlensäureamid. Aminosäuren lassen sich in Klassen einteilen je nach dem Kohlenstoffatom, an dem sich die Aminogruppe relativ zur Carboxygruppe befindet. Sind im Molekül mehrere Aminogruppen vertreten, so bestimmt das Kohlenstoffatom, dessen Aminogruppe dem Carboxy-Kohlenstoff am nächsten steht, um welche Klasse von Aminosäuren es sich handelt.


Die Bezeichnung weiterer Klassen der Aminosäuren ergibt sich nach dem gleichen Schema.

Die Aminosäuren einer Klasse unterscheiden sich durch ihre Seitenkette R. Ist die Seitenkette R verschieden von den anderen Substituenten, die sich am Kohlenstoff mit der Amino-Gruppe befinden, so befindet sich hier ein Stereozentrum und es existieren von der entsprechenden Aminosäure zwei Enantiomere. Enthält die Seitenkette R selbst weitere Stereozentren, so ergeben sich auch Diastereomere und die Zahl möglicher Stereoisomerer nimmt entsprechend zur Anzahl der weiteren Stereozentren zu. Von Aminosäuren mit "zwei" verschieden substituierten Stereozentren gibt es "vier" Stereoisomere.

Als "proteinogen" werden Aminosäuren bezeichnet, die in Lebewesen als Bausteine der Proteine während der Translation nach Vorgabe genetischer Information verwendet werden. Bei der Biosynthese von Proteinen, die an den Ribosomen einer Zelle stattfindet, werden im Zuge der Proteinbiosynthese ausgewählte Aminosäuren durch Peptidbindungen in bestimmter Reihenfolge zur Polypeptidkette eines Proteins verknüpft. Die Aminosäurensequenz des ribosomal gebildeten Peptids wird dabei vorgegeben durch die in der Basensequenz einer Nukleinsäure enthaltene genetische Information, wobei nach dem genetischen Code eine Aminosäure durch ein Basentriplett codiert wird.

Die proteinogenen Aminosäuren sind stets α-Aminosäuren. Bis auf die kleinste, Glycin, sind sie chiral und treten mit besonderer räumlicher Anordnung auf. Eine Besonderheit weist die Aminosäure Prolin auf, deren Aminogruppe ein sekundäres Amin besitzt, und die sich daher nicht so flexibel in eine Proteinfaltung einfügt wie andere proteinogene Aminosäuren – Prolin gilt beispielsweise als Helixbrecher bei α-helikalen Strukturen in Proteinen. Aufgrund der sekundären Aminogruppe wird Prolin auch als sekundäre Aminosäure – öfters fälschlicherweise bzw. veraltet auch als Iminosäure – bezeichnet.

Von den spiegelbildlich verschiedenen Enantiomeren sind jeweils nur die -Aminosäuren proteinogen (zur  / -Nomenklatur siehe Fischer-Projektion; in Fällen wie Hydroxyprolin gibt es weitere Stereoisomere). Die molekularen Komponenten des zum Aufbau der Proteine notwendigen zellulären Apparats – neben Ribosomen noch tRNAs und diese mit Aminosäuren beladende Aminoacyl-tRNA-Synthetasen – sind selber auch chiral und erkennen allein die -Variante.

Dennoch kommen in Lebewesen vereinzelt auch -Aminosäuren vor. Diese werden jedoch unabhängig von proteinogenen Stoffwechselwegen synthetisiert und dienen nicht dem ribosomalen Aufbau von Proteinen. So wird zum Beispiel -Alanin in Peptidoglycane der bakteriellen Zellwand eingebaut oder -Valin in bakterielle Cyclo-Depsipeptide wie Valinomycin. Verschiedene Arten von Archaeen, Bakterien, Pilzen und Nacktkiemern verfügen über "nichtribosomale Peptidsynthetasen" genannte Multienzymkomplexe, mit denen solche (nichtproteinogenen) Aminosäuren in ein nichtribosomales Peptid eingebaut werden können.

Für 20 der proteinogenen Aminosäuren finden sich Codons in der (am häufigsten gebrauchten) Standardversion des genetischen Codes. Diese werden daher als "Standardaminosäuren" oder auch "kanonische Aminosäuren" bezeichnet.

In Aminosäuresequenzen werden die Aminosäuren meist mit einem Namenskürzel im "Dreibuchstabencode" angegeben oder im "Einbuchstabencode" durch ein Symbol dargestellt.

Neben den oben angegebenen Codes werden zusätzliche Zeichen als Platzhalter benutzt, wenn aus der Proteinsequenzierung oder Röntgenstrukturanalyse nicht auf die genaue Aminosäure geschlossen werden kann.

Zu den natürlich vorkommenden Aminosäuren gehören außer den kanonischen die übrigen als nichtkanonische Aminosäuren bezeichneten Aminosäuren, wozu proteinogene und nicht-proteinogene zählen. Hierbei lassen sich mehrere Gruppen unterscheiden:




Der Mensch nutzt neben den 20 kanonischen auch Selenocystein als proteinogene Aminosäure. Von den 20 kanonischen Aminosäuren werden 12 vom menschlichen Organismus beziehungsweise durch im menschlichen Verdauungstrakt lebende Mikroorganismen synthetisiert. Die restlichen 8 Aminosäuren sind für den Menschen essentiell, das heißt, er muss sie über die Nahrung aufnehmen.

Der Einbau künstlicher, nahezu beliebig gebauter Aminosäuren im Zuge eines Proteindesigns ist unter anderem über die Ersetzung des Liganden in der entsprechenden Aminoacyl-tRNA-Synthetase möglich. Diese Verfahren sind teilweise so weit fortgeschritten, dass damit gezielt bestimmte Proteine eine Markierung erhalten können, die beispielsweise das Protein nach Behandlung mit spezifischen Reagenzien zur Fluoreszenz anregen (Beispiel: Einbau von Norbornen-Aminosäure via Pyrrolysyl-tRNA-Synthetase/Codon CUA). Damit ist eine genaue Lokalisierung des Proteins auch ohne Produktion und Reaktion mit Antikörpern möglich.<ref name="DOI10.1038/nchem.1250">Kathrin Lang, Lloyd Davis u. a.: "Genetically encoded norbornene directs site-specific cellular protein labelling via a rapid bioorthogonal reaction." In: "Nature Chemistry." 2012, S. 298–304, .</ref>

-Aminosäuren sind in der Biochemie von großer Bedeutung, da sie die Bausteine von Peptiden und Proteinen (Eiweißen) sind. Bisher sind über zwanzig sogenannte "proteinogene" Aminosäuren bekannt. Dies sind zunächst jene 20 -α-Aminosäuren, die als Standard-Aminosäuren durch Codons von je drei Nukleinbasen in der DNA nach dem Standard-Code codiert werden. Zu diesen "kanonisch" genannten Aminosäuren sind inzwischen zwei weitere hinzugekommen, Selenocystein und Pyrrolysin. Beide nicht-kanonischen sind ebenfalls α-Aminosäuren, bezogen auf die endständige Carboxygruppe ist die Aminogruppe am unmittelbar benachbarten Kohlenstoffatom gebunden (C). Darüber hinaus gibt es noch weitere Aminosäuren, die als Bestandteil von Proteinen oder Peptiden auftreten, jedoch nicht codiert werden.

Aminosäureketten mit einer Kettenlänge unter zirka 100 Aminosäuren werden meist als Peptide bezeichnet, bei den größeren ribosomal gebildeten spricht man von Proteinen. Die einzelnen Aminosäuren sind dabei innerhalb der Kette je über Peptidbindungen (Säureamid) verknüpft. Ein automatisiertes Verfahren zur Synthese von Peptiden liefert die Merrifield-Synthese.

In Form von Nahrung aufgenommene Proteine werden bei der Verdauung in -Aminosäuren zerlegt. In der Leber werden sie weiter verwertet. Entweder werden sie zur Proteinbiosynthese verwendet oder abgebaut ("siehe auch:" Aminosäureindex). Die wichtigsten Mechanismen des Aminosäurenabbaus sind:

Aminosäuren, die ein Organismus benötigt, jedoch nicht selbst herstellen kann, heißen "essentielle" Aminosäuren und müssen mit der Nahrung aufgenommen werden. Alle diese essentiellen Aminosäuren sind -α-Aminosäuren. Für Menschen sind Valin, Methionin, Leucin, Isoleucin, Phenylalanin, Tryptophan, Threonin und Lysin essentielle Aminosäuren. Bedingt essentielle oder "semi-essentielle" Aminosäuren müssen nur in bestimmten Situationen mit der Nahrung aufgenommen werden, zum Beispiel während des Wachstums oder nach schweren Verletzungen. Die übrigen Aminosäuren werden entweder direkt synthetisiert oder aus anderen Aminosäuren durch Modifikation gewonnen. So kann Cystein aus der essentiellen Aminosäure Methionin synthetisiert werden. Solange das Vermögen, aus Phenylalanin die Aminosäure Tyrosin herzustellen, noch nicht ausgereift ist, zählt auch diese neben den anderen zu den essentiellen Aminosäuren im Kindesalter. Aus ähnlichem Grund muss auch bei einer Phenylketonurie Tyrosin zugeführt werden. Daneben gibt es andere Erkrankungen, die den Aminosäurestoffwechsel beeinträchtigen und die Aufnahme einer eigentlich "nicht-essentiellen" Aminosäure unter Umständen erfordern. Alle essentiellen bzw. semi-essentiellen Aminosäuren, die der menschliche Körper benötigt, sind in Hühnereiern enthalten.

Pflanzen und Mikroorganismen können alle für sie notwendigen Aminosäuren selbst synthetisieren. Daher gibt es für sie keine "essentiellen Aminosäuren".

Die proteinogenen Aminosäuren lassen sich nach ihren Resten in Gruppen aufteilen (siehe Tabellenübersicht der Eigenschaften). Dabei kann eine Aminosäure in verschiedenen Gruppen gleichzeitig auftauchen. In einem Mengendiagramm lassen sich die Überlappungen der Gruppen grafisch darstellen.

Die Eigenschaften der Seitenkette von Cystein betreffend haben die Autoren unterschiedliche Ansichten: Löffler hält sie für polar, während Alberts sie für unpolar hält. Richtigerweise handelt es sich bei Schwefel um ein Heteroatom, folglich gilt: Die Seitenkette von Cystein hat schwach polare Eigenschaften.

Aufgrund der basischen Aminogruppe und der sauren Carbonsäuregruppe sind Aminosäuren zugleich Basen und Säuren. Als Feststoffe und in neutralen wässrigen Lösungen liegen Aminosäuren als Zwitterionen vor, das heißt die Aminogruppe ist protoniert und die Carboxygruppe ist deprotoniert.
Verallgemeinert lässt sich das Zwitterion so darstellen:

Als Zwitterion kann die protonierte Aminogruppe als Säure (Protonendonator) und die Carboxylatgruppe kann als Base (Protonenakzeptor) reagieren. In sauren Lösungen liegen Aminosäuren als Kationen und in basischen Lösungen als Anionen vor:

Die Ladung eines Aminosäuremoleküls hängt vom pH-Wert der Lösung ab. Bei einem Zwitterion mit einer sauren und einer basischen Gruppe ist bei neutralem pH-Wert die Gesamtladung des Moleküls null. Daneben besitzen die Seitenketten der Aminosäuren teilweise saure oder basische geladene Gruppen. Der pH-Wert mit einer Nettoladung von Null ist der isoelektrische Punkt (pH, pI) einer Aminosäure. Am isoelektrischen Punkt ist die Wasserlöslichkeit einer Aminosäure am geringsten.

Für das Säure-Base-Verhalten proteinogener Aminosäuren ist vor allem das Verhalten ihrer Seitenkette (fortan mit "R" bezeichnet) interessant. In Proteinen sind die NH- und COOH-Gruppen bei physiologischem pH-Wert (um pH 7) wegen der Peptidbindung nicht protonierbar und damit auch nicht titrierbar. Ausnahmen sind der Amino- und der Carboxy-Terminus des Proteins. Daher ist für das Säure-Base-Verhalten von Proteinen und Peptiden der Seitenkettenrest "R" maßgeblich.

Das Verhalten der Seitenkette "R" hängt von ihrer Konstitution ab, das heißt, ob die Seitenkette selbst wieder als Protonenakzeptor oder als Protonendonator wirken kann. Die proteinogenen Aminosäuren werden nach den funktionellen Gruppen eingeteilt in solche mit unpolarer oder polarer Aminosäureseitenkette und weiter unterteilt in nach Polarität sortierte Untergruppen: "aliphatische", "aromatische", "amidierte", "Schwefel-enthaltende", "hydroxylierte", "basische" und "saure" Aminosäuren.

Die Seitenketten von Tyrosin und Cystein sind zwar im Vergleich zu den anderen unpolaren Seitenketten relativ sauer, neigen aber erst bei unphysiologisch hohen pH-Werten zum Deprotonieren. Prolin ist eine sekundäre Aminosäure, da der N-Terminus mit der Seitenkette einen fünfatomigen Ring schließt. Innerhalb eines Proteins bindet der Carboxy-Terminus einer vorhergehenden Aminosäure an den Stickstoff des Prolins, welcher aufgrund der bereits erwähnten Peptidbindung nicht protonierbar ist. Histidin, Tyrosin und Methionin kommen jeweils in zwei Untergruppen vor.


Der pK-Wert ist der pH-Wert, bei dem die titrierbaren Gruppen zu gleichen Teilen protoniert und deprotoniert vorliegen; die titrierbare Gruppe liegt dann zu gleichen Teilen in ihrer basischen wie in ihrer sauren Form vor (siehe auch: Henderson-Hasselbalch-Gleichung).

Es ist meist üblich, anstatt vom pK vom pK zu sprechen, so vom "pK der Säure". In diesem Sinne müsste allerdings vom pK des Lysins als pK, vom "pK der Base" gesprochen werden. Aus Gründen der Vereinfachung wird diese Notation aber allgemein weggelassen, da sich auch aus dem Sinnzusammenhang ergibt, ob die Gruppe als Base oder Säure wirkt.

Der pK ist keine Konstante, sondern hängt von der Temperatur, der Aktivität, der Ionenstärke und der unmittelbaren Umgebung der titrierbaren Gruppe ab und kann daher stark schwanken.

Ist der pH höher als der pK einer titrierbaren Gruppe, so liegt die titrierbare Gruppe in ihrer basischen (deprotonierten) Form vor. Ist der pH niedriger als der pK der titrierbaren Gruppe, so liegt die titrierbare Gruppe in ihrer sauren (protonierten) Form vor:

Die Seitenketten "basischer Aminosäuren" sind in ihrer protonierten (sauren) Form einfach positiv geladen und in ihrer deprotonierten (basischen) Form ungeladen. Die Seitenketten der "sauren Aminosäuren" (einschließlich Cystein und Tyrosin) sind in ihrer protonierten (sauren) Form ungeladen und in ihrer deprotonierten (basischen) Form einfach negativ geladen. Da das Verhalten der Seitenkette ein ganz anderes ist, wenn sie geladen bzw. ungeladen ist, spielt der pH-Wert für die Eigenschaften der Seitenkette eine so wichtige Rolle.

Die titrierbaren Seitenketten beeinflussen zum Beispiel das Löslichkeitsverhalten der entsprechenden Aminosäure. In polaren Lösungsmitteln gilt: Geladene Seitenketten machen die Aminosäure löslicher, ungeladene Seitenketten machen die Aminosäure unlöslicher.

In Proteinen kann das dazu führen, dass bestimmte Abschnitte hydrophiler oder hydrophober werden, wodurch die Faltung und damit auch die Aktivität von Enzymen vom pH-Wert abhängt. Durch stark saure oder basische Lösungen können Proteine daher denaturiert werden.

18 der 20 proteinogenen Aminosäuren haben gemäß der Cahn-Ingold-Prelog-Konvention am α-Kohlenstoff-Atom die ("S")-Konfiguration, lediglich Cystein besitzt die ("R")-Konfiguration, da hier der Kohlenstoff mit der Thiolgruppe eine höhere Priorität als die Carbonsäuregruppe hat. Glycin ist nicht chiral, daher kann keine absolute Konfiguration bestimmt werden.

Zusätzlich zum Stereozentrum am α-C-Atom besitzen Isoleucin und Threonin in ihrem Rest "R" je ein weiteres stereogenes Zentrum. Proteinogenes Isoleucin ["R" = –C*H(CH)CHCH] ist dort ("S")-konfiguriert, Threonin ["R" = –C*H(OH)CH] ("R")-konfiguriert.

Es sind bislang über 400 "nichtproteinogene" (d. h. nicht während der Translation in Proteine eingebaute) Aminosäuren, die in Organismen vorkommen, bekannt. Dazu gehört etwa das -Thyroxin, ein Hormon der Schilddrüse, -DOPA, -Ornithin oder das in fast allen Arten von Cyanobakterien nachgewiesene Neurotoxin β-Methylaminoalanin (BMAA).

Die meisten nichtproteinogenen Aminosäuren leiten sich von den proteinogenen ab, die -α-Aminosäuren sind. Dennoch können dabei auch β-Aminosäuren (β-Alanin) oder γ-Aminosäuren (GABA) entstehen.

Zu den nichtproteinogenen Aminosäuren zählen auch alle -Enantiomere der proteinogenen -Aminosäuren. -Serin wird im Hirn durch die Serin-Racemase aus -Serin (seinem Enantiomer) erzeugt. Es dient sowohl als Neurotransmitter als auch als Gliotransmitter durch die Aktivierung des NMDA-Rezeptors, was zusammen mit Glutamat die Öffnung des Kanals erlaubt. Zum Öffnen des Ionenkanals muss Glutamat und entweder Glycin oder -Serin binden. -Serin ist an der Glycin-Bindungsstelle des Glutamatrezeptors vom NMDA-Typ ein stärkerer Agonist als Glycin selbst, war aber zum Zeitpunkt der Erstbeschreibung der Glycin-Bindungsstelle noch unbekannt. -Serin ist nach -Aspartat die zweite -Aminosäure, die in Menschen gefunden wurde.

Zu den synthetischen Aminosäuren gehört die 2-Amino-5-phosphonovaleriansäure (APV), ein Antagonist des NMDA-Rezeptors und das ökonomisch wichtige -Phenylglycin [Synonym: ("R")-Phenylglycin], das in der Seitenkette vieler semisynthetischer β-Lactamantibiotica als Teilstruktur enthalten ist. ("S")- und ("R")-"tert"-Leucin [Synonym: ("S")- und ("R")-β-Methylvalin] sind synthetische Strukturisomere der proteinogenen Aminosäure ("S")-Leucin und werden als Edukt in stereoselektiven Synthesen eingesetzt.

Es gibt auch Aminosulfonsäuren [Beispiel: 2-Aminoethansulfonsäure (Synonym: Taurin)], α-Aminophosphonsäuren und α-Aminophosphinsäuren. Das sind auch α-Aminosäuren, jedoch "keine" α-Amino"carbon"säuren. Statt einer Carboxygruppe (–COOH) ist eine Sulfonsäure-, Phosphonsäure- bzw. Phosphinsäuregruppe in diesen α-Aminosäuren enthalten.

Ein quantitativer photometrischer Nachweis von Aminosäuren kann unter anderem per Kaiser-Test mit Ninhydrin oder mit dem Folin-Reagenz erfolgen, wodurch primäre Amine nachgewiesen werden. Für sekundäre Amine werden der Isatin-Test oder der Chloranil-Test verwendet. Ebenso können Trennung und Nachweis von Aminosäuren per Kapillarelektrophorese oder per HPLC erfolgen, teilweise als Flüssigchromatographie mit Massenspektrometrie-Kopplung. Während die meisten Aminosäuren kein UV-Licht mit Wellenlängen über 220 nm absorbieren, sind die Aminosäuren Phenylalanin, Tyrosin, Histidin und Tryptophan aromatisch und absorbieren UV-Licht mit einem Maximum zwischen 260 nm und 280 nm. Die Aminosäurezusammensetzung eines Proteins kann durch Hydrolyse des Proteins untersucht werden. Die langsam eintretende Racemisierung der Aminosäuren in den ursprünglich ausschließlich aus L-Aminosäuren aufgebauten Proteinen wird bei der Aminosäuredatierung untersucht.

"Aminosäuren" werden entweder aus Naturstoffen durch Auftrennung eines hydrolysierten Proteins oder auf synthetischem Wege gewonnen. Ursprünglich diente die Entwicklung einer Synthese für die diversen Aminosäuren hauptsächlich der Strukturaufklärung. Inzwischen sind diese Strukturfragen gelöst und mit den verschiedenen Synthesen, soweit sie noch aktuell sind, werden gezielt die gewünschten Aminosäuren dargestellt. Bei den Synthesen entstehen zunächst racemische Gemische, die getrennt werden können. Eine Methode hierfür ist eine selektive enzymatische Hydrolyse, die zur Racematspaltung eingesetzt wird.

Nachfolgend ein Überblick über diverse Synthesen, die von Chemikern bereits ab Mitte des 19. Jahrhunderts entwickelt wurden. Einige dieser älteren Synthesen sind wegen geringer Ausbeuten oder sonstiger Probleme nur von historischem Interesse. Allerdings wurden diese alten Verfahren teilweise weiterentwickelt und einige sind auch noch heute zur Darstellung von Aminosäuren aktuell. Weitergehende Einzelheiten zu diesen Synthesen einschließlich der Gleichungen für die Synthesen sind unter den "Links" zu den "Synthesen" und den angegebenen "Aminosäuren" angeführt.


Industriell werden Aminosäuren heute nach folgenden Verfahren hergestellt:

Aminosäuren haben für die Ernährung des Menschen eine fundamentale Bedeutung, insbesondere weil die essentiellen Aminosäuren nicht selbst erzeugt werden können. In der Regel wird im Zuge einer ausgewogenen Ernährung der Bedarf an essentiellen Aminosäuren durch tierische oder eine geeignete Kombination verschiedener pflanzlicher Proteine (etwa aus Getreide und Hülsenfrüchten) vollkommen gedeckt. Pflanzliche Proteine haben meist eine geringere biologische Wertigkeit. Futtermittel in der Nutztierhaltung werden zusätzlich mit Aminosäuren angereichert, z. B. -Methionin und -Lysin, aber auch verzweigte Aminosäuren (Leucin, Isoleucin, Valin), wodurch deren Nährwert erhöht wird. Verschiedene Aminosäuren werden als Nahrungsergänzungsmittel verkauft.

Aminosäuren bzw. ihre Derivate finden Verwendung als Zusatz für Lebensmittel. Die menschliche Zunge besitzt einen Glutamatrezeptor, dessen Aktivierung allgemein mit einem gesteigerten Geschmack assoziiert ist. Daher wird als Geschmacksverstärker Natriumglutamat verwendet. Der Süßstoff Aspartam enthält eine Aminosäure. Aminosäuren sind Vorstufen für bestimmte Aromastoffe, die beim trockenen Garen von Speisen über die Maillard-Reaktion entstehen.

Aminosäuren werden in der Zellbiologie und Mikrobiologie als Bestandteile von Zellkulturmedien verwendet. In der Biochemie werden Derivate von Aminosäuren wie Photo-Leucin oder Photo-Methionin zur Strukturaufklärung von Proteinen und andere zur Molekülmarkierung verwendet. Daneben werden Aminosäuren auch als Hilfsstoffe eingesetzt, z. B. als Salzbildner, Puffer. In der Pharmazie bzw. Medizin werden -Aminosäuren als Infusionslösungen für die parenterale Ernährung und als Stabilisatoren bei bestimmten Lebererkrankungen angewendet. Bei Krankheiten mit einem Mangel von Neurotransmittern verwendet man -Dopa. Für synthetische Peptidhormone und für die Biosynthese von Antibiotika sind Aminosäuren notwendige Ausgangsstoffe. Magnesium- und Kalium-Aspartate spielen bei der Behandlung von Herz- und Kreislauferkrankungen eine Rolle.

Cystein, beziehungsweise die Derivate Acetylcystein und Carbocystein, finden zudem eine Anwendung bei infektiösen Bronchialerkrankungen mit erhöhtem Bronchialsekret. Zudem wird -Cystein als Reduktionsmittel in der Dauerwelle eingesetzt. Aminosäuren werden in der Kosmetik Hautpflegemitteln und Shampoos zugesetzt.

Aminosäuren können nach ihren Abbauwegen in "ketogene", "glucogene" und gemischt keto- und glucogene Aminosäuren eingeteilt werden. Ketogene Aminosäuren werden beim Abbau dem Citrat-Zyklus zugeführt, glucogene Aminosäuren der Gluconeogenese. Weiterhin werden im Stoffwechsel aus Aminosäuren verschiedene Abbauprodukte mit biologischer Aktivität (z. B. Neurotransmitter) gebildet. Tryptophan ist der Vorläufer von Serotonin. Tyrosin und sein Vorläufer Phenylalanin sind Vorläufer der Catecholamine Dopamin, Epinephrin (synonym Adrenalin) und Norepinephrin (synonym Noradrenalin). Phenylalanin ist der Vorläufer von Phenethylamin in Menschen. In Pflanzen ist Phenylalanin der Vorläufer der Phenylpropanoide. Glycin ist der Ausgangsstoff der Porphyrinsynthese (Häm). Aus Arginin wird der sekundäre Botenstoff Stickstoffmonoxid gebildet. Ornithin und S-Adenosylmethionin sind Vorläufer der Polyamine. Aspartat, Glycin und Glutamin sind Ausgangsstoffe der Biosynthese von Nukleotiden.

Bei verschiedenen Infektionen des Menschen mit Pathogenen wurde eine Konkurrenz mit dem Wirt um die Aminosäuren Asparagin, Arginin und Tryptophan beschrieben.





</doc>
<doc id="260" url="https://de.wikipedia.org/wiki?curid=260" title="Germania (Tacitus)">
Germania (Tacitus)

Die Germania ist eine kurze ethnographische Schrift des römischen Historikers Tacitus über die Germanen. Sie wurde seit der Frühen Neuzeit verstärkt gelesen und entfaltete auf diese Weise eine erhebliche Breitenwirkung. In der neueren Forschung wird das Werk durchaus kritischer betrachtet und auf die problematische Rezeptionsgeschichte hingewiesen.

Die "Germania" wird in aller Regel in das Jahr 98 n. Chr. datiert, auf der Grundlage der Formulierung:

Das zweite Konsulat Trajans fiel in das Jahr 98 n. Chr. Jedoch handelt es sich bei dieser Zeitangabe lediglich um einen "terminus post quem", an dem das Werk frühestens verfasst worden sein kann; ein absolutes Datum liegt somit nicht vor.

Ein neuerer Vorschlag von Roland Schuhmann, der von der Forschung noch nicht diskutiert worden ist, nimmt an, dass die Abfassung der "Germania" nach 103–106 n. Chr. anzusetzen ist, weil der Name "Pannoniis" im ersten Satz des Textes die Existenz zweier pannonischer Provinzen ("Pannonia superior" und "inferior", entstanden durch Teilung der Provinz Pannonien) voraussetzt, wenn er als Ländername verstanden wird; die traditionelle Auffassung sieht ihn als Völkernamen.

Die Schrift "Germania" ist ohne einen einheitlichen Titel überliefert. Die erste Erwähnung der Schrift findet sich in einem Brief des Humanisten Antonio Beccadelli an Guarino da Verona von April 1426: "Compertus est Cor. Tacitus de origine et situ Germanorum" („"Cornelius Tacitus de origine et situ Germanorum" ist in Erfahrung gebracht“). In einem Inventar von Niccolò Niccoli aus dem Jahre 1431 steht: "Cornelii taciti de origine & situ germanorum liber incipit sic" („"de origine et situ Germanorum liber" des Cornelius Tacitus fängt so an“). Pier Candido Decembrio, der den Codex Hersfeldensis (nach 1455, s. u. Rezeption) in Rom einsah, gibt den Titel als: "Cornelii taciti liber … de Origine et situ Germaniae" („von Cornelius Tacitus das Buch "de Origine et situ Germaniae"“). Beide Titelvarianten gehen auf den Hersfelder Codex zurück; die zweite Variante ist semantisch inkonsistent.

Aus der Antike ist kein Titel des Werks überliefert. Es gibt nur zwei Titel, die einigermaßen plausibel erscheinen: "De origine et situ Germanorum" („Über Ursprung und geographische Lage der Germanen“) und "De origine et moribus Germanorum" („Über Ursprung und Sitten der Germanen“). Für einen Werktitel "De origine et situ Germanorum" könnten zwei parallele Titelformulierungen Senecas sprechen: "De situ Indiae" („Die geographische Lage Indiens“) und "De situ et sacris Aegyptiorum" („Über die geographische Lage und die Heiligtümer der Ägypter“). Beide Titel bilden jedoch keine genauen Entsprechungen zur "Germania". "India" ist anders als der Völkername "Germani" ein Ländername, während in Senecas zweitem Buch nicht vom Ursprung, sondern von den Heiligtümern der Ägypter die Rede ist. Der aus der Renaissance überlieferte Titel "De origine et situ Germanorum" erscheint gewissermaßen als Kombination aus den beiden Titeln Senecas. Für "De origine et moribus Germanorum" würde eine Passage im Text selbst sprechen, denn in "Germania" c. 27,2 heißt es: "Haec in commune de omnium Germanorum origine ac moribus accepimus" („Dies haben wir im Allgemeinen über Ursprung und Sitten aller Germanen vernommen“). Der Titel erweckt allerdings den Eindruck, dass er aus diesem Kapitel übernommen ist. Da keiner der beiden Titel über jeden Zweifel erhaben ist, hat man der Schrift den Arbeitstitel "Germania" gegeben.

Zu Tacitus’ Lebzeiten befand sich das römische Reich auf seinem Höhepunkt. Geographisch hatte es fast seine größte Ausdehnung erreicht und erlebte auch kulturell eine Blüte. Die Grenzen zu Germanien waren gezogen und weitgehend gesichert worden. Nach der Varusschlacht im Jahre 9 n. Chr. waren die römischen Offensiven schließlich 16 n. Chr. eingestellt worden (siehe Germanicus); erst im späten 1. Jahrhundert hatten die Römer die Grenze unter Domitian leicht vorverschoben (siehe Dekumatland) und die beiden Rheinprovinzen (Germania inferior, Germania superior) eingerichtet. Einige germanische Stämme hatten sich mit dem neuen mächtigen Nachbarn durchaus arrangiert, andere standen Rom allerdings weiterhin feindlich gegenüber. Diese Situation erforderte lange Zeit eine hohe und kostspielige Truppenpräsenz an der Grenze des römischen Reiches zu den Germanen. Das Besondere an den germanisch-römischen Beziehungen ergibt sich daraus, dass "im Unterschied zur anderen großen Grenzzone [...] im Norden keine organisierte Großmacht Rom gegenüberstand".

In der "Germania", die sich in einen allgemeinen und einen besonderen Teil gliedert, beschreibt Tacitus Germanien, ansatzweise auch dessen Geographie und benennt verschiedene germanische Stämme vom Rhein bis zur Weichsel. Er stellt Sitten und Gebräuche der Germanen dar und hebt dabei ihre ihm zufolge sittliche Lebensweise hervor, wie ihr streng geregeltes Familienleben, ihren treuen und aufrichtigen Charakter, ihre Tapferkeit im Krieg und ihren Freiheitswillen. Er weist aber auch auf Schwächen hin, wie ihre Trägheit, ihren Hang zu Würfelspiel und übermäßigem Alkoholkonsum.

Tacitus beginnt mit den Grenzen Germaniens, seinem Volk, der Beschaffenheit des Landes und den Bodenschätzen. Dabei betrachtet er die Germanen als abgehärtet, ursprünglich und unvermischt mit anderen Völkern, als Urbevölkerung ihrer Heimat, da sie phänotypisch seinen Schilderungen nach keiner Ethnie der bekannten damaligen Welt ähnlich seien, und er sich auch nicht vorstellen könne, dass jemand freiwillig in solch eine Region, die seiner Ansicht nach sehr rau, unwirtlich und nur schwer überhaupt zu erreichen sei, einwandern könne. Er beschreibt Land und Klima als unfreundlich und trostlos, arm an fruchtbarem Boden und ohne wertvolle Bodenschätze.

Er fährt fort mit der Beschreibung der Kriegsführung, der Religion und Volksversammlungen, spricht dann über die germanische Rechtsprechung und die Rolle der Fürsten im Krieg. Dabei beschreibt er die Germanen als wilde Barbaren, schwach bewaffnet, aber tapfer im Kampf und voller Wertschätzung für ihre Frauen, als fromme Menschen, die auf Vorzeichen und Orakel vertrauen. Entscheidungen fielen, so Tacitus, in Versammlungen, die abhängig vom Stand des Mondes abgehalten würden. Hier kritisiert er aber eine gewisse Disziplinlosigkeit. Der Kampf, meint Tacitus, sei bei den Germanen höher bewertet als die Mühe täglicher Arbeit. Er zeichnet sogar das Bild eines faulen, dem Müßiggang verfallenen Volkes, das lieber seine Frauen und Alten arbeiten lasse als sich selber um Haus, Hof und Feld zu kümmern.

Die nächsten Abschnitte beschäftigen sich mit den Behausungen der Germanen, ihrer Wohnweise und Kleidung; es folgen Exkurse über Ehe, Erziehung und Erbrecht, bis die Rede auf Gastfreundschaft, Feiern und Spiele kommt. Der allgemeine Teil endet schließlich in einer Beschreibung des Ackerbaus und der Totenbestattung. Tacitus zeichnet auch hier wieder das Bild eines wilden, nachlässig bekleideten Volkes, das sich allerdings, und dafür lobt er die Germanen ausdrücklich, durch hohe Sittsamkeit auszeichne. Die Germanen seien monogam und dem Ehepartner gegenüber treu ergeben. Besonders diese Bemerkung führte zur Annahme, die Germania stelle einen Sittenspiegel dar, der an die Adresse der römischen Gesellschaft gerichtet sei. An kaum einer anderen Stelle betont Tacitus eine Eigenart germanischen Lebens so nachdrücklich.

Die Gastfreundschaft der Germanen wird lobend hervorgehoben, die dabei auftretenden Ausschweifungen aber auch dargestellt. Ihre Feiern, so Tacitus, dauerten oft tagelang und endeten nicht selten in Schlägereien der Betrunkenen und Totschlag. Hier erwähnt der Autor auch ihr einfaches Essen und das ihm unbekannte alkoholische Getränk (Bier), das die Germanen im Übermaß konsumierten. Es überrascht, dass fast im selben Atemzug ihre absolute Ehrlichkeit gerühmt wird. Verwundert stellt Tacitus dann fest, dass so ziemlich das Einzige, was die Germanen nüchtern und ernsthaft betrieben, das Würfelspiel sei. Hier setzten sie sogar ihre persönliche Freiheit als letzten Einsatz ein und ließen sich als Sklaven verkaufen. Landwirtschaft betrieben sie zwar gemeinschaftlich, aber stets auf niedrigem Niveau. Letzter Punkt dieses Teils ist die Darstellung der Totenbestattung, die als einfach und prunklos beschrieben wird, jedoch in würdevoller Verehrung der Verstorbenen.
In den letzten elf Kapiteln beschreibt Tacitus Bräuche und Besonderheiten einzelner Stämme und kommt auch auf diejenigen zu sprechen, die Germanien verlassen und sich in Gallien angesiedelt haben.

Erwähnt werden hier anfangs gallische Stämme, Helvetier und Bojer (Boier), die nach Germanien gezogen seien. Dem stellt er Treverer und Nervier gegenüber, die, seiner Darstellung nach, als Germanen in Gallien leben. Diese Zuordnung ist allerdings nicht ganz unproblematisch, wenngleich schon Gaius Iulius Caesar vermerkte, dass ein großer Teil der Belger sich germanischer Abstammung rühmte. Tacitus erwähnt Vangionen, Triboker und Nemeter am Rhein, besonders hebt er die Ubier hervor, die dem römischen Reich treu ergeben seien. Als besonders tapfer werden die Bataver am Niederrhein beschrieben, die Rom ebenso treu zur Seite stünden wie die Mattiaker in der Gegend um das heutige Wiesbaden.

Den kräftigen und militärisch gut organisierten Chatten sagt Tacitus nach, sie schnitten Haupthaar und Bart erst nach der Tötung eines Feindes. Dies sei die Bestimmung ihres Daseins.

Die Tenkterer seien geschulte Reiter, deren Nachbarn, die Brukterer, von anderen Germanen vernichtet worden seien. Er erwähnt hier die Angrivarier und Chamaver, die Dulgubnier und Chasuarier, schließlich die Friesen am Rand des Weltmeeres.

Als Nachbarn der Friesen erwähnt Tacitus die Chauken, die von der Nordseeküste bis an das Gebiet der Chatten siedeln. Sie seien, frei von Habgier und Herrschsucht, bei den übrigen Germanen sehr angesehen. Er kommt auf die Cherusker zu sprechen, nennt sie Tölpel und Toren - vielleicht in einem Reflex auf die verlorene Schlacht im Teutoburger Wald gegen den Cherusker Hermann - und endet mit der Erwähnung der "ruhmreichen Kimbern" und der für die Römer ebenfalls verlustreichen Kimbernkriege.

Den vorletzten und größten Abschnitt des besonderen Teils widmet Tacitus den Sueben. Diese bewohnten einen großen Teil Germaniens. Sie seien, anders als andere Stämme, keine einheitliche Volksgruppe und unterschieden sich von den übrigen durch ihre Haartracht "(Suebenknoten)". Bis in das hohe Alter hinein knoteten sie ihr Haar zu einer kunstvollen Frisur, allerdings nicht aus Schönheitsgründen, sondern um groß und furchterregend zu erscheinen. Er erwähnt öffentliche Menschenopfer bei der Untergruppe der Semnonen, nennt weiter Langobarden und andere Stämme. Ihre Verehrung gelte der Mutter Erde (Nerthus), der sie in einem Heiligtum "auf einer Insel des Weltmeeres" huldigen.

Der suebische Stamm der Hermunduren sei den Römern hingegen treu ergeben, sie dürften als einziger germanischer Stamm ohne Beaufsichtigung über die römische Grenze ziehen und Handel treiben. Neben vielen anderen erwähnt Tacitus Narister, Markomannen und Quaden, auch die "rechts des suebischen Meeres" (an der Ostküste der Ostsee) lebenden Aesti, die in Lebensweise und Religion den Sueben ähnelten, ihre Sprache aber gleiche der britannischen Sprache (d.h. einer Form des Keltischen). Sie sammelten Bernstein ("Glesum") und verkaufen ihn an die Römer, ohne zu wissen, wie er entstehe oder wo er herkomme. Tacitus endet mit den Sithonen, die so tief in die Knechtschaft gesunken seien, dass sie von einer Frau regiert würden.

Im letzten Kapitel der "Germania" bespricht Tacitus Peukiner, Veneter und Fennen, Stämme jenseits des Gebietes der Sueben, von denen er nicht weiß, ob er sie den Germanen zuordnen soll.

Tacitus selbst war jedoch nie in Germanien. Wahrscheinlich ist, dass er sein Wissen größtenteils aus literarischen Quellen bezog, wie aus Gaius Iulius Caesars Werk über den Gallischen Krieg "(De bello Gallico)" und dem darin enthaltenen Germanenexkurs. Womöglich zog er auch andere schriftliche Quellen zu Rate, in Frage kommen unter anderem der "Germanenexkurs" im Geschichtswerk des Titus Livius und die "bella Germaniae" („Germanenkriege“) des älteren Plinius. Beide Werke sind nicht oder nicht vollständig erhalten. Erwähnung in der "Germania" findet jedoch allein Caesar. Es gilt als wahrscheinlich, dass auch mündliche Berichte von zeitgenössischen Germanien-Reisenden in sein Werk eingeflossen sind. Die Beschreibung des Sueben-Knotens, der Opferriten und die Bestrafung der treulosen Ehefrau werden auf tatsächliche Beobachtung zurückgeführt.

Tacitus beschreibt seiner Leserschaft ein Volk, das sich anscheinend grundlegend von dem eigenen unterscheidet. Es ist anzunehmen, dass das Objekt seiner Beschreibung, die Germanen, dem römischen Volk äußerst fremd vorgekommen sein müsste, hätte er sich dabei nicht der Methode bedient, das Fremde „begrifflich und inhaltlich in die eigene Welt zu integrieren“. Diese römische Interpretation "(Interpretatio Romana)" fällt besonders bei der Beschreibung der germanischen Götter auf. So spricht Tacitus von Merkur (für Odin) als dem höchsten Gott und erwähnt Herkules (für Thor) und Mars (für Tyr). Auch bei der Beschreibung des Heerwesens (hier die Truppeneinteilung in Hundertschaften/Centurien) sowie der Trennung von Öffentlicher Sache "(res publica)" und Privatangelegenheiten "(res privatae)" ist dies erkennbar.

Tacitus sieht alle Germanen als ursprünglich an, d.h. alle haben dieselbe Herkunft und sind nicht mit anderen Völkern vermischt und seien auch nicht nach Germanien eingewandert. Charakterzüge, die er im allgemeinen Teil dem gesamten Volk zuschreibt, führt er auf diese gemeinsame Herkunft zurück. Das kann Tacitus allerdings nicht belegen, er geht schlicht davon aus, dass kein Volk freiwillig in dieses karge Land gezogen sein könnte, um sich mit den Germanen zu vermischen.

In der ganzen "Germania" ist erkennbar, dass er das Bekannte seiner Welt in der Welt der Germanen sucht, um es für sein römisches Publikum zu beschreiben und zu vergleichen. Das durchaus polarisierende Bild, das Tacitus dabei gibt (ehrenwerte Sitten, Freiheitsliebe und Moral versus primitive, lasterhafte und faule Lebensweise), lässt den heutigen Leser auch einen Eindruck der römischen Gesellschaft zu Zeiten Tacitus’ erahnen. Insofern kann die "Germania" nicht nur als Ethnographie der Germanen gesehen werden, sondern auch als Anhaltspunkt für das Verständnis von Tacitus’ eigener, römischen Gesellschaft.

Um die "Germania" richtig verstehen zu können, ist es unumgänglich, Tacitus’ Beweggründe zu kennen. Will er an seiner Zeit und Gesellschaft Kritik üben oder Überlegenheit beweisen? Will er lediglich ein fremdes Volk beschreiben und seinen römischen Zeitgenossen näher bringen, was ihnen selbst fremd und barbarisch erscheint? Dies zu verstehen ist Grundlage für die Bewertung seiner Arbeit.

Tacitus selbst äußert sich dazu jedoch nicht. Auch existiert zur "Germania" keine Einleitung oder ein Nachwort des Autors, in denen mögliche Absichten erläutert oder zumindest angedeutet werden. Die Forschung kann also nur vergleichbare Werke heranziehen (auch heutige Ethnographien) und/oder die Schrift im Kontext ihrer damaligen Zeit sehen. Tacitus’ "Germania" ist leider einzigartig für ihre Zeit. Antike ethnographische Schriften, die keine weitere Erläuterung (Exkurs) enthalten, sind uns nicht bekannt, was die Klärung dieser zentralen Frage erschwert. Die Wissenschaft zieht deswegen auch Tacitus’ andere Werke, hauptsächlich den "Agricola", heran. Das Werk im Kontext seiner Zeit zu sehen wird dadurch erschwert, dass wir nicht viel über die damalige öffentliche Meinung wissen.

In der Forschung ist die Frage nach den Absichten Tacitus’ ein zentraler Punkt und stark umstritten. Einige Theorien dominieren diese Diskussion, können aber vermutlich nie vollständig veri- oder falsifiziert werden. Möglich ist, dass alle zu einem gewissen Teil ihre Berechtigung haben.

Möglicherweise wollte Tacitus der Dekadenz der römischen Sitten ein positives Gegenbeispiel "(Sittenspiegel)" entgegenhalten; dafür spricht, dass er die Germanen an einigen Stellen stark idealisierte. Beispielsweise stellt er der Sittsamkeit germanischer Frauen "lüsterne Schauspiele" und "Verführung durch aufreizende Gelage" in Rom gegenüber. Es findet sich sogar explizite Kritik an den römischen Verhältnissen: Tacitus macht eigene Zwietracht und Bürgerkrieg für germanische Erfolge verantwortlich.

Andere Forscher halten das Werk nicht für eine sittliche Mahnung zur Aufrichtung der römischen Moral, sondern für eine objektive Ethnographie. Diese, stellenweise stark polarisierenden, negativen und positiven Gegensätze zu Tacitus’ eigener Kultur dienten demnach lediglich "dem Verständnis des Andersartigen". Dafür spricht, dass sich viele seiner Beschreibungen als richtig herausgestellt haben und durch die moderne Archäologie bestätigt wurden.

Diskutiert wird auch, dass Tacitus womöglich aufzeigen wollte, warum Rom in jahrzehntelangen Versuchen Germanien nie vollständig erobern konnte. Der Grund sei demnach die Gesellschaftsform und der freiheitsliebende Charakter der Germanen. Neuere Deutungen gehen sogar noch weiter: Tacitus wolle nicht nur erklären, warum Germanien nicht besiegt werden konnte, sondern sogar vor weiteren Eroberungsversuchen warnen.

Die Schrift hat, zusammen mit den anderen „Kleinen Schriften“ des Tacitus, nur in einem einzigen Exemplar die Zeit des Humanismus erreicht. Es wurde von Enoch von Ascoli in der Abtei Hersfeld aufgefunden und ca. 1455 nach Italien gebracht. Als Erster hat sich Enea Silvio Piccolomini, der spätere Papst Pius II., mit der Schrift befasst. Im mittelalterlichen Deutschland spielte der Begriff "Germanen" als Selbstbezeichnung für „die Deutschen“ kaum eine Rolle, versuchte man sich doch historisch in die Nähe der Römer zu stellen.

Um Begeisterung für einen Kreuzzug gegen die Türken zu entfachen, wurde die "Germania" auf dem Regensburger Reichstag 1471 benutzt, indem die kriegerischen Eigenschaften der Germanen hervorgehoben wurden. Es waren aber erst die deutschen Humanisten, die auf Tacitus aufmerksam wurden (Conrad Celtis, Aventinus, vor allem Ulrich von Hutten). Von da an hielt das Interesse der Deutschen an dem, was sie als „ihre Urgeschichte“ betrachteten, lange Zeit an, wenngleich jede Epoche ihre eigene, jeweils unterschiedliche Auslegung hatte. Die Humanisten schwärmten für die angebliche „germanische Reinheit“ und die Ursprünglichkeit ihrer Vorfahren, in diesem Sinne diente die "Germania" einer anachronistischen Identitätsstiftung. Erst mit Jacob Grimm (und Karl Viktor Müllenhoff) kam eine wissenschaftliche Betrachtungsweise hinzu.

Bereits im 19. Jahrhundert begann aber auch die wissenschaftliche Konstruktion eines Germanenmythos durch die Altertumswissenschaften. Über Gustaf Kossinna trug diese Entwicklung mit zur Entstehung der pseudo-wissenschaftlichen Rassenlehre des Nationalsozialismus bei. Nationalsozialistische Rassenpolitiker, allen voran Heinrich Himmler und die von ihm mitgegründete „Forschungsgemeinschaft Deutsches Ahnenerbe“, entstellten und missbrauchten die Aussagen bei Tacitus als Argumente für eine angebliche „rassische Überlegenheit“ der Deutschen und ihren millionenfachen Massenmord in den NS-Konzentrations- und Vernichtungslagern.

In der neueren Forschung wird hingegen auf die problematische Rezeptionsgeschichte und die Instrumentalisierung des Inhalts der Schrift kritisch hingewiesen, zumal die Gleichsetzung Germanen/Deutsche längst nicht mehr haltbar ist. Die Behandlung durch Eduard Norden, der das Werk 1920 in das Umfeld der antiken Ethnographie gestellt hat, auch und gerade im Vergleich zu der weithin herrschenden Germanenideologie, ist immer noch grundlegend. Die moderne Forschung betrachtet die "Germania" (etwa bezüglich Intention und Quellenkritik) kritischer als die ältere und ist teilweise auch zu neuen Bewertungen gelangt.

Die "Germania" wurde in die ZEIT-Bibliothek der 100 Bücher aufgenommen.







</doc>
<doc id="261" url="https://de.wikipedia.org/wiki?curid=261" title="Albany (New York)">
Albany (New York)

Albany [] ist die Hauptstadt des US-Bundesstaates New York und Verwaltungssitz des Albany County. Laut der letzten Volkszählung im Jahr 2010 hatte die Stadt 97.840 Einwohner (Schätzung 2016: rund 98.000, U.S. Census Bureau).

Albany liegt im Nordosten der Vereinigten Staaten nahe der Mündung des Mohawk Rivers in den Hudson River und 240 Kilometer nördlich der Metropole New York City.

Albany bildet zusammen mit den nahegelegenen Städten Cohoes, Troy, Watervliet und Schenectady den "Capital District".

¹ 

Die europäische Besiedlung des Gebiets begann 1614 mit dem Bau des niederländischen Forts Nassau, das nach vier Jahren aufgegeben wurde. 1624 wurde es durch Fort Oranje ersetzt, das westlich der heutigen Dunn Memorial Bridge lag und die Aufgabe hatte, den Pelzhandel zu sichern. Fort Oranje war die erste dauerhafte Siedlung der Kolonie Neu-Niederlande. Mit der Zeit entwickelte sich Fort Oranje zu einem Handelsposten für Pelze, und so gründete Pieter Stuyvesant im Jahr 1652 die Siedlung "Beverwyck". Als die Engländer 1664 die Kolonie Neu-Niederlande eroberten, änderten sie die Namen der Siedlungen. Zu Ehren des Herzogs von York und Albany wurde aus "Nieuw Amsterdam" New York und aus "Beverwyck" Albany. Ab 1685 gehörte die Siedlung zur britischen Kronkolonie New York. Mit der "Dongan Charta" erhielt Albany im Jahr 1686 das Stadtrecht.

Im Jahr 1754 wurde der Albany-Kongress abgehalten, aus dem eine Notstandsregierung hervorging, um die Stadt gegen die Franzosen verteidigen zu können.

Als sich die britischen Kolonien im Amerikanischen Unabhängigkeitskrieg 1775 vom Mutterland losgesagt hatten, wurde das Gebiet der Kolonie New York zunächst von britischen Truppen besetzt. Die Hauptstadt der Kolonie, Kingston, wurde niedergebrannt. 1786, fünf Jahre nach Ende des Krieges, trat die Kolonie den Vereinigten Staaten bei. Albany wurde anstelle des zerstörten Kingston 1797 Hauptstadt des Staates New York.

1807 begann die Dampfschifffahrt auf dem Hudson zwischen Albany und New York City. Ab 1819 war die Stadt über den Eriekanal mit den Großen Seen Nordamerikas verbunden.

Mit der Eröffnung der ersten Eisenbahnstrecke im New York State, die Albany und Schenectady verband, begann 1831 in Albany das Eisenbahnzeitalter. Die ebenfalls 1831 in New York gebaute Lokomotive DeWitt Clinton, die erste Dampflokomotive des Bundesstaates, benötigte damals für die Strecke von Albany nach Schenectady 46 Minuten.






















Mehrere Bauwerke und historische Distrikte in Albany sind in das National Register of Historic Places eingetragen:
Nach dem Zensus 2010 hatte Albany rund 98.000 Einwohner. Die Bevölkerung setzte sich zusammen aus 63 Prozent Weißen, 28 Prozent Schwarzen, 5,6 Prozent Latinos und 3,3 Prozent Asiaten. Von den Einwohnern stammten 17 Prozent von Iren ab, 12 Prozent von Italienern und 11 Prozent von Deutschen.

Die Autobahn I-87 verbindet Albany nach Süden mit New York City und nach Norden mit Montreal in Kanada. Die Autobahn I-90 verbindet die Stadt nach Osten mit Springfield in Massachusetts und nach Westen mit Syracuse. Nordwestlich der Stadt liegt der Albany International Airport. Es gibt Zugverbindungen mit Amtrak nach Süden (New York City), nach Norden (Montreal), nach Rutland in Vermont, nach Westen zu den Niagara-Fällen, Toronto und Chicago, und östlich nach Boston.

In Albany befinden sich die State University of New York at Albany (auch SUNY Albany oder University at Albany genannt) und im Vorort Loudonville das Siena College.

Seit 1979 findet in Albany der Freihofer’s Run for Women statt, einer der bedeutendsten Frauen-Straßenläufe weltweit.

Albany ist Sitz des 1847 errichteten römisch-katholischen Bistums Albany. Die Kathedrale "Immaculate Conception" wurde 1852 fertiggestellt.



Albany hat als Partnerstädte: 
Nach Albany ist ein Marskrater benannt.



</doc>
<doc id="262" url="https://de.wikipedia.org/wiki?curid=262" title="Aja">
Aja

Aja bezeichnet:


Name folgender Personen:
Aja ist der Familienname folgender Personen:
AJA steht als Abkürzung für:


</doc>
<doc id="263" url="https://de.wikipedia.org/wiki?curid=263" title="Allgemeiner Deutscher Tanzlehrerverband">
Allgemeiner Deutscher Tanzlehrerverband

Allgemeiner Deutscher Tanzlehrerverband e. V. (ADTV) ist die weltweit größte Dachorganisation von Tanzschulen und Tanzlehrenden. Er wurde 1922 in Halle (Saale) gegründet und hat seinen Sitz heute in Hamburg. Dem ADTV gehören rund 800 Tanzschulen und 2830 Tanzlehrende in Deutschland an.


Das Geschäftsführende Präsidium besteht aus dem Präsidenten, dem Vizepräsidenten, dem Leiter der Tanzlehrer-Akademie und dem Schatzmeister.

Regional gliedert sich der ADTV in sieben Regionalverbände, die teilweise mehrere Bundesländer umfassen:

Neben dem ADTV befasst sich der "Swinging World e. V." (früher "Tanzschulinhabervereinigung im ADTV e. V.") als Vereinigung der Tanzschulinhaber um die Belange der Tanzschulen.

So sind insgesamt folgende Unternehmungen um die Belange der Mitglieder bemüht:




</doc>
<doc id="265" url="https://de.wikipedia.org/wiki?curid=265" title="Almaty">
Almaty

Almaty (kasachisch//"Almaty", in der neuen inoffiziellen Lateinschrift "Almatı"), von 1867 bis 1921 /"Werny", 1921 bis 1993 Alma-Ata – von kasachisch алма/"alma" („Apfel“) und ата/"ata" („Großvater“) – , ist mit rund 1,7 Millionen Einwohnern die größte Stadt Kasachstans. Sie liegt im Südosten des zentralasiatischen Staates unweit der Grenze zu Kirgisistan, war von 1936 bis 1991 Hauptstadt der Kasachischen SSR und nach dem Zerfall der Sowjetunion bis 1997 von Kasachstan.

Almaty ist neben der Hauptstadt Astana noch immer das kulturelle, wissenschaftliche und wirtschaftliche Zentrum des Landes mit Universitäten und zahlreichen Sakralbauten, Museen und Theatern. Zu den bekanntesten Sakralbauten gehören die Christi-Himmelfahrt- und die Nikolaus-Kathedrale. Der 1983 fertiggestellte, 371,5 Meter hohe Fernsehturm Almaty auf dem Berg Kök-Töbe gehört zu den höchsten Bauwerken der Welt. Seit Anfang des 21. Jahrhunderts sind auch einige Wolkenkratzer in der Stadt entstanden. Die Stadt verfügt über zwei Bahnhöfe und zwei Flughäfen. Die am 1. Dezember 2011 eröffnete Metro Almaty, Straßenbahnen und Oberleitungsbusse sorgen für den innerstädtischen Transport.

Almaty befindet sich im Südosten von Kasachstan; nach Süden sind es vom Stadtzentrum etwa 25 km bis zur Grenze mit Kirgisistan und nach Osten etwa 300 km zur Grenze mit China (jeweils Luftlinie).

Die Stadt liegt am Nordfuß des nördlichsten Gebirgszugs des Tian Shan – Transili-Alatau (russ. "Zailijskij-Alatau"), dessen nur schwer überwindliche Bergketten mit Gletschern vom Pik Talgar (), der in der Stadt von fast überall zu sehen ist, gekrönt werden. Der größte Berg innerhalb des Stadtterritoriums heißt Kök-Töbe und ist hoch. Zudem befinden sich in der Umgebung von Almaty etwa 300 Gletscher, von denen der Korschenewski- und Tujuksu-Gletscher die größten sind.

Nördlich von Almaty liegt ein Gebiet mit Steppen und Halbwüsten, das schließlich in die Wüste Mujunkum (russ. "Mojynkum") übergeht.

Durch die Lage nördlich des Tian-Shan-Gebirges, eines Intraplatten-Orogens mit hoher Topografie, starken Horizontalverschiebungen und einer starken Seismizität ist die Region um Almaty häufig von Erdbeben betroffen, wodurch die Stadt in den letzten 250 Jahren mehrfach zerstört wurde (1770, 1807, 1865, 1887, 1889 und 1911). 1887 wurde die Stadt Werny komplett zerstört, und wieder an derselben Stelle aufgebaut. Das Kebin-Erdbeben von 1911 forderte mehr als 700 Menschenleben, die Stadt wurde auch schnell wieder aufgebaut. 1974 wurden zur Verbesserung der Analyse der seismischen Gefährdung vier Erdbebenwarten aufgebaut. Der Fernsehturm Almaty auf dem Berg Kök-Töbe ist speziell erdbebensicher aus einer Stahlrohrkonstruktion hergestellt, auf Beton wurde verzichtet.

Die Stadt ist heute in acht autonome Bezirke "(Awtonomnyj(-e) Okrug(-a))" unterteilt: Alatau, Almaly, Äuesow, Bostandyq, Schetyssu, Medeu, Nauryzbai und Türksib. Der bevölkerungsreichste Bezirk ist Bostandyk mit rund 330.000 Einwohnern, Nauryzbai ist mit 100.000 der Bevölkerungsärmste der Bezirke. Die ersten Verwaltungsbezirke der Stadt wurden 1936 gebildet. Damals unterteilte man das Stadtgebiet in die vier Stadtteile Proletarski (rus. Пролетарский), Leninski (rus. Ленинский), Stalinski (rus. Сталинский) und Frunsenski (rus. Фрунзенский). Im Jahr 1966 kam mit dem Bezirk Kalininski (rus. Калининский) ein Weiterer und 1972 mit dem Bezirk Auesowski (rus. Ауэзовский) nochmals ein Stadtteil hinzu. Nachdem die Bevölkerung Almatys Anfang der 1980er Jahre bereits auf mehr als eine Million angewachsen war, kamen auf Beschluss der sowjetischen Behörden am 17. Oktober 1980 zwei weitere Stadtbezirke hinzu: Alatauski (rus. Алатауский) und Moskowski (rus. Московский). 2014 kam mit Nauryzbaiski (rus. Наурызбайский) der bisher letzte Stadtbezirk hinzu.

Südlich von Almaty, in Kirgisistan, liegt in den Tian-Shan-Bergen der große Yssykköl. Dieser See ist das beliebteste Ausflugsziel der Stadtbewohner. 70 km nördlich der Stadt befindet sich die Qapschaghai-Talsperre, die 1970 am Fluss Ili errichtet wurde. Mit einer Stauseefläche von 1847 km² dient sie als Hauptreservoir für die Versorgung der Metropole mit Trinkwasser.

In den nahen Bergen des Transili-Alataus entspringen zahlreiche Flüsschen, die das Stadtgebiet durchqueren, darunter Ülken Almaty und Kitschi Almaty.

Die Stadt umgeben zahlreiche blühende Gärten (vor allem natürlich Apfelbaumgärten), Obst-, Getreide-, Tabak- und Melonenplantagen sowie Weinberge. Im Gebirgsvorland finden sich Haine mit Aprikosen, Weißdorn und Wildapfel. Etwas höher beginnen stammdichte Nadelwälder, Alpenwiesen und schließlich die schneebedeckten Eisgipfel. In den Parks und Gärten der Stadt wurden von Beginn an Pflanzen und Bäume aus allen Ecken der Welt angepflanzt und so gedeihen in Almaty und Umgebung bis heute Arten aus Nordamerika, der Krim, dem Kaukasus und aus Fernost.

Auch das Tierreich um Almaty ist artenreich. Außer den üblichen Nagetieren leben in den Bergwäldern Bären und große Katzen wie Luchse sowie das Wappentier der Stadt, der Schneeleopard. Den Tian-Shan bewohnen zudem Bergziegen und -schafe (Arhare). In den Steppenregionen trifft man auf Wölfe, Rot- und Steppenfüchse.

Um die außergewöhnliche Flora und Fauna des Transili-Alataus (Zalij-Alatau) zu bewahren, wurde 1935 das „Naturschutzgebiet von Almaty“ gegründet.

Almaty hat ein ausgeprägtes Kontinentalklima mit großen Tagestemperaturschwankungen. Da die einzelnen Bezirke sich hinsichtlich ihrer Höhenlage erheblich unterscheiden, liegen sie in verschiedenen Klimazonen. So kann an verschiedenen Enden der Stadt am selben Tag ein völlig anderes Wetter herrschen.

Trotz des Kontinentalklimas ist das Klima von Almaty wesentlich milder als das in Nord- oder Zentralkasachstan. Die sommerliche Hitze wird durch die recht hohe Lage der Stadt (650–950 m über NN) gedämpft. Auch in Sommernächten kann es daher ziemlich kühl werden.

Die Winter sind in der Regel schneereich, wobei die Kälte wiederum aufgrund von warmen Luftströmungen aus den Wüsten Zentralasiens gedämpft wird.

Die in der Stadt ständig sichtbaren Berge sind – auch in den Sommermonaten – stets mit Eis und Schnee bedeckt. Wenn im Frühjahr das Tauwetter einsetzt, fließen große Mengen Schmelzwasser von den Bergen in die Stadt, weswegen Almaty für seine vielen kleinen Flüsse und Bäche bekannt ist. Das Wasser wird in zahlreichen Gräben entlang beinahe jeder Straße abgeleitet, wodurch das Stadtklima im Sommer angenehm beeinflusst wird.

Die mittlere jährliche Niederschlagsmenge beträgt 656 mm; der Temperaturdurchschnitt im Juli liegt bei 23,7 °C, im Januar bei
−5,4 °C; der Jahresdurchschnitt beträgt 9,4 °C. Die höchste je gemessene Temperatur betrug 41,7 °C (im Juli 1997), die niedrigste −37,7 °C (im Februar 1951).
Schon im 10. Jahrhundert v. Chr. gab es auf dem Territorium von Almaty Siedlungen von Menschen, wie die Funde diverser Bronzen von Ausgrabungsgebieten nördlich des Orts belegen. Seit dem 7. Jahrhundert v. Chr. finden sich kulturelle Spuren von Saken und Usunen. Das bekannteste und interessanteste Zeugnis der sakischen Kultur ist der sogenannte „Altyn Adam“, „Goldener Mensch“ von Issyk-Kurgan bei der Stadt Issyk, 48 km von Almaty entfernt. Hierbei handelt es sich um eine vollständig erhaltene, reich verzierte Rüstung aus Gold, die einem jungen sakischen Fürsten gehörte.

Die Ausgrabungen belegen, dass es auf dem Territorium von Almaty spätestens seit dem 10. Jahrhundert n. Chr. vier größere Siedlungen gab, von denen drei nach etwa zwei Jahrhunderten zur Stadt Almatu zusammenwuchsen. Dieser Name wird zum ersten Mal auf einem silbernen Dirham aus dem Jahr 684 (nach unserer Zeitrechnung 1285/86) erwähnt. Solche Münzen wurden in der Vorgängerstadt des heutigen Almaty während der mongolischen Herrschaft (Khanat Tschagatai) geprägt. Der Name Almatu erscheint außerdem in einigen arabischen Quellen und in Reisenotizen eines chinesischen Mönchs.

Im 14. Jahrhundert wurde die Stadt durch Mongolen (wahrscheinlich im Zuge der Unterdrückung einer Rebellion) fast vollständig zerstört. Da der Handel an der Seidenstraße – die Existenzgrundlage der Stadt – ebenfalls zum Erliegen kam, verkümmerten die Reste von Almatu im 16. Jahrhundert. Ihre Ruinen waren noch Mitte des 19. Jahrhunderts zu sehen, die Russen benutzten sie als Steingrube für die Stadt Werny.

Bei der Erschließung Zentralasiens errichtete das Russische Reich mehrere Vorposten in der Region, um das Territorium zu sichern. Es war außerdem vertraglich zum Schutz der Kasachen gegen die Dschungaren verpflichtet.

Am 4. Februar 1854 gründete die Truppe unter dem Kommando von Major Michail Peremyschelski (russ. Михаил Дмитриевич Перемышельский) die Festung Wernoje („Die Treue“) als militärische Befestigungsanlage am Fuße des Transili-Alatau. Die Konstruktion der Anlage wurde bereits im Herbst des Jahres abgeschlossen; sie bestand aus einem fünfeckigen Bereich, der mit einem Holzzaun umgrenzt war, und an dessen zentralem Platz sich die Trainingseinrichtungen befanden. Die Bevölkerung umfasste zu dieser Zeit 470 Offiziere und Soldaten. Am 1. Juli 1855 trafen in Wernoje die ersten vertriebenen Kasachen ein und wenig später wurde auch russischen Bauern gestattet, sich im Ort niederzulassen. Dies führte dazu, dass sich auch im nahen Umland der Festung Menschen niederließen: Die russischen Zuwanderer gründeten unweit der Festung die Staniza (Kosakensiedlung) Bolshaya Almatinskaya und unter dem Zuzug von tatarischen Handwerkern und Kaufleuten wurden die Siedlungen Malaja Almatinskaja and Tatarskaja (Taschkentskaja) sloboda errichtet. Im Mai 1859 erreichte die Einwohnerzahl bereits die Größe von 5000.

Am 11. April 1867 wurde der Festung und der umliegenden Bebauung das Stadtrecht verliehen. Gleichzeitig wurde sie in Almatinsk umbenannt, was in der Bevölkerung auf Ablehnung stieß und kurze Zeit später wieder rückgängig gemacht worden war. Die Stadt Werny (die Endung wurde entsprechend der russischen Grammatik geändert), deren Bevölkerung mittlerweile die Zahl von 10.000 übertraf, wurde zur Hauptstadt des Gebietes von Semiretschje (Siebenstromland), das das heutige Nordost-Kasachstan und Teile Kirgisistans umfasste. Man teilte die Stadt in verschiedene Wohnbereiche auf, die wiederum den zwei neu geschaffenen Stadtbezirken zugeordnet wurden. Im Stadtzentrum befanden sich ein- und zweistöckige Gebäude, am Stadtpark (heute der Park der 28 Panfilowzy) wurden Verwaltungsgebäude und öffentliche Einrichtungen errichtet.

Am 1. Juni 1887 wurde Werny von einem Erdbeben innerhalb von rund zehn Minuten fast vollständig zerstört. Von 1700 Gebäuden hielt nur eines dem Beben stand. Unter Berücksichtigung der seismischen Gefahr wurde die Stadt innerhalb kurzer Zeit wieder aufgebaut, wobei durch den Gouverneur des Oblasts Semiretschje der Bau von Gebäuden nur aus Holz vorgeschrieben wurde, da diese einem Erdbeben eher standhalten könnten, als jene aus Ziegelsteinen. Das nächste große Beben ereignete sich am 3. Januar 1911. Das Kemin-Erdbeben hatte eine Stärke von 7,8 M auf der Momenten-Magnitude und zerstörte in der Stadt rund 700 Gebäude.

Ein Beschreibung von Werny für das Jahr 1909 gibt folgenden Überblick über die Stadt:

Am Vorabend des Ersten Weltkriegs war Werny bereits eine prosperierende Stadt mit 62 Lehranstalten, Fabriken, Banken, einem Telegraphen und einem kleinen Telefonnetz. 1907 wurde die Heilige Auferstehungskathedrale fertiggestellt, das mit einer Höhe von 45 Metern lange Zeit größte Gebäude der Stadt. Die Bevölkerung zählte 1909 ungefähr 37.000 Menschen.

Wie zahlreiche Städte im einstigen russischen Zarenreich wurde auch Werny von den Kommunisten umbenannt. Die Stadt bekam aber nicht den Namen eines bolschewistischen Führers, wie etwa die kirgisische Hauptstadt (Frunse, heute Bischkek). Werny gab man eine slawisierte Form seines früheren Namens. Es wurde nach den zwei Flüssen, an denen die Festung erbaut wurde, in Alma-Ata () umbenannt. Man hat die kasachische Endung "-tu" bzw. "-ty" (Attributendung) irrtümlich als Wort "„Ata“ – Vater, Opa" verstanden. So wurde aus der „Äpfelstadt“ der „Vater der Äpfel“. Die Entscheidung wurde am 5. Februar 1921 getroffen. Die Bezeichnung ist jedoch im Kasachischen grammatisch falsch: „Alma-Ata“ ist nur eine Aneinanderreihung zweier Wörter, ohne sie in Beziehung zueinander zu setzen.

Bereits am 2. März 1927 beschlossen das Zentrale Exekutivkomitee und der Rat der Volkskommissare, die Hauptstadt der Kasachischen Sozialistischen Sowjetrepublik von Ksyl-Orda nach Alma-Ata zu verlegen. Gleichzeitig mit der Anbindung von Alma-Ata an die Turkestan-Sibirische Eisenbahn zogen die Regierungsbehörden im Mai 1929 in die neue Hauptstadt um. Mit der Verlagerung der Hauptstadt nach Alma-Ata trieben die kommunistischen Führer weitere große Infrastrukturprojekte in der Stadt voran, so wurde bereits 1935 der Flughafen Alma-Ata eröffnet und nur zwei Jahre später nahm die Straßenbahn ihren Betrieb auf. Die Regierung der Kasachischen SSR erstellte für Alma-Ata für die Jahre 1929 und 1930 einen Aktionsplan, in dem Investitionen in Höhe von 6,5 Millionen Rubel in das Wohnungswesen, 2,9 Millionen Rubel für die Errichtung von Verwaltungsgebäuden und 2,2 Millionen Rubel für Versorgungseinrichtungen vorgesehen waren. Das enorme Wachstum der Stadt, dass sie seit ihrer Ernennung zur Hauptstadt zu verzeichnen hatte, richtete sich vor allem in nördliche Richtung hin zum Bahnhof Almaty-1 und nach Westen.

1941 wurde der Stadtname im Kasachischen wieder von Alma-Ata zu Almaty geändert, russisch blieb die Bezeichnung weiterhin Alma-Ata.
In den Kriegsjahren 1941–1945 wurden viele Fabriken, Behörden und Institute aus dem von der deutschen Wehrmacht besetzten Westen der Sowjetunion nach Zentralasien verlegt. Allein nach Alma-Ata wurden über 30 Industriebetriebe, 15 Hochschulen, 20 Forschungsinstitute und die Filmstudios von Moskau, Leningrad und Kiew verlegt. So kam es, dass fast alle sowjetischen Propagandafilme der Kriegszeit in Alma-Ata gedreht wurden. Während dessen kämpften viele Stadtbewohner an der Front. 48 von ihnen wurden mit dem Abzeichen „Held der Sowjetunion“ geehrt. In Alma-Ata bestand das Kriegsgefangenenlager 40 für deutsche Kriegsgefangene des Zweiten Weltkriegs.

Nach dem Krieg wuchs Alma-Ata in jeder Hinsicht schneller als je zuvor. Ein Jahr lang lebte hier Leonid Breschnew als Erster Sekretär des ZK Kasachstans, bevor er zum Generalsekretär der Partei aufstieg, und diese Bekanntschaft kam der Stadt während seiner Regierungszeit selbstverständlich zugute. Am meisten wurde die kasachische Hauptstadt vom Republikoberhaupt Dinmuchamed Kunajew begünstigt, der von 1960 bis 1986 mit einer kurzen Unterbrechung regierte. Unter ihm bekam die Stadt im Wesentlichen ihr heutiges Aussehen und sie wurde zur Millionenmetropole. Der Name Kunajew weckt in Kasachstan selbst heute noch positive Assoziationen.

Die Perestroika fing für Alma-Ata unter keinem guten Vorzeichen an. Im Dezember 1986 hatte das Plenum der ZK der Kasachischen SSR kurzerhand entschieden, Kunajew gegen Gennadi Kolbin auszutauschen, der das Land, die Leute und die Sprache nicht kannte. Daraufhin demonstrierten junge Leute auf einer Kundgebung gegen die Willkür des Zentralkomitees, die jedoch in einen Krawall ausartete (Scheltoksan-Unruhen). Es war die erste große Demonstration in der Sowjetunion seit zwei Jahrzehnten.

Die kommunistische Führung reagierte erst verspätet mit einer militärischen Aktion zur Niederwerfung der Demonstration – „Metel’ 86“ („der Schneesturm 86“). Hunderte von Menschen wurden verhaftet, zwei Jugendliche später als Anstifter erschossen. Das Zentralkomitee beschrieb die Proteste als einen nationalistischen Aufstand. Kunajew verstarb 1993.

Fast genau fünf Jahre nach der „Metel’ 86“, am 21. Dezember 1991, unterschrieben hier die Staatschefs der 12 Sowjetrepubliken die Alma-Ata-Erklärung über die Schaffung der Gemeinschaft Unabhängiger Staaten (GUS) und besiegelten damit den endgültigen Zerfall der Sowjetunion. Alma-Ata wurde zur Hauptstadt der unabhängigen Republik Kasachstan.

1993 wurde die Stadt erneut umbenannt, in Almaty, das heißt Stadt der Äpfel oder der Apfelbäume.

Bald entschied sich die neue kasachische Führung, die Landeshauptstadt von Almaty nach Aqmola (heute Astana, früher "Akmolinsk" und "Zelinograd") zu verlegen. Das „Gesetz über den Sonderstatus von Almaty“ garantiert jedoch der Stadt den Erhalt einer besonderen Stellung als historisches Wissenschafts-, Kultur- und Finanzzentrum Kasachstans. In Almaty verblieben einige Regierungsorganisationen, 36 ausländische Botschaften und Konsulate, Vertretungen der UNO und UNESCO. Der Umzug der Hauptstadt wurde mit der Erdbebengefährdung der Region um Almaty und mit der Platzknappheit für neue Bauprojekte begründet. Eine Rolle spielte mit Sicherheit die demografische Situation im Norden Kasachstans, wo die Mehrheit der Bevölkerung russischstämmig ist. Dieses würde einer eventuellen Abspaltung entgegenwirken.

Dennoch bleibt Almaty ein wichtiges kulturelles, wissenschaftliches und wirtschaftliches Zentrum des Landes.

Bis 1997 war Almaty Hauptstadt Kasachstans und des gleichnamigen Gebiets. Jetzt allerdings ist sie keines von beidem, denn die Landeshauptstadt wurde nach Aqmola (heute Astana) und die Gebietshauptstadt nach Taldyqorghan verlegt. Obwohl das Gebiet immer noch „Almaty Oblysy“ heißt, ist Almaty kein Teil davon. Sie ist eine autonome „Stadt mit Sonderstatus“, von denen es in Kasachstan noch zwei weitere gibt – Astana und Baikonur.

Derzeitiger Bürgermeister ("Äkim") von Almaty ist seit dem 8. August 2015 Bauyrschan Baibek. Während sowjetischer Zeit stand der Stadtverwaltung der Vorsitzende des Exekutivausschusses vor. Nachfolgend die Bürgermeister der Stadt seit 1992:


Almaty unterhält 16 Städtepartnerschaften mit Städten in Afrika, Asien, Europa und Nordamerika. Die erste Städtepartnerschaft wurde 1989 mit Tucson in den Vereinigten Staaten geschlossen, die letzte 2004 mit dem bulgarischen Warna vereinbart. Außerdem unterhält der Stadtbezirk Medeu eine Partnerschaft mit Alpen am Niederrhein in Deutschland. In der folgenden Tabelle sind die Orte, mit denen Almaty Städtepartnerschaften eingegangen ist (geordnet nach Jahr der Abschließung), aufgelistet:
Seit 1981 ist Almaty eine Millionenmetropole, eine der größten Städte der ehemaligen Sowjetunion und nach Taschkent die zweitgrößte Stadt Zentralasiens. Rund 8 % der Bevölkerung Kasachstans lebt in Almaty. Das Bevölkerungswachstum betrug in den letzten Jahren ca. 0,6 %, die Sterblichkeitsrate wurde von der Geburtenrate um fast 30 % übertroffen. Zum ersten 1. Januar 2015 lebten rund 1,642 Mio. Menschen in Almaty, von denen 0,75 Mio. männlich und 0,89 Mio. weiblich waren, wodurch sich ein Geschlechterverhältnis von dementsprechend etwa 45 zu 55 Prozent (m/w) ergab. Etwa 16 Prozent der Bevölkerung waren Kinder bis neun Jahre. Rund ein Viertel der Einwohner waren unter 20 Jahre alt. Zu den „Aksakalen“, wie die ehrbaren alten Männer und Frauen von über 80 Jahren in Kasachstan genannt werden, zählen ungefähr 1,5 %.

2003 sind 5630 Menschen ins Ausland verzogen, 839 davon nach Deutschland. Zugezogen sind 5496 Menschen, davon 1174 aus China und 46 aus Deutschland. Der Saldo der inneren Migration betrug + 18.658 Menschen.

Almaty ist eine internationale und kosmopolitische Stadt, in der Vertreter von annähernd 120 Nationen leben. Bis vor kurzem stellten Russen die Mehrheit in der Stadtbevölkerung dar – was bei den meisten Großstädten Kasachstans der Fall war. In der Zarenzeit waren laut der ersten Bevölkerungszählung von 1897 sogar 58 % der Bewohner russisch. Die Kasachen kamen mit nur 8,2 % sogar erst an dritter Stelle hinter den Uiguren mit 8,7 %. Bedingt durch die geographische Nähe des Uigurischen Autonomen Gebiets Xinjiang in der Volksrepublik China, zählen sie auch heute noch zur drittstärksten Minderheit der Stadt.

Heute sind die Russen mit 33 % und die Kasachen mit 51,1 % vertreten. Traditionell leben im Umkreis von Almaty die Kasachen des Älteren Shus, Sippe Dshany des Stammes Dulat bzw. Sippen Tschibyl und Aikym des Stammes Schapraschty. In der Stadt selbst ist die Stammeszugehörigkeit der kasachischen Bevölkerung höchst unterschiedlich und teilweise gar nicht mehr ermittelbar. Außer den Uiguren sind viele andere Turkvölker vertreten, wie Aserbaidschaner, Türken, Tataren und die ostturkestanischen Dunganen oder Dschungaren. Stärkste slawische Minderheit nach den Russen sind die Ukrainer mit 1,2 %. Viertgrößte Bevölkerungsgruppe mit einem Anteil von 1,9 % in Almaty sind Koreaner, die in Kasachstan und anderen Staaten der ehemaligen Sowjetunion sich selbst auch als Korjo-Saram bezeichnen und unter Stalin aus dem russischen fernen Osten nach Zentralasien verschleppt wurden. Der Anteil der Deutschen beträgt nur noch 0,6 %. Vor 15 Jahren gab es weitaus mehr Deutsche in Almaty, doch auch damals zählten sie kaum mehr als 3 %, denn deren Hauptsiedlungsgebiete waren vorwiegend Nord- und Zentralkasachstan.

Bei diesen Zahlen ist die starke zwischenstaatliche Migration zu beachten, die durch den Fall der Sowjetunion ausgelöst wurde. Nach deren Auflösung haben viele Russen, Ukrainer, Deutsche und Juden die Stadt und häufig auch das Land verlassen. Seit dem Ende der 1990er Jahre stabilisierte sich die Wirtschaftslage jedoch und die Migration ebbte deutlich ab. In den letzten Jahren kehrte sich der Trend sogar vollständig um, es wanderten zahlreiche Menschen aus ärmeren ehemaligen Sowjetrepubliken nach Almaty ein.

Die Hauptverkehrssprache ist, wie in den meisten Teilen Kasachstans, Russisch. Die Regierung bemüht sich seit der Unabhängigkeit aber, die kasachische Sprache ebenso wieder in der Bevölkerung zu verbreiten. Straßenschilder oder offizielle Dokumente sind meist zweisprachig. Daneben sind auch die Sprachen der jeweiligen Minderheiten in der Stadt verbreitet.

Almaty hat heute rund 1,7 Millionen Einwohner und ist somit die größte Stadt Kasachstans. Hatte die Stadt zur Zeit ihrer Gründung nur rund 400 Einwohner, stieg ihre Bevölkerungszahl bis zum Mai 1859 bereits auf 5.000 Einwohner an. In den folgenden Jahren stieg die Einwohnerzahl weiter an und erreichten 1913 bereits die Marke von 40.000 Einwohnern. Die Volkszählung 1926 ergab für das damalige Alma-Ata eine Bevölkerungszahl von 45.600 Menschen; nur 13 Jahre später zählte die Stadt mehr als 200.000 Einwohner. Seit 1982 ist Almaty eine Millionenstadt. Der rasante Bevölkerungsanstieg der letzten Jahre ist vor allem auf die Erweiterung der Stadtgrenzen zurückzuführen, wodurch viele umliegende Orte ins Stadtgebiet eingegliedert wurden und die Einwohnerzahl im Jahr 2014 um beinahe 150.000 zunahm.

¹ Volkszählungsergebnis

Die meisten Kasachen sind Muslime und gehören der sunnitischen Glaubensrichtung an. Der Großteil der Kasachen nahm ab dem 14. Jahrhundert unter dem Einfluss der Goldenen Horde den Islam an. Im Zusammenhang mit der Perestroika kam es Ende der 1980er Jahre auch bei den Muslimen Kasachstans zu einem Reformprozess. Im November 1989 kamen zahlreiche Imame in Almaty zusammen und gründeten die „Geistliche Verwaltung der Muslime Kasachstans“ ("Dukhovnoe upravlenie musul'man Kazakhstana" DUMK). Die Delegierten der DUMK wählten im Januar 1990 fünf Qazis, von denen einer für Almaty zuständig war. Die DUMK gründete außerdem 1990 ihr Höheres Islamisches Institut in Almaty, das eine zweijährige religiöse Ausbildung anbot, die im Jahre 1996 bereits 300 Studenten durchlaufen hatten. Die DUMK errichtete darüber hinaus im Stadtgebiet neue Moscheen, so auch die große Zentralmoschee, die 1997 eröffnet wurde. In den 1990er Jahren kam es bei den Muslimen Kasachstans zu einer Annäherung an die arabisch-islamischen Staaten. So unterstützten bereits 1997 arabische Stiftungen den Aufbau der Kasachisch-Arabischen Universität und der Kasachisch-Kuwaitischen Universität in Almaty. Nachdem die DUMK ihr Höheres Islamisches Institut geschlossen hatte, eröffnete sie als Ersatz dafür im Jahre 2001 die "Ägyptische Universität für Islamische Kultur Nur-Mubarak", ein Kooperationsprojekt mit dem ägyptischen Ministerium für religiöse Stiftungen. Diese Hochschule beherbergt ebenfalls eine Moschee.

Die meisten Christen in Almaty sind Mitglieder der Russisch-Orthodoxen Kirche. Die bedeutendsten russisch-orthodoxen Kirchengebäude sind die Anfang des 20. Jahrhunderts errichtete Christi-Himmelfahrt-Kathedrale, die eine der größten Sehenswürdigkeiten der Stadt ist, und die Nikolaus-Kathedrale. Almaty ist auch zweiter Sitz der russisch-orthodoxen Eparchie Astana und Almaty. In Almaty befindet sich außerdem eine Gemeinde der römisch-katholischen Kirche, die vorwiegend aus den deutschen und polnischen Minderheiten besteht. Die Stadt ist zudem Sitz des Bistums Allerheiligste Dreifaltigkeit zu Almaty mit der Dreifaltigkeitskathedrale. Eine kleine Gemeinde der Armenisch Apostolischen Kirche wurde erst 1994 gegründet. Sie unterhält mit der Kirche des Heiligen Karapet eine von nur drei armenisch apostolischen Kirchen in ganz Zentralasien; die zwei anderen befinden sich in Samarkand und Taschkent in Usbekistan. Die Gemeinde gehört zur armenisch apostolischen Diözese Neu-Nachitschewan und Moskau.

Des Weiteren gibt es eine hohe Anzahl an evangelischen Freikirchen, deren Einfluss sich bereits im 19. Jahrhundert erkennbar machte. Die katholischen Gemeinden bestehen meist aus Deutschen und Polen, die nach dem Zweiten Weltkrieg dort ansiedelten.

Das Kulturleben der südlichen Hauptstadt Kasachstans ist für zentralasiatische Verhältnisse außerordentlich reich und vielfältig.

Von den Theatern besitzt die Stadt neun staatliche und sieben nichtstaatliche. Das "Staatliche Akademische Theater für Oper und Ballett", das den Namen Abai – nach Abai Qunanbajuly – trägt, ist das älteste und bedeutendste nicht nur der Stadt, sondern auch der Republik. Erwähnenswert sind außerdem noch das "Staatliche Akademische Äuesow-Theater", das Staatliche Akademische "Lermontow-Theater für Drama", das "Theater 'Nowaja Szena'," die Nationalen Theater der Russen, Deutschen, Koreaner und Uiguren, sowie drei "Puppentheater".

Es gibt folgende städtische Museen in Almaty:

Das älteste noch erhaltene Gebäude der Stadt stammt von 1892. Dort residierte einst ein Kinderpflegeheim, heute ist dort das städtische Medizinkolleg. Im einst prunkvollen, reichen Stil erbaut ist das Geschäftshaus des Kaufmanns Gabdulwalijews, der heute den Namen „Kysyltan“ trägt. Typische Vertreter des russischen Kolonialstils des 19. Jahrhunderts sind die Städtische Lehranstalt, das Jungengymnasium, das Mädchengymnasium und das Offiziershaus im Park der 28 Panfilowzy (heute: Museum der Volksmusikinstrumente).

In den 1930er und 1940er Jahren errichtete man viele ansehnliche Gebäude, darunter das Akademische Theater für Oper und Ballett, die ehemaligen Häuser der Regierung und des Finanzministeriums. Die Akademie der Wissenschaften, das Kinder- und Jugendtheater, sowie der Bahnhof Almaty II folgten in den 1950er Jahren. 1970 wurde der Palast der Republik und in den späten 1970er Jahren das Hotel Kasachstan errichtet, das mit seinen knapp 130 Metern lange Zeit das höchste Gebäude Kasachstans war.

Der Fernsehturm Almaty, gelegen auf dem Berg Kök-Töbe, ist mit seinen 371,5 Metern eines der höchsten Bauwerke der Welt.

Die Hauptsehenswürdigkeit der Stadt ist die Heilige Christi-Himmelfahrt-Kathedrale (Sofienkathedrale von Turkestan), der 1907 erbaute Sitz des Bischofs von Turkestan. Die Kathedrale ist im „russischen“ Stil erbaut worden, deren Formen, Ornamente und die helle Farbenfröhlichkeit an die Terems erinnert, die alten russischen Paläste (ein Beispiel hiervon ist der Terem-Palast im Moskauer Kreml). Diese Kathedrale mit ihren Gewölben, Kuppeln, dem Glockenturm und einem System von Treppen und Galerien wird häufig mit der Basilius-Kathedrale in Moskau verglichen, der man die Züge des Barock verliehen hat. Die Kirche wurde in den russischen Katalog der 100 Weltwunder aufgenommen.

Die Kathedrale wurde vom Architekten A. P. Senkow geschaffen. Die Stadt Almaty (damals Werny) liegt in einem besonders erdbebengefährdeten Gebiet. Im Jahre 1887 geschah hier ein Erdbeben, das zehn Minuten dauerte und die ganze Stadt in Schutt und Asche legte. Damals merkte man, dass die Bauten aus Holz am wenigsten Schaden davontrugen, und so ist die Kathedrale vollständig aus Holz (genauer gesagt aus dem Tannenholz von Tian-Shan) errichtet worden. Senkow verwendete beim Bau nicht nur die neuesten architektonischen Erkenntnisse der damaligen Zeit, sondern er richtete sich auch nach historischen Vorbildern, wie die in seismisch aktiven Gebieten stehenden japanischen Pagoden. Das Ergebnis war, dass die Turkestan-Kathedrale als eines der wenigen Gebäude die zwei großen Erdbeben von 1910 und 1921 unversehrt überstand. Überraschenderweise ist die Kirche kein einziges Mal in Flammen aufgegangen und bleibt somit eine der wenigen vollständig erhaltenen hölzernen Sakralbauten der Welt.

Aus der Zarenzeit erhalten geblieben sind außer der Christi-Himmelfahrt-Kathedrale noch die Nikolaus-Kathedrale, die Peter-und-Paul-Kirche und die Kasaner Kathedrale. Die letzte ist in einem an ukrainisches Barock erinnernden Stil erbaut. In den 1990er Jahren wurde, in Anlehnung an die Moskauer Kathedralen, die orthodoxe Christus-Erlöser-Kathedrale erbaut. 2007 wurde die Sophienkathedrale neu erbaut. Im Norden der Stadt befindet sich die Paraskewi-Kirche.

Alle heute in Almaty stehenden Moscheen wurden erst in den 1990er Jahren erbaut. Besonders schön sind die Zentralmoschee, die Sultan-Kurgan-Moschee, die Moschee am Ryskulow-Prospekt, die Moschee im Orbita-Stadtviertel und die Nur-Mubarak-Moschee der Islamischen Universität. Die Tatarische Moschee des alten Werny ist hingegen nicht erhalten geblieben.

Des Weiteren gibt es eine moderne römisch-katholische Kathedrale der Heiligen Dreifaltigkeit.

Almaty verfügt außerdem über 13 "Ausstellungshallen" und "Kunstgalerien" (Zentrale Ausstellungshalle, „Tengri-Umaj“, „Tribuna“, „Ular“, Art-Zentrum „Alma-Ata“, Salon der Kunst und Numismatik etc.) und 12 "Kinos". Der "Zirkus Almaty" ist der älteste Zirkus in Kasachstan.

In der Zeit, als Almaty noch Hauptstadt war, entstanden die Berufsverbände der Künstler, Schriftsteller, Komponisten und Journalisten.

Der Fußballverein Kairat Almaty war zu Zeiten der Sowjetunion der Vorzeigeklub der Kasachischen SSR. Mit insgesamt 24 Spielzeiten in der höchsten Spielklasse der Sowjetunion ist Kairat Spitzenreiter für Mannschaften aus den zentralasiatischen Teilrepubliken und liegt in der ewigen Tabelle der Sowjetischen Liga auf dem vierzehnten Platz, knapp vor Pachtakor Taschkent. Heute nimmt der Verein am Spielbetrieb der Premjer-Liga teil, in der bereits zweimal die Meisterschaft gefeiert werden konnte. Darüber hinaus gewann Kairat siebenmal den Kasachischen Fußballpokal. Seine Heimspiele trägt der Verein im Zentralstadion von Almaty aus; er ist aus dem Sportverein Dinamo Alma-Ata hervorgegangen. In der zweiten kasachischen Liga ist die Stadt durch Zesna Almaty vertreten. In der Stadt sind auch die Frauenfußballmannschaft von CSHVSM Almaty und der Futsalverein MFK Kairat Almaty beheimatet.

Die Eishockeymannschaft HK Almaty nimmt an der Kasachischen Meisterschaft teil. Im Fraueneishockey wird die Stadt durch den Club Aisulu Almaty vertreten. Der Basketballverein BC Almaty spielt in der kasachischen National League.

Etwa 16 km südlich von Almaty befindet sich die international bekannte Eisschnelllaufbahn von Medeo. Die Bandy-Weltmeisterschaft 2012 wurde dort ausgetragen. Am 14. und 15. Februar 2015 fand in Medeo der WM-Grand-Prix von Kasachstan im Eisspeedway statt.

In Almaty stehen auch die einzigen Skisprungschanzen in Kasachstan. Ursprünglich gab es eine K 15, eine K 35, eine K 45 und eine K 70 Schanze, die aber abgerissen wurde. Für die Winter-Asienspiele 2011 wurden eine Normalschanze (K 95) und eine Großschanze (K 125) gebaut. Ende September 2010 wurden die beiden Schanzen mit einem Continental-Cup-Springen eröffnet. Beide Springen gewann der Pole Kamil Stoch. Am 30. August 2011 fand hier ein Sommer-Grand-Prix statt, den der Slowene Jurij Tepeš gewann. Der Schanzenkomplex heißt Gorney Gigant. Almaty bewarb sich um die Austragung der Nordischen Skiweltmeisterschaften 2019 sowie – ebenso vergeblich – um jene der Olympischen Winterspiele 1994, 2014 und 2022.

Die Stadt der Äpfel besitzt zahlreiche Parks und Erholungsanlagen.


Almaty ist das wirtschaftliche Zentrum Südkasachstans und neben Astana eines der wirtschaftlichen Zentren des Landes. Das Bruttoinlandsprodukt betrug im Jahr 2013 rund 42,6 Mrd. US-Dollar und betrug somit mehr als das Doppelte der Wirtschaftsleistung der Hauptstadt Astana. Der Anteil an der Gesamtwirtschaftsleistung Kasachstans belief sich auf 19 Prozent. Der überwiegende Teil der Wirtschaftsleistung stammt aus dem Dienstleistungssektor (hier vor allem Großhandel, Einzelhandel, Informations- und Kommunikationswesen, Transport und Lagerwesen sowie Immobilienwirtschaft) und nur rund zehn Prozent entfallen auf die Industrie. Der Landwirtschaftssektor hat mittlerweile keine Bedeutung mehr für Wirtschaftsleistung von Almaty. Auch der Tourismus und das Finanzwesen tragen einen bedeutenden Teil bei; beinahe ein Drittel aller Beschäftigten im kasachischen Finanzsektor sind in Almaty beschäftigt. Der Stadt kommt als Verteilungszentrum für Güter und als Drehkreuz für internationale Organisationen und Unternehmen eine wichtige Rolle in der regionalen Wirtschaft zu.

Das monatliche Einkommen pro Kopf beträgt rund 169.000 Tenge und ist damit wesentlich höher als der Landesdurchschnitt. Die Anzahl der Erwerbstätigen in der Stadt im ersten Quartal 2016 betrug 889.047 Personen, was zu einer Arbeitslosenquote von 5,3 Prozent führt.

Die Struktur der industriellen Produktion sieht wie folgt aus:
Am 1. Januar 2005 waren in der Stadt 1668 Betriebe registriert, davon 168 Groß- und Mittelbetriebe, auf die fast 78 % der städtischen Produktion entfällt. Die Palette der erzeugten Industriegüter ist sehr breit. Die Nahrungsmittelindustrie produziert Tee, Weine, Süßwaren, Nudeln, Milch- und Fleischprodukte; in anderen Bereichen werden Waschmaschinen, Fernseher, Teppiche, Lederschuhe, Trikotagen, Ziegelsteine, Metallkonstruktionen und vieles mehr hergestellt.

Der Außenhandel umfasste 2004 5.294,6 Mio. US$, die Quote der Arbeitslosen lag bei 8,9 %, die Durchschnittslöhne erreichten 192 US$ pro Monat. Über 577.000 Menschen in der südlichen Hauptstadt waren 2003 erwerbstätig.

Das Mega Center Alma-Ata ist eines der größten Einkaufszentren in ganz Kasachstan. Zudem beherbergt Almaty das größte Einkaufszentrum in Zentralasien, die Aport Mall.

Almaty ist der größte Messestandort Kasachstans. Das Atakent Expo Exhibition Centre, in dem die Messen stattfinden, ist das einzige Messezentrum im ganzen Land. Die bedeutendsten Messen, die in Almaty veranstaltet werden, sind die WorldFood Kazakhstan, die KazBuild, die Kazakhstan International Healthcare Exhibition, die Kazakhstan International „Oil & Gas“ Exhibition, die MiningWorld Central Asia und die Kazakhstan International Tourism Fair.

Ein wichtiger Wirtschaftszweig ist die Lebensmittelherstellung. So befinden sich die meisten bedeutenden Lebensmittelhersteller in der Stadt. Unter diesen sind Rakhat, der größte Süßwarenhersteller Kasachstans. Auch der Spirituosenhersteller Bacchus und RG Brands, ein Getränkehersteller, sind in Almaty ansässig.

Almaty ist auch Kasachstans wichtigster Finanzstandort und das Versicherungszentrum des Landes. So haben, wie die Kazkommertsbank, die Halyk Bank, die ATFBank und die BTA Bank, fast alle bedeutenden Kreditinstitute Kasachstans ihren Hauptsitz in Almaty.

Trotz der Verlegung der Hauptstadt nach Astana sind dennoch einige Staatsunternehmen wie etwa die Kazpost, Kazatomprom oder Air Astana in Almaty geblieben. Auch die Einzelhandelsunternehmen Meloman, die ABDI Company und Sulpak haben ihren Hauptsitz in Almaty.

Im Süden der Stadt befindet sich die Kasachische Börse, die einzige Börse im Lande. Almaty ist ebenfalls Standort der meisten an der Börse gehandelten Unternehmen, wie etwa Kasachstan Kagazy, Almatyenergosbyt und KazTransCom.

Gleichzeitig ist die ehemalige Hauptstadt der größte Medienstandort des Landes. Zahlreiche kasachische Fernsehsender, Rundfunksender und Zeitungen sind in der Stadt angesiedelt.

Der öffentliche Nahverkehr in Almaty wird zum größten Teil durch das kommunale Unternehmen Almatyelektrotrans (AET) organisiert. Es betreibt mehr als 100 Buslinien sowie derzeit acht Oberleitungsbuslinien. Da nach dem Zerfall der Sowjetunion nicht genügend finanzielle Mittel für den Betrieb und die Unterhaltung der öffentlichen Verkehrssysteme bereitgestellt wurde, verschlechterte sich der Zustand des Oberleitungsbus- und Straßenbahnsystems zunehmend. So wurde das Liniennetz in den letzten Jahren stetig verkleinert, sodass von den ursprünglich 25 O-Bus-Linen nur noch acht in Betrieb sind. Der Großteil des ÖPNV wird nach wie vor durch die zahlreichen Busse erbracht, die mittlerweile vollständig privatisiert sind. Um die Kontrolle über die Entwicklung des innerstädtischen Transports zu verbessern, begann die Stadt in den späten 1990er Jahren das Bussystem an private Anbieter auszugliedern. Dies führte allerdings nur zu einer Verschlechterung der Qualität im Nahverkehr, da die privaten Betreiber oft kleine Fahrzeuge mit wenigen Sitzen einsetzten. Um dieser Entwicklung entgegenzuwirken, führte man 2005 strengere Anforderungen für Betreiber ein, um Minibusse aus dem städtischen Verkehr wieder zu verbannen. Unter Führung des Entwicklungsprogramms der Vereinten Nationen begann 2013 die Optimierung und Modernisierung des Transportsystems der Stadt, was unter anderem die Anschaffung neuer Busse und die Optimierung des Liniennetzes vorsieht. Von den beiden Busstationen Sairan und Saychat bestehen regionale und überregionale Verbindungen.

Seit 1944 gibt es in Almaty, wie in vielen Städten der Sowjetunion auch, Oberleitungsbusse. In seiner größten Ausdehnung umfasste das Liniennetz des Oberleitungsbus Almaty eine Länge von 220 Kilometern auf 25 Linien; es wurde aber insbesondere in den 1990er Jahren massiv verkleinert und umfasst heute nur noch acht Linien. Nachdem in den letzten Jahren alle anderen Oberleitungsbussysteme in Kasachstan den Betrieb einstellten ist jenes in Almaty das letzte verbliebene System in Kasachstan. Bis 2015 betrieb AET auch ein Straßenbahnnetz, bestehend aus zwei Linien, deren Betrieb aufgrund maroder Infrastruktur und häufigen Unfällen mit Straßenbahnen bis auf weiteres eingestellt wurde.

Seit 2011 gibt es in der Stadt auch eine U-Bahn. Mit der Metro Almaty, deren Bau bereits 1988 begonnen hatte und deren Eröffnung mehrfach verschoben wurde, gibt es das erste Untergrundbahnnetz Kasachstans. Es verfügt über zunächst eine Linie mit neun Stationen, eine Erweiterung um zwei weitere Stationen ist für 2019 geplant.

Seit 1935 besteht der Flughafen Almaty. 1977/78 führte von Alma-Ata nach Moskau die erste Überschall-Flugverbindung der Welt (mit Tupolew Tu-144 als Passagierflugzeug). Heute gibt es verschiedene, tägliche internationale Flugverbindungen. Die Regionalflughäfen im Lande werden ebenfalls täglich angeflogen. Nordwestlich der Stadt liegt noch der kleine Flughafen Almaty-Boraldai.
Ab Frankfurt am Main gibt es eine tägliche Verbindung durch die Lufthansa.

Seit der Fertigstellung der Turksib (1930) ist Almaty auch per Zug erreichbar. Heute besitzt die Stadt zwei große Bahnhöfe, Bahnhof Almaty-1 und Bahnhof Almaty-2; außerdem noch einige kleinere Stationen der Regionalbahnen. Von Moskau erreicht man Almaty in vier Tagen ohne umzusteigen.

In Almaty gibt es eine Vielzahl von Schulen und Hochschulen. Neben 187 Mittelschulen gibt es 16 Gymnasien und Lyzeen. Den mittleren Berufsabschluss können junge Stadtbewohner an 21 Staats- und Republikskollegien bzw. 57 privaten Kollegien erwerben. Es gibt eine intensive Zusammenarbeit mit Schulen in Deutschland und Frankreich.

Almaty verfügt über 13 Universitäten, von denen aber viele eher den Standards deutscher Fachhochschulen entsprechen:
Weiterhin existieren nicht wenige Fachhochschulen und Akademien, von denen folgende erwähnenswert sind:

Wissenschaftliche Forschung konzentriert sich an der 1946 von Kanysch Imantajewitsch Satpajew gegründeten "Kasachischen Akademie der Wissenschaften." Sie ist auch die oberste wissenschaftliche Anstalt Kasachstans. In den Bergen bei Almaty stehen mehrere "Sternwarten" (Assy-Turgen Observatory, Tian Shan Observatory), die vom Astrophysikalischen Institut Fesenkow betrieben werden.


"Söhne und Töchter:"
Almaty ist Geburtsort zahlreicher prominenter Persönlichkeiten.



</doc>
<doc id="267" url="https://de.wikipedia.org/wiki?curid=267" title="Aldi">
Aldi

Aldi ist der Kurzname der beiden Discounter "Aldi Nord" und "Aldi Süd". Der Name „Aldi“ ist ein Akronym und steht für Albrecht Diskont. Aldi Nord und Aldi Süd sind zusammen nach Bruttoumsatz (Stand 2017) die erfolgreichsten Discounter-Konzerne weltweit.

Karl Albrecht sen. (1886–1943), der Vater von Theo (1922–2010) und Karl Albrecht (1920–2014), war ein gelernter Bäcker, bis er aus gesundheitlichen Gründen (Bäckerasthma) diese Arbeit aufgeben musste. Im Frühjahr 1913 machte er sich als Brothändler selbstständig, und seine Frau Anna Albrecht (geb. Siepmann) eröffnete unter dem Namen ihres Mannes am 10. April 1913 einen Tante-Emma-Laden in Essen-Schonnebeck (Huestraße 89). Wein wurde in den Anfangsjahren noch aus Fässern in Flaschen abgefüllt, Zucker und Mehl gab es aus Säcken. Die Kunden wurden noch persönlich von Verkäuferinnen bedient. Selbstbedienung war zu dieser Zeit noch völlig unüblich.

Nach dem Zweiten Weltkrieg übernahmen Karl und Theo 1945 den elterlichen „Tante-Emma“-Laden und gründeten die Albrecht KG. Das Warenangebot zu dieser Zeit bestand hauptsächlich aus Lebensmitteln. Aktionsware wurde nicht angeboten. Die Albrecht-Brüder expandierten immer mehr und eröffneten 1953 das erste Regionallager mit eigenem Bürogebäude in Essen-Schonnebeck. 1955 hatte die damalige "Albrecht KG" schon 100 Filialen (alle in Nordrhein-Westfalen).

Angeblich wegen des Streites, Zigaretten ins Sortiment aufzunehmen, beschlossen 1961 die Gebrüder Albrecht, fortan getrennte Wege zu gehen: sie teilten das Unternehmen "Albrecht KG" in "Aldi Nord" und "Aldi Süd" auf. Die nördlichen Filialen übernahm Theo Albrecht und verkaufte fortan auch Zigaretten, die südlichen Karl Albrecht (erst seit 2003 gibt es Zigaretten bei "Aldi Süd" zu kaufen). 1961 (im Jahr der Aufteilung) betrieben Karl und Theo Albrecht schon 300 Filialen in West-Deutschland mit einem Umsatz von ca. 90 Mio. DM. Zu dieser Zeit existierten bereits zwei getrennte Verwaltungen und Regionallager (von Theo Albrecht in Herten, von Karl Albrecht in Mülheim an der Ruhr), im selben Jahr schied Anna Albrecht, die Mutter der Gebrüder Albrecht, als Gesellschafterin aus.

Durch die in Westdeutschland aufkommende Selbstbedienung im Lebensmitteleinzelhandel stagnierten Anfang der 1960er Jahre bei den Albrecht-Brüdern die Umsätze in den etwa 300 sehr kleinen Bedienungsläden (auch Stubenläden genannt). Dieser Vertriebstyp hatte keine Zukunft mehr und verlor Umsätze an die großen Supermarktketten wie Edeka und Rewe. Karl und Theo Albrecht wandten sich ab 1960 ebenfalls dem Vertriebstyp Supermarkt zu und experimentierten mit etwa 20 bis 30 Albrecht-Supermärkten. Die Läden hatten eine Verkaufsfläche von etwa 150 bis 200 m² und führten neben einem mittelgroßen Sortiment von Trockenwaren auch Frischwaren wie Obst und Gemüse, Molkereiprodukte, Wurstwaren und Frischfleisch (bei Frischfleisch stützten sich die Albrecht-Brüder auf die Großfleischerei RUOS aus Essen als Partner). Der Test mit diesen Albrecht-Supermärkten scheiterte, da er weder in den Ladengrößen noch in der Sortimentsvielfalt der inzwischen davongeeilten Vollsortimenter-Konkurrenz ebenbürtig war. Diese noch unter dem roten Albrecht-Logo getesteten Märkte wurden bald wieder geschlossen bzw. konnten kurze Zeit später nach Umgestaltung auf Aldi-Discount genutzt werden.

Ein weiterer, ebenfalls nicht erfolgreicher Ausweg wurde 1961 durch einen Test mit dem aus den USA kommenden Vertriebstyp Cash & Carry an den Standorten Neuss und Mülheim/Ruhr unter dem Logo ALIO gestartet. Dieser Test erscheint aus heutiger Sicht ebenfalls als eher halbherzig, denn mit nur etwa 2000 bis 3000 m² Verkaufsfläche konnte sich Alio nicht gegen die bereits etablierte C&C-Konkurrenz, wie z. B. gegen die Märkte des damaligen Marktführers und Hauptkonkurrenten RATIO in Bochum und Münster, durchsetzen, die etwa zehnfache Fläche und viel größeres Sortiment hatten. Die Albrecht Lebensmittelmärkte befanden sich in den frühen 1960er Jahren in einer Krise, aus der die Tests mit Supermärkten und Cash & Carry-Märkten keinen Ausweg gebracht hatten. Unter dem Zwang zur Neuorientierung entwickelten Karl und Theo Albrecht die Idee "Lebensmittel-Discount"; sie gaben ihren Läden dieser für Europa völlig neuen Vertriebsform den Namen „ALDI“ ("AL"-brecht "DI"-scount).

Der betriebswirtschaftliche Grundgedanke zu diesem neuen Vertriebstyp lässt sich mit dem Satz „Discount ist die Kunst des Weglassens“ umschreiben. Im Vergleich zu den damals marktführenden Supermärkten ließen die Brüder Albrecht eine ganze Reihe der damals üblichen Dienstleistungs-Funktionen der Einzelhandels-Distribution einfach weg. Aldi-Fazit: Keine breiten und tiefgestaffelten Sortimente (nur schnelldrehende Grundnahrungsmittel, keine Doubletten), keine leicht verderblichen Frischwaren (damit keine kostenintensive Warenpflege, keine Bedienung, keine teuren Kühlmöbel, geringer Energieverbrauch), kein Preisetikett auf jedem Artikel (die Kassiererinnen hatten die Preise, zusammengefasst in relativ wenigen Preisgruppen, auswendig zu lernen, später über PLU-Nummern aufzurufen), kein Auspacken der Ware (verkauft wurde aus den aufgeschnittenen Versandkartons), keine teure Ladeneinrichtung (verkauft wurde von Paletten oder selbstgefertigten Holzregalen), keine Ladendekoration und Werbung, kein Kreditverkauf, keine damals üblichen Rabattmarken. Das knapp bemessene Filial-Personal wurde für alle anfallenden Arbeiten ausgebildet, so dass es bei hoher Arbeitsdichte ständig ausgelastet war. Dieses Weglassen von wesentlichen Einzelhandelsfunktionen brachte den Aldi-Märkten große Kostenvorteile gegenüber der Supermarkt-Konkurrenz. Diese Kostenvorteile ermöglichten es Aldi, trotz eines von Anfang an gut kalkulierten Gewinns den Verbrauchern große Preisvorteile zu bieten.

Die ersten Versuche mit solchen Discount-Läden fanden 1961 im Raum Dortmund und Bochum statt. In Serie ging der Vertriebstyp „Discount“ bei Aldi Süd (Leitung Horst Steinfeld, Geschäftsführer Aldi Mülheim). Die Organisation der Eröffnungen und die Führung dieser ersten Aldi-Filialen verantwortete Walter Vieth (damals Leiter des Bezirks westliches Ruhrgebiet/Niederrhein). Unter Verwertung ehemaliger Albrecht-Supermärkte wurden ab Juni 1962 im Wochenrhythmus und in dieser Reihenfolge die ersten ALDI-Discountmärkte eröffnet: Dinslaken (Neustraße), Walsum (Friedrich-Ebert-Straße), Bocholt (Nordstraße) und Wesel (Hohe Straße).

Die Verbraucher nahmen die neuen, sehr preiswerten Aldi-Märkte in kürzester Zeit an. Die Umsatzleistung pro Mitarbeiter war fast zehnmal höher als in den Albrecht-Supermärkten. Die Umsatz- und Renditewerte der ersten Serienmärkte und die schnelle Akzeptanz dieser Läden bei den Verbrauchern waren so überzeugend, dass die Albrecht-Brüder wenige Monate nach Eröffnung dieser ersten Märkte in die überregionale Multiplikation gehen konnten. Das für diese wohl einmalige Expansion notwendige Kapital erwirtschaftete das Discount-System selbst. Durch den raschen Warenumschlag (circa zehn Tage – „Schnelldreher“), die Barzahlung in den Läden und das übliche Zahlungsziel bei den Herstellern (30 Tage) war stets genügend Liquidität vorhanden, die Expansion ohne Bankkredite zu finanzieren. Die Einführung einer neuen Logistikstruktur zur schnellen Versorgung der Märkte mit einem großen Lager in Eichenau förderte die Entwicklung. Die Zentrale in Eichenau wurde dann auch Sitz der Familienstiftungen.

Karl und Theo Albrecht gelang mit der Idee Aldi die wohl erfolgreichste Einzelhandels-Innovation des 20. Jahrhunderts. Aldi ist bis heute (2014) Marktführer in Deutschland und weiteren Ländern. Sortiment und Filialgröße wurden vorsichtig erweitert; das Prinzip „Discount ist die Kunst des Weglassens“ gilt nach wie vor.

Aldi Süd begann 2016, seine deutschen Filialen mit einer hochwertigeren Ausstattung und verbesserter Aufenthaltsqualität auszustatten. Solche „Luxus-Filialen“ werden bereits in Amerika und Australien betrieben, in Deutschland entstand die erste Filiale dieser Art in Unterhaching. Bis 2019 sollen alle Filialen entsprechend umgebaut werden.

Aldi Nord und Aldi Süd sind im vollständigen Besitz von Familienstiftungen.

Karl Albrecht gründete 1973 die Siepmann-Stiftung, die 100 % an Aldi Süd hält. Haupt-Destinatäre (Begünstigte) der Siepmann-Stiftung sind Familienangehörige von Beate Heister (Tochter von Aldi-Süd-Gründer Karl Albrecht).

Im selben Jahr wurde für Aldi Nord die Markus-Stiftung durch Theodor Paul Albrecht und später die Lukas- und Jakobus-Stiftung, alle mit Sitz in Nortorf, gegründet. Diese halten die Anteile der Unternehmen Aldi Nord und Trader Joe’s. Destinatäre der Aldi-Nord-Stiftungen sind die Mitglieder des Familienstamms von Theodor Paul Albrecht. Sie erhalten Zuwendungen seitens der Stiftungen, die sich wiederum aus den Aldi-Nord-Erträgen speisen. Laut einem Bericht des manager Magazins sind die Aldi-Nord-Anteile der Stiftungen und ihre Destinatäre wie folgt:
1961 trennten Karl und Theo Albrecht das Stammhaus "Albrecht KG" in die bis heute aktuellen Unternehmen "Aldi Nord" und "Aldi Süd". Theo Albrecht junior ist das einzige Mitglied der Familie Albrecht, das noch aktiv im Discounter-Konzern Aldi tätig ist. "Aldi Nord" und "Aldi Süd" werden ausschließlich von familienfremden Managern geführt. Die operative Konzernführung von Aldi Nord besteht aus einem Verwaltungsrat mit Sitz in Essen. Aldi Süd wird von einem Koordinierungsrat aus Mülheim an der Ruhr geleitet.


Wichtige Entscheidungen bei Aldi Nord müssen durch die Stiftungsvorstände einstimmig getroffen werden. Der Vorstand der Markus-Stiftung ist auch das Kontrollgremium für den Verwaltungsrat, der die Geschäftsführung von Aldi Nord bildet. Die Markus-Stiftung wird im Vorstand von Cäcilie Albrecht (Witwe von Theo Albrecht sen.) und ihrem Sohn Theo Albrecht jun. geleitet. Zum Vorstand gehören auch Marc Heußinger (Aldi-Nord Geschäftsführer) und der Anwalt Emil Huber.

Die beiden Konzerne sind freundschaftlich verbunden und koordinieren im Aldi-Unternehmensausschuss gemeinsam ihre Geschäftspolitik. Das Bundeskartellamt betrachtet Aldi Nord und Aldi Süd als „faktischen Gleichordnungskonzern“ im Sinne von  Abs. 2 Aktiengesetz (Deutschland). Rechtlich, organisatorisch und seit 1966 auch finanziell sind beide Konzerne völlig unabhängig voneinander. In Deutschland betreiben Aldi Nord und Aldi Süd zusammen 66 Regionalgesellschaften, die wiederum ca. 4250 Aldi-Filialen kontrollieren (Stand April 2015). Die Regionalgesellschaften haben ihren Sitz oft außerhalb der größeren Ballungszentren; sie liegen meist nahe einem Autobahnanschluss, um die Effizienz der Belieferung der Filialen per Lkw zu erhöhen. Den Regionalgesellschaften, die als Kommanditgesellschaften (GmbH & Co. KG) geführt werden, steht jeweils ein Regionalgeschäftsführer vor. Dieser legt dem Verwaltungsrat (Aldi Nord) bzw. dem Koordinierungsrat (Aldi Süd), der als Kommanditist auftritt, Rechenschaft ab. Der Regionalgeschäftsführer wird unterstützt durch die unter ihm stehenden Abteilungsleiter. In jeder Regionalgesellschaft gibt es verschiedene Abteilungen, zum Beispiel Logistik, Verwaltung und IT. Den Abteilungsleitern unterstehen mehrere Verkaufsleiter (offiziell Regionalverkaufsleiter), die je einen Verkaufsbezirk von fünf bis sieben Filialen verantworten. Ein Verkaufsleiter ist auch gleichzeitig Disziplinarvorgesetzter und führt bis zu 100 Mitarbeiter. In jeder Aldi-Filiale gibt es einen Filialverantwortlichen, der für die Einteilung und Führung des Filialpersonals sowie für die Warendisposition, Abrechnung und vor allem für das Erreichen der entsprechenden Filialkennzahlen verantwortlich ist.

Eine flache Organisationshierarchie und einfache Unternehmensgrundsätze bilden das Unternehmensleitbild.

Die oft günstigen Preise bei Aldi sind auf eine effiziente Struktur, basierend auf rigoroser Mitarbeiterführung (bei allerdings meist überdurchschnittlicher Bezahlung), straffer Logistik, einer starken Position (durch einen hohen Grad an Marktmacht) gegenüber Lieferanten und spartanischer Präsentation der Waren (unter anderem lange Zeit Verzicht auf Fernsehwerbung), zurückzuführen.

Im Jahr 2016 haben Aldi Nord und Aldi Süd erstmals jeweils einen Nachhaltigkeitsbericht veröffentlicht, um über ihre soziale und ökologische Verantwortung Rechenschaft abzulegen.

Die Grenze zwischen Aldi Nord und Aldi Süd (auch "Aldi-Äquator" genannt) verläuft vom Westmünsterland über Mülheim an der Ruhr, Wermelskirchen, Gummersbach (in Gummersbach gibt es Nord- und Süd-Filialen), Siegen (in Siegen gibt es Nord- und Süd-Filialen, weil die Autobahn 45 die Grenze bildet), Marburg, nach Osten bis nördlich von Fulda. Die neuen Bundesländer sind (bis auf eine Filiale im thüringischen Sonneberg, die aus Bayern beliefert wird) vollständig Aldi-Nord-Gebiet.
Es gibt mehrere Tochtergesellschaften der beiden Aldi-Konzerne, die zentrale Aufgaben wie Einkauf und Immobilienverwaltung übernehmen, beispielsweise die "Aldi Einkauf GmbH & Co. oHG". Daneben besitzt Aldi auch eigene Kaffeeröstereien. Aldi Nord betreibt diese in Weyhe und Herten; die Röstereien von Aldi Süd sind in Mülheim an der Ruhr und in Ketsch.

Im Jahr 2017 gründete Aldi Süd das Tochterunternehmen "NewCoffee", unter dem die Kaffeeröstereien in Mülheim an der Ruhr und Ketsch als eigenständiges Tochterunternehmen der Unternehmensgruppe Aldi Süd firmieren.

Aldi Nord und Aldi Süd haben sich international wie folgt aufgeteilt:

Daten laut dem CR-Bericht 2016 Unternehmensgruppen Aldi Nord bzw. 2017 Aldi Süd

Aldi kündigte im Mai 2017 an, mit der Investition von 1,5 Mrd. Euro 400 Filialen in den USA neu zu eröffnen und die Mehrzahl der bestehenden auszuweiten. Analysten sehen, zusammen mit dem Auftreten von Lidl, der Reaktion von Walmart und Plänen von Amazon große Veränderungen der Branche Lebensmittelhandel. Seit 2014 sind schon 20 Lebensmittelhändler im Preiskrieg insolvent gegangen.

Die von Helmut Hofer im Jahr 1962 gegründete Filialkette "Hofer" wurde 1967 von Aldi Süd übernommen. Da der Name "Aldi" in Österreich nicht nutzbar ist (er gehört der Firma "Adel Lebensmittel Diskont"), firmiert sie seitdem als "Hofer KG". Das Aldi-Süd-Konzept wurde nach und nach umgesetzt. Das Hofer-Logo – ursprünglich ein weißer Schriftzug „Hofer“ auf blauem Balken – wurde später um die zwei Linienscharen des Aldi-„A“s ergänzt und gleicht heute (abgesehen vom Schriftzug "HOFER") dem Logo von Aldi Süd (siehe Bild).

Die "ALDI Suisse AG" ist ein Schweizer Unternehmen mit Hauptsitz in Schwarzenbach SG und gehört zur Unternehmensgruppe Aldi Süd.

Aldi ist nach Migros, Coop, Denner und Manor der fünftgrößte Detailhändler der Schweiz und beschäftigt inzwischen 3000 Mitarbeiter. Aldi zahlt einen deutlich höheren Mindestlohn als Migros und Coop. 2015 erwirtschaftete Aldi Suisse 1,42 Milliarden Euro (umgerechnet 1,52 Milliarden Franken) Nettoumsatz. Das macht im Durchschnitt 8,4 Millionen Franken pro Filiale. Die ersten vier Filialen wurden am 27. Oktober 2005 in Weinfelden, Amriswil, Altenrhein SG und Gebenstorf eröffnet.

Im Frühjahr 2017 erfolgte der Markteintritt von Aldi Süd nach China. Der Handel ausgewählter Produkte erfolgt hier ausschließlich online über die Plattform Tmall Global, einem Online-Marktplatz, der von der chinesischen "Alibaba Group" betrieben wird. In dem Online-Shop sollen demnach vor allem Weine und ungekühlte Lebensmittel angeboten werden. Australische Lieferanten würden einen Großteil des Sortiments für den Online-Shop stellen und bekommen so Zugang zum weltgrößten Markt mit 1,4 Milliarden Kunden, teilte Aldi Süd mit.

Aldi Süd hat seit Juni 2017 ein neues Logo. Das Emblem wirkt weniger kantig als sein Vorgänger.

Im Mannheimer Stadtteil Waldhof erprobte Aldi Süd ein neues Marktkonzept mit dem Namen "Tausendundeine Gelegenheit". In dem am 4. April 2005 eröffneten Laden wurden nicht abgesetzte Schnäppchenprodukte (Aldi-Aktionsware) zu nochmals stark ermäßigten Preisen verkauft. Da sich das Konzept jedoch nicht bewährte, wurde der Laden am 30. Juni 2007 wieder geschlossen.
Ende April 2017 eröffnete Aldi Süd im Kölner Mediapark als Pilotprojekt ein „Aldi Bistro“. Als Pop-Up-Konzept wurde das Bistro aus Schiffscontainern am Teich des Mediaparks errichtet, wo es für drei Monate betrieben wurde. Die als dreigängiges Menü oder einzeln erhältlichen Gerichte werden aus Produkten der Discounterkette hergestellt. Das Bistro verfügt auf 90 Quadratmetern Fläche über 50 Plätze, dazu kommen weitere 20 auf der Dachterrasse. Täglich gibt es ein Menü aus Vorspeise, Hauptgang und Nachtisch zum Einheitspreis von 7,99 Euro. Im Anschluss an die Station in Köln zog das Aldi Bistro nach München, wo es im Oktober 2017 eröffnet wurde und ebenfalls für drei Monate betrieben wird.

Umsätze und Erträge der Gruppe wurden bis zum Jahr 2000 nicht veröffentlicht. Seit dem Jahr 2001 werden die Zahlen zumindest für die Gesellschaften Nord im Bundesanzeiger veröffentlicht.
Der Umsatz 2010 in Deutschland betrug 22,5 Mrd. € (Aldi Nord 9,95 Mrd. €, Aldi Süd 12,5 Mrd. €), der weltweite Umsatz betrug 52,8 Mrd. €. Die Umsatzrendite betrug 2010 in Deutschland 3,0 % (Aldi Nord) bzw. 3,7 % (Aldi Süd).

Aldi Nord und Aldi Süd sind vollständig in Familienbesitz. Die Kapitalausstattung wird als sehr solide bezeichnet, nach eigenen Angaben hat Aldi keine Verbindlichkeiten. Soweit bekannt, ist Aldi Nord über seine Immobilientochter Eigentümer sämtlicher Logistikzentren. Der Filialbestand ist ebenfalls größtenteils Eigentum, gemietete Objekte werden im Zuge des Flächentausches und der Vergrößerung verstärkt durch eigene Objekte ersetzt. Aldi Süd ist ebenfalls Eigentümer fast aller Gebäude (Märkte, Logistikzentren) und Grundstücke, hat aber auch Fremdkapital aufgenommen, um die weitere Immobilienexpansion zu finanzieren. Damit verließ Aldi Süd den bisherigen Weg der totalen Unabhängigkeit von Kreditgebern durch das Vermeiden von Fremdkapital.

Bis etwa Anfang der 1980er Jahre hatte Aldi das Image eines "Arme-Leute-Ladens"; Aldi-Produkte galten zwar als qualitativ hinreichend solide, aber ohne Prestige. Auch heute sind arme Bevölkerungsschichten eine wichtige Zielgruppe von Aldi, jedoch gilt das nicht mehr als negativ für Aldis Image. Von Aldi vertriebene Produkte erhielten vielfach sehr gute Testergebnisse (z. B. bei Stiftung Warentest und bei Öko-Test).
2006 kauften drei Viertel der Haushalte regelmäßig bei Aldi ein. Im Fünfjahresrückblick der Stiftung Warentest waren 2004–2009 rund 40 % der angebotenen Aktionsprodukte ein Schnäppchen, 45 % waren von angemessenem Preis; die übrigen 15 % erschienen den Testern als zu teuer.

Die Grundidee ist, nur Produkte im Sortiment zu führen, die bei einem gewissen Mindestumsatz eine hohe Warenumschlagshäufigkeit aufweisen, sogenannte Schnelldreher. Das Sortiment ist somit verhältnismäßig schmal und besteht aus rund 1360 Basisartikeln, 170 Bioartikeln im Standard-, Saison- und Aktionsartikelsortiment und pro Woche etwa 80 Aktionsartikeln.

Die früher bei allen Markenartikeln übliche Bindung der Verbraucherpreise (Preisbindung der zweiten Hand) gab den Albrecht-Brüdern keine Möglichkeit, Markenartikel günstiger anzubieten. Deshalb blieb Aldi nur der Ausweg über markenfreie Produkte, die sogenannten No-Name-Produkte. Es galt, Hersteller zu finden, die Produkte speziell für Aldi mit Fantasie-Namen abpackten, die keiner Preisbindung unterlagen. Viele Markenartikler waren anfangs dazu nicht bereit, weil sie negative Reaktionen ihrer Bestandskunden befürchteten und auch tatsächlich erlebten. Aufgrund der raschen Expansion von Aldi und wegen ihrer von Anfang an guten Qualität erreichten diese Aldi-Eigenmarken schnell die Bekanntheit und den Distributionsgrad bekannter Markenartikel.
Für Aldi hat dieses Konzept den zusätzlichen Nutzen, dass der Verbraucher nicht eine bekannte Marke kauft, die er genausogut in jedem anderen Supermarkt erhalten kann, sondern eingestellt wird auf eine Meinung wie beispielsweise „Die Schokolade von Aldi ist gut“.

Als Markenware hat Aldi Produkte von Haribo, Ferrero, Coca Cola, Freixenet, Gerolsteiner, Red Bull und der Beiersdorf AG im Sortiment.

Aldi Nord verkauft seit 2004 loses Obst und Gemüse, das an der Kasse abgewogen wird. In österreichischen Hofer-Filialen wird seit Anfang 2008 ebenfalls loses Obst und Gemüse angeboten, das an der Kasse abgewogen wird. Seit 2016 ist auch bei Aldi Süd loses Obst und Gemüse erhältlich.

Seit 1998 werden bei Aldi Süd Tiefkühltruhen eingesetzt.

Sehr erfolgreich sind Nord und Süd im Kaffeegeschäft: Der gesamte Röstkaffee wird in eigenen Röstereien hergestellt. Aldi Nord lässt „Markus Kaffee“ bei der "Markus Kaffee GmbH & Co. KG" in Weyhe und Herten rösten. Das Aldi-Süd-Tochterunternehmen "NewCoffee" lässt „Amaroy Kaffee“ in Röstereien in Mülheim an der Ruhr und in Ketsch produzieren. Aldi hat auch den größten Weinabsatz in Deutschland und ist in vielen anderen Warengruppen ebenfalls Marktführer.
Die Filialen von Aldi Süd sind mit Backautomaten ausgerüstet. Der Brotbackautomat gibt Brotprodukte per Knopfdruck heraus. Im Juli 2010 erhob der Zentralverband des Bäckerhandwerks Klage gegen Aldi wegen irreführender Werbung. Kritisiert wurde die Werbung: „Ab sofort backen wir den ganzen Tag Brot und Brötchen für Sie – frisch aus dem Ofen.“

Aldi Nord und Aldi Süd führen eigene Biomarken, die die Anforderungen des deutschen staatlichen Bio-Siegels erfüllen.

Der steigende Anteil an Gebrauchsgütern zieht sich seit Anfang der 1990er Jahre wie ein roter Faden durch die Firmengeschichte nicht nur von Aldi, sondern auch von anderen Lebensmittel-Discountern. Im Unterschied zu Lebensmitteln haben Gebrauchsgüter den Charakter kurzzeitiger Aktionsangebote. Mitunter wird im Rahmen einer Themenwoche ein Sortiment artverwandter Artikel angeboten, z. B. ein breites Sortiment an Campingprodukten.

Während sich in der Frühzeit der Gebrauchsgüterbereich auf Textilien und Haushaltsgegenstände beschränkte, erweiterte sich das Angebot im Laufe der 1990er Jahre auf Unterhaltungselektronik. Einen Höhepunkt erreichte die Gebrauchsgütersparte durch den sogenannten Aldi-PC, einen in großen Zeitabständen für den Massenmarkt eigens von Aldi in Auftrag gegebenen Personal Computer.

Als erster Computer bei Aldi kam der „Aldi-C16“ im Frühjahr 1986 als Set zum Preis von 149 DM in den Handel, wobei das zuerst lediglich ein Abverkauf von Restbeständen von Commodore war, der aber einen Nachfrageboom auslöste. Der erste „Aldi-PC“ wurde 1995 auf den Markt gebracht, zur Zeit des beginnenden Internet-Booms. Auf die ersten Aldi-PCs gab es einen regelrechten Ansturm, da der Bedarf an Consumer-PCs auf dem Markt nicht sofort von den bisher den PC-Markt dominierenden Handelsketten gedeckt werden konnte und der Aldi-PC dank enorm hoher Absatzzahlen preisgünstig verkauft werden konnte. Besonders angesprochen waren dabei in erster Linie Familien mit unterem und mittlerem Einkommen.

Der seit Jahren selbe Handelspartner und Hersteller der meisten technischen Geräte, die es bei Aldi gibt, ist Medion.

In den Aldi-Süd-Filialen wurden von Anfang der 2000er Jahre bis etwa 2014 Produkte der Firma Medion vielfach unter der Pseudonym-Marke Tevion angeboten, die eine Eigenmarke von Aldi Süd ist und auch Produkte anderer Hersteller umfasst.

Die Aldi-Gruppe ist der achtgrößte Textilvermarkter in Deutschland; in diesem Segment setzt der Discounter – allgemein stagnierenden Verkaufszahlen im Textilbereich zum Trotz – über 1,095 Mrd. Euro pro Jahr um (2005).

Seit 2003/2004 bietet auch Aldi Süd in Deutschland Tabakwaren an. Aldi bezieht seine Tabakwaren bei Austria Tabak.

Aldi bietet in Deutschland seit Juli 2005 einen Online-Fotoservice, bei dem Papierabzüge von Digitalbildern nach Hause bestellt werden können.

Am 7. Dezember 2005 stieg Aldi nach guten Erfahrungen bei "Hofer" in Österreich auch in Deutschland ins Mobilfunkgeschäft ein. Sowohl Aldi Nord als auch Aldi Süd bieten den Kunden in Kooperation mit der Medion-Telefoniesparte "MedionMobile" den Prepaid-Tarif Aldi Talk an. In der Schweiz wird seit 2006 unter ebenfalls ein No-frills-Angebot für Mobiltelefonie angeboten.

Seit Januar 2007 vermitteln Aldi Nord und Aldi Süd auch in Deutschland und der Schweiz Pauschalreisen. Der ausführende Partner ist das Unternehmen "Berge & Meer", eine TUI-Tochter. Von 19. April 2013 bis Februar 2014 vermittelte Aldi Nord und Süd in Kooperation mit dem Bonner Busunternehmen "Univers" auf dem Internetportal "Aldi Reisen" auch Fernbusfahrten.

Seit Februar 2008 vertreibt Aldi Nord ein Sortiment von etwa 70 Zeitschriften (Tageszeitungen und Illustrierte).

Seit April 2008 wird ein Onlinebestellservice von Schnittblumen angeboten, wobei Aldi nur als Vermittler auftritt, den Auftrag wickelt das Unternehmen "fleurfrisch" ab, eine Tochtergesellschaft von Landgard, dem langjährigen Vertragslieferanten der Pflanzenangebote bei Aldi. Die eigentliche Zusammenstellung der Sträuße übernimmt ein von "fleurfrisch" beauftragter Bündelservice, der dafür fast ausschließlich Werkvertrags-Mitarbeiter beschäftigt.

Für das Jahr 2008 war auch der Vertrieb von Versicherungen in Kooperation mit Signal Iduna geplant. Nach Protesten des Bundesverbandes Deutscher Versicherungskaufleute zog sich Aldi aus der Kooperation zurück.

Über die Plattform Aldi life können Kunden seit 2015 Musik streamen. 2016 kamen E-Books und 2017 Computerspiele hinzu.

Im Jahr 2016 führte Aldi zum elften Geburtstag der Marke "Aldi Talk" einen eigenen Onlineshop auf Ihrer Website ein. Neben den Tarifen wurde dieser um Smartphones oder Tablets zum Verkauf erweitert.

Aldi Süd bietet seit November 2016 zertifizierten Grünstrom an.

Seit Juni 2017 bietet Aldi Süd über den Service „Aldi liefert“ ausgewählte Aktionsartikel an, die nicht in der Filiale erhältlich sind, sondern in der Filiale bezahlt und dann nach Hause geliefert werden.

Aldi gab in der gesamten Unternehmensgeschichte bis 2016 zu keinem Zeitpunkt Geld für externe Marketing-Agenturen aus. (Karl Albrecht, 1953: „Unsere Werbung liegt im billigen Preis.“)

Bis zu den Jahren 2007 (Aldi Nord) und 2008 (Aldi Süd) verzichteten beide Unternehmen auf eigene Pressestellen. Mittlerweile verfügen Aldi Nord und Aldi Süd über jeweils eigene Kommunikationsabteilungen. Aldi Süd ist auch in den Sozialen Medien Facebook, Instagram und Pinterest vertreten.

Die wöchentlichen Anzeigen in lokalen Zeitungen sehen seit Jahren gleich aus; sie zeigen die aktuellen Angebote ohne Werbeslogans unter dem Motto „Aldi informiert“. Die Zeitungsanzeigen lösten vorher regelmäßig erscheinende vierseitige Preislisten ab, die teilweise auch an die Haushalte verteilt wurden.

Die Zeitungsanzeigen von Nord und Süd wurden größer (seitdem 1/1 Seite) und farbig. Zudem liegen in den Märkten Flugblätter mit den Angeboten der nächsten Woche aus. Sowohl Aldi Süd als auch Aldi Nord haben ganzjährig drei Aktionen pro Woche, die zu einem mehrseitigen Prospekt zusammengefasst werden. Aldi Nord lässt Prospekte, Flugblätter und Zeitungsanzeigen von einer konzerneigenen Werbeagentur vorbereiten und schalten, die für den Konzern europaweit tätig ist.

Anfang der 2000er Jahre versuchte sich Aldi Nord in Sachen Merchandising und bot Aldi-Markt-Bausätze, Lkw-Modelle (siehe Foto) und Badetücher in den Aldi-Farben an. Der Aldi-Markt ist im Standardprogramm der Firma Faller.

2010 verzichtete Aldi in einigen Regionen auf Anzeigen in Tageszeitungen und ließ stattdessen die zweiwöchentlich erscheinende Werbebroschüre kostenlos an alle Haushalte verteilten.

Aldi Nord und Aldi Süd haben seit 2010 bzw. 2011 jeweils eine "Aldi-App" für Apple iOS und Android im Angebot. Darüber lassen sich die aktuellen Kundenprospekte aufrufen und Artikel aus dem Angebot in eine Einkaufsliste eintragen.

2016 haben Aldi Nord und Aldi Süd erstmals in der Unternehmensgeschichte Werbespots im deutschen Fernsehen und online geschaltet. Zum Wahlspruch „Einfach ist mehr“ wurde zudem eine Internetseite online gestellt. Aldi Süd wirbt zudem seit Herbst 2016 mit einem Blog um Kunden. Aldi Süd gibt das Kundenmagazin "Aldi inspiriert" heraus. Das kostenlose Heft erscheint alle zwei Monate.

Hofer in Österreich bringt seit 2010 Werbespots in österreichischen Privatsendern. Zur Ausgabe Mai 2012 der Zeitschrift "Universum" ließ Hofer einen Extra-Teil "Zurück zum Ursprung" drucken, die Zeitschrift lag in den Filialen zur freien Entnahme aus.

Aldi Suisse wirbt wöchentlich in größeren Tageszeitungen mit farbigen Inseraten. Daneben wird im Einzugsgebiet auch wöchentlich ein rund 16-seitiges Reklameheft verteilt, das fast ausschließlich für Non-Food-Produkte wirbt.

Vor der Verbreitung von Scannerkassen war es im Lebensmitteleinzelhandel üblich, jeden Artikel einzeln mit einem Preisetikett auszuzeichnen; dieses wurde von den Kassierern abgelesen und eingegeben. Bei Aldi gab es keine Preisetiketten: Bei Aldi Nord erfolgte die Registrierung der verkauften Artikel früher durch Eingabe einer dreistelligen PLU-Nummer. Bei Aldi Süd wurden die DM-Preise direkt eingegeben; lange Kassenbons ohne Artikelbezeichnung waren für den Kunden schlecht zu kontrollieren. Die Kassierer mussten damals die Preise aller Produkte bzw. die PLU-Nummern mithilfe von bebilderten Sortimentslisten auswendig lernen.

Im Laufe der Zeit wurde das Produktsortiment auch bei Aldi größer. Durch die Euro-Bargeldeinführung (Ende 2001/Anfang 2002) änderten sich sämtliche Preise. Aldi Süd stellte 2000 endgültig komplett auf Scannerkassen um, Aldi Nord Ende 2002. Ein Grund für die späte Einführung war, dass das Eintippen schneller ging als das Scannen mit den Geräten der ersten Generationen ohne omnidirektionalen Laser.

Da Aldi größtenteils Eigenmarken vertreibt, war es relativ einfach, den Strichcode auf den Produktverpackungen in unüblichen und teilweise ungenormten Größen sowie in größerer Anzahl auf verschiedenen Seiten der Verpackung zu platzieren. Die meisten Aldi-Verpackungen besitzen daher den EAN-Strichcode (unternehmensinterner verkürzter EAN-8-Code) auf mindestens drei Seiten, als lange Streifen oder als Banderole um die ganze Verpackung herum, wohingegen die Produkte in anderen Supermärkten einen genormten kleineren Strichcode an nur einer Stelle besitzen. Die Kassierer müssen daher Aldi-Artikel viel seltener drehen und wenden, um sie vom Scanner zu erfassen, was den Kassiervorgang beschleunigt.

In Deutschland erfolgten Tests mit der Bezahlung per EC-Karte 2004 in Filialen von Aldi Nord. Ab April 2005 folgte die flächendeckende Einführung der Zahlung per EC-Karte bei Aldi Nord und Süd, die bis Ende Oktober 2005 abgeschlossen wurde. Die Umstellung wurde durch das Unternehmen NCR durchgeführt. Aldi Suisse bot das bargeldlose Bezahlen seit ihrem Markteintritt Oktober 2005 in der Schweiz an.

Im Mai 2014 wurden in allen Filialen von Aldi Suisse NFC-fähige Kassenterminals eingeführt, im Juni 2015 zogen Aldi Süd und Nord in Deutschland und Dänemark nach. Diese ermöglichen kontaktloses Bezahlen per NFC-fähiger Debitkarte (Maestro, V Pay, PostFinance Card usw.) oder Kreditkarte oder einem NFC-fähigen Smartphone. Für das mobile Bezahlen wird eine sogenannte Mobile-Payment-App wie beispielsweise Apple Pay benötigt.

Seit Ende 2016 können Kunden bei Aldi Süd, die einen Einkauf von mindestens 20 Euro mit der EC-Karte bezahlen, kostenlos bis zu 200 Euro Bargeld abheben.

In der zweiten Jahreshälfte 2005 führte Aldi Nord als Rationalisierungsmaßnahme testweise Leergutautomaten ein. Aldi Süd übernahm dieses System im ersten Quartal 2006. Beide schlossen sich im Zuge dieser Maßnahme dem ILN-System an. Auch die neue Pfandregelung, die am 1. Mai 2006 in Kraft trat, zwang das Unternehmen zu diesem Schritt, da die sogenannten „Insellösungen“ beendet wurden.

Nach ausgiebigen Tests hat sich Aldi Nord für ein Rücknahmesystem des Herstellers Wincor Nixdorf entschieden, während Aldi Süd eine Entwicklung des Herstellers Tomra Systems vorzog. Bei beiden Geräten kommt ein System zur Anwendung, bei dem die PET-Flaschen unmittelbar nach der Abgabe gepresst werden. Seit Dezember 2014 nimmt die Unternehmensgruppe Aldi Süd Dosengebinde zurück. Die Unternehmensgruppe Aldi Nord nimmt sie erst seit dem 22. März 2015 zurück.

Bei Hofer in Österreich und Aldi Suisse werden keine Produkte mit Pfandgebinde verkauft, daher gibt es auch keine Rücknahme. Aldi Suisse hat alle Filialen mit Sammelstellen für Getränkekartons, Plastikflaschen, PET-Flaschen, Batterien, Leuchtmittel, CDs, DVDs, elektrische und elektronische Geräte ausgestattet. Aldi Suisse ist der einzige Lebensmitteleinzelhändler, welcher Getränkekartons schweizweit in allen Filialen zurücknimmt.

Seit 2006 bietet Aldi Süd in den an der Grenze zur Schweiz gelegenen Filialen ein System zur Rückerstattung der Umsatzsteuerdifferenz für die dort überproportional stark vertretenen Schweizer Kunden an. Jedoch erfolgt keine Barauszahlung, sondern über eine eigens geschaffene "Aldi-Süd-Tax-Free-Karte" eine bargeldlose Überweisung durch die Firma Global Refund. Sie gilt nur für Kunden über 18 Jahren. Anfangs war ein Mindesteinkauf von 40 € obligatorisch. Erstattet (wie bei allen Unternehmen, die sich dem Tax-Free-System angeschlossen haben) wurde aber nur ein Teil der USt. Bei Aldi waren das 75 % (25 % wurden zur Finanzierung des Systems einbehalten). Aldi sah sich jedoch dazu gezwungen (da der konkurrierende Einzelhandel in diesem Gebiet den Schweizer Kunden schon seit Jahren beinahe flächendeckend eine volle Rückerstattung anbietet und der Anteil dieser Kunden in den regionalen Filialen etwa 30 %, an manchen Wochentagen über 50 % beträgt), die 75-%-/40-€-Regelung Ende 2011 aufzuheben. Mittlerweile wird die volle USt. (100 %) erstattet und der Mindesteinkaufsbetrag wurde aufgehoben.

Aldi befolgt bei den Ladenöffnungszeiten die in der Branche üblichen Gepflogenheiten. Manchmal werden Anpassungen vorgenommen. Aldi-Filialen in großen Einkaufszentren haben in der Regel so lange wie die anderen Läden im Zentrum geöffnet.

Seit 1. September 2007 schließt Aldi Nord die Filialen samstags um 20 Uhr. Aldi Nord und Süd verlängern seit Herbst 2015 die Öffnungszeiten montags bis samstags teilweise bis 21 Uhr.

Grundsätzlich orientiert sich Aldi Suisse an den umliegenden Supermärkten – die sich diesbezüglich oft auf kommunaler oder regionaler Ebene zusammenschließen – im Rahmen der üblichen kantonalen Ladenöffnungszeiten. Aldi-Suisse-Märkte in Bahnhofsgebäuden haben auf Grund von entsprechenden Ausnahmeregelungen zusätzlich am Sonntag geöffnet.

Im gesamten "Lebensmitteleinzelhandel" belegt ALDI in Deutschland den vierten Platz nach Umsatz hinter den Unternehmen Edeka, Rewe, der Schwarz-Gruppe und vor der Metro-Gruppe (2010). Mit einem Umsatz im "Textilbereich" von rund 1,071 Mrd. Euro (2009) liegt ALDI hier derzeit auf Platz 8 der größten Textileinzelhändler Deutschlands. Im Bereich "Gesundheitsprodukte außerhalb der Apotheke" hatte Aldi im Jahr 2005 einen Marktanteil von rund 18 %. Laut einer Forsa-Umfrage sind 95 % der Arbeiter, 88 % der Angestellten, 84 % der Beamten und 80 % der Selbstständigen Kunden bei Aldi. Nach Informationen der Gesellschaft für Konsumforschung ging der Umsatz im Jahr 2007 erstmals um 1,5 % zurück und lag bei brutto 27 Mrd Euro. Der Marktanteil unter den Discountern ging dadurch um 0,6 Prozentpunkte auf 18,9 % zurück.

In der Schweiz belegte Aldi Suisse im Detailhandelsmarkt gemäß dem Umsatz 2012 den 5. Platz hinter der Migros, Coop, Denner sowie dem Manor. Der Umsatz betrug geschätzt 1,65 Milliarden Franken.

Das Schwarzbuch Markenfirmen wirft Aldi Süd vor, dass eine gewerkschaftliche Organisierung weitgehend vermieden werde, so gebe es keinen Gesamtbetriebsrat.

Im April 2004 kündigte Aldi mit sofortiger Wirkung seine bisher wöchentlich erscheinende, ganzseitige Anzeige in der Süddeutschen Zeitung, nachdem diese in einem kleinen Artikel am 7. April 2004 über „schikanöse Arbeitsbedingungen“ und „massive Wahlbehinderungen“ bei der versuchten Gründung der ersten Aldi-Betriebsräte in München berichtet hatte. Durch diesen Boykott entgingen der Zeitung Anzeigen im Gesamtwert von rund 1,5 Millionen Euro.

In einer Studie hat das Südwind-Institut Arbeitsrechtsverletzungen in chinesischen und indonesischen Zuliefererbetrieben von Aldi nachgewiesen. Dazu zählen unter anderem eine monatelange Zurückhaltung von Löhnen, Kautionszahlungen von Beschäftigten für Fabrikjobs und Kinderarbeit. Im Februar 2008 trat die Aldi-Gruppe der Business Social Compliance Initiative (BSCI) bei, nachdem für März 2008 die „Kampagne für Saubere Kleidung“ Aktionen in Deutschland für bessere Produktionsbedingungen der Aldi-Textilien in China und Indonesien organisiert hatte. Die Kampagne kritisierte jedoch, dass die BSCI keine unabhängige Verifizierungseinrichtung sei, in der Gewerkschaften und Nichtregierungsorganisationen an führender Stelle vertreten seien. Im Jahr 2009 wies das Südwind-Institut erneut auf unwürdige Arbeitsbedingungen in Aldi-Zuliefererbetrieben hin. Die Arbeitnehmerinnen müssten bis zu 90 Stunden pro Woche arbeiten, Fehler würden mit Geldstrafen geahndet. Die Beschäftigten erhielten keinen Mutterschutz und die Bildung von Gewerkschaften sei ihnen verboten. Auch 2010 wies das Südwind-Institut in einer neu veröffentlichten Studie auf Arbeitsrechtsverletzungen bei Aldi-Zulieferern in China hin. Im April 2008 stand Aldi Nord in der Kritik, weil jährlich 120.000 Euro an die Arbeitsgemeinschaft Unabhängiger Betriebsangehöriger (AUB) geflossen sind. Der Konzern räumte diese Zahlungen ein. Die Betriebsräte vieler Aldi-Nord-Regionalgesellschaften sind Mitglied in der AUB, diese selbst steht den Arbeitgebern nahe.

Anfang Januar 2013 wurde bekannt, dass Aldi Süd offenbar Mitarbeiter mit versteckten Kameras überwachen lasse. Ein früherer Detektiv des Konzerns berichtete im Nachrichtenmagazin "Der Spiegel", er habe auch über private Angelegenheiten der Beschäftigten Bericht erstatten sollen, insbesondere auch die finanzielle Situation und die Arbeitsgeschwindigkeit. Das Unternehmen wies die Vorwürfe zurück.

Mitarbeiter in einem Warenlager in Baden-Württemberg misshandelten 2013 Auszubildende. Missliebige Azubis von Aldi Süd seien im Zentrallager Mahlberg vom stellvertretenden Bereichsleiter und anderen Beschäftigten mit Frischhaltefolie an Pfosten gefesselt worden. Die Aldi-Mitarbeiter hätten dann die Gesichter der Auszubildenden mit Filzstiften beschmiert. Die Folie sei beim Fesseln derart stark gespannt gewesen, dass einer der gefesselten kaum noch atmen konnte. Vorgesetzte hätten die Schikanen beobachtet und gebilligt. Einige mit Smartphones aufgenommene Ausschnitte der Misshandlungen seien auch auf Seiten im Online-Netzwerk Facebook veröffentlicht worden. Dem Auszubildenden sei außerdem angedroht worden, bei weiterem Fehlverhalten in das Tiefkühlabteil des Zentrallagers gesperrt zu werden.

Aldi nutzt seine Marktmacht bei Verhandlungen mit Zulieferern. Aldi erwartet von seinen Lieferanten hingegen keine Zugeständnisse bei sinkenden Verkaufspreisen oder Werbekostenzuschüsse, Jubiläums-Rabatte oder Logistik-Optimierungsrabatte.

Im Schwarzbuch Markenfirmen werden Ausbeutung in der Rohstoffgewinnung und Umweltzerstörung als Kritik genannt.

Aldi wurde wegen seines unökologischen Angebots billiger Garnelen auf Kosten der Mangrovenwälder kritisiert. 2010 warf die Umweltschutzorganisation Robin Wood der Handelskette vor, dass auch zwei Bücher ihres Angebotes Fasern aus Mangrovenholz enthielten. Im Mai 2004 verkaufte Aldi in einer Sonderaktion Gartenmöbel aus indonesischem Meranti-Holz.
Aufgrund von Protesten von Umweltorganisationen und einzelnen Aktivisten, die Aldi aufforderten, sich nicht an der Zerstörung der letzten indonesischen Tropenwälder zu bereichern, erklärte Aldi, in Zukunft nur noch Artikel aus Holz mit FSC-Siegel vermarkten zu wollen.

Aldi Süd hat rund 1250 Filialen und seine Logistikzentren mit Photovoltaikanlagen ausgerüstet. An 50 Filialen gibt es für den Nutzer kostenfreie Stromtankstellen für Elektroautos und Elektrofahrräder. Nutzer können kostenfrei und ohne Registrierung ihre Elektrofahrzeuge aufladen. Die Reichweite eines Elektroautos soll so innerhalb von 30 Minuten um bis zu 80 Kilometer verlängert werden, die Reichweite eines Elektrofahrrades um acht Kilometer.







</doc>
<doc id="270" url="https://de.wikipedia.org/wiki?curid=270" title="Akkad">
Akkad

Akkad (sumerisch KUR URI, A.GA.DE) war eine Stadt in Mesopotamien. Im späten 3. Jahrtausend v. Chr. wurde sie unter Sargon von Akkad zum Zentrum seines Reiches erhoben. Dieses wird heute nach seiner Hauptstadt als "Reich von Akkad" bzw. "Akkadisches Großreich" bezeichnet, die entsprechende Periode der mesopotamischen Geschichte "Akkadzeit" (etwa 2340–2200 v. Chr) genannt. Außerdem ist die in verschiedenen Sprachstufen und Dialekten bis ins 1. Jahrhundert n. Chr. belegte semitische Sprache Mesopotamiens nach der Stadt benannt: Akkadisch.

Die Lage der Stadt war noch in neubabylonischer und persischer Zeit (6./5. Jahrhundert v. Chr.) bekannt, wurde aber später vergessen und ist auch heute noch nicht bekannt.

Auf Grund der Tradition, dass Sargon von Akkad vor dem Beginn seiner Herrschaft Mundschenk des Königs von Kiš war, wird Akkad bisweilen in der Nähe von Kiš vermutet (so noch Hans J. Nissen, wenn auch ohne konkreten Lokalisierungsvorschlag). Die Identifizierung mit der Ortslage Ischan Mizyad bei Kiš ließ sich allerdings durch archäologische Ausgrabungen nicht bestätigen. Unter Berufung darauf, dass Akkad nach antiken Quellen zeitweise zum elamischen Herrschaftsgebiet gehörte, neigt man heute eher zu einer nördlicheren Lokalisierung, und zwar am Tigris oberhalb der Einmündung des Diyala und südlich von Aššur. Nachdem eine Lokalisierung im Gebiet des heutigen Bagdad ebenfalls nicht bestätigt werden konnte, nimmt A. Westenholz als einer der besten Kenner der Akkad-Zeit an, dass die Stadt sich unter einem der großen bisher unerforschten Ruinenhügel in der Nähe der Einmündung des Adheim in den Tigris befindet. In dieselbe Richtung weisen auch die Überlegungen von Dietz-Otto Edzard, wonach Akkad im Bereich des „Flaschenhalses“ zu suchen sei, d. h. der Gegend, in der Euphrat und Tigris einander am nächsten kommen.

Die erste Erwähnung der Stadt stammt aus der Zeit von Enschakuschanna von Uruk, einem Herrscher, der etwa eine Generation älter war als Sargon von Akkad. Enschakuschanna benannte eines seiner Regierungsjahre nach der Plünderung von Akkad. Daraus ergibt sich, dass Sargon entgegen älteren Ansichten die Stadt nicht selbst gegründet hat; vielmehr war Akkad vor Sargon sogar schon so bedeutend, dass seine Plünderung in eine Jahresbezeichnung aufgenommen wurde.

Sargon von Akkad war nach alten Traditionen „Mundschenk“ (hoher Beamtentitel, nicht Diener bei Tisch) des Königs von Kiš, bevor er selbst – wahrscheinlich durch den Sturz seines ehemaligen Herrn – König wurde. Indem er siegreiche Kriege gegen Lugal-Zagesi von Uruk führte, der eine Art Oberherrschaft über das südliche Mesopotamien, darunter auch über Kiš, innehatte, unterwarf er sich ein größeres Herrschaftsgebiet, das er zu einem zentral verwalteten Staat zusammenfasste. Dass er das außerhalb der alten Kulturzentren liegende Akkad zum Mittelpunkt dieses Reiches machte, also keine der alten sumerischen Königsstädte, hängt damit zusammen, dass sein Zentralstaat gegenüber den älteren sumerischen Stadtstaaten etwas Neues sein sollte. Daher empfahl sich eine Residenz, in der keine älteren stadtstaatlichen Traditionen lebendig waren. Zugleich ist wohl davon auszugehen, dass Sargon selbst in Akkad bzw. seiner Umgebung familiär verwurzelt war. Von dort aus konnte er sich, gestützt auf Verwandte und andere Vertrauensleute, etwa Befreundete seines Stammes, eine Hausmacht aufbauen. Die aus Sargons Königsinschriften bekannte Nachricht, dass er in seinem gesamten Herrschaftsgebiet „Söhne von Akkad“ zu Statthaltern einsetzte, ist aus solchen Erwägungen verständlich. Indem er Vertrauensleute über die unterworfenen Gebiete einsetzte, schuf er eine enge Verbindung zwischen dem Herrschaftszentrum und den einzelnen zum Reich gehörigen Gebieten. Dass Sargon Akkad zur zentralen Hauptstadt ausbaute, geht auch aus der Mitteilung hervor, dass er Schiffe, die Waren aus fernen Ländern herbeibrachten, in Akkad vor Anker gehen ließ. Offensichtlich hat er in Akkad, das selbst bei der südlicheren Lokalisierung in der Nähe von Kiš hunderte von Kilometern vom Meer entfernt lag, einen Hafen angelegt, um das „Einfuhrmonopol“ (Hans J. Nissen) der neuen Hauptstadt gegenüber den älteren sumerischen Städten des Südens zu sichern. Die damit verbundene Bedeutung der Hauptstadt ergibt sich, wenn man bedenkt, wie wichtig der Fernhandel für das rohstoffarme Mesopotamien gewesen ist.

Sargons Staatsgründung war erfolgreich: Sein Reich wurde nach ihm noch von vier seiner Nachkommen in drei Generationen regiert: Es folgten ihm seine Söhne Rimuš und Maništušu, sein Enkel Naram-Sin, der nach Sargon selbst der bedeutendste König des Reiches von Akkad war, sowie dessen Sohn Šar-kali-šarri, der bis ca. 2200 v. Chr. herrschte (siehe auch: Liste der Könige von Akkad). Die zentralstaatliche Ordnung hat zweifellos zum Erfolg des Reiches beigetragen, allerdings haben alle akkadischen Könige gegen den Widerstand regionaler Kräfte kämpfen müssen. Bekannt ist etwa die große Revolte gegen Naram-Sin, die von den alten Königsstädten Ur und Kiš angeführt wurde, und die dieser offenbar mit äußerster Kraftanstrengung niederkämpfte. Sein Sieg hinterließ einen solch starken Eindruck, dass der König noch zu Lebzeiten göttliche Ehren als Stadtgott von Akkad zugesprochen bekam. Unter Naram-Sins Sohn Šar-kali-šarri zerfiel die Zentralgewalt aber immer mehr, nach seinem Tode kämpften verschiedene Kandidaten um die Königsherrschaft, und die innere Anomie ermöglichte es den Gutäern, die aus dem Zāgros-Gebirge ins mesopotamische Flachland einfielen, das Reich zu vernichten. Sie errichteten daraufhin eine Herrschaft, die sich in der Tradition der Könige von Akkad sah. Jedenfalls bezeichnete der Gutäerkönig Erridu-pizir in einer Inschrift den Familiengott der altakkadischen Dynastie als seinen Gott.

Das Reich von Akkad lebte in der geschichtlichen Erinnerung des Alten Orients fort. Als prominentestes Beispiel ist wohl eine biblische Notiz über Nimrod zu nennen, in der Erech für Uruk, Schinar für Sumer und Aššur für Assyrien steht:

Nimrod war der erste „Gewaltige“ auf der Erde, also der erste Großkönig. Dass hinter der Nimrod-Figur Erinnerungen an einen mesopotamischen Gott oder König stehen, wird allgemein zugegeben, wobei umstritten ist, an welche konkrete Gestalt zu denken sei. Die plausibelste These sieht darin eine Erinnerung an Naram-Sin von Akkad, dessen Name vielleicht zu „Nimrod“ verballhornt wurde. Einer der wichtigsten Könige des ersten mesopotamischen Großreiches wäre damit in der geschichtlichen Erinnerung zum ersten Großkönig überhaupt geworden, und diese Erinnerung hätte sich noch nach vielen Jahrhunderten bei den Nachbarvölkern der Mesopotamier erhalten. Andere Beispiele für das historische Nachleben des Reiches von Akkad sind spätere Erzählungen über Sargon von Akkad und Naram-Sin, die in Mesopotamien, aber auch bei den Hethitern entstanden bzw. überliefert wurden.

Was die Geschichte der Stadt nach dem Ende des Akkadischen Großreichs angeht, so zeigen Inschriften aus der Zeit der dritten Dynastie von Ur, dass Akkad immer noch Sitz eines Provinz-Gouverneurs war. Im Prolog des Codex Hammurapi erscheint es als Kultzentrum der altbabylonischen Zeit. König Nabonid von Babylon (555-539 v. Chr.) ließ Ausgrabungen in der Gegend des alten Akkad vornehmen, bei denen u. a. eine Inschrift des altakkadischen Königs Naram-Sin zu Tage kam. Die letzte antike Erwähnung der Stadt findet sich in einem Dokument aus der Zeit des Perserkönigs Dareios I. (522-486 v. Chr.).

Die bisher wichtigsten Fundorte der Akkad-Zeit sind die "Provinzresidenz" in Tell Brak, der "alte Palast" in Aššur, eine komplexere Siedlungsstruktur in Tell Asmar, die Städte Susa und Ninive. Die gefundenen Tontafeln geben Aufschluss über die Herrscher Akkads und ihre Regierungszeiten. In Ninive wurde die Bronzeplastik des Kopfes eines unbekannten akkadischen Herrschers gefunden, die Aufschluss über die künstlerischen Fertigkeiten jener Zeit gibt. In Susa wurde unter anderem die Siegesstele des Naram-Sin gefunden, die wie der Bronzekopf und verschiedene Rollsiegel von der Kunstfertigkeit der Akkad-Zeit zeugen. Kunst und Handwerk der Akkad-Zeit unterscheiden sich stark von den vorhergehenden und den nachfolgenden Dynastien. Rollsiegel tragen detailliertere, individuellere und anatomisch korrektere Darstellungen. Das vorher verbreitete Kleidungsstück, der Zottenrock, wurde mehr und mehr zur Bekleidung der Götter, die menschlichen Figuren trugen nun einfache glatte Gewänder.

Bisher liegen kaum Funde aus der Akkad-Zeit vor, die Aufschlüsse über Architektur oder Lebensweise geben. Versuche, die Geschichte der Epoche auf verschiedenen Ebenen (politisch, sozial …) zu rekonstruieren, müssen sich daher weitgehend auf Textquellen stützen. Ein weiteres Problem liegt darin, dass die meisten bisher entdeckten Fundstücke im 2. Jahrtausend v. Chr. als Beutestücke nach Susa verschleppt wurden und daher nicht mehr in ihrem ursprünglichen Kontext stehen.



</doc>
<doc id="271" url="https://de.wikipedia.org/wiki?curid=271" title="Akkadische Sprache">
Akkadische Sprache

Akkadisch ("akkadû", ak-ka-du-u; Logogramm: URI) ist eine ausgestorbene semitische Sprache, die stark vom Sumerischen beeinflusst wurde. Sie wurde bis ins erste nachchristliche Jahrhundert in Mesopotamien und im heutigen Syrien verwandt, in den letzten Jahrhunderten zunehmend vom Aramäischen verdrängt und diente zuletzt nur noch als Schrift- und Gelehrtensprache. Ihre Bezeichnung ist vom Namen der Stadt Akkad abgeleitet. Akkadisch war zusammen mit dem Aramäischen Volks- und Amtssprache in Mesopotamien sowie zeitweise die Sprache der internationalen Korrespondenz in Vorderasien bis nach Ägypten. Ihre beiden wichtigsten Dialekte waren Babylonisch und Assyrisch. Das Eblaitische wird von den meisten Forschern als nächster Verwandter des Akkadischen betrachtet.

Mit den übrigen semitischen Sprachen gehört das Akkadische zu den afroasiatischen Sprachen, einer Sprachfamilie, die in Vorderasien und Nordafrika beheimatet ist.

Innerhalb der semitischen Sprachen bildet das Akkadische eine eigene „ostsemitische“ Untergruppe. Es unterscheidet sich von nordwest- und südsemitischen Sprachen durch die Wortstellung Subjekt-Objekt-Verb (SOV), während die beiden anderen Zweige zumeist eine Verb-Subjekt-Objekt- oder Subjekt-Verb-Objekt-Stellung verwenden. Diese Wortstellung geht auf den Einfluss des Sumerischen zurück, das ebenfalls eine SOV-Stellung hat.

Daneben verwendet das Akkadische als einzige semitische Sprache die Präpositionen "ina" (Lokativ, also dt. in, an, bei, mit) und "ana" (Dativ-Allativ, also dt. für, zu, nach). Viele benachbarte, nordwestsemitische Sprachen, wie das Arabische und das Aramäische, haben stattdessen "bi/bə" (Lokativ) bzw. "li/lə" (Dativ). Die Herkunft der akkadischen Ortspräpositionen ist ungeklärt.

Im Gegensatz zu den meisten übrigen semitischen Sprachen hat das Akkadische nur einen Frikativ, nämlich "ḫ" . Es hat sowohl den glottalen als auch die pharyngalen Frikative verloren, die für die übrigen semitischen Sprachen typisch sind. Die Sibilanten (Zischlaute) des Akkadischen waren zumindest bis zur altbabylonischen Zeit (ca. 19. Jahrhundert v. Chr.) ausschließlich Affrikaten.

Altakkadisch ist auf Tontafeln seit etwa 2600 v. Chr. überliefert. Es wurde mit der von den Sumerern übernommenen Keilschrift geschrieben. Im Unterschied zum Sumerischen wurde diese jedoch im Akkadischen zu einer voll ausgebildeten Silbenschrift weiterentwickelt. Der Logogramm-Charakter dieser Schrift trat in den Hintergrund. Dennoch verwandte man vor allem bei sehr häufig gebrauchten Wörtern wie „Gott“, „Tempel“, u. a. auch weiterhin die entsprechenden Logogramme. So kann das Zeichen "AN" z. B. einerseits als Logogramm für „Gott“ stehen, andererseits den Gott An bezeichnen und auch als Silbenzeichen für die Silbe "-an-" verwendet werden. Daneben kommt das gleiche Zeichen als Determinativ für Götternamen zur Anwendung.

Das Beispiel 4 in der Abbildung rechts zeigt eine andere Eigenart des akkadischen Keilschriftsystems. Viele Silbenzeichen haben keinen eindeutigen Lautwert. Manche, wie z. B. "AḪ", differenzieren ihren Silbenvokal nicht. Auch in der anderen Richtung gibt es keine eindeutige Zuordnung. Die Silbe "-ša-" wird beispielsweise mit dem Zeichen "ŠA", aber auch mit dem Zeichen "NÍĜ" wiedergegeben, oft sogar innerhalb eines Textes wechselnd.

Das Altakkadische, das bis zum Ende des dritten vorchristlichen Jahrtausends verwendet wurde, unterscheidet sich sowohl vom Babylonischen wie auch vom Assyrischen und wurde von diesen Dialekten verdrängt. Bereits im 21. Jahrhundert v. Chr. waren diese beiden späteren Hauptdialekte deutlich unterscheidbar. Altbabylonisch ist, wie auch das ihm nahestehende Mariotische, deutlich innovativer als das etwas archaische Altassyrische und das sprachlich und geografisch entferntere Eblaitische. So findet sich im Altbabylonischen erstmals die Form "lu-prus" (ich will entscheiden) statt des älteren "la-prus". Dennoch hat auch Assyrisch eigene Neuerungen entwickelt, wie z. B. die „assyrische Vokalharmonie“, die jedoch nicht mit den Harmoniesystemen im Türkischen oder Finnischen zu vergleichen ist. Das Eblaitische ist sehr archaisch, es kennt noch einen produktiven Dual sowie ein nach Fall, Zahl und Geschlecht differenziertes Relativpronomen. Beides ist bereits im Altakkadischen verschwunden.

Altbabylonisch ist die Sprache König Hammurapis, der den in heutiger Zeit nach ihm benannten Codex Hammurapi, einen der ältesten Gesetzestexte der Welt, schuf. Ab dem 15. Jahrhundert v. Chr. spricht man von „Mittelbabylonisch“. Die Trennung ist dadurch bedingt, dass die Kassiten um 1550 v. Chr. Babylon eroberten und über 300 Jahre lang beherrschten. Sie gaben zwar ihre Sprache zugunsten des Akkadischen auf, beeinflussten die Sprache jedoch. In der Blütezeit des Mittelbabylonischen galt es in der gesamten Alten Welt des Orients, einschließlich Ägyptens, als Schriftsprache der Diplomatie. In diese Zeit fällt auch die Übernahme zahlreicher Lehnwörter aus nordwestsemitischen Sprachen und aus dem Hurritischen. Diese waren jedoch nur in den Grenzregionen des akkadischen Sprachgebiets gebräuchlich.

Auch das Altassyrische entwickelte sich im zweiten vorchristlichen Jahrtausend weiter. Da es jedoch eine reine Volkssprache war – die Könige schrieben Babylonisch –, sind nur wenige umfangreiche Texte aus dieser Zeit überliefert. Man spricht von „Mittelassyrisch“ bei dieser Sprache von etwa 1500 v. Chr. an.

Im 1. Jahrtausend v. Chr. wurde das Akkadische mehr und mehr als Amtssprache verdrängt. Zunächst bestanden ab etwa 1000 v. Chr. Akkadisch und Aramäisch parallel als Amtssprachen. Das wird auf vielen Abbildungen deutlich, auf denen ein Tontafelschreiber Akkadisch schreibt und ein Papyrus- oder Lederschreiber Aramäisch. Auch die zeitgenössischen Texte zeigen dies. Man spricht ab dieser Zeit von „Neuassyrisch“ bzw. „Neubabylonisch“. Ersteres erhielt im 8. Jahrhundert v. Chr. einen großen Aufschwung durch den Aufstieg des Assyrischen Reichs zur Großmacht. Im Jahre 612 v. Chr. wurde die Stadt Ninive und damit das assyrische Reich zerstört. Von da an gab es nur noch etwa zehn Jahre lang spärliche assyrische Texte.

Nach dem Ende der mesopotamischen Reiche, das durch die Eroberung des Gebiets durch die Perser kam, wurde Akkadisch, das dann nur noch in Form des „Spätbabylonischen“ existierte, als Volkssprache verdrängt, jedoch als Schriftsprache weiterhin verwendet. Auch nach dem Einmarsch der Griechen unter Alexander dem Großen im 4. Jahrhundert v. Chr. konnte sich die Sprache als Schriftsprache behaupten. Vieles deutet jedoch darauf hin, dass zu dieser Zeit Akkadisch als gesprochene Sprache bereits ausgestorben war oder zumindest nur noch in sehr geringem Umfang verwendet wurde. Die jüngsten Texte in akkadischer Sprache stammen aus der Mitte des dritten nachchristlichen Jahrhunderts.

Die akkadische Sprache wurde erst wiederentdeckt, als der Deutsche Carsten Niebuhr in dänischen Diensten 1767 umfangreiche Abschriften von Keilschrifttexten anfertigen konnte und in Dänemark präsentierte. Sofort begannen die Bemühungen, die Schrift zu entschlüsseln. Besonders hilfreich waren dabei mehrsprachige Texte, die unter anderem altpersische und akkadische Teile hatten. Dadurch, dass zahlreiche Königsnamen in diesen Texten vorkamen, konnte man zumindest einige Keilschriftzeichen identifizieren, die 1802 von Georg Friedrich Grotefend der Öffentlichkeit vorgestellt wurden. Bereits damals erkannte man, dass Akkadisch zu den semitischen Sprachen gehört. Der endgültige Durchbruch in der Entzifferung der Schrift und damit im Zugang zur akkadischen Sprache gelang in der Mitte des 19. Jahrhunderts Edward Hincks und Henry Rawlinson.

Die folgende Tabelle enthält zusammenfassend die bisher sicher identifizierten Dialekte des Akkadischen.

Einige Wissenschaftler (z. B. Sommerfeld (2003)) nehmen weiterhin an, dass das in den ältesten Texten verwendete „Altakkadisch“ keine Vorform der späteren Dialekte Assyrisch und Babylonisch war, sondern ein eigener Dialekt, der jedoch von diesen beiden verdrängt wurde und früh ausstarb.

Das Eblaitische in Nordsyrien (in und um Ebla) wird von manchen Forschern als ein weiterer akkadischer Dialekt betrachtet, meistens jedoch als eigenständige ostsemitische Sprache.

Da das Akkadische als gesprochene Sprache ausgestorben ist und über die Aussprache keine zeitgenössischen Aufzeichnungen gemacht wurden, lässt sich die exakte Phonetik und Phonologie nicht mehr erforschen. Jedoch können aufgrund der Verwandtschaft zu den übrigen semitischen Sprachen und auch der Varianten der Schreibungen innerhalb des Akkadischen einige Aussagen getroffen werden.

Die folgende Tabelle gibt die in der akkadischen Keilschriftverwendung unterschiedenen Laute wieder. Die IPA-Zeichen stellen die nach Streck 2005 vermutete Aussprache dar. In Klammern dahinter folgt die Transkription, die in der Fachliteratur für diesen Laut anzutreffen ist, sofern sie sich vom Lautschrift-Zeichen unterscheidet. Diese Transkription wurde für alle semitischen Sprachen von der Deutschen Morgenländischen Gesellschaft (DMG) vorgeschlagen und daher als DMG-Umschrift bezeichnet.

Für die Lateralaffrikate /š/ wird von einigen Wissenschaftlern eine frikativische Aussprache ( oder ) vermutet.

Daneben wird von den meisten Akkadologen die Existenz eines hinteren mittleren Vokals (o oder ) vermutet. Die Keilschrift bietet hierfür jedoch kaum Evidenz.

Alle Konsonanten und Vokale kommen kurz und lang vor. Konsonantenlänge wird durch Doppeltschreibung des betreffenden Konsonanten ausgedrückt, Vokallänge durch einen Querstrich über dem Vokal (ā, ē, ī, ū). Dieser Unterschied ist phonemisch, d. h. bedeutungsunterscheidend, und wird auch in der Grammatik ausgenutzt, z. B. "iprusu" (dass er entschied) vs. "iprusū" (sie entschieden).

Über die Betonung im Akkadischen ist nichts bekannt. Zwar gibt es einige Anhaltspunkte, wie die Vokaltilgungsregel, die im Folgenden kurz beschrieben wird, sowie einige Schreibungen in der Keilschrift, die eine Hervorhebung bestimmter Vokale darstellen könnten, jedoch konnte bisher keine Betonungsregel bewiesen werden.

Das Akkadische kennt eine Regel, die kurze (und wahrscheinlich unbetonte) Vokale löscht. Dies geschieht nicht mit Vokalen in der letzten Silbe von Wörtern und auch nur in offenen Silben, die einer anderen offenen Silbe mit kurzem Vokal folgen. Offene Silben sind dabei solche, die auf einen Vokal enden. Beispielsweise lautet das Verbaladjektiv (Partizip II) des Verbs "prs" (entscheiden, trennen) in seiner weiblichen Form "paris-t-um" (-t zeigt das feminine Geschlecht an, -um ist die Nominativ-Endung). Das /i/ wird nicht getilgt, da es sich in einer geschlossenen Silbe (/ris/) befindet. In seiner männlichen Form heißt es jedoch "pars-um", da in der zugrundeliegenden Form /pa.ri.sum/ das /i/ in einer offenen Silbe steht und auf eine kurze offene Silbe (/pa/) folgt.
In den späteren Sprachstufen des Akkadischen ist daneben eine generelle Tilgung kurzer Vokale im Wortauslaut zu beobachten.

Wie alle semitischen Sprachen verwendet auch das Akkadische die sogenannte Wurzelflexion. Die „Wurzel“ eines Wortes, die seine Grundbedeutung beinhaltet, besteht in der Regel aus drei Konsonanten, den sogenannten Radikalen. Die Radikale oder Wurzelkonsonanten werden in der Transkription manchmal mit großen Buchstaben wiedergegeben, z. B. "PRS" (entscheiden, trennen). Zwischen und um diese Wurzelkonsonanten werden im Akkadischen verschiedene Infixe, Präfixe und Suffixe gesetzt, die grammatikalische und wortbildende Funktionen besitzen. Das Konsonant-Vokal-Muster, das sich ergibt, differenziert die Grundbedeutung der Wurzel. Der mittlere Wurzelkonsonant (Radikal) kann einfach oder verdoppelt (gelängt) sein. Dieser Unterschied ist ebenfalls bedeutungsdifferenzierend. Beispiele hierfür finden sich im Abschnitt „Verbmorphologie“.

Die Konsonanten "ʔ", "w", "j" und "n" werden als „schwache Radikale“ bezeichnet. Wurzeln, die diese Radikale enthalten, bilden unregelmäßige Stammformen.

Dieses morphologische System unterscheidet sich deutlich von dem der indogermanischen Sprachen. Im Deutschen ändert sich beispielsweise die Wortbedeutung grundlegend, wenn man einzelne Vokale austauscht, z. B. „Rasen“ vs. „Rosen“. Allerdings ähnelt der Ablaut (z. B. Präsens „(wir) singen“ vs. Präteritum „(wir) sangen“), der schon urindogermanischen Alters ist, dem semitischen System.

Das Akkadische hat zwei grammatische Geschlechter, "männlich" und "weiblich". Weibliche Substantive und Adjektive haben meistens ein "-(a)t" am Ende des Stamms. Das Kasussystem ist einfach. Es beinhaltet im Singular drei Kasus (Nominativ, Genitiv und Akkusativ), im Plural jedoch nur zwei Kasus (Nominativ und Obliquus). Adjektive kongruieren in Kasus, Numerus und Genus mit dem Bezugswort und folgen diesem in der Regel.

Am Beispiel der Substantive "šarrum" (König) und "šarratum" (Königin) und des Adjektivs "dannum" (stark) wird in der folgenden Tabelle das Kasussystem im Altbabylonischen verdeutlicht:

Wie man sieht, unterscheiden sich die Endungen für Substantive und Adjektive nur im männlichen Plural. Einige Substantive, vor allem geografische Begriffe wie „Stadt“, „Feld“ u. ä. können im Singular zusätzlich einen Lokativ auf "-um" bilden. Dieser ist jedoch anfangs nicht produktiv und die resultierenden Formen stellen erstarrte adverbiale Bestimmungen dar. In neubabylonischer Zeit wird der "um"-Lokativ immer häufiger und ersetzt in vielen Formen die Konstruktion mit der Präposition "ina".

In späteren Entwicklungsstufen des Akkadischen ist, außer im Lokativ, zunächst die sogenannte Mimation (analog mit der Nunation, die im Arabischen auftritt), also das "-m", das in den meisten Kasusendungen auftritt, entfallen. Später fielen im Singular der Substantive Nominativ und Akkusativ zu "-u" zusammen. Im Neubabylonischen trat ein Lautwandel ein, durch den kurze Vokale im Wortauslaut verschwanden. Damit entfiel die Unterscheidung der Kasus außer bei den männlichen Nomen im Plural. In vielen Texten wurden die Kasusvokale jedoch weiterhin geschrieben, dies jedoch nicht konsequent und oft auch falsch. Da die wichtigste Kontaktsprache des Akkadischen in dieser Zeit das Aramäische war, das ebenfalls über keine Kasusunterscheidung verfügt, war diese Entwicklung wohl nicht nur phonologisch bedingt.

Das akkadische Substantiv besitzt drei verschiedene Status. Sie drücken die syntaktische Beziehung des Substantivs zu anderen Satzteilen aus. Der "status rectus" (regierter Status) ist dabei die Grundform. Der "status absolutus" (absoluter Status) wird verwendet, wenn das Substantiv in einem Nominalsatz (z. B. "A ist ein B") als Prädikat verwendet wird.

Folgt einem Substantiv ein Possessivsuffix oder ein Substantiv im Genitiv, so muss es im "status constructus" stehen, der oft genau wie der Status absolutus durch Abtrennen des Kasussuffixes gebildet wird.

Eine Genitivverbindung kann jedoch auch mit der Partikel "ša" hergestellt werden. Das Substantiv, von dem die Genitivphrase abhängt, steht dabei im Status rectus. Die gleiche Partikel wird auch zur Anknüpfung von Relativsätzen verwendet.

Bei den Verben werden vier Stämme unterschieden. Der Grundstamm (G-Stamm) ist die nicht-abgeleitete Form. Mit dem Dopplungsstamm (D-Stamm) werden Applikativ-, Kausativ- oder Intensivformen gebildet. Er erhielt seine Bezeichnung von der Dopplung des mittleren Radikals, die für D-Formen typisch ist. Die gleiche Dopplung tritt jedoch auch im Präsens der übrigen Stammformen auf. Der Š-Stamm (Stammbildungselement "š-") wird für Kausative verwendet. Im D- und Š-Stamm ändern die Konjugationspräfixe ihren Vokal in /u/. Der N-Stamm drückt Passiv aus. Das Stammbildungselement "n-" wird dabei an den folgenden ersten Konsonanten der Wurzel angeglichen, der dadurch gelängt wird (vgl. Bsp. 9 in der folgenden Tabelle). In einigen Formen steht es jedoch nicht direkt vor dem Konsonanten, wodurch die ursprüngliche Form /n/ erhalten bleibt (vgl. Bsp. 15).

Jeder der vier Stämme kann neben der normalen Verwendung einen Reflexiv- und einen Iterativstamm bilden. Die Reflexivstämme werden mit einem Infix "-ta-" gebildet. Daher werden sie auch Gt-, Dt-, Št- bzw. Nt-Stamm genannt, wobei der Nt-Stamm nur von sehr wenigen Verben gebildet wird. Für die Iterativstämme verwendet man ein Infix "-tan-", das jedoch nur im Präsens sichtbar ist. Die übrigen Zeitformen und Ableitungen der sog. "tan"-Stämme Gtn, Dtn, Štn und Ntn lauten wie die entsprechenden Formen der Reflexivstämme.

Von vielen Verben lassen sich auf diese Weise theoretisch viele tausend Formen bilden. Diese äußerst umfangreiche Verbmorphologie ist eines der besonderen Merkmale der semitischen Sprachen. Die folgende Tabelle zeigt einen kleinen Ausschnitt aus der Formenvielfalt der Wurzel "PRS" (entscheiden, trennen).

Eine finite Verbform des Akkadischen beinhaltet obligatorisch die Kongruenz zum Subjekt des Satzes. Diese wird stets durch ein Präfix, in einigen Formen zusätzlich durch ein Suffix realisiert. Wie bereits erwähnt, unterscheiden sich die Präfixe des G- und N-Stamms von denen im D- und Š-Stamm durch ihren Vokal.

In der folgenden Tabelle werden die einzelnen Kongruenzformen des Verbs "PRS" (entscheiden, trennen) im Präteritum der vier Stämme dargestellt (Übersetzung siehe Tabelle oben). Wie man sieht, werden die beiden grammatische Geschlechter nur in der 2. Person Singular und in der 3. Person Plural unterschieden.

Zusätzlich zur Subjektskongruenz können bis zu zwei pronominale Suffixe an das Verb antreten, die dann das direkte und das indirekte Objekt markieren. Diese Pronominalsuffixe sind in allen Verbstämmen gleich. Anders als bei den Kongruenzmorphemen werden die beiden grammatischen Geschlechter in der 2. und 3. Person sowohl im Singular als auch im Plural unterschieden.
Wenn sowohl direktes als auch indirektes Objekt pronominal markiert werden, geht das indirekte Objekt (Dativ) dem direkten (Akkusativ) voraus.
Die Suffixe für das indirekte Objekt der 1. Person Singular (‚mir‘, ‚für mich‘) entsprechen den Ventiv-Suffixen. Dabei steht "-am", wenn die Subjektskongruenz ohne Suffix auftritt, "-m" nach dem Suffix "-ī" und "-nim" nach den Suffixen "-ā" und "-ū".
Die Ventiv-Suffixe treten oft zusammen mit anderen Dativ-Suffixen oder mit den Suffixen der 1. Person Singular Akkusativ auf.

Die folgende Tabelle enthält die Formen der Objektssuffixe, wie sie im Altbabylonischen verwendet wurden:

Das "-m" der Dativsuffixe assimiliert sich dabei an folgende Konsonanten, vgl. Bsp. (7) unten. Die folgenden Beispiele illustrieren die Verwendung der beschriebenen Morpheme.

Eine sehr oft auftretende Form, die sowohl von Nomen, von Adjektiven als auch von Verbaladjektiven gebildet werden kann, ist der Stativ. Angefügt an prädikativ verwendete Substantive (im Status absolutus) entspricht diese Form dem Verb "sein" im Deutschen. Verbunden mit einem Adjektiv oder Verbaladjektiv wird ein Zustand ausgedrückt. Eine direkte Entsprechung hat der Stativ als Pseudopartizip im Ägyptischen. Die folgende Tabelle enthält am Beispiel des Nomens "šarrum" (König), des Adjektivs "rapšum" (breit) und des Verbaladjektivs "parsum" (entschieden) die einzelnen Formen.

Dabei kann "šarr-āta" sowohl „du warst König“, „du bist König“, als auch „du wirst König sein“ bedeuten, der Stativ ist also von Zeitformen unabhängig.

Neben der bereits erläuterten Möglichkeit der Ableitung verschiedener Verbstämme verfügt das Akkadische über zahlreiche Nominalbildungen aus den Verbwurzeln. Eine sehr häufig auftretende Nominalisierung ist die sogenannte "ma-PRaS"-Form. Sie kann den Ort eines Geschehens, die Person, die die Handlung ausführt, aber auch viele andere Bedeutungen ausdrücken. Ist einer der Wurzelkonsonanten (Radikale) ein labialer Laut (p, b, m), so wird das Präfix zu "na-". Beispiele hierfür sind: "maškanum" (Stelle, Ort) von "ŠKN" (setzen, stellen, legen), "mašraḫum" (Pracht) von "ŠRḪ" (prachtvoll sein), "maṣṣarum" (Wächter) von "NṢR" (bewachen), "napḫarum" (Summe) von "PḪR" (zusammenfassen).

Eine sehr ähnliche Bildung ist die "maPRaSt"-Form. Die Nomen, die dieser Nominalbildung entstammen, sind grammatisch weiblichen Geschlechts. Für die Bildung gelten die gleichen Regeln wie für die maPRaS-Form, z. B. "maškattum" (Depositum) von "ŠKN" (setzen, stellen, legen), "narkabtum" (Wagen) von "RKB" (reiten, fahren).

Zur Ableitung abstrakter Nomen dient das Suffix "-ūt". Die Substantive, die mit diesem Suffix gebildet werden, sind grammatisch weiblich. Das Suffix kann sowohl an Substantive, Adjektive, als auch an Verben angefügt werden, z. B. "abūtum" (Vaterschaft) von "abum" (Vater), "rabûtum" (Größe) von "rabûm" (groß), "waṣūtum" (Weggang) von "WṢJ" (weggehen).

Auch Ableitungen von Verben aus Substantiven, Adjektiven und Zahlwörtern sind zahlreich. Zumeist wird aus der Wurzel des Nomens oder Adjektivs ein D-Stamm gebildet, der dann die Bedeutung „X werden“ oder „etwas zu X machen“ besitzt, z. B. "duššûm" (sprießen lassen) von "dišu" (Gras), "šullušum" (etwas zum dritten Mal tun) von "šalāš" (drei).

Das Akkadische verfügt über Präpositionen, die aus einem einzigen Wort bestehen (z. B. "ina" (in, an, aus, durch, unter), "ana" (zu, für, nach, gegen), "adi" (bis), "aššu" (wegen), "eli" (auf, über), "ištu/ultu" (von, seit), "mala" (gemäß), "itti" (mit, bei)). Daneben gibt es jedoch einige mit "ina" und "ana" zusammengesetzte Präpositionen (z. B. "ina maḫar" (vor), "ina balu" (ohne), "ana ṣēr" (zu … hin), "ana maḫar" (vor … hin)). Unabhängig ihrer Komplexität stehen alle Präpositionen mit dem Genitiv.

Beispiele: "ina bītim" (im Haus, aus dem Haus), "ana … dummuqim" (um … gut zu machen), "itti šarrim" (beim König), "ana ṣēr mārīšu" (zu seinem Sohn).

Da in der Keilschrift die Zahlen zumeist als Zahlzeichen geschrieben werden, ist die Lautung vieler Zahlwörter noch nicht geklärt. In Kombination mit etwas Gezähltem stehen die Kardinalzahlwörter im Status absolutus. Da andere Fälle sehr selten sind, sind die Formen des Status rectus nur von vereinzelten Zahlwörtern bekannt. Die Zahlwörter 1 und 2 sowie 21-29, 31-39, 41-49 usw. kongruieren mit dem Gezählten im grammatischen Geschlecht. Die Zahlwörter 3-20, 30, 40 und 50 zeigen eine Genuspolarität, d. h. vor männlichen Substantiven steht die weibliche Form des Zahlworts und umgekehrt. Diese Polarität ist typisch für die semitischen Sprachen und tritt z. B. auch im klassischen Arabisch auf. Die Zahlwörter 60, 100 und 1000 lauten in beiden Geschlechtern gleich. Mit den Zahlwörtern ab zwei steht das Gezählte in der Mehrzahl. Bei paarweise vorhandenen Körperteilen kann eine Dualform (Zweizahl) beobachtet werden, die jedoch nicht mehr produktiv gebildet werden kann, z. B. "šepum" (Fuß) wird zu "šepān" (zwei Füße).

Die Ordnungszahlen werden bis auf wenige Ausnahmen durch Anfügen einer Kasusendung an die Nominalform "PaRuS" gebildet, wobei P, R und S durch die entsprechenden Konsonanten des Zahlwortes ersetzt werden müssen. Besonders auffällig ist, dass im Fall der Eins die Ordnungszahl und die Kardinalzahl gleichlauten. Bei der Vier tritt eine Metathese (Lautvertauschung) ein. Die folgende Tabelle enthält die männlichen und weiblichen Formen des Status absolutus einiger akkadischer Kardinalzahlen, sowie die entsprechenden Ordnungszahlen.

Beispiele: "erbē aššātum" (vier Ehefrauen) (männliches Zahlwort!), "meʾat ālānū" (einhundert Städte).

Außer den Zahlwörtern stehen alle Ergänzungen, die einem Substantiv angefügt werden, nach diesem Substantiv. Das betrifft sowohl Adjektive, Relativsätze als auch Appositionen. Zahlwörter hingegen gehen dem Gezählten voraus. In der folgenden Tabelle wird die Nominalphrase "erbēt šarrū dannūtum ša ālam īpušū abūja" (die vier starken Könige, die die Stadt gebaut haben, meine Väter) analysiert.

Die bevorzugte Satzstellung im Akkadischen ist Subjekt-Objekt-Prädikat. Die für semitische Sprachen ungewöhnliche Verbletztstellung ist Ergebnis eines jahrhundertelangen Sprachkontakts mit dem Sumerischen, das ebenfalls diese Satzstellung besitzt. Vor allem in literarischen Texten kommen im Akkadischen jedoch auch andere Reihenfolgen vor. Vor allem Chiasmen, d. h. Umkehrungen der Satzstruktur, sind sehr häufig anzutreffen. Ein Beispiel aus dem "Tonzylinder von Nabonid" (2:20-2:21) verdeutlicht dies:

Verbformen von Nebensätzen, die mit einer Konjunktion eingeleitet sind, tragen das Subordinativ-Suffix "-u", das jedoch entfällt, wenn ein anderes mit einem Vokal beginnendes Suffix antritt. Die einzige Konjunktion, die stets ohne Subordinativ in der Verbform auftritt, ist "šumma" (wenn, falls). Die Gründe dafür sind noch nicht geklärt. Einige weitere Konjunktionen sind "ša" (für Relativsätze), "kī(ma)" (dass, sodass, nachdem, als, sobald, wie), "ūm" (als, sobald, während), "adi" (solange bis), "aššum" (weil).

In Nominalsätzen wird im Akkadischen keine Kopula verwendet, d. h. kein Verb wie das deutsche "sein". Stattdessen steht das prädikativ gebrauche Substantiv oder Adjektiv im Stativ, wie zum Beispiel in "Awīlum šū šarrāq." (‚Dieser Mann ist ein Dieb.‘).

Der akkadische Wortschatz ist großenteils semitischen Ursprungs. Bedingt durch den sprachgeschichtlichen Sonderstatus der Sprache, dessentwegen man sie auch in eine eigene Untergruppe „Ostsemitisch“ einordnet, gibt es aber selbst im Grundwortschatz relativ viele Elemente ohne offensichtliche Parallelen in den verwandten Sprachen, z. B. "māru" „Sohn“ (semitisch sonst *"bn"), "qātu" „Hand“ (semit. sonst *"jd"), "šēpu" „Fuß“ (semit. sonst *"rgl"), "qabû" „sagen“ (semit. sonst *"qwl"), "izuzzu" „stehen“ (semit. sonst *"qwm"), "ana" „zu, für“ (semit. sonst *"li"), etc.

Durch den intensiven Sprachkontakt zunächst zum Sumerischen und später zum Aramäischen besteht der akkadische Wortschatz zu einem Teil aus Lehnwörtern aus diesen Sprachen. Die aramäischen Lehnwörter waren dabei in den ersten Jahrhunderten des 1. Jahrtausends v. Chr. hauptsächlich auf Nord- und Mittelmesopotamien beschränkt, während die sumerischen Lehnwörter im gesamten Sprachgebiet verbreitet waren. Neben den genannten Sprachen wurden einige Substantive aus dem Reit- und Haushaltswesen aus dem Hurritischen und aus dem Kassitischen entlehnt. Einige wenige Lehnwörter entstammen dem Ugaritischen.

Aufgrund der im Vergleich zu nichtsemitischen Sprachen sehr verschiedenen Wortstruktur war es den Akkadern nicht möglich, sumerische oder hurritische Verben in die semitische Wurzelflexion zu übernehmen. Aus diesem Grund wurden aus diesen Sprachen nur Substantive und einige Adjektive entlehnt. Da jedoch das Aramäische und das Ugaritische ebenfalls zu den semitischen Sprachen gehören und daher auch über eine Wurzelflexion verfügen, konnten aus diesen Sprachen einige Verben, aber auch viele Nomina übernommen werden.

Die folgende Tabelle enthält Beispiele für Lehnwörter im Akkadischen.

Aber auch das Akkadische war Quelle von Entlehnungen, vor allem ins Sumerische. Einige Beispiele sind: sum. "da-rí" (dauernd, von akk. "dāru"), sum. "ra-gaba" (Berittener, Bote, von akk. "rākibu").

Der folgende kleine Text ist der Paragraph 7 des Codex Hammurapi, der etwa im 18. Jahrhundert v. Chr. verfasst wurde. Die Abkürzungen "St.cs." und "St.abs." stehen für „Status constructus“ bzw. „Status absolutus“.

Übersetzung: ‚Wenn ein Bürger aus der Hand des Sohnes eines anderen Bürgers oder eines Sklaven eines Bürgers ohne Zeugen oder Vertrag Silber, Gold, einen Sklaven, eine Sklavin, ein Rind, ein Schaf, einen Esel oder irgendetwas anderes kauft oder in Verwahrung nimmt, ist dieser Bürger ein Dieb und wird getötet.‘









</doc>
<doc id="274" url="https://de.wikipedia.org/wiki?curid=274" title="Assyrisch">
Assyrisch

Das Adjektiv assyrisch bezieht sich auf



</doc>
<doc id="276" url="https://de.wikipedia.org/wiki?curid=276" title="Assur">
Assur

Assur steht für


Andere Schreibweisen sind Aššur, Aschschur und Aschur.

Siehe auch:


</doc>
<doc id="283" url="https://de.wikipedia.org/wiki?curid=283" title="Aioli">
Aioli

Aioli oder Allioli (frz.: "Aïoli"; aus dem Okzitanischen bzw. Katalanischen "all i oli", „Knoblauch und Öl“) ist eine aus dem Mittelmeerraum stammende kalte Creme, die vor allem aus Knoblauch, Olivenöl und Salz besteht. Aioli wird als Vorspeise mit Brot oder Oliven sowie als Beigabe zu Fleisch, Fisch und Gemüse serviert.

Bei der traditionellen Aioli, die nur aus Knoblauch und Öl besteht, werden Knoblauchzehen in einem Mörser zerrieben. Dann wird Öl in dünnem Strahl unter fortwährendem Rühren mit dem Stößel hinzugegeben, bis eine zähflüssige Creme entstanden ist. Wird zu viel oder zu schnell Öl hinzugegeben, kann sich die Emulsion wieder trennen und die Aioli gerinnen. Als Emulgator werden deshalb auch Milch, ein Stückchen gekochte Kartoffel oder Eigelb beigegeben. In der klassischen Küche wird die Aioli aus mit hartgekochtem Eigelb fein zerstoßenem Knoblauch, unter den wie bei einer Mayonnaise Öl aufgezogen wird, zubereitet und mit Zitronensaft und Cayennepfeffer abgeschmeckt.

Ein Rezept zur Aioli wurde erstmals im Jahr 1024 schriftlich festgehalten. Die Sauce wird aber bereits länger zubereitet. Aus der Aioli entstanden durch Verfeinerungen und Rezeptzugaben zahlreiche Varianten. Am bekanntesten ist heute die Variante aus Maó (spanisch "Mahón") auf Menorca. Häufig ist zu lesen, dies sei die Urversion der französischen Mayonnaise. Diese Auffassung lässt sich aber durch seriöse Quellen nicht belegen. Die Etymologie des Wortes Mayonnaise ist völlig unklar, es gibt mehrere Deutungen. 

Aioli ist aber auch ein typisch provenzalisches Gericht: gedünstete Kartoffeln, Karotten und grüne Bohnen, dazu Seeschnecken und ein Stück Fischfilet (klassisch ist gewässerter Stockfisch aus Kabeljau) und die oben beschriebene Aioli.


</doc>
<doc id="284" url="https://de.wikipedia.org/wiki?curid=284" title="Avatar">
Avatar

Avatar (Sanskrit "avatāra" ‚Herabkunft‘) steht für:


Film und Fernsehen:

Musik:
Siehe auch:


</doc>
<doc id="285" url="https://de.wikipedia.org/wiki?curid=285" title="Apulien">
Apulien

Apulien (ital. "Puglia" [] oder oft im Plural "Puglie" []; lat. "Apulia") ist eine in Südost-Italien gelegene Region mit der Hauptstadt Bari. Sie hat Einwohner (Stand ). Die Halbinsel Salento im Süden Apuliens bildet den „Absatz“ des sogenannten „italienischen Stiefels“.

Die Region erstreckt sich entlang des Adriatischen und des Ionischen Meers. Mit der Punta Palascìa erreicht die Küste bei Otranto den östlichsten Punkt Italiens, der nur noch 80 km von der albanischen Küste entfernt ist.

Das Gebiet besteht zu 53,3 % aus Ebenen, zu 45,3 % aus Hügelland und zu 1,5 % aus Gebirge. Damit ist Apulien die flachste Region Italiens. Die Landschaften teilen sich von Norden nach Süden in die bergige Halbinsel Gargano mit den vorgelagerten Tremiti-Inseln, der ebenen Tavoliere delle Puglie, der anschließenden Ebene Terra di Bari, der Kalkhochebene der Murge, der Küstenebene von Tarent und des Valle d'Itria, das die südlichste Region, die größtenteils ebene Halbinsel Salento abschließt. Das einzige Gebirge, die Monti della Daunia bilden die Grenze zu Kampanien und erreichen im Monte Cornacchia 1152 m Höhe.

Das Klima bietet milde Winter und heiße Sommer.

Der Hauptartikel Geschichte Apuliens behandelt die historische Entwicklung der südostitalienischen Region Apulien, die in etwa den Stiefelabsatz der italienischen Halbinsel einnimmt. 

Seit dem 26. Juni 2015 ist Michele Emiliano (PD) Präsident Apuliens.

Apulien besteht aus den Provinzen Foggia, Barletta-Andria-Trani, Tarent, Brindisi, Lecce und der Metropolitanstadt Bari. Die 2004 gegründete Provinz Barletta-Andria-Trani wurde erst mit den Wahlen zur Provinzversammlung am 6. und 7. Juni 2009 geschäftsfähig.

In der Region gibt es die folgenden Städte und größeren Orte. Bari ist mit einer Agglomeration von fast 600.000 Einwohnern die Metropole Apuliens und nach Neapel die zweitgrößte Stadt Süditaliens.
Stand: 31. August 2008

Im Vergleich mit dem pro-Kopf-BIP der EU (kaufkraftadjustiert) erreichte die Region Apulien im Jahr 2015 einen Index von 63 (EU 28: 100).

In den fruchtbaren Küstenebenen gedeihen neben Mandeln, Oliven, Getreide und Tomaten auch Kaktusfeigen, Trauben, Feigen sowie Zitrusfrüchte. Der milde Winter ist ideal für den Stängelkohl, der an wilden Brokkoli erinnert und die Basis des berühmten apulischen Gerichts "Orecchiette con cima di rapa" bildet.

Apulien ist ein wichtiges Weinbaugebiet mit vorwiegend gehaltvollen Rotweinen. Wichtige Rebsorten sind Primitivo, Negroamaro und Nero di Troia. Auf einer Anbaufläche von 107.571 Hektar wird eine Gesamtproduktion von 7.580.000 Hektolitern (DOC-Produktion: 259.900 hl) erzeugt. Bekannte Weinbau-Regionen sind Manduria, die Halbinsel Salento, die Gebiete um das Castel del Monte, Canosa di Puglia, Locorotondo und Foggia.

Apulien bildete luftfahrthistorisch das Sprungbrett Italiens in den Orient. Aus diesem Grund gibt es in dieser Region eine starke Konzentration von Zivil- und Militärflugplätzen. Die Betreibergesellschaft Aeroporti di Puglia betreibt neben dem Flughafen von Bari auch den Verkehrsflughafen Brindisi-Casale sowie die Flugplätze von Tarent und Foggia. Der Flughafen von Bari ist zusammen mit dem Flughafen Brindisi-Casale der wichtigste in der Region Apulien, Tarent und Foggia haben hingegen nur regionale beziehungsweise lokale Bedeutung.

Der Flughafen Bari liegt acht Kilometer nordwestlich von Bari, der Hauptstadt der Region Apulien. Er wurde Ende 2005 nach dem bürgerlichen Namen des verstorbenen Papstes Johannes Paul II. "Karol Wojtyla" benannt.

Die riesige Stahlfabrik ILVA des Riva-Konzern in Tarent stellt mehr als 30 % des italienischen Stahls her und beschäftigt 25.000 Menschen. Es ist die größte Anlage dieser Art in Europa. Wegen massiver Umweltbelastungen mit vielen Todesfällen in der Region sind allerdings Teile der Anlage zurzeit stillgelegt und der italienische Staat übernahm die Kontrolle über den wichtigsten Arbeitgeber in der Region.





</doc>
<doc id="290" url="https://de.wikipedia.org/wiki?curid=290" title="Argentinien">
Argentinien

Argentinien ( []) ist eine Republik im Süden Südamerikas. Es grenzt im Norden an Bolivien, im Nordosten an Paraguay, im Westen an Brasilien und Uruguay und im Südosten an Chile.

Der Landesname leitet sich von der lateinischen Bezeichnung für Silber – "argentum" – ab und stammt aus der spanischen Kolonialzeit, als man hier Edelmetalle zu finden hoffte. Bis zu seiner Unabhängigkeit 1816 war es Teil des spanischen Kolonialreiches. Politisch ist Argentinien eine präsidentielle Bundesrepublik, in der die einzelnen Gliedstaaten, Provinzen genannt, weitreichende Kompetenzen innehaben.

Mit einer Fläche von knapp 2,8 Mio. km² ist Argentinien der achtgrößte Staat der Erde und der zweitgrößte des südamerikanischen bzw. der viertgrößte des amerikanischen Doppelkontinentes. Wegen seiner großen Nord-Süd-Ausdehnung hat das Land Anteil an mehreren Klima- und Vegetationszonen. Im Hinblick auf die Einwohnerzahl steht es mit rund 43 Millionen Einwohnern in Südamerika an dritter (nach Brasilien und Kolumbien) und in ganz Amerika an fünfter Stelle. Etwa ein Drittel der Bevölkerung konzentriert sich im Ballungsraum der Hauptstadt Buenos Aires, die als bedeutendes Kulturzentrum Amerikas gilt, in dem unter anderem der Tango Argentino seinen Ursprung hat. Weitere Ballungszentren bilden die Städte Córdoba, Rosario, Mar del Plata und Mendoza. Große Teile des trockenen und kalten Südens sind dagegen nur sehr dünn besiedelt.

Wirtschaftlich spielen traditionell die Landwirtschaft, Viehzucht und der Rohstoffabbau eine große Rolle. Bis etwa 1950 war Argentinien eines der reichsten Länder der Erde, dann sorgte der herrschende Peronismus für einen staatlich gelenkten Industriesektor und das Wohlstandsniveau sank in der Folge auf ein mittleres Niveau ab. Erst in den 1990ern fand eine Umkehr dieser Politik statt und die verarbeitende Industrie sowie der Dienstleistungssektor konnten sich marktwirtschaftlich entfalten. International wird Argentinien oft zu den Schwellenländern gezählt. Nach dem von den Vereinten Nationen erhobenen Index der menschlichen Entwicklung zählt es seit 2011 jedoch zu den "sehr hoch entwickelten" Staaten. Es gehört unter den unabhängigen südamerikanischen Staaten gemeinsam mit Chile und Uruguay (Südkegel) zur Spitzengruppe in Hinblick auf das Pro-Kopf-Bruttoinlandsprodukt (Kaufkraftparität). Die Einkommensungleichheit (Gini-Koeffizient) lag 2009 im weltweiten Vergleich relativ hoch, aber immer noch unter dem Durchschnitt der lateinamerikanischen Staaten.

Argentinien hat eine Fläche von 2,78 Millionen km² und ist damit nach Brasilien der zweitgrößte Staat Südamerikas. Die Ausdehnung von Norden nach Süden beträgt 3694 km, die von Westen nach Osten an der breitesten Stelle circa 1423 km. Es grenzt im Osten an den Atlantischen Ozean, im Norden an Bolivien und Paraguay, im Nordosten an Brasilien und Uruguay; ihre jeweils längste gemeinsame Grenze bilden Chile und Argentinien im Westen des Landes.

Das gesamte westliche Grenzgebiet wird von den Anden eingenommen, der längsten kontinentalen Gebirgskette der Erde. Der zentrale Norden Argentiniens wird vom Gran Chaco, einer heißen Trockensavanne, eingenommen. Östlich davon schließt sich entlang des Río Paraná das Hügelland der Provinz Misiones an. Dort befinden sich am Dreiländereck Argentinien–Paraguay–Brasilien die Iguazú-Wasserfälle; sie sind etwa 2,7 Kilometer breit und zählen zu den größten der Erde. Südlich davon, zwischen den großen Strömen Río Paraná und Río Uruguay, liegt das feuchte und sumpfige Mesopotamia. Am Río de la Plata, dem gemeinsamen Ästuar dieser beiden Ströme, liegen die Stadt Buenos Aires und die gleichnamige Provinz Buenos Aires, das wirtschaftliche Herz Argentiniens. Hier konzentriert sich auch etwa ein Drittel der Einwohner des Landes.

Westlich und südlich von Buenos Aires erstrecken sich die Pampas, eine grasbewachsene Ebene, wo der größte Teil der Agrarprodukte des Landes erzeugt wird. In dieser Region befinden sich große Weizenfelder und Weideflächen für Rinder; die Ausfuhr von Rindfleisch ist jedoch seit 2005 von 771.000 Tonnen auf 190.000 Tonnen als Folge von Exportbeschränkungen und -verboten der Regierung eingebrochen.

Zwischen den Pampas und den Anden liegen im zentralen Argentinien die Gebirgszüge der Sierras Pampeanas. Diese Mittelgebirge erreichen Höhen von 2800 m in den Sierras de Córdoba und bis zu 6250 m in der Sierra de Famatina in La Rioja. Das im Süden Argentiniens gelegene Patagonien ist von starken Westwinden geprägt und hat ein sehr raues Klima. Dieses Gebiet, das etwa ein Viertel der Fläche des Landes ausmacht, ist sehr dünn besiedelt. Der tiefste Punkt des Landes und Gesamtamerikas ist die Laguna del Carbón mit 105 m unter dem Meeresspiegel. Sie befindet sich zwischen Puerto San Julián und Comandante Luis Piedra Buena in der Provinz Santa Cruz.

Ein etwa 60 km langer Abschnitt der Grenze zu Chile, der sich im Südpatagonischen Eisfeld befindet, ist nicht als klar gezogene Grenze markiert, sondern wird von einer zwischen den beiden Staaten vereinbarten besonderen Zone eingenommen.

Von Argentinien wird ein Sektor des antarktischen Kontinents beansprucht; dieser Anspruch kollidiert jedoch mit dem Antarktisvertrag, der seit 1961 in Kraft ist.

In den argentinischen Anden gibt es viele über 6000 m hohe Berge. Zu ihnen zählen der höchste Berg des amerikanischen Kontinents, der Aconcagua mit 6962 m Höhe und die beiden höchsten Vulkane der Erde, der Ojos del Salado mit 6880 m und der Monte Pissis mit 6795 m. In den Südanden sind die Berge weniger hoch; viele sind wegen des feuchtkalten Klimas stets schneebedeckt. Auch in den Sierras Pampeanas werden teilweise sehr große Höhen gemessen: Die "Sierra de Famatina" in der Provinz La Rioja erreicht ebenfalls über 6000 m. Die Höhen dieses Gebirgskomplexes fallen jedoch nach Osten hin ab, in den Sierras de Córdoba werden nur noch maximal 2800 Meter erreicht.

Die nördlichen "Patagoniden" (Mesetas Patagoniens) weisen im Südosten von Mendoza immerhin noch 4700 m Höhe auf, ihre Höhe nimmt nach Südosten hin ab. In den anderen Gebieten Argentiniens erreichen die Berge nur in Ausnahmefällen über 1000 m Höhe. Darunter fallen die "Sierras Australes Bonaerenses" (Sierra de la Ventana und Sierra de Tandil) an der Atlantikküste und das Hügel- und Bergland von Misiones.

Argentiniens Hydrologie wird von den Zuflüssen des Río de la Plata dominiert. Sein Einzugsgebiet umfasst etwa 5.200.000 km². Etwa ein Drittel hiervon liegt in Argentinien, der Rest in Bolivien, Brasilien, Paraguay und Uruguay. Zuflüsse des Río de la Plata sind der Río Paraná und der Río Uruguay. Im Norden an der Grenze zu Brasilien befindet sich der Iguazú-Nationalpark. Darin der Fluss Iguazú mit den Iguazú-Wasserfällen, welche dreimal so groß wie die Niagarafälle sind.
Das zweitgrößte Einzugsgebiet hat der Río Colorado in Nordpatagonien, dessen größter Zufluss, der Río Salado del Oeste, einen Großteil Westargentiniens entwässert, wobei jedoch ein Großteil seines Wasservolumens wegen des trockenen Klimas bereits auf dem Weg verdunstet oder in Sumpfgebieten versickert.

Argentinien weist zwei größere Seengebiete auf. Das umfangreichste liegt am Fuß der Südanden, wo sich eine lange Kette von Schmelzwasserseen von der Provinz Neuquén bis nach Feuerland erstreckt. Daneben finden sich in der westlichen zentralen Pampa und im südlichen Chaco zahlreiche Flachlandseen, die teilweise nur wenige Meter tief und oft salzhaltig sind. Der Flachlandsee Mar Chiquita mit 5770 km² in der Provinz Córdoba sowie die Andenseen Lago Argentino (1415 km²) und Lago Viedma (1088 km²) liegen im Nationalpark Los Glaciares, der zum UNESCO-Welterbe erklärt wurde. Dort befindet sich auch der Gletscher Perito Moreno.

Argentinien hat trotz seiner lang gestreckten Küstenlinie nur wenige Inseln. Die größte ist die zum Archipel Feuerland gehörende Isla Grande de Tierra del Fuego mit 47.020 km², die sich Argentinien (Provinz Tierra del Fuego, 21.571 km²) und Chile (25.429 km²) teilen.
Das einzige weitere Inselgebiet von Bedeutung ist der Süden der Provinz Buenos Aires, wo sich in den Buchten Bahía Blanca und Bahía Anegada zwei ausgedehnte Wattenmeere befinden. Die Inseln dort sind flach und mit Ausnahme der "Isla Jabalí", auf der der Badeort "San Blas" liegt, unbewohnt. Größte Insel ist die Isla Trinidad mit 207 km². Des Weiteren gibt es vor der patagonischen Küste einige kleinere Felseninseln.

Völkerrechtlich umstrittenes Territorium sind die Falklandinseln (auch "Malwinen", englisch "Falkland Islands", spanisch "Islas Malvinas"), eine Inselgruppe im südlichen Atlantik. Sie gehören geographisch zu Südamerika, liegen 600 bis 800 km östlich von Südargentinien und Feuerland bei 52° Süd und 59° West und sind britisches Überseegebiet. Seit 1833 werden sie von Argentinien beansprucht. Die Besetzung der Inseln durch Argentinien am 2. April 1982 löste den Falklandkrieg aus, der bis zum 14. Juni 1982 dauerte und mit einer Niederlage für Argentinien endete. Die größten Inseln der Falkland Islands sind Ostfalkland (Soledad) mit 6683 km² und Westfalkland (Gran Malvina) mit 5278 km². Unter demselben Status befindet sich das südöstlich von den Falklandinseln gelegene Territorium Südgeorgien und die Südlichen Sandwichinseln.

Argentinien hat von tropischen Gebieten im äußersten Nordosten über subtropische im restlichen Norden und eine ausgedehnte gemäßigte Klimazone bis hin zu kalten Klimaregionen im Süden und in den Anden nahezu alle Klimazonen in einem Land vereint.

Der Nordwesten Argentiniens ist im Bereich der Anden trocken mit einer kurzen Regenzeit im Sommer. In ihr findet man die Hochwüste Puna, deren Westen zu den regenärmsten Gebieten der Welt zählt, sowie den steppenhaften, unfruchtbaren Monte am Fuß der Anden in den Provinzen Mendoza, San Juan und La Rioja.

Die Osthänge der Voranden beherbergen subtropische Nebelwälder in den Provinzen Tucumán, Salta und Jujuy, die im Sommer wegen des Abregnens der feuchten Ostwinde sehr niederschlagsreich, im Winter aber relativ trocken sind. Nach Osten hin schließt sich der Gran Chaco im zentralen Norden an, seine Niederschläge konzentrieren sich auf den Sommer, das Gleiche gilt für die Region der Sierras Pampeanas in Zentralargentinien. In beiden Regionen nehmen die Niederschläge nach Westen hin ab.

Der Nordosten sowie die Pampa-Region sind das ganze Jahr über feucht, wobei die höchsten Niederschlagsmengen im subtropischen Regenwald der Provinz Misiones auftreten.

Der Süden (Patagonien) liegt in der Westwindzone, weshalb hier der westliche Teil mehr Niederschläge als der Osten erhält. Die Anden sind ständig feucht und von der Temperatur kühl gemäßigt. Sie wirken als Barriere für die feuchten Pazifikwinde, so dass das östlich anschließende patagonische Schichtstufenland sehr trocken und halbwüstenhaft ist. In dieser Region bestimmt der regelmäßig alle ein bis zwei Wochen vom Südwesten her blasende Pampero-Wind das Klima. Ein Sonderfall ist das Klima im südlichen Teil Feuerlands mit kühlem ozeanischem Klima, wo wegen der dort fehlenden Klimascheide der Anden sowohl pazifische als auch atlantische Einflüsse das Wetter bestimmen. Dort sind die Niederschlagsmengen relativ hoch und die Temperaturen weisen eine relativ geringe Abweichung zwischen Sommer und Winter auf.

Entsprechend den sehr unterschiedlichen Klimazonen Argentiniens variieren auch die Vegetation und die Tierwelt sehr stark. Insgesamt sind etwa zwölf Prozent der Landfläche bewaldet.

In den warmfeuchten tropischen und subtropischen Regenwäldern im Norden gedeihen tropische Pflanzen, wie Rosenhölzer "(Dalbergia)", Guajakholzbäume "(Guaiacum officinale)", Palisander "(Jacaranda mimosifolia)" und Quebracho-Bäume "(Schinopsis lorentzii)", aus denen Gerbsäure gewonnen wird, aber auch Palmen. Der Gran Chaco, ebenfalls im Norden Argentiniens, verfügt über eine savannenartige Vegetation, welche von den Algarrobo-Bäumen (hauptsächlich "Prosopis alba" und "Prosopis nigra") dominiert wird, Quebracho kommt auch vor. Der Süden und Osten des Chaco mit seinem milderen Klima wird intensiv landwirtschaftlich genutzt, während der Norden noch weitgehend ursprünglich ist.

Die Pampa ist geprägt von ausgedehnten Graslandschaften mit verschiedensten Gräsern. Von Eukalyptus "(Eucalyptus)", amerikanischen Platanen "(Platanus occidentalis)" und Akazien "(Acacia)" abgesehen, finden sich hier keine Bäume; die ersteren beiden Gattungen sind nicht heimisch. Aufgrund des sehr feinen steinfreien Bodens ist eine landwirtschaftliche Bebauung gut möglich, so dass sich nur noch wenig ursprüngliche Vegetation erhalten hat.

Patagonien liegt schon im Schatten der Anden und ist eine karge und weitestgehend baumlose Landschaft. Hier herrschen wie in der Pampa auch die Gräser vor, die Vegetation ist aber den wesentlich trockeneren Gegebenheiten angepasst. Daneben findet man verschiedenste krautige Gewächse und Sträucher. Wegen des steinigen Bodens ist Getreideanbau nicht möglich, stattdessen werden die Graslandschaften als Schafweide genutzt.

In den Vorgebirgen der Anden und auf Feuerland finden sich ausgedehnte Nadelwälder mit Fichten "(Picea)", Zypressen "(Cypressus)", Kiefern "(Pinus)", Zedern "(Cedrus)" und anderen Nutzhölzern. Nahe der chilenischen Grenze gibt es vereinzelte Gruppen von Scheinbuchen "(Nothofagus)". Die Baumgrenze liegt bei etwa 3500 m. In den trockenen nördlichen Hochlagen der Anden finden sich in den ariden Halbwüsten viele Kakteengewächse (Cactaceae) und Dornsträucher.

Die Blüte des Ceibos (Hahnenkammbaum oder Korallenbaum) ist als sogenannte „nationale Blume“ eines der Nationalsymbole.

Im tropischen Norden ist die Tierwelt äußerst vielfältig. Hier kann man hauptsächlich verschiedene Affenarten, Jaguare, Pumas, Ozelots, Waschbären, Nasenbären, Ameisenbären, aber auch Tapire, Nabelschweine und Reptilien wie Schlangen und Kaimane antreffen. Die Vogelwelt beherbergt im tropischen Norden Kolibris, Flamingos, Tukane und Papageien. In den Flüssen sind neben vielen anderen Fischen auch Piranhas zu finden. In der Pampa findet man Gürteltiere, Mähnenwölfe, Pampasfüchse, Pampaskatzen, Pampashirsche, Nandus, verschiedene Greifvögel wie Falken sowie Reiher. In den kargen Gebieten der Anden trifft man auf die wilden Lamas, Guanakos und Vikunjas, sowie auf den Andenkondor, der zu den größten Vögeln der Welt gehört. Raubtiere sind die Bergkatze, der Puma und der Andenschakal. An Salzseen finden sich häufig Zugvögel wie Flamingos. In Patagonien und Feuerland ist das Tierleben artenärmer. Auch hier leben Pumas, Nandus und Guanakos; der Patagonischer Huemul und Pudú (ein kleiner Hirsch) sind der südlichen Anden. Auf Feuerland nisten zudem Kormorane und Magellanspecht. Die patagonischen Küsten beherbergen Magellanpinguine und Kolonien von Südamerikanischen Seebären und Mähnenrobben. Die Küstengewässer Argentiniens beherbergen unter anderem Südkaper, Orcas und Commerson-Delfine, daneben Seehechte, Sardinen, Makrelen und Dorados.

Argentinien hat eine Bevölkerung von etwa 44 Millionen Einwohnern (laut dem "CIA World Factbook" mit Stand von Juli 2016). Dies entspricht einer Bevölkerungsdichte von 15,8 Einwohnern/km². Etwa 87 % der Bevölkerung leben in "Städten" von mehr als 2000 Einwohnern, wovon allein 11,5 Millionen auf die Agglomeration Gran Buenos Aires entfallen. Diese hat eine Bevölkerungsdichte von 2.989 Einwohnern/km². Die Stadt und die gesamte Provinz Buenos Aires zusammen haben 16,6 Millionen Einwohner, die Provinzen Córdoba und Santa Fe jeweils ca. drei Millionen, so dass in diesen drei im zentralen Teil des Landes gelegenen Provinzen zusammen mehr als 60 % der Bevölkerung leben. Weite Teile des übrigen Landes sind dagegen sehr dünn besiedelt, vor allem im trockenen Süden, wo nur etwa ein bis drei Einwohner/km² leben.

Die Lebenserwartung betrug im Zeitraum von 2010 bis 2015 76,0 Jahre (Frauen 79,8, Männer: 72,2)

Mehr als 90 % der Bevölkerung stammen nach der offiziellen Statistik von eingewanderten "Europäern" ab, hiervon 36 % von Italienern, 29 % von Spaniern und 3 bis 4 % von Deutschen. Im Raum Buenos Aires sowie in den Provinzen Chaco und Misiones spielt auch die polnische Kultur eine Rolle. Hierbei handelt es sich um Nachkommen polnischer Emigranten aus den 1920er Jahren. Bis Anfang der 1990er Jahre ging man von einem Anteil der Mestizen – Nachfahren sowohl von Europäern als auch von Indianern – unter 10 % aus. Nach neueren Berechnungen ist deren Anteil jedoch weitaus höher. Diese Diskrepanz kommt vermutlich daher, dass die Mestizen früher unter einer starken Diskriminierung zu leiden hatten und sich daher als „Weiße“ ausgaben.
Nur eine Minderheit der Argentinier sind Nachkommen von insgesamt 30 Ethnien, die vor dem Eintreffen der Spanier auf dem Landesterritorium lebten. Dies liegt einerseits daran, dass Argentinien vor der Kolonialzeit nur im Nordwesten dicht bevölkert war, zum anderen auch daran, dass die verbleibenden Indianer von den Spaniern und später von den Argentiniern weitgehend ausgerottet wurden. Vom staatlichen Indianerinstitut "INAI" wird die Zahl der Indianer auf etwa 1 Million, von Seiten der Indianerorganisationen wie der "AIRA" (Asociación de Indígenas de la República Argentina) jedoch auf mehr als 1,5 Millionen geschätzt.

In einem Sonderzensus des INDEC, der im Jahr 2004 durchgeführt wurde, wurde ermittelt, dass etwa 2,8 % aller argentinischen Haushalte indigene Haushaltsmitglieder haben. Dieser Anteil variiert allerdings von Provinz zu Provinz stark. So ist in der Provinz Jujuy der Anteil mit 10,5 % am größten. Am niedrigsten ist der Anteil in der Provinz Corrientes mit 1,0 %, in der Hauptstadt Buenos Aires beträgt er 2,3 %.

Die größten Gruppen sind die Kollas in Jujuy und Salta, die Mapuche (Araukaner) in Neuquén und Río Negro sowie die Wichí und Toba im Chaco und in Formosa. Nur eine Minderheit der Indianer lebt in ihren angestammten Siedlungsgebieten, viele sind in die Großstädte übergesiedelt, wo sie oft unter ärmlichen Bedingungen als schlecht bezahlte Arbeiter leben. So gibt es in Rosario und Resistencia Viertel, die nur von Toba-Indianern bewohnt werden, dasselbe gilt für Kollas in San Salvador de Jujuy und San Miguel de Tucumán. Seit den 1980er Jahren erstarken innerhalb dieser Stämme Bewegungen, die traditionelle Kultur gezielt zu erhalten und verbreiten, etwa über Radiostationen und an Schulen.

Die Zahl der Ausländer lag bei der Volkszählung 2001 bei 1.531.940 (4,2 % der Bevölkerung), dabei sind die größten Gruppen Paraguayer (325.046), Bolivianer (233.464), Italiener (216.718) und Chilenen (212.429). Den höchsten Anteil von im Ausland Geborenen haben die Provinz Santa Cruz (12 %), die Stadt Buenos Aires sowie Tierra del Fuego (beide 11 %).

Historisch gesehen wurde die größte Einwanderungswelle zwischen 1880 und 1930 verzeichnet, fast ausschließlich aus Europa. Danach flachte die Migration nach Argentinien immer weiter ab, abgesehen von einem kurzzeitigen Wiederaufflammen zur Zeit des Zweiten Weltkrieges. Nach einer Phase negativen Wanderungssaldos zwischen 1975 und 2001 ist die Bilanz seit der Argentinienkrise derzeit wieder leicht positiv. Heute wandern vor allem Bürger der Nachbarländer Bolivien, Paraguay und Uruguay sowie aus dem südamerikanischen Staat Peru nach Argentinien ein. Zu Zeiten der Pinochet-Diktatur fand die Einwanderung auch aus Chile statt, dies hat sich jedoch aufgrund der Redemokratisierung und des mittlerweile höheren Lebensstandards des Nachbarlandes nach 2001 umgekehrt. Insgesamt kommen etwa 68 % der Einwanderer aus amerikanischen Staaten. Etwa 2 % aller Einwanderer kommen aus Asien (hauptsächlich Koreaner).
Seit den 1990er Jahren findet man immer mehr Einwanderer aus Europa, die hauptsächlich wegen der unberührten Natur hierher ziehen. Im Unterschied zu den anderen Einwanderern weisen sie meist schon eine gesicherte Existenz auf oder sind Rentner, versuchen also durch den Umzug ihre Lebensqualität zu erhöhen. Andere Ausländergruppen (besonders Italiener und Spanier) sind noch lebende Einwanderer der Hauptwelle (bis 1950). Europäer repräsentieren etwa 28 % der Ausländer.

Seit der Argentinien-Krise zwischen 1998 und 2002 sind vermehrt Emigrationswellen aufgetreten. Argentinier verließen das Land in Richtung Europa und Nordamerika, in geringeren Maßen auch nach Brasilien und Chile. Diese Emigrationswelle ist jedoch aufgrund der relativ schnellen Erholung der argentinischen Wirtschaft weitgehend abgeebbt.

Argentinien hat seit dem 20. Mai 1955 keine Staatsreligion mehr, welche davor die römisch-katholische Konfession war. Katholizismus genießt nach der Verfassung aber einen bevorzugten Status. Etwa 76,5 % der Bevölkerung sind römisch-katholischen Glaubens. Neben dem Katholizismus bestehen offiziell über 2500 registrierte Kulte und Religionen. Darunter Protestantismus (9 %), Zeugen Jehovas (ca. 1,2 %), und andere (ca. 1,2 %) zum Beispiel der Pachamama-Kultus im Nordwesten Argentiniens, der durch Verschmelzung christlicher Riten mit indigenen Religionen entstand. Der Erzbischof von Buenos Aires, Jorge Mario Bergoglio SJ, wurde am 13. März 2013 durch das Konklave zum Papst gewählt und ist somit der erste Papst aus Lateinamerika. Bergoglio wählte den Namen Franziskus. Rund 13 % der Bevölkerung gab bei einer Umfrage 2013 („Die Religionen in Zeiten eines Papst Franziskus“) an, dass sie religionsfrei seien.

Alleinige Amtssprache ist in Argentinien Spanisch. Daneben gibt es eine Reihe von mehr oder weniger verbreiteten Minderheitensprachen, die von der indianischen Bevölkerung gesprochen werden. Die verbreitetsten darunter sind das Quechua (in zwei lokalen Varianten) und das Guaraní, in manchen Gegenden wird auch noch Mapudungun gesprochen. Am höchsten ist die Sprecherzahl von autochthonen Sprachen bei den Indianergruppen im Chaco, den Wichí, Pilagá und Toba, bei denen mehr als die Hälfte noch ihre angestammte Sprache versteht. Bei anderen Gruppen wie den Kolla und Mapuche ist diese Zahl weit geringer.

Die argentinische Aussprache des Spanischen unterscheidet sich deutlich von der in Spanien und auch von der in anderen lateinamerikanischen Ländern üblichen. Der Buchstabe "ll" wird wie das deutsche "sch" oder wie das französische "j" ausgesprochen, ebenso der Buchstabe "y" zwischen Vokalen und ein konsonantisches "y" am Wortbeginn; dieses Phänomen wird als Yeísmo bezeichnet. Der Buchstabe "z" wird immer wie ein stimmloses "s" ausgesprochen, das gleiche trifft auf das "c" vor "e" und "i" zu, dies nennt man Seseo. Des Weiteren herrscht in Argentinien der Voseo vor, d. h. anstatt des Personalpronomens "tú" für die 2. Person Singular wird "vos" verwendet. Die Verben werden dabei anders konjugiert (im Präsens immer endbetont und mit abweichenden Imperativformen). Weiterhin wird die 2. Person Plural "vosotros" auch in informeller Sprache durch die 3. Person Plural "ustedes" ersetzt, die im europäischen Spanisch nur die Höflichkeitsform ist. Darüber hinaus gibt es eine Reihe lexikalischer Abweichungen.

Während ein Großteil der Nachfahren italienischer Einwanderer in Argentinien die Sprache ihrer Vorfahren aufgegeben hat, wird von den Nachfahren der deutschsprachigen und englischsprachigen Einwanderer teilweise noch die Sprache ihrer Vorfahren gepflegt. So gibt es Stadtviertel im Großraum Buenos Aires, in denen man noch sehr viel Deutsch hört. In der Provinz Córdoba gibt es eine relativ große Kolonie von Überlebenden des Kriegsschiffs "Admiral Graf Spee" aus dem Zweiten Weltkrieg, die sich in Villa General Belgrano ansiedelte, wo heute noch teilweise Deutsch gesprochen wird.

"Siehe auch:" Río-de-la-Plata-Spanisch, Belgranodeutsch, Cocoliche, Quechua

In der Kolonialzeit lag der Schwerpunkt der argentinischen Bevölkerung lange im Nordwesten, und insbesondere in der Minenregion um Salta und Jujuy. Größte Stadt war das am Kreuzungspunkt mehrerer Handelsrouten gelegene Córdoba. Dies änderte sich mit der Einrichtung des Vizekönigreiches Río de la Plata 1776. Der Handel ließ nun die Bevölkerungszahl der Küstenregion im Osten des Landes (Buenos Aires, Santa Fe, Entre Rios) sprunghaft ansteigen, und nach der Erringung der Unabhängigkeit hatte sich die wirtschaftliche und politische Macht endgültig in dieser Region konzentriert. Das Gebiet südlich einer Linie etwa zwischen dem heutigen La Plata und Mendoza war dagegen bis zur Wüstenkampagne des General Roca in den 1870er Jahren noch von den Indianern bewohnt, es gab allerdings einige spanische und walisische Enklaven.
Die Einwanderungswelle 1880–1930 verstärkte die Dominanz der Küstenregion und besonders von Stadt und Provinz Buenos Aires zusätzlich, da sich der Großteil der Einwanderer in dieser Gegend niederließ. Der Nordwesten wurde mehr und mehr zu einer rückständigen und wirtschaftlich schwachen Region, in dem relativ wenig Einwanderung stattfand, und Patagonien befand sich erst am Beginn seiner Entwicklung. Der Großraum Buenos Aires wuchs so zwischen 1850 und 1914 von 150.000 auf 1,6 Millionen Einwohner.
Nach dem Versiegen des Einwandererstroms um 1930 brachte die Industrialisierung einen Binnenwandererstrom, dessen Ziel ebenfalls Buenos Aires und – mit Abstand – Córdoba und Rosario war. Dieser Strom hielt bis in die 1970er Jahre an und führte dazu, dass sich der Großraum rund um die Hauptstadt weit über das eigentliche Stadtgebiet von Buenos Aires ausdehnte.

1980 überschritt der Großraum Buenos Aires im nationalen Zensus zum ersten Mal die 10-Millionen-Marke und konzentrierte damit fast 40 % der Bevölkerung (damals 24 Millionen). Danach flachte das Wachstum der Städte der Küstenregion allerdings deutlich ab. Zwischen 1991 und 2001 verlor die Stadt Buenos Aires 7 % ihrer Einwohner, die Bevölkerung des Ballungsraums der Stadt insgesamt stieg nur noch leicht an, auch Rosario und Santa Fe stagnierten. Zum Wachstumsmagnet wurden dagegen abgelegene Regionen wie das wirtschaftlich boomende Patagonien, insbesondere die südlichsten Provinzen Provinz Tierra del Fuego und Santa Cruz (44 % bzw. 23 % Zuwachs zwischen 1991 und 2001), aber auch die Städte des Nordwestens wie Jujuy, Salta, La Rioja und Tucumán sowie der Ballungsraum Córdoba.

In Buenos Aires und den meisten Großstädten gibt es seit etwa 1980 das Phänomen der Stadtflucht: Viele, meist besser verdienende Einwohner siedeln von den Stadtzentren ins Umland um. Seit etwa 1990 hat sich dieses Phänomen durch die massenhafte Einrichtung von privaten Stadtvierteln und Country Clubs noch verstärkt. Die Ursache liegt in der als steigend empfundenen Kriminalität. Auch touristisch und landschaftlich interessante Orte erleben seit dieser Zeit einen Boom, was auch mit der steigenden Mobilität der Bevölkerung sowie der inzwischen deutlich besseren Verfügbarkeit von infrastrukturellen Dienstleistungen wie Telefon, Radio, Fernsehen und Internet selbst in weit entlegenen Gebieten zusammenhängt. So wurden aus ehemals kleinen Ferienorten wie Merlo, Pinamar und Villa Carlos Paz prosperierende, schnell wachsende Städte.

Die soziale Situation des Landes ist in mehrerlei Hinsicht durch eine starke Ungleichheit gekennzeichnet. So gibt es einerseits wie in ganz Lateinamerika ein großes Wohlstandsgefälle zwischen Ober- und Unterklasse.

Aber auch die Unterschiede zwischen den Regionen Argentiniens sind groß. So lag etwa die Armutsrate, die nach einem Warenkorb berechnet wird, im Jahr 2008 in der Hauptstadt Buenos Aires mit etwa 15 % nur etwas mehr als halb so hoch wie im Landesdurchschnitt (23 %), während sie in der Nordostregion bei 41 % liegt (Stand 2007). Eine Durchschnittsperson benötigte im März 2008 monatlich etwa 317 AR$, um nicht unter die Armutslinie zu fallen. In den meisten Haushalten ist es daher nötig, dass mehrere Familienmitglieder zum Einkommen beitragen. Dies zeigt auch die offizielle Statistik: So liegt das durchschnittliche monatliche Pro-Kopf-Einkommen bei etwa 1156 AR$ und damit nur knapp über der Armutsrate für Familien, während das durchschnittliche monatliche Haushaltseinkommen bei 2090 AR$ liegt (s. u.).

Die nördlichen Provinzen, besonders die Provinz Tucumán und der Nordosten (Chaco, Formosa, Santiago del Estero) waren bis um den Jahrtausendwechsel am stärksten von Armut und Unterernährung betroffen. Verschärft wurde diese Situation durch das relativ hohe Bevölkerungswachstum in dieser Region. Als relativ reich dagegen galten die zentralen Provinzen (Buenos Aires, Santa Fe, Córdoba, San Luis und Mendoza), aber auch der äußerste Süden (Santa Cruz und Tierra del Fuego).
Es sind neben den grenznahen Gegenden (beispielsweise Jujuy und Formosa) allerdings doch vor allem die reichen Zentralprovinzen, die am stärksten mit der städtischen Armut und damit mit der Bildung von Elendsvierteln zu kämpfen haben. Die Zuwanderung aus den ärmeren Nachbarländern Peru, Bolivien und Paraguay sowie die Binnenwanderung aus abgelegenen Gegenden des Landesinneren waren trotz einer Abschwächung in den 1990er Jahren ein Problem in den Großstädten, die die Zahl der Elendsviertelbewohner trotz sozialer Wohnungsprogramme weiterhin anwachsen ließ. So lag 2004 beispielsweise in Rosario der Anteil der Elendsviertelbewohner an der Gesamtbevölkerung bei über 15 %. Zudem kam Zuwachs für die Elendsviertel auch von den so genannten Neu-Armen, besonders in den wirtschaftlich kritischen Jahren 1989/1990, 1995 sowie zwischen 1998 und 2002.
In der Argentinien-Krise verschlechterten sich insbesondere in den Jahren 2001 und 2002 viele Indikatoren der sozialen Situation in kürzester Zeit. Die Armutsrate nach einem Warenkorb berechnet stieg auf über 50 %. Ab 2003 normalisierten sich die Werte langsam wieder, allerdings blieb bis 2006 die Armutsrate trotz eines Rückgangs weiterhin mit über 20 % deutlich über den Werten der 1990er Jahre. Dabei waren in der am stärksten betroffenen Región Noreste Argentino (Nordostregion) weiterhin fast die Hälfte der Bevölkerung arm.

Nach langjähriger Regentschaft der Peronisten war die Wirtschaft in einem desolaten Zustand. Der neue konservative Präsident musste 2016 ein hartes Sparprogramm fahren. Rund ein Drittel der Argentinier lebten zu jenem Zeitpunkt unter der Armutsgrenze.

Einige Daten zur sozialen Situation:

Bei der Armuts- und Elendsrate variieren die Einkommen, nach denen sich die Rate richtet, je nach Region, daher wird nur ein ungefährer Durchschnittswert angegeben. Bei der Inflationsrate wird der Wert nur im Großraum Buenos Aires errechnet. Die Daten des INDEC für den Preisindex wurden allerdings mehrfach angezweifelt, die Haltung internationaler Organisationen wie dem IWF dazu ist uneinig.

Die Forschung nimmt an, dass die Besiedlung des heutigen Argentinien durch den Menschen etwa 15000 v. Chr. von Nordamerika aus erfolgte.

Die im Pampa-Raum des heutigen Argentinien ansässigen Pampas-Indianer Het (Querandíes), Charrúa und andere kleine Stämme waren bis zum Eintreffen der Spanier nicht sesshaft und lebten als Jäger und Sammler oder Fischer. Die Stämme im Nordwesten des Landes hingegen (z. B. die Diaguita) praktizierten etwa ab der Zeit des frühen europäischen Mittelalters Ackerbau und Viehzucht und waren vor allem auf architektonischem Gebiet weit fortgeschritten. Im 13. und 14. Jahrhundert expandierte das Inka-Reich stark nach Süden und umfasste um 1450 weite Teile des Nordwestens Argentiniens bis in den Norden der heutigen Provinz Mendoza.

Die Europäer erreichten die Region erstmals mit der Reise Amerigo Vespuccis 1502. Das heutige Argentinien wurde im 16. Jahrhundert von den Spaniern aus zwei Richtungen kolonisiert: Von Peru aus nahmen sie die nordwestlichen Teile des Landes in Besitz (Gran Chaco), während andererseits vom Atlantik aus spanische Niederlassungen am Stromsystem des Río de la Plata gegründet wurden, darunter Buenos Aires. Dort konnten sich die Spanier im Jahre 1580 auf Dauer etablieren, nachdem ein erster Versuch zur Gründung einer spanischen Siedlung dort im Jahre 1536 am Widerstand der indigenen Pampas-Bewohner gescheitert war.

Die weiter südlich gelegenen Gebiete des heutigen Argentinien wurden zwar theoretisch auch von Spanien beansprucht, blieben aber in der Kolonialzeit faktisch außerhalb des spanischen Herrschaftsbereichs. Sie waren für 300 Jahre das Reich der araukanischen Reitervölker, die durch den kulturellen Einfluss des Mapuche-Volkes aus Chile bis in die 1880er Jahre ihre Unabhängigkeit wahren konnten.

Administrativ war das heutige Argentinien zunächst Teil des Vizekönigreichs Peru, das Südamerika mit Ausnahme der portugiesischen Einflusssphäre umfasste. Im Jahre 1776 wurde von diesem das Vizekönigreich des Río de la Plata mit Hauptstadt Buenos Aires abgespalten, welches neben Argentinien noch das heutige Bolivien, Paraguay und Uruguay umfasste.

Die unter dem Eindruck der Französischen Revolution und der Koalitionskriege in Europa am 25. Mai 1810 in Buenos Aires erklärte Unabhängigkeit hatte als Mai-Revolution zunächst nur lokale Wirkung, führte aber zu einem landesweiten Befreiungskrieg gegen die Spanier. Die Unabhängigkeit erlangte das Land schließlich am 9. Juli 1816 in San Miguel de Tucumán. Wie zuvor Paraguay im Jahre 1811 spalteten sich dann auch 1825 Bolivien und 1828 Uruguay von den damaligen Vereinigten Provinzen des Río de la Plata ab.

Zwischen 1816 und 1880 war die Entwicklung Argentiniens von Diktaturen (unter dem Bonarenser Gouverneur Juan Manuel de Rosas) und Bürgerkriegen geprägt. Die Provinzen waren zunächst weitgehend autonom, nur 1826–1827 konnte das Land kurzzeitig geeint werden. 1853 wurde zunächst ohne die abtrünnige Provinz Buenos Aires die heutige Argentinische Republik gegründet und eine föderalistische Verfassung in deren erster Hauptstadt Paraná verabschiedet. In den Jahren 1861 und 1862 schloss sich die Provinz Buenos Aires nach einer militärischen Auseinandersetzung wieder an, es wurden landesweite Wahlen ausgerufen, und erster gesamtargentinischer Präsident wurde Bartolomé Mitre. In dessen Regierungszeit fiel der Tripel-Allianz-Krieg 1864 bis 1870, in dem sich Argentinien gemeinsam mit Brasilien und Uruguay gegen expansive Tendenzen Paraguays durchsetzte, das sich zu dieser Zeit zu einer der stärksten Militärmächte Südamerikas entwickelt hatte. Argentinien gewann durch diesen Krieg das Gebiet der heutigen Bundesstaaten Misiones, Formosa und Chaco hinzu.

Die Jahre von 1880 bis 1912 waren durch die zahlreiche Einwanderung vor allem von Italienern und Spaniern gekennzeichnet, die sich in den Städten und in sogenannten „Kolonien“ auf dem Land ansiedelten. Politisch ist diese Zeit als Scheindemokratie zu bezeichnen, denn die Regierung Julio Argentino Roca und die folgenden Regierungen waren oligarchisch ausgerichtet, mit großem Einfluss der Großgrundbesitzer. Dem Gros der Bevölkerung wurden durch ein ausgeklügeltes Wahlbetrugssystem durch die Regierungspartei Partido Autonomista Nacional, die von 1874 bis 1916 ununterbrochen regierte, die politischen Rechte vorenthalten; auch die Einwanderer hatten kein Stimmrecht.

Ab 1893 verschärften sich die Grenzprobleme mit Chile, nachdem Bolivien einen Teil der Puna de Atacama an Argentinien abgetreten hatte. Diese war seit dem Salpeterkrieg von Chile besetzt. Zwischen Chile und Argentinien kam es zu einem Wettrüsten. Erst der britische König Edward VII. konnte 1902 den Grenzstreit schlichten. Patagonien und Feuerland wurden neu aufgeteilt, dabei fielen 54.000 km² an Chile und 40.000 km² an Argentinien.

1912 wurde vom Präsidenten und Leiter des liberalen Flügels der PAN, Roque Sáenz Peña, das allgemeine Wahlrecht für Männer eingeführt. In der Folge kam 1916 die aus der bürgerlichen Protestbewegung hervorgegangene Unión Cívica Radical an die Regierung. Es folgte die wechselhafte so genannte Etapa Radical von 1916 bis 1930. Die Unión Cívica Radical regierte bis 1930, als ein Militärputsch wieder ein konservatives System einführte. Vor allem die 1930er Jahre werden heute als "Década infame", als berüchtigtes Jahrzehnt bezeichnet, in dem die Demokratie nur auf dem Papier existierte und Wahlbetrug an der Tagesordnung war.

Im Laufe der ersten Hälfte der 1940er Jahre gelang es dem jungen Offizier Juan Domingo Perón, sich geschickt an die Macht zu manövrieren. Er war zunächst unter dem Militärregime Ramírez Minister für Arbeit und wurde wegen seiner weitreichenden Zugeständnisse an die Gewerkschaften schnell zu einem Volkshelden in der Arbeiterklasse, so dass nach seinem Sturz im Juli 1945 Massendemonstrationen seine Rückkehr erzwangen. Im Jahre 1946 wurde er zum Präsidenten gewählt.

Im Zweiten Weltkrieg war Argentinien offiziell neutral. Es sympathisierte zunächst mit den Achsenmächten, unterstützte gegen Kriegsende jedoch die Alliierten. Während des Krieges war Argentinien Zielland von Flüchtlingen aus Europa; nach dem Krieg fanden in Argentinien ebenso wie in anderen Staaten Lateinamerikas zahlreiche Nationalsozialisten und Faschisten Unterschlupf. Unter den prominentesten nationalsozialistischen Kriegsverbrechern in Argentinien waren Adolf Eichmann, der 1960 vom Mossad entführt und in Israel zum Tode verurteilt wurde, Josef Mengele sowie Walther Rauff. Über sogenannte "Schlüsselfirmen" wurden auch hohe Vermögenswerte der Nationalsozialisten nach Argentinien verschoben.

März 2015 wurde die Entdeckung eines in einem Waldgebiet des Naturparks Teyu Cuare etwa 1000 Kilometer nördlich der Landeshauptstadt Buenos Aires gelegenen Gebäudes aus den 1940er Jahren bekannt. Es wurde nie benutzt. Indizien wie Baustil und gefundene Gegenstände sprechen dafür, dass es als Versteck für flüchtige Nazi-Größen gedacht war, so das Zentrum für Stadtarchäologie (CAU). "Die nationale Kommission zur Aufklärung von Nazi-Aktivitäten (CEANA) schätzt, dass sich mindestens 180 Kriegsverbrecher in das südamerikanische Land abgesetzt haben."

Unter Perón, der mit faschistischem Gedankengut sympathisierte, verfolgte Argentinien das Ziel, durch Zugeständnisse an die Arbeiter den Kommunismus abzuwehren. In seiner ersten Regierungszeit wurde die Industrialisierung des Landes, die nach der Weltwirtschaftskrise um 1930 begonnen hatte, vertieft und eine Importsubstitutionspolitik durchgesetzt. Die forcierte Industrialisierung und die aktive Sozialpolitik führte zu einem nie gekannten und bis heute nicht wieder erreichten Wohlstandsniveau für die Massen, die deshalb das zunehmend autoritär werdende Regime unterstützten, jedoch auch zu steigender Inflation und Staatsverschuldung. In der zweiten Amtszeit Peróns kam es zu wirtschaftlichen Schwierigkeiten und Konflikten mit der mächtigen katholischen Kirche.

1955 wurde er bei einem Putsch abgesetzt und floh ins Exil.

Argentinien verzeichnete in der Folgezeit wirtschaftliche Höhen und Tiefen im Wechsel. Bis 1983 gab es eine Epoche der Instabilität, in der abwechselnd zivile und Militär-Regierungen das Land in der Hand hatten. Die demokratisch gewählten Regierungen Frondizis (1958–1962) und Illias (1963–1966) wurden von den antiperonistischen Militärs vorzeitig aus dem Amt geputscht. Von 1966 bis 1973 gab es unter Onganía und seinen Nachfolgern eine längere rechtskonservative Militärdiktatur, die jedoch nach Protesten der Bevölkerung 1973 schließlich aufgegeben wurde. Das Land fand kurzzeitig zur Demokratie zurück, der nach wie vor populäre Perón durfte wieder einreisen und konnte bald erneut die Macht erlangen.

Die zweite Amtszeit Peróns von Oktober 1973 bis zu seinem Tod am 1. Juli 1974 brachte nur eine geringfügige Beruhigung in die politischen und wirtschaftlichen Verhältnisse Argentiniens. Nach seinem Tod wurde seine dritte Ehefrau, Isabel Perón (genannt „Isabelita“), die er zur Vizepräsidentin gemacht hatte, auf Betreiben der peronistischen Partei als Präsidentin eingesetzt. Diese, eine ehemalige Nachtclubtänzerin, war mit diesem Amt völlig überfordert und diente lediglich als Marionette von rechten Peronisten wie José López Rega, der mit der Alianza Anticomunista Argentina schon unter Perón eine paramilitärische Gruppe eingesetzt hatte, die Regimegegner folterte und ermordete. Zudem nahmen wirtschaftliche Probleme zu, die Inflation stieg steil an. Mehrere Guerillagruppen (Guerilleros) wie die Montoneros waren in diesem Kontext aktiv und es kam zu verschiedenen Entführungen. Die Entführung des für Mercedes-Benz den Standort Argentinien betreuenden Produktionsleiters Heinrich Metz im Oktober 1975 (er kam später für ein Lösegeld in Höhe von mehreren Millionen US-Dollar wieder frei) löste eine Fluchtwelle unter den für deutsche Unternehmen in Argentinien tätigen Immigranten aus.

Im Jahr 1976 kam es erneut zu einem Militärputsch und es installierte sich unter der Führung von Jorge Rafael Videla eine Militärdiktatur, geleitet von einer Junta aus drei Mitgliedern, die mit einem offenen Staatsterror regierten. Die Zeit zwischen 1976 und 1978 wird daher auch als „Schmutziger Krieg“ bezeichnet. Unter den geschätzt 30.000 Desaparecidos („Verschwundenen“) befanden sich auch zahlreiche Studenten, deren Mütter sich zusammenschlossen, um auf dem Platz vor dem Regierungsgebäude (Plaza de Mayo) ungeachtet ihrer Selbstgefährdung zu demonstrieren, und damit in die Geschichte eingingen. Ziel der "Madres de Plaza de Mayo" ("Mütter der Plaza de Mayo"), war und ist es, Kenntnis über den Verbleib ihrer Kinder zu erhalten. Mittlerweile gibt es auch eine Organisation "Abuelas de Plaza de Mayo" ("Großmütter der Plaza de Mayo"), deren Zweck es ist, die in der Gefangenschaft geborenen und illegal zur Adoption freigegebenen Kinder der Verschwundenen in ihre Familie zurückzuführen. Nachdem man ihre Eltern getötet hatte, wurden die Waisen als Kriegsbeute von Menschen aufgezogen, die der Diktatur nahestanden. Nur etwa 100 dieser Kinder haben bis heute von ihrer wahren Identität erfahren. Von 400 weiteren fehlt trotz aller Bemühungen von Verwandten und den Suchenden bislang jede Spur. In späteren Gerichtsverfahren gegen verantwortliche Militärs, die nur mit Mühe durchgesetzt werden konnten, wurde bekannt, dass sich die militärischen Machthaber zahlreicher Menschen auf grausame Weise entledigt hatten: Die Opfer wurden betäubt und über dem Río de la Plata oder dem offenen Meer aus dem Flugzeug geworfen. Zu den Todesopfern der Diktatur gehörte 1977 auch die Deutsche Elisabeth Käsemann, der 2014 erstmals ausgestrahlte Dokumentarfilm Das Mädchen – Was geschah mit Elisabeth K.? enthält Stellungnahmen Hinterbliebener und politisch Verantwortlicher.

Um Souveränitätstreitigkeiten (siehe Beagle-Konflikt) über die Inseln an der südlichen Spitze Amerikas zu beenden, beauftragten Argentinien und Chile 1971 ein internationales Tribunal damit, über eine bindende Interpretation des Grenzvertrags von 1881 zu entscheiden. Das Schiedsgericht im Beagle-Konflikt entschied 1977, dass alle Inseln südlich der Isla Grande de Tierra del Fuego zu Chile gehören. 1978 erklärte Argentinien die Entscheidung für nichtig und bereitete die militärische Einnahme der Inseln (siehe Operation Soberanía) vor, nur durch die Vermittlung des Papstes konnte dies verhindert werden. Erst 1984, im Rahmen der Demokratisierung, anerkannte Argentinien – nach Austausch von Navigationsrechten und einer Verschiebung der maritimen Grenze nach Westen – im Freundschafts- und Friedensvertrag von 1984 zwischen Chile und Argentinien das Urteil endgültig.

Im April 1982 begann Argentinien unter dem neuen Junta-Chef Leopoldo Galtieri den Falklandkrieg gegen Großbritannien. Es ging um die Argentinien vorgelagerten Falklandinseln (in Argentinien als „Islas Malvinas“ bezeichnet), die nach argentinischer Rechtsauffassung zum eigenen Staatsgebiet gehören, jedoch ebenso von Großbritannien als eigenes Hoheitsgebiet betrachtet werden und seit 1833 unter dessen Verwaltung stehen. Die Invasion argentinischer Soldaten wurde von den Streitkräften des Vereinigten Königreichs mit Luftangriffen, einem Seekrieg und einer Landeoperation erfolgreich revidiert. Argentinien kapitulierte am 14. Juni 1982.

1983 kehrte das Land zur Demokratie zurück. Der erste Präsident dieser Epoche war Raúl Alfonsín (Unión Cívica Radical), der jedoch 1989 infolge einer schweren Wirtschaftskrise vorzeitig zurücktrat. Die Peronistische Partei kam mit Carlos Menem wieder an die Macht. Die neoliberale Wirtschaftspolitik Menems und die 1:1-Bindung des Argentinischen Peso an den US-Dollar war während seiner ersten Amtszeit äußerst erfolgreich und konnte das Land stabilisieren. Während seiner zweiten Amtszeit machten sich aber immer mehr die negativen Seiten dieser Wirtschaftspolitik bemerkbar.

Zwischen 1998 und 2002 fiel daher das Land erneut in eine schwere Wirtschaftskrise, in der die Wirtschaftskraft um 20 % zurückging. 1999 wurde die Regierung Menem durch eine Mitte-links-Koalition mit dem Präsidentschaftskandidaten Fernando de la Rúa abgelöst. De la Rúa konnte aber die verfahrene wirtschaftliche Situation, die sein Vorgänger hinterließ, nicht schnell und nachhaltig verbessern. Das zögerliche Handeln des Präsidenten, Streitereien innerhalb der Koalition und eine starke außerparlamentarische Opposition durch die Gewerkschaften, die traditionell den Peronisten nahestehen, schwächten de la Rúa zunehmend. Dies gipfelte Ende 2001 nach starken Unruhen und Plünderungen im Rücktritt von Präsident Fernando de la Rúa.

In der Folge gab es mehrere peronistische Interimspräsidenten, bis Eduardo Duhalde mit der Verwaltung der Krise beauftragt wurde. Im Mai 2003 wurde nach einer sehr chaotisch verlaufenden Präsidentschaftswahl Néstor Kirchner zum neuen Staatsoberhaupt gewählt, der dem sozialdemokratischen Flügel der Peronistischen Partei angehört. Trotz seines niedrigen Wahlergebnisses war Kirchner in seiner Amtszeit bei der Bevölkerung sehr beliebt, weil er die Krise erfolgreich überwinden und daher die Gesamtsituation des Landes verbessern konnte. Die Wirtschaft bekam einen starken Wachstumsschub: 2003 verbuchte Argentinien ein Wachstum des Bruttoinlandsproduktes in Höhe von +8,7 % gegenüber −10,9 % im Jahr 2002. Kirchner war jedoch auch Kritik ausgesetzt, insbesondere wegen seines autokratischen Führungsstils und zum Teil auch wegen seiner als Populismus gedeuteten Zusammenarbeit mit der Piquetero-Protestbewegung.

Bei den Wahlen zum argentinischen Senat und zur argentinischen Abgeordnetenkammer im Oktober 2005 gingen die Anhänger Néstor Kirchners mit etwa 40 % der Stimmen als Sieger hervor. Bei der Wahl um Senatorenposten der Provinz Buenos Aires gewann seine Frau Cristina Fernández de Kirchner gegen die Ehefrau des ehemaligen Präsidenten Eduardo Duhalde Hilda González de Duhalde, die ebenfalls der Peronistischen Partei angehört. Der Präsident wurde somit gestärkt und konnte sich in beiden Kammern auf eine breite Mehrheit auch innerhalb seiner eigenen Partei stützen.

Die Präsidentschafts- und Parlamentswahl am 28. Oktober 2007 konnten die regierenden Peronisten, insbesondere die Wahlplattform Kirchners, Frente para la Victoria, mit einem überwältigenden Sieg gewinnen. Cristina Fernández de Kirchner konnte sich schon im ersten Wahlgang mit 45,3 % der Stimmen durchsetzen und damit eine Stichwahl vermeiden. Sie trat das Präsidentenamt am 10. Dezember 2007 an. Auch im Parlament wurde der Kirchnerismo leicht gestärkt.

In der Folge war die Peronistische Partei von Flügelkämpfen betroffen. Mehrmals wurde sogar erwogen, die Partei auch offiziell zu spalten. Nachdem Kirchner aber 2008 den Parteivorsitz übernommen hatte, stabilisierte sich die Situation innerhalb der Regierungspartei wieder.

Bei den Parlamentswahlen am 28. Juni 2009 verlor die 'Frente para la Victoria' (FPV) allerdings. Daraufhin gab Néstor Kirchner den Parteivorsitz der Peronistischen Partei an den Gouverneur der Provinz Buenos Aires, Daniel Scioli, ab. Im Oktober 2010 erlag er einem Herzinfarkt.

Am 27. Oktober 2013 fanden in Argentinien Wahlen statt: ein Drittel der Senatoren wurde neu gewählt; daneben standen 127 der 257 Abgeordneten des Repräsentantenhauses zur Wahl.

2015 kam es zu einem Machtwechsel: Bei der Präsidentschaftswahl setzte sich in der ersten Stichwahl der argentinischen Geschichte Mauricio Macri, Parteivorsitzender der konservativen Partei Propuesta Republicana und seit 2007 Bürgermeister von Buenos Aires, knapp gegen den von der Regierung Kirchner unterstützten Kandidaten Daniel Scioli durch. Cristina Kirchner konnte laut der Verfassung Argentiniens nicht zur Wiederwahl antreten; sie war schon zwei Wahlperioden Präsidentin.

"Siehe auch:" Liste der Präsidenten von Argentinien, Argentinien-Krise

Nach der Verfassung von 1994 ist Argentinien eine föderalistische, republikanische Präsidialdemokratie. Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt das Land Platz 49 von 167 Ländern und gilt damit als eine „fehlerhafte Demokratie“. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „frei“ bewertet.

Der Präsident der Nation („Presidente de la Nación Argentina“, „Poder Ejecutivo Nacional“) ist Staatsoberhaupt und Regierungschef in Person und hat eine starke Stellung, unter anderem die Möglichkeit per Dekret zu regieren. Er wird gemeinsam mit dem Vizepräsidenten, der ihn bei Abwesenheit vertritt, alle vier Jahre (bis 1995: alle sechs Jahre) in zwei Wahlgängen direkt gewählt. Um in der ersten Runde zu gewinnen, muss der siegreiche Kandidat 45 oder mehr Prozent der gültigen Stimmen erreichen oder bei einem Wert zwischen 40 und 45 Prozent zehn Prozentpunkte Vorsprung vor dem Zweitplatzierten aufweisen, in allen anderen Fällen gibt es eine Stichwahl. Verzichtet einer der beiden erfolgreichsten Kandidaten in der ersten Runde auf die Teilnahme in der Stichwahl (zuletzt 2003), gilt der andere Kandidat als Sieger, der Drittplatzierte rückt also in diesem Fall nicht nach. Eine Präsidentschaft ist höchstens während zwei aufeinander folgenden Perioden möglich, eine erneute Kandidatur ist aber nach einer Pause von vier Jahren wieder erlaubt. Der Präsident muss unter anderem argentinischer Staatsbürger sein und bis zur Verfassungsreform 1994 dem römisch-katholischen Glauben angehören.

Die Legislative (Überbegriff: Congreso, Kongress, bestehend aus Abgeordnetenkammer und Senat) wird meist in allen Provinzen zu anderen Zeitpunkten gewählt.

Die Anzahl der Abgeordneten der Abgeordnetenkammer wird per Verhältniswahlrecht ermittelt und ist nach einem bestimmten Schlüssel auf die Provinzen verteilt, sie beläuft sich auf etwa einen Abgeordneten pro 152.000 Einwohner. Die Abgeordneten werden für vier Jahre gewählt, allerdings jeweils die Hälfte der Abgeordneten alle zwei Jahre. Die Anzahl der Senatoren beträgt drei je Provinz und drei für die autonome Stadt Buenos Aires. Der Senat wird im Gegensatz zur Abgeordnetenkammer nach einem Sonderfall des Mehrheitswahlrechts gewählt; zwei Senatorensitze erhält die Partei mit den meisten Stimmen, einen Sitz die Partei mit den zweitmeisten Stimmen. Die Senatoren werden für einen Zeitraum von sechs Jahren gewählt, alle zwei Jahre wird ein Drittel der Senatoren gewählt.

Seit der Wirtschaftskrise ist die Debatte um eine politische Reform aufgekommen, da das heutige System vor allem für die Wähler sehr undurchsichtig ist und sowohl Personenkult als auch Korruption begünstigt.

So werden beispielsweise die Wahlen zum Senat und dem Repräsentantenhaus meist gemeinsam mit Bürgermeisterwahlen durchgeführt, was aufgrund der so genannten "Listas Sábanas" zu Verzerrungen führt. Das liegt an der Tatsache, dass in Argentinien keine Kreuze auf Stimmzettel gemacht werden, sondern jede Partei ihren eigenen Stimmzettel "(Lista Sábana)" hat und man seine Stimme durch die richtige Auswahl des Stimmzettels abgibt. Man kann aber bei vielen gleichzeitigen Wahlen die Stimmen aufteilen. In diesem Falle muss man, wenn man Kandidaten verschiedener Parteien wählen möchte, die Stimmzettel auseinander schneiden und nur die entsprechenden Abschnitte in die Urne werfen. Von dieser Möglichkeit machen jedoch nur wenige Wähler Gebrauch, was bei Häufung von Wahlen am selben Tag zu Verzerrungen führt. "Listas Sábanas" (deutsch etwa: Betttuch(große)-Listen) heißen die Stimmzettel, weil sie oft sehr groß sind.

Die jeweiligen Mehrheitsverhältnisse in der Legislative werden ebenfalls kaum publik gemacht, was auch daran liegt, dass die Zusammensetzung sich fast jedes Jahr ändert.

Die Parteienlandschaft Argentiniens ist durch starke Zersplitterung und Unstetigkeit gekennzeichnet. Besonders die zweite Hälfte der 1990er Jahre bis zur Argentinien-Krise markierten eine deutliche Zäsur, nach ihr entstanden zahlreiche neue Gruppierungen, zum Teil aus Abspaltungen der traditionellen Parteien.

Eine der größten Parteien ist heute die aus der peronistischen Bewegung hervorgegangene PJ (Partido Justicialista, auf Deutsch meist: peronistische Partei genannt), die etwa 50 % des Wählerpotenzials auf landesweiter Ebene ballt. Dahinter folgt mit heute weitem Abstand die UCR (Unión Cívica Radical), die zwischen 1945 und 2003 faktisch ein Zweiparteiensystem mit der PJ gebildet hatte und mehrmals an der Regierung beteiligt war.

Die nach der Argentinien-Krise gegründeten Parteien ARI (sozialdemokratisch), Propuesta Republicana (konservativ, meist als PRO bezeichnet) sowie die älteste Linkspartei Partido Socialista sind regional von großer Bedeutung und gehen auf Landesebene vielfache Allianzen ein, die zum Teil auch Teile von PJ und UCR integrieren. Weiterhin gibt es zahlreiche mitgliederstarke Regionalparteien, die in ihren jeweiligen Provinzen dominante Stellungen einnehmen und ebenfalls wechselnd mit den landesweit aktiven Parteien koalieren. Das europäische Rechts-Links-Schema lässt sich in Argentinien daher nicht eindeutig auf bestimmte Parteien anwenden, da viele von ihnen häufig ihre Ausrichtung ändern. Einige Parteien, die in den 1990er Jahren zeitweise Erfolge verbuchen konnten, etwa die liberale "Acción por la República" und die sozialdemokratische "Frente Grande", die zwischen 1999 und 2001 in der Koalition Frente País Solidario an der Regierung beteiligt war, sind heute nur noch von lokaler Bedeutung.

Seit Ende der 1990er Jahre finden die hauptsächlichen Debatten zwischen den Flügeln des PJ statt, die ideologisch sehr verschieden sind. Die Flügel werden meist mit dem Namen ihrer führenden Persönlichkeit bezeichnet. Der momentan herrschende "Kirchnerismo" (ausgehend von Néstor und Cristina Kirchner), der seit 2003 eine eigene politische Allianz namens "Frente para la Victoria" unterhält, ist sozialdemokratisch orientiert, während der in den 1990er Jahren dominierende "Menemismo" wirtschaftsliberal eingestellt war. Ein weiterer Flügel war lange Zeit der in der Provinz Buenos Aires regierende, ursprünglich mit dem Kirchnerismus alliierte "Duhaldismo", wobei nach der Machtergreifung Kirchners durch Differenzen insbesondere im Verhältnis mit Carlos Menem die Allianz der beiden Blöcke zerbrach und der Duhaldismo insgesamt an Bedeutung verlor.

Bei den Parteien mit extremeren Orientierungen haben bei der Linken diverse kommunistische Parteien ("Partido Comunista Revolucionario, Partido Obrero, Izquierda Unida" und "Movimiento Socialista de los Trabajadores") eine gewisse Bedeutung, im Fall der Rechten nur die rechtskonservativ-nationalistische Partido del Campo Popular (aus dem MODIN hervorgegangen), die als Sammelbewegung für Nostalgiker der Militärdiktatur zwischen 1976 und 1983 gilt.

Der argentinische Präsident Mauricio Macri hatte gleich zu Beginn seiner Amtsperiode im Dezember 2015 erklärt, gute Beziehungen zu allen Ländern anstreben zu wollen. Sichtbar setzte er dabei auf die Wiederbelebung der Beziehungen zu Europa und den USA und eine Rückführung Argentiniens auf die Weltbühne. Hierzu zählte auch die schnelle Lösung des Konflikts mit den Hedgefonds in den USA im April 2016, durch die die Rückkehr des Landes auf die internationalen Finanzmärkte erzielt wurde. Priorität genießt für die Macri-Regierung ferner das Verhältnis zu den Ländern der Region, insbesondere zu Brasilien. Die Verfolgung des auf die Falklandinseln/Malwinen erhobenen Souveränitätsanspruchs bleibt von der Verfassung vorgegebenes Ziel argentinischer Außenpolitik, soll allerdings einer Zusammenarbeit mit Großbritannien in anderen Fragen nicht im Wege stehen.

Die Beziehungen zu den Nachbarn in der Region, insbesondere zu Brasilien, Chile und Uruguay sowie Fragen der regionalen Zusammenarbeit - vor allem in Mercosur und UNASUR - gehören zu den klassischen außenpolitischen Prioritäten Argentiniens.

Argentinien ist Mitglied in der Organisation Amerikanischer Staaten (OAS) sowie in der im Dezember 2011 gegründeten Gemeinschaft der Lateinamerikanischen und Karibischen Staaten (CELAC), deren Mitglieder alle 33 amerikanischen Staaten mit Ausnahme der USA und Kanadas sind.

Für das Verhältnis zu den USA hat die argentinische Regierung eine deutliche Belebung angekündigt, die USA haben die ersten wirtschafts- und außenpolitischen Schritte Argentiniens durch erste Gesten honoriert. Der ehemalige US-Präsident Obama besuchte im März 2016 Argentinien, die bilateralen Beziehungen gewannen deutlich an Dynamik. Wie sich das Verhältnis zu den USA unter der neuen Trump-Regierung entwickeln wird, ist noch offen.

Mit Blick auf die angestrebte Handelsdiversifizierung haben sich in den vergangenen Jahren die Beziehungen Argentiniens zu China und Indien sowie zu Russland verstärkt. China ist nach Brasilien inzwischen der zweitwichtigste Handelspartner Argentiniens. Die von der Vorgängerregierung abgeschlossenen Verträge mit China sind von der neuen Regierung inhaltlich überprüft und ggf. angepasst, allerdings nicht gekündigt bzw. revidiert worden. Im Mai 2017 wird Präsident Macri nach China reisen.

Argentinien gehört den G20 an, wird im Anschluss an die deutsche G20-Präsidentschaft für das Jahr 2018 deren Vorsitz übernehmen und ist aktives Mitglied der Vereinten Nationen (Truppensteller im Rahmen der VN-Mission MINUSTAH in Haiti). Es war 2013-2015 im VN-Menschenrechtsrat und 2013/2014 als nichtständiges Mitglied im VN-Sicherheitsrat vertreten.

"Siehe auch:" Mitgliedschaft Argentiniens in internationalen Organisationen

Das argentinische Militär hat in der Geschichte des Landes immer wieder eine dominierende Rolle gespielt. Besonders in der Zeit zwischen 1955 (Putsch gegen Juan Perón) und 1973 (Rückkehr und zweite Präsidentschaft Peróns) und in der Zeit zwischen 1974 (Tod Peróns) und 1983 (Niederlage im Falklandkrieg und Redemokratisierung) war Argentinien vom Militär direkt oder indirekt geprägt. ("Siehe auch:" Geschichte Argentiniens)

Unter den Präsidentschaften Raúl Alfonsíns (1983–1989) und Carlos Menems (1989–1999) wurden die Militärs entscheidend geschwächt und 1994 die Wehrpflicht abgeschafft. 1999 betrugen die Ausgaben für die Verteidigung nur noch 62 % der Ausgaben von 1983; im gleichen Zeitraum sind die Staatsausgaben allgemein auf 152 % der Ausgaben von 1983 angestiegen.

Die argentinischen Streitkräfte, Fuerzas Armadas de la República Argentina, hatten 2004 eine Personalstärke (Soldaten und Verwaltung) von insgesamt etwa 102.300 Personen (Heer: 50.900 Personen (41.400 Soldaten), Marine: 26.600 Personen (17.200 Soldaten), Luftwaffe: 23.600 Personen (13.200 Soldaten), Verteidigungsministerium und Generalstab: 1200 Personen).

Das Heer unterhält 200 Kampfpanzer vom Typ TAM, Transportpanzer vom Typ M113, die Marine drei U-Boote aus deutscher Produktion, fünf Zerstörer, fünf Fregatten und 14 Patrouillenboote, die Luftwaffe 130 Kampfflugzeuge. Der Anteil der Ausgaben für die Verteidigung am Bundeshaushalt beträgt etwa 7 %.

In Argentinien herrscht Schulpflicht von zehn Jahren. Es gibt neben den staatlichen Schulen auch eine hohe Zahl von privaten Schulen. Das Schulsystem ist in drei Stufen eingeteilt: "Inicial" (Vorschule; in der Regel 1 Jahr), „Primaria“ (in der Regel ab 6 Jahren mit zwei Grundstufen: EGB1 und EGB2; insgesamt sechs Schuljahre) und "Secundaria" (Sekundärstufe; drei Jahre EGB 3 bis einschließlich zur 9. Klasse und die anschließende 3-jährige Polimodalstufe).

Laut der Volkszählung des Jahres 2005 sind etwa 2,8 % der Bevölkerung über 15 Jahren Analphabeten, in Deutschland liegt dieser Wert offiziell bei etwa 0,6 %. Dabei sind starke regionale Disparitäten zu beobachten: in Tierra de Fuego im Süden liegt die Rate bei 0,73 %, im Norden des Landes wie etwa in der Provinz Chaco bei 8,96 %.

Von allen Argentiniern, die über 20 Jahre alt sind, haben 88 % die Schule besucht. Etwa 14 % haben die "Primaria" nicht abgeschlossen, circa 29 % haben eine abgeschlossene "Primaria", ungefähr 14 % haben die "Secundaria" nicht abgeschlossen, etwa 16 % haben eine abgeschlossene "Secundaria", circa 5 % einen höheren nicht-universitären Abschluss und etwa 5 % einen Universitätsabschluss. Das heißt etwa 73 % der Bevölkerung haben mindestens die "Primaria" abgeschlossen, circa 30 % mindestens die "Secundaria" und nur etwa 10 % haben einen weiterführenden Abschluss.

1995 wurde das Schulsystem in vielen Provinzen reformiert: Die ersten neun Jahre der Schulzeit werden seitdem als "EGB (Educación General Básica)" bezeichnet, die in mehrere Richtungen aufgeteilte weiterführende Schule stattdessen als 'Polimodal'. Es gibt eine Vielzahl von verschiedenen Schulabschlüssen (naturwissenschaftlich, sozialwissenschaftlich, technisch und wirtschaftlich orientiert), einige sind berufsbefähigende Techniker-Titel. Die Regierung Kirchner hat die Förderung der technischen Schulen von 5 auf 15 Millionen Pesos erhöht und sieht für 2006 eine Erhöhung auf insgesamt 260 Millionen Pesos vor. Die Förderung versucht seit 2003, die erheblichen Schwierigkeiten argentinischer Unternehmen, technisch qualifiziertes Personal zu rekrutieren, zu beheben.

Zum Besuch der Hochschulen berechtigen alle im Rahmen des Polimodal erlangten Abschlüsse, auch wenn der Studiengang nicht mit der Ausrichtung des Polimodals übereinstimmt.

Dieses System wird mit geringen Abweichungen in fast allen argentinischen Provinzen eingeführt; die Bezeichnungen variieren jedoch, so heißt beispielsweise in der Provinz Córdoba der EGB CBU (Ciclo Básico Unitario). 2005/2006 wurde diese Reform in einigen Provinzen, z. B. in Buenos Aires, teilweise überarbeitet und wieder ans alte System angenähert.

Konnte Argentinien in der ersten PISA-Studie 2003 bei einer inoffiziellen nachträglichen Erweiterung der Studie (offiziell nahm es nicht teil) verglichen mit anderen lateinamerikanischen Staaten bei weitem am besten abschneiden, fiel es 2006 bei der ersten offiziellen Teilnahme in nahezu allen Disziplinen hinter Uruguay, Chile und Mexiko, im Leseverständnis auch hinter Brasilien und Kolumbien zurück, wenn auch meist nur mit geringem Punkteabstand. Es existiert ein starkes Gefälle in der Qualität der Schulbildung zwischen Großstädten und ländlichen Regionen einerseits und zwischen Privatschulen und vielen staatlichen Schulen sowie sozialen Klassen und Milieus andererseits. Durch kontinuierliche interne Qualitäts-Tests seit Ende der 1990er Jahre versucht die Politik, dieses Problem in den Griff zu bekommen. Bei diesen Tests kam eine Bandbreite von durchschnittlich 30 % bis 80 % der möglichen Punktzahl heraus, wobei die schlechtesten Ergebnisse von Schulen in ländlichen Gegenden, die besten dagegen in den Privatschulen der Großstädte sowie in den so genannten "Colegios Universitarios" (von Universitäten abhängige Staatsschulen) erzielt wurden. Eine aktuelle Studie zum Erfolg des Gesetzes zur Finanzierung des argentinischen Bildungswesens liegt derzeit von CIPPEC "(Centro de Implementación de Políticas Públicas para la Equidad y el Crecimiento)" vor. CIPPEC ist ein Zusammenschluss verschiedener nationaler gesellschaftlicher Akteure mit Unterstützung der britischen diplomatischen Vertretung, die sich ein kritisches Monitoring der Bildungspolitik und ihrer Erfolge zum Ziel gemacht haben.

Argentinien hat eine Vielzahl von staatlichen und privaten Universitäten. 20 der insgesamt 41 privaten Universitäten haben in der Regierungszeit des neoliberalen Peronisten Menem ihre Pforten geöffnet. Das 1958 in Kraft getretene Gesetz zur Finanzierung der privaten Universitäten sieht ein Verbot finanzieller Unterstützung vor, erlaubt aber seit den 1990er Jahren unter Menem eine gezielte Förderung einzelner Forschungsprojekte.
In der politikkritischen Zeitschrift „Caras y Caretas“ erschien im Mai 2006 ein Artikel, der vor der wachsenden Nähe einiger privater Bildungseinrichtungen zu orthodoxen religiösen Institutionen warnt, wie z. B. der Universidad Austral zum Opus Dei.

Die älteste Universität ist die Universität von Córdoba, die 1613 gegründet wurde und heute die zweitgrößte des Landes ist (ca. 120.000 Studenten). Die größte Universität ist dagegen die Universität von Buenos Aires (UBA), die 1821 gegründet wurde und etwa 400.000 Studenten hat.

Das Bibliothekswesen in Argentinien ist vielgestaltig. So entstanden gegen Ende des 19. Jahrhunderts die ersten privat finanzierten "Bibliotecas populares" (Volksbibliotheken). Sie werden heute von der "Comisión Nacional Protectora de Bibliotecas Populares" gefördert. Diese organisiert auch Weiterbildungsveranstaltungen für das Bibliothekspersonal. Seit 1977 gibt es die "Confederación Argentina de Bibliotecas Populares". Ihre Mitglieder sind zumeist keine Bibliothekare, sondern Politiker. Daneben existieren 19 "Federaciones Provinciales".

Seit 1927 entstanden die "bibliotecas públicas municipales" (Öffentliche Stadtbibliotheken), die heute fast ausschließlich in Buenos Aires existieren. Seit 1944 untersteht diese der "Secretaría de Cultura de la Municipalidad de la Ciudad de Buenos Aires". Derzeit existieren in Buenos Aires 23 Stadtbibliotheken und 3 Bücherbusse, deren größte Benutzergruppe Schüler sind.

Die 1963 gegründete "Junta de Bibliotecas Universitarias Argentinas" (JUBIUA) vertritt die Interessen der staatlichen Universitätsbibliotheken gegenüber der Regierung und erarbeitet gemeinsame Zielvorgaben. Die privaten Universitätsbibliotheken verfügen nicht über eine institutionalisierte Zusammenarbeit.

Von den Schulen verfügen nur wenige über eigene Bibliotheken, die durch Buch- und Sachspenden sowie ehrenamtliche Tätigkeit der Eltern der Schüler finanziert werden. Derzeit wird ein Konzept zum Aufbau eines nationalen Schulbibliothekssystems erarbeitet.

Die "Biblioteca Nacional" (Nationalbibliothek der Republik Argentinien) wurde 1810 unter dem Namen "Biblioteca pública de Buenos Aires" gegründet. Seit 1884 ist sie die Nationalbibliothek. 1933 erhielt sie das Pflichtexemplarrecht. Ihr Buchbestand wird auf 800.000 bis 2,5 Millionen Bände geschätzt. Die "Biblioteca del Congreso de la Nación" (Parlamentsbibliothek) entstand 1859. Die Bibliothek ist Depotbibliothek internationaler Organisationen und besitzt schätzungsweise 1,5 Millionen Bestandseinheiten.

Die Provinzen (spanisch "provincias", Einzahl: "provincia") sind die Gliedstaaten des argentinischen Bundesstaates. Sie haben jeweils eine eigene Provinzverfassung, eine Provinzregierung unter Leitung eines direkt gewählten Gouverneurs "(gobernador)" und ein Parlament. Die Provinzen sind wiederum administrativ in "Departamentos" untergliedert. Ausnahme ist hier die Provinz Buenos Aires, die in "Partidos" untergliedert ist.

Es gibt 23 Provinzen und die autonome Stadt Buenos Aires, siehe Liste der Provinzen Argentiniens.

Ab Ende der 1980er-Jahre haben sich die Provinzen Argentiniens mit Ausnahme der Provinz Buenos Aires zu Regionen zusammengeschlossen, mit dem Ziel, die Wirtschafts-, Infrastruktur- und Entwicklungspolitik untereinander abzustimmen und Gegengewichte zur dominierenden Stellung des Großraums Buenos Aires zu bilden. Diese Regionen sind allerdings bisher keine offiziellen Gliedstaaten, sondern reine Interessengemeinschaften, sie haben also keinerlei offizielle politische Organe. Der Grad der Kooperation ist unterschiedlich.


Buenos Aires, dessen Ballungsraum 2017 etwa 14,9 Millionen Einwohner umfasst, ist politische Hauptstadt und wirtschaftliches Zentrum Argentiniens. Es ist umgeben von einer Reihe von selbstständigen Vorstädten, die zum Teil reine Schlafstädte sind, zum Teil aber auch selbst über Produktionsstätten verfügen. Córdoba, mit 1,6 Mio. Einwohnern die zweitgrößte Stadt des Landes, verfügt über größere Produktionsstätten und beherbergt die älteste Universität des Landes Universidad Nacional de Córdoba. Rosario in der Provinz Santa Fe (1,3 Mio. Einwohner) ist der zweitgrößte Hafen des Landes und ein Industrie- und Handelszentrum. Mendoza (1 Mio. Einwohner) ist vor allem für seinen Wein- und Obstanbau bekannt, dient aber auch als Brückenkopf für den Handel mit Santiago de Chile. San Miguel de Tucumán (883.000 Einwohner) ist die Geburtsstätte der Unabhängigkeit und wurde durch die intensive Landwirtschaft, insbesondere den Zuckerrohranbau, wirtschaftlich und kulturell bedeutsam, litt aber in den letzten Jahrzehnten unter der Krise in diesem Wirtschaftssektor und ist heute eine der Städte mit der größten Armutsrate des Landes. Die Universitäten in dieser Stadt haben allerdings überregionale Bedeutung und werden z. B. von Studenten aus Bolivien besucht.

"Siehe auch:" Liste der Städte in Argentinien

Das Eisenbahnsystem in Argentinien hat am 29. August 1857 mit der ersten Fahrt eines Zuges seinen Anfang genommen. Im Laufe der Zeit wurde das Schienennetz hauptsächlich von englischen Unternehmen relativ zügig ausgebaut und wurde zu einem Schlüssel für die Entwicklung des Landes. In den 1930er Jahren verfügte das Land mit 43.000 Kilometer Schiene über ein größeres Netz als die meisten Länder Europas. Das Eisenbahnsystem bestand aus mehreren unabhängigen privaten Unternehmen, die 1946 von Präsident Perón verstaatlicht wurden. Die Ende der 1950er Jahre hinzugezogenen US-amerikanischen Berater legten die Priorität auf den Straßenverkehr, so dass Bahnstrecken in großem Umfang stillgelegt wurden. Die Staatsbahn wurde 1992 von Carlos Menem wieder privatisiert, was zur Folge hatte, dass der Fahrgastbetrieb noch mehr reduziert wurde, die Eisenbahnergewerkschaft zerschlagen wurde, 50.000 Menschen arbeitslos wurden, ganze Landstriche verödeten und die Korruption im Eisenbahngeschäft stark zunahm. Heute hat das argentinische Schienennetz eine Länge von etwa 28.300 Kilometern in drei verschiedenen Spurweiten. Zwei Eisenbahnstrecken verbinden Argentinien mit Chile, weitere Strecken haben Verbindung mit Bolivien, Paraguay, Uruguay und Brasilien. Allerdings werden immer noch Strecken stillgelegt oder verfallen und werden nicht wieder instand gesetzt. Der Personentransport per Eisenbahn spielt generell nur noch im Großraum Buenos Aires für die Pendler eine Rolle. Bahnfernverbindungen gibt es noch bzw. wieder von Buenos Aires nach Córdoba, Mar del Plata, San Miguel de Tucumán, Santa Fe und nach Posadas. Die Züge benötigen für die gleiche Strecke jedoch wesentlich länger als Fernreisebusse und haben einen sehr eingeschränkten Fahrplan (z. B. Buenos Aires, Bahnhof Retiro – Córdoba zwei Fahrten pro Woche). Zwar hat die Regierung unter Néstor Kirchner im Jahre 2006 einen „Megaplan Eisenbahn“ aufgestellt, worin auch eine 710 km lange, mit bis zu 320 km/h betriebene Hochgeschwindigkeitsstrecke Cobra zwischen Buenos Aires und Córdoba für 2011 geplant wurde. Jedoch sieht die Realität so aus, dass die infolge dieses Planes wieder reaktivierten Strecken aufgrund technischer Defizite der Schienen oder des rollenden Materials oft gleich wieder stillgelegt werden mussten. Touristisch gesehen gibt es einige interessante Züge, z. B. den "Tren a las Nubes" in der Provinz Salta, "La Trochita" – die einzige dampfbetriebene Schmalspurbahn Argentiniens, die zwischen Esquel und Nahuel Pan verkehrt – sowie den Tren del Fin del Mundo in der Provinz Tierra del Fuego.

Die Rolle der Eisenbahn für den Personentransport wurde weitestgehend von modernen, klimatisierten Reisebussen übernommen. Es kann praktisch jeder Punkt des Landes mit dem Reisebus erreicht werden, und so sind die Busbahnhöfe heute neben den Flughäfen die meistgenutzten Infrastruktureinrichtungen. Der bedeutendste Busbahnhof Argentiniens ist Retiro in Buenos Aires. Von dort gibt es Busverbindungen in das ganze Land. Weitere stark frequentierte Busbahnhöfe und Drehkreuze finden sich in Córdoba (etwa 10 Stunden Reisezeit von Buenos Aires) und Mendoza (etwa 14–15 Stunden Reisezeit von Buenos Aires). Die längste Direktverbindung besteht zwischen San Salvador de Jujuy und Río Gallegos (3430 km, fahrplanmäßig 55 Stunden Fahrzeit), von wo aus man weiter nach Ushuaia fahren kann.

Das Straßennetz hat eine Gesamtlänge von etwa 215.000 km und verteilt sich auf National-, Provinz- und Gemeindestraßen. Die Qualität der Straßen variiert stark. Die großen Wirtschaftszentren sind mit asphaltierten und zum Teil gut ausgebauten Straßen verbunden, die meist über Mautgebühren von privaten Unternehmen gebaut und instand gehalten werden. In den Ballungszentren und auf einigen Hauptverbindungen existieren einige mehrspurige Autobahnen ("autopistas") und Schnellstraßen ("autovías"), die meist als reguläre National- und Provinzstraßen ausgeschildert sind. Die meisten Fernstraßen sind jedoch zweispurig und durch den Schwerlastverkehr oft stark belastet. In abgelegenen Gebieten sind häufig nur Schotter- und Erdpisten vorhanden. Da die Eisenbahn im Personenverkehr keine Rolle mehr spielt und dieser fast ausschließlich über die Straße abgewickelt wird, gibt es pro Jahr fast 10.000 Verkehrstote, was hochgerechnet auf die Einwohnerzahl eine höhere Zahl als in Indien ist.

Bekannteste touristische Strecke ist die Ruta Nacional 40 zwischen "Cabo Vírgenes" an der Südspitze des Festlandes (Provinz Santa Cruz) und La Quiaca, die das gesamte Land von Nord nach Süd durchquert.

Die nationale Fluggesellschaft Aerolíneas Argentinas wurde 1990 privatisiert und 2008 wieder verstaatlicht. Im Inlandsverkehr hat Aerolíneas gemeinsam mit der Tochtergesellschaft Austral Líneas Aéreas einen hohen Marktanteil; seit 2005 tritt mit der LAN Argentina ein größerer Konkurrent auf. Die zu den argentinischen Luftstreitkräften gehörende Líneas Aéreas del Estado (LADE) verbindet kleinere Städte in Patagonien.

Aufgrund der großen Entfernungen verfügt fast jede größere Stadt in Argentinien über einen Flughafen. Die Hauptstadt Buenos Aires selbst besitzt zwei Passagier-Flughäfen: Internationale Flüge, insbesondere alle Langstreckenverbindungen, werden überwiegend am Flughafen Ezeiza (EZE) abgewickelt. Darüber hinaus gibt es den Stadtflughafen Aeroparque Jorge Newbery (AEP), der überwiegend für Inlandsflüge, aber auch für kürzere internationale Strecken, genutzt wird.
Ungefähr 3100 km der Wasserwege sind schiffbar. Der Río de la Plata mit seinen Oberläufen Río Paraná und Río Uruguay ist der wichtigste Wasserweg. Über diese Flüsse wird auch ein Großteil der landwirtschaftlichen Exporte Argentiniens transportiert, die meist in der Region um Rosario auf hochseefähige Schüttgutfrachter verladen werden.

Argentinien erzeugt einen Großteil seiner Energie mithilfe von erneuerbaren Energien. Die Wasserkraft hat einen Anteil von 41 % der Stromerzeugung; weitere 7 % werden von Kernkraftwerken und 52 % von Wärmekraftwerken geliefert. Daneben besitzt Argentinien große Vorkommen an Erdgas. Diese Energieform wird zum Kochen und Heizen, aber auch vermehrt als Kraftstoff für Pkw eingesetzt. Mehr und mehr spielt der Import von Erdgas, z. B. aus Bolivien, eine größere Rolle; dies aufgrund der Misswirtschaft der ansässigen Energiekonzerne, der Verknappung der Erdgasreserven und des schnell steigenden Energieverbrauchs. 

Auf der anderen Seite wird der Energiemix seit 1994 durch die Windenergie ergänzt, die in der Provinz Chubut in Patagonien mit ihrem besonders windigen Klima bereits einen erheblichen Teil der Stromerzeugung übernimmt. Sie wird seit der zweiten Hälfte der 1990er Jahre gesetzlich gefördert und die Branche weist daher derzeit ein hohes Wachstum auf. Argentinien gilt als eines der Länder mit dem höchsten Windkraftpotenzial der Erde, lag jedoch in der Nutzung dieser Energieform 2006 nur auf dem dritten Platz in Lateinamerika hinter Mexiko und Brasilien. November 2015 kündigt China an, in Patagonien einen Windpark mit 200 MW Leistung zu errichten; bisher sind 187 MW Windkraft in Argentinien installiert, was 0,6 % Anteil an der Stromerzeugung entspricht. Die argentinische Regierung strebt an, bis 2025 6-7 GW Windenergieleistung zu installieren. Gerade in Padagonien herrschen exzellente Windbedingungen mit niedrigen Turbulenzraten und Windgeschwindigkeiten von bis zu 12 m/s im Jahresschnitt.

An der Nutzung der Kernenergie will das Land festhalten. Bereits seit 1974 ist ein deutscher Importreaktor am Standort Atucha im Betrieb. Der zweite argentinische Reaktor am Standort Embalse, in Betrieb seit 1983, war ein Import aus Kanada. Seit 1981 befindet sich ein weiterer aus Deutschland importierter Reaktor im Bau und soll 2013 als dritter argentinischer Kernreaktor den Betrieb aufnehmen. 

Der Bau eines vierten Reaktors wird seit 2006 in Erwägung gezogen, möglicherweise am Standort Embalse. Mögliche Bewerber für die Ausschreibung kommen aus Kanada, Frankreich, Russland, Japan, Südkorea, China und den USA. Eine entsprechende Anfrage erfolgte durch die argentinische Regierung. 2010 wurden Kooperationsverträge mit Russland und Südkorea unterzeichnet, wobei im Mai 2011 erstmals die Möglichkeit erwähnt wurde, zusammen mit der russischen Gesellschaft Rosatom einen Reaktor zu errichten. Am 12. Juli 2014 unterzeichnete Staatspräsidentin Cristina Kirchner und Wladimir Putin eine Vereinbarung über die russische Beteiligung am Projekt "Atucha 3". Bereits 2012 hatte Argentinien mit China eine Kooperation vereinbart zu Finanzierung und Bau des Projekts "Atucha 4".

Die staatliche Telekommunikationsgesellschaft ENTEL wurde 1990 privatisiert und an zwei ausländische Unternehmen – "Telefónica" (Spanien) und "Telecom" (Frankreich, heute in der Hand von Telecom Italia) – verkauft, die sich das Land aufteilten. Seitdem hat die Zahl der Telefonanschlüsse je Einwohner rasant zugenommen, denn nach der Privatisierung betrug die Einrichtungsgebühr für einen Telefonanschluss mit 100 US-$ nur noch ein Zehntel der früheren Gebühr, und auch die Wartezeit bis zum Anschluss hatte sich wesentlich verringert. Heute gibt es etwa 7,9 Millionen Festnetzanschlüsse (etwa 20 Anschlüsse je 100 Einwohner). Darüber hinaus gibt es etwa 10,2 Millionen Mobiltelefonanschlüsse. Im Jahre 2003 überstieg die Anzahl der Mobiltelefonverträge erstmals die Anzahl der Festnetzanschlüsse. Die Netzqualität und -abdeckung ist aber von Betreiber zu Betreiber sehr unterschiedlich. 2016 hatten 69,2 % der Bevölkerung Zugang zum Internet.

Das Internet wird nach wie vor von vielen Privatleuten nur in sogenannten "Telecentros" (Telekommunikationsläden) oder "Locutorios" (Call Shops) und in Internetcafés ("Ciber" genannt) genutzt. Wohlhabendere Privatleute und Unternehmen nutzen in der Regel Internet via DSL oder Kabel.

Auch der Postdienst wurde privatisiert. Die Bedingungen der Konzession wurde aber nicht eingehalten, und so versucht man den Postdienst neu zu vergeben. Neben dem größten Unternehmen Correo Argentino, das aus dem staatlichen Postdienst hervorging, gibt es noch mehrere kleinere Postdienste, z. B. OCA und Andreani.

Argentinien ist eine gelenkte Volkswirtschaft, die in mehreren Stufen seit den 1970er Jahren zunehmend dereguliert und privatisiert wurde. Unter Präsident Néstor Kirchner jedoch wurde diese Tendenz umgekehrt.

Argentinien ist mit einem Bruttoinlandsprodukt (BIP) von rund 604 Milliarden US-Dollar (2015) die größte Volkswirtschaft des spanischsprachigen Südamerikas. In Lateinamerika sind lediglich Brasilien und Mexiko wirtschaftlich bedeutender. Argentinien verfügt über eine im Regionalvergleich relativ gut entwickelte Industrie; wichtigste Sektoren sind die Nahrungsmittelindustrie und die Automobilindustrie (u.a. Volkswagen und Daimler), die wesentliche Anteile der Produktion nach Brasilien exportiert.

Die verarbeitende Industrie, Immobilien/Unternehmensdienstleistungen sowie der Handel tragen jeweils rund 10 % zum BIP bei. Der Beitrag der reinen Land- und Forstwirtschaft zum BIP liegt bei knapp 5 %; allerdings wird geschätzt, dass rund 1/3 der Arbeitsplätze direkt oder indirekt (zum Beispiel Transport, Verpackung) im Zusammenhang mit der Agrarindustrie stehen. Auch bei den Exporten dominiert der Anteil der Nahrungsmittel (rund 45 %) deutlich vor Auto(teile)-Exporten (um 10 %).

Im Jahr 2014 befand sich die argentinische Wirtschaft trotz guter Rahmenbedingungen für Rohstoffexporte auf einer Talfahrt mit massiver Abwertung des Peso. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Argentinien Platz 92 von 137 Ländern (Stand 2017-18). Im Index der Wirtschaftlichen Freiheit liegt das Land auf Platz 156 von 180 Ländern (2017). 

Nach dem Regierungswechsel musste die Regierung 2016 enorm sparen. Regierung, Weltbank und Währungsfonds sahen für 2017 eine deutlich niedrigere Inflation von 18 Prozent voraus.

Wertvolle Mineralerze und Gesteine finden sich in Argentinien nur in kleineren Mengen, so etwa Gold, Silber, Kupfer, Blei, Zink, Eisen, Zinn, Glimmer und Kalkstein. Wirtschaftlich bedeutender sind die Erdöl- und Erdgas-Vorkommen im Nordwesten, Neuquén, der Gegend rund um die Bucht Golfo San Jorge und vor der Küste.

Die argentinische Wirtschaft ist traditionell durch die Landwirtschaft geprägt. Bis in die 1950er Jahre wurden fast ausschließlich Agrargüter exportiert. Erst danach setzte eine Industrialisierung nennenswerten Umfanges ein. Die wirtschaftliche Entwicklung wurde jedoch von den verschiedenen Regierungen nach unterschiedlichen, teilweise widersprüchlichen Vorgaben reglementiert. Es entstand, vor allem unter dem Einfluss des Peronismus, ein breiter staatlich kontrollierter Sektor in Industrie, Handel und Dienstleistung. Dennoch hat Argentinien das Wohlstandsniveau der 1950er Jahre nie wieder erreicht. Korruption war und ist ein diesen Sektor durchziehendes Übel.

Die 1976 unter der Politik der Militärdiktatur eingeleitete massive Staatsverschuldung fügte der heimischen Wirtschaft schweren Schaden zu. Die Auslandsverschuldung stieg von unter 8 Mrd. US-Dollar im Jahr 1967 auf 160 Mrd. US-Dollar im Jahr 2001. Der Peso Ley musste mehrfach abgewertet werden. Der Falklandkrieg geht möglicherweise auch auf die wirtschaftlichen Probleme unter der Militärdiktatur zurück.

Nach der Rückkehr zur Demokratie 1983 erwies sich die Hyperinflation als eines der größten wirtschaftlichen Probleme des Landes. Der 1989 gewählte Präsident Carlos Menem führte daraufhin die 1:1-Bindung des argentinischen Peso an den US-Dollar ein. Dies führte fast schlagartig zu einem Ende der Inflation und zu einem deutlichen wirtschaftlichen Aufschwung. Auf längere Sicht hatte sie aber zur Folge, dass argentinische Produkte auf dem Weltmarkt teurer und Importware im Inland billiger wurden. Zahlreiche argentinische Produktionsbetriebe mussten schließen. Es kam zu einem schnell zunehmenden Ungleichgewicht zwischen dem (offiziellen) Wechselkurs der Währung und ihrer inneren Werthaltigkeit. Kapitalflucht setzte ein, und das ohnehin hoch verschuldete Land musste immer neue Kredite im Ausland aufnehmen, um alte Verbindlichkeiten bezahlen und Devisen für dringende Importe bereitstellen zu können. Gelegentlich wurden sogar Staatsbedienstete nicht mehr mit Geld, sondern mit Schuldverschreibungen bezahlt, und Geschäftsleute wurden gesetzlich verpflichtet, derartige Papiere als Zahlungsmittel anzunehmen. Anfangs wurde dies noch durch private Kapitalzuflüsse ausländischer Anleger überlagert, die sich in argentinische Unternehmen einkauften, besonders im Zuge der von Menem eingeleiteten Privatisierung von Staatsbetrieben. Doch schließlich hatte die Verschuldung so weit zugenommen und die Wirtschaftsleistung so weit abgenommen, dass Ende 2001 nach schweren Unruhen Präsident Fernando de la Rúa zurücktrat. Auslöser für die Unruhen war der sogenannte "Corralito", also das Einfrieren sämtlicher Bankguthaben.

Die folgende Regierung gab die Einstellung der Zahlungen auf Tilgung und Zinsen, also den Staatsbankrott, bekannt. Wegen fehlender Unterstützung der Partei trat der übergangsweise angetretene Präsident Adolfo Rodríguez Saá schon nach fünf Tagen wieder zurück. Es folgte der Peronist Eduardo Duhalde, der im Januar 2002 den argentinischen Peso zunächst auf 1,40 ARS/US-Dollar abwertete, um ihn dann wenig später ganz freizugeben.

Der IWF versorgte nach einer langen Verhandlung Mitte 2002, mit politischer Unterstützung der größten Industrienationen, Argentinien im Rahmen verschiedener Interimsabkommen mit frischem Geld. Damit konnte die argentinische Wirtschaft bereits im Jahr 2003 ein beachtliches Wachstum verzeichnen, vor allem weil nun Mittelabflüsse durch Kreditrückzahlungen nicht mehr stattfanden und wegen des nun deutlich billigeren Peso (3,5 bis 4 Argentinische Peso je US-Dollar). Allerdings wurde im März 2004 die Rückzahlung einer Rate von 3,1 Mrd. US-Dollar (etwa 2,5 Mrd. Euro) für einen im Rahmen der Interimsabkommen gewährten IWF-Kredite fällig. Erst unmittelbar vor dem letztmöglichen Termin wurde die Zahlung angewiesen. Vorausgegangen war ein mehrwöchiger Verhandlungspoker. Die argentinische Regierung wollte dabei erreichen, dass ein Bericht des IWF über die Bemühungen des Landes im Hinblick auf die Wiedergewinnung wirtschaftlicher Solidität möglichst positiv ausfiel. Dies galt als Voraussetzung für eine weitere Kreditgewährung durch den IWF. Über die Behandlung der Forderungen von privaten Gläubigern Argentiniens wurde bislang aber noch keine Einigung erzielt. Dies belastet weiterhin die Handelsbeziehungen des Landes.

Im IWF war lange umstritten, ob Argentinien die Voraussetzungen für die weitere Vergabe von Krediten erfüllt. Die Auflage, in „gutem Glauben“ zu verhandeln, hat die argentinische Regierung nach Ansicht der privaten Gläubiger nicht erfüllt. Stattdessen forderte Argentinien in den Verhandlungen zwischen 2002 und 2004 einen Kapitalschnitt, der auf 75 % Barwertverlust hinausläuft. Es liefen Klagen gegen Argentinien und den IWF vor dem Bundesverfassungsgericht mit dem Ziel der vollständigen Rückzahlung des geliehenen Geldes, die teilweise noch nicht abgeschlossen sind. Eine deutsche Gläubigerorganisation ist die "Interessengemeinschaft Argentinien e. V."

Anfang 2005 nahm die Regierung Verhandlungen mit den Inhabern argentinischer Staatspapiere zur Annahme eines Umschuldungsplanes auf. Dieser Plan umfasste neben einem erheblichen Kapitalschnitt die zeitliche Streckung der Verbindlichkeiten sowie eine Reduzierung des Zinses. Dabei wurde ausschließlich mit privaten Gläubigern und ihren Interessenvertretungen verhandelt. Hierbei war bislang bei inländischen Gläubigern eine deutliche Bereitschaft erkennbar, das Umschuldungsangebot zu akzeptieren. Bei ausländischen Gläubigern stießen die Vorschläge jedoch zunächst auf harten Widerstand.

Der Umschuldungsplan wurde von etwas mehr als 76 % der privaten Gläubiger innerhalb der gesetzten Frist akzeptiert. Eine kurzzeitige Streitigkeit mit einem Hedgefonds um 7 Milliarden Dollar verzögerte die Ausgabe der neuen Bonds allerdings um zwei Monate bis Ende Mai 2005.

"Siehe auch:" Argentinien-Krise

Die Tabelle des Wirtschaftswachstums Argentiniens zeigt den tiefen Einschnitt bei der argentinischen Wirtschaftskrise 2001/2002, der zeitlich nach der mexikanischen Tequila-, der Asien- und der Brasilienkrise stattfand.

Das Bruttoinlandsprodukt (BIP) betrug im Jahr 2003 376,2 Milliarden Arg$, dies entsprach etwa 103 Milliarden Euro. Davon entfielen etwa 43 % auf die Produktion von Waren und etwa 51 % auf die Erbringung von Dienstleistungen. Den größten Anteil am BIP hatten dabei die produzierende Industrie mit 22 %, die Landwirtschaft mit 10 %, der Groß- und Einzelhandel mit 11 % sowie die Vermietung von Gebäuden und Grundstücken mit ebenfalls 11 %.

Während der 1990er Jahre galt Argentinien als ein positives Beispiel für finanzielle Stabilität und erfolgreiche Marktreformen, doch stieg unter der Regierung Menem die Staatsverschuldung kontinuierlich an. Dies war eine der Ursachen für die Argentinien-Krise und den Staatsbankrott im Jahr 2001. Die Staatsanleihen wurden nicht mehr bedient. Die Gläubiger, die sich einem Umtausch unterwarfen, verloren ca. 70 % ihrer Anlage, darunter viele private Kleinanleger vor allem in Italien, Japan und Deutschland. Allein in Deutschland wurden mehrere hundert Urteile erstritten, welche die Republik Argentinien zur Zahlung der ausstehenden Schulden verpflichtet.
Am 31. Juli 2014 wurde bekannt, dass Argentinien zum zweiten Mal seit 2001 zahlungsunfähig sei.

Seit 1985 gehört Argentinien ununterbrochen zu den Top-5-Kreditnehmern des Internationalen Währungsfonds.

Argentinien war in den 1980er Jahren bekannt als ein Land mit einer sehr hohen Inflationsrate. Diese verstärkte sich ab Beginn der Redemokratisierung 1983 zunehmend zu einer Hyperinflation, deren Höhepunkt 1989 erreicht wurde. Im selben Jahr wurde unter der Regierung von Carlos Menem und seinem Wirtschaftsminister Domingo Cavallo die 1:1-Bindung des argentinischen Peso an den US-Dollar beschlossen. Diese Maßnahme konnte die Inflationsrate in der Folge relativ rasch auf „normale“ Werte drücken. Im Zeitraum zwischen 1994 und 1998 gab es keine nennenswerte Inflationsrate. Ab 1999 drehte die beginnende Wirtschaftskrise die Inflationsrate sogar in den deflationären Bereich. Mit der Argentinien-Krise, die um den Jahreswechsel 2001/2002 ihren Höhepunkt erreichte und mit der Erklärung des Default und einer Abwertung gegenüber dem Dollar verbunden war, stieg die Inflationsrate zunächst stark an, sank aber zwischenzeitlich wieder auf einstellige Werte. Seit das argentinische Statistikamt INDEC Anfang 2007 unter Regierungsaufsicht gestellt und die statistischen Berechnungsgrundlagen verändert wurden, wird die offizielle Inflationsrate von privaten Wirtschaftsinstituten und internationalen Organisationen in Zweifel gezogen. Deren Schätzungen für 2011 liegen bei ca. knapp 23 % (2010: ca. 25 %). Für 2012 wird mit ähnlichen Werten gerechnet. Die hohe Inflation schlägt sich in den letzten Jahren in den Abschlüssen der Tarifrunden nieder, bei denen die mächtigen Gewerkschaften meist Erhöhungen noch deutlich oberhalb der realen Inflationsraten erzielen konnten.

Der Außenhandel war in den vergangenen Jahren stark von der Argentinien-Krise geprägt. Die Importe gingen seit 1999 zurück. Im Jahresvergleich 2001/2002 hatten sie einen besonders starken Rückgang von 56 % und konnte sich erst 2003 wieder erholen. Die Exporte blieben von der Argentinien-Krise nahezu unberührt.

Die Exporte sind von landwirtschaftlichen Produkten dominiert. 31 % aller Exporte sind weiterverarbeitete, landwirtschaftliche Produkte, 25 % sind Rohstoffe (wobei hierzu auch landwirtschaftliche Produkte zählen), 25 % sind industrielle Produkte und 18 % sind Mineralöle und andere Energieträger.

Nach Handelsblöcken unterteilt gingen 2015 24 % aller argentinischen Exporte in den MERCOSUR, 23 % an ASEAN und China, Südkorea, Japan, Indien,  15 % an die EU und 10 % an NAFTA. Unter den einzelnen Abnehmerländern liegt Brasilien mit 17,8 % an erster Stelle, gefolgt von China mit 9,5 % und den USA und Chile mit 6,0 % bzw. 4,2 %. Bei den argentinischen Importen dominierten 2015 die Handelsblöcke ASEAN und China, Südkorea, Japan, Indien mit 28 %, gefolgt von Mercosur mit 23 % und der EU und NAFTA mit jeweils 17 %. Als Hauptlieferländer dominieren Brasilien mit 21,8 % und China mit 19,7 %, gefolgt von den USA mit 12,9 % und Deutschland mit 5,2 %.

Nach dem Korruptionswahrnehmungsindex von Transparency International lag Argentinien 2017 von 180 Ländern zusammen mit Kosovo, Benin und Swasiland auf dem 85. Platz, mit 39 von maximal 100 Punkten.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 141,7 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 115,9 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 4,7 % des BIP. Die Staatsverschuldung betrug 2016 279,6 Mrd. US-Dollar oder 51,3 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Ein scherzhafter Ausspruch von Jorge Luis Borges bezeichnet die Argentinier als „Italiener, die Spanisch sprechen und gerne Engländer wären, die glauben, in Paris zu leben.“ Dadurch kommt die Mischung des Volkes aus Einwanderern verschiedener europäischer Länder zum Ausdruck, der sich in der Kultur deutlich bemerkbar macht. Argentinien hat eine sehr aktive, multikulturelle und stark durch europäische Einflüsse geprägte Kulturszene. Vor allem in Buenos Aires gibt es ein vielfältiges Angebot an Veranstaltungen in den Bereichen Theater, Musik, Oper, Literatur, Film und Sport.

Argentinische Musik ist durch den Tango (und die verwandten Musikformen Milonga und Vals) bekannt geworden. Bekannteste Interpreten sind Carlos Gardel, Astor Piazzolla und Osvaldo Pugliese. Tango kann jedoch nicht auf die musikalische Dimension beschränkt werden, vielmehr ist Tango ein gesamtkulturelles Phänomen mit den zusätzlichen Aspekten Textdichtung und tänzerische Interpretation. Als solches begründet der Tango eine kulturelle Identität, die sehr viel zum Selbstverständnis der Argentinier, genauer genommen der „Porteños“ aus Buenos Aires, beiträgt.

Außerdem gibt es in Argentinien die in der traditionellen Musik verwurzelten Folkore-Interpreten. Zu den auch international beachteten Musikern zählen der als Atahualpa Yupanqui weltweit bekannt gewordene Héctor Roberto Chavero und die aus der Provinz Tucumán stammende Mercedes Sosa (1935–2009), die 1982 nach vier Jahren Exil in Madrid und Paris nach Argentinien zurückkehrte.

Neuerdings sind in Argentinien einige traditionelle Musikstile von der Popmusik her wiederbelebt worden. Zu nennen sind hier der fröhlich-leichte Tanz des Cuarteto, die urbane Musik der Stadt Córdoba, sowie einige Stile der von den Spaniern übernommenen nationalen Folklore, die durch Mischung mit anderen Stilen eine völlig neue Gestalt erlangt haben. Auch Musikstile aus anderen Teilen Südamerikas, allen voran die kolumbianische Cumbia, wurden von argentinischen Interpreten weiterentwickelt. So entstand als aktueller Beitrag Argentiniens zur Popmusik in Buenos Aires die Cumbia Villera („Elendsviertel-Cumbia“).

Im 19. Jahrhundert löst sich mit der Unabhängigkeit des Landes die argentinische Literatur von der spanischen – ohne dieses Erbe zu verleugnen. Durch die Thematisierung des Lebens der Gauchos in der Pampa gewinnt die Literatur eine deutliche nationale Komponente. Beispiele dafür sind "Fausto" (1866) von Estanislao del Campo, das in Gedichtform die Geschichte eines Gauchos erzählende und oft als argentinisches Nationalepos bezeichnete "El gaucho Martín Fierro" (1872) von José Hernández sowie das bereits 1845 entstandene "Facundo" von Domingo Faustino Sarmiento. In ähnlicher Traditionslinie steht auch die 1926 veröffentlichte Erzählung "Don Segundo Sombra" von Ricardo Güiraldes (deutsch bereits 1934: "Das Buch vom Gaucho Sombra").

Bekannte moderne Autoren sind Eduardo Mallea, Ernesto Sabato, Humberto Costantini, Julio Cortázar, Adolfo Bioy Casares, Manuel Puig, Victoria Ocampo, María Elena Walsh, Tomás Eloy Martínez, Roberto Arlt und besonders Jorge Luis Borges.

Bekannte Comic- und Cartoonautoren sind Guillermo Mordillo und Quino, der unter anderem Preisträger des Max-und-Moritz-Preises ist und die Reihe "Mafalda" schuf.

In vielen Städten gibt es eine lebhafte Theaterszene. Man könnte pro Woche leicht über 100 verschiedene Theaterstücke von professionellen und Laiengruppen ansehen. Besonders bekannt ist Rosario für seine Theatergruppen. Aktuell feiert die Akrobatik-Theatergruppe "De la Guarda" Erfolge in der ganzen Welt. Das bekannteste Theatergebäude Argentiniens ist das Opernhaus Teatro Colón in Buenos Aires.

Die argentinische Malerei gehört mit zu den führenden in Südamerika. Stilistisch ist die Malerei, im Gegensatz zu anderen lateinamerikanischen Ländern, weniger von indigenen Einflüssen bestimmt, sondern von der klassischen Moderne Europas. Herausragende traditionelle Maler Argentiniens sind Enrique de Larrañaga, Didimo Nardino und Horacio Politi.
Eine neue Generation von Malern wird zunehmend von Einflüssen der Populärkultur wie Graffiti und New Pop Art bestimmt.

Argentinien war eines der Pionierländer auf dem Gebiet des Stummfilms. Schon 1896 wurde der erste Film gedreht, der die argentinische Fahne zum Thema hatte. 1933 begann der Aufstieg der argentinischen Filmindustrie mit dem Aufkommen des Tonfilms. Damit begann die beste Zeit des argentinischen Kinos, die Filme dieses Landes wurden in der ganzen Welt gezeigt. Besonders bekannt wurden die „Tangofilme“ aus Buenos Aires, unter anderem mit dem Superstar Carlos Gardel.

Ab der Mitte der 1940er Jahre griff allerdings der Staat mittels Zensur und Einmischung in die Kinoszene ein. Besonders dramatisch wurde dies in den Militärregierungen (1966–1973 und 1976–1983). In den demokratischen Zwischenzeiten wurden jedoch künstlerisch sehr hochwertige Filme produziert.

1968 kam "La hora de los hornos" (deutsch: "Die Stunde der Hochöfen") von Pino Solanas heraus, ein Film, der als einer der Höhepunkte des politischen lateinamerikanischen Kinos gilt. Ein anderer politischer Filmemacher aus dieser Zeit ist Raymundo Gleyzer. Nach der Militärdiktatur begann das Kino, die Terrorherrschaft aufzuarbeiten. Es entstanden Filme wie "La Historia Oficial" (Luis Puenzo) (und Siegerfilm Oscar für den besten ausländischen Film), "La Noche de los Lápices" (Héctor Olivera), Die Neun Königinnen (Nueve Reinas) (Fabián Bielinsky) und später "Garage Olimpo" (Marco Bechis), die teils fiktive, teils wahre Fälle von sogenannten „Verschwundenen“ auf die Leinwand brachten.

1997 leitete "Pizza, Birra, Faso" (Adrián Caetano) die Epoche des „Nuevo Cine Argentino“ ein, in dem vor allem Geschichten aus dem Milieu der einfachen Leute und Elendsviertelbewohner verfilmt wurden.

Heute ist die argentinische Filmszene vor allem in Buenos Aires und in geringerem Maße auch in Rosario und Santa Fe sehr aktiv. Der international bekannteste Regisseur ist derzeit wohl der Berlinale-Gewinner Pino Solanas mit seinen sozialkritischen Filmen wie "Sur", "El viaje" ("Die Reise"), "Tangos – el exilio de Gardel" sowie den aktuellen Dokumentationen "Memoria del Saqueo" und "La Dignidad de los Nadies", die den Zustand von Politik und Gesellschaft des heutigen Argentinien beschreiben.

2010 gewann der argentinische Film "El secreto de sus ojos" ("In ihren Augen") neben anderen Auszeichnungen den Oscar für den besten nicht-englischsprachigen Film. Drehbuch und Regie stammen von Juan José Campanella.

Die Argentinier sind fußballbegeistert. Bereits 1893 wurde der argentinische Fußballverband AFA gegründet, dieser gehört somit zu den ältesten nationalen Fußballverbänden der Erde. Das erste Länderspiel der argentinischen Nationalmannschaft wurde 1902 gegen Uruguay ausgetragen. Seither hat die Nationalmannschaft 14 Mal die südamerikanische Fußballmeisterschaft, die Copa América, und zweimal die Fußball-Weltmeisterschaft gewonnen. Die beiden bekanntesten Fußballclubs sind River Plate und Boca Juniors, beide aus Buenos Aires. Bei Boca Juniors hat der bekannteste argentinische Fußballspieler Diego Maradona gespielt, der oft als einer der besten oder sogar als bester Fußballspieler des 20. Jahrhunderts überhaupt bezeichnet wird und inzwischen auch die Nationalmannschaft trainiert hat. Mittlerweile hat ihn Lionel Messi, der derzeit beim FC Barcelona unter Vertrag steht, als besten argentinischen Fußballer abgelöst. Die Spiele zwischen diesen beiden Mannschaften werden Superclásicos genannt und das öffentliche Leben steht dabei praktisch still.

Ein weiterer beliebter Sport in Argentinien ist Rugby in der Variante Rugby Union. Die argentinische Rugby-Nationalmannschaft, die „Pumas“, spielt mittlerweile auf höchstem internationalen Niveau und vollzog in den letzten Jahren eine große Entwicklung nach vorn. Bei der Weltmeisterschaft 2007 in Frankreich belegte sie den dritten Platz und schlug dabei Größen wie Frankreich und Schottland. Auch Basketball (bei Männern) und Hockey (bei Frauen) sind weit verbreitet, bei beiden Sportarten gehören die Nationalmannschaften mit zur Weltspitze.

Neben Fußball und anderen Ballsportarten genießt der Pferdesport, insbesondere das Polo ein großes Interesse in Argentinien. Die argentinische Polo-Nationalmannschaft gehört zu den besten der Welt und konnte bisher viermal den Sieg bei der Poloweltmeisterschaft erringen: 1987, 1992, 1998 und 2011. Im Gegensatz zu Polo, das eher von Mitgliedern der argentinischen Oberschicht gespielt wird, ist Pato, das den offiziellen Titel des argentinischen Nationalsports trägt, ein Spiel der einfachen Landbevölkerung, eine Art Basketball auf Pferden.

Im Gegensatz zu den Mannschaftssportarten sind die argentinischen Erfolge in Individualsportarten geringer. Ausnahme ist Tennis, bei dem mehrere Spieler bisher zur Weltspitze gehörten. Bekannt sind vor allem Guillermo Vilas, Juan Martín del Potro, Gastón Gaudio, David Nalbandian und früher bei den Damen Gabriela Sabatini. Von weiten Teilen der Bevölkerung werden auch Squash und Paddle Tennis gespielt. Auch im Schwimmsport gab es einige Vertreter in der Weltspitze, in der Leichtathletik dagegen wurden von wenigen Ausnahmen abgesehen nur auf südamerikanischer Ebene Erfolge erzielt. Im Kampfsport ist die beliebteste Disziplin Boxen, das trotz der relativ geringen internationalen Bekanntheit argentinischer Boxer ein reges Medieninteresse, auch bei den Frauen, hervorruft.

Im Motorsport ist wegen der landschaftlichen Bedingungen besonders die Rallye beliebt. Der bekannteste Vertreter ist jedoch der Formel-1-Fahrer Juan Manuel Fangio, vor Michael Schumacher lange Zeit Rekordweltmeister dieser Disziplin.
Im November 2012 fanden in Bahía Blanca im Rahmen der Speedway-Junioren U-21-Weltmeisterschaft zwei Finalläufe statt.

"Siehe auch:"


Sollte ein Feiertag auf einen Samstag oder Sonntag fallen, so ist der darauf folgende Montag meist arbeitsfrei. Diese Regelung gilt nicht für Neujahr, Ostern und Weihnachten, den Tag der Arbeit sowie den 24. März, 25. Mai und den 9. Juli.

Zusätzliche Feiertage, die für Angehörige der jüdischen Gemeinde arbeitsfrei sind (Daten sind variabel und richten sich nach dem jüdischen Kalender):


Zusätzliche Feiertage, die für die Angehörigen der muslimischen Gemeinde arbeitsfrei sind (Daten sind variabel und richten sich nach dem islamischen Kalender):


Typisch für die argentinische Esskultur ist das Rindfleisch, traditionell als Asado oder "Parrillada" auf einem Holz- oder Holzkohlegrill zubereitet. Des Weiteren sind der Locro, ein Maiseintopf mit zahlreichen Zutaten, und die Empanadas, gefüllte Teigtaschen, verbreitete argentinische Gerichte.

Bei den Getränken ist der Mate besonders charakteristisch, der auch in den Nachbarländern Uruguay, Paraguay, Chile sowie im Süden Brasiliens getrunken wird. Er ist ein teeartiger Aufguss aus den getrockneten und zerkleinerten Blättern des Mate-Strauchs "(Yerba Mate)", den man durch einen metallenen Trinkhalm, Bombilla genannt, und zumeist in geselliger Runde und bei jeder Gelegenheit trinkt. Dabei ist es üblich, dass nur ein (ebenfalls Mate genanntes) Trinkgefäß jeweils mit heißem, aber nicht kochenden Wasser neu aufgegossen und weitergereicht wird. Oft trinkt man den Mate-Tee auch als kalte Variante, die Tereré genannt wird. Argentinien besitzt außerdem mehrere große Weinanbaugebiete.

Homosexualität ist in Argentinien mittlerweile weitgehend gesellschaftlich akzeptiert. 2010 wurde die Ehe für homosexuelle Paare geöffnet; in der autonomen Stadt Buenos Aires und der Provinz Río Negro konnten gleichgeschlechtliche Paare bereits seit 2003 eine eingetragene Partnerschaft eingehen. Es bestehen jedoch auf Bundesebene keine Antidiskriminierungsgesetze zum Schutz der sexuellen Orientierung.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Argentinien Platz 50 von 180 Ländern. Die Presse des Landes gilt als weitgehend frei, es gibt jedoch laut der Nichtregierungsorganisation auch "Erkennbare Probleme" in der Unabhängigkeit des Journalismus.
Argentinien hat einen staatlichen Fernsehsender, Canal 7. Daneben gibt es eine Vielzahl von lokalen und nationalen, privaten Fernsehsendern, die über Antenne und Kabel zu empfangen sind. Des Weiteren eine große Anzahl von Sendern, die nur über Kabel und Satellit verbreitet werden.
Die bekanntesten Sender sind die per Antenne zu empfangenen Telefe, Canal 9, América TV und Canal 13, die in vielen Regionen auch lokale Programme ausstrahlen.

Einige argentinische Fernsehserien (darunter viele Telenovelas, Familienserien, aber auch wöchentliche Produktionen wie etwa Los Simuladores) sind wegen ihrer niedrigen Produktionskosten und der hohen Qualität zu einem Exportschlager vor allem nach Osteuropa geworden.

Mit dem Ziel einer stärkeren Integration Lateinamerikas ist Argentinien zusammen mit Uruguay, Kolumbien, Venezuela und Kuba an dem Satellitensender teleSUR beteiligt, der im Juli 2005 seinen Sendebetrieb aufgenommen hat.

Radio ist ein sehr beliebtes Medium in Argentinien. Es gibt eine Fülle von staatlichen und privaten Radiosendern. Von den privaten Radiosendern sind viele in "Cadenas", Radio-Ketten zusammengeschlossen und so kann man viele Sender aus Buenos Aires im ganzen Land empfangen. Der staatliche Auslands-Rundfunksender Radiodifusión Argentina al Exterior (RAE) existiert seit 1949. Die Sendungen werden in Deutsch, Englisch, Französisch und Spanisch auf den Frequenzen 6060, 11.710 und 15.345 kHz ausgestrahlt. Radio 360 in Euskirchen verbreitet die deutschsprachigen Sendungen auch als Podcast. Empfangsberichte werden von RAE mit QSL-Karten bestätigt, wenn Rückporto in Form von Internationalen Antwortscheinen beigefügt wird.

Es werden in Argentinien über 200 Tageszeitungen publiziert. Die auflagenstärksten erscheinen in Buenos Aires, zu nennen sind hier "Clarín", "La Nación" sowie die Boulevardzeitungen "Diario Popular", "La Razón", "Perfil", "Crónica", "Tiempo Argentino", "Ámbito Financiero" und "Buenos Aires Herald". Eine linksalternative Zeitung aus Buenos Aires ist "Página/12" mit detailliertem Kulturteil. Auflagenstarke Zeitungen aus anderen Städten sind "La Capital" (Rosario), die älteste heute noch erscheinende Zeitung des Landes, sowie "La Voz del Interior" (Córdoba) mit der höchsten Auflage im Landesinneren und "La Gaceta" (Tucumán). Erwähnenswert ist weiterhin "El Tribuno", die in drei verschiedenen Ausgaben in den Provinzen Salta, Tucumán und Jujuy herausgegeben wird.

In jüngerer Zeit haben eine Reihe von Zeitungen in den Großstädten Bedeutung erlangt, die vor allem in Bussen und Bahnen kostenlos verteilt werden (zum Beispiel "La Razón" und "El Diario del Bolsillo").

In Argentinien gibt es zudem eine große Anzahl von Zeitschriften und Wochenblättern. Die bekanntesten Nachrichtenmagazine sind "Noticias" und "Veintitrés", auflagenstarke Magazine des Boulevardjournalismus sind "Gente" und "Paparazzi". Des Weiteren erscheinen zahlreiche lokale Ausgaben internationaler Zeitschriften.

"Siehe auch:" Liste argentinischer Zeitungen

In Buenos Aires wird seit 1878 das "Argentinische Tageblatt" herausgegeben. Es erschien zwischen 1889 und 1981 täglich, wurde dann jedoch aus ökonomischen Gründen in eine Wochenzeitung umgewandelt. Zwischen 1880 und 1945 erschien zusätzlich die "Deutsche La Plata Zeitung".

Im Hörfunk gibt es beispielsweise im Programm des Senders "Radio Popular" eine Sendung mit dem Namen „Treffpunkt Deutschland“, die sonntags von 10 bis 14 Uhr über Mittelwelle 660 kHz sowie via Internet übertragen wird. Auf Kurzwelle 15345 kHz sendet montags bis freitags der Radiosender Radiodifusión Argentina al Exterior ein einstündiges Programm in deutscher Sprache, welches ebenfalls auch im Internet gehört werden kann.





</doc>
<doc id="292" url="https://de.wikipedia.org/wiki?curid=292" title="Antigen">
Antigen

Antigene (über engl. anti[body-]gen[erator]s letztlich von gr. ἀντί "anti" ‘[da]gegen’ plus gr. γεννάω "gennaō" ‘erzeuge/gebäre’) sind Stoffe, an die sich Antikörper und bestimmte Lymphozyten-Rezeptoren spezifisch binden können (wobei Letzteres ebenfalls bewirken kann, dass Antikörper gegen das Antigen produziert werden). 

Durch somatische Gen-Umlagerung können Lymphozyten Rezeptoren für fast alle möglichen Stoffe bilden. Diese Stoffe werden Antigene genannt. Die entsprechenden Rezeptoren der Lymphozyten heißen je nach Art der Lymphozyten B-Zell-Rezeptoren oder T-Zell-Rezeptoren. Ursprünglich wurde der Begriff allerdings nur auf Substanzen angewendet, die nach Injektion in einen fremden Organismus zur Antikörperbildung führten. Die spezifische Bindung von Antikörpern und Antigen-Rezeptoren an Antigene ist ein wesentlicher Teil der adaptiven Immunität gegen Pathogene. Antigene können also eine Immunantwort auslösen und damit "immunogen" wirken, jedoch ist nicht jedes Antigen auch immunogen (z. B. Haptene wirken nicht immunogen). Die Stelle des Antigens, die von dem entsprechenden Antikörper erkannt wird, heißt Epitop.

Antigene sind meistens Proteine, können aber auch Kohlenhydrate, Lipide oder andere Stoffe sein. Sie können entweder von B-Zell-Rezeptoren, T-Zell-Rezeptoren oder (von B-Zellen produzierten) Antikörpern erkannt bzw. gebunden werden. 

Antigene, welche von B-Zell-Rezeptoren oder Antikörpern erkannt werden, befinden sich auf den Oberflächen von eingedrungenen Fremdkörpern (z. B. auf Pollenkörnern, Bakterienoberflächen und im Kot von Hausstaubmilben) oder Zellen und weisen dort eine dreidimensionale Struktur auf, welche spezifisch von bestimmten B-Zell-Rezeptoren oder Antikörpern erkannt werden kann. Antigene auf Zelloberflächen werden als "Oberflächenantigene" bezeichnet. Diese werden unter anderem zur Entwicklung von Impfstoffen gegen Pathogene oder Tumoren eingesetzt oder werden vor einer Bluttransfusion oder Organtransplantation untersucht, um eine Immunreaktion gegen fremde Blutgruppen zu vermeiden.

Antigene, welche von T-Zell-Rezeptoren erkannt werden, sind denaturierte Peptidsequenzen von ca. zehn Aminosäuren, die von antigenpräsentierenden Zellen (APC) aufgenommen und zusammen mit MHC-Molekülen an der Oberfläche präsentiert werden. 

Auch körpereigene Strukturen, so auch Antikörper selbst, können als Antigene wirken, wenn sie fälschlicherweise als fremd angesehen werden (Autoantikörper). Dadurch wird eine Autoimmunreaktion ausgelöst, diese kann in schweren Fällen zu einer Autoimmunkrankheit führen.

Bestimmte niedermolekulare Stoffe, die alleine keine Antikörperreaktion hervorrufen können, sondern erst durch die Bindung an ein Trägerprotein eine Immunreaktion auslösen können, heißen Haptene. Diese Haptene waren bei der Erforschung der Bindung durch Antikörper an ein Antigen wichtig, indem sie als chemisch definierte und veränderbare Versuchsobjekte dienten. 
Dementsprechend wird eine (meist höhermolekulare) Substanz, die diese Reaktion alleine ermöglicht, als Vollantigen bezeichnet, ein Hapten als Halbantigen.

Kleine Moleküle wie einzelne Kohlenhydrate, Amino- oder Fettsäuren können keine Immunreaktion bewirken.

Der Nutzen der Antigenerkennung durch Lymphozyten liegt für den Organismus darin, körperfremde Substanzen, gegen die er keine erblich kodierten Rezeptoren besitzt, zu erkennen. Lymphozyten, die an körpereigene Substanzen (Autoantigene) binden, sterben ab, Lymphozyten, die an fremde Antigene binden, sind in der Lage, eine adaptive Immunantwort auszulösen. 

T-Lymphozyten (T-Zellen) erkennen Antigene nur, wenn diese auf den Oberflächen von anderen Zellen präsentiert werden.
Antigenpräsentierende Zellen (APCs) sind spezialisierte Zellen des Immunsystems, die den T-Zellen Antigene präsentieren. Zu den sog. professionellen APCs gehören Dendritische Zellen, Makrophagen und B-Zellen. Sie nehmen Substanzen über verschiedene Mechanismen wie beispielsweise durch Endocytose auf, verarbeiten sie und koppeln sie an MHC-Moleküle. Diese werden dann an der Plasmamembran präsentiert. Eine T-Zelle mit einem passenden T-Zell-Rezeptor (TCR) kann das Antigen dann als fremd erkennen und wird, wenn auch weitere costimulative Signale vorliegen, aktiviert. 

B-Lymphozyten (B-Zellen), welche mit ihrem B-Zell-Rezeptor (der membranständige Vorläufer des Antikörpers) an ein Antigen gebunden haben, werden je nach Antigen entweder direkt (TI-antigen) oder mit Hilfe einer T-Helferzelle aktiviert. T-Helferzellen, welche an einen Antigen-MHC-Komplex gebunden haben und das Antigen als fremd erkannt haben, scheiden Cytokine aus, die B-Zellen zur Antikörperproduktion anregen. Je nachdem, welche Cytokine in der Umgebung ausgeschüttet werden, findet ein Class-Switch in eine der Klassen (Ig G, Ig E, Ig A) statt. Antikörper werden von den Plasma-Zellen (aktivierte B-Zellen) sezerniert, binden spezifisch an das Antigen, markieren damit den Eindringling (Opsonisierung) und führen so zur Phagocytose der Fremdkörper. Diese Aufgabe übernehmen beispielsweise Makrophagen, welche mit ihren Fc-Rezeptoren an die konstante Region der Antikörper binden. Durch die Erkennung körperfremder Antigene können gezielt Eindringlinge wie Bakterien oder Viren bekämpft werden, ohne körpereigene Zellen zu schädigen.

Auch die Zellen eines fremden Menschen werden als körperfremd erkannt, denn die Struktur der Glykoproteine auf den Zelloberflächen ist bei jedem Menschen anders. Daher wirken sich diese menschlichen Antigene bei der Übertragung von organischem Material von einem Menschen auf einen anderen nachteilig aus, z. B. bei der Bluttransfusion oder Organtransplantation. Hier muss auf Blutgruppen- bzw. Gewebeverträglichkeit geachtet werden. Die Übertragung falscher Blutgruppen führt zur Verklumpung des Blutes, bei Transplantationen kann es zur Abstoßung des übertragenen Organs oder zur Schädigung des Empfängers durch das transplantierte Organ kommen (Graft-versus-Host Disease).

Antigene, die Allergien auslösen können, werden Allergene genannt. Sie erzeugen eine übermäßige Immunantwort auf ein harmloses Antigen.




</doc>
<doc id="293" url="https://de.wikipedia.org/wiki?curid=293" title="Allergen">
Allergen

Ein Allergen ist eine Substanz, die über Vermittlung des Immunsystems Überempfindlichkeitsreaktionen (allergische Reaktionen) auslösen kann. Die verschiedenen Überempfindlichkeitsreaktionen (Allergien, Pseudoallergien und Intoleranzen) sind im Artikel Allergie beschrieben. Dieser Artikel beschreibt die Stoffe.

Ein Allergen ist ein Antigen. Allergene haben keine chemischen Gemeinsamkeiten. Deswegen ist es nicht möglich, eine Chemikalie zu entwickeln, die Allergene zerstört. Die meisten Allergene sind Eiweiße oder Eiweißverbindungen. Das Immunsystem allergischer Patienten reagiert mit der Bildung von IgE-Antikörpern auf den Kontakt mit Allergenen. „Pseudoallergene“ sind demgegenüber Stoffe, bei denen das Immunsystem nicht beteiligt ist, wohl aber Mediatoren, wie z. B. die Histamine.

Allergene können nach verschiedenen Gesichtspunkten eingeteilt werden:

IgE-reaktive Allergene sind jene Antigene, gegen die sich die fehlgeleitete Immunantwort bei Typ-I-Allergien richtet. Diese Allergene kommen ubiquitär (überall) vor und jeder Mensch kommt auch mit ihnen in Kontakt, und zwar durch Inhalation, Nahrungsaufnahme oder Berührung. Bei gesunden Personen kommt es entweder zu keiner Immunantwort gegen Allergene oder zu einer milden Immunantwort mit der Bildung von Allergen-spezifischen IgG- und IgG-Antikörpern. Im Gegensatz dazu kommt es bei Allergikern zu einer Bildung von Allergen-spezifischen IgE-Antikörpern. Diese veränderte Immunantwort wird – vereinfacht dargestellt – einem verschobenen T-Helferzellen Typ 1 – Typ 2 (Th1-Th2)-Gleichgewicht zugeschrieben, mit einer Th2-dominierten Immunantwort bei Allergikern und einer Th1-dominierten Immunantwort bei gesunden Personen.

Allen IgE-reaktiven Allergenen ist gemeinsam, dass sie sehr gut wasserlösliche und eher sehr stabile Proteine oder Glycoproteine sind. Es handelt sich meist um kleine Proteine in der Größe von 5 bis 80 kDa. Sonst sind Allergene von ihrer Struktur, Aminosäure-Sequenz oder biologischen Funktion her, sehr unterschiedlich. Die Frage, „was ein Allergen zu einem Allergen macht“, konnte noch nicht befriedigend geklärt werden. Verschiedene Faktoren, wie z. B. die Art der Allergenaufnahme (z. B. durch Inhalation), die Partikelgröße, die enzymatische Aktivität einiger Allergene und die Tatsache, dass Allergene in ausgesprochen kleinen Mengen aufgenommen werden (nanogramm-Mengen reichen), scheinen einen Einfluss auf die Allergenizität zu haben. Alle Allergene haben gemeinsam, dass sie von dendritischen Zellen aufgenommen werden müssen und eine Differenzierung der dendritischen Zelle in eine Th2-induzierende aktivierte dendritische Zelle induzieren.
IgE-reaktive Allergene kommen in einer Vielzahl von Allergenquellen vor. Eine Allergenquelle kann mehrere verschiedene Allergene freisetzen. So sind für die Hausstaubmilbe "Dermatophagoides pteronyssinus" mehr als 20 verschiedene Allergene bekannt. Allergiker können gegen nur ein Allergen oder auch gegen mehrere Allergene einer Allergenquelle sensibilisiert sein, also IgE-Antikörper bilden. Unter „Hauptallergenen“ versteht man jene Allergene einer Allergenquelle, gegen die mehr als 50 % der Patienten mit der betreffenden Allergie, IgE-Antikörper bilden. Alle anderen sind „Nebenallergene“.

Kontaktallergene sind Auslöser von Typ-IV-Allergien. Das typische Krankheitsbild ist das allergische Kontaktekzem, das sich genau an den Körperstellen zeigt, die mit dem betreffenden Allergen in Kontakt kommen. Meist sind das die Hände, das Gesicht, die Unterschenkel oder der Nacken.

Zu den häufigsten Kontaktallergenen gehören Nickel, Thiomersal, Parfum, Cobalt, Formaldehyd, Perubalsam, Kolophonium, Isothiazolinone, Chrom, Thiuramix.

Besonders im beruflichen Umfeld spielen Kontaktallergene eine große Rolle. Berufsgruppen, die häufig betroffen sind, sind Köche, Friseure, Bäcker, Reinigungskräfte, Personal in der Möbelherstellung, in Fleisch und Fisch verarbeitenden Betrieben und in Gärtnereien.

Die EU-Richtlinie 94/27/EG („Nickel Directive“) legt fest, dass Nickel nicht in Schmuck und anderen mit der Haut in Berührung kommenden Produkten enthalten sein darf.

Beispiele für Kontaktallergene:


Inhalationsallergene oder Aeroallergene werden über die Atmung aufgenommen. Ein typisches Beispiel sind Birkenpollenallergene.

Beispiele für Inhalationsallergene:


Nahrungsmittel- und Arzneimittel-Allergene werden durch den Mund in den Körper aufgenommen.

Beispiele für Nahrungsmittel- und Arzneimittel-Allergene:


Es ist sehr schwierig und bisher kaum zufriedenstellend gelungen, die Allergie-auslösende Aktivität verschiedener Allergene miteinander zu vergleichen. Methoden zur Allergen-Bewertung wurden in den USA und in Australien entwickelt: Die FDA verwendet für Lebensmittelallergene die Bewertung nach LOAEL (Lowest Observed Adverse Effect Level), daneben bestehen Schwellenwerte für einige Allergene in ppm (mg Protein pro kg Lebensmittel) gemäß VITALKonzept (Voluntary Incidental Trace Allergen Labelling, Australien).

Entsprechend Lebensmittel-Kennzeichnungsverordnung muss der Gehalt bestimmter (häufiger) Allergengruppen in Lebensmitteln deklariert werden.

Obige Tabellen enthüllen zum Einen Widersprüche in der Bewertung des allergenen Potentials verschiedener Lebensmittel, was die Problematik dieser Quantifizierungen belegt. Zum Anderen liefern sie aber doch eine Basis, verschiedene Lebensmittel bezüglich ihrer Allergenität miteinander zu vergleichen und eine grobe Rangfolge abschätzen zu können.

Laut einer Studie werden Nahrungsmittelallergene wie beispielsweise Milchbestandteile, Haselnüsse, Meeresfrüchte, Ovalbumin oder Fischallergene in vitro durch Simulierung der sauren Magenverdauung mit Pepsin innerhalb von wenigen Minuten komplett verdaut, bei pH-Wert-Anhebung allerdings nicht. Daraus folgerten die Forscherinnen, dass Nahrungsmittelallergieprobleme mit einem erhöhten pH-Milieu des Magens in Zusammenhang stehen könnten. Säuglinge hätten erst am Ende des zweiten Lebensjahres Magensäurewerte wie Erwachsene. Auch Personen mit verminderter Magensäuresekretion oder nach Einnahme von Antazida, Sucralfat, H-Rezeptor-Blockern oder Protonenpumpeninhibitoren haben erhöhte pH-Werte im Magen.

Injektionsallergene gelangen durch Injektionen in den Körper. Dazu gehören auch die Insektengiftallergien, bei denen das Allergen durch Insektenstiche übertragen wird. 

Beispiele für Injektionsallergene:


Leitallergene geben wichtige Hinweise darauf, inwieweit andere Stoffe zu einer allergischen Reaktion führen können. So reagieren Birkenallergiker in 50 % der Fälle im Rahmen einer Kreuzallergie auch auf bestimmte Nahrungsmittel wie Äpfel und Birnen.

Häufige luftübertragene, nichtallergische Reizstoffe (Pseudoallergene) sind:


Andere Auslöser allergieähnlicher Beschwerden:


Garantiert keine Allergien lösen aus:

Bei Arbeitern, die mit Vitamin B1 Umgang haben, sind Allergien beschrieben.
Auf die Anwendung von Vitamin B12 sind bei Patienten Soforttyp-Allergien und Allergien vom verzögerten Typ beschrieben. Auch Vitamin E, INCI Tocopherol, kann in kosmetischen Produkten (hier als Antioxidans eingesetzt) Allergien auslösen. Allerdings sind diese Allergien nicht häufig.




</doc>
<doc id="294" url="https://de.wikipedia.org/wiki?curid=294" title="Ada Lovelace">
Ada Lovelace

Augusta Ada Byron King, Countess of Lovelace, allgemein als Ada Lovelace bekannt (geborene "Augusta Ada Byron"; * 10. Dezember 1815 in London; † 27. November 1852 ebenda), war eine britische Mathematikerin. Für einen nie fertiggestellten mechanischen Computer, die „Analytical Engine“ von Charles Babbage, veröffentlichte sie als Erste ein komplexes Programm: Es nahm wesentliche Aspekte späterer Programmiersprachen wie etwa ein Unterprogramm oder die Verzweigung vorweg. Aus diesem Grund wird sie heute nicht nur als erste Programmiererin der Welt, sondern als erster Programmierer überhaupt bezeichnet – fast einhundert Jahre vor den modernen Pionieren der Programmierung wie Grace Hopper, Jean Bartik oder Howard Aiken. Die Programmiersprache Ada und die Lovelace Medal wurden nach ihr benannt.

Augusta Ada Byron King, Countess of Lovelace, wurde am 10. Dezember 1815 als Tochter von Anne Isabella Noel-Byron, 11. Baroness Wentworth, und George Gordon Byron (genannt "Lord Byron") geboren. Byron hatte zahlreiche Affären und drei Kinder von drei Frauen, nur Ada war ehelich geboren. Ihre Mutter zog aufgrund andauernder Auseinandersetzungen mit Lord Byron am 16. Januar 1816 gemeinsam mit der einen Monat alten Ada zu ihren Eltern nach Kirkby Mallory. Am 21. April 1816 unterzeichnete Lord Byron eine Trennungsurkunde und verließ England wenige Tage danach. Lord Byron hatte keine Beziehung zu seiner Tochter, sie traf nie mit ihm zusammen. Als Ada acht Jahre alt war, starb er.

Adas mathematisch interessierte Mutter, die in ihrer Jugend von Hauslehrern auch in Naturwissenschaften und Mathematik unterrichtet worden war, ermöglichte Ada eine naturwissenschaftliche Ausbildung. Im Verlauf ihrer mathematischen Studien lernte sie die Mathematikerin Mary Somerville kennen sowie Charles Babbage, dessen Salon sie im Alter von 17 Jahren besuchte, mit dem sie dann über Jahre korrespondierte und dessen Mitarbeiterin sie wurde. Wesentlichen Einfluss auf ihren späteren Bildungsgang und auf ihr Hauptwerk – die "Notes" – hatte Augustus De Morgan, Professor am University College London, der selbst grundlegende Beiträge zur Entwicklung der mathematischen Logik lieferte. Lovelace nahm bei ihm ab 1841 auf eigene Initiative hin Unterricht. Sie zeigte in der Öffentlichkeit ein reges Interesse an verschiedenen mathematischen und naturwissenschaftlichen Fragestellungen und verstieß damit gegen Konventionen.

Mit 19 Jahren heiratete Ada Byron William King, 8. Baron King, der 1838 zum 1. Earl of Lovelace erhoben wurde. Auch er verfügte über eine mathematische Bildung und ließ sich, da Frauen zu dieser Zeit der Zutritt zu Bibliotheken und Universitäten untersagt war, ihr zuliebe in die Royal Society aufnehmen, wo er für sie Artikel abschrieb. Sie gebar drei Kinder in sehr kurzen Abständen, eine ihrer Töchter war Anne Blunt, 15. Baroness Wentworth. Ihre Rolle als Ehefrau und Mutter machte das wissenschaftliche Arbeiten immer schwieriger. In ihrer Korrespondenz mit Mary Somerville schrieb sie, dass sie eine unglückliche Ehe führe, weil ihr neben Schwangerschaften und Kinderbetreuung so wenig Zeit für ihr Studium der Mathematik und ihre zweite Leidenschaft, die Musik, bleibe; sie war eine „passionierte Harfenspielerin“. Um sich abzulenken, stürzte sie sich ins Gesellschaftsleben und hatte mehrere Affären. Mit großer Begeisterung wettete sie auf Pferde. Die letzten Jahre ihres Lebens, die sie aufgrund eines Zervixkarzinoms im Bett verbrachte, soll sie mit der Entwicklung eines mathematisch ausgefeilten „sicheren“ Wettsystems verbracht haben. Ada Lovelace starb im Alter von 36 Jahren.

1843 übersetzte sie die durch den italienischen Mathematiker Luigi Menabrea auf Französisch angefertigte Beschreibung von Babbages "Analytical Engine" ins Englische. Durch Babbage ermutigt, fügte sie ihre eigenen Notizen und Überlegungen zum Bau dieser geplanten Maschine hinzu. Diese "Notes" waren bei ihrer Veröffentlichung etwa doppelt so umfangreich wie Menabreas ursprünglicher Artikel.

Babbages Maschine wurde zu seinen Lebzeiten niemals gebaut. Einerseits war die Feinmechanik noch nicht weit genug entwickelt, um die Maschinenteile in der nötigen Präzision herzustellen, andererseits verweigerte das britische Parlament die Finanzierung von Babbages Forschungsprogramm, nachdem es die Entwicklung der Vorgängermaschine – der "Difference Engine" – bereits mit 17000 britischen Pfund gefördert hatte (ein Wert von rund 3,4 Millionen britischen Pfund im Jahr 2014).

Dessen ungeachtet legte Ada Lovelace in den "Notes" einen schriftlichen Plan zur Berechnung der Bernoulli-Zahlen in Diagrammform vor, welcher als das erste veröffentlichte formale Programm gelten kann.

In Lovelaces "Notes" finden sich eine Reihe dem Stand der Forschung um 1840 weit vorausgreifende Konzepte. Während ihre Beiträge zu Rechnerarchitektur und Grundlagen der Programmierung bis zu ihrer Wiederentdeckung in den 1980er Jahren weitgehend in Vergessenheit gerieten, spielten ihre Standpunkte zur künstlichen Intelligenz in erkenntnistheoretischen Debatten als „Lady Lovelace’s Objection“ bereits bei Begründung dieses Forschungsbereichs der Informatik eine gewisse Rolle (s. u.).

Eine Bemerkung zeigt, dass sie den entscheidenden Unterschied zwischen einer bloßen Rechenmaschine und einem Computer verstanden hatte:

Eine Rechenmaschine kann nur eine fixe Berechnung durchführen oder ist auf die manuelle Eingabe der durchzuführenden Operationen angewiesen. Mit der Programmierung dagegen kann man beliebig komplexe Algorithmen für den Rechner formulieren und automatisch ablaufen lassen.

Eine zweite Bemerkung beweist, dass sie auch die Möglichkeit erkannte, mit einem Computer mehr als nur arithmetische Aufgaben zu bearbeiten:

Babbages Motivation für die Analytical Engine war die Berechnung von Zahlentabellen für den Einsatz in Naturwissenschaft und Ingenieurwesen. Lovelace dagegen hatte das weitaus größere Potenzial der Maschine erkannt: Sie würde nicht nur numerische Berechnungen anstellen können, sondern auch Buchstaben kombinieren und Musik komponieren. Diese nämlich beruhe auf den Relationen von Tönen, welche sich als Zahlenkombinationen ausdrücken ließen.

Auch erkannte Ada Lovelace, dass die Maschine einen physischen Teil hat, nämlich die Kupferräder und Lochkarten, und einen symbolischen, also die automatischen Berechnungen, die in den Lochkarten codiert sind. Damit nahm sie die Unterteilung in Hardware und Software vorweg.

Babbage stellte Lovelace eine Formel für die Bernoulli-Zahlen zur Verfügung. Lovelace schrieb dazu in Tabellendarstellung Befehle auf, die die nötigen Schritte für die Berechnung für die verschiedenen Maschinenteile aufführten. In diesem Zusammenhang stellte sie Überlegungen darüber an, wie mathematische Operationen sich verbinden, verschachteln und wiederholen ließen, um die Anzahl der Instruktionen im Programm zu vermindern – heute würde man von Verzweigungen, Schleifen und Rekursion sprechen.

In den "Notes" schreibt Lovelace 1843: „Die Maschine kann [nur] das tun, was wir ihr zu befehlen vermögen, sie kann der [Anm. d. Ü.: gemeint "unserer"] Analyse folgen. Sie hat jedoch keine Fähigkeit zur Erkenntnis analytischer Verhältnisse oder Wahrheiten“. Umgangssprachlich postuliert Lovelace hier, dass eine Maschine im Gegensatz zum menschlichen Geist keine Fähigkeit zur Intuition habe und daher nicht zu eigener Erkenntnis befähigt sei.

Alan Turing geht in seinem Artikel "Computing Machinery and Intelligence" () aus dem Jahr 1950 auf diesen Einwand als „Lady Lovelace’s Objection“ ein. Die These (und Turings Widerspruch dagegen) ist seitdem immer wieder Gegenstand von Debatten sowohl in der Informatik als auch in der Philosophie.








</doc>
<doc id="295" url="https://de.wikipedia.org/wiki?curid=295" title="Ascorbinsäure">
Ascorbinsäure

Ascorbinsäure ist ein farb- und geruchloser, kristalliner, gut wasserlöslicher Feststoff mit saurem Geschmack. Sie ist eine organische Säure, genauer eine vinyloge Carbonsäure; ihre Salze heißen Ascorbate. Ascorbinsäure gibt es in vier verschiedenen stereoisomeren Formen, biologische Aktivität weist jedoch nur die -(+)-Ascorbinsäure auf. Eine wichtige Eigenschaft ist beim Menschen und einigen anderen Spezies die physiologische Wirkung als Vitamin. Ein Mangel kann sich bei Menschen als Skorbut manifestieren. Der Name ist daher abgeleitet von der lateinischen Bezeichnung der Krankheit, "scorbutus," mit der verneinenden Vorsilbe "a-" (weg-, un-), also die ‚antiskorbutische‘ Säure. Da Ascorbinsäure leicht oxidierbar ist, wirkt sie als Redukton und wird als Antioxidans eingesetzt.

Die -(+)-Ascorbinsäure und ihre Ableitungen (Derivate) mit gleicher Wirkung werden unter der Bezeichnung Vitamin C zusammengefasst. Der Sammelbegriff Vitamin C umfasst daher auch Stoffe, die im Körper zu -(+)-Ascorbinsäure umgesetzt werden können, wie zum Beispiel die Dehydroascorbinsäure (DHA).

Skorbut war bereits im 2. Jahrtausend v. Chr. im Alten Ägypten als Krankheit bekannt. Auch der griechische Arzt Hippokrates und der römische Autor Plinius berichten darüber.

Bis ins 18. Jahrhundert war Skorbut die häufigste Todesursache auf Seereisen. Im Jahre 1747 untersuchte der britische Schiffsarzt James Lind diese Krankheit. Er nahm zwölf Seeleute, die unter Skorbut litten, und teilte sie in sechs Gruppen zu je zwei Personen. Jeder Gruppe gab er zusätzlich zu den üblichen Nahrungsrationen einen weiteren speziellen Nahrungsmittelzusatz, darunter Obstwein, Schwefelsäure, Essig, Gewürze und Kräuter, Seewasser, sowie Orangen und Zitronen. Er stellte fest, dass die Gruppe, welche die Zitrusfrüchte erhielt, eine rasche Besserung zeigte. Im Jahr 1757 veröffentlichte Lind dieses Resultat. Doch erst 1795 ließ die britische Marine die Nahrungsrationen auf See mit Zitronensaft ergänzen. Zusätzlich wurden Sauerkraut und Malz zur Skorbutprävention eingesetzt. Lange Zeit wurde behauptet, dass Skorbut die Folge einer speziellen bakteriellen Erkrankung, Vergiftung, mangelnder Hygiene oder Überarbeitung sei.

Der Engländer George Budd vermutete bereits 1842, dass in der Nahrung spezielle essentielle Faktoren enthalten sein müssen. Fehlen diese, würden erkennbare Mangelerscheinungen auftreten. Diese Entwicklungen gerieten wieder in Vergessenheit, als die Reisedauer durch das Aufkommen der Dampfschifffahrt stark verkürzt wurde und dadurch die Gefahr des Mangels sank. Außerdem führte die fehlende exakte Identifikation des Vitamins dazu, dass wirksamer frischer Orangensaft durch billigeren gekochten Limettensaft ersetzt wurde. Zuletzt machte Ende des 19. Jahrhunderts die sogenannte "Ptomain-Theorie" von sich reden, die eine Nahrungsmittelvergiftung für den Skorbut verantwortlich machte. So kam es, dass auf den großen Polarexpeditionen wieder der Skorbut Einzug hielt und zwar mit frischen Lebensmitteln geheilt werden konnte, aber es hatte zunächst niemand ein korrektes Konzept für die Vorbeugung. Betroffen waren insbesondere die britische Arktisexpedition 1875–1876, die Jackson-Harmsworth-Expedition 1894–1897, Scotts Discovery-Expedition 1901–1904 und die Terra-Nova-Expedition 1910–1913.

Im Jahr 1907 entdeckten zwei norwegische Ärzte zufällig ein Tiermodell zur Erforschung des Skorbuts: Axel Holst und Theodor Frølich studierten ursprünglich den „Schiffs-Beriberi“ der Schiffsbesatzungen der norwegischen Fischereiflotte, und zwar anhand von Tauben als Versuchstiere. Sie gaben später Meerschweinchen dasselbe Futter aus Getreide und Mehl, die jedoch unerwarteterweise mit Skorbutsymptomen reagierten. Somit beobachteten Holst und Frølich erstmals den Skorbut, der bis dahin nur bei Menschen beobachtet wurde, an Tieren. Sie zeigten ferner, dass durch bestimmte Futterzusätze die Krankheit bei den Meerschweinchen geheilt werden konnte. Damit leisteten sie einen wesentlichen Beitrag zur Entdeckung des Vitamins C ab dem Jahre 1928 durch den Ungarn Albert Szent-Györgyi und den Amerikaner Charles Glen King.

Im Jahr 1912 entdeckte der Biochemiker Casimir Funk nach Studien zu der Mangelerkrankung Beriberi, dass diese durch das Fehlen der chemischen Substanz Thiamin (Vitamin B) verursacht wurde. Er prägte dafür das Kunstwort „Vitamin“, eine Zusammensetzung aus "vita" (Leben) und "Amin" (Aminogruppe). In Bezug auf Skorbut vermutete er fälschlicherweise einen ähnlichen Faktor und bezeichnete diesen als „Antiskorbut-Vitamin“ (heute: Vitamin C). Tatsächlich enthält Vitamin C keine chemische Aminogruppe, dennoch ist die Bezeichnung bis heute geblieben.

Im Jahr 1921 gab der Biochemiker Sylvester Zilva einer Mischung von aus Zitronensaft isolierten Substanzen, die in der Lage war, Skorbut zu heilen, die Bezeichnung "Vitamin C." Bereits 1927 gelang es dem ungarischen Wissenschaftler Albert von Szent-Györgyi Nagyrápolt, Vitamin C aus der Nebenniere, Orangensaft beziehungsweise Weißkohl zu isolieren. Die so isolierte Ascorbinsäure sandte er Zilva zu, der diese aber nach Analyse fälschlicherweise nicht als Vitamin C erkannte. Durch diesen Fehler verzögerte sich die Identifikation von Ascorbinsäure als Vitamin C um mehrere Jahre. In den 1920er Jahren verfehlten auch andere, wie zum Beispiel der Wissenschaftler Karl Paul Link oder Oberst Edward B. Vedder, den Nachweis dafür, dass Ascorbinsäure Skorbut heilen kann und das postulierte Vitamin C ist.

Zwischen 1928 und 1934 gelang es Szent-Györgyi sowie Joseph L. Svirbely und unabhängig davon Charles Glen King mit seinen Mitarbeitern, durch Kristallisationsversuche die für die Heilung von Skorbut verantwortliche Substanz zu isolieren.
Im Jahr 1931 isolierten King und Svirbely kristallines Vitamin C aus Zitronensaft und erkannten, dass diese Skorbut heilen kann und die physikalischen und chemischen Eigenschaften der damals noch kaum charakterisierten sogenannten "Hexuronsäure," der heutigen Ascorbinsäure, teilte. Szent-Györgyi wollte diese Säure zunächst „Ignose“ nennen (von "ignosco"), da sie trotz vieler Wissenslücken mit Hexosen verwandt war. Dieser Name wurde aber nicht akzeptiert. Da die Anzahl der Kohlenstoffatome (sechs C-Atome) bekannt war und die Substanz sich wie eine Säure verhält, wurde der Name "Hexuronsäure" von Szent-Györgyi eingeführt. Svirbely wechselte bald als Mitarbeiter zu Szent-Györgyi. Sie bewiesen, dass die bisher isolierten Substanzen mit Skorbut heilenden Eigenschaften (Vitamin C) mit der Hexuronsäure übereinstimmten. Damit stellte Szent-Györgyi fest, dass diese das lang gesuchte Vitamin C ist.

Die Struktur dieser damals noch Hexuronsäure genannten Verbindung wurde 1933 schließlich durch die Arbeiten von Walter Norman Haworth und dessen damaligen Assistenten Edmund Hirst aufgeklärt. Szent-Györgyi und Haworth änderten den Namen der Hexuronsäure schließlich in "-Ascorbinsäure," der bis heute akzeptiert wird. 1934 gelang Haworth und Tadeus Reichstein erstmals die Synthese künstlicher -Ascorbinsäure aus Glucose. Haworth erhielt 1937 für seine Forschungen am Vitamin C den Nobelpreis für Chemie, Szent-Györgyi den für Medizin. Seit 1967 propagierte Linus Pauling die Verwendung hoher Dosen von Ascorbinsäure als Vorbeugung gegen Erkältungen und Krebs, was jedoch umstritten ist. Pauling selbst nahm 18 g pro Tag ein und starb 1994 im Alter von 93 Jahren an Prostatakrebs. Der "Medical Observer online" berichtet im Januar 2012 über Untersuchungen, die zu dem Ergebnis kommen, dass Vitamin C zwar keinen Schutz vor Erkältungen biete, aber trotzdem von zentraler Bedeutung für das Immunsystem sei. So gäbe es zahlreiche Hinweise darauf, dass ausreichend Vitamin C das Immunsystem stärke.

Die industrielle Herstellung von Vitamin C begann 1934 durch Roche in der Schweiz. Die Nachfrage danach blieb anfangs gering.

In der Zeit des Nationalsozialismus (1933–45) förderten die Machthaber in Deutschland die Versorgung der Bevölkerung mit den damals gerade erst entdeckten Vitaminen sehr aktiv. Sie wollten so den „Volkskörper von innen stärken“, weil sie davon überzeugt waren, dass Deutschland den Ersten Weltkrieg auch als Folge von Mangelernährung verloren hatte. In "Vitamin-Aktionen" wurden Kinder, Mütter, Schwerstarbeiter und Soldaten mit Vitaminen versorgt, insbesondere mit Vitamin C. Nationalsozialistische Massenorganisationen wie die Deutsche Arbeitsfront und die Reichsarbeitsgemeinschaft für Volksernährung organisierten die Produktion und Verteilung von Vitamin-C-Präparaten. Hausfrauen wurden dazu aufgerufen, Hagebutten und Sanddorn zu sammeln, aus denen Brotaufstriche und andere Vitaminpräparate für die Wehrmacht hergestellt wurden. Noch 1944 bestellte die Wehrmacht 200 Tonnen Vitamin C, unter anderem bei Roche.

In der Nahrung kommt Vitamin C vor allem in Obst und Gemüse vor. Zitrusfrüchte wie Orangen, Zitronen und Grapefruits enthalten – in reifem Zustand unmittelbar nach der Ernte – viel Vitamin C. Grünkohl hat den höchsten Vitamin-C-Gehalt aller Kohlarten (105–120 mg/100 g verzehrbare Substanz). Rotkraut, Weißkraut und Sauerkraut sind ebenfalls Vitamin-C-Lieferanten. Sauerkraut war lange Zeit in der Seefahrt von Bedeutung, wo ein haltbares Vitamin-C-reiches Nahrungsmittel benötigt wurde. Die höchsten natürlichen Vitamin-C-Konzentrationen wurden in der Buschpflaume und im Camu-Camu gefunden. In Sauerkraut und Kohlgemüse ist Ascorbinsäure in Form von "Ascorbigen A" und "B" (C-2-Scatyl--ascorbinsäure) gebunden. Wird das Gemüse gekocht, zerfallen die Moleküle in -Ascorbinsäure und 3-Hydroxyindol, sodass es in gekochtem Zustand mehr Vitamin C enthalten kann als im rohen Zustand. Durch zu langes Kochen gelangt das Vitamin verstärkt in das Kochwasser. Viele Gemüsearten enthalten Ascorbinsäure-Oxidase, die insbesondere durch Zerkleinern mit dem Vitamin in Berührung kommt und dieses oxidiert. Das führt zum Beispiel bei Rohkost, die nicht sofort verzehrt wird, zu erheblichen Vitamin-C-Verlusten.

Die folgenden Angaben dienen nur der Orientierung, die tatsächlichen Werte hängen stark von der Sorte der Pflanze, der Bodenbeschaffenheit, dem Klima während des Wachstums, der Lagerdauer nach der Ernte, den Lagerbedingungen und der Zubereitung ab. Das Weizenkorn enthält zum Beispiel kein Vitamin C, sondern dies entsteht erst bei der Keimung.

Vitamin-C-Gehalt in Obst- und Gemüsesorten je 100 g (nach absteigendem Vitamin-C-Gehalt geordnet):
Vitamin-C-Gehalt in tierischen Produkten je 100 g (nach absteigendem Vitamin-C-Gehalt geordnet):


Die Jahresproduktion an Ascorbinsäure lag 2006 weltweit bei etwa 80.000 Tonnen und hat sich damit seit 1999 mehr als verdoppelt. Marktführer war lange Zeit die Schweizer Hoffmann-La Roche (30 % Weltumsatz), gefolgt vom BASF-NEPG-Kartell (auch etwa 30 %) und der Firma Merck. Im Jahr 2002 hat Hoffmann-La Roche seine Vitaminsparte für 3,4 Milliarden Schweizer Franken, etwa 2,1 Milliarden Euro, an die niederländische Koninklijke DSM verkauft.

Der größte Produzent von Ascorbinsäure ist heute die Volksrepublik China, wo sie ausschließlich biotechnologisch produziert wird.

Ascorbinsäure kann aus C5-Zuckern wie -Xyloson, -Lyxose, -Xylose und -Arabinose synthetisiert werden. Für die großtechnische Synthese dagegen wird in der chemischen Industrie aus der Ausgangssubstanz -Glucose – einer Hexose – über die Stufe des Sorbitols kristalline Ascorbinsäure, Natriumascorbat (E 301), Calciumascorbat (E 302) und Ascorbylmonophosphat hergestellt. Die 1934 entdeckte Reichstein-Synthese bildet die Grundlage der industriellen, chemisch-mikrobiologischen Produktion.
Zur Unterscheidung von diesem synthetisch hergestellten Produkt wird ein mit Hilfe gentechnisch veränderter Mikroorganismen hergestelltes Vitamin C international mit GMO-Vitamin C ("GMO," : „gentechnisch veränderter Organismus“) bezeichnet. GMO-Ascorbinsäure ist preiswerter; nach diesem Verfahren wird weltweit der größere Teil hergestellt.

Im Sonoyama-Verfahren wird Ascorbinsäure aus -Glucose hergestellt. Dabei wird dieses zunächst durch "Pantoea agglomerans" zu 2,5-Dioxo--Gluconsäure oxidiert. Ein zweiter Stamm, "Aureobacterium sp.," reduziert das Produkt zu 2-Oxo--Gulonsäure, das dann wie bei der Reichstein-Synthese zu -Ascorbinsäure umgesetzt wird. Es wird versucht, "P. agglomerans"-Stämme gentechnisch so zu verändern, dass diese aus Glucose in einem einstufigen mikrobiellen Verfahren 2-Oxo--Gulonsäure herstellen.

Ascorbinsäure bildet unter Normalbedingungen farblose Kristalle, die gegen Licht, Wärme und Luft beständig sind. Der Schmelzpunkt liegt bei 190–192 °C. Das Schmelzen erfolgt unter Zersetzung. In fester Phase bildet Ascorbinsäure zwei intramolekulare Wasserstoffbrückenbindungen, die maßgeblich zur Stabilität und damit zu den chemischen Eigenschaften der Endiol-Struktur beitragen.

Ascorbinsäure kristallisiert im monoklinen Kristallsystem mit den Gitterparametern a = 1730 pm, b = 635 pm, c = 641 pm, β = 102°11´. Die vier Moleküle der Einheitszelle sind paarweise durch Pseudoschraubenachsen verbunden. Die Moleküle bestehen aus einem Fünfring und der Seitenkette, wobei die Endiolgruppe planar ist. Infolge der guten Wasserlöslichkeit des Vitamins können die Verluste je nach Art und Dauer der Zubereitung in Lebensmitteln bis zu 100 % betragen.

Ascorbinsäure enthält mehrere Strukturelemente, die zu ihrem chemischen Verhalten beitragen: eine Lactonstruktur, zwei enolische Hydroxygruppen sowie eine sekundäre und eine primäre Alkoholgruppe. Der Lactonring ist nahezu planar.
Ascorbinsäure hat zwei asymmetrische Kohlenstoffatome (C4 und C5) und existiert damit in vier verschiedenen stereoisomeren Formen, die optische Aktivität aufweisen. Die Moleküle - und -Ascorbinsäure verhalten sich wie Bild und Spiegelbild zueinander, sie sind Enantiomere, ebenso die - und die -Isoascorbinsäure. -Ascorbinsäure und -Isoascorbinsäure sowie -Ascorbinsäure und -Isoascorbinsäure sind Epimere, sie unterscheiden sich in der Konfiguration nur eines Kohlenstoffatoms. Trotz dieser geringen Unterschiede sind die Stereoisomere der -Ascorbinsäure im Körper fast alle inaktiv, da die am Stoffwechsel beteiligten Enzyme spezifisch -Ascorbinsäure erkennen. Lediglich die -Isoascorbinsäure (E 315) weist eine geringe Wirkung auf.
Obwohl Ascorbinsäure keine der „klassischen“ sauren funktionellen Carbonsäure-, Sulfonsäure- oder Phosphonsäuregruppen aufweist, ist sie beträchtlich sauer. Mit einem pK-Wert von 4,25 ist sie saurer als Essigsäure mit pK = 4,8. Sie liegt damit unter physiologischen Bedingungen als "Ascorbat-Anion" AscH vor.

Dies ist zum einen auf die Endiol-Struktur zurückzuführen. Enole sind bereits deutlich saurer als Alkohole. Zusätzlich wird die Acidität bei Ascorbinsäure durch die zweite enolische Hydroxygruppe und durch die benachbarte Carbonylgruppe noch verstärkt. Zum anderen wird das nach Abspaltung des Protons entstehende Enolat-Anion mittels Keto-Enol-Tautomerie stabilisiert. Die dann am Sauerstoff bestehende negative Ladung wird dabei sowohl über die Doppelbindung zwischen den beiden Kohlenstoffatomen als auch über die Carbonylfunktion delokalisiert, also verteilt und somit stabilisiert. Strukturell könnte diese Gruppierung als vinyloge Carbonsäure aufgefasst werden, das heißt als eine Carbonsäure-Funktion mit „eingeschobener“ Kohlenstoff-Kohlenstoff-Doppelbindung zwischen Carbonylgruppe und Hydroxygruppe. Die Endiol-Struktur bedingt die reduzierenden (antioxidativen) Eigenschaften der Ascorbinsäure, da Endiole leicht zu Diketonen oxidiert werden können. Endiole mit benachbarter Carbonylgruppe heißen daher auch "Reduktone."
Die andere enolische Hydroxygruppe hat nur schwach saure Eigenschaften (pK = 11,79), da hier das Anion weniger mesomere Grenzstrukturen zur Stabilisierung ausbilden kann. Nach Abgabe beider Protonen entsteht aus Ascorbinsäure ein Dianion (Asc). Die intermediäre Form, die durch Abgabe eines Elektrons und eines Protons entsteht (AscH), ist eine sehr starke Säure (pK = −0,45). Sie hat wegen ihrer Kurzlebigkeit im Metabolismus keine Bedeutung.

Das Säurerest-Ion der Ascorbinsäure nennt sich Ascorbat. Es entsteht durch Übertragung eines Wasserstoffions (H, Proton) auf ein protonierbares Lösungsmittel, etwa Wasser. Deswegen lautet seine Summenformel CHO. Die Reaktion ist eine Gleichgewichtsreaktion:

Ascorbinsäure ist in wässrigen Lösungen ein starkes Reduktionsmittel. Hierbei kann es über Zwischenstufen zu Dehydroascorbinsäure (DHA) oxidiert werden. Dieser Prozess ist reversibel, so können beispielsweise Cystein, Dithiothreitol oder andere Thiole DHA zurück zu Ascorbinsäure reduzieren. In der Reduktions- und Oxidationswirkung liegt eine wichtige Eigenschaft von Vitamin C in biologischen Systemen.

In kristalliner Form ist Ascorbinsäure relativ stabil gegenüber Oxidation durch Luftsauerstoff. In wässriger Lösung geschieht die Oxidation wesentlich rascher, wobei eine Temperaturerhöhung, eine Erhöhung des pH-Wertes sowie die Anwesenheit von Schwermetallionen diese beschleunigen. Säuren wie Citronensäure, Oxalsäure oder Metaphosphorsäure sowie Komplexbildner wie 8-Hydroxychinolin wirken stabilisierend. Bei der Zubereitung von Nahrungsmitteln durch Kochen werden durchschnittlich 30 % der enthaltenen Ascorbinsäure oxidiert.

-Dehydroascorbinsäure (englisch "dehydro ascorbic acid," DHA) entsteht durch Oxidation von Ascorbinsäure. Im menschlichen Metabolismus kann sie zu -Ascorbinsäure reduziert werden und damit Vitamin C regenerieren. Dehydroascorbinsäure liegt in wässrigen Lösungen nahezu vollständig als Monohydrat (mono-DHA·HO) vor. Dabei bildet es einen Bizyklus, was durch Kernspinresonanzspektroskopie nachgewiesen wurde. Möglicherweise kann es noch ein zweites Molekül Wasser aufnehmen, um dann ein Dihydrat auszubilden. Auch Semi-Dehydroascorbinsäure sowie oxidierte Formen veresterter Ascorbinsäuren werden zur Gruppe der Dehydroascorbinsäure gezählt.

Generell wird Vitamin C in Form von DHA durch Glucosetransporter, hauptsächlich GLUT-1, in die Mitochondrien der Zellen transportiert, da nur sehr wenige Zellen über spezifische Vitamin-C-Transporter verfügen. Hierbei sind die meisten dieser Transporter Natriumionen-abhängig. Insbesondere das Gehirn ist auf eine Versorgung mit Ascorbinsäure angewiesen, das Vitamin kann jedoch nicht die Blut-Hirn-Schranke passieren. Dieses Problem wird dadurch umgangen, dass Dehydroascorbinsäure durch Glucosetransporter, zum Beispiel "GLUT1," durch die Schranke transportiert und in den Gehirnzellen zu Ascorbinsäure reduziert wird.

Es wird davon ausgegangen, dass Ascorbinsäure in Form von DHA intrazellulär transportiert wird. Hierbei soll extrazelluläre Ascorbinsäure zu DHA oxidiert, in die Zelle aufgenommen und dann wieder reduziert werden, da Ascorbinsäure selbst die Zelle nicht verlassen kann. DHA ist instabiler als -Ascorbinsäure. Je nach Reaktionsbedingungen (pH-Wert, An- beziehungsweise Abwesenheit von Reduktionsmitteln wie Glutathion) kann es entweder wieder zurück in Ascorbinsäure umgewandelt werden oder zu Diketogulonsäure (DKG) irreversibel hydrolysieren.

Ascorbinsäure findet hauptsächlich als Antioxidans Verwendung. Sie wird vielen Lebensmittelprodukten als Konservierungsmittel beziehungsweise Umrötungshilfsmittel, zum Beispiel bei der Herstellung von Brühwürsten, unter der Nummer E 300 zugesetzt. Weitere E-Nummern von Ascorbinsäurederivaten sind E 301 (Natriumascorbat), E 302 (Calciumascorbat), E 304a (Ascorbylpalmitat) und E 304b (Ascorbylstearat). Naturtrüber Apfelsaft kann bei der Herstellung mit Ascorbinsäure versetzt werden und wird dadurch deutlich heller, weil im natürlichen Saft vorhandene Chinone, die bei der Pressung durch Oxidation von Phenolen mit Luftsauerstoff und dem Enzym Polyphenoloxidase entstehen und eine braune Farbe bewirken, wieder reduziert werden. Ascorbylpalmitat wird zur Verhinderung der Autooxidation von Fetten eingesetzt und verhindert so, dass diese ranzig werden. Der Ascorbinsäurezusatz zu Mehlen als Mehlbehandlungsmittel soll das Gashaltevermögen und das Volumen der Teige vergrößern. Dies lässt sich durch die Ausbildung zusätzlicher Disulfidbrücken zwischen den Kleber-Strängen des Teiges erklären.
Auch im Pharma-Bereich dient Ascorbinsäure als Antioxidans zur Stabilisierung von Pharmaprodukten.

In der Küche wird Ascorbinsäure (in Rezepten meist als „Vitamin-C-Pulver“ bezeichnet) eingesetzt, damit geschnittenes Obst (meist Äpfel und Bananen) länger frisch bleibt und nicht braun wird.

Wegen ihrer reduzierenden Eigenschaft wird Ascorbinsäure vereinzelt als Entwicklungssubstanz in photographischen Entwicklern eingesetzt.

Zum Auflösen von Heroinbase vor der Injektion wird oft Ascorbinsäure mit dem Heroin aufgekocht.

Vitamin C ist ein Radikalfänger und hat eine antioxidative Wirkung (es wirkt also als Reduktionsmittel).

Weiterhin stellt Vitamin C ein wichtiges Coenzym für die Prolyl-4-Hydroxylase dar. Dieses Enzym wird bei der Biosynthese des Proteins (Eiweißes) Kollagen benötigt. Es wandelt integrierte Prolinreste in 4-Hydroxyprolyl-Seitenketten unter Verbrauch von molekularem Sauerstoff um. Hydroxyprolin ist für den stabilen Kollagenaufbau unerlässlich.

Ebenfalls innerhalb der Biosynthese von Kollagen, aber auch weiterer Proteine, findet mithilfe von Ascorbinsäure und des Enzyms Lysylhydroxylase die Hydroxylierung von -Lysin zum Hydroxylysin statt. Im Kollagen erfüllt dieses eine Funktion in der kovalenten Quervernetzung benachbarter Moleküle. Darüber hinaus kann Hydroxylysin im Kollagen und weiteren Proteinen glykosyliert werden, was zur Bildung von Glykoproteinen führt.

Mangel an Vitamin C führt zu einer verminderten Aktivität der Prolyl-Hydroxylierung und der Lysyl-Hydroxylierung und damit zur Instabilität von Kollagen. Da Kollagen in praktisch allen Organen und Geweben des menschlichen und tierischen Organismus vorkommt, vor allem im Bindegewebe, wird bei Mangel von Vitamin C Skorbut ausgelöst.

Bei der Hydroxylierung von Steroiden ist Vitamin C ein wichtiger Cofaktor. Darüber hinaus spielt es eine wichtige Rolle beim Aufbau von Aminosäuren wie beispielsweise dem -Tyrosin. Auch bei der Umwandlung von Dopamin zu Noradrenalin, im Cholesterin-Stoffwechsel (Ascorbinsäure spielt eine Rolle bei der Umwandlung von Cholesterol zu Gallensäure und senkt dadurch den Blut-Cholesterol-Gehalt), der Serotoninsynthese und bei der Carnitinbiosynthese wird Ascorbinsäure benötigt.

Mit Niacin und Vitamin B steuert Vitamin C die Produktion von -Carnitin, das für die Fettverbrennung in der Muskulatur benötigt wird. Weiterhin begünstigt es die Eisenresorption im Dünndarm.

Aufgrund der hohen Konzentration von Vitamin C im männlichen Sperma wird der Einfluss auf die Zeugungsfähigkeit derzeit untersucht. Vitamin-C-Gaben bei manchen unfruchtbaren Männern konnten vereinzelt die Spermienqualität erhöhen.

Die Stimulation der körpereigenen Abwehr, die dem Vitamin C oft zugeschrieben wird, wird unter anderem durch einen Schutz der Phagozytenmembran vor oxidativer Selbstzerstörung erklärt. Diese oxidative Selbstzerstörung kann sonst durch das bei der Phagozytose ausgelöste Halogenid-Peroxidase-System ausgelöst werden. Zudem wurde in Tierversuchen eine erhöhte Interferonproduktion sowie eine Aktivierung des Komplementsystems nach Gabe von Vitamin C beobachtet werden. Generell wurde bei Leukozyten im Blut, die einen wichtigen Stellenwert in der Immunabwehr einnehmen, ein hoher Ascorbinsäuregehalt festgestellt. Weiterhin scheint Vitamin C Einfluss auf zahlreiche weitere neutrophile Funktionen zu haben, wie die Chemotaxis, Aufnahme von Partikeln durch Phagozyten, Lysozym-beeinflusste nicht-oxidative Immunreaktion und die Stimulation des Hexose-Monophosphat-Shunts.

Der Stellenwert von Vitamin-C-Gaben zur Bekämpfung und Vorbeugung von Krankheiten wie der Erkältung ist wissenschaftlich allerdings umstritten, wobei größere Reviews einen generellen Trend sehen, dass während Vitamin C zwar keinen messbaren prophylaktischen Effekt bei saisonaler Erkältung hat, allerdings ein moderater positiver Effekt auf den Krankheitsverlauf beobachtet wurde. Dieser konnte in therapeutischen Studien allerdings nicht reproduziert werden. Es gibt auch Meta-Analysen, die zeigen, dass Nahrungsergänzungsmittel mit Vitamin C bei Erkältungen weder prophylaktisch helfen noch die Genesung beschleunigen können.

-Ascorbinsäure wirkt am Nicotinrezeptor des Typs α9α10 als positiver allosterischer Modulator. Hierdurch könnte es sich zur Akutbehandlung eines Schalltraumas empfehlen. Die wirksame Konzentration liegt bei 1–30 mM.

Der Bedarf an Vitamin C wird zum Teil sehr kontrovers gesehen. Die Zufuhrempfehlung für einen gesunden Erwachsenen beträgt laut Empfehlung der Deutschen Gesellschaft für Ernährung 100 mg/Tag. Die Meinungen hierüber gehen jedoch weit auseinander; die Empfehlungen anderer Gruppierungen liegen zwischen einem Bruchteil (zum Beispiel der Hälfte) und einem Vielfachen (zum Beispiel „so viel wie möglich“) dieses Wertes. Fest steht, dass Mengen bis zu 5000 mg kurzzeitig als unbedenklich gelten. Überschüssige Mengen werden vom Körper über den Urin ausgeschieden, da Vitamin C gut wasserlöslich ist (siehe auch Hypervitaminosen).

Bei einer ausgewogenen Mischkost kann in Deutschland davon ausgegangen werden, dass dem Körper alle lebensnotwendigen Vitamine, und daher auch Vitamin C, in ausreichendem Maße zugeführt werden. Die Versorgung mit Vitamin C ist in Deutschland knapp über der DGE-Empfehlung von 100 mg pro Tag. Daher sind Vitaminpräparate für einen gesunden Menschen, der sich abwechslungsreich und vollwertig ernährt, überflüssig. Die Empfehlung für Schwangere und Stillende liegt bei 110 beziehungsweise 150 mg täglich. Ursache für eine unzureichende Zufuhr ist meistens eine einseitige Ernährung. Dies betrifft vor allem Menschen, die nicht täglich frisches Obst und Gemüse verzehren.

Untersuchungen mit C-markiertem Vitamin C zeigen, dass der tägliche Ascorbatumsatz unabhängig von der Vitamin-C-Zufuhr nur etwa 20 mg beträgt. Somit genügen bereits knapp 20 mg täglich, um Skorbut zu vermeiden. Die Fachinformation des Bundesinstituts für Arzneimittel und Medizinprodukte (BfArM) gibt für Vitamin C einen täglichen Gesamtumsatz von etwa 1 mg/kg Körpergewicht an.

Für Vergleichszwecke interessant ist, dass für Meerschweinchen eine Tagesdosis von 10 bis 30 mg empfohlen wird (bei einem Gewicht von etwa 0,8 bis 1,5 kg), wobei sie diese ebenso wie der Mensch nicht selbst produzieren können. Im Gegensatz dazu produzieren viele Tiere selbst Vitamin C. Große Hunde oder kleine Kälber, die etwa das Körpergewicht eines Menschen haben, stellen 1 bis 2 g täglich her, bei Krankheit bis zu 10 g.

Studien zur Pharmakokinetik von Vitamin C zeigen, dass eine volle Sättigung der Körperreserven mit Vitamin C (3000 mg) eine tägliche Zufuhr von 200 mg erfordert. Immunzellen wie Lymphozyten, Neutrophile und Monozyten werden bereits bei einer täglichen Aufnahme von 100 mg Vitamin C gesättigt. Die vollständige Plasmasättigung wird bei Zufuhr von 1000 mg Vitamin C pro Tag erreicht. Die Bioverfügbarkeit nimmt bei oraler Einnahme mit steigender Einzeldosis stark ab. 200 mg werden noch nahezu vollständig aufgenommen. Aus diesem Grund ist es sinnvoller, mehrere Einzeldosen mit je 200 mg über den Tag verteilt zu sich zu nehmen, als einmalig 1000 mg.

Die Vitamin-C-Versorgung des Organismus spiegelt sich im Blutspiegel wider. Laut DGE sind geringere Konzentrationen als 20 µmol/l (0,35 mg/dl) mit vorklinischen Symptomen wie beispielsweise allgemeiner Müdigkeit, Leistungsschwäche, Infektanfälligkeit und schlechter Wundheilung verbunden. Offensichtliche klinische Mangelsymptome, die unter dem Begriff Skorbut zusammengefasst werden, treten erst bei Vitamin-C-Plasmaspiegeln unterhalb von 10 µmol/l (0,18 mg/dl) auf. Heute ist allgemein anerkannt, dass subklinische Vitamin-C-Defizite die Langzeitgesundheit negativ beeinflussen. Ein deutsches Konsensuspapier empfiehlt deshalb präventive Vitamin-C-Plasmaspiegel von mindestens 50 µmol/l (0,9 mg/dl) zur Verringerung des Arteriosklerose- und Krebsrisikos (DGE 2000). Die von der DGE empfohlene Vitamin-C-Tagesdosis von 100 mg bezieht sich ausschließlich auf Gesunde. Vitamin C ist eines der wichtigsten körpereigenen Antioxidantien. Ein Mehrbedarf bei Erkrankungen, die mit der Generierung von reaktiven Sauerstoffverbindungen (ROS) einhergehen, ist unbestritten. Er ist beim gegenwärtigen Stand der Erkenntnis nur noch nicht genau bezifferbar. Chronisch entzündliche Erkrankungen wie beispielsweise Arthritis, Allergien, Arteriosklerose, Krebs oder rezidivierende Infektionen sind nachweislich mit einem subklinischen bis klinischen Vitamin-C-Mangel (unter 30 µmol/l oder 0,54 mg/dl) und oxidativem Stress verbunden. Eine ständig zunehmende Anzahl epidemiologischer Studien zeigt den prophylaktischen Wert einer adäquaten diätetischen Vitamin-C-Aufnahme. Hier sind vor allem die Ergebnisse der EPIC-Studie zu nennen, die 2001 in der Zeitschrift „The Lancet“ publiziert wurden. Die Daten von fast 20.000 Männern und Frauen zeigten, dass eine Steigerung der Blutascorbatwerte um 20 μM (0,35 mg/dl) eine 20%ige Reduktion der Mortalität mit sich brachte.

Szent-Györgyi identifizierte 1933 das Vitamin C als wirksame Substanz gegen Skorbut.

Nur wenige Wirbeltiere, darunter Trockennasenprimaten (unter anderem der Mensch), Meerschweinchen und Echte Knochenfische, sowie einige Familien in den Ordnungen der Fledertiere und Sperlingsvögel, sind nicht zur Biosynthese von Ascorbinsäure aus Glucuronsäure befähigt. Ihnen fehlt das Enzym -Gulonolactonoxidase. Für diese Lebewesen ist Ascorbinsäure ein Vitamin, also essenziell. Für alle anderen Wirbeltiere ist Ascorbinsäure nur ein Metabolit. Lebewesen, die nicht in der Lage sind, Ascorbinsäure selbst zu synthetisieren, müssen diese in ausreichender Menge über die Nahrung aufnehmen, um nicht an Skorbut zu erkranken. In frisch gelegten Hühnereiern fehlt zwar die Ascorbinsäure, sie wird jedoch ab Brutbeginn hauptsächlich von der Membran des Dottersacks synthetisiert.

Studien, die den tatsächlichen Vitamin-C-Gehalt im Blut des Menschen bestimmen, beobachten häufiger als bislang angenommen eine Unterversorgung: Die NHANES-III-Untersuchung von 1988 bis 1994 stellte fest, dass 10 bis 14 % der untersuchten Amerikaner an einer ernsten Unterversorgung (unter 11 µmol/l) und 17–20 % an einer subklinischen (11–28 µmol/l) Unterversorgung leiden – insgesamt also mehr als ein Viertel der Bevölkerung. Die aktuelle NHANES-Erhebung für den Zeitraum 2003–2004 beobachtet eine erfreuliche Entwicklung: Eine ernste Unterversorgung betrifft nur noch 7,1 % der Bevölkerung. Einschneidend sind immer noch die Einkommensverhältnisse. Menschen mit niedrigem Einkommen leiden im Vergleich zu Gutverdienern doppelt so häufig an einer Unterversorgung (10–17 % versus 5–8 %). Zwei wesentliche Gründe für die insgesamt verbesserte Vitamin-C-Versorgung sind weniger Passivraucher, durch ein Rauchverbot in öffentlichen Einrichtungen und die zunehmende Einnahme von Vitaminpräparaten. Am subklinischen Mangel (unter 28 µmol/l) änderte sich kaum etwas – er trifft immer noch etwa 20 % der Amerikaner. Der sozioökonomische Einfluss auf eine gesundheitsbewusste Ernährung wird in einer schottischen Untersuchung deutlich: 44 % der Menschen mit niedrigem sozioökonomischen Status wiesen Vitamin-C-Blutspiegel unter 23 µmol/l und 20 % unter 11 µmol/l auf.
Aber Nichtrauchen und gute Schulbildung schützen nicht automatisch vor einer Unterversorgung. Eine kanadische Studie bestimmte in der Zeit von 2004 bis 2008 die Vitamin-C-Blutspiegel von knapp 1000 Nichtrauchern im Alter von 20 bis 29 Jahren an einer Campus-Universität. Jeder Dritte zeigte einen subklinischen Vitamin-C-Mangel (unter 28 µmol/l) und jeder Siebte defizitäre Werte unterhalb der Skorbutgrenze (unter 11 µmol/l). Dabei korrelierte der Mangel mit Übergewicht, Bluthochdruck und Entzündungsparametern.

Für Vitamin C ist die Hypervitaminose, wie sie beispielsweise bei Vitamin A vorkommen kann, sehr selten, da der Körper einen Überschuss an Ascorbinsäure wieder über die Nieren ausscheidet.

In einer vom National Institutes of Health (NIH) durchgeführten Studie wurden sieben Freiwillige zunächst mit einer ascorbinsäurearmen Diät ernährt und so ihre körpereigenen Vorräte an Vitamin C aufgebraucht. Als diese danach wieder mit Vitamin C versorgt wurden, begann die renale (über die Niere) Ausscheidung an unverändertem Vitamin C ab etwa 100 mg/d. Die Zufuhr über 400 mg/d wurde – soweit überhaupt im Darm aufgenommen (die Einnahme von Megadosen senkt die Resorptionsquote deutlich) – praktisch vollständig renal ausgeschieden. Ab etwa 1 g pro Tag steigen die Oxalat- und die Harnsäure-Konzentrationen im Urin. Da ein Teil der Ascorbinsäure im Stoffwechsel zu Oxalsäure umgesetzt wird, besteht bei entsprechend disponierten Menschen prinzipiell ein erhöhtes Risiko für Calciumoxalat-Nierensteine (CaCO). Schon bei normaler Zufuhr stammen etwa 30 bis 50 % des Plasmaoxalats aus dem Vitamin-C-Abbau. Der Oxalatspiegel im Urin steigt selbst erst an, wenn eine Tagesdosis von etwa 6 g überschritten wird.

Hohe orale Einzeldosen können einen vorwiegend osmotisch bedingten Durchfall auslösen. Die Dosis variiert von Person zu Person, wird mit etwa 5–15 g (1–3 gehäufte Teelöffel) für eine gesunde Person angegeben. Zu erwähnen ist, dass diese Toleranzgrenze bei Individuen, die an schweren Erkrankungen leiden, bis auf über 200 g ansteigen kann.

Bei Menschen mit Glucose-6-Phosphatdehydrogenase-Mangel (G6PD-Mangel, Favismus), einer insbesondere in Afrika sehr weit verbreiteten, erblichen Krankheit, können intravenöse Vitamin-C-Dosen, etwa 30 bis 100 g pro Infusion, zur Hämolyse führen.

Häufig wird Vitamin C, besonders wenn auf nüchternen Magen konsumiert, mit Verdauungsstörungen durch Übersäuerung des Magens in Verbindung gebracht. Dies kann unter anderem vermieden werden, indem Vitamin C nicht als Ascorbinsäure, sondern als Ascorbat (Salz der Ascorbinsäure, zum Beispiel Natriumascorbat) aufgenommen wird. Dies kann zum Beispiel durch die Zugabe von Backpulver (NaHCO) erreicht werden. Studien haben gezeigt, dass die Resorption von Vitamin C erhöht wird, wenn es zu Fruchtsäften wie zum Beispiel Orangensaft gemischt wird.

Bei der Ratte liegt der LD-Wert (die Dosis, bei der die Hälfte der Versuchstiere sterben) für Vitamin C bei 11,9 g pro Kilogramm Körpergewicht. Das entspricht bei einem 70 kg schweren Menschen einer Dosis von 833 g.

Therapeutisch und prophylaktisch eingesetzt wird die Überdosierung von Vitamin C zum Beispiel bei Harnwegsinfektionen. Durch die renale Ausscheidung der Ascorbinsäure wird der Urin sauer. In diesem sauren Milieu können die Erreger deutlich schlechter gedeihen. Eine regelmäßige Einnahme von Vitamin C erhöht jedoch die Bildung von Nierensteinen, zumindest ist das Risiko bei den untersuchten Männern doppelt so hoch.

Ascorbinsäure kann vom Menschen, Affen und einigen anderen Tierarten nur mit der Nahrung aufgenommen werden. Im Stoffwechsel der meisten anderen Lebewesen kann sie hingegen auch bedarfsabhängig synthetisiert werden.

Der Transportweg von Vitamin C erfolgt über Enterozytzellen des Darmes. Wie es von dort in den Blutstrom gelangt, ist noch nicht vollständig geklärt. Jedoch ist der Transport von Ascorbat beziehungsweise Dehydroascorbat (DHA) vom Blut in alle anderen Zellen genauer bekannt.

Die Aufnahme von Dehydroascorbat (DHA, vergleiche Abschnitt oben) in das Zellinnere (Zytosol) menschlicher Zellen findet mittels dreier Glucosetransporter statt, GLUT-1, GLUT-3 und GLUT-4. DHA konkurriert dabei mit Glucose, sodass ein Übermaß an Glucose effektiv die Aufnahme von DHA verhindern kann. Was Ascorbat anbetrifft, wird es zusammen mit je zwei Natriumionen mittels der Transportproteine SVCT1 und SVCT2 ins Zellinnere geschleust.

Ascorbinsäure wird von Bakterien, Pflanzen und Wirbeltieren mithilfe verschiedener Enzyme produziert. Ausgangssubstanzen sind hauptsächlich -Glucose beziehungsweise -Galactose.

Bei Pflanzen können neben -Glucose und -Galactose auch -Glucuronlacton, -Galacturonat beziehungsweise dessen Methylester die Biosynthese einleiten.

Bei Ratten wurde die Biosynthese am besten untersucht. Die Bildung der Ascorbinsäure beginnt mit der Oxidation von UDP--Glucose zu UDP-D-Glucuronsäure durch das Enzym UDP-Glucose-Dehydrogenase (). Oxidationsmittel ist dabei das NAD.

Trockennasenaffen, Meerschweinchen, Echten Knochenfischen sowie einigen Familien der Fledertiere und Sperlingsvögel fehlt das Enzym -Gulonolactonoxidase (B in obiger Abbildung) aufgrund eines genetischen Defekts, sodass sie Ascorbinsäure nicht synthetisieren können. Die genetische Mutation bei Trockennasenaffen trat vor etwa 65 Millionen Jahren auf. Diese Tiere waren seinerzeit in einer Gegend angesiedelt, die ganzjährig reich an Vitamin-C-haltigen Früchten war. Daher hatte dieser bei anderen Tieren letale Defekt keine negativen Auswirkungen. Auch einige Insekten wie die Wanderheuschrecken (Acrididae) können Ascorbinsäure nicht selbständig herstellen.

Eine wichtige Funktion der Ascorbinsäure im menschlichen Organismus beruht auf ihrer Eigenschaft als Reduktionsmittel. Sie ist also in der Lage, Elektronen auf andere Moleküle zu übertragen.

Zwei grundsätzliche Aufgaben können unterschieden werden:

Ascorbinsäure dient im tierischen Organismus als Radikalfänger, da sie in der Lage ist, ebensolche auf andere Moleküle zu übertragen. Die Grafik zeigt nicht den tatsächlichen Reaktionsmechanismus, sondern schematisch die Fähigkeit der Ascorbinsäure, unter Reaktion zur Dehydroascorbinsäure zwei Radikale einfangen zu können (vgl. obige Abbildung).

Bei der Verstoffwechslung des Sauerstoffs in der Zelle kann es zur Bildung des Hyperoxidradikals O kommen, wenn der molekulare Sauerstoff O bei der Endreaktion der Atmungskette statt vier Elektronen nur eines erhalten hat. Das Hyperoxidradikal ist aufgrund dieses Elektronenmangels extrem reaktiv und in der Lage, molekulare Zellstrukturen zu schädigen. Die Reaktion mit Ascorbinsäure überführt dieses in Wasserstoffperoxid:

Das Wasserstoffperoxid wird von dem Enzym Katalase abgebaut.

Sowohl Ascorbinsäure als auch dessen oxidierte Form (DHA) sind Cofaktoren für viele biochemische Reaktionen. Hierbei stellt Ascorbinsäure Elektronen für Kupfer(I)-abhängige Monooxygenasen beziehungsweise Eisen(III)-abhängige Dioxygenasen bereit. "In vitro" können auch andere Redoxfaktoren diese enzymatischen Reaktionen katalysieren.

Von Bedeutung ist diese Redoxeigenschaft der Ascorbinsäure beispielsweise bei der Synthese von Collagen im menschlichen Stoffwechsel. Zur Darstellung dieses Strukturproteins muss die Aminosäure -Prolin zu ihrer oxidierten Form, Hydroxyprolin, umgewandelt werden. Ascorbinsäure dient dazu, das in dieser Reaktion genutzte Reduktionsmittel Fe(II) zu regenerieren. Besteht ein Mangel an Vitamin C, kann die Bildung des Hydroxyprolins bei der Collagensynthese nur begrenzt erfolgen, sodass die typischen Symptome des Skorbuts wie Zahnfleischbluten, Zahnausfall und Hautschäden auftreten.

Die nach Oxidation entstehenden Produkte Semidehydroascorbinsäure und Dehydroascorbinsäure werden enzymatisch wieder zu Ascorbinsäure reduziert. Die Enzyme Cytochrom b-Reduktase und Thioredoxinreduktase katalysieren die Umwandlung von Semidehydroascorbinsäure zu Ascorbinsäure im Cytosol, unter Verbrauch von NADH beziehungsweise NADPH. Außerdem kann eine Reduktion über elektronentransferierende Membranproteine stattfinden. Dehydroascorbinsäure wird sowohl spontan mittels Glutathion oder NADPH reduziert als auch enzymatisch über die Glutathiontransferase Omega.

Der Abbau von Dehydroascorbinsäure wird bei Säugetieren durch Hydrolyse zur physiologisch inaktiven 2,3-Diketogulonsäure eingeleitet. Diese wird entweder zu Oxalat und -Threonsäure gespalten oder zu Kohlenstoffdioxid, Xylose, Xylulose decarboxyliert. Im Unterschied dazu haben Bakterien wie "E. coli" enzymatische Stoffwechselwege für den Abbau von Ascorbinsäure und wahrscheinlich auch für Dehydroascorbinsäure.

Um Ascorbinsäure quantitativ nachzuweisen, gibt es zahlreiche colorimetrische Methoden, etwa unter Verwendung von 2,4-Dinitrophenylhydrazin. Ascorbinsäure reagiert mit diesem zu einem Hydrazon, dessen Absorption messbar ist. Darüber hinaus kann 2,2′-Bipyridin zum colorimetrischen Nachweis dienen. Hierbei wird die Reduktionskraft der Ascorbinsäure genutzt, die Fe(III) zu Fe(II) reduziert. Fe(II) bildet dann mit 2,2′-Bipyridin einen farbigen Komplex. Es sind auch einige fluorometrische Nachweismethoden bekannt.

Ascorbinsäure lässt sich auch durch Titration mit Tillmans-Reagenz (2,6-Dichlorphenolindophenol, abgekürzt DCPIP) nachweisen, bei der das Reagenz durch die Ascorbinsäure zu einer Leukoverbindung reduziert wird. Dabei ist ein Farbumschlag von tiefblau zu farblos zu beobachten. Diese Methode eignet sich für eine schnelle Bestimmung, die aber an die Genauigkeit oben genannter Wege nicht heranreicht.

Ascorbinsäure kann auch spezifisch mittels Oxidation durch das Enzym Ascorbinsäure-Oxidase nachgewiesen werden, wobei die Änderung der Lichtabsorption bei einer Wellenlänge von 245 nm gemessen wird.

Die Gehaltsbestimmung wird im Europäischen Arzneibuch durch redoximetrische Titration mit 0,05-molarer Iodlösung unter Zusatz von Stärke durchgeführt (Iodometrie). Dabei verbraucht ein Mol Ascorbinsäure ein Mol Iod, das zu farblosem Iodid umgesetzt wird. Die Färbung durch den blauen Iod-Stärke-Komplex dient der Endpunktbestimmung. Da die zugesetzte Stärkelösung die Reaktion verzögert und einen schleppenden Umschlag verursacht, bietet sich eine Indikation mit Variamin an. Der maßanalytische Faktor beträgt 8,8065 mg Ascorbinsäure / ml 0,05 M Iodlösung.




</doc>
<doc id="296" url="https://de.wikipedia.org/wiki?curid=296" title="Anomie">
Anomie

Anomie (griech.: Kompositum aus α privativum zur Verneinung und der Endung "-nomie" für , „Ordnung, Gesetz“) bezeichnet in der Soziologie einen Zustand fehlender oder schwacher sozialer Normen, Regeln und Ordnung. Vor allem in England war der Begriff ursprünglich ein theologischer Ausdruck für das Brechen religiöser Gesetze. Zur Beschreibung einer Anomie wird umgangssprachlich und irreführend häufig auch das Wort Anarchie (Abwesenheit von Herrschaft) benutzt.

Der Begriff der Anomie wurde von Émile Durkheim (1858–1917), der ihn den Schriften des Philosophen Jean-Marie Guyau entlehnt hatte, in die Soziologie eingeführt. Der Rückgang von religiösen Normen und Werten führt nach Durkheim unweigerlich zu Störungen und zur Verringerung sozialer Ordnung. Aufgrund von Gesetz- und Regellosigkeit sei dann die gesellschaftliche Integration nicht länger gewährleistet. Diesen Zustand nannte Durkheim "anomie", die beim Individuum zu Angst und Unzufriedenheit führen müsse, ja sogar zur Selbsttötung führen könne („anomischer Suizid“). Durkheim benutzte den Begriff, um die pathologischen Auswirkungen der sich im Frühindustrialismus rasch entwickelnden Sozial- und Arbeitsteilung zu beschreiben. Die damit einhergehende Schwächung der Normen und Regeln für die Allokation von Waren führe zu einem verschärften Wettbewerb um die steigenden Prosperitätsgewinne.

Robert K. Merton hat den Begriff verfeinert, indem er die Regeln näher beschreibt, deren Fehlen zu Anomie führt:


Kulturelle Struktur:

Soziale Struktur:

Als Anomie wird nunmehr eine Dissoziation zwischen kulturellen Zielen und dem Zugang bestimmter sozialer Schichten zu dazu notwendigen Mitteln beschrieben. Dadurch schwächt sich die Bindung zwischen Mitteln und Zielen. 

Merton nennt fünf mögliche Reaktionsmuster des Menschen auf diese Dissoziation:

Gegenwärtig führe vor allem die Relativierung kultureller Mittel durch Pluralisierung und Individualisierung zu Problemen wie Orientierungslosigkeit, Verhaltensunsicherheit und gesellschaftlicher Desintegration.




</doc>
<doc id="299" url="https://de.wikipedia.org/wiki?curid=299" title="Alexandre Dumas der Jüngere">
Alexandre Dumas der Jüngere

Alexandre Dumas der Jüngere, auch "Dumas fils", (* 27. Juli 1824 in Paris; † 27. November 1895 in Marly-le-Roi) war ein französischer Romanschriftsteller und dramatischer Dichter. Er war der uneheliche Sohn von Alexandre Dumas dem Älteren und Marie-Catherine Labay, einer Näherin.

Dumas schlug 17-jährig, nachdem er das Collège Bourbon verlassen hatte, die schriftstellerische Laufbahn mit dem Gedichtband "Péchés de jeunesse" („Jugendsünden“) ein. Er begleitete seinen Vater auf dessen Reise durch Spanien und Nordafrika und veröffentlichte nach seiner Rückkehr den sechsbändigen Roman "Histoire de quatre femmes et d'un perroquet" (1847), der die Neugierde des Publikums erregte.

In dem Roman "Die Kameliendame" ("La dame aux camélias", 1848) erzählt Dumas realitätsnah die Geschichte einer Pariser Kurtisane, die früh an der Schwindsucht stirbt. 
In den beiden späteren Stücken "Diane de Lys" (1853) und "Le demi-monde" (1855) behandelt der Dichter fast dasselbe Thema, doch in wesentlich satirischerer Absicht und mehr, um nach Art des Komödiendichters seiner Zeit einen Spiegel vorzuhalten.

Dumas gilt als einer der Begründer des Gesellschaftsdramas und er setzte sich in fast allen seinen Stücken mit sozialen und gesellschaftlichen Problemen auseinander. Die Stellung der Frau nahm dabei eine besondere Rolle ein. So beschäftigte er sich mit den Rechten und Pflichten der Frau und den Fehlern der einschlägigen Gesetzgebung und gesellschaftlichen Anschauung im Roman "L'affaire Clémenceau" (1864) sowie in mehreren Flugschriften wie "Lettres sur les choses du jour", "L'homme-femme", "Tue-la!", "Les femmes qui tuent et les femmes qui votent" (1872–80) und in der größeren Streitschrift "Le divorce" (1880).

Im Jahr 1875 wurde Dumas in die Académie française aufgenommen, 1894 wurde er Mitglied der Ehrenlegion.

Der als anspruchslos und hilfsbereit für seine Freunde geltende Dumas erfreute sich persönlich allgemeiner Beliebtheit. 1864 heiratete er Nadeschda Naryschkina von Knorring (1826–1895), mit der er zwei Töchter hatte. Nach Naryschkinas Tod 1895 heiratete er Henriette Régnier de La Brière und starb im selben Jahr am 27. November in Marly-le-Roi.

Dumas’ bekanntestes Werk ist der Roman "Die Kameliendame" ("La dame aux camélias") von 1848, der das Schicksal einer Pariser Kurtisane und ihres Verehrers schildert. Trotz Schwierigkeiten mit der Zensur war der Roman ein außergewöhnlicher Erfolg. Nach Umarbeitung des Werks zu einem Bühnenstück wuchs seine Popularität noch: Das 1852 im Théâtre du Vaudeville erstmals aufgeführte Werk erlebte ohne Unterbrechung mehr als 100 Aufführungen. 1853 übernahm Giuseppe Verdi das Thema für seine Oper "La traviata".

Die französische Schauspielerin Sarah Bernhardt spielte ab 1880 die "Kameliendame" in dem Bühnenstück und feierte damit in Europa und den USA große Erfolge. Das Stück zeichnete sich durch überaus scharfe Beobachtung der gesellschaftlichen Zustände, sichere Behandlung der dramatischen Form und einen lebendigen, prickelnden Dialog aus; aber nach damaliger Auffassung war die Verherrlichung und Rehabilitierung des Lasters moralisch bedenklich. 

Im Jahr 1911 wurde Dumas' "Kameliendame" mit Sarah Bernhardt in der Hauptrolle erstmals verfilmt. In einer weiteren Filmversion von Regisseur George Cukor spielte Greta Garbo 1937 die Hauptrolle.





</doc>
<doc id="300" url="https://de.wikipedia.org/wiki?curid=300" title="Dumas">
Dumas

Dumas ist der Familienname folgender Personen:




Orte in den Vereinigten Staaten:
Sonstiges:


</doc>
<doc id="301" url="https://de.wikipedia.org/wiki?curid=301" title="Auge">
Auge

Das Auge (, ) ist ein Sinnesorgan zur Wahrnehmung von Lichtreizen. Es ist Teil des visuellen Systems und ermöglicht Tieren das Sehen. Die Aufnahme der Reize geschieht mit Hilfe von Fotorezeptoren, lichtempfindlichen Nervenzellen, deren Erregungszustand durch die unterschiedlichen Wellenlängen elektromagnetischer Strahlung aus dem sichtbaren Spektrum verändert wird. Bei Wirbeltieren gelangen diese Nervenimpulse über die Sehnervenbahnen zum Sehzentrum des Gehirns, wo sie schließlich zu einer optischen Wahrnehmung verarbeitet werden. 

Die Augen von Tieren unterscheiden sich in Aufbau und Funktionalität teilweise erheblich. Ihr biologischer Stellenwert und damit ihre Leistungsfähigkeit ist eng an die Anforderungen des jeweiligen Organismus angepasst. Auch die Anzahl der Augen ist ein evolutionäres Ergebnis der Lebensumstände. Manche Tiere, deren Orientierung weniger von visuellen Eindrücken bestimmt wird, benötigen lediglich eine grobe Unterscheidung von Hell und Dunkel, andere wiederum von Kontrast- und Bewegungsmustern. Höher entwickelte Augen dienen der kontrastreichen Bildwahrnehmung, deren Qualität mit der Fähigkeit steigt, Helligkeitsunterschiede sehr differenziert wahrzunehmen ("Minimum visibile"). Dies drückt sich wiederum in einer entsprechenden Sehschärfe ("Minimum separabile") aus, die bei Tag, Dämmerung oder Nacht sehr unterschiedlich sein kann. Wieder andere benötigen weniger ein kontrastreiches Sehen als vielmehr ein großes Gesichtsfeld oder eine differenzierte Farbwahrnehmung in verschiedenen Wellenlängenbereichen. 

Die Leistungsfähigkeit des Sehsinns und die Komplexität des anatomischen Aufbaus und der übergeordneten Bereiche zur Bilderzeugung und -verarbeitung steigt mit den Anforderungen der jeweiligen Lebensformen an die Qualität der visuellen Orientierung.

Das gemeingerm. Wort mhd. "ouge", ahd. "ouga" beruht auf der idg. Wurzel "ok-" „sehen; Auge“ (vgl. gleichbedeutend lateinisch "oculus").

Es gibt Schätzungen, dass Augen der verschiedensten Bauweisen im Laufe der Evolution etwa 40 mal neu entwickelt worden seien. Dennoch spielt das Pax-6-Gen sowohl bei den Tintenfischen als auch bei Säugetieren (Mäuse) sowie Insekten eine initiative Rolle bei der frühen Entwicklung der Augen. Bei der Fruchtfliege ("Drosophila melanogaster") hat das hierzu homologe Gen "eyeless" dieselbe Funktion. Deshalb liegt es nahe, dass all diese Augentypen einen gemeinsamen Ursprung haben. Orthologe von PAX-6 sind in vielen Chordatieren (stammesgeschichtlicher Ursprung im Präkambrium) zu finden. Fossilfunde belegen auch, dass es frühe Augen bereits vor 505 Millionen Jahren im Erdzeitalter Kambrium gab (z. B. das Lochkamera-Auge der Perlboote). Die ersten Linsen hatten Trilobiten in Facettenaugen vor 520 bis 500 Millionen Jahren.

Als Resultat einer visuellen Reizverarbeitung sind die Eigenschaften "Richtungssehen", "Sehschärfe", "Gesichtsfeld", "Farbsehen", "Formsehen" und "Bewegungssehen" zu nennen. Die Anforderungen der jeweiligen Lebensformen an diese Eigenschaften sind sehr unterschiedlich ausgeprägt. Zudem sind viele Spezies in der Lage, ihre Augen mit unterschiedlicher Präzision an verschiedene Objektentfernungen anzupassen (Akkommodation).

Manche Augentypen sind auf Grund ihrer anatomischen und physiologischen Entwicklung lediglich in der Lage, die Richtung auszumachen, aus der Licht auf ihre Sinneszellen fällt. Diese Eigenschaft lässt eine nur geringe visuelle Orientierung zu, stellt jedoch gegenüber der bloßen Wahrnehmung von Hell und Dunkel eine höhere Differenzierungsmöglichkeit dar.

Mit "Sehschärfe" wird die Fähigkeit eines Lebewesens bezeichnet, Konturen und Muster in der Außenwelt als solche zu erkennen. Ihre Qualität ist abhängig von: 


Zur Quantifizierung hat man verschiedene Parameter definiert. Die Winkel-Sehschärfe (angulare Sehschärfe) ist das Auflösungsvermögen, bei dem zwei Sehobjekte noch als getrennt wahrgenommen werden ("Minimum separabile"). Die Auflösung von 1' (einer Bogenminute) entspricht einer Ortsauflösung von etwa 1,5 mm bei 5 m Abstand. Je kleiner die Winkel-Sehschärfe ist, desto besser ist die Sehschärfe. Die dimensionslose Eigenschaft Visus wird definiert, indem die Bezugsgröße 1' in Beziehung zur individuellen Winkel-Sehschärfe gesetzt wird.

Visus = 1' / (individuelle Winkel-Sehschärfe)

Je größer der Visus ist, desto besser ist die Sehschärfe. Beispiel: wenn eine Person Punkte erst bei einem Winkelabstand von 2' trennen kann, hat sie einen Visus von 0,5. Statt Winkel können auch Entfernungen bestimmt werden. Wenn man als Bezugsgröße den Abstand d wählt, bei dem man zwei Punkte unter einem Winkel von 1' sieht, dann ist:

Visus = individueller Abstand / d

Beispiel: wenn eine Person erst im Abstand von 6 m die Punkte getrennt sehen kann, die bei 12 m einen Winkelabstand von 1' haben, hat sie einen Visus von 6/12 = 0,5.

Mit "Gesichtsfeld" bezeichnet man den Bereich des Außenraums, der bei ruhiger, gerader Kopfhaltung und geradeaus gerichtetem, bewegungslosem Blick mit unterschiedlicher Sensibilität visuell wahrgenommen werden kann. Man unterscheidet das monokulare Gesichtsfeld jeweils eines Auges von der Summe der Gesichtsfelder aller Augen eines Lebewesens. Sein Ausmaß wird in der Regel in der Einheit "Sehwinkelgrad" angegeben und unterscheidet sich je nach Lebewesen teils sehr deutlich. Beispiele des Ausmaßes eines horizontalen Gesichtsfeldes:


Die Farbwahrnehmung ist die Fähigkeit, elektromagnetische Wellen verschiedener Wellenlängen in ihrer Intensität zu unterscheiden. Diese Fähigkeit ist im ganzen Tierreich verbreitet. Das Absorptionsspektrum der wahrgenommenen und unterscheidbaren Wellenlängen charakterisiert artspezifisch die Qualität dieser Fähigkeit. Dazu muss das Wahrnehmungssystem mindestens zwei unterschiedliche Typen von "Lichtrezeptoren" besitzen, um die Zusammensetzungen des Lichts erkennen zu können.

Die einfachsten „Augen“ sind lichtempfindliche Sinneszellen auf der Außenhaut, die als passive optische Systeme funktionieren. Sie können nur erkennen, ob die Umgebung hell oder dunkel ist. Man spricht hier von Hautlichtsinn.

Insekten und andere Gliederfüßer haben Augen, die aus vielen einzelnen Augen zusammengesetzt sind. Diese Facettenaugen liefern ein rasterartiges Bild (nicht mehrfache Bilder, wie man vermuten könnte).

Neben den beschriebenen Augentypen mit lichtbrechenden Linsen findet man in der Natur gelegentlich auch Spiegelaugen. In den Augen der Kammmuschel "(Pecten)" wird das Bild durch Hohlspiegel erzeugt, die hinter der Netzhaut angeordnet sind. Die direkt vor der Netzhaut liegende Linse dient der optischen Korrektur des stark verzerrten Spiegelbildes. Die Spiegel sind nach dem Prinzip von reflektierenden Glasplatten gebaut. Mehr als 30 Schichten aus feinsten Guanin-Kristallen liegen dicht gestapelt, jede Schicht in eine Doppelmembran eingeschlossen. Auch andere Tiere haben Spiegelaugen, unter anderem der Tiefseekrebs "Gigantocypris", der Hummer und die Langusten. Diese Form hat sich offenbar dort durchgesetzt, wo es weniger auf die Bildqualität und mehr auf die Lichtausbeute ankommt.

Manche Lebewesen wie der Regenwurm besitzen am Körperende oder verstreut einzelne Lichtsinneszellen. Deren Lage relativ zum lichtabsorbierenden Körper des Wurms bestimmt die Richtungen des Lichteinfalls, für die diese Sinneszellen jeweils empfindlich sind. Dieses Prinzip ist bereits beim Einzeller Euglena verwirklicht: Der Photorezeptor liegt hier an der Basis der Geißel und wird durch einen pigmentierten Augenfleck einseitig beschattet. Das ermöglicht es der Zelle, sich zum Licht hin zu bewegen (Phototaxis).

Quallen und Seesterne besitzen viele nebeneinander liegende Lichtsinneszellen, die innen an eine Schicht aus Pigmentzellen anschließen können. Die Konzentrierung der Sinneszellen in solchen Flachaugen verbessert die Hell-Dunkel-Wahrnehmung.

In Pigmentbecheraugen liegen die Sehzellen vom Licht abgewandt (inverse Lage) in einem Becher aus lichtundurchlässigen Pigmentzellen. Das Licht kann nur durch die Öffnung des Bechers eindringen, um die Sehzellen zu stimulieren. Da daher immer nur ein kleiner Teil der Sehzellen gereizt wird, kann neben der Helligkeit auch die Einfallsrichtung des Lichts bestimmt werden. Solche Augen besitzen unter anderem Strudelwürmer und Schnecken.

Das Grubenauge unterscheidet sich vom Pigmentbecherauge durch die dem Licht zugewandte (everse) Lage der Sinneszellen und dadurch, dass die Grube mit Sekret gefüllt ist. In der Grube bilden die Sehzellen eine Zellschicht, die innen an eine Schicht von Pigmentzellen anschließt. Es ist also eine Weiterentwicklung des Flachauges. Es ermöglicht auch die Bestimmung der Intensität und der Einfallsrichtung des Lichts.

Lochaugen oder Lochkameraaugen sind weiterentwickelte Grubenaugen und funktionieren nach dem Prinzip der Lochkamera. Aus der Grube wird eine blasenförmige Einstülpung, die Öffnung verengt sich zu einem kleinen Loch und der Hohlraum ist vollständig mit Sekret gefüllt. Durch die erhöhte Anzahl der Sehzellen in einem Sehzellenepithel (Netzhaut) ist nun auch Bildsehen möglich. Das Bild ist jedoch lichtschwach, klein und steht wie bei einer Camera obscura auf dem Kopf. Die Schärfe des Bildes auf der Netzhaut hängt von der Anzahl der erregten Sehzellen ab. Da diese auch von der Entfernung vom Sehloch zum Gegenstand abhängt, ist beim Lochauge ein eingeschränktes Entfernungssehen möglich. Dieser Augentyp kommt rezent bei urtümlichen Kopffüßern wie den Perlbooten vor. Ein Lochauge mit verbesserter Leistung ist das Blasenauge, bei dem die Öffnung von einer durchsichtigen Haut bedeckt ist. Das Blasenauge entsteht aus einer Einstülpung der Epidermis, die mit einem Pigmentepithel und einer Sehzellenschicht ausgekleidet ist. Es kommt bei Hohltieren, Schnecken und Ringelwürmern vor. Je nach Durchmesser der Sehöffnung entsteht entweder ein helleres aber unschärferes oder ein dunkleres aber schärferes Bild.

Facettenaugen setzen sich aus einer Vielzahl von Einzelaugen (Ommatidien) zusammen, von denen jedes acht Sinneszellen enthält. Jedes Einzelauge sieht nur einen winzigen Ausschnitt der Umgebung, das Gesamtbild ist ein Mosaik aus allen Einzelbildern. Die Anzahl der Einzelaugen kann zwischen einigen Hundert bis hin zu einigen Zehntausend liegen. Die Auflösung des Facettenauges ist durch die Anzahl der Einzelaugen begrenzt und ist daher weit geringer als die Auflösung des Linsenauges. Allerdings kann die zeitliche Auflösung bei Facettenaugen deutlich höher sein als bei Linsenaugen. Sie liegt etwa bei fliegenden Insekten bei 250 Bildern pro Sekunde (also 250 Hz), was etwa dem vierfachen des menschlichen Auges mit 60 bis 65 Hz entspricht. Dies verleiht ihnen eine extrem hohe Reaktionsgeschwindigkeit. Die Farbempfindlichkeit des Facettenauges ist in den ultravioletten Bereich verschoben. Außerdem verfügen Spezies mit Facettenaugen über das größte Blickfeld aller bekannten Lebewesen. Zu finden sind diese Augen bei Krebsen und Insekten.

Zusätzlich besitzen viele Gliederfüßer Ocellen, kleinere Augen, die sich häufig auf der Stirnmitte befinden und sehr unterschiedlich aufgebaut sein können. Bei einfachen Ocellen handelt es sich um Grubenaugen. Besonders leistungsfähige Ocellen besitzen eine Linse oder, wie bei den Spinnentieren, auch einen Glaskörper, es handelt sich also um kleine Linsenaugen.

Das einfachste Linsenauge hat noch nicht den komplizierten Aufbau, den man vom Wirbeltierauge kennt. Es besteht aus nicht viel mehr als Linse, Pigmentzellen und Retina. Ein Beispiel hierfür ist das Linsenauge der Würfelqualle "Carybdea marsupialis". Zudem schauen die Augen an den vier Sinneskörpern am Schirmrand der Qualle in den Schirm hinein. Dennoch kann sie damit gut genug sehen, um Rudern auszuweichen, an denen sie sich verletzen könnte.

Auch manche Ocellen der Gliederfüßer sind einfache Linsenaugen.

Obwohl sich die Augen von Wirbeltieren, Tintenfischen und Einzellern im Aufbau stark ähneln, haben sie diese sehr ähnliche Funktionsweise unabhängig voneinander entwickelt. Dies wird bei der Bildung des Auges beim Embryo sichtbar: Während sich das Auge bei Wirbeltieren durch eine Ausstülpung der Zellen entwickelt, die später das Gehirn bilden, entsteht das Auge der Weichtiere durch eine Einstülpung der äußeren Zellschicht, die später die Haut bildet.

Ein Krötenauge besitzt schon die meisten Teile, die auch das menschliche Auge hat, nur die Augenmuskeln fehlen. Deshalb kann eine Kröte, wenn sie selber ruhig sitzt, keine ruhenden Gegenstände sehen, da sie nicht zu aktiven Augenbewegungen fähig ist und das Bild auf der Netzhaut dadurch verblasst, wenn es unbewegt ist.

Bei den höchstentwickelten Linsenaugen sammelt ein mehrstufiger dioptrischer Apparat das Licht und wirft es auf die Netzhaut, die nun zwei Arten von Sinneszellen enthält, Stäbchen und Zapfen. Die Einstellung auf Nah- und Fernsicht wird durch eine elastische Linse ermöglicht, die von Zonulafasern gestreckt bzw. gestaucht wird. Die besten Linsenaugen findet man bei Wirbeltieren.

So ist zum Beispiel bei Greifvögeln die Fähigkeit entwickelt, Objekte in einem Bereich der Netzhaut stark vergrößert zu sehen, was insbesondere beim Kreisen in großer Höhe beim Lauern auf Beute vorteilhaft ist.

Nachttiere wie Katzen, Eulen und Rehe, aber auch Schafe realisieren durch eine retroreflektierende Schicht (meist grün oder blau) hinter der Netzhaut einen Zugewinn an Empfindlichkeit, was ihnen als Nachttieren (Räubern wie Beute) zugutekommt (Siehe hierzu: Tapetum lucidum).

Bei Katzen findet man zusätzlich eine sogenannte Schlitzblende, die beim Öffnungsverhältnis größere Unterschiede als Lochblenden erlaubt. Beim Tagsehen werden aber bei Schlitzblenden periphere Strahlbündel weniger als bei Lochblenden unterdrückt, so dass die Sehschärfe beim Tagsehen weniger optimal ist.

Im Verhältnis zur Körpergröße sind die Augen bei nachtaktiven Tieren deutlich größer als bei den tagaktiven.

Für die Leistungsfähigkeit eines Auges ist neben der Form des Auges und der Zahl und Art der Stäbchen und Zapfen auch die Auswertung der Wahrnehmungen durch die Nervenzellen im Auge und im Gehirn sowie die Augenbewegungen und die Lage der Augen am Kopf sehr wesentlich.

Die Auswertung im Gehirn kann von Art zu Art stark variieren. So hat der Mensch sehr viel mehr unterschiedliche Bereiche zur Bildauswertung und zum Bilderkennen im Gehirn als ein Spitzhörnchen.

Die Augen der Wirbeltiere sind sehr empfindliche und teils hoch entwickelte Sinnesorgane. Sie liegen geschützt und eingebettet in einem Muskel-, Fett- und Bindegewebspolster in den knöchernen Augenhöhlen (Orbita) des Schädels. Bei landlebenden Wirbeltieren wird das Auge nach außen hin durch die Augenlider geschützt, wobei der Lidschlussreflex eine Schädigung durch Fremdkörper und andere äußere Einwirkungen verhindert. Zudem bewahrt er die empfindliche Hornhaut durch ständiges Benetzen mit Tränenflüssigkeit vor dem Austrocknen. Auch die Wimpern dienen dem Schutz vor Fremdkörpern, Staub und kleineren Partikeln.

Das Sehorgan "(Organon visus)" der Wirbeltiere kann in drei Untereinheiten gegliedert werden:

Mit einigen Ausnahmen entspricht der Aufbau des Wirbeltierauges dem des Menschen. Gleichwohl finden sich bei manchen Vögeln, Reptilien und wasserlebenden Wirbeltieren teils erhebliche Unterschiede hinsichtlich ihrer Funktionalität und Leistungsfähigkeit. Äußerlich sichtbar sind lediglich die Hornhaut, Sclera und Bindehaut, Iris und Pupille, sowie die Augenlider und ein Teil der abführenden Tränenwege (Tränenpünktchen).

Der Augapfel ("Bulbus oculi") ist ein fast kugelförmiger Körper, dessen Hülle aus drei konzentrischen Schichten, Lederhaut, Aderhaut und Netzhaut, besteht, die alle unterschiedliche Aufgaben haben. Der Innenraum des Augapfels enthält den Glaskörper "(Corpus vitreum)", sowie die Linse "(Lens)" und wird unterteilt in vordere und hintere Augenkammer ("Camera anterior" und "posterior bulbi"). Zudem besitzt der Augapfel ein optisches System, den sogenannten dioptrischen Apparat, welcher ein scharfes Sehen erst möglich macht. Dieses System besteht neben der Linse und dem Glaskörper aus dem Kammerwasser und der Hornhaut.

Zu den "Anhangsorganen" des Auges gehören der Tränenapparat, die Augenmuskeln, die Bindehaut und die Augenlider.

Der "Tränenapparat" landlebender Wirbeltiere besteht aus der für die Produktion von Tränen­flüssigkeit zuständigen Tränendrüse, sowie aus den zu- und ableitenden Gefässen und Kanälen, den Tränenwege, die die Tränenflüssigkeit transportieren. Das gesamte Organ dient der Versorgung der vorderen Augenabschnitte, ihrer Reinigung und ihrem Schutz. 

Um die Augen bewegen zu können, verfügt das Wirbeltierauge über sieben (beim Menschen sechs) äußere "Augenmuskeln". Sie sind unterteilt in vier gerade und zwei schräge Augenmuskeln, die das Auge jeweils in die unterschiedlichsten Richtungen ziehen können. Je nach Augenstellung verfügen die Muskeln über mehr oder weniger ausgeprägte Haupt- und Teilfunktionen, die sich in der Hebung, Senkung, Seitwärtswendung oder Rollung des Augapfels ausdrücken. Die so ausgelösten Augenbewegungen erfolgen einerseits mit dem Ziel, Objekte im Außenraum fixieren zu können, andererseits um das Blickfeld zu vergrößern. Zudem sind sie bei manchen Spezies Voraussetzung für die Entstehung von räumlichem Sehen.

Die "Bindehaut", auch "Konjunctiva" genannt, ist eine Schleimhaut im vorderen Augenabschnitt. Sie beginnt an der Lidkante und überzieht die hintere, dem Augapfel zugewandte Fläche der Augenlider. Dieser Schleimhautüberzug wirkt wie ein weiches Wischtuch und verteilt beim Lidschlag die Tränenflüssigkeit über der Hornhaut, ohne diese zu verletzen.

Das "Augenlid" ist eine dünne, aus Muskeln, Bindegewebe und Haut bestehende Falte, die ein Auge vollständig bedecken kann, um es unter anderem mittels eines Reflexes (Lidschlussreflex) vor äußeren Einwirkungen und Fremdkörpern zu schützen. Es verteilt bei jedem Lidschlag Tränenflüssigkeit, die sich in Form eines Tränenfilms über der vorderen Augapfelfläche anlagert und so die empfindliche Hornhaut sauber und feucht hält. Fische besitzen keine Augenlider.

Als "Sehbahn" bezeichnet man alle Übertragungsleitungen und neuronalen Verschaltungen des visuellen Systems vom Auge bis zum Gehirn. Hierzu zählen die Netzhaut im Auge, der Sehnerv bis zu seinem Verlauf an der Sehnervenkreuzung, sowie den sich daran anschließenden "Tractus opticus". Im seitlichen Kniehöcker des Thalamus im Zwischenhirn (Corpus geniculatum laterale) finden die ersten Verschaltungen der Sehbahn außerhalb der Netzhaut statt. Sie setzt sich fort als sogenannte "Gratioletsche Sehstrahlung" bis zur primären Sehrinde.





</doc>
<doc id="302" url="https://de.wikipedia.org/wiki?curid=302" title="Assoziativgesetz">
Assoziativgesetz

Das Assoziativgesetz ( „vereinigen, verbinden, verknüpfen, vernetzen“), auf Deutsch Verknüpfungsgesetz oder auch Verbindungsgesetz, ist eine Regel aus der Mathematik. Eine (zweistellige) Verknüpfung ist assoziativ, wenn die Reihenfolge der Ausführung keine Rolle spielt. Anders gesagt: Die Klammerung mehrerer assoziativer Verknüpfungen ist beliebig. Deshalb kann man es anschaulich auch „Klammergesetz“ nennen.

Neben dem Assoziativgesetz sind Distributivgesetz und Kommutativgesetz von elementarer Bedeutung in der Mathematik.


Als Verknüpfungen auf den reellen Zahlen sind Addition und Multiplikation assoziativ. So gilt zum Beispiel

Reelle Subtraktion und Division sind hingegen nicht assoziativ, denn es ist

Auch die Potenz ist nicht assoziativ, da

gilt.
Bei (divergenten) unendlichen Summen kann es auf die Klammersetzung ankommen. So verliert die Addition die Assoziativität bei:

Das Assoziativgesetz gehört zu den Gruppenaxiomen, wird aber bereits für die schwächere Struktur einer Halbgruppe gefordert.

Insbesondere bei nicht-assoziativen Verknüpfungen gibt es Konventionen einer seitigen Assoziativität.


Aber auch assoziative Operationen können Seitigkeit haben, wenn sie ins Unendliche zu iterieren sind.

Folgende Abschwächungen des Assoziativgesetzes werden an anderer Stelle genannt/definiert:



</doc>
<doc id="304" url="https://de.wikipedia.org/wiki?curid=304" title="Atharvaveda">
Atharvaveda

Der Atharvaveda (Sanskrit, m., अथर्ववेद, Atharvaveda, alternativ "Atharwaweda)" ist eine der heiligen Textsammlungen des Hinduismus. Er enthält eine Mischung von magischen Hymnen, Zauberformeln und anderem Material, das offenbar sehr unterschiedlichen Alters ist. Obwohl vieles sprachlich deutlich jünger ist als die anderen drei Veden (zumindest des Rigveda), finden sich in ihm auch sehr alte Passagen. Man schätzt, dass der Atharvaveda in der zweiten Hälfte des letzten vorchristlichen Jahrtausends kanonisiert wurde, und auch dann erst mit den anderen drei Veden auf eine Stufe gestellt wurde. Er liegt in zwei Rezensionen oder Schulen vor, der bekannteren Shaunaka-Version, und der erst in jüngster Zeit besser erforschten Paippalada-Version. Der Atharvaveda umfasst 20 Bücher in 731 Hymnen mit ungefähr 6000 Versen. Ungefähr ein Siebtel des Atharvaveda ist aus dem Rigveda entnommen. Der Atharveda ist entstanden, als die Sesshaftwerdung in der Gangesebene schon abgeschlossen war. Das Wort für Tiger kommt hier vor, im früheren Rigveda hingegen noch nicht.

Jeder der vier Veden, das sind Rigveda, Samaveda, Atharvaveda und Yajurveda, umfasst vier Textschichten. Die älteste Schicht sind jeweils die Samhitas (Hymnen), die nächste Schicht sind die Brahmanas (Ritualtexte), dann kommen die Aranyakas (Waldtexte) und zuletzt die Upanishaden (philosophische Lehren).

Die anderen drei Veden waren bestimmten Priestern im vedischen Opferritual zugeteilt: der "Hotri" („Rufer“) musste den Rigveda auswendig können, der "Udgatri" („Sänger“) musste den Samaveda beherrschen, und der "Adhvaryu" (Opferpriester) musste die Mantras des Yajurveda kennen. Als der Atharvaveda in den Kanon aufgenommen wurde, wurde er schlichterhand dem "Brahman" zugeordnet, obwohl dieser Priester eigentlich die drei anderen Veden auswendig können musste, damit er das Ritual aus dem Hintergrund beobachten und bei Fehlern einschreiten konnte. Deswegen wird er auch als „Arzt des Opfers“ bezeichnet. Die Zuordnung des Brahman zum Atharva Veda ist also eher willkürlich.

Im Vergleich zu den drei anderen Veden hatte der Atharvaveda immer die Reputation, vor allem mit Magie zu tun zu haben. Atharvan bedeutet ursprünglich Feuerpriester. Eine andere Sorte Priester waren die Angiras. Magische Formeln, die helfen den Kranken zu heilen, waren Sache der Atharvans. Schwarze Magie, um Feinden oder Rivalen zu schaden, war die Sache der Angiras. Die Heiligkeit des Atharvaveda wurde wegen dieser magischen Inhalte immer etwas in Zweifel gezogen. Der Atharvaveda ist von großer Bedeutung hinsichtlich der medizinischen Vorstellungen der damaligen Zeit. Die Lieder und Zauber zum Heilen von Krankheiten gehören zu den magischen Heilriten (bhaishajyani). Exorzismus und „Frauenriten“ (Liebesmagie) werden ebenso beschrieben. Der Atharvaveda öffnet also ein Fenster zu einer völlig anderen Welt als die des Rigveda.


</doc>
<doc id="305" url="https://de.wikipedia.org/wiki?curid=305" title="Anatomie">
Anatomie

Die Anatomie (dem Erkenntnisgewinn dienende ‚Zergliederung‘ von tierischen und menschlichen Körpern; aus ' „auf“ und ' „Schnitt“) ist ein Teilgebiet der Morphologie. Sie ist in der Medizin bzw. Humanbiologie (Anthropotomie), Zoologie (Zootomie) und Botanik (Phytotomie) die Lehre vom inneren Bau der Organismen. Der Begriff wird metonymisch auch auf andere Bedeutungszusammenhänge übertragen. 

Es werden Gestalt, Lage und Struktur von Körperteilen, Organen, Gewebe oder Zellen betrachtet. Die pathologische Anatomie befasst sich mit krankhaft veränderten Körperteilen. Die mikroskopische Anatomie befasst sich mit den feineren biologischen Strukturen bis zur molekularen Ebene und knüpft an die Molekularbiologie an. Die klassische Anatomie verwendet eine standardisierte Nomenklatur, die auf der lateinischen und der griechischen Sprache basiert.

Die frühesten erhaltenen anatomischen Studien finden sich im "Papyrus Edwin Smith", der auf etwa 1550 v. Chr. datiert wird. Behandelt werden u. a. das Herz und die Herzkranzgefäße, Leber, Milz und Nieren, Hypothalamus, Gebärmutter und Blase sowie die Blutgefäße.

Der "Papyrus Ebers" aus dem letzten Viertel des 16. Jahrhunderts v. Chr. enthält ein Traktat zum Herzen, in dem auch die Blutgefäße beschrieben werden.

Nomenklatur, Methodik und Anwendungen gehen auf die griechischen Ärzte der Antike zurück. Beschreibungen von Muskeln und Skelett finden sich im "Corpus Hippocraticum" (v. a. "Über die Knochenbrüche" und "Über die Gelenke"), wobei in der hippokratischen Medizin die menschliche Physiologie eine größere Bedeutung hatte als die Anatomie. Aristoteles beschrieb anhand der Sektion von Tieren die Anatomie der Wirbeltiere. Praxagoras von Kos kannte bereits im 4. Jahrhundert v. Chr. den Unterschied zwischen Arterien und Venen.

Eine erste anatomische Schule gab es im 2. Jahrhundert v. Chr. in Alexandria. Ptolemaios I. erlaubte die Leichenöffnung für anatomische Studien, meist an Exekutierten. Herophilos von Chalkedon führte die ersten wissenschaftlichen Obduktionen und auch Vivisektionen an Mensch und Tier durch. Er soll 600 Strafgefangene lebend seziert haben und gilt als „Vater der Anatomie“. Er verwarf die Ansicht von Aristoteles, das Herz sei der Sitz des Intellekts und nannte dafür das Gehirn. Weitere Anatomen in Alexandria waren Erasistratos und Eudemos von Alexandria.

Galenos von Pergamon fasste im 2. Jahrhundert n. Chr. das medizinische Wissen der antiken Ärzte systematisch zusammen. Als Arzt von Gladiatoren konnte er verschiedenste Arten von Wunden und so auch die Anatomie des Menschen genau studieren. Weitere Studien betrieb er mit Schweinen und Affen. Seine Schriften bildeten die Basis für die Werke des Mittelalters, so auch für den "Kanon der Medizin" von Avicenna.

Seit etwa 1300 wurden, vor allem in Oberitalien, gelegentlich anatomische Lehrsektionen vorgenommen. Derartige Demonstrationen dienten jedoch vor allem dem Zweck, die Lehren der antiken Autoren bzw. Autoritäten zu bestätigen.
Ab dem 15. Jahrhundert erfuhr die Anatomie, inspiriert durch Ideen des Humanismus und der Renaissance, neue Impulse. Nachdem im Mittelalter die Anatomie keine großen Fortschritte gemacht hatte, korrigierte der flämische Anatom Andreas Vesalius (1514–1564) die über Jahrhunderte kaum hinterfragten Annahmen bzw. Glaubenssätze, was viele seiner Kollegen empörte. Seine Arbeit machte ihn zum Begründer der modernen Anatomie.

William Harvey gilt als Entdecker des Blutkreislaufs im Abendland und als Wegbereiter der modernen Physiologie.

Die Anatomie nahm seitdem einen hohen Stellenwert in den bildenden Künsten ein, Sektionen an Menschen und Tieren gehörten zur Grundausbildung der Studenten. Künstler wie Michelangelo, Raffael, Dürer und Leonardo da Vinci (1452–1519) brachten Jahre mit dem Studium des menschlichen Körpers zu. Da Vincis Codex Windsor übertraf in seiner wissenschaftlichen Genauigkeit die Arbeiten des 62 Jahre später geborenen Vesalius. Die enge Zusammenarbeit von Künstlern und Anatomen ließ medizinische Schriften von außergewöhnlich hoher Qualität entstehen wie zum Beispiel das Lehrbuch des Flamen Philip Verheyen.

Im Zeitalter der Aufklärung errichtete man anatomische Theater, die neben dem wissenschaftlichen Wert einen hohen Schauwert hatten.

Den ersten populär gewordenen fotografischen Anatomieatlas veröffentlichten 1982/83 Johannes W. Rohen und Chihiro Yokochi.

Die makroskopische Anatomie beschäftigt sich mit dem Aufbau des Menschen, von Tieren oder Pflanzen, und zwar mit allen Dingen, die man mit dem bloßen Auge sehen kann. Beachtet hierbei werden nicht nur äußerlich sichtbare Strukturen, sondern insbesondere auch die Strukturen, welche nach Auf- und Auseinanderschneiden des Körpers zu beobachten sind.

Nach der Art der Herangehensweise wird die makroskopische Anatomie unterteilt:


Für die Untersuchung anatomischer Strukturen unterhalb des mit bloßem Auge sichtbaren Bereichs ist die Mikroskopische Anatomie (Histologie) zuständig. Sie beschreibt den Feinbau von Organen, Geweben und Zellen.

Die Embryologie beschreibt die Entstehung der anatomischen Strukturen während der Embryonalentwicklung. Anhand der Entstehungsgeschichte lassen sich vielfältige topografische und funktionelle Beziehungen erkennen. Auch für das Verständnis der Entstehung von Fehlbildungen sind embryologische Kenntnisse unverzichtbar.

Ein wichtiges Gebiet der Anatomie ist die Bereitstellung von Anschauungsmaterialien zur Arztausbildung. Dies geschieht in Präparierkursen und -übungen, Vorlesungsveranstaltungen, anatomischen Sammlungen, anatomischen Museen, vergleichenden anatomischen Sammlungen oder anatomischen Lehrsammlungen. Entsprechendes gilt für die Erstellung anatomischer Lehrbücher und Atlanten, in denen auch heute noch feine Zeichnungen (Strichzeichnungen) ihre didaktische Bedeutung haben.

Der Wiener Anatomieprofessor Josef Hyrtl schrieb in der zweiten Hälfte des 19. Jahrhunderts zur Anatomie: „[Sie] zerstört mit den Händen einen vollendeten Bau, um ihn im Geiste wieder aufzuführen, und den Menschen gleichsam nachzuerschaffen. Eine herrlichere Aufgabe kann sich der menschliche Geist nicht stellen. Die Anatomie ist eine der anziehendsten, und zugleich gründlichsten und vollkommensten Naturwissenschaften, und ist dieses in kurzer Zeit geworden, da ihre Aera erst ein Paar Jahrhunderte umfasst.“





</doc>
<doc id="306" url="https://de.wikipedia.org/wiki?curid=306" title="Andreas Vesalius">
Andreas Vesalius

Andreas Vesalius (auch "Andreas Vesal", latinisiert aus flämisch "Andries van Wezel", eigentlich "Andreas Witinck" bzw. Andries Witting van Wesel; * 31. Dezember 1514 in Brüssel; † 15. Oktober 1564 auf Zakynthos/Griechenland) war ein flämischer Anatom und Chirurg der Renaissance. Er gilt als Begründer der neuzeitlichen Anatomie und des morphologischen Denkens in der Medizin. Er war Leibarzt Kaiser Karls V. und König Philipps II. von Spanien.

Vesalius stammte aus einer alten Weseler Familie (der Name Vesal erinnert noch daran), die jedoch früh auswanderte. Der Vater war habsburgischer Leibapotheker am Kaiserhof Karls V. in Flandern.

Vesalius besuchte die Schule in Brüssel, studierte ab 1530 an der Universität Löwen alte Sprachen und Wissenschaften und erhielt dort seine humanistische Bildung. 1531 wechselte er zur Medizin. Vesalius ging 1533 nach Paris, um mit Miguel Serveto unter Jacques Dubois (Jacobus Sylvius) und Johann Winter galenische Medizin und Anatomie zu studieren. Er war jedoch von Sylvius’ strikter Anlehnung an Galen und von der realitätsfernen Ausbildung an der Universität enttäuscht und verließ Paris 1536 wegen des Dritten Krieges Karls V. gegen Franz I. wieder.

Vesalius kehrte nach Löwen zurück und beendete dort sein Studium. Weil er sich selbst Gewissheit über anatomische Einzelheiten verschaffen wollte, über die er an der Universität aus den Lehren von Galenos gehört hatte, verschaffte er sich dort die Leiche eines Hingerichteten und präparierte das Skelett. Hierbei stellte er Abweichungen zu den Angaben von Galenos fest. In Löwen konnte Vesalius dank guter Beziehungen zur Obrigkeit 1537 seine erste öffentliche Leichenöffnung (Sektion) durchführen.

Anfang 1537 gab Vesalius als Kandidat der Medizin (ein dem Master vergleichbarer Abschluss) in Brüssel sein philosophisches Erstlingswerk heraus, die "Paraphrasis ad nonum librum Rhazae", eine Beschäftigung mit den Theorien und Methoden des persischen Arztes Rhazes (Abu Bakr Muhammad ibn Zakariya ar-Razi), der etwa von 860 bis 925 gelebt hatte.

Danach ging er nach Oberitalien. Am 3. Dezember 1537 wurde er promoviert; tags darauf wurde er in Padua zum Professor der Chirurgie und Anatomie ernannt. So lehrte er die nächsten Jahre in Padua.

Später ging er nach Venedig, wo er 1537 zu Besuch gewesen war und dort in hervorragender Weise Pleuritis-Kranke operiert hatte. In Anerkennung seiner hervorragenden Kenntnisse erhielt Vesalius vom venezianischen Senat einen fünfjährigen Zeitvertrag als Professor für Chirurgie, mit Lehrverpflichtung in Anatomie. Im großen venezianischen Stadtspital konnte er nicht nur „seine anatomischen und medizinischen Kenntnisse vertiefen, sondern auch im Hinblick auf seine musischen Neigungen wesentliche Anregungen von der Malschule des Tizian … empfangen.“ Während seines Aufenthaltes in Venedig lernte er den gleichfalls vom Niederrhein stammenden Maler und Holzschneider Jan Stephan van Calcar kennen, der großen Einfluss auf die künstlerische Gestaltung seiner wissenschaftlichen Werke hatte.

Sechs anatomische Flugblätter für Studenten, die "Tabulae anatomicae sex", gab Vesal 1538 in Venedig heraus. Zumindest hier gilt als gesichert, dass Jan Stephan van Calcar die dazugehörigen Skelettzeichnungen anfertigte. Einen Monat später gab Vesalius eine Neuausgabe der "Institutiones anatomicae" des Johann Winter ohne dessen Wissen heraus. Sie war als Kompendium für Studenten gedacht. Vesalius Aderlassbrief erschien 1539, drei weitere Traktate zu einer großen Galen-Ausgabe verfasste er 1541.

In Bologna, der Scholarenuniversität, sezierte Vesalius 1540 öffentlich: Die erste Vorlesung fand in der Kirche San Salvador statt, die anatomische Demonstration in einem eigens dazu errichteten Anatomischen Theater unter dem sakralen Schutz der Kirche San Francesco.

Auch ein deutscher Medizinstudent war eingeladen worden, der Sektion beizuwohnen. Der aus Liegnitz stammende Balthasar Heseler (1508/1509–1567) berichtete später, Vesalius habe die Sektion vor etwa 200 Zuschauern, darunter 150 Studenten, vorgenommen. Zunächst habe er sich von der alten Vorgehensweise, deren Vertreter Galen und Mondino er namentlich genannt habe, distanziert, und – statt sofort Brust, Bauch und Schädel zu eröffnen – mit der Myologie (Muskellehre) begonnen, die bis zu Leonardo da Vinci völlig vernachlässigt worden war, und alle Details der Myologie und Osteologie (Knochenlehre) dargelegt. Während der Demonstration Vesalius’ habe Jacobus Erigius, ein Mitglied der Medizinischen Fakultät Bolognas, ebenfalls eine Leiche seziert und sich wegen seines unsachgemäßen Vorgehens den Spott des Ersteren zugezogen.

In den Jahren 1538 bis 1542 bereitete Vesalius das große Werk (deutsch: "Sieben Bücher über den Aufbau des menschlichen Körpers") vor, das die neuzeitliche Anatomie begründete. Die Konsequenz, Konzentration und der manische Eifer, die "Fabrica" zu vollenden, ließen ihn bei seinen Mitmenschen schweigsam und melancholisch ("taciturnus et melancholicus") erscheinen.

Während Vesal Professor und Prosektor war, sezierte er 1539 die Leichen aller in Padua Hingerichteten. 1540 folgten anatomische Demonstrationen in Bologna.

Im Vorwort zur "Fabrica" übte er vehemente Kritik an Galen, der selbst nie ein Hehl daraus gemacht hatte, nur Tierkadaver seziert zu haben. Dieses sorgfältig typographisch ausgestattete Lehrbuch zeigt rund 200 zum Teil ganzseitige Illustrationen. Darin vertrat Vesal entgegen der allgemeinen Überzeugung die Ansicht, allein der menschliche Leib sei der zuverlässige Weg zur Erkenntnis des menschlichen Körperbaus. Darüber hinaus zeichnete er darin, sich dabei auf Plinius beziehend, eine Abstammungslinie vom Affen über die Pygmäen hin zum Menschen.

"De humani corporis fabrica libri septem" (nebst deren Auszug für Chirurgen) erschien 1543 in Basel bei dem Verleger Johannes Oporinus. Vesalius hatte die Holzstöcke seiner Illustrationen, fertig geschnitten, zusammen mit den Probeabzügen nach Basel bringen lassen. Er selbst folgte 1543 nach und hielt in Basel im Mai ein anatomisches Kolloquium ab. Das hierbei von Vesalius präparierte sogenannte "Vesalsche Skelett" ist noch heute erhalten und das älteste Stück der anatomischen Sammlung in Basel. Es soll 1543 aus den sterblichen Überresten des Straftäters Jakob Karrer von Gebweiler präpariert worden sein.

1544 reiste Vesalius nach Pisa, nachdem er sich Karl V. als Leibarzt verpflichtet hatte, und hielt dort eine öffentliche Sektion ab. Auch ein Lehramt an der Universität Pisa wurde ihm angetragen, doch die Annahme des Rufs wurde ihm von Kaiser Karl V. verwehrt. Vesalius zog nach Brüssel und war weiter schriftstellerisch tätig. Er publizierte 1546 eine Abhandlung über die Chinawurzel und heiratete im selben Jahr. Als sich Kaiser Karl V. 1556 aus Spanien zurückzog, wollte er Vesalius, mit einer Leibrente versehen, dort zurücklassen. Ein Jahr zuvor, 1555, war die zweite Auflage der "Fabrica" erschienen, die in einer noch schöneren Typographie nach dem Entwurf des französischen Schriftsetzers Claude Garamond wieder zu einem Meisterwerk der europäischen Buchkunst geraten war (Bücher 1–5 kamen schon 1552 auf den Markt.) Unzählige kleinere Veränderungen hatte Vesal in diese Ausgabe eingearbeitet. Sie enthielt auch Antworten auf Angriffe gegen ihn und zudem war sie durch eine freiere Haltung gegenüber Galen gekennzeichnet.

Vesals wissenschaftliches Interesse erlosch nun zwar nicht, doch trat er in den Dienst Philipps II. von Spanien, dessen Hof 1559 nach Madrid verlegt wurde. Vesal war jetzt Arzt des niederländischen Hofstaates. Schließlich unternahm er 1564 eine Pilgerreise ins Heilige Land, von der er nicht mehr zurückkam: Während der Rückreise von Jerusalem erkrankte er und musste an Land gehen. In Zante starb er. Er soll von Pilgern bestattet worden sein.

Legenden um diesen frühen Tod brachten Vesalius mit der Inquisition in Verbindung. Hubertus Languetus schrieb ein Jahr nach Bekanntwerden seines Todes an den Arzt Caspar Peucer, Vesalius habe aus Versehen einen Menschen bei lebendigem Leib seziert und sei zur Strafe verpflichtet worden, nach Jerusalem zu reisen.

1987 wurde der Asteroid (2642) Vésale nach ihm benannt.





</doc>
<doc id="307" url="https://de.wikipedia.org/wiki?curid=307" title="Amulett">
Amulett

Ein Amulett ist ein tragbarer Gegenstand, dem magische Kräfte zugeschrieben werden, mit denen er Glück bringen (energetische, sakramentale Wirkung) und vor Schaden schützen (apotropäische Wirkung) soll. In seiner glückbringenden Eigenschaft und meist größerer Ausführung wird es auch als Talisman bezeichnet. Das Amulett hat mit seiner vermuteten magischen Wirkung Parallelen zur Votivgabe. Während die Votivgabe aber typischerweise an einem Ort mit vermuteter magischer Wirkung hinterlegt wird, dient das Amulett dazu am Körper getragen zu werden. Mit abnehmender Vermutung einer magischen Wirkung überwiegt immer mehr die Funktion als Schmuck oder als Zeichen der Zugehörigkeit zu einer meist religiösen Gemeinschaft.

Die genaue Etymologie des Wortes ist ungeklärt. Der lateinische Begriff "amuletum", von dem das deutsche Wort ab Anfang des 18. Jahrhunderts entlehnt ist, findet sich mehrfach in der "Naturalis historia" Plinius’ des Älteren (1. Jahrhundert n. Chr.) und wird von verschiedenen Autoren auf "amoliri" ‚abwenden, entfernen‘ zurückgeführt. Von anderen Wissenschaftlern wurde eine Herkunft aus der arabischen Wurzel "ḥ-m-l" () vermutet, gegen die Johann Gildemeister in der Zeitschrift der Deutschen Morgenländischen Gesellschaft argumentierte. Möglicherweise besteht als (gesundheitsfördernde bzw. schadenabwendende) ‚Speise aus Stärkemehl‘ eine Verwandtschaft mit griechisch-lateinisch "amylum"/"amulum" (Stärkemehl).

Amulette werden am Körper (oft auch als Schmuck) oder in der Kleidung getragen, in Fahrzeugen oder der Behausung aufbewahrt oder dem Vieh umgehängt. Sie können aus einer Vielzahl von Materialien bestehen und durch sie soll der Träger passiv geschützt werden.

Schon in der Vorgeschichte hängten sich Menschen Überreste (Zähne und Krallen) ihrer erlegten Beute um. Sie sollten dem Träger die Kraft des Tieres geben.

Amulette finden Anwendung in der Heilkunde, als Schutz von Schwangeren, gegen den bösen Blick und - beispielsweise die Muskutnuss - als Liebeszauber. Am Amulett wirkt die animistische Vorstellung, dass magische Kräfte auf den Menschen einwirken, denen er durch das Amulett entgegenwirken kann.

Amulette sind in allen Kulturen bekannt. Seit der Steinzeit nutzte man Muscheln oder Perlen und besondere Steine wie beispielsweise Bernstein und Bergkristalle. In keltischen Siedlungsresten wurden polierte, durchbohrte Schädelfragmente (Amulette?) bei Grabungen gefunden.

Bei den Arabern sind Amulette Ledertäschchen mit eingenähtem Papier, auf das eine Koransure oder ein magisches Zeichen geschrieben ist. Sie verbreiten die islamische Segenskraft Baraka. Eine amulettartige positive Wirkung entfalten im Volksglauben Buntmetalle, besonders Kupfer und Messing.

Der Glaube an die medizinische Wirksamkeit von Amuletten erfuhr in Europa besonders von der Frühen Neuzeit bis ins 17. Jahrhundert eine Hochblüte und findet sich etwa bei Paracelsus, Marsilio Ficino, Cornelius Agrippa und Giordano Bruno.

Als Amulett gelten bei den:


Der Bagdader Mathematiker, Philosoph und Arzt Qusta ibn Luqa ("Qusṭā ibn Lūqā al-Baʿlabakkī") war melchitischer Christ griechischer Abstammung und machte bereits in seiner um 900 entstandenen Schrift über den "Wert von Amuletten" Glauben und menschliche Einbildungskraft für deren Wirkung verantwortlich. Okkulte oder astrale Eigenschaften verneinte er.
In Europa wandte sich die christliche Kirche im Mittelalter ebenfalls gegen den Aberglauben, zu dem auch Amulette gerechnet wurden. Das hinderte den Volksglauben allerdings nicht daran, an Amuletten mit christlichem Bezug festzuhalten.

Auch hohe Kirchenmänner besitzen Glücksbringer. So sind etwa im Schatzinventar des Heiligen Stuhls von 1295 15 Natternzungenbäume verzeichnet. Als am 9. Februar 1749 der Fürstbischof Anselm Franz von Würzburg, zeitlebens ein Streiter gegen Aberglauben und Hexenwahn, nach einem Schlaganfall starb, fand man auf seiner Brust ein Amulett aus Messingblech, auf dem ein Pentagramm und einige Zauberformeln eingraviert waren.





</doc>
<doc id="308" url="https://de.wikipedia.org/wiki?curid=308" title="Arc de Triomphe de l’Étoile">
Arc de Triomphe de l’Étoile

Der Arc de Triomphe de l’Étoile (dt. "Triumphbogen des Sterns") oder kurz Arc de Triomphe, ist ein 1806 bis 1836 errichtetes Denkmal an der Place Charles-de-Gaulle in Paris. Er gehört zu den Wahrzeichen der Metropole. Unter dem Bogen liegt das Grabmal des unbekannten Soldaten aus dem Ersten Weltkrieg mit der täglich gewarteten Ewigen Flamme, im Französischen "Flamme du Souvenir" (dt. Flamme der Erinnerung) genannt, im Gedenken an die Toten, die nie identifiziert wurden. Das ganze Jahr hindurch finden Kranzniederlegungen und Ehrungen statt, die ihren Höhepunkt in der Parade am 11. November finden, dem Jahrestag des Waffenstillstands zwischen Frankreich und Deutschland im Jahr 1918.

Als Fußgänger gelangt man zum "Arc de Triomphe" nur durch eine Unterführung; der Triumphbogen verfügt über eine Aussichtsplattform. 

Der "Arc de Triomphe de l’Étoile" ist nicht zu verwechseln mit dem weniger bekannten und kleineren Arc de Triomphe du Carrousel, der sich zwischen dem Palais du Louvre und dem Jardin des Tuileries befindet.

Der Triumphbogen diente dem Ruhm der kaiserlichen Armeen und erscheint manchen heute als „Altar des Vaterlandes“, an dem die feierlichsten staatlichen Zeremonien Frankreichs stattfinden, die häufig von hier aus die Avenue des Champs-Élysées hinuntergehen bzw. hier enden.

Er steht im Zentrum des "Place Charles de Gaulle" (bis 1970 "Place de l’Étoile"), am westlichen Ausläufer der "Avenue des Champs-Élysées". Er ist Teil der „historischen Achse“, einer Reihe von Monumenten und großen Straßen, die aus Paris herausführen. Zwölf Avenuen gehen sternförmig von diesem Triumphbogen aus. Die heutige Form des Platzes entstand 1854, war in Grundzügen aber bereits seit dem späten 18. Jahrhundert so ähnlich angelegt worden, wenn auch nur mit vier Straßen.

Der Triumphbogen selbst wurde von Kaiser Napoleon I. nach der Schlacht von Austerlitz zur Verherrlichung seiner Siege 1806 in Auftrag gegeben. Am 15. August 1806 wurde der Grundstein zum Bau gelegt. Zwei Jahre dauerte der Bau der Fundamente. 1810 erhoben sich die vier Pylonen des Triumphbogens aber erst bis zu einer Höhe von 1 m. Aus Anlass von Napoleons Heirat mit der habsburgischen Prinzessin Marie-Louise ließ der Kaiser ein provisorisches Modell des Triumphbogens aus Holz und Stuck in originaler Größe errichten. Ähnlich dem Elefanten der Bastille stand diese Ehrenpforte längere Zeit als Platzhalter des unfertigen Monuments. Anders als im Falle des Elefanten kam es aber letztlich zum Weiterbau.

Als der zuständige Architekt Jean-François Chalgrin 1811 gestorben war und Napoleon 1814 abdankte, wurden die Bauarbeiten gestoppt. Louis XVIII. ließ sie 1824 unter der Leitung von Héricart de Thury wieder aufnehmen. 1830 entschied sich der „Bürgerkönig“ Louis-Philippe zur napoleonischen Konzeption zurückzukehren. Er und Adolphe Thiers entschieden über den figurativen Schmuck und seine Ausführenden. 1836, unter der Regierung des „Bürgerkönigs“, wurde der Bogen von Huyot und Blouet fertiggestellt. Die feierliche Einweihung war am 29. Juli.

Der Triumphbogen ist 49,54 m hoch, 44,82 m breit und 22 m tief. Der große Gewölbebogen misst 29,19 m in der Höhe und 14,62 m in der Breite, der kleine Bogen 18,68 m in der Höhe und 8,44 m in der Breite. Der Entwurf ist im Stil der antiken römischen Architektur gehalten. Die vier Figurengruppen an der Basis des Bogens zeigen "Der Triumph von 1810", "Widerstand", "Frieden" und "La Marseillaise" oder "Auszug der Freiwilligen von 1792" (von François Rude). Oben sind auf den Flächen rund um den Bogen Flachreliefs mit Nachbildungen von wichtigen revolutionären und napoleonischen Siegen eingelassen. Die Innenwände des Triumphbogens beherbergen ein kleines Museum und führen die Namen von 558 französischen Militärs, vorwiegend Generälen auf. Die Namen derjenigen, die im Kampf gefallen sind, sind unterstrichen. Siehe: Liste der Personennamen auf dem Triumphbogen in Paris

Berühmt ist dieser Triumphbogen auch wegen der bedeutenden Reliefs, die er trägt. Sie wurden 1833 in Auftrag gegeben bei den Bildhauern Antoine Étex, Jean-Pierre Cortot und vor allem François Rude. Die Ostfassade zeigt das berühmteste Relief, die "Marseillaise" (Auszug der Freiwilligen von 1792) von François Rude (1784–1855), die auch "Le chant du départ", also das Abschiedslied, genannt wird. Es ist eine Gruppe ausziehender Krieger – offensichtlich in revolutionärer oder erhoben nationaler Gesinnung, die – zumindest kann man das in dieser Szene vermuten – das neue Revolutionslied der "Marseillaise" auf den Lippen haben, das erst am 25. April 1792 komponiert worden war.

François Rude übertrifft mit dem heroischen Schwung seiner Darstellung die seiner Konkurrenten auf diesem Triumphbogen bei weitem. Er begann als akademischer Klassizist, aber mit diesem seinem bekanntesten Werk vollzog Rude als einer der ersten die Abkehr vom Klassizismus und die Hinwendung zur Romantik, zu einer neuen heroischen Leidenschaftlichkeit in der Bildhauerei, ähnlich wie Delacroix in der Malerei.

Interessant ist ein Vergleich der beiden Reliefs dieser Seite. Es handelt sich auf der anderen Seite um den „Triumph Napoleons nach dem Frieden von 1810“ (der „Triumph“ verherrlicht den Frieden von Wien) von Cortot. Das Relief von Cortot steht noch ganz in der Tradition der klassizistischen Statik, der gemessenen Heldenverehrung, des symmetrischen, wohlproportionierten Bildaufbaus – mit anderen Worten der „erhabenen Langeweile“.

Auch bei den Reliefs von Antoine Etex auf der Westseite ist diese Atmosphäre deutlich zu spüren, beispielsweise beim „Frieden“. Hier hat man noch den Eindruck, dass die Themen von einer Schauspielertruppe auf einer Theaterbühne dargestellt werden, dass hier Motive aus dem Arsenal zusammengestellt worden sind.

Auf den vier Außenseiten des Bogens befinden sich sechs Flachreliefs, die jeweils berühmte Schlachten zeigen. Unter den sechs Bildhauern ist auch Jean-Jacques Feuchère mit einer Darstellung des "Übergangs über die Brücke von Arcole" zu sehen.

Am 7. August 1919 durchflog Charles Godefroy mit einer Nieuport 11 „Bébé“ den Triumphbogen. Ein weiterer Weltkriegsveteran, Jean Navarre, war Wochen zuvor beim Üben für diesen Flug tödlich verunglückt. Im Oktober 1981 flog Alain Marchand durch den Triumphbogen.
Der Rundkurs der letzten Kilometer der Schlussetappe der Tour de France, die seit 1975 auf der Avenue des Champs-Élysées endet, umrundet den Arc de Triomphe. Bis 2013 führte der Rundkurs direkt vor dem Arc de Triomphe eine Wende aus (und umkreiste ihn somit nicht).

Am Abend des 9. Januar 2015 wurden die Worte „Paris est Charlie“ auf den Triumphbogen projiziert und illuminiert. Die Parole, eine Abwandlung von „Je suis Charlie“, ist das Bekenntnis der Pariser zu den demokratischen Werten der Meinungs- und Pressefreiheit und eine Solidaritätsbekundung mit den Opfern der terroristischen Verbrechen und mit dem Satiremagazin Charlie Hebdo.




</doc>
<doc id="309" url="https://de.wikipedia.org/wiki?curid=309" title="Akustik">
Akustik

Die Akustik (gr. "akoyein" ‚hören‘) ist die Lehre vom Schall und seiner Ausbreitung. Als Wissenschaftsgebiet umfasst sie sämtliche damit zusammenhängende Gesichtspunkte, so die Entstehung und Erzeugung, die Ausbreitung, die Beeinflussung und die Analyse von Schall, seine Wahrnehmung durch das Gehör und Wirkung auf Menschen und Tiere. Die Akustik ist ein interdisziplinäres Fachgebiet, das auf Erkenntnissen aus zahlreichen anderen Fachgebieten aufbaut, unter anderem der Physik, der Psychologie, der Nachrichtentechnik und der Materialwissenschaft.

Zu den wichtigsten Anwendungen der Akustik gehört neben der Erforschung und Minderung von Lärm auch das Bemühen, einen Wohlklang hervorzurufen oder eine akustische Information zu übertragen. Außerdem ist der Einsatz von Schall zur Diagnose oder zu technischen Zwecken eine wichtige Anwendung der Akustik.

Als eine erste systematische Beschäftigung mit der Akustik gilt die Einführung von Tonsystemen und Stimmungen in der Musik im 3. Jahrtausend v. Chr. in China. Aus der Antike ist die wissenschaftliche Beschäftigung mit der Akustik unter anderem von Pythagoras von Samos (ca. 570–510 v. Chr.) überliefert, der den Zusammenhang von Saitenlänge und Tonhöhe beim Monochord mathematisch analysierte. Chrysippos von Soli (281–208 v. Chr.) erkannte den Wellencharakter von Schall durch einen Vergleich mit Wellen auf der Wasseroberfläche. Der römische Architekt Vitruv (ca. 80–10 v. Chr.) analysierte die Schallausbreitung in Amphitheatern und vermutete die Ausbreitung von Schall als Kugelwelle. Er beschrieb ebenfalls die Wirkungsweise von Helmholtz-Resonatoren zur Absorption tieffrequenten Schalls.
Leonardo da Vinci (1452–1519) erkannte unter anderem, dass Luft als Medium zur Ausbreitung des Schalls erforderlich ist und dass sich Schall mit einer endlichen Geschwindigkeit ausbreitet. Von Marin Mersenne (1588–1648) stammt neben anderen wissenschaftlichen Erkenntnissen zur Natur des Schalls auch die erste Angabe einer experimentell bestimmten Schallgeschwindigkeit. Galileo Galilei (1564–1642) beschrieb den für die Akustik wichtigen Zusammenhang zwischen Tonhöhe und Frequenz. Joseph Sauveur (1653–1716) führte die Bezeichnung „Akustik“ für die Lehre vom Schall ein. Isaac Newton (1643–1727) berechnete als erster die Schallgeschwindigkeit auf Grund theoretischer Überlegungen, während Leonhard Euler (1707–1783) eine Wellengleichung für Schall in der heute verwendeten Form fand. Ernst Florens Friedrich Chladni (1756–1827) gilt als Begründer der modernen experimentellen Akustik; er erfand die Chladnischen Klangfiguren, die Eigenschwingungen von Platten sichtbar machen.

Mit Beginn des 19. Jahrhunderts setzte eine intensive Beschäftigung mit der Akustik ein und zahlreiche Wissenschaftler widmeten sich dem Thema. So fand Pierre-Simon Laplace (1749–1827) das adiabatische Verhalten von Schall, Georg Simon Ohm (1789–1854) postulierte die Fähigkeit des Gehörs, Klänge in Grundtöne und Harmonische aufzulösen, Hermann von Helmholtz (1821–1894) erforschte die Tonempfindung und beschrieb den Helmholtz-Resonator und John William Strutt, 3. Baron Rayleigh (1842–1919) veröffentlichte die „Theory of Sound“ mit zahlreichen mathematisch begründeten Erkenntnissen, die den Schall, seine Entstehung und Ausbreitung betreffen.

In der zweiten Hälfte des 19. Jahrhunderts werden erste akustische Mess- und Aufzeichnungsgeräte entwickelt, so der Phonautograph von Édouard-Léon Scott de Martinville (1817–1897) und später der Phonograph von Thomas Alva Edison (1847–1931). August Kundt (1839–1894) entwickelte das Kundtsche Rohr und setzte es zur Messung des Schallabsorptionsgrades ein.

Ab dem Beginn des 20. Jahrhunderts kam es zur breiten Anwendung der vorhandenen theoretischen Erkenntnisse zur Akustik. So entwickelte sich die von Wallace Clement Sabine begründete wissenschaftliche Raumakustik mit dem Ziel, die Hörsamkeit von Räumen zu verbessern. Die Erfindung der Elektronenröhre 1907 ermöglichte den breiten Einsatz elektroakustischer Übertragungstechnik. Paul Langevin (1872–1946) verwendete Ultraschall zur technischen Ortung von Objekten unter Wasser (Sonar). Heinrich Barkhausen (1881–1956) erfand das erste Gerät zur Messung der Lautstärke. Seit etwa 1930 erscheinen wissenschaftliche Fachzeitschriften, die sich ausschließlich Themen der Akustik widmen.

Zu einer der wichtigsten Anwendungen der Akustik entwickelt sich in der ersten Hälfte des 20. Jahrhunderts auch die Minderung von Lärm, so wird zum Beispiel der Schalldämpfer für die Abgasanlage von Kraftfahrzeugen immer weiter verbessert. Mit der Einführung von Strahltriebwerken um 1950 und der für den erfolgreichen Einsatz notwendigen Lärmminderung entwickelte sich die Aeroakustik, die wesentlich durch die Arbeiten von Michael James Lighthill (1924–1998) begründet wurde.

Innerhalb der Akustik werden eine Vielzahl unterschiedlicher Arbeitsgebiete behandelt:

Neben der Betrachtung zeitgemittelter Schallfeld- und Schallenergiegrößen wird oft die zeitliche Auslenkung gemessen, z. B. das Drucksignal, und einer Frequenzanalyse unterzogen. Für den Zusammenhang des so erhaltenen Frequenzspektrums mit dem Klang siehe Klangspektrum. Die zeitliche Veränderung innerhalb eines Schallereignisses wird durch Kurzzeit-Fourier-Transformation zugänglich. Die Veränderungen des Spektrums beim Prozess der Schallabstrahlung, -ausbreitung und Messung bzw. Wahrnehmung werden durch den jeweiligen Frequenzgang beschrieben. Den Frequenzgang des Gehörs berücksichtigen Frequenzbewertungskurven.

Die akustische Resonanzanalyse wertet die entstehenden Resonanzfrequenzen aus, wenn ein Körper durch eine impulshafte Anregung wie etwa einen Schlag in Schwingung versetzt wird. Ist der Körper ein schwingungsfähiges System, so bilden sich über einen gewissen Zeitraum bestimmte charakteristischen Frequenzen aus, der Körper schwingt in den so genannten natürlichen Eigen- oder Resonanzfrequenzen – kurz Resonanzen.

Bei der Ordnungsanalyse werden Geräusche oder Schwingungen von rotierenden Maschinen analysiert, wobei im Gegensatz zur Frequenzanalyse hierbei der Energiegehalt des Geräusches nicht über der Frequenz, sondern über der Ordnung aufgetragen wird. Die Ordnung entspricht dabei einem Vielfachen der Drehzahl.

Ein reflexionsarmer Raum, manchmal physikalisch unrichtig auch „schalltoter“ Raum genannt, besitzt Absorptionsmaterial an Decke und Wänden, so dass nur minimale Reflexionen auftreten und Bedingungen wie in einem Direktfeld D (Freifeld oder freiem Schallfeld) herrschen, wobei der Schalldruck mit 1/r nach dem Abstandsgesetz von einer Punktschallquelle abnimmt. Solche Räume eignen sich für Sprachaufzeichnungen und für die Lokalisation von Schallquellen. Wird auf einer gedachten Hüllfläche um die Schallquelle die senkrecht durch diese Fläche tretende Schallintensität gemessen, so kann die Schallleistung der Quelle bestimmt werden.

Ein Freifeldraum ist die spezielle Ausführung eines reflexionsarmen Raumes. Hier ist jedoch zusätzlich auch der Boden mit absorbierendem Material bedeckt. Da der Boden durch diese Maßnahme nicht mehr begehbar ist, wird meistens ein schalldurchlässiges Gitter darüber angeordnet, das den Zugang zum Messobjekt ermöglicht. Derartige Räume werden in der akustischen Messtechnik eingesetzt, um gezielte Schallquellenanalysen – auch unter dem Messobjekt – durchführen zu können.

Ein Hallraum dagegen wird so konstruiert, dass an jedem beliebigen Punkt im Schallfeld Reflexionen gleicher Größe aus allen Richtungen zusammentreffen. In einem idealen Hallraum herrscht daher mit Ausnahme des Bereiches direkt um die Schallquelle (siehe Hallradius) an jedem Ort derselbe Schalldruck. Ein solches Schallfeld wird Diffusfeld genannt. Da die Schallstrahlen aus allen Richtungen gleichzeitig einfallen, ist in einem Diffusfeld keine Schallintensität vorhanden. Um Resonanzen in einem Hallraum zu vermeiden wird er im Allgemeinen ohne parallel zueinander stehende Wände und Decken gebaut. Über Nachhallzeit-Messungen oder durch Referenzschallquellen kann der Raum kalibriert werden. Hierbei wird die Differenz zwischen dem an einem beliebigen Ort im Raum, weit genug außerhalb des Hallradius gemessenen Schalldruckpegel und dem Schallleistungspegel einer Schallquelle bestimmt. Diese Differenz ist frequenzabhängig und bleibt unverändert, solange sich der Aufbau des Raumes und der Absorptionsgrad der Wände nicht ändern. In einem Hallraum kann daher die Schallleistung einer Quelle theoretisch mit einer einzigen Schalldruckmessung bestimmt werden. Dieses ist z. B. für Fragestellungen im Bereich des Schallschutzes sehr nützlich.

Die meisten höheren Tiere besitzen einen Hörsinn. Schall ist ein wichtiger Kommunikationskanal, da er praktisch unmittelbare Fernwirkung besitzt. Mit Lautäußerungen ist den Tieren ein Mittel zur Reviermarkierung, Partner- oder Rudelsuche, zum Auffinden von Beute und zur Mitteilung von Stimmungen, Warnsignalen usw. gegeben. Der menschliche Hörbereich liegt zwischen der Hörschwelle und der Schmerzschwelle (etwa 0 dB HL bis 110 dB HL).

Bei der Erzeugung von Lauten im Rahmen der Lautlehre unterscheidet man im Allgemeinen zwischen stimmhaften und stimmlosen Phonemen. Bei den stimmhaften Phonemen, die als Vokale bezeichnet werden, werden beim Kehlkopf durch Vibration der Stimmbänder die „Roh“klänge erzeugt, die dann im Rachen- und Nasenraum durch verschiedene willkürlich beeinflussbare oder unveränderliche individualspezifische Resonanzräume moduliert werden. Bei stimmlosen Phonemen, den Konsonanten, ruhen die Stimmbänder, wobei der Laut durch Modulation des Luftstromes zustande kommt. Beim Flüstern werden selbst die Vokale nur durch Modulation des Spektrums des Rauschens eines hervorgepressten Luftstromes gebildet, wobei die Stimmbänder ruhen.

Fachleute für Akustik werden als Akustiker/in oder Akustikingenieur/in bezeichnet. Die englischen Berufsbezeichnungen sind acoustical engineer oder acoustician. Der übliche Zugang zu diesem Arbeitsfeld ist ein Studium im Bereich Physik oder ein entsprechendes ingenieurwissenschaftliches Studium. Hörgeräteakustiker/innen arbeiten im Fachbereich der Medizintechnik und verwenden in ihrem Beruf sowohl physikalisches, als auch medizinisches Fachwissen. 




</doc>
<doc id="310" url="https://de.wikipedia.org/wiki?curid=310" title="Amerika (Begriffsklärung)">
Amerika (Begriffsklärung)

Der Name Amerika steht im Allgemeinen für:
Der Name Amerika bezeichnet im Bereich der Künste und Medien:
Amerika ist der Name folgender Orte:

Amerika ist der Name mehrerer Schiffe, siehe Liste von Schiffen mit dem Namen Amerika, darunter:
Amerika ist der Familienname von:
Amerika steht weiterhin für:

Siehe auch:



</doc>
<doc id="311" url="https://de.wikipedia.org/wiki?curid=311" title="Afroasiatische Sprachen">
Afroasiatische Sprachen

Die afroasiatischen Sprachen (traditionell als "semito-hamitisch" oder "hamito-semitisch" bezeichnet) bilden eine Sprachfamilie, die in Nord- und Ostafrika sowie in Vorderasien verbreitet ist. Das Afroasiatische besteht aus sechs Zweigen: dem Ägyptischen, Berberischen, Semitischen, Kuschitischen, Omotischen und dem Tschadischen. Diese umfassen insgesamt etwa 350 Sprachen mit etwa 350 Millionen Sprechern. Etwa 40 der bekannten Sprachen sind heute ausgestorben.

Das Afroasiatische ist auch eine der vier großen Familien (Phyla) afrikanischer Sprachen, die Joseph Greenberg in seinen Arbeiten von 1949 bis 1963 etabliert hatte und die heute die Basis aller linguistischen Klassifikationen in Afrika bilden.
Das Gebiet der (rezenten) Sprachfamilie der afroasiatischen Sprachen grenzt im Süden an die Sprachfamilien der Niger-Kongo- und nilosaharanischen Sprachen und im Nordosten an den Sprachraum des Indogermanischen und der Turksprachen.

Die Bezeichnung „Afroasiatisch“ geht auf Joseph Greenberg zurück. Sie hat die ältere Benennung „Hamito-Semitisch“ vielfach abgelöst. Diese scheint insofern irreführend, als sie eine Zweiteilung in „semitische“ und „hamitische“ Sprachen suggeriert und im Zusammenhang mit der Hamitentheorie als rassistisch konnotiert empfunden werden kann. Als weitere Benennungen wurden "Afrasisch" (Igor M. Diakonoff), "Lisramisch" (Carleton T. Hodge) und "Erythräisch" (Leo Reinisch) vorgeschlagen; diese Termini haben jedoch, mit Ausnahme von "Afrasisch", kaum Anhänger gefunden. Anmerkung: Erythräisch ist in diesem Zusammenhang nicht mit der Bezeichnung einer von Christopher Ehret vorgeschlagenen hypothetischen Untergruppe des Afroasiatischen zu verwechseln.

Die ältere, in der Fachliteratur weit verbreitete Bezeichnung als hamito-semitisch geht auf die Völkertafel der Bibel zurück, die die Söhne Hams und Sems im hier gemeinten Sprachgebiet verortet. Die Begriffe sind nicht ethnisch gemeint und gruppieren die Sprachen in zwei differierende Zonen: Tatsächlich sind einerseits Koptisch und Berberisch in Nordafrika einander ähnlicher als beispielsweise Koptisch und die semitischen Sprachen Hebräisch, Arabisch, Aramäisch; andererseits weisen die semitischen Sprachen untereinander ein engeres Verwandtschaftsverhältnis auf, das sie von den nordafrikanischen Sprachen abhebt. Auch wenn Hamitisch als Bezeichnung für die offenbar auf afrikanischem Boden entstandenen „afroasiatischen“ Sprachen heute außer Gebrauch kommt, bleibt die Bezeichnung Semitisch weiterhin üblich.

Man unterscheidet heute in der Regel folgende sechs Primärzweige des Afroasiatischen:


Die in Äthiopien gesprochene Sprache Ongota (Birale) gehört möglicherweise auch zur afroasiatischen Familie und etabliert nach H. Fleming einen unabhängigen weiteren Zweig. Einige Wissenschaftler halten das Kuschitische nicht für eine genetische Einheit, sondern nehmen an, dass es aus zwei oder mehr direkt dem Afroasiatischen untergeordneten Primärzweigen besteht.

Die früher vorgenommene Teilung in semitische und hamitische Sprachen wird heute nicht mehr vertreten (dazu siehe den Artikel Afrikanische Sprachen). Es existieren mehrere Vorstellungen darüber, in welcher Reihenfolge und wann sich die einzelnen Primärzweige vom Proto-Afroasiatischen abspalteten. Ein linguistisch begründetes Szenario liefert Ehret 1995. Danach hat sich zuerst – vor mindestens 10.000 Jahren – der omotische Zweig vom Kern getrennt (dies wird heute nahezu von allen Forschern so gesehen, während die weiteren Stufen durchaus umstritten sind). Als nächste Zweige spalteten sich das Kuschitische und Tschadische ab, die Trennung des Restes (von Ehret "Boreafrasisch" genannt) in Ägyptisch, Berberisch und Semitisch erfolgte zuletzt. Es ist nach heutigem Kenntnisstand nicht möglich, eine auch nur annähernde absolute Chronologie dieser Abspaltungen anzugeben. Nach dem Modell von Ehret ergibt sich folgender „dynamischer“ Stammbaum des Afroasiatischen:

"Stammbaum und interne Gliederung des Afroasiatischen (nach Ehret 1995)"


Der von Ehret hier eingeführte Name "Erythräisch" (für Afroasiatisch ohne Omotisch) wurde von anderen Forschern für die gesamte afroasiatische Sprachfamilie verwendet, er konnte sich aber nicht gegen "Afroasiatisch" durchsetzen.

Das Ägyptische stellt eine Ausnahme unter den afroasiatischen Primärzweigen dar, da es aus nur einer einzigen Sprache besteht, die eine lückenlose Überlieferung über fast fünf Jahrtausende aufweist. Seine letzte Stufe, das Koptische, starb in der frühen Neuzeit als Alltagssprache aus. Das Ausbreitungsgebiet des Ägyptischen umfasste in historischer Zeit kaum mehr als das nördliche Drittel des Niltales, im 3. Jahrtausend v. Chr. wurde jedoch möglicherweise auch in der ägyptischen Westwüste ein dem Ägyptischen nahe verwandtes Idiom gesprochen, von dem sich einzelne Personennamen in ägyptischer Überlieferung finden. Durch seine lange Überlieferungsdauer ist das Ägyptische von besonderem sprachwissenschaftlichem Interesse, jedoch fehlen ihm trotz der frühen Überlieferung einige grundlegende morphologische und möglicherweise auch phonologische Eigenschaften des Afroasiatischen.

Die Berbersprachen wurden vor der Expansion des Islam und der damit verbundenen Ausbreitung des Arabischen beinahe in der gesamten Sahara gesprochen. Das heutige Hauptverbreitungsgebiet liegt in den Staaten Niger, Mali, Algerien, Marokko, Tunesien und im westlichen Libyen; kleine Sprachinseln haben sich auch im Nordosten der Sahara in Oasen wie Augila (Libyen) und Siwa (Ägypten) sowie im westlichen Mauretanien gehalten. Im Gegensatz zu den anderen Zweigen des Afroasiatischen (außer dem Ägyptischen) sind die Berbersprachen untereinander nahe verwandt und gehören fast vollständig zu zwei Dialektkontinua. Die bekanntesten Berbersprachen sind Kabylisch, Tamazight, Taschelhit, Tarifit sowie das Tuareg. Meistens wird auch die kaum bekannte libysche Sprache in aus den letzten vorchristlichen Jahrhunderten stammenden Inschriften in Algerien, Tunesien und Marokko zum Berberischen gerechnet. Ebenso dürfte auch das bis ins 17. Jahrhundert auf den kanarischen Inseln gesprochene Guanche eine Berbersprache gewesen sein.

Das Semitische ist heute mit etwa 260 Millionen Sprechern die sprecherreichste afroasiatische Sprachfamilie und wird im Nahen Osten, in Äthiopien und weiten Teilen Nordafrikas sowie auf Malta gesprochen, wobei der größte Anteil der Sprecher auf das Arabische entfällt. Einer Überlegung nach wird angenommen, dass die Urheimat der semitischen Sprachen auf der Arabischen Halbinsel lag und sich die Sprachfamilie erst durch die südarabischen Expansionen nach Äthiopien und später durch die arabischen Expansionen über Ägypten und Nordafrika und zeitweise bis nach Spanien ausbreitete. Andere verorten die Urheimat für die semitische Protosprache im nordöstlichen Afrika. Das Semitische wird allgemein in zwei Zweige aufgeteilt, wobei einer das ausgestorbene Akkadische bildet, das für die Rekonstruktion des Proto-Semitischen und damit auch der afroasiatischen Protosprache von besonderem Interesse ist. Auf den anderen, westlichen, Zweig entfallen die zentralsemitischen Sprachen wie Aramäisch, Hebräisch, Arabisch und Altsüdarabisch, die äthiosemitischen Sprachen wie Altäthiopisch und die neusüdarabischen Sprachen.

Die kuschitischen Sprachen werden in Ostafrika in den heutigen Staaten Sudan, Eritrea, Äthiopien, Somalia, Kenia, Uganda und dem nördlichen Tansania gesprochen. Die Einheit der kuschitischen Sprachen ist nicht unumstritten, da die einzelnen Zweige sich wesentlich unterscheiden; insbesondere die Zugehörigkeit des Bedscha wird diskutiert. Im Allgemeinen werden die folgenden Zweige unterschieden:

Die omotischen Sprachen werden von etwa 4 Millionen Sprechern nordöstlich des Turkanasees im südlichen Äthiopien gesprochen. Sie wurden zunächst für einen Zweig des Kuschitischen gehalten, inzwischen ist die von Harold Fleming begründete Abgliederung weitestgehend anerkannt. Die omotischen Sprachen sind schlechter erforscht als die Vertreter der anderen Zweige, dennoch kann bereits jetzt gesagt werden, dass sie in ihrer Struktur stark von den anderen afroasiatischen Primärzweigen abweichen. Die folgende Gliederung ist, von Einzelheiten abgesehen, allgemein anerkannt:

Die tschadischen Sprachen werden rund um den namensgebenden Tschadsee, hauptsächlich im Tschad, Niger und in Nigeria, gesprochen. Die bei weitem bekannteste und bedeutendste tschadische Sprache ist das Hausa, das in einem großen Gebiet um den Tschadsee als Lingua franca dient. Das Tschadische wird in vier Zweige aufgeteilt:

Die Verwandtschaft der semitischen Sprachen untereinander war Juden und Muslimen im Orient und Spanien schon lange bekannt, im christlichen Europa erkannte dies erstmals Guillaume Postel im Jahre 1538. Durch die wissenschaftliche Erforschung afrikanischer Sprachen in Europa, die in der ersten Hälfte des 17. Jahrhunderts einsetzte, wurde bald die Verwandtschaft weiterer Sprachen mit dem Semitischen erkannt. So rechnete Hiob Ludolf 1700 die äthiopischen Sprachen Altäthiopisch und Amharisch erstmals zum Semitischen, bald darauf fielen auch Ähnlichkeiten mit dem Koptischen und – nach der Entzifferung der Hieroglyphen – dem antiken Ägyptisch auf. 1781 führte August Ludwig von Schlözer den Begriff "Semitische Sprachen" ein, in Anlehnung daran prägte Johann Ludwig Krapf 1850 die Bezeichnung "Hamitische Sprachen" zunächst für die nicht-semitischen schwarzafrikanischen Sprachen. 1877 fügte F. Müller dieser Gruppe die afroasiatischen Berber- und Kuschitensprachen zu, während das ebenfalls afroasiatische Tschadisch unberücksichtigt blieb. Gleichzeitig fasste er bestimmte hamitische Sprachen und die semitischen Sprachen zum "Hamito-Semitischen" zusammen. Eine Neudefinition erfuhr der Begriff der hamitischen Sprachen durch Karl Richard Lepsius, der nun die flektierenden Sprachen Afrikas mit Genussystem unter dieser Bezeichnung zusammenfasste. Damit hatte Lepsius schon die wesentliche Masse der nichtsemitischen afroasiatischen Sprachen erfasst, jedoch erweiterte er diese Gruppe 1888 um einige nichtafroasiatische Sprachen, ebenso benutzte auch Carl Meinhof in seinem 1912 erschienenen Werk "Die Sprachen der Hamiten" "hamitisch" in einem sehr weiten Rahmen. In der Folgezeit wurde der "Hamito-Semitische Sprachstamm" um einige Sprachen reduziert und entsprach in den Grundzügen der heutigen Klassifikation, strittig blieb jedoch die Zugehörigkeit der tschadischen Sprachen, die erst in den 1950er Jahren von Joseph Greenberg endgültig etabliert wurde. Gleichzeitig prägte er den Begriff "afroasiatisch" als Ersatz für den eine ungerechtfertigte Aufteilung in hamitische und semitische Sprachen implizierenden Begriff "hamito-semitisch", welcher auf die Hamitentheorie Bezug nahm. Die heutige Form erhielt die Klassifikation des Afroasiatischen 1969 durch Harold Flemings Ausgliederung einiger äthiopischer Sprachen aus der kuschitischen Familie, die von da an als "Omotisch" einen eigenen Primärzweig des Afroasiatischen bildeten.

Die Rekonstruktion der afroasiatischen Protosprache gestaltet sich aufgrund der kurzen Überlieferungsgeschichte der meisten Zweige und der teilweise gravierenden Unterschiede zwischen den einzelnen Hauptzweigen sowohl im Bereich der Grammatik als auch im lexikalischen Bereich wesentlich schwieriger als z. B. die Rekonstruktion des Proto-Indogermanischen. Diese gravierenden Unterschiede lassen sich auf die verhältnismäßig große Zeittiefe des Proto-Afroasiatischen zurückführen, nach glottochronologischen Untersuchungen soll das Proto-Afroasiatische um 10.000-9.000 v. Chr. gesprochen worden sein.

Die Lage der Urheimat ist umstritten, da jedoch die Mehrzahl der afroasiatischen Sprachen in Afrika beheimatet ist, liegt eine Herkunft aus Afrika nahe. Besonders die östliche Sahara wird favorisiert. Aufgrund lexikalischer Übereinstimmungen des Afroasiatischen mit dem Indogermanischen, den kaukasischen Sprachen und dem Sumerischen sowie der kulturellen Stellung des rekonstruierten proto-afroasiatischen Vokabulars vertreten einige Wissenschaftler wie z. B. Alexander Militarev dagegen eine Urheimat in der Levante.

Die früheste durch Schriftquellen belegte afroasiatische Sprache ist das Alt- bzw. – genauer – Frühägyptische, dessen älteste Zeugnisse bis zum Ende des vierten vorchristlichen Jahrtausends zurückreichen. Einige Jahrhunderte später setzt die Überlieferung des Semitischen, zunächst des Akkadischen und im zweiten Jahrtausend v. Chr. westsemitischer Idiome ein. Die aus den Jahrhunderten vor Christi Geburt stammenden libyschen Inschriften aus Nordafrika werden zwar allgemein zum Berberischen gerechnet, sind aber bislang unverständlich; die frühesten Belege für das Kuschitische, Tschadische und Omotische finden sich sogar erst im Mittelalter bzw. der Neuzeit. Nur ein kleiner Teil der zahllosen tschadischen, kuschitischen und omotischen Sprachen ist heute zu Schriftsprachen geworden, unter diesen befinden sich Sprachen wie das Somali, das Hausa und das Oromo.

Die Transkription von Worten aus afroasiatischen Sprachen folgt in diesem Artikel im Wesentlichen den in der entsprechenden Fachliteratur üblichen Konventionen. Aufgrund der Unterschiede zwischen Konventionen in Semitistik, Ägyptologie und Afrikanistik ist die Umschrift daher nicht für alle Sprachen einheitlich.

Das Konsonantensystem des Proto-Afroasiatischen wird übereinstimmend mit etwa 33/34 Phonemen und teilweise auch velarisierten, palatalisierten und sonstigen Varianten rekonstruiert. Die Lautkorrespondenzen der Hauptzweige untereinander sind jedoch in zahlreichen Fällen unsicher, besonders gravierend sind die Meinungsverschiedenheiten hinsichtlich des Ägyptischen, die sich stark auf die innerägyptologische Diskussion auswirken. Beispielsweise ist umstritten, ob das Ägyptische emphatische Konsonanten aufwies und ob das ägyptische Phonem ʿ, das spätestens seit dem 2. Jahrtausend v. Chr. den Lautwert besaß, auf proto-afroasiatisches ʕ oder eine Reihe stimmhafter Plosive und Frikative zurückgeht. Dennoch sind einige allgemeine Aussagen möglich. Die meisten bzw. alle afroasiatischen Hauptzweige haben neben stimmhaften und stimmlosen konsonantischen Phonemen auch eine dritte Reihe, deren Mitglieder in Abhängigkeit von der Sprache glottalisiert, pharyngalisiert, ejektiv, velarisiert oder implosiv realisiert werden und traditionell als "emphatisch" bezeichnet werden. Oft bilden stimmhafte, stimmlose und emphatische Konsonanten triadische Gruppen. In mehreren Hauptzweigen sind pharyngale Frikative (, ) vorhanden.

Als klassisches Beispiel für ein typisch afroasiatisches Konsonantensystem kann dasjenige des Altsüdarabischen gelten. Es weist das konservativste System innerhalb des Semitischen auf und kommt darüber hinaus den für das Proto-Afroasiatische rekonstruierten Inventaren nahe:

Im Semitischen, Berberischen und Ägyptischen ist das Vorkommen von Konsonanten in Wurzeln beschränkt. Insbesondere dürfen meist unterschiedliche Konsonanten mit dem gleichen Artikulationsort nicht in einer Wurzel vorkommen.

Protosemitisch, Altägyptisch und möglicherweise das Protoberberische wiesen die drei Vokalphoneme "a", "i" und "u" auf, die Beziehungen dieser Vokale zu denen anderer Sprachen, die durchgehend mehr Vokale aufweisen, sind kaum gesichert. Nach Ehret 1995 besaß die Protosprache die Vokale "a", "e", "i", "o", "u", die lang und kurz auftreten konnten; die Rekonstruktion von Orel und Stolbova 1995 weicht ab. Zwar sind einige afroasiatische Sprachen Tonsprachen, doch ist unklar, ob das Proto-Afroasiatische deshalb ebenfalls eine Tonsprache war, wie Ehret 1995 annimmt.

Für das Semitische, Berberische und Ägyptische ist eine extensive Nutzung einer "Wurzelmorphologie" typisch, in der die lexikalische Information fast ausschließlich durch eine rein konsonantische Wurzel übermittelt wird, der die grammatische Information vor allem in Form von Vokalen beigefügt wird. Im Tschadischen und Kuschitischen findet sich nur ein begrenzt eingesetzter Ablaut; die Morphologie des Omotischen basiert dagegen fast ausschließlich auf Suffigierung und ist teilweise agglutinierend. In der wissenschaftlichen Diskussion geht man davon aus, dass das Proto-Afroasiatische zwar, beispielsweise zur Pluralbildung und zur Bildung von Aspektstämmen (siehe unten), ablautende Formen besaß, es lassen sich aber nur sehr wenige der vielen in den Sprachen vorhandenen Vokalisierungsmuster für das Proto-Afroasiatische rekonstruieren.

Für das Proto-Afroasiatische lässt sich ein zweiteiliges Genussystem mit den Genera Maskulinum und Femininum rekonstruieren, die sich nicht vollständig mit dem Sexus decken. Zu den sichersten Gemeinsamkeiten in der Nominal- und auch Pronominalmorphologie gehört ein feminines Bildungselement "t", das in vielen Sprachen an feminine Substantive suffigiert wird:
Während das Maskulinum in der Nominalmorphologie des Berberischen, Ägyptischen und Semitischen unmarkiert ist, wenden viele Sprachen in anderen Primärzweigen hierzu analog zum Femininum Morpheme wie "k" und "n" an.

Kuschitisch, Berberisch und Semitisch haben außerdem ein Kasussystem gemeinsam, von dem sich mögliche Spuren auch im Ägyptischen und Omotischen finden, wobei die Interpretation oder überhaupt Existenz des ägyptischen Befundes umstritten ist.
Die Reflexe des rekonstruierten Absolutivs fungieren in allen Sprachen als Objekt transitiver Verben und im Berberischen und Kuschitischen auch als Zitierform und extrahiertes Topik; von letzterem Gebrauch gibt es auch mögliche Reste im Semitischen. Das Subjekt wird mit Reflexen des Nominativsuffixes markiert; die Protosprache wird daher meist als Akkusativsprache angesehen. Da der Absolutiv der unmarkierte Kasus gewesen sein soll, vermuten einige Wissenschaftler, dass das Proto-Afroasiatische in einer früheren Stufe eine Ergativsprache gewesen sein könnte, in welcher das Nominativsaffix -"u" auf die Subjekte transitiver Verben begrenzt gewesen sein soll.

Alle Zweige des Afroasiatischen kennen die Numeri Singular und Plural, im Semitischen und Ägyptischen kommt ein Dual hinzu, für den sich ein Suffix *-"y" rekonstruieren lässt. Die Pluralbildung erfolgt allgemein, mit Ausnahme des Ägyptischen, in dem sich ein Suffix -"w" durchgesetzt hatte, auf vielfältige Art und Weise. Aufgrund ihrer großen Verbreitung können die Pluralsuffixe -"n", -"w" und die Pluralbildung durch Veränderung der Vokalstruktur (besonders nach dem Muster CVCaC u. ä.), Gemination und Reduplikationen als proto-afroasiatische Merkmale angesehen werden:



Über die ganze Sprachfamilie verbreitet sind außerdem einige Präfixe zur denominalen und deverbalen Nominalbildung, beispielsweise *"m"-, das zur Bildung deverbaler Substantive dient:



Ein Suffix *-"y" zur Bildung von denominalen Adjektiven, das oft mit der Genitivendung *-"i" in Verbindung gebracht wird, ist im Ägyptischen und Semitischen vorhanden:
Ähnliche Suffixe zur Bildung von Adjektiven finden sich auch im kuschitischen Bedscha.

Die Morphologie der Personalpronomina ist innerhalb des Afroasiatischen relativ konsistent. Den Kern bildete die folgende, in allen Zweigen erhaltene Reihe (Tabelle im Wesentlichen nach Hayward 2000; die angegebenen Pronomina sind oft in mehreren einzelsprachlichen Reihen verteilt. Die Dualformen im Ägyptischen und Semitischen bleiben hier unberücksichtigt.):
In allen Primärzweigen außer dem Omotischen treten diese Pronomina als klitische Objekts- und Possessivpronomina auf:
Einzelsprachlich haben formal verwandte Pronomina auch eine Reihe anderer Funktionen, so haben viele Sprachen formal ähnliche Subjektspronomina. Auch die Intransitive Copy Pronouns einiger tschadischer Sprachen sind formal ähnlich.

Daneben lässt sich wohl eine zweite Reihe rekonstruieren, deren Mitglieder frei stehen konnten und die oft aus einem Element "ʔan-" und einem auch für die Verbalkonjugation benutzten Suffix zusammengesetzt sind. Ehret 1995 rekonstruiert nur Formen für den Singular; in vielen Sprachen gibt es auch analog gebildete Pluralformen.

Ägyptisch und Semitisch haben weitere freie Pronomina, die aus den gebundenen Pronomina und -"t" zusammengesetzt sind, wie ägyptisch "kwt" > "ṯwt" „du (mask.)“, akkadisch "kâti" „dich (mask.)“.

Die Demonstrativpronomina werden in vielen afroasiatischen Sprachen aus kleinen Elementen zusammengesetzt, besonders genusanzeigenden Elementen *"n"-, *"k"- (Maskulinum), *"t"- (Femininum), die mit weiteren kleinen Elementen kombiniert werden:

In der Verbalmorphologie zeigen sich zwischen den Primärzweigen ähnliche Unterschiede wie sie schon bei der Substantivdeklination erkennbar wurden: Semitisch, Kuschitisch und Berberisch besitzen die "Präfixkonjugation", die durch Ablaut mehrere Aspektstämme unterscheidet (siehe unten) und Kongruenz mit dem Subjekt über Prä- und Suffixe markiert. Die folgende Tabelle illustriert das System der Personalaffixe der Präfixkonjugation:
Im Ägyptischen haben sich keine Spuren der Präfixkonjugation erhalten, stattdessen findet sich hier schon seit den frühesten Texten die (ägyptische) Suffixkonjugation, die keine Personalkonjugation kannte, aber das pronominale Subjekt durch suffigierte Personalpronomina ausdrückte: "sḏm=f" „er hört“, "sḏm.n nṯr" „der Gott hörte“. Die Evolution dieser Art der Konjugation ist umstritten, in Frage kommen hauptsächlich Verbalnomina und Partizipien.

Das Tschadische besitzt zwar eine Konjugation durch meist präverbale Morpheme, doch ist diese genetisch mit der Präfixkonjugation nicht verwandt, vielmehr stellen die Personapräfixe des Tschadischen modifizierte Formen der Personalpronomina dar. Beispiel: Hausa "kaa tàfi" „du gingst“. Im Omotischen erfolgt die Konjugation auf verschiedene Weise durch pronominale Elemente; das Verbalsystem des Proto-Omotischen ist höchstens in Ansätzen rekonstruierbar.

Neben der Präfixkonjugation besaß das Proto-Afroasiatische noch eine zweite Konjugationsmethode, in der die Kongruenz mit dem Subjekt ausschließlich durch Suffixe hergestellt wurde. Diese Art der Konjugation hat sich im Semitischen, Ägyptischen und Berberischen erhalten, sie verlieh dem Verb – im Akkadischen auch Substantiven und Adjektiven – offenbar eine stativische Bedeutung. Nach der Meinung einiger Wissenschaftler ist auch die Suffixkonjugation des Kuschitischen genetisch verwandt, bei ihr kann es sich aber auch, wie heute mehrheitlich angenommen wird, um eine sekundäre Bildung aus Verbalstamm plus präfixkonjugiertem Hilfsverb handeln. (Die altägyptischen und akkadischen Dualformen bleiben hier unberücksichtigt. Paradigmawörter: ägyptisch "nfr" „gut“, kabylisch "məqqər-" „groß sein“, akkadisch "zikarum" „Mann“):

Aspektstämme werden in vielen afroasiatischen Sprachen, vor allem solchen mit Reflexen der Präfixkonjugation, durch Ablaut gebildet. Meist wird davon ausgegangen, dass die Protosprache bereits mindestens zwei Aspektstämme gekannt hat: ein imperfektiver und ein perfektiver Stamm. Während der Vokal des perfektiven Stamms wohl lexikalisch festgelegt war, werden dem imperfektiven Stamm Ablaut nach "a" und/oder Gemination des vorletzten Stammkonsonantes als typische Bildungsmerkmale zugeordnet. Belege für diese Bildungsweisen finden sich in allen Hauptzweigen außer dem Ägyptischen und Omotischen, wenngleich deren Deutung als Reste eines ursprachlichen Imperfektstammes im Tschadischen angezweifelt wird:
Einige Wissenschaftler halten auch einen intransitiven Perfektstamm mit -"a"-, dessen Reflexe sich im Berberischen, Semitischen und Kuschitischen finden sollen, für rekonstruierbar. Das Bedscha (Nordkuschitisch) und die Berbersprachen besitzen in der Präfixkonjugation auch negative Verbalstämme, deren Bezug zum protosprachlichen System aber kaum erforscht ist. Der Verbalstamm, der in der Suffixkonjugation angewendet wird, hat im Semitischen und Ägyptischen bei dreikonsonantigen primären Verben die Form CaCVC-, im (Proto-)Berberischen dagegen meist *Cv̆Cv̄C. Über die Protosprache lassen sich daher keine näheren Aussagen machen. Je nach der Verteilung und Quantität der Vokale in der Präfixkonjugation lassen sich die Verben in verschiedene Klassen einteilen, die sich in ähnlicher Form auch im Ägyptischen finden und die teilweise auf die Protosprache zurückgehen können.

In fast allen afroasiatischen Sprachen werden auch Affixe und Infixe zur Bildung von Verbalstämmen angewendet, die aspektuelle, temporale und modale Unterscheidungen und in einigen tschadischen und omotischen Sprachen auch Fragesätze markieren. Bislang konnten allerdings keine derartigen Affixe für das Proto-Afroasiatische rekonstruiert werden.

Allen Hauptzweigen des Afroasiatischen ist ein hauptsächlich aus Affixen bestehendes System zur deverbalen Verbalbildung gemeinsam. Sehr weit verbreitet ist ein Affix *-"s"-, das zur Bildung kausativer, faktitiver und transitiver Verben dient:

Weitere weit verbreitete Affixe sind *-"t"- und *-"m"-, die Reflexivität, Reziprozität, Passivität, Intransivität und das Medium ausdrücken:

Reduplikation dient in vielen Sprachen zum Ausdruck verbaler Intensität oder Pluralität:

Einige Merkmale der Syntax sind innerhalb des Afroasiatischen besonders weit verbreitet. Ob es sich hierbei auch um Merkmale der Protosprache handeln könnte, wurde bisher nicht umfassend untersucht. In den meisten Sprachen folgen Objekte dem Verb, pronominale Objekte stehen dabei oft vor nominalen Objekten. Sind beide Objekte pronominal, folgt das direkte dem Indirekten; indirekte nominale Objekte folgen jedoch direkten. Diese drei Regeln sind im älteren Ägyptisch, vielen semitischen Sprachen, dem Tschadischen und Berberischen nahezu universell gültig:

Der für die Protosprache rekonstruierbare Wortschatz dürfte mehrere hundert Lexeme groß sein, seine Rekonstruktionen (Diakonoff u. a. 1993-7, Ehret 1995, Orel-Stolbova 1995) weichen jedoch, nicht zuletzt aufgrund der Unsicherheiten hinsichtlich der Rekonstruktion der Lautkorrespondenzen, stark voneinander ab. Nur für wenige Lexeme gibt es Belege in allen sechs Primärzweigen. Beispiele für mögliche Wortgleichungen gibt die folgende Tabelle.

Die Rekonstruktionen proto-afroasiatischer Wurzeln wurden Ehret 1995 entnommen (dort: ă=tiefer Ton; â=hoher Ton). Die einzelsprachlichen Reflexe sind verschiedenen Veröffentlichungen entnommen. Einzelne Reflexe erfordern gegensätzliche Lautentsprechungen, so fordert die Gleichung "jdmj" „roter Leinenstoff“ < Proto-Afroasiatisch *dîm-/*dâm- „Blut“ die Beziehung ägyptisch "d" < proto-afroasiatisch *d, während ägyptisch "ˁ3j" „groß sein“ als Reflex von *dăr- „größer werden/-machen“ die Beziehung ägyptisch "ˁ" < proto-afroasiatisch *d voraussetzt. Folglich kann nur eine dieser beiden Gleichungen richtig sein (sofern man keine komplexeren Regeln für *d rekonstruiert), in der Forschung werden beide Lautbeziehungen vertreten. Wo die Bedeutung des einzelsprachlichen Reflexes mit der rekonstruierten Wurzelbedeutung übereinstimmt, wurde diese nicht wiederholt.

Überblick

Lexikon und Phonologie



</doc>
<doc id="312" url="https://de.wikipedia.org/wiki?curid=312" title="Afrikanische Sprachen">
Afrikanische Sprachen

Der Begriff Afrikanische Sprachen ist eine Sammelbezeichnung für die Sprachen, die auf dem afrikanischen Kontinent gesprochen wurden und werden. Die Bezeichnung „Afrikanische Sprachen“ sagt nichts über eine sprachgenetische Verwandtschaft aus (→ Sprachfamilien der Welt, Sprachfamilie).

Zu den afrikanischen Sprachen zählen zunächst die Sprachen, die ausschließlich auf dem afrikanischen Kontinent gesprochen werden. Das sind die Niger-Kongo-Sprachen, die nilosaharanischen Sprachen und die Khoisan-Sprachen. Auch die afroasiatischen Sprachen rechnet man traditionell insgesamt zu den „afrikanischen Sprachen“ hinzu, obwohl Sprachen der semitischen Unterfamilie des Afroasiatischen auch oder nur außerhalb Afrikas – im Nahen Osten – gesprochen wurden und werden. Zum einen sind die semitischen Sprachen wesentlich auch in Afrika vertreten (z. B. das Arabische, viele Sprachen Äthiopiens und Eritreas), zum anderen stammt die afroasiatische Sprachfamilie wahrscheinlich aus Afrika. In diesem erweiterten Sinne gibt es 2.138 afrikanische Sprachen und Idiome, die von rund 1,101 Mrd. Menschen gesprochen werden. Die Sprache Madagaskars – Malagasy – gehört zur austronesischen Sprachfamilie und wird deshalb normalerweise nicht zu den „afrikanischen Sprachen“ gerechnet, ebenfalls nicht die europäischen indogermanischen Sprachen der Kolonisatoren (Englisch, Afrikaans, Französisch, Spanisch, Portugiesisch, Italienisch und Deutsch).

Die Afrikanistik ist die Wissenschaft, die sich mit den afrikanischen Sprachen und Kulturen befasst.

Seit den 1950er Jahren werden die afrikanischen Sprachen auf Grund der Arbeiten von Joseph Greenberg in vier Gruppen oder Phyla eingeteilt:


Die Forschung betrachtet die Greenberg'sche Klassifikation als methodisch unzureichend, um tatsächliche sprachgenetische Aussagen zu formulieren, die ähnlich belastbar sind, wie die sprachgenetische Aussagen zu anderen Sprachfamilien. Jedoch dient diese Schematik heute mangels Alternativen übereinstimmend als pragmatisches Ordnungsprinzip, z. B. für Bibliothekssystematiken.

Die innere Struktur dieser Sprachgruppen wird in den Einzelartikeln behandelt. Dieser Artikel beschäftigt sich mit der Klassifikation der afrikanischen Sprachen insgesamt.

Ob diese Sprachgruppen oder Phyla genetisch definierte Sprachfamilien bilden, wird in der Afrikanistik nach wie vor zum Teil strittig diskutiert. Jedenfalls geht auch das einzige aktuelle Standardwerk über afrikanische Sprachen insgesamt – "B. Heine and D. Nurse, African Languages – An Introduction (Cambridge 2000)" – herausgegeben und verfasst von führenden Afrikanisten unserer Zeit (B. Heine, D. Nurse, R. Blench, L.M. Bender, R.J. Hayward, T. Güldemann, R. Voßen, P. Newman, C. Ehret, H.E. Wolff u. a.) von diesen vier afrikanischen Phyla aus.

Dass Afroasiatisch und Niger-Kongo jeweils eine genetische Einheit bilden, gilt als nachgewiesen und wird allgemein akzeptiert.

Auch das Nilosaharanische wird von den Spezialisten dieses Gebiets (zum Beispiel L.M. Bender und C. Ehret) als gesicherte Einheit aufgefasst, deren Protosprache in Grundzügen zu rekonstruieren ist. Diese Meinung wird jedoch nicht von allen Afrikanisten geteilt, obwohl der Kern des Nilosaharanischen – Ostsudanisch, Zentralsudanisch und einige kleinere Gruppen – als genetische Einheit ziemlich unumstritten ist. Von wenigen bezweifelt wird die Zugehörigkeit der Sprachen Kunama, Berta, Fur und der Maba-Gruppe zum Nilosaharanischen. Stärkere Zweifel gelten für die „Outlier-Gruppen“ Saharanisch, Kuliak und Songhai, deren Zugehörigkeit zum Nilosaharanischen von mehreren Forschern bestritten wird. Dennoch kann vor allem nach den Arbeiten von Bender und Ehret keine Rede davon sein, dass das Konzept der nilosaharanischen Sprachen als Ganzes gescheitert sei. Selbst wenn sich die eine oder andere Außengruppe doch als eigenständig erweisen sollte, so wird der größere Teil des Nilosaharanischen als genetische Einheit Bestand haben.

Anders ist die Situation beim Khoisan: die Autoren dieses Abschnitts im oben genannten Übersichtswerk (T. Güldemann und R. Voßen) halten die auf Greenberg und mehrere Vorgänger zurückgehende Vorstellung einer genetischen Einheit der Khoisan-Sprachen nicht aufrecht, sondern gehen stattdessen von mindestens drei genetisch unabhängigen Einheiten (Nordkhoisan oder Ju, Zentralkhoisan oder Khoe, Südkhoisan oder ) aus, die früher zum Khoisan gerechneten Sprachen Sandawe, Hadza und Kwadi werden als isoliert betrachtet. Die Khoisan-Gruppe bilde einen arealen Sprachbund typologisch verwandter Sprachen, der durch lange Kontaktphasen entstanden sei. Diese Einschätzung der Khoisan-Gruppe als Sprachbund findet heute weite Zustimmung.

Die folgende Darstellung gibt einen tabellarischen Überblick über die Forschungsgeschichte der afrikanischen Sprachen. Die verwendeten Gruppenbezeichnungen sind teilweise modern, damit auch der Nichtfachmann den Zuwachs – oder Rückschritt – der gewonnenen Erkenntnisse verfolgen kann.



Methodisch ist seine Einteilung aufgrund der gewählten Methode (Lexikostatistik, bzw. Lexikalischer Massenvergleich) hochumstritten, da diese Methode erstens rein statistisch vorgeht und zweitens unzureichendes Material zugrunde legt (ausschließlich Wörterlisten meist zweifelhafter Güte) und drittens in Zeitalter zurückreicht, die mit anderen linguistischen oder archäologischen Methoden niemals erfasst geschweige denn bestätigt werden könnten. Daher wird die Greenberg-Klassifikation heute zwar mangels Alternative als Ordnungssystem (etwa zur Herstellung von systematischen Bibliothekskatalogen) weitgehend akzeptiert, ihr genetischer Aussagegehalt jedoch nur mit starken Vorbehalten angenommen.

Die Staatsgrenzen stimmen in Afrika nicht mit den Grenzen von Sprachen und Volksgruppen überein. Es haben sich, bis auf wenige Ausnahmen, keine einheitlichen Kulturnationen herausgebildet, so dass es keine Verbindung von Sprache, Volk und Staat gibt.

Die soziolinguistische Situation in Schwarzafrika ist in weiten Teilen durch eine Triglossie geprägt. Es haben sich neben den zahlreichen einheimischen Sprachen der einzelnen Volksgruppen (→ Vernakularsprache) infolge Wanderungsbewegungen, Handelswesen, vorkolonialer Reichsbildung, religiöse Missionierungen und teilweise auch durch die Unterstützung der Kolonialherren im Rahmen einer „Stammesselbstverwaltung“ und der britischen „Politik der mittelbaren Herrschaft“ bestimmte Sprachen als afrikanische Verkehrssprachen herausgebildet, welche die Aufgaben übernehmen, die Verständigung zwischen den Angehörigen der verschiedenen Volksgruppen zu ermöglichen. Sie spielen insbesondere eine wichtige Rolle in afrikanischen Städten, wo eine Bevölkerung lebt, die anders als die Landbevölkerung nicht mehr zuvörderst durch eine Volksgruppenzugehörigkeit geprägt ist. Diese Verkehrssprachen sind auch im Volksbildungswesen von Bedeutung und werden in einigen Medien und in der Literatur verwandt. Zu diesen Verkehrssprachen werden vor allem Swahili in Ostafrika, Hausa, Fulfulde, Kanuri, Igbo, Yoruba und die Mandesprachen Bambara, Dioula und Malinke in Westafrika gezählt. In Zentralafrika spielen Lingála, Kikongo und Sango eine Rolle. Neben den Vernakularsprachen und den afrikanischen Verkehrssprachen sind seit der Kolonialherrschaft Französisch, Englisch und Portugiesisch eingeführt worden. Diese Sprachen werden in den meisten schwarzafrikanischen Staaten weiterhin als Amts-, Gerichts- sowie Lehr- und Wissenschaftsprachen in den Universitäten und höheren Lehranstalten verwendet. Die Kenntnisse der europäischen Sprachen ist je nach Bildungsgrad, Land und Grad der Verstädterung recht unterschiedlich. Die Politik der Exoglossie erscheint vielen Staaten wegen der Sprachenvielfalt als vorzugswürdig. Insbesondere sollen der Vorwurf der Benachteiligung der anderen, nicht staatstragenden Ethnien (→Tribalismus) und eine wirtschaftliche Isolierung vermieden werden. Ausnahmen von der Triglossie sind nur Burundi und Ruanda. In Kenia, Uganda und Tansania wird Swahili gefördert und ist auch als Amtssprache verankert.

Gänzlich anders gestaltet sich die Lage in Nordafrika und am Horn von Afrika. Die vor den islamischen Eroberungen der Araber im Magreb vorherrschenden Berbersprachen sind durch das Arabische in den Hintergrund gedrängt worden. In Ägypten starb das Ägyptisch-Koptische aus. Arabisch ist für die weitaus meisten Nordafrikaner Muttersprache. Anders als in Schwarzafrika haben die nordafrikanischen Staaten die Sprache der Kolonialherren, Französisch, durch Arabisch als Amtssprache ersetzt. In Äthiopien wirkt Amharisch als Verkehrssprache; eine Kolonialsprache gibt es nicht. In Somalia ist Somali vorherrschend. Italienisch hat dort sehr stark an Boden verloren.




</doc>
<doc id="314" url="https://de.wikipedia.org/wiki?curid=314" title="Atomhülle">
Atomhülle

Die Atomhülle bzw. Elektronenhülle ist der äußere aus Elektronen bestehende Teil eines Atoms. Die Unterteilung eines Atoms in Atomkern und Atomhülle geht auf Ernest Rutherford zurück, der 1911 in Streuexperimenten zeigte, dass Atome aus einem winzigen, kompakten Kern umgeben von einer Hülle bestehen.
Wegen der geringen Masse der Elektronen bedingt die Unschärferelation, dass die Atomhülle etwa 20.000 bis 150.000 mal größer als der Atomkern ist. Trotz dieser großen räumlichen Ausdehnung beherbergt die Atomhülle aber nur etwa 1/2000 bis 1/6000 der Masse des gesamten Atoms.

Die Struktur der Elektronenhülle bestimmt nicht nur weitgehend die Größe der Atome, sondern auch die chemischen Eigenschaften. Für die chemischen Bindungen ist insbesondere der äußere Teil der Atomhülle verantwortlich, die Valenzschale. Die Verteilung der Elektronen in der Elektronenhülle eines Atoms auf verschiedene Energiezustände bzw. Aufenthaltsräume (das quantenmechanische Modell ist das Atomorbital) wird in den Artikeln Elektronenkonfiguration bzw. im entsprechenden Abschnitt des Artikels Atom behandelt.

Die Elektronenhülle eines Atoms wird in vielen einführenden Büchern zur Atomphysik ausführlich erklärt. Beispielhaft seien hier genannt


</doc>
<doc id="315" url="https://de.wikipedia.org/wiki?curid=315" title="Asselspinnen">
Asselspinnen

Die Asselspinnen (Pycnogonida, auch Pantopoda) werden trotz ihres Namens nicht zu den Spinnentieren gerechnet, sondern bilden eine eigene Klasse innerhalb der Kieferklauenträger (Chelicerata). Sie sind eine rein marine Tiergruppe mit einer Verbreitung in allen Weltmeeren und einem Verbreitungsschwerpunkt im Südpolarmeer Südlicher Ozean. Die Artenzahl wird auf über 1300 Arten geschätzt.

Die Pantopoda fallen vor allem durch einen, im Verhältnis zu den Beinen winzigen, stabförmigen Körper auf, der oft nur ein schmales Verbindungsstück zwischen den Beinbasen darstellt. Der Vorderkörper teilt sich in den ungegliederten Prosoma, der die ersten vier Extremitätenpaare trägt (darunter das erste Laufbeinpaar) und einen, durch Querfurchen in mehrere Segmente unterteilten, hinteren Abschnitt dem die weiteren Laufbein-Paare anhängen. In der Regel sind es vier, bei einigen Arten bis zu sechs Paare. Der Hinterkörper (Abdomen, Opisthosoma) ist extrem reduziert und meist nur eine kleine Ausbuchtung ohne Anhänge der am Ende den After trägt. Nur bei den fossilen Palaeopantopoden ist er noch sackförmig und lässt drei bis fünf Segmente erkennen. Neben Arten mit sehr langen Gliedmaßen, kommen auch kompaktere Formen vor. Die kleinsten Asselspinnen haben eine Größe von 1 bis 10 mm, die größten unter den in der Tiefsee Lebenden werden bis zu 900 mm groß. Die Länge des Körpers liegt zwischen 0,8 und 100 mm.

Die Extremitäten ähneln denen anderer Chelicerata, doch sucht man bei ihnen die Laden (Enditen) vergeblich. Unterschieden werden vier verschiedene Arten von Gliedmaßen. Das erste Paar, die scherenbesetzten Cheliceren (bei den Pycnogonida meist Cheliforen genannt), bestehen meist aus drei, seltener aus vier Gliedern. Ihnen kommt eine Bedeutung bei der Ernährung zu. Es folgen tasterartige Palpen von wechselnder Länge (bis zehngliedrig), die mit ihrer dichten Behaarung der Reizaufnahme dienen.

Das dritte Extremitätenpaar ist das sogenannte Brutbeinpaar. Es entspringt ventral, ist gleichfalls tasterartig und dient beim Männchen als Eiträger (Oviger). Die Eipakete werden vom Männchen gebildet, indem es mit seinen Brutbeinen in der vom Weibchen abgelegten Eimasse rührt und diese mit von den Beinen abgegebenem Kitt zu Klumpen verklebt. Je nach Größe der Eier kann ein Paket 50 bis 1000 Eier beinhalten. 
Mit Hilfe eines aus den letzten vier Gliedern des Beines bestehenden Ringes, der mit vielen Borsten besetzt ist, gewährleistet das Männchen den sicheren Transport. Die Borsten dienen dem Männchen z. B. zur Reinigung der Eier.
Bei den Weibchen hingegen ist dieses Beinpaar häufig zurückgebildet oder fehlt gänzlich.

Eine Besonderheit haben die nächsten Extremitätenpaare, die Laufbeine. Sie können, ähnlich denen der Weberknechte, bei Gefahr abgeworfen werden.
Das Abwerfen hat zwei Vorteile: zum einen greifen Feinde die ihnen überlassenen langen Beine an, während die Asselspinne die Flucht ergreift; zum anderen ist es von Vorteil ein verletztes Bein abzuwerfen, anstatt es mit sich herumzutragen und einen Flüssigkeitsverlust zu riskieren. Die Bruchstelle schließt sich sehr rasch, und das Bein wächst nach der nächsten Häutung nach. 
Die aus neun Gliedern bestehenden langen Laufbeine sind meist 4-, vereinzelt 5- (7 Arten, darunter Pentanymphon) oder selten sogar 6-paarig (2 Arten, Gattung Dodecalopoda). Diese mehrbeinigen Arten sind jeweils nahe mit achtbeinigen verwandt und gelten als sekundäre Abweichungen des Grundbauplans.
Das Endglied, der Praetarsus, ist meist klauenförmig ausgebildet, dazu noch oft mit einer Nebenklaue besetzt und dient unter anderem dem Festhalten der Nahrung.
Die drei vorderen Extremitäten-Paare können sehr variabel ausgebildet sein oder aber auch, genau wie Kiemen und Fühler, ganz fehlen.

Auch diese Vertreter der Kieferklauenträger besitzen, genau wie andere Gliederfüßer, ein Exoskelett mit Chitin-Einlagerungen. Wie bei den Spinnen werden in die darunterliegende Haut Exkrete eingelagert, sodass die Tiere oft bunt gezeichnet sind. Aber auch Vorratsstoffe werden dort eingelagert. Das Exoskelett ist sehr undurchlässig und manchmal außerordentlich dick. Dagegen fehlt jedwede Einlagerung von Kalk, was zur Folge hat, dass die Haut der Pantopoda leder- oder pergamentartig ist.

In der Haut liegen zahlreiche Drüsen, wie z. B. Kittdrüsen an den Femora der Beine der Männchen und Spinndrüsen an den Cheliceren der Larven.
Die Sinnesorgane sind gering entwickelt. Im Vorderkörper liegen auf einem Augenhügel vier kleine Linsenaugen (Medianaugen). An Hautsinnesorganen sind nur Sinnesborsten bekannt. Spaltsinnesorgane wurden bei dieser Klasse noch nicht gefunden.

Das Nervensystem ist primitiver als das anderer Chelicerata, da die Bauchganglien weitgehend getrennt bleiben. Das Unterschlundganglion innerviert Palpen- und Brutbeinsegment. Ein oder zwei Abdominalganglien treten während der Entwicklung noch auf, verschmelzen jedoch mit dem letzten Rumpfganglion. Die Ganglien im Rumpf sind meist deutlich sichtbar. Von diesen kann man oft starke Nervenstränge in die Beine ziehen sehen.

Der Mund liegt auf einem umfangreichen Rüssel (Proboscis), der ventralwärts oder nach vorn ragt. Der Rüssel besteht innen aus drei Längsteilen (Antimeren), einem dorsalen und zwei ventrolateralen. Der dreieckige Mund an der Rüsselspitze selbst ist mit drei borstenbesetzten Platten (Lippen) und drei beweglichen Chitinhaken besetzt.
Der im Rüssel liegende Teil des Darms wird als Pharynx bezeichnet. Sein dreikantiges Lumen wird durch radiäre, zur Rüsselwand ziehende Muskeln erweitert, und der hintere Bereich wird durch in das Lumen ragende Chitinhaken zu einem Reusenapparat. Ein Oesophagus führt in den Mitteldarm. Da der Rauminhalt des Rumpfes bedeutend kleiner ist als der der Beine, gibt es zusätzlich lange Ausläufer des Mitteldarms (Blindsäcke) die bis in die Beine, bei manchen Arten aber auch bis in die Cheliceren und Rüssel ziehen. Das hat zur Folge, dass die aufnehmende und verdauende Oberfläche beträchtlich vergrößert wird.
Die Speiseröhre, die von einem Apparat aus starren und beweglichen Borsten besetzt ist, der den groben aufgesogenen Nahrungsbrei fein zerkleinert, bis nur noch Zellbruchstücke zurückbleiben, ist lang und eng. Von hier aus gelangen diese in den Darm und werden hier von den Darmzellen, in denen die eigentliche Verdauung erfolgt, resorbiert.
Der gerade Endteil des Darmes mündet mit endständigem After.

Die Exkretion läuft über sogenannte Nephrocysten, den Ausscheidungszellen, ab. Nephridien und Malpighische Gefäße fehlen diesen Vertretern der Chelicerata völlig.

Das Blutgefäßsystem besteht nur aus einem Rückengefäß. Es weist zwei Paar Einströmöffnungen (Ostien) für das farblose Blut auf und durchzieht den Rumpf vom Hinterende bis zur Region der Augenhügel und ist dorsal mit breiter Fläche an der Rückenwand, ventral am Pericardialseptum angewachsen. Oft kommt noch ein unpaares Ostium am Hinterende dazu. Das Pericardialseptum durchzieht den Rumpf horizontal dicht über dem Darm und erstreckt sich auch in die Beine.

Da den Asselspinnen Kiemen fehlen, wird die Atmung von einem anderen Organ übernommen, wahrscheinlich dem Darm oder feinen Blutkapillaren, in die der Sauerstoff diffundiert.

Die Gonaden entstehen ventral am Pericardialseptum, erstrecken sich aber bis in die Beine, die auch den größten Anteil der Geschlechtsorgane enthalten. An den Coxen der Beine liegen auch die Genitalöffnungen, meist im zweiten Glied, daher findet man legereife Eier nie im Rumpf, sondern nur in den Beinen. Interessanterweise sind oft mehrere Paare von Öffnungen vorhanden, vielfach an allen Beinpaaren. Bei manchen Gattungen sind sie auf bestimmte Beinpaare beschränkt, am häufigsten jedoch auf die letzten. Zahl und Lage der Genitalöffnungen können in Abhängigkeit vom Geschlecht variieren.

Die abgelegten Eier, die je nach Art eine Größe von 0,02 bis 0,7 mm erreichen, werden wie schon beschrieben vom Männchen getragen. Die Entwicklung zeigt manche Eigenarten und ist auch innerhalb der Pantopoden nicht gleichartig. Die Furchung ist zunächst total, und kann je nach Dottergehalt äqual oder inäqual ausfallen. Früher oder später verschmelzen aber Zellen zu syncytialen Massen. Die Keimblätterbildung ist schwer verständlich. Dorsal werden große Zellen ins Innere verlagert, welche Entoderm (zum Teil von einer Urentodermzelle ausgehend) und Mesoderm bilden. Später tritt ventral die Längsrinne auf, die dem Blastoporusgebiet anderer Arthropoden entspricht. Das Mesoderm scheint sich stets über ein einfaches Streifenstadium in Muskeln und Bindegewebe umzuwandeln.

Die Embryonalentwicklung führt zu einer typischen Larve, der Protonymphon-Larve. Bei den drei Extremitätenpaaren, über die die Larve anfangs verfügt, handelt es sich um Cheliceren, Palpen und Brutbeine. Die Cheliceren der Protonymphon-Larve tragen eine seitlich in eine Röhre mündende Spinndrüse und zum Teil Scherendrüsen. Die beiden anderen Extremitäten sind jedoch nur dreigliedrige Haken, die später mehr oder weniger zurückgebildet werden, während die definitiven Palpen und Brutbeine durch Neubildung entstehen. Herz und After fehlen der Larve. Die Weiterentwicklung erfolgt durch schrittweise Bildung der Beine am Hinterkörper, die Stadien sind durch Häutungen getrennt. Nur selten bleiben die Larvalstadien an den Brutbeinen der Männchen (zum Beispiel Chaetonymphon), meist verlassen sie als Protonymphon die Brutbeine und leben in der nächsten Phase als Ekto- oder Endoparasiten (Phoxichilidium, Anoplodactylus) an anderen Tieren, vor allem Polypen.

Die Vertreter der Pycnogonida sind ausschließlich marin zuhause und leben zwischen Bodenbewuchs aller Art. Dabei sind sie nicht an eine bestimmte Tiefe gebunden, sondern sind sowohl an der Oberfläche, als auch in der Tiefsee in Tiefen von mehr als 4000 m heimisch. Einige besonders kleine Arten leben im Sandlückensystem (Interstitial). Abhängig sind sie einzig von einem bestimmten Salzgehalt, der bei ungefähr 3,5 % liegt. Des Weiteren bevorzugen sie kaltes Wasser. Daher findet man sie auffallend häufig in der Antarktis (etwa 250 der 1000 bekannten Arten, davon 100 Arten endemisch in der Antarktis und rund 60 in den subantarktischen Gewässern); dort auch in den großen Formen. In den warmen Meeren, zum Beispiel an den Küsten des Mittelmeeres, fand man bisher nur kleine Exemplare mit höchstens 30 mm Durchmesser.
Alle Pantopoden sind durchweg träge Tiere, wobei sich die kurzbeinigen, plumpen Arten durch ganz besondere Schwerfälligkeit auszeichnen. Sie lassen sich, gibt man sie in eine Schale, zu Boden sinken und bleiben regungslos liegen. Die schlankeren Formen können jedoch mehr oder weniger grazil schwimmen und sich auf diese Weise längere Zeit im freien Wasser aufhalten.
Alle Asselspinnen sind Kletterer, die sich langsam und bedächtig bewegen und sich an jedem geeigneten Gegenstand festklammern können. Die Fortbewegungsgeschwindigkeit ist recht langsam (ca. 1 bis 3 mm/s), kann jedoch in Gefahrensituation enorm gesteigert werden.

Alle Vertreter der Asselspinnen ernähren sich räuberisch. Zu ihrer Nahrung gehören ausschließlich weichhäutige Tiere, so beispielsweise Schnecken, Moostierchen und Schwämme, aber vor allem Hydroidpolypen. 
Die Nesselzellen der Polypen scheinen auf Asselspinnen keinerlei Wirkung zu haben. Die Nahrung, zum Beispiel ein Polypenköpfchen, wird mit einer Schere gefasst und mit dem Rüssel ausgesogen. Dieser Vorgang kann bis zu 10 Minuten dauern. Des Weiteren wurde auch die Aufnahme von Ruderfußkrebsen und Vielborstern beobachtet. Die Ruderfußkrebse werden mit Hilfe der Greifklauen der Laufbeine gepackt, zum Mund geführt und ausgesaugt. Außerdem wurde in Experimenten das Fressen von Muschelfleisch untersucht. Auch hier werden die Greifklauen der Beine eingesetzt, um das Fleisch festzuhalten und die Nahrung anschließend über den Proboscis aufzunehmen. Dieser Vorgang kann sich über mehrere Stunden erstrecken.

Eine Reihe von Arten lebt ektoparasitisch (auf Hohltieren, Schwämmen, Mollusken und Stachelhäutern).

Bei der Knotigen Asselspinne ("Pycnogonum litorale") wurde erstmals in einer marinen Räuber-Beute-Beziehung eine Methode der chemischen Abwehr gefunden. Es konnte nachgewiesen werden, dass die Gemeine Strandkrabbe ("Carcinus maenas"), die sonst nahezu alles frisst, Asselspinnen meidet, weil diese in allen Stadien einen sehr hohen Gehalt an 20-Hydroxy-Ecdyson haben. Diese Substanz ist ein Hormon, das bei Insekten und Krebstieren die Häutung (Ecdysis) auslöst. Für die Strandkrabben sind häufige Häutungen nachteilig, nicht zuletzt weil frisch gehäutete Tiere noch sehr weiche Mundwerkzeuge haben, die eine Nahrungsaufnahme für eine gewisse Zeit unmöglich machen. Die Asselspinnen steuern ihre Häutungen offenbar anders. Ein Häutungshormon für diese Tiere ist bisher noch unbekannt.

Auch für die Asselspinnen haben sich, wie bei allen Tier- und Pflanzengruppen, mehrere Namen erhalten. So wurden sie um 1815 vom Engländer William Elford Leach als "Podosomata" bezeichnet, was so viel wie „Körper (nur) aus Beinen“ bedeutet.
1863 wiederum beschrieb sie der deutsche Zoologe Carl Eduard Adolph Gerstäcker als "Pantopoda", was man frei mit „die Allesbeinigen“ übersetzen kann. Ein gewisses Maß an Unsicherheit drückt der deutsche Name "Asselspinnen" aus. Lange Zeit wurden sie zu den Krebsen gestellt, da sie aber auch spinnenförmig aussehen und auch einige Gemeinsamkeiten aufwiesen, reihte man sie in die Klasse der Spinnentiere ein. Da man aber ihre Eigentümlichkeit betonen wollte, nannte man sie letztendlich "Asselspinnen".

Zwischenzeitlich aufgekommene Vermutungen, die Asselspinnen bildeten einen eigenständigen Stamm basal zu allen anderen Arthropoda gelten heute als widerlegt. Die meisten Taxonomen setzen den Asselspinnen als basalster Gruppe der Chelicerata alle anderen Vertreter als Schwestergruppe gegenüber. Damit sind die Pfeilschwanzkrebse und die Spinnentiere miteinander näher verwandt als jede dieser Gruppen mit den Asselspinnen.

Die so dargestellte traditionelle Systematik (basierend vor allem auf morphologischen Reihen mit fortschreitender Reduktion einzelner Gliedmaßen) wird von moderneren molekularen und kladistisch-morphologischen Studien nur teilweise gestützt. Die meisten Ordnungen und einige Familien erwiesen sich als paraphyletisch.

Fossilien von Asselspinnen werden nur sehr selten gefunden. Eine Reihe problematischer Fossilien mit stark abweichendem Körperbau gelten heute nicht mehr als Stammgruppenvertreter, sondern wurden anderen Verwandtschaftskreisen zugeordnet. Die verbleibenden Arten können aufgrund ihres Bauplans modernen Ordnungen zugewiesen werden, ihre früher übliche Zusammenfassung als "Palaeopantopoda" ist demnach eine künstliche Einteilung. Besonders reich an fossilen Arten ist der unterdevonische Dachschiefer des Hunsrücks. Die ältesten Formen sind in Körpererhaltung (d. h. nicht nur als Abdruck) erhaltene Larven aus dem Oberkambrium von Schweden (dabei wurde die Körperwand durch Calciumphosphat ersetzt und das Tier anschließend in Kalkstein eingebettet, sog "Orsten"-Fossilien). Die adulten Formen dazu sind unbekannt.



</doc>
<doc id="316" url="https://de.wikipedia.org/wiki?curid=316" title="Astronomisches Objekt">
Astronomisches Objekt

Ein astronomisches Objekt (auch Himmelsobjekt oder Himmelskörper) ist ein Objekt, das von der Astronomie und der Astrophysik untersucht wird. Die Konsistenz der Objekte ist überwiegend

Dies sind Objekte der Kosmologie, wie die prinzipiellen Strukturen des Universums (Filamente und Voids) und – bisher noch – hypothetische oder in ihrer Natur noch nicht hinreichend geklärte Objekte, wie Schwarze Löcher oder Dunkle Materie.
Eine Orientierung einzelner Fachgebiete der Astronomie:

"Objekte in Erdnähe":
"Solare Objekte":
"Extrasolare Objekte": Objekte außerhalb der Grenzen des Sonnensystems (Deep-Sky-Objekte).


</doc>
<doc id="317" url="https://de.wikipedia.org/wiki?curid=317" title="Altersbestimmung (Archäologie)">
Altersbestimmung (Archäologie)

Bei der Altersbestimmung von archäologischen Funden gibt es verschiedene Datierungsmethoden, die man in zwei große Gruppen unterteilen kann, relative und absolute Altersbestimmung.

Im Regelfall gilt als Leitprinzip, dass untere Schichten eher abgelagert worden sind als obere, und somit ältere Schichten unter jüngeren zu finden sind (Stratigraphisches Prinzip). Ähnlich wie bei relativen Altersbestimmungen der Geologie wird nur die Abfolge der Schichten festgestellt, ohne das tatsächliche Alter zu messen.

Ausnahmen kommen etwa bei Umlagerungen oder Überschiebungen vor: So könnte in einem archäologischen Befund älteres Material durch Umschichtungen bei Bauarbeiten oder durch Erosion an einem Hang über jüngerem Material abgelagert worden sein. In der Geologie kommen vergleichbare Ereignisse vor: Ein Gesteinsblock wird durch tektonische Prozesse angehoben, und über einen (nicht angehobenen) jüngeren geschoben. Derartige Prozesse sind selten und durch Aufnahme eines Gesamtbildes identifizierbar.

Die zeitliche Änderung von Gegenstandsformen, verwendeten Materialien oder Handwerkstechniken führt dazu, dass sich die Zusammensetzung der Fundgegenstände in geschlossenen Funden wie Gräbern, Abfallgruben und Depotfunden in charakteristischer Weise verändert. Eine solche relativchronologische Abfolge lässt sich mit Kombinationsstatistiken oder in einer Seriation darstellen.

Veränderliche prozentuale Anteile von Artefakttypen können auch für die relative Chronologie ganzer archäologischer Kulturen oder Zeithorizonte ausschlaggebend sein. Für die relative Datierung ist also nicht die Anwesenheit oder das Fehlen eines einzelnen Objekts entscheidend, sondern seine relative Häufigkeit. Modellhaft lässt sich das so begründen, dass eine Form in einer Zeitstufe erfunden wird aber noch selten ist, in der nächsten Stufe ist sie dann allgemein bekannt und wird viel benutzt und in der nächsten wird sie bereits langsam von einer neuen Form verdrängt.

Eine chorologische Methode, die bei der Auswertung von Gräberfeldern erfolgreich sein kann, ist die so genannte Horizontalstratigraphie. Wenn in einem Bestattungsplatz unterschiedliche Regionen zu unterschiedlichen Zeiten benutzt wurden, sind auch chronologisch relevante Grabbeigaben einer gewissen Zeit nur in der zugehörigen Region des Gräberfeldes zu finden. Die Kartierung der Beigaben auf dem Gräberfeldplan erlaubt es dann, unterschiedliche Belegungsphasen im Kartenbild zu erkennen.

Wenn Funde von Frühmenschen (Hominiden) oder ihren Erzeugnissen in geologische Schichten eingebettet sind, lassen sie sich über diese datieren. Überreste aus fossilführenden Schichten lassen sich daher mit Hilfe von Leitfossilien genauer einordnen. Auch Großereignisse, die charakteristische überregionale Merkmale erzeugen, können den Vergleich von Schichten oder Gesteinen ermöglichen. Zum Beispiel hat sich eine Iridium-Schicht, die beim Aufprall eines großen Meteoriten entstanden ist, weltweit in alle Gesteine der damaligen Zeit eingelagert. Auch Ablagerungen von Vulkanasche lassen sich manchmal großräumig einer konkreten Eruption wie der des Vulkans vom Laacher See zuweisen.

In der Archäologie werden zahlreiche absolute Datierungsmethoden verwendet. Diese beruhen auf unterschiedlichen Ansätzen. Welche dieser Ansätze anwendbar und sinnvoll sind, entscheidet sich im Einzelfall des jeweiligen Befundes.

Viele Methoden zur Altersbestimmung in der Geologie liefern nur sehr grobe Absolutdaten. So waren etwa die heute ausgestorbenen Radionuklide Al oder Mn bei der Entstehung des Sonnensystems noch vorhanden. Mit diesen Methoden können z.B. das Entstehungsalter von Meteoriten oder einzelner Bestandteilen von Meteoriten relativ zueinander bestimmt werden. Erst durch Kalibrieren dieser relativen Datierungsmethoden mit absoluten Datierungsmethoden wie der Uran-Blei-Datierung können dann auch absolute Alter angegeben werden.
Bei den radiometrischen Methoden mit nicht ausgestorbenen Radionukliden wird gemessen, wie hoch der Anteil natürlich vorkommender radioaktiver Elemente und eventuell ihrer Zerfallsprodukte ist. Da die Halbwertszeit der radioaktiven Elemente bekannt ist, kann daraus das Alter berechnet werden.

Für das Alter von Gesteinen benötigt man Elemente mit sehr langen Halbwertszeiten. Dafür eignen sich unter anderem folgende Methoden (Halbwertszeit in Klammern, siehe auch Geochronologie):

Eine Besonderheit stellt die Aluminium-Beryllium-Methode dar, da sie vergleichend den Zerfall zweier Radioisotope nutzt, die nicht im Tochter-/Mutterisotopverhältnis stehen. Diese Methode der Oberflächenexpositionsdatierung wird auch zur Bestimmung des Alters von fossilen Hominiden-Knochen genutzt. Die Altersbestimmung erfolgt über das Aluminiumisotop Al und das Berylliumisotop Be im Mineral Quarz basiert auf dem (bekannten) Verhältnis von Al und Be, die beide durch kosmische Strahlung (Neutronen-Spallation, Myonen-Einfang) an der Oberfläche von Steinen/Mineralen entstehen. Das Verhältnis ist abhängig u. a. von der Höhenlage, der geomagnetischen Breite, der Strahlungsgeometrie und einer möglichen Schwächung der Strahlung durch Abschirmungen (z. B. Bedeckung).

In der Regel werden bei geologischen Datierungen sogenannte Isochronendiagramme verwendet. Vorteil dieser Technik ist es, dass die anfängliche Konzentration und Isotopenverhältnisse der Tochterelemente nicht bekannt sein müssen, man erhält sie vielmehr als ein weiteres Resultat, zusätzlich zum Alter der Probe. Des Weiteren hat die Isochrontechnik den Vorteil, dass zuverlässig ausgeschlossen werden kann, dass eventuelle Störungen durch Umgebungseinflüsse das gemessene Alter verfälscht haben könnten.

Der eigentliche Vorteil der radiometrischen Datierungsmethoden beruht darauf, dass die Bindungsenergien der Atomkerne um etliche Größenordnungen größer sind als die thermischen Energien der Umgebung in welcher potentielle Proben (meist Gesteine) überhaupt existieren können. Eine Beeinflussung der Zerfallsraten (Halbwertszeiten) durch Umgebungseinflüsse kann deshalb ausgeschlossen werden, so dass die radiometrischen Alter – besonders wenn sie unter Verwendung der Isochronmethode gewonnen wurden – als sehr zuverlässig gelten.

Eine weitere absolute Datierungsmethode ist die Fission-Track-Methode. Hier werden die durch die beim radioaktiven Zerfall (z. B. spontaner Zerfall von Uran oder Zerfall von K zu Ar) entstandenen hochenergetischen Zerfallsprodukte erzeugten Kristallschäden entlang deren Flugbahnen durch Anätzen unter dem Mikroskop sichtbar gemacht und abgezählt.

Bei der Warvenchronologie werden Warven, jährliche Sedimentablagerungen in Seen, ausgezählt. Der Boden bekommt durch diese Ablagerungen ein Streifenmuster. Insbesondere für Gegenden mit starker Schneeschmelze ist dieses Verfahren geeignet. Für die Eifelregion gibt es eine Chronologie der letzten 23.000 Jahre, für einen japanischen See für 45.000 Jahre und für den Lago Grande di Monticchio in Süditalien sogar für die letzten 76.000 Jahre.

Bei der Analyse von Eisbohrkernen werden die Schichten gezählt, die jedes Jahr durch den Schneefall gebildet werden.

Die Magnetostratigraphie nutzt die Tatsache, dass das Erdmagnetfeld sich im Lauf der Zeit oft umgepolt hat. Dieses Muster lässt sich in den Gesteinen wiederfinden und auszählen.

Zur Altersbestimmung menschlicher Hinterlassenschaften in der Archäologie sind meist Ausgangsisotopen mit kürzeren Halbwertszeiten erforderlich als in der Geologie. Hier wird vorwiegend die Radiokohlenstoffdatierung von organischen Materialien angewandt. 
Bei der Radiokohlenstoffdatierung wird der Gehalt an radioaktivem Kohlenstoff C, der eine Halbwertszeit von 5.730 Jahren hat, gemessen. Damit sind Altersbestimmungen bis zu 60.000 Jahren möglich. Bei älteren Proben ist der C-Anteil bereits zu gering, um noch gemessen werden zu können. Eine Schwierigkeit dieser Methode ist, dass der Anteil von C in der Erdatmosphäre nicht konstant ist. Diese Schwankungen können beispielsweise mit Hilfe der Dendrochronologie ermittelt werden.

Die Dendrochronologie ermöglicht es, mittels charakteristischer Jahrringe einiger Baumarten, z. B. Eichen, Datierungen vorzunehmen, die auf das Jahr genau sein können. Daher lässt sich bei guter Erhaltung die Errichtungszeit von Bauten mit erhaltenen Hölzern, etwa von Pfahlbauten oder Brunnen, die Bauzeit von Schiffen oder die Herstellung von Särgen bestimmen.

Bei der Münzdatierung liefern die Münzen einen "terminus post quem". Das bedeutet, dass ein Fund erst in die Erde gelangt sein kann, nachdem das (jüngste) Geldstück aus diesem Fund geprägt wurde. Dabei steht zunächst nicht fest, wie lange nach der Prägung der Münze dies geschehen ist.

Archäologische Funde einer Kultur können auch durch absolut datierte Importgegenstände aus anderen Kulturen datiert werden. Beispiele dafür sind etwa griechische Keramik oder Bronzegefäße in Fürstengräbern der späten Hallstattzeit in Ostfrankreich und Südwestdeutschland oder Römische Importe in der Kaiserzeit in der Germania Magna.

Gelegentlich lassen sich archäologische Fundzusammenhänge über bekannte historische Ereignisse datieren. So wurden beispielsweise die Städte Pompeji und Herculaneum durch den von Plinius dem Jüngeren beschriebenen Ausbruch des Vesuv am 24. August 79 zerstört und verschüttet, die Gebäude in diesen Städten müssen also vor diesem Vulkanausbruch errichtet worden sein. Für Funde aus diesen Orten gilt also ein "terminus ante quem" von 79.

Die Einwanderung der Langobarden in Italien fand nach historischen Quellen im Jahr 568 statt. Langobardische Gräberfelder in Italien datieren daher erst in die Zeit nach 568, die Einwanderung bietet hier einen "terminus post quem".

Die absolute Datierung über die Verbindung von archäologischen Funden und historischen Daten sollte generell mit großer Vorsicht vorgenommen werden, da die Gefahr eines Zirkelschlusses besteht (so genannte gemischte Interpretation). So könnte etwa die historisch belegte Zerstörung einer Siedlung dazu verleiten, eine dort gefundene Brandschicht vorschnell auf diese Zerstörung zu beziehen und nach ihr zu datieren, obwohl die Brandschicht tatsächlich von einem anderen, historisch nicht überlieferten Feuer stammt. Verhindern lässt sich ein solcher Zirkelschluss durch eine möglichst genaue, eigenständige archäologische Datierung und die Einordnung in einen größeren Zusammenhang. Für die oben genannten Beispiele bedeutet das, dass die Funde aus Pompeji (Münzen, Keramik usw.) auch ohne Kenntnis des genauen Datums eine Datierung der Zerstörung um 80 erlauben. Die Datierung der Einwanderung der Langobarden ist nach dem historischen Datum von 568 möglich, da die ältesten Grabbeigaben der Langobarden in Italien nach archäologischen Kriterien aus dem letzten Drittel des 6. Jahrhunderts stammen und spezielle Formen, etwa von Fibeln, dort vorher nicht bekannt gewesen sind, sondern nach genauen Vergleichsstücken von langobardischen Zuwanderern aus ihren bisherigen Wohngebieten in Pannonien mitgebracht wurden.

Einige Methoden eignen sich für relativ spezielle Anwendungsgebiete. Die Thermolumineszenzdatierung etwa dient zur naturwissenschaftlichen Altersbestimmung von Keramik. Inzwischen wurde auch die Argon-Argon-Datierung soweit verfeinert, dass die absolute Datierung historischer Ereignisse mit dieser in bestimmten Fällen möglich ist. So wurde sie 1997 verwendet, um mit ihr Bimsstein von dem Vesuv-Ausbruch, welcher Pompeji zerstörte, auf die Jahre von 72 bis 94 n. Chr. zu datieren. Damit liegt das in historischen Quellen genannten Datum (79 n. Chr.) im Bereich des quantifizierten Fehlers. Die unabhängigen Altersangaben bestätigen sich folglich gegenseitig. Eine neue Untersuchungsmethode stellt die Rehydroxylierung dar, nämlich in welchem Grade Sauerstoffbrücken in Keramik durch Eindringen von Wasser aufgebrochen worden sind. Auf diese Weise gelang es "Moira Wilson" von der University of Manchester und ihren Kollegen keramische Objekte im Alter bis zu 2000 Jahren recht genau zu bestimmen.

Im Regelfall sollte zunächst eine relative Chronologieabfolge erstellt werden, welche erst in einem zweiten Schritt mit absoluten Daten zusammengeführt wird. Wird dieses Prinzip missachtet, führt dies bei einer zu ungenauen oder fehlerhaften Absolutdatierung zu einer falschen Relativdatierung.




</doc>
<doc id="318" url="https://de.wikipedia.org/wiki?curid=318" title="Adrenalin">
Adrenalin

Adrenalin ( ‚an‘ und ‚Niere‘) oder Epinephrin ist ein im Nebennierenmark gebildetes Hormon, das zur Gruppe der Katecholamine gehört. Auch im Zentralnervensystem kommt Adrenalin vor, dort ist es als Neurotransmitter in adrenergen Nervenzellen vorhanden. Seine Effekte vermittelt Adrenalin über eine Aktivierung von G-Protein-gekoppelten Rezeptoren, den Adrenozeptoren.

Einmal ins Blut ausgeschüttet, vermittelt Adrenalin eine Herzfrequenzsteigerung, einen durch Blutgefäßverengung bewirkten Blutdruckanstieg und eine Bronchiolenerweiterung. Das Hormon bewirkt zudem eine schnelle Energiebereitstellung durch Fettabbau (Lipolyse) sowie die Freisetzung und Biosynthese von Glucose. Es reguliert die Durchblutung (Zentralisierung) und die Magen-Darm-Tätigkeit (Hemmung). Als Stresshormon ist es an der „Flucht- oder Kampfreaktion (fight-or-flight response)“ beteiligt.

Eine häufig gebrauchte Bezeichnung für Adrenalin (ursprünglich ein Markenname) ist Epinephrin (INN) ( "epí" ‚auf‘ und "nephrós" ‚Niere‘). Wenn in diesem Artikel oder in der wissenschaftlichen Literatur die Bezeichnung „Adrenalin“ ohne Präfix benutzt wird, ist ("R")-(–)-Adrenalin gemeint.

Den ersten Hinweis auf eine im Nebennierenmark vorkommende und von dort in die Blutbahn freigesetzte Substanz, die sich mit Eisen(III)-chlorid anfärben ließ, fand 1856 der französische Physiologe Alfred Vulpian. Dass diese Substanz außerordentliche pharmakologische Eigenschaften besitzen musste, stellten 1893/94 der praktizierende Arzt George Oliver und der Physiologe Edward Albert Schäfer fest. Dasselbe gelang 1894 dem Krakauer Physiologen Napoleon Cybulski mit seinem Assistenten Władysław Szymonowicz. 1896 publizierte der Augenarzt William Bates seine Beobachtungen.

John Jacob Abel stellte 1897 bzw. 1900 die noch unreine Substanz dar und gab ihr den Namen „Epinephrin“. Inspiriert durch seine Arbeiten isolierten Jokichi Takamine und Thomas Bell Aldrich (1861–1938) 1901 diese und ließen sie von der Firma Parke, Davis & Co. unter dem Namen „Adrenalin“ vertreiben. Obgleich Abels Epinephrin sich später als ein Artefakt der Isolierung herausstellte, wird der Name Epinephrin bis heute synonym für Adrenalin gebraucht.

1904 folgte die chemische Synthese. 1908 gelang Fritz Flaecher (1876–1938) die Trennung des Racemats in die beiden Enantiomere, wobei die wirksamere L-Form unter dem Namen "Suprarenin" auf den Markt gebracht wurde. 1919 führte Reinhard von den Velden (1880–1941) die erste intrakardiale Adrenalin-Injektion durch.

Adrenalin war das erste Hormon, das rein hergestellt und dessen Struktur bestimmt wurde. Die weitere Adrenalinforschung führte zu den beiden anderen körpereigenen Catecholaminen Noradrenalin und Dopamin.

Die Biosynthese von Adrenalin geht von den α-Aminosäuren -Tyrosin oder -Phenylalanin aus. Diese werden zu L-DOPA hydroxyliert. Nach einer Decarboxylierung zum biologisch aktiven Dopamin erfolgt eine enantioselektive Hydroxylierung zum Noradrenalin, welches ebenfalls aus dem Nebennierenmark freigesetzt werden kann und darüber hinaus als Transmitter in sympathischen Neuronen fungiert. Die "N"-Methylierung von Noradrenalin liefert schließlich das Adrenalin.
Die normale Konzentration von Adrenalin im Blut liegt unter 100 ng/l (etwa 500 pmol/l).

Die Biosynthese und die Freisetzung von Adrenalin kann durch nervale Reize, durch Hormone oder durch Medikamente gesteuert werden. Nervale Reizung fördert die Umwandlung von -Tyrosin zu -Dopa und von Dopamin zu Noradrenalin. Cortisol, das Hormon der Nebennierenrinde, fördert die nachfolgende Umwandlung von Noradrenalin zu Adrenalin.

Die Adrenalinproduktion kann auch durch einen negativen Feedback-Mechanismus reguliert werden. Ansteigende Adrenalinspiegel sind mit der -Tyrosin-Bildung negativ rückgekoppelt, bei erhöhten Adrenalinspiegeln wird also die -Tyrosin-Bildung gebremst.

Adrenalin wird nach seiner Freisetzung relativ schnell wieder abgebaut. So beträgt die Plasmahalbwertszeit von Adrenalin bei intravenöser Gabe nur eine bis drei Minuten. Am Abbau von Adrenalin sind insbesondere die Enzyme Catechol-O-Methyltransferase (COMT) und Monoaminooxidase (MAO) beteiligt. Das durch O-Methylierung (COMT) gebildete primäre Abbauprodukt Metanephrin (siehe Metanephrine) besitzt bereits keine nennenswerte biologische Aktivität mehr. Durch weitere, insbesondere oxidative Stoffwechselprozesse unter Beteiligung der Monoaminooxidase ist eine Metabolisierung zu Vanillinmandelsäure und 3-Methoxy-4-hydroxyphenylethylenglykol (MOPEG) möglich. Diese Stoffwechselprodukte werden in konjugierter (z. B. als Sulfate) und unkonjugierter Form über den Urin ausgeschieden. Der zuverlässige qualitative und quantitative Nachweis aller Metabolite gelingt durch die Kopplung verschiedener chromatographischer Verfahren.
Adrenalin ist ein Stresshormon und schafft als solches die Voraussetzungen für die rasche Bereitstellung von Energiereserven, die in gefährlichen Situationen das Überleben sichern sollen (Kampf oder Flucht). Diese Effekte werden auf subzellularer Ebene durch Aktivierung der G-Protein-gekoppelten Adrenorezeptoren vermittelt.

Von besonderer Wichtigkeit ist die Wirkung von Adrenalin auf das Herz-Kreislauf-System. Hierzu zählt u. a. der Anstieg des zentralen Blutvolumens, der durch Kontraktion kleiner Blutgefäße, insbesondere in der Haut und in den Nieren, über die Aktivierung von α-Adrenozeptoren geschieht. Zugleich wird eine β-Adrenozeptor-vermittelte Erweiterung zentraler und muskelversorgender Blutgefäße beobachtet.

Die Aktivierung von β-Adrenozeptoren führt zu einer erhöhten Herzfrequenz (positiv chronotrope Wirkung), einer beschleunigten Erregungsleitung (positiv dromotrope Wirkung), einer erhöhten Kontraktilität (positiv inotrope Wirkung) und einer Senkung der Reizschwelle (positiv bathmotrope Wirkung). Diese Effekte verbessern die Herzleistung und tragen mit der Konstriktion kleiner Blutgefäße zur Erhöhung des Blutdrucks bei. Nach Vorbehandlung mit Alpha-Blockern führt Adrenalin jedoch zu einer paradoxen, therapeutisch genutzten Senkung des Blutdrucks (Adrenalinumkehr). Auch sehr niedrige Adrenalindosen (< 0,1 µg/kg) können eine leichte Senkung des Blutdrucks bewirken, die mit einer selektiven Aktivierung von β-Adrenozeptoren der Blutgefäße erklärt wird.

Chronisch erhöhte Adrenalinspiegel werden mit einer Hypertrophie des Herzens in Verbindung gebracht.

Neben der oben genannten Funktion auf das Herz-Kreislauf-System ist die Steigerung der Atmung und eine vorübergehende Inaktivierung nicht benötigter Prozesse, z. B. der Verdauung, im Rahmen der Stresshormonfunktion des Adrenalins von Bedeutung. Adrenalin führt über eine Aktivierung von β-Adrenozeptoren zu einer Erschlaffung der glatten Muskulatur. Dies hat beispielsweise eine Ruhigstellung des Magen-Darm-Trakts (Hemmung der Peristaltik) und eine Erweiterung der Bronchien zur Erleichterung der Atmung als Folge (β-Adrenozeptoren). Ebenfalls über β-Adrenozeptoren kann Adrenalin eine Relaxation des Uterus von Schwangeren bewirken. Andererseits kann Adrenalin in Organen, die vorwiegend α-Adrenozeptoren exprimieren, eine Kontraktion der glatten Muskulatur vermitteln. So führt Adrenalin zu einer Kontraktion des Schließmuskels der Harnblase.

Die Freisetzung von Adrenalin aus der Nebenniere führt zu einer Mobilisierung von körpereigenen Energieträgern durch Steigerung des Fettabbaus (Lipolyse). Diese Lipolyse wird durch eine β-Adrenozeptor-vermittelte (vorwiegend β-Adrenozeptoren) Aktivierung der hormonsensitiven Lipase katalysiert. Ebenso führt ein Anstieg des Adrenalinspiegels zu einer Freisetzung und Neubildung von Glucose und damit zu einem Anstieg des Blutzuckerspiegels (β-Adrenozeptoren). Dieser Effekt wird durch α-Adrenozeptor-vermittelte Hemmung der Insulinproduktionen und die β-Adrenozeptor-vermittelte Freisetzung von Glucagon verstärkt. Im Muskel kommt es durch Adrenalin zu verstärkter Glucose-Aufnahme. Adrenalin führt ebenfalls zu einer Erhöhung des Energieumsatzes (vorwiegend β-Adrenozeptoren).

Beobachtete zentralnervöse Effekte als Stresshormon werden als reflektorisch angesehen, da in der Nebenniere gebildetes Adrenalin die Blut-Hirn-Schranke nicht passieren kann. Ungeachtet dessen konnte in einigen Neuronen des Zentralnervensystems vor Ort produziertes Adrenalin als Neurotransmitter nachgewiesen werden. Diese Neurone kommen insbesondere in der Area reticularis superficialis ventrolateralis vor. Die Funktion dieser adrenergen Neurone ist noch nicht genau bekannt, jedoch wird eine Rolle bei der zentralen Blutdruckregulation und beim Barorezeptorreflex diskutiert.
Das zentrale Nervensystem nimmt den Stressor wahr, daraufhin wird der Hypothalamus aktiv und aktiviert den Sympathicus. Dessen anregende Wirkung auf das Nebennierenmark bewirkt dessen Ausschüttung von Adrenalin und Noradrenalin.

Als Folge einer Adrenalinfreisetzung oder einer lokalen Adrenalinanwendung können Schweißproduktion, Gänsehaut (pilomotorischer Reflex) und eine Pupillenerweiterung (Mydriasis) beobachtet werden. Zudem bekommt man auch einen trockenen Mund. Adrenalin ist ferner an der Blutgerinnung und Fibrinolyse beteiligt.

Adrenalin (chemisch: ("R")-1-(3,4-Dihydroxyphenyl)-2-("N"-methylamino)ethanol) gehört zur Gruppe der Katecholamine, zu der auch Noradrenalin und Dopamin zählen. Die wirksame Form (Eutomer) des Adrenalins besitzt stereochemisch eine ("R")-Konfiguration [("R")-Adrenalin oder (−)-Adrenalin]. ("R")-Adrenalin ist etwa 20- bis 50-mal wirksamer als ("S")-Adrenalin.
Zur Synthese des Adrenalins sind in der Literatur mehrere Verfahren beschrieben. Das klassische Syntheseverfahren umfasst drei Schritte: Brenzkatechin (1) wird mit Chloressigsäurechlorid (2) zum 3,4-Dihydroxy-ω-chloracetophenon (3) acyliert. Die Reaktion entspricht indirekt der Friedel-Crafts-Acylierung, der bevorzugte Weg führt gleichwohl über die Ester-Zwischenstufe und schließt so eine Fries-Umlagerung mit ein. Die Aminierung des Chloracetophenons mit Methylamin ergibt das Adrenalon (4); die anschließende Reduktion liefert razemisches Adrenalin (5). Die Razematspaltung ist mit Hilfe von (2"R",3"R")-Weinsäure möglich.
Alternativ kann man auch 3,4-Dimethoxybenzaldehyd mit Blausäure zum Cyanhydrin umsetzen, dessen Oxidation dann ein Nitriloketon liefert. Durch katalytische Reduktion entsteht ein Aminoketon, dessen schonende "N"-Methylierung liefert dann das sekundäre Amin. Durch Hydrolyse der Phenyletherfunktionen, Reduktion und Racematspaltung gelangt man dann zum Adrenalin.

Wie alle Katecholamine ist Adrenalin oxidationsempfindlich. Ein Oxidationsprodukt des Adrenalins ist Adrenochrom. Für die Oxidation kann man Silber(I)-oxid (AgO) verwenden. Die Oxidation des Adrenalins kann auch in wässriger Lösung durch Spuren von Eisen- und Iodidionen katalysiert werden. Antioxidanzien, wie z. B. Ascorbinsäure und Natriummetabisulfit können die Bildung von Adrenochrom verlangsamen. Die Geschwindigkeit der Oxidation ist darüber hinaus vom pH-Wert der Lösung abhängig. Als Stabilitätsoptimum gilt ein leicht saurer pH-Wert.
In der Medizin wird Adrenalin vor allem als Notfallmedikament bei der Herz-Lungen-Wiederbelebung bei Herzstillstand und dem anaphylaktischen Schock eingesetzt. Es ist in verschiedenen Darreichungsformen erhältlich und verschreibungspflichtig.

Für die Anwendung in der Notfallmedizin wird Adrenalin intravenös, alternativ auch intraossär oder sehr selten intrakardial verabreicht. In den aktuellen Empfehlungen des European Resuscitation Council wird die Gabe von Adrenalin bei der Reanimation als Standard empfohlen, wenngleich keine placebokontrollierte Studie beim Menschen existiert, die einen Überlebensvorteil durch den Einsatz von Adrenalin nachweist.

Ein weiteres Hauptanwendungsgebiet von Adrenalin in der Medizin ist der Kreislaufschock, beispielsweise bei anaphylaktischen Reaktionen oder Sepsis. Die Behandlung anaphylaktischer Reaktionen und des Schocks erfolgt ebenfalls über eine intravenöse Verabreichung von Adrenalin. Sollte im akuten Schockgeschehen kein venöser Zugang geschaffen werden können, so kann Adrenalin auch intramuskulär verabreicht werden. Für Patienten mit schwerwiegenden allergischen Reaktionen in der Vergangenheit (z. B. drohende Erstickung durch Anschwellen der Stimmritze (Glottisödem)) stehen Adrenalin-Fertigspritzen zur Verfügung, die dann von dem Betroffenen nach einer Allergenexposition mit beginnender Symptomatik selbst appliziert werden können.

Für die Anwendung in der Herz-Lungen-Wiederbelebung und beim Schock stehen die den Blutkreislauf zentralisierenden Wirkungen des Adrenalins im Vordergrund. Durch eine Aktivierung von α-Adrenozeptoren wird eine Konstriktion kleiner Blutgefäße in der Haut und in den Nieren erreicht, während große zentrale Blutgefäße erweitert werden. Auf diese Weise soll Adrenalin den koronaren und zerebralen Perfusionsdruck steigern.

Für die Anwendung als Zusatzmedikation bei der akuten "Laryngitis subglottica" („Pseudo-Krupp“) steht Adrenalin als Lösung zur Inhalation (InfectoKrupp Inhal) zur Verfügung. Bis 2002 waren in Deutschland Adrenalin-haltige Inhalationspräparate auch für die Akutbehandlung des Asthma bronchiale zugelassen. Mit Inkrafttreten des FCKW-Verbots wurden diese jedoch vom Markt genommen. Die inhalative Anwendung anderer Adrenalinpräparate zur Akutbehandlung asthmatischer Beschwerden ist somit außerhalb der arzneimittelrechtlichen Zulassung und entspricht einem Off-Label-Use.

Die Anwendung des Adrenalins bei Atemwegserkrankungen basiert auf seiner bronchienrelaxierenden Wirkung, die über eine Aktivierung von β-Adrenozeptoren vermittelt wird. Systemische Nebenwirkungen nach Resorption müssen jedoch in Kauf genommen werden.

Adrenalin kann weiterhin zur lokalen Gefäßverengung bei Blutungen eingesetzt werden. Die gefäßverengende Wirkung wird auch zum Schließen von Cuts im Boxsport verwendet. Diese vasokonstriktive Wirkung beruht auf einer Aktivierung von α-Adrenozeptoren kleiner Blutgefäße in der Haut und im Muskelgewebe und ihrer darauf folgenden Verengung.

Adrenalin wird ferner als vasokonstriktiver Zusatz zu Lokalanästhetika verwendet, um deren Abtransport zu verlangsamen und damit ihre Wirkungsdauer zu verlängern.

Adrenalin ist das Mittel der zweiten Wahl bei Betablockervergiftungen und kann eingesetzt werden, wenn kein spezifischer β-Agonist zur Verfügung steht. Für diese Notfallanwendung besteht jedoch ebenfalls keine arzneimittelrechtliche Zulassung (Off-Label-Use).

Die Nebenwirkungen des Adrenalins entsprechen weitgehend seinen Hauptwirkungen und sind auf dessen Bedeutung als Stresshormon zurückzuführen. Adrenalin führt zu einer Kontraktion kleiner Blutgefäße, insbesondere der Haut und der Nieren, verbunden mit einem Blutdruckanstieg und, insbesondere bei lokaler Anwendung, vereinzelten Nekrosen. Bei systemischer Anwendung stehen kardiale Nebenwirkungen, wie z. B. Herzinsuffizienz, Angina-pectoris-Anfälle, Herzinfarkt, tachykarde Herzrhythmusstörungen, bis hin zum Kammerflimmern und Herzstillstand im Vordergrund. Daher ist seine Anwendung teilweise umstritten. Die systemische Anwendung von Adrenalin kann darüber hinaus eine Erhöhung des Blutzuckerspiegels (Hyperglykämie), eine Erniedrigung des Kaliumspiegels (Hypokaliämie), eine metabolische Azidose und eine Absenkung der Magnesiumkonzentration (Hypomagnesiämie) zur Folge haben. Des Weiteren können Mydriasis, Miktionsschwierigkeiten, Speichelfluss, Schwitzen bei gleichzeitigem Kältegefühl in den Extremitäten, Übelkeit, Erbrechen, Schwindel und Kopfschmerz beobachtet werden. Als psychische Nebenwirkungen durch den Einsatz von Adrenalin können Ruhelosigkeit, Nervosität, Angst, Halluzinationen, Krämpfe bis hin zu Psychosen auftreten.

Einige Inhalationsanästhetika, die das Herz für Katecholamine sensibilisieren, führen zu einer verstärkten Wirkung von Adrenalin am Herz und somit zu einer erhöhten Gefahr von Herzinsuffizienz, Angina-pectoris-Anfällen, Herzinfarkt und tachykarden Herzrhythmusstörungen.

Die Wirkungen und Nebenwirkungen von Adrenalin können ebenfalls durch eine Hemmung des Adrenalinabbaus oder einer vermehrten (Nor-)Adrenalinfreisetzung verstärkt werden. Dies ist insbesondere bei gleichzeitiger Anwendung von MAO-Hemmern, Levodopa, L-Thyroxin, Theophyllin, trizyklischen Antidepressiva und Reserpin zu beobachten.

Adrenalin seinerseits hemmt die blutdrucksenkende Wirkung von Alphablockern und die kardialen Effekte der Betablocker. Da Adrenalin zu einem Anstieg des Blutzuckerspiegels führt, ist die Wirkung oraler Antidiabetika herabgesetzt.

Adrenalin wird als Lösung intravenös verabreicht. Typischerweise ist die Konzentration in einer Ampulle 1 mg/ml (auch als Adrenalinlösung 1:1.000 oder Adrenalinlösung 0,1 %ig bezeichnet). Je nach Anwendungsgebiet ist es gebräuchlich, im Verhältnis 1:10 mit 0,9 % Natriumchloridlösung zu verdünnen (dann als Adrenalinlösung 1:10.000 oder Adrenalinlösung 0,01 %ig bezeichnet). Die Reanimationsdosis beträgt 1 mg alle 3–5 Minuten.
Bei der endobronchialen Anwendung (veraltet) wird meist 3:10 mit 0,9 % Natriumchloridlösung verdünnt. In der Intensivmedizin und zur Behandlung eines "Low-output-Syndroms" wird bei Erwachsenen eine Dosierung von 2–20 µg/min eingesetzt.

Ampullen (Injektionslösung)
Autoinjektoren (Injektionslösung in Fertigpen)
Inhalationslösung




</doc>
<doc id="320" url="https://de.wikipedia.org/wiki?curid=320" title="Arterie">
Arterie

Eine Arterie, lateinisch Arteria, ist ein Blutgefäß, welches (mit Ausnahme der Herzkranzarterien) Blut vom Herzen "weg" führt. Sie wird nach den an großen Arterien spürbaren Pulsen des Herzschlags auch Schlagader oder Pulsader genannt. Durch ihren Aufbau sollen Arterien den vom Herzen erzeugten Blutdruck stabil halten können. Im Körperkreislauf transportieren sie sauerstoff­reiches Blut („arterielles Blut“). Die vom rechten Herzen zu den Lungenflügeln abgehenden Arterien des Lungenkreislaufs hingegen enthalten sauerstoffarmes Blut. In den Arterien des Menschen sind nur etwa 20 % des gesamten Blutvolumens enthalten (post mortem wegen des Druckgefälles noch ca. 2 %). Arterien verzweigen sich in immer kleinere Arterien und dann über Arteriolen in so genannte Haargefäße (Kapillaren). Blutgefäße, die das Blut aus dem Körper zum Herzen zurücktransportieren, werden im Allgemeinen Venen genannt.

Lateinisch "arteria" stammt von ; von "a(ë)rter", ‚woran etwas aufgehängt wird‘ (in Bezug auf die an der Luftröhre bzw. den Bronchien aufgehängte Lunge), von altgriech. . Volksetymologisch wurde die Arterie bezogen auf bzw. "aër", ‚Luft‘, in der Annahme, Arterien seien mit Luft gefüllt (Arterien galten zudem als Leitungsbahn nicht nur für Blut, sondern auch für das ebenso lebenswichtige Pneuma).

Je nach Funktion und Lokalisation müssen Arterien verschiedenen Ansprüchen genügen und unterscheiden sich daher auch in ihrem Aufbau:

Die Gefäßwände der Arterien sind dicker (muskelreicher), haben eine deutlich ausgeprägtere Schichtung und sind weniger dehnbar als die Venen.

Grundsätzlich besteht eine arterielle Wand aus drei Schichten, deren Bestandteile alle lateinisch-anatomische Namen tragen; von der blutführenden Seite aus gesehen sind dies:

Die größte Arterie im menschlichen Körper ist die Aorta (Hauptschlagader) mit einem Durchmesser von etwa drei Zentimetern. Weitere größere Arterien sind:



</doc>
<doc id="321" url="https://de.wikipedia.org/wiki?curid=321" title="Ariel Scharon">
Ariel Scharon

Scharons Vater Schmuel kam aus Brest-Litowsk und die Mutter, die einer Familie von Subbotniki entstammte, aus Mogiljow in Weißrussland. Der Vater hatte gerade in Tiflis sein Betriebswirtschaftsstudium mit Vertiefung auf Einzelhandelskaufmann abgeschlossen, als er 1921 als aktiver Zionist vor der Roten Armee floh und zusammen mit der Mutter nach Palästina auswanderte. Seine Frau Vera Schneeroff konnte deshalb ihr Studium der Medizin nicht abschließen, was sie ihr Leben lang bereute. Im Unterschied zu vielen Einwanderern jener Zeit war sie weder sozialistisch eingestellt noch teilte sie den Zionismus ihres Mannes.

Die Familie zog in den Moschaw Kfar Malal, wo die Entscheidungen zwar kollektiv getroffen wurden, aber jeder sein eigenes Land besaß. Als einziger studierter Landwirt und wenig kompromissbereiter Mensch verstand es der Vater, sich wiederholt über die Entscheidungen der Gemeinschaft hinwegzusetzen, wodurch sich die Familie jedoch manchmal isolierte.

Bereits mit 13 Jahren beteiligte sich Scharon am Wachdienst des Moschaws und trat im Jahr darauf der Untergrundorganisation Hagana bei, dem Vorläufer der israelischen Armee. Seit 1941 besuchte er das Gymnasium in Tel Aviv. Dort legte er, der nie ein herausragender Schüler war, mit 17 Jahren das Abitur ab. Da sein Vater die Aktionen des Palmach gegen national-konservative Gruppen (wie Lechi und Etzel) ablehnte, die gegen die Briten kämpften, trat Scharon nicht dieser Eliteeinheit, sondern der Jewish Settlement Police bei. Schon seit dem 21. Dezember 1947 war die Hagana dauerhaft mobilisiert, und Scharon nahm an mehreren ihrer Aktionen teil.

Zu Beginn des israelischen Unabhängigkeitskrieges von 1948 war Scharon Zugführer in einer Infanteriekompanie, die zur Alexandroni-Brigade gehörte. Er kämpfte unter anderem am 26. Mai 1948 in der ersten Schlacht um Latrun, in der er schwer verwundet und sein Zug fast vollkommen ausgelöscht wurde. Später wurde er zum Aufklärungsoffizier im Bataillon ernannt, das zuerst im Norden gegen die irakische und später, kurz vor Kriegsende, im Süden gegen die ägyptische Truppen kämpfte.

Nach dem Krieg wurde die Alexandroni-Brigade in den Reservestatus versetzt. Scharon wurde Offizier der Aufklärung in der Golani-Brigade, in der er bald zum Hauptmann (Seren) befördert wurde und einen Bataillonskommandeurskurs besuchte. Im Jahr 1950 wurde er zum Aufklärungsoffizier für das gesamte Zentralkommando ernannt. Wegen der Folgen einer Malaria nahm Scharon 1951 eine mehrmonatige Auszeit und bereiste zum ersten Mal Europa und Nordamerika. Im November 1952 war Scharon unter der Führung von Mosche Dajan erstmals an Kommandoaktionen hinter den feindlichen Linien beteiligt. Am Ende des Jahres entschloss er sich jedoch zum Rückzug aus dem aktiven Dienst.

Er begann ein Studium der Geschichte und Kultur des Nahen Ostens an der Hebräischen Universität Jerusalem, an der sich 1947 bereits für Landwirtschaft eingeschrieben hatte. Am 29. März 1953 heiratete er seine erste Frau Margalit (kurz Gali), eine rumänische Jüdin, die er 1947 kennengelernt hatte. Margalit starb im Jahr 1962 bei einem Autounfall. Durch einen Unfall mit einem Gewehr der Familie starb auch ihr gemeinsamer Sohn Gur 1967 früh. Scharon heiratete später Margalits jüngere Schwester Lily, mit der er zwei Söhne, Omri und Gilad zeugte. Lily Scharon starb im Jahr 2000 an Lungenkrebs.

Im Jahr 1951 kam es zu 137, 1952 zu 162 und 1953 zu 160 Angriffen von Palästinensern auf den jungen Staat Israel. Da dessen Grenzen nur schwer zu überwachen waren, gelang es einigen Angreifern sogar, bis in die Vororte Tel Avivs vorzudringen. Dabei waren zahlreiche – meist zivile – Opfer zu beklagen. Das israelische Militär versuchte bei mehreren Gegenschlägen, die Zentren der paramilitärischen und terroristischen Angreifer zu treffen. Diese waren jedoch wenig effektiv, da die eingesetzten Truppen nicht speziell ausgebildet waren und oft schwere Verluste erlitten. Auch Scharon führte einen dieser misslungenen Gegenschläge aus. Seine militärische Analyse der Aktion bewog David Ben Gurion dazu, Mordechai Maklef mit der Gründung einer Spezialeinheit zu beauftragen, der Einheit 101. Ende Juli 1953 wurde Scharon mit deren Führung betraut. Daher musste er sein Studium zurückstellen.

Scharon wählte die 45 Mitglieder der Einheit sorgfältig aus. Seit Oktober 1953 wurden sie im Camp Sataf einem harten Training unterworfen. Die Einheit begann mit Militäraktionen im Feindesland, die sie als „Abschreckungsoperationen“ bezeichnete. Bei einem gemeinsam mit einer Kompanie Fallschirmjäger unternommenen Angriff auf das jordanische Dorf Qibya wurden 69 Menschen getötet. Die meisten Opfer waren Zivilisten, die sich trotz Räumungsbefehls in ihren Häusern versteckt hielten, die von den Israelis gesprengt wurden. In seiner Autobiografie schreibt Ariel Scharon: 

Nachdem Mosche Dajan Ende 1953 zum israelischen Generalstabschef ernannt worden war, wurde die Einheit 101 in die Fallschirmjägertruppe integriert. Scharon wurde der Kommandeur des Bataillons, das nach Einschätzung der israelischen Führung erfolgreich arbeitete. Nach der Qibya-Aktion wurden jedoch nur noch rein militärische Ziele angegriffen. Neben der herausgehobenen Position der Fallschirmjäger führte die Tatsache, dass Scharon seine persönlichen Kontakte zu Ben Gurion und Dajan zu scharfer Kritik an den Methoden der Armee und für seine persönlichen Ambitionen nutzte, zu Problemen Scharons mit seinen Vorgesetzten in der Armee. Zu Konflikten kam es auch mit dem neuen Verteidigungsminister Pinchas Lawon, der, besorgt um die außenpolitischen Auswirkungen der Aktionen, Scharon vergeblich zu zügeln versuchte. In diese Zeit fällt auch der großangelegte Angriff der Einheit auf das ägyptische Hauptquartier in Gaza am 28. Februar 1955, der einer der Gründe für die verstärkte Inanspruchnahme sowjetischer Militärhilfe durch Gamal Abdel Nasser war. Eine weitere bedeutende Aktion war der Angriff auf das jordanische Militärhauptquartier in Kalkilia im Oktober 1956.

In der Suezkrise spielte Scharons 202. Fallschirmjäger-Brigade eine entscheidende Rolle. Das 890. Fallschirmjäger-Bataillon sicherte nach einer Luftlandung den Ostausgang des strategisch wichtigen Mitla-Passes. Der Rest der Brigade unter Scharon kämpfte sich in zwei Tagen auf dem Landweg die 200 km durch feindliches Gebiet zum Pass vor.

Scharon bat mehrmals erfolglos darum, den Pass angreifen zu dürfen, erhielt aber nur Erlaubnis, ihn aufzuklären, um ihn, falls er unbesetzt sein sollte, später einzunehmen. In großzügiger Auslegung seiner Anweisungen schickte Scharon einen für reine Aufklärungszwecke sehr starken Spähtrupp, der in der Passmitte durch schweres Feuer gebunden wurde. Scharon schickte daraufhin auch den Rest seiner Brigade zur Unterstützung. In dem sich nun entwickelnden Gefecht konnten die Israelis den Pass erobern, wobei 38 israelische Soldaten fielen. Mehrere Jahre später gingen einige Teilnehmer der Schlacht an die Presse und warfen Scharon vor, er habe seine Aufklärer leichtfertig in Gefahr gebracht, um die Ägypter zu provozieren. Andere Veteranen der Aktion nahmen Scharon hingegen in Schutz.

Der Mitla-Zwischenfall fand das Missfallen von Scharons Vorgesetzten und brachte seine militärische Karriere auf Jahre hinaus beinahe zum Stillstand. Er blieb Kommandeur der Fallschirmjäger, bis er im Herbst 1957 von Dajan für ein Jahr nach Großbritannien auf das Staff College Camberley geschickt wurde. Dort schrieb er eine analytische Arbeit mit dem Titel: "Command Interference in Tactical Battlefield Decisions: British and German Approaches". Nach seiner Rückkehr wurde er Oberst und Kommandeur der Infanterie-Schule, eine Aufgabe, die ihm wegen ihrer Theorielastigkeit nicht zusagte. Später kam das Kommando einer Reserve-Infanteriebrigade hinzu. Scharon begann auch einen Panzer-Lehrgang und besuchte einen Abendkurs für Jura bei der Tel Aviver Abteilung der Hebräischen Universität (den Abschluss machte er schließlich 1966). Auf Druck von Ben Gurion ernannte ihn Tzur schließlich zum Kommandeur einer Reserve-Panzerbrigade, abermals eine inaktive Rolle, die ihm aber wegen seines Interesses für die strategische Bedeutung von Panzern eher zusagte. Erst als Ende 1963 Jitzchak Rabin, der spätere israelische Premierminister, zum Generalstabschef ernannt wurde, wurde Scharon wieder einbezogen und zum Kommandeur des Nordkommandos unter Avraham Joffe ernannt. 1966 wurde er schließlich von Rabin in den Rang eines Generalmajors (Aluf) befördert, zum Direktor des militärischen Trainings ernannt und Kommandeur einer Reserve-Division.

Vor dem Sechstagekrieg machte sich Scharon zusammen mit Joffe und Matti Peled dafür stark, die ursprüngliche Taktik eines Stufenplans, der das schrittweise Meistern verschiedener Fronten und Konfliktherde vorsah, durch einen größer angelegten Präventivschlag an mehreren Fronten zu ersetzen. Scharon plante einen Angriff, der sowohl gleichzeitig und von Beginn an alle verfügbaren Kräfte ins Kampfgeschehen einbindet, als auch die gesamte Sinaifront umfassen sollte. Nach der Ernennung Dajans zum Verteidigungsminister konnte sich diese Vorstellung durchsetzen. Im Krieg kommandierte Scharon die mächtigste Panzerdivision an der Sinaifront (die beiden anderen Divisionen waren die von Tal und Joffe), der der Durchbruch im Gebiet von Kusseima und Abu-Ageila gelang. Es war schließlich auch Scharon, der die 6. ägyptische Division vernichtend schlug. Rabin ernannte Scharon daraufhin zum Kommandeur des Sinai, wodurch er auch für die Versorgung der in der Wüste verstreuten ägyptischen Soldaten zuständig war.

Als Chef der militärischen Ausbildung begann er sofort nach dem Krieg, verschiedene Ausbildungszentren in das Westjordanland zu verlegen, um die Gebiete zu sichern. Am Ende hatte er beinahe alle ehemaligen jordanischen Militärlager und Kasernen besetzt, die an den wichtigen strategischen Punkten lagen. Er versuchte auch Dajan davon zu überzeugen, die Familien der Soldaten in der Nähe dieser Kasernen anzusiedeln, war jedoch zunächst nicht erfolgreich. Im Jahr 1969 wurde er Chef des Südkommandos der israelischen Streitkräfte.

Nach dem Angriff arabischer Staaten auf Israel zum Auftakt des Jom-Kippur-Krieges 1973 wurde Scharon aus dem militärischen Ruhestand zurückgerufen. Ihm gelang es in eigenmächtiger Initiative, unter Missachtung der Anweisungen seines Vorgesetzten Generalleutnant Chaim Bar-Lew, mit seiner Panzerdivision die ägyptischen Angriffslinien im Sinai zu durchbrechen, südlich der Bitterseen die auf dem Ostufer verbliebene 3. Ägyptische Armee einzukesseln und über den Sueskanal überzusetzen. Die israelische Armee stand nun jenseits des Sueskanals, etwa 100 km vor Kairo. So half er entscheidend mit, eine drohende Niederlage Israels abzuwenden.

Von 1973 bis 1974 und von 1977 bis 2006 war er Abgeordneter der Knesset. In der Likud-Regierung von Menachem Begin amtierte Scharon zunächst als Landwirtschaftsminister (1977–1981), dann als Verteidigungsminister (1981–1983). Als Landwirtschaftsminister wurde er ab 1977 einer der wichtigsten Fürsprecher der Siedlerbewegung.
Während einer israelischen Militärintervention im Süd-Libanon verübten im Rahmen des libanesischen Bürgerkriegs die mit Israel lose verbündeten libanesisch-christlichen Falange-Milizen 1982 in den palästinensischen Flüchtlingslagern Sabra und Schatila ein Massaker an palästinensischen Kämpfern und Zivilisten. Ein israelischer Untersuchungsausschuss, die Kahan-Kommission, gab 460 Opfer als gesichert an und ging aufgrund von Geheimdienstinformationen von etwa 800 zivilen und militärischen Opfern aus, während der Palästinensische Rote Halbmond von 2000 Opfern spricht. An Israels indirekter Beteiligung als seinerzeitige militärische Besatzungsmacht, die nicht einschritt, entzündete sich nationale und vor allem internationale Empörung. Die Kommission warf Scharon zwar nicht Komplizenschaft, aber doch fahrlässiges Unterlassen vor und befand ihn daher 1983 als politisch indirekt mitschuldig, wodurch er als Verteidigungsminister zum Rücktritt gezwungen wurde.

In den folgenden Kabinetten blieb Scharon zunächst Minister ohne Geschäftsbereich (1983–1984), von 1984 bis 1990 Minister für Handel und Industrie und Bauminister (1990–1992). In dieser Zeit entwickelte er weitreichende israelische Siedlungspläne im palästinensischen Westjordanland mit dem umstrittenen Siedlungsring um Ostjerusalem, zu dem auch Ma'ale Adumim gehört.

Nach dem Regierungswechsel 1992, bei dem die Arbeitspartei unter Jitzchak Rabin den Likud ablöste, war Scharon Mitglied der Knesset. Dort gehörte er der außenpolitischen und der Verteidigungskommission an. Als schärfster innenpolitischer Gegner Rabins kritisierte Scharon Rabin wegen des Oslo-Friedensprozesses als Verräter. 1996, im Jahr nach der Ermordung Rabins, errang der Likud unter Benjamin Netanjahu einen neuen Wahlsieg; Scharon wurde Minister für die nationale Infrastruktur und förderte in dieser Funktion massiv den Ausbau der israelischen Siedlungen in den besetzten Palästinensergebieten. 1998 ernannte Netanjahu Scharon zum Außenminister. In diesem Amt fordert Scharon seine Landsleute auf, sich in den besetzten Gebieten „so viele Berggipfel wie möglich zu nehmen“.

1999 besiegte die Arbeitspartei unter Ehud Barak den Likud, dessen Vorsitzender Netanjahu in den Strudel einer Finanzaffäre geraten war. Netanjahu trat als Parteichef zurück und Scharon wurde am 27. Mai 1999 zunächst übergangsweise, am 2. September 1999 mit 53 % der abgegebenen Stimmen dann endgültig sein Nachfolger.

Am 28. September 2000 besuchte Scharon in Begleitung von rund 1000 Journalisten, Polizisten, Militärs und Politikern, den sowohl von Muslimen als auch von Juden und Christen als heilig deklarierten Tempelberg in Jerusalem, um zu verdeutlichen, dass der Tempelberg auch den Juden gehört. Er wollte damit auch deutlich machen, dass Israel die Kontrolle über ein vereinigtes Jerusalem an jedem Ort behalten müsse. Bei seinem Besuch, begleitet von zahlreichen bewaffneten Sicherheitskräften, sagte Scharon, er sei mit einer Friedensbotschaft gekommen: „Ich bin überzeugt, dass wir mit den Palästinensern zusammenleben können.“

Obwohl der Besuch mit der moslemischen Verwaltung des Tempelbergs abgestimmt war, kam es zu gewalttätigen Auseinandersetzungen; bei Demonstrationen im Anschluss wurde auch mit scharfer Munition auf Demonstranten geschossen und etliche verletzt und getötet. Der Tempelbergbesuch Scharons fällt zeitlich mit dem Beginn der Zweiten Intifada zusammen, welche nach arabischer Lesart durch diesen ausgelöst wurde, sich aber in jedem Fall schon seit längerer Zeit angekündigt hatte. Die Palästinenser bezeichnen die zweite Intifada auch als "„Al-Aqsa“"-Intifada, benannt nach der gleichnamigen Moschee auf dem Tempelberg. Es gab auch schon unmittelbar davor Anschläge und Pläne der bewaffneten Palästinenser-Gruppen für den bewaffneten Aufstand.

Scharon gewann am 6. Februar 2001 die Wahl um das Ministerpräsidentenamt und wurde daraufhin am 7. März 2001 Israels Premierminister. Besonders populär war bei den Wählern sein Versprechen, dem Sicherheitsbedürfnis der israelischen Bevölkerung höchste Priorität einzuräumen und den Terror zu beenden. Dieses Versprechen konnte er allerdings während seiner Amtszeit nicht erfüllen. Scharon lehnte Jassir Arafat als Gesprächspartner auf palästinensischer Seite ab, warf ihm Urheberschaft am Terror vor, isolierte Arafat international und stellte ihn in der weitestgehend zerstörten Muqataa unter Hausarrest.

Bei der Wahl 2003 erreichte Scharon mit seiner Likud-Partei einen großen Wahlerfolg. In der zweiten Amtszeit von Scharons Regierung wurde mit der Errichtung einer 720 km langen Sperranlage teilweise inmitten der Palästinensergebiete begonnen, die über eine Distanz von 20 km mit Beton verstärkt und deren internationaler rechtlicher Status äußerst umstritten ist.

Am 23. März 2004 kündigte die Hamas zum wiederholten Male und als Reaktion auf die gezielte Tötung ihres Führers Ahmad Yasins an, Scharon ermorden zu wollen. Nur wenige Tage nach der Tötung Yasins geriet Scharon erneut unter Druck. Abgeordnete der Schinui-Partei, die an der Regierung beteiligt waren, forderten Scharons Rücktritt. Am 28. März hatte die Generalstaatsanwältin Edna Arbel bekanntgegeben, dass sie gegen Scharon und seine Söhne Anklage wegen Korruption erheben wollte. Mitte Juni 2004, entschied der israelische Generalstaatsanwalt Menachem Masus nach monatelangen Ermittlungen, Regierungschef Scharon nicht anzuklagen. Da der Verdacht nicht zu erhärten war und somit eine Verurteilung unwahrscheinlich erschien, wurde das Verfahren eingestellt. Scharon hatte gleichzeitig mit Masus auch einen anderen Konflikt: Dieser hatte Scharon öffentlich getadelt, da Scharon in Bezug auf das Westjordanland und den Gazastreifen von den "„besetzten Gebieten“" sprach – abweichend vom offiziellen israelischen Sprachgebrauch, der "„umstrittene Gebiete“" verwendet. Scharon legte trotz des schwebenden Ermittlungsverfahrens keinen gesteigerten Wert auf ein entspanntes Verhältnis zum Chefankläger und bestand weiterhin auf seiner Wortwahl.

Im Dezember 2003 legte Scharon den als „Scharon-Plan“ bekannten einseitigen Abzugsplan aus dem Gazastreifen und Teilen des Westjordanlandes vor, wonach alle Siedlungen im Gazastreifen und vier im Westjordanland aufgelöst werden sollten. Trotz internationaler Kritik an der fehlenden Abstimmung mit den Palästinensern sahen viele diesen Plan als Schritt in die richtige Richtung und Abkehr von der bisherigen Siedlungspolitik Israels. Andere sahen darin nur die Einsicht, dass der militärische Aufwand, die Siedlungen in Gaza zu halten, auf Dauer nicht tragbar war. Der Plan kostete Scharon Sympathien bei der Siedlungsbewegung und der politischen Rechten Israels, brachte ihm aber Zustimmung im gemäßigten und linken Spektrum sowie bei internationalen Bündnispartnern. Um den Plan, der seiner früheren Politik widersprach, durchzusetzen, beendete er die Koalition mit Schinui und Schas und ging eine Große Koalition mit der Arbeitspartei ein. Innerparteilich hatte er einen Machtkampf mit den Gegnern des Plans unter Finanzminister Benjamin Netanjahu zu bestehen, der im August 2005 kurz vor Vollzug des Gaza-Abzugs von seinem Amt zurücktrat.

Am 21. November 2005 kündigte Scharon seinen Rücktritt als Premierminister und den Austritt aus dem Likud an. Nachdem der Widerstand im Likud gegen den Abzug gewachsen war, hatte er im selben Monat eine neue Partei mit dem Namen Kadima („Vorwärts“) gegründet, die bei den folgenden Neuwahlen ihre gute Chance nutzte.

Am 18. Dezember 2005 erlitt Scharon einen leichten Schlaganfall. Danach wurde ein offenbar angeborener Herzfehler entdeckt, der am 5. Januar 2006 operiert werden sollte. Am Vorabend der Operation wurden starke Hirnblutungen festgestellt, Scharon musste sich in den nächsten Tagen mehreren neurochirurgischen Operationen unterziehen. Die Regierungsgeschäfte wurden an den stellvertretenden Ministerpräsidenten Ehud Olmert übertragen. Bei Tests am 14. Januar wurden zwar Gehirnaktivitäten in beiden Hirnhälften gemessen, es gab jedoch keine Anzeichen für ein Erwachen aus dem Koma.

Es galt als sicher, dass Scharon sein Amt nicht mehr würde ausüben können. Dies brachte eine schwierige Situation für die israelische Politik mit sich, da insbesondere die in den letzten Jahren verfolgte Politik gegenüber den Palästinensern und die neue Partei Kadima eng mit der Person Scharons verbunden waren. In der israelischen Öffentlichkeit wurde Kritik an der medizinischen Versorgung Scharons laut; man hätte ihm demnach nicht gestatten sollen, ohne ärztliche Begleitung auf seine abgelegene Farm zurückzukehren. Ein Journalist der Zeitung Ha'aretz formulierte: "„Israel hat nun zwei Ministerpräsidenten verloren, weil sie nicht ausreichend geschützt wurden: Rabin durch Gewalt und Scharon durch Krankheit.“"

Am 11. Februar 2006 entschieden sich die Ärzte zu einer weiteren Notoperation, nachdem Untersuchungen Schäden am Verdauungstrakt des Politikers und Probleme bei der Blutversorgung der inneren Organe gezeigt hatten. Erklärungen der behandelnden Ärzte zufolge sei Scharons Zustand nach der Operation „kritisch, aber stabil“. Anfang April 2006 erfolgte ein weiterer chirurgischer Eingriff zur Schließung der Schädelöffnungen, die durch die vorherigen Operationen verursacht worden waren. Am 11. April 2006 beschloss das israelische Kabinett, Scharon für dauerhaft amtsunfähig zu erklären. Sein Nachfolger im Ministerpräsidentenamt wurde sein Stellvertreter Ehud Olmert.

Ariel Scharon wurde als Wachkoma-Patient auf die Rehabilitationsstation des Chaim Sheba Medical Center verlegt, einem Krankenhaus in Tel Hashomer, einem Stadtteil von Ramat Gan nahe Tel Aviv. Sein langjähriger Berater Dov Weisglass sagte am 21. April 2008 der "Jerusalem Post", Scharons Zustand habe sich wenig verändert. Scharon atme ohne die Hilfe medizinischer Geräte und könne nach dem Urteil der Ärzte wahrscheinlich noch lange in diesem Zustand bleiben. Im November 2010 wurde Scharon versuchsweise für einige Tage auf seine Farm im Süden Israels verlegt.

Im Oktober 2010 wurde Scharons Zustand durch eine Installation des israelischen Künstlers Noam Braslavsky in der Kishon-Galerie in Tel-Aviv, die eine lebensechte Wachsfigur Scharons in einem Krankenbett zeigt, erneut in die Öffentlichkeit gerückt. Da die Installation an ein Beatmungsgerät angeschlossen ist, hebt und senkt sich der Brustkorb, wodurch die Szene noch realitätsgetreuer wirkt.

Im Januar 2011 sagte sein persönlicher Arzt, Scharon reagiere auf Kneifen und öffne die Augen, wenn man ihn anspreche. Am 2. Januar 2014 wurde bekannt, dass mehrere innere Organe versagt hätten und Scharon in Lebensgefahr schwebe. Ariel Scharon verstarb schließlich im Alter von 85 Jahren am 11. Januar 2014 an multiplem Organversagen in jenem Krankenhaus bei Ramat Gan, in dem er seit 2006 behandelt wurde.

Sharon wurde in einem Staatsbegräbnis im Beisein von hochrangigen ausländischen Politikern wie US-Vizepräsident Joe Biden, dem ehemaligen britischen Premier Tony Blair, dem Ex-Ministerpräsidenten der Niederlande Wim Kok, dem russischen Außenminister Sergei Lawrow, Tschechiens Ministerpräsident Jiří Rusnok und Deutschlands Außenminister Frank-Walter Steinmeier beigesetzt. Er wurde am 13. Januar 2014 neben seiner zweiten Frau Lily auf der Familienfarm "Havat Shikmim" begraben, die im Norden der Wüste Negev nahe Sderot liegt.
Scharon war Parteivorsitzender des Likud und Gründer von dessen Abspaltung Kadima. Vor seiner Zeit als Regierungschef hatte er verschiedene Ministerposten inne. So war Scharon Landwirtschaftsminister, zweimal Verteidigungsminister sowie Außenminister. Nach seinem Schlaganfall Ende 2005 musste er im April 2006 als Ministerpräsident für amtsunfähig erklärt werden.

Der ehemalige General wirkte aufgrund seiner Biografie und seiner Politik stark polarisierend. Viele Israelis betrachten ihn als Helden, der ihr Land seit dem Unabhängigkeitskrieg stets in bedeutenden Positionen mitgeprägt hat. Weite Teile der arabischen und der internationalen Öffentlichkeit sehen in ihm jedoch vor allem den Mitverantwortlichen für das Massaker von Sabra und Schatila und den militärischen Hardliner. Andererseits setzte er 2005 die Aufgabe der israelischen Siedlungen im Gazastreifen durch. Viele Anhänger der israelischen Rechten, speziell der Siedlerbewegung, die lange Zeit ihren Vorkämpfer in ihm sahen, wurden infolge dieser Politik zu seinen Gegnern. Aus ihrer Sicht hat er sich als Ministerpräsident gegenüber den Palästinensern als zu kompromissbereit gezeigt.

Anlässlich seines Todes im Januar 2014 würdigten viele hochrangige Politiker Sharons Verdienste. US-Präsident Barack Obama bezeichnete ihn als jemanden, der dem Staat Israel sein Leben gewidmet habe. Auch Bundeskanzlerin Angela Merkel würdigte ihn als Patrioten mit großen Verdiensten um sein Land.





</doc>
<doc id="323" url="https://de.wikipedia.org/wiki?curid=323" title="Département Alpes-Maritimes">
Département Alpes-Maritimes

Das Département Alpes-Maritimes [] ist das französische Département mit der Ordnungsnummer 06. Es liegt im Südosten des Landes in der Region Provence-Alpes-Côte d’Azur und ist nach den Seealpen benannt, die hier das Grenzgebirge zu Italien bilden.

Das Département ist Teil der Provence und reicht von der Côte d’Azur bis in das alpine Hinterland. Es grenzt im Westen an das Département Var, im Nordwesten an das Département Alpes-de-Haute-Provence, im Osten und Norden an Italien (Piemont und Ligurien) und im Süden an das Mittelmeer, wo es das Fürstentum Monaco umschließt. Hauptstadt ist Nizza, weitere bekannte Städte sind Cannes und Grasse.

Beschreibung: In Silber auf schwarzem Dreiberg mit blauen Wellen ein roter Adler mit gleichgefärbter Krone.

Claude Salicis listet für das Département 26 Dolmen, 39 Pseudodolmen, 183 Tumuli und 33 Menhire () auf. 

Die Römer hatten bereits im Jahr 7 v. Chr. eine Provinz Alpes Maritimae gegründet. Deren Hauptstadt war "Cemenelum", heute Cimiez, ein Ortsteil von Nizza. Während ihrer größten Ausdehnung Ende des 3. Jahrhunderts umfasste die Provinz auch Digne und Briançon, ihre Hauptstadt war nach Embrun verlegt worden.

Ein Département Alpes-Maritimes mit der Hauptstadt Nizza existierte in Frankreich bereits von 1793 bis 1815. Dessen Grenzen unterschieden sich von denen des heutigen Départements, zumal es Monaco und Sanremo umfasste.

Das aktuelle Département Alpes-Maritimes wurde 1860 geschaffen, als die Grafschaft Nizza zu Frankreich kam. Es wurde aus der Grafschaft, die das Arrondissement Nizza bildete, und einem Teil des Départements Var, die das Arrondissement Grasse bildete, geformt. Letzteres erklärt, warum der Fluss Var das gleichnamige Départment nicht durchfließt: Er bildete zuvor die Grenze zwischen Frankreich und der Grafschaft Nizza, heute jedoch die Grenze zwischen den beiden Arrondissements.

1947 wurde das Territorium des Départements Alpes-Maritimes um die Gemeinden Tende und La Brigue erweitert, deren Einwohner im gleichen Jahr in einem Referendum für den Anschluss an Frankreich votiert hatten und demzufolge Italien die beiden Dörfer abtreten musste. So ging auch endgültig das Schisma der Provence, das mit der Abtrennung von Nizza und dessen Territorium im Jahre 1388 entstanden war, zu Ende.

Die ursprünglichen provenzalischen (bzw. okzitanischen) Ortsnamen und andere geografische Namen (die während der Zugehörigkeit an dem Haus Savoyen italianisiert waren) wurden beim Anschluss der Grafschaft Nizza an Frankreich – anders als das auf Korsika der Fall war – weitestgehend frankophonisiert, so auch die im letzten Abschnitt genannten Gemeinden. Eine Ausnahme könnte die Gemeinde Isola darstellen. Auch von Nizza ist neben dem französischen "Nice" noch die italienische Bezeichnung bekannt, jedoch örtlich nicht gebräuchlich. Darüber hinaus sind auch im vom Département umgebenen Fürstentum Monaco die Stadtbezirke Monte-Carlo, Larvotto, Les Moneghetti in ihrer italienischen Schreibweise erhalten geblieben.

Die amtliche Sprache ist Französisch. Bedingt durch die Geschichte der Grafschaft Nizza, die zwischen 1388 und 1860 von der Provence verwaltungsmäßig abgetrennt war, wird noch in Nizza und Umgebung ein Dialekt der provenzalischen bzw. okzitanischen Sprache gesprochen, welche eine altprovenzalische Form hat und Nissart genannt wird. In den alpinen Teilen des Département (nördlich) wird Alpinprovenzalisch (Gavot) und Brigasque gesprochen, während im Westen das Maritimprovenzalische noch zu hören ist.



Das Département ist im Norden von den Seealpen mit dem südlichsten Dreitausender der Alpen (Mont Clapier 3045 m und der "Cime du Gélas" 3143 m), im Osten von den Ligurischen Alpen und im Westen durch die provenzalischen Voralpen begrenzt. Dadurch bietet sich im Gebiet Vallée du Verdon, Vallée de la Tinée, Vallée du Var, Vallée de l'Estéron, Vallée du Cians, Vallée de la Vésubie und Vallée de la Roya hervorragende Wandermöglichkeiten.
Mehrere große französische GR-Fernwanderwege, "Sentiers de grande randonnée", durchziehen das Département (GR 4, GR 5, GR 52, GR 52A und die Via Alpina) und führen zum Teil auch durch den Nationalpark Mercantour.

Bekannte Wintersportorte sind Isola 2000, Auron, Beuil, Valberg, Peira-Cava und Camp d'Argent am Col de Turini.

Sehenswerte Orte des Départements:

Messstation: Cap Ferrat, 200 Meter vom Meer entfernt
Tage pro Jahr mit

Stand 1991



</doc>
<doc id="324" url="https://de.wikipedia.org/wiki?curid=324" title="Amphore">
Amphore

Eine Amphore bzw. Amphora (von altgriechisch "amphoreus" ‚zweihenkliges Tongefäß‘; gebildet aus "amphí" ‚auf beiden Seiten‘ sowie "phérein" ‚tragen‘) ist ein bauchiges enghalsiges Gefäß mit zwei Henkeln meist aus Ton, aber auch aus Metall (Bronze, Silber, Gold). Durch zwei Henkel sollte ursprünglich das Tragen erleichtert werden. Amphoren sind zu den antiken Vasen zu zählen.

Als Amphore wird jede Töpferware betrachtet, die zwei Henkel hat und deren Basis, die häufig aus einer Spitze oder aus einem Knopf besteht, die vertikale Aufrechthaltung schlecht oder gar nicht ermöglicht.

Die Amphora ist auch eine Maßeinheit. Das Volumen als römisches Hohlmaß beträgt einen römischen Kubikfuß, das sind etwa 26,026 l.

Amphoren wurden in der Antike als Vorrats- und Transportgefäße unter anderem für Öl, Oliven und Wein sowie für Honig, Milch, Getreide, Garum, Südfrüchte wie Datteln und anderes benutzt. Sie wurden in jenen Regionen hergestellt, in denen die Transportgüter erzeugt wurden, also etwa dort, wo Wein- oder Olivenanbau stattfand. Je nach Inhalt ist das Volumen unterschiedlich, Fassungsvermögen betragen zwischen 5 und 50 Liter.

Häufig wurden sie als Einwegbehälter nach dem Transport weggeworfen, so besteht der Monte Testaccio in Rom zu großen Teilen aus Amphorenscherben. Andere Exemplare fanden eine neue Verwendung, etwa als Urne bei Brandbestattungen oder zur Abdeckung der Toten bei Körpergräbern.

Heute werden Amphoren nur mehr zu Zierzwecken, beispielsweise als Vase, hergestellt. Eine besondere Rolle spielt die Amphore bis heute bei der Herstellung spezieller Weine, dem sogenannten „Amphorenwein“. Dieser Ausbau ist vor allem bei „biodynamischen Weinen“ beliebt, aber auch geschwefelte Weine aus Georgien werden häufig in speziellen Amphoren ausgebaut. Siehe auch: Quevri-Wein.

Ein Wandel der Formen sowie häufige Aufschriften bieten Datierungsmöglichkeiten. Absolut datierbare Funde aus Schiffswracks und anderen geschlossenen Funden erlauben eine zeitliche Einordnung. Die Chronologie der vorrömischen Eisenzeit Mitteleuropas bezieht auch die Amphorenchronologie mit ein.

Da Herkunft und Inhalt vieler Amphorenformen bekannt sind, erlauben archäologische Funde darüber hinaus die Rekonstruktion von Handelsverbindungen. Zahlreiche Amphoren weisen auch Amphorenstempel auf.

Es gibt unterschiedliche Typen von Amphoren, die zu verschiedenen Zeiten gebräuchlich waren:

Bei der Halsamphora sind die Henkel am Hals angebracht, der durch einen deutlichen Knick vom Bauch abgegrenzt ist. Es gibt zwei verschiedene Typen der Halsamphora:
Einige Sonderformen der Halsamphora weisen gewisse Besonderheiten auf:

Die Bauchamphora hat im Gegensatz zur Halsamphora keinen abgesetzten Hals, vielmehr geht der Bauch in einer Rundung in den Hals über. Ab der Mitte des 5. Jahrhunderts wurde sie kaum noch hergestellt. Die Pelike ist eine Sonderform der Bauchamphora. Bei ihr ist der Bauch nach unten versetzt, der größte Durchmesser liegt also im unteren Bereich des Vasenkörpers. Die Pelike wurde erstmals am Ende des sechsten Jahrhunderts erschaffen.

Eine Sonderform sind die Panathenäische Preisamphoren mit schwarzfiguriger Bemalung, die zum athenischen Panathenäenfest hergestellt wurden und – offenbar aus kultischen Gründen – die schwarzfigurige Malweise noch jahrhundertelang nach ‚Erfindung‘ der rotfigurigen Malweise beibehielten.

Römische Amphoren dienten vorwiegend zum Transport und zur Lagerung von Grundnahrungsmitteln wie Olivenöl, Wein, Fischsaucen, Früchten und Getreide. Die Kapazität lag häufig bei 25 bis 26 Litern, was erklärt, dass der Begriff "amphora" sich im Laufe der Zeit zu einer wichtigen Maßeinheit für Flüssigkeiten wandelte (26,2 l). Große bauchige Olivenölamphoren aus der Baetica vom Typ "Dressel 20" konnten mit einem Inhalt von 70 l bisweilen auch ein Gesamtgewicht von 100 kg erreichen. Gelegentlich sind Stempel auf diesen angebracht worden, wobei die Forschung unsicher ist, ob diese von den Töpfereien der Amphoren oder vom Produzenten des Olivenöls aufgebracht wurden. Wie die aufgemalten oder eingeritzten Zahlen und Buchstaben ("graffiti" bzw. "tituli picti") sind sie eine bedeutende epigraphische Quelle zur Wirtschaftsgeschichte.

Bis in die 1960er Jahre standen besonders die Amphorenstempel und -formen im Mittelpunkt. In den 1970er und 1980er Jahren fanden internationaler Diskussionsforen zur Amphorenforschung, darunter zur Typologie und Chronologie, statt. Ungefähr 1990 wurden die Amphoren aus Augst/Kaiseraugst zum ersten Mal ausgewertet, die zur Grundlage der Bearbeitung von Amphoren aus Mainz dienten.

Als Zeugen einer vergangenen Handels- und Konsumware stellen sie wichtige potenzielle Informationsträger zur Wirtschaftsgeschichte der Römerzeit dar und geben Auskunft über das Konsumverhalten der damaligen Bevölkerung. Die Amphoren blieben eine lange Zeit unbeachtet. Die erste archäologische Untersuchung unternahm Dressel 1899. Er stellte eine typologische Klassifizierung der Amphoren her, die noch heute als Grundlage für die Bezeichnung der verschiedenen Amphorentypen dient: Weinamphoren, wie Dr. 1, Dr. 2-4, Dr. 5, und Ölamphoren, wie Dr. 20 und Dr. 23, werden weiterhin nach ihm benannt (Dr.=Dressel).

Der deutsche Archäologe Heinrich Dressel kategorisierte Ende des 19. Jahrhunderts die zu seiner Zeit bekannten Amphoren. Die von ihm benannten Typen tragen seinen Namen, ergänzt um eine numerische Bezeichnung, die den Amphorentyp markiert (siehe Bildbeispiel "Dressel 1B"). Dressels Arbeit entstand unter anderem aus der Beschäftigung mit stadtrömischen Funden, darunter mit dem Monte Testaccio einer der größten Fundkomplexe römischer Amphoren, und wurde aufgrund der Kleininschriften im "Corpus Inscriptionum Latinarum" veröffentlicht. Weitere römische Amphorentypen sind nach Forschern wie dem italienischen Unterwasserarchäologen Nino Lamboglia oder Fundorten wie Augst benannt.
Die Behälter wurden meistens dort hergestellt, wo sie zur Abfüllung von Waren benötigt wurden und von wo aus sie verkehrsgünstig zu ihren Absatzgebieten und Bestimmungsorten abtransportiert werden konnten. Aus Form und Herkunft der Amphoren ist es möglich, die transportierten Produkte bzw. Handelswege zu bestimmen. Die Amphoren werden wie die übrige Keramik häufig nach Form, Herkunft und zusätzlich nach Inhalt klassifiziert, da ihre Anzahl zur Bestimmung nur nach Form oder Herkunft zu hoch sind.

Durch die naturwissenschaftlichen Untersuchungen zur Herstellung der Amphoren werden zur Bestimmung der Herkunft die Art der Tonmischung und die Brenntemperatur erkundet. Zuerst wird geprüft, welche und wie viele Tonarten bei der Herstellung benutzt wurden und ob die Gefäße ein natürliches sedimentäres Gefüge haben, bzw. ob zusätzliche Magerungen zur Tonmischung hinzugefügt wurden. Danach bestimmt man die Brenntemperatur. Obwohl auch die Schwach- und Überbrandproben existieren, wurde als Normalbrand eine Brenntemperatur von ca. 950 °C für die meisten italischen Amphoren angestrebt.

Amphoren wurden überwiegend als Transportmittel, Vorratsspeicher oder als Grabbeigaben verwendet. Aber sie dienten in der Antike hauptsächlich zum Transport bestimmter Lebensmittel, sozusagen die Container der Antike. Die Amphoren wurden im Süden mit verschiedene Waren gefüllt und speziell in den Norden verhandelt, wo wegen des anderen Klimas entsprechende Produkte nicht angebaut und hergestellt werden konnten. Die Haupthandelsrouten über Wasserwege führten im Mittelmeer. Großsegler hatten Platz für bis zu 10.000 Amphoren, da sie mehrmals gestapelt werden konnten. In der Regel wurde die gleiche Ware in gleiche Gefäße abgefüllt. Nur in Einzelfällen gibt es Hinweise auf außergewöhnliche Amphoreninhalte. Zu den in den Amphoren importierten Waren kann man beispielsweise Olivenöl aus adriatischen Süditalien (Brindisi) und Nordafrika (Tripolitana I), eingelegte Oliven aus Marokko (Schörgendorfer 558), Weine aus Katalonien, Südfrankreich, Italien (Dressel 2-4), Kreta, Rhodos (Camulodunum 184) und Nordafrika, Fischsauce aus adriatischen Oberitalien (Dressel 6A) oder Feigen und Datteln aus Ägypten und Syrien zählen.

Entweder dominiert innerhalb einer Warengruppe die Amphoren eines Typs oder liegen Amphoren verschiedener Formen in mehr oder weniger gleichen Mengen vor. Bemerkenswert ist, dass wenn sie ihre Funktion, den Warentransport, erfüllt hatten, wurden sie kein zweites Mal in gleicherweise verwendet. Sie wurden entweder ohne weitere Nutzung als Müll entsorgt, oder etwa zum Sarg, Urinal, Baumaterial oder auch antiken Molotow-Cocktail umfunktioniert. Sie waren für eine Weiterverwendung attraktiv aufgrund ihrer massenhaften Verfügbarkeit. In Augst und Kaiseraugst wurden knapp 6.000, in Mainz 5.000, im Mainzer Umland 7.500, in Legionslager von Dangstetten und in Neuss jeweils 1.500 Amphoren gefunden. Über ihre vielfältigen Einsatzbereiche geben neben den Schrift- und Bildquellen auch die archäologischen Befunde und Funde Auskunft. Einige der berühmtesten Beispiele zu den Schütthügeln bzw. Abfalldeponierungen, die aus Amphoren bestehen, wären der Schutthügel des Legionslagers auf dem Kästrich in Mainz und der Amphoren-Depot am Dimesser Ort und am Hopfengarten in Mainz.
In die noch ungebrannten Amphoren werden bestimmte Stempel eingedrückt, die man Graffiti oder Marken "ante cocturam" nennt. Bei den Mainzer Amphoren sind mehr als 200 Ritzungen und Marken zu verzeichnen. Nur wenige davon erlauben Aussagen zu Warenkennzeichnung und Warenbesitzern. "Graffiti" und Marken "ante cocturam" stehen in Zusammenhang mit der Gefäßproduktion und beziehen sich weder auf die abzufüllende Ware noch ihren späteren Besitzer.

Zahlreiche post cocturam-Ritzungen sind derart stark verkürzt oder fragmentiert erhalten, dass eine Deutung nicht möglich ist. Die "Graffiti post cocturam" enthalten vor allem Hohlmasse und nennen Personen oder Gruppen, die als mögliche Produktbesitzer zu interpretieren sind. Die "Graffiti" vermitteln damit andere Informationen als die Pinselaufschriften.

Firmenzeichen wie Dreizack, Anker, Palmette oder Stern geben zusätzlich Auskunft über die Herkunft der Amphoren.
Anders als das importierte Tafelgeschirr handelt es sich bei den Amphoren um reine Transportbehälter, die in Siedlungen, Gräbern und Schiffswracks gefunden werden und größtenteils aus dem Fernhandel stammen. Ihre geographisch und teilweise weite Streuung entspricht dem Vertrieb und Absatz des Inhalts. Daher werden in den Verbreitungskarten die Reise dieser Amphoren dargestellt, nicht das Herstellungsgebiet, die nur durch die Stempel oder naturwissenschaftliche Untersuchungen lokalisiert werden.

Durch die Analysen an den Resten der Inhalte werden Form, Chronologie, Herkunft und importierte Handelsware (meistens mediterranen Lebensmittel) bestimmt. Dies gilt insbesondere für die Amphoren der frühen und mittleren Kaiserzeit, während in der Spätantike der Zusammenhang Form vs. Inhalt nicht immer klar ist.

Von Ausnahmen abgesehen handelt es sich bei den kartierten Fundplätzen um Siedlungsfunde, also im Rahmen der Siedlungsaktivitäten geleerter und schließlich weggeworfener Amphoren, die aus der Literatur und durch Autopsie bekannt geworden sind. Die Verbreitung der Amphoren spiegelt allerdings – wie immer bei archäologischen Karten – auch den Forschungsstand. Verbreitungskarten von Amphoren gab es bisher hauptsächlich für den Mittelmeerraum. Mit der Verteilung und damit den Fragen von Absatzgebieten und Handelswegen in den Provinzen nördlich der Alpen befasste man sich noch wenig.
Antiken Amphoren ähnlich sind der Amphoriskos und die Pithos.




</doc>
<doc id="327" url="https://de.wikipedia.org/wiki?curid=327" title="Akustikkoppler">
Akustikkoppler

Der Akustikkoppler ist ein Gerät zur Übertragung von digitalen Daten über eine analoge Teilnehmeranschlussleitung. Akustikkoppler wurden in den 1970er bis gegen Ende der 1980er Jahre verwendet und erlauben die Datenübertragung über den Hörer von Fernsprechtischapparaten. Bei einem Akustikkoppler ist keine elektrische Verbindung mit dem Festnetzanschluss nötig, die in vielen Ländern und bei vielen Netzbetreibern nicht erlaubt war.

Akustikkoppler kommen zum Einsatz, wenn kein analoges Modem zur Verfügung steht oder eine elektrische Verbindung des Modems mit dem Telefonnetz nicht möglich oder erlaubt ist. Akustikkoppler nutzen den Telefonhörer eines bestehenden Telefons zum Senden und Empfangen der modulierten Tonsignale. Sie verfügen dazu über ein Mikrofon und einen Lautsprecher, die an den entsprechenden Gegenstücken im Telefonhörer befestigt werden.
Es gibt zwei Arten von Akustikkopplern:


Aufgrund ihrer Bauart sind Akustikkoppler störanfällig gegenüber externen Geräuschen und abhängig von der Qualität des Telefons bzw. des Telefonhörers. Aufgrund des unvermeidlichen Verlusts an Signalqualität durch den akustischen Übertragungsschritt erreichen Akustikkoppler nicht die Datenübertragungsraten der direkten elektrischen Verbindung des Modems mit dem Telefonnetz. Bei älteren Akustikkopplern reichen die Übertragungsraten nur von 300 bis 2.400 Bit/s. Spätere Modelle, zum Beispiel "Konexx Coupler" oder "Road Warrior Telecoupler II", erreichen in der Praxis Datenübertragungsraten bis zu 33.600 Bit/s; damit ist etwa das Abholen von E-Mails aus einer Telefonzelle in vertretbarer Zeit möglich.

In den 1980er-Jahren war der Betrieb von selbst gebauten Akustikkopplern im Telefonnetz der Deutschen Bundespost illegal und mit hohen Geldstrafen belegt. Trotzdem nahm die Zahl der selbst gebauten Geräte deutlich zu, nachdem der Chaos Computer Club 1985 in der Hackerbibel eine vergleichsweise einfach zu realisierende Bauanleitung für einen Selbstbau-Akustikkoppler – das sogenannte „Datenklo“ – veröffentlicht hatte.

Parallel zu den Akustikkopplern wurden auch die ersten direkt mit der Telefonleitung verbundenen Modems verfügbar. Auch hier war der Anschluss frei erhältlicher Geräte an das deutsche Telefonnetz verboten; die Post erlaubte in ihrem Netz nur die Verwendung Post-eigener Modems, die entweder monatlich gemietet oder zu – im Vergleich mit frei erhältlichen Geräten – überteuerten Preisen von der Post gekauft werden mussten.

Akustikkoppler werden im deutschen Sprachraum gelegentlich auch als "Datenfön" bezeichnet, nach der einst populären "Dataphon"-Baureihe S21 der Firma Woerltronic aus Cadolzburg.


</doc>
<doc id="328" url="https://de.wikipedia.org/wiki?curid=328" title="Akronym">
Akronym

Ein Akronym (von "ákros" „Spitze, Rand“ sowie "ónoma", dorisch und äolisch "ónyma", „Name“) ist ein Sonderfall der Abkürzung. Akronyme entstehen dadurch, dass Wörter oder Wortgruppen auf ihre Anfangsbestandteile gekürzt werden.

Für den Begriff Akronym gibt es zwei konkurrierende Definitionen:

Den großen Wörterbüchern des Deutschen zufolge, z. B. Duden und Wahrig, ist ein Akronym ein Kurzwort, das aus den Anfangsbuchstaben mehrerer Wörter zusammengesetzt ist, wobei "EDV" (elektronische Datenverarbeitung) als Beispiel genannt wird. "ADAC", "PC" und "TÜV" sind demnach Akronyme, da sie aus den Anfangsbuchstaben der ihnen zugrunde liegenden Ausdrücke bestehen. Keine Akronyme sind Abkürzungen wie "Abk.", "lt.", "Betr." oder "kpl."

In Fachlexika der Linguistik finden sich weitere Definitionen: „Aus den Anfangsbuchstaben oder -silben einer Wortgruppe oder eines Kompositums gebildete Abkürzung, die als Wort verwendet wird.“ Die Sprachwissenschaftlerin Hadumod Bußmann definiert den Begriff entsprechend. Anders als in der ersten Definition werden hier also nicht nur Anfangsbuchstaben, sondern auch (gekürzte) Anfangssilben berücksichtigt.

Bußmann unterteilt Akronyme in unterschiedliche Typen:

Duden, Wahrig sowie Bußmann und Glück behandeln "Initialwort" als Synonym für "Akronym". Nach Duden und Wahrig ist ein Initialwort damit eine Sonderform des Buchstabenworts (siehe unten), die sich nur aus den Anfangsbuchstaben, also den Initialen der Wörter, zusammensetzt. So steht zum Beispiel das Initialwort LASER für "". Nach Bußmann und Glück ist auch das Silbenkurzwort ein Akronym.

Es können auch Namen als Grundlage für Initialwörter eingesetzt werden. Bei der nach Axel Lennart Wenner-Gren benannten "Alwegbahn" war zudem eine Assoziation mit „alle Wege“ durchaus beabsichtigt.

"Silbenkurzwörter" (auch Silbenwörter) sind Abkürzungen, die aus den Anfangssilben der zugrundeliegenden Ausdrücke bestehen: Kripo für Kriminalpolizei, Trafo für Transformator, Elko für Elektrolytkondensator. Diese sind verwandt mit den Kopfwörtern, wie Auto für Automobil oder Akku für Akkumulator, und den Schwanzwörtern, wie Bus für Omnibus.

Eine ähnliche Kurzwortform wird aus dem Anfang mehrerer Wörter gebildet. Zum Beispiel steht "Haribo" für Hans Riegel aus Bonn (Süßwarenhersteller) sowie "Chipitts" für die Region um die Städte Chicago und Pittsburgh in den USA. Hierbei werden oft Zusammensetzungen genutzt, die gut zu sprechen sind.

Weitere Beispiele von Silbenkurzwörtern:

Als Apronym bezeichnet man ein Akronym, das ein bereits existierendes Wort ergibt. Dies bedeutet, dass potenziell jedes Wort ein Apronym werden kann, wenn die einzelnen Buchstaben als Anfangsbuchstaben einer Phrase stehen können. Die meisten Apronyme haben einen gewollten Bezug zu der Sache, die sie bezeichnen. Beispiele:


Apronyme dienen oft als Namen für EU-Förderprogramme oder US-amerikanische Gesetze. Zum Beispiel steht Erasmus für European community action scheme for the mobility of university students oder LEADER für Liaison entre actions de développement de l’économie rurale. Die Abkürzung USA PATRIOT Act steht für ".

Als Backronym [ˈbækɹənɪm] („rückwärts-Apronym“) bezeichnet man Wörter, die erst nachträglich die (oft scherzhafte) Bedeutung einer Abkürzung erhalten haben. Beispiele hierfür:

Ein Akronym kann mehrschichtig (verschachtelt) sein. Ein Beispiel hierfür ist BDSM: B&D, D&S, S&M stehen für ". 

Als rekursives Akronym bezeichnet man ein Akronym oder eine Abkürzung, die in der Erklärung ihrer Bedeutung auf sich selbst verweist. Rekursive Akronyme findet man häufig in der Computertechnik. Beispiele:

Es gibt verschiedene Wortformen, die den Akronymen ähneln, ohne dass sie einer der beiden angegebenen Definitionen genügen.

Ein Buchstabenwort ähnelt dem Initialwort, setzt sich aber aus beliebigen Einzelbuchstaben der Vollform der Wörter zusammen: zum Beispiel DAX als Abkürzung für Deutscher Aktienindex, wo der letzte Buchstabe des gekürzten Ausgangswortes berücksichtigt ist.

Die Schreibweise von Akronymen besteht meist aus einer Aneinanderreihung von Großbuchstaben. Bei Akronymen, die wie ein Wort ausgesprochen werden, hat sich aber im Lauf der Zeit auch eine Schreibweise entwickelt, die derjenigen normaler Substantive gleicht (z. B. "Radar, Laser, Aids, Nato, Unicef"; nicht aber "KKW, SMS, HIV, USA"). Da Akronyme ohne Punkt geschrieben werden, ist in solchen Fällen weder durch die Aussprache noch durch das Schriftbild erkennbar, dass es sich ursprünglich um ein Kunstwort handelt; jedoch widerspiegelt sich dadurch die Aussprache im Schriftbild.

Im Internet werden Akronyme häufig verwendet, um eine Handlung oder eine Gemütslage auszudrücken. So ist LOL (Laughing Out Loud) die Bezeichnung, wenn ein Chatter lachen muss. ROFL (') ist noch eine Steigerung, in dem Fall kann sich der Chatter vor Lachen kaum noch halten. Wie diese beiden Beispiele werden die meisten Chat-Akronyme aus der englischen Sprache übernommen. Ein weiteres häufig verwendetes Akronym ist AFK ('), das verwendet wird, um eine vorübergehende Abwesenheit mitzuteilen. Auch in Foren häufig verwendet werden IMHO (') und AFAIK (').

Begriffe wie „cu“ oder „l8r“ sind keine Akronyme, sondern homophone Abkürzungen, das heißt, sie klingen gelesen wie der auszudrückende Satz (', '), sind aber keine Initialworte.

Um auf Webseiten Wörter als Abkürzungen zu markieren, stehen die zwei HTML-Elemente codice_1 (von englisch "" „Abkürzung“) und codice_2 zur Verfügung. Screenreader erkennen diese Elemente. Sie brauchen also nicht mehr zu „raten“, ob es sich bei einem Wort um eine Abkürzung handelt, sondern passen die Aussprache entsprechend an. Beiden Elementen kann zugewiesen werden, wofür die Abkürzung steht. Dies kann dann von einem Vorleseprogramm (engl. „screen reader“) anstelle der Kurzform wiedergegeben werden. Die Wahl zwischen codice_1 und codice_2 gibt dem Programm einen Hinweis darauf, ob die Abkürzung als Wort – codice_2 – oder in einzelnen Buchstaben – codice_1 – vorgelesen werden sollte.

Vom World Wide Web Consortium wird empfohlen, vorrangig codice_1 zu benutzen. Diese Vereinfachung geht allerdings auf Kosten der Barrierefreiheit. Akronyme, die eigentlich als Wort gesprochen werden sollten, werden nicht mehr als solche erkannt.

Die Abkürzungen werden mittels Start- und End-Tags als Elemente ausgezeichnet. Mit dem codice_8-Attribut wird die Bedeutung angegeben. Für Vorleseprogramme, die nicht darauf eingestellt sind, die Bedeutung vorzulesen, spielt die Wahl des Elements eine Rolle.
<abbr title="Hypertext Markup Language">HTML</abbr>

Der Rechner liest „Haa Tee Emm El“ vor.

<acronym title="National Aeronautics and Space Administration">NASA</acronym>

Durch die Auszeichnung als codice_2 liest der Screenreader „Nasa“ und nicht „Enn Aa Ess Aa“ vor.

Generell gilt, dass Kurzwörter, also auch Akronyme, bedeutungsgleich mit den Ausdrücken verwendet werden, die ihnen zugrunde liegen (= Vollformen). Abweichend davon kann der Plural auch mit "-s" gebildet werden. Auch die Wortbildung eröffnet bei Akronymen besondere Möglichkeiten: So kann man eine "-ler"-Ableitung bilden, die bei der Vollform nicht möglich ist: "SPDler."

Das Prinzip der Gleichwertigkeit von Vollform und Akronym hinsichtlich ihrer Bedeutung setzt jedoch voraus, dass dem Verwender die Vollform auch bekannt ist. Wenn dies nicht der Fall ist, kann es zu Bedeutungswandel und Lexikalisierung kommen. Lexikalisierungstendenzen zeigen sich zum Beispiel bei der Bezeichnung "BAföG," das meist als monetäre Leistung und nicht länger als das dahinter stehende "Bundesausbildungsförderungsgesetz" verstanden wird.

Ähnlich verläuft es bei der „SMS“: „SMS“ bedeutet "" und beschreibt den Dienst, der das Versenden von Kurzmitteilungen ermöglicht. Die Nachricht selbst wäre also eher eine „SM“ (oder „Kurznachricht“). Trotzdem hat es sich eingebürgert, als „SMS“ die Nachricht zu bezeichnen, zumal die korrekte Abkürzung („SM“) im allgemeinen Sprachgebrauch schon vergeben ist.

Es gibt Kritiker, die Wortbildungen wie "LCD-Display" ablehnen, da das „D“ in der Abkürzung bereits für ' steht ('). Ähnlich verhält es sich mit dem HIV-Virus, wo das „V“ bereits für „Virus“ steht, der ABM-Maßnahme (ABM = Arbeitsbeschaffungsmaßnahme), dem CSS-Stylesheet (CSS = Cascading Style Sheet), dem PDF-Format (PDF = Portable Document Format) oder der PIN-Nummer (PIN = Persönliche Identifikationsnummer).

Im deutschen Handelsrecht wird aus diesem Grund eine GmbH ("Gesellschaft mit beschränker Haftung") als "mbH" bezeichnet, wenn sich der Begriff "„Gesellschaft“" bereits im Eigennamen des Unternehmens befindet (z. B. Württembergische Eisenbahn-Gesellschaft mbH).

Im Englischen wird diese Redundanz rekursiv als „RAS-Syndrom“ ("Redundantes-Akronym-Syndrom-Syndrom") bezeichnet. Diese Verdopplungen können rhetorisch als Tautologie (als Aussage) beziehungsweise als Pleonasmus (als Ausdruck) gesehen werden.





</doc>
<doc id="331" url="https://de.wikipedia.org/wiki?curid=331" title="Amnesty International">
Amnesty International

Amnesty International (von ‚ Begnadigung, Straferlass, Amnestie) ist eine nichtstaatliche (NGO) und Non-Profit-Organisation, die sich weltweit für Menschenrechte einsetzt. Grundlage ihrer Arbeit sind die Allgemeine Erklärung der Menschenrechte und andere Menschenrechtsdokumente, wie beispielsweise der Internationale Pakt über bürgerliche und politische Rechte und der Internationale Pakt über wirtschaftliche, soziale und kulturelle Rechte. Die Organisation recherchiert Menschenrechtsverletzungen, betreibt Öffentlichkeits- und Lobbyarbeit und organisiert unter anderem Brief- und Unterschriftenaktionen für alle Bereiche ihrer Tätigkeit.

Amnesty International wurde 1961 in London von dem englischen Rechtsanwalt Peter Benenson gegründet. Ihm soll die Idee zur Gründung gekommen sein, als er in der Zeitung zum wiederholten Mal von Folterungen und gewaltsamer Unterdrückung las, mit der Regierungen gegen politisch andersdenkende Menschen vorgingen. In einem 1983 geführten Interview erinnerte sich Benenson, dass der Artikel von zwei portugiesischen Studenten gehandelt habe, die in einem Restaurant in Lissabon auf die Freiheit angestoßen hatten und daraufhin zu Haftstrafen verurteilt worden waren. Nachträgliche Recherchen ergaben, dass es sich möglicherweise um eine Notiz in The Times vom 19. Dezember 1960 handelte, die allerdings keine Details über die „subversiven Aktivitäten“ der Verurteilten enthielt. Am 28. Mai 1961 veröffentlichte Benenson in der britischen Zeitung "The Observer" den Artikel „The Forgotten Prisoners“ („Die vergessenen Gefangenen“), in dem er mehrere Fälle nennt, darunter Constantin Noica, Agostinho Neto und József Mindszenty, und die Leser aufrief, sich durch Briefe an die jeweiligen Regierungen für die Freilassung dieser Gefangenen einzusetzen. Er schrieb: „Sie können Ihre Zeitung an jedem beliebigen Tag der Woche aufschlagen und Sie werden in ihr einen Bericht über jemanden finden, der irgendwo in der Welt gefangen genommen, gefoltert oder hingerichtet wird, weil seine Ansichten oder seine Religion seiner Regierung nicht gefallen.“ Die aus diesem Artikel entstandene Aktion "Appeal for Amnesty, 1961" gilt als der Anfang von Amnesty International. Zu den Gründungsmitgliedern gehörten Eric Baker und der irische Politiker Seán MacBride, der von 1961 bis 1974 auch Präsident der Organisation war. 

Obwohl sich Amnesty International als Organisation beschreibt, die für Menschen aller Nationalitäten und Religionen offensteht, kamen die Mitglieder anfangs vor allem aus der englischsprachigen Welt und Westeuropa. Diese Beschränkung ließ sich mit dem Kalten Krieg erklären. Versuche, Amnesty-Gruppen in Osteuropa zu gründen, stießen auf große Schwierigkeiten. Das lag nicht nur an der staatlichen Repression, sondern auch an unterschiedlichen Interessen, die westliche und osteuropäische Menschenrechtsaktivisten verfolgten.

Das Logo ist eine mit Stacheldraht umwickelte Kerze. Es wurde von der englischen Künstlerin Diana Redhouse geschaffen, die sich durch das Sprichwort "Es ist besser, eine Kerze anzuzünden, als sich über die Dunkelheit zu beklagen" inspirieren ließ.

Die deutsche Sektion hatte bereits in den 1970er Jahren beschlossen, dieses Logo für sich nicht mehr zu verwenden. Stattdessen wurde bis 2008 ein blau-weißes Logo mit Kleinbuchstaben genutzt. In Deutschland, Österreich und der Schweiz wurde bis Mitte 2008 eine heute nicht mehr verwendete Schreibweise mit Kleinbuchstaben und Abkürzungen verwendet: amnesty international, ai oder amnesty. Mitte 2008 wurde international ein neues, einheitliches Layout eingeführt, das die Farben Gelb und Schwarz verwendet. Das Logo enthält den Schriftzug "Amnesty International" in Großbuchstaben und die mit Stacheldraht umwickelte Kerze.

Die bundesdeutsche Sektion wurde Ende Juni 1961, zwei Monate nach Gründung der internationalen Organisation, von Gerd Ruge, Carola Stern und Felix Rexhausen in Köln gegründet und im Juli 1961 als erste Sektion anerkannt. Damals hieß sie „Amnestie-Appell“. Sie setzte sich zum Beispiel für in der DDR inhaftierte politische Gefangene ein. Nach dem Fall der Mauer wurde die Organisation auch in den neuen Bundesländern aktiv, wo sie bis dahin verboten war.

Amnesty International Österreich wurde am 4. Mai 1970 gegründet. AI Österreich gehörte am 14. November 2001 zu den ersten 44 Organisationen, die das Österreichische Spendengütesiegel verliehen bekamen. Generalsekretär ist Heinz Patzelt (Stand Januar 2016).

Offiziell gegründet wurde die Schweizer Sektion 1970; Doch schon 1964 gab es die erste Sektion in Genf, deren Initiator Seán MacBride war, damaliger Generalsekretär und Mitbegründer von Amnesty International. Der erste Mitarbeiter wurde 1976 eingestellt, im Jahre 1987 waren es 14, und im Jahr 2000 schon deren 28. Derzeitige Geschäftsleiterin ist Manon Schick.

Amnesty International zählt nach eigenen Angaben mehr als sieben Millionen Mitglieder und Unterstützer in mehr als 150 Staaten. In 53 Staaten gibt es Sektionen, die eine kontinuierliche Menschenrechtsarbeit garantieren. Die größeren Sektionen unterhalten in der Regel ein Sekretariat mit hauptamtlichen Mitarbeiterinnen und Mitarbeitern. Die Sektion koordiniert die Arbeit der Mitglieder und ist die Verbindungsstelle zwischen den Gruppen und dem Internationalen Sekretariat in London. Die Sektionen entsenden Vertreter in den internationalen Rat ( "International Council Meeting, ICM"), das oberste Gremium von Amnesty auf internationaler Ebene, das alle zwei Jahre zusammentritt. Der Rat legt Strategie und Arbeitsweise von Amnesty fest und wählt das Internationale Exekutivkomitee, dem die Führung der laufenden Geschäfte der Organisation obliegt. Unter der Verantwortung des Exekutivkomitees steht auch das Internationale Sekretariat in London, an dessen Spitze der Internationale Generalsekretär steht. Von 2001 bis Dezember 2009 war dies die Bengalin Irene Khan. Ihr Nachfolger ist Salil Shetty, der aus Indien stammt. Seit kurzem bemüht sich die Organisation, ihre Präsenz in Ländern des globalen Südens zu verstärken, indem dort neue Büros eingerichtet werden. Die dazu notwendige Umstrukturierung, auch im sensiblen Bereich der Recherchearbeit, ist derzeit umstritten (Stand 2013).

Mitgliedschaft und Strukturen sind in der Satzung und einem Arbeitsrahmen geregelt. Mitglieder können sich einer Gruppe anschließen. Von Gruppen wird aktiver Einsatz durch gezielte Aktionen vor Ort, Briefeschreiben, Öffentlichkeitsarbeit und Spendeneinwerbung erwartet. Alle Mitglieder erhalten auch unabhängig von Gruppenaktivitäten Mitmachangebote. In Deutschland gibt es rund 30.000 Mitglieder, davon ca. 9000 in über 600 lokalen Gruppen, die in 43 Bezirke aufgeteilt sind. Daneben gibt es sogenannte Koordinationsgruppen, die die Arbeit zu einzelnen Ländern oder bestimmten Menschenrechtsthemen sektionsweit koordinieren. Etwa 70.000 Förderer unterstützen die Organisation durch regelmäßige Beiträge. Geleitet und nach außen vertreten wird die deutsche Sektion durch einen ehrenamtlichen Vorstand, der aus sieben Mitgliedern besteht. Vorstandssprecherin ist Gabriele Stein, die gemeinsam mit dem Vorstandsmitglied für Finanzen, Roland Vogel, den Verein gesetzlich vertritt.

1999 bezog Amnesty International Deutschland Räume im „Haus der Demokratie und Menschenrechte“ in der Greifswalder Straße in Berlin. 2012 gab das Sekretariat seinen Sitz in Bonn endgültig auf. Aus Platzgründen sind aber nur noch das Büro des Bezirks Berlin-Brandenburg sowie das Regionalbüro Ost im „Haus der Demokratie und Menschenrechte“ ansässig, das Sekretariat der Sektion befindet sich nun in der Zinnowitzer Straße. Darüber hinaus gibt es Regionalbüros in München (seit 2011) und in Düsseldorf (seit 2016), welche die Mitglieder im Süden bzw. Westen Deutschlands unterstützen.

Das Sekretariat erledigt administrative Aufgaben für die Mitglieder, macht Öffentlichkeitsarbeit und übernimmt Lobbyismusarbeit. Es beschäftigt über 60 Teil- und Vollzeitkräfte und wird von Markus N. Beeko als Generalsekretär geleitet, der zum September 2016 Selmin Çalışkan abgelöst hat.

Einmal jährlich findet über zweieinhalb Tage zu Pfingsten die Jahresversammlung der deutschen Sektion statt. Alle Mitglieder sind antrags- und stimmberechtigt, Gruppen haben zusätzliches Stimmrecht. Förderer haben kein Stimmrecht und können nicht teilnehmen. Die Jahresversammlung wählt den siebenköpfigen, ehrenamtlichen Vorstand und beschließt Schwerpunkte der inhaltlichen Arbeit der Sektion. Die Diskussionen sind vertraulich („intern“), nur auf Beschluss der Jahresversammlung können einzelne Beschlüsse öffentlich gemacht werden.

Die deutsche Sektion finanziert sich überwiegend aus Mitglieds- und Fördererbeiträgen und Spenden, zu einem geringeren Teil aus Erbschaften, Verkaufserlösen, Geldbußen und Sammlungen. Seit mehreren Jahren führt die Organisation „Direktdialoge“ in Städten durch, um Förderer zu gewinnen. Im Jahr 2016 wurden ca. 20,3 Millionen Euro eingenommen. Davon wurden etwa 5,9 Millionen Euro an das internationale Sekretariat abgeführt. Zur Unterstützung der Arbeit von Amnesty International wurde im Mai 2003 die "Stiftung Menschenrechte – Förderstiftung Amnesty International" mit Sitz in Berlin gegründet.

Jährlich erscheint der „"Amnesty International Annual Report"“, der die Menschenrechtslage in ca. 160 Ländern und Territorien beschreibt. Die deutsche Version erscheint einige Monate später im S. Fischer Verlag.

Die Organisation recherchiert fortlaufend zur Menschenrechtssituation weltweit und führt Aktionen gegen spezifische Menschenrechtsverletzungen durch. Der Jahresbericht der Organisation ("Amnesty International Report") enthält einen Überblick über die Lage der Menschenrechte in fast allen Ländern der Erde.

Die Organisation hat sich sieben Ziele unter dem Motto "Gerechtigkeit globalisieren!" gesetzt:


Von 2005 bis 2009 lief die internationale Kampagne „Gewalt gegen Frauen verhindern“, die sich gegen die vielfältigen Formen von Gewalt gegen Frauen, sowohl staatlicherseits als auch im häuslichen Umfeld, wandte. Nach einer schwierigen und kontroversen internen Diskussion beschloss die internationale Ratstagung der Organisation 2007 in Morelos, Mexiko, eine begrenzte Position zum Schwangerschaftsabbruch. So soll die völlige Entkriminalisierung gefordert werden sowie Staaten aufgefordert werden, Abtreibung im Falle von Vergewaltigung, sexueller Nötigung, Inzest und bei schwerwiegender Gefahr für das Leben einer Frau zu legalisieren. Die Organisation bekräftigt, dass viele gesellschaftliche Faktoren und Zwänge zu ungewollten Schwangerschaften beitragen und damit auch zu der – weltweit jährlich in ca. 26 Millionen Fällen illegalen – Entscheidung der Frauen.

Im Mai 2016 nahm die Organisation, inklusiver aller ihrer Landesverbände, die Forderung auf, Prostitution zu legalisieren. Man setze sich für die Menschenrechte der Sexarbeiter ein, nicht für ein Recht auf käuflichen Sex. Der Entscheidung waren drei Jahre Sichtung von Forschungsberichten verschiedener Institutionen wie der WHO, UNAIDS und dem UN-Sonderberichterstatter für das Recht auf physische und mentale Gesundheit und ein Beschluss des Entscheidungsgremiums "International Council Meeting" vorausgegangen.

Zu den typischen Aktionsformen der Organisation zählen:

Die Organisation führt immer wieder große und kleine, internationale Themenkampagnen durch, die teilweise über mehrere Jahre angelegt sind.

Internationale größere Schwerpunkte sind derzeit (2014/15):

Bis 2013, zum Abschluss des Vertrags über den Waffenhandel am 2. April 2013 war Amnesty an der Kampagne "Control Arms" beteiligt.

1988 gab es eine internationale Amnesty-Konzerttour unter dem Titel "Human Rights NOW!". Am 10. Dezember 2005 – dem Internationalen Tag der Menschenrechte – wurde ein neues Musikprojekt unter dem Titel "Make Some Noise" gestartet. Dabei veröffentlichten bekannte internationale Künstler, darunter The Black Eyed Peas, Serj Tankian und The Cure, Coverversionen von John-Lennon-Songs exklusiv auf der Website von Amnesty. Parallel zur Musik werden dort konkrete Kampagnen und Fälle vorgestellt.

Die deutsche Sektion vergibt seit 1998 alle zwei Jahre den Amnesty International Menschenrechtspreis.
2016 wurde der Preis an den indischen Rechtsanwalt Henri Tiphagne vergeben.

Regierungen und nahestehende Kommentatoren, die von Amnesty International in ihren Berichten kritisch beurteilt werden, haben verschiedentlich Kritik an Amnesty geübt. So wurde Amnesty z. B. aus China, Russland und dem Kongo Einseitigkeit gegen nicht-westliche Länder bei seinen Beurteilungen vorgeworfen sowie, dass die Sicherheitsbedürfnisse (z. B. bei der Bekämpfung von Rebellen) nicht genügend beachtet würden. Umgekehrt wurde Amnesty z. B. nach der Kritik an der israelischen Politik im Gazastreifen vom American Jewish Congress angegriffen. Als im Mai 2005 ein Amnesty-Bericht den USA eine Spitzenstellung bei Menschenrechtsverletzungen zuwies (siehe hierzu: Gefangenenlager der Guantanamo Bay Naval Base), bezeichnete ein Pressesprecher des Weißen Hauses dies als lächerlich und behauptete, die Angaben entsprächen nicht den Tatsachen. 

Neben Vorwürfen der Einseitigkeit gab es kritische Stimmen, die bemängelten, Amnesty sei zu sehr auf Öffentlichkeitsarbeit ausgerichtet. Im Jahr 2002 warf der Jura-Professor Francis Boyle (ehemaliges AI-Exekutivkomiteemitglied in den USA) Amnesty vor, an erster Stelle stünde die öffentliche Aufmerksamkeit ("publicity"), dann würden Spendengelder und Mitglieder angeworben, es fänden interne Machtkämpfe statt, und die Menschenrechte als Ziel kämen erst am Schluss.

Auf der internationalen Ratstagung in Dakar im August 2001 wurde eine Ausweitung des Mandats auf den Einsatz auch für wirtschaftliche, soziale und kulturelle Rechte beschlossen. Danach äußerten einige Mitglieder, AI verliere an Profil und dehne sein Betätigungsfeld zu sehr aus. AI könne zu einem „Menschenrechts-Gemischtwarenladen“ mutieren und an Glaubwürdigkeit verlieren. AI solle sich weiterhin auf bürgerliche und politische Rechte konzentrieren. Diese Bedenken wurden im Jahre 2010 in einem BBC-Beitrag zum 50. Geburtstag der Organisation aufgegriffen. Darin wurde behauptet, Amnesty International habe es bis dato nicht geschafft, eine nennenswerte Anzahl von Mitgliedern außerhalb von Europa, den Vereinigten Staaten, Kanada, Australien und Neuseeland zu gewinnen.

Amnesty International vertrat vor dem Ersten Golfkrieg (1990–1991) die Brutkastenlüge – die von einer US-amerikanischen PR-Firma fabrizierte Geschichte, irakische Truppen hätten Babys aus Brutkästen eines kuwaitischen Krankenhauses gerissen.

Amnesty wurde 2002 vorgeworfen, das Apartheid-System in Südafrika nie als Ganzes verurteilt zu haben.

Im April 2007 machte AI bekannt, von nun an für eine Entkriminalisierung des Schwangerschaftsabbruches in gewissen Grenzen sowie für ein Recht auf einen Schwangerschaftsabbruch im Falle von Vergewaltigung, Inzest oder schwerwiegender Gefahr für Gesundheit oder Leben der Mutter einzutreten. Der Kurienkardinal der römisch-katholischen Kirche, Renato Raffaele Martino, äußerte in einem Interview, Katholiken und kirchliche Organisationen sollten überlegen, ob sie AI weiter unterstützen könnten. Amnesty International erwiderte darauf, sich nicht für ein universelles Recht auf Abtreibung einzusetzen, sondern für die Entkriminalisierung von Frauen in einer Notlage. In der Umsetzung dieser Politik bezeichnete AI im Jahre 2009 ein völliges Verbot des Schwangerschaftsabbruchs als Folter im Sinne der Anti-Folterkonvention.

2014 führte Amnesty International eine interne Konsultation dazu durch, ob die Prostitution und ihr Umfeld entkriminalisiert werden sollen. Die internen Dokumente gerieten an die Öffentlichkeit und lösten international eine heftige Debatte aus. In Deutschland kritisierte u. a. die feministische Zeitschrift "Emma" die Überlegungen, auch Bordelle und die Arbeitgeber von Prostitution zu entkriminalisieren. Ein entsprechendes Dokument, das auf dem Treffen des Internationalen Rates der Menschenrechtsorganisation vom 7. bis 11. August 2015 in Dublin beschlossen wurde, erregte erneut internationales Aufsehen. Kritik an der Unterstützung der Legalisierung der Prostitution kommt u. a. aus katholischen Kreisen, wobei in diesem Zusammenhang auch die Bezeichnung „Menschenrechtsorganisation“ in Frage gestellt wird.

Anfang 2017 kritisierte Amnesty International die operativen Maßnahmen der Kölner Polizei zur Silvesternacht 2016/17 als rassistisch (Racial Profiling), während die Polizei und die meisten Politiker solchen Vorwürfen widersprachen und die Verhinderung einer Wiederholung von gruppenweisen sexuellen Übergriffen wie Silvester 2015/16 lobten.






</doc>
<doc id="337" url="https://de.wikipedia.org/wiki?curid=337" title="Abteilung (Biologie)">
Abteilung (Biologie)

Die Abteilung ("Divisio") ist eine hierarchische Stufe der biologischen Systematik.

In der Botanik ist sie nach dem ICBN als höchste Rangstufe unterhalb des Reichs ("Regnum") bzw. Unterreichs ("Subregnum") vorgesehen. Abteilungen enden auf -phyta bei Pflanzen bzw. auf -mycota bei Pilzen. Die Abteilung in der Botanik entspricht dem Stamm ("Phylum") in der zoologischen Systematik und wird entsprechend auch in der Botanik gelegentlich als "Phylum" bezeichnet.

In der zoologischen Systematik wurde die Abteilung teilweise als Rangstufe zwischen dem Reich und dem Stamm verwendet. Andere Autoren nannten andere Rangstufen Abteilung (divisio), teilweise zwischen Klasse und Ordnung, einige auch zwischen Ordnung und Familie. Andere haben den Begriff zur Unterteilung von Gattungen verwendet. Diese Verwendungen sind nicht mehr gebräuchlich.



</doc>
<doc id="338" url="https://de.wikipedia.org/wiki?curid=338" title="Art (Biologie)">
Art (Biologie)

Die Art, auch Spezies oder Species (abgekürzt oft "spec.", von ‚Art‘), ist die Grundeinheit der biologischen Systematik. Jede biologische Art ("Spezies") ist ein Resultat der Artbildung. Bislang gelang keine allgemeine Definition der „Art“, die die theoretischen und praktischen Anforderungen aller biologischen Teildisziplinen gleichermaßen erfüllt. Vielmehr existieren in der Biologie verschiedene Artkonzepte, die zu unterschiedlichen Klassifikationen führen. Historisch wie auch aktuell spielen zwei Ansätze von Artkonzepten eine wichtige Rolle:


Mit dem Aufkommen der Kladistik ist seit den 1950er Jahren der auf dem biologischen Artbegriff beruhende, chronologisch definierte phylogenetische Artbegriff hinzugekommen, nach dem eine Art mit der Artspaltung, also der Bildung zweier Arten aus einer Ursprungsart, beginnt und mit ihrer erneuten Artspaltung oder aber ihrem Aussterben endet.

Das Problem der Artdefinition besteht aus zwei Teilproblemen:


Die Hauptunterschiede der verschiedenen Artkonzepte liegen dabei auf der Ebene der Rangbildung. Eine Gruppe von Lebewesen unabhängig von ihrem Rang bezeichnen Taxonomen als "Taxon" (in der Botanik auch "Sippe").

Eine Art als Taxon ist eine gemäß den Regeln der Taxonomie und der biologischen Nomenklatur formal beschriebene und benannte Form von Lebewesen. Eine taxonomische Art stellt eine wissenschaftliche Hypothese dar und kann unabhängig von einem Artkonzept sein, sofern man zumindest akzeptiert, dass Arten reale und individuelle Erscheinungen der Natur sind. Die Art ist eine Rangstufe der klassischen, auf Carl von Linné zurückgehenden Taxonomie. Einige rein merkmalsbezogen arbeitende Systematiker sind der Ansicht, Arten wären mehr oder weniger willkürlich zusammengestellte, künstliche Gruppen, nur die Individuen seien letztlich real: Manche gehen dabei so weit, dass der Artbegriff wie alle anderen Rangstufen ihrer Ansicht nach besser abgeschafft werden sollte. Die meisten Biologen sind aber der Ansicht, dass Arten natürliche Einheiten mit realer Existenz darstellen; es gäbe dann Artkriterien, an denen sich reale Arten identifizieren ließen. Dieser Vorstellung liegt letztlich eine Unterscheidung zwischen durch Genfluss oder horizontalem Gentransfer geprägten Einheiten unterhalb des Artniveaus und den Arten, bei denen dies nicht zutrifft (engl.: lineages), zu Grunde. Für viele Biologen, darunter Anhänger eines phylogenetischen Artkonzepts (vgl. unten), sind sie sogar die einzigen in diesem Sinne natürlichen taxonomischen Einheiten

Der wissenschaftliche Name einer Art (oft lateinischen oder griechischen Ursprungs) setzt sich nach der von Carl von Linné 1753 eingeführten binären Nomenklatur aus zwei Teilen zusammen, die beide kursiv geschrieben werden. Der erste Teil dieses Namens ist der groß geschriebene Gattungsname. Der zweite Teil wird immer klein geschrieben und in der Botanik und bei Prokaryoten als Epitheton („specific epithet“) bezeichnet, in der Zoologie als Artname oder Artzusatz („specific name“) Um Verwechslungen zwischen dem Artzusatz und dem gesamten Artnamen, also dem Binomen aus Gattungsname und Artzusatz, zu vermeiden, werden in der Zoologie entweder die eindeutigen englischen Begriffe verwendet oder hinzugefügt oder gelegentlich und informell auch Begriffe wie „epithetum specificum“ oder „epitheton specificum“ verwendet.

Beispiele

Sowohl in der Botanik (Code Article 46) als auch in der Zoologie (Code Article 51) wird empfohlen, dem wissenschaftlichen Artnamen die Namen der Autoren beizufügen, die die Art beschrieben haben, zumindest, wenn es um taxonomische oder nomenklatorische Fragen geht. Dies ist zum Beispiel wichtig, um Homonyme zu erkennen, das sind Fälle, in denen zwei Autoren versehentlich zwei verschiedene Arten mit demselben Namen benannt haben. Im Geltungsbereich des International Code of Botanical Nomenclature wird es empfohlen, die Autorennamen abzukürzen, wobei in der Regel das Namensverzeichnis von Brummit und Powell als Grundlage dient (vergleiche Artikel Autorenkürzel der Botaniker und Mykologen), „L.“ steht beispielsweise für Linné.


Nach den Regeln des International Code of Zoological Nomenclature sollen zumindest einmal in jedem wissenschaftlichen Text dem Artnamen die Autor(en) und das Jahr der Publikation hinzugefügt werden (Code Recommendation 51a). Wenn im entsprechenden Fachgebiet zwei Autoren mit demselben Nachnamen tätig waren, soll der abgekürzte Vorname hinzugefügt werden, um Eindeutigkeit herzustellen. Wenn die Art heute in eine andere Gattung gestellt wird als in die, in der sie ursprünglich beschrieben wurde, müssen Autor(en) und Jahr in Klammern gesetzt werden (Code Article 51.3). Zwischen Autor und Jahr wird in der Regel ein Komma gesetzt.


Die Philosophen der Antike kannten noch keine systematischen Konzepte und somit keinen Artbegriff im heutigen Sinne. Von Aristoteles sind als erstem Philosophen Schriften bekannt, in denen zwei getrennte – allgemein philosophisch zu verstehende – Begriffe είδος ("eidos", ins Deutsche mit „Art“ übersetzt) und γένος ("genos", deutsch „Gattung“) voneinander abgrenzt werden. In seinen Kategorien charakterisiert er anhand eines Beispiels aus der Welt der Lebewesen diese als zweite Wesenheiten (δεύτεραι ουσίαι), die in dem Einzelnen vorhanden sind. So ist ein einzelner Mensch in der Art Mensch vorhanden und ein einzelnes Pferd in der Art Pferd, beide gehören jedoch zur Gattung des Lebenden (ζῷον "zoon").

In seiner Historia animalium (Περί τα ζώα ιστοριών) wendet Aristoteles die Begriffe είδος und γένος auch auf das Tierreich an, ohne dabei jedoch eine taxonomische Ordnung aufzustellen. Vielmehr spricht er von der Überlappung von Eigenschaften der Tierarten (ἐπάλλαξις "epállaxis") und der Notwendigkeit, eine einzelne Art anhand mehrerer nebengeordneter Merkmale zu definieren. Dennoch beschäftigt er sich bei der Beschreibung der Arten mit einzelnen charakteristischen Merkmalen. Der Begriff είδος wird auch nicht im Sinne eines heutigen Artbegriffes konsequent als unterste Kategorie zwischen dem einzelnen Lebewesen und γένος verwendet, vielmehr kann die Bedeutung meist am besten mit „Form“, „Gestalt“ oder „Wesen“ wiedergegeben werden, während Tierarten in der Regel mit γένος bezeichnet werden.

Laut biblischer Schöpfungs­geschichte im 1. Buch Mose schuf Gott zwischen dem 3. und 6. Schöpfungstag die Pflanzen und Tiere, „ein jegliches (jedes) nach seiner Art“ (zehnmal Zitat „nach seiner Art“, , zu verstehen als „Wesensart“, hebräisch "min" מין bzw. למינה, ). In der Septuaginta wird מין mit γένος (κατὰ γένος „nach/gemäß der Art“, ) übersetzt, in der Vulgata dagegen uneinheitlich, manchmal mit "genus" und manchmal mit "species", wobei auch die Präpositionen wechseln ("secundum speciem suam, secundum species suas, in species suas, juxta genus suum, secundum genus suum, in genere suo", ). Es wird hier auch eine Aussage zur Fortpflanzung der Pflanzen und Tiere „nach ihrer Art“ getroffen, indem Gott in spricht: „Es lasse die Erde aufgehen Gras und Kraut, das Samen bringe, und fruchtbare Bäume auf Erden, die ein jeder nach seiner Art Früchte tragen, in denen ihr Same ist“, sowie in zu den Tieren des Wassers und der Luft: „Seid fruchtbar und mehret euch.“

Diese biblischen Aussagen wie auch Aristoteles waren bis in die Neuzeit prägend für die Vorstellungen der Gelehrten des Abendlandes. Pierre Duhem führte 1916 für die philosophische Auffassung vom Wesen oder der „Essenz“ eines Individuums den Begriff des Essentialismus ein. Nach Auffassung von Ernst Mayr passten der Schöpfungsglaube und die letztendlich auf Platon zurückgehende Vorstellung von einer „unveränderlichen Essenz“ (είδος als Wesen) gut zusammen und bildeten die Grundlage für einen „essentialistischen Artbegriff“, wie er vom Mittelalter bis ins 19. Jahrhundert hinein dominierte. Hiernach gehören alle Objekte, welche dieselbe Essenz gemeinsam haben, derselben Art an. Laut Mayr war „[d]er Essentialismus mit seiner Betonung von Diskontinuität, Konstanz und typischen Werten (Typologie)“ der Hintergrund für typologische Artkonzepte, nach denen ein Individuum auf Grund seiner – in der Regel morphologischen – Merkmale (Typus) immer eindeutig einer bestimmten Art angehört.

Erkennbar ist dies auch bei John Ray, der 1686 in seiner "Historia plantarum generalis" die Arten der Pflanzen als Fortpflanzungsgemeinschaften mit beständigen Artkennzeichen definiert, nachdem er „lange Zeit“ nach Anzeichen für ihre Unterscheidung geforscht habe: „Uns erschien aber keines [kein Anzeichen] zuverlässiger als die gesonderte Fortpflanzung aus dem Samen. Welche Unterschiede auch immer also im Individuum oder der Pflanzenart aus dem Samen derselben hervorgehen, sie sind zufällig und nicht für die Art kennzeichnend. […] Denn die sich nach ihrer Art unterscheiden, bewahren ihre Art beständig, und keine entspringt dem Samen der anderen oder umgekehrt.“

Carl von Linné stellte mit seinen "Species Plantarum" 1753 und dem "Systema Naturae" 1758 als erster ein enkaptisches, auf hierarchisch aufbauenden Kategorien (Klasse, Ordnung, Gattung, Art und Varietät, jedoch noch nicht Familie) beruhendes System der Natur auf, wobei er für die Art die binäre Nomenklatur aus Gattungsnamen und Artepitheton einführte. Hierarchisch bedeutet dabei, dass die Einheiten auf unterschiedlichen Ebenen zu Gruppen zusammengefasst werden, wobei die in der Hierarchie höherstehenden Gruppen durch allgemeine, die tieferstehenden Gruppen durch immer speziellere Merkmale zusammengefasst werden (ein bestimmtes Individuum gehört also seiner Merkmalskombination gemäß in eine Art, eine Gattung, eine Familie usw.). Enkaptisch bedeutet, dass die in der Hierarchie tieferstehenden Gruppen in jeweils genau eine Gruppe der höheren Hierarchiestufe eingeschachtelt werden, also zum Beispiel jede Art in eine und genau eine, Gattung. In seiner "Philosophia botanica" formuliert er: „Es gibt so viele Arten, wie viele verschiedene Formen das unendliche Seiende am Anfang schuf; diese Formen, nach den hineingegebenen Gesetzen der Fortpflanzung, brachten viele [weitere Formen] hervor, doch immer ähnliche.“ Darüber hinaus bezeichnet er die Art und die Gattung als Werk der Natur, die Varietät als Werk des Menschen, Ordnung und Klasse dagegen als vom Menschen geschaffene Einheit. „Die Arten sind unveränderlich, denn ihre Fortpflanzung ist wahres Fortdauern.“

Während Georges-Louis Leclerc de Buffon 1749 noch verneint, dass es in der Natur irgendwelche Kategorien gäbe, revidiert er später diese Sicht für die Art und formuliert einen typologischen Artbegriff mit einer Konstanz der Arten: „Der Abdruck jeder Art ist ein Typ, dessen wesentliche Merkmale in unveränderlichen und beständigen Wesenszügen eingeprägt sind, doch alle Nebenmerkmale variieren: Kein Individuum gleicht vollkommen dem anderen.“

Jean-Baptiste de Lamarck, der bereits von einer Transformation der Arten ausgeht, betrachtet dagegen die Art und alle anderen Kategorien als künstlich. 1809 äußert er sich in seiner "Philosophie zoologique": „Die Natur hat nicht wirklich Klassen, Ordnungen, Familien, Gattungen, beständige Arten herausgebildet, sondern allein Individuen.“ Dies hindert ihn jedoch nicht daran, auf dem Gebiet der Taxonomie sehr produktiv zu sein, deren Kategorien er praktisch zu nutzen weiß.

Charles Darwin, der von der Art sogar im Titel seines Grundlagenwerkes "On the Origin of Species" (Über die Entstehung der Arten) von 1859 spricht, scheut sich vor einer Formulierung eines Artbegriffs. Laut Ernst Mayr kann man aus seinen Notizbüchern aus den 1830er Jahren schließen, dass er damals die Vorstellung von einer Art als Fortpflanzungsgemeinschaft hatte. In seiner "Entstehung der Arten" bezeichnet er jedoch die Begriffe der Art und der Varietät unmissverständlich als künstlich: „Aus diesen Bemerkungen geht hervor, dass ich den Kunstausdruck „Species“ als einen arbiträren und der Bequemlichkeit halber auf eine Reihe von einander sehr ähnlichen Individuen angewendeten betrachte und dass er von dem Kunstausdrucke „Varietät“, welcher auf minder abweichende und noch mehr schwankende Formen Anwendung findet, nicht wesentlich verschieden ist. Ebenso wird der Ausdruck „Varietät“ im Vergleich zu bloßen individuellen Verschiedenheiten nur arbiträr und der Bequemlichkeit wegen benutzt.“

Ähnlich äußert sich auch Alfred Russel Wallace 1856 in seiner Grundlagenarbeit über die Ritterfalter (Papilionidae) im Malaiischen Archipel, in der er verschiedene Verläufe der Evolution durch natürliche Zuchtwahl erklärt. Er bezeichnet Arten als „lediglich stark gekennzeichnete Rassen oder Lokalformen“ und geht dabei auch darauf ein, dass Individuen unterschiedlicher Arten generell als unfähig angesehen werden, fruchtbare gemeinsame Nachkommen zu zeugen, doch sei es nicht einmal in einem von tausend Fällen möglich, das Vorliegen einer Vermischung zu überprüfen.

Seit Darwin ist die Ebene der Art gegenüber unterscheidbaren untergeordneten (Lokalpopulationen) oder übergeordneten (Artengruppen bzw. höheren Taxa) nicht mehr besonders ausgezeichnet. Innerhalb der Taxonomie unterlag die Artabgrenzung Moden und persönlichen Vorlieben, es gibt Taxonomen, die möglichst jede unterscheidbare Form in den Artrang erheben wollen („splitter“), und andere, die weitgefasste Arten mit zahlreichen Lokalrassen und -populationen bevorzugen („lumper“).

Ende des 19. Jahrhunderts wurden biologische Artkonzepte einer Fortpflanzungsgemeinschaft diskutiert. Erwin Stresemann äußert in diesem Sinne bereits 1919 in einem Artikel über die europäischen Baumläufer klare Vorstellungen über Artbildung und genetische Isolation: „Es will nur die Tatsache im Namen zum Ausdruck bringen, dass sich die [im Laufe der geographischen Separation] zum Rang von Spezies erhobenen Formen physiologisch so weit voneinander entfernt haben, dass sie, wie die Natur beweist, wieder zusammenkommen können, ohne eine Vermischung einzugehen.“

Beherrschend im wissenschaftlichen Diskurs wurden die biologischen Artkonzepte der Fortpflanzungsgemeinschaft mit Theodosius Dobzhansky und Ernst Mayr seit der 2. Hälfte des 20. Jahrhunderts. Dobzhansky verknüpft den Artbegriff – ähnlich wie Stresemann – mit der Artbildung und definiert 1939 Arten als das „Stadium des Evolutionsvorgangs […], in dem Formengruppen, die sich bisher untereinander fortpflanzen oder jedenfalls dazu fähig waren, in zwei oder mehr gesonderte Gruppen aufgeteilt werden, die sich aus physiologischen Ursachen nicht untereinander fortpflanzen können“, während Mayr 1942 formuliert: „Arten sind Gruppen von natürlichen Populationen, die sich tatsächlich oder potentiell untereinander vermehren und fortpflanzungsmäßig von anderen derartigen Gruppen getrennt sind.“ In einem erweiterten biologischen Artbegriff bezieht Mayr 2002 die ökologische Nische mit in die Begriffsdefinition ein: „Eine Art ist eine Fortpflanzungsgemeinschaft von (fortpflanzungsmäßig von anderen isolierten) Populationen, die eine spezifische Nische in der Natur einnimmt.“ Mayr stellt die Bedeutung der Art in der Biologie als natürliche „Einheit der Evolution, der Systematik, der Ökologie und der Ethologie“ heraus und hebt sie hierin von allen anderen systematischen Kategorien ab.

Aus praktischen Erwägungen überdauern bis heute auch typologische Artkonzepte. Nach wie vor benennt die als Autorität bezeichnete Person, welche die Artbeschreibung einer neuen Art (species nova) als erste veröffentlicht, diese anhand der arttypischen Merkmale des Typusexemplars mit einem selbst gewählten Artnamen aus dem Gattungsnamen und dem Artepitheton.

Demgegenüber hebt der britische Paläoanthropologe Chris Stringer hervor: Alle Art-Konzepte sind „von Menschen erdachte Annäherungen an die Realität der Natur.“

Laut Ernst Mayr beginnt die Geschichte des Artbegriffs in der Biologie mit Carl von Linné. Er hebt in seinen Arbeiten zur Wissenschaftsgeschichte hervor, dass der Essentialismus das abendländische Denken in einem Ausmaß beherrscht habe, wie es bisher noch nicht in vollem Umfang gewürdigt werde, und setzt dabei auch typologische mit essentialistischen Artbegriffen gleich. Demgegenüber hebt Mary Winsor hervor, dass etwa die Verwendung von Typusarten als Prototypen für höhere Kategorien unvereinbar mit dem Essentialismus sei, und John S. Wilkins betont, dass die – von Winsor als „Methode der Exemplare“ bezeichnete – Typologie der Biologen und der Essentialismus keineswegs zwangsläufig verknüpft sind. Während Essenzen definierbar und allen Angehörigen einer Art eigen seien, ließen sich Typen instantiieren und seien variabel.

Anfang des 21. Jahrhunderts waren zwischen 1,5 und 1,75 Millionen Arten beschrieben, davon rund 500.000 Pflanzen. Es ist jedoch davon auszugehen, dass es sich bei diesen nur um einen Bruchteil aller existierenden Arten handelt. Schätzungen gehen davon aus, dass die Gesamtzahl aller Arten der Erde deutlich höher ist. Die weitestgehenden Annahmen reichten dabei Ende der 1990er-Jahre bis zu 117,7 Millionen Arten; am häufigsten jedoch wurden Schätzungen zwischen 13 und 20 Millionen Arten angeführt. Eine 2011 veröffentlichte Studie schätzte die Artenzahl auf 8,7 ± 1,3 Millionen, davon 2,2 ± 0,18 Millionen Meeresbewohner; diese Schätzung berücksichtigte allerdings nur Arten mit Zellkern (Eukaryoten), also weder Bakterien noch Viren.

Jay Lennon und Kenneth Locey von der Indiana University schätzten auf Basis der Ergebnisse von 3 Großprojekten, die Mikroben in Medizin, Meer und Boden behandeln, die Artenanzahl auf der Erde im März 2016 auf 1 Billion (10). Insbesondere die kleinen Lebensformen der Bakterien, Archeen und Pilze wurden bisher stark unterschätzt. Moderne Genom-Sequenzierung macht genaue Analysen möglich.

Über die Gesamtzahl aller Tier- und Pflanzenarten, die seit Beginn des Phanerozoikums vor 542 Mio. Jahren entstanden, liegen nur Schätzungen vor. Wissenschaftler gehen von etwa einer Milliarde Arten aus, manche rechnen sogar mit 1,6 Milliarden Arten. Weit unter einem Prozent dieser Artenvielfalt ist fossil erhalten geblieben, da die Bedingungen für eine Fossilwerdung generell ungünstig sind. Zudem zerstörten Erosion und Plattentektonik im Laufe der Jahrmillionen viele Fossilien. Forscher haben bis 1993 rund 130.000 fossile Arten wissenschaftlich beschrieben.

Es kann gezeigt werden, dass bei Verwendung des phylogenetischen Artkonzepts mehr Arten unterschieden werden als beim biologischen Artkonzept. Die Vermehrung der Artenzahl, z. B. innerhalb der Primaten, die ausschließlich auf das verwendete Artkonzept zurückgehen, ist als „taxonomische Inflation“ bezeichnet worden. Dies hat Folgen für angewandte Bereiche, wenn diese auf einem Vergleich von Artenlisten beruhen. Es ergeben sich unterschiedliche Verhältnisse beim Vergleich der Artenzahlen zwischen verschiedenen taxonomischen Gruppen, geographischen Gebieten, beim Anteil der endemischen Arten und bei der Definition der Schutzwürdigkeit von Populationen bzw. Gebieten im Naturschutz.

Typologisch definierte Arten sind Gruppen von Organismen, die in der Regel nach morphologischen Merkmalen (morphologisches Artkonzept) unterschieden werden. Es können aber auch andere Merkmale wie zum Beispiel Verhaltensweisen in analoger Weise verwendet werden. Eine nach morphologischen Kriterien definierte Art wird Morphospezies genannt.

Beispiele:

In der Paläontologie kann in der Regel nur das morphologische Artkonzept angewandt werden. Da die Anzahl der Funde oft begrenzt ist, ist die Artabgrenzung in der Paläontologie besonders subjektiv. Beispiel: Die Funde von Fossilien zweier Individuen in der gleichen Fundschicht, also praktisch gleichzeitig lebend, unterscheiden sich stark voneinander:

Diese Probleme werden mit zunehmender Zahl der Funde und damit Kenntnis der tatsächlichen Variationsbreite geringer, lassen sich aber nicht vollständig beseitigen.

Das morphologische Artkonzept findet häufig Verwendung in der Ökologie, Botanik und Zoologie. In anderen Bereichen, wie etwa in der Mikrobiologie oder in Teilbereichen der Zoologie, wie bei den Nematoden, versagen rein morphologische Arteinteilungsversuche weitgehend.


Bakterien zeigen nur wenige morphologische Unterscheidungsmerkmale und weisen praktisch keine Rekombinationsschranken auf. Deshalb wird der Stoffwechsel als Unterscheidungskriterium von Stämmen herangezogen. Weil ein allgemein akzeptiertes Artkriterium fehlt, stellen Bakterienstämme so die derzeit tatsächlich verwendete Basis zur Unterscheidung dar. Anhand biochemischer Merkmale wie etwa der Substanz der Zellwand unterscheidet man die höheren Bakterientaxa.

Man testet an bakteriellen Reinkulturen zu ihrer „Artbestimmung“ deren Fähigkeit zu bestimmten biochemischen Leistungen, etwa der Fähigkeit zum Abbau bestimmter „Substrate“, z. B. seltener Zuckerarten. Diese Fähigkeit ist sehr einfach erkennbar, wenn das Umsetzungsprodukt einen im Kulturmedium zugesetzten Farbindikator umfärben kann. Durch Verimpfung einer Bakterienreinkultur in eine Reihe von Kulturgläsern mit Nährlösungen, die jeweils nur ein bestimmtes Substrat enthalten („Selektivmedien“), bekommt man eine sog. „Bunte Reihe“, aus deren Farbumschlägen nach einer Tabelle die Bakterienart bestimmt werden kann. Dazu wurden halbautomatische Geräte („Mikroplatten-Reader“) entwickelt.

Seit entsprechende Techniken zur Verfügung stehen (PCR), werden Bakterienstämme auch anhand der DNA-Sequenzen identifiziert oder unterschieden. Ein weithin akzeptiertes Maß ist, dass Stämme, die weniger als 70 % ihres Genoms gemeinsam haben, als getrennte Arten aufzufassen sind. Ein weiteres Maß beruht auf der Ähnlichkeit der 16S-rRNA-Gene. Nach DNA-Analysen waren dabei zum Beispiel weniger als 1 % der in natürlichen Medien gefundenen Stämme auf den konventionellen Nährmedien vermehrbar. Auf diese Weise sollen in einem ml Boden bis zu 100.000 verschiedene Bakteriengenome festgestellt worden sein, die als verschiedene Arten interpretiert wurden. Dies ist nicht zu verwechseln mit der Gesamtkeimzahl, die in der gleichen Größenordnung liegt, aber dabei nur „wenige“ Arten umfasst, die sich bei einer bestimmten Kulturmethode durch die Bildung von Kolonien zeigen.

Viele Unterscheidungskriterien sind rein pragmatisch. Auf welcher Ebene der Unterscheidung man hier Stämme als Arten oder gar Gattungen auffasst, ist eine Sache der Konvention. Die physiologische oder genetische Artabgrenzung bei Bakterien entspricht methodisch dem typologischen Artkonzept. Ernst Mayr, leidenschaftlicher Anhänger des biologischen Artkonzepts, meint daher: „Bakterien haben keine Arten“.

Daniel Dykhuizen macht darauf aufmerksam, dass – entgegen mancher Anschauung – Transformationen, Transduktionen und Konjugationen (als Wege des DNA-Tauschs zwischen Stämmen) nicht wahllos, sondern zwischen bestimmten Formen bevorzugt, zwischen anderen quasi nie ablaufen. Demnach wäre es prinzipiell möglich, ein Artkonzept entsprechend dem biologischen Artkonzept bei den Eukaryonten zu entwickeln. Frederick M. Cohan versucht dagegen auf Basis von Ökotypen, ein Artkonzept zu entwickeln.

Gegen Ende des 19., Anfang des 20. Jahrhunderts begann sich in der Biologie allmählich das Populationsdenken durchzusetzen, was Konsequenzen für den Artbegriff mit sich brachte. Weil typologische Klassifizierungsschemata die realen Verhältnisse in der Natur nicht oder nur unzureichend abzubilden vermochten, musste die biologische Systematik einen neuen Artbegriff entwickeln, der nicht auf abstrakter Unterschiedlichkeit oder subjektiver Einschätzung einzelner Wissenschaftler basiert, sondern auf objektiv feststellbaren Kriterien. Diese Definition wird als "biologische Artdefinition" bezeichnet, "„Sie heißt „biologisch“ nicht deshalb, weil sie mit biologischen Taxa zu tun hat, sondern weil ihre Definition eine biologische ist. Sie verwendet Kriterien, die, was die unbelebte Welt betrifft, bedeutungslos sind.“" Eine biologisch definierte Art wird als "Biospezies" bezeichnet.

Der neue Begriff stützte sich auf zwei Beobachtungen: Zum einen setzen sich Arten aus Populationen zusammen und zum anderen existieren zwischen Populationen unterschiedlicher Arten biologische Fortpflanzungsbarrieren. "„Die "[biologische]" Art besitzt zwei Eigenschaften, durch die sie sich grundlegend von allen anderen taxonomischen Kategorien, etwa dem Genus, unterscheidet. Erstens einmal erlaubt sie eine nichtwillkürliche Definition – man könnte sogar so weit gehen, sie als „selbstoperational“ zu bezeichnen –, indem sie das Kriterium der Fortpflanzungsisolation gegenüber anderen Populationen hervorhebt. Zweitens ist die Art nicht wie alle anderen Kategorien auf der Basis von ihr innewohnenden Eigenschaften, nicht aufgrund des Besitzes bestimmter sichtbarer Attribute definiert, sondern durch ihre Relation zu anderen Arten.“" Das hat – zumindest nach der Mehrzahl der Interpretationen – zur Folge, dass Arten nicht Klassen sind, sondern Individuen.

Das Kriterium der Fortpflanzungsfähigkeit bildet den Kern des biologischen Artbegriffs oder der Biospezies. Eine Biospezies ist eine Gruppe sich tatsächlich oder potentiell miteinander fortpflanzender Individuen, die voll fertile Nachkommen hervorbringen:

Dabei sollen die Isolationsmechanismen zwischen den einzelnen Arten biologischer Natur sein, also nicht auf äußeren Gegebenheiten, räumlicher oder zeitlicher Trennung basieren, sondern Eigenschaften der Lebewesen selbst sein:
Die Kohäsion der Biospezies, ihr genetischer Zusammenhalt, wird durch physiologische, ethologische, morphologische und genetische Eigenschaften gewährleistet, die gegenüber artfremden Individuen isolierend wirken. Da die Isolationsmechanismen verhindern, dass nennenswerte zwischenartliche Bastardisierung stattfindet, bilden die Angehörigen einer Art eine Fortpflanzungsgemeinschaft; zwischen ihnen besteht Genfluss, sie teilen sich einen Genpool und bilden so eine Einheit, in der evolutionärer Wandel stattfindet.

Beispiele:



Das biologische Artkonzept findet häufig Verwendung in der Ökologie, Botanik und Zoologie, besonders in der Evolutionsbiologie. In gewisser Weise bildet es das Standardmodell, aus dem die anderen modernen Artkonzepte abgeleitet sind oder gegen welches sie sich in erster Linie abgrenzen. Die notwendigen Charaktere (Fehlen natürlicher Hybriden bzw. gemeinsamer Genpool) sind bisweilen umständlich zu überprüfen, in bestimmten Bereichen, wie etwa in der Paläontologie, versagen biologische bzw. populationsgenetische Artabgrenzungen weitgehend.

Nach diesem Konzept wird eine Art als (monophyletische) Abstammungsgemeinschaft aus einer bis vielen Populationen definiert.
Eine Art beginnt nach einer Artspaltung (siehe Artbildung, Kladogenese) und endet


Phylogenetische Anagenese ist die Veränderung einer Art im Zeitraum zwischen zwei Artspaltungen, also während ihrer Existenz. Solange keine Aufspaltung erfolgt, gehören alle Individuen zur selben Art, auch wenn sie unter Umständen morphologisch unterscheidbar sind.

Das phylogenetische Artkonzept beruht auf der phylogenetischen Systematik oder „Kladistik“ und besitzt nur im Zusammenhang mit dieser Sinn. Im Rahmen des Konzepts sind Arten objektive, tatsächlich existierende biologische Einheiten. Alle höheren Einheiten der Systematik werden nach dem System „Kladen“ genannt und sind (als monophyletische Organismengemeinschaften) von Arten prinzipiell verschieden. Durch die gabelteilige (dichotome) Aufspaltung besitzen alle hierarchischen Einheiten oberhalb der Art (Gattung, Familie etc.) keine Bedeutung, sondern sind nur konventionelle Hilfsmittel, um Abstammungsgemeinschaften eines bestimmten Niveaus zu bezeichnen. Der wesentliche Unterschied liegt weniger in der Betrachtung der Art als in derjenigen dieser höheren Einheiten. Nach dem phylogenetischen Artkonzept können sich Kladen überlappen, wenn sie hybridogenen Ursprungs sind.

Zusätzlich kommen folgende Schwierigkeiten hinzu:


Ein weiterer Versuch, Arten in der Zeit klar abzugrenzen, ist das chronologische Artkonzept (Chronospezies). Auch hier wird die Art zunächst anhand eines anderen Artkonzepts definiert (meist das morphologische Artkonzept). Dann werden nach den Kriterien dieses Konzepts auch die Artgrenzen zwischen in einer Region aufeinanderfolgenden Populationen definiert. Dieses Konzept findet vorwiegend in der Paläontologie Anwendung und ist daher in der Regel eine Erweiterung des morphologischen Artkonzeptes um den Faktor Zeit:

Dieses Konzept ist dann gut anwendbar, wenn praktisch lückenlose Fundfolgen vorliegen.

In der Paläontologie, speziell in der Paläoanthropologie erweist sich die Zuordnung zu Arten und sogar die Zuordnung zu Gattungen allein anhand fossiler Knochen als schwierig. Anstelle einer kontravalenten Zuordnung wird daher von John Francis Thackeray eine wahrscheinlichkeitstheoretische Zuordnung vorgeschlagen. Anstelle der Frage, ob ein Fossil zur Spezies A und ein anderes zur Spezies B gehört, wird die Wahrscheinlichkeit, dass beide zur selben Spezies gehören, errechnet. Dazu wird eine möglichst große Reihe von Paaren unterschiedlicher morphometrischer Messpunkte von je zwei Individuen verglichen, bei denen die Artzugehörigkeit unsicher ist. Die Messwertpaare weichen stets voneinander ab. Sie streuen in Form einer Gaußschen Normalverteilung. Innerhalb dieser Verteilung wird definiert, in welchem Intervall um den Mittelwert (z. B. 2 Sigma) beide Individuen als derselben Art zugehörig betrachtet werden. Liegen die Messpunkte außerhalb des vorgegebenen Intervalls, werden die beiden Individuen als zwei verschiedene Arten betrachtet.

Für detaillierte und aktuelle Diskussionen spezieller Themen:



</doc>
<doc id="339" url="https://de.wikipedia.org/wiki?curid=339" title="Anaerobie">
Anaerobie

Anaerobie (zu "aer" ‚Luft‘ und ‚Leben‘; mit Alpha privativum α(ν)- "a(n)-" ‚ohne‘) bezeichnet Leben ohne Sauerstoff (Disauerstoff O). Lebewesen, die für ihren Stoffwechsel keinen molekularen Sauerstoff brauchen, werden als anaerob bezeichnet. Jene Anaerobier, die durch O gar gehemmt oder sogar abgetötet werden, werden genauer "obligat anaerob" benannt.

Anaerobe einzellige Organismen sind die ältesten Formen des Lebens auf der Erde, noch vor den ersten oxygen photosynthetisch aktiven Einzellern im Präkambrium, die O ausschieden. Mit dessen Anreicherung in der Hydrosphäre und der Atmosphäre änderten sich die Lebensbedingungen großräumig (siehe Große Sauerstoffkatastrophe). Die heute lebenden anaeroben Organismen benötigen ebenfalls alle keinen Sauerstoff für ihren Stoffwechsel und lassen sich grob danach unterschieden, wie gut sie mit einer sauerstoffhaltigen Umgebung zurechtkommen:

Lebensräume, in denen kein Sauerstoff enthalten ist, werden als "anoxisch" bezeichnet, mit früherem Sprachgebrauch auch als "anaerob" (Gegensatz: "oxisch", Sauerstoff enthaltend, früher "aerob").

Im Unterschied zur aeroben Atmung werden bei der anaeroben für den oxidativen Energiestoffwechsel anstelle von O andere Elektronenakzeptoren als Oxidationsmittel verwendet. Häufig verwendete alternative Elektronenakzeptoren sind: Nitrat, dreiwertige Eisen-Ionen (Fe), vierwertige Mangan-Ionen (Mn), Sulfat, Schwefel, Fumarat und Kohlenstoffdioxid (CO). Diese Redox-Reaktionen werden als anaerobe Atmung bezeichnet.

In der Tabelle sind Typen der anaeroben Atmung aufgeführt, die in der Umwelt weit verbreitet sind (zum Vergleich ist die aerobe Atmung mit dabei). Die Reihung der Atmungsprozesse erfolgte nach Möglichkeit nach dem Standard-Redoxpotential des Elektronenakzeptorpaars in Volt bei einem pH-Wert von 7. Die tatsächlichen pH-Werte können abweichen (z. B. bei Acetogenese).

Nicht als anaerobe Atmung, sondern als Gärung werden Vorgänge bezeichnet, bei denen kein externer Stoff als terminaler Elektronenakzeptor verwendet wird. Gärungsorganismen sind vor allem:


Manche Turbellarien, Ringelwürmer und Enteroparasiten wie Bandwürmer beherbergen anaerobe Bakterien und können durch diese Symbiose auch unter anoxischen Bedingungen leben.

Das Verhalten von Mikroorganismen gegenüber Sauerstoff, ihre Identifikation als Aerobier, Anaerobier, Aerotoleranter oder fakultativer Anaerobier, kann durch Kultur in einem Sauerstoffkonzentrationsgradienten ermittelt werden. Dabei kultiviert man sie in einem Gelnährmedium, das sich in einem einseitig geschlossenen Glasrohr (Reagenzglas, Kulturröhrchen) befindet und in das Sauerstoff nur vom oberen, offenen Ende durch Diffusion eindringen kann. Auf diese Weise bildet sich ein Sauerstoffkonzentrationsgradient aus mit hoher Sauerstoffkonzentration oben und niedriger Sauerstoffkonzentration unten. Die Mikroorganismen werden in sehr geringer Menge gleichmäßig im Gelnährmedium verteilt, in dem sie ortsgebunden sind und sich nicht fortbewegen können. Dort, wo sich die Mikroorganismen hinsichtlich der Sauerstoffkonzentration unter geeigneten Bedingungen befinden, vermehren sie sich und man kann nach einer gewissen Zeit einen Bewuchs mit bloßem Auge erkennen. Die Zone, in der sich Bewuchs zeigt, ist ein Indikator für das Verhalten der Mikroorganismen gegenüber Sauerstoff, wie aus dem Bild deutlich wird.

Anaerobie ist unter anderem bei der Kultivierung von Mikroorganismen von Bedeutung. Sollen gegenüber O empfindliche Mikroorganismen kultiviert werden oder sollen fakultativ anaerobe Mikroorganismen unter anoxischen Bedingungen kultiviert werden, so ist es erforderlich, bei der Kultur O auszuschließen. Hierbei werden sogenannte Anaerobentechniken verwendet. Ein Beispiel ist die Kultur in einer Anaerobenkammer: Darin erreicht man mit einer Gasatmosphäre aus 10 Vol.-% H + 10 Vol.-% CO + 80 Vol.-% N anoxische Bedingungen, die es ermöglichen, anaerobe Mikroorganismen zu kultivieren.




</doc>
<doc id="340" url="https://de.wikipedia.org/wiki?curid=340" title="Awk">
Awk

awk ist eine Programmiersprache (Skriptsprache) zur Bearbeitung und Auswertung strukturierter Textdaten, beispielsweise CSV-Dateien. Der zugehörige Interpreter war eines der ersten Werkzeuge, das in der Version 3 von Unix erschien; es wird auch heute noch vielfach zusammen mit sed in Shell-Skripten eingesetzt, um Daten zu bearbeiten, umzuformen oder auszuwerten. Der Name "awk" ist aus den Anfangsbuchstaben der Nachnamen ihrer drei Autoren Alfred V. Aho, Peter J. Weinberger und Brian W. Kernighan zusammengesetzt.

Eine Version von awk ist heute in fast jedem Unix-System, das historisch auf UNIX zurückzuführen ist, sowie in jeder Linux-Distribution zu finden. Ein vergleichbares Programm ist aber auch für fast alle anderen Betriebssysteme verfügbar.

Die Sprache arbeitet fast ausschließlich mit dem Datentyp Zeichenkette (). Daneben sind assoziative Arrays (d. h. mit Zeichenketten indizierte Arrays, auch Hashs genannt) und reguläre Ausdrücke grundlegende Bestandteile der Sprache.

Die Leistungsfähigkeit, Kompaktheit, aber auch die Beschränkungen der awk- und sed-Skripte regten Larry Wall zur Entwicklung der Sprache Perl an.

Die typische Ausführung eines awk-Programms besteht darin, Operationen – etwa Ersetzungen – auf einem Eingabetext durchzuführen. Dafür wird der Text zeilenweise eingelesen und anhand eines gewählten Trenners – üblicherweise eine Serie von Leerzeichen und/oder Tabulatorzeichen – in Felder aufgespalten. Anschließend werden die awk-Anweisungen auf die jeweilige Zeile angewandt. 

awk-Anweisungen haben folgende Struktur:

Für die eingelesene Zeile wird ermittelt, ob sie die Bedingung (oft ein Regulärer Ausdruck) erfüllt. Ist die Bedingung erfüllt, wird der Code innerhalb des von geschweiften Klammern umschlossenen Anweisungsblocks ausgeführt.
Abweichend davon
kann ein Statement auch nur aus einer Aktion 

oder nur aus einer Bedingung 

bestehen. Fehlt die Bedingung, so wird die Aktion für jede Zeile ausgeführt. Fehlt die Aktion, so wird als Standardaktion das Schreiben der ganzen Zeile ausgeführt, sofern die Bedingung erfüllt ist.

Der Benutzer kann Variablen innerhalb von Anweisungsblöcken durch Referenzierung definieren, eine explizite Deklaration ist nicht notwendig. Der Gültigkeitsbereich der Variablen ist global. Eine Ausnahme bilden hier Funktionsargumente, deren Gültigkeit auf die sie definierende Funktion beschränkt ist. 

Funktionen können an beliebiger Stelle definiert werden, die Deklaration muss dabei nicht vor der ersten Nutzung erfolgen. Falls es sich um Skalare handelt, werden Funktionsargumente als Wertparameter übergeben, ansonsten als Referenzparameter. Die Argumente bei Aufruf einer Funktion müssen nicht der Funktionsdefinition entsprechen, überzählige Argumente werden als lokale Variablen behandelt, ausgelassene Argumente mit dem speziellen Wert "uninitialized" – numerisch Null und als Zeichenkette den Wert des leeren Strings – versehen.

Funktionen und Variablen aller Art bedienen sich des gleichen Namensraums, so dass gleiche Benennung zu undefiniertem Verhalten führt. 

Neben benutzerdefinierten Variablen und Funktionen stehen auch Standardvariablen und Standardfunktionen zur Verfügung, beispielsweise die Variablen codice_1 für die gesamte Zeile, codice_2, codice_3, … für das jeweils i-te Feld der Zeile und codice_4 (von engl. ) für den Feldtrenner, sowie die Funktionen gsub(), split() und match().

Die Syntax von awk ähnelt derjenigen der Programmiersprache C. Elementare Befehle sind Zuweisungen an Variablen, Vergleiche zwischen Variablen sowie Schleifen oder bedingte Befehlsausführungen (if-else). Daneben gibt es Aufrufe sowohl zu fest implementierten als auch zu selbst programmierten Funktionen.

Ausgeben von Daten auf der Standardausgabe ist durch den „codice_5“-Befehl möglich. Um etwa das zweite Feld einer Eingabezeile auszudrucken, wird der Befehl

Bedingungen sind in awk-Programmen entweder von der Form 

oder von der Form

Reguläre Suchmuster werden wie beim grep-Befehl gebildet, und Matchoperatoren sind ~ für "Muster gefunden" und !~ für "Muster nicht gefunden". Als Abkürzung für die Bedingung „$0 ~ /"reguläres Suchmuster"/“ (also die ganze Zeile erfüllt das Suchmuster) kann „/"reguläres Suchmuster"/“ verwendet werden.

Als spezielle Bedingungen gelten die Worte "BEGIN" und "END", bei denen die zugehörigen Anweisungsblöcke vor dem Einlesen der ersten Zeile bzw. nach Einlesen der letzten Zeile ausgeführt werden. 

Darüber hinaus können Bedingungen mit logischen Verknüpfungen zu neuen Bedingungen zusammengesetzt werden, z. B.

Dieser AWK-Befehl bewirkt, dass von jeder Zeile, die mit "E" beginnt und deren zweites Feld eine Zahl größer 20 ist, das dritte Feld ausgegeben wird.

Einige einfache Programmbeispiele, die man z. B. unter Linux einfach in einer Shell eingeben kann: 

echo Hallo Welt | awk '{print $1}'
echo Hallo Welt | awk '{print $2}'

erzeugt die Ausgaben „codice_6“ bzw. „codice_7“
echo Hallo Welt | awk '{printf "%s, %s!\n",$1,$2}'

erzeugt die Ausgabe „codice_8“ sowie einen Zeilenumbruch.
awk '$9 ~ /[45]../' /var/log/apache2/access?log

gibt alle Zeilen einer Webserver-Logdatei aus, deren neunte Spalte drei Zeichen enthält, deren erstes eine 4 oder 5 ist. In der neunten Spalte steht üblicherweise der HTTP-Statuscode. Die ausgegebenen Zeilen gehören zu Anfragen, die fehlgeschlagen sind.

Die erste awk-Version aus dem Jahr 1977 erfuhr 1985 eine Überarbeitung durch die ursprünglichen Autoren, die als "nawk" (new awk) bezeichnet wurde. Sie bietet die Möglichkeit, eigene Funktionen zu definieren, sowie eine größere Menge von Operatoren und vordefinierten Funktionen. Der Aufruf erfolgt zumeist dennoch über „awk“, seit eine Unterscheidung zwischen beiden Versionen obsolet geworden ist.

Das GNU-Projekt der Free Software Foundation stellt unter dem Namen "gawk" eine nochmals erweiterte freie Variante zur Verfügung.

Eine weitere freie Implementierung ist "mawk" von Mike Brennan. mawk ist kleiner und schneller als gawk, was allerdings durch einige Einschränkungen erkauft wird.

Auch "BusyBox" enthält eine kleine awk-Implementation, womit diese Sprache auch für Embedded Linux und Android zur Verfügung steht.




</doc>
<doc id="341" url="https://de.wikipedia.org/wiki?curid=341" title="Achim von Arnim">
Achim von Arnim

Achim von Arnim (eigentlich "Carl Joachim Friedrich Ludwig von Arnim"; * 26. Januar 1781 in Berlin; † 21. Januar 1831 in Wiepersdorf, Kreis Jüterbog-Luckenwalde) war ein deutscher Schriftsteller. Neben Clemens Brentano und Joseph von Eichendorff gilt er als ein wichtiger Vertreter der Heidelberger Romantik.

Arnims Vater war der wohlhabende Königlich Preußische Kammerherr Joachim Erdmann von Arnim, der aus dem uckermärkischen Familienzweig Blankensee stammte und Gesandter des preußischen Königs in Kopenhagen und Dresden und später Intendant der Berliner Königlichen Oper war. Arnims Mutter Amalie Caroline von Arnim, geb. Labes starb drei Wochen nach seiner Geburt.

Arnim verbrachte Kindheit und Jugend zusammen mit seinem älteren Bruder Carl Otto bei seiner Großmutter Caroline von Labes in Zernikow und Berlin, wo er von 1793 bis 1798 das Joachimsthalsche Gymnasium besuchte. Er studierte von 1798 bis 1800 Rechts- und Naturwissenschaften und Mathematik in Halle (Saale). Noch als Student schrieb er zahlreiche naturwissenschaftliche Texte unter anderem den "Versuch einer Theorie der elektrischen Erscheinungen" sowie Aufsätze in den "Annalen der Physik".
Im Haus des Komponisten Johann Friedrich Reichardt lernte er Ludwig Tieck kennen, dessen literarische Arbeiten er bewunderte. 1800 wechselte Arnim zum naturwissenschaftlichen Studium nach Göttingen, wo er Johann Wolfgang von Goethe und Clemens Brentano begegnete. Unter deren Einfluss wendete er sich von den naturwissenschaftlichen Schriften eigenen literarischen Arbeiten zu. Nach Beendigung des Studiums im Sommer 1801 schrieb er, beeinflusst von Goethes "Werther", seinen Erstlingsroman "Hollin’s Liebeleben".

Arnim unternahm von 1801 bis 1804 eine Bildungsreise quer durch Europa zusammen mit seinem Bruder Carl Otto. 1802 begegnete er in Frankfurt erstmals seiner späteren Frau Bettina und bereiste zusammen mit Brentano den Rhein. Ende 1802 besuchte er in Coppet Frau von Staël und 1803 traf er in Paris erstmals mit Friedrich und Dorothea Schlegel zusammen. In diesem Jahr reiste Arnim weiter nach London und blieb bis Sommer 1804 in England und Schottland.

Nach seiner Rückkehr entwarfen Arnim und Brentano erste konkrete Pläne zur Herausgabe einer Volksliedersammlung, die schließlich 1805 unter dem Titel "Des Knaben Wunderhorn" erschien. Arnim ging mit Goethe in Weimar die gesammelten und teils von Arnim und Brentano stark bearbeiteten Lieder der Sammlung durch. 1805 traf er in Frankfurt den Rechtsgelehrten Friedrich Karl von Savigny (1779–1861), der ihn schätzen lernte und mit dem ihn eine lebenslange Freundschaft verband. Seit dem 11. November 1808 schrieben sie sich regelmäßig.

Die Veröffentlichung weiterer Bände verzögerte sich durch den deutsch-französischen Krieg. Nach der Niederlage Preußens bei Jena und Auerstedt folgte Arnim dem geflohenen Königshof nach Königsberg. Dort machte er im Kreis um den Reformer Freiherrn vom Stein politische Vorschläge. 1807 reiste Arnim zusammen mit Reichardt zu Goethe nach Weimar, wo auch Clemens und Bettina Brentano waren. Gemeinsam fuhren sie nach Kassel, wo Arnim erstmals die Brüder Grimm traf, mit denen er sein Leben lang befreundet blieb.

Arnim zog 1808 nach Heidelberg, Clemens Brentano folgte ihm und dort vollendeten sie ihre Arbeit an der Volksliedersammlung. Der zweite und dritte Band des "Wunderhorns" erschien und außerdem schrieb Arnim Aufsätze für die "Heidelbergischen Jahrbücher". In dem Kreis von Romantikern um Joseph Görres, der der Heidelberger Romantik ihren Namen verdankt, gab Arnim die "Zeitung für Einsiedler" heraus, an der neben Brentano, Görres und den Brüdern Grimm auch Tieck, Friedrich Schlegel, Jean Paul, Justinus Kerner und Ludwig Uhland mitarbeiteten. Dieser Kreis wandte sich überwiegend aus politischen Gründen dem Mittelalter zu, um über diese Epoche eine nationale Einheit zu stiften, der ästhetische Aspekt interessierte dabei weniger. Arnim verließ Heidelberg Ende 1808 und besuchte Goethe auf dem Heimweg nach Berlin. Seit 1809 lebte Arnim in Berlin, wo er sich erfolglos um ein Amt im preußischen Staatsdienst bewarb.

In Berlin veröffentlichte Arnim seine Novellensammlung "Der Wintergarten", arbeitete für Kleists "Berliner Abendblätter" und gründete 1811 die "Deutsche Tischgesellschaft", später "Christlich-Deutsche Tischgesellschaft" genannte patriotische Vereinigung, zu der zahlreiche Politiker, Professoren, Militärs und Künstler der Berliner Gesellschaft gehörten und in der nur christlich getaufte Männer Zutritt hatten.
1810 verlobte sich Arnim mit Bettina, das Paar heiratete am 11. März 1811. Die Arnims hatten sieben Kinder: Freimund, Siegmund, Friedmund, Kühnemund, Maximiliane, Armgart, und Gisela von Arnim. Das Paar lebte meist getrennt, sie in Berlin, er auf seinem Gut Wiepersdorf. Bald nach der Hochzeit reisten sie gemeinsam nach Weimar, um Goethe zu besuchen. Ein heftiger Streit Bettinas mit Goethes Frau Christiane führte zu einer lebenslangen Entfremdung zwischen Goethe und Arnim. 1813 während der Befreiungskriege gegen Napoleon befehligte Arnim als Hauptmann ein Berliner Landsturmbataillon. Von Oktober 1813 bis Februar 1814 war er Herausgeber der Berliner Tageszeitung "Der Preußische Correspondent", gab diese Stellung aber wegen Streitigkeiten mit dem Erstherausgeber Barthold Georg Niebuhr auf. Ebenfalls 1813 trat er der Gesetzlosen Gesellschaft zu Berlin bei.

Von 1814 bis zu seinem Tode 1831 (Gehirnschlag) lebte Arnim überwiegend – unterbrochen von gelegentlichen Reisen und längeren Berlinaufenthalten – auf seinem Gut in Wiepersdorf und nahm mit zahlreichen Artikeln und Erzählungen in Zeitungen, Zeitschriften und Almanachen sowie mit Buchveröffentlichungen am literarischen Leben Berlins teil. Seine Frau und die Kinder lebten vor allem in Berlin. 1817 erschien der erste Band seines Romans "Die Kronenwächter". Arnim schrieb vor allem für den "Gesellschafter" und hatte zeitweilig eine eigene Rubrik in der "Vossischen Zeitung".

1820 besuchte Arnim Ludwig Uhland, Justinus Kerner, die Brüder Grimm und zum letzten Mal Goethe in Weimar. In seinen letzten Lebensjahren hatte Arnim immer wieder mit finanziellen Problemen zu kämpfen. Großer literarischer Erfolg blieb aus.

Achim von Arnim starb am 21. Januar 1831 in Wiepersdorf.

Arnim hinterließ eine Fülle von Dramen, Novellen, Erzählungen, Romanen, Gedichten und anderen Arbeiten. Er wird heute zu den bedeutendsten Vertretern der deutschen Romantik gezählt.

Vor allem über das "Wunderhorn" wirkte er auf Spätromantiker und Realisten ein wie etwa Eduard Mörike, Heinrich Heine, Ludwig Uhland und Theodor Storm. Die Sammlung enthält etwa 600 Bearbeitungen deutscher Volkslieder und gehört zu den wichtigsten Zeugnissen einer von der Romantik propagierten "Volksdichtung". Enthalten sind Liebes-, Kinder-, Kriegs- und Wanderlieder vom Mittelalter bis ins 18. Jahrhundert. Goethe empfahl "Des Knaben Wunderhorn" zur Lektüre über alle Standesgrenzen hinweg, da es ihm für die einfachste Küche ebenso wie für das Klavier der Gelehrten geeignet erschien.

Arnims Novellen zeugen von der Hinwendung des Autors zum Übernatürlichen. Die Erzählung "Isabella von Ägypten" vermischt Fiktion und Realität und nimmt so Elemente des Surrealismus vorweg; die traumhafte Phantastik wird mit historischen Bezügen verbunden. Poetologisch stellte Arnim seine Literatur in den Dienst der politischen Erneuerung, die er nicht durch politische Arbeit, sondern in der Kunst verwirklichen wollte. Deshalb hat er öfter volkstümliche Stoffe wiederbelebt. Arnims unvollendet gebliebener Roman "Die Kronenwächter" trieb die Erneuerung des historischen Romans in Deutschland voran. Er zeigt Missstände von Arnims Gegenwart in Form einer geschichtlichen Erzählung.

Als Lyriker wird Arnim weniger als die Zeitgenossen Brentano und Eichendorff wahrgenommen, obwohl er ein reiches und vielgestaltiges lyrisches Werk hinterlassen hat; auch fast alle seine erzählerischen Werke enthalten Gedichte und Lieder.

Die zeitgenössischen Urteile über Arnim gingen weit auseinander: Heine schrieb, Arnim sei „ein großer Dichter und einer der originellsten Köpfe der romantischen Schule. Die Freunde des Phantastischen würden an diesem Dichter mehr als an jedem anderen deutschen Schriftsteller Geschmack finden.“ Goethe dagegen sah Arnims Werk als ein Fass, an dem der Küfer vergessen habe, die Reifen festzuschlagen.

Ein Gegenstand anhaltender Diskussion in der Arnim-Forschung ist der Zusammenhang von nationalem Engagement und antisemitischer Denunziation. Mehrfach, etwa in der Erzählung "Die Versöhnung in der Sommerfrische" von 1811, benutzt Arnim die traditionelle Gegenüberstellung von Christen und Juden, um ein konstruiertes „deutsches Wesen“ in der Opposition zu einem vermeintlichen „jüdischen Wesen“ zu profilieren.

1995 gründeten die Herausgeber der historisch-kritischen "Weimarer Arnim-Ausgabe (WAA)" die Internationale Arnim-Gesellschaft mit Sitz in Erlangen.






In Göttingen wurde 1909 eine Göttinger Gedenktafel an seinem Göttinger Wohnhaus in der Prinzenstraße 10/12 angebracht.




</doc>
<doc id="342" url="https://de.wikipedia.org/wiki?curid=342" title="Algorithmus">
Algorithmus

Ein Algorithmus ist eine eindeutige Handlungsvorschrift zur Lösung eines Problems oder einer Klasse von Problemen. Algorithmen bestehen aus endlich vielen, wohldefinierten Einzelschritten. Damit können sie zur Ausführung in einem Computerprogramm implementiert, aber auch in menschlicher Sprache formuliert werden. Bei der Problemlösung wird eine bestimmte Eingabe in eine bestimmte Ausgabe überführt.

Der Übergang zwischen Algorithmus und Heuristik ist fließend.

Werner Stangl greift ihn folgend: "Ein Algorithmus bezeichnet eine systematische, logische Regel oder Vorgehensweise, die zur Lösung eines vorliegenden Problems führt. Im Gegensatz dazu steht dabei die schnellere, aber auch fehleranfälligere Heuristik."
Ein Heurismus versucht, basierend auf evtl. unvollständigen Eingangswerten, ein wahrscheinliches Ergebnis zu erzeugen.

Rechenvorschriften sind eine Untergruppe der Algorithmen. Sie beschreiben Handlungsanweisungen in der Mathematik bezüglich Zahlen.

Andere Algorithmen-Untergruppen sind z. B. (Koch-)Rezepte, Gesetze, Verordnungen, Regeln, Verträge.

Schon mit der Entwicklung der Sprache ersannen die Menschen für ihr Zusammenleben in größeren Gruppen Verhaltensregeln, Gebote, Gesetze – einfachste Algorithmen. Mit der Sprache ist auch eine geeignete Möglichkeit gegeben, Verfahren und Fertigkeiten weiterzugeben – komplexere Algorithmen. Aus der Spezialisierung einzelner Gruppenmitglieder auf bestimmte Fertigkeiten entstanden die ersten Berufe.

Der Algorithmusbegriff als abstrakte Sicht auf Aufgabenlösungswege trat zuerst im Rahmen der Mathematik, Logik und Philosophie ins Bewusstsein der Menschen. Ein Beispiel für einen mathematischen Algorithmus aus dem Altertum ist der Euklidische Algorithmus.

Das Wort "Algorithmus" ist eine Abwandlung oder Verballhornung des Namens des arabischen Rechenmeisters und Astronomen Muḥammad Ibn-Mūsā al-H̱wārizmī, dessen Namensbestandteil (Nisba) "al-Chwarizmi" „der Choresmier“ bedeutet und auf die Herkunft des Trägers aus Choresmien verweist. Sein Lehrbuch "Über die indischen Ziffern" (verfasst um 825 im Haus der Weisheit in Bagdad) wurde im 12. Jahrhundert aus dem Arabischen ins Lateinische übersetzt und hierdurch in der westlichen Welt neben Leonardo Pisanos "Liber Abaci" zur wichtigsten Quelle für die Kenntnis und Verbreitung des indisch-arabischen Zahlensystems und des schriftlichen Rechnens. Mit der lateinischen Übersetzung al-Hwarizmis wurde auch der Name des Verfassers in Anlehnung an die Anfangsworte der ältesten Fassung dieser Übersetzung ("Dixit Algorismi" „Algorismi hat gesagt“) latinisiert. Aus al-Hwarizmi wurde mittelhochdeutsch "algorismus," "alchorismus" oder "algoarismus –" ein Wort, das aus dem Lateinischen nahezu zeitgleich und gleichlautend ins Altfranzösische ("algorisme", "argorisme)" und Mittelenglische ("augrim", "augrym") übersetzt wurde. Mit Algorismus bezeichnete man bis um 1600 Lehrbücher, die in den Gebrauch der Fingerzahlen, der Rechenbretter, der Null, die indisch-arabischen Zahlen und das schriftliche Rechnen einführen. Das schriftliche Rechnen setzte sich dabei erst allmählich durch. So beschreibt etwa der englische Dichter Geoffrey Chaucer noch Ende des 14. Jahrhunderts in seinen "Canterbury Tales" einen Astrologen, der Steine zum Rechnen („augrym stones“) am Kopfende seines Betts aufbewahrt:

In der mittelalterlichen Überlieferung wurde das Wort bald als erklärungsbedürftig empfunden und dann seit dem 13. Jahrhundert zumeist als Zusammensetzung aus einem Personennamen "Algus" und aus einem aus dem griechischen (Nebenform von ) in der Bedeutung „Zahl“ entlehnten Wortbestandteil "-rismus" interpretiert.

Algus, der vermutete Erfinder dieser Rechenkunst, wurde hierbei von einigen als Araber, von anderen als Grieche oder zumindest griechisch schreibender Autor, gelegentlich auch als „König von Kastilien“ (Johannes von Norfolk) betrachtet. In der volkssprachlichen Tradition erscheint dieser „Meister Algus“ dann zuweilen in einer Reihe mit großen antiken Denkern wie Platon, Aristoteles und Euklid, so im altfranzösischen "Roman de la Rose", während das altitalienische Gedicht "Il Fiore" ihn sogar mit dem Erbauer des Schiffes Argo gleichsetzt, mit dem Jason sich auf die Suche nach dem Goldenen Vlies begab.

Auf der para-etymologischen Zurückführung des zweiten Bestandteils "-rismus" auf griech. , beruht dann auch die präzisierende lateinische Wortform "algorithmus", die seit der Frühen Neuzeit, anfangs auch mit der Schreibvariante "algorythmus", größere Verbreitung erlangte und zuletzt die heute übliche Wortbedeutung als Fachterminus für geregelte Prozeduren zur Lösung definierter Probleme annahm.

Algorithmen sind eines der zentralen Themen der Informatik und Mathematik. Sie sind Gegenstand einiger Spezialgebiete der Theoretischen Informatik, der Komplexitätstheorie und der Berechenbarkeitstheorie, mitunter ist ihnen ein eigener Fachbereich Algorithmik oder Algorithmentheorie gewidmet. In Form von Computerprogrammen und elektronischen Schaltkreisen steuern Algorithmen Computer und andere Maschinen.

Für Algorithmen gibt es unterschiedliche formale Repräsentationen. Diese reichen vom Algorithmus als abstraktem Gegenstück zum konkret auf eine Maschine zugeschnittenen Programm (das heißt, die Abstraktion erfolgt hier im Weglassen der Details der realen Maschine, das Programm ist eine konkrete Form des Algorithmus, angepasst an die Notwendigkeiten und Möglichkeiten der realen Maschine) bis zur Ansicht, Algorithmen seien gerade die Maschinenprogramme von Turingmaschinen (wobei hier die Abstraktion in der Verwendung der Turingmaschine an sich erfolgt, das heißt, einer idealen mathematischen Maschine).

Algorithmen können in Programmablaufplänen nach DIN 66001 oder ISO 5807 grafisch dargestellt werden.

Der erste für einen Computer gedachte Algorithmus (zur Berechnung von Bernoullizahlen) wurde 1843 von Ada Lovelace in ihren Notizen zu Charles Babbages Analytical Engine festgehalten. Sie gilt deshalb als die erste Programmiererin. Weil Charles Babbage seine Analytical Engine nicht vollenden konnte, wurde Ada Lovelaces Algorithmus nie darauf implementiert.

Algorithmen für Computer sind heute so vielfältig wie die Anwendungen, die sie ermöglichen sollen. Vom elektronischen Steuergerät für den Einsatz im KFZ über die Rechtschreib- und Satzbau-Kontrolle in einer Textverarbeitung bis hin zur Analyse von Aktienmärkten finden sich tausende von Algorithmen. Hinsichtlich der Ideen und Grundsätze, die einem Computerprogramm zugrunde liegen, wird einem Algorithmus in der Regel urheberrechtlicher Schutz versagt. Je nach nationaler Ausgestaltung der Immaterialgüterrechte sind Algorithmen der Informatik jedoch dem Patentschutz zugänglich, so dass urheberrechtlich freie individuelle Werke, als Ergebnis eigener geistiger Schöpfung, wirtschaftlich trotzdem nicht immer frei verwertet werden können. Dies betrifft oder betraf z. B. Algorithmen, die auf der Mathematik der Hough-Transformation (Jahrzehnte alt, aber mehrfach aktualisiertes Konzept mit Neu-Anmeldung) aufbauen, Programme, die das Bildformat GIF lesen und schreiben wollten oder auch Programme im Bereich der Audio- und Video-Verarbeitung, da die zugehörigen Algorithmen, wie sie in den zugehörigen Codecs umgesetzt sind, oftmals nicht frei verfügbar sind. Die entsprechenden Einsparpotentiale für alle Anwender weltweit (für den Rete-Algorithmus wurde einst 1 Million USD auf DEC XCON genannt) dürften heute problemlos die Grenze von einer Milliarde USD im Jahr um ein zigfaches überschreiten.

Die mangelnde mathematische Genauigkeit des Begriffs Algorithmus störte viele Mathematiker und Logiker des 19. und 20. Jahrhunderts, weswegen in der ersten Hälfte des 20. Jahrhunderts eine ganze Reihe von Ansätzen entwickelt wurde, die zu einer genauen Definition führen sollten. Formalisierungen des Berechenbarkeitsbegriffs sind die Turingmaschine (Alan Turing), Registermaschinen, der Lambda-Kalkül (Alonzo Church), rekursive Funktionen, Chomsky-Grammatiken (siehe Chomsky-Hierarchie) und Markow-Algorithmen.

Es wurde – unter maßgeblicher Beteiligung von Alan Turing selbst – gezeigt, dass all diese Methoden die gleiche Berechnungsstärke besitzen (gleich "mächtig" sind). Sie können durch eine Turingmaschine emuliert werden, und sie können umgekehrt eine Turingmaschine emulieren.

Mit Hilfe des Begriffs der Turingmaschine kann folgende formale Definition des Begriffs formuliert werden:

"Eine Berechnungsvorschrift zur Lösung eines Problems heißt genau dann Algorithmus, wenn eine zu dieser Berechnungsvorschrift äquivalente Turingmaschine existiert, die für jede Eingabe, die eine Lösung besitzt, stoppt."

Aus dieser Definition sind folgende Eigenschaften eines Algorithmus ableitbar:


Darüber hinaus wird der Begriff Algorithmus in praktischen Bereichen oft auf die folgenden Eigenschaften eingeschränkt:


Die Church-Turing-These besagt, dass jedes intuitiv berechenbare Problem durch eine Turingmaschine gelöst werden kann. Als formales Kriterium für einen Algorithmus zieht man die Implementierbarkeit in einem beliebigen, zu einer Turingmaschine äquivalenten Formalismus heran, insbesondere die Implementierbarkeit in einer Programmiersprache – die von Church verlangte Terminiertheit ist dadurch allerdings noch nicht gegeben.

Der Begriff der Berechenbarkeit ist dadurch dann so definiert, dass ein Problem genau dann "berechenbar" ist, wenn es einen (terminierenden) Algorithmus zu dem Problem gibt, das heißt, wenn eine entsprechend programmierte Turingmaschine das Problem "in endlicher Zeit" lösen könnte.

Turingmaschinen harmonieren gut mit den ebenfalls abstrakt-mathematischen berechenbaren Funktionen, reale Probleme sind jedoch ungleich komplexer, daher wurden andere Maschinen vorgeschlagen.

Diese Maschinen weichen etwa in der Mächtigkeit der Befehle ab; statt der einfachen Operationen der Turingmaschine können sie teilweise mächtige Operationen, wie etwa Fourier-Transformationen, in einem Rechenschritt ausführen.

Oder sie beschränken sich nicht auf eine Operation pro Rechenschritt, sondern ermöglichen parallele Operationen, wie etwa die Addition zweier Vektoren in einem Schritt.

Ein Modell einer echten Maschine ist die ' (kurz ') mit folgenden Eigenschaften:

Ein Algorithmus einer seq. ASM soll

Ein Algorithmus ist determiniert, wenn dieser bei jeder Ausführung mit gleichen Startbedingungen und Eingaben gleiche Ergebnisse liefert.

Ein Algorithmus ist deterministisch, wenn zu jedem Zeitpunkt der Algorithmusausführung der nächste Handlungsschritt eindeutig definiert ist. Wenn an mindestens einer Stelle mehr als eine Möglichkeit besteht (ohne Vorgabe, welche zu wählen ist), dann ist der gesamte Algorithmus "nichtdeterministisch".

Beispiele für deterministische Algorithmen sind Bubblesort und der euklidische Algorithmus. Dabei gilt, dass jeder deterministische Algorithmus determiniert, während aber nicht jeder determinierte Algorithmus deterministisch ist. So ist Quicksort mit zufälliger Wahl des Pivotelements ein Beispiel für einen determinierten, aber nicht deterministischen Algorithmus, da sein Ergebnis bei gleicher Eingabe und eindeutiger Sortierung immer dasselbe ist, der Weg dorthin jedoch zufällig erfolgt.

Nichtdeterministische Algorithmen können im Allgemeinen mit keiner realen Maschine (auch nicht mit Quantencomputern) "direkt" umgesetzt werden.

Beispiel für einen nichtdeterministischen Algorithmus wäre ein Kochrezept, dass mehrere Varianten beschreibt. Es bleibt dem Koch überlassen, welche er durchführen möchte. Auch das Laufen durch einen Irrgarten lässt an jeder Verzweigung mehrere Möglichkeiten, und neben vielen Sackgassen können mehrere Wege zum Ausgang führen.

Die Beschreibung des Algorithmus besitzt eine endliche Länge, der Quelltext muss also aus einer begrenzten Anzahl von Zeichen bestehen.

Ein Algorithmus darf zu jedem Zeitpunkt seiner Ausführung nur begrenzt viel Speicherplatz benötigen.

Ein Algorithmus ‚terminiert überall‘ oder ‚ist terminierend‘, wenn er nach endlich vielen Schritten anhält (oder kontrolliert abbricht) – für jede mögliche Eingabe. Ein nicht-terminierender Algorithmus (somit zu keinem Ergebnis kommend) gerät (für manche Eingaben) in eine so genannte Endlosschleife.

Für manche Abläufe ist ein nicht-terminierendes Verhalten gewünscht: Z. B. Steuerungssysteme, Betriebssysteme und Programme, die auf Interaktion mit dem Benutzer aufbauen. Solange der Benutzer keinen Befehl zum Beenden eingibt, laufen diese Programme beabsichtigt endlos weiter. Donald E. Knuth schlägt in diesem Zusammenhang vor, nicht terminierende Algorithmen als rechnergestützte Methoden "(Computational Methods)" zu bezeichnen.

Darüber hinaus ist die Terminierung eines Algorithmus (das Halteproblem) nicht entscheidbar. Das heißt, das Problem, festzustellen, ob ein (beliebiger) Algorithmus mit einer beliebigen Eingabe terminiert, ist nicht durch einen Algorithmus lösbar.

Der Effekt jeder Anweisung eines Algorithmus muss eindeutig festgelegt sein.


Die Erforschung und Analyse von Algorithmen ist eine Hauptaufgabe der Informatik, und wird meist theoretisch (ohne konkrete Umsetzung in eine Programmiersprache) durchgeführt. Sie ähnelt somit dem Vorgehen in anderen mathematischen Gebieten, in denen die Analyse eher auf die zugrunde liegenden Konzepte als auf konkrete Umsetzungen ausgerichtet ist. Algorithmen werden zur Analyse in eine stark formalisierte Form gebracht und mit den Mitteln der formalen Semantik untersucht.

Die Analyse unterteilt sich in verschiedene Teilgebiete. Beispielsweise wird das Verhalten von Algorithmen bezüglich Ressourcenbedarf wie Rechenzeit und Speicherbedarf in der Komplexitätstheorie behandelt; die Ergebnisse werden als asymptotische Laufzeiten angegeben. Der Ressourcenbedarf wird dabei in Abhängigkeit von der Länge der Eingabe ermittelt, das heißt, die angegebene Komplexität hängt davon ab, wie groß die Zahlen sind, deren größter gemeinsamer Teiler gesucht wird, oder wie viele Elemente sortiert werden müssen etc.

Das Verhalten bezüglich der Terminierung, ob also der Algorithmus überhaupt jemals erfolgreich beendet werden kann, behandelt die Berechenbarkeitstheorie.

Der älteste bekannte nicht-triviale Algorithmus ist der euklidische Algorithmus. Spezielle Algorithmus-Typen sind der randomisierte Algorithmus (mit Zufallskomponente), der Approximationsalgorithmus (als Annäherungsverfahren), die evolutionären Algorithmen (nach biologischem Vorbild) und der Greedy-Algorithmus.

Eine weitere Übersicht gibt die Liste von Algorithmen und die .

Obwohl der etymologische Ursprung des Wortes arabisch ist, entstanden die ersten Algorithmen im antiken Griechenland. Zu den wichtigsten Beispielen gehören das Sieb des Eratosthenes zum Auffinden von Primzahlen, welches im Buch "Einführung in die Arithmetik" von Nikomachos beschrieben wurde und der euklidische Algorithmus zum Berechnen des größten gemeinsamen Teilers zweier natürlicher Zahlen aus dem Werk „die Elemente“. Einer der ältesten Algorithmen, die sich mit einer reellen Zahl beschäftigen, ist der zur Approximation von formula_1, was zugleich auch eines der ältesten numerischen Verfahren ist.

Das Wort Algorithmus stammt aus dem 9. Jahrhundert von dem choresmischen Mathematiker Al-Chwarizmi, der auf der Arbeit des aus dem 7. Jahrhundert stammenden indischen Mathematikers Brahmagupta aufbaute. In seiner ursprünglichen Bedeutung bezeichnete ein Algorithmus nur das Einhalten der arithmetischen Regeln unter Verwendung der indisch-arabischen Ziffern. Die ursprüngliche Definition entwickelte sich mit Übersetzung ins Lateinische weiter.

Bedeutende Arbeit leisteten die Logiker des 19. Jahrhunderts.
George Boole der in seiner Schrift "The Mathematical Analysis of Logic" den ersten algebraischen Logikkalkül erschuf, begründete damit die moderne mathematische Logik, die sich von der traditionellen philosophischen Logik durch eine konsequente Formalisierung abhebt.
Gottlob Frege entwickelte als erster eine formale Sprache und die daraus resultierenden formalen Beweise.
Giuseppe Peano reduzierte die Arithmetik auf eine Sequenz von Symbolen manipuliert von Symbolen. Er beschäftigte sich mit der Axiomatik der natürlichen Zahlen. Dabei entstanden die Peano-Axiome.

Die Arbeit von Frege wurde stark von Alfred North Whitehead und Bertrand Russell in ihrem Werk Principia Mathematica weiter ausgearbeitet und vereinfacht. Zuvor wurde von Bertrand Russell die berühmte russellsche Antinomie formuliert, was zum Einsturz der naiven Mengenlehre führte. Das Resultat führte auch zur Arbeit Kurt Gödels.

David Hilbert hat um 1928 das Entscheidungsproblem in seinem Forschungsprogramm präzise formuliert. Alan Turing und Alonzo Church haben für das Problem 1936 festgestellt, dass es unlösbar ist.




</doc>

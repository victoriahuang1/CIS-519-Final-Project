<doc id="7771" url="https://de.wikipedia.org/wiki?curid=7771" title="Brasilien">
Brasilien

Brasilien (, gemäß Lautung des brasilianischen Portugiesisch [] ) ist der flächen- und bevölkerungsmäßig fünftgrößte Staat der Erde. Es ist das größte und mit über 200 Millionen Einwohnern auch das bevölkerungsreichste Land Südamerikas, von dessen Fläche es 47,3 Prozent einnimmt. Brasilien hat mit jedem südamerikanischen Staat außer Chile und Ecuador eine gemeinsame Grenze.

Die ersten Spuren menschlicher Besiedlung durch Paläo-Indianer reichen ca. 30.000 Jahre zurück. Nach der Entdeckung Amerikas und der Aufteilung des südamerikanischen Kontinents durch den Vertrag von Tordesillas wurde Brasilien eine portugiesische Kolonie. Diese mehr als drei Jahrhunderte andauernde Kolonialzeit, in der Einwanderer verschiedenster Herkunft (freiwillig oder gezwungenermaßen) nach Brasilien kamen, trug erheblich zur ethnischen Vielfalt des heutigen Staates bei. Nach der im Jahre 1822 erlangten Unabhängigkeit, auf die eine Zeit der konstitutionellen Monarchie folgte, wurde das Land 1889 als "Vereinigte Staaten von Brasilien" zu einer Republik. Nach der Zeit der Militärdiktatur von 1964 bis 1985 kehrte das Land zur Demokratie mit einem präsidentiellen Regierungssystem zurück.

Der Name "Brasilien" geht auf den portugiesischen Namen "pau-brasil" des Brasilholz-Baumes ("Caesalpinia echinata"), der ein wichtiges Ausfuhrprodukt zur Zeit der frühen Kolonisation aus den Wäldern der Atlantikküste war, zurück. "Brasa" bedeutet im Portugiesischen „Glut“ und „glühende Kohlen“; das Adjektiv "brasil" („glutartig“) bezieht sich auf die Farbe des Holzes, das, wenn geschnitten, rot leuchtet (Brasilin) und in Europa zum Färben von Stoffen benutzt wurde.

Brasiliens Landschaft ist von ausgedehnten Regenwäldern des Amazonas-Tieflands im Norden und Hochebenen, Hügeln und Gebirgen im Süden geprägt. Während die landwirtschaftliche Basis des Landes im Süden und in den Savannengebieten des Mittelwestens (Cerrado) liegt, lebt der Großteil der Bevölkerung in der Nähe der Atlantikküste, wo sich auch fast alle Großstädte befinden.

Brasilien hat zehn Nachbarstaaten. Es grenzt – mit Ausnahme von Chile und Ecuador – an alle südamerikanischen Staaten (von Nordosten gegen den Uhrzeigersinn gesehen): an Französisch-Guayana mit 730 km, Suriname mit 593 km, Guyana mit 1298 km, Venezuela mit 1819 km, Kolumbien mit 1645 km, Peru mit 2995 km, Bolivien mit 3400 km, Paraguay mit 1290 km, Argentinien mit 1132 km und Uruguay mit 985 km. Die gesamte Grenzlänge beträgt 15.887 km und ist damit nach der Volksrepublik China und Russland die drittlängste Landgrenze der Erde.

Der kontinentale Teil Brasiliens liegt in zwei Zeitzonen, einige vorgelagerte Inseln gehören zu einer dritten. Siehe hierzu: Zeitzonen in Brasilien.

Der höchste Gipfel ist der 2994 m hohe Pico da Neblina, der im gleichnamigen Nationalpark nahe der Grenze zu Venezuela und Guayana liegt. Der zweithöchste Berg ist der Pico 31 de Março (2973 m). Der dritthöchste Berg ist der Pico da Bandeira (2891 m). Berühmter allerdings sind der 710 m hohe Corcovado mit der 30 m hohen Erlöser-Statue wegen seines Blickes über Rio de Janeiro sowie der seiner konischen Form wegen berühmte 395 m hohe Zuckerhut.

Der wichtigste Fluss ist der Amazonas, seine Wasserführung von 209.000 m³/s macht ihn zum weitaus wasserreichsten Fluss der Erde, größer als die sieben nächstkleineren Flüsse der Welt zusammen. Der längste Fließweg seines Flusssystems misst 6448 km; in dieser Hinsicht wird er nur noch vom wesentlich wasserärmeren Nil übertroffen. Die bedeutendsten Nebenflüsse, der Rio Madeira und der Rio Negro, sind bereits mit den größten Strömen anderer Kontinente vergleichbar. Es folgen der Rio Icá und der Rio Tapajós.

Der Süden Brasiliens gehört bis auf einen schmalen Küstenstreifen zum Einzugsgebiet der Flüsse Uruguay (1790 km) und Paraná (3998 km). Der Paraná ist fast durchgehend aufgestaut; in Itaipú liegt das zweitgrößte Wasserkraftwerk der Welt. Einer seiner Nebenflüsse hat dem Staat Paraguay seinen Namen gegeben; ein anderer ist durch die Iguazú-Wasserfälle bekannt.

Die Lagoa dos Patos bei Porto Alegre ist mit über 10.000 km² die größte Lagune Brasiliens und die zweitgrößte Südamerikas. Danach kommt die weniger als halb so große Laguna Merín, südlich der Stadt "Rio Grande".

Zum brasilianischen Hoheitsgebiet gehören auch einige Inseln im Atlantik, z. B. die etwa 800 km vor der Küste gelegenen Sankt-Peter-und-Sankt-Pauls-Felsen, die nur mit einem Leuchtturm bebaut sind, und die ehemalige Sträflingskolonie Fernando de Noronha, die nicht weit von der Felsgruppe entfernt ist. Beide liegen auf dem mittelatlantischen Rücken. Vulkanischen Ursprungs sind die Inseln Trindade und Martim Vaz, die zum Bundesstaat Espírito Santo gehören. Das ovale Rocas-Atoll erstreckt sich über mehrere Kilometer und wurde aufgrund der außergewöhnlichen Tier- und Pflanzenwelt als Weltnaturerbe aufgenommen.

Die größte Insel aber ist Marajó zwischen der Mündung des Amazonas und dem Rio Pará, der zum Mündungsgebiet des Rio Tocantins gehört. Sie ist mit einer Fläche von etwa 48.000 km² größer als beispielsweise die Schweiz. Da aber große Teile in der Regenzeit überschwemmt sind, ist die Insel nur an einigen Orten besiedelt. Da das Nordufer von Marajó eine Meeresküste ist, gilt die Ilha do Bananal im Rio Araguaia mit ihrer Fläche von 20.000 km² als größte Flussinsel der Welt. Sie liegt in einem Nationalpark im Bundesstaat Tocantins und ist größer als beispielsweise Jamaika.

Das Klima Brasiliens, das zwischen 5° nördlicher Breite und 34° südlicher Breite liegt, ist überwiegend tropisch mit geringen jahreszeitlichen Schwankungen der Temperaturen. Nur im subtropischen Süden herrscht ein gemäßigteres Klima.
Besonders im Amazonasbecken gibt es reichhaltige Niederschläge, man findet jedoch auch relativ trockene Landstriche mit teilweise lang anhaltenden Dürrezeiten, besonders im Nordosten des Landes. In den höheren Lagen im Süden des Landes fällt im Winter der Niederschlag gelegentlich als Schnee.

Im Süden befindet sich an der Grenze zu Bolivien und Paraguay ein ausgedehntes Feuchtgebiet, das Pantanal.

Noch vor Kolumbien, Mexiko und Indonesien ist Brasilien das artenreichste Land der Erde. Entdeckt wurden bislang unter anderem rund 55.000 Blütenpflanzen-, über 3000 Süßwasserfisch-, 921 Amphibien-, 749 Reptilien- und 51 Primaten-Arten. Weil die Waldfläche stetig verkleinert wird, ist ein hoher Anteil dieser Tierarten in seinem Bestand gefährdet (vgl. Abschnitt Umwelt).

Der immergrüne tropische Regenwald des Amazonasgebiets ist die größte zusammenhängende Waldfläche Brasiliens. Bislang wurden dort mehr als 2500 Baumarten entdeckt. Fast alle dieser bis zu 60 m hohen Bäume finden sich im von Überschwemmungen verschonten Eté-Wald der "Terra Firme", die 98 % des Amazonasgebiets umfasst. In diesem Gebiet wachsen u. a. der Gummibaum ("caucho"), verschiedene Farb- und Edelhölzer (z. B. Palisander), Fruchtbäume (z. B. Paranussbaum) und Heilpflanzen. Auffällig sind die etwa 1000 verschiedenen Farn- und Orchideenarten. Neben der Terra firme gibt es die "Várzea", die bei Hochwasser überschwemmt ist. Dort wachsen Jupati- und Miriti-Palmen. Das "Igapó"-Gebiet ist dagegen ständig überschwemmt. Als typische Pflanze in diesem Gebiet gilt die Açaí-Palme. Auf dem Amazonas, aber vor allem auf seinen Nebenflüssen, wachsen Seerosen, deren Blüten 30 bis 40 cm groß werden können. Entlang der Küste Amazoniens (mit Ausnahme der eigentlichen Amazonasmündung) finden sich ausgedehnte Mangrovenwälder, die allerdings mit sechs Mangrovenbaum-Arten verhältnismäßig artenarm sind.

Besonders bekannt sind im gesamten Amazonasgebiet vor allem Papageien, Tukane und Kolibris. Es sind extrem viele Insekten- und Schmetterlingsarten bekannt. Größere Waldtiere sind der Tapir, das Wildschwein (Pekari), der Jaguar und der Puma. Daneben bevölkern kleinere Wildkatzen, Affen, Faultiere, Gürteltiere und Ameisenbären den Regenwald. An den Ufern und Flachwässern leben Anakondas, Kaimane und Capybaras („Wasserschweine“ – die größten Nagetiere der Welt) und weitere Säugetiere wie Riesenotter, Flussdelfine und Seekühe im tieferen Wasser. Auch zahlreiche Fischarten (etwa 1500) sind im Amazonas beheimatet. Darunter einer der größten bekannten Süßwasserfische der Welt: Der "Pirarucú" ist 2 m lang und wiegt etwa 100 kg. Ein Zitteraal, der 800-Volt-Stromschläge austeilt, und die Piranhas, manche Arten gut 30 cm lang, sind ebenso außergewöhnlich.

Der äußerste Nordosten Brasiliens, früher ebenso aus Regenwald bestehend, wird mittlerweile fast ausschließlich für Zuckerrohr-Plantagen und den Anbau von Baumwolle genutzt. Vereinzelt lassen sich noch Mangroven und Palmenhaine finden.

Die typische Vegetation des semiariden Berg- und Hochlandes im Zentrum "(Cerrado") und im Nordosten des Landes ("Sertão") ist die Savanne, von Baum- und Grassavannen, nach Nordosten hin, zu mit Laubbäumen durchsetzter Strauchsavanne. Typische Bewohner dieser Trockenzonen sind Großer Ameisenbär, Mähnenwolf, Pampashirsch, Nandu und verschiedene Gürteltiere. All diese Arten und daneben auch große Raubkatzen wie Jaguare und Pumas werden etwa im Nationalpark Emas, der eine Welterbestätte bildet, geschützt.

Das Pantanal weist eine noch größere Tier- und Pflanzenvielfalt auf. Charakteristisch sind neben zahlreichen Vogelarten Flachlandtapir, Sumpfhirsch, Wasserschwein und Kaiman. Die Sumpfregion im Mittelwesten des Landes steht sieben Monate im Jahr unter Wasser. Höher gelegene Gebiete der Region sind überwiegend Savanne. Wie auch in denen des Cerrado und sogar im Amazonasgebiet machen sich dort Weiden für Rinder breit.

In den küstennahen Gebirgen des Südens und Südostens finden sich die Schwerpunkte der kolonialen Erschließung und die am dichtesten besiedelten Gebiete. Anstelle des ursprünglichen atlantischen Regenwaldes, Lebensraum für Affen und zahlreiche andere Tierarten, dominieren Kaffeeplantagen. Die ursprüngliche Vegetation ist nur noch in einigen Nationalparks zu finden.

Der Süden zeigt subtropische Vegetation; die ursprünglichen Wälder aus Araukarien, die eine Höhe von bis zu 40 m erreichen, wurden größtenteils für Holzgewinnung zerstört. Heute sind Niedergrassteppen in dieser Region häufiger.

In Brasilien gibt es 62 Nationalparks (Parques Nacionais). Schutzgebiete ähnlichen Charakters gibt es unter dem Namen Estação Ecológica. Es gibt auch Schutzgebiete der Bundesstaaten (Parques Estaduais) und auf Gemeindeebene. Diese und weitere Flächen wurden wegen ihrer ökologischen, wissenschaftlichen, touristischen und erzieherischen Bedeutung unter Schutz gestellt.

Einige Organisationen, die sich Natur- und Artenschutz verschrieben haben, sind:


Während der atlantische Küstenregenwald bereits zu rund 93 % zerstört ist und die Reste stark fragmentiert sind, ist der tropische Regenwald des Amazonasgebietes eines der größten noch verbliebenen Urwaldgebiete der Welt. Bis zur Ankunft der Europäer wurde er von der indigenen Urbevölkerung extensiv und nachhaltig genutzt, so dass die herbeigeführten Veränderungen der Ökosysteme der Artenvielfalt eher nutzten als schadeten. Viele der modernen Landnutzungsänderungen hingegen fügen den Wäldern immense Schäden zu. Das sind vor allem Rodungen für die Schaffung landwirtschaftlicher Flächen, die plantagenartige Land- und Forstwirtschaft (z. B. Jari-Projekt), aber auch Infrastrukturprojekte wie Straßen (zum Beispiel die Transamazônica und die Perimetral Norte), Minen (z. B. Serra dos Carajás) und Großstaudämme (selbst an direkten Nebenflüssen des Amazonas wie Tucuruí oder Belo Monte). Dabei wirkt sich nicht nur der Flächenverbrauch durch die Bauvorhaben selbst aus. Die zugehörigen Straßen machen die Gebiete für den (heute vorwiegend illegalen) Holzeinschlag verfügbar.

Das Holz aus diesen Wäldern wird nur zum Teil von der lokalen Bevölkerung genutzt (z. B. als Feuerholz oder für bereits in Brasilien hergestellte höherwertige Produkte wie Sperrholz, Zellstoff oder Baumaterial). Ein großer Teil wird international gehandelt. In Brasilien gibt es rund 2500 Unternehmen, die tropisches Hartholz kaufen und verkaufen. Die meisten von ihnen sind ausländische Großunternehmen. Zwar sind einige Tropenhölzer wie z. B. Mahagoni mittlerweile gesetzlich geschützt, der Handel geht jedoch illegal weiter.

Nach Angaben der FAO waren 2010 noch 60,1 % der Landesfläche mit Urwald bedeckt, im Vergleich zu 66,9 % im Jahr 1990 (ohne Berücksichtigung aufgeforsteter Flächen). Im Zeitraum 2000–2005 lag der Urwaldverlust bei jährlich 32.000 km². Auf die Gesamtfläche der Wälder bezogen gingen in den letzten 20 Jahren jährlich rund 0,5 % verloren.

Von 2004 (ca. 27.000 km² jährlich) bis 2012 waren die Raten rückläufig. 2005 wurden 18.793 km² bekanntgegeben, 2006 waren es 14.039 km². Nach Angaben des deutschen BMZ lag die Entwaldungsrate 2012 „nur“ noch bei rund 4570 km² (das ist etwas weniger als die Fläche der Balearen oder 0,09 % der gesamten Regenwaldfläche Brasiliens). Von August 2012 bis Juli 2013 hat die Rodung allerdings wieder auf 5800 km² zugenommen.

Den Rückgang des Verlustes von Primärwald führte die Regierung Brasiliens auf die Durchsetzung ihrer Umweltstandards zurück, Umweltschützer sehen die Stärke des Real und die fallenden Sojapreise als Gründe. In der Folge beriet im Januar 2008 ein Notfallkabinett der Regierung über Maßnahmen. Die Behörden zum Schutz des Regenwaldes haben mit Geld- und Personalmangel sowie Korruption zu kämpfen. Nur im Rahmen von Schutzgebieten erfährt der Amazonaswald eine relative Sicherung. So konnte 2002 das weltweit größte Schutzgebiet (Tumucumaque) eines tropischen Regenwalds im Norden Brasiliens gegründet werden.

Brasilien hat Mitte 2008 einen Fonds zum Schutz des Amazonas-Regenwaldes ins Leben gerufen und erstmals einen Zusammenhang zwischen diesem Schutz und der globalen Erwärmung akzeptiert. Die Regierung plant bis zum Jahr 2021 Investitionen von mehreren Millionen Euro, um an Stelle der Rodung nachhaltige wirtschaftliche Grundlagen für die Amazonasbevölkerung zu entwickeln. Das Land verhält sich aber gegenüber ausländischen Einflussnahmen in seine Amazonas-Politik abwehrend.

Regenwaldböden sind nährstoffarm, daher ist die Vegetation auf die Wiederverwertung der Nähr- und Mineralstoffe aus der toten Biomasse angewiesen. Im tropisch heißfeuchten Klima zersetzen Mikroorganismen Laubstreu in sehr kurzer Zeit und führen sie den Pflanzen wieder zu, wohingegen kaum bodenbildende Prozesse stattfinden. Wenn aber der Wald entfernt wird und die Humusschicht gegen Sonne und Niederschläge ungeschützt liegt oder sich somit keine neue mehr auf dem unfruchtbaren Unterboden bilden kann, trocknen diese aus und es kommt zu Erosion. Sind die gerodeten Flächen größer, kann sich der Wald dort nicht regenerieren.

Bäume binden Kohlenstoffdioxid, das in der Atmosphäre einen Treibhauseffekt bewirkt. Die in Brasilien freigesetzten Treibhausgase gehen zu drei Vierteln auf Brandrodungen und zu einem Viertel auf die Verbrennung fossiler Brennstoffe zurück.

Ein weiteres Umweltproblem ist der Bauxit- und Goldtagebau, der die Flüsse vergiftet und die lokale Bevölkerung gefährdet. Die Goldgräber (Garimpeiros) verwenden zum Auswaschen des Goldes Quecksilber (Amalgamverfahren). Die giftigen Dämpfe entweichen in die Luft, und das Schwermetall verseucht Gewässer, Böden und Grundwasser und verursacht damit schwerwiegende Gesundheitsschäden bei Mensch und Tier.

Wie überall zieht die Förderung von Erdöl Probleme nach sich: 2000 erlitt der Fluss Iguaçu eine Ölpest. Ein Jahr später sank vor der brasilianischen Küste die damals größte Bohrplattform der Welt und bedrohte das dortige Ökosystem. Städte haben mit Luftverschmutzung und Abwasserproblemen zu kämpfen.

In Brasilien wird dem Kraftstoff eine gewisse Menge Alkohol beigemischt. Neben umwelttechnischen Gründen (Reduzierung der Schadstoffemissionen) sind dafür hauptsächlich die Kosten verantwortlich: Ethanol ist deutlich billiger als Automobil- und Flugbenzin. Der Anteil an Ethanol im Benzin ist gesetzlich geregelt und wurde 2006 von ehemals 25 % auf 20 % gesenkt. In Brasilien kann man Autos fahren, die einen Ethanol-, Benzin- oder einen Flex-Fuel-Motor besitzen. Das dreimillionste Flex-Fuel-Auto wurde im Dezember 2005 verkauft. Auch die ersten Flugzeuge fliegen mit Ethanol, was die Luftverschmutzung insgesamt reduziert. Das erste mit Alkohol betriebene Flugzeug der Welt, die EMB-202 Ipanema, wurde 2002 von Embraer in Brasilien gebaut. Brasilien ist der viertgrößte Auto- und mit 12.000 Flugzeugen der zweitgrößte Flugzeug-Produzent der Welt.

Brasilien hat sich an diesen Umweltabkommen beteiligt: Ramsar-Konvention (1971), Washingtoner Artenschutzübereinkommen CITES (1973), Biodiversitätskonvention (1992), Basler Konvention (1989), Rahmenübereinkommen über Stoffe, die zum Abbau der Ozonschicht führen, Kyoto-Protokoll (1997).

Brasiliens Bevölkerung erlebte im Laufe des 20. Jahrhunderts eine enorme Expansion und wuchs von knapp 70 Millionen im Jahre 1950 auf über 200 Millionen heute an. In Zukunft wird allerdings nur noch ein moderater Zuwachs erwartet. Die brasilianische Bevölkerung ist noch sehr jung. 23,27 % sind unter 15 Jahre alt und nur 7,8 % über 64 (Stand: 2015). Das mittlere Alter beträgt 31,1 Jahre, die mittlere Lebenserwartung liegt bei 74,7 Jahren. Sie lag 2015 bei der männlichen Bevölkerung bei 71,0 Jahren und bei der weiblichen bei 78,4 Jahren.

2014 betrug die Geburtenziffer 15 Neugeborene auf 1000 Einwohner. Die durchschnittliche Kinderzahl je Frau betrug 1,8. Mit der Urbanisierung und dem steigenden Wohlstand ist die Geburtenziffer deutlich gesunken. In den 1950er Jahren lag die Fertilität pro Frau noch bei über 6 Kindern. Die Sterbeziffer betrug sechs auf 1000 Einwohner und ist damit, dank noch jungem Durchschnittsalter und steigender Lebenserwartung, sehr niedrig.

Brasilien gehört somit zu den Ländern, in denen die Fruchtbarkeit innerhalb weniger Jahrzehnte rapide gefallen ist. Wegen früherer hoher Fruchtbarkeitsraten gibt es noch relativ viele junge Menschen, doch es befindet sich in der fünften Phase des demographischen Überganges. In dieser Phase liegt die Kinderzahl pro Frau unter dem langfristig zur Konstanthaltung der Bevölkerung notwendigen Niveau von 2,1 und die Bevölkerung wird langfristig gesehen ohne Zuwanderung abnehmen. Es wird angenommen, dass es ab 2025 zu einer Überalterung der Bevölkerung und somit zu einem Mangel an Arbeitskräften bei gleichzeitiger Zunahme der älteren Bevölkerung kommen wird.

85,7 % der Bevölkerung lebten im Jahr 2015 in den Städten, die sich durch rasantes Wachstum und Wildwuchs auszeichnen; in zuvor unerschlossenen Gebieten der Städte haben sich Favelas genannte Armensiedlungen gebildet.

Etwa 90 % der Bevölkerung konzentrieren sich auf die Bundesstaaten der Ost- und Südküste Brasiliens mit einer Bevölkerungsdichte von 20 bis über 300 Einwohner/km². Der Rest Brasiliens, mit dem Amazonas und den Bergregionen, hat zwar die weitaus meiste Fläche, aber nur eine Bevölkerungsdichte von unter fünf bis 20 Einwohnern/km². Der Hauptstadtdistrikt Distrito Federal do Brasil als Stadtstaat und der Bundesstaat Rio de Janeiro haben eine Bevölkerungsdichte von über 300 Einwohner/km².

Das Migrationssaldo pro 1000 Einwohner liegt bei 0. Das bedeutet, dass ungefähr gleich viele Personen nach Brasilien einwandern wie auswandern. Obwohl ein großer Teil der Bevölkerung Brasiliens historische Wurzeln im Ausland hat sind heute nur 0,1 % der Bevölkerung außerhalb Brasiliens geboren, womit Brasilien einen der niedrigsten Ausländeranteile der Welt hat. Insgesamt befanden sich 2015 ca. 713.000 Migranten im Land wovon die größte Gruppe Portugiesen waren. Im selben Jahr lebten knapp 20.000 in Deutschland geborene Personen in Brasilien.

Ursprünglich vier Bevölkerungsgruppen bilden die brasilianische Bevölkerung. Sie sind heute jedoch so umfassend vermischt, dass eine klare Zuordnung oft nicht mehr möglich ist. Diese Gruppen sind:


Etwa die Hälfte der brasilianischen Bevölkerung hat einen nicht unerheblichen Anteil afrikanischer Vorfahren, die vom 16. bis zum 19. Jahrhundert als afrikanische Sklaven in das Land gebracht wurden. Die Schwarzen haben sich jedoch im Laufe der Zeit stark mit der europastämmigen Bevölkerung vermischt. Laut einer genetischen Studie aus dem Jahre 2013 ist die Bevölkerung Brasiliens im Durchschnitt zu ca. 60 % europäischer Abstammung, zu ca. 25 % afrikanischer Abstammung und zu ca. 15 % indianischer Abstammung. Europäische Abstammungslinien sind am stärksten im Süden des Landes verbreitet mit 74 % und am wenigsten im Norden mit 51 %. Afrikanische Gene sind am stärksten im Nordosten verbreitet mit 28 % und am schwächsten im Süden mit 15 %. Indigene Abstammung ist am stärksten im dünn besiedelten Norden des Landes verbreitet mit 32 % und am schwächsten im Süden mit 11 %. Übergänge zwischen ethnischen Gruppen sind in Brasilien oft fließend, da die große Mehrheit der Bevölkerung von mehr als einer Bevölkerungsgruppe abstammt. So waren Brasilianer, die sich selbst als Weiße bezeichnen, zu 75 % europäischer Abstammung, während Brasilianer, die sich selbst als Schwarze bezeichnen, zu 58 % afrikanischer Abstammung waren.

Nach einer Erhebung des IBGE im Jahre 2005 (2016) bezeichnen sich rund 49,9 % (45,5 %) der Brasilianer selbst als Weiße, 43,2 % (45 %) als Mischlinge ("pardo") und 6,3 % (8,6 %) als Schwarze, 0,7 % (0,9 %) als Gelbe oder Indigene. Der größte Teil der afrobrasilianischen Bevölkerung lebt im Nordosten. Das Selbstverständnis und das Verhältnis der Rassen untereinander ist nicht frei von Konflikten und unbearbeitet. 70 Prozent der von Armut betroffenen sind Afrobrasilianer und auch bei der Kriminalität und deren Opfer sind drei Viertel der Betroffenen dunkelhäutig.

Die indigenen Völker in Brasilien lebten partiell von Fischfang und Sammeln Jagd, zudem von dem fragilen Ökosystem angepasster Bodenbewirtschaftung. Ein großer Teil der einheimischen Bevölkerung starb im Zuge der europäischen Kolonialisierung, meist an eingeschleppten Krankheiten, aber auch infolge von Zwangsarbeit oder durch Versklavung. Der Großteil der außerhalb des Regenwalds lebenden Indianer, insbesondere in den Städten, wurde, soweit er Gewalt und Epidemien überlebte, assimiliert und vermischte sich mit den europäischen Einwanderern. Von schätzungsweise fünf bis sechs Millionen Indios in der Zeit um 1500 brach die Bevölkerungszahl bis zum Jahr 1950 auf 100.000 ein.

Bis 1997 wuchs die indigene Bevölkerung wieder auf etwa 300.000. Nach Angaben der brasilianischen Botschaft leben heute ungefähr 410.000 Indios im Land, was rund 0,2 % der Bevölkerung entspricht. 2005 gab es Berichte über einen erneuten Anstieg der Zahl der in Brasilien lebenden Indios auf etwa eine halbe Million. Das größte indigene Volk Brasiliens sind die Guaraní mit circa 46.000 Angehörigen in sieben Bundesstaaten.

100.000 bis 200.000 Indios leben heute in Städten, wodurch die indianische Kultur zunehmend verloren geht. Es bestehen zwar zahlreiche Reservate im Amazonasgebiet, doch leben nur wenige gemäß ihrer hergebrachten Kultur. Durch die Abholzung des Urwalds wird ihr Lebensraum rapide zerstört. Dabei werden die erwirtschafteten Erlöse aus dem Amazonasgebiet heraustransferiert, es mangelt also an Investitionen vor Ort oder gar Entschädigungen. Minenarbeiter und Goldgräber belasten nicht nur Flüsse und Böden mit schwerem Gerät und giftigen Chemikalien, sie bringen auch Krankheiten und Gewalt. Der Regierung wird dabei Mitschuld vorgeworfen, da Mörder nur selten strafrechtlich verfolgt werden. Außerdem vergibt sie Genehmigungen zur wirtschaftlichen Nutzung von Gebieten (z. B. zur Ölförderung), die von Indios bewohnt sind. Aufgrund dieser extrem schlechten Erfahrungen meiden an die hundert Völker möglichst jeden Kontakt.

Demgegenüber steht die offizielle Rechtsposition der Indigenen in Brasilien. Bereits 1988 wurden ihnen als Folge der internationalen Debatte um die ILO-Konvention 169 in der Verfassung (Art. 231) weitgehende Rechte garantiert, die das traditionelle Leben, die Selbstbestimmung sowie auch die Eigentums- und Nutzungsrechte an ihrem Land enthalten. Im August 2017 schützte ein Gericht die Rechte der Indios gegen eine "Zeitgrenze", wonach sie den Anspruch auf im Jahr 1988 nicht bewohnte Gebiete verloren hätten.

Brasilien hat im internationalen Vergleich eine sehr entwickelte Debatte über sogenannte „traditionelle Völker und Gemeinschaften“ (Povos e Comunidades Tradicionais). Mit dieser originär brasilianischen Bezeichnung werden alle lokalen Gemeinschaften zusammengefasst, die eine an Traditionen und Subsistenzwirtschaft orientierte Lebensweise führen. Entscheidend dabei ist dass die Zuordnung unabhängig von der ethnischen Zugehörigkeit ist, und so zählen hierzu nicht nur indigene Gruppen, sondern auch nicht-indigene Gruppen wie die Quilombolas, die von schwarzen Sklaven abstammen, oder die Kautschukzapfer, die europäische und indianische Vorfahren haben.

Traditionelle Völker und Gemeinschaften sind Kulturen, die sich im Laufe ihrer Geschichte erkennbar häufiger für die Bewahrung der bestehenden Strukturen positioniert haben. Da dies immer ein aktiver Prozess ist, sind sie weder primitiver noch weniger dynamisch als die „modernen Kulturen“. Zudem muss man beachten, dass die Zuordnung relativ ist, da die Unterscheidung von „traditionell“ und „modern“ eine subjektive Wertung ist, die vom Zeitgeist abhängt und die einseitig aus der Sicht der Modernen erfolgt!

Ganz im Gegenteil betrachtet die Wissenschaft sie heute als die Gruppen, die bisher am wenigsten zur ökologischen und klimatischen Gefährdung des Planeten beigetragen haben. Sie haben eine große Zahl von traditionell nachhaltigen Lebens- und Wirtschaftsweisen entwickelt, die an die jeweiligen Ökosysteme angepasst sind. Gleichzeitig sind es gerade diese Gruppen, die unter ökonomischen Erschließungsprojekten sowie ökologischen und klimatischen Veränderungen besonders zu leiden haben. Die vielfältigen und massiven Konflikte lassen befürchten, dass viele lokale Gemeinschaften trotz politischer Verbesserungen ihre Territorien verlieren werden und damit ihre spezifischen kulturellen Ausdrucksformen.

Während die Landrechte der Indigenen seit der Gründung Brasiliens eine Rolle in der Politik spielen, begann die Debatte über die Rechte für nicht-indigene, lokale Gemeinschaften erst in den 1980er Jahren. Es begann mit den Kautschukzapfern im Bundesstaat Acre: Sie forderten gesicherte Territorien und das Recht auf eine nachhaltige regionale Wirtschaftsweise und entwickelten dazu die Idee der Sammelgebiete. Diese Bestrebungen führten bis 2007 zur Ausweisung von 65 solcher Nutzreservate (Reservas Extrativistas, RESEX) in Amazonien mit einer Gesamtfläche von 117.720 km². Davon ermutigt stellten bald auch andere traditionelle Gemeinschaften wie beispielsweise die Amazonas-Flussanwohner und die Babaçu-Sammlerinnen ähnliche Forderungen, die ebenfalls erfolgreich waren. 2004 wurde mit der „Nationalen Kommission für traditionelle Völker und Gemeinschaften“ erstmals eine Vertretung eingerichtet, die nicht nur indigenen Völkern nutzen sollte. 2007 wurde schließlich das rechtlich bindende „Dekret für Traditionelle Völker und Gemeinschaften“ (Decreto 6040) vom Präsidenten der Republik unterzeichnet. Darin wird neben der Festschreibung der traditionellen Rechte explizit auf eine nachhaltige Entwicklung und Ökonomie hingewiesen, ohne die die langfristige Existenz dieser Gruppen kaum vorstellbar ist. Im Gegensatz zu den von der Verfassung garantierten Landrechten der Indigenen und Quilombolas enthält das Dekret allerdings keine Verpflichtung zur Ausweisung konkreter Gebiete. Zweifellos hat sich die Rechtsposition der lokalen Gemeinschaften seit den 1980er Jahren deutlich verbessert. Da die Entwicklungspolitik Brasiliens derzeit jedoch nach wie vor auf die Ausbeutung der Naturressourcen setzt und die Zerstörung der Ökosysteme und der destruktive Kulturwandel weiterhin dramatisch fortschreiten, ist gerade die Sicherung der Territorien der entscheidende Punkt für den langfristigen Fortbestand der lokalen Kulturen.

Brasilien ist das einzige portugiesischsprachige Land Amerikas. Das brasilianische Portugiesisch hat einen eigenen Charakter. Es unterscheidet sich in der Aussprache und durch eine leicht abgewandelte Orthographie und Grammatik von der europäischen Variante. Das (brasilianische) Portugiesisch ist alleinige Amtssprache und für mindestens 97 % der Bevölkerung Muttersprache.

Die Indianersprachen werden nur noch von etwa 0,1 % der Bevölkerung gesprochen, dazu zählen Guaraní, Makú, Tupi und Gês, wobei die letzten beiden vorrangig im Amazonasgebiet verbreitet sind, wo der Einfluss der Europäer gering blieb. In den Küstengegenden sind die Indianersprachen praktisch vollständig verdrängt worden. Guaraní hatte zu Kolonialzeiten eine größere Bedeutung und ist nur knapp daran gescheitert, Amtssprache des Landes zu werden. Insgesamt werden in Brasilien 188 verschiedene Sprachen und Idiome gesprochen.

Aufgrund der Einwanderung gibt es in Brasilien zahlreiche Minderheitensprachen.

Bis zu 1,5 Millionen Brasilianer sprechen Deutsch als Muttersprache. Damit ist Deutsch die zweithäufigste Muttersprache des Landes. Nachfahren der Auswanderer aus Pommern beherrschen zuweilen das Ostpommersche (Niederdeutsch) wesentlich besser, während ihr Hochdeutsch kein muttersprachliches Niveau erreicht. Eine besonders starke pommersche Minderheit lebt im Bundesstaat Espírito Santo.

Weiterhin sprechen etwa 500.000 Menschen Italienisch, 380.000 Japanisch und 37.000 Koreanisch.

Dabei muss berücksichtigt werden, dass bei den Sprachminderheiten die Zahl der Sprecher sehr optimistisch berechnet ist. Diese Volksgruppen gehörten teilweise zu den ersten Siedlern, und ihre Nachfahren verstehen fast nur noch Portugiesisch. In den Ortschaften, die als Zentren für Einwanderer galten, entstanden oftmals brasilianische Dialekte der Einwanderersprache. Beispiele sind Talian, brasilianisches Italienisch, und das Riograndenser Hunsrückisch, brasilianisches Deutsch. Bis ins 20. Jahrhundert hinein gab es (besonders im Süden) ganze Gemeinden, in denen ausschließlich Deutsch oder Italienisch gesprochen wurde, da insbesondere die deutschen Auswanderer und deren Nachfahren über eine gute Infrastruktur aus Schulen, Vereinen u. Ä. verfügten und zumeist in relativ geschlossenen Kolonien lebten. Als während des autoritären Regimes des Estado Novo (1937–1945) eine Nationalisierungskampagne betrieben wurde, geriet die deutsche Gemeinschaft zunehmend unter Druck, da der Staat den Assimilierungsprozess forcierte. Der Eintritt Brasiliens in den Zweiten Weltkrieg bot den entsprechenden Anlass, um die Sprachen der Feindstaaten zu verbieten und deutsche und italienische Schulen zu schließen, woraufhin das Portugiesische auch in diesen Ortschaften Einzug hielt.

Sortiert nach Bundesstaaten:






Santa Catarina

Englisch ist als Fremdsprache nicht so etabliert wie in europäischen Ländern. Obwohl Englisch normalerweise in den Schulen unterrichtet wird, fasst die Sprache nur langsam Fuß in Brasilien. Auch in den Großstädten ist es nicht selbstverständlich, dass die Leute Englisch sprechen oder verstehen. Für gewöhnlich verstehen die Brasilianer aber zumindest ansatzweise Spanisch, auch wenn sie die Sprache selbst nicht sprechen. Als Folge der verstärkten wirtschaftlichen Zusammenarbeit der lateinamerikanischen Länder im Mercosul wird die Bedeutung des Spanischen gegenüber dem Englischen noch zunehmen. In den Grenzgebieten zu anderen südamerikanischen Ländern bildete sich das sogenannte Portunhol heraus, eine Mischsprache aus Portugiesisch und Spanisch, die die Verständigung erleichtert. Besonders im Grenzgebiet zu Paraguay ist diese Mischsprache häufig anzutreffen, dies vor allem deshalb, weil die Grenzstadt Ciudad del Este ein wichtiger Handelsplatz für die brasilianischen Straßenhändler („Sacoleiros“) ist.

Laut dem Zensus des Jahres 2010 bekennen sich 64,6 % der Bevölkerung zur römisch-katholischen Kirche. Dieser Anteil schrumpft seit Jahren immer weiter: Lag er 1960 noch bei 91 %, nahm er bis 1985 auf 83 % ab und betrug im Jahr 2000 nur noch 73,6 %. Teile des brasilianischen Katholizismus sind stark von afrobrasilianischen Traditionen beeinflusst.

22,2 % der Bevölkerung sind Protestanten. Diese Konfession kam seit dem 19. Jahrhundert mit deutschen Einwanderern ins Land. Im 20. Jahrhundert haben aber vor allem nordamerikanische Missionskirchen Erfolge erzielt. So gab es seit etwa 1960 eine Zunahme protestantischer Sekten und Freikirchen. Heute gibt es 35.000 Freikirchen in Brasilien. 2,0 % sind Anhänger des Spiritismus, 0,3 % bekannten sich zu afro-brasilianischen Religionen wie Candomblé und Umbanda. Bemerkenswert ist der laut einer Untersuchung von 2006 hohe Anteil von Anhängern oder Sympathisanten der Pfingstbewegung (15 Prozent), was demnach prozentual der dritthöchste der Welt in einem Staat ist. Zu beachten ist dabei allerdings die beträchtliche Unschärfe der Zuordnung; ein Großteil der Zuordnung der Anhänger der Pfingstbewegung dürfte sich mit der allgemeineren Zuordnung Protestanten überschneiden.
Des Weiteren gibt es knapp 1.400.000 Zeugen Jehovas, etwa 225.000 Mormonen, 245.000 Buddhisten, meist Nachkommen japanischer Einwanderer, 107.000 Juden, über 35.000 Muslime, meist Nachkommen syrisch-libanesischer Einwanderer, und mehr als 5500 Hindus. 8,0 % erklärten, keiner Religion anzugehören.

Im Jahr 2000 zählte man etwa 17.100 Anhänger indigener südamerikanischer Religionen; das sind 0,01 % der Brasilianer und rund 4,1 % der indigenen Bevölkerung; Tendenz: stark rückläufig. Aggressive Missionstätigkeiten – trotz Verbot der Zwangsmissionierung – führen allerdings nicht nur zum christlichen Glauben, sondern gleichsam zu einem erheblichen Kulturwandel, der mit einer Zerstörung der traditionellen Weltanschauungen der Menschen einhergeht (u. a. Moralvorstellungen, Verhältnis zur Umwelt, überliefertes Wissen, Sozialstrukturen). Überdies setzen sich viele Missionare über die geltenden Quarantänevorschriften hinweg, so dass viele Indigene an eingeschleppten Krankheiten sterben. Vielfach kam es jedoch zu synkretistischen Vermischungen ethnischer- und christlicher Religion(en) und es ist davon auszugehen, dass eine große Zahl Indigener sich nur nach außen zum Christentum bekennt.

Brasilien weist eine stark ungleiche Verteilung der Vermögen auf. Der Gini-Koeffizient betrug im Jahr 2000 0,78 (0 bedeutet vollständige Gleichverteilung, 1 bedeutet, alles Vermögen gehört einem Haushalt). Dies steht im Zusammenhang mit der ungleichen Landverteilung.
So waren bis 1998 2,8 % der Bauern Großgrundbesitzer mit zusammen 57 % der Agrarfläche, wohingegen 90 % der Bauern sich 22 % der Fläche teilen mussten. Etwa fünf Millionen Familien gelten als landlos. Laut einer Studie beträgt der durchschnittliche Vermögensbesitz je erwachsene Person 17.485 US-Dollar. Im Median liegt er jedoch bei nur 4.591 US-Dollar (Weltdurchschnitt: 3.582 US-Dollar), was auf eine hohe Vermögensungleichheit hindeutet. Mehr als 70 % der brasilianischen Bevölkerung besitzt weniger als 10.000 US-Dollar an Vermögen.

Afro-Brasilianer, die sieben Prozent der Bevölkerung ausmachen, sind überproportional in der armen Bevölkerung vertreten. Nicht viel besser ergeht es den Indios. Ein Gleichstellungs- und Anti-Hunger-Programm gibt es seit 2003.

Die Alphabetisierungsrate des Landes lag 2015 bei 92,2 %, das Schulabgangsalter bei 16 Jahren. Die Schule zu besuchen ist Pflicht. In die Bildung fließt ein ähnlich großer Teil des Bruttosozialprodukts wie in Europa; in absoluten Zahlen ist das Bildungsbudget etwa so groß wie das deutsche (2004). In Brasilien teilt sich diese Summe jedoch auf eine mehr als doppelt so große und im Durchschnitt wesentlich jüngere Bevölkerung auf. Die staatlichen Schulen genießen einen schlechten Ruf. Deshalb schicken finanziell besser gestellte Eltern ihre Kinder auf private Schulen. Diese unterscheiden sich in der Höhe des Schulgeldes und der Qualität des Unterrichts erheblich. In den letzten PISA-Studien lag Brasilieni im unteren Viertel der teilnehmenden Staaten. Im PISA-Ranking von 2015 erreichen brasilianische Schüler Platz 66 von 72 Ländern in Mathematik, Platz 64 in Naturwissenschaften und Platz 60 beim Leseverständnis.

In 150 Universitäten werden fast 2,8 Millionen Studenten unterrichtet. Etwas mehr als die Hälfte der Hochschulen sind staatlich. Sie sind für alle Menschen mit qualifizierendem Schulabschluss nach einer Aufnahmeprüfung frei zugänglich und gebührenfrei. Die privaten Hochschulen finanzieren sich über unterschiedlich hohe Studiengebühren. Entsprechend schwankt ihre Ausstattung und die Qualität der Lehre. An den staatlichen Hochschulen werden zweimal jährlich einheitliche und offizielle Aufnahmeprüfungen, sogenannte "vestibulares", abgenommen. Die Bewerberzahl übersteigt meist bei weitem die Anzahl der vorhandenen Studienplätze. Bewerber bereiten sich deshalb nach dem Schulabschluss oft mit sogenannten "cursinhos" auf das "vestibular" vor, die von privaten Bildungseinrichtungen angeboten werden und dementsprechend kostenpflichtig sind. Wer im "vestibular" keinen Studienplatz erhält, hat die Möglichkeiten, bis zum nächsten Semester zu warten und das "vestibular" erneut zu absolvieren oder auf einer der privaten Hochschulen zu studieren.

Bekannt sind die Forschungen zur Nutzung regenerativer Energien, die zum Beispiel beim Bau des Wasserkraftwerks Itaipú (Vorbild des Dreischluchtendamms) Anwendung fanden. Auch der Motorenbau verdient Beachtung: Das erste Auto mit Alkoholmotor lief 1979 in Brasilien vom Band, und der Ingenieur Vincente Camargo entwickelte im Jahr 2005 den ersten Alkoholmotor (Methanol) für Flugzeuge, der von der Flugzeugbaufirma (Neiva-Embraer) als erstes erprobt wurde. Forschung zur Luftfahrt findet besondere Beachtung in Brasilien. Alberto Santos Dumont – nach dem der nationale Flughafen in Rio de Janeiro benannt ist – war um 1900 ein Erfinder und Luftfahrtpionier.

Da dem staatlichen Gesundheitswesen nur wenig Geld zur Verfügung steht, sind viele Krankenhäuser stark renovierungsbedürftig und veraltet. Obwohl nur 15 % der Ausgaben für Gesundheit in die Krankheitsprävention fließen, konnte die Säuglingssterblichkeit seit 1970 um zwei Drittel gesenkt werden. Ein Arzt betreut im Durchschnitt 633 Patienten, 87 % der Bevölkerung erhalten sauberes Trinkwasser. Die häufigsten Todesursachen sind Herzerkrankungen, Krebs, aber auch Unfälle und Gewalt.

Ab 2013 holte die brasilianische Regierung tausende ausländische Ärzte ins Land, vor allem Kubaner.

In Brasilien wird jeder im Krankenhaus oder beim Arzt behandelt, ohne eine Krankenversicherung zu besitzen. Dennoch haben viele eine Privatkrankenversicherung, die ihnen die Behandlung in privaten Häusern ermöglicht.

Anfang des Jahrzehnts verklagten Pharmakonzerne aus der ganzen Welt den Staat wegen Patentrechtsverletzungen. Dem zugrunde lag die Forderung der brasilianischen Regierung, die teuren ausländischen Medikamente zu verbilligen und somit auch den erkrankten Brasilianern zugänglich zu machen, die sich die entsprechenden Medikamente nicht leisten können. Da die Konzerne dieser Forderung nicht nachkamen, stellte Brasilien für über 100.000 der mittlerweile etwa 660.000 HIV-Infizierten kostenlose Medikamente für die antiretrovirale Therapie zur Verfügung. 2001 wurde die Klage jedoch fallengelassen. 2005 kam es zu einem ähnlichen Streit zwischen Brasilien und der US-Pharmaindustrie.

Quelle: UN

Die Kriminalitätsrate liegt weit über dem weltweiten Durchschnitt und die Mordrate gehört zu den höchsten der Welt. So starben gemäß einer Statistik von 2012 mindestens 56.337 Menschen durch Mord oder Totschlag. Dies entspricht einer Zahl von über 154 Tötungsdelikten pro Tag. Die Polizei hat vor allem in den Städten mit Morden, Entführungen, Raubüberfällen und organisierten Drogen- und Kriminellensyndikaten (wie etwa das Comando Vermelho in Rio de Janeiro und das Primeiro Comando da Capital in São Paulo) zu kämpfen. Das Polizistengehalt ist niedrig, deswegen gilt die Polizei als besonders korruptionsanfällig. Es ereignen sich zudem zahlreiche Fälle, in denen Polizeiangehörigen Machtmissbrauch bis hin zu Erpressung und Mord vorgeworfen wird. Auch innerhalb der Justiz ist Korruption weit verbreitet. Das Leben der Kleinbauern und Indios auf dem Land ist durch Konflikte mit Großgrundbesitzern und Unternehmen gefährdet, die nach Rohstoffen suchen.

Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Brasilien 2016 von 176 Ländern zusammen mit Indien, Volksrepublik China und Weißrussland auf dem 79. Platz, mit 40 von maximal 100 Punkten.

Um die hohe Zahl an Gewaltopfern zu verringern, wurde im Januar 2004 ein Gesetz vorgeschlagen, das den privaten Waffenbesitz verbieten sollte. Dieser Gesetzesvorschlag ist 2005 per Volksreferendum abgelehnt und deshalb ausgesetzt worden. Als einer der Gründe dafür wurde mangelndes Vertrauen in die Polizei genannt.

Laut einem Bericht der UNODC vom 7. Oktober 2011 lag die Mordrate bei 22,7 Delikten pro 100.000 Einwohner. Sao Paulo wird im Bericht als vorbildlich bei der Bekämpfung von Gewalt angeführt. Prävention, Projekte und Maßnahmen der Repression gegen kriminelle Organisationen waren demnach die Hauptursachen. Touristen sollten zudem bestimmte Bezirke in den Großstädten meiden.

Trotz als fortschrittlich geltender Gesetzgebung zur Gleichberechtigung Homosexueller ist die Anzahl der gewalttätigen Übergriffe auf Lesben und Schwule im internationalen Vergleich sehr hoch. Dies wird auf der jährlichen "Parada do Orgulho GLBT de São Paulo", der weltweit größten Gay-Pride-Parade, thematisiert.

Alle Bundesstaaten verfügen über jeweils zwei Behörden, welche die hauptsächliche Polizeiarbeit übernehmen: die Militärische Polizei und die Zivile Polizei. Während erstere für die öffentliche Ordnung zuständig ist, wird zweite hauptsächlich zu zwecken der Strafverfolgung tätig. Zusätzliche verfügen einige Großstädte über eine städtische Polizei ("Guarda Municipal"). Die Força Nacional de Segurança setzt sich aus Mitgliedern der verschiedenen Staatpolizeien zusammen und kann im Krisenfalle von den Gouverneuren der Staaten zur Hilfe gerufen werden. Außerdem stellt die Força Nacional Feuerwehren und Rettungsdienste in einigen Regionen.

Auf Bundesebene übernimmt die Bundespolizei neben der allgemeinen Strafverfolgung die Grenzschutzaufgaben. Daneben unterstehen dem Bund jeweils eine eigene Polizei für Bundesstraßen und Schienenwegen.

Im Februar 2017 waren in 1.424 Strafanstalten 650.000 Menschen, davon ungefähr 40.000 Frauen, inhaftiert. Der Anteil lag bei 316 Gefangenen auf 100.000 Menschen. Im Jahr 2000 waren es noch 232.000 Gefangene. Ein Grund für die Zunahme liegt an der hohen Zahl (ca. 30 %) von Untersuchungshäftlingen, die noch nicht gerichtlich verurteilt sind. Die Schaffung von Haftplätzen konnte mit dem Bedarf in den letzten Jahren nicht mehr mithalten. Es fehlten 2017 mindestens 250.000 Plätze. Einige Gefängnisse werden durch private, gewinnstrebige Unternehmen geführt.

Viele Gefängnisse werden durch kriminelle Banden beherrscht. Vor allem das "Primeiro Comando da Capital" ist nicht nur in São Paulo, sondern in nahezu allen Gefängnissen in Brasilien präsent. Es kommt regelmäßig zu organisierten Aufständen oder Massakern unter rivalisierenden Bandenmitgliedern.

Die ältesten Spuren menschlichen Lebens wurden in der Caverna da Pedra Pintada im Bundesstaat Piauí gefunden. Die ältesten datierten Funde stammen aus der Zeit um 11.700 BP. Um 7580 BP wurde dort Keramik genutzt (Paituna-Phase). Ebenfalls zu den ältesten Kulturen zählt die Itaparica-Phase, am Abrigo do Sol in Mato Grosso do Sul fanden sich ähnlich alte Spuren aus der Zeit zwischen 11.500 und 6000 BP (Dourado-Tradition). Skelettfunde belegen, dass die Küstengebiete des heutigen Brasilien um 8000 v. Chr. bewohnt waren. Die Nutzung von Nussbäumen lässt sich bis 8500 v. Chr. in Amazonien zurückverfolgen, echte Landwirtschaft setzte zwischen 6000 und 2700 v. Chr. ein – hier ist noch Vieles unklar. Vielfach wurde durch Brand das Wachstum bestimmter Pflanzen, wie etwa Palmen gefördert, die Nahrung lieferten, ein Prozess, der spätestens im 4. oder 3. vorchristlichen Jahrtausend nachweisbar ist; hinzu kam Gartenbewirtschaftung und eine zunehmende Sesshaftigkeit vieler Gruppen. Im 2. Jahrhundert n. Chr. muss die Bodennutzung äußerst intensiv gewesen sein, worauf die sogenannte "Amazonian Dark Earth" hinweist.

Die frühen Bewohner veränderten durch Anpflanzung bestimmter Pflanzenarten sowie durch Bodenverbesserung das Ökosystem des Amazonasbeckens grundlegend. Auch ihre Ansiedlungen – etwa auf der riesigen Flussinsel Marajó – waren weit größer als lange angenommen. Darüber hinaus bauten viele Gruppen sogenannte Mounds, häufig Begräbnishügel, die an der Küste Brasiliens, wenn sie aus Muscheln bestanden, als "sambaquis" bezeichnet werden. Andere stellten zeremonielle Zentren oder Residenzen dar. Der Mound-Komplex von Ibibate im bolivianischen Amazonien umfasst 11 ha, auf Marajo fanden sich allein 40 Mounds.

In der Provinz Mato Grosso fanden sich zahlreiche geplante Orte, in denen Fischzucht und Landwirtschaft bis in die Zeit um 1500 betrieben wurden. Die bis zu 60 ha großen Städte waren durch ein Straßennetz miteinander verbunden – obwohl in den meisten Gebieten das Kanu das Fortbewegungsmittel war –, es fanden sich Dämme und künstliche Teiche. Wie an vielen Stellen Amerikas dürften die Menschen am Xingu Epidemien zum Opfer gefallen sein, vor allem den Pocken.

Schon 1494 beschlossen Portugal und Spanien die Aufteilung Südamerikas im Vertrag von Tordesillas. In diesem wurde unter Vermittlung von Papst Alexander VI. eine Trennung der Interessenssphären festgeschrieben, so dass die gesamte Westküste spanische, und die (zu diesem Zeitpunkt noch allgemein unbekannten) Küstenabschnitte des heutigen Brasiliens portugiesische Kolonie würden. Voraussetzung für eine legitime Herrschaft war dabei die konsequente Katholisierung der Einheimischen. Am 22. April 1500 landete der portugiesische Seefahrer Pedro Álvares Cabral beim heutigen Porto Seguro (im Süden des Bundesstaates Bahia) und nahm das Land für die portugiesische Krone in Besitz. Die Zeit von 1500 bis 1530 war von Tauschhandel mit den Einheimischen geprägt. Um jedoch den Franzosen, die den Vertrag von Tordesillas als nicht bindend betrachteten, und die mit den Tupinambá Tauschgeschäfte für Rotholz machten, Einhalt zu gebieten, beschloss die portugiesische Krone, europäische Siedler nach Brasilien zu schicken.

1549 wurde das heutige Salvador da Bahia ("São Salvador da Bahía de Todos os Santos") zur Hauptstadt ernannt. Ab 1530 wurden Indios aus dem Landesinnern an die Küste gebracht, die die Arbeit auf den Zuckerrohrplantagen im Nordosten verrichten mussten. Wegen harter Arbeit, Verfolgung und Anfälligkeit der Indios für europäische Krankheiten starben viele von ihnen. Die Kolonialherren versuchten daraufhin, die verlorengegangene Arbeitskraft durch Sklaven aus Afrika zu ersetzen. Die Afrikaner wurden nach ihrer Verschleppung zwangsweise getauft, behielten jedoch faktisch ihre traditionellen Religionen bei. Dies war die Ursache für die Entstehung der typisch brasilianischen synkretistischen Kulte Candomblé und Umbanda. Bis 1580 brachten die Portugiesen das ganze Land faktisch unter ihre Kontrolle.

1629 hatten sich die Niederländer in der Nähe des heutigen Recife niedergelassen und 1637 unter Führung von Johann Moritz von Nassau-Siegen diese Anbaugebiete erobert, die daraufhin nochmals kurz aufblühten. Bis 1654 stand der Nordosten, v. a. das Gebiet um Pernambuco, unter niederländischer Kontrolle. In der Schlacht von Guararapes wurden die niederländischen Truppen im selben Jahr entscheidend geschlagen und wieder vertrieben.

Reiche Barockstädte entwickelten sich im 17. Jahrhundert, als Bandeirantes-Expeditionen das Hinterland erkundeten und neben anderen Bodenschätzen auch Gold und Diamanten entdeckten. Im selben Jahrhundert bauten entflohene Sklaven einfache Siedlungen, sogenannte Quilombos, auf. Als in den Quilombos Aufstände gegen die Unterdrückung der Schwarzen ausbrachen, zerstörte man bis 1699 alle Siedlungen wieder. 1763 wurde Rio de Janeiro zur Hauptstadt ernannt, weil sich das wirtschaftliche Zentrum des Landes auf den Süden verlagerte. 25 Jahre später führte der Offizier und Zahnarzt Tiradentes einen Aufstand an, der aber scheiterte. 1792 wurde der heutige Nationalheld Brasiliens hingerichtet. Gleichzeitig begann ein Konflikt mit Spanien, weil die Bandeirantes-Expeditionen die Westgrenze Brasiliens entgegen den Vereinbarungen verschoben.

1807 fielen französische Truppen von Napoleon Bonaparte nach Portugal ein, woraufhin der portugiesische König João VI. unter britischem Schutz nach Brasilien (erst Bahia, später Rio de Janeiro) flüchtete und dort erstmals den bis dahin strikt verbotenen Auslandshandel erlaubte. Mit der Übersiedlung des Königs und des gesamten Hofstaates bekam Brasilien den Status eines gleichberechtigten Mitglieds des Mutterlandes, und die Hauptstadt Rio de Janeiro war faktisch das Zentrum des damaligen portugiesischen Weltreichs mit Ausnahme des französisch besetzten Portugals. Auf dem Wiener Kongress 1815 wurde Brasilien mit Portugal gleichgestellt.

Nach Abzug der französischen Truppen aus Portugal musste König João VI. 1821 gegen seinen Willen wieder nach Portugal zurückkehren, um seinen Thronanspruch zu sichern. Er überließ die Herrschaft über Brasilien seinem Sohn Pedro. Pedro I. erklärte am 7. September 1822 in São Paulo die Unabhängigkeit Brasiliens von Portugal und machte sich am 22. September zum ersten brasilianischen Kaiser. Zwei Jahre später begann die gezielte deutsche Einwanderung in Brasilien mit Gründung der ersten Kolonie São Leopoldo in Rio Grande do Sul. 1828 löste sich die Provinz Uruguay, die man 1821 als Cisplatinische Provinz von Argentinien annektiert hatte, nach drei Jahren Krieg zwischen Brasilien und Argentinien und erklärte ihre Unabhängigkeit von Brasilien. Drei Jahre später kam es zu einem Militäraufstand, weswegen Kaiser Pedro I. abdankte und die Herrschaft auf seinen fünfjährigen Sohn Pedro II. übertrug. Der ehemalige Kaiser Pedro I. ging zurück nach Portugal und trat dort als portugiesischer König Pedro IV. das Erbe seines Vaters an.

Ein Zusatzpunkt der 1822 geschaffenen Verfassung ermöglichte noch am Tag der Abdankung Pedros I. einige Reformen. So wurde die Einsetzung eines einzigen Regenten beschlossen. In der Farrapen-Revolution 1835 spaltete sich mit Rio Grande do Sul erneut eine Provinz ab, die fortan die Republik Piratini bildete, bis sie nach einem zehnjährigen Krieg mit den Regierungstruppen wieder ins Kaiserreich eingegliedert wurde. In der Regentenzeit gab es eine Reihe von weiteren Aufständen im Norden und Nordosten, die relativ schnell niedergeschlagen wurden und vor allem viele Arme das Leben kosteten.

Im Jahre 1840, also noch vor seiner Volljährigkeit, wurde Pedro II. zum Kaiser gekrönt. 1864 erklärte Paraguay Brasilien den Krieg. Nach fünf Jahren besiegten Brasilien, Uruguay und Argentinien die Truppen Paraguays im blutigsten Krieg der lateinamerikanischen Geschichte. Obwohl die Kriegsjahre dem Land zusetzten, erlebte Brasilien aufgrund des Kautschukbooms eine gute wirtschaftliche Entwicklung. Brasilien besaß das Monopol auf Kautschuk und konnte deshalb durch dessen Export große Einnahmen erzielen.

Die Sklaverei wurde 1888 von Kronprinzessin Isabella, einer Tochter Pedros II., mit dem „Goldenen Gesetz“ (Lei Áurea) offiziell abgeschafft. Obwohl Sklaverei bereits seit 1853 geächtet worden war, führte das Verbot zu Aufständen von Großgrundbesitzern und der Armee. In der Folge putschte sich das Militär an die Macht, woraufhin der Kaiser am 15. November 1889 ins Pariser Exil ging und den Weg für die erste Republik freimachte.

Die erste brasilianische Republik mit föderativer Verfassung wurde am 24. Februar 1891 von Marschall Manuel Deodoro da Fonseca als "Vereinigte Staaten von Brasilien" ("República dos Estados Unidos do Brasil") ausgerufen. In der Folgezeit etablierte sich ein oligarchisches System. Der Wohlstand war durch die große Kaffee-Nachfrage gesichert und die Wirtschaft konzentrierte sich auf diesen Zweig. In den Ersten Weltkrieg trat Brasilien offiziell auf Seite der Alliierten gegen Deutschland ein, beteiligte sich aber nicht aktiv. In den Kriegsjahren ging die Nachfrage nach Kaffee stark zurück. In den 1920er Jahren forderten große Teile der Bevölkerung ein Ende der Oligarchie.

Als 1930 die Kaffeepreise nochmals einbrachen, führte Getúlio Vargas, der „Vater der Armen“, einen Aufstand an und wurde so Präsident. In den ersten Monaten seiner Regierungszeit wuchs die Wirtschaft Brasiliens spürbar. 1937 wurde die Herrschaft Vargas als „wohlwollender Diktator“ festgeschrieben, 1942 erklärte er auf Druck der USA den Krieg gegen die Achsenmächte. Er entsandte ein 25.000 Mann starkes Kontingent (Força Expedicionária Brasileira) nach Italien, das unter anderem in der Schlacht um Monte Cassino eingesetzt wurde. Nach dem Ende des Zweiten Weltkriegs wurde Vargas von der Armee abgesetzt.

Schon fünf Jahre später wählte ihn das Volk erneut zum Präsidenten. Weil sich die USA gegen die sozialistische Politik Brasiliens stellte und daraufhin Rechte und die Armee Vargas’ Rücktritt forderten, beging er 1954 Selbstmord. Vargas’ Nachfolger Juscelino Kubitschek sorgte mit Hilfe der Partido Trabalhista Brasileiro (PTB) für neue, ausländische Investoren, die die brasilianische Wirtschaft in den späten 1950er Jahren ankurbelten. 1960 wurde dann Jânio da Silva Quadros zum Präsidenten gewählt. Nach seinem Amtsantritt 1961 versuchte er, die Abhängigkeit von den USA zu lösen und den defizitären Staatshaushalt zu konsolidieren. Nach nur wenigen Monaten im Amt trat er wieder zurück, sein Nachfolger wurde der Vize-Präsident João Goulart, kurz nachdem die neue Hauptstadt Brasília nach drei Jahren Bauzeit eingeweiht worden war. Auch Goulart war in der Bevölkerung nicht unumstritten, weshalb seine Befugnisse in den ersten drei Präsidentschaftswahlen eingeschränkt waren.

Im Jahre 1964 putschte das Militär und setzte João Goulart ab. Das neue Regime unter Marschall Humberto Castelo Branco unterdrückte die linke Opposition und entzog etwa 300 Personen die politischen Rechte. Ein 1965 verabschiedetes Gesetz schränkte die bürgerlichen Freiheiten ein, sprach der Nationalregierung weitere Machtbefugnisse zu und bestimmte die Wahl des Präsidenten und Vizepräsidenten durch den Kongress.

Der ehemalige Kriegsminister Marschall Artur da Costa e Silva, Kandidat der Regierungspartei ARENA (Aliança Renovadora Nacional; deutsch: Allianz zur nationalen Erneuerung) wurde 1966 zum Präsidenten gewählt. Die Brasilianische Demokratische Bewegung (MDB, Movimento Democrático Brasileiro), die einzige legale Oppositionspartei, weigerte sich aus Protest, einen Kandidaten für die Wahl aufzustellen, weil die Regierung alle ernst zu nehmenden Gegenkandidaten nicht zugelassen hatte. 1966 gewann die ARENA auch die National- und Parlamentswahlen. Das Jahr 1968 stand im Zeichen von Studentenunruhen und Streiks. Das Militärregime reagierte mit politischen Säuberungsaktionen und Zensur. Im August 1969 wurde Costa entmachtet. Das Militär bestimmte General Emílio Garrastazu Médici zu seinem Nachfolger, der Kongress wählte ihn zum Präsidenten. Unter Médici wurden die Repressionen verstärkt und in der Folge nahmen die revolutionären Aktivitäten zu. Der römisch-katholische Klerus erhob seine kritische Stimme immer öfter und prangerte die Bedingungen der armen Bevölkerung an.

1974 wurde General Ernesto Geisel, nach seiner Militärkarriere Präsident der Petrobras, der staatlichen Ölmonopolgesellschaft, zum brasilianischen Präsidenten gewählt. Aufgrund der relativen politischen Stabilität und gezielter Förderung der Industrie war die Zeit der Militärmachthaber zugleich eine Zeit des Wirtschaftsbooms; viele Investoren – auch aus Deutschland – haben in den 1970er Jahren in Brasilien investiert. So avancierte São Paulo zur „größten deutschen Industriestadt außerhalb Deutschlands“ dieser Zeit.

Anfang der 80er Jahre schwächte die Militärregierung die Repression deutlich ab, bis schließlich 1985, auch aus Mangel an eigenen Optionen aus dem Militärkader und bereits inmitten einer Wirtschaftskrise mit galoppierender Inflation, freie Wahlen zugelassen wurden.

Der Wahlsieger Tancredo Neves wurde kurz vor seiner Amtseinsetzung in Brasília ins Krankenhaus eingeliefert. Wegen eines Magengeschwürs wurde er sieben Mal operiert. Er starb am 21. April 1985 an Infektionen, die er sich bei der Operation zugezogen hatte. Präsident wurde dann der zum Vizepräsidenten gewählte José Sarney. Sarney hatte mit enormen Auslandsschulden, Hyperinflation und Korruption zu kämpfen, was er mit dem „Plano Cruzado“ zuerst recht erfolgreich versuchte. Darüber hinaus musste er die neue Demokratie stabilisieren.

In demokratischen Wahlen wurde 1990 Fernando Collor de Mello zum Nachfolger Sarneys gewählt. Die ersten Monate seiner Amtszeit verbrachte er mit der Bekämpfung der Inflation, die zeitweise 25 % monatlich erreichte. Am 26. April 1991 wurde Mercosur (portugiesisch "Mercosul") gegründet. Dieser "Gemeinsame Markt des Südens", den die Staaten Argentinien, Paraguay und Uruguay gemeinsam mit Brasilien gründeten, ist ein Binnenmarkt mit mehr als 230 Millionen Einwohnern, der die Wirtschaft der Mitgliedsländer und dadurch die Stellung Lateinamerikas in der Welt stärken sollte.

Im Jahr 1992 wurde Collor von seinem Bruder Pedro der Korruption bezichtigt, was zu Untersuchungen durch Kongress und Presse führte. Die sich verdichtenden Hinweise auf Bestechlichkeit und Veruntreuung von Staatsmitteln gaben den Anstoß zu Massendemonstrationen und Unruhen in den großen Städten Brasiliens. Im Oktober des gleichen Jahres stimmte der Kongress für Collors Absetzung, der daraufhin zurücktrat. Verfassungsgemäß wurde Vizepräsident Itamar Franco sein Nachfolger.
Im Jahre 1993 konnte die Bevölkerung Brasiliens in einem Referendum sowohl über die Staats- als auch über die Regierungsform entscheiden. Die Wahl fiel dabei eindeutig auf Republik (statt Monarchie) mit präsidialem (statt parlamentarischem) Regierungssystem. 1994 wurde eine umfassende Währungsreform beschlossen, wodurch die Hyperinflation beendet werden konnte. Hauptverantwortlich für die Einführung der neuen Währung sowie einer Reihe weiterer Maßnahmen (insgesamt als „Plano Real“ bezeichnet) war Fernando Henrique Cardoso, der diesen Erfolg bei seiner Präsidentschaftskandidatur nutzen konnte und im Oktober 1994 sowie ein weiteres Mal im Oktober 1998 zum Präsidenten gewählt wurde. Zur Sanierung des Haushalts beschloss das Parlament die Privatisierung von Staatsmonopolen, dennoch stieg die Staatsverschuldung unter der Präsidentschaft von Cardoso von 28,1 % auf 55,5 % des Bruttoinlandsprodukts an. Von 2003 bis 2011 war Luiz Inácio Lula da Silva von der Arbeiterpartei PT Präsident Brasiliens. Er legte Wert auf die Verringerung der Staatsverschuldung, setzte aber auch soziale Programme wie Fome Zero ("Null Hunger") und "Bolsa Família" ("Geldbörse für Familien") um. 2004 führte Brasilien erstmals in seiner Geschichte UN-Friedenstruppen an, das Militär entsandte 1.470 Soldaten nach Haiti.

Im Jahre 2011 wurde Dilma Rousseff als erste Frau zum Staatsoberhaupt Brasiliens gewählt. Trotz ihres umstrittenen, harten Regierungsstils, der sich sehr von dem ihres Mentors Lula abhebt, betrugen im März 2012 ihre Zustimmungswerte 72 Prozent, im März 2013 waren sie auf 79 Prozent angestiegen. Mitte Juni begann jedoch eine Gruppe von jungen Menschen, welche die Fahrpreiserhöhungen bei öffentlichen Transportmitteln in São Paulo ablehnte, zu protestieren. Die gewaltvolle Repression, mit der die Polizei auf die Demonstrierenden reagierte, löste eine Kette von landesweiten Protesten hervor: In den folgenden Wochen gingen die Menschen zu Hunderttausenden auf die Straße. Gekämpft wurde zusätzlich gegen die Austragung der Fußballweltmeisterschaft 2014, Korruption, Missachtung von Menschenrechten, Angriffe auf soziale Rechte sowie auf die Rechte von Frauen, Homosexuellen und Indigenen. Ein Großteil der Kritik richtete sich an die Regierung und ihre zu wenig soziale Politik. Präsidentin Rousseff reagierte darauf mit dem Versprechen eines „großen Pakts“ für ein besseres Brasilien. Von Juni auf Juli sanken die Zustimmungswerte von Präsidentin Rousseff auf 31 Prozent ab.

Die tiefe Vertrauenskrise in das politische System wurde mit der Absetzung Rousseffs im Jahr 2016 nicht behoben, da diese an einem System scheiterte, das "älter ist als die Demokratie", jedoch auch von der Arbeiterpartei unter ihrer und der Präsidentschaft ihres Vorgängers Lula gepflegt anstatt verändert worden war. Roussefs Nachfolger Temer verlor innert eines halben Jahres sechs seiner Minister wegen Korruptionsvorwürfen, während das Land schon das zweite Jahr in Folge in einer Rezession steckte. Im Mai 2017 ermittelte das Oberste Gericht auch gegen den Präsidenten Temer. Nicht nur die staatliche Erdölfirma Petrobras, sondern damit auch der Baukonzern Odebrecht und der weltgrösste Fleischhändler JBS waren in die Korruption verwickelt.

Brasilien ist eine präsidiale Bundesrepublik. Sie besteht aus Bund, Bundesstaaten und Kommunen. Die gesetzgebende Gewalt im Bund wird vom Nationalkongress ausgeübt (Abgeordnetenkammer und Senat). Die 513 Abgeordneten werden für 4 Jahre, die 81 Senatoren für 8 Jahre gewählt. Die Verfassung wurde am 5. Oktober 1988 beschlossen und seither mehrfach ergänzt.

Der Präsident wird mit einer absoluten Mehrheit der Stimmen für die Dauer von vier Jahren direkt vom Volk gewählt. Er kann im Anschluss daran nur einmal wiedergewählt werden (oder erneut nach Unterbrechung). Die Bundesregierung besteht aus dem Staatsoberhaupt (zugleich Regierungschef), dem Vizepräsidenten sowie den derzeit 26 Bundesministern. Die nächsten Präsidentschafts-, Gouverneurs- und Parlamentswahlen finden Ende 2018 statt.

Brasilien gliedert sich in 26 Bundesstaaten sowie den Bundesdistrikt mit der Hauptstadt Brasília. Die Bundesstaaten besitzen eigene Verfassungen und Gesetze, die den Grundsätzen der Bundesverfassung entsprechen müssen. Die Regierungschefs der Bundesstaaten, die Gouverneure, werden für 4 Jahre direkt gewählt. 

Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt Brasilien Platz 51 von 167 Ländern und gehört damit gilt damit als eine „fehlerhaften Demokratie“. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „frei“ bewertet. Nur Parteimitglieder können gewählt werden, eine Parteigründung erfordert unter anderem der Beibringung von mindestens 500.000 Unterschriften aus mindestens einem Drittel aller Bundesstaaten. 

Brasilien wurde 1964 bis 1985 vom Militär regiert. In dieser Zeit litten vor allem die Indios unter Menschenrechtsverletzungen, die Wirtschaft wurde zwar unterstützt, gleichzeitig wurden jedoch große Prestigeprojekte (Transamazônica, das Wasserkraftwerk Itaipú, das Kernkraftwerk Angra dos Reis, Autobahnen) angestoßen. Folge dieser Politik waren eine hohe öffentliche Verschuldung und unrentable Staatsbetriebe. Ab 1985 folgte die "Nova República" (Sechste Republik).

Die Verfassung aus dem Jahr 1988 gewährt der Bundesregierung weitgehende Befugnisse. Der Präsident wird für eine Amtsperiode von vier Jahren direkt vom Volk gewählt. Seit 1998 kann er einmal wiedergewählt werden. Er besitzt eine weitreichende exekutive Gewalt, ist Staatsoberhaupt und Regierungschef und stellt das Kabinett zusammen.

Nach einer Übergangsbestimmung wurde 1993 ein Referendum über die Staats- (Monarchie oder Republik) und Regierungsform (Präsidial- oder parlamentarisches System) abgehalten. Die Bevölkerung entschied mit jeweils großer Mehrheit (87 % bzw. 69 %) für die Republik und ein Präsidialsystem. Im vierten Versuch wurde Luiz Inácio Lula da Silva, genannt "Lula", 2002 zum Präsidenten gewählt.

Ein politisches Problem Brasiliens sind schwache Parteien ohne ideologisch begründete Programme. Diese bilden Koalitionen, die bisher nur kurz hielten, somit müssen Gesetze meist durch Absprachen verabschiedet werden. Viele kleine Parteien und Korruption (1992 wurde der damalige Präsident Fernando Collor de Mello aus diesem Grund des Amtes enthoben) führen zu einer politisch sehr instabilen Lage und zu einer nahezu zur Untätigkeit verdammten öffentlichen Verwaltung. Auch der vor allem beim einfachen Volk beliebte Ex-Präsident Lula da Silva musste sich mit seiner Parteiführung Korruptionsvorwürfen stellen, die nicht ausgeräumt wurden.

Bei der Präsidentschaftswahl 2014 wurde die seit 2011 amtierende Präsidentin Dilma Roussouf wiedergewählt. Gestiegene Lebenshaltungskosten und die sinkende Wirtschaftsleistung Brasiliens im Zuge fallender Rohstoffpreise führten auch 2015 und 2016 zu landesweiten Großdemonstrationen. Im März 2016 holte Rousseff ihren populären Vorgänger Lula da Silva als Kabinettschef in ihre Regierung zurück. Damit reagierte sie auf ein ihr drohendes Absetzungsverfahren, welches sie jedoch nicht mehr verhindern konnte und welches am 12. Mai 2016 zu ihrer Suspendierung und schließlich am 31. August 2016 zu ihrer Amtsenthebung führte. Kritisiert wurde, dass damit ein nur für schwere juristische Verstöße vorgesehenes Verfahren für einen politischen Meinungsschwenk genutzt wurde. Ihr Nachfolger im Präsidentenamt ist ihr vorheriger Vizepräsident Michel Temer von der Partido do Movimento Democrático Brasileiro (PMDB).

Das brasilianische Parlament, der Nationalkongress oder "Congresso Nacional", besteht aus zwei Kammern:


Nach den Wahlen von 2006 haben zwanzig Parteien den Einzug in die Abgeordnetenkammer geschafft, wobei zwei Drittel der Abgeordneten von fünf Parteien gestellt werden. Die stärkste Kraft ist die Partido do Movimento Democrático Brasileiro (PMDB) mit 91 vor der Partido dos Trabalhadores (PT) mit 82 Abgeordneten. Im Senat sind derzeit dreizehn Parteien vertreten. Die meisten Senatoren (jeweils 23 %) gehören der PMDB und den Democratas (ehemals PFL) an.

In der Innenpolitik Brasiliens spielen Parteien insgesamt eine weniger zentrale Rolle als zum Beispiel in Deutschland. Die Parteienlandschaft ist stark zersplittert.

Im Senat sind derzeit (Stand Mai 2013) folgende Parteien vertreten (in Klammern Anzahl der Senatoren):


Im Abgeordnetenhaus sind derzeit (Stand Februar 2011) 20 Parteien vertreten (in Klammern Anzahl der Sitze):


Wichtige Parteien des letzten Jahrhunderts, die mittlerweile aufgelöst sind:


Die Wahl 2002, die in einem klaren Sieg der linken Arbeiterpartei PT endete, hatte einen hohen Stellenwert für die Entwicklung der noch jungen Demokratie, denn erstmals wurde ein größerer Machtwechsel vollzogen. Im ersten Jahr der Regierung gelang eine wirtschaftliche Stabilisierung, der wieder einsetzenden Inflation und anderen Problemen wurde konsequent entgegengewirkt. Auch eine Rentenreform wurde gegen Protest aus den eigenen Reihen beschlossen. Der Kampf gegen die Armut wurde mit verschiedenen Programmen und durchwachsenem Erfolg angegangen.

Die schwerste Krise der Legislaturperiode durchlebte die Regierung Lulas im Sommer 2005. Der PTB, Koalitionspartei in der Regierung, wurde Korruption vorgeworfen, was deren Vorsitzender Roberto Jefferson massiv bestritt und ähnliche Vorwürfe gegen zwei andere Regierungsparteien richtete. Sie würden ein Monatsgeld erhalten und dann den Gesetzesvorschlägen kollektiv zustimmen. Finanziert werde das angeblich durch Spenden großer Unternehmen, die dafür Staatsaufträge bekommen hätten. Daraufhin nahmen die Polizei und Untersuchungsausschüsse des Kongresses Ermittlungen auf, die immer mehr finanzielle Nebengeschäfte der Politiker aufdecken konnten. Dutzende Politiker – auch Berater des Präsidenten und Minister der Regierungsparteien, insbesondere des sich bis dahin als „sauber“ präsentierenden PT – legten ihr Mandat im Kongress nieder. Auch wenn eine persönliche Verwicklung bisher nicht nachgewiesen werden konnte, litt das Ansehen des Präsidenten stark unter den Vorwürfen. Reformen zum Wahl- und Parteifinanzierungssystem wurden in Angriff genommen, aber noch nicht beschlossen.

Antiamerikanismus ist in einigen Bevölkerungsteilen vorhanden. Manche Brasilianer betrachten die US-Politik als „neoimperialistisch“ oder zumindest „hegemoniell“ und befürchten eine zu starke Einflussnahme der USA auf Lateinamerika. Lula setzte sich seinerseits für ein starkes Lateinamerika ein und ging auf vorsichtige Distanz zur amerikanischen Politik. In der bisherigen Außenpolitik wurde ein offener Streit mit den USA aber vermieden. Gleichzeitig distanzierte sich Lula jedoch vom sozialistischen/marxistischen Kurs des ehemaligen venezolanischen Präsidenten Hugo Chávez, obwohl in den folgenden Jahren die Wirtschaftsbeziehungen intensiviert wurden. Dilma Rousseff hat die Beziehungen zu Venezuela unter Maduro hingegen leicht abgeschwächt, mitunter wegen der weiter angespannten wirtschaftlichen, politischen und menschenrechtlichen Situation in Venezuela. Michel Temer bekräftigte nach der amerikanischen Präsidentschaftswahl 2016 die amerikanisch-brasilianischen Beziehungen und setzt auf eine Intensivierung der Wirtschaftskooperation.

Auch unter der Regierung Rousseffs nach Lula hat sich die innenpolitische Lage mit Hinsicht auf die Wirtschaft und Sicherheitslage nicht signifikant verändert. Dies und der verspürte Stillstand im Land führten unter anderem zu sozialen Spannungen und Protesten im Vorfeld der Fußball-WM 2014 in Brasilien. Nachdem der Senat am 31. August 2016 endgültig nach anhaltenden Skandalen und starker oppositioneller Kritik für die Absetzung Rousseffs in einer Abstimmung befand, übernahm Michel Temer die Funktion des Staatschefs mit einer liberal-konservativen Regierung bis zur nächsten Wahl 2018. Nach seiner Amtsübernahme kündigte Temer Kürzungen, Entlassungen, Privatisierungen, eine Rentenreform und die Liberalisierung des Arbeitsmarkts an um der Rezession und schwierigen wirtschaftlichen Lage entgegenzuwirken, sowie um den Staatshaushalt zu entlasten. Ein stark wachsender Teil des Staatshaushaltes von über 10 Prozent wird allein für die Renten aufgewendet.

Während eines Polizeistreiks im Februar 2017 im Südosten Brasiliens im kleinen Bundesstaats Espirito Santo für höhere Löhne kam es zu mehr als hundert Morden.

2014 machte die Nationale Wahrheitskommission in ihrem Schlussbericht eine Vielzahl an Menschenrechtsverletzungen publik, die während der Militärdiktatur der Jahre 1964 bis 1985 begangen worden waren. Aufgrund des Amnestiegesetzes von 1979 gibt es keine juristische Aufarbeitung.

Die schwerwiegendsten derzeitigen Menschenrechtsverletzungen betreffen den Menschenhandel, wobei die sexuelle Ausbeutung von Kindern und Jugendlichen hervorgehoben wird, dann die exzessive Gewaltanwendung durch Polizei und Gefängnispersonal bis hin zu Folter und illegalen Hinrichtungen, die meist ungeahndet bleiben. Auch werden die Haftbedingungen als unzumutbar bezeichnet. Marginalisierte und Bewohner der Favelas sind am häufigsten Opfer dieser Gewaltanwendungen. In mehreren Bundesstaaten werden in einem Bericht Zwangsarbeit und Kinderarbeit konstatiert.

Indigene Völker und ethnische Minderheiten werden diskriminiert. Konflikte um Land führten zu zahlreichen Tötungen, Tausende wurden Opfer von Zwangsräumungen. Die Aufarbeitung von Menschenrechtsverletzungen verläuft schleppend oder versandet.

Brasilien als größtes Land Lateinamerikas (Fläche, Bevölkerung, Wirtschaft) nimmt eine regionale und globale Führungsrolle ein. Zu den wichtigsten Zielen und Schwerpunkten der brasilianischer Außenpolitik gehören:
Brasilien ist Mitglied u. a. folgender internationaler Organisationen:

Nach den Jahrzehnten der Militärdiktatur herrscht in Politik und Bevölkerung eine gewisse Vorsicht gegenüber den Streitkräften. Darüber hinaus sieht sich das Land keiner wirklichen äußeren Bedrohung gegenüber. Die lateinamerikanischen Staaten sind untereinander militärisch verbündet, was Sicherheit und Stabilität in der Region festigt.

Es besteht eine allgemeine Wehrpflicht für wehrfähige Männer über 18 Jahren. Der Etat des Verteidigungsministeriums lag nach einem Höhepunkt im Jahr 2014 mit 25 Milliarden im Jahr 2016 bei etwas über 23 Milliarden US-Dollar. und seit 2006 im Bereich von 1,3 bis 1,5 Prozent des BIP. Noch im Jahr 1992 waren die Ausgaben auf 8 Milliarden gefallen im Vergleich zu 14 Milliarden im Jahr 1988.

Mit etwa 190.000 Mann ist das Heer die bei weitem größte Teilstreitkraft Brasiliens. Mit etwa 500 Kampfpanzern und 1500 gepanzerten Fahrzeugen wäre das Land im Ernstfall kaum in der Lage, das weite und schwer zugängliche Hinterland zu sichern. In Friedenszeiten wird die Armee auch zum Katastrophenschutz und Rettungsdienst sowie für wissenschaftliche Dienste (auf der Antarktis-Forschungsstation Comandante Ferraz) eingesetzt. Zudem werden die Bundesstraßen vom Militär gebaut.
Innerstaatliche Bedrohungen, wie Kriminalität oder Terrorismus sind in Brasilien Sache der Polizeikräfte. Das Militär kann auf Anfrage des Gouverneurs im betroffenen Bundesstaat auch für polizeiliche Tätigkeiten eingesetzt werden, sofern der Notstand deklariert wird, so z. B. in der Stadt Rio de Janeiro 2008 und 2017.

Die Luftwaffe beschäftigte im Jahr 2005 73.500 Personen und ist damit die größte in Lateinamerika. Ihrer aufgrund der riesigen Landflächen und weiten Seegebiete hohen Bedeutung gemäß ist die Luftwaffe modern ausgestattet. Flugzeuge und Helikopter stammten zumeist aus den USA oder aus Europa, aber auch vom brasilianischen Flugzeugbauer Embraer, um das Militär unabhängig von ausländischen Importen zu machen. 
Auch die Marine ist modern und gut ausgerüstet. Durch das große Flusssystem, das sich bis weit ins Landesinnere erstreckt, ist die Marine auch im Inland einsetzbar. Sie besitzt daher viele Patrouillenboote und leichte Kampfschiffe, die die Binnengewässer sichern. In dieser Funktion unterstützt die Marine auch das brasilianische Heer und besitzt Amphibienfahrzeuge und sogar Kampfpanzer. Für den Einsatz auf hoher See stehen mehrere Kampfschiffe zur Verfügung sowie einige modifizierte U-Boote aus deutscher Fabrikation. Brasilien unterhält außerdem einen Flugzeugträger samt Ausstattung. Es handelt sich dabei aber eher um ein Prestigeobjekt, das die Vormachtstellung Brasiliens im lateinamerikanischen Raum unterstreichen soll.

Brasilien ist der fünftgrößte Waffenexporteur der Welt. Während der Militärdiktatur bestand ein langjähriges, geheimes Kernwaffenprojekt. Deutschland war Brasiliens wichtigster Partner auf dem Gebiet der (friedlich genutzten) Kernenergie und unterstützte das Land unter anderem mit der Lieferung von Kernreaktoren und Anlagen zur Uran-Anreicherung. Wie viel deutsches Wissen und Erfahrung tatsächlich in das Kernwaffenprogramm floss, und inwiefern die deutsche Regierung über das brasilianische Atomprojekt wusste, lässt sich allerdings nur schwer sagen. Wahrscheinlich gab es auch eine Kooperation mit Argentinien, das ebenfalls ein geheimes Atomprogramm unterhielt. In den 80er Jahren war das Kernwaffenprojekt bereits sehr weit entwickelt.

Mit dem Übergang in die Demokratie gab Brasilien schließlich das Vorhaben auf, Kernenergie für militärische Zwecke zu nutzen. Offiziell wurde das Atomwaffenprogramm mit der Unterzeichnung des Atomwaffensperrvertrags 1998 beendet.

Im Jahr 2004 übernahm das Land zum ersten Mal in seiner Geschichte eine größere Verantwortung und Rolle im Rahmen einer UN-Friedensmission in Haiti. 1.470 Soldaten waren im Karibikstaat stationiert, und im Juli 2004 übernahm Brasilien die Führung der internationalen Truppen bis zum Abzug im Jahr 2017.

Am 10. Juli 2007 gab Präsident Luiz Inacio Lula da Silva bekannt, das brasilianische Atomprogramm einschließlich der Anreicherung von Uran und dem etwaigen Bau eines Atom-U-Boots auszuweiten. Dafür sind im Haushalt bis 2015 insgesamt 1,040 Milliarden Real (rund 395 Millionen Euro) geplant.

Brasilien ist in 26 Bundesstaaten und einen Bundesdistrikt (Distrito Federal) gegliedert.
Diese sind in fünf Regionen aufgeteilt:



Während des brasilianischen Kaiserreichs war Rio de Janeiro Hauptstadt Brasiliens und hatte den Status "Município Neutro" (Neutrale Stadt), was in etwa einem Hauptstadtdistrikt gleichzusetzen ist. Mit der Schaffung des Bundesstaats und der einhergehenden Umwandlung der Provinzen in Bundesstaaten wurde 1889 aus dem Município Neutro ein "Distrito Federal" (Bundesdistrikt). 1960 wurde die Hauptstadt nach Brasília verlegt, ebenso der Distrito Federal. Der Sonderdistrikt um Rio de Janeiro war zeitweilig in den Bundesstaat Guanabara umgewandelt, bis Guanabara 1975 in den Bundesstaat Rio de Janeiro eingegliedert wurde.

Der Distrito Federal hat eine besondere Bedeutung. Er ist in der Verfassung festgeschrieben und ist direkt der brasilianischen Regierung unterstellt.

75 % der Bevölkerung Brasiliens leben in den Städten.

Die bevölkerungsreichsten Großräume (jeweils mit ihrer Hauptstadt) sind São Paulo mit etwa 21,4 Millionen Einwohnern (2017), Rio de Janeiro mit etwa 12,2 Millionen (2017), Belo Horizonte mit etwa 5,9 Millionen (2017), der Hauptstadtdistrikt Brasília mit etwa 4,4 Millionen (2017), Porto Alegre mit etwa 4,2 Millionen (2017), Salvador da Bahia mit etwa 4,0 Millionen (2017), Fortaleza und Recife mit jeweils etwa 3,9 Millionen (2017) und Curitiba mit etwa 3,5 Millionen Einwohnern.

São Paulo ist die größte Stadt Brasiliens, Südamerikas und gleichzeitig auch die größte der südlichen Hemisphäre und der wirtschaftliche Motor Brasiliens. São Paulo ist das größte deutsche Investitionszentrum außerhalb der EU und der USA. Als industrielles Zentrum des Landes zieht die Stadt kontinuierlich Einwanderer an, so dass sich die Einwohnerzahl innerhalb von 40 Jahren verdoppelte. Dieser rapide Bevölkerungszuwachs brachte der Stadt eine vorrangige Stellung in Bezug auf Finanzen, Kultur und Wissenschaft ein, aber auch Verkehrsprobleme, Umweltverschmutzung und Kriminalität.

Rio de Janeiro war fast 200 Jahre lang Hauptstadt Brasiliens, bis im Jahre 1960 Brasília zur Hauptstadt ernannt wurde. Dennoch ist Rio de Janeiro die bekannteste Stadt des Landes. Bei Touristen ist sie beliebt wegen des Karnevals und der Strände, die zu den schönsten der Welt zählen. Der Tourismus hat in Rio einen hohen wirtschaftlichen Stellenwert, aber auch produzierende Industrie ist in der Stadt beheimatet. Abseits der Urlaubszentren hat die Stadt mit den typischen Problemen einer Großstadt zu kämpfen, vorrangig mit Kriminalität und Armut großer Bevölkerungsteile.

Die Hauptstadt Brasília wurde in den 1960er Jahren innerhalb von drei Jahren erbaut. Sie ist eine klassische Planhauptstadt, wurde von Lúcio Costa im Auftrag des damaligen Präsidenten Kubitschek geplant, und Oscar Niemeyer entwarf die Regierungsgebäude. Brasília sollte ursprünglich als glänzendes städtisches Vorbild dienen. Allerdings ging die Entwicklung in wichtigen Punkten nicht so voran, wie es die Pläne vorsahen, und so ist Brasília in den äußeren Bezirken mittlerweile ebenfalls von Favelas geprägt. Heute hat die Stadt knapp 200.000 Einwohner, die Metropolregion zählt etwa 4,4 Millionen Menschen.

An der Spitze der brasilianischen Gerichtsbarkeit steht das Supremo Tribunal Federal mit Sitz in der Hauptstadt Brasília.

Mit einem Bruttoinlandsprodukt (BIP) von rd. 1800 Mrd. USD (2016) ist Brasilien die siebtgrößte Volkswirtschaft der Welt. Das Pro-Kopf-Einkommen betrug zur gleichen Zeit ca. 8.700 USD. Die wirtschaftliche Struktur Brasiliens ist gekennzeichnet durch die Kernsektoren Dienstleistungen mit ca. 65 %, Industrie mit 17 % und Agrarwirtschaft mit ca. 6,7 % BIP-Anteil („Agrarbusiness“/Produktion und Verarbeitung von Agrarrohstoffen insgesamt 25 % des BIP).

Hohe Wachstumsraten und solider Beschäftigungszuwachs erhöhten bis vor wenigen Jahren signifikant das globale wirtschaftspolitische Interesse an Brasilien. Dank der Explosion der weltweiten Rohstoffpreise, steigender Löhne und eines verbesserten Zugangs zu Verbraucherkrediten konnte das BIP kräftig expandieren.

Als sich vor wenigen Jahren jedoch das Ende des Wirtschaftsbooms angesichts sinkender Rohstoffpreise, steigender Verschuldung des Privatsektors und sehr niedriger Produktivität ankündigte, versuchte die Regierung, durch höhere Staatsausgaben und Subventionen das Wirtschaftswachstum künstlich hochzuhalten – mit dem Ergebnis eines dramatischen Haushaltslochs (Fiskaldefizit liegt bei ca. 10 %) und eines zunehmend erodierenden Vertrauens von Unternehmern, Investoren und Konsumenten. Brasilien befindet sich nun in einer schweren Rezession.

Nachdem das BIP 2015 um 3,8 % gesunken ist, dürfte es 2016 erneut deutlich geschrumpft sein (−3,4 %). Für 2017 wird mit einer leichten Erholung der Wirtschaftsleistung von rund 0,5 % gerechnet. Auch die Lage auf dem Arbeitsmarkt hat sich in den letzten zwei Jahren deutlich verschlechtert. Noch vor einem Jahr lag die Arbeitslosigkeit bei 8,6 % und ist mittlerweile bei über 12 %. Mit über 200 Mio. Einwohnern bleibt der starke Binnenmarkt mit über 80 % Anteil am BIP der Haupt-Konjunkturmotor. Die Außenwirtschaft spielt mit rund 20 %-Anteil am BIP eine vergleichsweise geringe Rolle. Eine besonders große Herausforderung für das Wirtschaftswachstum stellt die – auch im internationalen Vergleich – sehr niedrige und weiter sinkende Investitionsquote von deutlich unter 20 % des BIP dar Brasilien ist ein Gründungsmitglied der BRICS-Staaten. Die größten Probleme des Landes sind zum einen der Verfall der Rohstoffpreise, der Korruptionsskandal um das Staatsunternehmen Petrobras, die allgemein hohe Korruption im Land, geringe Produktivität der Unternehmen und die schlechte Infrastruktur. Im Jahr 2017 könnte Brasilien wieder zum Wachstum zurückkehren.

Die südamerikanische Zollunion Mercosul stärkt zwar den Markt in Lateinamerika, aber neben Brasilien haben auch andere lateinamerikanische Länder wirtschaftliche Probleme, wie z. B. Argentinien, Venezuela und Ecuador. Neben den lateinamerikanischen Staaten sind die Volksrepublik China, die USA und die Europäische Union die wichtigsten Handelspartner. Im Außenhandel hat die Volksrepublik China die USA im März 2009 als wichtigsten Handelspartner Brasiliens überholt.

Ein besonderer Wachstumsschub wurde von der Fußball-Weltmeisterschaft 2014 und den Olympischen Spielen 2016 erwartet, aber bei beiden Großveranstaltungen übersteigen die Kosten die Einnahmen bei weiten. Deshalb ist es im Vorfeld der WM und der Olympischen Spiele zu massiven Protesten gegen die Veranstaltungen gekommen.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Brasilien Platz 80 von 137 Ländern (Stand 2017–18). Im Index der Wirtschaftlichen Freiheit belegte Brasilien 2017 Platz 140 von 180 Ländern.

Hauptproblem bei der Ausschöpfung dieses ökonomischen Potentials sind allerdings die hohen Kosten („Custo Brasil“) im Land. Darunter fallen etwa Kosten durch die schlechte Logistikinfrastruktur, hohe Steuern, hohe Finanzierungskosten oder das hohe Lohnniveau verbunden mit Fachkräftemangel im Land. Laut Industrieverband CNI stiegen die Lohnkosten im Jahr 2012 mit 5,1 Prozent doppelt so stark wie die Umsätze der Unternehmen in Brasilien. Hohe Logistikkosten verbrennen 20 Prozent der Umsätze der Unternehmen. Der Custo Brasil bedeutet grundsätzlich hohe Steuern. Begünstigungen zur Förderung von Investitionen werden regional gewährt, vor allem im Hinterland. Ein weiteres Problem sind hohe Finanzierungskosten. Die Notenbank hat seit Mitte 2011 die Zinsen zwar deutlich gesenkt, was auch die Überbewertung der Landeswährung Real korrigiert hatte. Langfristige zinsgünstige Kredite mit einem Niveau von 5 Prozent p. a. vergibt lediglich die nationale Entwicklungsbank BNDES. Finanzierungskosten für ausländische Unternehmen in Brasilien sind höher als für nationale Unternehmen. Dem Custo Brasil wird seitens der Regierung nun der Kampf angesagt. Bis 2016 sind Investitionen in Infrastruktur und Logistik von knapp 70 Milliarden Euro geplant.

Die brasilianische Landwirtschaft hat eine große Bedeutung nicht nur für das Land selbst, sondern auch für den Rest der Welt. Theoretisch könnte Brasilien etwa eine Milliarde Menschen ernähren, weshalb es als Ernährer der Welt gilt. Durchschnittlich werden 40 % des Bruttoinlandsproduktes mit der Landwirtschaft und den mit ihr verwandten Industriezweigen erwirtschaftet und etwa 43 % aller Exporte sind landwirtschaftliche Güter. Insgesamt gibt es in Brasilien 248 Millionen Hektar landwirtschaftliche Nutzfläche, wozu jährlich etwa 2 Millionen Hektar Neuland hinzukommen. Speziell in Mittelbrasilien gibt es Betriebe, die Flächen von 100.000 Hektar oder mehr bewirtschaften. Sie haben Brasilien zum Kostenführer bei landwirtschaftlichen Massengütern wie Zucker, Sojabohnen, Mais, Kaffee, Orangensaft, Rindfleisch, Schweinefleisch und Geflügelfleisch werden lassen. Die Landwirtschaft Brasiliens hat ihr Potenzial dabei noch nicht ausgeschöpft, es gibt noch große Landreserven und auch durch Intensivierung der Landwirtschaft könnten die Erträge noch weiter gesteigert werden. Die Entwicklung der Landwirtschaft wird vor allem durch Mängel an der Infrastruktur des Landes, durch die Entfernung der Anbaugebiete zu den Exporthäfen für landwirtschaftliche Produkte und durch den hohen Kapitaleinsatz für die Düngung der Felder begrenzt.

An der brasilianischen Landwirtschaft wird kritisiert, dass sie riesige Mengen an Kunstdünger und Pestiziden einsetzt, dass Produkte für den Export in Monokulturen auf sehr großen Flächen angebaut werden und dass die Arbeitsverhältnisse für die Landarbeiter sehr schlecht sind. Zahlreiche Felder werden heute für den Anbau von Exportprodukten oder von Pflanzen für die Energiegewinnung verwendet, anstatt Nahrungsmittel für die lokale Bevölkerung darauf anzubauen. Des Weiteren sind die Eigentumsverhältnisse stark konzentriert: etwa 50 Unternehmen teils ausländische Unternehmen dominieren die Landwirtschaft Brasiliens und ihre vor- und nachgelagerten Industriesektoren, während 150.000 Landarbeiterfamilien kein Land besitzen.

Wichtige brasilianische Unternehmen sind Petrobras (Erdöl), Companhia Vale do Rio Doce (Bergbau), Gerdau (Metallverarbeitung), Embraer (Flugzeugbau), Organização Odebrecht (Baugewerbe) und BRF (Lebensmittel). Auch große ausländische Unternehmen wählten Brasilien zum Schwerpunkt ihrer südamerikanischen Aktivitäten, so der Volkswagen-Konzern (Volkswagen do Brasil), Nestlé, Parmalat, Anheuser-Busch InBev (AmBev) oder auch der Fiat-Konzern. Die Daimler AG (2016) und die Bayerischen Motorenwerke (2014) haben in Iracemápolis bzw. Araquari Pkw-Produktionsstätten eingerichtet.

Der Ölkonzern Petrobras ist eine Staatsfirma und zählt zu den größten Energieunternehmen der Welt. Seit 2014 wird der Konzern immer wieder vom bisher größten Korruptionsskandal in der Geschichte Brasiliens erschüttert. Der Konzern wird stark vom Einbruch des Erdölpreises in Mitleidenschaft gezogen, so wurde im Jahr 2015 ein Verlust von 8,6 Mrd. € angehäuft. Somit ist Petrobras einer der wenigen großen Energiekonzerne, der Verluste schreibt.

Der Bergbaukonzern Vale ist der größte Eisenerzproduzent der Welt. Neben Bergwerken und Verladehäfen gehört ihm auch ein Großteil des brasilianischen Bahnnetzes. 1997 wurde der Staatsbetrieb privatisiert. Indirekt hat allerdings die öffentliche Hand über staatliche Pensionsfonds und die Investitionsbank BNDES noch großen Einfluss. 2007 übernahm Vale, geführt von Roger Agnelli (1959/60–2016), den kanadischen Wettbewerber Inco, den größten Nickelproduzenten der Welt. Im Zuge des Verfalls der Rohstoffpreise, insbesondere von Eisen, kam auch Vale in massive Bedrängnis. Der Konzern wies im Jahr 2015 einen Verlust in Höhe von 13,2 Mrd. US-Dollar auf.

Auch der Flugzeughersteller Embraer hat einen staatlichen Hintergrund, gehört inzwischen aber mehrheitlich privaten Eignern. Unter Mauricio Botelho entkam der Konzern einer schweren Krise. Embraer produziert heute Regional- und Geschäftsreiseflugzeuge sowie militärisch umgerüstete Regional-Jets und militärische Trainer mit Turbopropantrieb. Da Boeing und Airbus nur Flugzeuge oberhalb der Größe von Embraer-Maschinen verkauft, sind Embraer Jets heutzutage ein fester Bestandteil der weltweiten Linienluftfahrt. Lufthansa CityLine oder auch Air Dolomiti fliegen mit E-195. Air France Régional und die Air France Tochter CityJet fliegen ERJ135.

Bis zum Ende des 19. Jahrhunderts lebte die Bevölkerung vor allem vom Export von Agrarprodukten. Dann gab es aufgrund der beginnenden Industrialisierung des Landes einen zunehmenden Mangel an Arbeitskräften, der sich nach Abschaffung der Sklaverei im Jahre 1888 noch weiter verschärft hatte. Dies lockte eine große Zahl von Einwanderern an, die größten Gruppen unter ihnen, neben Portugiesen und Spaniern, waren Deutsche, Italiener, Polen und Japaner.

Während des Ersten Weltkriegs geriet das Land in eine wirtschaftliche Krise, weil die wichtigsten Exportartikel (Kaffee, Zucker etc.) von einem enormen Preisverfall betroffen waren. Hilfe kam durch Kapital und Einwanderer aus Großbritannien. Mit Ausnahme des Ersten Weltkriegs wuchsen die Wirtschaft und das Verkehrsnetz in den ersten 30 Jahren des 20. Jahrhunderts stetig.

1917 kam es zu ersten großen Streikwellen in São Paulo und Rio de Janeiro, auf die die Regierung mit Unterdrückung reagierte. In den 1920er Jahren bildeten sich Arbeiterparteien und Gewerkschaften, doch dies brachte keine stärkere Stellung im Staat mit sich, da sie keine Vertretung in oberen Schichten hatten. Auch die Leutnantbewegung Tenentismo ab 1922 konnte daran nichts ändern, da Versuche einer Revolution scheiterten.

Ein aktuelles Problem der brasilianischen Wirtschaft ist die steigende Urbanisierung und Zuwanderung der Landbevölkerung in die Städte. Allein in Brasilia steigt sie pro Jahr um drei Prozent, was in den Armenvierteln katastrophale Auswirkungen hat.

Mit großen, gut entwickelten Landwirtschafts-, Bergbau-, Produktions- und Dienstleistungssektoren auf der einen Seite und einem großen Vorrat an Arbeitskräften auf der anderen ist die brasilianische Wirtschaft heute die kräftigste Südamerikas und gewinnt auf dem Weltmarkt an Bedeutung. Die wichtigsten Exportprodukte sind Kaffee, Kakao, tropische Früchte, Sojabohnen, Zucker und Eisenerz. 40 % der brasilianischen Agrarausfuhren gehen in die EU, 17 % in die USA.

Anfang 2011 betrug die Anbaufläche für Soja 24,08 Mio. Hektar (240 800 km²). Eine Steigerung von 611 000 Hektar im Vergleich zum Erntejahr 2009/2010.

Die Zuckerindustrie in Brasilien ist ein bedeutender Wirtschaftsfaktor des Landes. Mit einer Produktion von mehr als 500 Millionen Tonnen Zuckerrohr, die zu etwa gleichen Teilen zu Zucker und Bioethanol und zu einem kleinen Teil zu Zuckerrohrschnaps verarbeitet werden, ist die Zuckerindustrie Brasiliens mit Abstand die größte weltweit. Auf den meist von „Zuckerbaronen“ beherrschten Zuckerrohrplantagen herrschen äußerst schlechte Bedingungen. Menschen arbeiten teilweise in sklavenähnlichen Verhältnissen in riesigen Monokulturen.

Zu den größten Herausforderungen für die brasilianische Wirtschaft zählen nach wie vor die Inflation und die Kluft zwischen einer wohlhabenden, gut ausgebildeten Bevölkerungsminderheit und der schlecht ausgebildeten Mehrheit, die größtenteils am Rande des Existenzminimums lebt. Es gibt eine große Bewegung von Landlosen, die Movimento dos sem terra (MST), die für eine Landreform kämpfen.


Die wichtigen Wirtschaftskennzahlen Bruttoinlandsprodukt, Inflation, Haushaltssaldo und Außenhandel entwickelten sich in den letzten Jahren folgendermaßen:

Folgende Rohstoffe werden in Brasilien abgebaut: Eisen, Mangan, Kohle, Bauxit, Nickel, Erdöl, Zinn, Silber, Diamanten, Gold, Erdgas, Uran. Täglich werden 1,5 Millionen Barrel Erdöl gefördert, Uran ist im Landesinnern vorhanden, der Bauxit-Tagebau verschmutzt die Flüsse und gefährdet so die Umwelt. Brasilien ist der weltgrößte Lieferant für Eisen. Die Vorkommen sollen den Eisenbedarf der Erde für die nächsten 500 Jahre decken. Bei Tantal ist Brasilien zweitwichtigster Exporteur. Etwa 60 % aller verarbeiteten Edelsteine (ausgenommen Diamanten) stammen aus Brasilien. Auch hat Brasilien eine bedeutende Stahlproduktion, die allerdings durch die Eingriffe der USA geschmälert wurde. So durfte Brasilien nur Stahl minderer Qualität produzieren, welchen US-Unternehmen nicht verarbeiten wollten.
Der Tourismus ist in Brasilien noch nicht sehr bedeutend und macht nur etwa 0,5 % des Bruttosozialprodukts aus. Der weltweite Durchschnitt liegt bei 10 %. Die jährliche Besucherzahl liegt bei etwa 4,8 Millionen. Beliebt sind vor allem die Strände und der Karneval von Rio de Janeiro, die Hauptstadt Brasília, das Amazonasbecken, der Nordosten mit seinen Stränden und Kultur und die Iguazú-Wasserfälle. Die relativ geringe Anzahl an Touristen (auf einen Besucher kommen in Brasilien 37 Einheimische, in Deutschland nur etwa 4,6) ist auf verschiedene Faktoren zurückzuführen. Die Infrastruktur ist dem Tourismus wenig förderlich, In- und Auslandsflüge sind teuer, da es im ganzen Land nur wenige Charterflüge gibt. Der Nordosten, von Inlandtouristen häufig aufgesucht, wird langsam von Europäern entdeckt. Nicht nur die Natur und die reiche Kultur, sondern auch die Entfernungen spielen eine Rolle. Fortaleza liegt z. B. etwa sieben Flugstunden von Lissabon und etwa acht Flugstunden von Madrid entfernt. Es gibt mittlerweile direkte Flüge aus verschiedenen europäischen Städten nach Fortaleza, Natal, Recife und Salvador.

Der brasilianische Finanzmarkt ist zunehmend in das internationale Finanzsystem integriert. Den Mittelpunkt des brasilianischen Finanzmarktes bilden die internationalen und nationalen Banken und der Aktienmarkt. Letzterer zeichnet sich dabei durch hohe Transparenz (im Vergleich zu anderen BRICS-Staaten) und Teilnahme internationaler Akteure aus. Auch in Amerika und Europa werden brasilianische Firmen mittels ADRs gehandelt. Heutige Zentralbank Brasiliens ist die Banco Central do Brasil. Die frühere Zentralbank Banco do Brasil gab 1986 diese Funktion ab und ist mittlerweile die größte Bank Brasiliens. Größte Regionalbank ist daneben die Banco do Estado de São Paulo. Zu den größten Privatbanken Brasiliens gehören die Banco Bradesco, die Itaú Unibanco, die HSBC sowie die Banco Real. Die größten Banken sind dabei mittlerweile zumeist international tätig. Daneben existieren lokale Banken (Caixa), die den Bundesstaaten oder Bezirken zuzurechnen sind bzw. privatisiert wurden.

Mittlerweile sind auch viele der großen deutschen Banken wie die Deutsche Bank, Commerzbank, Landesbank Baden-Württemberg, WestLB und BHF-Bank in Brasilien vertreten.

Internationalem Kapital sind wenig Schranken gegeben. Der brasilianische Real kann frei gegen andere Währungen floaten, jedoch kann die Regierung mittels der Zentralbank durch sog. Open Market Aktionen Einfluss darauf ausüben.

Eine zunehmend wichtigere Rolle kommt lokalen Asset Managern wie Maua Investimentos zu, die zunehmend zu einer eigenständigen Entwicklung brasilianischer Hedgefonds und Private-Equity-Gesellschaften beitragen. Sie verringern so die Abhängigkeit von internationalen Managern und weiten den Derivatemarkt aus. Viele dieser brasilianischen Beteiligungsunternehmen haben auch Projekte in anderen lateinamerikanischen Ländern.

Eine wichtige Grundlage der weiteren Entwicklung liegt neben den politischen Rahmenbedingungen auch in der universitären Ausbildung. Einige Universitäten, wie etwa die PUC in Rio oder die USP in São Paulo sind stark mit lokalen Finanzakteuren vernetzt und verfügen über eine gute Reputation in Lateinamerika.

Ausländische Direktinvestitionen (ADI) erreichten 18,2 Milliarden US$ im Jahre 2004 und Brasilien stieg auf den siebten Platz in der von AT Kearney publizierten Liste der attraktivsten ADI-Länder.

Der Staatshaushalt umfasste 2015 Ausgaben von umgerechnet 641,2 Mrd. US-Dollar. Dem standen Einnahmen von umgerechnet 631 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 0,6 % des BIP. Die Staatsverschuldung betrug 2015 67,3 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:


Das Straßennetz Brasiliens ist mit etwa zwei Millionen km das zweitlängste der Welt, annähernd 200.000 km sind asphaltiert. Der brasilianische Name für Fernstraße ist "Rodovia". Annahmen zufolge nehmen jährlich mehr als 1,2 Milliarden Reisende den Weg über die Fernstraßen, nur 80 Millionen fliegen.

Die Straßen befinden sich allerdings oft in desaströsem Zustand, im Norden allgemein schlechter als im Süden. An allen größeren Überlandstraßen befinden sich deshalb auch "Borracharias" (Pannenstellen für Reifen) am Straßenrand. Busse verkehren zwischen allen größeren Städten in regelmäßigen Intervallen und auch zwischen kleineren Städten einigermaßen zuverlässig. Es gibt verschiedene Preisklassen vom einfachen Reisebus bis hin zum vollklimatisierten Bus mit Fernsehern und Reisebegleitern.

Es herrscht Rechtsverkehr. Die Bezeichnung der Fernstraßen enthält den Bundesstaat, in dem sie liegen, und die Richtung, in die sie verlaufen. Ein Sonderfall sind Fernstraßen, die nach Brasília führen:


So liegt zum Beispiel die Autobahn SP-280 im Bundesstaat São Paulo und verläuft von West nach Ost. Neben ihrem offiziellen Namen sind einige Straßenverbindungen auch noch nach berühmten Persönlichkeiten benannt.

Die Bahnverbindungen wurden ausgedünnt, aber es besteht noch ein Schienennetz von fast 30.000 km Länge. Zu Beginn des 20. Jahrhunderts war die Eisenbahn für den wirtschaftlichen Aufschwung besonders wichtig. Mit dem rasanten Ausbau des Straßennetzes verlor sie diese herausragende Stellung. Mittlerweile hat diese in Brasilien nur noch geringe bis gar keine Bedeutung mehr. Der Güterverkehr wird mit LKWs oder Schiffen abgewickelt; für den öffentlichen Personenfernverkehr werden normalerweise Busse verwendet. Auf Strecken durch die Berglandschaft verkehren noch nostalgische Züge, die als touristische Attraktionen dienen.

Wegen der sehr großen Entfernungen werden auch Flugreisen innerhalb Brasiliens immer wichtiger. Allerdings sind die Kosten für die meisten Brasilianer zu hoch, so dass sie auch lange Reisen mit dem Bus unternehmen. Es etablieren sich aber immer mehr Fluggesellschaften, die nach Vorbild europäischer Billigfluglinien erschwingliche Flüge innerhalb des Landes anbieten. Der größte Flughafen des Landes ist der Aeroporto Internacional de São Paulo/Guarulhos in Guarulhos bei São Paulo mit jährlich fast 40 Millionen Passagieren. Um die zwei überlasteten Flughäfen von São Paulo zu entlasten, ist in Campinas, in 80 km Entfernung von São Paulo, der Ausbau des dortigen Flughafens Viracopos zum größten Flughafen Lateinamerikas mit einer Kapazität von jährlich bis zu 55 Millionen Passagieren in Planung.

Die Binnenschifffahrtswege haben insgesamt eine Länge von rund 50.000 km. Die Handels- und Frachtflotte besteht aus etwa 475 Schiffen. Die größten brasilianischen Häfen liegen in Belém, Fortaleza, Ilhéus, Imbituba, Manaus, Paranaguá, Porto Alegre, Recife, Rio de Janeiro, Rio Grande, Salvador, Santos und Vitória. Liste der Seehäfen in Brasilien.

In Brasilien gab es 2005 39,7 Millionen Telefone, was einen Anstieg um 20 Millionen Anlagen im Vergleich zu 1997 bedeutet. Außerdem sind etwa 80 Millionen Mobiltelefone im Umlauf. Auch hier ist der Anstieg zu 1997 (4 Millionen Mobiltelefone) deutlich. Das Telefonsystem funktioniert gut. Ortsgespräche sind teilweise kostenlos. Es existieren drei Koaxial-Tiefsee-Kabel, national ist das Funk-Relais-System gut ausgebaut, auch das Satellitensystem funktioniert gut.

Die Stromerzeugung in Brasilien beruht weitgehend auf der Nutzung regenerativer Quellen, insbesondere auf der Wasserkraftnutzung, die im Jahr 2011 für rund 80 % der gesamten Stromproduktion verantwortlich war. Die übrigen erneuerbare Energien hatten einen Anteil von 6,6 %, fossile Energien lagen bei ca. 10 % und die Kernenergie bei knapp 3 %. Der ursprünglich vorgesehene Neubau von vier Kernkraftwerksblöcken wurde nach der Nuklearkatastrophe von Fukushima überdacht, stattdessen soll nun die Windenergie erheblich ausgebaut werden (siehe auch Energiewende).

Noch 2001 stammten über 90 % der Stromerzeugung aus Wasserkraftwerken, deren Ausbaupotential mittlerweile jedoch weitgehend erschöpft ist. Problematisch erwiesen sich zudem wiederkehrende zwei- bis dreijährige Dürrephasen, die in den Jahren 2001 und 2002 zu Stromausfällen und sozialen und politischen Problemen führten. Zudem führt das Wirtschaftswachstum zu einer stark steigenden Stromnachfrage, die eine Erweiterung des Kraftwerksbedarfes notwendig macht. Daher setzt Brasilien zur Diversifizierung der Erzeugungsstruktur stark auf den Ausbau der Windenergie, die sich insbesondere in Nordbrasilien komplementär zur Wasserkraft verhält und dieses deshalb sehr gut ergänzt. Zudem verfügt Brasilien onshore wie offshore über ein sehr großes Windenergiepotential mit hohen Windgeschwindigkeiten. 2001 wurde das PROEOLICA-Programm aufgelegt, das 2004 durch das PROINFA-Programm ergänzt wurde, das den allgemeinen Ausbau der erneuerbaren Energien (Kleinwasserkraft, Biomasse, Solar) vorsieht, die zu diesem Zweck mit Einspeisevergütungen gefördert wurden. Ende 2013 waren in mehr als 140 Windparks mehr als 3,3 GW Windenergieleistung in Brasilien installiert. Ende 2017 waren in Brasilien Windkraftanlagen mit einer Gesamtleistung von 12.763 MW installiert, womit Brasilien auf Rang 8 weltweit lag.

Die Windenergie zählt in dem Land aufgrund günstiger Standortbedingungen zu den preiswertesten Stromerzeugungsformen. Auch weltweit gehört Brasilien zu den Ländern, in denen die Nutzung der Windenergie im internationalen Vergleich mit am günstigsten ist. Die Stromgestehungskosten von Windkraftanlagen liegen mittlerweile bei unter 60 US-Dollar/MWh, umgerechnet ca. Euro/MWh. Bei Ausschreibungen für Energiekontrakte werden für Windenergieprojekte Tiefstpreise bis zu 50,2 US-Dollar/MWh erzielt. Bis 2020 sollen Windkraftanlagen 10 % der Stromproduktion decken.

Daneben verfügt Brasilien über bedeutende Erdölreserven und fördert seit den 1980er Jahren die Ethanolproduktion mittels Zuckerrohr. Infolgedessen war Brasilien lange der weltweit wichtigste Bioethanolproduzent.

Erdölpipelines haben in Brasilien eine Länge von knapp 3000 km, Erdöl-Produkte werden in einem Pipeline-Netz mit einer Länge von knapp 5000 km transportiert und die Erdgasleitungen haben insgesamt eine Länge von etwa 4250 km.

"Siehe:" Brasilianischer Verpackungsmarkt

Im Jahre 2002 wurde die Verfassung in der Hinsicht verändert, dass die Anteile ausländischer Unternehmen an den nationalen Medien nicht größer als 30 % sein dürfen.

In Brasilien gibt es etwa 530 Tageszeitungen, die eine geschätzte Gesamtauflage von 6,5 Millionen Exemplaren haben. Die bekanntesten von ihnen sind "Folha de São Paulo", "Estado de São Paulo", "O Día" und "O Globo". Letztere gehört zur Globo-Gruppe, die die brasilianische Medienlandschaft beherrscht und der vorgeworfen wird, einzelne Parteien oder Kandidaten zu favorisieren. "Rede Globo" ist auch einer der Marktführer, was die Produktion an Telenovelas angeht. Rund 80 % der Produktionen werden exportiert. Ihre jetzige Stellung wird aber von internationalen Konzernen und dem Internet bedroht. Ein staatlicher Radiosender stand 1997, neben über 2.900 privaten Stationen, für ganz Brasilien mit etwa 70 Millionen Radiogeräten zur Verfügung. Darüber hinaus gibt es 19 staatliche und etwa 250 private Fernsehsender. Die Reichweite des Mediums Fernsehen ist in Brasilien relativ groß: 90,3 % der Haushalte hatten 2003 einen Fernseher. Im Jahre 1999 gab es 200 Internetdienstanbieter. 2016 nutzten 139 Millionen Menschen das Internet, 66,4 Prozent der Bevölkerung. Eine Zensur des Online-Angebots gibt es nicht.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Brasilien Platz 103 von 180 Ländern. Bei der Situation der Pressefreiheit im Land gibt es laut der Nichtregierungsorganisation „erkennbare Probleme“. Ein großes Hindernis für die Unabhängigkeit von Medien und Pressefreiheit ist, dass sich nahezu alle großen Medienkonzerne des Landes in den Händen einiger weniger Personen befinden.

In Brasilien hat sich die Kunst aus der Religion entwickelt. Während der Kolonialzeit war die Sakralkunst dominierend. Unter anderem wurden zahlreiche Kirchen der verschiedenen Konfessionen künstlerisch gestaltet. Die Zusammenarbeit zwischen Holzschnitzern, Steinmetzen und Malern verlief so eng, dass auch die Farbwahl untereinander abgestimmt war und die Kirchen heute zu den schönsten Amerikas zählen. Üppig ausgestattet waren die Kirchen schon im 17. Jahrhundert, die größten und wertvollsten Kunstwerke entstanden aber erst im 18. Jahrhundert.

Im 20. Jahrhundert gewann der Impressionismus, der in Europa schon in den letzten Jahren des 19. Jahrhunderts auftrat, immer mehr an Bedeutung. Bedeutende Künstler dieser Zeit waren Anita Malfatti, Manuel Santiago und José Pancetti, noch angesehener war aber Candido Portinari. Er selbst gilt als größter Künstler Brasiliens des vergangenen Jahrhunderts. Da er mit hochgiftigen Farben malte, erkrankte er an Krebs und verstarb früh. Seine berühmten Kunstwerke hängen in Gebäuden wie der UN-Zentrale in New York. Kunstkritikern zufolge wird in seinen Werken die Originalität Brasiliens am besten hervorgehoben.

In den 1940er und 1950er Jahren entwickelte sich der soziale Realismus. Portinaris Kunstwerke mit sozialen Themen werden diesem Stil zugeordnet. Aus der abstrakten Kunst der 1960er bis 1980er Jahre stechen vor allem die Brüder Thomaz und Arcangelo Ianelli und die Grafikerin Fayga Ostrower hervor. Eine bedeutende Gruppe brasilianischer Künstler ist unter dem Namen Gruppe der 19 bekannt. Ihr gehören zum Beispiel die Surrealisten Mario Gruber und Otavio Araujo an. Marcelo Grassman, ebenfalls einer der 19, erstellte mittelalterliche Stiche. Das vierte Mitglied ist Lena Milliet, die zu den ersten brasilianischen Frauen zählt, die in der Kunst Anerkennung fanden. Daneben ist Carlos Araujo, der mit riesigen Ölbildern berühmt wurde, zu nennen. Mit demselben Stil arbeitet Nora Beltran, die die politischen und sozialen Zustände in Brasilien karikiert. Gustavo Rosa malt zwar nicht surrealistisch, dennoch ironisch.

Heutzutage ist die Biennale von São Paulo das größte Kunstereignis in Brasilien. In dieser Veranstaltung liegt der Schwerpunkt auf Gemälden von international renommierten Künstlern. Auch Rio de Janeiro ist ein Kunstzentrum. Kleinere, weniger bekannte Orte haben bei Experten aber ebenfalls ein hohes Ansehen, beispielsweise der zentralbrasilianische Ort Goiás. Recife ist für João Câmara und Gilvan Samico bekannt. Fortaleza ist für Raimundo Cela und Antonio Bandeira bekannt. Brasiliens berühmtester und in den Augen vieler bester Holzschnitzer ist Maurino Araujo, weshalb auch seine Heimatstadt Minas Gerais unter Kunstliebhabern bekannt ist.

Die Kunst der Indianer ist sehr vergänglich. Für aufwändige Körperbemalungen benötigt man oft mehrere Tage, doch die Farben halten oftmals nicht viel länger. Auch der bunte Federschmuck als Kopfbedeckung ist nur selten in Museen zu sehen.

Die brasilianische Musik ist von portugiesischen, afrikanischen und indigenen Musiktraditionen beeinflusst worden. Über die indigene Musik der vorkolonialen Zeit ist kaum etwas bekannt, die erste Beschreibung datiert aus dem Jahre 1568. Ein französischer Pastor beschrieb damals in einem Buch über seine Reise in das Land die Tänze und Gesänge der Ureinwohner. Die Musik veränderte sich unter dem Einfluss der europäischen Siedler und afrikanischen Sklaven.

Die Kunstmusik wird in Brasilien als "música erudita", gelehrte Musik, bezeichnet. Lange Zeit beschränkte sie sich auf die Kirchenmusik und konzentrierte sich während dieser als "barocco mineiro" bezeichneten Epoche auf Minas Gerais und in geringerem Maße Rio de Janeiro. Zwischen 1760 und 1800 gab es in Minas Gerais fast 1.000 Musiker, viele davon freie Mulatten. Zu diesen gehörte José Maurício Nunes Garcia (1767–1830), dessen Werk vor allem Kirchenmusik, aber auch einige weltliche Werke umfasst und der von der Wiener Klassik beeinflusst war.

Einen bedeutenden Entwicklungsschub erfuhr die brasilianische Musik, als 1808 der portugiesische Hof aufgrund des napoleonischen Krieges nach Rio de Janeiro flüchtete. Das Königshaus beschäftigte nun zahlreiche einheimische Musiker und die neue Residenz zog auch europäische Musiker an. Auf diese Weise kamen neue, weltliche musikalische Impulse ins Land. Die Rückkehr des portugiesischen Hofes nach Lissabon hatte 1822 eine schwere Krise für die "música erudita" zur Folge.

Seit der Mitte des 19. Jahrhunderts entfaltete sich das musikalische Leben wieder durch die verstärkte Einwanderung europäischer Immigranten nach Brasilien. Nachdem in den 1830er Jahren verschiedene Musikgesellschaften und ein Konservatorium in Rio gegründet worden waren, entstanden in den größeren Städten mehrere Theater, von denen vier ein eigenes Orchester besaßen. Vor allem in der Hauptstadt Rio wurden europäische, vor allem italienische Opern bereits kurz nach ihrer Erstaufführung gespielt. Mit der Oper "A Noite de São João" von Elias Álvares Lôbo wurde 1860 die erste brasilianische Oper uraufgeführt. 1870 feierte die Oper "O Guarani" von Antônio Carlos Gomes sogar an der Mailänder Scala Premiere und wurde anschließend in ganz Europa gespielt. Weitere Uraufführungen seiner Opern in Mailand folgten in den nächsten Jahren.

Vor der Jahrhundertwende orientierten sich die brasilianischen Musiker zunehmend an der deutschen und französischen Kunstmusik, wenn auch die italienische Oper beim Publikum weiterhin großen Erfolg hatte. In den Vordergrund rückte jetzt kammermusikalische und symphonische Musik. Ihre Ausbildung hatten fast alle Komponisten in Europa erhalten.

Im Jahre 1922 kam es durch die "Semana de Arte Moderna" (Woche der modernen Künste) zu einer musikalischen Revolution. Mit Heitor Villa-Lobos an der Spitze entstand eine Gruppe neuer Komponisten, die Elemente der brasilianischen Folklore in ihre moderneren Lieder einbauten. In den 1950er Jahren kam der Bossa Nova auf. Diese Musikrichtung gilt als die „brasilianische Variante des Jazz“: sie lehnt sich an nordamerikanischen Jazz an, bleibt aber geprägt von südamerikanischen und afrikanischen Rhythmen. Als bekanntester Vertreter und Mitbegründer des "Bossa Nova" gilt Antônio Carlos Jobim. Zusammen mit Sänger/Gitarrist João Gilberto und Texter Vinícius de Moraes verhalf er dem Stil in den 1960ern zu großem internationalem Erfolg, nicht zuletzt aufgrund des berühmtesten Songs brasilianischer Herkunft, „Garota de Ipanema“, englisch „The Girl from Ipanema“. Jobim erreichte für Brasilien eine derart große Bedeutung, dass man den internationalen Flughafen von Rio de Janeiro nach ihm benannte.

Einen der größten Hits des "Bossa Nova" in den 60er Jahren hatte der Bandleader und Pianist Sérgio Mendes mit seiner Version der Jorge Ben-Komposition "Mas que nada". Dieser Titel wurde noch weitere unzählige Male kopiert. Heute wird der "Bossa Nova" vorwiegend von den älteren Brasilianern gehört. Der Tropicalismo (auch "Tropicália") entstand Ende der 1960er Jahre zur Zeit der Militärdiktatur. Musikalisch ist er eine Mischung aus Bossa Nova, Folk und Rock; das wesentliche Element ist aber ein gemeinsames politisches Bewusstsein der Künstler. Ihre Abneigung gegen die Diktatur und die Einschränkung ihrer Rechte fand im "Tropicalismo" ihren Ausdruck. Die Texte sind daher im Allgemeinen regimekritisch. Nicht wenige Musiker mussten ins Exil gehen. Wichtige Vertreter sind Gilberto Gil und Chico Buarque, die es mit geschickter Chiffrierung ihrer Liedtexte sogar geschafft haben, die Zensur zu umgehen und ihre Lieder in Brasilien zu veröffentlichen. Gilberto Gil war vom 1. Januar 2003 bis zum 30. Juli 2008 Kulturminister von Brasilien; seine Zielsetzung war, den Zugang zur Kultur zu demokratisieren. Er unternimmt Reisen in entlegene Gebiete des Landes, um den Menschen dort zu sagen, dass sie wichtige Träger der brasilianischen Kultur sind.

Entgegen ihrem Namen hat die Música Popular Brasileira, oft mit "MPB" abgekürzt, nur wenig mit dem gemeinsam, was man hierzulande unter "Popmusik" versteht. Die Bezeichnung umfasst eine Vielzahl an Stilrichtungen, die aber stets typische Elemente aus einzelnen Regionen des Landes aufgreifen. In Brasilien gilt MPB als Ausdruck des musikalischen und nationalen Selbstverständnisses. In diesem Sinne stellt MPB gewissermaßen eine Weiterentwicklung der brasilianischen Folklore dar.
Die wohl bekannteste brasilianische Musikform ist der Samba. Er entstand aus der Musik der afrikanischstämmigen Bevölkerungsteile und ist sehr rhythmuslastig. Populär wurde der Samba durch den jährlichen Karneval in Rio de Janeiro. Dort präsentieren sich die größten und renommiertesten Sambaschulen in riesigen Paraden im Wettstreit um den Titel der „besten Sambaschule Brasiliens“. Neben den Umzügen zur Karnevalszeit spielen die Bands manchmal auch auf den Straßen oder unterstützen mit ihrer Musik politische Demonstrationen und Streiks.

Es gibt eine unüberschaubare Zahl an regionaltypischen Musikstilen, die sich entsprechend den verschiedenen kulturellen Eigenheiten der jeweiligen Gebiete entwickelt haben. Música Nordestina ist ein Sammelbegriff für die Musik aus dem Nordosten, der eine besonders große musikalische Vielfalt besitzt. Hier sind Instrumente wie Akkordeon und Gitarre vorherrschend. Recife im Speziellen ist bekannt für den Frevo, der auch Einflüsse aus der Militärmusik besitzt. Forró wird von Trios mit Trommel, Triangel und Akkordeon gespielt. Ein traditioneller afro-brasilianischer Stil ist Maracatu, der mit großen Trommeln, Glocken und Rasseln gespielt wird.

Eine besondere Rolle als musikalischer Impulsgeber spielt Salvador da Bahia. Seit 1949 nehmen hier Afoxé-Blocos an den Karnevalszügen teil, die ihre Wurzeln in der Musik des Candomblé haben und auch im Zusammenhang mit der Freiheitsbewegung der afrobrasilianischen Bevölkerung zu sehen sind. Seit den 1980er Jahren ist in Salvador der Samba-Reggae entstanden.

Besonders in den regionaltypischen Musikrichtungen kommen Instrumente afrikanischen Ursprungs zum Einsatz, so zum Beispiel die Berimbau, ein bogenförmiges Rhythmusinstrument mit einem hohlen Kürbis an einem Ende, oder die Xequerê, ein mit Muscheln bestücktes Schüttelinstrument.

In den letzten Jahren setzte sich vor allem bei den Jugendlichen die Musikrichtung "Axé" durch, insbesondere im Bundesstaat Bahia. "Axé" ist eine Mischung aus "Samba", "Pagode" und "Pop", enorm rhythmusbetont und gut tanzbar. Sie wird gegenüber dem "Samba" immer mehr bevorzugt (ausgenommen in der Karnevalszeit). Bekannte Sängerinnen des "Axé" sind Daniela Mercury, Ivete Sangalo und Claudia Leitte. In den offenen Cafés in Brasilien, in denen das Publikum meist um die 30 oder 40 Jahre alt ist, wird aber in erster Linie "Pagode" gespielt.

Im brasilianischen Hinterland ist die Música Sertaneja (oder „Música Caipira“) ein populärer und typisch brasilianischer Musikstil. Er zeigt Einflüsse der portugiesischen Musik und wird mit der viola caipira, einer zwölfsaitigen Variante der Gitarre, gespielt. Bekannte Sänger des „Música Sertaneja“ sind Sérgio Reis, Renato Teixeira und Almir Sater, sowie Duos wie Zezé di Camargo & Luciano, Chitãozinho & Xororó und César Menotti & Fabiano.

Im Bundesstaat Rio Grande do Sul gibt es eine besondere Musiktradition der Gauchos mit Einflüssen aus Uruguay und Argentinien.

Ende der 90er Jahre entwickelte sich „Brazilectro“, ein Mix aus dem englischen Drum and Bass und dem brasilianischen Bossa Nova.

Das erste erhaltene Dokument, das als brasilianische Literatur bezeichnet werden kann, ist ein Brief von Pero Vaz de Caminha an Manuel I. von Portugal, in dem Brasilien im Jahre 1500 beschrieben wurde. In den nächsten beiden Jahrhunderten machten Beschreibungen von Reisenden über das „Portugiesische Amerika“ und seine Einwohner die brasilianische Literatur aus, so wurden zum Beispiel die Berichte des deutschen Soldaten Hans Staden berühmt. Außerdem wurde aus dieser Zeit religiöse Literatur gefunden. Neoklassizismus war in der Mitte des 18. Jahrhunderts weit verbreitet. In der Kolonialzeit war der für seine Goldminen bekannte Bundesstaat Minas Gerais Zentrum der Literatur. Ab etwa 1836 beeinflusste die Romantik die brasilianische Literatur. In dieser Zeit entstanden die ersten Standardwerke der Landesliteratur. Auf die Romantik folgte der Realismus, bei dem Joaquim Maria Machado de Assis als bester und populärster brasilianischer Schriftsteller hervorstach. Zwischen 1895 und 1922 ist kein einheitlicher Stil zu erkennen, doch einige Züge der Moderne gab es schon, so dass diese Periode „Vor-Moderne“ genannt wird. Seit der "Semana de Arte Moderna" (Woche der modernen Künste) 1922 wurde die Moderne der dominierende Stil.

Die berühmtesten Autoren dieser Zeit sind Mário de Andrade und Oswald de Andrade; ebenfalls internationale Bekanntheit erlangt hat Jorge Amado. Der brasilianische Erfolgsautor Paulo Coelho ist zurzeit der weltweit meistgelesene Autor. 2004 erhielt Lygia Bojunga Nunes den bedeutenden Astrid-Lindgren-Gedächtnis-Preis für Kinderliteratur.

Brasilien: mit 108 Lizenzen im Jahr 2004 Deutschlands zweitwichtigster Lizenzabnehmer auf dem amerikanischen Kontinent (nach den USA mit 175 Lizenzen). Fehlende Sprachkenntnisse und hohe Übersetzungskosten sind trotzdem noch Barrieren. Die Buchmesse São Paulo ist die vielleicht wichtigste Südamerikas.

2013 ist Brasilien Gastland der Frankfurter Buchmesse – als zweites Land zum zweiten Mal.

Aufgrund der Größe des Landes ist es schwierig, die brasilianische Küche zu definieren. Es ist gesichert, dass sie durch die portugiesische Kolonisation beeinflusst wurde. Als Nationalgericht gilt die Feijoada, ein Bohneneintopf aus schwarzen Bohnen mit allerlei Fleisch. Üblicherweise wird Feijoada mit Reis, Farofa (ein Maniokmehl) und Orangenscheiben serviert. Wegen des großen Abstands zwischen den Orten sind die Verpflegungsstationen an Fernstraßen wichtig. Hier wird zwischen kommerziell betriebenen Snackbars mit großem Angebot an Sandwiches und anderen einfachen Gerichten und kleinen, familiären Haltepunkten, die meist nur ein Gericht (Reis, Kartoffeln oder Bohnen mit einer Fleischsorte) bieten, unterschieden.

Im Amazonasbecken herrschen die primitiven Indianerhütten vor, in anderen Bundesstaaten, zum Beispiel Minas Gerais, sind dagegen prachtvolle und historische Städte, erbaut im Barock, und ebenso prachtvoll dekorierte Kirchen zu finden (Ouro Preto, Mariana, Congonhas). Kolonialarchitektur bestimmt in einigen Küstenstädten des Nordostens noch das Bild (Olinda). Die größten Architekten des Landes Oscar Niemeyer, der als Wegbereiter der brasilianischen Architektur gilt, sein früherer Dozent Lúcio Costa und Roberto Burle Marx gestalteten gemeinsam den schönsten brasilianischen Wohnpark „Pampulha“ in Belo Horizonte. Der damalige Initiator war der spätere Präsident Juscelino Kubitschek, der in einer seiner ersten Amtshandlungen als mächtigster Mann im Staat das dreiköpfige Team erneut zusammenrief, um das Projekt Brasília zu beschließen. Denn die Hauptstadt Brasília ist Höhepunkt brasilianischer Architektur, sie wurde erst in den 1960er Jahren errichtet und unterliegt einem genauen Plan. Nach einer Ausschreibung, bei der der Sieger mit Lúcio Costa schon vorher feststand, plante Costa den Aufbau der Stadt, Niemeyer war wie schon in Pampulha für die meisten Gebäude zuständig und Burle Marx entwarf Plätze und Parks. Brasília ist heute berühmt für modernistische Gebäude.

Zu den Meisterwerken der brasilianischen Moderne gehören auch die Bauwerke Paulo Mendes da Rocha, der 2006 den Pritzker-Preis erhielt, und in den Jahrzehnten ab 1954 das Bild der Metropole São Paulo durch den Club Athletico Paulistano (1958), die Kapelle von Sankt Peter in Campos de Jordão, Brasilien (1987) und das Museu Brasileiro de Escultura – Brasilianisches Skulpturenmuseum in São Paulo (1988) formten. Diesen durch streng geometrischen Betonbauten geprägten Avantgarde-Stil bezeichnet man unzutreffend als „brasilianischer Brutalismus“.

National- und auch Volkssport des Landes ist Fußball. Das erste Fußballspiel fand 1894 statt, rund 10 Jahre später dürften die ersten Spieler mitgespielt haben, die keine europäischen Vorfahren hatten. Die brasilianische Fußballnationalmannschaft ist fünfmaliger Weltmeister und damit die erfolgreichste Nationalmannschaft der Welt. Darüber hinaus gewann Brasilien achtmal die Copa América, die Südamerika-Meisterschaft. Für viele Fußballfreunde gilt darüber hinaus Pelé als einer der besten Fußballer aller Zeiten. Auch andere Spieler wie Arthur Friedenreich, Garrincha und Zico zählten zu den besten ihrer Zeit. Die Auszeichnung als Weltfußballer des Jahres erhielten zudem Romário, Ronaldo, Rivaldo, Ronaldinho und Kaká. Auch in der aktuellen Mannschaft spielen viele international bekannte Stars. Die Nationalmannschaft der Damen gehört ebenfalls zur Weltspitze, auch wenn ihr ein WM- oder Olympiasieg noch nicht gelang, und hat mit Marta Vieira da Silva die wohl beste Spielerin der Welt in ihren Reihen. Ein Großteil der Bevölkerung spielt aber Fußball unter einfacheren Verhältnissen, beispielsweise in den Favelas auf Sandplätzen (Campos). Für viele Kinder und Jugendliche in den Favelas ist die Aussicht, Fußballprofi zu werden, eine der wenigen Möglichkeiten, der Armut zu entgehen.

Brasilien war innerhalb von gut zwei Jahren Schauplatz der beiden bedeutendsten Sportereignisse der Welt: 2014 wurde in Brasilien die Fußballweltmeisterschaft ausgetragen. Das Land war einziger Bewerber für den Austragungsort der WM. 2016 fanden die Olympischen Sommerspiele in Rio de Janeiro statt. Dies war die erste Austragung Olympischer Spiele auf dem südamerikanischen Kontinent.

Sehr beliebt ist auch Volleyball. Die Nationalmannschaft der Herren wurde 2002, 2006 und 2010 Weltmeister, die der Damen 2008 und 2012 Olympiasieger. Besonders für Beachvolleyball ist das südamerikanische Land bekannt, bei Weltmeisterschaften gewann es mehr Medaillen als jedes andere Land. Zudem wurde Footvolley, eine Mischung aus Fuß- und Volleyball, in Brasilien erfunden. Eine weitere beliebte Mannschaftssportart ist Basketball. Die Nationalmannschaft der Herren wurde zweimal Weltmeister, die Damen-Nationalmannschaft gewann 1994 den WM-Titel.

Brasilien ist Gastgeber des Großen Preises von Brasilien, zur Zeit eins von zwei Formel-1-Rennen in Lateinamerika. Das Land hat mit Emerson Fittipaldi, Nelson Piquet und Ayrton Senna drei mehrfache Weltmeister hervorgebracht. Starke Anteilnahme fand in der Bevölkerung die Beerdigung Ayrton Sennas 1994. Zwei Rennstrecken wurden für Formel-1-Rennen genutzt: Das Autódromo Internacional Nelson Piquet bei Rio de Janeiro und das Autódromo José Carlos Pace bei Interlagos.

Erfolgreichster Tennisspieler Brasiliens ist Gustavo Kuerten, der drei Mal die French Open gewinnen konnte, bedeutendster Leichtathlet des Landes war der Dreispringer und zweifache Olympiasieger Adhemar da Silva. Der Olympiasieger und mehrfache Weltmeister César Cielo ist der erfolgreichste Schwimmer des Landes. Auch im Segelsport ist Brasilien erfolgreich. Mit Rodrigo Pessoa, Olympiasieger 2004 und Weltmeister 1998 und seinem Vater Nelson, Europameister 1966, besitzt Brasilien auch erfolgreiche Springreiter.

Als typisch brasilianisch kann Capoeira bezeichnet werden, was besser mit dem Begriff "Kampftanz" denn mit "Kampfsportart" kategorisiert wird. Capoeira wurde von der schwarzen Bevölkerung praktiziert. Da es den Sklaven nicht erlaubt war, irgendeine Art von Waffen zu tragen, entwickelten sie Capoeira als Form der Selbstverteidigung: Sie verbindet kämpferische Elemente mit Akrobatik, Spielerei und Tanz. In den vergangenen Jahrzehnten entwickelte sich eine gewisse Mode um Capoeira. Sie ist mittlerweile in der ganzen brasilianischen Bevölkerung verbreitet und erfreut sich auch im Ausland Beliebtheit. Im Zuge der in den letzten Jahren wachsenden Verbreitung der Kampfsportarten und -künste aus den Mixed Martial Arts (MMA), insbesondere Grappling, erlangten international Vale Tudo, Luta Livre und Brazilian Jiu-Jitsu (BJJ) große Anerkennung.





</doc>
<doc id="7780" url="https://de.wikipedia.org/wiki?curid=7780" title="Bauingenieurwesen">
Bauingenieurwesen

Das Bauingenieurwesen ist eine Ingenieurwissenschaft, die sich mit der Konzeption, Planung, Entwurf, Konstruktion, Berechnung, Herstellung und dem Betrieb von Bauwerken des Hoch-, Verkehrs-, Tief- und Wasserbaus auseinandersetzt. In diesem Zusammenhang werden ebenfalls Fragen des technischen Umweltschutzes behandelt, beispielsweise Lärmschutz, Gewässer- und Bodenschutz sowie zugehörige Schadstoffuntersuchungen.

Die Berufsbezeichnung lautet Bauingenieur und zählt zur Berufsgruppe der Ingenieure. Das Studium des Bauingenieurwesens an Universitäten und Fachhochschulen schließt mit einer akademischen Graduierung ab als Bachelor und weiterführend als Master. Bis zur Umsetzung des Bologna-Prozesses (Kernphase war der Zeitraum Studienbeginn 2003–2006 bei den zuvor 8- bis 10semestrigen Diplomstudiengängen) war bei universitären Studiengängen der akademische Grad "Diplom-Ingenieur" (üblicherweise abgekürzt mit "Dipl.-Ing." bzw. "Dipl.-Ing. Univ." in Bayern) sowie an Fachhochschulen "Diplom-Ingenieur (FH)" (abgekürzt mit "Dipl.-Ing.(FH)") üblich. Folglich waren in Deutschland die meisten Studienabschlüsse des Bauingenieurwesens bis ca. zum Jahr 2008 noch mit dem akademischen Grad des Diplomingenieurs betitelt.

Nicht durchsetzen konnte sich im Rahmen des Bologna-Prozesses die angestrebte Umbenennung des weitgefächerten Berufsbildes in Zivilingenieur, was einer sinnvollen Anlehnung an die Berufsbezeichnungen im frankophonen (frz.: "genie civil") und im englischsprachigen Raum (engl.: "civil engineer") entsprochen hätte.

Im Wort "Bauingenieurwesen" steckt der Begriff "Ingenieur im Bauwesen". Die Ingenieurbezeichnung ist in diesem Zusammenhang bereits seit dem frühen Mittelalter bekannt. Es leitet sich von dem lateinischen Wort "" ab und bedeutet "produktiver Geist, Verstand, geistreicher Mensch". Diesen Titel erhielten im 12. und 13. Jahrhundert Menschen, die sich auf den Bau und die Bedienung von Kriegsgerät verstanden.

Diese Bedeutung behielt das Wort "Ingenieur" viele Jahrhunderte bei und wird beispielsweise im mathematischen Lexikon von Christian Wolf aus dem Jahr 1716 erwähnt. Dort heißt es, der Ingenieur sei ein. .

Johann Rudolf Fäsch ergänzt im Jahr 1735 in seinem "Kriegs-, Ingenieur, und Seelexicon": .

Das Bauingenieurwesen zählt zu den ältesten Ingenieurwissenschaften. Erste Gebäude wurden nach der neolithischen Revolution gegen Ende der Steinzeit gebaut. Bei den Assyrern, Babyloniern und Persern – den frühen Hochkulturen Mesopotamiens – wurden Ingenieure an Palast- oder Tempelschulen ausgebildet. Unterrichtet wurde Lesen und Schreiben der Keilschrift sowie die Berechnung der Neigung von Wasserleitungen, der Erdaushub von Ausschachtarbeiten oder die Belastbarkeit von Mauern. Dieselben Ingenieure, die in Friedenszeiten den Bau von Palästen, Brücken, Tempeln, Stadtmauern oder Aquädukten beaufsichtigten, waren im Kriege mit militärischen Verwaltungsaufgaben betraut. Bemerkenswerte Bauwerke sind die Djoser-Pyramide des Baumeisters Imhotep, der Palast von Persepolis, sowie die Sieben Weltwunder. In der Antike sind die Römer bekannt für ihre vielen Brücken und Straßen. Sie entwickelten auch verbesserte Krane mit Flaschenzug und Laufrad. Im frühen Mittelalter stand vor allem der Ausbau der Klöster im Vordergrund, später der Bau von Burgen und Kathedralen.

Für das Bauingenieurwesen sind zwei Bauwerke von besonderer Bedeutung. Der Dom von Florenz war zu Beginn des 15. Jahrhunderts beinahe fertig. Es fehlte nur noch die Kuppel, die wegen des für damalige Verhältnisse gewaltigen Durchmessers von 45 Metern unmachbar schien. Man fand keine Möglichkeit, ein Lehrgerüst in den benötigten Abmessungen zu errichten. Brunelleschi fand durch theoretische Überlegungen heraus, dass er die Kuppel ohne Gerüst bauen kann, falls sie eine elliptische Form besitzt. Hierin zeigte sich bereits ein langsamer Übergang vom Erfahrungswissen der Baumeister hin zu theoretischem Wissen der Ingenieure.

Den Wendepunkt für das Bauingenieurwesen brachte die Renovierung des Petersdomes 1742. Hier vertraute man erstmals auf die Berechnungen von Mathematikern auf Grundlagen der Mechanik, die den Einbau von weiteren Zugringen als Verstärkung für das baufällige Gebäude als ausreichend erachteten. Den Vorschlag der erfahrenen Baumeister, die ganze Kuppel abzutragen, verwarf man.

Im 17. Jahrhundert wurden viele Länder von den Regierungen vermessen, um die Verwaltung zu verbessern. Die Landesvermessung Frankreichs war ein Projekt, das von der Académie des sciences durchgeführt wurde und über ein Jahrhundert dauerte. In der Renaissance breiteten sich immer mehr die neuen Kanonen aus; die Burgen verloren ihren militärischen Wert. Verteidigungsanlagen wurden nun flach und massiv erbaut. Die Festungsbaukunst wurde zu einer neuen Disziplin, in der die Geometrie eine große Rolle spielte. Der französische Festungsbaumeister Vauban baute bis 1700 etliche Festungen und nahm an vielen Belagerungen teil. 1675 wurde das Corps des ingénieurs du génie militaire gegründet, das die militärischen Festungsbauingenieure erstmals zusammenfasste. Zwischen 1663 und 1681 wurde der Canal du Languedoc gebaut, das seit der Antike größte Kanalbauprojekt. Außerdem wurde in Frankreich der Straßen- und Brückenbau vom Staat vorangetrieben. Dazu wurden 1716 die zivilen Ingenieure zum Corps des ingénieurs des ponts et chaussées zusammengefasst. Im 18. Jahrhundert wurden auch erste Schulen für die Ausbildung neuer Ingenieure gegründet. Dazu zählen die Ecole des ponts et chaussées 1747 ("Schule für Brücken und Straßen") die Ecole du Génie Militaire 1748 in Mézières ("Schule für Militärpioniere") und die Ecole des Mines 1783 ("Schule für Bergbau"). 1794 wurde schließlich die École polytechnique gegründet, die auch für andere Ingenieurwissenschaften international eine große Bedeutung hat. Hier wurden in zwei Jahren Unterricht die gemeinsamen mathematisch-naturwissenschaftlichen Grundlagen für das anschließende Studium auf einer der vorgenannten Spezialschulen vermittelt. Nach dem Vorbild der Ecole Polytech wurden in Deutschland zu Beginn des 19. Jahrhunderts viele Polytechnische Schulen gegründet, die im Laufe des Jahrhunderts zu Technischen Hochschulen und schließlich zu Technischen Universitäten aufgewertet wurden. Im liberalen England war der Bau von Straßen, Brücken und Kanälen Sache der privaten Wirtschaft. Die britischen Bauingenieure schlossen sich unter Führung des berühmten Ingenieurs John Smeaton 1771 zur Society of Civil Engineers zusammen. Trotz ihres großen Einflusses verfiel sie letztendlich in eine Dauerkrise. Sie wurde 1818 von der Institution of Civil Engineers von Thomas Telford abgelöst.

Das Bauingenieurwesen gliedert sich in eine Vielzahl verschiedener Fachgebiete, die den technischen Bereich des gesamten Bauwesens umfassen:

In all diesen Teilgebieten sind Bauingenieure maßgebend beschäftigt und übernehmen dort u.a. die Planung, die Bemessung, die Kostenkalkulation, die Ausführungsleitung bzw. -steuerung, das baubetriebliche Controlling sowie die sicherheitstechnische Bauüberwachung von Anlagen und Bauwerken. Die Beteiligung ist dabei je nach Art und Funktion des Bauwerks unterschiedlich stark ausgeprägt.

Die ursprüngliche Differenzierung des Berufsbildes nach den Sparten Hochbau, Tiefbau und Ingenieurbau gilt als überholt, weil sich zum Einen die wenigsten Bauwerke eindeutig auf einen dieser Bereiche eingrenzen lassen und zum Anderen auf Grund der extremen Spezialisierung im Berufsbild des Bauingenieurs in den vergangenen Jahrzehnte die Spartenzuordnung verglichen mit dem Fachgebiet keine bedeutende Rolle mehr spielt. Der organisatorische Aufbau der Hochschulausbildung von Bauingenieuren ist daher heutzutage nach den oben genannten Fachgebieten gegliedert. Die universitäre Organisation spiegelt sich in den entsprechenden Lehrstühlen innerhalb der Fakultäten für Bauingenieurwesen wider.

Da der Beruf eine naturwissenschaftliche Ausrichtung besitzt, sind Technikbegeisterung, logisches und analytisches Denkvermögen, hohes Konzentrationsvermögen und Ausdauer bzw. Geduld von Vorteil. Ebenso sind in vielen Bereichen des Berufsbildes soziale Kompetenz und wirtschaftliches Handeln von hoher Bedeutung. Der versierte Umgang mit Informationstechnik stellt im Bauwesen wie in allen Bereichen von Naturwissenschaft und Technik eine Grundvoraussetzung für den Studienerfolg dar. Studienvoraussetzung ist ein Zeugnis der Hochschulreife (abhängig von der Hochschulart: allgemeine oder fachgebundene Hochschulreife oder Fachhochschulreife), der Studiengang selbst kann mit einem Numerus clausus beschränkt sein.

Der Studiengang „Bauingenieurwesen“ wird an vielen Universitäten, Technischen Universitäten und Fachhochschulen in Deutschland, Österreich und der Schweiz angeboten. Das Studium des Bauingenieurwesens ist neben Elektrotechnik und Maschinenbau einer der drei klassischen Studiengänge für angehende Ingenieure.

Nebenstehende Grafik zeigt die Zahl der Studienanfänger und Absolventen im Fach Bauingenieurwesen
an den verschiedenen Hochschularten in Deutschland. Generell ist aus der Grafik ein abwärtsgerichteter Trend zu erkennen, der mit der stark schwankenden Konjunkturlage im Baubereich verbunden ist.

Aufgrund der Vereinheitlichung der Strukturen der Hochschulausbildung in Europa im Bologna-Prozess wurden bis Ende 2010 bereits die meisten Ingenieurstudiengänge vom bisherigen Diplomstudiengang auf das anglo-amerikanische Bachelor- und Master-System umgestellt.

Einige Hochschulen bieten ein „duales Studium“ oder auch Verbundstudium an. In diesem Fall besteht die Möglichkeit sowohl das Studium zum Bauingenieur mit Bachelor-Grad zu absolvieren, als auch einen Meisterbrief, Gesellenprüfung im Bauhandwerk vorausgesetzt, zu erwerben. Damit soll die Qualifizierung der Absolventen für bestimmte Berufstätigkeiten verbessert und die Anstellungschancen erhöht werden.

Das teilweise noch angebotene Diplomstudium dauert nach der Regelstudienzeit zehn Semester. Die Regelstudienzeit für das Bachelorstudium beträgt meistens sechs Semester und im Masterstudium vier Semester. Es gibt aber auch Modelle in denen die Regelstudienzeit variiert, der Bachelor kann dann sieben Semester dauern und der Master drei. Diese Unterschiede folgen aus den unterschiedlichen Angeboten der Hochschulen. Das Universitätsstudium ist im Allgemeinen theoretischer und wissenschaftlicher ausgerichtet als an Fachhochschulen. In Fachhochschulen wird dagegen verstärkt anwendungsorientiertes Wissen vermittelt. Für das Studium des Bauingenieurwesens ist an Universitäten und Fachhochschulen normalerweise ein Grundpraktikum abzuleisten, das allerdings bei einer geeigneten Berufsausbildung entfallen kann. An Fachhochschulen ist des Weiteren ein praktisches Studiensemester eingeplant.

Mit erfolgreichem Studienabschluss wurde bisher der akademische Grad eines Diplomingenieurs verliehen (bei einem FH-Diplomstudiengang mit Angabe der Hochschule). Die Abschlussbezeichnungen lauten nach der Umstellung fortan beispielsweise Bachelor of Engineering und Master of Engineering oder Bachelor of Science und Master of Science.

In Österreich wird der akademische Hochschulgrad „Dipl.-Ing.“ auch als „DI“ abgekürzt. Dem Absolventen einer 5-jährigen schulischen Ausbildung an einer Höheren Technischen Lehranstalt (HTL) kann in Österreich – auf Antrag – die Standesbezeichnung „Ingenieur der Fachrichtung Bauwesen“ verliehen werden.

Die wissenschaftliche Weiterqualifikation als „Doktor der Ingenieurwissenschaften (Dr.-Ing.)“ ist in einem mehrsemestrigen Promotionsverfahren an einer Universität bzw. Technischen Hochschule möglich. Voraussetzung dafür ist ein Universitätsdiplom- oder ein Masterdiplomabschluss.

Auch an Berufsakademien werden Bauingenieure ausgebildet. Im Gegensatz zu Hochschulabsolventen erhält der BA-Absolvent keinen akademischen Hochschulgrad, sondern die staatliche Bezeichnung als „Dipl.- Ing. (BA)“ zuerkannt. An einigen akkreditierten Berufsakademien ist der Abschluss als Bachelor möglich.

Konzipieren, Planen, Berechnen, Konstruieren, Organisieren, aber auch Verwalten sind die wichtigsten Tätigkeitsmerkmale des Bauingenieurs. Technische Lösungen von Bauingenieuren sind immer einerseits der Sicherheit (Standsicherheit, Betriebssicherheit, Gebrauchstauglichkeit) und andererseits der Wirtschaftlichkeit verpflichtet. Bauingenieure arbeiten sowohl in Unternehmen aller Größenordnungen in Bauindustrie und Bauhandwerk als auch in Ingenieurbüros unterschiedlichster Größen. Auch im Bereich der öffentlichen Verwaltung sind Bauingenieure beschäftigt. Sie können Angestellte, Freiberufler oder Beamte sein. Häufig arbeiten Bauingenieure eng mit Architekten und Stadtplanern zusammen.
Für Bauingenieure gibt es eine eigene Laufbahnprüfung (Beamtenlaufbahn) im öffentlichen Dienst.

Bauingenieure sind in unterschiedlichen Teilbereichen (Überschneidungen möglich) des Bauingenieurwesens tätig und werden dann unterschiedlich bezeichnet. So werden Ingenieure, die im Bereich Hochbau arbeiten als Tragwerksplaner oder Statiker bezeichnet. Für Projektleiter einer Baustelle hat sich der Begriff Bauleiter durchgesetzt. Wasserbauingenieure arbeiten im Wasserbau, Verkehrswegeplaner im Verkehrswegebau und Tiefbauingenieure beschäftigen sich mit Tiefbauaufgaben. Die Immobilienverwaltung und Gebäudeüberwachung bzw. -steuerung wird von so genannten Facilitymanagern übernommen.

Der Bauingenieur erlangt durch seine Tätigkeit ein beträchtliches Maß an Verantwortung für Menschen und Umwelt. Die Bauwerke müssen sowohl hinsichtlich der Standsicherheit als auch der Gebrauchstauglichkeit gewissen Anforderungen genügen. Werden diese zum Beispiel infolge fehlerhafter statischer Berechnung, Missachtung anerkannter Regeln der Technik oder Vernachlässigung der auf ihn übertragenen Bauüberwachung nicht erfüllt, haftet der verantwortliche Bauingenieur für diese Fehler. Werden durch die resultierende Mangelhaftigkeit des Bauwerks – zum Beispiel durch einen hierdurch bedingten Einsturz – Leib und Leben von Personen gefährdet, diese verletzt oder gar getötet, drohen dem Verantwortlichen bei gutachtlich belegter Kausalität und Fahrlässigkeit im Strafverfahren Geldstrafe oder gar Freiheitsstrafe.

Die Haftbarmachung von Bauingenieuren für ihre Fehler ist bereits aus dem Altertum überliefert. So ist in dem 1901/02 im persischen Susa wiederentdeckten, ursprünglich aus Mesopotamien stammenden und auf das 18. Jahrhundert v. Chr. datierten "Codex Hammurapi" auf einer Diorit-Säule in Keilschrift zu lesen:

Auch in der aktuellen Gesetzgebung gelten harte Strafenandrohungen. Das deutsche Strafgesetzbuch, § 319 (Baugefährdung), legt fest:

Das Schweizerische Strafgesetzbuch, Artikel 229, schreibt vor:





</doc>
<doc id="7782" url="https://de.wikipedia.org/wiki?curid=7782" title="Baumeister">
Baumeister

Die Berufsbezeichnung Baumeister unterliegt nicht nur einem historischen Wandel, sondern ändert sich auch mit den Bauaufgaben und führt historisch vom Dombaumeister über den Hofbaumeister zum Baumeister. Die an Festungsbauten beteiligten Baumeister bezeichnet man als Festungsbaumeister.

Der "Baumeister" übernimmt die Ausführung von Bauarbeiten aller Art, teils auch die Bauplanung und Bauleitung. Der Begriff ist in Österreich und der Schweiz auch heute noch eine Berufsbezeichnung, in Deutschland wird er dagegen nicht mehr in seiner ursprünglichen Bedeutung verwendet. Baumeister waren zumeist gelernte Steinmetze, Maurer und Zimmerer, manchmal waren auch andere Berufe wie Schreiner oder Stuckateur Grundlage für einen Baumeister. Die Lehrzeit begann in der Regel bereits mit 13 Jahren. Nach der Lehre und Gesellenprüfung begab sich der Anwärter auf Wanderschaft. Um den Titel Meister zu erlangen fertigten sie ein Meisterstück. Mit dem Entstehen der Baugewerkschulen – heute Fachhochschulen – war deren Besuch obligatorisch zum Erlangen eines Baumeistertitels. Persönliche Eignung und Durchsetzungskraft waren ausschlaggebend für die erfolgreiche Ausübung der Tätigkeit.

Im 19. Jahrhundert waren die Baumeister vom Entwurf bis zur Realisierung für ein Bauwerk zuständig. In der Regel unterschieden sie sich von Architekten dadurch, dass sie zusätzlich zum meist eigenen Entwurfsatelier auch eine eigene Baufirma zur Verfügung hatten.

Der Begriff wird für Leiter von existierenden Bauhütten verwendet. Dombaumeister ist heute mit Bau und Erhaltung von Domen und Münstern verbunden. Im Falle eines Münsters wird auch der Begriff Münsterbaumeister verwendet. Im Fall des Dresdner Zwingers trägt der Leiter der zugehörigen Bauhütte den Titel Zwingerbaumeister.

Ein "Hofbaumeister" war ein an den Sitz eines regierenden Fürsten oder Herrschers "(Hof)" berufener, also ein von einem Landesherren mit der Planung und Durchführung öffentlicher Bauvorhaben betrauter Baumeister. Je nachdem welche Aufgaben der Hofbaumeister hatte, konnte er durchaus auch Dombaumeister als auch umgekehrt sein. Der Begriff Hofbaumeister verschwand mit dem Ende monarchisch regierender Herrscher.

Diese Begriffe kennzeichnen einen Dienstrang in der staatlichen bzw. kommunalen Beamten-Hierarchie (vgl. Baubeamter), der je nach historischem Kontext und Dienstherrn ganz unterschiedliche Positionen beschreibt. Träger dieser (in Deutschland heute überwiegend in Baden-Württemberg, Bayern und Sachsen gebräuchlichen) Berufsbezeichnung haben eine akademische Ausbildung (z. B. an einer Technischen Hochschule) absolviert und ein staatliches Examen abgelegt.

Dombaumeister waren ausgebildete Handwerker, Steinmetz- und Steinbildhauermeister, die in der Zeit der Gotik eine Bauhütte leiteten. Die aus dem Steinmetzhandwerk und der Bauhüttentradition hervorgegangenen mittelalterlichen Dombaumeister werden in zeitgenössischen Quellen "Werkmeister" oder "magister operis" bezeichnet. Gegen Ende der Gotik ging der Bau der Dome und damit die Zahl der Bauhütten zurück. Große Bauhütten, wie z. B. die in Straßburg, bestanden bis ins 19. Jahrhundert. Der Begriff Dombaumeister wandelte sich, wie die Bauaufgaben, zu dem des "Baumeisters".

Die Renaissance bildete einen "Baumeister" neuen Typs heraus. Dieser war handwerklich ausgebildet und neben seiner Funktion als Architekt auch Unternehmer. In der Renaissance waren Baumeister nicht mehr nur Steinmetzen und Steinbildhauer: Baumeister Elias Holl war ausgebildeter Handwerker, allerdings ein Maurer.

Im Barock und Rokoko erfolgte die Baumeisterausbildung erstmals in Frankreich an staatlichen Bauschulen. In Deutschland (deutscher Sprachraum) jener Zeit lernten die Baumeister, die Handwerker waren, aus praktischer Erfahrung und aus den sog. Werkmeisterbüchern. Eine Ausnahme bildete die sog. Vorarlberger Bauschule, die sich ausschließlich mit dem Sakralbau befasste. Die Baumeister waren zwar noch ausgebildete Handwerker, in ihrer Hauptaufgabe waren sie aber Planer und Organisatoren.

Im 19. Jahrhundert, im Zuge der Industrialisierung, bildete sich der Beruf des "Architekten" als eigene akademische Disziplin heraus. Neue Bauaufgaben (Statik, ingenieurtechnische Berechnungen usw.) erforderten eine theoretische Ausbildung an Architekturschulen und -akademien.

Baumeister wurden im 20. Jahrhundert als Bauingenieure und Architekten an Hochschulen (Universitäten, Technische Hochschulen) und an höheren Fachschulen (Ingenieurschulen und Ingenieurakademien), später Fachhochschulen, ausgebildet und bis heute (2008) bezeichnet man teilweise diejenigen Personen als Baumeister, die bei Bauvorhaben sowohl die künstlerische als auch die technische und administrative Projektleitung haben.
Die akademischen Berufe des Architekten und Bauingenieurs entwickelten sich durch die zunehmende Komplexität des Bauwesens und die immer größer werdenden Ansprüche hinsichtlich Konstruktion (Statik) und Architektur. Das seinerzeitige Aufgabengebiet eines Baumeisters umfasste die heutigen Berufsfelder des Architekten, des Bauingenieurs und die eines Bauunternehmers.

Der "Baumeister" ist auch in Deutschland weiterhin eine geschützte Berufsbezeichnung. Architekten und Bauingenieure dürfen sich in der Regel nicht so nennen.


In Österreich und der Schweiz handelt es sich bei dem Begriff "Baumeister" immer noch um eine konkrete Berufsbezeichnung. Während Architekten und Bauingenieure überwiegend im planenden und kontrollierenden Bereich anzutreffen sind, stellt der Baumeister den einzigen universell einsetzbaren Baufachmann dar.

Der Baumeister ist berechtigt:


Die Baumeisterprüfung wird in den meisten Fällen von Handwerkern mit zusätzlicher theoretischer Ausbildung, aber auch von Ingenieuren nach drei Praxisjahren abgelegt. Nach Ablegung einer Baumeisterprüfung ist er zur Projektentwicklung, -leitung und Projektsteuerung berechtigt, zum Projektmanagement sowie zur Übernahme der Bauführung. Im Rahmen seiner Gewerbeberechtigung kann er seinen Auftraggeber vor Behörden und Körperschaften öffentlichen Rechts vertreten.
In Österreich und der Schweiz darf sich nur Baumeister nennen, wer die Baumeisterprüfung erfolgreich abgelegt hat. In Österreich ist der Baumeister ein Berufstitel (kein akademischer Titel) und wird mit BM oder Bmstr. abgekürzt. Hier ist die Baumeisterprüfung eine universelle Prüfung ohne Möglichkeit auf Nachsicht. In der Schweiz hingegen kann man die Baumeisterprüfung für Hochbau und für Tiefbau getrennt ablegen. Nach bestandener Prüfung darf man den Titel dipl. Baumeister führen. Die Abschlussprüfungen dauern insgesamt in der Schweiz nach abgelegten 14 Modulprüfungen 16 Stunden und in Österreich 123 Stunden.

Die Leiter der Bauausführung hießen oft Werkmeister ("wercmeistere") oder Baumeister; sie gingen zumeist aus dem Steinmetzhandwerk hervor und waren die mittelalterlichen Architekten. Auch Bezeichnungen wie "magister operis" kamen vor. Bei der Ausführung hatten der Steinmetzmeister ("magister lapicidae") und der Maurermeister ("magister caementari") sowie der "Sculptor" Bedeutung. Die Meister der Bauausführung wechselten bei jedem Bauwerk häufiger, schon auf Grund der langen Bauzeiten.

Bekannt wurden einige bedeutende Dombaumeister oder Bau- und Steinmetzmeister der Gotik:


Die Meister konnten oft an ihrem Steinmetzzeichen erkannt werden, eine im Mittelalter übliche Markierung, die sie auf ihrer Arbeit anbrachten.




</doc>
<doc id="7783" url="https://de.wikipedia.org/wiki?curid=7783" title="UCI">
UCI

UCI steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="7784" url="https://de.wikipedia.org/wiki?curid=7784" title="Hochbau">
Hochbau

Der Hochbau ist das Teilgebiet des Bauwesens, das sich mit der Planung und Errichtung von Bauwerken befasst, die mehrheitlich oberhalb der Geländelinie liegen (z. B. Gebäude wie Wohnhäuser oder Türme). Bauwerke, die sich mehrheitlich unterhalb oder auf der Geländelinie befinden, werden dem Tiefbau zugeordnet.

Die üblichen Hochbauprojekte unterscheiden sich im Wesentlichen durch ihre Nutzung und den damit verbundenen Anforderungen an Gestaltung, Wirtschaftlichkeit, Konstruktion und technischen Einrichtungen. Sie lassen sich nach Sommer in folgende acht Kategorien gliedern:

Neben dieser grundsätzlichen Einteilung gibt es aber immer auch Mischformen mit kombinierten Nutzungen wie beispielsweise Wohn- und Geschäftshäuser oder Produktionsgebäude mit Verwaltung. Auch lassen sich nicht immer alle Bauwerke eindeutig dem Hoch- oder Tiefbau zuordnen streng genommen mitunter weder dem Einen noch dem Anderen. (z. B. Brücken oder Schleusen). Abweichend von dieser Einteilung unterscheiden Historiker meist nur zwischen Sakralbauten und Profanbauten.

Die an einem Bauvorhaben Beteiligten lassen sich in fünf Gruppen zusammenfassen:

Die Planung kann auch vom Auftraggeber selbst übernommen werden, sofern er über
entsprechende Kompetenzen und Kapazitäten verfügt, dies gilt auch für die öffentliche Hand.

Im Hochbau kommen ausschließlich folgende Bauweisen zum Einsatz:

Welche dieser Bauweisen an einem bestimmten Bauvorhaben eingesetzt wird, hängt von individuellen Faktoren ab.

Der Hochbau wird in folgende Materialien kategorisiert:



</doc>
<doc id="7792" url="https://de.wikipedia.org/wiki?curid=7792" title="Systole">
Systole

Die Systole ([alt]griechisch – "[das] Zusammenziehen" oder "[die] Kürzung") ist ein Teil des Herzzyklus. Vereinfacht formuliert ist es die Anspannungs- und dadurch Blut-Ausströmungsphase des Herzens, im Gegensatz zur Diastole, der Erschlaffungs- und somit Blut-Einströmungsphase. Bei der Systole wird das Blut also aus der rechten und linken Herzkammer (Ventrikel) herausgepresst. Die Systole beschreibt die Pumpleistung des Herzens. Sie bestimmt den Puls und die Pulsamplitude.

Die Dauer der Systole bleibt auch bei Änderung der Herzfrequenz ziemlich konstant, wohingegen die Dauer der Diastole erheblich variiert. Die Systole ist beim erwachsenen Menschen etwa 300 Millisekunden lang.

Die Systole der Kammer wird in eine kurze mechanische "Herzmuskel-Anspannungsphase" und eine länger dauernde "Blut-Ausströmungsphase" unterteilt. Unmittelbar vor der Anspannungsphase sind die Kammern mit Blut gefüllt, Segel- und Taschenklappen sind geschlossen. Der Herzmuskel kontrahiert, wodurch der Druck ansteigt. In der Ausströmungsphase übersteigt der Druck in den Kammern den Druck in "" (Lungenarterie) und Aorta, die Taschenklappen öffnen sich, und Blut strömt in die großen Gefäße.

Die mechanische Ausströmungsphase beginnt

Die mechanische Ausströmungsphase endet

Vor der Systole sind die Kammern gedehnt und mit Blut gefüllt, die Vorhöfe sind kontrahiert. Während der Systole ziehen sich die beiden Kammern gleichzeitig zusammen. Das Blut wird somit in die Lungenarterie und die Aorta gepumpt, von wo aus es bis in die Peripherie von Körper- und Lunge strömt. Gleichzeitig erweitern sich die Vorhöfe und füllen sich mit Blut. Damit während der Systole das Blut nicht aus den Kammern in die Vorhöfe fließt, wird deren Zugang mit den "Segelklappen" ventilartig verschlossen. Nach der Systole erschlafft der Muskel (Diastole). Das in den Vorhöfen gesammelte Blut strömt durch die sich öffnenden Segelklappen in die Herzkammern. Die Taschenklappen sind nun geschlossen, damit das Blut, welches durch die Systole in die Aorta und Pulmonalarterie gepumpt wurde, nicht in die Kammern zurückfließen kann.

Die Erregbarkeit des Herzmuskels ist während der Systole aufgehoben (absolute Refraktärität).

Beachte: Die Systole des Vorhofes fällt in die späte Diastole der Herzkammer.



</doc>
<doc id="7793" url="https://de.wikipedia.org/wiki?curid=7793" title="Diastole">
Diastole

Die Diastole der Kammern des Herzens (griechisch "διαστολή" „die Ausdehnung“) ist die Entspannungs- und Füllungsphase, im Gegensatz zur Systole, der Anspannungs- und Austreibungsphase. Die Diastole der Vorhöfe findet während der Systole der Kammern statt.

Mechanisch beginnt die Diastole mit dem Erschlaffen der Kammermuskulatur und gleichzeitigem Schluss der Taschenklappen zu den großen Arterien. Sie endet mit dem Schluss der Segelklappen und Wiedereröffnung der Taschenklappen. Im EKG ist dies die Phase zwischen Ende der T-Welle bis Beginn der Q-Zacke. Teilweise wird als so genannte „elektrische Diastole“ die Phase zwischen Beginn der T-Welle und Beginn der darauffolgenden P-Welle bezeichnet. In anderer Literatur wird die elektrische Diastole mit der mechanischen gleichgesetzt. Echokardiographisch ist die diastolische Füllung der Ventrikel durch E- und A-Welle gekennzeichnet.

Die mechanische Diastole wird in vier Phasen unterteilt:
Störungen der Diastole können als Herzrhythmusstörung, z. B. Vorhofflimmern, auftreten oder als Einschränkung in der Qualität der diastolischen Füllung der Herzkammern. Diese kann zunächst ohne schwere Symptome aber bei Zunahme der Störung zur Leistungseinschränkung im Alltag durch Atemnot bei körperlicher Belastung bis hin zu Herzschwäche, dem diastolischen Herzversagen, führen.



</doc>
<doc id="7796" url="https://de.wikipedia.org/wiki?curid=7796" title="Apoptose">
Apoptose

Die Apoptose ( "apóptosis", von "apopíptein" ‚abfallen‘) ist eine Form des programmierten Zelltods. Es ist ein „Suizidprogramm“ einzelner biologischer Zellen. Dieses kann von außen angeregt werden (etwa durch Immunzellen) oder aufgrund von zellinternen Prozessen ausgelöst werden (etwa nach starker Schädigung der Erbinformation). Im Gegensatz zum anderen bedeutenden Mechanismus des Zelltods, der Nekrose, wird die Apoptose von der betreffenden Zelle selbst aktiv durchgeführt und ist somit Teil des Stoffwechsels der Zelle. Dadurch unterliegt diese Form des Zelltods strenger Kontrolle und es wird gewährleistet, dass die betreffende Zelle ohne Schädigung des Nachbargewebes zugrunde geht.

Im Unterschied zu den anderen Formen des programmierten Zelltods spielen bei der Apoptose proteolytische Enzyme, sogenannte Caspasen, eine zentrale Rolle.

Apoptose und Nekrose lassen sich normalerweise optisch leicht unterscheiden: Während bei der Apoptose ein Schrumpfen der Zelle einsetzt und ein Abbau der DNA durch Endonukleasen in definierte Stücke stattfindet (als DNA-Leiter bekannt und mittels Elektrophorese und sog. TUNEL-Methode nachweisbar), schwillt bei der Nekrose die Zelle an, wobei deren Plasmamembran zerstört wird. Als Folge kommt es zu lokalen Entzündungen, da Cytoplasma und Zellorganellen in den Extrazellularraum freigesetzt werden, welche durch Makrophagen (Fresszellen) beseitigt werden müssen. Im Vergleich zur Nekrose ist die Apoptose die häufigere Form des Zelltods. In bestimmten Fällen lassen sich Apoptose und Nekrose allerdings nicht scharf voneinander trennen. Der Übergang zwischen beiden Formen des Zelltods ist dann fließend und wird Aponekrose genannt.

Der deutsch-schweizerische Naturforscher Carl Vogt entdeckte 1842 als Erster die Apoptose beim Studium der Entwicklung von Kaulquappen der Gemeinen Geburtshelferkröte. Die große Bedeutung dieser Entdeckung wurde aber erst über 100 Jahre später erkannt. 1972 prägten John F. R. Kerr, Andrew Wyllie und Alastair R. Currie von der University of Aberdeen den Begriff ‚Apoptose‘ (engl. "apoptosis").

Während der Entwicklung eines Organismus ist Apoptose essentiell:

Aber auch im adulten Organismus ist sie unerlässlich:
Gegenwärtig wird die Apoptose besonders im Zusammenhang mit der Krebs­entstehung und verschiedenen Autoimmunerkrankungen erforscht. Ein Ziel der Krebsforschung ist es, kontrollierte Apoptose bei entarteten Zellen auszulösen. Doch auch die Krebszellen nutzen den Apoptosemechanismus, um menschliche Abwehrzellen, sogenannte tumorinfiltrierende Lymphozyten (TILs), auszuschalten. So findet man an der Oberfläche verschiedener Tumorzelllinien ein Apoptose-auslösendes Protein, den CD95-Liganden ("Fas Ligand"). Diesen Mechanismus bezeichnet man als "tumor counterattack".

Die Frage, welche Rolle Apoptose bei neurodegenerativen Krankheiten (wie z. B. Morbus Alzheimer, Chorea Huntington, Morbus Parkinson, ALS) spielt, wird derzeit ebenfalls heftig diskutiert, und auf diesem Gebiet laufen verschiedenste Forschungen.

Auch in einzelligen Organismen wurden Anzeichen von Apoptose gefunden. In "Saccharomyces cerevisiae" (Backhefe, Bierhefe) werden – besonders in alten Zellen – verschiedene Marker von Apoptose (DAPI, TUNEL-Färbung) sichtbar. Über evolutionäre Gründe für das Vorhandensein von Apoptose in Einzellern wird spekuliert. Eine Theorie besagt, dass sich einzelne schadhafte Zellen opfern und zum Wohle des Kollektivs „Suizid“ begehen. Dadurch werden Nährstoffe eingespart, die somit den anderen Zellen zur Verfügung stehen. Ziel ist es schließlich, das Genom zu erhalten, welches ja auch in den anderen Zellen praktisch identisch vorhanden ist.

Der Ablauf der Apoptose lässt sich lichtmikroskopisch verfolgen. Zuerst löst sich die betreffende Zelle aus dem Gewebsverband. Im weiteren Verlauf färbt sich die Zelle mehr und mehr eosinophil an und wird zunehmend kleiner. Außerdem bilden sich an der Zellmembran sichtbare Bläschen. Der Zellkern wird kleiner und dichter gepackt. Er kann im Verlauf der Apoptose auch in mehrere Teile zerfallen. Am Ende des Vorgangs bleibt ein homogen eosinophiles Apoptosekörperchen. Dieses wird dann durch Phagozytose abgebaut. Der programmierte Zelltod löst dabei keine Entzündungsreaktion aus.

Die Apoptose lässt sich mittels bildgebender Verfahren, wie beispielsweise Positronen-Emissions-Tomographie, Fluoreszenzbildgebung ("fluorescence imaging") sowie Magnetresonanztomographie makroskopisch "in vivo" nachweisen (molekulare Bildgebung). Als "Tracer" werden modifizierte Aminosäuren, wie (5-Dimethylamino)-1-napththalinsulfonyl-α-ethyl-fluoralanin (NST-732) oder "N","N"′-Didansyl--cystin, verwendet.

Der Vorgang der Apoptose lässt sich in zwei Phasen unterteilen: Initiations- und Effektorphase.

Bei der Initiationsphase unterscheidet man zwei Vorgänge: Den extrinsischen und den intrinsischen Weg. Man unterscheidet hiernach auch in Apoptose Typ I und Typ II.

Der extrinsische Weg wird eingeleitet durch Liganden­bindung an einen Rezeptor der TNF-Rezeptorfamilie (z. B. CD95). Diese sogenannten Todesrezeptoren besitzen in ihrem zytoplasmatischen Teil eine Todesdomäne (DD, „death domain“). Liganden sind zum Beispiel der Tumornekrosefaktor (TNF) und andere Zytokine, die beispielsweise von T-Lymphozyten abgesondert werden.

Durch die induzierte Trimerisierung des Rezeptors bilden die Todesdomänen eine Struktur, an die nun Adaptermoleküle mit eigener Todesdomäne durch homotypische Interaktionen binden können. In einem ersten Schritt wird das „TNF-Rezeptor-assoziierte Protein“ (TRADD) rekrutiert. Anschließend bindet an die DD des TRADD das „Fas-assoziierte Protein mit Todesdomäne“ (FADD). FADD besitzt neben der DD auch eine Todeseffektordomäne (DED, „death effector domain“), über die die proCaspase 8 mit ihrer DED an den Komplex bindet. Diese kann sich nun durch die entstandene hohe lokale Konzentration autokatalytisch aktivieren. Die aktive Caspase 8 löst ihrerseits die sogenannte Caspase-Kaskade aus, wodurch in einer signalverstärkenden Rückkopplung weitere Caspase-8-Moleküle aktiviert werden.

Über diesen Mechanismus sterben beispielsweise bei AIDS-Patienten auch zahlreiche nicht infizierte Leukozyten ab: Das HI-Virus regt mittels des Proteins Nef noch nicht erkrankte Abwehrzellen zum programmierten Zelltod an. Der Hemmstoff Fasudil kann diesen Mechanismus unterbinden.

Durch mangelnden Kontakt mit der extrazellulären Matrix werden Zellen ebenfalls apoptotisch. Dieser Vorgang wird als Anoikis bezeichnet.

Beim intrinsischen Weg oder der Apoptose des Typs II kommt es durch noch nicht genau bekannte Mechanismen zur Freisetzung von Cytochrom c und anderen pro-apoptotischen Faktoren wie Smac/DIABLO aus den Mitochondrien in das Zytoplasma.
Dieser Weg kann ausgelöst werden durch Tumor-Suppressoren, wie beispielsweise p53, einem Transkriptionsfaktor, der durch Schädigung der DNA aktiviert wird. p53 stimuliert die Expression pro-apoptotisch wirkender Mitglieder der Bcl-2 Familie (z. B. Bax, Bad). Diese führen dann zur Freisetzung der pro-apoptotischen Faktoren – wie etwa Cytochrom c – aus dem mitochondrialen Intermembranraum. Jedoch wirken viele toxische Substanzen, wie z. B. Chemotherapeutika, auch direkt auf die Mitochondrien und können so die Typ-II-Apoptose induzieren. Die Bindung von Cytochrom c und dATP an Apaf-1 (apoptotischer Protease-Aktivierungsfaktor-1) bewirkt eine Konformationsänderung des Proteins. Durch diese Konformationsänderung wird die Proteinbindedomäne CARD (Caspase-Rekrutierungs-Domäne) von Apaf-1 zugänglich, so dass sie an die CARD Domäne der Procaspase 9 binden kann. Die Bildung dieses Heterodimers ist eine Voraussetzung für die autolytische Aktivierung der Caspase 9. Dieser Komplex wird Apoptosom genannt und stellt die aktive Form der Caspase 9 dar. Analog zu Caspase 8 initiiert aktive Caspase 9 die Caspase-Kaskade.
Eine Signalverstärkung dieses Weges wird innerhalb der Caspase-Kaskade durch Caspase 7 vermittelt, welche nicht nur Substrate spaltet, die an der Ausführung der Apoptose beteiligt sind, sondern ihrerseits auch die Caspase 9 aktiviert.

Zellen, die vielleicht auf Grund einer zu geringen intrazellulären Menge an Caspase 8 nicht die Typ-I-Apoptose zu initiieren vermögen, können den mitochondrialen Weg zur Signalverstärkung aktivieren. Dazu spaltet die Caspase 8 das zytosolische Protein Bid („BH3 interacting domain death agonist“). Das entstehende C-terminale Spaltprodukt tBid („truncated Bid“) vermittelt nach der Translokation in die Mitochondrien die Freisetzung von pro-apoptotischen Faktoren und führt zur Aktivierung der Caspase 9.

Stressreaktionen des Endoplasmatischen Retikulums, die beispielsweise durch deregulierte Entleerung des ER-Calciumspeichers, Glucosemangel, Hypoxie oder missgefaltete Proteine (Unfolded Protein Response) hervorgerufen werden können, können Apoptose initiieren. Es gibt dabei einen Transkriptionsfaktor- und einen Caspase-abhängigen Signalweg.

Sogenannte Effektorcaspasen, vornehmlich Caspasen 3, 6 und 7 führen zum apoptotischen Tod der Zelle. Sie sind selbst aktiv am Abbau von Lamin (in der Zellkernmembran) und Actin (Teil des Zytoskeletts) beteiligt. Andererseits aktivieren sie sekundäre Zielproteine (z. B. Caspase aktivierte DNase, CAD, oder andere Caspasen) durch limitierte Proteolyse. Die DNase spaltet genomische DNA an internukleosomalen gekennzeichneten Regionen (linker region) und produziert 180–185 bp Fragmente. Dieses charakteristische Längenmuster lässt sich in einer Agarose-Gel-Elektrophorese als „Apoptoseleiter“ darstellen. Die Darstellung der „Apoptoseleiter“ ist deshalb eine sensitive Methode, um Apoptose vom ischämischen oder toxischen Zelltod abzugrenzen. Ein weiterer Aspekt ist die caspasevermittelte Unterdrückung der DNA-Reparatur.

Letztlich schnürt sich die Zelle nach und nach in kleinen Vesikeln ab, die wiederum durch spezialisierte „Fresszellen“ (Phagozyten) aufgenommen werden. Im Gegensatz zur Nekrose bleibt hierbei die Zellmembran intakt.

Der Austritt von Cytochrom c aus Mitochondrien ins Zytoplasma, der ein allgegenwärtiges Anzeichen für Apoptose ist, tritt beim extrinsischen Weg erst spät während der Apoptose auf und ist eher Resultat der Apoptose als ihr Auslöser.

Beim extrinsischen Weg unterscheidet man ferner zwischen aktiver (durch Aktivierung von Rezeptoren induziert) und passiver (ausgelöst durch Entzug von Wachstumsfaktoren, z. B. Neurotrophine) Apoptose.

Die wichtigsten bei der Unterdrückung der Apoptose beteiligten Proteine sind die anti-apoptotischen Mitglieder der Bcl-2 Familie (Bcl-2 und Bcl-x) und die IAPs (Apoptose-inhibitorische Proteine, engl. "inhibitor-of-apoptosis proteins"), wie beispielsweise Survivin. Weiter stromaufwärts liegen die Proteinkinase B (Alternativbezeichnung: Akt), z. B. in Zusammenhang mit Rezeptoren der Trk-Familie (siehe Neurotrophin) und Transkriptionsfaktoren der FOXO-Familie sowie der Transkriptionsfaktor NF-κB.

In Vielzellern werden sterbende (apoptotische) Zellen schnell und effizient von spezialisierten oder dafür vorbereiteten Fresszellen (Phagozyten) entfernt. Das gängige Konzept besagt, dass die Beseitigung dieser Zellen ohne Entzündung (Inflammation) verläuft oder sogar eine entzündungshemmende (anti-inflammatorische) Reaktion auslöst. Im Gegensatz dazu löst die Beseitigung nekrotischer Zellen eher eine entzündungsfördernde (pro-inflammatorische) Reaktion aus. Nicht nur die sterbende Zelle selbst, sondern auch die während des Zelltodes freigesetzten Substanzen, tragen zum Prozess der Beseitigung der toten Zellen und der daraus folgenden Antwort des Immunsystems bei.

Kontrollierter Zelltod ist für die Homöostase multizellulärer Organismen von existentieller Bedeutung. Während der permanenten Zellerneuerung muss der Körper täglich Milliarden durch Apoptose entstandene Zellleichen entfernen. Eine effiziente Clearance apoptotischer Zellen ist von fundamentaler Bedeutung, weil diese andernfalls dazu tendieren, sekundär nekrotisch zu werden, intrazelluläre Bestandteile freizusetzen und dadurch Entzündung und Autoimmunität auszulösen.

In gesunden, multizellulären Organismen werden apoptotische Zellen unverzüglich entweder von phagozytosefähigen Nachbarzellen oder von spezialisierten Fresszellen (Phagozyten) aufgenommen. Das Problem, das sich aus diesem Szenario ergibt, ist, wie es den spezialisierten Phagozyten gelingt, ihre Beutezelle rechtzeitig zu erreichen, insbesondere, wenn sie sich nicht in der direkten Umgebung der sterbenden Zellen aufhalten. Eine Möglichkeit ist, dass sterbende Zellen lösliche Mediatoren absondern, die die Phagozyten anlocken. Im Überstand apoptotischer Zellen wurden folgende Stoffe als „find-me“-Signale (Chemoattraktantien) identifiziert:

Die Untersuchung der Clearance apoptotischer Zellen hat ein komplexes Netzwerk von Interaktion und Kommunikation zwischen sterbenden Zellen und Phagozyten aufgedeckt. Neben Zell-Zell-Kontakten sind lösliche Faktoren beteiligt, die von apoptotischen Zellen freigesetzt werden. Es wurden viele verschiedene Chemoattraktantien beschrieben, die den Prozess der Clearance sterbender Zellen organisieren. Diese pleomorphen Mediatoren sind außerdem an der Regulation der Immunantwort beteiligt. Sie bestimmen den Grad der Inflammation und steuern damit die Entscheidung zwischen Immunaktivierung und Toleranzinduktion. Interessanterweise führen physische Störungen, wie chronische Inflammation, Autoimmunität und der Verlust über die Kontrolle von Tumoren zu einer Deregulation der Produktion und/oder der Funktion einiger dieser Faktoren. Daher sind „find-me“-Signale vielversprechende Biomarker für verschiedene Krankheiten und damit potentielle Targets künftiger therapeutischer Interventionen.

Die Clearance apoptotischer Zellen ist der letzte Schritt bei der Entfernung alter, beschädigter, infizierter und gefährlicher Zellen in den Geweben multizellulärer Organismen. Der Prozess schont dabei das umgebende Gewebe so gut wie möglich. Apoptotische Zellen durchlaufen enorme morphologische Veränderungen. Dazu gehören Kontraktion, Membran-Blebbing (Bläschenbildung) und eine apoptotische Zellform. Das Membran-Blebbing trägt aktiv zur Erkennung und Aufnahme toter Zellkörper und zur Induktion autoreaktiver Antikörper bei. Unter Blebbing versteht man die Bildung von Membranbläschen auf der Oberfläche der apoptotischen Zelle. Diese Bläschen sind von Lipiden der Zytoplasmamembran umgeben und enthalten Teile des Inhaltes der sterbenden Zelle. Blebs sind ballonförmige Bläschen, die sich durch Auswölbung der Plasmamembran auf der Zelloberfläche bilden. Sie entstehen in einem dynamischen Prozess während der Apoptose auf der gesamten Zelloberfläche. Der Bildung von Blebs geht ein erhöhter hydrostatischer Druck in der Zelle voraus, der durch die Actomyosin-gesteuerte Kontraktion der Zelle verursacht ist. Neugeformte Blebs enthalten noch kein Aktin oder andere zytoskeletale Proteine. Später polymerisieren dann schnell zytoskeletale Vorläuferproteine, was zur Rückbildung der Blebs führt. Der Vorgang der Bleb-Bildung und -Rückbildung wiederholt sich während des Prozesses der Apoptose. In der späten Apoptose können individuelle Blebs mit Zellorganellen und kondensiertem Chromatin gefüllt werden. Abgeschnürte Chromatin-gefüllte Blebs können als Viromimetika zur Induktion anti-nukleärer Antikörper beitragen. Oberflächenblebs und abgegebene membranumhüllte Mikropartikel werden meist von Makrophagen im näheren Umfeld aufgenommen.

Für ihre Entdeckungen, die genetische Regulation der Organentwicklung und des programmierten Zellsterbens betreffend, erhielten die Wissenschaftler Sydney Brenner (Großbritannien), H. Robert Horvitz (USA) und John E. Sulston (Großbritannien) im Jahre 2002 den Nobelpreis für Medizin.




</doc>
<doc id="7797" url="https://de.wikipedia.org/wiki?curid=7797" title="Hans Baluschek">
Hans Baluschek

Hans Baluschek (* 9. Mai 1870 in Breslau; † 28. September 1935 in Berlin) war ein deutscher Maler, Grafiker und Schriftsteller. Er gehörte zur Berliner Secession, war Mitglied im Deutschen Künstlerbund und gehörte dem Verband Deutscher Illustratoren an. Nach 1920 war er aktives Mitglied der SPD. 1929 bis Anfang 1933 war er Leiter der Großen Berliner Kunstausstellung.

Baluschek war ein Hauptvertreter des deutschen kritischen Realismus, wobei er selbst jede Form des „-ismus“ für seine Kunst ablehnte, und stellte anklagend das Leben des Proletariats dar. Seine Bilder haben entsprechend vor allem die Menschen des Arbeiterstandes in Berlin zum Thema.

Bekannt wurde er vor allem durch seine Gemälde, Illustrationen von Büchern wie Peterchens Mondfahrt und Beiträgen für verschiedene Zeitschriften der Weimarer Republik.

Hans Baluschek war der Sohn von Franz Baluschek, Regierungslandmesser und Eisenbahningenieur. Er hatte drei Schwestern, von denen allerdings zwei bereits im Kindesalter an Tuberkulose verstarben. Durch die Euphorie in Breslau als preußischer Residenzstadt nach dem Deutsch-Französischen Krieg 1870/1871 versuchte sich Franz Baluschek als selbstständiger Unternehmer im Eisenbahnbereich und wirkte vor allem in Haynau (heute: Chojnów), das entsprechend für seinen Sohn neben Breslau zu dessen Hauptwohnorten wurde. Durch den Vater wurde zudem die Faszination für die Eisenbahn bereits in der frühen Kindheit erstmals manifestiert.

Im Jahr 1876 zog die Familie mit dem erst sechsjährigen Hans Baluschek nach Berlin, und bis 1886 wechselte sie insgesamt fünfmal die Wohnung, wobei sie immer in den sich ausbreitenden Neubaugebieten für Arbeiter vor dem Halleschen und dem Kottbusser Tor, dem heutigen Berlin-Kreuzberg blieb. Berlin befand sich zu dieser Zeit in einer durch die Weltwirtschaftskrise 1873 ausgelösten Depression und insbesondere die private Eisenbahnindustrie befand sich nach dem Zusammenbruch der Unternehmen von Bethel Henry Strousberg in einer sehr schwierigen Lage. Franz Baluschek arbeitete als königlicher Eisenbahningenieur bei der staatlichen Eisenbahn, in die die privaten Unternehmen überführt wurden, und konnte so die Familie ernähren, die in bürgerlichem bis kleinbürgerlich-proletarischem Milieu inmitten von anderen Arbeiterfamilien lebte. Nach dem Besuch der Gemeindeschule wurde Hans Baluschek mit neun Jahren in das Ascanische Gymnasium aufgenommen, das 1875 gegründet worden war. Es gehörte zu den wenigen höheren Schulen in Berlin, welche die Schüler auf der Basis eines humanistischen und naturwissenschaftlichen Lehrplans unterrichtete.

In den Jahren 1882 bis 1886 stellte der russische Künstler Wassili Wereschtschagin in mehrere Bildzyklen seine Gemälde vom Russisch-Osmanischen Krieg 1877–1878 und andere Kriegsdarstellungen aus, die in Berlin viel diskutiert wurden und den Künstler aufgrund seiner Inhalte und des ungewohnten Realismus populär werden ließen. Für Baluschek stellte der Besuch der Ausstellungen ein entscheidendes und prägendes Erlebnis dar. Er begann damit, Bilder zu kopieren und selbst zu malen. In seinen frühen Werken versuchte er sich unter anderem an Kriegsdarstellungen, die Wereschtschagin nachempfunden waren; auch in späteren Kriegsbildern zeigt sich der deutliche Einfluss dieses Vorbilds.

Sein Vater wurde 1887 für den Eisenbahnbau auf der Insel Rügen nach Stralsund versetzt, wo Baluschek die beiden letzten Jahre seiner Schulzeit bis zum Abitur verbrachte. Hier traf er auf den Lehrer Max Schütte, der seine Schüler mit den Ideen und Zielen des Sozialismus vertraut machte und über Klassenstrukturen der Gesellschaft und ökonomische Zusammenhänge aufklärte. Aufgrund des noch gültigen Sozialistengesetzes wurde Schütte aus dem Lehrdienst entlassen. Baluschek und seine Mitschüler begannen mit dem Studium sozialistischer Schriften und den in Deutschland populär werdenden Schriften Leo Tolstois und Emile Zolas. 1889 beendete Baluschek seine Schullaufbahn mit dem Abitur und dem Wunsch, Maler zu werden.

Nach seinem Abitur erhielt Hans Baluschek noch im selben Jahr die Zulassung für das Studium an der Königlichen Akademie der bildenden Künste in Berlin und lernte hier Martin Brandenburg kennen, mit dem ihn lebenslang eine enge Freundschaft verband. Die Hochschule wurde von Anton von Werner geleitet, der sie trotz vieler Neuerungen sehr konservativ führte. Er lehnte vor allem die durch den deutschen Impressionismus geprägten Strömungen um die gerade populär werdenden Maler Max Liebermann, Lesser Ury und Franz Skarbina ab und war bemüht, keinerlei Einfluss dieser künstlerischen Ausprägungen in den Unterricht der Akademie einfließen zu lassen. Stattdessen legte er Wert auf bewährte Themen der akademischen Malerei und stellte vor allem die Historienmalerei, die in der offiziellen Kunstwahrnehmung die höchste Wertschätzung genoss, in den Fokus der Ausbildung.

Baluschek wohnte in Berlin-Schöneberg; sein ältestes bekanntes Skizzenbuch stammt aus dem Jahr 1889 und zeigt ihn in einem Selbstbildnis als Student mit Mütze und Band in der Couleur eines Corpsstudenten. Ob er Teil einer Studentenverbindung war, ist allerdings nicht bekannt; spätere Bilder zeigen Kenntnisse der Organisation und auch in seinen Novellen wird das Thema aufgegriffen. In den frühen Arbeiten finden sich zudem auffällig häufig Kriegsszenen und militärische Kampfszenen neben Darstellungen des Stralsunder und des Berliner Straßenlebens. in den 1890er Jahren nimmt die Anzahl der Darstellungen der sozialen Klassenunterschiede und des Arbeiterlebens in Berlin deutlich zu, wodurch er sich von der akademischen Malerei zunehmend löste.

Im Sommer 1893 beendete Baluschek sein Studium an der Akademie, um als freier Künstler zu arbeiten. Anders als die meisten akademisch ausgebildeten Maler konzentrierte er sich weiter auf die Klassenunterschiede und wurde so sehr schnell zu einem Außenseiter des wilhelministischen Kunstbetriebs. Er ließ sich vor allem durch die Schriften von Gerhart Hauptmann, Leo Tolstoi, Henrik Ibsen, Johannes Schlaf und Arno Holz beeinflussen, die den Mittelpunkt der naturalistischen Literaturbewegung in Berlin darstellten und verband sie mit seinen Studien theoretischer Schriften der sozialistischen Literatur sowie weiterer Studien der Medizin, Philosophie und Volkswirtschaft.

Die Hauptzeit der künstlerischen Findung Baluscheks begann 1894 und reichte bis zum Beginn des Ersten Weltkriegs im Jahr 1914. In dieser Zeit entwickelte er seine individuelle Position in der Kunstszene Berlins, in der er die Opposition zur traditionellen akademischen Malerei zunehmend verstärkte und Freundschaften mit Gleichgesinnten aufbaute. Diese fand er vor allem unter Künstlern des Umfelds Liebermanns. Seine Motive stellten vor allem die Randbereiche Berlins dar, in denen durch die Baustellen für den Wohnungsbau und die Eisenbahn ein enormes Wachstum stattfand. Den Fabrikanlagen, Friedhöfen und vor allem den Menschen, die er als Protagonisten seiner Werke nutzte, begegnete er hier. Der literarische Naturalismus wurde für ihn zur entscheidenden künstlerischen Prägung, die seinen Kampf gegen die Konventionen und die Autorität der Inhalte und der Formalia begleitete und seinen sehr eigenständigen Stil bis in das 20. Jahrhundert definierte.

In seinem 1894 entstandenen Bild "Mittag", in dem er einen Ausschnitt aus einem Zug von Frauen und Kindern darstellte, die in Körben ihren Männern in den Fabriken das Mittagessen bringen, zeigt sich diese Prägung sehr deutlich. Die Protagonistinnen sind „durch die gleiche endlose Schufterei und die kaum unterschiedlichen dürftigen Wohnbedingungen […] zu entindividualisierten Typen geworden. […] Jede einzelne der Frauen ist lediglich Bestandteil der Menge, denn nicht die Einzelpersonen, sondern die in gleichen Verrichtungen funktionierende Menschenmenge stellt einen gesellschaftlichen Faktor dar.“

Beim "Eisenbahner-Feierabend" aus dem Jahr 1895 wird dieser Inhalt fortgeführt. Die Personenmasse wird hier durch die Arbeiter selbst dargestellt, die vor einem Hintergrund aus Bahnanlagen, Schornsteinen und Oberleitungen müde von der Arbeit kommen und teilweise von ernst blickenden Kindern empfangen werden. Zur Zeit der Entstehung unterhielt Baluschek eine freundschaftliche Beziehung zu dem Literaten Richard Dehmel, der durch Gedichte wie "Der Arbeitmann" und "Vierter Klasse" bekannt wurde und dessen 1896 erschienene Gedichtsammlung "Weib und Welt" ein von Baluschek entworfenes Deckblatt bekam. Baluschek zeichnete 1897 ein Porträt des Lyrikers. Weitere Verbindungen bestanden zu Hermann Bang, Cäsar Flaischlen, Hans Land und vor allem Arno Holz, zu dessen engerem Freundeskreis er gehörte. 1897 spielte Baluschek in Holz’ selbst finanzierter Vorführung der "Sozialaristokraten" unter dem Pseudonym Fritz Gieseke die Rolle des „Sprödowski“, seine erste und einzige Rolle als Schauspieler. Holz wird für Baluschek als Schlüsselfigur des Naturalismus und geistiger Mentor betrachtet, wobei Baluscheks Arbeiten erst begannen, als der literarische Naturalismus bereits abebbte.

Baluschek entwickelte eine eigene Maltechnik, die vor allem auf Aquarellen und Gouachen aufbaut, Ölfarben benutzte er dagegen vergleichsweise selten. Der Untergrund wurde mit Ölkreidestiften vorbereitet, um einen sehr farbigen und zugleich stumpfen Gesamteindruck zu bilden. Laut Baluschek sollte dies der Berliner Atmosphäre entsprechen, „wie ich sie mir in ihrem grauen Charakter empfinde.“ Er schrieb weiter: „Mir war die Ölfarbe für diesen Zweck zu satt und zu speckig; außerdem gestattet sie mir bei den verhältnismäßig kleinen Formaten nicht den scharfen Ausdruck der Gesichtslinien meiner Figuren und gewisse Einzelheiten, wie der gespitzte Stift, mit dem ich farbig zeichnen konnte.“

In der zweiten Hälfte der 1890er Jahre trat Baluschek mehr und mehr in das Bewusstsein der Berliner Kunstszene, vor allem durch seine Ausstellungen in den Jahren 1895, 1896 und 1897 in der Galerie Gurlitt gemeinsam mit Martin Brandenburg, bei denen er erstmals seine Bilder einem größeren Publikum präsentierte. Obwohl es bereits vorher Darstellungen aus dem Berliner Klein- und Spießbürgertum gab und auch Max Liebermann, Franz Skarbina, Fritz von Uhde und andere Maler des deutschen Realismus Darstellungen aus der Arbeitswelt und Großstadtszenen malten, waren Baluscheks Bilder für seine Zeit neuartig und außergewöhnlich. Laut Bröhan (2002) unterschied sich Baluschek „durch eine direkte Wahrhaftigkeit, die seinen gemalten Wirklichkeitsausschnitten etwas beunruhigend Provozierendes gaben“. Die Darstellung der unmenschlichen Lebensumstände und der trostlosen Arbeitsbedingungen kamen hinter der oftmals amüsanten Fassade hervor. Der Kritiker Willy Pastor zeigte auf, dass sich „in dieser harmlosen Novellistik etwas verbarg, daß mehr war als bloße Erzählung“. Nach seiner Darstellung gingen die Kritiker amüsiert von Bild zu Bild oder wandten sich ab, weil Baluschek zum „geschmacklosen Volke der Naturalisten“ gehörte und sich durch „zu wenig Parfüm, zu viel Pfütze“ auszeichnete.

Deutlich wird dieser Kontrast unter anderem bei Werken wie "Vergnügungspark – In der Hasenheide" (1895), in dem die oberflächliche Feststimmung durch die Gesichter der Protagonisten und die Darstellung der Jahrmarktbuden relativiert wird. In dem Bild "Hier können Familien Kaffee kochen" (1895) wird die an sich kommunikative Darstellung von sechs Frauen vor Kaffeekannen durch die verlebten und faltigen Gesichtszüge durchbrochen, während in "Tingeltangel" (1890) das Innere eines mit Kaiserbüste und schwarz-rot-goldenem Behang geschmückten Vergnügungsetablissements dargestellt und durch die Darbietungen eines Lustmädchens kontrastiert wird. Im "Berliner Rummelplatz" mit einer farbenprächtigen Karusselldarstellung wird einem zigaretterauchenden Arbeiterjungen ein luftballonaufblasendes Kind gegenübergestellt. Einen Vorgriff auf die Neue Sachlichkeit stellt das Aquarell "Neue Häuser" (1895) dar, dass ohne Schönung einen monotonen und menschenleeren Häuserkomplex in Fabriknähe zeigt.

Aufgrund der Unzufriedenheit der Berliner Künstler gegenüber der Vormachtstellung der offiziellen Kunstanschauung des Anton von Werner und die überfüllten Kunstausstellungen mit großen Bildermengen kam es im auslaufenden 19. Jahrhundert in Berlin zu einer Spaltung der Kunstszene. Unter der Leitung Leistikows wurde 1892 die Vereinigung der XI als exklusive Ausstellungsgemeinschaft gegründet. Auch Baluschek wurde gebeten, sich an den Ausstellungen der Vereinigung zu beteiligen.

Durch den Skandal um die Absetzung einer Ausstellung Edvard Munchs im Herbst 1892 durch Anton von Werner kam es zu weiterer Unzufriedenheit innerhalb der Berliner Künstlerschaft, die 1898 in der Gründung der Berliner Secession durch die modernen Künstler der Stadt, ebenfalls angeführt von Leistikow, mündete. Auch Baluschek gehörte zu den Gründungsmitgliedern der Secession und wurde zum Schriftführer gewählt. Gemeinsam mit Käthe Kollwitz, Otto Nagel und Heinrich Zille vertrat er die bodenständige und sozialkritische Kunst in der Secession, wodurch sie sich von den weitgehend durch den französischen Impressionismus, Pointillismus und Symbolismus beeinflussten Künstlern der Vereinigung unterschieden. Während Zille und Kollwitz als Zeichner allerdings auf die Schwarz-Weiß-Ausstellungen der zeichnenden Künste angewiesen waren, konnte Baluschek seine Gemälde regelmäßig auf den Ausstellungen der Secession präsentieren und stellte damit eine ständige Provokation für die konservativen Kreise dar. Bereits das Bild "Singknaben" (1895), das Baluschek zur ersten Ausstellung der Secession 1899 präsentierte, kontrastierte mit der Gesellschaft, die in eleganter Garderobe zu diesem gesellschaftlichen Ereignis erschienen war. Während man in Folge die „harmlosere Seite der Secession“ durchaus als Gewinn betrachtete, war die „Elendsmalerei“ beispielsweise für den nationalliberalen Reichstagsabgeordneten Waldemar von Oriola ein „zügelloses Produkt jenseits ästhetischer Normen“.

Baluschek heiratete 1902 die Theaterschauspielerin Charlotte von Pazatka-Lipinsky, die er einige Jahre vorher durch seine Verbindungen zur Theaterwelt kennengelernt hatte. 1900 schuf er eine gemalte Liebeserklärung in Form eines Märchenbildes, auf der er selbst als Elfenritter einer Dame mit des Gesichtszügen von Charlotte von Pazatka-Lipinsky eine Rose überreicht. Gemeinsam mit ihr zog er in ein Haus in der Klopstockstraße in Berlin-Tiergarten. Die anfangs sehr romantische Ehe verlief jedoch unbefriedigend und wurde 1913 kinderlos geschieden.

Im Jahr 1904 erschien in der Reihe "Moderne Illustratoren" erstmals eine Monographie über Hans Baluschek von Hermann Eßwein, angeregt durch den Verleger Reinhard Piper. Die Serie, die neben Baluschek die Illustratoren Thomas Theodor Heine, Eugen Kirchner, Adolf Oberländer, Edvard Munch, Henri de Toulouse-Lautrec und Aubrey Beardsley porträtierte, konzentrierte sich auf die Arbeiten zur Buchillustration, zeigte jedoch im Fall von Baluschek neben seinen Märchenillustrationen vor allem seine Bilder zu Berlin.

1908 wurde Baluschek Teil des Vorstands der Berliner Secession; in der Folge geriet jedoch auch diese zunehmend in die Kritik. Die Offenheit, die sie bei ihrer Gründung gegenüber der neuen Malerei des Impressionismus zeigte, wandelte sich mit dem Aufkommen des Expressionismus. Max Liebermann verhinderte als Leiter der Secession eine Ausstellung von Henri Matisse; andere Künstler wie das Secessionsmitglied Max Beckmann beschwerten sich über „eine unverschämte Frechheit nach der anderen“. 1910 kam es nach Ablehnung weiterer Künstler zur Abspaltung der Neuen Secession um Georg Tappert und Max Pechstein und zur „Ausstellung von Werken Zurückgewiesener der Berliner Secession“. 1913 wurde mit der Herbstausstellung mit Werken von Edvard Munch, Pablo Picasso und Ernst Ludwig Kirchner ein letzter Versuch unternommen, die Situation in der Berliner Secession zu beruhigen. Im selben Jahr führten jedoch massive Vorwürfe gegen Paul Cassirer in seiner Doppelfunktion als Jurymitglied der Secession und als Kunstverkäufer zum Austritt von 42 Künstlern aus der Secession. Darunter befanden sich Max Liebermann und der gesamte Vorstand, die nun die Freie Secession gründeten. In der Berliner Secession blieb vor allem Lovis Corinth als international bekannter Künstler zurück, der die Vereinigung weiterhin leitete.

Baluschek heiratete im Jahr nach seiner Scheidung seine ehemalige Malereischülerin Irene Drösse, die 25 Jahre jünger war als er. Mit ihr blieb er bis zu seinem Tod zusammen. In den Weltkriegsjahren 1916 und 1918 brachte sie die gemeinsamen Töchter Regine und Renate zur Welt.

Der Erste Weltkrieg hatte sowohl auf das künstlerische Umfeld in Berlin wie auch auf die einzelnen Künstler einen großen Einfluss. Die Kriegserklärung des Deutschen Reichs gegen Russland und Frankreich führte in der Bevölkerung zu einer Entladung aufgestauter Spannung, die durch eine vorher stattgefundene kriegstreibende und aggressive Stimmung aufgebaut wurde. Auch in der Künstlerschaft kam es zu kriegsoptimistischen Äußerungen, etwa durch Lovis Corinth, Karl Scheffler oder Thomas Mann; patriotische Arbeiten entstanden. Nur wenige Künstler wie Käthe Kollwitz und Otto Nagel ließen sich von dieser Stimmung nicht mittreiben. Zur künstlerischen Unterstützung des Krieges erschienen Zeitschriften wie die von Paul Cassirer herausgegebene "Kriegszeit", für die auch Max Liebermann und Hans Baluschek Arbeiten beisteuerten. Heinrich Zille erfand für den "Ulk" die humoristischen Figuren „Vadding und Korl“, „die das Fronterlebnis als unfreiwilligen Sonntagsspaziergang erscheinen lassen“, und für die wöchentlich erschienenen "Künstlerblätter zum Krieg" arbeiteten neben Liebermann, Corinth, Zille und Baluschek auch Philipp Franck, Friedrich Kallmorgen und Martin Brandenburg. Auch Max Slevogt, Gerhart Hauptmann, Ernst Barlach, August Gaul und viele andere beteiligten sich mit ihren Arbeiten an der patriotischen Unterstützung der Kriegstruppen oder meldeten sich wie Richard Dehmel, Erich Heckel und Max Beckmann sogar freiwillig zum Armeeeinsatz.

Wie bei anderen war die Beteiligung an dieser Unterstützung auch bei Baluschek auf eine trotz seiner Auflehnung grundsätzlich positive Einstellung gegenüber der konstitutionellen Monarchie und zugleich einer seit langem vorhandenen Unzufriedenheit über die Bevorzugung vor allem der französischen Kunst in der deutschen Künstlerszene zurückzuführen. Bereits in den Vorjahren hatte Baluschek sich an Kunstausstellungen des Werdandibundes 1907/1908 beteiligt und militaristische Werke zur Erinnerung an die Befreiungskriege gegen Napoleon Bonaparte 1813–1815 durch Zeichnungen von Militärangehörigen in privater Umgebung unterstützt. Aufgrund der sich rasch abzeichnenden antisemitischen und intoleranten Einstellungen des Bundes brach er mit dem Zusammenschluss.

Im Jahr 1915 erschien eine Mappe mit dem Titel "Der Krieg 1914–1916" mit 22 Bildern Baluscheks, die vom Verband der deutschen Kranken-Pflegeanstalten vom Roten Kreuz herausgegeben wurde. Sie enthielt einen „glühend patriotischen Text“ des Historikers Richard Du Moulin-Eckart, der von Zeichnungen Baluscheks von modernem Kriegsgerät wie Mörsern und anderen Geschützen, U-Booten, Flugzeugen und Zeppelinen illustriert war. Hinzu kamen zwölf ganzseitige Farbtafeln mit Kriegsszenen, „in denen unter die Kriegsfurie geratene Menschen in grauenhaften Szenen [bei] der Vernichtung des Feindes“ gezeigt werden. Abgebildet sind Kriegszerstörungen, Verwundete und Leichen in verschiedenen Kriegsszenen, wobei die Tafelserie in dem Bild "Die Hilfe" endet, auf dem ein Rot-Kreuz-Zelt mit Verwundeten dargestellt ist.

Baluschek meldete sich wie andere Kollegen ebenfalls wahrscheinlich freiwillig zum Kriegsdienst und wurde im Landsturm 1916 an der Westfront und später im Osten eingesetzt. Während dieser Zeit illustrierte er weiterhin Kriegsszenen, u. a. im "Wachtfeuer", die jedoch nüchterner wurden und die die von den Frauen übernommenen Dienste ihrer Männer zeigen. Baluscheks enger Freund Martin Brandenburg wurde bereits 1915 durch einen Kopfschuss schwer verwundet und verlor ein Auge, 1919 starb er an den Folgen dieser Kriegsverletzung. In seinem Bild "Zur Heimat", bei dem ein Sarg unter soldatischer Ehrbezeigung verladen wird, verarbeitete Baluschek 1917 den Kontrast zwischen der vaterländischen Hingabe des Soldaten und der Opferung seines Lebens. Das Ende des Krieges und vor allem der für Deutschland katastrophale Ausgang erschütterten Baluschek und viele andere. Die Novemberrevolution 1918 nahm er nur aus der Distanz wahr. Baluschek malte 1918 nur wenig, sein Œuvre beschränkt sich in dieser Zeit auf wenige Zeichnungen der Berliner Straßenkämpfe und ein "Selbstporträt", das Baluschek in ruhiger Konzentration zeigt.

In den Folgejahren traten vor allem Illustration von Märchen in den Vordergrund. Einem breiten Publikum sind bis heute seine Illustrationen zu "Peterchens Mondfahrt" aus dem Jahr 1919 vertraut, die er im Auftrag des Klemm-Verlags für das von Gerdt von Bassewitz geschriebene Märchen schuf. Für diesen Auftrag malte und zeichnete Baluschek 16 ganzseitige Farbtuschzeitungen und 37 Federzeichnungen. Bereits in früheren Jahren hatte er sich gelegentlich mit Fantasiedarstellungen befasst und sich als Buchillustrator einen entsprechenden Ruf erarbeitet – die Bilder zu "Peterchens Mondfahrt" wurden zu seinen bekanntesten Märchenillustrationen. Anders als etwa Max Slevogt, der im Auftrag von Bruno Cassirer in den 1920er Jahren Märchen illustrierte, konnte sich Baluschek in die Gedankenwelt der Kinder eindenken und schuf entsprechende fantasievolle Bilder.

Hans Baluschek illustrierte weitere Kinder- und Märchenbücher für den Klemm-Verlag, darunter "Was der Kalender erzählt" (1919), "Pips, der Pilz" (1920), "In’s Märchenland" (1922), "Prinzessin Huschewind" (1922) und "Von Menschlein, Tierlein, Dinglein" (1924). Zudem illustrierte er für den Comenius Verlag eine Ausgabe von "Grimms Märchen" (1925). Hinzu kamen Kostümzeichnungen, Plakate und Bühnenbildentwürfe für das Theater und teilweise auch für den Film. Außerdem gestaltete er 1927 die Kellerräume der Weinstube Lutter & Wegner mit phantasievollen und zugleich humoristischen Szenen aus Berlin.

Baluschek war, wie viele andere Künstler, durch den Ausgang des Krieges in eine Krise geraten, zugleich nutzte er jedoch die sich bietenden Möglichkeiten zur Neugestaltung aktiv. Er entschloss sich, die am 11. August 1919 in Weimar ausgerufene Weimarer Republik aktiv zu unterstützen und vor allem im Bereich der Kultur und Bildung Einfluss zu nehmen. So war er 1920 unter den ersten Organisatoren und Dozenten der neu gegründeten "Volkshochschule Groß-Berlin" und lehrte dort Malerei. Bereits 1919 gehörte er dem amtlichen Filmprüfungsausschuss an, wo er versuchte, den oberflächlichen Unterhaltungsfilmen mit der Förderung politischer Filme entgegenzuwirken. Der 1929 von Piel Jutzi gedrehte Film "Mutter Krausens Fahrt ins Glück", der als erster echter Zille-Film gefeiert wurde, stand unter dem Protektorat von Baluschek, Otto Nagel und Käthe Kollwitz. Ebenfalls 1919 gehörte er zu den Gründern des "Bundes für proletarische Literatur", und 1924 wurde er neben Arno Holz, Martin Andersen Nexø, Karl Henckell, Paul Kampfmeyer und Friedrich Wendel in den literarischen Beirat des sozialdemokratischen "Bücherkreises" berufen.

Im Jahr 1920 trat er in die SPD ein und wurde Vorsitzender der Kunstdeputation in Schöneberg. Ebenfalls 1920 erschien sein Novellenband "Enthüllte Seelen". Gemeinsam mit den Schauspielern Erwin Piscator und Leopold Jessner wurde er unter dem Vorsitz von Berlins Oberbürgermeister Gustav Böß Bürgerdeputierter in der "Deputation für Kunst- und Bildungswesen" und damit zuständig für Wirtschaftsfragen im Bereich der Kunst und Künstler. Er spielte eine führende Rolle bei der Gründung der "Unterstützungskasse Berliner Künstler". Im Reichsverband bildender Künstler Deutschlands wurde er zeitweise Vorsitzender.

Baluschek zeichnete für die Zeitschriften "Der wahre Jakob", "Lachen links", "Frauenwelt", "Kulturwille", "Der Bücherkreis", "Proletarier" und die "Illustrierte Reichsbannerzeitung" sowie für Schulbücher und Romane, wobei sich seine Begeisterung vom technischen Fortschritt, insbesondere für den Schienenverkehr zeigte. Innerhalb der SPD gehörte Baluschek dem linken Flügel an. Er hatte keine Berührungsängste mit kommunistischen Aktivitäten. Sein Gemälde "Zukunft" von 1920 erschien als Titelblatt der kommunistischen Zeitschrift "Sichel und Hammer". Zu den Amsterdamer Internationalen Antikriegstagen 1924 brachte Otto Nagel die Broschüre "8 Stunden" der Künstlerhilfe heraus, eine Reaktion auf den Aufruf der KPD zum „Aufruf zur Erhaltung des 8-Stunden-Tags“, der unter anderem von Baluschek, Zille, Dix, Grosz, Sella Hasse, E. Johansson, Völker, Schlichter und E. Hoffmann unterzeichnet wurde.

Baluschek eröffnete 1923 gemeinsam mit dem Reichspräsidenten Friedrich Ebert die Große Berliner Kunstausstellung und wurde 1929 bis 1933 wurde deren Leiter. Zugleich war er Vorsitzender der Kunstdeputation seines Wohnbezirks Schöneberg und bemühte sich um die Wahrung der geschichtlichen Überlieferung des Bezirks. So verfasste er für eine Ausstellung die Schrift "Das alte Schöneberg im Bilde". Er erhielt eine Ehrenwohnung im "Atelierturm" in den damals gerade neu erbauten Ceciliengärten im Ortsteil Schöneberg, in der er lebte und arbeitete.

Die Nationalsozialisten setzten Baluschek 1933 als „marxistischen Künstler“ von seinen Ämtern ab und schlossen ihn später von allen Arbeits- und Ausstellungsmöglichkeiten aus. Seine Werke brandmarkten sie als „Entartete Kunst“ im Gegensatz zur sogenannten Deutschen Kunst. 1933 und 1934 waren seine Arbeiten aber noch auf der "Großen Berliner Kunstausstellung" zu sehen.

Am 28. September 1935 starb Hans Baluschek im Berliner Franziskus-Krankenhaus und wurde auf dem Wilmersdorfer Waldfriedhof in Stahnsdorf beigesetzt (Grabstelle: Abt. L I–S III–334).
Hans Baluschek gehörte nicht zu den bekanntesten Künstlern der Berliner Secession, entsprechend war seine Rezeption vor allem in der Bundesrepublik Deutschland verhältnismäßig gering, während sie in der DDR vor allem durch Aktivitäten des Märkischen Museums durchaus vorhanden war. Hier gab es regelmäßig zu runden Todestagen kurze Gedenkmeldungen über Baluschek, so etwa zu seinem 30. Todestag in der Zeitung "Neue Zeit" am 28. September 1965. Zudem wurden seine Bilder der arbeitenden Bevölkerung regelmäßig zur Illustration verwendet.

Sein Grab ist als Ehrengrab der Stadt Berlin gewidmet.
Ausstellungen gab es vor allem zu runden Todestagen des Künstlers, eine Besonderheit stellte die Sonderausstellung zum 100-jährigen Bestehen des Märkischen Museums im Jahr 1974 dar. 1975 zeigte die Staatliche Kunsthalle Karlsruhe Gemälde, Zeichnungen und Grafiken anlässlich des 40. Todestags Baluscheks und 1985 fand eine Sonderausstellung zu seinem 50. Todestag statt, erneut im Märkischen Museum. Die letzte größere Ausstellung wurde 1991 in der Kunsthalle Berlin gezeigt, organisiert durch den Berliner Kunstsammler Karl H. Bröhan.

In der Semperstraße wurde am Haus Ceciliengärten 27 in Berlin-Schöneberg, in dem Hans Baluschek eine Ehrenwohnung hatte, am 28. September 1981 eine Gedenktafel für Baluschek angebracht und vom damaligen Volksbildungsstadtrat Ottokar Luban übergeben. Die Tafel zeigt neben dem Text „Hier lebte, malte, zeichnete und schrieb Hans Baluschek, 1929–1933“ eine Straßenszene in der für Baluschek typischen Art.

Seit 2004 trägt eine Grünverbindung in Berlin seinen Namen: Der Hans-Baluschek-Park ist eine schmale Grünanlage zwischen den S-Bahnhöfen Priesterweg und Südkreuz mit einer Länge von 1,5 Kilometer und einer Größe von sieben Hektar.

Eine seiner Schülerinnen war Anna Dräger-Mühlenpfordt.





</doc>
<doc id="7798" url="https://de.wikipedia.org/wiki?curid=7798" title="Freiburg im Breisgau">
Freiburg im Breisgau

Freiburg im Breisgau (, ; abgekürzt "Freiburg i. Br." oder "Freiburg i. B.") ist eine kreisfreie Großstadt in Baden-Württemberg. Von 1945 bis zur Gründung des Landes Baden-Württemberg am 25. April 1952 war Freiburg im Breisgau die Landeshauptstadt des Landes Baden (Südbaden). Die südlichste Großstadt Deutschlands ist Sitz des Regierungspräsidiums Freiburg sowie des Regionalverbands "Südlicher Oberrhein" und des Landkreises Breisgau-Hochschwarzwald. Sie wird von diesem Landkreis fast völlig umschlossen, dem sie selbst nicht angehört; als kreisfreie Stadt bildet Freiburg einen "Stadtkreis".

Gegenwärtig hat das am Fluss Dreisam gelegene Freiburg 227.590 Einwohner "(Stand 31. Dezember 2016)" und nimmt damit auf der Liste der größten Städte Baden-Württembergs nach Stuttgart, Karlsruhe und Mannheim die vierte Stelle ein. Zusammen mit den Landkreisen Breisgau-Hochschwarzwald und Emmendingen bildet die Stadt die (Wirtschafts-)Region Freiburg mit insgesamt circa 630.000 Einwohnern. Sie liegt in der trinationalen Metropolregion Oberrhein mit circa sechs Millionen Einwohnern.

Die Altstadt mit ihren Wahrzeichen – besonders dem Münster und den Bächle – ist Ziel von jährlich über drei Millionen Besuchern.

Mit der 1457 gegründeten Albert-Ludwigs-Universität zählt Freiburg zu den klassischen deutschen Universitätsstädten.

Freiburg liegt im Südwesten Baden-Württembergs am südöstlichen Rand des Oberrheingrabens und am westlichen Fuße des Schwarzwaldes.
Die nächstgelegenen Großstädte sind Mülhausen (frz. "Mulhouse") im Elsass, etwa 46 Kilometer Luftlinie südwestlich, Basel, etwa 51 Kilometer südlich, Straßburg, etwa 66 Kilometer nördlich, Zürich, etwa 85 Kilometer südöstlich, Karlsruhe, etwa 120 Kilometer nördlich sowie Stuttgart, etwa 133 Kilometer nordöstlich von Freiburg. Durch Freiburg fließt die Dreisam.

Die Ausdehnung der Stadt in nord-südlicher Richtung beträgt 18,6 Kilometer, in ost-westlicher Richtung 20 Kilometer. Von der Gemarkungsgrenze sind es bis zur Grenze nach Frankreich 3 Kilometer, bis zur Grenze mit der Schweiz 42 Kilometer. Freiburg weist einen Höhenunterschied von über 1000 Metern auf, von Waltershofen bis zum Schauinsland .

Der Straßenname „Auf der Zinnen“ erinnert an die ehemalige Stadtmauer der Stadt. Etwa 200 Meter nördlich davon verläuft der 48. nördliche Breitengrad. Die Stelle ist auf beiden Seiten der Nord-Süd-Durchgangsstraße, die hier Habsburgerstraße heißt, durch eine Schrift in Pflastersteinen verschiedener Farben hervorgehoben, so dass die geografische Breite erkennbar ist.

Folgende Städte und Gemeinden grenzen an die Stadt Freiburg; sie werden im Uhrzeigersinn, beginnend im Norden, genannt und liegen alle im Landkreis Breisgau-Hochschwarzwald, außer Vörstetten, das zum Landkreis Emmendingen gehört: Vörstetten, Gundelfingen, Glottertal, Stegen, Kirchzarten, Oberried (Breisgau), Münstertal/Schwarzwald, Bollschweil, Horben, Au (Breisgau), Merzhausen, Ebringen, Schallstadt, Bad Krozingen, Breisach am Rhein, Merdingen, Gottenheim, Umkirch und March.
Freiburg liegt an der Grenze zwischen Schwarzwald und Oberrheingraben. Diese langgestreckte Verwerfung verläuft mitten durch das Stadtgebiet. Die östlichen Stadtteile liegen in einem Verbindungstal zum Zartener Becken zwischen den Bergen Roßkopf im Norden und Brombergkopf im Süden. Die südlichen Stadtteile Kappel und Günterstal liegen schon im Schwarzwald. Der Schlossberg, ein Ausläufer der Vorbergzone, ragt wie eine Nase direkt ins Innenstadtgebiet. Das Gestein unterhalb des sog. Greifenegg-Schlössles sowie im westlichen Bereich des Augustinerweges wurde zum Bau der hochmittelalterlichen Stadtmauer abgebaut.

Mit dem südöstlich gelegenen 1284 Meter hohen Schauinsland gehört der Gipfel eines der höchsten Berge des Schwarzwaldes zum Freiburger Stadtgebiet. Mit mehr als 1000 Metern ist Freiburg unter den deutschen Großstädten jene mit dem größten Höhenunterschied innerhalb des Stadtgebiets. Die westlichen Stadtteile liegen weitgehend auf einem Schwemmkegel, der während der letzten Eiszeit entstand. Im Süden liegt der Schönberg, der zur Vorbergzone zählt, einem Teil des alten Gebirges, und der beim Einbrechen des Oberrheingrabens nur teilweise abgerutscht ist.

Im Stadtgebiet Freiburg bestehen folgende sieben Naturschutzgebiete. Damit stehen 593,1 Hektar des Stadtgebiets unter Naturschutz, das sind 3,85 Prozent.


Seit 1997 hat Freiburg eine Baumschutzsatzung. Dennoch kommt es immer wieder zu umstrittenen Fällungen von Bäumen.

Freiburg liegt in einer Zone mit warm- und feucht-gemäßigtem Klima, wobei es große Unterschiede gibt: In der Ebene ist es wärmer und trockener, in den Bergzonen eher kühler und feuchter. Wegen der mittleren Durchschnittstemperatur von 11,4 °C ist Freiburg eine der wärmsten Großstädte Deutschlands.
So wurde während der Hitzewelle 2003 am 13. August offiziell 40,2 Grad gemessen. Dies ist die zweithöchste jemals in Deutschland registrierte Temperatur. Mit der fortlaufenden Erderwärmung hat sich die durchschnittliche Jahresmitteltemperatur seit der Bezugsperiode 1961–1990 von 9,7 °C auf 11,4 °C erhöht (Bezugsperiode 1981–2010), in der Bezugsperiode 1990–2013 sogar auf 11,8 °C.

Die mittlere jährliche Niederschlagsmenge ist mit 837 mm kaum höher als der langjährige deutsche Durchschnitt von gut 800 mm. Der meiste Niederschlag fällt in den Sommermonaten Mai bis August mit einem Spitzenwert von 107 mm im Juni. Im Februar fällt der geringste Niederschlag mit 50,6 mm.

Eine Spezialität des sommerlichen Stadtklimas ist der nach dem östlich gelegenen Höllental genannte „Höllentäler“. Einige Zeit nach Eintritt der Dunkelheit durchlüftet der Bergwind von den Höhen des Schwarzwalds mit großer Regelmäßigkeit Teile der Stadt. Nach Auffassung von Wetterexperten wie Jörg Kachelmann oder Hans von Rudloff ist dieser Wind nicht kühl, wie häufig vermutet und oft gefühlt, sondern eher föhnartig warm. Der Fallwind soll deshalb der Stadt die meisten Tropennächte in Deutschland mit Temperaturen durchgehend über 20 °C bescheren.

Freiburg hat 28 Stadtteile, die vorwiegend zu statistischen Zwecken in 42 Stadtbezirke gegliedert sind. In den bei der Kreisreform des ehemaligen Landkreises Freiburg eingegliederten Stadtteilen Ebnet, Hochdorf, Kappel, Lehen, Munzingen, Opfingen, Tiengen und Waltershofen wurde die Ortschaftsverfassung eingeführt. Damit erhielten diese Orte einen von der Bürgerschaft der Ortschaft gleichzeitig mit dem Gemeinderat zu wählenden Ortschaftsrat mit einem Ortsvorsteher an der Spitze sowie eine örtliche Verwaltung. Die Ortschaftsräte sind zu allen wichtigen die Ortschaft betreffenden Angelegenheiten zu hören. Die endgültige Entscheidung über eine Maßnahme obliegt jedoch dem Gemeinderat der Gesamtstadt Freiburg.

Die Stadtteile im Einzelnen (mit Ordnungsnummer):

Eine erste Erwähnung von Siedlungen im Bereich des heutigen Freiburg, der "Wiehre", "Zähringen" und "Herdern", findet sich in einem Dokument aus dem Jahr 1008. Um 1091 baut der Zähringer-Herzog Bertold II. das "Castrum de Friburch" (Ruine Leopoldsburg) auf dem Schlossberg. Der Siedlung der Dienstleute und Handwerker am Fuße des Berges verlieh Bertolds Sohn Konrad im Jahre 1120 das Markt- und Stadtrecht. An Stelle der inzwischen zu kleinen Kirche veranlasste Bertold V. um 1200 den großzügigen Bau des heutigen Münsters, der v. a. durch die Einkünfte der Silberminen im Schwarzwald finanziert wurde, die wesentlich zum Wohlstand der Freiburger Bürger beitrugen.

Nach dem Aussterben der Zähringer übernahmen 1218 die Grafen von Urach die Herrschaft und nannten sich fortan die Grafen von Freiburg. Nach häufigeren Streitereien mit den Grafen um die Finanzen kaufte sich die Freiburger Bürgerschaft 1368 mit 20.000 Mark Silber von der Herrschaft des ungeliebten Egino III. los und unterstellte sich dem Schutz des Hauses Habsburg.

Freiburg musste den neuen Herrschern Kriegsleute stellen und Finanzhilfe leisten. In der Schlacht bei Sempach siegten die Schweizer Eidgenossen 1386 gegen den österreichischen Herzog Leopold III. und löschten dabei einen Großteil des Freiburger Adels aus. Die Zünfte beherrschten danach den Stadtrat. Freiburg war bis 1427 Reichsstadt. Als Herr der österreichischen Vorlande stiftete Erzherzog Albrecht 1457 die Freiburger Universität.

Im Jahre 1498 hielt Maximilian I. Reichstag in Freiburg. Unter dem Zeichen des Bundschuhs erhoben sich in der gleichen Zeit die Bauern am Oberrhein, doch der Aufstand bei Freiburg unter Joß Fritz im Jahr 1513 wurde verraten. 1525 nahmen im Deutschen Bauernkrieg Bauern unter Führung von Hans Müller Freiburg ein und zwangen den Stadtrat, einer evangelisch-christlichen Vereinigung beizutreten. Als 1529 in Basel die Bilderstürmer den Protestantismus durchsetzten, flohen der Fürst der Wissenschaft Erasmus von Rotterdam und das Basler Domkapitel ins katholische Freiburg. Mit der Vollendung des Hochchors, der 1513 durch den Konstanzer Weihbischof geweiht wurde, war 1536 das Münster endgültig fertiggestellt.

Kurz nach Beginn des Dreißigjährigen Krieges 1620 übernahmen die Jesuiten die Universität Freiburg. Im Jahre 1632 besetzten die Schweden unter General Horn die Stadt, die in den folgenden Jahren mehrmals den Besitzer wechselte. Eine kaiserlich-bayrische Armee unter den Generälen Franz von Mercy und Jan van Werth nahm 1644 Freiburg ein. Anschließend kam es zur Schlacht bei Freiburg zwischen den Bayern und französisch-weimarischen Truppen.

In der zweiten Hälfte des 17. Jahrhunderts kam es unter Ludwig XIV. immer wieder zu Übergriffen auf rechtsrheinisches Gebiet. Nach dem Holländischen Krieg musste Kaiser Leopold I. 1679 im Frieden von Nimwegen die Stadt Freiburg samt Lehen sowie Betzenhausen und Kirchzarten der Krone Frankreichs überlassen. Nachdem Ludwig XIV. Sébastien Le Prestre de Vauban angewiesen hatte, die Stadt zu einer modernen Festung auszubauen, besuchte der König 1681 Freiburg, um den Fortschritt der Arbeiten persönlich zu begutachten. Er übernachtete im Basler Hof. Im Frieden von Rijswijk 1697 durfte Ludwig XIV. die im Elsass besetzten Gebiete einschließlich der freien Reichsstadt Straßburg behalten, musste aber Freiburg an die Habsburger zurückgeben. Gegen Ende des Spanischen Erbfolgekriegs besetzte Marschall Claude-Louis-Hector de Villars 1713 Freiburg erneut. Im zweiten österreichischen Erbfolgekrieg schlugen die Franzosen unter Marschall François de Franquetot die Österreicher bei Weißenburg (Elsass) (5. Juli 1744). Als die französischen Truppen Freiburg räumen mussten, zerstörten sie die Festungsanlagen gründlich. Lediglich das "Breisacher Tor" blieb als Teil der vaubanschen Bauten erhalten.
Französische Revolutionstruppen nahmen Freiburg 1796 ein. Nach drei Monaten befreite Erzherzog Karl die Stadt. Als der Herzog von Modena Herkules III. im Frieden von Campo Formio 1797 seine italienischen Besitzungen verlor, erhielt er vier Jahre später 1801 im Frieden von Lunéville als Kompensation den Breisgau. Herkules III. war mit diesem Tausch nicht einverstanden, da er seine Verluste nicht für ausreichend kompensiert erachtete. Deshalb suchte er den Breisgau nach 1801 nicht auf. Die Regierungsgeschäfte führte der Freiherr Hermann von Greiffenegg, der den Breisgau formal erst am 2. März 1803 für das Haus Este in Besitz nahm. Nach Herkules’ Tod im Oktober 1803 fiel der Breisgau an seine ins Haus Habsburg eingeheiratete Tochter Maria Beatrice. Doch dieses modenisch-habsburgische Zwischenspiel dauerte nur kurz, denn durch Verfügung Napoleons fielen der Breisgau und die Ortenau 1805 an Baden, das seit 1803 Kurfürstentum war. Die Schlussakte des Wiener Kongresses bestätigte 1815 den Verbleib Freiburgs beim Großherzogtum Baden.

1821 löste Freiburg Konstanz als Bischofssitz ab. Im Jahr 1827 wurde Freiburg Sitz des neu gegründeten Erzbistums Freiburg. 1845 wurde die Bahnstrecke in Richtung Offenburg eröffnet. Die Revolution von 1848 entlud sich im Südwesten Deutschlands besonders heftig, obgleich Baden 1818 während der Restauration eine recht liberale Verfassung erhalten hatte. In Freiburg kam es zu blutigen Barrikadenkämpfen, an denen neben badischen Regierungstruppen hessische Verbände beteiligt waren.
Mit der Reichsgründung von 1871 nahm die Stadt am allgemeinen Wirtschaftsaufschwung in Deutschland teil. Unter Oberbürgermeister Otto Winterer erhielt Freiburg mit der Bebauung neuer Stadtteile im Stile des Historismus sein Gesicht. Schon ab 1901 fuhr eine elektrische Straßenbahn.

Im Ersten Weltkrieg bombardierten französische Flugzeuge am 14. Dezember 1914 die offene Stadt Freiburg. Das Ereignis schockierte die Einwohner. Als ein Luftangriff im April 1915 einen Erwachsenen und sieben Kinder tötete, hatte dies eine Fluchtwelle aus der Stadt zur Folge.

Die Rückkehr des Elsass zu Frankreich nach dem verlorenen Krieg traf Freiburg wirtschaftlich besonders hart.

Zwei Reichskanzler in den Anfangsjahren der Weimarer Republik kamen aus Freiburg: Constantin Fehrenbach und Joseph Wirth.

Auch in Freiburg übernahmen 1933 die Nationalsozialisten die Macht. Unter dem Rektorat Martin Heideggers wurde die Universität gleichgeschaltet. 1938 ging in der Reichspogromnacht die Freiburger Synagoge in Flammen auf. 1940 wurden im Rahmen der sogenannten Wagner-Bürckel-Aktion die in Freiburg noch verbliebenen Juden mit einem Sammeltransport ins südfranzösische Internierungslager Gurs deportiert.

Die Luftwaffe führte irrtümlich einen Bombenangriff auf Freiburg am 10. Mai 1940 durch, bei dem 57 Menschen ums Leben kamen. Unter dem Decknamen "Operation Tigerfish" bombardierte die britische Royal Air Force am Abend des 27. November 1944 die Stadt, wobei etwa 2800 Bürger getötet wurden. Nach dem Angriff erhob sich nur noch das relativ unbeschädigte Freiburger Münster aus den Trümmern der im nördlichen Teil vollkommen zerstörten Altstadt, doch hatten die starken Detonationswellen das Kirchenschiff abgedeckt. Mit neuen Ziegeln, die aus Basel gespendet wurden, konnte das Münster bis Januar 1946 wieder fast vollständig gedeckt werden.

Freiburg wurde im April 1945 von den Franzosen besetzt. Im Oktober hielt General de Gaulle in Freiburg eine Siegesparade ab. Infolge der Aufteilung Deutschlands in verschiedene Besatzungszonen wurde Freiburg 1946 die Landeshauptstadt des neugegründeten Bundeslandes Baden. Ministerpräsident war der gebürtige Freiburger Leo Wohleb, der im Colombischlössle residierte, während der Landtag im Historischen Kaufhaus tagte. Nach einer Volksabstimmung ging 1951 Südbaden – trotz des erbitterten Widerstands breiter Kreise der Bevölkerung – im Bundesland Baden-Württemberg auf.

Die Studentenunruhen der späten 1960er Jahre fanden auch in Freiburg ihren Niederschlag. Das gewachsene politische Bewusstsein führte in den 1970er Jahren zur Beteiligung vieler Freiburger am erfolgreichen Widerstand der Kaiserstühler Bauern gegen das geplante Kernkraftwerk Wyhl. Im Gefolge dieser Ereignisse entwickelte sich in der Stadt eine starke autonome Szene und ein breites ökologisch orientiertes Spektrum. Freiburg wurde zu einer Hochburg der neu gegründeten Grünen und wird daher als Ökohauptstadt Deutschlands bezeichnet. Auch wissenschaftlich und wirtschaftlich entwickelte sich in Freiburg ein Klima, das der Stadt eine führende Rolle als Umweltstadt verschafft hat – sie trat bei der Expo 2010 in Shanghai als „green city“ auf.

Freiburg wurde durch seine verkehrsgünstige Lage und die Hochschulen und Forschungseinrichtungen zunehmend eine beliebte Stadt für Kongresse, Messen und Tagungen, insbesondere durch das Konzerthaus Freiburg und die Messe Freiburg. Der internationale Städtetourismus spielt eine starke Rolle.

1986 war die Stadt Gastgeber der siebten Landesgartenschau Baden-Württemberg, was für die Entwicklung der westlichen Stadtteile von großer Bedeutung war und zudem die Einrichtung der Ökostation zur Folge hatte. Ein starker Bevölkerungszuwachs forderte den Ausbau alter und die Errichtung neuer Wohngebiete. Auf einem von der französischen Garnison 1992 verlassenen Gelände der ehemaligen Vauban-/Schlageter-Kaserne entstand der international bekannte Stadtteil "Vauban". 1993 erfolgte der Spatenstich zum neuen Stadtteil "Rieselfeld".

1996 überschritt die Stadt die Bevölkerungszahl von 200.000 Einwohnern. Darunter sind etwa 30.000 Studenten, die an der Universität und vier weiteren Hochschulen studieren.
Als Sitz des Erzbistums und kirchlicher Einrichtungen wie des Deutschen Caritasverbandes ist Freiburg ein Zentrum der katholischen Kirche. 1978 fand in Freiburg der 85. Deutsche Katholikentag statt, an dem u. a. Mutter Teresa teilnahm. Am 24. und 25. September 2011 besuchte Papst Benedikt XVI. im Rahmen seines Deutschlandbesuches Freiburg auf Einladung von Robert Zollitsch, des damaligen Freiburger Erzbischofs und Vorsitzenden der Deutschen Bischofskonferenz. Der Pontifex feierte u. a. auf dem Flugplatz Freiburg eine Jugendvigil und am 25. September 2011 mit über 100.000 Gläubigen eine Eucharistie-Feier. Außerdem traf er Missbrauchsopfer, führte Gespräche mit Helmut Kohl, Verfassungsrichtern sowie dem Präsidium des Zentralkomitees der deutschen Katholiken und hielt eine ekklesiologisch ausgerichtete Rede vor 1500 geladenen Gästen im Konzerthaus Freiburg.

Durch seine Lage in der Trinationalen Metropolregion Oberrhein und als Nachbarstadt u. a. von Straßburg bekommt Freiburg eine zunehmende Bedeutung für das Zusammenwachsen Europas. Die Stadt ist Sitz von Konsulaten und Honorarkonsulaten verschiedener europäischer Staaten. Das Regierungspräsidium Freiburg, die Stadtverwaltung, die Universität Freiburg und viele andere Einrichtungen arbeiten eng mit den Partnerorganisationen in den benachbarten Ländern Frankreich und der Schweiz zusammen. Als Stadt, die gegen Ende des 17. Jahrhunderts (1677–1697) zum Königreich Frankreich gehörte und nach dem Zweiten Weltkrieg Standort einer großen Garnison der französischen Besatzungsmacht war, hat Freiburg seit jeher eine Vorreiterrolle in den Beziehungen zum Nachbarland. Freiburg arbeitet besonders eng mit den französischen Städten Mülhausen und Colmar zusammen. Franzosen spielen eine bedeutende Rolle als Arbeitskräfte und Kunden in der Wirtschaftsregion Freiburg. Wichtige Beiträge zu den kulturellen und politischen Beziehungen beider Staaten leisten das „Centre culturel français“ (CCF) Conrad Schroeder und das Frankreich-Zentrum der Universität. 2001 und 2010 fanden in Freiburg deutsch-französische Gipfeltreffen der Staats- und Regierungschefs statt. Auch mit der schweizerischen Nachbarstadt Basel bestehen seit jeher enge Beziehungen (siehe Erasmus von Rotterdam und Basler Hof), die bis heute gepflegt werden.

Vor der ersten Eingemeindung umfasste das Stadtgebiet 3005 Hektar. Folgende ehemals selbständige Gemeinden beziehungsweise Gemarkungen wurden in die Stadt Freiburg eingegliedert:

Freiburg wuchs nicht nur durch Eingemeindungen, sondern auch durch neue Stadtteile. In den 1960er Jahren waren dies die Stadtteile Weingarten und Landwasser, in den 1990er Jahren die Stadtteile Rieselfeld und Vauban.

Im Spätmittelalter und der frühen Neuzeit lebten in Freiburg zwischen 5.000 und 10.000 Menschen. Freiburg war die größte Stadt zwischen Basel und Straßburg. Erst mit dem Beginn der Industrialisierung im 19. Jahrhundert beschleunigte sich das Bevölkerungswachstum. Hatte die Stadt 1800 9.050 Einwohner, so waren es 1900 bereits 62.000.

Im Zweiten Weltkrieg war die Stadt das Ziel alliierter Luftangriffe. Die Bevölkerungszahl sank von 110.110 im Jahr 1939 um 18,9 Prozent auf 89.275 im Dezember 1945. Schon 1947 überschritt die Einwohnerzahl durch die Flüchtlinge und Vertriebenen aus den deutschen Ostgebieten wieder die Grenze von 100.000. Bis 1996 verdoppelte sich diese Zahl auf 200.000.

Mit einem Bevölkerungswachstum von 32 Prozent im Zeitraum von 1980 bis 2012 wächst die Stadt als eine der schnellsten im Land. 2009 war der Stadtkreis mit einem Zuwachs von 1954 Einwohnern der Kreis mit dem größten Zuwachs in Baden-Württemberg, 2011 lag er auf Platz zwei hinter Stuttgart.

Mit einem Durchschnittsalter seiner Bewohner von 40 Jahren lag der Stadtkreis Freiburg im Jahr 2011 an der Spitze der Kreise Baden-Württembergs. Der Ausländeranteil zum 1. Januar 2013 betrug 13,7 Prozent.

Neben der amtlichen Wohnbevölkerungszahl des Statistischen Landesamtes gibt es mit der Einwohnerfortschreibung des städtischen Einwohnermeldeamtes eine weitere amtliche Bevölkerungszahl, die mit 210.277 Einwohnern zum 1. Januar 2012 deutlich unter der von der Landesbehörde fortgeschriebenen Zahl von 229.144 Einwohnern liegt. Je beim Statistischen Landesamt gemeldeten Einwohner erhält die Stadt derzeit 750 Euro pro Jahr als Mittelzuweisung des Landes. Eine Übernahme der Zahlen des städtischen Einwohnermelderegisters durch das Statistische Landesamt würde daher einen Einnahmeverlust von etwa 15 Millionen Euro pro Jahr bedeuten, was nicht im Interesse der Stadt liegt. Die durch den Zensus 2011 ermittelten Einwohnerzahlen für Bund, Länder und Kommunen wurden am 31. Mai 2013 veröffentlicht und liegen mit 210.600 Einwohnern unterhalb der bisher geführten Bevölkerungszahlen des Statistischen Landesamtes.

Ende 2016 waren 34,3 % der Einwohner katholisch, 22,0 % evangelisch und 43,7 Prozent konfessionslos oder Mitglieder anderer Konfessionen oder Religionen.

Freiburg gehörte bis 1805 zu Österreich, und so blieb die Stadt katholisch, während umliegende Dörfer wie Haslach, Opfingen und Tiengen und ganze Landstriche, die dem Markgrafen von Baden unterstanden, im Zuge der Reformation evangelisch wurden. Die Stadt gehörte kirchlich bis 1821 zum Bistum Konstanz. Im gleichen Jahr wurde Freiburg Sitz eines römisch-katholischen Erzbischofs, der jedoch wegen Differenzen zwischen der badischen Regierung und dem Heiligen Stuhl erst 1827 sein Amt antreten konnte. Die Grenzen des Erzbistums Freiburg decken sich mit den Grenzen des ehemaligen Landes Baden und des früheren preußischen Fürstentums Hohenzollern. Bischofskirche ist das Freiburger Münster. Zur Kirchenprovinz Freiburg gehören die beiden Suffraganbistümer Mainz und Rottenburg-Stuttgart sowie bis 1929 auch die Bistümer Limburg und Fulda. Der Erzbischof von Freiburg trägt den Titel eines Metropoliten (Oberrheinische Kirchenprovinz). Der "Deutsche Caritasverband" hat seinen Sitz in Freiburg.

Als Freiburger Stadtpatrone werden der Heilige Georg (die Freiburger Fahne zeigt das Georgskreuz), Bischof Lambert von Lüttich und der Katakombenheilige Alexander (Märtyrer) verehrt. Im Freiburger Münster, auf dem Münsterplatz sowie in den Museen und Archiven der Stadt finden sich zahlreiche Darstellungen dieser Heiligen, u. a. von Hans Baldung Grien, Hans Holbein dem Jüngeren und Gregorius Sickinger.

Mit dem Anfall des Breisgaus 1805 an das von evangelischen Fürsten regierte Großherzogtum Baden zogen vermehrt Protestanten in die Stadt. Bei den damaligen Verhandlungen zwischen der badischen Regierung und dem Freiburger Stadtrat bot dieser, um die Karlsruher für die Erhaltung der Universität positiv zu stimmen, die Errichtung einer evangelischen Kirche an. Die Freiburger Protestanten gehören heute, sofern sie nicht Glieder einer Freikirche sind, zum im Januar 2007 neu gebildeten Stadtdekanat Freiburg innerhalb des Kirchenkreises Südbaden der Evangelischen Landeskirche in Baden. In Freiburg befindet sich der Sitz der Evangelisch-Lutherischen Kirche in Baden, einer lutherischen Freikirche. Zudem gibt es ein breites Spektrum weiterer protestantischer Freikirchen: Die Calvary Chapel in der City, die Chrischonagemeinde, die Christengemeinde in Lehen, die christliche Missionsgemeinde, die Evangelisch-Freikirchliche Gemeinde (Baptisten), die Freie evangelische Gemeinde, die Heilsarmee, die Liebenzeller Gemeinde, die Mennonitische Gemeinde und die Methodistische Gemeinde.

Seit dem späten 19. Jahrhundert besteht in Freiburg eine alt-katholische Gemeinde, deren Kirche die ehemalige Klosterkirche der Ursulinen im "Schwarzen Kloster" am Rande der Altstadt ist. Den griechisch-, serbisch-, russisch- und rumänisch-orthodoxen Gemeinden wurde die katholische Kirche "Maria Schutz" für ihre Gottesdienste zur Verfügung gestellt.

Außerdem gibt es in Freiburg eine Anglikanische Gemeinde und die Neuapostolische Kirche mit zwei Gemeinden in der Wiehre und in Weingarten, eine Gemeinde der Kirche Jesu Christi der Heiligen der Letzten Tage, die anthroposophisch geprägte Christengemeinschaft, die Jesus Freaks sowie die Zeugen Jehovas.

Nachdem sich schon vor 1230 Juden in der Stadt aufgehalten hatten, soll sich seit 1230 in der Gegend der Webergasse eine Gemeinde gebildet haben. Im Jahre 1310 hatten die Grafen von Freiburg vom Kaiser das lukrative Judenregal erworben, d. h. die Abgaben der in Freiburg lebenden Juden gingen direkt an Konrad und seinen mitregierenden Sohn Friedrich. Diese stellten am 12. Oktober 1338 den ansässigen Juden einen umfassenden Sicherungs- und Freiheitsbrief aus. Doch bereits am 1. Januar 1349 war dieser nichts mehr wert. Obgleich die Pest in Freiburg noch nicht ausgebrochen war, wurden Juden verdächtigt, diese verbreitet zu haben, und wurden festgenommen. Alle Freiburger Juden mit Ausnahme der Schwangeren wurden am 31. Januar 1349 verbrannt. Die Kinder der Ermordeten wurden zur Taufe gezwungen. Nach diesem Pogrom ließen sich Juden nur zögerlich wieder in Freiburg nieder. Da beschloss im Jahre 1401 der Stadtrat ein Dekret, "daz dekein Jude ze Friburg niemmerme sin sol", welches König Sigismund mit der Ewigen Vertreibung 1424 offiziell bestätigte. Erst 1809 wurde den Juden wieder ein ständiger Aufenthalt in der Stadt erlaubt, die dann 1836 eine jüdische Gemeinde gründeten.

In der Pogromnacht 1938 wurde die 1870 errichtete Synagoge von Nationalsozialisten in Brand gesetzt und zahlreiche Geschäfte und Wohnungen jüdischer Freiburger wurden verwüstet und geplündert ohne dass die Polizei oder Feuerwehr eingriff. Die männlichen wohlhabenden jüdischen Bewohner wurden zur Schutzhaft in Konzentrationslager (Buchenwald und Dachau) verschleppt, um sie zur Emigration zu nötigen und ihr Vermögen zu arisieren. Am 22. Oktober 1940 wurden die im Lande verbliebenen badischen zusammen mit den pfälzischen Juden in das Lager Camp de Gurs in Südfrankreich deportiert. Einer der Sammelplätze in Freiburg war der Annaplatz. Im Pflaster der Stadt erinnern „Stolpersteine“ an die Opfer der Judenverfolgung während der Naziherrschaft. Der Journalistin Käthe Vordtriede der Volkswacht (Freiburg im Breisgau) wurden sogar zwei Stolpersteine gewidmet. Der erste vor dem Vordtriede-Haus Freiburg im Jahr 2006 und der zweite vor dem Regierungspräsidium Freiburg oder Basler Hof im Frühjahr 2013. Dort befand sich bis 1941 der Sitz der Gestapo oder Geheime Staatspolizei. Unliebsame Personen wurden dort grausam verhört, inhaftiert und schlimmstenfalls deportiert. Als Lösung blieb nur noch Flucht oder Emigration. Die Familie Vordtriede hatte noch Glück und konnte rechtzeitig entkommen. 

Nach 1945 konstituierte sich eine neue jüdische Einheitsgemeinde, die "Israelitische Gemeinde Freiburg", die mittlerweile durch die Zuwanderung von Juden aus der ehemaligen Sowjetunion auf rund 750 Mitglieder angewachsen ist. Benjamin Soussan, von 1991 bis 2010 Rabbiner der Gemeinde, führte den orthodoxen Ritus ein. Von 1985 bis 1987 errichtete die Gemeinde zwischen Münsterplatz und Stadtgarten eine neue Synagoge. Seit Juli 2004 ist durch die kleine "Egalitäre Jüdische Chawurah Gescher" eine weitere Gemeinde hinzugekommen, die sich der Union progressiver Juden angeschlossen hat.

Für die verstorbenen Einwohner jüdischen Glaubens gibt es eigene Begräbnisstätten: den jüdischen Friedhof in der Elsässer Straße und ein neues Gräberfeld auf dem "Friedhof St. Georgen".

Mehrere islamische Organisationen unterschiedlicher Herkunft und religiöser Ausrichtung unterhalten in Freiburg insgesamt vier Gebetsstätten und Moscheen. Anhänger des Buddhismus finden im Tibet-Kailash-Haus, das 2007 vom Dalai Lama besucht wurde, oder im buddhistischen Zentrum der Karma-Kagyü-Schule Anlaufstellen. Außerdem gibt es seit 2004 den Heidenhain (auch: "Hain der Heiden") in Freiburg, der sich als Treffpunkt und Anlaufstelle für Neopaganismus versteht.
Schließlich gibt es eine kleine Bahá'í-Gemeinde bestehend aus circa 20 Mitgliedern.

Im früher katholisch-konservativen Freiburg wurde 1962 mit Eugen Keidel zum ersten Mal ein Sozialdemokrat zum Oberbürgermeister gewählt. Ihm folgte 1982 sein Parteikollege Rolf Böhme im Amt, der im Jahr 2002 ausschied. Mittlerweile gilt die Stadt als eine Hochburg der Grünen. Dies äußert sich nicht nur in der Wahl des ersten grünen Oberbürgermeisters einer deutschen Großstadt, sondern auch in durchgehend überdurchschnittlich hohen Wahlergebnissen. Bei den Bundestagswahlen 2002 und 2005 wurde der Wahlkreis Freiburg mit 25,0 beziehungsweise 22,8 Prozent der Zweitstimmen bundesweit bester Wahlkreis dieser Partei. Nachdem die Grünen bei der Europawahl 2004 im Stadtkreis 36,8 Prozent erzielten, setzte sich bei der Landtagswahl im März 2006 allerdings die CDU mit 30,3 Prozent wieder als stärkste politische Kraft durch. Bei der Landtagswahl von 2011 konnte das Ergebnis der Grünen aber mit 34,5 (Freiburg I) bzw. 39,9 Prozent (Freiburg II) dem landesweiten Trend entsprechend nochmals gesteigert werden.

Das Direktmandat im Deutschen Bundestag für den Wahlkreis Freiburg hat seit 2013 Matern von Marschall von der CDU inne. Über die Landesliste vertreten zusätzlich seit 2002 Kerstin Andreae (Bündnis 90/Die Grünen) sowie seit 2017 Tobias Pflüger (Die Linke) die Stadt im Bundestag. Im 15. Landtag von Baden-Württemberg ist Freiburg mit drei Abgeordneten vertreten: für den Wahlkreis Freiburg-Ost Reinhold Pix (Bündnis 90/Die Grünen, Direktmandat) sowie für den Wahlkreis Freiburg-West Edith Sitzmann (Bündnis 90/Die Grünen, Direktmandat) und Gabi Rolland (SPD).

An der Spitze der Stadtverwaltung stand vor 1806 der Schultheiß als Vorsitzender des Gerichts. Mit dem Übergang des Breisgaus an Baden wurde das Freiburger Stadtrecht von 1520 außer Kraft gesetzt und die badische Gemeindeverfassung mit einem direkt gewählten Bürgermeister an der Spitze der Verwaltung eingeführt. Johann Josef Adrians – noch von den Zünften als Stadtoberhaupt gewählt – wurde 1806 in seinem Amt bestätigt und mit dem Titel Oberbürgermeister geehrt, doch schränkte die badische Gemeindeordnung die kommunale Selbstverwaltung Freiburgs erheblich ein. Das Sagen hatte ein von der Regierung eingesetzter Stadtdirektor. Ab 1832 trugen dann Freiburg Stadtoberhäupter den Titel Bürgermeister und nannten sich erst ab 1875 wieder Oberbürgermeister.

Der Freiburger Oberbürgermeister ist gleichzeitig stimmberechtigter Vorsitzender des Gemeinderates.

Die Oberbürgermeister seit 1806:

Bei der Oberbürgermeisterwahl am 25. April 2010 wurde bei einer Wahlbeteiligung von 45,2 Prozent der bisherige Amtsinhaber Dieter Salomon (Bündnis 90/Die Grünen) von 50,5 Prozent der Wähler für weitere acht Jahre wiedergewählt. Seine beiden Mitbewerber kamen ebenfalls aus Freiburg: Der Bürgermeister für Kultur, Jugend und Soziales und Integration Ulrich von Kirchbach (SPD) erhielt 29,2 Prozent der abgegebenen Stimmen und der Hochschullehrer Günter Rausch (Wechsel im Rathaus (WiR)) 20,1 Prozent.

Der Gemeinderat besteht aus 48 gewählten Mitgliedern. Den Vorsitz mit Stimmrecht hat als zusätzliches Mitglied der Oberbürgermeister.

Bei der Kommunalwahl am 25. Mai 2014 konnten alle 13 angetretenen Listen Sitze im Gemeinderat erringen. Erstmals war das Mindestalter für das aktive Wahlrecht von 18 auf 16 Jahre gesenkt worden. Durch die Änderung des Sitzzuteilungsverfahrens vom D’Hondt-Verfahren, das große Parteien begünstigt, zum Sainte-Laguë-Verfahren hatten auch Listen mit relativ wenigen Stimmen die Chance, Vertreter in den Gemeinderat zu entsenden.

Nach der Gemeinderatswahl 2014 haben sich „Junges Freiburg“, die zuvor mit den Grünen eine Fraktion bildeten, nunmehr mit der „Grünen Alternative Freiburg“ und „Die Partei“ zu einer neuen Fraktion (JPG) zusammengeschlossen. „Freiburg Lebenswert“ und „Für Freiburg“ kooperieren in einer weiteren Fraktion. Wie bisher bilden die „Kulturliste“, die „Linke Liste“ und die „Unabhängigen Frauen“ die Fraktionsgemeinschaft „Unabhängige Listen“ (UL). Die FDP hat den Fraktionsstatus verloren, da ihr nur noch zwei Stadträte angehören. Die anderen Parteien und die Freien Wähler haben wie bisher eigene Fraktionen. Ende Januar 2017 beschloss der Gemeinderat einstimmig die Unterzeichnung der Charta der Vielfalt.

Das Wappen der Stadt Freiburg zeigt ein rotes durchgehendes Kreuz auf weißem Grund. Es ist das Wappenzeichen des heiligen Georg, des ältesten Stadtpatrons. Das Stadtsiegel Freiburgs zeigt eine stilisierte Burg in rot auf weißem Grund mit zwei Turmbläsern auf den äußeren Türmen. Dieses Siegel sieht man in Farbe nur vereinzelt im Stadtgebiet, auf den Kanaldeckeln der Innenstadt dagegen ist diese Darstellung, gegossen in Eisen, häufiger zu sehen. Die stilisierte Darstellung der Burg diente als Vorbild für das 1896 im Sternwald erbaute Freiburger Wasserschlössle.

Häufig sieht man noch ein Wappen mit einem schwarzen Adlerkopf oder Rabenkopf auf goldenem Grund. Dieses Wappen ist nach 1327 aus der Freiburger Münzmarke entstanden. Anfangs zeigten die in Freiburg geprägten Münzen den ausgebreiteten Adler, also das Wappenbild der Grafen von Freiburg. Nachdem die Freiburger 1327 den Grafen das Münzrecht abgekauft hatten, prägte die Stadt zur Unterscheidung Münzen, die nur noch den Kopf eines Adlers zeigten. Dieser wurde bald als Kopf eines Raben (alemannisch „Rappen“) angesehen, weshalb die kleine Münze auch als „Rappenpfennig“ bezeichnet wurde. Im Jahre 1399 bildeten Freiburg und andere oberrheinische Städte wie z. B. Basel zur Handelserleichterung untereinander den Rappenmünzbund. So wurde der Rappenpfennig Namensgeber für den Schweizer Rappen. Oft wird an historischen Gebäuden oder auf Gemälden das Stadtwappen zusammen mit dem Wappen von Österreich gezeigt, ein Hinweis auf die lange Zugehörigkeit der Stadt zu Vorderösterreich.

Die Flagge der Stadt Freiburg zeigt wie im Wappen das Georgskreuz, ein rotes durchgehendes Kreuz auf weißem Grund. Sie ist identisch mit der Flagge Englands, dessen Schutzpatron wie in Freiburg der heilige Georg ist. Sie wird vor allem als Hochkantflagge gehisst, ist aber auch waagerecht zu sehen. Diese Flagge wird seit etwa 1368 benutzt, als Freiburg zu den Habsburgern kam.

In Freiburg fanden bisher fünf Bürgerentscheide statt:

Die Abstimmungen hatten eine Beteiligung zwischen 22 Prozent (Linienführung Straßenbahn 1999) und 50 Prozent (Kultur- und Tagungsstätte 1988). Eine Verbesserung der Erfolgschancen von Bürgerentscheiden ist auf die Verringerung des Quorums von 30 auf 25 Prozent durch Beschluss des Landtages von Baden-Württemberg im Juli 2005 zurückzuführen.

Im vierten und ersten erfolgreichen Bürgerentscheid, den die Bürgerinitiative Wohnen ist Menschenrecht (WiM) initiiert hatte, entschieden die Bürger im November 2006 mit großer Mehrheit, dass die Stadt Freiburg Eigentümer der städtischen Wohnungen bleiben soll.

Seit den 1970er Jahren werden in Freiburg zunehmend Formen kooperativ-demokratischer Bürgerbeteiligung durchgeführt. Sie werden auch erweiterte Bürgerbeteiligung genannt (d. h. über die gesetzlich vorgeschriebenen Formen hinausgehende Bürgerbeteiligung).

Zwei Begründungen für die Zunahme der Beteiligung an den politischen Entscheidungen werden in Freiburg genannt: zum einen sei dies eine Antwort auf das zunehmende Selbstbewusstsein und Forderung der Bürger nach Mitbestimmung am Gemeinwesen. Zum anderen wird Bürgerbeteiligung als eine Form der Anerkennung und Wertschätzung bürgerschaftlichen Engagements betrachtet. Beobachtungen zeigen, dass eine Kommune, die mehr Mitentscheidungsmöglichkeiten eröffnet, tendenziell auch mehr engagierte Bürger hat.

Folgende Formen der erweiterten Bürgerbeteiligung sind in Freiburg zu beobachten (gegliedert nach zunehmender Reichweite):

Mit der Bürgerbeteiligung verbunden hat sich in Freiburg seit Anfang der 1990er Jahre eine von der Stadt, Wohlfahrtsverbänden und Vereinen getragene hochspezialisierte Infrastruktur gebildet, die das ehrenamtliche, freiwillige Engagement fördert:


Nach dem Zweiten Weltkrieg waren Städtepartnerschaften in Europa ein Weg, um die Verständigung unter Menschen verschiedener Nationen im direkten Kontakt zu ermöglichen und damit den Frieden zu stabilisieren. In diesem Geist wurde 1959 die Partnerschaft mit Besançon geschlossen, der mit Innsbruck, Padua und Guildford weitere folgten. Die kontinentalen Städte dieser Phase sind von etwa gleicher Größe und Struktur, sind touristisch attraktive Universitätsstädte und alte Habsburgerstädte mit reicher Vergangenheit. Das gilt ebenfalls für die später hinzugekommene Stadt Granada und das erheblich größere Lemberg.

Als moderne Verkehrsmittel und die verbesserte Telekommunikation die Welt kleiner werden ließen, kamen mit Madison in den USA und Matsuyama in Japan Städte in Übersee hinzu. Der Einfluss, den diese Partnerschaften auf die kulturelle Entwicklung Freiburgs haben, zeigt sich unter anderem in der Einsendung von 700 Beiträgen zu einem Haiku-Wettbewerb in Matsuyama, von denen 25 ausgezeichnet wurden.

Die Partnerschaft mit der Stadt Isfahan in Iran ist die erste und bisher einzige Partnerschaft einer deutschen mit einer iranischen Stadt. Sie ist eine der lebendigsten Partnerschaften mit vielfältigem Austausch auf kulturellem Gebiet. Auf Grund der Leugnung des Holocausts durch den ehemaligen iranischen Präsidenten Mahmud Ahmadinedschad und des Atomstreits kämpft die Partnerschaft mit Schwierigkeiten.

Die Partnerstädte Freiburgs im Überblick:

Traditionell freundschaftliche Beziehungen, die nicht schriftlich fixiert sind, gibt es zu den elf anderen von den Herzögen von Zähringen neu gegründeten oder geförderten Städten in der Schweiz und in Süddeutschland: zu Bern, Thun, Burgdorf, Freiburg im Üechtland, Murten, Rheinfelden, Neuenburg, Villingen, Bräunlingen und Weilheim an der Teck sowie zum Klosterort St. Peter auf dem Schwarzwald, der Grablege der meisten Zähringer.

Das Theater Freiburg ist ein Drei-Sparten-Theater mit Schauspiel, Musiktheater und Ballett. Es werden drei Bühnen bespielt: Das Große Haus, das Schauspielhaus und die Kammerbühne. Für besondere Anlässe stehen noch der „Werkraum“ und das Winterer-Foyer zur Verfügung. Die Tanztruppe pvc (physical virus collective) tritt als Kooperationsprojekt an den Theatern in Freiburg und Heidelberg gleichermaßen auf. Das Philharmonische Orchester spielt im Konzerthaus. Intendant ist seit der Spielzeit 2017/2018 Peter Carp.

Wie die meisten von Städten betriebenen Theater ist das Freiburger Haus aufgrund der kommunalen Finanznot großen Sparzwängen unterworfen. Trotzdem konnte die Fachzeitschrift „Die deutsche Bühne“ nach einer Umfrage unter Theaterkritikern dem Freiburger Stadttheater den ersten Rang für die „ungewöhnlich überzeugende Theaterarbeit abseits großer Theaterzentren“ zusprechen (Herbst 2007).

Weiter gibt es eine Vielzahl kleinerer Theater:

In Freiburg gibt es eine lebendige Improvisationstheater-Szene mit ungefähr 10 professionellen und Laiengruppen, die an unterschiedlichen Orten auftreten.

Freiburg verfügt über mehrere Anbieter, die szenische (theaterähnliche) Stadtführungen mit oft professionellen Schauspielern anbieten (Historix-Tours, Freiburg Kultour, Timewalking). Auch werden abends Ghost-Walks angeboten.

Mitte Juni wird mit den alldienstäglichen Orgelkonzerten im Münster (bis Ende September), bei denen Interpreten Programme vom Frühbarock bis zur Moderne präsentieren, die Festivalsaison in Freiburg eröffnet. Im Lauf des Jahres finden in Freiburg viele Kulturfestivals statt, zum Beispiel im Februar das Reportage-Festival MUNDOlogia, ein Open-Air-Theatersport-Festival, das Internationales Tanzfestival, im Sommer seit 1983 das Internationale Zelt-Musik-Festival (ZMF), der Münstersommer mit Konzerten, Theater, Lesungen und Ausstellungen sowie viele weitere besondere Veranstaltungen.

Seit 2005 wird im Stadtteil Ebnet und auf dem Schlossgelände beim "Ebneter Kultursommer" von Ende Mai bis Juli eine bunte Veranstaltungsreihe präsentiert.

Alle zwei Jahre im Mai findet mit dem Freiburger film forum ein renommiertes Festival des ethnografischen Films statt. Seit 2002 findet bei Freiburg am Tunisee alljährlich im Juli das Musikfestival Sea of Love statt. Sea of Love war 2011 mit circa 25.000 Besuchern nach dem Southside-Festival in Neuhausen ob Eck das meistbesuchte Musikfestival in Baden-Württemberg.

Der renommierte Freiburger Musiker Murat Coşkun veranstaltet jährlich Anfang August im E-Werk das weltweit bedeutendste Rahmentrommel-Festival Tamburi Mundi für Perkussion-Begeisterte, mit öffentlichen Konzerten, Workshops und Sessions.

Zum Saisonausklang findet im September das von E-Werk und Jazzhaus gemeinsam veranstaltete Freiburger Jazzfestival statt, bei dem internationale Stars der Jazzszene zu hören sind.

Mit zahlreichen Orchestern und Chören, darunter einigen mit internationalem Ruf, weist Freiburg ein reges Musikleben auf. Wichtige Impulsgeber sind – in unterschiedlichen Stilrichtungen – die seit 1946 bestehende Musikhochschule Freiburg mit Studenten und Meisterschülern aus aller Welt sowie die Jazz- und Rockschule Freiburg. Darüber hinaus gibt es das Experimentalstudio der Heinrich-Strobel-Stiftung im Funkhaus des Südwestrundfunks, das seit seiner Gründung 1969 einer der wichtigen Impulsgeber für Neue Musik ist.



Zentrale Treffpunkte der Folk-, Jazz- und Rockszene sind das Restaurant "Waldsee", die "Wodan-Halle" im Ganter Hausbiergarten und das Jazzhaus, das von der internationalen Elite regelmäßig beschallt wird. Der Waldi-Heidepriem-Preis wird regelmäßig von der Stadt Freiburg verliehen. Er ist nach dem Modern-Jazz-Pianisten Waldi Heidepriem benannt, der bis zu seinem Tod 1998 in Freiburg tätig war und am Aufbau des Jazzhaus' Freiburg beteiligt war. Weitere bekannte Jazzkünstler aus Freiburg sind Thomas Heidepriem, Dieter Ilg, sowie das Cécile Verny Quartet und die Freiburg Soul-Formation tok tok tok. Neben dem Jazzhaus sind vor allem das E-Werk, mit seinen größeren Konzerten in den Bereichen Rock, Pop und Weltmusik („creole in concert“), das Ruefetto sowie das Waldsee mit seiner Veranstaltungsreihe „Jazz ohne Stress“ beliebte Treffpunkte der Freiburger Jazz-Szene. Arrangiert von dem örtlichen Konzertveranstalter "KOKO & DTK Entertainment" sind im Konzerthaus und in der SICK-Arena, der multifunktionalen Eventhalle der Messe Freiburg, regelmäßig bekannte Rock- und Pop-Größen anzutreffen. Wer gern Country, Rockabilly oder 60s Modbeat hört, ist im "Great Räng Teng Teng" richtig. Wer lieber Alternative oder Punk hört, geht ins traditionsreiche "Café Atlantik" beim Schwabentor.

Darüber hinaus gibt es eine Vielzahl von Musikvereinen, Blaskapellen, Laienchören und Bands verschiedener Stilrichtungen, die eher lokale Bedeutung haben.

Freiburg hat ein besonders kinobegeistertes Publikum. Bezogen auf die Einwohnerzahl gibt es hier die meisten Kinogänger in Deutschland. Durchschnittlich besucht jeder Freiburger fast sechs Mal im Jahr ein Kino, um einen Film anzusehen. Der bundesdeutsche Durchschnitt liegt bei 1,66.

Im CinemaxX eines bundesweit vertretenen Kinobetreibers als Multiplex-Kino werden überwiegend Hollywood-Mainstream-Filme gezeigt. Daneben hat Freiburg mit den "Friedrichsbau/Apollo"-Kinos und dem "Kandelhof" fünf Säle eines ortsansässigen Betreibers, die als Programmkino bespielt werden. Diese Kinos sind durchgängig für ihr anspruchsvolles Programm ausgezeichnet worden, zuletzt 2007 als erste in Deutschland mit dem „Europa Cinemas Award“ für die beste Programmgestaltung. Mittlerweile hat der gleiche Betreiber auch die sechs Säle der "Harmonie" übernommen. Im Sommer gibt es ein Freilichtkino dieses Veranstalters in der Innenstadt. Bis 2015 fand in "Friedrichsbau" und "Harmonie" jährlich im Sommer das "Freiburger Filmfest" statt, das überwiegend Freiburger Erstaufführungen von Arthousefilmen präsentiert.

Das nicht-kommerzielle Kommunale Kino Freiburg ist ebenfalls mehrfach für seine Programmarbeit ausgezeichnet worden. Alle zwei Jahre veranstaltet das Kommunale Kino das Freiburger Film Forum mit Schwerpunkt auf dem ethnographischen Film. Es gilt als eines der wichtigsten Filmfestivals auf diesem Gebiet in Deutschland.

Als einer der ältesten studentischen Filmclubs Deutschlands zeigt der 1957 gegründete Akademische Filmclub Freiburg an der Universität Freiburg e.V. ein eigenes Programm. Dieses wird während des Semesters bis zu fünfmal pro Woche mit einem 35-mm-Projektor sowie mit einem Digitalprojektor in einem großen Hörsaal des Kollegiengebäudes II der Universität vorgeführt.

Seit 1985 findet die Schwule Filmwoche Freiburg statt, eines der ältesten Schwulen-Festivals im deutschsprachigen Raum. Bis 2000 war die Schwule Filmwoche im Kommunalen Kino zu Gast, seit 2001 sind alle Vorführungen im "Kandelhof".

In Freiburg ist die Kool Filmdistribution ansässig, ein 1997 gegründeter unabhängiger Verleih für internationale Arthouse- und Independent-Filme. Jährlich kommen über diesen Weg etwa sieben Filme mit bis zu 50 Kopien in die Kinos, u. a. "Der Schmetterling" mit Michel Serrault, "Die große Verführung" oder "Zurück nach Dalarna".

Auch als Drehort wurde Freiburg entdeckt. Meist entstehen hier romantische Fernsehkomödien wie "Der Vollgasmann" oder "Manche mögen’s glücklich". Daneben gab es das Drama Zeit der Zimmerbrände mit Uwe Ochsenknecht sowie den mit Heike Makatsch. Die erste Folge des neuen "Tatort Schwarzwald" wurde 2016 gedreht, in dem Freiburg aber nur als Sitz der Polizeidirektion eine Rolle spielt. Der italienische Horrorfilm Suspiria aus dem Jahr 1977 spielt ebenfalls in Freiburg (im Haus zum Walfisch), wurde aber größtenteils auswärts gedreht.

Freiburg beherbergt mehrere städtische Museen, die sich größtenteils aus den früheren „Städtischen Sammlungen“ entwickelt haben. Das größte Museum der Stadt ist das Augustinermuseum (Museum für Kunst- und Kulturgeschichte am Oberrhein) am Augustinerplatz, eines der bedeutendsten Museen in Südbaden. Eine Abteilung des Augustinermuseums ist das seit 1994 im Wentzingerhaus am Münsterplatz untergebrachte Museum für Stadtgeschichte, das sich vornehmlich mit der Entwicklung Freiburgs und dem Bau des Freiburger Münsters beschäftigt. Ebenfalls am Augustinerplatz liegt das Naturmuseum Freiburg, in dem ein Überblick zur Geologie und Mineralogie sowie zur heimischen Tier- und Pflanzenwelt gegeben wird. Nicht nur bei Kindern äußerst beliebt ist die Schau „Vom Ei zum Küken“, die jährlich in den Wochen vor Ostern präsentiert wird. Ein weiteres städtisches Museum ist das als Abteilung des Augustinermuseums 1985 eröffnete Museum für Neue Kunst in der Marienstraße, in dem moderne und zeitgenössische Kunst, angefangen vom Expressionismus Anfang des 20. Jahrhunderts bis hin zu den aktuellen Entwicklungen der letzten Jahre, ausgestellt werden. Das Archäologische Museum Colombischlössle (ehem. Museum für Ur- und Frühgeschichte) befindet sich seit 1983 im Colombipark am Rotteckring. Die Dauerausstellung präsentiert Funde von der Altsteinzeit bis ins Mittelalter, darunter Kunstwerke der eiszeitlichen Jäger und Sammler Südbadens, die älteste Glasschale nördlich der Alpen sowie Zeugnisse des mittelalterlichen Freiburgs. Familiennachmittage, interaktive Führungen und Mitmach-Stationen veranschaulichen die Besonderheiten jeder Epoche. So werden beispielsweise mehrmals im Jahr Aktionen für Kinder angeboten, bei denen sie selbst ausprobieren können, wie in der Steinzeit Feuer gemacht wurde. Im Mai 2012 wurde im Gewerbegebiet Hochdorf das Zentrale Kunstdepot der Stadt eingeweiht.

Das neugeschaffene „Uniseum“ im Gebäude der „Alten Universität“ zeigt Exponate aus der Geschichte der Universität und stellt die Entwicklung der Hochschule bis heute dar. Die Akademie der Polizei Baden-Württemberg beherbergt ein Kriminalmuseum, in dem unter anderem der Einbruch in die Burg Hohenzollern aus dem Jahr 1952 beschrieben ist (Führung nach Voranmeldung).

Seit Oktober 2004 betreibt die Stadt das Kunsthaus L6 im Stadtteil Zähringen. Dort gibt es Ateliers für bildende Künstler, Proberäume für Bands, ein Wohnatelier für Gastkünstler, eine Künstlerwerkstatt und eine Halle für Ausstellungen aktueller Kunst aus der Region Freiburg.

Der Kunstverein Freiburg, gegründet 1827 und damit einer der ältesten Kunstvereine in Deutschland, präsentiert aktuelle Kunst in seiner Ausstellungshalle, der ehemaligen Schwimmhalle des aufgegebenen Marienbades.

Des Weiteren gibt es noch einige privat betriebene Museen in Freiburg: Im „Kunstraum Alexander Bürkle“ wird seit 2004 internationale zeitgenössische Kunst ausgestellt. Dort ist in einer Dauerausstellung auch die „Sammlung Rosskopf“ zeitgenössischer bildender Kunst zu sehen. – Die „Stiftung für konkrete Kunst Roland Phleps“ zeigt in ihrer Skulpturenhalle im Stadtteil Zähringen insbesondere Stahlskulpturen des Namensgebers und in wechselnden Ausstellungen Arbeiten verwandter Künstler. – In der Turmstraße befindet sich das Freiburger Fasnetmuseum, in dem Masken und Kostüme (alemannisch "„Häs“") und die Geschichte der „Freiburger Fasnet“ ausgestellt werden. – Im Schwabentor befindet sich die Zinnfigurenklause, in der historische Dioramen (z. B. zu Martin Luther oder den Bauernkriegen) aus Zinnfiguren ausgestellt sind. Das private Kleine Stuck-Museum befindet sich im Stadtteil Brühl und ist das einzige seiner Art in ganz Deutschland.

Im „Freiburger Hausberg“, dem Schauinsland, befindet sich das Besucherbergwerk Schauinsland. Es handelt sich dabei um ein stadtgeschichtlich wichtiges Bergwerk, in dem Silber-, Blei- und Zinkerze abgebaut wurden. Es liegt am „Erzkasten“ oberhalb von Hofsgrund. Heute wird es von einer privaten Forschergruppe betrieben. In den Wintermonaten bleibt es geschlossen.

Der Reinhold-Schneider-Preis ist der Kulturpreis der Stadt Freiburg im Breisgau, der seit 1960 in der Regel alle zwei Jahre turnusgemäß wechselnd in den Bereichen Literatur, Musik und Darstellende Kunst verliehen wird.

Durch die westlichen Stadtteile von Freiburg verläuft die Mundartgrenze zwischen nieder- und hochalemannisch (Opfingen und Tiengen gehören zum hochalemannischen, die übrigen Stadtteile zum niederalemannischen Dialektraum). Freiburgs alemannischer Name lautet – wie zur Zeit der Stadtgründung – "Friburg". Der ursprüngliche Freiburger Dialekt ist jedoch bei den Einheimischen weitgehend einer mehr oder weniger niederalemannisch gefärbten hochdeutschen Umgangssprache gewichen, die wie das Schwäbische diphthongiert und viele Charakteristika des Alemannischen verloren hat.

Die Stadt ist Heimat des Fußball-Clubs SC Freiburg, der seit 1978 durchgehend in der ersten oder zweiten Bundesliga vertreten ist. 2016 stieg der Verein zum fünften Mal in seiner Geschichte in die höchste Spielklasse auf. Der Verein wurde deutschlandweit durch besonders konstante Personalpolitik bekannt. Präsident war von 1972 bis zu seinem Tod im Jahr 2009 Achim Stocker. Langjähriger Trainer war Volker Finke (1991–2007), auf dessen Initiative die Fußballschule des Clubs zurückgeht. 2004 feierte der SC Freiburg sein 100-jähriges Bestehen. Trainer ist seit Dezember 2011 Christian Streich. Die Frauenmannschaft spielt in der 1. Frauen-Bundesliga.

Neben dem SC Freiburg gibt es den Freiburger Fußball-Club, welcher 1907 Deutscher Meister wurde.

Im Eishockey spielt der EHC Freiburg (die Wölfe) in der Saison 2010/2011 in der 2. Eishockey-Bundesliga. In der Saison 2003/2004 spielte der EHC in der DEL, der höchsten Spielklasse im deutschen Eishockey.

Die Volleyball-Herrenmannschaft der FT 1844 Freiburg spielt seit 2001 in der zweiten Bundesliga.

Auch in der Mannschaftssportart Handball spielt die Damenmannschaft der HSG Freiburg in der 3. Frauen-Handball-Liga mit.

Durch die Eisvögel USC Freiburg ist Freiburg in der ersten Basketballliga der Frauen repräsentiert. In der Saison 2005/2006 belegten die Eisvögel den zweiten Platz nach dem Ende der Rückrunde, in der Saison 2006/2007 den vierten Platz.
Die Herrenmannschaft des USC spielt in der Saison 2009/10 in der ProA (2. Bundesliga). Ihre letzte erstklassige Saison spielte das Freiburger Herrenteam 1998/1999.

Die Stadt ist auch Heimat des Freiburger Rugby Club e. V. von 1982. Der Club spielt aktuell in der 2. Bundesliga Süd. Die Heimspielstätte des Clubs liegt in March-Hugstetten, wo sich der einzige Rugby-Sportplatz in der weiteren Umgebung befindet.

Jenische aus Freiburg gewannen in einer gemischten Mannschaft das weltweit erste Bootsch-Turnier 2005 in Singen.

Auch amerikanische Sportarten erfreuen sich in Freiburg nicht zuletzt wegen der ausländischen Gaststudenten großer Beliebtheit. Die Stadt wird im American Football durch die Freiburg Sacristans vertreten, die in der Saison 2008 zum ersten Mal in der Regionalliga Mitte des American Football Verband Baden-Württemberg e. V. antraten und die Saison mit einem dritten Platz beendeten.
Die "Freiburg Knights" spielen sowohl mit der Baseball- als auch der Softball-Mannschaft in der Verbandsliga Baden-Württemberg des BWBSV auf ihrem angestammten Platz im Dietenbachgelände.
Zudem gibt es in Freiburg die Lacrosse-Mannschaft "PTSV Jahn Freiburg Pumas", die in der 1. Landesliga Baden-Württemberg ein Damen- und ein Herrenteam stellt.

Die Schützengesellschaft Freiburg schießt seit vielen Jahren in der Bundesliga im Bogenschießen. Der Schützenverein Freiburg-St. Georgen schießt in der Südbadenliga Luftpistole und war auch schon mit dem Luftgewehr in der Südbadenliga aktiv.

Eine weitere bedeutende Sportart ist der Radsport. Im Jahr 2000 war Freiburg zum vierten Mal Etappenort der Tour de France. Im Juni 2004 wurde hier die Deutsche Radsportmeisterschaft ausgetragen, im November 2005 wurde mit der Hallenradsport-Weltmeisterschaft ein internationaler Wettbewerb ausgerichtet.

Freiburg ist Sitz des Olympiastützpunkts Freiburg-Schwarzwald, der verschiedene Leistungszentren in der Region unterhält: Ski Nordisch, Radsport, Leichtathletik und andere. In Freiburg ist das Leistungszentrum für Ringen und ein Sportinternat ansässig. Es besteht eine enge Kooperation zur Sportmedizin und zur Sportwissenschaft an der Universität.

Im März 2004 fand erstmals der "Freiburg-Marathonlauf" als Breitensportveranstaltung statt.

Mit der Freiburger Turnerschaft von 1844 hat der größte Sportverein Südbadens seine Heimat in Freiburg. Er hatte 2004 den Zuschlag erhalten, im Jahre 2009 die 54. Rollkunstlauf-Weltmeisterschaft in Freiburg auszurichten. Etwa 1000 Rollsportler aus über 25 Nationen kämpften vom 10. bis 21. November 2009 um die in zehn Disziplinen zu vergebenden Titel.

Der Billardverein "BSC Freiburg-Kaiserstuhl" spielte von 2004 bis 2006 in der 2. Poolbillard-Bundesliga und gewann 2011 den Deutschen Mannschafts-Pokal. 2015 zog er nach Denzlingen und wurde in "PS Denzlingen" umbenannt.

Daneben gibt es in Freiburg über 200 Sportvereine, die etwa 100 Breitensportarten anbieten und dafür gute Trainingsmöglichkeiten bieten.

In Freiburg gibt es insgesamt neun Schwimmbäder, davon drei Freibäder. Sie werden vom städtischen Unternehmen "Freiburger Stadtbau GmbH" beziehungsweise deren "Tochtergesellschaft Regio Bäder GmbH" betrieben.

Das größte Freiburger Bad ist das "Eugen-Keidel-Bad", ein Mineral-Thermalbad mit einer großen Bade- und Saunalandschaft. Über eine Buslinie ist es an den ÖPNV angeschlossen, dennoch ist die Lage weit vor den Toren der Stadt im Wald gelegen eher autogerecht. Es wird von der Freiburger Stadtbau GmbH betrieben.

Im Westen Freiburgs in unmittelbarer Nachbarschaft des Seeparks befindet sich das "Westbad", das vor allem als Sportbad ausgelegt ist. Es besitzt ein durch eine bewegliche Trennwand teilbares 50-m-Becken, ein Nichtschwimmer- und ein Babybecken sowie ein fünf Meter tiefes Sprungbecken mit 10-Meter-Sprunganlage. Im Außenbereich befinden sich Liegewiesen, Spielbereiche, Kinderbecken und ein aufgrund der Finanznot der Stadt seit längerem und auf längere Zeit nicht in Betrieb befindliches Außenschwimmbecken. Neben den Deutschen Schwimmmeisterschaften 1979 fanden dort im Jahr 2000 die Deutschen Kurzbahnmeisterschaften statt.

In Innenstadtnähe befindet sich das "Faulerbad", das als Freizeitbad mit 25-m-Schwimmerbecken, Nichtschwimmerbecken, Liegewiese und einer Sauna konzipiert ist.

Im Stadtteil Haslach befindet sich das Gartenhallenbad Haslach mit mehreren Becken, darunter einem Nichtschwimmer- und einem Sprungbecken bis fünf Meter und einer Liegewiese. Weitere Sportbäder befinden sich in Lehen mit einem 17-m-Becken und Hochdorf.

Unter den Freibädern das größte ist das Strandbad in direkter Nachbarschaft des SC-Stadions auf der Grenze der östlichen Stadtteile Waldsee und Ebnet. Es hat ein Nichtschwimmerbecken mit einer 91-m-Rutsche, ein Planschbecken und ein 50-m-Becken sowie großzügige Liegeflächen, Umkleiden und Verpflegungseinrichtungen. Hüpfburg und Beachvolleyballfelder runden das breite Angebot ab. Die Becken werden durch Solarenergie beheizt.

Im Stadtteil Wiehre befindet sich das Lorettobad mit separatem „Damenbad“, das nur Frauen und Kindern zugänglich ist.

Das dritte Freibad ist das Freibad St. Georgen mit einem 25-m-Becken in Verbindung mit einem Nichtschwimmerbereich mit Breitrutsche. Die Liegewiese ist teilweise in Hügeln geschwungen und bietet einen ausgedehnten Sportbereich unter anderem mit Trampolin und Spielfeldern.

Die Freibäder sind nur während der Sommersaison geöffnet. Die übrigen sechs Schwimmbäder sind mit unterschiedlichen Pausen ganzjährig geöffnet.


In Freiburg gibt es bemerkenswerte Gebäude aus allen Epochen der Stadtgeschichte. Ein großer Teil der historischen Altstadt wurde beim Bombenangriff am 27. November 1944 zerstört. Erstaunlicherweise blieben das Münster, die Südostecke des Münsterplatzes mit Historischem Kaufhaus, Wentzingerhaus und Alter Wache sowie die beiden noch erhaltenen mittelalterlichen Stadttore nahezu unversehrt. Nach dem Krieg wurde die Altstadt weitgehend in den historischen Proportionen mit damaliger Grundstücksgröße und Traufhöhe wieder aufgebaut.

Seit 2014 hat Freiburg einen Gestaltungsbeirat. In diesen wurden für drei Jahre fünf Fachleute aus den Gebieten Architektur, Stadtplanung und Landschaftsarchitektur gewählt. Sie sollen die Stadt bei wichtigen, das Stadtbild prägenden Bauprojekten beraten, und fachliche Argumente beisteuern.

2014 wurde im Gemeinderat auch die Einrichtung einer Kunstkommission beschlossen. Sie soll die Stadtverwaltung und die politischen Gremien bei der Stadtgestaltung fachlich beraten. Die fünf Sachverständigen aus den Bereichen Bildende Kunst, Kunstvermittlung und Architektur werden vom Oberbürgermeister berufen und arbeiten ehrenamtlich.

Zu den beachtenswerten Gebäuden gehören:

Das Freiburger Münster ist das Wahrzeichen der Stadt und ihr bedeutendstes Gebäude. Es wurde als Pfarrkirche errichtet und hat deshalb nur einen Hauptturm. Unter den zahlreichen Kunstwerken sind unter anderem der Hochaltar und der Schnewlin-Altar von Hans Baldung Grien, der sogenannte Oberriedaltar von Hans Holbein d. J. sowie einige sehr schöne mittelalterliche Glasfenster besonders hervorzuheben, die zum Teil Stiftungen der Handwerkerzünfte waren. Der 116 Meter hohe gotische Turm des Münsters überragt alle Gebäude der Stadt. Er wurde von dem Kunsthistoriker Jacob Burckhardt 1869 in einer Vortragsreihe im Vergleich mit Basel und Straßburg mit folgenden Worten ausgezeichnet: „Und Freiburg wird wohl der schönste Turm auf Erden bleiben“. Daraus entwickelte sich das wohl häufig gehörte, aber nicht schriftlich überlieferte Zitat vom „schönsten Turm der Christenheit“. Rund um das Freiburger Münster wird täglich (außer Sonntags) der Markt gehalten.

Am Münsterplatz: Das Historische Kaufhaus von 1532 mit prächtigen Skulpturen habsburgischer Herrscher fällt auf durch seine ochsenblutrote Farbe, die mit farbigen Ziegel gedeckten Ecktürmchen und seine Treppengiebel. Das Haus „Zum Schönen Eck“ von 1761 (Wentzingerhaus) errichtete Johann Christian Wentzinger als sein eigenes Wohn- und Atelierhaus im spätbarocken Stil. Seit 1994 beherbergt es das Museum für Stadtgeschichte. Die "Alte Wache", als Hauptwache der Österreichischen Wachgarnison im Jahre 1733 errichtet, dient nach unterschiedlichsten Nutzungen heute als „Haus der Badischen Weine“.

Die Alte Münsterbauhütte ist das älteste erhaltene Gebäude der Altstadt mit Sichtfachwerk. Es wurde als Gebäude der „Münsterfabrik“ errichtet, die seit dem 13. Jahrhundert für den Bau der Freiburger Pfarrkirche zuständig war. Später wurde das Gebäude um das Fachwerkgeschoss aufgestockt, das dann eine Wohnung für den Turmwächter bot. Heute unterhält der Münsterbauverein hier einen kleinen Laden sowie die Münsterpfarrei eine Begegnungs- und Beratungsstätte. In Sichtweite liegt das Erzbischöfliche Ordinariat, der Verwaltungsbau des katholischen Erzbistums Freiburg. Der 1903–1906 in historisierender Form errichtete Bau hat eine im spätromanischen Stil reich ornamentierte Natursteinfassade. Das Innere weist neben byzantinisch anmutenden Stilelementen auch auf den Jugendstil. Gegenüber steht das von Christoph Arnold, einem Schüler von Friedrich Weinbrenner, geplante „Collegium Borromaeum“ mit der Konviktskirche aus den Jahren 1823 bis 1826, das heute Priesterseminar für die Erzdiözese Freiburg ist. Nicht weit davon entfernt steht das 2002 erbaute Erzbischöfliche Archiv, ein kubischer, mit Sandstein verkleideter Bau.

Zwei Kirchen der historischen Altstadt verdienen Beachtung. Die gotische Kirche St. Martin (Franziskanerkirche) am Rathausplatz ist im Innern betont schlicht und hatte ursprünglich als Bettelordenskirche keinen Turm. Ein Glockenturm mit spitzem Helm wurde erst zwischen 1890 und 1893 errichtet. Nach der Zerstörung im Zweiten Weltkrieg erhielt der Turm ein Pyramidendach. Die barocke Universitätskirche (Jesuitenkirche) neben der Alten Universität wurde im Zweiten Weltkrieg fast völlig zerstört und nach dem Wiederaufbau nicht wieder farbig gefasst. Herausragendes und auffallendes Kunstwerk im Chorraum ist seit 1988 eine 16 Meter hohe Skulptur des leidenden Christus, geschaffen und als Leihgabe zur Verfügung gestellt vom Münstertäler Künstler Franz Gutmann.

Anstelle der 1944 zerstörten Alten Ludwigskirche am nördlichen Rand der Innenstadt wurde zwischen 1952 und 1954 nach den Plänen von Horst Linde ein modernes Kirchengebäude mit wegweisender Architektur im Stadtteil Herdern gebaut. Ein weiterer bemerkenswerter zeitgenössischer Kirchenbau ist die Maria-Magdalena-Kirche der Kölner Architektin Susanne Gross im Stadtteil Rieselfeld. Diese Kirche steht der evangelischen und katholischen Gemeinde mit je einem Kirchenraum zur Verfügung und wurde 2004 eingeweiht.

Im ehemaligen Kloster der Augustinereremiten, dessen älteste Teile aus dem 14. Jahrhundert stammen, ist das Augustinermuseum untergebracht.

Die beiden noch von der mittelalterlichen Stadtbefestigung erhaltenen Tortürme prägen das Bild der Innenstadt. Das ältere Martinstor wurde 1901 um fast das Dreifache auf 60 Meter erhöht und erhielt einen Dachaufbau im Stil des 15. Jahrhunderts. Das Schwabentor wurde ebenfalls 1901 auf fast doppelte Höhe aufgestockt und mit durchbrochenen Treppengiebeln im Stil norddeutscher Stadttürme versehen. Diese wurden 1954 wieder abgebaut und der Torturm erhielt ein Pyramidendach nebst Glockentürmchen mit Zwiebelhaube.

In der oberen Altstadt, nicht weit vom Schwabentor, steht das Hotel/Restaurant "Zum roten Bären"; es gilt als ältester Gasthof Deutschlands. Die sehr tief liegenden Fundamente stammen aus der Zeit vor 1120, dem Zeitpunkt der Stadtgründung.

Jakob Villinger von Schönenberg, Großschatzmeister Maximilians I. – der König war von 1490 bis 1519 Landesherr Freiburgs –, erbaute das 1515 fertiggestellte „Haus zum Walfisch“. Dort wohnte der Humanist und Philosoph Erasmus von Rotterdam nach seiner Flucht aus dem protestantischen Basel in den Jahren 1529 bis 1531. Nach zahlreichen Umbauten im 18. und 19. Jahrhundert erwarb 1905 die Stadt das Haus und stellte es 1909 der Sparkasse Freiburg zur Verfügung. Das Haus Zum Walfisch brannte 1944 infolge des britischen Bombenangriffs aus, doch es blieb nach seinem Wiederaufbau der Hauptsitz der heutigen Sparkasse Freiburg-Nördlicher Breisgau.

Nicht weit davon, in der Kaiser-Joseph-Straße, steht einer der bedeutendsten Profanbauten Freiburgs, der „Basler Hof“. Er entstand Ende des 15. Jahrhunderts durch den Umbau mehrerer älterer Häuser durch Konrad Stürtzel, Hofkanzler Kaiser Maximilians I. Von 1587 bis 1677 diente das Gebäude als Exilresidenz für das Basler Domkapitel, das sich wegen der Reformation in Basel nicht mehr halten konnte. 1698–1802 war es Amtssitz der Vorderösterreichischen Regierung. Eine ähnliche Funktion übt es heute aus: hier ist der repräsentative Dienstsitz des Regierungspräsidenten. Der größte Teil der Ämter des Regierungspräsidiums Freiburg ist in einem Neubau im Westen der Stadt untergebracht.

Zwischen dem westlichen Rand der Altstadt und dem Hauptbahnhof befinden sich ebenfalls interessante Gebäude aus verschiedenen Epochen: Das so genannte Colombischlössle am Rotteckring, 1869–71 auf der Bastion „St. Louis“ der einstigen vaubanschen Befestigung als herrschaftliche Villa in neugotischen Stil erbaut, liegt in einem kleinen Park, in dem zu Schauzwecken Weinstöcke mitten in der Stadt zu finden sind. 1947–1951 war hier der Sitz der (süd-)badischen Landesregierung. Seit 1983 ist das Archäologische Museum der Stadt hier untergebracht. Das Stadttheater wurde 1905–1910 auf der Bastion „Dauphin“ der schon erwähnten Befestigung mit Jugendstilelementen errichtet. Es wurde 1944 fast völlig zerstört und nach dem Zweiten Weltkrieg wieder aufgebaut. Am Platz der Alten Synagoge bildet es mit den Kollegiengebäuden I und II der Universität ein eindrucksvolles Ensemble. – Daneben liegt die Universitätsbibliothek Freiburg, die 1978 an der Stelle eines Gymnasiumsgebäudes aus der Zeit des Historismus als Betongebäude errichtet wurde. Nach 30 Jahren wurde sie nach Plänen des Basler Büros "Degelo Architekten" neu errichtet. Das Konzerthaus Freiburg liegt nahe dem Hauptbahnhof. Seine Realisierung war in der Bürgerschaft heftig umstritten.

Der Bahnhofsturm Freiburg in der Bismarckallee ist mit 19 Stockwerken und einer Höhe von 60 Metern nach dem Münster und den Wohngebäuden in der Krozinger Straße Nr. 52 und Nr. 78 das vierthöchste Gebäude der Stadt. Er gehört zu dem 1997 neu errichteten Gebäudekomplex „Forum Hauptbahnhof Freiburg“. Der Bahnhofsturm wird auch „Solar Tower“ genannt, da ein großer Teil der Südfassade aus Solarzellen besteht. In dem Bürohochhaus befinden sich Büros sowie in den oberen beiden Stockwerken eine Club-Lounge (Bar). Zu dem Gebäudekomplex gehört auch ein „Office Tower“ genanntes zweites Hochhaus und das Planetarium Freiburg. Außerdem befindet sich am Hauptbahnhof das Hochhaus „Inter City Hotel“. Die drei Hochhäuser (Solar Tower, Office Tower und Inter City Hotel) ergeben eine kleine Skyline.

Der "Münsterplatz" ist der größte gepflasterte Platz in Freiburg. Hier findet bis auf Sonntag jeden Vormittag der Wochenmarkt statt, auf der Nordseite der Bauernmarkt und auf der Südseite der Händlermarkt. Im Mittelalter lag um das Münster herum der Friedhof. Auf der Nordseite sind die Umrisse der ehemaligen Beinhauskapelle im Pflaster kenntlich gemacht. An der Westseite des Platzes stehen zwei Brunnen, nördlich eine Kopie des Fischbrunnens, dessen Vorlage 1483 vom Meister "Hans von Basel" geschaffen wurde und ehemals auf der Marktgass (der heutigen Kaiser-Joseph-Straße) stand, und im Süden der Georgsbrunnen aus dem Beginn des 16. Jahrhunderts mit einer vergoldeten St.-Georgs-Statue. Die drei Sandsteinsäulen vor dem Hauptportal des Münsters gehen zurück auf eine Stiftung von 1719 durch die drei vereinigten Stände Vorderösterreichs; sie tragen Skulpturen der beiden „jüngeren“ Stadtpatrone Lambert von Maastricht und Alexander, die Gottesmutter Maria als Patronin des Münsters flankierend.

Der "Augustinerplatz" ist einer der zentralen Plätze der Freiburger Altstadt. Umgeben vom ehemaligen Augustinerkloster – dem heutigen Augustinermuseum – und den Resten der ehemaligen Stadtmauer, ist der Platz ein beliebter Treffpunkt der Freiburger Bevölkerung. An der unterschiedlichen Bodenpflasterung ist der Verlauf der alten Stadtmauer erkennbar. Nach dem Abbruch der ehemals benachbarten Feierling-Brauerei und der Neugestaltung des Platzes hat sich ein Phänomen entwickelt, das scherzhaft mit den Verhältnissen an der Spanischen Treppe in Rom verglichen wird. Auf der Treppenanlage des Augustinerplatzes und in deren Umgebung genießen in den Sommernächten viele Menschen das städtische Leben mit mediterranem Flair. Da jedoch bisweilen der hohe Lärmpegel die Nachtruhe der Anwohner beeinträchtigt, wurde 2009 mit dem Aufstellen der "Säule der Toleranz", die um 23 Uhr von Regenbogenfarben auf Rotlicht schaltet, ein Versuch gemacht, den Lärmpegel zu senken. Der Erfolg blieb jedoch aus, und die 18.000 Euro teure Säule sorgte eher für Spott.

Den "Platz der Alten Synagoge" (frühere Bezeichnungen: Theaterplatz, Europaplatz) entlang der westlichen Seite der ehemaligen Stadtbefestigung begrenzen jetzt die Kollegiengebäude I und II der Universität, die Universitätsbibliothek und das Freiburger Stadttheater. Auf einem Teil des jetzt freien Rasenplatzes vor dem Kollegiengebäude II stand bis zur Pogromnacht im November 1938 die Synagoge. Der Name des Platzes und eine Gedenkplatte erinnern daran, ebenso ein Wegschild nach Gurs, den Ort, an den 1940 die meisten Juden aus Freiburg verschleppt wurden. Der Platz soll im Zusammenhang mit einem neuen Verkehrskonzept der Stadt Freiburg eine besondere Bedeutung zur westlichen Erweiterung der Innenstadt übernehmen. Eine Lücke zwischen den Kollegiengebäuden I und II führt auf den von Universitätsgebäuden umgebenen „Platz der Weißen Rose“ zur Erinnerung an den studentischen Widerstand während des Nationalsozialismus.

Der "Rathausplatz" hieß früher nach dem ehemaligen Kloster Franziskanerplatz, von dem nur die gotische Martinskirche und ein Teil des Kreuzgangs erhalten ist. Das Alte Rathaus und das Neue Rathaus begrenzen den Platz auf der anderen Seite. Auf dem Platz steht ein Brunnen mit dem Denkmal für den Mönch Berthold Schwarz aus dem ehemals benachbarten Kloster, der angeblich das Schießpulver (Schwarzpulver) erfunden hat.

"Oberlinden" und "Unterlinden" sind beschauliche Plätze in unterschiedlichen Vierteln der Altstadt mit je einer namengebenden alten Linde. Die „Obere Linde“ steht seit 1729, den Brunnen von 1861 krönt eine Barockmadonna von Franz Hauser (1651–1717).
Der "Kartoffelmarkt" ist ein beliebter Altstadt-Platz. In seiner Mitte steht ein 1911 vom Kaufmann Ludwig Rau gestifteter Brunnen, der von Carl Anton Meckel und Ludwig Kubanek gestaltet wurde. Vielen Freiburgern gilt der "Adelhauser-Platz" als der schönste Platz Freiburgs, klein und abseits der großen Fußgängerströme. Er trägt seinen Namen nach dem dort ansässig gewesenen Kloster Adelhausen, von dem die Kirche noch erhalten ist. Auf dem Klosterareal sind, nach zahlreichen Umbauten der vergangenen Jahrhunderte, die das Kloster kaum mehr erkennen lassen, einige Museen untergebracht.

In der historischen Mitte Freiburgs auf der Kreuzung von Kaiser-Joseph-Straße und Bertold-/Salzstraße befindet sich der "Bertoldsbrunnen". Dort treffen alle Straßenbahnlinien der Stadt zusammen, in verkehrsarmen Zeiten sogar zu gleichen Zeiten, um ein Umsteigen zu ermöglichen.


Im Zukunftsatlas 2016 belegte die kreisfreie Stadt Freiburg im Breisgau Platz 50 von 402 Landkreisen und kreisfreien Städten in Deutschland und zählt damit zu den Orten mit „hohen Zukunftschancen“. 

Die Stadt liegt im „Verdichtungsraum Freiburg“, der neben der Stadt Freiburg die Gemeinden Au (Breisgau), Bötzingen, Gundelfingen, Kirchzarten, March, Merzhausen und Umkirch des Landkreises Breisgau-Hochschwarzwald sowie die Städte und Gemeinden Emmendingen, Denzlingen und Waldkirch des Landkreises Emmendingen umfasst.
Für die Region Südlicher Oberrhein bildet Freiburg neben Offenburg ein Oberzentrum, von denen für ganz Baden-Württemberg nach dem Landesentwicklungsplan 2002 insgesamt 14 ausgewiesen sind. Das Oberzentrum Freiburg übernimmt für die Gemeinden Au, Bötzingen, Buchenbach, Ebringen, Eichstetten am Kaiserstuhl, Glottertal, Gottenheim, Gundelfingen, Heuweiler, Horben, Kirchzarten, March, Merzhausen, Oberried, St. Märgen, St. Peter (Hochschwarzwald), Schallstadt, Sölden, Stegen, Umkirch und Wittnau die Funktion eines Mittelbereichs.

Freiburg ist ein regionales Wirtschaftszentrum. Es dominiert der Dienstleistungssektor sowie der öffentliche Dienst. Größter Arbeitgeber der Stadt ist die Universität mit dem Universitätsklinikum, gefolgt von zahlreichen Landes- und untergeordneten Behörden. Durch die Nähe zur Universität haben sich kleinere Unternehmen aus den Bereichen Solartechnik, Informations- und Medientechnologie sowie Medizintechnik und Biotechnologie angesiedelt.

Zahlreiche weitere Fach- und Publikumsmessen, insbesondere zu Solarenergietechnik, machen Freiburg zu einem wichtigen regionalen Messestandort. Dem wurde im Jahr 2000 Rechnung getragen, indem die Messe Freiburg auf ein neues Gelände mit moderner Hallen-Infrastruktur umzog.

Darüber hinaus spielt der Fremdenverkehr eine herausragende Rolle. Die Stadt gilt als Tor zum Schwarzwald und gehört zu den beliebtesten Reisezielen in Südwestdeutschland. Die Stadt liegt an der Badischen Weinstraße und an der „Grünen Straße – Route verte“, einer touristischen Straßenverbindung von den Vogesen im Elsass in den Schwarzwald. Im Jahr 2007 wurden erstmals mehr als eine Million Übernachtungen in der Stadt gezählt, zuletzt waren es im Jahr 2013 wieder knapp über eine Million Übernachtungen. Mit einem Verhältnis von 5000 Übernachtungen auf 1000 Einwohner gehört Freiburg damit zur Spitzengruppe der touristischen Ziele. Seit 2012 weisen 18 Stelen mit Stadtplan und Hinweisen zu den wichtigsten Sehenswürdigkeiten in der Altstadt Fußgängern den Weg.

Etwa 43 Prozent der Freiburger Gemarkung sind mit Wald unterschiedlicher Ausprägung (Mooswald, Mittelwald und Bergwald) bedeckt, rund ein Drittel davon ist Eigentum der Stadt, die damit zu den größten kommunalen Waldbesitzern in Deutschland gehört. Die wichtigsten Baumarten, die auch wirtschaftlich genutzt werden, sind Buche, Stieleiche, Fichte, Tanne und Douglasie. Über die wirtschaftliche Nutzung hinaus hat der Wald aber auch ökologische Bedeutung und ist ein wichtiger Bestandteil des Erholungs- und Freizeitangebots. Eine der beiden Forstdirektionen des Landes Baden-Württemberg sowie die hier ansässige Forstliche Versuchsanstalt des Landes zeugen von der Bedeutung des Waldes für die Stadt.

Auch der Weinbau spielt in Freiburg eine nicht unbedeutende wirtschaftliche Rolle. Die Stadt grenzt an drei badische Weinbaubereiche: Markgräfler Land, Tuniberg und Kaiserstuhl mit unterschiedlichen typischen Rebsorten. Mit rund 650 Hektar Rebfläche ist Freiburg die größte Weinbaustadt und eine der größten Weinbaugemeinden in Deutschland – dies vor allem durch die Eingemeindung mehrerer Weinbaugemeinden im Westen der Stadt in den 1970er Jahren. Aber auch auf kleinen Flächen der Innenstadt wird noch heute Wein angebaut. Die Bedeutung des Weinbaus für die Stadt wird unterstrichen durch das hier ansässige Staatliche Weinbauinstitut und den Sitz des Badischen Weinbauverbandes. Auch die Universität baut seit 1985 wieder eigenen Wein an, nachdem diese Tradition seit 1806 unterbrochen war. Davor hing das Gehalt der Professoren direkt vom Ertrag des Weinbaus ab.

Nach der Kommunalabfrage 2007 des Bundes der Steuerzahler Baden-Württemberg stehen städtische Steuereinnahmen von 224 Millionen Euro 2006 und geschätzte Steuereinnahmen für 2007 von 234 Millionen Euro Schulden in Höhe von 475 Millionen Euro gegenüber. Die Schulden setzen sich aus 335 Millionen Euro Schulden des Kämmereihaushaltes und 140 Millionen Euro Schulden der städtischen Eigenbetriebe und Sondervermögen zusammen.

In der Region Freiburg mit dem Stadtkreis Freiburg und den Landkreisen Breisgau-Hochschwarzwald und Emmendingen waren 1987 circa 230.000 Erwerbstätige, 2007 bereits circa 302.000 Erwerbstätige. Etwa 170.000 Beschäftigte gab es 1987 in Freiburg, 2007 circa 201.000.

Freiburg ist für sein sonniges, warmes Klima, für die Forschung und Produktion im Bereich Solarenergie bekannt. Im Stadtteil Vauban liegt die Solarsiedlung am Schlierberg mit 59 Plusenergiehäusern. Die Bedeutung der Solarenergie für Stadt und Region spiegelt sich ferner in der Fachmesse "Intersolar" wider, die für die recht kleine Messe Freiburg eine hohe internationale Bedeutung hat. Diese Messe wird seit 2007 in München abgehalten, aber weiter von Freiburg aus mitveranstaltet. Eine weitere internationale Fachmesse ist die "INTERbrossa-BRUSHexpo", die alle vier Jahre für ihre Branchen Weltleitmesse ist. Freiburg hat sich für seine Vorreiterrolle in der Solarenergie den Ruf als "Sonnenstadt" erworben. Im März 2014 wurde Freiburg als "Energie-Kommune" ausgezeichnet.

Im Vergleich mit anderen Städten ähnlicher Größe ist der Anteil des PKW-Verkehrs am gesamten innerstädtischen Verkehrsaufkommen in Freiburg gering. Sehr hoch ist hingegen der Anteil des Fahrrad-Verkehrs. Die Länge des Freiburger Straßen-, Rad- und Fußwegenetzes beträgt rund 1290 Kilometer. Davon sind 191 Kilometer Hauptverkehrsstraßen (Kreis-, Landes- und Bundesstraßen und Hauptverkehrsstraßen), 439 Kilometer Nebenstraßen, 200 Kilometer selbstständig geführte Rad- und Fußwege, 460 Kilometer Wirtschaftswege.

Freiburg liegt verkehrsgeografisch günstig an den großen europäischen Verkehrsmagistralen Rhein–Saône–Rhône–Mittelmeer und Rhein–Gotthard–Italien. Die Autobahn A 5 verbindet Freiburg in Richtung Norden mit Straßburg, Karlsruhe, Mannheim und Frankfurt am Main und in Richtung Süden mit Mülhausen (Mulhouse) und Basel. Freiburg verfügt über drei Autobahnausfahrten: Nord, Mitte und Süd. Außerdem liegt Freiburg an den Bundesstraßen B 3 (Buxtehude–Weil am Rhein) und wird von der B 31 (Breisach am Rhein–Lindau) durchquert. Die B 294 beginnt in Freiburg und führt über Freudenstadt und Pforzheim nach Bretten.

Freiburg liegt an einer Ferienstraße. Dies ist die grenzüberschreitende Grüne Straße/Route Verte, die in den Vogesen in Contrexéville beginnt, bei Breisach am Rhein den Rhein überschreitet und in der Nordroute in Lindau und in der Südroute in Konstanz endet.

In den 1960er Jahren war die Bundesautobahn 86 über Freiburg und Donaueschingen geplant. Die „Schwarzwaldautobahn“ A 86 sollte von der jetzigen A 5 Ausfahrt Freiburg-Nord am nördlichen Stadtrand vorbei durch mehrere Tunnel in den Schwarzwald geführt werden. Das Projekt wurde jedoch 1975 wieder eingestellt. Dadurch ist die B 31 eine der wichtigsten Ost-West-Verbindungen in Baden-Württemberg, wobei der komplette Durchgangsverkehr die Stadt durchqueren muss. Derzeit befindet sich ein Freiburger Stadttunnel in Planung, der die Innenstadt entlasten soll.
Freiburg war in den 1970er Jahren eine der ersten Städte, die durch die Sperrung der Innenstadt für den KFZ-Verkehr eine Fußgängerzone schufen. Heute verfügt die Stadt über ein dynamisches Parkleitsystem, das auf die Anzahl der verfügbaren freien Parkplätze in den zahlreichen Parkhäusern am Rande der autofreien Innenstadt hinweist. Das Quartier Vauban ist als weitgehend autofreie Neubausiedlung konzipiert.

Mit Freiburg – Friedrichshafen – München wurde im April 2012 von MeinFernbus die erste innerdeutsche Fernbuslinie eröffnet. Seit 2013 gibt es zahlreiche Verbindungen, z. B. nach Stuttgart, Düsseldorf, Frankfurt am Main, Berlin sowie Prag und auf den Balkan, u. a. von DeinBus.de und Eurolines.

Der Anteil des Radverkehrs am gesamten Verkehrsaufkommen lag 2016 in Freiburg bei rund 34 Prozent und damit im Vergleich mit anderen Städten ähnlicher Größenordnung sehr hoch. Für die Freiburger Verkehrspolitik hat die Förderung des Radfahrens hohe Priorität, und seit das Fahrrad als Verkehrsmittel im Alltag eine Renaissance erlebt, gilt die Freiburger Verkehrspolitik vielerorts als vorbildlich. Zweimal erhielt die Stadt die Auszeichnung "Fahrradfreundliche Kommune". Ein immer wiederkehrendes Problem ist der Mangel an Abstellplätzen in der Stadtmitte.

Im Jahr 2019 sollen an 55 Stellen im Stadtgebiet 400 Fahrräder zum Ausleihen zur Verfügung stehen. Seit März 2018 stehen 50 Mieträder eines privaten Anbieters im Stadtgebiet.

Der Freiburger Hauptbahnhof bedient vier Bahnstrecken und wird täglich von circa 65.000 Menschen benutzt. Die Oberrheinbahn (Mannheim–Basel), mit Anschlüssen u. a. nach Zürich, Bern und Mailand befindet sich zurzeit im viergleisigen Ausbau, da sie zukünftig als Zubringer zur Neuen Eisenbahn-Alpentransversale (NEAT) in Richtung Gotthard-Basistunnel dienen soll. Über Offenburg–Kehl besteht in Straßburg eine Hochgeschwindigkeitsverbindung (TGV Est) nach Paris. Gute Intercity-Verbindungen bestehen auch über Karlsruhe nach Stuttgart und München. Über Müllheim–Neuenburg hat Freiburg seit August 2013 eine direkte Hochgeschwindigkeitsverbindung über den TGV-Bahnhof Mülhausen in Richtung Paris, Lyon und Marseille, später auch nach Barcelona erhalten. Die Höllentalbahn führt von Freiburg über den Schwarzwald nach Donaueschingen mit Anschlüssen nach München und Konstanz.

Neben den Regionalzügen der DB Regio AG verbindet die Breisgau-S-Bahn (BSB), die von der SWEG betrieben wird, die umliegenden Städte und Gemeinden mit dem Oberzentrum Freiburg: Die Elztalbahn führt über Waldkirch durch das Elztal nach Elzach, und die Breisacher Bahn führt nach Breisach über Gottenheim und Ihringen; in Gottenheim und Breisach besteht Anschluss an die den Kaiserstuhl umrundende Kaiserstuhlbahn der SWEG. Diese Gesellschaft betreibt im Umland auch die Münstertalbahn zwischen Bad Krozingen und Münstertal, wobei einzelne Kurse von und bis Freiburg geführt werden.

Außer dem Hauptbahnhof gibt es an den einzelnen Eisenbahnstrecken weitere Bahnhöfe oder Haltepunkte:

Im Eisenbahngüterverkehr ist Freiburg kein Eisenbahnknoten mehr; der Freiburger Güterbahnhof dient jedoch als Terminal für die Rollende Landstraße von Freiburg nach Novara in Norditalien.

Den städtischen Nahverkehr bedienen fünf Straßenbahn- und 21 Buslinien der Freiburger Verkehrs AG (VAG), die auch das nahe Umland an die Stadt anbinden. Freiburg gehört zum Tarifgebiet des Regio-Verkehrsverbunds Freiburg.

Eigens für die VAG konstruierte die Waggonfabrik Duewag drei GT8-Serien. Die GT8K und GT8N sind bis heute im Einsatz. Letztere verfügen über ein Niederflur-Mittelabteil. Weiterhin gehören zur Fahrzeugflotte 26 GT8Z des gleichen Herstellers mit 48 Prozent Niederfluranteil. Schließlich verkehren auf den am stärksten frequentierten Strecken Triebwagen des Typs „Siemens Combino“, davon acht „Combino Basic“ und zehn „Combino Advanced“, sowie seit 2015 auch die neuen Urbos des spanischen Unternehmens CAF, Typ "Urbos 100" mit je vier Drehgestellen.

Die VAG betreibt ebenfalls die Schauinslandbahn, Deutschlands längste (3,6 Kilometer) Kabinen-Umlauf-Seilbahn, mit der die Höhen des Schauinslands, Freiburgs Hausberg (1284 m), gut zu erreichen sind.

Seit Dezember 2017 gibt es wieder ein Frauennachttaxi.

Der seit 1908 bestehende Flugplatz in Freiburg ist als Verkehrslandeplatz für Flugzeuge bis zehn Tonnen zugelassen. Er befindet sich im Westen der Stadt, nahe dem neuen Messegelände.

Der meiste Flugverkehr wird jedoch über den EuroAirport Basel-Mulhouse-Freiburg im benachbarten Oberelsass abgewickelt. Zum EuroAirport besteht vom Freiburger Hauptbahnhof aus eine Busverbindung mit einer Fahrzeit von 55 Minuten. Zusätzlich gibt es Buszubringer zu den ebenfalls nahe gelegenen Flughäfen Karlsruhe/Baden-Baden (circa 1:05 h) und Straßburg, (circa 1:05 h). Bahnverbindungen bestehen umstiegsfrei zum Flughafen Frankfurt (circa 2:10 h) sowie mit Umstieg in Basel oder Zürich zum Flughafen Zürich. (circa 2:15 h)

Um 1120 wurde Wasser über hölzerne Leitungen vom Mösle der Stadt zugeführt. 1462 werden die Regeln für die städtischen Rinnen und Kanäle in einer Runzordnung festgelegt. 1732 bestehen schon 79 Leitungen und 57 Brunnen. Seit 1842 werden erste eiserne Leitungen verwendet. Freiburg wird durch das Wasserwerk Ebnet (seit 1876) und das Wasserwerk Hausen an der Möhlin (seit 1970) versorgt. Von Ebnet wird das Wasser in die Hochbehälter auf dem Schlossberg und den Hochbehälter im Freiburger Wasserschlössle am Sternwald (seit 1896) gepumpt und von Hausen in den Hochbehälter am Schönberg. Auf anderen Anhöhen der Stadt gibt es weitere kleine Hochbehälter. Seit 1975 werden Polyethylenrohre zum Wassertransport genutzt. Die Wasserversorgung erfolgt durch die badenova AG & Co.KG.

Die Wassergewinnung erfolgt in den zwei Einzugsgebieten Ebnet und Hausen. In Ebnet erfolgt die Grundwassergewinnung aus zwei Grundwassersammlern und insgesamt neun Tiefbrunnen. In Hausen bestehen sechs Tiefbrunnen. Die maximale Tiefe der Brunnen liegt bei 117 m. Elf Hochbehälter mit 120 bis 20.000 Kubikmeter stehen zur Verfügung.
Das Wasser aus Hausen wird ohne weitere Behandlung dem Verbrauch zugeführt. Das Wasser hat den Härtebereich Mittel, mit 2,28 mmol/L, enthält 9,9 mg Natrium, 25 mg Chlorid, 24,7 mg Sulfit und 25,7 mg Nitrat pro Liter. Das Einzugsgebiet umfasst 130 Quadratkilometer und liefert circa fünf Millionen Kubikmeter.
Das Wasser aus Ebnet wird noch zur Neutralisierung und Entkeimung behandelt. Das Wasser hat den Härtebereich weich, mit 0,99 mmol/L, enthält 8,5 mg Natrium, 13,1 mg Chlorid, 10,7 mg Sulfit und 13,6 mg Nitrat pro Liter. Das Einzugsgebiet umfasst etwa 258 Quadratkilometer und liefert ungefähr 11 Millionen Kubikmeter.

Im Jahr 2008 wurden insgesamt 17,7 Millionen Kubikmeter Wasser verbraucht. Die Maximalförderung lag bei 65.800 Kubikmeter am Tag. Das Versorgungsnetz hat einschließlich Hausanschlüssen eine Länge von 1325 Kilometer und umfasst 35.236 Hausanschlüsse. Von 348 Liter/Tag und Einwohner im Jahr 1899 sank der Verbrauch mit Einführung von Wasseruhren im Jahr 1913 auf 186 Liter. Heute liegt er bei circa 100 Liter/Tag und Einwohner.
Zur Entsorgung von Brauch- und Regenwasser wurden ab dem 13. Jahrhundert die Freiburger Bächle sowie der Gewerbekanal genutzt. Daneben wurden hauseigene Gruben zur Sammlung der Exkremente genutzt. Ab 1868 übernahm für kurze Zeit ein Unternehmen an Stelle von Landwirten, die Entleerung der Gruben. Ab 1887 übernahm die Stadt diese Aufgabe. Bis Ende der 1980er Jahre wurde Abwasser in der Rieselfeldanlage ausgebracht. Der Abwasserzweckverband Breisgauer Bucht sammelt und reinigt seit 1966 das Abwasser. Seit 1980 erfolgt dies im gemeinsamen Klärwerk in Forchheim (Kaiserstuhl).

Das Tochterunternehmen der badenova GmbH und Co. KG, die badenova WärmePlus liefert mit einem Fernwärmenetz von 61 km, aus 136 Erzeugungsanlagen 404 Millionen kWh Wärme. Des Weiteren besteht auf dem Gelände der Universitätsklinik Freiburg ein Ferndampfnetz.

Zu den größeren privaten Arbeitgebern zählen der Halbleiterhersteller TDK-Micronas, die zum amerikanischen Northrop Grumman-Konzern gehörende LITEF GmbH sowie Solvay Acetow, ein Werk des belgischen Solvay-Konzerns, das Materialien für Zigarettenfilter herstellt. Das Unternehmen EFD Induction, von 1950 bis 1996 "Fritz Düsseldorf GmbH" (FDF), ist Europas größter Hersteller von Induktions-Härteanlagen und gehört zusammen mit der "EFD Härterei F. Düsseldorf GmbH" (Hochdorf) zum "EFD-Induction"-Konzern mit Sitz in Skien/Norwegen. Der Baustoffhändler "Götz + Moriz" ist in Freiburg angesiedelt. Größte ansässige Brauerei ist die Brauerei Ganter. Bedeutend ist das Milchverarbeitungsunternehmen Schwarzwaldmilch, das mehrheitlich in der Hand der Schwarzwälder und Breisgauer Milchbauern ist. Seit Januar 1996 ist Freiburg Sitz des Briefzentrums 79 der Deutschen Post AG. Seit 1962 ist Freiburg Standort des inzwischen zum Pfizer-Konzern gehörenden Arzneimittelherstellers Gödecke. Im Jahr 1863 wurde das Unternehmen Raimann zur Herstellung von Maschinen für die Holzbearbeitung gegründet. Es gehört mittlerweile zur Michael Weinig AG und ist das älteste industrielle Unternehmen in Freiburg. Die beiden Unternehmen Mez und Madeira Garnfabrik sind bzw. waren bereits seit Anfang des 20. Jahrhunderts in der Textilindustrie tätig.

Seit 1895 gibt es in Freiburg mit Hellige einen Hersteller von wissenschaftlichen und medizinelektronischen Apparaten, insbesondere von Elektrokardiographen. Heute gehört Hellige zu General Electric, wobei Freiburg zu den bedeutendsten Standorten von GE Healthcare in Deutschland zählt.

Mit den zwei Großverlagen Herder und Haufe, einer Vielzahl von kleineren Verlagen sowie dem Sitz der Badischen Zeitung ist Freiburg ein bedeutender Verlags- und Medienstandort.

Die Solar-Fabrik AG Freiburg gehörte bis zur Insolvenz zu den führenden Solarunternehmen in Europa. Ursprünglich nur Hersteller von Solarmodulen, war es mit ihren zahlreichen internationalen Tochterunternehmen in aller Welt in allen Bereichen der Photovoltaik tätig: Handel und Aufbereitung von Wafern, Produktion von Solarzellen und -modulen sowie die Produktion von Solarkraftwerken.

Auch die IT-Branche ist in Freiburg prominent vertreten. Dazu gehören der deutsche Marktführer im Bereich „Kaufmännische Software“ Lexware, United Planet als führendes Unternehmen bei Portalsoftware und die Managed-Services-Tochter der IDS Scheer Consulting. Außerdem sind in Freiburg die Paragon Software Group, die Jedox AG, der eCommerce-Spezialist Oxid, der E-Mail-Marketing-Experte Inxmail sowie die börsennotierte Kofax Deutschland AG, ein Tochterunternehmen der kalifornischen Kofax, Inc., ansässig. Der internationale CMS-Anbieter Jahia Solutions Group SA hat Freiburg 2016 als Standort für seine deutsche Niederlassung gewählt.

Im Bereich des Bankensektors haben neben der Sparkasse Freiburg-Nördlicher Breisgau und der Volksbank Freiburg eG die Bankhaus E. Mayer AG ihren Hauptsitz in Freiburg. Letztgenannte ist die einzige Privatbank in Freiburg und der weiteren Umgebung. Darüber hinaus bestehen Filialen weiterer überregionaler Banken.

Deutschlands größtes Forstberatungsunternehmen, die "UNIQUE forestry and land use GmbH", hat ihren Hauptsitz in Freiburg.

Die Stadtverwaltung Freiburg ist seit Januar 2011 in fünf Dezernate eingeteilt, denen jeweils eine Reihe von städtischen Ämtern untersteht. Dezernat I wird von Oberbürgermeister Dieter Salomon (Grüne) geleitet und ist zuständig für die Haupt- und Personalverwaltung und für Organisation, Recht, Regionales und Öffentlichkeitsarbeit. Dezernat II wird geleitet von Bürgermeisterin Gerda Stuchlik (Grüne) und ist das Dezernat für Umwelt, Schule und Bildung. Dezernat III, geleitet von Bürgermeister Ulrich von Kirchbach (SPD), ist zuständig für Kultur, Jugend und Soziales und Integration. Er ist gleichzeitig erster Bürgermeister und Stellvertreter des Oberbürgermeisters. Stefan Breiter (CDU) tritt zum 1. April 2018 die Nachfolge von Otto Neideck an, der das Dezernat IV für Finanz-, Wirtschafts- und Wohnungswesen, zentrale IT, öffentliche Ordnung, Bürgerservice, Feuerwehr und Sport leitet. Das neu gebildete Dezernat V wird vom parteilosen Martin Haag geleitet und umfasst die Bereiche Stadtentwicklung und Bauen, Tiefbau mit Verkehrsplanung, Stadtgrün und Gebäudemanagement.

Das Landratsamt Breisgau-Hochschwarzwald hat zwar seinen Sitz in der kreisfreien Stadt Freiburg, verwaltet aber im Wesentlichen nur den die Stadt umgebenden Landkreis. Allerdings erstreckt sich die Zuständigkeit einiger Ämter des Landkreises (insbesondere Gesundheitsamt, Versorgungsamt) auch auf den Stadtkreis Freiburg.

In Freiburg befinden sich mehrere Landesbehörden, so das Regierungspräsidium Freiburg mit Sitz des Präsidenten im Basler Hof, zu dem als neue Abteilungen auch Polizeipräsidium, Oberschulamt, Forstdirektion und das Landesamt für Geologie, Rohstoffe und Bergbau gehören.

Des Weiteren sind in Freiburg die „Forstliche Versuchs- und Forschungsanstalt Baden-Württemberg“, die Akademie der Polizei Baden-Württemberg, das „Staatliche Weinbauinstitut“, das „Chemische und Veterinäruntersuchungsamt Freiburg“ sowie die beiden Finanzämter Freiburg-Stadt und Freiburg-Land ansässig.

Die Justizvollzugsanstalt Freiburg dient dem Vollzug von Freiheitsstrafen ab 15 Monaten bis lebenslänglich sowie der Sicherungsverwahrung für ganz Baden-Württemberg.

Das Staatsarchiv Freiburg bewahrt seit 1806 die schriftliche Überlieferung der staatlichen Behörden im Gebiet des Regierungsbezirks Freiburg auf. Seit 2005 ist es eine Abteilung des Landesarchivs Baden-Württemberg.

Neben diesen Landesbehörden gibt es auch Behörden des Bundes: die in der Lehener Straße angesiedelte örtliche Dienststelle der Bundesagentur für Arbeit, einen Standort des Bundesamt für Strahlenschutz, das Bundesarchiv-Militärarchiv, die regionale Geschäftsstelle der Bundesanstalt Technisches Hilfswerk, eine Außenstelle des Beschaffungsamtes der Bundeszollverwaltung, eine Außenstelle der Bundesnetzagentur, das Wasser- und Schifffahrtsamt Freiburg sowie eine "Mobile Kontroll- und Überwachungseinheit" des Bundespolizeiamtes Weil am Rhein, das dem Bundespolizeipräsidium Süd (München) untersteht, sowie Dienststellen der Bundeswehr. Am Rande der Altstadt in der Neuburg befindet sich die Filiale der Deutschen Bundesbank.

Weitere Organisationen in der Rechtsform einer Körperschaft des öffentlichen Rechts haben ihren Sitz in Freiburg:

Freiburg verfügt über ein Amtsgericht und ein Landgericht, die zum Oberlandesgerichtsbezirk des Oberlandesgerichtes Karlsruhe gehören. Einige Zivilsenate des Oberlandesgerichtes Karlsruhe haben ebenfalls ihren Sitz in Freiburg.

Außer den Gerichten der ordentlichen Gerichtsbarkeit gibt es in Freiburg ein Arbeitsgericht (sowie eine Kammer des Landesarbeitsgerichtes), Außensenate des Finanzgerichts Baden-Württemberg, ein Sozialgericht (zuständig für den Stadtkreis Freiburg, die Landkreise Breisgau-Hochschwarzwald, Ortenaukreis, Emmendingen, Lörrach und Waldshut sowie in Knappschaftssachen für das gesamte Landesgebiet von Baden-Württemberg) und ein Verwaltungsgericht (zuständig für den Regierungsbezirk Freiburg).

Die Notfallrettung der Stadt wird vom Deutschen Roten Kreuz und dem Malteser Hilfsdienst erbracht. Die Integrierte Leitstelle betreibt das DRK gemeinsam mit der Stadt Freiburg und dem Landkreis Breisgau-Hochschwarzwald im Amt für Brand- und Katastrophenschutz. Freiburg verfügt über die Lehrrettungswache 1 (Freiburg-Betzenhausen) sowie die Rettungswache 2 unweit des Hauptbahnhofes. Das DRK betreibt die drei Notarzteinsatzfahrzeuge des Rettungsdienstes, zwei sind am St. Josefskrankenhaus stationiert, ein weiteres am Notfallzentrum der Universitätsklinik Freiburg. Am Flugplatz Freiburg ist ein Hubschrauber der DRF stationiert; neben Intensivtransporten kommt er auch in der Notfallrettung auf Anforderung der Leitstelle Freiburg zum Einsatz.

In Freiburg gibt es nur eine einzige regionale Tageszeitung, die "Badische Zeitung", deren Verbreitungsgebiet sich von Offenburg im Norden bis zum Hochrhein im Süden und in den Hochschwarzwald hinein erstreckt. Sie erscheint Montag bis Samstag außer an Feiertagen. Am Sonntag kommt aus dem gleichen Verlagshaus kostenlos „Der Sonntag in Freiburg“ beziehungsweise in Lörrach und Basel „Der Sonntag im Dreiland“, sodass der Badische Verlag als Herausgeber ein Monopol an den Tageszeitungen hält.

Daneben gibt es seit November 1988 das offizielle „Amtsblatt der Stadt Freiburg“ (Titel bis 2002: "StadtNachrichten"), in dem redaktionelle Beiträge, städtische Termine und Öffnungszeiten, Bekanntmachungen und Stellenanzeigen der Stadt Freiburg sowie Beiträge der im Gemeinderat vertretenen Fraktionen und Gruppierungen abgedruckt sind. Redaktionell ist das Amtsblatt auf Themen beschränkt, die unmittelbaren Bezug zur Stadtverwaltung haben. Das Amtsblatt erscheint alle 14 Tage freitags in einer Auflage von 106.000 Exemplaren und wird kostenlos an alle Freiburger Haushalte verteilt. (Stand März 2014)

Jeweils mittwochs erscheint mit einer Auflage von 107.500 Exemplaren (Stand Mai 2016) der „Freiburger Wochenbericht“, das älteste noch bestehende deutsche Anzeigenblatt. Es wird kostenlos an alle Haushalte verteilt und enthält auch redaktionelle Artikel. Donnerstags erscheint in ähnlicher Aufmachung seit 1983 der kostenlose „Freiburger Stadtkurier“. Er hat eine Auflage von 115.500 Exemplaren (Stand Juni 2009).

Außer diesen fünf größeren Zeitungen gibt es noch einige weitere kleinere Zeitungen und Zeitschriften aus und für Freiburg, wie Stadtteilzeitungen oder die Bürgerblätter lokaler Ortsvereine.

Die mittwochs und samstags kostenlos erscheinende „Zypresse“ ist ein Offertenblatt für private und geschäftliche Kleinanzeigen. Sie wurde 1984 gegründet und hat eine Auflage von 50.000 (mittwochs) bzw. 60.000 (samstags, nur Freiburg) Exemplaren (Stand Mai 2016). Samstags erscheinen im Umkreis Lörrach/Basel sowie im Großraum Offenburg/Lahr zusätzlich Regionalausgaben. Die Zypresse steht in Konkurrenz zum Offertenblatt "schnapp.de" der Badischen Zeitung, das jeweils donnerstags erscheint und crossmedial - also unter enger Verzahnung mit dem Online-Auftritt - ausgerichtet ist.

Außer diesen Anzeigenblättern gibt es noch weitere Kultur- und Veranstaltungsmagazine, die monatlich erscheinen. Dazu gehören die beiden Stadtmagazine „Fipps-Freiburg“ und „Kultur Joker“ sowie die zwei Kulturmagazine „Freiburg aktuell“ und „Chilli – das freiburger stadtmagazin“. Ergänzt wird das Angebot an Kultur- und Veranstaltungsmagazinen durch die jeweils freitags erscheinende Beilage "bz-ticket.de" in der Badischen Zeitung. Alternativ gibt es seit 1998 auch eine Straßenzeitung, der „FREIeBÜRGER“. Diese Zeitung wird monatlich von meist sozial benachteiligten Menschen in den Straßen von Freiburg verkauft.

In Freiburg gibt es ein Funkhaus des öffentlich-rechtlichen Südwestrundfunks, das SWR Studio Freiburg, in dem unter anderem Sendungen für SWR4 Baden-Württemberg und das SWR Fernsehen produziert werden. Des Weiteren war das SWR Sinfonieorchester Baden-Baden und Freiburg bis 2016 im Konzerthaus Freiburg ansässig, jetzt gibt das SWR Symphonieorchester dort Konzerte.

Als Privatsender werbefinanziert ist das Regionalradio baden.fm. Das dritte Freiburger Radio, Radio Dreyeckland, das in der Nähe der Innenstadt produziert wird und aus der Anti-Atom-Bewegung entstanden ist, ist das älteste Freie Radio in Deutschland. Dazu gibt es seit 2006 den ebenfalls terrestrisch empfangbaren Sender uniFM. Dieser Sender ist das Lern- und Ausbildungsradio der Pädagogischen Hochschule und der Universität.

Außer diesen speziell auf Freiburg konzentrierten Medien gibt es noch unter anderem das Radio Regenbogen für die gesamte Region Baden.

Im Ortsteil Lehen betreibt der SWR eine Sendeanlage für Mittelwelle (Frequenz 828 kHz, Sendeleistung 10 kW, Geografische Koordinaten des Senderstandorts: ) und UKW-Hörfunk, welche als Antennenträger einen 92 Meter hohen, gegen Erde isolierten Stahlfachwerkmast mit viereckigem Querschnitt verwendet. Der Senderstandort Freiburg-Lehen ist einer der ältesten in Deutschland.

Ein breites Angebot an Schulen in Freiburg kann sehr vielfältigen Ausbildungsansprüchen gerecht werden.

Das Grundangebot bilden 40 Grund- und Hauptschulen, davon einige in privater Hand, die flächendeckend über die Stadt verteilt sind. An weiterführenden Schulen gibt es acht Realschulen, davon zwei private, elf allgemeinbildende Gymnasien unterschiedlicher Ausprägung, darunter einige in privater Trägerschaft (z. B. die Freie Christliche Schule Freiburg), sowie seit 1972 das Deutsch-Französische Gymnasium, eine von zwei solcher Schulen in Deutschland mit zweinationalem Abschluss. Seit 2010 bietet das Goethe-Gymnasium Freiburg eine Schüler-Ingenieur-Akademie an. Es gibt insgesamt fünf Gesamtschulen, von denen eine die öffentliche Staudinger-Gesamtschule ist und die vier anderen der Waldorfpädagogik zuzurechnen sind. Mit dem UWC Robert Bosch College gibt es in Freiburg das einzige "United World College" in Deutschland. Es handelt sich dabei um ein Internat für 16- bis 19-Jährige aus über 70 Nationen.

Daneben sind in der Stadt vier berufliche Gymnasien und zehn berufliche Schulen für den kaufmännischen und gewerblichen Bereich ansässig.

Die privaten Jazz & Rock Schulen Freiburg, gegründet 1984, bieten im "International Music College Freiburg" eine Ausbildung zum Profimusiker in den Musikrichtungen Jazz, Rock und Pop an. In der "Allgemeinen Musikschule Freiburg" gibt es Musikunterricht für Kinder und Erwachsene und im "Zentrum für Musikpädagogik" Fortbildungen in Zusammenarbeit mit der Popakademie Baden-Württemberg.

Schließlich befinden sich in der Stadt noch zehn Förderschulen, darunter vier Förderschulen für lernbehinderte Kinder und Jugendliche, vier Schulen für geistig oder mehrfach behinderte Kinder, darunter zwei private, eine Schule für Erziehungshilfe und eine Sprachheilschule.

Darüber hinaus ergänzen das breite Bildungsangebot eine Reihe weiterer Schulen in privater Trägerschaft, etwa in den Bereichen Elementarbildung, Familienpflege, Grafik/Design und Schauspiel.

Im Bereich der Erwachsenen- und Weiterbildung bieten unter anderem die Volkshochschule Freiburg und das Katholische Bildungswerk ein breites Spektrum von Fortbildungsmöglichkeiten. Mehrere Sprachschulen ergänzen dieses Angebot, zu denen das Goethe-Institut zu rechnen ist.

In Freiburg befinden sich mehrere Hochschulen mit insgesamt knapp 30.000 Studenten.
Die im Jahr 1457 gegründete Albert-Ludwigs-Universität ist eine der ältesten und renommiertesten Hochschulen Deutschlands mit etwa 20.000 Studenten. Sie prägt nachhaltig das Leben der Stadt: So finden sich rund um die Universität viele gut besuchte Cafés und Kneipen. Die Universität ist nicht nur wegen der Studenten relevant, sie ist mit ihren circa 13.000 Arbeitsplätzen (einschließlich Klinikum) einer der wichtigsten Arbeitgeber in Südbaden.

Die Pädagogische Hochschule Freiburg (PH) im Stadtteil Littenweiler wurde 1962 aus den früheren Akademien für Lehrerbildung I und II gebildet und hat seit 1971 den Status einer wissenschaftlichen Hochschule.

Im Stadtteil Oberau befindet sich die Hochschule für Musik Freiburg, welche 1946 von der Stadt Freiburg gegründet und später vom Land Baden-Württemberg als staatliche Hochschule weitergeführt wurde.

Neben den genannten staatlichen Hochschulen gibt es weitere Hochschuleinrichtungen:

In Freiburg befinden sich mehrere Forschungsinstitute:

Mit der Ehrenbürgerwürde zeichnet die Stadt Freiburg Personen aus, die sich um das Ansehen der Stadt oder das Wohlergehen ihrer Bürger besonders verdient gemacht haben. Die Freiburger Ehrenbürger reichen vom Rokoko-Bildhauer Johann Christian Wentzinger bis zum ehemaligen Oberbürgermeister Rolf Böhme und dem Mäzen Eugen Martin in der Gegenwart.

In Freiburg sind bedeutende Persönlichkeiten geboren worden. Manche sind nach ihrer Geburt oder später weggezogen und haben ihren Wirkungskreis andernorts gefunden und sind erst dort bekannt geworden. Es sind Künstler wie Johann Christian Wentzinger, Julius Bissier, Manolo Lohnes oder Edith Picht-Axenfeld, Politiker wie Karl von Rotteck, Joseph Wirth, Leo Wohleb, Hans Maier oder Wolfgang Schäuble, Wissenschaftler wie Carl Christian Mez oder Karl Rahner, Erfinder wie Edwin Welte oder Engelbert Zaschka und Schauspieler wie Til Schweiger.

Mit Freiburg sind viele Persönlichkeiten verbunden, die entweder hier zeitweise gelebt haben oder ihren Wirkungskreis hier gefunden haben und bekannt geworden sind. Dazu gehören Philosophen wie Edmund Husserl und Martin Heidegger, Wissenschaftler wie Walter Eucken und Arnold Bergstraesser, Schriftsteller wie Alfred Döblin, Reinhold Schneider und Christoph Meckel, Künstler wie Hans Baldung Grien, Schauspieler wie Alexandra Maria Lara, Nobelpreisträger wie Friedrich August von Hayek, Georges Köhler und Hermann Staudinger, Heilige wie Edith Stein, Fußballtrainer wie Joachim Löw.




</doc>
<doc id="7805" url="https://de.wikipedia.org/wiki?curid=7805" title="Reinhold Begas">
Reinhold Begas

Reinhold Begas (* 15. Juli 1831 in Schöneberg; † 3. August 1911 ebenda) war ein deutscher Bildhauer. Er war der Sohn des Malers Carl Joseph Begas und gilt als Hauptvertreter der neobarocken Berliner Bildhauerschule.

Reinhold Begas erhielt seine erste Schulung durch den Bildhauer Ludwig Wilhelm Wichmann in Berlin. 1846–1851 war er Schüler des Bildhauers Christian Daniel Rauch an der Akademie Berlin, die 1815 bis 1850 unter der Leitung von Johann Gottfried Schadow stand. 1848 wurde er Mitarbeiter Rauchs. 1852 errang er einen ersten Erfolg mit dem Gips der Gruppe "Hagar und Ismael" auf der Akademie-Ausstellung Berlin. Dank eines Stipendiums wurde ihm 1856 bis 1858 ein Romaufenthalt ermöglicht. Dort lernte er Arnold Böcklin und Anselm Feuerbach kennen. Hier entstand 1857 die Marmorgruppe "Amor und Psyche", die von einer Skulptur des in Rom tätigen Basler Bildhauers Ferdinand Schlöth beeinflusst ist und in der Nachfolge der klassizistischen Thorvaldsen-Schule steht.

1861 erhielt er einen Ruf an die ein Jahr zuvor gegründete Großherzoglich-Sächsische Kunstschule Weimar, wo bereits Böcklin lehrte und es zur ersten Begegnung mit Franz Lenbach kam. Er blieb dort bis 1863 und kehrte anschließend nach Berlin zurück. 1863 bis 1864 war er erneut in Rom, 1865 bis 1869 wieder in Berlin. 1868 schuf er die lange verschollene, 2009 in Italien wiederentdeckte Skulptur aus Carrara-Marmor "Pan als Lehrer des Flötenspiels", die sich heute im "Begas Haus" in Heinsberg befindet, einem regionalen Museum für die von dort stammende Künstlerfamilie Begas. 1869 und 1870 arbeitete er in Rom und Paris. Danach wirkte er zumeist in Berlin, wo er 1883 in den preußischen Orden „Pour le Mérite für Wissenschaft und Künste“ aufgenommen wurde. Seine Berliner Zeit wurde nur kurz von einem Aufenthalt in Rom 1892 unterbrochen.

Er erhielt zahlreiche Aufträge zu Porträtbüsten, Denkmälern und Kleinplastiken. Von 1871 bis zu seinem Tod 1911 war er Mitglied des Vereins Berliner Künstler und Mitglied der Akademie der Künste Berlin, deren Meisteratelier er von 1876 bis 1903 leitete.

Seine monumentalen Arbeiten waren charakteristisch für das preußische Berlin der Kaiserzeit. Insbesondere Kaiser Wilhelm II. schätzte das Pathos der Arbeiten von Begas und verschaffte ihm nach 1888 eine Vielzahl an repräsentativen Aufträgen. Die bekanntesten Beispiele hierfür sind das Nationaldenkmal für Kaiser Wilhelm I., enthüllt 1897, die künstlerische Oberleitung an der „Siegesallee“ (1895–1901, zerstört), für die er selbst zwei Gruppen beisteuerte, und das 1901 fertiggestellte Bismarck-Nationaldenkmal vor dem Reichstagsgebäude (1938 an den Großen Stern versetzt).

Reinhold Begas schuf außerdem 1886 bis 1891 den "Neptunbrunnen" am Berliner Stadtschloss (1950 abgeräumt, 1969 vor dem Berliner Rathaus aufgestellt).

Begas wurde auf dem Alten Kirchhof der Zwölf-Apostel-Gemeinde an der Kolonnenstraße beigesetzt. Das Grab ist als Ehrengrab der Stadt Berlin gewidmet.

Begas war mit Johanna Margarethe Augusta Philipp, genannt „Grete“ oder „Gré“ verheiratet und hatte zwei Söhne, u. a. den Bildhauer Werner und Freddy sowie eine Tochter Molly. Er lebte in einer von ihm im italienischen Stil errichteten Villa mit Atelier in der Stülerstraße 2–4 in Berlin-Tiergarten.
1898 errichtete er für seine an Tuberkulose erkrankte Frau eine Villa in Baden-Baden. Das Haus ist noch erhalten.


Durch das Preußische Denkmal-Institut in Neuss können folgende Denkmäler nachgewiesen werden, die von Bildhauer Reinhold Begas geschaffen wurden:





</doc>
<doc id="7809" url="https://de.wikipedia.org/wiki?curid=7809" title="Markgräflerland">
Markgräflerland

Das Markgräflerland ist eine Region in Baden-Württemberg im äußersten Südwesten Deutschlands; sie grenzt im Westen an Frankreich und im Süden an die Schweiz.

Das historische Gebilde gleichen Namens entstand am 8. September 1444 durch den Zusammenschluss der Herrschaft Rötteln und der Herrschaft Badenweiler sowie der Landgrafschaft Sausenburg. Das Land war im Besitz der Markgrafen von Hachberg-Sausenberg, einer Nebenlinie des Hauses Baden und nach deren Erlöschen der Markgrafen von Baden, später der Markgrafen von Baden-Durlach.

1556 wurde das Markgräflerland reformiert, wodurch es zu einer protestantischen „Insel“ im sonst katholischen Vorderösterreich wurde.

Im heutigen Sprachgebrauch wird mit dem Begriff "Markgräflerland" vor allem das Oberrheingebiet mit den Weinbergen südlich Freiburg im Breisgau bis Basel bezeichnet. Historisch betrachtet verläuft die Nordgrenze der Region etwa 20 km südlich von Freiburg ungefähr in einer Linie von Heitersheim bis Sulzburg entlang des Sulzbachs. Weitere Abgrenzungen bilden die im Rhein verlaufenden Staatsgrenzen: im Süden bei Kleinbasel zur Schweiz, im Westen zum Elsass (Frankreich); außerdem im Osten der Schwarzwald mit dem Blauen.
Zur Region gehören somit vor allem die südwestlichen Ausläufer des Schwarzwalds mit seiner Vorbergzone hinein in die Rheinebene, z. B. das Kandertal und das untere und mittlere Wiesental: wo sich dieses zum Oberrheintal öffnet, liegt Lörrach, die größte Stadt des Markgräflerlands, welche auch als „Hauptstadt“ der Region bezeichnet wird. Vierzehn Kilometer Wiesental-aufwärts liegt Schopfheim, die älteste Stadt des Markgräflerlands: Es liegt somit größtenteils im Landkreis Lörrach, der nördliche Teil ab Auggen liegt im Landkreis Breisgau-Hochschwarzwald. Größere Flüsse bzw. Bäche sind die Wiese, die Kander und der Klemmbach (Müllheim/Oberweiler).

Bis 1803 glich das Markgräflerland einem Flickenteppich. Orte wie Schliengen gehörten bis dahin zum Fürstbistum Basel, das Fürstentum Heitersheim mit seinen Orten war selbstständig. Hauptsächlich aber war das Markgräflerland von Vorderösterreich und Frankreich umgeben. Selbst vor der Reformation 1556 waren im Markgräflerland unterschiedliche Herrschaften ansässig: die Zähringer, die Staufer, die Röttler, die Sausenberger, die Hachberger und einige Klöster mit ihren geistlichen Herrschaften usw.

In Müllheim residierten die Vögte der Markgrafen von Baden; die Stadt Neuenburg am Rhein ist eine Gründung der Zähringer und alte Zoll- und Verkehrsstation an einer früheren Furt über den Rhein.

Der Osten des Markgräflerlands liegt zum Teil im Schwarzwald, der aus einem alten Gebirge mit einem Gneissockel und Granitanteilen besteht und nach Westen in das Hügelgelände des die hiesige Vorbergzone einnehmenden Markgräfler Hügellandes mit fruchtbarem, lösshaltigem Boden übergeht. Weiter schließt sich die Markgräfler Rheinebene mit der Niederterrasse und der Rheinniederung an mit ebenfalls lösshaltigen Böden, die zum Rhein hin in sand- und kieshaltige Böden übergehen. Geologisch ist dies das Überbleibsel eines Grabenbruchs und eines Schwemmlössgebiets eines Flusstals. Durch die geologische Aktivität bei der Entstehung des Grabenbruchs im Oberrheintal und die damit verbundene, im Boden noch vorhandene geothermische Aktivität sind im Markgräflerland Thermalquellen entstanden, was die Römer schon zu schätzen wussten, die z. B. in Badenweiler eine Therme bauten. In einigen Tälern des Schwarzwaldes sind Spuren von Silber- und Bleierzen zu finden. Es gibt Fundstellen und Zeugnisse über deren Abbau durch die Römer und die nachfolgenden Herrschaften in diesem Gebiet, u. a. in Badenweiler und Sulzburg.

Das Wappen enthält die Wappen der zusammengeschlossenen Herrschaften. Heraldisch rechts oben: Markgrafschaft Baden, heraldisch links oben: Herrschaft Sausenberg, heraldisch rechts unten: Herrschaft Rötteln, heraldisch links unten: Herrschaft Badenweiler. Dieses Wappen wurde so und in diversen ähnlichen Formen verwendet, bis das Markgräflerland 1806 ein Teil des Großherzogtums Baden wurde.

Dieses Gebiet wurde durch verschiedene Stämme der Kelten besiedelt. Im Jahre 70 eroberten die Römer dieses Gebiet. Es wurde unter Kaiser Titus Flavius Vespasianus kultiviert. Die zuvor hier lebenden Kelten wurden assimiliert. Die Römer errichteten auf den Hügeln Siedlungen und Gehöfte. Diese wurden "Villa Urbana" genannt. Die Reste einer "Villa Urbana" sind in Heitersheim östlich des Malteserschlosses zu sehen. Das Gebiet wurde von Soldaten, Offizieren, Beamten, Händlern, Gutsherren und Veteranen besiedelt. Die Veteranen erhielten für ihre Dienste vom Senat oder Kaiser Grundstücke in den eroberten Gebieten, damit man das Gebiet und die Urbevölkerung so schneller romanisieren konnte.

Für die Besiedelung des Gebiets wählte man die Hügel aus. Diese boten aufgrund der strategisch günstigen und erhabenen Lage einen Überblick über das Oberrheintal. Ein weiterer Aspekt war das Klima und die Gesundheit. Das Oberrheintal war damals ein ausgedehnter Auwald, mit unzähligen Seen und Tümpeln mit abgestandenem Wasser. Diese wurden nur beim Hochwasser des "Fluvius Rhenus" (Rhein) mit neuem Wasser gespeist. Das Klima war im Sommer in der Rheinebene schwülwarm. Die Römer umgaben sich in ihren besetzten Gebieten gerne mit ihrer von zu Hause aus gewohnten Kultur. Sie gestalteten ihre Siedlungen wie eine kleine römische Provinzstadt. Da sie unter anderem auch den Wein liebten, brachten sie Reben mit, um sie hier anzubauen. Reste von römischen Bauten sind noch heute in diesem Gebiet zu besichtigen, z. B. die Villa Urbana in Heitersheim oder die römischen Badruinen in Badenweiler.

Das hiesige Gebiet war ein Teil des rechtsrheinischen römischen Agri decumates, auf deutsch das Zehntland. Dieses Gebiet war durch den Rhein, die Donau und den nordöstlich gelegenen Limes (um 100 von den Römern errichtet) gesichert. Die Alamannen, ein Stamm der Germanen, eroberten um 230 das südliche rechtsrheinische Gebiet. Die Römer gaben Agri decumates auf und zogen sich 260 hinter den Rhein zurück. Dort errichteten sie den Donau-Iller-Rhein-Limes. Die verlassenen römischen Bauten wurden zerstört oder gerieten in Vergessenheit. Die Alamannen hielten zunächst nichts von der römischen Kultur. Die römischen Gebäude wurden abgerissen und meist als Steinbruch verwendet. Später bauten die Alamannen so genannte Höhenburgen auf, um das Gebiet zu überwachen. Sie errichteten Gutshöfe und eine Verwaltung nach römischem Vorbild. Die Alamannen unternahmen oft Raubzüge vom ehemaligen Zehntland aus ins benachbarte römische Gallien. Sie wurden aber dabei von römischen Heeren abgewehrt. Erst 455 gelang es den Alamannen von hier aus über den Rhein zu expandieren. Sie eroberten Teile der römischen Provinz Gallien. Es folgten Konflikte mit den Franken, welche nach Süden expandierten. Die Alamannen führten mit den Franken von 496 bis 507 Krieg, in welchem die Franken den entscheidenden Sieg bei Zülpich unter ihrem König Chlodwig I. erringen konnten. Das alamannische Gebiet fiel an das Frankenreich der Merowinger. Das Gebiet des späteren Markgräflerlandes und des Breisgaus wurde Besitz fränkischer Adliger. Um 775 beschenkten fränkische Adlige verschiedene Klöster mit Grundbesitz aus diesem Gebiet u. a. wegen des Seelenheils. Zwischen 900 und 955 fielen die Ungarn in dieses Gebiet ein, es kam zu Verwüstungen und Plünderungen. Danach wurde das Gebiet von Gaugrafen verwaltet, welche der Kaiser einsetzte. 962 konfiszierte Kaiser Otto I. Gebiete vom abtrünnigen Gaugrafen Guntram aus dem Breisgau. Otto I. vermachte sie an den Bischof Konrad aus Konstanz, einem Welfen. Dieser setzte für seine Güter einen Lehens-Meier ein, während er als Vogt dieses Gebiet für seinen Bischof verwaltete. Nach dem Tode Bischof Konrads im Jahre 975, übernahmen die Dompröpste seiner Kirche diese Gebiete. Sie wurden damals Dompropsteigüter genannt.

In den folgenden Jahrhunderten kamen mächtige Adelsfamilien aus dem Gebiet des späteren Markgräflerlands zu großen Besitztümern. Diese vergrößerten, vererbten oder verloren ihr Gebiet im Laufe der Zeit.

Im 11. Jahrhundert eroberten die aus dem nördlichen Schwaben stammenden Herzöge von Zähringen viele Gebiete. Sie kamen unter anderem auch in den Besitz des heutigen Markgräflerlandes und des Breisgaus. Der bekannteste unter ihnen war der von 1078 bis 1111 regierende Berthold II. von Zähringen. In den Jahren 1075 bis 1122 fand der Investiturstreit statt. Die Zähringer standen auf der siegreichen päpstlichen Seite. Sie konnten somit viele klösterliche und weltliche Besitze der Verlierer an sich bringen. Die hiesigen Gebiete der Zähringer wurden seit 1122 durch deren Vögte verwaltet. Diese residierten auf der Burg in Badenweiler. Die Zähringer Herrschaft von Badenweiler kam 1147 als Mitgift für die Prinzessin Clementine von Zähringen an Heinrich den Löwen, einem Welfen-Fürsten. Die Expansionsversuche der Hohenstaufer gefiel den Zähringern nicht. Sie gründeten 1175 die Stadt Neuenburg am Rhein. Damit hatten sie den Rheinübergang ins Elsass für sich gesichert und konnten so von Benutzern des Rheinübergangs Tribut verlangen. Nach dem Tod von Berthold V. erlosch 1218 die männliche Linie der Zähringer, deren Gebiete kamen an die Grafen von Freiburg.

Der Staufer Kaiser Friedrich I. Barbarossa zwang den Welfen Heinrich den Löwen, diese Gebiete 1157 gegen Besitzungen im Harz zu tauschen. Damit kam die ehemalige Zähringer Herrschaft Badenweiler in den Besitz der Hohenstaufer, welche auch Besitzungen im benachbarten Elsass hatten. Es war naheliegend, Verbindungen von dort nach Badenweiler zu schaffen. Nachdem die Hohenstaufer ausgestorben waren, kam Badenweiler 1268 an die Grafen von Freiburg.

Die Herren von Rötteln und der Ort Lorracho (Lörrach) wurden 1102 erstmals in einer Urkunde des Klosters St. Alban bei Basel erwähnt. Bischof Burkhard von Basel setzte Dietrich von Rötteln als Schirmvogt über die rechtsrheinischen Besitzungen des Klosters ein. Dietrich III. von Rötteln starb 1204. Er hatte seinen Söhnen große Besitzungen im Wiesental hinterlassen. Seine Söhne hatten hohe Ämter, Walter I. war Kapitular zu Konstanz und Basel, Liuthold I. wurde Bischof von Basel, Konrad I. war Stadtgründer von Schopfheim, welches für das sich später bildende Markgräflerland von erheblicher Bedeutung war. Dietrich IV. erhielt die Burg Rotenburg im Kleinen Wiesental. Die erste urkundlich belegte Erwähnung der Burg stammt aus dem Jahr 1259. Liuthold II. von Rötteln war der letzte männliche Überlebende seines Geschlechtes. Er schenkte 1315 die Rötteler Herrschaft dem Markgrafen Heinrich von Hachberg-Sausenberg, Sohn seiner Nichte Agnes von Rötteln. Die auf der Burg Hochberg bei Emmendingen ansässigen Markgrafen von Hachberg-Sausenberg wurden die neuen Herren über die Herrschaft Rötteln. Die Markgrafen von Hachberg-Sausenberg zogen von der Sausenburg auf die Burg Rötteln um. Sie errichteten dort ihre Verwaltung und setzten auf der Burg Sausenburg Vögte ein. Am 19. Mai 1316 starb Liuthold II. von Rötteln als letzter männlicher Vertreter der Herren von Rötteln. Im Jahr 1332 zogen die Basler vor die Burg Rötteln und belagerten sie, weil Markgraf Rudolf II. von Hachberg-Sausenberg im Streit den Basler Bürgermeister erstochen hatte. Im letzten Augenblick gelang es aber, durch Vermittlung den Streit beizulegen. Pfeilspitzen, Armbrustbolzen usw., die bei der Burg Rötteln gefunden wurden, datieren von dieser Belagerung. 1356 war ein schweres Erdbeben in diesem Gebiet. Basel wurde zerstört, die Burg Rötteln erlitt schwere Schäden.

Im Anfang des 12. Jahrhunderts schenkten die Herren von Kaltenbach (aus dem Ort Kaltenbach bei Malsburg-Marzell) Ländereien an das Kloster St. Blasien. Dieses Kloster kam so in den Besitz von Sausenberg. Es errichtete weitere Propsteien in diesem Gebiet: In Bürgeln, in Sitzenkirch und in Weitenau, einem Ortsteil von Steinen. Bürgeln ist ein noch heute erhaltenes Schloss auf der Gemarkung Schliengen bei Schallsingen. Die Markgrafen von Hachberg erwarben 1232 die Sausenburg auf dem Gebiet von Malsburg-Marzell vom Kloster St. Blasien. Im Jahre 1300 fand die Erbteilung unter den Markgrafen von Hachberg statt. Markgraf Rudolf I. bekam die südlichen Ländereien und wurde 1306 zum Begründer der Sausenberger Linie. Er nannte sich von da an Markgraf von Hachberg-Sausenberg. Die Schenkung der Herren von Rötteln an die Hachberg-Sausenberg ist die erste Etappe in der Entwicklung des Markgräflerlandes. Johann, der letzte der Grafen von Freiburg, schenkte 1444 seine Herrschaft Badenweiler seinen Neffen Rudolf IV. und Hugo von Hachberg-Sausenberg. Durch den Zusammenschluss der Herrschaft Rötteln, der Landgrafschaft Sausenburg und der Herrschaft Badenweiler entstand damit am 8. September 1444 das Markgräflerland.

Die Grafen von Freiburg waren die Nachkommen der Grafen von Urach und 1218 in den Besitz der Gebiete der Zähringer gekommen. Nachdem Egino II., ein Sohn des Grafen Konrad I. von Freiburg, gestorben war, wurde dessen Gebiet 1272 aufgeteilt. Ein Sohn des Grafen Egino II. von Freiburg namens Heinrich erhielt die südlichen Gebiete mit der Herrschaft Badenweiler. Die Grafen aus der Linie Heinrichs starben 1303 ohne männliche Nachkommen aus. Ihr Gebiet ging an die in diese Linie eingeheirateten Grafen von Straßberg. Der Besitz kam 1385 an den Grafen Konrad III. von Freiburg zurück. Er war ein Nachkomme der direkten Linie von Egino II. Durch Schulden dieser Grafen wechselte der Besitz immer öfter, u. a. für kurze Zeit an die Habsburger, die es 1418 nach dem Konstanzer Konzil, wieder an den Grafen Konrad III. von Freiburg zurückgaben. Die Burg Badenweiler wurde 1409 im Krieg des Grafen von Freiburg mit dem Fürstbischof von Basel beschädigt und danach wieder erneuert. Wegen der Enklaven Schliengen und Istein, welche zum Bistum Basel gehörten, gerieten die beiden Herrschaften oft miteinander darüber in einen Streit. Johann, der letzte der Grafen von Freiburg, vermachte 1444 seine Herrschaft Badenweiler an die Söhne von Wilhelm, dem Markgrafen von Hachberg-Sausenberg.

Die Grafen von Strassberg stammten aus der Nähe des heutigen Neuenburg (Schweiz). Sie übernahmen 1303 die Herrschaft Badenweiler von den Grafen von Freiburg. Durch diese kam der Sparren in das Wappen von Badenweiler und vieler andere Ortschaften, welche unter dessen Herrschaft waren, auch in das Wappen des Markgräflerlandes. Die Grafen von Strassberg starben 1363 aus und so kam Badenweiler an die Grafen von Fürstenberg bei Donaueschingen, diese hatten den Besitz jedoch nur für kurze Zeit.

Die zweite und letzte Etappe in der Entwicklung des Markgräflerlandes wurde am 8. September 1444 abgeschlossen, als die Markgrafen von Sausenberg-Rötteln durch Schenkung auch die Herrschaft Badenweiler erwarben. Im Jahr 1503 kam das Markgräflerland durch Erbfolge an die Markgrafschaft Baden unter Christoph I..

Ab 1525 wüteten die Bauernkriege; die aufständischen Bauern verloren ihn, jedes Haus in der Markgrafschaft musste fünf Gulden an den Markgrafen zur Entschädigung entrichten.

Am 1. Juni 1556 schloss sich der Markgraf, und dadurch nach damaligem Recht („Cuius regio, eius religio“, dt. sinngemäß "wes' Untertan ich bin, des' Glaub' ich bin") auch seine Untertanen, der lutherischen Reformation an. Jeder Ort im Markgräflerland wurde protestantisch. Beim Zukauf der Gemarkung Gersbach vom katholischen Vorderösterreich musste die Bevölkerung daher zur evangelischen Konfession wechseln.

Von 1618 bis 1648 tobte der Dreißigjährige Krieg: Abwechselnd zogen die schwedischen, die kaiserlichen und die französischen Truppen, verschiedene Hilfsheere und marodierende Soldaten plündernd und mordend durch. Der Bevölkerungsverlust war enorm und wurde durch Zuzug von Einwanderern aus dem Gebiet der Eidgenossenschaft ausgeglichen.

Von 1672 bis 1679 dauerte der Holländische Krieg: Französische Truppen rückten ins Markgräflerland ein; sie forderten hohe Tribute an Futtermitteln und Geld. Dabei wurde am 8. Juni 1677 u. a. Seefelden ausgeplündert. Während dieses Krieges wurden 1678 die Burgen Rötteln, Sausenburg und Badenweiler durch die Armee des französischen Marschalls François de Créquy zerstört; sie wurden danach nicht mehr aufgebaut.

Von 1689 bis 1697 folgte der pfälzische Krieg. Die Ereignisse ähnelten sich, nun auch von den heranrückenden kaiserlichen Truppen begangen, welche die Franzosen zurückwarfen. Danach kamen die zuvor französisch besetzten Gebiete wieder zurück an das Reich.

Von 1701 bis 1714 dann der Spanische Erbfolgekrieg; das Markgräflerland wurde 1702 von Plünderungen und Requirierungen durch französische Truppen nicht verschont.

Im Jahr 1727 wurde der Sitz der Markgrafen von Badenweiler nach Müllheim verlegt; von 1733 bis 1738 folgten der Polnische und 1740 bis 1746 der Österreichische Erbfolgekrieg. Diese forderten während der erneuten französischen Besatzung von den Orten im Markgräflerland nochmals Tribut, wenn auch in geringerem Ausmaß.

Von 1746 an war das Markgräflerland wieder ohne Besatzung. Es wurde nun von Markgraf Karl Friedrich von Baden-Durlach regiert. Im Jahr 1783 schaffte er hier die Leibeigenschaft ab und förderte den Weinbau.

Von 1791 bis 1815 war Baden in die Koalitionskriege und napoleonischen Kriege verwickelt. Als enger Verbündeter Napoleon Bonapartes erhielt Baden 1805 nach dem Frieden von Pressburg den bisher vorderösterreichischen Breisgau. Danach bestand erstmals eine direkte Landverbindung zu den anderen nordbadischen Landesteilen und das isolierte Inseldasein des Markgräfler Landes hatte ein Ende.


Das Markgräflerland zeichnet sich durch ein günstiges von der Burgundischen Pforte beeinflusstes Klima aus und wird häufig auch als Toskana Deutschlands bezeichnet.

Die überdurchschnittlich hohe Sonnenscheindauer von über 1700 Stunden im Jahr (Mittelwert Deutschland: 1541 Stunden) macht die Region mit einer Jahresdurchschnittstemperatur von 10,8 °C zu einer der sonnigsten und wärmsten Gegenden in ganz Deutschland. Die warmen Südwestwinde, die durch die Burgundische Pforte ins Land strömen, sind die Ursache dafür, dass der Markgräfler Frühling oft schon drei Wochen früher als im Rest Deutschlands beginnt.
Auch sorgen die Westhänge des Schwarzwaldgebirges dafür, dass Regenwolken vom Atlantik genug Feuchtigkeit für das Markenzeichen der Region – den Weinanbau – ins Land bringen. Mit 70 l/m² Regen in den Sommermonaten genug für die Reben und dennoch nicht zu viel für Urlauber, die sich am Sonnenschein erfreuen wollen. Gleichzeitig bilden die Gebirge von Schwarz- und Odenwald eine effektive Barriere gegen allzu kalte Winde im Winter und begünstigen so ein ganzjährig mildes Klima.

Der Weinbaubereich Markgräflerland reicht vom Grenzacher Horn sowie Weil am Rhein im Süden bis nach Ebringen kurz vor die Tore Freiburgs im Norden und umfasst die Vorbergzone zwischen Rheinebene und Schwarzwald. Typischer Wein der Region ist der Gutedel. Dieser wurde um 1780 vom badischen Markgrafen Karl Friedrich von Baden aus dem schweizerischen Vevey ins Markgräflerland gebracht. Aufgrund des günstigen Klimas gedeihen aber auch Burgundersorten.

Zu den Traditionen des Markgräflerlands gehört die Tracht mit der markanten Hörnerkappe. Heute wird diese Kleidung vorwiegend in Trachtenvereinen und zu besonderen (unter Umständen folkloristisch angehauchten) Anlässen gepflegt, doch noch bis etwa 1930 wurde die Tracht von der ländlichen Bevölkerung allgemein zu festlichen Anlässen getragen.

Neben der auch hier ausgiebig und umfangreich gefeierten alemannischen Fastnacht (siehe auch: Basler Fasnacht und Buurefasnacht) ist das Scheibenschlagen ein beliebter und bekannter Brauch in der zu Ende gehenden Winterzeit.

Eine besondere kulinarische Spezialität sind die Winzerschnitten aus dem Markgräflerland.

Am 12. Oktober 2017 werden zwei zusammengehörende Briefmarken in der Serie „Deutschlands schönste Panoramen“ veröffentlicht, welche den südlichen Vorsprung des Ehrenstetter Ölbergs zeigen und auf das Markgräflerland Bezug nehmen.




</doc>
<doc id="7818" url="https://de.wikipedia.org/wiki?curid=7818" title="Österreichische Euromünzen">
Österreichische Euromünzen

Die österreichischen Euromünzen sind die in Österreich in Umlauf gebrachten Euromünzen der gemeinsamen europäischen Währung Euro, die seit dem 1. Jänner 1999 in Österreich gültig ist (Beitritt zur Eurozone).

Österreichische Euromünzen haben ein unterschiedliches Design für jede Münze, wobei es ein gemeinsames Leitmotiv für jede der drei Münzserien gibt. Die kleinen Münzen zeigen österreichische Blumen, symbolisch für die Umwelt. Die mittleren zeigen Beispiele der österreichischen Architektur, über die Epochen verteilt. Die beiden großen Münzen zeigen berühmte Persönlichkeiten. Alle Motive stammen aus der Hand von Josef Kaiser und zeigen die zwölf Sterne, das Prägejahr und die Flagge Österreichs. Nach den heraldischen Regeln der Tingierung ist deren Farbfolge Rot-Weiß-Rot durch Schraffur dargestellt, allerdings resultiert bei der 50-Cent-Münze wegen der um 90° gedrehten Darstellung daraus die Farbfolge Blau-Weiß-Blau (was der der Flagge El Salvadors entspräche). Auf allen Münzen ist noch einmal der Wert angegeben, wobei das Wort "Eurocent" als "Euro Cent" ausgeführt ist.

Alle österreichischen Münzen werden in der Münze Österreich in Wien geprägt, die mit der Herstellung der Euromünzen im November 1998 begann. Allerdings tragen alle Münzen, die in den Jahren 1998–2002 geprägt wurden, das Jahr 2002.

Die acht Motive der österreichischen Euromünzen sind:

Die ab 2007 neu gestaltete Vorderseite der Euromünzen (neue Europakarte) wurde in Österreich erst 2008 eingeführt.

Der Österreichische Kleinmünzsatz (abgekürzt "KMS", 1 Cent bis 2 Euro) wird auch vollständig, in limitierter Auflage und mit höherer Prägequalität (Handgehoben oder Polierte Platte) herausgegeben. Seit 2011 werden die Kursmünzensätze in polierter Platte in einer Verpackung mit einem neuen Design angeboten. Die Folder der Münzen in handgehobener Qualität sind jedes Jahr einem bestimmten Thema gewidmet. Seit 2012 erscheint jährlich ein Baby-Euro-Münzensatz, der wie der Österreichische Kleinmünzensatz alle Kursmünzen von 1 Cent bis 2 Euro in handgehobener Prägequalität enthält. Der Österreichische Kleinmünzensatz und der Baby-Euro-Münzensatz haben das gleiche Ausgabedatum. Im Jahr 2014 kam zusätzlich der Jahrgangssatz „43. World Money Fair – Berlin“ mit einer Auflage von 2500 Stück heraus, wird aber unten nicht angeführt.

Die österreichischen Euromünzen entsprechen nicht vollständig den 2008 erlassenen Gestaltungsrichtlinien der Europäischen Kommission. In diesen sind Empfehlungen für die Gestaltung der nationalen Seiten der Kursmünzen festgelegt. Unter anderem sollen auf der nationalen Seite der Euro-Umlaufmünzen keine Angaben zum Nennwert der Münze angegeben sein und es soll auch die Bezeichnung der Währung oder ihrer Unterteilung nicht wiederholt werden, sofern die Angabe nicht in einem anderen Alphabet erfolgt. Dies wird von den österreichischen Umlaufmünzen nicht eingehalten. Ebenso stellt die abgebildete Flagge Österreichs keine Landeskennung im Sinne der Leitlinien dar. Die Länder, deren Münzen den Empfehlungen noch nicht entsprechen, können die notwendigen Anpassungen jederzeit vornehmen; bis spätestens zum Ablauf des Übergangszeitraums, 20. Juni 2062, müssen sie diese vollziehen.

Es folgt eine Auflistung der Auflagen der einzelnen Euromünzen. In diesen Auflagenzahlen sind die Münzen in den Kleinmünzensätzen inkludiert (sowohl handgehoben als auch polierte Platte und ab dem Jahr 2012 auch jeweils 5000 Stück des Baby-Euro-Münzensatzes; im Jahr 2014 auch die 2500 Stück vom Jahrgangssatz „43. World Money Fair – Berlin“).

Es folgt eine Auflistung der einzelnen Münzausgaben pro Jahr, einmal nach Metall geordnet, einmal nach dem Wert der Münzen geordnet (inklusive der Philharmoniker und 2-Euro-Gedenkmünzen; entscheidend ist das aufgeprägte Jahr, nicht das Ausgabedatum - was vor allem bei Neujahrsmünzen entscheidend ist). Da seit dem Jahr 2011 die 5-Euro-Münzen und seit dem Jahr 2012 auch die 10-Euro-Münzen sowohl in Kupfer als auch in Silber erscheinen, könnte man diese Münzen auch zweimal zählen (je einmal bei Kupfer und einmal bei Silber, bzw. je zweimal bei 5-Euro bzw. 10-Euro). In der folgenden Tabelle werden diese Münzen nur als Silbermünzen gezählt. Wenn man auch die Kupfermünzen mitzählen will, so gelten die Werte in den runden Klammern. Seit 2017 sind 10-Euro-Silbermünzen in polierter Platte farbig, während die handgehobenen 10-Euro-Silbermünzen farblos geblieben sind. Wenn man diese farbigen Münzen dazuzählen will, so gelten die Werte in den eckigen Klammern:

→ "Hauptartikel:" 2-Euro-Gedenkmünzen

Österreich hat bis heute folgende 2-Euro-Gedenkmünzen herausgegeben:
→ "Hauptartikel: Wiener Philharmoniker"

1989 begann die Münze Österreich mit der Prägung der Anlagemünze Wiener Philharmoniker. Anfangs in Schilling und seit 2002 in Euro. Es gibt eine Silberversion mit einem Nennwert von 1,50 Euro und seit dem Jahr 2014 fünf Stückelungen in Gold mit Nennwerten von 4 bis 100 Euro. 2004 wurde anlässlich des 15-jährigen Bestehens der Wiener Gold-Philharmoniker einmalig eine 1000-Unzen-Version mit einem Nennwert von 100.000 Euro und einer Auflage von 15 Stück geprägt. 2009 wurden zum zwanzigsten Jubiläum 20-Unzen-Goldmünzen ausgegeben mit einem Nennwert von 2000 Euro und einer Auflage von 6027 Stück (dreimal 2009, je 2009 wurden nach Europa, nach Amerika und nach Asien (Japan) verkauft). Im Jahr 2016 begann die Münze Österreich mit der Prägung einer Platinversion des Wiener Philharmonikers mit einem Nennwert von 100 Euro. Ab 2017 gibt es auch eine Platinmünze mit einem Nennwert von 4 Euro.

Bei dieser Reihe leuchten die abgebildeten Tiere im Dunkeln nach. Dazu ausgegeben wird ein Sammelalbum, in das alle geplanten 12 Ausgaben eingesteckt werden können.

Zusätzlich zu diesen Serien wurden von 2011 bis 2013 drei 20-Euro-Münzen im Rahmen des europäischen Silberprogramms herausgebracht:

Jede Münze der Reihe „Klimt und seine Frauen“ trägt auf der Bildseite einen Buchstaben. Alle fünf Münzen der Reihe ergeben zusammen den Namen K-L-I-M-T.




</doc>
<doc id="7822" url="https://de.wikipedia.org/wiki?curid=7822" title="Lindau">
Lindau

Lindau ist der Name folgender Orte:


Gemeinden:

Gemeindeteile:

historisch:

sowie:
Lindau ist der Familienname folgender Personen:

Lindau ist der Name folgender Schiffe:
Siehe auch:


</doc>
<doc id="7830" url="https://de.wikipedia.org/wiki?curid=7830" title="Cos">
Cos

Cos steht für:
Personen:
Cós steht für:
cos steht als Abkürzung für:
CoS steht als Abkürzung für:
COS steht als Abkürzung für:
ČOS steht als Abkürzung für:
im National Register of Historic Places gelistete Objekte:
Siehe auch:


</doc>
<doc id="7831" url="https://de.wikipedia.org/wiki?curid=7831" title="Sin">
Sin

Sin oder Šin steht für:

Sin ist der Familienname folgender Personen:


sin steht als Abkürzung für:

SIN steht für:

SiN steht für:

SIN steht als Abkürzung für:

SIN steht als Kfz-Kennzeichen für:


Siehe auch:


</doc>
<doc id="7833" url="https://de.wikipedia.org/wiki?curid=7833" title="American Football">
American Football

American Football (engl. für „Amerikanischer Fußball“), oder auch kurz "Football", ist eine aus den Vereinigten Staaten stammende Ballsportart und die populärste Variante einer Reihe von als Gridiron Football bezeichneten Sportarten.

Im Verlauf eines Spiels, das in vier Vierteln zu 15 (oder 12) Minuten ausgetragen wird, versuchen zwei Mannschaften aus je elf Spielern, den Spielball in die gegnerische Endzone zu bringen oder ein "Field Goal" zu erzielen, um Punkte zu gewinnen. Die sich im Ballbesitz befindende Mannschaft ("Offense", engl. für „Angriff“) kann durch Werfen ("Passing") sowie Laufen ("Rushing" oder "Running") einen Raumgewinn erreichen, der schließlich durch einen "Touchdown" oder ein erzieltes "Field Goal" zu Punktgewinnen führt. Die verteidigende Mannschaft ("Defense", engl. für „Verteidigung“) versucht, die "Offense" daran zu hindern und selbst in Ballbesitz zu kommen. Wenn die Offense einer Mannschaft auf das Feld kommt, hat sie vier Versuche, einen Raumgewinn von zehn "Yards" oder mehr zu erlangen. Schafft sie dies, erhält sie vier neue Versuche, gelingt ihr dies nicht oder verliert sie den Ball durch eine "Interception" oder einen "Fumble", geht das Angriffsrecht an den Gegner. Wenn die "Defense" die "Offense" bis in ihre eigene Endzone zurückdrängt und dort den gegnerischen Ballträger tackelt, kann sie einen "Safety" erzielen. Gewinner ist die Mannschaft, die nach Ablauf der Spielzeit die meisten Punkte erzielt hat.

Grundgedanke des Spiels ist es, Raum zu gewinnen. Da ein Spielfeld (100 Yards) räumlich begrenzt ist, wird das Erreichen der Endzone mit Punkten belohnt. Punkte in unterschiedlicher Anzahl können auf verschiedene Weisen erzielt werden.

Die Regeln weichen je nach Organisation teilweise voneinander ab. Im Amateurbereich, wie auch im Weltverband "International Federation of American Football" (IFAF), gelten nahezu unverändert die "NCAA"-Regeln. Die US-amerikanische Profiliga "National Football League" hat teilweise abweichende Regeln.

Punkte können erzielt werden, wenn der Football mittels eines Lauf- oder eines Passspielzugs ("Run" bzw. "Pass") über die gegnerische "Goalline" getragen oder in der Endzone gefangen wird. Der Spieler muss bei einem Passspielzug mit beiden Beinen (NFL-Regeln) oder einem Bein (NCAA-Regeln) in der Endzone aufkommen und dabei den Ball kontrollieren, d. h., ihn sicher gefangen haben. Bei einem Laufspielzug genügt es, wenn der Ball die imaginäre Goalline durchstößt, während der ihn kontrollierende Spieler sich innerhalb des Spielfeldes befindet bzw. die Innenseite eines Cones (Spielbegrenzungsfähnchens) berührt. Selbiges gilt, wenn ein Pass vollständig ist und der "Receiver" (Fänger) nach dem "Catch" (Fang) bis in die Endzone läuft. Dies ist ein Touchdown (TD), der sechs Punkte zählt.
Touchdowns können auch nach einer Interception oder einem Fumble erzielt werden "(Defensive Touchdown)". Ebenso nach einem Kickoff- oder einem Punt-Return und auch nach einem verpassten Field Goal.

Nach einem Touchdown hat die angreifende Mannschaft zudem die Möglichkeit, den Spielstand durch einen "Point after Touchdown" (PAT, engl. "Punkt nach dem Touchdown", das Kicken des Balles durch die gegnerischen Torstangen) um einen oder durch eine "Two-Point Conversion" (engl. "Zwei-Punkt-Verwandlung", das erneute Tragen oder Werfen des Balles in die Endzone des Gegners) um zwei Punkte zu erhöhen. Die "Two-Point Conversion" hat jedoch geringere Erfolgsaussichten. Der PAT wird in der Regel von der gegnerischen 2-Yards-Linie, in der NFL seit der Saison 2015 von der 15-Yard-Linie, ausgeführt, die Conversion von der 3-Yard-Linie, können aber durch eine Strafe bedingt auch aus größerer Entfernung beginnen.

Falls ein Touchdown nicht mehr erreichbar erscheint, kann ein Kick durch die gegnerischen Torstangen versucht werden ("Field Goal"), der bei Erfolg drei Punkte einbringt.

Darüber hinaus kann die verteidigende Mannschaft einen "Safety" (zu Boden Bringen des Ball führenden Spielers in seiner eigenen Endzone) erzielen, der der betreffenden Mannschaft zwei Punkte einbringt. Gelingt es der verteidigenden Mannschaft bei einem PAT-Versuch oder der "Two-Point Conversion" den Ball zu erobern und ihn in die gegnerische Endzone zu tragen, erhält sie ebenfalls zwei Punkte.

American Football wird als eine Folge von Spielzügen "(Plays)" gespielt. Alle aufeinanderfolgenden "Plays" eines Teams, ohne dass das Angriffsrecht wechselt, nennt man "Drive".

Zu Beginn eines Spielzuges befindet sich eine Mannschaft in Ballbesitz und somit im Angriff "(Offense)". Sie muss versuchen, durch Pass- oder Laufspielzüge Raum zu erobern, um schließlich die Endzone zu erreichen und Punkte zu erzielen. Ein Spielzug startet, wenn der Ball "gesnapt" (bewegt) wird.

Der "Offense" stehen jeweils vier Versuche "(Downs)" zur Verfügung, um mindestens zehn Yards Raumgewinn zu erreichen und damit das Angriffsrecht für weitere vier Versuche zu erhalten (neues "First Down"). Gelingt ihr dies nicht, muss sie den Ball abgeben, und die andere Mannschaft erhält das Angriffsrecht "(Turnover on Downs)".

Der Quarterback (oder ein anderer offensiver Spieler) versucht, den Ball einem fangberechtigten Spieler der Offense zuzuwerfen, der eine vorher festgelegte Passroute läuft. Fangberechtigt ist ein Spieler, wenn er nicht an der Line of Scrimmage steht oder eine der beiden äußersten Positionen an der Line of Scrimmage besetzt. Der Werfer muss sich hinter der Line of Scrimmage befinden. Der Pass kann unvollständig "(incomplete)" sein, gefangen "(Catch)" oder von der Defense abgefangen (Interception) werden. 

Ein Pass ist "incomplete", wenn er den Boden berührt (durch einen schlechten Wurf oder einen Verteidiger) oder "out of Bounds" ist (gefangen, ohne dass der Receiver einen Fuß "in bounds" (im Spielfeld) hatte, in der NFL müssen sogar beide Füße "in bounds" sein). Der nächste Versuch startet auf der Höhe der alten Ballposition. Nach einem "Catch" darf der Spieler so weit laufen, wie er kann. Wird er zu Boden gebracht oder verlässt er das Spielfeld, ist der Spielzug beendet. Das nächste "Down" startet an der Stelle, wo der vorherige Spielzug gestoppt wurde. Bei einem Passspielzug dürfen die Offensive Linemen nicht vor dem Pass über die Line of Scrimmage (downfield) gehen. Pro Spielzug ist nur ein Vorwärtspass erlaubt. Wird der Quarterback getackled, bevor er die Line of Scrimmage überquert oder bevor er einen Pass wirft, zählt das als "Sack".

Eine besondere Variante des Passspiels ist der Lateralpass. Dabei wird der Ball parallel zur Line of Scrimmage bzw. nach hinten geworfen. Dies darf, ebenso wie sogenannte "Handoffs", beliebig oft pro Spielzug wiederholt werden. Des Weiteren ist es dem Quarterback erlaubt, selbst als Ballträger zu fungieren und Raumgewinn zu erzielen "(Scrambling)". Der Spielzug zählt dann als Laufspielzug.

Laufspielzüge werden durch eine Übergabe des Balls oder durch ein einfaches Zuwerfen ("Pitch"/"Lateral" – kein Vorwärtspass) an einen Ballträger 
eingeleitet. Ballträger sind gewöhnlich Runningbacks, also Halfback und Fullback. Aber auch jeder andere fangberechtigte Spieler der Offense kann Ballträger sein. Der Ballträger versucht nach Ballerhalt so weit wie möglich in Richtung der gegnerischen Endzone zu kommen, während seine Mitspieler versuchen, die Verteidiger zu blocken, d. h., vom Tacklen abzuhalten. Der Spielzug endet mit einem Tackle, dem Verlassen des Spielfeldes oder, falls es der Ballträger bis in die gegnerische Endzone schafft, mit einem Touchdown.

Kickspielzüge werden in sogenannte "Non-Scrimmage Kicks" und "Scrimmage Kicks" unterteilt. 

Ein "Non-Scrimmage Kick" ist ein Kickspielzug, der ohne vorherigen Snap ausgeführt wird. Nach einem Touchdown, einem Field Goal und zu Beginn jeder Halbzeit muss mit dem Kickoff ein Kickspielzug ausgeführt werden. Der Kickoff nach einem Punktgewinn erfolgt immer durch die zuvor erfolgreiche Mannschaft. Ein Kickoff ist nach 10 Yards oder nach einer Berührung des Balles durch einen Spieler des "Receiving-Teams" frei. Möchte das "Kicking-Team" den Ball möglichst schnell zurück haben, erfolgt der Kickoff möglichst kurz, um eine bessere Chance zu haben ihn zu erobern ("Onside Kick"). Gleichzeitig besteht die Gefahr, dass der Gegner den Ball in einer guten Position übernimmt. Um dieses Risiko zu minimieren, kann der Ball auch weit nach hinten geschossen werden, wo das Receiving-Team ihn zurücktragen kann.

Nach einem Safety erfolgt von der zuvor nicht erfolgreichen Mannschaft ein "Free Kick" (auch "Safety Kick") von der eigenen 20-Yard-Linie. Dieser Kick kann ebenfalls von beiden Mannschaften erobert werden.

Fängt ein Spieler des "Receiving-Teams" den Ball nach einem Kickoff, Punt oder Free Kick mittels eines "Fair Catchs", so kann die Mannschaft im darauf folgenden Spielzug vom Ort des Fair Catches einen "Fair Catch Kick" ausüben. Hierbei wird versucht, ein "Field Goal" zu erzielen. Der Versuch erfolgt wie ein Kickoff, also ohne vorherigen Snap, nur ohne "Kicking-Tee". Diese Regel gibt es allerdings nicht in der NCAA.

Ein "Scrimmage Kick" ist ein Kickspielzug, der mit einem "Snap" beginnt. Zumeist wird dieser ausgeführt, wenn nach drei Versuchen absehbar ist, dass der nötige Raumgewinn für ein neues erstes Down nicht erzielt werden kann. Dann wird der Ball im vierten Versuch in der Regel durch einen sogenannten "Punt" möglichst weit in Richtung gegnerischer Endzone gekickt, damit der Gegner das Angriffsrecht in einer möglichst schlechten Position übernehmen muss. Ein Punt kann dabei von der gegnerischen Mannschaft ("Receiving-Team") im Feld gefangen werden und möglichst weit nach vorne getragen werden ("Punt Return"). Ist man schon nahe beim gegnerischen Tor, versucht man anstelle eines Punts ein "Field Goal" zu erzielen.

Die Spielzeit in den USA beträgt vier mal 15 Minuten "(Quarter)". Highschoolteams sowie die Amateurmannschaften Europas spielen lediglich vier mal zwölf Minuten. Dabei handelt es sich um eine echte (Netto-)Spielzeit. Die Pausen zwischen den Quartern betragen zwei Minuten und die Halbzeitpause maximal 20 Minuten. Die Uhr wird bei einer Auszeit, nach einem "Kickoff", einem unvollständigen Pass, wenn der Ballträger "out of Bounds" geht, nach erzielten Punkten, bei manchen Strafen und nach einer Two-Minute Warning (Auszeit durch die Schiedsrichter, zwei Minuten vor dem Ende jeder Halbzeit) gestoppt. Wenn der Ballträger "in bounds" gestoppt wird, läuft die Uhr weiter. Ein laufender Spielzug wird durch Fouls, Auszeiten oder Ballbesitzwechsel nicht unterbrochen. Dementsprechend wird die Uhr in solchen Fällen erst nach Beendigung des Spielzugs gestoppt. Ein angefangener Spielzug wird immer fertig gespielt, auch wenn die Spielzeit im jeweiligen Quarter ausgelaufen ist. Die Uhr wird entweder bei Freigabe des nächsten Spielzuges oder erst beim Snap wieder gestartet. Ein Spielzug muss in der NFL, im College (NCAA) und in Deutschland situationsabhängig 40 Sekunden nach dem Ende des letzten Spielzugs oder 25 Sekunden nach Ballfreigabe durch den Referee beginnen, nach einem "Timeout" nach 60 Sekunden (angezeigt auf der "Play Clock"). Daraus ergeben sich je nach Spielsituation gegen Ende des Spiels viele strategische Möglichkeiten. Hat die führende Mannschaft den Ball, kann sie Zeit schinden, indem sie Laufspielzüge ausführt und die Uhr herablaufen lässt. Die Defense kann dann mit Timeouts das Verstreichen von 40 Sekunden verhindern. Im Gegenzug wird das kurz vor Ende zurückliegende Team Pässe nahe der Seitenlinie spielen, um die Uhr möglichst oft anzuhalten. Durch das sehr häufige Anhalten der Uhr dauert ein Football-Spiel in der Regel zwischen zweieinhalb (Amateure) und dreieinhalb Stunden (NFL).

Sollte nach der regulären Spielzeit ein Punktegleichstand herrschen, folgt eine "Overtime". Dabei unterscheiden sich die Regeln von NFL und NCAA grundsätzlich voneinander. Bei der NFL wird eine 10-minütige Verlängerung nach dem Sudden-Death-Prinzip gespielt. Zuerst erfolgt der Münzwurf "(Toss)". Der Gewinner entscheidet, ob er angreift "(Offense)" oder verteidigt "(Defense)" bzw. in welche Richtung er spielen möchte. Für gewöhnlich wird das Angriffsrecht gewählt. Es folgt anschließend wie gewöhnlich der "Kickoff". Es gewinnt dann die Mannschaft, die zuerst Punkte erzielt, egal auf welche Weise, außer es handelt sich dabei um ein Field Goal der zuerst angreifenden Mannschaft beim ersten "Drive", dann erhält die andere Mannschaft den Ballbesitz. Erzielt jedoch eine Mannschaft einen Touchdown oder eine Safety, gewinnt diese sofort und das Spiel ist beendet. Sollten nach Ende der 10 Minuten keine Punkte erzielt worden sein (oder immer noch Gleichstand bestehen, weil beispielsweise beide Mannschaften in ihrem jeweils ersten Ballbesitz ein Field Goal erzielt haben), so endet das Spiel unentschieden. In Entscheidungsspielen, z. B. den "Play-offs", wird auch nach dem Sudden-Death-Prinzip gespielt, aber hier dauert die Verlängerung 15 Minuten. Sollte dann noch keine Entscheidung gefallen sein, werden solange "Overtimes" angehängt, bis eine Entscheidung herbeigeführt ist.

Nach den NCAA-Regeln hat jedes Team in der "Overtime" einen "Drive", wobei dieser jeweils an der gegnerischen 25-Yards-Linie gestartet wird. Im Gegensatz zum Sudden-Death haben beide Teams gleiche Chancen. Sollte nach einer "Overtime" Gleichstand herrschen, werden so lange "Overtimes" angehängt, bis es eine Entscheidung gibt. Ab der dritten Overtime ist ein "PAT" nicht mehr zulässig. Nach einem "Touchdown" muss daher eine "Two-Point Conversion" gespielt werden.

Eine Regelverletzung wird mit einer Strafe (engl. "Penalty" [] ()) geahndet. American Football hat eines der umfangreichsten Regelwerke aller Sportarten. Wegen seiner physischen Härte besteht ein hohes Verletzungsrisiko. Die meisten Regeln dienen daher dazu, Verletzungen der Spieler zu vermeiden. Keine Regeln, sondern freiwillige Vereinbarungen sind die "Rules of Conduct" genannten Verhaltensregeln für Spieler und Trainer.

Beim American Football werden Strafen durch die Schiedsrichter mit Hilfe von gelben Flaggen, die auf den Ort des Fouls "(Spot of Foul)" geworfen werden, angezeigt. Der Grund ist, dass viele Strafen nicht sofort zur Unterbrechung des Spielzuges führen, sondern erst im Anschluss verhängt werden. Bei Strafen gegen beide Teams heben diese sich meist gegenseitig auf.

Grundsätzlich werden Regelverstöße mit Yards-Strafen, d. h. mit Raumverlust, geahndet. Das gefoulte Team kann dabei meist entscheiden, ob es die Strafe annimmt (der Versuch wird mit dem entsprechenden Raumverlust wiederholt) oder ablehnt (der nächste Versuch wird ganz normal gespielt). Wird durch eine Strafe gegen die Defense die "Line to Gain" (die Linie, die die Offense erreichen muss, um vier neue Versuche zu bekommen) erreicht, erhält die Offense ein neues "First Down". Einige Strafen beinhalten auch ein automatisches "First Down".

Die Endzone kann durch Strafen im normalen Spielverlauf nicht erreicht werden. Ausnahme bildet ein sogenannter "Palpably Unfair Act" (offenkundig unfaires Vorgehen), bei dem es dem Referee, nach Beratung mit seinen Kollegen, erlaubt ist, auch einen Touchdown oder anderen Score als Strafe zu verhängen. Würde eine Strafe den Abstand zur Endzone mehr als halbieren, wird dieses Verfahren angewandt "(Half the Distance to the Goal)", nicht jedoch bei "Pass Interference", da dort am Punkt des Fouls weitergespielt wird.

Bei besonders schweren Vergehen kann ein Spieler auch vom Spiel ausgeschlossen "(ejected)" werden. Dies gilt insbesondere bei Fouls mit Verletzungsabsicht, grob unsportlichem Verhalten sowie Beleidigung von Schiedsrichtern und anderen Spielteilnehmern (gegnerische Spieler, Trainer, Zuschauer). Priorität hat immer der Schutz der Spieler vor Verletzungen und die Kontrolle des Spielgeschehens.

Einige der wichtigsten Regelverstöße und die Strafen nach den NFL-Regeln
Die NFL ist bis heute eine der wenigen Sportligen, in denen der Videobeweis zur Überprüfung strittiger Szenen eingeführt wurde. Strittige Entscheidungen sind z. B., ob es ein Fumble war, wo genau der Spielzug endete oder ob ein Pass im Feld gefangen wurde. Solange sein Team noch mindestens ein Timeout hat, kann ein Head Coach zweimal pro Spiel eine solche Überprüfung durch das Werfen einer roten Flagge auf das Spielfeld beantragen, sodass die fragliche Entscheidung, sofern ihm Recht gegeben wird, revidiert wird. Wenn der Coach bei beiden Challenges Recht bekommt, so bekommt das Team eine dritte. Eine verlorene Challenge resultiert in der Aberkennung eines Timeouts. Nach der "Two-Minute Warning" (die letzten zwei Minuten vor Ende jeder Halbzeit) sowie in einer Verlängerung (bei Gleichstand nach dem 4. "Quarter") kann nur noch der Oberschiedsrichter und der offizielle Spielerbeobachter eine Challenge beantragen.

Gespielt wird auf einem 120 Yards (109,73 Meter) langen und etwa 53 Yards (48,46 Meter) breiten Spielfeld, das in zwölf gleich große Abschnitte zu je zehn Yards eingeteilt ist (sogenanntes "„Gridiron“" oder „Eisengitter“). Die einhundert Yards in der Mitte werden als aktives Spielfeld benutzt, die restlichen zehn Yards an jedem Spielfeldende haben im Spielablauf eine besondere Bedeutung; sie wurden erst 1912 eingeführt und heißen "„Endzonen“".

Die Begrenzungen des Spielfelds (die Seiten- und Endauslinien) gehören in ihrer ganzen Breite nicht mehr zum Spielfeld. Wer auf diese tritt, befindet sich demzufolge schon im Aus. Die Goallinien andererseits gehören in ihrer ganzen Breite zu den Endzonen.

Am Ende jeder Endzone befindet sich ein Goal, das wie eine überdimensionierte Stimmgabel aussieht. Ein nach hinten gebogener und als Sicherheit für die Spieler gepolsterter Pfosten trägt eine Querstange, welche sich 10 Fuß (3,05 Meter) über dem Boden befindet; die senkrechten Stangen am Ende der Querstange reichen 30 Fuß (in der NFL seit 2014 35 Fuß) in die Höhe. Am oberen Ende jeder senkrechten Stange befindet sich eine rote Windfahne zur Orientierung für die Kicker. Die senkrechten Stangen sind in hohen Spielklassen 18 Fuß und 6 Zoll (18,5 Fuß (5,64 Meter)) voneinander entfernt, in niedrigen Spielklassen 23 Fuß und 4 Zoll (7,11 Meter). 
Ausgehend von den Endzonen sind im Abstand von je fünf Yards Querlinien eingezeichnet, und alle zehn Yards befindet sich eine entsprechende Beschriftung. Die Zählung der Yard-Linien beginnt an beiden Endzonen bei Null (genannt "„Goalline“") und trifft sich dann in der Mitte an der 50-Yards-Linie. Der Bereich von der 20-Yards-Linie bis zur Endzone wird als die sogenannte "„Red Zone“" bezeichnet, da bei einem Ballbesitz in diesem Bereich die Wahrscheinlichkeit, erfolgreich zu punkten, relativ hoch ist.

Darüber hinaus wird das Spielfeld in Längsrichtung von zwei parallelen Reihen von "„Hash Marks“" unterteilt. Endet der letzte Spielzug außerhalb dieser Markierungen, startet der nächste Spielzug auf der nächstgelegenen "„Hash Mark“". Die "„Hash Marks“" haben außerdem eine Ein-Yard-Unterteilung, die den Schiedsrichtern beim korrekten Platzieren des Balles hilft. Die "„Hash Marks“" haben im Profifootball einen Abstand von 18,5 Fuß (5,64 Meter), beim Amateur- und Collegefootball von 40 Fuß (12,20 Meter).

Amateur-Football-Partien in Europa werden meist auf einem Fußballplatz ausgetragen. Da diese deutlich breiter als ein Footballfeld sind und der Abstand der Tore nicht den 120 Yards (109,728 Meter) entspricht, die ein Footballfeld erfordert, kann das Feld entweder in zwölf gleich große Abschnitte unterteilt werden und die Messkette in ihrer Länge entsprechend angepasst (wird z. B. in Deutschland angewandt), oder es wird das aktive Spielfeld auf unter 100 Yards verkürzt und die Kette bei zehn Yards belassen (wird z. B. in Österreich angewandt). Falls die Fußballtore selbst nicht durch ein Footballtor ersetzt werden können, wird mittels Polster an den Pfosten das Verletzungsrisiko der Spieler vermindert. Mit zusätzlichen Peilstangen an den Pfosten wird dann ein Footballtor improvisiert.

American Football wurde erstmals 1869 an Universitäten, zum Beispiel der Rutgers University und der Princeton University, die am 6. November 1869 das allererste Spiel bestritten, im Osten der Vereinigten Staaten gespielt. In den nächsten Jahren hielten vor allem die Universitäten Harvard, Yale, Columbia und Princeton einige Turniere ab. Es hat seine Wurzeln im Fußball, Rugby und Canadian Football. Ungefähr bis zur Gründung der National Football League (NFL) 1920 war American Football gleichbedeutend mit College Football, der durch die National Collegiate Athletic Association (NCAA) organisiert wurde.

Ein wichtiger Football-Offizieller war Walter Camp, der unter anderem 1880 die Line of Scrimmage sowie 1882 die, vorerst jedoch nur drei, Versuche "(Downs)" einführte und 1883 die Spielerzahl je Mannschaft auf elf begrenzte.

In seinen Anfangsjahren war American Football weit gefährlicher als heute. Die Spieler hatten keine Schutzausrüstung, und viele der heute gültigen Regeln zum Schutz der Spieler existierten nicht. Insbesondere wurde der Ballträger oft von seinen Teamkameraden vorwärts geschoben. Nachdem im Jahre 1905 achtzehn Tote infolge von Spielunfällen zu beklagen waren, forderte US-Präsident Theodore Roosevelt neue Regeln, um das Spiel sicherer zu machen. Dies führte 1906 zur Einführung der neutralen Zone zwischen den Linien, zur Regel, dass mindestens sechs (gegenwärtig sieben) Spieler an der Line of Scrimmage stehen müssen, sowie zu verschiedenen anderen Schutzregeln. Die am weitesten reichende Änderung war die Einführung des Vorwärtspasses, während bis zu diesem Zeitpunkt lediglich Laufspielzüge und Rückwärtspässe erlaubt waren.

1910 wurden schließlich die verschränkten Formationen verboten, was zu einem Rückgang von zum Teil tödlichen Verletzungen führte. 1912 wurden die Größe des Spielfeldes und die Zählweise der Punkte neu festgelegt sowie der vierte Versuch "(Down)" eingeführt. Damit erhielt das Spiel schließlich seine moderne Form. Bis heute werden allerdings jedes Jahr Regeln modifiziert, sowohl mit dem Ziel der verbesserten Sicherheit der Spieler als auch im Bestreben, die Attraktivität für den Zuschauer weiter zu steigern.

1932 wurde American Football bei den Olympischen Spielen in Los Angeles als Demonstrationssportart ausgetragen, wurde jedoch nie olympisch. Erst im Dezember 2013 erkannte das Internationale Olympische Komitee American Football als Sport an.

Zu Varianten des American Football gehören Canadian Football und Arena Football, auch wenn diese sich in manchen Regeln deutlich unterscheiden. Das Spielprinzip, die Grundlagen, das Spielgerät, die Aufteilung des Feldes und viele andere Komponenten sind jedoch weitgehend identisch. Auf den ersten Blick erkennbar sind die abweichenden Spielfeldgrößen sowie die Mannschaftsstärken. Der Australian Football gehört nicht zu dieser Gruppe, sondern ähnelt stark dem Rugby. Im Freizeitsport wird Flag Football in kleinen Teams ohne jegliche Schutzausrüstung gespielt, wobei ein Tackle durch Wegnehmen einer am Gürtel befestigten „Flagge“ simuliert wird. Für körperlich Behinderte wurde die Variante des Rollstuhlfootballs entwickelt.

Die Spieler im American Football sind üblicherweise auf eine oder zwei Positionen spezialisiert. Da bei jedem Spielzug ausgewechselt werden darf, können immer die für den geplanten Spielzug am besten geeigneten Akteure eingesetzt werden. Dabei können die Head Coaches während der Regular Season und den Play-offs aus einem Kader von maximal 53 aktiven Spielern auswählen, wobei ein Spieler der Offense, Defense und/oder dem Special Team zugeordnet ist. Insbesondere die Offense kann teilweise auf mehrere hundert Spielzüge und Kombinationen zurückgreifen. Als Gedächtnisstütze und zur Vermeidung von Fehlern tragen viele Spieler ein Band am Arm, an dessen Innenseite Zahlen, Namen, Positionen, Spielzüge und anderweitige Dinge zum Spielverlauf in Stichpunkten notiert sind. Die Spieler, die zu Beginn des Spiels die Stammformation einer Mannschaft bilden, werden dabei Starter genannt.

Der "Quarterback" (QB) ist der zentrale Spieler der Offense. Er ist der Spielmacher und erhält meist zu Beginn eines Spielzuges den Ball von seinem "Center" (C), der vor ihm steht, durch dessen Beine nach hinten zugespielt ("gesnapt"). Damit ist der Center bei jedem Spielzug am Ball. Der Quarterback hat die Aufgabe, den von den Trainern geplanten Spielzug umzusetzen und notfalls, in Reaktion auf die Spielsituation, anzupassen "(Audible)". Üblicherweise übergibt er den Ball dann an einen Ballträger "(Runningback)" oder wirft ihn zu einem Passempfänger "(Receiver)".

Vor dem Quarterback stehen die fünf "Offensive Linemen" (OL). Sie werden unterschieden in "Center", "Guards" und "Tackles" (von innen nach außen). Diese üblicherweise sehr großen und schweren Spieler haben die Aufgabe, den "Quarterback" vor den Verteidigern zu schützen ("Pocket"-Bildung beim Pass) und bei Laufspielzügen den Weg für den Ballträger freizublocken. Tackles sind dabei die schwersten und kräftigsten Spieler im Angriff. Guards haben ähnliche Aufgaben wie Tackles. Ein Guard wird gelegentlich auch für so genannte Pull-Manöver eingesetzt. Dabei blockt er nicht von seiner ursprünglichen Position aus, sondern zieht hinter der O-Line nach außen, läuft dann erst feldabwärts und räumt dem Ballträger den Weg frei. Die Linemen dürfen keine Pässe empfangen.

Die Ballträger selber werden "Runningback" (RB) oder "Tailback" genannt, da am hinteren Ende der Angriffsformation aufgestellt. Man unterscheidet zwischen "Fullback" (FB) und "Halfback" (HB). Der Fullback ist schwerer und kräftiger als der Halfback und wird in Situationen eingesetzt, in denen nur wenige Yards Raumgewinn erzielt werden müssen. Ansonsten fungiert er überwiegend als Vorblocker für den Halfback und als zusätzlicher Blocker bei Passspielzügen. Bei der Aufstellung gibt es auch hier verschiedene Formationen (z. B. Wishbone-, I-, Pro-Formation).

Bei einem Pass wird der Ball vom "Quarterback" in der Regel zu einem der "Wide Receiver" (WR) geworfen, der aufgrund seiner hohen Geschwindigkeit sehr schnell und weit in das gegnerische Territorium vordringen kann oder kürzere Routen läuft. Weitere Optionen sind die Runningbacks oder Tight Ends (TE). Legale Passempfänger sind alle Spieler außer der O-Line. Mindestens sieben Spieler müssen beim Snap an der Line of Scrimmage stehen.

Der "Tight End" ist an einem Ende der Offensive Line aufgestellt, wie ein zusätzlicher Lineman. Er ist aber passempfangsberechtigt. Der Tight End ist ein Allroundspieler, der je nach Situation blockt wie ein "Offensive Lineman" oder den Ball fängt wie ein "Wide Receiver". Zudem wechselt er oft als „Man in Motion“ vor dem Snap seine Position, um dann z. B. auch als Vorblocker für Laufspielzüge oder auch selbst als „Runningback“ zu fungieren.

Allen defensiven Spielern ist gemein, dass sie Raumgewinn verhindern sollen, indem sie den Ballträger stoppen, Pässe verhindern oder sonst wie störend eingreifen sollen. Hinzu kommen aber noch positionsspezifische Aufgaben.

Die "Defensive Linemen" (DL) stehen der "Offensive Line" direkt gegenüber, wobei diese Spieler auch eine vergleichbare schwere Statur haben. Die Abwehrlinie soll das Freiblocken von Lücken für den gegnerischen Runningback verhindern. Bei Passspielzügen sollen sie den Quarterback durch Druck zu Fehlern zwingen oder gleich "sacken". Bei den Defensive Linemen wird zwischen "Defensive Ends" (DE) und "Defensive Tackles" (DT) unterschieden. Die Defensive Ends stehen an den Enden der Defensive Line. Sie sind agiler als ihre O-Line-Kollegen, da sie Läufe des gegnerischen Ballträgers über die Außenseite verhindern bzw. von außen Druck auf den gegnerischen Quarterback ausüben sollen und damit längere Wege gehen müssen. Die Defensive Tackles sollen in der Mitte die Stellung halten und verhindern, dass dort Raumgewinne erzielt werden. Manche Teams benutzen zwei Tackles, manche drei, andere dagegen nur einen. Der mittlere Mann wird dann auch "Nose Tackle" oder "Nose Guard" genannt, weil er dem Center des Gegners „Nase an Nase“ gegenübersteht.

Die "Linebacker" (LB) stehen dicht hinter der Defensive Line. Sie müssen kräftig genug sein, um den Durchbruch eines Runningbacks zu stoppen oder bei "Blitzes" druckvoll zum Quarterback vorzudringen. Gleichzeitig sind sie auch in der Passverteidigung wichtig, da sie den vorderen Bereich gegen kurze, schnelle Pässe abdecken können müssen. Bei Spielzügen mit vier oder fünf Receivern sind die Linebacker aber nicht so flink, dass sie die Receiver bei langen Pässen decken können. Damit die Offense aus dieser Überzahlsituation (viele schnelle Wide Receiver gegen wenige schwere Linebacker) nicht zu viele Vorteile ziehen kann, werden daher die Linebacker gegen Cornerbacks ausgetauscht (Nickel- und Dime-Formation).

Die hintere Verteidigungsreihe bilden die "Safeties" (S), die zusammen mit den "Cornerbacks" (CB) die "Defensive Backs" (DB) (auch Secondary genannt) darstellen. Die Cornerbacks verteidigen hauptsächlich gegen ein gegnerisches Passspiel, die Safeties sind dagegen eher eine Art letzte Bastion, wenn es den vorderen Reihen nicht gelungen ist, einen Ballträger zu stoppen. Bei den Safeties unterscheidet man zwischen dem „Strong Safety“ (SS) und „Free Safety“ (FS). Der Strong Safety ist kräftiger und steht etwas näher an der Line of Scrimmage (oft auch in der Linebacker-Reihe, circa fünf Yards hinter der Line), weil er gegen den Laufspielzug arbeitet und den Tight End abdeckt, der eher kurze Laufrouten hat und deutlich schwerer als ein gewöhnlicher Receiver ist. Der „Free Safety“ hat eher Cornerback-artige Eigenschaften. Er agiert als zusätzlicher Cornerback im tiefen Rückraum und deckt entweder die tiefe Zone ab oder hilft Cornerbacks beim Covern der Receiver.

Damit die Abwehrspieler nicht unkontrolliert eigenständig agieren, gibt es hier (wie auch in der Offense) sehr genau vorausgeplante Spielzüge, die vom "Defensive Coordinator" und dem "Headcoach" während des Spieles angesagt werden, um auf die Offense(-Formation) zu reagieren.

Die gebräuchlichsten Aufstellungen in der Defense sind die "4–3" und die "3-4-Defense" die unter dem Sammelbegriff "7-Man-Front" zusammengefasst werden. Bei der "4-3-Defense" befinden sich vier Spieler in der "Defensive Line", drei "Linebacker" dahinter sowie je zwei "Cornerbacks" und "Safeties" auf dem Feld. Eine "3–4" ist beweglicher, man kann durch die vier Linebacker leichter einen Blitz (Angriff auf den gegnerischen Quarterback) durchführen und/oder die Passempfänger decken. Allerdings benötigt man drei starke Männer in der Defensive Line, die gegen fünf direkte Gegenspieler bestehen müssen.

Im Amateur- und Collegebereich werden auch häufiger "8-Man-Fronts" gespielt. Dazu gehören zum Beispiel die "5–3", "4–4" und "6–2". Diese Fronten eignen sich besser gegen das Laufspiel, weisen aber beim Passspiel größere Schwächen auf. Dies ist auch der Grund, warum man diese Fronten selten im Profibereich sieht.

Typische Passverteidigungen sind die "Nickel", "Dime" und "Quarter". Bei diesen werden ein oder mehrere Defensive Backs gegen Lineman und/oder Linebacker ausgetauscht.
In Shortyardage- und Goalline-Situationen wird eine so genannte Goalline Defense gespielt. Diese besteht normalerweise aus mindestens 6 Defense Linemen, die die Gaps der Offense schließen sollen.

"Special Teams" treten nur in besonderen Spielsituationen an, meist wenn der Ball gekickt werden soll, also wenn eine Mannschaft durch den "Kicker" (K) den "Kickoff" durchführt, ein "Field Goal" versucht oder der "Punter" (P) "punten" will. Da diese weiter weg als ein Quarterback stehen, wird hierfür ein längerer Snap durch den Center benötigt, weshalb hierbei ein Spezial-Center, der sogenannte "Long Snapper" (LS), zum Zuge kommt.

Beim Kickoff wird der Ball von der Mitte der eigenen 30-Yard-Linie (bei Amateurligen oft von der 35) getreten, und ein gegnerischer Ballempfänger "(Kickoff-Returner)" versucht, den Ball so weit wie möglich zurückzutragen. Ein Field-Goal-Versuch beendet den Ballbesitz, egal bei welchem der vier Downs er versucht wird. Bei Ballbesitz zwischen 35-Yard-Linie und der Endzone spricht man von "Field-Goal-Reichweite" ("Field Goal Range"), da mit Endzonenbreite und weiteren ca. sieben Yards insgesamt 50 Yards Distanz erreicht werden, aus der man dem Kicker noch ein erfolgreiches Field Goal zutraut. In günstigen Situationen (z. B. "Windy City" Chicago) sind aber auch Field Goals aus über 60 Yards möglich.

Der "Returner" (je nach Situation Kick Returner, Punt Returner oder Return Specialist genannt) soll den Ball fangen und in Richtung gegnerische Endzone tragen. Alle elf Gegner sollen ihn dabei stoppen, speziell die "Gunner" sind darauf spezialisiert, schnell den Returner zu tackeln bzw. zu einem Fair Catch zu zwingen. Der Returner kann auch vor dem Fang des Balles durch Schwenken der Arme über dem Kopf einen so genannten "Fair Catch" anzeigen. Dann darf er vom Gegner nach dem Fang nicht angegriffen werden, kann aber keinen weiteren Raumgewinn erzielen.

Da ein "Kickoff" im Gegensatz zum "Punt" immer ein „freier Ball“ ist und somit von beiden Mannschaften aufgenommen werden kann, muss der Returner entscheiden, ob er in der Situation ist, den Ball sicher zu fangen und noch Raumgewinn zu erzielen, oder ob er schon so von den anstürmenden Gegnern unter Druck steht, dass er den "Fair Catch" anzeigt. Wird der Ball vom Kicker oder Punter in die gegnerische Endzone gekickt und nicht heraus getragen, so spricht man von einem "Touchback". Nach einem Touchback startet die empfangende "(receiving)" Mannschaft den Angriffsversuch von der eigenen 20-Yard-Linie (in der NFL ab der Saison 2016 von der 25-Yard-Linie). Fängt ein Receiver den Ball weit in der eigenen Endzone und will den Ball, z. B. wegen anstürmender Gegner, nicht mehr ins Spiel bringen, so kann er sich in der Endzone hinknien, was ebenfalls in einem Touchback resultiert.

Auch auf der Seite des nicht kickenden Teams gibt es Spezialisten. So gehen etwa die "Kick Blocker" bzw. "Punt Blocker" aggressiv auf den Kicker bzw. Punter während der Trittbewegung drauf und versuchen, den anfliegenden Football zu blocken.

Aufgrund der Komplexität des American Footballs wird eine Footballmannschaft von mehreren Trainern gecoacht. Der Head Coach ist der Oberste in der Trainerhierarchie. Er ist für die Betreuung der Mannschaft zuständig und überwacht sowohl das Training als auch alle Entscheidungen in einem Spiel. Zusätzlich ist er für die Entwicklung der Spielzüge verantwortlich. Unter ihm agieren im Trainerstab der Offensive Coordinator, der Defensive Coordinator und der Special Teams Coordinator, welche die Mannschaftsteile (Offense, Defense oder Special Teams) betreuen und im Spiel teilweise die Spielzüge ihren Mannschaftsteilen ansagen. Zusätzlich kann es weitere Trainer geben, beispielsweise für bestimmte Positionen, körperliche Leistungsfähigkeit oder koordinative Fähigkeiten. Amateurmannschaften haben meistens drei bis fünf Trainer, (semi-)professionelle Mannschaften über zehn Trainer.

Aufgrund der Komplexität (das Regelwerk der NCAA hat mit Regeln und Ausnahmen knapp 700 Anwendungen) und des oft unübersichtlichen Spielgeschehens gibt es beim American Football eine ganze Schiedsrichter-Crew. Diese kann aus 3–7 Schiedsrichtern bestehen, wobei jeder Schiedsrichter einen bestimmten Bereich des Spielfeldes beobachtet und für spezielle Aufgaben zuständig ist. Oberschiedsrichter ist der "Referee", umgangssprachlich hin und wieder auch "Whitecap" genannt, erkennbar an seiner weißen Kappe (die anderen Schiedsrichter haben schwarze Kappen). Er positioniert sich im Backfield der Offense und richtet über Downs und Strafen. Weitere Schiedsrichter sind der "Umpire", der sich jeweils zwischen oder hinter den Linebackers aufstellt und meistens den Ball sichert und für den nächsten Spielzug positioniert. An die Line of Scrimmage stellen sich jeweils der "Linesman" und "Line Judge". Ersterer ist für die Pflicht-Kette "(Line to Gain Indicator)" verantwortlich, letzterer für die Beobachtung der Vorwärtsbewegung ("Forward Progress") sowie eine optionale Kette. Für die weiten Pässe sind "Back Judge", "Field Judge" und "Side Judge" zuständig. Je nach Crewstärke sind unterschiedliche Schiedsrichter für die offizielle Spielzeit zuständig: In einer 3er Crew der Umpire, in einer 4er der Line Judge, in einer 5er und 7er Crew der Back Judge sowie in einer 6er der Side Judge. In den Profiligen ist diese Aufgabenverteilung mitunter abweichend.

Zur Ausrüstung der Schiedsrichter gehören unter anderem die gelben Flaggen ("Penalty Flags") zum Markieren eines Fouls und die weißen (in Profiligen blauen) "Beanbags" (Bohnensäcke) zum Markieren wichtiger Punkte.

Das Laufspiel umfasst die Spielzüge, die keinen Vorwärtspass beinhalten, der Ball also durch den Snap, den Hand-Off oder einen Lateral Pass zum neuen Ballträger gelangt.

Das Laufspiel wird taktisch in drei Konzepte unterteilt: "Power Running Game", "Quickness Running Game" und "Finesse Running Game". Die verschiedenen Konzepte können miteinander kombiniert werden. Das Power Running Game war vor allem in den Anfangsjahren des American Footballs das dominierende Konzept. Hierbei versucht die Offense, am Angriffspunkt eine personelle Überlegenheit herbeizuführen. Das Quickness Running Game basiert auf dem Versuch, den Angriffspunkt so schnell zu erreichen, dass der Defense keine Zeit für eine optimale Reaktion bleibt. Größerer Raumgewinn wird hierbei nicht angestrebt. Drei Yards Raumgewinn gelten bereits als voller Erfolg. Beim Finesse Running Game versucht die Offense, durch Täuschungsmanöver die Defense zu schwächen und die dabei entstehenden Lücken zu nutzen. 

Zusätzlich wird das Laufspiel nach der Art der Blocksetzung unterschieden. Hierbei unterscheidet man zwischen dem Man-Blocking, bei dem jeder Spieler außer dem Läufer einen oder mehrere Spieler zugeteilt bekommt, die er blocken soll, und dem Zone-Blocking, bei dem jeder Spieler eine Zone zugeteilt bekommt, in der er jeden Spieler der sich in der Zone befindet blockt.

Im Laufe der 1980er Jahre gewann das Laufspiel mit den „Zone Runs“ und den „Stretch Plays“ eine neue Dimension. Grund für diese Entwicklung war die zunehmende Popularität der "8-Man-Fronts", also das Aufstellen von acht Verteidigern nahe der Line of Scrimmage. Dadurch war die Defense zahlenmäßig überlegen, da durch das Ausfallen des Quarterbacks, des Ballträgers und der beiden äußeren Receiver nur sieben Blocker zur Verfügung standen. Vorrangiges Ziel dieser Varianten ist es, der Defense die herkömmlichen Reaktionsmöglichkeiten auf schnell durchschaubare Laufspiele zu nehmen. Bei Zone Runs und ähnlichen Spielzügen werden nicht direkt einzelne Gegenspieler angegriffen. Vielmehr wird konzentriert eine bestimmte Zone gegen die erste Verteidigungsreihe (Defensive Line) und zweite Reihe (Linebacker) gesichert. Hierzu macht die Offensive Line nach dem Snap beispielsweise zuerst einen Schritt zur Seite anstatt nach vorne, um eine Zone freizuschieben. Hinsichtlich des Gelingens eines solchen Spielzugs trägt der balltragende Runningback mehr Verantwortung als bei einem Standardlaufspiel. Denn anstatt dass sich durch die Bemühungen der Offensive Line möglichst eine durch den Spielzug geplante Gasse öffnet, bieten sich je nach Reaktion und Stärke der Verteidigung meist mehrere Möglichkeiten, die Verteidigungslinie zu durchlaufen. Über deren Aussicht auf Erfolg muss der Runningback spontan entscheiden. Unterschieden wird in "Inside" und "Outside Zone Plays", welche sich im Anlaufwinkel des Runningbacks zur Line of Scrimmage unterscheiden. Beim Inside Zone Play bleibt der Runningback zwischen den beiden Tackles, was ihm erlaubt auch die Seite zu wechseln, falls sich dort eine Lücke öffnet. Beim Outside Zone Play visiert er einen Punkt außerhalb der Tackles an. Bei diesem Winkel ist es schwieriger, die Angriffsseite zu wechseln, erlaubt aber dem Runningback, außerhalb der Formation anzugreifen.

Das Passspiel wird in drei Kategorien aufgeteilt: "Drop Back Pass", " Roll-" oder "Sprint Out Pass" und "Play Action Pass". Unterscheidungskriterium ist dabei die Bewegung des Quarterbacks.

Beim Drop Back Pass bewegt sich der Quarterback nach dem Snap gerade nach hinten, während die Offensive Line um ihn herum einen Halbkreis bildet ("Pocket"). Aus dieser Position kann er üblicherweise das gesamte Spielfeld überblicken. Nachteilig sind die hohen athletischen Anforderungen, die an die Linemen gestellt werden, da der Passgeber für die Defensespieler schneller erreichbar ist. Der Drop Back Pass wird nochmals nach der Länge des Drop Backs eingeteilt, welcher auf die Länge der Passrouten abgestimmt ist. Hierbei wird zumeist der 3 Step Drop Back für schnelle Spielzüge, der 5 Step Drop Back für mittlere Spielzüge und der 7 Step Drop Back für lange Spielzüge genutzt. Eine Sonderstellung hat dabei die Shotgun, bei der sich der Quarterback bereits in seiner endgültigen Stellung aufstellt. Da diese der Offense das Überraschungsmoment nimmt, wird sie meist nur eingesetzt, wenn ersichtlich ist, das ein Passspielzug folgt.

Beim Roll Out Pass und Sprint Out Pass läuft der Quarterback nach dem Snap in Richtung eines Spielfeldrandes. Geschieht dies ohne jede Verzögerung spricht man vom Sprint Out, vollführt er zuvor andere Bewegungsabläufe, so spricht man vom Roll Out. Bei dieser Form des Passspiels wird die der Laufrichtung entgegengesetzte Seite der Offensive Line entlastet, da die Abwehrspieler auch nach Überwindung des Linemen eine erheblich größere Strecke zurücklegen müssen. Der Quarterback sollte hierbei jedoch ein guter Sprinter sein. Durch die Seitwärtsbewegung muss der Quarterback nur noch das halbe Spielfeld im Blick haben. Dies vereinfacht zwar die Beobachtung der Verteidigung, verringert aber die Anzahl der anspielbaren Passempfänger, da es ein sehr hohes Risiko bergen würde einen Pass auf die andere Spielfeldseite gegen die Laufrichtung zu werfen.

Beim Play Action Pass täuscht der Quarterback vor dem Pass eine Ballübergabe an einen Läufer an. Dies soll die Verteidiger zu einer verzögerten Reaktion auf den Pass bewegen.

Viele Spielzüge sind darauf ausgelegt, die Verteidigung zu verwirren. Bei Fakes wird ein Spielzugart (z. B. ein Kickspielzug) angetäuscht und dann eine andere Spielzugart ausgeführt (z. B. ein Passspielzug). Fakes machen einen nicht unbedeutenden Teil der Taktiklastigkeit des Spieles aus.

Da die Defense flexibel auf die Spielzüge der Offense reagieren muss, gibt es außer den Grundaufstellungen und den zu verteidigenden Zonen oder Gegenspielern kaum festgelegte Spielzüge. Einige Ausnahmen:

Dabei versucht die Defense Druck auf den Quarterback auszuüben, indem ein oder mehrere Spieler die Offense-Line durchbrechen bzw. umgehen. Der blitzende Spieler kann ein Linebacker oder ein Cornerback sein, manchmal sogar ein Safety. Wie bei den Offense-Spielzügen hängt der Erfolg eines Blitzes neben der Athletik und Schnelligkeit der Spieler vor allem vom Überraschungsmoment ab. Erkennt der Quarterback, woher der Blitz kommt, hat er eine geschwächte Stelle der Verteidigung vor sich. Zum Teil werden Blitzes auch nur angetäuscht, um den Quarterback zu verunsichern oder ihn zu einer schlechten Entscheidung zu verleiten.

Der "Defensive Stunt" ist eine weitere Variante, die Offense unter Druck zu setzen. Die D-Line-Spieler und Linebacker stellen sich in die für ihre Formation gewöhnlichen Positionen auf, tauschen aber ihre Assignments (Aufgaben) nach dem Snap mit dem Nebenmann oder mit einem vorher abgesprochenen Partner. So greift z. B. ein Defense End in der Mitte an und der Defense Tackle übernimmt die Außenseite. Das soll Abstimmungsschwierigkeiten innerhalb der gegnerischen Offensive Line hervorrufen.

Ein ähnliches Ziel verfolgen die "D-Line Shifts". Auch hierbei stellen sich die Defensive Linemen zum Beispiel in einem Gap auf, wechseln aber kurz vor dem Snap die Position (zum Beispiel Head-on zum O-Line Spieler). Das kann mehrere Wirkungen haben. Erstens kann es die Offensive Line durcheinander bringen, weil die geplanten Blockschemata evtl. nun nicht mehr passen und es zu spät ist, um sich neu abzusprechen. Zweitens zwingt es den Quarterback zu möglichen "Audibles", die den Spielzug ändern, wenn er sieht, dass z. B. die D-Line stark auf die Seite des geplanten Spielzuges shiftet und ihn für die Offense unmöglich macht. Das wiederum verrät der Defense etwas über den geplanten Spielzug.

Die Rückennummern haben üblicherweise eine feste Zuteilung zu den Positionen, nicht zuletzt zur Orientierung der Schiedsrichter. Zwar ist diese Zuordnung nach den Regeln der NCAA nicht zwingend vorgeschrieben, allerdings wird nachdrücklich empfohlen die Nummern nach dem auch in der NFL üblichen Schema zu vergeben. Für die Offensive Line ist bei mindestens fünf Spielern die Nummerierung mit Nummern zwischen 50 und 79 während normaler Spielzüge allerdings vorgegeben, da sie beispielsweise keine Bälle fangen und den Ball auch sonst nur als freien Ball (z. B. Fumble) berühren bzw. während Passspielzügen vor dem Werfen des Balls nicht nach vorne laufen dürfen. Meist werden die Nummern nach folgendem Schema vergeben:


Des Weiteren gibt es in vielen Teams so genannte "retired numbers" (zurückgezogene Nummern). Die Nummern gehörten meist früher besonders großartigen Spielern, werden mit diesen assoziiert und zum Andenken an diese Spieler bzw. als Ehrung nicht mehr vergeben.

Der Weltverband International Federation of American Football (IFAF) organisiert unter anderem die American-Football-Weltmeisterschaften und sorgt dafür, dass American Football bei Veranstaltungen wie den World Games 2005 vertreten ist. Seit 1999 werden alle vier Jahre Weltmeisterschaften ausgetragen. Zweimal (1999 in Italien und 2003 in Deutschland) gewann Japan und dreimal (2007 in Japan, 2011 in Österreich und 2015 in den USA) die Vereinigten Staaten, die mit einem Team aus College-Spielern antraten.

American Football ist vor allem in Nordamerika verbreitet. In den USA gilt es seit den 1970er Jahren als die populärste Sportart überhaupt (vorher dominierte Baseball). Praktisch jede High School und jedes College besitzt ein Team auf unterschiedlich hohem Leistungsniveau, die in verschiedenen Ligen innerhalb der NCAA bzw. NAIA organisiert sind. Die mehreren hundert Mannschaften in den oberen Ligen des College Football spielen jeden Herbst etwa zwölf Spiele innerhalb ihrer jeweiligen Gruppe.

Die bekannteste nordamerikanische Profiliga ist die "National Football League" (NFL), die seit 1920 existiert, mithin erst Jahrzehnte nach dem College Football entstand. Das Finale der NFL, der so genannte Super Bowl, ist das wichtigste Fernseh-Event der USA und gilt als weltweit populärste jährliche Einzel-Sport-Veranstaltung. Daneben gab und gibt es immer wieder Konkurrenzligen, etwa die USFL oder die XFL.

Im Gegensatz zu den USA, wo Football traditionell von September bis über den Jahreswechsel hinaus gespielt wird (College-Bowl-Spiele Anfang Januar, Super Bowl am ersten Februarsonntag), wird in Europa im Sommerhalbjahr von Frühling bis Herbst gespielt.

Der American Football in Deutschland begann Mitte der 1970er Jahre mit den Frankfurter Löwen (Gründungsjahr 1977) und den Düsseldorf Panthers, wobei meist in Deutschland stationierte US-Soldaten als Spieler und Trainer mitwirkten. Seit 1975 gibt es American Football in Österreich, einige der Vorreiter waren Richard Plenk in Wien und Stefan Herdey in Graz. Seit den 1980ern wird auch American Football in der Schweiz gespielt.

Die höchste reguläre Liga in Deutschland ist die "German Football League" (GFL), die in eine Nord- und eine Südgruppe mit je 8 Teams zweigeteilt ist (Stand 2017). Das Finale der GFL ist der German Bowl. Unterhalb der GFL befindet sich eine ebenso zweigeteilte 2. Bundesliga mit 16 Mannschaften. Es folgen diverse weitere Ligen (Regionalligen, Oberligen, Verbandsligen, Landesligen, Aufbauligen). Ebenso gibt es umfangreichen Spielbetrieb im Jugendbereich.

Seit Ende der 1980er Jahre wird Football auch auf Hochschulebene gespielt. Das Finale ist der jährlich ausgetragene Hochschulbowl. Kurz darauf treffen sich die besten Hochschulspieler in der universitären Nationalmannschaft, den GERmaniacs.

Die höchste reguläre Liga in Österreich ist die "Austrian Football League" (AFL). Das Finale der AFL ist der Austrian Bowl.

Seit den 1980ern werden Europameisterschaften unter Nationalmannschaften sowie auf Vereinsebene der "Eurobowl", der Euro-Cup und der Federations-Cup, die durch den EFAF-Cup abgelöst wurden, durch die European Federation of American Football (EFAF) ausgetragen. Zur Saison 2014 wurden die europäischen Wettbewerbe neu organisiert, sodass sich seitdem die sechs besten Teams Europas in der Big6 European Football League treffen. Als weiterer Wettbewerb wurde der EFL Bowl eingeführt, in dem die besten nicht in der Big6 vertretenen Vereinsmannschaften aufeinander treffen.

Ab 1991 organisierte die NFL die World League of American Football im Frühjahr. Nach dem Rückzug aller nichteuropäischen Mannschaften folgte 1998 die Umbenennung in NFL Europe. Diese wurde 2007 eingestellt. Seit 2007 versucht die NFL, durch die "NFL International Series" American Football in Europa populärer zu machen.

Alle vier Jahre gibt es eine American-Football-Europameisterschaft, welche zuletzt 2014 in Österreich stattfand. Amtierender Europameister ist Deutschland, das sich 2014 den dritten Titel nach 2010 und 2001 sicherte. Rekordsieger ist Finnland mit fünf Titeln, die mit Ausnahme des ersten (1985) sowie des bislang letzten (2000) alle in den 1990er Jahren gewonnen wurden, als die Europameisterschaft noch unregelmäßig in Zweijahres-Intervallen abgehalten wurde.

American Football ist in der Volksrepublik China nicht populär. Erst 2012 wurde mit der American Football League of China die erste Amateurliga gegründet, von der sich 2015 die China Bowl Alliance abspaltete. Seit 2016 existiert mit der China Arena Football League (CAFL) eine professionelle Arena-Football-Liga. 2016 wurde die U-19 American-Football-Weltmeisterschaft in Harbin ausgetragen. Es war das erste internationale Footballturnier in China. Laut CAFL und National Football League (NFL) wuchs die Anzahl an Chinesen im Alter von 15-54, die sich selbst als Fans des American Footballs bezeichnen von 1,6 Millionen im Jahr 2010 auf 14,1 Millionen im Jahr 2013. Probleme bilden die hohe Luftverschmutzung in der Nähe von Metropolregionen, welche ein Spiel im Freien gesundheitlich risikoreich machen.

Wissenschaftliche Untersuchungen haben einen Zusammenhang zwischen den immer wieder sehr harten Kopfstößen im American Football und Krankheiten wie Alzheimer, Depressionen und Demenz gefunden, die durch Gehirnerschütterungen und zahlreiche Hirntraumata bedingt sein sollen. Diese Krankheiten sind oft Spätfolgen und treten erst zehn bis 20 Jahre nach Karriereende auf. Aus einer 2007 veröffentlichten Studie an 2.552 ehemaligen NFL-Spielern, die am Center for the Study of Retired Athletes an der University of North Carolina durchgeführt wurde, ergab sich ein sehr starker Zusammenhang zwischen der Anzahl der Gehirnerschütterungen und der Rate diagnostizierter Depressionen.
Es zeigte sich, dass von 595 ehemaligen NFL-Spielern, die drei oder mehr Gehirnerschütterungen während ihrer aktiven Laufbahn hatten, 20,2 % an Depression litten. Darüber hinaus wurde bei den 2.552 Untersuchten ein 37 % höheres Risiko festgestellt, an Alzheimer zu erkranken, als bei anderen Männern gleichen Alters.

Wissenschaftler am Center for the Study of Traumatic Encephalopathy (CSTE) der Boston University haben unter Federführung von Ann McKee und Robert Cantu bis zum Jahr 2012 durch post mortem-Autopsien der Gehirne eine chronisch traumatische Enzephalopathie (CTE) bei 68 von 85 untersuchten ehemaligen Sportlern festgestellt, darunter bei 34 von 35 untersuchten NFL-Profis. Die Sportler hatten sich wiederholt ein Schädel-Hirn-Trauma in Form von Gehirnerschütterungen zugezogen. Gehirnforscher an der University of California, Los Angeles haben im Jahr 2012 auch bei noch lebenden Ex-Profis CTE mittels PET-Hirnkartierung nachgewiesen.





</doc>
<doc id="7839" url="https://de.wikipedia.org/wiki?curid=7839" title="Genom">
Genom

Das Genom, auch Erbgut eines Lebewesens oder eines Virus, ist die Gesamtheit der materiellen Träger der vererbbaren Informationen einer Zelle oder eines Viruspartikels: Chromosomen, Desoxyribonukleinsäure (DNA) oder Ribonukleinsäure (RNA) bei RNA-Viren, bei denen RNA anstelle von DNA als Informationsträger dient. Im abstrakten Sinn versteht man darunter auch die Gesamtheit der vererbbaren Informationen (Gene) eines Individuums.

Die Bezeichnung "Genom" wurde, nach der durch Thomas Hunt Morgan gelungenen Verknüpfung der Chromosomentheorie der Vererbung mit der durch Wilhelm Johannsen aufgestellten Hypothese von Genen als Erbeinheiten, 1920 von Hans Winkler geprägt. Das Teilgebiet der Genetik, das sich mit der Erforschung des Aufbaus von Genomen und der Wechselwirkungen zwischen Genen befasst, wird als "Genomik" () bezeichnet.

Die für die Vererbung von Eigenschaften und Merkmalen erforderliche und auf der Ebene der Zellen und der Individuen weitergegebene Information ist in der DNA enthalten, und zwar in der Sequenz (Abfolge) der DNA-Basen Adenin (A), Guanin (G), Cytosin (C) und Thymin (T). Ribonukleinsäuren verwenden an Stelle des Thymins die Base Uracil (U). Jeweils drei aufeinanderfolgende Basen bedeuten nach der Regel des genetischen Codes eine Aminosäure.

Man unterscheidet codierende und nichtcodierende Abschnitte der DNA. Nach Maßgabe der Basensequenz der codierenden Abschnitte (Gene) werden im Zuge der Genexpression aus Aminosäuren Proteine gebildet. Aber auch nichtcodierende Bereiche können wichtige Funktionen aufweisen, so etwa bei der Genregulation. Außerdem gibt es die sogenannten Pseudogene: durch Mutationen funktionslos gewordene und vom Organismus nicht mehr abgelesene Gene.

Die meisten Organismen besitzen neben der chromosomalen DNA des Zellkerns (deswegen auch "Karyom" genannt) weiteres genetisches Material in anderen Zellteilen. Bei diesen Eukaryoten (Tiere, Pflanzen, Pilze und Protisten) haben die Mitochondrien, bei Pflanzen und Algen des Weiteren die Plastiden, eigene kleine Genome. Prokaryoten (Bakterien und Archaeen) enthalten vielfach zusätzliche, relativ kurze, in sich geschlossene DNA-Moleküle, die als Plasmide bezeichnet werden.

Bei den Eukaryoten besteht das Kern-Genom (Karyom) aus mehreren bis zahlreichen strangförmigen Chromosomen. Die Anzahl der Chromosomen ist artspezifisch verschieden und kann zwischen zwei (beim Pferdespulwurm) und mehreren hundert (bei manchen Farnen) variieren. Außerdem ändert sich die Chromosomenzahl beim Wechsel der Kernphase (Meiose und Karyogamie). Charakteristisch für eukaryotische Genome ist weiterhin ein hoher Anteil an nichtcodierender DNA (beim Menschen etwa 95 %) und die Intron-Exon-Struktur der Gene.

Bei den Prokaryoten liegt die DNA als langes, in sich geschlossenes Molekül vor. Daneben können kürzere, ebenfalls in sich geschlossene DNA-Moleküle, sogenannte Plasmide, in variabler Anzahl vorhanden sein. Diese können unabhängig von der Haupt-DNA vervielfältigt und an andere Prokaryotenzellen weitergegeben werden (Konjugation), auch über Artgrenzen hinweg. Sie enthalten in der Regel nur wenige Gene, die zum Beispiel Resistenzen gegen Antibiotika vermitteln.

Prokaryotische Genome sind im Allgemeinen wesentlich kleiner als eukaryotische. Sie enthalten relativ geringe nichtcodierende Anteile (5-20 %) und auch nur wenige oder gar keine Introns.

Die Genome der Mitochondrien und Plastiden sind wie prokaryotische Genome organisiert (vgl. Endosymbiontentheorie). Sie enthalten jedoch nur einen geringen Teil der für die Funktion dieser Organellen benötigten Gene, weshalb diese Organellen als „semi-autonom“ bezeichnet werden.

Virale Genome sind sehr klein, da in ihnen nur recht wenige Proteine codiert sind und die genetische Information zudem hochgradig verdichtet ist, indem etwa verschiedene Gene überlappen oder manche Abschnitte zugleich in beiden Leserichtungen als Gene fungieren können. Sie können aus DNA oder RNA bestehen, und diese können einzel- oder doppelsträngig sowie linear, zirkulär oder segmentiert vorliegen. Eine Besonderheit stellen die Retroviren dar, deren RNA-Genom mittels reverser Transkription in DNA „übersetzt“ und in das Wirtsgenom integriert werden kann. Die Eigenschaften der Genome der Viren sind wichtige Kriterien bei deren Einteilung (Virus-Taxonomie).

Als Genomgröße wird die in einem Genom vorhandene Menge an DNA bezeichnet. Bei Eukaryoten bezieht sich diese Angabe gewöhnlich auf den haploiden Chromosomensatz, dies wird auch als C-Wert bezeichnet. Es wird entweder die Anzahl der vorhandenen Basenpaare (bp) oder die Masse der DNA in der Einheit pg (Pikogramm) angegeben. 1 pg doppelsträngiger DNA besteht aus etwa 0,978·10 bp, also aus knapp einer Milliarde Basenpaaren. Üblich sind auch die Bezeichnungen Kilo-Basenpaar (kbp oder kb) für 1.000 Basenpaare und Mega-Basenpaar (Mbp oder Mb) für eine Million Basenpaare.

Nach neueren Untersuchungen besitzt der Südamerikanische Lungenfisch ("Lepidosiren paradoxa") mit 80 pg (7,84 × 10 bp) das größte bisher bekannte tierische Genom. Ältere, aber wohl ungenauere Untersuchungen zeigen mit etwa 133 pg noch größere Genome, die ebenfalls bei Lungenfischen, allerdings bei der afrikanischen Art Äthiopischer Lungenfisch ("Protopterus aethiopicus") gefunden wurden. Mit 0,04 pg (weniger als 50 Millionen Basenpaare) besitzt das zum primitiven Tierstamm Placozoa gehörende, auf Algen lebende, etwa 2 mm große, wenig differenzierte "Trichoplax adhaerens" das kleinste bisher bekannte "tierische" Genom. Die Zahl der Basenpaare des Darmbakteriums "Escherichia coli" ist nur um einen Faktor 10 kleiner. Das kleinste bisher quantifizierte "bakterielle" Genom besitzt der Blattfloh-Endosymbiont "Carsonella ruddii": Sein zirkuläres DNA-Molekül enthält nur knapp 160.000 Basenpaare, in denen sämtliche Informationen gespeichert sind, die er zum Leben braucht.

Die DNA einer einzelnen menschlichen Zelle ist aneinandergereiht etwa 1,80 m lang.
Ein Basenpaar auf einem DNA-Strang hat theoretisch einen Informationsgehalt von 2 bit, da es 2 = 4 Zustände (A/T/G/C) annehmen kann. Mit etwa 3,27 Milliarden Basenpaaren hätte das Genom des Menschen demnach einen maximal möglichen Informationsgehalt von 6,54 Milliarden bit oder 780 MiB. Der tatsächliche Informationsgehalt liegt vermutlich deutlich darunter, da große Teile der DNA nichtcodierende Sequenzen aufweisen, die allerdings zumindest teilweise regulatorische Funktionen haben.

Ein Vergleich der Genomgröße mit der Komplexität und dem Organisationsgrad des Organismus ergibt keinen klaren Zusammenhang. So haben Schwanzlurche größere Genome als Reptilien, Vögel und Säugetiere. Lungenfische und Knorpelfische haben größere Genome als Echte Knochenfische, und innerhalb von Taxa wie den Blütenpflanzen oder Protozoen variiert die Genomgröße in hohem Maß. Dies wird als „"C-Wert-Paradoxon"“ bezeichnet. Die größte DNA-Menge weisen einfache Eukaryoten wie einige Amöben sowie die Urfarne mit rund einer Billion Basenpaaren auf. Diese Arten enthalten einzelne Gene als tausendfache Kopien und lange nicht proteincodierende Abschnitte.

Die DNA von Genomen verschiedener Organismen, die entweder für die medizinisch-pharmazeutische oder anwendungsorientierte Forschung oder auch für die Grundlagenforschung relevant sind, wurde annähernd vollständig „sequenziert“ (man spricht auch fälschlicherweise vom „Entschlüsseln“), das heißt, ihre Basensequenz wurde ermittelt (per DNA-Sequenzierung, teilweise nach einer Genomamplifikation). Die Basensequenzen werden über das Internet u. a. vom NCBI bereitgestellt.






</doc>
<doc id="7841" url="https://de.wikipedia.org/wiki?curid=7841" title="Metamorphose (Geologie)">
Metamorphose (Geologie)

Die Gesteinsmetamorphose (gr. "metamórphosis" „Verwandlung“, „Umgestaltung“) ist die Umwandlung der mineralogischen Zusammensetzung eines Gesteins durch geänderte Temperatur- und/oder Druckbedingungen. Dabei entsteht aus dem Ausgangsgestein, das auch als Protolith oder "Edukt" bezeichnet wird, ein metamorphes Gestein ("Metamorphit").

Bei der Metamorphose kommt es unter den veränderten physikalischen Bedingungen zu Mineralreaktionen, also zur Neu- oder Umbildung von Mineralen, wobei das Gestein im festen Zustand verbleibt. Schmilzt ein Gestein durch eine Erhöhung der Temperatur auf, so spricht man von Anatexis.

Die Bezeichnung stammt von Charles Lyell, die Idee vertrat aber schon James Hutton im 18. Jahrhundert (und einige andere).

Eine Abgrenzung der Gesteinsmetamorphose von der Diagenese, den Prozessen, die zur Bildung von Sedimentgesteinen aus Sedimenten führen, kann nicht exakt gezogen werden, da es auch bei der Diagenese zu Mineralneu- und umbildungen kommen kann. Es gibt verschiedene Definitionen, nach denen von einer Metamorphose zu sprechen ist, wenn bestimmte Minerale auftreten oder nicht mehr vorhanden sind bzw. bestimmte Druck- und Temperaturgrenzen überschritten wurden.

Bei der Metamorphose bleibt die chemische Zusammensetzung des Gesteins oft unverändert, man spricht dann von isochemischer Metamorphose. Da an einer Metamorphose immer auch fluide Phasen beteiligt sein können, ist diese Bedingung selten streng erfüllt. Wenn der Elementbestand eines Gesteins wesentlich verändert wird, liegt eine Metasomatose vor. Dies trifft nicht zu, wenn nur HO oder CO zu- oder abgeführt werden.

Die Metamorphose eines Gesteins wird durch Druck und Temperatur beeinflusst. Man spricht von einer prograden Metamorphose, wenn Druck und Temperatur während der Metamorphose zunehmen, und von einer "retrograden Metamorphose" oder Diaphthorese, wenn Druck und Temperatur während der Metamorphose abnehmen.

Prinzipiell beobachtet man zwei Arten der Umwandlung von Gesteinen:

Durch "Phasenumwandlungen" (Mineralreaktionen) entstehen neue Minerale aus den vorhandenen. Minerale können nur unter bestimmten Druck- und Temperaturbedingungen miteinander existieren. Sind diese Bedingungen nicht mehr erfüllt, können die Minerale miteinander zu anderen Mineralen reagieren. Solche Mineralreaktionen sind oft sehr komplex. Manche der neu gebildeten Minerale setzen bei diesen Reaktionen andere Stoffe wie z. B. Wasser frei oder nehmen sie auf, dadurch kommt es zu dem oben angesprochenen Phänomen der Metasomatose.

Bei der Kristallisation von Mineralen kommt es zu "Gefügeumwandlungen" im Gestein. Durch die Einregelung, durch Drucklösungsprozesse nach dem Rieckeschen Prinzip oder das orientierte Wachstum von Mineralen bildet sich im Gestein eine Schieferung aus, die umso ausgeprägter ist, je mehr Schichtsilikate (Glimmer) im Gestein vorhanden sind.

Der mögliche Verlauf einer Gesteinsmetamorphose ist abhängig von den dabei durchlaufenen Druck- und Temperaturbedingungen. Diese können sehr verschieden sein und so unterschiedliche Metamorphosetypen hervorrufen. Gesteine, die einen bestimmten Metamorphosetyp durchlaufen haben, tragen häufig charakteristische Merkmale, zum Beispiel bestimmte Mineralparagenesen, Gefügemerkmale u. a. davon.

Der Zusatz „Regional“ besagt, dass diese Art der Metamorphose über große Volumina (z. T. über mehrere 1000 km³) stattfindet, meistens hervorgerufen durch tektonische Senkung großer Teile der Erdkruste. Hierbei geraten Gesteine durch Versenkung etwa durch Faltung oder Subduktion an Kontinentalrändern unter hohen Druck- und/oder Temperatur, die die Umwandlung der Minerale gleichermaßen bestimmen. Typische Gesteine sind zum Beispiel Glimmerschiefer, Gneise, Amphibolite.

Die druckbetonte Metamorphose ist ein typisches Kennzeichen von Subduktionszonen. Hierbei wird verhältnismäßig kaltes Material ozeanischer Kruste versenkt. Die dabei ablaufende Metamorphose wird daher von vergleichsweise niedrigen Temperaturen und hohen Drücken bestimmt. Gesteine, die die druckbetonte Metamorphose durchlaufen haben, sind durch typische Minerale gekennzeichnet, wie Glaukophan in Blauschiefern oder Omphazit in Eklogiten.

Die Kontaktmetamorphose ist die temperaturbetonte Metamorphose. Kontaktmetamorphe Gesteine finden sich vor allem im Umfeld magmatischer Intrusionen. Das heiße Magma heizt das umgebende Gestein auf und führt so dessen Metamorphose herbei. Der Bereich der Metamorphose heißt Kontakthof. Ein typisches Merkmal kontaktmetamorpher Gesteine sind die durch Mineralreaktionen hervorgerufene Knotenbildung sowie häufig das Fehlen einer Schieferung. Bei der Kontaktmetamorphose können Hornfelse, Frucht- und Knotenschiefer entstehen.

Diese sehr extreme Art der Metamorphose wird durch heftige Stoßwellen hervorgerufen und kann zur Zertrümmerung ganzer Gesteinspartien und zur Zerstörung von Kristallgittern führen. Sie ist auf Meteoritenkrater (und auf die Orte unterirdischer Atombombenversuche) beschränkt. Im Bereich des Einschlagkraters werden hohe Temperaturen und Drücke erzeugt, wobei Gesteine aufgeschmolzen und herausgeschleudert werden und dann zu kugeligen Glasaggregaten erstarren (Tektite). Typische Kennzeichen für die Impaktmetamorphose ist das Auftreten von Hochdruckmineralen wie zum Beispiel Coesit oder, bedingt durch den Kollaps von Kristallgittern, von diaplektischem Glas. Die Impaktmetamorphose führt zur Zertrümmerung von Gesteinskörpern, die makroskopisch sichtbar ist (z. B. im Suevit des Nördlinger Rieses).

Die Dislokations-Metamorphose wird auch Dynamometamorphose genannt. In aktiven Störungszonen wird das Gestein durch die Bewegung zweier Blöcke gegeneinander stark verändert. Reagiert das Gestein dabei auf mechanische Beanspruchung spröde, das heißt, es zerbricht und wird zermahlen, so entstehen dabei Kataklasite. Wenn das Gestein duktil auf mechanische Beanspruchung reagiert, entstehen durch Neukristallisation Mylonite mit charakteristischem, durch die stete Bewegung geprägtem Gefüge.

Bei Erdbeben tritt eine kurzzeitige und plötzliche Bewegung von Gesteinspartien auf. Öffnen sich dabei Hohlräume, können darin durch die plötzliche Druckentlastung Implosionsbrekzien entstehen, die den Hohlraum wieder auffüllen. Durch die bei einer plötzlichen Bewegung an der Bewegungsfläche entstehende Reibungswärme kann dies zu kurzzeitigem Aufschmelzen von Gesteinspartien und zur Bildung von Pseudotachyliten führen. Ebenso können durch den Kollaps von Kristallgittern diaplektische Gläser entstehen.

Es gibt unterschiedliche Systeme zur Beschreibung des Metamorphosegrades, den ein Gestein erreicht hat. In Analogie zur Fazies von Sedimenten können metamorphe Bedingungen (Druck, Temperatur) durch metamorphe Faziesgruppen zusammengefasst werden.
Eine andere Möglichkeit ist die Bestimmung des Metamorphosegrades anhand bestimmter Mineralreaktionen. Der Umstand, dass bestimmte Minerale aus anderen entstanden sind zeigt dabei, dass bestimmte Grenztemperaturen bzw. Grenzdrücke überschritten wurden.

Gegenüber der Diagenese kann die Metamorphose nicht eindeutig abgegrenzt werden, da bei der diagenetischen Umwandlung eines Sedimentes in ein Gestein ähnliche Prozesse ablaufen. Häufig wird eine willkürliche Abgrenzung vorgenommen, wenn bestimmte Druck- und Temperaturverhältnisse überschritten wurden. Der Grenzbereich zur Diagenese wird oft als Anchimetamorphose bezeichnet.

Die Anatexis, die zur partiellen oder vollständigen Aufschmelzung von Gesteinen führt, ist ebenfalls ein Vorgang im Grenzbereich der Gesteinsmetamorphose. Metamorphose findet immer im festen Zustand statt, während bei der Anatexis Schmelzen gebildet werden.

Bei der Metasomatose wird die allgemeine chemische Zusammensetzung des betreffenden Gesteins (Gesteinschemismus) durch Stoffaustausch verändert, während die eigentliche Metamorphose isochemisch ist, d. h., die allgemeine chemische Zusammensetzung des Gesteins ändert sich nicht.





</doc>
<doc id="7842" url="https://de.wikipedia.org/wiki?curid=7842" title="Kuba">
Kuba

Kuba (spanisch "Cuba" , amtliche Bezeichnung República de Cuba) ist ein Inselstaat in der Karibik. Er grenzt im Nordwesten an den Golf von Mexiko, im Nordosten an den Atlantischen Ozean und im Süden an das Karibische Meer. Hauptstadt des Landes ist Havanna, die größte Metropole der Karibik.

Kolumbus nannte die Insel bei ihrer Entdeckung zunächst "Juana" nach dem Prinzen Don Juan. 1515 ordnete dessen Vater Fernando II., König von Spanien, die Umbenennung nach "Fernandina" an. Nach ihm war bisher nur eine Insel der Bahamas (heute: Long Island) benannt.

Der Name „Cuba“ stammt wahrscheinlich aus der Sprache der Kariben oder der Taíno. Die Wörter „coa“ (Ort) und „bana“ (große) bedeuten so viel wie „großer Platz“. Kolumbus schrieb, er sei an einem Ort gelandet, den die indigenen Einheimischen „Cubao“, „Cuban“ oder „Cibao“ nannten. Diese bezogen sich offensichtlich auf eine Bergregion in der Nähe des Landungsortes im Osten Kubas.

Der kubanische Schriftsteller und Etymologe José Juan Arrom beschrieb 1964 folgende Wortherkunft: Demnach existiert in der Sprache der Arawak der Begriff „kuba-annakan“ bzw. „cubanacán“, was so viel wie „Land oder Provinz in der Mitte“ bedeutet. Damit sei quasi als gesichert anzunehmen, dass „Cuba“ so viel wie „Land“ oder „Provinz“ in der Sprache der Einheimischen hieß.

Der Archipel gehört zu den Großen Antillen. Es besteht neben der gleichnamigen Hauptinsel Kuba, der größten der Karibik, aus der Isla de la Juventud (früher "Isla de Pinos") und rund 4195 kleineren und kleinsten Inseln mit einer Gesamtfläche von 110.860 km².

Die maximale Ausdehnung der Hauptinsel beträgt von West "(Cabo San Antonio)" nach Ost "(Punta Maisí)" 1250 Kilometer. Die schmalste Nord-Süd-Ausdehnung beträgt 31 Kilometer. Der Abstand zum amerikanischen Festland beträgt 154 Kilometer nach Key West (USA) und 210 Kilometer nach Yucatán (Mexiko). Da die Umrisse entfernt an ein Krokodil erinnern, wird Kuba auch gern als der „grüne Kaiman“ (spanisch: "caimán verde") bezeichnet.

Die kubanischen Feuchtwälder sind eine Ökoregion tropischer Regenwälder auf Kuba und der Isla de la Juventud.

Der höchste Punkt ist der Pico Turquino (1974 m ü. NN) in der Sierra Maestra.

Die Hauptstadt Havanna ist mit circa zwei Millionen Einwohnern die größte Stadt Kubas, gefolgt von Santiago de Cuba, Camagüey und Holguín.

Im Südosten der Insel, an der Guantánamo-Bucht, befindet sich die "Guantanamo Bay Naval Base", ein Marinestützpunkt der US-Marine. Rechtsgrundlage ist ein Vertrag von 1934, dessen Gültigkeit zwischen Kuba und den USA strittig ist.
Die Zeitzone Kubas ist UTC−5, während der Sommerzeit UTC−4.

Das Klima ist tropisch und wird vom Nordostpassat geprägt. Es gibt eine trockenere Jahreszeit von November bis April und eine regnerische Jahreszeit von Mai bis Oktober.

Kuba liegt im Einzugsgebiet von tropischen Wirbelstürmen, die sich jährlich von Juni bis November über dem Atlantik und in der Karibik bilden. Nicht selten trifft dabei ein schwerer Hurrikan kubanisches Festland und richtet schwere Verwüstungen an, welche das wirtschaftlich schwache Kuba besonders hart treffen. Insbesondere die meist in Leichtbauweise errichteten Privathäuser sind den starken Winden schutzlos ausgeliefert. Jedoch besitzt Kuba einen sehr gut funktionierenden Katastrophenschutz, so dass es, im Gegensatz zu den Nachbarinseln, selten zu einer größeren Anzahl von Todesfällen kommt.

Die Hurrikansaison 2008 mit drei schweren Hurrikanen, die Kuba trafen, – Gustav, Ike und Paloma – war eine der schlimmsten Naturkatastrophen in den letzten 50 Jahren. Es wurden hunderttausende Wohnungen zerstört, die Infrastruktur stark beschädigt und große Teile der Ernten vernichtet. Die Gesamtschäden werden auf um die zehn Milliarden US-Dollar geschätzt, rund zwanzig Prozent des kubanischen Bruttoinlandsproduktes von 2007. Sieben Menschen kamen ums Leben.

Kuba ist seit der Verwaltungsreform von 1976 und ihrer im Januar 2011 in Kraft getretenen Novellierung in 15 Provinzen und das Sonderverwaltungsgebiet Isla de la Juventud unterteilt:


Diese Provinzen sind, mit Ausnahme des "Municipio especial Isla de la Juventud", wiederum in insgesamt 168 Municipios untergliedert, die in etwa einem Landkreis in Deutschland entsprechen. Meist sind sie nach der Stadt benannt, in der sich der Verwaltungssitz des Municipio befindet.

Vor der Reform der Verwaltungsgliederung von 1976 gab es in Kuba sechs Provinzen: "Pinar del Río", "Havanna" und "Matanzas" im Westen sowie "Las Villas", "Camagüey" und "Oriente" in Zentral- und Ostkuba. Die danach entstandene Provinz La Habana wurde 2011 in die neuen Provinzen "Artemisa" und "Mayabeque" aufgespalten. Die Neueinteilung der Provinzen war Teil einer Verwaltungsreform, welche auch eine klarere Arbeitsteilung der "Poder Popular" und eine Erweiterung der Kompetenzen der einzelnen Provinzen vorsieht. Außerdem sollte durch die Schaffung neuer regionaler Zentren das Zugehörigkeitsgefühl der dort lebenden Kubaner gestärkt und die Qualität der staatlichen Dienste effizienter gestaltet werden.

Der mit Abstand größte Ballungsraum in Kuba und der ganzen Karibik ist Havanna mit einer Einwohnerzahl von 2.581.619 (Stand 1. Januar 2005). Damit konzentriert sich ein Viertel der Bevölkerung des Landes in der Hauptstadtregion.

Die zehn größten Städte Kubas sind:

In Kuba leben derzeit (Stand 2016) etwa 11,24 Millionen Menschen, davon über zwei Millionen in der Hauptstadt Havanna. Insgesamt leben 76,8 Prozent der Kubaner in städtischen Gebieten. Die Inselbevölkerung wuchs in der Vergangenheit kontinuierlich um eine Million Menschen je Jahrzehnt. Ursachen für das hohe Bevölkerungswachstum sind die hohe Lebenserwartung und die mit 4,9 Fällen je 1000 Geburten geringe Kindersterblichkeit. Aufgrund zurückgehender Geburtenrate (1,5 Kinder pro Frau) und Emigration hat sich die Bevölkerungsentwicklung inzwischen verlangsamt.
Prognosen zufolge wird die Bevölkerung im Jahre 2015 leicht zurückgegangen sein auf etwa 11,2 Millionen Menschen, im Jahre 2025 auf etwa 11,1 Millionen. Der Anteil der über 60-Jährigen soll dann 26 % der Gesamtbevölkerung betragen (2012: 18,3 %). Der sich abzeichnende demographische Wandel wird in Kuba deutlich langsamer einsetzen als beispielsweise in Deutschland, wo bereits heute knapp 26 % der Bevölkerung 60 Jahre und älter sind. Den Angaben des letzten Zensus von 2012 zufolge beträgt die Bevölkerung 11.167.325 Menschen.

Durch ein spanisches Gesetz, das Ley de Memoria Histórica "(Gesetz des Historischen Gedenkens)", von dem Kinder und Enkel von Flüchtlingen des Spanischen Bürgerkrieges profitieren, haben 150.000 bis 200.000 Kubaner das Anrecht auf die spanische Staatsbürgerschaft.
Die kubanische Bevölkerung teilt sich laut Eigenangaben der beim Zensus 2012 befragten Personen wie folgt auf:

Auffällig sind dabei die großen regionalen Unterschiede: Während sich in den westlichen Provinzen durchschnittlich 70–80 Prozent als Weiße bezeichnen, sind es in den östlichen Provinzen des Landes deutlich weniger. In Santiago de Cuba bezeichnen sich beispielsweise nur 25,6 Prozent der Einwohner als Weiße, 60 Prozent als Mulatte oder Mestize und 14,4 Prozent als Schwarze. In Havanna ergibt sich ein differenziertes Bild: Dort bezeichnen sich 58,4 Prozent als Weiße, 26,6 als gemischt und 15,2 Prozent als Schwarze.

Das präkolumbische Volk der Taíno, das die Insel vor der Ankunft der Spanier besiedelte, ist ausgestorben.

In Kuba wird Spanisch gesprochen. Jedoch weist die dort gesprochene Variante einige Besonderheiten zur in Spanien gesprochenen Hochsprache und auch zu den im übrigen Hispanoamerika gesprochenen spanischen Dialekten auf. Ein Großteil dieser Varietäten findet sich jedoch auch in anderen spanischsprachigen Ländern der Karibik, insbesondere in der Dominikanischen Republik, Puerto Rico und den Karibikküsten von Kolumbien und Venezuela. Minderheitensprachen, wie beispielsweise indianische Sprachen, existieren praktisch nicht.

Die grammatikalische Besonderheit, welche die Sprache mit dem übrigen Lateinamerika gemeinsam hat, ist die Nutzung von "ustedes" (Sie) als 3. Person Plural anstatt von "vosotros" (ihr - 2.Person Plural).

Die Aussprache ist ähnlich den übrigen spanischsprachigen Ländern in der Karibik und hat ihre historischen Wurzeln wohl hauptsächlich in den Regionen Spaniens, aus denen die erste größere Einwanderungswelle stammte, nämlich den Kanaren und aus Südspanien, und zeichnet sich unter anderem durch den sogenannten Seseo aus. So werden die im Hochspanisch unterschiedlichen Laute /θ/ (engl. "th") und /s/ immer wie /s/ ausgesprochen. Das Verschlucken einiger Konsonanten, wie des /s/ am Silben- und Wortende sowie des /d/ und /b/ zwischen Vokalen ist ebenfalls typisch. Auch wird (vor allem von Ostkubanern) häufig statt /r/ am Silbenende /l/ ausgesprochen: "puerta" (Tür) gerät dann zu "puelta" und "por favor (bitte) zu "pol favol.

1992 wurde Kuba durch Verfassungsänderung von einem atheistischen Staat zu einem säkularen, wodurch Gläubigen die Mitgliedschaft in der Kommunistischen Partei (PCC) ermöglicht wurde.

Als Kubas Hauptreligionen gelten der Katholizismus und die Santería, eine Mischreligion (Synkretismus). Sie basiert auf der traditionellen Religion der westafrikanischen Yoruba und ist stark mit christlichen Elementen vermischt. Als unpolitische und unorganisierte Form der Religionsausübung erhält die Santería seit einigen Jahren eine staatliche Förderung. Schätzungen zufolge sind etwa 35 Prozent der Kubaner katholisch getauft, darunter auch viele Santería-Anhänger. Nach Angaben des Vatikans seien 60 Prozent der Bevölkerung Katholiken. Schutzpatronin Kubas ist die Virgen de la Caridad del Cobre (Barmherzige Jungfrau von El Cobre), die in der Santería auch für die Göttin der Flüsse und der Liebe Ochún steht.

Neben der katholischen Kirche sind in den letzten Jahren zahlreiche kubanisch-protestantische Gemeinden entstanden, auch mehr als 96.000 Zeugen Jehovas werden inzwischen gezählt. Bereits seit 1492 gibt es das Judentum in Kuba, etwa 1500 Kubaner zählen sich dazu.

Kuba und die dort lebenden Arawak gerieten in der ersten Hälfte des 16. Jahrhunderts unter spanische Kontrolle. Innerhalb weniger Jahrzehnte wurden die indigenen Völker durch Gewalt und Krankheit praktisch ausgerottet. Für den sehr arbeitsintensiven Zuckerrohranbau kauften sich die spanischen Pflanzer im 17. und 18. Jahrhundert zehntausende Sklaven, vorwiegend aus Westafrika.

Die Kämpfe der Kolonie um Unabhängigkeit begannen 1868 und dauerten mit Unterbrechungen bis zum Abzug der Spanier im Jahr 1898 an, als die USA intervenierten (Spanisch-Amerikanischer Krieg).

Im Zehnjährigen Krieg (1868–1878) und im Kleinen Krieg (1878–1879) um die Unabhängigkeit waren die Kubaner noch gescheitert. Am 10. Dezember 1898 erklärte Spanien im Friedensvertrag von Paris den Verzicht auf Kuba und die Philippinen. Zuvor kämpften der kubanische Nationalheld José Martí und die Oberbefehlshaber Máximo Gómez und Antonio Maceo im Unabhängigkeitskrieg seit 1895 mit einer sehr kleinen Armee gegen über 200.000 Spanier.

Nach dem Ende des Spanisch-Amerikanischen Krieges besetzten die USA die Insel, bis sie schließlich 1902 die formale Unabhängigkeit erlangte. Die Souveränität war bis 1934 jedoch durch das "Platt Amendment" eingeschränkt, das den USA bei Beeinträchtigung US-amerikanischer Interessen ein jederzeitiges Interventionsrecht in Kuba gab. Ein Überrest dieser US-amerikanischen Sonderrechte ist der gegen den erklärten kubanischen Willen noch heute von den USA aufrechterhaltene Marinestützpunkt Bahía de Guantánamo "(Guantánamo Bay)", dessen Militärgefängnis infolge der Terroranschläge am 11. September 2001 internationale Bekanntheit erlangte.

Anfang 1959 stürzten die kubanischen Revolutionäre unter der Führung von Fidel und Raúl Castro, Camilo Cienfuegos und dem Argentinier Ernesto Guevara, genannt "Che", den kubanischen Diktator Fulgencio Batista und errichteten ab 1961 (Deklaration von Havanna) einen sozialistischen Staat. Die damit verbundenen Enteignungen von US-Firmen und US-Bürgern führten zu einem dauerhaften Embargo der USA und weiterer westlicher Staaten gegen Kuba. Kuba suchte und fand Unterstützung bei den sozialistischen Staaten Osteuropas, insbesondere der damaligen Sowjetunion.

Aufgrund der strategischen Lage Kubas eskalierte 1962 der Konflikt zwischen den USA und der UdSSR in der sogenannten Kubakrise. Noch heute leidet Kuba unter wirtschaftlichen Sanktionen und ist als eines von wenigen Ländern nicht Mitglied in supranationalen Bündnissen.

In mehreren Flüchtlingswellen verließen tausende Kubaner ihre Heimat, von denen sich ein Großteil in Florida, insbesondere in Miami (siehe Little Havana), ansiedelte.

Mit dem Ende der kommunistischen Diktaturen in Osteuropa nach den Revolutionen im Jahr 1989 fielen Kubas wichtigste Handelspartner und Geldgeber (Sowjetunion und übrige RGW-Staaten) weg und Kuba erlebte zu Beginn der 1990er-Jahre eine schwere Wirtschaftskrise, die 1993 ihren Höhepunkt erreichte. Hatte Kuba zuvor fast seine gesamte Zuckerernte in die sozialistischen Staaten Osteuropas verkauft und im Gegenzug zwei Drittel seiner Nahrungsmittel, fast das gesamte Öl und 80 Prozent seiner Maschinen und Ersatzteile von dort bezogen, so waren auf einmal 85 Prozent seines Außenhandels weggebrochen. Die Industrie und das Transportwesen kamen wegen Ölmangels zum Erliegen und infolge drastischer Nahrungsmittelrationierungen kam es erstmals seit vielen Jahren zu Unterernährung auf der Insel. 1992 beschloss die Regierung, als Ersatz für den verlorengegangenen Außenhandel die Tourismusindustrie zu entwickeln.
Unter der Führung von Carlos Lage wurde die Wirtschaft dezentralisiert und Marktwirtschaft und Devisenhandel in einigen Nischen zugelassen. Joint-Venture-Geschäfte im Bereich des Tourismus, die Zusammenarbeit mit neuen Außenwirtschaftspartnern (unter anderem Spanien, Italien, Kanada, Brasilien, Volksrepublik China, Venezuela), die Entdeckung von neuen Ölvorkommen und die Vermarktung der bedeutenden Nickelvorkommen trugen zur Stabilisierung der kubanischen Wirtschaft bei. Allerdings entstanden auch soziale Disparitäten.

Nachdem durch die notwendig gewordene Wiedereingliederung Kubas in den karibischen Wirtschaftsraum ein gewisser wirtschaftlicher Aufschwung zu verzeichnen war, führten die Hurrikans von 2008, die sich zeitlich mit dem Höhepunkt der globalen Wirtschaftskrise überschnitten, zu einer erneuten Verschärfung der Krise. Die kubanische Bevölkerung spricht von ihr seitdem als der "segunda crisis de los 90" (zweiten Krise der 90er). 2006 angekündigte Wirtschaftsreformen wurden fünf Jahre später von der kubanischen Nationalversammlung als neue „Leitlinien der Wirtschafts- und Sozialpolitik“ "(lineamientos de la política económica y social)" gebilligt. 2012 war die Versorgungskrise aus den Jahren um 2008 überwunden. Die Atmosphäre ist laut dem Spiegel-Korrespondenten Jens Glüsing „offener und entspannter“, der wirtschaftliche Aufschwung sei überall zu spüren. Die inflationsbereinigten Gehälter erreichten 2011 jedoch weiterhin lediglich 51 % des Wertes von 1989.

Im Mai 2013 bestätigte der FAO-Generaldirektor José Graziano da Silva in einem Gespräch mit Raúl Castro, dass Kuba das 1996 beim Weltgipfel der FAO in Rom definierte Ziel der Halbierung der Zahl der unterernährten Personen vorzeitig erreicht habe. Kuba befindet sich unter den 16 Ländern, die weltweit bei der Bekämpfung des Hungers die größten Fortschritte vorzuweisen hätten. Die Öffnung der der zentralistischen Staatswirtschaft erfolgte jedoch so zurückhaltend, dass keine belebende Effekte auftraten und sich das Land 2017 gar in einer Rezession befand.

Kuba gilt in der Politikwissenschaft als bürokratisch-autoritärer Staat. Gewaltenteilung existiert hier faktisch nicht.<ref name="ipg 3/2002">Hans-Jürgen Burchardt: "Kuba nach Castro: Die neue Ungleichheit und das sich formierende neopopulistische Bündnis", Friedrich-Ebert-Stiftung, Internationale Politik und Gesellschaft 3/2002</ref> Gemäß der marxistisch-leninistischen Ideologie der herrschenden Kommunistischen Partei Kubas handelt es sich um deren positiv besetzte Ausprägung einer Diktatur des Proletariats. Da der Bevölkerung keine Auswahlmöglichkeiten über eventuelle politische Alternativen zur Verfügung stehen, kann über die Zustimmungsrate nur spekuliert werden.

Formal ranghöchstes und gesetzgebendes Organ ist das Parlament (Asamblea Nacional del Poder Popular), welches den Staatsrat (Consejo de Estado) und den Ministerrat wählt. Tatsächlich ist das nur zweimal im Jahr zusammentretende Parlament relativ einflusslos und hat vor allem die Aufgabe, Entscheidungen abzusegnen und die Regierung formal zu entlasten. Seit Einführung des Parlaments 1976 gab es bis auf eine Ausnahme in keiner der vielen Abstimmungen von Seiten der rund 600 Abgeordneten eine einzige Gegenstimme zu einem von der politischen Führung vorgelegten Entwurf, auch wenn der neue Präsident Raúl Castro die im kubanischen politischen System übliche Einstimmigkeit in einer programmatischen Rede 2008 als „für gewöhnlich fiktiv“ kritisierte, was er seitdem bei mehreren Gelegenheiten wiederholt hat. Im Dezember 2013 stimmte die LGBT-Aktivistin und Präsidententochter Mariela Castro gegen den Regierungsentwurf eines neuen Arbeitsgesetzbuchs, weil sie darin die Rechte von HIV-Infizierten und Transgendern nicht ausreichend gewürdigt sah.

Die eigentliche politische Entscheidungsmacht liegt ausschließlich im Staats- und Ministerrat. Dadurch, dass in diesen Gremien in der Regel dieselben Personen sitzen, die zugleich auch noch die höchsten Posten in der einzig zugelassenen kommunistischen Partei bekleiden, beschränkt sich die Machtausübung auf wenige Personen. Ihre Legitimationsquelle bezieht die kubanische Regierung vor allem aus einem aus Jahrhunderten von Fremdbestimmung herrührenden Nationalismus und der Feindschaft gegenüber den USA, welche dieses Gefühl durch ihre Embargo- und Einmischungspolitik weiter verstärken. Der Versuch der USA, oppositionelle Gruppen aufzubauen, setzt kritische Stimmen sofort des Verdachts der Konterrevolution und des Landesverrats aus und legitimiert somit deren repressive Verfolgung.

Wahlen finden unter Kontrolle der Regierung statt: Der Staatsrat setzt für deren Organisation, Ablauf und Auswertung die Nationale Wahlkommission ein, die wiederum die Wahlkommissionen der Provinzen besetzt – eine Kontrollkette, die sich bis zu den für die einzelnen Wahlbezirke verantwortlichen Kommissionen fortsetzt. Jeder Kubaner ab einem Alter von 16 Jahren darf wählen (aktives Wahlrecht) und gewählt werden (passives Wahlrecht). Für Abgeordnete der Nationalversammlung gilt das Mindestalter von 18 Jahren. Auf der untersten Ebene der Munizipialparlamente stehen zwischen zwei und acht Kandidaten pro Parlamentssitz zur Auswahl. Die Wahl zwischen den Kandidaten findet in vom örtlichen CDR organisierten Einwohnerversammlungen offen per Handzeichen statt. Für die Wahl der Provinzparlamente und der obersten Nationalversammlung gibt es pro Parlamentssitz genau einen Kandidaten. Dabei werden jeweils 50 Prozent von der jeweils untergeordneten Volksversammlung bestimmt, die restlichen 50 Prozent direkt vom Volk gewählt.

Die Abgeordneten werden von einem Ausschuss der PCC bzw. der Massenorganisationen ausgewählt. Sie dürfen keinen Wahlkampf betreiben und müssen sich gemäß der Verfassung dem sozialistischen System verpflichten. Den Wählern werden nur wenige Grunddaten der Kandidaten zur Kenntnis gegeben: Name, Alter, Beruf, formales Bildungsniveau. Der Frauenanteil im kubanischen Parlament ist mit 48 % im Jahr 2014 (2000: 28 %) der höchste aller Länder Lateinamerikas und der Karibik. Im Politbüro, dem höchsten Entscheidungsgremium der Kommunistischen Partei, das die politischen Leitlinien des Staates vorgibt, ist unter den 15 beim VI. Parteitag 2011 gewählten Mitgliedern jedoch nur eine einzige Frau vertreten, was einem prozentualen Anteil von 6,6 entspricht. Ungefähr fünf Prozent der Stimmen werden regelmäßig als weiß (gegen alle Kandidaten) markiert.

Über fast 50 Jahre vereinigte Revolutionsführer Fidel Castro die zentralen politischen Ämter in seiner Person. Er war zuletzt Staatspräsident, Vorsitzender des Staats- und des Ministerrates, Generalsekretär des Zentralkomitees der Kommunistischen Partei Kubas und auch Oberbefehlshaber der Streitkräfte. Die Posten des Staatsratspräsidenten, des Oberbefehlshabers der Streitkräfte und des KP-Generalsekretärs übergab er am 1. August 2006 wegen einer lebensbedrohlichen Darmerkrankung an seinen Bruder Raúl Castro.

Am 24. Februar 2008 wurde Raúl Castro vom Parlament zum Staats- und Ministerpräsidenten gewählt und vertritt seitdem eine Linie der politischen Kontinuität bei gleichzeitiger Konzentration auf Maßnahmen zur Behebung der extrem kritischen wirtschaftlichen Lage. Im April 2011 übernahm Raúl Castro auch das Amt des KP-Generalsekretärs. Neben dem Personalwechsel sehen manche Beobachter auch eine Änderung des Systems von einem "charismatischen Sozialismus" unter Fidel hin zu einem "bürokratischen Sozialismus" unter Raúl Castro, der weniger auf die Mobilisierung der Bevölkerung setzt und mehr administrative Effizienz und wirtschaftliche Reformen verspricht. Im Jahr 2013 war für das Frühjahr 2018 ein Wechsel des Staats- und Regierungschefs angekündigt worden; Raúl Castro würde demnach nur das (mächtige) Präsidium der KP verbleiben.
Nach seiner Teilgenesung und seinem erklärten Verzicht auf eine Rückkehr in die Führungsverantwortung trat Fidel Castro seit Juli 2010 bis zu seinem Tod im November 2016 gelegentlich wieder in der Öffentlichkeit auf.

Bei einfachen Zivil- und Strafverfahren auf den unteren Ebenen stellen Laien-, sonst Berufsrichter die Mehrheit. Alle Richter werden von der Volksvertretung ihrer jeweiligen Ebene gewählt. Gerichte und Anwaltschaft sind nicht unabhängig. Das kubanische Rechtssystem entspricht nicht westlichen Standards, insbesondere in politischen Verfahren, gewährleistet aber eine funktionierende Gerichtsbarkeit. Das höchste Gericht ist das „Oberste Volksgericht“, dessen Präsident vom Staatsratsvorsitzenden nominiert und von der Nationalversammlung gewählt wird. Seit 1998 ist Rubén Remigio Ferro Präsident des Obersten Volksgerichts, nachdem er seit 1997 als dessen Vizepräsident und zuvor als Kader im Zentralkomitee der Kommunistischen Partei und in von ihr abhängigen Massenorganisationen tätig war.

Die Rechtsanwälte, die direkten Rechtsbeistand für Privatpersonen leisten, d. h. nicht in Behörden oder Unternehmen angestellt sind, sind in Kuba in Anwaltskollektiven (, ‚kollektive Anwaltskanzleien‘) organisiert. Diese entstanden zu Beginn der sozialistischen Herrschaft auf Betreiben von mit den egalitären Zielen der Revolution sympathisierenden Anwälten, um einen öffentlichen Zugang zu Rechtsdienstleistungen zu gewährleisten. Durch ein Gesetz von 1973 wurden diese Kollektive institutionalisiert, wodurch Anwälten, die nicht auf diese Weise organisiert sind, nur noch in Ausnahmefällen eine Vertretung von Privatpersonen möglich ist. Diese Kollektive besitzen eine gewisse organisatorische Unabhängigkeit, Einnahmen und Ausgaben werden jedoch zentral über die "Organización Nacional de Bufetes Colectivos" abgewickelt, welche auch gewisse administrative Vorgaben macht. Diese wird von gewählten Vertretern der Kollektive geleitet, untersteht jedoch der Aufsicht des Justizministeriums. Erklärtes Ziel der Kollektive ist, mit ihrer Tätigkeit zur sozialistischen Entwicklung beizutragen, was in Konflikt mit dem Auftrag, die Interessen der Klienten zu vertreten, stehen kann. Einem 1998 erschienenen Fachaufsatz zufolge ist allerdings kein Fall bekannt, in dem ein Ausschluss aus dem System der Kollektive aus ideologischen Gründen erfolgt sei.

Die Todesstrafe existiert nur noch formal, sie wurde zuletzt 2003 für die bewaffnete Entführung einer Personenfähre ausgesprochen und vollstreckt. Ende Dezember 2010 wurde die letzte zur Vollstreckung anstehende Todesstrafe von Kubas Oberstem Gerichtshof in eine Haftstrafe umgewandelt. Des Weiteren darf die Todesstrafe nicht an unter 20-Jährigen vollzogen werden sowie an Frauen, die zum Tatzeitpunkt oder zum Zeitpunkt des Strafvollzugs schwanger sind.

Die Situation in den kubanischen Gefängnissen gilt als unbefriedigend. Insbesondere politische Gefangene berichten regelmäßig von unzumutbaren Haftbedingungen. Nach Berichten ehemaliger Gefängnisinsassen sind primitivste Lebensbedingungen, verweigerte medizinische Versorgung, Isolationshaft, Misshandlungen und teilweise Folter an der Tagesordnung. Die Regierung verweigert internationalen Menschenrechtsgruppen und einheimischen unabhängigen Organisationen Zugang zu den Gefängnissen. Zwar behauptet die kubanische Regierung, dass Kuba – abgesehen vom US-Gefangenenlager Guantanamo – frei von Folter sei, unabhängige Beobachter wie Amnesty International, das Internationale Rote Kreuz oder den UN-Sonderberichterstatter über Folter lässt man aber seit Jahren nicht ins Land, um die Situation in den Gefängnissen zu inspizieren.

Im Mai 2012 machte die kubanische Regierung über einen Artikel der Tageszeitung "Granma" erstmals Angaben über die Gesamtzahl der Häftlinge: 57.337. Dies bedeutet einen extrem hohen Anteil von Gefangenen in Relation zur Gesamtbevölkerung (510 pro 100.000), der nach weltweiten Vergleichsstudien nur von sechs Staaten übertroffen wird, darunter die USA und Russland. Mögliche Gründe für die hohe Zahl der Gefangenen gab die Regierung nicht an, stattdessen lobte der Zeitungsartikel das kubanische Strafvollzugssystem als vorbildlich: So bilde die Resozialisierung ein zentrales Element des Systems. Die Regierung betreibt hierzu Programme, die es den Gefängnisinsassen ermöglichen sich fortzubilden, Sport zu betreiben und sich kulturell zu betätigen. Nach Regierungsangaben nehmen im Jahr 2012 mit etwa 27.000 Gefängnisinsassen knapp die Hälfte aller Gefangenen des Landes diese Bildungsprogramme in Anspruch, 24.000 besuchen spezialisierte Kurse. Nach diesen Angaben verrichten auch 23.000 Insassen soziale Arbeit auf freiwilliger Basis. Auch Konzerte finden in den Gefängnissen statt. Nach 49 Jahren des Verbots genehmigte die kubanische Regierung der katholischen Kirche 2008 erstmals die Abhaltung von Weihnachtsgottesdiensten in mehreren Gefängnissen. Jugendliche zwischen 16 und 18, die straffällig werden, werden nur in speziellen Jugendgefängnissen untergebracht, in denen ihnen Bildung zusteht um ihre soziale Reintegration zu fördern. Seit 2007 investiert die kubanische Regierung verstärkt in die Gefängnisinfrastruktur, mit dem Ziel, die Haftbedingungen bis 2017 zu verbessern.

Gemäß der Verfassung ist die führende Rolle im Staate der Kommunistischen Partei Kubas (Partido Comunista de Cuba) zugewiesen, welche sie gemeinsam mit den Massenorganisationen ausübt. Sie versteht sich als Avantgarde der kubanischen Nation. Andere Parteien sind nicht zugelassen.

Der PCC hat über 800.000 Mitglieder. Die Parteizugehörigkeit fördert den beruflichen und gesellschaftlichen Aufstieg. Für höhere Positionen in Wirtschaft, Militär und Staat ist eine Mitgliedschaft in der Partei Voraussetzung.
Am 28./29. Januar 2012 tagte die I. Nationale Parteikonferenz der PCC in Havanna. Grundlage der Konferenz war ein Entwurf vom Oktober 2011, der in über 65.000 Treffen der Parteimitglieder diskutiert worden war. Dabei wurden 78 von 96 Punkten modifiziert und fünf neue in das Dokument aufgenommen. Inhaltlich ging es bei der Konferenz, die sich als Fortsetzung der Politik des VI. Parteitages verstand, um die zukünftige Rolle der PCC in der kubanischen Gesellschaft sowie um ihren internen Arbeitsstil. Die mehr als 800 Delegierten bekräftigten das Festhalten am Einparteiensystem, beschlossen aber gleichzeitig eine Ausweitung der internen Demokratie. Es wurde entschieden, dass die Diskriminierung aufgrund von Geschlecht, Hautfarbe oder religiöser Anschauung bekämpft werden soll. Außerdem werden hohe Regierungsposten auf eine Amtszeit von zweimal fünf Jahren beschränkt. Raúl Castro schloss sich hierbei ausdrücklich mit ein. Zudem werden Partei- und Regierungsämter stärker getrennt werden. Die Partei soll die politische, nicht die juristische Führung des Landes sein. Die Medien sollen mit mehr Informationen versorgt werden und die Verbindung zur Jugend gestärkt werden. In den nächsten Jahren sollen 20 % der ZK-Mitglieder jungen Nachwuchskräften Platz machen. Auch wurde der Korruption der Kampf angekündigt, die ein viel größerer Feind für die Revolution sei als die Sabotageakte der USA.

Das parlamentarische System in Kuba besteht aus den sogenannten Versammlungen der Volksmacht "(Asamblea del Poder Popular)". Sie sind in drei Ebenen aufgeteilt: die Nationalversammlung "(Asamblea Nacional del Poder Popular)", den Volksversammlungen auf Provinzebene sowie auf der Ebene der Municipios (Landkreise). Die Wahlen dazu bezeichnet die kubanische Regierung als „frei, geheim und gleich“. Kubanische Bürger dürfen ab einem Alter von 16 Jahren wählen und ab 18 Jahre gewählt werden.

Auf den beiden oberen Ebenen werden die Kandidaten für das jeweilige Parlament durch eine Wahlkommission, gebildet aus Vertretern der sechs Massenorganisationen, ausgewählt. Diese stehen gemäß Verfassung unter direkter Kontrolle der Kommunistischen Partei (PCC), der die Führungsrolle der Gesellschaft zukommt. Die Wahl selbst sollte nach Willen der kubanischen Regierung per Einheitsstimme für alle Kandidaten "(voto unido)" – auf ein Parlamentssitz kommt genau ein Kandidat – stattfinden. Eine "weiße Wahl" "(voto en blanco)", also die Wahl keiner der auf dem Stimmzettel stehenden Kandidaten, ebenso wie Streichungen oder Anmerkungen werden als ungültig gewertet.

Auf kommunaler (munizipialer) Ebene erfolgt die Kandidatenwahl in Bürgerversammlungen, die durch die "Komitees zur Verteidigung der Revolution" "(CDR)" organisiert werden. Jeder Bürger hat dort das Recht, Kandidaten vorzuschlagen. Abgestimmt über diese Kandidaten wird in offener, nicht geheimer Wahl. Nur wer mindestens 50 % der Stimmen in so einer Bürgerversammlung erhält, wird als Kandidat bei der Wahl zur "Versammlung der Volksmacht" zugelassen. Oppositionelle Kandidaten sind praktisch chancenlos.

Die Legislaturperiode beträgt fünf Jahre auf nationaler und Provinzebene sowie zweieinhalb Jahre auf kommunaler Ebene. Die gewählten Volksvertreter müssen ihren Wählern regelmäßig Rede und Antwort stehen; das Mandat kann ihnen jederzeit wieder entzogen werden. Die Wähler können den Kommunalparlamenten jederzeit Vorschläge oder Probleme ihrer Gegend unterbreiten. Laut Angaben des kubanischen Parlaments wurden in der Wahlperiode 2010-12 insgesamt 209.000 dieser Eingaben eingereicht, für mehr als 60 % konnten Lösungen gefunden werden.

Wahlwerbung ist nur den staatlichen Medien erlaubt, nicht jedoch dem einzelnen Kandidaten. Von ihm werden nur Passfoto und Kurzlebenslauf bekannt gegeben, nicht jedoch seine politischen Positionen oder seine Pläne in der Politik. Laut kubanischer Regierung soll dies sicherstellen, dass nicht der Kandidat mit dem meisten Geld gewinnt, sondern die gesamte Bevölkerung entsprechend ihrem Anteil sich in den Parlamenten repräsentiert sieht. Dennoch sind vor allem in höheren Volksvertretungen „Arbeiter, Bauern, Schwarze und niedrige Dienstleitungsberufe“ eher unterrepräsentiert. Und obwohl nur fünf Prozent der kubanischen Bevölkerung Parteimitglieder sind, liegt deren Anteil bei den Abgeordneten der Asamblea Nacional bei fast einhundert Prozent. Tatsächlich dient das Wahlsystem dazu, die Herrschaft der Revolutionselite um die Castro-Brüder zu sichern.

Die UN-Menschenrechtskommission bewertete die Wahlen in Kuba als undemokratisch, da die Ergebnisse praktisch vorher feststehen.

Eine der Regierung und der Kommunistischen Partei gegenüberstehende, organisierte Opposition ist im politischen System Kubas nicht vorgesehen, nicht regierungskonforme Parteien oder Organisationen der Zivilgesellschaft sind illegal.

Die innerkubanische Opposition versucht grundsätzlich, eine Transformation auf Kuba zu erreichen, dabei bestehen jedoch teilweise große ideologische und strategische Meinungsverschiedenheiten unter konkurrierenden Gruppierungen. Zudem besitzt die Regierung wirksame Instrumente der Kontrolle und Repression (siehe Abschnitt Menschenrechtssituation).

Zu den prominentesten Vertretern der Opposition gehören gegenwärtig die Menschenrechtsgruppe „Damen in Weiß“, die Organisation "Unión Patriótica de Cuba" (UNPACU, zu deren Führungsmitgliedern die ehemaligen politischen Gefangenen Guillermo Fariñas und José Daniel Ferrer gehören) und die vor allem im Ausland beachtete Journalistin und Bloggerin Yoani Sánchez. Eine große Zahl von Regierungsgegnern ist im Exil aktiv, das durch die von der Regierung nicht mehr behinderte Auswanderung kubanischer Oppositioneller weiter Zulauf erhält. Politische Äußerungen oder Aktionen von Kubanern im Ausland sind jedoch auf der Insel kaum wahrzunehmen.

Als einzige kubanische Institution hat sich die Katholische Kirche Kubas während der Präsidentschaft Raúl Castros in wenigen Einzelfällen als Vermittlerin zwischen Regierung und Opposition eingesetzt. Das wichtigste Beispiel hierfür war die Entlassung Dutzender politischer Gefangener, die 2010 in großer Mehrheit gemeinsam mit ihren Familien ins Exil nach Spanien ausgeflogen wurden. Die Kirche bietet innerhalb der eigenen Gebäude, Veröffentlichungen und Veranstaltungen einen eingeschränkten Freiraum für politische Meinungsäußerungen, die von der Regierungsposition abweichen können. Diese Äußerungen reichen von geduldeten Demonstrationen der „Damen in Weiß“ auf Kirchengelände über eigene Hirtenbriefe der Bischofskonferenz bis zur Ausrichtung gesellschaftswissenschaftlicher Kolloquien.

Gewerkschaften unter Führung der Zentralgewerkschaft Central de Trabajadores de Cuba, Komitees zur Verteidigung der Revolution (Comités de Defensa de la Revolución, CDR), der Frauenverband und Jugendverbände, wie die Pionierorganisation José Martí, die Jungen Kommunisten und die Föderation der Hochschulstudenten, bilden Massenorganisationen, welche fast jeden Kubaner in das staatliche System einbinden und zugleich sein Sozialverhalten kontrollieren („Augen und Ohren der Revolution“). Die Massenorganisationen sind wie die PCC hierarchisch aufgebaut.

Die Regierung erreicht mit Hilfe der Massenorganisationen eine starke Mobilisierung der Bevölkerung. Bei wochenlangen Demonstrationskampagnen bringt sie beinahe jeden erwachsenen Kubaner mindestens einmal auf die Straße (Rekord: sieben Millionen Teilnehmer). Für das Verfassungsreferendum 1976 zur Festschreibung des Sozialismus haben die CDR die Unterschriften von fast 93 Prozent der Bevölkerung gesammelt.

Arbeitnehmerorganisationen außerhalb des staatlichen Gewerkschaftsbundes sind verboten.

Viele bürgerliche und politische Rechte, insbesondere die auf freie Meinungsäußerung, Presse-, Vereinigungs-, Versammlungs- und Bewegungsfreiheit, werden massiv beschnitten. Es gibt keine unabhängige Gerichtsbarkeit. Menschenrechtsvereinigungen sind nicht zugelassen. Die kubanische Verfassung garantiert viele Grundrechte, etwa die Kunstfreiheit, die Meinungsfreiheit und die Religionsfreiheit, nur mit der Einschränkung, dass ihre Ausübung nicht gegen die Revolution oder die sozialistischen Ziele gerichtet sein darf. Kuba war und ist das einzige sozialistische Land, in dem die Freimaurerei nicht verboten ist. Es gibt hier etwa 30.000 Freimaurer.
HIV-Infizierte und Homosexuelle wurden in Kuba lange Zeit diskriminiert und HIV-Infizierte unter Haft gewalttätigen Repressalien ausgesetzt. Zwar habe sich die Situation in den letzten Jahren stark verbessert, dennoch beklagen Betroffene weiterhin Übergriffe der Polizei gegen sexuelle Minderheiten.

Internationale Menschenrechtsorganisationen wie Human Rights Watch und Amnesty International dokumentieren insbesondere die politisch motivierte Verhaftung und Verurteilung von Regierungskritikern. Von 75 politischen Dissidenten, die nach ihrer Inhaftierung 2003 zu 28 Jahren Haft verurteilt wurden, saßen 2008 noch 55 in den Gefängnissen, unter schlechter medizinischer Versorgung und unter Misshandlungen leidend. Mitte 2010 erreichte die Katholische Kirche in Kuba unter Verhandlungsführung von Kardinal Jaime Ortega die Zusage der kubanischen Regierung, alle bis dahin verbliebenen 52, von Amnesty International als gewaltfreie politische Gefangene geführten Häftlinge freizulassen. Bis Ende 2010 kamen 41 politische Gefangene frei. Bis auf einen wurden alle zusammen mit den engsten Familienangehörigen nach Spanien ausgewiesen, welches sich zur Aufnahme der Dissidenten bereit erklärte. Ende März 2011 wurden die restlichen Gefangenen der im Rahmen des Schwarzen Frühlings 2003 festgenommenen "Gruppe der 75" freigelassen. Zwei von ihnen wurde gestattet, in Kuba zu bleiben.
Am 24. Dezember 2011 kündigte Präsident Raúl Castro eine Amnestie an, die rund 3000, vor allem nicht politische Gefangene betreffen sollte. Seitdem setzt die kubanische Regierung verstärkt auf Kurzfestnahmen von Regierungsgegnern. Fünf von Amnesty International als gewaltfreie Gewissensgefangene anerkannte politische Gefangene wurden im Januar 2015 entlassen, drei davon auf Bewährung. Ein politischer Gefangener saß zu dieser Zeit noch seine einjährige Haftstrafe wegen „öffentlicher Störung“ ab. Die Repressionen gegen Oppositionelle gingen jedoch weiter.

Unabhängige Journalisten und Menschenrechtsaktivisten werden regelmäßig belästigt, eingeschüchtert und vorübergehend festgenommen. Es wird von Misshandlungen durch Fußtritte und Schläge berichtet. Die Haftbedingungen sind hart und führen zum Teil zu körperlichen Problemen bei den Häftlingen. Oppositionelle werden darüber hinaus regelmäßig sogenannten Actos de Repudio ausgesetzt. Dabei zieht ein organisierter Mob vor dem Haus des Oppositionellen auf und beschimpft ihn und seine Familie stundenlang und lautstark als „Würmer“ (spanisch: "gusanos") und Verräter. Teilweise geht dies bis zur straffreien Zerstörung von Eigentum der Betroffenen.

Der institutionelle Rassismus des früheren Kubas wurde nach dem Sieg der Revolution abgeschafft. Jedoch wurden rassistische Denkweisen und latente Benachteiligung des schwarzen Bevölkerungsteils seitdem nicht überwunden. In prestigeträchtigen Führungspositionen oder in Jobs, welche Deviseneinkommen versprechen, beispielsweise im Tourismus, sind Weiße überproportional vertreten. Auch bei der Zulassung für privates Kleingewerbe oder bei Geldüberweisungen von emigrierten Verwandten im Ausland sind Schwarze indirekt benachteiligt.

Seit Ende 2007 wird vereinzelt öffentliche Kritik an den Zuständen geduldet. So hatte Raúl Castro, damals noch Interimsstaatschef, dazu aufgerufen, über die zukünftige Entwicklung des Landes zu diskutieren, die Kubanerin Yoani Sánchez berichtet in einem Blog aus Kuba öffentlich über die Alltagsprobleme der Kubaner. Dennoch habe sich die Menschenrechtssituation gemäß Amnesty International in einer Stellungnahme vom August 2013, in der sie fünf neue Kubaner als gewaltlose Gewissensgefangene benannte, unter Raúl Castro nicht signifikant verbessert. Die namentlich bekannten politischen Gefangenen stellten nur „die Spitze des Eisberges“ alltäglicher staatlicher Repressionen dar. Die einzige positive Ausnahme sei das im Januar 2013 in Kraft getretene Migrationsgesetz, welches nun auch Regierungskritikern das Reisen ins Ausland erlaube.

Die sozialen Menschenrechte sind in Kuba teilweise gut umgesetzt. So gilt zum Beispiel das Recht auf Bildung für die Region als vorbildlich, ebenso die Gesundheitsversorgung. Der allgemeine Lebensstandard ist hingegen, gemessen nach dem Standard industrialisierter Länder, auf niedrigem Niveau. Dies betrifft vor allem die Wohnsituation und die Versorgung mit Gütern des täglichen Bedarfs. Daran trage laut Amnesty allerdings auch das US-Embargo gegen Kuba eine Mitschuld. Der internationale Programmkoordinator des Bevölkerungsfonds der UNO (UNFPA) in Kuba, Jesús Robles, hat im Juli 2011 die Arbeit der kubanischen Regierung bei der Förderung und dem Schutz von Frauen, Jugendlichen und Kindern hervorgehoben. Der Staat garantiert Müttern einen Mutterschaftsurlaub mit Lohnausgleich und dem Recht zur anschließenden Rückkehr in den Beruf. Eltern von Neugeborenen bekommen für das erste Jahr pro Monat einen voll bezahlten Tag freigestellt, um die Gesundheit des Kindes in der Kinderklinik zu überprüfen.

Eines der obersten Ziele der Revolution war die Gleichberechtigung von Mann und Frau. Im Jahr 1953 gingen 13,9 % der Frauen einer Arbeit nach, im Jahr 1980 waren es 31,1 %, im Jahr 2008 bereits 38 % (siehe Diagramm). Der Frauenanteil bei technischen Berufen beträgt 65,7 %, der Anteil an weiblichen Führungskräften 39,1 %. 65 % der Hochschulabsolventen sind weiblich. Dennoch gibt es auch hier einen Unterschied zwischen offiziellem Regierungsdiskurs und gelebter Praxis. Die meisten Frauen sind der belastenden Doppelrolle zwischen Beruf und Haushalt ausgesetzt. Je höher die Führungsebene in der Arbeitswelt oder innerhalb der Regierung, desto niedriger wird der Frauenanteil. Im 15-köpfigen Politbüro war 2012 nur eine einzige Frau vertreten. Unter den einflussreichsten Personen Kubas befindet sich vermutlich keine einzige Frau. Frauenrechtlerinnen beklagen noch heute vorherrschende „Entscheidungsinstanzen, in denen noch immer patriarchale und machistische Muster vorherrschten.“ Diese und andere Probleme werden zwar regelmäßig auf Konferenzen und Tagungen der Massenorganisationen, beispielsweise des Frauenverbandes FMC, angesprochen und diskutiert, jedoch sind die Möglichkeiten, tatsächliche tiefgreifende Veränderungen herbeizuführen, eng begrenzt. Im Zweifel hat insbesondere auf Funktionärsebene die Staats- bzw. Parteiräson Vorrang vor der Interessenvertretung.

Am 21. Juni 2010 wurde Kuba in das Vizepräsidium des Menschenrechtsrates der Vereinten Nationen gewählt. Der Botschafter Havannas bei der UNO, Rodolfo Reyes Rodríguez wurde für das Amt bestimmt.

Im Januar 2013 trat eine international beachtete Reisegesetznovelle in Kraft, die bisher von hohen bürokratischen Hürden behinderte Auslandsreisen von Kubanern grundsätzlich stark vereinfachte. Die Maßnahme war lange erwartet worden, seit die Regierung im Mai 2011 bekannt gegeben hatte, die bisherigen restriktiven Bestimmungen zu überprüfen. Mit der Reform wurde die für Kubaner bisher für jede einzelne Auslandsreise notwendige und mit hohen Kosten verbundene Ausreisegenehmigung abgeschafft, für die zudem jeweils eine Einladung aus dem Ausland erforderlich war. Außerdem wurde erstmals auch Minderjährigen die Möglichkeit zu Auslandsreisen eingeräumt, der zulässige Höchstaufenthalt im Ausland auf 24 Monate erweitert und zahlreichen aus Kuba geflohenen Kubanern die bisher verbotene Heimreise nach Ablauf gewisser Fristen grundsätzlich gestattet. Auch mehrere bisher an der Ausreise gehinderte Oppositionelle konnten das Land ab Februar 2013 für zeitweise Auslandsaufenthalte verlassen, während allerdings anderen die Ausstellung eines Reisepasses auch weiterhin aus politischen Gründen verweigert wurde. Ein grundsätzliches Recht auf Ausreise besteht weiterhin nicht. Das Gesetz gab den Behörden die ausdrückliche Möglichkeit, die Ausreise aus nicht näher definiertem „öffentlichen Interesse“ zu versagen. Ein Reisepass ist für weite Teile der Bevölkerung unerschwinglich: Er kostet 100 CUC, rund fünf durchschnittliche Monatsgehälter, und muss alle zwei Jahre zum gleichen Preis verlängert werden.

Eine legale Ausreise aus Kuba, egal ob zu touristischen Zwecken oder zur Auswanderung, war nur nach einem aufwändigen Genehmigungsverfahren möglich, das jedoch mit einer Mitte Januar 2013 in Kraft getretenen Novellierung des Migrationsgesetzes für die meisten Kubaner stark vereinfacht und verbilligt wurde. Das kubanische Strafgesetzbuch sieht Haftstrafen von ein bis drei Jahren oder Geldstrafen bei ungenehmigten Ausreisen oder Ausreiseversuchen vor. Bevorzugtes Auswanderungsziel sind die USA.

Insgesamt sind in den Jahren nach dem Sieg der Revolution hunderttausende Kubaner in die USA geflohen. Dies waren in der ersten Welle bis ca. 1962 zu einem hohen Teil Familien der Oberschicht und oberen Mittelschicht Kubas. Danach folgen aber auch viele Angehörige der Mittelschicht und der Arbeiterklasse.

Zu einer großen Emigrationswelle nach der Revolution kam es 1980, als über US-amerikanische Sender die Nachricht verbreitet wurde, dass die peruanische Botschaft in Havanna Visa für die Ausreise nach Peru ausstelle, mit denen eine Weiterreise in die USA möglich sei. In Anbetracht des Ansturms von zehntausenden Ausreisewilligen, die zum Teil seit langem über Pässe verfügten, forderte der peruanische Botschafter Polizeischutz an. Als eine Gruppe diesen Polizeischutz durchbrach, in der Botschaft politisches Asyl beantragte und von den Peruanern nicht ausgeliefert wurde, hob die kubanische Regierung die Abriegelung der peruanischen Botschaft auf. Die unhaltbaren Zustände auf dem Botschaftsgelände wurden am 17. April dadurch beendet, dass Fidel Castro in einer Rede die Möglichkeit eröffnete, auch ohne Visum mit dem Schiff vom Hafen Mariel aus in die USA auszureisen. Die Schiffe wurden bis zur 12-Meilen-Zone vor die US-amerikanische Küste eskortiert. Bis zum 31. Oktober 1980 verließen ca. 125.000 Kubaner das Land. In einer Rede anlässlich des 1. Mai 1980 bezeichnete Fidel Castro, unterstützt durch entsprechende Sprechchöre des Publikums, die Botschaftsflüchtlinge als arbeitsscheuen Abschaum. Juan Carlos Zaldívar verarbeitete die damaligen Ereignisse im Dokumentarfilm "90 Miles".

Um diese Einwanderungswelle zu beenden, schloss die US-Regierung unter dem Präsidenten Carter mit der kubanischen Regierung ein Abkommen, das die legale Einreise über festgelegte Quoten regeln sollte, aber von der nachfolgenden Reagan-Regierung nicht mehr eingehalten wurde.

Zur bislang letzten großen Auswanderungswelle kam es im August 1994. Am 5. August kam es aufgrund der schwierigen Versorgungssituation während der Spezialperiode, die im Sommer des Jahres 1994 ihren Höhepunkt erreichte, in Havanna zu den als Maleconazo bekannt gewordenen Unruhen. Zwar deeskalierte die Situation wieder relativ rasch, unter anderem weil der immer noch hochgeachtete und charismatische Regierungschef Fidel Castro persönlich erschien, um die Situation zu beruhigen, jedoch wies Castro am 7. August die Aufhebung der Küstenüberwachung an und löste damit erneut eine große Massenflucht aus Kuba aus, die auch als "Balsero-"(Flößer-)"Krise" bekannt ist und während der wohl mehr als 33.000 Kubaner in die USA flüchteten.

Die Vereinigten Staaten unter der Regierung von Bill Clinton handelten daraufhin mit Kuba ein Migrationsabkommen aus. Die USA erklärten sich bereit, jedes Jahr 20.000 Visa auszustellen, die eine legale Einwanderung ermöglichen. Im Gegenzug verpflichteten sich die USA, alle illegalen Flüchtlinge, die sie auf See aufgreifen, unverzüglich wieder nach Kuba abzuschieben "(wet feet, dry feet policy)". Die tatsächliche Zahl der ausgestellten Visa lag jedoch meist deutlich darunter. 2007 waren es 15.000.

Seit 1962 durften kubanische Auswanderer den größten Teil ihres Besitzes weder verkaufen noch ins Ausland mitnehmen, auch bei Überschreitung der genehmigten Dauer eines vorübergehenden Auslandsaufenthaltes wurde zurückgelassenes Eigentum verstaatlicht. Diese Bestimmungen erloschen mit Inkrafttreten des novellierten Migrationsgesetzes Mitte Januar 2013.

Insgesamt verließen über eine Million Kubaner seit der Revolution ihre Heimat.

Gegen das Ausscheren Kubas aus ihrer Hegemonie unterstützten die USA eine Gruppe von Exil-Kubanern, welche die neue Regierung militärisch beseitigen wollte. Der erfolglose Eingriff ist als Invasion in der Schweinebucht bekannt. In Folge wurde eine umfassende Wirtschafts-, Handels- und Finanzblockade seitens der USA gegen Kuba begonnen, die mit den Enteignungen gegen US-Bürger begründet wird. Mit dem 1992 erlassenen Torricelli Act wurde eine Verschärfung der Sanktionen eingeführt, gefolgt von dem 1996 in Kraft getretenen Helms-Burton Act.

Das Embargo wird von den Vereinten Nationen nicht gebilligt. Die UN-Generalversammlung verabschiedet seit 1992 jährlich eine Resolution, welche die Aufhebung aller Sanktionen gegen Kuba fordert – zuletzt im Oktober 2011: 186 Stimmen dafür, zwei Gegenstimmen (USA und Israel), drei Enthaltungen (die Marshall-Inseln, Mikronesien, und Palau).

Im Jahr 2000 wurde von US-Seite das Embargo hinsichtlich des Verbots des Nahrungsmittel- und Medikamentenexport durch den "Trade Sanctions Reform and Export Enhancement Act" "(Gesetz zur Reform der Handelssanktionen und Exportverbesserungen)" stark gelockert. Viele andere Handelsbeschränkungen blieben jedoch bestehen. Die bisher für Kuba durch die Blockade entstandenen Schäden werden von Kubas Regierung mit ca. 89 Mrd. US-Dollar angegeben. Von kubanischen Oppositionellen und anderen Kritikern der kubanischen Regierung wird die Wirkung des US-Handelsembargos jedoch stark bezweifelt. Es diene im Gegenteil nur als Vorwand, um die „völkerrechtswidrigen Verhältnisse“ zu rechtfertigen, deren Hauptursache in der „kollektiven Produktionsweise“ liege. Der Historiker Michael Zeuske geht davon aus, dass die kubanische Regierung, trotz der massiven wirtschaftlichen Schäden, nicht wirklich an einer Aufhebung des Embargos interessiert sei, sonst würde es wohl schon längst nicht mehr existieren. Tatsächlich sichere es durch eine Polarisierung des Nationalbewusstseins der kubanischen Bevölkerung bis heute das Überleben der Castro-Regierung. Auch Raúl Castro lehnt eine einseitige Schuldzuweisung der wirtschaftlichen Schwierigkeiten Kubas an die „Blockade“ ab. Vielmehr seien strukturelle Probleme der staatlichen Zentralwirtschaft dafür verantwortlich, wie er zum Beispiel im Dezember 2010 in einer Rede vor der Nationalversammlung anmerkte.

Trotz des Embargos sind die USA inzwischen ein wichtiger Handelspartner Kubas, bei den Importen liegen sie inzwischen an sechster Stelle. Der kubanische Staat importiert jährlich Nahrungs- und Futtermittel im Wert einer halben Milliarde Dollar aus den USA. Durch Geldsendungen exilkubanischer Gemeinden in den USA an ihre Familienangehörigen fließen der kubanischen Volkswirtschaft jährlich ca. eine Milliarde US-Dollar zu, was in etwa den Einnahmen der kubanischen Tourismusindustrie entspricht. Frühere, zuletzt von Präsident George W. Bush abgesenkte Obergrenzen für Geldsendungen von US-Bürgern an direkte Familienangehörige in Kuba wurden 2009 von Präsident Obama aufgehoben.
Die US-Regierung unterstützt auch Teile der Opposition in Kuba, so war für das Jahr 2006 15 Millionen US-Dollar im Haushalt für die Unterstützung von kubanischen Oppositionsgruppen und exilkubanischen Organisationen in Miami vorgesehen (Quelle: USAID Kuba-Programm), die zum Teil unmittelbar von der US-amerikanischen Interessenvertretung in Havanna an die Zielorganisationen ausgezahlt werden oder über die Exilorganisationen in Miami verteilt werden. Im Jahr 2014 wurde bekannt, dass die USA zwischen 2010 und 2012 mittels des Mikroblogging-Dienstes ZunZuneo versuchten, ein von der kubanischen Regierung nicht kontrolliertes Kommunikationsnetzwerk aufzubauen, das langfristig auch als Werkzeug zur Koordination regierungsfeindlicher Aktionen geplant war.

Im Dezember 2014 wurde eine neue Phase der bilateralen Beziehungen eingeleitet. Man vereinbarte einen Gefangenenaustausch unter anderem zwischen dem USAID-Mitarbeiter Alan Gross und den drei noch verbliebenen Miami Five. Des Weiteren wurde eine Neuaufnahme der diplomatischen Beziehungen angekündigt.

Ende Mai 2015 gab man bekannt, dass die USA in Havanna in Kürze eine Botschaft eröffnen werden. Kuba wird von der Liste der Terrorismus unterstützenden Staaten gestrichen, auf der es bislang stand. Damit entfallen zahlreiche Sanktionen gegen das Land. Am 20. Juli 2015 nahmen beide Länder wieder offizielle diplomatische Beziehungen auf. Die Botschaft der Vereinigten Staaten in Havanna wurde am 14. August 2015 offiziell wieder eröffnet. Seit dem 17. September hat Kuba mit dem bisherigen Leiter der kubanischen Interessenvertretung wieder offiziell einen Botschafter in den USA. 

Kuba steht in einem engen Bündnis mit dem vom verstorbenen Präsidenten Hugo Chávez geprägten Venezuela. Das Land liefert Öl unter Weltmarktpreisen an Kuba. Dafür schickt Kuba medizinisches Personal und Helfer für die Alphabetisierung nach Venezuela. 2006 wurden während der Operation Milagro tausende Venezolaner in Kuba operiert. Ein gemeinsames Projekt ist auch die Bolivarianische Alternative für Amerika (ALBA). Gute Beziehungen verbinden Kuba auch mit dem von Evo Morales regierten Bolivien und mit der Volksrepublik China. Am 29. April 2006 unterzeichneten die Präsidenten der Staaten Kuba, Venezuela und Bolivien den Handelsvertrag der Völker. Im Dezember 2008 trat Kuba der Rio-Gruppe bei. Kuba ist auch Mitglied der CELAC. Ebenfalls pflegt Kuba freundschaftliche Beziehungen mit Vietnam und Nordkorea, mit letzterem insbesondere auch auf militärischem Gebiet.

Kuba ist Mitglied der Bewegung der blockfreien Staaten.

Von Anfang an waren die kubanischen Revolutionäre internationalistisch und global ausgerichtet und wollten die Revolution auf möglichst viele andere Länder ausbreiten. Obwohl Kuba noch selbst ein Entwicklungsland war, engagierte sich die Regierung in afrikanischen, lateinamerikanischen und asiatischen Ländern auf militärischem, medizinischem und pädagogischem Gebiet. Ab Mitte der 1960er-Jahre rückte Afrika ins Zentrum der außenpolitischen Aktivitäten, wo afrikanische Revolutionäre wie Patrice Lumumba, Amilcar Cabral und Agostinho Neto (siehe auch Kubanischer Militäreinsatz in Angola) die Kubaner um Unterstützung für ihre Bewegungen baten. So unterstützten sie diplomatisch und mit militärischen Mitteln auch z. B. die südafrikanischen Befreiungstruppen beim Sturz des Apartheidregimes.

Trotz eigener wirtschaftlicher Probleme unterstützt Kuba andere Entwicklungsnationen insbesondere im medizinischen Bereich. Im Rahmen der "Operación Milagro" („Wunder“) werden Augenoperationen für Menschen aus Entwicklungsländern auf Kuba durchgeführt. Bis Mai 2009 wurden 24.000 ukrainische Kinder, Opfer des Atomunfalls in Tschernobyl, in Kuba kostenlos behandelt. Die Kosten dafür werden auf etwa 350 Millionen US-Dollar allein für die Medikamente geschätzt.
Im Auslandseinsatz waren bzw. sind kubanische Ärzte und Krankenschwestern in der Regel für zwei Jahre (unter Umständen auch per Jahresvertrag) tätig, vor allem in anderen lateinamerikanischen Ländern, u. a. in Haiti, Venezuela, Bolivien, Zentralamerika und seit 2013 – im Rahmen des Programmes „Mais Médicos“ (mehr Ärzte) zur Versorgung ländlicher Regionen – in Brasilien. Dazu kommen Einsätze zur Katastrophenhilfe, u. a. nach den Erdbeben 2005 in Kaschmir und 2008 in Pakistan. Nach der Erdbebenkatastrophe von 2010 und zur Bekämpfung der Cholera-Epidemie wurden rund 1200 kubanische Mediziner und Helfer nach Haiti entsandt.
Im Oktober 2014 schickte Kuba 165 Ärzte und Krankenpfleger nach Sierra Leone, um die Ebola-Seuche zu bekämpfen. Jedoch wird die Qualität der kubanischen Massenausbildung an Medizinern international zunehmend infrage gestellt.

Kubanische Auslandsengagements stellen eine wichtige Quelle für Deviseneinnahmen dar. Die jährlichen Einnahmen werden auf rund 4,6 Milliarden US-Dollar geschätzt. Normalerweise verlangt Kuba für einen im Ausland tätigen Arzt vom Gastgeberland rund 2.500 Dollar pro Monat; Brasilien zahlt rund 4.000 Dollar monatlich. Die Löhne der kubanischen Beschäftigten werden unmittelbar an die kubanische Regierung überwiesen, die bis zu 93 % der Zahlungen einbehält, so eine Studie der Ärzteorganisation „Solidaridad Sin Fronteras“ (Solidarität ohne Grenzen). In Brasilien verbleiben den kubanischen Ärzten und Pflegern gut 10 % ihres Lohnes. Infolgedessen „desertieren“ mehr und mehr kubanische Mediziner in Brasilien und vor allem aus Venezuela, wo mehr als 10.000 von ihnen eingesetzt sind (Stand 2015). Hunderte, wenn nicht mehr als 1000 von ihnen sind aus dem Dienst in Venezuela nach Kolumbien geflüchtet.

Haiti sei einer der wenigen Staaten, die für Kubas medizinische Dienstleistungen nicht bezahlen müssen. Insofern die kubanischen Ärzte in Haiti durchweg in von ausländischen Hilfswerken finanzierten Projekten arbeiten, sind es dort jene Hilfswerke, die für die Kosten aufkommen.

Kubas medizinisches Hilfsprogramm wurde 2015 von John Kirk Professor für lateinamerikanische Studien der Dalhousie University Halifax in Kanada für den Friedensnobelpreis vorgeschlagen.

Bildung ist in Kuba kostenlos und es besteht eine 9-jährige Schulpflicht. Kuba hat ein dreigeteiltes Bildungssystem, das aus Grund-, Mittel-, und Oberschule besteht.
Kubas Bildungssystem gehört zu den besten in Lateinamerika und dies sowohl vor als auch nach der Revolution. 2001 lagen die kubanischen Schüler der vierten und fünften Klasse bei einem Test der UNESCO weit vor den anderen lateinamerikanischen Ländern. Die Einschulungsquote liegt bei 100 Prozent, Analphabetismus geht gegen null. Nach dem "UNESCO-Education for All Development Index" gehört Kuba zu den hochentwickelten Ländern der Welt im Bildungsbereich mit einer gut ausgebildeten Bevölkerung.

In den letzten Jahren herrscht jedoch ein immer akuter werdender Lehrermangel. Viele Lehrer arbeiten, trotz ihrer guten Ausbildung, genauso wie zahlreiche Ärzte und andere Hochqualifizierte, lieber im Tourismussektor, weil allein das Trinkgeld ein Vielfaches eines kubanischen Gehalts beträgt. Auch verleiht Kuba viele Lehrer, als Ausgleich für verbilligtes Öl aus Venezuela, an verschiedene befreundete Staaten Lateinamerikas, um dort beim Aufbau eines funktionierenden Bildungssystems zu helfen. Diesen Lehrermangel versucht die kubanische Regierung mit sogenannten „Nothilfelehrern“, 16- bis 18-jährigen Schulabgängern, die in Schnellkursen auf ihre Aufgaben vorbereitet werden, und durch Teleklassen, also Unterricht per Videokassette, zu kompensieren. Außerdem sollen schon pensionierte Lehrer wieder in den aktiven Schuldienst gelockt werden. Der Anteil der jungen Notstandslehrer sei inzwischen auf knapp 50 Prozent gestiegen, was einen qualifizierten Unterricht nahezu unmöglich mache. Dennoch gibt es auch in jüngster Zeit immer wieder Ansätze, das Bildungssystem zu erhalten und effizienter zu gestalten. 
Durch eine Initiative zur Förderung der Kultur werden im Zeitraum 2011–2012 mehr als zwei Millionen Schüler Theater-, Musik-, Zeichen- und anderen künstlerischen Unterricht erhalten. Zudem gab es in den letzten Jahren Lohnerhöhungen für die Lehrkräfte des Landes.

Das Schulwesen steht für Jungen auch im Dienst vormilitärischer Ausbildung, ältere Schüler lernen den Umgang mit Waffen. Die Lehrer müssen jährlich jeden Schüler und auch dessen Eltern nach der politischen Ausrichtung und den politischen Aktivitäten schriftlich beurteilen.

Das Studium auf Kuba ist kostenlos, allerdings müssen alle Studenten nach ihrem Abschluss drei Jahre lang für den Staat einen Sozialdienst ableisten. In Kuba ist der Frauenanteil unter den Studenten höher als in jedem anderen lateinamerikanischen Land. Ebenso schneiden kubanische Studenten in den Bereichen Mathematik, Naturwissenschaften und Sprachen besser ab als ihre Kommilitonen in Lateinamerika.

Teil des kubanischen Bildungswesens ist auch, dass Schüler und Studenten regelmäßig in Landinternate geschickt werden, wo sie neben ihrer Ausbildung unbezahlt in der Landwirtschaft arbeiten.

Der kubanische Staat garantiert jedem kubanischen Bürger eine medizinische Versorgung. Die medizinische Behandlung ist für Kubaner grundsätzlich kostenlos, für Medikamente aus der Apotheke müssen die Patienten eine Zuzahlung leisten. Viele Arzneimittel sind nur gegen Dollar erhältlich.

Das kubanische Gesundheitssystem zeichnet sich durch eine gute Vorsorge, eine hohe Ärztedichte (theoretisch 160 Einwohner je Arzt, ein Drittel davon ist jedoch im Ausland tätig) und eine hohe Integration (Polikliniken) aus. Jede Siedlung verfügt über einen sogenannten „Familienarzt“. Familienärzte residieren in Gebäuden, die im gesamten Land einem identischen Bauplan folgen. In diesen befinden sich sowohl die Praxis als auch die Wohnung des Arztes, was eine Verfügbarkeit von 24 Stunden gewährleisten soll. 

Die Säuglingssterblichkeit ist eine der niedrigsten (2010, 4,5 Säuglinge pro 1000 Geburten) und die Lebenserwartung eine der höchsten auf dem gesamten amerikanischen Kontinent. Laut dem kubanischen Arzt und Dissidenten Darsi Ferrer wird diese Zahl allerdings durch eine außerordentlich hohe Zahl von Abtreibungen von Risikoschwangerschaften erreicht. 99,9 % der kubanischen Kinder werden in Einrichtungen des öffentlichen Gesundheitssystems geboren. Nach Angaben von UNICEF entspricht die Abdeckung und Qualität von kinder- und mütterfreundlichen Krankenhäusern in Kuba den weltweit höchsten Standards. Die UN-Kinderrechtskonvention ist laut dem UNICEF-Vertreter für Kuba, José Juan Ortiz Brú, in diesem Land am besten umgesetzt.

Nach einem Bericht der WHO aus dem Jahr 2012 gehört Kuba zu den Ländern mit der weltweit niedrigsten Tuberkuloserate, es kommen 7 Fälle auf 100.000 Einwohner. Auch wurde weiter in den Arbeitsschutz investiert. So sank die Anzahl der Arbeitsunfälle pro 1.000 Arbeiter in Kuba von 5,2 im Jahr 1999 auf 1,6 im Jahr 2011 (Deutschland: 25,8 pro 1.000 Arbeiter). Laut einem Ranking der NGO "Save the Children" ist Kuba das lateinamerikanische Land, das Müttern die besten Bedingungen bietet. Die Studie berücksichtigte Faktoren wie die allgemeinen Bedingungen des Gesundheitswesens, das Bildungsniveau sowie den wirtschaftlichen und politischen Status der Mütter. Des Weiteren wurde der Wohlstand der Kinder beachtet, die Sterblichkeitsrate unter fünf Jahren und der prozentuale Anteil der unterernährten Kinder.

Jedoch gibt es Probleme: Zwar hat Kuba theoretisch eine der höchsten Ärztedichten der WeltViele medizinische Einrichtungen sind baufällig und die medizinischen Geräte oft veraltet und in schlechtem Zustand. Auch fehlen häufig wichtige Medikamente, und die hygienischen Verhältnisse lassen zu wünschen übrig. Es kommt in den Polikliniken zu langen Wartezeiten, weil etwa 40.000 Ärzte im Ausland arbeiten und dem Staat damit pro Jahr 6 Milliarden Euro bringen. Die Zahl der Familienärzte sank zwischen 2009 und 2014 um 62 %, von über 32.000 auf unter 13.000. Die Ärzte sind nicht höher bezahlt als andere Arbeiter und Angestellte und erhalten nur einen Bruchteil dessen, was sie das Ausland kosten, als Lohn ausbezahlt. Ein verlässlicher Rettungsdienst existiert nicht. Außerdem mangelt es an Medikamenten wie Antibiotika und an medizintechnischer Ausrüstung in Chirurgie und Zahnmedizin. Vorhandene medizinische Infrastruktur ist nur bedingt nutzbar. In der Ausbildung der Ärzte an moderner Hitech-Medizin gibt es große Defizite.
Die "Revolutionären Streitkräfte Kubas" "(Fuerzas Armadas Revolucionarias – FAR)" umfassen heutzutage ca. 49.000 Mann. Es besteht Wehrpflicht für Männer. Die Zahl der Angehörigen der regulären Streitkräfte ist seit dem Ende des Kalten Krieges stark gesunken. Damals betrug deren Stärke rund 300.000 Mann. Allein im Angola-Konflikt wurden 430.000 kubanische Soldaten eingesetzt. Kein anderes lateinamerikanisches Land engagierte sich militärisch derart stark außerhalb des eigenen Kontinents.

Weiterhin gibt es die rund eine Million Mann starken paramilitärischen "Milizen zur Territorialverteidigung" "(MTT – Milicias de Tropas Territoriales)". Deren Angehörige sind Zivilisten und haben in ihren Wohn- und Arbeitsgebieten Zugang zu Waffen. Sie sind für einen Guerillakrieg gegen mögliche Invasoren ausgebildet und bilden in Kriegszeiten einen Teil der militärischen Streitkräfte, mit der Aufgabe die gegnerischen Kräfte zu binden und damit den Einheiten der regulären Armee Zeit zur Mobilmachung zu geben.

Der Armee untersteht ebenfalls die Zivilverteidigung. Eigentlich zur Organisation der Bevölkerung im Verteidigungsfall eingerichtet, bestehen die heutigen Hauptaufgaben darin, die Bevölkerung vor den Folgen von Naturereignissen, insbesondere den jährlich auftretenden Hurrikanen zu schützen. Dies geschieht sehr effizient, so dass trotz teilweise immenser Sachschäden normalerweise kaum Menschen zu Schaden kommen.

Kuba zählte vor der Revolution, gemäß Pro-Kopf-BIP, zu den reichsten Ländern Lateinamerikas.
Seit den 1870er Jahren waren die Einkommen unter den höchsten in Südamerika. Seine Infrastruktur, wie zum Beispiel das Verkehrs- und Telekommunikationsnetz, war auf dem modernsten Stand. Auch das Gesundheits- und Schulwesen konnte sich mit den Staaten der Ersten Welt messen. Kuba war der weltweit größte Exporteur von Zucker, und die Vereinigten Staaten kauften jährlich eine große und garantierte Menge Zucker zu festgesetzten Preisen auf. Jedoch herrschten riesige Ungleichgewichte hinsichtlich der Verteilung des Volksvermögens sowohl zwischen den sozialen Schichten als auch zwischen Stadt und Land, insbesondere zwischen der Hauptstadt Havanna und den östlichsten Teilen des Landes. Der Einfluss von US-Direktinvestoren auf die kubanische Volkswirtschaft war zwar nach wie vor recht groß, jedoch stetig rückläufig.

Trotz der widrigsten äußeren Umstände sind Kubas wirtschaftliche Probleme in erster Linie inneren Entwicklungsblockaden geschuldet.

Heute ist Kuba eine der letzten bestehenden sozialistischen Volkswirtschaften. Nach dem Ende der Sowjetunion kam es mit dem Wegfall des wichtigsten Handelspartners Kubas 1991 zu einer ökonomischen Krise (genannt "período especial en tiempo de paz" = besondere Periode in Friedenszeiten; kurz: "período especial"/Sonderperiode), die bis heute andauert. Die RGW-Staaten hatten Kubas landwirtschaftliche Produkte über dem Marktpreis gekauft und Finanzhilfen geleistet, allein die Sowjetunion zahlte zuletzt 5 Milliarden Dollar jährlich.

Wegen der großen wirtschaftlichen Schwierigkeiten wurde der US-Dollar ab 1993 offizielles Zahlungsmittel neben dem Peso. Seit dem 8. November 2004 ist der US-Dollar durch den Peso Convertible ersetzt.

Die desolate Wirtschaftslage zwang die Regierung zu marktwirtschaftlichen Reformen, um die Grundversorgung der Bevölkerung sicherzustellen. Es entstand neben der Planwirtschaft ein zweiter Wirtschaftsbereich mit marktwirtschaftlichen Elementen. Erstmals wurden Familien- und Einpersonenbetriebe ("trabajo de cuenta propia" – Arbeit auf eigene Rechnung) zugelassen, einige Staatsbetriebe wurden nach betriebswirtschaftlichen Erkenntnissen geführt und Bauern durften einen Teil ihrer produzierten Waren selbst verkaufen. Später wurden diese vorsichtigen Reformen Richtung Marktwirtschaft zwar nicht vollständig rückgängig gemacht, jedoch wurde die Vergabe von Lizenzen deutlich restriktiver gehandhabt. Auch viele bestehende Familienbetriebe konnten die zunehmend restriktiveren Auflagen nicht mehr erfüllen und mussten schließen.

Zur Nutzung ausländischen Investitionskapitals wurden Joint-Ventures mit kubanischen Staatsunternehmen gegründet, wobei letztere ihrerseits aufgrund der strategischen Wichtigkeit vom Militär kontrolliert werden. Die Joint-Ventures mit ausländischen Firmen unterliegen Beschränkungen. Sie dürfen ihre kubanischen Mitarbeiter nicht selbst aussuchen und müssen deren Lohn in Dollar an die Regierung zahlen. Die Mitarbeiter erhalten den normalen kubanischen Lohn in Pesos. Ein Großteil des Lohnes wird so abgeführt.

Im September 2010 kündigte die kubanische Regierung umfassende Reformen an, um mit einer graduellen Ausweitung von Marktmechanismen und selbständiger Arbeit den strukturellen Wirtschaftsproblemen zu begegnen. Dieser von Raúl Castro als alternativlos dargestellte Kurs, der an die Reformpolitik Chinas und Vietnams erinnert, wurde von der Nationalversammlung im Dezember 2010 bekräftigt. Die geplanten Maßnahmen umfassen unter anderem die Entlassung von 500.000 Staatsbediensteten, mehr als zehn Prozent des im Staatssektor beschäftigten Personals, bis März 2011. Arbeitslosengeld in Höhe von bis zu 60 % des Basismonatslohns gibt es nur für langjährig Beschäftigte, jedoch je nach Beschäftigungsdauer maximal fünf Monate. Insgesamt gebe es laut Raúl Castro beim Staat einen Überhang von gut einer Million Beschäftigten. Dennoch fehlen insbesondere in der Landwirtschaft, im Bauwesen und in der Industrie zahlreiche Arbeiter. Auch bei den Akademikern gebe es Fehlentwicklungen. Es wurde zu viel gegen den volkswirtschaftlichen Bedarf ausgebildet, was nun korrigiert werden müsse. Der Zugang zu Universitäten soll erschwert, das Niveau des Hochschulstudiums angehoben werden. Jedoch wird der Mangel an qualifizierten Lehrern beklagt, um Fachkräfte bedarfsgerecht auszubilden. Weiterhin hofft die Regierung, dass zahlreiche der Entlassenen nun in der Privatwirtschaft Anstellung finden. Dazu wurden die Bedingungen für das Arbeiten auf eigene Rechnung gelockert – es dürfen jetzt auch familienfremde Angestellte beschäftigt werden – und die möglichen Branchen auf zum Beispiel Schönheitssalons und Friseure erweitert. Mit Genehmigung der Regierung haben sich bis Mitte 2011 rund 310.000 Beschäftigte selbständig gemacht, die meisten davon in Lebensmittelproduktion und -verkauf. Während die offizielle Arbeitslosenrate bei rund 2,5 Prozent liegt, schätzen selbst regierungsnahe Gewerkschafter, dass die tatsächliche Erwerbslosenquote beim Zehnfachen, nämlich bei rund 25 Prozent liegen dürfte. Während der halbjährlich stattfindenden Parlamentssitzung im Juli 2014 zeigte sich die Regierung jedoch enttäuscht von den bisherigen Ergebnissen. Das Wirtschaftswachstum hatte nicht ihren Erwartungen entsprochen.

An den geplanten Wirtschaftsreformen gibt es von Seiten der Experten zahlreiche Bedenken. Zum einen wird bezweifelt, dass diese nur halbherzige Öffnung in Richtung Marktwirtschaft bei möglichst gleichbleibender zentralstaatlicher Kontrolle auf Dauer funktioniert. Außerdem steht der geplanten Freisetzung von bis zu 50 Prozent der staatlichen Arbeitsplätze kein adäquates Angebot im Privatsektor gegenüber, in dem sich die entlassenen Arbeiter und Angestellten eine neue Beschäftigung suchen sollen. Dort sind nämlich bisher nur rund 180 relativ einfache Betätigungsfelder erlaubt, sodass dort Männer wie Frauen zum großen Teil weit unter ihrer Qualifikation arbeiten müssen.

Kuba befindet sich seit etwa 2009 in einer extremen Wirtschaftskrise, bedingt durch die Hurrikansaison 2008 und Kubas ineffizienter Wirtschaft. Im Unterschied zu früher werden seit dem Amtsantritt von Raúl Castro auch in offiziellen Diskursen der kubanischen Regierung, insbesondere vom Regierungschef selber, nicht mehr externe Umstände wie US-Blockade oder ungünstiger Weltmarkt als Hauptursache der wirtschaftlichen Probleme genannt, sondern es wird mehr auf strukturelle Probleme der zentral gelenkten Staatswirtschaft verwiesen. Vor allem gelte es, Misswirtschaft und Korruption in den staatlichen Betrieben zu bekämpfen.

Kuba ist auf Betreiben der USA aus dem von IWF und Weltbank beherrschten internationalen Finanzsystem praktisch ausgeschlossen. Auch ein Kooperationsabkommen mit der EU ist bisher nicht zustande gekommen. Kuba hatte 1999 entsprechende Verhandlungen einseitig abgebrochen. Dennoch blieb die EU zunächst einer der wichtigsten Handelspartner Kubas. Im Jahre 2000 stammten mehr als die Hälfte sowohl der Direktinvestitionen als auch der Importe von EU-Ländern. Inzwischen sind Venezuela und China die wichtigsten Handelspartner und Kreditgeber Kubas.

In Kuba gibt es zwei offizielle Währungen, der Peso Cubano (CUP oder MN für "Moneda Nacional") als die ursprüngliche Währung, in der die staatlichen Löhne ausgezahlt und die wesentlichen einheimischen Grundnahrungsmittel und einfachen Dienstleistungen bezahlt werden, sowie den Peso convertible (CUC), der als Ersatz-Devisenwährung direkt an den Wert des US-Dollars gekoppelt ist und insbesondere für importierte Waren und höherwertige Dienstleistungen erforderlich ist. Seit dessen Einführung nimmt die Zahl der Artikel des täglichen Gebrauchs zu, die nur noch in CUC und damit zu Preisen verkauft werden, die für Verbraucher ohne direkten Zugang zu Devisen schwer erschwinglich sind.<ref name="taz/ökonomische Apartheid">Knut Henkel: "Gegen Kubas ökonomische Apartheid." In: "die tageszeitung", 27. Februar 2009.</ref> Die Nachfrage nach Waren des täglichen Bedarfs für nationale Währung übersteigt auch nach offiziellen Angaben deutlich das Angebot.

Das seit März 2005 staatlich festgelegte Tauschverhältnis ist 1:24 beim Kauf von kubanischen Pesos für CUC und umgekehrt 25:1 wenn man kubanische Pesos in Pesos convertibles eintauschen will. In der volkswirtschaftlichen Gesamtrechnung wird jedoch eine Relation von 1:1 zwischen beiden nationalen Währungen angesetzt. Ende Juli 2013 kündigte Raúl Castro an, die beiden Währungen zusammenzufügen, da deren Dualität die Wirtschaftsreformen behindere. Das Kabinett hat am 22. Oktober 2013 dazu einen Zeitplan für einen Übergangsprozess gebilligt.

Das Wachstum des Bruttoinlandsproduktes (BIP) erholte sich seit der Wirtschaftskrise von 1993 (0,7 Prozent) auf drei Prozent im Jahre 2004. Nach staatlichen Angaben wuchs die Wirtschaft im Jahre 2005 um 11,8 Prozent (Schätzung der CEPAL: 3 %), im Jahr 2006 um 12,5 %. Für das Jahr 2007 gab das kubanische Wirtschaftsministerium ein Wachstum von 7,5 % an, für 2008 werden 8 % prognostiziert. Die offiziellen Zahlen sind für Vergleiche mit anderen Ländern ungeeignet, da Kuba zur Berechnung des BIP eine eigene, international nicht anerkannte, Berechnungsmethode, das „PIP Social Sostenible“ (Nachhaltiges Soziales BIP), anwendet, das freie oder stark subventionierte Leistungen des Staates besonders mit einrechnet. Andere Quellen schätzen das Wirtschaftswachstum im Jahr 2006 geringer ein (7,6 %, 8 % und 9,5 %).

Die Produktion ist bis 2009 auf 48 % des Wertes von 1989 gesunken. Kubas Außenhandelsbilanz ist stark negativ, das Land muss mehr Güter importieren, als es exportieren kann. Im ersten Quartal 2009 entfielen insgesamt 80 Prozent des Außenhandels auf Importe. Die Auslandsverschuldung und das Handelsdefizit sind 2009 die höchsten Lateinamerikas. Verbindlichkeiten bei ausländischen Staaten und Investoren können nur teilweise bedient werden.

Letztendlich dürften die hohen offiziellen Wachstumsraten seit der Jahrtausendwende hauptsächlich den hohen Subventionen aus Venezuela und dem bis 2008 hohen Nickelpreis geschuldet sein. Im privaten Konsum der Kubaner kam das Wirtschaftswachstum jedoch kaum an.
Derzeit arbeitet Kuba an der Überarbeitung der Statistik, um in Zukunft vergleichbare Daten liefern zu können.

Inzwischen gewinnt die Nickelproduktion an Bedeutung, hier wirken sich die aktuell hohen Stahlpreise günstig aus. Außerdem werden folgende Rohstoffe in größeren oder kleineren Mengen abgebaut: Chrom, Kobalt, Kupfer, Eisen, Mangan, Gold und Silber sowie geringe Mengen an Erdöl und Erdgas.
Nach Schätzungen der staatlichen Ölgesellschaft CUPET verfügt Kuba vor seinen Küsten über Ölvorkommen von bis zu 20 Milliarden Barrel, was ungefähr den noch vorhandenen Reserven der USA entspricht und fast das Doppelte der Reserven Mexikos ausmacht. Der Geologische Dienst der USA schätzt Kubas Ölreserven auf rund 9 Milliarden Barrel sowie rund 60 Milliarden Kubikmeter Erdgas. Trotz beträchtlicher Investitionen – darunter 2012 in den Einsatz einer Bohrinsel mit über 3,6 Kilometern Bohrtiefe – haben bisherige Probebohrungen verschiedener ausländischer Ölfördergesellschaften noch keine Möglichkeit einer rentablen Förderung des Öls ergeben, weswegen nun wieder verstärkt in die Förderung auf dem Festland investiert wird. Die geschätzte Erdölproduktion lag 2014 unter 30 Prozent des Verbrauchs.

In der Landwirtschaft ist der Zucker immer noch das wichtigste Exportgut, gefolgt vom Tabak. Im Jahr 2000 exportierte Kuba 2,9 Mio. Tonnen Zucker, von denen die Hauptabnehmer Russland mit 42 %, die westlichen Industriestaaten mit 31 % und China mit 9 % waren. Die Zuckerproduktion sank jedoch von 9 Millionen Tonnen 1987 auf 2,5 Millionen Tonnen 2006. 2010 hatte Kuba die schlechteste Zuckerrohrernte seit mehr als 100 Jahren, es wurde etwa eine Million Tonnen Zucker produziert. Theoretisch ist Kuba ein fruchtbares Land, wo dreimal jährlich geerntet werden könnte. Die Geographie des Landes mit vorwiegend flachem oder hügeligem Land und günstiger Bodenbeschaffenheit bietet fast ideale Bedingungen. Viel Land liegt jedoch brach und Kuba importiert mehr als die Hälfte seiner Lebensmittel, in gewissen Jahren sogar Zucker aus Brasilien. 
Kuba gab jährlich bis 2,5 Milliarden US-Dollar für den Lebensmittelimport aus. Im 2008 mussten 84 % der Lebensmittel importiert werden, auch rund 80 % der Grundnahrungsmittel im Wert von ca. einer Milliarde Dollar, welche über das Libreta-System für rationierte und subventionierte Waren verteilt werden, darunter Reis, Kartoffeln, Bohnen und Fleisch.

Die vor dem Zusammenbruch der Sowjetunion und dem damit einhergehenden Ausfall von Treibstoffen und finanziellen Mitteln hochmechanisierte und mit chemischer Unterstützung arbeitende zentralistisch gesteuerte industrielle Landwirtschaft musste sich Anfang der 1990er Jahre komplett umorientieren. Es fehlte plötzlich sowohl an Treibstoff für die Landmaschinen als auch an Düngemitteln und Pestiziden. Die staatliche Produktion landwirtschaftlicher Güter brach ein. Der Not gehorchend, bildete sich zunächst eine zunehmend besser funktionierende, auf privater Basis arbeitende urbane Landwirtschaft heraus. Sie versorgte 80 % der Bevölkerung mit weitgehend lokal erzeugten Bioprodukten und machte Kuba damit unbeabsichtigt zum weltweiten Führer der Ökologischen Landwirtschaft. Die 1992 gegründete Asociación Cubana de Agricultura Orgánica (ACAO – deutsch: Kubanische Vereinigung für Organische Landwirtschaft) wurde 1999 für ihre dementsprechende Pionierarbeit mit dem als "Alternativen Nobelpreis" bekannten Right Livelihood Award ausgezeichnet. Wenige Monate später ließ die kubanische Regierung die inzwischen 30.000 Personen umfassende ACAO verbieten. Die weitgehend ökologische Landwirtschaft blieb jedoch bis heute bestehen und könnte als Vorbild für die Anpassung der Landwirtschaft in anderen Ländern mit Erdölknappheit dienen.

Das randtropische Klima sorgt für gute Voraussetzungen, bereitet allerdings auch erhebliche Probleme: Durch die vermehrt auftretenden Hurrikane mit hoher Intensität und durch die immer wieder vorkommenden Dürreperioden werden oft große Teile der Ernte vernichtet. Die Nahrungsmittelproduktion Kubas war insgesamt von 2001 bis 2007 rückläufig. Die Geflügelproduktion beispielsweise hatte sich nach der Überwindung der Hauptschwierigkeiten der Sonderperiode fast halbiert. Nach einem Minus von 6 % im Jahr 2006 konnte sich der Landwirtschaftssektor aber im Jahr 2007 wieder erholen, er war im Jahr 2007 mit einem Wachstum von 22,4 % der am stärksten gewachsene Wirtschaftssektor Kubas bei einem Gesamtwirtschaftswachstum von 7 %. Dies war offensichtlich auf die ergriffenen Maßnahmen zur Reduzierung des Zahlungsrückstandes des Staates gegenüber den Erzeugern, die Anhebung der Abnahmepreise für deren Produkte und günstigen klimatischen Bedingungen zurückzuführen.

Von den 3,5 Millionen Hektar Land wird rund die Hälfte nicht oder mangelhaft genutzt. Nur 32 Prozent der Flächen werden von Kooperativen bearbeitet, der Rest von privaten Bauern. Rund 900.000 Menschen arbeiten als Bauern oder in Kooperativen und es gab um 2015 nur einen Traktor auf fast 15 Beschäftigte. Neben meist 30-jährigen Traktoren kommen auch noch 2016 Ochsen und Pferde zum Einsatz.
Um die landwirtschaftliche Produktion anzukurbeln und die Abhängigkeit von den teuren Einfuhren zu mindern, wurden seit September 2008 ungenutzte landwirtschaftliche Flächen an landlose Arbeiter und Bauern vergeben. Für Privatleute gelten die Pachtverträge über zehn Jahre und für Kooperativen 25 Jahre. Die Nutzungsrechte können weder vererbt, noch verkauft werden. Kubanische Experten halten die bisher durchgeführten punktuellen Reformen für unzureichend und fordern stattdessen strukturelle Reformen in Richtung mehr Marktwirtschaft. Die landwirtschaftliche Produktion konnte bis 2012 nicht wesentlich gesteigert und die Importabhängigkeit nicht verringert werden.

Im Jahr 2011 wuchs die landwirtschaftliche Produktion (ohne die Zuckerindustrie) um 8,7 %, nach einem Rückgang um 2,5 % im Vorjahr, lag aber weiterhin unter dem Niveau von 2005. Der Plan für die urbane Landwirtschaft wurde mit 105 % übererfüllt. Es wurden 1.052.000 Tonnen Gemüse geerntet. Für 2012 war eine Produktion von 1.055.000 Tonnen geplant.

Wohl mehr als die Hälfte der landwirtschaftlichen Anbaufläche lagen um jene Zeit brach. Das bedeutete auch, dass bis 85 Prozent der Nahrungsmittel eingeführt werden mussten, oft aus den USA.

Bis zum Jahr 2016 hatte, trotz der seit acht Jahren gewährten Erlaubnis für Kleinbauern, Ackerland zu pachten, die Lebensmittelproduktion kaum zugenommen. Den Bauern fehlte es an Saatgut, Maschinen und Dünger um die brach liegenden Flächen zu bearbeiten. Gut 70 Prozent alleine der Grundnahrungsmittel wurden immer noch importiert.

Der Beginn des Massentourismus auf Kuba wurde Anfang der 1920er-Jahre durch die Prohibition in den Vereinigten Staaten ausgelöst. Kuba wurde ein beliebtes Reiseziel der US-Amerikaner, da es nah an Florida lag und keinen Beschränkungen des Glücksspiels und der Prohibition wie in den USA unterlag.

Nach dem Sieg der Revolution 1959 reisten in den folgenden dreißig Jahren nur eine geringere Zahl von Gästen, besonders aus der Sowjetunion und den Ostblockstaaten, nach Kuba. Aufgrund des Embargos ist US-Bürgern, die vor der Revolution den Großteil der Besucher ausmachten, Tourismus in Kuba verboten. Viele US-Amerikaner umgehen dieses Verbot, indem sie über Drittländer nach Kuba reisen. Es gibt nur sehr wenige direkte Verkehrsverbindungen zwischen den USA und Kuba, die hauptsächlich von Exilkubanern für Verwandtenbesuche benutzt werden, die jedoch ebenfalls reglementiert sind.

Nach der Auflösung des Ostblocks und der wirtschaftlichen Krise in Kuba suchte die Regierung neue Devisenquellen für Kuba. Mit Hilfe international tätiger Tourismusunternehmen wurden seit Anfang der 1990er-Jahre Joint-Ventures gegründet, die Hotels und touristische Einrichtungen hauptsächlich in den Haupttourismusgebieten errichteten und betreiben. Die hohen Trinkgelder in Devisen lockten viele hochqualifizierte Kubaner in Jobs des Tourismusgewerbes. Touristikmitarbeiter werden auch speziell an Universitäten des Landes mit eigens eingerichteten Studiengängen ausgebildet.

Der heutige Pauschaltourismus konzentriert sich auf wenige Gebiete, insbesondere Varadero, die Region Havanna, das Valle de Viñales, Cayo Coco und die Nordküste bei Holguín (Playa Guardalavaca).
Durch die hohe Anzahl an Ärzten und ein entwickeltes Gesundheitssystem bietet Kuba gute Voraussetzungen für Gesundheitstourismus. Touristen verbinden ihren Ferienaufenthalt mit einer medizinischen Behandlung oder Reisen für Spezialbehandlungen wie Augenoperationen und Zahnarztbehandlungen nach Kuba.

Heute hat der Tourismus eine Spitzenstellung in der Wirtschaft des Landes bekommen und ist die wichtigste Einnahmequelle für Devisen geworden. Um die zuletzt sinkenden Touristenzahlen wieder zu steigern, wurden verschiedene Maßnahmen, wie Senkung der Landegebühren auf den Flughäfen, Senkung der Kerosinpreise auf Weltmarktniveau sowie eine schnellere Abfertigung der Touristen bei der Einreise beschlossen.

Im Jahr 2010 stieg die Zahl der ausländischen Besucher gegenüber dem Vorjahr um 4 % auf 2,5 Millionen (2009 2,4 Mio.) Touristen. Den mit Abstand größten Anteil unter den Touristen stellen die Kanadier mit 945.000 Besuchern im Jahre 2010. Danach kommen offensichtlich – von der offiziellen Statistik nicht separat ausgewiesen – Reisende aus den USA, zumeist Kubanoamerikaner auf Familienbesuch, mit rund 400.000 Besuchern, die höchste Zahl seit dem Sieg der Revolution 1959.

Kubas Industrie ist international überwiegend nicht wettbewerbsfähig. Der Bedarf an Industriegütern kann nicht durch eigene Produktion gedeckt werden. Die Industrieproduktion war 2006 nur halb so groß wie 1989.

Kuba verfügt über eine hochentwickelte Biotechnologie, die z. B. in der Landwirtschaft aus Mangel an Energie sowie synthetischen Düngern und Pflanzenbehandlungsmitteln biologische Anbaumethoden fördert. Die kubanische Pharmaindustrie vermarktet weltweit zahlreiche kubanische Patente auf Medikamente. Kuba zählt zu den ersten Ländern, in denen Impfstoffe gegen Meningitis B und C, Hepatitis B, ein therapeutischer Impfstoff gegen Lungenkrebs und ein Medikament für die Behandlung von Geschwüren des Diabetikerfußes entwickelt wurden. Medizinische Produkte sind mit einem Volumen von 350 Millionen US-Dollar (2007) zum zweitwichtigsten Exportgut Kubas geworden.

Weiterhin existiert eine moderne Produktionsstätte für Solarmodule.

Die wichtigsten Handelspartner sind Venezuela und China.

Die Löhne und vor allem die Renten gelten für die Masse der Kubaner als sehr gering, so dass die meisten sich bemühen müssen, im informellen Sektor etwas dazu zu verdienen oder aus der Produktion ihrer Betriebe zu stehlen. Innerkubanischen Berechnungen zufolge benötigte eine kubanische Durchschnittsfamilie im Jahr 2002 rund das Doppelte ihres regulären Einkommens zum Überleben. Auch Kubas Präsident Raúl Castro bemerkte 2007 in einer Rede, dass das Gehalt eines Kubaners klar unzureichend sei, um sämtliche Notwendigkeiten des täglichen Lebens zu erfüllen. Das durchschnittliche Monatseinkommen für Berufstätige stieg von 2011 bis 2016 gemäß offiziellen Angaben von 455 Pesos auf 640 Pesos pro Monat, d. h. von rund 19 US-Dollar auf 26 US-Dollar. Dabei profitierte vor allem medizinisches Personal von kräftigen Lohnerhöhungen. Die Mindestrente für Berufstätige betrug 2005 etwa 150 Pesos (ca. 7 US-Dollar) je Monat. Kubaner, die nicht von regelmäßigen Dollar-Überweisungen ihrer Verwandten aus dem Ausland profitieren, was auf mehr als die Hälfte der Bevölkerung zutrifft, sind von Armut bedroht.<ref name="Giga 2/2015">Bert Hoffmann: "Kuba-USA: Wandel durch Annäherung", GIGA Focus Lateinamerika 2/2015</ref>

Es existiert eine Art Bezugsscheinsystem, Libreta genannt, das den rationierten Bezug von subventionierten Waren, hauptsächlich Lebensmittel erlaubt. Diese reichen jedoch nur für ca. 10 bis 14 Tage eines Monats. Der Rest des täglichen Bedarfs muss auf dem freien Markt oder sogar in Devisenläden gekauft werden, was aber bei einem Durchschnittseinkommen von umgerechnet ca. 15 Euro je Monat äußerst schwierig ist.

Im jährlich herausgegeben Index der menschlichen Entwicklung (HDI) belegt Kuba regelmäßig vergleichsweise gute Werte. Nachdem das Land im Jahr 2010 wegen unzureichender Angaben zur Kaufkraftparität vorübergehend nicht gelistet wurde, belegte es 2014 Platz 44 auf dem Index und lag damit gleichauf mit Bahrain und noch vor Kuwait und dem EU-Mitglied Kroatien. Der schlechteste EU-Staat, Bulgarien, rangierte 14 Plätze hinter Kuba auf Platz 58. In Lateinamerika nahm Kuba 2012 hinter Chile und Argentinien (Platz 40 und 45) den fünften Platz ein. Insbesondere im Bereich der Bildung und Gesundheit konnte Kuba Erfolge vorweisen. Außerdem hat Kuba demzufolge im Vergleich zum Rest Lateinamerikas und Teilen der restlichen Welt eine niedrigere Kindersterblichkeitsrate (nur 5,5 von 1000 Kindern sterben), höhere Lebenserwartung (79,3 Jahre – 4,6 Jahre mehr als durchschnittlich in Lateinamerika) und praktisch keinen Analphabetismus.

Kubas hohe Einstufung im HDI, von der Regierung gerne zitiert, stößt in der Wissenschaft auf Kritik. Kubas Berechnungsmethoden zum Bruttoinlandsprodukt sind international nicht anerkannt, vor allem weil die Umsätze in den zwei Landeswährungen nicht korrekt verrechnet werden. Dies macht die Berechnung des kaufkraftbereinigten Bruttonationaleinkommens pro Kopf der Bevölkerung schwierig. Das UNDP, welches den HDI und den deutlich detaillierteren Human Development Report (HDR) herausgibt, hat deshalb ein eigenes Verfahren entwickelt, die Kaufkraftparität zu schätzen. Der kanadische Ökonom Archibald Ritter hält Kubas Statistiken im HDR für „undurchsichtig“. Der Wirtschaftswissenschaftler und Soziologe Hans-Jürgen Burchardt warnt davor, allein aus diesen Studien Rückschlüsse auf den wahren Lebensstandard der kubanischen Bevölkerung zu schließen, da die Regierung, trotz unbestreitbarer Erfolge im Sozialbereich, die darin enthaltenen Statistikwerte gezielt optimieren würde. Auch das International Journal of Epidemiology stellte sich die Frage, warum zum Beispiel die Kindersterblichkeit auf dem Niveau der Industriestaaten sei, die Zahl der Totgeburten jedoch deutlich über deren Niveau liege, und vermutet, dass Fälle von dem einen, im HDR vertretenen Index zum anderen verschoben werden.

Ende Januar 2006 erhielt Kuba vom UN-Welternährungsprogramm ein Zertifikat, in dem ihm bestätigt wird, das einzige Land Lateinamerikas und der Karibik ohne unterernährte Kinder zu sein. Nur zwei Prozent würden Eisenmangelerscheinungen zeigen. 2011 wurde das auch von UNICEF bestätigt. Dennoch ist Kuba nicht frei von Hunger. Zudem kann durch die im Land herrschende Zensur im Einzelfall nicht unabhängig geprüft werden, ob die von der Regierung gemachten Angaben auch stimmen. Insbesondere während der Versorgungskrise in den 1960er-Jahren sowie während der Sonderperiode in den 1990ern waren größere Teile der Bevölkerung von einer schlechten Ernährungslage betroffen. Die Finanzkrise 2008 hat dieses Phänomen wieder gehäufter auftreten lassen. Vor allem ältere Menschen in den Städten mit niedrigen Renten und ohne Zugang zu Landwirtschaft oder zum Dollar gehören zum gefährdeten Personenkreis. Insgesamt dürfte sich die Anzahl der Kubaner, die sich maximal eine Mahlzeit pro Tag leisten können, nach Schätzungen des Historikers und Kubakenners Michael Zeuske um 2012 zwischen 30 und 35 Prozent bewegen. Durchschnittlich muss eine kubanische Familie heutzutage 70 bis 90 % ihres Einkommens allein für Lebensmittel ausgeben.

Auch in anderen Bereichen stagnierte das Wohlstandswachstum oder fiel relativ hinter andere lateinamerikanische Länder zurück (Telekommunikation, Automobilversorgung, Elektrizitäts- und Nahrungsmittelversorgung).
Viele Häuser sind alt, renovierbedürftig und überfüllt. Es herrscht akute Wohnungsnot. Manche Wohngegenden gleichen entsprechenden Problemvierteln von Städten in anderen lateinamerikanischen Staaten, wie den brasilianischen Favelas oder den argentinischen Villas Miserias, in denen teilweise sogar die ärztliche Versorgung fehlt. Marode Trinkwasserversorgungssysteme, begünstigt durch starke Regenfälle und hohe Temperaturen, führten im Sommer 2012 zum ersten Ausbruch der Cholera seit 130 Jahren. Die Krankheit galt in Kuba eigentlich als ausgerottet. Während die offizielle Berichterstattung über das wahre Ausmaß der Epidemie sehr zurückhaltend ist, werden unabhängige Journalisten, die sich dieses Themas annehmen, strafrechtlich verfolgt.

Noch immer sind viele Konsumgüter rationiert und selbst mit den Lebensmittelkarten oft nicht verfügbar. Selten ist vor allem Fleisch. Weitaus stärker wirkt jedoch der Zugang zu Devisen vor allem über Tourismus und Verwandte im Ausland, meist in den USA. Auch das Zweiwährungssystem wirft große Probleme auf. Viele Waren des täglichen Bedarfs und erst recht nahezu alle höherwertigen Produkte, wie elektronische Geräte, sind nur gegen den an den US-Dollar angelehnten Peso Convertible (CUC) erhältlich. Dieser muss derzeit gegen 25 Pesos Cubanos je CUC in der Wechselstube ("CADECA" – Casa de Cambio) umgetauscht werden. Kubaner, die keine Verwandten im Ausland haben, die sie regelmäßig durch Geldsendungen unterstützen oder auch sonst keinen Zugang zu Devisen haben, können sich dies kaum leisten. In Kuba wird dies inoffiziell als "ökonomische Apartheid" bezeichnet.

Für Funktionäre der Kommunistischen Partei und Offiziere der Streitkräfte existieren ein unabhängiges, privilegiertes Versorgungssystem, eigene Clubs und spezielle Urlaubsorte, wo sie und ihre Familien preiswert Urlaub machen können.

Neben dem Staat betreibt auch die katholische Kirche Kubas ein soziales Netz im Rahmen ihrer Möglichkeiten. Soziale Hilfe außerhalb des Staates wird jedoch nicht gern gesehen und möglichst unterbunden. Ausnahmen gelten nur für die politische Entwicklungshilfe der zahlreichen Solidaritätsvereine außerhalb Kubas, die bereit sind, mit dem Staat zusammenzuarbeiten.

Zum Staatshaushalt machen die kubanischen Behörden keine international vergleichbaren Angaben. Nach veröffentlichten Schätzungen der US-amerikanischen CIA umfasste der Haushalt 2016 Ausgaben von umgerechnet 58,59 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 52,37 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 7,7 % des BIP. Kuba hat eine der höchsten Staatsquoten weltweit.

Die Staatsverschuldung betrug – ebenfalls nach CIA-Schätzung – zum Jahresende 2016 32,7 % des BIP (im Vergleich zu 34,6 % im Vorjahr). Kubas Bonität wurde von Moody’s Ende 2015 unverändert mit Caa2 bewertet. Die letzten offiziellen Angaben zur Staatsverschuldung stammen von 2008 und sind aufgrund der Angabe in nicht-konvertierbaren kubanischen Pesos (die im Ausland keinen Wert haben) nicht verwertbar: 11,6 Mrd. Pesos oder 19,1 % des kubanischen BIP. Nach Recherchen der Europäischen Union betrug der Schuldenstand Kubas 2008 (ohne die Schulden gegenüber der ehemaligen Sowjetunion in Höhe von geschätzten 28 Mrd. US-Dollar) 31,7 Mrd. US-Dollar, von denen 20 Milliarden von Kuba nicht mehr bedient werden. Bezüglich der Auslandsverschuldung konnte Kuba im Jahr 2013 mit Mexiko, Russland, China und Japan eine Art Umschuldungsabkommen abschließen, wobei allein im Falle Russland rund 29 Milliarden US-Dollar Schulden erlassen wurden.

Die kubanische Infrastruktur wurde durch die Sonderperiode zu Beginn der 1990er schwer getroffen. Durch die Auflösung der Sowjetunion und des Ostblockes waren kurzfristig keine Ersatzteile mehr verfügbar und Treibstoff konnte nur noch auf dem Weltmarkt gegen Devisen beschafft werden. Der öffentliche Verkehr mit Zügen und Bussen musste deshalb stark eingeschränkt werden. Durch die wirtschaftliche Erholung Kubas hat sich die Situation inzwischen wieder weitgehend normalisiert.

Die staatliche Eisenbahngesellschaft "Ferrocarriles de Cuba" betreibt das einzige noch für den Personenverkehr in Betrieb stehende staatliche Eisenbahnnetz auf einer karibischen Insel. Es gehört zu den ältesten weltweit (seit 1836) und umfasst über 4500 Kilometer (ohne Strecken für Zuckertransport).

Kuba verfügt über ein gut ausgebautes Straßennetz, darunter eine Autobahn, die durch den geringen Motorisierungsgrad aber nur schwach befahren ist. Die Straßen sind jedoch in einem teilweise sehr schlechten Zustand.

Überlandbusse werden durch das Unternehmen "Astro" betrieben, zu dem auch die Viazul-Busse für Touristen gehören.
Seit der Revolution durften Kubaner privat keine Automobile besitzen; ausgenommen waren Fahrzeuge, die vor der Revolution 1959 bereits im Land waren. Bedingt durch diese besondere Lage befinden sich sehr viele Oldtimer, meist amerikanische, im Land. Im April 2011 wurde der Gebrauchtwagenhandel liberalisiert, und seit 2014 dürfen Kubaner auch Neuwagen kaufen. Der Staat behält jedoch das Importmonopol und bietet die Fahrzeuge zu einem Vielfachen des Preises an, wie er beispielsweise in Europa üblich ist.

Die kubanischen Fluggesellschaften Cubana, Aerogaviota und Aerocaribbean betreiben vom Flughafen Havanna José Martí als Drehkreuz aus ein dichtes Netz aus Inlandsflügen, sowie Auslandsflügen z. B. nach Kanada, Mexiko und Spanien.
Nach einer 55-jährigen Unterbrechung gibt es seit dem 31. August 2016 wieder Linienflüge zwischen Kuba und den USA.

"Siehe auch: Liste der Flughäfen in Kuba"

Die Bedeutung der Schifffahrt beschränkt sich auf Fährverbindungen zur Isla de la Juventud und weiteren vorgelagerten Inseln, sowie Fähren über die Hafenbuchten von Santiago de Cuba, Cienfuegos und Havanna.

Bis 2013 soll der Hafen von Mariel zum größten Containerhafen der Karibik ausgebaut werden. Der Bau erfolgt durch ein Joint-Venture mit dem brasilianischen Unternehmen Odebrecht und dem kubanischen Unternehmen Almacenes Universal S.A.
Die Gesamtinvestitionen betragen 600 Millionen US$.
Die Hafeneinfahrt soll eine Breite von 700 Metern erhalten, die es erlaubt, zwei große Containerschiffe gleichzeitig aufzunehmen. Außerdem wird der Hafen für Schiffe mit bis zu 15 Metern Tiefgang zugänglich sein (Im Vergleich dazu: Der Hafen von Havanna ermöglicht nur 11 Meter Tiefgang). Am Ende der Ausbauarbeiten soll das Terminal eine Kapazität von 850.000 bis 1 Million Container verwalten können (Hafen Havanna: 350.000 Container). Dieser Ausbau soll es Mariel ermöglichen, große Containerschiffe zu empfangen, die über den Panamakanal von Asien her nach Kuba fahren. Auch soll Mariel optimale Bedingungen für US-amerikanische Container bieten. Mariel soll damit den Hafen von Havanna für Frachtaufgaben ablösen, in Zukunft wird dieser nur noch touristisch genutzt werden.

Nationaler Energieversorger ist der Staatsbetrieb "Sistema Eléctrico Nacional", an dessen Netz 96 % der kubanischen Haushalte angeschlossen sind. Die Steckdosen weisen 110 Volt auf. In vielen Bereichen (z. B. Krankenhäuser, Touristen-Hotels) wird auch 220 Volt verwendet.

Die Energieversorgung beruht fast ausschließlich auf fossilen Brennstoffen.
Nahezu die Hälfte wird nur mit Schweröl erzeugt. Nimmt man die Erzeugung durch lokale Diesel- und andere Verbrennungsmotoren hinzu, steigt der fossile Anteil auf 86 %. Dazu kommen noch einmal knapp 10 % aus Gaskraftwerken. Der Anteil erneuerbarer Energie ist demnach sehr gering.
Die eigene Erdölförderung war während der sowjetischen Überversorgung vernachlässigt worden, sodass diese nicht mehr wettbewerbsfähig war und Kuba auf teure Importe angewiesen war. Die Energieeffizienz leidet stark unter den veralteten Kraftwerken und Stromnetzen. Die Stromerzeugungskosten für den kubanischen Staat liegen bei 15,75 Eurocent (Stand 2014). Zum Vergleich liegen die Stromgestehungskosten z. B. der Windenergie in Deutschland je nach Standortgüte zwischen 4,5 ct/kWh auf sehr guten und 10,7 ct/kWh auf sehr schlechten Standorten.

Die maximale Gesamtleistung aller Kraftwerke Kubas beträgt 5.852,5 MW, der Strombedarf zu Spitzenlastzeiten liegt bei ca. 2500 MW. Im Jahr 2010 wurden 17.395,5 GWh Strom erzeugt. Die Energieversorgung des Landes gilt als marode und veraltet, weswegen es zu regelmäßigen Stromabschaltungen kommt.

Erste Projekte zur Nutzung der Windenergie, Wasserkraft und Photovoltaik laufen. Seit Februar 2007 speist eine vom französischen Windkraftanlagenhersteller Vergnet gelieferte, 3,4 Mio. Dollar teure Pilotanlage östlich von Nueva Gerona auf der Isla de la Juventud insgesamt 1,65 MW ins Netz ein. Aufgrund der hohen Gefahr durch Tropenstürme können die je 275 kW starken Generatoren automatisch zu Boden gesenkt werden.

Die im Jahre 2006 ausgerufene „Energierevolution“ "(Revolución energética)" hat auch eine Senkung des Stromverbrauchs zum Ziel. Dafür wurden Glühlampen durch Energiesparlampen ersetzt. Außerdem wurden über 2,5 Mio veraltete Kühlschränke gegen modernere Modelle ausgetauscht. Der Kaufpreis von mehr als einem durchschnittlichen Jahresgehalt kann über einen 10 Jahre laufenden, einkommensabhängig verzinsten Kredit abgezahlt werden. Die Zahl der Stromausfälle ist seit dieser Zeit zurückgegangen. Seit Mitte 2016 treten jedoch wieder vermehrt großflächige Stromabschaltungen auf, nachdem Venezuela wegen der dort herrschenden extremen Wirtschaftskrise die Lieferung subventionierten Öls um 40 Prozent reduzierte.

Das kubanische Telefonnetz befindet sich ähnlich wie nahezu sämtliche andere Infrastruktur in einem schlechten Zustand. Der Telekommunikationsverkehr unterliegt starken Kontrollen. Die Handynetzabdeckung betrug 2013 über 75 Prozent. 2013 begann die versuchsweise Ausstrahlung von digitalem Fernsehen in der Umgebung von Havanna. Die landesweite Umstellung auf DVB-T soll 2016 beginnen und mit der Abschaltung der letzten analogen Signale 2021 abgeschlossen sein.

Für das Telekommunikationsnetze ist das staatliche Telekommunikationsunternehmen ETECSA verantwortlich. Das Mobilfunknetz wird von der Tochtergesellschaft "Cubacel" (Kennung C_Com) betrieben und deckt fast die gesamte Insel ab. Es werden die GSM-Frequenzen von 850 und 900 MHz sowie das vor allem in Nordamerika verbreitete TDMA verwendet. Im März 2017 wurde auch das UMTS-Netz in Betrieb genommen. Die Netzabdeckung umfasste zunächst hauptsächlich Havanna, die Provinzhauptstädte und einige touristisch relevante Regionen.

Die Durchdringung der kubanischen Bevölkerung mit Telefonen oder Handys ist schwach ausgeprägt. 2007 gab es bei einer Einwohnerzahl von 11,2 Millionen nur rund 910.000 Telefonanschlüsse in Privathand, Handys gab es nach offiziellen statistischen Angaben 330.000. Für Ende 2008 wurden ca. 480.000 aktive Handyverträge gemeldet. Mitte 2013 gab es in Kuba je 1,7 Millionen aktive Handys sowie 1,2 Millionen private Telefonanschlüsse Mitentscheidend hierfür waren der Wegfall staatlicher Beschränkungen (Kubaner können seit Ende 2008 ohne bürokratische Hürden einen Mobilvertrag eröffnen), Tarifsenkungen (günstigere SMS und kostenfreie Anrufe aus dem In- und Ausland), sowie die vereinfachte Möglichkeit, kubanische Handykarten via Internet aus dem Ausland aufzuladen.

Der Zugang zum Internet hat sich seit der Normalisierung der Beziehungen zwischen Kuba und den Vereinigten Staaten im Jahr 2014 stark verbessert. Im Juli 2015 wurde der Preis für eine Stunde Internetzugang von 4,50 CUC auf 2 CUC reduziert. Zusätzlich dürfen Kubaner seitdem auch die ETECSA WiFi HotSpots nutzen, und sind nicht mehr nur auf die veralteten ETECSA Internet-Terminals angewiesen. 2016 hatten offiziellen Zahlen zufolge 32,4% der Bevölkerung Zugang zu Internetdiensten. 5% der Haushalte haben einen Internetanschluss.

Eine seit 2008 geplante unterseeische Glasfaserkabelverbindung zwischen Venezuela und Kuba nahm Mitte 2012 ein Jahr verspätet ihren Betrieb auf. Obwohl es laut Mitarbeitern der staatlichen Telekommunikationsfirma ETECSA funktionsfähig war, wurde das venezolanischen Angaben zufolge 70 Millionen Euro teure Kabel zunächst fast zwei Jahre lang nicht genutzt. Als Grund wurde Korruption genannt. Auch Zusammenhänge mit dem Arabischen Frühling werden vermutet, wonach das kubanische Regime plötzlich wieder unregulierte Internetzugänge fürchte.

Im Januar 2013 wurde die Inbetriebnahme des Unterseekabels auch für den Internetverkehr bestätigt, nachdem es zunächst für die Durchleitung des internationalen Telefonverkehrs benutzt wurde. Seit dem 4. Juni 2013 können Kubaner in 118 Internetcafés der Marke "Nauta" Internetzugang mit einer Geschwindigkeit von mindestens 2 MBit für 2 CUC pro Stunde in Anspruch nehmen. Der Import von WiFi-Routern wurde erleichtert.

Das Kabel hat rund 3000-fache Bandbreite der Satellitenkanäle, mit denen Kuba bisher an das weltweite Datennetz angeschlossen war, verläuft zwischen der venezolanischen Stadt Camuri auf dem Meeresboden und erreicht Kuba in Siboney bei Santiago de Cuba. Es ist 1602 Kilometer lang – das Elffache der kürzestmöglichen Entfernung zum kontinentalen Festland (Florida: 144 km). Die kubanische Regierung lehnte es „aus Sicherheitsgründen“ ab, ihren Internetverkehr über die USA zu leiten, obwohl Internet und Telekommunikation von den Embargobestimmungen ausgenommen sind.

Kuba gehört zu den ersten Staaten auf der Welt, welche die Forderung nach einer umweltverträglichen wirtschaftlichen Entwicklung in die Verfassung aufnahmen. Eine umfassende Umweltschutzgesetzgebung in Verbindung mit Umwelterziehungsprogrammen und zahlreichen Umweltschutzprojekten trugen dazu bei, dass Kuba das Land mit der besten ökologischen Bilanz im Verhältnis zum Lebensstandard ist. Es ist weltweit das einzige Land, das vom WWF eine „nachhaltige Entwicklung“ bescheinigt bekam, das heißt, Kuba verfügt über einen entwickelten Lebensstandard bei gleichzeitiger ökologisch nachhaltiger Entwicklung. Dennoch hat die ökonomische Entwicklung im Zweifel eindeutig Priorität gegenüber der Umweltpolitik.

Im Jahr 2011 flossen 10,4 % der Gesamtinvestitionen in den Umweltschutz, die Investitionssumme hierfür erhöhte sich von 233 Mio. Pesos im Jahr 2006 auf 452 Mio. im Jahr 2011. Hauptziele der Investitionen sind der Schutz der Gewässer (68,4 %) und die Wiederaufforstung (16,5 %).

Bedingt durch die Ölknappheit nach der Auflösung der Sowjetunion war Kuba gezwungen, viele Rationalisierungen und Einsparungen vorzunehmen. Die starke Verringerung des Individualverkehrs, die Ersetzung von Maschinen in der Landwirtschaft durch Ochsenkarren, Austausch von veralteten Motoren in Fahrzeugen oder neue Wege bei der Energieerzeugung, zum Beispiel durch Solarenergie, haben die ökologische Bilanz stark verbessert. Die im Jahre 2005 begonnenen Einsparmaßnahmen und Verbrauchsreduzierungen von Strom, vor allem durch staatliche Kampagnen, bspw. zum Austausch von Glühlampen durch Energiesparlampen, sind erfolgreich. Hinzu kommt eine allgemeine Rohstoffknappheit, die zu einer äußerst geringen Verwendung von Verpackungsmaterialien führt. Der umfangreiche Einsatz von Chemie in der Landwirtschaft wurde durch den Mangel an importierten Düngemitteln eingeschränkt.

Die Fläche natürlichen Waldes hat entgegen dem weltweiten Trend seit 1990 zugenommen. Im Jahr 2007 pflanzten die Kubaner 136 Millionen Bäume. Im Jahr 2012 sind 27,3 Prozent ihrer Insel wieder bewaldet. Bis 2015 soll die Waldfläche 29,3 % der Insel einnehmen. Im Jahr 1959 waren im Vergleich dazu 13,6 % bewaldet.

Die Erfüllung der im Protokoll von Montreal eingegangenen Verpflichtung, bis Ende 2007 50 Prozent der Substanzen zu eliminieren, die der Ozonschicht schweren Schaden zufügen, konnte im September 2007 mit 74 Prozent Abbau nachgewiesen werden.

Die Ende 1980 von den Vereinten Nationen als eine der weltweit am stärksten verschmutzten und nicht mehr zu rettenden klassifizierte Hafenbucht von Havanna wurde nach Angaben der kubanischen Regierung erfolgreich gesäubert, wobei 17.000 Fass verwertbares Erdöl aus dem Wasser der Hafenbucht geborgen werden konnte.

Besondere Umweltprobleme verursacht der Nickelbergbau im Gebiet Moa an der Nordostküste durch ungenügend behandelte kontaminierte Rückstände. Das Alter vieler Betriebe bedingt einen geringen Umweltschutzstandard und eine mangelhafte Entsorgung von Industrieabfällen.

Auf Kuba stehen insgesamt 211 Gebiete unter besonderem Naturschutz. Damit sind 20 Prozent der Oberfläche Kubas ökologisch geschützt. Das System der Schutzgebiete in Kuba ist sehr entwickelt und in unterschiedliche Kategorien eingeteilt:

Insgesamt existieren in Kuba 73 Naturreservate mit unterschiedlichem Schutzstatus, wie z. B. 14 Nationalparks und sechs Biosphärenreservate
Der berühmteste Nationalpark Kubas, der Nationalpark Parque Nacional Alejandro de Humboldt, befindet sich im Osten Kubas in den Provinzen Holguín und Guantánamo.

Das 5000 km² große Feuchtgebiet auf der Zapata-Halbinsel mit Dutzenden endemischen Tier- und Pflanzenarten wird von Experten der UNO-Umweltbehörde für Lateinamerika und die Karibik als das bestgehütete in der Region geschätzt.

In Kuba sind zahlreiche Musikstile und Tänze entstanden, die zum Teil international Verbreitung fanden. Zu ihnen gehören der Son, der Mambo, die Salsa, der Danzón, die Rumba, der Cha-Cha-Cha und die alte und neue Trova (Nueva Trova).

Durch die Übersiedlung vieler Süd- und Mittelamerikaner in die USA während des Zweiten Weltkrieges kam es sehr schnell zu einer leichten Vermischung aus kubanischen Rhythmen und dem Jazz. Nach 1945 wurde kubanische Musik auch in Westafrika sehr beliebt und beeinflusste das Highlife.

Gegen Ende der 1990er-Jahre wurde durch den Film "Buena Vista Social Club" von Wim Wenders eine Kuba-Welle ausgelöst. Neben der bis dahin schon international verbreiteten modernen kubanischen Musik wurde wieder die Musik der 1940er-Jahre zum Exportschlager. Der Film berichtet über die Arbeit von Ry Cooder mit einer Gruppe von kubanischen Musikern, die fast alle bereits das Rentenalter erreicht hatten. In der Folge veröffentlichten die beteiligten Musiker teils eigene Solo-Alben, die internationale Verkaufserfolge wurden. 

Um das Jahr 2005 herum hatte weltweit der Reggaeton, moderne kubanische Musik meist jugendlicher Gruppen, einen kurzen, heftigen Boom. Seinen Ursprung hat der Reggaeton in Puerto Rico und Panama. Einige dieser Hits mit meist "schlüpfrigen" Texten tauchten seinerzeit sogar in europäischen Charts auf. Nur wenige Monate später war zumindest der globale Hype wieder vorbei. Stilelemente des Reggaeton wurden allerdings in der Folge immer wieder verwendet und beeinflussten vor allem in der Mitte der 2010er zahlreiche internationale Hits.

Vor der Revolution gab es auf Kuba keine eigenständige Filmproduktion. Die wenigen Filme, die auf Kuba produziert wurden, ahmten den Stil US-amerikanischer Produktionen nach.

1959 wurde das Kubanische Filminstitut Instituto Cubano del Arte e Industria Cinematográficos (ICAIC) gegründet, das zunächst überwiegend Dokumentar-, Zeichentrick- und Lehrfilme produzierte. Sein Gründungsdirektor war Alfredo Guevara, ein enger Vertrauter Castros seit gemeinsamer Studienzeiten, der bis zu seinem Tod 2013 die zentrale Figur der kubanischen Filmkultur blieb. Der poetische Kurzfilm "PM", der das Nachtleben Havannas dokumentierte, wurde 1961 von der revolutionären Zensur verboten und löste eine den gesamten Kulturbetrieb betreffende Debatte aus, die Fidel Castro mit seinen „Worten an die Intellektuellen“ beendete, in denen er ihre künstlerische Freiheit den Interessen seiner Regierung unterordnete. Der 1964 in Kuba gedrehte Film "Soy Cuba" war eine sowjetisch-kubanische Koproduktion mit Micheil Kalatosow als Regisseur, die kubanischen Filmschauspieler und Mitarbeiter des Films begründeten später einen eigenständigen kubanischen Filmstil. Regisseure wie Tomás Gutiérrez Alea ("Der Tod eines Bürokraten" – "Muerte de un Burócrata", 1964) und Humberto Solás ("Lucia", 1968) führten nicht nur unter Cineasten zu einer internationalen Anerkennung des kubanischen Films. 1977 produzierte das ICAIC innerhalb eines Jahres 10 abendfüllende Filme und 61 Kurzfilme. Aufgrund der Wirtschaftskrise zu Beginn der 1990er Jahre wurde die kubanische Film- und Fernsehproduktion zurückgefahren, so dass in den 1990er-Jahren fast nur noch vom Ausland, besonders von Spanien finanzierte Filme hergestellt wurden.
Bemerkenswert ist der für einen Oscar nominierte Film "Erdbeer und Schokolade" (1993) nach einer Kurzgeschichte von Senel Paz, der gekonnt das Thema Homosexualität in der kubanischen Gesellschaft thematisiert. Erst neuerdings gibt es wieder eine eigenständige kubanische Filmproduktion, die mit Streifen wie "Suite Habana" (Regie: Fernando Pérez, 2003) die Traditionen des kubanischen Films fortsetzt.

Seit 1986 gibt es die von Gabriel García Márquez mit begründete Internationale Hochschule für Film und Fernsehen in San Antonio de los Baños, an der Studenten aus aller Welt, besonders aber Lateinamerikaner und auch Kubaner ausgebildet werden.

2017 wurden ca. 3000 kubanische Filmplakate in das Weltdokumentenerbe der UNESCO aufgenommen

Auswahl bekannter kubanischer Schriftsteller:


Die kubanische Küche ist eine Fusion aus spanischer, afrikanischer und karibischer Küche. Die Rezepte haben viele Gewürze und Techniken mit der spanischen und afrikanischen Kochkunst gemeinsam, mit einigem Einfluss aus dem Karibikraum in Würze und Aroma. Es gibt aber große Unterschiede z. B. zur mexikanischen Küche. Dagegen existiert ein kleiner, aber erwähnenswerter Einfluss der chinesischen Küche.

Auf Grund historischer Gegebenheiten wurde die kubanische Bevölkerung nicht gleichmäßig auf der Insel verteilt. Die afrikanischen Sklaven stellten die Mehrheit in den Zuckerrohrplantagen, jedoch waren sie in den meisten Städten in der Minderheit. Die Tabakplantagen waren hauptsächlich von armen spanischen Bauern, meist von den Kanarischen Inseln, besiedelt. Im östlichen Teil der Insel siedelten außerdem eine große Zahl französischer, haitianischer und karibischer Immigranten, hauptsächlich während der haitianischen Revolution, sowie Saisonarbeiter für die Zuckerernte, während dies im westlichen Teil nicht so der Fall war. Stattdessen waren bis in die 1950er-Jahre dort hauptsächlich europäische Einwanderer ansässig. So entwickelte sich die kubanische Küche unter lokalen Gegebenheiten und den spezifischen demografischen Einflüssen.

Historisch bedingt sind in vielen Rezepten Gewürzmischungen beschrieben. Die Grundlage der meisten Gerichte ist Reis mit schwarzen oder roten Bohnen, "congrí" oder "moros y cristianos" („Mauren und Christen“) genannt, deren Zutaten in der Regel problemlos in den staatlichen Geschäften erhältlich sind. Die Versorgungslage mit anderen Nahrungsmitteln gestaltet sich mitunter schwierig, da die staatlichen Geschäfte nur ein sehr eingeschränktes Angebot haben und oft von Engpässen betroffen sind, und auf den freien Bauernmärkten hohe Preise verlangt werden. Viele Kubaner in den Städten versorgen sich mit knappen bzw. teuren Lebensmitteln, wie zum Beispiel Fleisch, über Beziehungen zur Landbevölkerung oder halten sich Kleintiere auf Balkonen oder Dächern. Insofern variiert die kubanische Küche heute auch stark zwischen Land und Stadt.

Touristen, die in den Häusern einheimischer kubanischer Familien (casas particulares) untergebracht sind, bietet sich nach Absprache die Möglichkeit, die kubanische Küche zu versuchen. Kubanische Restaurants bieten in von Touristen frequentierten Gegenden oftmals eine Menükarte an, deren Preise in den zwei Währungen CUC und Moneda Nacional ausgeschrieben sind. Dort angebotene Speisen sind öfters nicht erhältlich und das Angebot deutlich eingeschränkter als in der Speisekarte angegeben. Die „Standards“ "moros y cristiano" und diverse Varianten aus Hühnchenfleisch sind aber in der Regel erhältlich. Alternativen dazu sind "Paladares" (dt. „Gaumen“), privat, oftmals in Privatwohnungen, betriebene Restaurants, die reichhaltige und abwechslungsreiche Küche anbieten, allerdings zu Preisen, die nur für Ausländer bezahlbar sind und an westeuropäisches Niveau heranreichen.

In kubanischen Städten sind kleine Verkaufsstände auf Straßen verbreitet, die eine Vielzahl an belegten Brötchen, Pizza oder lateinamerikanische Snacks anbieten. Auch aus Erdgeschossfenstern von Wohnungen wird so verkauft. So bekommt man eine kleine, einfache aber äußerst sättigende Pizza für einen Preis von etwa 5 Pesos (ca. 20 Euro-Cent).

Der Sport hat in Kuba einen hohen Stellenwert. Sportarten wie Baseball oder Boxen waren und sind sehr populär. In heutigen Tagen wird der Sport staatlicherseits stark gefördert.

Kuba nimmt an zahlreichen internationalen Wettbewerben, wie den Olympischen Sommerspielen und den Panamerikanischen Spielen teil. Die medaillenversprechendsten Sportarten sind der Baseball, Judo der Frauen, Ringen (griechisch-römisch), Boxen und Leichtathletik. Beachtenswert sind außerdem die Erfolge im Volleyball, Handball, Freistilringen, Kunst- und Turmspringen, Schach, Radrennen, Taekwondo und Kanusport. Auf dem ewigen Medaillenspiegel der Panamerikanischen Spiele befindet sich Kuba auf dem 2.Platz.
Die Kubanische Fußballnationalmannschaft nahm bisher erst ein Mal an einer WM-Endrunde teil.

Im April 2017 durchquerte der Österreicher Jacob Zurl als Erster die Hauptinsel Kuba in Längsrichtung im autobetreuten Non-Stop-Langstrecken-Radrennstil.

Die kubanischen Massenmedien sind Staatseigentum nach Kapitel VI Art. 52. der Verfassung von 1976. Das gesamte Medienwesen dient entsprechend auch der Propaganda des Staates. Die Lenkung und Kontrolle der über die Medien verbreiteten Inhalte obliegt der Abteilung für Ideologie des Zentralkomitees der Kommunistischen Partei Kubas, die von Rolando Alfonso Borges geleitet wird. Durch die Wirtschaftskrise von 1993 bedingt, ist das Angebot, das es an Printmedien (Zeitungen und Bücher) und Kinos auf Kuba gab, sehr stark eingeschränkt worden, während andere Medien wie Fernsehen und Internet, wenn auch nicht in gleichem Maße, zugenommen haben.

Die kubanische Presse steht unter alleiniger Kontrolle der Regierung, der Kommunistischen Partei Kubas und der kommunistischen Massenorganisationen (Gewerkschaften, Frauenföderation etc.). Den größten Verbreitungsgrad haben folgende kubanische Zeitungen, die alle auch über eine teilweise mehrsprachige Internet-Version verfügen. Die Zeitungen und Zeitschriften haben trotz ihrer nur allmählich wieder steigenden Auflagen sehr viele Leser, da sie in der Regel in der Nachbarschaft systematisch untereinander ausgetauscht werden und eine faktische Monopolstellung innehaben. Folgende Zeitungen und Zeitschriften seien genannt:


Unabhängiger Journalismus wird konsequent verfolgt. Insbesondere Berichte über die Lage auf Kuba oder deren Weitergabe an ausländische Medien ist strengstens untersagt. Kritische unabhängige Journalisten publizieren ihre Texte auf ausländischen Internetseiten wie CubaNet. Auf der anderen Seite bemüht sich die Regierung auch zu verhindern, dass sich Bürger aus kubakritischen Quellen informieren können, Radio Martí, ein Radiosender der US-Regierung in spanischer Sprache, wird ständig gestört und Internetseiten werden gefiltert.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Kuba Platz 173 von 180 Ländern. In Kuba sitzen zwei Journalisten in Haft.

Es gibt auf Kuba fünf staatliche Fernsehkanäle (Cubavisión, die beiden Bildungskanäle Canal Educativo 1 und 2, Tele Rebelde und Multivisión), die per analoger Antenne von der gesamten Bevölkerung empfangen werden können. Nahezu alle kubanischen Haushalte verfügen über, allerdings mitunter sehr alte, Fernsehgeräte. Für den Empfang im Ausland sendet der über Satellit ausgestrahlte Kanal "Cubavisión Internacional" ein 24-Stunden-Programm.

Seit Juli 2005 strahlt der Satellitensender teleSUR sein Programm für Lateinamerika aus, an dem Kuba mit 19 % Einlage beteiligt ist. In Kuba selbst wurde zunächst nur Tageszusammenfassung des Programms auf dem Sender "Canal Educativo 2" gezeigt. Seit Januar 2013 wird das Programm in zwei Zeitfenstern von 8 Uhr morgens bis 16:30 Uhr nachmittags sowie von 20 Uhr bis 1 Uhr nachts live gesendet.

Satellitenempfang und der Besitz von Empfangsschüsseln sind in Kuba für Privatleute verboten. Für touristische Einrichtungen wie Hotels werden eine Auswahl internationaler Satellitenprogramme, darunter beispielsweise die DW-TV oder CNN, in ein nationales Fernsehkabelnetz eingespeist, das von der staatlichen Firma "Telecable" betrieben wird. Das spanischsprachige CNN en Español wurde im Januar 2011 aus der Senderliste gestrichen.

Neben zahlreichen Radiosendern mit gemischten Programmen und reinen Musiksendern gibt es unter anderen den nach eigenen Angaben ältesten 24-Stunden-Nachrichtensender Radio Reloj (Radio Uhr) mit ständiger Zeitansage. Ausländische Sender können, soweit technisch möglich, frei empfangen werden (mit Ausnahme des ständig gestörten US-Senders Radio Martí).

Bis zum Abkommen zwischen den USA und Kuba zur Wiederaufnahme der diplomatischen Beziehungen im Jahr 2014 war das Internet in Kuba, auch aus Angst vor Verlust des Medienmonopols seitens des Staates, nur unter starken Restriktionen zugänglich. Als Teil dieses Abkommens sagten die USA zu, das Embargo für den Export von Ausrüstung und Dienstleistungen im Bereich der Telekommunikation zu beenden. Der Mitte 2015 im US Kongress eingebrachte Cuba DATA Act soll die gesetzliche Basis für das Engagement von amerikanischen Telekommunikationsfirmen in Kuba schaffen. Mit Sprint und Verizon haben seitdem bereits zwei US-Unternehmen Sprach- und Datendienste für Kuba angekündigt.

Die Anfänge des kubanischen Internets reichen zurück ins Jahr 1994, als mit Hilfe der UNESCO ein Backbone für die ganze Insel installiert wurde, der nur für die Anbindung der Ärzte an nationale und internationale medizinische Datenbanken gedacht war und staatlicher Kontrolle unterliegt. Private Internetzugänge sind auch heute in Kuba praktisch nicht vorhanden. Ausnahmen bilden lediglich die schon genannten Ärzte, Wissenschaftler und regierungstreue Journalisten. Ansonsten sind die Kubaner bislang gezwungen, öffentliche Zugangsmöglichkeiten zu nutzen. Diese werden seit 2015 stark ausgeweitet. Neben den bis zu 10 Dollar pro Stunde teuren Hotelanschlüssen gibt es seitdem zunehmend die Möglichkeit, sich in einen der WLAN-Hotspots einzuwählen. Deren Zahl begann bei 35 und belief sich gegen Ende 2015 auf rund 60. Bei Kosten von zwei Dollar pro Stunde liegt der Tarif jedoch weiterhin jenseits dessen, was sich ein kubanischer Durchschnittsverdiener mit 25 Dollar Monatseinkommen leisten kann.
2011 wurde Kuba über ALBA-1 von Venezuela aus an das internationale Glasfasernetz angeschlossen. Zuvor lief die Kommunikation über langsame Satellitenverbindungen. Offiziell in Betrieb ging das Kabel jedoch erst zwei Jahre später. Seitdem verbessern sich die Zugangsmöglichkeiten der Kubaner zum Internet langsam, aber stetig. War es zuvor nur in Touristenhotels möglich, für sechs bis zehn konvertible Pesos (CUC) ins Internet zu gehen, oder für 1,50 CUC pro Stunde an Computern der Post internationale E-Mails zu schreiben, wurden nun zahlreiche Internetcafés der staatlichen Telekommunikationsfirma ETECSA eingerichtet, in denen man für 4,50 CUC pro Stunde ins Internet gehen konnte. 2014 wurde der mobile E-Mail-Dienst Nauta eingerichtet, der es erlaubte, via den mobilen Datendienst GPRS internationale E-Mails zu versenden und empfangen. Mitte 2015 wurde Nauta um WLAN-HotSpots in mehreren großen Städten erweitert, an denen je nach Ausbaustufe 50 bis 100 Menschen gleichzeitig online gehen können. Im Juli 2015 wurden die Zugangspreise von 4,50 CUC auf 2 CUC pro Stunde reduziert. Angesichts des kubanischen Durchschnittsverdiensts von 20 bis 25 CUC monatlich bleiben dies prohibitive Preise für den Großteil der kubanischen Bevölkerung, die damit weiterhin vom World-Wide-Net abgeschnitten ist. Ein Angebot von Google, Kuba kostenlos mit W-LAN-Antennen zu versorgen, wurde von der Regierung abgelehnt. Ziel sei es demnach nicht, die Kubaner mit Internet zu versorgen, sondern die Revolution zu unterminieren. Trotzdem ist vor allem seit der Öffnung der WLAN-HotSpots die Zahl der Internetnutzer in Kuba stark gestiegen. Vor allem Kubaner mit Verwandtschaft in den USA sowie eine wachsende Anzahl von Beschäftigten in der Tourismusbranche haben Zugang zu Devisen und können sich den Internetzugang mit eigenen Tablets, Smartphones oder Laptops leisten.

Im Jahr 2014 hatten in Kuba basierend auf Daten der ITU 27 Prozent der Bevölkerung Zugang zum Internet. Der Großteil davon hat allerdings nur Zugriff auf E-Mail-Dienste und staatliches Intranet. Zugang zum internationalen World-Wide-Web hatten Schätzungen zufolge 2015 lediglich 5 % der Bevölkerung, was die mit niedrigste Rate in ganz Lateinamerika ist. Es kamen 2011 rund 7 Computer auf 100 Einwohner, die allermeisten davon stehen jedoch in staatlichen Einrichtungen und lediglich 60 % sind an das Netz angeschlossen.

Am Welttag gegen Internetzensur (12. März) listete die Menschenrechtsorganisation Reporter ohne Grenzen Kuba (unter anderem in den Jahren 2009, 2010, 2011, 2015) als eines der zwölf Länder, die als Feinde des Internets gelten.

Kuba brachte im Februar 2007 die Betaversion der eigenen Suchmaschine 2x3 heraus. Abrufbar sind 150.000 offizielle Seiten, von der staatlichen Presse bis hin zu Fidel Castros Reden. Im Dezember 2010 startete das kubanische wikibasierende Online-Lexikon EcuRed mit rund 20.000 Artikeln, die die offizielle kubanische Sicht auf die Welt zeigen.

Die Regierung unter Raúl Castro hat angekündigt, dass trotz der zwischenzeitlichen Aufhebung des Verbots zum Kauf von Computern für Privatpersonen die Beschränkungen des Internetzugangs nicht so schnell beseitigt werden. Gründe seien vor allem die beschränkten technischen und ökonomischen Kapazitäten. Die meisten Haushalte hätten ja noch nicht einmal einen Telefonanschluss. 2008 wurden die Beschränkungen für den Kauf und die Nutzung von Mobiltelefonen gelockert.

Die kubanischen Gesetze drohen mit einer Haftstrafe von bis zu 20 Jahren für das Posten von illegalen Inhalten auf ausländischen Websites. Der illegale Zugang zum Internet wird mit fünf Jahren Haft bestraft. Für die praktische Durchführung der Internetzensur zeichnet sich die renommierte Informatik-Universität (UCI) verantwortlich.

Trotz aller Restriktionen entwickelte sich das Internet in den letzten Jahren zunehmend auch innerhalb Kubas zu einem Medium für den Austausch regierungsunabhängiger Informationen, vorwiegend per E-Mail. Gleichzeitig entwickelte sich ab ungefähr 2007 eine regimekritische Bloggerszene. Zu den international bekanntesten Bloggern gehören Yoani Sánchez, ihr Ehemann Reinaldo Escobar und Claudia Cadelo. Zwar wurde seitens der kubanischen Behörden toleriert, dass diese Blogs im Ausland zu lesen sind, jedoch war der Zugriff auf diese Blogs bis Februar 2011 innerhalb Kubas gesperrt. Weitere Blogs sind Havana Times, herausgegeben vom US-Amerikaner Circles Robinson, mit zahlreichen jungen und älteren Autoren aus Kuba, "Voces Cubanas", herausgegeben von Reinaldo Escobar, und "La Joven Cuba"

Mit der Ausdehnung des Engagements auf die kubanische Öffentlichkeit bekamen die Blogger jedoch auch zunehmend Probleme mit dem Sicherheitsapparat. Die Spanne der Repressionen reichte von Bedrohungen, über kurzzeitige Festnahmen, bis hin zu sogenannten "Actos de Repudio" (wörtlich „Akte der Ablehnung“, tatsächlich geht es jedoch um Einschüchterung).

Später änderte sich die Strategie der kubanischen Regierung: So wurden rund tausend regierungstreue „revolutionäre“ Blogger installiert, um den dissidenten Bloggern zu begegnen. Unter anderem werfen sie Yoani Sánchez und ihren Kollegen vor, von der US-Regierung bezahlt zu werden. Auch werden häufig Gerüchte über das Privatleben der Blogger veröffentlicht, mit dem Ziel, diese zu schädigen. Den USA wird vorgeworfen, einen sogenannten „Cyberkrieg“ gegen Kuba zu führen. Dieser würde nicht „Bomben und Gewehrkugeln, sondern mit Informationen, Kommunikation, Algorithmen und Bytes“ geführt. Dies sei „eine neue Form der Invasion, die von der entwickelten Welt ausgeht“. Die „Cyberdissidenten“ um Yoani Sánchez würden als Teil dieses Krieges aufgebaut.

Ende 2011 wurde in Kuba ein Klon von Facebook namens "Red Social" (Soziales Netzwerk) freigeschaltet. Dieses ist ausschließlich im kubanischen Intranet erreichbar und soll vor allem Studenten eine Alternative zu ausländischen sozialen Netzwerken im Internet wie Facebook oder Twitter bieten, die obwohl auch von offiziellen Stellen in Kuba selbst reichlich genutzt, als Teil des sogenannten „Cyberkrieges“ gegen das revolutionäre Kuba bezeichnet werden. Zweck dürfte es sein, den Informationsfluss besser zu kontrollieren und den Zugang zu den in diesen Netzwerken vorhandenen freien Informationen zu erschweren bzw. zu verhindern.

Auch der als Fidel Castro nahestehend geltende Ignacio Ramonet, autorisierter Biograph Castros, kritisierte den beschränkten Zugang der kubanischen Bevölkerung zum Internet: „Ohne eine hinreichend breite Auffahrt ins "www" droht die Insel den Anschluss an die internationale Entwicklung zu verlieren.“, so der Herausgeber der Le Monde Diplomatique.

Seit dem 9. Februar 2015 ist die US-amerikanische Online-Videothek Netflix auch in Kuba verfügbar.

Weltweit gibt es bei vielen mit dem Sozialismus verbundenen Menschen einen „Mythos Kuba“. Das kubanische Staatswesen wird als ein(ziger) gelungener Versuch des Sozialismus gesehen, der Vorbildcharakter nicht nur für die „Dritte Welt“ habe und den es zu verteidigen gelte. Große Sympathien hat das kubanische Modell auch in weiten Teilen Süd- und Mittelamerikas. Gründe hierfür sind beispielsweise:


In gleichem Maße wird die kubanische Regierung gerade von vielen Nichtlinken deutlich abgelehnt. Insbesondere in den USA ist sie als eine der letzten Bastionen des Kommunismus direkt vor der Haustür vielen ein Dorn im Auge. Sie argumentieren:

Differenzen dieser Art tragen in entsprechenden Medien zu einer sehr ideologisierten und schwierigen Auseinandersetzung bei.





</doc>
<doc id="7846" url="https://de.wikipedia.org/wiki?curid=7846" title="Teilung Berlins">
Teilung Berlins

Die Teilung Berlins in West- und Ost-Berlin von 1948 bis 1990 war eine Folge des Zweiten Weltkriegs und der anschließenden deutschen Teilung sowie des Konflikts zwischen dem Ostblock bzw. Warschauer Pakt und der NATO.

Das von den Siegermächten in vier Besatzungszonen geteilte Deutschland und die Vier-Sektoren-Stadt Berlin wurden damit jahrzehntelang Spielball der Auseinandersetzung zwischen den beiden Blöcken. Im Kalten Krieg verlief hier ein Teil des sogenannten „Eisernen Vorhanges“. 

Die politische Teilung Berlins manifestierte sich zunächst ab Juni 1948 in der Berlin-Blockade durch die Sowjetunion, die zur Berliner Luftbrücke und im September 1948 zur Spaltung der Berliner Stadtverordnetenversammlung führte. Sie festigte sich mit der Gründung der beiden deutschen Staaten und ihrem jeweiligen Anspruch auf Berlin. 

Die Teilung Berlins wurde mit dem Bau der Berliner Mauer ab dem 13. August 1961 vollendet. Zuvor waren unkontrollierte Übertritte zwischen West- und Ost-Berlin trotz politischer Teilung noch möglich. Die Wende in der DDR, die Öffnung der Mauer am 9. November 1989 und die Wiedervereinigung Deutschlands am 3. Oktober 1990 beendeten nach 40 Jahren die Teilung Berlins.

Der Verlauf der ehemaligen Mauer ist in Teilen des Stadtzentrums heute anhand einer Linie von doppelten Kopfsteinpflastersteinen nachzuvollziehen.



</doc>
<doc id="7847" url="https://de.wikipedia.org/wiki?curid=7847" title="Autor">
Autor

Ein Autor ( ‚Urheber‘, ‚Schöpfer‘, ‚Förderer‘, ‚Veranlasser‘) ist der Verfasser oder geistige Urheber eines sprachlichen Werkes, das aber auch illustriert sein und zuweilen mehr Bilder als Text enthalten kann (z. B. Bilderbuch, Comic, Fotoroman). Meist verfassen Autoren im weitesten Sinn „literarische“ Werke, die den Gattungen Epik, Drama und Lyrik oder auch der Fach- und Sachliteratur zugeordnet werden. Als Autoren werden zudem, wenn auch seltener und meist mit eher juristischem Beiklang, die Urheber von Werken nicht-literarischer Natur (etwa der Musik, Fotografie, Filmkunst) sowie einzelner Wissenschaftszweige (vgl. auch Softwareautoren, Gesetzesautoren, Autoren in der Zoologie als Erstbeschreiber eines Lebewesens etc.) bezeichnet. Darüber hinaus hat sich für die Erfinder von Autorenspielen "(German-style Games)" die Bezeichnung Spieleautor etabliert.

Das Verständnis von Autorschaft ist geschichtlichen Veränderungen unterworfen. Im Mittelalter verwiesen die Begriffe Autor und Autorität mit großer Selbstverständlichkeit aufeinander. Der Rechtssprache entstammend, bezeichnete "auctor" den Urheber, Verfasser oder Sachwalter eines Werkes. Dabei schloss die Wortbedeutung, anders als in der Neuzeit, grundsätzlich den Aspekt der Autorität "(auctoritas)" ein: Verfasser waren gemeint, die hohes Ansehen erworben und breite Anerkennung gefunden hatten.

Besonders die medialen Umbrüche von der Mündlichkeit zur Schrift und von der Handschrift zum Buchdruck förderten die Ablösung der Person des Autors und ihrer Autorität von ihrem (reproduzierbaren und vor Verfälschung zu schützenden) Werk, zunächst jedoch eher in Gattungen der theologischen und wissenschaftlichen Literatur. Erst seit der Genieästhetik des Sturm und Drang bildete sich ein Konzept des „autonomen, schöpferischen, über sein Werk herrschenden belletristischen Autors“ heraus. Das 19. und 20. Jahrhundert bilden die Hochphase dieses emphatischen, idealisierten Autorbegriffs.

Seit den 1960er-Jahren wurde Kritik an der Verabsolutierung der Autorpersönlichkeit laut (Roland Barthes: "Der Tod des Autors", Michel Foucault: "Was ist ein Autor?").

In Teilen der Literaturtheorie (Erzähltheorie) wird zwischen Autor und Erzähler unterschieden: Der Autor ist der Schreibende des Textes und der Erzähler der Erzählende der Geschichte und ist dabei eine vom Autor geschaffene Instanz.

Der Begriff Autor wurde von Philipp von Zesen durch den Ausdruck "Verfasser" eingedeutscht.

Autorschaft umfasst in der Gegenwart ein Recht am geistigen Eigentum. Zum Schutz des Werkes dienen das Urheberrecht (welches nicht veräußerlich ist) und das Verwertungsrecht.

Der Begriff "Autor" bzw. dessen Urheberschaft für ein in der Regel meist schriftlich niedergelegtes Werk oder dessen Konzeption gilt unabhängig von ihrer Veröffentlichung oder (bisherigen) Nichtveröffentlichung. Die Urheber- bzw. Autorschaft findet dabei jedoch grundsätzlich verschiedene Bedeutung, nicht zuletzt nach Maßgabe ihrer Zielsetzung einer nicht beabsichtigten bzw. beabsichtigten und tatsächlich erreichten Öffentlichkeitswirkung von Name und Werk.




Autoren bilden sich im europäischen Sprachraum meist im Selbststudium aus. Vereinzelt werden hierfür an Universitäten und Fachhochschulen Lehrgänge und Workshops angeboten. Qualifizierungsmöglichkeiten werden zudem in Kompaktseminaren (z. B. durch den Börsenverein des Deutschen Buchhandels oder den Verband deutscher Schriftsteller) oder berufsbegleitenden Lehrgängen (z. B. über die Freie Journalistenschule) angeboten.

An der Universität für angewandte Kunst Wien, der Universität Hildesheim und seit 1995 an der Universität Leipzig (Deutsches Literaturinstitut Leipzig) gibt es zudem nach US-amerikanischem Vorbild eine schreibhandwerkliche Ausbildung beziehungsweise einen Studiengang zum diplomierten Schriftsteller. Gasthörer können diese Seminare ebenfalls besuchen. Darüber hinaus bieten zahlreiche Schreibwerkstätten, wie z. B. das Junge Literaturforum Hessen-Thüringen oder die Marburger Sommerakademie, angehenden Autoren interaktives Training oder ein Coaching durch bereits etablierte Schriftsteller an.

Einen anderen Zugang zur Autorenschaft finden nicht wenige auch über ein Studium der Publizistik.

Seit Einführung der Personal Computer ist es für Autoren relativ einfach, ein Manuskript als digitalen Datensatz zu erstellen, der wiederum als Druckvorlage für Verlage oder von Selbstpublikationen dienen kann.

Ein Autor sucht in der Regel eine Veröffentlichung über Verlage zu erreichen. Der Verlag übernimmt für den Autor die Korrektur, die Herstellung (Layout, Druck, ISBN-Registrierung, Pflichtexemplare usw.) und den Vertrieb. Im Gegenzug tritt der Autor das Verwertungsrecht (komplett oder teilweise) an den Verlag ab. Der Autor erhält für seine Tätigkeit vom Verlag eine Vergütung und/oder Tantiemen.

Hat sich ein Verlag zur Veröffentlichung bereiterklärt, wird der Autor oft aufgefordert, das Werk zusammen mit einem Lektor zu überarbeiten und begutachten (Peer-Review) zu lassen. Sobald ein Autor erfolgreich für einen Verlag gearbeitet hat, wird es dem Autor wesentlich leichter fallen, „seinem“ Verlag zukünftige Werke auch schon in der Ideenphase zu präsentieren und zusammen mit dem Lektor auszuarbeiten. Besonders erfolgreiche Autoren (Bestseller) werden dann auch vom Verlag aufgefordert, neue Werke zu erschaffen. Dies kann für den Autor wiederum die Basis zum Aushandeln einer entsprechend besseren Vergütung sein.

Sachbuchautoren erstellen in der Regel zuerst ein Konzept zu ihrem Werk. Dieses beinhaltet einen groben Themenüberblick, Hinweise auf vergleichbare Werke und ein möglichst fertig ausgearbeitetes Inhaltsverzeichnis mit dem sich dann auch die angestrebte Seitenzahl festlegen lässt. Da der zukünftige Verlag häufig eigene Vorstellungen vom (inhaltlichen) Aufbau und der zielgruppengerechten Gestaltung eines Werkes hat, können so noch Änderungen berücksichtigt werden, da bei Vertragsabschluss noch nicht das fertige Werk existieren muss, sondern dieses dann erst als Auftragsarbeit durch den Autor entsteht. Dieses Vorgehen findet hier bei neuen wie auch bei renommierten Autoren seine Anwendung.

Bei der Annahme von Manuskripten oder Ideen noch unbekannter Autoren verhalten sich die meisten Verlage jedoch sehr zögerlich. Dieses Verhalten liegt meist in der Qualität begründet, da die Verlage nicht selten mehrere hundert solcher Texte pro Woche erhalten und davon lediglich einen pro Jahr zur Veröffentlichung auswählen, wenn sie sich von ihm auch merkantilen Erfolg versprechen.

Ein Autor als Selbstverleger geht ein unternehmerisches Risiko ein, das sich allerdings seit Einführung der Veröffentlichungsformen Book-on-Demand und E-Book erheblich senken lässt. So kann der Autor für seine Selbstpublikationen den Herstellungsprozess und vertriebsrelevante Dinge wie z. B. die Aufnahme in das Verzeichnis lieferbarer Bücher inzwischen relativ kostengünstig an "Self-Publishing-Plattformen" delegieren. Zudem muss er bei dieser Art der Herstellung nicht mehr eine von ihm in Druck gegebene und im Voraus bezahlte Auflage eines Titels verwalten und steht somit auch nicht in der Gefahr, auf ihr „sitzen zu bleiben“. Allerdings muss sich der Autor einer Selbstpublikation in jedem Fall auch selbst um die Vermarktung und Bewerbung seiner Werke kümmern – oder damit gegen zuweilen stark überhöhte Gebühren jemand anderen beauftragen.

Nach wie vor verhilft eine Veröffentlichung (allein) auf diesem Weg nur selten zu einer Steigerung der Reputation bzw. Anerkennung eines Autors – insbesondere nicht als so genannter „Vanity publisher“. So werden Autoren in Deutschland mit lediglich im Selbstverlag bzw. als Selbstpublikation erstellten Buchtiteln weder im Verband deutscher Schriftsteller noch in die "Autorendatenbank" des Friedrich-Bödecker-Kreises aufgenommen. Zudem wirkt sich der Unterschied zu einem professionellen Autor auch vor deutschen Finanzämtern aus, wenn ein "Hobbyautor" u. a. Publikationskosten von insgesamt mehreren tausend Euro nicht als Verluste bzw. Werbungskosten steuerlich absetzen kann, da er „mit seiner Autorentätigkeit keinen Totalgewinn hätte erzielen können“ und „die Bereitschaft zur Übernahme nicht unerheblicher Druckkosten spreche dafür, dass überwiegend private Interessen und Neigungen für die Tätigkeit ursächlich gewesen seien“.

Da inzwischen jedoch ganz allgemein viele Buchtitel bereits nach einem Jahr aus dem Verlagsprogramm genommen werden, damit vergriffen und im Buchhandel nicht mehr erhältlich sind, sehen sich auch renommierte Autoren immer mehr zur Erstellung von Selbstpublikationen gezwungen – insbesondere jene, die einen Teil ihrer Einkünfte durch Lesungen bestreiten und dann nicht mehr auf lieferbare Exemplare ihrer Bücher verweisen und sie verkaufen können. (Siehe hierzu z. B. auch das Label Edition Gegenwind, unter dem eine Autorengemeinschaft ihre vergriffenen Titel selbst herausbringt.)

Im Januar 2005 einigten sich Belletristikverlage und der Verband deutscher Schriftsteller darauf, dass zehn Prozent vom Nettopreis jedes verkauften Hardcover-Exemplars künftig als Honorar an den Autor eines Buches fließen sollen.
Für Taschenbücher gelten gesonderte Regelungen, bei bis zu 20.000 verkauften Exemplaren sollen die Autoren fünf Prozent erhalten. Diese Regelungen haben allerdings nur empfehlenden Charakter, in der Praxis sind auch niedrigere Tantiemen üblich. Der Erlös aus der Verwertung buchferner Nebenrechte geht meistens zu 60 Prozent, der aus anderen Nebenrechten zur Hälfte an den Autor. Sachbuchautoren handeln häufig ihr Honorar mit dem Verlag im Rahmen der Konzeptionsphase vor Aufnahme der Schreibtätigkeit aus. Um die zwölf Prozent sind bei "Erfolgsautoren" üblich, jedoch keineswegs verbindlich. Häufig wird auch ein Garantiehonorar vereinbart, das dem Autor bei Vertragsabschluss oder bis zum Abgabetermin oder nach Manuskriptabgabe ausgezahlt wird und das dann mit gegebenenfalls später anfallenden Tantiemen verrechnet wird. Dadurch ist der Autor nicht auf den kommerziellen Erfolg des Buches angewiesen, partizipiert aber dennoch an guten Verkaufszahlen.

Verwertungsgesellschaft für die Autoren verschiedener Sparten (Journalisten, Schriftsteller, Drehbuchautoren) ist die VG Wort. Sie verwertet – ähnlich der GEMA bei Musikstücken – die durch Aufführung, Sendung, Kopie und Publizierung entstandenen Tantiemen für die Autoren und schüttet die entstandenen Beträge einmal jährlich an die Autoren aus.

Vor allem bei wissenschaftlichen Publikationen kommt es immer häufiger vor, dass ein Werk mehrere Autoren und Koautoren aufweist. Um die Anzahl von Publikationen einer Person vergleichbar zu zählen, gibt es in der Bibliometrie verschiedene Zählweisen:


In vorangegangener Zeit nur von untergeordneter Bedeutung, erfährt die Mehrautorenschaft mit der Verbreitung des Internets einen beträchtlichen Zuwachs.





</doc>
<doc id="7849" url="https://de.wikipedia.org/wiki?curid=7849" title="Tonika">
Tonika

Tonika (, "tonisch" zu τόνος "tonos" ‚Spannung‘).
„Tonika heißt in der dur-moll-tonalen Musik der Grundton der Tonart, die nach ihm benannt wird, z. B. C-Dur nach c, a-Moll nach a. Die funktionale Harmonielehre versteht unter Tonika den darauf errichteten Dreiklang, den Hauptklang der Tonart (in C-Dur c-e-g, in a-Moll a-c-e).“ Sie ist die Bezeichnung für die erste Stufe einer Tonart.

Der Name "Tonika" geht auf den von Jean-Philippe Rameau (1683–1764) erdachten Begriff „l'accord tonique“ („der Akkord des Grundtones od. der Akkord mit der besonderen Betonung“) zurück, mit dem dieser das wesentliche Merkmal der Tonika zu umschreiben suchte, nämlich ihre Fähigkeit, wie ein Magnet im Zentrum "aller harmonischen Spannungsfelder" zu stehen. Daher wird die Tonika auch häufig mit dem Begriff „tonales Zentrum“ umschrieben.

Spätestens seit Einführung der Funktionstheorie ist der Begriff Tonika eine fest umrissene Größe, auch und gerade unter dem Aspekt, im Rahmen einer Kadenz einen Bezugspunkt zu den beiden Dominanten (Dominante, Subdominante) und zu anderen leitereigenen Akkorden zu bilden. Die Tonika steht nach der klassischen Harmonielehre gewöhnlich am Anfang und am Schluss eines Musikstückes. Fast alle Musikstücke der klassischen europäischen Musik besitzen eine Grundtonart.

Die klassische Harmonielehre, in der nur Oktaven, Quinten, Terzen und Sexten als Konsonanzen gelten, lässt als Tonikaklänge nur Dreiklänge zu. Unverzichtbar ist dabei der Grundton; fehlen die Quinte, die Terz oder beides, wird der entstehende Klang als Vertreter des eigentlichen Tonika-Dreiklangs aufgefasst.

Nach der modernen Harmonielehre kann der Tonikadreiklang durchaus zu einem Vierklang erweitert werden. So wird in der Popmusik die Dominante häufig in eine um das Intervall einer großen Sexte erweiterte Dur-Tonika aufgelöst. Trotzdem behält die Tonika auch in diesem Fall ihre Funktion als konsonant klingendes, tonales Zentrum. Der Grund hierfür liegt in veränderten Hörgewohnheiten: Die "Sixte ajoutée" wurde in der späten Barockmusik von Jean-Philippe Rameau theoretisch formuliert und stellte damals noch ein ausgesprochen dissonantes Intervall dar, Zuhörer heutiger Zeit empfinden dieses Intervall dagegen als absolut konsonant.

Abhängig vom Tongeschlecht oder der Musikrichtung kann die Tonika um weitere leitereigene Töne ergänzt werden. Im Jazz ist zum Beispiel die Erweiterung einer Dur-Tonika um eine große Septime üblich. Auch hier sorgen veränderte Hörgewohnheiten dafür, dass dieses eigentlich sehr dissonante Intervall die Funktion der Tonika als tonales Zentrum nicht beeinträchtigt. Auch die Erweiterung der Tonika um das Intervall einer None wird gelegentlich praktiziert.

Einen Sonderfall bildet die Erweiterung der Dur-Tonika im Blues. Hier wird sehr häufig eine kleine Septime ergänzt, die in diesem speziellen Fall nicht Bestandteil der zu Grunde liegenden Durtonleiter ist, sondern der auf dem Grundton der Tonika aufbauenden natürlichen Molltonleiter entnommen ist. Dadurch wird in diesem Tonika-Akkord praktisch Dur mit Moll vermischt. Diese kleine Septime darf nicht mit der Blue Note verwechselt werden.



</doc>
<doc id="7851" url="https://de.wikipedia.org/wiki?curid=7851" title="Micky Maus">
Micky Maus

Micky Maus () ist eine von Walt Disney und Ub Iwerks erschaffene Zeichentrickfigur in Form einer anthropomorphen Maus und gehört aufgrund ihrer weltweiten Bekanntheit zu einer der berühmtesten Kunstfiguren. Micky Maus ist die bekannteste Figur aus der Disneywelt und trat auch als Comicfigur auf. Als Notlösung erfunden und anfänglich nur in Zeichentrickfilmen zu sehen, avancierte Micky Maus rasch auch zum Comicstar und wurde im Laufe der folgenden Jahre auch international zu einem großen Erfolg.

Obwohl Micky Maus, den sein Schöpfer anfangs Mortimer Mouse nennen wollte, bereits in dem Stummfilm "Plane Crazy" auftauchte, erreichte er seine große Bekanntheit erst durch den Film "Steamboat Willie", der am 18. November 1928 im New Yorker Colony Theatre uraufgeführt wurde. Dieses Datum gilt auch als Geburtstag von Micky Maus. Bereits in "Steamboat Willie" tauchte auch Mickys späterer Widersacher "Black Pete" ("Kater Karlo") auf.

Die Popularität dieses Films war nicht zuletzt darin begründet, dass es der erste bekanntere Zeichentrickfilm mit Ton war. Die Stimme der dort ihre Freundin Minnie beschützenden Maus war die von Walt Disney selbst, und dessen Wunsch, diese auch alle hören zu lassen, hätte ihn fast ruiniert. Bei den Tonaufnahmen hatten die Röhren des Verstärkers ihren Geist aufgegeben, und das kleine Studio von Disney, seinem Bruder Roy und ihrem Partner Ub Iwerks stand damit vor der Pleite. Walt verkaufte sein Auto, um Stimme und Orchesterbegleitung produzieren zu können. Von der ersten Sprechrolle in "The Karnival Kid" (1929) bis 1946 sprach Disney in den Filmen Micky selbst. Von dem Abschnitt "Mickey and the Beanstalk" im Film "Fun and Fancy Free" übernahm James G. MacDonald bis in die Mitte der 1970er-Jahre die Sprechrolle. Ab "Mickey’s Christmas Carol" (1983) bis zu seinem Tod im Mai 2009 war Wayne Allwine für Mickys Stimme zuständig.

Disney war durch äußere Umstände mehr oder weniger „gezwungen“ worden, diese Figur überhaupt zu entwickeln. Bis dahin hatte er mit dem recht erfolgreichen "Oswald the Lucky Rabbit" sein Geld verdient. Die Rechte daran hatte er jedoch nach einem Rechtsstreit an seine ehemaligen Finanziers verloren. Daraufhin soll er in Zusammenarbeit mit Ub Iwerks, so die bis heute erzählte Geschichte, die Comic-Maus „Mortimer“ erdacht haben. Seine Frau Lillian fand den Namen zu aufgeblasen und schlug „Mickey“ vor.

Bei seinen ersten Auftritten war Micky vom Aussehen her nicht viel mehr als ein Oswald mit kürzeren Ohren und verlängertem Schwanz. Schon bald aber änderte sich das Aussehen von Micky. Zunächst bekam er umrandete Augen statt punktförmiger und war so zu einer größeren Mimik fähig. Ab dem Film "The Opry House" (1929) trägt Micky Handschuhe. Eine Eigenart aber wurde nicht abgeändert: Wie fast alle Trickfiguren hat auch Micky nur vier Finger. Dies sollte den Animationsaufwand verringern. Aus diesem Grund trägt Micky auch sehr lange nur eine einfache Hose mit zwei Knöpfen und erhält erst spät vollständige Kleidung.

Nach Mickys großen Erfolgen in den späteren 1920er- und 1930er-Jahren erreichten schließlich ursprüngliche Nebenfiguren, wie der 1934 erfundene Donald Duck, eine größere Beliebtheit. Ab den 1940er-Jahren entstanden daher nur noch wenige Filme mit Micky Maus in der Hauptrolle.

1940 sollte Micky Maus mit seinem Auftritt als Zauberlehrling in dem Film "Fantasia" an den früheren Erfolg anknüpfen. In Farbe und mit Stereoton zeigte der Film Techniken der Animation, die damals bahnbrechend waren. Der Film wurde jedoch ein Flop und spielte seine Produktionskosten zunächst nicht ein. Erst spätere Wiederaufführungen ab den 1960er-Jahren brachten den Erfolg.

Micky Maus ist neben Donald Duck, den Rugrats und den Simpsons eine der wenigen Trickfilm- bzw. Comicfiguren, die es zu einem Stern auf dem berühmten Walk of Fame in Hollywood gebracht hat (bei der Adresse 6925 Hollywood Blvd.).

Disney hatte schon bei der Vermarktung des glücklichen Hasen erste Erfahrungen damit gemacht, eine Figur als Marke aufzubauen; bei Micky wurde das Merchandising zu einem enormen Geschäft. 1930 erschien das erste Lizenzprodukt: gegen eine Gebühr von 300 Dollar durfte ein Unternehmen Schulmappen mit der Maus bedrucken. Drei Jahre später war die Markenmacht so gewachsen, dass sie ein ganzes Unternehmen retten konnte. Die Uhrenfabrik Ingersoll-Waterbury widerstand der sicher drohenden Pleite dank der Lizenz zur Produktion von Micky-Maus-Uhren. Innerhalb weniger Jahre verdiente Disney mit solchen Geschäften Millionen. Neben diesen wirkte Micky in zahlreichen Videospielen mit, unter anderem auch in der erfolgreichen Kingdom-Hearts-Reihe.
Mit "Micky Epic" erschien im November 2010 für Nintendo Wii ein Adventure, das auch die Comic/Zeichentrick-Anfänge von Micky genauer unter die Lupe nimmt. Im November 2012 erschienen mit "Disney Micky Epic 2: Die Macht der 2" (Wii, Wii U, Xbox 360, PS3, PS Vita) und "Disney Micky Epic: Die Macht der Fantasie" (3DS) zwei weitere Ableger des Franchises.

Nur zwei Jahre nach seinem Leinwanddebüt, am 13. Januar 1930, erschien der erste Comic mit Micky Maus in amerikanischen Tageszeitungen (auch Plane Crazy betitelt), bald lasen weltweit Millionen Menschen die Geschichten. Die Zeitungscomicstrips wurden zum großen Teil von Floyd Gottfredson gezeichnet, der Figuren aus den Filmen übernahm (z. B. "Goofy") und neue (wie "Das schwarze Phantom", "Kommissar Hunter", "Gamma") einführte. Für die Comic-Hefte war insbesondere Paul Murry von Bedeutung.

Da ab 1950 in Europa eine hohe Nachfrage nach Disney-Comics entstand, die mit dem Material aus Amerika nicht befriedigt werden konnte, wurden ab den 1950er Jahren viele Comics in Italien gezeichnet. Hier ist insbesondere Romano Scarpa zu nennen. Heutzutage spielen die Vereinigten Staaten von Amerika als Produzent von Disney-Comics keine Rolle mehr. Die in Deutschland erscheinenden Geschichten stammen fast ausschließlich aus Dänemark und Italien, z. T. noch aus den Niederlanden und vereinzelt anderen Ländern (Frankreich, Brasilien).

Der Produktionsort lässt sich aus dem Storycode ableiten, der meistens im ersten Bild der Geschichten eingefügt ist. So stammt z. B. eine Geschichte mit dem Code I TL 1723 aus Italien, eine mit dem Code H 23148 aus den Niederlanden. Geschichten mit dem Buchstaben D wurden für den dänischen Egmont-Verlag produziert, der auch Zeichner aus anderen Ländern - u. a. Amerika - beschäftigt.

In Deutschland erscheinen die Comics seit 1951 regelmäßig in der Heftreihe „Micky Maus“. Schon in den 1930er Jahren hatte es vereinzelte Abdrucke von Micky-Comics in Zeitungen gegeben und 1937 eine kurzlebige Schweizer "Micky Maus Zeitung".

In den LTB-Geschichten tauchen nicht selten einige seiner Vorfahren auf, zum Beispiel als Ritter Micky von Wohldavien ("Der Kampf der Finsternis gegen das Licht", 160) oder als der 18-jährige, auf verlangen des Volks zum Ritter geschlagene Mickos ("Der Ritter ohne Furcht und Adel", LTB 203), der beschließt, ausschließlich anderen zu helfen, bevor er bei einem Sturz vom Turm beim Versuch, die Königin zu retten, fast ums Leben kommt.

In deutschen Fassungen der Serien, in denen Micky Maus auftaucht, wird er seit Mitte der 1990er Jahre von Mario von Jascheroff gesprochen.

Nach den ursprünglichen gesetzlichen Regelungen der USA wäre Mickey Mouse heute kein markenrechtlich geschütztes Produkt mehr. Die Verlängerung des Schutzes wurde jedoch durch eine Gesetzesänderung ermöglicht, den sogenannten "Sonny Bono Copyright Term Extension Act".




</doc>
<doc id="7852" url="https://de.wikipedia.org/wiki?curid=7852" title="Superman">
Superman

Superman ist der Name einer Comicfigur, die in den 1930er-Jahren von den beiden US-Amerikanern Jerry Siegel und Joe Shuster geschaffen wurde.

Die Figur wird gemeinhin als der erste Superheld der Comicgeschichte betrachtet und zählt zu dem Kreis der fiktiven Charaktere mit dem weltweit höchsten Wiedererkennungswert.

Die Idee und das ursprüngliche Konzept für Superman wurde in den frühen 1930er-Jahren von den beiden amerikanischen Teenagern Jerry Siegel und Joe Shuster entwickelt. 1932 begannen Siegel und Shuster – zwei begeisterte Science-Fiction-Fans, die sich während ihrer gemeinsamen Schulzeit in Cleveland, Ohio, kennengelernt hatten – in ihrer Freizeit ein selbstgestaltetes Laienmagazin herauszugeben. In diesem Organ, das sie kurz "Science Fiction" nannten, veröffentlichten sie selbst verfasste Geschichten in ihrem Lieblingsgenre. Während Siegel als Autor fungierte, verlagerte Shuster sich darauf, Bildmaterial zur Illustration von Siegels Geschichten zu zeichnen.

Im Jahr 1933 erstellten sie die erste Ausgabe von "Science Fiction". Sie platzierten unter anderem eine Kurzgeschichte mit dem Titel "The Reign of the Super-Man". Diese handelte von einem glatzköpfigen Bösewicht, der plant, mit Hilfe seiner übermenschlichen mentalen Fähigkeiten (Telepathie, Telekinese etc.) die Herrschaft über die Menschheit zu erlangen.

In den folgenden Jahren unterzogen Siegel und Shuster ihre Idee einer Generalrevision: Zum einen entschieden sie, ihre Figur von einem Schurken zu einem „Kämpfer für das Gute“ zu machen, und zum anderen ersetzten sie seine mentalen „Superkräfte“ durch physische Fähigkeiten, in der Annahme, dass diese ein größeres Erzählpotenzial besitzen würden und sich außerdem optisch eindrucksvoller in Szene setzen lassen würden. Bei der Ausarbeitung der neuen Figur war der US-amerikanische Comiczeichner Tony Strobl beteiligt, den Siegel und Shuster während ihres gemeinsamen Studiums kennengelernt hatten. Strobl bezweifelte allerdings den Erfolg dieser Figur.

Mit ihrem veränderten Konzept versuchten Siegel und Shuster in den Jahren 1934 bis 1938 verschiedene Zeitungshäuser dazu zu bringen, die Figur als Comicstrip in ihren Zeitungen zu veröffentlichen. Nachdem diese Versuche nicht gefruchtet hatten, versuchten die beiden, verschiedene Verlage, die zu dieser Zeit in Amerika das neue Medium des Comic"heftes" auf dem Markt zu etablieren begannen, von ihrer Idee zu überzeugen. Ihren Bemühungen waren dabei zunächst jahrelang nur Misserfolg beschieden: Ein Verlag nach dem anderen lehnte die Veröffentlichung von Geschichten um die Superman-Figur ab, da diese „unreif“, „pubertär“ und „kindisch“ sei und keine Aussichten auf Erfolg habe. Erst der Verlag National Publications fand sich im Frühjahr 1938 bereit, Siegels und Shusters Superman eine Chance zu geben. Nachdem der Verlag die Idee in den vorherigen Jahren mehrmals abgelehnt hatte, sah er sich durch zwei Gründe veranlasst, das Konzept zu testen: Zum einen waren andere Konzepte für Comicreihen aus der Werkstatt von Siegel und Shuster, die National Publications in sein Programm genommen hatte – so die in den "Detective Comics" erscheinenden Abenteuer um den geheimnisvollen Doctor Occult – leidlich erfolgreich gewesen. Ein noch wichtigerer Anlass für den Verlag, auf das Superman-Konzept zurückzugreifen, dürfte der Zeitdruck gewesen sein, dem man sich bei der Vorbereitung der ersten Ausgaben der "Action Comics" ausgesetzt sah: Obwohl der Start der Serie kurz bevorstand, gab es nicht genug Material, um alle Seiten der Hefte füllen zu können. Von den 200.000 gedruckten Comics dieser Ausgabe existieren nach Schätzungen noch 100 Exemplare. 1938 kostete das Heft 10 Cent. 2014 wurde auf eBay eine dieser Erstausgaben für 3,2 Millionen Dollar versteigert.

Nachdem "Action Comics" #1 zu einem phänomenalen Verkaufserfolg geworden war, stellte National Publications Marktstudien an, um die Gründe für den Erfolg des Heftes herauszufinden. Zur Überraschung aller Beteiligten wurde festgestellt, dass die neuartige Figur Superman, die das Titelbild des Heftes geziert hatte, der Hauptgrund für den reißenden Absatz des Heftes war. National Publications machte die Superman-Geschichten daraufhin zu einem festen Bestandteil der "Action Comics", in denen sie fortan in jeder Ausgabe und als „Hauptserie“ erschienen, das heißt als vorrangige Reihe gegenüber den anderen, im hinteren Teil der Hefte abgedruckten Reihen mit anderen Figuren.

Siegel und Shuster fuhren fort, die Superman-Geschichten gemeinsam mit einem wachsenden Mitarbeiterstab bis in die 1940er-Jahre zu gestalten. Die Rechte an ihrer Schöpfung traten sie für einen Scheck über 130 Dollar an den Verlag ab. Später wurde dies zum Anlass eines langjährigen Rechtsstreites, durch den Siegel und Shuster die Rechte an ihrer Figur zurückzugewinnen versuchten. Erst 1978 gewährte Time Warner, der Besitzer von DC-Comics, dem Verlag, in dem die National Publications später aufgegangen waren, den beiden eine jährliche Rente von 24.000 Dollar und nannte die Namen der beiden als Schöpfer der Figur in den Initial-Credits jeder Superman-Geschichte.

In den Vereinigten Staaten lag die Auflage der "Action Comics" 1940 bereits bei 800.000 Ausgaben, während man durch die dreimonatlich erscheinenden "Superman"-Hefte mit jeder neuen Ausgabe mehr als eineinviertel Millionen Exemplare absetzen konnte. Unter den in Übersee kämpfenden G.I.s waren die verschiedenen Superman-Comicserien während der Kriegsjahre die am häufigsten gelesenen Periodika. Hinzu kamen Superman-Comicstrips in mehr als 250 Sonntagszeitungen sowie eine dreimal pro Woche ausgestrahlte Superman-Radiosendung und, ab 1941, eine von den "Fleischer Studios" produzierte Reihe von Zeichentrickfilmen.

Auch außerhalb von Amerika wurde Superman in kurzer Zeit zu einem Verkaufsschlager: Von den englischsprachigen Ländern ausgehend fanden die Superman-Comics bald auf der ganzen Welt ihr Publikum. So waren Superman-Comics sogar im fernen Japan bereits in den frühen 1940er-Jahren erhältlich, wo unter anderem der japanische Kaiser Hirohito zu den begeisterten Lesern der Abenteuer des "All American Heroes" gehörte.

In Deutschland war die Veröffentlichung der Superman-Comics zunächst auf Veranlassung des nationalsozialistischen Regimes untersagt. Propagandaminister Joseph Goebbels, dem die Kontrolle oblag, welche Druckerzeugnisse damals in Deutschland erscheinen durften, brachte mit dem Ausruf „"Superman ist ein Jude!"“ in einer Reichstagssitzung des Jahres 1942 unmissverständlich zum Ausdruck, was das Regime von dem amerikanischen Superhelden dachte. Nach einer nach nur drei Heften eingestellten Superman-Serie in den 1950er-Jahren, die den Titel "Supermann" getragen hatte, erscheinen die Geschichten um Superman hierzulande seit 1966 nahezu ohne Unterbrechungen.

Während des Kalten Krieges wurde Superman erneut zur Symbolfigur der amerikanischen Werte und Lebensart. Diesmal als Exponent der „freien Welt“ des amerikanischen Einflussbereiches im Gegensatz zur „unfreien Welt“ der Sowjetunion und ihrer Satellitenstaaten. Obwohl die Superman-Comics im Gebiet des Warschauer Paktes nicht offiziell erscheinen durften – der Generalsekretär der KPdSU, Nikita Chruschtschow, erklärte hierzu gegenüber dem Superman-Chefredakteur Mort Weisinger während eines Besuchs Weisingers in Moskau „Auch der Mann aus Stahl ist nicht in der Lage, den Eisernen Vorhang zu durchbrechen“ – zirkulierten die Superman-Comics im „Osten“ in großer Zahl durch private Verbreitung unter der Hand.

Für das Erscheinungsbild und den Charakter ihres Helden griffen Siegel und Shuster auf eine Vielzahl fiktiver Figuren und realer Personen zurück.

Einflüsse für den eigentlichen Superman waren Hugo Danner, der mit gewaltigen physischen Fähigkeiten ausgestattete Held aus Philip Wylies Roman "Gladiator" aus dem Jahr 1930, sowie der Schauspieler Douglas Fairbanks, an dessen dynamisch-akrobatischen Leinwandauftritten als Freibeuter, Musketier und Ähnlichem mehr sich Shuster bei der Gestaltung von Supermans „Himmelsturnereien“ orientierte.

Zu den weiteren Inspirationen, die weniger direkt in Superman einflossen, zählten die Hauptfiguren von abenteuerlichen Groschenheften wie Edgar Rice Burroughs' "Tarzan" und Johnston McCulleys "Zorro", von dem die Idee der Doppelidentität mit entlehnt wurde: Wie der junge Held der Zorro-Geschichten bei Tag den sanftmütigen Don Diego gibt und bei Nacht zu Zorro, dem feurigen Kämpfer für das Gute wird, ließen Siegel und Shuster Superman ein Doppelleben als introvertierter Zeitungsreporter Clark Kent einerseits und als verwegener Streiter für Recht und Gerechtigkeit in Gestalt des Superman führen. Das Erscheinungsbild von Supermans Alter Ego als bebrillter Softie-Reporter empfanden Siegel und Shuster – beide selbst Brillenträger – dabei einerseits sich selbst und andererseits Auftritten von Harold Lloyd in einigen seiner Filme aus den 1920er-Jahren nach, in denen dieser schlaksig und mit dicken Brillengestellen den Überweichling mimt. Den Namen Clark Kent setzten die beiden wiederum aus den Vornamen der Schauspieler Clark Gable und Kent Taylor zusammen.

„Superman“ ist die gängige englische Übersetzung des deutschen Wortes „Übermensch“. Es ist nicht klar, ob den Schöpfern der Superman-Figur Nietzsches Ideen vom Übermenschen bekannt waren und wie weit sie davon – direkt oder indirekt – beeinflusst wurden.

Den Namen für die Stadt, in der Clark Kent lebt und arbeitet, haben die zwei Erfinder aus einem ihrer Lieblingsfilme: "Metropolis" von Fritz Lang.

Im Anschluss an den gewaltigen Erfolg der ersten Superman-Geschichte in "Action Comics" #1 wurden die Geschichten um den „Mann aus Stahl“ zum Hauptinhalt der Serie. Andere Features wie des DC-Universums Zatara oder Congo Bill traten demgegenüber in den Hintergrund, was sich unter anderem darin widerspiegelt, dass Superman praktisch ausnahmslos alle Titelbilder der Serie für sich vereinnahmen konnte. Seit den 1950er-Jahren sind die Superman-Geschichten, abgesehen von gelegentlichen „Backup-Reihen“ um andere Figuren, die in unregelmäßigen Abständen einmal kurzzeitig in die Serie eingebaut werden, der einzige permanente Inhalt der Serie. Mit einer mehr als siebzigjährigen Veröffentlichungsgeschichte sind die "Action Comics" mittlerweile die am längsten kontinuierlich erscheinende Comicserie weltweit. Mit einer Ausgabenzahl, die sich der Heftnummer #900 annähert, besitzt die Serie zudem den zweithöchsten Nummernstand einer Comicserie überhaupt. Allerdings läuft seit 2011 infolge von "The New 52" die zweite Reihe.

Bereits 1939 begann National Publications/DC zudem damit, sein Superman-Programm kontinuierlich auszubauen. Mit dem Start der gleichnamigen Serie "Superman" begann eine lange Reihe weiterer Veröffentlichungen beziehungsweise Veröffentlichungsformate um Siegel und Shusters Superhelden. Spätere Serien waren und sind unter anderem "World’s Finest Comics" (gemeinsame Geschichten von Superman mit seinem dunklen Mitstreiter Batman), "Superman: Man of Steel", "Adventure Comics" (mit Geschichten um Supermans jugendliches Ich Superboy) sowie die Spin-off-Serien "Steel", "Supergirl" und "Superboy", die Figuren in den Mittelpunkt stellen, die ursprünglich als Nebenfiguren von Superman in dessen Comics eingeführt worden sind.

Der Erfolg Supermans zog eine ganze Reihe von weiteren Superhelden nach sich: 1939 Batman, 1940 The Flash und Green Lantern, 1941 Wonder Woman. Der Konkurrenzverlag "Timely" (später Marvel), schuf die Superhelden Captain America, Human Torch und Namor (im Original: "Namor the Sub-Mariner"). Während des Zweiten Weltkrieges gab es insgesamt 160 verschiedene Superheldentitel von mehr als zwei Dutzend Verlagen, mit einer Gesamtauflage von 300 Millionen Heften und einem jährlichen Umsatz von 30 Millionen Dollar.

Im Verlauf der Jahre unterzog sich die Figur des "Superman" immer wieder verschiedenen Wandlungen. Neben Anpassungen an zeitgeschichtliche Entwicklungen umfassten diese zum Teil auch Veränderungen am Aussehen, den Fähigkeiten und Schwächen, der Hintergrundgeschichte und den Gegnern und Ähnliches. Die wesentlichen Änderungen fanden entlang der verschiedenen Entwicklungsperioden amerikanischer Mainstream-Comics statt. Insofern unterscheidet man heute zwischen der Superman-Darstellung des Golden Age (1935–1953), des Silver Age (1953–1970), des Bronze Age (1970–1986) sowie des Modern Age (seit 1986).

Anfang der 1940er-Jahre wurde das Comic "World’s Finest Comics" publiziert, das zunächst getrennte Geschichten der verschiedenen Superhelden vorsah – dabei immer auch jeweils Geschichten von Superman und Batman, den beiden Hauptfiguren des DC Comics-Verlages. Während der 1950er-Jahre nahm die Popularität der Superhelden-Comics zunehmend ab. Infolgedessen kam es zu Ausgabenkürzung und zu einer Zusammenlegung der Geschichten rund um Superman und Batman zu einer einzigen, gemeinsamen Geschichte. Ab da begann bei DC die Vorstellung, dass alle ihre Superhelden in einem gemeinsamen Universum existieren. Zugleich wurden damit die Weichen für die spätere "Gerechtigkeitsliga" (auch: "Liga der Gerechten"; Original: "Justice League of America") gestellt – eine Superheldenvereinigung, der im Verlauf der Zeit auch Superman und Batman beitraten, und zu deren fünf Gründungsmitglieder populäre Figuren wie Green Lantern, The Flash und Aquaman gehören.

Mit Beginn des "Modern Age" Mitte der 1980er-Jahre erfuhr die Figur des Superman – neben vielen anderen – eine grundlegende Änderung. Auslöser war ein Umdenken des herausgebenden Verlages DC Comics, welches sich in einer Miniserie bzw. einem Ereignis namens "Crisis on Infinite Earths" („Krise der Parallelerden“) manifestierte. Bis zu diesem Zeitpunkt hatte es in der DC-Welt mehrere unterschiedliche Paralleluniversen gegeben, mit denen die Unterschiede zwischen den älteren und der damals aktuellen Version von Superman, aber auch den Versionen anderer Figuren, erklärt werden sollten. Obwohl seit Jahren praktisch alle Geschichten in einem einzigen Universum (der sogenannten Erde-1) stattfanden und es nur gelegentliche Ausflüge nach Erde-2 (Golden-Age-Versionen der Superhelden) oder Erde-3 (moralische Inversion: Bekannte Superhelden sind dort böse, Superfeinde wie Lex Luthor hingegen gut) gab, wurde dieses Konstrukt als zu kompliziert für die Leser empfunden und bereinigt. Mit der "Crisis" wurde die Herkunft vieler Figuren, wie Green Lantern (auch: "Grüne Leuchte") oder Aquaman, umgeschrieben und zusammengeführt. Andere Figuren, wie Supergirl, ließ man sterben. Viele Änderungen hatten aber unbedachte Auswirkungen, die in mehreren, serienüberspannenden Geschichten korrigiert werden sollten. Dieser Prozess ist noch nicht abgeschlossen, derzeit nähert sich Superman samt assoziierter Figuren und Titel wieder seiner früheren Version an.

Supermans Herkunftsgeschichte wurde im Laufe der Jahre immer wieder verändert, bleibt im Kern aber die folgende:

Der auf dem fernen Planeten "Krypton" lebende Wissenschaftler "Jor-El" entdeckt, dass die Zivilisation der Kryptonier durch eine nahende Katastrophe dem Untergang geweiht ist.

Um seinen dreijährigen Sohn "Kal-El" zu retten, schickt er ihn mit einem Raumschiff zur Erde. Das Raumschiff landet dort im amerikanischen Bundesstaat Kansas, am Stadtrand Smallvilles. Der Junge wird als Waise von dem kinderlosen Farmerehepaar Kent gefunden, das ihm den Namen Clark gibt und ihn als eigenen Sohn großzieht.

Schon bald erkennt er seine besonderen Kräfte. Nach seiner Schulzeit bereist er die Welt und entschließt sich, das Verbrechen zu bekämpfen. So entsteht seine zweite Identität: Superman, "der Mann aus Stahl". Superman kann fliegen (bis 1950 nur besonders hoch und weit springen), ist stark wie eine Lokomotive, schneller als eine Pistolenkugel und nahezu unverwundbar.

Im Alltag arbeitet er für den "Daily Planet", die wichtigste Zeitung seiner Heimatstadt "Metropolis" (Synonym für „die Großstadt“), was ihm die Möglichkeit bietet, gleich an Ort und Stelle zu sein, wenn er gebraucht wird. Ebenso berühmt wie der Mann aus Stahl selbst ist seine Kollegin beim "Daily Planet", "Lois Lane", die ihn zwar anhimmelt, aber von seinem vermeintlich tollpatschigen Alter Ego "Clark Kent" nichts wissen will.

Aufgrund seiner großen Popularität wurde Superman bereits während des Zweiten Weltkrieges, insbesondere nach dem amerikanischen Kriegseintritt im Dezember 1941, zu Propagandazwecken instrumentalisiert. In den Jahren 1942 bis 1945 entstanden zahlreiche Geschichten, in denen er auf Seiten der Alliierten gegen deutsche und japanische Truppen kämpft. Führende feindliche Politiker wie Hitler, Tojo oder Mussolini durfte der Held vor allem auf den Coverbildern seiner Comics desavouieren, zum Beispiel, indem er ihnen zusammen mit Batman auf der Titelseite einer Ausgabe der "World’s Finest Comics" auf einem Jahrmarkt Torten ins Gesicht schleudert. Bei den amerikanischen Truppen, die die Comics häufig von ihren Angehörigen als Teil von Verpflegungspaketen mitgeschickt bekamen, erfreuten sich solche Geschichten und Karikaturen, trotz ihrer Verharmlosung des Kriegsgeschehens, großer Beliebtheit.

In der Zeit des Kalten Krieges unterblieb eine direkte Konfrontation Supermans mit dem Kommunismus als dem ideologischen Gegner. Stattdessen brachten die Macher der Comics ihre Haltung mit subtileren Mitteln zum Ausdruck. So ließen sie Superman auf den Titelblättern seiner Comics in nationalstolzer Manier mit der amerikanischen Flagge oder mit dem Weißkopfadler, dem nationalen Symbol der Vereinigten Staaten, auf seinen Schultern oder seinem Arm auftreten. Die Haltung der wertemäßigen Überlegenheit des Westens über den Ostblock wurde auch mit der selbstbewussten Formel unterstrichen, Superman kämpfe für „Wahrheit, Gerechtigkeit und die amerikanische Lebensart“ („Truth, Justice and the American Way“), eine Losung, die Ende der Vierzigerjahre aufkam und in den 1950ern durch die Fernsehserie mit George Reeves popularisiert wurde.

Eine gegenteilige Interpretation verfolgte der Psychiater Fredric Wertham, der in seinem 1954 erschienenen Buch "Seduction of the Innocent" Superman als jugendgefährdend ansah.

Neuere Interpretationen des Superman-Stoffes vermeiden demgegenüber eine ideologische Vereinnahmung Supermans: In den Comics erscheint er seit den 1980er-Jahren verstärkt als Beschützer der ganzen Welt, anstatt als ein dezidiert amerikanischer Held. Einige in den Vereinigten Staaten – zumindest politisch – mehrheitsfähige Positionen lehnt Superman dabei seit jeher ab: So weigert er sich beharrlich, selbst seine schlimmsten Widersacher zu töten, was im Kontrast zu der in zahlreichen US-Bundesstaaten existierenden Todesstrafe steht.

Seine übermenschlichen Kräfte werden dadurch erklärt, dass sein Körper unter unserer gelben Sonne (im Gegensatz zur roten Sonne Kryptons) mit Energie aufgeladen wird. Dies sollte an seiner extrem dichten Molekularstruktur liegen. Auch war die Schwerkraft auf Krypton sehr viel höher als auf der Erde. Jeder Kryptonier hat damit auf der Erde dieselben (oder ähnliche) Fähigkeiten. Diese wurden oft verändert, bleiben im Kern aber folgende:


Durch das Kombinieren verschiedener Eigenschaften ergeben sich weitere Superkräfte, wie etwa eine „Superpuste“, bei der teilweise riesige Luftmengen eingeatmet, verdichtet und dann wieder kontrolliert so ausgeatmet werden, dass der entstehende Luftstrom zur Kühlung bis hinab zu extrem niedrigen Temperaturen genutzt werden kann. Außerdem dient diese komprimierte Luft als Atemluftvorrat, da Superman nicht in allen Versionen ohne Sauerstoff leben kann.

Je nach Version/Medium sind die Kräfte unterschiedlich stark ausgelegt. So schwankt die Stärke von „kann Wolkenkratzer anheben“ bis „kann Planeten aus der Bahn schieben“ und die Fluggeschwindigkeit von Überschall- bis Überlichtgeschwindigkeit. In manchen Geschichten wird Superman nachts schwächer und muss sich erst wieder von der Sonne „aufladen“ lassen.

1945 tauchte zum ersten Mal die mysteriöse Substanz "Kryptonit" in der Superman-Radio-Show auf. Man hatte es erfunden, um den kurzzeitigen Stimmenwechsel der Hauptperson zu erklären, da der ursprüngliche Sprecher wegen einer Krankheit ausgefallen war.

Von Kryptonit gibt es viele verschiedene Formen:
Die häufigste und zugleich wohl bekannteste Variante ist "Grünes Kryptonit". Es wirkt wie ein radioaktives Gift. Es schwächt Superman und seine Körperaura und kann ihn sogar töten, wenn er ihm über einen längeren Zeitraum ausgesetzt ist.

Das ebenfalls relativ bekannte "Rote Kryptonit" hat in den Comics sehr unterschiedliche Folgen. Spätestens seit der Serie "Lois & Clark" macht es ihn schrittweise (je nach Dauer der Aussetzung) böse und lässt ihn jede Hemmung verlieren. Es ist gleichzusetzen mit dem künstlichen grünen Kryptonit aus dem Film "Superman III". In der Fernsehserie "Superman – Die Abenteuer von Lois und Clark" (Folge 2.20 "Das Rote Kryptonit") lässt es Superman gleichgültig bis depressiv werden. Durch die Nähe zu rotem Kryptonit wird er unkontrollierbar und legt dabei auch seine sonstige Zurückhaltung Frauen gegenüber ab.

In der TV-Serie "Smallville" tritt noch eine weitere Form auf, das Blaue Kryptonit, in dessen Nähe Kryptonier ihre Kräfte verlieren und menschliche Eigenschaften annehmen.

Der klassische Prä-"Krisen"-Superman verlor zudem unter roter Sonnenstrahlung seine Superkräfte. Unter orangefarbener Sonne halbierten sich seine Kräfte. Auch in neuerer Zeit wird Superman durch rotes Sonnenlicht geschwächt.

Superman ist darüber hinaus verletzbar durch Magie. Außerdem kann sein Mitleid ausgenutzt oder seine Psyche angegriffen werden. Wie jedes Lebewesen hat auch Superman damit zu kämpfen, nicht überall zugleich sein oder nicht dauerhaft ohne Atmung, Schlaf und Nahrung leben zu können.

In Supermans 80-jähriger Geschichte wurde sein Kostüm, das sich von Akrobatenkostümen herleitet, von vielen gezeichnet – aber die wichtigsten Elemente seines Kostüms blieben unangetastet. Superman trägt ein blaues Kostüm komplettiert von roten Überhosen, roten Stiefeln, einem gelben Gürtel und einem langen roten Cape. Weiterhin gibt es ein hoch stilisiertes Superman-Insigne – bestehend aus einem großen roten „S“ umschlossen von einem gelben Schild mit einem roten Rand – welches auf seiner Brust angebracht ist. Das in frühen Abenteuern wie ein mittelalterliches Familienwappen anmutende Superman-Symbol entwickelte sich stetig zum uns bekannten und wurde über die Jahre immer größer, bis es Ende der 1990er-Jahre fast die gesamte Brust umspannte.

Laut "Superman" #146 (Vol. 1) erfand Martha Kent das Kostüm, als sie einen Super-Strampelanzug für den wachsenden Superman entwickelte, weil die Kleidung, die sie im Geschäft kaufte, immer wieder beim Spielen zerstört wurde. Glücklicherweise hatten die Kents die Weitsicht, die drei Decken – eine rote, eine blaue und eine gelbe – mit denen Superman in seiner Rakete die Erde erreichte, aufzuheben. Jahre später änderte Martha Kent das Kostüm, und es wurde zu dem heute bekannten Superman-Kostüm.

Supermans Kostüm wird oft als unzerstörbar beschrieben, da es, wie der Mann aus Stahl selbst, vom Planeten Krypton stammt. Dadurch hält es Feuer, Pistolenkugeln und sogar Explosionen stand. Manche Inkarnationen zeigen jedoch eine Vielzahl an Kostümen in Clark Kents Kleiderschrank, die er zusammen mit seiner Mutter schneiderte. Da dieses Material von der Erde stammt, hat er durch seine Arbeit einen hohen Verschleiß.

Dieser Wechsel wurde in der Comicserie "Man of Steel" von John Byrne etabliert und erklärt, dass Supermans Körper eine schützende Aura produziert, die nicht nur ihn unverwundbar macht, sondern sich auch einige Millimeter reicht und damit enganliegende Kleidung schützt. Diese Variante wird auch in der Fernsehserie "Superman – Die Abenteuer von Lois & Clark" verwendet.

1998 folgte dann in "Superman" #123 (Vol. 2) ein kompletter Wandel seines Kostüms. Superman wurde in ein Wesen aus Energie verwandelt und musste in einen blau-weißen, extra dafür entworfenen, Anzug gesteckt werden, der seine neue Existenzform zusammenhielt. Später wurde er vom Cyborg Superman in ein blaues und ein rotes Energiewesen gespalten, welche aber durch die "Millennium Giants" wieder zusammengeführt wurden und die auch seine ursprüngliche kryptonische Gestalt wiederherstellten.

Für "Superman Returns" (2006) wurde ein moderneres Kostüm mit einem dunkleren Rot und einem modifizierten Logo entworfen. Auch in den Comics erfolgte eine Veränderung, als zum Neustart des DC-Universums ("The New 52", 2011) Superman ein nun völlig neues Kostüm erhielt, welches nicht von seinen Eltern, sondern von Brainiac stammt.

Der letzte Wandel folgte dann für "Man of Steel" (2013). Das Kostüm wurde dahingehend verändert, dass man die rote über dem Dress liegende „Unterhose“ ganz entfernte und auch den Gürtel nicht mehr stechend Gelb, sondern größtenteils in und an das dunklere Blau des Kostüms anpasste. Auch das Superman-Logo auf der Brust wurde leicht verändert.

Weitere Darstellungen von Superman in Filmen, Serien, Videospielen oder Musik.

Als einer der bedeutendsten Superhelden bot Superman schon immer viele Gelegenheiten für Parodien. So erschien ab 1962 die Figur "Wunderwarzenschwein" (Wonder Wart-Hog) des amerikanischen Zeichners Gilbert Shelton.

Paul Murry erfand 1965 Goofys Superhelden-Ego Supergoof, dessen Geheimnis Erdnüsse waren, die im amerikanischen Original mit Staub eines Meteoriten bestrahlt waren und nach ihrem Verzehr Superkräfte verliehen. Supergoof besitzt dieselben Fähigkeiten und Kräfte wie Superman, allerdings wirken die Nüsse nur eine begrenzte Zeit und seine geistigen Fähigkeiten bleiben dabei dieselben.

In Akira Toriyamas Dr. Slump (1980–1984) taucht die Figur des "Suppaman" auf, der sich in einen „Superhelden“ verwandelt, wenn er Umeboshi isst. "Suppaman" trägt ein Kostüm mit dem japanischen Schriftzeichen für „su“, außerdem fällt ihm eine Locke ins Gesicht.

"Radioactive Man", die Lieblingscomicfigur von Bart Simpson, ist eine weitere Parodie auf Superman, was auch dessen bürgerlicher Name "Claude Kane III" (dieselben Initialen wie Clark Kent) zum Ausdruck bringt.
Radioactive Man hat seit der ersten Staffel immer wieder Auftritte in der Trickfilmserie "Die Simpsons" bzw. in den vom Panini Verlag herausgegebenen Simpsons-Comicheften.

In Monty Pythons Sketch "Bicycle Repair Man" ist der Fahrradmechaniker der Superheld, während alle „gewöhnlichen Bürger“ Superman-Kostüme tragen.

Die Drawn-Together-Figur "Captain Hero" parodiert allgemein die proamerikanischen Comicsuperhelden, aber ganz besonders Superman. Dies zeigte sich eindeutig in einer Folge, in der Captain Hero reumütig seine Kräfte in einer Eiskristallhöhle (wie aus den bekannten Superman-Filmen) abgibt und dadurch im Rollstuhl landet, den er durch Pusten fortbewegen konnte. Aufgrund des Todes von Christopher Reeve, der kurz vor der Erstausstrahlung dieser Folge verstarb, wurde die Folge zunächst nicht gesendet, später jedoch wurde entschieden, die Episode in der darauffolgenden Staffel nachzuholen.

Der Animationsfilm "Megamind" parodiert Superman ebenfalls mit dem Charakter des "Metro Man". Dieser ist in der Lage zu fliegen, hat übermenschliche Kräfte – und tolles Haar, wie dessen Gegenspieler bemerkt. Doch schon zu Beginn des Films hängt Metro Man seine Superheldenkarriere an den Nagel und täuscht seinen Tod vor, da ihm sein Job zu routinemäßig wurde. Wie Superman wurde Metro Man auf die Erde geschickt und kommt ursprünglich als Außerirdischer von einem anderen Planeten.

Auch "Underdog", eine Zeichentrickserie von 1964, ist eine Superman-Parodie. Der Cartoonhund Shoeshine Boy ist in Wirklichkeit der Superheld Underdog. Wenn Bösewichte auftreten, zieht sich Shoeshine Boy in eine Telefonzelle zurück, um sich dort in den Superhelden zu verwandeln und das Böse zu bekämpfen. Underdog spricht, im Gegensatz zu seinem Alter Ego Shoeshine Boy, immer in Reimen. 2007 wurde auf Grundlage dieser Serie ein Spielfilm gedreht, nur dass Shoeshine ein echter Hund ist und nicht wie manche Tiere in der Serie ein Mix aus Mensch und Tier.





</doc>
<doc id="7853" url="https://de.wikipedia.org/wiki?curid=7853" title="Dominante">
Dominante

Dominante ( (Adj.) oder einfach: "dominante" (Subst.) von ; u. ; ), auch Oberdominante, bezeichnet in der Harmonielehre die fünfte Stufe einer Tonleiter und die Funktion aller darauf basierenden Akkorde. Die Dominante liegt eine Quinte über der Tonika und bildet zusammen mit dieser und der Subdominante (auch "Unterdominante" genannt) eine der drei Hauptstufen bzw. Hauptfunktionen der tonalen Harmonik.

Die Quint-Fortschreitung von der tonikalen zur dominantischen Funktion erzeugt eine Erwartung (psychologische 'Spannung') an einen Rückfall in die tonikale Ruhe-Lage (die Harmonik schreitet von der Tonika fort, also heraus aus der tonikalen Ruhelage).
Der Rückfall in die tonikale Ruhelage wird als Eintreten der erwarteten Kadenz (Lösung der Spannung) empfunden. Das Auflösungsbestreben der Dominante in die Tonika wird durch die Strebetendenz des im Dominantakkord enthaltenen Leittons unterstützt.

Im „natürlichen Moll“ ist die große Septime als Leitton zur Tonika nicht leitereigen. Auf der 5. Stufe der Moll-Tonart steht ein Moll-Akkord, dessen Terz "g" (hier bezogen auf a-Moll) einen Ganztonschritt unter dem Tonikagrundton "a" steht. Um auch hier die von Dur vertraute Strebewirkung zu erhalten, wird stattdessen auch in Moll als Dominante ein Dur-Akkord verwendet. Dazu wird die zugrunde liegende Tonleiter durch Erhöhung des "g" zum "gis" zum harmonischen Moll umgebildet. Wird dieser Leitton nicht verwendet, spricht man verdeutlichend von einer Moll-Dominante.

Die Dominant-Spannung kann durch das Hinzufügen eines weiteren Leittons zum Dominant-Dreiklang deutlich verschärft werden. Durch Hinzunahme einer weiteren (kleinen) Terz entsteht der Dominantseptakkord. Dieser enthält nun mit dem vierten Ton der zugrunde liegenden Tonleiter (in "C-dur" das "f"), welches als abwärtsführender Leitton (Gleitton) bestrebt ist, sich mit einem Halbtonschritt in das darunter liegende "e" aufzulösen. Da der Akkord (wieder bezogen auf "C-Dur") jetzt die Töne "h" und "f" enthält, die – gleichzeitig – nur in C-Dur (und der Tonikaparallele, die hier aber keine Rolle spielt) vorkommen können, legt er dadurch die Tonart "C-Dur" eindeutig fest. Durch den Tritonus zwischen Terz und Septime ist der Dominantseptakkord so spannungsreich (und durch Hörerfahrung so geläufig), dass er automatisch in der Funktion einer Dominante wahrgenommen wird.

Der Dominantseptakkord kann durch sogenannte „Überterzung“ (Hinzufügen einer weiteren Terz) zum Dominantseptnonakkord erweitert werden. Beide Akkorde wirken auch in „verkürzter“ Form, also bei fehlendem Grundton, dominantisch. Weitere Überterzungen ergeben den Dominantundezim- und den Dominanttredezimakkord.

Dominant-Funktion im erweiterten Sinn haben auch alle Akkorde, die einen hohen Spannungsgehalt in sich tragen und in einen nachfolgenden, spannungsärmeren Klang auflösen. Akkorderweiterungen, welche im durmolltonalen System generell dissonant sind, eignen sich besonders gut als Dominante. Auch alterierte Akkorde der fünften Stufe wirken vornehmlich dominantisch.

Der Begriff "Dominante" wird sowohl in der Stufen- als auch der Funktionstheorie verwendet, jedoch in leicht unterschiedlicher Bedeutung. In der Stufentheorie werden als Dominantakkorde nur solche bezeichnet, deren Grundton die V.  Stufe einer Tonleiter ist und die diesen Grundton auch wirklich enthalten. In der Funktionstheorie dagegen werden alle Akkorde, die eine Auflösungstendenz zur Tonika aufweisen, als Dominanten bezeichnet, selbst wenn der Dominantgrundton (V. Stufe) gar nicht in ihnen vorkommt. So wird z. B. in C-Dur der Septakkord der VII. Stufe (H-d-f-a) von der Funktionstheorie in dominantischer Funktion gesehen und als „verkürzter“ oder „stellvertretender“ Dominantseptnonakkord (G-H-d-f-a) mit fehlendem Grundton interpretiert.

Im Jazz kann die Dominante unterschiedlich alteriert werden. Zum Beispiel in C-dur als Septakkord (G-H-D-F=G7), als Septnonenakkord mit kleiner oder übermäßiger None (G-H-D-F-As=G7/b9 oder G-H-D-F-Ais=G7/#9), als Septakkord mit hochalterierter Quinte (G-H-Dis-F=G7/#5), als Undezimakkord (G-H-D-F-A-C=G7/9/11) oder als Tredezimakkord mit kleiner oder übermäßiger Tredezime (G-H-D-F-A-Es=G7/9/b13 bzw. G-H-D-F-A-Eis=G7/9/#13)

Das Wort "Dominante" ist älter als die dur-moll-tonale Musik. Bereits 1615 verwendete Salomon de Caus diese Bezeichnung bei authentischen Kirchentönen für die 5., bei plagalen für die 4. Stufe. Allgemein wurde die Bezeichnung "Dominante" oft synonym für die anderen Benennungen des Rezitationstons von Kirchentönen (Repercussa, Tenor, Tuba) gebraucht. Am Anfang des 18. Jh. gehörte die "Dominante" als 5. Ton der Leiter neben "Finalis" und "Mediante" (nach Brossard) zu den "Sons essentielles" (wesentlichen Tönen) eines Modus. 

Die heutige Bedeutung des Begriffs als eine der drei Grundfunktionen tonaler Harmonik geht auf Jean-Philippe Rameau zurück. Dieser verstand unter Dominante im Allgemeinen jeden Ton, der Basis eines Septakkords ist, wobei letzterer sich in einen Akkord mit einem um eine Quinte tieferen Grundton auflöst. Die "dominante tonique" (von Marpurg als "tonische Dominante" übersetzt) ist der Spezialfall des auf der Quinte über dem Grundton errichteten Septakkords, der sich in den Tonikadreiklang auflöst (was dem heutigen Dominantverständnis recht nahekommt). Von Rameaus unmittelbaren Nachfolgern übernahmen nur wenige (z. B. Johann Friedrich Daube) die neue Lehre von den Grundfunktionen.

Jean-Jacques Rousseau schwächte die hervorhebende Bedeutung der Termini Tonika, Dominante und Sub-Dominante wieder etwas ab, indem er die Benennung der einzelnen Tonleiterstufen weiter ausbaute (z. B. "Sus-dominante" für die 6. Stufe). 
Bei Heinrich Christoph Koch und Gottfried Weber wird jedoch ausdrücklich zwischen wesentlichen bzw. Hauptharmonien (Tonika-, Dominant- und Subdominantdreiklang) und zufälligen bzw. Nebenharmonien einer Tonart unterschieden. Weber weist auch als einer der ersten darauf hin, dass der Dreiklang auf der Oberdominante immer (auch in Moll) ein Durdreiklang ist. Die endgültige Festigung des Dominant-Begriffs geschah durch Moritz Hauptmann, der diesen von der Quinte, dem zweiten der drei direkt verständlichen Intervalle (Oktave, Quinte, Großterz), ableitete. Die heute übliche Funktionsbezeichnung "D" für die Dominante wurde von Hugo Riemann eingeführt.

Die "Dominante" wurde in der ersten Hälfte des 20. Jahrhunderts im deutschsprachigen Raum auch "Oberdominante" genannt.



</doc>
<doc id="7854" url="https://de.wikipedia.org/wiki?curid=7854" title="Subdominante">
Subdominante

Subdominante, auch Unterdominante bezeichnet die vierte Stufe einer diatonischen Tonleiter (also die Quarte über bzw. Quinte unter der Tonika) und die Funktion der darauf basierten Akkorde. Die Bezeichnung Subdominante ("sous-dominante") wurde von Jean-Philippe Rameau eingeführt und bedeutet nach heutigem Verständnis, dass sie im Gegensatz zur (Ober-) Dominante, die sich eine Quinte "über" der Tonika befindet, eine Quinte "unter" der Tonika liegt.

Beispiele:

In ihrer klanglichen Wirkung stellt die Subdominante (Funktionssymbol: S, in Moll: s) ein Gegengewicht zur Dominante (D) dar und wird oft als Ruhepol empfunden. In ihrer Zielrichtung ist sie im Vergleich zur Dominante, die zur Tonika (T/t) zurückstrebt, wesentlich offener. Sie wird häufig als Ausgang für anstehende Modulationen verwendet. Gegenüber T und D hebt sie sich (insbesondere in der Moll-Variante) durch ihren vergleichsweise farbigen Klangcharakter ab.

Die Subdominante ist Bestandteil der gängigen Kadenz S - D - T. Der Plagalschluss als Akkordfortschreitung "Subdominante - Tonika" wirkt weniger zwingend als der authentische Schluss "Dominante - Tonika". 

Die Wirkung der Subdominante lässt sich durch das Hinzufügen der großen Sexte verstärken (in C-Dur ein dem F-Dur-Dreiklang hinzugefügtes d, in c-Moll entsprechend f-as-c-d). 

Der so entstehende "Subdominantquintsextakkord" wurde bereits von Jean-Philippe Rameau beschrieben "(„l'accord de la sixte ajoutée“, S bzw. s)". Gebräuchlich ist auch die Kurzbezeichnung „Rameau-Akkord“. Im Barock (Bach-Choral) und in der Wiener Klassik war er charakteristisches Element einer abschließenden Kadenz. In der Romantik (z. B. Brahms, Tschaikowsky) wurde er in der Moll-Form gern plagalschlüssig als besonders reizvolle, emotionale Farbwirkung auch in Durtonarten eingesetzt (in C-dur also f-as-c-d).

Es gibt auch den subdominantischen Akkord mit Sexte "statt" Quinte, den man als subdominantischen Sextakkord bezeichnet (kurz: S bzw. s, in C-dur f-a-d, in c-Moll f-as-d). Dieser Akkord besteht aus den gleichen Tönen wie der leitereigene Dreiklang der 2. Stufe.

Ein weiterer subdominantischer Akkord ist der "neapolitanische Sextakkord" (kurz: „Neapolitaner“, s). Er ist - in Moll und Dur gleichermaßen - eine Variante der Moll-Subdominante, bei der aber die Quinte durch die tonleiterfremde kleine Sexte ersetzt wird (in C-Dur und c-Moll f-as-des). Da das Tonmaterial des Neapolitaners mit dem im Quintenzirkel weit entfernten Des-Dur-Akkord identisch ist, bietet er sich für Modulationen an.



</doc>
<doc id="7855" url="https://de.wikipedia.org/wiki?curid=7855" title="Batman">
Batman

Batman ( für "Fledermausmann") ist eine von Bob Kane erdachte und durch Bill Finger vor dem Erscheinen weiterentwickelte Comicfigur. Finger veränderte das ursprünglich steife Cape in ein wallendes und konzipierte Batman als zweite Identität des Milliardärs Bruce Wayne. Batman erschien erstmals im März 1939 in dem Comic-Magazin "Detective Comics" (Ausgabe 27); nach diesem Magazin nannte sich später dessen Verlag in DC Comics um und ist nun im Besitz von Time Warner.

Die Figur Batman wurde 1939 von dem Autor Bill Finger und dem Zeichner Bob Kane geschaffen. Beide wurden vorrangig durch die Romanfigur "Zorro", "D'Artagnan" von "Die drei Musketiere" und die Comicfigur "The Shadow" beeinflusst. "The Bat Whispers", ein Stummfilm von Roland West (1930) und der messerscharfe Verstand "Sherlock Holmes" waren ebenso Vorbilder.
Hinter Batman verbirgt sich der Milliardär Bruce Wayne. Der Name Bruce Wayne geht auf den schottischen Freiheitskämpfer Robert the Bruce und den amerikanischen Nationalhelden Mad Anthony Wayne zurück (so Bill Finger in Sterankos History of Comics 1, 1970). Batman benutzt ab 1940 keine Schusswaffen mehr, da ihn das einerseits auf eine Stufe mit den Verbrechern, die er jagt, setzen würde, und andererseits die Produzenten des "Detective Comics" Angst bekamen, dass jugendliche Leser Batman nachahmen könnten.

Als Kind musste Bruce Wayne mit ansehen, wie ein Straßenräuber seine Eltern, Thomas und Martha Wayne, in einer dunklen Gasse erschießt. Nach einer großzügigen Spende „übersieht“ das zuständige Sozialamt die Waise. Bruce wird daher vom Butler, "Alfred Pennyworth", aufgezogen. Er legt einen Schwur auf dem Grab seiner Eltern ab, "Gotham City" vom Verbrechen zu säubern. Dafür trainiert er hart und studiert auf der ganzen Welt Kriminalistik, Chemie, Mathematik, Physik, Technik und zahlreiche Selbstverteidigungsarten.

Er gibt sich den Namen "Batman", als er, auf der Suche nach einem Symbol, das den Ganoven Angst einjagen soll, eine Fledermaus sieht, die sich in seine Villa verirrt hat, und entwirft ein entsprechendes Kostüm. Wayne ist der Ansicht, dass Kriminelle von Natur aus ein „feiges und abergläubisches Pack“ sind und seine Verkleidung sie daher zusätzlich in Angst und Schrecken versetzen soll. Als Motivation dient Bruce Wayne der Antrieb, der weltbeste Kriminalist, Nahkämpfer und Athlet seiner Zeit zu werden, um diese Fertigkeiten im Kampf gegen das Verbrechen einzusetzen. Im wahren Leben gibt sich Bruce Wayne als stinkreicher Schnösel und Frauenheld aus, was jedoch nur dem Schein dient, um seine Geheimidentität zu schützen.

Batman ist kein Superheld im engeren Sinne, wie etwa Superman, da er über keinerlei Superkräfte verfügt. Seine Überlegenheit basiert auf Intelligenz, Willenskraft, hartem Training, seinen technischen Hilfsmitteln und seinem enormen Familienvermögen, um all das zu finanzieren.

Von Anfang an hatte sich Batman vorgenommen, wenn nötig, auch mit äußerster Gewalt gegen das Verbrechen vorzugehen; doch zwei eiserne Regeln hatte er sich gesetzt: Er würde niemals einen anderen Menschen töten (wenn es sich vermeiden ließe) und er würde, traumatisiert durch die Ermordung seiner Eltern, niemals eine Schusswaffe benutzen.

Die Öffentlichkeit von Gotham City steht Batman gespalten gegenüber. Manche sehen in ihm den Retter und Helden, andere kritisieren ihn als gesetzlosen Rächer mit fragwürdigen Methoden. Eine davon ist die Geheimidentität als "Matches Malone", mit der Batman nach dem Tod des echten Matches Malone ermittelt, um nicht als Batman aufzufallen.

Mit Superman verbindet Batman seit geraumer Zeit eine Art Freundschaft. Immerhin vertrauen die beiden einander so weit, dass sie sich sogar ihre jeweiligen Geheimidentitäten offenbart haben. Außerdem hat Superman Batman einen Ring aus Kryptonit anvertraut, der die einzige Waffe gegen ihn ist, falls er einmal zu einer Gefahr werden sollte.

Im Laufe der Zeit lernt Bruce auch viele attraktive Frauen kennen, doch abgesehen von der Reporterin Vicki Vale, die fast seine Identität aufgedeckt hätte, waren alle Beziehungen ein ziemliches Wagnis: So scheint ihn viel mit der mysteriösen Catwoman zu verbinden, doch bis heute steht das Gesetz zwischen ihnen. Auch Talia al Ghul, die Tochter seines alten Widersachers Ra’s al Ghul, ist wegen ihrer Loyalität zu ihrem Vater ein großer Risikofaktor. Mit dieser hat Batman allerdings auch einen Sohn, Damian. Batman erfuhr dies jedoch erst nach Jahren. Damian steht inzwischen als fünfter Robin an Batmans Seite. Bei Poison Ivy schließlich ist sich Batman bis heute nicht sicher, ob seine Gefühle für sie nicht allein auf ihre Pheromone zurückzuführen sind.

Ab 2006 wurden in vielen Batman-Comics Science Fiction-Elemente eingebaut. Diese wurden als Halluzinationen Waynes aufgrund vom Einfluss von bewusstseinsverändernden Gasen und ausgiebigem Reizentzug-Training erklärt. Diese Entwicklung gipfelte in den Comics "Batman R.I.P.", in dem Batman von Doktor Hurt in den Wahnsinn getrieben und "Final Crisis", in dem er von Darkseid erschossen wird.

In Batman Heft #54 in Deutschland kehrte Bruce Wayne zurück nach Gotham und stellte sich seinem „Mörder“, Doktor Hurt. So verkörperten fortan zwei Männer Batman: Bruce Wayne (in den Serien "Batman Incorporated" und "Batman: The Dark Knight") und Dick Grayson (in "Batman", "Detective Comics" und "Batman and Robin").

Im September 2011 wurden 52 "(The New 52)" DC Comics-Serien neu gestartet. Nun ist Bruce Wayne wieder der einzige Charakter, der als Batman bezeichnet werden kann. Dick Grayson kehrt in sein Kostüm als Nightwing zurück.

Es wird seit längerem diskutiert, ob Batmans Motive – Bekämpfung des Verbrechens und der Korruption in Gotham City – tatsächlich so selbstlos sind, wie sie auf den ersten Blick erscheinen. Kritiker bemerken, dass der Milliardär Bruce Wayne durch sein Alter Ego Batman im Grunde nur das System schützt, aus dem er seinen Wohlstand zieht. Allerdings lässt dieser Ansatz den psychologischen Hintergrund von Bruce Waynes Batman-Werden, die Ermordung seiner Eltern, außer Betracht. Zudem muss erwähnt werden, dass Batman in erster Linie kein Gesetzeshüter ist und auf Gerechtigkeit pocht, sondern vor allem Rachegefühle seinen Antrieb ausmachen. Manche Comicversionen, wie etwa die von Frank Miller oder Jim Lee, zeigen des Weiteren einen Batman, der seine Ziele fanatisch, fast wie ein Psychopath, verfolgt und beinahe rücksichtslos gegen seine Feinde vorgeht, wenngleich er seine selbst gesetzten Richtlinien befolgt. Außerdem zeigen diese Versionen, dass Batman durch sein Auftreten und seine Kostümierung zu einem großen Teil auch selbst die Feinde schuf, die er nun bekämpft.

Zwar gab es Jahrzehnte, in denen Batman gelegentlich einen flapsigen Spruch auf den Lippen hatte oder man ihn gar mal lächeln sah (ein Lachen war allerdings schon immer sehr selten), doch ein Strahlemann ist er nie gewesen. Schon immer war Batman ein ambivalenter Charakter, ein grimmiger und verbitterter Kerl, der ernst und asketisch seiner Aufgabe nachgeht. So meint Steve Kups vom Panini Verlag, Batman sei „ohnehin nicht immer sonderlich liebenswert auftretend, manchmal aber ein außerordentliches Arschloch“ (bezogen auf die Reihe "All Star Batman" von Miller und Lee). Der deutsche Batman-Experte Lars Banhold diagnostiziert bei Batman zudem gar Züge eines Soziopathen; ernst, humorlos und mit fragwürdigen Moralvorstellungen bezüglich Rache und Selbstjustiz.

Eine eindeutige politische Ausrichtung von Batman ist laut Lars Banhold kaum möglich. Vielmehr richte sich diese je nach Interpretation des jeweiligen Autors. So wirke er in manchen Comics anarchistisch, in anderen wiederum faschistoid.

Batman gilt als einer der beliebtesten Superhelden, der auch den verlagseigenen Superman schlägt. Gründe hierfür dürften der mitunter missmutige Batman mit all seinen Schwächen und Kanten sein, der damit, gegenüber Superman, mehr Leser anspricht. Der Schriftsteller Dietmar Dath beschreibt dies in der FAZ so, dass Superman so ist, wie wir gerne wären, Batman dagegen so ist, wie wir sind, nur besser.

Das Kostüm des „Dunklen Ritters“ erlebte im Laufe der Jahrzehnte zahlreiche Wandlungen, das mit zunehmender Fortdauer der Batman-Saga vom schlichten Held-in-Strumpfhosen-Modell zur hilfreichen High-Tech-Rüstung wurde. Sein Kostüm war stets reich an Bedeutungsschattierungen und trotz allen Wandels stets wiedererkennbar. Die Gemütsart des jeweils aktuellen Batman spiegelt sich auch in seinem Kostüm wider: So ist der Umhang mal in einem satten Königsblau, mal in einem tiefen Schwarz gehalten. Die Fledermaus auf der Brust des Helden war anfangs schwarz, wurde dann Teil eines Emblems auf gelbem Grund, gedacht als Zielscheibe für Scharfschützen, daher gepanzert, bis sie wieder zum dunkelsten Teil des Kostüms wurde. Auch die Fledermausohren haben sich im Laufe der Zeit gewandelt: In der Real-Serie aus den 1960er Jahren mit Adam West wirken diese eher wie mickrige Mäuseohren, während diese in anderen Versionen, wie beim düsteren Batman von Bernie Wrightson, wie lange, fast schon teuflische Hörner wirken. Vergleichbar verhält es sich mit seinem Körperanzug, der einmal ein helles Grau (mit schwarzer Überhose), ein andermal tiefes Schwarz aufweist.

Besondere Bedeutung kam dabei schon immer dem Batgürtel zu, der um die Hüfte geschlungenen Wunderkiste mit Rauchbomben, Chemikalien, Seil, „Batarang“ und allem, was zur Rettung in letzter Sekunde nötig ist. Zudem entwickelt er mit dem immensen Vermögen seiner Eltern (er besitzt den Weltkonzern "Wayne Enterprises") einige einmalige Fahrzeuge wie das Batmobil, die zu seinem Kostüm passen und ebenfalls einiges an technischen Spezialerfindungen enthalten. Diese lagert er in der "Bat-Höhle", einer großen Tropfsteinhöhle unter seinem Herrenhaus "Wayne Manor".

Auch die Statur wandelte sich im Laufe der Jahre. War Wayne alias Batman anfangs und die ersten Jahrzehnte eher schlank und sportlich-muskulös, wandelte sich sein Bild ab Mitte der 1980er Jahre, das bis heute prägend ist: breites Kreuz, kräftiges Kinn und breite Wangenknochen.








In Deutschland wurde Batman nur langsam populär. Erstmals bekamen deutsche Comicleser den Dunklen Ritter in der 14-täglich erschienenen, farbigen "Buntes-Allerlei"-Heftreihe des Aller-Verlages zu sehen, bei der von 1953 bis 1954 hauptsächlich Superman-Geschichten veröffentlicht wurden. In der ersten Ausgabe des Jahres 1954 erscheint Batman (zusammen mit Robin) an der Seite von Superman in einem Nachdruck des US-World's-Finest-Heftes Nr. 66 (1953) und ein zweites Mal in "Buntes Allerlei" 11/54 bei einer erneuten Zusammenkunft mit Superman. Anschließend verschwand Batman in deutschen Gefilden wieder in der Versenkung.

1966 läutete der Stuttgarter Ehapa-Verlag eine neue Batman-Ära in Deutschland ein. Zunächst kam es zu einem weiteren Batman-Gastauftritt (in Superman 2/67). Ab der nächsten Ausgabe (3/67) teilten sich die beiden Helden dann die Heftreihe bis zu ihrem Ende 1985 (26/85) als "Superman und Batman". Das zweiwöchentlich erschienene Comicheft, deren 32 Seiten sich die beiden nun teilten, wurde mit der Veröffentlichung von Batman-Taschenbüchern und -Sonderbänden ergänzt. Mitte der 1970er Jahre und Anfang der 1980er Jahre erlebte der Dunkle Ritter dabei seine beste Zeit. Dabei erschienen des Weiteren von 1976 bis 1985 weitere 44 Batman-Sonderhefte, 23 Batman-Superbände (1974–1986) und 41 Batman-Taschenbücher (1978–1988).

Nachdem Ehapa 1988 sein Batman-Programm eingestellt hatte, übernahm der Hethke-Verlag fortan die Veröffentlichung von Batman-Comics für den deutschen Markt. Dabei erschienen 9 großformatige Batman-Hefte (1989–1991), 6 Batman-Klassik-Alben (1990–1991), mit Nachdrucken alter Batman-Geschichten, 22 Softcover-Alben (1989–1992), sowie 32 Sonderbände (1989–1992).

Parallel zu Hethke nahm ab 1989 der Carlsen Verlag die Veröffentlichung von Batman-Comics in die Hand und setzte dabei auf dicke Paperbacks, angefangen mit dem Druck von Frank Millers "Die Rückkehr des Dunklen Ritters". Bis 1998 wurden so unter Carlsen 33 Bände veröffentlicht, darunter auch, in 10 Bänden, die vollständige "Knightfall"-Saga.

Seit den frühen 1990er Jahren erfuhren Batman-Comics, vor allem durch die beiden Realverfilmungen von Tim Burton und die Zeichentrickserie "" großen Aufschwung. Ab 1995 begann daher Dino Entertainment mit der Veröffentlichung der Comic-Begleitserie zu "The Animated Series", womit nach fast 10 Jahren wieder Batman-Comics an deutschen Kiosken verkauft wurden. 1997 übernahm Dino zudem auch die Veröffentlichung der Real-Serie, deren Heftausgabe der Verlag durch zahlreiche "Specials" und Sonderbände ergänzte. Der Verlag setzte sich eine originalgetreue, lückenlose und chronologisch korrekte Veröffentlichung zum Ziel und schloss mit 63 regulären Ausgaben (auch mit einigen sogenannter "Time-Warp"-Boxen) die Lücke zwischen deutscher und US-amerikanischer Ausgabe. 2001 stellte der Dino Verlag sein Superheldenprogramm ein.

Direkt im Anschluss übernahm der Panini Verlag die Veröffentlichung von Superhelden-Comics auf dem deutschen Markt, darunter auch Batman. Von 2001 bis 2003 lief dabei die erste Serie, der darauf 2005 bis 2006 die nächste folgte. Seit Anfang 2007 wird monatlich die aktuelle Batman-Comicreihe veröffentlicht. Des Weiteren druckt auch Panini weitere Batman-Sonderausgaben. Die Ausgaben von Panini erfahren viel Lob aufgrund ihrer hohen Qualität, sowie der Originaltreue. Allerdings zielt Panini mit mehr als doppelt so hohen Einzelheftpreisen wie noch zu Hochzeiten des Dino-Verlages nicht mehr so sehr auf den Massenmarkt, sondern in allererster Linie auf Sammler ab.

Im Verlag Pabel-Moewig erschien zwischen 1956 und 1976 eine Heft- und Buchserie mit dem Titel „"Die Schwarze Fledermaus"“ mit weitgehenden Parallelen zu Batman. So agiert der Protagonist im Fledermauskostüm gegen das Verbrechen und hat als Alter Ego eine bürgerliche Existenz.

So wie Superman wurden während der Jahre des Zweiten Weltkrieges auch die Batman-Comics als Propaganda gegen das nationalsozialistische Deutschland benutzt. Anders als Superman kämpfte Batman jedoch nicht an der Front und erhielt so als Ausgleich schon früh mächtige Feinde, die über besondere Kräfte verfügten und ihm das Leben schwer machten.

Batman hat heute eine der umfangreichsten und bizarrsten Sammlungen von Gegnern in der gesamten Comicwelt. Einer seiner außergewöhnlichsten und bekanntesten Gegenspieler war der Joker, der in den 1980er Jahren sogar den damaligen Robin (Jason Todd), Batmans jüngeren Helfer, tötete, sowie Barbara Gordon (Batgirl) niederschoss und somit von der Hüfte abwärts lähmte. Diese nannte sich von nun an „Oracle“ und unterstützt Batman als Informationsquelle und Hackerin. Allerdings verließ sie später mit ihrem Vater die Stadt. Der Joker ist im Gegensatz zu Batman ironisch und nicht selten parodiert er seine Gegenspieler. Elementare Fragen und unlogische Zusammenhänge werden gerne von den eigenen Autoren durch den Joker ins Lächerliche gezogen. Aus diesem Grunde ist der Joker auch bei vielen treuen Fans der Batman-Comics und -Verfilmungen beliebter als Batman selbst.

In den 1960er Jahren war er kurzzeitig Mitglied der Gerechtigkeitsliga und trug entscheidend dazu bei, dass die Superhelden eine Renaissance erlebten. Und seit einiger Zeit ist er auch an der Seite seines Freundes Superman (sogar als Anführer) wieder dabei.

Auch sein Charakter änderte sich. War Batman lange Zeit ein gutgelaunter, verbrecherjagender Playboy und Millionär mit unglaublichen Technik-Gimmicks gewesen, verdunkelte sich das Bild ab den 1980er Jahren. Sozialkritische Elemente wurden Teil der Geschichten; Batman hatte mit Armut, Elend, sogar mit Black Panthers und radikalen Feministinnen zu tun.

Diese Entwicklung spitzte Frank Miller weiter zu. 1986 machte er in "„Die Rückkehr des dunklen Ritters“" "(„The Dark Knight Returns“)" aus Bruce Wayne einen resignierten Mittfünfziger, der seit zehn Jahren nicht mehr als Batman in Erscheinung getreten ist, sich frustriert mit Autorennen die Zeit totschlägt und selbst provozierte, gefährliche Situationen danach beurteilt, ob sie „ein guter Tod“ wären. Angesichts einer extrem brutalen Jugendbande kehrt Batman zurück, verprügelt Ganoven mit Genuss und fühlt sich endlich wieder „wie ein Mann mit 20, 30“. Batman erscheint damit zwar noch immer getrieben vom Trauma der Ermordung seiner Eltern, seine Nachtarbeit dient ihm allerdings ebenfalls zur Überwindung einer ziemlich spät einsetzenden Midlife-Crisis – und der nie ausgesprochenen Einsicht, dass er sonst mit seinem Leben eigentlich wenig anzufangen weiß. Beide Elemente machten Batman deutlich glaubwürdiger, nachvollziehbarer und damit auch für ein erwachsenes Lesepublikum wieder attraktiv.

Als weiterer Meilenstein gilt die Graphic Novel "Arkham Asylum – Ein düsteres Haus in einer finsteren Welt" von Grant Morrison und Dave McKean. "Arkham Asylum" wurde erstmals Ende 1989 veröffentlicht und gilt bis heute mit mehr als 500.000 verkauften Exemplaren als erfolgreichste Graphic Novel überhaupt. Der Comic, der sich explizit an erwachsene Leser richtet, zeigt keinen heldenhaften Dunklen Ritter, sondern einen Zweifler, der mit seinen Ängsten konfrontiert ist und erstellt so ein interessantes Psychogramm des Helden. Zeichnerisch und erzähltechnisch hob sich die anspruchsvolle Graphic Novel vom Markt ab und gilt, neben Frank Millers „"Die Rückkehr des dunklen Ritters"“ und „"The Killing Joke"“ von Alan Moore, als der Superhelden-Meilenstein der 1980er Jahre, da diese das Genre nachhaltig prägten.

1992 schließlich wurde Bruce Wayne in der "Knightfall"-Saga vom Bösewicht Bane das Rückgrat gebrochen. In der Folgezeit, in der er im Rollstuhl saß, wurde Wayne daher, gegen den Willen von Alfred und Nightwing, als Batman von Jean Paul Valley (alias "Azrael") vertreten. Dieser war jedoch psychisch labil, äußerst brutal und hatte keine Skrupel, seine Gegner auch zu töten. Nach Waynes Genesung kam es zwischen beiden zum Kampf, und der echte Batman konnte über Azrael triumphieren und seinen Platz wieder einnehmen. Über den Ausgang dieses Kampfes konnten die US-amerikanischen Leser zuvor abstimmen, denn die Autoren des Comics und die Verlagsleiter waren in Sorge, der „alte“ Batman könne mit seinen klassischen Werten nicht mehr mithalten mit dem „modernen“ und brutalen Batman, in seiner Hi-Tech-Rüstung. Jedoch stimmte eine überwältigende Mehrheit für den klassischen Batman.

Batman wurde auch weiter von Heimsuchungen geplagt. In der Saga „Seuche“ wurde seine Heimatstadt von einer Epidemie heimgesucht. In einer weiteren Saga wurde die Stadt durch ein Erdbeben verwüstet. Anschließend erklärte die US-Regierung die Stadt zum Niemandsland. In jüngster Vergangenheit wurde Bruce Wayne sogar des Mordes angeklagt und floh vor dem Prozess.

1999 trat im Rahmen der in naher Zukunft angesiedelten Zeichentrickserie "Batman of the Future" (Originaltitel: "Batman Beyond") der junge Terry McGinnis in die Fußstapfen des mittlerweile ergrauten Bruce Wayne, um so Rache an dem Mörder seines Vaters, einem Handlanger von Derek Powers (dem neuen Vorstandschef von Waynes Konzern) nehmen zu können und Waynes Erbe im Fledermauskostüm anzutreten.

2001 startete DC Comics eine besondere Comicreihe, die sich "Just imagine, Stan Lee created …" nannte, in welcher der Spider-Man-Erfinder Stan Lee Geschichten schrieb, wie er die DC-Charaktere geschaffen hätte. In dieser Serie ist Batman ein Schwarzer mit dem Namen Wayne Williams. Auch wird er erst nach einem Gefängnisaufenthalt zu Batman. Robin taucht zwar später ebenfalls auf, wird aber hier nicht Batmans Partner.

2002 setzte Frank Miller seine „Dark Knight“-Saga mit "The Dark Knight Strikes Again" (auch kurz „DK2“ genannt) fort, konnte damit aber weder erzähltechnisch noch inhaltlich neue Akzente setzen.

2003 trat der dunkle Ritter in einer der erfolgreichsten Batman-Geschichten aller Zeiten einem unbekannten Gegner namens Hush entgegen, dem es gelang, alle wichtigen Feinde Batmans für seine Zwecke zu instrumentalisieren.

2005 startete die Serie "All Star Batman". Erneut aus der Feder von Frank Miller und gezeichnet vom wohl renommiertesten Superheldencomiczeichner der Gegenwart, Lee. Darin wird die Geschichte um die Ermordung Robins Eltern neu erzählt und dessen erste Begegnung mit Batman und die folgende Aufnahme. Auffällig ist bei dieser Serie, dass Batman erneut als „riesiger Kotzbrocken“ und Antipath dargestellt wird.

Die Figur des Batman lieferte die Grundlage für zahlreiche Parodien:














Neben den als "Batman"-Filme vermarkteten Trickfilmen hatte Batman auch Auftritte in anderen Filmen:






Ferner existiert eine Vielzahl von nichtautorisierten Fan-Produktionen.

In Deutschland erschien 1989 anlässlich des Films "Batman" von Tim Burton eine kurze Hörspielreihe namens "Batman" im Label "OH ha". Die Reihe bestand aus insgesamt neun Episoden, welche jeweils eine Länge von ca. 30-40 Minuten hatten. Das Hörspiel bezieht sich auf den Spielfilm von Burton, welcher in der ersten Folge neu erzählt wird. So tauchen alle aus diesem Film wichtigen Charaktere wieder auf. Neben dem Joker (welcher den Sturz aus dem Film überlebt) und seinen Gehilfen gibt es keine anderen der bekannten Feinde Batmans. Das Hörspiel thematisiert die Freundschaft zwischen Batman und Jim Gordon wesentlich stärker, als es die Filme taten. Für die Aufnahmen wurden ausnahmslos neue Sprecher verwendet.

Weiterhin erschien zu dem Spielfilm "Batman Forever" ein zweiteiliges Hörspiel, basierend auf der Filmtonspur und ergänzt mit einem Erzähler.

Am 2. August 2013 gab das Label Highscore Music die Partnerschaft mit Warner Bros. bekannt. Bis 2014 wurden auf Basis der Geschichten "Gotham Knight" und "Inferno" insgesamt sieben neue Hörspiele veröffentlicht. Die Geschichte setzt vom Zeitpunkt her nach dem Film „Batman Begins“ ein, so dass u. a. die von Dr. Crane verursachten Probleme Batman weiterhin beschäftigen. Als dritte Staffel folgt 2014/15 die sechsteilige Umsetzung der Geschichte "No Man´s Land". Gesprochen wird Batman in dieser Serie von Sascha Rotermund.

Die Figur des Batman ist in der bildenden Kunst immer wieder verwendet worden. So hat der Künstler Mark Chamberlain Batman und Robin als homoerotisches Paar dargestellt. H. C. Artmann greift dieses Thema in seinem Gedicht "Batman und Robin, die liegen im Bett" ebenfalls auf. Eine andere Interpretation ist von Cherry Goldenberg geschaffen worden. Batman ist ebenfalls Motiv von Lithographien des Pop-Art-Künstlers Mel Ramos.





</doc>
<doc id="7861" url="https://de.wikipedia.org/wiki?curid=7861" title="Xbox">
Xbox

Die Xbox ist eine von Microsoft entwickelte Spielkonsole, die größtenteils auf leicht modifizierten PC-Komponenten basiert. Ihr Nachfolger war Ende 2005 die Xbox 360.

Die Erfolge von Sony, Nintendo und Sega ermutigten Microsoft Ende der 1990er Jahre, selbst eine Spielkonsole auf den Markt zu bringen. Die Xbox gehört zur selben Konsolengeneration wie die Sony PlayStation 2, der Nintendo GameCube oder die Sega Dreamcast.
Bei der Sega Dreamcast konnte man bereits mit dem von Microsoft gelieferten Microsoft Windows CE erste Erfahrungen sammeln, weitere Erfahrungen hatte Microsoft mit der Windows-Spiele-Schnittstelle DirectX einbringen können. So war es nur ein kleiner Schritt, eine auf PC-Bauteilen basierende Konsole mit eigenem Betriebssystem auf Grundlage der Windows NT Architektur zu entwickeln.

Der Name „Xbox“ wurde aus dem internen Arbeitstitel „Box“ und der Schnittstelle „DirectX“ zusammengesetzt. Eigentlich nur als interne Bezeichnung vorgesehen, wurde er dann der offizielle Name der ersten Spielkonsole von Microsoft.

Die Xbox wurde zuerst am 15. November 2001 in den USA, dann am 22. Februar 2002 in Japan und am 14. März 2002 in Europa veröffentlicht. Die Start-Titel waren u. a. ", Amped: Freestyle Snowboarding, Dead or Alive 3, Project Gotham Racing" und "Oddworld: Munch’s Oddysee."

In Deutschland wurde die Xbox beim Verkaufsstart für 479 € angeboten, nicht einmal sechs Wochen später wurde der Preis auf 299 € gesenkt, etwas später dann auf 249 €. Im August 2004 kostete die Xbox schließlich 149 €, was dem Preis der PlayStation 2 des Konkurrenten Sony entsprach. Zum Produktions- und Auslieferungsende im November 2006 wurde die Xbox in Deutschland in vielen Geschäften für 99 € verkauft. Gleichzeitig stellten viele Spielehersteller die Produktion von Spielen für die Xbox ein und entwickelten nur noch für die Nachfolgekonsole Xbox 360.

Bis Ende 2005 wurden weltweit ca. 21,9 Mio. Xbox-Geräte verkauft, davon aber nur ca. 1,8 Mio. in der gesamten Asien-Pazifik-Region. Wie hoch die Verkäufe der Xbox nach der Veröffentlichung des Nachfolgers Xbox 360 waren, wurde von Microsoft aus Marketinggründen nie bekanntgegeben.

Am 2. März 2009 hat Microsoft die Unterstützung für die Xbox eingestellt.

Der Aufbau der Xbox unterscheidet sich kaum von dem eines gewöhnlichen PC. Die Hardwaredaten im Einzelnen:

Die Xbox ist mit 26 cm × 32 cm × 10 cm relativ groß und schwer (3,4 kg), da das Netzteil in der Konsole eingebaut ist. Der zum Verkaufsstart in EU und US dem Gerät beigelegte Controller war ebenfalls vergleichsweise wuchtig im Vergleich zu dem von Launch ab kleineren in Japan erhältlichen S Controller und somit für kleinere Hände nicht sonderlich geeignet. Aufgrund dessen brachte Microsoft den in Japan von Launch ab propreitären kleineren Controller, den „Xbox-Controller S“, auch ausserhalb Japan Angebot, der schließlich auch als offizieller Controller in den Lieferumfang der Xbox aufgenommen wurde. Die Spiele "Enter the Matrix" oder "Atari Anthology" für die Xbox wurden mit HDTV-Support, also mit einer Auflösung von 1080i und 5.1-Sound entwickelt. Insgesamt gibt es nur 12 Spiele die entweder 720p oder 1080i unterstützen. In Deutschland wird die HDTV-Ausgabe – wie in allen Ländern mit dem TV-Standard PAL – allerdings nicht unterstützt. Die HDTV-Ausgabe ist somit auf NTSC-Konsolen beschränkt. Der Ton wird in Echtzeit vom Audiochip in Dolby Digital kodiert.

Für die bequemere Steuerung während der Wiedergabe von DVDs brachte Microsoft eine Fernbedienung mitsamt IR-Sender heraus, welcher direkt in den Controllerport der Xbox eingesteckt wird. Der Xbox-Controller hat außerdem einen Erweiterungsschacht auf der Oberseite. Dort kann man sogenannte Memory Units (Speicherkarten, von Microsoft und Drittherstellern erhältlich) sowie Zusatzadapter z. B. für Headsets oder USB-Adapter einstecken.

Hardwarehersteller wie Bigben oder MadCatz entwickelten kurz nach Veröffentlichung der Xbox eigene Zusatzhardware. Dazu gehören Controller, Lenkräder oder Erweiterungsadapter für den Controller.

Die erste Generation von Microsofts Konsole gab es in schwarz und grün sowie vielen Sondereditionen.
Auch die Controller existieren in den jeweiligen Farben der Sondereditionen.

Mit speziell angepassten Linux-Distributionen (z. B. Gentoox oder Xebian) wird aus der Spielekonsole ein vollwertiger Computer, der mit Tastatur und Maus (spezieller USB-Adapter erforderlich) wie jeder PC bedient werden kann und durch die Netzwerkkarte internettauglich ist. Da die vier Controller-Anschlussbuchsen das USB-Protokoll zur Kommunikation benutzen, können über ein Adapterkabel normale USB-Geräte an diesen Ports betrieben werden.

Seit Erscheinen der Xbox existiert eine überaus große Community, die sich mit der Modifikation der Xbox, sowohl im Funktionsumfang als auch im Aussehen, beschäftigt. Dadurch kann der Funktionsumfang des Gerätes stark erweitert werden. Das geschieht entweder durch den Einsatz eines Modchips oder durch Verwendung einer Softmod (siehe weiter unten). Durch diese Modifikationen kann unsignierter Programmcode geladen werden. So kann der Anwender auf eine große Palette von Anwendungen zugreifen, wie zum Beispiel Emulatoren, Mediaplayer, Portierungen von bekannten PC-Spielen, Dateimanager, Webbrowser und alternativen Dashboards. Auch Linux kann auf der Xbox benutzt werden. Fast alle Applikationen werden mit dem Xbox Development Kit (XDK) programmiert.

Eine mittels Modchip oder Softmod modifizierte Xbox kann mit Programmen wie zum Beispiel Xbox Media Center zu einem Multimedia-Gerät umgebaut werden. Die Software befähigt die Xbox, sämtliche Dateiformate abzuspielen, die der MPlayer beherrscht. Dazu zählen unter anderem DivX, Xvid, WMV, MOV, H.264 und MPG.

Weiterhin lassen sich auf diese Weise DVDs und Musikdateien abspielen und Bilder auf dem Fernseher darstellen. Diese Möglichkeiten können wahlweise über die lokale Festplatte, über das lokale Netzwerk oder auch per Stream aus dem Internet benutzt werden.

Das bei PC-Nutzern beliebte Casemodding (Verändern der Gehäuse durch Lichtquellen, Lackierungen, o. Ä.) fand auch unter den Xbox-Besitzern sehr schnell Anhänger.
Im Netz kursieren Anleitungen, wie man seine Xbox auf die unterschiedlichsten Arten verschönern kann, angefangen bei einer sog. „Portbeleuchtung“, wo kleine LEDs in die Controllerports eingesetzt und verlötet werden bis hin zu aufwendigen Gehäusearbeiten.

Einige Modchips verfügen außerdem über Anschlussmöglichkeiten für ein externes Display. So lässt sich die äußere Erscheinung der Xbox der von handelsüblichen CD/DVD-Playern angleichen.

Mit dem Erscheinen der Xbox 360 stellten viele Spielehersteller die Entwicklung von Spielen für die Xbox schnell ein, so dass im Jahr 2006 nur noch wenige neue Spiele für die Xbox erschienen sind und es 2007 so gut wie keine neuen Spiele für die Xbox mehr gab. Microsoft selber hat sich schon lange vor der Veröffentlichung der Xbox 360 ganz von der Xbox1-Unterstützung verabschiedet und den Softwarebereich Drittanbietern überlassen.
Der Hersteller Electronic Arts war einer der letzten Spielehersteller, die ihre Sportspiele (FIFA, NHL, NBA) auch für die ältere Xbox umsetzten und veröffentlichten. Das letzte Spiel für die Xbox ist Madden NFL 09 und erschien im August 2008.

Am 15. April 2010 stellte Microsoft die Unterstützung der Xbox ein. Mit dem Ende des Xbox Live Service werden weder Spiele noch Hardware der ersten Gerätegeneration weiter unterstützt. Das gilt selbst dann, wenn die Spiele auf der Xbox 360 laufen.

Käufer des Spiels Halo 2 erhalten als Ausgleich eine dreimonatige Goldmitgliedschaft, die Möglichkeit zur Teilnahme am Betatest von , sowie 400 MS-Punkte.



</doc>
<doc id="7862" url="https://de.wikipedia.org/wiki?curid=7862" title="Laserdisc">
Laserdisc

Die Laserdisc (LD), auch Laservisiondisc genannt, ist ein optisches Speichermedium für Videos im Heimgebrauch. Wegen ihrer hohen Qualität wurden LDs auch im professionellen Bereich eingesetzt. Die Abtastung erfolgt berührungslos durch einen Laser. Im Gegensatz zu Tonbändern, Schallplatten, Videobändern und anderen Bildplatten-Techniken gibt es daher mechanisch keine Abnutzung. Das Videosignal wurde im Gegensatz zur Video-CD und DVD analog aufgezeichnet. Dasselbe galt in der Anfangszeit der Laserdisc auch für das Audiosignal, 1987 wurde jedoch neben der analogen Stereo-Audiospur eine digitale eingeführt. Es wurde auch mit digitalen Tonspuren wie zum Beispiel Dolby Digital 5.1 oder DTS experimentiert. Kapitel und Features konnten wie bei der DVD direkt angewählt werden. Mit 30 cm Durchmesser hatte die Laserdisc eine im Vergleich zu anderen optischen Medien durchaus beachtliche Größe. Dieser Durchmesser ist identisch mit dem einer Langspielplatte. Um die Jahrtausendwende wurde sie von der DVD nahezu völlig verdrängt und hat heute nur noch in Sammlerkreisen Bedeutung.

Das erste System, welches einen Laserstrahl zum Abtasten von Bildinformationen nutzte, wurde von MCA im Jahre 1971 entwickelt und 1972 unter dem Namen "DiscoVision" der Öffentlichkeit präsentiert. Es gelangte allerdings erst im Jahre 1978 zur Marktreife und wurde im amerikanischen Markt eingeführt.

Aufgrund von Produktionsfehlern bei der Reflexionsschicht der Bildplatten, die zu einem raschen Auftreten von „laser rot“ führten (engl. „rot“ = „Fäulnis“), wurden von MCA und Sony umfangreiche Rückrufaktionen durchgeführt. Noch heute erhaltene DiscoVision-Bildplatten sind daher in der Regel mit „laser rot“ befallen und nur mit Bildstörungen oder gar nicht mehr abspielbar. Sie gelten daher eher als Sammlerstücke.

Parallel wurde von Philips ein System entwickelt und unter dem Namen "LaserVision" veröffentlicht, das in Europa und den USA angeboten wurde. Die ersten LaserVision-Player wurden am 26. Mai 1982 in den Handel gebracht. In Deutschland wurde das System mit weiterentwickelten Abspielgeräten einige Zeit später eingeführt.

Mit einem Helium-Neon-Laser wurden die analog abgespeicherten, unkomprimiert vorliegenden Bild- und Toninformationen abgetastet, wobei die Toninformationen in HiFi-Stereoton vorlagen und von den Bildinformationen getrennt gespeichert waren.

Während sich das Format in Europa im privaten Markt nicht durchsetzen konnte, war es wegen seiner hohen Qualität in 2 % (1998) der US-Haushalte und 10 % (1999) der Haushalte in Japan anzutreffen. Die LD war das erste Medium mit exklusiven Spezialeditionen, die Extras wie Audiokommentare, Trailer und Hintergrundberichte beinhalteten. Oft wurden die Filmemacher für die LD-Editionen befragt oder fertigten neue Transfers an. Diese Basis etablierte eine Industrie, die heute der BluRay und der DVD zu ihren Erfolgen verhilft.

Anders als im privaten Bereich entwickelte sich die Bildplatte für kommerzielle Anwendungen. Grundlage dieser Entwicklung waren interaktive Steuerungen (z. B. Teleselect, ILDIS), die die Möglichkeiten des Einzelbildzugriffs in Verbindung mit einem Computer oder einer Datenbank nutzten. Pilotprojekte im Point-of-Sales-Bereich waren die „Berufsbilder“ der Bundesanstalt für Arbeit und „Der Landkreis Celle“. Ein weiterer Einsatzbereich war die Aus- und Weiterbildung. Die Bedeutung solcher Projekte lag vor allem darin, dass damit zukünftige Anwendungen im Bereich der Breitbandkommunikation vorweggenommen und erprobt werden konnten. Am deutlichsten wurde dies im Projekt MEDKOM, wo ein Bildplattenwechsler als zentrales Speichermedium eingesetzt wurde.

Pioneer entwickelte das LaserVision-System weiter und stellte 1986 das Nachfolgesystem LaserDisc vor, welches im NTSC-Format zusätzlich zu den beiden analogen zwei digitale Tonspuren mit 44,1 kHz enthielt. Im PAL-System fielen die analogen Spuren zugunsten der digitalen weg; für alle vier war hier keine Bandbreite vorhanden.

Im Jahr 1992 wurde „Dolby Digital Surround“ mit 5.1 oder 6.1 EX beim LaserDisc-System eingeführt. Dabei lagen die Toninformationen auf einer der beiden normalerweise analog genutzten Tonspuren als moduliertes HF-Signal vor, das mithilfe eines in aller Regel externen Demodulators in ein Basisband-Signal umgewandelt werden musste, bevor es via S/PDIF an den Verstärker weitergeleitet werden konnte. Einige meist hochpreisige AV-Receiver hatten den Demodulator seinerzeit noch integriert und boten entsprechend einen RF-AC3-Eingang an, etwa der Denon AVC-3800: Entgegen der oft – selbst in Fachliteratur – irreführenden Ausführungen ist das frequenzmodulierte HF-Signal sowohl auf der LaserDisc in seiner Natur digital, wenn auch am RF-Ausgang als moduliertes Breitband-Signal vorliegend. Wäre dies nicht der Fall, wäre eine verlustfreie Rekonstruktion der AC3-Frames nicht möglich.

„Dolby Digital Surround“ war aufgrund der benötigten „analogen“ Tonspur nur bei Bildplatten möglich, die im NTSC-Format aufgezeichnet waren. Die zweite „analoge“ Tonspur enthielt oft eine spanische oder französische Sprachfassung in Mono oder einen Kommentar des Regisseurs.

Im Jahr 1995 wurde auf der digitalen Tonspur „dts Digital Surround“ eingeführt. LaserDiscs und CDs erlauben eine Datenrate von 1235 kbit/s, DVDs eine Datenrate von 754,5 oder 1509,75 kbit/s. Der „dts Digital Surround“-Ton ist sowohl bei LaserDiscs im PAL- als auch bei NTSC-Format verfügbar. Technische Voraussetzung für das Nutzen von DTS-Ton ist ein digitaler Tonausgang am Abspielgerät. Da der Ton von DTS-LaserDiscs in PAL nur mit einem DTS-Dekoder abgehört werden konnte und DTS noch nicht weit verbreitet war, sind LDs mit DTS in PAL selten. Bei solchen in NTSC wurde daher meist zusätzlich eine Abmischung in Dolby Surround auf den analogen Spuren untergebracht.

Im Jahr 1996 wurde mit dem Film „Mikrokosmos“ als Kodierungsverfahren PAL+ eingeführt.

In den späten 1990er Jahren brachte Pioneer einige LD-/VCD-/CD-/DVD-Player auf dem Markt. Es waren die einzigen Player, die DVDs und LaserDiscs abspielen konnten. Das letzte Modell, das in Deutschland angeboten wurde, war der Pioneer DVL-919E. Der Preis lag damals bei rund 2800 DM.

Im Gegensatz zur Video-CD (VCD) oder DVD wird das Videobild auf der LD analog gespeichert. Beim Mastering wird das Videosignal moduliert und an den Nulldurchgängen des Signals abwechselnd als „Vertiefung“ und „Nicht-Vertiefung“ gepresst. In einem LD-Spieler folgt ein PLL-Synthesizer diesen Vertiefungen und regeneriert daraus das ursprüngliche Signal.
Fehlererkennung und -korrektur sind prinzipbedingt nicht möglich.

Die Bildqualität wird trotz der Beschränkungen durch das PAL- beziehungsweise NTSC-Farbsystem als ausgezeichnet beurteilt.

LDs gibt es in den drei Größen 30 cm (LP), 20 cm (EP) und 12 cm. Die beiden großen Formate können beidseitig bespielt sein.

Die Aufzeichnung erfolgt in verschiedenen Umdrehungsmodi: CAV oder CLV. CAV (Constant Angular Velocity) erlaubt Zeitlupe und Standbild in optimaler Qualität, die Spielzeit ist jedoch auf 30 min (NTSC) oder 36 min (PAL) je Seite beschränkt. CAV Disks rotieren immer mit 1500/min (PAL) oder 1800/min (NTSC). Um Spielfilme auf dem beidseitig abspielbaren Medium unterzubringen, wurde das CLV-Verfahren entwickelt. CLV (Constant Linear Velocity oder auch Extended Play genannt) erlaubt bei gleicher Qualität 60 min (NTSC) oder 64 min (PAL) pro Seite. Hier rotieren die Disks zunächst auch sehr schnell, werden aber im Laufe des Films langsamer, da, wenn der Lesekopf zum äußeren Durchmesser hin gewandert ist, bis zu 3 Bilder pro Umdrehung gespeichert sind. Bei CLV sind Zeitlupe und Zeitraffer nur bei Playern mit „digital frame store“ (wie beispielsweise beim Pioneer DVL-909 oder -919) möglich.

Die 12-cm-Version wird häufig als "CD-Video" bezeichnet, hat aber nichts mit dem DVD-Vorläufer Video-CD zu tun. Es können 6 min Bild und Ton und weitere 20 min nur Ton aufgezeichnet werden. Ein zusätzlicher Nur-Ton-Anteil kann von jedem CD-Spieler wiedergegeben werden. Die reguläre Videospur ist jedoch völlig inkompatibel zu CD- oder DVD-Formaten und kann von entsprechenden Laufwerken nicht gelesen werden.

In Deutschland wurden bis 1985 nur Laserdiscs mit analogem Ton veröffentlicht. Diese verfügten über zwei Tonspuren für die Wiedergabe in Stereo oder Zweikanalton, sowie – bei entsprechender Kodierung des Tonsignals – in Dolby Surround Pro Logic.

1986 kamen die ersten Platten mit digitalem Ton (16-bit, 44,1 kHz – entsprechend der Audio-CD) auf den Markt. Dabei handelte es sich um zwei zusätzliche Tonspuren für Stereo- oder Dolby Surround Pro Logic-Wiedergabe, die im Fall von PAL-LaserDiscs die analogen Tonspuren ersetzten, im Fall von NTSC-LaserDiscs aber parallel zu den beiden Analogtonspuren angeordnet wurden. Jede NTSC-LaserDisc mit digitalem Ton enthält also gleichzeitig auch die analogen Tonspuren. Die meisten Player erlaubten es dem Benutzer, jederzeit im Film zwischen analogem und digitalem Ton umzuschalten – wichtig für Laserdiscs, die z. B. den eigentlichen Filmton auf den beiden Digitalspuren hatten, während als Bonusmaterial Kommentare des Regisseurs zu den einzelnen Szenen auf die Analogtonspuren abgelegt wurden; darüber hinaus waren die Analogspuren eher komprimiert abgemischt, während die Digitalspuren bewusst mit maximaler Dynamik aufgenommen wurden – hier konnte man also nach persönlichem Geschmack auswählen. Auch zweisprachige Versionen von Laserdiscs waren damit theoretisch möglich, allerdings wurde von dieser Möglichkeit so gut wie nie Gebrauch gemacht. Unter Benutzung aller vier Tonspuren (zwei analoge, zwei digitale) wären sogar drei- oder viersprachige LaserDiscs möglich gewesen, in diesem Fall allerdings nur mit Ton in mono, bzw. einmal stereo und zweimal mono im Fall einer dreisprachigen LD.

Darüber hinaus erlaubte das Format auch die Verwendung von DTS und Dolby Digital (auch als "AC-3" bezeichnet). Die Datenrate von DTS ist dabei auf "fullrate DTS" festgelegt (1536 kbit/s), während bei der Normierung der DVD auch eine neue "halfrate DTS" von 768 kbit/s eingeführt wurde, um Platz zu sparen. Bei Dolby Digital ist die maximale Bitrate geringer als auf DVDs (384 kbit/s zu 448 kbit/s). Eine Dolby-Digital-Tonspur ist nur auf Laserdiscs möglich, die der NTSC-Fernsehnorm entsprechen. Hierzu wurde der Dolby-Digital-codierte Ton auf das RF-Signal des linken Analogkanals auf der NTSC-Laserdisc aufmoduliert (bei NTSC-Laserdiscs konnten wegen der geringeren Videobandbreite neben dem Digitalton zwei analoge Audiokanäle erhalten bleiben). Um das Signal in ein Standarddigitalsignal umzuwandeln, wird zur Decodierung ein AC3-RF-Ausgang am LD-Spieler und ein AC3-RF-Eingang am Verstärker benötigt. Da diese Eingänge nur in den sehr teuren Spitzenklassegeräten eingebaut waren, gab (und gibt) es spezielle AC3-RF-Demodulatoren zu kaufen, die diese Wandlung (AC-3 RF auf AC-3 S/PDIF) erledigen. Hierzu wird der AC3-RF-Ausgang des Laserdiscplayers am RF-Eingang des Demodulators angeschlossen. Der Demodulator wird wiederum per Digitalkabel (Koaxial oder TOSLINK) an den Verstärker angeschlossen. Bekannte Hersteller dieser Geräte waren Yamaha, Kenwood, Sony und Pioneer, heute gibt es sie nur noch von Kleinserienherstellern wie BDE Elektronik. Teilweise gab es auch externe Dolby-Digital-Decoder (wie zum Beispiel den Yamaha DDP-1 oder DDP-2) mit eingebautem RF-Signal-Wandler. Diese Geräte mit (für damalige Zeiten) weitaus besseren AC3-Decodern wurden auch genutzt, wenn der im Verstärker eingebaute Decoder qualitativ minderwertig war oder schlichtweg nur Dolby Surround vorhanden war.

Kurzzeitig gab es auch Versuche, Laserdiscs mit Dolby-Digital-Ton in Deutschland einzuführen. Hierzu wurden NTSC-Laserdiscs mit deutschem Ton gepresst. Das war technisch möglich, da viele der damaligen Player sowohl ein PAL-, als auch ein NTSC-Signal ausgeben konnten. Durchgesetzt hat sich das Format allerdings nicht – in Deutschland sind gerade einmal zwei Laserdiscs mit AC3-Ton erschienen ("True Lies" und "The Long Kiss Goodnight").

Die einzige deutsche Laserdisc mit DTS-Tonspur war Schlafes Bruder, die daneben auch eine von drei deutschen LDs mit anamorpher Bildaufzeichnung war.


Die Laserdisc wurde zur Zeit ihrer Herstellung überwiegend von High-End-Usern benutzt. Das hatte verschiedene Gründe: Für High-End-User stand die der VHS-Kassette überlegene Bildqualität sowie der ausgezeichnete Ton im Vordergrund. In Deutschland trugen vor allem die Firmen "Laser Paradise" und "Astro" zur Verbreitung des Mediums bei. Beide Firmen pressten überwiegend Horror- und Splatterfilme auf das Medium, wie etwa Dawn of the Dead oder auch Tanz der Teufel.

Da die Laserdisc niemals Massenmedium war, gab es sie überwiegend nur in großen Metropolen in den Fachabteilungen der Elektromärkte zu kaufen. Einige wenige engagierte Versandhändler wie beispielsweise „Frankfurt Laserdiscs“ boten sie auch überregional an und sorgten so für eine größere Verbreitung.

Ein weiterer Anbieter war die Berliner Firma Laser-Eye-Land, die mit Eigenimporten aus Japan, USA, Hongkong und Singapur versuchte, die Verbreitung voranzubringen. Dort wurden auch spezielle „Uncut“-Versionen bekannter Action- und Horrorfilme vertrieben, die auf deutschen Laserdiscs und VHS-Kassetten oft nicht zu erwerben waren.

Die Laserdisc unterschied sich von der VHS-Kassette neben dem Ton und der Bildqualität vor allem in puncto Zusatzmaterial: Auf Laserdisc gab es ausführliches Bonusmaterial wie ein "Making of", Interviews, Audiokommentare, entfallene Szenen und oft auch kleine Zeitschriften oder andere Gimmicks. Daran liegt es wohl auch, dass sich immer noch viele alte (und auch neue) Fans um die Laserdisc scharen, sie weiterhin sammeln oder ihre Sammlung komplettieren. Derart aufwendig gestaltete Laserdiscs wurden meist als Box in einer speziellen Sammlerversion veröffentlicht, die allerdings auch recht teuer war. Als 1999 die letzte deutsche Laserdisc gepresst wurde, waren viele Firmen schon auf die aufstrebende DVD umgestiegen. Die Laserdisc wurde erst in ihrer letzten Zeit durch den massiven Preisverfall dem „normalen Konsumenten“ zugänglich und so erinnern sich nun viele an das damalige Interesse für die LD.

Interessant für Neueinsteiger ist die Laserdisc heute durch das „abgeschlossene“ Sammelgebiet. Es gibt nur eine überschaubare Anzahl von Titeln, zum Beispiel etwa 1200 deutsche oder circa 140 NTSC-Titel mit DTS-Ton oder 23 in Widescreen (16:9 oder „Squeeze“) oder 54 Veröffentlichungen von Astro Records and Filmworks.

In Japan wurde ab 1992 eine "Hi-Vision-LD" (oder auch HD-LD / MUSE LD) angeboten. Diese hatte eine noch bessere Bildqualität (HD-TV mit 1035i) gegenüber der normalen LD, setzte sich aber nicht durch und wurde 1997 wieder vom Markt genommen.

Eigene Aufzeichnungen mit Laserdisc-Recordern waren auch möglich, allerdings nur im CAV-Mode. Geräte wie der Sony LVR300 kosteten ungefähr 18.000 US$. Dazu gab es spezielle Rohlinge im Caddy.

Einem breiten Publikum bekannt wurde die Laserdisc durch die Berufsinformationszentren (BIZ) der Bundesagentur für Arbeit, in denen man über viele Jahre Informationsfilme auf Laserdisk („Bildplatte“) ansehen konnte. Es wurde die 30-cm-LP-Version verwendet, die meistens einseitig bespielt war.

Auch zu Schulungszwecken – zum Beispiel bei der Bundeswehr oder der damaligen Deutschen Bundespost, bzw. Deutschen Post AG – wurde die Bildplatte verwendet. Dem Betrachter stand dabei die Möglichkeit offen, interaktiv in den weiteren Verlauf einzugreifen und somit die fortführende Handlung bzw. die folgenden Filmsequenzen (in beschränktem Ausmaß) selbst zu bestimmen. Bei der Deutschen Post AG wurde die Bildplatte Ende der 90er Jahre zugunsten der Video-CD ausgemustert. IBM setzte die Laserdisc zur Schulung ihrer Händler ein. Ein PC mit DOS-Betriebssystem steuerte dabei einen externen Bildplattenspieler.

Einigen ist die Laserdisc auch bekannt durch ihren Einsatz in den Spielhallen-Spielen Dragon’s Lair und Space Ace.

Von 1982 bis 1999 wurden circa 1200 deutschsprachige Spielfilme auf Laserdisc veröffentlicht. Die letzten deutschen LDs erschienen im Herbst 1999. Über die wirklich letzte deutschsprachige Laserdisc gibt es widersprüchliche Angaben:


Die weltweit letzte LaserDisc wurde 2001 in Japan hergestellt. Der Titel heißt „Tokyo Raiders“ und wurde am 21. September 2001 veröffentlicht.

Die Veröffentlichung von auf Laserdisc in Japan war bis zum Erscheinen der normalen DVD-Fassung die einzige erhältliche Fassung dieses Films in Dolby-Digital 5.1 EX.

In dem Film "Zurück in die Zukunft II" kann man in der Szene, in welcher Marty und Doc Brown Jennifer in der Zukunft betäubt in einer Seitenstraße ablegen möchten, im Hintergrund eine große Menge Laserdiscs sehen, welche anscheinend entsorgt werden sollen.




</doc>
<doc id="7863" url="https://de.wikipedia.org/wiki?curid=7863" title="Stimmung">
Stimmung

Stimmung bezeichnet:

Siehe auch:



</doc>
<doc id="7865" url="https://de.wikipedia.org/wiki?curid=7865" title="At-Zeichen">
At-Zeichen

Das At-Zeichen oder kurz At [] ( „bei“), gemäß einer Vermutung zu seiner Herkunft auch Ad-Zeichen oder kurz Ad ( „bei“), ist das Schriftzeichen @. Umgangssprachliche Bezeichnungen sind "Affenschwanz," "Affenohr," "Affenschaukel," "Klammeraffe". Das At-Zeichen ist grundlegender Bestandteil von E-Mail-Adressen, es steht dort zwischen Benutzername und Domain. Außerdem wird es als Symbol für das Internet genutzt, zum Beispiel auf Wegweisern zu Internetcafés.

Der Ursprung des Symbols ist unklar, es gibt mehrere Hypothesen. Zwei davon werden im Mittelalter angesiedelt: entweder die Entstehung als handschriftliche Verschmelzung (Ligatur) der Buchstaben "a" und "d" des lateinischen Wortes "ad" (deutsch: zu, an‚ bei) oder als Abkürzungszeichen. So soll es im Italien des 16. Jahrhunderts als Hohlmaßeinheit genutzt worden sein.
Nach einer weiteren Theorie hat sich das @ als Ligatur aus französisch "à" mit derselben Bedeutung wie heute entwickelt, z. B. "2 Stück à 500 Gramm" (= zu je 500 Gramm).

Die schlüssigste Theorie ist, dass die Mauren das Zeichen als Maßeinheit auf die Iberische Halbinsel brachten, was heute noch auf aus dieser Zeit stammenden Wein- und Olivenfässern zu sehen ist. In der Folge nutzten es spanische, portugiesische und dann auch französische Kaufleute, die mit Stieren und Wein handelten, als ein Maß für Festes und Flüssiges namens "arroba," etwa zehn Kilogramm (25 Libras) oder 15 Liter. Das Wort ist arabisch, bedeutet „das Viertel“. Die Einheit "arroba(s)" wurde mit dem Zeichen @ dargestellt. Der Name "arroba" für das @ hat sich seither in Spanien, Frankreich, Portugal und Brasilien erhalten.

In den Akten des Reichskammergerichts aus dem 18. Jahrhundert wurde das @ mit der Bedeutung "contra" („gegen“) benutzt: beispielsweise "Maier @ Müller".

Nach gängiger Typografen-Meinung ist das @-Zeichen eine Ligatur, die schon als altes Bleigusszeichen in der Monotype-Schriftenbibliothek in London Mitte des 19. Jahrhunderts auftaucht. Es handelte sich um ein kaufmännisches Zeichen, das damals "commercial a" genannt wurde. Aus Preisangaben wie "5 apples @ 10 p" ergibt sich die Bedeutung: "five apples at 10 pence" („5 Äpfel zu 10 Pence“). Seit den 1880er Jahren ist das @ auf englischen Schreibmaschinen nachgewiesen.

Das Fieldata-Computerprojekt des "US Army Signal Corps" aus den 1950er Jahren verwendete einen Zeichensatz, welcher auch der ursprüngliche Zeichensatz der Computer der Serie 1100 von Univac war und der großen Einfluss auf den späteren ASCII-Zeichensatz haben sollte. Hier wurde dieses Zeichen als Master Space (MS) bezeichnet und diente zur Einleitung von Steueranweisungen.

Bei der Erfindung der E-Mail 1971 wurde nach einem noch ungenutzten Zeichen im Schriftsatz amerikanischer Fernschreiber (ASCII) gesucht, das zwischen Benutzer- und Rechnername gesetzt wird und die beiden Namen eindeutig trennen sollte. Dabei stieß Ray Tomlinson auf das @ und benutzte es als Symbol für "at" in E-Mail-Adressen. Die Benennung "at" (= bei) passte auch deshalb, weil der Benutzername vor dem @ eine Person und die Domain hinter dem @ ursprünglich meist den Großrechner des Betriebs oder Instituts bezeichnete, bei dem die Person arbeitete.

Im Englischen wird durchgängig die Aussprache des Wortes „at“ benutzt (wie in "I’m at home"), also []; das Zeichen heißt dort heute "at sign" oder "commercial at". Der Name wird im Deutschen in der Regel in der Aussprache angepasst und [] („Ätt“) ausgesprochen.

Seit Anfang 2004 ist das At-Zeichen offizieller Bestandteil des Morsecodes: ·−−·−· (Eingabe wie A, gefolgt ohne Pause von C).

Am 22. März 2010 gab das New Yorker Museum of Modern Art (MoMa) bekannt, dass das Zeichen @ in den Bestand des Museums aufgenommen wurde.

Das Zeichen besteht aus einer zumeist geschlossenen („einstöckigen“) Form des kleingeschriebenen lateinischen „a“, zumeist auch in geradem („normalem“) Schriftschnitt rechtsgeneigt ähnlich einer kursiven Form mit einem am Auslauf rechts unten ansetzenden, die gesamte a-Form linksläufig in zumeist etwa gleichmäßigem Abstand umlaufenden und nahe dem Anschlusspunkt frei (in Serifenschriften zumeist in einem spitzen Auslauf) endenden Bogen "(„Affenschwanz“)". Die Höhe und Lage der a-Form kann dem Kleinbuchstaben „a“ entsprechen (also von der Grundlinie bis zur x-Linie reichen), häufig ist die a-Form aber auch kleiner und liegt dann zwischen den genannten Linien. Der Bogen ragt in jedem Fall unter die Grundlinie und über die x-Linie, muss aber nicht unten die p-Linie oder oben die H-Linie erreichen.

Im kursiven Schriftschnitt kann, sofern die a-Form (wie zumeist) schon in geradem Schriftschnitt kursiv erscheint, die gleiche Glyphe wie bei geradem Schriftschnitt verwendet werden. In einigen Schriftarten ist hier bei unveränderter a-Form die Rundungsachse des umschließenden Bogens schräggestellt.

Im englischen Sprachraum war das Zeichen schon lange vor dem Aufkommen des E-Mail-Verkehrs in Gebrauch, daher existiert dort keine spezielle Verbindung von @ mit dem Internet. Die symbolische Assoziation mit dem Internet in Deutschland und anderen nicht-englischsprachigen Ländern ergibt sich daraus, dass das Zeichen in diesen Ländern vor dem Internet-Boom höchstens Programmierern und Heimcomputer-Nutzern bekannt war, aber im Schriftverkehr traditionell keine Rolle spielte.

Das Zeichen @ wird gelegentlich auch in naturwissenschaftlichen Zusammenhängen, in Datenblättern von elektronischen Bauteilen und in sonstigen Texten zur verkürzten Schreibweise von englisch "at" oder deutsch "bei" verwendet. Beispiel:

In chemischen Formeln wird das Zeichen @ für endohedrale Komplexe verwendet, bei denen ein Teilchen in einem anderen Teilchen eingeschlossen ist. Beispielsweise ist in He@C ein Heliumatom in einem kugelförmigen Molekül aus Kohlenstoffatomen gefangen.

Auch im US-Sport wird das @ gelegentlich eingesetzt, um das "at" zu ersetzen. Die Heimmannschaft wird hier als zweites genannt. Beispiel: Arizona Cardinals @ Seattle Seahawks

In älteren Programmiersprachen (zum Beispiel in einigen Dialekten von BASIC oder im Datenbanksystem dBASE) wurde "@" manchmal auch als Operator für Positionsangaben verwendet, der nur in manchen Dialekten vorhandene BASIC-Befehl PRINT @ 12,10,"HALLO" stellt zum Beispiel das Wort HALLO in der 10. Zeile und ab der 12. Spalte des Bildschirms dar.

In einigen Programmiersprachen, beispielsweise in der Entwicklungsumgebung Delphi verwendeten Variante von Object Pascal, wird mit "@" die Speicheradresse einer Variable ermittelt. var p: Pointer; d: Double; begin p := @d; end; ermittelt zum Beispiel die Adresse der Gleitkommavariable "d" und speichert sie in der Zeigervariablen "p".

In der Programmiersprache PHP wird "@" als Fehlerkontrolloperator vor einem Funktionsaufruf notiert, um Fehlermeldungen der aufgerufenen Funktion zu unterdrücken.

In der Programmiersprache Forth bezeichnet "@" den „Fetch“-Operator, der den Inhalt einer Speicheradresse auf den Stack legt. Der dazugehörige Speicheroperator „Store“ wird mit „!“ bezeichnet, so drückt man beispielsweise codice_1 durch codice_2 aus.

Im Betriebssystem OpenVMS wird das "@"-Zeichen zum Starten von DCL-Kommandoprozeduren benutzt.

In Objective-C leitet das "@" das Literal eines String-Objectes ein, beispielsweise @"Simon sagt". Ebenso sind dort (gegenüber C) neue Schlüsselwörter mit einem voranstehenden "@" gekennzeichnet (zum Beispiel @interface, @synchronized etc.).

In Haskell ist das @ (gelesen "as," englisch für "als") ein Schlüsselwort, um dem gesamten Wert eines Pattern-Matchings einen Bezeichner zuzuordnen. Beispielsweise bezeichnet im Ausdruck codice_3, der eine nicht-leere Liste matcht, der Name codice_4 die gesamte Liste, während codice_5 das erste Element und codice_6 die Restliste bezeichnet; codice_4 entspricht also codice_8. Ein solches Pattern wird als "as-pattern" bezeichnet.

      "Bitte den beachten!"

In Deutschland war die Zulässigkeit des At-Zeichens und anderer Sonderzeichen, die nicht (wie z. B. das Kaufmanns-Und „&“) Bestandteil der deutschen Rechtschreibung sind, in im Handelsregister einzutragenden Firmennamen zunächst umstritten, kann aber heute (Stand 2013) als prinzipiell möglich gesehen werden. Noch um das Jahr 2000 herum wurde dies regelmäßig abgelehnt, beispielsweise in einem Beschluss des Oberlandesgerichtes Braunschweig vom 27. November 2000 (Az. 2 W 270/00, „met@box“), oder in einem Urteil des Landgerichts München I vom 3. April 2001 (Az. 17 HTK 24115/00, „D @ B“). Bereits 2004 ließ das Landgericht Berlin dies jedoch zu (Az. 102 T 122/03, „T@S GmbH“), mit Beschluss vom 12. Februar 2009 (Az. 17 HKT 920/09, „@p oHG“) auch das Landgericht München I. In der letztgenannten Entscheidung wurde als wesentlich herausgestellt, dass das „@“ als das englische Wort "„at“" aussprechbar ist und somit der Aussprechbarkeit des Firmennamens nicht im Wege steht. Eine Verwendung des „@“ als eine modische Schreibweise des Buchstabens „a“ sei hingegen nicht im Handelsregister eintragungsfähig.

In Österreich ist das Zeichen @ seit dem zum 1. Januar 2007 erfolgten Inkrafttreten des Unternehmensgesetzbuches (UGB) grundsätzlich eintragungsfähig, aber nur dann zulässig, wenn an der Aussprache keine Zweifel bestehen (je nach Verwendung: „at“ oder „a“ oder „Klammeraffe“).

In Deutschland wurde „@“ im Jahr 2012 als Wortmarke für verschiedene Nizza-Klassen (unter anderem für Nahrungsmittel und Bekleidung) geschützt. Gegen die Eintragung wurde 2013 Löschungsantrag aufgrund Nichtigkeit wegen absoluter Schutzhindernisse eingereicht. 2014 löschte das deutsche Patent- und Markenamt die Marke. Das Bundespatentgericht bestätigte diese Löschung 2017 und führte aus, dass das @-Zeichen ein grundlegender Bestandteil von E-Mail-Adressen sei.

Eine internationale Registrierung wurde 2014 zurückgewiesen.

Die im südlichen Sudan gesprochene Sprache Koalib verwendet das lateinische Alphabet mit Erweiterung um einige zusätzliche Buchstaben, darunter einem dem At-Zeichen ähnlichen Buchstaben, der in der Schreibung arabischer Lehnwörter zur Umschrift des Ain dient. Ein im Jahr 2004 gestellter Antrag, diese Zeichen als lateinische Buchstaben in Unicode aufzunehmen, wurde abgelehnt, nachdem Bedenken vorgebracht wurden, ein dem At-Zeichen zu ähnlicher Buchstabe würde Spoofing erleichtern und so Sicherheitslücken schaffen. Ein weiterer 2012 gestellter Antrag, nur den Großbuchstaben als nicht in URLs zugelassenes Sonderzeichen zu codieren und für den Kleinbuchstaben auf das vorhandene At-Zeichen zu verweisen, wurde ebenfalls nicht angenommen, da die Sprachgemeinschaft stattdessen die Zeichen Ⓐ/ⓐ (U+24B6 , U+24D0 ) verwenden könne.

Die sich mit Minderheitensprachen befassende Organisation SIL International pflegt in diesem Rahmen eine Liste von nicht in Unicode aufgenommenen Zeichen, denen sie Codepunkte in der „Private Use Area“ von Unicode zuweist. Hier sind die At-Zeichen-ähnlichen Buchstaben als U+F247 und U+F248 enthalten. Seit dem 15. Februar 2013 sind mit der Version 6.2a dieser Liste jedoch diese Buchstaben als „deprecated“ gekennzeichnet, und auch hier wird stattdessen die Verwendung der eingekreisten Buchstaben Ⓐ/ⓐ empfohlen.

Die aktuelle Verschriftung der Sprache der heute in Oklahoma lebenden Yuchi verwendet das At-Zeichen für den Laut [æ] wie in englisch "at". Da diese Verschriftung keine Großschreibung verwendet (sondern lateinische Großbuchstaben als eindeutige Schreibung spezifischer Phoneme einsetzt), wird keine Großbuchstaben-Variante des „@“ verwendet.

In spanischsprachigen Ländern wird das At-Zeichen am Ende von Wörtern „kreativ“ eingesetzt, um eine geschlechtsneutrale Kurzschreibung zu erreichen. Es steht dann wahlweise für den Buchstaben "a" (weiblich) oder "o" (männlich). Die Verwendung tritt sowohl in informeller Kommunikation (z. B. Chat) als auch in offiziellen Dokumenten auf. Ein Beispiel ist das "Certificado Médico de nacid@ viv@" (Lebendgeborenen-Zertifikat) in Bolivien.

Im internationalen Zeichenkodierungssystem Unicode ist das Zeichen im Block Basis-Lateinisch enthalten als U+0040 . Dies ist die gleiche Position wie im älteren ASCII-Zeichensatz.

Im Internet-Dokumentenformat HTML wird es folgendermaßen kodiert:

Auf der üblichen deutschen (MF2-) Tastatur liegt das @-Zeichen als dritte Belegung auf der Taste und kann mit Hilfe der Taste eingegeben werden. Unter Windows-Systemen kann man auch statt die beiden Tasten und verwenden, was sich aber nicht empfiehlt, da diese Tastenkombination auf Windows-Systemen auch deaktiviert sein kann. Auf der Deutschschweizer Tastatur liegt es als dritte Belegung auf der 2-Taste, daher +.

Auf deutschen Apple-Tastaturen liegt das Zeichen als dritte Belegung seit Mac OS 9.1 auf der Taste und kann mit Hilfe der Wahltaste eingegeben werden. Davor lag es auf ++. Auf Schweizer Apple-Tastaturen liegt das Zeichen als dritte Belegung auf der G-Taste, daher +.

Bei der Neo-Tastaturbelegung liegt es auf  + .

Auf der britischen Tastatur liegt das Zeichen über dem Apostroph und auf der amerikanischen Tastatur über der Ziffer 2. Hier wird das Zeichen mit Hilfe der erreicht, also ohne das auf diesen Tastaturen meist gar nicht vorhandene .

Weiterhin ist es unter vielen Betriebssystemen möglich, das Zeichen mittels Eingabe seines ASCII-Codes 64 auf dem Ziffernblock bei gedrückter -Taste zu schreiben: +.

Kann das Zeichen nicht dargestellt werden, weil es in der verwendeten Schriftart oder dem Zeichensatz fehlt (beispielsweise im Teletext oder bei Schreibtelefonen), so kann es eventuell durch Hilfswörter wie „at“ (engl.) oder auch ‚an‘, ‚bei‘, ‚per‘, ‚pro‘, ‚für‘ oder ‚je‘ ersetzt werden. Bis 1982 konnte ein von Leerzeichen umschlossenes codice_11 statt codice_12 auch im damaligen Internet bei E-Mail-Adressen verwendet werden. Mit Ersetzung des RFC 733 durch RFC 822 entfiel diese Möglichkeit.

Da allerdings praktisch alle modernen Rechnersysteme und -schriften auf Unicode oder dem älteren ASCII-Standard basieren, kann das Zeichen problemlos weltweit dargestellt, verarbeitet, übertragen und archiviert werden. Eine Ersetzung aus technischen Gründen ist deshalb kaum nötig. Auch wenn die verwendete Tastatur das Zeichen nicht aufweist, kann es praktisch immer über eine entsprechende Funktion des Betriebssystems oder des jeweiligen Texteditors eingefügt werden.

In neuerer Zeit wird das Zeichen im Internet auch oft durch andere Zeichenfolgen wie zum Beispiel „(a)“, „(at)“, „[at]“ oder die oben erwähnten Hilfswörter ersetzt, um es Spambots zu erschweren, eine Zeichenfolge als E-Mail-Adresse zu erkennen.




</doc>
<doc id="7866" url="https://de.wikipedia.org/wiki?curid=7866" title="Wüste">
Wüste

Als Wüste bezeichnet man die vegetationslosen oder vegetationsarmen Gebiete der Erde. In Wüsten bedeckt die Vegetation weniger als 5 % der Oberfläche. Ursache für Wüsten sind entweder fehlende Wärme ("Kältewüste", "Eiswüste") der subpolaren und subnivalen Regionen, Überweidung oder Wassermangel ("Trockenwüste", "Hitzewüste"). Wüsten zählen zur Anökumene.

Die Sandwüste wird im Arabischen Erg genannt, in der westlichen Sahara und in der Libyschen Wüste auch "Edeyen". Eine Sandwüste ist eine Wüste mit einer Oberfläche, die überwiegend aus Quarzsand besteht, der durch die Bodenerosion einer Kieswüste entstand oder aus anderen Regionen eingeweht wurde. Sandwüsten nehmen, obwohl sie weithin fälschlich als Synonym für das Phänomen "Wüste" angesehen werden, nur etwa 20 % der Wüstenflächen der Erde und auch der Sahara ein.

Die Lebensbedingungen in den Sandwüsten sind härter als in anderen. Es gibt sie mit und ohne Dünen, die relativ stabil und in ihrem unteren Teil verfestigt sein können wie im südlichen Sandmeer und dort sog. "Gassis" bilden, oder die wie im nördlichen Sandmeer Ägyptens – etwa um Farafra – als Wanderdünen vorkommen in Gestalt von (je nach vorherrschender Windrichtung) Quer-, Längs-, Stern- oder Sicheldünen. Die höchsten Sanddünen findet man in Algerien, die längste ist der Abu Muharek mit ca. 600 km. Gut befahrbar sind nur verfestigte Sandebenen, ansonsten sind insbesondere Dünenfelder wie der Erg von Bilma auch mit Geländewagen nur mühsam passierbar. Die weltweit größte Sandwüste ist die Rub al-Chali in Arabien, und die zweitgrößte ist die Taklamakan.

Kieswüsten heißen lediglich in der Westsahara "Reg", in der Zentralsahara nennt man sie "Serir". Kieswüsten entstehen nach Erosion von Stein- oder Felswüsten (Akkumulation von gröberen Korngrößen durch Ausblasung der feineren Korngrößen) oder durch die Ablagerung von Kies im Vorfeld von Gletschern. Eine weitere Ursache ist ein physikalischer Effekt, den man auch bei gefrier­getrocknetem Kaffee findet, wo sich, wenn man den Behälter lange genug schüttelt, an der Oberfläche immer größere Partikel ansammeln, da die kleineren viel leichter nach unten rutschen, nur dass dieser Vorgang in der Wüste, wo Feuchtigkeit, Wind und die Temperaturunterschiede für die Bewegung der Sandkörner sorgen, bedeutend langsamer abläuft. Wagenspuren halten sich hier besonders lange. Kieswüsten sind gut passierbar.

Stein- oder Felswüsten nennt man auch Hammada. Die Oberfläche dieses Wüstentyps ist übersät mit dicht blockigem, kantigem Schutt- oder Felsmaterial, angesammelt als Ergebnis der physikalischen Verwitterung und der Auswehung des Feinmaterials. Meist sind es mit Geröll bedeckte Hochflächen. Mit dem Auto kaum passierbar, außer auf alten Karawanenstraßen, die man gewöhnlich wie in anderen Wüstenformen an den "Alamat" erkennt (kleine Steinpyramiden als Wegzeichen) sowie an den Kamelgerippen, die sie säumen. Auf der Oberfläche der Gesteine findet sich vermehrt Wüstenlack.

Salzwüsten nennt man in Algerien und Tunesien "Schott", in der zentralen und Ostsahara "Sebkha", in Libyen "Grara". Salzwüsten entstehen meist in ariden, abflusslosen Sedimentbecken durch starke Verdunstung. Sehr viele Wüsten des Typs liegen im Iran und Zentralasien. Sie sind schwer passierbar und wegen der Tümpel und Sumpffelder unter der Salzkruste möglichst zu meiden. Das Salz dieser Schotts repräsentiert allerdings nicht die Überreste eines alten Meeres – die Tethys (Ozean) gibt es schon seit 66 Millionen Jahren nicht mehr –, sondern es entstammt den Auswaschungen von aus umgebenden Bergländern herunter­geschwemmten Ablagerungen, die oft reichlich Salz enthalten, wobei es sich in abflusslosen Senken wie z. B. der Qattara-Senke naturgemäß ansammelte und dicke, stark salzangereicherte Ton- und Lehmflächen entstehen ließ, sog. Salztonebenen bzw. "Alkaliflats". Nach Niederschlägen wandelten diese sich zu Salzseen oder Salzsümpfen, die aus einem schlammigen Gemisch aus Ton, Salz und Sand bestehen. Die Namen des parallel zur Straße Kairo – Alexandria verlaufenden nordägyptischen Wadi El-Natrun, des libyschen Ortes El Atrun auf der Cyrenaika und der nordwestsudanesischen Oase El-Atrun sind Zeichen dieser Situation.

Dem geomorphologischen Typ der Eiswüste entspricht der klimatische Begriff der Kältewüste (siehe unten).

Die Wüsten der Erde können klimatisch in fünf Typen eingeteilt werden, je nach der Ursache für ihre Trockenheit.

Subtropische Wüsten, auch Passatwüsten oder Wendekreiswüsten genannt, liegen in zwei breiten Bändern die fast die ganze Erde umspannen, bei einer geographischen Breite bis zu etwa 30° beidseits des Erdäquators. Beispiele sind die größten Teile der Sahara und die Kalahari.

Sowohl auf der nördlichen als auch auf der südlichen Halbkugel werden die Luftmassen vom Urpassat kommend von den dort häufig auftretenden Hochdruckgebieten zum Absteigen gezwungen. Das erwärmt sie, wodurch die relative Luftfeuchtigkeit abnimmt und trockene, wolkenlose Klimaverhältnisse aufkommen.

Die Hochdruckgebiete kommen durch die innertropische Konvergenzzone, kurz ITC, zustande. Durch die starke Sonneneinstrahlung über einen großen Winkel wird in der Äquatorregion die Erde besonders stark erwärmt. Ebenso verdunstet viel Wasser. Da es in der Tropopause eine Inversionsschicht gibt, können die Luftmassen nicht weiter aufsteigen. Sie werden nach Norden und Süden abgelenkt. Durch die Kondensation des Wasserdampfes beginnt es zu regnen. In der Wendekreisregion beginnt die abgekühlte Luft, in der keine Feuchtigkeit mehr enthalten ist, abzusinken. Absteigende Luftmassen bewirken stets eine Auflösung der Wolken. In Bodennähe strömt die Luft wieder in die Äquatorregion zurück. Durch die Coriolisablenkung entstehen die Passatwinde.

Die Kalte Küstenwüste ist in vielfacher Hinsicht eine besondere Form der Subtropischen Wüste. Passate und spezielle Meeresströmungen verstärken ihre Trockenheit. Das kalte aufsteigende Wasser des Meeres kühlt die über ihr lagernden Luftmassen ab. Die in diesen Luftmassen enthaltene Luftfeuchtigkeit kondensiert, die relative Luftfeuchtigkeit steigt also und es bilden sich Wolken. Die Wolken haben allerdings so viel an Temperatur verloren, dass sie nicht mehr aufsteigen können – es entsteht eine stabile Schichtung und daher Nebel. Kommen diese Luftmassen nun in die Wüste, so werden sie erhitzt und verlieren stark an relativer Luftfeuchtigkeit, die Wolken lösen sich auf. „So nah am Wasser und doch so arm an Wasser“, hat Alexander von Humboldt einmal die Küstenwüste der Atacama beschrieben.

Weltweit gibt es drei gut entwickelte Fälle dieses Wüstentyps. Die Namib an der Küste von Südwestafrika, die Atacama, an der chilenischen und peruanischen Küste und die Wüste an der Pazifikküste von Niederkalifornien in Mexiko. Einige Grenzfälle existieren an der Nordwestküste von Afrika, auf der östlichsten der Kanarischen Inseln, an der Nordwestküste Australiens und möglicherweise an der Küste von Somalia.

Regenschattenwüsten sind durch die Gestalt der Erdoberfläche bedingt und werden daher auch Reliefwüsten genannt. Sie treten im Inneren der Kontinente auf, vor allem an hohen Gebirgsketten oder in Beckenlagen. In solchen Regionen fällt nur geringer Niederschlag, weil sie im Regenschatten auf der windabgewandten Seite von Randgebirgen liegen.

Die feuchten Luftmassen werden vor den Gebirgen zum Aufsteigen gezwungen. Oben auf der Gebirgskette ist die Luft kühler und kann daher weniger Wasser speichern: Die feuchten, kalten Luftmassen sind zum Abregnen gezwungen. Auf der anderen Seite der Gebirgskette erwärmt sich die Luft insgesamt (aufgrund der feuchtadiabatischen Abkühlung und der trockenadiabatischen Erwärmung) und die warmen, trockenen Luftmassen sinken. Unten bilden sich aufgrund der Wärme und Trockenheit Wüsten. Eine typische Regenschattenwüste ist die Wüste Juda.

Binnenwüsten befinden sich südlich der südlichen oder nördlich der nördlichen Wendekreise. Am bekanntesten sind die Wüste Gobi, die Taklamakan und der Great Basin.

Kontinentale Binnenwüsten und Regenschattenwüsten werden von manchen Forschern als "außertropische Wüsten" zusammengefasst.

Die Polargebiete sind Wüsten. Sie erhalten nur sehr geringe Niederschläge und die Feuchtigkeit liegt meist in gefrorener Form vor, wodurch das Wasser für Pflanzen nicht zur Verfügung steht. Durch die herrschenden extrem niedrigen Temperaturen ist der Boden gefroren und die Luft sehr trocken. Ein bekanntes Beispiel sind die hyperariden McMurdo-Trockentäler in der Antarktis, die zu den trockensten Gebieten der Erde zählen.

Windwüsten findet man auf subantarktischen Inseln im Südatlantik, südlich des 50. Breitengrades südlicher Breite. In diesem Gebiet ständiger Weststürme, die das ganze Jahr über Nieselregen und Nebel begleiten, können mangels Windschutz keine Bäume gedeihen. Man trifft lediglich Moose, Farne und Flechten an.

Die Halbwüste stellt eine Landschaftszone dar, die geringfügig feuchter als die echte Wüste, aber immer noch trockener als die Dornsavanne ist. Sie befindet sich meist am Rand (in der Übergangszone) einer solchen „Vollwüste“ – siehe auch Sahelzone.

In edaphischen (bodenbedingten) Wüsten werden zugeführte Niederschläge im stark wasser­durch­lässigen Boden sehr schnell abgeführt. Wasser kann sich nicht oder nur sehr schlecht im Boden speichern, es fehlt für pflanzliches Wachstum. So bilden die riesigen Schotterflure im Isländischen Hochland trotz erheblicher Niederschlags- und Schmelzwassermengen eine Wüstenlandschaft.

Die brasilianischen Lençóis Maranhenses sind ein vegetationsarmes Gebiet in einer ausgedehnte Dünenlandschaft. Hier herrschen weder Trockenheit noch Kälte.

Das Überleben in Wüstengebieten, mit ihren von Wassermangel geprägten besonderen Umweltbedingungen, zwingt Pflanzen und Tiere, aber auch den Menschen zu jeweils ganz spezifischen Anpassungen. Regenschauer sind selten, doch wenn es einmal regnet, dann meist sehr heftig. Danach "blüht die Wüste auf": Es wachsen farbenprächtige Wüstenpflanzen, die aber wegen des fehlenden Wassers einen kurzen Lebenszyklus haben. Dennoch gewährleisten u. a. auch diese kurzen Vegetationsperioden ein häufig erstaunlich reiches Tierleben.

Wüsten sind durch Vegetationsarmut oder gar Vegetationslosigkeit gekennzeichnet, nur etwa ein Viertel aller Wüstenflächen sind überhaupt bewachsen. Die vorhandene Vegetation (Xerophyten, Halophyten) wird durch an Trockenheit oder verstärkte Salzverträglichkeit angepasste Sträucher, Gräser und bestimmte tiefwurzelnde Bäume (z. B. Akazien in der Kalahari) bestimmt. Sie unterscheiden sich in wassersparenden, wasserspeichernden, unterirdisch überdauernden Pflanzen und in Pflanzen mit kurzer Vegetationszeit. So ist zum Beispiel in der Nebelzone der Namib-Wüste der Strauch "Arthraerua leubnitziae" (ein Fuchsschwanzgewächs) als häufigster Vertreter der ständigen Vegetation heimisch, er kann die hohe Luftfeuchtigkeit der Nebelschwaden nutzen. Pflanzen wie dieser gelingt es auch während der extremen und lange anhaltenden Dürreperioden (am Beispiel der Arthraerua leubnitziae mehrere Tausend Jahre) ihren Wasserhaushalt aufrechtzuerhalten.

In vielen Wüsten der Welt sind trotz der vermeintlich lebensfeindlichen Bedingungen zahlreiche Tierarten anzutreffen. So sind zum Beispiel in der Gobi neben anderen Großtieren die Kropfgazelle und der Steppeniltis heimisch, zuweilen findet man auch Schneeleoparden und Wölfe. Noch wesentlich zahlreicher als Säugetiere sind in den ariden Gebieten Reptilien und vor allem die außerordentlich anpassungsfähigen Gliederfüßer (z. B. Insekten und Skorpione) anzutreffen.

Gerade die in heißen Sandwüsten lebenden Tiere weisen häufig sehr augenfällige Anpassungen an die hohen Oberflächen­temperaturen des Sandes auf: so haben Insekten, die tagsüber auf dem Sand laufen, meist außergewöhnlich lange Stelzbeine, da die Temperatur schon wenige Zentimeter über dem Sand deutlich abnimmt. Hierdurch und durch eine schnelle Fortbewegung, sind die Tiere in der Lage, sich vor tödlicher Überhitzung zu schützen. Auch die langen Beine der Kamele könnten sich als Schutz vor der Abstrahlungshitze entwickelt haben.

Unabhängig von Klima, Temperatur und Breitengrad gibt es in trockenen Wüsten einen enormen Reichtum an unterschiedlichen Bakterienarten. Entscheidend für das Gedeihen dieser Mikroorganismen ist der pH-Wert (Säuregrad) des Bodens: so bieten Böden mit neutralem pH-Wert, wie sie in trockenen Wüsten und Wäldern vorkommen, den Bakterien optimale Lebensbedingungen. Dagegen findet man erstaunlicherweise in sauren Böden, wie z. B. denen der südamerikanischen Regenwälder, nur sehr wenige Bakterien.

In kulturhistorischer Hinsicht spielte die Wüste seit der Antike eine wichtige Rolle in der europäischen Historiographie und Literatur. Einerseits symbolisierte die Wüste seit Herodot das Fremde und Andersartige, das sich dem europäischen Zugriff entzog. Andererseits bot die Wüste aber auch Rückzugsmöglichkeiten. Insbesondere durch die Bibel (Auszug aus Ägypten der Israeliten, Versuchungen Christi) und die spätere hagiographische Literatur (Eremiten) wurde ein Bild der Wüste nach Europa transportiert, das im Kern bis heute fortwirkt. Durch die Domestizierung des Dromedars gelang es dem Menschen, tiefer in die großen Wüsten vorzudringen oder sie zu durchqueren. Dadurch konnte die Wüste zum Lebensraum des Menschen werden.

Das Entstehen neuer und die Ausbreitung bestehender Wüsten ist meist vom Menschen verursacht (Desertifikation). Dazu zählen Überweidung, unangepasster Ackerbau und Entwaldung. Natürliche Ursachen für Verwüstung sind Dürreperioden, Ausbreiten von Sanddünen oder Ausfransen von Wüstenrändern. Verwüstung wird durch Ausblasung (Wind), Abschwemmung (Wasser), Versalzung und Skelettierung gefördert.

Die UN-Organisation UNCCD kämpft gegen die weitere Ausbreitung der Wüsten. Das Jahr 2006 wurde zum Internationalen Jahr der Wüsten und Wüstenbildung erklärt.

Alle Wüsten der Erde zusammengenommen bedecken etwa ein Fünftel der gesamten Landfläche der Erde, das sind fast 30 Millionen Quadratkilometer. Werden auch die Halbwüsten mit hinzugerechnet, so ergibt sich etwa ein Drittel der Landfläche, also etwas weniger als 50 Millionen Quadratkilometer. Insgesamt bedecken sie knapp 10 % der gesamten Erdoberfläche.
Trockenwüsten können starken Temperaturschwankungen unterliegen, abhängig von Meeresentfernung und Jahreszeit. Tagsüber erhitzt sich der Boden aufgrund der schlechten Wärmeleitung des quarzhaltigen und luftdurchsetzten Wüstenbodens nur oberflächlich. Zudem kann dieser im Vergleich zu feuchten Böden nur wenig Wärmeenergie speichern (Wasser kann etwa sechsmal so viel Energie speichern wie Sand). Durch die geringe Wolkenbildung dringt tagsüber Wärmestrahlung zwar ungedämpft zu Boden und erhitzt diesen sehr stark (bis zu etwa 70 °C), allerdings strahlt nachts Wärme wieder ungehindert ins Weltall ab (Wolken wirken als Isolierungsschicht, sowohl vom Weltall zur Erde als auch umgekehrt). Das führt zu Temperaturunterschieden von 50 K und mehr, insbesondere im „Winter“ und weit vom für Temperaturausgleich sorgenden Meer entfernt.

Dieser Effekt ermöglicht auch in den trockensten Wüsten ein bescheidenes Leben. Wegen der starken Abkühlung wird ein bodennaher Taupunkt erreicht. Pflanzen und andere Lebewesen können dann von den gebildeten Tautropfen leben.

Aufgrund der starken Temperaturschwankungen wird die physikalische Verwitterung in der Wüste enorm gefördert. Die chemische Verwitterung erfolgt hingegen wegen des Wassermangels nur sehr langsam (vgl. Wüstenlack).





</doc>
<doc id="7867" url="https://de.wikipedia.org/wiki?curid=7867" title="Faschismus">
Faschismus

Faschismus war zunächst die Eigenbezeichnung einer politischen Bewegung, die unter Führung von Benito Mussolini in Italien von 1922 bis 1943/45 die beherrschende politische Macht war und ein diktatorisches Regierungssystem errichtete ("siehe" Italienischer Faschismus).

Ab den 1920er Jahren wurde der Begriff für alle extrem nationalistischen, nach dem Führerprinzip organisierten antiliberalen und antimarxistischen Bewegungen, Ideologien oder Herrschaftssysteme verwendet, die seit dem Ersten Weltkrieg die parlamentarischen Demokratien abzulösen suchten. Die Verallgemeinerung des Faschismus-Begriffs von einer zeitlich und national begrenzten Eigenbezeichnung zur Gattungsbezeichnung einer bestimmten Herrschaftsart ist umstritten, besonders für den Nationalsozialismus in Deutschland. Mit Neofaschismus bezeichnet man Strömungen und Parteien, die nach 1945 an die Tradition des Faschismus anknüpfen.

Der aus dem italienischen Wort für Bund – "fascio" – abgeleitete Begriff Faschismus wird von Historikern als "„gewissermaßen inhaltsleer“" beschrieben, da er "„so gut wie nichts über das Wesen dessen aus[sagt], was faschistisch ist oder sein soll“". Darin unterscheide sich dieser "Ismus" ganz entscheidend von anderen "Ismen", wie Konservativismus, Liberalismus oder Sozialismus. „Ein "fascio" ist ein Verein, ein Bund,“ daher wären Faschisten wörtlich übersetzt „Bündler“ und „Faschismus“ wäre Bündlertum.

Die Etymologie des Wortes "fascio" wird meist abgeleitet vom lateinischen "fasces". Diese "Rutenbündel" waren Machtsymbole zu Zeiten des Römischen Reiches, die die Liktoren vor den höchsten römischen Beamten, den Konsuln, Prätoren und Diktatoren, hertrugen.

Im 19. Jahrhundert bezeichnete das Wort "fascio" das Selbstverständnis der italienischen National- und Arbeiterbewegung als revolutionäre Kraft. So symbolisierte das Rutenbündel in der nationalen Bewegung im 19. Jahrhundert die Einheit der Nation, und "fascio" bezog sich im seit 1870 geeinten Italien auf unabhängige und sogar anarchistische Arbeiterorganisationen.

Der Begriff "Fascismo", der um 1900 zum Banner der revolutionären Arbeiterbewegung avanciert war, wurde ab 1919 mit den „Fasci di combattimento“ identifiziert: jene „Kampfbünde“, die Mussolini im März 1919 gründete.

Eine Definition von „Faschismus“ gestaltet sich als schwierig, da weder der Begriff an sich etwas über sein Wesen aussagt (siehe oben), noch die meisten europäischen Bewegungen der Zwischenkriegszeit, die im Allgemeinen als faschistisch bezeichnet werden, dieses Wort überhaupt verwendet haben – anders als fast alle kommunistischen Parteien und Regime, die es vorzogen, sich als kommunistisch zu bezeichnen.

Was Faschismus ist oder sein soll, wurde vornehmlich von seinen Gegnern bestimmt, die Theorien des bzw. über den Faschismus entwickelt haben. Seit den 1920er Jahren ist eine intensive Debatte um den Faschismus als umfassenden Gattungsbegriff geführt worden, der nicht nur die von Mussolini geführte Bewegung und Diktatur erklären, sondern ähnliche Organisationen und Regimes in anderen europäischen Staaten kennzeichnen soll. Die empirische Forschung hat dabei vorrangig auf die Identifizierung von strukturellen Kernelementen des Faschismus gezielt.

Ein übergreifender (generischer) Faschismusbegriff, der die bis zum Ende des Zweiten Weltkriegs bestehenden Regime in Italien, Deutschland und Japan umfasst, ist in der historischen Forschung umstritten. Einige Historiker wollen den Begriff auf Italien beschränken. Andere wie Bernd Martin halten „Faschismus“ als Gattungsbegriff nur für die „Bewegungsphase“ für sinnvoll:

Faschismusforscher wie zum Beispiel Roger Griffin, die von einem generischen Faschismusbegriff ausgehen, zielen auf den ideologischen Kern des Faschismus:

Mussolini gründete 1915 für Italiens Kriegseintritt die "Fasci d’azione rivoluzionaria" und bildete am 23. März 1919 aus den "Fasci dēi lavoratōri" und "Fasci siciliani" die Bewegung der "Fasci italiani di combattimento" („Italienischer Kampfverband“), der ein Rutenbündel zu seinem Zeichen machte. Er bestand anfangs überwiegend aus Anhängern des Syndikalismus, einer Weiterentwicklung des Gewerkschafts-Sozialismus, bis Mussolini ihn 1921 scharf gegen Sozialismus und Kommunismus abgrenzte. Damit wurde seine nun "Partito Nazionale Fascista" (PNF) genannte Partei auch von bürgerlichen Mittelschichten wählbar und von Teilen der katholischen Kirche, des Beamtentums und der Armee Italiens unterstützt.

Mit Hilfe von Paramilitärs, Straßenterror, einem starken Personenkult, Massenpropaganda und dem wirksam inszenierten „Marsch auf Rom“ eroberte Mussolini 1922 das Amt des italienischen Ministerpräsidenten. Er baute dann schrittweise mit einem Ermächtigungsgesetz, Verbot der übrigen Parteien, Aufhebung der Bürgerrechte und Pressefreiheit, Ausbau der Parteimiliz und politischen Morden bis 1925 eine Einparteiendiktatur unter einem von ihm geführten „Großen Faschistischen Rat“ in Italien auf.

1932 legte er die Ideologie seines Staatssystems schriftlich vor "(La dottrina del fascismo)": Merkmale waren ein extremer Nationalismus, eine durch Krieg angestrebte Großmachtstellung für Italien im Mittelmeerraum, die Betonung des „Willens zur Macht“ (Friedrich Nietzsche), des autoritären Führerprinzips (Vilfredo Pareto), der „direkten Aktion“ als „schöpferischem Gestaltungsprinzip“ (Georges Sorel) und einer totalitären, von einer Geheimpolizei überwachten Verschmelzung von Staat und alleinregierender Partei. Die sozialrevolutionäre Komponente der Aufstiegszeit trat zurück; verordnete Einheitsorganisationen von Arbeitern und Unternehmern sollten Klassenkampf unterbinden. Um neben der Macht auch die Hegemonie im Sinne Antonio Gramscis zu gewinnen, übernahm der Staat auch die Sportbewegung. Hiermit sollten Körperkult, Verherrlichung von Kraft, Männlichkeit, Demonstration der italienischen Überlegenheit in körperbezogenen Aktivitäten wie Sport, Fußball-Weltmeisterschaft und Olympischen Spielen gewonnen werden. Das "Comitato Olimpico Nazionale Italiano" wurde verstaatlicht und der Spitzensport mit Staatsamateuren international leistungsfähig gemacht.

Als Kennzeichen des Faschismus nach italienischem Vorbild gelten daher voluntaristische und futuristische Politikkonzepte, die den Machtwillen ökonomischen Zwängen vorordnen und die künftige radikale Umgestaltung der Gesellschaft als nationale Bestimmung anstreben, eine offen terroristische und diktatorische Herrschaftsform, die sich als Volkswille ausgibt, mit ausgeprägtem Personenkult und einer starken Ästhetisierung der Politik, die gegensätzliche Interessen und Strömungen überwölben und zusammenhalten soll.

Die faschistische „Neue Ordnung“ Italiens unterschied sich durch ihren Etatismus deutlich vom NS-Regime, indem Mussolinis "starker Staat" die alten Eliten einband. 

Zur Eroberung von Lebensraum "(spazio vitale)" war das faschistische System auf kriegerische Expansion aufgebaut. Von 1923 bis 1934 führte Italien den zweiten Italienisch-Libyschen Krieg, ab 1935 den Abessinienkrieg, ab 1936 beteiligte es sich am spanischen Bürgerkrieg, 1939 folgte die italienische Besetzung Albaniens, 1940 der Eintritt in den Westfeldzug und der griechisch-italienische Krieg, 1941 die Beteiligung am Balkanfeldzug gegen Jugoslawien und die Kämpfe gegen die Sowjetunion und in Nordafrika. Die italienische Repression in den besetzten Gebieten Afrikas mit der Liquidierung der äthiopischen Intelligenz und des Klerus ist mit dem deutschen Besatzungsterror in Polen vor dem Überfall auf die Sowjetunion vergleichbar. Zur Repression gegen die Untergrundbewegung auf dem Balkan wurde die gleiche Strategie der verbrannten Erde, der ethnischen Säuberungen, der Masseninternierung in italienischen Konzentrationslagern, der Geiselnahme, Geiselerschießung und der italienischen Kolonisation übernommen wie sie zuvor vom italienischen Militär in Afrika praktiziert worden war. Dabei stand für die Faschisten fest, es auf dem Balkan und in italienisch-Ostafrika mit kulturell, wenn nicht auch mit biologisch minderwertigen Rassen zu tun zu haben. Durch diesen Antiafrikanismus und Antislawismus lud sich die Repression auf.

Anfangs war der Faschismus nicht antisemitisch ausgerichtet. Wiederholt lehnte Mussolini in öffentlichen Äußerungen den Rassismus und Antisemitismus der Nationalsozialisten ab, in dem er eine Wiederkehr des „Germanismus“ sah, den er in seiner Jugend stets bekämpft habe. Erst seit Mitte der 1930er Jahre gab es infolge der politischen Koalition Mussolinis mit dem Deutschen Reich antisemitische Agitationen, die dann auch in den Erlass der italienischen Rassengesetze mündete. Diese Politik zielte aber niemals auf Vernichtung der europäischen Juden, sondern auf ihre Entrechtung, Enteignung und Vertreibung.

Die folgenden Tabellen beruhen auf den Forschungsergebnissen der "vergleichenden" Faschismusforschung und behandeln ausschließlich faschistische "Bewegungen", die von dieser überwiegend als solche eingestuft werden.

Die "Jungägyptische Partei" wurde im Oktober 1933 als eine radikal-nationalistische Gruppierung mit religiöser Orientierung von den 22-jährigen Ahmed Husayn und Fathi Radwan gegründet. Das Ziel der Partei war die Schaffung eines Großreiches durch die Eingliederung des Sudans an Ägyptens, welches die Rolle einer „Führungsmacht sowohl innerhalb der arabischen als auch der islamischen Welt“ einnehmen sollte. Die Partei verfügte mit den sogenannten "Grünhemden" über eine paramilitärische Organisation. Die Jungägyptische Partei orientierte sich mit dem politischen Machtzuwachs Deutschlands auch am nationalsozialistischen Deutschen Reich, dem Gegner Großbritanniens, und verfolgte ebenfalls die Strategie eines nationalen Kapitalismus. Unter dem Druck der Regierung wurden die Grünhemden im Jahre 1938 verboten.

Der "Brasilianische Integralismus" war eine rechtsextreme politische Bewegung in Brasilien, welche sich in der 1932 gegründeten Partei Ação Integralista Brasileira (Integralistische Aktion Brasiliens) formierte. Die Integralisten erlangten unter der Präsidentschaft von Getúlio Vargas zeitweise politischen Einfluss, wurden jedoch mit der Ausrufung des Estado Novo im Jahr 1937 aufgelöst. Ein integralistischer Putschversuch 1938 gegen den Präsidenten scheiterte und führte zur endgültigen Zersplitterung der Bewegung.

Die "Nationalsozialistische Bewegung Chiles" oder auch "Nacismo" war eine nationalsozialistische Partei in Chile. Obwohl die Partei gemessen an Mitgliederzahlen und Wahlergebnissen immer eine Kleinpartei blieb, war sie nicht unbedeutend, insbesondere wegen eines Putschversuches 1938. Wichtigste Persönlichkeit war der „Jefe“ Jorge González von Marées. Anfang 1939 taufte sich die Partei in "Vanguardia Popular Socialista" um und distanzierte sich vom Faschismus.

Der "Revisionistische Maximalismus", der Teil der "Brit HaBirionim"-Fraktion des Revisionistischen Zionismus war, war eine von Abba Ahimeir, Uri Zvi Greenberg und Joshua Yeivin erdachte Ideologie. Sie verband Faschismus mit Zionismus: Ihr Ziel war es einen „Judenstaat“ nach dem Vorbild des faschistischen Italien zu gründen. 
1933 verhaftete die britische Verwaltung mehrere Mitglieder, einschließlich Ahimeirs und klagte sie des Mordes an Chaim Arlosoroff an. Obwohl freigesprochen, litt das Ansehen der Gruppe unter dieser Anklage, was zu ihrer Isolierung und schließlich zu ihrer Auflösung führte.

Der revolutionäre Impuls zahlreicher Theoretiker (wie Kita Ikki oder Takabatake Motoyuki), Gruppierungen und Parteien ab den 1920er Jahren war schwächer als in Europa ausgeprägt und eher auf die Vorherrschaft einer bürokratischen, nichtdemokratischen, konstituellen Monarchie auf Basis traditioneller Werte als auf eine völlig neue Ordnung gerichtet. Die ab 1936 stärksten Gruppen, die nach der Hitlerjugend geschaffene "Großjapan-Jugendpartei" (, "Dai-Nippon Seinen-tō") und die politische Partei "Gesellschaft des Östlichen Weges" (, "Tōhōkai"), waren keine faschistischen Bewegungen, kamen aber faschistischen Organisationen am Nächsten. Der japanische Autoritarismus ab 1940 kann eher als ein komplexes Gemenge von Staatsbürokraten, konservativen Wirtschaftsführern und militärischen Prätorianern beschrieben werden.

Die Anfangsperiode der Shōwa-Zeit von 1926 bis 1945, speziell ab dem Angriff auf China 1937, als Faschismus zu bezeichnen ist problematisch. Dennoch wird der Ausdruck "Tennō-Faschismus" durchaus verwendet. Westliche Wissenschaftler räumen den Unterschieden zu den europäischen Faschismen breiteren Raum ein, modifizieren den Begriff zu „Militär- oder Kaisersystemfaschismus“, oder lehnen ihn – trotz Parallelen hinsichtlich Autoritarismus, Militarismus, imperialen Anspruch und rassischer Ideologie – in Bezug auf Japan als ungeeignet ab. So hält George M. Wilson das Konzept eines „japanischen Faschismus“ für verfehlt, da in Japan keine politische Bewegung die Macht an sich reißen wollte, die formelle verfassungsmäßige Autorität zumindest nach außen intakt geblieben sei und ein gewisses Maß an Pluralismus weiter existiert habe. Gregory J. Kasza verweist auf das Fehlen wesentlicher Elemente des Faschismus, wie einer Einheits- oder Massenpartei oder eines „Führers“, sowie auf die großteils kriegsbedingte Einführung „typisch faschistischer“ Elemente. Die Reihenfolge von „Bewegung – Ideologie – Regime“ des europäischen Faschismus sei in Japan genau in umgekehrter Reihenfolge anzutreffen. Ein Versuch der Etablierung einer Einheitspartei auf Konsensbasis war die "Taisei Yokusankai" (1940–1945) von Premierminister Konoe Fumimaro, die jedoch von inneren Grabenkämpfen beherrscht war und aus der beispielsweise die "Tōhōkai" 1941 wieder austrat. Vor der Shūgiin-Wahl 1942 gründete Premierminister Tōjō Hideki die "Yokusan Seijikai" (), verbot alle anderen Parteien und nahm alle gewählten Abgeordneten zwangsweise auf.

Die Ossewabrandwag-Bewegung wurde 1939 von calvinistischen Buren gegründet. Die Organisation war der nationalsozialistischen Regierung in Deutschland gegenüber positiv eingestellt und wandte sich vehement gegen die Teilnahme der Südafrikanischen Union am Zweiten Weltkrieg auf Seiten der Alliierten. Die Mitglieder weigerten sich, am Krieg teilzunehmen, und schikanierten uniformierte Soldaten. Am 1. Februar 1941 kam es in Johannesburg zu einem Gewaltausbruch, bei dem 140 Soldaten durch OB-Mitglieder verletzt wurden. 
Die "Stormjaers" („Sturmjäger“) waren der paramilitärische Flügel der Organisation und waren der SA nachempfunden. Diese verübten während des Krieges Sprengstoffanschläge auf Versorgungsleitungen und Bahnstrecken. 1941 hatte die Ossewabrandwag rund 350.000 Mitglieder. 
Im Dezember 1942 war die Ossewabrandwag durch Präsident Jan Smuts verboten worden; Tausende Mitglieder, unter ihnen der spätere Premierminister Vorster, wurden bis zum Kriegsende in Internierungslagern inhaftiert. Die Gruppierung löste sich 1952 endgültig auf.

Die pansyrische "Syrische Soziale Nationalistische Partei" wurde 1932 von dem griechisch-orthodoxen Journalisten Antun Sa'ada in Beirut gegründet. Der Politikwissenschaftler Gilbert Achcar bezeichnete sie als „ein(en) levantinischen Klon der Nazi-Partei in fast jeder Hinsicht: in ihrer politischen Ideologie, einschließlich der Aufklärungsfeindlichkeit, und ihrer geographisch-rassisch-nationalistischen Theorie mit pseudowissenschaftlichem Anstrich ebenso wie in der Organisationsstruktur und im Führerkult. Sogar die Parteifahne in Rot und Schwarz mit einer vierzackigen Schraube anstelle des Hakenkreuzes ist der Nazi-Fahne nachempfunden.“ 
Nachdem die Bewegung von Deutschland bei einem geplanten Putschversuch 1935 nicht unterstützt wurde, distanzierte sich diese allmählich vom Nationalsozialismus und Sa'ada emigrierte schließlich 1938 nach Südamerika.

Im Libanon wurde außerdem 1936 die "Kata’ib" von Pierre Gemayel gegründet und war von der spanischen Falange inspiriert. Die ursprünglichen Uniformen beinhalteten die Braunhemden. Die Partei nahm im libanesischen Kampf um die Unabhängigkeit von Frankreich teil, welche 1943 erreicht wurde.

1933 ursprünglich als "Friends of New Germany" von Heinz Spanknöbel in Chicago gegründet, entwickelte sich der "Amerikadeutsche Bund" zur größten nationalsozialistischen Organisation in den USA. Der Amerikadeutsche Bund bekannte sich zur idiosynkratischen „Verfassung, der Fahne, und einem von weißen Nichtjuden gelenkten, wahrhaft freien Amerika“. Er verfolgte mehrere Ziele, darunter den Kampf gegen den von Samuel Untermyer initiierten, jüdischen Warenboykott NS-Deutschlands, die Bildung einer Urzelle für eine neue US-Armee im Kampf gegen den Kommunismus und die Übernahme von den Teilen der NS-Wirtschaft, die man zur Wiederherstellung nach der Weltwirtschaftskrise für sinnvoll hielt. Der Bund war nach dem Führerprinzip unter dem „Bundesführer“ als „historischer Persönlichkeit“ organisiert. Nach der NS-Vorstellung, dass Blut wichtiger ist als Staatsbürgerschaft oder Geburtsort, waren alle Deutschamerikaner, die man „Deutsche in Amerika“ nannte, somit dem „Vaterland“ verbunden. Adaptiert wurden u. a. der Hitlergruß, Blut-und-Ehre-Gürtel, Hakenkreuz-Fahnen.
Im Jahr 1939 wurde Bund-Führer Fritz Kuhn wegen Unterschlagung von Geldern seiner Organisation und Steuerhinterziehung zu mehreren Jahren Haft verurteilt. Ihm folgten für jeweils kurze Zeit mehrere neue Bund-Führer. Die Organisation löste sich in der Folgezeit auf.








</doc>
<doc id="7874" url="https://de.wikipedia.org/wiki?curid=7874" title="Tragwerksplaner">
Tragwerksplaner

Der Tragwerksplaner (auch "Statiker") entwirft das Tragwerk von Gebäuden, Ingenieurbauwerken und anderen baulichen Anlagen. Er gehört zu den Projektanten eines Bauvorhabens. Meist erstellt er den nach dem Bauordnungsrecht erforderlichen Standsicherheitsnachweis.
Grundlage seiner statischen Berechnungen sind Last- und Tragfähigkeitsannahmen sowie Berechnungsmodelle, die er üblicherweise den entsprechenden Normen (Allgemein anerkannte Regeln der Technik) entnimmt. Die Tätigkeit des Tragwerksplaners kann mit weiteren Aufgaben wie z. B. der Wärmeschutzberechnung oder dem Brandschutznachweis verbunden sein. 

Ziel seiner Tragwerksplanung ist es, die erforderliche Tragfähigkeit und Gebrauchstauglichkeit einer Baukonstruktion während der vorgesehenen Lebensdauer mit den Forderungen nach Wirtschaftlichkeit und Ästhetik in Einklang zu bringen. Zur Umsetzung dieser Aufgabenstellung wird meist die statische Berechnung angewandt, welche auf den Regeln der Baustatik beruht. In Ausnahmefällen dienen Versuche als Nachweis der Realisierbarkeit. 

Ein verantwortlicher Baustatiker oder Tragwerksplaner wird oft umgangssprachlich als Statiker bezeichnet.

Im Sinne des Bauordnungsrechts ist in Deutschland der Tragwerksplaner als Entwurfsverfasser oder als ein vom Entwurfsverfasser oder vom Bauherrn herangezogener Sachverständiger tätig. Als Entwurfsverfasser hat er dafür zu sorgen, dass die für die Ausführung notwendigen Zeichnungen, Berechnungen und Anweisungen geliefert werden und den genehmigten
Bauvorlagen, den öffentlich-rechtlichen Vorschriften und den als Technische Baubestimmungen eingeführten technischen Regeln entsprechen.

Im Regelfall ist der Tragwerksplaner ein Bauingenieur. Im Rahmen seines Studiums werden diesem umfassende Kenntnisse sowohl der Statik von Tragstrukturen als auch der baustoffspezifischen Bemessungen (Beton- und Stahlbetonbau, Holz- und Stahlbau, Grundbau usw.) vermittelt. 

Auch dem Großteil der Architekten werden während der Ausbildung in der Tragwerkslehre die Grundprinzipien zur Planung der Tragwerke erläutert. Bei den staatlich geprüften Bautechnikern und den Bauhandwerksmeistern (Zimmerer- und Maurermeister) gehören grundlegende Statikkenntnisse ebenfalls zur Ausbildung. Architekten, Bautechniker und Handwerksmeister sollen so in die Lage versetzt werden, Statiken zu lesen und zu verstehen. Zimmerermeister haben früher selbst Statiken angefertigt.

Eine offizielle Beschreibung der Tätigkeitsbereiche des Tragwerksplaners gibt es für Deutschland in der Honorarordnung für Architekten und Ingenieure.




</doc>
<doc id="7875" url="https://de.wikipedia.org/wiki?curid=7875" title="Leistungsverzeichnis">
Leistungsverzeichnis

Ein Leistungsverzeichnis (kurz LV) ist Bestandteil einer Leistungsbeschreibung und beschreibt in Form von Teilleistungen eine im Rahmen eines Auftrages zu erbringende Gesamtleistung. Bereits für die Ausschreibung kann die Leistung durch das Leistungsverzeichnis beschrieben werden. Alternativ gibt es die Leistungsbeschreibung nach Leistungsprogramm.

Die Teilleistungen des Leistungsverzeichnisses werden oft als Positionen bezeichnet. Häufig wird das Leistungsverzeichnis durch eine allgemeine Beschreibung des Vertragsgegenstandes ergänzt. Leistungsverzeichnisse sind Grundlage der Aufträge in zahlreichen Branchen. In vielen Bereichen werden Leistungsverzeichnisse durch bestehende Regelwerke, Normen und Vorschriften ergänzt. Teilweise werden auch vereinheitlichte Textbausteine zur Beschreibung der Leistungen verwendet. Zum Beispiel im Bau-, Baunebengewerbe und in der Haustechnik.

Im Regelfall wird ein Leistungsverzeichnis hierarchisch in Gruppenstufen gegliedert (z. B. Los, Gewerk, Abschnitt, Titel), in denen dann unter Ordnungszahlen die verschiedenen Teilleistungen aufgeführt sind.

Die Vorteile des Leistungsverzeichnisses sind im Allgemeinen die klare und vollständige Darstellung des gesamten Vertrags-Solls, auch als Grundlage für die Einholung mehrerer vergleichbarer Angebote im Wettbewerb und die nachfolgende Erstellung eines Preisspiegels.
Innerhalb eines Leistungsverzeichnisses werden verschiedene Teilleistungen, auch Positionen genannt, unterschieden:
Ein Leistungsverzeichnis ist tabellarisch aufgebaut und besteht aus folgenden Teilen:

Für die Erstellung von Leistungsverzeichnissen verwendet man heutzutage im professionellen Bereich spezielle Software. Besonders im Bauwesen, wo Leistungsverzeichnisse für die Ausschreibung von Bauleistungen üblich sind, haben sich sogenannte AVA-Systeme etabliert. Dabei steht "AVA" als Akronym für die Prozesse Ausschreibung, Vergabe und Abrechnung. Umfangreiche dynamisch zu generierende Texte werden dabei vom Gemeinsamen Ausschuss Elektronik im Bauwesen zur Verfügung gestellt. Leistungsverzeichnisse werden im Bauwesen überwiegend elektronisch ausgetauscht. Dafür stellt der GAEB ein entsprechendes Datenaustauschverfahren für Deutschland und die ÖNORM ein entsprechendes Datenaustauschverfahren für Österreich zur Verfügung.

Öffentliche Ausschreibungen unterliegen dem komplizierten Vergaberecht, das zahlreiche verschiedene Fälle unterscheidet. Danach ist zum Beispiel bei Bauleistungen nach § 7 Abs. 1 Nr. 1 VOB/A "die Leistung eindeutig und so zu beschreiben, dass alle Bewerber die Beschreibung im gleichen Sinne verstehen müssen und ihre Preise sicher und ohne umfangreiche Vorarbeiten berechnen können". Nach § 7 Abs. 1 Nr. 1 VOB/A ist "erforderlichenfalls die Leistung auch zeichnerisch oder durch Probestücke darzustellen oder anders zu erklären". Im Leistungsverzeichnis dürfen nach VOB seit 2007 keine Hersteller und Produktbezeichnungen genannt werden.

Im Vergaberecht ist auch die Vergabe selbst geregelt. 





</doc>
<doc id="7876" url="https://de.wikipedia.org/wiki?curid=7876" title="Statische Berechnung">
Statische Berechnung

Eine Statische Berechnung (umgangssprachlich auch "Statik") ist die Berechnung der Kräfte, Spannungen und Verformungen einer Konstruktion beispielsweise im

Ziel ist es, festzustellen, ob die Konstruktion mit ausreichender Sicherheit und nicht unter der geplanten Belastung versagen (brechen, knicken usw.) wird, oder zu untersuchen, welche Belastungen die Konstruktion aushält, ohne zu versagen. Die Belastungen und Materialkennwerte werden mit Teilsicherheitsfaktoren beaufschlagt, um unter anderem Vereinfachungen des jeweiligen Berechnungsverfahrens sowie Streuungen der Lastannahmen und Materialeigenschaften auszugleichen.
Des Weiteren ist es Aufgabe der Statik, die Gebrauchstauglichkeit einzelner Bauteile zu gewährleisten (Verformungen und Schwingungen erträglich zu begrenzen).

Es wird unterschieden nach:

Im Bauingenieurwesen bezeichnet man die gesamte Konstruktionsplanung eines Bauwerks als Tragwerksplanung. Die Statische Berechnung ist ein Teil davon; im Wesentlichen handelt es sich dabei um den rechnerischen Teil. Bestandteil der statischen Berechnung sind aber auch statische Übersichtspläne (sogenannte statische Positionspläne) in denen sich die einzelnen Positionen der Berechnung und auch die wesentlichen Bauteilabmessungen, Baustoffe usw. wiederfinden.

Die meisten Bauvorhaben bedürfen einer Genehmigung durch die zuständige Bauaufsichtsbehörde. Im Rahmen des Bauantragsverfahrens kann diese eingeholt werden. Zu den erforderlichen Nachweisen gehört unter anderem eine von einem Sachverständigen erstellte statische Berechnung des Bauwerks. Sie dient dem Nachweis der Standsicherheit, der Gebrauchstauglichkeit und der Dauerhaftigkeit.

Nach den letzten Bauordnungsrechtsnovellen ist allerdings nur noch bei Sonderbauten die Statik zusammen mit dem Bauantrag zur Prüfung einzureichen. Für alle anderen Bauvorhaben reicht eine spätere Vorlage – diese muss in den meisten Fällen jedoch vor Baubeginn sein.

Für Bauwerke ab mittlerer Schwierigkeit muss der Bauherr die Statik durch einen Prüfstatiker überprüfen lassen (Fremdüberwachung der Berechnung). Für einfache Bauvorhaben wie Ein- oder Zweifamilienhäuser ist im Regelfall keine externe Prüfung erforderlich (Hier ist die Eigenüberwachung der Tragwerkplaners selbst, z. B. durch einen zweiten Ingenieur im Büro oder durch den Abgleich mit ähnlichen Berechnungen aus der Vergangenheit hinreichend).

Die statischen Berechnungen werden durch Tragwerksplaner (Statiker) aufgestellt, die über ausreichende Kenntnisse und Erfahrungen verfügen müssen. In den meisten Bundesländern werden inzwischen Listen mit bauvorlageberechtigten Tragwerksplanern geführt, bei denen dieses formal zutrifft. Teilweise wird von den Tragwerksplanern auch noch der Nachweis eines ausreichenden Versicherungsschutzes verlangt.

Eine übertriebene Genauigkeit in der statischen Berechnung ist nicht erforderlich, da die Auslastung in Prozent angegeben wird. Das heißt nur 3 signifikante Stellen sind im Ergebnis notwendig. Gleichzeitig sind sowohl Materialkennwerte und Lastannahmen sowie teilweise auch die Rechenmodelle mit Ungenauigkeiten behaftet. Diese Ungenauigkeiten wurde bei der Kalibrierung der (Teil-)Sicherheitsfaktoren Rechnung getragen. Durch die Verwendung von Rechenmaschinen und Computern ergeben sich heute zwar große theoretische Genauigkeiten. Prinzipiell sind aber immer noch Genauigkeiten von 3 bis 4 signifikanten Stellen (nicht Nachkommastellen) ausreichend. Zu Zeiten als die statischen Berechnungen noch mit dem Rechenschieber gemacht wurde, wurde eine Genauigkeit von 3 Zahlenstellen als ausreichend erachtet. Mehr ist auf dem Rechenschieber auch kaum ablesbar.
Im Maschinenbau werden statische Berechnungen unter anderem für Krane und Fundamente aufgestellt.
Die Klassifikationsgesellschaften geben Regeln zur Dimensionierung von Bauteilen heraus, die die statische Berechnung unterstützen und teilweise ersetzen. Wenn davon abgewichen wird, ist mit einer eigenen statischen Berechnung ein Festigkeitsnachweis zu erbringen. Statische Berechnungen bestehen aus der Längsfestigkeit - das Schiff wird näherungsweise als Biegebalken unter dem ungleichmäßig verteilten Einfluss von Gewicht, Ladung und Auftrieb betrachtet - und aus der Querfestigkeit, in der eine herausgeschnittene „Scheibe“ unter dem Einfluss von Eigengewicht, Ladung und hydrostatischem Druck nach Balkentheorie berechnet wird. Ähnlich wie der Prüfstatiker im Bauingenieurwesen erbringen Klassifikationsgesellschaften die Dienstleistung, Festigkeitsrechnungen im Schiffbau und schiffbaunahen Branchen zu zertifizieren.

Mit der Formel für das Biegemoment einer Gleichlast am Einfeldträger lassen sich auch viele schwierigere statische Systeme in guter Näherung (auf der sicheren Seite) berechnen. Sie wird daher für Überschlagsberechnungen – insbesondere ohne EDV − gerne verwendet. Bei der Vordimensionierung können erfahrene Tragwerksplaner häufig die notwendigen Dimensionen ohne Berechnung festlegen.




</doc>
<doc id="7877" url="https://de.wikipedia.org/wiki?curid=7877" title="Standsicherheit">
Standsicherheit

Die Standsicherheit ist die Anforderung an bauliche Anlagen, nicht einzustürzen. Im Rahmen des rechnerischen Standsicherheitsnachweises wird sie als Quotient zwischen den aufnehmbaren und den vorhandenen Beanspruchungen eines Tragwerks berechnet.

Im Bauwesen und in der Statik wurden verschiedene Normen entwickelt, die für bestimmte Standsicherheitsnachweise eine erforderliche Standsicherheit definieren.

Zum Nachweis der Standsicherheit müssen verschiedene Versagensmechanismen einzeln nachgewiesen werden. Sie können in Systemversagen und örtliches Versagen untergliedert werden. Bei einem Systemversagen wird das Gesamtsystem instabil. Ein Beispiel dafür wäre das Kippen einer Wand. 
Bei einem örtlichen Versagen tritt an einem örtlich begrenzten Bereich eine für das verwendete Material zu große Beanspruchung auf. Beispielsweise wird die maximal aufnehmbare Spannung für eine Mörtelfuge in einer Mauerwerkswand überschritten. Dies kann zu unerwünschten Rissen in der Wand führen. Je nach Tragreserven im Gesamtsystem kann ein örtliches Versagen auch zu einem Systemversagen führen.

Die Berechnung der Beanspruchungen (in der Regel Spannungen) erfolgt über die Lösung von Differentialgleichungen. In der Regel können die Differentialgleichungen nicht exakt gelöst werden. Es werden daher physikalische oder numerische Näherungslösungen ermittelt. Ein Beispiel für eine physikalische Näherung ist die Plattentheorie, bei der das Tragwerk einer Decke über Zustandsgrößen für eine Fläche ermittelt werden. Ein Beispiel für ein numerisches Näherungsverfahren ist die Finite-Elemente-Methode (FEM).

Ein Beispiel für ein einfaches Verfahren zur Standsicherheitsberechnung ist das Kragträgerverfahren, das mit Balkentheorie auskommt.



</doc>
<doc id="7879" url="https://de.wikipedia.org/wiki?curid=7879" title="A">
A

A sowie a (gesprochen: []) ist der erste Buchstabe des klassischen und modernen lateinischen Alphabets. Er steht für unterschiedlich ausgesprochene Vokallaute. Er entspricht dem Alpha im Griechischen und dem Buchstaben А im Kyrillischen Alphabet. Der Buchstabe A hat in deutschen Texten eine durchschnittliche Häufigkeit von 6,51 % und ist somit der sechsthäufigste Buchstabe in deutschen Texten hinter dem R und vor dem T. Im Morsealphabet werden A und a mit •– dargestellt.

Von Fremdwörtern und Namen abgesehen ist das A der einzige Buchstabe in der deutschen Sprache, der zweifach am Anfang eines Wortes stehen kann, etwa im Wort Aal.

Die aus dem proto-semitischen Alphabet stammende Urform des Buchstabens ist wahrscheinlich der Kopf eines Ochsen. Die Phönizier gaben diesem Buchstaben den Namen Aleph (Ochse). Im phönizischen Alphabet im 9. Jahrhundert v. Chr. war das Schriftzeichen bereits stark stilisiert, die Hörner des Ochsen wurden durch zwei Striche nach rechts angedeutet. Der Lautwert des Aleph bei den Phöniziern war der Knacklaut . Bereits bei den Phöniziern hatte Aleph die erste Stelle im Alphabet inne, was im hebräischen Alphabet übernommen wurde, als Aleph (), außerdem besteht eine Verwandtschaft mit dem ersten Buchstaben des arabischen Alphabets, dem Alif ().

Als die Griechen das phönizische Alphabet übernahmen, drehten sie das Zeichen um 90° und machten daraus das Alpha. Dabei hatten sie keinen Bedarf an dem stimmlosen glottalen Plosiv der Aussprache, der bei den Phöniziern vorhanden war, und da das Griechische reich an Vokalen war, verwendeten sie das Zeichen für den Lautwert . Bei den ältesten griechischen Schriftstücken aus dem 8. Jahrhundert v. Chr. wurde der Buchstabe dabei noch liegend verwendet, die um 90° gedrehte Version, die in späteren Schriftstücken auftauchte, setzte sich durch.

Die Etrusker übernahmen das frühgriechische Alpha und ließen es größtenteils unverändert. Lediglich zur besseren Schreibung (von rechts nach links) versahen sie das Zeichen mit einem Abschwung nach links. Als die Römer das lateinische Alphabet schufen, verwendeten sie das A aus dem etruskischen Alphabet, der Lautwert ist ebenfalls seit den Griechen beibehalten worden. Dieses Alphabet wird bis heute für eine Vielzahl von Schriftsprachen genutzt, darunter für die meisten der europäischen Sprachen, wovon es Eingang in das kyrillische Alphabet fand.

Das Grundaussehen der Großbuchstaben ist das zweier (symmetrisch) diagonal oben zusammenlaufender Linien und einer Waagerechten in der Mitte, jedoch folgen nicht alle Schriftarten und -familien diesem Konzept. Die Außenlinien können in Richtung und Strichstärke asymmetrisch sein oder unten parallel verlaufen und oben in einen Bogen (z. B. Bauhaus) oder in eine vierte, waagerechte Linie (z. B. Siebensegmentanzeige) übergehen. Teilweise entfällt die Mittellinie, wodurch sich das Aussehen einem griechischen Lambda (Λ) oder einem vergrößerten kleinen n annähert. In manchen gebrochenen Schriften (z. B. Fraktur) sinkt die Mittellinie auf die Grundlinie und die linke Außenlinie wird zu einem nach innen gewölbten Bogen, wodurch der Buchstabe oben statt unten offen ist. In der Schreibschrift wird häufig die Mittellinie sowie manchmal die Spitze oben als Schleife ausgeführt; in manchen Schreibweisen sieht der Großbuchstabe dem runden Kleinbuchstaben sehr ähnlich (z. B. Sütterlin).

Es gibt zwei Grundformen der Kleinbuchstaben: offen und geschlossen/rund. Beide haben sich über die Schreibschrift (mit Feder) aus dem Großbuchstaben entwickelt. Die offene Form ähnelt einem kleinen, um 180° gedrehten e mit einem Abschluss unten rechts. Die geschlossene Form, bei der die Mittellinie ganz fehlt, hat Ähnlichkeit mit einem kleinen o, dem auf der rechten Seite eine senkrechte Tangente angefügt wurde, bzw. dem lateinischen Alpha (ɑ). Ansonsten sind je nach Schriftart die für alle Buchstaben üblichen Schleifen und Serifen anzutreffen. In Standard-Druckschriften wird meist die offene, in kursiven und in Schreibschriften die geschlossene Minuskel verwendet.

In Unicode wird das große „A“ durch U+0041 und das kleine „a“ durch U+0061 dargestellt. Im ASCII ist das große „A“ der Code 65, das kleine „a“ der Code 97, daraus folgt im binär aufgebauten Dualsystem die Zeichenfolge 01000001 für das große „A“ und 01100001 für das kleine „a“. Im EBCDIC ist der Code für das große „A“ die 193 und für das kleine „a“ die 129. Die numerischen Darstellungen in HTML und XML sind „&#65;“ und „&#97;“ für den Groß- und den Kleinbuchstaben.

Neben diesen direkten Darstellungen gibt es noch diverse bildliche oder sonstige Darstellungen des Buchstabens „A“. Dazu gehört beispielsweise der Morsecode: ·– . In der Brailleschrift wird das „A“ durch eine Erhebung dargestellt. Weitere Darstellungsformen gibt es in der Gebärdensprache in Form der geschlossenen Faust im Fingeralphabet, in der optischen Telegrafie sowie im international gültigen Flaggenalphabet:

Der Buchstabe A steht in verschiedenen Sprachen für unterschiedliche Vokallaute.

Im Deutschen und vielen anderen Sprachen steht er für den offenen ungerundeten Zentral- oder Vorderzungenvokal []: A ist der klangreichste der Vokale, bei dessen Hervorbringen der Stimmton frei aus den weit geöffneten Lippen hervorkommt, während die Zunge in eine flache Stellung niedergedrückt wird. In Norddeutschland gibt es zwei verschiedene Phoneme des Vokals: den Kurzvokal [a], wie z. B. in "satt", und den weiter hinten im Rachenraum gebildeten (Ungerundeter offener Hinterzungenvokal) Langvokal [ɑː], wie z. B. in "Rat", der sogar gerundet sein kann. Üblicher jedoch im deutschen Sprachraum ist eine eher zentrale Aussprache für beide.

Die Länge des Vokals ist unterschiedlich gekennzeichnet.

Langer Vokal:


Kurzer Vokal:


Aus einem A kann sich unter bestimmten Bedingungen ein Umlaut „ä“ bilden.


Abgeleitete Zwielaute (Diphthonge) sind:





</doc>
<doc id="7884" url="https://de.wikipedia.org/wiki?curid=7884" title="Statik">
Statik

Statik (gr. "statikos" „zum Stillstand bringend“) steht für:




Statik ist der Name folgender Personen:

Siehe auch:


</doc>
<doc id="7893" url="https://de.wikipedia.org/wiki?curid=7893" title="Dichter">
Dichter

Dichter ist eine spezifisch deutsche Wortbildung für einen Verfasser von Dichtung im Sinne von sprachlicher und schriftstellerischer Kunst.

Der Dichter, der die gesamte Schaffensbreite von Lyrik über Kurzgeschichten und Erzählungen bis hin zum Schauspiel bzw. Theater beherrscht und damit Einfluss auf die Sprache und die Gesellschaft nimmt, wie es etwa in der klassischen Literatur um 1800 ausgeprägt war, ist eine Idealvorstellung, in der Wirklichkeit aber seltene Ausnahmeerscheinung.
Der Begriff fand im 18. und 19. Jahrhundert im Deutschen den Vorzug gegenüber dem des Poeten, der von da an für den belächelten Liebhaber von Versen stand, den „Kauz“, der keine Beachtung des modernen Marktes fand. Ihm gegenüber war der „Dichter“ der Autor hoher Literatur, der in Emphasen des Sturm und Drang, der Romantik und des Nationalismus des 19. und frühen 20. Jahrhunderts zum Seher, Genie und, im herausragenden Fall, geistigen Führer der Nation stilisiert wurde. Textproduzenten ohne diesen Anspruch waren lediglich „Schriftsteller“, die von ihrem Schreiben im Sinne einer (handwerklichen) Berufsausübung lebten, während der Dichter am Ende anerkannt, von der Würdigung leben würde, die ihm die Nation zukommen ließ. Die Einrichtung von Dichterpreisen bzw. Dichterlesungen der Preußischen, heute Deutschen Akademie für Sprache und Dichtung entspricht diesem Begriffsverständnis.

Im Laufe des 20. Jahrhunderts verlor der Begriff „Dichter“ an Rang gegenüber den Bezeichnungen „Autor“ und „Schriftsteller“. Dies resultierte aus einer Konzentration des literarischen Feldes auf publikumswirksame Sparten wie beispielsweise Romanliteratur, Kriminalliteratur, Bühnenliteratur und indirekt aus technischen Entwicklungen, die höhere und preiswerte Auflagen von Büchern (später auch die Verbreitung durch Rundfunk, Film und Fernsehen) ermöglichte – Formate, in denen die Persönlichkeit des Verfassers (im Unterschied zu klassischen Formaten des Vortrags und der Autorenlesung) in den Hintergrund trat. Weiter verwendet wurde der Begriff lediglich für Autoren von Gedichten und sprachlich anspruchsvollen Texten, die sich weitgehend außerhalb des (kommerziellen) Marktes bewegen; aber auch in dieser Verwendung ist er heute hauptsächlich bei historischen Autoren anzutreffen (etwa bei Rainer Maria Rilke, weniger schon bei Paul Celan und kaum noch bei Durs Grünbein).




</doc>
<doc id="7895" url="https://de.wikipedia.org/wiki?curid=7895" title="Gemeinde (Begriffsklärung)">
Gemeinde (Begriffsklärung)

Gemeinde steht für:

Die Gemeinde steht für:
Siehe auch:


</doc>
<doc id="7896" url="https://de.wikipedia.org/wiki?curid=7896" title="VCD">
VCD

VCD steht als Abkürzung für:



</doc>
<doc id="7897" url="https://de.wikipedia.org/wiki?curid=7897" title="Verkehrsclub Deutschland">
Verkehrsclub Deutschland

Der ökologische Verkehrsclub Deutschland e. V. (VCD) ist ein Verkehrsclub, der sich für eine Verkehrswende im Sinne einer sozial- und umweltverträglichen Mobilität aller Verkehrsteilnehmer einsetzt.

Im Juni 1986 wurde der Verkehrsclub Deutschland als Reaktion auf die Verkehrspolitik der damaligen von CDU/CSU und FDP gebildeten Bundesregierung und als ökologischer Kontrapunkt zu den Automobilclubs, wie dem ADAC, von Mitgliedern verschiedener Umweltverbände und -initiativen ins Leben gerufen. Die offizielle Gründung erfolgte am 19. Juli 1986. Der VCD versteht sich als verbraucherorientierter Umweltverband, der sich für nachhaltige Mobilität einsetzt und die Interessen aller ökologisch orientierten Verkehrsteilnehmer (neben Autofahrern also auch Fahrradfahrer, Fußgänger, Bahn- und ÖPNV-Nutzer) vertritt.

Da Zufußgehen und Fahrradfahren zu den umweltfreundlichsten Fortbewegungsarten überhaupt zählen, setzt sich der VCD auch für diese Mobilitätsvarianten ein. Während der Fahrrad-Masterplan sich an die Politik wendet, werden mit Aktionen und Kampagnen wie "Zu Fuß zur Schule" und "FahrRad!" gezielt vor allem Kinder und Jugendliche angesprochen. Neben dem Aspekt der Verkehrssicherheit – wer sich selbstständig im Straßenverkehr fortzubewegen weiß, kann auch dessen Gefahren besser einschätzen – spielt hier auch die Verkehrsverlagerung (weg vom automobilen "Elterntaxi") und somit der Umweltaspekt eine Rolle.

Den öffentlichen Personennahverkehr (ÖPNV) sieht der VCD besonders im Stadtverkehr als umweltfreundliche Alternative zum Auto. Auch hier legt der VCD Wert auf Kundenfreundlichkeit – also auf gute Anschlüsse, Bequemlichkeit, Pünktlichkeit, hohe Taktfrequenzen, guten Service und faire Preise. Die Kundenorientierung wird u. a. im Wettbewerb "Königliche Verhältnisse in Bus und Bahn" verglichen. Für den ÖPNV werden, analog zum Fernverkehr, rechtsverbindliche und bundesweit einheitliche Fahrgastrechte gefordert.

Bekannt wurde der VCD vor allem dadurch, dass er im Rahmen seiner breiten Kampagne für den „Halb-Preis-Pass“ nach dem Schweizer Vorbild Halbtax-Abo 1992 die Deutsche Bundesbahn und die Deutsche Reichsbahn dazu bewegen konnte, die BahnCard einzuführen. Zehn Jahre setzte er sich für den Erhalt der "BahnCard 50" ein. Die Bahnprivatisierung begleitet der VCD kritisch. Seit 2001 testet der VCD die Kundenfreundlichkeit der Deutschen Bahn regelmäßig aus Kundensicht. Service, Pünktlichkeit und Beratung zählen zu den Schwerpunkten der Kriterien beim VCD-Bahntest. Der VCD bewertet auch Neubau-, Ausbau- und Stilllegungspläne der Bahn aus der Perspektive der Bahnnutzer und Steuerzahler. Er streitet für seiner Ansicht nach notwendige und sinnvolle Projekte und gegen solche, die er für schädlich hält, wie z. B. Stuttgart 21. Außerdem setzt er sich für Lärmminderung und Lärmschutz an Bahnstrecken ein, insbesondere an den Hauptachsen des Güterverkehrs. Der VCD fordert zudem eine dauerhafte Erhaltung der Nachtreisezüge als ökologische und bequeme Möglichkeit des Reisens sowie einen Ausbau des grenzüberschreitenden Nachtzugnetzes innerhalb Europas, und lehnt Ausdünnungen des Streckennetzes sowie Streichung von Angeboten im Nachtreisezugverkehr ab.

Im Bereich Autoverkehr folgt der VCD dem Leitgedanken „so viel wie nötig, so wenig wie möglich“. Ein sinnvoller und nachhaltiger Umgang mit dem Auto wird propagiert. Der VCD fördert Car-Sharing, gibt Tipps zum spritsparenden Fahren und für das Reisen ohne Auto. Die ökologischen Kfz-Schutzbriefe und Kfz-Versicherungen der VCD Service GmbH bietet Fahrern mit umweltgerechten Autos entsprechende Vergünstigungen. Seit 2002 unterstützt der VCD das Bündnis „Kein Diesel ohne Filter“ für den serienmäßigen Einbau von Rußpartikelfiltern in Dieselfahrzeuge.
Im Güterverkehrsbereich setzt der VCD auf Verkehrsverlagerung vom Lkw auf Bahn und Schiff, auf umweltorientiertes Flottenmanagement und Lkw-Maut.

Seit 1989 bringt der VCD alljährlich seine sogenannte VCD Auto-Umweltliste auf den Markt, in welcher aktuelle serienmäßige Automodelle mit Verbrennungsmotor anhand ihrer Umweltverträglichkeit bewertet werden. Neben dem CO-Ausstoß zählen vor allem Lärmbelastung, Rußpartikel-, Stickoxid- und Benzolausstoß zu den Bewertungskriterien.
Allerdings sind in der aktuellen Liste (2013/2014) ausschließlich Fahrzeuge mit Verbrennungsmotor gelistet, Elektroautos werden vom VCD inklusive der Vorkette zur Bereitstellung der Antriebsenergie betrachtet, welche bei den Fahrzeugen mit Verbrennungsmotor unbeachtet bleibt und auf der Basis des NEFZ so speziell Hybridautos besondere Vorteile bei der Wertung verschafft.
2007 erschien erstmals zusätzlich eine Kaufberatung, die unter dem Titel „Welches Auto soll es sein?“ fünf verschiedenen Käufertypen Empfehlungen für jeweils fünf umweltschonende Automodelle unterbreitet.

Der VCD macht sich für ein Tempolimit von 120 km/h auf deutschen Autobahnen stark. Dabei spielen Sicherheitsaspekte (geringere Geschwindigkeit – geringere Unfallfolgen) ebenso eine Rolle wie der Klimaschutz: Durch Tempolimits würde der CO-Ausstoß der Pkw gesenkt. Darüber hinaus verbrauchten Motoren, deren Leistung nicht auf Geschwindigkeiten jenseits von 120 km/h ausgelegt sei, weniger Benzin.

Darüber hinaus setzt sich der VCD mit seiner Initiative „Tempo 30 für mehr Leben“ auch für eine Herabsetzung der Regelgeschwindigkeit in geschlossenen Ortschaften ein.
Er beteiligt sich auch in einem Bündnis mit anderen Umweltverbänden an einer Europäischen Bürgerinitiative mit dem Namen "30kmh – macht die Straßen lebenswert!"

Text: "Wir schlagen ein EU-weites reguläres Tempolimit von 30 km/h (20mph) für städtische Gebiete / Wohngebiete vor. Lokale Autoritäten können andere Tempolimits festsetzen, wenn sie nachweisen können, wie die Umwelt- und Sicherheitserfordernisse für die schwächsten Straßenverkehrs-Teilnehmerinnen erfüllt werden."
Diese EBI ist bei der europäischen Kommission registriert und konnte seit 13. November 2012 auch online unterzeichnet werden. Bis 20. April 2013 hatten 19.535 Europäer unterzeichnet. Die Sammlung läuft laut EU-Webseite noch bis zum 13. November 2013.

Der VCD protestierte gegen die 2009 eingeführte, von ihm als „Abwrackprämie“ bezeichnete Umweltprämie in der von der Bundesregierung beschlossenen Form und rief dazu auf, die Prämie für die Anschaffung von Fahrrädern und Fahrkarten zu beantragen.

Mit dem Ziel, das massive Wachstum und die Umweltfolgen (Lärm-, Schadstoffbelastung, Flächenverbrauch) des als extrem klimaschädlich geltenden Flugverkehrs einzudämmen, wirbt der VCD für eine ganze Reihe von Maßnahmen: Der VCD fordert, eine Kerosinsteuer einzuführen, da er es als ungerecht erachtet, dass die Bahn Mehrwert-, Öko- und Mineralölsteuern zahlen muss, während der Flugverkehr davon befreit ist. Die Einbeziehung des Flugverkehrs in den europäischen Emissionshandel und eine (nationale) Ticketabgabe sieht er als wichtige Schritte, um vor allem dem Boom der sogenannten Billigfluggesellschaften beizukommen. Hinzu kommt die Forderung nach einem bundesweiten Nachtflugverbot von 22 bis 6 Uhr.

Für mehr Sicherheit und Rücksicht auf den Straßen setzt sich der VCD mit seinen Kampagnen für generelle Tempo-30-Begrenzungen innerorts und zur nachhaltigen Mobilitätserziehung sowie mit dem verkehrspolitischen Konzept "Vision Zero", einem Masterplan für das Ziel null Verkehrstote, ein.

Nicht nur der demographische Wandel, auch die alltäglichen Gewohnheiten führen dazu, dass heutzutage so viele Senioren wie nie zuvor mobil sind. Dabei setzen sich viele ältere Menschen auch noch in hohem Alter hinters Steuer. In Zusammenarbeit mit seinem Arbeitskreis »Seniorenmobilität« will der VCD zeigen, dass man im Alter nicht unbedingt das eigene Auto braucht, um mobil zu bleiben, und dass gerade Mobilität jenseits des Autos dazu führt, dass ältere Menschen länger selbstständig mobil sein können. Er engagiert sich für sichere und gute Fuß- und Radwegeverbindungen und setzt sich zudem dafür ein, dass der öffentliche Personenverkehr ausgebaut und servicefreundlicher gestaltet sowie Barrierefreiheit bei der Planung von öffentlichen Räumen und Verkehrsmitteln großgeschrieben wird. Der VCD bietet älteren Menschen zudem Informationen und Beratung, damit der Umstieg vom Auto nicht als Verzicht begriffen wird. Von Juli 2012 bis August 2015 koordinierte der VCD das auf drei Jahre ausgelegte Verbundprojekt „Klimaverträglich mobil 60+“, das er gemeinsam mit der Bundesarbeitsgemeinschaft der Senioren-Organisationen und dem Deutschen Mieterbund durchführte.

Damit auch der Urlaub nachhaltigen Kriterien folgt, engagiert sich der VCD vor allem für umweltfreundliche Reiseketten (Bus, Bahn, Fahrrad) und autofreie Mobilität am Urlaubsort. Mit seinen Projekten "Mobil im Urlaub", "Reiselust" und "Nachhaltige Klassenfahrten" bietet der VCD Verbrauchern entsprechende Tipps und Handlungsleitfäden. In Kooperation mit den Umweltverbänden BUND, NABU und WWF sowie der Deutschen Bahn setzt sich der VCD im Rahmen von „Fahrtziel Natur“ für einen sanften Tourismus in Großschutzgebieten ein.

Von DMM, B.A.U.M. und VCD wurde 2007 der "CSR Mobilitätspreis" für umweltbewusste Geschäftsreisen ins Leben gerufen, um Unternehmen in ihren Bemühungen um Corporate Social Responsibility (CSR) zu unterstützen. Damit werden Unternehmen ausgezeichnet, die bei der Organisation ihrer Geschäftsreisen Umweltaspekte berücksichtigen und damit einen Beitrag zur nachhaltigen Entwicklung leisten. Außerdem soll sich eine Plattform entwickeln, die den Austausch zwischen Unternehmen und die Erstellung ökologischer Reisekonzepte vorantreibt.

Der Verein untergliedert sich in die zwölf Landesverbände Baden-Württemberg, Bayern, Hessen, Rheinland-Pfalz, Saarland, Nordrhein-Westfalen, Niedersachsen, Bremen, Brandenburg sowie den mangels Mitgliederdichte zusammengesetzten Verbänden "Elbe-Saale" (Sachsen, Sachsen-Anhalt und Thüringen), "Nord" (Hamburg und Schleswig-Holstein) und "Nordost" (Berlin und Mecklenburg-Vorpommern). Außerdem in – allerdings nicht flächendeckend – etwa 160 Regional-, Kreis- und Ortsverbände.

VCD-Mitglieder können hier aktiv mitarbeiten. Sie entsenden zudem Delegierte auf die jährlich stattfindende Bundesdelegiertenversammlung, die den Bundesvorstand wählt, die langfristigen Ziele festlegt und über die Finanzen des Verbandes wacht.

Der "Länderrat" mit Vertretern, die aus den Landesverbänden entsandt werden, berät und kontrolliert den Bundesvorstand.

Der Bundesvorstand des VCD setzt die politischen Schwerpunkte des Vereins, legt die verbandspolitischen Ziele fest und repräsentiert den VCD nach außen. Die Mitglieder des Bundesvorstands (laut Satzung mindestens fünf, höchstens sieben Personen) werden alle zwei Jahre von der Bundesdelegiertenversammlung gewählt. Der Bundesvorstand lässt sich von den Mitgliedern des von ihm berufenen "Wissenschaftlichen Beirats" beraten.

Die Umsetzung der Verbandsbeschlüsse sowie das operative Tagesgeschäft erfolgt in der Bundesgeschäftsstelle des VCD in Berlin, in der 44 hauptamtliche Mitarbeiter (Stand: 15. Juni 2009) tätig sind.

Der VCD ist Unterzeichner der Initiative Transparente Zivilgesellschaft.

Die VCD Service GmbH bietet ökologisch orientierten Verbrauchern speziell zugeschnittene Versicherungsangebote. Dazu zählen neben Altersvorsorgepolicen, Unfall-, Privat-, Reiserücktritt-, Rechtsschutz- und Reisekrankenversicherung vor allem die nach eigenen Angaben erste Kfz-Versicherung "Eco-Line", die sich an der ökologischen Qualität des Autos (vor allem Schadstoffausstoß und Schadstoffklasse) orientiert. Hinzu kommen Pkw-Schutzbriefe mit Öko-Bonus (für schadstoffarme Autos), Personen- und Fahrrad-Schutzbriefe sowie der Fußgänger-Rechtsschutz für nichtmotorisierte Verkehrsteilnehmer.

Die "fairkehr Verlags GmbH" ist eine Tochtergesellschaft der VCD Service GmbH. Neben diversen Fahrplankarten, Ausflugsführern und den VCD-Hauspublikationen (z. B. "Zügig durch Europa" oder VCD Auto-Umweltliste) gehört vor allem die Zeitschrift "fairkehr", das Mitgliedermagazin des VCD, zu ihrer Produktpalette.
Die "fairkehr" erscheint seit September 1987 regelmäßig und informiert schwerpunktmäßig über die Themen aktuelle Umwelt-/Verkehrspolitik, Mobilität und nachhaltiger Tourismus. Ein breiter Service- und Meinungsteil ergänzen das Angebot. Aktuell erscheinen sechs Ausgaben pro Jahr.

Von 2004 bis 2009 war der VCD Träger des vom Bundesministerium für Ernährung, Landwirtschaft und Verbraucherschutz (damals Bundesverbraucherministerium) initiierten und finanzierten Projekts "Schlichtungsstelle Mobilität". Die Schlichtungsstelle vermittelte in Streitfällen zwischen Kunden und Unternehmen des öffentlichen Fernverkehrs (Bahn, Bus, Flugzeug, Fähre – keine Pauschalreisen), wenn die Parteien im ersten Schritt keine Einigung erzielten.

Der VCD ist Mitglied in der Klima-Allianz Deutschland, einem Zusammenschluss aus verschiedenen Entwicklungsorganisationen, Vertretern der beiden großen christlichen Kirchen, Umweltverbänden und anderen Gruppen (z. B. attac oder dem Deutschen Alpenverein). Die Klima-Allianz versteht sich als breites, gesellschaftspolitisches Bündnis, das für mehr und entschlossenere Klimapolitik eintritt und entsprechende Denkanstöße liefern will. So setzt sie sich u. a. für den Ausbau regenerativer Energien sowie der Kraft-Wärme-Kopplung, für Tempolimits auf Autobahnen und eine Flugticketabgabe ein.

Der VCD Landesverband Baden-Württemberg ist Partner im Aktionsbündnis gegen das Projekt Stuttgart21. Mit dem Konzept Kopfbahnhof 21 hat das Aktionsbündnis ein Alternativkonzept zum Projekt der Deutschen Bahn vorgestellt. Der Stellvertretende Landesvorsitzende Klaus Arnoldi nahm an den Schlichtungsgesprächen um Stuttgart21 teil.

Die Allianz pro Schiene ist ein im Jahr 2000 gegründetes breites Bündnis aus der bahnnahen Wirtschaft (Bahn- und Bauindustrie, Eisenbahnverkehrsunternehmen, Banken und Versicherungen), Umwelt- und Fahrgastverbänden, Verkehrs- und Automobilclubs, Gewerkschaften und Bahnfreunden, das sich als „Gegengewicht zur Betonpolitik der mächtigen Straßenlobby“ sieht. Ziel ist es, den Anteil der Schiene im Personen- und Güterverkehr wieder deutlich zu erhöhen. Der VCD ist Gründungsmitglied, und sein Vorstandsvorsitzender wirkt im Vorstand der Allianz pro Schiene mit.

Partnerorganisationen des VCD arbeiten in der Schweiz (Verkehrs-Club der Schweiz – VCS) und in Österreich (Verkehrsclub Österreich – VCÖ). Zwischen diesen Verbänden bestehen Kontakte und Kooperationen.

Transport and Environment (T & E) ist die Dachorganisation von 36 nichtstaatlichen europäischen Organisationen aus dem nachhaltigen Verkehrsbereich (z. B. VCD). Sie besteht seit 1989, nimmt Einfluss auf EU-Entscheidungen und bündelt die nationalen Aktivitäten seiner Mitglieder auf europäischer Ebene.

Der VCD ist zudem Mitglied im Europäischen Fahrgastverband (European Passengers' Federation – EPF).




</doc>
<doc id="7898" url="https://de.wikipedia.org/wiki?curid=7898" title="Schwarzes Loch">
Schwarzes Loch

Ein Schwarzes Loch ist ein Objekt, das in seiner unmittelbaren Umgebung eine so starke Gravitation erzeugt, dass weder Materie noch Information (etwa Licht- oder Radiosignale) diese Umgebung verlassen kann. Nach der Allgemeinen Relativitätstheorie verformt eine ausreichend kompakte Masse die Raumzeit so stark, dass sich ein Schwarzes Loch bildet.

Der Begriff wurde 1967 durch John Archibald Wheeler etabliert (nicht aber erfunden). Er verweist auf den Umstand, dass sich im Außenraum von hinreichend kompakten Massen oder Energieanhäufungen ein durch den Ereignishorizont charakterisiertes Raumgebiet bildet, in das Materie nur hineinfallen, aber nicht wieder hinausgelangen kann "(Loch)," und das auch eine elektromagnetische Welle, wie etwa sichtbares Licht, niemals verlassen kann (daher "schwarz").

Schon 1783 spekulierte der britische Naturforscher John Michell über "Dunkle Sterne," deren Gravitation ausreicht, um Licht gefangen zu halten. In einem Brief, der von der Royal Society publiziert wurde, schrieb er:
Die Idee schwerer Sterne, von denen korpuskulares Licht nicht entkommen könne, wurde im Jahr 1796 auch von Pierre Simon Laplace in seiner "Exposition du Système du Monde" beschrieben. Er schuf dafür den Begriff „Dunkler Körper“ (corps obscur). Diese Ideen bewegten sich innerhalb der newtonschen Physik.

Nachdem Albert Einstein 1915 die Feldgleichungen der allgemeinen Relativitätstheorie aufgestellt hatte, gab der deutsche Astronom Karl Schwarzschild 1916 erstmals eine Metrik an, die Schwarzschild-Metrik, die dem Gravitationsfeld einer punktförmigen Masse entspricht. Die Schwarzschild-Lösung beschreibt Größe und Verhalten eines nichtrotierenden und nicht elektrisch geladenen statischen Schwarzen Lochs mit dem sogenannten "Ereignishorizont" bei formula_1 und einer "zentralen Singularität" bei formula_2. Dabei steht formula_3 für die Gravitationskonstante, formula_4 für die Masse des Schwarzen Lochs und formula_5 für die Lichtgeschwindigkeit.

Würde zum Beispiel die gesamte Masse der Sonne zu einer Kugel mit nur drei Kilometer Radius reduziert, dann könnte von deren Oberfläche kein Lichtstrahl nach außen gelangen. Die gesamte Masse unserer Erde (r ≈ 6378 km) würde sich erst bei einem Radius von unter einem Zentimeter in ein Schwarzes Loch verwandeln.

Mit den Kruskal-Szekeres-Koordinaten in den 1950er Jahren konnte mathematisch gezeigt werden, dass ein "externer" Beobachter, der einen "internen" Beobachter auf das Schwarze Loch zustürzen sieht, den Eindruck gewinnen muss, dass sich der "interne" Beobachter dem Ereignishorizont nur asymptotisch annähert, mit trotz regelmäßiger Aussendung immer langsamer eintreffenden Signalen. Dagegen überquert der interne Beobachter selbst den Ereignishorizont schnell, ohne etwas Besonderes zu verspüren, obwohl er von jetzt ab nicht mehr umkehren und mit dem externen Beobachter keine Signale mehr austauschen kann und sehr bald von der Singularität bei formula_2 verschlungen wird.

In den späten 1920er Jahren zeigte der indische Astrophysiker Subrahmanyan Chandrasekhar, dass für ein astrophysikalisches Objekt ohne Kernreaktionen eine gewisse Grenzmasse, die sogenannte Chandrasekhar-Grenze, existiert. Objekte oberhalb dieser Massengrenze kollabieren zu Neutronensternen oder zu Schwarzen Löchern, aber nicht wie erwartet zu Weißen Zwergen. Chandrasekhars Arbeiten führten zu einer Kontroverse mit dem Astronomen Arthur Eddington. Ersterer war der Überzeugung, dass Sterne oberhalb der Massengrenze zu Objekten kollabieren könnten, deren Gravitation elektromagnetische Strahlen einfangen könnte. Eddington erwartete aber, dass es einen Mechanismus gibt, der den Zusammenbruch verhindern würde. Robert Oppenheimer wies 1939 zusammen mit Robert Serber und George Michael Volkoff anhand von Modellrechnungen nach, dass beim Kollaps eines großen Sterns ein Schwarzes Loch entstehen würde.

Der Mathematiker Roy Kerr beschrieb 1963 mit der Kerr-Metrik eine Lösung für ein rotierendes Schwarzes Loch. Bis dahin wurden die Begriffe "schwarze Sterne" oder "gefrorene Sterne" verwendet – letzterer als Metapher dafür, dass nach der Theorie aufgrund der gravitativen Zeitdilatation von außen gesehen am Rand des Schwarzen Lochs die Zeit stillzustehen scheint.

Der Begriff „Schwarzes Loch“ ist erstmals 1964 nachgewiesen in einem Bericht der Wissenschaftsjournalistin Ann Ewing über ein Symposion der American Association for the Advancement of Science zu den verschiedenen Endstadien von Sternen. Die Autorin gab Hong-Yee Chiu als Organisator sowie Alastair Cameron, Charles Misner, Volker Weidemann und John Beverly Oke als Redner an, ohne den Urheber des Ausdrucks zu benennen. Etabliert wurde der Begriff 1967, nachdem John Archibald Wheeler bei einer Konferenz einen Ersatz für den langen Ausdruck „gravitationally completely collapsed object“ suchte und den Vorschlag eines unbekannt gebliebenen Zuhörers aufgriff.

Im Jahr 1971 folgte mit der Entdeckung von Cygnus X-1 der erste beobachtbare Kandidat für ein Schwarzes Loch. 1974 stellte Stephen Hawking die Theorie auf, dass Schwarze Löcher eine Strahlung abgeben, die Hawking-Strahlung. Nachdem Hawking bereits 1971 theoretisch gezeigt hatte, dass der Ereignishorizont niemals kleiner werden kann, veröffentlichten 2002 Abhay Ashtekar und Badri Krishnan eine Lösung für die Beschreibung wachsender Schwarzer Löcher, ohne dabei eine Näherung verwenden zu müssen, was bei den Feldgleichungen der allgemeinen Relativitätstheorie nur selten gelingt.

Allgemein hat die Masse eines Körpers immer Gravitationskräfte zur Folge. Wenn die Masse auf ein genügend kleines Volumen begrenzt ist "(siehe auch Roche-Grenze)," hält sich der Körper von allein zusammen: Die Gravitationskraft führt zu einer Kompression des Körpers. Normalerweise gibt es Gegenkräfte im Inneren, die eine weitere Kompression aufhalten, was zu einem Gleichgewicht zwischen Gravitation und den Gegenkräften führt. Bei den Gegenkräften kann es sich je nach Objektgröße um den Thermodynamischen Druck, um die Abstoßung zwischen den Atomen oder Nukleonen oder um den Fermi-Druck handeln. Die letzte stabile Massengrenze liegt bei etwa 1,5 bis 3,2 Sonnenmassen (Tolman-Oppenheimer-Volkoff-Grenze); bei Objekten, die leichter sind, kann der Entartungsdruck in der in entartetem Zustand vorliegenden Materie einem Gravitationskollaps erfolgreich entgegenwirken.

Wenn eine kritische Dichte überschritten wird, reichen die Gegenkräfte nicht mehr aus, um die Gravitation zu kompensieren. Ein Gravitationskollaps ist die Folge: Die Gravitationskraft steigt schneller an als die durch Abstoßung der Teilchen resultierenden Gegenkräfte. Dadurch beschleunigt sich der Prozess selbst. Die Masse fällt auf ein verschwindendes Volumen zusammen. Die immer weiter ansteigende Gravitation verzerrt lokal den Raum und den Ablauf der Zeit. Das bedeutet, dass – von einem äußeren Beobachter betrachtet – der Kollaps immer langsamer abläuft und sich das Volumen nie auf einen einzelnen Punkt zusammenzieht.

Schwarze Löcher können aus massereichen Sternen am Ende ihrer Sternentwicklung entstehen. Sterne der Hauptreihe oberhalb von ca. 40 Sonnenmassen enden über die Zwischenstufen Wolf-Rayet-Stern und Supernova als Schwarzes Loch. Sterne mit Massen zwischen ca. 8 und ca. 25 Sonnenmassen sowie alle massereichen Sterne mit hoher Metallizität enden als Neutronenstern. Liegt ihre Masse zwischen ca. 25 und ca. 40 Sonnenmassen, können Schwarze Löcher durch Rückfall des bei der unvollständigen Supernova abgesprengten Materials entstehen.

Da die Masse erhalten bleibt, wächst die Dichte des Körpers über alle Grenzen. Solche Körper krümmen die Raumzeit um sich herum so stark, dass man anschaulich von einem Loch im Gefüge des Raums sprechen könnte, man nennt sie jedoch exakter Singularität. Die Singularität wird von einem Raumzeitbereich umgeben, aus dem weder Materie noch Information nach außen gelangen kann. Die Grenze dieses Bereichs ist der sogenannte Ereignishorizont, die Entfernung des Ereignishorizontes von der Singularität ist der sogenannte Schwarzschildradius.

Der Ereignishorizont ist kein physisches Gebilde, er bezeichnet nur einen Ort oder genauer eine Grenzfläche. Ein Beobachter, der durch den Ereignishorizont hindurchfällt, würde daher selbst nichts davon bemerken. Relativistische Effekte (Allgemeine Relativitätstheorie) führen aber dazu, dass ein von einem zweiten, weit entfernten Beobachter betrachteter Körper aufgrund der Zeitdilatation unendlich lange braucht, um den Ereignishorizont zu erreichen, wobei er zunehmend in rotverschobenem Licht erscheint und lichtschwächer wird.

Das Gravitationsfeld im Außenraum kugelförmiger, nichtrotierender und elektrisch ungeladener Körper wird durch die Schwarzschild-Metrik beschrieben. Sie gilt nicht nur für Schwarze Löcher, sondern für alle Körper mit diesen Eigenschaften und stellt für Sterne oder Planeten aufgrund deren geringer Rotationsgeschwindigkeit meist eine gute Näherung dar. Die Größe des Schwarzschildradius beträgt für ein Schwarzes Loch von einer Sonnenmasse etwa 2,9 Kilometer, für ein Objekt von einer Erdmasse etwa 9 Millimeter.

Es ist ein weitverbreiteter Irrtum, dass das Gravitationsfeld eines Schwarzen Loches beziehungsweise die von ihm hervorgerufene Krümmung von Raum und Zeit bei üblichen Entfernungen von außerordentlich großer Stärke sei. Da sowohl Schwarze Löcher als auch Sterne von derselben Metrik beschrieben werden, würde sich am Gravitationsfeld im Sonnensystem nichts ändern, wenn man die Sonne durch ein Schwarzes Loch gleicher Masse ersetzte. Abgesehen vom Fehlen des Sonnenlichts wäre lediglich in unmittelbarer Umgebung des Schwarzen Loches (innerhalb etwa des vorherigen Kernradius der Sonne) ein enormer Zuwachs der Gravitationsbeschleunigung festzustellen.

Das rotierende Schwarze Loch ist eine allgemeinere Form dieses astrophysikalischen Phänomens. Als rotierende Schwarze Löcher werden solche bezeichnet, die einen Eigendrehimpuls besitzen. Wie alle Schwarzen Löcher verursachen auch sie, bedingt durch ihre enorme Gravitation, eine entsprechend große Veränderung der geometrischen Struktur von Raum und Zeit (siehe: Raumzeitkrümmung). Bei einem rotierenden Schwarzen Loch nimmt die Singularität jedoch eine Kreis- oder Ringform an und reißt die Raumzeit um sich herum mit anstatt sie nur zu krümmen: Der Raum wird in der Drehrichtung des Schwarzen Lochs mitgedreht. Diese Art der Raumzeitkrümmung erscheint nicht bei einem ruhenden Schwarzen Loch, sondern tritt bei rotierenden Schwarzen Löchern sozusagen zusätzlich außerhalb des Ereignishorizonts mit der Form eines an den Polen abgeplatteten Rotationsellipsoids auf. Alle Objekte um ein rotierendes Schwarzes Loch werden mitgedreht, eben weil sich auch die Raumzeit selbst mitdreht. Einem zu seiner Umgebung stillstehenden Beobachter käme es so vor, als würde sich das ganze Universum um ihn drehen. Dieser Effekt nimmt mit der Entfernung stark ab. Aber bis zu einem bestimmten Abstand (der sogenannten "statischen Grenze"), in einem Bereich, der Ergosphäre genannt wird, ist die Drehgeschwindigkeit so hoch, dass alle Objekte (und auch Energie wie Lichtstrahlen) wiederum schneller als Licht sein müssten, um die Drehgeschwindigkeit auszugleichen, also nicht mitzurotieren. Die Winkelgeschwindigkeit eines Teilchens am eigentlichen Ereignishorizont entspricht genau der Rotationsgeschwindigkeit des Schwarzen Loches und nimmt nach außen ab, die Bahngeschwindigkeit entspricht dabei aber immer der Lichtgeschwindigkeit. Das heißt nicht, dass seine Eigengeschwindigkeit größer als die Lichtgeschwindigkeit ist, sondern dass es innerhalb der Ergosphäre keine nicht mitrotierenden Teilchen geben kann. Dieses "Frame-Dragging" ist ein Extremfall des seit 1918 bekannten Lense-Thirring-Effekts. Eine Besonderheit dieses Bereichs ist, dass die kinetische Energie in diesem Bereich aus Sicht eines äußeren Beobachters negativ sein kann. Ein Teilchen, das sich in der Ergosphäre befindet, kann deshalb so in zwei Teilchen zerfallen, dass die kinetische Energie eines der beiden größer ist als die des ursprünglichen Teilchens. Das betreffende Teilchen kann die Ergosphäre verlassen, während sein Komplement mit negativer kinetischer Energie (ohne weitere Wechselwirkung) notwendig und in endlicher Eigenzeit den Ereignishorizont überschreitet. Die scheinbar aus dem Nichts generierte Energie wird der Rotationsenergie des Schwarzen Lochs entzogen. Dieser Mechanismus zur Energiegewinnung wurde zuerst von Roger Penrose vorgeschlagen.
Die Ausdehnung der Ergosphäre ist vom Polarwinkel (entspricht dem Komplementärwinkel der geographischen Breite auf der Erde) abhängig; sie ist null an den Polen des rotierenden Schwarzen Lochs – d. h., statische Grenze und Ereignishorizont fallen hier zusammen – und erreicht einen vom Drehimpuls des Schwarzen Lochs abhängigen Abstand – maximal den doppelten Schwarzschildradius – in der Äquatorregion. Der Drehimpuls eines Schwarzen Lochs ist dabei, wie unten beschrieben wird, begrenzt.

Einige Beobachtungen, beispielsweise von extrem schnellen Materiestrahlen "(Jets)," die das Gebiet außerhalb des Ereignishorizonts senkrecht zur Akkretionsscheibe verlassen, werden durch Effekte beschrieben, die nur innerhalb einer Ergosphäre oder bei Vorhandensein derselben auftreten können. Aus allgemeinen Überlegungen zur Drehimpulserhaltung kann man schließen, dass alle Schwarzen Löcher rotieren, zumindest zum Zeitpunkt ihrer Entstehung. Aber natürlich zeigen nur sehr schnell rotierende Schwarze Löcher starke Auswirkungen, der unter Frame-Dragging bekannten Phänomene. Andererseits "verdrillt" jede rotierende Masse, unabhängig vom Auftreten eines Ereignishorizonts, also auch der Planet Erde, die umgebende Raumzeit. Diese Effekte bei der Erde sollten durch Messungen zum Beispiel mit Hilfe der LAGEOS-Satelliten quantifiziert werden. Erste Ergebnisse aus dem Jahr 1997 lagen noch so dicht am Bereich der Messungenauigkeit, dass sie kontrovers diskutiert wurden, erst eine Wiederholung der Messung im Jahr 2004 mit dem Satelliten Gravity Probe B bestätigte den Sachverhalt.

Ein Schwarzes Loch lässt sich durch lediglich drei physikalische Kenngrößen vollständig beschreiben (sogenannte "Haarlosigkeit" Schwarzer Löcher): Masse, Drehimpuls und Elektrische Ladung. Die Multipolmomente entfallen. Es gibt also folgende Klassen:

Formell ergibt sich ein Schwarzes Loch aus einer speziellen Vakuumlösung der allgemeinen Relativitätstheorie, der sogenannten Schwarzschild-Lösung (nach Karl Schwarzschild, der diese Lösung als erster fand), bzw. für rotierende und elektrisch geladene Schwarze Löcher aus der Kerr-Newman-Lösung. Eine „Vakuumlösung“ ist eine Lösung der Vakuumfeldgleichungen – also etwa im Außenraum um einen Stern herum, wo sich näherungsweise keine Materie aufhält und damit der Energie-Impuls-Tensor verschwindet. Im Zentrum des Schwarzen Loches befindet sich eine physikalische Singularität; die Krümmung der Raumzeit wird an dieser Stelle unendlich groß und die Gleichungen der Relativitätstheorie versagen, weil für die Beschreibung dieses Ortes eine „Theory of Everything“ (TOE) notwendig wäre. Die ganze Masse des Schwarzen Loches ist in einem Punkt (bei rotierenden Schwarzen Löchern in einem Ring ohne Ausdehnung) konzentriert. Nach heutigem Stand des Wissens kann dies zustande kommen, weil die Gravitation in einem Schwarzen Loch so groß ist, dass keine der anderen drei Grundkräfte der Physik der Komprimierung entgegenwirken kann. Die gesamte Materie stürzt in sich zusammen und konzentriert sich in der Singularität. Aus diesem Grund ist die Dichte der Singularität unendlich groß.

Die Grenze, ab der keine Information mehr zu einem im Unendlichen befindlichen Beobachter gelangen kann, heißt Ereignishorizont, ihr Radius ist der Schwarzschildradius. Da ein nichtrotierendes Schwarzes Loch von außen gesehen kugelförmig ist, hat auch der Ereignishorizont die Form einer Kugeloberfläche. Schwarze Löcher können bei gegebener Masse nicht eine beliebig große Ladung und nicht einen beliebig großen Drehimpuls besitzen. Setzt man nämlich in die entsprechenden Lösungen der allgemeinen Relativitätstheorie eine zu hohe Ladung und/oder einen zu hohen Drehimpuls ein, so ergibt sich statt eines Schwarzen Loches eine sogenannte nackte Singularität: Es bildet sich zwar eine zentrale Singularität aus, jedoch ist diese nicht von einem Ereignishorizont umgeben: Man kann sich vorstellen, dass durch die Drehung der Raumzeit die einfallende Materie so stark beschleunigt würde (Zentrifugalkraft), dass sie die Gravitation wieder aufhebt. Im Ergebnis würde es somit keinen Ereignishorizont geben, da die Materie wieder entkommen könnte. Allerdings kann man zeigen, dass aus einem normalen Schwarzen Loch durch Zufuhr von Ladung oder Drehimpuls keine nackte Singularität entstehen kann, denn die gleichzeitig zugeführte Energie würde seine Masse ausreichend erhöhen, sodass also stets verhindert wird, dass aus dem gewöhnlichen Schwarzen Loch eines mit einer nackten Singularität entsteht (Roger Penrose nannte dies Kosmische Zensur).

Der Ereignishorizont wird bei Sternen, die zu nicht rotierenden Schwarzen Löchern kollabierten, von Lichtstrahlen begrenzt (der sogenannten Photonensphäre). Diese Lichtstrahlen sind die letzten, die noch nicht von der Gravitation des Schwarzen Loches angezogen wurden. Im Falle von rotierenden Schwarzen Löchern (siehe oben) gibt es nicht nur einen Radius, auf dem Lichtstrahlen die Singularität umkreisen können, sondern unendlich viele innerhalb der Ergosphäre. Nahe der Singularität, also deutlich innerhalb des Schwarzschildradius, ist die Verzerrung der Raumzeit so stark, dass für ein hineinfallendes Objekt auch der Empfang von Nachrichten sich auf einen schrumpfenden Horizont beschränkt. Dieses nur theoretisch zugängliche Phänomen wird asymptotisches Schweigen genannt.

Für Schwarze Löcher folgen aus der allgemeinen Relativitätstheorie Gesetze, die auffallend jenen der Thermodynamik gleichen. Schwarze Löcher verhalten sich ähnlich wie ein Schwarzer Strahler, sie haben also eine Temperatur. Es gelten im Einzelnen die folgenden Gesetze:

Quantentheoretische Überlegungen zeigen, dass jedes Schwarze Loch auch Strahlung abgibt. Dies scheint im Widerspruch zu der Aussage zu stehen, dass nichts das Schwarze Loch verlassen kann. Jedoch lässt sich der Vorgang als Produktion von Teilchen/Antiteilchen-Paaren nahe am Schwarzschildradius deuten, bei dem eines der Teilchen ins Zentrum des Schwarzen Lochs fällt, während das andere in die Umgebung entkommt. Auf diese Weise kann ein Schwarzes Loch Teilchen abgeben, ohne dass etwas den Ereignishorizont von innen nach außen überschreitet. Die Energie für diesen "Hawking-Strahlung" genannten Prozess stammt aus dem Gravitationspotential des Schwarzen Lochs. Das heißt, es verliert durch die Strahlung an Masse.

Von außen betrachtet sieht es also so aus, als würde das Schwarze Loch „verdampfen“ und somit langsam kleiner werden. Den Teilchen der Hawking-Strahlung kann eine Wellenlänge und damit auch eine Temperatur zugeordnet werden. Diese Temperatur ist umgekehrt proportional zu der Masse des Schwarzen Lochs. Dies bedeutet für sehr kleine Schwarze Löcher, dass sie sehr heiß sein und dementsprechend stark strahlen müssten. Wenn es beim Urknall sehr kleine Schwarze Löcher gab, dann wären sie daher in der Zwischenzeit vollständig verdampft. Die dabei entstehende Strahlung wäre sehr charakteristisch und könnte als Nachweis solcher Löcher dienen. Diese Strahlung wurde jedoch bisher nicht gefunden. Daraus ergibt sich eine Obergrenze für die Anzahl der beim Urknall entstandenen kleinen Schwarzen Löcher.

Aus Sternen der Hauptreihe entstandene Schwarze Löcher sind dagegen so kalt, dass sie nur sehr wenig Hawking-Strahlung abgeben. Wenn man alleine die Hawking-Strahlung betrachtet, ergibt sich als Zeit bis zum vollständigen Verdampfen ein Wert, der das Alter des Universums um dutzende Größenordnungen übersteigt. Die Temperatur eines aus einem Stern entstandenen Schwarzen Lochs ist deutlich niedriger als die Temperatur der Hintergrundstrahlung. Das bedeutet, dass das Schwarze Loch mehr Energie und damit Masse aus der Wärmestrahlung des Universums aufnimmt, als es durch Hawking-Strahlung abgibt.

Hawking erkannte 1974 nach Vorarbeiten des israelischen Physikers Jacob Bekenstein, dass Schwarze Löcher eine formale Entropie und somit auch eine formale absolute Temperatur "T" haben. Die formale Entropie "S" eines Schwarzen Lochs ist proportional zur Oberfläche "A" seines Horizonts und sonst nur von Naturkonstanten abhängig; die formale Temperatur ist umgekehrt proportional zur Masse:

Dabei ist formula_20 das reduzierte Plancksche Wirkungsquantum, "c" die Lichtgeschwindigkeit, formula_21 die Kreiszahl Pi, formula_22 die Boltzmannkonstante, "G" die Gravitationskonstante, "M" die Masse und formula_23 der Schwarzschildradius.

Da ein Schwarzes Loch stetig Energie in Form von Hawking-Strahlung verliert, wird es nach einer bestimmten Zeitspanne formula_24 vollständig zerstrahlt sein, sofern es während dieser Zeitspanne keine neue Masse aufnehmen kann. Diese Zeitspanne berechnet sich durch
wobei formula_4 die Masse des Schwarzen Loches zu Beginn der Zeitspanne und formula_27 eine Konstante ist.

Ein Eindeutigkeits-Theorem von Werner Israel besagt, dass ein Schwarzes Loch vollständig durch Masse (siehe Schwarzschild-Metrik), elektrische Ladung (siehe Reissner-Nordström-Metrik) und Drehimpuls (siehe Kerr-Metrik) charakterisiert ist. Das veranlasste John Archibald Wheeler zur Aussage „Schwarze Löcher haben keine Haare“. Man spricht deshalb auch vom No-Hair-Theorem, oder auch Keine-Haare-Theorem bzw. Glatzensatz. Weitere Informationen aus dem Inneren seien nicht zu erhalten, auch nicht durch die Hawking-Strahlung, da sie rein thermisch sei.

Das No-Hair-Theorem legt nahe, dass Schwarze Löcher einen Verlust an Information bewirken, da die bei der Auflösung entstehende Hawking-Strahlung keine Information über die Entstehungsgeschichte des Schwarzen Lochs enthält. Anders als bei allen sonstigen durch die Quantenmechanik oder die Relativitätstheorie beschriebenen Vorgängen ist es prinzipiell nicht möglich, dass die Entstehung, das Wachstum und die Auflösung eines Schwarzen Loches in umgekehrter Reihenfolge passiert. Das bedeutet, dass diese Vorgänge durch einen eindeutigen Zeitpfeil ausgezeichnet sind. Diese Verletzung der Unitarität der Zeitentwicklung wird auch als Informationsparadoxon Schwarzer Löcher bezeichnet.

Prominente Vertreter dieser Sicht waren Kip Thorne und lange Zeit auch Stephen Hawking. Stephen Hawking änderte jedoch seine Meinung und erklärte auf der 17. International Conference on General Relativity and Gravitation (18.–23. Juli 2004 in Dublin), dass Schwarze Löcher doch Haare haben könnten. Weiterhin nehmen unter anderem Roger Penrose, John Preskill und Juan Maldacena an, dass zumindest gewisse Informationen zusätzlich nach außen dringen könnten. Auch in seinem Buch "Das Universum in der Nussschale" äußert Stephen Hawking die Annahme, dass Schwarze Löcher bei ihrem Ableben die gesammelte Information wieder abgäben. Das Informationsparadoxon ist von Joseph Polchinski im Feuerwall-Paradoxon verschärft worden. 2013 schlugen Juan Maldacena und Leonard Susskind eine Lösung durch die Äquivalenz von Quantenverschränkung und Wurmlöchern vor (ER-EPR-Vermutung), weiter ausgebaut durch einen expliziten Vorschlag solcher "durchquerbarer" Wurmlöcher durch Ping Gao, Daniel Louis Jafferis und Aron C. Wall (siehe Wurmloch).

Ein neuerer Ansatz schlägt vor, das No-Hair-Theorem anhand der Präzession der Bahnellipsen zweier eng um Sagittarius A* umlaufender Sterne zu testen. Wenn das No-Hair-Theorem zutrifft, dann sollte das Verhältnis der beiden Präzessionsraten nur vom Drehimpuls des vermuteten Schwarzen Lochs Sagittarius A* abhängen. Sollte sich herausstellen, dass das Verhältnis der Präzessionsraten komplizierteren Beziehungen gehorcht, so wäre das No-Hair-Theorem widerlegt.

Zwei Wege der Entstehung binärer Schwarzer Löcher werden unterschieden. Zum einen kann sie herrühren aus zwei stark wechselwirkenden Galaxien, wenn diese kollidiert sind und offenbar Swing-by-Vorgänge eine Rolle spielen. Als Beispiel einer vorausgegangenen Kollision wird vermutet, dass das supermassereiche Schwarze Loch im Zentrum von M87 durch Verschmelzung entstanden ist.

Zum anderen kann ein wechselwirkender Doppelstern der Ausgangspunkt sein, wenn beide Sterne sehr massereich sind. Nach einem Wind Roche-Lobe Overflow entsteht normalerweise ein Schwarzes Loch plus ein weißer Zwerg. Alternativ kann der Overflow aber untypisch verlaufen und zwischenzeitlich eine gemeinsame Hülle entstehen, sodass sich letztlich zwei Schwarze Löcher bilden.

Wenn ein Schwarzes-Loch-Paar entstanden ist, kann es nach einer Phase des Umkreisens zu einem einzigen Schwarzen Loch verschmelzen. Im 300 Millionen Lichtjahre entfernten Galaxienhaufen Abell 400 hat man Hinweise auf die bevorstehende Verschmelzung zweier Schwarzer Löcher gefunden. 2015 wurde erstmals eine solche Kollision nachgewiesen, als vorhersagegemäß im letzten Sekundenbruchteil vor der Verschmelzung das Ausmaß der Beschleunigung bei gleichzeitiger Abgabe von Materie bzw. Energie derartig groß war, dass die so erzeugte Gravitationswelle in den LIGO-Observatorien gemessen werden konnte.

Schwarze Löcher werden nach der Entstehungsweise und aufgrund ihrer Masse in nebenstehend gezeigte Klassen verteilt, auf die im Folgenden eingegangen wird:

Supermassereiche (auch "supermassiv" genannte) Schwarze Löcher können die millionen- bis milliardenfache Sonnenmasse haben. Sie befinden sie sich in den Zentren von hellen elliptischen Galaxien und im Bulge der meisten, oder sogar aller Spiralgalaxien. Wie sie entstanden sind und wie ihre Entstehung mit der Entwicklung der Galaxien zusammenhängt, ist Gegenstand aktueller Forschung.

So ist die starke Radioquelle Sagittarius A* (kurz Sgr A*) im Zentrum der Milchstraße ein supermassives Schwarzes Loch von 4,3 Millionen Sonnenmassen. Vor wenigen Jahren lag die Massenabschätzung, die auf der Beobachtung von Gaswolken (z. B. der sogenannten Mini-Spirale) fußte, noch bei etwa 2,7 Mio. Sonnenmassen. Dank verbesserter Auflösung und Empfindlichkeit der Teleskope konnte die Masse für das Schwarze Loch im Zentrum der Galaxis genauer angegeben werden, indem die Bahnkurven beispielsweise von S0-102 oder S0-2 analysiert wurden.

Natarajan und Treister haben ein Modell entwickelt, das eine obere Massengrenze in der Größenordnung von 10 Milliarden Sonnenmassen vorhersagt. Die Begründung liegt – anschaulich erklärt – darin, dass die hineinstürzende Materie durch die Gravitationskraft eines solchen supermassiven Schwarzen Lochs derart beschleunigt wird, dass sich ein stabiler Orbit außerhalb des Schwarzschild-Radius ergibt. Zusätzlich wirken auch die elektromagnetische Strahlung und die „Materiewinde“, die von der Materie in der Akkretionsscheibe ausgestrahlt werden, als Widerstand gegen weiter einfallende Materie, sodass sich letztlich ein Gleichgewicht zwischen einfallender und abgestoßener Materie einstellt.

2008 hat ein schweizerisches Team der Eidgenössischen Technischen Hochschule Lausanne (EPFL) um Alexander Eigenbrod ein energiereiches Ringgebilde um einen 10 Milliarden Lichtjahre entfernten Quasar, das Einsteinkreuz im Sternbild Pegasus, am VLT beobachtet und damit die Theorie der supermassereichen Löcher sehr gut bestätigt.

Im Zentrum der Galaxie M87 wurde ein Schwarzes Loch mit einer Masse von 6,6 Milliarden Sonnenmassen nachgewiesen. Aktuelle Rekorde stellen ein Schwarzes Loch von 18 Milliarden Sonnenmassen dar, das im Quasar OJ 287 entdeckt wurde (2008), und eines von geschätzten 21 Milliarden Sonnenmassen im Zentrum der Galaxie NGC 4889 (2011). Im November 2012 wurde in der Galaxie NGC 1277 ein Schwarzes Loch entdeckt, das mit rund 17 Milliarden Sonnenmassen ca. 14 % der gesamten Masse der Galaxie umfasst. Mit einem supermassiven Schwarzen Loch von etwa 20 Milliarden Sonnenmassen gehört der Quasar APM 08279+5255 (ca. 12 Milliarden Lichtjahre entfernt), um den 2011 enorme Mengen an Wasserdampf entdeckt wurden, ebenfalls zu den massereichsten bisher bekannten Kandidaten.

Supermassive Schwarze Löcher wurden auch in (ultrakompakten) Zwerggalaxien gefunden (zuerst 2014 in M60-UCD 1), was darauf hinweist, dass diese als „normale“ Galaxien entstanden, denen durch Kollisionen mit größeren Galaxien ein Großteil der Sterne entrissen wurde.

Im September 2017 wurde die Entdeckung eines doppelten supermassiven Schwarzen Loches veröffentlicht, die mit Hilfe der Very Long Baseline Interferometry (VLBI) beobachtet werden konnten. Hierbei handelt es sich um zwei einander im Abstand von 1,1 Lichtjahren umkreisende Schwarze Löcher mit einer Gesamtmasse von 36 Millionen Sonnenmassen in der 380 Millionen Lichtjahre entfernten Spiralgalaxie NGC 7674.

Mittelschwere Schwarze Löcher von einigen hundert bis wenigen tausend Sonnenmassen entstehen möglicherweise infolge von Sternenkollisionen und -verschmelzungen. Anfang 2004 veröffentlichten Forscher Ergebnisse einer Untersuchung von Nachbargalaxien mit dem Weltraumteleskop Chandra, in der sie Hinweise auf Mittelschwere Schwarze Löcher in sogenannten ultrahellen Röntgenquellen (ULX) fanden. Danach gab es allerdings aufgrund von Beobachtungen mit dem VLT und dem Subaru-Teleskop starke Zweifel daran, dass ULX mittelschwere Schwarze Löcher sind.

Neue Kandidaten sind die Zentren der Kugelsternhaufen Omega Centauri in der Milchstraße und Mayall II in der Andromeda-Galaxie, sowie in der Spiralgalaxie Messier 82 und in einer Zwerg-Seyfert-Galaxie.

Stellare Schwarze Löcher stellen den Endzustand der Entwicklung massereicher Sterne dar. Sterne, deren Anfangsmasse kleiner als drei Sonnenmassen ist, können nicht zu einem Schwarzen Loch werden. Sie beenden ihr Leben als vergleichsweise unspektakulär auskühlender Sternenrest (Weißer Zwerg/Neutronenstern). Sterne, deren Anfangsmasse drei Sonnenmassen übersteigt (etwa Blaue Riesen), durchlaufen am Ende ihres Lebens die höheren Stufen der Nukleosynthese bis zum Siliciumbrennen. Sie explodieren in einer Kernkollaps-Supernova, wobei der übrigbleibende Sternenrest zu einem Schwarzen Loch kollabiert, sofern er noch mehr als 2,5 Sonnenmassen besitzt (Tolman-Oppenheimer-Volkoff-Grenze). Ansonsten können Sterne bis zur 15-fachen Sonnenmasse – abhängig davon, wie viel Masse sie als Supernova verlieren – auch als Neutronenstern enden, wenn die verbleibende Masse zwischen 1,5 und 2,5 Sonnenmassen liegt. Neutronensterne können sich – beispielsweise als kompakter Begleiter in einem Röntgendoppelstern – durch die Akkretion weiterer Materie auch im Nachhinein noch zu Schwarzen Löchern entwickeln.

Durch die Beobachtung von Gravitationswellen konnte im September 2015 die Verschmelzung zweier stellarer Schwarzer Löcher mit etwa 36 und 29 Sonnenmassen beobachtet werden. Das resultierende Schwarze Loch hat eine Masse von etwa 62 Sonnenmassen (die Energie von 3 Sonnenmassen wurde als Gravitationswellen abgestrahlt). Dies ist das massereichste bekannte stellare Schwarze Loch (Stand: März 2016).

Ein weiteres sehr massereiches Schwarzes Loch in der Zwerggalaxie IC 10 im Sternbild Kassiopeia hat eine Masse von 24 bis 33 Sonnenmassen. Es ist Teil eines Doppelsternsystems. Das Schwarze Loch wurde indirekt durch die in ihrer Stärke schwankende Röntgenstrahlung des begleitenden Sterns entdeckt, was ein Hinweis auf ein periodisch die Quelle verdeckendes Objekt sein kann. Berechnungen aus Daten des Satelliten "Swift" sowie des Gemini-Teleskops auf Hawaiʻi bestätigten die Vermutungen.

Rekordhalter mit der aktuell geringsten Masse ist XTE J1650-500, ebenfalls ein Röntgendoppelstern mit vielleicht nur ca. 3,8 Sonnenmassen. Als Kandidat für das kleinste Schwarze Loch wird zurzeit IGR J17091-3624 untersucht. Es handelt sich um ein Doppelsternsystem aus einem normalen Stern und einem Schwarzen Loch, das anhand der Veränderungen seines Röntgensignals auf weniger als drei Sonnenmassen geschätzt wird.

Anfang der 1970er Jahre stellte Stephen W. Hawking als Erster die Vermutung auf, neben den durch Supernovae entstandenen Schwarzen Löchern könnte es auch sogenannte primordiale Schwarze Löcher geben. Das sind Schwarze Löcher, die sich bereits beim Urknall in Raumbereichen gebildet haben, in denen die lokale Massen- und Energiedichte genügend hoch war (rechnet man die ständig abnehmende Materiedichte im Universum zurück, so findet man, dass sie in der ersten Tausendstelsekunde nach dem Urknall die Dichte des Atomkerns überstieg). Auch der Einfluss von Schwankungen der gleichmäßigen Dichteverteilung (siehe hierzu kosmische Hintergrundstrahlung) im frühen Universum war für die Bildung von primordialen Schwarzen Löchern ausschlaggebend, ebenso die beschleunigte Expansion während der Inflationsphase nach dem Urknall. Damals könnten sich kleine Schwarze Löcher mit einer Masse von etwa 10 Kilogramm gebildet haben. Seit Mitte der 1990er Jahre wird diskutiert, ob die kürzesten auf der Erde gemessenen Gammastrahlungsausbrüche von verstrahlenden primordialen Schwarzen Löchern stammen könnten, denn deren berechnete Lebensdauer liegt in der Größenordnung des Alters des heutigen Universums.

Aus seinen Überlegungen über kleine Schwarze Löcher folgerte Hawking im Jahre 1974 die Existenz der nach ihm benannten Hawking-Strahlung, dass also Schwarze Löcher Materie nicht nur schlucken, sondern auch wieder freisetzen können. Obwohl die Existenz von primordialen Schwarzen Löchern keineswegs gesichert ist, haben sich also allein aus hypothetischen Betrachtungen wertvolle neue Erkenntnisse im Bereich der Kosmologie, der Quantenphysik und der Relativitätstheorie ergeben.

Nach einigen vereinheitlichten Theorien, wie der Stringtheorie, sollte die Mindestmasse für Schwarze Löcher weit unterhalb der Planck-Masse liegen, sodass Schwarze Mikro-Löcher beim Betrieb zukünftiger Teilchenbeschleuniger entstehen könnten. In der Tat wurde aus diesem Grund seit 2008 gegen den Betrieb des LHC-Beschleunigers opponiert und sogar geklagt. Die Klage wurde 2012 letztinstanzlich abgelehnt. Gegen die Befürchtung, ein solches Mikroloch könnte in den Erdkern fallen, dort wachsen und sich schließlich die ganze Erde einverleiben, spricht nicht nur die wahrscheinlich extrem geringe Lebensdauer der hypothetischen Mikrolöcher, sondern auch, dass der Erde unter der noch viel energiereicheren kosmischen Strahlung seit Milliarden Jahren nichts passiert ist.

Eine direkte Beobachtung von Schwarzen Löchern gilt als praktisch unmöglich. Aktuellen Theorien zufolge sind Schwarze Löcher in der Lage, Energie in Form von sogenannter Hawking-Strahlung abzugeben. Sollte dies zutreffen, würde das bedeuten, dass Schwarze Löcher allmählich „verdampfen“, wobei dieser Prozess umso schneller verläuft, je kleiner die Masse des Schwarzen Loches ist. Doch die Hawking-Strahlung wäre so energiearm, dass sie vom üblichen Hintergrund nicht zu unterscheiden wäre. Außerdem ist das Schwarze Loch selbst mit nur wenigen Kilometern Durchmesser im Verhältnis zu den kosmischen Entfernungen für eine Beobachtung viel zu klein.

Beobachtet werden dagegen die Auswirkungen auf Materie außerhalb des Ereignishorizonts.

Insbesondere von Bedeutung für die Entdeckung von Schwarzen Löchern sind die Folgen des Hineinfallens der Materie. Da der Ereignishorizont ein für kosmische Verhältnisse sehr kleines Gebiet umschließt, erreicht die einfallende Materie auch schon in einem Bereich vor dem Ereignishorizont eine sehr hohe Verdichtung und Beschleunigung durch die Gravitationskräfte. Bei rotierenden Schwarzen Löchern geschieht dies in Form einer Akkretionsscheibe. Dabei reibt die Materie aneinander und gibt große Mengen Energie frei, sowohl als elektromagnetische Strahlung als auch als Beschleunigung von Teilchen durch elektromagnetische Felder und Stoßvorgänge. Ein Resultat dieser Vorgänge sind "Materiestrahlen," die senkrecht zur Akkretionsscheibe entlang einer Achse durch das Schwarze Loch ausgestoßen werden. Besonders auffällig sind diese Jets bei supermassiven Schwarzen Löchern: Dort strömen die geladenen Teilchen unter so großen Beschleunigungen ins intergalaktische Medium, dass sie weit über ihre Ursprungsgalaxie hinausreichen. Außerdem erzeugen beschleunigte geladene Teilchen Synchrotronstrahlung, was bei solchen Jets zu starken Gammastrahlenemissionen führt. Beobachtet wurde dies z. B. Ende 2007 bei dem Schwarzen Loch im Zentrum der Galaxie 3C 321. Ein weiteres bekanntes Beispiel ist die Galaxie M 87 mit dem eindrucksvollen Jet ihres zentralen Schwarzen Lochs.

Historisch unterteilt man viele Arten von aktiven Galaxienkernen, je nach unserem Blickwinkel auf das Objekt, die Energieskalen der Prozesse und die Aktivität (wie viel Materie gerade in das Objekt strömt). Ein Beispiel sind die Quasare.


Sagittarius A* ist ein supermassereiches Schwarzes Loch im Zentrum der Milchstraße. Seit 1992 wird seine Umgebung vor allem im infraroten Bereich von einem Team von Astronomen untersucht. Dabei wurden die Umlaufbahnen und die Geschwindigkeiten von 28 Sternen vermessen. Eingesetzt wurden Nah-Infrarot-Kameras mit adaptiver Optik beim Very Large Telescope in Cerro Paranal in Chile, der bildgebende Spektrograph Sinfoni, die Speckle-Abbildungskamera SHARP I und andere Instrumente der europäischen Südsternwarte. Außerdem wurden Beobachtungen des Keck-Teleskops auf Hawaiʻi, des New Technology Teleskops sowie Aufnahmen des Hubble-Teleskops ausgewertet.

Die Untersuchungen zeigten, dass die zentrale Masse nur durch ein Schwarzes Loch erklärt werden kann und dass circa 95 % der gesamten Masse im beobachteten Sektor sich in diesem Schwarzen Loch befinden muss. Die Vermessung der Infrarot- und Röntgenemission in der Akkretionszone deutet darauf hin, dass das Schwarze Loch einen hohen Drehimpuls aufweist.

Neben dem vermuteten zentralen Schwarzen Loch in unserer Galaxie, nämlich Sagittarius A* mit ca. 4,3 Millionen Sonnenmassen, gibt es eine Reihe weiterer vermuteter kleiner Schwarzer Löcher, die in der Milchstraße verteilt sind und eine Masse von einigen wenigen bis einem Dutzend Sonnenmassen aufweisen. Sie alle sind Bestandteile von Doppel- oder Mehrfachsternsystemen, ziehen von ihrem Partner scheinbar in einer Akkretionsscheibe Materie ab und strahlen im Röntgenbereich.

Neueste Forschungsergebnisse zeigen, dass sich in der Sternengruppe "IRS 13," die nur drei Lichtjahre von Sgr A* entfernt liegt, ein zweites Schwarzes Loch mit vergleichsweise geringen 1300 Sonnenmassen befindet. Es ist derzeit nicht geklärt, ob es sich in Zukunft mit Sgr A* vereinigen wird, ob es sich auf einer stabilen Umlaufbahn befindet oder sich sogar von ihm entfernt.

Im Januar 2005 wurden mit dem Röntgen-Teleskop Chandra Helligkeitsausbrüche in der Nähe von Sgr A* beobachtet, die darauf schließen lassen, dass sich im Umkreis von etwa 70 Lichtjahren 10.000 bis 20.000 kleinere Schwarze Löcher befinden, die das supermassereiche zentrale Schwarze Loch in Sgr A* umkreisen. Einer Theorie zufolge sollen diese das zentrale Schwarze Loch in regelmäßigen Abständen mit Sternen aus der Umgebung „füttern“.

In der Galaxie NGC 6240 befinden sich zwei Schwarze Löcher, die einander im Abstand von 3000 Lichtjahren umkreisen und in einigen hundert Millionen Jahren verschmelzen werden.

Das derzeit im beobachtbaren Universum am weitesten entfernte stellare Schwarze Loch wurde von Astronomen der Europäischen Südsternwarte (ESO) mit Hilfe des Very Large Telescope am Paranal-Observatorium in der Galaxie NGC 300 aufgespürt.

Das erste Schwarze Loch außerhalb unserer Galaxie wurde 1982 in der etwa 150.000 Lichtjahre entfernten Großen Magellanschen Wolke nachgewiesen und bildet eine Komponente des Röntgendoppelsterns "LMC X-3."

Im Zentrum von NGC 4889 befindet sich (Stand Dezember 2011) das größte bisher direkt gemessene Schwarze Loch, mit einer Masse von geschätzten 21 Milliarden Sonnenmassen („best fit“ aus dem Bereich 6 bis 37 Milliarden Sonnenmassen).

Das Schwarze Loch mit der Katalognummer SDSS J0100+2802 ist sehr alt, von der Erde aus wird der Zustand 875 Millionen Jahre nach dem Urknall beobachtet. Seine Masse betrug zu diesem Zeitpunkt bereits rund zwölf Milliarden Sonnenmassen. Es ist unklar, wie es so früh so massereich werden konnte.

Es wurden einige alternative Erklärungen für ultrakompakte dunkle Objekte vorgeschlagen, die ohne Singularitäten auskommen und kein Informationsparadoxon aufweisen. Da diese Modelle keine mit heutigen Mitteln beobachtbaren Vorhersagen machen, durch die sie sich von einem Schwarzen Loch unterscheiden ließen, ist die Akzeptanz in der Fachliteratur gering. Ein Beispiel sind die hypothetischen Gravasterne, auch „Quasi Black Hole Objects“ (QBHO) genannt. Die Erfinder der Theorie, Mazur und Mottola, haben vorgeschlagen, dass die Theorie eine Lösung des Informationsparadoxons Schwarzer Löcher darstellt und dass Gravasterne Quellen für Gammablitze sein könnten. Die Theorie erreichte in der Öffentlichkeit nur wenig Interesse, da die Theorie keinen Vorteil gegenüber der Theorie der Schwarzen Löcher hat und rein spekulativ ist. Ein weiterer Versuch, auf der Stringtheorie aufbauend das Informationsparadoxon zu lösen, stammt von Samir Mathur. Nach diesem „Fusselknäuel-Modell“ verhüllt der Ereignishorizont ein Konglomerat aus Branen und Strings, ist selbst nicht scharf abgegrenzt.

Schwarze Löcher werden in der Science-Fiction-Literatur oft als mögliches Mittel zum überlichtschnellen Transport, so etwa in Stanisław Lems Roman "Fiasko," bzw. als ultimative Möglichkeit der Energiegewinnung dargestellt, wie bspw. in der Fernsehserie "Stargate."

Der Film „Das schwarze Loch“ von 1979 – mit Maximilian Schell und Anthony Perkins in den Hauptrollen –, der unter anderem die starke Gravitationskraft Schwarzer Löcher thematisiert, wurde 1980 für zwei Oscars nominiert.
Der Film Interstellar aus dem Jahr 2014 von Regisseur Christopher Nolan beinhaltet ebenfalls die Thematiken vom Schwarzen Loch und den Gravitationskräften.
In der Fernsehserie Andromeda gerät das Raumschiff "Andromeda Ascendant" nahe an den Ereignishorizont eines Schwarzen Lochs, wodurch Schiff und Besatzung aufgrund der Zeitdilatation bis zur Bergung und damit für 300 Jahre in der Zeit einfroren.






</doc>
<doc id="7900" url="https://de.wikipedia.org/wiki?curid=7900" title="VHS">
VHS

Die Abkürzung VHS steht für:


Die Abkürzung Vhs. steht für:



</doc>
<doc id="7903" url="https://de.wikipedia.org/wiki?curid=7903" title="DVD">
DVD

Die DVD ist ein digitaler, optischer Datenspeicher, der im Aussehen einer CD ähnelt, aber über eine höhere Speicherkapazität verfügt. Das Akronym „DVD“ geht ursprünglich auf die Abkürzung von Digital Video Disc zurück, später wurde die Abkürzung als Digital Versatile Disc (engl. für "digitale vielseitige Scheibe") interpretiert.

Mitte der 1990er Jahre konnte sich die Compact-Disc als Massenspeicher-Medium bei Computern durchsetzen. Dadurch wuchsen nicht nur die Anwendungsfelder, sondern auch die Bedürfnisse der Verbraucher und der Unterhaltungsindustrie. Gewünscht wurde ein Medium, mit dem Videos ähnlich komfortabel gehandhabt werden konnten wie Musik- und Sprachaufnahmen mit der CD. Zwar gab es dies bereits als "Video-CD" (VCD) und "Laserdisc" (LD), jedoch konnten auf der VCD maximal 74 Minuten (in knapper VHS-Qualität) und auf der LD maximal 128 Minuten Videomaterial (in voller Sendequalität) untergebracht werden. Dies führte bei Spielfilmen dazu, dass die VCD/LD mitten im Film gewechselt/umgedreht werden musste, ähnlich wie früher bei der Kompaktkassette oder der Schallplatte. 

Die Unterhaltungsindustrie arbeitete daran, die Speicherkapazität der CD weiter zu erhöhen. Hierbei gab es zwei unterschiedliche Konzepte: Sony und Philips betrieben die Entwicklung der "Multimedia-CD" (MMCD), Toshiba und Time Warner favorisierten die "Super Density CD" (SD).

Auf Druck der Filmindustrie, die nicht mehr, wie bei der Markteinführung der Videorekorder, mehrere Standards unterstützen wollte, einigten sich die Konkurrenten in Tokio am 15. September 1995 auf einen gemeinsamen Standard. Da die DVD zunächst als reines Speichermedium für Videodaten gedacht war, stand DVD anfangs für „Digital Video Disc“. Dies wurde jedoch geändert, als andere Verwendungsmöglichkeiten abzusehen waren. Als Alternative wurde „Digital Versatile Disc“ ("versatile" = vielseitig) ins Spiel gebracht, konnte sich aber nicht durchsetzen. Der aktuelle offizielle Standpunkt des DVD-Forums ist, dass DVD einfach drei Buchstaben ohne exakt festgelegte Bedeutung sind.

Ein Jahr später, 1996, kamen die ersten Abspielgeräte und DVD-Medien in den Handel. Zuvor mussten Unstimmigkeiten bezüglich des Verschlüsselungsverfahrens (CSS) ausgeräumt werden. Zudem gelang es der Filmindustrie, mit einem Regionalcode Marktkontrolle zu gewinnen. Mit dem Code soll verhindert werden, dass zum Beispiel eine DVD aus den USA auf einem europäischen Gerät abspielbar ist. Die Filmindustrie fürchtete hier Umsatzeinbußen, da Filme in den USA oft schon auf dem Videomarkt erhältlich sind, während sie in Europa noch gar nicht im Kino gezeigt wurden. Als Vertriebsstrategie der Anbieter ist auch bekannt, dass durch die regionale Beschränkung der Anwendbarkeit in unterschiedlichen Regionen unterschiedliche Preise („Marktpreise“) erzielt werden können.

Sowohl der verwendete Wiedergabeschutz Content Scramble System als auch der Regionalcode sind mittlerweile leicht zu umgehen. Die Industrie reagierte darauf einerseits mit rechtlichen Maßnahmen und andererseits mit dem Druck auf die Hersteller von DVD-Laufwerken, die Abfrage des Regionalcodes gerätetechnisch zu implementieren.

Ende 1996 waren die ersten DVD-Brenner im Handel verfügbar, die Preise lagen jedoch bei ca. 10.000 DM und der Preis eines 3,6-GB-Rohlings lag oberhalb von 100 DM.

Mittlerweile wird die Blu-ray Disc als Nachfolger der DVD beworben, die sich gegen das Konkurrenzformat HD DVD ab März 2008 durchsetzen konnte. Dabei werden durch Abtastung der noch enger gesetzten Pits und Lands mit einem blau-violetten Laserstrahl noch höhere Datenmengen untergebracht. Sie sollen vor allem hoch aufgelöste Videoinhalte speichern, die eine wesentliche höhere Speicherkapazität benötigen, als sie eine DVD bieten kann.
<br>

Die DVD gibt es in drei Varianten betreffs ihrer Beschreibbarkeit:

Diese gibt es mit verschiedenen Inhalten ("DVD-Formate") wie DVD-Video, DVD-Audio, DVD-ROM und Hybrid-Varianten.

Besonders die beschreibbare Formate und die DVD-RAM lassen sich mit dem bloßen Auge von den gepressten anhand ihrer Datenseite unterscheiden, da diese aufgrund ihrer Legierungen Farben wie etwa Blau, Violett oder Braun aufweisen. Die DVD-RAM hat außerdem charakteristische sichtbare Sektormarken.

Die DVD wird für folgende drei Verwendungszwecke eingesetzt, für die jeweils eigene DVD-Formate für spezielle Datenstrukturen geschaffen wurden:

Der codice_1-Ordner spielt bei Audio-DVDs eine zentrale Rolle. Bei der "DVD-Video" sind jedoch auch die Audiodaten im codice_2-Ordner zu finden, genauso wie die diversen Sprachversionen, Untertitel, Kapitelinformationen und Sonderfeatures (alle innerhalb sogenannter „"VOB"-Containerdateien“). Das bei DVDs üblicherweise eingesetzte Kompressionsformat ist MPEG-2, nach dem DVD-Standard ist jedoch auch noch das qualitativ weit unterlegene MPEG-1-Format vorgesehen, das ansonsten vor allem bei Video-CDs Anwendung findet. Der "MPEG-2-Videostream" (das Videobild) wird bei der Erstellung einer Video-DVD gemeinsam mit dem "Audiostream" und gegebenenfalls mit anderen Datenblöcken verwoben („gemuxt“, s. Multiplexing) und in einer ".VOB"-Datei („Video Object“) angelegt, die laut DVD-Standard nie größer als 1 Gigabyte sein darf. Wird diese Datenmenge überschritten, wird in den Programmen zur DVD-Erstellung ("DVD Authoring Software") automatisch eine neue ".VOB"-Datei angelegt. Die ".VOB"-Datei dient also als „Containerdatei“ aller Programmströme.

Beim Wechsel der Dateien ist der Übergang wegen des in den DVD-Playern integrierten Puffers nicht wahrnehmbar. Beim Abspielen werden abwechselnd Video-, Audio- und gegebenenfalls Steuerungsinformationen ausgelesen, zwischengespeichert und wiedergegeben. Der codice_2-Ordner enthält außerdem die ".IFO"-Datei mit der Menüführung und meist mehrere ".BUP"-Dateien, die jedoch nur als Backup der ".IFO"-Datei dienen.

Als Tonspuren sind mehrere Formate zugelassen; neben dem datenintensiven – weil unkomprimierten – linearen PCM-Datenstrom gibt es mehrere Komprimierungsverfahren: mp2 (meist 192–256 Kbit/s) in beliebigen Bitraten für Stereoton, Dolby-Digital- oder DTS-Mehrkanalton bis 5.1-Surround (meist 448 Kbit/s). Auch SDDS ist für die DVD spezifiziert, es gibt jedoch für den Heimbereich weder entsprechende Decoder noch DVDs mit SDDS-Tonspur. Ebenso wenig hat sich das Tonformat MPEG-2 Multichannel durchsetzen können.

Insgesamt stehen für den gesamten Datenstrom 10,08 Mbit/s zur Verfügung, für den Audiostrom maximal 6144 Kbit/s. Die Bildqualität der Video-DVD hängt nicht so sehr von der Bandbreite des Video-Streams als vielmehr von der Effizienz der Komprimierung ab. Oft wird mittels "MPEG-Encoder" in mehreren Durchläufen kodiert, um ein Höchstmaß an Effizienz zu erreichen. Bei "MPEG-2" können die Datenströme im VBR-Verfahren komprimiert werden, d. h., dass die Bandbreite an verschiedenen Stellen des Films stark abweichen kann (variable Bitrate).

Die Bitrate hängt dabei von der gerade anfallenden Datenmenge ab, sodass zum Beispiel bei bewegungsarmen Szenen Bandbreite und damit Speicherplatz auf der DVD gespart werden kann. Bei "MPEG-2" werden bei aufeinanderfolgenden Bildern in der Regel nur die Unterschiede zum vorausgehenden Bild gespeichert ("P-" oder "B-Frames"), um auf der Disc Platz zu sparen. Mehr Informationen dazu im Artikel DVD-Video.

"Daten-DVDs" (DVD-ROM) sind anders als Video-DVDs keinen Restriktionen unterworfen und können beliebige Ordner und Dateien enthalten. Als Dateisysteme werden entweder die im Computerbereich vorherrschenden Formate ISO 9660 und ISO/Joliet oder UDF verwendet; beide Systeme können innerhalb des "UDF Bridge"-Formates (ISO 9660 Level 3 Layer) kombiniert werden. Ebenso wie CDs können auch DVDs in mehreren Sessionen (Sitzungen) beschrieben werden (Multiborder, analog zu Multisession bei CDs). Einige ältere Betriebssysteme oder DVD-Player können jedoch nur auf die erste Session zugreifen, weshalb es sich in diesem Fall empfiehlt, die DVD in einem Zug zu beschreiben. Zum Auslesen der restlichen Sessions dienen Zusatzprogramme wie IsoBuster, die auch unter älteren Betriebssystemen laufen.

Daneben gibt es auch die "Hybrid-DVD", die die Eigenschaften einer DVD-Video, DVD-Audio oder DVD-ROM in einer DVD kombiniert. Eine solche Hybrid-DVD enthält Videos, Musik und Computerdaten und präsentiert im DVD-Spieler, DVD-Rekorder oder DVD-Laufwerk des Computers die jeweils abspielbaren Inhalte.

Technisch lässt sich eine Hybrid-DVD sehr einfach realisieren, weil die DVD-Video und DVD-Audio auf der DVD-ROM basieren. Die DVD-ROM speichert alle Inhalte als Dateien nach dem UDF-Dateisystem ab. Für die DVD-Video und DVD-Audio müssen nur zwei weitere Festlegungen vorgenommen werden: Die erlaubten Dateiformate und der Ablageort auf der DVD. Für die Hybrid-DVD ist besonders der Ablageort interessant. Wird eine DVD-Video oder Hybrid-DVD zum Beispiel in einen DVD-Rekorder eingelegt, so sucht dieser die Filmdateien im Unterverzeichnis codice_2. Nach dem gleichen Schema sucht ein DVD-Player die Audiodaten im Unterverzeichnis codice_1. Im DVD-Laufwerk eines Computers sind hingegen alle Dateien einer DVD ersichtlich, weil dieser jede DVD als eine DVD-ROM behandelt.

Der Endbenutzer kann nicht nur käufliche DVDs abspielen (die im Presswerk hergestellt wurden), sondern er kann mit einem DVD-Brenner auch eigene DVD-Videos, DVD-Audio oder DVD-ROMs erstellen. DVD-Brenner sind beispielsweise in Computern und Hi-Fi-DVD-Rekordern eingebaut und benötigen beschreibbare DVD-Formate. Historisch haben sich aus Kostengründen die fünf verschiedenen DVD-Formate DVD−R, DVD+R, DVD−RW, DVD+RW und DVD-RAM mit einfacher und doppelter Speicherkapazität (DL – Double Layer) entwickelt. Sie werden nach folgender Systematik bezeichnet:

Die drei DVD-Formate, die vom DVD-Forum stammen, werden auch als "Minus-Standard" bezeichnet. Nur diese Formate dürfen auch das offizielle DVD-Logo tragen. Entsprechend werden die zwei DVD-Formate von der DVD+RW-Allianz mit einem „+“ auch als "Plus-Standard" bezeichnet. Die DVD-Formate nach dem Plus-Standard sind technisch einfacher aufgebaut, wodurch zum Beispiel die DVD+RW andere Schreibmethoden als die DVD-RW unterstützt. Auch sind die Lizenzgebühren für die Patentnutzung bedeutend niedriger. Diese anfänglichen Preisvorteile des Plus-Standards gibt es inzwischen durch den harten Wettbewerb mit dem Minus-Standard nicht mehr.

Die Formatvielfalt führte anfangs zu einer Kaufzurückhaltung bei den Konsumenten, da unklar war, welches beschreibbare DVD-Format die größere Investitionssicherheit aufweist. Die Industrie reagierte seit 2003 darauf mit (preisgünstigen) Multi-Brennern, die sowohl das Minus- als auch das Plus-Format unterstützten.

Seit 2004 werden auf dem Massenmarkt auch beschreibbare DVDs mit zwei anstatt nur einer Datenschicht angeboten. Sie werden mit „DL“ bezeichnet, was im Minus-Format für „Dual Layer“ ("DVD−R DL"), im Plus-Format dagegen für „Double Layer“ ("DVD+R DL") steht. Beiden Formaten gemein sind die zwei übereinander geklebten Schichten "auf derselben Seite" der Platte, die gewisse Veränderungen im Aufbau der DVD notwendig machten. Nur so kann auch die zusätzliche Schicht beschrieben und gelesen werden. Die DVD±R DL bietet 8,5 GB Fassungsvermögen pro Medium, also etwa das 1,8-fache einer Single-Layer-DVD. Ihre zusätzliche Kapazität reicht oftmals aus, um große Einzeldateien (etwa hochauflösende Videos) auf einen einzigen Datenträger zu brennen, anstatt sie auf zwei herkömmliche DVDs aufzuteilen und ohne die Daten dafür erneut komprimieren zu müssen. Für RW-Medien dagegen sind diese Änderungen hin zum DL-Datenträger nicht möglich. Zu geringe Reflexionseigenschaften verhindern die zuverlässige Nutzung der zweiten Datenschicht.

Daneben existieren auch noch "doppelseitige" Medien in den Formaten DVD−R, DVD+R und DVD-RAM. Diese fassen tatsächlich 2 × 4,7 GB, also 9,4 GB pro Medium, da sie im Prinzip aus zwei einzelnen DVDs bestehen. Nachteilig wirkt sich dies besonders bei großen Dateien aus, da diese nicht wie bei ±DL-DVDs zusammenhängend gespeichert werden können. Jede Seite repräsentiert einen eigenständigen Datenträger, und um auf den jeweils anderen Datenbestand zugreifen zu können, muss die DVD bisher noch entnommen und gewendet werden, da es Laufwerke, die beide Seiten gleichzeitig nutzen können, bisher nicht gibt. Da die Double-Layer-/Dual-Layer-DVDs trotz geringerer Gesamtkapazität diverse Vorteile bieten (höhere maximale Dateigröße, Platz für ausführliche Beschriftung etc.), waren doppelseitige DVD±R-Rohlinge wegen der steigenden Nachfrage vorübergehend kaum noch erhältlich, seit Juli 2008 hat sich die Marktlage entspannt. Doppelseitige DVD-RAM sind ebenfalls erhältlich. Theoretisch waren so 17 GByte Speicherplatz möglich. Diese Möglichkeit wurde inzwischen technisch realisiert und wird als DVD-17 bezeichnet.

Die beschreibbaren DVD-Formate lassen sich nach ihrer Veränderbarkeit und Datensicherheit unterscheiden.


Die Standard-Datenrate 1× entspricht bei DVDs einer Geschwindigkeit von 11,08 Mbit/s (1,385 MB/s oder etwa 1,32 MiB/s) und damit in etwa einem CD-Laufwerk mit dem Geschwindigkeitsfaktor 9×. Die Schreibgeschwindigkeit 1× entspricht somit definitionsgemäß der maximalen Datenrate, die beim "Abspielen" einer standardkonformen Video-DVD auftreten kann.

Aktuelle DVD-Laufwerke schaffen Brenn- und Lesegeschwindigkeiten bis zu 24×. Die volle Geschwindigkeit wird dabei allerdings nur am äußeren Rand der DVD erreicht, während im Inneren deutlich langsamer gelesen und geschrieben werden kann.

DVDs benötigen zum Abspielen einen eigenen DVD-Spieler. Zur Unterscheidung zu normalen CD-ROM-Laufwerken ist dieser auf der Vorderseite mit dem DVD-Emblem gekennzeichnet. Im Vergleich zu den CDs wird bei DVDs mit Lasern kürzerer Wellenlänge gearbeitet, und wegen der gleichzeitig kürzeren Strahlengänge der Fokussierungsoptiken resultieren daraus kleinere Laserspots, mit denen in den Datenträgerschichten entsprechend kleinere Strukturen gelesen und geschrieben werden können.

Zur Langzeitarchivierung sind beschreibbare DVD-Formate nach einhelliger Expertenmeinung nicht geeignet, einzige Ausnahme könnte eventuell die DVD-RAM darstellen, wobei auch hier die Langzeithaltbarkeit nicht sicher erwiesen ist. Verbatim bietet in Österreich und der Schweiz eine lebenslange Garantie, allerdings nicht in Deutschland. Diese gilt für alle von Verbatim hergestellten optischen Datenträger, deckt allerdings nur Herstellungsfehler ab, nicht jedoch normale Abnutzung und unsachgemäße Behandlung. Im ungünstigsten Fall können DVD±R und DVD±RW auch schon nach wenigen Monaten Datendefekte aufweisen.

Eine Haltbarkeit der Daten von bis zu 1000 Jahren verspricht seit 2012 der Hersteller Millenniata für seine M-Disc. Millenniata beruft sich dabei auf Tests des US-Militärs. Zum Beschreiben sind M-Disc-fähige Brenner erforderlich. Ansonsten wird eine M-Disc wie eine gewöhnliche DVD gehandhabt.

Die Spezifikationen sehen die folgenden DVD-Typen in der Version 2 vor. Auf die Darstellung der seltenen Version 1 wird verzichtet.

Eine für DVD-RW 16fache Schreibgeschwindigkeit verwendete Laserdiode hat z. B. folgende Daten:

Die geringere Wellenlänge gegenüber CD-Lasern sowie die größere Apertur der Fokussieroptik ermöglichen einen kleineren Fokus und somit kleinere schreib- und lesbare Pits.

Die im Vergleich zur CD bei gleicher Geometrie der Disk etwa sechsmal so hohe Datenkapazität der DVD wird durch weniger als halb so lange Pits bei einem weniger als halb so großen Spur-Abstand sowie mehr Fläche für die Daten durch einen schmaleren Lead-In Bereich erreicht. Double Layer DVDs benötigen etwa 10 % längere Pits, weswegen zwei Schichten in dieser Konfiguration nicht die doppelte Kapazität einer Single Layer DVD bieten. Die feineren Strukturen der DVD sind anfälliger gegenüber Kratzern und Verschmutzungen, was durch die verwendete zweidimensionale Fehlerkorrekturmethode mehr kompensiert wird.
Im Gegenzug sind CDs extrem sensibel bezüglich Kratzer "auf der Oberseite", da diese im Allgemeinen direkt die Datenschicht darstellt. Daher sollten CDs nicht mit eindrückenden oder kratzenden Stiften wie Kugelschreiber oder (harten) Bleistiften beschriftet werden.

Auf der DVD werden zwei Reed-Solomon-Codes C1(182, 172, 11) und C2(208, 192, 17) eingesetzt, die durch Verkürzung aus einem Reed-Solomon-Code (255, 245, 11) bzw. (255, 239, 17) entstehen. C1 dient der Zeilencodierung und C2 der Spaltencodierung. Die so entstehende Matrix dient der Fehlerkorrektur, wobei in den Zeilen jeweils 5 Fehler und in den Spalten jeweils 8 Fehler korrigiert werden können.
Die Bits formula_1 mit formula_2 und formula_3 sind Paritätsbits, die beim Codieren entstehen.

Beim Interleaving der DVD wird ein 182 × 208-Byte Frame in 16 Frames bestehend aus 182 × 13 Bytes aufgeteilt. Dabei wird je eine Paritätszeile (formula_4) ans Ende eines 182 × 12-Byte Frames verschoben. Das heißt die formula_1 werden folgendermaßen in einer neuen Matrix formula_6 angeordnet:

Die so erzeugte Matrix wird ähnlich wie bei der CD decodiert. Dabei können maximal 4832-Bits oder ein Flächenfehler mit 2932 Bits korrigiert werden.

Zur Anwendung beider erweiterter Techniken braucht es höherfrequente (das heißt, die Farbe des Lasers ist in Richtung Blau verschoben) und genauere Laser als zum Auslesen einer CD. Um die zweite Datenschicht lesen zu können, muss der Laser dazu noch leicht anwinkelbar sein. Zusammen mit der veränderten Laserfokussierung ist es so möglich, die untere („verdeckte“) Schicht lesen zu können.

Die Binärdaten auf einer DVD werden nach der „Eight-to-Fourteen-Modulation-plus“ (EFMplus) geschrieben. Diese stellt sicher, dass sich nach minimal 3 und maximal 11 Takten die Polarität des ausgelesenen Signals ändert. Das geschieht, wenn der Laser in der Spur einen Übergang von einer Vertiefung („pit“) zu einem Abschnitt ohne Vertiefung („land“) passiert oder umgekehrt.

Der Hintergrund ist hierbei folgender: Die Abschnitte mit Vertiefungen bzw. ohne Vertiefungen müssen lang genug sein, damit der Laser die Veränderung erkennen kann. Würde man ein Bitmuster direkt auf den Datenträger schreiben, würden bei einem alternierenden Signal (1010101010101010…) falsche Werte ausgelesen, da der Laser den Übergang von 1 nach 0 beziehungsweise von 0 nach 1 nicht verlässlich auslesen könnte. Die EFMplus-Modulation erweitert das Signal von 8 auf 14+2 Bit auf und wählt die 2 Füllbits so, dass die oben erwähnte Forderung, dass sich nach minimal 3 und maximal 11 Takten die Polarität ändert und ein Übergang von 1 nach 0 oder umgekehrt geschieht, erfüllt wird. Die CD verwendet eine simplere 8-zu-14-Bit-Methode mit insgesamt 3 Füllbits namens EFM, woher der jetzt eigentlich falsche Name bei der DVD rührt(?); korrekter wäre „Eight-to-Sixteen“.

Nachdem die ersten DVD-Brenner (DVD-Schreiber) nur eine Datenmenge von 3,56 GB auf einen einmal beschreibbaren DVD-Rohling speichern konnten, wurde die Kapazität später auf die volle Größe einer DVD-5 (4,7 GB) angehoben und zusätzlich wiederbeschreibbare Medien mit diesem Fassungsvermögen vorgestellt. Seit Mitte 2004 beherrschen DVD-Brenner auch die Doppelschicht-Technik (""), welche die Speicherung von Daten auf einem zweischichtigen Rohling ermöglicht. Die zweite Datenschicht besitzt weiter gesetzte Pits und Lands, um ein Lesen durch die untere Schicht hindurch zu ermöglichen, und ist somit kleiner. So fasst ein solcher Rohling statt 9,4 GB (die Kapazität zweier DVD-5) lediglich zirka 8,5 GB.


Generell werden die Discs aller DVD-Formate aus zwei einzelnen aus Polycarbonat gespritzten Kunststoffscheiben von etwa 0,6 mm Dicke hergestellt (Ausnahme: Ecodisc). Dabei hat die untere „Halbscheibe“ (Layer 0) einen Stapelring als Abstandshalter. Die obere „Halbscheibe“ (Layer 1) trägt bei dem Format DVD-5 keine nutzbaren Informationen und wird daher als „Dummy“ bezeichnet. Die eigentliche DVD entsteht, wenn beide Hälften mit unter UV-Licht aushärtendem Lack verklebt werden („bonden“; Schichtdicke etwa 50 µm). Um eine gleichbleibende Qualität der hergestellten Scheiben zu gewährleisten, sind in den Produktionsanlagen üblicherweise hochauflösende Kamerasysteme, so genannte Inline Scanner, integriert. Stichprobenartig kommen auch Offline-Messlaufwerke zum Einsatz, um die elektrischen Signale der DVD zu analysieren. Sollte sich beim Verkleben der zwei Layers Luft einschleichen (ein so genannter Bondingfehler), kann die DVD schnell Schaden nehmen. So kann beispielsweise beim Einlegen der DVD ein Teil eines Layers absplittern. Hierdurch kann eine Unwucht entstehen, die zu weiteren Schäden an der DVD oder sogar am Abspielgerät führen kann. Allerdings sind diese Fehler in der Regel nur ein optisches Problem und haben keinen Einfluss auf die Abspielbarkeit der DVD.

Beim Herstellvorgang gibt es große Unterschiede zwischen bespielten Medien (Kauf-Videos) und unbespielten/wiederbeschreibbaren Scheiben (Rohlinge). Die Formate DVD-5, -9 und -10 können meist auf derselben Maschine hergestellt werden. Dabei werden die Informationen mit Matrizen (Stamper) in das heiße Polycarbonat gepresst (Spritzprägen). Um die Daten für die Laufwerke lesbar zu machen, werden die Halbscheiben mit Metall beschichtet (sputtern). Bei DVD-5, DVD-10 und dem Layer 1 bei DVD-9 wird Aluminium vollreflektierend gesputtert (etwa 50 nm). Da bei DVD-9 beide Informationsschichten von einer Seite gelesen werden, wird die untere (Layer 0) halbtransparent mit Gold, Silizium oder Silberlegierungen beschichtet (etwa 10–15 nm). DVD-14 und DVD-18 erhält man, indem zwischen die Halbscheiben eine bereits mit weiteren DVD-Strukturen versehene Folie eingebracht wird.

Beim Brennen ist kein Glasmaster erforderlich, sondern nur ein Computer, ein DVD-Brenner und ein Brennprogramm.

Für das Brennen benötigt man DVD-Rohlinge, die in unterschiedlichen Qualitäten als DVD−R, DVD+R, DVD−RW, DVD+RW und DVD-RAM erhältlich sind. Durch die verschiedenen DVD-Formate und die Tatsache, dass diese teilweise erst nach der Definition des ursprünglichen DVD-Regelwerks spezifiziert wurden und Varianten desselben sind, besteht eine gewisse Wahrscheinlichkeit, dass die gebrannte DVD auf einigen DVD-Playern nicht abspielbar sein wird. Deswegen sollte man sich nach der Kompatibilität des Brenners und der gewünschten Abspielgeräte vor dem Kauf der Rohlinge genau erkundigen. Einige DVD-Brenner bieten die Möglichkeit, DVD+R- und DVD+RW-Rohlinge mit dem Book Type DVD-ROM zu kennzeichnen und dadurch deren Akzeptanz durch ältere DVD-Abspielgeräte deutlich zu erhöhen.

Häufig müssen die DVDs nach dem Brennen finalisiert werden. Bei DVD+RW und DVD-Ram ist ein Finalisieren nicht notwendig, es wird aber empfohlen, ein DVD-Menü zu erstellen.

Für den Labelaufdruck bei der DVD stehen ebenso wie bei der CD verschiedene Drucktechniken zur Verfügung:

Im Siebdruck sind bis zu sechs Labelfarben möglich, es können Schmuckfarben (HKS oder Pantone) gewählt werden. Siebdruck ist derzeit die gängigste Variante, um CDs oder DVDs zu bedrucken, wird aber zunehmend vom Offsetdruck verdrängt. Der Siebdruck ist geeignet für gepresste CDs und DVDs; auch die Rohlingsbedruckung im Siebdruck ist möglich. Im Siebdruck sind die Farben sehr brillant.

Im Trockenoffset sind vier Labelfarben möglich (CMYK-Farbmodell), kombiniert mit dem Siebdruck bis zu sechs Labelfarben (CMYK im Offset und zusätzlich weiß Vollfläche und eine Schmuckfarbe oder Glanzlack im Siebdruck). Auf Grund der höheren Auflösung als im Siebdruck ist der Offsetdruck ideal für fotorealistische Darstellungen. Seit Anfang 2004 ist der Offsetdruck nicht nur für gepresste CDs und DVDs, sondern auch für CD-Rohlinge und DVD-Rohlinge möglich.

Bei diesem Druckverfahren wird mit einem speziellen Drucker Farbe von einem Farbband durch Erhitzung des Druckkopfes auf die CD oder DVD übertragen. Technisch bedingt ist das Druckverfahren eher für Schriften und Logos geeignet. In der Praxis wird dieses Verfahren bei kleinen Auflagen (gebrannte CDs und DVDs) angewendet.

Der Thermoretransferdruck ist die Weiterentwicklung des Thermotransferdrucks. Das Labelmotiv wird im Thermotransferdruckverfahren auf ein Übertragungsband gedruckt und davon dann eine Folie auf die CD oder DVD aufgebracht. Durch diese Technik ist eine bessere Auflösung möglich. So kann bereits bei Kleinauflagen ein fotorealistischer Druck erreicht werden.

Es gibt spezielle DVD- bzw. CD-Rohlinge, die gegenüber der Datenseite eine weiße Druckseite besitzen. Diese besteht aus einem speziellen, saugfähigen Material, das ein Verlaufen der Tinte verhindern soll. Zum Bedrucken sind spezielle Drucker nötig, deren Technologie sich kaum von der unterscheidet, die zum Bedrucken von Papier genutzt wird. Entsprechend gibt es auch Drucker, die sowohl CDs, DVDs als auch Papier bedrucken können. Praktisch findet dieses Verfahren nur bei Heimanwendern und sehr kleinen Auflagen von gebrannten Medien eine Anwendung.

Diese Methode ist für den Heimanwender nicht zu empfehlen. Wie bei einem Bimetall wölbt sich die DVD bei Temperaturunterschieden, da sich der Aufkleber und die Polycarbonat-Scheibe unterschiedlich stark ausdehnen. Im Gegensatz zu normalen CDs reichen bei einer DVD schon geringe Verzerrungen aus, dass der Player die Daten nicht mehr lesen kann. Dieser Effekt wird durch die Wärme im Inneren des DVD-Players noch verstärkt, so dass beklebte DVDs häufig erst nach einer gewissen Spieldauer ausfallen. Um dies zu verhindern, sind spezielle DVD-Aufkleber aus Kunststofffolie erhältlich, die sich gleichmäßig mit der Scheibe ausdehnen sollen. Nachteilig ist zudem, dass in der Regel durch den Aufkleber eine Unwucht entsteht. In DVD-Laufwerken kann diese Unwucht bei hohen Drehzahlen zu einer Ablösung des Aufklebers oder sogar einem Zerreißen der DVD führen.

Mit Folienstiften, CD-Markern und anderen Schreibern für glatte Flächen können DVDs natürlich auch von Hand beschriftet und bemalt werden. Dies ist die günstigste und schnellste Methode. DVDs sind – anders als CDs – recht unempfindlich gegen Stifte, die die Oberfläche verkratzen oder chemisch angreifen, da ihre Datenschicht mittig liegt und somit von einer relativ dicken Kunststoffschicht geschützt ist.

Beschriftung des Datenträgers durch den Laser direkt im Laufwerk. Dies setzt einen speziellen Brenner und geeignete Rohlinge voraus, die ein solches Verfahren beherrschen. Die Vorteile des Laser-Labels liegen im Bedrucken auch in mobilen Betrieb, die Vermeidung von Neuinvestitionen in neue Drucker und der Möglichkeit, in mehreren Sessions zu drucken (Lightscribe). Nachteilig sind der ausschließlich monochrome Druck, die geringe Qualität, das geringe Drucktempo (optimaler Kontrast bei über 30 min. Druckzeit) sowie die teuren und immer schwerer erhältlichen Rohlinge.

Vor Jahren hörte man immer wieder von einer neuerfundenen "Einweg-DVD", die besonders den Spielfilmverleih von Videotheken revolutionieren sollte. Sobald die DVD aus der luftdichten Hülle entfernt wird und mit Sauerstoff in Berührung kommt, erfolgt eine chemische Reaktion, welche die DVD innerhalb von 8 bis 48 Stunden unbrauchbar macht. Nach Ablauf dieser Zeit kann die DVD vom Kunden einfach weggeworfen werden, weswegen diese DVDs auch "Wegwerf-DVDs" genannt werden. Dem Vorteil, dass man diese DVDs der Videothek nicht mehr zurückbringen muss und somit auch Verzugsgebühren kein Thema mehr sind, stehen ökologische Nachteile gegenüber, auch wenn das Produktmaterial vollständig recycelbar ist.

Die Firma Flexplay hatte eine solche Einweg-DVD unter dem Namen "EZ-D" herausgebracht. Diese wurde ab September 2003 von Buena Vista Home Entertainment am US-Markt getestet. Es erschienen Datenträger mit einem Film für etwa 5 bis 7 US-Dollar kurz vor der eigentlichen Premiere desselben. Das Produkt fand jedoch nicht genug Käufer, so dass dieses Anfang 2004 bereits wieder aus den Verkaufsregalen verschwand. Ein ähnliches Verfahren hatte auch schon die Firma SpectraDisc zuvor vorgestellt, nach deren Prinzip die Einweg-DVDs jedoch aufgrund von Lichtempfindlichkeit unbrauchbar wurden.

Eine andere Variante vertrieb die Firma DVD-D Germany Ltd bis 2012. Die Daten auf der "DVD-D" (D für engl. 'disposable', dt. 'Wegwerfartikel') sind nach dem ersten Abspielen 48 Stunden lesbar, danach erscheint im Player „No disc“. Die Datenzerstörung wird durch die Rotation im Abspielgerät gestartet, gelöscht wird dabei nach Angaben der Firma das Steuerungsmenü der DVD. Allerdings gibt es zum Mechanismus keine genaueren Angaben. Teilweise wird dies begründet mit Flüssigkeits-Tanks in der DVD, die durch die Rotation aufbrechen (Zentrifugalkraft). Auf der Website des Unternehmens DVD-D Germany wurden bereits mehrere Kinofilme in diesem Format zum Preis von rund 4 € angeboten.

Bestrebungen zu einer besseren ökologischen Verträglichkeit der DVD führten zur Entwicklung der Ecodisc. Diese DVD besteht aus nur einer Polycarbonat-Scheibe und hat nur 8 Gramm Gewicht. Die Speicherkapazität ist mit 4,7 GB gleich groß wie bei der DVD-5.

Während heutzutage das DivX-Format als Videokompressionsalgorithmus bekannt ist, bezeichnet DIVX eine spezielle Pay-per-View-Variante in den USA, die heute nicht mehr existiert. Im Jahr 1998 kam die Idee auf, zu den damals noch erheblich teureren DVDs eine Billigvariante anzubieten, die 48 Stunden lang abgespielt werden konnte, jede darüber hinausgehende Nutzung war kostenpflichtig. Zur Dekodierung und Abrechnung der Filme wurden spezielle, mit einem Modem ausgestattete Player benötigt, die sich regelmäßig mit einem speziellen Server verbanden, um Abrechnungsdaten zu übertragen.

DIVX benutzte ein MPEG-4-Derivat, das mit speziellen DIVX-Flags zur Identifizierung und Dekodierung des Films versehen war. Letzten Endes konnte sich das System der DVD gegenüber nicht durchsetzen und endete nach nur einem Jahr als Flop.

Im Jahr 2010 wurde die DVD Opfer des Aprilscherzes der renommierten Computerzeitschrift c’t. Dem Artikel nach sollten gepresste DVDs anfällig für Bakterienbefall sein. Als Merkmal wurden Flecken auf den DVDs genannt. Das Bakterium könne ganze Stapel – allerdings lediglich neuerer – DVDs zerstören und würde sich auch über infizierte Laufwerke verbreiten. Die Leser wurden aufgerufen, ihre DVD-Sammlung umzusortieren, so dass zwischen neuen immer zwei alte DVDs stehen, ihre DVD-Laufwerke zu behandeln und tagelang nicht zu benutzen. Ein fingiertes Schreiben vom Verband der Videothekenbesitzer, datiert auf den 1. April, und die Verwendung der Nummer eines in der Fernsehserie Lost vorkommenden Impfstoffs als Bestellnummer enttarnten den Artikel als Aprilscherz.





</doc>
<doc id="7907" url="https://de.wikipedia.org/wiki?curid=7907" title="Säugetiere">
Säugetiere

Die Säugetiere (Mammalia) sind eine Klasse der Wirbeltiere. Zu ihren kennzeichnenden Merkmalen gehören das Säugen des Nachwuchses mit Milch, die in den Milchdrüsen der Weibchen produziert wird, sowie das Fell aus Haaren, das sie in Kombination mit der gleichwarmen Körpertemperatur relativ unabhängig von der Umgebungstemperatur macht. Bis auf wenige Ausnahmen sind Säugetiere lebendgebärend. Säugetiere sind an Land am artenreichsten verbreitet, doch bevölkern sie auch Luft und Wasser. Das Verhaltensspektrum der Säugetiere ist breit und flexibel, einige Gruppen zeigen komplexe soziale Gefüge. Anfang 2018 wurden weltweit 6399 rezente Arten unterschieden.
Die Säugetiere werden in drei Unterklassen eingeteilt: die eierlegenden Ursäuger (Protheria), die Beutelsäuger (Metatheria) und die Höheren Säugetiere oder Plazentatiere (Eutheria), zu welchen auch der Mensch zählt. Diejenige Richtung der speziellen Zoologie, die sich der Erforschung der Säugetiere widmet, wird als Mammalogie bezeichnet.

Säugetiere zählen zu den Landwirbeltieren (Tetrapoda) innerhalb des Taxons der Wirbeltiere (Vertebrata) und teilen somit die Merkmale dieser Gruppen, die hier nicht einzeln wiedergegeben werden.

Ein Fellkleid aus Haaren ist eines der wichtigsten Merkmale der Säugetiere. Auch wenn manche Arten (zum Beispiel die Wale) praktisch haarlos sind, haben sie sich doch aus behaarten Vorfahren entwickelt und zeigen zumindest in ihrer Embryonalentwicklung Haarwuchs. Die meisten Säugetierarten sind zeit ihres Lebens am überwiegenden Teil des Körpers behaart. Haare bestehen hauptsächlich aus dem Protein Keratin. Die Haare der Tiere können mehrere Funktionen haben:

Säugetiere sind in der Regel durch ein heterodontes Gebiss mit vier verschiedenen Zahntypen charakterisiert, die Schneidezähne (Incisivi), Eckzähne (Canini), und zwei Arten von Backenzähnen (Prämolaren und Molaren). Die Zahl der einzelnen Zahntypen wird mit der Zahnformel wiedergegeben. Ein heterodontes Gebiss ist ein wichtiges Unterscheidungsmerkmal von den homodonten (gleichförmigen) Gebissen der Reptilien und vor allem bei der Einordnung von Fossilien von Bedeutung. Bei den meisten Säugetieren gibt es einen einmaligen Zahnwechsel ("Diphyodontie"). Zunächst werden Milchzähne angelegt ("lacteale Dentition"), die später durch die „zweiten“ oder bleibenden Zähne ("permanente Dentition") ersetzt werden. Lediglich die Molaren werden nicht ersetzt, sondern kommen erst mit den bleibenden Zähnen.

Eine Reihe von Säugetiergruppen besitzt wurzellose Zähne, die zeitlebens weiterwachsen und durch Abrieb abgenutzt werden. Dazu zählen beispielsweise die Nagezähne der Nagetiere oder die Stoßzähne der Elefanten, des Narwals, des Walrosses und anderer Arten.


Ein Exklusivmerkmal der Säugetiere sind die drei Gehörknöchelchen Hammer ("Malleus"), Amboss ("Incus") und Steigbügel ("Stapes"). Diese befinden sich im Mittelohr, sie nehmen die Schwingungen des Trommelfells auf und leiten sie an das ovale Fenster des Innenohres weiter.

Stammesgeschichtlich können die Gehörknöchelchen von Bestandteilen ursprünglicher Kiemen- bzw. Kieferbögen abgeleitet werden: Der Steigbügel vom "Hyomandibulare", welches bei den Fischen Bestandteil des Suspensoriums und bei anderen Landwirbeltieren als "Columella" ausgebildet ist, Amboss und Hammer vom "Quadratum" sowie von einem Teil des durch Knochen ersetzten "Meckelschen Knorpels", dem "Articulare". Das Trommelfell wird von einem fast ringförmigen Knochen, dem Tympanicum, umschlossen.

Bei den anderen Wirbeltieren bilden Quadratum und Articulare das primäre Kiefergelenk, welches bei den Säugetieren während der fetalen Entwicklung durch ein an anderer Stelle entstehendes, sekundäres Kiefergelenk ersetzt wird. Dieses wird von den Deckknochen "Dentale" und "Squamosum" gebildet. Der Übergang vom primären zum sekundären Kiefergelenk wurde funktionell möglich, als die Gelenkachsen beider infolge der Größenzunahme des Gehirns bzw. Hirnschädels bei den Cynodontia in eine Linie zusammenfielen.


Im Zuge ihrer Entwicklungsgeschichte haben die Säugetiere nahezu alle Lebensräume besiedelt und sich dabei in eine Vielzahl von Formen aufgeteilt. Eine Reihe von Arten hat sich an eine aquatische (wasserlebende) Lebensweise angepasst, am spezialisiertesten sind die Wale, deren Körperbau Ähnlichkeiten mit den Fischen aufweist. Die Vordergliedmaßen sind zu Flossen (Flipper) umgestaltet, die Hintergliedmaßen sind rückgebildet und der Schwanz ist zu einer Fluke umgebildet. Bei anderen Taxa wie Robben und Seekühen ist die Anpassung an das Wasser weniger weit fortgeschritten. Die Fledertiere sind neben den Vögeln und den ausgestorbenen Flugsauriern die einzigen Wirbeltiere, die zum aktiven Fliegen fähig sind. Sie weisen stark verlängerte Finger auf, die die Flughaut spannen. Daneben hat eine Reihe von Säugetiertaxa unabhängig voneinander Gleitmembranen entwickelt, die ihnen einen passiven Gleitflug ermöglichen, dazu zählen die Riesengleiter, die Gleit- und Dornschwanzhörnchen aus der Gruppe der Nagetiere sowie drei Familien gleitender Beuteltiere (die Gleit-, Ring- und Zwerggleitbeutler). Verschiedenste Säugetiere sind an eine unterirdisch-grabende Lebensweise angepasst, diese haben einen walzenförmigen Körperbau mit kurzen, oft zu Grabwerkzeugen erweiterten Gliedmaßen entwickelt. Zahlreiche Arten führen eine arboreale (baumbewohnende) Lebensweise, diese sind oft durch greiffähige Pfoten mit opponierbarem Daumen und Greifschwanz charakterisiert. Bewohner von Grasländern und anderen offenen Habitaten weisen oft eine Reduktion der Zehenanzahl und die Herausbildung von verhornten Zehen oder Hufen auf, andere haben stark vergrößerte Hinterbeine und eine springende Fortbewegung entwickelt. Viele Arten, vorwiegend kleinere, versteckt lebende, weisen hingegen einen gedrungenen Körperbau mit kurzen Gliedmaßen auf, darunter zahlreiche Nagetiere und Insektenfresser.

Auch hinsichtlich der Größe gibt es beträchtliche Unterschiede: Als kleinste Säugetiere gelten die Schweinsnasenfledermaus und die Etruskerspitzmaus, die jeweils nur 2 Gramm Körpergewicht erreichen. Der Blauwal hingegen gilt als das größte Tier, das jemals auf der Erde lebte, und erreicht in Ausnahmefällen bis zu 150 Tonnen Gewicht, was das 75-Millionen-fache der kleinsten Säuger darstellt.

Säugetiere sind weltweit verbreitet, sie finden sich auf allen Kontinenten, in allen Ozeanen sowie auf den meisten Inseln. Ursäuger sind auf Australien und Neuguinea beschränkt, Beutelsäuger leben im australisch-ozeanischen Raum sowie in Nord-, Mittel- und Südamerika. Höhere Säugetiere haben eine weltweite Verbreitung, waren aber bis zur Ankunft des Menschen in Australien nur durch relativ wenige Arten vertreten, namentlich Fledertiere und Echte Mäuse. Auf abgelegenen Inseln gab es bis zur Ankunft des Menschen nur eine eingeschränkte Säugetierfauna, so waren auf vielen Inseln, darunter Neuseeland, Fledertiere die einzigen Säuger.

Säugetiere haben nahezu alle Regionen der Erde besiedelt und kommen in den meisten Lebensräumen vor. Man findet sie auch in Wüsten und Regenwäldern, im Hochgebirge und in den Polarregionen. Zu den wenigen Regionen, in denen sich (zumindest bis auf zeitweilige Aufenthalte des Menschen) keine Säuger finden, zählt das Innere des antarktischen Kontinents. Mehrere Gruppen von Säugetieren, die Meeressäugetiere haben sich dem Leben im Meer angepasst, in der Tiefsee finden sich allerdings nur wenige, spezialisierte Walarten.

So unterschiedlich die Säugetiere in Bezug auf ihren Körperbau und ihre Lebensräume sind, so unterschiedlich sind auch ihre Lebensweisen. Es finden sich tag-, dämmerungs- und nachtaktive sowie kathemerale (sowohl am Tag als auch in der Nacht aktive) Arten. Auch hinsichtlich des Sozialverhaltens gibt es beträchtliche Unterschiede, neben strikt einzelgängerischen Arten gibt es andere, die in Gruppen von bis zu Tausenden von Tieren zusammenleben. Manche Arten haben komplexe Verhaltensmuster entwickelt, sie etablieren eine strenge Rangordnung innerhalb der Gruppe und kommunizieren untereinander mittels Lauten, Gesten oder Körperhaltungen. Obwohl es die Ausnahme ist, so gibt es auch Säugetiere, die Gifte zur Verteidigung oder zur Jagd einsetzen (siehe: Giftige Säugetiere).

Einige Säugetiere vermeiden klimatisch extreme Zeiten und den damit verbundenen Nahrungsmangel, indem sie in einen Winterschlaf oder einen Torpor (Starrezustand) verfallen, etwa in kalten oder trockenen Jahreszeiten. Dabei fällt die Körpertemperatur nahezu auf die Umgebungstemperatur ab, Atmung und Herzschlag verlangsamen sich und der Stoffwechsel wird reduziert.

Der Geruchssinn spielt eine bedeutende Rolle in der Lebensweise der Säugetiere, unter anderem bei der Nahrungssuche und bei der Fortpflanzung, wo Pheromone die Paarungsbereitschaft signalisieren. Auch für das Territorialverhalten ist der Geruch bedeutend, etliche Arten markieren ihr Territorium mittels Urin, Kot oder spezieller Drüsensekrete.

Im Allgemeinen ist bei Säugetieren das Gehör gut entwickelt. Eine Sonderform ist die Echoortung, bei der anhand des zurückkehrenden Echos ausgesandter Schallwellen die eigene Position bestimmt oder Beute lokalisiert werden kann. Bei zwei Taxa, den Zahnwalen und den Fledermäusen, ist die Echolokation besonders ausgeprägt, sie findet sich aber auch bei anderen Gruppen.

Auch der Tastsinn dient der Wahrnehmung der Umwelt. Viele Arten haben zu diesem Zweck spezielle Tasthaare (Vibrissae) entwickelt, die außerordentlich empfindlich sind und durch Muskelbewegungen gesteuert werden können. Auch die Haut selbst ist ein Sinnesorgan, bestimmte Körperteile sind besonders reich an Mechanorezeptoren, zum Beispiel die Fingerspitzen der Primaten oder die Nasen- beziehungsweise Rüsselregion vieler Arten. Der bestentwickelte Tastsinn aller Säuger wird im Allgemeinen dem Sternmull zugesprochen. Erwähnt seien in diesem Zusammenhang noch die feinen Elektrorezeptoren im Schnabel der Kloakentiere, die auf die Muskelbewegung der Beutetiere reagieren. Auch in der sozialen Interaktion ist der Tastsinn oft bedeutend, zum Beispiel bei der von vielen Tieren praktizierten gegenseitigen Fellpflege („Grooming“).

Die Bedeutung des Gesichtssinnes ist stark unterschiedlich. Oft spielt er jedoch nur eine untergeordnete Rolle, insbesondere bei unterirdisch lebenden Tieren, deren Augen oft rückgebildet sind. Große Augen und ein relativ gutes Sehvermögen haben dagegen beispielsweise die Katzen und die Primaten. Auch die Position der Augen ist ausschlaggebend: während Räuber meist nach vorne gerichtete Augen haben, die ein räumliches Sehen und somit eine genauere Entfernungsabschätzung ermöglichen, sind die Augen von Beutetieren oft seitlich angebracht, was einem nahezu vollständigen Rundumblick und der frühestmöglichen Erkennung von Gefahren dient.

Eine Gemeinsamkeit aller Säugetiere ist der verglichen mit anderen Tieren gleicher Größe hohe Energie- und demzufolge Nahrungsbedarf, der eine Folge der gleich bleibenden Körpertemperatur ist. Einige Arten verzehren täglich nahezu Nahrung im Ausmaß ihres eigenen Körpergewichtes. Hinsichtlich der Art der Nahrung gibt es eine gewaltige Bandbreite, es finden sich Pflanzenfresser (Herbivoren), Fleischfresser (Carnivoren) und ausgeprägte Allesfresser (Omnivoren). Die Anzahl und der Bau der Zähne sowie die Ausgestaltung des Verdauungstraktes spiegeln die Ernährungsweise wider. Fleischfresser haben einen kurzen Darm, um die rasch entstehenden Fäulnisgifte ihrer Nahrung zu vermeiden. Pflanzenfresser, deren Nahrung im Allgemeinen schwerer verdaulich ist, haben eine Reihe von Strategien entwickelt, um die Inhaltsstoffe bestmöglich verwerten zu können. Dazu gehören unter anderem ein längerer Darm, ein mehrkammeriger Magen (zum Beispiel bei Wiederkäuern oder Kängurus) oder die Caecotrophie, das nochmalige Verzehren des Kotes bei Nagetieren und Hasen. Rein blätterfressende (folivore) Arten (zum Beispiel Koalas oder Faultiere) nutzen ihre nährstoffarme Nahrung bestmöglich aus, indem sie ausgesprochen lange Ruhephasen einlegen.

Eine Form des Lernverhaltens ist die Prägung, bei Säugetieren ist die olfaktorische Prägung, das heißt die Sensibilisierung für verschiedene Gerüche, häufiger als bei anderen Wirbeltiergruppen. Oft dient die Prägung zur Erkennung von Verwandten, etwa der Mutter oder den Geschwistern. Mit prägungsähnlichen Erfahrungen kann auch die Nahrungspräferenz bestimmt werden. Gelernte Aktionen können auch tradiert, das heißt weitergegeben werden. Voraussetzung dafür ist das Leben in Gruppen mit Sozialstrukturen. Die meisten Säugetiere zeigen in der Jugendphase Spielverhalten, manche sogar bis ins hohe Alter. Häufig kommt es zu Sozialspielen mit Spielpartnern, in denen beispielsweise von fleischfressenden Tieren das Anschleichen an die Beute oder bei Huftieren die Flucht eingeübt wird. Oft erfolgen anschließend Rollenwechsel von Angreifern und Verteidigern. Auch Objektspiele kommen vor, indem Gegenstände berührt oder in Bewegung versetzt werden.

Die meisten Säugetierarten sind entweder polygyn (ein Männchen paart sich mit mehreren Weibchen) oder promiskuitiv (Männchen und Weibchen paaren sich mit mehreren Partnern). Da das Tragen und das Säugen für die Weibchen zeit- und energieintensiv sind, könnten die Männchen mehr Jungtiere zeugen als die Weibchen gebären können. Daraus ergibt sich in vielen Fällen ein polygynes Verhalten, bei dem sich relativ wenige Männchen mit vielen Weibchen fortpflanzen und sich vielen Männchen keine Paarungsmöglichkeit bietet. Eine Folge davon sind oft heftige Rivalenkämpfe zwischen den Männchen um das Paarungsvorrecht und in manchen Fällen eine Wahlmöglichkeit seitens des Weibchens. Daraus resultieren bei vielen Säugetieren komplexe Verhaltensweisen oder anatomische Merkmale in Hinblick auf die Fortpflanzung. Viele Arten sind durch einen Geschlechtsdimorphismus (Männchen sind oft deutlich größer und schwerer als Weibchen) charakterisiert, auch als eine Folge des Selektionsdruckes der Männchen im Hinblick auf eine Verbesserung der Paarungschance.

Schätzungen zufolge leben drei Prozent aller Säugetierarten in monogamen Beziehungen, in welchen sich ein Männchen während der Paarungszeit nur mit einem einzigen Weibchen fortpflanzt. In diesen Fällen beteiligt sich das Männchen meistens zumindest teilweise an der Jungenaufzucht. Manchmal hängt das Paarungsverhalten auch von den Umweltbedingungen ab: bei knappen Ressourcen paart sich das Männchen nur mit einem Weibchen und hilft bei der Aufzucht mit, bei Nahrungsreichtum kann das Weibchen das Jungtier allein großziehen und die Männchen paaren sich mit mehreren Partnerinnen.

Die Polyandrie (ein Weibchen paart sich mit mehreren Männchen) findet sich nur selten im Säugetierreich, zum Beispiel bei manchen Krallenaffen. Bei diesen Tieren kümmert sich hauptsächlich das Männchen um den Nachwuchs.

Erwähnt seien noch manche Arten der Sandgräber, einer in Afrika lebenden Nagetiergruppe, wie der Nackt- oder der Graumull. Diese pflegen eine eusoziale Lebensweise: Ähnlich wie bei manchen Insekten ist in einer Kolonie ein einziges Weibchen, die „Königin“, fruchtbar und paart sich mit mehreren Männchen, während die übrigen Tiere als unfruchtbare Arbeiter die notwendigen Tätigkeiten zur Versorgung der Gruppe verrichten.

Hinsichtlich der Gebärweise gibt es zwischen den drei Unterklassen der Säugetiere die augenfälligsten Unterschiede.

Merkmal der Ursäuger ist eine gemeinsame Körperöffnung für die Ausscheidungs- und Fortpflanzungsorgane, die Kloake. Der Penis der Männchen ist ausschließlich samenführend und an der Spitze gespalten. Die Ursäuger unterscheiden sich von allen anderen Säugetieren darin, dass sie nicht lebendgebärend sind, sondern Eier legen. Diese sind klein (rund 10 bis 15 Millimeter Durchmesser) und ähneln mit ihrer ledrigen Schale und dem großen Dotter mehr Reptilien- als Vogeleiern. Die ein bis drei Eier werden vom Weibchen rund zehn Tage lang bebrütet. Neugeschlüpfte Ursäuger sind nackt und klein und sind in ihrem embryoartigen Zustand mit neugeborenen Beuteltieren vergleichbar. Ein Beispiel für Ursäuger ist das Schnabeltier (Ornithorhynchus anatinus), das an der Ostküste Australiens beheimatet ist.

Die Beutelsäuger unterscheiden sich im Bau der Fortpflanzungsorgane deutlich von Höheren Säugetieren. Bei ihnen ist der Fortpflanzungstrakt verdoppelt, Weibchen haben zwei Uteri und zwei Vaginae, auch die Männchen besitzen einen gespaltenen oder doppelten Penis mit davorliegendem Scrotum. Die Tragzeit ist kurz (12 bis 43 Tage), Rekordhalter ist die Schmalfußbeutelmaus "Sminthopsis macroura" mit nur 10,5 bis 11 Tagen. Die meisten Arten entwickeln keine Plazenta, allerdings ist bei manchen Beutelsäugern (zum Beispiel Koalas oder Nasenbeutlern) ein primitiver Mutterkuchen vorhanden. Die Neugeborenen kommen durch einen zwischen den Vaginae liegenden Geburtskanal zur Welt, der bei vielen Arten eigens für die Geburt angelegt wird. Neugeborene Beutelsäuger sind klein und im Vergleich zu den Höheren Säugetieren unterentwickelt. Das Gewicht des Wurfes beträgt stets weniger als 1 % des Gewichts der Mutter, die Babys der Rüsselbeutler wiegen gar nur fünf Milligramm und sind somit die kleinsten neugeborenen Säugetiere überhaupt. Neugeborene Beutelsäuger haben erst rudimentär entwickelte Organe, lediglich die Vordergliedmaßen sind gut entwickelt, da der Nachwuchs aus eigener Kraft zu den Zitzen der Mutter krabbeln muss.

Viele, aber bei weitem nicht alle Beutelsäuger besitzen einen Beutel, in welchem sich die Zitzen befinden. Die Weibchen mancher Arten haben einen permanenten Beutel, bei anderen wird er erst während der Tragzeit ausgebildet, wieder bei anderen hängen die Jungtiere frei an der Zitze der Mutter, lediglich durch ihr Fell oder Hautfalten verborgen. Neugeborene hängen sich mit dem Mund an die Zitze und bleiben die ersten Lebenswochen fix damit verbunden. Die Säugezeit dauert im Vergleich zu den Höheren Säugetieren länger.

Früher wurde die Gebärweise der Beutelsäuger als eine primitive, im Vergleich zu den Höheren Säugetieren unterentwickelte Methode betrachtet. Auch die Verdrängung mancher Beuteltiere durch eingeschleppte Plazentatiere hat zu diesem Vorurteil beigetragen. Abgesehen davon, dass dieses „Fortschrittsvorurteil“ hin zur Entwicklung des Menschen in der modernen Systematik weitgehend abgelöst wurde und etliche Beuteltierarten ihr Verbreitungsgebiet sehr erfolgreich ausgedehnt haben, bietet die Fortpflanzungsmethode der Beutelsäuger auch Vorteile: zum einen ist die für die Mutter anstrengende Tragzeit verkürzt, zum anderen kann weit schneller als bei Plazentatieren erneut ein Jungtier zur Welt gebracht werden, sollte das früher geborene sterben.

Die Höheren Säugetiere oder Plazentatiere umfassen bei weitem die meisten Arten. Beide deutsche Namen für dieses Taxon sind aber etwas unglücklich gewählt: Das Wort „höher“ spiegelt einen Fortschritt wider, der in der modernen Systematik nicht haltbar ist, und auch manche Beutelsäuger haben eine einfache Plazenta.

Schlüsselmerkmal der Höheren Säugetiere ist der Trophoblast (die äußere Zellschicht eines befruchteten Eis). Diese Schicht stellt eine immunologische Barriere dar und ermöglicht ein langes Heranwachsen im Mutterleib. Beutelsäuger haben keinen Trophoblast, die Tragezeit muss beendet sein, bevor die Immunabwehr der Mutter voll wirksam wird. Die Plazenta der Höheren Säugetiere ist durch das Allantochorion (eine Zottenhaut) charakterisiert. Die Zotten (Villi) sorgen für eine effizientere Ernährung des Keimes.

Die Dauer der Schwangerschaft und die Anzahl der Neugeborenen ist auch von der Lebensweise abhängig. Nesthocker (zum Beispiel Raubtiere oder Nagetiere) haben eher eine kurze Tragzeit und eine hohe Wurfgröße, während Nestflüchter (zum Beispiel Paarhufer und Wale) eine lange Tragzeit und eine niedrige Wurfgröße aufweisen. So beträgt die Trächtigkeitsdauer bei manchen Hamsterarten nur 16 Tage, während sie bei Afrikanischen Elefanten bis zu 25 Monate dauern kann.

Das namensgebende Merkmal der Säugetiere ist, dass das Weibchen die neugeborenen Kinder mit Milch ernährt, einer Nährflüssigkeit, die in Milchdrüsen produziert wird. Diese setzen sich aus äußerlich abgrenzbaren Drüsenkomplexen („Mammarkomplex“) zusammen, von denen jeder meist in einer Warze endet, die Zitze, beim Menschen auch Brustwarze, genannt wird. Eine Ausnahme bilden die Ursäuger, wo die Neugeborenen die Milch direkt von den Milchdrüsenfeldern aus dem Fell der Mutter lecken. Die Anzahl der Drüsenkomplexe ist je nach Art unterschiedlich und hängt mit der durchschnittlichen Wurfgröße zusammen, so haben Menschen oder Pferde nur zwei, Große Tenreks hingegen 24 oder bis zu 32. Die Ernährung mit Milch wird als Säugen beziehungsweise beim Menschen als Stillen bezeichnet und solange durchgeführt, bis das Jungtier fähig ist, feste Nahrung zu sich zu nehmen.

Das Säugen hat große Konsequenzen für Jungtiere und Weibchen. Neugeborene erhalten ohne viel Aufwand eine fett- und nährstoffreiche Nahrung, die ein schnelles Wachstum gewährleistet, sind aber im Gegenzug auf die Präsenz der Mutter angewiesen. Ein Ammenverhalten, das heißt, dass Weibchen auch fremde Kinder säugen, ist nur von wenigen Arten (zum Beispiel Löwen) bekannt. Mit dem Säugen gehen in den meisten Fällen auch eine intensive Brutpflege und ein fürsorgliches Verhältnis zu den Jungen einher. Für die Weibchen wiederum bedeutet das Säugen, viel Zeit und Energie investieren zu müssen.

So unterschiedlich die Gestalt und Lebensweise der Säugetiere ist, so unterschiedlich ist auch ihre Lebenserwartung. Generell leben kleinere Arten weniger lang als größere Arten, die Fledertiere bilden jedoch eine Ausnahme von diesem Muster. Während männliche Breitfuß-Beutelmäuse durchweg im Alter von rund elf Monaten sterben, nachdem sie sich das erste Mal fortgepflanzt haben, können größere Säugerarten mehrere Jahrzehnte alt werden. Von den an Land lebenden Arten kommt keine an das Alter des Menschen heran, bei dem durch die Verbesserung der Medizin mittlerweile ein Höchstalter von 122 Jahren (Jeanne Calment) belegt ist. Neben dem Menschen dürften die Elefanten mit bis zu 80 Jahren die Landsäugetiere mit der höchsten Lebenserwartung sein. Allerdings werden manche Walarten deutlich älter, das bisher älteste bekannte Säugetier war ein Grönlandwal mit 211 Jahren.

Anmerkung: Obwohl auch der Mensch zoologisch zu den Säugetieren gehört, wird er selbst im Folgenden nicht behandelt. Stattdessen wird das Verhältnis des Menschen zu den übrigen Säugetieren thematisiert.

Säugetiere haben die menschliche Geschichte entscheidend mitgeprägt. Schon seit jeher haben Menschen ihr Fleisch gegessen und ihr Fell und ihre Knochen verarbeitet. Sie wurden als Reit- und Arbeitstiere eingesetzt; bis heute werden sie als Milchlieferanten, als Wach- und Labortiere verwendet. Umgekehrt haben auch die Menschen maßgeblichen Einfluss auf die meisten Säugetierarten. Manche Gattungen haben im Gefolge des Menschen ihr Verbreitungsgebiet drastisch vergrößert oder sind als Neozoen in fremden Regionen eingebürgert worden. Vielfach jedoch sind durch Bejagung und Zerstörung des Lebensraumes ihre Populationen eingeschränkt und ihr Verbreitungsgebiet drastisch verringert worden. Eine ganze Reihe von Säugern ist schließlich durch direkten oder indirekten menschlichen Einfluss unwiederbringlich von der Erde verschwunden.

Eine Reihe von Säugetierarten wird vom Menschen wegen ihres, meist wirtschaftlichen, Nutzens gehalten. Zu diesem Zweck domestizierte Tiere werden als Nutztiere bezeichnet. Es werden darüber hinaus Wildtiere gejagt oder halbdomestizierte Tiere im Freiland gehalten und später gefangen (Beispiele sind Hutewälder oder die Rinder- und Pferdezucht in Amerika).




Aus vielen der oben genannten Gründe beschränkte sich der Mensch nicht nur auf die Jagd, sondern versuchte auch, gewisse Tierarten in seiner Nähe zu halten und nachzuzüchten. Die Domestizierung von Nutztieren begann zumindest vor rund 10.000 bis 15.000 Jahren, beim Haushund deuten genetische Studien allerdings an, dass dieser Prozess schon vor mehr als 100.000 Jahren begonnen haben könnte. Im achten Jahrtausend v. Chr. dürften bereits Wildziege, Wildschaf und Wildrind, etwas später auch das Wildschwein zu Hausziege, Hausschaf, Hausrind und Hausschwein domestiziert worden sein. Nutztiere dienten zunächst vorwiegend als Nahrungsmittellieferanten, später wurden dann auch Tiere zur Arbeitstätigkeit eingesetzt, so seit rund 3000 v. Chr. das Hauspferd und das Lama. Der Prozess der Domestizierung verlief vielschichtig, genetische Studien deuten an, dass bei vielen Haustieren in unterschiedlichen Regionen dieser Schritt mehrmals unabhängig voneinander vonstattenging. Weitere domestizierte Säugetiere sind Rentier, Dromedar, Hauskatze, Frettchen, Esel, Farbmaus, Farbratte, Goldhamster, Kaninchen und Meerschweinchen.

Als Schädlinge werden Tierarten bezeichnet, die dem Menschen gegenüber Schaden anrichten. Der Begriff ist abhängig von Wertvorstellungen und vor allem der wirtschaftlichen Perspektive und daher kein Begriff der Biologie.

Eine Reihe von Säugetieren gilt als Landwirtschafts- oder Nahrungsmittelschädlinge, das heißt, sie ernähren sich entweder direkt in den zur Nahrungsmittelproduktion genutzten Gebieten oder an Aufbewahrungsorten von den vom Menschen produzierten Nahrungsmitteln. Durch die großflächige Einführung von Agrarflächen kommt es zu einem Überangebot an Nahrung für manche Tierarten, das in deren starker Vermehrung und somit weiterer Schädigung resultiert. Vor allem in Entwicklungsländern lässt sich dieser Trend beobachten. Zu den hierzulande bekanntesten Nahrungsmittelschädlingen zählen Mäuse, insbesondere die Hausmaus und Ratten wie die Haus- oder Wanderratte, die sich als Kulturfolger dem Menschen angeschlossen haben und eine weltweite Verbreitung erlangt haben. Einige Tiere (darunter Flughunde und zahlreiche Nagetierarten) ernähren sich direkt von den Feldfrüchten, andere sorgen durch ihre unterirdische Lebensweise für Schäden an den Wurzeln. Die Viehwirtschaft sieht in fleischfressenden Tieren, vor allem Raubtieren eine Nahrungskonkurrenz, zumindest zwei Arten, der Falklandfuchs und der Beutelwolf sind durch Bejagung ausgestorben. In analoger Weise sieht die Fischerei Robben und andere fischfressende Säuger als wirtschaftliche Gefahr und verfolgt sie.

Das Ausmaß der tatsächlichen Bedrohung, die als „Schädlinge“ bezeichnete Tiere anrichten, ist ungewiss und dürfte oft übertrieben dargestellt werden. Häufig ist der Mensch die Hauptursache dafür, indem er massiv in den natürlichen Lebensraum der Tiere eingreift. Durch die Umwandlung der Habitate in landwirtschaftlich genutzte Flächen und die Verringerung des Nahrungsangebotes werden viele Arten gezwungen, sich neue Nahrungsquellen zu erschließen. Diese stehen dann in Konkurrenz zu den wirtschaftlichen Interessen und leiten die Verfolgung ein. Trotzdem wird mit exzessiven Bejagungen, Vergiftungen und mit anderen Methoden Jagd auf diese „Schädlinge“ gemacht, was sich oft fatal auf die Population auswirkt.

Menschen sind manchmal auch direkten Bedrohungen durch die Säugetiere ausgesetzt. Im Bewusstsein verankert sind dabei vorwiegend die Fälle der großen menschenfressenden Raubtiere, wobei insbesondere der Tiger einen Ruf als „Menschenfresser“ genießt. Tötungen durch Raubtierbisse beschränken sich jedoch auf wenige Einzelfälle im Jahr. Ungleich gefährlicher sind Säugetiere jedoch als Krankheitsüberträger. So sterben jedes Jahr 40.000 bis 70.000 Menschen an der Tollwut, die meisten davon in unterentwickelten Ländern. Hauptübertragungsursache ist der Biss durch infizierte Tiere wie Hunde, Katzen, Dachse, Waschbären und Fledermäuse. Eine weitere berüchtigte Krankheit ist die Pest, die durch auf Hausratten und anderen Nagetieren parasitierende Flöhe, in seltenen Fällen auch direkt übertragen wird. Pest-Epidemien und -Pandemien kosteten Millionen Menschen das Leben, bei der als Schwarzer Tod bekannten Pandemie Mitte des 14. Jahrhunderts starben schätzungsweise ein Drittel der Menschen in Europa.

Viele Säugetiere spielen in der Kulturgeschichte eine bedeutende Rolle. Auffallend große, starke oder gefährliche Tiere dienen als Wappentiere, als Totem- oder Clansymbole. Als „Heilige Tiere“ gelten manche Arten als Manifestationen von Göttern und genossen besonderen Schutz, so heilige Kühe und Hanuman-Languren in Indien oder Katzen und Schakale im alten Ägypten. Auf der anderen Seite wurden manche Säugetiere als Vertreter dämonischer Mächte gesehen, so Fledermäuse oder Katzen. Stereotype Vorstellungen von Eigenschaften bestimmter Tierarten, wie der sture Esel oder der schlaue Fuchs finden sich in zahllosen Erzählungen und Märchen und prägen zum Teil bis heute den Schimpfwortschatz.

Durch vielfältige Eingriffe in die Natur ist der Mensch für den Populationsrückgang oder das Aussterben vieler Säugetierarten verantwortlich. Inwieweit die Bejagung für das Aussterben zahlreicher Großsäuger am Ende des Pleistozäns (vor 50.000 bis 10.000 Jahren) schuld ist, ist umstritten, dieses Aussterben korreliert zumindest teilweise mit der weltweiten Ausbreitung des Menschen (siehe dazu auch den Punkt unter Entwicklungsgeschichte). Aus Berichten und Darstellungen lässt sich zumindest ein deutlicher Schwund des Verbreitungsgebietes für zahlreiche Spezies seit der Antike ableiten. Auch die heutige Situation ist für viele Säugetierarten besorgniserregend. So kommt eine unter der Federführung der International Union for Conservation of Nature (IUCN) stehende Kommission aus rund 1.700 Wissenschaftlern aus 130 Ländern zu dem Ergebnis, dass heute mindestens 20–25 % – unter Umständen aber bis zu 36 % – aller Land- und Meeressäugetierarten vom Aussterben bedroht sind. Die IUCN listet 514 Arten, also rund 10 %, als stark bedroht ("critically endangered") oder bedroht ("endangered"), insgesamt sind mindestens 1.141 der derzeit 5.487 rezenten Säugetierarten akut bedroht. Vier Arten, das Przewalski-Pferd, die Saudi-Gazelle, die Säbelantilope und der Schwarzfußiltis, gelten als in freier Wildbahn ausgestorben ("extinct in the wild"), das heißt, es gibt nur mehr die Bestände in menschlichen Zuchtprogrammen. Die Gründe für die Gefährdung zahlreicher Arten liegen hauptsächlich im zunehmenden Verlust des Lebensraumes durch Umwandlung in landwirtschaftlich genutzte Gebiete und Siedlungen, in der Umweltverschmutzung und in der Bejagung, da man viele Arten als nützlich oder schädlich ansieht. Ein weiterer Faktor ist die Schädigung des natürlichen Gleichgewichts durch die absichtliche oder unbewusste Einschleppung von Neozoen. Die Verfolgung durch verwilderte Hauskatzen und Haushunde sowie die Nahrungskonkurrenz durch Mäuse, Ratten, Hasen und andere stellen insbesondere in Regionen, wo diese Arten natürlicherweise nicht heimisch waren (wie zum Beispiel Australien oder viele Inseln), ein großes Problem dar.

Die oben genannten Gründe haben dazu geführt, dass laut IUCN 73 Säugetierarten in den letzten Jahrhunderten ausgestorben sind, dazu zählen der Schweinsfuß-Nasenbeutler, vier Känguruarten, der Beutelwolf, der Falklandfuchs, drei Gazellenarten, der Blaubock, die Stellersche Seekuh, zwölf Fledertierarten und zahlreiche Nagetiere wie etliche Baumratten und Riesenhutias. Es steht zu erwarten, dass diese Liste in den nächsten Jahren noch länger werden wird.

Die Säugetiere sind wahrscheinlich – entgegen anders lautenden Theorien, die Mitte des 20. Jahrhunderts verbreitet waren – eine monophyletische Gruppe: Sie stammen alle von einem gemeinsamen Vorfahren ab und umfassen auch alle Nachkommen dieses Vorfahren. Die drei Untergruppen, Ursäuger, Beutelsäuger und Höhere Säugetiere, sind ebenfalls jeweils monophyletische Taxa. Die meisten Systematiken fassen die Beutel- und Höheren Säuger zum Taxon Theria zusammen und stellen dieses den Ursäugern gegenüber. Einige Forscher vertreten aber die Ansicht, die Ursäuger hätten sich aus den Beutelsäugern entwickelt.

Ungleich unübersichtlicher wird das Bild, wenn fossile Taxa in den Stammbaum eingebunden werden. Neben den üblichen Meinungsunterschieden der Wissenschaftler kommt hinzu, dass von zahlreichen Gattungen lediglich Zähne und Kieferteile gefunden wurden. Die detaillierte Untersuchung der Zähne ist daher eines der Schlüsselkriterien zur Bestimmung der Evolution der Säugetiere.

Unstrittig ist, dass sich die Säugetiere aus den Synapsiden entwickelt haben, einer Reptiliengruppe, die durch ein einzelnes Schädelfenster charakterisiert war und ihre Blütezeit im Perm-Zeitalter hatte. Innerhalb der Synapsiden entwickelten sich die Therapsiden, die sogenannten „Säugerähnlichen Reptilien“, die bereits einige der Säugermerkmale wie ein differenziertes Gebiss und möglicherweise Körperbehaarung aufwiesen. Eine Gruppe der Therapsiden waren die Cynodontia, die unter anderem durch ein vergrößertes Gehirn und eine spezielle Kieferform gekennzeichnet waren. Die Säugetiere und ihre näheren Verwandten werden im Taxon der Eucynodontia zusammengefasst, deren bekanntester Vertreter "Cynognathus" war. Als Schwestertaxon der Säuger gelten entweder die Tritheledontidae, eine Gruppe sehr kleiner, fleischfressender Tiere oder die Tritylodontidae, eine Gruppe bis zu 1 Meter langer Pflanzenfresser. Für jede der beiden Gruppen sprechen gewisse anatomische Merkmale, die Mehrheit der Forscher gibt jedoch den Tritheledontidae den Vorzug.

Die Nicht-Säugetiere innerhalb der Therapsiden wurden nach und nach von den Dinosauriern verdrängt, die letzten starben in der Unterkreide aus.

Umstritten ist, welches Tier als das älteste Säugetier zu bewerten ist. Einige Tiere weisen im Bau des Ohres, des Unterkiefers, des Kiefergelenkes und der Zähne einen Übergangsstatus zwischen Reptilien und Säugern auf, manche Forscher bezeichnen sie deshalb als Mammaliaformes, also „Säugerartige“ oder Proto-Mammalia und ordnen sie noch nicht den Säugetieren im eigentlichen Sinn ("sensu stricto") zu, andere fassen die Säuger weiter ("sensu lato") und rechnen diese bereits dazu.


Die Säugetiere im engeren Sinn (Mammalia sensu stricto), in Abgrenzung zu den Säugetieren im weiteren Sinn beziehungsweise Mammaliaformes (siehe oben), werden definiert als die Gruppe, die den letzten gemeinsamen Vorfahren aller heutigen Säugetiere sowie dessen Nachkommen umfasst. Dieses Taxon ist zumindest seit dem mittleren Jura belegt, die Entwicklungsgeschichte innerhalb dieser Gruppe ist jedoch in einem hohen Ausmaß umstritten.


Generell waren die Säugetiere des Mesozoikums klein, die meisten erreichten nur die Größe von Mäusen oder Ratten. Aus den Zähnen schließt man bei den meisten Arten auf eine aus Insekten und anderen Wirbellosen bestehende Nahrung, aus der Form des Gehirns und der Sinnesorgane auf eine hauptsächlich nachtaktive Lebensweise. Es bleibt die Frage, warum der Großteil der mesozoischen Säuger in Größe, Körperbau und Lebensweise relativ einheitlich blieb, zumal es in einem entwicklungsgeschichtlich sehr kurzen Zeitraum (rund 5 Millionen Jahre) nach dem Beginn des Känozoikums zu einer enormen Radiation hinsichtlich der Größe und Ernährungsweise kam. Generell wird diese Frage mit der Konkurrenz durch die Dinosaurier beantwortet, die, solange sie existierten, durch den ausgeübten Selektionsdruck größere Säuger verhinderten. Diese Sichtweise wird manchmal in Frage gestellt: Aufgrund des enormen Größenunterschiedes und der unterschiedlichen Lebensweise mit den Dinosauriern, die vermutlich tagaktiv waren, hätte es zumindest eine Reihe mittelgroßer Säuger geben können. Daher wurden verschiedene physiologische Einschränkungen postuliert, zum Beispiel eine mangelnde Fähigkeit zur Kühlung der Körpertemperatur oder die noch nicht völlig ausgereiften Kau- und Verdauungsapparate.

In jüngerer Zeit gab es allerdings einige neue Funde, die auf eine höhere Spezialisierung der mesozoischen Säuger hinweisen. So war "Castorocauda" zumindest teilweise wasserbewohnend, "Volaticotherium" war mit Gleitmembranen ausgestattet und "Fruitafossor" zeigt eine an Ameisenbären erinnernde Anpassung an eine insektenfressende Lebensweise. "Repenomamus" schließlich, der in der Unterkreide in China lebte, erreichte eine Länge von über 1 Meter und sein Gewicht wird auf 12 bis 14 Kilogramm geschätzt. Er ist der bislang größte aus dem Mesozoikum bekannte Säuger und hat sich auch von kleinen Dinosauriern ernährt.

Die Beutelsäuger waren, abgesehen von vereinzelten Funden in Ostasien, auf Nordamerika beschränkt. Zu den ältesten heute noch bestehenden Gruppen gehören die Beutelratten, deren Vorfahren schon aus dieser Zeit bekannt sind.

Die Höheren Säugetiere spalteten sich in die heute durch molekulargenetische Untersuchungen bestimmten Überordnungen (Nebengelenktiere, Afrotheria, Laurasiatheria, Euarchontoglires) auf, was durch tektonische Verschiebungen, unter anderem das Auseinanderbrechen Gondwanas gefördert wurde. Diese Aufspaltungen werden allerdings hauptsächlich durch molekulargenetische Berechnungen belegt, Fossilienfunde von Höheren Säugetieren aus der Oberkreide sind sehr selten und bislang nur aus Nordamerika und Ostasien belegt. Zu den bekanntesten Gattungen dieser Epoche zählen "Asioryctes", die Leptictida, die möglicherweise Vorfahren der Insektenfresser sind, die Zalambdalestidae (mögliche Vorfahren der Nagetiere), die Zhelestidae (mögliche Vorfahren der „Huftiere“) und "Cimolestes" (eventuell ein Urahn der Raubtiere). Generell ist aber die Zuordnung zu heutigen Taxa umstritten, zweifelsfrei mit heutigen Arten verwandte Säugetiere traten erst im Paläozän auf.

Mit Ausnahme der Multituberculata dürften am Ende der Kreidezeit die meisten der oben beschriebenen Seitenlinien der Säugetiere ausgestorben gewesen sein.

Mit dem Aussterben der Dinosaurier wurden viele ökologische Nischen frei, die von einer Vielzahl neu entstehender Säugetiergruppen besetzt wurden. Im Verlauf des Känozoikums entwickelten sich die Säugetiere zu der dominanten Wirbeltiergruppe auf dem Land. Es bildeten sich die heutigen Ordnungen heraus, wobei die Entwicklungsgeschichte keineswegs geradlinig verlief, sondern durch evolutionäre Sackgassen, Verdrängungsprozesse und wieder gänzlich ausgestorbene Säugetiergruppen geprägt war. Die Entwicklungslinien in manchen Gruppen (zum Beispiel bei Pferden oder Rüsseltieren) sind dabei relativ gut durch Fossilienfunde belegt und erforscht. Eine besondere Rolle nahm Südamerika ein, das während der längsten Zeit des Känozoikums von anderen Kontinenten getrennt war. Durch die Insellage drangen viele Arten in ökologische Nischen vor und es entwickelte sich eine einzigartige Fauna, unter anderem mit Sparassodonta („Beutelhyänen“), einer Gruppe fleischfressender Beuteltiere, mit den Paucituberculata, einer formenreichen Beuteltiergruppe, die heute noch in den Mausopossums weiterlebt und mit den Südamerikanischen Huftieren (Meridiungulata). Nach Entstehen der mittelamerikanischen Landbrücke drangen Säuger aus dem Norden vor und verdrängten die einheimischen Arten größtenteils.

Die meisten Säugetierordnungen sind seit dem Eozän belegt, darunter auch die Vorfahren der wohl spezialisiertesten Gruppen, der Fledertiere und Wale. Im gleichen Zeitabschnitt bildeten sich die ersten riesenhaften Formen wie "Uintatherium"; diese Entwicklung gipfelte in "Paraceratherium" (auch unter den Namen "Baluchitherium" oder "Indricotherium" bekannt), dem mit 5,5 Metern Schulterhöhe und 10 bis 15 Tonnen Gewicht größten bekannten Landsäugetier.

Ihre größte Artenvielfalt erreichten die Säuger im Miozän; seither verschlechterten sich die Klimabedingungen kontinuierlich, bis hin zu den Eiszeiten des Pleistozän. Die klimatischen Verschiebungen, verbunden mit den Einflüssen des Menschen, sorgen seither für einen Rückgang der Artenvielfalt.

Am Ende des Pleistozäns (vor 50.000 bis 10.000 Jahren) kam es weltweit zu einem Massenaussterben von großen Säugetieren. Mit Ausnahme Afrikas und des südlichen Asiens starben alle Arten mit über 1000 Kilogramm Gewicht und 80 % aller Arten mit 100 bis 1000 Kilogramm Gewicht aus. In Australien fand dieser Prozess vor rund 51.000 bis 38.000 Jahren statt, hier verschwanden unter anderem "Diprotodons" (nashorngroße Beuteltiere), Beutellöwen ("Thylacoleo carnifex"), und bis zu 3 Meter hohe Riesenkängurus (Gattung "Procoptodon"). In Eurasien erstreckte sich dieser Vorgang über einen längeren Zeitraum, von vor 50.000 bis 10.000 Jahre, und erreichte mit dem Ende der letzten Kaltzeit seinen Höhepunkt. Zu den in Europa um 10.000 vor Christus ausgestorbenen Tieren zählen unter anderem das Wollhaarmammut ("Mammuthus primigenius"), das Wollnashorn ("Coelodonta antiquitatis"), der Riesenhirsch ("Megaloceros giganteus"), das Steppenwisent ("Bos priscus"), der Höhlenlöwe ("Panthera spelaea") und der Höhlenbär ("Ursus spelaeus"). In Amerika lag das Aussterben in einem engen Zeitrahmen (vor rund 11.000 bis 8.000 Jahren), hier verschwanden unter anderem die Mammuts, das Amerikanische Mastodon und andere Rüsseltiere, Säbelzahnkatzen, Riesenfaultiere und Riesengürteltiere (Glyptodontidae).

Inwieweit klimatische Veränderungen oder die Bejagung durch den Menschen (Overkill-Hypothese) die Hauptschuld dafür tragen, ist immer noch umstritten. Für die Bejagung sprechen die Tatsachen, dass der Zeitpunkt des Aussterbens zumindest zum Teil mit der weltweiten Ausbreitung des Menschen übereinstimmt und dass bei keiner der früheren Aussterbephasen eine derartige Einschränkung hinsichtlich der Größe beobachtet werden konnte. Auch müssten die klimatischen Vorgänge am Ende der letzten Kaltzeit eher zu einer Erhöhung der Artenanzahl beigetragen haben, wie sie meist in wärmeren Perioden beobachtet werden kann. Vertreter der Bejagungshypothese führen auch einen analogen Vorgang auf Inseln, die erst später besiedelt wurden, an. So sind auf Madagaskar, wo erst seit rund 1500 Jahren Menschen leben, in den darauf folgenden Jahrhunderten unter anderem die dortigen Flusspferde und zahlreiche große Primatenarten verschwunden, darunter die Riesenlemuren "Megaladapis". Gegner der Bejagungshypothese behaupten, die primitiven Jagdmethoden der frühen Menschen hätten keinen so großen Einfluss auf die Populationsgröße haben können, und verweisen auf Afrika, wo es schon viel länger Menschen gegeben hat und wo es zu keinem nennenswerten Massenaussterben gekommen ist. Auch seien die klimatischen Veränderungen dermaßen komplex gewesen, dass eine Vielzahl von Faktoren berücksichtigt werden müsste.

In jüngerer Zeit mehren sich die Thesen, dass eine Vermischung beider Faktoren die Schuld am Massenaussterben trägt. So sei für die durch klimatische Veränderungen bereits in Mitleidenschaft gezogenen Populationen die Jagd der ausschlaggebende Punkt für die Ausrottung gewesen. Auch ökologische Faktoren können eine Rolle gespielt haben: So führte die Dezimierung großer Grasfresser zur Ausbreitung von Wäldern, was sich fatal auf die noch vorhandenen Populationen auswirkte. Andere Forscher geben auch den ausgedehnten Brandrodungen eine Teilschuld.

In dieser Diskussion spielt aber nicht nur der rein wissenschaftliche Aspekt eine Rolle, sondern auch die anthropologische Komponente, je nachdem ob man in diesem Massenaussterben das letzte einer langen Reihe von natürlichen Aussterbevorgängen in der Natur sieht oder den ersten von vielen zerstörerischen Eingriffen des Menschen in seine Umwelt.

Anschließend ein etwas vereinfachtes Kladogramm der Landwirbeltiere, gefolgt von ausführlicheren Darstellungen über eventuelle Unsicherheiten und Streitpunkte.

Die Säugetiere werden in drei Unterklassen mit rund 25 bis 30 Ordnungen unterteilt, die ihrerseits bei den Beutelsäugern und höheren Säugetieren noch einmal zwei beziehungsweise vier übergeordneten Gruppen zugeteilt werden können. Eine detailliertere Systematik mit allen Familien findet sich unter Systematik der Säugetiere.

Einige Bemerkungen zu dieser Systematik:

Der unter Systematik gezeigte Stammbaum stützt sich teilweise auf molekulargenetische Analysen. Da diese bei ausgestorbenen Tiergruppen nicht möglich sind, lassen sie sich nur schwer in die Systematik einordnen. Existierende Systeme, wie das von Malcolm C. McKenna and Susan K. Bell, die sowohl lebende als auch ausgestorbene Säugerordnungen enthalten, widersprechen sich teilweise mit der hier gewählten Systematik. Deshalb werden hier die ausgestorbenen Säugetierordnungen der Beutelsäuger (Metatheria) und der Höheren Säugetiere (Eutheria) extra aufgelistet.

Ausgestorbene Ordnungen der Beutelsäuger:

Ausgestorbene Ordnungen der Höheren Säugetiere:

Ältere Säugetierordnungen, die weder zu Beuteltieren noch zu Höheren Säugern gehören, sind weiter oben bei den Säugetieren im engeren Sinne aufgeführt.




</doc>
<doc id="7908" url="https://de.wikipedia.org/wiki?curid=7908" title="Papst">
Papst

Papst (von ‚ kindl. Anrede "Papa"; Kirchenlatein '; , neuhochdeutsch "Babst") ist der deutschsprachige geistliche Titel für den Bischof von Rom"' als Oberhaupt der römisch-katholischen Kirche. Weitere Bezeichnungen sind u. a. "Heiliger Vater", "Pontifex Maximus".

Der Amtsinhaber Jorge Mario Kardinal Bergoglio SJ mit dem Papstnamen Franziskus wurde im Konklave am 13. März 2013 zum 266. Papst gewählt. Sein Vorgänger Benedikt XVI. wird seit seinem Ausscheiden aus dem Amt als "Papa emeritus" (emeritierter Papst) bezeichnet.

Das Amt des Papstes, der bischöfliche Stuhl des Bistums Rom, ist als Heiliger Stuhl bekannt. Er ist ein nichtstaatliches Völkerrechtssubjekt und vertritt in internationalen Beziehungen den Staat Vatikanstadt und die gesamte Kirche. Neben dem Papst als personale Repräsentation gehören zum Heiligen Stuhl auch die Verwaltungseinrichtungen der römischen Kurie.

Als absoluter Monarch ist der Papst Souverän des Staates Vatikanstadt und besitzt die gesetzgebende, ausführende und richterliche Gewalt.

Die Kathedralkirche des Bistums Rom und somit Bischofssitz des Papstes ist die Lateranbasilika. Residenz des Papstes ist seit 1871 der Apostolische Palast.

Nach Lehre der römisch-katholischen Kirche und einiger anderer katholischer Kirchen ist der jeweils amtierende Papst Nachfolger des Apostels Petrus, der nach der Überlieferung um das Jahr 67 in Rom den Märtyrertod erlitt. Der Tradition zufolge war Petrus erster Bischof von Rom. Die dogmatische Konstitution des Zweiten Vatikanischen Konzils über die Kirche, "Lumen gentium", bezeichnet den Papst als „das immerwährende und sichtbare Prinzip und Fundament für die Einheit der Vielheit sowohl von Bischöfen als auch von Gläubigen“. Der Anspruch des Petrus und seines Nachfolgers auf Leitungsgewalt wird aus mehreren Bibelstellen abgeleitet, vor allem aus dem „Felsenwort“ und dem „Schlüsselwort“ , auch von („stärke deine Brüder“) und („weide meine Lämmer“).

Umstritten ist, ob der erste Clemensbrief aus dem Jahre 98 – nach manchen aus dem Jahre 69 – bereits eine Vorrangstellung der Gemeinde von Rom dokumentiert oder als brüderliche Ermahnung unter Gleichberechtigten anzusehen ist. In diesem Brief an die Gemeinde von Korinth fordert die Gemeinde von Rom von den Korinthern die Rücknahme von abgesetzten Presbytern. Der Brief nimmt Bezug auf das Martyrium der Apostel Petrus und Paulus in Rom.

In der römisch-katholischen Kirche stammt die erste bekannte Verbindung der Bezeichnung "papa" mit dem Bischof von Rom erst aus der Zeit des Marcellinus († 304), der in der Grabinschrift des Diakons Severus so bezeichnet wird. Bischof Siricius (Amtszeit 385–399) trug als erster die Eigenbezeichnung "papa". Als ausschließliche Amtsbezeichnung für den Bischof von Rom wird der Begriff von Gregor I. von 590 bis 604 gesetzlich festgeschrieben.

Spätestens ab dem 2. Jahrhundert war im griechischen Orient "Papa" allgemein eine Ehrenbezeichnung für christliche Würdenträger. Das Oberhaupt der koptischen Kirche, die seit dem Konzil von Chalcedon 451 nicht mehr in Gemeinschaft mit der griechischen oder lateinischen Kirche steht, trug spätestens seit Heraclas (232–248) ebenfalls den Titel "Papa"; im Deutschen meist als Papst oder Patriarch von Alexandria übertragen.

Seit der Amtszeit von Leo I. (440–461) führt der römische Papst die Bezeichnung Pontifex Maximus, die seit dem 5. Jahrhundert v. Chr. in der römischen Verwaltung verwendet wurde und die später bis zu Kaiser Gratian der Kaiser des Römischen Reichs als oberster Priester der römischen Religionen trug. Etymologien für die Bezeichnung „Pontifex“ sind unter anderem „Brückenbauer“ oder „Pfadbahner“.

Im Mittelalter gab es wiederholt gleichzeitig mehrere Päpste, da zu Lebzeiten eines bereits kanonisch gewählten Papstes ein Gegenpapst erhoben wurde. Ursachen waren, dass sich das Kardinalskollegium spaltete und Kaiser oder stadtrömische Adelsfamilien in die Papstwahl eingriffen. Auch war die Exklusive eine Eingriffsmöglichkeit katholischer Monarchen in die Papstwahl. Solche Eingriffe sind seit Pius X. unter Androhung der Exkommunikation verboten. Vor dem 13. Jahrhundert residierte der Papst im Lateran. Im 15. Jahrhundert gewann der Konziliarismus an Auftrieb, der Konzilien höhere Autorität zusprach als päpstlichen Entscheidungen, aber bald zurückgedrängt wurde.

Die Titel des Papstes sind nach dem Annuario Pontificio, dem Jahrbuch des Heiligen Stuhls, die folgenden:


Der Titel "Patriarch des Abendlandes" () beziehungsweise "Patriarch des Westens" wurde von den Päpsten nach dem Konzil von Chalzedon 451 angenommen und 1500 Jahre lang geführt. Das "Patriarchat des Abendlandes" war das einzige der fünf altkirchlichen Patriarchate, das im Weströmischen Reich lag. Aus ihm entwickelte sich die Lateinische Kirche. Papst Benedikt XVI. legte den Titel nach seiner Papstwahl nieder, er wurde daher im Annuario Pontificio des Jahres 2006 aus der offiziellen Papsttitulatur entfernt. Unabhängig vom Titel eines Patriarchen wird der Papst von einigen Kanonisten als "Patriarch der Westkirche" betrachtet, aus dem sich seine Befugnisse und die Jurisdiktionsgewalt in der Lateinischen Kirche ergeben.

Zusätzlich zu dieser offiziellen Titulatur wird der Papst auch als Pontifex Maximus (in Inschriften oft als "P. M." oder "Pont. Max." abgekürzt) oder auch als („Bischof der katholischen Kirche“) bezeichnet.

Dokumente werden vom Papst gewöhnlich mit seinem Papstnamen unterzeichnet, wobei dem eigentlichen (in der Regel latinisierten) Namen direkt die Abkürzung "PP." (für „papa“) folgt mit gegebenenfalls angehängter Ordnungszahl: „Ioannes Paulus PP. II.“, „Benedictus PP. XVI.“ und „Franciscus PP.“.

Als Anrede des Papstes benutzen Katholiken meist "Heiliger Vater". Dem diplomatischen Protokoll entspricht die Bezeichnung beziehungsweise Anrede des Papstes als Heiligkeit oder als Heiliger Vater.

Dem Papst kommt im Recht der katholischen Kirche die zentrale Rolle zu. Die umfassenden Kompetenzen sind in den canones 331 bis 335 des kirchlichen Gesetzbuches (CIC) bzw. in den gleichlautenden Normen des Gesetzbuches für die mit Rom unierten katholischen Ostkirchen (CCEO) normiert.

Die Bischöfe von Rom verstehen sich seit ältester Zeit als Nachfolger des Apostels Petrus und Inhaber des Petrusdienstes gemäß "Matthäus 16,18":

Gemäß () lebt im Papst als Bischof von Rom das von Jesus Christus an Simon Petrus übertragene Amt fort. Der Papst hat nicht nur einen Ehrenvorrang vor den übrigen Bischöfen, er ist vielmehr Haupt des Bischofskollegiums und als solcher mit wirklichen Kompetenzen über die Gesamtkirche ausgestattet. Ein Ehrenvorrang der römischen Bischöfe „in der Liebe“ wird prinzipiell von vielen Kirchen und Konfessionen anerkannt und im Can. 6 des Konzils von Nicaea als Gewohnheit bezeichnet. Seine dogmatische und rechtliche Tragweite ist jedoch von Anfang an Gegenstand innerchristlicher Kontroversen. Die Lehre, dass die Bischöfe von Rom als Nachfolger des Petrus exklusive Vorrechte, nämlich den Jurisdiktionsprimat und bei Lehraussagen ("ex cathedra") Unfehlbarkeit genießen, wird nur von Gliedern der katholischen Kirchen, die den Papst als Oberhaupt anerkennen, geglaubt.

Der Primatsanspruch des Papstes wird dogmatisch aus dem Petruswort in Matthäus 16 hergeleitet. Als Nachfolger des Apostels Petrus, irdischer Stellvertreter Jesu Christi und Hirte der Universalkirche verfügt der Papst in der römisch-katholischen Kirche „über höchste, volle, unmittelbare und universale ordentliche Gewalt, die er immer frei ausüben kann“ (). Näher bestimmt wird diese Gewalt als:

Der Papst ist Träger der Höchstgewalt ("potestas suprema"), das heißt, dass es in der Kirche keine Gewalt gibt, die ihm rechtlich übergeordnet ist. In diesem Zusammenhang stellt sich die Frage, wie mit ungeeigneten, beispielsweise häretischen Päpsten umgegangen werden soll. Mittelalterliche Kirchenrechtler wie Huguccio waren der Überzeugung, ein Papst gehe automatisch ("ipso facto") des Amtes verlustig, wenn er offenkundig "a fide devius" („vom Glauben abgekommen“) sei. Gegebenenfalls stellt ein Konzil oder auch nur das Kardinalskonsistorium den Glaubensabfall fest. Diese Konzeption ist nicht vereinbar mit der neuzeitlichen Entwicklung der Lehre von Papst und Kirche, vor allem seit den Dogmen des Ersten Vatikanischen Konzils. Einen häretischen Papst kann es gemäß diesem Konzil nicht mehr geben, weil seine Lehrsätze irreformabel sind, wenn sie feierlich – also nach katholischer Überzeugung als im Glauben verpflichtend – geäußert werden: Der Papst müsste von Amts wegen feierlich einen irrigen Satz lehren, was er wegen des bewahrenden Beistands des Heiligen Geistes aber nicht kann. Eine kirchenrechtliche Regelung ist daher für solche Fälle in der katholischen Kirche nicht vorgesehen, weil sie nicht vorkommen können.

Der Begriff der Vollgewalt ("potestas plena") bezeichnet eine Gewaltenfülle in materieller und formeller Hinsicht (→ "plenitudo potestatis"). Materiell bedeutet sie, dass sich die Primatialgewalt des Papstes nicht auf bestimmte Sachgebiete beschränkt, sondern sich auf alle Angelegenheiten der Kirche erstreckt, also auf die klassischen Bereiche des Lehrens, Heiligens und Leitens. In formaler Hinsicht bedeutet Vollgewalt, dass die Amtsgewalt des Papstes Exekutive, Legislative und Judikative umfasst. So ist der Papst oberster Gesetzgeber der Kirche und nur an das göttliche Recht ("ius divinum"), welches als solches unveränderlich ist, gebunden. Bezüglich rein kirchlichen Rechts ("ius mere ecclesiasticum") kann er jederzeit neue Kanones erlassen, alte streichen oder von ihnen befreien (dispensieren).

Der Papst ist oberster Richter der Kirche und selbst keinem kirchlichen Gericht unterworfen ("prima sedes a nemine iudicatur"). Urteile des Papstes sind demgemäß stets letztinstanzlich und unanfechtbar. Mit Ausnahme bestimmter Fälle () ist die Rechtsprechung an entsprechende Gerichte der Kurie delegiert. Als oberster Verwalter der Kirche ist der Papst mit der Aufsicht über das ganze kirchliche Leben betraut. Dabei bedient er sich vor allem seiner Kurie, der Nuntien und besonderer Visitatoren. Zudem besteht für jede Bischofskonferenz die Pflicht, alle fünf Jahre in Rom über das kirchliche Leben auf dem Gebiet der Konferenz Bericht zu erstatten (Ad-limina-Besuch).

Die Primatialgewalt ist unmittelbar ("potestas immediata"). Das bedeutet, dass sich der Papst ohne Einschaltung eines Zwischenorgans jeder Sache annehmen kann. Er kann so unter Ausschluss aller (originär zuständigen) Instanzen eine Sache an sich ziehen und sich eine bestimmte Entscheidung vorbehalten ("affectio papalis"). Umgekehrt kann sich jeder Gläubige direkt an den Papst wenden, ohne einen bestimmten Instanzenweg einhalten zu müssen (). Die "affectio papalis" wird freilich nur subsidiär angewandt, damit die Kirchenverfassung nicht ausgehöhlt wird. Die Unmittelbarkeit der päpstlichen Gewalt ist durch die auf göttlichem Recht beruhende Eigenständigkeit des Bischofsamts begrenzt. Die Amtsgewalt des Papstes tritt damit in der Regel nicht in Konkurrenz zur Amtsgewalt der Bischöfe.

Universalgewalt ("potestas universalis") bedeutet, dass sich die Primatialgewalt auf die ganze Kirche, also auf alle Teilkirchen (z. B. Bistümer) und kirchlichen Teilgemeinschaften bezieht. Der Papst ist also „Universalbischof der katholischen Kirche“, wobei zu berücksichtigen ist, wie die Unmittelbarkeit der päpstlichen Gewalt verstanden wird.

Die Bezeichnung der Primatialgewalt als wirkliche bischöfliche Gewalt ("potestas vere episcopalis") geht vor allem auf Bestrebungen zurück, die Primatialgewalt deutlich von der weltlichen Gewalt für das äußere Kirchenregiment zu unterscheiden und sie so gleichzeitig dem weltlichen Einfluss zu entziehen. Die Primatialgewalt ist also eine geistliche Gewalt, was heute nicht mehr in Frage steht.

Dass der Papst von seiner Primatialgewalt frei Gebrauch machen kann, bedeutet, dass er hierbei von keiner kirchlichen Instanz gehindert werden kann.

Als "Bischof von Rom" ist der Papst Leiter der römischen Ortskirche. Die Führung der Amtsgeschäfte ist weitgehend an den Kardinalvikar für das Bistum Rom delegiert. Dogmatisch und kirchenrechtlich ungeklärt ist die Frage, ob die Personalunion des römischen Bischofsamtes und des Petrusdienstes göttlichen Ursprunges bzw. Rechtes und damit unaufhebbar ist oder nicht.

Eine notwendige Residenzpflicht des Bischofs von Rom in der Stadt Rom scheint selbstverständlicher als sie tatsächlich war: Während des Abendländischen Schismas haben mehrere Bischöfe von Rom ihre Bischofsstadt und ihre Bischofskirche in ihrer Amtszeit nie gesehen.

Die christliche Gemeinde der Stadt Rom führt in ihrer Bischofsliste an erster Stelle den Apostel Petrus. Überliefert und in den ersten Jahrhunderten unbestritten ist dessen Martyrium und Grab in Rom am vatikanischen Hügel.

Kathedrale des Bistums Rom ist die Lateranbasilika. Dort befindet sich der Sitz des päpstlichen Kardinalvikars und seiner Behörde. Sie ist die ranghöchste der römischen Patriarchalbasiliken.

Zum Papst kann nach dem Kirchenrecht jeder gläubige männliche Katholik gewählt werden. Dabei erhält der Erwählte volle und höchste Gewalt in der Kirche durch die Annahme der rechtmäßig erfolgten Wahl (). Wenn der Gewählte noch nicht Bischof ist, ist er sofort zum Bischof zu weihen. Die Wahl erfolgt auf Lebenszeit.

Der Papst wird im Konklave, einer Versammlung aller Kardinäle, die bei Eintritt der Sedisvakanz jünger als 80 Jahre sind, auf Lebenszeit gewählt. Diese Altersbeschränkung gibt es erst seit Paul VI. Das Konklave wird heute in der Sixtinischen Kapelle am Petersdom abgehalten. Der letzte Papst, der zum Zeitpunkt seiner Wahl nicht Kardinal, sondern Erzbischof war und der Wahlversammlung darum selbst nicht angehörte, war Urban VI. im Jahre 1378.

Die 1996 mit der Konstitution "Universi Dominici Gregis" eingeführte Änderung der Wahlordnung, wonach nach dem 30. bzw. 33. erfolglosen Wahlgang – abhängig vom Zeitpunkt des ersten Wahlgangs – abweichend von der normalerweise geforderten Zweidrittelmehrheit zuzüglich einer Stimme auch eine absolute Mehrheit ausreicht, wurde 2007 von Papst Benedikt XVI. mit dem Motu proprio "De aliquibus mutationibus in normis" wieder rückgängig gemacht, allerdings werden nach dem 30. bzw. 33. Wahlgang nur noch Stichwahlen durchgeführt.

Die „papstfreie Zeit“, in der für einen verstorbenen oder zurückgetretenen Amtsinhaber noch kein Nachfolger bestimmt oder der Heilige Stuhl aus anderen Gründen vakant (unbesetzt) ist, nennt man Sedisvakanz. Während dieser Zeit wird die Leitung der Kirche durch das Kardinalskollegium wahrgenommen. Dieses besitzt nach den Normen der Apostolischen Konstitution "Universi Dominici Gregis" jedoch nur sehr eingeschränkte Kompetenzen. Es darf nur ordentliche Angelegenheiten und solche, die keinen Aufschub dulden, entscheiden. Fragen, die der Jurisdiktion des Papstes zugewiesen sind, darf das Kollegium nicht an sich ziehen. Auch päpstliche Gesetze und die Rechte des Apostolischen Stuhls und der Römischen Kirche darf es nicht antasten. Die Hauptaufgabe liegt bei der Vorbereitung der Papstwahl.

Aufgabe des Papstes ist die Leitung der Gesamtkirche. Hierzu bedient er sich seiner amtlichen Gewalten, insbesondere der Primatialgewalt.

Der Papst stellt so die Einheit der in Teilkirchen (Bistümer, Kirchen eigenen Rechts) aufgeteilten Kirche sicher. Fragen und Sachen, die die Kirche als Ganzes betreffen, sind seiner Amtsgewalt reserviert. Allein der Papst darf Bistümer errichten, neu umschreiben oder aufheben, die Erlaubnis zur Bischofsweihe erteilen, religiöse Institute aufheben und über Selig- und Heiligsprechungen abschließend befinden. Zudem sind dem Papst gewisse Prozesse, etwa Ehenichtigkeitsverfahren von Staatsoberhäuptern oder Prozesse gegen Kardinäle reserviert. Im Hinblick auf die unierten Ostkirchen sind bei alldem die Rechte der Patriarchen und Metropoliten zu beachten, die im CCEO geregelt sind.

Zur Leitung der Gesamtkirche bedient sich der Papst eines umfangreichen Verwaltungsapparats, der römischen Kurie. Die Kompetenzen und Zuständigkeiten der Kurienbehörden ist in der Apostolischen Konstitution Pastor Bonus geregelt.

Der Papst ist Souverän des Staates der Vatikanstadt. Der 1929 durch die Lateranverträge gegründete Staat ist eine absolute Wahlmonarchie, der Papst Träger der gesetzgeberischen, rechtsprechenden und ausführenden Gewalt. Die Verwaltung des Staats ist an eine Kurienbehörde, die Päpstliche Kommission für den Staat der Vatikanstadt delegiert.

Behinderung bedeutet, dass der Papst aus irgendeinem Grund dauerhaft an der Amtsausübung gehindert ist (Gefangenschaft, Exil, Geisteskrankheit). Erledigung des päpstlichen Stuhls tritt mit Amtsverzicht (can. 332 § 2 CIC) oder Tod des Papstes ein. Im Fall der Behinderung oder der Erledigung darf hinsichtlich der Leitung der Gesamtkirche nichts verändert werden.

Die Möglichkeit des Amtsverzichts

"→ Siehe auch: Liste von Päpsten, die auf das Amt verzichtet haben"

Ein Papst kann jederzeit auf das Amt verzichten. Nach Kanonischem Recht () „[…] ist zur Gültigkeit verlangt, daß der Verzicht frei geschieht und hinreichend kundgemacht […] wird.“ Der Amtsverzicht bedarf nicht der Annahme irgendeiner kirchlichen Stelle und kann daher nicht verhindert oder aufgeschoben werden. Dass Päpste auf das Amt verzichteten, kam in der Kirchengeschichte sehr selten vor und fand meist unter äußerem Druck statt: Papst Pontianus legte 235 sein Amt nieder, nachdem er nach Sardinien verbannt worden war. 537 verzichtete der auf der Insel Ponza gefangengehaltene Papst Silverius auf das Papstamt. 1415 wurde Gregor XII. beim Konzil von Konstanz zum Amtsverzicht gedrängt. Coelestin V. (1294) und Benedikt XVI. (2013) verzichteten freiwillig auf ihr Amt.

Die päpstlichen Insignien bestehen aus

Als Alltagsbekleidung trägt der Papst gewöhnlich eine weiße Soutane, ein weißes Zingulum (Gürtel) und einen weißen Pileolus (Scheitelkäppchen); Paul VI. trug darunter „barocke“ Kniebundhosen. Für kältere Tage steht dem Papst ein weiter roter Umhang, der sogenannte Mantello, zur Verfügung. Als weitere traditionelle Kopfbedeckung kann der Papst in der kalten Jahreszeit einen mit Hermelinfell gefütterten Camauro tragen (so Johannes XXIII. und Benedikt XVI.). Auf seiner Brust trägt der Papst wie jeder katholische Bischof das Pektorale, ein Brustkreuz an einer Halskette. Für kälteres Wetter hat der Papst zudem einen weißen Mantel mit doppelreihigem Knopfbesatz.

Bei der Liturgie trägt der Papst ein Messgewand, fakultativ darunter die Dalmatik, Mitra und über dem Messgewand das Pallium. Bei nichteucharistischen Liturgien, etwa zum Stundengebet, trägt er das Pluviale und Albe, und bei besonderen Anlässen wie beispielsweise beim Empfang von Staatsbesuchen kann er über seiner Soutane ein weißes Rochett (Chorhemd) und eine rote Mozetta (Schulterüberwurf) aus Seide oder Samt anlegen. Die Winterversion der Mozetta ist aus rotem Samt und hat einen Hermelinsaum. Während der Osterzeit trug Benedikt XVI. die bis zu Paul VI. übliche weiße Mozetta aus Damast, die ebenfalls mit einem weißen Fellsaum versehen ist. Die rote Mozetta stammt noch aus der Zeit, als der Papst die Farbe Rot trug. Zu hohen Festtagen kann der Papst den Fanon tragen, ein ihm vorbehaltenes kreisrundes Schultergewand. Zu Empfängen trug der Papst früher einen Rauchmantel, die Tiara und weiße Pontifikalhandschuhe.

Nach der erfolgten Wahl wird der neue Papst gefragt, welchen Namen er annimmt. Die Namenswahl unterliegt der freien Entscheidung des Papstes. Aus der Wahl des Namens versuchen Beobachter politische Ziele des neuen Papstes abzuleiten, indem die charakteristischen Eigenschaften von früheren Päpsten und Heiligen dieses Namens untersucht werden. Der Name Pius war vom Ende des 18. bis zur Mitte des 20. Jahrhunderts der mit Abstand am häufigsten gewählte Name. Seit dem Tod von Pius XII. (1958) wurde er nicht mehr gewählt.

Päpste können Namen annehmen, die die latinisierte Form ihres bürgerlichen Namens darstellen (Hadrian VI. = Adrian Florisz, Marcellus II. = Marcello Cervini), was jedoch seit dem 16. Jahrhundert nicht mehr vorgekommen ist. Viele Päpste nehmen die Namen bedeutender Vorgänger an wie Leo und Gregor oder jene von Heiligen wie Paul VI., nach Apostel Paulus. Andere gehen nach der Bedeutung der Namen (Pius = fromm; Innozenz = unschuldig). Einige Päpste wählen ihren Namen aus persönlichen Gründen wie Johannes XXIII., zu Ehren seines Vaters.

Ursprünglich behielten die Päpste nach der Wahl ihren eigenen Vornamen. Der erste Papst, der einen neuen Namen annahm, war Johannes II. im Jahr 533. Er hieß eigentlich Mercurius und wollte als Papst nicht den Namen eines heidnischen Gottes tragen. Jedoch blieb die Annahme eines neuen Namens bis zum Ende des 1. Jahrtausends eine Ausnahme und wurde erst mit Sergius IV. im Jahre 1009 zur Regel.

Der erste Name, der wiederholt von einem Papst getragen wurde, war Sixtus (durch Sixtus II. im Jahr 257). Seitdem werden die Namen, die mehrfach angenommen werden, mit nachgestellten römischen Ziffern versehen. Die Päpste der Antike und des Frühmittelalters trugen jedoch häufig Namen, die kein zweites Mal angenommen wurden. Einige der antiken Namen wie Clemens und Pius wurden ab dem Hochmittelalter und damit dem Aufkommen der Namenswahl wieder aufgegriffen.

Albino Luciani wählte in Erinnerung an seine beiden Vorgänger mit Johannes Paul I. den ersten Doppelnamen der Papstgeschichte, zugleich war dies der erste neue Papstname seit Lando von 913 bis 914. Sein Nachfolger Karol Wojtyła wählte ebenfalls den Papstnamen Johannes Paul II. Der Name von Benedikt XVI. nimmt Bezug auf Benedikt XV. von 1914 bis 1922, der vergeblich versuchte, den Ersten Weltkrieg zu verhindern bzw. zu beenden, sowie auf den Mönchsvater und Patron Europas, Benedikt von Nursia. Jorge Mario Bergoglio wählte wiederum als erster den Namen Franziskus mit Bezug auf Franz von Assisi, den Begründer des Franziskanerordens, und dessen Ziel einer „armen Kirche“, die sich für die Bedrängten und Bedürftigen einsetzt. Auch über einen Bezug zum heiligen Franz Xaver, einem der Begründer des Jesuitenordens, dem Kardinal Bergoglio angehört, wurde nach der Wahl spekuliert.

Der Papst ist durch kirchliches und weltliches Recht gegen Akte physischer Gewalt geschützt. Can. 1370 § 1 droht als Strafe für solche Gewalt gegen den Papst die Exkommunikation an. Gemäß Artikel 8 des Lateranvertrags wird ein Attentat oder die Anstiftung zu einem solchen mit denselben Strafen bedroht wie entsprechende Handlungen gegen den italienischen König bzw. heute den Staatspräsidenten.

Ein Verfahren zur Absetzung eines Papstes ist nicht vorgesehen und nach heutigem Selbstverständnis des Papsttums nicht möglich. Im Laufe der Kirchengeschichte kam es jedoch wiederholt zur Erhebung von Gegenpäpsten etwa durch den römisch-deutschen Kaiser oder interessierte Machtzirkel, die um den mit großer weltlicher Macht ausgestatteten Papstthron kämpften. Wer in die Geschichte als Gegenpapst einging, hing oft davon ab, welcher Kandidat sich im Kampf um den päpstlichen Stuhl letztlich durchsetzen konnte. Bekannte Fälle waren:


Der universale Primatsanspruch des Bischofs von Rom entwickelte sich im Lauf des ersten Jahrtausends und gipfelte im "Dictatus Papae" von 1075. Der Papst gilt in der römisch-katholischen Kirche als oberster Herr der Gesamtkirche und Stellvertreter Christi auf Erden – ein Anspruch, der, abgesehen von den katholischen Unierten Kirchen, von allen übrigen Kirchen nicht anerkannt wird.

Das erste Vatikanische Konzil von 1869 bis 1870 erhob die Glaubensüberzeugung, der Papst sei, wenn er ex cathedra spricht, in Glaubensfragen unfehlbar, zum Dogma. Auch dieser Anspruch wird von den übrigen Kirchen abgelehnt; als Folge entstand zudem die Altkatholische Kirche. Ausdrücklich angewendet wurde das Unfehlbarkeitsdogma seit 1870 ein einziges Mal, 1950 bei der Formulierung des Dogmas von der leiblichen Aufnahme Mariens in den Himmel. Enzykliken und Lehrschreiben des Papstes sind für die römisch-katholische Kirche zwar bindend, aber nicht ohne weiteres als unfehlbare Lehrentscheidungen anzusehen. Die theologische Diskussion hierüber ist nicht abgeschlossen.

In der Alten Kirche gab es folgende fünf maßgebliche Patriarchen in der Reihenfolge des durch ökumenische Konzile definierten Ehrenvortritts:

Damals schon galt unter den Christen der römische Bischofssitz als „primus inter pares“, da Rom die Hauptstadt des Römischen Reiches war und die Kirche von Rom insbesondere durch die Gräber der „Apostelfürsten“ Petrus und Paulus als verehrungswürdig angesehen wurde. Der Kirchenhistoriker Eusebius von Caesarea († 339) notiert das Martyrium von Petrus und Paulus in Rom als eine in der ganzen Kirche bekannte Tatsache. Irenäus von Lyon († um 202) gibt die römische Ortstradition wieder, wonach das römische Bischofsamt sich in direkter Nachfolge vom Apostel Petrus herleite, der der erste Vorsteher (episkopos) der römischen Christengemeinde gewesen sei. Auch das Patriarchat von Antiochia beruft sich darauf, dass Petrus, bevor er nach Rom gegangen sei, dort seit dem Jahr 38 der erste Bischof war. Ebenso führen sich die übrigen Patriarchate und einige weitere östliche Bischofssitze auf einen Apostel zurück. Ob Petrus wirklich in Rom gewesen ist, ist unter Historikern allerdings umstritten.

Die römische Petrustradition ist historisch nicht ausgeschlossen, war aber in den ersten Jahrhunderten kein wichtiges Thema. Für die Anwendung von "Mt 16,18" auf die Bischöfe von Rom als Petrusnachfolger findet sich das früheste schriftliche Zeugnis bei Papst Damasus I. im 4. Jahrhundert. Dort wird die römische Kirche erstmals exklusiv als „sedes apostolica“ (apostolischer Stuhl) bezeichnet – eine Sonderstellung, die von den übrigen Patriarchaten nicht anerkannt wird. Durch die Teilung des Römischen Reiches wurden aber die monarchischen Tendenzen des einzigen westlichen (lateinischen) Patriarchensitzes weiter begünstigt.

Scharfe Kritiker sehen im Papsttum die Fortsetzung des Machtanspruchs des alten Roms und das Papstamt wird aus protestantischer Sichtweise sehr skeptisch, wenn auch nicht ausschließlich negativ beurteilt. Die konstantinische Wende rief einen völlig anderen Menschenschlag als den bisherigen an die Spitze der noch jungen Kirche. Während in den ersten Jahrhunderten Christen noch grausam verfolgt wurden und zum Christsein außerordentlich viel Mut gehörte, war nun das Christentum Teil der kaiserlichen Machtpolitik geworden und bot begehrenswerte, weil gut bezahlte und einflussreiche Ämter. Die römische Kirche hatte im Westen die traditionelle Vorherrschaft Roms übernommen. Versuche, sie auf die übrigen Patriarchate auszudehnen, scheiterten jedoch. In der Folge setzte sich das Papsttum in Westeuropa mehr und mehr auch als weltliche Herrschaft durch.

Eine Stellvertreterschaft Gottes, die aus der Bibel nicht stichhaltig abzuleiten sei, habe ihr Vorbild dagegen im römischen Kaisertum. Originär ist der Titel des Pontifex Maximus dem römischen Kaiser vorbehalten und findet nach dem Untergang des römischen Reiches eine Übertragung auf den Bischof von Rom. So stellte sich der Papst im Hochmittelalter in geistlichen und weltlichen Fragen als Gebieter über Könige und Völker, was sich jedoch ab dem 14. Jahrhundert immer weniger durchsetzen ließ. Auch auf religiösem Gebiet kam es im Spätmittelalter zu einer immer stärkeren Diversifikation, wobei die Kirche allerdings gegen Andersdenkende in ihrem Machtbereich vorging.





</doc>
<doc id="7909" url="https://de.wikipedia.org/wiki?curid=7909" title="Universum (Begriffsklärung)">
Universum (Begriffsklärung)

Universum steht für:



Siehe auch:


</doc>
<doc id="7912" url="https://de.wikipedia.org/wiki?curid=7912" title="Unendlichkeit">
Unendlichkeit

Der Begriff Unendlichkeit bezeichnet die Negation bzw. Aufhebung von Endlichkeit, weniger präzise auch deren „Gegenteil“. Sein mathematisches Symbol ist das Unendlichzeichen formula_1. Theoretisch beschreibt der Begriff „unendlich“ ein Objekt oder einen Vorgang ohne Ende oder Schluss, aber möglicherweise mit Anfang oder Beginn. In der Geometrie würde also ein Strahl oder eine Kreisbahn als unendlich beschrieben werden.

Präzisierung fand der Unendlichkeitsbegriff vor allem in der Mathematik, wesentlich initiiert durch das Werk Bernard Bolzanos, Georg Cantors und Richard Dedekinds, welches in die Mengenlehre und insbesondere in die Theorie der unendlichen Mengen und der transfiniten Kardinalzahlen mündete.

Die Unendlichkeit lässt sich geistes- oder naturwissenschaftlich „nur“ abstrakt entwickeln und wird auf Objekte und Begriffe angewendet, die keine räumlichen und/oder zeitlichen Grenzen haben.

In der Theologie und manchen philosophischen Konzeptionen (wie der Natürlichen Theologie) ist die Unendlichkeit eines der Attribute Gottes, während die Schöpfung per se endlich ist. Das Wesen des Unendlichen ist insbesondere ein Thema der Metaphysik sowie der Mystik, etwa in der Kabbala unter dem Namen "En Sof" oder bei christlichen Mystikern wie Nikolaus von Kues und Meister Eckhart.

In der Philosophie existieren seit Aristoteles zwei Auffassungen vom Begriff des Unendlichen: das aktual Unendliche und das potentiell Unendliche. Die Scholastik unterscheidet demgemäß zwischen dem potentiell Unendlichen ("Indefiniten"), das ohne Ende vermehrt werden kann und dem aktuell Unendlichen ("Infiniten"), das jede Grenze positiv ausschließt. Im engen und eigentlichen Sinn kommt demnach nur Gott die aktuelle Unendlichkeit zu. Sie ist die grenzenlose Fülle des Seins, jedoch nicht in einem pantheistischen Sinne misszuverstehen.

Leonardo da Vinci symbolisierte die Unendlichkeit mit der Unendlichkeitsmaschine.

Hegel prägte den Begriff der "schlechten Unendlichkeit" (Enzyklopädie § 93 f.), unter der er in dialektischer Weise eine Abgrenzung zur Endlichkeit versteht.

In der Astronomie wurde angesichts der Tiefe und Weite des Sternhimmels oft die Vorstellung eines unendlich ausgedehnten Weltraums entwickelt. Auch in Bezug auf die Zeit ist das Konzept der Unendlichkeit bekannt, hier verwendet man den Begriff Ewigkeit. Während die Höhere Mathematik oft mit dem Abstraktum „unendlich“ operiert, ist in der theoretischen Physik eher das Phänomen der Singularität von Bedeutung – etwa im Zusammenhang mit den Begriffen Urknall (Beginn des sichtbaren Universums) und Schwarzes Loch. Als Singularität wird ein Punkt in der Raumzeit bezeichnet, an dem Masse in einem ausdehnungslosen Punkt mit unendlicher Dichte konzentriert ist.

Neben der unendlichen Ausdehnung zu immer weiter zunehmenden Größen wird der Begriff auch für die unendliche Teilbarkeit, das unendlich Feine verwendet, dessen Grenze null ist, null aber nicht erreicht. Aus der Negation des unendlich Feinen und deren Paradoxien ergab sich die ursprüngliche griechische „Atomtheorie“ des „Unteilbaren“.

In der Mathematik gibt es keinen definierten Begriff mit dem Namen „Unendlichkeit“, jedoch wird das Adjektiv "unendlich" zur näheren Charakterisierung einiger mathematischer Begriffe verwendet. In der Regel ist diese Charakterisierung komplementär zum Begriff "endlich".




</doc>
<doc id="7915" url="https://de.wikipedia.org/wiki?curid=7915" title="Ostpreußen">
Ostpreußen

Ostpreußen war die namensgebende Provinz des Staates Preußen. Als „Königreich Preußen“ lag Ostpreußen außerhalb des Heiligen Römischen Reiches und des Deutschen Bundes, bis der nach ihm benannte Staat Preußen 1867/71 im Deutschen Reich aufging.

Das ursprüngliche Preußenland war das Stammland der baltischen Prußen. Durch Anordnungen des Kaisers und des Papstes zur Christianisierung und der damit beauftragten Eroberung des Landes durch den Deutschen Orden im 13. Jahrhundert entstand der Deutschordensstaat, dessen Territorium auch „Preußen“ genannt wurde.

Infolge des Zweiten Friedens zu Thorn verblieb 1466 nur der östliche Teil Preußens unter dem Orden "(Prussia Orientalis)", während das Fürstbistum Ermland "(Warmia)" und der abtrünnige westliche Teil "(Prussia Occidentalis)" autonom wurden und sich dem polnischen König unterstellten (Personalunion). Im Zuge der Reformation wurde der östliche Teil unter dem letzten Hochmeister des Deutschen Ordens in Preußen, Albrecht von Preußen, 1525 als Herzogtum Preußen zum ersten protestantischen Staatswesen in Europa unter Suzeränität des polnischen Königs.

Durch die dynastische Vereinigung mit dem Kurfürstentum Brandenburg 1618 wurde es auch "Brandenburgisches Preußen" genannt. Im Vertrag von Wehlau übergab 1657 der König von Polen seine Suzeränitätsrechte über das Herzogtum Preußen an den Kurfürsten von Brandenburg und seine Nachfahren, die dadurch souveräne "Herzöge in Preußen" wurden. In der Hauptstadt Königsberg krönte sich 1701 Kurfürst Friedrich III. als Friedrich I. zum "König in Preußen". Der Name „Preußen“ ging im Verlauf des 18. Jahrhunderts auf den gesamten Staat der Hohenzollern in ihrer Eigenschaft als Könige von Preußen und Kurfürsten von Brandenburg innerhalb und außerhalb des Heiligen Römischen Reichs über.

Nach der Ersten Teilung Polens verfügte König Friedrich II. von Preußen 1772, dass die bisherige Provinz Preußen, erweitert um das Ermland, nach der Vereinigung aller "Lande Preußen" den vorherigen lateinisch Namen "Prussia Orientalis" jetzt in Deutsch "Ostpreußen" erhalten solle und das annektierte Polnisch-Preußen den Namen "Westpreußen". Beide Provinzen bildeten mit dem Netzedistrikt in der Preußischen Monarchie zwischen 1772 und 1793 das "Königreich Preußen".

In der Provinz Preußen, zu der Ostpreußen von 1829 bis 1878 gehörte, lagen nach der Gründung des Norddeutschen Bundes 1867 und der Gründung des Deutschen Reichs 1871 sowohl dessen nördlichste als auch östlichste Punkte. Nach dem Friedensvertrag von Versailles 1919, der den Kriegszustand des Ersten Weltkriegs beendete, war Ostpreußen zwischen 1920 und 1939 durch den „Polnischen Korridor“ vom übrigen Deutschland territorial abgetrennt.

Durch das Potsdamer Abkommen kam das nördliche Ostpreußen einschließlich der Provinzhauptstadt Königsberg nach dem Ende des Zweiten Weltkriegs 1945 unter vorläufige Verwaltung der Sowjetunion und das südliche Ostpreußen unter polnische Verwaltung. Eine endgültige Regelung wurde einem gesamtdeutschen Friedensvertrag vorbehalten. De facto wurde Ostpreußen völkerrechtswidrig administrativ der Volksrepublik Polen beziehungsweise der UdSSR eingegliedert, wobei an die Stelle der nahezu vollständig vertriebenen Bevölkerung Polen bzw. Sowjetbürger traten.

Die DDR erkannte die Grenze zu Polen bereits 1950 an, die Bundesrepublik Deutschland zunächst 1972 indirekt. Im Zwei-plus-Vier-Vertrag und dem deutsch-polnischen Grenzvertrag von 1990 erklärten die Vertragspartner die Außengrenzen der Deutschen Demokratischen Republik und der Bundesrepublik Deutschland als endgültig für das vereinte Deutschland. Damit gehört der Südteil des früheren äußersten deutschen Ostgebiets auch völkerrechtlich zu Polen und der nördliche als Exklave zum heutigen Russland (damals noch UdSSR).

Das historische Ostpreußen erstreckt sich an der Ostseeküste vom Weichseldelta bis nördlich der Memelmündung bei Memel/Klaipėda, wo bei Nimmersatt „das Reich sein Ende hat“. Das nördlich der unteren Memel am Kurischen Haff gelegene Memelland wurde 1920 durch den Völkerbund von Ostpreußen abgetrennt, war von 1923 bis Anfang 1939 von Litauen annektiert und gehört seit dem Kriegsende wieder zu Litauen. Der nördliche Teil (etwa 35 %) des restlichen Ostpreußens ist heute der russische Oblast Kaliningrad, der südliche Teil (etwa 65 %) die polnische Woiwodschaft Ermland-Masuren. Im Mai 1939 umfasste Ostpreußen, einschließlich des Memellandes, 39.840 km² mit 2.649.017 Einwohnern. Es war mit 66,5 Einwohnern je km² vergleichsweise dünn besiedelt. In der Hauptstadt Königsberg lebten damals 372.000 Einwohner.

Das Landschaftsbild des nördlichen Ostpreußen wird von leicht gewelltem Flachland mit Moränenhügeln, größtenteils versteppten Wiesen und Feldern sowie viel Wald bestimmt, der von breiten Flussniederungen und Moorgebieten unterbrochen wird. Größte Flüsse sind der Pregel und die Memel, weitere Flüsse sind die Łyna bzw. Lawa "(Alle)", die Angrapa "(Angerapp)", die Krasnaja "(Rominte)" und die Dejma "(Deime)". Im Norden der Oblast befindet sich – angrenzend an das Kurische Haff – die Elchniederung "(Losinaja Dolina)" und das Große Moosbruch, eine Moorlandschaft, die zum Teil trockengelegt worden ist.

Im Südosten liegt die Rominter Heide mit dem Wystiter See und dem Wystiter Hügelland. Weite Teile der dünnbesiedelten Landschaft im südlichen Ostpreußen sind durch die Masurische Seenplatte geprägt.
Im Westen ragt das Samland als Halbinsel in die Ostsee. Im Südwesten liegt das Frische Haff. Ostpreußen hatte Anteil an der Kurischen und der Frischen Nehrung.

Große Teile des Bodens gehören zu den Bodenklassen 4 und 5. Als Rohstoffe sind Sand und Kies für das Bauwesen und Lehm, Torf und Ton für die keramische Industrie interessant. Etwa 30 Prozent des Gebietes sind von Wäldern bedeckt.

Durch die geringe Bevölkerungsdichte (66,5 Einwohnern je km²) konnten sich in Ostpreußen viele im Rest des damaligen Deutschlands bereits ausgestorbene Tiere erhalten. So gab es 1945 in Ostpreußen eine Population von Elchen und Wölfen. Auffällig sind noch heute (2012) die vielen Störche in Ostpreußen, was bereits Wesentliches über die dort vorherrschenden Landschaftsformen und ihre Bewirtschaftung aussagt.

Archäologische Funde bezeugen menschliche Besiedlung an der Südküste der Ostsee nach dem Ende der Eiszeit (die Vereisung endete in Litauen z. B. um 16.000 v. Chr.), etwa im Allerød-Interstadial (11. Jahrtausend v. Chr.). Im End-Mesolithikum sind sowohl Neman- als auch Narva-Kultur vertreten. Im Neolithikum ist die Haffküsten-Kultur, eine Gruppe der Schnurkeramik, nachgewiesen.
In der frühen Eisenzeit (6. - 1. Jhd. v. Chr.) lebte im Gebiet zwischen Ermland und Memel die Träger der Westbaltische Hügelgräberkultur.

Zwischen Braunswalde und Willenberg nahe Marienburg wurde im Jahre 1873 ein eisenzeitliches Gräberfeld mit etwa 3000 Gräbern gefunden. Die nach dieser Fundstätte benannten Braunswalde-Willenberg-Funde, heute auch als Wielbark-Kultur bezeichnet, zeichnet sich durch eine Mischung skandinavischer und kontinentaler Elemente aus und wird allgemein als Zeichen für die Zuwanderung der Goten angesehen. Zu ihrem Verbreitungsgebiet gehörte nur der äußerste Westen Ostpreußens. Die Goten waren im letzten Jahrhundert vor der christlichen Zeitenwende in das Gebiet um die untere Weichsel gekommen, wanderten aber ab etwa 200 n. Chr. nach Südosten ab.

Im restlichen Gebiet Ostpreußens war seit dem 1. Jahrhundert n. Chr. die archäologische "Westbaltische Kultur" verbreitet, mit der Olsztyn-Gruppe, der "Sudauer Gruppe", der Dollkeim-Gruppe und der Memelland-Gruppe.
Spätestens die Träger dieser Kultur müssen als baltische Gruppen angesehen werden.

98 n. Chr. berichtete Tacitus in seiner Germania über das Volk der "Aesti gentes". Diese waren aller Wahrscheinlichkeit nach die Vorgänger der ab dem 9. Jahrhundert als "Prußen" bezeichneten westbaltischen Stämme.
Im 2. Jahrhundert erwähnte Claudius Ptolemäus die Stämme der "Galindoi" und "Sudinoi", die wahrscheinlich den westlichen "(Olsztyn-Gruppe)" bzw. den östlichen Teil "(Sudauer Gruppe)" des später ostpreußischen Gebietes bewohnten.

In seiner um 550 verfassten "Getica" zählt der gotische Geschichtsschreiber Jordanes die "Aesti" zum Gotischen Reich, das bis etwa 375 nördlich des Schwarzen Meeres gelegen hatte.

Im 9. Jahrhundert wird erstmals ein Volk namens "Pruzzi" erwähnt, von einem als Bayerischer Geograph bekannten Chronisten.

Der Angelsachse Wulfstan bereiste die Ostseeländer im 10. Jahrhundert. In seinem Bericht unterschied er das östlich der Weichsel gelegene „Witland“ vom westlich des Flusses gelegenen Land der Winoten und bezeichnete seine Einwohner, wie die antiken Autoren auch als „Ēstas“.

Die ostbaltischen Litauer wurden im 11. Jahrhundert erstmals beschrieben. Doch erst mit der Zeit der Christianisierung und der damit verbundenen Schriftkultur fing man an, schriftliche Dokumente zu führen, die detaillierte Informationen enthalten.

Die Prussia-Sammlung war die bedeutendste Sammlung archäologischer Fundstücke.


Das Stammesland der Prußen (Pruzzen) lag an der Ostseeküste, nordöstlich des späteren Polens und südwestlich von Litauen. Nördlich erstreckte es sich bis an die untere Memel, westlich bis an die untere Weichsel, wobei beide Flüsse wohl keine scharfe Siedlungsgrenze bildeten. So wird auch von slawischen Siedlungen im Kulmerland berichtet und Linguisten verweisen auf Fluss- und Ortsnamen westlich der Weichsel bis an die Persante sowie auf Wörter baltischen Ursprungs in der kaschubischen Sprache.

Das von baltischen Stämmen an der Ostseeküste besiedelte Gebiet wurde seit dem 10. Jahrhundert zur Interessensphäre der in der Region entstehenden christlichen Staaten. Alle Anstrengungen zur Eroberung des Gebietes standen auch unter dem Vorwand der Missionierung. Die Kaiser des Heiligen Römischen Reiches, im Hochmittelalter die mächtigsten weltlichen Herrscher des Abendlandes, erhoben Anspruch auf nicht christianisierte Gebiete. So Kaiser Friedrich II. in der Goldbulle von Rimini 1226 an den Deutschen Orden.

Die Versuche der polnischen Herrscher, ihre Macht an die noch von Heiden bewohnte Ostseeküste auszudehnen, zeigten nur in Pommern Erfolg. Über einen dieser Vorstöße, bei dem 997 der Missionsbischof Adalbert von Prag im Auftrag von Bolesław I. in die Gegend östlich von Danzig vordrang, berichtet dessen Autobiografie "Vita Sancti Adalberti."

Konrad, der Herzog von Masowien, erlitt gegen die Prußen empfindliche Rückschläge. Das laut der Älteren Olivachronik zu großen Teilen von Polen besiedelte Kulmerland wurde laut der Chronik des Peter von Dusburg durch Prußen verwüstet. Die Vorstöße der Prußen bedrohten sogar seine Machtbasis Masowien. Der erste Bischof von Preußen wurde 1209 ernannt: Der Zisterzienser Christian von Oliva, vorher Abt von Łękno, nahm seinen Sitz 1215 im 30 Jahre zuvor gegründeten Kloster Oliva, außerhalb Preußens im Herzogtum Pommerellen der Samboriden. Seine Christianisierungsbemühungen waren zunächst nicht von dauerhaftem Erfolg. Der von Konrad I. und Bischof Christian gemeinsam ins Leben gerufene Ritterorden "Milites Christi Prussiae", zumeist Orden von Dobrin genannt, konnte zwar Masowien sichern, aber keine Herrschaft über Preußen etablieren.

Herzog Konrad von Masowien bat den Deutschen Ritterorden um militärische Unterstützung im Kampf gegen die Prußen und bot ihm dafür Landrechte an. Im Jahre 1224 wurde Wilhelm von Modena vom Papst zum Legaten für Preußen und Samland benannt. Die Landrechte für das zu erobernde Gebiet ließ sich der Orden 1226 durch den römisch-deutschen Kaiser Friedrich II. garantieren und 1230 durch Konrad von Masowien im Vertrag von Kruschwitz. Dieser wird heute als Diktat des Ordens, wenn nicht als Fälschung angesehen. 1231 legte der Orden Thorn an. Papst Gregor IX. bescheinigte 1234 dem Orden in der Bulle von Rieti, dass seine Eroberungen nur der Kirche, aber keiner weltlichen Lehenshoheit unterstehen sollten.

Der Orden eroberte das Land mit aus europäischen Adligen zusammengestellten Truppen in Kreuzzügen. Er sicherte seine Eroberungen durch Burgenbau, holte mit Hilfe von Lokatoren deutsche Siedler ins Land, Teil der Deutschen Ostkolonisation. Zahlreiche Städte und Dörfer wurden gegründet. Die Unstimmigkeiten über die Landverteilung zwischen dem Orden und Bischof Christian wurden bis vor den Papst gebracht. 1245 teilte der päpstliche Legat Wilhelm von Modena das Preußenland in vier Bistümer ein. Die vier Bistümer unterstanden dem Erzbischof von Riga. Es dauerte jedoch bis 1283, ehe die heidnischen Prußen endgültig unterworfen waren.

Über das vertraglich vereinbarte Gebiet hinaus eroberte der Deutsche Orden 1309 auch das christliche Pommerellen mit Danzig, das der letzte Herzog, Mestwin II., nach zeitweiliger Abtrünnigkeit wieder Polen zugesagt hatte. Diese Eroberung wurde von Polen 1343 anerkannt. Die Grenze zu Litauen, das sich im Widerstand gegen den Orden als Staat bildete, wurde erst 1422 dauerhaft festgelegt. Sitz des Ordens war zunächst Venedig, dann seit 1309 die Marienburg in Preußen, nach der Schutzheiligen des Deutschen Ordens Maria benannt.
Zu den Konflikten des Ordens mit Polen um die Ausdehnung der territorialen Herrschaft gesellten sich im 15. Jahrhundert Konflikte mit den Städten in seinem Gebiet wegen seiner Versuche, auch den lukrativen Handel an sich zu ziehen. So kam es zu kriegerischen Auseinandersetzungen, bei denen der Deutsche Orden auf der einen Seite, die preußischen Städte und das Königreich Polen auf der anderen Seite standen.

Nach seiner Niederlage in der Schlacht bei Tannenberg 1410 wurde die Macht des Ordens geschwächt. Er musste im Ersten Thorner Frieden 1411 und im Frieden vom Melnosee 1422 Herrschaft und Ansprüche auf Samaiten aufgeben. Der Friede von Brest 1435 schloss jegliche Ansprüche Dritter (insbesondere des Heiligen Römischen Reiches) am Ordensland aus. Nachdem sich die westpreußischen Stände im Preußischen Bund organisiert und 1454 dem König von Polen unterstellt hatten, kam es zum Dreizehnjährigen Krieg, der 1466 mit dem Zweiten Thorner Frieden endete. Der Deutsche Orden hatte das Kulmerland, Ermland, Pogesanien und Pomerellen an die polnische Krone abzutreten, Königliches Preußen oder Preußen Königlichen Anteils genannt. Somit blieb auch das Ermland (als exemptes eigenständiges Fürstbistum) unter Regentschaft des Polnischen Königs und damit bis zur Ersten Teilung Polens 1772 vom Preußen des Ordens und der Hohenzollern getrennt. Da die schon 1457 eroberte Ordensburg Marienburg mit abgetreten werden musste, wurde der Sitz des Ordens nach Königsberg verlegt. Der Orden war außerdem dem Polnischen König zu Treueeid und Heeresfolge verpflichtet.

1511 wurde Albrecht von Preußen Hochmeister des Deutschen Ordens. Er verweigerte dem Polnischen König zunächst den Treueeid. Kaiser Maximilian I. schloss im Jahre 1515 auf dem Wiener Fürstentag Verteidigungs- und Heiratsbündnisse mit den Jagiellonen und erkannte schließlich die Beschlüsse des Thorner Friedens an, nachdem sie bis dahin von Kaiser und Papst abgelehnt worden waren.

Nachdem ihm dessen Unterstützung im vierjährigen Reiterkrieg versagt worden war, ging Hochmeister Albrecht auf Distanz zum Kaiser. Er schloss Frieden mit Polen, führte 1525 die Reformation ein und machte den Ordensstaat zum weltlichen Herzogtum Preußen. Die erbliche Herzogswürde ließ er sich unter Anerkennung der polnischen Lehenshoheit vom polnischen König Sigismund I. bestätigen.

Vom Heiligen Römischen Reich Deutscher Nation wurde die Säkularisation des preußischen Ordensstaates nicht anerkannt. Die Vertreter des Deutschen Ordens im Reich wählten einen neuen Hochmeister, Walther von Cronberg, welcher aber nicht wie bisher in Königsberg, sondern in Mergentheim seinen Sitz einnahm. 1527 erhielt Cronberg vom Kaiser die Berechtigung, sich „Administrator des Hochmeistertums“ zu nennen. Auf dem Reichstag zu Augsburg 1530 wurde dieser mit den Rechten des Deutschen Ordens und dem Lande Preußen belehnt. Diese Entscheidung hatte in der Praxis keine Bedeutung. Der weltliche Einfluss Cronbergs endete faktisch an den Grenzen der Balleien innerhalb des Reichs. Maximilian III., der Sohn Kaiser Maximilians II., führte bis 1618 den Titel eines "Administrators von Preußen". Danach nannte man das Amt "Hoch- und Deutschmeister". Die in Preußen hoheitslosen Hoch- und Deutschmeister des Deutschen Ordens hatten durch den Kaiser seit 1526 den gleichen Stand im Heiligen Römischen Reich wie ein Fürstbischof. 1531/34 wurde Herzog Albrecht unter Bann gesetzt, der jedoch unwirksam blieb.

1525 schuf Albrecht eine Gebietseinteilung, die bis 1722 Bestand hatte. Das Herzogtum war nun in drei Kreise vom Ausmaß späterer Regierungsbezirke eingeteilt: Samland, Natangen und Oberland. Aus den bisherigen Ordenskomtureien wurden die Hauptämter im Zuschnitt späterer Landkreise. In jedem Hauptamt gab es mehrere Ämter, die teils Kammerämter waren, teils missverständlicherweise wiederum Kreis (Creyß) genannt wurden. Diese Ämter waren für Wirtschaft und Rechtsangelegenheiten der unfreien Bauern zuständig. Die unterste Verwaltungsgliederung waren die Bezirke, die teilweise auch Dörfer genannt wurden, obwohl sie in der Regel mehrere Siedlungen umfassten.

1544 gründete Herzog Albrecht die Universität Albertus-Universität in Königsberg. Die kulturellen Leistungen in seiner Amtszeit waren die Prutenischen Tafeln, die Erstellung preußischer Landkarten sowie eine Münzreform, die eine Harmonisierung der Münzen (praktisch eine Währungsunion) des Herzogtums mit den Münzen Preußen königlichen Anteils und Polen-Litauens herbeiführte. In diese Zeit fielen auch die Aufnahme evangelischer Flüchtlinge und besonders die erstmaligen Übersetzungen religiöser Schriften in verschiedene Sprachen der neuen preußischen Bürger aus den Nachbarländern.

Nach dem Tod Herzog Albrechts im Jahre 1568 kam dessen fünfzehnjähriger Sohn Albrecht Friedrich an die Regierung. Wegen dessen Geisteskrankheit setzte 1577 der polnische König Stephan Báthory den Ansbacher Hohenzollern Georg Friedrich als Administrator von Preußen ein; ihm folgte 1605 mit Joachim Friedrich erstmals ein Kurfürst von Brandenburg, dann 1608 Johann Sigismund, Albrechts Schwiegersohn.

Als Albrecht Friedrich 1618 kinderlos starb, fiel das Herzogtum Preußen 1618 an die brandenburgische Linie der Hohenzollern, zu diesem Zeitpunkt unter Johann Sigismund. Dieser verband das Kurfürstentum Brandenburg und das Herzogtum Preußen in einer Personalunion. Nun wurde das Herzogtum Preußen auch "Brandenburgisches Preußen" genannt und bis 1701 oft als Fürstentum bezeichnet (so in Kirchenbüchern vor 1700). Im Vertrag von Wehlau 1657 verzichtete Polen auf die Lehenshoheit über das Herzogtum Preußen. Damit besaßen die Kurfürsten von Brandenburg hier, anders als in ihren im Heiligen Römischen Reich liegenden brandenburgischen Territorien, die volle Souveränität.

Die Souveränität nutzte der brandenburgische Kurfürst Friedrich III., um sich 1701 in Königsberg als Friedrich I. zum „König in Preußen“ zu krönen. In einer Standeserhebung erhob dieser sein Herzogtum Preußen zum "Königreich Preußen".

1722 wurde eine neue Gebietseinteilung geschaffen, die bis 1808 Bestand hatte: Es wurden zwei Kammerdepartements geschaffen, die direkt dem Generaldirektorium in Berlin unterstanden, die "Ostpreußische" oder "Deutsche Domänenkammer zu Königsberg" und die "Littauische Domänenkammer zu Gumbinnen". In beiden Bezirken gab es Immediatstädte, Mediatstädte, Domänenämter und adelige Güter. Zur effektiveren Verwaltung der Einkünfte und der Marsch- und Einquartierungsaufgaben waren für die Immediatstädte, die nämlich eigene Justiz- und Kameral-Abteilungen hatten, "steuerrätliche Kreise" und für die anderen Städte sowie das „platte Land“ "landrätliche Kreise" eingerichtet. 

Während des Siebenjährigen Kriegs (1756–1763) wurde Ostpreußen vorübergehend von russischen Truppen besetzt. 

1772 wurde das bis dahin unter polnischer Hoheit stehende Ermland in mehrere Kreise geteilt und diese dem Königsberger Kammerdepartement nachgeordnet, dafür die westlichsten altpreußischen Kreise dem neugeschaffenen "Kammerdepartement Marienwerder".

Von 1422 bis 1945 war Schirwindt der östlichste Vorposten Preußens und Deutschlands.





Bei der Ersten Teilung Polens 1772 erwarb Preußen unter Friedrich II. Polnisch-Preußen, das zu Westpreußen wurde. Das Gebiet des Fürstbistums Ermland verschmolz mit dem bisherigen „Königreich“, wobei dieses am 31. Januar 1773 in einem Verwaltungsakt die Bezeichnung "Ostpreußen" erhielt. Zwischen 1773 und 1792 bestand das „Königreich Preußen“ aus den Provinzen West- und Ostpreußen und dem Netzedistrikt. Hauptstadt Ostpreußens war bis zum Ende des Zweiten Weltkriegs Königsberg. Von 1824 bis 1829 waren Ost- und Westpreußen personell und von 1829 bis 1878 real in einer Provinz administrativ vereinigt. 1878 wurde diese wieder geteilt.

Bei der preußischen Verwaltungsreform von 1815 bis 1818 wurde eine Verwaltungseinteilung geschaffen, die im Wesentlichen bis 1905 bestand. Nun gehörte Memel (heute Klaipėda) zum Regierungsbezirk Königsberg.

Durch seine gemeinsame Grenze mit dem Russischen Reich und seine vorgeschobene geographische Lage wurde Ostpreußen im Ersten Weltkrieg zu einem entscheidenden Schauplatz der Ostfront; hier lagen die einzigen Gebiete des Deutschen Kaiserreiches, die während des Weltkriegs von fremden Truppen besetzt waren. Gekämpft wurde auch in kleinen Gebieten im Reichsland Elsass-Lothringen. Die verlustreichen Schlachten an der Westfront fanden auf französischem und belgischem Territorium statt.

Der russische Vormarsch wurde in der zweiten Schlacht von Tannenberg zum Stehen gebracht. Die verantwortlichen Generale Paul von Hindenburg und Erich Ludendorff legten hier die Grundlage zu ihrer großen Popularität, die sie während der Weimarer Republik auf unterschiedliche Weise nutzten: Hindenburg als konservativer Reichspräsident, Ludendorff als Putschist und Verbündeter Adolf Hitlers.

Zu Beginn des Krieges war Ludwig von Windheim Oberpräsident in Ostpreußen. Kränklich und dem Krieg nicht gewachsen, wurde er auf militärisches Drängen hin abgelöst. Nachfolger wurde Adolf von Batocki. Von der kurzen russischen Besetzung Ostpreußens blieben nur Königsberg und fünf Landkreise verschont. Die Schäden waren enorm: 39 Städte und etwa 1900 Dörfer waren verwüstet. Batocki schrieb: „Die Äcker verdorben, der in höchster Blüte stehende Viehstand vernichtet, die Betriebsanlagen zerstört, das Volk grausamlichst getötet und vertrieben, die Gebäulichkeiten zerschossen, zersprengt und verbrannt, so lag die Provinz da, als sie wieder in deutsche Hände kam.“ Batockis historische Verdienste um den Wiederaufbau der Provinz ließen ihn zum „Vater des Landes“ werden. Mitten im Krieg begann neben der staatlichen Wiederaufbauhilfe eine groß angelegte private Hilfsaktion. Die „Ostpreußenhilfe“ – nicht zu verwechseln mit Osthilfe (Deutsches Reich) – wurde Dachorganisation von zuletzt 61 Patenschaftsvereinen im ganzen Reich. Sie halfen bis Mitte der 1920er-Jahre beim Wiederaufbau Ostpreußens.

Durch den Versailler Vertrag, der am 10. Januar 1920 in Kraft trat und unter anderem die Abtretung deutscher Gebiete an Polen beinhaltete, wurde Ostpreußen durch den Polnischen Korridor geographisch vom übrigen Deutschen Reich abgetrennt. Als Exklave war es nur auf dem Seeweg oder über polnisches Gebiet zu erreichen. Das Weichseldelta wurde der unter Völkerbundsmandat geschaffenen Freien Stadt Danzig zugeteilt, die eigenständige staatliche Institutionen hatte, aber wirtschaftlich und militärisch mit Polen verbunden war, als Kompromiss zwischen dem polnischen Drängen auf einen leistungsfähigen Hafen und einer deutschen Bevölkerungsmehrheit von über 90 %.

Der südwestliche Teil des ostpreußischen Kreises Neidenburg musste ohne Volksabstimmung an Polen abgetreten werden, hauptsächlich weil der Hauptort Soldau (Działdowo) als Bahnknotenpunkt mit Verbindungen den direkten Verkehr zwischen Warschau und Danzig ermöglichte (→ Marienburg-Mlawkaer Eisenbahn). Daraus wurde der neue Powiat Działdowo (Kreis Soldau) gebildet, der der polnischen Woiwodschaft Pommern angeschlossen wurde.

Der Artikel 28 des Versailler Vertrages bestimmte die Grenzen Ostpreußens neu. Die westliche Grenze verlief nun entlang der Weichsel und Nogat und schloss damit Gebiete von fünf bisher westpreußischen Kreisen und die Stadt Elbing ein. Die Bestimmung enthielt den Vorbehalt möglicher Änderungen infolge der Ergebnisse der im Artikel 94 bis 98 festgelegten Abstimmungen über die zukünftige Staatszugehörigkeit. Es wurden zwei Abstimmungsgebiete gebildet.


Der Versailler Vertrag sicherte in Artikel 89 dem Deutschen Reich die ungehinderte Durchfahrt nach Ostpreußen zu. Konkretisiert wurde das Durchfahrtsrecht für die Eisenbahn zunächst Ende 1920 in einem provisorischen Abkommen, das am 21. April 1921 durch ein endgültiges Abkommen ersetzt wurde. Dennoch war der Verkehr zwischen dem deutschen Kernland und der Provinz Ostpreußen auf dem Landweg problematisch. Der Bahnverkehr erfolgte mit verplombten Zügen, bei denen in den ersten Jahren die Fenster zugehängt wurden und nicht geöffnet werden durften. Ab Ende der 1920er-Jahre wurden die restriktiven Bestimmungen allmählich gelockert. 1939 bedienten neun tägliche und zwei saisonale D-Zug-Paare sowie etwa 20 Güterzugpaare den Verkehr von und nach Ostpreußen. Auch der Straßenverkehr, für den feste Transitstraßen ausgewiesen und von Polen Visums- und Straßenbenutzungsgebühren erhoben wurden, war immer wieder beeinträchtigt. 1922 wurde vom Reichsverkehrsministerium daher der Seedienst Ostpreußen eingerichtet, der über den Seeweg eine Verbindung zwischen Ostpreußen und dem Kernland des Deutschen Reiches unter Umgehung polnischer Kontrollen herstellte. Der Seedienst Ostpreußen bestand bis 1939.

Das Verhältnis zwischen der Weimarer Republik und Polen war in der Zwischenkriegszeit generell angespannt. Vor allem in den ersten Jahren kam es entlang der gemeinsamen Grenze zu Auseinandersetzungen, auch mit Waffeneinsatz. Die Abtrennung Ostpreußens wurde in der Weimarer Republik parteiübergreifend als ungerecht und Verstoß gegen das Selbstbestimmungsrecht angesehen. Reichsaußenminister Gustav Stresemann ging daher nie auf die verschiedenen polnischen Vorschläge ein, analog zu den Verträgen von Locarno ein „Ost-Locarno“ abzuschließen und die Grenze zu Polen als unverletzlich zu erklären.

Bei der letzten demokratischen Reichstagswahl im März 1933 erzielte die NSDAP in Ostpreußen mit 56,5 % den größten Stimmenanteil in einem Wahlkreis des Deutschen Reichs. Zusammen mit den Stimmen der DNVP stimmten die wahlberechtigten Ostpreußen zu 67,8 % für rechtsextreme Parteien. Der zweithöchste Wert, der nur vom Wahlkreis Pommern übertroffen wurde (mit 73,3 %). Zum Vergleich: Im Wahlkreis Köln-Aachen erzielten alle rechtsextremen Parteien 35,7 %.

Gauleiter und damit eigentlicher lokaler Machthaber in Ostpreußen wurde der aus dem Rheinland stammende Erich Koch. Für die deutschen Forderungen nach Wiederanschluss Danzigs und Rückgabe des Korridors signalisierten die Westmächte, die sich im Zuge der Appeasementpolitik zuvor meist nachgiebig gegenüber dem Revisionsstreben Hitlers gezeigt hatten, 1939 Unnachgiebigkeit und drohten mit Krieg.

Ein knappes halbes Jahr später begann mit dem Überfall auf Polen der Zweite Weltkrieg.
Nach der schnellen Besetzung des Landes wurden neben den 20 Jahre zuvor abgetretenen preußischen Provinzen Westpreußen und Posen weitere Teile Polens annektiert. Noch 1939 wurde dort ein neuer Regierungsbezirk Zichenau gebildet, der der Provinz Ostpreußen zugeordnet wurde. Ferner trat der neue Landkreis Sudauen zur Provinz, während die früher westpreußischen Gebiete um Elbing und Marienwerder an den neuen Reichsgau Danzig-Westpreußen fielen. Die neu an Ostpreußen angegliederten Gebiete waren jedoch ethnisch praktisch rein polnische Gebiete, die auch historisch nie zuvor in engerer Verbindung mit Ostpreußen gestanden hatten (abgesehen von einer kurzen Episode nach den polnischen Teilungen). Der erhebliche jüdische Bevölkerungsanteil wurde unmittelbar nach der Besetzung von den nationalsozialistischen Unterdrückungs- und später von den massenhaften Vernichtungsmaßnahmen (Umsiedlungen in Ghettos, „Vernichtung durch Arbeit“ und den Abtransport in Vernichtungslager) getroffen.

Gegen Ende des Zweiten Weltkrieges wurde Ostpreußen von der Roten Armee nach verlustreichen Kämpfen in der Schlacht um Ostpreußen erobert. Die nationalsozialistische Gauleitung unter Gauleiter Erich Koch unterließ die rechtzeitige Evakuierung der Bevölkerung und stellte selbständige Fluchtbewegungen unter schwere Strafe. Ähnlich wie Soldaten „bis zum letzten Mann“ in sinnlosen Stellungs- und Kesselschlachten verheizt wurden, anstatt sich geordnet zurückziehen zu dürfen, machten sich die Machthaber somit direkt mitschuldig am Tod von unzähligen deutschen Zivilisten, die hätten gerettet werden können.

Als die Front des Zweiten Weltkrieges Ostpreußen erreichte, wurde die Evakuierung durch das Militär und den Staatsapparat zunächst behindert bzw. verhindert (u. a. durch Verordnungen), dann in letzter Minute (Januar 1945) unter denkbar schlechtesten Bedingungen (tiefster Winter, Abschnürung des Landweges) ungeordnet begonnen. Dadurch war ein Großteil der Zivilbevölkerung unmittelbar Kampfhandlungen ausgesetzt.

Ein Teil der Bevölkerung konnte sich auf dem Landweg mit Pferdefuhrwerken (die in Flüchtlingstrecks zogen) nach Westen retten. Aber nachdem die Rote Armee im Laufe der Schlacht um Ostpreußen bei Elbing das Frische Haff erreicht hatte, war der Landweg abgeschnitten. Tausende ertranken bei der Flucht über das Eis zur vermeintlich rettenden Frischen Nehrung, die weiter nach Danzig führte, oder wurden ohne jegliche Deckung Opfer von Jagdflugzeugen, die gezielt auf die Trecks schossen.
Ein anderer Teil wurde über die Ostsee (vor allem über den Hafen Pillau) evakuiert. Die Evakuierung wurde am 21. Januar 1945 durch Großadmiral Karl Dönitz eingeleitet; die Maßnahme bekam später den Namen "Unternehmen Hannibal".

Insgesamt forderte die Flucht unter Kriegsbedingungen größtenteils im Winter sehr viele Tote. Es wird geschätzt, dass von den bei Kriegsende etwa 2,4 Millionen Bewohnern Ostpreußens ungefähr 300.000 unter elenden Bedingungen auf der Flucht ums Leben gekommen sind. Unter den Menschen, die bei den Versenkungen der "Wilhelm Gustloff", der "General von Steuben" und der "Goya" im Frühjahr 1945 starben, befanden sich auch viele Flüchtlinge aus Ostpreußen, einige Tausend pro Schiff.

Noch anwesende Bewohner, vom Vormarsch der Roten Armee eingeholte Flüchtlinge oder nach dem (teils temporären) Ende der Kampfhandlungen zurückkehrende Bewohner wurden vielfach von sowjetischen Soldaten, misshandelt, vergewaltigt und getötet oder zur Zwangsarbeit in der Sowjetunion verschleppt. In diesem Kontext ist beispielsweise das Massaker von Nemmersdorf im Oktober 1944 zu nennen, als erstmals seit August 1914 russische Truppen nach Ostpreußen vorstießen. Alexander Solschenizyn "(Ostpreußische Nächte)" und Lew Kopelew waren als Angehörige der Roten Armee Augenzeugen und haben später als Dissidenten auf diese und andere sowjetische Kriegsverbrechen (z. B. die Massenerschießungen polnischer Offiziere im Massaker von Katyn) hingewiesen. Die Verantwortlichen wurden im Hinblick auf die weltpolitische Lage weder international noch in der Sowjetunion zur Verantwortung gezogen.

Die Bewohner Ostpreußens sind von 1945 bis 1947 zu über 90 % aus ihrer Heimat in das besetzte Deutschland westlich der Oder-Neiße-Linie vertrieben worden. Im südlichen Teil unterzogen polnische Behörden die verbliebenen Einwohner einer auf ethnischen Kriterien beruhenden „nationalen Verifizierung“. Als „Deutsche“ eingestufte Personen wurden vertrieben, „Autochthone“ – das heißt Angehörige der nach Auffassung der polnischen Behörden angestammten slawischen Bevölkerung – durften bleiben. Ausreichend für die Einstufung als „autochthon“ war hierbei bereits ein polnisch-klingender Nachname oder masurische oder polnische Sprachkenntnisse innerhalb der Familie. Facharbeitern wurde ebenfalls ein Bleiberecht eingeräumt, um Fabriken wieder besser in Betrieb nehmen zu können.

Bis zum Oktober 1946 waren 70.798 Personen in dieser Form „verifiziert“, d. h. polnische Staatsbürger geworden, 34.353 verblieben „unverifiziert“. Vor allem im Raum Mrągowo (Sensburg) verweigerten viele Einwohner diesen Verifizierungsprozess, im Frühjahr 1946 waren hier von 28.280 Personen 20.580 nicht „verifiziert“, im Oktober verblieben 16.385 Menschen ohne polnische Staatsbürgerschaft. Auch die eingebürgerten „Autochthonen“ wurden aufgrund ihres vorwiegend evangelischen Glaubens und ihrer oft rudimentären Sprachkenntnisse weiterhin als Deutsche betrachtet und Diskriminierungen unterworfen. Im Februar 1949 wurde der ehemalige Chef der stalinistischen Geheimpolizei Urząd Bezpieczeństwa (UB) von Lodz, Mieczysław Moczar, Wojwode von Olsztyn. Es begann eine letzte, von brutaler Folter und Gewalt gekennzeichnete „Verifizierungsaktion“, nach deren Abschluss lediglich noch 166 Masuren nicht „verifiziert“ waren.

Insgesamt verblieben etwa 160.000 Vorkriegseinwohner im südlichen Ostpreußen, deren übergroße Mehrheit das Land in den folgenden Jahrzehnten als Spätaussiedler verließ. Das nördliche Ostpreußen fiel an die Russische Sowjetrepublik und wurde als Oblast Kaliningrad zu einem Militärsperrbezirk, in den selbst Sowjetbürger nur mit Sondergenehmigung einreisen konnten.

Nach dem Potsdamer Abkommen wurde Ostpreußen vorbehaltlich einer endgültigen Friedensregelung (→ Zwei-plus-Vier-Vertrag) zwischen der Volksrepublik Polen und der Sowjetunion aufgeteilt. Das nördliche Gebiet um Königsberg wurde daraufhin von der Russischen Sowjetrepublik annektiert. Es wurde überwiegend mit Russen aus Zentralrussland und dem Gebiet des heutigen Föderationskreises Wolga sowie mit Weißrussen und Ukrainern besiedelt. Der polnische Anteil östlich der Oder und Neiße wurde auf die neu gegründeten Woiwodschaften Gdańsk, Olsztyn und Suwałki aufgeteilt. Hier wurden in erster Linie Polen aus Zentralpolen und im Rahmen der Aktion Weichsel aus Südostpolen vertriebene Ukrainer angesiedelt. Die Hauptstadt Königsberg wurde 1946 zu Ehren des sowjetischen Politikers Michail Kalinin in Kaliningrad umbenannt; ebenso wurden sämtliche Orte im sowjetischen Anteil – sofern sie nicht aufgelöst oder zu größeren Einheiten zusammengefasst wurden – umbenannt.

Die DDR erkannte 1950 die Oder-Neiße-Linie im Görlitzer Vertrag als ihre Grenze zu Polen an. Dieser Anerkennung wird allerdings vielfach die völkerrechtsverbindliche Wirkung abgesprochen. Auch die Bundesrepublik Deutschland, welche den Alleinvertretungsanspruch für Gesamtdeutschland und alle Deutschen, also bis Anfang der 1970er Jahre auch für das Staatsgebiet der DDR erhob, verfolgte unter Bundeskanzler Willy Brandt im Rahmen der „Neuen Ostpolitik“ fortan die Anerkennung der Grenzziehung vorbehaltlich eines endgültigen Friedensvertrages (→ Ostverträge). Zur Zeit der „Zwei-plus-Vier“-Gespräche soll in Moskau Joachim von Arnim, der Leiter des politischen Referats der deutschen Botschaft, in einem Gespräch dem sowjetische Generalmajor Geli Batenin, der ein Interesse der Sowjetunion an Verhandlungen über Ostpreußen signalisiert habe, entgegnet haben, es ginge bei den Verhandlungen nur „um die Bundesrepublik Deutschland, die DDR und das ganze Berlin“. Nach dem Beitritt der DDR zur Bundesrepublik und der Bildung der neuen Länder gab das nun souveräne Deutschland am 14. November 1990 mit dem deutsch-polnischen Grenzvertrag jegliche Gebietsansprüche außerhalb der Bundesrepublik auf. Spätestens mit dessen Inkrafttreten 1992 sind deutsche Gebietsansprüche auf die ehemaligen deutschen Ostgebiete, und damit auch auf Ostpreußen, vollständig erloschen und die Grenzen endgültig anerkannt worden.

Nach der Verwaltungsreform 1975 wurde das polnische Ostpreußen in neue Woiwodschaften eingeteilt: Elbląg und Olsztyn sowie Teile von Ciechanów und Suwałki. Nach einer erneuten Verwaltungsreform am 1. Januar 1999 im polnischen Südteil bildet dieses Gebiet seither fast in seiner Gesamtheit die Woiwodschaft Ermland-Masuren mit der Hauptstadt Olsztyn; das frühere Nordostpreußen bildet heute die russische Oblast Kaliningrad mit der Hauptstadt Kaliningrad. Nach der Auflösung der Sowjetunion ist diese Region nun eine Exklave der Russischen Föderation. Einige russische Einwohner nennen die Stadt heute häufig „Kjonigsberg“, „Kenig“ oder „Kenigsberg“. Eine Rückbenennung (wie bei Sankt Petersburg, Nischni Nowgorod und Twer) wurde 1993 in einer Volksabstimmung abgelehnt.

In der Zeit von 1878 bis 1945 hat sich die territoriale Verwaltungsgliederung innerhalb der überwiegend landwirtschaftlich strukturierten Provinz Ostpreußen nur allmählich verändert. Allerdings sind 1920 und 1939 die Außengrenzen erheblich verändert worden.

Von 1808 bis 1945 bestanden die beiden Regierungsbezirke Gumbinnen und Königsberg. Aus den südlichen Kreisen dieser Bezirke entstand am 1. November 1905 der neue Regierungsbezirk Allenstein. 1723–1808 hießen diese Bezirke Kriegs- und Domänenkammer-Departement Litauen und Ostpreußen.

Nach der Einrichtung des polnischen Korridores wurde der früher westpreußische Regierungsbezirk Marienwerder teilweise, gemeinsam mit einigen Kreisen aus dem ehemaligen Regierungsbezirk Danzig (Elbing und Marienburg) zum 1. Juli 1922 als Regierungsbezirk Westpreußen mit dem Sitz in Marienwerder der Provinz Ostpreußen angegliedert, aber am 26. Oktober 1939 um annektierte, polnische Gebiete erweitert und wieder als Regierungsbezirk Marienwerder dem neuen Reichsgau Danzig-Westpreußen zugeordnet.

Am 26. Oktober 1939 wurde aus anderen polnischen Gebieten der neue Regierungsbezirk Zichenau "(Ciechanów)" der Provinz Ostpreußen einverleibt. Den "nicht" förmlich nach Ostpreußen eingegliederten Bezirk Bialystok, der am 1. August 1941 aus den Gebieten der weißrussischen Sowjetrepublik, die bis 1939 zu Polen gehört hatten, gebildet worden war, verwaltete der ostpreußische Oberpräsident und Gauleiter Erich Koch als Chef der Zivilverwaltung faktisch wie ein Reichsgebiet.

Außer dem bereits 1818 bestehenden Stadtkreis Königsberg i. Pr. entstanden im Laufe der Zeit die folgenden weiteren Stadtkreise: Es wurden die Städte Tilsit (1896), Insterburg (1901), Allenstein (1910) und Memel (1918) aus ihren Landkreisen ausgegliedert und bildeten eigene Stadtkreise. Das westpreußische Elbing war bereits seit 1874 Stadtkreis und gehörte von 1922 bis 1939 zu Ostpreußen.






1765 wurde Johann Friedrich von Domhardt Präsident der Gumbinner und Königsberger Kriegs- und Domänenkammern und damit der erste Oberpräsident in Ostpreußen. Ihm folgte 1791 Friedrich Leopold von Schrötter, der 1795 Minister für Ost- und Neu-Ostpreußen wurde. 1814–1824 war Hans Jakob von Auerswald Oberpräsident von Ostpreußen. Unter seinem Nachfolger Theodor von Schön (1824–1842) wurden West- und Ostpreußen zur Provinz Preußen vereinigt. Ihm folgten



An 100 % fehlende Stimmen = Nicht im Provinziallandtag vertretene Wahlvorschläge.

Die Provinz bildete für die Wahlen zum Reichstag der Weimarer Republik den Wahlkreis 1.

Bis 1945 war die Wirtschaft Ostpreußens überwiegend agrarisch geprägt. Bodenschätze fehlten nahezu. Aufgrund der geringen Bevölkerungsdichte von gebietsweise nur knapp 50 Menschen je km² (Stand: 1938) war der land- und forstwirtschaftliche Sektor auf den Export seiner Überschüsse angewiesen. 

Als fruchtbar galten die Niederungsgebiete zwischen der Nogat und der Memel sowie ein Teil des Baltischen Landrückens, oft mit guten Lehmböden. Andere Gebiete besaßen mitunter nur dürftigen Sandboden. Die Bewässerung über Seen und Flüsse glich den Mangel an Niederschlägen meist aus. 

Nachteilig war das verhältnismäßig kühle Klima, das z. B. die mittlere Januartemperatur im Südosten 5° unter Null lag. Die Obstblüte begann meist erst Ende Mai, auch das Getreide war spät erntereif. Darum lohnte es sich nicht, zwischen der Ernte des Sommergetreides und der Aussaat des Wintergetreides noch eine Zwischenfrucht zu pflanzen. Haupterzeugnisse waren Roggen und Kartoffeln. Schwach ausgebildet waren der Anbau von Flachs (Königsberg, Insterburg, Allenstein) und Tabak (Elbing). 

Profitabel war die Viehwirtschaft, so die extensive Rinderzucht und damit verbunden die Herstellung von Molkereiprodukten in der Region um Tilsit. Im Süden Ostpreußens verlegte man sich indes auf die reine Fleischproduktion, mit der Aufzucht von „Magervieh“ (Mastvieh), Schafen und Gänsen. Hinzu kam die Pferdezucht, wobei sich das Hauptgestüt Trakehnen einen internationalen Ruf erwarb.

Die Forstwirtschaft profitierte von den üppigen Laubholzbeständen im Gebiet der Seenplatte; von Bedeutung waren ebenso die Kiefernwälder im Raum Rominten-Johannisburg.

Der Bernstein zählte zu den wenigen Bodenschätzen Ostpreußens, gab aber nur einigen tausend Menschen Arbeit. Er wurde im Tagebau bei Palmnicken gewonnen und in der Manufaktur in Königsberg verarbeitet. Das Fehlen von Steinkohle als Energieträger war ein Hindernis für den Aufbau einer nennenswerten Industrie. Das geringe Gefälle der Tieflandflüsse machte auch die Nutzung der Wasserkraft nahezu unmöglich.

Darum beschränkte sich das Gewerbe fast ausschließlich auf die Verarbeitung der land- und forstwirtschaftlichen Rohzeugnisse in Mühlen, Brennereien, Stärkefabriken und Sägewerken. Zwei Ausnahmen waren der Eisenbahnbau in Elbing und der Schiffsbau in Königsberg.

Hinderlich war das unzureichende Verkehrswegenetz. Die Flüsse waren bis zu vier Monate vereist und konnten nur von Fahrzeugen bis zu 400 Tonnen genutzt werden, der Oberländische Kanal verkraftete gar nur Kähne bis maximal 100 Tonnen. Den Meereszugang behinderte zudem die verhältnismäßig starke Dünenbildung an der Küste.



Die ostniederdeutschen und ostmitteldeutschen Dialekte, die in Ostpreußen gesprochen wurden, werden im Preußischen Wörterbuch erfasst und beschrieben.

Das von den Ureinwohnern gesprochene, baltische Altpreußisch war im 17. Jahrhundert ausgestorben.Im Jahr 1925 gaben 97,2 % der Einwohner Deutsch, 1,8 % Masurisch, 0,9 % Polnisch und 0,1 % Litauisch als Muttersprache an. Auf den Nehrungen wurde unter Fischern Nehrungskurisch gesprochen.








</doc>
<doc id="7916" url="https://de.wikipedia.org/wiki?curid=7916" title="Baruch de Spinoza">
Baruch de Spinoza

Baruch de Spinoza (, , latinisiert , geboren am 24. November 1632 in Amsterdam, gestorben am 21. Februar 1677 in Den Haag) war ein niederländischer Philosoph und Sohn portugiesischer Immigranten sephardischer Herkunft und portugiesischer Muttersprache. Er wird dem Rationalismus zugeordnet und gilt als einer der Begründer der modernen Bibel- und Religionskritik.

Die marranisch-jüdische Familie Spinoza (auch "Despinosa" oder "d’Espinosa" geschrieben) stammte von iberischen Juden (Sephardim) ab, die aus Vidigueira in Portugal, via Nantes und Rotterdam, eingewandert waren. Wahrscheinlich sind Spinozas Vater und Onkel zwischen 1615 und 1623 nach Amsterdam gezogen.

Spinoza wurde am 24. November 1632 als "Bento de Espinosa" in einem Haus im Amsterdamer "Judenviertel", heute Waterlooplein und Umgebung, geboren. Acht Tage später wurde er in der jüdischen Gemeinde als "Baruch" eingeführt. Sein Vater war Miguel oder Michael de Spinoza (gestorben 1654), auch als Gabriel Alvares d’Espinosa bekannt, seine Mutter, dessen zweite Frau, war Hanna Debora Senior (gestorben 1638). Michael de Spinoza war mehrere Male einer der "Parnassim", Aufseher der sephardischen Gemeinde, und war an der Zusammenführung der drei Schulen und der Gründung der (alten) sephardischen Synagoge an der Houtgracht beteiligt.

Über Spinozas Jugend ist zuverlässig nur bekannt, dass er im Alter von fünf Jahren mit dem Vater, seinem älteren Bruder Isaak (gestorben 1649) und dem jüngeren Bruder Gabriel in das Mitgliederverzeichnis des Fördervereins Ets Haim eingeschrieben wurde, der zur Vergabe von Stipendien an die Schüler der Schule Talmud Tora gegründet wurde. In dieser Schule wurden die meisten männlichen Gemeindemitglieder in den ersten vier Klassen in die religiöse Kultur der Gemeinde eingewiesen, bevor einige die Klassen 5–7 durchliefen, um zu Gemeindevorstehern, vor allem aber zu Rabbinern ausgebildet zu werden. Da Spinoza als 18- oder 19-Jähriger in einer Mitgliederliste der Klassen 5–7 aus dem Jahre 1651 nicht vorkommt, hat er diese höheren Klassen wahrscheinlich nicht besucht.

In den Gemeindebüchern kommt er erst wieder nach dem Tode seines Vaters (März 1654) vor, und zwar in dem Spendenbuch, dem zufolge er im Monat nach dem Tod des Vaters und als dessen Nachfolger (ältester noch lebender Sohn) mehrere Zahlungen leistete. Spinoza betrieb in Nachfolge seines Vaters dessen Handelsunternehmen. Als er im Frühjahr des folgenden Jahres die Verschuldung des vom Vater übernommenen Geschäfts erkannte, ließ er sich als 23-Jähriger – und damit nach geltendem Recht noch minderjährig – als Vollwaise einen Vormund bestellen. Dieser machte für ihn die nachträgliche Nichtannahme der Erbschaft geltend, obwohl Spinoza bereits einige Gläubiger seines Vaters befriedigt hatte. Die Ablehnung der Erbschaft wurde von einem Amsterdamer Gericht als rechtsgültig anerkannt. Spinoza entledigte sich damit aller finanziellen Verbindlichkeiten gegenüber den Geschäftspartnern seines Vaters. Das Unternehmen wurde aber unter gleichbleibender Firma bis 1664 fortgeführt. In diesem Jahr bevollmächtigte sein Bruder Gabriel als Alleininhaber vor der Auswanderung nach Barbados zwei andere Kaufleute damit, die Interessen des Geschäfts wahrzunehmen. Gabriel wanderte nach Jamaika aus, wohin alte Geschäftsbeziehungen bestanden.

Wohl in der ersten Hälfte der 1650er Jahre kam Spinoza in Kontakt mit Mennoniten. In der Lateinschule des Ex-Jesuiten Franciscus van den Enden (1602–1674) lernte er Latein. Er konnte hier seinen Gesichtskreis erweitern und wurde unter anderem mit dem Gedankengut von Descartes und der Spätscholastik bekannt. Die jüdischen Rationalisten wie Maimonides oder Gersonides waren ihm vermutlich schon zuvor vertraut.

Im Jahre 1656 äußerte Spinoza zusammen mit dem erst 1655 aus Portugal über Hamburg in die Gemeinde zugewanderten Arzt und Freidenker Juan de Prado und mit Manuel Ribeira starke Zweifel an verschiedenen für die Gemeinde zentralen Glaubenslehren. Am 27. Juli 1656 wurde er dann wegen seiner angeblich schlechten Ansichten und Handlungen und nachdem mildere Maßnahmen nichts genutzt hatten, von der Amsterdamer portugiesischen Synagoge mit dem Bann (Cherem) ausgeschlossen. Zusätzlich verboten die Rabbiner jeden schriftlichen oder mündlichen Kontakt mit ihm. Spinoza war zu diesem Zeitpunkt erst 23 Jahre alt und hatte noch nichts veröffentlicht. Nach dem Bann verfasste Spinoza eine umfangreiche Verteidigungsschrift, in der er seine bibel- und religionskritischen Ansichten entwickelte, die er später in den theologisch-politischen Traktat aufnahm.

Spinoza hielt sich häufig und noch während des Jahres 1659 in Amsterdam auf und verkehrte weiter mit de Prado und Ribeira. Der Biograph Lucas berichtet, dass er auf Betreiben der Rabbiner vom Magistrat für einige Zeit aus Amsterdam verwiesen wurde und sich deshalb in Rijnsburg niederließ. Allerdings existieren darüber keine amtlichen Nachrichten oder weitere Berichte. Für einen Wohnsitz außerhalb Amsterdams spricht die Erwähnung eines Studiums in Leiden 1658/1659 durch den Zeugen der spanischen Inquisition, Tomás Solano y Robles.

Um seinen Lebensunterhalt zu sichern, beschäftigte er sich recht erfolgreich mit der Herstellung von Mikroskopen und Ferngläsern.

Bereits um 1660 war Spinozas bibel- und religionskritische Haltung auch in Rijnsburg bekannt. Er arbeitete am "Tractatus de intellectus emendatione" (Über den Fortschritt des Verstehens) und "Korte Verhandeling van God, de Mensch, en deszelos Welstand" (Kurzer Traktat von Gott, dem Menschen und seinem Glück), in der schon Ideen seines späteren Hauptwerks "Ethik…" anklingen. Sein Ruf als scharfsinniger Kenner und sein eigenwilliges Weiterentwickeln der Philosophie Descartes’ zog das Interesse vieler Gelehrter auf sich. So hatte er Kontakt mit Henry Oldenburg, der später einer der Sekretäre der neu gegründeten Royal Society in London werden sollte.

1663 veröffentlichte Spinoza die "Renati Descartes principiorum philosophiae" (PPC), das einzige Werk, das zu seinen Lebzeiten unter seinem Namen erschien. 1669 zog er nach Den Haag. Hier erhielt er im Februar 1673 einen Ruf als Professor an die kurpfälzische Universität Heidelberg, der jedoch von dem beauftragten Vertrauten des Kurfürsten Karl I. Ludwig so abgefasst worden war, dass Spinoza ihn ablehnte.

Seit 1670 bemühte sich die Kirche bei den staatlichen Stellen, ein Verbot von Spinozas im selben Jahr und anonym erschienenen "Tractatus theologico-politicus (TTP)" durchzusetzen, was jedoch erst 1674, zwei Jahre nach der Ermordung der liberalen Regenten, der Brüder de Witt, Erfolg hatte. 1675 wurde die Kirchengemeinde im Haag erneut tätig, da das Gerücht umging, Spinoza habe ein neues Buch fertiggestellt; dabei kann es sich nur um die "Ethik" gehandelt haben. In Den Haag erhielt Spinoza Besuch bedeutender Gelehrter, darunter Ehrenfried Walther von Tschirnhaus und Gottfried Wilhelm Leibniz, die vom "Tractatus theologico-politicus" stark beeindruckt waren.

Spinoza starb plötzlich im Alter von 44 Jahren am Samstag den 21. Februar 1677 um 3 Uhr Nachts in seinem Zimmer in einem Gasthaus an der Paviljoensgracht in Den Haag. Die Umstände seines Todes sind nicht näher bekannt, vielleicht aber war seine lebenslange Tuberkulose die Ursache, damals diagnostiziert als "Schwindsucht". Über seinen Tod und sein Begräbnis schrieb J. Colerus in "La vie de B. Spinoza" von 1706, der seine Wirtsleute im Gasthaus dazu persönlich befragt hat. Demnach hatte er um 4 Uhr Nachmittag nach einem Arzt mit den Initialen L. M. verlangt, der bei ihm am Bette blieb bis er verstarb. Am 25. Februar wurde er begraben. Der Nachlass einschließlich seiner Bibliothek wurde inventarisiert und versteigert, nachdem Spinozas Schwester Rebecca und ihr Stiefsohn Daniel de Casseres ihre Erbansprüche geltend gemacht und auch sein Vermieter ausstehende Zahlungen eingefordert hatten. Der "Tractatus politicus" blieb unvollendet.

Freunde wie Lodewijk Meyer bereiteten Spinozas nachgelassene Manuskripte zur Veröffentlichung vor. Diese erfolgte noch im Todesjahr 1677 unter dem Titel "B. D. S. Opera Posthuma". Das Buch enthielt die "Ethik", den "Tractatus politicus", den "Tractatus de intellectus emendatione" sowie Briefe und seine gleichfalls unvollendete hebräische Grammatik. Autographen von Spinoza werden unter anderem in der Gottfried Wilhelm Leibniz Bibliothek aufbewahrt.

Spinoza nimmt in der Philosophiegeschichte eine Sonderstellung ein. Er gehörte weder einer etablierten philosophischen Schule an, noch begründete er selber eine neue. Er war einer der radikalsten Philosophen der frühen Neuzeit. Seine "Ethica, ordine geometrico demonstrata" ist der Form nach in "synthetischer" Darstellung und, wie es der Titel andeutet, nach der Methode von Euklids Elementen in „Grundbegriffen“, „Axiomen“, „Theoremen“, „Demonstrationen“ und „Korollarien“ abgefasst, wodurch sie den Anschein unumstößlicher Gewissheit erweckte. Spinoza verfasste eine Metaphysik und Ethik in der Art eines Geometrielehrbuches.

Die Philosophie Spinozas hat vor allem ein ethisch-praktisches Ziel: Er möchte von den illusorischen Lebenszielen das einzig Wahre unterscheiden, das ihm, wenn er es erreichen würde, eine stabile und wirklich befriedigende Freude verschaffen könnte. Um dies zu ermöglichen, entwickelte er eine Ethik (vor allem in den drei letzten Büchern der "Ethik"), deren Grundlagen (die in den ersten beiden Büchern der "Ethik" dargelegt werden) metaphysischer Natur sind. Die ethischen und metaphysischen Reflexionen forderten eine propädeutisch-methodologische Arbeit, der Spinoza sich im "Tractatus de intellectus emendatione" unterzog. Da aber die Ethik in seinen Augen von der politischen Philosophie untrennbar ist, entwickelte er sowohl im Rahmen des "Tractatus theologico-politicus" als auch des "Tractatus politicus" ein eigenständiges politisches Denken.

Die vier Zweige des Denkens Spinozas sind:


In den Propositionen 1–15 hielt er fest: Gott ist die unendliche, substantiell in ihren Eigenschaften konstante, einheitliche und ewige Substanz:

Spinoza kombiniert das traditionelle Verständnis der Substanz als „In-sich-Sein“ "(in se est)" mit der Feststellung, dass eine Substanz nur aus sich allein begriffen werden könne "(per se concipitur)" bzw. erklärbar sei.

Aus diesen beiden Axiomen Spinozas folgt zwingend, dass bei Annahme mehrerer voneinander unterschiedener Substanzen etwas diesen Gemeinsames zugrunde liegen muss, da sich die Substanzen ohne ein Gemeinsames nicht voneinander unterscheiden lassen. Die Definition einer einzelnen Substanz könne nur über ihre Unterschiedenheit "(differentia)" von den übrigen Substanzen erfolgen. Damit wäre aber keine Substanz mehr aus sich heraus begreifbar, sondern nur in Bezug zu den übrigen.

Daraus ergibt sich unter Annahme von Spinozas Satz „vom aus sich heraus zu begreifenden Seienden“, dass es nur eine einzige Substanz geben könne. Diese Substanz ist daraus folgend mit all ihren Eigenschaften unendlich und absolut und wurde von Spinoza mit Gott gleichgesetzt.

Der Einwand einer möglichen endlichen Substanz wird durch zwingende Schlussfolgerungen aus den ersten beiden Axiomen Spinozas zur Substanz widerlegt.

Eine endliche Substanz müsste wiederum an eine andere Substanz angrenzen, was die oben behandelten Definitionsprobleme der unmöglichen Differenzierung von Substanzen nach dem Axiom "per se concipitur" aufwerfen würde.

Eine endliche Substanz benötigte außerdem einen kausal vorhergehenden Verursacher ihrer Existenz, was eine zweite Substanz zusätzlich zwingend erforderlich macht und wiederum entsprechende Probleme in Bezug auf die Anfangsaxiome aufwirft.

Spinoza folgerte, dass eine Substanz nicht von einer anderen hervorgebracht werden könne:

Bei der offenbleibenden Frage nach der wirklichen Existenz einer als Gott benennbaren Substanz verwendet Spinoza den älteren ontologischen Gottesbeweis, nach dem eine Substanz keine weitere Ursache haben darf und demnach nur als Ursache ihrer selbst "(causa sui)" vorzustellen wäre. Ursache einer Substanz selbst vermag hier aber nur etwas zu sein, bei dem das Wesen zugleich auch die Existenz impliziert "(cuius essentia involvit existentiam)" bzw. dessen Natur nicht anders begriffen werden kann denn als existierend "(cuius natura non potest concipi, nisi existens)".

Der Kosmos bzw. das Universum selbst ist diese Substanz, es gibt nichts außerhalb von ihr, sie ist in nichts Anderem, und somit sind alle Gegenstände Eigenschaften dieser Substanz; daher ist einer der Hauptgedanken bei Spinoza der, dass Gott in allem Seienden vorhanden ist. Es ist geläufig, diese Theorie Pantheismus zu nennen (vom Griechischen "pan": alles, und von "theos" Gott). Jedoch ergibt sich von Proposition 16 an ein subtiler Bedeutungswandel: Spinozas Gott ist die Ursache aller Dinge, weil alles ursächlich und notwendigerweise aus der göttlichen Natur folgt: „auf die selbe Weise, wie aus der Natur des Dreiecks von Ewigkeit und in Ewigkeit folgt, dass seine drei Winkel gleich zwei rechten sind“. In diesem Sinne war Gott auch nicht frei, die Welt zu erschaffen (oder es zu unterlassen).

Das, was unser Intellekt von dieser Substanz erkennen kann, nannte er ihre „Attribute“; zwei dieser Attribute sind „Denken“ (Geist) und „Ausdehnung“ (Materie). Gleichlautend mit Descartes konstatierte Spinoza also einen Gegensatz zwischen Geist und Materie; anders als jener sah er sie jedoch nicht als zwei verschiedene Substanzen (Dualismus), sondern als verschiedene Attribute einer einzigen Substanz (Monismus). Da Geist und Materie keine gegensätzlichen Substanzen sind, schien Spinoza der cartesianische Einwand gegen die Möglichkeit der Wechselwirkung zwischen Geist und Materie, Seele und Leib, beseitigt. Aus dem Grundgedanken des Monismus folgerte er, dass zwischen der (idealen) Gesetzmäßigkeit des Ideenreichs und der (mechanischen) der Körperwelt kein Gegensatz bestehen kann, sondern jeder Idee (von unendlich vielen) ein Gegenstand der körperlichen Welt entsprechen muss (psychophysischer Parallelismus).

Aus dem unendlichen Wesen Gottes ("natura naturans" = schöpferische Natur = die Substanz) folgt Unendliches auf unendlich unterschiedliche Weise ("natura naturata" = geschaffene Natur = was wir als Erscheinungen wahrnehmen). Dies gilt sowohl hinsichtlich der Folge und Verknüpfung der Ideen wie auch hinsichtlich der materiellen Weltordnung ("ordo et connexio idearum idem est ac ordo et connexio rerum"; „"Die Ordnung und Verknüpfung der Ideen ist dieselbe wie die Ordnung und Verknüpfung der Dinge."“). Daraus folgt: So wie in der Welt der materiellen Körper keine Wirkung ohne (zwingende) Ursache möglich ist, so ist in der Geisteswelt ein Willensentschluss ohne Motiv nicht möglich. Damit schloss Spinoza jede Willensfreiheit aus (auch die seines Gottes – siehe oben). Alles geschieht aus kosmischer Notwendigkeit; den Begriff „Wille Gottes“ nannte er (im Anhang zum 1. Teil der "Ethik") „das Asyl der Unwissenheit“: „Und so werden sie nicht ablassen, weiter nach den Ursachen der Ursachen zu fragen, bis man seine Zuflucht zum Willen Gottes genommen hat, das heißt, zur Freistatt der Unwissenheit.“

Manche Objekte entspringen unmittelbar dem unendlichen göttlichen Wesen; dies sind absolut gültige und unveränderliche geometrische Sätze und Naturgesetze bzw. die Logik und die Gesetzmäßigkeiten des Seelenlebens. Je weniger direkt die Verbindung zur göttlichen Substanz, desto individueller und auch vergänglicher ist ein Objekt.

Da nach Spinoza „die Substanz“ als solche weder Intelligenz noch Willen besitzt, gibt es keine Vorsehung, keinen Heilsplan; da sie Ursache ihrer selbst ist, gibt es auch kein blindes Verhängnis. Die Ethik geht zurück auf die „Ontologie“ Gottes, die Spinoza entwirft. Der Mensch kann Anteil an der göttlichen Natur haben, das Ziel ist eine Entwicklung der Welt gemäß der natürlichen Notwendigkeit der Gesetze Gottes. Die Ethik Spinozas verlangt, die Dinge so zu schauen, wie Gott sie schaut: ganzheitlich. Das bedeutet unter dem Gesichtspunkt der Ewigkeit (sub specie aeternitatis), jede Einzelheit (Idee, Gegenstand oder Vorgang) als Bestandteil eines einheitlichen Weltganzen zu sehen. Eine klare Abkehr von aristotelischen Vorstellungen ist Spinozas Behauptung, es gebe keine Zweckursachen, sondern lediglich wertneutrale, „wirkende“ Ursachen, die nur „notwendig“ seien. Alle Ursachen haben eine Dynamik zu Lebenserhaltung und zum „Nutzen“ (was hier aber nicht mit dem Utilitarismus verwechselt werden sollte).

Affektenlehre: Wenn vorige Begründungen auf Gott und die positive Dynamik der natura naturans zurückgeleitet werden können, so bringt auch der Mensch etwas in die Ethik ein. Spinoza konzipiert eine Lehre von Affekten und Leidenschaften. Diese werden als eine „Bejahung des Lebens“ verstanden. Spinoza entwickelte eine sehr genaue Theorie der Affekte, die Fragen der Anreize und Wirksamkeit von Affekten bearbeitet. Er unterscheidet zwischen angemessenen Affekten aktiver Gestaltung und inadäquaten Affekten (Ideen), die wir erleiden. Es geht darum, in den Ursachen nicht unterzugehen, nicht Knecht der Affekte zu werden, sondern sie zu gestalten. Demut ist keine Tugend: „Demut ist eine Trauer, die daraus entspringt, daß der Mensch seine Ohnmacht oder Schwachheit betrachtet.“

Das Gute ist die Erhaltung des Lebens und nicht eine welttranszendente Idee. Das Streben nach Selbsterhaltung führt nicht schon dazu, dass der Strebende sich auch tatsächlich selbst erhält. Das wahrhaft Gute entwickelt Spinoza im ausdrücklichen Kontrast zu diesem bloßen Meinen. Es ist „wahrer Nutzen“ und deshalb mehr: Das Gute ist nicht dasjenige, wovon wir eine gute Meinung haben, sondern etwas, um das wir wissen. Es ist nicht nur scheinbar, sondern wahrhaft nützlich. Dementsprechend könne nur wahrhaft gut genannt werden, was uns tatsächlich am Leben hält, und nicht, was wir meinend für unsere Selbsterhaltung erstreben. Wenn das Erstrebte zu einer Vernichtung oder zur Minderung des eigenen Seins führe, sei es in Wahrheit schlecht, obgleich es erstrebt wird und in der Perspektive des Strebenden gut ist.[4]

Der Mensch hat aber die Möglichkeit, seine Affekte zu beherrschen, und zwar mithilfe der Vernunft. Es ist wichtig, die Affekte zu verstehen, um zu Ideen zu kommen. Diese können geordnet und besser gesteuert werden. Eine solche Klarheit führt letzten Endes an das Ziel.

Da der Mensch von jeher nach vollkommener Erkenntnis strebt, und da Gott vollkommen ist, muss es sein Ziel sein, eins mit Gott zu werden. Da Gott in allem ist, muss daher das Ziel des Menschen sein, eins mit der (göttlichen) Natur zu werden; wenn man dies erreicht, erreicht man die höchste Form der Existenz – und damit den Frieden. Diese (intellektuelle) Liebe zu Gott (amor intellectualis Dei) steht bei Spinoza neben der Resignation, also der Ergebung in die Naturnotwendigkeit. Sie bilden den Kern der rein rationalen, also leidenschaftslosen Ethik Spinozas, der sich selbst in eine Reihe mit Sokrates und den Stoikern stellte. Zu dieser Ethik gehörte eine Philosophie des Glücks: „"Die Glückseligkeit ist nicht der Lohn der Tugend, sondern selbst Tugend; und wir erfreuen uns ihrer nicht deshalb, weil wir die Gelüste hemmen, sondern umgekehrt, weil wir uns ihrer erfreuen, deswegen können wir die Gelüste hemmen."“ Dies klingt egoistisch. Es darf aber nicht vergessen werden, dass für Spinoza „wahre“ Lebenserhaltung nur in der Gemeinschaft möglich ist. Spinoza bezeichnet diesen Weg als schwer, aber gangbar.

Im Anhang zum ersten Teil seiner Ethik sagt Spinoza, dass Unwissenheit kein hinreichender Grund sei: „Ignorantia non est argumentum.“ Damit wendet er sich gegen jene Theologen, die den Willen Gottes als Ursache aller Erscheinungen hinstellen mit der alleinigen Begründung, andere Ursachen seien nicht bekannt.[5] Dies ist ein Plädoyer Spinozas für die rationale Begründung seiner Ethik.

Der "Tractatus theologico-politicus" erschien 1670 in Amsterdam und wurde anonym und mit irreführenden Angaben über seinen Ursprung, zum Beispiel bezüglich Druckort und Namen des Druckers, veröffentlicht. Denn Spinoza erschienen in Anbetracht der veränderten politischen Verhältnisse in den Niederlanden seine in der Schrift entwickelten philosophischen und theologischen Ideen über die Denkfreiheit und die Religion als zu brisant. Dass er mit seiner Einschätzung richtig lag, zeigten die spätere Ermordung der von ihm geschätzten liberalen Regenten sowie das Verbot seines Werks 1674.

Nach Spinoza sind die Schriften der Bibel nicht fehlerfrei und können nicht wortwörtlich von Gott inspiriert sein. Kritisches Lesen sei daher unerlässlich und lasse verschiedene Widersprüche zwischen bestimmten Textstellen erkennen. Die Autoren (für die Bücher Mosis nahm er Esra als ersten an, mit „Verschlimmbesserungen“ durch spätere) müssten in ihrem historischen Zusammenhang gesehen werden, man habe ihre jeweiligen Glaubensvorstellungen zu berücksichtigen. Die Heilige Schrift belehre uns nicht über die Natur Gottes und seinen Heilsplan, sondern lehre uns Gehorsam und die Liebe zu Gott und den Mitmenschen. Um dies zu verstehen, brauche man keine raffinierte Unterweisung in Philosophie oder als Theologe ausgebildet zu sein. Sogenannte Wunder seien missverstanden und missbraucht worden für pseudomoralische Zwecke. Nur erstarrte Dogmen und Rituale hielten Judentum und Christentum noch am Leben. Philosophie und Naturrecht könnten auch nicht in Konflikt kommen mit dem (so verstandenen) Text der Heiligen Schrift. Mit dieser Auffassung wurde Spinoza zu einem der Begründer der modernen historisch-kritischen "Bibelanalyse".

Spinozas "Staatslehre" gründet sich auf die Überzeugung, Menschen, die durch Rationalität befreit seien, seien automatisch wohltätig und tolerant – auch gegenüber den Fehlern anderer, die noch von ihren Leidenschaften gesteuert werden. Da allerdings die Menschen im Allgemeinen sich nicht von Ratio leiten ließen, müsse der Staat Regeln setzen und durchsetzen. Damit zeigte Spinoza im "Tractatus theologico-politicus" auch die Grenzen von Philosophie und Naturrecht auf, wie er sie sah: Das Individuum müsse seine Rechte der Gemeinschaft überantworten. Es müsse dem Staat in Allem gehorchen, auch gegen seine private Überzeugung; ausgenommen seien lediglich Anweisungen, die dem universellen Moralgefühl widersprächen (etwa „Töte deine Eltern!“). Dieser Gehorsam störe auch nicht die menschliche Autonomie, da die Individuen die Obrigkeit ja selbst autorisiert hätten und Gebote sowieso im ureigensten Interesse des als Egoisten gesehenen Individuums lägen. Spinoza befürwortete die Demokratie, da es unwahrscheinlich sei, dass die Mehrheit einer großen Wählerschaft irrational entscheide. Die Freiheit zu philosophieren (Gedanken- und Redefreiheit) sei mit Frömmigkeit und Frieden im Staat vereinbar, ja letztere müssten ohne sie zugrunde gehen.

Wie Gott, so hat auch der menschliche Geist Ideen: Erfahrungen und Ratio. Erfahrungen ("experientia vaga") sind unzuverlässig (damit befand sich Spinoza ganz im Einklang mit seinen Zeitgenossen); sie liefern kein wahres Wissen von unseren Erkenntnisobjekten, denn sie präsentieren uns nur ein unvollständiges, vergängliches und trügerisches Bild dessen, was der Betrachter zu sehen meint. Diese Sinneserfahrung – ebenso die Erinnerung ("ex signis") – erlaubt uns nur oberflächliches „Wissen“, wie es aus einer bestimmten Perspektive und zu einem bestimmten Zeitpunkt erscheint. Das Ergebnis ist ein konfuses und verstümmeltes Wissen (einschließlich des Glaubens an Zufall und des Aberglaubens), es ist das Gegenteil von wahrer Einsicht in das Wesen der Dinge.

Diese Einsicht (Ratio) andererseits ist – nach Spinoza – notwendigerweise wahr und richtig. Wir gewinnen sie nicht anders als durch deduktive Logik, also rationales Denken. Dies bedeutet, nicht nur zu beobachten und lediglich die Beziehungen eines Gegenstandes (Idee, Objekt oder Vorgang) zu anderen Dingen zu erfassen, sondern Einsicht in sein Verhältnis zu den „Attributen“ Gottes und den „Modi“, die daraus folgen (die Naturgesetze), zu gewinnen. Wahres Wissen von einem derartigen Gegenstand erklärt, weshalb er existiert und weshalb er so ist und nicht anders sein kann. Dieses Wissen ist abgelöst von Raum und Zeit („"sub specie aeternitatis"“) und damit unvergänglich und unwandelbar. Auch gibt es (auf Grund der weltimmanenten Notwendigkeit) für den Einsichtigen nur wertneutrale Ursachen; wer von „gut“ oder „schlecht“ spricht, verfügt nur über oberflächliches „Wissen“.

Spinozas Konzept von rationaler Erkenntnis ist von einem ungetrübten, radikalen Optimismus bezüglich der Fähigkeiten des menschlichen Geistes gekennzeichnet. Er meinte, wir könnten nicht nur sämtliche Geheimnisse der Natur klären, sondern auch Gott adäquat erkennen: „Die menschliche Seele hat eine adäquate Erkenntnis der ewigen und unendlichen Wesenheit Gottes.“

Die Philosophie Spinozas, die anfänglich nur in Holland einen kleinen Kreis von Anhängern besaß (u. a. Gerardus de Vries und Lodewijk Meyer), fand ein Jahrhundert später bei Denkern wie Lessing, Herder oder Goethe Anklang. Der Aufklärer Pierre Bayle bezeichnete Spinozas Philosophie hingegen als die „monströseste und absurdeste“ Hypothese, die man sich vorstellen könne. 1744 erschien als eine erste gründliche Kritik "B. v. S. Sittenlehre, widerleget von dem berühmten Weltweisen unserer Zeit Herrn Christian Wolf." David Hume bezeichnete Spinozas Philosophie als eine abscheuliche („hideous“) Theorie. Großes Aufsehen erregte schließlich Friedrich Heinrich Jacobi mit seiner Veröffentlichung "Über die Lehre des Spinoza in Briefen an den Herrn Moses Mendelssohn" (erste Fassung 1785), worin er Lessing postum des „Spinozismus“ bzw. der Gottlosigkeit verdächtigte und Moses Mendelssohn als dessen Freund darüber auszufragen anfing. Dies ging als „Pantheismusstreit“ in die Philosophiegeschichte ein.

Um 1800 griffen die Frühromantiker Fichte, Schlegel, Schleiermacher, Schelling und Hegel Spinozas Ideen teilweise auf und diskutierten sie im Zusammenhang mit den Kritiken Kants. Auch Dichter wie William Wordsworth, Samuel Taylor Coleridge, Percy Shelley und Georg Büchner ließen sich von Spinoza inspirieren. Ludwig Feuerbach pries Spinoza als den „Moses der modernen Freigeister und Freidenker“. Heinrich Heine schrieb:

Friedrich Nietzsche hat sich stark zu Spinozas Denken hingezogen gefühlt. Insbesondere seine Kritik an der Teleologie, sein Immoralismus und der „Conatus“ als Vorwegnahme des „Willens zur Macht“ haben Nietzsche fasziniert. Allerdings hat er Spinoza nicht im Original gelesen, sondern in der Vermittlung des Philosophiehistorikers Kuno Fischer. Der Begründer der deutschen Soziologie Ferdinand Tönnies stützte seine Willenstheorie auf Spinoza und stellte 1887 dessen Ausspruch "Voluntas atque intellectus unum et idem sunt" („Wille und Verstand sind ein und dasselbe“) als Motto über das Axiomenkapitel seines Grundlagenwerks "Gemeinschaft und Gesellschaft". Spinozas Abhandlung "Über Ursprung und Wesen der Gefühlsbewegung" ist ein konsistent ausgearbeitetes System, das phänomenale Kausalität in der Wechselwirkung von emotionalen Spannungszuständen der wahrnehmenden Personen theoretisch erfasst. Die darin enthaltenen Beobachtungen haben eine Reihe von Sozialpsychologen beeinflusst.

Im 20. Jahrhundert sprach der Kulturhistoriker Egon Friedell von Spinozas „alles zerfressender […] pathologischer Logik.“ Die Theorie von einem unpersönlichen Gott, der sich selbst liebt und nicht um die Welt kümmert, von einer sich selbst verursachenden Natur, die jede Willensfreiheit ausschließt, erschien ihm als das Resultat eines „selbstherrlichen Rationalismus“. Carl Schmitt hasste Spinoza, weil er ihn für verantwortlich hielt für „die dreisteste Beleidigung, die jemals Gott und dem Menschen zugefügt worden ist und die alle Flüche der Synagoge rechtfertigt“, nämlich das "sive" („oder auch“) der Formel "Deus sive Natura", die Gleichsetzung von Gott und Natur. Spinoza verstoße, indem er Gott naturalisiere, gegen die übergroße Macht einer Autorität, den strengen, göttlichen Vater. Prägnant erscheint die an Gilles Deleuze angelehnte Beschreibung von Spinozas Denken durch Slavoj Žižek:

Zu Spinozas 250. Todestag im Jahre 1927 wurde auf dem Friedhof der Nieuwe Kerk in Den Haag, wo sich sein Grab befindet, eine Gedenktafel angebracht, deren lateinische Inschrift lautet: „Diese Erde birgt die Gebeine Benedictus de Spinozas, die einst in der neuen Kirche beigesetzt waren.“

Ebenfalls 1927 erklärte Joseph Klausner, ordentlicher Professor für hebräische Literatur an der Hebrew University in Jerusalem, das jüdische Volk habe mit dem "Cherem" gegen Spinoza eine schreckliche Sünde begangen und solle den Ketzer-Bannfluch aufheben. Aus seiner Rede: „Spinoza, dem Juden, rufen wir … zu: Der Bann ist aufgehoben! Das Unrecht des Judentums gegen dich ist hiermit aufgehoben, und deine Sünde, die du auch immer an ihm begangen haben magst, sei dir vergeben. Unser Bruder bist du, unser Bruder bist du, unser Bruder bist du.“ Das wurde zwar eine Weile in Jerusalemer Intellektuellenkreisen diskutiert – aber niemand leitete etwas Konkretes in die Wege.

Erst 1956, zum 300. Jahrestag von Spinozas Exkommunikation, flammte die Diskussion wieder auf. H. F. K. Douglas, ein holländischer Bewunderer Spinozas, regte die Errichtung eines weiteren Denkmals an und bat den israelischen Ministerpräsidenten David Ben-Gurion, der sich selbst als Spinozisten bezeichnete, um eine Unterstützung, die auch gewährt wurde. Eine Organisation humanistischer Juden aus Haifa, die Spinoza für den Erzvater des jüdischen Humanismus hielt, spendete eine schwarze Basaltplatte, die neben der alten Gedenktafel an der Nieuwe Kerk angebracht wurde. Die neue Tafel zeigt ein Relief von Spinozas Kopf, das Wort "caude" (Vorsicht) von seinem Siegelring und die Unterschrift "amcha" עמך (dein Volk). Ebenso wie holländische Regierungsvertreter nahm der israelische Botschafter an der Enthüllung teil. Orthodoxe Mitglieder der Knesset stellten deshalb einen Misstrauensantrag gegen David Ben Gurion und Außenministerin Golda Meir. Auch sonst regte sich Widerstand gegen die Rehabilitierung. Im Jahr 2012 bat die Portugiesisch-Israelitische Gemeinde in Amsterdam ihren Oberrabbiner Haham Pinchas Toledano, den Bann gegen Spinoza aufzuheben. Dieser lehnte jedoch ab, da Spinozas Auffassungen unverändert als ketzerisch zu betrachten seien.


Der "Tractatus theologico-politicus" wurde 1674 zusammen mit Thomas Hobbes’ "Leviathan" von der holländischen Regierung verboten.











</doc>
<doc id="7919" url="https://de.wikipedia.org/wiki?curid=7919" title="Hans Geiger (Physiker)">
Hans Geiger (Physiker)

Johannes „Hans“ Wilhelm Geiger (* 30. September 1882 in Neustadt an der Haardt; † 24. September 1945 in Potsdam) war ein deutscher Physiker. Bekannt wurde er durch den nach ihm benannten und von ihm zusammen mit seinem Doktoranden Walther Müller entwickelten Geigerzähler (auch "Geiger-Müller-Zählrohr" genannt).

Hans Geiger studierte ab 1902 Physik und Mathematik in Erlangen, wo er Mitglied der Burschenschaft der Bubenreuther war und in den ersten beiden Semestern nebenbei seinen einjährigen Militärdienst ableistete. 1904 verbrachte er auch ein Semester an der Ludwig-Maximilians-Universität München. 1906 legte er sein zweites Staatsexamen ab und wurde promoviert in Erlangen bei Eilhard Wiedemann mit der Arbeit "Strahlungs-, Temperatur- und Potentialmessungen in Entladungsröhren mit starken Strömen". Nach dem Studium wechselte er als Assistent zu Arthur Schuster nach Manchester und blieb dies auch ab 1907 unter dessen Nachfolger Ernest Rutherford, dessen 1911 aufgestelltes Atommodell zum Teil auf Geigers Entdeckungen beruhte (siehe Rutherfordstreuung). Er arbeitete dabei neben Rutherford unter anderem auch mit Ernest Marsden. Am Ende seiner Zeit in Manchester 1912 galt Geiger als internationale Autorität für Messungen der Radioaktivität, was sich auch in einem Buch mit Wilhelm Makower niederschlug.

1912 ging Geiger zurück nach Deutschland zur Physikalisch-Technischen Reichsanstalt in Berlin-Charlottenburg, wo er ein Labor für Radioaktivität aufbaute und mit James Chadwick zusammenarbeitete, der ihm aus Manchester gefolgt war und den er auch in der Zeit seiner Internierung während des Ersten Weltkriegs unterstützte, sowie mit Walther Bothe. Während des Ersten Weltkriegs diente er als Artillerie-Offizier und arbeitete in Fritz Habers Gastruppe (dem Pionierregiment 35) für den Gaskrieg mit. Nachdem er sich 1924 in Berlin habilitiert hatte, wechselte Geiger 1925 als Professor an die Christian-Albrechts-Universität zu Kiel. 1924 bis 1925 führte er mit Bothe die Methode der Koinzidenzmessung ein, die sie bei Untersuchung des Comptoneffekts benutzten. Für dieses Experiment erhielt Bothe später – nach dem Tod von Geiger – den Nobelpreis. Unter anderem zeigten sie mit ihrem Experiment auch die Gültigkeit der Erhaltungssätze von Energie und Impuls auf atomarer Ebene, was damals zeitweise (unter anderem von Niels Bohr) bezweifelt wurde. Zusammen mit seinem Doktoranden Walther Müller entwickelte er in Kiel 1928 das Geiger-Müllersche-Zählrohr (landläufig als „Geigerzähler“ bekannt), welches 1929 der Öffentlichkeit vorgestellt wurde.

1929 wechselte Geiger an die Eberhard Karls Universität Tübingen und wurde schließlich 1936 Direktor des Physikalischen Instituts der Technischen Hochschule Berlin als Nachfolger des von den Nationalsozialisten aus dem Amt gedrängten Gustav Hertz. Dort befasste er sich insbesondere mit Kosmischer Strahlung.
Geiger war 1920 mit Karl Scheel Gründungs-Herausgeber der Zeitschrift für Physik und bis 1945 einer der Herausgeber. Nach dem Tod von Scheel hatte er ab 1936 die Schriftleitung. 1926 war er Herausgeber des Handbuchs der Physik im Springer Verlag.

1939 nahm er an den Gründungssitzungen des Uranvereins teil und sein Rat, die Forschungen zur Kernenergie zu intensivieren, hatte mit ausschlaggebendes Gewicht bei deren Sitzung im September. Auf der Sitzung des Reichsforschungsrats 1942 über die weitere Unterstützung der Kernenergieforschung sprach er sich gegen eine weitere Fortführung der Arbeiten aus.

Hans Geiger starb am 24. September 1945, kurz nach der Räumung seines Hauses in Potsdam (es lag im Sperrkreis der Konferenz der alliierten Siegermächte in Potsdam) in einem Krankenhaus. Schon seit 1942 hatte er sich aus seinen wissenschaftlichen Ämtern zurückgezogen aufgrund einer schweren rheumatischen Erkrankung.

Hans Geiger wurde auf dem Neuen Friedhof Potsdam beigesetzt. Sein Grab hat sich erhalten. Die nach West-Berlin übergesiedelte Familie ließ auf dem Friedhof Grunewald einen zweiten Grabstein aufstellen, der sich ebenfalls erhalten hat.

1929 erhielt er die Hughes-Medaille der Royal Society, 1937 die Duddell-Medaille der London Physical Society und 1934 den Arrhenius-Preis der Akademischen Verlagsgemeinschaft Leipzig. Seit 1932 war er korrespondierendes Mitglied der Sächsischen Akademie der Wissenschaften und seit 1936 Mitglied der Preußischen Akademie der Wissenschaften. Seit 1936 war er Beisitzer im Vorstand der Deutschen Physikalischen Gesellschaft. Im Jahr 1935 wurde er zum Mitglied der Leopoldina und 1937 zum korrespondierenden Mitglied der Göttinger Akademie der Wissenschaften gewählt.

Zu seinen Doktoranden zählt Otto Haxel, der auch Assistent bei ihm an der TH Berlin war.

2000 wurde der Asteroid (14413) Geiger nach ihm benannt. Das Hans-Geiger-Gymnasium in Kiel-Ellerbek und ein Hörsaal des Physikzentrums der Christian Albrechts-Universität zu Kiel sind ebenfalls nach ihm benannt, ebenso eine Grundschule und eine Straße in seinem Geburtsort Neustadt; in weiteren Städten sind Neubaustraßen nach ihm benannt.

Hans Geiger war der Sohn des Gymnasial-Lehrers und späteren Professors für Indologie und Iranistik Wilhelm Geiger und der Bruder des Klimatologen Rudolf Geiger. Er war mit Elisabeth Heffter, Tochter des Berliner Pharmakologen Arthur Heffter, verheiratet und hinterließ drei Söhne, Jürgen, Klaus und Roland.

Geiger hat sich bis zu seinem Tod 1945 nie öffentlich für oder gegen die Nazis geäußert. Er war kein Freund der Deutschen Physik und wurde 1927 von Philipp Lenard als „Anglophiler“ als Nachfolger auf seinem Lehrstuhl abgelehnt.
Wolff schreibt, dass Geiger gemeinsam mit Max Wien und Werner Heisenberg mit einem Memorandum, Zitat: „[…] der deutschen Physik entgegentrat […]“.

Es gibt auch Hinweise, dass Hans Geiger sich für Kollegen und Studierende, die wegen der Nürnberger Gesetze Probleme bekamen, einsetzte.
Lieselott Herforth, eine Studentin, die bei Hans Geiger die Diplomprüfung ablegte, merkt an:
„… er nahm auch meine Studienfreundin, die als „Halbjüdin“ lediglich als Hörer eingetragen werden durfte (dies auch nur, weil ihr Vater Arzt im Ersten Weltkrieg war), als Diplomandin an. Und das 1939/40! Sie konnte als Externe 1940 mit mir zusammen die Diplomprüfung ablegen.“.
Ernst Stuhlinger bemerkt: "„Erst viel später wurde bekannt, daß Professor Geiger damals manchen seiner unglücklichen Kollegen, die sich zum Auswandern gezwungen sahen, durch seine nahen und sehr freundlichen Beziehungen zu Lord Rutherford und zu anderen einflussreichen Engländern zum Aufbau einer neuen Existenz im Ausland verholfen hat“".
Nach Kriegsende beschlagnahmten sowjetische Soldaten Geigers Haus in Potsdam. Swinne merkt zu diesem Sachverhalt an: "„Im Juni 1945 wurde Geigers Haus beschlagnahmt und abgeriegelt, da in der Nähe die Potsdamer Konferenz durchgeführt wurde.“"

Geiger hat sich aber auch nicht immer für die Interessen von Kollegen eingesetzt. Hans Bethe, der aufgrund der Nürnberger Gesetze aus dem Staatsdienst entlassen wurde (seine Mutter war jüdisch) und der sich gerade für eine Lehrstuhlvertretung in theoretischer Physik in Tübingen aufhielt, bat Geiger um seine Hilfe, die dieser verweigerte. Bethe schrieb dazu an Sommerfeld
„[…] jedenfalls bekam ich auf Anfrage, was geschehen würde, den inliegenden Brief von Geiger, dessen Kürze ich eigentlich als fast beleidigend empfinde und nach dessen Wortlaut ich nicht mehr glaube, dass ich in Tübingen noch viele Worte zu reden habe.“ Der genaue Inhalt des Briefes ist nicht bekannt. Auch in einem Oral-History-Interview mit Charles Weiner 1967 äußert sich Bethe enttäuscht über Geigers Reaktion, ohne diesen aber namentlich zu erwähnen.





</doc>
<doc id="7920" url="https://de.wikipedia.org/wiki?curid=7920" title="Scharia">
Scharia

Die Scharia ( im Sinne von „Weg zur Tränke, Weg zur Wasserquelle, deutlicher, gebahnter Weg“; auch: „religiöses Gesetz“, „Ritus“), abgeleitet aus dem Verb , beschreibt „die Gesamtheit aller religiösen und rechtlichen Normen, Mechanismen zur Normfindung und Interpretationsvorschriften des Islam“.

Ein einziger Gott "(Allah)" gilt in diesem Rechtssystem als der oberste Gesetzgeber . Sein Gesetz sei ein Teil der göttlichen Offenbarung im Koran. Bei der Scharia handelt es sich um kein kodifiziertes Rechtssystem, sondern „ein Regelwerk, welches sich stets im Wandel befindet“. Scharia ließe sich deshalb nur verstehen, wenn man die „Rechtsquellen- und Rechtsfindungslehre“ "(uṣūl al-fiqh)" statt „inhaltliche[r] Einzelregelungen“ betrachtet.
Das eingedeutschte Wort „Scharia“ geht auf die arabische Wurzel "š - r - ʿ" zurück. Arabischsprachige Anhänger von Religionen des Nahen Ostens nutzen es als gleich für eine prophetische Religion in ihrer Totalität. Daraus entstanden Begriffe wie Scharīʿat Mūsā bzw. Scharīʿat al-Mūsā (das Gesetz/die Religion Moses'), Scharīʿat Madschūs (die zoroastristische Religion) oder allgemein für Monotheisten als Bezeichnung für ihre Religion (Scharīʿatunā). Im Islam bezeichnet Scharia die „Regeln und Regulierungen, die das Leben von Muslimen bestimmen“ und Koran sowie Sunna entstammen.

Der Begriff Scharia hat, was den Islam angeht, seinen Ursprung im Koran. Erwähnt wird er dort jedoch nur an einer einzigen Stelle: Sure 45, Vers 18, wo er ursprünglich den Pfad in der Wüste bezeichnet, der zur Wasserquelle führt. Davon leiten Muslime einen göttlichen Ursprung der Scharia ab. 

Die oben Verbform "scharaʿa" tritt im Korantext an zwei Stellen auf: 

Sowie 

In Ahmad ibn Hanbals Musnad tritt das Nomen Scharia im Singular an einer Stelle auf. Dort heißt es, dass „die Gemeinschaft auf dem Scharia (Weg/Pfad)“ bleiben solle. Im Plural tritt es Scharia in Verbindung mit Islam ("šarāʾiʿ al-islmām") und Iman ("šarāʾiʿ al-īmān") sowie in der Aufzählung „der Glauben rührt aus den Pflichten, der Scharia, den Hudūd und der Sunna“ ("inna li-l-īmān farāʾiḍ wa-šarāʾiʿ wa-ḥudūd wa-sunan") auf. Als Verb taucht scharaʿa an einer Stelle auf: „Gott hat für seine Propheten ein System von Regeln niedergelegt“ ("šaraʿa li-nabi-hi sunan al-hudā").

Scharia ist ein Begriff, den neben dem Islam auch andere monotheistische Religionen im Nahen Osten verwendet haben. Hier einige Beispiele.

Der Jakobite ʿĪsā ibn Ishāq ibn Zurʿa benutzte in einem polemischen Werk gegen Juden das Wort Scharia als ein System von Gesetzen, das Propheten den Menschen bringen. Die christliche Religion und das Gesetz des Messias gibt er mit Scharīʿat al-Masīh und Sunnat al-Masīh wieder.

Zur Übersetzung des hebräischen Wortes Tora verwendete der arabischsprachige Juden Saʿadia Gaon Scharia, im Sinne von Gesetz zum Beispiel in : ("šarīʿat allāh" für ‚das Gesetz Gottes‘) und in : ("hāḏihi š-šarīʿat..: „Dies ist das Gesetz des Brandopfers“"). In Gaons Tafsīr aus dem 10. Jahrhundert beschreibt Scharia also stets eine Regel oder ein System von Regeln. Bemerkenswert ist dies, weil der Begriff Scharia verwendet wird, obwohl dafür an einigen Stellen auch das arabische Wort für Tora ("at-taurāt") auftritt.

In seinem theologischen Werk "Kitāb al-amānāt wa-l-iʿtiqādāt" (dt.: Buch der Ehrlichkeiten und Religionen) bezeichnet der Begriff Scharia individuelle Rechte und Recht als ein von Gott offenbartes System. Gaon unterscheidet zudem zwischen rationalen und offenbarten Gesetzen. Das Verb scharaʿa mit Gott als Subjekt bezeichnet darüber hinaus an einer Stelle „ein Gesetz niederlegen“.

„Die Scharia basiert auf dem Koran und auf der sich ab der Mitte des 7. Jahrhunderts herausbildenden Überlieferung vom normsetzenden Reden und Handeln Mohammeds“, welches sich in der Sunna manifestiert. Dabei ist die Scharia keine kodifizierte Gesetzessammlung (wie etwa deutsche Gesetzestexte im Bürgerlichen Gesetzbuch oder im Strafgesetzbuch), sondern eine „Methode und Methodologie der Rechtsschöpfung“. 

Handlungen muslimischer Gläubiger unterscheiden sich dabei in den sogenannten fünf Beurteilungen 

Im islamischen Normenfindungsprozess wird zwischen kultischen und rituellen Vorschriften () des Menschen einerseits und seine Beziehungen zu seinen Mitmenschen () andererseits unterschieden. Ein in europäischem Sinne festgelegtes „Familienrecht“, „Erbrecht“, „Strafrecht“ – oder andere – kennt das islamische Rechtssystem nicht. Seine Darstellung ist den Rechtsschulen in ihren Fiqh-Büchern, mit teilweise deutlich kontroversen Rechtsauffassungen, vorbehalten.

Diese Widersprüche soll ein Muslim akzeptieren. Das Forschen nach der Bedeutung und inneren Logik der göttlichen Gesetze ist nur zulässig, soweit Gott selbst den Weg dazu weist. Somit ist die religiöse Wertung aller Lebensverhältnisse die Grundtendenz der Scharia.

In Bezug auf den ethisch-religiösen Bereich ist laut Abū l-Hasan al-Aschʿarī die Sharia als „[…] die Gesamtheit der auf die Handlungen des Menschen bezüglichen Vorschriften Allāhs zu verstehen“. In diesem Kontext ist ethisch-religiös als die Aspekte der göttlichen Ordnung, die das sittliche Verhalten der Menschen betreffen, zu verstehen.

Unter den „Wurzeln der Rechtsfindung“ ("uṣūl al-fiqh") versteht man die Gesetzeswissenschaft im Islam, deren Gegenstand die Scharia ist. Sie wird Es entspricht der "iuris prudentia" (Rechtswissenschaft) der Römer und erstreckt sich auf alle Beziehungen des religiösen, bürgerlichen und staatlichen Lebens im Islam. Die religiösen Gesetze werden in den Büchern des Fiqh dargelegt und erörtert. Ibn Chaldūn erklärt dazu: 
Fiqh ist kein starres Rechtssystem, das unwandelbar alle Zeiten überlebt hat und an allen Orten gültig ist. Islamwissenschaftler, Arabisten und Ethnologen (beispielsweise Gudrun Krämer, Thomas Bauer, Ingrid Thurner) betonen immer wieder, dass Meinungspluralismus keineswegs in Widerspruch zur Scharia steht.

Scharia speist sich aus einer Vielzahl von Quellen. Koran und Hadīth sind von allen islamischen Strömungen als Quellen anerkannt, hinsichtlich der restlichen Quellen herrscht kein Konsens.

Zwar ist der Koran die wichtigste Quelle des islamischen Rechts. Allerdings enthält er nur einige Rechtsnormen, ferner einzelne Anweisungen, die lediglich als Grundlage einer allgemeinen, umfassenden Gesetzgebung gelten können. Laut Rohe weisen circa 500 Verse einen rechtlichen Bezug auf. Die meisten davon behandeln religiöse Ritualvorschriften ("ʿibābāt") und nur einige Dutzend beschäftigen sich mit straf- und zivilrechtlichen Fragestellungen. Die letzte Kategorie lässt sich noch in Erb-, Ehe- und Familienrecht sowie einige Strafbestimmungen und die Almosensteuer untergliedern.

Da viele dieser Stellen im Koran aber nicht eindeutig sind, haben Exegeten die Verse in solche aufgeteilt, die keiner Auslegung bedürfen ("muḥkam") und in solche, deren Bedeutung sich nicht von vornherein erschließt. Es bildete sich deshalb ein eigenes Genre heraus, welches sich mit der Auslegung des Korans beschäftigt: Tafsīr. In zwölfer-schiitischen Kreisen wird sogar angenommen, dass die Menschen nach der Entrückung des letzten Imām Muhammad al-Mahdī die genau Bedeutung des Korans nicht mehr erfassen könnten. Schließlich könnten die wahre Bedeutung des Korans und seinen normativen Charakter nur die zwölf Imāme verstehen.

Die zweite wichtige Quelle des islamischen Rechts ist für die Sunniten die Sunna Muhammads. Während sich das islamische Recht herausgebildet hat, galten und gelten noch heute Sunniten die Überlieferungen über die Prophetengenossen ebenfalls als Teil der Sunna. Großteils werden im sunnitischen Islam heute nur noch diejenigen Überlieferungen Muhammads anerkannt, die er in seiner Funktion als Prophet und nicht als Mensch getätigt hat. Dafür gibt es mehrere Aussprüche Muhammads selbst, mit dem dieser selektive Gebrauch begründet wird. Beispielsweise heißt es in einem Hadīth: „In euren weltlichen Angelegenheiten wisst ihr besser Bescheid (als ich).“ Muslime kritisieren dennoch andere Muslime, dass manchen diese Trennung schwer falle.

Schiiten dagegen erkennen neben den Überlieferungen Muhammads diejenigen der zwölf Imāme an.

Der Konsens (Idschmāʿ) konstituiert die erste Quelle des islamischen Rechts, die menschengemacht ist. Darunter versteht man den „Konsens aller relevanten Gelehrten in Übereinstimmung mit Koran und Sunna“.

Auch der Analogieschluss ist eine Quelle der Scharia.

Das „Für-Besser-Halten“ ist vor allem in der hanafitischen Rechtsschule ein beliebtes Instrument. Andere Rechtsschulen lehnen Istihsān mit Verweis auf Willkür ab, sehen es in manchen Fällen aber auch als zulässig an. Hanafiten gebrauchen ihn oft, um andere Rechtsquellen, vor allem den Qiyās, zu umgehen.

Der allgemeine Nutzen, auch "al-masālih al-mursala", fand bei den Hanbaliten, den Malikiten und den Schafiiten Eingang. Istislāh ist ein Instrument, welches es dem Rechtsgelehrten erlaubt, bei seiner Entscheidung den allgemeinen Nutzen als Grund für seine Entscheidung anzugeben.

"Die Auffassungen der (einzelnen) Prophetengenossen" können in manchen Fällen ebenso Teil der Scharia sein und als Quelle für eine Entscheidung herangezogen werden.

Das Gewohnheitsrecht, auch ʿāda, ist anerkannt, sofern es Regeln innerhalb der Scharia nicht widerspricht. Durch die Integration lokaler Bräuche in das islamische Recht finden sich noch heute vor allem an den Rändern der islamischen Welt Beispiele, die wenig mit den Gepflogenheiten der Scharia gemein haben. Dadurch wurde auch die Ausbreitung des Islams erleichtert.

Alles, was zu Verbotenem führen kann, wird durch das „Versperren der Mittel“ ebenfalls verboten. Hanbaliten und Malikiten beziehen in ihre Beurteilung vor allem die Absicht ("nīya") mit ein, während Hanafiten und Schafiiten die Mittel nur versperren, wenn ein Verbot mit großer Wahrscheinlichkeit vermieden werden soll.

Die Beibehaltung, auch "Normen derer vor uns" ("šarʿ man qablanā"), bezeichnet den Fortbestand einmal begründeter Rechtsverhältnisse. Denn nur so könne beispielsweise erworbenes Eigentum sicher sein.

Die Islamwissenschaftler Otto Spies und Erich Pritsch sehen in der Gültigkeit der Scharia einen grundsätzlichen Unterschied zum europäischen Recht:

Dieser Auffassung widerspricht Rohe in hohem Maße:

Rohe zitiert den Juristen der frühen Abbasidenzeit ʿĪsā ibn ʿAbān als Beispiel für eine von Spies und Pritsch vertretene Ansicht. Jedoch betont Rohe, dass diese Sichtweise nicht verbreitet ist.

Die Scharia umfasst neben den Rechten von Muslimen auch Nicht-Muslime, die auf islamischem Territorium leben. Diese wurden zwar bis zu einem gewissen Grad beschützt, standen Muslimen jedoch nicht gleichberechtigt gegenüber. Die Benachteiligung von Nicht-Muslimen waren von staatlicher Seite in vielen Fällen institutionalisiert. So durften sie keine hohen Staatsämter bekleiden und keinen Militärdienst absolvieren. Allerdings kam es zwischenzeitlich auch immer wieder zu Lockerungen solcher Vorschriften. In solchen Zeiten stiegen Nicht-Muslime oft in hohe Ämter auf.

Laut Rohe spiegelt das islamische Unterhaltsrecht die Lebensbedingungen patriarchalischer Großfamilien wider. Deshalb sind traditionell Männer unterhaltspflichtig. Falls der Mann dieser Pflicht aus materiellen Gründen nicht nachkommen kann, ist die Frau gegenüber ihren Kindern dafür zuständig, für diese zu sorgen. Die nächste Instanz wäre dann – außer bei den Malikiten – die Großeltern. Sollte ein Mann seinen Pflichten während der Ehe nicht nachkommen, ist es der Frau erlaubt, die Scheidung einzureichen. In den meisten Fällen wird ihr dies auch erlaubt, wenn ein Dritter für den Unterhalt aufkommt. Vor allem für Männer aus ärmeren Schichten stellt dies nicht selten ein Problem dar.

Söhne haben bis zur Volljährigkeit einen Anspruch auf Unterhalt, Töchter bis zur Heirat und ab dem Tod ihres Ehemanns. Eltern, Großeltern und Enkel haben ebenfalls das Recht, einen Anspruch auf Unterhalt zu stellen, wenn sie ökonomisch nicht auf eigenen Beinen stehen. Entfernte Verwandte muss jedoch nur ein reicher Mann versorgen. Über die Höhe der Zahlungen herrscht kein Konsens. Laut den "Fatāwā ʿĀlamgīrīya" soll die Höhe jedoch am möglichen Erbanteil bemessen werden. Auch die Konkurrenz zwischen Kindern und Eltern des Unterhaltspflichtigen ist Thema vieler Debatten.

Im Falle einer Scheidung variierte die Höhe des Entgelts für die Frau stets. Meist wurde ihr jedoch mindestens das Brautgeld ("mahr") zugesprochen. Falls die Scheidung von ihr ausgegangen ist, entfielen die Kosten für den Mann im Normalfall. Daher gibt es Beispiele aus der Geschichte, die belegen, dass Frauen auch zur Scheidung gezwungen wurde.

Neuere Entwicklungen spiegeln teils noch immer patriarchalische Vorstellungen wider. In Ägypten hat beispielsweise der Sohn bis zu seiner Volljährigkeit einen Anspruch auf Unterhalt, während ihn die Tochter bis zur Ehe oder bis zum Eintritt ins Berufsleben hat. Sie ist jedoch weder verpflichtet zu heiraten noch zu arbeiten. In Marokko, Tunesien, Libyen und den VAE jedoch muss eine vermögende Ehefrau ebenfalls zum Unterhalt beitragen.

Wenn sich ein Mann von seiner Frau scheiden lässt, muss er beispielsweise in Tunesien gemäß dem Lebensstandard während der Ehe weiterhin für seine Frau sorgen, bis sie wieder heiratet. In Ägypten hat die Frau einen Anspruch auf Unterhaltszahlungen für zwei Jahre, in Algerien kann der Mann, der sich willkürlich von seiner Frau geschieden hat, zu Zahlungen verurteilt werden. Dies trifft auch auf ihn zu, sollte die Frau sich berechtigterweise von ihm scheiden. Im Iran muss der Mann im Falle einer Scheidung der Frau das restliche Brautgeld, „Unterhalt und eine angemessene Ausstattung zur Verfügung“ stellen. Sollte die Frau sich weigern, dies anzunehmen und ihre ehelichen Pflichten nicht verletzt haben, dann hat sie zudem Anspruch auf ein finanzielles Entgelt für ihre haushaltichen Dienste während der Ehe.

Scharia wird unterschiedlich angewandt, je nach Land oder Region unterscheidet sich ihre Ausprägung. Folgend einige Beispiele.

Zurzeit ist die Scharia Rechtsgrundlage in einigen nördlichen Bundesstaaten Nigerias, auf den Malediven, im Iran, dem Senegal, in Saudi-Arabien (Artikel 23 lt. Verfassung), Bangladesch, Mauretanien, Afghanistan, Sudan, Gambia, Katar, Kuwait, Bahrain, Libyen, der indonesischen autonomen Provinz Aceh, Jemen – dort nebst Anwendung von Stammesgesetzen – und in Teilgebieten Pakistans. In Somalia errang im Juni 2006 nach jahrelangem Bürgerkrieg die Miliz Union islamischer Gerichte die Macht in der Hauptstadt Mogadischu, eine Gruppe, die sich durch das Ziel definiert, eine auf der Scharia basierende Rechtsordnung einzuführen (beispielsweise Verbot, die Fußball-WM 2006 im Fernsehen zu verfolgen). Diese Gruppe wurde Ende 2006 wieder gestürzt.

2010 begannen in vielen arabischen bzw. nordafrikanischen Ländern Revolutionen (zusammenfassend Arabischer Frühling genannt). Im Zuge dieser Revolutionen kam es in diesen Ländern zu Wahlen bzw. Verfassungsreferenden. In vielen Ländern wurde bzw. wird diskutiert, welche Rolle der Islam in Gesellschaft und Rechtssystem haben soll.

Allgemein verbreitet ist die Umsetzung im zivilrechtlichen Bereich beispielsweise in Algerien, Indonesien und Ägypten.

In einigen Staaten gilt die Scharia vollständig, etwa in Saudi-Arabien und Mauretanien. Zuweilen gilt die Scharia nur in islamisch dominierten Landesteilen (Nigeria oder Indonesien, siehe auch Scharia-Konflikt in Nigeria).

So wird zum Beispiel in Ländern wie Somalia und Sudan, wo Hadd-Strafen vollstreckt werden, auch die Schwangerschaft einer unverheirateten Frau oder einer Ehefrau, deren Ehemann abwesend ist, als Beweis für Unzucht genommen. In einigen Ländern werden selbst vergewaltigte Frauen aufgrund solcher „Beweisführung“ bestraft.

Die Bedeutung der Scharia nimmt seit etwa Mitte der 1970er Jahre in allen islamischen Ländern wieder kontinuierlich zu. Auch in der laizistischen Türkei mehren sich politisch einflussreiche Stimmen, die die Rückkehr zum islamischen Scharia-Recht fordern. Im Zuge der Revolution in Ägypten gab es im März 2011 ein Verfassungsreferendum.

Demgegenüber finden jedoch auch immer mehr alternative Interpretationsansätze der Scharia in der islamischen Welt Gehör. Diese Intellektuellen fordern dazu auf, bei der Auslegung des Korans den historischen Kontext zu beachten. Beispiele sind Fazlur Rahman in Pakistan, Muhammad Schahrur in Syrien, Abdulkarim Sorusch im Iran, Muhammad Abed al-Jabri in Algerien, Hassan Hanafi in Ägypten und nicht zuletzt viele Theologen in der Türkei.

Die praktische Umsetzung des islamischen Rechts ist in den islamischen Ländern sehr unterschiedlich. In manchen Staaten gibt es eine theokratische Identität von offiziellem Recht und Scharia, in anderen wurde die Scharia abgeschafft, in manchen hat sie – im Sinne eines Rechtspluralismus – lediglich für einen Teil der Bevölkerung Gültigkeit.

In der Türkei wurde die Scharia mit der Verfassung vom 20. April 1924 unter Mustafa Kemal Atatürk abgeschafft.

In Tunesien wurde sie mit der Verfassung vom 1. Juni 1959 abgeschafft. Lediglich Artikel 38 der tunesischen Verfassung schreibt fest, dass der Präsident ein Muslim sein muss.

In Malaysia existiert ein duales Rechtssystem, in dem islamische Gerichtshöfe parallel zu zivilstaatlichen Institutionen operieren. Drei der 13 Bundesstaaten des Landes erlauben etwa die Auspeitschung nach den Regeln der Scharia, obwohl dies landesweit nach dem Kriminalstrafrecht verboten ist.

Im Jahr 1990 wurde bei der 19. Außenministerkonferenz der Organisation der Islamischen Konferenz (OIC) die "Kairoer Erklärung der Menschenrechte im Islam" beschlossen, welche als Leitlinie der z. Zt. 57 islamischen Mitgliedstaaten auf dem Gebiet der Menschenrechte gelten soll. In den abschließenden Artikeln 24 und 25 wird die religiös legitimierte islamische Gesetzgebung, die Scharia, als einzige Grundlage zur Interpretation dieser Erklärung festgelegt.

Die Erklärung wird von Islam-Vertretern als islamisches Gegenstück zur Allgemeinen Erklärung der Menschenrechte der UNO gesehen, von der sie jedoch erheblich abweicht, da sie die Scharia zur Grundlage der Menschenrechte erklärt.

In westlichen Industriestaaten sowie in sonstigen nicht islamisch geprägten Ländern der Welt kann die Scharia – vermittelt über das jeweilige Internationale Privatrecht des Landes – Rechtswirkung entfalten. Allerdings findet die Geltung etwa in Deutschland ihre Grenzen im Ordre public: So werden Normen, die mit rechtlichen Grundvorstellungen unvereinbar sind, nicht angewendet. 

Grundlage für rituelle Vorschriften ist das "Fiqh al-aqallīyāt" (Minderheitenfiqh), welches eine Erleichterung für im Westen lebende Muslime erreichen möchte.

In Deutschland verweigerte 2007 eine Richterin am Amtsgericht Frankfurt am Main einer Frau, von ihrem gewalttätigen marokkanischen Mann noch vor Ablauf des Trennungsjahres geschieden zu werden, und begründete dies damit, dass es im marokkanischen Kulturkreis üblich sei, dass der Mann gegenüber der Frau ein Züchtigungsrecht ausübe; damit habe die Frau bei der Heirat rechnen müssen: . Die Verteidigerin hat die Richterin daraufhin als befangen abgelehnt. Dennoch gibt es Entscheidungen deutscher Gerichte, die explizit Bezug auf die Scharia nehmen.

In Großbritannien wird die Scharia nicht von den staatlichen Gerichten angewendet. Es gibt für bestimmte Fälle religiöse Schiedsgerichte, die auf freiwilliger Basis von den Parteien angerufen werden können. Dabei kommt die Scharia zur Anwendung, soweit sie nicht gegen Common Law verstößt. Im Februar 2008 hat das Oberhaupt der anglikanischen Kirche, der Erzbischof von Canterbury Rowan Williams, es gegenüber der BBC als „unvermeidlich“ bezeichnet, dass Elemente der Scharia im britischen Common Law anerkannt werden. Durch eine „konstruktive Adaption“ von Scharia-Elementen könnten zum Beispiel muslimischen Frauen westliche Ehescheidungsregeln erspart werden. Dabei gehe es nicht darum, „Unmenschlichkeiten“ der Gesetzespraxis in einigen islamischen Ländern in den Westen zu übertragen. Williams’ Einlassungen stießen in Großbritannien und innerhalb der anglikanischen Kirche vielfach auf Entrüstung, dabei wurde unter anderem darauf verwiesen, dass es nicht unterschiedliche Rechtssysteme für verschiedene Bevölkerungsgruppen innerhalb Großbritanniens geben dürfe. 

Eine gegenteilige Meinung vertritt der ehemalige anglikanische Bischof von Rochester Michael Nazir-Ali, der selbst wegen Morddrohungen pakistanischer Muslime nach Großbritannien geflohen ist. Die von den britischen Zivilgerichten ergangenen Scheidungsurteile haben aus der Sicht der islamischen Rechtsprechung keine Gültigkeit. Dr. Ahmad al-Dubayan, der Vorsitzende des Rates für Schariagerichte in Großbritannien "(Islamic Sharia Council)", sagte 2016, dass die Situation mit den sich immer weiter verbreitenden Schariagerichten ein großes Problem sei. Er wisse nicht, wie viele dieser Gerichte es in der Zwischenzeit in Großbritannien gebe. Der Innenausschuss im britischen Unterhaus begann eine Ermittlung hinsichtlich der Ausbreitung des islamischen Rechts. Muslimische Verbände kritisierten dieses Vorgehen unmittelbar nach Bekanntwerden als Einmischung in die Religionsfreiheit.

In Griechenland gilt für die muslimische Minderheit (Pomaken und Türken in Westthrakien) in Angelegenheiten, die den persönlichen Status und das Familienrecht betreffen, die Scharia, sofern die Angehörigen der Minderheit ihre Angelegenheiten nach der Scharia anstelle des griechischen Rechts geregelt haben möchten. Das geht auf den Vertrag von Sèvres zurück.

Der kanadische "Arbitration Act" (1991) erlaubte es Christen, Juden und Muslimen in der Provinz Ontario, häusliche Dispute (wie Scheidungs-, Vormundschafts- und Erbschaftsklagen) vor einem religiösen Schiedsgericht zu verhandeln, wenn alle Parteien damit einverstanden waren. Die Urteile dieser Schiedsgerichte waren, sofern sie nicht geltendem kanadischen Recht widersprachen, rechtskräftig. Damit wurde die Scharia in Ontario in Spezialfällen von muslimischen Gerichten angewendet. Im September 2005 wurde der "Arbitration Act" (auch wegen internationaler Proteste durch Frauenrechtsorganisationen) derart geändert, dass Entscheidungen auf Grund von religiösen Gesetzen nicht mehr möglich sind.

In den Vereinigten Staaten (Rechtssystem: Common Law, das sich vor allem auf frühere Präzedenzfälle stützt und daher von einzelnen Richtern leichter beeinflusst werden kann), haben 2010 die Bundesstaaten Tennessee und Louisiana die Anwendung der Scharia gesetzlich untersagt. In den Bundesstaaten Florida, Mississippi, Utah konnte solch eine gesetzliche Untersagung nicht durchgesetzt werden. In zwölf Bundesstaaten gibt es Anfang 2011 Gesetzesinitiativen, die die Anwendung der Scharia unterbinden sollen.

In Dänemark verfolgt eine islamistische Gruppe namens „Ruf zum Islam“ das Ziel, in muslimischen Wohngegenden in Kopenhagen Scharia-Zonen einzurichten, in denen eine „Moralpolizei“ das Verbot von Alkohol, Glücksspiel und Nachtleben überwacht. Ähnliche Lobbygruppen soll es inzwischen auch in Großbritannien, Belgien, Frankreich und Spanien geben.

In den Niederlanden ist die Diskussion über die Einführung der Scharia in vollem Gange, nachdem der damalige niederländische Justizminister Piet Hein Donner, ein Christdemokrat, im September 2006 erklärte, er könne sich die Einführung der Scharia in den Niederlanden gut vorstellen, wenn die Mehrheit der Wähler dafür wäre. Mittlerweile wird diese Möglichkeit auch in universitären Kreisen ernsthaft diskutiert. Ein Symposium an der Universität Tilburg widmete sich dem Thema "Sharia in Europe" am 3. Mai 2007 und lud dazu u. a. die palästinensisch-amerikanische Islamwissenschaftlerin Maysam al-Faruqi von der Georgetown University in Washington, D.C., ein. Al-Faruqi erachtet Scharia und niederländisches Recht als kompatibel miteinander: „Beide Rechtssysteme können mühelos nebeneinander bestehen“.

Kritiker halten der Anwendung der Scharia in westlichen Ländern entgegen, dass die Scharia nicht mit der Allgemeinen Erklärung der Menschenrechte vereinbar sei. Der Europäische Gerichtshof für Menschenrechte in Straßburg (EGMR) urteilte in mehreren Verfahren, dass die Scharia „inkompatibel mit den fundamentalen Prinzipien in der Demokratie“ sei.

Der Politologe Bassam Tibi untersucht die Fragestellung, ob es eine spezifische arabische oder islamische Demokratie gibt. Aus seiner Sicht ist die "islamistische" Scharia ein totalitäres Konzept. Die Politisierung und „Shariasierung“ des Islam sei nicht vereinbar mit der Demokratie. Er nennt es „das Paradox der demokratischen Scharia“. Auf der anderen Seite gebe es im Islam bestimmte Reformen, die eine Quelle der demokratischen Legitimität sein könnten. 

Im Sufismus (islamische Mystik) hat die Scharia einen Stellenwert als Basis für den Weg des Gottessuchenden. Weitere Stationen sind in der Reihenfolge: Tariqa (der mystische Weg), Haqiqa (Wahrheit) und Ma‘rifa (Erkenntnis). Der bekannte Mystiker Ibn Arabi (1165–1240) beschreibt diese vier Stationen folgendermaßen: Auf dem Niveau von Scharia gibt es „dein und mein“. Das heißt, dass das religiöse Gesetz individuelle Rechte und ethische Beziehungen zwischen den Menschen regelt. Auf dem Niveau von Tariqa „ist meins deins und deins ist meins“. Von den Sufis wird erwartet, dass sie sich gegenseitig als Brüder und Schwestern behandeln, den jeweils anderen an seinen Freuden, seiner Liebe und seinem Eigentum teilhaben lassen. Auf dem Niveau der Wahrheit (Haqiqa) gibt es „weder meins noch deins“. Fortgeschrittene Sufis erkennen, dass alle Dinge von Gott kommen, dass sie selbst nur die Verwalter sind und in Wirklichkeit nichts besitzen. Diejenigen, die die Wahrheit erkennen, interessieren sich nicht für Besitz und allgemeine Äußerlichkeiten, Bekanntheit und gesellschaftlichen Stand inbegriffen. Auf dem Niveau der Erkenntnis (Ma’rifa) gibt es „kein ich und kein du“. Der Einzelne erkennt, dass nichts und niemand von Gott getrennt ist.





</doc>
<doc id="7922" url="https://de.wikipedia.org/wiki?curid=7922" title="At-Tabarī">
At-Tabarī

Abū Dschaʿfar Muhammad ibn Dscharīr at-Tabarī (; * 839 in Amol, Tabaristan; † 19. Januar 923 in Bagdad) war ein bedeutender islamischer Historiker und Gelehrter persischer Abstammung. Über sein Leben sind nur wenige Daten erhalten.

Er ist nicht mit dem etwas älteren christlichen Arzt ʿAlī ibn Sahl Rabban at-Tabarī zu verwechseln, der um die Mitte des 9. Jahrhunderts zum Islam konvertierte und eine medizinische Enzyklopädie sowie eine Widerlegung des Christentums abfasste.
Das Leben Abū Dschaʿfar Muhammad ibn Dscharīr at-Tabarīs kann nur aus Fragmenten und aus Werken späterer Zeit zusammengestellt werden.

Abū Dschaʿfar Muhammad ibn Dscharīr at-Tabarī entstammte einer wohlhabenden Familie aus Āmol in Tabaristan (heute Māzandarān im Iran). Von seinem Vater, einem Landbesitzer, erbte er genug, um finanzieller Sorgen ledig sein Leben ganz der Gelehrsamkeit widmen zu können. Auf diesem Weg konnte er seine Unabhängigkeit vom Einfluss eines Patrons wahren. Zwar unterrichtete er später zwei Jahre lang die Kinder des Abbasiden-Wesirs ʿUbayd Allāh ibn Yahyā ibn Chāqān, jedoch soll er laut Anekdoten niemals eine offizielle Stelle als Qādī angestrebt haben. Seinem Reichtum zum Trotz hat at-Tabarī stets einen bescheidenen Lebensstil geführt, u.a. soll er laut der Biographie Maslama ibn al-Qāsim al-Qurtubīs das Zölibat (arabisch: "ḥaṣūr") der Ehe vorgezogen haben.

Nach seinen eigenen Auskünften soll at-Tabarī im Alter von sieben Jahren ein "Hāfiz" und mit acht Imam gewesen sein. Als Zwölfjähriger verließ er seine Heimat und trat seine ausgedehnte Studienreise nach Syrien, Ägypten, Bagdad, Kufa und Basra „auf der Suche nach Wissen“ ("fī ṭalab al-ʿilm") an. Er studierte bei zahlreichen Lehrern, unter anderem bei ʿAbd Allāh ibn Humaid ar-Rāzī, den er in seinem Geschichtswerk mehrfach zitiert. In Bagdad – damals eine Hochburg der islamischen Gelehrsamkeit – wollte er bei Ahmad ibn Hanbal studieren, doch starb dieser kurz nach seiner Ankunft in der Stadt. Später wurden at-Tabarīs Lehrstunden von Hanbaliten gestört und seine Schüler angegangen. Hintergrund war, dass at-Tabarī mit der Dscharīrīya (DMG: "Ǧarīrīya") eine andere Rechtsschule begründet hatte und diese die Autorität Ahmad ibn Hanbals als Rechtsgelehrter letztlich ohne Erfolg anfgefochten hatte.

Nach weiteren Studienreisen und der Pilgerfahrt nach Mekka und Medina kehrte er gegen 870 nach Bagdad zurück und widmete sich den letzten 50 Jahren seines Lebens ganz seiner schriftstellerischen Tätigkeit.

At-Tabarīs berühmteste Werke sind seine Annalen ("taʾrīch") und sein Korankommentar (""). Er beschäftigte sich auch mit Rechtswissenschaften "(fiqh)", "Hadithen" und anderen Wissenschaftsdisziplinen.

Bei den „Annalen“ – manchmal auch „Die Geschichte“ genannt – handelt es sich um at-Tabarīs Universalgeschichte, die meist mit "Muchtasar taʾrīch ar-rusul wa-l-mulūk wa-l-chulafāʾ" () wiedergegeben wird und die von der Schöpfungsgeschichte über die biblischen Propheten bis in at-Tabarīs Zeit (915) reicht. Das annalistisch zusammengestellte Werk ist bis heute eine der wichtigsten Quellen über die islamische Frühzeit und die Dynastien der Umayyaden und Abbasiden. Ausführliche Aufmerksamkeit gilt dem großen Aufstand der Zandsch zwischen 869 und 883, einem der bemerkenswertesten Ereignisse in der langen Geschichte der Sklaverei im Islam. 

Der Verfasser wertet ältere Materialien der islamischen Geschichtsschreibung aus, die ihm entweder schriftlich zur Verfügung standen, oder durch Korrespondenzen zugänglich gemacht worden waren. Für das südliche Zentralasien (Chorasan) bezieht er sich gänzlich auf den arabischen Historiker al-Madāʾinī (752 – um 840). Zu vielen Monographien, die heute nicht mehr vorliegen – wie die Schriften von Abū Michnaf, al-Wāqidī und Saif ibn ʿUmar – erhielt er die Überlieferungsrechte von seinen Lehrern; zugleich griff er auch auf mündliche Überlieferungen seiner Zeitgenossen zurück. Aus der Prophetenbiographie des Ibn Ishāq zitiert at-Tabarī den ersten Teil des Werkes in einer Rezension, die bei Ibn Hischām nicht erhalten ist.

Seine Angaben zur Geschichte des Neupersischen Reiches der Sassaniden sind von unschätzbarem Wert für die Forschung, da er hier auf heute verlorene spätantike Quellen zurückgreifen konnte. Die Darstellung der sassanidischen Geschichte „schließt sich an die Jesu und der Byzantiner an und führt unmittelbar zur Vita Mohammeds, dem Ziel der Geschichte.“ Das Geschichtswerk wurde schon im 10. Jahrhundert durch Muḥammad Balʿamī in gekürzter Form ins Persische übersetzt; denn es konnte den Persern die Erkenntnis vermitteln, dass die von Mohammed verkündete Religion die gottgewollte Bestimmung Persiens war. 

Das Gesamtwerk wurde erstmals 1879 bis 1901 von europäischen Orientalisten unter der Leitung von Michael Jan de Goeje in Leiden herausgegeben (siehe auch Theodor Nöldeke und Eugen Prym) und seitdem mehrfach nachgedruckt. Eine englische Übersetzung erschien unter dem Titel "The History of al-Tabari. An Annotated Translation" bei State University of New York Press, Albany 1985–1998.

Sein Korankommentar, "Dschāmiʿ al-bayān ʿan taʾwīl āy al-Qurʾān" , entstand ungefähr zwischen 896 und 903. Vom Werk waren bis zu Beginn des 20. Jahrhunderts nur einige Fragmente bekannt; die Entdeckung des vollständigen Exemplars in der Privatbibliothek des Gouverneurs von Ha'il war für die koranwissenschaftlichen Forschungen von entscheidender Bedeutung. Die im Jahre 1903 in Kairo erstmals erschienene und dann mehrfach nachgedruckte Ausgabe umfasst 30 Bände. Die Neuedition des Kommentars mit der Berücksichtigung neuer Handschriftenfunde und umfassenden Indices ist im Jahr 2001 in Kairo erschienen. At-Tabarī kommentiert darin den gesamten Korantext Vers für Vers. Zuerst werden lexikalische Fragen erklärt, darauf folgt die Darstellung der historischen Hintergründe der Offenbarung, ferner verschiedene traditionelle Auslegungen der Inhalte, die Erörterung der Frage der Abrogation. Abschließend gibt der Verfasser sein eigenes Urteil über die wahrscheinlichste Auslegung an.

At-Tabarī stützte sich in seinem Kommentar überwiegend auf schriftliche Quellen und zitierte Überlieferungen von Qatāda ibn Diʿāma, Mudschāhid ibn Dschabr, ʿAbdallāh ibn Wahb, as-Suddī und vielen anderen, deren koranexegetische Schriften entweder verloren gegangen oder nur in Fragmenten vorhanden sind. Die Bedeutung dieser Koranexegese in der islamischen Gelehrsamkeit bestätigt auch, dass sie rund achtzig Jahre nach dem Wirken des Verfassers, gegen 1000-1001, im islamischen Westen in andalusischem Duktus auf Pergament aufgezeichnet worden ist.

In der Rechtswissenschaft ("Fiqh") neigte at-Tabarī zunächst der schafiitischen Rechtsschule zu und studierte bei den Schülern von asch-Schāfiʿī sowohl in Bagdad als auch in Fustāt. In Ägypten verkehrte er auch in Kreisen von Malikiten. In Bagdad wirkte er anschließend zehn Jahre als Mufti der Schafiiten. Gegen Ende seines Lebens entwickelte er seine eigene Rechtsschule, deren Anhänger man nach seinem Vatersnamen die „Dscharīrīya“ nannte. Einige seiner Schüler verfassten Abhandlungen über die Verteidigung seiner Lehren, die wir allerdings nur nach ihren Titeln bei Ibn an-Nadīm kennen: „Einführung in die Rechtsschule at-Tabarīs und ihre Verteidigung“, „Der Konsens (Idschma) gemäß der Rechtslehre von Abū Dschaʿfar“ und andere Schriften, die islamische Theologen (mutakallimun) im 10. Jahrhundert verfasst haben. Die Inhalte dieser Schriften, in deren Titeln stets von „Madhhab at-Tabarī“, bzw. „Madhhab Abī Dschaʿfar“ (Die Schule von at-Tabarī bzw. Abū Dschaʿfar) und „Fiqh at-Tabarī“ (Die Jurisprudenz von at-Tabarī), nicht aber von der sogenannten „Dscharīrīya“ die Rede ist, sind unbekannt. Bei Ibn an-Nadīm wird einer der bekanntesten Traditionarier genannt, der auf dem Gebiet der Jurisprudenz seinem Zeitgenossen at-Tabarī nahestand: Abū Muslim al-Kaddschī aus Basra († 904). Der schafiitische as-Subki († Juli 1370), Verfasser einer umfassenden Gelehrtenbiographie der Schafiiten, nennt unter den Werken at-Tabaris ein Kitāb Ahkām Scharāʾiʿ al-Islām und fügt hinzu: „er (at-Tabarī) verfasste es wie sein Idschtihad ihn dazu befähigte“. Ob dieses Buch eine Zusammenfassung seiner eigenen Lehren darstellt, sagt as-Subki allerdings nicht.

Dafür nennt Ibn ʿAsākir denselben Werktitel in seiner Damaszener Gelehrtenbiographie mit dem Kommentar: „es ist seine Rechtslehre (madhhab), die er für sich ausgesucht, beherrscht und damit argumentiert hat. Es besteht aus 83 Kapiteln.“ In einem weiteren Werk, so Ibn ʿAsākir, unter dem Titel Scharh as-Sunna hat at-Tabarī seine eigene Rechtslehre (madhhab) gemäß den Ansichten der Gefährten Mohammeds, ihrer Nachfolger und den Rechtsgelehrten in den islamischen Provinzen dargelegt. Inhaltliche Einzelheiten sind auch hier nicht überliefert. Sowohl diese Schriften als auch die seiner Schüler sind verloren gegangen und werden auch in der juristischen Literatur der Folgegenerationen nicht zitiert. Somit ist die Rekonstruktion einer eigenen Rechtsschule – im Gegensatz zur Rechtslehre von al-Auzāʿī († 774) – nicht möglich. Ibn an-Nadīm hat in seinem "Fihrist" nicht nur die Titel von at-Tabarīs zahlreichen Rechtsschriften zusammengestellt, sondern auch die seiner Anhänger. Den Abschnitt, in dem Ibn ʿAsākir die Schriften at-Tabarīs aufzählt, hat Ignaz Goldziher bereits im Jahre 1895 nach einer Handschrift mit dem Hinweis publiziert, dass bei dem Damaszener Gelehrtenbiographen auch Werktitel erhalten sind, die Ibn an-Nadīm offenbar nicht gekannt hat. 

Sein bedeutsames Werk auf dem Gebiet der Jurisprudenz unter dem Titel

In diesem nur fragmentarisch erhaltenen Werk stellt at-Tabarī die Lehren führender Juristen der Frühzeit wie Mālik ibn Anas, Abū Hanīfa, asch-Schāfiʿī, ferner die von al-Auzāʿī und dem in Kufa beheimateten Sufyān ath-Thaurī dar, schließt aber Ahmad ibn Hanbal als primären Hadithgelehrten und Nichtjuristen, genauso wie die Theorien der Muʿtazila, aus. Der Verfasser selbst äußert sich zu den vorgestellten kontroversen Ansichten der genannten Rechtsschulen nicht; er hebt lediglich diejenigen Punkte hervor, in denen die Vorgänger Konsens (Idschma) erzielt haben. Somit ist das Werk eine wertvolle Kompilation schriftlich nicht mehr vorhandener Rechtslehren aus dem späten 2. und frühen 3. islamischen Jahrhundert.

Joseph Schacht würdigt dieses Werk mit den folgenden Worten:
Auf dem Gebiet des Hadith sind Teile aus seinem erhalten. Es ist nach den letzten Gewährsmännern der Prophetensprüche angeordnet (Musnad). Die vorliegenden Teile behandeln die von ʿAbdallāh ibn ʿAbbās, ʿUmar ibn al-Chattāb und ʿAlī ibn Abī Tālib vermittelten Aussagen Mohammeds. at-Tabari erklärt jede Tradition zunächst nach linguistischen Aspekten und bestimmt ihren Stellenwert als beweiskräftige Belege im Ritualrecht, soweit sie der Sunna, der zweiten Quelle der islamischen Jurisprudenz, entsprechen. Ibn an-Nadim kannte dieses Werk unter diesem Titel und vermerkt, dass der Verfasser es nicht vollendet hatte.





</doc>
<doc id="7924" url="https://de.wikipedia.org/wiki?curid=7924" title="Hebräische Sprache">
Hebräische Sprache

Hebräisch ( "‘Ivrit", ) gehört zur kanaanäischen Gruppe des Nordwestsemitischen und damit zur afroasiatischen Sprachfamilie, auch semitisch-hamitische Sprachfamilie genannt.

Die Basis aller späteren Entwicklungsformen des Hebräischen ist die Sprache der heiligen Schrift der Juden, der hebräischen Bibel, deren Quellschriften im Laufe des 1. Jahrtausends v. Chr. entstanden und kontinuierlich redigiert und erweitert und schließlich um die Zeitenwende kodifiziert wurden. (Alt-)Hebräisch wird daher oft mit dem Begriff „Biblisch-Hebräisch“ gleichgesetzt, selbst wenn dies weniger sprachhistorisch als literaturhistorisch begründet ist: Althebräisch als die Sprache des größten Teiles des Alten Testamentes. In der Bibel wird die Sprache "sefat kena‘an" („Sprache Kanaans, Jes 19,18“) genannt. Nach der Zerstörung des Jerusalemer Tempels durch Nebukadnezar II. im Jahre 586 v. Chr. und dem darauf folgenden babylonischen Exil kam die dortige Amtssprache Aramäisch unter den Juden in Umlauf, sodass das Hebräische fortan in Konkurrenz zum Aramäischen stand und viele Einflüsse von diesem aufnahm.

Nach der Zerstörung des Zweiten Tempels zu Jerusalem im Jahre 70 n. Chr. verlagerte sich das Zentrum jüdischen Lebens von Judäa nach Galiläa und ins Exil. Etwa ab dem Jahre 200 hörte Hebräisch auf, Alltagssprache zu sein. Es blieb indessen eine Sakralsprache, wurde jedoch nie ausschließlich zu liturgischen Zwecken benutzt, sondern auch zur Abfassung von philosophischen, medizinischen, juristischen und poetischen Texten, so dass sich das Vokabular des Mittelhebräischen im Laufe der Jahrhunderte erweitern konnte. Es ist ebenfalls bezeugt, dass sich die verstreuten jüdischen Gemeinden zur Verständigung untereinander des Hebräischen bedienten.

Die Erneuerung des Hebräischen mit dem Ziel seiner Etablierung als jüdische Nationalsprache in Palästina begann im späten 19. Jahrhundert auf Initiative von Elieser Ben-Jehuda. 1889 gründete er in Jerusalem den „Rat der hebräischen Sprache“, den Vorläufer der Akademie für die hebräische Sprache, mit dem Ziel, die seit etwa 1700 Jahren kaum noch gesprochene Sprache der Bibel wiederzubeleben. In der Folgezeit entstand das moderne Hebräisch (Ivrit), dessen Unterschiede zum biblischen Hebräisch im Schriftbild und der Morphologie äußerst gering, in der Syntax und dem Vokabular aber z. T. gravierend sind.

Man unterscheidet drei Entwicklungsstufen: Alt-, Mittel- und Neuhebräisch. Daneben gibt es eine eher literarisch definierte Einteilung in Bibelhebräisch, Mischnahebräisch, mittelalterliches Hebräisch und modernes Hebräisch. Diese Einteilung ist im akademischen Hebräischunterricht üblich.

Das Althebräische ist aufs Engste mit der phönizisch-punischen Sprache sowie den anderen semitischen Varietäten der vorderasiatischen Mittelmeerküste verwandt; die meisten Linguisten betrachten heute das Kanaanäische (mit dem Hebräischen als einer von mehreren Mundarten) und das Phönizische als dieselbe Sprache. Sprachwissenschaftlich gesehen ist (Alt-)Hebräisch ein südkanaanäischer Dialekt des 1. Jahrtausends v. Chr., der in einem Dialektkontinuum mit den kanaanäischen Sprachen Moabitisch, Ammonitisch, Edomitisch, Ugaritisch, Phönizisch usw. stand. Der älteste bekannte hebräische Text ist der auf eine Tontafel niedergeschriebene Gezer-Kalender von 925 v. Chr., der heute in Istanbul ausgestellt ist. Es gibt ältere Zeugnisse der verwandten Dialekte.

Das berühmteste Werk in althebräischer Sprache ist die jüdische Bibel, der Tanach (im christlichen Sprachgebrauch Altes Testament genannt). Die ältesten erhaltenen Abschriften biblischer Texte sind die Schriftrollen vom Toten Meer. Sie wurden 1947 in Qumran gefunden und stammen aus der Zeit zwischen dem 3. Jahrhundert v. Chr. und dem späten 1. Jahrhundert n. Chr. Sie weisen zahlreiche Unterschiede zur heutigen kodifizierten jüdischen Bibel auf und umfassen auch Schriften, die in den Kanon der jüdischen Bibel nicht eingegangen sind.

Mittelhebräisch ist die Sprache spätbiblischer Texte sowie der hebräischen Teile der rabbinischen Literatur und der mittelalterlichen jüdischen Literatur. Geprägt wurde sie maßgeblich vom hebräisch-aramäischen Diglossieverhältnis, das vom Babylonischen Exil bis zum Ende der rabbinischen Epoche bestimmend war.

Im Perserreich wurde Aramäisch, in Form des sog. Reichsaramäisch, zur Verwaltungssprache. Kerngebiet dieser Sprache war zuvor der syrische Raum um Damaskus. Aramäisch etablierte sich für ca. 700 Jahre als ethnische und politische Grenzen des Nahen Ostens überschreitende Umgangssprache, zu der seit dem Sieg Alexanders des Großen über die Perser das Griechische in Konkurrenz trat. Erst das Auftreten des Arabischen drängte beide Sprachen fast ganz zurück. Die Juden benutzten Aramäisch für Bibelübersetzungen (Targumim) und im Talmud. Wie das Hebräische gehört Aramäisch zum nordwestlichen Zweig der semitischen Sprachen und ist somit dem Hebräischen nahe verwandt. In der "mittelhebräischen" Phase wurden zahlreiche aramäische Ausdrücke und Redewendungen ins Hebräische übernommen, vor allem aber die aramäische Schrift, die als Quadratschrift bis heute in Gebrauch ist, während die Aramäer ihre Schrift zu verschiedenen Kursiven weiterentwickelten und die Quadratschrift aufgaben. Auch die Syntax änderte sich in dieser Phase grundlegend (Übergang von der Struktur Prädikat–Subjekt–Objekt hin zu Subjekt–Prädikat–Objekt sowie von der parataktischen, das heißt Hauptsätze bevorzugenden Syntax hin zum hypotaktischen Prinzip, d. h. zu Hauptsatz-Nebensatz-Konstruktionen). Das Mittelhebräische umfasst das spätbiblische Hebräisch und das Hebräisch der rabbinischen Literatur, d. h. der Werke der jüdischen Gelehrten insbesondere Palästinas und Babyloniens nach der Zerstörung des Zweiten Tempels (70 n. Chr.).

Während fast zwei Jahrtausenden war Hebräisch keine als Muttersprache gesprochene Sprache, sondern meist Zweit- oder Drittsprache von Juden, das heißt in der Regel von gebildeten jüdischen Männern in allen Teilen der Diaspora. In der traditionellen jüdischen Ausbildung wurde viel Zeit darauf verwendet, Tora, Mischna, Gemara und rabbinische Kommentare im hebräischen (und zum Teil aramäischen) Original zu lesen. Der wichtigste Beitrag zum Erhalt des biblischen Hebräisch stammt von den Masoreten, die vom 7. bis zum 10. nachchristlichen Jahrhundert zum nur mit Konsonanten geschriebenen Bibeltext Vokale, Akzente und so genannte Teamim hinzufügten, das heißt Angaben für den liturgischen Gesang im Gottesdienst. Da die entsprechenden Zeichen hauptsächlich aus Punkten bestehen, spricht man von „Punktation“ (hebräisch „Nikud“). Die bedeutendsten Masoreten wirkten im 9./10. Jahrhundert n. Chr. in Tiberias am See Genezareth.

Vor allem zwei Familien sind hier bedeutsam: Ben Ascher und Ben Naftali. Nachdem bereits vorher in Babylonien wie in Palästina Texte punktiert worden waren, schuf Aaron ben Mosche ben Ascher das ausführlichste und gründlichste Punktationssystem, das sich schließlich auch durchsetzte. Der allgemein anerkannte jüdische hebräische Bibeltext, der seit dem 16. Jahrhundert auch von christlichen Theologen der exegetischen Arbeit am Alten Testament zugrunde gelegt wird, geht auf die Familie Ben Ascher zurück. In der Biblia Hebraica Stuttgartensia ist der masoretische Text nach der ältesten vollständigen Handschrift dieser Textform, dem Codex Leningradensis abgedruckt. Die Arbeiten von Paul Kahle zu verschiedenen masoretischen Systemen und der Vergleich mit griechischen Umschriften des Hebräischen in der Septuaginta und der Hexapla des Origenes haben gezeigt, dass die Masoreten von Tiberias in ihrer Punktation nicht von der gängigen Aussprache des Hebräischen ausgingen, so wie sie mündlich tradiert worden war, sondern zum Teil ein ideales philologisches Konstrukt schufen, das vor allem religiösen Bedürfnissen genügen sollte.

Die Geschichte des neuzeitlichen Hebräisch zerfällt in zwei Perioden, die aber nicht für die gesamte Judenheit Gültigkeit besaßen. Wesentliche Neuerungen (eine Abwendung von der rabbinischen Tradition und die Betonung der vermeintlichen Reinheit und Ursprünglichkeit des biblischen Sprachstils) brachte die jüdische Aufklärung im 18./19. Jahrhundert. Auch bei den Juden anderer Regionen kam es – ohne Beeinflussung durch die Entwicklungen in Mitteleuropa – zu einer Renaissance des Hebräischen, zum Beispiel im Irak. Einen weiteren Neuanfang, der schließlich zur Reetablierung als muttersprachlich weitergegebenes Idiom in Palästina führte, bewirkten die Zionisten vom Ende des 19. Jahrhunderts an. Diese Sprachform wird meist als Modernhebräisch oder Ivrit bezeichnet. Jedoch bezeichnet das Wort Ivrit im Hebräischen selbst ohne Hinzufügung eines qualifizierenden Adjektivs die gesamte hebräische Sprache aller Perioden; Neuhebräisch heißt auf Hebräisch ha-Ivrit ha-chadascha.

Unter Ivrit wird außerhalb Israels meist die jüngste Entwicklungsphase des Neuhebräischen verstanden: Das infolge der Aufklärung und des Zionismus entstandene israelische Hebräisch ist das Ergebnis einer Wiederbelebung des Entwicklungsstandes der biblischen Sprache, den die Masoreten von Tiberias mit ihrem Vokalisationssystem entwickelt haben, jedoch auf der Basis einer späteren, eher mittelhebräischen Syntax, die zudem Einflüsse europäischer Sprachen aufweist.

Die relativ wenigen spezifisch althebräischen Formen werden in Israel verstanden und in der Schule gelehrt, in der Alltagssprache jedoch nicht verwendet (z. B. Pausalformen); zudem haben zahlreiche biblische Wörter heute eine andere Bedeutung. Insbesondere ist das althebräische Aspektsystem des Verbs seit dem Mittelhebräischen einem Tempussystem gewichen; die althebräischen Aspekte wurden bei der Rückbesinnung auf das Bibelhebräische in der Neuzeit nicht in die moderne Sprache übernommen.

Vielen gilt das Hebräische als Beispiel einer erfolgreichen Umwandlung einer alten Literatur- und Sakralsprache zu einer modernen Nationalsprache. Dies wurde von David Ben Gurion, dem ersten Ministerpräsidenten des Staates Israel, mit folgendem Ausspruch kommentiert: „Wenn Moses heute zurückkäme und um ein Stück Brot bäte, verstünde man ihn.“ Eine derartige Auffassung wurde teilweise auch von Hebraisten und Semitisten vertreten (Ullendorff), ist aber umstritten (Brockelmann).

Kritiker verweisen in diesem Zusammenhang u. a. auf die für semitische Sprachen untypische Aussprache des heutigen Hebräisch, das nur noch in der Orthografie, aber nicht in der Phonetik die spezifisch semitischen Laute erkennen lässt (d. h. zwischen "Aleph" und "Ajin", "Kaf" und "Qof", "Thet" und "Taw", "Chet" und "Khaf" etc. unterscheidet) und somit über wesentlich weniger Laute verfügt als die meisten anderen semitischen Sprachen. Die genannten Buchstabenpaare bilden heute im Hebräischen Homophone. Nur Israelis mit arabischer Muttersprache differenzieren gelegentlich noch einzelne dieser Laute. Eine vergleichbare Entwicklung ist nur im Maltesischen zu erkennen, das sich aufgrund seiner Isolation vom arabischen und seiner Jahrhunderte währenden Anbindung an den italienischen Sprachraum in mancher Hinsicht, insbesondere im Bereich der Phonetik „europäisiert“ hat.

Siehe hebräisches Alphabet sowie die Einträge unter den einzelnen Buchstaben, von Aleph bis Taw. Schreibrichtung von rechts (oben) nach links.

Für die Grammatik des modernen Hebräisch siehe Ivrit.

Das Althebräisch gehört wie alle semitischen Sprachen grundsätzlich zu den Kasussprachen. Seit dem Ausfall der Kasusflexion in der kanaanäischen Gruppe der semitischen Sprache werden jedoch schon ab dem 10. Jh. v. Chr. zur Unterscheidung von Subjekt und Objekt keine Fälle mehr verwendet, sondern das Objekt kann optional mit einer speziellen "nota objecti" markiert werden, das ist allerdings nur bei determinierten Objekten möglich. Flexion spielt jedoch eine wichtige Rolle bei der Bildung und Ableitung von Verben, Substantiven, der Genitivkonstruktion Status constructus, die auf Hebräisch "Smichut" ( – „Stützung“) genannt wird.

Beispiele für die Genitivverbindung (Smichut):

"bájit" () = Haus; "lechem" () = Brot; "bējt lechem" () = Haus des Brotes (Bethlehem).

In Genitivverbindungen wird der bestimmte Artikel vor ihren letzten Bestandteil gesetzt:

"alija" () = Rückführung, Repatriierung; "no`ar" () = Jugend; "alijat ha-no`ar" () = die Rückkehr (nach Israel) der Jugendlichen.

Das Besitzverhältnis kann mithilfe der klassischen Kurzform (Substantiv mit Pronominalendung) oder einer längeren, umschreibenden Phrase wiedergegeben werden, z. B. von :Sohn = "ben":

Die hebräische Sprache kennt zwei grammatikalische Geschlechter bzw. Genera: männlich und weiblich. Weibliche Substantive und Namen enden meistens mit "…a" () oder "…t" (). Beispiel: Sarah (), `Ivrith (). Es gibt jedoch auch einige Ausnahmen, beispielsweise endet das Wort „lájla“ ( – Nacht) mit dem Buchstaben „He“ und ist trotzdem grammatikalisch männlich. Es können auch weibliche Nomen männliche Endungen tragen. Abstrakta werden meistens dem weiblichen Genus zugeordnet.

Betont wird meistens die letzte Silbe, in einigen Fällen auch die vorletzte Silbe, bei Fremdwörtern auch andere Silben ( "univérsita" „Universität“). Die Betonung ist (im Neuhebräischen) schwach phonemisch, es gibt also gelegentlich Wortpaare, die sich nur durch die Betonung unterscheiden ( "birá" „Hauptstadt“, "bíra" „Bier“). Manche Personennamen können auf zweierlei Weise betont werden und erhalten dadurch einen jeweils unterschiedlichen emotionalen Beiklang.

Hebräische Substantive und Adjektive können mit dem bestimmten Artikel „ha“ definiert werden. Unbestimmte Substantive bzw. Adjektive tragen gar keinen Artikel. Der bestimmte Artikel wird zusammen mit dem zugehörigen Wort geschrieben. Beispiel: "no`ar" = Jugend, "hano`ar" = die Jugend. Wird der Artikel vorgesetzt, erhält der folgende Konsonant meist einen Punkt („Dagesch forte“), der Verdopplung anzeigt. Vor Konsonanten, die nicht verdoppelt werden können, erhält der Artikel ein langes -a ("qametz").

Außer im Bibelhebräischen verfügen hebräische Verben über drei Tempora: Vergangenheit, Zukunft und Gegenwart. Streng genommen sind aber nur Vergangenheit und Zukunft echte Konjugationen mit Formen für die 1., 2. und 3. Person im Singular und Plural, während für die Gegenwart das Partizip verwendet wird. Hier hat jedes Verb wie das hebräische Adjektiv vier Formen: Maskulinum Singular, Femininum Singular, Maskulinum Plural, Femininum Plural. Die Person wird durch Hinzufügen des Personalpronomens angezeigt. Ein Beispiel für die Bildung des Partizips:

Im Althebräischen ist eine klare Trennung zwischen „Gegenwart“, „Vergangenheit“ und „Zukunft“ nicht möglich. Beim finiten Verb werden zwei Aktionsarten unterschieden, verteilt auf zwei Konjugationen, die traditionell „Perfekt“ und „Imperfekt“ genannt werden:

Darüber hinaus gibt es im Bibelhebräischen zwei Ableitungen dieser Konjugationen, die deren Sinn ins Gegenteil verkehren:

Die jeweilige Consecutivum-Form unterscheidet sich von der Normalform des Perfekts oder Imperfekts dadurch, dass die Kopula „und“ vorangestellt wird. Im Falle des Imperfectum Consecutivum wird zudem der nachfolgende Konsonant verdoppelt (hebräisch מְדֻגָּשׁ, m'duggash), und die Betonung verlagert sich oft auf die vorletzte Silbe. Im Imperfectum Consecutivum werden auf der vorletzten Silbe betonte Perfekt-Formen endbetont. Wegen des vorgeschalteten „und“ können Consecutivum-Formen immer nur am Anfang des Satzes oder Halbsatzes stehen; kein anderer Satzteil, auch keine Verneinung darf vorgeschaltet werden.

Moderne Grammatiken haben die traditionellen Bezeichnungen „Perfekt“ und „Imperfekt“ aufgegeben, da diese versuchen, die Aktionsart inhaltlich zu beschreiben, was an der jeweiligen Consecutivum-Variante scheitert. Das Perfectum Consecutivum beschreibt gerade keine „perfekte“, abgeschlossene Handlung, sondern im Gegenteil eine „imperfekte“, unabgeschlossene. Also ist der Terminus „Perfekt“ ungenau. Das Gleiche gilt analog für „Imperfekt“. Die neuen Bezeichnungen beschreiben nicht mehr den Inhalt, sondern allein die äußerliche Form: Das Perfekt heißt nun Afformativ-Konjugation (abgekürzt: AK) und das Imperfekt Präformativ-Konjugation (PK). AK weist darauf hin, dass alle Formen dieser Konjugation (bis auf eine) eine Endung haben, also ein Affix oder Afformativ (sg.: kataw-ti, kataw-ta, kataw-t, kataw, katew-a; pl.: kataw-nu, ketaw-tem, ketaw-ten, katew-u); PK weist auf das Präfix oder Präformativ, die Vorsilbe, hin, die alle Formen dieser Konjugation erhalten (sg.: e-chtow, ti-chtow, ti-chtew-i, ji-chtow, ti-chtow; pl.: ni-chtow, ti-chtew-u, ti-chtow-na, ji-chtew-u, ti-chtow-na). Die Consecutivum-Formen werden AK bzw. PK mit Waw conversivum, also umkehrendem Waw, genannt. Der Buchstabe Waw steht für die Kopula „und“, die im Hebräischen mit diesem Buchstaben geschrieben wird. PK mit Waw conversivum (Imperfectum Consecutivum) ist das typische Erzähltempus der biblischen Texte und wird daher auch Narrativ genannt.

Die Funktion des Waw conversivum ist einzig für das Bibelhebräische belegt und findet in anderen semitischen Sprachen, etwa dem Arabischen oder Aramäischen, keine Entsprechung.

Die Grundlage zur Ableitung sämtlicher Konjugationsformen ist die „Wurzel“ (Wortstamm), die sich aus den Konsonanten zusammensetzt, die in allen oder den meisten Formen des Verbes und seiner Ableitungen vorkommen. Beim hebräischen Verb für „schreiben“ sind das: , also „k-t-w“. Je nachdem, welche Form gebildet werden soll, werden die für die Form typischen Vokale dazwischengesetzt; in vielen Formen kommen außerdem konjugationstypische Vor- und/oder Nachsilben hinzu (vgl. die oben aufgeführten Formen des Partizips und von AK und PK). Demnach findet Konjugation im Hebräischen wie in allen semitischen Sprachen vor, in und nach dem in der Regel rein konsonantischen Wortstamm statt; die meisten Wurzeln bestehen aus drei Konsonanten. Dagegen besteht der Wortstamm in den europäischen Sprachen aus Vokalen und Konsonanten, die in allen Formen unverändert bleiben, vgl. „sag“ in „sagen“, „sage“, „sagtest“, „gesagt“ usf.; Konjugation findet vor und/oder nach dem Stamm statt. (Ausnahmen bilden nur die unregelmäßigen Stämme, etwa „schreib“, das in manchen Formen zu „schrieb“ wird, oder „geh“, das zu „ging“ oder „gang“ werden kann.)

Neben AK, PK und Partizip kennt das Hebräische Infinitiv- und Imperativformen. Vorvergangenheit und Futur II sind dagegen unbekannt. Auch gibt es nahezu keine spezifischen Modalformen (Konjunktiv); sie sind fast immer mit PK identisch (oder durch geringfügige Veränderung hiervon abgeleitet).

Anders als etwa lateinische oder deutsche Verbstämme können hebräische Wurzeln nach mehreren Mustern konjugiert werden, z. B. als „Intensivstamm“ oder „Kausativ“. Es gibt also abgesehen von den als AK und PK bezeichneten Konjugationen, die Aktionsart oder Tempus bezeichnen, weitere Konjugationen, von denen jede ein eigenes AK und PK sowie Infinitive und Imperative bildet. Durch diese zusätzlichen Konjugationen (Intensivstamm, Kausativ) wird die Grundbedeutung der Wurzel variiert; sie sind das wichtigste Instrument bei der Bildung neuer Wörter und überaus produktiv. Im Folgenden drei Beispiele für Infinitive der Wurzel „k-t-w“ in verschiedenen Konjugationen:


Die Konjugationen sind darüber hinaus die Grundlage vieler Substantivbildungen, etwa:


Allgemeine Erklärung der Menschenrechte, Artikel 1:

In den Jahrhunderten der Diaspora verwendeten die Juden zahlreiche Sprachen wie Jiddisch, Ladino bzw. Judezmo, Karaimisch, Judäo-Arabisch und andere, die zwar nicht direkt vom Hebräischen abstammen, jedoch zahlreiche hebräische Lehnwörter aufweisen und fast stets mit dem hebräischen Alphabet notiert wurden. Weiterhin existieren einige Sprachen sozialer Gruppen (Soziolekte) mit deutlichem hebräischen Einfluss (zumeist sekundär über das Jiddische), zum Beispiel Rotwelsch und Jenisch.

Bis zum heutigen Tag werden biblische Zitate und Anspielungen in der Alltagssprache verwendet, besonders in gläubigen Milieus. Die Verse des Hoheliedes sind in unzähligen Varianten vertont worden; beim Eintritt des Frühlings wird oftmals Kapitel 2, Vers 11 zitiert: „Denn siehe, der Winter ist vergangen, der Regen ist vorbei, die Blumen zeigen sich im Lande.“

Darüber hinaus gibt es eine ganze Reihe von Sprichwörtern und Redensarten, deren biblischer Ursprung weitestgehend vergessen wurde. So geht z. B. das Sprichwort „Wer andern eine Grube gräbt, fällt selbst hinein“ auf einen Vers aus Sprüche 26,27 zurück. Ebenso sind Begriffe wie „Adamsapfel“ und „Feigenblatt“ der Bibel entliehen.

Durch die Bibelübersetzung von Martin Luther sind einige Ausdrücke und Redewendungen mit biblischem Hintergrund in die deutsche Sprache aufgenommen worden. Beispiele: "sicher wie in Abrahams Schoß", "Jubeljahr", "Kainsmal".

Vornamen hebräischen Ursprungs sind weit verbreitet: Achim, Benjamin, Daniel, David, Hanna, Jakob, Joachim, Joel, Johann, Johanna, Jonas, Jonathan, Joseph, Judith, Maria, Michael, Miriam, Rebekka, Samuel, Sarah, Susanne und viele andere.

Einige hebräische Wörter sind über das Jiddische in die deutsche Sprache gelangt, z. B. "Tacheles" aus hebräisch "tachlit" = Zweck, Sinnvolles, "meschugge" aus "meshugá" = verrückt/übergeschnappt, "malochen" aus "melacha" = Arbeit, "koscher" aus "kascher" = rein, tauglich, "dufte" wahrscheinlich aus "tov" = gut, "betucht" eventuell aus "batuach" = sicher, "Stuss" aus "schtut" = Unsinn (aus: [alt]). Auch manche Redewendungen haben möglicherweise einen hebräischen Ursprung. Dazu gehört "Wissen, wo der Barthel den Most holt", das über das Rotwelsche ins Deutsche gekommen sein könnte. Die Bedeutung wäre in diesem Fall "Wissen, wo man mit einem Brecheisen (ברזל, barzel = Eisen) zu Geld kommen, also einen Tresor knacken kann (מעות, ma'ot = Kleingeld, in der aschkenasischen Aussprache maos, wovon auch die Slangausdrücke "Moos" oder "Mäuse" für "Geld" stammen dürften). Unwahrscheinlich ist dagegen die weit verbreitete Ableitung des Wunsches zum Jahreswechsel "Einen guten Rutsch" von "Rosch ha-Schana" = "Anfang (Wörtlich: Kopf) des Jahres", weil das Wort "Rosch" in jüdischen Neujahrswünschen nie vorkommt; man wünscht sowohl auf Jiddisch als auch auf Hebräisch immer nur "ein gutes Jahr". Mit Sicherheit auszuschließen ist ein hebräischer Ursprung des Ausdruckes "Es zieht wie Hechtsuppe", der angeblich auf "hech supha" ("starker Wind") zurückgehen soll: Das Wort "hech" existiert im Hebräischen überhaupt nicht, und das mit "supha" transkribierte Wort סופה (Sturm) wird "sufa" ausgesprochen.

Aus historischen Gründen befinden sich viele Wörter aus dem Geschäftsleben darunter. Da den Juden im christlichen Europa jahrhundertelang kaum andere Erwerbsquellen erlaubt wurden als Handel oder Geldwesen, sind diese Gebiete wichtige sprachliche Schnittstellen. Hierher gehören die Ausdrücke "Kies" im Sinne von (Taschen)geld aus "kis" = Tasche; "Pleite" aus "peleta" = Flucht, Entkommen; "Reibach" aus "rewach" = Gewinn, oder Ausdrücke der Kriminalität z. B. "Ganove" (von hebräisch "ganav" = Dieb). Siehe dazu auch "Liste deutscher Wörter aus dem Hebräischen und Jiddischen".

Bei der Schaffung von Ivrit seit Ende des 19. Jahrhunderts wurden aus den europäischen Sprachen Ausdrücke entlehnt (z. B. Sigarja = Zigarette, Telefon, Telewisija = Fernsehen etc.).
Die modernen Monatsnamen in Israel entsprechen den deutschen Bezeichnungen: Januar, Februar, März usw. Die einzige Abwandlung ergibt sich beim Monat August, der "Ogust" ausgesprochen wird, da die Vokalverbindung "au" im Hebräischen ungewöhnlich ist. Die Neubildung "iton" ("Zeitung") aus "et" = Zeit basiert auf dem deutschen Wort. Das Deutsche als Bildungssprache in Osteuropa spielte indirekt auch bei der Belebung des Hebräischen in Palästina durch die mittel- und osteuropäischen Zionisten eine nicht unbedeutende Rolle, insbesondere bei der Erweiterung des Wortschatzes. Auch das umgangssprachliche Hebräisch hat etliche deutsche bzw. jiddische Ausdrücke aufgenommen, z. B. „spritz“, „Schluck“, „Spitz“, "Wischer" (für Scheibenwischer) etc. Auch im handwerklichen Sektor finden sich einige deutsche Ausdrücke, wie bspw. „Stecker“ oder „Dübel“, der allerdings – aufgrund des im Hebräischen fehlenden Ü-Lautes – „Diebel“ ausgesprochen wird.












</doc>
<doc id="7926" url="https://de.wikipedia.org/wiki?curid=7926" title="Arabisches Alphabet">
Arabisches Alphabet

Das arabische Alphabet () ist u. a. das Alphabet der arabischen Sprache und besteht aus 28 Buchstaben. Zur Bildung von Wörtern werden mit sechs Ausnahmen alle Buchstaben entsprechend der Laufrichtung der arabischen Schrift von rechts nach links verbunden.

Die Buchstaben Alif, Dāl, Dhāl, Rā, Zāy und Wāw (Nr. 1, 8, 9, 10, 11, 27) werden nicht nach links verbunden. Darauf weist jeweils ein Sternchen (*) hin.
Ursprünglich waren Alif, Bā, Dschīm, Dāl (heute die Buchstaben Nr. 1, 2, 5 und 8) die ersten vier Buchstaben. Das Wort "Abdschad" für „Alphabet“ bezieht sich auf diese vier Buchstaben in der klassischen Reihenfolge, es entspricht also der Bezeichnung „ABC“ für das lateinische Alphabet. Die alte Reihung lässt sich noch am Zahlenwert der Buchstaben ablesen (siehe Tabelle rechts). Vor der Einführung der indischen Ziffern wurden die Buchstaben für die Niederschrift von Zahlen verwendet, teilweise auch noch heute, siehe Abdschad-Zahlensystem. Diese alte Reihenfolge des Alphabets wurde später durch die heute gültige ersetzt, in der Schriftzeichen mit ähnlicher Form zusammengefasst sind.

Arabisch ist wie alle semitischen Schriften eine Konsonantenschrift. Aus dem Vokalbestand des Arabischen werden nur das lange a, i und u durch eigene Buchstaben wiedergegeben:

Waw und Ya bezeichnet man auch als Halbvokale, da sie im Silbenanlaut wie das englische w (و) bzw. das deutsche j (ي) ausgesprochen werden.

Viele Wörter beginnen mit einem Alif . Dieses Anlaut-Alif hat keinen eigenen Lautwert, sondern ist lediglich der Träger für das Hamza. Ein darauffolgender kurzer Anlautvokal wird in der Regel nicht, und falls doch, dann mit dem entsprechenden Hilfszeichen wiedergegeben. Hat das Alif am Wortanfang ausnahmsweise tatsächlich einen eigenen Lautwert als langes a, so wird es mit dem Hilfszeichen Madda gekennzeichnet.

Anders das Verbindungsalif: Am Anfang des Artikels ("al-"), der Verbstämme VII bis X sowie einigen Wörtern wie ("ibn" = Sohn) und ("ism" = Name) ist das "Alif" stumm; es erhält ein "Wasla"-Zeichen (ein ṣād ص ohne Schlussbogen). Am Satzanfang (bzw. nach einer „Pause“) wird das Hamza doch gesprochen (es erfolgt ein Stimmabsatz im Anlaut, wie im Deutschen). In der nicht-vokalisierten Schreibung wird das Wasla nicht geschrieben, sondern das Alif steht ohne Zusatzzeichen: 

Im Auslaut kann für das lange a auch das "Alif maqsura" („kurzes Alif“) stehen. Vokale in fremden Eigennamen werden abhängig von Aussprache und Betonung mit Langvokalen umschrieben, wobei o zu Waw und e zu Ya werden, oder gar nicht wiedergegeben werden.

Kurze Vokale werden in der Schrift nicht wiedergegeben. Nur in ganz bestimmten Fällen werden sie durch Hilfszeichen markiert: im Koran immer, um die unverfälschte Originalform zu garantieren, manchmal in Gedichten sowie in Lehrbüchern für Schulanfänger. Diese Hilfszeichen sind keine Buchstaben, sondern nur Lesehilfen und gehören folglich auch nicht zum Alphabet.

Falls sie notiert werden, so erscheinen Fatha, Kasra und Damma in verdoppelter Form zur Kennzeichnung der Nunation am Ende unbestimmter Nomen. Träger dieser Vokalzeichen können alle Buchstaben sein. Sie werden entweder über (Fatha, Damma) oder unter (Kasra) den betreffenden Buchstaben gesetzt, der in der Aussprache immer nach dem Buchstaben realisiert wird.

Die Verdoppelung (Verstärkung) eines Buchstaben wird im Arabischen mit einem "taschdid" ( auch: "schadda" ) über dem Buchstaben angezeigt (1). Tritt als Vokalzeichen "kasra" für „i“ hinzu, steht es normalerweise nicht unter dem Buchstaben, sondern darüber, aber unter dem "schadda".

Trägt ein Konsonant keinen Vokal, wird dies durch die Zeichen (2a) und (2b) angezeigt. Sie heißen im Inlaut "sukūn" (, Ruhe) und im Auslaut "dschazma" (, Abschnitt). Die Herkunft vom "dschīm" des "dschazma" ist in der Form (2b) noch rudimentär zu erkennen.

Endet das vorhergehende Wort auf einen Vokal, fällt das „a“ des Artikels "al-" aus. Dies kann durch das Zeichen (9) ("wasla" ) über dem "alif" angezeigt werden.

Mit dem Hamza () verfügt das arabische Alphabet über ein Zeichen für einen zusätzlichen Konsonanten, das im Inlaut einen stimmlosen glottalen Plosiv bezeichnet, im Auslaut in achtloser Aussprache wegfallen kann und im Anlaut dafür sorgt, dass die poetischen Silbenregeln eingehalten werden. Hamza wird meist mit einem „Träger“ geschrieben, der Alif, Waw oder Ya sein kann. Dadurch können zwei gleiche Buchstaben aufeinander treffen (z. B. im Wort "ru'ūs" ); in diesem Fall sind auch Schreibungen mit Hamza ohne Träger oder mit defektivem Langvokal anzutreffen. Die Form des Hamza (ء) ist vom Ain abgeleitet.

Im Arabischen wird wie in Europa die indische Zahlschrift verwendet. Da die indischen Ziffern über die arabische Welt nach Europa kamen, heißen sie im Deutschen „arabische Ziffern“. Die Unterschiede in der Form sind in Europa später entstanden. Es werden in den verschiedenen Ländern drei verschiedene Varianten benutzt: die „europäischen“, die „arabischen“ und die „persischen“ Ziffern.

Zahlen (sowohl europäische als auch arabische) werden im Gegensatz zu Wörtern immer von links nach rechts geschrieben. Die Araber übernahmen diese indische Schreibweise bei den Zahlen.

Im Arabischen werden meistens folgende Ziffern verwendet:

Die persischen Varianten der arabischen Ziffern werden in Iran, Pakistan, Afghanistan und Indien verwendet.

Für Fremdwörter werden im Arabischen einige Sonderzeichen verwendet:

, , und repräsentieren Phoneme, die im Arabischen selbst nicht Verwendung finden, wohl aber in anderen mit dem arabischen Alphabet geschrieben Sprachen, also z. B. in Iran, Afghanistan, Pakistan und Indien für Sprachen wie Persisch bzw. Dari, Urdu, Paschtu, Kurdisch und Türkisch (vor Einführung der lateinischen Schrift) genutzt werden. Im Arabischen selbst werden die genannten Konsonanten wie folgt in das eigene Phonemsystem integriert:

Eine Übersicht über fremdsprachliche Sonderzeichen gibt die Liste arabisch-basierter Alphabete.

In Domainnamen, Internetforen sowie bei der Benutzung von Chatprogrammen, die den Gebrauch arabischer Schriftzeichen nicht ermöglichen, werden häufig lateinische Buchstaben und arabische Ziffern in europäischer Ausprägung verwendet, um arabische Wörter zu schreiben; aufgrund einer wahrgenommenen Ähnlichkeit repräsentiert dabei: die Ziffer „2“ das Hamza, die Ziffer „3“ das Ain, die Ziffer „5“ das Cha, die Ziffer „6“ das Ṭa, die Ziffer „7“ das Ḥa und die Ziffer „8“ das غ (Ġain).


Umschrift

Textverarbeitung


</doc>
<doc id="7927" url="https://de.wikipedia.org/wiki?curid=7927" title="Wetter">
Wetter

Als Wetter (v. althochdt.: "wetar" = Wind, Wehen) bezeichnet man den spürbaren, kurzfristigen Zustand der Atmosphäre (auch: messbarer Zustand der Troposphäre) an einem bestimmten Ort der Erdoberfläche, der unter anderem als Sonnenschein, Bewölkung, Regen, Wind, Hitze oder Kälte in Erscheinung tritt.

Die Meteorologie klassifiziert das "örtliche Wetter" einer bestimmten Zeit anhand der verschiedenen Phänomene in der Troposphäre, dem unteren Teil der Atmosphäre. Den Verlauf des Wetters bestimmt die von Sonnenstrahlung und regionaler Energiebilanz geprägte atmosphärische Zirkulation.

Physikalisch lässt sich ein Wetter durch thermodynamische Zustandsgrößen wie etwa Druck, Temperatur, Dichte beschreiben. Ein „Wetter“ in diesem Sinne kann auch in einem Labor erzeugt werden. Darüber hinaus gibt es solche Zustände und Wetterphänomene (zum Beispiel Winde) auch auf anderen Planeten, die eine Atmosphäre haben.

Das Wetter charakterisiert den Zustand der Atmosphäre an einem bestimmten Ort und zu einem bestimmten Zeitpunkt. Kennzeichnend sind die meteorologischen Elemente Strahlung, Luftdruck, Lufttemperatur, Luftfeuchtigkeit und Wind, sowie die daraus ableitbaren Elemente Bewölkung, Niederschlag, Sichtweite etc. Das Wetter ist das augenblickliche Bild eines Vorganges (Wettergeschehen), das sich hauptsächlich in der Troposphäre abspielt. Es kann sich – im Gegensatz zur Wetterlage und Witterung – mehrmals täglich ändern.
Das Wetter kann man als ein System betrachten, das vor allem von den Elementen Temperatur, Niederschlag, Bewölkung, Wind und Luftdruck geprägt wird. Zwischen einigen der Elemente bestehen Zusammenhänge (Korrelation oder Kausalität), zwischen anderen nicht.

Die Meteorologen erfassen die einzelnen Elemente des Wetters mit Messgeräten und die Wetterlage mit Begriffen wie stabil oder wechselhaft, heiter oder wolkenfrei, 3/8 bewölkt, bedeckt oder trüb, Nebeltendenz, regnerisch, Regenschauer oder stürmisch.

Umgangssprachlich sind sehr unscharfe Begriffe üblich:

Die Meteorologie untersucht das Wetter, quantifiziert seine einzelnen Elemente und charakterisiert sie durch eine Reihe fundamentaler sowie spezieller Größen (Wetterelemente):

Diese Grundgrößen werden in Wetterstationen, auf Wetterschiffen und Leuchttürmen, mit Wetterballons oder Radiosonden, mit Flugzeugen und Bojen gemessen. Wettersatelliten, andere Erdbeobachtungssatelliten und Spionagesatelliten (letztere liefern Wetterinformationen als 'Nebenprodukt') beobachten die Troposphäre aus dem Weltall und sammeln besonders viele Informationen zur Bewölkung (auch zu großflächigen Wolkensystemen), zu Wellenhöhen und Wasseroberflächentemperaturen auf Meeren und zu Luftströmungen.

Messinstrumente die der Messung von Wetterelemente dienen nennt man Wettermessgerät (siehe auch Wetterstation, Wetterhäuschen) bzw. danach was sie messen (z. B. Windmesser, Regenmesser, Hygrometer, Thermometer).

Das Wetter findet fast ausschließlich in den unteren 10 Kilometern der irdischen Lufthülle statt, der Troposphäre. Nur hier gibt es merkliche Bewölkung, weil der Wasserdampf als entscheidender Faktor nicht über die Tropopause (je nach Ort und Jahreszeit etwa 8 bis 15 km hoch) hinaus gelangen kann.

Überwiegend prägen die unteren 2 km der Peplosphäre das Wetter. Hier findet sich oft Dunst durch Anreicherung von Aerosolen, und die nächtliche Abkühlung durch Wärmestrahlung. Die Bodenreibung bremst den geostrophischen Wind, weshalb er mehr in Richtung zum tieferen Druck weht als in größerer Höhe.

Der primäre Motor des Wetters ist die Energieeinstrahlung der Sonne und die Abstrahlung (Licht und Infrarot) zu den Wolken bzw. in den Weltraum. Das erfassen heute neben terrestrischen Messungen auch großräumig Satelliten und Wetterschiffe, Radiosonden und andere moderne Methoden gut.

Für den "Verlauf" des Wetters sind jedoch die Strömungs-Verhältnisse in der Atmosphäre entscheidend, die von ihrer wechselnden Feuchtigkeit und den globalen Windsystemen abhängen, ferner von der regional unterschiedlichen Wärmereflexion der Erdoberfläche (Albedo), vom Gelände (insbesondere den Gebirgen, Küsten und Wüsten) und von starken lokalen Einflüssen (zyklische Winde, Neigung und Bewuchs von Berghängen...), und vom Widerstand gegen Winde, über den die Rauheit der Oberfläche (Wälder, Windschneisen, große Gebäude usw.) entscheidet.

Daher sind in Mitteleuropa nur dann "lokal exakte Wetterprognosen" möglich, wenn alle diese Einzelheiten einer Modellierung oder verlässlichen Erfahrung zugänglich sind. Letztere wissen auch Laien zu nutzen – siehe die vielfach bewährten Bauernregeln mit „wetterzeigenden“ Bergen (Wetterstein, Wolkenstein usw.) oder typischen Wolken-Formationen wie Schönwetter- und Schäfchenwolken, Nebel, Regen- und Fetzenwolken, Cirren, Föhnmauern usw.

"Hauptartikel: Wettervorhersage"

Ausgehend vom durch großflächige Messungen erfassten Wetter und damit dem Zustand der Atmosphäre werden in der Meteorologie Wettermodelle genutzt, um die weitere Entwicklung des Wetters zu prognostizieren. Davon abgesehen ist es jedoch auch möglich, auf lokaler Ebene und mit vergleichsweise wenig Hilfsmitteln gute Vorhersagen zu geben, wozu jedoch auch mehr oder weniger umfangreiche Kenntnisse notwendig sind.

Für eine Reihe von Unternehmen hat das Wetter Auswirkungen auf die betrieblichen Erfolgsgrößen. Klassische Beispiele dafür sind die Landwirtschaft und die Getränkeindustrie, bei denen Wetter sich stark auf den Umsatz auswirken kann. Während bei der Landwirtschaft überwiegend die Erntemengen betroffen sind, schwankt bei den Abfüllern von Mineralwasser und Erfrischungsgetränken der Absatz in Abhängigkeit zur Temperatur. Zu den weiteren Branchen, bei denen sich das Wetter stark auswirken kann, gehören die Baubranche sowie die Tourismus- und Freizeitindustrie. Für einige Unternehmen kann das Wetterrisiko so signifikant sein, dass es gezielt im Risikomanagement des Unternehmens beobachtet und beispielsweise über so genannte Wetterderivate abgesichert wird.

Das Landgericht Cottbus beurteilte 2012 Wetter als höhere Gewalt. Demnach geht schlechtes Wetter nicht zu Lasten des Auftraggebers; es gehört nicht zur Risikosphäre eines Bestellers von Bauleistungen.

Die Wetterlage spielt bei vielen kriegerischen Auseinandersetzungen eine wichtige Rolle. Beispiele:

Seit Anfang der 1950er Jahre forscht auch das Militär über Möglichkeiten, das Wetter lokal zu beeinflussen. Eine Anwendung solcher Techniken wäre jedoch ein Verstoß gegen die ENMOD-Konvention.


"Für allgemein meteorologische Literatur siehe Meteorologie."

Deutschland
Österreich
Schweiz
Belgien
Luxemburg


</doc>
<doc id="7928" url="https://de.wikipedia.org/wiki?curid=7928" title="Unwetter">
Unwetter

Unwetter, auch Extremwetterereignis oder Wetteranomalie ist ein Sammelbegriff für extreme Wetterereignisse. Diese Wetterereignisse bewirken oft hohe Sachschäden, Katastrophen und Lebensgefahr für viele Menschen.

Extremereignisse im Sinne der Meteorologie sind Wetterlagen, die in ihrem Verlauf (dargestellt in Wetterelementen) signifikant vom Durchschnitt abweichen. Als Basis dient eine klimatologische Normalperiode, ein geographischer Bezug zu einer Klimaklassifikation, als Maß der Ausnahmeerscheinung die Jährlichkeit der Wetterelemente und anderer Wirkungsfaktoren (wie die Hochwasserpegel), wie auch der Versicherungsschaden oder der gesamtwirtschaftliche (Versicherter und unversicherter Direktschaden, Folgeschäden und Wiederherstellung, einschließlich der Opfer). Dem Begriff liegt keinerlei präzise Definition zugrunde, sondern ist ein pragmatischer Ausdruck der Dokumentation von Klima und Wetter in der Klimafolgenforschung oder Versicherungswesen: 

Extremereignisse sind von besonderer historischer und wirtschaftlicher Bedeutung. Als klimatologische Indikatoren sind sie aber ungeeignet: zum einen treten sie sehr unregelmäßig ein, und zum anderen muss der Mittelwert einer Normalperiode bekannt sein, um eine Wetteranomalie als solche klassifizieren zu können. Der aktuelle langfristige Mittelwert setzt sich aber genau aus den eintretenden Wetterereignissen zusammen, aktuelle Extremereignisse können also nur mit abgelaufenen Bemessungszeiträumen verglichen werden / in Kontexte gesetzt werden.

Der Deutsche Wetterdienst definiert folgende Ereignisse als Unwetter (Stufe 3 der Kriterienskala im Bereich 0–4), wenn die genannten Schwellen überschritten werden:

Folgende Ereignisse werden noch für Unwetterwarnungen seitens der meteorologischen Dienste herangezogen:

Folgende Ereignisse werden allgemein noch als speziellere Unwetter angesehen:

Darüber hinaus wurden seit dem Jahr 1993 von der Internationalen Zivilluftfahrt-Organisation (ICAO) neun Volcanic Ash Advisory Center eingerichtet, die weltweit den Luftraum auf Vulkanasche überwachen und gegebenenfalls den Luftverkehr warnen. Diese gehören wegen der meteorologischen Prognose der Zugbahnen der Aerosolemissionen zum Themenfeld.


Zwischen 1980 und 2016 haben sich z. B. in Deutschland nach dem Versicherungskonzern "Münchener Rück" die durch Extremwetter (Gewitter) verursachten Schäden von durchschnittlich rund 580 Mio. auf über 2 Mrd. Euro praktisch vervierfacht.

Bis 2100 könnten laut einer in der Fachzeitschrift "The Lancet Planetary Health" veröffentlichten Studie des "Joint Research Centre" der Europäischen Kommission jährlich bis zu zwei Drittel der europäischen Bevölkerung durch Wetterextreme betroffen sein, ohne weitere Anpassungsmaßnahmen an den weltweiten Klimawandel zwischen 2071 und 2100 in der EU, Schweiz, Norwegen und Island pro Jahr 80.000 bis 240.000 Menschen sterben. Zwischen 1981 und 2010 sind hiernach jährlich rund 3.000 Europäer durch Wetterkatastrophen um ihr Leben gekommen. 99 % der Wetter-Todesopfer zwischen 2070 und 2100 könnten aufgrund von Hitze sterben.

Anfang September 2017 brachte mit über 6 Mio. Betroffenen im US-Bundesstaat Florida der Hurrikan Irma eine der größten Evakuierungsaktionen mit sich.

Neben Schäden, Verletzten und Toten, sorgen Unwetter auch für Vertreibung von Menschen. 2016 waren fast 24 Millionen wegen Wetterextremen auf der Flucht und das vor allem in armen Gebieten. In reicheren Ländern waren dagegen nur knapp eine Million Menschen im Jahr betroffen.





</doc>

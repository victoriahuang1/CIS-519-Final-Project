<doc id="3391" url="https://de.wikipedia.org/wiki?curid=3391" title="Milch">
Milch

Milch ist eine weiße, trübe Emulsion bzw. kolloidale Dispersion von Proteinen, Milchzucker und Milchfett in Wasser. Die Milch vieler domestizierter Tiere nutzt der Mensch als Nahrungsmittel. Gebildet wird sie in den Milchdrüsen von Säugetieren, die damit ihre Neugeborenen nähren.

Das Substantiv "Milch", ebenso wie "Molke" und das zugehörige Verb "melken", sind germanischsprachiges Gemeingut (schwed. "mjölk", dän. "mælk", niederl. "melk", engl. "milk", isländ. "mjólk" usw.). Althochdeutsch "miluh" (8. Jh.) wandelt sich zu mittelhochdeutsch "milich, milch". Die fachsprachliche Mehrzahlbildung lautet "Milche" oder auch "Milchen".

„Milch“ ist allgemein der Name für eine Nährflüssigkeit, die bei Säugetieren (Mammalia) von weiblichen Individuen nach einer Schwangerschaft durch Drüsen der Milchleisten über Mamillen bzw. Zitzen an Brüsten (Mammae) oder Eutern dem saugenden Nachwuchs (Säugling) als (zunächst einziges) Nahrungsmittel zur Verfügung gestellt wird.

Im Deutschen wird der Ausdruck „Milch“ vornehmlich als ein Synonym für "Kuhmilch" gebraucht. Die Milch des Menschen wird distinguierend hingegen explizit als Muttermilch bezeichnet. Im Handel innerhalb der Europäischen Union darf allein die Milch von Kühen als „Milch“ bezeichnet werden (§ 2 der Milchverordnung definiert „Milch“ als „das durch ein- oder mehrmaliges tägliches Melken gewonnene, unveränderte Eutersekret von zur Milchgewinnung gehaltenen Kühen“). Bei Milch anderer Säugetiere muss zusätzlich die Tierart angegeben werden (beispielsweise Ziegenmilch, Schafmilch, Pferdemilch bzw. Stutenmilch, Kamelmilch, Büffelmilch). Dementsprechend werden Soja-Getränke im Handel auch nicht als „Sojamilch“ ausgewiesen.

Die Entwicklung der Milchwirtschaft begann im Zuge der so genannten neolithischen Revolution mit der Domestikation von Ziegen und Schafen, etwa vor 10.000 Jahren, in Westasien und andernorts, sowie mit der Domestikation von Auerochsen (Ur) vor etwa 8.500 Jahren vor allem in Südosteuropa.

Für die Nahrungsmittelindustrie Europas sind Milchkühe der Hauptlieferant, in den Bergen, ertragsschwachen Gegenden und in früheren Zeiten auch das Schaf (Schafsmilch) und die Ziege (Ziegenmilch). Für Trinkmilch melkt der Mensch auch Hauspferde (Stutenmilch) und Hausesel (Eselsmilch), Yaks in West-China/Tibet, in den Anden Südamerikas teilweise auch Lamas (selten). Hoch im Norden wird auch die Milch der Rentiere genutzt; in Asien und Italien zur Käseproduktion (Mozzarella di Bufala) werden Wasserbüffel gemolken und Büffelmilch gewonnen; im arabischen Raum wird, neben Ziegen- und Schafmilch, Milch von Kamelen konsumiert. Mäusemilch wird ausschließlich zu Versuchszwecken gewonnen.

In manchen Kulturen, welche meist aus Hirten und Nomaden hervorgegangen sind, steht die Milchtier­haltung, die Milch und ihre Produkte (etwa Käse, Joghurt) im Mittelpunkt der Ernährung und damit auch des Lebens. Ähnlich ist dies auch in der westlichen Welt. Andererseits gibt es auch Völker, die außer Muttermilch gar keine Milch verwenden.

Der als "Veredelung" bezeichnete Herstellungsprozess von zahlreichen Milchprodukten kann als kontrollierter „Verderb“ aufgefasst werden, da hier vor allem der originären Milchflora zugehörende Milchsäurebakterien wirken. Gleiches gilt auch für die Zugabe von Lab, was bewirkt, dass die Milch – ähnlich wie die gesäuerte – koaguliert.

Die Verarbeitungsstätten nennt man Molkereien (früher teilweise auch "Meiereien") bzw. Käsereien, typische Produkte sind Sahne, Butter und Buttermilch, Käse, Sauermilch.

Milchrohstoffe (Derivate für die Weiterverarbeitung) sind etwa Milchpulver, Molkepulver (Speiseeisproduktion, Zusätze zu anderen Lebensmitteln), Lactose (Milchzucker) und Ähnliches in Lebensmittelherstellung, Pharmazie, Kosmetika usw., Kasein als Klebersubstanz in zahlreichen Branchen.

Die Nahrungsmittelindustrie verarbeitet die Milch in zahlreichen Formen und zu vielfältigen Produkten (Produktgruppe: "Milchprodukte"), angefangen von Butter, Rahm, der Verkäsung bis hin zu Backwaren- oder Speiseeis­herstellung, sowie Derivaten, vom Einsatz in der Fleisch­verarbeitung oder in der Fertignahrungs­herstellung bis hin zur Pharmazie und Kosmetika "(Milchrohstoffe)".

Im Jahr 2016 wurden laut Ernährungs- und Landwirtschaftsorganisation FAO weltweit 659 Millionen Tonnen Kuhmilch produziert. Folgende Tabelle gibt eine Übersicht über die 20 größten Produzenten von Kuhmilch weltweit, die insgesamt 74,6 % der Gesamtmenge produzierten. Außerdem befinden sich in dieser Tabelle die Zahlen für die Schweiz und Österreich zum Vergleich:

In Europa wurden 2016 knapp 215,6 Mio. Tonnen Kuhmilch produziert. Die größten Produzenten waren Deutschland, Frankreich und Großbritannien.

Der Milchkonsum steigt weltweit stark an, jedoch überproportional hauptsächlich in Form von verarbeiteten Milchprodukten. In der Schweiz ist zwischen 2004 und 2015 der Pro-Kopf-Konsum von Milch um knapp 22 Kilo oder mehr als ein Viertel gesunken.

Die Spitzenreiter beim Pro-Kopf-Verbrauch waren bei der Vollmilch Kasachstan mit 258,44 kg, bei der Butter Neuseeland mit 9,25 kg und beim Käse Island mit 30,82 kg pro Kopf und Jahr.

Die Einteilung in Handelsklassen erfolgt in Deutschland durch die Milch-Güteverordnung. Die Kriterien umfassen die Gesamtkeimzahl (niedrige Werte sprechen für Betriebshygiene und gute Tiergesundheit), Eiweiß- und Fettgehalt, Gefrierpunkt (Abweichungen deuten auf Streckung mit Wasser) und Hemmstoffe wie Antibiotika, welche die Weiterverarbeitung der Milch zu Joghurt oder Käse behindern und zum Lieferstopp für den Landwirt führen.

Die Dichte von Kuhmilch ist von der Temperatur abhängig; sie beträgt für homogenisierte und pasteurisierte, 3,5 % Fett enthaltende frische Vollmilch bei einer Temperatur von 20 °C etwa 1,032 g/cm³.

In der Milch sind Kohlenhydrate, Eiweiße, Vitamine und Spurenelemente im Wasser gelöst, Milchfett im Wasser emulgiert. Anteile der einzelnen Inhaltsstoffe sind jedoch von Tierart zu Tierart unterschiedlich. Bei Tierarten, die einen sehr energieintensiven Stoffwechsel betreiben, ist die Milch besonders reich an Fetten, Proteinen und Kohlenhydraten. Dazu gehört z. B. die Milch von Walen und Eisbären. Innerhalb einer Art haben auch Fütterung, Haltung, Laktations­zeit sowie Gesundheitszustand und Alter der Tiere Einfluss auf die Zusammensetzung der Milch.

Die häufigsten Proteine, die etwa 80 % der Gesamtproteinmenge ausmachen, sind die Caseine. Die übrigen Proteine werden auch als "Molkenproteine" zusammengefasst. Unter der Bezeichnung Molkenproteine fasst man beta-Lactoglobulin, alpha-Lactalbumin, Serumalbumin, Immunglobulin und Proteosepepton zusammen. Die beim längeren Kochen von Milch zu beobachtende Entwicklung einer Haut an der Oberfläche wird durch die hitzeinduzierte Denaturierung von Albumin verursacht.

Das wichtigste Kohlenhydrat in der Milch ist Lactose (4,6 % in Kuhmilch), daneben sind Galactose, Glucose und Spuren anderer Kohlenhydrate enthalten. Der pH-Wert von Milch schwankt zwischen 6,7 für frische Milch bis etwa 4,5 für saure Milch.

Der natürliche Fettgehalt von Kuhmilch liegt bei ca. 4,2 %. Zur Einstellung des Fettgehalts wird die Milch zunächst in einer von Wilhelm Lefeldt für diesen Zweck entwickelten Milchzentrifuge (auch Separator genannt) in Rahm, Magermilch und Nichtmilchbestandteile getrennt. Durch Zentrifugal- und Zentripetalkräfte ordnen sich diese drei Komponenten mit steigender Dichte wie folgt von innen nach außen an: Rahm (innen; Dichte von reinem Milchfett: 0,93 kg/dm³), Magermilch (mittig; Dichte 1,035–1,038 kg/dm³), Nichtmilchbestandteile (außen durch die höchste Dichte). Die Nichtmilchbestandteile werden außen im Separator als Zentrifugenschlamm gesammelt und nach einer bestimmten Zeit bei einer Teilentleerung der Trommel entfernt. Anschließend kann der Fettgehalt der Magermilch durch Hinzufügen von Rahm beliebig eingestellt werden. Die Bestimmung des Fettgehalts kann nach Weibull-Stoldt oder nach Röse-Gottlieb erfolgen.

Bezeichnungen wie "Landmilch" mit z. B. 3,8 % Fettgehalt, "Fitmilch" usw. sind keine genormten Bezeichnungen. Milcherzeugnisse oder milcherzeugende Betriebe werden auch häufig nach der Region benannt, aus der sie hauptsächlich stammen, beispielsweise "Gmundner Milch" oder "Berchtesgadener Milch".

Ab-Hof-Milch ist Rohmilch, die Verbraucher direkt beim Milchviehbetrieb erhalten.

Bei "Lactosefreier Milch" ist der Fettgehalt nicht normiert, bei ihrer Herstellung wird der Milchzucker enzymatisch in Glucose und Galactose gespalten (siehe Unverträglichkeit).

In der landwirtschaftlichen Erzeugung spricht man von "fettkorrigierter Milch" (FCM, ), wenn diese 4 Prozent Fett aufweist.

Sowohl die frische Milch als auch aufkonzentrierte Milchprodukte wie Kondensmilch oder Kaffeesahne werden durch Erhitzen haltbar gemacht.

Dazu gibt es verschiedene Verfahren:

Die Haltbarkeit bezieht sich immer auf die ungeöffnete, von der Molkerei abgefüllte Milchpackung. Nach der erstmaligen Verwendung ist auch haltbare Milch im Kühlschrank aufzubewahren und sollte innerhalb weniger Tage verbraucht werden. Je nach Kühlschranktemperatur kann die Haltbarkeit auch im geöffneten Zustand deutlich verlängert werden. Eine Absenkung der Kühlschranktemperatur von 7 °C auf 5 °C kann die mikrobielle Stabilität zusätzlich um einige Tage verlängern.

Die Effekte der Verfahren zur Haltbarmachung auf den Vitamingehalt der Milch sind unterschiedlich: Studien fanden nach Pasteurisation reduzierte Gehalte an Vitamin B1, B12, E und Folsäure. Der Gehalt an Vitamin B6 blieb unverändert; bei Vitamin A zeigte sich eine Steigerung des Gehalts nach Pasteurisation.
Soweit Vitaminverluste eintreten, liegen diese nach Pasteurisation bei weniger als 10 %. Der Vitaminverlust durch die Herstellung von ESL-Milch liegt bei durchschnittlich 10 %. Die Ultrahocherhitzung hat einen Verlust von etwa 20 % des Vitamin B12 zur Folge; die Verluste bei Vitamin B1, C und Folsäure belaufen sich auf 10 %. Lagerungsbedingt können sich bei Vitamin B1 und B6 Verluste einstellen. Das Ausmaß hängt von der Lagertemperatur und der verwendeten Erhitzungstechnik ab (direkt mittels Dampfinjektion oder indirekt), bei direkt erhitzter H-Milch oder ESL-Milch zusätzlich davon, wie gründlich die Milch anschließend entgast wurde. Sterilisierung bedeutet den höchsten Vitaminverlust, je nach Vitamin und Lagerdauer 20-100 %. Der Vitaminverlust durch einfaches Abkochen von Rohmilch liegt bei 10–30 %.

Eine andere Konservierungs­art der Milch ist die energieintensive Trocknung zu Milchpulver oder teilweise Entziehung des Wassers (Kondensmilch). Milchpulver finden etwa Verwendung in der Schokoladenindustrie, bei der Herstellung von Säuglingsnahrung, in der parenteralen Ernährung oder als Aufzuchtfutter für Kälber.

Bei Rohmilch oder Milch, die lediglich einem thermischen Behandlungsschritt unterzogen worden ist, sammelt sich das Milchfett nach einiger Zeit an der Oberfläche an und bildet eine Rahm­schicht. Durch die Homogenisierung wird dies verhindert, indem die Größe der Fetttröpfchen auf unter 1 µm Durchmesser reduziert wird und damit die Aufrahmung aus physikalischen Gründen nur über einen sehr langen Zeitraum stattfindet. 

Ziel der Homogenisierung ist es, den mittleren Durchmesser der in der Milch vorhandenen Fettkügelchen (mittlerer Durchmesser der nativen Globule 10 bis 30 µm) unter hohem Druck (150 bis 300 bar) stark zu reduzieren (mittlerer Tropfendurchmesser 1 bis 2 µm), damit die Milch nicht aufrahmt und wegen der vergrößerten Gesamtoberfläche leichter verdaut werden kann. Industriell geschieht diese „Zerkleinerung“ der Fetttröpfchen in großem Maßstab. Dazu wird die Milch unter hohem Druck auf eine Metallplatte gespritzt. Im Homogenisator wirksame Kräfte sind Scher-/Dehnkräfte, Prallströmungen, aber hauptsächlich Kavitation. Physikalisch gesehen kann zwar eine so behandelte Milch immer noch aufrahmen, allerdings steigt die für eine sichtbare Aufrahmung benötigte Zeit sehr stark an, so dass man über die Produktlebensdauer vereinfacht von einer „Aufrahmungsstabilität“ spricht. Die Homogenisierung allein führt allerdings "nicht" dazu, dass die Milch aus mikrobieller Sicht länger haltbar wird.

Milchersatz oder Milchersatzprodukte sind Nahrungsmittel, die geschmacklich oder optisch sowie vom Fett- oder Eiweißgehalt her Milch (oder Milcherzeugnissen) ähneln, ohne aus dieser hergestellt zu sein. Diese werden in der Regel aus pflanzlichen Produkten gewonnen, etwa Getreide- und Sojamilch.

100 ml Milch enthalten 120 mg Calcium, das der Mensch zum Knochenaufbau benötigt. Milch enthält außerdem viele essentielle Aminosäuren, die für den Körperzellenaufbau benötigt werden. In Käse ist der Anteil an Calcium höher. Die "Nurses’ Health Study" zeigte allerdings, dass erhöhter Milchkonsum allenfalls tendenziell Knochenbrüchen vorbeugt. Auch lässt sich das Calcium aus der Milch isoliert nicht resorbieren, es wird dazu Vitamin D benötigt, das nicht in ausreichendem Maße in der Milch enthalten ist. Eine im Jahr 2005 veröffentlichte Metaanalyse sechs prospektiv untersuchter Kohorten fand, dass geringer Milchkonsum (weniger als ein Glas täglich) mit keinem signifikanten Anstieg des Frakturrisikos assoziiert war.

Michael de Vrese vom Max Rubner-Institut erklärte im Jahr 2006, dass „[…] die Vorteile des Milchkonsums […] die etwaigen Risiken übertreffen […].“ Es sei bewiesen, dass ein ausreichender Milchkonsum Osteoporose, Bluthochdruck, Herzinfarkt und Übergewicht vorbeuge.

Eine Untersuchung der Universität Kopenhagen aus dem Jahr 2007 hat ergeben, dass das in der Milch enthaltene Calcium oder andere Cofaktoren in Milchprodukten die Fettmenge im Blut nach den Mahlzeiten reduziert. Danach ist die Menge des Blutfettes bei Personen, die Calcium aus Milchprodukten aufnehmen, um 15–19 % niedriger als bei Vergleichspersonen, die Calcium über Calciumsupplemente aufnehmen.

Das Bundesinstitut für Risikobewertung (BfR) sieht in Kuhmilch eines der „wichtigsten Allergie auslösenden Lebensmittel im Kindesalter“ (neben Hühnerei, Fisch, Soja, Weizen und Erdnüssen/Nüssen). Bei Vorliegen einer familiären Neigung (Atopie) könne es infolge einer Nahrungsmittelallergie zu Neurodermitis, Heuschnupfen und Asthma bronchiale kommen. Unter anderem Kuhmilch stelle auch für Erwachsene ein wichtiges Nahrungsmittelallergen dar. Allergische Reaktionen auf Hühnerei und Kuhmilch verlören sich allerdings häufig in den ersten Lebensjahren. Das Institut empfiehlt, unabhängig von einer möglichen erblichen Disposition, mindestens während der ersten 4–6 Lebensmonate zu stillen und keine Kuhmilch (oder andere Beikost) zu geben. Liegt eine Allergie gegen die Molkenproteine α-Lactalbumin oder β-Lactoglobulin vor, kann Hitzebehandlung helfen, die Milch für den Allergiker verträglich zu machen.

Milch kann von Rückständen pharmazeutischer Wirkstoffe betroffen sein. Da Tierarzneimittel in der Tierhaltung erlaubt sind, müssen Wartezeiten eingehalten werden, damit die Tiere den Großteil wieder ausscheiden, bevor ihre Produkte zum Verbraucher gelangen. Zum Schutz des Verbrauchers vor Nebenwirkungen (wie das Hervorrufen von Krebs oder Erbgutschädigungen) gelten Rückstands-Höchstmengen. Die produzierte Milch wird stichprobenartig auf bestimmte Rückstände untersucht.

In Deutschland wird je 15.000 Tonnen Milch eine Probe entnommen; für das Jahr 2005 ergaben sich so 1.834 Proben. 2005 musste die Milch in den Mitgliedstaaten der EU auf mindestens 45 Stoffe untersucht werden. In Deutschland geht man über diese Vorgabe der EU-Kommission jedoch deutlich hinaus: im Jahr 2004 wurde hier auf 351 Stoffe getestet. In den Jahren zwischen 1998 und 2003 wiesen durchschnittlich 0,1 Prozent der in Deutschland untersuchten Milchproben Rückstände auf, die unzulässig waren oder die eine definierte Höchstmenge überschritten. Die Laboratorien der Bundesländer fanden vor allem das in der Tierhaltung verbotene Antibiotikum Chloramphenicol sowie Phenylbutazon, ein nicht zugelassenes, entzündungshemmendes Mittel. In Deutschland macht das Bundesamt für Verbraucherschutz und Lebensmittelsicherheit (BVL) für jedes Bundesland konkrete Vorgaben über die Zahl der zu untersuchenden Tiere oder die tierischen Erzeugnisse, die zu untersuchenden Stoffe, die anzuwendende Methodik und die Probenahme. Bei Betrieben, die im Verdacht stehen, dass Tiere mit verbotenen Stoffen behandelt werden oder dass Tierarzneimittel nicht fachgerecht angewendet werden, führen die Behörden für Lebensmittelüberwachung gezielte Proben durch.

Studien deuten auf eine Wechselbeziehung zwischen dem regionalen Kuhmilchverbrauch hin: In Japan kamen Mitte der 1980er-Jahre auf 100.000 Einwohner weniger als zwei Kinder, die jährlich neu an Typ-1-Diabetes erkrankten (damaliger Pro-Kopf-Verbrauch: 38 Liter pro Jahr). In Finnland waren es 29 Kinder auf 100.000 Einwohner (damaliger Pro-Kopf-Verbrauch: 229 Liter pro Jahr). Finnische und kanadische Wissenschaftler fanden in den Blutproben mehrerer hundert Kinder, die neu an Diabetes erkrankt waren, Antikörper gegen einen Bestandteil des Milchproteins, das dem natürlichen Protein p69 ähnelt. Sie vermuten, dass das Immunsystem von Babys, die eine genetische Anfälligkeit für Diabetes haben und vor dem fünften oder sechsten Monat Kuhmilch trinken, bei jeder Virusinfektion irrtümlich das natürliche Protein attackiert, wodurch die Bauchspeicheldrüse geschädigt werden kann.

Hubert Kolb, Immunbiologe am Deutschen-Diabetes-Zentrum, führte 2004 Beobachtungen an, die einem diabetogenen Effekt von Kuhmilchproteinen widersprechen. Er verwies auf Daten, die nach seiner Ansicht dagegen sprechen, dass Kuhmilchproteine einen spezifischen Effekt auf die Entstehung von Typ-1-Diabetes haben:

Rohmilch ist weitgehend unbehandelte, lediglich filtrierte Milch und kann, abhängig von den hygienischen Bedingungen, schon ab Euter mit Krankheitserregern belastet sein. Beim Verzehr können diese auf den Menschen übertragen werden und Infektionskrankheiten wie Salmonellose, Campylobacter-Enteritis, Staphylokokken-Infektionen, Listeriose, Brucellose, Darmtuberkulose, Brainerd Diarrhoe oder Enterohämorrhagische Colitis auslösen. Für die Herstellung und den Verkauf von Rohmilch und Rohmilchprodukten gelten in der Europäischen Union besondere Hygienevorschriften, die das Infektions­risiko minimieren sollen.

Wissenschaftliche Nachweise für gesundheitliche oder nährwertbezogene Vorteile von Rohmilch fehlen. Mit der Ausnahme des Vorkommens von Staphylokokken-Enterotoxinen lassen sich durch Pasteurisierung oder Ultrahocherhitzung die mikrobiologischen Gefahren praktisch ausschließen. Lediglich bei Fehlern in der Technik der Wärmebehandlung oder durch anschließende Rekontamination kann auch pasteurisierte Milch einen Auslöser von Infektionen bilden. Diese Unfälle zählen in der modernen Molkereitechnologie allerdings zur Ausnahme.

Milch und viele Milchprodukte enthalten hohe Mengen an Calcium. Zwei prospektive Kohortenstudien zeigten, dass der Konsum von Calciumdosen > 2000 mg pro Tag mit einem erhöhten Risiko für Prostatakrebs einhergeht. Zwei andere prospektive Kohortenstudien brachten keinen Zusammenhang für Calciumdosen von 1330 und 1840 mg pro Tag. Als Hintergrund für die Risikoerhöhung wird eine mangelhafte Produktion von Vitamin D verdächtigt. Eine hohe Calciumzufuhr vermindert die körpereigene Cholecalciferol-Produktion und präklinische Studien zeigten mehrere potenziell nützliche Effekte des Vitamins bezüglich Prostatakrebs. Sonn u. a. fanden 2005 im Rahmen einer Übersichtsarbeit unter neun prospektiven Studien fünf, die einen Zusammenhang zwischen dem Konsum von Milchprodukten und dem Prostatakrebsrisiko herstellten. In welchem Ausmaß der Calciumkonsum im Verhältnis zum Fettkonsum aus Milch und Milchprodukten zum Risiko beiträgt, ist unklar. Eine Metaanalyse von Gao u. a. kam 2005 zu dem Schluss, dass die hohe Aufnahme von Milchprodukten und Calcium mit einem leicht erhöhten Prostatakrebsrisiko verbunden sein könne. Diese Schlussfolgerung basiert nach Einschätzung durch Severi u. a. jedoch auf „relativ schwacher statistischer Evidenz“ und einer „sehr kleinen Effektgröße“. Zudem konnten die Kritiker der Metaanalyse Studiendaten vorweisen, die nicht zur Schlussfolgerung von Gao u. a. passen. Laut "Harvard School of Public Health" könne man nicht zuversichtlich sein, dass ein hoher Milch- oder Calciumkonsum empfehlenswert sei. Wissenschaftler der University of Hawaii gehen davon aus, dass ein übermäßiger Konsum an fettarmen Milchprodukten die Wahrscheinlichkeit, an Prostatakrebs zu erkranken, erhöht. Einer gemeinsamen Bewertung durch den "World Cancer Research Fund" und das "American Institute for Cancer Research" aus dem Jahr 2007 zufolge senkt Kuhmilch wahrscheinlich das Darmkrebsrisiko. Wegen der Hinweise, dass sehr hohe (≥ 2 g) Calciumtagesdosen das Prostatakrebsrisiko möglicherweise erhöhen, sprechen die Autoren jedoch keine krebsrelevanten Empfehlungen bezüglich des Konsums von Kuhmilch aus. Laut Michael de Vrese vom "Max Rubner-Institut" ist das Prostatakrebsrisiko aus Milchkonsum noch nicht abschließend bewertet.

Chen u. a. untersuchten 2002 den Zusammenhang zwischen der Nahrungsaufnahme und dem Parkinsonrisiko. Sie fanden eine positive Assoziation zwischen der Kuhmilchaufnahme und dem Parkinsonrisiko bei Männern, nicht jedoch bei Frauen. Als Ergebnis einer weiteren Datenauswertung im Jahr 2007 ergab sich zusätzlich eine Risikoerhöhung für weibliche Konsumenten von Milchprodukten, wobei die betrachteten Frauen weniger von der Risikoerhöhung betroffen waren als die Männer. Die Autoren schlossen aus den Daten, dass der Konsum von Milchprodukten das Risiko für Parkinson erhöhen könne, besonders bei Männern. Jedoch seien weitere Studien zur Untersuchung der Befunde und des zugrundeliegenden Mechanismus nötig. Eine japanische Kontrollstudie fand 2011 keinen Zusammenhang zwischen der Aufnahme von Milchprodukten und der Parkinsonkrankheit. Der Konsum von Milch und Milchprodukten ist als möglicher ätiologischer Faktor bis heute (Stand: 2014) in der Diskussion. Jedoch ist die Erklärung für den epidemiologisch beobachteten Zusammenhang bis heute unbekannt. Es fehlt der wissenschaftliche Nachweis, dass Milch einen Risikofaktor für die Parkinsonkrankheit darstellt. Einschränkungen des Milchkonsums mit der Absicht, hierüber die Entwicklung oder das Fortschreiten der Parkinsonkrankheit zu verhindern, werden nicht empfohlen.

Es existieren Unverträglichkeiten auf das Nahrungsmittel Milch und die daraus hergestellten Produkte. Sie basieren darauf, dass Milchbestandteile im Körper nicht hinreichend aufgespalten werden können (wegen Lactoseintoleranz oder Milcheiweiß-Unverträglichkeit) oder darauf, dass sonstige Inhaltsstoffe der Milch nicht vertragen werden. Die Fähigkeit, den in der Milch enthaltenen Milchzucker auch als Erwachsener verdauen zu können, ist eine genetisch recht junge Entwicklung und wird auf 8000 bis 9000 Jahre geschätzt. Zu ihrer Ausbreitung kam es vermutlich zuerst in Nord- und Mitteleuropa mit oder kurz nach dem dortigen Beginn des Neolithikums, als sich mit Beginn der Viehzucht die Verträglichkeit als positives Selektionskriterium erwies.

Zur Verdauung der Lactose ist das Enzym Lactase erforderlich, dessen Produktion bei Kleinkindern während der Stillzeit voll ausgeprägt ist, in späteren Jahren aber teilweise oder vollständig zurückgeht. Klinische Versuche haben ergeben, dass ein Teil der Menschen aufgrund dessen bei der Aufnahme von Lactose mit Beschwerden (Durchfall, Blähungen, Völlegefühl, Magen­drücken, Aufstoßen, Meteorismus, Koliken, Bauchschmerzen, Darm­krämpfen, Übelkeit bis zum Erbrechen, Migräne­attacken, Kreislaufproblemen, Schwächeanfällen) reagieren (Lactoseintoleranz). Wenn diese Symptome bei Konsum von Milchprodukten in normalen Mengen eintreten, kann eine Milchunverträglichkeit vorliegen. Dieser kann durch Nahrungs­umstellung oder Einnahme von Lactasetabletten begegnet werden. Etwa 10–15 Prozent aller Erwachsenen in Europa vertragen keine lactosehaltige Milch.

Die größte Konzentration Erwachsener, die Lactose verwerten können, findet sich in Europa nördlich der Alpen. Über 95 Prozent der Norddeutschen, Niederländer, Dänen, Schweden und anderer Skandinavier verfügen über eine körpereigene Laktase­enzymgenese, um ihr ganzes Leben lang Lactose verdauen zu können. Ein Großteil der mittel- und südasiatischen erwachsenen Bevölkerung verträgt im Erwachsenenalter keine Kuhmilch mehr, bei ihnen besteht eine Lactoseintoleranz.

Es gibt zahlreiche Hersteller lactosefreier Milch und Milchprodukte. Dazu wird der Milch das Enzym Lactase zugesetzt, die die Lactose in ihre Ausgangszucker Glucose und Galactose spaltet, die Verdauung also quasi vorwegnimmt. Diese lactosefreie Milch schmeckt süßer als Milch, weil Glucose und Galactose süßer schmecken als der ursprüngliche Milchzucker.




</doc>
<doc id="3393" url="https://de.wikipedia.org/wiki?curid=3393" title="Medici">
Medici

Die Familie der Medici ( []; vollständig de’ Medici) aus Florenz bildete vom 15. bis 18. Jahrhundert eine einflussreiche italienische Dynastie, aus der Großherzöge der Toskana, Päpste und zwei Königinnen von Frankreich hervorgingen.

Die Medici erwarben ihren Reichtum im Textilhandel, der von der Gilde Arte della Lana betrieben wurde. Auf dieser Basis begründeten sie ein modernes Bankwesen und dominierten – auch durch ihre Beziehungen zum Papsttum – die europäische Finanzwelt der frühen Neuzeit. Ihr Mäzenatentum ermöglichte und prägte die Renaissance in Florenz.

Die Medici führten einen Rossstirnschild, eine besondere Schildform, die nur in der italienischen Heraldik auftritt.

Blasonierung: In Gold sechs schwebende Pillen (Kugeln), 1:2:2:1 gestellt, die oberste blau mit drei goldenen Lilien belegt, die anderen rot. Die Lilien des französischen Königswappens wurden 1465 von Ludwig XI. als Gnadenzeichen verliehen, bis dahin waren die Pillen einheitlich rot.

Wenn ein Vollwappen der Großherzöge der Toskana abgebildet wird, sind die Decken rot-golden, und auf dem gekrönten Helm befindet sich eine rote Lilie, darauf ein (naturfarbener oder schwarzer) Sperber, mit dem Schnabel und der erhobenen rechten Klaue einen durch goldenen Ring gezogenes silbernes Band mit der Devise "semper" haltend.

Die frühere Helmzier der Medici war auf gekröntem Helm ein wachsender schwarzer Hund mit silbernem Halsband. Die Helmdecken außen gold mit roten Kugeln bestreut, innen silbern-schwarzer Eisenhutfeh.

Die aus dem Florentiner Umland stammenden Medici lassen sich erstmals in der 2. Hälfte des 13. Jahrhunderts als Amtsinhaber in der Gilde der Kaufleute nachweisen. Sie gehörten zum bürgerlichen Patriziat der Stadt, die wie die meisten oberitalienischen Städte des Mittelalters republikanisch-oligarchisch regiert wurde "(siehe dazu ausführlicher die Artikel Geschichte von Florenz bzw. Italienischer Adel)". 

Mit Salvestro de’ Medici (1331–1388), der 1378 zum Gonfaloniere gewählt wurde, traten sie erstmals ins Rampenlicht. Er spielte eine schillernde Rolle in den Auseinandersetzungen zwischen den konservativen Gilden und den revolutionären Wollwebern (Ciompi-Aufstand). Für kurze Zeit übte er diktatorische Macht aus, wurde aber 1382 verbannt. In dieser Zeit waren die Medici eine wohlhabende, aber keineswegs die führende Kaufmannsfamilie in Florenz. Im 14. Jahrhundert gehörten vielmehr die Florentiner Familien Bardi und Peruzzi zu den größten Bankiers Europas. 
Mit Salvestros Neffen Giovanni di Bicci de’ Medici (1360–1429) und der Entwicklung des Banco Medici begann der Aufstieg der Familie: Giovanni wurde zum Bankier des Gegenpapstes Johannes XXIII., der freilich vom Konzil von Konstanz 1415 abgesetzt wurde. In Florenz vermittelte bzw. taktierte er zwischen dem Stadtadel (unter Führung des Gonfaloniere Maso degli Albizzi) und dem Volk. 

Der ältere Zweig der Familie, die "Medici di Cafaggiolo", beherrschte danach Florenz bis 1537 mit zwei Unterbrechungen (1494–1512 und 1527–1530), darunter vor allem Cosimo de’ Medici, genannt „il Vecchio“, der Alte, (1389–1464). 1433 wurden bei der Wahl zur neuen Signoria überwiegend Anhänger der Albizzi (unter der Führung von Rinaldo degli Albizzi) ausgelost, so dass der Bewerber Cosimo de’ Medici zuerst inhaftiert und dann nach Urbino verbannt wurde, nachdem ein Mordversuch auf ihn misslungen war. Er widersetzte sich jedoch dem Urteil und ging nach Venedig ins Exil, von wo aus er die Geschäfte des von ihm zu einem bedeutenden Unternehmen gemachten Banco Medici weiter leitete. Unter anderem wegen einer ungerechten Steuerpolitik der Albizzi und päpstlicher Interventionen konnte Cosimo de’ Medici jedoch bereits Anfang 1434 nach Florenz zurückkehren. Die Anhänger der Albizzi wurden nun ihrerseits verbannt, die Albizzi unterwarfen sich den Medici und überließen ihnen die Herrschaft. Cosimo de'Medici war nunmehr der mächtigste unter den etwa 80 Bankiers von Florenz. 1444 gab er bei dem Architekten Michelozzo den Bau des Palazzo Medici Riccardi in Auftrag, der 1460 vollendet wurde. 
Cosimos Sohn Piero di Cosimo de’ Medici (1416–1469) war durch seinen Reichtum de facto der führende Bürger von Florenz. Sein Sohn Lorenzo il Magnifico („der Prächtige“, 1449–1492) war ebenfalls offiziell nur Bankier, regierte aber ohne rechtliche Amtsgewalt mit Geld und geschickter Politik faktisch die Stadt, während seine Protegés die offiziellen Ämter ausübten. Er überstand nur knapp die Pazzi-Verschwörung des Jahres 1478, der sein Bruder Giuliano zum Opfer fiel. In der Folge erfuhr er aber durch den Frieden von Lodi 1454 eine Stärkung seiner Position. Den Beinamen "il Magnifico" erhielt Lorenzo durch seine großzügige Förderung der schönen Künste, vor allem von Literatur, Malerei und Bildhauerei, darunter auch von Sandro Botticelli und Michelangelo Buonarroti. Unter seiner Herrschaft wurde Florenz zur wichtigsten Stadt der Künste während der Renaissance. 

Sein ältester Sohn Piero di Lorenzo de’ Medici übernahm das Bankgeschäft und die politische Führungsrolle, während der zweite Sohn, Giovanni, als Leo X. 1513 zum ersten Papst aus der Familie gewählt wurde († 1521). Noch im Jahr seiner Wahl erhob er seinen Neffen, Pieros Sohn Lorenzo, in den päpstlichen Adelsstand (somit die erste Nobilitierung eines Medici) während Pieros jüngster Bruder Giuliano 1515 mit dem Herzogtum Nemours belehnt wurde (womit bereits der Aufstieg in den Hochadel vollzogen war). 1516 wurde dann Lorenzo mit dem Herzogtum Urbino belehnt, wenn auch nur vorübergehend. 1523 wurde der Vetter Giulio als Clemens VII. (1523–1534) zum zweiten Medici-Papst gewählt. (Der dritte, Alessandro Ottaviano de´ Medici, ein Neffe Leos X., regierte als Papst Leo XI. im April 1605 nur für einige Wochen.)

Piero di Lorenzo de’ Medici wurde 1494 bei der Errichtung von Girolamo Savonarolas „Gottesstaat“ vertrieben, und erst 1512 gelang dessen Sohn Lorenzo die Rückkehr, der 1527 aber die erneute Vertreibung folgte. Nur mit Hilfe des Vetters Giulio (inzwischen Papst Clemens VII.) und des Kaisers war es im Anschluss an die Krönung Kaiser Karls V. 1530 in Bologna möglich, diese zweite Vertreibung aus der danach kurzzeitig erneuerten Republik Florenz (1527–1530) rückgängig zu machen.
Die Umwandlung der Republik in eine Monarchie war jedoch nicht aufzuhalten. Lorenzo di Piero war 1516 von seinem Onkel, Papst Leo X., zum Herzog von Urbino ernannt worden, sein unehelicher Sohn Alessandro de’ Medici (möglicherweise auch ein Sohn des Papstes Clemens VII.) regierte seit 1523 bis zu seiner Vertreibung 1527 die Republik Florenz ähnlich einem Fürsten, nach seiner Wiedereinsetzung ab 1531 offen als vom Kaiser ernannter „Herzog der Republik“. Seine Ermordung 1537 durch Lorenzino de’ Medici, einen Verwandten, wird als letztes Aufflackern der Republik begriffen, kann aber auch einfach die gekränkte Eitelkeit eines bei einer Erbregelung Übergangenen gewesen sein. Lorenzos Tochter, Alessandros Halbschwester, war Caterina de’ Medici, die 1533 den späteren französischen König Heinrich II. heiratete. Alessandro selbst hatte 1536 Margarete von Parma, eine uneheliche Tochter Kaiser Karls, geheiratet.
Nach dem Tod Alessandros setzte sich der von diesem favorisierte Cosimo I. aus der jüngeren Linie der Medici durch. Seit 1537 als Herzog in Florenz und seit 1569 als päpstlicher Großherzog der Toskana (der Titel wurde 1575 dem Großherzog Francesco I. von seinem Schwager Kaiser Maximilian II. bestätigt, da die Toskana nicht dem Kirchenstaat unterstand, sondern zu Reichsitalien gehörte) hatte er die Erblichkeit der Medici-Herrschaft eingeführt. Seine Nachkommen, unter denen vor allem Cosimo II. (1590–1621) als Beschützer Galileo Galileis, und Maria de’ Medici, die Tochter Francescos und Ehefrau des französischen Königs Heinrich IV., herausragen, regierten die Toskana bis zu ihrem Aussterben 1737.

Nach dem Tod des letzten, kinderlosen Großherzogs Gian Gastone 1737 fiel das Großherzogtum aufgrund des Erlöschens der regierenden Medicis im Mannesstamm als erledigtes Fahnlehen an das Reich zurück und konnte von Kaiser Karl VI. neu vergeben werden. Es ging – entsprechend einer Vereinbarung zwischen den europäischen Mächten – auf Franz Stephan von Lothringen über, den Schwiegersohn des Kaisers und Ehemann Maria Theresias, der 1745 selbst zum Kaiser gewählt werden sollte. Dieser trat dafür sein ererbtes Herzogtum Lothringen an Frankreich ab. Das Privatvermögen der Medicis fiel hingegen an die überlebende Schwester Gian Gastones, die Pfälzer Kurfürstinwitwe Anna Maria Luisa de’ Medici (1667–1743). Diese hinterließ die gesammelten Kunstschätze der Medici der Stadt Florenz, darunter den Palazzo Pitti und die heute in den Uffizien ausgestellte Gemäldesammlung. 

Eine bedeutende netzwerkanalytische Studie von John Padgett und Christopher Ansell zeigt, dass eine Grundlage für den Aufstieg der Medici die besondere Struktur ihres Beziehungsnetzes zwischen den Jahren 1400 und 1434 nach dem Ciompi-Aufstand war. Das Netzwerk der Medici zeigt bei den Beziehungen der Medici in ihrer Partei eine Trennung von Heirats- und ökonomischen Beziehungen. So handelten sie zwar mit den aufsteigenden Wolltuchhändlern, heirateten aber hauptsächlich in noble Familien, die nicht aus ihrem Viertel, San Giovanni, kamen. Dies führte dazu, dass sie in ihrer Partei, anders als die Oligarchen, also die herrschende Elite um die Albizzi, eine zentrale und mächtige Position innehatten und nicht durch andere Familien erpressbar waren.

Die größten Leistungen mit Hilfe des Geldes der Medici wurden in Kunst und Architektur vollbracht. Giovanni di Bicci förderte Masaccio und beauftragte Filippo Brunelleschi 1419 mit der Wiederherstellung der Basilica di San Lorenzo di Firenze, die zur Grablege der Medici wurde. Die Künstler, die Cosimo de’ Medici um sich hatte, waren Donatello und Filippo Lippi. Ihr wichtigster Beitrag war jedoch die Förderung Michelangelos, der für eine Reihe von Familienmitgliedern arbeitete, beginnend mit Lorenzo il Magnifico, mit dem er den Mittagstisch teilte. Zu seiner Zeit wurden in Florenz so bedeutende Künstler und Gelehrte gefördert wie Angelo Poliziano, Cristoforo Landino, Giovanni Pico della Mirandola, Francesco Granacci, Sandro Botticelli, Marsilio Ficino und Leonardo da Vinci. Über die reine Beauftragung von Künstlern hinaus waren die Medici auch erfolgreiche Sammler, deren Erwerbungen heute den Kern der Uffizien, des Kunstmuseums der Stadt Florenz bilden.

In der Architektur gehen einige bedeutende Bauwerke in Florenz auf die Medici zurück:

Ende April 2004 haben italienische Wissenschaftler unter der Leitung des Paläopathologen Gino Fornaciari damit begonnen, die Gräber von insgesamt 49 Mitgliedern der Familie zu öffnen. Ziel sei es, die genauen Todesursachen, Krankheiten sowie Ernährungsgewohnheiten der Angehörigen zu erforschen. Zunächst wurden verschiedene Grabmäler in den Medici-Kapellen sowie in der Kirche San Lorenzo geöffnet. Dabei wurde gleich zu Beginn der Untersuchungen eine sensationelle Entdeckung gemacht: Unter der Gruft von Gian Gastone wurde eine bislang nicht bekannte Krypta entdeckt, in der acht Leichen gefunden wurden. Die italienischen Wissenschaftler versuchen nun herauszufinden, unter welchen Umständen die Toten – unter denen sich auch Kinder und Jugendliche befinden – ums Leben gekommen sind und warum sie versteckt bestattet wurden. Es wird angenommen, dass es sich dabei ebenfalls um Angehörige des Medici-Geschlechts handelt.

Obwohl das Projekt erst in der kommenden Zeit zu Ende gebracht sein wird, steht inzwischen fest, dass die Medici entgegen der gängigen Meinung nicht an Gicht, sondern an einer genetisch bedingten Form der Arthrose erkrankt waren. Zudem aßen sie neben Fleisch auch viel Gemüse und Getreide. Außerdem litten viele Mitglieder der Familie an Karies. An den Knochen von Eleonora von Toledo konnte man feststellen, dass sie vermutlich häufig geritten ist. Eine weitere überraschende Erkenntnis liegt darin, dass entgegen anderslautenden Behauptungen von Historikern des 16. Jahrhunderts Don Garzia de’ Medici seinen Bruder Giovanni nicht getötet hat. An dem Skelett konnten keine Verletzungen gefunden werden. Stattdessen starben beide Brüder vermutlich an Malaria, die sie sich bei Jagdausflügen in der Maremma zugezogen haben könnten.




Ein Guglielmo Medici hatte sich vor den Auseinandersetzungen zwischen Guelfen und Ghibellinen nach Neapel geflüchtet und trat dort ab 1269 als Richter in Erscheinung. Seine Nachfahren waren in Gragnano bei Neapel wohnhaft und stifteten dort im 16. Jahrhundert mehrere Kirchen; die Linie ist später erloschen. Andere Zweige erloschen im 18. und 19. Jahrhundert. 

Von einem Giuliano im 14. Jahrhundert stammten zwei weitere Zweige ab: Sein Enkel Giuliano begründete die "Marchesi della Castellina", dessen Bruder Antonio die "Fürsten von Ottajano". Lorenzo de’ Medici (1599–1648) aus dem ersten Zweig erhielt 1628 von Großherzog Ferdinando II. de’ Medici den Ort Castellina Marittima als Feudallehen mit dem Titel "Marchese della Castellina". Der Zweig nahm durch Einheirat einer Margherita Tornaquinci ab 1730 den Namen "Medici Tornaquinci" an und existiert noch.

Vom zweiten Bruder stammte Ottaviano de'Medici (1484–1546) ab, der 1531 "Gonfaloniere di Giustizia" in Florenz wurde. Sein Sohn Alessandro wurde 1574 Erzbischof von Florenz und amtierte 1605 kurzzeitig als Papst Leo XI. Dessen Bruder Bernadetto erwarb 1567 das Schloss und die Herrschaft Ottaviano am Fuß des Vesuvs bei Neapel. 1600 erhielt der Zweig im Königreich Neapel den Titel "Principe di Ottajano" verliehen und 1693 "Duca di Sarno", 1822 erbte er auch den Titel "Duca di Miranda" (von den Gaetani dell'Aquila d'Aragona-Caracciolo). Dem Zweig entstammte u. a. "Luigi de' Medici di Ottajano" (1759–1830), der 1814 das Haus Bourbon-Sizilien auf dem Wiener Kongress vertrat und 1816–20 und 1822–30 als Premierminister des Königreichs beider Sizilien amtierte. Vom 1894 erloschenen neapolitanischen Hauptzweig gingen das Schloss Ottaviano und der Palazzo Miranda in Neapel an weibliche Nachfahren über, während die Titel an einen noch existierenden Nebenzweig fielen.










</doc>
<doc id="3394" url="https://de.wikipedia.org/wiki?curid=3394" title="Makro">
Makro

Ein Makro ist in der Softwareentwicklung eine unter einer bestimmten Bezeichnung (Makroname) zusammengefasste Folge von Anweisungen oder Deklarationen, um diese (anstelle der Einzelanweisungen, i. d. R. an mehreren Stellen im Programm) mit nur einem einfachen Aufruf ausführen zu können. Alle Anweisungen des Makros werden automatisch an der Programmstelle ausgeführt, an denen das Makro codiert wurde.

Makros sind eine Variante von Unterprogrammen und können (je nach Implementierung) auch mit Parametern aufgerufen werden. Man unterscheidet Systemmakros (z. B. OPEN (Dateien), PRINT …) und von Benutzern selbst erstellte Makros (z. B. um Prüf- oder Berechnungsfunktionen auszuführen, z. B. IBAN prüfen).

Makros werden z. B. in der Tabellenkalkulation, in der Textverarbeitung und in Datenbanken eingesetzt. Die Arbeit mit diesen Programmen wird erleichtert und beschleunigt, indem eine Befehlsfolge, die häufig gebraucht wird, mit Hilfe eines Makros aufgerufen werden kann. Je nach Softwaretyp können Makros auch über einen Tastenschlüssel oder mit Hilfe eines Menünamens aufgerufen werden.

Üblicherweise werden Programme in einer der Programmiersprachen (z. B. Assembler, BASIC, Pascal) kodiert. Es ist auch möglich, innerhalb einer Software (z. B. Microsoft Office) eine eigene Programmiersprache zu benutzen. Dabei kann die Programmiersprache softwarebezogen (z. B. Programmierungen in Microsoft Excel mit VBA) oder zur Programmierung ganz allgemein angewandt werden (z. B. Lösungen eines mathematischen Problems).

Das Wort "Makro" ist entlehnt aus dem gleichbedeutenden engl. "macro", eine Verkürzung von "macroinstruction", wörtlich in etwa „Groß-Befehl“ (griechisch, "makros" bedeutet „groß“ oder „weit“).

Ein Makro in der Programmierung ist ein kleines Stück Programmcode, das von einem Interpreter oder Präprozessor durch ein größeres Stück Programmcode ersetzt wird. Damit ist es möglich, oft wiederkehrende Programmstrukturen durch Kürzel zu vereinfachen oder literale Konstanten durch semantische Bezeichnungen zu ersetzen.


Hier sind es nicht die Programmierer, sondern die Benutzer der Software, die per Makros gewisse Arbeitsschritte effizient zusammenfassen können.




</doc>
<doc id="3395" url="https://de.wikipedia.org/wiki?curid=3395" title="Menschheit">
Menschheit

Menschheit (von althochdeutsch "mennisgheit" „menschliche Natur, menschliches Wesen“) bezeichnet die Gesamtheit aller Menschen, im engeren Sinne die gesamte Weltbevölkerung, oder in einem abstrakteren Sinne die soziale Einheit aller Menschen, beispielsweise bei vermuteten oder tatsächlichen gemeinsamen Entscheidungen oder Interessen sämtlicher Menschen.



</doc>
<doc id="3396" url="https://de.wikipedia.org/wiki?curid=3396" title="Mimi Leder">
Mimi Leder

Mimi Leder (* 26. Januar 1952 in New York, New York) ist eine US-amerikanische Filmregisseurin. 

Mimi Leder ist eine Tochter des Regisseurs Paul Leder und ihrer Mutter Etyl Leder. Sie begann ihre Karriere in den 1970ern als Drehbuchautorin bei Projekten wie "Spawn of the Slithis" (1978) and "Hill Street Blues" (1981). 1986 begann Leder mit der Regie bei "L.A. Law". In den 1990er Jahren führte sie bei rund einem Dutzend Episoden von "Emergency Room" Regie und gewann dafür Emmy Awards. 

In den späten 1990er- und frühen 2000er-Jahren führte sie Regie bei mehreren Actionfilmen und Dramen wie "The Peacemaker", "Deep Impact" und "Das Glücksprinzip". Ihr Hauptaugenmerk liegt aber auf Fernsehproduktionen.

Das American Film Institute zeichnete sie 1999 mit dem Franklin J. Schaffner Award aus.

Leder ist mit Gary Werntz verheiratet und hat mit diesem eine Tochter, Hannah Werntz, welche in einigen von Leders Filmen mitgespielt hat.




</doc>
<doc id="3397" url="https://de.wikipedia.org/wiki?curid=3397" title="Mount Everest">
Mount Everest

Der Mount Everest ist ein Berg im Himalaya und mit einer Höhe von der höchste Berg der Erde. Er gehört zu den 14 Achttausendern und zu den Seven Summits. Der Mount Everest ist seit 1856 nach dem britischen Landvermesser George Everest benannt. Auf Nepali heißt der Berg Sagarmatha, auf Tibetisch Qomolangma (deutsche Aussprache „Tschomolangma“; englische Umschrift "Chomolungma").

Der Mount Everest befindet sich im Mahalangur Himal in der Region Khumbu in Nepal an der Grenze zu China (Autonomes Gebiet Tibet); der westliche und südöstliche seiner drei Gipfelgrate bilden die Grenze. Auf nepalesischer Seite ist er Teil des Sagarmatha-Nationalparks, der zum UNESCO-Welterbe gehört. Auf der Nordseite gehört er zum "Qomolangma National Nature Reserve", das mit dem von der UNESCO ausgewiesenen "Qomolangma-Biosphärenreservat" korrespondiert.

Edmund Hillary und Tenzing Norgay gelang am 29. Mai 1953 die Erstbesteigung des „dritten Pols“. Am 8. Mai 1978 bestiegen Reinhold Messner und Peter Habeler den Gipfel erstmals ohne zusätzlichen Sauerstoff.

Auf Nepali wird der Berg "Sagarmatha" („Stirn des Himmels“) und auf Tibetisch "Jo mo klungs ma" (in offizieller Transkription: "Qomolangma"; „Mutter des Universums“) genannt. Der chinesische Name "Zhūmùlǎngmǎ Fēng" ist eine phonetische Wiedergabe des Tibetischen. Die heute in Europa übliche Transkription ist das englische "Chomolungma". Das in deutschsprachigen, vor allem älteren, Texten verwendete Tschomolungma wird zugunsten von Chomolungma aus neueren deutschsprachigen Quellen verdrängt.
Sir George Everest war lange Jahre Leiter der Großen Trigonometrischen Vermessung Indiens und Surveyor General of India. Unter seinem Nachfolger Andrew Scott Waugh wurde der zunächst als „Peak b“ bezeichnete Gipfel 1848 erstmals von Indien aus vermessen, da Nepal den Zugang zu seinem Territorium verweigerte. Nach weiteren Vermessungsarbeiten über Entfernungen bis zu 200 km folgten umfangreiche, komplexe Berechnungen durch Radhanath Sikdar in den Computing Offices in Dehradun, der 1852 zu dem Ergebnis kam, dass der inzwischen als „Peak XV“ („Gipfel 15“) bezeichnete Gipfel mit 29.002 Fuß (8840 m) höher ist als alle anderen bisher bekannten Berge. Um letzte Zweifel an der Genauigkeit der Vermessungen wegen der großen Entfernungen auszuräumen, gab Andrew Waugh dieses Ergebnis erst 1856 in einem Schreiben an die Royal Geographical Society bekannt. Dabei benannte er den Berg zu Ehren seines Vorgängers als "Mount Everest". Die heutzutage gebräuchliche Aussprache von Mount Everest lautet , Sir George sprach seinen eigenen Nachnamen allerdings aus.

Vor allem im deutschen Sprachraum war der Berg lange als "Gauri Sankar" bekannt. Dies beruhte auf einem Missverständnis des deutschen Himalaya-Pioniers Hermann von Schlagintweit. Dieser hatte 1855 versucht, den eben erst als höchsten Berg der Erde errechneten, aber unbekannten Peak XV zu erkunden. Aus der Nähe von Kathmandu betrachtete er die Westseite des Gebirges und sah einen Berg, der in Richtung des Everest lag und alle anderen Berge überragte. Dieser Berg war den Nepali als Gaurisankar bekannt, Schlagintweit hielt ihn jedoch für den mysteriösen Peak XV. Auf diesem Irrtum beruhend und aus Ablehnung des englischen Namen Mount Everest zu Gunsten des „schönen alten Namen[s] Gaurisankar“ wurde in Deutschland diese Bezeichnung für den höchsten Berg der Welt in die Atlanten aufgenommen und in Schulen gelehrt. 1903 wurde festgestellt, dass es sich beim Gaurisankar um einen anderen, nämlich den 7145 m hohen Peak XX handelt; seine Entfernung zum Everest beträgt 58 km.

Wie im Grunde alle markanten Gipfel der Khumbu-Region ist auch der Mount Everest für die Sherpas ein heiliger Berg. Der Buddhismus ist bei diesem Volk mit ursprünglicheren Religionen, insbesondere Animismus und Bön, gepaart. Nach der Auffassung der Sherpas bewohnen Geister und Dämonen Quellen, Bäume und eben auch die Gipfel. Der Mount Everest ist nach Ansicht der Buddhisten der Sitz von Jomo Miyo Lang Sangma, einer der fünf „Schwestern des langen Lebens“, die auf den fünf höchsten Gipfeln des Himalaya wohnen. Jomo Miyo Lang Sangma gibt den Menschen Nahrungsmittel.
Der große Heilige Padmasambhava, der den Buddhismus von Indien nach Tibet brachte, veranstaltete der Sage nach einen Wettlauf zum Gipfel des Mount Everest. Nachdem Padmasambhava einige Zeit auf dem Gipfel meditiert und mit den Dämonen gekämpft hatte, wurde er von einem Lama der Bön-Religion herausgefordert. Es ging um die Frage, wer von beiden mächtiger sei. Der Lama der Bön-Religion machte sich noch in der Nacht auf den Weg, getragen von seiner magischen Trommel, Padmasambhava erst bei Tagesanbruch. Er gewann trotzdem den Wettlauf, weil er, auf einem Stuhl sitzend, von einem Lichtstrahl direkt zum Gipfel gebracht wurde. Nachdem Padmasambhava einige Zeit oben gewartet hatte, ließ er seinen Stuhl zurück und begann mit dem Abstieg. Der Bön-Lama gab sich geschlagen und ließ seine Trommel zurück. Bis heute sagt man, dass die Geister die Trommel schlagen, wenn eine Lawine zu Tale donnert.

Auf Grund dieser Bedeutung wird vor einer Besteigung von den Sherpas eine Opferzeremonie durchgeführt, die sogenannte Puja-Zeremonie. Die Sherpas sind davon überzeugt, dass eine Puja zwingend notwendig ist, um Unheil abzuwenden. Dieses Opferfest ist für ihren Seelenfrieden unabdingbar, und im Allgemeinen nehmen auch alle westlichen Expeditionsteilnehmer daran teil, da sonst, nach dem Glauben der Sherpas, die Berggötter zornig würden, und zwar nicht nur gegenüber den Ausländern, sondern besonders auch gegenüber den Sherpas, die solches zugelassen hätten.

Religiöse Symbole wie Manisteine und ein Stupa mit Gebetsfahnen, die mit Mantras bedruckt sind, finden sich am Fuß des Mount Everest. Auf dem Weg zum Everest-Basislager (Mount Everest Trek), am Thokla-Pass zwischen Dingboche und Lobuche, wurde eine Gedenkstätte für die Opfer des Everest angelegt. Den Toten ist mit einem sogenannten Steinmann, einem Stapel aufgetürmter Steine, oder einer Stele die letzte Ehre erwiesen.

Der Mount Everest ist, wie der gesamte Himalaya, während der alpidischen Gebirgsbildung entstanden. Die Konvergenz der indischen Platte und der eurasischen Platte führte zur Schließung der Tethys mit Beginn vor etwa 50 Millionen Jahren im Eozän und in Folge zur Kollision der Kontinente Indien und Asien. Die wesentlich kleinere indische Platte schiebt sich nach wie vor mit einer Rate von etwa drei Zentimetern pro Jahr unter Eurasien. Der Mount Everest wächst infolge der mit der Kollision verbundenen Verdickung der kontinentalen Kruste noch immer, allerdings nur wenige Millimeter im Jahr. Die fortdauernde Hebung wird dabei durch isostatische Ausgleichsbewegungen verursacht, die aus dem Dichteunterschied der gestapelten Kruste im Bereich des Gebirges und der (dichteren) Asthenosphäre folgt. Der Krustenblock erhält dabei einen Auftrieb wie ein Korken im Wasser.

Durch den hohen Umschließungsdruck und die hohen Temperaturen bei der Versenkung in den oberen Erdmantel wurden die ursprünglichen Krustengesteine metamorph umgewandelt. Im Everest-Massiv besteht die unterste Einheit der Gipfelpyramide (oberhalb ) hauptsächlich aus metamorphen Gesteinen, vor allem dunklen, biotitreichen Gneisen. Die Grenze zu den darüberliegenden Graniten ist durch pegmatitische Gänge relativ deutlich markiert. Der sogenannte "„Nuptse-Granit“" (benannt nach dem Nachbarberg Nuptse) ist ein heller Granit, der neben Quarz und Feldspat hauptsächlich Muskovit, Biotit und Turmalin enthält. Er verwittert typischerweise zu großen Blöcken. Auf über Höhe liegen metamorph überprägte Sedimentgesteine. Diese fossilreichen mikritischen Kalksteine bis Tonsteine enthalten Trilobiten, Ostrakoden sowie Seelilien und Haarsterne (Crinoidenkalk). Sie haben eine helle gelbliche Farbe und werden als "„Gelbes Band“" bezeichnet. Sie wurden bei der Gebirgsbildung auf den kristallinen Sockel aus Gneisen und Graniten überschoben und fallen nach Südwesten ein. Daher sind die Gesteine an der Südwestseite des Everest niedriger anzutreffen als an der Nordseite. Die oberste Gipfelpyramide des Everest besteht aus grauem Kalkstein mit Einlagerungen von Sand und Schutt, der nur in geringem Maße der Metamorphose unterlag.

Durch das Erdbeben vom 25. April 2015 wurde der Mount Everest nach übereinstimmenden Messungen der chinesischen National Administration of Surveying, Mapping and Geoinformation und US-amerikanische Geologen um drei Zentimeter in südwestlicher Richtung verschoben. Das Beben vom 12. Mai 2015 hatte keine Auswirkungen auf die Lage des Berges. Weitere Untersuchungen ergaben, dass der Berg sich innerhalb von zehn Jahren zuvor mit einer Geschwindigkeit von vier Zentimetern pro Jahr um insgesamt 40 Zentimeter nach Nordosten verschoben sowie um 0,3 cm pro Jahr angehoben hat.

Die Gipfelpyramide ist durch Erosion und gewaltige Gletscher modelliert. Die drei Hauptkämme – Westgrat, Nord-/Nordostgrat und Südostgrat – untergliedern den Gipfel in drei Hauptwände – Südwestwand, Nordwand und Ostwand (Kangshung-Flanke). Außerdem trennen die Grate die drei sich vom Mount Everest und seinen Nachbargipfeln ergießenden Gletscher: Khumbu-Gletscher, Rongpu-Gletscher (auch "Rongbuk-Gletscher") und Kangshung-Gletscher.

Südostgrat und Westgrat sowie deren Fortsetzungen bilden die weitere Grenze zwischen Tibet und Nepal. Der Südostgrat verbindet den Mount Everest mit dem hohen Lhotse, der niedrigste Punkt dieses Grats ist der hohe Südsattel (South Col). Im weiteren Verlauf setzt sich der Grat vom Lhotse in Richtung Lhotse Shar () und Peak 38 () fort. Der Westgrat läuft zunächst in einen Nebengipfel – die sogenannte "Westschulter" – aus, welche zum "Lho-La-Pass" () abfällt und sich dann in die Bergkette aus Khumbutse (), Lingtren () und Pumori () fortsetzt. Der auf tibetischer Seite befindliche Nordostgrat zielt vom Gipfel über drei Felsstufen und drei Felsnadeln bis zum östlichen Rongpu-Gletscher hinunter. Von ihm zweigt unterhalb der Stufen und oberhalb der Nadeln der Nordgrat auf einer Höhe von ab und verbindet den Mount Everest über den niedrigsten Punkt am Nordsattel () mit dem hohen Changtse. Vom Lhotse zieht auf nepalesischer Seite in westliche Richtung der lange Bergkamm des Nuptse (), der vom Mount Everest durch das Tal des Schweigens und den Khumbu-Gletscher getrennt wird.

Die Wände des Everest sind unterschiedlich gegliedert. Die Südwestwand zum Tal des Schweigens weist zwei markante Pfeiler aus. Sie ist im Ganzen steil (um 60–70 Grad). Die Nordwand ist im Wesentlichen gegliedert durch zwei hochgelegene Couloirs, das Norton-Couloir und das Hornbein-Couloir. Die Neigung der Nordwand variiert um 40–45 Grad. Die stark vergletscherte Ostwand oder Kangshung-Wand hat drei Hauptpfeiler. Sie ist im unteren Teil sehr steil (bis 80 Grad), und im oberen, schwächer geneigten Teil von Hängegletschern markiert.

Die klimatischen Bedingungen am Mount Everest sind extrem. Im Januar, dem kältesten Monat, beträgt die Durchschnittstemperatur auf dem Gipfel −36 °C und kann auf Werte bis zu −60 °C fallen. Auch im wärmsten Monat, dem Juli, steigen die Temperaturen nicht über die Frostgrenze, die Durchschnittstemperatur auf dem Gipfel beträgt dann −19 °C.

Im Winter und Frühling herrschen Winde aus westlichen Richtungen vor. Die feuchtigkeitsbeladene Luft kondensiert zu einer weißen, nach Osten zeigenden Wolke (verfälschend häufig als „Schneefahne“ bezeichnet). Wegen dieser Wolkenfahnen hielt man den Himalaya ursprünglich für eine Vulkankette. Anhand der Wolkenfahne des Mount Everest schätzen Bergsteiger auch die Windgeschwindigkeit auf dem Gipfel ab: Bei etwa 80 km/h steht sie rechtwinklig zum Gipfel, bei höheren neigt sie sich nach unten und bei niedrigeren nach oben. Im Winter prallt der südwestliche Jetstream auf den Gipfel und kann Windgeschwindigkeiten von bis zu 285 km/h verursachen. Von Juni bis September gelangt der Berg unter den Einfluss des Indischen Monsuns. In dieser Zeit fallen die meisten Niederschläge, und heftige Schneestürme prägen das Wetter.

Wie in allen Hochgebirgsregionen kann es zu raschen Wetterumschwüngen kommen. Dies gilt auch für die beiden Besteigungssaisonen im Mai und Oktober. Plötzlich einsetzende Temperaturstürze, Stürme und Schneefälle von bis zu drei Metern pro Tag sind nicht außergewöhnlich. Zumeist gibt es in der jeweiligen Saison nur wenige Tage mit stabilem Wetter – die sogenannten „Fenstertage“ –, an denen eine Besteigung am ehesten möglich ist.

Der Luftdruck auf dem Gipfel des Mount Everest beträgt gemäß der barometrischen Höhenformel 325,4 hPa und entspricht knapp einem Drittel des Normaldrucks auf Meeresspiegelniveau. Hierdurch verschiebt sich der Siedepunkt des Wassers von 100 °C bei Normalbedingungen auf nur 70 °C, und der Sauerstoff-Partialdruck der Luft beträgt nur noch ein Drittel im Vergleich zur Meereshöhe. Hinzu kommen extreme Temperaturschwankungen und starke Winde. An diese äußerst lebensfeindliche Umwelt konnten sich nur wenige Tiere anpassen, Blütenpflanzen sind im Bereich des ewigen Eises nicht mehr zu finden.

"Euophrys omnisuperstes", ein kleiner Vertreter der Springspinnen (Salticidae), wurde bereits 1924 von R.W.G. Hingston bis zu einer Höhe von beobachtet. Seine Nahrungsgrundlage blieb lange ein Rätsel. Erst 1954 entdeckte man, dass sie sich von Fliegen und Springschwänzen (Collembola) ernähren, die bis zu einer Höhe von anzutreffen sind. Letztere leben von Pilzen und Flechten, die herangewehtes organisches Material abbauen. Bei der 1924 durchgeführten Everestexpedition wurden Flechten zwischen 4600 und gesammelt. Darauf basierend konnte R. Paulson 1925 etwa 30 Arten nachweisen.

Von den Wirbeltieren sind nur einige Vögel in der Lage, sich der extremen Höhe dauerhaft anzupassen. Die Streifengans ("Anser indicus") hält sich bis in Höhen von auf. Alpenkrähen ("Pyrrhocorax pyrrhocorax") wurden selbst am hohen Südsattel beobachtet, wo sie sich von Abfällen, aber auch von tödlich verunglückten Bergsteigern ernähren. Der Leichnam von George Mallory, den man auf ca. fand, wurde vermutlich ebenfalls von Vögeln angefressen.

Die Höhe des Mount Everest wurde in vielen Messungen bestimmt. Dabei ergaben sich Höhenangaben zwischen 8844 und . Auf Grund der Höhe (Todeszone) und der Eisschicht auf dem Gipfel gestaltet sich die Messung schwierig. Die Eisschicht auf dem Gipfel wird nicht in die Höhe mit eingerechnet, da sie starken Schwankungen unterliegt. Die exakte Höhe muss sich folglich auf die Höhe des Felssockels darunter beziehen. Bei den ersten Messungen war dies noch nicht möglich. Ein weiteres Problem ist die Bezugsgröße Meeresspiegel. Chinesische Messungen gehen vom definierten Nullpunkt eines Pegels in Qingdao, nepalesische Messungen vom Nullpunkt eines Pegels in Karatschi aus. Die Distanz beider Orte beträgt mehr als 6000 Kilometer, und allein aus diesem unterschiedlichen Bezugssystem ergeben sich deutliche Differenzen. Darüber hinaus basieren GPS-Höhenangaben auf einem vereinfachten Modell der Erde, dem Referenzellipsoid des World Geodetic System 1984. Bei solchen Messungen muss also noch die Differenz zwischen Geoid und Referenzellipsoid berücksichtigt werden, wie beispielsweise bei der Messung im Mai 2004 geschehen.

Die Angabe für die Gipfelhöhe des Mount Everest ist seit der ersten Messung im Jahre 1848 mehrfach korrigiert worden. 1856 wurde aus Angaben von sechs verschiedenen Vermessungsstationen errechnet. Die Stationen befanden sich allerdings über 150 Kilometer vom Everestmassiv entfernt, da die Vermesser des britischen Indian Survey nicht nach Nepal einreisen durften. Bis dahin sah man den Dhaulagiri (), den ersten entdeckten Achttausender, und ab 1838 den Kangchendzönga () als höchsten Berg an.

Die lange Zeit geltende Höhenangabe von war 1954 vom Survey of India aus den Messdaten von insgesamt zwölf Vermessungsstationen als Mittelwert errechnet worden. Diese Angabe wurde von einer chinesischen Expedition im Jahre 1975 bestätigt – sie stellte fest.

Auch eine im September 1992 als erste mit modernen Mitteln angestellte Höhenvermessung eines chinesisch-italienischen Expeditionsteams direkt am Berg ergab mit nahezu den gleichen Wert. Die dabei verwendeten Daten stammten sowohl aus Messungen mit herkömmlichen Theodoliten als auch aus Lasermessungen und GPS-Signalen.

Sehr genaue Messungen mit Hilfe mehrerer GPS-Empfänger am 5. Mai 1999 ergaben eine Höhe von . Jene Angabe basiert auf der Höhe des Felssockels. Die Stärke der Schicht aus Eis und Schnee am Gipfel schwankt je nach Jahreszeit und Niederschlagsmengen der Monsunzeit etwa im Bereich zwischen einem und drei Metern.

Bei einer Messung im Mai 2004 wurden acht Radarreflektoren am Gipfel verankert und so die Höhe des Felssockels bestimmt. Im Anschluss wurde die jeweilige Höhe der Radarprofile ermittelt. Von dieser Höhe wurde dann die Dicke der Eisschicht abgezogen. Der Everest hatte nach dieser Messung eine Höhe von , mit einer Ungenauigkeit von ±0,23 Meter. Damit konnte die Höhe aus dem Jahr 1992 bestätigt werden.

Die neueste Messung stammt aus dem Mai 2005, durchgeführt wiederum von einer chinesischen Expedition. Sie ergab für den Berg eine Höhe von , bei einer Ungenauigkeit von ±0,21 Meter. Er ist damit etwa 3,7 Meter niedriger als seit der chinesischen Messung von 1975 angenommen. Allerdings bezieht sich die aktuelle Angabe, wie auch schon die von 1999 und 2004, nur auf den reinen Felssockel. Diese letzte Untersuchung wurde von Chinas Nordseite und nicht vom nepalesischen Süden aus unternommen und dauerte ein Jahr. Eingesetzt wurden Radardetektoren sowie Lasermessgeräte und Satellitenortungssysteme.

Während der Mount Everest die höchste Erhebung über dem Meeresspiegel ist, existieren noch zwei weitere Berge, die den Titel „höchster Berg der Erde“ verdienen. Vom Fuß des Berges aus gemessen ist dies der Vulkan Mauna Kea auf Hawaii, vom Erdmittelpunkt aus gerechnet der Chimborazo in Ecuador.

Die britische Armee-Expedition von Francis Younghusband bahnte sich im Jahre 1904 gewaltsam ihren Weg durch Tibet, um das Land zur Öffnung seiner Grenzen und Gewährung von Handelsprivilegien zu zwingen. Dabei wurde von J. Claude White auch die erste detaillierte Fotografie der Ostflanke von Kampa Dzong aus (etwa 150 Kilometer Entfernung) angefertigt. Bei den eigentlichen Erkundungs- und Besteigungsexpeditionen wurde dagegen versucht, eine Genehmigung durch den Dalai Lama zu erhalten. Es dauerte bis zum Jahr 1921, ehe er der Royal Geographical Society diese Erlaubnis aussprach, die daraufhin 1921, 1922 und 1924 Expeditionen entsandte.

Die erste Britische Erkundungsexpedition wurde 1921 in das Gebiet entsendet. Hier ging es aber noch nicht primär um die Besteigung des Berges, sondern um geologische Vermessungen, die Kartierung des Gebietes und eine erste Erkundung möglicher Aufstiegsrouten. Teilnehmer der Expedition beendeten die Vermessung von 31.000 Quadratkilometern. Im Verlaufe dieser Expedition entdeckte George Mallory vom Lhakpa La aus eine gangbare Route zum Gipfel, die seitherige Standard-Nordroute durch das Tal des östlichen Rongpu-Gletschers auf den Nordsattel. Ein kurzfristig angegangener Besteigungsversuch scheiterte auf dem Nordsattel am einsetzenden Monsun.

In den Jahren 1922 und 1924 wurden mehrere Besteigungsversuche unternommen, die vor allem mit dem Namen Mallory verbunden sind. Beim dritten Versuch der 1922er Expedition löste sich am 7. Juni während des Aufstiegs vom Lager III eine Lawine und riss sieben Träger in den Tod. Im Mai/Juni 1924 unternahm eine Expedition mehrere Anläufe, musste sie aber wegen schlechten Wetters immer wieder aufgeben. Mallory und Andrew Irvine kehrten von ihrem Aufstiegsversuch nicht mehr zurück. Bis heute gibt es Diskussionen um die Frage, ob sie auf dem Gipfel waren oder bereits vorher zu Tode kamen. Mallorys Leiche wurde 1999 ohne eindeutigen Beweis für das Erreichen des Gipfels gefunden, Irvine ist nach wie vor verschollen.

Auch in den 1930er Jahren versuchten mehrere britische Expeditionen, den Gipfel des Mount Everest zu erreichen, bei denen erstmals auch Tenzing Norgay als Träger dabei war. In den 1940er Jahren gab es keine ernsthaften Besteigungsversuche.

In den 1950er Jahren gab es einen Wettlauf zweier Nationen um den Gipfel. Infolge der chinesischen Okkupation war Tibet für Ausländer nicht mehr zugänglich, jedoch hatte das Königreich Nepal, das zwischen 1815 und 1945 Ausländern die Einreise und damit die Erkundung des Himalaya verwehrt hatte, seine Blockadehaltung inzwischen aufgegeben und einzelne Expeditionen genehmigt. Die Südwestseite des Everest war kaum bekannt, Mallory konnte zwar 1921 vom Lho La aus einen Blick auf die Südseite und in das Western Cwm werfen, ob von dort aus aber der Berg besteigbar oder zumindest der Südsattel erreichbar sei, blieb unbekannt. 1951 erkundete eine britische Expedition diesen Zugangsweg.
1952 wurden zwei Schweizer Expeditionen genehmigt, die die heutige Normalroute bis auf eine Höhe von 8600 Metern eröffneten.

1953 wurde die neunte britische Expedition zum Mount Everest, diesmal unter der Leitung von John Hunt ausgerichtet. Nachdem mehrere Hochlager errichtet worden waren, wurden zwei Seilschaften gebildet. Die erste Seilschaft sollte quasi einen Schnellschuss wagen, die zweite dann bei Misserfolg das letzte Hochlager weiter nach oben verlegen. So sollte der Erfolg sichergestellt werden.

Die erste Seilschaft bestand aus Tom Bourdillon und Charles Evans. Sie erreichten am 26. Mai den Südgipfel, mussten aber aufgeben, weil die von Bourdillon und seinem Vater entwickelten geschlossenen Sauerstoffsysteme infolge Vereisung versagten. Dies kostete sie so viel Zeit, dass ein weiterer Aufstieg keine Chance für einen sicheren Abstieg gelassen hätte. Die zweite Seilschaft verwendete nun ein traditionelles, offenes Sauerstoffsystem. Zwei Tage später schafften es der Neuseeländer Edmund Hillary und die Sherpas Tenzing Norgay und Ang Nyima, das letzte Lager auf eine Höhe von zu verlegen. Ang Nyima stieg dann wieder ab, während Hillary und Norgay am 29. Mai um 6:30 Uhr Richtung Gipfel aufbrachen. Da sie weiter oben am Berg losstiegen, erreichten sie den Südgipfel bereits um 9:00 Uhr. Gegen 10:00 Uhr erreichten sie eine Felsstufe, die später Hillary Step genannt wurde und die das letzte bergsteigerische Hindernis darstellt. Gegen 11:30 Uhr standen sie auf dem Gipfel.

Hillarys erste Worte an seinen langjährigen Freund George Lowe nach seiner Rückkehr waren: "" Die Meldung von der erfolgreichen Erstbesteigung erreichte London am Morgen des 2. Juni 1953. Das war Tag der Krönung von Elisabeth II. Am 16. Juli wurde Hillary der Order of the British Empire verliehen, der gleichzeitig seine Erhebung in den Adelsstand des britischen Königreichs bedeutete. Norgay wurde von Elisabeth II. durch die Verleihung der George Medal geehrt. Die Erstbesteigung löste ein großes internationales Echo aus und wurde als Eroberung des „dritten Pols“ (nach Nord- und Südpol) gefeiert.

Wer von beiden zuerst auf dem Gipfel stand, war Gegenstand eines heftigen Disputs. Ein Gipfelfoto existiert nur von Tenzing Norgay, da dieser nicht in der Lage war, die Kamera zu bedienen und somit Hillary nicht ablichten konnte. Hillary sagte mal, dass der Gipfel des Everest kein geeigneter Ort sei, um dort jemandem das Fotografieren beizubringen. Tenzing Norgay wurde von asiatischer Seite als Erstbesteiger gefeiert und ihm sogar eine Unterschrift unter ein entsprechendes Dokument abgenötigt. Er gab aber 1955 zu, dass Hillary zuerst seinen Fuß auf den Gipfel setzte. Beide betonten jedoch, dass die Erstbesteigung das Werk eines Teams war und blieben lebenslang befreundet.

Hillary sagte auch, dass er sich nach Spuren umgesehen habe, die eventuell von einem Gipfelgang von Mallory oder Irvine stammten, habe aber nichts erkennen können, das zeige, dass zuvor schon einmal jemand dort gewesen sei. Der Frage, ob er denke, dass er tatsächlich der erste ganz oben gewesen sei, entgegnete er unwiderleglich, er wisse, dass er ganz oben war und auch heil wieder heruntergekommen sei – und das sei das, worauf es ankomme.

1956 war erneut eine Schweizer Expedition am Berg. Den Bergsteigern Ernst Schmied und Jürg Marmet am 23. Mai sowie einen Tag später Dölf Reist und Hansruedi von Gunten gelang die zweite beziehungsweise dritte Besteigung auf der Route der Erstbesteiger. Zuvor gelang Ernst Reiss und Fritz Luchsinger im Rahmen dieser Expedition am 18. Mai die Erstbesteigung des benachbarten Lhotse. Der letzte Überlebende der Expedition "Hansruedi von Gunten" erzählte, die Teilnehmer hätten sich kaum vorstellen können, dass danach noch jemand "da hinauf wolle".

1960 wurde der Mount Everest erstmals von tibetischer Seite aus (Nordostgrat) durch eine chinesische Expedition bestiegen. Die Bergsteiger Wang Fu-chou, Konbu und Qu Yinhua waren vermutlich die ersten, die den Second Step erklettern konnten. Das letzte Stück soll Chu Ying-hua sogar barfüßig von den Schultern eines Teamkollegen gemeistert haben. Diese Besteigung wurde jedoch vereinzelt angezweifelt, da es keine sichere Dokumentation für den Gipfelsieg gibt. Ein damals veröffentlichtes Foto zeigt aber den Berg oberhalb des Second Step. Mittlerweile wird diese Besteigung offiziell anerkannt. Besser dokumentiert und daher vereinzelt noch als Erstbesteigung dieser Route angesehen ist die einer ebenfalls chinesischen Expedition im Jahr 1975.

1963 eröffnete die erste amerikanische Expedition unter der Leitung von Norman Dyhrenfurth eine neue Route über den Westgrat. Tom Hornbein und Willi Unsoeld stiegen vom Tal des Schweigens aus auf die Westschulter, folgten dem Westgrat, mussten dann aber wegen zu großer technischer Schwierigkeiten auf dem Grat in die Nordwand ausweichen. Sie stiegen in der seither „Hornbein-Couloir“ genannten Schlucht der Nordwand zum Gipfel und führten dann die erste Überschreitung des Mount Everest durch, indem sie ihren auf der Südroute angestiegenen Kameraden im Abstieg folgten. Diese Überschreitung war zugleich die erste Überschreitung eines Achttausenders überhaupt.

Am 16. Mai 1975 stand mit der Japanerin Junko Tabei die erste Frau auf dem Gipfel. Wenig später erreichte die Tibeterin Phantog als Teilnehmerin der chinesischen Nordgrat-Expedition als zweite Frau den Gipfel. Im selben Jahr wurde die Südwestwand, die sich 2500 m aus dem Tal des Schweigens erhebt, von einer britischen Expedition unter Leitung von Chris Bonington durch Doug Scott und Dougal Haston zum ersten Mal bewältigt. An dieser Wand waren zuvor bereits sechs Expeditionen gescheitert. Die Schlüsselstelle in der Route ist die Überwindung eines gewaltigen Felsbandes oberhalb der schneegefüllten Rinne. Doug Scott und Dougal Haston biwakierten beim Abstieg eine Nacht in einer Schneehöhle am Südgipfel ().

Am 3. Mai 1978 war mit Robert Schauer der erste Österreicher auf dem Gipfel. Schauer glückte 18 Jahre später eine zweite und 2004 eine dritte Besteigung. Nur fünf Tage später, am 8. Mai 1978, bestiegen Reinhold Messner und Peter Habeler den Gipfel erstmals ohne zusätzlichen Sauerstoff. Weitere drei Tage später erreichte Reinhard Karl aus derselben Expedition als erster Deutscher den Gipfel. Weitaus weniger bekannt ist, dass im Herbst desselben Jahres Hans Engl als erster Deutscher den Gipfel ebenfalls ohne zusätzlichen Sauerstoff erklomm. Dem Österreicher Franz Oppurg gelang am 14. Mai 1978 die erste Solobesteigung des Mount Everest. Die erste deutsche Frau stand 1979 auf dem Gipfel: Hannelore Schmatz kam aber beim Abstieg ums Leben. Die wohl schwierigste Grat-Route, der direkte Westgrat, wurde ebenfalls 1979 durch eine jugoslawische Expedition gemeistert. Andrej Štremfelj und Jernej Zaplotnik überwanden schwierigste Felspassagen.

In den 1980er Jahren gelangen die erste Winter- und die erste Alleinbegehung sowie neue, schwierige Routen auf den Gipfel. Die erste Winterbegehung der Südsattelroute praktizierte 1980 eine polnische Expedition. Am 17. Februar erreichten Leszek Cichy und Krzysztof Wielicki den Gipfel, wobei sie mit Temperaturen von bis zu −45 °C und Windgeschwindigkeiten von fast 200 km/h zu kämpfen hatten. Im selben Jahr gelang Reinhold Messner die erste Alleinbegehung des Berges im reinen Alpinstil. Zudem wurde die Nordwand von den Japanern Takashi Ozaki und Tsuneo Shigehiro erstmals vollständig durchstiegen. Jerzy Kukuczka war mit einer polnischen Expedition am Südpfeiler erfolgreich. 1982 eröffnete eine sowjetische Expedition eine neue Route über den Südwestpfeiler. Die Ostwand wurde 1983 durch die US-Amerikaner Louis Reichardt, Kim Momb und Carlos Buhler bezwungen. 1986 durchstiegen Erhard Loretan und Jean Troillet das Hornbein-Couloir.

Im Jahr 1995 wurde der lange Nordostgrat vollständig bis zum Gipfel begangen. Im selben Jahr schaffte es die Schottin Alison Hargreaves als erste Frau ohne zusätzlichen Sauerstoff auf den Gipfel.

1996 wurde die Saison durch zwölf Todesfälle überschattet, die bis zu diesem Zeitpunkt tödlichste Saison am Mount Everest. In einem am Mittag aufziehenden Höhensturm kamen mehrere Bergsteiger aus der Gipfelzone nicht mehr zurück zu ihren Zelten, unter anderem sehr erfahrene Expeditionsleiter, die zuvor schon mehrfach oben gewesen waren. Einzelheiten dazu stellt der Artikel "Unglück am Mount Everest (1996)" dar.

Der britische Abenteurer Bear Grylls bestieg 1998 als damals jüngster Brite mit 23 Jahren den Mount Everest.

Am 27. Mai 1999 schaffte Helga Hengge über die Nordroute als erste deutsche Frau die erfolgreiche Besteigung.

Evelyne Binsack erreichte am 23. Mai 2001 als erste Schweizerin den Gipfel. 2004 eröffnete eine russische Expedition eine neue Route durch die Nordwand, die weitgehend eine Direttissima darstellt. Am 30. Mai erreichten Pawel Schabalin, Ilja Tuchvatullin und Andrej Mariew den Gipfel.

Anlässlich der Olympischen Sommerspiele 2008 in Peking wurde die olympische Fackel während des Fackellaufes am 8. Mai 2008 von Bergsteigern von der tibetischen Seite auf den Gipfel gebracht. Um dies medial besser präsentieren zu können, wurde die Straße zum nördlichen Basislager im Rongpu-Tal befestigt, und der Mobilfunkanbieter China Mobile installierte im Jahr 2007 insgesamt drei Sendemasten (auf 5200, 5800 und Höhe). Vor und während der Besteigung mit der Fackel war der Berg für alle anderen Bergsteiger gesperrt.

Die Südkoreaner Park Young-seok, Kang Ki-seok, Jin Jae-chang und Shin Dong-min eröffneten im Jahr 2009 eine neue Route in der Westwand.

Als erste Österreicherin erreichte Sylvia Studer gemeinsam mit ihrer Tochter Claudia und ihrem Mann Wilfried 2010 den Gipfel.

Am 18. April 2014 kamen auf der nepalesischen Seite 16 Menschen (darunter drei Vermisste) durch eine Lawine im Khumbu-Eisbruch auf 5800 Meter Höhe ums Leben, das bis dahin folgenreichste Unglück in der Besteigungsgeschichte.

Aufgrund des Erdbebens vom 25. April 2015 kam es zu Lawinen im Bereich des Basislagers. Dabei kamen mindestens 18 Personen ums Leben, was das bisher schwerste Unglück in der Besteigungsgeschichte darstellt. Zum Zeitpunkt des Unglücks am Samstag hielten sich nach offiziellen Angaben etwa 1000 Bergsteiger und Träger am Mount Everest auf. Nach dem Erdbeben verboten die chinesischen Behörden aufgrund möglicher Gefahren durch locker gewordenes Eis und Gestein die weitere Besteigung des Mount Everest über die Nordroute bis zum Vormonsun 2016. In Nepal wurde kein offizielles Besteigungsverbot erlassen. Da aber der Khumbu-Eisbruch nicht erneut versichert werden konnte, endeten auch auf der Südroute die Besteigungsversuche. Durch das Erdbeben wurde auch der Hillary Step zerstört.

Die aus Bielefeld stammende Anja Blacha ist mit 26 Jahren die jüngste deutsche Frau, die den Gipfel des Mount Everest erreichte. In der gleichen Gruppe stieg der blinde österreichische Bergsteiger Andy Holzer auf und erklomm damit die Seven Summits. Sie stiegen unter Zuhilfenahme von Flaschensauerstoff über die Nordroute auf und standen am 21. Mai 2017 auf dem Gipfel.

Seit den 1980er Jahren ist eine regelrechte Everest-Euphorie ausgebrochen, was zu einem deutlichen Anstieg der Zahl der Gipfelbesteigungen geführt hat. Während bis 1979 – also innerhalb von 27 Jahren seit der Erstbesteigung – nur 99 Menschen auf dem Gipfel waren (drei von ihnen zweimal), verdoppelte sich die Zahl der Gipfelbesteigungen zwischen 1980 und 1985 – innerhalb von nur sechs Jahren. 1993 erreichten erstmals mehr als 100 Menschen in einem Jahr den Gipfel. Im Jahr 2003 konnten mit 266 erstmals mehr als 200 Besteigungen gezählt werden. In der Rekordsaison 2007 wurde der höchste Punkt von 604 Bergsteigern erreicht. Da in diesem Jahr einige Bergsteiger mehrmals auf dem Gipfel standen, konnten sogar 630 Besteigungen gezählt werden. Das Spektrum der Gipfelaspiranten reicht von erfahrenen Alpinisten bis zu weniger geübten, die sich auf die von ihren Bergführern gelegten Fixseile verlassen müssen.

Die Kosten hierfür betragen zwischen 13.000 und 65.000 US-Dollar. Im Jahr 2013 stiegen von der nepalesischen Seite 32 Teams auf, mit denen 242 Bergsteiger (davon 34 weiblich) den Gipfel erreichten. Alleine die Gebühren für Genehmigungen für diese 32 Teams betrugen 2.525.000 US-Dollar. Das sind 80 Prozent aller vom Staat eingenommenen Besteigungsgebühren im Jahr 2013 (für einige Berge in Nepal werden die Genehmigungen nicht vom Staat, sondern von der „Nepal Mountaineering Association“ vergeben). Etwa ein Drittel aller Bergsteiger am Everest gehören zu einer kommerziellen Expedition. Nach wie vor sind Besteigungen ohne Flaschensauerstoff selten.

Das Expeditions-Bergsteigen am Everest in der klassischen „Himalaya-Belagerungstaktik“ wird seit den Erfolgen Messners (1978 Besteigung mit Habeler, 1980 Alleingang, beide Male ohne zusätzlichen Sauerstoff) immer kritischer betrachtet: Die Anziehungskraft des höchsten Berges der Erde lockt viele, die sich dieser Herausforderung nur stellen können, wenn sie sich umfangreich Hilfe kaufen; Träger, die sie vom Schleppen aller Lasten außer der minimalen persönlichen Ausrüstung entbinden, sogar die Zelte und die Schlafsäcke werden von Sherpas getragen, damit der teuer zahlende Kunde seine Kräfte für den Gipfel aufsparen kann. Von vielen renommierten Bergsteigern wird der Mount Everest wegen des großen Andrangs mittlerweile gemieden.

Bei zwei kommerziellen Besteigungen fanden 1996 zwölf Menschen den Tod, weil sie hoch oben von plötzlichen Wetterumschwüngen überrascht wurden. Diese Ereignisse sind in dem IMAX-Film "Everest – Gipfel ohne Gnade" und in den Büchern von Jon Krakauer ("In eisige Höhen") und Anatoli Bukrejew ("Der Gipfel. Tragödie am Mount Everest") beschrieben. Die Routen auf den hohen Hängen des Mount Everest sind von den Leichen gestorbener Bergsteiger gesäumt: Über 200 Menschen ließen beim Versuch der Besteigung ihr Leben. Die Versuchung, unbedingt auf dem höchsten festen Punkt der Erdoberfläche stehen zu wollen, ist groß für viele nicht ausreichend Erfahrene. Anstrengung und Sauerstoffmangel führen zu schlechteren Reaktionen und eingeschränkten Denkvermögen, welche die Entscheidung zur Umkehr bei widrigen Verhältnissen erschwert.

An manchen der jeweils sehr wenigen „Fenstertage“ im Jahr (im Mai, vor dem Aufkommen des Monsuns) stauen sich an den klettertechnisch schwierigeren, mit Fixseilen gesicherten Stellen die Aufstiegswilligen teils mehrere Stunden lang: Die Zeit verrinnt, man kühlt beim Warten aus, und die Gefahr steigt, nicht mehr bei Tageslicht absteigen zu können. Wer hoch oben am Everest in die zweite Nacht gerät (der Endaufstieg muss in der Nacht davor vor Mitternacht beginnen), hat extrem schlechte Aussichten, ohne schwere körperliche Schäden (erfrorene Zehen, Füße, Finger, Nase) wieder vom Berg herabzukommen. Auch die Hilfsmöglichkeiten der Bergführer sind in der extremen Umgebung auf den letzten zweitausend Höhenmetern sehr begrenzt. Hilfeleistung unterbleibt oft auch wegen des Risikos eigener gesundheitlicher Schäden oder wegen der Vereitelung der eigenen Gipfelchancen.

Ein weiteres Problem dieser Art von „Tourismus“ ist, dass die Umweltverschmutzung der Lager durch Müll (Zelte, Sauerstoffflaschen, Speisereste, Dosen und Medikamente) rapide zugenommen hat. Der Südsattel wurde schon als „höchste Müllkippe der Erde“ tituliert. Mittlerweile wird von administrativer Seite verstärkt versucht, diese Begleiterscheinungen zu reduzieren. Jede Expedition muss ein Müllpfand hinterlegen, das nur zurückbezahlt wird, wenn die gesamte Ausrüstung und sogar die Fäkalien aus dem Basislager wieder abtransportiert werden. Zudem werden in regelmäßigen Abständen Expeditionen ausgerichtet, die Müll aus den Hochlagern vom Berg herunterholen. Seit dem Frühjahr 2014 sind Bergsteiger sogar verpflichtet, mindestens 8 Kilogramm Altmüll auf dem Abstieg einzusammeln und mitzubringen.
Auch private Initiativen versuchen das Problem zu mildern. Der Japaner Ken Noguchi hat (Stand 2007) fünf Säuberungsexpeditionen ausgerichtet und dabei neun Tonnen Abfälle abtransportiert.
Im Jahr 2010 startete eine Initiative von 20 Sherpas unter Leitung von Namgyal Sherpa, die das Ziel hatte, den Berg von mindestens 3000 kg Bergsteiger-Müll (alte Zelte, Seile, Sauerstoffflaschen, Nahrungsmittelverpackungen, u. a.) zu säubern. Nebenbei sollten auch die Leichen von mehreren Bergsteigern (u. a. Gianni Goltz † 2008, Rob Hall † 1996) geborgen werden. 2018 konzentriert sich die Abfallsammelkampagne auf recyclebare Materialien. 
In der Nähe des Everest sind hotelähnliche Lodges entstanden. Sie befinden sich nicht an den traditionellen Siedlungsschwerpunkten und bieten „Komforttrekkern“ einen gewissen Luxus. ("siehe auch" Mount Everest Trek).

Seit der Entdeckung von 1852, dass der Everest der höchste Berg der Erde ist, vergingen 101 Jahre bis zu seiner Erstbesteigung. 
15 Expeditionen versuchten dies vergeblich; dabei starben 21 Menschen.
Bis Ende 2006 gab es über 14.000 Besteigungsversuche, 3057 davon gelangen. Nur etwa einer von fünf Aspiranten gelangte auf den den Gipfel. Bis Ende 2010 wurden insgesamt 5104 Gipfelerfolge gezählt. Davon wurden nur 173 Besteigungen ohne Zusatzsauerstoff durchgeführt. 

Bis 2013 starben am Everest insgesamt 248 Menschen – 140 auf nepalesischer und 108 auf tibetischer Seite.

Häufige Todesursachen sind Abstürze, Erfrierung, Erschöpfung, Höhenkrankheit und Lawinen. Die meisten Bergsteiger verunglücken oberhalb von während des Abstiegs. Besteigungen ohne Flaschensauerstoff sind durchschnittlich nur halb so oft erfolgreich und mit einem doppelt so großen Todesrisiko behaftet wie Besteigungen mit Flaschensauerstoff. Am 23. Mai 2010, dem bis dahin größten Ansturm, standen 169 Menschen auf dem Gipfel.

Die schnellste Besteigung gelang dem Sherpa Pemba Dorjee, der am 21. Mai 2004 den Aufstieg vom Basislager zum Gipfel in nur 8:10 Stunden schaffte. Auf der Nordroute hält Christian Stangl seit dem Jahr 2006 mit 16:42 den Rekord, wobei er allerdings am vorgeschobenen Basislager startete. Hans Kammerlander brauchte auf derselben Route zehn Jahre zuvor nur wenige Minuten länger. Bei diesen Schnellbesteigungen ist aber zu beachten, dass der genaue Startpunkt bei jeder Besteigung anders war und sie deshalb kaum miteinander verglichen werden können. Die meisten Besteigungen hat bisher der Sherpa Appa durchgeführt, der mittlerweile 21 Mal (Stand: Mai 2011) auf dem Gipfel stand.

Der jüngste Besteiger war der US-Amerikaner Jordan Romero, der im Jahr 2010 im Alter von 13 Jahren den Gipfel erreichte. Mit einem Alter von 80 Jahren war der Japaner Yūichirō Miura am 23. Mai 2013 der älteste Mensch auf dem Gipfel. Er war damit auch der älteste Mensch überhaupt, der je auf einem Achttausender stand. Am 6. Mai 2017 starb der 85 Jahre alte Min Bahadur Sherchan vermutlich an einem Herzinfarkt im Basislager auf der Südseite des Berges, bevor er seinen abermaligen Versuch starten konnte, nach seinem Rekord 2008 einen neuen Altersrekord aufzustellen.

Die bisher älteste Frau, die auf dem Everest war, ist die Japanerin Tamae Watanabe. Sie erreichte erstmals am 16. Mai 2002 als schon damals mit 63 Jahren älteste Besteigerin den Gipfel über die Südostroute von Nepal aus. Durch eine erneute Begehung des Gipfels am 19. Mai 2012 über die Nordroute von Tibet aus erhöhte sie ihren eigenen, bis dahin ungeschlagenen Altersrekord auf 73 Jahre.

Am Everest gibt es bis heute insgesamt 20 Routen. Die beiden Standardrouten sind die Südroute und die Nordroute. Die weiteren Routen sind technisch deutlich schwieriger und zum größten Teil nur einmal begangen worden.

Endpunkt aller Routen ist ein nur etwa zwei Quadratmeter großes Gipfelplateau. Die chinesische Nordroute ist im Vergleich zur nepalesischen Südroute mit etwa 40.000 US-$ (Stand 2005) für den zahlenden Kunden um ein Drittel „preiswerter“, wenn man sich einer der zahlreichen geführten Expeditionen anschließt. Der Grund dafür sind logistische Vorteile (niedrigere Gebühren für die staatliche Genehmigung einer Expedition, Zahl der notwendigen Yaks und Träger, Zahl der Sauerstoffflaschen und weiteres). Die prozentuale Erfolgsrate der Nordroute ist jedoch aufgrund der sehr weiten Wege geringer als auf der Südroute. In jedem Fall muss man sich der Gefahren des geringen Sauerstoffgehalts der Luft, plötzlicher Wetterumschwünge und heftiger, äußerst kalter Winde auf den Graten bewusst sein. Der Aufenthalt in der sogenannten „Todeszone“ oberhalb ist auf der Nordroute um ein bis zwei Tage länger; dementsprechend ist das Risiko, wegen widrigen Wetters oben festzusitzen oder gar unterwegs in Nebel oder Schneesturm zu geraten, auf der Nordseite höher.

Die Südroute gilt als Standardroute und wurde auch bei der Erstbesteigung gewählt. Vom Basislager auf der nepalesischen Südseite auf etwa führt sie zunächst durch den Khumbu-Eisbruch "(Khumbu Icefall)": eine steile Passage, in der das Gletschereis aus dem Tal des Schweigens 600 Meter abfällt und in große Blöcke – sogenannte Séracs – zerbricht, die den Aufstieg sehr erschweren. Da sie aufgrund der Eisbewegung jederzeit umstürzen können, ist es nur zu kühlen Tageszeiten ratsam, sie zu durchklettern. Der Khumbu-Eisbruch wird jeweils zu Saisonbeginn von einem Team aus Sherpas mit Leitern und Fixseilen gesichert. Diese gesicherte Route wird von allen Expeditionen gemeinsam genutzt.

Der weitere Verlauf der Route führt durch das Tal des Schweigens (Western Cwm, „kuum“ gesprochen, aus dem Walisischen). Das "Western Cwm" ist ein von Mount Everest, Lhotse und Nuptse eingeschlossenes Kar mit etwa 3 Kilometer Länge und das höchstgelegene Kar der Erde. Nach Durchquerung dieses Talkessels setzt sich der Weg über die vergletscherte westliche Lhotse-Flanke fort. Sie ist etwa 60 Grad steil und umfasst 1000 Höhenmeter. Im oberen Teil der Wand führt die Route über den Genfer Sporn zum zwischen Lhotse und Everest gelegenen Südsattel "(South Col)" auf etwa Höhe, wo fast alle Expeditionen das Hochlager für die Gipfeletappe einrichten. Es wurden allerdings auch schon noch höher gelegene Lager eingerichtet. Vom Südsattel aus führt der Weg den Grat des Everest hinauf bis zum Südgipfel etwa 100 Höhenmeter unterhalb des eigentlichen Gipfels, dann über das bis 2015 letzte große Hindernis, eine etwa zwölf Meter hohe, fast senkrechte Felskante, den Hillary Step.

Die Alternative zur Südroute ist die Nordroute von der chinesischen Seite aus. Sie beginnt im Rongpu-Tal mit einem Basislager in etwa Höhe und führt in einem Zweitagestrek mit Yak-Transport in das Tal des östlichen Rongpu-Gletschers, wo sich am Fuß der Nordsattel-Wand das vorgeschobene Basislager (ABC, "advanced base camp") befindet. Dann geht die Tour den Steilhang hinauf auf den Nordsattel "(North Col)" mit etwa Höhe, von wo aus die ausgesetzten Gipfelgrate (Nordgrat und Nordostgrat) den weiteren Aufstieg über geringer geneigte Grate (im Vergleich zur steileren Südroute) ermöglichen. Ernsthaftes kräftezehrendes und klettertechnisches Hindernis ist hinter dem letzten Lager in etwa Höhe hoch auf dem oberen Grat die mittlere der drei Felsstufen "(Second Step)" mit einer Fußhöhe auf etwa . Der Second Step weist eine Kletterhöhe von etwa vierzig Metern auf, die letzten fünf Meter sind fast senkrecht. Hier wurde von einer chinesischen Expedition im Jahr 1975 eine Leiter befestigt. Von dort führt die zumeist auf dem Grat verlaufende Route noch recht weit und auch über das bis zu 50 Grad steile Gipfelschneefeld. Bei seiner Alleinbegehung des Mount Everest umging Reinhold Messner den Second Step und wählte einen Weg durch das Norton-Couloir.

Zwei der drei Hauptwände wurden bereits in etwa direkter Falllinie zum Gipfel (Direttissima) begangen: 1975 die Südwestwand und 2004 die Nordwand. An der Ostwand "(Kangshung-Wand)" gibt es zwei erstiegene Routen, die jedoch nicht als Direttissime gezählt werden können. Die Ostwand- oder Kangshung-Direttissima ist somit bislang unbewältigt. Unternähme man diese, so müsste man in einer der – vom Fuß zum Gipfel gerechnet – höchsten Wände der Erde, einen weit mehr als 3500 Meter hohen, steilen und lawinengefährdeten Felsen durchsteigen.

Am 3. April 1933 wurde der Mount Everest erstmals von einem Flugzeug überflogen, einer Westland PV-3 (Kennzeichen: G-ACAZ) und einer sie begleitenden Westland PV-6 (G-ACBR), beide ausgerüstet mit einem Bristol-Pegasus-Motor. Unter der Leitung von Douglas Douglas-Hamilton, Lord Clydesdale und späterer 14. Duke of Hamilton, wurden während des Fluges mit offenen Doppeldeckern wichtige Erkenntnisse über Flüge in großer Höhe gesammelt, die zur weiteren Entwicklung der Druckkabine beitrugen.

Der Franzose Didier Delsalle landete am 14. und am 15. Mai 2005 als erster Mensch auf dem Gipfel des Mount Everest: mit einem speziell präparierten Hubschrauber vom Typ Eurocopter AS 350 B-3, mit „Hover Landings“, das heißt bei fast voller Motorleistung, nur eben aufgesetzt, um bei Gefahren oder Böen unmittelbar wieder starten zu können. Er stieg am Gipfel nicht aus und konnte dort auch keine Lasten aufnehmen.

2007 überflog Bear Grylls den Mount Everest mit einem Motorschirm.





</doc>
<doc id="3399" url="https://de.wikipedia.org/wiki?curid=3399" title="Monopol">
Monopol

Als Monopol (Zusammensetzung aus „allein“ und "" „verkaufen“) wird in den Wirtschaftswissenschaften eine Marktsituation (Marktform) bezeichnet, in der für ein ökonomisches Gut nur "ein" Anbieter vorhanden ist. Bedeutungsgleich ist das "reine Monopol," von Marktformen zu unterscheiden, in denen beispielsweise nur in einem kleineren Teilbereich Monopolstrukturen vorherrschen (wie im Fall "monopolistischer Konkurrenz," siehe auch unten der Abschnitt "Quasi-Monopol").

Mitunter wird auch, entgegen der etymologischen Bedeutung ("pōlein" „verkaufen“) sowie der in diesem Artikel zugrunde gelegten Definition eine Marktsituation als Monopol bezeichnet, in der nur ein Nachfrager auftritt. Diese Form ist ein "Nachfragemonopol" in Abgrenzung vom oben skizzierten "Angebotsmonopol". Üblich ist für das Nachfragemonopol jedoch die (auch etymologisch stimmige) Bezeichnung "Monopson".

Der Begriff "Monopol" wird abseits der Wirtschaft mitunter gesellschaftlich und politisch gebraucht, etwa für das Gewaltmonopol des Staates oder das Informationsmonopol.

Die Entstehung eines Monopols fängt streng genommen dort an, wo zum ersten Mal der Wettbewerbsgedanke entspringt. Der eine möchte besser sein als der andere, so die Devise. Folglich wird versucht, die eigene Position durch Setzen differenzierter Parameter (Preis, Menge, Werbung) zu verbessern, um einen entscheidenden Wettbewerbsvorteil bis hin zur Idealvorstellung der Marktmacht zu realisieren. Hierzu lassen sich verschiedene theoretische Ansätze aufstellen, die die Kernfrage, wo genau Monopole entstehen, versuchen zu beantworten. Eine erste Vermutung lässt sich im späten Mittelalter finden (12. bis 15. Jahrhundert), in dem Waren auf dem Markt gehandelt wurden und versucht wurde, die Produkte (Fisch, Obst, Gemüse) schneller als die Konkurrenz zu verkaufen. Es könnte aber auch eventuell durch die ersten entstandenen Manufakturen gegen Ende des 17. Jahrhunderts zur Barockzeit sein. Oder ist es vielmehr der Industrialisierung geschuldet, in der der Gedanke der Effizienz, Wohlstand und Produktivitätsfortschritt eine entscheidende Rolle gespielt hat.

Einen weiteren möglichen Ansatz beschreibt Pierenkemper. Er beschreibt die Wirtschaftsgeschichte und fängt dabei in der Neuzeit an (seit 1500). Durch die immer weiter voranschreitende Industrialisierung im 19. Jahrhundert, geprägt von Schumpeter und seinem Schöpferischen Prozess der Zerstörung, beschäftigt er sich mit der Entstehung des Industrialisierungsprozesses und dessen Wachstum. Dabei unterscheidet er zwischen fünf Entwicklungsstadien (siehe: Wirtschaftsstil).

In der 4. Phase der Entwicklungsstadien, der Reifephase, ist es möglich die aktuellste Technologie zu benutzen und damit die Ressourcen optimal einzusetzen. Es entstehen ganz neue Berufszweige und Unternehmen schließen sich erstmals zu "MONOPOLEN", Trusts und Kartellen zusammen. Durch diese Monopolgründung oder monopolartige Stellung, können Kosten minimiert (subadditive Kostenstrukturen eines einzelnen Unternehmens), andere Mitstreiter aus dem Markt verdrängt werden bzw. es Ihnen schwer macht, sich überhaupt auf dem herrschenden Markt zu etablieren.






In diesem Kapitel wird das Monopol als statisch angesehen. Ein Markt in dem jegliche Anreize zum Wettbewerb und damit die verbundenen Innovationen ausbleiben, kann per Definition kein dynamischer Markt sein. Ein dynamischer Markt hingegen ist durch die Schumpetersche Theorie der Innovationen geprägt. Demnach ermöglichen Pioniergewinne eine temporäre Monopolstellung und lösen damit Anreize zum Wettbewerb und den damit verbundenen Innovationen aus.

Der vollkommene Markt ist ein Modell in der Wirtschaftswissenschaft. Im vollkommenen Markt stehen sich viele Anbieter und viele Nachfrager gegenüber. Es wird vorausgesetzt, dass die Güter homogen sind und vollständige Markttransparenz besteht. Auf der Nachfrager-Seite gibt es keine zeitlichen, räumlichen oder persönlichen Präferenzen. Im vollkommenen Markt haben alle Unternehmen, die dem gleichen Marktsegment entsprechen, die gleiche Produktqualität. Es gilt: Käufer sind Preisnehmer und die Anbieter sind Mengenanpasser.

Auf dem unvollkommenen Markt sind die Güter nicht gleichartig. Vollständige Transparenz ist bei den Marktteilnehmern nicht vorhanden und Nachfrager haben persönliche, räumliche oder zeitliche Präferenzen. Die Anbieter können unbeschränkt in den Markt ein- oder austreten. Auf die monopolistische Konkurrenz trifft vor allem die Unterschiedlichkeit der Güter zu. Deshalb gibt es für die Anbieter einen kleinen monopolistischen Preisspielraum.

Im vollkommenen Markt wäre der Gewinn des Monopolisten am höchsten, weil er keinerlei Beschränkungen oder Hindernisse hat. Wenn die Grenzkosten gleich den Grenzerlösen entsprechen maximiert der Monopolist seinen Gewinn. Die Theorie des vollkommenen Marktes existiert in der Realität nicht. Es gibt zwar ein paar Märkte, die dem vollkommenen Markt sehr nah kommen (etwa den Finanzmärkten), aber dennoch gibt es nur unvollkommene Märkte. Aufgrund dieser Feststellung liegt es auf der Hand, dass der Monopolist in der Realität seinen Gewinn nicht maximieren kann, wie die Theorie es beschreibt. Der Monopolist wird eingeschränkt durch den Staat, durch Substitute von anderen Firmen oder durch eine fehlende Markttransparenz. Im Vergleich zum Polypol auf dem vollkommenen Markt, kann der Anbieter im Polypol auf dem unvollkommenen Markt seinen Preis höher als seine Grenzkosten setzen und produziert eine geringere Menge. Das Marktergebnis ist aufgrund der Überkapazität und der höheren Preise aus ökonomischer Sicht ineffizient.

Das Monopol existiert idealtypisch ohne marktregulierenden Einfluss, beispielsweise weil ein Anbieter alleine Zugriff auf bestimmte Rohstoffe hat oder alleinig über bedeutende Technologien verfügt (etwa durch Marktvorsprung). Häufig ergibt sich das natürliche Monopol aus natürlichen Markteintrittsbarrieren, insbesondere wenn eine aufwendige flächendeckende Infrastruktur erforderlich ist, wie bei Eisenbahnnetzen oder der Versorgung mit Strom, Wasser oder Gas. Ein natürliches Monopol im engeren Sinn ist eine Marktform, in der ein Unternehmen mit sinkenden Durchschnittskosten bei steigender Produktionsmenge die Nachfrage befriedigen kann; in diesem Fall produziert ein einzelnes Unternehmen dauerhaft kostengünstiger und kann Konkurrenten vom Markt verdrängen.

Wenn es auf einem Markt zwar mehr als einen Anbieter gibt, von denen einer aufgrund eines sehr starken natürlichen Wettbewerbsvorteils eine marktbeherrschende Stellung hat handelt es sich um ein Quasi-Monopol. Es handelt sich um kein echtes Monopol, kommt diesem in seinen Auswirkungen aber nahe.

Quasi-Monopole sind besonders häufig in der Informationstechnologie zu finden: Die Gestaltung von Software und Daten richtet sich häufig nach bestimmten Quasi-Standards, an denen nur ein Anbieter die Rechte hat oder bei denen es für Wettbewerber zu aufwendig wäre, kompatible Produkte zu einem wettbewerbsfähigen Preis zu entwickeln. (Letzteres insbesondere, da die kostengünstige digitale Reproduktion es dem Marktführer leicht macht, einen Wettbewerber preislich zu unterbieten und ihm so den Markteintritt zu erschweren.) Wer nach einem solchen Quasi-Standard arbeitende Software oder Daten nutzen oder mit anderen austauschen will, ist auf die Produkte dieses Anbieters angewiesen. Das bekannteste Beispiel ist Microsoft, das mit Microsoft Windows ein Quasi-Monopol für PC-Betriebssysteme und mit Microsoft Office ein Quasi-Monopol für Office-Suiten innehat. Quasi-Monopole im Bereich der Informationstechnologie können unter Umständen durch die konsequente Nutzung offener Standards verhindert werden.

Ein weiteres Beispiel für ein Quasi-Monopol war Lego auf dem Markt für Steckbausteine aus Plastik.

Kollektivmonopol (auch "Vertragliches Monopol"): Das Monopol existiert, da sich alle Anbieter oder Nachfrager auf gemeinsame Leistungen und Preise festlegen (etwa durch ein Kartell) und so der Wettbewerb ausgeschaltet wird. In den meisten Ländern sind solche Absprachen in der Regel illegal (in Deutschland: Gesetz gegen Wettbewerbsbeschränkungen).

Als rechtliches Monopol wird ein Monopol bezeichnet, das aufgrund einer gesetzlichen Bestimmung existiert. Diese Form findet sich heutzutage beim Staat (beispielsweise im Außenhandelsmonopol in Artikel 14h der Sowjetischen Verfassung von 1936) und bei (auch ehemaligen) Staatsbetrieben (wie Postmonopol); die wenigen Ausnahmen wie das Zündwarenmonopol, Branntweinmonopol oder das Salzregal sind in modernen Wirtschaftsordnungen weitgehend abgeschafft. In der Vergangenheit herrschte in der Feuerversicherung ein Monopol durch die Versicherungspflicht bei den regionalen öffentlich-rechtlichen Versicherern. Monopole aufgrund von Patenten und anderen immateriellen Monopolrechten wie dem Urheberrecht werden zu dieser Kategorie gezählt.

Wird angenommen, wie klassischerweise der Fall, dass die Monopolsituation exogen vorgegeben ist (beispielsweise durch staatliche und technologische Marktzutrittsbarrieren), braucht ein Monopolist bei der Preisgestaltung keine Rücksicht auf (etwaige) Wettbewerber zu nehmen. Der Monopolist kenne die gegebene Marktnachfragefunktion formula_1 bzw. deren Inverse (Preis-Absatz-Funktion) formula_2. Seine Kostenfunktion sei formula_3. Er maximiert davon ausgehend seine Erlösfunktion. Es läßt sich entweder vorstellen, dass er die Angebotsmenge wählt, oder aber, dass er den Preis wählt. Zu beachten ist dabei lediglich, dass sich die jeweils andere Größe unmittelbar aus seiner Entscheidung ergibt: Wählt er einen Preis formula_4, folgt aus der Nachfragefunktion, dass er maximal genau formula_5 Einheiten verkaufen kann; wählt er die Menge formula_6, folgt aus der (inversen) Nachfragefunktion wiederum, dass sich auf dem Markt der Preis formula_7 ergibt. Folgende Maximierungsprobleme liefern demgemäß dasselbe Preis-Mengen-Paar
Aus praktischen Gründen wird üblicherweise letzteres Problem betrachtet. Lösen mithilfe der Produktregel und Umstellen liefert die Bedingung erster Ordnung für das Gewinnoptimum
wobei formula_11 die Preiselastizität der Nachfrage zur Gütermenge formula_12 ist. Wird nun unterstellt, dass formula_11 gemäß dem (Gesetz der Nachfrage) negativ ist, ergibt sich aus den vorstehenden Gleichungen:
Im Optimum des Monopolisten entspricht also der Lerner-Index dem Kehrwert der betragsmäßigen Nachfrageelastizität. Für die Nachfrageelastizität gilt mit dieser Bedingung formula_15, weil formula_16 und formula_17 gemäß Annahme.

An dieser Bedingung ist somit direkt ablesbar, dass der Monopolpreis über den Grenzkosten liegt. Hieran wird der Unterschied zum Fall vollkommenen Wettbewerbs deutlich, wo im Optimum der Preis den Grenzkosten entspricht. Zudem ist ersichtlich, dass der mögliche Preis umso höher im Verhältnis zu den Grenzkosten sein kann, je inelastischer die Güternachfrage ist.

Im rechts liegenden Schaubild (Abb. 1) ist die allokative Ineffizienz im Monopolfall dargestellt. Auf der Abszisse befindet sich die Menge und auf der Ordinate der Preis. Die LDG (Langfristige Durchschnittskosten) und die LGK (Langfristige Grenzkosten) fallen aus Vereinfachungsgründen zusammen. Außerdem ist eine Nachfragekurve abgebildet, die die LDK/LGK im Punkt E schneidet. Die maximale Wohlfahrt wäre das Dreieck ADF. Im schwarz markierten Dreieck lässt sich die Wohlfahrtsminderung "CEF" erkennen. Wie kommt es zu einer Wohlfahrtsminderung. Der Monopolist setzt seinen Cournot-Preis bei Punkt C. Dort sind für Ihn, wie oben bereits erwähnt, die Gewinnspannen am höchsten. Aber die Konsumenten, die ein Gut des Monopolisten von Punkt C bis Punkt E der Nachfragekurve kaufen würden, können nicht befriedigt werden durch den zu hoch gesetzten Preis den Monopolisten. Der optimale Allokationspunkt wäre der Schnittpunkt zwischen LGK und der Nachfragekurve. Da dies aber nicht durch den Cournot-Preis zustande kommt, findet keine optimale Verteilungswirkung zwischen Nachfrage und Angebot statt. Die Verteilung ist daher suboptimal und wirft die Frage der Regulierung auf, wie in den zwei nächsten Fällen zu sehen ist.

Im 2. rechts liegenden Schaubild (Abb. 2) ist die technische Ineffizienz dargestellt. Auf der Abszisse befindet sich die Menge und auf der Ordinate der Preis. Technische Ineffizienz bedeutet, dass die tatsächlichen Kosten den gesamtwirtschaftlich geringsten Kosten zur realen gegebenen Output-Menge nicht entsprechen. Da der Monopolist sinkende Anreize zur Kostendisziplin hat (Monopolist wird durch fehlenden Wettbewerb nicht mehr diszipliniert), verändert sich die optimale LGK/ LDK-Gerade (Langfristige Grenzkosten/Langfristige Durchschnittskosten) nach oben zur GK/DK- Gerade. Die schwarz markierte Fläche ist nach wie vor der allokative Wohlfahrtsverlust. Durch diese technische Ineffizienz verschiebt sich der Punkt C zu C' nach oben. Dies hat zur Folge, dass sich der Wohlfahrtsverlust um die rot markierte Fläche vergrößert (also rot und schwarz stellen hier den aktuellen Wohlfahrtsverlust unterhalb der Nachfragekurve dar - Punkte: CC'DF). Die grüne Fläche BB'D'F stellt die technische Ineffizienz für den Monopolisten dar. In Höhe der grünen Fläche hat der Monopolist nun mehr Kosten zu tragen, da er nicht zu den geringstmöglichen Kosten produziert. Der Gewinn des Monopolisten schmälert sich.

Harvey Leibenstein unterscheidet bei der technischen Ineffizienz noch zwischen X-Ineffizienzen von Typ I und Typ II.

Typ I: Insiderrenten

Typ II: Technologien mit Irreversibilität

Im 3. rechts liegenden Schaubild (Abb. 3) ist die Letzte der Ineffizienzen im Monopol „Die qualitative Ineffizienz“ dargestellt. Um das Schaubild besser verstehen zu können wird zunächst die Ausgangssituation N1 und GK, DK (Grenzkosten/Durchschnittskosten) mit dem Schnittpunkt D (optimale Wohlfahrt A1 ,A ,D) betrachtet. Nun verschiebt sich N1 nach N2 (Nachfrager werden weniger) aufgrund qualitativer Mängel von Produkten, die bei gegebenen Kostenbedingungen nicht den Präferenzen der Kunden entspricht. Die Grenzerlöskosten-Kurve schneidet nun die GK,DK nicht mehr in Punkt B1, sondern in B2 und führt daher zum neuen Cournot Punkt C2. Die schwarz markierte Fläche ist, wie in den Schaubildern davor der Wohlfahrtsverlust durch die allokative Ineffizienz. Nun kommt die rote Fläche durch die Qualitätsminderung hinzu. Die grüne Fläche zeigt die qualitative Ineffizienz durch die Links-Verschiebung der Nachfragekurve. Die Konsumenten zahlen jetzt weniger für das Gut des Monopolisten als vorher, da sie mit der Qualität nicht mehr zufrieden sind. Letztendlich verringert sich die Wohlfahrt und der Gewinn des Monopolisten.

Soll der Wohlfahrtsverlust quantifiziert werden, kann auf das Schaubild von Angebot- und Nachfragekurve zurückgegriffen werden (siehe Abb. 4). Im vollständigen Wettbewerb (Polypol) entsprechen sich Angebot und Nachfrage; die gehandelte Menge beträgt "X", der zugehörige Preis "p". Wie oben gezeigt, ist der Preis in einem Monopol jedoch höher (hier: "p") und die Menge entsprechend geringer (hier: "X"). Dadurch ergeben sich bei der Analyse der Konsumenten- und Produzentenrente erhebliche Verschiebungen. In Abb. 1 entspricht die Konsumentenrente nur noch dem grau unterlegten Dreieck. Denn Konsumenten, die eine Zahlungsbereitschaft von weniger als dem Monopolpreis haben, erwerben das Gut erst gar nicht; die Produzentenrente entspricht der gesamten hellblauen Fläche. Im Vergleich zum Polypol ist der rechteckige Teil der blauen Fläche auch der Produzentenrente zuzurechnen ist – im Polypol war sie noch Bestandteil der Konsumentenrente. Der Grund dafür besteht wiederum im höheren Preis oder der Tatsache, dass dieser nun weiter oberhalb der in der Angebotsfunktion zum Ausdruck kommenden Grenzkosten liegt.

Da die Menge aber nur "X" beträgt, folgt insgesamt, dass die Summe aus Konsumenten- und Produzentenrente im Fall des Monopols geringer ist als im Polypol: Die rot eingefärbten Flächen zählen nicht mehr dazu, sie sind gewissermaßen infolge der ineffizient geringen Bereitstellung des Gutes „verloren“, es wird im Englischen entsprechend als "deadweight loss" bezeichnet. Die gesamte rote Fläche bildet das so genannte Harberger-Dreieck.

Durch verschiedene Formen der Preisdiskriminierung kann das Monopolergebnis beeinflusst werden. Kann ein Monopolist beispielsweise "perfekt diskriminieren" (Preisdiskriminierung 1. Grades), kann er von jedem Konsumenten dessen Reservationspreis (das heißt den höchsten Preis, den dieser zu zahlen bereit ist) verlangen und das Gut an diejenigen Nachfrage verkaufen, die die höchste Wertschätzung haben. Es tritt dann kein Wohlfahrtsverlust ein, weil er dieselbe Menge absetzt, wie sich auch im Wettbewerbsfall ergeben würde: Der perfekt preisdiskiminierende Monopolist verkauft jedem sein Gut, der eine marginale Zahlungsbereitschaft aufweist, die mindestens den Grenzkosten der Produktion entspricht, und er verlangt dafür einen Betrag, der genau der individuellen Zahlungsbereitschaft entspricht.

Dieses Ergebnis ist dementsprechend pareto-effizient, weil jede Änderung der resultierenden Allokation zur Folge hätte, dass der Monopolist (der ja eben die vollständige Rente erhält) schlechter gestellt wird.

Die bisher betrachteten Monopole sind allesamt Einprodukt-Monopole, das heißt ein Anbieter ist Monopolist bezüglich eines Gutes. Die Situation verändert sich, wenn der Anbieter Monopolist für mehrere Güter ist, weil zwischen den beiden Gütern Interdependenzen bestehen können (Substitutions- bzw. komplementäre Beziehung) – dadurch ändert sich das Preissetzungsverhalten des Monopolanbieters.

Sei formula_18 (formula_19) die Nachfrage nach Gut formula_20 in Abhängigkeit von den Preisen formula_21 aller Güter; sei formula_22 die additiv-separable Kostenfunktion in Abhängigkeit von der Menge aller angebotener Güter. Das Maximierungsproblem (hier aus Gründen der Vereinfachung und der Nutzbarkeit des Resultats bezüglich des Preises formuliert) lautet

und führt auf die Bedingungen erster Ordnung

wobei

Das heißt: Im Gewinnoptimum des Mehrprodukt-Monopolisten (auch: Multiprodukt-Monopolisten) gilt für jedes angebotene Gut, dass der Lerner-Index für dieses Gut (linke Seite der Bedingung) dem so genannten Ramsey-Index für das Gut (rechte Seite) entspricht. Zu beachten ist, dass dies impliziert, dass bei Vorliegen von Substituten (formula_25) der Lerner-Index größer ist als der Kehrwert der (Eigenpreis)elastizität, mithin also dass ein höherer Preis gesetzt wird als im Fall des Einprodukt-Monopolisten (siehe oben). Umgekehrtes gilt für den Fall von Komplementärgütern (formula_26) – hier liegt der gesetzte Preis sogar unterhalb des Preises, den der Monopolist anstreben würde, wenn er nicht auf beiden Gütermärkten eine Monopolstellung innehätte.

Der Begriff „Monopolresistenz“ bedeutet, dass ein Monopol durch angreifende Marktteilnehmer oder Newcomer geschützt ist. Dies ist meistens bei natürlichen Monopolen der Fall. Ein natürliches Monopol entsteht immer dann, wenn die Produktion eines Gutes durch ein einzelnes Unternehmen kostengünstiger ist, als es von mehreren Unternehmen am Markt erbracht werden könnte. Natürliche Monopole entstehen aufgrund von Unteilbarkeiten. Unteilbarkeiten lassen die Produktionsmenge in großen Abständen wachsen. Diese Eigenschaft steht im Kontrast zu der Theorie des vollkommenen Markts. Durch diese subadditiven Kostenstrukturen produziert ein Unternehmen mit Monopolstellung effizient und lässt damit keine Lücken, die potentielle Newcomer für den Markteintritt nutzen könnten (dieser Zusammenhang läuft unter der Prämisse, dass die Märkte homogen sind). Wird der Newcomer trotz Effizienz des Monopols in den Markt eintreten, wird dieser mit einem sofortigen Verlust rechnen müssen, da die Gesamtkosten pro produzierte Einheit über dem Marktpreis liegen. Daher wird ein sofortiger Austritt erfolgen. Das Monopolunternehmen hat den Markt damit unbestreitbar gemacht.

Im Folgenden wird gezeigt, unter welchen theoretischen Bedingungen Märkte bestreitbar gemacht werden können (oder das Gegenteil von nicht bestreitbaren Märkten):

1. Newcomer haben die gleichen Rechte und Bedingungen wie das bereits bestehende Unternehmen und zwar kostenminimale Produktionstechnologien, Zugänge zu Input-Märkten und es existieren keine Subventionen

2. Kein Unternehmen hat Nachfragevorteile

3. Es gibt keine Eintrittsbarrieren (Ein- und Austritt sind damit frei) und damit keine Irreversiblen Kosten

4. Alle Unternehmer streben nach Gewinnmaximierung

5. Der Eintritt in den Markt wird nur unter Voraussetzung der Profitabilität gemacht

6. Profitabilität wird für den Newcomer definiert als die Unterbietung der gegebenen Marktpreise mit Gewinnabschlag

Ein Monopol ist stets bestrebt seine alleinige Marktführerschaft zu halten. Dafür ist es notwendig seine Produkte mit dem qualitativ höchsten Anspruch auszustatten. Allerdings sind viele Produkte durch andere ersetzbar (das heißt, sie können substituiert werden). Dabei muss unter Umständen auf einzelne Eigenschaften verzichtet werden. Dies spielt oft jedoch eine untergeordnete Rolle, wenn dadurch ein günstigerer Preis erzielt wird und eine größere Auswahl von Anbietern zur Verfügung stehen. Dies begrenzt die praktische Wirkung von Angebotsmonopolen.

Um diese Situation zumindest abzumildern, gibt es verschiedene Strategien:

Da eine Monopolstellung höchstmöglichen Gewinn verspricht, wird ein Monopolist darauf abzielen, den Markt auch weiterhin vor möglichen Konkurrenten abzuschirmen. Um dies zu erreichen, wird immer wieder zu unlauteren oder marktverzerrenden Mitteln gegriffen. Beispiel einer solchen Praxis ist das Dumping: Produkte werden eine gewisse Zeit lang zu nicht kostendeckenden Preisen angeboten, bis der Konkurrent aus dem Markt verdrängt wurde, um anschließend die Preise wieder zu erhöhen. Diese Situation kann durch ein Kartell entstehen, oder durch ein Oligopol. Auch hier gibt es zum Teil gesetzliche Einschränkungen (siehe Verbot des Verkaufs auf Verlust in Frankreich).

Falls Monopole nicht aus natürlichen Gründen aufgebrochen werden, greift gelegentlich der Staat aus wettbewerbsrechtlichen Gründen ein. Meistens liegt in diesen Fällen ein Verstoß gegen die jeweilige nationale Gesetzgebung vor (in Deutschland sind dies das Gesetz gegen den unlauteren Wettbewerb und das Gesetz gegen Wettbewerbsbeschränkungen).

Beispielsweise wurde das Unternehmen Microsoft, ein Quasi-Monopolist, wegen Missbrauchs seiner Marktmacht verurteilt.

Nach der Theorie bestreitbarer Märkte reicht häufig die Bestreitbarkeit des Monopols aus: Hierzu muss glaubwürdig angedroht werden, dass die Monopolstellung verloren gehen kann, wenn bestimmte Vorgaben nicht eingehalten werden. Die Glaubwürdigkeit steigt insbesondere, wenn die Marktaustrittskosten gering sind.

Wenn der Monopolist nicht effizient produziert (siehe allokative, technische oder qualitative Ineffizienz) und damit folglich einen zu hohen Preis für die Güter festsetzt, entsteht eine Eintrittsmöglichkeit für potentielle Newcomer auf dem Markt.

Im Anschluss an William J. Baumols Theorie bestreitbarer Märkte wird die Ansicht vertreten, dass das Vorliegen eines Monopols keine Maßnahmen von Wettbewerbsbehörden erfordern würde, weil der Wettbewerb zwar nicht in Form mehrerer Anbieter "sichtbar" sei, er wirke aber in latenter Weise. Je nach den Rahmenbedingungen, die sein Monopol begründen, könne ein Monopolist bei seinen Entscheidungen in Bezug auf die Preissetzung oder sein Leistungsangebot (Menge, Qualität), bestimmten Restriktionen unterworfen sein. Dadurch sinke seine Monopolrente.

Ein bestreitbarer Markt könne einen Monopolisten disziplinieren. Liegt der Angebotspreis über den Markteintrittskosten, besteht die Gefahr, dass Angebotskonkurrenten in den Markt eintreten. Liegt der Angebotspreis über den Preisen für Substitutionsgüter können Nachfrager auf vergleichbare Produkte umsteigen. Der Angebotspreis sinkt dann im Extremfall bis zur Preisuntergrenze, die wie beim Oligopol oder beim Polypol entweder durch die Durchschnittskosten oder die Grenzkosten bestimmt wird. Ein jederzeit angreifbares Monopol wird als morphologisches Monopol bezeichnet. Es wird häufig zumindest für kurze Zeit für wünschenswert gehalten, um für technischen Fortschritt zu sorgen.

Ordoliberale wie Walter Eucken weisen dieses Argument jedoch zurück, da jedes Monopol grundsätzlich der wirtschaftspolitischen Zielsetzung eines optimalen Allokationsgleichgewichts widerspreche.

Ein Monopol gilt als regulierungsbedürftig, wenn es Ineffizienzen aufweist oder kein fairer und effizienter Wettbewerb stattfinden kann.

Generell sind zwei allgemeine Formen hinsichtlich der Regulierung von Unternehmen zu unterscheiden. Die erste Form der Regulierung (häufig in Amerika praktiziert) sieht vor, dass private Unternehmen durch staatliche Institutionen kontrolliert werden müssen. Hierbei hat der Staat nur eine indirekte Einflussnahme auf die Unternehmen. In der zweiten Form der Regulierung, kann der Staat allerdings eine direkte Einflussnahme auf das Unternehmen ausüben. Dies geschieht, in dem der Staat Anteile bei Unternehmen erwirbt oder das komplette Unternehmen übernimmt. Somit tritt der Staat als Eigentümer auf und kann die Regulierungsmaßnahmen nach seinen Wünschen mit/gestalten (die zweite Form tritt häufig in Deutschland/Europa auf).

Konkrete Beispiele sind die Deutsche Bahn oder die Deutsche Post AG.

Die Ziele der Regulierung sind daher die Maximierung der Wohlfahrt, den Preis nahe an die Grenzkosten des Unternehmens zu bringen sowie die Rahmenbedingungen für fairen Wettbewerb zu schaffen. Dafür können verschiedene Regulierungsmethoden zum Einsatz kommen.

1. Preisregulierung nach historischen Kosten
2. Preisregulierung nach effizienten Kosten
3. Price-Cap-Regulierung

formula_27

formula_28= Anteil des Güterbündels am Gesamtumsatz der Vorperiode

formula_29 = Preis des Güterbündels in der Periode t

RPI = Retail Price Index (gesamtwirtschaftliche Inflationsrate der Periode t)

X = Preisabschlag (anzuwendender Produktivitätsfaktor)

Die Formel gibt die „Veränderung des regulierten Preises im Rahmen der Änderung des allgemeinen Preisniveaus abzüglich eines unternehmensspezifischen Korrekturfaktors für den Produktivitätsfortschritt“ an.

Vorteile

Probleme

Mögliche Probleme in der Praxis sind, dass die Regulierungsbehörden (meist Monopolkommission) nicht weiß, wie hoch die langfristigen Grenzkosten des zu produzierenden Unternehmens sind. Außerdem kann es sein, wenn das Unternehmen weniger Einnahmen durch die Regulierung hat, dass infolge dessen auch die Qualität der jeweiligen Produkte leiden muss.

Beispiele für Angebotsmonopole sind:

Unter einem "Nachfragemonopol" (auch Monopson) wird eine Marktsituation mit mehreren Anbietern, aber nur einem Nachfrager verstanden. Ein Beispiel ist der Rüstungsmarkt in einer geschlossenen Volkswirtschaft. In der Realität kommt ein Monopson nur sehr eingeschränkt vor. Bei "wenigen" Anbietern und einem Nachfrager wird von einem "beschränkten Monopson" gesprochen. Diese Marktform findet sich häufig bei Ausschreibungsverfahren im Schienenpersonennahverkehr; dort treten eine Landesverkehrsgesellschaft als Nachfrager und die Eisenbahnverkehrsunternehmen, die sich um den angebotenen Verkehrsvertrag bewerben, als Anbieter auf.

Beispiele für Nachfragemonopole (meistens beschränkte Nachfragemonopole) sind:

Stehen einem Monopolisten nur wenige Nachfrager/Anbieter gegenüber, handelt es sich um ein beschränktes Monopol. Treten auf beiden Seiten nur ein Anbieter und ein Nachfrager auf, handelt es sich um ein bilaterales Monopol. Dies ist zu unterscheiden von einer Situation mit zwei Anbietern, dem so genannten Duopol. Gibt es auf einem Markt zwar mehr als einen Anbieter oder Nachfrager, aber dennoch nur sehr wenige, ist dieses ein Oligopol.




</doc>
<doc id="3401" url="https://de.wikipedia.org/wiki?curid=3401" title="Madrider Abkommen über die internationale Registrierung von Marken">
Madrider Abkommen über die internationale Registrierung von Marken

Das Madrider Abkommen über die internationale Registrierung von Marken (kurz: Madrider Markenabkommen oder MMA) von 1891 ist ein Abkommen zwischen einer Vielzahl von Ländern, durch welche nationale Marken eines Verbandsstaates auch in den anderen Verbandsstaaten Schutz genießen können und somit eine international registrierte (IR) Marke geschaffen werden kann. Das Abkommen wurde in den Jahren 1900, 1911, 1925, 1934, 1957 revidiert. Eine wichtige weitere Revision geschah 1967 in Stockholm. Vertragssprache dieses Abkommens ist nach Regel 6 des Abkommens Französisch. Hilfsweise können Gesuche auch in Englisch abgefasst werden. Das Madrider Markenabkommen wird von der WIPO in Genf verwaltet und die fälligen Gebühren werden in Schweizer Franken bemessen. Im Jahre 2007 wurden bei der WIPO 38.471 Anträge auf Registrierung eingereicht.

Um das MMA nutzen zu können, muss folgendes Prozedere durchlaufen werden:

Das MMA vereinfacht so eine mühsamere und teurere Parallelanmeldung bei einer Mehrzahl von nationalen Markenämtern. Die IR-Marke ist jedoch noch in den ersten fünf Jahren nach internationalen Registrierung vernichtbar, sofern die Basismarke innerhalb der fünf Jahre keinen Schutz mehr genießt. Die IR-Marke genießt eine Schutzdauer von 10 Jahren kann aber immer wieder für einen Zeitabschnitt von zehn Jahren, gerechnet vom Ablauf des vorhergehenden Zeitabschnitts an, durch Zahlung der Grundgebühr und gegebenenfalls der Zusatz- und Ergänzungsgebühren erneuert werden.

Neben dem MMA ist im Jahre 1989 ein Zusatzprotokoll geschlossen worden. Dieses steht rechtlich selbständig neben dem MMA, wirkt sich aber auch auf das MMA aus. Staaten müssen, auch wenn sie schon Mitgliedstaaten des MMA sind, dem Protokoll gesondert beitreten. Die Gesamtheit der Staaten, die beiden Abkommen beigetreten sind, heißen Madrider Verband.

Auch Organisationen können dem Protokoll beitreten, so ist zum Beispiel die Europäische Gemeinschaft Verbandsmitglied. Somit verbindet das Protokoll die international registrierte Marke mit dem System der EG-Marke. Über eine IR-Marke kann also eine Gemeinschaftsmarke geschaffen werden. Weiterhin erleichtert das Protokoll die Registrierung der IR-Marke, so dass zum Beispiel die internationale Anmeldung schon mit Stellung des nationalen Markenantrages eingereicht werden kann und nicht bis zur Eintragung der nationalen Marke gewartet werden muss. Auch sind die Fristen und Schutzdauer länger.




</doc>
<doc id="3402" url="https://de.wikipedia.org/wiki?curid=3402" title="Marke (Recht)">
Marke (Recht)

Eine Marke oder ein Markenzeichen wurde mit der Markenrechtsreform 1995 offiziell in Deutschland eingeführt. Das Warenzeichen als traditionelle Bezeichnung war mit dem notwendig gewordenen Schutz von Dienstleistungen als Produkt nicht mehr umfassend genug aussagekräftig geworden. Nun erweitert ein besonderes, rechtlich geschütztes Zeichen, das vor allem dazu dient, Waren oder Dienstleistungen eines Unternehmens von konkurrierenden Waren oder Dienstleistungen anderer Unternehmen zu unterscheiden, den Kanon der Immaterialgüterrechte.

Eine Marke kann aber auch dazu verwendet werden, um ein ganzes Unternehmen oder das Leistungsangebot eines ganzen geographischen Orts (Land, Region, Stadt) eindeutig zu kennzeichnen und von konkurrierenden Unternehmen oder Angeboten abzugrenzen.

Marken können eine einzelne Darstellung oder eine Kombination von einem oder mehrerer Buchstaben, Zeichen, Wörter, Namen, Slogans, Logos, Symbolen, Bildern, Klängen, Klangfolgen bzw. von Erscheinungsformen und Mustern von und für Produkte verschiedener Art sein.

Markenrechte sind ähnlich wie Patente und Urheberrechte immaterielle Monopolrechte, oft auch als geistiges Eigentum bezeichnet.

Der juristische Begriff der Marke unterscheidet sich von dem der Marke im Marketing. Dort umfasst er nicht nur ein geschütztes Zeichen, sondern auch die Gesamtheit der Eigenschaften eines Wirtschaftsgutes, das mit einem Markennamen in Verbindung steht.

Der Begriff Marke geht auf das mittelhochdeutsche Wort „marc“ zurück, das für „Grenze oder Grenzgebiet“ steht (siehe auch Mark).

Während das bereits von einer „Marke“ sprach, prägte der Gesetzgeber in dem Warenbezeichnungsgesetz aus dem Jahre 1894 den Rechtsbegriff „Warenzeichen“. Das Warenzeichengesetz von 1936 hielt an diesem Sprachgebrauch fest. Im Zuge der erwähnten Markenrechtsreform in Deutschland wurde am 1. Januar 1995, mit der Einführung des Markengesetzes (MarkenG), im Gesetzestext das Warenzeichen wieder durch die Marke ersetzt, da bereits seit Ende der 1960er Jahre nicht nur Waren, sondern auch Dienstleistungen unter einem geschützten Zeichen vertrieben werden können. Der Begriff des Warenzeichens war jedoch nicht nur deswegen inzwischen zu eng gefasst. Denn auch der Wandel von der bisher schlichteren Bezeichnung einer Ware hin zur „Markierung“ mittels einer wesentlich vielfältiger gewordenen Wiedergabe von Wort, Bild, Form und Ton als ein Werbeträger, waren der Grund für den Wechsel der Bezeichnungen. In der Literatur und im allgemeinen Sprachgebrauch werden die Begriffe „Warenzeichen“ und „Marke“ allerdings noch häufig synonym verwendet.

Dem Markengesetz zufolge versteht man unter einer Marke ein Zeichen, das dazu dient, Waren und Dienstleistungen eines Unternehmens von denen anderer Unternehmen zu unterscheiden (Herkunftsfunktion). Daher umfasst eine Marke immer ein Zeichen und eine Sammlung von Waren oder Dienstleistungen, die durch das Zeichen von gleichen oder ähnlichen Waren oder Dienstleistungen anderer Unternehmen unterschieden werden können.

Die Geschichte der Marke geht auf die Zeit zurück, in der Händler begannen, die bislang in namenlosen Säcken beschafften Produkte – zumeist Lebensmittel – mit einem Etikett, zu versehen: Die Ware wurde in einer Art „Händlerverpackung“ abgefüllt. Somit waren – zumindest im Lebensmittelbereich – die Handelsmarken die ersten Marken ihrer Zeit.

Ähnliche Bestrebungen hatten jedoch auch Produzenten und Handwerker. So setzt seit dem Mittelalter jeder Handwerker sein Zeichen (Signet) an eine bestimmte Stelle wie beispielsweise Balken oder Gemäuer.

Herkunftsbezeichnungen, Herstellerzeichen und Signets im Sinne der Handelsmarke sind schon seit der griechischen Antike bekannt und finden sich auf Töpferwaren und Figuren, die vornehmlich als Exportartikel produziert wurden. Sie dienten (wie auch heute noch) der Qualitäts- und Originalitätssicherheit der Käufer des vertreibenden Handelshauses und deren Partnern. Die gleiche Funktion hatten auch Siegel, die Gefäße und Säcke verschlossen, und die ebenfalls zu den Vorfahren der „Marke“ zählen.

Auch aus der Antike sind bereits Siegel und Abzeichen, oder auch einfach das Benutzen bestimmter Formen von Verpackungen und Behältnissen, bekannt, die die Herkunft und Zugehörigkeit bestimmter Produkte zu bestimmten Werkstätten oder Herstellungsgebieten symbolisieren sollten. Welche Rechte und Pflichten damit verbunden waren, ist heute nur noch schwer feststellbar. Genau wie heute standen diese Zeichen aber für eine gewisse Qualität und wurden höchstwahrscheinlich auch schon gefälscht und nachgeahmt, um eine solche zu simulieren.

Eine weitere Wurzel ist im Bereich der Künste zu sehen. Seit der Renaissance wurde es üblich, dass Künstler ihre Werke nicht mehr namenlos fertigten, sondern sie signierten. Hier waren schon bald deutliche Profilbildungen erkennbar, nicht nur bezüglich handwerklichen Könnens, sondern auch bezüglich Stil, Kunstauffassung und ideologischem Standpunkt des Künstlers. Bald kam es zu regelrechten Verehrungen von Künstlerpersönlichkeiten. Sie gipfelten im Genie-Kult um 1800. Das Konzept, den Künstlernamen als Marke für künstlerische Autorschaft zu verwenden, ließ sich auf zahlreiche Kreativprodukte übertragen. Marke in diesem Sinne als Kennzeichnung künstlerischer Autorschaft ist heute auch im weiten Spektrum des Designs und für urheberrechtliche geschützte Werke anderer Art präsent.

Im Jahr 1887 wurde in Großbritannien die Pflicht zur Anbringung der Kennzeichnung "Made in Germany" mit dem "Merchandise Marks Act" eingeführt – allerdings nicht als Qualitätssymbol, sondern um britische Konsumenten vor der als minderwertig geltenden Konkurrenz aus Deutschland zu schützen.

Der Wert des immateriellen Monopolrechts einer eingetragenen Marke bemisst sich nach deren den Absatz des Markeninhabers und seine Unternehmungen fördernde Wirkung. Die entsprechende methodische Bewertung kann weder durch Experiment noch durch Messung festgestellt werden, ohne die Marke zu beschädigen. Ersatzweise sind detaillierte Bewertungsverfahren bekannt, die das Gesamtproblem in überschaubare Teilprobleme zerlegen. Eine experimentelle, aber in der Validität beschränkte Ersatzmethode ist das Experiment durch Aufbau einer neuen Marke in demselben Markt und das Messen entsprechenden Aufwandes. Daher ist fast ausschließlich die Taxierung des Wertes einer Marke allein durch eine Schätzung möglich.

Die häufigsten Markenformen sind die Wort- und die Bildmarken. Darüber hinaus gibt es die kombinierten Wort-/Bildmarken. Letztere bestehen aus einem Wort- und einem Bildbestandteil, die einen Gesamteindruck ergeben. Darüber hinaus sind in den letzten Jahren weitere Markenformen hinzugekommen. Insbesondere die abstrakten Farbmarken, Hörmarken, Geruchsmarken, Slogans und dreidimensionale Markenformen. Die (durch europarechtliche Vorgaben beeinflusste) Rechtsentwicklung zur Eintragungsfähigkeit und zum Schutzbereich solcher Marken im Verletzungsfall ist noch nicht abgeschlossen.

Liste der Markenformen gemäß Markenverordnung und geläufige Unterformen:

In erster Linie entsteht der Markenschutz durch die Eintragung der Marke. In vielen Staaten wie Deutschland, Österreich, der Schweiz und Liechtenstein regeln dies nationale Patentämter. Im Einzelnen sind dies das Patent- und Markenamt in München, das Patentamt in Wien, das Eidgenössische Institut für Geistiges Eigentum in Bern und das Amt für Volkswirtschaft (AVW), Fachbereich Immaterialgüterrechte in Vaduz. Nahezu sämtliche Staaten der Welt verfügen über ähnliche Einrichtungen und haben ähnliche Vorschriften, die die Eintragung von Marken vorsehen. Diese nationalen Marken gelten jeweils nur für ein Land. Internationale oder europäische Marken sind gemäß der Pariser Übereinkunft, und dem Madrider Abkommen beim Europäischen Patentamt einzutragen.

Es gibt unterschiedliche Wege zur Entstehung des Markenschutzes. Eine Marke kann auch durch Benutzung entstehen, sofern die Marke Verkehrsgeltung erworben hat (Benutzungsmarke, Nr. 2 MarkenG). Dies ist dann der Fall, wenn ein erheblicher Teil der Abnehmer der von der Marke beanspruchten Waren und Dienstleistungen diese Marke einem Unternehmen zuordnen.

Darüber hinaus kann auch durch eine notorische Bekanntheit der Marke Markenschutz entstehen (Notorietätsmarke, Nr. 3 MarkenG).

Den überwiegenden Teil der Marken in Deutschland stellen die Registermarken (Registermarke, Nr. 1 MarkenG) dar, da es eines erheblichen Aufwandes bedarf, Verkehrsgeltung oder gar notorische Bekanntheit für eine Marke zu erzielen. In der Regel sind Benutzungsmarken nur bei Waren oder Dienstleistungen anzutreffen, die ein sehr kleines, spezielles Publikum ansprechen, beispielsweise im Spezialmaschinenbau. Die Registermarke, auf die im Folgenden eingegangen wird, ist ein förmliches, absolutes Immaterialgüterrecht.

Vor der Markeneintragung sind vom Anmelder einige Punkte zu beachten, damit im Anschluss zum einen der Markenschutz überhaupt gewährt wird und zum anderen im Falle der Eintragung optimaler Markenschutz besteht. Häufig unterschätzen Unternehmen diese Tatsache und melden ihre neue Marke aus Unwissenheit ohne sorgfältige Vorbereitung beim entsprechenden Amt an. Entsprechend groß ist dann die Überraschung, wenn es zu Widersprüchen von Markeninhabern bereits bestehender und damit prioritätsälterer Marken kommt. Die Wahrscheinlichkeit von Übereinstimmungen von Silben oder ganzer Wortbestandteile neuer Namen mit bereits registrierten Kennzeichen steigt von Tag zu Tag. Im markenrechtlichen Sinne spricht man in diesem Fall dann von Kollisionen, die weitreichende Konsequenzen haben können.

Zudem gehört es zum modernen Markenmanagement, den Markenauftritt frühzeitig zu planen und zu steuern. Störungen oder Verzögerungen können den Erfolg einer neuen Marke entscheidend beeinflussen und darüber hinaus in Falle von Verletzungen bestehender Markenrechte zu teuren Rechtsstreitigkeiten führen. Deshalb wird häufig empfohlen, vor Markenanmeldung folgende Fragen zu klären:

Die Markenämter prüfen solche Fragen nicht. Es obliegt dem Markenanmelder selbst, dafür Sorge zu tragen, dass die neue Marke keine Rechte anderer Marken verletzt. Deshalb werden vor einer Anmeldung professionelle Markenrecherchen durch Patent- oder Markenanwälte und durch spezialisierte Rechercheunternehmen empfohlen.

Eigenrecherchen oder „In-House-Recherchen“ über gängige Suchmaschinen können der erste Schritt sein. Im nächsten Schritt müssen die entsprechenden Datenbanken durchsucht werden. Die Recherche- und Suchstrategie und die damit verbundene Qualität der Ergebnisse ist abhängig von den verwendeten Datenquellen sowie der praktischen Erfahrung der Rechercheure. Ermittelt die Markenrecherche keine relevanten, bestehenden Marken, so gibt das dem Anmelder die notwendige Sicherheit, weiter mit der neuen Marke agieren zu können.

Allgemein wird bei Marken zwischen zwei Recherchearten unterschieden:

Diese Rechercheart klärt, ob die neue Marke in identischer Form für die gleichen oder ähnlichen Waren oder Dienstleistungen schon von Mitbewerbern angemeldet und vom entsprechenden Markenamt registriert wurde. Identität bedeutet in diesem Fall absolute Identität, das heißt, "Coca Cola" ist gleich Coca Cola und nur Coca Cola und nicht Koka Kola.

Ähnlichkeitsrecherchen prüfen, ob die neue Marke bei den relevanten Markenämtern in ähnlicher Form für gleiche oder ähnliche Waren oder Dienstleistungen schon angemeldet wurde und entsprechender Markenschutz besteht. Ähnlichkeit bedeutet in diesem Fall optische oder klangliche Ähnlichkeit, z. B. "adidas" ist ähnlich "abibas" und "adi dash". Bei der Ähnlichkeitsrecherche wird im Normalfall auch die Identität geprüft. Professionelle Ähnlichkeitsrecherchen nutzen entsprechende Algorithmen, um schriftbildliche und phonetische Ähnlichkeiten mit älteren Schutzrechten zu ermitteln, und gehen damit über die Möglichkeiten der von den Markenämtern zur Verfügung gestellten Datenbanken hinaus.

Bevor mit einer Recherche begonnen wird, müssen folgende Fragen geklärt werden:

Die Rechercheart ergibt sich in erster Linie aus der neuen Marke oder dem Begriff selbst. Grundsätzlich gilt, je beschreibender der Begriff ist (Online-Lexikon), desto mehr wird die Identitätsrecherche angeraten. Je generischer der Begriff ist (Wikipedia), desto sinnvoller ist die Ähnlichkeitsrecherche.

Die Länder und Wirtschaftsräume, in denen die neue Marke später benutzt werden soll, determinieren die abzuprüfenden Datenbanken und -quellen. Soll eine Marke ausschließlich auf dem deutschen Markt benutzt werden (wird also auch in der Zukunft keine Internationalisierung der Marke angestrebt) genügt es, nur in den Markenbeständen des DPMA zu recherchieren.

Für die Eintragung von Marken ist es auch wichtig, für welche Waren und Dienstleistungen der Markenschutz erlangt werden soll. Hierfür dient die internationale Markenklassifikation (Nizza-Klassifikation), die die verschiedenen Waren- und Dienstleistungsarten in insgesamt 45 Klassen untergliedert. Das Angebot, das mit der neuen Marke benannt werden soll, bestimmt insofern die Klassen, für die der Schutz erlangt werden soll. Auch hier sollte man die strategische Ausrichtung der Marke, inkl. Lizenzierungen, beachten.

Nach Klärung dieser Fragen ist zu entscheiden, ob man die Recherche selbst durchführt oder ein Rechercheunternehmen beauftragt. Bei dieser Entscheidung kann die Homepage des Deutschen Patent- und Markenamtes (DPMA) helfen. Ein erfahrener Markenfachmann kann aufgrund seiner Ausbildung und Tätigkeit besser in der Lage sein, Kollisionen der neuen Marke mit bestehenden Marken festzustellen und Auskünfte zu erteilen oder Maßnahmen zu ergreifen.

Werden keine Kollisionsgefahren gesehen, dann steht dem Eintragungsverfahren zunächst nichts mehr entgegen.

Grundlage zur Erlangung des Markenschutzes durch eine Registermarke ist ein Antrag, der beim Deutschen Patent- und Markenamt (DPMA) einzureichen ist. Dieser Antrag muss Angaben enthalten, die es erlauben, die Identität des Anmelders festzustellen, sowie eine Wiedergabe der Marke und eine Angabe der Waren oder Dienstleistungen, für die die Eintragung beantragt wird ( Abs. 2 MarkenG, siehe Markenklassifikation). Ist der Antrag mit den genannten Angaben beim DPMA eingegangen, so wird ein sogenannter Anmeldetag begründet ( Abs. 1 MarkenG). Dieser Anmeldetag bestimmt den Zeitrang der Anmeldung. Dieser Antrag kann zwar zunächst formlos eingereicht werden, jedoch verlangt das DPMA die Verwendung des entsprechenden Formulars, das nachgereicht werden kann ( Abs. 1 MarkenG). Für den Zeitrang kommt es jedoch auf den Antrag, nicht auf das Formular an. Steht der Anmeldetag der Marke fest, so wird diese Anmeldung veröffentlicht (§ 33 Abs. 3 MarkenG).

Anschließend beginnt die Prüfung auf „absolute Schutzhindernisse“ beim DPMA. Diese absoluten Schutzhindernisse sind in , und MarkenG definiert. Im Wesentlichen wird hierbei geprüft, ob das als Marke angemeldete Zeichen überhaupt geeignet ist, Waren oder Dienstleistungen eines Unternehmens von denjenigen anderer Unternehmen zu unterscheiden (Unterscheidungseignung, § 3 MarkenG), ob das Zeichen grafisch darstellbar ist (§ 8 Abs. 1 MarkenG), ob das Zeichen für die beanspruchten Waren oder Dienstleistungen unterscheidungskräftig ist (§ 8 Abs. 2 Nr. 1 MarkenG), sowie ob ein Freihalteinteresse an dem Zeichen für Mitbewerber besteht (§ 8 Abs. 2 Nr. 2 MarkenG).

Darüber hinaus darf die Marke nicht ausschließlich aus Zeichen oder Angaben bestehen, die im allgemeinen Sprachgebrauch zur Bezeichnung der Waren oder Dienstleistungen üblich geworden sind (§ 8 Abs. 2 Nr. 3 MarkenG). Außerdem werden Marken nicht eingetragen, die zu einer notorisch bekannten Marke mit älterem Zeitrang ähnlich oder mit ihr identisch sind. Führt diese Prüfung zu keinen Beanstandungen, wird die Marke in das Markenregister eingetragen. Die Gebühr für eine Markeneintragung beträgt in Deutschland 300 Euro, nicht zu verwechseln mit der Verlängerungsgebühr, siehe Schutzdauer.
Ist eine Marke im Markenregister eingetragen, darf sie vom Markeninhaber mit dem Symbol ® (von englisch "registered trade mark" = eingetragene Waren- oder Dienstleistungsmarke) neben der Marke gekennzeichnet werden. Es besteht aber keine Pflicht zu dieser Kennzeichnung.

Auf internationaler Ebene existieren seit 1883 die Pariser Verbandsübereinkunft (PVÜ), in der wesentlich einheitliche Regeln für Patente und Handelsmarken vereinbart sind und seit 1891 das Madrider Abkommen (MMA), das Vereinbarungen über die internationale Registrierung von nationalen Marken trifft.
Nach dem sogenannten Madrider System, benannt nach dem Madrider Abkommen und dem Protokoll zum Madrider Abkommen, können international registrierte Marken (IR-Marken) erlangt werden. Die dafür zuständige Weltorganisation für Geistiges Eigentum (WIPO) in Genf erteilt dabei ein Bündel von IR-Marken, die in ihrem Schutzumfang den nationalen Marken gleichstehen.

Durch die europäische Gemeinschaftsmarkenverordnung der Mitgliedsstaaten der EU wurde das Harmonisierungsamt für den Binnenmarkt in Alicante gegründet. Dieses ist für die Erteilung von Gemeinschaftsgeschmacksmustern und Gemeinschaftsmarken (EU-Marken) zuständig, die in allen (derzeit 28) Mitgliedstaaten der Europäischen Union gelten. Der administrative Aufwand einer Anmeldung ist vergleichbar mit der nationalen Eintragung.

Als eine weitere zwischenstaatliche, aber EU-unabhängige Organisation von 38 Staaten des Kontinents einschließlich der Türkei hat für Einträge und europäisches Markenrecht das Europäische Patentamt – wie das regionale deutsche – seinen Sitz in München.

Im Gegensatz zum mitteleuropäischen, deutschsprachigen Raum gibt es im Angloamerikanischen eine Vorstufe mit Rechtsstatus der Eintragung. Zum einen für Waren die Unregistered Trademark (TM) und für Dienstleistungen die Service Mark (SM). Der gleichwertige Eintrag in beiden Hemisphären von Marken ist dann die Registered Trademark, deren Zeichen das „R“ im Kreis ist (→ ®).

Ein weiterer Unterschied besteht aber auch bei den bearbeitenden Institutionen. Beide Einrichtungen, im Vereinigten Königreich das Intellectual Property Office (United Kingdom) (IPO), im südwalisischen Newport ansässig, sowie in den USA das United States Patent and Trademark Office (PTO oder USPTO) in Alexandria im Staat Virginia, haben zusätzlich den Schutz Geistigen Urheberrechts zur Aufgabe.

Ähnlich wie in angloamerikanischen Institutionen erfolgt in der Russischen Föderation die Eintragung von Markenrechten zusammen mit Patenten und dem Geistigen Urheberrecht. Die betreffende Registratur ist hier der „Föderative Dienst Geistiger Urheberschaft“ (Роспатент-RussPatent). Auch die vergebenen Bezeichnungen entsprechen den drei angloamerikanischen Äquivalenten TM, SM und ®.

Eine Markenüberwachung ist die systematische und permanente Beobachtung der relevanten Markenregister nach möglichen Kollisionsmarken. Sie ist nach erfolgreicher Eintragung empfehlenswert, um die Marke gegen mögliche Gefahren verteidigen zu können.

Seit dem Jahr 2000 wurden allein in Deutschland jedes Jahr im Schnitt knapp 70.000 neue Marken beim DPMA angemeldet. Somit ist die Wahrscheinlichkeit recht hoch, dass neu angemeldete Marken gegen Schutzrechte anderer bereits registrierter und damit prioritätsälterer Marken verstoßen. Hinzukommt, dass das Amt nicht prüft, ob es bereits gleiche oder ähnliche Marken anderer Markeninhaber in gleichen oder ähnlichen Waren- und Dienstleistungsklassen gibt.

Diese bewusste oder unbewusste Identität oder Ähnlichkeit zu bestehenden Marken führt zu Bedrohungen wie Nachahmung, Markenpiraterie, Verballhornung, Verunglimpfung, Bootlegging, Verwässerung oder Verwechslung. Der Markeninhaber der bestehenden, älteren Marke erfährt von der Bedrohung im Regelfall erst dann, wenn es zu spät ist, das heißt, wenn sich die neue Marke am Markt auf Kosten der alten Marke positioniert und etabliert hat. Die Verteidigung der Rechte der alten Marke ist dann gegebenenfalls schwierig und kostenintensiv.

Um der Schwächung der eigenen Marke frühzeitig begegnen zu können, bieten spezialisierte Dienstleistungsunternehmen sogenannte Markenüberwachungen an. Deren Gegenstand ist die Marke in ihrer registrierten Form. Bei Wortmarken sind dies der oder die Wortbestandteil(e) oder Silben, bei Wort-Bild-Marken entsprechend die Wort- und Bildbestandteile und bei Bildmarken die Grafik oder die bildliche Darstellung. Dies gilt analog für Farb- und Hörmarken. Die Marke wird üblicherweise in allen Markenregistern, in denen sie angemeldet ist und in allen Waren- und Dienstleistungsklassen überwacht.

Die Überwachungsunternehmen kontrollieren in regelmäßigen Abständen die entsprechenden Markenneuanmeldungen in den relevanten Registern und Klassen auf sogenannte Kollisionen. Das heißt, es werden systematisch zur Überwachungsmarke identische und optisch, akustisch oder konzeptionell ähnliche Marken ermittelt.

Die Überwachungsergebnisse bekommt der Auftraggeber (Unternehmen, Anwalt oder Kanzlei) in Form sogenannter Kollisionshinweise oder Überwachungsberichte mitgeteilt. Diese enthalten in der Regel den Namen der Kollisionsmarke, Darstellung der Marke, Register, Waren- und Dienstleistungsklassen, Inhaber und Vertreter der Kollisionsmarke sowie Informationen zu den Widerspruchsfristen.

Der Kollisionshinweise und der Überwachungsbericht ist für den Markeninhaber oder seinen Anwalt dann die Entscheidungsbasis, ob er gegen die Kollisionsmarke vorgehen wird und welche Maßnahmen er ergreift.

Nachdem die Eintragung der Marke im Markenblatt veröffentlicht worden ist, können Inhaber von eingetragenen Marken, Benutzungsmarken oder geschäftlichen Bezeichnungen (Unternehmenskennzeichen und Werktitel) mit älterem Zeitrang innerhalb von drei Monaten Widerspruch gegen die Eintragung der jüngeren Marke erheben ( MarkenG). Der Widerspruch ist schriftlich zu erheben, muss aber nicht begründet werden. Notwendig ist lediglich, dass die angegriffene Marke, das Widerspruchskennzeichen (also die Marke oder die geschäftliche Bezeichnung mit älterem Zeitrang) sowie die Identität des Widersprechenden für das DPMA feststellbar sind. Diese Angaben können nach Ablauf der Dreimonatsfrist nicht nachgereicht werden. Auch hierzu soll ein Formblatt des DPMA verwendet werden ( Abs. 2, MarkenV).

Im Widerspruchsverfahren, das ein kursorisches, auf eine Vielzahl von Fällen ausgerichtetes, standardisiertes Verfahren ist, wird nun geprüft, inwieweit Identität ( Abs. 1 Nr. 1 MarkenG) oder Verwechslungsgefahr ( Abs. 1 Nr. 2 MarkenG) zwischen der jüngeren Marke und dem älteren Widerspruchskennzeichen besteht. Die Prüfung der Verwechslungsgefahr erfolgt für jede Ware oder Dienstleistung einzeln. Je ähnlicher die einzelnen Waren oder Dienstleistungen einander sind, desto unähnlicher müssen die Marken oder Kennzeichen sein und umgekehrt. Außerdem wird die Kennzeichnungskraft des Widerspruchskennzeichens, also seine Fähigkeit, bei identischen Waren oder Dienstleistungen vom Verkehr wiedererkannt zu werden, berücksichtigt. Als Folge daraus können Teile des Waren- und Dienstleistungsverzeichnisses der jüngeren Marke oder die gesamte jüngere Marke gelöscht werden.

Ein Widerspruch kann darauf gestützt werden, dass ein älteres, identisches oder verwechslungsfähiges Kennzeichen oder eine notorisch bekannte Marke entgegensteht (=relative Schutzhindernisse) oder dass die Marke für einen Agenten oder Vertreter des Markeninhabers angemeldet wurde. Wenn der Widerspruch auf eine ältere eingetragene Marke gestützt ist, so muss diese gemäß MarkenG auch rechtserhaltend benutzt worden sein, sofern der Inhaber der jüngeren Marke dies bestreitet, es sei denn, zwischen der Eintragung der älteren Marke und der Veröffentlichung der Eintragung der jüngeren Marke oder der Entscheidung über den Widerspruch liegt nur ein Zeitraum von weniger als fünf Jahren (Benutzungsschonfrist, Abs. 1 MarkenG).

Ferner kann die Marke nach ihrer Eintragung jederzeit mittels eines Löschungsverfahrens aus dem Register gelöscht werden. Löschungsgründe sind Verfall ( MarkenG), absolute Schutzhindernisse ( MarkenG), das heißt, die Marke selbst eignet sich nicht als Herkunftshinweis nach MarkenG oder relative Schutzhindernisse ( MarkenG), also ältere Marken oder Kennzeichen. Beim DPMA kann die Löschung wegen Verfalls oder absoluter Schutzhindernisse beantragt werden. Bei den Zivilgerichten kann Klage auf Löschung wegen Verfalls oder Bestehens älterer Rechte erhoben werden .

Der Inhaber der jüngeren Marke kann im Widerspruchsverfahren auch die Einrede der Nichtbenutzung erheben.

Grundsätzlich muss eine Marke rechtserhaltend benutzt werden, ansonsten kann sie wegen Verfalls nach MarkenG gelöscht werden. Aus einer für die betreffenden Waren oder Dienstleistungen nicht benutzten Marke können keine Rechte mehr geltend gemacht werden. In den ersten fünf Jahren nach Eintragung ist jedoch keine Benutzung zur Durchsetzung des Markenrechts erforderlich, vgl. MarkenG. Die Entfernung unbenutzter Marken aus dem Register ist sinnvoll, da geschützte Zeichen, wenn sie nicht mehr benötigt werden, der Allgemeinheit wieder zur Verfügung stehen sollen. Im Gegensatz zum Patent, das eine erfinderische Leistung belohnt, ist eine solche Leistung bei einer Marke nicht zu erkennen. Als Konsequenz daraus hat der Gesetzgeber die Benutzungspflicht vorgesehen, die jedoch nicht für junge Marken innerhalb der Benutzungsschonfrist gilt.

Wird im Löschungsverfahren die Benutzung nur für einige, jedoch nicht für alle Waren oder Dienstleistungen nachgewiesen, dann wird die Marke für die nicht benutzten Waren oder Dienstleistungen gelöscht und verbleibt mit denjenigen, für die die Benutzung nachgewiesen wurde. Somit verbleibt die Marke zwar im Register – schließlich kann das DPMA die Benutzung einer Marke nicht laufend kontrollieren – aber derjenige, der im Verletzungsverfahren aus der Marke in Anspruch genommen wird, kann die Einrede der Nichtbenutzung erheben, wodurch der Inhaber der älteren Widerspruchsmarke keine Rechte mehr geltend machen kann.

Wird im Widerspruchsverfahren vom Inhaber der jüngeren Marke die Einrede der Nichtbenutzung erhoben, so kann freilich der Widersprechende die Benutzung seiner Marke glaubhaft machen. Der Widerspruch hat dann nur Erfolg, wenn dem Widersprechenden die Glaubhaftmachung der Benutzung gelingt, was sich im Einzelfall als schwierig herausstellen kann. In der Praxis ist es daher für einen Markeninhaber überaus sinnvoll, alte Verpackungen seiner Waren sowie die dazugehörigen Rechnungen und Lieferscheine – auch über die gesetzlichen, insbesondere steuerrechtlichen Aufbewahrungsfristen hinaus – aufzubewahren. Es muss eine direkte erkennbare Verknüpfung der Marke mit der Ware oder Dienstleistung zu erkennen sein, beispielsweise als sichtbare Aufschrift oder als konkrete Produktbeschreibung innerhalb einer Rechnung. Der gemeinsame Abdruck der Ware mit dem Markennamen in einer Broschüre ohne direkte Verknüpfung bietet jedoch Schwierigkeiten bei der Glaubhaftmachung. Die Benutzung muss klar auf Zeitabschnitte und Regionen bezogen sein; ein Nachweis, europaweit in den letzten vier Jahren x Stück verkauft zu haben ist zu unpräzise. Zur Glaubhaftmachung sind auch eidesstattliche Versicherungen zugelassen.

In der Praxis ergeben sich Grundsätze, die im Folgenden anhand von Beispielen nicht abschließend dargestellt sind.


Der Markeninhaber hat gegen einen Verletzer einen Unterlassungsanspruch gegen die kennzeichenmäßige Benutzung der Marke, sofern das verwendete Zeichen mit der eingetragenen Marke identisch oder verwechselbar ähnlich ist und das Zeichen für identische oder ähnliche Dienstleistungen oder Waren verwendet wird, für die die Marke eingetragen ist (umgangssprachlich „Markenpiraterie“). Vom Verletzer kann Unterlassung, Beseitigung durch Vernichtung der widerrechtlich mit der Marke versehenen Waren oder zumindest Entfernung der Marke, Auskunft über den Umfang der Benutzung und Schadenersatz verlangt werden. Der Schadenersatz kann auf drei verschiedene Weisen berechnet werden: Der Markeninhaber kann Zahlung einer angemessenen Lizenzgebühr, Herausgabe des Verletzergewinns oder Ersatz der eigenen Mindereinnahmen verlangen. Auf Antrag (vgl. Strafantrag) des Verletzten wird auch eine strafrechtliche Verfolgung eingeleitet.

Wenn Rechte aus der eingetragenen Marke geltend gemacht werden, so muss diese auch benutzt sein, es sei denn, die Marke befindet sich noch in der Benutzungsschonfrist nach MarkenG von fünf Jahren, die nach Eintragung beginnt.

Gemäß Satz 1 MarkenG entsteht der Markenschutz durch die Eintragung eines Zeichens als Marke in das vom Patentamt geführte Register. Daher kann sich die Öffentlichkeit darüber informieren, welche Kennzeichen geschützt sind und welche nicht. Hierzu führt das Deutsche Patent- und Markenamt ein öffentlich zugängliches Register, in dem alle bibliografischen Daten, auch der Namen des Markeninhabers, hinterlegt sind.

Die Frage der Akteneinsicht bei Markenanmeldungen (also noch nicht eingetragenen Marken) wurde zuletzt in der BGH-Entscheidung vom 10. April 2007, I ZB 15/06 behandelt. Demnach wird zwischen dem berechtigten Interesse des Antragstellers und dem Geheimhaltungsinteresse (darunter fällt auch das Recht auf informationelle Selbstbestimmung) des Anmelders abgewogen. Der Markenanmelder hat die Gründe darzulegen, die gegen eine Akteneinsicht sprechen.

Die Schutzdauer ist grundsätzlich nicht beschränkt. In Deutschland, wie auch in Österreich und der Schweiz – hier erst seit dem 1. April 1993, alle davor angemeldeten Marken haben eine 20-jährige Schutzdauer – beträgt sie zwar ab dem Tag der Anmeldung zehn Jahre, kann aber gegen fristgerechte Zahlung einer Gebühr (in Deutschland 750 Euro) beliebig oft um weitere zehn Jahre verlängert werden. Der Markenschutz erlischt daher nur durch Nichtzahlung der Gebühr, durch Nichtbenutzung oder durch ein „Verkommen“ der Marke zum Gattungsbegriff.

Der Bundesgerichtshof hat in der Entscheidung vom 21. Juli 2005 (Az. I ZR 293/02) über verschiedene eingetragene Wort- und Wort-/Bildmarken des Handelskonzerns Otto entschieden und das Unternehmen schließlich zur Löschung wegen Nichtbenutzung verurteilt. Hier waren die Marken lediglich auf Katalogen und Versandtaschen angebracht, nicht aber auf der Ware selbst. Der Bundesgerichtshof hat darin keine geeignete rechtserhaltende Benutzung der Marke gesehen.

Das Recht an einer Marke kann gemäß MarkenG auf einen anderen Inhaber übertragen werden oder im Wege der Rechtsnachfolge übergehen. Die Änderung der Inhaberschaft kann sich auf alle angemeldeten Waren und Dienstleistungen oder auch nur auf Teile davon beziehen. Die Eintragung des Inhaberwechsels beim Deutschen Patent- und Markenamt verursacht keine Gebühren. Wenn nur Teile übertragen werden, ist jedoch eine Gebühr in Höhe von 300 Euro zu zahlen.





</doc>
<doc id="3403" url="https://de.wikipedia.org/wiki?curid=3403" title="Moving Picture Experts Group">
Moving Picture Experts Group

Die Moving Picture Experts Group (MPEG, engl. "„Expertengruppe für bewegte Bilder“") [] ist eine Gruppe von Experten, die sich mit der Standardisierung von Videokompression und den dazugehörenden Bereichen, wie Audiodatenkompression oder Containerformaten, beschäftigt. Umgangssprachlich wird mit „MPEG“ meistens nicht die Expertengruppe, sondern ein spezieller MPEG-Standard bezeichnet.

Drei- oder viermal jährlich kommt die MPEG in fünftägigen Treffen zusammen. Etwa 350 Experten aus 200 Unternehmen und Organisationen aus 20 Ländern nehmen an diesen Treffen, den MPEG-Meetings, teil. Die Gruppe wurde von Leonardo Chiariglione, damals Vizepräsident der Media Group des italienischen Forschungszentrums CSELT, gegründet. Chiariglione ist immer noch der Präsident.

Die offizielle Bezeichnung für MPEG ist ISO/IEC JTC1/SC29/WG11 (International Organization for Standardization/International Electrotechnical Commission, Joint Technical Committee 1, Subcommittee 29, Working Group 11). Die Standards werden mit der Internationalen Fernmeldeunion (ITU) abgeglichen und größtenteils in gemeinsamen Arbeitsgruppen entwickelt. Prominentestes Beispiel ist der MPEG-4 AVC Standard, der im Wortlaut identisch als ITU-T H.264 verabschiedet wurde.

Die Gruppe hat folgende Komprimierungsformate zu einem ISO-Standard geführt. H.26x ist die Benennung als ITU-Standard und entstanden aus der Zusammenarbeit mit MPEG:
MPEG standardisiert nur den Bitstream (Abfolge der Bits) und den Dekodierer (als sogenannte Terminal-Architektur). Der Kodierer wird nicht standardisiert, so dass Raum für Effizienzsteigerungen bleibt. Es werden Musterimplementierungen ("verification models") vorgeschlagen, die aber weder besonders schnell noch besonders effizient sind, da sie lediglich die Machbarkeit zeigen. Daher schreiben kommerzielle Anbieter die Implementierungen von MPEG-Kodierern von Grund auf neu, um entweder effizientere, qualitativ bessere Umsetzung des Originalmaterials in den codierten Datenstrom oder eine schnellere Implementierung zu erreichen.

Die MPEG spezifiziert sowohl Containerformate als auch Codecs. Dadurch können beispielsweise in MPEG-2 kodierte Videospuren auch in (technisch allerdings unterlegenen) AVI-Containern abgelegt werden und nicht nur in hauseigenen MPEG-Containern.

Üblicherweise ist eine MPEG-Norm in mehrere Teile aufgeteilt (bspw. ISO-IEC 14496: MPEG-4).




</doc>
<doc id="3404" url="https://de.wikipedia.org/wiki?curid=3404" title="Makroökonomie">
Makroökonomie

Die Makroökonomie (von griechisch "μακρός" "makros" „groß“; "οἶκος", "oíkos" „Haus“ und "νόμος", "nomos" „Gesetz“), auch "Makroökonomik", "makroökonomische Theorie" oder "Makrotheorie", ist ein Teil der Volkswirtschaftslehre.
Die Makroökonomie ist eine Wissenschaft, die sich mit dem gesamtwirtschaftlichen Verhalten der Wirtschaftssektoren, mit der Analyse der gesamtwirtschaftlichen Märkte und deren Zusammenhängen, befasst.

Der Begriff Makroökonomik wird in der Literatur überwiegend synonym verwendet. Manche Autoren unterscheiden jedoch zwischen der Makroökonomik als der Wissenschaft und der Makroökonomie als deren Untersuchungsgegenstand.

Die Makroökonomie ist die Wissenschaft der gesamtwirtschaftlichen Vorgänge. Das Tableau économique von François Quesnay ist das erste moderne makroökonomische Totalmodell. Quesnay modellierte einen einfachen Wirtschaftskreislauf. Die Wirtschaftsbeziehungen dreier Klassen bestehend aus den Bauern, den Kaufleuten und Handwerkern und den Grundeigentümern wurden erklärt.

Die Makroökonomie versucht, die wesentlichen Bestimmungsgründe, die internationalen Unterschiede und die zeitliche Entwicklung makroökonomischer (gesamtwirtschaftlicher) Schlüsselvariablen, wie zum Beispiel gesamtwirtschaftliche Produktion von Gütern und Dienstleistungen, Gesamteinkommen, Arbeitslosigkeit, Inflation und Zahlungsbilanz, zu erklären. Als Begründer der Makroökonomie gilt John Maynard Keynes, der 1936 die erste "simultane" Analyse der makroökonomischen Schlüsselvariablen vorlegte.

Wichtige Teilgebiete der makroökonomischen Theorie sind die Volkswirtschaftliche Gesamtrechnung, die Einkommens- und Beschäftigungstheorie, die Wachstumstheorie und die Konjunkturtheorie. Die Trennung zwischen Mikroökonomie und Makroökonomie ist bisweilen problematisch. Viele Teilgebiete der Volkswirtschaftslehre, insbesondere die monetäre Theorie (Geldtheorie und Theorie der Geldpolitik), die Finanzwissenschaft, die Außenwirtschaftstheorie und die Verteilungstheorie, weisen Elemente der Mikroökonomie und der Makroökonomie auf.

Mittelpunkt makroökonomischer Theorien ist schließlich die Frage nach der Rolle des Staates im gesamtwirtschaftlichen Kontext; aus den Theorien werden Empfehlungen an die Wirtschaftspolitik abgeleitet. Regierungen versuchen die gesamtwirtschaftliche Entwicklung zu beeinflussen. So werden durch Änderungen bei Steuern, Zinsen oder Staatsausgaben politisch definierte Ziele wie Preisniveaustabilität, Vollbeschäftigung, außenwirtschaftliches Gleichgewicht und/oder Wirtschaftswachstum angestrebt (siehe auch Magisches Viereck). Makroökonomische Kenngrößen spielen im politischen Legitimationsprozess eine wichtige Rolle, da sie von den Wählern als Hinweis auf die Qualität der Arbeit einer Regierung gedeutet werden.

Moderne makroökonomische Theorien (DSGE-Modelle) leiten Entscheidungen meist aus individuellen Optimierungen her (sogenannte Mikrofundierung). Weil bloße Rationalitätsannahmen dem Sonnenschein-Mantel-Debreu-Theorem zufolge kaum Implikationen haben, ist die Makroökonomik stärker empirisch orientiert. Sie wählt die Annahmen so, dass die Modellergebnisse mit stilisierten Fakten in Einklang stehen. Hierzu gehören etwa die langfristige Konstanz des Kapitalkoeffizienten oder die Tatsache, dass Investitionen im Konjunkturverlauf stärker schwanken als der Konsum.

Die makroökonomische Analyse versucht die komplexe wirtschaftliche Wirklichkeit auf eine überschaubare Anzahl wesentlicher Zusammenhänge zu vereinfachen. In der Regel wird dabei die Anzahl der betrachteten Märkte auf vier reduziert:


Auf dem Gütermarkt wird das homogene Inlandsprodukt gehandelt, das fiktiv in den privaten Konsum, den Staatskonsum, die Investitionen sowie die Importe und Exporte aufgespalten wird.

In der Makroökonomie existieren verschiedene Erklärungsansätze. Es lässt sich indessen rechtfertigen, letztlich nur von zwei Erklärungsmustern (Paradigmen) zu sprechen. Auf der einen Seite steht die klassische Makroökonomik, die durch den Monetarismus und die Neue Klassische Makroökonomik neu begründet und verfeinert wurde. Auf der anderen Seite steht der Keynesianismus.

Die Volkswirtschaftslehre unterscheidet sich in zwei große Hauptteile, die Mikroökonomie und die Makroökonomie. Diese lassen sich nicht exakt voneinander trennen, sondern ergänzen sich gegenseitig.

Mikroökonomie und Makroökonomie analysieren beide die Verhaltensweisen von Wirtschaftssubjekten. Im Mittelpunkt der Mikroökonomie steht dabei das einzelne Wirtschaftssubjekt, bei der Makroökonomie hingegen das Durchschnittsverhalten der Wirtschaftssubjekte.
Das heißt, dass in der Mikroökonomie zum Beispiel die Nachfrage eines einzelnen Haushaltes untersucht wird, während die Makroökonomie die aggregierte Nachfrage, also die gesamte Nachfrage aller Haushalte analysiert.
Hinzu kommt, dass in der Makroökonomie auch Bereiche in die Untersuchung mit einbezogen werden, die in der Mikroökonomie nicht berücksichtigt werden, wie zum Beispiel die Bedeutung des Staates oder des Auslandes.

Zusammenfassend ist festzustellen: Die Mikroökonomie befasst sich hauptsächlich mit einzelnen Märkten, also mit Märkten von bestimmten Gütern und deren Analyse.
Die Makroökonomie hingegen betrachtet die aggregierten Einzelmärkte wie den Gütermarkt oder den Geldmarkt und die daraus resultierenden wirtschaftlichen Gesamtzusammenhänge.
Dabei greift die Makroökonomie auf die in der Mikroökonomie untersuchten Einzelentscheidungen der Wirtschaftssubjekte zurück.

Erste Fragen zu ökonomischen Sachverhalten finden sich bereits in der Antike. So werden schon in „Politeia“ von Platon Aussagen zu den Vorteilen der Arbeitsteilung oder in den Erörterungen des Wertes von Aristoteles werden Aussagen zum Geld und zum Zins getroffen. Dies sind Gedanken, welche sicherlich bereits zu früherer Zeit aufkamen.

Das Besondere an den Griechen und Römern, den Scholastikern und Naturrechtsphilosophen ist allerdings, dass sie nicht die Volkswirtschaftslehre als solche in den Vordergrund stellten, sondern sie die ökonomischen Probleme immer im Zusammenhang mit einer anderen Wissenschaft betrachteten, beispielsweise der Ethik, der Rechtswissenschaft oder der politischen Philosophie. Eine eigentliche Wirtschaftswissenschaft liegt hier allerdings nicht vor. Jedoch sollte man die Mitarbeit dieser Autoren nicht unterschätzen, da ihr Beitrag einer der beiden Urquellen ökonomischer Forschung gleichwertig ist.

Eine andere Quelle ist das Werk diverser Autoren, welche sich mit praktischen und politischen Fragen der Wirtschaftsführung und auch der Verwaltung befassten. Zu diesen Autoren zählen unter anderem Lehrer der damaligen Verwaltungshochschulen, Bürokraten, Politiker und Geschäftsleute. Da sie Praxiserfahrung mitbrachten, war ihnen die analytische Darstellung ihrer Ideen weniger wichtig als das eingebrachte Tatsachenwissen. Im 16. und 17. Jahrhundert stieg die Zahl der Veröffentlichungen derart an, dass die Hauptaussagen der formulierten Gedanken den Namen Merkantilismus prägten. Hauptaufgabe des Merkantilismus war es, die nationale Handelskraft zu unterstützen und die Beschaffung von Einnahmen für die Schatzkammer der Fürsten (auch camera genannt). Daraus leitet sich die Bezeichnung Kameralismus ab, welche den deutschen Merkantilismus bezeichnet. Sie stellt einen Vorläufer der deutschen Finanzwissenschaft dar.

Eine erste wichtige Persönlichkeit in der Entstehung der ökonomischen Analyse ist Sir William Petty. Petty stellte die These des volkswirtschaftlichen „Überschusses“ und damit die ausschlaggebende Eigenschaft einer Klassischen Ökonomik.
Mitte des 18. Jahrhunderts schloss sich in Frankreich eine Gruppe von Autoren zusammen und gründeten eine Schule. Sie nannten sich „les économistes“, heute würde man sie als Physiokraten bezeichnen. Der Arzt Francois Quesnay war Gründer und Oberhaupt dieser Einrichtung. Er war der erste, der eine gesamte Analyse des Wirtschaftsprozesses aufstellte, die auch die Kreislauftheorie mit einbeschloss. Diese Leistung ist beachtenswert. Beeinflussung erhielten die Physiokraten von Richard Cantillon, der erstmals näher auf das Problem der Allokation einging und der zeigte, welchen Einfluss die Nachfrage über eine Änderung der relativen Preise auf die Zusammensetzung der volkswirtschaftlichen Produktion hat. Anne Robert Jacques Turgot, der eine enge Verbindung zu den Physiokraten besaß, erarbeitete zu dieser Zeit ebenfalls eine weitestgehend umfassende Wirtschaftstheorie und entwickelte außerdem Beiträge, aufgrund deren man ihn heute zu den herausragenden klassischen Ökonomen zählt.

Der Beginn der Klassik liegt im Jahr 1770, gehört also zu dem Jahrzehnt, in dem Turgots Hauptwerk und das wohl populärste ökonomische Buch, nämlich „An Inquiry into the Nature and Causes of the Wealth of Nations“ von Adam Smith, entstanden. Smith war Professor für Moralphilosophie an der Universität Glasgow und die Bedeutung seines Werkes kam einerseits von seinem eigenen analytischen Einsatz, zum größten Teil aber vom Festhalten der bereits vorhandenen Erkenntnisse. Durch sein Werk wurde die Volkswirtschaft als selbständiges Wissensgebiet anerkannt. Orientiert man sich an Joseph Schumpeters Aussage, dass die Erkenntnis einer Wissenschaft die Erkenntnis ihrer selbst sei, so war dies der Schritt, mit dem die Ökonomie eine eigenständige wissenschaftliche Disziplin wurde.

Die Klassik macht nur Sinn, wenn man sie im Kontext der ihr vorangegangenen Epochen betrachtet. Mit dem Werk von Smith und den Beiträgen der Vorläufer als Grundlage beschäftigten sich die Ökonomen der Klassik mit allen essentiellen Fragen, die den Inhalt der heutigen Volkswirtschaftslehre ausmachen.

Einen Kernpunkt der Klassischen Ökonomie stellt das Harmonieprinzip dar. Damit gemeint ist die Gewissheit der Funktionstüchtigkeit einer Marktwirtschaft, wie Smith sie mit seiner „invisible hand“ bildlich darstellt. Die „unsichtbare Hand“ in Form des Preismechanismus rief aufgrund der rationalen und individuellen Handlungsweise des Einzelnen eine Ordnung der Wirtschaftspläne hervor. Hintergrund des Ganzen ist eine natürliche Ordnung, die sich der gegebenen Ordnung möglichst anpassen sollte, um eine größtmögliche gesellschaftliche Wohlfahrt herzustellen. Sehr wichtig ist dabei, dass sich der Staat weitestgehend aus dem Wirtschaftsleben heraushält. Der Staat sollte lieber seinen zwei wichtigen Funktionen nachkommen, nämlich der inneren und äußeren Sicherheit sowie der Verwirklichung eines geeigneten Rechtssystems mit Handlungsfreiheit sowie Schutz des Privateigentums.

Das Hauptinteresse des Klassikers galt vielmehr den Aussagen, die bereits zum Überschuss getroffen wurden. Für sie waren die größten Probleme die Entstehung des Überschusses, wie er auf die unterschiedlichen Bevölkerungsschichten verteilt werden sollte und seine eventuelle Verwendung für „Luxuskonsum“ oder „Ersparnis“. Auf die Verwendung des Überschusses legten die Ökonomen besonderes Augenmerk. Sie entscheiden sich klar für die zweite Verwendungsmöglichkeit, da eine Erhöhung der Ersparnisse langfristig dem "Wohlstand der Nationen" dient und diesen erhöht.

Außergewöhnliche Volkswirte dieser Epoche sind neben Adam Smith vor allem Thomas Malthus, Jean Baptiste Say, David Ricardo und John Stuart Mill.

Die Idee einer „Klassischen“ Periode hat ihren Ursprung bei Karl Marx, der ebenso bedeutsam ist wie die Klassiker und John Stuart Mill als deren letzten Vertreter ansah. Das Ende der klassischen Epoche befindet sich, wenn man der Marxschen Einteilung folgt, im Jahr 1870 und ist gleichzeitig der Beginn der Neoklassik.

Der Marginalismus als Überschrift für Grenzbetrachtungen, zu denen unter anderem die Grenzkosten und der Grenznutzen zählen, gehört ohne jeden Zweifel zu den wichtigsten Innovationen der Neoklassik. Er ermöglichte es, das wirtschaftliche Verhalten auf einzelne Individuen zurückzuführen. Während der klassischen Epoche stand noch die makroökonomische Betrachtungsweise im Vordergrund, während in der Neoklassik ein universelles Individuum im Mittelpunkt der Analyse steht. Diese Sicht kann man als mikroökonomisch bezeichnen.

In der Werttheorie kam der neoklassische Marginalismus zum ersten Mal zum Ausdruck und zog außerdem noch eine weitere einschneidende Veränderung nach sich. War für die Klassiker noch der Wert eines Gutes gleich den Kosten der Produktion, so sind die frühen Neoklassiker der Meinung, dass derjenige Preis, den die Nachfrager zu zahlen bereit sind, also der Marktpreis, sich durch den Grenznutzen bestimmt. Die Wertlehre, welche hier Anwendung findet ist im Unterschied zur Klassik subjektiv geprägt.

Auch ist der Schwerpunkt, welcher im Mittelpunkt der Überlegungen steht, ein anderer als der in der klassischen Epoche. Die Neoklassik befasst sich hauptsächlich damit, wie knappe vorhandene Ressourcen verteilt werden. Diese Überlegungen führten zum Ergebnis, dass sich die Angebotsstruktur durch den relativen Preismechanismus der Nachfragestruktur angleicht und somit eine optimale Allokation entsteht.

Als Autoren, die fast zeitgleich und unabhängig voneinander die „marginalistische Revolution“ einleiteten, sind William Stanley Jevons, Carl Menger und Léon Walras zu nennen. Das Hauptthema ihrer Ausführungen zu diesem Thema ist der Grenznutzen. Durch das Anwenden der Grenznutzentheorie erreichten Jevons und Walras, dass heute gebräuchliche mathematische Anwendungen als Standard eingeführt wurden.

Alfred Marshall, Professor an der Universität Cambridge, war einer der außergewöhnlichsten Autoren der Neoklassik. Durch seine Aussagen zum objektiven und subjektiven Wert wurde die geometrische Darstellung von Angebots- und Nachfragefunktionen entwickelt. Die Angebotskurve stellt hierbei den objektiven, die Nachfragekurve den subjektiven Teil dar. Durch die Zusammenführung beider Funktionen kann man den Marktpreis und den natürlichen Preis ermitteln.
Weitere wichtige Autoren der Neoklassik sind Irving Fisher, Vilfredo Pareto, Knut Wicksell sowie Arthur Cecil Pigou.

Geschichtlich betrachtet liegt das Ende der Neoklassik zu Beginn des Ersten Weltkrieges. Analytisch betrachtet findet sich dafür kein Zeitpunkt, ein Ende ist nicht genau datiert. Die Neoklassische Art der Analyse wird auch heute noch sehr oft verwendet.

In der zweiten Dekade des 20. Jahrhunderts trat der Ökonom John Maynard Keynes, dessen Höhepunkt sein 1936 erschienenes Werk „General Theory of Employment, Interest and Money“ ist, in den Vordergrund. Keynes ist es zu verdanken, dass die Makroökonomie heute eine so große Bedeutung hat.

Die aufgrund von Keynes mehr in den Mittelpunkt gerückte Makroökonomie anstelle der Mikroökonomie ist mit der Änderung des Erklärungszieles verbunden. In der Keynschen Theorie wird hauptsächlich das Beschäftigungsproblem betrachtet. Keynes hat sich in seinen Ausführungen hauptsächlich mit der Beobachtung des Auslastungsgrades nicht vollbeschäftigter Produktionsfaktoren beschäftigt. Diese Änderung der Betrachtungsweise hängt mit zwei Faktoren zusammen. Zum einen mit der zur damaligen Zeit stattfindenden Weltwirtschaftskrise. Und zum zweiten mit der Umstellung von der langfristigen auf die kurzfristige Analyse. 

Eine weitere wesentliche Veränderung, welche durch Keynes ins Rollen kam, war die Spaltung der Wirtschaftstheorie. Neben der neoklassischen Analysetechnik rief Keynes eine zweite Art der Analyse ins Leben. Diese war allerdings so anders konzipiert, dass ein Vergleich undenkbar ist.

Unter einer geschlossenen Volkswirtschaft ist eine Volkswirtschaft ohne Handelsbeziehungen zum Ausland zu verstehen. Demnach sind Exporte und Importe gleich Null. Diese Annahme steht im Widerspruch zur Realität, denn alle modernen Volkswirtschaften haben zahlreiche und komplexe Handelsbeziehungen mit dem Rest der Welt.

Der Gütermarkt umfasst in der Makroökonomie alle Märkte, auf denen Waren und Dienstleistungen gehandelt werden. Auf ihm kommen das aggregierte Angebot und die aggregierte Nachfrage einer Volkswirtschaft zusammen. Der Gütermarkt beinhaltet somit sowohl den Konsum als auch die Investitionen.

Bei der grafischen Betrachtung des Gütermarktgleichgewichts trifft man auf die IS-Funktion. Diese stellt die Fülle aller Zusammenstellungen von Zinssatz und Volkseinkommen dar, bei denen ein Gleichgewicht besteht.

Unter einem Geldmarkt ist ein Ort zu verstehen, auf welchem alle geldlichen Transaktionen stattfinden. Dabei werden Einnahmen und Ausgaben saldiert und in einer bestimmten Geldhaltung niedergeschlagen. Um die gewünschte Nachfrage zu ermitteln muss man zwei Sichtweisen aufgreifen. Zum einen ergibt sich diese aus dem Bedürfnis nach sofortiger Abwicklung laufender Transaktionen. Diese sogenannte "Transaktionskasse" verhält sich proportional zum Einkommen. Je höher das Einkommen ist umso mehr Transaktionen können getätigt werden. Zweitens ist trotz positiver, aber niedriger Zinsen eine Geldhaltung aus dem Vermögensmotiv sinnvoll, wenn steigende Zinsen zu erwarten sind. Mit der zu erwartenden Zinssteigerung ist nämlich auch mit Kursverlusten bei Wertpapieren zu rechnen. Die daraus abgeleitete "Spekulationskasse" steigt also mit sinkendem Zins. Schlussfolgernd wird das Geldangebot durch die Kreditvergabe bzw. die Wertpapierkäufe der Notenbank bestimmt. Dies ist mittels der LM-Funktion grafisch abbildbar. Bei gegebener Geldmenge verläuft die Geldmarktgleichgewichtskurve LM mit positiver Steigung im Zins-Einkommens-Koordinatensystem.
Bei diesem Modell werden nun die beiden Teilmärkte "Gütermarkt" und "Geldmarkt" verbunden. Aus der Ableitung des Schnittpunkts der beiden Kurven ergibt sich das IS-LM-Modell.
Dieses Modell basiert auf dem makroökonomischen Fixpreismodell von John R. Hicks. Durch die Zusammensetzung von Geld- und Gütermarkt legt das IS-LM-Modell die Gleichgewichtswerte des Zinssatzes und des Volkseinkommens fest. Es eignet sich zur kurzfristigen Untersuchung der Globalsteuerung.

Auf dem Arbeitsmarkt treffen Angebot und Nachfrage nach Arbeitskraft zusammen. Dabei fungieren Arbeitnehmer als Anbieter von Arbeit. Die Unternehmen fragen diese dann nach. Für Sie stellt die Arbeit somit einen Produktionsfaktor dar. Demnach ist die Arbeitsnachfrage auch im Zusammenhang mit der Produktionsbedingung zu sehen, welche durch die volkswirtschaftliche Produktionsfunktion beschrieben werden.

In den vergangenen Jahren hatte der Arbeitsmarkt immer häufiger mit Wachstumsschwächen zu kämpfen. Sichtbar wurde dies vor allem durch den hohen Grad der Arbeitslosigkeit.

Wie schon aufgeführt, stellt das Güter-Geldmarktmodell den Zusammenschluss der beiden Teilmärkte dar. Die grafische Darstellung erfolgt durch das IS-LM-Modell. Wenn nun das Gleichgewicht auf dem Arbeitsmarkt hinzugezogen wird, gelangen wir zum AS-AD-Modell. Dieses geht von der Annahme aus, dass eine natürliche Arbeitslosenquote besteht, welche sich mittelfristig einstellt. Nur wenn tatsächliche und natürliche Arbeitslosigkeit übereinstimmen kommt es zum Gleichgewicht auf dem Arbeitsmarkt. Mit der AS-Kurve wird jetzt weiterführend das gesamtwirtschaftliche Angebot betrachtet. Es wird demzufolge nicht mehr – wie noch im IS-LM-Modell – von einem völlig preiselastischen Angebot (und damit konstanten Preisen) ausgegangen. Vielmehr berücksichtigt man nun mögliche Preisreaktionen und deren Folgen für das gesamtwirtschaftliche Gleichgewicht.

Bei der offenen Volkswirtschaft wird das Ausland mit einbezogen, d. h. Importe und Exporte spielen im Gegensatz zur geschlossenen Volkswirtschaft eine entscheidende Rolle.
Besondere Bedeutung haben hierbei die Außenbeziehungen einer Volkswirtschaft. Diese werden in der Außenwirtschaftsrechnung erfasst, welche eine Nebenrechnung der Volkswirtschaftlichen Gesamtrechnung (VGR) ist. Die wichtigsten Teile der Außenwirtschaftsrechnung sind die Zahlungsbilanz und die Erfassung der Wechselkurse.

Die makroökonomische Totalanalyse betrachtet alle Märkte (Güter-, Geld-, Wertpapier- und Arbeitsmarkt) im Zusammenhang. Sie umfasst alle Interaktionen zwischen Konsumenten und Produzenten auf den Märkten, also die Handlungen aller Wirtschaftssubjekte. Zur Vereinfachung werden dabei bestimmte fixe Daten und Verhaltensweisen verwendet und die Analyse wird auf die Preistheorie und das Streben in Richtung der Gleichgewichtszustände reduziert. Obwohl diese Analyseart theoretisch umfassender ist, werden aus Kostengründen oder aufgrund der aufwändigen Datenbeschaffung meist Partialmodelle eingesetzt.

Die Grundlage des makroökonomischen Gleichgewichts basiert auf der Annahme einer "unsichtbaren Hand" von Adam Smith. Demnach stellt sich langfristig gesehen in einem Marktsystem eine Gleichgewichtssituation ein.

Die Totalanalyse untersucht hierbei vor allem folgende Fragen:

Die Finanzpolitik beinhaltet alle Entscheidungen bezüglich der Budgets oder Haushaltspläne im Staat, vor allem die Festlegung der Höhe und Art der Einnahmen und Ausgaben.

Lohnpolitik umfasst alle Maßnahmen zur Gestaltung von Höhe, Struktur und Ausrichtung der Löhne.

Der vollkommene makroökonomische Arbeitsmarkt (neoklassischer Ansatz) ist teilweise unvereinbar mit der modernen volkswirtschaftlichen Realität. Freie Lohnbildung aus Nachfrage und Angebot ist in Teilen des Arbeitsmarktes möglich, aber meist werden die Lohnsätze zwischen den Gewerkschaften und den Unternehmerverbänden ausgehandelt und in Tarifverträgen festgehalten.

Die Festlegung von Steuern T und Staatsausgaben G (Erhöhung bzw. Senkung) durch die Regierung nennt man Fiskalpolitik. Hierzu zählen alle Entscheidungen zu Staatskäufen, Transferzahlungen und Steuerstruktur. Fiskalpolitik ist ein Teil der Finanzpolitik.

Soll ein staatliches Budgetdefizit abgebaut werden, indem der Staat die Steuern erhöht bzw. die Staatsausgaben senkt, so wird dies kontraktive Fiskalpolitik genannt. Wird dagegen das Staatsdefizit ausgeweitet durch eine Steuersenkung bzw. Erhöhung der Staatsausgaben, so wird dies als expansive Fiskalpolitik bezeichnet.

Es werden folgende makroökonomische Krisen unterschieden:

Inflation ist ein Anstieg des allgemeinen Preisniveaus über längere Zeit.

Deflation ist eine negative Inflation, das heißt, das allgemeine Preisniveau fällt.

Depression ist eine lang anhaltende Rezession.

Hyperinflation bezeichnet eine sehr hohe Inflation.

Als Rezession wird ein negatives BIP-Wachstum in zwei oder mehr aufeinander folgenden Quartalen bezeichnet (negative Wachstumsraten).

Als Stagnation wird eine konjunkturelle Phase ohne wirtschaftliches Wachstum bezeichnet.

Stagflation bezeichnet eine Kombination von Stagnation und Inflation.





</doc>
<doc id="3405" url="https://de.wikipedia.org/wiki?curid=3405" title="Maschinensprache">
Maschinensprache

Maschinensprache (auch Maschinencode oder nativer Code genannt) ist eine Programmiersprache, in der die Instruktionen, die vom Prozessor eines Computers direkt ausgeführt werden können, als Sprachelemente festgelegt sind – die „Programmiersprache eines Computers“. Die Menge und die formale Struktur/Syntax dieser auch „Anweisungen“ oder „Befehle“ genannten Instruktionen sind je Prozessortyp als Befehlssatz definiert.

Ein "Befehl" ist hierbei eine Anweisung an den Prozessor, eine bestimmte Operation durchzuführen, beispielsweise eine Addition oder einen Wertevergleich. Jede funktionelle Leistung eines Prozessors ist Ergebnis der Ausführung von "Maschinencode" bzw. eines "Maschinenprogramms", das heißt eines in Maschinensprache vorliegenden Programms.

"Abgrenzung zu anderen Programmiersprachen:" Programmcode in Maschinensprache wird heutzutage kaum mehr von Programmierern direkt erzeugt, sondern unter Nutzung einer höheren Programmiersprache oder einer Assemblersprache, wobei erst mithilfe eines Compilers und/oder Assemblers und/oder Linkers ausführbarer Maschinencode entsteht. Wird von „Programmierung in Maschinensprache“ gesprochen, ist heute üblicherweise die Programmierung in Assemblersprache gemeint. Aus den Anweisungen eines in einer Interpretersprache erstellten Programms erzeugt ein Interpreter programmintern die Maschinenbefehle, die vom Prozessor auszuführen sind.

"Alternative Bezeichnungen und Bedeutungen:" Im Sprachgebrauch werden Ausdrücke wie „Maschinencode, Maschinensprache, Binärcode, nativer Code, Programmcode usw.“ häufig synonym benutzt (Beispiel siehe). Dabei stehen sie je nach Kontext für zwei unterschiedliche Bedeutungen:

Maschinenprogramme finden in allen Geräten mit einem Prozessor Verwendung, also von Großrechnern über Personal Computer und Smartphones bis hin zu eingebetteten (embedded) Systemen in modernen Waschmaschinen, Radios oder Steuerungen im Kraftfahrzeug für ABS oder Airbag. Auf PCs stehen sie unter anderem in ausführbaren Dateien. Zusätzlich gibt es eine BIOS oder UEFI genannte Firmware, die den Bootvorgang einleitet und einige wichtige Funktionen fürs Betriebssystem bereitstellt.

Maschinenprogramme findet man beispielsweise auf Windows-PCs in Dateien mit der Dateinamenserweiterung „.exe“. Unter vielen anderen Betriebssystemen werden ausführbare Dateien auch ohne besondere Endung und meist in speziellen Programmbibliotheken geführt und anders bezeichnet, z. B. unter z/OS als Lademodul. Bei vielen eingebetteten (embedded) Systemen oder Mikrocontrollern befinden sich das Maschinenprogramm oder Teile davon, z. B. ein Bootloader, permanent im ROM.

Maschinenprogramme in einer Datei können mit einem Hex-Editor betrachtet, theoretisch auch verändert oder sogar erstellt werden. Üblicherweise erfolgt beides jedoch über den Quelltext des Programms, der programmiersprachenabhängig von einem Assembler oder Compiler in ein Maschinenprogramm übersetzt wird. Maschinencode kann durch einen Disassembler wieder in Assemblerformat rückübersetzt werden, die Umwandlung in eine höhere Programmiersprache durch einen Decompiler unterliegt jedoch starken Einschränkungen.

Das Programm im "Maschinencode" besteht aus einer Folge von Bytes, die sowohl Befehle als auch Daten repräsentieren. Da dieser Code für den Menschen schwer lesbar ist, werden in der Assemblersprache die Befehle durch besser verständliche Abkürzungen, sogenannte Mnemonics, dargestellt. Dabei können der Operationscode, Quell- und Zielfelder sowie andere Angaben in den Befehlen mit symbolischen Bezeichnern (wie MOVE, PLZ, LAENGE) notiert werden, ggf. ergänzt um numerische Zahlenwerte, z. B. für eine individuelle Längenangabe, Registernummern usw.


Die meisten der vorgenannten, zur Assemblersprache genannten Aspekte gelten in ähnlicher Weise auch für höhere Programmiersprachen – wobei diese sich gegenüber der Assemblersprache durch weitere (Leistungs-)Merkmale unterscheiden.

Intern ist jeder Befehl der Maschinensprache durch einen oder mehrere Zahlenwerte kodiert. Diese Zahlenwerte bestehen aus dem Opcode, der die Art des Befehls festlegt, eventuell gefolgt von einem oder mehreren Bytes an Daten zu diesem Befehl. Eine sinnvolle Folge von solchen Zahlencodes im Hauptspeicher bzw. als Datei gespeichert bildet demnach ein Programm. Es gibt nun verschiedene Arten, solche Programme zu erstellen:

Gegeben sei das folgende Programm in der Programmiersprache C, das die Summe der Zahlen "a"=2 und "b"=3 berechnet und das Ergebnis "c" an den Aufrufer zurückliefert:
int main() {

Das Kompilieren dieses Programms kann folgenden Maschinencode ergeben:

Der Compiler schreibt diesen Maschinencode, gemeinsam mit weiteren zur Ausführung notwendigen Informationen, in eine sogenannte ausführbare Datei. Zur Ausführung wird der Maschinencode vom Lader des Betriebssystems in den Arbeitsspeicher geladen. Anschließend ruft es die Funktion "main" des Programms auf, und die CPU beginnt mit der Abarbeitung der Maschinenbefehle.

"Das Beispiel beschreibt Maschinencode für IBM-Großrechner, z. B. der Serie OS/390:"

Der Maschinencode entsteht beim Assemblieren bzw. beim Kompilieren der Quellcodedateien und wird vom „Linkage Editor“, ggf. unter Hinzufügen weiterer Module, als ausführbares Programm in einer Programmbibliothek bereitgestellt. Zur Ausführung wird dieses Programm in den Hauptspeicher geladen. Der Maschinencode dieser Programme enthält Befehle und Daten gemischt - wie dies bei Computern der Von-Neumann-Architektur möglich ist (im Gegensatz z. B. zur Harvard-Architektur).

Die "Daten" werden entsprechend dem festgelegten Speicherformat angelegt. Der Wert „12“ kann dabei z. B. folgendes Aussehen haben (Darstellung hexadezimal, in minimaler Länge):

Bei längeren Datenfeldern existieren ggf. führende Nullen zusätzlich oder bei Text nachfolgende Leerstellen.
Für jedes vorgesehene Datenfeld ist eine 'Adresse' festgelegt, an der es beginnt und wo es entsprechend seiner Länge und seinem Format gespeichert ist.

Die "Befehle" bestehen aus dem Befehlscode und – je nach Befehl – Parametern unterschiedlicher Struktur. Die nachfolgenden Beispiele sind hexadezimal dargestellt. Befehlsbeispiele:

C5.1C.92A4.8C2B (Trennpunkte nur zur besseren Lesbarkeit eingefügt):

47.80.B654:

<usw>

Im "Assemblercode" könnte diese Codierung z. B. wie folgt aussehen:

Von einer "Hochsprache" generiert könnte der Quellcode dagegen lauten:

Bei „Bedingung erfüllt“ wird nach XXX (= reale Adresse 6C4A64) verzweigt, andernfalls wird im Maschinencode mit codice_1 fortgefahren. Häufig generieren Hochsprachen zusätzliche Befehle, z. B. um Feldlängen oder Datenformate zu egalisieren, Register zu laden oder Adressen in Arrays zu berechnen.

Man erkennt, dass die Befehle unterschiedliche "Längen" aufweisen. Das Steuerwerk des Rechners erkennt die Länge an den ersten beiden Bits des Befehlscodes und schaltet das Befehlszählregister dementsprechend weiter. An genau dieser Stelle wird das Programm fortgesetzt – falls kein Sprungbefehl auszuführen ist.

"Speicheradressen" werden im Maschinencode immer durch eine (oder zwei) Registerangabe(n), zusätzlich optional durch eine im Befehl angegebene „Distanz“ dargestellt. Zur Ausführung wird beim Programmstart ein bestimmtes Register vom Betriebssystem mit der Adresse geladen, an die das Programm in den Speicher geladen wurde. Von diesem Wert ausgehend, werden im Programmcode (bei ASS programmiert, bei Hochsprachen generiert) die Basisregister geladen, wodurch die mit relativen Adressen versehenen Befehle die tatsächlichen Speicherstellen ansprechen.
Zur Ausführung von "Systemfunktionen" (wie Ein-/Ausgabebefehle, Abfrage von Datum/Uhrzeit, Tastatureingabe, Laden von Unterprogrammen u. v. a.) wird im Maschinenprogramm lediglich ein Systemaufruf mit dem Befehl 'SVC' (Supervisor Call) abgesetzt. Im zweiten Byte ist die auszuführende Funktion spezifiziert (Verzeichnis siehe); weitere Parameter für die Funktion werden über eine in ihrer Struktur festgelegte Datenschnittstelle übergeben, auf deren Adresse ein implizit vereinbartes (nicht im Befehl angegebenes) Register zeigt. Beispiel: X'05 08' = LOAD, Parameter = Pgm-Name etc. Die die aufgerufenen Funktionen ausführenden Befehle sind Maschinencode des Betriebssystems. Sie werden dort ausgeführt und führen anschließend zu dem dem SVC folgenden Befehl zurück.

"Hinweis:" Die genannten Befehlskürzel sind nur Beispiele; Befehle werden prozessorabhängig unterschiedlich benannt.

"Adressierung und Ergebnisanzeige:" Fast alle Befehle adressieren die betroffenen Speicherpositionen (häufig Quelle/Ziel, zu vergleichend/Vergleichswert usw.) über definierte Register. Ebenso gibt der Prozessor seine Ergebnisse und relevante Zusatzinformationen über festgelegte Register und/oder über Flags im Statusregister zurück. Dies ermöglicht es, im weiteren Programmablauf diese Informationen auszuwerten und darauf zu reagieren. Die Länge der Befehle und die Größe von Quell- und Zieloperanden können je nach Architektur unterschiedlich sein.

Beispiel: Ein Additionsbefehl wie ADC (add with carry) signalisiert dem weiteren Programmablauf ein Überschreiten des gültigen Wertebereichs über das Setzen des Carry- und Overflow-Flags hinaus.

"Unterschiede:" Der Befehlsvorrat einzelner Prozessoren ist unterschiedlich. Nicht alle Befehle sind auf jedem Prozessortyp und in jeder Prozessor-Generation verfügbar.

Beispiel: Ein einfacher Grundbefehl wie "SHL/SHR", der einen Registerwert um eine bestimmte Anzahl von Stellen nach links oder rechts verschiebt ist schon im 8086 vorhanden. Die mächtigere Variante "SHLD/SHRD", welche zusätzlich die entstehenden Leerstellen aus einem anderen Integerwert auffüllt, ist erst ab dem 80386 implementiert.

"Mächtigkeit:" Der Befehlsvorrat eines Prozessors stellt dabei Befehle unterschiedlich mächtiger Funktionalität bereit. Neben einfachen, einstufigen Grundoperationen stehen auch Befehle zur Verfügung, die mehrere Operationen in einem Befehl bündeln.

Beispiele: Der Befehl "CMP" (compare) ermöglicht den Vergleich zweier Werte auf <,>, =. Der Befehl XCHG (exchange) vertauscht die Positionen zweier Operanden. Der Befehl "CMPXCHG" (compare and exchange) kombiniert diese beiden Befehle und ermöglicht einen bedingungsabhängigen Datenaustausch in einem Befehl. Während der Befehl "BT (bit test)" nur den Zustand eines einzelnen Bits in einem Integerwert prüft, ermöglichen es die Befehle "BTC, BTR", und "BTS" darüber hinaus, das geprüfte Bit abhängig vom Ergebnis der Prüfung zu setzen "(BTS)", zu löschen "(BTR)", oder zu invertieren "(BTC)".

Generell unterscheidet man zwischen CPUs mit RISC- ("Reduced instruction set computer") oder CISC- ("Complex instruction set computer") Befehlssatz. Erstere haben einen bedeutend weniger mächtigen Befehlssatz, können jeden einzelnen Befehl aber typischerweise in einem Taktzyklus abarbeiten. Moderne CPUs mit CISC-Befehlssatz (darunter fallen heute fast ausschließlich x86-kompatible CPUs) dekodieren zur schnelleren Abarbeitung die komplexen CISC-Befehle zur Ausführung intern in eine RISC-ähnliche Mikrocontroller-Sprache.

"Performance:" Jeder Befehl wird in einer in Datenblättern angegebenen Anzahl von Taktzyklen des Prozessors abgearbeitet. Deren Kenntnis ermöglicht es dem Programmierer (bei extrem zeitkritischen Anwendungen) beispielsweise, Befehle mit vielen Taktzyklen durch mehrere, in der Summe aber effizientere Befehle zu ersetzen.

Grundlegende Maschinen-Befehle lassen sich in folgende Kategorien unterteilen:


In vielen modernen Prozessoren sind die Befehle der Maschinensprache, zumindest die komplexeren unter ihnen, intern durch Mikroprogramme realisiert. Das ist insbesondere bei der CISC-Architektur der Fall.



</doc>
<doc id="3406" url="https://de.wikipedia.org/wiki?curid=3406" title="Monochrome Malerei">
Monochrome Malerei

Bei der monochromen Malerei beschränkt sich der Künstler auf den Gebrauch einer Farbe oder eine Farbe ist zumindest vorherrschend. Nicht unter diesen Begriff gerechnet werden graphische Werke, wobei aber Lavierung in Tusche als Grenzfall beziehungsweise eigenständige Werkgattung gesehen wird, so etwa Chinesische Tuschmalerei.

Die Bezeichnung "Monochromie" kommt vom altgriechischen μὀνος, monos = „einzig“ und χρῶμα, chroma = „Farbe“. Sie ist der Gegensatz zur Polychromie (Vielfarbigkeit). Als Monochrome Malerei wird daher die einfarbige Malerei bezeichnet, dabei kann es sich beispielsweise um eine Leinwand handeln, die mit nur einer Farbe bemalt ist. Es kann sich ebenso um eine flächige Ausbreitung einer einzelnen Farbe handeln, hierzu können, je nach Auslegung des Begriffs, auch Farbnuancen und Abstufungen einer Farbe (beispielsweise unterschiedliche Blautöne) hinzugerechnet werden. Monochrome Malerei kann daher auch als Einfarbmalerei bezeichnet werden, wobei der Farbe eine das Werk beherrschende Bedeutung zukommt.

Bei der monochromen Malerei werden oftmals hellere und dunklere Tonwerte einer einzigen Farbe verwendet, sie findet sich beispielsweise in Ornamenten oder Mustern auf Tapeten und Stoffen. Dies wird daher oftmals auch als „Ton-in-Ton-Malerei“ bezeichnet.

Als Ton-in-Ton-Malerei wird eine meist harmonisch wirkende Malerei bezeichnet, die aus sehr eng verwandten Farbtönungen besteht. Hierbei ist ein Grundton (Mittelton) vorherrschend. Eine frühe Phase der Monochrommalerei findet sich bereits vom Barock bis zum Historismus. Diese Malweise wird auch als "Camaieu" bezeichnet. Wenn lediglich Grautöne verwendet werden, spricht man von "Grisaille". Daneben gibt es andere Formen, etwa Sepiamalerei im Braun der Tintenfischtinte. 

Die Bezeichnung Camaieu stammt vom französischen „peinture en camaïeu“ („gemalt wie eine Kamee“) und bezeichnet eine monochrome Ausgestaltung der Malerei in mehreren Tönungen ein und derselben Farbe. Diese Art der Bildgestaltung war schon im 16. Jahrhundert in Italien gebräuchlich. Im 18. Jahrhundert wurden sie beispielsweise als Miniaturen ausgeführt oder unter Glas, als Dekor in Dosendeckeln oder für Porzellan, eingesetzt. Dabei sollte der Eindruck beispielsweise eines Elfenbeinreliefs erweckt werden. Wurden dafür ausschließlich Gelbtöne verwendet, so nannte man diese Unterart „Cirage“. Porzellangegenstände wurden im ersten Drittel des 18. Jahrhunderts bevorzugt mit Purpur und Eisenrot, später mit Kupfergrün, in Grau und Sepia dekoriert.

In der Moderne entwickelte sich eine Monochromatik in extremer Buntmalerei bis hin zu streng monochromatischen Werken, die völlig abstrakt in (nahezu) einem Farbwert gehalten sind. Kasimir Malewitsch ist ein bekannter Maler des 20. Jahrhunderts, der sich damit beschäftigte. Daneben gilt Yves Klein als bedeutender Vertreter. Seine Bildkomposition sind ganz in einem Ultramarinblau mit dem Namen "International Klein Blue" gehalten, der auch Bestandteil der Bildtitel ist. Er selbst nannte sich „Yves le Monochrome“ und experimentierte seit 1946 mit dieser Praxis.

Zu den wichtigsten Vertretern der deutschen monochromen Malerei gehörte Raimund Girke. In einem großen Artikel des "zeitmagazin", einer Beilage der Wochenzeitung "Die Zeit", wurde er einmal als „Der weiße Riese“ bezeichnet, was mit seiner 1957 begonnenen Beschränkung auf die Farbe Weiß zusammenhängt, die er fortan nur noch in geringem Maße modulierte, erst sehr konstruktiv, häufig auf Grundformen wie Kreis und Quadrat beschränkt, später auch freizügiger mit starkem Hang zur gestischen Malerei. Aber immer war die Farbe auf Weiß und Abtönungen des Weiß ins Graue und Blaue beschränkt.



</doc>
<doc id="3407" url="https://de.wikipedia.org/wiki?curid=3407" title="Muslim">
Muslim

Ein Muslim oder Moslem () ist ein Angehöriger des Islam und wird auch als Mohammedaner (eigentlich ein ‚Anhänger der Lehren Mohammeds‘) bezeichnet. Bei dem Wort "Muslim" handelt es sich um das Partizip Aktiv zum IV. Stamm von → : „"Der sich (Gott) ergebende"“. Das im IV. Stamm ausgedrückte "sich unterwerfen" ist nicht im Sinne einer weltlichen Kapitulation zu verstehen, die mit dem X. Stamm ausgedrückt wird: → .

Die weibliche Form im Deutschen ist Muslimin oder Moslemin. In jüngerer Zeit wird für die weibliche Form auch das arabische Wort "Muslima" verwendet. Der Begriff "Muselman", früher auch „Muselmann“, gilt im Deutschen als historisch-literarisch bis veraltet, gehört aber in Formen, die an diesen Begriff anklingen, in mehreren anderen Sprachen, darunter auch solchen islamisch geprägter Länder, zum aktuellen Sprachgebrauch (z. B. französisch "musulman", türkisch "Müslüman", persisch "mosalmān").

Die Zahl der Muslime weltweit wird auf 1,57 Milliarden geschätzt. Damit sind sie nach den Christen die zweitgrößte Religionsgemeinschaft.

Die Zahl der Muslime wird in Deutschland anhand der Herkunft und der Mitgliedschaft in islamischen Vereinen geschätzt, da der Islam nicht in öffentlich-rechtlichen Religionsgemeinschaften organisiert ist, in denen Muslime eingeschriebenes Mitglied sind. Der Islam kennt keinen mit der Kirchenmitgliedschaft vergleichbaren Status.

Muslim ist, wer das islamische Glaubensbekenntnis (arabisch Schahāda) im vollen Bewusstsein gesprochen hat. Bindend ist es nach islamischen Recht, wenn er dies vor zwei volljährigen muslimischen Zeugen spricht. Jeder kann dieses aber auch allein aussprechen. Nach islamischem Selbstverständnis ist jedes Neugeborene ein Muslim ("siehe" Fitra) und wird gegebenenfalls erst später durch äußere Einflüsse (z. B. Erziehung) vom islamischen Glauben abgebracht. Mit dem Eintritt in die Geschlechtsreife bekunden auch sie dies durch das Sprechen des Glaubensbekenntnisses (u. a. bei jedem Gebet).

Muslim ist, nach islamischem Selbstverständnis, ein Monotheist, der Mohammed als letzten Propheten Gottes (Allahs) anerkennt. Orthodoxe Muslime glauben, dass der Koran das offenbarte Wort Gottes ist, das Mohammed durch den Erzengel Gabriel übermittelt wurde.

Der hanafitische Rechtsgelehrte asch-Schaibānī zitiert in seinem "Kitāb as-Siyar" einen Hadith, dem zufolge der Prophet Mohammed sagte: „Muslime sollen sich einander gegen den Außenstehenden unterstützen, das Blut aller Muslime hat den gleichen Wert, und derjenige, der am niedrigsten steht (d. h. der Sklave), kann alle anderen binden, wenn er einen Treueid leistet.“

Im Koran wird zwischen Muslimen, die sich rein formal zum Islam bekennen, und wirklichen Gläubigen ("mu'min") unterschieden: „Wir haben den Islam angenommen“ ("aslamnā"), das Bekenntnis zum Islam, ist nur eine Äußerung ("qaul" = „Parole“), Glaube ("īmān") dagegen ist sowohl Äußerung als auch Tat. Die Annahme des Islam durch verbale Bekundung während Mohammeds Wirken war zunächst die Garantie dafür, dass die arabischen Stämme der Arabischen Halbinsel von den Muslimen weiter nicht mehr bekämpft wurden. Damit stuft der Koran den Glauben höher ein als den bloß formalen Eintritt in den Islam. Die Exegese interpretiert an dieser Stelle das Schlüsselwort "aslamnā" („wir haben den Islam angenommen“) nicht nur in dem sonst üblichen Sinne der Unterwerfung unter den (einzigen) Gott, sondern versteht die Worte der Beduinen im Sinne von „sich ergeben“ und „kapitulieren“ ("istaslamnā") aus Furcht vor Gefangenschaft und weiterer kriegerischen Auseinandersetzung.

Die Sufis unterscheiden ebenfalls zwischen einem Muslim und einem „Gläubigen“. Nach ihrer Auffassung unterwirft sich ein Muslim lediglich äußerlich den Geboten Gottes, ein Gläubiger "glaubt" jedoch auch unerschütterlich daran und ist sich dessen bewusst, dass er ununterbrochen „vor seinem Schöpfer steht“.

In einigen Ländern wird bzw. wurde der Begriff „Muslim“ auch als ethnische Bezeichnung verwendet. Dies war zum Beispiel in der Sozialistischen Föderativen Republik Jugoslawien (1962–1992) der Fall. Hier produzierten in den 1960er Jahren muslimische marxistische Historiker eine große Anzahl von Werken über die Geschichte der bosnischen Muslime und lieferten eine „wissenschaftliche“ Legitimierung für die Anerkennung einer muslimischen Staatsnation. Bei der Volkszählung von 1971 gab die überwältigende Mehrheit der bosnischen Muslime ihre Nationalität als „Muslim im Sinne einer Nation“ an. Diese Bezeichnung wurde 1974 in der neuen jugoslawischen Verfassung offiziell anerkannt. Ein Problem an dem Konzept der neuen bosnischen muslimischen Nationalität war seine Zweideutigkeit, denn der Begriff konnte sowohl die Zugehörigkeit zu einer religiösen Gemeinschaft als auch einer Nationalität bedeuten. Ein Atheist muslimischer Nationalität konnte mithin nicht von einem muslimischen Gläubigen einer anderen Nationalität (albanisch, türkisch) unterschieden werden. Um das Problem zu beseitigen, wurde das Wort Muslim, wenn es die Nationalität bezeichnete, mit großem Anfangsbuchstaben geschrieben ("Musliman"), wenn es die Religionszugehörigkeit bezeichnete, dagegen mit kleinem Anfangsbuchstaben ("musliman"). Die jugoslawische Politik bemühte sich in der Folgezeit darum, das muslimische Nationalitätskonzept von jeglicher religiöser Konnotation fernzuhalten, doch haben anthropologische Studien gezeigt, dass diese Unterscheidung nicht vollständig aufrechterhalten werden konnte. Auch in den 1980er Jahren war für viele bosnische Muslime die nationale Identität noch eng mit dem Islam verbunden.

Die Bezeichnung „Mohammedaner“ für einen Muslim wird von Muslimen im deutschen Sprachraum im Allgemeinen abgelehnt, da Mohammed zwar verehrt, aber nicht angebetet wird und damit – gemessen an der Bezeichnung „Christ“ – nicht den Stellenwert Jesu im Christentum einnimmt.

Das arabische hingegen findet sich auch in anderen islamischen Literatursprachen wie Persisch, (Osmanisch-) Türkisch oder Urdu.

Die Vergöttlichung Mohammeds ist einzelnen Strömungen des Islam jedoch nicht gänzlich fremd: So galt der Muhammadiyya („Die Mohammedaner“) im Irak des 8. und 9. Jahrhunderts Mohammed sowohl als der unbekannte Gott, der sich dem Menschen nicht erschließt, als auch als einzig wahre Manifestation Gottes auf Erden. Auch in einigen Strömungen des Sufismus setzt ab etwa 1100 eine nicht unumstrittene Mohammedmystik ein, für die Mohammed Logos oder universelles Geistwesen ist, das entsprechend verehrt wird.

Im arabischsprachigen islamischen Schrifttum, z. B. in der Koranexegese von Ibn Kathīr, benutzt man die Ausdrücke wie „prophetische Gesetzgebung“ als Synonym zur „mohammedanischen Gesetzgebung“. Die islamische Gemeinschaft bezeichnet Ibn Kathir auch als „mohammedanische Umma“. Ibn Hadschar al-ʿAsqalānī spricht neben der Sunna des Propheten Mohammed auch von der „mohammedanischen Sunna“ bzw. von der „mohammedanischen Botschaft“.



</doc>
<doc id="3409" url="https://de.wikipedia.org/wiki?curid=3409" title="Magnoliengewächse">
Magnoliengewächse

Die Magnoliengewächse (Magnoliaceae) sind eine Familie der Bedecktsamigen Pflanzen (Magnoliopsida). Sie umfasst zwei Gattungen mit etwa 227 Arten.

Magnoliengewächse sind verholzende Pflanzen: Bäume oder Sträucher. Sie sind immergrün oder laubabwerfend. Die wechselständigen und spiralig angeordneten Laubblätter sind gestielt, groß und einfach. Die Blattspreite ist einfach oder gelappt. Sie besitzen große Nebenblätter.

Die Blüten stehen meist einzeln end- oder achselständig. Die gestielten Blüten sind zwittrig (selten eingeschlechtig) und oft groß und prächtig. Anders als die meisten Bedecktsamigen Pflanzen, deren Blütenteile wirtelig angeordnet sind, stehen bei den Magnoliengewächsen alle Blütenblätter (s. l.), also die Blütenhüll-, die Staub- und Fruchtblätter spiralig an der Blütenachse, azyklisch angeordnet und meist in Vielzahl (mehr als je sechs). Die meist vielen freien und oberständigen Fruchtblätter enthalten meist zwei, selten bis 20 Samenanlagen. Als Früchte werden Balg- oder Hülsenfrüchte gebildet, die spiralig in zapfenförmigen Samenständen stehen. Da diese Anordnung auch bei frühen fossilen Bedecktsamern gefunden wird, gelten die Pflanzen dieser Familie als entwicklungsgeschichtlich alt (oder primitiv). Die Bestäubung erfolgt meist durch Käfer.

Blütenformel: formula_1 formula_2 oder formula_3.

Diese Pflanzenfamilie ist in Amerika und Ostasien verbreitet. Die überwiegende Zahl der Arten wächst in den gemäßigten und tropischen Zonen Südostasiens, vom Himalaja ostwärts bis nach Japan und südostwärts über die malaiische Inselgruppe bis nach Neuguinea und Neubritannien. Die übrigen Arten sind in den gemäßigten südöstlichen Teilen Nordamerikas über das tropische Südamerika bis nach Brasilien verbreitet.

Die Systematik der Familie Magnoliaceae wird kontrovers diskutiert (siehe AGP-Website und Literatur unten). Dabei wurde vorgeschlagen, dass es zwei Unterfamilien gibt. Die Unterfamilie Liriodendroidae mit der einzigen Gattung "Liriodendron" und die Unterfamilie Magnolioideae. Die Unterfamilie Magnolioideae enthält nach Meinung einiger Autoren die einzige, umfangreiche Gattung "Magnolia" oder nach Meinung besonders chinesischer Autoren (siehe "Flora of China" 2008) bis zu 16 kleine Gattungen.

Die Liriodendraceae stellten früher eine eigenständige Familie dar, deren zwei Arten jetzt den Magnoliengewächsen zugeordnet werden.

In der Familie Magnoliaceae gibt es nur zwei Gattungen (Darstellung nach beispielsweise Hans P. Nooteboom) mit etwa 297 Arten:




</doc>
<doc id="3411" url="https://de.wikipedia.org/wiki?curid=3411" title="Malpighiengewächse">
Malpighiengewächse

Die Malpighiengewächse (Malpighiaceae) sind eine Familie in der Ordnung der Malpighienartigen
(Malpighiales) der Bedecktsamigen Pflanzen (Magnoliopsida).
Die etwa 1300 Arten sind hauptsächlich in den Tropen Südamerikas beheimatet. Die Familie Malpighiaceae ist wahrscheinlich in der Oberkreide vor etwa 68 Millionen Jahren in Südamerika entstanden.

Es sind meist verholzende Pflanzen: Sträucher, Bäume und Lianen, aber auch ausdauernde krautige Pflanzen. Die einfachen Laubblätter sind meist gegenständig. Es sind Nebenblätter vorhanden. 

Es sind vier oder fünf Kelchblätter vorhanden. Es sind meist fünf Kronblätter vorhanden, welche gestielt sind, der Fachausdruck ist genagelte Kronblätter. Die Blüten enthalten zwei bis 15, meist fünf oder zehn Staubblätter. Zwei bis fünf, meist drei Fruchtblätter sind zu einem unterständigen Fruchtknoten verwachsen. An der Frucht sind oft noch die Kelch- und Staubblätter erhalten.


Die Familie Malpighiaceae wurde 1789 durch Antoine Laurent de Jussieu in "Genera Plantarum", S. 252 unter der Bezeichnung „Malpighiae“ aufgestellt. Typusgattung ist "Malpighia" 

Die Systematik der Familie Malpighiaceae wird kontrovers diskutiert und von den Autoren unterschiedlich in Unterfamilien und Tribus gegliedert. Die Familie Malpighiaceae wird seit 2010 in zwei Unterfamilien gegliedert und enthält 64 bis 68 Gattungen mit 1250 bis 1300 Arten:

Die Familie Malpighiaceae umfasst folgende Gattungen:





</doc>
<doc id="3413" url="https://de.wikipedia.org/wiki?curid=3413" title="Myrtengewächse">
Myrtengewächse

Die Myrtengewächse (Myrtaceae) bilden eine Pflanzenfamilie in der Ordnung der Myrtenartigen (Myrtales). Die Familie wird in zwei Unterfamilien und einige Triben gegliedert; insgesamt umfasst sie etwa 131 Gattungen mit etwa 4620 Arten. Sie sind vor allem in Australien (etwa 85 Gattungen) und in der Neotropis beheimatet.

Neben der namenstiftenden Myrte gehören zur Familie der Myrtengewächse (Myrtaceae) Pflanzenarten wie die Gewürznelke und die Eukalyptus-Arten. Viele Arten liefern ätherischen Ölen für Parfümherstellung und Pharmazie. Einige Arten liefern eßbare Früchte, einige "Psidium"-Arten (beispielsweise die Guave) und "Campomanesia"- sowie "Eugenia"-Arten.

Die Pflanzenarten dieser Familie sind meist immergrüne (einige "Eucalyptus"-Arten sind laubabwerfend) Gehölze: Bäume und Sträucher. Sie sind helophytisch bis xerophytisch. Sie enthalten meist reichlich ätherische Öle in schizolysigenen Exktretbehältern. Das Sekundäre Dickenwachstum geht von einem konventionellen Kambiumring aus. Das deutlichste Unterscheidungsmerkmal dieser Familie ist allerdings, dass sich das Phloem innerhalb des Xylem befindet, bei allen anderen Pflanzen ist dies umgekehrt. Sie weisen bikollaterale Leitbündel auf. 

Die meist gegenständig, seltener wechselständig und spiralig oder wirtelig angeordneten Laubblätter besitzen je nach Art eine sehr unterschiedliche Größe sowie Form und sie können gestielt sein. Die einfache Blattspreite ist ledrig bis krautig und ganzrandig. Die Blattflächen können drüsig gepunktet sein. Die Blätter können normal ausgerichtet sein oder um 90° gedreht. Die Stomata befinden sich meist nur auf einer Blattfläche, oder bei senkrechtgestellten Blättern auf beiden Blattflächen und sind meist anomocytisch oder seltener paracytisch. Bei vielen Arten liegt Heterophyllie vor. Nebenblätter fehlen oder sind nur klein (beispielsweise "Calythrix").

Die Blüten stehen selten einzeln oder meist in end-, seitenständigen oder zwischen den Nodien stehenden (deutlich bei "Beaufortia"-, "Callistemon"- und "Melaleuca"-Arten), zymöse, ährige, schirmrispige, rispige oder köpfchenförmige (beispielsweise "Actinodium") Blütenstände zusammen. Es können Tragblätter vorhanden sein. Bei manchen Arten wirken die Blütenstände mit ihren Tragblättern als Pseudanthien. Unter den Blüten stehen oft zwei Deckblätter. Die Pflanzen sind selten zweihäusig getrenntgeschlechtig (diözisch) oder polygamomonözisch. 
Ihre selten eingeschlechtigen oder meist zwittrigen, meist radiärsymmetrischen, selten etwas zygomorph (betrifft meist nur das Androeceum) Blüten sind meist vier-, seltener fünfzählig. Es ist oft ein becher- bis röhrenförmig vertiefter Blütenboden vorhanden (= Hypanthium). Es sind meist vier oder fünf (drei bis sechs), freie oder verwachsene Kelchblätter vorhanden oder sie sind nur noch rudimentär zu erkennen. Die vier- oder fünf Kronblätter sind frei oder verwachsen. Die Farben der Kronblätter reichen von weiß bis gelb, oder von rot über rosa- bis purpurfarben, aber blau kommt nicht vor. In der Regel sind, sekundär vermehrt, sehr viele (20 bis 150) freie Staubblätter vorhanden, seltener vier oder fünf, acht oder zehn; sie befinden sich meist am Rand des Hypanthiums. Die Staubblätter können alle fertil oder teilweise zu Staminodien umgewandelt sein. Die Staubfäden können zu Bündeln verwachsen sein (beispielsweise "Lophostemon") und besitzen selten Anhängsel (beispielsweise "Corynanthera"). Die Pollenkörner besitzen meist drei (zwei bis vier) Aperturen und sind selten colpate, meist colporat oder porat, manchmal syncolpat. Es kann ein Diskus vorhanden sein. Meist zwei bis fünf, selten bis zu 16 Fruchtblätter sind zu einem synkarpen, mittel- bis unterständigen, selten fast oberständigen, meist zwei- bis fünf- (ein- bis 16-)kammerigen Fruchtknoten verwachsen. Einkammerigen Fruchtknoten können 30 bis 150 Samenanlagen enthalten, ansonsten enthält jede Fruchtknotenkammer 2 bis 50 Samenanlagen; sie sind hemianatrop bis anatrop und meist bitegmisch und crassinucellat. Jede Blüte enthält nur einen Griffel mit einer Narbe. Die Bestäubung erfolgt durch Insekten (Entomophilie) oder Vögel (Ornithophilie). 
Die fleischigen oder trockenen, bei Reife sich öffnenden oder geschlossen bleibenden Früchte können Kapselfrüchte, Steinfrüchte, Nuss oder Beeren sein. Die Samen enthalten kein Endosperm und können geflügelt (bei einigen "Eucalyptus"-Arten) oder ungeflügelt sein. Der Embryo ist gerade bis mehr oder weniger stark gekrümmt, manchmal gedreht. Über häufige Polyembryonie wurde berichtet. 

Die Chromosomengrundzahlen betragen n = meist 11 (5 bis 12).

Die Familie der Myrtaceae wurde 1789 durch Antoine Laurent de Jussieu in "Genera Plantarum", S. 322–323 aufgestellt. Typusgattung ist "Myrtus" Synonyme Myrtaceae sind: Heteropyxidaceae nom. cons., Kaniaceae , Leptospermaceae , Myrrhiniaceae , Psiloxylaceae .

Die Familie der Myrtengewächse (Myrtaceae) wird seit Wilson et al. 2005 in zwei Unterfamilien sowie etwa 17 Tribus gegliedert und umfasst etwa 131 bis 138 Gattungen mit insgesamt etwa 4620 Arten:




</doc>
<doc id="3414" url="https://de.wikipedia.org/wiki?curid=3414" title="Malvengewächse">
Malvengewächse

Die Malvengewächse (Malvaceae) sind eine Familie in der Ordnung der Malvenartigen (Malvales) innerhalb der Bedecktsamigen Pflanzen. Die Familie wird heute in neun Unterfamilien gegliedert mit etwa 243 Gattungen und enthält etwa 4.225 bis 4.300 Arten. Die Malvaceae haben eine weltweite Verbreitung. Bekannteste Nutzpflanzen sind Gemüse-Eibisch, Kakaobaum und Baumwolle. Diese Familie enthält einige Arten, die medizinisch oder für Tees genutzt werden. Viele Arten und besonders ihre Sorten sind Zierpflanzen für Parks, Gärten und Räume.

Es gibt krautige Pflanzen: einjährige bis ausdauernde und verholzende Pflanzen: Sträucher und Bäume, sehr selten Lianen. Bei den verholzenden Arten ist die Borke faserig. Die Malvengewächse besitzen oft Schleimzellen. Meist sind auf vielen Pflanzenteilen Haare vorhanden, es handelt sich meist typischerweise um Sternhaare. Selten sind Dornen oder Stacheln vorhanden. Es können extraflorale Nektarien vorhanden sein.

Die meist wechselständigen Laubblätter sind gestielt. Die fingeradrige Blattspreite ist handförmig gelappt bis geteilt oder ungeteilt. Der Blattrand ist glatt, gekerbt, gezähnt oder gesägt; wenn er nicht glatt ist endet eine Blattader je Blattzahn. Es sind Nebenblätter vorhanden, sehr selten sind sie reduziert.

Selten sind Arten deren Blüten einhäusig (monözisch) oder zweihäusig (diözisch) getrenntgeschlechtig sind. Die Blüten stehen in seitenständigen, unterschiedlich aufgebauten, oft zymösen Blütenständen zusammen oder die Blütenstände sind reduziert bis auf eine Blüte. Kauliflorie tritt bei einigen tropischen Arten auf (bekanntes Beispiel Kakaobaum ("Theobroma cacao")). Manchmal stehen einige Hochblätter zusammen oder es ist bei sehr vielen Gattungen ein Nebenkelch aus meist drei, selten mehr Hochblättern vorhanden.
Die meist gestielten Blüten sind typischerweise radiärsymmetrisch, selten etwas zygomorph (beispielsweise "Helicteres") oder asymmetrisch (beispielsweise "Mansonia"). Die fünfzähligen Blüten sind selten eingeschlechtig, meist zwittrig mit doppeltem Perianth. Die meist fünf Kelchblätter sind oft an ihrer Basis verwachsen und berühren sich im übrigen Bereich nur (valvat). Die meist fünf weitgehend freien Kronblätter überdecken sich gedreht (contort). Bei einigen Arten sind die Kronblätter reduziert oder fehlen. Bei manchen Arten ist die Basis der Staubblätter mit den Kronblättern verwachsen.

Bei der Familie der Malvaceae ist ursprünglich nur der innere Kreis mit fünf Staubblättern vorhanden. Bei vielen Taxa existiert als Besonderheit der Blüten eine sogenannte sekundäre Vielzähligkeit der Staubblätter (bis über 1000), dabei wurden die Glieder des inneren Kreises zentripetal vermehrt, wodurch fünf Staubblattgruppen entstanden sind. Oft sind Staminodien vorhanden. Die Staubfäden sind mindestens an ihrer Basis verwachsen. Bei den Unterfamilien Bombacoideae und Malvoideae sind die Staubfäden der vielen Staubblätter zu einer den Stempel umgebenden Röhre verwachsen, der sogenannten Columna. Zahlreiche Vertreter der Unterfamilien Grewioideae, Helicteroideae und Sterculioideae weisen ein Androgynophor auf, das Staubblätter und Gynoeceum aus der Blüte heraushebt. Die Pollen sind in den Unterfamilien verschieden. Zwei bis viele oberständige Fruchtblätter sind frei oder zu einem Fruchtknoten verwachsen. Der Griffel endet in einer kopfigen oder oft fünflappigen Narbe. In den Blüten sind Nektarien vorhanden, die aus zusammengefassten Drüsenhaaren bestehen und sich meist auf den Kelchblättern befinden.

Die Bestäubung erfolgt bei vielen Taxa durch Insekten (Entomophilie), einige Taxa besonders der Neuen Welt sind auf Vögel (Ornithophilie) spezialisiert.

Bei den Malvaceae gibt es ein breites Spektrum an Fruchttypen. Meist werden Kapselfrüchte oder Spaltfrüchte gebildet, seltener Beeren, Steinfrüchte oder Nüsse. Viele Früchte enthalten Haare. Bei manchen Arten besitzen die Früchte an der Oberfläche Haare oder Stacheln.

Die Samen können Haare (bekannt von Baumwolle), Flügel oder einen Arillus (beispielsweise "Durio") besitzen.

Bei den Taxa mit Kapselfrüchten sind die Samen die Verbreitungseinheiten (Diasporen); ansonsten sind meist die Früchte die Diasporen. Sie werden überwiegend durch Wind (bekannt von "Tilia", Anemochorie) oder Tiere, selten durch Wasser verbreitet; von Myrmekochorie wird selten berichtet.

An Inhaltsstoffen sind Cyclopropenoid-Fettsäuren (Malvalsäure, Sterculiasäure) und Terpenoid-basierte Chinone vorhanden.

Die Chromosomenzahlen sind in den Unterfamilien unterschiedlich: Bombacoideae n = 36 (−46), Brownlowioideae n= 10, Byttnerioideae n = (5-7) 10 (−13), Dombeyoideae n = 19, 20, 30 etc., Grewioideae n = 7-9 (10), Helicteroideae n = 9, 14, 20, 25 etc., Malvoideae n = 5-20 (-mehr), Sterculioideae n = (15, 16, 18) 20 (21 etc.), Tilioideae n = 41.

Die Erstveröffentlichung des Familiennamens Malvaceae erfolgt 1789 durch Antoine-Laurent de Jussieu in "Genera Plantarum", 271. Die Typusgattung ist "Malva" 

Vor allem molekulargenetische Untersuchungen führten zu großen Änderungen in der Systematik der Ordnung der Malvales. In die Familie der Malvengewächse (Malvaceae) wurden einige Taxa neu eingegliedert, darunter die ehemaligen Familien der Lindengewächse, der Wollbaumgewächse und der Sterkuliengewächse. Diese neu eingeordneten ehemaligen Familien sind nun nur noch Unterfamilien. Die meisten Taxa, die die bisherige Familie der Malvengewächse bildeten, sind jetzt in der Unterfamilie Malvoideae zu finden; deshalb hier die Bezeichnung „Malvengewächse im engeren Sinne“. Die Familie ist jetzt gegliedert in neun Unterfamilien.

Synonyme für Malvaceae sind: Bombacaceae nom. cons., Brownlowiaceae , Byttneriaceae nom. cons., Dombeyaceae , Durionaceae , Fremontiaceae nom. illeg., Helicteraceae , Hermanniaceae , Hibiscaceae , Lasiopetalaceae , Melochiaceae , Pentapetaceae , Philippodendraceae , Plagianthaceae , Sparmanniaceae , Sterculiaceae nom. cons., Theobromataceae , Tiliaceae nom. cons., Triplochitonaceae nom. nud.

Die Familie Malvaceae wird heute in neun Unterfamilien gegliedert mit etwa 243 Gattungen und enthält etwa 4225 bis 4300 Arten. (Wenn ein Artikel zur Unterfamilie vorhanden ist, dann sind diese Angaben zu Tribus und Gattungen dort zu finden):







Kladogramm nach APWebsite:
Einige Malvengewächse stellen wichtige Futterpflanzen für den Malven-Dickkopffalter ("Carcharodus alceae") dar. So ernähren sich die Raupen dieser Schmetterlingsart vor allem von Weg-Malven ("Malva neglecta") und Moschus-Malven ("Malva moschata").

Viele Arten und besonders ihre Sorten sind Zierpflanzen für Parks, Gärten und Räume. Die am häufigsten kultivierte Art ist wohl der Chinesische Roseneibisch ("Hibiscus rosa-sinensis") mit hunderten von Sorten, der in den Parks und Gärten der frostfreien Gebiete wichtig ist, aber auch als Zimmerpflanze verwendet wird. Bechermalven, Stockrosen und viele mehr sind in den Gemäßigten Breiten in den Gärten zu finden. Einige Baumarten werden in den Tropen in Parks und Alleen gepflanzt.

Es gibt eine Vielzahl von Arten, die vom Menschen vielfältig genutzt werden. Bekannt als Nutzpflanze ist der Kakaobaum. Okra ("Abelmoschus esculentus") ist ein Gemüse. Die Früchte und Blätter des Afrikanischen Affenbrotbaumes sind essbar. Bekannt ist die Durian-Frucht.

Diese Familie enthält einige Arten die für Tees genutzt werden. Die medizinische Wirkung vieler Arten beruht vor Allem auf dem enthaltenen Schleim. Bekannt sind auch die Kolabäume ("Cola"). Die Wilde Malve, auch „Große Käsepappel“ genannt, ist eine der ältesten bekannten Gemüse- und Heilpflanzen.

Baumwolle und Kenaf, Java-Jute oder Ostindische Hanfrose ("Hibiscus cannabinus") sind wichtige Faserpflanzen.




</doc>
<doc id="3417" url="https://de.wikipedia.org/wiki?curid=3417" title="Mega">
Mega

Mega steht für:


Eigennamen:
MEGA steht für:

MEGA steht als Abkürzung für: 


</doc>
<doc id="3418" url="https://de.wikipedia.org/wiki?curid=3418" title="Mobilfunk">
Mobilfunk

Mobilfunk ist die Sammelbezeichnung für den Betrieb von beweglichen Funkgeräten.

Darunter fallen vor allem tragbare Telefone, (Mobiltelefone, "siehe auch:" Mobilfunknetz) und in Fahrzeuge eingebaute Wechselsprechgeräte (etwa Taxifunk). Es existieren jedoch viele weitere Anwendungsbereiche, wie zum Beispiel mobile Datenerfassung, Funkrufdienste, Telemetrie, See- und Binnenschifffahrtfunkdienste, Jedermannfunk und Amateurfunk, die nicht ortsgebunden sind.

Die deutsche Bundesnetzagentur spricht von "Öffentlichen zellularen Mobilfunkdiensten".

Das „öffentliche bewegliche Landfunknetz“ (Autotelefon, öffentlicher beweglicher Landfunkdienst, öbL) war ein öffentliches Funknetz der Deutschen Bundespost für Nachrichtenverbindungen von beweglichen Funkstellen untereinander und mit Endstellen des öffentlichen Fernsprechnetzes. Die in diesem Netz gebotenen Dienste hießen „öffentliche bewegliche Landfunkdienste“.

Mit dem Landfunknetz war es möglich, über ein Autotelefon In- und Auslandsgespräche über das öffentliche Fernsprechnetz herzustellen: „Der Verbindungsaufbau von einem Teilnehmer des Fernsprechnetzes zum PKW-Teilnehmer erfolgt über die gewöhnlichen Vermittlungseinrichtungen, eine Überleitvermittlung (die in das Funknetz überleitet) sowie über diejenige Landfunkstelle, die dem PKW-Teilnehmer geographisch am nächsten liegt“. Die Fahrzeuge mussten mit einer Sprechfunkanlage ausgerüstet sein und sich im Versorgungsbereich einer festen Landfunkstelle befinden.

Daneben existierten auch noch satellitengestützte Mobilfunknetze. Die Nutzung dieser Systeme, obwohl sie weltweit funktionieren, ist wegen unhandlicher und teurer Endgeräte und hoher Gesprächskosten nur in speziellen Bereichen sinnvoll. Es etabliert sich aber immer mehr durch bessere Technik und günstigere Preise dort, wo kein Festnetz oder terrestrisches Mobilfunknetz vorhanden ist, so etwa im Nahen Osten.

Grob unterschieden werden Einwegesystem (Simplex-Betrieb), Mehrwegesysteme (Halb-Duplex und Duplex) und Mehrbenutzersysteme (Multiplex).
Bei Einwegesystemen ist nur der Empfang (Funkruf) oder seltener nur das Senden (Rundfunk, autonome Pegelstandsmelder der Wasserwirtschaft, Wetterstationen oder auch „Abhörwanzen“) möglich.
Bei Mehrwegesystemen kann das Endgerät sowohl senden als auch empfangen. Ist dies gleichzeitig möglich (wie bei Mobiltelefonen), so spricht man von Vollduplex.

Mobilfunk unterteilt sich in einen öffentlichen und einen nicht öffentlichen Teil.

Die Aufteilung in öffentliche und nichtöffentliche Funkdienste stammt noch aus der Zeit vor der Postreform I. Derzeit ist in Deutschland die Bundesnetzagentur für die Vergabe der Frequenzen und Genehmigung der Funklizenzen zuständig.

Unter den "nicht öffentlichen Mobilfunk" fallen zum Beispiel der Flugfunk und der Betriebsfunk. Das Spektrum der Nutzer des Betriebsfunks reicht von der Polizei, der Feuerwehr und anderen Hilfsorganisationen (BOS-Funk = Behörden und Organisationen mit Sicherheitsaufgaben) über Verkehrsbetriebe (zum Beispiel Flugfunk) und Taxi-Unternehmen bis zu privaten Unternehmen, beispielsweise des Baugewerbes.

Als CB-Funk ("Citizens Band Radio", „Bürgerfrequenzband“, Jedermann-Funk, Bürgerfunk) bezeichnet man einen Funk im 11-Meter-Band auf insgesamt 80 Kanälen im Frequenzbereich von 26,565 MHz bis 27,405 MHz. Die Reichweite liegt bei etwa 10–15 km bei der höchstzulässigen Ausgangsleistung. Der Betrieb von mobilen Anlagen (insbesondere in Kfz) ist in Deutschland seit 1975 genehmigungs- und gebührenfrei.

Der Amateurfunk ist privater Funkverkehr, der das erfolgreiche Ablegen einer Prüfung voraussetzt, die zu einer staatlichen Genehmigung (Amateurfunkgenehmigung) führt. Jedem Funkamateur ist ein Rufzeichen zugeteilt. Die Frequenzen für Funkamateure sind international vereinbart. Insbesondere über Kurzwelle kann jeder Punkt der Erde erreicht werden. Auch ist die Verwendung von Amateurfunksatelliten möglich. Die Nutzung ist jedoch nur für rein private Zwecke erlaubt; das Austauschen von politischen Informationen oder der erwerbsmäßige Gebrauch ist zum Beispiel verboten. In den USA waren die ersten Funkamateure bereits 1911 aktiv, heute weltweit über eine Million, davon 78.000 in Deutschland (Stand 12/2003).

Die "öffentlichen Mobilfunknetze" werden von Mobilfunkbetreibern zur Verfügung gestellt. Sie können von jedermann benutzt werden. Die öffentlichen Mobilfunknetze sind die Funktelefonnetze, die Funkrufnetze, das Rheinfunknetz und das Seefunknetz.

Schnurlostelefone haben die Aufgabe, ein Funktelefon kurzer Reichweite über eine Basisstation mit dem drahtgebundenen Fernsprechnetz zu verbinden sowie interne Verbindungen zwischen mehreren, mit der Basisstation verbundenen, Mobilteilen zu ermöglichen. Dies kann jeweils als nicht-öffentliches Mobilfunknetz interpretiert werden.

Der Mobilfunk fand vor allem durch die GSM-Netze für Mobiltelefone (umgangssprachlich Handys) Mitte der 1990er Jahre starke Verbreitung. Dadurch wurde er zu einem gesellschaftlichen Phänomen "(„Handy-Etikette“)".

An Sendemasten, die oft aus technischen Gründen an exponierten Lagen aufgestellt werden und dadurch Einfluss auf das Landschaftsbild nehmen, wird teilweise aus ästhetischen Gründen Kritik geübt.

Durch das Vermieten von Montageflächen für Mobilfunkantennen an geeigneten Bauwerken können Einnahmen erzielt werden. Auf diese Weise werden wenigstens teilweise die Kosten zum Unterhalt etwa historischer Sendetürme als technisches Denkmal gedeckt, wie beispielsweise beim Sendeturm des Senders Gleiwitz, des letzten bestehenden Holzsendeturms.

Heute ist der Mobilfunk ein bedeutender wirtschaftlicher Faktor. Größtenteils private Mobilfunkgesellschaften konkurrieren um Marktanteile in einem in Deutschland gesättigten Markt. Die Mobilfunktechnik spielte eine besondere Rolle in der Entwicklung der New Economy und bei der Schaffung zahlreicher neuer Arbeitsplätze.

2010 wurde in Deutschland mit Mobilfunkdienstleistungen ein Umsatz von rund 141 Milliarden Euro erzielt; für 2016 wurden die Mobilfunkumsätze weltweit auf insgesamt rund 1,24 Billionen US-Dollar prognostiziert. Diese bedeutende Marktmacht und das Marktpotential beeinflussen die Objektivität wissenschaftlicher klinischer Studien.

Die Internationale Agentur für Krebsforschung (IARC) stuft Handystrahlung als "„möglicherweise krebserregend“" ein. Sie verweist dabei auf eine Studie aus dem Jahr 2004, die bei intensiver Nutzung von Mobiltelefonen ein um 40 % erhöhtes Risiko für die Entstehung eines Glioms ermittelte.

Dagegen finden etliche wissenschaftliche Institutionen (beispielsweise der in Österreich tätige Wissenschaftliche Beirat Funk), dass für eine biologische Wirkung von Mobilfunkgeräten keine Belege existieren würden.

Der Betrieb des Mobilfunknetzes verbraucht ungefähr 60 Gigawattstunden pro Jahr. Der Energieverbrauch des Mobilfunknetzes setzt sich aus dem Verbrauch der Basisstationen und der Zugangsnetze zusammen. Dabei entfällt der weitaus größere Teil auf die Basisstationen. Zu Beginn der Verbreitung der Netze stand die Verfügbarkeit und Leistungsfähigkeit im Vordergrund "(Quality of Service, QOS)"; Energieeinsparpotentiale wurden vernachlässigt. Dadurch könnten sich in Zukunft beträchtliche Verbesserungen ergeben. Die Optimierung des Energiebedarfs im Mobilfunknetz ist daher ein wesentlicher Aspekt in den Ansätzen zur "Green IT".

Erste Zukunftsvorstellungen von „Taschentelephonen“, mit denen jedermann „sich mit wem er will wird verbinden können, einerlei, wo er auch ist“, wurden 1910 in dem Buch "Die Welt in 100 Jahren" publiziert. Seit 1926 gab es einen Vorläufer des öffentlichen Mobilfunks in Deutschland, den Zugfunk in Form einer handvermittelten öffentlichen Sprechzelle im F-Zug Berlin – Hamburg. Seit 1950 gibt es in der Bundesrepublik Deutschland öffentliche Mobilfunknetze; sie waren nicht auf eine breite Vermarktung ausgelegt. Die Mobilfunknetze in Deutschland sind mit fortlaufenden Großbuchstaben benannt.

Die Mobilfunknetze im GSM-Standard wurden in Deutschland anfänglich in D-Netze (900 MHz) und E-Netze (1800 MHz) unterteilt; mittlerweile haben alle vier deutschen GSM-Netze aber auch (in geringerem Umfang) Frequenzen im jeweils anderen Band. E-Plus nutzt das E noch als Markenbestandteil, ebenso bis Anfang 2013 Vodafone das D in der Firmierung Vodafone D2 GmbH. Seitdem verzichtet Vodafone auf den Zusatz „D2“ als Netzhinweis und firmiert als Vodafone GmbH.

Das Gesprächsvolumen umfasste in Deutschland im Jahr 2010 91 Milliarden Minuten pro Jahr und hatte damit einen Marktanteil von über 19 Prozent.



Während Mobiltelefonie in anderen Staaten bereits als alltägliches Kommunikationsmittel etabliert worden war, blieben Mobiltelefone in Nordkorea mehrere Jahre lang verboten. Im Dezember 2002 wurden dann an ausgewählte Bürger versuchsweise 20.000 Mobiltelefone abgegeben, die im Juni 2004 wieder eingezogen wurden.

2007 bot der ägyptische Mischkonzern "Orascom Group" einen Weiterbau der langjährigen Bauruine des Ryugyŏng Hot’el an, sofern dessen Mobilfunktochter Orascom Telecom ein nordkoreanisches Mobilfunknetz aufbauen und über 25 Jahre lang betreiben könne. Daraufhin wurde im Folgejahr das Joint Venture Cheo Technology gegründet, das zu 75 Prozent Orascom und zu 25 Prozent dem nordkoreanischen Staat gehört und eine Lizenz zum Betrieb eines UMTS-Mobilfunknetzes in der Hauptstadt Pjöngjang erhielt. 2008 nahm es das Mobilfunknetz Koryolink in Betrieb. Die ersten vergebenen Telefonnummern begannen mit den Ziffern „1912“, dem Geburtsjahr des ehemaligen Präsidenten Kim Il-sung.
Seitdem ist das Telefonieren innerhalb des nordkoreanischen Inlands bei verhältnismäßig hohen Anschaffungskosten möglich, eine SIM-Karte mit 50 MB Datenvolumen kostet an die 200 Euro. Für Auslandstelefonate existiert eine separate Frequenz, die jedoch in Nordkorea lebenden Ausländern vorbehalten ist. Verbindungen zwischen den beiden Frequenzen sind nicht möglich. Koryolink verzeichnete Ende 2009 90.000 Teilnehmer, Ende 2010 430.000 und im Februar 2012 eine Million.

Bislang sind weder Gespräche ins Ausland noch die Nutzung des mobilen Internets möglich. Weiterhin dürfen die offiziell importierten Mobilgeräte nicht über Memory-Cards, Videokameras oder eine Bluetooth-Funktion verfügen.

Gleichwohl wurde 2013 die Produktion des ersten nordkoreanischen Smartphones „Arirang AS1201“ (entspricht den chinesischen Uniscope U1201) verlautbart. Bereits ein Jahr später erschien ein weiteres Modell, das „Arirang AP121“, welches allerdings mit dem chinesischen Smartphone „THL W200“ identisch ist.

Sämtliche Smartphones laufen mit Android, verfügen jedoch über keinen App-Store wie Google Play. Eigene Applikationen zu installieren, ist nicht möglich; das muss bei staatlich zugelassenen Händlern geschehen.

Medienberichten zufolge müssen Personen, die einen Mobilfunk-Anschluss haben möchten, neben ihren persönlichen Daten auch eine Erklärung darüber abgeben, dass sie keine Anrufe tätigen, deren Inhalt Staatsgeheimnisse berührt, und das Gerät auch nicht missbräuchlich verwenden. Zudem soll die Erlaubnis der Sicherheitsbehörden nötig sein. Im Grenzgebiet zu China sollen Menschen aber mithilfe von chinesischen Geräten auf das Netz des Nachbarlandes zugreifen und somit auch Auslandsgespräche führen können.

Das Einführen von Handys war bis zum 7. Januar 2013 verboten. Entsprechende Geräte mussten bei Einreise abgegeben werden und wurden dem Eigentümer erst bei Ausreise wieder ausgehändigt.


Mobilfunkanbieter:




</doc>
<doc id="3420" url="https://de.wikipedia.org/wiki?curid=3420" title="Master Boot Record">
Master Boot Record

Der (kurz MBR) enthält ein Startprogramm für BIOS-basierte Computer (IBM-PC-kompatible Computer) und eine Partitionstabelle. Er befindet sich im ersten Sektor eines in Partitionen aufteilbaren Speichermediums wie beispielsweise einer Festplatte. Das Konzept aus Startprogramm und Partitionstabelle wurde 1983 mit dem IBM-PC XT und MS-DOS/PC DOS 2.0 eingeführt. Als Partitionstabelle hat sich der MBR als De-facto-Standard für Speichermedien aller Art, z. B. USB-Sticks, Speicherkarten oder externe Festplatten, etabliert und wird auch als PC-Partitionsschema bezeichnet. Seit ca. 2010 wird bei großen Speichermedien (> 2 TiB) zunehmend die GUID-Partitionstabelle (GPT-Partitionsschema) verwendet.
Als der IBM-PC entwickelt wurde, waren Speichermedien zunächst nicht in Partitionen unterteilt. Die Urfassung des IBM-PC, das Modell 5150 von 1981, hatte zwei 5¼″-160-kB-Disketten-Laufwerke. Das mitgelieferte Betriebssystem PC DOS 1.0 (MS-DOS 1.14) konnte dann auch nur mit dieser speziellen Konfiguration umgehen.

Das BIOS, eine Neuentwicklung für den IBM-PC, erwartete an Position CHS 0:0:1 (Spur 0, Kopf 0, Sektor 1) den 512 Bytes großen Bootsektor, lud diesen in den Speicher und führte ihn aus. PC DOS 1.0 enthält an dieser Stelle einen Bootsektor, der von dieser fixen Konfiguration ausgeht: 160-kB-Speichermedien mit 8 Sektoren pro Spur.

Mit Erscheinen des IBM-PC XT 1983 wurden neuere Disketten-Laufwerke und erstmals auch eine Festplatte eingeführt. Der Bootsektor musste also angepasst werden, da nicht mehr von einer fixen Konfiguration ausgegangen werden konnte. Mit PC DOS 2.0 (MS-DOS 2.0) wurde daher für Disketten der ' (VBR) eingeführt, der einen ' (kurz BPB) für die unterschiedlichen Diskettenformate enthält und vom Code im VBR ausgewertet wird. Somit konnten auch Disketten mit 9 Sektoren pro Spur für die neuen 360-kB-Laufwerke verwendet werden und weitere zukünftige Diskettengeometrien wurden dadurch ermöglicht. Für die Festplatte wurde der " eingeführt, der nun eine Partitionierung erlaubte. Eine weitere der Einführung von Festplatten geschuldete Neuerung waren Unterverzeichnisse, die mit dem Backslash „\“ getrennt wurden. (Siehe FAT-Dateisystem).

Aus Kompatibilitätsgründen wurde die Konvention, den Bootsektor in den Speicher zu laden und auszuführen, beibehalten und sollte bei Datenträgern mit mehreren Partitionen nicht verletzt werden. Die Partitionstabelle ist somit innerhalb des Bootsektors platziert. Die Funktion des Bootloaders ist derart modifiziert, dass der Programmcode im MBR zunächst nur die enthaltene Partitionstabelle auswertet und im Chainloading-Prinzip den eigentlichen Bootloader der aktiven Partition lädt und ausführt. Die Firmware (das BIOS) des Computers braucht daher nichts von Partitionen zu wissen.

Der " (MBR) besteht bei startfähigen Datenträgern aus zwei Teilen:

Auf IBM-PC-kompatiblen Computern wird die Partitionstabelle im Normalfall vom Startprogramm ausgewertet – dieses übernimmt die Funktion eines Bootloaders für BIOS-basierte x86-Computer.

Auf Datenträgern, von denen kein Betriebssystem gestartet werden kann oder soll, wird von einem bereits laufenden System nur die Partitionstabelle verwendet – ein eventuell vorhandenes Startprogramm wird dabei nicht genutzt. Dies ist zum Beispiel der Fall, wenn ein USB-Stick, der eine MBR-Partitionstabelle enthält, an einem Computer oder einem anderen Gerät mit USB-Schnittstelle verwendet wird.

Bei der GUID-Partitionstabelle (), die bei modernen Systemen zunehmend den MBR als dessen Nachfolger ersetzt, wird der aus Kompatibilitätsgründen enthaltene MBR zum Schutz mit einer den gesamten Speicherbereich belegenden Partition erstellt, damit die in den darauffolgenden Datenblöcken vorhandene GUID-Partitionstabelle und die darin definierten Partitionen nicht unbeabsichtigt überschrieben werden ().

Um mit dem historischen IBM-PC kompatibel zu bleiben, wird von einem BIOS im Zuge des Startens () normalerweise der erste Sektor eines angeschlossenen Speichermediums gelesen und ausgeführt. Dieser Sektor wird daher auch als Bootsektor bezeichnet. Beim ' übernimmt das Startprogramm die Kontrolle, indem es die Partitionstabelle auswertet und ein ausgewähltes Betriebssystem startet. Im Standard-MBR von MS-DOS/PC DOS 2.0 ist dies der ', der die erste aktive (als bootfähig gekennzeichnete) Partition findet und anschließend den Bootsektor dieser Partition ebenfalls lädt und ausführt.

Im Allgemeinen ist es daher nicht das BIOS, das die aktive Partition sucht und den Bootsektor (VBR) dieser Partition startet. Einige Hersteller entwickelten jedoch BIOS-Varianten mit zusätzlichen Funktionen, wie einem Auswahlmenü, um das Starten von einer beliebigen Partition zu erlauben; auch Funktionen, die weitere herstellerspezifische Programme von einer Partition nachladen, sind verbreitet. Dies bietet beispielsweise IBM/Lenovo auf vielen Laptops der ThinkPad-Reihe in Form einer „Servicepartition“, deren Programme direkt aus dem BIOS heraus gestartet werden können. Bei vielen Compaq-Modellen wiederum ist das BIOS-Setup nur über externe Programme erreichbar: Die Taste , die vom Anwender während der BIOS-Phase gedrückt werden muss, um das BIOS zu konfigurieren, ist dabei nur dann verfügbar, wenn auch ein nur dafür vorgesehenes DOS-Betriebssystem inklusive Setup- und Diagnoseprogrammen in einer speziellen Partition (Typ codice_1) auf der ersten Festplatte vorhanden ist. Alternativ lässt sich dieses BIOS nur noch über eine Startdiskette mit den entsprechenden Programmen konfigurieren.

Obwohl der IBM-PC ursprünglich jede Art von Bootcode unterstützte, prüfen manche BIOS-Varianten vor dem Start, ob eine gültige Partitionstabelle vorhanden ist. Wenn ein PC-System nur den MBR verwendet, merkt der Anwender davon in fast allen Fällen nichts, doch führt dies z. B. bei der Verwendung von GPT-formatierten Datenträgern dazu, dass der Startvorgang durch jenes BIOS unterbrochen wird oder ein Starten dadurch sogar grundsätzlich unmöglich ist. Auch können auf solchen BIOS-Varianten keine Betriebssysteme gestartet werden, die keinen MBR verwenden oder benötigen (wie z. B. FreeBSD oder Eigenbau-Betriebssysteme). Selbst wenn ein gültiger MBR vorhanden ist, prüfen manche BIOS-Varianten zusätzlich, ob eine aktive Primärpartition existiert und ob diese einen Bootsektor enthält. Dieses Verhalten führt dazu, dass ein Betriebssystem, dessen Bootloader im MBR selbst installiert wird und das deswegen keine Partition als aktiv markiert, nicht starten kann und das BIOS stattdessen eine meist uneindeutige Fehlermeldung ausgibt.

Neben der Bezeichnung "" finden sich auch folgende alternative Bezeichnungen für den MBR:

Der Bootloader des "Master Boot Record" ist ein kleines Programm, das vom BIOS aufgerufen wird. Dieses lädt den Bootloader immer im 8086-kompatiblen 16-Bit-Modus . Beim Standard-MBR von MS-DOS/PC DOS 2.0 heißt dieses Startprogram ", sucht in der Partitionstabelle nach „sichtbaren“ und „aktiven“ Partitionen und lädt anschließend den Bootsektor der ersten aktiven Partition und führt diesen aus. Im Chainloading-Verfahren wird dann das eigentliche Betriebssystem geladen.

Der ' wurde bei jedem Betriebssystem von Microsoft ein wenig verändert, entspricht aber im Wesentlichen immer noch dem ursprünglichen Programmcode von MS-DOS 2.0. Da er grundsätzlich jedes Betriebssystem auf der ersten aktiven Partition starten kann, wird ein ' mit enthaltenem " auch als Standard-MBR bezeichnet.

Der Bootloader kann jedoch durch speziell dafür vorgesehene Programme ersetzt bzw. erweitert werden. Populär sind vor allem Bootmanager, die das vorher enthaltene Startprogramm auslagern und im MBR ersetzen, um stattdessen ein Auswahlmenü oder ähnliches anzuzeigen und so z. B. das Starten von beliebigen Partitionen ermöglichen. Auch gibt es , welche BIOS-Limitierungen durch Geometrieumsetzung umgehen sollen (z. B. um bereitzustellen). Derartige Programme nutzen oft eine eigene Partition oder einen reservierten (unpartitionierten) Bereich auf der Festplatte, von dem weitere Dateien oder weiterer Programmcode nachgeladen wird.

Auch Bootviren nutzen den Bootloader des MBR, um vor dem eigentlichen Betriebssystemstart bereits aktiviert zu sein.

Ein fehlender oder defekter Bootloader führt dazu, dass vom Datenträger überhaupt nicht gestartet werden kann, selbst wenn in einer der Partitionen ein Betriebssystem mit intaktem Bootsektor vorhanden ist. Da einige Bootloader zusätzliche Dateien nachladen, liegt ein Defekt auch dann vor, wenn zwar der MBR und die Partition des Betriebssystems intakt sind, aber die Dateien des Bootloaders fehlerhaft sind oder fehlen.

Da das Startprogramm () nur zum Starten () eines IBM-PC-kompatiblen Computers vorhanden sein muss, wird es normalerweise bei der Installation eines Betriebssystems in den MBR geschrieben. Dies kann auf unterschiedliche Weise geschehen. DOS und Windows beispielsweise schreiben den "Master Boot Code" immer auch beim Erstellen einer Partitionstabelle durch das Dienstprogramm codice_2. Ist jedoch bereits ein MBR vorhanden, so wird mit codice_2 normalerweise der nicht verändert, bei der Installation eines Betriebssystems hingegen meistens schon. Bei den meisten Linux-Distributionen wird der Bootloader im MBR nur durch das Installieren eines spezifischen Linux-Bootloaders initiiert, wenn für diesen als Installationsort der MBR angegeben wurde.

Verbreitete Startprogramme im MBR sind:

Unter DOS kann der MBR bei nicht partitionierten Datenträgern mit dem zu DOS gehörigen codice_4 angesehen und bearbeitet werden. Wenn der Datenträger dagegen partitioniert ist, so kann mit codice_4 nur der Boot-Sektor (Boot Record) der jeweiligen Partition angesehen werden. Um auf den MBR zuzugreifen, wird ein Diskeditor (wie z. B. Diskedit.exe, PTEdit.exe etc.) benötigt. MS-DOS und die auf MS-DOS aufbauenden Windowsvarianten (Windows 95 bis Windows Me) überschreiben den Bootloader im MBR bei der Installation ohne Rücksicht auf seinen bisherigen Inhalt.

Im Falle eines Fehlers im MBR kann unter DOS mit dem undokumentierten Befehl codice_6 ein neuer Standard-Masterbootrecord geschrieben werden. Dabei wird der gesamte MBR mit Ausnahme der Partitionstabelle überschrieben, wodurch Bootviren entfernt werden können, sofern das Virus nicht den Schreibzugriff auf den Master Boot Record erkennt und abfängt. Allerdings wird bei diesem Vorgehen auch ein eventuell vorhandener Bootmanager entfernt, da seine Anweisungen durch den Standard-Code von MS-DOS überschrieben werden.

Einige Microsoft-Windows-Versionen der NT-Linie, u. a. XP, nicht jedoch NT 4 und 2000, überschreiben den Bootloader (die ersten 446 Bytes des MBR) bei jeder Neuinstallation zwar auch, berücksichtigen aber noch vorhandene Informationen über ein älteres Microsoft-Betriebssystem, also MS-DOS und andere darauf basierende Windows-Versionen. In diesem Fall wird der NTLDR mit Auswahlmöglichkeit zwischen den verschiedenen installierten Microsoft-Betriebssystemen vorkonfiguriert.

Für Windows 2000, Windows XP und Windows Server 2003 gibt es die Wiederherstellungskonsole, hier dient der Befehl codice_7 zur Reparatur des MBR (mit Ausnahme der Datenträgersignatur) und codice_8 zur Reparatur des Bootsektors einer Partition (beide codice_9-Befehle nur für x86-Systeme).

Im Gegensatz zu älteren NT-basierten Windows-Systemen startet Vista aus dem Bootsektor den Bootloader „BOOTMGR“ (unter NT/XP war dies „ntldr“). BOOTMGR entnimmt die Informationen zum bootenden Betriebssystem nicht der Datei "boot.ini", sondern der Datei \Boot\BCD, welche entweder auf der EFI-System-Partition liegt oder direkt auf der Systempartition bei BIOS-Systemen.

Zum Finden und Adressieren des Startlaufwerks verwendet BOOTMGR die Datenträgersignatur (Bytes 440–443 im MBR) und den Laufwerkstartoffset (in Bytes). Beide Werte sind in der BCD-Datei als BcdDeviceData-Elemente mit der Bezeichnung "X1000001" gespeichert. Zum Ermitteln des Startlaufwerks vergleicht BOOTMGR die Datenträgersignatur aller erkannten Datenträger mit den gewählten Booteinträgen. Wurde die Datenträgersignatur nicht gefunden bzw. verändert, verweigert Windows den Start mit einem „winload error "0xc000000e"“.
Hat man die Startpartition verschoben oder ist der Laufwerkstartoffset aus einem anderen Grund fehlerhaft, quittiert BOOTMGR das mit „winload error "0xc0000225"“.

Die BCD-Datei ist im Gegensatz zur boot.ini keine einfache Textdatei, sondern eine Binärdatei (genauer vom Aufbau her ein Registry Hive) und kann nicht mit einem normalen Texteditor bearbeitet werden. Neben Regedit und dem programmiertechnischen Zugriff auf die BCD-Daten per WMI-Interface ist BCDEdit.exe zurzeit das einzige von Microsoft bereitgestellte Tool, das eine umfassende Bearbeitung der BCD-Datei ermöglicht. Es kann in einem laufenden Windows (oder von einer entsprechenden Windows-PE-CD) an der Kommandozeile aufgerufen werden.

Alternativ kann beim Booten von der Windows-CD über den Punkt „Computer reparieren/Systemwiederherstellungsoptionen“ das Tool codice_10 in der Eingabeaufforderung aufgerufen werden. codice_11 schreibt einen neuen MBR, codice_12 einen neuen Startsektor in die Systempartition.
Eine Drittanbieter-Alternative ist das Tools EasyBCD, das eine Bearbeitung der BCD-Datei mittels grafischer Oberfläche ermöglicht.

Linux-Distributionen oder Solaris installieren meist den Bootlader GRUB, seltener den älteren LILO, SYSLINUX oder andere. GRUB ist in der Lage, verschiedene Betriebssysteme (z. B. Linux, BSD, macOS oder Windows) zu starten.

Beim Systemstart wird zuerst die „Stage 1“ des GRUB aus dem MBR eingelesen und ausgeführt. Sie hat nur die Aufgabe, die sogenannte „Stage 2“ zu finden. Entweder wird der Sektor des Datenträgers, in dem die „Stage 2“ beginnt, in der „Stage 1“ hinterlegt oder es wird die Partitionsnummer und der Dateipfad angegeben. Im ersten Fall wird direkt die „Stage 2“ geladen. Im zweiten Fall wird zuerst die „Stage 1.5“, die aus einem Dateisystem-Treiber besteht, aus dem Bootsektor einer Partition geladen. Die „Stage 2“ liegt bei unixoiden Betriebssystemen normalerweise in der Datei "/boot/grub/stage2". Die „Stage 2“ liest die Konfiguration aus "/boot/grub/menu.lst" oder "/boot/grub/grub.cfg" ein, zeigt ein Bootmenü an und lädt anschließend den Kernel und sein Image oder startet den Bootloader eines anderen Betriebssystems, wie Windows.

Bei LILO werden alle zum Booten nötigen Daten direkt in den MBR geschrieben. LILO startet immer direkt den ersten Sektor des Kernels. Die LILO-Konfigurationsdatei ("/etc/lilo.conf") dient einzig dem Programm "/sbin/lilo" dazu, einen MBR zu erzeugen. Am Start des Systems ist nur der von "/sbin/lilo" erzeugte MBR beteiligt. Durch dieses einfache Konzept geht einiges an Flexibilität verloren. So muss bei jeder Änderung am Kernel oder an den Bootoptionen ein neuer MBR geschrieben werden. Bootmanager wie GRUB oder Syslinux werten jedes Mal beim Booten ihre Konfigurationsdatei aus und sind zum Teil sogar in der Lage, diese während des Bootmenüs zu modifizieren.

Mit dem Befehl codice_13 (bei LILO: codice_14) wird die „Stage 1“ von GRUB in den MBR geschrieben und, falls vorgesehen, die „Stage 1.5“ in den Bootsektor einer Partition. Unter Linux kann man den Code-Teil (samt Partitionstabelle) des MBR (s.o) normalerweise mit dem Befehl codice_15 ("/dev/hda" ursprünglich für den ersten IDE-; "/dev/sda" für den ersten SCSI-, S-ATA-, IEEE-1394- oder USB-Datenträger, heute auch für IDE) in einer Datei als Sicherungskopie speichern. Zum Wiederherstellen genügt analog codice_16. Da dieses Vorgehen aber den "gesamten" ersten Sektor der Platte neu schreibt, geht eine zwischenzeitlich geänderte Partitionierung wieder verloren! Daher empfiehlt es sich, nach jeder Umpartitionierung eine neue Backup-Kopie des MBR zu erstellen.

Löschen kann man den GRUB- oder LILO-MBR mit dem Befehl codice_17. Dabei wird der Boot-Code des MBR und die Partitionstabelle (Bestandteil des 512 Bytes großen Sektors!) komplett mit Nullen überschrieben. Soll die Partitionstabelle jedoch erhalten bleiben und nur der eigentliche Bootcode überschrieben werden, darf man nur die ersten 440 Bytes überschreiben, z. B.: codice_18.

Auch die Installation eines anderen Bootloaders (z. B. NTLDR) löscht in der Regel den Bootcode des zuvor installierten Systems vollständig, lässt die Partitionstabelle jedoch unangetastet.

Der Zusatz codice_19 in den gegebenen Beispielen wirkt sich nur aus, wenn das Ziel eine normale (schon früher erstellte) Image-Datei ist. Diese würde andernfalls ohne diesen Zusatz nach dem letzten geschriebenen Byte abgeschnitten werden. Bei Block-Devices (also Festplatten, Partitionen) als Ziel wirkt die Option sich nicht aus.

Die Datenträgersignatur (auch "Festplattenidentifikation", engl. "Disk Signature" bzw. "Disk identifier") wird von Windows 2000 und XP verwendet, um Datenträger, die mit einer klassischen Partitionstabelle versehen sind (von Microsoft "Basisdatenträger" genannt), eindeutig zu identifizieren. Dadurch ist die Datenträgersignatur z. B. bei der Zuordnung von Laufwerksbuchstaben zu Partitionen von entscheidender Bedeutung: Während frühere Windows-Versionen auf die von MS-DOS ermittelten Laufwerksbuchstaben zurückgreifen, sind diese Zuordnungen ab Windows 2000 in Registry-Schlüsseln, u. a. HKLM\SYSTEM\MountedDevices, gespeichert. Bei bestimmten Systemkonstellationen wird die Datenträgersignatur auch in der Datei codice_20 verwendet, die Teil des Bootloaders von Windows NT (NTLDR) ist. Windows vergibt die Datenträgersignatur bei der Initialisierung eines neuen Datenträgers.

Insbesondere nach Datensicherungen können Probleme auftauchen, die mit der Datenträgersignatur zusammenhängen:


Der Befehl codice_7 in der Wiederherstellungskonsole von Windows XP lässt die Signatur unverändert. Dagegen frischt der Befehl codice_6, der den Boot-Code neu schreibt, auch die Datenträgersignatur auf.

Der Linux-Befehl codice_23"<nowiki><Datenträger></nowiki>"codice_24 überschreibt die Datenträgersignatur mit Nullen, wodurch Windows dazu gebracht wird, eine neue Datenträgersignatur zu erzeugen.

Die Partitionstabelle gibt Auskunft über die Partitionen auf einem Datenträger. Ist sie fehlerhaft oder fehlt sie gar vollständig, ist der Zugriff auf die Daten der Partitionen nicht ohne weiteres möglich. In vielen Fällen lässt sich jedoch die Partitionstabelle mit Programmen wie TestDisk wiederherstellen. Der Eintrag jeder Partition enthält zwei CHS-Adressen (die Start- und die Endadresse) sowie eine 32-Bit-lange LBA-Adresse und die 32-Bit-lange LBA-Partitionsgröße. Dadurch lassen sich mit herkömmlichen Partitionstabellen über CHS-Adressierung maximal etwas über 8 GB (ca. 7,8 GiB) große Datenträger und über LBA-Adressierung etwas über 2 TB (knapp 2 TiB) große Partitionen bzw. etwas über 4 TB (knapp 4 TiB) große Datenträger nutzen (wenn vom Betriebssystem unterstützt). Um größere Datenträger oder Partitionen ansprechen zu können, muss die Nachfolgetechnologie GUID Partition Table (GPT) genutzt oder die Sektorengröße vergrößert werden.

Die Tabelle befindet sich an Byte 446 (1BE) des jeweiligen Sektors und hat maximal 4 Einträge à 16 Byte, sie ist also 64 Byte groß. Daran anschließend folgt die Magic Number, die aus den beiden Byte "55" und "AA" in den Byte 510 (1FE) und 511 (1FF) des Sektors besteht.

Die nebenstehende Tabelle veranschaulicht die Lage der Partitionstabelle (innerhalb des Bootsektors). Die vier Partitionseinträge sind farblich hervorgehoben. Gelb, grün, cyan und blau sind die Einträge für die vier primären Partitionen. Die Signatur ist rot hinterlegt.
Ein solcher Eintrag ist wie folgt gegliedert:

Unbenutzte Einträge sind mit Nullen gefüllt. Die Einträge sind im Little-Endian Format abgespeichert.

Die CHS-Einträge sind wie folgt kodiert:
Dabei werden die Zylinder von codice_25 bis codice_26 (1024 Zylinder), die Köpfe von codice_25 bis codice_28 (256 Köpfe) und die Sektoren von codice_29 bis codice_30 (63 Sektoren) gezählt.

Die Adressierung durch CHS (1024 Zylinder × 256 Heads × 63 Sektoren × 512 Bytes) ergibt eine maximal adressierbare Größe von 8.455.716.864 Bytes und erlaubt somit eine eindeutige Adressierung nur bei Festplatten bis ca. 8,4 GB bzw. 7,8 GiB. Bei solchen (vergleichsweise kleinen) Platten kann also eine Konvertierung von CHS nach LBA vorgenommen werden. Heutige Festplatten haben aber in der Regel deutlich größere Kapazitäten. Die CHS-Werte erhalten daher ihre Maxima vereinfacht mit „1023, 63, 255“, die tatsächliche Datenträgergröße wird jedoch durch das Logical Block Addressing (LBA) angegeben. Aus der Begrenzung der Sektorangabe im „IBM-PC-kompatiblen“ Partitionseintrag auf 32 Bit und der dort üblichen Sektorgröße von 512 Bytes ergibt sich eine maximale Partitionsgröße von knapp 2 TiB ((2−1) × 512 Bytes). Unterteilt man einen Datenträger in mehr als nur eine Partition und lässt die letzte Partition mit maximaler Partitionsgröße auf dem letzten LBA-Sektor beginnen, so lässt sich eine 4,4 TB (ca. 4 TiB) große Festplatte fast vollständig benutzen. Bedingung ist aber, dass die vorherige(n) Partition(en) in der Summe noch nicht an die 2 TiB–Grenze stoßen, der letzte adressierbare LBA-Sektor somit noch frei für die letzte Partition ist, weil sonst das Erstellen dieser letzten Partition fehlschlägt.

Ältere Betriebssysteme können Beschränkungen der Datenträgergröße aufweisen. Beispielsweise ist Microsoft Windows XP 32 auf knapp 2,2 TB ((2−1) × 512 Bytes) beschränkt. Ein größerer Datenträger wird auch im unpartitionierten Zustand mit falscher Größe ausgewiesen. Der ausgelöste Überlauf lässt den z. B. ca. 3 TB großen Datenträger nur ca. 800 GB groß scheinen. Schreibzugriffe könnten Datenverluste zur Folge haben.

Bei größeren logischen Sektorengrößen erhöhen sich diese Grenzen entsprechend (z. B. bei 4-KB-Sektoren auf 32 TiB bzw. 16 TiB), allerdings können ältere Betriebssysteme wie z. B. Windows XP nur von Datenträgern mit 512-Byte-Sektoren booten.

Der Eintrag an Offset 0x04 steht u. a. für:

Bei einem am PC partitionierten Medium wird die primäre Partitionstabelle vor der Signatur am Ende des ersten Sektors (Master Boot Record) abgelegt.
Die Partitionen in der Partitionstabelle des MBR heißen "Primärpartitionen". Eine einzige Partition im MBR kann jedoch auch als erweitert (extended, Typ 5 oder F) markiert sein. Diese verweist dann im Eintrag Startsektor auf die erste "erweiterte Partitionstabelle".

Diese befindet sich im ersten Sektor der erweiterten Partition. Jede erweiterte Partitionstabelle definiert genau eine "logische Partition" und verweist bei Bedarf auf die nächste erweiterte Partitionstabelle. Die erweiterten Partitionstabellen funktionieren nach dem Prinzip der verketteten Liste, daher sind hinter den primären Partitionen beliebig viele logische Partitionen möglich. Als Länge des Eintrags der Typ-5-Partition im MBR ist die Summe aller verketteten logischen Partitionen eingetragen. Da immer ein Sektor von der erweiterten Partitionstabelle eingenommen wird, können erweiterte und logische Partition nicht exakt gleich groß sein.

In einer erweiterten Partitionstabelle werden somit maximal die ersten zwei Einträge benutzt. Die Startsektoren werden hier nicht mehr relativ zum Anfang der Festplatte angegeben:

Alte Betriebssysteme erwarten den Start einer Partition immer an den Zylindergrenzen. Daher ergibt sich auch heute noch bei verbreiteten Betriebssystemen eine Lücke von 63 Sektoren zwischen erweiterter Partitionstabelle und dem Startsektor der entsprechenden logischen Partition. Diese Sektoren in der Lücke können z. B. für einen Bootmanager oder zum Verstecken von „geheimen“ Daten, aber auch von Bootsektorviren verwendet werden. Auch wurde dort früher eine BIOS-Erweiterung untergebracht, um auch Festplatten mit mehr als 8 GiB zu unterstützen, falls das BIOS des Mainboards dazu nicht imstande war.

Aufgrund der Entwicklung hin zu 4-kiB-Sektoren ist aber ein Alignment auf 63 Sektoren äußerst ungünstig. Daher wird neuerdings eine Lücke von 64 verwendet. Auch andere Zweierpotenzen sind gängig.

Die Signatur (auch Magische Zahl genannt) besteht aus den 2 Byte 55 und AA. Auf Little-Endian-Systemen wird dies als 16-Bit-Zahl AA55 interpretiert. Ist die Signatur vorhanden, so geht das BIOS davon aus, dass ein gültiger Bootsektor vorhanden ist. Wird die Signatur nicht gefunden, vermutet das BIOS einen neuen bzw. gelöschten Datenträger. Der Bootvorgang wird dann abgebrochen und eine Fehlermeldung, etwa „Non-System or Non-Bootable Disk“ ausgegeben. Natürlich ist eine korrekte Signatur keine Garantie für einen vorhandenen und funktionierenden Boot-Code. Sie dient lediglich dazu, zu verhindern, dass „leere“ Bootsektoren oder Zufallsdaten ausgeführt werden.

Auch bei Datenträgern, die nicht zum Booten verwendet werden, kann eine fehlende Signatur Auswirkungen haben. Es gibt BIOS-Versionen, die Datenträger ohne gültige Signatur in einem langsameren Modus betreiben. Vor der Geschwindigkeitsmessung (Benchmark) z. B. eines neuen Datenträgers empfiehlt sich daher, zunächst den MBR der betreffenden Platte zu initialisieren.

Bei EFI-basierten Computern kommt anstelle der MBR-Partitionstabelle die neuere "GUID Partition Table" (GPT) zum Einsatz. Aus Gründen der Abwärtskompatibilität enthält jede GPT einen sogenannten Schutz-MBR. Dieser sorgt dafür, dass Betriebssysteme bzw. Programme, die noch nicht mit GPT zurechtkommen, statt eines leeren Datenträgers eine einzige Partition sehen, die über den ganzen Datenträger geht. Dieser Schutz-MBR lässt sich außerdem dazu benutzen, MBR-basierte Betriebssysteme auf EFI-basierten Computern zu installieren, indem die Partitionen der GPT auf die MBR-Partitionstabelle abgebildet werden. Diesen Trick benutzt Apple bei seiner Boot-Camp-Software, die die Installation von nicht GPT-kompatiblen älteren Windows-Versionen auf Intel-basierten Macs erlaubt. Auch die EFI-Applikation "rEFIt" kann mit dem Programm "gptsync" GPT und MBR-Partitionstabelle synchronisieren.



</doc>
<doc id="3422" url="https://de.wikipedia.org/wiki?curid=3422" title="Marktzins">
Marktzins

Der Marktzins () ist im Finanzwesen die allgemeine Bezeichnung für den Zinssatz auf einem Finanzmarkt.

Da der Zinssatz allgemein als Preis gilt, ist der Marktzins der Preis auf einem Finanzmarkt. Die Höhe des Marktzinses hängt von Laufzeit, Marktliquidität und Währung eines Finanzprodukts und der im Rating zum Ausdruck kommenden Bonität des Emittenten ab, im Marktzins ist also stets eine Risikoprämie eingepreist. Zudem spielen die Zinserwartungen der Marktteilnehmer im Hinblick auf die Marktentwicklung des Marktzinses eine Rolle. Aufgrund seines objektiven Zustandekommens durch die Gesetze des Marktmechanismus' dient er auch außerhalb der Finanzwirtschaft bei Nichtbanken als Bezugswert.

Die Märkte, auf denen Zinsen in Erscheinung treten, sind primär Märkte für Geld, Kredit und Kapital und sekundär für Waren (Commodities), mit deren Handel finanzwirtschaftliche Transaktionen verbunden sind. Der Zins stellt hier einen auf die Laufzeit abgestellten Preis für die Nutzung von Geld, Kredit oder Kapital (oder Waren beim Warenkredit) dar. 

Zu den Finanzmärkten gehört der Geld- und der Kapitalmarkt. Entsprechend gibt es Geldmarktzinsen und Kapitalmarktzinsen. Deshalb existiert kein einheitlicher Marktzins, weil das Zinsniveau auf diesen Marktsegmenten im Regelfall unterschiedlich ist. Auf dem Geldmarkt bestehen als Marktzins die Zinssätze für den Tages- und Termingeldhandel oder für Geldmarktpapiere. Als wichtige Geldmarktzinsen gelten:
Insbesondere auf den Geldmärkten ist der Marktzins abhängig vom Leitzins der jeweiligen Zentralbank, im Eurosystem vom Hauptrefinanzierungsinstrument.

Der Kapitalmarkt kennt als Marktzins die Dividendenrendite (Aktien) und bei Anleihen die Emissionsrendite (Primärmarkt) oder die Umlaufrendite (Sekundärmarkt).

Geld- und Kapitalmarktzinsen weisen außer der Höhe des Zinsniveaus auch weitere Unterschiede auf. Eine Untersuchung der deutschen Geldmarktzinsen im Zeitraum zwischen Januar 1996 und Januar 2006 zeigte fast jede Woche eine Marktzinsänderung, wobei die Häufigkeit der Änderungen mit zunehmender Zinsbindung zunahm. Die Zinszyklen bei Kapitalmarktzinsen hingegen zeigten sich im Vergleich zu den Geldmarktzinsen im selben Zeitraum als weniger stark ausgeprägt. Während die Marktzinsänderungsdauer auf dem Geldmarkt laufzeitabhängigen Schwankungen unterworfen war, konnte auf dem Kapitalmarkt kein Zusammenhang zwischen Zinsbindung und Anpassungshäufigkeit festgestellt werden. 

Der schwedische Ökonom Knut Wicksell unterschied 1898 zwischen dem „natürlichen Zins“ und dem Marktzins („Geldzins“). Für ihn war der „natürliche Zins“ ein Zins, bei dem gerade so viel gespart wird wie Kapitalbedarf für Investitionen vorhanden ist. Der Marktzins ergab sich aus dem Geschäftsgebaren der Kreditinstitute und war ein Ungleichgewichtszins. Beide Zinsarten stimmten überein, wenn die Banken bei ihrem Geschäftsgebaren von ihrer Geldschöpfungsmöglichkeit keinen Gebrauch machten, so dass es keine Auswirkungen auf das Preisniveau der Güterpreise gebe. 

Die Vorstellungen von John Maynard Keynes ähnelten sehr Wicksells Thesen. In seinem 1930 erschienenen Buch „A Treatise on Money“ versteht Keynes unter Marktzins den Zusammenhang zwischen langfristigem Anleihezins und kurzfristigem Bankzins. Die Trägheit des Marktzinses ist nach Keynes auf die Unvollkommenheiten des Kreditmarktes zurückzuführen. Später verwendete Keynes den Marktzins in seinem Buch Allgemeine Theorie der Beschäftigung, des Zinses und des Geldes vom Februar 1936, um nachzuweisen, wann sich eine Investition noch lohnt. Wenn die Grenzleistungsfähigkeit des eingesetzten Kapitals größer als der Marktzins ist, werden sich die Unternehmer für Investitionen entscheiden und umgekehrt. Der tatsächliche Investitionsumfang wird demnach ausgedehnt, bis „es keine Klasse von Kapitalgütern mehr gibt, deren Grenzleistungsfähigkeit den aktuellen Zinssatz übersteigt“. 

Liegt ein optimaler Kapitalbestand vor, entspricht die Grenzleistungsfähigkeit des eingesetzten Kapitals formula_1 in einer Verhaltensgleichung dem Marktzins formula_2:

Bei einem Kapitalbestand unterhalb des Optimums ist die Grenzleistungsfähigkeit des eingesetzten Kapitals höher als der Marktzins und umgekehrt, es kommt solange zu Investitionen, bis die Grenzleistungsfähigkeit des Kapitals mit dem Marktzins übereinstimmt.

Die Rechtsprechung entwickelte zum Zwecke des Verbraucherschutzes zunächst Kriterien, um übermäßigen Zinsforderungen von Kreditinstituten in Kreditverträgen Einhalt zu gebieten. Die Obergrenze sollte der Zinswucher, eine Unterart des Wuchers, sein. Liegt der Tatbestand des Zinswuchers vor, so sind die betroffenen Kreditverträge nichtig mit der Folge, dass auch die übermäßigen Zinsforderungen nicht gelten. Im Jahre 1966 begann der Bundesgerichtshof (BGH) damit, den Tatbestand des Zinswuchers anzuwenden. Er hielt das Kriterium des Wuchers bei kleineren Geschäftskrediten mit einem Jahreszins von 114,3 % und 130,9 % für erfüllt. Benutzte er hier noch absolute Zinshöhen, so begann er im Juli 1982 bei Ratenkreditverträgen mit einem Vergleich zwischen Vertragszins und Marktzins. Wucher liegt demnach vor, wenn der effektive Vertragszins den Marktzins relativ um 97 % überschreitet, also ungefähr die doppelte Zinshöhe erreicht. Der BGH zieht jedoch eine berechtigte Grenze. Danach handelt es sich nicht um Zinswucher, wenn ein ungewöhnlich hohes Kreditrisiko für die vereinbarte Zinshöhe spricht.

Das Gesetz erwähnt den Marktzins etwa bei der Bewertung der Rückstellungen in Abs. 2 Satz 2 HGB, wonach diese Bilanzposition mit einer Restlaufzeit von mehr als einem Jahr mit dem entsprechenden durchschnittlichen Marktzins abzuzinsen ist. Für Pensionsrückstellungen und ähnliche langfristige Rückstellungen ist ein durchschnittlicher Marktzins mit einer Restlaufzeit von 15 Jahren zugrunde zu legen.

Der Marktzins ist als Referenzzinssatz im Kreditgeschäft der Kreditinstitute Bestandteil in Kreditverträgen aller Art (Dispositionskredite, Konsumkredite, Lombardkredite, Kontokorrentkredite, Investitionskredite, Immobilienfinanzierungen), auch im internationalen Kreditverkehr (Fazilitäten/Kreditlinien, Stand-by-Kredite, Roll-over-Kredite, Revolving Credit Facility). In Kreditverträgen ist der Marktzins (Kreditzins) näher zu definieren, insbesondere ob er als Festzins oder variabler Zins gelten soll, wie lange er als vereinbart gilt (Zinsbindungsfrist, Zinsgleitklausel) und welche Kreditmarge einem Referenzzinssatz zugeschlagen wird. Das gilt auch für alle Anleihen. Im Passivgeschäft bestimmt der Marktzins (Habenzins) die Preise für zinsorientierte Finanzprodukte (Sichteinlagen, Termingelder, Spareinlagen oder Sparbriefe). Hier gelten die gleichen Transparenzvorschriften wie im Kreditgeschäft. Bei Investitionen ist der Marktzins für Unternehmer die Vergleichsgröße bei der Ermittlung der Grenzleistungsfähigkeit des eingesetzten Kapitals. 

Häufig wird der Begriff Marktzins auch für den Vergleichswert des „marktüblichen Zinses“ verwendet. Hierunter versteht der Bundesfinanzhof (BFH) einen Zinssatz für vergleichbare Darlehen, den Banken ihren Kunden im fraglichen Zeitraum gewährten, wofür er deshalb die Zinsstatistik der Bundesbank zu Rate zieht. Veröffentlichungen verschiedener Marktzinsen gibt es beim Basiszinssatz oder bei den seit Januar 2003 einheitlich in allen EU-Mitgliedstaaten erhobenen MFI-Zinsstatistiken der Bundesbank und EZB. Diese weisen Durchschnittszinssätze aus, die sich aus den von Kreditinstituten im Kreditgeschäft angewandten Zinssätzen ergeben. Die Zinssätze werden als volumengewichtete Durchschnittssätze über alle im Laufe des Berichtsmonats abgeschlossenen Neuvereinbarungen für Einlagen und Kredite berechnet. 



</doc>
<doc id="3425" url="https://de.wikipedia.org/wiki?curid=3425" title="Mann (Begriffsklärung)">
Mann (Begriffsklärung)

Mann steht für:
Mann als Name:
Siehe auch:



</doc>
<doc id="3426" url="https://de.wikipedia.org/wiki?curid=3426" title="Moosfarne">
Moosfarne

Die Moosfarne ("Selaginella") sind eine Pflanzengattung, die zu den Bärlapppflanzen (Lycopodiopsida) gehört. Moosfarne sind die einzige Gattung der Familie der Moosfarngewächse (Selaginellaceae) und der Ordnung der Moosfarnartigen (Selaginellales). Die Gattung umfasst weltweit etwa 700 Arten. Es sind meist kleine, krautige Pflanzen, die sich durch ihre Heterosporie, das heißt verschiedenartige Sporen, auszeichnen. Der Verbreitungsschwerpunkt sind die Tropen, mit relativ wenigen Arten in den gemäßigten Zonen.

Die Moosfarne ähneln in ihrem Habitus ein wenig manchen Moosen, was sich im ersten Teil des Gattungsnamens niederschlägt. In ihren anatomischen Merkmalen und ihrer Fortpflanzung sind sie jedoch eindeutig Gefäßsporenpflanzen.

Sie besitzen meist niederliegende oder aufrechte, reich gabelig (dichotom) verzweigte Sprossachsen. Einige Arten sind rasenbildend. Wenige Arten klettern an Sträuchern empor und werden mehrere Meter hoch.

An den Gabelungsstellen des Sprosses sitzen häufig zylindrische, gestreckte, nach unten gerichtete farb- und blattlose Sprosse, die Wurzelträger oder Rhizophoren. Als Sprossachsen entstehen diese exogen (an der Oberfläche) im Gegensatz zu den endogen (im Inneren) entstehenden Wurzeln, und verzweigen sich ebenfalls gabelig. An ihren Enden stehen Wurzelbüschel.

Die Leitbündel der Sprossachse können eine zentrale Protostele bilden, aber auch eine Distele oder Siphonostele. Ein sekundäres Dickenwachstum fehlt. Sehr selten kommen bereits echte Tracheen vor, in denen die Querwände zwischen den Zellen aufgelöst sind, während die Seitenwände die charakteristischen treppenförmigen Verdickungen aufweisen. Die Endodermis einiger Arten (etwa "Selaginella kraussiana") wird von röhrenförmigen Zellen mit Casparyschen Streifen gebildet (Trabeculae).
Die Blätter sind klein und schuppenartig (Mikrophylle). Sie sitzen schraubig oder – häufiger – gegenständig in vier Zeilen an der Sprossachse. Meist gibt es zwei Reihen von kleinen Oberblättern und zwei Reihen von großen Unterblättern (Anisophyllie). Bei niederliegenden Formen weist bei den Oberblättern die Unterseite nach oben, bei den Unterblättern die Oberseite. Die Blätter haben eine unverzweigte Mittelrippe. Das Mesophyll ist nur selten in Schwamm- und Palisadenparenchym differenziert, in der Regel jedoch dem Schwammparenchym ähnlich. Die Epidermiszellen besitzen Chloroplasten, häufig nur einen schüsselförmigen pro Zelle. Auch im Mesophyll besitzen die Zellen mancher Arten nur einen schüsselförmigen Chloroplasten. In der Blattachsel, am Grund der Blattoberfläche sitzt eine kleine, häutige, chlorophyllfreie Schuppe (Ligula). Sie besitzt bei einigen Arten einen direkten Anschluss an die Tracheiden der Sprossachse und dient der raschen Wasseraufnahme nach Regenfällen.

Die Sporophyllstände sind endständig. Sie sind einfach oder verzweigt, vierkantig radiär oder dorsiventral. An jedem Sporophyll steht nur ein einzelnes Sporangium, das an der Blattachsel entspringt. Die Sporangien bilden große (weibliche) Megasporen oder kleine (männliche) Mikrosporen, die Moosfarne sind also heterospor. Beide Sorten kommen an ein und demselben Sporophyllstand vor, die Geschlechtsbestimmung erfolgt bereits in der Diplophase (diplomodifikatorische Geschlechtsbestimmung). Meist befinden sich die Megasporangien am unteren Teil des Sporophyllstandes.

Im Megasporangium entwickelt sich nur eine Megasporenmutterzelle weiter, die anderen gehen zugrunde. Aus ihr entwickeln sich durch Meiose vier Megasporen mit buckliger Wand. Bei einigen Arten ist die Zahl reduziert, so bildet "Selaginella rupestris" nur eine Megaspore pro Sporangium; bei anderen ist die Anzahl vermehrt, "Selaginella willdenowii" bildet bis zu 42. Die Sporangienwand öffnet sich bei Reife mit Klappen.

In den flach gedrückten Mikrosporangien entwickeln sich viele Mikrosporen, die nach dem Öffnen mit zwei Klappen ausgestreut werden.

Die Sporangienwand ist aus drei Zellschichten zusammengesetzt, wobei die mittlere im reifen Sporangium sehr schmal ist. Die innerste ist die Tapetenschicht, die der Ernährung der sich entwickelnden Sporen dient, jedoch erhalten bleibt und sich nicht auflöst, also ein Sekretionstapetum ist. Die Öffnung der Sporangien erfolgt durch einen Kohäsionsmechanismus entlang einer vorbezeichneten Linie. Dabei werden die Sporen ausgeschleudert. Bei einigen Arten verbleiben jedoch die Megasporen an der Mutterpflanze ("S. rupestris"), sodass die Embryos an der Mutterpflanze keimen.
Die Gametophyten sind stark reduziert und verlassen die Sporenwand nicht, beziehungsweise kaum, sie sind also endospor.

Die Mikrosporen entwickeln sich meist bereits im Sporangium weiter. Die Sporenzelle teilt sich in eine kleine, linsenförmige Zelle, die alleine das Prothallium vorstellt und meist als Rhizoide gedeutet wird, sowie in eine große Zelle, aus der das einzige Antheridium entsteht. Sie bildet acht sterile Wandzellen und zwei oder vier zentrale spermatogene Zellen. Letztere teilen sich noch mehrmals und bilden die sich abrundenden Spermatozoid-Mutterzellen. Die Wandzellen lösen sich auf und bilden eine Schleimschicht, in deren Mitte sich die Spermatozoid-Mutterzellen befinden. Die Prothalliumzelle bleibt allerdings erhalten. Das Ganze ist weiterhin von der Sporenwand umgeben. Bricht die Mikrosporenwand auf, entlassen auch die Mutterzellen die Spermatozoiden. Diese sind keulenförmig, schwach gekrümmt und besitzen zwei lange Geißeln. Pro Mikrospore werden 128 bis 256 Spermatozoiden gebildet.

Die Megasporen beginnen ihre Entwicklung teilweise auch schon im Sporangium. Die Entwicklung unterscheidet sich dabei teilweise je nach Art. Eine Megaspore hat etwa den zehnfachen Durchmesser einer Mikrospore. Die weiblichen Prothallien sind daher weniger stark reduziert als die männlichen. Der Zellkern befindet sich zunächst am Scheitel der Spore. Er teilt sich durch freie Kernteilungen in viele Tochterkerne, die sich dem Wandplasma entlang nach unten verteilen. Nach oder noch während dieses Vorgangs beginnt vom Scheitel her die Bildung von Zellwänden. Es entstehen zunächst große Prothallienzellen, die sich weiter in kleine Zellen teilen. Am Scheitel werden dann einige wenige Archegonien gebildet. Die Bildung der Archegonien findet meist erst statt, wenn die Sporen das Sporangium verlassen haben. Die Sporenwand platzt dann am Scheitel entlang der drei Sporenkanten auf und das Prothallium quillt etwas über den Sporenrand hinaus. Das Prothallium ist und bleibt chlorophyllfrei. Es bildet drei Höcker, an denen Rhizoiden sitzen. Deren Funktion ist die Aufnahme von Wasser und auch die Verankerung des Prothalliums. Bei manchen Arten (etwa "Selaginella galeotti") sind diese Höcker weit aus der Hülle ragende „Arme“. Ein oder zwei Archegonien werden befruchtet. Dazu ist wie bei allen Gefäßsporenpflanzen die Anwesenheit von Wasser nötig, damit die Spermatozoiden zur Eizelle im Archegonium gelangen können.

Nach der Befruchtung teilt sich die Zygote in zwei Zellen: die obere vergrößert sich stark, teilt sich im unteren Bereich noch einige Male und entwickelt sich zum Embryoträger (Suspensor). Die untere Zelle entwickelt sich zum eigentlichen Embryo, der sehr bald in ein Blattpaar, den Sprossscheitel, die Wurzel und den Fuß gegliedert ist. Bereits das erste Blattpaar im Embryo besitzt die oben beschriebenen Ligulae.

Der Suspensor schiebt den Embryo in das Prothallium hinein, das der Ernährung dient. Mit dem Fuß nimmt der Embryo die Nährstoffe auf. Der Sprossscheitel wächst nach oben, die Wurzel nach unten aus der Megaspore heraus; der junge Keimling bleibt zunächst noch mit dem Fuß im Prothalliumgewebe verankert. Er ähnelt damit einer keimenden Samenpflanze.
Die Moosfarne haben ihren Verbreitungsschwerpunkt in den feuchten Tropenwäldern und wachsen als Bodendecker. Einige Arten kommen auch in gemäßigten Breiten vor. Die mitteleuropäischen Arten wachsen vor allem in lückigen Rasen, an Felsen und Mauern.
Wenige Arten sind an Trockenstandorte angepasst. Ein Beispiel dafür ist die Unechte Rose von Jericho ("Selaginella lepidophylla") aus Mittelamerika, deren Sprosse sich bei Trockenheit einrollen.

Die Moosfarne sind eine Ordnung der Bärlapppflanzen. Diese sind die basale Gruppe innerhalb der Gefäßpflanzen, das heißt, die Farne sind mit den Samenpflanzen näher verwandt als mit den Bärlapppflanzen. Innerhalb der Bärlapppflanzen haben genetische Studien gezeigt, dass die Moosfarne die Schwestergruppe der Isoetales (Brachsenkrautartige) sind. Das Kladogramm zeigt die Verwandtschaftsverhältnisse innerhalb der rezenten Bärlapppflanzen:

Die Gattung "Selaginella" beinhaltet 690 Arten (Stand: Oktober 2015). Die größte Formenmannigfaltigkeit findet sich in den Tropen. In Europa kommen nur fünf Arten vor, von denen nur die ersten beiden in Mitteleuropa heimisch sind.
Die beiden folgenden Arten werden in Europa kultiviert und verwildern gelegentlich:


Die ältesten Fossilien der Selaginellales sind aus dem Karbon vor 300 Millionen Jahren bekannt. "Selaginellites" war bereits heterospor und sah den heutigen Arten sehr ähnlich. Ansonsten ist die fossile Überlieferung dieser krautigen Sippe sehr spärlich.

Der Name "Selaginella" ist die Verkleinerungsform von lateinisch "selago" = Tannen-Bärlapp, in Anlehnung an das ähnliche Erscheinungsbild.

Die Moosfarne haben keine wesentliche wirtschaftliche Bedeutung. Einige Arten werden als immergrüne Zierpflanzen kultiviert und vor allem kurz vor Silvester parallel zum Glücksklee als sog. "Glücksmoos" in den Handel gebracht, etwa "Selaginella martensii", "Selaginella willdenowii", "Selaginella kraussiana" und "Selaginella uncinata". Etliche werden als Hängepflanzen genutzt. Da sie frostempfindlich sind, werden sie meist nicht im Freien gezogen. Auch die Unechte Rose von Jericho wird im Handel angeboten.




</doc>
<doc id="3436" url="https://de.wikipedia.org/wiki?curid=3436" title="Muse (Mythologie)">
Muse (Mythologie)

Die Musen ( "Mousai," Einzahl "Mousa") sind in der griechischen Mythologie Schutzgöttinnen der Künste. Die Überlieferung der uns heute bekannten neun Musen stammt von Hesiod.

Noch in der homerischen "Odyssee" wird die Muse im einsetzenden Vorgesang, dem Proömium, als Namenlose im Singular angerufen: „Nenne mir, Muse…“

Hesiod (sechstes Jahrhundert v. Chr.) hat die Zahl der Musen in seiner "Theogonie" auf neun festgelegt: Nach ihm sind sie die Töchter der Mnemosyne, der Göttin der Erinnerung, und des Zeus, und auch die von ihm genannten Namen sind kanonisch. Sie werden Mnemoniden oder olympische Musen genannt. Allerdings wies Hesiod ihnen noch keine speziellen Zuständigkeitsbereiche und Attribute zu. Diese werden erst später unterschieden; doch auch dann wechselten die Zuschreibungen von Funktionen und Attributen noch einigermaßen willkürlich. Erst nach und nach gab es eine sich festigende Zuordnung von Name, Funktion und Attribut:


Eine vermutlich ältere Musentrias überlieferte der Reiseschriftsteller Pausanias (um 115–180 n. Chr.) unter den folgenden Namen und Zuständigkeitsbereichen:

Als Musen wurden auch die titanischen Musen (Μοῦσαι Τιτανίδες, "Moûsai Titanídes") genannt. Cicero unterschied deren vier:
Sie sollen die Töchter von Zeus (oder Uranos) und der Plusia gewesen sein, daher ihr Name. Platon gruppiert hingegen Hesiods "Terpsichore", "Erato", "Kalliope" und "Urania" zur Vierzahl.

Als drei apollonische Musen (Μοῦσαι Απολλωνίδες, "Moûsai Apollōnídes") oder auch "delphische Musen" wurden drei Töchter des Apollon bezeichnet:
Sie stellen die drei Saiten der Lyra des Apollo dar und sollen auf dem Helikon gewohnt haben. Der erste Namenssatz geht auf Eumelos von Korinth (siebtes Jahrhundert v. Chr.) zurück, der zweite auf Plutarch, er gibt dort darüber hinaus eine vierte Muse an:
Nete, Mese, Hypate spielen als Elemente der Tetraktys eine bedeutsame Rolle in der antiken Musiktheorie. Als vierte kam gelegentlich die "Paramese" hinzu, da die Saitenzahl der Lyra und die sich an ihr orientierende Musiktheorie variabel waren.

In anderer Tradition existierte eine Gruppe von sieben Musen, die nach Johannes Tzetzes von Epicharmos (fünftes Jahrhundert v. Chr.) erwähnt worden sein sollen, die sogenannten pierischen Musen oder Pieriden mit den folgenden Namen:
Diese sieben sollen die Töchter des Pieros, Stammvaters des thrakischen Volks der "Píeres" oder "Piereíes" (Πίερες Θράκες, Πιερείες), die später von den Makedonen ins nördliche Küstengebiet jenseits des Strymon vertrieben wurden., und einer pimpleischen Nymphe namens Antiope gewesen sein (Cicero, de natura deorum, III 54). Das antike Städtchen Pimpleia wird an der Stelle des heutigen Litochoro vermutet und gab den Musen außerdem den Beinamen der Pimpleiden.

Hingegen kennt Ovid neun pierische Musen, deren Mutter Euippe gewesen sein soll; sie stammen aus Ägypten und fordern die „jüngeren“ olympischen Musen heraus (Wettstreit der Mnemoniden und Pieriden). Nach ihrer Niederlage werden sie zur Strafe für ihr anmaßendes Verhalten in Elstern verwandelt. Diese neun Töchter des Pieros wurden auch Vögeln gleichgesetzt und trugen gelegentlich die Namen: "Colymbas, Lyngx, Cenchris, Cissa, Chloris, Acalanthis, Nessa, Pipo" und "Dracontis".

Während die Namen der Musen bei Hesiod lediglich Aspekte der Tanz- und Dichtkunst betonen, werden sie in der späteren Antike auf unterschiedliche Musikinstrumente und Gattungen bezogen, woraus die angegebene kanonische Zuordnung von „Aufgabengebieten“ der Musen hervorgeht.

Die zum Gefolge Apollons zählenden Musen sollen am böotischen Berg Helikon bei der Quelle Hippokrene zu finden sein, die durch einen Hufschlag des geflügelten Musenrosses Pegasos freigelegt wurde. Daher rührt der zum Teil für sie benutzte Name Helikoniades. Anderen Angaben zufolge wohnen die Musen auf dem – dem Apollon geweihten – Parnass oberhalb von Delphi, bei der kastalischen Quelle, deren Wasser Begeisterung und Dichtergabe verleihen soll.

Die Heiligtümer der Musen heißen "Museion" (woraus das heutige Wort "Museum" entstand), auch das deutsche Wort "Musik" – von μουσική τέχνη, der „Kunst der Musen“ – verdankt seinen Namen den Göttinnen. Als Personifizierung oder Werkzeug einer Muse kann die Muse betrachtet werden. Die Römer setzten die Musen mit den Camenae gleich.

Am Anfang antik-griechischer Epen und Hymnen steht oft eine Anrufung der Muse. So beginnt Homers "Odyssee" mit den berühmten und vielzitierten Versen: "Nenne mir, Muse, die Taten des vielgewanderten Mannes, / Welcher so weit geirrt, nach der heiligen Troja Zerstörung".
Auch etliche römische Dichter bitten die Muse um Inspiration (Vergil in der "Aeneis)", oder um Dauer für ihr Gedicht (Catull in den "Carmina").

Nach der Ächtung der Musen durch die mittelalterliche Kirche folgten Dichter der Neuzeit wie Dante, Shakespeare, Milton diesem Gebrauch wieder. Die neun Gesänge von Goethes "Hermann und Dorothea" tragen die Namen der neun Musen. Macht Klopstock in seinem "Messias" von der Anrufung Gebrauch, indem er statt der Muse die unsterbliche Seele andichtet (), so spielt Vladimir Nabokov im Titel seiner autobiographischen Schrift "Speak, Memory" zugleich auf Mnemosyne, Göttin der Erinnerung und Mutter aller Musen, an.

In deutschen humanistischen Gymnasien gehörten die Musen zum Unterrichtsstoff. Vor dem Jahr 1839 hatte ein (anonymer) deutscher Dichter die Namen der Musen und ihre Bestimmungen in folgende, für die Schüler einprägsame, Hexameter gebracht: 

<poem style="margin-left:8em;">
Klio lehrt die Geschichte der Völker; tragische Spiele
Sind der Melpomene heilig, komische liebet Thalia:
Schlachtgesänge tönt der Kalliope stolze Dromete;
Tänzer beschützt Terpsichore, Flötenspieler Euterpe
Erato singet der Liebenden Glück; Urania wandelt
Unter den Sternen; Polyhymnia (Polymnia) herrscht im Reiche der Redner.
</poem>

Es handelt sich um eine sinngemäße Übertragung aus dem Lateinischen.


Übersichtsdarstellungen in Handbüchern
Gesamtdarstellungen und Untersuchungen



</doc>
<doc id="3437" url="https://de.wikipedia.org/wiki?curid=3437" title="Mnemosyne (Mythologie)">
Mnemosyne (Mythologie)

Mnemosyne (, von , vergleiche ) ist eine Gestalt der griechischen Mythologie sowie ein Fluss in der Unterwelt, dessen Wasser im Gegensatz zur Lethe nicht Vergessen, sondern Allwissenheit herbeiführte.

Mnemosyne ist die Tochter des Uranos und der Gaia, gehört zu den Titanen und gilt als Göttin der Erinnerung.

Mnemosyne ist gemäß Hesiod Mutter der neun Musen, die sie dem Zeus in Pierien am Olymp gebar. Hesiod erzählt, dass sich die beiden fern der übrigen Götter neun Nächte lang vereinigten. In älteren Werken sind es auch oft nur drei Musen, die Mnemosyne geboren haben soll. Und manchmal werden diese auch „Mneiai“ genannt, was eine Mehrzahlform des Namens ihrer Mutter darstellt.


</doc>
<doc id="3439" url="https://de.wikipedia.org/wiki?curid=3439" title="Maxglan">
Maxglan

Maxglan ist ein Stadtteil der Landeshauptstadt Salzburg in Österreich.

Der geschlossene Siedlungsraum von Maxglan wird im Norden von der Westbahnlinie begrenzt, im Osten von der Linie Aiglhofstraße - Müllner Almkanalarm - Neutorstrasse - Bräuhausstrasse - Glan. Im Süden bilden die Wiesen des Salzburger Flughafens die Grenze, im Westen gegen Taxham hin die Rosa-Hofmann-Straße und die bis zur Bahn gedachte Verlängerung der Favoritagasse. Die Grenze zwischen Maxglan-Riedenburg und dem eigentlichen Stadtteil Riedenburg, der Bräuhausstrasse wird dabei manchmal unterschiedlich gezogen. In Maxglan leben heute 14.000 Menschen. Maxglan ist damit der drittgrößte Stadtteil Salzburgs nach Liefering und Lehen.

Im späten 19. und frühen 20. Jahrhundert war Maxglan (nach der Bevölkerungszahl) die nach der Landeshauptstadt zweitgrößte Gemeinde des Landes Salzburg.

Funde im Raum der einstigen Gemeinde Maxglan sind aus der Steinzeit, der Bronzezeit (Siedlungsreste) und der Hallstattzeit (Grabhügel mit Keramik, Schmuck und Waffen) gefunden worden. Aus römischer Zeit sind Grabfunde und auch Steinreliefe bekannt. Im Frühmittelalter hieß der Ort nach dem Fluss "Glan" (1222: „huebe in glan“). Der Name Glan war also damals Bezeichnung für eine Ortschaft und gleichzeitig für einen Bach. Wohl seit dem 15. Jahrhundert hieß der Ort nach dem Kirchenpatron auch St. Maximilian. Die Gebeine des heiligen Maximilian von Celeia, dessen Gestalt historisch nicht belegt ist, wurden der Legende nach in der ersten Maxglaner Kirche nahe dem Glanbach bestattet. Der Name St. Maximilian an der Glan, hieß später auch "Maxlan" oder und seit dem späten 18. Jahrhundert Maxglan. Der Ort war dank seiner fruchtbaren Ackerböden lange ein vergleichsweise wohlhabendes Bauerndorf, in dem aber auch viele Wäscher und Handwerker lebten.

Am 7. Juni 1935 wurde vom Salzburger Landtag die Erweiterung des Gebietes der Landeshauptstadt Salzburg beschlossen. Mit Wirksamkeit vom 1. Juli 1935 kam der Großteil der bisher eigenständigen Gemeinde Maxglan zur Stadt Salzburg. Einige Randflächen wurden an die umliegenden Gemeinden Wals, Siezenheim und Leopoldskron abgetreten, einige kleinere, von diesen Ortschaften abgetrennte und ebenfalls nach Salzburg eingemeindete Flächen werden hingegen seitdem als zum Stadtteil Maxglan gehörig aufgefasst.
Urkundlich ist die Kirche seit 1323 erwähnt, die erst eine Filiale der Dompfarre war. 1516–1519 entstand dann der erste größere Kirchenbau unter Erzbischof Leonhard von Keutschach. In der Barockzeit wurde die Kirche mehrfach umgestaltet und erhielt vier barocke Altäre. Seit 1906 ist die Kirche Pfarrkirche. Die Uneinigkeit ob Neumaxglan eine eigene Kirche erhalten oder die alte Kirche erweitert werden sollte verhinderte etliche Jahrzehnte eine bauliche Erweiterung. Erst 1952–1956 konnte der heute noch bestehende Zubau vorgenommen werden, wodurch die Kirche mit ihren zwei Längsschiffen zu einer Doppelkirche wurde. Am alten Friedhofseingang ist auch ein alter römischer Grabstein zu bewundern.

Das Pfarrzentrum St. Benedikt entstand an der Stelle des alten Loiplgutes, wobei nach 1960 zuerst das alte Bauerngut trotz seines schlechten Bauzustandes für kirchliche Gemeindearbeit, vor allem die Jugendarbeit genutzt wurde. Im Jahr 1980 konnte das neue Pfarrzentrum eingeweiht werden. Der große Festsaal, Emmaussaal genannt, dient verschiedensten Aufgaben. Der Emmaussaal für 100 Personen, die sechs Räume für die Jugendarbeit, für die Seniorentreffen aber auch den Singkreis und Besprechungszimmer sind hier untergebracht.

Der Friedhof stammt in seinen Ursprüngen vermutlich aus dem 14. Jahrhundert. Er wurde 1519 gemeinsam mit dem damaligen Kirchenneubau neu geweiht. 1883 und 1901 erfolgten im Zuge des starken Wachstumes der Gemeinde erste Vergrößerungen des zuvor kleinen Dorffriedhofes. Beim Neubau der Kirche mussten etliche Gräber umgebettet werden, die Gräber wurden 1945–1949 in einem angrenzenden, von der Stieglbrauerei erworbenen Grundstück neu angelegt. 1958 wurde an jenem Ort, an dem das im Zweiten Weltkrieg durch Fliegerbomben zerstörte Karlbauerngut lag, ein Kriegerdenkmal errichtet, das 1960 geweiht wurde. Damals war der Friedhof längst wieder zu klein geworden und die Bevölkerung verlangte von der Stadtgemeinde rasch Abhilfe. Aber erst 1969 erfolgte auf einem Grundstück, das zuvor der Österreichischen Post gehört hatte, eine weitere Vergrößerung. 1975 wurde im Friedhof vom Architekten Erich Flir geplant die neue Aussegnungshalle errichtet. Die letzte Friedhofserweiterung fand nach 1990 statt.

Auf dem Maxglaner Friedhof wurde der 1994 beim Großen Preis von San Marino in Imola tödlich verunglückte Roland Ratzenberger bestattet. Auch der österreichische Schauspieler Hugo Lindinger hat hier seine Grabstätte.

Am 29. Dezember 1921 gründete sich mit der Salzburger Kunstfilm-Industrie-AG die erste Filmproduktionsgesellschaft des Landes Salzburg. Maßgebliche Betreiber waren der Filmregisseur Rudolf Oppelt (1893–1971) und der Maxglaner Brauerbesitzer Heinrich Kiener, der als Mitgründer unter anderem die Räumlichkeiten in den leer stehenden, landwirtschaftlichen Gebäuden der Stieglbrauerei zu Salzburg zur Verfügung stellte. Mit über 700 m² Nutzfläche war es damals, samt Garderoben, Laboratorium, Entwicklungs- und Kopieranstalt, technischem Büro und 5000 m² Freigelände das größte Filmatelier Österreichs. Das bis heute unverändert bestehende Studio zählt damit zu den ältesten Filmateliers der Welt. 1922 übernahm die Wiener PAX Film-GmbH, der auch mehrere Kinos gehörten, die Gesellschaft.

Die erste Produktion war der Dokumentarfilm "Die Festspiele 1921" mit historisch wertvollen Aufnahmen von den Salzburger Festspielen jenes Jahres. Der erste Spielfilm "Die Tragödie des Carlo Prinetti" entstand im Sommer 1921 und wurde am 29. Jänner 1924 in Wien uraufgeführt. Das turbulente Drama über das Leben eines Halleiner Salinenarbeiters, zu dem Rudolf Oppelt das Drehbuch verfasste, zeigte den aufkommenden Stummfilmstar Alphons Fryland (1889–1953) als Hauptdarsteller. Es war zugleich die letzte Produktion der Salzburger Kunstfilm, die, samt ihren Wiener Filialbetrieben - darunter einem Filmverleih - wegen der Filmwirtschaftskrise 1925 schließen musste.

Das 1927 von Franz Wolf eröffnete Lichtspielhaus Maxglan war zu seiner Zeit das zweitgrößte Kino Österreichs außerhalb der Bundeshauptstadt Wien nach dem Annenhofkino in Graz. Der Eintritt betrug zwischen 70 Groschen und 2 Schilling für den Logenplatz. In dem mit modernen Klapp-Fauteuils der Wiener Firma Thonet ausgestatteten Sälen konnte die Bevölkerung Maxglans und der angrenzenden Stadt Salzburg jeweils mittwochs, samstags sowie an Sonn- und Feiertagen die damals aktuellen Kinohighlights konsumieren.

Maxglan besteht aus folgenden Teilen: Altmaxglan mit der zentralen Kirchensiedlung und der randlichen Mühldorfstraßensiedlung, Burgfried, Neumaxglan mit der Aiglhofsiedlung, Maxglan-Riedenburg mit der Torschauersiedlung und Glanhofen.

Maxglan war im frühen Mittelalter ein Bauerndorf, das sich um die Kirche St. Maximilian herum entwickelte. Ein hier gefundener und in der Südwand eingebauter "Radlstein", Teil eines verloren gegangenen antik-römischen Reliefs belegt die alte Geschichte. Im nächsten Nahbereich finden sich aber auch bronzezeitliche und eisenzeitliche Funde.



Zum Namen: Der Burgfried war vermutlich ein uralter befestigter Zufluchtsort von Maxglan. „Burgfried“ (burccride) hieß der Schutzbereich eines Fürstenhofes, einer Stadt oder eines Marktes. Vielleicht lag der älteste befestigte Siedlungskern von Maxglan geschützt von der Glan unmittelbar westlich des heutigen Burgfrieds an der dortigen kleinen Terrassenkante. In der heutigen Lage zwischen Glan- und Mühlbach ist aber ein solcher befestigter Ort kaum vorstellbar.

Der Burgfried ist ein Maxglaner Stadtteil, der zwischen den lange großteils unverbauten Flächen zwischen Neumaxglan und Altmaxglan, in Richtung Aiglhof gelegen ist. Räumlich gesehen liegt Burgfried dabei inselartig zwischen der Glan und dem als Werkskanal abzweigenden und wieder in die Glan mündenden Maxglaner Mühlbach. Hier wohnten erhöht an der Glan in historischer Zeit verschiedene Wäscher und Handwerker. An historischen Wäscherhäusern sind (Stand 1830) hier zu nennen: Wäscherlenz (Burgfriedgasse 3, erhalten), Wäscher-Münzer-Haus, Wäscher-Brandhofer-Haus, Wäscher-Marx-Haus, Wäscher-Eglhamer-Haus, Putzwäscherhaus und das Wäscher-Gartner-Haus. Der Raum Burgfried selbst war bis um 1800 eine nasse, oft überschwemmte Weide. An diesen Abschnitt grenzt heute der Stölzlpark. Er ist benannt nach dem Maxglaner Bürgermeister, Landeshauptmann-Stellvertreter und Rechtsanwalt Dr. Arthur Stölzl, der in der großzügigen Villa Arthur wohnte, zu dessen Park der heutige Stölzlpark gehörte. In diesem Park befinden sich heute auch der Verkehrsgarten der Stadt Salzburg, ein Kindergarten und das Maxglaner Heimathaus.

Neumaxglan ist ein Stadtteil, der wesentlich noch in der Gemeinde Maxglan zu Zeiten der Monarchie zwischen 1860 und 1914 entstand. Er liegt zwischen Maxglaner Mühlbach und dem Müllner Arm des Almkanales. Hier liegen die Sonderschule und die Hauptschule von Maxglan. Auch die große Stieglbrauerei siedelte sich hier 1863 an.

Vor 1850 bestand hier neben dem an der Glan gelegenen Rochusmeierhof nur ein Bauerngehöft, das Zillnergut. Der Zeit gemäß wurde diese Verbauung plangemäß und rasterartig als weitgehend dreigeschossiger Einzelhausbau vorgenommen. Durch den Aufschwung Neumaxglans stieg die Maxglaner Bevölkerung von 1890 (3465 Einwohner) auf 7204 Einwohner im Jahr 1910 an. Das aufstrebende Neumaxglan und das behäbige Altmaxglan war lange recht gegensätzlich. So leisteten sich etwa beide Teile getrennte Feuerwehren. Die 1919 eröffnete Stieglbahn, eine ausschließlich dem Güterverkehr dienende Anschlussbahn, zog in der Folge weitere Betriebe an.

Die Aiglhofsiedlung entstand am Rand zum Stadtteil Mülln auf den wesentlich zu Maxglan gehörigen Aiglhoffeldern und wird zusammen mit den angrenzenden, dicht verbauten Gebieten von Mülln und Lehen vielfach bereits als eigener kleiner Stadtteil wahrgenommen. Der Name leitet sich von dem sich bereits in Mülln befindlichen Aiglhof – auch Lindhof genannt – ab. Westlich des Aiglhofes beginnt der Stadtteil Lehen, nördlich liegt Mülln, östlich der Stadtteil Riedenburg. Innerhalb Maxglans grenzt das Gebiet an Burgfried und Neumaxglan.

Geprägt wird das dicht bebaute Wohngebiet von der Aiglhofstraße, samt der Aiglhofkreuzung und der Innsbrucker Bundesstraße, die nach der Ignaz-Harrer-Straße in Lehen als die Straße mit dem höchsten Verkehrsaufkommen in der Landeshauptstadt gilt. An Öffentlichen Einrichtungen befinden sich hier die Sonderschule "Aiglhof I" und im nahen Mülln die Volksschule sowie das Salzburger Landeskrankenhaus.

Die Verbauung der Aiglhoffelder wurde bereits 1927 in der damals noch eigenständigen Gemeinde Maxglan konkret geplant. Das preisgekrönte Siegerprojekt des Architekten Lois Welzenbacher (1889–1955) sah die Errichtung von 8.000 Wohnungen vor, konnte aus Geldmangel aber nicht verwirklicht werden.

Zu der in Salzburg herrschenden Wohnungsnot kam nach dem „Anschluss Österreichs“ an das Deutsche Reich noch der Bedarf an Unterkünften für die Umsiedler aus Südtirol und für die Offiziere des Wehrkreis-Generalkommandos XVIII hinzu. Im Rahmen eines neuen Aiglhof-Siedlungsprojektes entstanden bis 1941 drei Siedlungen, die Südtiroler-Siedlung (heute Aiglhof I), die Siedlung "Hötzendorfstraße-Römergasse-Radetzkystraße" (Aiglhof 2) und die direkt an die Aiglhofstraße grenzende, eigentliche "Aiglhofsiedlung" (Aiglhof 3).

Die Straßen im Aiglhof wurden großteils nach Generälen und Admirälen der Österreichischen Monarchie, wie beispielsweise Franz Conrad von Hötzendorf, Wilhelm von Tegetthoff, Eduard von Böhm-Ermolli und Viktor Graf Dankl von Krasnik benannt.

Die Aiglhofsiedlung ist mit den Linien 2 und 4 des Oberleitungsbus Salzburg sowie den Albus-Autobuslinien 20, 24 und 28 erreichbar. Mit der Inbetriebnahme der S-Bahn-Station "Salzburg-Aiglhof" im Bereich der Unterführung der Rudolf-Biebl-Straße ist der dicht verbaute Stadtteil seit dem Jahr 2009 an das Netz der Salzburger S-Bahn angeschlossen. Die Züge der Linien S2 und S3 halten im 15-Minuten-Takt und erreichen den Hauptbahnhof in wenigen Minuten.

Der südöstliche Teil von Maxglan wird Maxglan-Riedenburg genannt. Er bildet dabei das Bindeglied zum heute eigenständigen Stadtteil Riedenburg mit seiner zentralen Pfarrkirche St. Paul und wurde wesentlich nach dem Zweiten Weltkrieg verbaut. Zuvor gab es hier nur einzeln stehende Gehöfte entlang der Kendlerstraße und einige Wohnbauten entlang der kurzen Stichstraßen Höglstraße-Krailnstraße und Hagmüllerstraße-Waldstraße. Der heutigen Siedlungsraum zwischen Kendlerstraße und Glan wurde erst infolge der Begradigung und Kanalisierung der Glan bebaubar.

Der Brauereibetrieb war ursprünglich dort, wo derzeit noch das Gesundheitsamt der Stadt Salzburg untergebracht ist. Der Gasthof und die Brauerei lagen zuerst damit an der dortigen Stieglgasse, die ihrerseits an eine frühere Stiege zum dortigen Almkanal erinnert. Die Brauerei ist 1482 erstmals urkundlich erwähnt. 1863 übersiedelte diese Stieglbrauerei nach Maxglan-Riedenburg. Der Name des landwirtschaftlichen Betriebes der Stieglbrauerei, der Rochushof, erinnert an den Pestpatron, den heiligen Rochus und das dortige Pesthaus (Lazareth zum heiligen Rochus) samt Pestfriedhof, das Fürsterzbischof Paris Lodron 1636 hier errichten ließ. Sigismund Graf Schrattenbach baute das nicht mehr benötigte Pestspital zu einem Arbeitshaus um, von dem der lateinische Spruch „Abstine aut autine“ „Meide (die Straftat) oder leide (die Folgen)“ über dem dortigen Portal angebracht heute noch berichtet.

Umgeben von Wiesenresten am Nordrand der Landebahn des Flughafens entwickelte sich ausgehend von einem ersten Siedlungskern am Torschauerweg, der schon um 1935 entstanden war, der kleine Siedlungsraum nach 1953 bis zur heutigen Größe. Der Name „Torschauersiedlung“ erinnert an den ehemals deutschen Ort Torschau in der Mittel-Batschka (Serbien), aus dem etwa 300 vertriebene „Volksdeutsche“ hier 1953 eine neue Heimat fanden. Die ersten sechs Häuser wurden hier von der ersten "Donauschwäbischen Baugenossenschaft" in Österreich „Neusiedler“ errichtet und auf Initiative und Antrag eines der Mitbegründer dieser Genossenschaft aus Torschau, Peter Henkel, als Torschauerweg genannt, nachdem das Amt der Salzburger Landesregierung dem Antrag auf diese Straßenbezeichnung zugestimmt hatte.

"Glanhofen", eine kleine Gehöftgruppe, war bis 1935 eine rein bäuerliche Ortschaft im Gemeindegebiet von Maxglan. Heute liegt der alte Siedlungskern mit seinen kleinen Wohnbauten unmittelbar am Rand der Landebahn des Flughafens und unmittelbar an der stark befahrenen Innsbrucker Bundesstraße. Glanhofen wäre als eigenständiger Stadtteil viel zu klein, es tritt im Stadtgefüge als westlichster Teil des Stadtteiles Maxglan in Erscheinung.

Im Gebiet von Maxglan befinden sich heute der Salzburger Flughafen, die Stieglbrauerei zu Salzburg und das Stammhaus der Spedition Lagermax.

In Maxglan gibt es eine große Pfadfindergruppe. Seit 18. Mai 2009 gibt es eine Junge ÖVP Maxglan deren Obmann Martin Zirngibl ist.

Für ein reges Vereinsleben sorgt der Arbeiter-Sportklub Salzburg, der mehrere Sektionen, darunter auch Fußball – einstmals gegründet als "Sportklub Vorwärts Maxglan" – unterhält. Die Frauenfußballmannschaft des ASK Maxglan war 2007/2008 Vize-Meister der 2. Liga Mitte. Seit dem Verlust der Heimstätte im Jahr 2007 spielt die Fußball-Landesligamannschaft des ASK in Liefering. Eine Heimkehr in den ursprünglichen Stadtteil wird angestrebt, lässt sich aber derzeit nicht verwirklichen. Die Damen des ASK Maxglan spielen ab 2008/09 in der Nachbargemeinde Wals-Siezenheim. Bis 2007 war die ASK-Sportanlage zudem auch Spielstätte des American Football Team Salzburg Bulls die nunmehr ihre Heimspiele auf der ASV-Anlage in Itzling austragen.

Nutznießer des Verlustes der Sportanlage des ASK ist der neugegründete SV Austria Salzburg, der seither die in MyPhone Austria Stadion umbenannte Anlage als gemietete Heimstätte nützt.

Dem Tennissport widmet sich der aus der Tennissektion des ASK entstandene "TC ASKÖ Maxglan".

Seit 1902 gibt es den Turnverein Maxglan, den Weltmeisterschaftsteilnehmer wie Heinrich Ulamec und Andrea Schalk bekannt machten.

Maxglan ist Namensgeber des Maxglaner Zigeunermarsches von Tobi Reiser.



</doc>
<doc id="3442" url="https://de.wikipedia.org/wiki?curid=3442" title="Mathematisches Modell">
Mathematisches Modell

Ein mathematisches Modell ist ein mittels mathematischer Notation erzeugtes Modell zur Beschreibung eines Ausschnittes der beobachtbaren Welt. Dieses Modell kann in beliebigen, begrenzten Bereichen der beobachtbaren Realität, wie z. B. den Naturwissenschaften, den Wirtschafts- oder Sozialwissenschaften, der Medizin oder den Ingenieurwissenschaften Anwendung finden. Mathematische Modelle erlauben eine logische, strukturelle Durchdringung je nach Art hinsichtlich von geltenden Gesetzmäßigkeiten, erlaubten und nicht erlaubten Zuständen, sowie seiner Dynamik mit dem Ziel, diese Erkenntnisse auf das modellierte System zu übertragen.

Der Prozess zur Erstellung eines Modells wird als "Modellierung" bezeichnet. Die Erstellung eines mathematischen Modells für einen Realitätsausschnitt ist nicht mehr Aufgabe der Mathematik, sondern des jeweiligen Wissenschaftsgebietes. Inwieweit ein mathematisches Modell Vorgänge in der Realität korrekt beschreibt, muss durch Messungen überprüft und validiert werden.

Ein mathematisches Modell stellt somit einen Realitätsbezug her, der für mathematische Teilgebiete im Allgemeinen nicht vorhanden sein muss.

Dass Modellvorstellungen eine zunehmend wichtige Rolle in der wissenschaftlichen Theoriebildung spielen, wurde bei der Diskussion von Atommodellen Anfang des 20. Jahrhunderts klar erkannt. Aufgrund der wissenschaftstheoretischen Vorbildfunktion der Physik hat sich der Begriff Modell, wie andere ursprünglich physikalische Begriffe auch, in andere Disziplinen ausgebreitet.

Modellgestützte Methoden sind nicht auf die Naturwissenschaften beschränkt. Zum Beispiel beruhen die bekannten zweidimensionalen Auftragungen funktionaler Zusammenhänge in den Wirtschaftswissenschaften auf radikal vereinfachender Modellbildung.

"Hauptartikel für Systeme innerhalb der Systemtheorie:" Systemtheorie

Mathematische Modelle modellieren Systeme.

Vereinfacht lässt sich ein System als eine Menge von Objekten beschreiben, die durch Relationen verbunden sind. Ein System kann dabei ein natürliches System (etwa ein See, ein Wald), ein technisches System (etwa ein Motor oder eine Brücke), aber auch ein virtuelles System (etwa die Logik eines Computerspiels) sein.

Ein System ist von seiner Umgebung umhüllt. Diese Umgebung wirkt von außen auf das System ein. Derartige Einwirkungen, werden als Relationen formula_1 bezeichnet. Ein System reagiert auf Einwirkungen durch Veränderungen von Systemvariablen.

Grundsätzlich hat ein System auch Wirkungen nach außen, also auf die Umgebung. Im Rahmen der Modellierung von Systemen wird diese nach außen gerichtete Wirkung jedoch üblicherweise vernachlässigt.

Ein System wird von der Umgebung durch klar definierte Systemgrenzen abgeschlossen. Das bedeutet, dass für die Modellierung ausschließlich die definierten Relationen wirksam sind. Als Beispiel sei die Untersuchung des Phosphoreintrags in einen See genannt. Im Rahmen eines Modells soll als einzige Quelle ein in den See mündender Fluss betrachtet werden, die Grenze des Systems ist in diesem Beispiel dann die Relation „Fluss“. Weitere in der Natur auftauchende Quellen (Grundwasser, Schiffsverkehr, Fische, und so weiter) werden im Modell nicht berücksichtigt.

Die Definition eines konkreten Systems als Untersuchungsgegenstand bei der Modellierung von mathematischen Modellen erfolgt durch den Analytiker entsprechend dem Untersuchungsziel.

Schematisch lässt sich ein System über ein sogenanntes Boxmodell darstellen.

formula_2

Die Box ist dabei das modellierte System. Die Eingangsrelation formula_3 sind symbolisch die Einwirkungen der Umwelt auf das modellierte System und der ausgehende Pfeil symbolisiert die Veränderungen des Systems. Zwischen den Systemvariablen formula_4 selbst, können beliebige weitere Relationen bestehen.

In der Praxis werden Boxmodelle als Denkhilfe benutzt. Die grafische Darstellung eines Systems vereinfacht die Erkennung von Systemvariablen. Ein modelliertes System kann dabei aus beliebig vielen weiteren Subsystemen bestehen, die jeweils wieder ein eigenes Boxmodell darstellen.

Das Boxmodell wird insbesondere in den Ingenieurswissenschaften bei der Erstellung von Computermodellen benutzt. Dabei stellen die Modelle jeweils insgesamt ein Boxmodell (genauer die Relationen innerhalb des Systems) dar. Jedes grafische Element ist wiederum ein eigenes Boxmodell. Zur Vereinfachung wurden dabei unterschiedliche grafische Symbole für Boxmodelle benutzt, etwa ein gewendeltes Symbol um die Systemvariable einer Spule darzustellen.

Im Rahmen der Modellierung sind Systeme denkbar, die Wirkungen nach außen haben, aber keine Eingangsrelationen. Etwa ein System welches Zeittakte produziert. Es sind auch Systeme denkbar, die zwar über Eingangsrelationen aber nicht über Auswirkungen verfügen. Zum Beispiel zum Monitoring von Werten.

Nach dem Grad der Bestimmtheit eines Boxmodells lassen sich Boxmodelle in Black-Box- und White-Box-Modelle unterscheiden. Black-Box-Modelle beschreiben das Verhalten eines Systems in Form einer Gleichung, ohne dabei die Komplexität des Systems zu berücksichtigen. White-Box-Modelle versuchen dagegen ein System so genau wie möglich zu modellieren.

Die Wahl eines dieser Modelle ist abhängig vom Untersuchungsgegenstand. Soll ein mathematisches Modell lediglich als Berechnungshilfe dienen, ist eine Black Box ausreichend. Soll das innere Verhalten eines Systems, etwa bei einer Simulation untersucht werden, muss zwangsweise eine White Box erstellt werden.

"Hauptartikel für physikalische Dimensionen:" Dimension (Größensystem). "Hauptartikel für mathematische Dimensionen:" Dimension (Mathematik)

Die Dimension eines Systems ist die Anzahl der Zustandsvariablen, mit der das mathematische Modell beschrieben wird.

Eine Modellgleichung ist das formelle mathematische Modell eines Systems in Form einer Funktion.

Modellgleichungen haben grundsätzlich die Form formula_5


Grundsätzlich kann jede der Mengen leer sein. Oft besteht die Menge auch nur aus einem Element. Es ist daher üblich in einer konkreten Modellgleichung nur benötigte Mengen anzugeben und die Elemente der benötigten Teilmengen per Index zu bestimmen. Abhängig vom Wissenschaftsbereich für den eine Modellgleichung erstellt wird, bekommen die Elemente einer Modellgleichung andere Variablennamen.

In den verschiedenen Fachgebieten werden unterschiedliche Variablennamen benutzt, passend zur Fachsprache und üblichen Variablennamen des Gebiets.
Statische Modelle
Dynamische Modelle

Nach dem Änderungsverhalten
Nach der Kontinuität
Nach der Vorhersagbarkeit
Nach der Anzahl der Systemvariablen
Nach der Art der Gleichungen und Gleichungssysteme
Nach dem Wissenschaftszweig

Magnetismus kann verschiedene Ursachen haben; in einem einzelnen Magneten können verschiedene Mechanismen wirken, die den Magnetismus hervorbringen, verstärken oder abschwächen; der Magnet kann aus kompliziert aufgebauten, verunreinigten Materialien bestehen; und so weiter. In dieses Durcheinander versucht man Licht zu bringen, indem man Modellsysteme untersucht. Ein physikalisches Modell für einen Ferromagneten kann etwa so lauten: eine unendlich ausgedehnte (man sieht also von Oberflächeneffekten ab), periodische (man sieht also von Gitterfehlern und Verunreinigungen ab) Anordnung atomarer Dipole (man konzentriert sich auf den Magnetismus gebundener Elektronen und beschreibt diesen in der einfachsten mathematischen Näherung).

Um das soeben eingeführte physikalische Modell eines Ferromagneten zu untersuchen, sind verschiedene Methoden denkbar:

Man wählt Parameter aus, die man einerseits aus experimentellen Untersuchungen an realen Ferromagneten kennt und die man andererseits auch für das Modell bestimmen kann; im konkreten Beispiel zum Beispiel die magnetische Suszeptibilität als Funktion der Temperatur. Wenn Vorbild und Modell in diesem Parameter übereinstimmen, dann kann man zurückschließen, dass das Modell relevante Aspekte der Wirklichkeit korrekt wiedergibt.

Die wohl bekanntesten und ältesten Anwendungsbeispiele für mathematische Modelle sind die natürlichen Zahlen, die die Gesetzmäßigkeiten beim „Zählen“ konkreter Objekte beschreiben, die erweiterten Zahlenmodelle, die das klassische „Rechnen“ beschreiben, sowie die Geometrie, die die Landmessung ermöglichte.

Elektrotechnik

Physik

Astronomie

Chemie

Mathematik

Spieltheorie (Volkswirtschaftslehre)

Betriebswirtschaftslehre

Soziologie

Mathematische Modelle sind eine vereinfachte Darstellung der Realität, nicht die Realität selbst. Sie dienen der Untersuchung von Teilaspekten eines komplexen Systems und nehmen dafür Vereinfachungen in Kauf. In vielen Bereichen würde eine vollständige Modellierung aller Variablen zu einer nicht mehr beherrschbaren Komplexität führen. Modelle, insbesondere solche, die menschliches Verhalten beschreiben, stellen nur eine Annäherung an die Wirklichkeit dar. Es ist nicht immer möglich, mit Modellen die Zukunft berechenbar zu machen.





</doc>
<doc id="3443" url="https://de.wikipedia.org/wiki?curid=3443" title="Léon Walras">
Léon Walras

Marie-Esprit-Léon Walras (* 16. Dezember 1834 in Évreux, Normandie; † 5. Januar 1910 in Clarens, heute Montreux, Schweiz) war ein französischer Ökonom. Er begründete die Lausanner Schule und gilt als einer der führenden Vertreter der Neoklassik und Urheber des allgemeinen Gleichgewichtsmodells.

Léon Walras war das älteste von vier Kindern des Antoine-Auguste Walras (1801–1866) aus Montpellier und dessen Ehefrau, der Notarstochter Louise Aline de Sainte Beuve (1811–1892), aus Évreux. Durch den beruflichen Aufstieg des Vaters vom Lehrer zum Professor für Philosophie verbrachte Léon Walras seine Kindheit und Jugend in Paris sowie in mehreren Städten Nordfrankreichs, wo er von 1844 bis 1850 das Collège in Caen und anschließend das Lycée in Douai besuchte. Er erlangte 1851 den Abschluss des "Bachelier-ès-lettres." Nach dem Besuch weiterer Kurse in allgemeiner und spezieller Mathematik über jeweils ein Jahr schloss er im Jahr 1853 mit dem "Bachelier-ès-sciences" ab. Bei der Aufnahmeprüfung (Concours) für die École polytechnique in Paris fiel er zweimal durch. „Sein Studiengang zeigt die praktische Unfähigkeit des Denkers; Mißerfolge, wie sie nicht anders zu erwarten sind, wenn man sich für die École polytechnique durch das Studium Descartes‘ und Newtons vorbereitet; Unlust an ausgefahrenen Bahnen, wie sie jeder Studierende empfindet.“ Er las Cournots "Recherches sur les principes mathématiques de la théorie des richesses" (dt.: "Untersuchungen über die mathematischen Prinzipien der Theorie des Reichtums") und Lagrange.

Als er 1853 zum Studium in die Hauptstadt zurückkehrte, war das gesellschaftliche und kulturelle Leben noch immer von den revolutionären Ideen der 1848er geprägt. Dieser Lebensstil und diese Einstellung der so genannte Bohemien beeinflusste auch Léon Walras. Er führte während und nach seiner Studienzeit in Paris zunächst ein relativ unkonventionelles Leben. Da er an seiner favorisierten Hochschule nicht angenommen wurde, begann er 1854 ein Ingenieur-Studium an der "École des Mines". Schnell stellte er aber fest, dass er nur wenig Interesse am Ingenieurwesen hatte. Daher vernachlässigte er sein Studium und beschäftigte sich vermehrt mit der Literatur, Philosophie, Geschichte, Kunstkritik und Literaturkritik sowie Wirtschaftspolitik und Sozialwissenschaft. Ob er trotzdem einen Abschluss als Ingenieur gemacht hat, lässt sich nicht zweifelsfrei klären.

Léon Walras verkehrte in intellektuellen Kreisen in Paris und versuchte sich als Romanautor. Der im Jahr 1858 veröffentlichte Roman "Francis Sauveur" schildert das Leben eines Studenten im Paris der 1850er und soll über starke autobiographische Züge verfügen. Die Abkehr vom Studium wurde von seinen Eltern nicht gern gesehen, insbesondere von seinem Vater. Sie hatten erhofft, dass ihr Sohn Karriere als Bergbauingenieur machte. In einem von Léon Walras als sehr prägend genannten Gespräch gab Auguste Walras seinem Sohn deutlich zu verstehen, dass dieser zumindest das väterliche wirtschafts- und sozialwissenschaftliche Werk fortzusetzen habe. Daher versuchte Léon Walras, sich in den Jahren 1859 bis 1862 mit der journalistischen Arbeit für das "Journal des Economistes" und "La Presse" zu etablieren. Er nutzte die Argumentationen seines Vaters, um die leitenden Gedanken Pierre-Joseph Proudhons zu widerlegen. So entstand in den Jahren 1859/60 sein erstes wirtschaftswissenschaftliches Werk: "L’économie politique et la justice; Examen critique et réfutation des doctrines économiques de M. P. J. Proudhon précédés d'une introduction à l'étude de la question sociale." Mit diesem Aufsatz nahm er im Juni 1860 an einem Wettbewerb beim internationalen Steuerkongress in Lausanne teil und gewann den vierten Preis.

Im gleichen Jahr begann er im "Journal des Economistes" seine sechsteilig geplante Artikelserie "Paradoxes Économiques" zu veröffentlichen. Der erste Teil "Paradoxes économiques I (Que le sens commun n’est point le critérium de la science en général, ni en particulier celui de l’économie politique)" beschrieb eine Diskussion zwischen einem Kaufmann und einem Wirtschaftsexperten über unterschiedliche praktische Probleme, wie etwa den Freihandel oder Steuern. Der zweite Teil der Serie war bereits geschrieben, als der Autor sich mit seinem Herausgeber überwarf, so dass jeder weitere Artikel zurückgewiesen wurde. Nachdem er nun im "Journal des Economistes" nicht mehr veröffentlichen konnte, kündigte Walras auch bei "La Presse."

Seine Bestrebungen, in Frankreich als Dozent für Ökonomie an einer Universität unterzukommen, scheiterten. Die elf französischen Lehrstühle für Wirtschaftswissen waren mit orthodoxen Ökonomen besetzt und wurden laut Walras’ eigener Aussage nur durch Vetternwirtschaft weitergegeben. Auch sein Bestreben, eine eigene Zeitung über Politik und Sozialwissenschaften zu veröffentlichen, hatte keinen Erfolg, da er die staatliche Genehmigung nicht erhielt. Léon Walras nutzte die Bekanntschaft seines Vaters mit Horace Say, dem Sohn von Jean-Baptiste Say, um eine Anstellung in der Verwaltung der Eisenbahngesellschaft "Chemins de Fer Du Nord" zu finden. Léon Say, der Sohn von Horace Say und spätere Finanzminister, war im Jahr 1862 einer der Direktoren der Eisenbahngesellschaft. Léon Say verhalf ihm später zu einem Posten im Direktorium einer Wechselbank. In den Jahren von 1865 bis 1868 war er dort für die Buchhaltung zuständig und konnte daher auch Artikel bei der von der Bank finanzierten Genossenschaftszeitschrift "Le Travail" veröffentlichen. Nach der Liquidation der Bank Ende 1868 bekam er eine Anstellung im Büro des Bankiers J. Hollander.

Walras lebte über zehn Jahre mit seiner ersten Ehefrau Célestine Aline Ferbach (1834–1879) zusammen, bevor er sie im Jahr 1869 heiratete. Mit dieser Heirat wurde seine 1863 geborene Zwillingstochter Marie Aline legitimiert, während ihre Zwillingsschwester bereits im Säuglingsalter verstorben war. Léon Walras adoptierte, nachdem er Célestine geheiratet hatte, deren Sohn Georges (1857–1934), der einer vorangegangenen nichtehelichen Beziehung entstammte. Célestine Walras starb 1879 nach dreijähriger Krankheit. Um die Kosten für die Behandlung zu decken, verschuldete sich Léon Walras erheblich. Seine finanzielle Situation verbesserte sich erst im Jahr 1884, als er Léonide Désirée Mailly (1826–1900) heiratete, da diese eine Leibrente in die Ehe einbrachte. Im Jahr 1892 erhielt Walras nach dem Tod seiner Mutter ein Erbe in Höhe von 100.000 Francs, mit dem er seine Schulden bezahlen konnte und sich eine eigene Leibrente erkaufte. 

Im Jahr 1870 kam für Léon Walras’ Werdegang die entscheidende Wende. Die Regierung des Schweizer Kantons Waadt lud ihn ein, am Concours für den neu eingerichteten Lehrstuhl der politischen Ökonomie an der Universität von Lausanne teilzunehmen. Die Verantwortlichen erinnerten sich noch immer an seinen im Jahr 1860 beim Steuerkongress eingebrachten Beitrag. Da die siebenköpfige Berufungskommission aus drei Mitgliedern der Kantonsregierung bestand und Léon Walras einen der vier Wirtschaftspolitik-Professoren positiv beeindrucken konnte, ging er als Sieger aus dem Concours hervor. 

Die ersten Jahre in Lausanne waren seine produktivsten. Er besann sich auf die Kernaussagen der väterlichen Wirtschaftstheorie zurück, dass Knappheit und Nutzen die Quelle des Wertes bildeten und mathematische Hilfsmittel zu benutzen seien. Dabei zog er auch Cournots Werk in seine Überlegungen ein. Sein Vater hatte sich bereits bemüht, eine Theorie dahingehend zu skizzieren, aber erst Léon Walras konnte sie formulieren. Seine bedeutendsten Werke entstanden in den 1870ern. Im Jahr 1874 erschien der Aufsatz "Principe d’une théorie mathématique de l'échange" (etwa "Prinzip einer mathematischen Theorie des Tausches") im "Journal des économistes," nachdem er im Jahr zuvor vom Herausgeber bereits einmal zurückgewiesen worden war.
Im Jahr 1874 wurde die erste Auflage seines bedeutendsten Werkes herausgegeben, die "Éléments d'économie politique pure, ou théorie de la richesse sociale" (in Englisch: "Elements of Pure Economics, or the Theory of Social Wealth", dt.: "Mathematische Theorie der Preisbestimmung der wirtschaftlichen Güter"). Sie bestehen aus dem "Prinzip einer mathematischen Theorie des Tausches" aus dem Jahr 1873, den "Gleichungen des Tausches" aus dem Jahr 1875 sowie den "Gleichungen der Produktion, Gleichungen der Kapitalisierung und des Kredites" aus dem Jahr 1876. Ludwig Gebhard von Winterfeld (1853–1904), ein preußischer Offizier, übersetzte dieses Werk erstmals im Jahr 1881 in die deutsche Sprache, als er sich im Winter 1880/81 in Lausanne aufhielt. 

Ab 1874 nahm Léon Walras auch Kontakt mit den unterschiedlichsten Wirtschaftswissenschaftlern seiner Zeit auf, unter anderem mit William Stanley Jevons, Antoine-Augustin Cournot, Alfred Marshall, Carl Menger, Philip Wicksteed, Francis Ysidro Edgeworth, Vilfredo Pareto und Knut Wicksell. Er wollte seine Ideen bekannt machen und sah in dieser Kontaktaufnahme zu den wirtschaftswissenschaftlichen Größen seiner Zeit den Weg dazu. Wenig erfolgreich waren die Versuche, seine Ideen und Erkenntnisse, vor allem Geld- und Währungslehre, in Frankreich zu etablieren.

Um die hohen Arztkosten der Behandlung seiner Frau Célestine begleichen zu können, musste er neben seiner Tätigkeit an der Universität noch anderen Beschäftigungen nachgehen. Er gab Nachhilfe, schrieb Artikel unter einem Pseudonym für die "Gazette de Lausanne" und die "Bibliothèque Universelle," außerdem war er auch als Berater für die Versicherungsgesellschaft "La Suisse" tätig.

Nach seiner Erbschaft im Jahr 1892 wollte Léon Walras sich ursprünglich nur für ein Jahr von seiner Lehrtätigkeit zurückziehen, um seine gesundheitlichen Probleme zu überwinden, doch er nahm seine Dozententätigkeit nie wieder auf. Als Honorarprofessor war er weiterhin für die Universität in Lausanne tätig, musste aber keine Vorlesungen halten. Seinen Lehrstuhl an der Hochschule übernahm Vilfredo Pareto. Seine weiteren wissenschaftlichen Arbeiten waren dann trotz zahlreicher Veröffentlichungen weniger innovativ als redaktionell. Léon Walras’ politische Interessen entsprachen denen seines Vaters, so etwa die Ideen zur Reformierung des Steuersystems und der Landverstaatlichung. Mit "Études d’économie sociale; Théorie de la répartition de la richesse sociale" veröffentlichte Léon Walras im Jahr 1896 eine Sammlung seiner Artikel und Lesungen aus dreißig Jahren über Sozialpolitik. Im Jahr 1898 veröffentlichte er in "Études d’économie politique appliquée; Théorie de la production de la richesse sociale" eine Sammlung seiner Zeitungsartikel und Lesungen sowie Korrespondenz zu wirtschaftswissenschaftlichen Themen. Hierin formulierte er am Beispiel des Aktienmarkts das nach ihm benannte Walras-Gesetz. Die "Éléments d’économie politique pure, ou théorie de la richesse sociale" hat er bis zum Jahr 1900 in vier jeweils überarbeiteten Auflagen publiziert. Er war äußerst stolz auf seine Mitgliedschaft bei den wissenschaftlichen Gesellschaften in Europa und den USA sowie die am 21. Juni 1909 erfolgte Ehrung seines vierzigjährigen Dienstjubiläums an der Universität von Lausanne, als eine Bronzebüste aufgestellt wurde.

Walras entwickelte gleichzeitig, aber unabhängig (diese Unabhängigkeit ist umstritten) von Stanley Jevons und Carl Menger das Konzept des Grenznutzens („rareté“ bei Walras).

Seine wesentliche Leistung bestand in der Entwicklung eines allgemeinen Gleichgewichtsmodells der Volkswirtschaft. Hierbei agieren beliebig viele Haushalte und Unternehmen auf beliebig vielen Märkten. Walras vermutete die Existenz eines Preissystems mit der Eigenschaft, dass Angebot und Nachfrage auf allen Märkten zugleich übereinstimmen. Formal bewiesen wurde diese Vermutung erst später durch Abraham Wald und im Rahmen des Arrow-Debreu-Gleichgewichtsmodells.







</doc>
<doc id="3444" url="https://de.wikipedia.org/wiki?curid=3444" title="Mozart (Begriffsklärung)">
Mozart (Begriffsklärung)

Mozart steht für:


Mozart ist der Familienname folgender Personen:


Künstlername:
Siehe auch:


</doc>
<doc id="3445" url="https://de.wikipedia.org/wiki?curid=3445" title="Morphologie (Linguistik)">
Morphologie (Linguistik)

Die Morphologie (von "morphé" ‚Gestalt‘, ‚Form‘, und "lógos" ‚Wort‘, ‚Lehre‘, ‚Vernunft‘), auch: Morphematik oder Morphemik, ist eine linguistische Teildisziplin, deren Untersuchungsobjekt das Wort als größte und das "Morphem" als kleinste Einheit ist. Sie untersucht die Struktur von Wörtern, deren Aufbau und Regularitäten des Aufbaus.

Durch strukturalistisches Vorgehen ("Segmentieren", "Substituieren" und "Klassifizieren") werden die kleinsten bedeutungstragenden oder mit einer grammatischen Funktion versehenen Einheiten (Morpheme) identifiziert. Klassifiziert wird nach dem Kriterium der Bedeutung und der Unabhängigkeit. 

In der traditionellen Grammatik heißt die Morphologie Formenlehre. Sie behandelt, vom Wort ausgehend, die Analyse der Flexions­formen und der Wortarten und beinhaltet außerdem auch die Wortbildung, die sich freilich gerade nicht mit Wortformen, sondern mit Wortstämmen befasst.

Der Begriff „Morphologie“ wurde im 19. Jahrhundert von den Sprachwissenschaftlern aus einer anderen wissenschaftlichen Disziplin übernommen, um typische Wortbildungsmuster zu beschreiben. Ursprünglich stammt der Ausdruck von Johann Wolfgang von Goethe, der ihn für die Lehre von den Formen, besonders in der Botanik, eingeführt hat. August Schleicher übernahm ihn 1859 für die Sprachwissenschaft. Den Begriff „Morphem“ verwendet Leonard Bloomfield bereits in seinem Aufsatz "" (1926): .

Die Morphologie ist mit ihren Analysemethoden und Begriffen ganz wesentlich durch den amerikanischen Strukturalismus geprägt; Bloomfield (1933) und Zellig S. Harris (1951) widmen ihr in ihren grundlegenden Werken eigene Kapitel. 

Der Status der Morphologie hat sich immer wieder geändert, sowohl bei der Frage, welche Bereiche der Sprachbeschreibung ihr zuzurechnen sind, als auch hinsichtlich ihrer Einbettung in die Regelsysteme der verschiedenen Grammatikmodelle. Zur Abgrenzung der Morphologie von der Syntax s. den Artikel über Syntax. Der Grenzbereich zwischen Morphologie und Syntax ist die Morphosyntax und erforscht die gegenseitigen Beeinflussungen von morphologischen und syntaktischen Prozessen. Die gegenseitigen Abhängigkeiten zwischen morphologischen und phonologischen Prozessen, also Vorgängen betreffend die Sprachlaute, behandelt die Morphonologie.

Morphologie als Untersuchung der Wortstruktur generell umfasst in der Regel Wortbildung und Flexion. Einige Schulen betrachten Wortbildung aber als eigene Disziplin. 

Der Unterschied zwischen Flexion und Wortbildung besteht im Wesentlichen darin, dass durch Wortbildung neue Wörter entstehen, während die Flexion die grammatischen Funktionen der Wörter im Satz zum Ausdruck bringt. So wird aus dem Substantiv „(die) Tat“ durch Wortbildung, beispielsweise durch Ableitung mit dem Präfix „un-“, das neue Wort „Untat“. Durch Flexion aber entsteht aus „Tat“ in einem Satz wie „Die Taten müssen bestraft werden“ kein neues Wort, sondern mit der Form „Taten“ wird das zusätzliche Merkmal Plural angezeigt. „Tat“ und „Untat“ sind demnach zwei verschiedene Wörter, während „Tat“ und „Taten“ zwei Formen desselben Wortes darstellen. Der gleiche Fall liegt etwa bei „schreiben“ und „beschreiben“ vor (zwei Wörter) bzw. „schreiben“ und „schreibst“ (zwei Wortformen). Flexionsmerkmale können auch mehr oder weniger bedeutungshaltig sein (etwa im Fall des Plurals).

Abgrenzungschwierigkeiten zwischen Wortbildung und Flexion können dann auftreten, wenn in Flexion und Wortbildung die gleichen grammatischen/semantischen Funktionen zum Ausdruck kommen. Im Deutschen ist das grammatikalische Geschlecht (Genus) eine solche Kategorie: Einerseits gibt es eine Genusflexion bei Artikeln, Adjektiven und Pronomen, das heißt Wörter werden je nach grammatikalischem Geschlecht unterschiedlich flektiert bzw. „der“, „die“ und „das“ sind flektierte Formen des bestimmten Artikels; andererseits existiert bei Substantiven auch eine Genusableitung: aus „Löwe“ wird durch Wortbildung mit dem Suffix „-in“ die weibliche Form „Löwin“. Der Unterschied zwischen Flexion und Wortbildung liegt darin, dass das Auftreten von Flexion auch Gegenstand grammatischer Regeln ist: Ein Artikel muss immer flektiert werden, wenn er in einem Satz verwendet wird; im Regelfall kann man aber nur wenige Substantive durch Genusableitung verändern; in jedem Fall verhalten sich diese dann aber wie eigenständige Wörter, und die Wahl geschieht rein nach der Mitteilungsabsicht.

Die Termini „Morph“, „Allomorph“ und „Morphem“ sind Bezeichnungen für die kleinsten bedeutungs- oder funktionstragenden Bestandteile eines Wortes. Als Morphe bezeichnet man die hinsichtlich ihres Typs noch nicht klassifizierten Einheiten. Beispielsweise liegen in den Wörtern „Lehr-er“, „Kind-er“ und „größ-er“ drei "-er"-Morphe vor. Erst nach Eruierung ihrer Funktion und Bedeutung kann man sie bestimmten Morphemen zuordnen: Das "-er" in „Lehrer“ wird zur Bildung des maskulinen „Nomen Agentis“ benutzt, "-er" in „Kinder“ zur Bildung des Plurals und "-er" in „größer“ zur Bildung eines Komparativs. 

Haben Morphe mit unterschiedlicher Form dieselbe Funktion, handelt es sich um sogenannte Allomorphe eines bestimmten Morphems. So kodieren beispielsweise die Affixe "-er" in „Kinder“, "-e" in „Hunde“, "-(e)n" in "Fragen", "-s" in „Autos“, aber auch das Nullmorphem, wie in „der/die Wagen“, an deutsche Nomen angehängt jeweils Plural; sie sind somit Allomorphe des Pluralmorphems. Haben verschiedene Morpheme dieselbe Form, so handelt es sich um einen Fall von Synkretismus.

Es lassen sich verschiedene Verfahren oder Regeln unterscheiden, die bei der Flexion und der Wortbildung zu beobachten sind. 

Zur Flexion zählen Konjugation und Deklination. Viele Autoren zählen auch die Steigerung, Komparation zur Flexion.

An das Grundmorphem "brauch-" wird "e" als Flexionsmorphem für 1. Person Singular Präsens Indikativ Aktiv angehängt.

In einigen Theorien wird die Flexion allerdings nicht einer separaten Ebene der Morphologie zugeordnet, sondern in das Gebiet der Syntax eingegliedert, da das Erscheinen von Flexion syntaktischen Regeln unterliegt.

Derivation bezeichnet Wortbildung durch Kombination von Wortstämmen und Affixen. 

An das Grundmorphem "Gesund" wird "heit" angehängt, ein Derivationsmorphem, um Adjektive in Substantive zu überführen. Bei der Bildung des Wortes "Freundlichkeit" wird das Affix "-keit" an einen bereits zusammengesetzten Stamm "freund-lich" angefügt.

Komposition bedeutet die Bildung von Wörtern aus (in der Regel) zwei Wortstämmen, die selbst komplex sein können. Die Bestandteile können also ihrerseits Komposita sein oder Derivationsprodukte.

Durch Kombination des Grundmorphems "Sprach(e)" mit dem aus Derivation entstandenen Wort "Wissenschaft" (Ableitung von "Wissen", dies gebildet aus "wiss"+"en") entsteht ein Kompositum. Im Falle des Dreifachkompositums "Schifffahrtsgesellschaft" ist zwischen dem Kompositum "Schifffahrt" und dem Simplex "Gesellschaft" das Fugenelement "-s-" eingefügt. Ein anderes Fugenelement ist etwa "-e-" wie in "Schwein-e-braten" (vorwiegend in Deutschland, dagegen "Schwein-s-braten" vorwiegend in Österreich). In den Fällen "Sprachwissenschaft" und "Schulhof" wird bei den ersten Grundmorphemen "Sprache" und "Schule" der Auslaut getilgt.

Hier unterscheidet man in:

– die Abkürzung, bei der man die Anfangsbuchstaben der einzelnen Morpheme, aus denen sich das Wort zusammensetzt, einzeln ausspricht

– das Akronym, das denselben Regeln wie die Abkürzung folgt, wobei hier jedoch ein neues phonetisches Wort entsteht

– die Kürzung, bei der Wortmaterial gelöscht wird, um ein weniger kompliziertes Wort zu erstellen

Als Konversion wird die Bildung eines neuen Worts nur durch Änderung der Wortart eines existierenden Worts oder Wortstamms bezeichnet. Konversionen sind z. B. Infinitive von Verben, die ohne Hinzufügung eines Affixes in Substantive überführt werden. Konversionen von Substantiven in Verben sind auch möglich, ebenso Konversionen von Adjektiven in Verben.

Linguisten sind sich uneinig, ob die Konversion zur Derivation gezählt werden sollte. Manche Linguisten betrachten die Konversion als Derivation, in der einem existierenden Wort ein Nullmorphem hinzugefügt wird (auch „Zero-Morphem“ genannt und vielfach in der Form „Ø-Morphem“ geschrieben). Die Konversion wird in diesem Zusammenhang „Null-Ableitung“ genannt. Andere Linguisten betrachten die Konversion nur als eine Änderung der Wortklasse ohne Änderung der Form.

Hierbei verschmelzen zwei bestehende Wörter zu einem neuen. Die Ausgangswörter sind nicht mehr vollständig erkennbar.

Aus Sicht der Informatik lassen sich viele morphologische Phänomene mit regulären Ausdrücken formal beschreiben, besonders wenn sie rein aus Affigierungen ohne weitere Veränderungen des Materials bestehen. Einige Phänomene allerdings, so die arabische Derivationsmorphologie, sind mit regulären Sprachen nicht zu erfassen.




</doc>
<doc id="3446" url="https://de.wikipedia.org/wiki?curid=3446" title="Metastase">
Metastase

Als Metastase (, „Wanderung“) wird in der Medizin eine Absiedelung eines krankhaften Geschehens in einem vom Ursprungsherd der Krankheit entfernten Körperteil bezeichnet. Meistens erfolgt eine solche Verschleppung über das Blut oder die Lymphe im Zusammenhang mit einer Krebserkrankung.


Hier wird nur der Begriff "Metastase" im engeren Sinne von "Geschwulstmetastase" behandelt.

Die Fähigkeit eines Tumors zur "Metastasierung" (Bildung von Metastasen) verschlechtert die Heilungschancen einer Krebserkrankung erheblich. Die tatsächlichen Heilungschancen hängen von der Art und der Lokalisation des Tumors ab. So können einige Tumoren, wie etwa Lymphome, trotz Metastasenbildung noch recht gut auf eine medikamentöse Behandlung ansprechen, während aggressive solide Tumoren infolge des invasiven Wachstums auch ohne Metastasierung so lebensbedrohlich sind, dass sie zum Tod führen können.

Schon sehr kleine Tumoren können metastasieren, z. B. ein Brustkrebs von einem Zentimeter Durchmesser in 20 % aller Fälle. Durchschnittlich werden bei 30 % aller Patienten mit Malignomen Metastasen schon bei der Erstdiagnose festgestellt. Bei weiteren 30 % findet man sie erst im weiteren Behandlungsverlauf. Bei Vorliegen von sichtbaren Fernmetastasen sind in der Regel zahlreiche Mikrometastasen vorhanden, was die Prognose erheblich verschlechtert.

Metastasen entstehen, indem sich Krebszellen vom ursprünglichen Tumor ablösen, mit dem Blut oder mit der Lymphe wandern und sich in anderen Körperteilen wieder ansiedeln und vermehren. Je nach dem Ausbreitungsweg heißen sie "hämatogene" (Blut) oder "lymphogene" (Lymphe) Metastasen. Ob Krebszellen metastasieren, hängt nach neuesten Forschungsergebnissen von ihrer Fähigkeit ab, embryonale Transkriptionsfaktoren einzuleiten.

Voraussetzung für die Metastasierung ist, dass der Krebs invasiv wächst, d. h. in angrenzende Strukturen hinein mit Durchbruch in Blut- oder Lymphgefäße. Gutartige Tumoren metastasieren definitionsgemäß niemals. Aber auch nicht jeder invasiv wachsende Tumor verursacht Metastasen. Wenn ein solcher Tumor keine Metastasen bildet, bezeichnet man ihn als "semimaligne" („halb bösartig“). Dies ist beispielsweise bei Basalzellenkrebs der Fall.

Solange ein Tumor keine Metastasen bildet, bezieht sich der Begriff "Invasion" auf sein Wachstum in umgebendes Gewebe hinein. Normalerweise wird in diesem Sinne zwischen "Invasion" und "Metastasierung" (Metastasenbildung) unterschieden. Man kann aber auch das Eindringen der Metastasen in andere Regionen des Körpers als "Invasion" bezeichnen und den Begriff in diesem Sinne weiter fassen.

Die Fähigkeit, körpereigene Sperren wie die Basalmembran oder die Blut-Hirn-Schranke zu durchdringen, ein Blut- oder Lymphgefäß aktiv aufzusuchen (Chemotaxis), in das Gefäß einzudringen, sich während der Wanderung im Blut gegen das körpereigene Immunsystem zu wehren, andernorts das Gefäß zu verlassen und sich schließlich zu vermehren, wird als "Invasivität" bezeichnet. Sie ist eine aktive Leistung maligner Krebszellen, und zwar abhängig von den jeweiligen genetischen Besonderheiten des individuellen Tumors. Nur etwa 0,01 % aller im Blut zirkulierenden Krebszellen schaffen es schließlich, eine metastatische Kolonie zu bilden.

Die biochemischen Vorgänge, die bestimmte Zellen zur Metastasierung befähigen und andere nicht, sind Gegenstand intensiver Forschung. Es konnte z. B. gezeigt werden, dass die verminderte Expression von Cadherinen („Klebemolekülen“) auf ihrer Zellmembran die Tumorzellen beweglicher macht. Für die Anheftung der Zellen im Zielgebiet sollen andere Membranstrukturen, die sogenannten Integrine, eine Rolle spielen. Es sind bereits Onkogene und Tumorsuppressorgene identifiziert worden, deren Expression die Metastasierungstendenz erhöhen bzw. senken kann.

Passiv (etwa durch Biopsienadeln) abgelöste und verschleppte Tumorzellen verursachen nur sehr selten Metastasen.

Neuere Diagnoseverfahren belegen die Wichtigkeit der zirkulierenden Tumorzellen epithelialen Ursprungs (CETC, "circulating epithelial tumor cells"). In der adjuvanten Situation, das heißt nach der operativen Entfernung des Primärtumors, werden verschiedene Chemotherapieverfahren eingesetzt, um die restlichen im Körper verbliebenen und im Blut zirkulierenden Tumorzellen zu vernichten. Im Jahr 2005 zeigten Gianni Bonadonna und Kollegen, dass ca. 50 % der Chemotherapien zu keinen wesentlich erhöhten Lebensverlängerungen bei den Patienten geführt haben. Dieses Ergebnis kann darauf zurückgeführt werden, dass bei der Auswahl der Chemotherapeutika auf eine Biopsie des Primärtumors zurückgegriffen wird. Einige Wissenschaftler nehmen mittlerweile an, dass die Auswahl der nach den S3-Leitlinien zur Verfügung stehenden Therapeutika mittels Chemosensitivitätstests (In-vitro-Bluttests, bei denen die Absterberate der Tumorzellen unter Zugabe der Therapeutika in einer Blutprobe vor Verabreichung getestet werden) die Wirkungswahrscheinlichkeit der Therapeutika stark verbessern könnte.

Der erste Zielort für Metastasen liegt in der Regel stromabwärts, also für Tumoren der Körperwand und Extremitäten in der Lunge und im Gehirn, für Tumoren des Darmes in der Leber (denn das vom Darm kommende Blut fließt zunächst in die Leber und dann erst zum Herzen zurück). Die Lymphgefäße führen zu Lymphknoten, die meist in räumlicher Nähe zum Tumor liegen, für den Brustkrebs z. B. in der Achselhöhle. Es ist aber durchaus möglich, dass ein erster Zielort übersprungen wird. Manche Tumorarten metastasieren in ganz spezifische Organe, etwa Lungenkrebs in die Nebennieren, offenbar infolge von Oberflächeneigenschaften der Krebszellen. Dieser Zielmechanismus (Organotropie, "homing" tumors) ist noch nicht völlig verstanden.

Bei osteoklastischen Knochenmetastasen beeinflussen die Tumorzellen die körpereigenen Osteoklasten so, dass diese vermehrt Knochenhartsubstanz abbauen, um das weitere Wachstum der Metastase zu ermöglichen. Nach heutigem Wissensstand sind metastasierende Krebszellen im Gegensatz zu anderen Zellen außerdem dazu in der Lage, umgebende Blutgefäße zum Aussprossen zu veranlassen (Angiogenese). Nur deswegen kann die neue Metastase mit Blut versorgt werden. Tumoren ohne angiogenetische Fähigkeit werden nicht größer als 0,3 mm.

Man unterscheidet gemäß der TNM-Klassifikation lokale Metastasen, regionäre Metastasen und Fernmetastasen:



Je nach Lokalisation und histologischem Typ metastasieren maligne Tumoren in unterschiedlichem Maße lymphogen und hämatogen. Auch die Lokalisation von Fernmetastasen ist in großem Maße abhängig von Histologie und Lokalisation des Primärtumors. Nach dem gegenwärtigen Kenntnisstand können Fernmetastasen selbst keine Metastasen bilden. Sie entstehen offensichtlich ausschließlich aus Zellen des Primärtumors.

Ist ein ganzes Organ, beziehungsweise eine Körperhöhle diffus von Tumorzellen durchsetzt, bezeichnet man dies als Karzinose (oder Karzinomatose). Beispiel: Bauchfellkarzinose bei Darmkrebs.

Verschiedene Organe haben verschiedene bevorzugte Lokalisationen der Fernmetastasen. In der Regel wird dabei der Pfortader- vom Cava-Typ unterschieden.

Einige Tumoren haben ihren ersten Metastasierungsort typischerweise im Knochen, so Prostata (Lendenwirbel- und Beckenbereich), Brustdrüse (Wirbelsäule), Nieren.

Verschiedene Tumoren metastasieren auch häufig in die Haut, es handelt sich dabei vor allem um Metastasen des malignen Melanoms sowie anderer Hauttumoren. Auch Lymphome befallen häufig die Haut, allerdings bezeichnet man diesen Befall durch eine Systemerkrankung nicht als Metastase.

Fast niemals von Metastasen betroffen sind Herz, Milz und Nieren. Das ist eine erstaunliche Tatsache, weil diese Organe einen hohen Blutdurchfluss haben. Warum sie seltener befallen werden, ist nicht sicher geklärt. Ebenfalls sehr selten sind Metastasen in anderen Organen wie Bauchspeicheldrüse, Magen, Darm (außer durch lokales Wachstum), Schilddrüse, Thymus und Nebenniere.

Bei Krebserkrankungen werden neben dem Primärtumor in der Regel die Lymphknoten der Abflussbahn und „suspekte“ Lymphknoten aus der vorhergehenden Diagnostik (körperliche Untersuchung, Computertomografie, Magnetresonanztomografie) mit entnommen und histopathologisch auf regionäre Metastasen untersucht. Bei unklarer Lymphabflussbahn, z. B. bei Hauttumoren, wird über das Einspritzen radioaktiv markierter Farbe der Sentinel-Knoten („Wächterknoten“) mittels Geigerzähler und Farbmarkierung operativ aufgesucht. Lymphknotenmetastasen verschlechtern die Prognose und führen bei kurativem Ansatz zu einer aggressiveren Therapie.

Zur Lokalisation von Fernmetastasen werden bei gesichertem Primärtumor im Sinne eines Stagings routinemäßig folgende Untersuchungen veranlasst:

Bei HNO-Tumoren gehört daneben zur Metastasensuche die Panendoskopie. Bei gynäkologischen und urologischen Tumoren ist ein lokoregionäres Tumorwachstum in umliegende Organe besonders häufig, deshalb ist die wichtigste Staginguntersuchung die Computertomografie des kleinen Beckens.





</doc>
<doc id="3448" url="https://de.wikipedia.org/wiki?curid=3448" title="Meder">
Meder

Meder bezeichnet:
Meder ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="3451" url="https://de.wikipedia.org/wiki?curid=3451" title="Marokko">
Marokko

Marokko (, / ), Langform Königreich Marokko, ist ein Staat im Nordwesten Afrikas. Er ist durch die Straße von Gibraltar vom europäischen Kontinent getrennt. Als westlichstes der fünf (mit Westsahara sechs) Maghrebländer grenzt es im Norden an das Mittelmeer, im Westen an den Atlantischen Ozean und im Osten an Algerien. Marokkos Südgrenze ist wegen des Westsaharakonfliktes bis zum Abhalten eines UN-Referendums über die zukünftige Zugehörigkeit der Westsahara international umstritten.

Marokko ist seit 1956 unabhängig und gemäß Verfassung von 1992 eine konstitutionelle Monarchie. Die bedeutendsten Städte des nordafrikanischen Landes sind Casablanca, die Hauptstadt Rabat, Fès, Marrakesch, Agadir, Tanger und Meknès. Neun Stätten gehören zum Welterbe in Marokko.

Während sich das Land in der eigenen offiziellen Staatsbezeichnung "al-Mamlaka al-Maghribīya" (übersetzt "Das Land des Sonnenuntergangs)" als „Maghrebinisches Königreich“ bezeichnet (bis in die 1960er „Scherifisches Maghrebinisches Königreich“), hat sich international die europäische Ableitung des Namens der ehemaligen Hauptstadt Marrakesch () für das gesamte Königreich Marokko durchgesetzt.

Im Vergleich mit anderen afrikanischen Staaten besitzt Marokko kein großes Staatsgebiet, in seiner Oberflächenform zeigt es jedoch ein überaus wechselvolles Bild. Im Wesentlichen lassen sich folgende natürliche Einheiten unterscheiden: die Küstenregionen im Norden und Westen; die atlantische Region mit der Marokkanischen Meseta; die montane Region mit dem Hohen und Mittleren Atlas und dem Rifgebirge; schließlich die transmontane Region mit den Plateaus im nordöstlichen Grenzgebiet, dem Antiatlas und den Beckenlandschaften im Randbereich der Sahara.

Die Mittelmeerküste ist überwiegend steil und felsig und weist viele Kaps und Buchten auf. Nur im Mündungsbereich des Moulouya nahe der algerischen Grenze erweitert sich die mediterrane Küstenlandschaft zu einem Becken. Im Westen läuft der gebirgige Küstenabschnitt in der sichelförmig nach Europa gerichteten Nordwestspitze Afrikas aus.

Die Atlantikküste dagegen ist eine flache, kaum gegliederte Ausgleichsküste mit starkem Sandtransport und deshalb nur schlecht für Häfen geeignet. Landeinwärts folgen hier breitere Küstenebenen wie die Niederung des Sebou bei Kenitra und die weitläufige Küstenmeseta von Casablanca. Weiter zum Innern steigt das Gelände auf etwa 450 m über dem Meeresspiegel zum zentralen Teil der Marokkanischen Meseta an, einer weiten Tafellandschaft, die auch als Binnenmeseta oder Hochebene von Marrakesch bezeichnet wird. Sie besteht hauptsächlich aus schwach gewellten, steppenhaften Hochflächen, über die vereinzelt Inselberge aufragen.

Im Süden und Osten wird die Meseta von den markanten Gebirgszügen des Hohen und Mittleren Atlas umrahmt. Dieses gewaltige Faltengebirge wurde im Tertiär beim Zusammenstoß der Afrikanischen mit der Eurasischen Platte aus dem damaligen Sedimentationsbecken herausgehoben. Erdbeben wie das von Agadir im Jahre 1960 zeugen davon, dass die gebirgsbildenden Vorgänge in diesem Raum bis heute nicht abgeklungen sind. Das Atlasgebirge bildet gleichsam das morphologische Rückgrat des Landes und stellt sowohl eine naturräumliche als auch eine wirtschaftlich-kulturelle Barriere dar. Als wichtige Klimascheide trennt der Gebirgswall das atlantisch-mediterrane Marokko vom saharisch geprägten Landesteil.

Der Hohe Atlas erstreckt sich in leichtem Bogen über rund 800 km von Südwesten nach Nordosten. Mit seinen schroffen, gratigen Gebirgsformen und den steilen Gipfeln hat er Hochgebirgscharakter. Hier liegen die höchsten Erhebungen des gesamten Atlas-Gebirgssystems, ja ganz Nordafrikas, darunter auch der höchste Berg Marokkos, der 4167 m hohe Jabal Toubkal.

Nach Nordosten setzt sich der Hohe Atlas im niedrigeren algerischen Sahara-Atlas fort; im zentralen Marokko schließt sich, nördlich versetzt, auf über 300 km der Mittlere Atlas an. Dieser besitzt in seiner östlichen, steil zur Moulouya-Senke abfallenden Kette ebenfalls über 3000 m hohe Gipfel, weist ansonsten jedoch eher Mittelgebirgsformen auf. Den nördlichen Abschnitt des marokkanischen Atlasgebirges bildet das bis zu 2456 m hohe Rif, ein wildzerklüfteter Gebirgsbogen, der sich von der Straße von Gibraltar parallel zur Mittelmeerküste bis zur Mündungsebene des Moulouya erstreckt. Die Längsfurche zwischen dem Rif und dem Mittleren Atlas, die „Pforte von Taza“, ist das wichtigste west-östliche Durchgangstal Marokkos.

Östlich des Moulouya-Tals, das im nördlichen Marokko die montane von der transmontanen Region trennt, steigt das Gelände allmählich zu weiten, steppenhaften Plateaus an, die zum Hochland der Schotts in Algerien überleiten. Die Gebirgszüge südöstlich des Atlashauptkammes, der Anti-Atlas und seine östliche Fortsetzung Jabal Sarhro sowie der südlich parallel ziehende Jabal Bani, gehören ihrem Aufbau nach nicht mehr zu den tertiären Faltengebirgen, sondern sind Teil der alten afrikanischen Masse. Im Süden davon erstrecken sich Randlandschaften der Sahara, zu denen auch die Beckenregion des Tafilalt und die Senke des Draa gehören.

Im Gebiet der Westsahara folgen auf eine breitere Küstenebene bis über 350 m ansteigende, von Wadis zerschnittene und mit Dünen überzogene Sandsteinplateaus, die gleichfalls der Sahara zuzurechnen sind.

Das Klima Marokkos zeigt einen Übergang vom mediterran beeinflussten Nordwesten des Landes zum saharisch-kontinentalen Südosten und Süden. Der Hohe und Mittlere Atlas, die zusammen als eigener Klimaraum aufzufassen sind, bilden mit ihrem Hauptkamm die Klimascheide. Der nordwestliche Landesteil hat trockenheiße Sommer mit einer mittleren Augusttemperatur von 23 °C und mittleren Temperaturmaxima zwischen 26 °C (Casablanca) und 29 °C (Tanger). Die Winter sind mild (Januarmittel 12 °C) und regenreich, wobei die Niederschlagsmengen nach Süden hin geringer werden (Tanger 900 mm, Agadir 200 mm Jahresniederschlag). Landeinwärts nimmt der mildernde Einfluss des Meeres rasch ab, so dass in der zentralen Meseta und im Atlasgebirge ausgeprägtes Kontinentalklima herrscht: In Marrakesch (Augustmittel 29 °C) können im Sommer 45 °C erreicht werden, während im Winter die Temperaturen um den Gefrierpunkt liegen können; an Niederschlag fallen kaum 250 mm. Dagegen bringen Steigungsregen an der Westabdachung der Gebirge zum Teil mehr als 1000 mm Niederschlag pro Jahr, der über 1000 m Meereshöhe in den Wintermonaten gewöhnlich als Schnee fällt. In den südlich des Atlas gelegenen Sahara-Randgebieten herrscht extrem trockenheißes Wüstenklima. Es fallen nur unregelmäßig Niederschläge, die selten 200 mm im Jahr erreichen, so dass Ackerbau lediglich in Oasen mit Bewässerung betrieben werden kann. Während der Sommermonate weht zeitweise der Scirocco, ein heißer, staubbeladener Wind aus der Sahara.

Auch die Pflanzenwelt ist durch das Atlasgebirge zweigeteilt: Nordwestlich des Gebirges überwiegt der mediterrane Bewuchs, südöstlich davon die Wüstensteppe. Geschlossene Waldbestände mit Stein- und Korkeichen, Thujen, Atlas-Zedern und Aleppokiefern finden sich noch in den regenreichen Gebirgszonen und den westlichen Ebenen; sie bedecken nur etwa ein Zehntel der Landesfläche. Im südlichen Küstenbereich wachsen Argan-Bäume und Jujuben. Im übrigen Marokko hat der jahrhundertelange Raubbau die Mittelmeervegetation – soweit sie nicht Kulturflächen weichen musste – auf Baumheiden, Erdbeerbäume, Pistazien, Wacholderarten und Zwergpalmen reduziert. Oberhalb der Waldgrenze (bei 3100 m) gibt es eine Stufe von Polsterpflanzen. Jenseits des Atlasgebirges ist Trockensteppenvegetation mit Büschelgräsern und Dornsträuchern vorherrschend; in der nordöstlichen Hochsteppe wächst das widerstandsfähige Halfagras. In den wenigen Oasen werden Dattelpalmen kultiviert.

Die wildlebenden Tiere haben sich in die dünnbesiedelten Gebiete Marokkos zurückgezogen; einige Arten, wie etwa der Leopard und der Wüstenluchs, sind vom Aussterben bedroht. Weitere Säugetiere des Landes sind Berberaffen (Magots), Gazellen, Hyänen, Schakale und Wüstenfüchse (Fenneks); auch Reptilien (Eidechsen, Chamäleons, Schildkröten, Schlangen) kommen zahlreich vor. Bis 2003 wurden 452 verschiedene Vogelarten in Marokko nachgewiesen. 209 Arten, 49 % der nachgewiesenen Vogelarten, brüten regelmäßig im Land, während 15 Arten nur unregelmäßig im Land brüten. Unter den nachgewiesenen Vogelarten befinden sich Störche, Adler, Geier, Bussarde und Milane. Es gibt mehrere Nationalparks in Marokko. Das Gebiet um den Jabal Toubkal im Hohen Atlas wurde bereits 1942 zum Nationalpark erklärt. Der Ifrane-Nationalpark schützt ausgedehnte Zedernwälder, in denen Berberaffen leben.

Um 1950 lebten 9 Millionen Menschen in Marokko, 2017 waren es 35,7 Millionen;. Das Bevölkerungswachstum betrug 2015 1,3 % und das Durchschnittsalter 28 Jahre. Die Fertilitätsrate betrug 2016 2,12 Kinder pro Frau. Auf 1000 Einwohner kamen im selben Jahr 18,4 Geburten und 4,8 Todesfälle. Aufgrund der sinkenden Geburtenziffer beginnt die noch sehr junge Bevölkerung langsam zu altern.

Etwa 45 % der Bevölkerung sind Berber einschließlich 21 % arabisierte Berber. Sie sind heute zumeist sesshafte Bauern; nur eine Minderheit lebt noch als Nomaden oder halbnomadisch in abgelegenen Gebieten des Mittleren Atlas oder auf den Hochplateaus im Osten des Landes. Rund 44 % der Marokkaner sind arabischstämmig. Nordmarokko mit der alten Metropole Fès ist eher arabisch (34 % der Bevölkerung Araber, 25 % arabisierte Berber), Südmarokko und dessen Metropole Marrakesch eher berberisch (30 % der Bevölkerung Berber) geprägt. Dazu kommen 10 % ursprünglich aus Mauretanien stammende Mauren und 1 % Sonstige. Hierzu zählen mehrere Tausend Ausländer, darunter vor allem Franzosen, Spanier, Italiener, Tunesier und Algerier.

Die Bevölkerung ist in Marokko sehr ungleich verteilt. Zwei Drittel der Einwohner leben auf etwa einem Zehntel der Landesfläche im Nordwesten oder Westen. Ballungsgebiete sind die Küstengebiete im Norden und Nordwesten und das Sebou-Tiefland. Marokko hat eine alte Stadtkultur; 2015 lebten 60,2 % der Bevölkerung in Städten. Die Urbanisierung schritt langsamer voran als in anderen afrikanischen Staaten. 27,3 % der Einwohner waren 2015 jünger als 15 Jahre; 6,1 % waren älter als 65 Jahre. Die Lebenserwartung betrug 74,3 Jahre (Frauen: 75,3 Jahre/Männer: 73,3 Jahre).

Etwa 90 % der Marokkaner sprechen das Marokkanische Arabisch, genannt "Darija", den Hassania-Dialekt hingegen nur etwa 0,7 % der Bevölkerung. Von Marokkanern berberischer Abstammung werden verschiedene Berbersprachen gesprochen, gut die Hälfte der Marokkaner beherrscht eine Berbersprache. Zu den Berbersprachen in Marokko gehören Mazirisch (auch "Tamazight", im Mittleren Atlas), Ghomara, Tarifit (im Rif-Gebirge), Taschelhit (in Südmarokko), Tassoussit (in der Sous-Region), Senhaja de Srair und heute nur noch vereinzelt Judäo-Berberisch.
Die offiziellen Sprachen Marokkos sind das Arabische und das Mazirische. Französisch wird im gesamten Land als Handels-, Bildungs- und inoffizielle Arbeitssprache benutzt. Bei der staatlichen Eisenbahn "Office National des Chemins de Fer" (ONCF) ist es die Betriebssprache. Im Norden Marokkos, der Westsahara und um Sidi Ifni wird zusätzlich Spanisch gebraucht. Englisch gewinnt als Sprache der gebildeten Jugend an Bedeutung.

Staatsreligion ist der Islam. Rund 98,7 % der Bevölkerung sind Muslime, davon 90 % Sunniten malikitischer Richtung. 1,1 % der Einwohner bekennen sich zum Christentum (meist Katholiken; siehe Christentum in Marokko) und 0,2 % zum Judentum. In der Volksreligion ist der Glaube an Geister – als Erbe der vorislamischen Berber – tief verwurzelt.

Von 1972 – ein Jahr vor dem Anwerbestopp für Arbeitsmigranten in zahlreichen europäischen Ländern – bis 2005 hat sich die Zahl der in wichtigen europäische Ländern (ohne Großbritannien und Skandinavien) lebenden Marokkaner verneunfacht. Zahlreiche Marokkaner leben vorübergehend im Ausland, vor allem in West- und Südeuropa, oder haben ihr Land auf der Suche nach besseren wirtschaftlichen Perspektiven dauerhaft verlassen. In vielen Ländern bilden Marokkaner und Marokkanischstämmige die größte muslimische Gemeinde. Etwa 1,2 Millionen leben in Frankreich, etwa 750.000 in Spanien, etwa 500.000 in Italien, etwa 350.000 in Belgien, etwa 330.000 in den Niederlanden und etwa 100.000 in Deutschland. Weitere Gemeinden existieren auch in Norwegen, Schweden und Großbritannien. Kleinere, aber rasant anwachsende Gemeinden höher qualifizierter Auswanderer leben in den USA (mindestens 100.000) und Kanada (mindestens 78.000). Schätzungsweise 300.000 Marokkaner leben in anderen Staaten des Maghreb oder in Staaten im Nahen Osten.

Mit der wachsenden Wirtschaftskraft des Königreichs wandern immer weniger Marokkaner in andere Staaten aus. Dafür steigt die illegale Zuwanderung von Schwarzafrikanern "(Subsahariens)". Marokko hat sich seit Mitte der 1990er Jahre zu einem Transitland vorwiegend für Migranten aus Westafrika entwickelt, die aus wirtschaftlichen oder politischen Gründen ihre Heimat verlassen haben; von diesen wollen immer mehr dauerhaft im Land bleiben. 2005 lebten 25.000 schwarzafrikanische Migranten aus Subsahara-Afrika legal in Marokko. Da die Einreise nach Europa massiv erschwert worden ist, entschließen sie sich dazu, in Marokko zu bleiben. Neben den afrikanischen Migranten residierten im selben Jahr 28.000 Europäer im Land, überwiegend in Städten wie Marrakesch.

Marokko liegt beim Pro-Kopf-Einkommen in der höheren Gruppe der afrikanischen Staaten. Die Sozialversicherung umfasst Alters-, Hinterbliebenen- und Invalidenrenten. Auch Leistungen bei Krankheit, Schwangerschaft sowie Familienbeihilfen werden gewährt. Versichert sind allerdings nur Arbeitnehmer in Industrie und Handel, beziehungsweise Genossenschaftsmitglieder. Die Arbeitslosigkeit (2015 durchschnittlich 9,6 %) ist besonders unter Jugendlichen hoch (19,6 %). Viele männliche Jugendliche wandern daher in europäische Staaten aus. Um das Problem der Arbeitslosigkeit zu lösen, gehen staatliche Maßnahmen in Richtung „Marokkanisierung“, das heißt Verdrängung ausländischer Fachkräfte. Die Inflation lag 2015 durchschnittlich bei 1,6 %. Das Gesundheitswesen ist im Vergleich zu anderen afrikanischen Staaten gut entwickelt. Die medizinische Versorgung der Stadtbevölkerung ist allerdings wesentlich besser als die der Landbevölkerung. Knapp die Hälfte aller Ärzte praktiziert in Casablanca und Rabat. Hauptprobleme der Gesundheitsvorsorge sind die Bekämpfung der Durchfall- und Parasitenkrankheiten, der Malaria und teilweise noch der Mangelernährung. Im Jahr 2015 betrugen die Gesundheitsausgaben 2,0 % des Bruttoinlandsprodukts (BIP). Zugang zu Sanitäreinrichtungen hatten 2015 77 % der gesamten Bevölkerung, aber nur 66 % auf dem Land. Allgemeine Schulpflicht besteht für 7- bis 13-Jährige, jedoch werden nur 91 % aller Jungen und lediglich 88 % aller Mädchen eingeschult. Bei leicht sinkender Tendenz sind noch 27,6 % der Erwachsenen Analphabeten, vor allem ist die ältere Gesellschaft vom Analphabetismus betroffen, die nie eine Schule besuchen musste. Das Schulsystem ist dreistufig: auf fünf Jahre Grundschulausbildung folgen in der Sekundarausbildung eine vierjährige Unterstufe und eine dreijährige Oberstufe. Daran schließt die Hochschulausbildung an. Universitäten befinden sich in Rabat, Casablanca, Oujda, Marrakesch, Ifrane, Fes, Tanger, Tetouan und vereinzelt in kleineren Städten. Auch Fachhochschulen wurden gegründet wie z. B. in Al-Hoceima. Im Allgemeinen ist die Ausstattung der meisten Hochschulen – auch der neueren – noch völlig unzureichend. Auf eine lange Tradition kann die islamische Al-Qarawiyin-Universität in Fes zurückblicken, die bereits 859 gegründet wurde.

Entwicklung der Lebenserwartung in Marokko über Zeit

Bereits im 2. Jahrtausend v. Chr. besiedelten Berber-Stämme das Gebiet des heutigen Marokko. Vom 12. Jahrhundert v. Chr. an gründeten die Phönizier an der Küste Handelsniederlassungen, darunter auch Karthago im Gebiet des heutigen Tunesien, das seit dem 8. Jahrhundert v. Chr. Stützpunkte im Mittelmeerraum errichtete. Im Innern des Landes bildete sich wahrscheinlich schon im 4. Jahrhundert v. Chr. das Königreich Mauretanien heraus, das durch den Zusammenschluss mehrerer Berber-Stämme entstanden war.

Nach der Zerstörung Karthagos im Dritten Punischen Krieg 146 v. Chr. gerieten die Handelsniederlassungen an der Küste wie auch das Königreich Mauretanien unter römischen Einfluss. 33 v. Chr. wurde das Gebiet römisches Protektorat und schließlich 42 n. Chr. als "Mauretania Tingitana" mit der Hauptstadt Tingis (heute Tanger) und "Mauretania Caesariensis" mit der Hauptstadt Caesarea (heute Cherchell in Algerien) zu römischen Provinzen. Rom errichtete in der Folge zum Schutz gegen die im Gebirge und in der Sahara lebenden Berber-Stämme im Süden einen Grenzwall (Limes).

429 fielen die Vandalen in Nordafrika ein, konnten sich jedoch nur bis 477 in Tanger und Ceuta behaupten. Unter Kaiser Justinian I. (527–565) stießen oströmische Truppen bis zur Straße von Gibraltar vor, beschränkten aber ihre Herrschaft im heutigen Marokko ebenfalls auf diese beiden Städte und befestigten sie.

Um 700 erreichten die Araber bei ihren Vorstößen nach Westen die Gegend, begannen mit der Islamisierung der unterworfenen Bevölkerung und benannten sie nach dem arabischen Wort für Westen oder Sonnenuntergang „Maghreb“: "Al-Maghrib" ist heute der offizielle Name Marokkos. Ein islamisierter Berber, Tariq ibn Ziyad, setzte dann 711 mit einer Reitertruppe von Ceuta über die Meerenge nach Spanien über und eroberte das Westgotenreich. Der Ort der Landung, der „Felsen des Tarik“ (arabisch "Jabal Tariq"), trägt seinen Namen: Gibraltar.

Die Araber konnten den Widerstand in Nordafrika zunächst jedoch nicht brechen; gegen die Herrschaft der Kalifen kam es um 750 zu zahlreichen Berber-Aufständen. 789 begründete schließlich Mulay Idris als Idris I. die Dynastie der Idrisiden mit der Hauptstadt Fès. Das Reich war bis Ende des 10. Jahrhunderts Zentrum des Islam in Nordafrika. Die von 1062 bis 1147 herrschenden Almoraviden, Angehörige einer Berber-Sekte aus dem Süden, verlegten die Hauptstadt nach Marrakesch. Die Almohaden (1147 bis 1269) machten Marokko zum Herzstück eines Reiches, das sich von Sizilien im Osten über das Atlasgebirge bis weit nach Spanien hinein erstreckte. Die Herrschaft der folgenden Dynastie, der Meriniden, währte etwa 150 Jahre; die Hauptstadt Fès wurde zu einem Zentrum von Kunst und Wissenschaft. 1420 ergriffen die Wattasiden die Macht, gerieten in der Folge aber immer mehr unter den Druck europäischer Mächte. 1492 wurde die Rückeroberung Spaniens durch die Christen (Reconquista) mit der Einnahme Granadas abgeschlossen.

Ab Beginn des 16. Jahrhunderts hatten Portugiesen und Spanier damit begonnen, an der marokkanischen Küste Stützpunkte anzulegen; Spanien hatte bereits unmittelbar nach Abschluss der Reconquista Sidi Ifni und Melilla besetzt. Um 1520 kontrollierte Portugal alle wichtigen Atlantikhäfen des Landes. Unter der mächtigen Dynastie der Saadier entwickelten sich im 16. und 17. Jahrhundert Handelsbeziehungen zu den europäischen Staaten. Frankreich errichtete in den wichtigsten Hafenstädten Konsulate.

Um 1669 ergriffen die Alawiden, die noch heute in Marokko herrschende Dynastie, die Macht. Sie befreiten nach und nach die meisten der von Spanien und Portugal besetzten Küstenstädte. Nur Ceuta, Melilla und Sidi Ifni blieben spanisch. Marokko war das erste Land, das die jungen USA 1777 offiziell anerkannte. Der "Moroccan-American Treaty of Friendship" von 1783, der von amerikanischer Seite von John Adams und Thomas Jefferson unterzeichnet wurde, ist somit auch der längste ungebrochene Freundschaftsvertrag der USA mit einem anderen Staat.

Nach der Eroberung Algeriens (ab 1830) versuchte Frankreich, seinen Einfluss auf Marokko weiter auszudehnen. 1843/44 kam es zum Krieg, der mit einer Niederlage der marokkanischen Truppen endete. Infolgedessen wurde Marokko zum Zankapfel der miteinander konkurrierenden europäischen Mächte.

Zu Beginn des 20. Jahrhunderts kam es im Zuge dieser Entwicklung zu einer Konfrontation Frankreichs mit dem Deutschen Reich, das versuchte, gegen den wachsenden französischen Einfluss in Marokko eigene wirtschaftliche und politische Interessen durchzusetzen. 1905 stattete Kaiser Wilhelm II. dem Sultan in Tanger einen demonstrativen Besuch ab (→Erste Marokkokrise). Dennoch stand das Deutsche Kaiserreich in der Konferenz von Algeciras 1906 mit seinen Ansprüchen isoliert da und es musste im Berliner Marokko-Kongo-Vertrag von 1911 Marokko als französisches Einflussgebiet anerkennen (→Zweite Marokkokrise/„Panthersprung nach Agadir“).

Bereits ein Jahr später wurde das Land im Protektoratsvertrag vom November 1912 in die Protektorate Französisch-Marokko und Spanisch-Marokko im Norden aufgeteilt; die Stadt Tanger erhielt 1923 als Tanger-Zone internationalen Status. Formal blieb der Sultan Herrscher Marokkos.

Im Süden unterstützte Tihami al-Glawi, das Oberhaupt des einflussreichen Glaoui-Berberstammes, von Anfang an das französische Protektoratsregime gegen den Führer des antikolonialen Aufstandes Ahmed al-Hiba (El Hiba) in Südmarokko und Westsahara. Letzterer hatte den Kampf gegen die Kolonialmacht von seinem Vater Mā al-ʿAinin übernommen. Der einhellige Widerstand der Berber im Norden ging zu dieser Zeit von Moha ou Hammon aus, dessen Stammsitz Khénifra die Franzosen am 12. Juni 1914 eroberten. Am 13. November 1914 fügten die unter Moha ou Hammon versammelten Berbertruppen einige Kilometer südlich von Khénifra den Franzosen die schwerste Niederlage während der „Befriedungsaktionen“ zu. Dabei starben 613 französische Soldaten und für den Generalresidenten Hubert Lyautey schien danach das gesamte Protektorat zu scheitern.

Auch nach dem Ersten Weltkrieg erhoben sich immer wieder Berber. Unter der Führung von Abd al-Karim brach 1921 in der spanischen Zone der Aufstand der Rif-Kabylen aus. Die Unruhen erfassten auch das französische Protektorat. Erst 1926 gelang es Frankreich und Spanien gemeinsam, den Aufstand niederzuschlagen. Unter Sultan Mohammed V. (1927 bis 1961), der im Zweiten Weltkrieg auf Seiten Frankreichs stand, konnte die arabisch-nationalistische Unabhängigkeitsbewegung an Einfluss gewinnen. 1944 konstituierte sich die „Partei der Unabhängigkeit“ "(Al-hizb al-istiqlal)".

Anfang der 1950er Jahre kam es aufgrund der wachsenden Unabhängigkeitsbestrebungen zu Spannungen zwischen dem Sultan und der französischen Protektoratsverwaltung. Im August 1953 verbannten ihn die Franzosen nach Madagaskar und setzten seinen Onkel Muhammad Mulay ibn Arafah als Sultan ein. Daraufhin wurde das Land von einer Welle nationaler Empörung gegen die Fremdherrschaft erfasst. Frankreich und Spanien konnten ihre Protektoratsmacht nicht mehr aufrechterhalten. Muhammad V. konnte im Jahr 1955 zurückkehren.

Die volle Unabhängigkeit von Frankreich und Spanien erlangte das Land 1956. Lediglich die Enklaven Ceuta, Melilla und Sidi Ifni (bis 1969) blieben in spanischem Besitz. 1957 nahm Muhammad V. den Königstitel an. Nach seinem Tode 1961 folgte ihm sein Sohn als Hassan II. auf den Thron, der von Anfang an einen Kurs der Westorientierung mit starker Anlehnung an Frankreich und das Europa der späteren EG anstrebte. Die Spannungen mit dem unabhängigen Algerien führten 1963 zum algerisch-marokkanischen Grenzkrieg. In der gesamtarabischen Politik bemühte er sich um eine Mittlerrolle. 1971/72 und 1983 misslangen Versuche, eine Republik zu errichten.

1976 entließ Spanien seine Provinz Spanisch-Sahara (Westsahara) in die Unabhängigkeit. Mauretanien und Marokko teilten das Land kurzerhand unter sich auf. Kurz danach setzten die Kampfhandlungen zwischen der marokkanischen Armee und Einheiten der Frente Polisario (Volksbefreiungsbewegung der Westsahara) sowie Truppenteilen Algeriens ein, das die Polisario unterstützte. Diese rief die „Demokratische Arabische Republik Sahara“ aus und gründete eine Exilregierung. 1979 schloss Mauretanien einen Friedensvertrag mit der Polisario und räumte seinen Anteil an der Westsahara. Daraufhin okkupierte Marokko das ganze Territorium. Seither tobte in der Westsahara ein blutiger Krieg, der Marokko stark belastete. Im August 1988 stimmten das in der Westsahara-Frage international zunehmend isolierte Marokko wie auch die Polisario dem Westsahara-Plan der Vereinten Nationen zu, der einen Waffenstillstand vorsah sowie die Durchführung einer Volksabstimmung über das zukünftige Schicksal des okkupierten Territoriums. 1991 wurde ein Waffenstillstand vereinbart. Das Referendum wurde seither aber immer wieder verschoben, weil beide Seiten keine Einigung über die genaue Zahl der Stimmberechtigten erzielen konnten. Unterdessen betreibt Marokko eine umfassende Besiedlungspolitik in der Westsahara. Ein Großteil der westsaharischen Bevölkerung lebt in Flüchtlingslagern in Algerien.

Mit Spanien existieren noch ungelöste Territorialstreitigkeiten über die Exklaven Ceuta und Melilla sowie über die küstennahen Inseln Isla Perejil, Chafarinas, Alhucemas und Vélez de la Gomera. Die Souveränität Spaniens über die genannten Gebiete wird von Marokko nicht anerkannt. Der Streit eskalierte 2002, als ein winziges Kontingent marokkanischer Truppen die Isla Perejil besetzte. Ein spanisches Armeekommando überwältigte die marokkanischen Soldaten unblutig und repatriierte sie. Der Streit wurde dabei diplomatisch durch die Vermittlung der USA und der EU entschärft. Ungeachtet dieses kleinen Zwischenfalles gestaltet sich die praktische Zusammenarbeit spanischer und marokkanischer Behörden vor Ort ausgezeichnet, was beide Seiten stets offiziell beteuerten. Die amerikanisch-marokkanischen Beziehungen sind dagegen so gut, dass die USA Marokko im Juni 2004 den Status eines hauptverbündeten Alliierten außerhalb der NATO zuerkannten.

König Mohammed VI. setzte im April 2004 eine unabhängige nationale "Kommission für Gleichheit und Versöhnung" ein, die sich mit der Aufarbeitung von Menschenrechtsverletzungen aus der Regierungszeit seines Vaters, König Hassans, befassen sollte. Ab Dezember 2004 fanden öffentliche Anhörungen ehemaliger Gefangener statt, die auch im Radio und Fernsehen übertragen wurden. Um die Idee der nationalen Versöhnung nicht zu gefährden, wurden die Beschuldigten nicht beim Namen genannt. Hauptziel ist nicht die strafrechtliche Verfolgung der Täter, sondern die moralische Wiedergutmachung für die Opfer und ihre Familien. Die Lage der Menschenrechte bot dennoch Anlass zur Kritik. Die Organisation Reporter ohne Grenzen erhob zur selben Zeit schwere Vorwürfe gegen die Regierung wegen der Inhaftierung und Folterung von Journalisten. Außerdem waren im Zusammenhang mit den Terroranschlägen vom 11. September 2001 sowie von Casablanca (16. Mai 2003) und Madrid (2004) zwischen 2.000 und 7.000 Personen verhaftet worden. Deshalb startete im Mai 2005 ein neues Programm zur wirtschaftlichen und sozialen Entwicklung der Slums, die als Hauptnährboden für islamistische Gewalt gelten.

Anfang 2011 kam es unter dem Eindruck des Arabischen Frühlings zu Protesten in mehreren Städten, bei denen eine demokratische Verfassung gefordert wurde. Die Staatsspitze reagierte darauf mit einem Verfassungsreferendum, das von der Oppositionsbewegung jedoch boykottiert wurde. Die mit 98 % Zustimmung angenommene Verfassungsänderung schreibt erstmals Tamazight als Amtssprache neben Arabisch fest und verschiebt einige Kompetenzen vom König auf den Premierminister und das Parlament. Auch ist der König nun verpflichtet, den Premierminister aus der Partei zu ernennen, die bei den Wahlen die meisten Parlamentssitze erhalten hat. Bisher hatte er diesbezüglich freie Hand.

Gemäß der Verfassung von 1992, zuletzt geändert 1996 und 2011, ist Marokko eine nominelle konstitutionelle Monarchie, deren derzeitiges Staatsoberhaupt seit dem 23. Juli 1999 König Mohammed VI. ist, der der Dynastie der Alawiden angehört. Er ist Oberbefehlshaber der Streitkräfte. Der König ernennt nicht nur den Ministerpräsidenten, der in der Regel von der stärksten politischen Partei des Parlamentes vorgeschlagen wird, sondern auch einzelne Minister und muss dem gesamten Kabinett zustimmen. Außerdem hat er das Recht, das Parlament jederzeit aufzulösen und den Ausnahmezustand zu verhängen. Im Vergleich zu europäischen Monarchen hat der marokkanische König weitergehende Kompetenzen unter einer eingeschränkten Gewaltenteilung. Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt Marokko Platz 105 von 167 Ländern und gilt damit als eine „Hybridregime“ aus demokratischen und autoritären Elementen. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „teilweise frei“ bewertet.

Unter dem Eindruck der Revolutionen in Tunesien und Ägypten demonstrierten am 20. Februar 2011 auch die Marokkaner für politische Reformen und mehr Demokratie. Als Reaktion darauf schlug König Mohammed VI. am 17. Juni 2011 eine Verfassungsreform vor, die am 1. Juli 2011 in einem Referendum bestätigt wurde. Gemäß der Reform gibt der König einen Teil seiner bisherigen Rechte an Parlament und Premierminister ab. Er ist außerdem verpflichtet, den Regierungschef aus der Partei mit den meisten Parlamentssitzen auszuwählen.

Ministerpräsident war seit November 2011 Abdelilah Benkirane, zuvor Generalsekretär der moderat islamistischen Partei für Gerechtigkeit und Entwicklung (PJD), der seit den Parlamentswahlen in Marokko 2011 stärksten Partei im Parlament. Nach der Parlamentswahl in Marokko 2016, welche die PJD ebenfalls gewann, gelang es Benkirane nicht, eine neue Regierung zu bilden. Am 15. März 2017 wurde er daher vom König als Ministerpräsident offiziell entlassen, sowie sein Parteikollege Saadeddine Othmani mit der Regierungsbildung betraut.

Marokko verfügte nach der Verfassungsreform von 1996 über ein Zweikammersystem aus Nationalversammlung und Senat. Die Nationalversammlung bestand aus 325 Mitgliedern, die alle fünf Jahre direkt gewählt werden; 30 Sitze waren für Frauen reserviert. Wahlberechtigt waren alle Marokkaner ab dem Alter von 20 Jahren. Die Nationalversammlung konnte mit Zweidrittelmehrheit dem Ministerpräsidenten das Misstrauen aussprechen. Der Senat bestand aus 270 Mitgliedern, die alle neun Jahre in indirekter Wahl bestimmt wurde. Die vom Parlament verabschiedeten Gesetze bedurften der Zustimmung des Monarchen. Um die Reformen zügig umzusetzen, wurde die Parlamentswahl um etwa zehn Monate auf den 25. November 2011 vorverlegt. Nach der neuen Verfassung werden insgesamt 395 Parlamentssitze vergeben, davon 305 Sitze über Parteilisten in 92 Wahlbezirken. Die weiteren 90 Sitze werden über eine so genannte "nationale Liste" gewählt; 60 Sitze sind für Frauen und 30 Sitze für junge Abgeordnete unter 40 Jahren reserviert.

Die letzte Wahl nach der alten Verfassung fand im September 2007 statt. Sie galt als geordnet und transparent, allerdings lag die Wahlbeteiligung bei nur 37 % – ein historisches Tief. Stärkste Parteien wurden Istiqlal(PI), PJD, welche in der Folge den Ministerpräsidenten stellte, die MP, RNI und UNFP. Die moderat islamistische Partei für Gerechtigkeit und Entwicklung (PJD) wurde zweitstärkste Partei.

Die Parlamentswahl in Marokko 2011, an welcher 31 nationale Parteien bzw. Listen teilnahmen, gewann die Partei für Gerechtigkeit und Entwicklung, frz. "Parti de la justice et du développement" (PJD). Die Wahlbeteiligung lag trotz Boykottaufrufen einiger Oppositionsgruppen bei knapp 45 % und war damit gegenüber der letzten Wahl deutlich höher. Dieser Prozentsatz bezieht sich allerdings lediglich auf die Zahl der registrierten Wähler, die trotz Bevölkerungswachstums mit rund 13,5 Millionen geringer war als 2007 (ca. 15 Millionen). Die wahlberechtigte Bevölkerung insgesamt betrug rund 21 Millionen.

Die Parlamentswahl in Marokko 2016, mit 24 teilnehmenden Parteien, wurde erneut von der PJD für sich entschieden – es konnten sogar Stimmengewinne erzielt werden. Der bisherige Ministerpräsident Abdelillah Benkirane wurde daher vom König erneut mit der Regierungsbildung beauftragt, welche allerdings scheiterte. Am 15. März 2017 wurde Benkirane daher trotz seines Wahlsieges als Ministerpräsident entlassen.
Die restlichen 7 Sitze verteilen sich auf vier weitere Parteien.

Die Verfassung des Landes gewährt eine unabhängige Judikative. Das Rechtswesen orientiert sich weitgehend am französischen Vorbild. Im Familien- und Erbrecht gilt die Moudawana, die europäisches Zivilrecht enthält und auf die Gesetze des sunnitischen Islam (Schari’a) zurückgeht. Für Juden gilt talmudisches Familienrecht. Höchste juristische Instanz ist der Oberste Gerichtshof in Rabat. Dessen Richter werden vom König ernannt.

Amnesty International sieht die Rechte auf Meinungs-, Vereinigungs- und Versammlungsfreiheit im Zusammenhang mit der staatlichen Sicherheit stark eingeschränkt. Die Regierung reagiert teilweise sehr intolerant auf Meinungen oder Informationen, die als Angriff auf die Monarchie gewertet werden. So wurden Menschenrechtsverteidiger, Journalisten und weitere Personen strafrechtlich verfolgt, weil sie Korruption angeprangert und die Behörden kritisiert hatten.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Marokko Platz 133 von 180 Ländern. In Marokko sitzen vier Blogger und Bürgerjournalisten in Haft. Laut dem Bericht der Nichtregierungsorganisation ist die Situation der Pressefreiheit im Land „schwierig“.

Im Korruptionswahrnehmungsindex (CPI) der Transparency International belegte das Land gemeinsam mit Liberia, Indonesien, Mazedonien und Kolumbien Platz 90 von 176 Ländern (Stand 2016). 

Frauen und Männer haben laut Verfassung (von 2011) die gleichen Rechte. Die marokkanische Verfassung verlangt außerdem, dass Frauen bei der Eheschließung volljährig sein müssen, obwohl gewisse Ausnahmen zulässig sind. Allerdings hat das marokkanische Justizministerium in einer Studie festgestellt, dass die Zahl von minderjährig Verheirateten von 2004 bis 2013 auf fast das Doppelte angestiegen ist (von 18.341 auf 35.152). Laut Genfer Weltwirtschaftsforum rangiert Marokko bei den Frauenrechten auf Platz 133 von 142.

Das marokkanische Strafgesetz Artikel 489 bedroht gleichgeschlechtliche Handlungen – unabhängig vom Geschlecht der Personen – mit einer Gefängnisstrafe von sechs Monaten bis zu drei Jahren und Geldbuße. Im 19. Jahrhundert und bis in die 1960er Jahre galt Marokko als insbesondere in der arabischen Welt tolerantes Land in Bezug auf sexuelle Zuwendung unter Männern, die auch heute graduell sichtbar gelebt wird.

2009 wurden 25 Teilnehmer an einem Pilgerfest zu Ehren des Heiligen "Sidi Ali Ben Hamduch," der laut Legende homosexuell gewesen sein soll, aufgrund des Verdachts festgenommen, schwul zu sein; in vielen Jahren davor war die Teilnahme von homosexuellen Paaren an dieser Veranstaltung toleriert worden. Die Homosexuellenorganisation Kifkif (von Gleich zu Gleich) ortet Rückschritte und berichtet konkret von einer Kampagne der islamistischen Partei PJD und einer Fatwa gegen die „Beschönigung der Homosexualität“. Es kommt immer wieder zu Razzien und Verhaftungen von Männern.

Marokko ist Mitglied der


Wegen der Aufnahme der Arabischen Demokratischen Republik Sahara (Westsahara) in die Afrikanische Union (AU) hatte Marokko als einziger afrikanischer Staat seine Mitgliedschaft der AU, die Marokko mitbegründet hat, 33 Jahre lang zurückgezogen. Am 30. Januar 2017 wurde Marokko wieder in die Afrikanische Union aufgenommen.

Marokko ist ein wichtiger Partner der Europäischen Union (EU). Das im Jahr 2000 in Kraft getretene Assoziierungsabkommen bildet den allgemeinen Rahmen für die Beziehungen zwischen der EU und Marokko. Marokko und die EU kooperieren im Rahmen der Europäischen Nachbarschaftspolitik (ENP).

Im Jahr 1987 stellte Marokko einen Beitrittsantrag auf EG-Mitgliedschaft. Dieser wurde vom Rat der EG am 14. Juli 1987 aus geografischen Gründen abgelehnt. Im Rahmen der Euro-Mediterrane Partnerschaft kooperiert Marokko mit der EU.

Im Jahr 2012 schlossen EU und Marokko ein Liberalisierungsabkommen mit Maßnahmen zur gegenseitigen Liberalisierung des Handels mit landwirtschaftlichen Erzeugnissen, landwirtschaftlichen Verarbeitungserzeugnissen, Fisch und Fischereierzeugnissen, dessen räumlicher Geltungsbereich sich mit dem des Assoziierungsabkommens EU-Marokko deckt.

Im Dezember 2011 gab der Rat der Europäischen Kommission grünes Licht für die Aufnahme von Handelsverhandlungen mit Marokko. Seit dieser Zeit verhandelt die EU mit Marokko über ein vertieftes und umfassendes Freihandelsabkommen.

Am 7. Juni 2013 vereinbarten EU und Marokko eine Mobilitätspartnerschaft, wodurch die Verhandlungen über ein Abkommen zur Erleichterung der Visaerteilungsverfahren für bestimmte Personenkategorien (unter anderem Studenten, Wissenschaftler und Geschäftsleute) eintreten und die Verhandlungen über ein Abkommen zur Rückübernahme illegaler Migranten wieder aufnehmen konnten. 

Das marokkanische Militär wurde 1956 nach der Unabhängigkeit von Frankreich und Spanien gegründet. Es ist heute in fünf Teile gegliedert.

Zwischen 1951 und 1963 bestanden in Marokko mehrere Basen des US-amerikanischen Strategic Air Command.

Das Militär kämpfte 1973 im Jom-Kippur-Krieg, nahm 1975 am Grünen Marsch teil und griff 1977 in den Shaba-Konflikt zwischen Zaire und Angola ein. Am 31. August 2006 wurde die allgemeine Wehrpflicht in Marokko abgeschafft.
Die Militärausgaben beliefen sich im Jahr 2015 auf 3,268 Mrd. US-$.

Im Rahmen eines Dezentralisierungsprogrammes wurden 1997 die Präfekturen und Provinzen des Landes zu 16 Regionen (arab. ) zusammengefasst; diese wurden im Jahr 2015 auf 12 reduziert. An der Spitze jeder Region steht ein vom König ernannter Wali (Gouverneur).

Die Regionen sind weiter untergliedert in 13 Präfekturen () und 62 Provinzen (), wobei die Präfekturen weitgehend urbane Gebiete umfassen und die Provinzen eher rural geprägt sind. Die unterste Ebene der Gebietskörperschaften stellen über 1500 Gemeinden () dar. Zwischen den Gemeinden und der Provinz- und Präfekturebene wurden im Zuge einer Dekonzentrationsprogramms Kreise (), Paschaliks () und Caïdats () eingerichtet.

Dakhla-Oued Ed-Dahab, der größere Teil von Laâyoune-Sakia El Hamra und ein kleiner Teil von Guelmim-Oued Noun bilden die Westsahara, deren Zugehörigkeit zu Marokko international nicht anerkannt ist.

Die Einwohnerzahlen der größten Städte, zum Teil mit Vororten, nach einer Berechnung von 2012:

Die Stützen der marokkanischen Volkswirtschaft sind die Landwirtschaft und der Bergbau; daneben wird an einem Aufbau (der Infrastruktur) der Fischindustrie gearbeitet. Von sehr großer Bedeutung ist die Phosphatgewinnung. Die zunehmende Verarbeitung der Rohphosphate in der eigenen Düngemittel- und Chemieindustrie steigert den Ausfuhrwert. Um die Industrialisierung auch in anderen Bereichen voranzutreiben, bemüht sich Marokko um die Ansiedlung ausländischer Investoren. Eine weitere Öffnung des Marktes wurde Mitte der neunziger Jahre angekündigt.

Marokko verfügt über eine marktwirtschaftlich orientierte Wirtschaftsordnung, die den Schutz des Eigentums sowie Gewerbe- und Niederlassungsfreiheit und Wettbewerb vorsieht. Zu Zeiten Hassans II. stellte Marokko einen Antrag zur Aufnahme in die EG, der jedoch abgelehnt wurde. Marokkos Ziel einer engeren Anbindung an die EU ist mit der Unterzeichnung eines Assoziationsabkommens mit der EU im Jahr 1996 (in Kraft getreten im Jahr 2000) ein Stück näher gerückt. Seit den späten 1980er Jahren bemüht sich das Königreich, seine Staatsbetriebe zu privatisieren.

Mit den USA wurde 2004 eine Freihandelszone vereinbart. Das US-amerikanisch-marokkanische Freihandelsabkommen wurde im Juli 2004 vom US-Senat ratifiziert und trat am 1. Juli 2006 in Kraft, es ermöglicht den Handel für 95 % der Industrie- und Konsumgüter ohne Zölle. Seither ist das Handelsvolumen zwischen beiden Ländern um mehr als das 20-fache gestiegen.

Mit der EU gilt seit 2012 ein Freihandelsabkommen für Agrarprodukte, sowie ein Fischereiprotokoll seit 2014. Anfang 2014 war die dritte Verhandlungsrunde zu einem Freihandelsabkommen mit der Europäischen Union abgeschlossen.
Laut Urteil des Europäischen Gerichtshofs vom Dezember 2016 sind Produkte aus dem Territorium Westsahara vom Abkommen jedoch ausgenommen.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Marokko Platz 71 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegte Marokko 2017 Platz 86 von 180 Ländern.

Marokko ist reich an Phosphat; etwa 75 % des weltweit geförderten Phosphats stammen aus Marokko. Daneben gibt es Vorkommen von Erdöl, Erdgas, Kohle, Salz, Eisenerz, Blei, Kupfer, Zink, Silber, Gold, Mangan, Nickel, Cobalt. Seinen Bedarf an Energie kann Marokko nur zu rund 13 % aus eigenen Mitteln decken. In der umstrittenen Westsahara sind ebenfalls große Mengen an Phosphat vorhanden, darüber hinaus werden dort große Erdöl- und Erdgas-Vorkommen vermutet.

Marokko beschränkt die Ausfuhr von gefundenen Meteoriten ungewöhnlicherweise nicht rechtlich. Dies führt vielfach zur Meteoritensuche in der Wüste, auch dem Einschmuggel von Meteoriten aus Nachbarländern wie etwa Libyen und einem breiten öffentlichen Marktangebot. Ein Crowdfunding zum Ankauf des zweiteiligen größeren Mondmeteorits "Oued Awlitis 001" um 110.000 € für das Naturhistorische Museum Wien misslang im Januar 2015.

Laut dem "Office National de l’Electricité et de l’Eau Potable" (ONEE) betrug die installierte Leistung der Kraftwerke in Marokko am Ende des Jahres 2016 8.261,7 MW, davon entfielen auf thermische Kraftwerke 5.412 MW und auf Wasserkraftwerke 1.770 MW. Dargebotsabhängige Windkraftanlagen und Solarkraftwerke waren im Umfang von 898 MW bzw. 181 MW installiert. Insgesamt wurden im Jahr 2016 30,8 TWh produziert, davon 0,4 TWh durch Pumpspeicher, 3 TWh durch Windkraftanlagen und 0,4 TWh durch Solarkraftwerke. Im Jahre 2011 lag Marokko sowohl bzgl. der jährlichen Erzeugung mit 23,65 Mrd. kWh als auch bzgl. der installierten Leistung mit 6.413 MW an Stelle 70 in der Welt. Das mit Stand 2014 größte Kraftwerk des Landes ist das Kohlekraftwerk Jorf Lasfar mit einer installierten Leistung von 2.056 MW, das ca. 1/3 des Strombedarfs Marokkos abdeckt.

Das Verbundnetz Marokkos ist seit 1997 mit dem europäischen Verbundsystem synchronisiert, als ein erstes Drehstrom-Seekabel (400 kV, 700 MW) von Spanien aus verlegt wurde. 2006 folgte ein weiteres Seekabel mit derselben Leistung, so dass die Übertragungskapazität zwischen Spanien und Marokko jetzt bei 1.400 MW liegt. Marokko bezieht jährlich über 5 TWh aus Spanien.

Marokko ist im ländlichen Raum zu 99,4 % elektrifiziert, dieser Wert lag Mitte der 1990er Jahre noch bei 20 %.

Marokko hat sich ambitionierte Ziele gesetzt, die Energiewirtschaft durch eine Energiewende zu transformieren und damit Nachhaltigkeit und Energiesicherheit der Energiewirtschaft Marokkos zu stärken. Die Umstellung von fossilen auf Erneuerbare Energien bei gleichzeitiger Steigerung der Energieeffizienz, die im Rahmen des Nationalen Energieplans vorangetrieben wird, gilt als wichtigste Aufgabe der marokkanischen Politik. Um unabhängiger von fossilen Energieimporten zu werden, investiert das Land in den Ausbau von Wind- und Solarenergie. Am 10. Mai 2013 wurde mit einem symbolischen Spatenstich des Königs Mohammed VI. die Umsetzung des marokkanischen Solarplans, bis zum Jahr 2020 2 GW Solarkapazität aufzubauen, gestartet. Als erstes wird das solarthermische Kraftwerk Ouarzazate errichtet, das unter anderem von Deutschland mit rund 770 Millionen € gefördert wird.

Im Februar 2013 teilte der französische Energieversorger GDF Suez mit, dass er in Marokko den Windpark Tarfaya bauen (bzw. von Siemens bauen lassen) und betreiben will. Nahe dem Küstenstädtchen Tarfaya wurden 131 Windkraftanlagen mit einer Nennleistung von 301 Megawatt errichtet.Der Windpark ging 2014 in Betrieb.

Im März 2014 wurde ein Tender für weitere 850 MW an Windkraftkapazität ausgeschrieben. Bis 2020 sollen 2 GW Windleistung installiert sein und parallel dazu eine eigene Windkraftindustrie aufgebaut werden. Zu diesem Zeitpunkt waren landesweit 495 MW in Betrieb, 450 MW in Bau und über 500 MW in Planung.

Im Februar 2016 ging der erste Teil des solarthermischen Kraftwerks Ouarzazate in Betrieb; zur UN-Klimakonferenz in Marrakesch beschloss das Land in einem Bündnis mit anderen vom Klimawandel betroffenen Staaten (CVF), so schnell als möglich komplett auf erneuerbare Energien umzusteigen.

Zur UN-Klimakonferenz in Marrakesch 2016 konnte sich das Land mit dem Neubau von Solarkraftwerken als Vorreiter für den Klimaschutz in Afrika präsentieren, bis zum Jahr 2030 soll die Hälfte des Stroms mit Sonne, Wind- und Wasserkraftwerken erzeugt werden: In Ouarzazate ist die Strahlungsintensität der Sonne mit jährlich 2500 Kilowattstunden pro Quadratmeter doppelt so hoch wie in Deutschland, für die tägliche Bedarfsspitze am Abend kann die Sonnenenergie mit 537.000 Parabolrinnen (Noor I) in einem Silo mit Spezialsalz gespeichert und mit einer Turbine abgerufen werden. Neben den Grosskraftwerken sollen künftig aber auch Solarmodule auf Hausdächern zur Energieversorgung beitragen.

Die Landwirtschaft Marokkos machte 2003 17 % am BIP aus, kann jedoch als wichtigster Wirtschaftssektor betrachtet werden, da hier 43,6 % der erwerbstätigen Bevölkerung beschäftigt sind. Landwirtschaftlich genutzt werden vor allem der Westen und Nordwesten Marokkos; rund 18 % der Landesfläche sind Ackerland. Umfangreiche Bewässerungskulturen finden sich in den Küstenebenen Rharb (Sebou-Niederung) und Sous sowie bei Marrakesch und Fès; um weitere Flächen bewässern zu können, werden zusätzlich Staudämme gebaut. Die ungleiche Landverteilung zwischen den kleinen Bauern und den Großgrundbesitzern, die den größten Teil des Bodens bewirtschaften, konnte auch durch mehrere Agrarreformen kaum verändert werden. Angebaut werden Getreide (Weizen, Gerste, Mais, Hirse, Reis), Hülsenfrüchte, Zuckerrüben, Datteln, Sonnenblumen, Erdnüsse, Oliven, Zitrusfrüchte (vor allem Orangen), Baumwolle, Wein, Mandeln, Aprikosen, Erdbeeren, Frühkartoffeln, Spargel, Artischocken und Tabak. Die Viehzucht in den Steppen der Meseta, im Osten des Landes und in den Gebirgen wird teilweise nomadisch betrieben (Schafe, Ziegen, Rinder, Esel, Dromedare, Pferde, Geflügel). Rund 10 % des Waldbestandes sind Korkeichen; Marokko ist der drittgrößte Korkproduzent der Welt, nach Portugal und Spanien. Küsten- und Hochseefischerei an der Atlantikküste (Sardinen und Schalentiere) sind bedeutend für den Export.

Auf einer Fläche von ca. 250.000 Hektar wird Cannabis angebaut, um Haschisch zu erzeugen, das in Europa einen Marktanteil von etwa 70 % besitzt. Vom Export, der etwa 3.000 Tonnen Haschisch pro Jahr umfasst, leben schätzungsweise 200.000 Bauern mit Familien, also ca. 1 Million Menschen.

Im Industriesektor, dem Bergbau und der Bauwirtschaft wurden 2003 insgesamt 30 % des BIP erwirtschaftet, dort beschäftigt sind aber nur 19,7 % aller Erwerbstätigen. Die Industrie ist stark auf den Binnenmarkt ausgerichtet; ausländische Märkte gewinnen jedoch an Bedeutung. In der Nahrungsmittelindustrie herrschen die Zucker- und Ölerzeugung sowie die Herstellung von Obst-, Gemüse- und Fischkonserven vor. Günstig entwickelt haben sich Metall- und Kunststoffverarbeitung sowie Kraftfahrzeugindustrie und Montage von Elektrogeräten. Es gibt ferner eine bedeutende chemische Industrie, Zementproduktion und Erdölverarbeitung. Ein nach wie vor wichtiger Wirtschaftszweig ist das traditionelle Handwerk (Teppiche, Leder-, Kupfer-, Gold- und Silberarbeiten).

Das größte Unternehmen Marokkos ist die Firma OCP mit Hauptsitz in Casablanca. OCP ist Weltmarktführer in der Phosphat- und Düngemittel-Produktion.

Der Haupthandelspartner ist Europa, z. B. in der "Pulindustrie", d. h. gekochte Nordseegarnelen werden zum Pulen (Entfernen der Chitin­hülle) nach Polen, Russland oder Marokko gebracht, weil das Pulen dort 20-mal billiger ist als in Deutschland das Maschinenpulen. Die Jobs sind begehrt. Die Arbeiter, meist Frauen (30 % können lesen und schreiben), können etwa 150 Euro im Monat verdienen. Wenn die Garnelen nach Deutschland zurückkommen, sind sie etwa drei Wochen alt.

Das Geld zur Modernisierung der Fischindustrie kommt von der EU. Sie zahlte für Fanglizenzen seit 2007 36 Millionen Euro jährlich. Häfen wurden modernisiert, Fischereizentren und Forschungseinrichtungen wurden gebaut. Allerdings wurden durch die industriellen Massenfangmethoden die Gewässer systematisch leergefischt, die einheimischen Fischer konnten nicht mithalten, um zu Überleben, befördern sie als Fährleute Wirtschaftsflüchtlinge nach Europa (= Kanarische Inseln). „So sorgt die EU indirekt selbst für seeerfahrene ‚Reiseunternehmer/innen‘ und Bootsflüchtlinge“.

Im Dienstleistungsbereich wurden 2003 54 % des BIP erwirtschaftet, wobei 36,7 % der Erwerbstätigen in diesem Sektor arbeiten.

Marokko ist eines der bedeutendsten Reiseziele Nordafrikas und erwirtschaftet 10 % seiner Devisen durch den Tourismus. Etwa 80 % der Touristen, die Marokko besuchen, sind Europäer; die größte Gruppe stellten im Jahr 2013 Franzosen (33 %), gefolgt von Spaniern (12,8 %) und Deutschen (4,46 %). Marokko empfing 2013 zehn Millionen Touristen; 2012 waren es 9,4 Millionen, 2008 waren es insgesamt acht Millionen, die einen Umsatz von ca. 115 Milliarden Dirham generierten. Außer der vielfältigen Landschaft und den kulturellen Unterschieden bietet Marokko eine Vielzahl von Sehenswürdigkeiten seiner orientalischen Geschichte. Die am meisten besuchten Städte sind Marrakesch, Agadir, Casablanca, Tanger, Fès, Ouarzazate und Rabat. Im Jahr 2011 betrugen die Einnahmen durch Touristen in Marokko rund 7307 Millionen US-Dollar.
Der Tourismus ist einer der wichtigsten Standbeine für die wirtschaftliche Entwicklung Marokkos. Er trägt etwa zehn Prozent zum BIP bei und sorgt gegenwärtig für mehr als 500.000 Arbeitsplätze.

Marokko verfügte im Jahr 2013 über 207.572 klassifizierte Hotelbetten. Hinzu kommen tausende von Übernachtungsmöglichkeiten in Riads, renovierte traditionelle Häuser meist in den Altstädten.

Exportiert wurden 2004 Güter in einem Gesamtwert von 9,6 Mrd. US-$. Hauptexportgüter Marokkos sind Konsumgüter, die 37 % des Exportvolumens (davon 31 % Textilien) ausmachten. 27 % waren Halbwaren (8 % Phosphorsäure, 6 % Transistoren, 5 % Düngemittel), 16 % Nahrungsmittel, 8 % Investitionsgüter und 7 % Rohstoffe. Hauptabnehmer marokkanischer Güter sind Frankreich (33 %), Spanien (17 %), Großbritannien (8 %), Italien (5 %), die USA (4 %), Indien (4 %) sowie Deutschland und Brasilien (jeweils 3 %).

Importiert werden nach Marokko jährlich Güter im Gesamtwert von 14,9 Mrd. US-$. Hauptsächlich handelt es sich hierbei um industrielle Vorprodukte und Halbwaren (23 %), Konsumgüter (23 %), Investitionsgüter (21 %), Rohöl (9 %), Nahrungsmittel (9 %), Brennstoffe (7 %) sowie Tiere und Pflanzen (5 %). Hauptlieferanten dieser Importgüter sind Frankreich (18 %), Spanien (12 %), Italien (7 %), Deutschland (6 %), Russland (6 %), Saudi-Arabien (5 %) und die VR China (4 %).

Seit 2012, dem Jahr der Eröffnung des Renault-Werks in Tanger im Norden von Marokko, stiegen die Ausfuhren der Automobilindustrie stetig. Im Jahr 2015 exportierte diese Waren im Wert von 4,45 Mrd. EUR (im Jahr 2014 waren es 3,8 Mrd. EUR), vor Phosphatprodukten mit 4,1 Mrd. EUR und Landwirtschaft und Fischerei mit 3,9 Mrd. EUR. Die Ausfuhren der Textil- und Lederindustrie beliefen sich im Jahr 2013 auf 2,8 Mrd. EUR. In diesem Jahr exportierte das Offshoring und der Elektronikbereich jeweils 0,7 Mrd. EUR. Die junge Luftfahrtindustrie exportierte im Jahr 2014 Waren im Wert von 0,7 Mrd. EUR.

Seine defizitäre Handelsbilanz kann das Land durch Überweisungen der im Ausland tätigen Marokkaner sowie durch steigende Einnahmen aus dem Tourismus teilweise ausgleichen. Allein in Europa leben ca. 2,5 Millionen marokkanische Arbeitsemigranten, die durch ihre Überweisungen für eine Devisenzufuhr von etwa 5 Milliarden Euro sorgen. Im Jahr 2013 wurde Marokko von fast 10 Millionen Touristen besucht. Dies brachte 5,2 Mrd. EUR ins Land.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 29,4 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 25,2 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 1,1 % des BIP.

Die Staatsverschuldung betrug 2016 64,6 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Die Verkehrswege sind vor allem im Nordwesten gut ausgebaut. Das Straßennetz umfasst 62.000 km, ca. die Hälfte ist asphaltiert. Über 1677 km sind Autobahnen. Im Bereich der Großstädte sind die Straßen zum Teil überlastet.

Das vom "Ministère de l’Equipement, du Transport et de la Logistique" unterhaltene Straßennetz wird in vier Kategorien klassifiziert: Autobahnen, Nationalstraßen, Regionalstraßen und Provinzialstraßen.

Die Eisenbahn wird von der staatlichen "Office National des Chemins de Fer (ONCF)" betrieben. Das Rückgrat des Eisenbahnnetzes (2109 Kilometer Streckenlänge) bildet die Strecke von Oujda an der algerischen Grenze über Fes und Casablanca nach Marrakesch, von der mehrere Stichbahnen abzweigen. Über 1000 km der Eisenbahn sind elektrifiziert. Es sind zwei TGV-Linien zwischen Tanger und Agadir und zwischen Casablanca und Oujda geplant, die bis zum Jahr 2030 in Betrieb gehen sollen. Die erste Etappe, die LGV Tanger–Kenitra, soll im Frühjahr 2018 eröffnet werden. Im Schienengüterverkehr ist der Transport von Phosphat zu den Häfen am Atlantik mit etwa 27 Millionen Tonnen bedeutsam.

Marokko besitzt ein gut ausgebautes Flugnetz mit 15 internationalen Flughäfen und einer Vielzahl kleiner nationaler Flughäfen. Führender Flughafen ist Casablanca. Der für den Tourismus wichtigste Flughafen ist Agadir. Führende Airline ist die staatliche Royal Air Maroc. Betrieben werden die Flughäfen durch die "Office National des Aéroports (ONDA)".

In Tanger wurde 2006 bis 2008 eines der größten Hafenprojekte des Mittelmeerraumes realisiert. Gebaut wurden ein Containerhafen mit Tiefwasser-Containerterminal, Terminals für Schüttgut, Stückgut, Öl und Gas sowie ein Fährhafen für 5 Millionen Passagiere und 500.000 Fahrzeuge im Jahr. 
Der Hafen Tanger konkurriert mit dem gegenüber gelegenen Hafen Algeciras (Spanien).

Der Schulbesuch ist seit 1963 für 5- bis 13-Jährige obligatorisch und wurde 2002 auf die bis zu 15-Jährigen erweitert. Die Einschulungsquote beträgt 92 %, von den 15-Jährigen besucht jedoch nur noch die Hälfte die Schule. Der Unterricht erfolgt in den beiden ersten Schuljahren ausschließlich in arabischer Sprache, danach werden Mathematik und Naturwissenschaften in französischer Sprache unterrichtet. Die Alphabetisierungsrate betrug 2015 72,4 % (Frauen: 62,5 %, Männer:82,7 %).

Etwa seit dem Jahr 2000 wurden die Bildungsanstrengungen stark erhöht. Das Bildungsbudget übersteigt seither das vieler anderer arabischer Staaten, jedoch gilt die Effizienz nach Analysen der Weltbank immer noch als sehr gering. Die Abbrecherquote in der Sekundarstufe ist hoch; weniger als 15 Prozent der Schüler erreichen das Abitur. Marokko bildete 2003 mit Jemen und dem Irak nach Weltbankanalysen die Schlussgruppe bei einem Ranking der Schulleistungen in den arabischen Ländern. In Mathematik und Naturwissenschaften stellte Marokko 2003 das absolute Schlusslicht im Vergleich arabischer Länder dar. Ein Schwerpunkt der Bildungsanstrengungen wurde seither auf die Informatikausbildung gelegt. Doch sind auch die neugegründeten Schulen und Hochschulen unterfinanziert. Es fehlt an Computern, Lehrbüchern und Plätzen für Praktika; die Fachrichtungsstruktur geht z. T. am Bedarf vorbei, so dass zahlreiche Absolventen der Sekundarschulen und Hochschulen nur schwer eine angemessene Beschäftigung im Land finden.

Die Ursachen dieser Schwächen sieht die Weltbank vor allem in der quantitativ und qualitativ unzureichenden Primarschulbildung, zu der der mit hohen Kosten modernisierte Überbau des Sekundarschulwesens und der Hochschulen in einem Missverhältnis steht, sowie in einer traditionalistischen pädagogischen und didaktischen Ausbildung, zentralistischen Entscheidungsstrukturen und einer fehlenden Evaluation. Dementsprechend ist das berufsbildende Schulwesen nur schwach entwickelt; es fehlt an praktischen Ausbildungsmöglichkeiten.


Marokko gilt bis heute als beliebter Schauplatz für Historien- und Bibelverfilmungen. Viele namhafte Regisseure, darunter Ridley Scott (Gladiator, Königreich der Himmel) und Franco Zeffirelli (Jesus von Nazareth) haben hier ihre Filme gedreht. Auch wurde hier zwischen 1993 und 2001 die 13-teilige TV-Serie "Die Bibel" produziert. Viele der Einwohner von Ouarzazate, Aït-Ben-Haddou und der unmittelbaren Umgebung sowie deren Familien leben von der Filmindustrie, da sie bei der Produktion oft für die authentische Komparserie sorgen.






</doc>
<doc id="3452" url="https://de.wikipedia.org/wiki?curid=3452" title="Minos">
Minos

Minos () ist in der griechischen Mythologie Sohn des Zeus und der Europa - und der Bruder von Rhadamanthys und Sarpedon. Er war ein König von Kreta, der Gemahl der Pasiphaë.

Nach Diodor (4,60,3) gab es zwei Könige namens Minos, Großvater und Enkel. Während der Großvater der Bruder von Rhadamanthys und Sarpedon war, habe der Enkel Pasiphaë zur Gemahlin genommen. Entsprechend der Parischen Chronik lebte der erste kretische König Minos in der zweiten Hälfte des 15. Jahrhunderts v. Chr., ein zweiter König Minos geriet in der ersten Hälfte des 13. Jahrhunderts v. Chr. mit Athen in Konflikt.

Mit Pasiphaë war Minos der Vater von Akakallis, Androgeos, Ariadne, Deukalion, Glaukos, Katreus, Phaidra und Xenodike.

Herodot bezeichnet Minos als den Errichter der ersten Thalassokratie, die die Piraterie im östlichen Mittelmeer erfolgreich bekämpfte und die Quelle des kretischen Reichtums war. ("Siehe auch" Seeherrschaft.) Nach Thukydides soll Minos die Karer von den Kykladen vertrieben und große Teile des ägäischen Meers beherrscht haben.

Bezüglich der Frage, ob Kreta während der minoischen Zeit wirklich eine Thalassokratie war, herrscht bis jetzt in der Forschung keine Einigkeit.

Die Nymphe Britomartis war eine Tochter des Zeus und somit Halbschwester des Minos. Dieser verliebte sich in sie und verfolgte das wilde Mädchen neun Monate lang durch die Berge Kretas. Als er sie auf einem steilen Felsen des Diktegebirges fast ergreifen konnte, blieb ihr Kleid an einem Myrtenzweig hängen; sie rettete sich durch einen Sprung ins Meer und landete in den Netzen von Fischern, die sie in Sicherheit brachten. Artemis erhob sie später in den Rang einer Göttin.

Um die eheliche Treue ihres Gemahls sicherzustellen, belegte Pasiphaë den König mit einem Zauber: Bei der Umarmung entströmten seinem Leib Schlangen, Skorpione und Tausendfüßler. Da traf Prokris auf der Insel ein, die eben im Streit mit Kephalos lag. Sie heilte Minos von seinem Leiden. Er machte ihr dafür einen unfehlbaren Speer und den schnellen, unsterblichen Hund Lailaps zum Geschenk – Gaben, die einst sein Vater Zeus der Europa überreicht hatte.

In einer anderen Version hieß es nur, dass Prokris die einzige war, die mit Minos ungestraft verkehren konnte, weil sie sich zuvor mit der Essenz einer Heilpflanze wappnete.

Nach einer Erzählung wurden er und seine Brüder von Asterios, dem König von Kreta, adoptiert. In der Frage, wer dessen Nachfolge antreten sollte, kam es zum Streit zwischen den dreien. Minos rief Poseidon um Beistand an und versprach, was immer aus dem Meer erschiene, dem Gott zu opfern. Poseidon schickte ihm einen prächtigen Stier, so dass damit der Streit entschieden war und Minos König von Kreta wurde. Der Stier gefiel ihm allerdings so gut, dass er ihn nun nicht opfern wollte, sondern ein anderes Rind darbringen ließ. Poseidon erkannte den Betrug und schlug zur Strafe Minos’ Gemahlin Pasiphaë mit dem Verlangen, sich mit dem Stier zu vereinen. Sie ließ sich von Daidalos ein hölzernes Gestell bauen, das mit Kuhhaut verkleidet war. Darin verbarg sie sich und ließ sich so von dem Stier begatten. Als Frucht dieser Vereinigung gebar sie ein menschenfressendes Ungeheuer: den Minotauros, ein Wesen mit menschlichem Körper und Stierkopf.

Im Zuge seiner achten Arbeit brachte Herakles den Stier auf die Peloponnes. Dort richtete das wilde Tier großen Schaden an. Androgeos, einer von Minos’ Söhnen, wollte seine Geschicklichkeit im Kampf gegen den Stier erproben, fiel diesem aber zum Opfer. Als Minos die Nachricht erhielt, brach er zu einem Rachefeldzug gegen Athen auf; manche sagten nämlich, König Aigeus von Attika habe Androgeos zu dem Tier geschickt.

Er zog zunächst gegen Megara, wo Nisos herrschte, ein Bruder des Aigeus. Nisos verlieh eine purpurne Locke Unsterblichkeit. Seine Tochter Skylla verliebte sich jedoch in den fremden König und schnitt ihrem Vater die Locke ab. Minos siegte, dankte es dem Mädchen aber schlecht: Er ließ sie an seinem Schiff angebunden durch das Meer schleifen.

Gegen das wehrhafte Athen allerdings konnte Minos zunächst nichts ausrichten. Also rief er seinen Vater Zeus um Hilfe an; dieser erhörte ihn und sandte Pest und Hungersnot. Den Athenern erlegte Minos nach ihrer Kapitulation eine grausame Steuer auf: Alle neun Jahre mussten sie sieben Jünglinge und sieben Jungfrauen nach Kreta senden, wo diese dem Minotauros geopfert wurden.

Für den Minotauros hatte er von Daidalos ein Labyrinth erbauen lassen. Die Jünglinge und Jungfrauen wurden hineingeschickt und so dem Ungeheuer zum Fraß vorgeworfen. Erst Theseus, der Sohn des Aigeus, beendete diesen Opferritus, indem er selbst mitfuhr und den Minotauros tötete. Dabei war ihm Minos’ Tochter Ariadne mit ihrem Faden behilflich; nur so konnte er aus dem Labyrinth wieder herausfinden. Als der Held Kreta verließ, nahm er Ariadne mit sich.

Erzürnt sperrte Minos daraufhin den Architekten Daidalos mit dessen Sohn Ikarus in das Labyrinth. Sie konnten entkommen und flohen mit Hilfe selbstgebauter Schwingen von der Insel. Ikaros überlebte den Flug nicht, weil er der Sonne zu nahe kam. Dabei schmolz das Wachs zur Befestigung der Federn und er stürzte ins Meer. Minos aber verfolgte Daidalos bis nach Sizilien, wo dieser bei König Kokalos Schutz gefunden hatte. Kokalos empfing den Kreterkönig mit vorgetäuschter Gastfreundschaft, ließ ihn jedoch im Bade töten. Je nach Version wurde Minos dort von Kokalos’ Töchtern ertränkt oder durch kochendes Wasser ums Leben gebracht. Lange danach soll Theron, Tyrann von Akragas, die Gebeine von König Minos gefunden und nach Kreta zurückgeschickt haben.

Nach seinem Tode herrschte der weise irdische König Minos mit goldenem Zepter als Richter der Toten in der Unterwelt, an der Seite seines Bruders Rhadamanthys und seines Halbbruders Aiakos.

Nach Minos wurde die Kultur von Altkreta durch Arthur Evans als minoisch bekannt, was auf eine Benennung durch Arthur Milchhoefer zurückgeht. Bedeutende Zeugnisse dieser Kultur sind die Palastanlagen von Phaistos und Knossos; Letztere war wegen ihrer verwinkelten Architektur möglicherweise der Ursprung der Legende vom Labyrinth.

Die Verbindung von Minos mit der minoischen Kultur gilt jedoch als nicht gesichert, da ihn Homer nicht nur als Sohn des Zeus, sondern auch als Achäer bezeichnet, die zumeist mit den mykenischen Griechen gleichgesetzt werden. Die retrospektive Vereinnahmung wurde jedoch betrieben, wie die von Zeus und Europa, um die Bedeutung des Zeus zu erhöhen. Zu Blütezeit der kretischen Hochkultur etwa 1600 v. Chr. war der Zeusglaube noch nicht installiert. Dieser wurde vermutlich in den Dunklen Jahrhunderten oder sogar erst 800 v. Chr. durchgesetzt. Die Parische Chronik datiert Minos zwischen 1462 und 1423 v. Chr., was dem spätminoischen Übergang von der Neupalastzeit der Phase SM I B zur dritten oder kretomykenischen Palastzeit in SM II auf Kreta entspräche.




</doc>
<doc id="3453" url="https://de.wikipedia.org/wiki?curid=3453" title="Magnetische Feldkonstante">
Magnetische Feldkonstante

Die magnetische Feldkonstante formula_4, auch Magnetische Konstante, Vakuumpermeabilität, oder Induktionskonstante, ist eine physikalische Konstante, die eine Rolle bei der Beschreibung von Magnetfeldern spielt. Sie gibt das Verhältnis der magnetischen Flussdichte zur magnetischen Feldstärke im Vakuum an. Der Kehrwert der magnetischen Feldkonstanten (mit einem Vorfaktor formula_5) tritt als Proportionalitätskonstante im magnetostatischen Kraftgesetz auf. Im Internationalen Einheitensystem (SI) hat die magnetische Feldkonstante den Wert:
mit den Einheiten Newton (N) und Ampere (A).

Historisch hatte die Konstante formula_4 verschiedene Namen. Bis 1987 sprach man von der „magnetischen Permeabilität des Vakuums“. Jetzt heißt sie in der Physik und in der Elektrotechnik "magnetische Feldkonstante".

Bei formula_8 ist formula_4 die Vakuumpermeabilität und formula_10 die relative Permeabilität.

Aus den Maxwell-Gleichungen ergibt sich im SI ein einfacher Zusammenhang zwischen der magnetischen Feldkonstante, der elektrischen Feldkonstanten formula_11 und der Lichtgeschwindigkeit formula_12:

Der Wert und die Einheit der magnetischen Feldkonstanten (in SI-Einheiten) ergeben sich aus der Definition des Ampere als Einheit der Stromstärke. Im Vakuum gilt für die Kraft auf zwei parallele, stromdurchflossene Leiter:
Hierbei sind formula_16 und formula_17 die Ströme in den Leitern, formula_18 die Länge der Leiter und formula_19 deren Abstand zueinander.

Für formula_20, formula_21 und formula_22 gilt per Definition:

Durch Einsetzen in die Gleichung für die Kraft zwischen den beiden Leitern und Auflösen nach formula_4 erhält man:

Die Einheit von formula_4 kann in verschiedenen SI-Einheiten ausgedrückt werden:

Verwendete Einheiten: H – Henry, m – Meter, V – Volt, s – Sekunde, A – Ampere, T – Tesla, J – Joule, N – Newton, kg – Kilogramm, C – Coulomb.


</doc>
<doc id="3454" url="https://de.wikipedia.org/wiki?curid=3454" title="Manga">
Manga

Manga [] (jap. ) ist der japanische Begriff für Comics. Außerhalb von Japan bezeichnet er meist ausschließlich aus Japan stammende Comics, wird aber auch für nichtjapanische Werke verwendet, die visuell und erzählerisch stark an japanische Vorbilder angelehnt sind. Eine klare Abgrenzung von Manga durch Stilmerkmale ist wegen der großen formalen und inhaltlichen Vielfalt des Mediums in Japan nicht möglich. Zu den wichtigsten vom Manga beeinflussten Comickulturen gehören die koreanischen Manhwa sowie Manhua aus dem chinesischen Raum. Viele als typisch angesehene Stilelemente von Manga finden sich auch im japanischen Animationsfilm, dem Anime, wieder.

In Japan stellen Manga einen bedeutenden Teil der Literatur sowie der Medienlandschaft dar. Der Mangamarkt ist der weltweit größte Comicmarkt. Die Wurzeln des japanischen Comics reichen bis in das Mittelalter zurück. Seine heutige Form ist jedoch wesentlich durch die westlichen Einflüsse im 19. und 20. Jahrhundert geprägt. Seit den 1990er Jahren sind Mangas neben Animes und Computerspielen ein erfolgreiches kulturelles Exportgut Japans.

Ähnlich wie der westliche Begriff „Comic“ ist auch „Manga“ in seiner Bedeutung eher unscharf und stark abhängig von Kontext, Motiven der Verwendung und Adressaten des Begriffs. Im westlichen Sprachgebrauch versteht man unter Manga in erster Linie Comics, das heißt in Bildfolgen erzählte Geschichten oder Vorgänge, japanischen Ursprungs. Im erweiterten Sinne werden auch Werke aus anderen Ländern vom Begriff erfasst, die sich der japanischen Zeichentraditionen zuordnen lassen und typische Stilelemente des Mangas aufweisen. Da diese jedoch breit gefächert sind, einer zeitlichen Veränderung unterliegen und sich mit anderen Comickulturen überschneiden, ist diese Zuordnung stets subjektiv. So bürgerte sich im englischen Sprachraum der Begriff OEL Manga (Original English Language Manga) für japanisch beeinflusste Werke ein. Als allgemeinere Bezeichnung gibt es den „global manga“, zu dem selbst Manhwa aus Korea gezählt werden können. Während dieses – vor allem von Fans vertretene – Verständnis von Manga ausländische Produktionen einschließt, werden experimentelle oder europäisch aussehende japanische Werke oft davon ausgeschlossen, da sie nicht dem erwarteten Stil entsprechen. Des Weiteren wird der Begriff Manga von Verlagen unter Marketinggesichtspunkten immer wieder anders eingesetzt. Beispielsweise etablierte sich unter amerikanischen Verlegern ein Verständnis von Manga als „Comic-Taschenbücher einer gewissen Größe und Preisklasse, die sich vor allem an Mädchen und Frauen richten“. Die genaue Definition und Abgrenzung von Manga ist Gegenstand stetiger Debatten sowohl unter Fans als auch im wissenschaftlichen Diskurs.

Als kennzeichnende Merkmale einer Definition von Manga nach Stilmerkmalen werden eine niedliche bis kindliche Darstellung der Figuren, oft mit großen Augen, eine hochgradig kodifizierte Bildsprache und lange, filmartig erzählte Geschichten sowie dafür eingesetzte Erzählstrategien, Bild- und Seitengestaltungen verwendet. Für die Kombination dieser Merkmale – insbesondere denen des Charakterdesigns – wird auch die Bezeichnung "Japanese Visual Language" (JVL, zu dt. „japanische visuelle Sprache“, nach Neil Cohn) verwendet, ohne dass diese dabei per se als Definition von Manga herangezogen werden. Die Merkmale treffen auch nur auf einen Teil der japanischen Comickultur zu, der jedoch außerhalb Japans besonders präsent ist und daher das Bild prägt. Oft werden sie auch mit der Veröffentlichungsform als Serien kleinformatiger Taschenbücher identifiziert und so beispielsweise gemeinsam mit Comics als serielle Veröffentlichungen zu Bilderbüchern abgegrenzt. Auch speziell in Japan verbreitete Veröffentlichungsformen wie Magazine und dortige Produktionsprozesse werden als Merkmal herangezogen, wobei diese und das Format bereits wieder einen besonderen Japanbezug aufweisen. Ebenso werden viele der stilistischen Merkmale bisweilen auf japanische Kulturmerkmale zurückgeführt, sodass auch eine Definition von Manga nach Stilmerkmalen indirekt auf eine Definition nach japanischem Ursprung zurückgeführt werden kann. In einem Überblick über verschiedene Definitionsversuche stellt Zoltan Kacsuk fest, dass sich alle direkt oder indirekt auf Japan beziehen. Bei einer Definition nach Stil folgt dieser den Veränderungen in Japan, während stilistische Änderungen außerhalb Japans eher als Abrücken vom Manga selbst betrachtet werden. Die Kombination der üblichen Stil-, Genre- und Erzählmerkmale, die Veröffentlichungsform in Magazinen und der damit fast zwangsläufig verbundene japanische Ursprung ergeben ein Verständnis von Manga im engeren beziehungsweise engsten Sinne („Manga Proper“ nach Jaqueline Berndt). Frederik L. Schodt fasst Manga zusammen als „eine japanische Erzählkunst mit langer Tradition, die eine aus dem Westen importierte physische Form angenommen hat“.

In Japan selbst wird der Begriff „Manga“, wie auch „Comic“ ( "komikku") für alle Arten von Comics verwendet, unabhängig von ihrer Herkunft. So allgemein verwendet, wird er oft in Katakana geschrieben. Neben Bildgeschichten werden auch Einzelbilder im üblichen Stil von Comics und Karikaturen als Manga bezeichnet. In Japan ist das Bild vom Manga im Gegensatz zum Westen statt nur von langen Geschichten auch durch Comicstrips geprägt. Jedoch ist das Wort „Manga“ lange Zeit als umgangssprachlich wahrgenommen worden, so dass von Seiten der Verleger „komikku/komikkusu“ häufiger verwendet wird, da das Wort kultivierter klinge. Die japanische Comicforschung begreift Manga vorrangig als in Magazinen erscheinende, serielle grafische Erzählform. In der öffentlichen Wahrnehmung Japans und in der Forschung auch über Japan hinaus bestehen zwei Begrifflichkeiten vom Manga: die japanische Comickultur nach dem Zweiten Weltkrieg oder die Gesamtheit aller in Japan entstandenen Comics und Bildgeschichten seit dem frühen Mittelalter. Während ihrer Entstehung und noch bis Mitte des 20. Jahrhunderts wurden auch japanische Animationsfilme als „Manga“, später „Manga Eiga“ (, „Manga-Filme“) bezeichnet. Ab den 1970er Jahren setzte sich dann der Begriff Anime durch und Manga Eiga wird heute nur noch wenig verwendet, insbesondere aber vom Studio Ghibli.

Für das Wort „Manga“ finden sich verschiedene direkte Übersetzungen. Darunter sind „spontan“, „impulsiv“, „ziellos“, „unwillkürlich“, „bunt gemischt“, „ungezügelt/frei“, „wunderlich/skurril“ und „unmoralisch“ für die erste Silbe, die zweite bedeutet „Bild“. Das Wort bzw. die Kombination der Zeichen , die wie alle japanischen Sinnzeichen aus China stammen, ist seit dem 12. Jahrhundert in Japan in Gebrauch, doch änderte sich die Bedeutung danach häufig und drastisch. So ordnet Hirohito Miyamoto als erste Verwendung die Bezeichnung eines Löffler-Vogels (, hier in der Lesung "mankaku") zu, später eine Bezeichnung für chaotisches Schreiben oder Zeichnen, in der der Vogel als Metapher dient, und schließlich für „Produktion und Sammlung großer Mengen von Zeichnungen unterschiedlicher Motive in verschiedenen Stilen“. Die Einführung der heutigen Bedeutung oder sogar die Erfindung wird oft dem Künstler Katsushika Hokusai zugeschrieben, jedoch verwendete er es weder als erster noch in der heutigen Bedeutung, sondern noch im Sinne von „Sammlung von Zeichnungen“. Wahrscheinlich machte er es jedoch als Bezeichnung für seine Holzschnitte populärer, sodass es Ende des 19. Jahrhunderts in neuem Begriffsinhalt wieder aufgegriffen wurde: „Manga“ bezeichnete nun Karikaturen, einfache humoristische Zeichnungen sowie Zeichenkunst allgemein. Dabei war im Kontext mit politischen Karikaturen die Bedeutung als „ungezügelt/frei“ besonders wichtig. Schließlich wurde das Wort von Kitazawa Rakuten für seine Comicserien aufgegriffen. In der Bezeichnung für diese festigte sich der Begriff in der folgenden Zeit und setzte sich nach dem Zweiten Weltkrieg endgültig durch. In jüngerer Zeit gab es mehrere Versuche, das Wort umzudeuten beziehungsweise umzuschreiben, um seine unmittelbare Bedeutung dem Medium entsprechend neutraler zu halten. Darunter waren eine Schreibweise als MAN画, „MAN“ im Sinne von engl. „Menschheit“ und „erwachsen“, oder eine Änderung des ersten Zeichens in das für „Tausende, Alles“, das ebenfalls „man“ gesprochen wird.

In den westlichen, das heißt zunächst den englischen, Sprachgebrauch eingeführt wurde der Begriff durch Frederik L. Schodt durch sein Buch "Manga! Manga! The World of Japanese Comics" von 1983 als Bezeichnung für japanische Comics. In den 1990er Jahren etablierte sich der Begriff in diesem Sinne – auch in Europa. Seit zunehmend außerhalb Japans Comics entstehen, die sich stark an japanischen Vorbildern orientieren, und das Bild vom Manga durch die über längere Zeit international erfolgreichen Serien geprägt wird, wird der Begriff auch im weiteren Sinne oder bezogen auf Stilmerkmale verwendet.

In der Kulturwissenschaft ist umstritten, zu welcher Zeit der Manga entstand beziehungsweise ab welchem Zeitpunkt man bei japanischen Comics von Manga sprechen kann. Die Ansichten reichen von einer Ursprungssuche in der mittelalterlichen japanischen Kultur mit ihren Karikaturen und Bildrollen über die Satiren, Drucke und Skizzen der Edo-Zeit, deren heute bekanntester Künstler Katsushika Hokusai ist, über die Umbrüche in der japanischen Kultur und die Einflüsse des Westens um 1900, als Kitazawa Rakuten das Medium prägte, bis zu Osamu Tezuka, der nach dem Zweiten Weltkrieg neue Erzählformen und Themen fand. Die Auffassung einer weit in die Vergangenheit zurückreichenden Geschichte des Mangas gerät zunehmend in Kritik. Sie ignoriere Brüche in der japanischen Kulturgeschichte und diene dazu, Manga als Kulturgut gegen Vorurteile gegenüber Massenmedien zu verteidigen, sowie Manga aus politischen Gründen als urjapanisches Kulturgut auszuweisen. Dabei würden die starken westlichen Einflüsse seit dem Ende des 19. Jahrhunderts, die für den heutigen Manga von großer Bedeutung sind, verschwiegen oder durch bewusste Wahl der historischen und modernen Beispiele ausgeblendet. Es bestehen erzählerisch wie stilistisch große Differenzen zwischen heutigem Manga und der japanischen Kunst bis ins 19. Jahrhundert. Als deutlichen Unterschied nennt Neil Cohn die Erzählstrategien und das Vokabular an Symbolen. Manche japanische Tradition wurde auch über japanisch beeinflusste westliche Bewegungen wie dem Jugendstil reimportiert. Dennoch bestehen Gemeinsamkeiten, so nennt Jean-Marie Bouissou beispielhaft den absurden und Fäkalhumor sowie die Verbindung von Menschlichem und Übermenschlichem als inhaltliche Konstanten vom Mittelalter bis heute. Bezüglich der erzählerischen Tradition wird darauf verwiesen, dass das Erzählen in Bildern in Japan in unterschiedlicher Form immer präsent war, während es in der Comicgeschichte anderer Länder starke Brüche und Lücken gibt. Auf diese Weise gab es stets eine hohe Akzeptanz für das Medium, auch ohne dass eine lineare Entwicklung stattgefunden hätte.

Umstritten ist auch die Bedeutung Osamu Tezukas, dessen Person und Leistung seit den 1990er Jahren in der Mangaforschung ein zentrales Thema sind. Auf der einen Seite wird er als Schöpfer eines „neuen und ureigen japanischen“ Mediums Manga angesehen. Andere weisen darauf hin, dass manche von Tezukas vermeintlich neuen Erzähltechniken bereits früher in Japan ausprobiert wurden, jedoch noch keinen großen Erfolg hatten, und dass er nicht in allem die Vorreiterrolle hatte, die ihm zugesprochen wird. Dass Osamu Tezuka mit seinen Werken erheblich zur Popularisierung des Mediums beigetragen hat, ist unstrittig. In der Hervorhebung Tezukas als Schöpfer des Mangas vor allem in den 1990er Jahren lag auch das Interesse, das Medium als besonders modern darzustellen – im Gegensatz zur Behauptung einer langen Manga-Tradition durch diejenigen, die die Ursprünge im Mittelalter oder der Edo-Zeit suchten.

Die ältesten bekannten Vorläufer der japanischen Comic-Kunst sind Zeichnungen und Karikaturen aus dem frühen 8. Jahrhundert, die im Jahr 1935 bei Restaurierungsarbeiten am Hōryū-Tempel in Nara auf der Rückseite von Deckenbalken entdeckt wurden. Buddhistische Mönche begannen schon früh, Bildergeschichten auf Papierrollen zu zeichnen. Diese werden Emakimono genannt. Das bekannteste dieser Werke ist die erste von insgesamt vier "chōjū jinbutsu giga" (, "Tier-Person-Karikaturen"), die dem Mönch Toba Sōjō (1053–1140) zugeschrieben werden: Dabei handelt es sich um eine Satire, in der sich Tiere wie Menschen verhalten und auch buddhistische Riten karikiert werden. Diese wurden so erfolgreich, dass derartige Werke bald allgemein als „Toba-e“ (Toba-Bilder) bezeichnet wurden. Im 13. Jahrhundert begann man, Tempelwände mit Zeichnungen von Tieren und vom Leben nach dem Tod zu bemalen. In der Wende vom 15. zum 16. Jahrhundert kamen Holzschnitte auf, die in Hefte gebunden und vor allem an die wohlhabende Mittelschicht verkauft wurden. Auch Zeichnungen aus dem Alltagsleben bis hin zu erotischen Bildern kamen hinzu. Seit der Zeit der ersten noch religiösen Zeichnungen war die Kunst oft von einem absurden Humor bis hin zum Fäkalhumor geprägt. Die ersten auch breitere Schichten erreichenden Werke waren die Ōtsu-e Mitte des 17. Jahrhunderts.
Ab dem späten 17. Jahrhundert folgten ihnen Ukiyo-e genannte Holzschnittbilder, die das unbeschwerte Leben, Landschaften und Schauspieler bis hin zu sexuellen Ausschweifungen zum Inhalt hatten und rasch massenhafte Verbreitung in der Mittelschicht der Edo-Zeit fanden. Einer der Künstler dieser Holzschnitte war Katsushika Hokusai (1760–1849), der dafür den bereits zuvor anderweitig gebräuchlichen Begriff „Manga“ aufgriff. Auch andere Künstler, so Aikawa Minwa noch etwas früher als Hokusai, verwendeten das Wort für ihre Werke. Die "Hokusai-Manga" sind Skizzen, die in insgesamt 15 Bänden veröffentlicht wurden und keine zusammenhängende Geschichte erzählen, sondern Momentaufnahmen der japanischen Gesellschaft und Kultur zeigen. In der Rückschau fiel Hokusai durch die Perspektive des Japonismus eine übergroße Rolle als Ukiyo-e-Künstler zu, sodass er zeitweise auch als Erfinder des Wortes Manga galt.

Ein weiterer Teil der Edo-Kultur waren die Kusazōshi (Allerlei-Bücher), die in einer Mischung aus Bildern und Texten Geschichten erzählen und je nach Farbe ihres Umschlags als Akahon (rot), Aohon (grün) und Kurohon (schwarz) bezeichnet wurden, sowie die in großen Auflagen verbreiteten Kibyōshi (Schriften mit gelbem Umschlag). Sie thematisierten das zeitgenössische Leben und entwickelten sich zu einem beliebten Massenmedium. Bild und Text waren in ihnen eng verzahnt und es traten bereits einige Formen von Sprechblasen und ähnlichen Integrationen von Text in Bildern auf. Daneben gab es weiterhin sogenannte Toba-e, die nun vor allem Karikaturen und Satiren des täglichen Lebens zeigten.

Nach Ende der Abschließung Japans und der damit einhergehenden zunehmenden Öffnung des Landes gewann in der zweiten Hälfte des 19. Jahrhunderts das westliche Verlagswesen an Einfluss. Neben neuen, verbesserten Drucktechniken ließ man sich vom Stil europäischer Karikaturen inspirieren, zu dessen Verbreitung in Japan die Satiremagazine "The Japan Punch" (1862–1887, gegründet von Charles Wirgman) und "Tôbaé" (ab 1887, gegründet von Georges Bigot, benannt nach dem Mönch Sōjō Toba) beitrugen, sowie von den in den USA neu entstehenden "comic strips" in den Zeitungen. Es folgten weitere Karikaturenmagazine nach ihrem Vorbild und einige der entstehenden japanischen Zeitungen hatten Comic-Beilagen. Diese hatten satirischen Charakter und richteten sich ausschließlich an Erwachsene, einige der Publikationen und Künstler waren ständig von Zensur und Verhaftungen bedroht. Um 1900 entstanden die ersten Magazine für Kinder, bereits nach Geschlechtern getrennt: Das "Shōnen Sekai" für Jungen 1895 und 1902 "Shōjo kai" für Mädchen. Diese enthielten zunächst aber nur wenige Comics oder Karikaturen.

Als erster Vorläufer von Manga in heutigem Sinne gilt die 1902 von Rakuten Kitazawa (1876–1955) gezeichnete Geschichte "Tagosaku to Mokubē no Tōkyō Kembutsu" („Tagosakus und Mokubes Besichtigung von Tokio“). Kitazawa griff „Manga“ wieder auf, verwendete es in Abgrenzung zu den älteren Kunstformen und setzte es schließlich als Bezeichnung für die Erzählungen in Bildern durch. Er gründete 1905 das Satiremagazin "Tōkyō Puck" – benannt nach dem britischen (später amerikanischen) Satiremagazin "Puck" – und 1932 die erste japanische Schule für Karikaturisten. 1935 startete mit "Manga no Kuni" das erste Fachmagazin für Zeichner. Rakutens Publikationen für Erwachsene folgten bald erste Magazine mit Mangas für Kinder und Jugendliche, so 1907 das "Shōnen Puck" und für Mädchen 1908 das "Shōjo no Tomo" sowie viele weitere. In den 1920er Jahren erscheinen vermehrt amerikanische Comic-Strips in Japan, die Vorbild für japanische Künstler werden. In der Folge findet auch die Sprechblase deutlich stärker Verwendung als früher. Der in dieser Zeit entstandene Yonkoma-Manga, Comicstrips aus vier Bildern, ist noch heute in Japan verbreitet. Mit "ero-guro-nansensu" ("ero"tisch, "gro"tesk, "nonsens") entstand ein den Experimenten in anderen Kunstrichtungen und dem Lebensgefühlt der 1920er Jahre entsprechendes Genre für Erwachsene.

In den 1930er Jahren und insbesondere während des Zweiten Weltkriegs wurde auch der Manga von der Regierung für Propaganda genutzt. Zeichner waren gezwungen, harmlose Alltagsgeschichten, Durchhalte- und Heldengeschichten oder direkte Propaganda zum Einsatz in der Armee oder beim Gegner zu produzieren. Auch Anleitungen oder Erläuterungen in Form von Mangas erschienen häufiger. Der Druck führte dazu, dass sich einige Zeichner auf Geschichten für Kinder verlegten und dabei im Gegensatz zu den bis acht Panel umfassenden Strips der Zeit davor erstmals Erzählungen in vielen Kapiteln und mit über 100 Seiten Gesamtumfang entstanden. Diese erschienen in stärker den Comics gewidmeten Magazinen wie dem "Yōnen Club", das 1931 seine höchste Auflage von fast 1 Million erreichte. Die japanische Regierung initiierte 1940 die staatliche Dachorganisation "Shin Nippon Mangaka Kyōkai" („Neue Vereinigung der Manga-Zeichner Japans“). Doch eine vollständige Kontrolle des bis Ende des Krieges weitverbreiteten Mediums gelang nicht und der Umgang mit populären, aber dem Feind zugeordneten Figuren wie Micky Maus blieb ambivalent: mal waren sie in den Geschichten Symbole des Feinds, mal willkommene Gäste. Die erfolgreichsten Serien in der zunehmend militaristischen Gesellschaft waren "Norakuro" über die Militärkarriere eines Soldaten in Hundegestalt und "Tank Tankuro", Geschichten über einen Panzer mit beliebigen Waffen, sonstiger Ausstattung und Verwandlungsmöglichkeiten. Beide Serien wurden von der erfolgreichen Vermarktung von Spielzeugartikeln und ähnlichem begleitet. In den letzten Jahren des Kriegs wurden die Publikationen auf Grund der Rohstoffknappheit immer stärker eingeschränkt und Mangas als Zeitverschwendung diskreditiert.

Nach der japanischen Kapitulation war während der Besatzungszeit das japanische Verlagswesen zunächst strengen Vorgaben der Vereinigten Staaten unterworfen, die das Ziel hatte, dem Militarismus in Japan ein Ende zu bereiten – jedoch waren deren Vorgaben lockerer als die der japanischen Regierung während des Krieges. Zugleich war leichte und erschwingliche Unterhaltung, die Flucht aus dem tristen Alltag bot, sehr gefragt. Es gründeten sich zahlreiche neue Kleinverlage, Mangas erschienen wieder in den Zeitungen und in eigenen Magazinen, aber viele auch in Form von sehr billig produzierten Akahon Manga („Rotbuchmanga“ nach dem roten Einband). Der einflussreichste Wegbereiter des modernen Manga war Osamu Tezuka. Beeinflusst vom Stil der frühen Disney-Zeichentrickfilme und von expressionistischen deutschen und französischen Filmen schuf er nach seinen ersten Zeitungsstrips 1947 "Shintakarajima". Die Geschichte basierend auf "Die Schatzinsel" war erstmals eine lange Erzählung mit filmischer Inszenierung, die den Story-Manga begründete. Diese Strömung wurde für den modernen Manga prägend und macht den Großteil der international erfolgreichen Werke aus. Osamu Tezuka prägte durch weitere erfolgreiche Serien und seinen Einfluss auf andere Künstler die Entwicklung des Mediums. Im gleichen Jahrzehnt erschienen von ihm unter anderem "Astro Boy" und "Kimba, der weiße Löwe", die das Segment für Jungen beeinflussten, und mit "Ribbon no Kishi" die erste Abenteuerserie für Mädchen. Ab den 1960er Jahren war er mit den Verfilmungen seiner Serien, an deren Produktion er maßgeblich beteiligt war, auch für den Anime prägend. Später widmete sich Tezuka mit ernsthafteren, sozialkritischen Serien einem älteren Publikum. Bis zu seinem Tod 1989 schuf er über 700 Geschichten in unterschiedlichsten Genres. Etwa 70 Prozent der Manga-Publikationen seiner Zeit orientieren sich an den von Tezuka popularisierten Darstellungskriterien, so eine Schätzung von Megumi Maderdonner.

Das von Tezuka und anderen jungen Nachkriegskünstlern umgestaltete Medium Manga erreichte in den 1950er Jahren große Popularität, begünstigt durch den wirtschaftlichen Aufschwung. Vorherrschend waren Alltagsthemen sowie Science-Fiction und klassische Abenteuer-Geschichten, daneben das aufkommende Angebot an romantischen bis abenteuerlichen Geschichten für Mädchen. Parallel entwickelte sich ein florierender Leihbüchereimarkt, da der Kauf von Büchern für viele noch zu teuer war, und in diesem ein Segment von gewalt- und sexhaltigen, erzählerisch experimentelleren Comics. Auch das japanische Papiertheater Kamishibai, das in den 1950er Jahren mit Aufkommen des Fernsehens ausstarb, hatte Einfluss auf diese Strömung. Die Bewegung grenzte sich vom Manga der an Kinder gerichteten Magazine ab und formierte sich 1959 als Gekiga. Dessen Höhepunkt lag in den 1960er Jahren, als im Kontext der Studentenproteste sozialkritische, gewalthaltige aber auch in der Alltagsumgebung angesiedelte Geschichten für Erwachsene ein größeres Publikum fanden. Diese Themen kamen auch stärker in die großen Magazine. Sport-Manga, teils auch mit sozialkritischer Handlung, wurden beliebter und 1968 kam mit "Harenchi Gakuen" der erste erotische Manga heraus – im "Shōnen Jump", einem großen Magazin für männliche Jugendliche. Mit dem Niedergang des Leihbüchereimarkts Anfang der 1960er Jahre gingen die dort verbliebenen Künstler zu den großen Verlagen oder gründeten neue, an ein erwachsenes Publikum gerichtete Magazine. Aus dem Gekiga und den an erwachsenes männliches Publikum gerichteten Magazinserien ging der Seinen-Manga hervor, geprägt von Kriminal- oder Abenteuergeschichten und kantigeren, realistischeren Zeichenstilen. Zugleich fand Anfang der 1970er Jahre ein derberer Humor sowie allgemein mehr Sex- und Gewaltdarstellungen Verbreitung.

Nachdem in der Nachkriegszeit eine weibliche Leserschaft mit Geschichten fast ausschließlich männlicher Künstler aufgewachsen war, drängten in den 1970er Jahren zunehmend Frauen in den Manga. Die Gruppe der 24er und andere griffen Elemente aus bereits erfolgreichen Geschichten wie denen Tezukas sowie aus Kabuki und Takarazuka-Theater auf, schufen neue Erzähl- und Gestaltungsmittel und widmeten sich in ihren Geschichten Abenteuer, Liebe und Sexualität. Geschichten spielten oft an exotischen Schauplätzen mit authentischer oder aktueller japanischer Mode. Auch Homosexualität wird erstmals behandelt und das Genre Shōnen-ai begründet, in dem Liebe zwischen Männern im Mittelpunkt steht. Etwas später entstand mit Josei auch eine Gattung für erwachsene Frauen. Die siedelte nun Serien für Mädchen und Frauen stärker in der japanischen Gesellschaft an, die Geschichten wurden kritischer und nahmen sich größere Freiheiten.
Zur gleichen Zeit entwickelten sich erste Magazine für erotische Manga. In den an Jugendliche gerichteten Magazinen wurde in den 1970er Jahren Science Fiction wieder populärer, darunter vor allem Mecha-Serien mit riesigen Robotern, die von in ihnen sitzenden Menschen gesteuert werden, und Space Operas. Bedeutende Serien dieser Entwicklung waren "Mazinger Z" von Go Nagai und die Werke von Leiji Matsumoto.
Zugleich begann sich eine organisierte Fanszene mit eigenen Veranstaltungen und Publikationen zu entwickeln, die wiederum auf die professionelle Szene wirkte.
In den 1980er Jahren wurden Science-Fiction-Serien tiefgründiger, nahmen mehr Bezug auf die Welt ihrer Leser und stellten sich häufiger philosophischen Fragen. Unter diesen waren auch "Akira" und "Ghost in the Shell", die zusammen mit ihren Anime-Verfilmungen als erste Mangas einen Erfolg in der westlichen Welt erreichen konnten. Der erfolgreichste Manga des Jahrzehnts war jedoch "Dragon Ball", der etwas später, aber umso erfolgreicher nach Europa und Nordamerika kam. Auf die Popularität sowohl von unterhaltsamen als auch anspruchsvollen Mangas in diesen Ländern folgte in den 1990ern schnell auch die Entstehung einer Fanszene dort.
Nachdem das Medium populärer wurde und in den 1980er Jahren zunehmen sexualisierte und gewalthaltige Inhalte auch in Publikationen für Jugendliche und Kinder erschienen, wuchs die gesellschaftliche Kritik in Japan. Auf den Fall des sogenannten Otaku-Mörders Tsutomu Miyazaki, der 1989 vier Mädchen tötete und Anime- und Manga-Fan war, folgte eine gesellschaftliche Debatte um einen Zusammenhang zwischen Medien und Gewalt. Sie führte zunächst zur zeitweisen Stigmatisierung von Mangalesern als generell gemeingefährlich sowie 1991 zu Verhaftungen von Redakteuren, Verlegern und Zeichnern. In der Folge entwickelte sich eine Selbstzensur der großen Verlage und eine stärkere inhaltliche Differenzierung nach Altersgruppen sowie entsprechende Markierungen von Publikationen für Erwachsene, aber auch eine Bewegung von Künstlern gegen die Einschränkung ihrer Freiheit.

Bis zu den 1990ern wurden in Japan mit Serien zu Hobbys wie Pachinko und für Otaku oder mit Boys Love auch Nischenmärkte besetzt. Aus der in den vorherigen beiden Jahrzehnten entwickelten Genrevielfalt wurde nun eine zunehmende Vielfalt an Titeln nach einem Anfangserfolg in Japan auch weltweit herausgebracht. Auch alternative beziehungsweise Underground-Manga kamen im westlichen Ausland heraus. Außerhalb Japans inspirierten die Veröffentlichungen lokale Zeichner zur Nachahmung der Stilmittel und Themen, auch durch Mischung mit traditionell amerikanischen Genres und Figuren wie Superhelden. So brachten die großen Verlage DC und Marvel zeitweise Manga-Versionen einiger ihrer Helden heraus. 2001 wurde in Frankreich die Bewegung Nouvelle Manga proklamiert, die frankobelgische und japanische Comictraditionen verbinden will. In Japan selbst erschien ab 1997 mit dem Piraten-Abenteuer "One Piece" die kommerziell erfolgreichste Serie, die auch international populär wurde – ähnlich wie weitere an Jugendliche gerichtete Abenteuer- und Actionserien, darunter "Naruto" und "Bleach". Zugleich gehen seit 1995 die Verkaufszahlen der Manga-Magazine zurück. Die Tradition der japanischen Comicstrips erlebte in den 1990ern durch sogenannte Moe-Yonkoma, die sich auf humoristische Weise den Alltag schöner junger Mädchen widmen, eine neue Richtung. Nach 2000 kamen Webmanga auf, ebenfalls als Comicstrips, jedoch nicht unbedingt im klassischen Vier-Bild-Schema. Zugleich kamen sowohl illegale wie legale digitale Verbreitungswege für Mangas auf, insbesondere wurden Plattformen zum Lesen auf Handys entwickelt. Der Konsum von Serien über unlizenzierte Scanlations wird als wirtschaftliche Bedrohung wahrgenommen.

Im Laufe der 1990er Jahre erhielt der Manga als allgemeines Medium und Kunstform immer mehr Anerkennung. Im Herbst 2000 erkannte die japanische Regierung Manga und Anime offiziell als eigenständige, förderungswürdige Kunstform an und das Medium wurde zum Pflichtstoff im Kunstunterricht, wobei man auf eine Darstellung des Mangas als traditionelle japanische Kunst Wert legte. Um die Popularität japanischer Kultur international zu stärken und auch wirtschaftlich und politisch zu nutzen, wurden besonders ab 2008 Fördermaßnahmen in Form von Stipendien und Auszeichnungen geschaffen. Für diesen Ansatz prägte sich der Begriff Cool Japan. Jedoch gab es sowohl 2002 als auch ab 2010 auch staatliche Bemühungen, sexuelle Darstellungen von unter 19-Jährigen zu beschränken und die Jugendschutzgesetze wieder strenger auszulegen, was zu großen Protesten in Teilen der Zeichner- und Fanszene führte. Auch Debatten um die Wirkung von Gewaltdarstellungen auf Jugendliche und Kinder kamen nach Gewalttaten immer wieder auf. 2015 kam es zu Kritik an sexuellen Darstellungen vor allem Minderjähriger in einigen Mangas, nun vor allem von außen mit der Motivation, international gegen Kinderpornografie vorzugehen. Im gleichen Jahr gab es eine Initiative japanischer Abgeordneter, das Medium als wichtiges kulturelles Exportgut mit einem Nationalen Manga-Zentrum noch stärker zu fördern.

Mangas sind meist in Schwarz-Weiß gehalten und werden entsprechend der traditionellen japanischen Leserichtung von „hinten“ nach „vorne“ und von rechts nach links gelesen. Die außerhalb Japans bekannteste Form von Manga sind die Story-Manga, die eine lange, oft detailreiche Geschichte erzählen und viele Tausend Seiten umfassen können. Daneben gibt es Comic-Strips, sogenannte Yonkoma (Vier-Bilder-Comic). Darüber hinaus gibt es die Bezeichnung Koma-Manga für Comicstrips, die sich nicht an die klassische, in vier Bildern abgeschlossene Form halten.

Für den Story-Manga – und damit für das Bild von Manga außerhalb Japans allgemein – prägend ist eine filmische, das heißt inhaltlich ausgebreitete Erzählweise. Bewegungen, Handlungen und Szenerie werden in vielen Details gezeigt, auf viele Bilder kommt nur wenig Text. Der Erzählrhythmus ist auf eine Übereinstimmung von Erzählzeit und erzählter Zeit ausgerichtet, sodass das Gefühl „dabei zu sein“ gefördert wird. Zur Rhythmisierung wird sowohl die Komposition der Einzelbilder, so die gewählten Blickwinkel, als auch die Gesamtkomposition der Seiten eingesetzt. Ein im Manga im Gegensatz zu anderen Comic-Kulturen oft auftretendes Erzählmittel ist die Darstellung von Bewegungen in kleinen Schritten. Zusammen mit dem starken Gebrauch von Lautmalereien, die in die Bildkompositionen integriert werden, und Symbolen ermöglicht dies einen schnellen Lesefluss. Durch die ausgebreitete Erzählweise werden die Geschichten oft hunderte oder tausende Seiten lang und bieten Raum für inhaltliche Tiefe und differenzierte Charakterzeichnung. Szenenbilder werden oft in mehreren Bildern einzelner Details stückweise zusammengesetzt und es finden sich vergleichsweise viele Bilder ohne unmittelbare Handlung oder handelnde Figur. Auch die Gesamtgestaltung einer Seite, das heißt die Zusammenwirkung der Panel zu einem Metapanel sind von großer Bedeutung. Entsprechend werden im Vergleich zu amerikanischen Comics häufiger Nahaufnahmen und einzelne Personen statt Personengruppen in einem Panel gezeigt. Der Vergleich zeigt auch, dass japanische Comics weniger Text beinhalten und sich stärker auf die visuelle Umsetzung des Erzählten konzentrieren.
Als Form des Comics stehen dem Manga zunächst alle Stilmittel des Comics zur Verfügung. Dazu zählen die Vereinfachung und Übertreibung von Figuren, Bewegungen und Posen, der Einsatz von Symbolen, Lautmalerei und Typografie. Einige Elemente sind für den Manga von besonders großer Bedeutung, wie etwa der Einsatz von Posen, was auch durch den Einfluss des Kabuki-Theaters erklärt werden kann. Bewegungslinien und Überblendungen werden nicht nur für sich bewegende Objekte verwendet, sondern auch in Form von „subjektiver Bewegung“ für die statische Umgebung, gesehen aus der Perspektive der Bewegung. Die Vermittlung von Emotionen geschieht in erster Linie über die Augen, die dazu auch symbolhaft verzerrt werden. Seit dem Shōjo-Manga der 1970er Jahre werden besonders große Augen und darüber hinaus abstrakte Hintergründe verwendet, um Emotionen und Stimmungen zu vermitteln. Erstmals verwendete Osamu Tezuka größere, vor allem in den Pupillen detaillierter ausgearbeitete Augen, um Charakter und Gefühle der Figuren darzustellen. Eine Steigerung der Stilisierung ist Super Deformed, wobei eine Figur spontan in eine kindliche Form schlüpft, die einen Emotionsausbruch oder eine komische Situation überzeichnet zeigt, um dann wieder in ihr normales, realistischeres Erscheinungsbild zu wechseln. Das Charakterdesign entspricht bisweilen, aber nicht zwangläufig dem japanischen Niedlichkeitskonzept Kawaii, weitere ästhetische Konzepte zur Darstellung von Schönheit und Attraktivität sind Bishōjo (weiblich) und Bishōnen (männlich). Neil Cohn nennt als prägende Merkmale „große Augen, große Frisuren, kleine Münder und ein spitzes Kinn sowie kleine Nasen“. Zur Verbreitung und Bekanntheit gerade dieser Merkmale trugen ab den 1990er auch viele Manga-Zeichenanleitungen bei, die diesen Stil stets reproduzierten.

Den Ursprung der übergroßen Augen vermutet Frederik Schodt in einem während der Öffnung Japans nach Westen um 1900 entstandenen Schönheitsideal, europäisch auszusehen. Dieses habe sich verselbstständigt, zugleich sei für die Vermittlung von Emotionen besonders geeignet. Ähnlich verhält es sich mit oft anzutreffendem, scheinbar „weißem“ Äußeren auch japanischer Figuren, insbesondere den hellen Haaren und Augen. Eine weitere Herkunft dieser Stilmittel ist die in der Regel fehlende Farbe und die vereinfachten Gesichtsformen, sodass unterschiedliche Haar- und Augenfarben zur Unterscheidung der Charaktere eingesetzt werden.

Durch die eigene Geschichte des Mangas konnte sich ein eigenständiges, umfangreiches Vokabular an Symbolen entwickeln, die für nicht-japanische Leser zunächst schwer verständlich sein können. Ähnlich verhält es sich mit dem Einsatz von Typografie und Lautmalereien, wobei diese meist allgemeinverständlicher sind. Die Summe dieses Vokabulars zusammen mit der Erzählweise des Story-Mangas als Grammatik nennt Neil Cohn „Japanese Visual Language“ (JVL), „japanische visuelle Sprache“. Ihre als typisch wahrgenommenen Erscheinungsformen seien wie ein „Standard-Dialekt“, neben den sich viele abweichende und von ihm abstammende „Dialekte“ gesellen – in Japan wie international – und der sich mit der Zeit verändere. Die etablierten Stilmittel und verwendeten Symbole sowie die Möglichkeit der ausführlichen Darstellung der Inhalte über mehrere Seiten anstatt dicht gepackter Panel ermöglichen eine sehr schnelle Rezeption der Seiten – wie Frederik Schodt schon 1983 feststellte. Laut der Redaktion des "Shōnen Magazine" wende der Leser nur durchschnittlich 3,75 Sekunden pro Seite auf. Der hohe Symbolgehalt der Seiten ermögliche aber dennoch die Vermittlung vergleichsweise vieler Informationen und bedarf zugleich einer hohen Lesekompetenz.

Das Medium Manga ist in Japan inhaltlich stark differenziert und deckt jede Altersgruppe und jedes literarische Genre ab. Mit der Zeit haben sich für Manga in Japan mehrere Untergruppen herausgebildet, die je eine demografische Zielgruppe ansprechen. Dies rührt insbesondere aus den Magazinen, die sich je einer Zielgruppe verschreiben. Diese sind:
Teils wird auch von "Kazoku Manga" für Kinder und Familien und "Silver Manga" für das ältere Publikum gesprochen. All diese an Zielgruppen orientierten Gattungen sind jedoch nicht mit der tatsächlichen Leserschaft gleichzusetzen. Diese ist für jede der Gattungen oft vielfältig. Die Gattungen geben stattdessen einen Hinweis auf den Inhalt: So sind Shōnen-Manga eher actionhaltig, während bei Shōjo-Serien Romantik im Mittelpunkt steht. Ursprünge für die Teilung der Gattungen nach Geschlechtern liegen in der in Japan seit dem Mittelalter starken Trennung der Lebensbereiche von Männern und Frauen, besonders im kulturellen Bereich. Daneben besteht die klassische Einteilung nach Genres wie Thriller, Science-Fiction und Romantik, die die inhaltliche Vielfalt des Mediums abbilden. Es haben sich auch Genres herausgebildet, die für das Medium bzw. für Japan spezifisch sind und die es so in anderen Comickulturen nicht gibt. Dazu gehören Geschichten über Spiele, Hobbys oder andere Freizeitbeschäftigungen, zum Beispiel Serien, die Jugendlichen traditionelle japanische Kultur wie Kalligrafie und Teezeremonie nahebringen, Serien des Sport-Genres, Pachinko- und Mah-Jongg-Mangas. Rekishi-Manga und speziell Jidai-geki beschäftigen sich mit japanischer Geschichte. Einige Genres beschäftigen sich mit Aspekten des Alltags, so geht es in Ikuji-Manga um die Erziehung von Kindern und im Gourmet-Genre um Essen und Kochen. Salaryman-Manga beschäftigen sich in Form von Komödien und Dramen mit dem Berufsalltag des Durchschnittsjapaners, der auch die Zielgruppe des Genres ist, und eine Gruppe anderer Serien verschreibt sich in ähnlicher Weise handwerklichen, traditionellen oder ungewöhnlichen Berufen. Oft steht dabei, wie bei Geschichten über Hobbys, die Karriere eines Anfängers und der Wettbewerb mit Kollegen und Konkurrenten im Zentrum. In den fantastischen Genres haben sich spezifische Untergenres wie Magical Girl über Mädchen(gruppen), die sich in Kämpferinnen gegen das Böse verwandeln können, oder Mecha als Untergenre von Science-Fiction mit Fokus auf riesige Kampfroboter herausgebildet. Fantasy-Serien sind oft beeinflusst durch die japanische Shintō-Religion mit ihren zahllosen Göttern und Dämonen, aber auch durch die Mythologien und Sagenwelten anderer asiatischer Länder und Europas. Auch Elemente, wie man sie in Abenteuer- und Rollenspielen findet, sind verbreitet.

Das Feld erotischer Geschichten unterteilt sich in pornografische Hentai und eher erotische Etchi-Manga, wobei die Verwendung der Begriffe in Japan und dem Ausland unterschiedlich ist. Erotische Geschichten und der relativ freizügige Umgang mit Sexualität in der Populärkultur haben in Japan eine lange Tradition, so gab es in der Edo-Zeit viele solche Ukiyo-e, Shunga genannt. Jedoch gab es auch Zeiten, in denen solche Themen im Manga nicht vorkamen, was sich erst ab den 1950er Jahren änderte, sodass keine direkte Verbindung von Shunga zu modernen erotischen Manga gezogen werden kann. Spezielle Untergenres sind Yaoi und Yuri mit homoerotischen Geschichten mit Männern bzw. Frauen. Yaoi bildet auch zusammen mit dem romantischen Shōnen Ai den Boys-Love-Manga, wobei letzterer der in Japan gebräuchliche Begriff ist, während sich die beiden anderen im westlichen Markt erhalten haben. Zielgruppe der homoerotischen Geschichten sind in der Regel nicht homosexuelle Leser, sondern das jeweils andere Geschlecht. In Hentai als auch in Etchi-Manga sind, wie in der japanischen Erotik allgemein üblich, Sexszenen oft in eine humoristische oder parodistische Erzählung eingebettet. Sexuelle Gewalt und Fetische werden vergleichsweise häufig thematisiert. Erotische und pornografische Geschichten sind in Japan stark geprägt von der unter der amerikanischen Besatzung entstandenen Gesetzgebung, die die Darstellung des erwachsenen Genitalbereichs und andere „anstößige“ Inhalte unter Strafe stellte (§ 175 des jap. Strafgesetzbuchs). Dies wurde von vielen Künstlern umgangen, indem die Figuren und ihre Genitalien kindlich gezeigt wurden. Zusammen mit einem Ideal von erstrebenswerter Jugend, Naivität und Unschuld (Kawaii) beförderte das die Entstehung vieler erotischer und pornografischer Geschichten mit kindlichen Figuren und die Etablierung der Genre Lolicon und Shotacon. Auch wenn die Auslegung der Gesetze gelockert wurde, blieb diese Strömung erhalten. Andere Wege, die Zensurgesetzgebung zu umgehen, sind die Verwendung von Balken oder Verpixelung wie im Film, Auslassungen oder von Symbolbildern mit Früchten, Tieren und anderem.

Außer den in großen Magazinen veröffentlichten Serien gab es seit dem Zweiten Weltkrieg auch alternative Manga, in denen Stile, Erzählmittel und Themen erprobt werden, die kein großes Publikum finden. So begründete Yoshihiro Tatsumi Ende der 1950er-Jahre den Gekiga, der sich an eine erwachsene Leserschaft richtete. Dieser ging später jedoch in dem größeren Genre Seinen auf. Ab den 1960er Jahren waren die Magazine "Garo" und "COM" Plattformen unabhängiger Künstler, allerdings konnte sich nur "Garo" längere Zeit halten – bis 2002. Auch darüber hinaus gibt es Werke ähnlich des Underground Comix im Westen. Jedoch verwischen die Grenzen zwischen „Underground“ und „Mainstream“ in Japan stärker, weil der Markt sehr groß ist und auch ungewöhnlichen Werken und Künstlern kommerziellen Erfolg ermöglichen kann.

Neben den rein fiktiven Geschichten gibt es Mangas mit Sachgeschichten sowie fiktive Erzählungen mit Bildungs- und Aufklärungsinhalten, beispielsweise in Form von eingeschobenen Erläuterungen. Im japanischen Alltag finden sich sogar Mangas als Gebrauchsanweisungen oder als Hinweise im öffentlichen Raum. Mangas greifen auch immer wieder aktuelle gesellschaftliche und politische Themen und Ereignisse auf. So reagierten viele Künstler und Verlage auf das Erdbeben von Kōbe 1995 und das Tōhoku-Erdbeben 2011, den folgenden Tsunami und das Reaktorunglück. Alte Serien zu Erdbeben und Nuklearkatastrophen wurden wieder aufgelegt und neue geschaffen, die sich mit dem plötzlich veränderten Alltag und den Gefahren des Unglücks auseinander setzten. Die Beschäftigung mit Politik und insbesondere mit dem Militär – zu dem sich ein eigenes Genre etablierte – ist jedoch ambivalent. Während um 1970 mehrere Serien auch Auswirkungen auf den politischen Diskurs hatten, ist konkrete politische Kritik in Mangas jenseits allgemeiner pazifistischer oder ökologischer Botschaften heute selten. Manche Serien folgen aber dem Werdegang von Politikern, wie dies ähnlich bei der Porträtierung anderer Berufe geschieht. Und während manche Mangas kritisch mit Krieg und Militär umgehen, gibt es auch Magazine, deren Serien sich technischen oder strategischen Perspektiven widmen und ein Publikum von Technik- und Militär-Fans bedienen.

Autoren von Manga werden Mangaka genannt. Der Begriff wurde 1909 von Kitazawa Rakuten geprägt. Schätzungen zufolge gibt es in Japan etwa 2.500 Mangaka. Von diesen können jedoch nur etwa 20 % als professionelle Zeichner von ihrer Tätigkeit leben. Darüber hinaus gibt es eine große Zahl an Amateurzeichnern, die außerhalb der Verlage veröffentlichen. Während bis in die 1960er Jahre fast ausschließlich Männer als Manga-Zeichner tätig waren, sind in dem Beruf heute auch viele Frauen tätig und ähnlich erfolgreich wie ihre männlichen Kollegen. Dabei schaffen häufiger Frauen Mangas für weibliches Publikum und Männer für männliches Publikum. Redaktionen sind in allen Sparten vor allem von Männern besetzt. Die Einstiegshürden für einen Künstler sind verglichen mit anderen Medien gering, da er allein, ohne formelle Ausbildung und mit wenig materiellem Aufwand tätig werden kann. Der Weg in den Beruf führt häufig über Wettbewerbe und Einreichungen bei den Magazinen oder Künstler werden durch Dōjinshi – Fanpublikationen im Selbstverlag – bekannt genug, dass ein Verlag auf sie aufmerksam wird. Auch die Arbeit als Assistent bei etablierten Mangaka vermittelt Erfahrung und Übung, um danach eigene Serien zu schaffen. Nicht wenige der Assistenten bleiben bei dieser Tätigkeit jedoch ein Leben lang. Die Bezahlung wird üblicherweise an die Seitenzahl gebunden. In den 1980er Jahren erhielt ein Zeichner 15 bis 250 US-Dollar pro Seite, bei etwa gleicher Bezahlung von Männern und Frauen. In dieser Zeit gehörten auch erstmals einige Zeichner mit über 1 Million Dollar Jahreseinkommen zu den bestverdienenden Japanern: Shinji Mizushima, Fujio Fujiko und Akira Toriyama. Weitere Einnahmen entstehen den Künstlern aus dem Rechteverkauf, da sie im Gegensatz zu US-Comickünstlern die Rechte üblicherweise behalten und nicht an Verlage verkaufen, sowie durch Auftragsarbeiten und bei besonders bekannten auch durch Werbeauftritte. Der Verbleib der Rechte bei den Künstlern führt auch dazu, dass nur sehr selten eine Serie von anderen Künstlern fortgeführt wird und dass die meisten Autoren im Laufe ihrer Karriere viele Serien und Figuren schaffen.

Die Arbeit der meisten Zeichner findet unter großem Zeit- und Erfolgsdruck statt, da die Magazine in festem, teils wöchentlichem Rhythmus erscheinen und für jede Ausgabe ein neues Kapitel fertiggestellt sein muss. Da über die Magazine schnell und viele Rückmeldungen der Leser den Verlag erreichen, kann dieser auch zeitnah über die Absetzung einer Serie entscheiden. So ist es üblich, dass 10 Wochen nach Start der Serie über die Fortführung entschieden wird. Die Künstler nehmen teils große persönliche und gesundheitliche Einschränkungen in Kauf. Längere Unterbrechungen der Arbeit sind kaum möglich. Erfolgreiche Künstler arbeiten oft an mehreren Serien, die Wochenenden durch und schlafen nur vier oder fünf Stunden pro Tag. Wenn Termine gehalten werden müssen, werden auch Nächte durchgearbeitet. Neben dem Druck der Verlage führt auch der allgemeine gesellschaftliche Druck zu hoher Arbeitsmoral in Japan sowie das Prestige, möglichst viele Serien gleichzeitig zu veröffentlichen, zu diesem Arbeitspensum. In der Regel beschäftigt ein Künstler daher mehrere Assistenten, die Hilfsarbeiten ausführen wie das Zeichnen von Hintergründen, das Tuschen der Zeichnungen oder den Einsatz von Rasterfolien. Die Interaktion beziehungsweise Einbeziehung der Assistenten variiert jedoch stark: während manche Zeichner sie nur Nach- und Detailarbeiten übernehmen lassen und alle kreativen Arbeiten selbst erledigen, arbeiten andere ähnlich wie bei Filmproduktionen mit einem Team, dessen Ideen in das Werk einfließen und das selbstständig Teile übernimmt. Die Assistenten sind nicht immer angestellte Zeichner, sondern bisweilen auch Freunde oder Familienangehörige der Künstler. Erfolgreiche Künstler beschäftigen außerdem neben oft zehn oder mehr Assistenten auch einen Manager. Durch die geforderte Arbeitsgeschwindigkeit ist die Verwendung von Hilfsmitteln wie Rasterfolie zur Flächenfüllung oder vorgefertigter Hintergründe üblich.

Die Aufteilung der Hauptarbeit in einen Zeichner "(mangaka)" und einen Szenaristen ("(manga) gensakusha") ist selten, kommt aber eher bei Serien für Jugendliche vor. Die Szenaristen erreichen dabei selten die gleiche Popularität wie die Zeichner. Sie werden vor allem von Künstlern engagiert, auf denen nach einem ersten Erfolg die Erwartung liegt, weitere beliebte Serien zu schaffen. Vor allem jüngeren Künstlern fehlt dazu jedoch der Erfahrungsreichtum, sodass sie auf Szenaristen oder andere Ideengeber zurückgreifen. Das sind nicht selten auch die Redakteure der Magazine, für die die Künstler arbeiten. Diese wählen die Inhalte – Themen, Stimmungen und Stile – der Geschichten für das Magazin aus und suchen entsprechende Künstler, um die gewünschte Mischung im Magazin zu erreichen und damit die Zielgruppe ansprechen zu können. Darüber hinaus greifen die Redakteure nicht selten auch in die Entwicklung der Geschichten ein, halten engen Kontakt mit den Zeichnern, achten auf Einhaltung von Terminen. So hat das Magazin und dessen Redaktion oft erheblichen Einfluss auf den Inhalt einer Mangaserie, die darin erscheint.

In Japan erscheinen Mangas in unterschiedlichen Formen:

Verkaufsorte sind sowohl Kioske als auch Buchläden und Spezialgeschäfte sowie rund um die Uhr geöffnete Konbini. Außerdem gibt es Automaten, die Magazine verkaufen. Seit Anfang der 1990er Jahre existieren Manga Kissa – Cafés mit zum Lesen ausliegenden Mangas.

Als Dōjinshi bzw. "Dōjin" bezeichnet man von Fans gezeichnete inoffizielle Fortsetzungen oder Alternativgeschichten zu bekannten Anime bzw. Manga oder Spielen. In Japan werden sie oft von spezialisierten Kleinverlagen oder in Eigeninitiative veröffentlicht. Obwohl sie als Verwertung des geschützten Original-Materials fast immer Urheberrechte verletzen, gehen Verlage und Künstler fast nie dagegen vor. Stattdessen ist die Interaktion der Fans mit den Werken wesentlicher Bestandteil der Mediennutzung. Daneben werden auf dem Dōjinshimarkt auch viele Eigenschöpfungen veröffentlicht.

Während im Jahr 1967 in Japan von weniger als 50 Manga-Magazinen insgesamt 78 Millionen Exemplare verkauft wurden, waren es im Jahr 1994 von 260 Manga-Magazinen 1.890 Millionen Exemplare und 2006 1.260 Millionen verkaufte Exemplare. "Shōnen Jump", das erfolgreichste Magazin, erlebt wie die meisten anderen seit 1997 einen stetigen Rückgang des Absatzes: Mitte der 1990er-Jahre lag er noch bei sechs Millionen Exemplaren pro Woche – 2015 waren es 2,4 Millionen. In den Jahren zuvor verloren die großen Magazine häufiger 10 % und mehr ihrer Auflage pro Jahr. An zweiter Stelle nach "Shōnen Jump" steht das "Weekly Shōnen Magazine" mit etwa 1 Million verkauften Exemplaren pro Woche. Hohe Verkaufszahlen in ihren Sparten hatten 2017 das Kindermagazin "CoroCoro Comic" (780.000), die Seinen-Magazine "Young Jump" und "Big Comic" (je etwa 500.000), das Shōjo-Magazin "Ciao" (450.000) und die Josei-Magazine "For Mrs." und "Elegance Eva" (150.000).

Einzelbände erfolgreicher Serien haben üblicherweise Erstauflagen von 300.000 bis 500.000. Den Rekord hält gegenwärtig Band 56 der Serie "One Piece": Anfang Dezember 2009 wurde er in einer Erstauflage von 2,85 Millionen Exemplaren ausgeliefert, wofür der Shueisha-Verlag mit einer neunseitigen Zeitungsanzeige warb. Bis zum Jahr 2017 verkauften folgende Serien in Japan über 100 Millionen Exemplare (Summe der Verkaufszahlen aller Bände):


"Golgo 13" (gestartet 1968) und "Kochikame" (gestartet 1977) gehören zugleich auch zu den am längsten ununterbrochen laufenden Manga-Serien und zu denen mit der größten Anzahl an Sammelbänden.

Die Vermarktung von Manga findet oft im Zusammenspiel mit Anime-Serien und Kinofilmen, Videospielen, Spielzeugen, Hörspielen und weiteren Medien statt. Dabei kann eine erfolgreiche Mangaserie über die Adaptionen an Reichweite und damit an weiterer Popularität gewinnen oder sogar inhaltlich verändert werden, um andere Teile des Medienverbunds zu unterstützen, die erfolgreicher waren. Auch erscheinen wiederum Manga als Adaptionen anderer Medien. Die Wechselwirkungen in der Vermarktungskette ermöglichen es den Verlagen Risiken zu reduzieren. Zugleich drängen sie zur Konformität und können Innovationen bremsen, da diese noch nicht am Markt erprobt sind. Bei erfolgreichen Serien wird die Adaptionskette manchmal mehrfach wiederholt, das heißt sowohl erneut adaptiert als auch die Adaptionen erneut als Manga – zum Beispiel als Spin-off – umgesetzt, dem wiederum Umsetzungen in anderen Medien folgen. Im Laufe der Zeit wurde die Vermarktung immer schneller. Bei den ersten Verfilmungen in den 1960er Jahren vergingen zwischen Erstveröffentlichung des Mangas und der Adaption noch Jahre. Bereits bei "Dr. Slump" 1980 lagen zwischen dem ersten Kapitel des Mangas und der Premiere des ersten Films nur sechs Monate. Durch die zahlreichen Adaptionen haben Mangas einen erheblichen Einfluss auf das japanische Kino und Fernsehen.

Vergleichbar mit US-amerikanischen Comic-Verfilmungen gibt es in der japanischen Filmindustrie seit der Jahrtausendwende zunehmend Bestrebungen, Mangas als Realfilme oder -serien umzusetzen; Beispiele hierfür sind Touch, Ichi the Killer, Oldboy oder Uzumaki. Immer mehr japanische Regisseure sind mit Manga aufgewachsen, und der Fortschritt der Tricktechnik ermöglicht mittlerweile die Adaption selbst komplexester Szenen. Zudem können bei einer Manga-Umsetzung die Fans des Originalwerkes auch ohne großen Werbeaufwand erreicht werden. Zu den erfolgreichsten Realverfilmungen von Manga zählen unter anderem die Fernsehserie zu Great Teacher Onizuka (1998), deren letzte Folge die höchste jemals erreichte Einschaltquote eines Serienfinales im japanischen Fernsehen hatte, und der Kinofilm zu Nana (2005), der mit einem Einspielergebnis von umgerechnet ungefähr 29 Millionen Euro auf Platz 5 der erfolgreichsten japanischen Kinofilme des Jahres kam. Mit Death Note (2006) war eine Manga-Umsetzung erstmals von vornherein als zweiteilige Kinofassung ausgelegt.

Auch jenseits von Adaptionen hatten Mangas Einfluss auf die japanische Spieleindustrie. Viele Spiele greifen Stilelemente aus dem Manga auf und textbasierte Adventure-Spiele sind üblicherweise mit Illustrationen ähnlich eines Mangas versehen, sodass sie als technische Weiterentwicklung des grafischen Erzählmediums verstanden werden können. Auch hinsichtlich Inhalt und Genre nehmen japanische Spiele immer wieder Anleihen beim Manga.

Seit den 1990er Jahren setzt sich die japanische Gegenwartskunst verstärkt mit der Ästhetik von Mangas auseinander, auch bestärkt durch deren anhaltende und internationale Popularität. Zu Beginn der 2000er Jahre kam es zeitweise zu einem Boom dieser Auseinandersetzung, die das japanische Kunstgeschehen, große Ausstellungen und Kataloge dominierte. Es werden Bezüge zu bekannten Mangaserien oder deren Figuren, typische Designs, Vereinfachung und Niedlichkeit oder sequenzielle Elemente aufgegriffen. Im Rahmen dessen erschien auch das "Super Flat Manifesto" von Takashi Murakami, in dem dieser eine Tradition „abgeflachter“ japanischer Pop-Art mit Wurzeln in der Edo-Zeit postuliert. Dieser Manga-bezogene Trend löste die vorhergehende Auseinandersetzung mit Zen-Minimalismus, Abstraktion und Aktionskunst in der japanischen Kunst ab.

Auch unter den Künstlern gibt es Verbindungen zu anderen Medien. Viele Mangaka begeistern sich für Filme oder schauen schon zum Sammeln von Inspirationen viele Filme. Manche von ihnen sagen auch, dass sie zunächst in die Filmbranche gehen wollten. In der Vergangenheit sind auch viele weniger erfolgreiche – und manche erfolgreiche – Mangazeichner später zum Film oder zur Prosa gewechselt. Einige sind auch in beiden Feldern tätig. So ist Hayao Miyazaki zwar als Anime-Regisseur bekannter, schuf jedoch auch einige Manga-Serien. Zuletzt kommt es auch häufiger vor, dass Schriftsteller Szenarien für Mangas schreiben oder ihre Werke als Manga adaptiert werden. Darüber hinaus haben Mangas Einfluss genommen auf die japanische Literatur, sowohl inhaltlich als auch stilistisch. Schließlich entstand aus der Mischung von Manga und Prosa in Japan die Light Novel: Unterhaltungsromane für eine jugendliche Zielgruppe mit einigen Illustrationen im typischen Manga-Stil.

Manga sind eine der Hauptsäulen des japanischen Verlagswesens. Die Sparte macht seit vielen Jahren gut ein Drittel aller Druckerzeugnisse in Japan aus. Der Comic-Markt besteht in Japan, anders als in anderen Ländern, fast ausschließlich aus einheimischen Produktionen. Neben dem Markt der Verlage existiert auch ein Markt für Fanpublikationen, zu dem keine genauen Zahlen erhoben werden können, auf dem aber ebenfalls große Summen umgesetzt werden. Dagegen ist der Sammlermarkt von geringerer Bedeutung als in anderen Ländern.

1978 erreichte der Umsatz der Manga-Branche 184,1 Milliarden Yen. Auf dem Höhepunkt des Manga-Magazin-Markts 1995 setzte die Branche 586 Mrd. Yen um. Statistisch gesehen kaufte jeder Japaner pro Jahr 15 Manga. Während lange Zeit die Magazine den Großteil der Umsätze erwirtschafteten – 2002 noch zwei Drittel der Verkäufe – zeigte sich ab der Jahrtausendwende ein neuer Trend: Während im Jahr 2004 die Gesamteinnahmen bei Magazinen noch bei ca. 255 Milliarden Yen und bei Taschenbüchern bei ca. 250 Milliarden Yen lagen, gingen im Jahr 2005 die Einnahmen bei Magazinen auf 242 Milliarden Yen zurück (und lagen damit nur noch bei etwa 70 % der Einnahmen des Jahres 1995), während die Einnahmen bei Taschenbüchern auf 260,2 Milliarden Yen stiegen. 2016 machten Taschenbücher mit 194,7 Mrd. Yen dann etwa zwei Drittel des Print-Manga-Marktes von 296,3 Mrd. Yen aus. Zugleich waren Magazine, selbst die mit dem größten Umsatz, nie besonders profitabel, sondern stets Instrumente zum Testen der Serien. Gewinne erzielen die Verlage erst mit Taschenbüchern und weiterer Vermarktung. In dieser Kombination von Magazinen als Marktöffner und Taschenbüchern als Gewinnbringer wird ein Grund für den wirtschaftlichen Erfolg des Mediums in der 2. Hälfte des 20. Jahrhunderts gesehen. Gründe für den Rückgang der Verkäufe seit den 1990er Jahren sind die lange andauernde wirtschaftliche Krise und die Konkurrenz durch neue Unterhaltungsprodukte wie Computerspiele, Internet und Smartphones. Außerdem altert und schrumpft die japanische Bevölkerung insgesamt, wodurch die für Manga besonders wichtige Gruppe der Kinder, Jugendlichen und jungen Erwachsenen kleiner wird. Darüber hinaus werden wieder mehr Mangas geliehen oder gebraucht gekauft als in den 1990er Jahren, sodass bei gleicher Leserzahl weniger Exemplare abgesetzt werden.

Zwar verliert der Print-Markt weiter an Bedeutung, allerdings wird dies seit 2014 durch den stark wachsenden Digitalmarkt wie Manga-E-Books abgefangen, so dass der Manga-Markt sich insgesamt auf etwa 450 Mrd. Yen stabilisiert hat. 2017 überstieg der Umsatz von digital verkauften Serien erstmals dem von Taschenbüchern.

Der Markt wird von wenigen großen Verlagen dominiert; Kōdansha, Shōgakukan und Shūeisha (eine 50%ige Tochter Shōgakukans) erwirtschaften etwa 70 % des Umsatzes, während sich viele kleine und mittlere Verlage den Rest aufteilen. Zu den bedeutenderen mittleren Verlagen zählen Hakusensha, Akita Shoten, ASCII Media Works (seit 2013 Teil von Kadokawa), Square Enix, Kadokawa Shoten, Ōzora Shuppan, Futabasha und Shōnen Gahōsha. Kleinere Verlage spezialisieren sich oft auf bestimmte Genres oder Zielgruppen.

Seit dem verstärkten Aufkommen digitaler Verbreitung in den 2000er Jahren werden auch Mangas häufiger und international illegal verbreitet. Dies wird durch Künstler und Verlage zunehmend als wirtschaftliche Bedrohung wahrgenommen. Neue Geschäftsmodelle wie verstärkte Angebote von legalen digitalen Mangas wie auch kostenlose Bereitstellung bei Finanzierung über Werbung oder Verkauf von Merchandising werden dem entgegengesetzt, jedoch ohne dass immer zufriedenstellende finanzielle Ergebnisse erzielt werden können. Die Künstler selbst haben, im Gegensatz zu Musikern, kaum die Möglichkeit, Geld durch Live-Auftritte zu verdienen. Befördert wird die digitale, illegale Verbreitung auch durch Leser, die ihre Mangas zur besseren Lagerung für den Eigenbedarf scannen, dann aber auch anderen zugänglich machen.

Im Gegensatz zu vielen anderen Ländern, in denen Comics als reine Kinder- und Jugendliteratur bzw. nur als Unterhaltungsmedium gelten, sind Comics in Japan als gleichberechtigtes Medium und Kunstform anerkannt und werden von Menschen aus allen gesellschaftlichen Gruppen konsumiert. Comic lesende Pendler oder Geschäftsleute sind nichts Ungewöhnliches, auch Politiker bis zu Premierministern geben Mangalesen als Hobby an oder nutzen Mangas als Medium. Der öffentliche Raum ist besonders während der Fahrt zur Arbeit ein üblicher Ort zur Rezeption von Manga-Magazinen oder Sammelbänden. Außerdem besteht eine große Tauschkultur („mawashiyomi“), sodass viele Bände durch mehrere Hände gehen, und Manga Kissa – Cafés mit ausliegenden Mangas – sind als Orte zur preiswerten Lektüre beliebt. Von Verkäufern verboten, aber dennoch verbreitet ist "tachiyomi" („im Stehen lesen“) – das Lesen von Mangas in der Auslage der Läden, ohne sie zu kaufen.

Manga spiegeln in Japan wie andere Medien auch gesellschaftliche Werte und Entwicklungen wieder. So haben die Samurai-Traditionen des Bushidō im Shōnen-Manga Niederschlag gefunden. Zugleich erfährt der Umgang mit diesen Werten immer wieder Veränderung – von der Glorifizierung des Krieges bis zu Darstellung persönlicher Dramen vor historischem Hintergrund oder die Übertragung der Werte in Sport und Beruf. Auf der anderen Seite haben Manga-Serien durch die allgemeine Anerkennung als Kunstform und starke Verbreitung seit vielen Jahrzehnten auch selbst Einfluss auf andere Medien, Kunstformen und die japanische Kultur.

Die Ästhetik von Manga ist in der japanischen Kultur so weit akzeptiert und verbreitet, dass sie oft nicht nur für Manga selbst, sondern auch für Schilder, Illustrationen und Werbefiguren verwendet wird. Als Werbeträger dienen sowohl etablierte Figuren aus bekannten Mangaserien als auch eigens für die Werbung geschaffene Figuren. Dazu kommen Produkte, die aus der Vermarktung erfolgreicher Serien entstehen. Die Strategie hinter dem Einsatz der Manga-Ästhetik außerhalb der Serien und insbesondere in Anleitungen und Schildern liegt darin, über ikonische Zeichen und Figuren Orientierung zu vermitteln und komplizierte Abläufe verständlich zu machen. Um die Überwindung von Sprachbarrieren geht es in der Regel nicht, da die Illustrationen fast nur von Japanern angeschaut werden und mit japanischem Text versehen sind.

Der Erfolg von Manga in Japan wird gern damit erklärt, dass die Alphabetisierung schon lange hoch war, das Fernsehen relativ spät eingeführt wurde oder es in den großen Städten viele Pendler gibt, die Manga auf der Fahrt lesen. Jason Thompson erklärt den Erfolg eher mit der Fähigkeit der Autoren, sehr lange Geschichten zu erzählen, die ihre Leser mitreißen sowie mit einem größeren Fokus der Aufmerksamkeit und der Urheberrechte auf den Künstlern statt auf Franchises, der zugleich zu einem starken Wettbewerb unter den Künstlern führt. Frederik Schodt sieht einen Grund für den Erfolg der Comics in dem in Japan herrschenden Leistungsdruck ab der Mittelschule sowie dem Leben in dicht besiedelten Städten. Dies führe dazu, dass sich viele eine Freizeitbeschäftigung suchen, die schnell und kurz sowie ohne Störung von anderen konsumiert werden kann, überallhin mitgenommen werden kann und zugleich die Möglichkeit bietet, aus dem anstrengenden, wenig motivierenden Alltag zu entfliehen, wie es ähnlich auch Paul Gravett beschreibt. Im Gegensatz zu den Vereinigten Staaten sei es der Comicszene in Japan außerdem gelungen, die seit den 1950er Jahren immer wieder bestehenden Zensurbestrebungen – vor allem von Lehrer- und Elternverbänden – sowohl politisch wie wirtschaftlich zu überstehen. Von anderen wird der Erfolg des Mediums auch japanischen Besonderheiten zugeschrieben: So gebe es in Japan durch die aus einer großen Zahl ursprünglich bildhafter Symbole bestehenden Schrift und die lange Tradition stilisierender beziehungsweise karikierender Malerei eine Affinität zu entsprechend symbolhaften Darstellungen und bildlichem Erzählen. Insbesondere direkt nach dessen Tod wurde auch Osamu Tezuka persönlich, seinem Engagement und dem großen Umfang und der Vielfalt seines Werks ein großer Anteil am Erfolg des Mediums zugeschrieben. Der Kulturhistoriker Tomafusa Kure nahm an, Mangas hätten in Japan den Platz der Literatur ausgefüllt, nachdem diese im 20. Jahrhundert immer intellektueller geworden sei, sich auf psychologische Zustände konzentriert habe und damit viele Leser verloren hatte.

Kritik am Medium Manga gibt es in Japan seit den 1950er Jahren, als auch in den USA und Europa Comics im Zentrum gesellschaftlicher Medienkritik standen. Wegen der bereits früh ausgeprägten Differenzierung nach Zielgruppen verschiedenen Alters hatte die Kritik von Eltern und Pädagogen in Japan jedoch wenig Widerhall gefunden. Erst nachdem in den 1980er Jahren sexuelle Darstellungen auch in Publikationen für Jugendliche und Kinder zunahmen, verstärkte sich die Kritik. Sie gipfelte in der Debatte nach dem Fall des sogenannten Otaku-Mörders Tsutomu Miyazaki, der 1989 vier Mädchen tötete und Anime- und Manga-Fan war. Daraus entstand eine Diskussion um einen Zusammenhang zwischen den Medien und Gewalt. In dieser wird jedoch auch angeführt, dass es in Japan mit seinem sehr hohen Manga-Konsum eine der geringsten Raten an Verbrechen, insbesondere Gewaltverbrechen gibt und daher eine Verbindung zwischen beidem nicht ersichtlich sei. So wird auch argumentiert, dass die unter anderem durch Manga große Verfügbarkeit von sexuell wie an Gewalttaten freizügigen Inhalten eher dazu führt, dass weniger Gewalttaten begangen werden. Das Verhältnis der japanischen Medien und Gesellschaft zu sexualisierten und gewalthaltigen Darstellungen, die Auslegung der diesbezüglichen Gesetze und deren Abwägung mit der Kunstfreiheit ist nach wie vor ungeklärt.

Ähnliche Diskussionen gab es um die Verbindung von Manga zur Sekte Ōmu Shinrikyō, die in Japan einen Giftgasanschlag verübte und deren Begründer in seiner Ideologie von Science-Fiction-Serien beeinflusst war. Ebenfalls Anfang der 1990er Jahre gab es eine Kampagne gegen die stereotype Darstellung von Afrikanern und Afroamerikanern in Mangas. Die Stereotype mit Ursprüngen in Kolonialismus, Sklaverei und Rassismus wurden bis dahin kaum bemerkt oder unbedacht humoristisch eingesetzt. Diese wurden nun international von Minderheitenvertretern stärker wahrgenommen, jedoch wegen Auswahl und mangelnder Sprachkenntnis teils in wiederum verzerrter Form. In Folge der Debatte wurden einige Geschichten umgezeichnet oder ältere Werke kommentiert und bei Redaktionen und Zeichnern herrschte danach ein größeres Problembewusstsein, aber auch die Sorge, unberechtigter Kritik ausgesetzt zu werden. Die Kritik, Mangas würden verhindern, dass Kinder richtig lesen lernen, wurde nur selten erhoben. Ihr gegenüber steht die in Japan sehr hohe Alphabetisierung von etwa 99 %.

Paul Gravett beobachtete in der Leserschaft von Mangas mehrere Gruppen: neben den meisten Lesern, die nur gelegentlich lesen und nur einige wenige Serien verfolgen, gibt es eine deutlich kleinere Gruppe von Fans des Mediums sowie innerhalb diesen die besonders aktive Gruppe der Otaku, wie besonders besessene Fans teilweise genannt werden. Daneben gibt es einen Markt von Sammlern seltener Manga-Ausgaben, der jedoch deutlich kleiner als die Sammlerszene in den USA ist – auch weil die meisten Serien immer wieder neu aufgelegt werden. Otaku wurden seit Aufkommen der Szene in den 1980er Jahren von der Gesellschaft und Medien als gestörte Stubenhocker gebrandmarkt und grenzten sich – auch als Reaktion darauf – bewusst von der Gesellschaft ab. Für Fans, die besonders viele Serien sammeln und nicht mehr alle Bücher lagern können, wird zunehmend eine Dienstleistung namens "jisui" angeboten: ein Unternehmen scannt für den Kunden dessen Bücher, die er dann nur noch als digitale Kopie aufbewahrt.

Seit den 1970er Jahren entwickelte sich eine starke Fanszene, die wesentliche Überschneidungen zu der von Animes aufweist. Fans sind oft selbst kreativ tätig. Beliebte Formen sind Fanart, Fanfiction und Dōjinshi, wobei die Verlage in Japan dabei meist dulden, dass bei den selbstverlegten Fortsetzungen oder Alternativerzählungen der Fans Urheberrechte verletzt werden. Auch werden von Fans Veranstaltungen organisiert: Der zwei Mal jährlich seit 1975 in Tokio stattfindende Comic Market (auch ‚Comiket‘ genannt) ist nicht nur die größte Dōjin-Messe Japans, sondern mit 35.000 Ausstellern und über 500.000 Besuchern sogar die größte Comic-Veranstaltung der Welt. Die kreative Fanszene, in der neben Adaptionen bekannter Werke auch Eigenschöpfungen entstehen und angehende Künstler ihre ersten Werke publizieren, wird von jungen Frauen dominiert. Das rührt auch daher, dass in der japanischen Gesellschaft der Druck auf Männer größer ist, schnell zu studieren und berufstätig zu werden, sodass sie seltener Gelegenheit für zeitaufwändige Hobbys haben. Cosplay – das Verkleiden als Figur aus einer Mangaserie – ist ein beliebtes Hobby in der Fanszene. Dies reicht bis zu Cosplay-Cafés, in denen die Bedienung kostümiert ist.

Forschung zum Manga und eine Szene von Kritikern konnte sich trotz weiter Verbreitung und gesellschaftlicher Anerkennung bis in die 1990er Jahre nicht entwickeln. Allein den Comicstrips war ab den 1960er Jahren bereits einige Aufmerksamkeit von Forschern und Kritikern zuteil geworden. Das erste Museum, das sich umfangreich mit Manga beschäftigte und gezielt sammelte, war das 1989 eröffnete Kawasaki City Museum. Ein erstes Symposium zum Manga veranstaltete die Japanische Gesellschaft für Kunstgeschichte im Jahr 1998; 2001 wurde die Japanische Gesellschaft für Manga-Studien (Nihon Manga Gakkai) gegründet, die jährliche Konferenzen veranstaltet. Ein erster Studiengang entstand 2006 an der Seika-Universität Kyōto, weitere folgten. Im gleichen Jahr eröffnete das Kyōto International Manga Museum. Das Forschungsfeld konzentriert sich bisher auf historische Betrachtungen des Mediums sowie spezifische Aspekte der visuellen Sprache und des Erzählens. Außerdem wurden soziologische Themen wie die Verbindung von Genres und Gender oder Subkulturen untersucht.

Zu den bedeutendsten im Manga-Bereich verliehenen Preisen gehören als älteste Auszeichnung der vom gleichnamigen Verlag 1956 ins Leben gerufene Shōgakukan-Manga-Preis für die besten Mangas, sowie der seit 1977 verliehene Kodansha-Manga-Preis und der seit 1997 von der Zeitung Asahi Shimbun jährlich in vier Kategorien vergebene Osamu-Tezuka-Kulturpreis für herausragende Zeichner und Personen oder Institutionen, die sich um die Mangas besonders verdient gemacht haben.

In jüngerer Zeit werden auch Auszeichnungen von politischen und Kulturinstitutionen vergeben. Dies sind der Internationale Manga-Preis des japanischen Außenministeriums, der Japan Media Arts Award des japanischen Kulturamts und der von mehreren Museen vergebene GAIMAN Award. Darüber hinaus werden Mangas in Japan immer wieder auch mit Literaturpreisen ausgezeichnet.

Die internationale Verbreitung von Manga wurde durch den vorhergehenden Erfolg von Anime, japanischen Animationsfilmen, gefördert. Auch dabei spielte, wie bei der Geschichte des Mangas selbst, Osamu Tezuka eine bedeutende Rolle, da gerade die von ihm produzierten Verfilmungen seiner Manga-Serien im Ausland großen Erfolg hatten. Die Verbreitung von Anime-Serien insbesondere in den 1990er Jahren führte schließlich dazu, dass die Stilmerkmale von Manga einem jungen Publikum vertraut wurden und sie dem Medium offener gegenüber standen. Darüber hinaus waren diese Serien stark mit Merchandisingprodukten wie Spielzeug verknüpft, in deren Gefolge auch die Mangaserien vermarktet wurden. Die vorhergehende Popularisierung von Anime hatte insbesondere in den 1990er Jahren jedoch auch die Folge, dass nach der selektiven Wahrnehmung einiger pornografischer Anime als „der Anime“ das Vorurteil eines sexualisierten, potentiell gefährlichen Mediums auch auf Manga übertragen wurde. Auch die Verwendung der Manga-Ästhetik in Werbung für erotische Angebote im nächtlichen Fernsehprogramm leistete dem Vorschub. Das Vorurteil eines von Gewalt und Sex durchdrungenen und zugleich kindischen Mediums bestand bereits seit den 1960er Jahren, als es im Westen erste Berichte über japanische Comics gab, die Aufsehen erregende Beispiele herausgriffen. Paul Gravett führt diese auch auf das allgemeine Vorurteil eines unreifen, der Führung bedürftigen Japans zurück, das in der Nachkriegszeit in den Vereinigten Staaten verbreitet war. Auch während der beginnenden internationalen Popularisierung in den 1990er Jahren wurden Vorurteile durch selektive Darstellung der entweder besonders kinderfreundlichen oder avantgardistischen, verstörenden Werke noch bestärkt. Auch unter japanophilem Publikum war Manga wenig beliebt, da das Medium als Gegensatz zur japanischen Hoch- und Hofkultur wie Teezeremonie und Gartenkunst wahrgenommen wurde, als hedonistische, rebellische Popkultur. Begünstigt wurde die Wahrnehmung als gefährliches Medium auch dadurch, dass die früh international erschienenen, positiv angenommenen Verfilmungen für ein jüngeres Publikum nicht als japanisch beziehungsweise als Anime wahrgenommen wurden. Durch Synchronisation und Genreauswahl – bevorzugt wurden kulturell neutrale Science-Fiction- und Fantasy-Inhalte – blieb die japanische Herkunft weitgehend verborgen. Dies trug auch zum schnellen Erfolg von Anime bei, dagegen war die Leserichtung für Manga eine Hürde. Daher wurden die lizenzierten Ausgaben zunächst oft gespiegelt. Die spezifische Dominanz von Bildern gegenüber Text, archetypisch-symbolische Gestaltung der Figuren und der filmische Erzählstil dagegen machen das Medium Manga international leicht verständlich. Ähnliches gilt für die stark stilisierten und nicht als japanisch erkennbaren Charakterdesigns und die inhaltliche Vielfalt, die auch eine Vielzahl an Werken ohne Bezüge zur japanischen Kultur bietet. Jedoch ist international – im Gegensatz zur Situation in Japan selbst – fast nur der Story-Manga bekannt und verbreitet.

Der Erfolg von Manga außerhalb Japans und insbesondere bei der Generation der 1980er- und 1990er Jahre wird gern mit seiner Andersartigkeit erklärt, die der Abgrenzung zu anderen Kulturprodukten und der Elterngeneration diene. Jedoch war dieser Generation der Stil von Manga gar nicht so fremd, da sie bereits über Animeserien damit vertraut wurden. Außerdem sind die in den 1990er Jahren erfolgreichen Serien für ein jüngeres Publikum entstanden, bieten Charaktere, mit denen sich die Leser leicht identifizieren können, und laden zur kreativen Auseinandersetzung in Form von Fanart ein. Auch die schnell gewachsene Fanszene trug zur Attraktivität des Mediums bei. Die Ästhetik von Manga und Anime fand mit der internationalen Popularisierung der Medien auch über die Konsumenten der japanischen Werke hinaus Verbreitung, Akzeptanz und Gefallen, sodass sie auch in nicht-japanischen Produktionen aufgegriffen wurde. Besonders in der Fanszene, aber auch darüber hinaus, entstand außerdem ein größeres Interesse an anderen Aspekten der japanischen Kultur. In einigen Jugendkulturen entstand seit den 2000er Jahren eine Japan-Mode, sodass Japan hier größeren Einfluss als die früher Vorbild gebenden Vereinigten Staaten hat. Manga wurde zusammen mit Anime unter dem Schlagwort Cool Japan zu einem kulturellen Botschafter Japans, in deren Kontext auch andere Aspekte der japanischen Kultur vermittelt werden sollen und die zu einem Gegenstand und Mittel japanischer Außenpolitik wurden. Dem dient auch der 2007 vom japanischen Außenministerium ins Leben gerufene Internationale Manga-Preis.

Ein für die Verbreitung wichtiger Weg war das Internet, über das zunächst Informationen über das Medium und dann zunehmend auch übersetzte Serien leicht zugänglich wurden. Diese waren wie zuvor schon die in der Fanszene kursierenden Kopien illegal, dennoch hatte diese Verbreitung großen Anteil an der Popularisierung und schließlich auch dem kommerziellen Erfolg von Mangas außerhalb Japans. Die im Laufe der 2000er Jahre immer einfachere Zugänglichkeit und Umfang der illegalen Kopien wird jedoch auch für einige Marktschwankungen, wie in den USA, mitverantwortlich gemacht und nimmt anders als in Japan einen erheblichen Anteil des Konsums ein. Dem gegenüber steht, dass viele Serien nur sehr spät, langsam oder nie außerhalb Japans veröffentlicht werden und daher illegale Kopien oft der einzig mögliche Zugang sind. Viele Scanlationgruppen werden außerdem von den Verlagen toleriert, da sie ihre Fanübersetzungen zurückziehen und zum Kauf aufrufen, sobald die Serie für das Zielpublikum offiziell veröffentlicht wurde. Scanlations haben auch Druck auf die Verlage ausgeübt, Manga möglichst originalgetreu zu veröffentlichen und waren Vorbild beispielsweise bei der unveränderten Übernahme von Lautmalereien und japanischen Anreden in Übersetzungen. Das trug dazu bei, vielen Manga-Veröffentlichungen im Westen einen eher exotischen Charakter zu verleihen, anstatt dass sie an das heimische Publikum angepasst wurden.

Manga hat Comics im Westen, die hier zuvor fast ausschließlich an männliche Leser gerichtet waren, für ein weibliches Publikum attraktiv gemacht und dem Medium so eine wesentlich größere Leserschaft erschlossen. Der weibliche Teil der Fanszene ist oft auch deutlich stärker selbst kreativ in Form von Fanart. Teils im Westen geäußerter Kritik, Mangas seien sexistisch, würden Frauen herabsetzen oder Vergewaltigung verherrlichen, traten Timothy Perper und Martha Cornog 2002 in einer Studie entgegen. Nach Auswertung aller in drei Jahren auf Englisch erschienenen Werke stellten sie fest, dass Mangas keinesfalls frauenfeindlich seien, sondern im Gegenteil vielfältige feministische Qualitäten besäßen und der Sexualität gegenüber zwar positiv eingestellt seien, dabei aber Widerstand gegen sexuelle Übergriffe übten, die sie darstellen. Die interkulturell anschlussfähigen und übertragbaren Stilmerkmale und Erzählstrategien des Manga wurden in den etablierten internationalen Fanszenen aufgegriffen, in denen Werke entstanden, die von den japanischen Vorbildern inspiriert waren. Auf diese Weise entstanden diverse internationale „Ableger“, die sich oft stärker mit Manga identifizieren als mit der jeweils nationalen Comickultur. Eng mit der japanischen Kultur verbundene Aspekte des japanischen Comics gingen dabei verloren, die Stilmerkmale wurden globalisiert. Unterstützt wurde dies durch den Erfolg von "How-to-Draw-Manga"-Anleitungen, die diese Merkmale außerhalb Japans verbreitet und standardisiert haben.

Allgemein kann Manga zusammen mit Anime als ein Beispiel kultureller Globalisierung gelten, das nicht vom Westen ausging. Im Prozess der Verbreitung von Mangas zeigen sich sowohl globale Homogenisierung als auch Heterogenisierung, indem Teile der japanischen Kultur weltweit wirken, dabei aber selbst oft von ihrem Ursprung entfremdet und kulturell angepasst werden. In einem zweiten Schritt werden diese importierten Aspekte in lokale Comickulturen integriert. Darüber hinaus wirkt Manga einer kulturellen Dominanz der Vereinigten Staaten entgegen, wobei sich diese Wirkung kaum über den Kulturbereich von Comics hinaus erstreckt.

Im Vergleich zu Amerika und Europa hatten japanische Comics in Ostasien bereits früh Erfolg und fanden insbesondere in Südkorea, Taiwan und Hongkong schnell Verbreitung. Jedoch waren viele der Veröffentlichungen zunächst nicht lizenziert, sodass kein Geld an die Urheber gezahlt wurde. Erst mit dem Rückgang der Verkäufe in Japan übten die japanischen Verlage und die Regierung Druck in den Nachbarländern aus, auf dass die illegalen Veröffentlichungen bekämpft und Lizenzen erworben wurden. Die Serien waren weiterhin populär und ermöglichten den Verlagen zusätzliche Einnahmen. Als Gründe für den schnellen Erfolg von Mangas in den Nachbarländern Japans – meist gegen die dortige Zensur und gesellschaftliche Widerstände – werden die gute Lesbarkeit und Überlegenheit von Layout und Erzähltechniken und die kulturelle Nähe genannt. Mit dieser Nähe gehe eine leichtere Identifizierung mit Inhalten und Figuren einher. Dazu komme die oft größere inhaltliche Freiheit als bei einheimischen Produktionen, gerade in Hinblick auf Humor, Sex und Gewalt, und damit ein höherer Unterhaltungswert. Außerdem ist die Übersetzung einfacher, da das Spiegeln meist nicht nötig ist.

In Taiwan sind Manga so erfolgreich, dass die meisten großen japanischen Magazine auch hier erscheinen. Zugleich war das Land lange Zeit eines derjenigen mit den meisten illegalen Kopien – der größte dieser Verlage war Tong Li Publishing, der über 1000 Werke ohne Lizenz herausbrachte. Manga war wegen der japanischen Kolonialherrschaft über Taiwan politisch nicht erwünscht und daher Zensur unterworfen. Diese griff wegen der illegalen Kopien jedoch kaum und Taiwan war trotz der Einschränkungen Sprungbrett für die Weiterverbreitung von Mangas in andere Länder Ostasiens. So konnten Mangas seit den 1950er Jahren den Comicmarkt in Taiwan und die lokale Comickultur prägen. Zeitweise wurde dies von den Zensurbehörden hingenommen, später wurden anti-japanische Strömungen genutzt, um den einheimischen Comic zu fördern. In den 1990ern entwickelte sich ein Lizenzmarkt, in den die meisten der vorherigen Piratenverlage einstiegen.

Auf Korea hatten die japanischen Comics bereits früh Einfluss, da während der japanischen Kolonialherrschaft viele japanische Produkte in das Land kamen und ein reger Austausch stattfand. Nach der Befreiung 1945 waren japanische Produkte dagegen geächtet und lange Zeit deren Import und Verbreitung gesetzlich verboten. Dennoch gelangten ab den 1950er Jahren immer mehr Mangaserien als unlizenzierte Kopien nach Südkorea, wo sie häufig ohne Herkunfts- und Autorangabe oder mit koreanischen Autoren versehen verbreitet wurden. Die offiziell durch die koreanische Zensur gelangten Kopien waren oft durch koreanische Zeichner von der japanischen Vorlage abgezeichnet. Den Höhepunkt erreichte der nicht-lizenzierte Markt in den 1980er Jahren, ehe ab 1990 das Verbot japanischer Produkte gelockert und Lizenzverträge abgeschlossen wurden. So wurde erstmals überhaupt bekannt, dass viele der beliebten Serien japanischen Ursprungs waren, was zu nationalistisch motivierter Kritik und Sorge um zu großen japanischen Einfluss führte. Mangas wurden für die Darstellung von Gewalt und Sexualität kritisiert und als schlechter Einfluss auf die koreanischen Kinder dargestellt. Zugleich entwickelte sich ein Magazin- und Taschenbuchmarkt nach japanischem Vorbild, auf dem zunehmend auch die einheimischen Künstler Fuß fassen konnten. Als Folge der Kritik an Mangas fördert die südkoreanische Regierung seitdem stärker die nationale Comicbranche, den Manhwa. Die starken japanischen Einflüsse auf diese sind wegen der Ablehnung japanischer Kultur gesellschaftlich umstritten oder werden sogar verleugnet.

Nach China kamen Mangas über Kopien aus Taiwan und Hongkong. Doch erste Einflüsse der japanischen Comics gab es bereits in den 1920er Jahren. Aus diesen und japanischen Einflüssen in Hongkong in den 1960ern entstand der Manhua, der chinesische Comic. Der zur gleichen Zeit in der Stadt florierende Markt illegaler Kopien funktionierte ähnlich wie in Taiwan und Korea und wurde in den 1990ern durch einen Lizenzmarkt abgelöst. Parallel dazu wurden aus Japan einfache Arbeiten in der Manga-Branche an Zeichner in China ausgelagert, was zu einem erneuten Einfluss des Mangas auf die lokale Comickultur führte, einschließlich einer größeren Genrevielfalt nach japanischem Vorbild. Der größte Verlag im Markt, sowohl bei Importen als auch Eigenproduktionen, ist Jade Dynasty Group. Die ersten offiziell in China veröffentlichten Mangas waren ab 1981 "Astro Boy" und "Kimba". Auf die Verbreitung von Mangas in China und Taiwan folgten auch die Staaten Südostasiens. In Singapur waren Mangas Ende der 1990er Jahre die beliebtesten Comics. Dort, in Thailand, den Philippinen und Malaysia verbreiteten sich wie in den Nachbarländern Japans zunächst illegale Kopien in großem Maße, ehe lizenzierte Veröffentlichungen folgten. In jedem der Länder dominieren Mangas die Comicmärkte.

In den Vereinigten Staaten hatte der Import von Mangas lange Probleme mit der etablierten Comickultur. Es musste nicht nur Verständnis für die anderen Stile und Erzählweisen entstehen, sondern Comics waren fest mit Superhelden und der Zielgruppe von Sammlern und männlichen Jugendlichen verknüpft und zwei Verlage dominieren den Markt. So bedurfte es vieler Versuche, ehe erfolgreiche Vermarktungswege und Zielgruppen für Mangas gefunden waren. Japanische Comics wurden daher in den Vereinigten Staaten ab den 1970er Jahren zunächst nur einer kleinen Gruppe bekannt: Fans japanischer Animationsfilme oder Japanischstämmige. Der erste in den USA veröffentlichte übersetzte Manga war "Barfuß durch Hiroshima", der 1978 von einer in San Francisco und Tokio tätigen Fan-Übersetzergruppe privat verlegt, aber nach kurzer Zeit wieder eingestellt wurde. Größere Verbreitung fanden bald darauf zwei Kurzgeschichten von Shinobu Kaze: Seine zehnseitige Geschichte "Violence Becomes Tranquility" erschien im März 1980 im Comicmagazin "Heavy Metal", und die sechsseitige Geschichte "Heart And Steel" im Februar 1982 im Magazin "epic". 1982 folgte ein Versuch, die auch vom "Barfuß-durch-Hiroshima"-Schöpfer Keiji Nakazawa stammende Kurzgeschichte "I Saw It" herauszubringen und in dem von Art Spiegelman herausgegebenen Avantgarde-Magazin RAW wurden im Mai 1985 mehrere Kurzgeschichten von Zeichnern des japanischen Magazins Garo veröffentlicht. Im gleichen Jahr erschien mit dem "Mangazine" erstmals ein amerikanisches Magazin, das Fancomics im Stil von Mangas gewidmet war. Mit zunehmender Popularität des Mangas entwickelte sich mit dem OEL Manga („Original English Language Manga“) ein eigenes Marktsegment. Auch darüber hinaus nahmen Mangas seit den 1980er Jahren Einfluss auf die amerikanische Comicszene – so ließen sich Art Spiegelman und Frank Miller vom Medium inspirieren. Ab Mai 1987 erschien bei First Comics die Manga-Serie "Lone Wolf & Cub", deren erste Bände aufgrund des großen Verkaufserfolges bereits nach kurzer Zeit nachgedruckt werden mussten. Noch im gleichen Jahr brachte Eclipse Comics zusammen mit Viz die Manga-Serien "Kamui", "Mai the Psychic Girl" und "Area 88" als zweiwöchentlich erscheinende Comichefte heraus. Viz war als Ableger des japanischen Verlags Shogakukan gegründet worden und ist als Viz Media noch immer im Markt aktiv. Im Jahr 1988 folgte Marvel Comics mit der Veröffentlichung von "Akira", das zu einem Wegbereiter für Manga- und Anime weltweit wurde. In der folgenden Zeit wurde Dark Horse zum größten Manga-Verlag in den USA neben Viz.

Die ersten Mangas in den USA waren zur Anpassung an die übrigen Comicpublikationen auf Albenformat vergrößert und auf westliche Leserichtung gespiegelt worden. In dieser Phase waren die meisten Manga-Figuren daher scheinbar Linkshänder und japanische Schriftzeichen auf Schildern und Plakaten wurden seitenverkehrt abgedruckt. Mitte der 1990er Jahre erschienen erste Fan- und Fachmagazine sowie vermehrt auch Mangas für Japan-Interessierte oder Studierende, deren Zahl zugleich durch die Verbreitung von Anime und Manga zunahm. Der Verkauf lief noch über Comicläden. Das änderte sich erst mit dem Erfolg von "Sailor Moon" ab 1997, der zudem zur Veröffentlichung weiterer Mangas für ein weibliches Publikum führte. Der Verlag Mixx Entertainment hatte daran, unter seinem neuen Namen Tokyopop, großen Anteil und wurde zu einem der größten Manga-Verlage der USA. 2004 kam Del Rey Manga hinzu, die eine Partnerschaft mit Kōdansha eingingen. Als erster Manga-Band in original japanischer Leserichtung in den USA erschien zwar bereits 1989 "Panorama of Hell" bei Blast Books, doch erst Tokyopop brachte ab 2002 Manga-Serien konsequent ungespiegelt heraus. Statt der Spiegelung finden jedoch immer wieder andere Anpassungen statt, wenn die Darstellungen nach amerikanischen Vorstellungen zu gewalthaltig oder zu stark sexualisiert sind. Dies hängt immer wieder auch damit zusammen, dass in den USA auf ein jüngeres Publikum abgezielt wird als in Japan.

Im Jahr 2005 betrug der Umsatz des nordamerikanischen Manga-Marktes etwa 125 bis 145 Millionen Euro, und unter den 100 meistverkauften Comicbänden in den USA waren 80 Manga-Bände. Mangas waren das am stärksten wachsende Segment des amerikanischen Verlagswesens. Ende der 2000er Jahre jedoch gingen die Verkäufe in den USA wieder zurück: Von 2007 bis 2009 schrumpfte die Zahl der Verkäufe um 30 bis 40 %. Das wird als Konsolidierung eines überstrapazierten Marktes und Folge eines Überangebots gesehen – das Medium Manga selbst blieb beliebt und verschwand dadurch nicht, die Fanszene ist seither eher noch gewachsen.

Als erster Manga in Europa erschien "Bushido Muzanden" von 1969 bis 1971 in Fortsetzungskapiteln in einem französischen Kampfsportmagazin. Ab 1978 erschien mit dem französischsprachigen Magazin "Le Cri Qui Tue" aus der Schweiz das erste europäische Manga-Magazin. Der erste auf Spanisch veröffentlichte Manga war die Gekiga-Kurzgeschichte "Good-Bye" von Yoshihiro Tatsumi im Jahr 1980 in Ausgabe Nr. 5 der Underground-Comiczeitschrift „El Víbora“. Die Zeitschrift veröffentlichte im Laufe der nächsten Jahre weitere Geschichten dieses Zeichners. Als erste Serie auf Spanisch erschien ab 1984 "Candy Candy Corazón", die auch in Italien mit großem Erfolg veröffentlicht wurde. Es folgten Science-Fiction-Serien von Go Nagai und Leiji Matsumoto in Frankreich, Spanien und Italien. Generell waren es der italienische und der spanische Comicmarkt, die sich in Europa als erste in größerem Maße dem Manga öffneten. Den ersten Erfolg hatten Mangas in Spanien, wo sie schnell dem Erfolg von Anime-Fernsehserien in den 1980er Jahren folgten. In rascher Folge erschienen vor allem Shōnen und Seinen-Serien wie "Crying Freeman" und "City Hunter". Mehrere neue Verlage und kurzlebige Fanmagazine entstanden. Heute wird der Markt von drei großen sowie mehreren kleinen, stark spezialisierten Verlagen bedient. Auch in Italien nahm der Vertrieb von Mangas im Laufe der 1990er Fahrt auf. Es erschienen die Magazine "Mangazine" und "Zero", die mehrere Serien herausbrachten, später folgten weitere Magazine. Auch hier dominierten nun Mangas für das männliche Publikum und neue, spezialisierte Verlage entstanden. Ende des Jahrzehnts kam die Veröffentlichung im japanischen Taschenbuchformat auf sowie eine größere inhaltliche Vielfalt. Italien war der größte europäische Absatzmarkt für Manga, ehe es etwa 2000 von Frankreich abgelöst wurde. 10 bis 13 Verlage veröffentlichen Mangas in Italien und setzten bei Bestsellern bis zu 150.000 Exemplare "(Dragon Ball)" oder 75.000 Exemplare ("Inu Yasha", "One Piece") pro Band ab.

1990 begann Glénat mit der französischsprachigen Veröffentlichung von "Akira". Doch in den folgenden Jahren hielten sich die großen Verlage noch zurück, da es gesellschaftlich und in der Branche noch Vorbehalte gegenüber dem als gewalthaltig geltenden Manga gab. Das änderte sich ab 1993 langsam, als die Nachfrage in Folge der schon mehrere Jahre im französischen Fernsehen laufenden Anime-Serien immer größer wurde. Édition Tonkam und Pika brachten ab 1994 und 1995 als spezialisierte Verlage eine größere Vielfalt in das französische Manga-Angebot. Versuche, Manga-Magazine nach japanischem Vorbild zu etablieren, scheiterten nach kurzer Zeit. Dennoch stieg der Manga-Anteil am französischen Comicmarkt von 10 % im Jahr 2001 auf 37 % im Jahr 2008 und das Land wurde zum größten Markt für Manga in Europa mit zeitweise 37 Manga-Verlagen. Der Bestseller "Naruto" verkaufte 220.000 Exemplare pro Band. Auch der Markt im benachbarten Belgien ist seit den 1990er Jahren stetig gewachsen. Hier werden, ähnlich wie in Frankreich, auch sehr viele anspruchsvollere Titel für ein erwachsenes Publikum, beispielsweise von Jirō Taniguchi, verlegt. Seit 2003 haben Mangas im frankobelgischen Raum so viel Akzeptanz, dass sie regelmäßig mit Comicpreisen ausgezeichnet werden.

In allen westeuropäischen Ländern ging der Erfolg von Manga einher mit Krisen der nationalen Comicmärkte in den 1980er und 1990er Jahren, ausgelöst durch das aufkommende Privatfernsehen oder inhaltliche und verlegerische Stagnation. Der Manga-Markt in Großbritannien entwickelte sich später als in den meisten übrigen europäischen Ländern. Während im Jahr 2001 etwa 100.000 Manga-Bände mit einem Gesamtumsatz von umgerechnet 2 Millionen Euro verkauft wurden, waren es im Jahr 2005 knapp 600.000 Bände mit einem Gesamtumsatz von umgerechnet 7,6 Millionen Euro. Die meisten Mangas in Großbritannien werden nach wie vor aus den USA eingeführt, der erste britische Manga-Verlag wurde im August 2005 gegründet, der zweite 2006. Jedoch gab es schon seit 1991 mit Manga Entertainment einen Anime-Vertrieb im Vereinigten Königreich, der auch Mangas veröffentlichte.

Nach Russland kamen Mangas bereits in den 1980er Jahren durch sowjetische Diplomaten und es entstand eine kleine Fanszene, die sich über oft illegale Wege Kopien japanischer Comics beschaffte. Erst 2005 erschien beim Verlag Sakura Press mit "Ranma ½" der erste lizenzierte Manga in Russland. Im gleichen Jahr kamen erste Mangas in Polen heraus, die dort großer gesellschaftlicher Kritik ausgesetzt waren. 2010 machte das Segment 70 % des polnischen Comicmarkts aus und wird von zwei polnischen Verlagen und Egmont bedient. In den Ländern Nordeuropas entstanden vereinzelt eigene Verlage, die sich jedoch nicht alle lange halten konnten. Die Märkte werden daher auch von internationalen Verlagen und deren lokalen Töchtern bedient.

Ab 2000 nahm der künstlerische Einfluss des Mangas auf die europäischen Zeichner zu. Frederic Boilet, der bereits mit japanischen Zeichnern gearbeitet hatte, proklamierte 2001 die Bewegung des "Nouvelle Manga". Der Austausch mit den japanischen Künstlern sollte verstärkt werden, von den japanischen Erzähltechniken und der Vielfalt der Inhalte und Zielgruppen gelernt und die Unterschiede zwischen den nationalen Comic-Traditionen beseitigt werden. Viele französische Künstler ließen seitdem Manga-Stilmittel in ihre Arbeiten einfließen. Es entstand der Begriff "Euromanga" für diese Werke.

Der Begriff „Manga“ als Name für die Werke Hokusais wurde in der deutschsprachigen Kunstliteratur bereits seit Ende des 19. Jahrhunderts verwendet, meist in der heute veralteten Schreibweise „Mangwa“. Die erste und zugleich negativ gefärbte Verwendung des Wortes „Manga“ für japanische Comics in deutschsprachigen Medien findet sich in einer Sonderbeilage der Zeitschrift "stern" von 1977: "„(…) Höchste Auflagen haben die „Mangas“, Strip-Magazine mit Sadismen, bei deren Anblick vermutlich gar der alte Marquis de Sade noch Neues hätte lernen können“." Die ersten Manga-Veröffentlichungen in Deutschland, zuerst 1982 "Barfuß durch Hiroshima – Eine Bildergeschichte gegen den Krieg" von Keiji Nakazawa im Rowohlt Verlag, hatten wenig bis keinen Erfolg, obwohl sie hinsichtlich Leserichtung, Veröffentlichungsform und teils auch Farbe an westliche Lesegewohnheiten angepasst waren. So wurde "Akira" 1991 zwar gespiegelt und koloriert in Alben veröffentlicht, wurde aber nicht mehr als ein Achtungserfolg.

Der Durchbruch für Manga in Deutschland kam Ende 1996 mit der ungespiegelten Serie "Dragonball" des Carlsen-Verlags. Die Veröffentlichung in originaler Leserichtung war vom Lizenzgeber vorgegeben worden, erwies sich aber als vorteilhaft und wurde zum Standard für Manga-Veröffentlichungen in Deutschland: Die japanische Leserichtung betont Authentizität, grenzt zum restlichen Comic-Angebot ab und senkt die Kosten für die Verlage, die dies in Form geringerer Preise an den Leser weitergeben. Den meisten Bänden ist seitdem auf der letzten bzw. der nach westlicher Leserichtung ersten Seite eine kurze Anleitung zum Lesen von rechts nach links beigeheftet. Mittlerweile erscheinen allein bei den größten deutschen Manga-Verlagen Carlsen Manga, Egmont Manga, Tokyopop, Planet Manga und Kazé jährlich über 800 Manga-Bände.

Die Entwicklung des Manga-Booms in Deutschland lässt sich zum Beispiel an den Umsatzzahlen des Carlsen-Verlags ablesen: Während der Verlag 1995 Manga für knapp 400.000 Euro verkaufte, lag sein Manga-Umsatz im Jahr 2000 bei über vier Millionen Euro und 2002 bei über 16 Millionen Euro. Im Jahr 2005 lag der Manga-Bruttoumsatz in Deutschland bei 70 Millionen Euro. Egmont Manga und Anime (EMA) war mit einem Jahresumsatz von 15 Millionen Euro Marktführer, im Jahr 2006 lag laut GfK-Angaben Carlsen Comics mit einem Marktanteil von 41 % knapp vor EMA (38 %). Das Segment umfasste in dem Jahr etwa 70 % des deutschen Comicmarktes und wurde zum drittwichtigsten Markt für Manga in Europa. Dabei lief der Vertrieb ab den 2000er Jahren nicht nur über den Fach- und Zeitschriftenhandel, sondern auch über die meisten Buchläden. Das Angebot wird oft in der Nähe von Jugendbüchern platziert und der Erfolg verhalf auch anderen speziell japanischen Erzählformen wie Light Novel nach Deutschland. Die positive Entwicklung des Marktes hält auch über das Jahr 2010 hin an. So stiegen die Umsätze mit Manga von 2014 zu 2015 um fast 15 %, während der Buchmarkt insgesamt schrumpfte. Die deutsche Comicbranche ist wie keine andere in Westeuropa abhängig vom Manga.

Die Veröffentlichungen geschehen zwar fast immer ungespiegelt, jedoch manchmal auf andere Weise verändert oder es wird bereits verändertes Material aus den USA übernommen, um anderen Vorstellungen über die Darstellbarkeit von Sex und Gewalt entgegen zu kommen oder Kritik vorzubeugen. So werden meist auch Swastika, die in Ostasien ein verbreitetes Glückssymbol sind, entfernt, weil sie in Deutschland mit dem Nationalsozialismus assoziiert werden. Mangas werden in Deutschland fast ausschließlich in Form von Taschenbüchern (meist im japanischen Tankōbon-Format) veröffentlicht. Der Versuch, auch monatlich erscheinende Manga-Magazine nach japanischem Vorbild in Deutschland zu etablieren, scheiterte Anfang des 21. Jahrhunderts nach einigen Jahren: Die Magazine "Manga Power" und "Manga Twister" wurden wegen unzureichender Verkaufszahlen und "Banzai!" wegen Lizenzproblemen wieder eingestellt. Das Magazin "Daisuki" hielt sich noch bis Mai 2012.

Die Manga-Leserschaft war laut einer großangelegten Umfrage von 2005 im Wesentlichen zwischen 14 und 25 Jahren alt, nur 12 % älter als 25. Dieser kleine Teil älterer Fans spielte jedoch eine große Rolle in der Etablierung der Szene, so in der Gründung von Magazinen, Veranstaltungen und Plattformen. 70 % der Befragten waren weiblich. Frauen machen auch die überwiegende Mehrheit des selbst kreativen Teils der Fanszene aus. Die thematischen und ästhetischen Interessen sind außerordentlich breit gestreut, auch wenn fantastische Stoffe vorherrschen, und es wird von den Befragten eine große Bandbreite an Lieblingswerken genannt. Allein bei der Frage nach dem ersten gelesenen Manga stechen "Dragonball" und "Sailor Moon" hervor.

Nachdem der Comicmarkt in Deutschland seit den 1980er Jahren rückläufige Verkäufe bei stetig steigenden Preisen verzeichnete, wurde dieser Trend durch den Erfolg von Manga in den 1990er Jahren gebrochen. Die günstige Veröffentlichungsform und die neuen Inhalte sprachen nun wieder ein breiteres, jüngeres und erstmals weiblicheres Publikum an. Seit der Jahrtausendwende haben Manga auf etablierten deutschen Literaturveranstaltungen wie der Frankfurter Buchmesse und der Leipziger Buchmesse eigene Messebereiche. Das Segment wurde – in Verbindung mit Cosplay – zu einem der Publikumsmagneten der Messen und bringt ihnen viele junge Besucher. Beim Sondermann-Preis der Frankfurter Buchmesse gab es zeitweise zwei Kategorien für Manga – national und international – und es entstanden Manga-Zeichenwettbewerbe, die vom Manga inspirierte deutsche Künstler suchen. Seit Anfang der 2000er etablierten sich so auch einige deutsche Künstler, die aus der Manga-Fanszene stammen und Manga-typische Stile, Erzählstrategien, Themen und Genres in ihren Werken aufgreifen.





</doc>
<doc id="3455" url="https://de.wikipedia.org/wiki?curid=3455" title="Mesozoikum">
Mesozoikum

Das Mesozoikum ( ‚mittlerer‘, ‚mitten‘ und ' ‚leben‘), auch Erdmittelalter oder Erdmittelzeit"', ist ein Erdzeitalter, das vor etwa Millionen Jahren begann und vor etwa Millionen Jahren endete. Es wird in Trias, Jura und Kreide gegliedert. Das Mesozoikum ist innerhalb der Erdgeschichte der mittlere Zeitabschnitt des Phanerozoikums. In diesem Äon folgt das Mesozoikum auf das Paläozoikum (Erdaltertum) und wird vom Känozoikum (Erdneuzeit) abgelöst.

Der Begriff wurde von John Phillips 1841 als Bezeichnung für das 'mittlere Tierleben' (middle animal life), das dem des Paläozoikum folgt, definiert. In seine ursprüngliche Definition nimmt er bereits die Kreide, das „Oolithic“ (= Jura) sowie den „New Red sandstone“ auf. Allerdings konnte bereits Murchison 1841 zeigen, dass der untere Teil des New Red sandstone zum Perm gehört, wodurch die noch heutige gebräuchliche Gliederung (s. o.) etabliert wurde.

Stellung des Mesozoikums im Phanerozoikum:

Das Mesozoikum wird in folgende Systeme untergliedert (oben das jüngste, unten das älteste):

Das Mesozoikum begann nach einer ökologischen Katastrophe (Perm-Trias-Grenze) am Ende des Perms (zugleich das Ende des Paläozoikums), deren Ursache noch nicht eindeutig aufgeklärt ist. Bei diesem größten bekannten Massenaussterben der Erdgeschichte starben zwischen 75 % und 90 % aller Tier- und Pflanzenarten aus. Dies ermöglichte die Evolution einer neuartigen Fauna und Flora.

Die Dinosaurier entwickelten sich während der Trias aus den Kriechtieren und sollten die Ökosysteme der Erde bis zum Ende der Kreidezeit dominieren. Aus der Gruppe der Theropoden entwickelten sich die Vögel. Darüber hinaus erschienen die ersten kleinen Säugetiere, Blütenpflanzen und die meisten Bäume, die wir heute kennen.

Hinweise deuten darauf, dass am Ende des Mesozoikums ein Meteorit nahe der Yucatánhalbinsel (Mexiko) einschlug. Dieser sogenannte KT-Impakt wird vielfach für das Aussterben von 50 % aller Tier- und Pflanzenarten verantwortlich gemacht – darunter alle größeren Wirbeltiere (einschließlich der Dinosaurier), viele Pflanzengattungen sowie ein Großteil der Meeresfauna/-flora (Ammoniten, Belemniten).

Während zu Beginn des Mesozoikums alle Kontinente zu einem „Superkontinent“ (Pangaea) vereint waren, nehmen gegen Ende der Kreidezeit die auseinanderdriftenden Kontinente allmählich ihre heutige Konstellation ein.
Durch das sukzessive Aufbrechen des einstigen Superkontinents Pangaea war das Klima im Mesozoikum einer beständigen Veränderung unterworfen.

Am Beginn des Zeitalters finden sich noch riesige Wüsten, bedingt durch die enorme Landfläche Pangaeas – vergleichbar mit der Sahara im heutigen Afrika (siehe hierzu Kontinentalklima).

Mit der Entstehung der Tethys zu Beginn des Mesozoikums wurde das Klima allerdings bald feuchter, die Kontinente begannen sich aufzuteilen.
Das Klima wurde insgesamt tropisch und ähnelte sich weltweit sehr stark, da das Meer noch nicht wie heute in Atlantik, Pazifik und Indischen Ozean unterteilt war und die Meeresströmungen sich gleichmäßiger verteilen konnten. Selbst die Pole waren eisfrei.

Erst mit der Entstehung des Atlantiks und des beginnenden Zerfalls von Pangaea in Kontinente im Jura wurden die weltweiten Meeresströmungen unterbrochen und der weltweite Ausgleich der Klimazonen blieb aus. Dies lässt sich anhand der Jahresringe von versteinertem Holz feststellen. Gegen Ende der Kreide gab es durch den fehlenden globalen Temperaturausgleich erstmals Jahreszeiten mit Kaltzonen im Norden und Süden der Erde.





</doc>
<doc id="3456" url="https://de.wikipedia.org/wiki?curid=3456" title="Moiren">
Moiren

Die Moiren oder Moirai, Sg. Moira () sind in der griechischen Mythologie eine Gruppe von Schicksalsgöttinnen. Ihre Entsprechung in der römischen Mythologie sind die Parzen. Bei den Etruskern stehen die Moiren über den Göttern.

Als Appellativum bedeutet das Wort "moira" einen Teil des Ganzen, wie den Anteil an einer Kriegsbeute. Seit Homer steht es darüber hinaus für das allen Lebewesen von Geburt an zugeteilte Schicksal, das als zwangsläufige Folge der göttlichen Rollenverteilung entsteht. Der Begriff "moira" ist inhaltlich ambivalent, da er zwar in der Regel mit Unheil verbunden wird und euphemistisch für den Tod gebraucht wird, aber auch für das Glück des vom Schicksal Begünstigten stehen kann. Die negative Hauptbedeutung überwiegt nicht nur in der Literatur, sie wird auch aus zahlreichen Grabinschriften vom sechsten vorchristlichen Jahrhundert bis in die Spätantike deutlich.

In der ältesten Literatur, den Epen Homers, kommt die Moira fast ausschließlich in der Einzahl vor, jedoch nicht im Sinne einer einzelnen Göttin, sondern als personifiziertes Schicksal jedes einzelnen Menschen. Die Unterscheidung zwischen dem appellativen Gebrauch von "moira" und des Gebrauchs als Personifikation ist dabei oft nicht möglich. Deutlich als Göttin erkennbar wird sie etwa, wenn sie im Kampf gemeinsam mit Thanatos oder den Keren erscheint. In der "Ilias" erscheint sie als diejenige, die jeden nach Ablauf seiner Lebenszeit dem Ende zuführt, etwa wenn Lykaon sagt, dass sie ihn zum zweiten Mal den Peliden ausgeliefert habe, oder wenn Hektor ihretwegen allein vor den Mauern Trojas bleiben muss und sie ihn heimsucht, als sein Leben beendet wird. Im Kampf führt sie Amphios dem Aias und Tlepolemos dem Sarpedon zu, da ihre Zeit zu sterben gekommen ist. Die Vorstellung, dass sie für die Sterblichen bei der Geburt einen Faden spinnt, in den das Schicksal bereits hineingesponnen wurde, erscheint bei Hektor, als er nach seinem Tod von Hunden angefressen wird, oder bei Achilleus, der, wenn seine Zeit gekommen ist, alles ertragen muss, was ihm das Schicksal zugesponnen hat, jedoch nicht vorher, als seine Zeit nicht gekommen war und Hera ihn noch beschützen kann. In dieser Bedeutung erscheint sie auch, wenn Agamemnon sagt, dass Zeus, Moira und Erinys ihm Verblendung ins Herz gegeben haben. In der Mehrzahl erscheinen die Moiren bei Homer nur ein einziges Mal und das auch ohne Namensnennung. In der "Odyssee" erscheinen die spinnenden Schwestern an einer Stelle als die Kataklothes ().

Das Verhältnis der Götter zu den Moiren scheint darauf hinzudeuten, dass die Götter das von ihnen bestimmte Schicksal nicht abändern können. Zeus will Sarpedon retten, dessen Zeit abgelaufen ist, kann es aber nicht ohne die sonstige Ordnung zu zerstören. Besonders er als oberster Gott kann die bestehende Ordnung nicht stören und wird deshalb auch als Zeus Moiragetes verehrt.

In der nachhomerischen Literatur treten die Moiren meist als Trias auf, ihre Namen sind Klotho (), Lachesis () und Atropos (). Über ihre Abstammung gibt es verschiedene Varianten. In Hesiods "Theogonie" werden die drei Moiren an einer Stelle als Töchter der Nyx, an anderer Stelle als Töchter des Zeus und der Themis und als Schwestern der Horen genannt. Bei den Orphikern sind sie Töchter der Nyx oder der Gaia und des Uranos, Epimenides nennt sie die Töchter von Kronos und Euonyme.

Die Literatur bezeugt die anhaltende Vorstellung der großen Macht der Moiren, die jedoch nie verbindlich ausgestaltet wurde. Da die Moiren als überall anwesend gedacht werden konnten, erscheinen sie auf dem Olymp ebenso wie im Hades, dem Tartaros oder unter den Menschen. Ihre Zuschreibungen variieren zwischen den Extremen als Chthonioi bis zu Olympioi an der Seite des Zeus und sie werden sowohl in die Nähe der Horen als auch der Erinyen und Keren gerückt. In Hesiods "Schild des Herakles" stehen sie auf dem Kampfplatz bereit, wenn die Keren sich auf ihre Opfer stürzen, die Keren erscheinen hier lediglich als Vollstrecker des von den Moiren besiegelten Schicksals. Zur Betonung ihrer Nähe zueinander werden die Erinyen bei den Orphikern als Moiren bezeichnet und in einem Hymnus bei Stobaios werden Klotho und Lachesis angerufen, damit diese die Horen schicken. Bei Pindar erscheinen sie als Göttinnen des Rechts, die bösem Ansinnen fern stehen. Sie führen Zeus die Themis als Gattin zu, unterstützen Herakles dabei, die Olympischen Spiele zu stiften, sind die Göttinnen, die mit Eileithyia bei einer Geburt erscheinen, und werden ganz allgemein als hilfreiche Göttinnen angerufen. Pausanias erwähnt bei einer Bildbeschreibung der Tyche, diese sei bei Pindar eine der Moiren gewesen. Aischylos betont die Verwandtschaft zu den Erinyen, die bei ihm gemeinsam für die Manneskraft und das Glück der Bräute und als Ordnerinnen des Rechts gepriesen werden, auch wenn das Recht von Apollon durchbrochen wird. In "Der gefesselte Prometheus" wird die Macht der Moiren und Erinyen als gleichermaßen für Menschen und Götter bindend dargestellt: Sie führen das Steuerruder der Notwendigkeit, selbst Zeus als derjenige, der das Gesetz des Schicksals regelt, kann dem bereits bestimmten Schicksal nicht entkommen. Bei Euripides werden die Moiren von Apollon überlistet, werden aber auch als die Zeus' Thron am nächsten Sitzenden angerufen, und in der "Bibliotheke des Apollodor" erwirkt Apollon durch Bitten die Verlängerung des Lebens von Admetos. Bei Aristophanes erscheinen sie in der Unterwelt, singen aber auch auf der Hochzeit von Zeus und Hera.

In älteren Kunstwerken finden sich noch keine Attribute der Moiren, später dann das Szepter. In römischer Zeit trägt Klotho eine Spindel, Lachesis ein Losstäbchen oder einen Globus und Atropos Schriftrolle oder -tafel oder eine Sonnenuhr. Die Bibliotheke des Apollodor schildert sie in ihrem Kampf gegen die Giganten Agrios und Thoon als mit Eisenkeulen bewaffnet.

Zur etruskischen Religion gehörte die Vorstellung von der Vergänglichkeit des Menschen und der Völker. So waren den Etruskern acht oder nach anderen Quellen zehn "saecula" der Existenz gegeben. Ein Saeculum reichte dabei bis der letzte, der im vorangehenden Saeculum geboren worden war, starb. Der Tod des Einzelnen konnte durch religiöse Rituale um bis zu zehn Jahre, das Ende des etruskischen Volkes um bis 30 Jahre hinausgezögert werden. Einen ersten Aufschub gewährte der oberste Gott, Tinia, für einen zweiten aber „waren die noch über den Göttern waltenden Moiren zuständig, deren Namen auszusprechen den Etruskern nicht erlaubt war.“




</doc>
<doc id="3457" url="https://de.wikipedia.org/wiki?curid=3457" title="Mettmann">
Mettmann

Mettmann ist die Kreisstadt des Kreises Mettmann im niederbergischen Land. Sie gehört zum Regierungsbezirk Düsseldorf in Nordrhein-Westfalen.

Mettmann liegt am Rande des rheinischen Schiefergebirges an einer nicht immer eindeutig zu ziehenden Grenze zwischen Rheinland und Bergischem Land. Aus politischer Sicht betrachtet gehört die Kreisstadt mit dem gesamten Kreis Mettmann zum Rheinland, nach historischen Maßstäben und heutigem Regionalbewusstsein wird sie eher dem Bergischen Land zugerechnet.
Der niedrigste Punkt im Stadtgebiet mit 75 m ü. NN befindet sich im Neandertal im Bereich des Museums, der höchste Punkt
mit 203 m ü. NN liegt an der Stadtgrenze zu Wülfrath (Kreuzung Meiersberger Str./Mettmanner Str.).
Die Stadt grenzt im Westen an die Landeshauptstadt Düsseldorf. Es folgen im Uhrzeigersinn die Städte Ratingen, Wülfrath, Wuppertal, Haan und Erkrath.

Der Name „Mettmann“ kommt aus dem Altdeutschen, von „Medamana“, was so viel wie „Am mittelsten Bach“ bedeutet. Die erste urkundliche Erwähnung der Siedlung erfolgte im Jahre 904. In einer Urkunde von König Ludwig dem Kind werden dem Kloster Kaiserswerth diversen bereits früher übertragene Kaiserhöfe am Niederrhein bestätigt. Eine der angeführten Höfe ist ein Kaiserhof im Gebiet von Mettmann. Diesen Hof erhielt später (1248) Graf Adolf II. von Berg zusammen mit einem Königshof in Rath vom deutschen König Wilhelm von Holland geschenkt.

In einer weiteren Urkunde von 1072 schenkte König Heinrich IV. der Collegiatskirche in Kaiserswerth diverse „villare“ im westlichen Bereich zwischen Wupper und Ruhr. Einer der angeführten Siedlungen, in dem die Gehöfte lagen, war Mettmann. Im 12. Jahrhundert lag wie später unter den Herzögen von Berg vermutlich bereits ein Gericht der Herrschaft Hardenberg in Mettmann. Zu dieser Zeit hatte das Kloster Kaiserswerth unverändert zehntpflichtiges Eigentum in Mettmann, da Erzbischof Adolf von Köln dies dem Kloster erneut geschenkt hatte.

Im Jahre 1363 war Mettmann Sitz der Amtsverwaltung eines von acht Ämtern im Bergischen Land, es sind die acht Ämter, die in der Urkunde zur Übertragung des Landes Blankenberg an Berg genannt sind. Spätestens im 13./14. Jahrhundert erhielt der Ort eine eigene Stadtmauer, die bis in die Zeit des Dreißigjährigen Krieges bestand. Im August 1424 erhob Herzog Adolf VII. Mettmann mit der zugehörigen Honschaft zu einer Freiheit. Dies war durch die neu erworbenen Rechten mit erheblichen Steuer- und Marktprivilegien verbunden, die dem Ort zu wirtschaftlicher Blüte verhalfen. 

Aus einer Steueraufstellung von 1624 ist ersichtlich, dass das Amt Mettmann zu dieser Zeit relativ wohlhabend war. Das Steueraufkommen betrug 827 Reichsthaler/Jahr. Hierin enthalten waren 80 Reichsthaler von der „Freiheit Mettmann“. Zum Vergleich: das Steueraufkommen der bergischen Metropole Düsseldorf lag bei nur 348 Reichsthaler.

Bereits zu Beginn des Hochmittelalters liegen Informationen über das Christentum im Bereich Mettmann vor. Das sich gebildete Kirchspiel Mettmann gehörte zu dieser Zeit zum „Decania Nussiensis“ (Neuss). In einer Urkunde von 1198 wird sowohl der Pfarrer als auch eine Kirche in Mettmann angeführt. Zum im 14. Jahrhundert gebildeten bergischen Amt Mettmann gehörten neben dem Kirchspiel Mettmann auch die Kirchspiele Gerresheim mit Hubbelrath, Erkrath mit Eller und Wülfrath.

Bereits Mitte des 16. Jahrhunderts konnten die Protestanten unter dem Kaplan Dietmar Heiß eine reformierte Gemeinde in Mettmann bilden, zu der 1546 der überwiegende Anteil der Bevölkerung gehörte. Auch für die Lutheraner waren Anhänger, wenn auch in deutlich geringerer Anzahl, nachweisbar. 1672 war für diese unter dem Prediger Johann Polus eine kleine Gemeinde in Mettmann vorhanden. 

Ab der 1580er Jahre mussten die Protestanten ihre Gottesdienste in Privathäuser abhalten, da eine zunehmende Unterdrückung erfolgte, die in der Amtszeit des schwachsinnigen Herzogs Johann Wilhelm ihren ersten Höhepunkt erreichte. Um die 1610er Jahre wurde zwar für wenige Jahre die Unterdrückung ausgesetzt verstärkte sich jedoch ab der 1620er Jahre bis zu einem offiziellen Verbot der evangelischen Lehre 1636 unter Pfalzgraf Wolfgang Wilhelm. 
Letzterer war 1613 vom lutherischen zum katholischen Glauben übergetreten und versuchte die Gegenreformation zum katholischen Glauben in seinem Herrschaftsgebieten durchzusetzen. Erschwerend kam in dieser Zeit kriegerische Aktionen hinzu, die auch im Bereich von Mettmann zu Einquartierungen und Plünderungen der am Konflikt beteiligten Truppen führten. 

Trotz der Unterdrückung konnten die Reformierten 1640 ein Gebäude für kirchliche Aktivitäten in Mettmann anmieten. Dieses Gebäude wurde 1646 gekauft, durfte aber nur sehr eingeschränkt benutzt werden. Trotz Ende des Dreißigjährigen Krieges 1648 endete die Unterdrückung für die Protestanten in Herzogtum Berg nicht. Beispielsweise wurde die verarmte protestantische Gemeinde noch Anfang des 18. Jahrhunderts in Mettmann gezwungen für den Wiederaufbau der eingestürzten katholischen Kirche Geld aufzubringen. 

Bereits 1729 hatte die reformierte Gemeinde den Neubau einer Kirche beschlossen. Deren Grundsteinlegung konnte aus Geldmangel erst im April 1774 erfolgen und die Einweihung der neuen Kirche war am 14. Dezember 1780. Fast zu gleicher Zeit am 16. November 1768 hatte auch die deutlich kleinere Gemeinde der Lutheraner ihre neue kleine Kirche eingeweiht. 1824 kam es zu einer Vereinigung der Reformierten und der Lutheraner zur evangelischen Gemeinde Mettmann. Zu diesem Zeitpunkt waren 2868 Personen reformiert und 750 Lutheraner. Nach der Vereinigung wurde 1826 die Kirche der Lutheraner verkauft. Zuerst wurde sie zum Rathaus und danach zum Amtsgericht umgebaut.

1806 kam das Herzogtum Berg und mit ihm auch das Amt Mettmann unter Napoleon zum Großherzogtum Berg. Mettmann war eines der sechs Kantone im Arrondissement Düsseldorf. Im Amt Mettmann lebten zu dieser Zeit laut einer Volkszählung von 1804 insgesamt 9484 Personen. 4664 waren davon Katholiken, 4065 Reformierte, 716 Lutheraner und elf Juden und achtundzwanzig Sonstige. Von den größeren Höfen waren neun Rittergüter.

Nach Ende der Franzosenzeit in Deutschland wurde ab 1815 das Großherzogtum und damit Mettmann ein Teil von Preußen. Der Kreis Mettmann, bestehend aus den fünf Bürgermeistereien Haan, Hardenberg, Mettmann, Velbert und Wülfrath, entstand.

Seit 1846 bildete die Bürgermeisterei Mettmann eine Gemeinde gemäß der "Gemeinde-Ordnung für die Rheinprovinz" vom 23. Juli 1845. Am 23. Oktober 1856 wurde der Gemeinde Mettmann von König Friedrich-Wilhelm IV. von Preußen die Rheinische Städteordnung und damit das Stadtrecht verliehen.

Nach der Machtübertragung an die Nationalsozialisten diente die Villa Koburg der SA als Sammelstelle und Folterkeller für Verhaftete aus dem Kreis Mettmann. Zum Gedenken an die vielen getöteten und misshandelten Opfer befindet sich am zentralen Lavalplatz in Mettmann ein Denkmal.

Am 29. März 1943 stürzte ein Wellington-Bomber der "Royal Air Force" in Metzkausen ab. Er hatte die Kennung X3814 und war in dieser Nacht an einem Angriff auf Bochum beteiligt. Die fünf Besatzungsmitglieder, vier Kanadier und ein Engländer, kamen dabei ums Leben. Ehrenamtliche Mitarbeiter des LVR-Amtes für Bodendenkmalpflege fanden Teile des Bombers im Bereich des Golfplatzes.

Acht Monate später, am 3. Dezember 1943, stürzte ein britischer Halifax-Bomber mit dem Kennzeichen HR876 nach Flakbeschuss am Gut Benninghof ab. Der Bomber befand sich auf dem Rückflug von einem Angriff auf Berlin. Die siebenköpfige Besatzung, die aus zwei Kanadiern und fünf Briten bestand, wurde auf dem Düsseldorfer Nordfriedhof beerdigt und später umgebettet. Die letzte Ruhestätte fanden sie auf dem Ehrenfriedhof im Klever Reichswald. Im Jahre 2014 haben ehrenamtliche Mitarbeiter des LVR-Amtes für Bodendenkmalpflege Teile des Bombers geborgen.

Im Laufe der Jahre wechselte der Sitz des Landkreises mehrfach. Neben den Städten Elberfeld und Vohwinkel (heute Teile Wuppertals) war vor allem Düsseldorf lange Zeit Sitz der Kreisverwaltung. Nachdem Mettmann den Zweiten Weltkrieg weitgehend ohne gravierende sichtbare Spuren überstanden hatte, ging der Sitz der Kreisverwaltung 1954 an die Stadt zurück.

In dem an die Stadt Wülfrath angrenzenden Ortsteil Obschwarzbach haben sich nach 1945 bevorzugt Flüchtlinge und Spätaussiedler eine neue Heimat geschaffen.

2004 konnte Mettmann sein 1100-jähriges Bestehen feiern und gehört so zu den ältesten Gemeinden im Bergischen Land.

Zur Stadt Mettmann gehört seit dem 1. Januar 1975 Metzkausen, das bis dahin als Sitz der Amtsverwaltung zum Amt Hubbelrath gehörte, das im Zuge der kommunalen Neuordnung zu diesem Tag aufgelöst wurde.

Nach der Kommunalwahl am 25. Mai 2014 setzt sich der Rat der Stadt wie folgt zusammen:

Der Bürgermeister wird seit 2009 für eine Amtszeit von sechs Jahren gewählt.

Partnerstadt:

Zusammenarbeit:

Patenschaft:

Freundschaften:

Namenspatenschaft:

Die Kreisstadt Mettmann führt ein Stadtwappen, eine Flagge und ein Dienstsiegel sowie ein Signet (Logo). 
Blasonierung: „In Blau zwei durch einen Torbau mit goldenem (gelbem) Tor verbundene goldene (gelbe) Kirchen, die rechte mit Spitzdach, die linke mit Doppelzwiebel, auf den mit schwarzen Wetterhähnen bekrönten Türmen; dazwischen schwebend eine goldene (gelbe) Krone, ein waagerechtes goldenes (gelbes) Zepter und eine goldene (gelbe) Zunftlade.“

Im Gegensatz zum Kreiswappen und den Wappen der meisten Städte im bergischen Land fehlt der bergische Löwe im Wappen der Kreisstadt. Stattdessen zeigt sie die durch ein Stadttor miteinander verbundene Evangelische Kirche und die Katholische Lambertuskirche sowie eine Krone, ein Zepter und eine Zunftlade zwischen beiden Kirchtürmen. Das Wappen entstammt einem Zunftabzeichen der "Fabricanten und Handwerker der Freiheit Mettmann" von 1778. Es wurde Stadtwappen vom 19. Jahrhundert bis 1938 und erneut seit 1946. 1966 wurde es geringfügig in die heutige Form verändert.

„Die Farben der Kreisstadt sind blauweiß.“
Beschreibung der Flagge: „Die Stadtflagge führt die Farben blau-weiß-blau quergestreift, im Verhältnis 1:5:1 mit dem Stadtwappen in der Mitte des weißen Feldes.“

Mettmann ist Sitz des für den Kreis Mettmann sendenden Lokalradios Radio Neandertal. Bekanntheit erlangte Mettmann auch durch den im Januar 2004 erschienenen Kinofilm "Samba in Mettmann" von und mit Hape Kerkeling. Das 1907 in der Düsseldorfer Straße eröffnete „Weltspiegel-Kinocenter“ zählt heute zu den ältesten Lichtspielhäusern Deutschlands. Es wurde bis 2016 in der dritten Generation geführt.

Neben der Stadthalle, in der auch die Stadtbibliothek untergebracht ist und in der in losen Abständen Veranstaltungen stattfinden, ist für das Kulturangebot von Bedeutung, dass Mettmann zum Einzugsbereich von fünf Großstädten (Düsseldorf, Köln, Duisburg, Essen und Wuppertal) gehört.

Ein seit 1861 bestehender Verein ist der Gesellschaft Verein zu Mettmann, eine klassische Sozietät.


Das Neandertal (auch: Neanderthal) gehört teilweise zu Mettmann. Es ist nach dem evangelischen Liederdichter Joachim Neander, der dort seine Spaziergänge zu machen pflegte, benannt. Hier wurden beim Kalksteinabbau die Überreste des nach seinem Fundort Neandertal 1 benannten Neandertalers gefunden. Heute ist das Neandertal ein beliebter Ausflugsort; das Neanderthal-Museum an der Stadtgrenze zu Erkrath hat den berühmten Fund und die menschliche Urgeschichte zum Thema.


Bis 2012 war in der Mettmanner Innenstadt eine historische Straßenbahn der Kreis Mettmanner Straßenbahn zur Besichtigung aufgestellt. Ein weiteres Beispiel für eine Wandlung in der Stadt ist der Jubiläumsplatz im Stadtzentrum von Mettmann, der vor allem in den vergangenen Jahrzehnten vielen Veränderungen unterlag. Hier wurden ab Mitte der 1960er Jahre viele historische Fachwerkhäuser durch damals moderne Bauten (zum Beispiel das Gebäude der Kreissparkasse) ersetzt. Die letzte Änderung kam Anfang dieses Jahrhunderts im Vorlauf der Feierlichkeiten zum 1100-jährigen Bestehen der Stadt. Der Jubiläumsplatz bekam einen neuen Bodenbelag, weg von den Platten und wieder hin zum Kopfsteinpflaster. Das Kunstwerk „Erdstrahlen“ wurde 2006 aufgestellt.

Im August 2005 wurde am westlichen Ortseingang von Mettmann mit der Kreispolizeibehörde eines der modernsten Bürogebäude in Nordrhein-Westfalen fertiggestellt.


Bereits im 19. Jahrhundert etablierte sich Mettmann als Standort für die Besteckindustrie. Vor allem Besteck der 1895 gegründeten Firma "Seibel" konnte sich über die Grenzen Deutschlands einen Namen machen. Heute führt die Firma "Seibel Designpartner GmbH", ansässig im Gewerbegebiet Mettmann Ost, die Tradition fort und stellt dort unter anderem die Besteckserie mono-a her.

Die Georg Fischer GmbH produziert am Standort Mettmann seit 1907 Gussteile aus Kugelgraphitguss (Sphäroguss) für Personenwagen und Nutzfahrzeuge. Sie ist damit die letzte metallverarbeitende Zulieferfirma für verschiedene Automobilhersteller, die vor Ort tätig ist. GF beschäftigt rund 1000 Mitarbeiter und hat zurzeit eine Versandmenge von ca. 190.000 Tonnen pro Jahr. Die Firma "Meckenstock", die sich ebenfalls auf Zieh-, Press- und Stanzteilproduktion spezialisiert hatte, beschäftigte bis zu Beginn des 21. Jahrhunderts noch 250 Mitarbeiter. Im Jahr 2000 wurde Insolvenz angemeldet. Die Gebäude unweit der Regiobahn-Haltestelle Mettmann-Stadtwald stehen seitdem leer.

Weitere bekannte Firmen mit Sitz in Mettmann sind der Tiefkühlkost-Hersteller und -Vertrieb Eismann Tiefkühl-Heimservice GmbH sowie das besonders durch seine Schuhpflegeprodukte bekannt gewordene Unternehmen "Centralin". Mettmann ist auch Sitz des Verkehrsunternehmens Regiobahn GmbH, das die hier endende S-Bahn-Linie (S 28) betreibt.

Mit dem im Jahr 2006 eingerichteten, etwa 10 Hektar großen "Innovationspark Mettmann West" versucht die Stadt höherwertiges Gewerbe, bevorzugt aus dem Technologiesektor, anzusiedeln. Als erstes Unternehmen wechselte im Jahr 2006 die Firma "Bode Timm & Partner GmbH", ein Vermarkter von Anzeigenblättern, von Düsseldorf dorthin.
Dort befindet sich unter anderem seit 2008 die "MAG-Autowelt" (Opel, Honda, Chevrolet).

Am 14. März 2013 wurde auf dem Gelände des früheren Karstadt- bzw. Hertie-Komplexes ein Einkaufszentrum unter dem Namen "Königshof-Galerie" eröffnet.

Durch Mettmann führt die Bahnstrecke Düsseldorf-Derendorf–Dortmund Süd. In Mettmann Stadtwald endet die Regiobahn-Linie S 28 (Mettmann–Erkrath–Düsseldorf–Neuss-Kaarst), welche werktags alle 20 Minuten und am Wochenende alle 30 Minuten verkehrt. Sie bedient drei Haltestellen in Mettmann: Mettmann Stadtwald, Mettmann Zentrum und Neanderthal. Die Wiederbelebung der alten rheinischen Strecke in den neunziger Jahren wurde maßgeblich vom ehemaligen Stadtrat Paul-Heinz Schuh initiiert. Die Linie wird gegenwärtig über die bestehende Strecke zum Bahnhof Dornap-Hahnenfurth und weiter über eine neu zu bauende Abzweigung nach Wuppertal-Vohwinkel ausgebaut werden. Des Weiteren ist eine Verlängerung der Linie im Westen über Viersen nach Venlo im Gespräch.

Von 1930 bis 1952 verkehrte zwischen Mettmann und Gruiten der Fahrdrahtbus Mettmann–Gruiten, dies war der erste neuzeitliche Oberleitungsbus Deutschlands. Er wurde von Gustav Kemmann entwickelt. Kemmann stammte aus Mettmann und suchte die anspruchsvolle Strecke im Bergischen Land als Teststrecke aus.

Zwischen dem 19. Juli 1909 und dem 17. Mai 1952 verkehrten in Mettmann zudem die Straßenbahnen der Kreis Mettmanner Straßenbahn GmbH.

Der übrige öffentliche Nahverkehr in Mettmann wird durch die Rheinbahn betrieben. Sie betreibt dort 13 Buslinien.

Mit dem Auto ist Mettmann über die A 3 (Anschlussstelle Mettmann), A 46/A 535 (Anschlussstelle Wuppertal-Dornap), A 44 (Anschlussstelle Ratingen-Schwarzbach) und die B 7 (aus Richtung Düsseldorf/Wuppertal) erreichbar. Der Düsseldorfer Flughafen ist 20 Kilometer entfernt, der Flughafen Köln/Bonn 50 Kilometer.

Mettmann besitzt mit dem evangelischen Krankenhaus in der Gartenstraße ein modernes Krankenhaus mit 237 Betten, das in den kommenden Jahren weiter ausgebaut werden soll. Die Gründung dieses Hauses geht auf das Jahr 1877 zurück.

Für seine Einwohner hält Mettmann eine große Anzahl verschiedener Schulformen sowie eine eigene Musikschule, eine Volkshochschule, eine Stadtbibliothek, sowie ein umfangreiches Stadtarchiv vor. Seit Oktober 2009 ist Mettmann mit der Eröffnung der FHDW (Fachhochschule der Wirtschaft) Hochschulstandort.

Grundschulen

Sekundarstufe I

Sekundarstufe II

Förderschulen

Berufsschulen

Volkshochschule

Hochschule

Ralph Roese wurde in den Jahren 1931 und 1932 Deutscher Motorrad-Straßenmeister in der Klasse bis 1000 cm³. Anschließend wechselte er zum Automobilrennsport und errang 1938 einen beachtenswerten dritten Platz beim 24-Stunden-Rennen von Spa-Francorchamps. 1939 wurde er deutscher Sportwagenmeister in der Klasse bis 1,5 Liter Hubraum. Der dritte Gesamtrang bei der Mille Miglia 1940 gilt als sein international größter Erfolg. Alle Rennen bestritt er als Werks- und Privatfahrer auf Fahrzeugen von BMW.

Der Verein "Mettmanner TV" spielte von 1969 bis 1974 fünf Jahre lang in der Tischtennis-Bundesliga. 1971/72 wurde er Deutscher Meister.

Der Hockey- und Tennisclub Mettmanner THC (früher THC Mettmann) ist 2011 entstanden aus der Fusion der beiden Clubs THC Mettmann und MTC 80.
Das Hockeyteam spielte in der Oberliga (damals höchste Spielklasse), das Tennisteam in der Regionalliga (damals zweite Liga). Heute spielt die 1. Herren-Mannschaft im Hockey in der 1. Verbandsliga, auf dem Feld wie auch in der Halle. Das Damen-Team spielt in der 2. Verbandsliga. Eine 1. Herren- und 1. Damen-Mannschaft im Tennis existiert zurzeit nicht, dafür aber gibt es eine Vielzahl von Mannschaften in verschiedenen Altersgruppen. Der MTHC verfügt über 12 Außenplätze, sowie 4 Hallenplätze. Als Clubhaus dient das erweiterte Clubhaus des früheren MTC 80.
Der zurzeit mitgliederstärkste Sportverein (über 5000 Mitglieder) ist "Mettmann-Sport e. V." Er wurde am 17. Januar 2005 in das Vereinsregister aufgenommen und trat die Nachfolge dreier alteingesessener Mettmanner Vereine (MTV, MSC, TSV Metzkausen) an. Das Herrenteam der Handball-Abteilung spielt derzeit in der Oberliga, die Damen in der Regionalliga.

2011 hat die Stadt Mettmann an der Hasseler Straße in Metzkausen bestehende Sportplätze, die für Fußball und Leichtathletik genutzt wurden, zum zentralen Sportzentrum „Am Pfennig“ zusammengefasst. Grund dieser Maßnahme war die Umwidmung des Sportgeländes „Am Stadtwald“ zu einem Wohngebiet. Die Sportanlage des Heinrich-Heine-Gymnasiums wurde saniert, die bestehende Tennisanlage des fusionierten Tennis- und Hockeyclubs erweitert, zwei Fußballspielfelder sowie Trainingsplätze neu angelegt und ein Funktionsgebäude errichtet.

2004 wurde das Mettmanner Freibad zu einem Naturfreibad mit Strandbereichen, Strandkörben, Matschspielplatz und Wasserrutsche umgebaut.

Am 2. Juli 2017 hat die 2. Etappe der Tour de France (Radsport) auf ihrem Weg von Düsseldorf nach Lüttich unter anderem durch Mettmann geführt.

Das Klima in Mettmann ist gemäßigt, aber warm. Durch die Lage zwischen Rheinland und Bergischem Land sind die Winter relativ mild und die Sommer mäßig warm. Der wärmste Monat ist mit 18,0 °C Durchschnittstemperatur der Juli, am kältesten ist es mit 1,5 °C im Januar. Die Jahresdurchschnittstemperatur beträgt 9,7 °C. Die Niederschläge verteilen sich gleichmäßig über das ganze Jahr mit einem Maximum im Juni mit 88 und einem Minimum im Februar mit 55 Millimetern. Pro Jahr fallen im Durchschnitt 856 Millimeter Niederschlag.






</doc>
<doc id="3458" url="https://de.wikipedia.org/wiki?curid=3458" title="Menge">
Menge

Menge (von mittelhochdeutsch "manic" „viel“, vgl. "mannigfach") steht für:

Menge ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="3460" url="https://de.wikipedia.org/wiki?curid=3460" title="Max Born">
Max Born

Max Born (* 11. Dezember 1882 in Breslau; † 5. Januar 1970 in Göttingen) war ein deutsch-britischer Mathematiker und Physiker. Für grundlegende Beiträge zur Quantenmechanik wurde er 1954 mit dem Nobelpreis für Physik ausgezeichnet.

Max Born wuchs in Breslau auf. Er stammte aus einer großbürgerlichen, deutsch-jüdischen Familie. Sein Vater Gustav Born (1851–1900) war Professor für Anatomie und Embryologie an der Universität Breslau, sein Großvater Marcus Born der erste jüdische Distriktarzt in Preußen. Seine Mutter Margarete Kaufmann kam aus einer Familie der Textilindustrie in Breslau und war musikalisch interessiert, sie starb aber, als Born vier Jahre alt war. 1890 heiratete der Vater erneut. Das Elternhaus war kulturell aufgeschlossen und zu den Freunden seines Vaters zählten Albert Neisser und Paul Ehrlich. Born hatte noch eine jüngere Schwester Käthe (* 1883) und einen Halbbruder Wolfgang aus der zweiten Ehe seines Vaters mit Bertha Lipstein, der später Professor für Kunstgeschichte am City College of New York wurde. Als Student war er häufig Gast bei Neisser, bei dem viele bekannte Musiker verkehrten (Ferruccio Busoni, Artur Schnabel, Edwin Fischer, Carl Flesch) und Literaten wie Gerhart Hauptmann.

Nach Besuch des humanistischen König-Wilhelm-Gymnasiums studierte Max Born ab 1901 in Breslau (wo die Mathematiker Otto Toeplitz und Ernst Hellinger Kommilitonen waren), Heidelberg (wo er James Franck kennenlernte und sich mit ihm befreundete), Zürich und Göttingen zuerst Rechtswissenschaften und Moralphilosophie, später Mathematik, Physik und Astronomie. Er promovierte 1906 bei Carl Runge in Göttingen ("Beiträge zur Bestimmung der Lichtbrechungsverhältnisse doppeltbrechender Krystalle durch Prismenbeobachtungen"). Während seines Studiums hatte er den Unwillen des sehr einflussreichen Mathematikers Felix Klein erregt, als er es zunächst ablehnte, seine Arbeit über die elastische Linie als Preisarbeit an der Universität einzureichen (1906 gewann er den Preis). In seiner Doktorprüfung mied er deshalb Klein als Prüfer und wählte Astronomie unter Karl Schwarzschild als Nebenfach, sein anderer Prüfer war Woldemar Voigt. Danach war er physikalischer Assistent (unbezahlter Privatassistent) von David Hilbert, wobei er auch eng mit Minkowski zusammenarbeitete. Seinen Wehrdienst, der durch sein Asthma abgekürzt wurde, leistete er nach seiner Promotion. Ein halbes Jahr später hielt er sich in Cambridge bei Joseph Larmor und J. J. Thomson auf. 1908/09 studierte er bei Otto Lummer und Ernst Pringsheim senior in Breslau Experimentalphysik (da in seinem Praktikumsexperiment ein Kühlschlauch riss, der das Labor unter Wasser setzte, riet ihm Lummer allerdings von einer weiteren Karriere auf diesem Gebiet ab), befasste sich aber auch mit der Relativitätstheorie (und speziell der Theorie starrer Körper in der Relativitätstheorie und Theorie des Elektrons). Er kehrte Ende 1908 zu Hermann Minkowski nach Göttingen zurück, mit dem er von Dezember 1908 bis zu dessen plötzlichen Tod nach einer Blinddarmoperation im Januar 1909 über die Relativitätstheorie zusammenarbeitete.

Er habilitierte sich 1909 in Göttingen ("Untersuchungen über die Stabilität der elastischen Linie in Ebene und Raum, unter verschiedenen Grenzbedingungen"). Er setzte darin seine Preisarbeit fort, wobei er auch eine experimentelle Apparatur zur Überprüfung der Theorie baute.

Nach Minkowskis Tod gab Born dessen physikalische Arbeiten aus dem Nachlass heraus. Ein weiteres Forschungsfeld war die Theorie atomarer Kristallgitter. Hierüber veröffentlichte er 1915 das Buch "Dynamik der Kristallgitter". Mit Theodore von Kármán entwickelte er die Born-von-Karman-Theorie der spezifischen Wärme von Festkörpern.

Bei Beginn des Ersten Weltkrieges teilte er die allgemeine Kriegsbegeisterung, war aber wegen seines Asthmas nicht fronttauglich. 1915 trat er als Funker in den Heeresdienst und wurde Mitglied einer Gruppe von Technikern und Physikern unter der Leitung von Max Wien. In dieser Zeit war er bei der Artillerieprüfungskommission in einer Gruppe unter der Leitung von Rudolf Ladenburg mit Schallortungs-Versuchen befasst. Er bemühte sich in dieser Zeit, auch andere Physiker und Mathematiker vom Fronteinsatz abzuziehen und so über den Krieg zu retten.

Zu seinen Mitarbeitern zählten z. B. Alfred Landé, Erwin Madelung, Fritz Reiche.

Born war nach der Habilitation zunächst ab 1912 Privatdozent in Göttingen, war 1914/1915 außerordentlicher Professor an der Universität Frankfurt und wurde 1915 dann außerordentlicher Professor für theoretische Physik an der Friedrich-Wilhelms-Universität Berlin, wo er mit Max Planck, Albert Einstein und Walther Nernst zusammenarbeitete. 1919 erhielt er seinen ersten Lehrstuhl (ordentlicher Professor) in Frankfurt am Main (wobei er seinen Lehrstuhl mit dem von Max von Laue tauschte, der nach Berlin ging).

Born war von 1921 bis 1933 Professor in Göttingen. Hier entwickelte er unter anderem mit Wolfgang Pauli, Werner Heisenberg, Pascual Jordan und Friedrich Hund große Teile der modernen Quantenmechanik. Nach ihm benannte Verfahren wie die Born-Oppenheimer-Näherung in der Molekülphysik (1928) und die Bornsche Näherung in der Streutheorie erinnern an seine Pionierleistungen.

Er entwickelte die "statistische Interpretation der Wellenfunktion", die später als Kopenhagener Deutung bekannt wurde und für die er 1954 den Nobelpreis für Physik erhielt.

Bereits 1948 wurde ihm die Max-Planck-Medaille verliehen, 1950 die Hughes-Medaille.

Max Born beschäftigte sich auch mit theoretischer Optik, über die er mit Emil Wolf ein heute noch bedeutendes Lehrbuch geschrieben hat.

Mit Albert Einstein verband Born eine lebenslange enge Freundschaft, auch wenn Einstein die Arbeiten Borns zur Quantentheorie skeptisch betrachtete. Sein Briefwechsel mit Einstein, der unter anderem für die Geschichte der Interpretation der Quantenmechanik interessant ist, wurde in Buchform veröffentlicht.

1934 entwickelte er mit Leopold Infeld die Born-Infeld-Theorie, eine relativistische, nichtlineare Verallgemeinerung der Elektrodynamik, die die in der üblichen Elektrodynamik divergierende elektromagnetische Selbstenergie des Elektrons (oder allgemein von Punktladungen) endlich machen sollte. Sie fand später in der Stringtheorie (Eichfelder auf D-Branen) neue Anwendung.

Vor allem Born ist Anfang des 20. Jahrhunderts die Herausbildung einer fruchtbaren Schule theoretischer Physiker in Göttingen zu verdanken, zu der auch viele durchreisende ausländische Physiker kamen. Zu seinen Doktoranden zählen Maria Goeppert-Mayer, Victor Weisskopf, Robert Oppenheimer, Siegfried Flügge, Friedrich Hund, Pascual Jordan, Maurice Pryce (in Cambridge), Herbert S. Green (in Edinburgh).

Im Jahr 1933, nach der Machterlangung der Nationalsozialisten, wurde Max Born wegen seiner jüdischen Vorfahren und seiner pazifistischen Einstellung zwangsbeurlaubt, aufgrund des Berufsbeamtengesetzes der Hitler-Regierung. 1936 wurde ihm auch die deutsche Staatsbürgerschaft entzogen. Er emigrierte nach England (1939 wurde er britischer Staatsbürger) und hatte zunächst ab 1933 eine Dozentur in Cambridge, dann ab 1936 eine Professur an der Universität von Edinburgh, wo er bis zu seiner Rückkehr nach Deutschland 1953 blieb.

1936 wurde Max Born eine Stelle am "Indian Institute of Sciences" in Bangalore angeboten. Sein Gastgeber, C. V. Raman, versuchte, eine dauerhafte Stelle für ihn zu schaffen. Er scheiterte, da die Verwaltung der Meinung war, theoretische Physik sei spekulativ und für die Industrie nutzlos. Nach ca. 6 Monaten verließ Born Indien.

In Großbritannien engagierte sich Born für die Notgemeinschaft deutscher Wissenschaftler im Ausland, um anderen verfolgten Akademikern Stellen zu vermitteln.

Am 28. Juni 1953 wurde er zum Ehrenbürger von Göttingen ernannt, wo man später auch eine Straße nach ihm benannte. Sein Grab befindet sich auf dem Göttinger Stadtfriedhof, obwohl er zuletzt nicht in Göttingen selbst, sondern in dem 68 km entfernten Bad Pyrmont lebte.

Neben seinen physikalischen Untersuchungen hat sich Max Born immer wieder mit Reden zu philosophischen und gesellschaftspolitischen Themen Gehör zu verschaffen versucht. 1957 war er einer von 18 Unterzeichnern des Göttinger Manifests, das sich gegen die geplante atomare Aufrüstung der Bundeswehr wandte.

In diesem Zusammenhang hat er wiederholt auf die wichtige Rolle hingewiesen, die seine Frau Hedwig für die Herausbildung und Überprüfung seiner eigenen Standpunkte spielte. Mit ihr zusammen verfasste er unter anderem das Buch "Der Luxus des Gewissens – Erlebnisse und Einsichten im Atomzeitalter" (1958).

Er war seit 1913 mit Hedwig (Hedi) Martha Ehrenberg (1891–1972) verheiratet, einer Tochter des Juristen Victor Ehrenberg, mit der er drei Kinder hatte: Irene (1914–2003), Gritli und Gustav Victor Rudolf Born, ein bekannter britischer Pharmakologe. Die Sängerin und Schauspielerin Olivia Newton-John ist eine Tochter seiner Tochter Irene. Auch die Sozialwissenschaftlerin und Musikerin Georgina Born ist eine Enkelin Max Borns. Seiner Frau Hedi zuliebe trat er 1914 dem lutherischen Glauben bei.




Im Wintersemester 1962/63 fand anlässlich des achtzigsten Geburtstages von Max Born am Physik-Fachbereich der Universität Göttingen ein Festkolloquium statt, auf dem Werner Heisenberg über seine damals so genannte „Weltformel“ referierte und auch Friedrich Hund anwesend war (beide Assistenten Max Borns in den zwanziger Jahren). Bei der Diskussion nach dem Vortrag sprang Max Born, der in der Mitte der ersten Bank gesessen hatte, wie ein junger Sportler über die Brüstung und malte nach wenigen Worten eigene Formeln an die Tafel. Die Formeln, die mit der sog. Born-Infeld-Theorie aus den dreißiger Jahren zusammenhingen, verstanden damals allerdings die wenigsten Zuhörer, aber das war auch nicht beabsichtigt: „Für die jungen Leute“ wolle er (der Achtzigjährige[!]) nur "einige Anregungen geben".




</doc>
<doc id="3461" url="https://de.wikipedia.org/wiki?curid=3461" title="Silikatverwitterung">
Silikatverwitterung

Die Silikatverwitterung bezeichnet die hydrolytische Zersetzung von Silikaten, also die chemische Reaktion der Mineralbestandteile mit den formula_1- und formula_2-Ionen des dissoziierten Wassers. Besonders der hydrolytischen Zersetzung ausgesetzt sind Verbindungen, die aus einer schwachen Säure und/oder einer schwachen Base bestehen. Dies sind vor allem Carbonate (die zu Calciumhydrogencarbonat dissoziieren) und Silikate – somit ist ein Großteil der gesteinsbildenden Minerale der Hydrolyse unterworfen.

Für die Verwitterung der Silikate soll der Prozess der Hydrolyse am Beispiel des Kalifeldspats erläutert werden, da er mit ca. 20 % Volumenanteil eines der häufigsten Minerale in der Erdkruste darstellt.

Gerät die Mineraloberfläche in Kontakt mit Wasser (als Lösungsmittel), werden die am Rand liegenden Ionen im Kristallgitter hydratisiert (vor allem die formula_3-Ionen), was zu einer Lockerung der Bindung führt und in weiterer Folge die formula_3-Ionen durch die Protonen des Wassers abgespalten werden:

formula_5

Durch die in der Natur vorkommenden Bodenporenlösungen werden die formula_2-Ionen jedoch durch die Protonen der enthaltenen Säuren neutralisiert. Dabei werden die formula_3-Ionen ausgewaschen, adsorbiert oder als wichtiger Bodennährstoff durch die Pflanzen aufgenommen. Schreitet die Anlagerung der Protonen und formula_2-Ionen an das Kristallgitter des Feldspats voran, werden innerhalb der silikattypischen Tetraederverbindungen die formula_9 - und formula_10 -Bindungen aufgelöst.

Schlussendlich wird die Mineralstruktur unter Bildung von Aluminiumhydroxid und Orthokieselsäure, die beide Endprodukte der Silikatverwitterung darstellen, vollständig zerstört:

formula_11

Dieser Schritt der Silikatverwitterung wird Desilifizierung genannt, da der Lösung ein großer Teil des Silikats entzogen wird.

Andere Silikate, wie zum Beispiel Glimmer, Hornblenden oder Olivin, sind im Prinzip genauso von diesen Verwitterungsprozessen betroffen. Aus den ionaren und molekularen Zersetzungsprodukten der hydrolytischen Spaltung können als feste Produkte sekundäre (Ton-)Minerale rekristallisieren (zum Beispiel Kaolinit oder Illit). Dabei können die neugebildeten Minerale das Volumen des aufgelösten Minerals ausfüllen und dadurch seine einstige Gestalt annehmen (Pseudomorphose) oder sich an der Gesteinsoberfläche abscheiden.




</doc>
<doc id="3462" url="https://de.wikipedia.org/wiki?curid=3462" title="Masse (Physik)">
Masse (Physik)

Die Masse, auch Ruhemasse, ist eine Eigenschaft der Materie. In den meisten physikalischen Größensystemen ist sie eine der Basisgrößen. Sie wird gemäß dem internationalen Einheitensystem in der Einheit Kilogramm angegeben. Das Formelzeichen ist meist formula_1. Die Masse ist eine extensive Größe. Jedes physikalische System hat eine Masse. Sie ist unabhängig von der Bewegung gegenüber einem Bezugssystem. Ist die Masse eines in Bewegung befindlichen Systems nicht null, sind die beiden mit der Bewegung verbundenen Erhaltungsgrößen, Impuls und kinetische Energie, zu seiner Masse proportional.

Sowohl die auf einen Körper wirkenden als auch die von ihm verursachten Gravitationskräfte sind proportional zu seiner Masse. Ebenso bestimmt sie die Trägheit, mit der der Bewegungszustand des Körpers auf Kräfte reagiert. Diese doppelte Rolle der Masse ist Inhalt des Äquivalenzprinzips.

Ferner bestimmt die Masse eines Systems dessen Ruheenergie. Aufgrund der Äquivalenz von Masse und Energie unterscheiden sich die beiden Größen Masse und Ruheenergie nur durch den konstanten Faktor Lichtgeschwindigkeit zum Quadrat (formula_2).

Die Masse wird außerhalb der Physik, besonders in der Umgangssprache, auch als "Gewicht" bezeichnet. Dabei sollte beachtet werden, dass dieses Wort auch für die verwandte, aber nicht identische Bedeutung Gewichtskraft stehen kann.

Der physikalische Begriff "Masse" wurde Mitte des 17. Jahrhunderts geprägt, als Johannes Kepler, Galileo Galilei, Isaac Newton, Christiaan Huygens (und andere) mit dem Studium der Bewegungen von Körpern auf der Erde und am Himmel die Grundlagen der modernen Naturwissenschaften legten. Aus den Beobachtungen, wie sich die Geschwindigkeit eines Körpers durch Stoß oder Krafteinwirkung ändert, wurde geschlossen, dass jedem Körper eine unveränderliche Größe zukommt, die seine Trägheit verursacht. Dies entsprach dem älteren philosophischen Begriff "„quantitas materiae“," der die Menge der in einem Körper enthaltenen Materie bezeichnen sollte. Newton definierte diese Größe, indem er von der Dichte und dem Volumen eines Körpers ausging, und bezeichnete sie fortan mit „Masse“. Demnach ließ sich Newton von dem damals gängigen Verständnis leiten, reine Materie existiere in Form von kleinen, gleich beschaffenen Partikeln, die mit äthergefüllten Zwischenräumen jeweils verschiedener Größe die verschiedenen realen Körper bilden. Daraus entwickelte sich schließlich der Massebegriff der klassischen Mechanik. Seine genauen Eigenschaften sind:


Die Eigenschaft Nr. 1 ist ein Teil des zweiten newtonschen Gesetzes und definiert die Bedeutung der physikalischen Größe "Masse" durch ihre Trägheit, allerdings setzt sie die Definition der Größe "Kraft" voraus. Die Eigenschaft Nr. 2 ist Teil des newtonschen Gravitationsgesetzes, das zur Grundlage der genauen Beschreibung der Erdanziehung und der Planetenbewegung wurde. Sie liefert die benötigte Kraftdefinition, indem sie die Gravitationskraft konkret angibt und damit alle weiteren Kräfte durch Vergleich mit der aus der Gravitation folgenden Gewichtskraft zu messbaren Größen macht. Die im Gravitationsgesetz enthaltene Feststellung, dass es die durch Trägheit definierte Masse ist, welche die Gravitation verursacht, wird als "Äquivalenz von träger und schwerer Masse" bezeichnet. Die Eigenschaften Nr. 3 und 4 der Masse ergeben sich in der newtonschen Mechanik als Folgerungen aus der definierenden Eigenschaft Nr. 1. Die Massenerhaltung (Eigenschaft Nr. 5) ist eine Erfahrungstatsache zunächst aus dem Bereich der Mechanik, deren Gültigkeit Ende des 18. Jahrhunderts (vor allem durch Antoine de Lavoisier) auch auf die chemischen Vorgänge ausgeweitet werden konnte. Zusammen entsprechen die drei letztgenannten Eigenschaften genau der Vorstellung von einer unzerstörbaren Substanz, aus der die materielle Welt besteht.

Bis etwa Mitte des 18. Jahrhunderts wurden die wichtigen Erhaltungsgrößen Impuls und kinetische Energie herausgearbeitet, die mit der Masse eines in Bewegung befindlichen Körpers verbunden sind:

Diese beiden Erhaltungssätze für Impuls und Energie sind grundlegend sowohl für die klassische als auch für die moderne Physik und gelten in der gegebenen Formulierung exakt in beiden Bereichen. Auf ihrer Grundlage kann man eine neue Definition der Masse geben, die im Ergebnis mit den fünf oben genannten Eigenschaften übereinstimmt, aber keine von ihnen schon voraussetzt. Man benötigt dazu noch die genaue Festlegung, wie die Beschreibung eines physikalischen Vorgangs abzuändern ist, wenn man in ein bewegtes Bezugssystem wechselt. Es ist kein Rückgriff auf den Kraftbegriff nötig, der nach Ernst Mach, Gustav Kirchhoff, Heinrich Hertz und anderen im 19. Jahrhundert als ungeeignet für einen wissenschaftstheoretisch befriedigenden Grundbegriff kritisiert wurde.

Im Rahmen der klassischen Physik, und damit auch in der Alltagswelt, gelten alle fünf oben genannten Eigenschaften der Masse. In der von Relativitätstheorie und Quantenphysik geprägten modernen Physik gelten sie nur noch näherungsweise.

Hendrik Lorentz entdeckte zu Beginn des 20. Jahrhunderts, dass für elektrodynamische Vorgänge ein Wechsel des Bezugssystems nicht mithilfe der Galilei-Transformation, sondern mittels der Lorentz-Transformation vollzogen werden muss. Albert Einstein erkannte, dass dies für jedes physikalische Phänomen gilt, auch im Bereich der Mechanik. Das lässt den Zusammenhang zwischen der Kraft und der von ihr bewirkten Änderung der Geschwindigkeit weit komplizierter werden als in der klassischen Definition der Masse (Eigenschaft Nr. 1) angenommen. Außerdem folgt, dass ein System bei Änderung seiner inneren Energie (das ist der Energieinhalt, den er in seinem Ruhesystem hat) eine dazu proportionale Änderung seiner Masse erfährt. Die Masse eines zusammengesetzten Körpers hängt also nicht nur von den Massen seiner Bestandteile ab, sondern auch von den kinetischen und potenziellen Energien, die diese haben, wenn der Körper als Ganzes ruht. So verliert ein Körper beim Zusammensetzen aus einzelnen Bestandteilen an Masse, wenn Bindungsenergie frei wird, man spricht vom Massendefekt. Umgekehrt vergrößert sich seine Masse, wenn seine Bestandteile sich heftiger bewegen, wie das etwa bei Erwärmung der Fall ist. Dabei ergeben sich die betreffenden Energiewerte stets so, dass man den Wert der Masse bzw. Massenänderung mit dem Quadrat der Lichtgeschwindigkeit multipliziert. Dieser Umrechnungsfaktor ist eine universelle Konstante. Mithin lassen sich Veränderungen der Masse und der Energie überhaupt nicht voneinander trennen, vielmehr besteht eine allgemeine Äquivalenz von Masse und Energie.

Die Äquivalenz von Masse und Energie gilt immer. Einem in Ruhe befindlichen Körper muss man entsprechend seiner Masse formula_1 eine Ruheenergie formula_4 zuschreiben (Einsteinsche Gleichung). Umgekehrt muss man nach derselben Gleichung einem System immer auch eine Masse zuschreiben, wenn es Ruheenergie besitzt, d. h. wenn es beim Gesamtimpuls null noch Energie hat. Dies bleibt im Alltag meist verborgen, wird aber besonders deutlich bei der gegenseitigen Vernichtung (Annihilation) von zwei massebehafteten Elementarteilchen, wenn man den Prozess in deren Schwerpunktsystem betrachtet, also im Ruhesystem des Zweiteilchensystems. Es entsteht Vernichtungsstrahlung mit einer Energie, die durch die Ruheenergie des verschwundenen Zweiteilchensystems gegeben ist. Sie hat den Gesamtimpuls null, wie vorher das Zweiteilchensystem auch. Diesem Strahlungsfeld muss auch dieselbe Masse zugeschrieben werden wie dem Zweiteilchensystem, denn es lässt sich kein Unterschied feststellen. Auch masselose Objekte (z. B. zwei oder mehr Lichtquanten) können also Systeme bilden, die eine Masse haben.

Die oben angegebenen klassischen Eigenschaften der Masse können daher nur näherungsweise gültig bleiben, nämlich für den klassischen oder nichtrelativistischen Grenzfall, d. h. für massebehaftete Körper mit geringer Geschwindigkeit. Nach den Erfordernissen der Speziellen und der Allgemeinen Relativitätstheorie müssen sie wie folgt umformuliert werden:


Im Endergebnis definiert man ganz allgemein die Masse mittels der Gleichung formula_4 durch die Ruheenergie. Damit ist die Masse eine Lorentzinvariante, so wie die nach Newton definierte Masse eine Galilei-Invariante ist. Daher stimmen beide Definitionen der Masse nicht nur im Wert überein, sondern teilen eine tiefliegende Beziehung, an der aber auch ihr Unterschied deutlich wird: Beide Definitionen der Masse ergeben sich in gleicher Weise allein aus dem Erhaltungssatz für den Impuls, wenn man ihn einmal im Ruhesystem formuliert und ein zweites Mal in einem dagegen bewegten Bezugssystem (s. u.). Vollzieht man den Übergang von einer zur anderen Beschreibung mit der nur näherungsweise richtigen Galilei-Transformation, gelangt man zum klassischen Begriff der Masse, vollzieht man ihn mit der Lorentz-Transformation, gelangt man zum modernen Begriff der Masse.

Die ursprüngliche Bedeutung der Masse als Maß für die Menge der Materie ist nicht mehr aufrechtzuerhalten.

Zu dem genauen Gebrauch der Benennung „Masse“ ist eine historische Anmerkung nötig: Um die klassische Definition der Masse in eine auch relativistisch korrekte zu überführen, wurde zunächst die Masse eines Systems einfach durch seinen gesamten Energiegehalt definiert, d. h. durch die Summe aus Ruheenergie und kinetischer Energie, dividiert durch den erwähnten universellen Faktor formula_2. So ergab sich die sogenannte „relativistische Masse“, die oft aber einfach nur als „Masse“ bezeichnet wurde. Je kleiner die kinetische Energie eines Systems gegenüber seiner Ruheenergie ist, desto besser stimmen beide Definitionen überein. Jedoch ist diese „relativistische Masse“ für ein und dasselbe System je nach dem Bezugssystem, in dem es beobachtet wird, so verschieden wie seine kinetische Energie. Die „relativistische Masse“ eines Systems variiert also mit seiner Geschwindigkeit, ist lediglich ein anderer Ausdruck für seine jeweilige Gesamtenergie und hat keine eigenständige Bedeutung. Nur die zur Ruheenergie gehörende Masse charakterisiert das System als solches; sie erhielt in diesem Zusammenhang den neuen Namen „Ruhemasse“.

Diese Benennungen sind zwar noch häufig anzutreffen, werden aber zunehmend durch die moderne Wortwahl verdrängt, nach der mit "Masse" eine vom Bezugssystem unabhängige Systemeigenschaft bezeichnet werden soll. Danach steht „Masse“ ausschließlich für die zur Ruheenergie gehörende Masse, ist also eine vom Bezugssystem unabhängige Größe und gleichbedeutend mit der früheren „Ruhemasse“.

Einstein selbst begründet den Wechsel zur neuen Wortwahl im Jahre 1948: }</math> eines bewegten Körpers zu sprechen, da für formula_7 keine klare Definition gegeben werden kann. Man beschränkt sich besser auf die „Ruhe-Masse“ formula_1. Daneben kann man ja den Ausdruck für momentum und Energie geben, wenn man das Trägheitsverhalten rasch bewegter Körper angeben will.

Die Herleitung des Massenbegriffs aus der Impulserhaltung beleuchtet sowohl Unterschiede als auch Ähnlichkeiten zwischen der klassischen und der relativistischen Physik. Als Ergebnis der Herleitung sieht man: Wenn jedem Körper einzeln eine zu seiner Geschwindigkeit formula_11 parallele Größe formula_12 zugeordnet werden kann, so dass die Summe dieser beiden Größen bei einem inelastischen Stoß konstant bleibt, dann muss es zu jedem Körper einen vom Bezugssystem unabhängigen Wert formula_1 geben, mit dem formula_14 (klassisch) bzw. formula_15 (relativistisch) gilt. Man bezeichnet formula_12 als den Impuls und formula_1 als die Masse des Körpers. Hiermit ist eine eigenständige Definition der Masse gegeben, die allein auf der Impulserhaltung beruht. Weiter ergibt sich, dass außer der Summe der Impulse in der klassischen Physik auch die Summe der Massen erhalten bleibt, in der relativistischen Physik aber die Summe der Größen formula_18, die (bis auf den universellen Faktor formula_2) die Energien der einzelnen Körper angibt.

Für diese Herleitung betrachtet man den vollkommen unelastischen Stoß, d. h. zwei Körper (formula_20), die sich aufeinander zubewegen und zu einem einzigen (formula_21) vereinigen. Die Impulse (formula_22) sind jeweils parallel zur Geschwindigkeit (formula_23), mit zunächst unbekannten Faktoren (formula_24). Impulserhaltung bedeutet:
Das ergibt die Gleichung:
Die Faktoren formula_24 können in noch unbekannter Weise auch von der jeweiligen Geschwindigkeit abhängen. Sicher sind sie aber gleich in dem Fall, dass beide Körper vollkommen gleich beschaffen sind: formula_28. In diesem Fall gilt, wenn der Stoß in dem Ruhesystem (formula_29) des im Stoß gebildeten Körpers formula_21 betrachtet wird:
Daher ergibt sich, dass in diesem Bezugssystem die Geschwindigkeiten der beiden gleichen stoßenden Körper entgegengesetzt gleich sein müssen:

Die beiden Geschwindigkeiten sind aber nicht entgegengesetzt gleich, wenn derselbe Stoß in einem mit der Geschwindigkeit formula_33 bewegten Bezugssystems betrachtet wird. Darin bewegt sich nach dem Stoß der Körper formula_21 mit Geschwindigkeit formula_35. Die Gleichung der Impulserhaltung lautet nun:

Darin sind formula_37 die Geschwindigkeiten der beiden stoßenden Körper im bewegten Bezugssystem.


Nach der in der klassischen Physik gültigen Galilei-Transformation gilt die einfache Addition der Geschwindigkeiten
und folglich

Diese Gleichung zwischen den drei Geschwindigkeiten ist mit der obigen Gleichung zwischen den drei Impulsen nur dann verträglich, wenn
Denn mit zwei verschiedenen Faktoren formula_42 kann sich kein zu formula_35 paralleler Vektor ergeben. Aus der ersten Gleichung der vorigen Zeile folgt nun, dass der Faktor formula_7 für alle Geschwindigkeiten gleich ist. Er ist identisch mit der aus der älteren Definition bekannten Masse. Damit gilt allgemein (mit den üblichen Symbolen):
Mit Kenntnis dieser Gleichung kann die Überlegung auf den Fall verschiedener Massen formula_46 verallgemeinert werden. Einsetzen in die Gleichung der Impulserhaltung führt auf das Ergebnis:
Demnach ist in der klassischen Mechanik die Masse eine additive Erhaltungsgröße.


In diesem Fall muss man statt der Galilei-Transformation die Lorentz-Transformation zugrunde legen. Dann gilt statt der einfachen Addition der Geschwindigkeitsvektoren das relativistische Additionstheorem. Daraus folgt (nach längerer Rechnung): Nicht formula_48 ist parallel zu formula_35, sondern der Vektor formula_50. Multipliziert mit einer Konstante, die hier im Vorgriff schon mit formula_1 bezeichnet wird, muss sich der Gesamtimpuls formula_52 ergeben. Folglich sind die beiden Impulse der stoßenden Körper durch
gegeben. Dies geht für kleine Geschwindigkeiten, wo formula_54 gesetzt werden kann, in die nichtrelativistische Formel formula_45 über, womit der konstante Faktor formula_1 sich tatsächlich als die Masse in relativistischer Definition erweist. Die Gleichung der Impulserhaltung lautet nun

und ermöglicht damit wieder die Bestimmung von formula_58. Es zeigt sich, dass die Masse in der relativistischen Mechanik keine additive Erhaltungsgröße ist, denn es gilt:
Nach dieser Gleichung ist die nach der relativistischen Formel formula_60 berechnete Energie eine additive Erhaltungsgröße.

Die vor allem in der Festkörperphysik gebräuchliche effektive Masse der Teilchen ist eine Größe, die in gewisser Hinsicht zur Masse analog ist. Sie wird aus der Energie-Impuls-Relation formula_61 gewonnen, indem diese in Anlehnung an die nicht-relativistische Gleichung formula_62 interpretiert wird. Die effektive Masse kann im Gegensatz zur echten Masse vom Impuls abhängen und in bestimmten Wertebereichen sogar negativ werden.

Da der Impuls eines Teilchens der Masse formula_1, das sich mit Geschwindigkeit formula_64 bewegt, in relativistischer Physik
beträgt (Herleitung siehe Viererimpuls), hängen die Energie und der Impuls mit der Masse durch die "Energie-Impuls-Beziehung"
zusammen. Im vierdimensionalen Raum aller denkbaren Energie- und Impulswerte liegen gemäß dieser Gleichung die physikalisch möglichen Energien eines Teilchens der Masse formula_1 auf einer dreidimensionalen Fläche, der sogenannten Massenschale. Sie ist ein Hyperboloid (formula_68 beschreibt eine Hyperbel in der formula_69-formula_70-Ebene).

Die Energie-Impuls-Beziehung gilt auch für Photonen. Sie sind masselos und bewegen sich stets mit Lichtgeschwindigkeit. Die Energie eines Photons ist bis auf einen Faktor formula_71 der Betrag seines Impulses, seine Masse verschwindet:

Die SI-Basiseinheit der Masse ist das Kilogramm mit dem Einheitenzeichen kg. Im Zusammenhang mit geschäftlichen Vorgängen ist in den meisten Industrieländern die Verwendung des Kilogramms als Masseneinheit rechtlich vorgeschrieben.
Historisch waren zahllose Gewichtsmaße in Verwendung, die teilsweise auch unspezifisch je nach Gegend, Zeit und Produkt Hohlmaßen, Packeinheiten, Traglasten und anderem entsprachen und daher schwer präzise anzugeben sind; siehe "Alte Maße und Gewichte."

Für die Angabe der Masse von Atomen und Molekülen ist die atomare Masseneinheit (amu) weit verbreitet.

Bei Elementarteilchen ist eine Angabe in Elektronenvolt geteilt durch das Quadrat der Lichtgeschwindigkeit üblich.

Die direkte Messung der Masse erfolgt am ruhenden Körper durch Vergleich mit einer Referenzmasse. Zwei Massen sind gleich, wenn sie im selben Schwerefeld die gleiche Gewichtskraft haben. Dies kann man mit einer Balkenwaage überprüfen. Dabei ist die Stärke des Schwerefeldes unerheblich, es muss nur von Null verschieden und an den Orten der beiden Körper gleich sein. Zur Festlegung der Masseneinheit siehe Urkilogramm.

Die Masse kann auch über Kräfte und Beschleunigungen bestimmt werden. In der newtonschen Mechanik ist jede Bewegungsänderung proportional zu der Kraft, welche die Bewegungsänderung verursacht hat (s. u.: formula_73). Masse ist somit die Proportionalitätskonstante zwischen Kraft und Beschleunigung:
Die Beschleunigung formula_75 gibt dabei die durch eine Kraft formula_76 verursachte Geschwindigkeitsänderung an.

Beispiel zur trägen Masse: Wird ein Körper durch die konstante Kraft formula_77 innerhalb des Zeitintervalls formula_78 um formula_79 schneller, so ist seine Beschleunigung:
Seine Masse beträgt dann:
Beispiel zur schweren Masse: Durch die Gravitation der Erde werden frei fallende Körper mit
beschleunigt. Ein Körper, der auf der Erdoberfläche mit der Gewichtskraft formula_77 angezogen wird, hat die Masse

In der newtonschen Mechanik ist die Masse eine extensive Größe. Das bedeutet, dass zwei Körper der Masse formula_1 insgesamt die doppelte Masse formula_86 haben. Intensive Größen ändern sich bei der Systemverdopplung nicht. Mit der Masse verwandt sind folgende intensive Größen:

In der klassischen Physik ist die Masse eine Erhaltungsgröße. Das bedeutet, dass sich die Masse in einem geschlossenen System nicht ändert. Wenn beispielsweise ein Stück Holz verbrennt, dann haben nach der klassischen Physik die entstehenden Verbrennungsabgase und die Asche nach der Verbrennung exakt die gleiche Masse wie das Holzstück und der verbrauchte Luftsauerstoff vor der Verbrennung. Dies wird als selbstverständliche empirische Tatsache angenommen, ohne dafür eine Begründung zu geben.

Ebenso wenig erklärt die klassische Mechanik die Äquivalenz von schwerer und träger Masse.

Als schwere Masse bezeichnet man sowohl die Quelle der Gravitationskraft als auch die „Gravitationsladung“. Die von der Masse formula_93 auf die Masse formula_94 ausgeübte Kraft ist
wobei die Massen punkt- oder kugelförmig gedacht sind und formula_96 der Vektor von formula_93 nach formula_94 ist. formula_99 ist die Gravitationskonstante, eine Naturkonstante.

Die träge Masse formula_1 ist in der newtonschen Mechanik das, was sich einer Beschleunigung widersetzt. Um den Bewegungszustand eines Körpers zu ändern, muss man daher eine Kraft formula_101 aufwenden. Je größer diese Kraft ist, umso stärker ändert sich der Impuls. Dies wird durch das 2. newtonsche Axiom, das Aktionsprinzip, ausgedrückt:
Daraus ergibt sich mit dem Impuls formula_103 für Körper mit konstanter Masse die Bewegungsgleichung zu „Kraft ist gleich Masse mal Beschleunigung“, der „Grundgleichung der Mechanik“:
Hier ist die träge Masse also der Proportionalitätsfaktor zwischen Kraft und Beschleunigung.

In der speziellen Relativitätstheorie wird die Masse so definiert, dass sie eine lorentzinvariante Größe ist, die im Grenzfall kleiner Geschwindigkeiten mit der Masse der klassischen Physik übereinstimmt. Dazu geht man von der Energie-Impuls-Relation eines Systems aus und stellt sie nach der Masse formula_1 um:

Darin ist formula_107 die Energie und formula_108 der Betrag des Impulses des Systems.

Damit ist die so definierte Größe formula_1 die durch die Konstante formula_71 dividierte Norm des relativistischen Vierervektors formula_111 (siehe Energie-Impuls-Vektor), folglich eine Lorentzinvariante. Diese Größe stimmt mit der im Gültigkeitsbereich der klassischen Mechanik definierten Masse überein, d. h. für Geschwindigkeiten, die klein gegenüber der Lichtgeschwindigkeit sind. Dies geht aus der Beziehung zwischen Impuls und Geschwindigkeit hervor:

Diese Gleichung erhält man aus der Energie-Impuls-Relation, wenn die Geschwindigkeit, mit der sich das System bewegt, eingeführt wird:

Der Faktor formula_114 heißt auch Lorentzfaktor.

In der speziellen Relativitätstheorie ist der Impuls formula_108 also nicht wie bei Newton das Produkt von Masse formula_1 und Geschwindigkeit formula_117. Die newtonsche Formel gilt nur als Näherung im nichtrelativistischen Grenzfall.

Um dennoch die newtonsche Formel beibehalten zu können, wurde der Begriff relativistische Masse

eingeführt, sodass formula_119 gilt. In diesem Zusammenhang wird die Masse formula_1 oft als formula_121 geschrieben und „Ruhemasse“ genannt (siehe oben unter „Wortgebrauch“), denn für formula_122 gilt

Der Begriff der relativistischen oder relativistisch veränderlichen Masse wird in der populären Literatur heute noch benutzt. In der Fachsprache wird er jedoch zunehmend vermieden, damit der Begriff der Masse konsequent für eine vom Beobachter unbeeinflusste Eigenschaft des Teilchens oder Systems verwendet werden kann. Zudem führt die relativistische Masse formula_124 an Stelle von formula_1 nur in den Gleichungen für den Impuls formula_126 und für die relativistische Energie formula_127 zu richtigen Ergebnissen. Im newtonschen Gravitationsgesetz eingesetzt bringt sie aber falsche Ergebnisse hervor, ebenso im 2. newtonschen Gesetz, wenn man es als formula_128 schriebe.

Der letztgenannte Mangel ergibt sich aus der Definition der Kraft formula_76 als die zeitliche Änderung des Impulses, in der speziellen Relativitätstheorie also:

Bildet man hieraus die Größe formula_131 und damit die Größe

so lässt sich nach der Beschleunigung formula_75 umstellen:

Demnach hat die Beschleunigung grundsätzlich auch eine Komponente in Richtung der Geschwindigkeit. Diese ist für kleine Geschwindigkeiten formula_135 aber zu vernachlässigen. Dann entspricht diese Gleichung der Grundgleichung der newtonschen Mechanik:

Man sieht aber, dass die Richtung der Beschleunigung nur dann parallel zur Kraft ist, wenn diese genau senkrecht oder parallel zur Geschwindigkeit einwirkt. Andernfalls hat die Beschleunigung auch einen Anteil, der parallel oder antiparallel zur Geschwindigkeit ist und mit zunehmender Geschwindigkeit anwächst. Zudem ergibt sich die Beschleunigung um den Faktor formula_137 geringer, wenn die Kraft in oder entgegengesetzt zur Richtung der Geschwindigkeit einwirkt als senkrecht dazu. Welche Beschleunigung eine Kraft formula_101 bewirkt, hängt also nach Größe und Richtung von der Geschwindigkeit des Körpers ab. Es gibt keinen einfachen Proportionalitätsfaktor zwischen Kraft und Beschleunigung wie in der newtonschen Mechanik. Die unterschiedliche Trägheit "in Richtung" der Bewegung und "quer dazu" hatte man zunächst mit den Begriffen der longitudinalen und transversalen Masse zu erfassen versucht, die aber heute nicht mehr verwendet werden.

In diesem und anderen Artikeln wird die Größe formula_124 nicht weiter verwendet, und das Symbol formula_1 für die Masse hat stets die Bedeutung der Ruhemasse, also formula_141 Zu beachten ist jedoch, dass mitunter, vor allem in älteren Texten, das Symbol formula_1 für die relativistische Masse steht.

Die Ruheenergie formula_143 ist die Energie eines Körpers oder Systems in seinem Ruhesystem, d. h. in dem Bezugssystem, in dem sein Gesamtimpuls null ist. Die Ruheenergie ist eine Eigenschaft des Systems, die nicht von seinem Bewegungszustand abhängt. Aus der oben angegebenen Energie-Impuls-Relation folgt die berühmte einsteinsche Gleichung:

Damit ist die Ruheenergie durch die Masse des Systems eindeutig bestimmt und umgekehrt. Beide Größen unterscheiden sich nur durch den konstanten Faktor formula_2 und sind daher äquivalent.

Die Ruheenergie von Teilchen wirkt sich insbesondere bei Erzeugungs- (z. B. Paarbildung) und Vernichtungsvorgängen (z. B. Annihilation) aus. Die Ruheenergie des Elektrons beträgt 0,511 MeV, diejenige eines Protons 938 MeV. Von der Ruheenergie eines Photons zu sprechen, ist ein Widerspruch in sich, denn es gibt kein Bezugssystem, in dem das Photon keinen Impuls hat. Richtig ist stattdessen für das Photon die Aussage formula_146.

Für ein System aus mehreren nicht wechselwirkenden Teilchen sind die Gesamtenergie und der Gesamtimpuls die Summen der jeweiligen Größen aller Teilchen. Die Energie-Impuls-Relation lautet daher
wobei formula_148 die Anzahl der Teilchen ist. Hierbei ist die invariante Masse des Mehrteilchensystems formula_149 im Allgemeinen nicht gleich der Summe der Massen der Einzelteilchen. Multipliziert man die invariante Masse mit dem konstanten Faktor formula_2, so ergibt sich daraus die Ruheenergie des Systems, in diesem Zusammenhang auch als Schwerpunktsenergie bezeichnet. Diese umfasst nicht nur die Ruheenergien der einzelnen Teilchen, sondern auch ihre Relativbewegung gegenüber dem Schwerpunkt. Zur Erläuterung stelle man sich ein Gefäß vor, das ein Gas enthält. Fügt man dem Gas Energie zu, indem man es komprimiert oder erhitzt, so hat das Gefäß als Ganzes eine erhöhte Schwerpunktsenergie und damit eine größere invariante Masse. Im Detail betrachtet verändert sich die Masse der einzelnen Gasmoleküle dabei nicht, wohl aber ihre kinetische Energie relativ zum gemeinsamen Schwerpunkt.

Die Schwerpunktsenergie ist – ebenso wie die invariante Masse – invariant unter Lorentztransformation. Sie gibt den Energiebetrag an, der für die Erzeugung neuer Teilchen bei einer Teilchenkollision zur Verfügung steht, und ist daher in der experimentellen Teilchenphysik von Bedeutung.

Gibt ein geschlossenes System Energie über die Systemgrenzen z. B. in Form von Strahlung ab, so verringert sich der Energieinhalt des Systems und damit seine Masse. In diesem Sinne ist die Masse in der modernen Physik keine Erhaltungsgröße mehr, wenngleich sich dies in alltäglichen Situationen kaum bemerkbar macht.

Bei Kernreaktionen werden jedoch Energiemengen umgesetzt, die gegenüber der Ruhenergie der Kernbausteine nicht mehr zu vernachlässigen sind. Die Bindungsenergie führt dazu, dass ein Atomkern eine wägbar geringere Masse hat als die Summe seiner Bausteine. Die Differenz wird Massendefekt genannt. Die Bindungsenergie liegt bei den meisten Atomkernen zwischen 7 und 9 MeV pro Nukleon und bewirkt dadurch einen Massendefekt zwischen 0,7 und 0,9 Prozent. Sehr leichte Atomkerne (H, H, He, Li, Be, B) weisen mit 1 bis 6 MeV geringere Bindungsenergien pro Nukleon und mit 0,1 und 0,6 Prozent geringere Massendefekte auf.

Die Bindungsenergie chemischer Bindungen liegt mit typischen 2 bis 7 eV pro Bindung (pro Nukleon wäre sie entsprechend dem Molekülgewicht noch einmal deutlich kleiner) um 7 bis 9 Größenordnungen darunter. Daher konnte ein chemischer Massendefekt bisher noch nicht durch Wägung nachgewiesen werden. Zwar liegen die Werte bei einigen Reaktionen im Bereich der Nachweisgrenze aktueller Massekomparatoren (formula_151 Prozent): Der größte chemische Massendefekt ist formula_152 Prozent bei der Bindung formula_153. Zu formula_154 gehört ein Massendefekt von formula_155 Prozent.

Da bei chemischer Bindung der Massendefekt so klein ist, dass er bei keiner Wägung zu bemerken wäre, konnte Ende des 18. Jahrhunderts von Antoine de Lavoisier der Massenerhaltungssatz aufgestellt werden. Diese Erkenntnis trug maßgeblich zur Abkehr von der Alchemie und Phlogistontheorie bei und wurde damit eine wichtige Grundlage der auf den Begriff der chemischen Elemente gestützten Chemie.

In der allgemeinen Relativitätstheorie wird der freie Fall von Körpern im Gravitationsfeld als kräftefrei verstanden. Eventuell wirkende Kräfte würden bewirken, dass die Bahnkurven vom freien Fall abweichen. Wird der Körper vom freien Fall abgehalten, ist eine Kraft nötig, deren Größe zur trägen Masse des Körpers proportional ist.

Die Weltlinien frei fallender Teilchen sind die Geraden (genauer: Geodäten) der Raumzeit. Sie sind vollständig durch den anfänglichen Ort und die anfängliche Geschwindigkeit festgelegt und hängen nicht von anderen Eigenschaften wie Größe oder Masse des frei fallenden Teilchens ab (Äquivalenzprinzip). Da die Raumzeit gekrümmt ist, ergibt die Projektion der Geodäten auf den dreidimensionalen Ortsraum normalerweise keine Geraden, sondern beispielsweise Wurfparabeln.

Quelle der Gravitation ist in der Grundgleichung der Allgemeinen Relativitätstheorie der Energie-Impuls-Tensor, der sich aus Energiedichte, Impulsdichten, Energieströmen und Impulsströmen zusammensetzt. Da die Energie ruhender Körper durch ihre Masse bestimmt ist, bewirkt allein deren Masse die Gravitation. Kann man die Bewegung der gravitationserzeugenden Körper vernachlässigen und ist die Geschwindigkeit der frei fallenden Teilchen klein gegen die Lichtgeschwindigkeit, so wirkt sich die Masse der gravitationserzeugenden Körper wie in Newtons Gravitationstheorie aus. Für Licht als Testteilchen trifft diese Einschränkung nicht zu: Es wird an der Sonne doppelt so stark abgelenkt, wie nach Newton zu erwarten wäre.

Im Standardmodell der Elementarteilchenphysik wird der Ursprung der Massen der Elementarteilchen durch den Higgs-Mechanismus erklärt. Durch Wechselwirkung mit dem Higgs-Feld, das indirekt durch die Beobachtung des Higgs-Bosons nachgewiesen wird, erhalten sie eine Masse, da das Higgs-Feld auch im Vakuum nicht verschwindet. Nur die Masse des Higgs-Bosons selbst wird hierdurch nicht erklärt. In supersymmetrischen Theorien könnte ein ähnlicher Mechanismus auch durch andere Teilchen (Goldstinos) vermittelt werden (siehe auch Goldstonetheorem und Gravitino).

Die Massen der Baryonen, zu denen auch Proton und Neutron gehören, sind allerdings ca. 100-mal größer als die Massen der drei Quarks, aus denen sie bestehen. Die Baryonenmassen werden dynamisch erklärt. Ansätze zur Berechnung liefern Gitterrechnungen in der Quantenchromodynamik (QCD). Halb anschaulich kann man mit der geringen Ausdehnung der Baryonen von etwa 10 m argumentieren: Wenn sich die Quarks im Baryon auf so kleinem Raum konzentrieren, haben sie eine so kurze de-Broglie-Wellenlänge, dass ihre kinetische Energie formula_156 nach Einsteins Formel formula_157 erhebliche Masse bedeutet. Drei solcher Konstituenten-Quarks ergeben dann tatsächlich etwa die Masse des Protons oder Neutrons.

Die Baryonen machen den größten Teil der Masse sichtbarer Materie aus. Es wird vermutet, dass schwach wechselwirkende massereiche Teilchen (englisch "," abgekürzt WIMP) wie etwa das hypothetische leichteste supersymmetrische Teilchen (englisch "," abgekürzt LSP) die nicht sichtbare "Dunkle Materie" aufbauen könnten.

Im allgemeinen Sprachgebrauch wird die Masse eines Objekts auch als Gewicht bezeichnet. Beispiele sind das Übergewicht, Leergewicht, Abtropfgewicht oder Gewichtsangaben in Kochrezepten. Dies trifft auch auf viele Gesetze und Verordnungen zu. Beispiele sind das Deutsche Mutterschutzgesetz und das Schweizer Straßenverkehrsgesetz.

Beim Gleichsetzen von Masse und Gewichtskraft kann der Eindruck entstehen, die Masse hänge von der vor Ort herrschenden Schwerkraft ab. So ist die folgende Aussage missverständlich: „Auf dem Mond wiegt ein 60 kg schwerer Mensch nur ungefähr 10 kg.“ Klarer ist: „Ein Mensch mit einem ‚Gewicht‘ auf der Erde von 60 kg wiegt auf dem Mond ungefähr so viel, wie ein Mensch mit einem ‚Gewicht‘ von 10 kg auf der Erde wiegt.“





</doc>
<doc id="3463" url="https://de.wikipedia.org/wiki?curid=3463" title="Margaret Mitchell">
Margaret Mitchell

Margaret Munnerlyn Mitchell (* 8. November 1900 in Atlanta, Georgia; † 16. August 1949 ebenda) war eine US-amerikanische Schriftstellerin.

Für ihren 1936 erschienenen Südstaaten-Roman "Vom Winde verweht" ("Gone with the Wind") wurde sie 1937 mit dem Pulitzer-Preis ausgezeichnet. Die gleichnamige Verfilmung aus dem Jahr 1939 mit Vivien Leigh und Clark Gable in den Hauptrollen ist einer der erfolgreichsten Filme aller Zeiten.

Margaret Mitchell wurde in Atlanta als Tochter von Mary Isabelle „Maybelle“ Stephens und dem Rechtsanwalt Eugene Muse Mitchell geboren. Ihr Spitzname, unter dem sie später auch zuweilen in der Öffentlichkeit auftrat, war Peggy. Nachdem Mitchell ihre Schul- und Collegeausbildung beendet hatte, übernahm sie nach dem frühen Tod der Mutter im Jahr 1918 den elterlichen Haushalt und begann, für die Zeitung "Atlanta Journal" eine wöchentliche Kolumne in der Sonntagsausgabe zu schreiben.

Am 2. September 1922 heiratete Mitchell Berrien „Red“ Upshaw, von dem sie sich 1924 wieder scheiden ließ. Am 4. Juli 1925 heiratete sie Upshaws Freund John Marsh.

Mitchell begann 1926, "Vom Winde verweht" zu schreiben, als sie wegen Verletzungen und Arthritis längere Zeit ans Bett gefesselt war. Ihr Mann John Marsh kaufte ihr eine gebrauchte Remington-Reiseschreibmaschine, auf der sie innerhalb der folgenden zehn Jahre das mehr als 1000 Seiten umfassende Buch fertigstellte. Der Roman erschien am 30. Juni 1936. Im gleichen Jahr verkaufte Mitchell die Filmrechte an ihrem Buch für 50.000 US-Dollar an den Produzenten David O. Selznick. 1937 wurde Mitchell für "Vom Winde verweht" mit dem Pulitzer-Preis ausgezeichnet.

Ab den 1940er Jahren arbeitete Mitchell ausschließlich ehrenamtlich für wohltätige Zwecke. Unter anderem sponserte sie die medizinische Ausbildung von unterprivilegierten College-Absolventen, gründete Notfallaufnahmen für Schwarze und Weiße am Grady Hospital in Atlanta, engagierte sich in der Afro-Amerikanischen Bewegung für Rechte und Ausbildung der Schwarzen in Atlanta und half während des Zweiten Weltkriegs in verschiedenen Krankenhäusern.

Am 11. August 1949 wurde Mitchell beim Überqueren der Peachtree Street von einem betrunkenen Taxifahrer angefahren; ihre Verletzungen waren so schwer, dass sie nicht mehr aus dem Koma erwachte. Margaret Mitchell starb am 16. August 1949 im Grady Hospital und ist auf dem Oakland Cemetery in Atlanta begraben.

1965 wurde Mitchell postum der „Shining Light Award“ von der Atlanta Gas Light und der Radiostation WSB für ihre „Verdienste im Sinne der Menschlichkeit“ verliehen. 1989 wurde das Margaret Mitchell House, in dem "Vom Winde verweht" geschrieben wurde, von Atlantas Bürgermeister zu einem der offiziellen Wahrzeichen der Stadt ernannt. 1997 wurde es für die Öffentlichkeit als Museum freigegeben.

1987 erschien das Buch "Stolz und unbeugsam wie Scarlett. Margaret Mitchells Briefe an ihren Jugendfreund Allen Edee" aus der Zeit zwischen 1919 und 1921. 1996 folgte das Buch "Insel der verlorenen Träume", ein Roman, der 1995 zusammen mit Briefen und Fotos von Margaret Mitchell im Nachlass eines Jugendfreundes entdeckt worden war. Im Jahr 2000 erschien "Before Scarlett", eine Sammlung von bis dahin unveröffentlichten Geschichten und Novellen, die Margaret Mitchell im Alter zwischen sieben und 18 Jahren geschrieben hatte.





</doc>
<doc id="3464" url="https://de.wikipedia.org/wiki?curid=3464" title="Magmatisches Gestein">
Magmatisches Gestein

Magmatisches Gestein oder Erstarrungsgestein ist Gestein, das durch abkühlungsbedingtes Erstarren einer Gesteinsschmelze (Magma) entstanden ist. Die Magmatite sind neben den Sedimentgesteinen (Sedimentiten) und den Metamorphiten eine der drei Gesteinshauptgruppen. Der Chemismus des entstehenden Gesteins ist von mehreren Faktoren abhängig: 

Nach ihrer Erstarrungstiefe und den sich daraus ergebenden Gefügemerkmalen lassen sich magmatische Gesteine petrografisch in zwei Gruppen unterteilen: Plutonite (Tiefengesteine) entstehen durch Erstarren von Magma tief im Erdinneren, Vulkanite (Ergussgesteine) entstehen aus Magma, das aus dem Erdinneren bis zur Erdoberfläche aufgedrungen, als Lava an der Erdoberfläche ausgetreten und dort erstarrt ist. Ob die Abkühlung und Erstarrung der Gesteinsschmelze ober- oder unterirdisch erfolgt, hat einen deutlichen Einfluss auf die Textur des entstehenden Gesteins. Prinzipiell gilt hierbei: Je schneller die Schmelze erkaltet, desto feinkristalliner („feinkörniger“) wird das Gestein. Je langsamer das Magma abkühlt (bei guter Isolierung durch ein mehrere Kilometer mächtiges Deckgebirge), desto größere Kristalle können sich in der erkaltenden Schmelze bilden.

"Plutonite" (nach Pluton, dem griechischen Gott der Unterwelt) oder "Tiefengesteine" werden Magmatite genannt, welche innerhalb der Erdkruste – gewöhnlich in einer Tiefe von einem bis mehreren Kilometern – langsam in einer Magmakammer auskristallisieren. Der entsprechende Gesteinskörper wird als Pluton, oder, wenn er besonders große Abmessungen aufweist und komplex aufgebaut ist, auch als Batholith bezeichnet. Die Plutonite haben eine mittel- bis grobkristalline („grobkörnige“) Struktur, das heißt, die einzelnen Mineralkörner, aus denen das Gestein besteht, sind mit bloßem Auge erkennbar. Der am weitesten verbreitete und bekannteste Vertreter ist der Granit.

"Vulkanite" (nach Vulcanus, dem römischen Gott des Feuers) oder "Ergussgesteine" sind jene Magmatite, die aus einer Gesteinsschmelze hervorgegangen sind, die bis an die Erdoberfläche gelangt ist. Sie werden auch als Extrusiv-, Ausbruchs-, Effusiv- und Vulkanische Gesteine bezeichnet. Das Aufdringen von Magma bis zur Erdoberfläche und die damit verbundenen Naturerscheinungen werden unter dem Begriff Vulkanismus zusammengefasst. Das aus einem Vulkan austretende flüssige Magma (und allgemeinsprachlich auch das nach der Erstarrung daraus hervorgegangene Gestein) wird "Lava" genannt. Weil sie infolge des extremen Temperaturunterschiedes an der Erdoberfläche rasch erstarren, sind vulkanische Gesteine oft sehr feinkörnig oder sogar glasig, da kaum Zeit zum Wachstum größerer Kristalle bleibt. Nicht selten kristallisiert jedoch ein Teil der Schmelze langsam in größerer Tiefe aus. Daher ist für viele Vulkanite ein porphyrisches Gefüge, mit größeren "Einsprenglingen" in einer feinkörnigen "Grundmasse" typisch. Der am weitesten verbreitete und bekannteste Vertreter ist der Basalt.

Eine Übergangsform zwischen plutonischen und vulkanischen Gesteinen sind die "magmatischen Ganggesteine" (auch Subvulkanite, Übergangsmagmatite, Mesomagmatite oder Mikroplutonite genannt). Sie bilden sich, wenn ein Magma in einer geringen Krustentiefe bzw. in relativer Nähe zur Erdoberfläche weder besonders langsam noch besonders schnell auskristallisiert. Ein spezieller subvulkanischer Gesteinskörper ist der Lakkolith. Ein typischer Vertreter für ein subvulkanisches Gestein ist der Dolerit, das subvulkanische Äquivalent des Basalts bzw. Gabbros (daher auch "Mikrogabbro" genannt).

Die magmatischen Gesteine können auch nach ihrer chemischen Zusammensetzung gruppiert werden. Dabei unterscheidet man anhand des Verhältnisses (mol-%) von KO+NaO zu SiO zwischen 
und

Während bei subalkalinen Magmatiten der Anteil von KO+NaO relativ niedrig im Verhältnis zu SiO ist, ist er bei Alkali-Magmatiten relativ hoch.

Magmatisches Gestein kann nach verschiedenen Kriterien klassifiziert werden. Eine der gebräuchlichsten Methoden ist die Einteilung im Streckeisendiagramm anhand des Mineralbestandes. Weitere gängige Methoden berücksichtigen die "Helligkeit" (Farbindex), die Textur oder den Chemismus (z. B. den SiO-Gehalt oder den Gehalt an Alkalien, Eisen, Magnesium oder anderen Elementen).

Die Bildung magmatischer Gesteine ist ein Schritt im Kreislauf der Gesteine. Genauso wie jedes Gesteins aufgeschmolzen werden kann, kann auch jedes magmatische Gestein nachfolgend prinzipiell weitere Schritte in diesem Zyklus durchlaufen und dabei in andere Gesteinstypen umgewandelt bzw. überführt werden:





</doc>
<doc id="3465" url="https://de.wikipedia.org/wiki?curid=3465" title="Maximum">
Maximum

Maximum steht für:

Siehe auch:



</doc>
<doc id="3466" url="https://de.wikipedia.org/wiki?curid=3466" title="Minimum">
Minimum

Minimum (lat. "minimum" „das Kleinste“) steht für:
Siehe auch:



</doc>
<doc id="3468" url="https://de.wikipedia.org/wiki?curid=3468" title="Masse (Soziologie)">
Masse (Soziologie)

Masse bezeichnet in der Soziologie eine große Anzahl von Menschen, die konzentriert auf relativ engem Raum physisch miteinander kommunizieren und/oder als Kollektiv gemeinsam sozial handeln. Der Begriff wird oft abwertend gebraucht („dumme Massen“, „Vermassung“), andererseits können Massen als soziale Bewegungen auch kulturell hochstehende Werte wie Gerechtigkeit und Gleichheit ins Bewusstsein der öffentlichen Meinung bringen oder sie als „revolutionäre Massen“ aktiv politisch durchsetzen. 

„Masse“ wird umgangssprachlich auch oft synonym zu „einfachen Leuten“, „Ungebildeten“, zur „Arbeiterklasse“, allgemeiner auch zum „Volk“ bzw. zur „Bevölkerung“ gebraucht. Gegenbegriffe sind dann „Individuum“, „bedeutende Einzelne“, auch „die Gebildeten“. Der Soziologe Vilfredo Pareto stellt die „Masse“ den „Eliten“ und „Reserveeliten“ gegenüber, ähnlich unterscheidet Charles Wright Mills zwischen „Masse“ und „Machtelite“.

Andererseits ist ein zweiter, soziologischer Gegenbegriff auch die „Menge“. Während „Massen“ oft (meist spontane, manchmal aber auch geplante) Hierarchien aufweisen (insbesondere in Form von Anführern und Rädelsführern), sind „Mengen“ unstrukturiert, nur situativ verbunden (z. B. alle Passanten in einer Einkaufsstraße) und tendieren im Gegensatz zu „Massen“ nicht dazu, geschlossen, also aktiv und intentional zu handeln.

In der Regel geht Massenbildung mit einer Verhaltensenthemmung und einer temporären Überschreitung von sozialen Normen einher. Dies kann sowohl positive Affekte freisetzen (insbesondere soziale Nähe bei Festen und Feiern) als auch negative wie Hass und Aggression (etwa in Form des Lynchmobs). Mitunter können Massenstimmungen sehr rasch umschlagen (der redensartliche „Wankelmut der Massen“), wenn äußere Ereignisse eintreten oder sich Gerüchte innerhalb der Masse verbreiten. 

Dennoch folgt das soziale Handeln von Massen eigenen Gesetzmäßigkeiten (vgl. „Figuration“). Ihre Teilnehmer werden dabei von gemeinsamen Intentionen und Interessen, aber auch von kollektiven emotionalen Affekten und unbewussten Regungen koordiniert. Oft orientieren sich Massen auch an kollektiven Symbolen und Ideen, etwa politischen oder religiösen Konzepten. 

Eine aktuelle Masse ist alsdann zur Schaffung eigener sozialer Dynamik in der Lage: Im negativen Fall reichen diese von Gerüchtverbreitung, Plünderung, Panik bis hin zu Pogrom und Lynchjustiz, im positiven Fall aber auch von einer friedlichen Kundgebung oder Demonstration über den politischen Widerstand gegen Tyrannei und die Befreiung von Diktaturen in Aufständen und Revolten – bis hin zur revolutionären Ausrufung einer neuen Gesellschaftsform, wie es in der Französischen Revolution 1789 in Frankreich oder in der Februarrevolution 1917 und der folgenden Oktoberrevolution in Russland der Fall war. 

Nicht zwangsläufig, aber oft werden Massen dabei von einzelnen (Rädels-)Führern angeleitet und dabei mitunter zu Taten verführt, die sie außerhalb der Masse als Individuen nicht begehen würden. Die negativen Aspekte der Manipulierbarkeit des Individuums, sein „Aufgehen“ in der Masse, sind insbesondere in bzw. vor Kriegen (vgl. Hurra-Patriotismus) sowie im Massenkult des Nationalsozialismus zum Vorschein gekommen. Eine anschauliche Beschreibung des rauschhaften Augusterlebnisses beim Ausbruch des Ersten Weltkriegs in Deutschland bietet Heinrich Mann in seinem Roman "Der Untertan": 

Herrschende beriefen und berufen sich zu ihrer Legitimation nicht selten auf eine Unterstützung der Massen, erwartbar in Demokratien, aber auch in ‚populistischen‘ Diktaturen (vgl. Tyrannis).

Im Marxismus wird die Masse als (zumindest potenziell) revolutionärer, nach Emanzipation strebender Teil der Gesellschaft (insbesondere in Form des Proletariats) als möglicher Träger einer sozialen Revolution gesehen. Gerade der Bezug auf die Interessen der Massen unterscheidet in den Augen der Marxisten die kommunistische Bewegung von anderen historischen sozialen Bewegungen. So schreibt Karl Marx 1848: "„Alle bisherigen Bewegungen waren Bewegungen von Minoritäten oder im Interesse von Minoritäten. Die proletarische Bewegung ist die selbständige Bewegung der ungeheuren Mehrzahl im Interesse der ungeheuren Mehrzahl.“" Doch verstanden Engels und Marx, die Masse der Proletarier von den Massenauftritten (den „Emeuten“ und Aufläufen) der Elenden, des „Lumpenproletariats“, zu unterscheiden.

Nach Vilfredo Pareto übernimmt jedoch nach einer erfolgreichen Revolution niemals die „Masse“ selber die Herrschaft, sondern immer eine „Reserveelite“, die die Masse auf ihre Seite gebracht und derart instrumentalisiert hat.

Spontane Massengeschehnisse, insbesondere in Form von sozialen Protestbewegungen, sind allgemein vor allem in krisenhaften Zusammenhängen erwartbar.

Soziale Krisen aufgrund von Inflation, Hungersnöte, Seuchen oder militärischer Konflikte führten seit jeher zu Massenhandeln – daher auch die Bedeutung von Massen in Gestalt "ad hoc" zusammen tretender Volksversammlungen oder Heeresversammlungen (im Römischen Reich oft bis hin zur Ausrufung eines neuen Kaisers). Generell liegt eine Hauptursache für Massenhandeln darin, dass neuartige Krisen nicht durch die in gewohnten Gemeinschaften bewährten Notmaßnahmen bewältigt werden können.

Das historische Auftreten von Menschen als Massen ist also nicht nur ein Phänomen höherer Zivilisationen oder gar der Neuzeit, wo es seine Ursache in der mit dem Bevölkerungswachstum verbundenen sinkenden Dichte und Verkleinerung sozialer Netzwerke, zumal bei steigender Urbanisierung und damit der Verunsicherung durch zumeist wirtschaftliche Krisen hat.

In der Neuzeit findet sich dagegen eine ganz neue Form der Massenbildung: die Massenkommunikation durch Massenmedien wie Zeitung, Rundfunk oder Fernsehen. Aufgrund ihrer großen Reichweite und Kapazität bieten Massenmedien einerseits die Möglichkeit, große Menschenmassen kommunikativ miteinander zu vernetzen, andererseits gehen sie aufgrund ihrer starren Sender-Empfänger-Struktur meist mit einseitigen Machtstrukturen einher. In der Regel im Besitz großer Konzerne oder des Staates, bilden Massenmedien eigene Strategien zur Massenformierung aus, um die öffentliche Meinung in ihrem Interesse bzw. im Interesse Dritter zu beeinflussen (Propaganda) oder um mehr Waren abzusetzen (Werbung). 

Elias Canetti schreibt der Masse vor allem den wichtigen Vorgang der "Entladung" zu. Erst dieser Vorgang, wo sich eine Mehrzahl für oder gegen etwas entscheidet, ist nach ihm konstituierendes Element jeglicher Massenbildung. Vorher ist sie quasi nicht existent. Erst wenn Individuen "ihre Verschiedenheiten loswerden und sich als "gleiche" fühlen" entsteht eine Masse mit einer ihr innewohnenden Handlungsrichtung.

Theodor W. Adorno kritisierte die industriell produzierte Massenunterhaltung in seinem Schlagwort der „Kulturindustrie“. Eine wesentliche Motivation der kulturindustriell betriebenen „Vermassung“ durch Massenmedien sieht er in der ideologischen Gleichschaltung der Individuen zu gehorsamen Konsumenten und Untertanen.



Untersucht wurde das Massenphänomen u. a. von 




</doc>
<doc id="3469" url="https://de.wikipedia.org/wiki?curid=3469" title="Midnight Commander">
Midnight Commander

GNU Midnight Commander, kurz "mc", ist ein freier Klon des DOS-Tools Norton Commander (→ zweispaltige Ansicht) und gehört zu den bekanntesten Konsolen-Programmen unter Linux.

Über ein VFS ist auch ein transparenter Zugriff auf Archive und Netzwerkserver möglich, wie z. B. FTP oder Samba.

Der Midnight Commander wurde 1994 von Miguel de Icaza entwickelt und war dessen erster Beitrag zum GNU-Projekt. Über seine Intention, den mc zu schreiben, sagt Icaza selbst:

In früheren Versionen von 1999 bis 2001 hatte mc zusätzlich eine grafische Benutzeroberfläche "gmc" und war der erste Dateimanager des Gnome-Projekts, galt aber als sehr fehlerhaft. Nach der Integration von Nautilus als Standard-Dateimanager in GNOME wurden die grafischen Funktionen wieder aus mc entfernt. Das fehlende GUI ist aber auch ein Vorteil, da sich der "mc" so auch über eine Telnet- oder SSH-Verbindung (die in einer Konsole läuft) auf einem Zielsystem nutzen lässt.

Zwischenzeitlich wurden zwei Windows-Portierungen des Midnight Commanders umgesetzt, davon wird jedoch nur noch "mcwin32" weiterentwickelt.

Die Entwicklung des Midnight Commanders stand mehrere Jahre still, bevor ihn Anfang 2009 eine Gruppe von Programmierern wiederbelebte und Version 4.6.2 veröffentlichte. Trotz der relativ langen Zwischenzeit, in der das Projekt nicht weiterentwickelt wurde, hat es auch heute noch eine sehr weite Verbreitung.



</doc>
<doc id="3470" url="https://de.wikipedia.org/wiki?curid=3470" title="Mittelland">
Mittelland

Mittelland heißen folgende geographische Objekte:


Mittelland bezeichnet


Siehe auch:


</doc>
<doc id="3473" url="https://de.wikipedia.org/wiki?curid=3473" title="Milgram-Experiment">
Milgram-Experiment

Das Milgram-Experiment ist ein erstmals 1961 in New Haven durchgeführtes psychologisches Experiment, das von dem Psychologen Stanley Milgram entwickelt wurde, um die Bereitschaft durchschnittlicher Personen zu testen, autoritären Anweisungen auch dann Folge zu leisten, wenn sie in direktem Widerspruch zu ihrem Gewissen stehen. Der Versuch bestand darin, dass ein „Lehrer“ – die eigentliche Versuchsperson – einem „Schüler“ (ein Schauspieler) bei Fehlern vermeintlich einen elektrischen Schlag versetzte. Ein Versuchsleiter (ebenso ein Schauspieler) gab dazu Anweisungen. Die Intensität des elektrischen Schlages sollte nach jedem Fehler erhöht werden. Diese Anordnung wurde in verschiedenen Variationen durchgeführt.

Angeregt wurde Milgram durch den US-amerikanischen Psychiater Jerome D. Frank, der bereits 1944 der Frage nachgegangen war, wovon die Gehorsamkeitsbereitschaft willkürlich ausgewählter Personen abhängt. Frank verlangte damals von seinen Probanden den Verzehr von zwölf geschmacklosen Keksen – vgl. das Soda-Cracker-Experiment. Der Gruppe wurde gesagt, dass der Verzehr salzloser Kekse wissenschaftlich notwendig sei. Überraschend weigerten sich nur zehn Prozent der Teilnehmer, die Kekse zu essen.

Das Milgram-Experiment sollte ursprünglich dazu dienen, Verbrechen aus der Zeit des Nationalsozialismus sozialpsychologisch zu erklären. Dazu sollte die „Germans-are-different“-These geprüft werden, die davon ausging, dass die Deutschen einen besonders obrigkeitshörigen Charakter haben. Nach den ersten Ergebnissen der Untersuchung in New Haven schien dies jedoch nicht mehr notwendig, auch weil die Untersuchung in ihrem Aufbau wesentlich grundsätzlicher angelegt war. Milgram erhielt für diese Arbeit 1964 den jährlich vergebenen Preis der American Association for the Advancement of Science in der Kategorie Sozialpsychologie. Die American Psychological Association hingegen schloss Milgram wegen des Experimentes für ein Jahr aus, nachdem ein Kritiker ihm in der Zeitschrift "American Psychologist" vorgeworfen hatte, ein „traumatisierendes“ Experiment vorgenommen zu haben, das „potenziell schädlich“ für die Versuchspersonen sei. Vor allem wegen dieser Kritik, die auch von zahlreichen anderen Fachleuten geäußert wurde, verweigerte die Harvard University Milgram später eine Anstellung. Milgram hielt dazu anschließend selbst fest:

Die Ergebnisse des Milgram-Experiments wurden zunächst in einem Artikel mit dem Titel "Behavioral study of obedience" veröffentlicht, der in dem renommierten "Journal of abnormal and social psychology" erschien. 1974 publizierte Milgram sein Werk: "Obedience to Authority. An Experimental View", in dem er die Ergebnisse in einen breiteren Kontext einordnete. Die deutsche Ausgabe kam im selben Jahr heraus.

Milgram bezieht sich darin unter anderem auf das 1963 in New York erschienene Werk der politischen Theoretikerin Hannah Arendt "Eichmann in Jerusalem. Ein Bericht von der Banalität des Bösen." Dieses Konzept der Banalität des Bösen, so argumentiert er, komme der Wahrheit sehr nahe. Die fundamentalste Erkenntnis der Untersuchung sei, dass ganz gewöhnliche Menschen, die nur ihre Aufgabe erfüllten und keinerlei persönliche Feindschaft empfinden, zu Handlungen in einem Vernichtungsprozess veranlasst werden können.

Der US-amerikanische Historiker Alfred W. McCoy vermutet, Milgram habe das Experiment im Rahmen des CIA-MKULTRA-Programms zur Forschung über Bewusstseinskontrolle durchgeführt. Darauf deute nicht nur der Zeitpunkt hin, sondern auch „das Thema, die militärischen Verbindungen, die umstrittene Finanzierung durch die NSF und deren Ablehnung aller späteren Projekte Milgrams“. Diese Vorwürfe werden auf der Webseite von Milgrams Biograph Thomas Blass ausführlich diskutiert und bestritten.

Der ganze Ablauf des Experiments ist wie ein Theaterstück inszeniert, bei dem alle außer dem Probanden eingeweiht sind. Solch eine Experimentalanordnung übernahm Milgram von seinem Lehrer Solomon Asch. Eine Versuchsperson und ein Vertrauter des Versuchsleiters, der vorgab, ebenfalls Versuchsperson zu sein, sollten an einem vermeintlichen Experiment zur Untersuchung des Zusammenhangs von Bestrafung und Lernerfolg teilnehmen. Ein offizieller Versuchsleiter (Experimentator, "V") bestimmte den Schauspieler durch eine fingierte Losziehung zum „Schüler“ "(S)", die tatsächliche Versuchsperson zum „Lehrer“ "(L)". Die Verabreichung eines elektrischen Schlags, mit einer Spannung von 45 Volt, sollte der Versuchsperson die körperlichen Folgen elektrischer Schläge vergegenwärtigen. Zudem wurde das an einen elektrischen Stuhl erinnernde Versuchsinventar gezeigt, auf dem der „Schüler“ getestet werden sollte. Diese Versuchsanordnung mit der gewollten Assoziation wurde von den Probanden zu keinem Zeitpunkt in Frage gestellt.

Der Versuch bestand darin, dass der „Lehrer“ dem „Schüler“ bei Fehlern in der Zusammensetzung von Wortpaaren jeweils einen elektrischen Schlag versetzte. Dabei wurde die Spannung nach jedem Fehler um 15 Volt erhöht. In Wirklichkeit erlebte der Schauspieler keine elektrischen Schläge, sondern reagierte nach einem vorher bestimmten Schema, abhängig von der eingestellten Spannung. Erreichte die Spannung beispielsweise 150 Volt, verlangte der Schauspieler, von seinem Stuhl losgebunden zu werden, da er die Schmerzen nicht mehr aushalte. Dagegen forderte der dabei sitzende Experimentator, dass der Versuch zum Nutzen der Wissenschaft fortgeführt werden müsse. Wenn die Versuchsperson Zweifel äußerte oder gar gehen wollte, forderte der Experimentator in vier standardisierten Sätzen zum Weitermachen auf. Die Sätze wurden nacheinander, nach jedem geäußerten Zweifel der Versuchsperson, gesprochen und führten nach dem vierten Mal zu einem Abbruch des Experimentes seitens des Versuchsleiters. Damit die Sätze immer gleich ausfielen, wurden sie vorher mit dem Schauspieler eingeübt, insbesondere auch, um einen drohenden Unterton zu vermeiden.


Es gab noch weitere Standardsätze in antizipierten Verlaufssituationen: Wenn die Versuchsperson fragte, ob der „Schüler“ einen permanenten physischen Schaden davontragen könne, sagte der Versuchsleiter: „Auch wenn die Schocks schmerzvoll sein mögen, das Gewebe (tissue) wird keinen dauerhaften Schaden davontragen, also machen Sie bitte weiter!“ Auf die Aussage des „Lehrers“, der „Schüler“ wolle nicht weitermachen, wurde standardmäßig geantwortet: „Ob es dem Schüler gefällt oder nicht, Sie müssen weitermachen, bis er alle Wörterpaare korrekt gelernt hat. Also bitte machen Sie weiter!“ Wenn nach der Verantwortung gefragt wurde, sagte der Versuchsleiter, er übernehme die Verantwortung für alles, was passiert. Die Versuchsperson reagierte auf die Stromschläge mit auf Band aufgenommenen Schmerzensäußerungen. Diese hatten Milgram in Prätestversionen des Experiments zunächst gefehlt, die Gehorsambereitschaft war dann aber so hoch, dass er sie hinzufügte.

Der „Schüler“ war in diesem Fall ein unauffälliger Amerikaner irischer Abstammung und repräsentierte einen Menschentyp, mit dem Fröhlichkeit und Gelassenheit verbunden wurde. Mit dieser Auswahl sollte eine Beeinflussung der Handlungsweise durch eine mentale Disposition des Probanden vermieden werden. Zudem war es wichtig, dass die Versuchspersonen weder von dem Versuchsleiter noch von dem „Schüler“ unbeabsichtigt beeinflusst werden konnten. Der „Lehrer“ konnte selbst bestimmen, zu welchem Zeitpunkt er das Experiment abbrechen wollte. Der Versuchsleiter verhielt sich sachlich, seine Kleidung war in einem unauffälligen Grauton gehalten. Sein Auftreten war bestimmt, aber freundlich.
Die Versuchspersonen wurden über eine Anzeige in der Lokalzeitung von New Haven (Connecticut) gesucht, wobei die angegebene Gage von vier US-Dollar plus 50 Cent Fahrtkosten schon für das bloße Erscheinen in Aussicht gestellt wurde. Das Experiment fand in der Regel in einem Labor der Yale-Universität statt und war in der Anzeige als unter der Leitung von Prof. Stanley Milgram stehend gekennzeichnet.

Folgende Tabelle gibt die Anzahl der Versuchspersonen (Vpn) (n=40), die das Experiment abbrachen, abhängig von der Stärke der letzten applizierten „Schocks“ wieder.

26 Personen gingen in diesem Fall bis zur maximalen Spannung von 450 Volt und nur 14 brachen vorher ab.

Das Ergebnis des ersten Experimentes war derart überraschend, dass Milgram über zwanzig Varianten mit jeweils abweichenden Parametern durchführte. Auch andere Forscher führten Varianten durch.

Eine Variation betraf die Nähe zwischen „Lehrer“ und „Schüler“. Dabei wurden folgende vier experimentelle Bedingungen gestellt:

In der letzten Versuchsanordnung musste der Proband, geschützt durch einen Handschuh, die Hand des „Schülers“ auf eine Metallplatte drücken, die vermeintlich elektrisch geladen war.

Folgende Tabelle gibt den Zusammenhang zwischen einigen variierenden Versuchsbedingungen, dem Anteil der Versuchspersonen (Vpn), die den maximalen Schock versetzten, und die dazugehörige durchschnittliche Schockstärke an.

In der ersten Versuchsreihe waren 65 Prozent der Versuchspersonen bereit, den „Schüler“ mit einem elektrischen Schlag mit den maximalen 450 Volt zu „bestrafen“, allerdings empfanden viele einen starken Gewissenskonflikt. Kein „Lehrer“ brach das Experiment ab, bevor die 300-Volt-Grenze erreicht war. In der vierten Versuchsanordnung, in der die Versuchspersonen den direkten Kontakt zum „Schüler“ hatten, war die erreichte Volt-Stufe am niedrigsten.

In einer Reihe von Versionen des Experiments wurde die Autorität des Versuchsleiters variiert.

Wenn der Versuchsleiter der Bitte des Schülers um Abbruch nachkam und die Versuchsperson zum Abbruch des Experiments aufforderte, so folgte letztere der Anweisung ausnahmslos.

In einer Variante des Versuchs, in der zwei Versuchsleiter den Versuch leiteten und dabei Uneinigkeit über die Fortsetzung des Experimentes vorspielten, wurde das Experiment in allen Fällen von der Versuchsperson abgebrochen.

In einer Reihe von Variationen wurde nachgewiesen, dass beim Widerstreit der Appelle nicht der Widerspruch an sich und nicht der allgemeine Status, sondern die situationsspezifische Autorität ausschlaggebend ist:

Wurden zwei Versuchsleiter eingesetzt, von denen einer die faktische Rolle des Versuchsleiters übernahm, wohingegen der andere Versuchsleiter den „Schüler“ spielte und um Abbrechen bat, gingen 65 Prozent der Teilnehmer bis zum Maximum.
Drängte ein „zweiter Lehrer“ statt des Versuchsleiters auf die Fortsetzung des Experimentes, während der Versuchsleiter neutral blieb, so applizierten verhältnismäßig wenige (25 Prozent) der Versuchspersonen den maximalen Schock.

In einer Variation von Jerry Burger aus dem Jahr 2009 ließen sich durch eine dritte Person ohne Autorität, die ab den ersten Schreien (75V) auf Abbruch des Experiments drängte, nur wenige Versuchspersonen zum Abbrechen bewegen, solange der Versuchsleiter auf Fortsetzung bestand.

Das Ergebnis einer Erweiterung des Experiments im Jahre 1965 war dagegen, dass die Haltung anderer „Lehrer“ einen Einfluss hat. Der Anteil der bedingungslos gehorchenden Probanden nahm stark ab (auf 10 Prozent), sobald zwei weitere vermeintliche „Lehrer“ an dem Experiment teilnahmen, die dem Versuchsleiter Widerstand entgegensetzten. Befürworteten die zwei „Lehrer“ allerdings die Fortführung des Experimentes, so folgten dem 90 Prozent der Probanden.

Bei einer weiteren Variation gab sich der Versuchsleiter nicht als Forscher der renommierten Universität Yale aus, sondern als Wissenschaftler des fiktiven kommerziellen „Research Institute of Bridgeport“, dessen Räume sich in einem heruntergekommenen Bürogebäude eines Geschäftsviertels in Bridgeport (Connecticut) befanden. Hierbei sank die Zahl der Probanden, die die höchste Spannung einsetzten, von 65 Prozent auf 48 Prozent. Dieser Unterschied ist allerdings nicht statistisch signifikant.

Bestandteil einer anderen Variation war es, dass Milgram den Raum verließ und ein Schauspieler, der sich als Proband darstellte, das Experiment leitete. Hier sank der Anteil der Probanden, die bis zur Höchststufe gingen, auf 20 Prozent.

Zudem wurde die Präsenz des Versuchsleiters variiert, der entweder direkt im Raum, nur über Telefon erreichbar oder abwesend sein konnte. Die Instruktionen erfolgten im letzten Fall über ein Tonbandgerät.

Die Abwesenheit des Versuchsleiters bewirkte, dass die Gehorsamsrate dreimal niedriger ausfiel als in der Versuchsanordnung mit seiner Anwesenheit.

In einer Versuchsanordnung, in der Frauen die Elektroschocks austeilen sollten, ergab sich kein signifikanter Unterschied in der Abbruchrate gegenüber Versuchen mit männlichen Probanden:
Im Jahr 2006 wurde das Experiment von Jerry Burger an der Santa Clara University unter modifizierten Bedingungen wiederholt. Es wurden Frauen beteiligt, die maximale Spannung betrug 150 Volt. 70 % der Probanden, die allesamt Milgrams Experiment nicht kannten, gingen bis zur Maximalstärke. Der Unterschied gegenüber Milgrams Original-Experiment (83 % der Probanden gingen bis 150 V) ist statistisch nicht signifikant.

Das Experiment ist in unterschiedlichen Varianten in anderen Ländern wiederholt worden. Die Ergebnisse bestätigten generell einander, was eine kulturübergreifende Gültigkeit der Ergebnisse zeigt.

Alle Versuchspersonen im Originalversuch zeigten einen aufgewühlten Gemütszustand, hatten Gewissenskonflikte und waren aufgeregt. Besonders ein nervöses Lachen fiel Milgram auf, das 35 Prozent der Versuchspersonen von sich gaben. Ein Beobachter beschrieb die emotionale Lage eines „Lehrers“ folgendermaßen:
Es zeigte sich, dass Personen, die die persönliche Verantwortung für ihr Verhalten hoch veranschlagten, das Experiment eher abbrachen und dem Versuchsleiter widersprachen.

Um den ethischen Aspekten gerecht zu werden, erhielten die Probanden nach Abschluss der Versuchsreihe detaillierte Informationen über das Experiment und dessen Ergebnisse. Um eventuelle Langzeitschäden zu erkennen, wurden in einer Stichprobe die Versuchspersonen ein Jahr nach dem Experiment erneut besucht und befragt. Laut Milgram zeigte das Experiment keine schädlichen Auswirkungen auf die Psyche der Versuchspersonen. 83 Prozent der Teilnehmer gaben an, im Nachhinein froh zu sein, an dem Experiment teilgenommen zu haben. Nur ein Proband von Hundert bedauerte seine Teilnahme. Die meisten Teilnehmer gaben an, etwas über sich gelernt zu haben und Autoritätspersonen daher in Zukunft misstrauischer gegenüberstehen zu wollen. Demgegenüber berichten andere Langzeitstudien von Nervenzusammenbrüchen und posttraumatischen Belastungsstörungen, und einzelne Teilnehmer hatten noch vierzig Jahre später, als sie nochmals untersucht wurden, gesagt, sie seien diesen Schock, dieses Trauma nie mehr losgeworden, also ein Trauma, Täter gewesen zu sein. Der Freiburger Psychologe Joachim Bauer folgert, „dass dieses Experiment die betroffenen Personen gegen ihre eigene Intuition, gegen ihre natürlichen mitmenschlichen Instinkte […] dazu gebracht hat, hier der Autorität zu folgen“.

Heutzutage würde ein vergleichbares Experiment von vielen Psychologen als unethisch zurückgewiesen werden, da es die Versuchspersonen einem starken inneren Druck aussetzt und man sie über den wahren Zweck des Experiments täuscht. An vielen Universitäten stellte man als Reaktion auf diesen Versuch ethische Richtlinien über die Zulassung von psychologischen Experimenten auf. Ob das gewonnene Wissen bei Militär und Geheimdiensten Anwendung fand, ist nicht bekannt.

Milgram kommentierte die Ergebnisse seines Experiments so:
Bis heute gilt der Autoritätsgehorsam theoretisch als nur unzureichend geklärt. Obwohl Milgram eine Persönlichkeitsbasis für Autoritätsgehorsam und Verweigerung vermutete, konnte er diese nicht belegen. Stattdessen ging er von zwei Funktionszuständen aus:

Das Experiment zeigte, dass die meisten Versuchspersonen durch die Situation veranlasst wurden, sich an den Anweisungen des Versuchsleiters und nicht an dem Schmerz der Opfer zu orientieren. Die Veranlassung war am wirksamsten, wenn der Versuchsleiter anwesend war, und am wirkungslosesten, wenn die Instruktionen per Tonband oder Telefon erfolgten. Auch die Nähe zum „Schüler“ beeinflusste die Bereitschaft zum Abbruch des Versuches. So gingen ohne Rückmeldung der „Schüler“ praktisch alle Versuchspersonen bis zur höchsten Schockstufe, während beim direkten Kontakt nur noch 30 Prozent die Höchststufe erreichten.

Drei wesentliche methodische Aspekte wurden an dem Versuchsaufbau kritisiert:

Milgram selbst war von den Ergebnissen des Versuchs überrascht. Studenten und Kollegen, denen er von dem Versuch erzählt hatte, schätzten die Zahl derjenigen, die bis zum Maximum gehen, äußerst gering ein. Von Milgram und anderen wurden verschiedene Gründe genannt, die zu solch einer hohen Zahl an gehorsamen Probanden führten. Als mögliche Begründung für das Verhalten der Versuchspersonen kann der Wunsch der Testperson gesehen werden, das freiwillig begonnene Experiment auch tatsächlich abzuschließen und den Erwartungen der Wissenschaftler zu entsprechen (sog. normativer sozialer Einfluss). Die zufällige Auslosung von Lehrer und Schüler schafft zudem eine scheinbar faire Situation. Hinzu kommt, dass die Versuchssituation für die Probanden neu war und deshalb kein erlerntes Handlungsmuster existierte (sog. informativer sozialer Einfluss). Zudem hatten sie kaum Zeit, sich auf die überraschende Situation einzustellen. Ein anderer Erklärungsversuch zielt auf den graduellen Charakter des Experimentes ab, der psychologisch alltäglichen Verhaltensmustern entspricht, diese aber durch die kontinuierliche Steigerung der „Bestrafungsbereitschaft“ sukzessive in Richtung außerordentlicher Verhaltensweisen verschiebe (sog. Dissonanzauflösung). Dies mache die Abschätzung der Folgen für die Probanden schwierig. Dazu passe, dass das Verhalten der Probanden durch die Veränderung situationaler Variablen, etwa der Distanz zum Schüler oder der Anwesenheit des Versuchsleiters, beeinflusst werde, nicht durch das Vorliegen einer charakterlichen Disposition.

Soziologisch ist das Experiment daher als Beleg für die Wirksamkeit der Norm des Gehorsams gesehen worden. Über die Sozialisation erlerne das Individuum Gehorsamkeit und Unterordnung. Zunächst im familiären System, später in der Institution Schule. In beiden gesellschaftlichen Kontexten, die für die Prägung des Individuums entscheidend seien, werde Folgsamkeit und Unterordnung positiv sanktioniert. Die Gehorsamkeitsnorm ist an Institutionen und Individuen gebunden, die über einen hohen sozialen Status und/oder Autorität verfügen. Denn wie sich in den Variationen des Versuches andeutete, sinkt mit dem sozialen Status des Versuchsleiters die Bereitschaft zur Gehorsamsleistung. Insbesondere wenn die Autorität in einen bürokratischen Prozess eingebunden ist, der die Delegation der Verantwortung auf eine Institution ermöglicht, steigt die Chance auf Gehorsam selbst bei Befehlen, die als unmoralisch empfunden werden.

Das Experiment wurde vielfach als Beleg dafür verstanden, dass fast jeder Mensch unter bestimmten Bedingungen bereit ist, nicht seinem Gewissen zu folgen, sondern einer Autorität. Daher wird es zur Erklärung der Frage herangezogen, warum Menschen foltern oder Kriegsverbrechen begehen. Wegen seiner spektakulären Ergebnisse wurde das Experiment in einer breiten Öffentlichkeit wahrgenommen. Die New York Times titelte zum Beispiel: „Fünfundsechzig Prozent folgen in einem Test blind dem Befehl, Schmerzen zuzufügen“. Die Times erkennt die Gefahr einer ungebremsten Gehorsamsbereitschaft an und sieht in dem Experiment eine Erklärung für die Verbrechen der Nationalsozialisten und amerikanische Gräueltaten in Vietnam. Andere Blätter kritisieren Milgram und die Yale-Universität für die Zerreißprobe, vor die sie die Probanden stellten.

Auch gab es sehr unterschiedliche Interpretationen der Ergebnisse und der konditionierenden Faktoren. Erich Fromm etwa behauptete, Grund für die Bereitschaft, dem Versuchsleiter zu gehorchen, sei das besonders hohe Ansehen, das die Wissenschaft als Institution in Amerika besäße. Das entscheidende Ergebnis sei nicht die Zahl der Teilnehmer, die die Schüler mit den höchsten Spannungen bestraften, sondern der bei fast allen Teilnehmern beobachtbare ausgeprägte Gewissenskonflikt. Die Zahl der Teilnehmer ohne Gewissenskonflikt sei bei Milgram jedoch nicht genannt. Fromm sieht die Berichte über die innere Aufgewühltheit und das Leiden der Probanden beim Handeln gegen das eigene Gewissen als Beleg für die Stärke des moralischen Bewusstseins.

Arno Gruen deutet die psychosomatischen Reaktionen der Befragten als ein Zeichen der Entfremdung.

Der US-amerikanische Evolutionsbiologe Marc Hauser sieht in dem Experiment eine Bestätigung seiner, in dem Buch "Moral Minds" dargelegten Theorie, dass das menschliche Gehirn evolutionär veranlagte Kapazitäten besitzt, Autorität zu folgen, wie sie auch bei Primaten zu finden sind.

Aus dem Jahr 1973 stammt ein Theaterstück des britischen Autors Dannie Abse mit dem Titel "The Dogs of Pavlov", das durch die Untersuchung inspiriert ist.

1976 sendete die CBS einen Film namens "The Tenth Level", in dem William Shatner einen an Milgram angelehnten Charakter spielte, der ein ähnliches Experiment durchführte.

Regisseur Henri Verneuil baute das Milgram-Experiment in seinen Film "I wie Ikarus" aus dem Jahr 1979 ein. Vordergründig handelt der Film von den Geschehnissen rund um einen Präsidentenmord in einem imaginären Staat; Parallelen zum Attentat auf John F. Kennedy waren wohl erwünscht.

Die deutsche Fernseh-Dokumentation "Abraham – Ein Versuch" entstand 1970 an der Forschungsstelle für Psychopathologie und Psychotherapie der Max-Planck-Gesellschaft in München. Sie zeichnet das deutsche Nachfolge-Experiment optisch in allen Einzelheiten nach. Die Ausstrahlung sorgte gerade im Zusammenhang mit der deutschen Geschichte für Diskussionen.

Im Jahr 1986 nahm der Musiker Peter Gabriel, der Milgram bewunderte, ein Lied mit dem Titel "We Do What We’re Told (Milgram’s 37)" auf.

Der preisgekrönte Kurzfilm "Atrocity" (2005) spielt das Experiment nach.

Der britische Zauberkünstler und Mentalist Derren Brown wendet in einer 2006 ausgestrahlten TV-Sendung („The Heist“) das Milgram-Experiment an, um Kandidaten auszusortieren, die mental dazu beeinflusst werden sollen, einen Raubüberfall zu begehen.

In der Serie Malcolm mittendrin wird ein ähnliches Experiment durchgeführt, mit Verweis auf das Milgram-Experiment.

In der Episode "Das Böse – steckt der Teufel in jedem von uns?" der Reihe Galileo Mystery demonstriert ProSieben 2008 unter anderem das Milgram-Experiment.

2008 nennt sich Folge 17 von Staffel 9 der amerikanischen Krimi-Serie Law and Order - Special Victims Unit „Autorität“. Darin geht es um einen Mann, der sich am Telefon als „Detective Milgram“ ausgibt und Menschen durch diese fingierten Anrufe dazu bringt, unfreiwillig u.a. junge Frauen sexuell zu belästigen. Er wird freigesprochen und beginnt mit einer wachsenden Anhängerschaft gegen blinden Gehorsam zu demonstrieren. Auch zwei Detectives müssen unfreiwillig an einer Abwandlung des Milgram-Experiments teilnehmen.

Im Frühjahr 2009 wurde das Experiment unter Nutzung der „Autorität des Fernsehens“ statt der der Wissenschaft im Rahmen einer vermeintlichen Spielshow in Frankreich wiederholt und aufgezeichnet. Der Film von Christophe Nick wurde am 18. März 2010 erstmals im Abendprogramm (mit dem Vermerk: nicht für Kinder unter 12 Jahren) auf dem Fernsehsender France 2 mit anschließender Diskussionsrunde ausgestrahlt. In dem Fernsehexperiment gingen 80 Prozent der Teilnehmer bis zur höchsten Bestrafungsstufe.

Der zweite Track des 2009 erschienenen Albums "InBetweenTheLines" der französischen Ska-Punk Band P.O. Box trägt den Namen "So Milgram knew it".

2009 erschien auch das Album "Avoid The Light" der Postrock-Band Long Distance Calling, auf dem ein Lied den Titel „I Know You, Stanley Milgram!“ trägt.

In der Dokumentation "Entdecke! Das Böse in uns" ("US-Original: Curiosity: How Evil Are You?") des Discovery Channels 2011 wird Bezug auf das Experiment genommen und Stanley Milgram im Experiment sowie Interview gezeigt.

Im Lied "Caesar" von I Blame Coco findet sich der Satz „It's the Milgram device all over again“, der auf das Experiment anspielt.

In der Folge „The Mutilation of the Master Manipulator“ der TV-Serie Bones (Staffel 10, Episode 9, Erstausstrahlung am 4. Dezember 2014) wird ein Psychologie-Professor ermordet, der Milgram-Experimente durchführte. Hauptverdächtiger ist zunächst ein Versuchsteilnehmer, der im Experiment glaubte, seinem „Schüler“ einen tödlichen Stromschlag versetzt zu haben.





</doc>
<doc id="3474" url="https://de.wikipedia.org/wiki?curid=3474" title="Myanmar">
Myanmar

Myanmar, amtlich Republik der Union Myanmar (, "Pyidaunzu Thanmăda Myăma Nainngandaw", []), allgemeinsprachlich Birma oder Burma, ist ein Staat in Südostasien und grenzt an Thailand, Laos, die Volksrepublik China, Indien, Bangladesch und den Golf von Bengalen. Das Land stand seit 1962 unter einer Militärherrschaft, bis diese am 4. Februar 2011 einen zivilen Präsidenten als Staatsoberhaupt einsetzte.

Zur Aussprache von "Myanmar" gibt Duden [] an, also Betonung auf dem ersten a. Im Deutschen ist jedoch die Betonung auf dem zweiten a üblich. Im Englischen variiert die Aussprache von "Myanmar" erheblich. Myanmar ist im deutschen Sprachraum auch unter der früheren Bezeichnung "Birma" und im Vereinigten Königreich, in Australien und den USA nach wie vor auch als "Burma" bekannt.

"Burma" und "Myanmar" sind eigentlich zwei Varianten derselben Bezeichnung. Die Schreibweisen "Burma" (englisch ausgesprochen) und davon abgeleitet "Birma" (in Deutschland) entsprechen dem birmanischen Namen "Bama" [] mit verhältnismäßig dumpfem „a“ als erstem Vokal. "Bama" und "Myanma" sind seit jeher die einheimischen Bezeichnungen der größten Bevölkerungsgruppe, der Bamar, für sich selbst und für ihr Land. Der Übergang von „B“ zu „M“ ist fließend. Dazu kommen weitere Varianten je nach Dialekt. Die Form "Myanma(r)" entstammt der Schriftsprache und findet sich daher eher in historischen Dokumenten, während "Bama" umgangssprachlich verwendet wird. Vermutlich entstand "Bama" durch vereinfachte Aussprache aus "Myanma".

Das -r in "Myanmar" wird im Birmanischen nicht gesprochen und auch nicht geschrieben: "Myanma". Das -r wurde für die Schreibung im Englischen hinzugefügt, um die Länge der letzten Silbe anzuzeigen. Auch in "Burma" (englisch ausgesprochen) repräsentiert das r keinen zusätzlichen Konsonanten. Die Aussprache von "Burma" (englisch) ist tatsächlich sehr ähnlich wie die von "Bama" (birmanisch).

Die Etymologie des Namens ist ungeklärt. Als Bezeichnung des Volkes taucht der Name bereits in Inschriften aus dem 12. Jahrhundert auf: zuerst in einer Mon-Inschrift aus dem Jahr 1102 (dort in der Form "Mirma"), dann im Jahr 1190 erstmals in einer burmesischen Inschrift (dort schon in der aktuellen Form "Mranma" [မြန်မာ], die auch heute noch benutzt wird – der scheinbare Wechsel von "r" zu "y" betrifft nur die Transkription, nicht die birmanische Rechtschreibung).

Seit den 1920er Jahren hatte es Bestrebungen gegeben, einen einheitlichen Begriff für alle im jetzigen Myanmar beheimateten Volksgruppen zu finden. So wurde mehrmals "Bama" durch "Myanma" ersetzt und umgekehrt.

Die offizielle Umbenennung des Landes in „Republik der Union Myanmar“ "(Pyidaunzu Thanmăda Myăma Nainngandaw)" durch das Militär erfolgte durch das Gesetz Nr. 15/89 vom 18. Juni 1989. Dies war in erster Linie ein Vorhaben mit Außenwirkung. Das Land sollte sich als selbstbewusster Staat präsentieren, der die Kolonialzeit endgültig überwunden hat. Durch das Gesetz Nr. 15/89 wurde auch die offizielle Schreibweise vieler Ortschaften neu bestimmt. Hierfür wurden die Namen in ihrer ursprünglichen Form, also ohne Veränderungen durch kolonialen Einfluss, und nach ihrer Aussprache ins lateinische Alphabet transkribiert.

Die Vereinten Nationen übernahmen den neuen Namen des Staates wenige Tage nach der Verkündung durch das Militär. Dem sind mittlerweile viele Staaten gefolgt. Die Vereinigten Staaten, Australien sowie weitere Staaten und nichtstaatliche Organisationen halten als Zeichen ihrer Missbilligung des Regimes am Namen "Burma" fest. Auch die Friedensnobelpreisträgerin Aung San Suu Kyi sprach sich 1996 in einem Interview für das Magazin "Marie Claire" für die Beibehaltung von "Burma" aus, zum einen wegen der fehlenden Mitwirkung des Volkes, zum anderen, da der Begriff "Myanmar" eben nicht die Vielfalt der Volksgruppen im Lande widerspiegele.

Die deutschschweizerischen und österreichischen Zeitungen verwenden vorwiegend "Burma", während sich die deutschsprachigen Agenturen auf die Bezeichnung "Birma" geeinigt haben (Stand 2007). Mehrere deutsche Medien, darunter Spiegel und FAZ, verwenden dennoch überwiegend die Variante "Burma" (neben "Birma" und "Myanmar"). In der DDR wurde die Namensform "Burma" verwendet. Eine Person aus Myanmar wird "Burmese" beziehungsweise "Burmesin" genannt, die Begriffe "Myanmare" beziehungsweise "Myanmarin" werden kaum verwendet.

Die Einwohner Myanmars bezeichnen ihren Staat meist kurz "Myanma Naingngan" („Myanmarischer Staat“).

Myanmar grenzt im Norden und Osten an die Volksrepublik China sowie weiter südlich im Osten an Laos und Thailand und im Süden an den Indischen Ozean. Der südlichste Teil Myanmars liegt auf der Malaiischen Halbinsel. Das Andamanische Meer trennt Myanmar von den westlich gelegenen indischen Inseln der Andamanen und Nikobaren. Im Westen grenzt Myanmar an Bangladesch und die indischen Bundesstaaten Mizoram, Manipur, Nagaland und Arunachal Pradesh.

Im Norden liegt das Kachin-Bergland, ein südlicher Ausläufer des Himalaya, und an der Grenze Myanmar–Indien–China liegt der Hkakabo Razi. Mit seinen ist er der höchste Berg Südostasiens. Entlang der Küste am Golf von Bengalen erstrecken sich Sumpfgebiete, dahinter liegt das Arakan-Joma-Gebirge mit bis zu hohen Bergen. Im Osten des Landes liegt das Shan-Hochland mit Erhebungen von bis zu . In der Mitte des Landes, entlang des Irrawaddy, liegt Zentralmyanmar mit seinen fruchtbaren Böden. Die bedeutendsten Flüsse neben dem Irrawaddy sind Thanlwin, Sittaung, Chindwin und Mekong.

40 % der Fläche werden von Primärwald bedeckt, wobei die Waldfläche jährlich um 1,2 % abnimmt. Vor der Westküste der Malaiischen Halbinsel liegt eine abgesunkene Gebirgslandschaft, der Mergui-Archipel mit rund 800 Inseln, eine noch weitgehend unberührte Inselgruppe.

Die größte Stadt des Landes ist Yangon (Rangun) mit rund 5.160.000 Einwohnern (2014). Rangun war früher auch die Hauptstadt von Myanmar (seit 2005 ist Naypyidaw die Hauptstadt). Weitere große Städte sind Mandalay, Mawlamyaing, Bago und Pathein.

Myanmar befindet sich – mit Ausnahme des äußersten Nordens – im Einflussbereich des indischen Monsuns. Durch das Relief bedingt sind die Ausprägungen des Monsuns in den einzelnen Landesteilen unterschiedlich.

Im Wesentlichen lassen sich drei Jahreszeiten unterscheiden:

Das Bevölkerungswachstum hat sich in den letzten Jahren abgeschwächt. Lag es 1998 noch bei 1,5 Prozent, sank es über 1,2 Prozent 2002 auf 1,1 Prozent 2006 und 2011. Bei der letzten Volkszählung 2014 hatte Myanmer 51.486.253 Einwohner, die sich auf folgende ethnische Gruppen aufteilen: Bamar 69 Prozent, Shan 8,5 Prozent, christliche Karen 6,2 Prozent, muslimische Rohingya 4,5 Prozent, Mon 2,4 Prozent, Chin 2,2 Prozent, Kachin 1,4 Prozent, Inder 1 Prozent, Han 1–2 Prozent. Insgesamt gibt es rund eine Million Umgesiedelte im eigenen Land.

Die Lebenserwartung stieg von 57,2 Jahren 2002 auf 66,6 Jahre 2016, die Säuglingssterblichkeit sank von 77 pro Tausend Neugeborenen 2002 auf 49 pro Tausend 2011. Etwa 0,6–2 Prozent der Bevölkerung werden als HIV-positiv geschätzt. Die Alphabetisierungsrate betrug 2015 93,1 %. Die Fertilität pro Frau betrug 2,0 Kinder.

Quelle: UN

Myanmar ist ein Vielvölkerstaat mit rund 52 Millionen Einwohnern, die 135 verschiedenen Ethnien angehören. Die größte Ethnie ist mit 70 % Bevölkerungsanteil die der Birmanen "(Bamar)". Die Shan sind die zweitgrößte Volksgruppe (8,5 %) und leben hauptsächlich im Shan-Staat des Landes, in Gebieten ab etwa 1000 Metern Höhe.

6,2 % stellen die überwiegend christlichen Karen und 2,4 % gehören zu den Mon. Die Padaung gehören zur Sprachgruppe der Mon-Khmer und umfassen etwa 150.000 Personen. Sie leben im südlichen Kachin- und im Shanstaat. 2,2 % sind Chin (Tschin) und 1,4 % Kachin.

Hauptsächlich im Rakhaing-Staat leben etwa 730.000 Arakanesen. Andere Quellen geben ihren Anteil an der Gesamtbevölkerung sogar mit 4 % an. Ebenfalls im Rakhaing-Staat leben die muslimischen Rohingya, denen der Status als Volksgruppe verwehrt wird. Die Rohingya werden vom Staat nicht als ethnische Gruppe anerkannt, erhalten nicht die myanmarische Staatsangehörigkeit und gelten laut den Vereinten Nationen als „am stärksten verfolgte Minderheit der Welt“. Sie sprechen eine eng mit dem Bengali verwandte indogermanische Sprache. Viele von ihnen sind nach Bangladesch geflohen.

Ferner stellen Han 1–2 % und Inder 1 % der Bevölkerung. Die einzelnen Völker sprechen ihre eigenen Sprachen, Englisch ist Handelssprache. Amtssprache ist die birmanische Sprache.

Anteile der Religionen an der Bevölkerung:

Die am weitesten verbreitete Religion in Myanmar ist der Buddhismus. Einige der berühmtesten buddhistischen Kunstwerke (Statuen) im asiatischen Raum befinden sich hier. Vorherrschend ist die frühbuddhistische Theravada-Schule, die im 20. Jahrhundert auch maßgeblichen Einfluss auf die Buddhismus-Rezeption im Westen hatte. So fußen viele der Standardwerke der Vipassana-Meditation (zum Beispiel Nyanaponika: „Geistesschulung durch Achtsamkeit“) auf den Lehren birmanischer Dharma-Meister wie Mahasi Sayadaw, Chanmyay Sayadaw U Janaka, Ledi Sayadaw oder Sayadaw U Pandita. Zu den wichtigsten Heiligtümern zählen vor allem die Shwedagon-Pagode in Rangun, der Goldene Fels südöstlich von Bago und der Mount Popa in der Nähe von Bagan.

In der buddhistischen Volksreligion ist der Geisterglaube an die Nats weit verbreitet. Nats haben menschliche Züge, Gefühle, Wünsche und Bedürfnisse, sind gut, hilfreich oder böse und gehässig, vor allem aber mächtig. Sie können, wenn erzürnt, großes Unheil bringen. Während der ihnen gewidmeten Feste werden sie durch "Nat-Gadaw", weibliche Medien (häufig auch Transvestiten) in Trance und Tanz verkörpert. Bei den niederen Nats ist der Bezug zu animistischen Vorstellungen deutlich, denn sie leben in oder bei alten Bäumen oder Steinen, auf Bergen oder an Flüssen. Häufig haben sie nichtmenschliche Gestalt. Die an Bäumen, Feldern, Gewässern oder in Dörfern errichteten Nat-Schreine "(nat-sin)" ähneln den Geisterhäuschen "(san phra phum)" in Thailand.

Zum Christentum bekennen sich nach offiziellen Angaben 4 % der Bevölkerung, vor allem in den Volksgruppen der Chin und der Karen, die einem im Jahre 2007 bekannt gewordenen Regierungsprogramm „zur Zerstörung der christlichen Religion in Myanmar“ zufolge, systematisch vertrieben werden sollen.

Zum Islam gehören vor allem die Angehörigen der Volksgruppe der Rohingya.

Myanmar ist seit einigen Jahren eines der Länder mit besonders hoher AIDS-Zuwachsrate, die von der Junta lange bestritten wurde, was das Problem verschlimmerte. Ursachen sind vor allem die Prostitution, besonders in Rangun, und die verbreitete, traditionelle Drogenabhängigkeit, die infolge der durch den jahrzehntelangen Bürgerkrieg eingetretenen gesellschaftlichen Zerrüttung noch verschärft wird. Bei der Ernährung der Bevölkerung konnten starke Fortschritte gemacht werden. 2015 waren 16,9 % der Bevölkerung unterernährt. Im Jahr 2000 waren es noch 48,1 % der Bevölkerung. 

Entwicklung der Lebenserwartung
Quelle: UN

Der Bildungssektor ist in Myanmar, das eine ausgesprochene Bildungstradition hat, unter dem Militärregime besonders stark geschrumpft. Mehrere Hochschulen wurden vorübergehend oder ganz geschlossen, vor allem aus Angst vor Studentenaufständen und vor der Kritik einer intellektuellen Elite. Lernfreiheit und freie Fächerwahl bestehen nicht, dafür ist es möglich, gewisse Fächer per Fernkurs zu studieren. Auch die Verbreitung von Büchern im universitären Bereich ist stark beschränkt, so kann etwa ein Medizinstudent keine Geschichtsbücher ausleihen. 2015 konnten 93,1 % der Bevölkerung lesen und schreiben.

In Myanmar galt offiziell noch bis 2013 das angloamerikanische Maßsystem, das weltweit sonst nur von den USA und Liberia verwendet wird. 2013 wurde der Übergang zum metrischen System beschlossen.

Im 11. Jahrhundert gründete König Anawrahta das erste birmanische Reich. Im 19. Jahrhundert fiel Birma nach mehreren Kriegen unter britische Herrschaft. Der letzte König von Birma wurde mit seiner Familie durch die britische Besatzung ins Exil nach Indien geschickt, wo er auch starb. Birma wurde Teil von Britisch-Indien.

Im Zweiten Weltkrieg wurde Birma von Japan okkupiert und ein Marionettenstaat errichtet. Nach Kriegsende 1945 wurde das Land von den Briten besetzt und erneut in ihr Kolonialreich integriert.

1948 wurde Birma in die Unabhängigkeit entlassen. Seither halten bewaffnete Konflikte in verschiedenen Landesteilen an, wo ethnische Minderheiten gewaltsam für mehr Autonomie oder Unabhängigkeit kämpfen. Nach einer kurzen demokratischen Phase bis 1962 wurde Birma von verschiedenen Militärregimen kontrolliert.

Von 1961 bis 1971 war der birmanische Politiker Maha Thray Sithu U Thant der dritte Generalsekretär der Vereinten Nationen. Als es wegen der Weigerung der Regierung Ne Win, ihm ein Staatsbegräbnis auszurichten, in Rangun zu Unruhen kam, wurden diese gewaltsam niedergeschlagen.

Am 18. Oktober 1965 verabschiedete der Revolutionsrat ein Gesetz, nach dem alle Wirtschaftsunternehmen verstaatlicht wurden. Wenig später wurden alle christlichen Missionare zum Ende des Jahres 1966 ausgewiesen.

Am 8. August 1988 gipfelten monatelange Unruhen (8888 Uprising) wegen der Wirtschaftspolitik des Militärs unter Führung von General Ne Win in der gewaltsamen Niederschlagung von Protesten in der Hauptstadt Rangun mit mehreren Tausend Toten. Ein neues Militärregime unter General Saw Maung etablierte sich als "Staatsrat für die Wiederherstellung von Recht und Ordnung (SLORC)." 1989 wurde das Land in "Myanmar" umbenannt. Als 1990 bei demokratischen Wahlen die oppositionelle Nationale Liga für Demokratie (NLD) einen Erdrutschsieg errang, wurden die Wahlen vom Militärregime für ungültig erklärt, und es kam zu einer blutigen Niederschlagung von friedlichen Studentenprotesten. Das Regime blieb an der Macht.

Im November 2005 begann die Regierung mit der Verlegung des Regierungssitzes von Rangun nach Naypyidaw in der Nähe der Stadt Pyinmana (Mandalay-Division). Begründet wurde der Schritt offiziell mit der gegenüber Rangun zentralen Lage der neuen administrativen Hauptstadt. Inoffizielle Spekulationen reichten von der Furcht vor einer ausländischen Invasion vom Meer aus, über Einflüsse von Astrologen auf die Militärmachthaber bis zur Abschottung des Regimes aus Furcht vor möglichen neuen Volksaufständen.

In der Reihe der Kritiker des Regimes erschienen im Dezember 2005 erstmals auch die ASEAN-Staaten. Bereits im März 2005 hatte Myanmar auf die turnusmäßige Übernahme des jährlich wechselnden Vorsitzes innerhalb ASEAN zugunsten der Philippinen verzichtet. Ein von den USA im Weltsicherheitsrat eingebrachter Resolutionsentwurf, der das Militärregime zur Einhaltung der Menschenrechte und zur Freilassung aller politischen Gefangenen auffordern sollte, wurde im Januar 2007 mit den Stimmen der Vetomächte Volksrepublik China und Russland abgelehnt.

Die im August 2003 vom damaligen Premierminister Khin Nyunt verkündete „Road Map“ für den Weg zur Demokratie nahm mit der erneuten Einberufung der Nationalen Versammlung, die eine neue Verfassung erarbeiten sollte, ihren Lauf. Nach knapp zehnmonatigen Beratungen zwischen dem 17. Mai 2004 und dem 3. September 2007 erklärte der Vorsitzende der Kommission für die Einberufung der Nationalen Versammlung, Generalleutnant Thein Sein, dass man sich auf eine neue Verfassung geeinigt habe, die einen ersten Schritt zur Demokratisierung des Landes darstelle. Einen Termin für ein Referendum über den Verfassungsentwurf oder für freie Parlamentswahlen nannte er jedoch nicht.

Am 15. August 2007 wurden sämtliche Subventionen auf Kraftstoffe gestrichen. Die hierdurch auf bis zu 500 Prozent ansteigenden Preise für flüssigen Treibstoff und Gas waren der Anstoß zu Protestdemonstrationen, die sich bis Ende September auf das ganze Land ausweiteten. Sie wurden am 26. September gewaltsam niedergeschlagen. Dabei wurden nach unterschiedlichen Angaben zwischen zehn und mehreren Tausend Mönche und Demonstranten getötet.

Im Februar 2008 setzte die Militärjunta ein Referendum über die neue Verfassung im Mai 2008 an. Nach dem Terminplan sollten demokratische Wahlen 2010 stattfinden.

In der Nacht zum 3. Mai 2008 wurden Teile des Landes durch den Tropensturm Nargis verwüstet. Nach UNO-Schätzungen vom 9. Mai starben 63.000 bis 101.000 Menschen und rund eine Million wurde obdachlos. Nach Regierungsangaben vom 24. Juni starben 84.537 Menschen, 53.836 gelten als vermisst. Die Militärjunta verweigerte Helfern den Zugang zum Irrawaddy-Flussdelta und beschlagnahmte Hilfsgüterlieferungen aus dem Ausland.

Ungeachtet der Katastrophe führte das Regime am 10. Mai 2008 das Verfassungsreferendum wie geplant durch. Lediglich in den am schwersten betroffenen Gebieten wurde der Termin um zwei Wochen verschoben. Nach massiver Wahlfälschung und Einschüchterung verkündete das Militär schließlich eine 92,48-prozentige Zustimmung der wahlberechtigten Bevölkerung zur neuen Verfassung.

Am 7. November 2010 fanden die ersten Wahlen seit 1990 statt, woraufhin am 4. Februar 2011 der vorherige Premierminister Thein Sein zum ersten Präsidenten Myanmars seit 1988 ernannt wurde; dieser ist ein Than Shwe nahestehender General. Während die Parlamentswahlen von 2010 noch von der NLD boykottiert wurden, beteiligte sich die führende Oppositionspartei am 1. April 2012 erstmals seit 1990 wieder bei Parlamentswahlen. Bei den Nachwahlen wurden 45 der 664 Sitze in der Volksversammlung neu vergeben. 43 dieser 45 Sitze erhielt die Opposition mit der Friedensnobelpreisträgerin Aung San Suu Kyi.

Am 18. September 1988 war die bisherige Verfassung der Sozialistischen Republik von 1974 außer Kraft gesetzt worden. Bis 2010 galt dieses Regierungssystem:

Seit 1993 wurde über eine neue Verfassung beraten. Mit Verweis auf die fehlende Verfassung hatte die Militärregierung jahrelang freie Wahlen verhindert. Im Regierungsentwurf für eine neue Verfassung wurde als neuer offizieller Name "Pyidaungsu Thamada Myanmar Naing-Ngan Daw (Republik der Union von Myanmar)" vorgeschlagen. Außerdem wurde eine Änderung von Staatsflagge und Staatssiegel geplant. Die Diskussionen hierüber verzögerten die Fertigstellung der Verfassung bis zum 3. September 2007. Die Verfassung wurde im Mai 2008 zur Abstimmung vorgelegt. Diese schreibt immer noch Vorrechte des Militärs fest, etwa, dass ein Viertel der Parlamentsmandate an Militärangehörige vergeben werden müssen.<ref name="/AA_Laenderinfos_Myanmar_Innenpolitik">Länderinformation Myanmar: Innenpolitik auswaertiges-amt.de</ref> Nach offizieller Lesart wurde diese Verfassung mit 92,48 Prozent Jastimmen angenommen. Erst am 22. Oktober 2010, rund zwei Wochen vor den für den 7. November 2010 angesetzten Wahlen, wurde der o. a. Namensvorschlag umgesetzt, zusätzlich wurden Flagge und Wappen geändert.

Durch ein im März 2010 von der Militärregierung veröffentlichtes Wahlgesetz war die führende NLD-Politikerin und Friedensnobelpreisträgerin Aung San Suu Kyi von den Parlamentswahlen am 7. November 2010 ausgeschlossen worden. Das Gesetz besagt, dass Strafgefangene, zu denen in Myanmar faktisch auch politische Gefangene gezählt werden, nicht Mitglieder einer politischen Partei sein dürfen. Gleichzeitig wurde die NLD dazu gezwungen, Aung San Suu Kyi aus der Partei auszuschließen, sollte sie an den Wahlen teilnehmen wollen. Obwohl das neue Gesetz auf internationale Kritik stieß, annullierte die Militärregierung gleichzeitig das Ergebnis der Parlamentswahl aus dem Jahr 1990, da es nicht mehr mit der neuen Verfassung übereinstimme. Die NLD hatte die Wahl im Jahr 1990 mit großer Mehrheit für sich entscheiden können. Erst auf internationalen Druck entließ die Militärregierung Myanmars Aung San Suu Kyi am 13. November 2010 aus ihrem insgesamt 15 Jahre währenden Hausarrest.

Am 7. November 2010 wurden erstmals auf Grundlage der Verfassung von 2010 Allgemeine Wahlen 2010 in Myanmar durchgeführt. Die Union Solidarity and Development Party stellte vom 4. Februar 2011 bis zum 15. März 2016 den Staatspräsidenten Thein Sein. Das Amt des Ministerpräsidenten ist bislang unbesetzt, Than Shwe hat somit kein politisches Amt mehr inne. In der Verfassung von 2008 ist das Amt des Ministerpräsidenten nicht mehr vorgesehen. Der Staatspräsident wird durch ein spezielles Wahlkollegium bestimmt, das aus drei verschiedenen Parlamentariergruppen besteht. Es handelt sich somit im weitesten Sinne um ein System parlamentsgebundener Exekutivgewalt.

Am 15. März 2016 wurde Htin Kyaw zum Staatspräsidenten gewählt.

Seit April 2011 sind in Myanmar die Anfänge eines Demokratisierungsprozesses zu verzeichnen. Anlass und Nahziel dieser neuen Politik ist die Lockerung der internationalen Handelsblockaden, die das Land in der Vergangenheit stark isoliert hatten. Unter anderem wurden nach Informationen des Europäischen Auswärtigen Diensts die große Mehrzahl politischer Gefangener freigelassen, neue Vorschriften im Arbeits- und Investitionsrecht erlassen, die Kontrolle der Medien gelockert und mehr als 120 Gewerkschaften genehmigt. Im Juli 2013 kündigte Thein Sein die Freilassung aller politischen Gefangenen bis zum Jahresende an, die ersten der etwa 150 Gefangenen kamen daraufhin eine Woche später frei.

Am 1. April 2012 fanden Nachwahlen statt, nachdem zahlreiche Abgeordnete Regierungsämter übernommen und insgesamt 157 Kandidaten von 17 Parteien sich für die Nachbesetzung der freigewordenen 45 Parlamentssitze beworben hatten. Anteilmäßig waren lediglich sechs Millionen Wähler des 54-Millionen-Volks zu diesen Nachwahlen wahlberechtigt. An der Zusammensetzung des Parlaments hat die Nachwahl kaum etwas geändert, da die neu gewählten Abgeordneten mit 45 von insgesamt 664 Abgeordneten nur sieben Prozent aller Mandate innehaben. Die vom Militär dominierte Union Solidarity and Development Party hat rechnerisch nach wie vor eine klare Mehrheit. Allerdings zog Aung San Suu Kyi als Spitzenkandidatin der Partei NLD im Ergebnis der Nachwahlen erstmals in das Parlament ein – nicht ohne sehr kritisch darauf hinzuweisen, dass die neue Verfassung immer noch Vorrechte des Militärs festschreibt, etwa dass ein Viertel der Parlamentsmandate an Militärangehörige vergeben werden müssen. Suu Kyi wurde auch Vorsitzende des Unterausschusses für Rechtsstaatlichkeit. Die NLD gewann laut Medienberichten in 112 von 129 Wahllokalen die meisten Stimmen. Dies wird vielerorts als Aufbruchzeichen für eine weitere Demokratisierung gewertet.

Bei der folgenden Wahl am 8. November 2015 gewann die NLD 77 % der Sitze im Parlament. Aung San Suu Kyi konnte jedoch nicht selbst Präsidentin werden, da ihre beiden Söhne britische Pässe haben. Im März 2016 wurde Htin Kyaw, ein enger Vertrauter von Aung San Suu Kyi, zum neuen Präsidenten gewählt. Aung San Suu Kyi wurde in der Folge zur Außenministerin ernannt.

Menschenrechtsorganisationen werfen Regierung und Militär Menschenrechtsverstöße wie Zwangsarbeit, Zwangsräumung von Dörfern, Folter, Vergewaltigungen und Einsatz von Kindersoldaten in den bis heute andauernden Kämpfen gegen Aufständische vor, vor allem gegen ethnische Minderheiten wie die Karen. Auch manche Rebellengruppen sollen Kinder rekrutiert und Zivilisten zur Zwangsarbeit verpflichtet haben.

Ende Juni 2007 hat das Internationale Komitee vom Roten Kreuz (IKRK) öffentlich der Regierung schwere Menschenrechtsverletzungen vorgeworfen. Normalerweise äußert das IKRK seine Kritik vertraulich, doch da die Machthaber Myanmars nicht auf die Vorwürfe reagierten, habe man die Vorwürfe publik gemacht. Neben der Misshandlung von Gefangenen wurde vor allem die Verfolgung der Karen kritisiert.

Der muslimischen Minderheit in Myanmar, den Rohingya, wird die Staatsangehörigkeit verwehrt und ihre Bürgerrechte werden stark eingeschränkt. Von den Vereinten Nationen werden die Rohingya als eine der am stärksten verfolgten Minderheiten der Welt bezeichnet. Im Mai 2013 ordnete die Regierung „zum Abbau von ethnischen Spannungen“ an, dass die Rohingya im Rakhaing-Staat nicht mehr als zwei Kinder haben dürfen, da ihre nach Angaben der Regierung zehnmal so hohe Geburtenrate angeblich die buddhistische Mehrheit in eine Minderheit verwandeln könne. Dem gleichen Ziel diente das Verbot der Polygamie für die Muslime in den an Bangladesch angrenzenden Ortschaften Buthidaung und Maundaw.

Die Menschenrechtsorganisation Human Rights Watch berichtete auch 2017 über schwere Menschenrechtsverletzungen durch die Sicherheitskräfte. Das Militär führe ausgedehnte und systematische Angriffe gegen Rohingya und gehe dabei mit Zwangsabschiebungen, Mord und Vergewaltigung gegen die Zivilbevölkerung vor. Im Zuge von Militäroperationen gegen Rohingya-Dörfer kam es zu hunderten Tötungen und Vergewaltigungen. Brad Adams, Direktor der Asien-Abteilung von Human Rights Watch berichtete: „Die Soldaten töteten und vergewaltigten Hunderte Rohingya mit einer grausamen Effizienz, die sich nur durch vorherige Planung erreichen lässt.“

Im Dezember 2017 wurden die Reuters-Reporter Wa Lone und Kyaw Soe Oo verhaftet, die an einem Bericht über das Vorgehen der Armee gegen die muslimische Minderheit der Rohingya gearbeitet hatten. Die Behörden werfen ihnen „illegale Informationsbeschaffung mit dem Ziel der Weitergabe an ausländische Medien“ vor.

In der Rangliste 2011 von „Reporter ohne Grenzen“ zur Lage der Pressefreiheit lag Myanmar auf Platz 169 von 179, verbesserte sich aber bis 2014 auf Rang 145 von 180 und stand 2017 auf Rang 131 von 180. In Myanmar sitzen 5 Journalisten in Haft. Im August 2012 wurden die strengen Zensurregelungen gelockert. Veröffentlichungen müssen demnach nicht mehr vorab von der staatlichen Prüfbehörde genehmigt werden, die Zensurbehörde wurde allerdings nicht aufgelöst und publizierte Texte müssen nachträglich auch weiterhin bei der Prüfstelle eingereicht werden. Am 1. April 2013 erschienen erstmals seit 1962 wieder private Zeitungen in Myanmar, zuvor hatte das Informationsministerium acht von 14 Bewerbungen um eine Lizenz stattgegeben.

Aufgrund seiner Lage befindet sich Myanmar seit seiner Unabhängigkeit in einer besonderen Beziehung zu seinen großen Nachbarn Indien und China.

Myanmar unterhält seit 1988 eine besondere Beziehung zur Volksrepublik China, es besteht jedoch keine formelle Allianz. Die Volksrepublik China hat Überlandstraßen zwischen der chinesischen Grenze und dem Zentrum Myanmars in der Mandalay-Ebene finanziert und deren Ausbau logistisch unterstützt. Diese Straßen sind panzertauglich konstruiert und sichern den Chinesen den strategischen Zugang zum Indischen Ozean. Lange Zeit war auch in offiziellen Stellungnahmen die Auffassung verbreitet, China betreibe seit 1994 auf den Großen und Kleinen Kokosinseln nördlich der indischen Andamanen und Nikobaren einen Stützpunkt für Fernmelde- und Elektronische Aufklärung (SIGINT) und einen Flugplatz.

Die Streitkräfte von Myanmar, "Tatmadaw" (birmesisch: တပ်မတော်) genannt, spielen seit der Staatsgründung eine entscheidende politische Rolle. Sie umfassen rund 428.000 Soldaten und 72.000 paramilitärische Kräfte wie die Grenztruppen und Spezialpolizeikräfte. Der Verteidigungsetat belief sich 2012 auf etwa 2,5 Milliarden US-Dollar und entsprach 4,8 Prozent des Bruttoinlandsprodukts des Landes.

Nach dem Dritten Britisch-Birmanischen Krieg im Jahre 1885 wurde Birma vollständig von Großbritannien unterworfen und am 1. Januar 1886 Teil von Britisch-Indien. Der letzte König von Birma, Thibaw Min, wurde mit seiner Familie durch die britische Besatzung ins Exil nach Indien geschickt. Ende der 1930er Jahre wurde die britische Kolonialstreitmacht in Birma, die bisher Teil der British Indian Army gewesen war, in einer einzigen Einheit, der "Birma-Armee", die aus britischen, indischen und nur einer geringeren Zahl burmesischen Soldaten bestand, zusammengefasst. Der Ausbruch des Zweiten Weltkriegs 1939 wurde von birmesischen Nationalisten als Gelegenheit gesehen, Zugeständnisse von der britischen Kolonialmacht im Gegenzug für die Unterstützung der Kriegsbemühungen zu erzwingen. Andere, darunter die antikolonialistische "Thakin"-Bewegung unter Kodaw Hmaing, lehnten jegliche Unterstützung für den Krieg ab und warteten bereits auf eine bevorstehende japanische Invasion, welche Birma von der britischen Herrschaft befreien sollte. Januar 1942 bis Juli 1942 wurde Birma tatsächlich von Japan erobert und des Marionettenstaat des Ba Maw errichtet. Die "Burma Independence Army" (BIA) war eine paramilitärische Widerstandsorganisation, die im Laufe des Feldzuges gegen die britischen Truppen an der Seite der Japaner gekämpft hatte. Unter der japanischen Besatzung wurde sie nun als "Burma Defence Army" (BDA) neu organisiert, ihr wurde der Status einer nationalen Armee zugewiesen. Die Verbände der BDA, die im August 1942 bis zu 18.000 Mann stark geworden war, standen unter dem Befehl der Thirty Comrades, eine Elite der birmesischen Widerstandsführer die von den japanischen Truppen ausgebildet worden waren. Der wichtigste der Thirty Comrades war Aung San, der den militärischen Befehl über die Truppen der BDA übernahm. Er verhandelte Anfang 1945 mit den Alliierten und daraufhin wechselte die Seiten. Dennoch versuchten nach Kriegsende die Briten, ihre Kolonialherrschaft wieder herzustellen.

Bereits 1948 wurde Birma aber in die Unabhängigkeit entlassen. Seither halten bewaffnete Konflikte in verschiedenen Landesteilen an, wo ethnische Minderheiten gegen das Militär gewaltsam für mehr Autonomie oder Unabhängigkeit kämpfen. Nach einer kurzen demokratischen Phase bis 1962 wurde Birma von verschiedenen Militärregimen kontrolliert. General Ne Win war von 1962 bis 1974 als Vorsitzender des Revolutionsrates der Union Staatsoberhaupt, außerdem Premierminister; nach der Einführung einer neuen Verfassung wurde er von 1974 bis 1981 Präsident. General Saw Maung putschte sich am 18. September 1988 mit der Unterstützung Ne Wins an die Macht, entmachtete den wenige Wochen vorher vom Parlament gewählten zivilen Staatspräsidenten Maung Maung und ließ am gleichen Tag die Demonstrationen der Demokratiebewegung niederschlagen und somit die Militärherrschaft wiederherstellen. Seit April 2011 waren in Myanmar die Anfänge eines Demokratisierungsprozesses zu verzeichnen, die Macht des Militärs wurde zurückgedrängt. Im März 2016 wurde Htin Kyaw, ein enger Vertrauter der bisherigen Oppositionsführerin Aung San Suu Kyi, zum neuen Präsidenten gewählt. Aung San Suu Kyi wurde in der Folge zur Außenministerin ernannt. Im Hintergrund hat das Militär aber weiter großen Einfluss. Wer das harte Vorgehen der Armee gegen die muslimische Rohingya-Minderheit steuert, ist unklar. Die Vereinten Nationen verurteilten die Vertreibungen im September 2017 als systematisch und damit als ethnische Säuberung.

Das Heer Myanmars ist in 13 Regionalkommandos gegliedert und umfasst 30 Infanteriedivisionen, zwei Panzerdivisionen und zehn Artilleriedivisionen. Zu ihrer Ausrüstung zählen rund 130 Kampfpanzer vom Typ T-72, über 300 Kampfpanzer chinesischer Nachbauten des T-54, rund 150 amphibische Panzer vom Typ PT-76 und 140 Radpanzer EE-9 Cascavel. Die Artillerie verfügt über ein Arsenal an verschiedenen älteren Typen von Haubitzen, Geschützen und Mehrfachraketenwerfern aus verschiedenen Produktionsländern, u. a. sind sie sowjetischer Herkunft. Die Luftverteidigung verwendet ebenfalls ältere Systeme sowjetischer Herkunft, wie etwa SA-6 Gainful und SA-11 Gadfly. Bedeutende moderne Systeme sind die SA-16 Gimlet, SA-18 Grouse und das russische Kurzstrecken-Luftabwehrraketen-System SA-15 Gauntlet (Tor M1).

Der Luftwaffe Myanmars gehören 15.000 Soldaten an, gegliedert auf acht Luftwaffenstützpunkte und sechs Luftverteidigungssektoren. Sie ist ausgerüstet mit 52 Jagdflugzeugen des chinesischen Typs Chengdu J-7, mehr als 12 Kampfflugzeugen vom Typ MiG-29, 48 NAMC Q-5 (Nanchang A-5) und weiteren Flugzeugen.

Über den Umfang und die Ausrüstung der Marine Myanmars gibt es keine aktuellen Daten. Zur Verfügung stehen den offenbar 16.000 Soldaten mindestens drei Korvetten und eine größere Anzahl an Patrouillenbooten.

Formal gehört auch die nationale Polizei "The People's Police Force" seit ihrer Reorganisation 1995 zu den Streitkräften.

Myanmar gliedert sich in sieben "Staaten", sieben "Regionen" und ein "Unionsterritorium". Die Landesteile, die überwiegend von der größten Volksgruppe Myanmars, den "Bamar", besiedelt sind, heißen "Regionen" (bis 2008 "Divisionen"), die Bereiche, die überwiegend von Minderheiten bewohnt werden, "Staaten". Das Unionsterritorium umgibt die Hauptstadt des Landes.

Die Minderheiten-"Staaten" bilden zum überwiegenden Teil die Außengrenzen Myanmars; im Uhrzeigersinn beginnend im Südwesten:


Von den sieben "Regionen" verfügen zwei über Außengrenzen auf dem Festland, die übrigen fünf liegen entweder im Binnenland oder am Meer:


Das 2010 eingerichtete "Unionsterritorium" liegt zentral in Myanmar um die Hauptstadt des Landes:


Die "Staaten" und "Regionen" sind weiter untergliedert in Distrikte und Gemeinden.

Hauptstadt war bis November 2005 Rangun, die größte Stadt des Landes. Ab Dezember 2005 wurden die Regierungsbehörden in eine neue Hauptstadt westlich der Kleinstadt Pyinmana verlegt, ungefähr 320 km nördlich von Rangun gelegen. Am 6. Februar 2006 war der Umzug sämtlicher Ministerien offiziell abgeschlossen. Die neue Hauptstadt bekam am 22. März 2006 den Namen Naypyidaw („Heimstatt der Könige“). Es wird behauptet, dass die Regierung die Hauptstadt vor allem aus Angst vor Aufständen in der Großstadt Rangun erbauen ließ. Mehr als 10 Jahre nach der Ernennung zur Hauptstadt Myanmars, gleicht Naypyidaw in weiten Teilen immer noch einer Geisterstadt. 

Mit einem Bruttoinlandsprodukt von 1269 Euro (2016) pro Einwohner gehört Myanmar zu den ärmeren Ländern der Welt, wächst allerdings seit der wirtschaftlichen Öffnung des Landes schnell. 70 % der Beschäftigten arbeiten in der Landwirtschaft; in ihr werden 43 % des BIP erzeugt, während die Industrie 20 % und der Dienstleistungssektor 37 % beitragen. Vor der Diktatur stand das Land wirtschaftlich sehr gut da und wurde auch „Kornkammer Südostasiens“ genannt, bekannt auch als Kupfer- und Edelsteinlieferant.

Durch die politische Öffnung Myanmars werden ausländische Firmen wie Nissan oder Coca-Cola angezogen. Zuvor war Myanmar stark isoliert.

Im Index der Wirtschaftlichen Freiheit belegte Myanmar 2017 Platz 146 von 180 Ländern.

Myanmar kämpfte lange Zeit mit einer starken Inflation; die einheimische Währung Kyat verlor im Zeitraum von 1990 bis 2001 durchschnittlich 34,6 % pro Jahr an Wert; in den Jahren 2002/03 beschleunigte sich die Inflationsrate auf durchschnittlich 46,9 %. Durch die wirtschaftliche Öffnung des Landes durch die Regierung konnte die Inflation drastisch gesenkt werden. Im Jahr 2009 betrug sie nur circa 1,5 %, für 2010 wuchs sie jedoch erneut auf 9,6 % an. 2014 betrug die Inflation etwa 5,9 %, im Jahr 2015 etwa 11,5 %. Der inoffizielle Wechselkurs zum US-Dollar spiegelt dies eindrücklich wider: Bis 2007 stieg der Preis für 1 US-Dollar auf 1.300 Kyat, während er 2010 im Mittel unter 1000 Kyat sank. Ende Juli 2011 lag er gar bei 785 Kyat/US-Dollar.

Ein großes Problem des Staates ist der hohe Grad an Korruption. Myanmar belegte 2011 den vorletzten Platz von 183 Staaten, die in der Korruptionsstatistik der Organisation Transparency International gelistet sind. Ein anderes großes Problem sind die exorbitanten Ausgaben für Militär, Polizei und Geheimdienste, die seit Jahren über 50 % des Staatsbudgets ausmachen.

Steigende Lebensmittel- und Treibstoffpreise und die Willkürherrschaft des Regimes sorgen bei den Einwohnern für große Unzufriedenheit, die sich zumeist hinter vorgehaltener Hand, aber auch öffentlich äußert. Stromausfälle sind recht häufig.

Die Handelsbilanz war für das Jahr 2010 mit Importen im Wert von 4,532 Milliarden US$ und Exporten im Wert von 7,841 Milliarden Euro stark positiv. Beide Werte sind tatsächlich weitaus höher, da über die Grenzen zu Thailand, China, Indien und Bangladesch im großen Stil geschmuggelt wird. Wichtigste Exportgüter sind Erdgas sowie land- und forstwirtschaftliche Erzeugnisse, während die Importe zu einem großen Teil aus Konsumgütern, Halbfertigwaren und Investitionsgütern bestehen. Aus dem Land geschmuggelt werden vor allem Drogen, Edelsteine, Holz und Reis, während ins Land vornehmlich Konsumgüter und Treibstoff gelangen. Eine ganze Reihe europäischer und amerikanischer Firmen haben sich wegen zu schlechter wirtschaftlicher Aussichten, wegen übertriebener Bürokratie oder der Menschenrechtslage aus Myanmar wieder zurückgezogen; dagegen expandieren besonders Firmen aus Japan, Korea, Singapur und China im Land.

Fast die Hälfte der Exporte geht nach Thailand (46,6 %); nächstkleinere Handelspartner sind Indien (12,9 %), China (9 %) und Japan (5,6 %). Größte Importländer dagegen sind China (33,1 %), Thailand (26,3 %) und Singapur (15,2 %). Der größte Importeur myanmarischer Waren in Europa ist Deutschland (Stand 2006: 102 Millionen Euro). Der Export nach Birma lag bei 32 Millionen Euro, was nach Meinung des Auswärtigen Amtes mit den schlechten wirtschaftlichen und politischen Bedingungen im Land zusammenhängt. Der "Ostasiatische Verein", eine deutsche Unternehmerorganisation, ist seit 1997 in Rangun vertreten und plant eine Steigerung der unternehmerischen Aktivitäten.

Die installierte Leistung der Kraftwerke in Myanmar lag im Jahre 2010 bei 3045 MW und 2013 bei 3735 MW, davon entfielen auf Wasserkraftwerke 2780 MW (74 %). Es gibt Schätzungen, dass das Wasserkraftpotential der vier größten Flüsse in Myanmar – Irrawaddy, Thanlwin, Chindwin und Sittaung – bei 100.000 MW liegt. Die Regierung Myanmars hat daher ehrgeizige Pläne, dieses Potential auch auszuschöpfen. Allerdings ist der aktuelle Status bei verschiedenen Projekten unklar, wie z. B. bei der Myitsone-Talsperre oder der Tasang-Talsperre.

In Myanmar werden hochwertige Jade und Edelsteine gefördert. Berühmt sind die Taubenblut-Rubine aus den Minen in der Nähe der Stadt Mogok. Dort kommen auch Spinell, Saphir und einige andere Minerale und Edelsteine in hervorragender Qualität vor. Einzigartig ist das Vorkommen von Painit. Gold wird ebenfalls gewaschen, wobei eine beträchtliche Menge davon von Pilgern in Form von hauchdünnen Blättchen auf Zedis (Stupas), Buddha-Statuen und den Goldenen Felsen geklebt wird.

Zudem fördert Myanmar täglich etwa 20.000 Barrel Erdöl (Stand 2014) sowie jährlich 13,1 Milliarden Kubikmeter Erdgas (2013). 25.000 Barrel Erdölprodukte wurden täglich verbraucht (2013) und rund 8.500 Barrel Erdölprodukte täglich importiert (2012). Die Ausbeutung und Weiterverarbeitung wird einerseits von der staatlichen Ölgesellschaft MOGE (Myanmar Oil and Gas Enterprise) vorgenommen und andererseits von ausländischen Ölkonzernen wie den französischen Konzernen Total und Elf sowie Texaco, Unocal, Amoco, British Premier of UK, Nippon Oil. Total baut mit Unocal eine Gaspipeline von Myanmar nach Thailand. Zwei Milliarden Dollar sollen dafür veranschlagt sein.

Myanmar bietet ein großes Angebot an Sehenswürdigkeiten. Der Shwedagon-Stupa in der früheren Hauptstadt Rangun ist der größte und vermutlich wertvollste Stupa der Welt. Er ist vom Sockel bis zur Turmspitze mit Gold bedeckt.

Weitere Hauptattraktionen des Landes sind die weitläufigen Anlagen der alten Hauptstadt Bagan mit über 2000 Sakralbauten aus vier Jahrhunderten, der Goldene Fels bei Kyaikto im Mon-Staat, der Inle-See mit den schwimmenden Gärten, die Stadt Mandalay als kulturelles Zentrum mit vielen Sehenswürdigkeiten auch in ihrer Umgebung. Eine davon ist die zweitschwerste freihängende läutbare Glocke der Welt, die Mingun-Glocke. Sie wurde 1808 auf Veranlassung König Bodawpayas angefertigt. Der im Westen gelegene Rakhaing-Staat besitzt den Strand von Ngapali, der für Urlauber eines der beliebtesten Reiseziele ist.

Bekamen Touristen früher nur Visa für maximal eine Woche, so öffnete sich das Land vor einigen Jahren aus wirtschaftlichem Druck und wirbt zunehmend aktiv für den Devisen bringenden Tourismus. Dafür wurden Flughäfen und Straßen mithilfe von Zwangsarbeitern ausgebaut. Viele Menschenrechtsorganisationen und tourismuskritische Vereinigungen (beispielsweise Tourism Concern) riefen Touristen jedoch lange zum Boykott des Landes auf, da diese ihrer Ansicht nach durch Reisen nach Myanmar das Militärregime unterstützten und ihre Devisen nicht bei der Bevölkerung ankämen. Andere regierungsunabhängige Institutionen in Myanmar dagegen warnen vor einem Boykott, da viele Arbeitsplätze z. B. bei Hotels, Airlines, Restaurants, Souvenirgeschäften u. v. m. unmittelbar und mittelbar vom Tourismus abhängen und für diese Menschen von existenzieller Bedeutung sind. Auch sind Touristen eine wichtige Quelle unabhängiger Informationen, die sonst kaum den Weg in und aus dem Land finden.

In den vergangenen Jahren verzeichnete das Land einen starken Zuwachs der Besucherzahlen. In den Jahren 2007 und 2008 besuchten insgesamt 220.000 Touristen Myanmar, 2012 waren es bereits eine Million, 2013 zwei Millionen und im Jahr 2014 über drei Millionen Reisende. Für das Jahr 2015 haben sich die Tourismusverantwortlichen das Ziel von fünf Millionen Besuchern gesetzt.

An der Grenze zu Laos und Thailand hat Myanmar Anteil am sogenannten Goldenen Dreieck, in dem Schlafmohn angebaut wird, um aus ihm Opium zur Heroinproduktion zu gewinnen. Die Bedeutung Myanmars als Lieferant für den weltweiten Heroinmarkt ist zuletzt (2010) durch große Ernteausfälle und dem dadurch bedingten Rückgang der Drogenproduktion in Afghanistan und einer Vergrößerung der Anbauflächen wieder gestiegen. Myanmar nimmt in der Welt eine Spitzenposition bei der Produktion von Amphetaminen ein, die auf chemischem Weg leichter, billiger und von der Witterung unabhängiger als Mohn produziert werden können. Sie werden in schwer auffindbaren Dschungel-Fabriken tonnenweise hergestellt und vor allem über Thailand und China in die ganze Welt exportiert. Teilweise sollen die Regierungsvertreter daran mitverdienen, indem mit den involvierten aufständischen Ethnien Waffenstillstände gegen Beteiligungen an den Einnahmen aus dem Drogenhandel ausgehandelt wurden.

Auf Wildtiermärkten in Myanmar wird mit Elfenbein-, Nashorn-, und Tigerprodukten gehandelt. Myanmar ist Transit- und Herkunftsland illegaler Wildereiprodukte bedrohter Tierarten.

Illegale Arbeit im benachbarten Thailand ist eine weitere inoffizielle Einkommensquelle. Insbesondere finden Menschen aus Myanmar als niedrig entlohnte Haushaltshilfen, Pflegerinnen und Kindermädchen Arbeit in Thailand.

Im März 2011 kam es im Grenzgebiet zu Thailand zu einem schweren Erdbeben. Das Militär hinderte ausländische Hilfskräfte daran, bis an den Ort des Epizentrums zu gelangen, was offensichtlich deswegen geschah, um Ausländern keinen Einblick in die Drogengeschäfte der Armee zu gewähren. Viele Bauern werden zum Opium-Anbau gezwungen. Es gibt andere Gegenden, in denen nicht das Militär das Sagen hat, sondern einzelne Rebellengruppen. Dort kontrollieren diese den Drogenanbau.

Myanmar dient China als Transferroute für den Erdöl- und Erdgastransport aus dem Nahen und Mittleren Osten sowie als Lieferant von Strom. Dabei besitzt das Land eine gehobene Relevanz für die chinesische Energiesicherheit, da über den Landweg die verwundbare Malakka-Straße zumindest zu einem Teil umgangen werden kann. Allein seit März 2010 wurden Investitionen der Volksrepublik China von knapp 8,2 Mrd. US-$ beschlossen, wovon rund 3,6 Mrd. US-$ auf den Bau des Myitsone-Wasserkraftprojekts im Kachin-Staat entfallen. Letzterer wurde 2011 unilateral von der Regierung Myanmars ausgesetzt, was zu einer deutlichen Abkühlung der bilateralen Beziehungen führte.

In der Bevölkerung Myanmars wird das chinesische Engagement als bedrohlich und ausbeuterisch angesehen. Es kursieren Berichte über Ausbeutung, Enteignungen, Zerstörung der lokalen Infrastruktur sowie Missachtung des Umweltschutzes, so dass in vielen Gegenden des Landes eine anti-chinesische Stimmung herrscht. Viele der reichsten Unternehmer in Myanmar sind chinesischer Abstammung, was bei den einheimischen Unternehmern auf Missfallen stößt.

Die Firma Asia World ist der größte Konzern in Myanmar. Er ist in den Bereichen Infrastruktur, Energie, Bau und Transport sowie Import und Export aktiv. Außerdem gehört ihm eine Kette von Supermärkten. Ungefähr die Hälfte aller Investitionen in Myanmar stammen aus Unternehmen, die zu diesem Konzern gehören. Asia World erhielt den Zuschlag bei vielen sino-burmesischen Großprojekten im Land (Tiefseehafen in Kyaukpyu, Erdölpipeline, Dammprojekte).

Asia World gehört Htun Myint Naing, besser bekannt unter dem Namen Steven Law, der aus der Familie eines Drogenbarons stammt und als reichster Mann Myanmars gilt. Steven Law steht, neben verschiedenen anderen, dem Militär nahestehenden Personen, seit 2008 unter US-amerikanischen Sanktionen. Im Mai 2016 wurden die US-Sanktionen gegen Steven Law verschärft.

Der Staatshaushalt umfasste 2015 Ausgaben von umgerechnet 4,47 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 2,68 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 2,7 % des BIP. Für das Gesundheitssystem gab der Staat 2,3 % des BIP aus (2014).

Die Staatsverschuldung betrug 2016 23,7 Mrd. US-Dollar und damit noch 35,8 % des BIP. Gegenüber dem Jahr 2004 (63,2 %) konnte sie damit stark abgebaut werden.

Myanmar verfügt derzeit über zwei internationale und 16 lokale Flughäfen, die von nationalen Fluggesellschaften bedient werden. Der größte Flughafen ist der Internationale Flughafen Rangun. Auch die Stadt Mandalay besitzt einen internationalen Flughafen. Aufgrund des desolaten Straßennetzes und der Größe des Landes ist das Flugzeug das mit Abstand schnellste Verkehrsmittel im Land. Der Sicherheits- und Qualitätsstandard wird teilweise jedoch als rückständig beschrieben. 2011 wurde ein Kooperationsvertrag mit der deutschen Fritz Werner Werkzeugmaschinen AG geschlossen, einige der wichtigsten Flug- und Seehäfen auszubauen bzw. zu modernisieren.

Myanmar verfügt über eine gemessen an der Größe des Landes relativ hohe Anzahl von Fluggesellschaften. Insbesondere seit dem Ende der reinen Militärdiktatur 2010 gab es viele private Neugründungen.

Mit Stand Juni 2015 betreiben folgende myanmarische Airlines Linien- und Charterflüge:

Ziele außerhalb Myanmars werden aktuell nur von MAI und Golden Myanmar Airlines angeflogen, die anderen Gesellschaften bedienen die über dreißig Flugziele innerhalb Myanmars.

Der Ursprung des fast ausschließlich in der Meterspur errichteten Eisenbahnnetzes geht auf die britische Kolonialzeit zurück. Die erste Eisenbahnlinie wurde 1869 zwischen Rangun und dem nordwestlich gelegenen Letpadan eröffnet. 1889 folgte die Linie von Rangun nach Mandalay, die später noch weiter nordwärts bis Myitkyina verlängert wurde. Im Zweiten Weltkrieg ließen die Japaner von Kriegsgefangenen die sogenannte Todeseisenbahn von Thanbyuzayat nach Thailand errichten. Diese Strecke erlangte durch den Film "Die Brücke am Kwai" große Berühmtheit. Sie wurde aber bereits kurz nach Ende des Krieges demontiert. Heute hat das Streckennetz eine Länge von 5031 km (Stand 2008). Grenzüberschreitende Linien existieren nicht. Rückgrat des Netzes ist die von Mawlamyaing über Rangun und Mandalay nach Myitkyina verlaufende Nord-Süd-Strecke. Innerhalb dieser Strecke kommt dem 622 km langen Abschnitt zwischen Rangun und Mandalay eine besondere Bedeutung zu, die sich unter anderem in seinem teilweise zweigleisigen Ausbau und dem Einsatz moderner und auch nach westlichen Gesichtspunkten komfortabler Expresszüge ausdrückt. Von der Nord-Süd-Strecke führen Stichstrecken unter anderem nach Lashio, Shwenyaung, Bagan und Pyay.

Fehlende Investitionen haben zum Verschleiß der Strecken geführt, so dass diese sich heute weitgehend in einem schlechten Zustand befinden. Der Verkehr wird von der staatlichen Gesellschaft Myanma Railways mit Diesellokomotiven abgewickelt. Dampflokomotiven wurden außerordentlich lange, bis etwa 2005 in großer Zahl eingesetzt. Die eingesetzten Züge erreichen oftmals nur Reisegeschwindigkeiten von 30 km/h oder weniger. Selbst die zwischen Rangun und Mandalay verkehrenden Expresszüge benötigen für die 622 km etwa 16 Stunden. Fahrpläne existieren zwar, sie sind aber für den täglichen Betriebsablauf kaum von Bedeutung, da Verspätungen von bis zu mehreren Stunden aufgrund des mangelhaften Streckenzustandes und wegen Unfällen an der Tagesordnung sind. Ebenso kommen aber auch Abfahrten mehrere Stunden vor dem Plan vor. Bei großen Verspätungen lässt die Bahngesellschaft auch schon einmal Züge ausfallen, um die Wagen- und Lokumläufe wieder zu ordnen. Angesichts einer Netzlänge von fast 4.000 km ist die Zahl der täglich eingesetzten Zugpaare mit etwa 100 vergleichsweise gering.

Myanma Railways hat seit 1988 folgende wichtige Strecken gebaut:
Zurzeit sind die Strecken Kyangin (nördl. Irawadi-Region) – Pakokku (515 km) und Kathar – Bhamo (152 km) (Sagaing-Region – Kachin-Staat – Volksrepublik-China-Verbindung) im Bau.

Zum Zustand und Betrieb der Eisenbahn siehe die Reiseberichte.

Der Straßenverkehr hat sich in Myanmar zum wichtigsten Verkehrsträger entwickelt. Das Straßennetz ist insgesamt 34.377 km lang (Stand 2010), davon sind 358 km Autobahnen. Nur ein geringer Anteil des Straßennetzes ist asphaltiert. Der Straßenverkehr wird oft durch die schwierigen klimatischen Verhältnisse behindert. Während der Regenzeit werden zahlreiche Straßen unterspült, in der Trockenzeit weicht die Hitze den Asphalt auf.

Das Benzin ist rationiert. Privatfahrer erhalten offiziell nur neun Liter Benzin am Tag. 

Die klassische burmesische Musik unterscheidet sich trotz der frühen Einflüsse aus Indien und China und ab dem 18. Jahrhundert aus Thailand in Melodie und Rhythmus deutlich von der Musik der Nachbarländer. Zu den ersten Instrumenten, die mit der Ausbreitung des Buddhismus im 1. und 2. Jahrhundert n. Chr. aus Indien kamen, gehörten "Vinas," das waren Stabzithern und Bogenharfen. Die alte Bogenharfe "saung gauk" hat als einzige dieser einst über Asien verbreiteten Instrumentengattung in Myanmar überlebt und genießt bis heute als Nationalinstrument höchste Wertschätzung. Die "saung gauk" ist ein Instrument für die Gesangsbegleitung in der feinen höfischen Kammermusik in geschlossenen Räumen, deren Tradition in der Liedsammlung "Mahagita" zusammengefasst ist. Ein großer Teil dieser Sammlung von etwa 500 heute bekannten Liedern geht auf Myawaddy Mingyi U Sa (1766–1853), den bedeutendsten Komponisten und "saung-gauk"-Spieler der Konbaung-Zeit zurück. Zur Kammermusik zählen auch kleine Instrumentalensembles, in denen außerdem das Xylophon "patala" (verwandt mit dem thailändischen "ranat" und dem kambodschanischen "roneat"), die Längsflöte "palwe" und die Zither "do min" eingesetzt werden.

Die klassische laute Musik für draußen, für Festveranstaltungen, Tänze und Geisteranbetungsrituale "(Nat Pwe)" bieten die "hsaing-waing"-Ensembles. Sie bestehen aus dem namensgebenden Melodieinstrument, einem Kreis von 21 gestimmten Trommeln; einem Kreis mit 21 Buckelgongs ("kyi waing" oder "kyi naung"); ein weiteres Gongspiel "(maung zaing)", ebenfalls in einem Holzgestell; der Doppelrohrblattoboe "hne" (abgeleitet von persisch "surnai") und diverse Trommeln als Rhythmusinstrumente. Bei zwei weiteren, in ländlichen Regionen bei Festen und religiösen Prozessionen gespielten Ensembles stehen große Trommeln im Vordergrund: das Ensemble der bis zu drei Meter langen Bechertrommel "ozi" und der Fasstrommel "dhopat." Beide Gruppen verwenden außerdem Buckelgongs, Paarbecken und Bambusklappern.

Die verschiedenen Volksgruppen haben eigene Instrumente und eine eigene traditionelle Musik. Die Mon verwenden ein aus alter Zeit stammendes Zupfinstrument, die dreisaitige Stabzither "mí-gyaùng saung," die sich von hier weiter in Südostasien verbreitet hat. Nach dem Aussehen wird sie Krokodilzither genannt.

Als erstes westliches Musikinstrument wurde Ende des 19. Jahrhunderts das Klavier (burmesisch: "sandaya") eingeführt, bis 1920 hatte es zu einem großen Teil die Liedbegleitung von der "saung gauk" übernommen. Ähnlich erfolgreich war die Geige (burmesisch: "deyaw"), später kam die Hawaii-Gitarre hinzu. Alle musikalischen Übernahmen aus dem Ausland, ob es sich um Melodien, Tonskalen (die pentatonische Stimmung stammt aus Thailand) oder Instrumente handelte, wurden grundsätzlich den einheimischen Hörgewohnheiten angeglichen und trugen zur Erweiterung der eigenen Musik bei.

Westliche klassische Musik konnte sich nicht durchsetzen. Einer Rückbesinnung auf die eigene Tradition seit Beginn der nationalen Unabhängigkeitsbewegung in den 1920er Jahren steht die Begeisterung für westliche Popmusik bei der jüngeren Generation gegenüber. Deren weit verbreitete, lautstarke und teilweise auch gelungene Nachahmung mit auf Burmesisch verfassten bzw. übersetzten Texten lässt sich als Ausdruck eines Freiheitswillens verstehen. Die Popmusik wird von der Regierung politisch kritisiert und von der älteren Generation moralisch verurteilt.

Seit 1993 findet das staatlich finanzierte "Soyaketi"-Festival der darstellenden Künste jährlich im Oktober/November statt. In zweieinhalb Wochen werden Wettbewerbe in den Sparten Gesang, Instrumentalmusik, Gesangskomposition, Tanz und Marionettentheater durchgeführt. Seit 2007 findet die Veranstaltung nicht mehr wie zuvor in Yangoon, sondern am neuen Regierungssitz in Naypyidaw statt.

Das größte und wichtigste Fest in Burma ist das burmesische Neujahrsfest "Thingyan", das dem thailändischen "Songkran" entspricht. Im Volksmund auch als Wasserfest bezeichnet, wird es an drei hintereinanderfolgenden Tagen im April – dem heißesten Monat – mit viel Wasser gefeiert. Mit Wasserpistolen, Wassereimern und sonstigen mit Wasser gefüllten Behältern ziehen die Burmesen auf Ladeflächen von LKW oder zu Fuß durch die Stadt und machen jeden nass. Es gibt auch Umzüge, bei denen zu kräftigen Rhythmen getanzt wird.

Das farbenfrohe "Phaungdaw U"-Fest wird zu Ehren Buddhas im Herbst auf dem Inle-See gefeiert. Dabei wird eine Buddha-Statue auf einer geschmückten Barke über den See gefahren.

Liste weiterer wichtiger Feste in Myanmar:

Die traditionellen Kalagas sind kunstvoll bestickte Wandteppiche aus Seide, Flanell, Wolle, Filz und Spitze auf Baumwolle und Samt.






Allgemein

Politik


</doc>
<doc id="3475" url="https://de.wikipedia.org/wiki?curid=3475" title="Mönchengladbach">
Mönchengladbach

Mönchengladbach ist eine kreisfreie deutsche Großstadt im Westen Nordrhein-Westfalens. Die als Oberzentrum eingestufte Stadt liegt im Regierungsbezirk Düsseldorf, sie ist außerdem Bestandteil der Metropolregion Rhein-Ruhr. Das Stadtgebiet von Mönchengladbach erstreckt sich nördlich der Niersquelle, das Mönchengladbacher Stadtzentrum befindet sich rund 25 Kilometer westlich der nordrhein-westfälischen Landeshauptstadt Düsseldorf.

Die Stadt Mönchengladbach besteht in ihren heutigen Grenzen seit der Gebietsreform, die am 1. Januar 1975 in Kraft trat, nachdem die (alte) kreisfreie Stadt Mönchengladbach mit der kreisfreien Stadt Rheydt und der Gemeinde Wickrath (Kreis Grevenbroich) zur neuen kreisfreien "Stadt Mönchengladbach" vereinigt wurde.

Die Einwohnerzahl stieg im Jahr 1921 erstmals auf über 100.000, so dass Mönchengladbach der Status der Großstadt zuerkannt wurde. Auch in Rheydt wurden ab dem Jahr 1968 mehr als 100.000 Einwohner gezählt. Am betrug die Einwohnerzahl der Gesamtstadt .

Mönchengladbach verfügt als einzige Stadt in Deutschland über zwei Hauptbahnhöfe (Mönchengladbach Hauptbahnhof und Rheydt Hauptbahnhof).

Mönchengladbach liegt etwa 16 Kilometer westlich des Rheins im Niederrheinischen Tiefland am Ostabfall der Schwalm-Nette-Platte gegen die lössbedeckte Kempen-Aldekerker-Platte und die Niersniederung. Obwohl Mönchengladbach größtenteils im Flachland liegt, sind der Süden (Stadtteil Odenkirchen) und das Stadtzentrum (um Bökelberg und Abteiberg) vergleichsweise hügelig. Diese Hügelketten beschränken sich aber auf das Innere der Stadt. Große Teile des Stadtgebietes sind von Wäldern und Parks bedeckt. Der Hardter Wald und Teile Rheindahlens im Westen gehören zum Naturpark Maas-Schwalm-Nette.

Der höchste Punkt ist die mit nach dem Zweiten Weltkrieg aus Trümmern aufgeschüttete Rheydter Höhe. Der niedrigste Punkt sind die Nierswiesen im Stadtteil Donk mit . Die Länge der Stadtgrenze beträgt insgesamt 86 Kilometer. Die größte Nord-Süd-Ausdehnung beträgt 17; die größte West-Ost-Ausdehnung 18 Kilometer. Der Mittelpunkt des Stadtgebiets liegt im Stadtteil Pongs.

Alle in Mönchengladbach entspringenden Gewässer (bzw. solche, die früher dort entsprungen sind) werden vom Braunkohletagebau – Garzweiler II – beeinflusst, der südlich von Mönchengladbach stattfindet. Das Gebiet nähert sich Mönchengladbach; die Umweltauswirkungen sind erheblich.

Der Gladbach, der der Stadt den Namen gegeben hat, hatte seine Quelle im Ortsteil Waldhausen in oder nahe der ehemaligen Brauerei Hensen. Heute ist die Quelle versiegt. Der Gladbachkanal, der ursprünglich auch von der Gladbachquelle gespeist wurde, verläuft entlang des alten Bachbettes zum großen Teil unterirdisch und mündet im Osten der Stadt in die Niers. Zu sehen ist er in Lürrip bis zu seiner Mündung in die Niers. Im Stadtgebiet wird der Verlauf des ehemaligen Gladbachs durch Schilder gekennzeichnet.

Den Süden und Osten des Stadtgebietes durchfließt die Niers, die früher wenige hundert Meter südlich der Stadtgrenze zu Erkelenz bei Kuckum im Kreis Heinsberg entsprang. Infolge der Sümpfungsmaßnahmen im Zuge des Tagebaus Garzweiler II und des damit verbundenen Wegfalls des Ortes Kuckum wurde die Niersquelle im April 2006 auf die Mönchengladbacher Golfsportanlage verlegt. Seitdem entspringt die Niers im Wasserhindernis zwischen den Bahnen 12 und 17 und gelangt dann quer über die Fairways der Bahnen 18 und 1 wieder in ihr altes Flussbett. Die Niers verlässt das Stadtgebiet im Norden der Stadt an der Grenze zu Viersen unweit der Trabrennbahn, bevor der Fluss bei Gennep in den Niederlanden in die Maas mündet.

Südwestlich der Stadtgrenze (in Wegberg) entspringt die Schwalm. Sie mündet bei Swalmen in den Niederlanden in die Maas. Einer der Flüsse, der auf Mönchengladbacher Stadtgebiet entspringt und in die Schwalm mündet, ist der Knippertzbach.

Folgende Städte und Gemeinden grenzen an die Stadt Mönchengladbach. Sie werden im Uhrzeigersinn, beginnend im Nordwesten genannt:

Schwalmtal, Viersen und Willich (alle Kreis Viersen), Korschenbroich und Jüchen (Rhein-Kreis Neuss) sowie Erkelenz und Wegberg (Kreis Heinsberg).

Das Stadtgebiet Mönchengladbachs besteht seit 2009 aus vier (vorher zehn) Stadtbezirken, die in 44 Stadtteile unterteilt sind. In jedem Stadtbezirk gibt es eine Bezirksvertretung unter Vorsitz eines Bezirksvorstehers. Die Bezirksvertretungen sind zu wichtigen, den Stadtbezirk betreffenden Angelegenheiten zu hören und werden bei jeder Kommunalwahl neu gewählt.

Die Stadtbezirke mit ihren zugehörigen Stadtteilen

Die Stadtbezirke und Stadtteile sind im Einzelnen:

Weitere Informationen zum Thema befinden sich in der Liste der Stadtbezirke und Stadtteile von Mönchengladbach.

Statistische Kennzahlen für Mönchengladbach (Stand: 31. Dezember 2013):
Seit dem 10. Jahrhundert sind mehr als 20 verschiedene Schreibweisen des Stadtnamens bekannt, u. a. "Gladebach", "Monichgladebacg", "Moenchsgladbach", "Monnike Gladbeeck" und "Munneke Glebbek". Bis zum Ende des 19. Jahrhunderts hieß die Stadt in wechselnden Schreibweisen "Gladbach". Als Gladbach am 1. Januar 1888 kreisfrei wurde, erhielt es zur besseren Unterscheidung von Bergisch Gladbach den Namen "München-Gladbach", abgekürzt "M. Gladbach". Der Name leitet sich von den 974 in Gladbach siedelnden Mönchen ab.

Vom 1. August 1929 bis zum 31. Juli 1933 war München-Gladbach zusammen mit Rheydt und anderen Gemeinden ein Teil der kreisfreien Stadt Gladbach-Rheydt mit etwa 200.000 Einwohnern. Nach deren Auflösung hieß es "München Gladbach" (ohne Bindestrich), abgekürzt wiederum "M. Gladbach".

Am 20. Dezember 1950 wurde bei Beibehaltung der Schreibweise "M. Gladbach" die Aussprache in "Mönchen Gladbach" geändert, um Verwechslungen mit München zu vermeiden. Am 11. Oktober 1960 wurde die heute noch gebräuchliche Form Mönchengladbach eingeführt, welche auch bei der erneuten Zusammenlegung aufgrund des Düsseldorf-Gesetzes am 1. Januar 1975 mit Rheydt und Wickrath gewählt wurde; diese Namenswahl war in der Bevölkerung umstritten.

Die ersten Siedlungen auf dem Gebiet des heutigen Mönchengladbach sind etwa 300.000 bis 400.000 Jahre alt und zeigen Nachweise des späten "Homo erectus" (= "Homo heidelbergensis") und des Neandertalers. Fundstellen sind vor allen im südlichen Teil des heutigen Stadtgebiets im Umfeld der Niers und südöstlich von Schloss Rheydt entdeckt worden.

Im Jahr 2013 haben Forscher südlich von Schloss Rheydt, auf der ehemaligen rechten Seite der Niers, eine größere Siedlung aus der Altsteinzeit gefunden, die zeigt, dass auch vor 13.000 Jahren der Mensch im Umfeld der Niers dauerhaft lebte. Aus der Jungsteinzeit und der Bronzezeit sind zahlreiche Hügelgräber im Hardterwald erhalten. Weitere Funde wurden vor allem in der Umgebung von Mülfort, Wickrathberg, Giesenkirchen-Nord, Giesenkirchen-Mitte, Wanlo und Rheindahlen gemacht. Um 50 v. Chr., beim Eintreffen der ersten römischen Truppen unter Cäsar, war das keltische Gebiet, vermutlich zu den Eburonen gehörig, auf einmal unbesiedelt, vermutlich war die Bevölkerung durch die Römer ermordet worden.

Zur Römerzeit gehörte das Gebiet des heutigen Mönchengladbach zur Provinz Germania inferior. Es existierten im 1. bis 3. Jahrhundert, in Mönchengladbach-Mülfort, ein römisches Straßendorf (Vicus Mülfort) mit einer Furt über die sumpfige Niers sowie wichtige Straßenverbindungen ins römische Köln, Xanten, Neuss und an die Maas. Nach der Zerstörung des römischen Dorfes durch die Franken 352 n. Chr. war das Gebiet unbesiedelt.

Um etwa 800 n. Chr. siedelten sich Franken im Gebiet des heutigen Odenkirchen-Sasserath, Wickrath und auf dem Abteiberg an. Im Jahr 974 kam es zum Bau des Gladbacher Münsters und der Gründung einer Abtei durch den Kölner Erzbischof Gero und seinen Begleiter, den Trierer Mönch Sandrad. Der Bau der ersten Klosterkirche fand neben dem Ort statt, an dem 954 die Ungarn die von Balderich, einem "Vornehmen des Reiches", kurz vor 800 erbaute Kirche zerstört hatten.
Die Mönche trieben eine Besiedlung voran und legten dazu im 12. Jahrhundert nördlich der Kirche einen Markt an. Handwerker und Gewerbetreibende ließen sich in der Folgezeit hier nieder. Stadtrechte wurden in den Jahren 1364 bis 1366 erteilt. Die "Stadt" erhielt eine steinerne Stadtmauer, die von den Bürgern zu unterhalten war (Reste davon sind noch am Geroweiher erhalten, ebenso der "Dicke Turm", ein Wehrturm am Waldhausener Berg und an der Rückseite des Kapuzinerplatzes). Bis Ende des 18. Jahrhunderts gehörte die Stadt zum Amt Grevenbroich im Herzogtum Jülich.

Am 4. Oktober 1794 marschierten französische Revolutionstruppen in das Stadtgebiet ein. Einen Tag zuvor war die Festung Jülich übergeben worden. Als im Jahr 1801 der deutsche Kaiser Franz II. im Frieden von Lunéville das linke Rheinufer an Frankreich abtrat, wurde Gladbach den französischen Religionsgesetzen unterworfen. Die Abtei wurde aufgelöst; die letzten 31 Mönche verließen das Kloster am 31. Oktober 1802. Die bedeutende, über die Grenzen Deutschlands hinaus bekannte Bibliothek der Abtei wurde zu großen Teilen geraubt oder vernichtet. Von 1798 bis 1814 gehörte die Mairie Gladbach zum Kanton Neersen im Arrondissement de Crévelt im französischen Département de la Roer.

Ab dem Jahr 1815 stand Gladbach unter preußischer Verwaltung und wurde Sitz des Kreises Gladbach, der 1929 aufgelöst wurde. In diesem Zuge wurde Gladbach auch Sitz der gleichnamigen Bürgermeisterei, die 1859 in die Stadt Gladbach und die Bürgermeisterei Obergeburth geteilt wurde.

Bis Ende des 19. Jahrhunderts hieß die Stadt "Gladbach" beziehungsweise früher Gleidebach (zahlreiche andere Schreibweisen sind überliefert). Als die Stadt am 1. Januar 1888 kreisfrei wurde, erhielt sie zur besseren Unterscheidung von Bergisch Gladbach den Namen München-Gladbach, was „M. Gladbach“ geschrieben wurde. Am 24. Oktober 1890 wurde in Köln der Volksverein gegründet, wählte aber M. Gladbach als Sitz. Die Bürgermeisterei Obergeburth wurde 1907 in Bürgermeisterei München-Gladbach-Land umbenannt. Stadtkreis und Kreisstadt im preußischen Regierungsbezirk Düsseldorf, links vom Rhein (1919) 64.031 Einwohner, mit dem südlich angrenzenden Rheydt einen geschlossenen Wohnplatz von rund 130.000 Bewohnern bildend. Amtsgericht, Handelskammer, Gymnasium und Realgymnasium, Lykeion (eines mit Aufbau), höhere Fachschule für Textilindustrie; Sitz des Volksvereins für das katholische Deutschland. Hauptsitz der rheinischen Textilindustrie (unter anderem Baumwollspinnereien, Webereien, Färbereien), Eisengießereien, Maschinenfabriken. Die Landgemeinde München-Gladbach hatte 22.614 Einwohner.

1921 wurden die Stadt Rheindahlen, die Bürgermeisterei München-Gladbach-Land und Neuwerk mit der Stadt München-Gladbach vereinigt. 1929 wurde München-Gladbach mit Rheydt und anderen Gemeinden (unter anderem Rheindahlen, Hardt, Giesenkirchen und Odenkirchen) zur Doppelstadt "Gladbach-Rheydt" vereinigt, jedoch 1933 wieder geteilt (auf Wunsch des Reichspropagandaministers Joseph Goebbels, der ein gebürtiger Rheydter war). Danach hieß Alt-München-Gladbach offiziell "München Gladbach" (ohne Bindestrich) oder kurz "M. Gladbach".

In der Nacht zum Pfingstsonntag (11./12. Mai) 1940 flogen Flugzeuge der Royal Air Force (RAF) Luftangriffe auf 15 Einzelziele im Raum Aachen-Mönchengladbach-Düsseldorf-Viersen. Das erste Bombardement traf Mönchengladbach, vier Menschen starben.
Die Wehrmacht hatte am 10. Mai 1940 den Westfeldzug begonnen; die Bahnstrecke Mönchengladbach–Aachen wurde für den Transport von Truppen und Fahrzeugen benutzt.
Der Angriff richtete sich nicht (wie gelegentlich behauptet) gegen die Stadt und war auch nicht der Beginn des Flächenbombardements der Royal Air Force gegen deutsche Städte.
Der schwerste Großangriff des Krieges auf München-Gladbach und Rheydt fand in der Nacht vom 30. auf den 31. August 1943 statt. Beim letzten Großangriff am 1. Februar 1945 wurden aus 160 Flugzeugen 1200 Spreng- und 65.000 Brandbomben abgeworfen. Bei Kriegsende waren beide Städte zu etwa zwei Dritteln zerstört, rund 2000 Zivilpersonen waren bei den Luftangriffen gestorben.

Zwischen dem 26. Februar und 1. März wurde von Süden aus das Gebiet des heutigen Mönchengladbach während der Operation Grenade durch die Alliierten erobert. Dabei wurde im Grenzlandstadion eine Krankenhauseinheit, das 41. Evacuation Hospital mit 750 mobilen Betten, vom 4. März bis 2. April 1945 stationiert.

Noch in den letzten Kriegstagen errichteten US-Truppen zwischen Mongshof, Wickrathberg und Hochneukirch das neun Quadratkilometer große Gefangenenlager „Wiesenlager Wickrathberg“, das später auch unter Führung der britischen Streitkräfte stand. Dort lebten bei voller Belegung mindestens 150.000 deutsche Soldaten. Die Soldaten verbrachten die gesamte Zeit unter sehr harten Bedingungen im Freien und schliefen hauptsächlich in selbst gegrabenen Erdlöchern. Auch die Verpflegung war sehr spärlich. Zeugen sprachen von 20 Todesfällen am Tag, andere von insgesamt 226 Toten. Die genaue Zahl der Todesopfer ist nicht bekannt. Das Lager existierte von April bis September 1945.

Nach dem Krieg erlangte in beiden Städten die Textilindustrie langsam wieder an Bedeutung; später wurde sie durch den Maschinenbau ergänzt. 1950 wurde die Sprechweise der Stadt M. Gladbach in "Mönchen Gladbach" geändert, um Verwechslungen mit der Stadt München zu vermeiden, die Schreibweise M. Gladbach blieb aber zunächst erhalten. 1960 wurde schließlich die (bis heute gebräuchliche) Form "Mönchengladbach" eingeführt. Im Jahr 1974 hatte die Stadt insgesamt 147.567 Einwohner, davon Alt-Mönchengladbach 81.756 (hiervon wiederum Stadtmitte 58.511 und Volksgarten 23.245), Rheindahlen 27.707, Hardt 17.142 und Neuwerk 20.962. 1975 schlossen sich – im Zuge der Gebietsreform in Nordrhein-Westfalen – Mönchengladbach, Rheydt und die Gemeinde Wickrath zusammen.

Mönchengladbach und Rheydt gehörten anfangs zum Bistum Lüttich. Unter Erzbischof Everger von Köln († 999) kam das Gebiet zum Erzbistum Köln und war dem Archidiakonat des Propstes des St. Viktor-Doms in Xanten, Dekanat Süchteln unterstellt. Die Abtei Gladbach war jedoch relativ autonom und übte über die Stadt Gladbach die kirchlichen Rechte aus. Bis 1802 verblieb das Gebiet beim Erzbistum Köln. In Gladbach konnte die Reformation nach reformiertem Bekenntnis zwar zunächst Fuß fassen, sich dann aber wegen des Widerstands des Abtes nicht durchsetzen, so dass Gladbach eine überwiegend katholische Stadt blieb. Im 16. und 17. Jahrhundert bestand in der Stadt jedoch noch eine radikal-reformatorische Täufergemeinde, die trotz mehrerer Konfiskationen und Ausbürgerungen noch bis zur letzten Vertreibungswelle 1722 Bestand hatte. Die den hochdeutschen Mennoniten zugerechnete Gemeinde soll unter anderem die Weberei in Gladbach eingeführt haben. Stimmen in der Stadt, die Täufer aus wirtschaftlichen Gründen in Gladbach zu tolerieren, fanden keine Umsetzung. Die anderen Protestanten konnten zu Beginn des 17. Jahrhunderts in der „Halle“ auf dem Markt ihre Gottesdienste abhalten. 1684 konnten sie an der Ostseite der Stadt vor der Stadtmauer auch eine Kirche bauen; diese wurde 1857 abgebrochen. Der Neubau der heutigen Christuskirche in der Kapuzinerstraße erfolgte bis 1852. Gegen Ende des 19. Jahrhunderts waren etwa 80 Prozent der Bevölkerung katholisch und 20 Prozent waren Protestanten.

Anders war es in Rheydt: Die reformatorische Bewegung setzte dort um 1550 ein. 1560 gab es reformierte Geistliche. 1632 traten die Pfarrer und die Gemeinde in Rheydt auf Druck des protestantischen Herrscherhauses Jülich zum Protestantismus über. Sie unterstanden der Jülicher Provinzialsynode und dem Gladbacher Quartier. Daher war Rheydt eine überwiegend protestantische Stadt. Hier zogen im 19. Jahrhundert im Zuge der Industrialisierung vermehrt Katholiken zu, so dass das Zahlenverhältnis beider Konfessionen relativ ausgeglichen war; später veränderte sich dies stark zugunsten der Katholiken.

Im Jahr 1802 wurden die katholischen Pfarrgemeinden dem neu gegründeten Bistum Aachen zugeordnet, das jedoch 1821/1825 bereits wieder aufgehoben wurde, so dass das Gebiet Gladbach/Rheydt ab 1821 erneut zum wiedererrichteten Erzbistum Köln kam. 1930 wurde es dem wieder neuerrichteten Bistum Aachen zugeordnet. Mönchengladbach und Rheydt wurden Sitze von Dekanaten und heute gehören alle Pfarrgemeinden der Stadt Mönchengladbach zur "Region Mönchengladbach". Hauptkirche der Stadt ist das Mönchengladbacher Münster.

Mit dem Übergang des Herzogtums Jülich an Preußen 1815 wurden die protestantischen Gemeinden Gladbach und Rheydt sowie die Gemeinden der heutigen Stadtteile Mönchengladbachs Glieder der ab 1817 evangelischen Kirche in Preußen (unierte Kirche) beziehungsweise dessen rheinischer Provinzialkirche. Gladbach wurde Sitz eines Superintendenten, aus dem später der Kirchenkreis Gladbach (heute Gladbach-Neuss) innerhalb der evangelischen Kirche im Rheinland hervorging. In den 1960er Jahren wurden die Kirchengemeinden Mönchengladbach und Rheydt in mehrere Kirchengemeinden aufgeteilt. Alle Kirchengemeinden der heutigen Stadt gehören jedoch zum Kirchenkreis Gladbach-Neuss, der aber auch Gemeinden außerhalb der Stadt Mönchengladbach umfasst. Evangelische Kirchengemeinden im Stadtgebiet sind heute die Gemeinden folgender Kirchen: Christuskirche, Friedenskirche, Großheide, Hardt, Odenkirchen, Rheindahlen (Martin Luther), Rheydt und Wickrathberg.

Am 4. September 2009 wurde die "Jugendkirche Rheydt" in Mönchengladbach offiziell eröffnet. Ihr Mittelpunkt ist das "Haus der Jugendkirche" im Zentrum von Mönchengladbach-Rheydt. Hier werden die punktuelle Jugendarbeit und die Kräfte aus den verschiedenen Bezirken unter einem Dach gebündelt. Das "Haus der Jugendkirche" ist ein ehemaliges Schwesternwohnheim und gehört zur Evangelischen Kirchengemeinde Rheydt.

Folgende Freikirchen sind in Mönchengladbach vertreten: eine Gemeinde der Evangelisch-methodistischen Kirche, seit 1867 eine Evangelisch-Freikirchliche Gemeinde (Baptisten), eine Christengemeinde (eine Pfingstgemeinde im BFP), sowie die Siebenten-Tags-Adventisten.
Der „Treffpunkt-H Christen in Mönchengladbach“, eine freie charismatische Gemeinde, ist eine Freikirche und gehört zum „Forum Leben Netzwerk“. Weiterhin hat die Freie evangelische Gemeinde, kurz FeG, ein Gemeindegründungsprojekt mit zwischenzeitlich regelmäßigen Gottesdiensten in Mönchengladbach.

Zudem gibt es weitere christliche religiöse Vereinigungen: die Christengemeinschaft, die Neuapostolische Kirche, die Mormonen (Kirche Jesu Christi der Heiligen der Letzten Tage) und die Christadelphians.

Ab dem 18. Jahrhundert bis in die 1930er Jahre bestand in Wickrath und Odenkirchen eine jüdische Gemeinde.
Im 19. Jahrhundert existierte jeweils eine Synagoge in Mönchengladbach, Wickrath, Rheydt und Odenkirchen. Im Novemberpogrom 1938 wurden sie zerstört. Nach dem Zweiten Weltkrieg bildete sich in Mönchengladbach wieder eine jüdische Gemeinde. 1967 wurde ein neues Gemeindezentrum mit einer Synagoge eingeweiht (Gemeindehaus mit Gebetssaal).

In den letzten Jahrzehnten bildete sich auch eine größere muslimische Gemeinde in Mönchengladbach, bestehend vor allem aus Einwanderern aus muslimischen Ländern und deren Nachkommen. Moscheen betreiben die islamischen Verbände Türkisch-Islamische Union der Anstalt für Religion/DITIB (Diyanet Merkez Camii/Zentrum-Moschee), Verband der Islamischen Kulturzentren/VIKZ und Millî Görüş/IGMG (Hz.-Bilal-Moschee) sowie die Vereine ar-Rahman (auf Arabisch), Nimet Vakfi (Dergah), Islamischer Kulturverein der Roma sowie IAIK Masjid Hijrat e. V. (auf Pashto und Dari). Den örtlichen Aleviten wurde 2012 die bis dahin evangelisch-methodistische Kreuzkirche als neuer Cem-Gemeindesaal übereignet. Im Jahr 2011 verhinderte eine Bürgerinitiative nach langen Auseinandersetzungen in Mönchengladbach-Eicken die Eröffnung einer radikalen salafistischen Moschee.

Gemäß Zensus 2011 waren 53,5 % der Bevölkerung Mönchengladbachs katholisch, 19,9 % evangelisch, 1,5 % orthodox, 4,4 % gehörten anderen und 20,6 % keiner öffentlich-rechtlichen Religionsgemeinschaft an – zu den letzteren gehören auch die Mönchengladbacher Muslime.

Das Stadtgebiet Mönchengladbachs entwickelte sich wie folgt:
Die französische Besatzung bildete im Jahr 1798 aus dem alten, ehemals zum Herzogtum Jülich gehörenden Amt Gladbach mit der Stadt Gladbach fünf Bürgermeistereien:


Bereits zwei Jahre später, im Jahr 1800, wurde Hardt der Bürgermeisterei Gladbach zugeordnet, doch gab es bis 1. Januar 1807 noch eine eigene Verwaltungsstelle. Zum 1. Januar 1836 erhielt Hardt jedoch wieder seine Selbständigkeit und wurde um die Honschaften Hehn und Winkeln sowie den restlichen Teil von Vorst, die alle bis dahin zur Bürgermeisterei Obergeburth gehörten, vergrößert. Der Rest von Obergeburt wurde nach Gladbach eingemeindet, ebenso die Honschaften Eicken, Hardterbroich und Teile von Lürrip, die bis dahin zur Bürgermeisterei Oberniedergeburth gehörten. Der Rest von Oberniedergeburth (Üdding und Lürrip teilw.) wurde mit Unterniedergeburt zur Gemeinde Neuwerk zusammengeschlossen. Somit gab es ab 1836 noch drei Gemeinden: "Stadt Gladbach" und Gemeinden "Hardt" und "Neuwerk". Gladbach war jedoch weiterhin in die drei Spezialgemeinden Gladbach, Oberniedergeburth und Obergeburth unterteilt. Teile von Obergeburth wurden 1859 als Gemeinde "Obergeburth" ausgegliedert, während die anderen Teile Obergeburths und Oberniedergeburth bereits seit 1857 zur Stadt Gladbach gehörten, die zum 1. Januar 1888 unter dem Namen „München-Gladbach“ kreisfrei wurde. Zum 1. Januar 1907 wurde die Bürgermeisterei Obergeburth in "Gladbach-Land" umbenannt, bevor diese zum 1. August 1921 mit der Gemeinde Neuwerk und der "Stadt Rheindahlen" (Stadtrechte seit 1354, bis 1876 nur Dahlen) nach München-Gladbach eingemeindet wurde.

Am 1. August 1929 erfolgte die Vereinigung der kreisfreien Stadt München-Gladbach mit der Gemeinde Hardt, der kreisfreien Stadt (seit 1907) Rheydt, der Stadt Odenkirchen und dem Amt Schelsen (mit der Gemeinde Giesenkirchen) zur neuen kreisfreien „Stadt Gladbach-Rheydt“. Doch wurde diese Stadt bereits am 1. August 1933 wieder in zwei Städte aufgeteilt: Stadt „München Gladbach“ (mit Hardt) und Stadt „Rheydt“ (mit Odenkirchen, Giesenkirchen und Schelsen).

Beide Städte wurden dann im Rahmen der nordrhein-westfälischen Gebietsreform am 1. Januar 1975 unter Einbeziehung der Gemeinde Wickrath (1933 gebildet aus dem Amt Wickrath mit den Ortschaften Beckrath, Wickrathhahn, Herrath, Wickrathberg und Buchholz sowie der Gemeinde Wanlo) zur heutigen kreisfreien Stadt Mönchengladbach vereinigt.

Das Zusammengehörigkeitsgefühl der Stadtteile gilt (Stand 2004) als wenig ausgeprägt. Laut einer Studie der Stadtentwicklung Mönchengladbach vom Juni 2004 ist die Stadt eine „nach wie vor innerlich zerrissene Großstadt“, deren „Handeln bisweilen nicht durch das gemeinschaftliche Ziel (bestimmt ist), die Gesamtstadt voranzubringen, sondern vielmehr dadurch, die Stadtbezirksnachbarn mit kritischem Blick zu beobachten“.

Im Jahr 1890 lebten in "München-Gladbach" rund 50.000 Menschen. Am 1. August 1921 schlossen sich die kreisfreie Stadt München-Gladbach (64.031 Einwohner 1919), die Landgemeinden München-Gladbach (22.614 Einwohner 1919) und Neuwerk (11.836 Einwohner 1919) sowie die Stadt Rheindalen (8308 Einwohner 1919) zur neuen Großstadt München-Gladbach mit etwa 110.000 Einwohnern zusammen. Am 1. August 1929 stieg durch die Eingemeindung von Rheydt (45.095 Einwohner 1925), Odenkirchen (20.023 Einwohner 1925) und weiterer Orte die Einwohnerzahl der neuen kreisfreien Stadt "Gladbach-Rheydt" auf rund 200.000. Nach der am 1. August 1933 erfolgten Ausgliederung der Stadt Rheydt (mit Odenkirchen, Giesenkirchen und Schelsen) und den Zerstörungen im Zweiten Weltkrieg halbierte sich die Bevölkerung der Stadt "München-Gladbach" im Jahr 1945 auf nur noch 106.000 Einwohner.

Am 11. Oktober 1960 erfolgte die Umbenennung in "Mönchengladbach". 1968 überschritt die Einwohnerzahl der Stadt Rheydt die Grenze von 100.000, wodurch sie zur Großstadt wurde. Die Eingemeindung von Rheydt (99.963 Einwohner) und Wickrath (13.961 Einwohner) am 1. Januar 1975 brachte einen Zuwachs von 114.000 Personen auf 263.000 Einwohner. Im Jahr 1996 erreichte die Einwohnerzahl der Stadt mit 266.873 ihren historischen Höchststand. Am 31. Dezember 2007 betrug die „Amtliche Einwohnerzahl“ für Mönchengladbach nach Fortschreibung des Landesamtes für Datenverarbeitung und Statistik Nordrhein-Westfalen 260.018 (nur Hauptwohnsitze und nach Abgleich mit den anderen Landesämtern).

Statistische Kennzahlen für Mönchengladbach:

An der Spitze der Stadt Gladbach standen zunächst die Gerichtsschöffen. Ferner gab es einen Vogt als Vertreter des Landesherrn. Ab 1400 sind erstmals Bürgermeister erwähnt, von denen es zunächst sechs auf Lebenszeit gewählte gab. Diese bildeten den Magistrat. Drei waren Schöffen, die anderen drei von der Bürgerschaft gewählt. Schied ein Schöffe aus, trat der vom Amtmann ernannte Schöffe an dessen Stelle. Schied ein Mitglied der von den Bürgern gewählten Mitglieder aus, wurde aus mehreren Kandidaten ein neuer gewählt. Vom gesamten Magistrat traten zwei Bürgermeister in jährlichem Wechsel als regierender und beisitzender Bürgermeister hervor. Sie leiteten die Stadtverwaltung. Einen Rat gab es erstmals seit 1446, dessen Vorsitzender der regierende Bürgermeister war. In französischer Zeit wurde ab 1797 die Munizipalverfassung eingeführt. Stadtoberhaupt war der ehrenamtliche „Maire“, dem zwei ehrenamtliche Beigeordnete und ein Municipalrat zur Seite standen. 1815 wurde die preußische Bürgermeistereiverfassung, 1845 die Gemeindeordnung der Rheinprovinz und 1856 die preußische Städteordnung. Danach leitete ein Bürgermeister beziehungsweise ab 1869 meist ein Oberbürgermeister die Stadtverwaltung. Ihm stand auch weiterhin ein Rat zur Seite.

Während der Zeit des Nationalsozialismus wurde der Oberbürgermeister von der NSDAP eingesetzt. Nach dem Zweiten Weltkrieg bestimmte die Militärregierung der Britischen Besatzungszone einen neuen Oberbürgermeister und führte 1946 die Kommunalverfassung nach britischem Vorbild ein. Danach gab es einen vom Volk gewählten „Rat der Stadt“, dessen Mitglieder die Bezeichnung „Stadtverordnete“ tragen. Der Rat wählte anfangs aus seiner Mitte den ehrenamtlichen Oberbürgermeister als Vorsitzenden und Repräsentanten der Stadt. Des Weiteren wählte der Rat ab 1946 ebenfalls einen hauptamtlichen Oberstadtdirektor als Leiter der Stadtverwaltung. 1999 wurde landesweit die Doppelspitze in der Stadtverwaltung aufgegeben. Seither gibt es nur noch den hauptamtlichen Oberbürgermeister. Dieser ist Vorsitzender des Rates, Leiter der Stadtverwaltung und Repräsentant der Stadt. Er wurde 1999 erstmals direkt vom Volk gewählt. Erste direkt gewählte Oberbürgermeister in Mönchengladbachs wurde Monika Bartsch (CDU). Die Büros der Stadtverwaltung sind heute auf die Innenstädte der ehemals selbstständigen Kommunen Mönchengladbach und Rheydt verteilt.

In der XV. Amtsperiode (2009 bis 2014) des Stadtrats von Mönchengladbach waren sieben Parteien und eine Wählergruppe vertreten. SPD, Bündnis 90/Die Grünen und FDP hatten sich auf Basis eines Kooperationsvertrages zur Ratsmehrheit zusammengeschlossen.

Erstmals seit dem Zweiten Weltkrieg war die CDU nicht an der Ratsmehrheit beteiligt. Der Stadtrat besteht aus 66 Stadträten und dem direkt gewählten und stimmberechtigten Oberbürgermeister, der die Sitzungen leitet, aber dem Stadtrat nicht angehört.


In der XVI. Amtsperiode (2014 bis 2019) sind neun Parteien/Wählergruppen und ein parteiloses Mitglied im Rat vertreten: 29 Sitze entfallen auf die CDU (41,5 % der Stimmen), 20 auf die SPD (29,4 %), 7 auf die Grünen (10,7 %), jeweils 3 auf die FDP (4,5 %) und Linke (4,5 %) und jeweils einer auf Pro NRW (1,9 %), die Freien Wähler (1,8 %), die Piraten (1,5 %), die PARTEI (1,3 %) und die NPD (0,9 %). Das parteilose Ratsmitglied, das für die AfD kandidierte und später aus der Partei austrat, erreichte 1,5 %.

In der Wahlperiode 2014–2020 haben CDU und SPD eine Kooperation vereinbart.


Die Gesamtsumme der Verschuldung der Stadt Mönchengladbach belief sich zum Jahresende 2012 auf 1,439 Milliarden Euro. Jeder Einwohner war dementsprechend mit 5.647 Euro verschuldet.

Der Stadt Mönchengladbach ist mit Urkunde des Regierungspräsidenten in Düsseldorf vom 7. Februar 1977 das Recht zur Führung eines Wappens und einer Flagge verliehen worden. Der Entwurf des Wappens stammte von Walther Bergmann.

Die heraldische Beschreibung des Wappens liest sich wie folgt:
Der Wechselbalken steht für die Herren von Quadt, die in Wickrath bis 1796 regierten. Der Abtsstab steht für die Benediktiner, die bis 1802 auf dem Abteiberg Mönchengladbach ansässig waren und das Kreuz symbolisiert die Herren von Bylandt, die in Rheydt herrschten.

Die Stadt Mönchengladbach führt ein Banner Rot-Gold (Gelb)-Rot im Verhältnis 1:4:1, längsgestreift mit dem über die Mitte nach oben verschobenen Wappenschild der Stadt, sowie eine Hissflagge Rot-Gold (Gelb)-Rot 1:4:1 quer gestreift mit dem zur Stange verschobenen Wappenschild der Stadt.

Die alte Stadt Mönchengladbach führte von 1903 bis 1929 und vom 1. August 1933 bis 31. Dezember 1974 ein anderes Wappen, das den Hl. Vitus sowie einen schwarzen Löwen, beide von einem blauen Wellenbalken getrennt, zeigte. Der Hl. Vitus ist der Patron der Benediktinerabtei und damit der Stadt, der Löwe das Wappentier der Herren von Jülich, die die Siedlung Gladbach zur Stadt erhoben. Der Wellenbalken symbolisiert den Gladbach und machte damit das Stadtwappen zu einem „redenden Wappen“. Die Sterne symbolisieren die "Vierzehn Nothelfer", zu denen Vitus gezählt wird. Der Entwurf des Wappens nach Anregung des damaligen Oberbürgermeisters Hermann Piecq stammt von dessen Schwager Felix Hauptmann, dem das bereits seit dem Mittelalter verwendete städtische Siegel als Vorbild diente.

Das Wappen der Stadt Gladbach-Rheydt wurde vom 21. Juni 1932 bis 31. Juli 1933 verwendet und stammt im Entwurf vom damaligen Leiter des städtischen Archivs, Julius Koenzgen, der dazu Elemente des Gladbacher und des Rheydter Wappens kombinierte.

Mönchengladbach unterhält Städtepartnerschaften mit folgenden Städten:


Mönchengladbachs industrieller Aufstieg wurde vor allem durch die Entwicklung der Textilindustrie von der Mitte des 19. bis zur Mitte des 20. Jahrhunderts geprägt. Daneben entwickelte sich auch eine textilorientierte Maschinenindustrie.

Nach dem Zweiten Weltkrieg setzte ein umfassender Strukturwandel ein, bei dem die Bedeutung der Textilindustrie abnahm und neue Wirtschaftszweige an Bedeutung gewannen. Derzeit sind nur noch 7 Prozent der Beschäftigten in der einst so dominanten Textil- und Bekleidungsindustrie tätig (Beispielsweise Van Laack und gardeur). Im Rahmen der erfolgreichen Diversifizierung des Wirtschaftsstandorts haben die Stadt und Vertreter der Unternehmen vor Ort im Jahr 1997 die Wirtschaftsförderung Mönchengladbach (WFMG) gegründet. Die WFMG hat gemeinsam mit der Hochschule Niederrhein ein Fünf-Säulen-Modell für die künftige Ausrichtung des Wirtschaftsstandorts entwickelt. Hiernach sind Textil und Mode, Maschinenbau und Elektrotechnik, Logistik, die Kreativwirtschaft sowie das Gesundheitswesen als Leitbranchen für die wirtschaftliche Zukunft der Stadt Mönchengladbach bestimmend. Im Rahmen einer aktiven Clusterpolitik hat die WFMG zum Teil eigene Netzwerke für diese Branchen initiiert.

Größter Arbeitgeber in Mönchengladbach ist die Santander Consumer Bank, die ihren Hauptsitz an der Aachener Straße in der Mönchengladbacher Innenstadt hat.

Zu den Produkten der breit gefächerten Wirtschaftsstruktur der Stadt zählen unter anderem Werkzeug- und Spinnmaschinen (Dörries Scharmann, Monforts, Trützschler, Schlafhorst), automatische Förderanlagen, Signal- und Systemtechnik (Scheidt & Bachmann), Transformatoren (SMS Meer), Kabel (Nexans Deutschland ehemals Kabelwerk Rheydt), Druckerzeugnisse sowie Nahrungs- und Genussmittel.

Darüber hinaus sind noch Bierbrauereien in Mönchengladbach vertreten, wo vor allem niederrheintypische Altbiere produziert werden.

Vor allem in den 1970er und 1980er Jahren lockte das attraktive Nachtleben junges Szene-Publikum aus Düsseldorf an. Inzwischen spielt insbesondere der Tourismus eine gewisse Rolle. Mönchengladbach verfügt über etwa 40 Hotels (zum Beispiel: Hotel Burgund) und Gasthöfe sowie eine Jugendherberge im Stadtteil Hardter Wald. Insgesamt sind etwa 2000 Betten vorhanden. Die Zahl der Übernachtungen beträgt jährlich über 200.000.

Statistische Kennzahlen für Mönchengladbach:

Die beiden Hauptbahnhöfe Mönchengladbach und Rheydt liegen an den Hauptstrecken von Köln nach Venlo, Duisburg–Mönchengladbach sowie Aachen–Düsseldorf. Mönchengladbach Hbf hat darüber hinaus einen S-Bahn-Anschluss nach Düsseldorf und Wuppertal (Linie S 8). Weitere Bahnhöfe befinden sich in Odenkirchen, Wickrath und Rheindahlen, außerdem liegen die Haltepunkte Herrath und Lürrip (S-Bahn) sowie Genhausen (RB) im Stadtgebiet.

Nach Forderungen aus der Politik wird der Mönchengladbacher Hauptbahnhof renoviert. Aufzüge und teilweise neue Treppen sind mittlerweile fertiggestellt. Weiterhin wurde die Empfangshalle bis zum Jahr 2011 im Rahmen der Fußball-Weltmeisterschaft der Frauen 2011 modernisiert und neu strukturiert. Der Fernverkehr der Deutschen Bahn AG bediente zwischen 2001 und 2009 die Stadt nur in Ausnahmefällen: Wenn auf der Bahnstrecke Amsterdam–Arnhem oder der ebenfalls planmäßigen Strecke des ICE International von Amsterdam nach Frankfurt (Main) Gleisbauarbeiten stattfinden, wird er ab Utrecht über ’s-Hertogenbosch, Eindhoven, Venlo und Mönchengladbach nach Köln umgeleitet. Dies galt auch für City Night Line-Züge.

Seit Dezember 2009 verkehren je ein ICE und ein IC freitags und sonntags, seit 2013 täglich ein IC von Aachen über Mönchengladbach und Rheydt nach Berlin und zurück. Damit ist Mönchengladbach wieder an den regelmäßigen Fernverkehr der Bahn angeschlossen.

Neben den Bahnlinien werden im öffentlichen Personennahverkehr (ÖPNV) die Omnibuslinien von der NEW mobil und aktiv Mönchengladbach (früher NVV mobil und aktiv) betrieben (Montag bis Freitag etwa 5.00 Uhr bis etwa 23.30 Uhr, Samstag etwa 5.30 Uhr bis 0.00 Uhr, Sonntag etwa 9.00 Uhr bis etwa 23.30 Uhr). Alle Linien sind zu einheitlichen Preisen innerhalb des Verkehrsverbundes Rhein-Ruhr (VRR) zu benutzen. An den Wochenenden und in den Nächten vor Feiertagen verkehren bis 3.15 Uhr stündlich Nachtbusse auf sechs Linien als Nachtexpress aus den Stadtzentren von Mönchengladbach und Rheydt in die anliegenden Stadtteile. Mönchengladbach ist eine von wenigen größeren Städten im VRR-Gebiet, die über keine Straßenbahn mehr verfügt. Die ehemalige Straßenbahn Mönchengladbach wurde 1969 stillgelegt.

Der Verkehrslandeplatz Mönchengladbach wird hauptsächlich durch die private und gewerbliche Allgemeine Luftfahrt genutzt. Ein Linienverkehr findet gegenwärtig nicht statt. Unweit von Mönchengladbach liegt der internationale Flughafen Düsseldorf, der auch Mehrheitsgesellschafter des Mönchengladbacher Flughafens ist (70,03 %). Weitere Anteile halten die NEW mobil und aktiv Mönchengladbach GmbH (29,96 %) und die Stadt Willich (0,01 %). Aufgrund der für den Luftverkehr mit größeren Jets zu kurzen Start- und Landebahn von 1200 m sollte der Flughafen ausgebaut werden um einen erneuten Linienflugbetrieb zu ermöglichen. Im hierzu notwendigen Planfeststellungsverfahren bei der Bezirksregierung Düsseldorf gab es im Regionalrat Düsseldorf keine Mehrheit für eine Verlängerung der Start- und Landebahn. Die Düsseldorfer Flughafengesellschaft hat aktuell keine Pläne für einen Ausbau des Mönchengladbacher Flughafens.

Mit dem Automobil ist die Stadt über die Bundesautobahnen A 44 (Aachen–Kassel), A 46 (Heinsberg–Bestwig) und A 52 (Roermond–Essen-Marl) sowie A 61 (Venlo–Ludwigshafen) zu erreichen. Mit dem Autobahnkreuz Mönchengladbach im Nordosten und dem Autobahnkreuz Mönchengladbach-Wanlo im Südosten gibt es zwei Autobahnkreuze im Stadtgebiet. Ferner führen die Bundesstraßen B 57, B 59 und B 230 durch das Stadtgebiet. Die Entfernungen in die umliegenden größeren Städte betragen nach Düsseldorf 30 Kilometer, nach Duisburg 35 Kilometer, nach Krefeld 20 Kilometer, nach Köln 60 Kilometer, nach Aachen 60 Kilometer, nach Venlo in den Niederlanden 35 Kilometer und nach Roermond, ebenfalls in den Niederlanden, 35 Kilometer.

Mönchengladbach liegt an der EUROGA-Radroute. Die überwiegend flache niederrheinische Landschaft bietet viele Möglichkeiten für Freizeittouren, zum Beispiel entlang der Niers und auf der NiederRheinroute. Beschilderte Routen auf dem Radverkehrsnetz NRW existieren auch in die benachbarten Kreise Viersen, Neuss und Heinsberg sowie bis nach Düsseldorf und Krefeld. Der Ausbau des innerstädtischen Radwegenetzes war in der Vergangenheit Gegenstand politischer Diskussionen, soll aber offenbar kurzfristig angegangen werden.
Am Hauptbahnhof Rheydt befindet sich eine moderne Radstation, die ein komfortables und sicheres Abstellen von Fahrrädern kombiniert mit weiteren Serviceleistungen ermöglicht.
Eine weitere Radstation am Hauptbahnhof Mönchengladbach befindet sich in Planung und soll 2016 eröffnet werden.

An der Lüpertzender Straße befindet sich die lokale Radiostation Radio 90,1 Mönchengladbach. Sie ging am 29. September 1990 auf Sendung, am 30. Oktober 1990 wurde die erste Sendung im Rahmen des Bürgerfunkes über die Frequenzen des Senders ausgestrahlt.

Seit dem 19. Januar 2009 sendet der Regionale Fernsehsender CityVision täglich von 16:00 bis 24:00 Uhr auf dem Kabelkanal S18 Nachrichten aus Mönchengladbach und dem Umland. Begonnen wurde Jahr zuvor mit der Ausstrahlung auf öffentlichen Leinwänden, zum Beispiel am Alten Markt, in Krankenhäusern und/oder in Fitnessstudios.

In Mönchengladbach wird keine eigene Tageszeitung herausgegeben. Über das lokale und regionale Geschehen berichten die Rheinische Post und die Westdeutsche Zeitung sowie zwei kostenlose, werbefinanzierte Zeitungen, die einmal wöchentlich an alle Haushalte verteilt werden (mittwochs der Stadtspiegel und sonntags der Extra-Tipp am Sonntag). Der "Hindenburger" ist eine Stadtzeitschrift rund um Lifestyle, Kultur, Freizeit und Gastronomie, die monatlich kostenlos erscheint und als Auslage in Geschäften verteilt wird. Das "GURU Magazin" mit Ausrichtung auf Gesellschaft und Lifestyle wird an Haushalte direkt verteilt. Seit 1997 gibt es eine Lokalausgabe der Straßenzeitung Fiftyfifty, die von Menschen in sozialer Not verkauft wird und vom Diakonischen Werk Mönchengladbach e. V. herausgegeben wird.

In der seit Dezember 2007 existierenden Online-"Bürgerzeitung Mönchengladbach" können die Bürger selber unter der Creative Commons Artikel veröffentlichen, Themenschwerpunkt ist die Kommunalpolitik. Kommunalpolitiker nutzen dieses Medium.

In Mönchengladbach befindet sich auch ein Mittelwellensender des AFN, der auf 1143 kHz mit 1 kW Sendeleistung arbeitet. Er verwendet als Sendeantenne einen 45,5 Meter hohen, gegen Erde isolierten Sendemast bei 51° 10′ 2″ N, 6° 23′ 56″ O.


In Mönchengladbach befinden sich die Fachbereiche Oecotrophologie, Sozialwesen, Textil- und Bekleidungstechnik und Wirtschaftswissenschaften der Hochschule Niederrhein – University of Applied Sciences, deren Sitz sich in Krefeld befindet. Die Fachhochschule wurde 1971 durch Zusammenschluss von zwölf Fach- und Ingenieurschulen, die sich auf mehrere Städte verteilten, gegründet.

In Mönchengladbach findet sich ausserdem das gesamte Spektrum an allgemein bildenden und beruflichen Schulen in öffentlicher und in privater Trägerschaft. Insgesamt besitzt die Stadt 95 Schulen, von den Grundschulen bis zur Hochschule, darunter auch eine Waldorfschule.

Die Musikschule der Stadt Mönchengladbach bietet eine musikalische Ausbildung von Kindern bis zur Vorbereitung der Aufnahmeprüfung an einer Musikhochschule. Mehr als 4000 Schüler nehmen die Angebote wahr.

Die medicoreha Welsink Akademie führt mit ihren staatlich anerkannten Fachschulen für Physiotherapie und Ergotherapie für 450 Auszubildende die praktische Ausbildung in den Kliniken Maria Hilf durch.

Das MG-Wolfpack spielt aktuell in der Oberliga NRW und trägt die Heimspiele im Grenzlandstadion aus. Die Footballabteilung ist Teil des American Sports Mönchengladbach e. V. Head Coach für 2016 ist David Lux.

Dem 1. PBC Neuwerk gelang in der Saison 2014/15 der Aufstieg in die 1. Poolbillard-Bundesliga. Seit der Saison 2016/17 spielt der Verein wieder in der 2. Poolbillard-Bundesliga. Mit Sascha Jülichmanns wurde 2014 zudem ein Mönchengladbacher Deutscher Meister.

Der Rheydter Turnverein 1847 e. V. mit seiner Bogensportabteilung ist Mönchengladbachs erfolgreichster Verein im Bogenschießen. Er hat jeweils ein Team in der 1. Bundesliga Nord und in der Regionalliga West. 2008 hat es die Bundesligamannschaft zum ersten Mal in der Vereinsgeschichte in das Bundesligafinale geschafft.

Mönchengladbachs größter, bekanntester und erfolgreichster Fußballverein ist Borussia Mönchengladbach. Der Traditionsklub feierte in den 1970er Jahren fünf deutsche Meisterschaften und zwei UEFA-Cup-Siege. Der Verein spielt in der Bundesliga und trägt seine Heimspiele im Stadion im Borussia-Park aus. Borussias U-23-Mannschaft spielt in der Regionalliga und trägt ihre Heimpartien in der Regel im Grenzlandstadion aus. Der älteste Fußballverein der Stadt ist der 1. FC Mönchengladbach. Aus diesem Verein ging Günter Netzer hervor, der von dort zur Borussia Mönchengladbach wechselte. Der Klub wurde 1894 gegründet und spielt aktuell in der Landesliga. Er ist zudem der älteste Fußballverein im Westdeutschen Fußballverband.

In der Stadt sind zahlreiche Vereine mit Junioren- und Seniorenteams vertreten. Die aktuell höchstklassige Mannschaft ist die 1. Mannschaft (Männer) der Handballabteilung von Borussia Mönchengladbach, die in der Oberliga Niederrhein aktiv ist. Heimspiele werden in der Jahnhalle ausgetragen. Aus dem Stadtgebiet sind mehrere Mannschaften ebenfalls in der Verbandsliga vertreten (Frauen: Rheydter TV 1847, Tschft Lürrip; Männer: TV Einigkeit Geistenbeck).

Der Gladbacher HTC spielt in der Feldhockey-Bundesliga. Der Rheydter Spielverein spielt in der 2. Feldhockey-Bundesliga. Der Gladbacher HTC wurde 2002 deutscher Meister und trägt die Heimspiele auf der vereinseigenen Anlage oder im SparkassenPark Mönchengladbach aus. Dort fand vom 6. bis 17. September 2006 die Feldhockey-Weltmeisterschaft der Herren 2006 statt. Der Rheydter Spielverein ist der älteste Hockeyverein in Mönchengladbach, er wurde 2001 der bislang letzte Deutsche Pokalsieger. Er trägt seine Spiele auf dem eigenen Kunstrasenplatz am Grenzland-Stadion aus. Der RSV war am Ende der Saison 2008/09 in die 1. Bundesliga aufgestiegen.

Der 1. Judo-Club Mönchengladbach ist derzeit der stärkste Judoverein am Niederrhein. Die Frauenmannschaft kämpft seit 2000 in der 1. Bundesliga, die Männermannschaft stieg 2008 in die 1. Bundesliga auf. Der bekannteste Judoka des 450 Mitglieder zählenden Vereins ist der dreifache Olympiateilnehmer und Bronzemedaillengewinner von 2012 Andreas Tölzer.

Auf der Trabrennbahn Mönchengladbach veranstaltet der „Verein zur Förderung des Rheinischen Trabrennsportes e. V.“ (bis Ende 2007 „Trabrennverein Mönchengladbach e. V.“) regelmäßig Trabrennen.

Die Vereinigten Städtischen Bühnen Krefeld und Mönchengladbach bieten Oper, Operette, Musical, Schauspiel und Ballett in den Theaterhäusern beider Städte. Das Kooperationstheater wurde 1950 gegründet. Das begleitende Orchester der Bühnen sind die „Niederrheinischen Sinfoniker“, die auch Sinfoniekonzerte geben.

Ferner gibt es das „BIS“ im ehemaligen Museum auf der Bismarckstraße, ein Zentrum für alle Arten von Kultur, so auch Malerei, Jazz und zeitgenössische Musik, „Die Spindel“, eine Kleinkunstbühne im Don Antonio (Haus Ohlenforst), „Das Rote Krokodil“, eine Kleinkunstbühne im Stadtteil Wickrath, die Kleinkunstbühne „Theater im Gründungshaus“ im Stadtteil Eicken und den „Theater Spielplatz“, Mönchengladbachs kleinste Bühne. Sie bietet Kabarett, Theater und Literatur und befindet sich in Rheydt.

Die „MusicCommunity MG“ ist ein loser Zusammenschluss von Veranstaltern und Kulturförderern in Kooperation mit dem Fachbereich Weiterbildung und Musik der Stadt Mönchengladbach. Jährlich werden auf zehn Veranstaltungen Künstler aus dem Bereich der Popularmusik gefördert. Mit den zweitägigen Musikfestivals citymovement indoor (2006) und dem citymovement outdoor (2007) hat die MusicCommunity MG die Tradition des größten Umsonst- und Draußen-Festivals der Region wieder aufgenommen. Einen weiteren Neuanfang des Umsonst- und Draußen-Festivals am Platz der Republik gibt es seit 2009 unter dem Namen „Horst-Festival“ unter der Organisation des AStA der HSNR.

Mönchengladbacher Musikgruppen mit internationalen Erfolgen waren in den 1970er Jahren die Krautrock Band Wallenstein und Anfang bis Mitte der 1990er Jahre die Rockband Sun. Zwischen 1985 und 1991 galt die Jazz-Rock-Pop Band „Newtation“ als regional erfolgreich. Bundesweit bekannt ist EA80, eine der ältesten deutschen Punkbands, die Hard-Rock-Band Motorjesus und der Rapper Eko Fresh.

Siehe: Liste der Museen in Mönchengladbach

Am 27. Januar 2006 hat Gunter Demnig im Stadtteil Odenkirchen die ersten drei Stolpersteine in Mönchengladbach verlegt. Bis November 2013 waren es im ganzen Stadtgebiet 241 an 69 Orten.





Klimatisch zeichnet sich Mönchengladbach durch eine Gunstlage aus, die von der Nordsee und vom Golfstrom beeinflusst wird. Die Winter sind schneearm und der Frühling setzt recht früh ein. Im Jahresdurchschnitt fällt ein Niederschlag von ca. 801 mm.
Am 1. Juli 1956 wurde das Kfz-Kennzeichen "MG" eingeführt. Es wird bis heute durchgehend ausgegeben.
Das frühere Kfz-Kennzeichen "RY" (Stadt Rheydt) wurde bisher nicht wieder eingeführt.

Einige der in Mönchengladbach geborenen Persönlichkeiten haben national oder international Bekanntheit, teilweise auch Anerkennung erlangt. Die international bedeutendsten dürften der Ingenieur Hugo Junkers, der Philosoph Hans Jonas, die ehemaligen Formel-1-Fahrer Nick Heidfeld und Heinz-Harald Frentzen sowie die Fußballer Jupp Heynckes und Günter Netzer sein. Joseph Goebbels, der Minister für Propaganda und Volksaufklärung während der Zeit des Nationalsozialismus, wurde in Rheydt geboren. Zu den national bekannten Persönlichkeiten zählen unter anderem der ehemalige Ministerpräsident des Landes Nordrhein-Westfalen Franz Meyers, der Tierfilmer Heinz Sielmann, der Kabarettist Volker Pispers, der Autor Walter Moers, der Wissenschaftsastronaut Reinhold Ewald, der Opernsänger Hermin Esser, der Komiker Monty Arnold und der virtuose Gitarrist Joscho Stephan. Ebenfalls ist der Fernsehmoderator Joko Winterscheidt aus Mönchengladbach.





</doc>
<doc id="3476" url="https://de.wikipedia.org/wiki?curid=3476" title="Rheydter Höhe">
Rheydter Höhe

Die Rheydter Höhe ist ein Trümmerberg im Mönchengladbacher Stadtteil Pongs, Stadtbezirk Süd. Im Volksmund wird die Erhebung als „Monte Clamotte“, „Monte Scherbelino“ oder „Rheydter Müllberg“ bezeichnet. Das Plateau des Hügels stellt mit den höchsten Punkt des Stadtgebiets dar. Mit einer Höhe von von Fuß bis Gipfel ist er der höchste Trümmerberg Deutschlands.

Die "Rheydter Höhe" entstand 1945 aus Trümmern, die die Bombardierung der Städte Mönchengladbach und Rheydt hinterlassen hatte. Durch die Bombardierung wurden 65 Prozent der beiden Stadtgebiete zerstört. In den 1950er Jahren deponierte man große Mengen Hausmüll auf dem Trümmerberg. Um das Wachstum von Pflanzen und Bäumen zu ermöglichen, fügte man eine rund ein bis zwei Meter hohe Humusschicht auf den Hausmüll. Dies hatte in den 1990er Jahren die Folge, dass Giftgasemissionen aus den Trümmern traten, von denen heute nichts mehr zu bemerken ist. Allerdings kann man etwa 30 Meter vor der Hochebene, dem Aussichtsplateau, vereinzelt Stücke von Plastiktüten an den Serpentinenwänden erkennen, die mit der Zeit vom Regen freigespült wurden.

Am nördlichen Fuß des Trümmerbergs befindet sich ein Wasserspielplatz, ein Weiher sowie der Rheydter Stadtwald mit Grillplätzen, Minigolf, einem Café sowie Möglichkeiten für andere Aktivitäten.

Die Dahlener Straße (eine der Hauptverkehrsstraßen der Stadt, verbindet Rheydt mit Rheindahlen) und die A 61 laufen südlich beziehungsweise westlich an der "Rheydter Höhe" vorbei.



</doc>
<doc id="3480" url="https://de.wikipedia.org/wiki?curid=3480" title="Mikronation">
Mikronation

Als Mikronation, Fantasiestaat oder Scheinstaat werden Gebilde bezeichnet, die wie eigenständige souveräne Staaten auftreten und den Anschein erwecken, mit staatlicher Autorität zu handeln, obwohl bei ihnen Eigenschaften fehlen oder wenigstens stark umstritten sind, die einen Staat nach Völkerrecht ausmachen. International anerkannte souveräne Staaten oder Institutionen verweigern daher die Anerkennung oder reagieren gar nicht, weil sie diese Gebilde und deren Ansprüche nicht ernst nehmen. Teilweise nehmen sich die Gründer selbst nicht ernst oder betreiben die Mikronation lediglich als Staatssimulation oder Spiel im Internet (virtuelle Nation).

Die "Staatshandlungen" einer Mikronation und von dieser herausgegebene Ausweise, Urkunden und Dokumente gelten im Rechtsverkehr als unwirksam oder bestenfalls als das zivilrechtliche Auftreten der privaten Interessengruppe oder Einzelperson, die hinter der Mikronation steht. Überwiegend handelt es sich dabei um Personen, die als exzentrisch gelten, oder damit eigentlich ganz andere, meist wirtschaftliche oder lokalpolitische Interessen verfolgen.

Die Herkunft und Bedeutung des Begriffs ist umstritten. Vor allem im deutschsprachigen Internet bezeichnen sich virtuelle Nationen, die lediglich als Simulation oder Online-Spiel existieren, selbst als "Mikronationen" oder abgekürzt "MN" bzw. "µN". Seit einigen Jahren setzt sich "Mikronation" jedoch zunehmend als Oberbegriff für alle Erscheinungsformen von "Schein- und Fantasiestaaten" bis hin zu "Cybernationen" und "Staatssimulationen" durch und findet auch für historische Anomalien oder Staatsgründungen Verwendung, die gescheitert sind oder nur ein zweifelhaftes Maß an Erfolg und Aufmerksamkeit gefunden haben.

"Mikronationen" sind von Zwergstaaten klar zu unterscheiden, deren Staatsgebiet zwar extrem klein ist, die aber alle Merkmale eines "echten" Staates nach Völkerrecht aufweisen und von anderen Staaten anerkannt werden. Näherliegend wäre der Vergleich mit den atypischen Völkerrechtssubjekten Heiliger Stuhl, Souveräner Malteserorden oder Internationales Komitee vom Roten Kreuz, denen es ebenfalls an den Voraussetzungen zu einem Staat fehlt. Diese sind jedoch von allen oder zumindest einer Mehrheit der Staaten international anerkannt.

Problematischer ist die Unterscheidung zu Gebilden wie den stabilisierten De-facto-Regimes. Auch diese werden gar nicht oder nur von wenigen Staaten offiziell als Staat anerkannt, und das Vorliegen von Staatsmerkmalen ist in der Regel umstritten. Mikronationen verfügen in der Regel jedoch nicht annähernd über deren Größe und Bevölkerung oder sind zumindest nicht in der Lage, ein vergleichbares Maß an tatsächlicher Kontrolle (Staatsgewalt) auszuüben.

Meist findet sich eine kleine Gruppe von Menschen oder gar eine Einzelperson, die den Herrschaftsanspruch für einen neuen Staat erklärt. Das neue Gebilde beansprucht entweder ein völkerrechtlich unklares Gebiet oder behauptet die Unabhängigkeit durch Abspaltung aus einem bestehenden Staat. Charakteristisch für viele Mikronation-Projekte ist das Erklären von unbewohnten Inseln bzw. Atollen anderer anerkannter Staaten zum eigenen Staatsgebiet. Der Besitz eines Territoriums soll die Legitimität des Anspruchs als Staat unterstreichen. Aus demselben Grund haben die meisten Mikronationen eine Flagge, Nationalhymne oder auch Ehrenzeichen und geben Pässe, Briefmarken und Münzen heraus.

Einige Mikronationen, die territoriale Ansprüche erheben, haben damit größere Bekanntheit erlangt, wie z. B. die Hutt-River-Provinz (eine Farm in Western Australia, die sich von Australien für unabhängig und zu einem freien Fürstentum erklärt hat), die Freistadt Christiania bei Kopenhagen, Sealand (eine ehemalige Seefestung vor der britischen Südost-Küste in der Nordsee) oder die Conch Republic (Key West in Florida). Am Wiener Vergnügungspark Prater steht seit 1982 das Kugelhaus der „Republik“ Kugelmugel – unbewohnt, ohne Strom- und Wasseranschluss und hinter Stacheldrahtzaun.

Oft ist es schwierig, eine Mikronation von einem imaginären Land zu unterscheiden. Rein virtuelle Nationen bezeichnen sich besonders in Deutschland selbst oft als Mikronationen und existieren nur im Internet als Website. Diese nehmen in der realen Welt keine staatlichen Rechte in Anspruch, sondern verstehen sich als Online-Spiel oder Simulation.

Mitglieder dieser virtuellen Mikronationen erwerben zunächst virtuelle Bürgerrechte und bilden dann ein komplettes Staatswesen und dessen wirtschaftliche Beziehungen als Politik- und Wirtschaftssimulation nach. Es gibt demokratische und diktatorische, auf die Gegenwart, aber auch auf Historie oder fiktive Zukunft bezogene Mikronationen. Aus privaten Foren, Wikis oder Portalseiten zu diesem Thema lässt sich schließen, dass allein im deutschsprachigen Internet zwischen 70 und 120 solcher Mikronationen existieren. Teilweise unterhalten Mikronationen diplomatische Beziehungen untereinander und lassen sich gemeinsam auf virtuellen Karten von Internet-Welt-Organisationen verzeichnen.

Einige virtuelle Mikronationen betreiben auch die Erforschung des virtuellen Weltraums oder beanspruchen außerirdische Territorien, z. B. auf dem Mars.




</doc>
<doc id="3498" url="https://de.wikipedia.org/wiki?curid=3498" title="Plasmid">
Plasmid

Plasmide sind kleine, in der Regel ringförmige, autonom replizierende, doppelsträngige DNA-Moleküle, die in Bakterien und in Archaeen vorkommen können, aber nicht zum Bakterienchromosom (Kernäquivalent) zählen, also extrachromosomal vorliegen (Abb. 1). Nur selten treten Plasmide auch in Eukaryoten auf (z. B. als "2-Mikrometer-Ring" in "Backhefe"). Ihre Größe beträgt 1 bis über 1000 kBp.

Plasmide können viele verschiedene Gene enthalten. Vermitteln diese Gene z. B. eine Antibiotika-Resistenz, kann daraus für das Wirtsbakterium ein Selektionsvorteil resultieren. Jedes Plasmid enthält mindestens eine Sequenz, die als Replikationsursprung (engl. "Origin of Replication", kurz ORI) dient. Ist der Replikationsursprung kompatibel zu dem Bakterienstamm, so kann das Plasmid unabhängig von der chromosomalen DNA repliziert werden (Abb. 2). Plasmide sind somit autonom replizierend (Replikons) und werden vererbt. Je nach Art des Replikationsursprungs liegen in einer Bakterienzelle wenige ("low-copy", aufgrund von Iterons, Antisense-RNA oder ctRNA) oder sehr viele Kopien ("high-copy") vor.

"Episome" sind bei Bakterien Plasmide, die sich in die chromosomale DNA des Wirtsorganismus integrieren können, wie das F-Plasmid (Abb. 3). Dort können sie für lange Zeit verbleiben, werden dabei mit jeder Zellteilung des Wirts mit repliziert und können sogar zu einem integralen Bestandteil seiner DNA werden. Sie können jedoch auch wie andere Plasmide eigenständig existieren.

Bei den Eukaryoten gelegentlich vorkommende virale Episome wie die cccDNA des Hepatitis-B-Virus oder die Episome der Herpesviren und Adenoviren integrieren nicht in das Genom des Wirtes, sondern liegen daneben im Zellkern vor. Eine Ausnahme bilden die Episome von Pockenviren, welche im Zytosol auftreten. Virale Episome sind Mechanismen zur Immunevasion bei Infektionen mit persistenten Pathogenen.

Es gibt zwei grundlegende Gruppen von Plasmiden, konjugierende und nicht-konjugierende. "Konjugierende" Plasmide enthalten ein so genanntes "tra"-Gen, das die "Konjugation", den parasexuellen Austausch von Plasmiden zwischen zwei Bakterien, auslösen kann (Abb. 4). "Nicht-konjugierende" Plasmide haben diese Fähigkeit nicht, sie können aber zusammen mit konjugierenden Plasmiden während der Konjugation übertragen werden.

Verschiedene Plasmidtypen können nebeneinander in ein und derselben Zelle existieren, in "Escherichia coli" z. B. bis zu sieben. Sind zwei Plasmide zueinander inkompatibel, wird eines von ihnen zerstört. Abhängig von der Fähigkeit, nebeneinander in derselben Zelle zu existieren, können sie also in "Inkompatiblitätsgruppen" eingeteilt werden.

Die Klassifikation von Plasmiden kann durch ihre Funktion erfolgen. Es werden fünf Hauptklassen unterschieden:

Eine besondere Art von Plasmiden stellen die sog. Ti-Plasmide (Tumor inducing) dar. Sie sind oft ein Bestandteil von bestimmten Bakterien ("Agrobacterium tumefaciens" oder "Agrobacterium rhizogenes") und werden von diesen in Pflanzen übertragen. Dort verursachen sie die einzige bekannte Krebserkrankung in Pflanzen.

Ein weiteres besonderes Plasmid ist pR1SE von "Halobacterium lacusprofundi R1S1", gefunden 2017 in der Antarktis. Dieses ist in der Lage, Vesikel (Schutzblasen) auszubilden. Es wird diskutiert, ob pR1SE eine Übergangs- oder Zwischenform zu Viren darstellen könnte.

Plasmide sind wichtige Werkzeuge der Molekularbiologie, Genetik, Biochemie und anderer biologischer und medizinischer Bereiche. Sie werden in der Gentechnik als Vektoren bezeichnet und dazu benutzt, um Gene zu vervielfältigen oder zu exprimieren. Die gezielte Anpassung eines Vektors wird als "Vektordesign" bezeichnet.
Viele der für diese Zwecke eingesetzten Plasmide sind kommerziell erhältlich, sie werden aber auch zwischen Labors geteilt und sind häufig bei nicht-kommerziellen Anbietern, wie "Addgene", erhältlich. Sie leiten sich von den natürlich vorkommenden Plasmiden ab und besitzen noch deren strukturelle Bestandteile wie den Replikationsursprung ("Origin of Replication", ORI). Außerdem wurden sie so verändert, dass sie leicht für Klonierungen verwendet werden können (sie enthalten dafür eine so genannte MCS ("Multiple Cloning Site"), welche Erkennungssequenzen für eine Vielzahl von Restriktionsenzymen enthält): Das zu vervielfältigende Gen wird in Plasmide eingefügt, die über ein Gen mit einer Antibiotika-Resistenz verfügen. Dann werden diese Plasmide in Bakterien eingebracht, die auf einem mit dem entsprechenden Antibiotikum behandelten Nährmedium wachsen. Es werden also nur die Bakterien überleben, die das Plasmid mit der Information für die Antibiotika-Resistenz aufgenommen haben. Bakterien, die das Plasmid nicht aufgenommen haben, sterben durch das Antibiotikum ab. So wirkt das Antibiotikum als Selektionsmarker, der nur die Bakterien mit dem Resistenzplasmid überleben lässt. Meist wird durch das Einfügen des Gens in das Plasmid ein anderes Gen (Reportergen) unterbrochen, welches dann nicht mehr exprimiert werden kann. Diese fehlende Eigenschaft kann zum Screening genutzt werden, weil nur solche Bakterien, welche ein Plasmid mit dem gewünschten Gen aufgenommen haben, diese Eigenschaft nicht mehr besitzen.
Bringt man Gene in Plasmide ein, welche dafür sorgen, dass die eingebrachten Gene in großer Menge exprimiert werden ("Expressionsplasmide"), kann man die entsprechenden Genprodukte in großen Mengen gewinnen. Diese Vorgehensweise erlaubt heute die Herstellung von rekombinantem Insulin (also dem Humaninsulin identischen Insulin), welches früher aufwendig aus Bauchspeicheldrüsen von Schweinen isoliert werden musste.

Soll ein DNA-Abschnitt in einen Organismus (Bakterium, Pflanze, Tier, Mensch etc.) eingebracht werden, können ebenfalls Plasmide als Überträger verwendet werden. Bei Tier und Mensch ist dieses Verfahren nur bei Zellen in Zellkultur wirklich etabliert, da dort die Zellen als Monolage gut zugänglich sind. Je nach Zelltyp benötigt man für den Transfer entweder gefällte DNA (Muskelzellen durch Endocytose) oder in Liposomen "verpackte" DNA. Es existieren weitere Methoden wie Elektroporation, welche die Transfektionseffizienz weiter erhöhen können.

Plasmide kann man durch geeignete Verfahren im Elektronenmikroskop sichtbar machen, etwa durch Färbung mit Uran-Atomen (BAC-Spreitung). Die Ringstruktur des Plasmids in Form einer um sich selbst gewundenen DNA hat dabei topologische Konsequenzen. Die Plasmide können in drei verschiedenen Gestaltformen vorliegen. Die Supercoil-Form ist die natürliche Konformation des Plasmids. Da die DNA-Doppelhelix um sich selbst gewunden ist und sich in einem geschlossenen Plasmid nicht entwinden kann, entsteht eine Torsionsspannung, wodurch sich das Plasmid im Raum um sich selbst krümmt. Der gleiche Effekt ist bei Telefonschnüren zu beobachten, die sich um ihre eigene Achse festwinden. Bei der offenkettigen Form ist einer der beiden DNA-Stränge an einer Stelle gebrochen (einem sogenannten "nick)," wodurch sich der offene Strang frei um den fixierten drehen kann; dadurch entspannt sich die Torsionsspannung, das Plasmid liegt offen vor. Bei der linearen DNA sind beide Stränge gebrochen, die Kreisstruktur ist aufgehoben.

Die Unterscheidung von Plasmiden und Chromosomen ist nicht ganz einfach. Eine Definition beinhaltet, dass Plasmide kleiner als Chromosomen sind. Allerdings ist z. B. das Plasmid pSymB von "Sinorhizobium meliloti" doppelt so groß wie das primäre Genom von "Mycoplasma pneumoniae". Eine andere Definition ist, das Plasmide keine essentiellen Gene tragen. Allerdings hängt die Frage, was ein „essentielles Gen“ ist, vom Lebensumfeld ab. Eine weitere Definition besagt, dass Plasmide in der Zelle in mehr Kopien vorliegen als Chromosomen. Allerdings liegen Plasmide wie das F-Plasmid in "E. coli" und viele andere in gleicher Kopienzahl wie das Chromosom vor. Ein entscheidendes Charakteristikum eines Chromosoms ist, dass seine Replikation mit dem Zellzyklus koordiniert ist. Das heißt, der Start der Replikation erfolgt bei gleichmäßigem Wachstum immer im gleichen Alter der Bakterienzelle. Nach dieser Definition wäre das zweite Chromosom von "V. cholerae" tatsächlich ein Chromosom. Für Replikons an der Grenze zwischen Chromosomen und Plasmiden wurde der Begriff Chromid vorgeschlagen.





</doc>
<doc id="3504" url="https://de.wikipedia.org/wiki?curid=3504" title="Nordsee">
Nordsee

Die Nordsee (veraltet "Westsee", "Deutsches Meer") ist ein Teil des Atlantischen Ozeans. Sie ist ein Schelfmeer und liegt im nordwestlichen Europa. Bis auf die Meerengen beim Ärmelkanal und beim Skagerrak ist sie auf drei Seiten von Land begrenzt und öffnet sich trichterförmig zum nordöstlichen Atlantik. In einem 150-Kilometer-Bereich an der Küste leben rund 80 Millionen Menschen.

Die Nordsee selbst ist ein wichtiger Handelsweg und dient als Weg Mittel- und Nordeuropas zu den Weltmärkten. Die südliche Nordsee ist zusammen mit dem angrenzenden Ärmelkanal die am dichtesten befahrene Schifffahrtsregion der Welt. Unter dem Meeresboden befinden sich größere Erdöl- und Erdgasreserven, die seit den 1970er Jahren abgebaut werden. Kommerzielle Fischerei hat den Fischbestand des Meeres in den letzten Jahrzehnten vermindert. Umweltveränderungen entstehen auch dadurch, dass die Abwässer aus Nordeuropa und Teilen Mitteleuropas direkt oder über die angrenzende Ostsee in das Meer fließen.
Die Nordsee liegt größtenteils auf dem europäischen Kontinentalschelf. Eine Ausnahme bildet lediglich ein schmales Gebiet der nördlichen Nordsee vor Norwegen. Die Nordsee wird begrenzt von der Insel Großbritannien im Westen und dem nord- und mitteleuropäischen Festland mit Norwegen (Nordost), Dänemark (Ost) sowie Deutschland (Südost), Niederlande (Süd), Belgien und Frankreich (Südwest).

Im Südwesten geht die Nordsee durch die Straße von Dover in den Ärmelkanal über, im Osten hat sie über Skagerrak und Kattegat Kontakt zur Ostsee und nach Norden öffnet sie sich trichterförmig zum Europäischen Nordmeer, das im Osten des Nordatlantiks liegt.

Neben den offensichtlichen Grenzen durch die Küsten der Anrainerstaaten wird die Nordsee durch eine gedachte Linie vom norwegischen Lindesnes hin zum dänischen Hanstholm in Richtung Skagerrak abgegrenzt. Die nördliche Grenze zum Atlantik ist naturräumlich weniger eindeutig. Traditionell wird eine gedachte Linie von Nordschottland über die Shetlands bis hin zum norwegischen Ålesund angenommen; nach dem Oslo-Pariser-Abkommen von 1962 verläuft sie etwas weiter westlich und nördlich entlang von 5° westlicher Länge und 62° nördlicher Breite auf Höhe des norwegischen Geirangerfjords.

Die Nord-Süd-Ausdehnung beträgt 1120 Kilometer von 50° 56' N bis 62° N. Die maximale Breite von Osten nach Westen beträgt 1001 Kilometer von 4° 26' W bis 9° 50' O.

Die Flächenausdehnung der Nordsee beträgt rund 575.000 Quadratkilometer bei einer durchschnittlichen Tiefe von 94 Metern, das ergibt ein Wasservolumen von rund 54.000 Kubikkilometer.

Für Fischerei- und Wettervorhersagen wie beispielsweise die des Seewetterdienstes Hamburg wird die Nordsee international in verschiedene Seegebiete untergliedert:

Westliche Nordsee von Norden nach Süden:

Östliche Nordsee von Norden nach Süden:

In die Nordsee münden zahlreiche Fließgewässer, von denen einige sehr wasserreich sind:
Das heutige Nordseebecken bildete sich im Tertiär heraus. Ihre heutigen Umrisse erhielt die Nordsee jedoch erst im frühen Holozän vor etwa 8.000 Jahren. Auch der jetzige Zustand ist nur ein Stadium in der dynamischen Entwicklung der Nordsee: langfristig lässt sich weiterhin ein Anstieg des Meeresspiegels beobachten, der über die letzten 7500 Jahre gerechnet bei etwa 33 Zentimeter/Jahrhundert liegt (mittleres Tidenhochwasser an den deutschen Küsten). Im 20. Jahrhundert stieg das Wasser um etwa 20 bis 25 Zentimeter.

In der Weichseleiszeit waren wie in den anderen Eiszeiten auch große Wassermengen im Eis der Gletscher gebunden, das Inlandeis Skandinaviens war bis zu drei Kilometern dick. Der Meeresspiegel lag auf dem Höhepunkt der Weichseleiszeit bis zu 120 Meter unter dem heutigen Stand, die Küstenlinien verliefen etwa 600 Kilometer nördlich des heutigen Stands. Große Teile der Nordsee lagen damals trocken. Am Ende der Weichseleiszeit lag der Meeresspiegel etwa 60 Meter unter dem heutigen Normalnull, wobei die Küstenlinie nördlich der heutigen Doggerbank verlief. Die gesamte südliche Nordsee war Festland, das sogenannte Doggerland, die britischen Inseln und das europäische Festland waren eine zusammenhängende Landmasse. In den darauf folgenden Jahrtausenden stieg das Wasser, wobei dieser Anstieg im Laufe der Zeit an Geschwindigkeit abnahm.

Vor etwa 9850 bis 7100 Jahren wurden Teile des Elbe-Urstromtals überflutet. Etwas später öffnete sich der Ärmelkanal und das Wattenmeer begann sich zu bilden. In der darauf folgenden Zeit wechselten Phasen stärkeren Wasseranstiegs (Transgression) mit solchen einer Wassersenkung (Regression). Vor etwa 5000 Jahren (3000 v. Chr.) lag der Meeresspiegel an der südlichen Küste etwa vier Meter unter dem heutigen Niveau, um den Beginn der christlichen Zeitrechnung knapp zwei Meter unter dem heutigen Meeresspiegel. Nachdem er zwischenzeitlich anstieg, sank er um 1000 wieder auf das Niveau zu Beginn der christlichen Zeitrechnung, um schließlich in mehreren Schüben langsamer weiter zu steigen.

Die Nordsee ist ein Schelfmeer mit einer durchschnittlichen Tiefe von nur 94 Metern. Der Meeresboden liegt größtenteils auf dem Schelf, und so steigt die Tiefe von 25 bis 35 Metern im südlichen Teil auf bis zu 100 bis 200 Metern am Kontinentalhang zwischen Norwegen und nördlich der Shetlandinseln. Der gesamte südliche Teil des Meeres ist dabei höchstens 50 Meter tief. Die Ausnahme bildet die Norwegische Rinne; an dieser tiefsten Stelle misst die Nordsee 725 Meter. Die flachste Stelle abseits der Küstengebiete liegt in der Doggerbank. Die südliche Nordsee wird von zahlreichen großen Sandbänken durchzogen.

Die Nordsee wird generell in die flache südliche Nordsee, die Zentralnordsee, die nördliche Nordsee und die Norwegische Rinne mit dem Übergang Skagerrak unterteilt. In der südlichen Nordsee geht der Ärmelkanal in die Straße von Dover über. Die Southern Bight liegt vor der niederländischen und belgischen Küste, die Deutsche Bucht inklusive der Helgoländer Bucht vor der deutschen Küste. Das Flachwassergebiet der Doggerbank begrenzt die deutsche Bucht hin zur Zentralnordsee. Das Wattenmeer zieht sich an der südlichen Küste von Den Helder in den Niederlanden nahezu die gesamte deutsche Nordseeküste entlang bis Esbjerg in Dänemark.

Die Flachwasserzone Doggerbank ist etwa halb so groß wie die Niederlande mit einer Tiefe zwischen nur 13 Metern bis zu höchstens 20 Metern. Sie ist als Ort zum Fischfang berühmt, bei Stürmen brechen hier sogar häufiger die Wellen.

Die Norwegische Rinne ist durchschnittlich 250 bis 300 Meter tief, wird am Übergang zum Skagerrak bis zu 725 Metern tief und spielt eine wichtige Rolle beim Wasseraustausch mit Ostsee und Atlantik. Entlang der Norwegischen Rinne fließt der Norwegische Strom, über den der größte Teil des Nordseewassers in den Atlantik fließt. Ebenso fließt hier ein Großteil des aus der Ostsee stammenden Wassers nach Norden. In der Zentralnordsee, etwa 200 Kilometer östlich der schottischen Stadt Dundee finden sich im "Devil's Hole" weitere Gräben. Die wenige Kilometer langen Gräben gehen in einer Umgebung, die etwa 90 Meter Wassertiefe hat, auf 230 Meter hinunter.

Die Straße von Dover erreicht Meerestiefen von etwa 30 Metern, der Meeresgrund fällt nach Westen hin bis zum Ende des Ärmelkanals bis zu 100 Meter ab. Zwischen den Niederlanden und Großbritannien liegen Tiefen zwischen 20 und 30 Metern, die bis zu 45 Meter an der friesischen Front gehen.

Der Salzgehalt des Meerwassers ist orts- und jahreszeitenabhängig und liegt zwischen 15 und 25 Promille in der Nähe der Flussmündungen und bis zu 32 bis 35 Promille in der nördlichen Nordsee.

Die Temperatur kann im Sommer 25 °C erreichen und 10 °C im Winter. Die Temperatur variiert dabei stark abhängig vom Einfluss des Atlantiks und der Wassertiefe, vor allem wegen der Meeresströmungen. In der tieferen nördlichen Nordsee, in einem Gebiet südlich und östlich der Shetlands, ist die Wassertemperatur durch das einströmende Atlantikwasser das ganze Jahr über fast konstant bei 10 °C, während an der sehr flachen Wattenmeerküste die größten Temperaturunterschiede auftreten und es in sehr kalten Wintern auch zu Eisbildung kommen kann.

Von 1965 bis 2010 stieg die Durchschnittstemperatur der deutschen Nordsee um 1,67 °C, die der Ozeane stieg im Mittel um 0,74 °C.

Das Austausch-Salzwasser der Nordsee fließt durch den Ärmelkanal und entlang der schottischen und englischen Küsten aus dem Atlantik in die Nordsee. Größte Süßwasserzulieferer sind die in die Ostsee mündenden Flüsse, die über das Skagerrak ihren Abfluss in die Nordsee finden. Die Nordseeflüsse entwässern etwa 841.500 Quadratkilometer und bringen pro Jahr ungefähr 296 bis 354 Kubikkilometer Frischwasser ins Meer. Die Ostseeflüsse entwässern mit 1.650.000 Quadratkilometer knapp das doppelte Gebiet und tragen 470 Kubikkilometer Frischwasser jährlich bei.

Entlang der dänischen und norwegischen Küsten fließt das Wasser im Norwegischen Strom in den Atlantik zurück. Dieser bewegt sich vor allem in einer Wassertiefe von 50 bis 100 Metern. Das Brackwasser der Ostsee und aus Nordsee- und Fjorden stammendes Frischwasser sorgen für einen relativ niedrigen Salzgehalt des Stroms. Ein Teil des wärmeren einfließenden Atlantikwassers dreht entlang des Stroms wieder nordwärts und sorgt für einen warmen Kern im Gewässer. Im Winter hat der Strom eine Temperatur von 2 bis 5 °C; die Salinität beträgt weniger als 34,8 Promille. Das durch eine Front getrennte Atlantikwasser der Nordsee ist hingegen über 6 °C warm; der Salzgehalt liegt bei mehr als 35 Promille.

In etwa ein bis zwei Jahren ist das Wasser im Meer komplett ausgetauscht. Innerhalb des Meeres lassen sich anhand von Temperatur, Salzgehalt, Nährstoffen und Verschmutzung klare Wasserfronten erkennen, die im Sommer ausgeprägter sind als im Winter. Große Fronten sind die „friesische Front“, die Wasser aus dem Atlantik von Wasser aus dem Ärmelkanal trennt und die „dänische Front“, die Küstenwasser vom Wasser der Zentralnordsee trennt. Die Einmündungen aus den großen Flüssen gehen nur langsam in Nordseewasser über. Wasser aus Rhein und Elbe beispielsweise lässt sich noch bis zur nordwestlichen Küste Dänemarks klar vom Seewasser unterscheiden.

Die Auswirkung von Stoffeinträgen aus Flüssen und der Atmosphäre auf die Wasserzirkulation lassen sich als komplexe Szenarien nur mit Hilfe von modernen numerischen Verfahren berechnen.

Die Gezeiten werden durch die Gezeitenwellen aus dem Nordatlantik ausgelöst, da die Nordsee selbst zu klein und zu flach ist, um eine nennenswerte Tide auszubilden. Ebbe und Flut wechseln sich in einem Rhythmus von etwa 12 h 25 min ab, genauer: Der Zeitabstand bis zur übernächsten Tide beträgt in der Regel 24 h 50 min. Die Gezeitenwelle läuft bedingt durch die Corioliskraft an der Ostküste Schottlands und Englands in südlicher Richtung und erreicht 10 bis 11 Stunden nach Eintreffen in Schottland die Deutsche Bucht. Sie umläuft dabei zwei oder drei amphidromische Punkte. Eine Amphidromie liegt kurz vor der Straße von Dover. Sie bildet sich durch die Gezeitenwelle, die über den Ärmelkanal einläuft, und beeinflusst die Gezeiten in dem schmalen Gebiet De Hoofden in der Southern Bight zwischen Südengland und Belgien und den Niederlanden. Rechnet man diesen Punkt mit, so braucht die Gezeitenwelle von Nordschottland bis Borkum zwölf Stunden länger. Die beiden anderen amphidromischen Punkte liegen kurz vor der Küste Südnorwegens und auf einer Schnittlinie zwischen Süddänemark und Südschottland über der Jütlandbank auf 55° 25' N, 5° 15' O. Sie bilden ein einziges Feld, um das die Gezeiten herumlaufen.
Der Tidenhub liegt an der Küste Südnorwegens bei unter einem halben Meter, erhöht sich aber, je weiter eine Küste von der Amphidromie entfernt liegt. Flache Küsten und trichterartige Verengungen erhöhen den Tidenhub. Am größten ist er in der Wash an der englischen Küste, wo ein Tidenhub von 6,8 Metern erreicht wird. Durch Interferenzen mit den Tidenwellen aus dem Ärmelkanal gibt es an der niederländischen Küste bei Rotterdam gespaltene Niedrigwasser und bei Den Helder periodisch zwei- bis dreigipflige Hochwasser. An der deutschen Nordseeküste beträgt der Tidenhub je nach Küstenform und -lage zwischen zwei und viereinhalb Metern. Vor der jütländischen Küste lässt der Tidenhub nach und in Skagerrak und Kattegatt laufen die Gezeitenwellen aus.

In Flachwasserbereichen, also nicht zuletzt in der Deutschen Bucht, wird der tatsächliche Tidenhub jedoch stark von weiteren Faktoren wie der Küstenlage und dem herrschenden Wind oder Sturm beeinflusst (Sturmflut). In den Mündungsgebieten der Flüsse kann ein hoher Wasserstand der Flüsse den Fluteffekt maßgeblich verstärken.
Starke Gezeiten, große algen- und Seetangreiche Flachwasserbereiche, der Strukturreichtum und der große Nährstoffvorrat in der See sorgen für ein vielfältiges Leben in der Nordsee.

Die Nordsee bietet eine Reihe sehr verschiedener Lebensraumtypen, die von unterschiedlichen Biozönosen bewohnt sind. So unterscheidet man grundsätzlich in die Lebensräume der Küstengebiete, die verschiedene Küstentypen wie die Steilküsten, Felsküsten und Sandküsten beinhalten, von den tatsächlichen aquatischen Lebensräumen. Wichtige Übergangsgebiete stellen im Fall der Nordsee zudem die Salzwiesen und die Wattflächen dar, die sich durch einen Wechsel der Lebensbedingungen abhängig von Ebbe und Flut auszeichnen. In der Nordsee liegt das größte und artenreichste Wattenmeer der Welt. Auch die Bereiche der großen Flussmündungen, die Ästuare, die sich durch eine Durchmischung des in die Nordsee fließenden Süßwassers und des salzigen Nordseewassers auszeichnen, stellen einen eigenen Lebensraumtyp dar.

Die aquatischen Lebensräume lassen sich zudem in das Freiwasser, das sogenannte Pelagial, sowie den Gewässerboden, das Benthal, unterscheiden. Die benthischen Lebensräume wiederum unterscheiden sich durch ihre Tiefe sowie durch ihre Bodenbeschaffenheit. So können sie felsig, kiesig oder sandig sein, außerdem können sie mehr oder weniger bis gar keine Schlickschichten tragen.
Die Nordsee leidet durch direkte Einleitungen von Schadstoffen, durch die Schadstoffbelastungen, die die Flüsse mit sich führen, und vor allem in den Küstenregionen unter den Belastungen, die die menschliche Nutzung mit sich bringt. Der Küstenschutz hat an der gesamten südlichen Nordseeküste einen stark landschaftsverändernden Einfluss. Tourismus und Freizeitgestaltung spielen hier eine ambivalente Rolle – zum einen belasten sie die Küstengebiete stark, zum anderen aber geben sie einen direkten ökonomischen Anreiz, die Landschaft weitgehend unversehrt und „schön“ zu erhalten. Wegen Überfischung schrumpfte in den 1970er Jahren vor allem die Population des Nordseeherings. Die Kabeljau-Bestände sind trotz einer gemeinsamen EG-Regulierung aus dem Jahre 1983 in den letzten Jahren extrem zurückgegangen.

Zum Schutz der Nordsee trafen die Anliegerstaaten verschiedene Abkommen. Das Bonner Abkommen von 1969 war das erste internationale Abkommen zum Umweltschutz in der Nordsee und betraf ausschließlich die möglichen negativen Folgen der Ölförderung.

Die Abkommen von Oslo (1972) und Paris (1974) beschäftigten sich erstmals in größerem Maßstab mit Schadstoffen im Meer; in ihrer Folge verabschiedeten die Anliegerstaaten 1992 die Oslo-Paris-Konvention. Für den Umweltschutz an den Küsten sind die Anliegerländer zuständig, die zu diesem Zweck verschiedene nationale Regelungen getroffen haben. In Deutschland bilden die Nationalparks Wattenmeer in Schleswig-Holstein, Niedersachsen und Hamburg die größten deutschen Nationalparks.

Die Nordseeküste befindet sich derzeit in keinem Endzustand, sondern ist in stetiger Bewegung. In jüngerer Zeit trug neben singulären Naturereignissen, wie Sturmfluten, auch die in den letzten 500 Jahren gezielt betriebene Landgewinnung durch den Menschen zur Änderung der Küstenlinie bei.

Die Nordsee wird durch ihre größte Insel, Großbritannien, nach Westen begrenzt, allerdings liegt nur die Ostküste an der Nordsee. Zu den größten Inselgruppen, die komplett in der Nordsee liegen, gehören die Shetlandinseln und Orkney.

Die nördlichen Nordseeküsten sind glazial geprägt durch die großen Gletscher, die auf ihnen zu den verschiedenen Eiszeiten lagen. Dadurch entstand eine stark gegliederte und zerklüftete Küstenlandschaft. Die Fjorde entstanden durch Gletscher, welche aus dem Hochgebirge durch sie hindurchzogen und in den Untergrund tiefe Rinnen schnitten und schabten. Während des folgenden Anstiegs des Meeresspiegels füllten sich diese mit Wasser. Sie weisen oft steile Küstenlinien auf und sind für Nordseeverhältnisse sehr tief. Fjorde kommen insbesondere an der Küste Norwegens vor.

Fjärde sind ähnlich wie die Fjorde aufgebaut, jedoch meist flacher mit breiteren Buchten, in denen sich auch oft kleinere Inseln befinden. Die Gletscher, die zu ihrer Entstehung führten, konnten den Untergrund auf einem größeren Gebiet beeinflussen und räumten so weitere Strecken des Landes ab. Fjärde finden sich vor allem an der schottischen und nordenglischen Nordseeküste. Einzelne Inseln in den Fjärden oder Inseln und Küste sind heute oft durch Nehrungen oder Halbinseln aus Sandablagerungen verbunden. Lokal heißen diese Tombolos.

Die Fjärde gehen nach Süden in eine Kliffküste über, die vor allem aus Moränen der Eiszeitgletscher entstanden sind. Durch den horizontalen Aufprall der Nordseeküste entstehen hier Abbruchküsten; das Material, das dabei abbricht, ist wichtiger Sedimentlieferant für das Watt auf der anderen Seite der Nordsee. Große Ästuare (Trichtermündungen) mit den dazugehörigen Watt- und Marschgebieten unterbrechen diese Kliffküste. Große Mündungen im Süden Englands gehören zu den Flüssen Themse und Humber.

Sowohl in Südnorwegen als auch an der schwedischen Küste des Skagerraks finden sich Schären. Entstanden ähnlich wie Fjorde und Fjärde hatten hier die Gletscher noch größeren Einfluss auf die Landschaft, so dass diese weiträumig abgetragen wurde. Strandflaten, die sich vor allem in Südnorwegen finden, sind Gesteinsplatten, die oft mehrere Kilometer Ausdehnung haben, fast vollkommen abgeschliffen wurden und heute oft wenige Meter unter der Meeresoberfläche liegen.

Die Flachküste der südlichen und östlichen Küste bis hinauf nach Dänemark ist in ihren Grundzügen zwar ebenfalls eiszeitlich geformt, ihre Form wird jedoch vor allem durch das Meer und Sedimentablagerungen bestimmt. Der gesamte Küstenverlauf ist flach, die Tiden überschwemmen oft große Landstriche und geben diese danach wieder frei. Das Wasser lagert Sedimente ab. Im mikrotidalen Bereich (bis 1,35 Meter Tidenhub), wie etwa an der niederländischen oder der dänischen Küste, bilden sich Strandwälle mit Dünen. Im mesotidalen Bereich (1,35 bis 2,90 Meter Tidenhub) bilden sich Barriereinseln, im makrotidalen Bereich (über 2,90 Meter), wie etwa in der Elbmündung, bilden sich unterseeische Sandbänke.

Die niederländischen West- und die deutschen Ostfriesischen Inseln sind Barriereinseln. Sie entstanden an den Brandungskanten des Meeres, an denen durch die Brandung Sedimente aufgeschüttet und hinter denen durch die brechenden Wellen Sedimente abgetragen wurden. Im Laufe der Zeit sammelten sich so Sandplaten an, die schließlich nur noch von Sturmfluten überschwemmt wurden. Die ersten Pflanzen begannen auf den Sandbänken zu siedeln, das Land verfestigte sich.
Obwohl sie heute befestigte Inseln sind, befinden sich einige von ihnen auch weiterhin in Bewegung. Für die ostfriesische Insel Juist beispielsweise sind seit 1650 fünf verschiedene Kirchplätze nachweisbar, da der Ort des Kirchenbaus mit der sich verlagernden Insel Schritt halten musste. Zeitweise bestand Juist auch aus zwei Inseln, bevor es wieder zusammenwuchs. Die benachbarte Insel Wangerooge verschob sich in den letzten dreihundert Jahren einmal um ihre komplette Länge nach Osten. Aufgrund der herrschenden Umweltbedingungen wird auf den Ostfriesischen Inseln an den Westküsten Land abgetragen, während sich an den Ostküsten Sedimente ablagern. Die Westküsten werden deshalb heutzutage verstärkt von den Menschen geschützt. Der Wattstrom (auch Balje, Gatt oder Tief) zwischen den Inseln dient zum Durchfluss der Gezeiten, so dass dort die Strömung ein Zusammenwachsen der Inseln verhindert.
Die Nordfriesischen Inseln sind hingegen aus den Resten alter Geestkerninseln entstanden, die durch Sturmfluten und Wassereinwirkungen teilweise abgetragen und vom Hinterland getrennt wurden. Sie sind deshalb oft höher und in ihrem Kern weniger stark Veränderungen ausgesetzt als die südlich liegenden Inseln. Außerhalb des Kerns finden sich aber dieselben Prozesse wie an West- und Ostfriesischen Inseln, besonders ausgeprägt auf Sylt, wo ein Durchbruch der Insel im südlichen Bereich droht, während der Lister Hafen im Norden versandet.

Die Halligen sind Reste des in mittelalterlichen Sturmfluten untergegangenen Marschlandes. Ihre Gestalt war in der Vergangenheit großen Veränderungen ausgesetzt. Von einmal über hundert Halligen existieren heute nur noch zehn, die übrigen sind entweder abgetragen oder ans Festland angedeicht worden.

Die sich nördlich anschließenden Dänischen Wattenmeerinseln sind aus Sandbänken entstanden. Noch bis in das 20. Jahrhundert war die Versandung der Inseln ein großes Problem. Zum Schutz der Inseln wurden kleinere Wälder angelegt.

An der südöstlichen Küste finden sich ebenfalls viele ausgedehnte Ästuare wie die von Maas, Rhein, Weser, Elbe oder Eider.

Besonders die Southern Bight veränderte sich durch Landgewinnung, denn die Niederländer waren dabei besonders aktiv; das größte Projekt dieser Art war die Abdeichung und die Landgewinnung am IJsselmeer.
Zwischen Esbjerg (Dänemark) im Norden und Den Helder (Niederlande) im Westen erstreckt sich das Wattenmeer. Dies ist eine von Ebbe und Flut geprägte Landschaft, von der wichtige Teile mittlerweile zum Nationalpark erklärt wurden. Die Insel Helgoland bildet einen Ausnahmefall, da sie nicht durch das auflaufende Watt entstand, sondern erheblich älter ist und aus Buntsandstein besteht. Die Festlandsküste im Bereich des Wattenmeers ist bis auf kurze Abschnitte, etwa bei Schobüll und Cuxhaven-Duhnen, durch Deiche gesichert.

Sturmfluten gefährden besonders die Küsten der Niederlande, Belgiens, Deutschlands und Dänemarks. Diese sind relativ flach, so dass bereits eine relativ geringe Erhöhung des Wasserstandes ausreicht, um weite Landstriche unter Wasser zu setzen. Zudem sind Stürme aus westlichen Richtungen an der Nordsee besonders heftig, so dass die gefährdetsten Stellen die südöstlichen Küsten sind. Winde aus Nordwest treffen dabei vor allem die Niederlande und die niedersächsische Küste, Winde aus West- bis Südwest die schleswig-holsteinische Küste. Im Laufe der Geschichte kosteten Sturmfluten hunderttausenden Menschen das Leben, diese Fluten formten maßgeblich die heutige Küstengestalt mit. Bis in die frühe Neuzeit hinein lagen die Opferzahlen oft bei mehreren zehntausend oder gar hunderttausend Opfern pro Flut. Inwieweit diese Zahlen zuverlässig sind, kann aber nach heutigem Wissen nur schwer eingeschätzt werden.

Die erste aufgezeichnete Flut war die Julianenflut in den Niederlanden, deren Datumsangabe (17. Februar 1164) allerdings heute bezweifelt wird. Die Erste Marcellusflut 1219 traf vor allem Westfriesland und Ostfriesland, das damals noch bis zur Weser reichte; mit ihr begann der Jadebusen zu entstehen. Bei der Sturmflut von 1228 überliefern die Chroniken 100.000 Tote. Die Zweite Marcellusflut oder "Grote Mandränke" von 1362 traf Süd- und Ostküste der Deutschen Bucht, wieder überliefern die Chroniken 100.000 Tote, die vordere Küstenlinie Nordfrieslands wurde weitgehend zerstört und große Landflächen dauerhaft an die See verloren. Dabei versank auch die heute sagenumwobene Stadt Rungholt. Die Insel Strand entstand. Bei der Burchardiflut ("Zweite Grote Mandränke") 1634 wurde unter anderem die Insel Strand zerstört. Übrig blieben die Halligen. Bei der Neujahrsflut 1721 wurde die Düne von Helgoland getrennt.

Im 20. Jahrhundert trafen schwerwiegende Sturmfluten die Niederlande mit der Hollandsturmflut, die am 1. Februar 1953 für über 2.000 Tote sorgte und die Hamburger Sturmflut am 16./17. Februar 1962, bei der 315 Hamburger starben. Die „Jahrhundertflut“ von 1976 und die „Nordfrieslandflut“ von 1981 brachten die höchsten bisher gemessenen Wasserstände an der Nordseeküste. Da nach der Hamburger Flut jedoch der Deichbau und der Küstenschutz erheblich verbessert worden war, kam es hier nur zu Sachschäden.

Vom 26. bis zum 28. Februar 1990 wurden innerhalb von drei Tagen fünf Fluten vom Sturm auf maximale Höhen getrieben. In Büsum wurden Windgeschwindigkeiten bis 160 km/h gemessen. Aufgrund des verbesserten Küstenschutzes kam es jedoch nur zu einigen Sachschäden.
Der Übergangsbereich zwischen Land und Meer an den Gegenden mit flacher Küste war ursprünglich stark amphibisch geprägt. Das Land bestand aus zahlreichen Inseln und Halligen, die durch Flüsse, Bäche und Moore getrennt waren. Das „Festland“ wurde regelmäßig überflutet. In den besonders durch Sturmfluten bedrohten Gegenden siedelten die Menschen zuerst auf natürlichen Erhebungen wie Geestzungen, Dünen oder Uferwällen. Letztere boten aber nur in Phasen sinkender Meeresspiegel hinreichend Schutz. So wurden schon im 1. bis 4. Jahrhundert Siedlungen auf Warften errichtet – künstlichen Hügeln von teilweise mehreren Metern Höhe. Die zweite Warftenperiode begann im 7. Jahrhundert und hielt bis ins 20. Jahrhundert an.

Die ersten Deiche waren kleine Ringdeiche um einzelne Felder, die im Sommerhalbjahr ausreichten, die Feldfrüchte, vor Allem Hafer und Pferdebohnen, bis zur Ernte zu schützen, aber von den schweren Sturmfluten des Winterhalbjahrs überflutet wurden. Ab dem Beginn des Hochmittelalters begannen die Menschen, die vereinzelten Ringdeiche zu einer Deichlinie direkt an der Küste zusammenzufassen und so langfristig den amphibischen Bereich zwischen Land und Meer in Festland zu verwandeln.

Zwar war man schon im 13. Jahrhundert stolz auf den „Goldenen Ring“, einen Deich in gleicher Höhe um ganz Friesland, aber zunächst war die Koordination noch schlecht und die Mittel der einzelnen Landgemeinden unzureichend. Zudem lag bei örtlicher Selbsthilfe die Last der Reparatur von Deichbrüchen bei denjenigen, die am stärksten von einem Meereseinbruch geschädigt worden waren. Erst staatliche Koordination und wirtschaftliche Potenz wie die der Grafschaft Oldenburg konnte die Dienste der Marschbauern und kommerzieller Unternehmer zu effektiven Deichbauten zusammenfassen. Vorbild beim Deichbau waren jahrhundertelang die Niederlande, noch heute ist dort der Rijkswaterstaat die mächtigste Behörde im Lande. Von ihnen wurden mit technischen Errungenschaften auch Irrwege übernommen. Da Erdarbeiten ohne maschinelle Hilfe sehr aufwändig sind und mancherorts auf weichem Untergrund nicht einmal Fuhrwerke (Stürzkarren) eingesetzt werden konnten, stützte man die Flanken der Deiche mit Holzkonstruktionen, um größere Deichhöhen zu erreichen. Die so gebauten Stackdeiche erwiesen sich bei Sturmfluten als anfällig gegen überschlagende Wellen. Zudem wurde das verbaute Holz zunehmend von der Schiffsbohrmuschel zerfressen, die durch den internationalen Seeverkehr aus tropischen Gewässern eingeschleppt worden war. Mit der Verfügbarkeit von Baumaschinen konnten ab dem späten 19. Jahrhundert immer größere Erdmassen zu breiteren und höheren Deichen aufgehäuft werden. Auf besonders weichem Untergrund werden aber Deiche durch Spundwände verstärkt, weil zusätzlich aufgeschüttete Erde im Untergrund versinkt.

Eine der ersten großen Maßnahmen, dem Meer durch Verkürzung der Deichlinie weniger zu bieten, war der 1593 (Vorarbeiten) bis 1615 angelegte Ellenser Damm. Als größtes Einzelbauwerk entstand 1927 bis 1932 der Abschlussdeich, der die Zuiderzee zum IJsselmeer machte. Nach der niederländischen Watersnood 1953 und der Sturmflut 1962 an der deutschen Nordseeküste und in Hamburg wurden nicht nur die Deiche noch einmal erhöht. Seither wurden vor allem im Rhein-Maas-Schelde-Delta aber auch an der deutschen Nordseeküste zahlreiche Flussmündungen und Meeresarme durch Sperrwerke gesichert. Um die Küste als natürlichen Lebensraum nicht zu sehr zu beeinträchtigen, sind diese Sperrwerke zunehmend so eingerichtet und gesteuert, dass sie normale Gezeitenströme ganz oder teilweise zulassen und nur bei Sturmflut geschlossen werden, vgl. die Renaturierung der Luneplate.
Heutiger Küstenschutz an der flachen Nordseeküste besteht aus mehreren Ebenen. Das Deichvorland nimmt dem Meer bereits einiges an Kraft, mit dem es auf den Deich treffen kann. Liegt der Deich direkt am Meer, ist ein besonders gesicherter Schardeich notwendig. Der Seedeich wurde im Laufe der Zeit immer höher (bis zu 10 Meter) und bekam ein flacheres Profil, um ebenfalls die Angriffskraft der Wellen zu schwächen. Moderne Deiche sind bis zu 100 Meter breit. Dahinter folgt ein Deichverteidigungsweg und meist weiteres dünn besiedeltes Land. Ältere Deichlinien im Hinterland werden mancherorts als zusätzlicher Schutz erhalten, vielerorts aber abgetragen, in Marschen und Poldern ist selbst Erde kostbar.

Auch Dünen tragen zum Küstenschutz bei. Mancherorts, besonders an der holländischen Küste zwischen Hoek van Holland und Den Helder, bilden sie den alleinigen Schutz. Andernorts, etwa in Zeeland und auf einigen Nordfriesischen Inseln, wurden sie durch Deiche verstärkt. Sie werden heute mit Strandhafer bepflanzt, um sowohl Erosion durch Wind und Wasser als auch das Wandern der Dünen selbst zu vermindern. Besonders aufwändige Maßnahmen des Küstenschutzes sind die Deltawerke in den Niederlanden oder Sandvorspülungen vor der deutschen Insel Sylt.

Die südliche Nordseeküste ist sehr dicht besiedelt und wird dementsprechend stark genutzt. In einem 150-Kilometer-Bereich an der Küste leben 80 Millionen Menschen, davon fast die gesamte Bevölkerung der Niederlande und Belgiens, fast alle davon in urbanen Gegenden. In diesen Bereichen haben die Küstenregionen eine Bevölkerungsdichte von über 1.000 Einwohner pro Quadratkilometer. Der Küstenabschnitt zwischen Hamburg und Brüssel ist stark industrialisiert. Hier findet sich eine der größten Ansammlungen von Schwerindustrie weltweit.

Kanalverbindungen:

Obwohl die faktische Kontrolle der Nordsee seit der Zeit der Wikinger entscheidend für die Machtverhältnisse in Nordwesteuropa war und sich seit dem Ersten Englisch-Niederländischen Seekrieg zur Frage der Weltpolitik entwickelte, gehörte die Nordsee bis nach dem Zweiten Weltkrieg juristisch niemandem, die angrenzenden Staaten nahmen nur schmale Küstengewässer für sich in Anspruch. In den letzten Jahrzehnten hat sich dies allerdings gewandelt.

Die an die Nordsee angrenzenden Länder beanspruchen die Zwölfmeilenzone. Die seewärtige Grenze dieser Zone bildet die Grenze des deutschen Hoheitsgebietes. Die Fläche der Nordsee innerhalb des Hoheitsgebietes ist als Seewasserstraße eine Bundeswasserstraße.

In der Zwölfmeilenzone nehmen die Länder beispielsweise das exklusive Recht zur Fischerei wahr. Island konnte in den sogenannten Kabeljaukriegen international eine 200-Meilen-Zone der Fischfangrechte durchsetzen, der sich die EU-Staaten anschlossen und so faktisch die Nordsee gegenüber anderen Ländern verschlossen. Der Fischfang ist auf EU-Staaten und den Anrainerstaat Norwegen begrenzt; andere Länder müssen spezielle Abkommen schließen. Die Koordination beruht auf der gemeinsamen Fischereipolitik der EU und Verträgen zwischen der EU und Norwegen.

Nachdem unter der Nordsee Bodenschätze gefunden worden waren, nahm Norwegen die Rechte des Übereinkommens über den Festlandsockel für sich in Anspruch, woraufhin die anderen Staaten ebenso verfuhren. Der Nordseeboden ist weitgehend entsprechend dem Mittellinienprinzip aufgeteilt, nach dem die Grenze zwischen zwei Küstenstaaten auf einer gedachten Mittellinie liegt. Nur zwischen den Niederlanden, Deutschland und Dänemark wurde der Boden nach langwierigen Auseinandersetzungen und einem Spruch des Internationalen Gerichtshofs anders verteilt. Da Deutschland aufgrund seiner geografischen Positionen sonst nur einen sehr kleinen Teil Boden im Verhältnis zur Küstenlinie bekommen hätte, gehört nun noch ein weiteres Feld, der sogenannte Entenschnabel, zur deutschen ökonomischen Zone.

In Bezug auf Umweltschutz und Meeresverschmutzung hat die 25- bzw. 50-Meilen-Zone des MARPOL "(marine pollution)"-Abkommens Geltung. Die Oslo-Pariser-Abkommen beschäftigen sich ebenfalls mit Fragen des Meeresschutzes in der gesamten Nordsee. Im Wattenmeer sind jeweils die nationalen Staaten zuständig, die dieses Problem national unterschiedlich lösen; um eine gemeinsame Politik in Bezug auf das Wattenmeer zu gewährleisten, tagt die Trilaterale Wattenmeerkommission.

Für Schiffssicherheit und eine Koordinierung des Seeverkehrs soll die Europäische Agentur für die Sicherheit des Seeverkehrs sorgen, die Anfang 2003 ihre Arbeit aufnahm. Die Kommission gehört zur EU, Norwegen und Island haben als direkt betroffenen Staaten ebenfalls einen Sitz in ihr. Nach dem 1978 verabschiedeten Paris Memorandum of Understanding haben sich unter anderem alle EU-Staaten verpflichtet regelmäßig 25 Prozent der Schiffe, die einen EU-Hafen anlaufen auf die Einhaltung internationaler Sicherheitsbestimmungen zu überprüfen. Das Wattenmeer und die Küsten Großbritanniens, Belgiens und Frankreichs wurden als Particularly Sensitive Sea Area ausgezeichnet. In der Nordsee gelten ebenso wie in der Ostsee die strengsten Bestimmungen der MARPOL-Konventionen zur Abwasser- und Müllentsorgung vom Schiff aus.

1958 entdeckten Geologen bei Slochteren in der niederländischen Provinz Groningen ein Erdgasfeld. Es stand zu vermuten, dass sich weitere Felder unter der Nordsee befinden würden, jedoch waren zu diesem Zeitpunkt die Besitzrechte an der Nordsee im Hochseebereich unklar. 1966 begannen Probebohrungen, 1969 entdeckte die Phillips Petroleum Company im norwegischen Sektor das "Ekofisk"-Feld – damals eines der 20 größten Erdölfelder der Welt, das sich zudem durch sehr hochwertiges schwefelarmes Öl auszeichnete. Die erste kommerzielle Ausbeutung erfolgte ab 1971, das Ekofisk-Öl wurde erst mit Tankern, ab 1975 mit einer Pipeline ins englische Cleveland und seit 1977 auch mit einer weiteren Pipeline ins deutsche Emden geleitet. In größerem Maßstab beuten die Ölkonzerne die Vorräte der Nordsee jedoch erst seit der Ölkrise aus, als der international steigende Ölpreis dies wirtschaftlich attraktiv machte und die notwendigen hohen Investitionen ermöglichte. In den 1980ern und 1990ern folgten weitere große Entdeckungen von Ölfeldern. Obwohl die Produktionskosten vergleichsweise hoch sind, haben die hohe Qualität des zu fördernden Öls, die politische Stabilität der Region und die Nähe zu den Absatzmärkten Westeuropas die Nordsee zu einer wichtigen Ölregion werden lassen.

Mittlerweile gibt es im Meer 450 Bohrinseln, die Nordsee ist das wichtigste Gebiet der Offshore-Förderindustrie. Die meisten Plattformen befinden sich im britischen Sektor der Nordsee, gefolgt vom norwegischen, dem niederländischen und dem dänischen Sektor. Der britische und der norwegische Sektor enthalten dabei mit Abstand die größten Ölreserven. Schätzungen gehen davon aus, dass sich allein im norwegischen Sektor 54 Prozent der Öl- und 45 Prozent der Gasreserven befinden. Bedeutende Ölfelder sind neben dem Ekofisk-Feld auch das norwegische "Statfjord"-Feld, zu dessen Erschließung erstmals die Norwegische Rinne mit einer Pipeline überwunden wurde. Das norwegische Staatsunternehmen Statoil erhält, einem norwegischen Gesetz entsprechend, mindestens 50 Prozent der Anteile an Ölfeldern, die im norwegischen Sektor liegen. Das größte Erdgasfeld der Nordsee ist das "Troll"-Feld. Es liegt in der Norwegischen Rinne in einer Tiefe von 345 Metern, so dass große Anstrengungen unternommen werden mussten, um es überhaupt zu erschließen. Die Bohrplattform ist mit 472 Metern Höhe und 656.000 Tonnen Gewicht die größte Offshore-Bohrplattform und das größte jemals von Menschen transportierte Objekt.

Im deutschen Sektor befinden sich nur zwei Plattformen, es handelt sich bei ihm um den am wenigsten erschlossenen Sektor in dieser Hinsicht. Das größere der beiden Felder ist das Ölfeld Mittelplate.

Ihren Hochstand erreichte die Förderung 1999, als fast 6 Millionen Barrel (950.000 Kubikmeter) Erdöl und 280.000.000 Kubikmeter Erdgas täglich gefördert wurden. Mittlerweile gilt die Nordsee als erschlossenes Rohstoffgebiet, in dem kaum noch größere Entdeckungen zu erwarten sind. Alle großen Ölkonzerne sind an der Förderung beteiligt, in den letzten Jahren haben aber große Ölkonzerne wie Shell oder BP die Ölförderung in dem Gebiet bereits eingestellt und die Fördermenge geht seit 1999 aufgrund fehlender Reserven kontinuierlich zurück. Der Preis von Brent Crude, einer der ersten in der Nordsee geförderten Ölsorten, wird heute als Standard- und Vergleichspreis für Erdöl aus Europa, Afrika und dem Nahen Osten genutzt.
Neben Öl und Gas entnehmen die Anrainerstaaten dem Meeresboden jedes Jahr mehrere Millionen Kubikmeter Sand und Kies. Diese werden vor allem für Bauvorhaben, zur Sandaufschüttung an Stränden und zum Küstenschutz verwendet. Größte Entnehmer 2003 waren die Niederlande (etwa 30 Millionen Kubikmeter) und Dänemark (etwa 10 Millionen Kubikmeter im Nordseeraum). 2005 entnahm Deutschland der Nordsee etwa 740.000 Kubikmeter.

Auf Grund der geologischen Entstehung befinden sich unter der Nordsee auch umfangreiche Kohleflöze. Im jüngsten Report der British Geological Survey (BGS) werden die Vorräte auf drei Billionen Tonnen bis 23 Billionen Tonne Kohle geschätzt. Um diese unterseeischen Mengen zu nutzen, plant das Unternehmen „Five-Quarter“ durch „Deep Gas Winning“ eine umweltfreundliche Variante zu finden. Dazu würden in sehr dünnen Bohrungen Sauerstoff und ultraerhitzter Wasserdampf injiziert und es wird Synthesegas, Wasserstoff und Kohlenmonoxid, sowie Methan und Kohlendioxid in den Lagerstätten unter Wasser freigesetzt. Diese Art des Abbaus erfordert keinen Zusatz weiterer Chemikalien wie beim Fracking an Land.

Die Nordsee-Anrainerstaaten, allen voran Großbritannien und Dänemark, nutzen seit dem Ende der 1990er Jahre die küstennahen Bereiche der Nordsee zur windbetriebenen Stromproduktion. Erste Windkraftanlagen entstanden vor der englischen Küste (Blyth im Jahre 2000) sowie der dänischen Küste (Windpark Horns Rev im Jahre 2002).

Seit 2001 bestehen Planungen, auch in der deutschen Wirtschaftszone der Nordsee Offshore-Windparks zu errichten, welche die gegenüber Windparks an Land erheblich stärkeren und gleichmäßigeren Winde auf See nutzen können. Bisher wurden 697 Windkraftanlagen an 10 Standorten genehmigt (Stand Dezember 2005). Gegen diese Windparks werden jedoch auch Bedenken vorgetragen: Befürchtet werden beispielsweise Schiffskollisionen und eine Beeinträchtigung der Meeresökologie, vornehmlich während des Fundamentbaus. Hinzu kommt, dass die Entfernung zu den Abnehmern zu einem Transportverlust von Energie führt und der Neubau von Leitungen im Wattenmeer erforderlich sein könnte, das jedoch fast komplett als Biosphärenreservat und Nationalpark ausgewiesen ist.

Energiegewinnung aus dem Meer befindet sich noch in den Anfangsstadien. Während die südliche Nordsee nach Meinung der meisten Experten zu geringen Tidenhub, Wellen und Strömungen für derartige Versuche aufweist, könnten sich an den Küsten Norwegens und am Übergang zwischen Nordsee und Irischer See geeignete Stellen für Wellen- und Strömungskraftwerke finden. Erste Versuche mit dem Wellenkraftwerk "Wave Dragon" wurden von 2003 bis zum Januar 2005 an der dänischen Küste abgeschlossen. Eine Mini-Pilotanlage für ein Osmosekraftwerk existiert in der Nähe der norwegischen Stadt Trondheim.

Seit etwas über hundert Jahren wird an der südlichen Nordseeküste Fischfang in kommerziellem Ausmaß praktiziert. Fischfang in der Nordsee konzentriert sich auch heute noch auf den südlichen Teil und die Küstengewässer, wobei vor allem mit Grundschleppnetzen gearbeitet wird.

Durch stetige technische Weiterentwicklung dehnten sich die Fangmengen bis in die 1980er Jahre beständig aus, bis sie mit etwa 3 Millionen Tonnen pro Jahr einen Höchststand erreichten. Seitdem ging die Fangmenge zurück, heute werden etwa 2,3 Millionen Tonnen pro Jahr gewonnen, aber mit teilweise erheblichen Unterschieden in einzelnen Jahren. Neben dem angelandeten Fisch gehen Schätzungen zufolge jährlich in der Nordsee ca. 150.000 Tonnen nicht marktfähiger Beifangfisch und rund 85.000 Tonnen tote oder geschädigte Wirbellose als Beifang wieder über Bord.

Vom angelandeten Fisch wird etwa die Hälfte zu Fischmehl und Fischöl verarbeitet. Zu den wichtigen gefangenen Fischen gehören Makrele, Kabeljau, Schellfisch, Wittling, Seelachs, Scholle und Zungen. Ebenfalls werden Nordseegarnele, Hummer und Krabben (Kurzschwanzkrebse) gefangen. Verschiedene Muschelarten wie Miesmuscheln, Kammmuscheln oder Austern werden in Kulturen gezüchtet, so dass man bei der Ernte nicht von Fischerei im eigentlichen Sinne sprechen kann.

Der Fischfang in einer so dicht besiedelten Umgebung auf technischem Hochstand bringt die Gefahr der Überfischung mit sich.

Obwohl die Fangquoten seit 1983 von der EG/EU reguliert werden, leiden vor allem Schellfisch und Kabeljau durch den Fang. Alleine die Schleppnetzfischerei Dänemarks kostet jährlich 5.000 Schweinswalen das Leben. Seit den 1960er Jahren wurde versucht, die Fischbestände durch verschiedene Regelungen wie bestimmte Fangzeiten, eine begrenzte Zahl von Fischereischiffen usw. zu schonen, diese Regeln wurden aber nicht systematisch angewandt, so dass sie kaum Entlastung brachten. Seitdem mit dem Vereinigten Königreich und Dänemark zwei wichtige Fischereinationen Mitglied der Europäischen Gemeinschaft wurden, versuchen diese mit Hilfe der Gemeinsamen Fischereipolitik das Problem in den Griff zu bekommen, Norwegen hat in der Hinsicht verschiedene Abkommen mit der EG getroffen.

Im Einzugsbereich der Flüsse, die in die Nordsee münden, leben auf ungefähr 850.000 Quadratkilometern etwa 160 Millionen Menschen. Die Ströme entwässern einen Großteil Westeuropas, darunter ein Viertel Frankreichs, drei Viertel Deutschlands, fast die gesamte Schweiz und Großbritannien, die Hälfte Jütlands, die gesamten Niederlande und Belgien, den Süden Norwegens sowie kleine Teile von Österreich. In diesem Bereich findet sich die größte Ansammlung weltweiter Industrie, allein 15 Prozent der Weltindustrieproduktion finden im Einzugsbereich der Nordsee statt.

Europas größte Häfen befinden sich an der Nordsee. Dabei konzentriert sich die Schifffahrt vor allem auf sechs große Häfen. Die kleineren Regionalhäfen haben an Bedeutung verloren; der Containerbetrieb in den vier größten Häfen (Rotterdam, Antwerpen, Hamburg und Bremen/Bremerhaven) hat sich von 1991 bis 2000 um etwa zwei Drittel erhöht. Mit Abstand größter und wichtigster Hafen ist Rotterdam. Nach eigener Auskunft ist das "Hinterland des Hafens ganz Europa". Es gibt wöchentliche Feeder-Verbindungen in 140 andere Städte. Skandinavien und der Ostseeraum werden hauptsächlich über Bremerhaven und Hamburg bedient. Ein Sammelbegriff für (wichtige) Nordseehäfen ist Nordrange.

In der Nordsee fanden in den frühen 1990ern 27,5 Prozent der weltweiten Schiffsbewegungen statt, mit steigender Tendenz. Der größte Teil dieser Bewegungen fand in der südlichen Nordsee statt, wiederum ein größerer Teil davon auf der Schifffahrtsstraße zwischen Elbmündung und Ärmelkanal. Seit den späten 1960er Jahren gilt er in der Nordsee ein System der Zwangswege: um den Schiffsverkehr möglichst reibungslos und unfallfrei zu gestalten, werden sowohl spezielle Tiefwasserwege ausgewiesen als auch sich behindernder Schiffsverkehr systematisch getrennt. Die wichtigsten Tiefwasserwege laufen von der Straße von Dover in die Deutsche Bucht. Große Häfen haben jeweils eigene Zugangswege; im Bedarfsfall (nämlich dann wenn sich Sedimente in Fahrrinnen abgelagert haben) stellen Baggerschiffe wieder die benötigte Mindest-Wassertiefe her.

Die Nordsee ist viel befahren; auf ihr verlaufen wichtige Handels- und Verkehrswege. Unter Seefahrern ist sie berüchtigt, unter anderem wegen des „Blanken Hans“ und der Untiefen wie der „Große Vogelsand“. Grundseen und sehr schwerer Seegang zu Zeiten der Sturmfluten in Frühling und Herbst haben zu vielen Schiffsunglücken geführt, die in früheren Zeiten gelegentlich auch Strandräubern als Verdienstquelle gedient haben.

An den Küsten werden sowohl die Strände als auch die Küstengewässer touristisch genutzt. Touristisch besonders erschlossen sind dabei die belgische, niederländische, deutsche und dänische Küste. In Großbritannien gibt es einzelne Touristenorte an der Nordseeküste. Der Küstentourismus konzentriert sich in England auf die Kanalküste.

Windsurfen und Segeln sind wegen des immer vorhandenen Windes beliebte Wassersportarten. Die Nordsee gilt wegen der starken Gezeiten und der vielen Flachwassergebiete in Küstennähe als wesentlich schwieriger zu segelndes Gebiet als Ostsee oder Mittelmeer, so dass hier weit weniger Segler unterwegs sind als an den anderen Küsten.

Das Wattwandern an den nordfriesischen Inseln und Halligen, den dänischen und ostfriesischen Inseln, aber auch Angeln und Sporttauchen, beispielsweise das Wracktauchen bei Scapa Flow, ist möglich.

Die besonderen klimatischen Bedingungen an z. B. der deutschen Nordseeküste gelten als gesundheitsfördernd. Bereits im 19. Jahrhundert nutzten Reisende ihren Aufenthalt an der Küste als Kur-Urlaub. Die günstigen Klimafaktoren von Luft, Temperatur, Wasser, Wind und Sonnenstrahlung aktivieren Abwehrkräfte und Kreislauf, stärken das Immunsystem und wirken heilend insbesondere auf Haut und Atemwege. Im Sinne der Thalasso-Therapie werden neben den klimatischen Gegebenheiten dabei zur Kuranwendung auch Meerwasser, Schlick, Sole, Algen und Meersalz als Heilmittel genutzt.

Eine Besonderheit waren in Deutschland die bis in die 1990er Jahre durchgeführten Butterfahrten, die als Schiffsfahrten außerhalb der Hoheitsgewässer einen zollfreien Einkauf ermöglichten.

Der Atlas "Geographike Hyphegesis" des Claudius Ptolemäus aus dem 2. Jahrhundert n. Chr. führt die Nordsee unter dem griechischen Namen "Germanikòs Ōkeanós". Dieser Name gelangte durch Lehnübersetzung als "Oceanus Germanicus" oder "Mare Germanicum" ins Lateinische, von da ins Englische als "German Sea" und ins Deutsche als "Deutsches Meer".

Die im spätmittelhochdeutschen belegte Bezeichnung "nordermer" oder "nortmer" wurde im 17. Jahrhundert durch den heute geläufigen Namen "Nordsee" ersetzt (niederländisch "Noordzee"). In der niederländischen Sprache bildet die "Noordzee" ein Gegensatzpaar mit der "Zuidersee" – der ‚südlichen See‘, von Friesland und der Nordseeküste aus gesehen. Bedingt durch die Verbreitung des von den Hansekaufleuten genutzten Kartenmaterials setzte sich der Name "Nordsee" (engl. "North Sea", frz. "Mer du Nord" etc.) allmählich europaweit durch.

Daneben gebräuchliche Namen waren lange Zeit "Mare Frisicum" (Friesisches Meer) und "Westsee", dessen dänische Entsprechung "Vesterhav" neben "Nordsø" heute noch üblich ist.

Die erste geschichtlich verbürgte intensive Nutzung der Nordsee als Verkehrsweg erfolgte durch die Römer. 55 und 54 v. Chr. drang Julius Caesar in Britannien ein (siehe Caesars Britannienfeldzüge). 12 v. Chr. ließ Drusus eine Flotte von über 1000 Schiffen bauen und über den Rhein in die Nordsee segeln. Der überlegenen Zahl, Taktik und Technik der Römer hatten die Friesen und Chauken nichts entgegenzusetzen, und als die Römer zu den Mündungen von Weser und Ems vordrangen, mussten sich die dort ansässigen Stämme ergeben.

5 v. Chr. konnten die römischen Kenntnisse über die Nordsee im Rahmen eines militärischen Vorstoßes unter Tiberius bis hin zur Elbe deutlich erweitert werden: Plinius beschreibt, dass römische Seeverbände an Helgoland vorbeikamen und sich bis an die Nordostküste Dänemarks vorwagten.

Mit der Eroberung Britanniens durch Aulus Plautius (43 n. Chr.) begann ein reger und regelmäßiger Schiffsverkehr zwischen den Häfen in Gallien (Portus Itius) und denen in England. Die römische Ära dauerte knapp 350 Jahre und endete mit dem Rückzug der römischen Legionen um das Jahr 400.

Im verbleibenden Machtvakuum auf der britischen Insel stießen die ursprünglich aus dem heutigen Norddeutschland und Dänemark stammenden Sachsen, Angeln und Jüten mit der nächsten großen Wanderungsbewegung über die Nordsee vor. Sie waren während der römischen Besatzungszeit Britanniens bereits als Söldner während der Spätphase des Römischen Reiches eingesetzt worden, überquerten in den Jahrhunderten der Völkerwanderung zahlreich die Nordsee und siedelten sich im Süden und Osten Englands an, wobei sie die ursprünglich dort lebenden Kelten in die Gebiete des heutigen Schottlands und Wales vertrieben.

Ungefähr im 7. Jahrhundert wanderten die ursprünglich aus den heutigen Niederlanden stammenden Friesen über die Nordsee auf die nordfriesischen Inseln Sylt, Amrum und Föhr aus. In einer zweiten Einwanderungswelle im 11. Jahrhundert wurde auch das jütländische Festland zwischen Eider und Wiedau in Südjütland besiedelt, wo die Friesen auf die Jüten stießen. Das nordfriesische Siedlungsgebiet macht heute einen Großteil des Kreises Nordfriesland aus.

Die nächste größere Wanderungswelle über die Nordsee brachte die vor allem aus dem heutigen Dänemark und Norwegen stammenden Nordmannen auf die britischen Inseln. Mit dem Überfall auf Lindisfarne 793 begannen die Plünderungszüge der Wikinger, die die nächsten hundert Jahre vor allem als Piraten und Plünderer unterwegs waren. Sie überfielen küstennahe Klöster, Gehöfte und Städte und fuhren auf den Flüssen landeinwärts. Dem "Anglo-Saxon Chronicle" zufolge begannen sie ab 851, auch zu siedeln. Diese Wanderungsbewegungen aus Skandinavien hielten bis etwa 1050 an.

Alfred dem Großen von Wessex gelang es als erstem sächsischen König, den Wikingern Widerstand zu leisten, indem er eine eigene Flotte aufstellte. Er konnte das Gebiet von den Dänen befreien und gilt als erster englischer König. Während das Meer die britischen Angelsachsen von den germanischen Stämmen getrennt hatte, hielten die Skandinavier die gesamte Zeit über die Nordsee Kontakt zur alten Heimat. Somit gehörte der größte Teil der britischen Inseln und der nördliche Teil des Meeres fest zum Machtbereich skandinavischer Könige, den Wikingern.

Hardiknut war der letzte dänisch-britische König, nach seinem Tod zerfiel das Nordseereich auf Grund innerer Konflikte, die politische Union zwischen Skandinaviern und Briten über die Nordsee hinweg war getrennt. Nachdem diese Trennung erfolgte, begann die Nordsee vorerst ihre Bedeutung zu verlieren. Seit dem Einfall Wilhelms des Eroberers aus der Normandie im heutigen Frankreich orientierten sich die britischen Inseln ebenso wie die westlichen Küstenregionen der Nordsee entlang der großen europäischen Flüsse nach Süden in Richtung Mittelmeer und Orient.

Die wichtigste Verbindung zur Außenwelt für Norddeutschland und Skandinavien war hingegen die Ostsee, wo die Hanse ihre Blütezeit erlebte. Der einzig bedeutendere Handelsweg über die Nordsee führte durch die deutsche Bucht von Flandern in die Häfen der Hansestädte.

Die Hanse hatte ihren Schwerpunkt zwar in der Ostsee, wichtige Kontore befanden sich aber auch im norwegischen Bergen (Bryggen), dem Stalhof im englischen London und dem flandrischen Hansekontor in Brügge.

Der Aufstieg Brügges begann für die Nordsee nicht untypisch mit einer Sturmflut, die 1134 eine tiefe Fahrrinne, den Zwin, riss, die das Anlaufen größerer Handelsschiffe in die Stadt ermöglichte. Zwischen Brügge und London begann sich ein lebhafter Handelsverkehr mit britischer Wolle und flandrischen Tüchern zu entwickeln.

Ab dem 13. Jahrhundert reisten deutsche Hanse-Kaufleute regelmäßig nach Brügge und London und begannen, eine regelmäßige Handelsroute in diese Städte aufzubauen. Brügge wurde zum Endpunkt der Ost-West-Handelslinie mit dem Peterhof in Nowgorod in Russland und war über den Schiffsverkehr zugleich mit Frankreich, Italien, Spanien und den Niederlanden verbunden.

Schon 1441 musste die Hanse die wirtschaftliche Gleichberechtigung der Niederländer anerkennen, nachdem Brügge als wichtigstem Kontor der Hanse mit Antwerpen ein mächtiger Konkurrent erwachsen war und sich die Niederlande zusätzlich mit den Dänen als den „Herren des Sunds“ verbündet hatten. Die Niederländer begannen nach der gewonnenen Grafenfehde, in die Handelsgebiete der Hanse vorzudringen und einen eigenen Ostseehandel zu betreiben.

Die vereinigten Niederlande entwickelten sich im 16. Jahrhundert zur ersten Welthandelsmacht. Für die Geschichte niederländischer Händler diente die Nordsee selbst nur mehr als Startpunkt für ihre Fahrten über die Ozeane. Sie war zum Tor zur Welt geworden, die Herrschaft über die Nordsee war ausschlaggebend dafür, einen direkten Weg zu den Märkten der Welt zu haben.

Während des Achtzigjährigen Krieges begannen die Niederlande auch mit einem groß angelegten Überseehandel – sie jagten Wale nahe Spitzbergen, betrieben Gewürzhandel mit Indien und Indonesien, gründeten Kolonien in Brasilien, Nordamerika (Nieuw Nederland), Südafrika und in der Karibik (vergleiche auch Die große Tulpenmanie). Der Reichtum, den sie aus diesem Handel anhäuften, führte im 17. Jahrhundert zum „Goldenen Zeitalter“ („de gouden eeuw“) der Niederlande.

1651 verhängte England die Navigationsakte, die vielen niederländischen Handelsinteressen schadete. Der Kampf um die Akte mündete 1652 in den Ersten Englisch-Niederländischen Krieg, der 1654 mit dem Frieden von Westminster endete; die Navigationsakte wurde durch die Niederlande anerkannt.

1665 erklärten die Engländer den Niederländern erneut den Krieg: Es begann der Zweite Englisch-Niederländische Krieg. Mit Unterstützung der Franzosen, die in der Zwischenzeit in die Spanischen Niederlande – heute Belgien – einmarschiert waren, gewannen die Niederländer die Oberhand. Engländer und Niederländer schlossen 1667 den Frieden von Breda, nachdem der niederländische Admiral Michiel de Ruyter einen großen Teil der englischen Flotte auf der Themse zerstört hatte. Es wurde vereinbart, dass die Engländer die niederländischen Besitzungen in Nordamerika (das Gebiet um das heutige New York City) behalten durften, während die Niederländer Suriname von den Engländern erhielten. Auch die Navigationsakte wurde zu Gunsten der Niederlande modifiziert.
Das Jahr 1672 wurde in den Niederlanden als das „Rampjaar“ („Katastrophenjahr“) bekannt: England erklärte der Republik den Krieg, gefolgt von Frankreich, dem Fürstbistum Münster und Kurköln, die eine Allianz gegen die Niederlande bildeten. Frankreich, Kurköln und das Fürstbistum Münster marschierten in die Republik ein, während die Landung der Engländer an der Küste nur knapp verhindert werden konnte.

Die Niederländer bezogen den südlichen Nordseeraum als Hinterland ein: In Schleswig-Holstein zeugen noch heute zahlreiche Hinterlassenschaften von Holländern, die einwanderten oder Handelsgüter mitbrachten. Die Niederländer brachten über das Meer ihre technische Meisterschaft in Deichbau und Entwässerungstechnik mit. Hausbau- und Landwirtschaftstechniken wurden von Holland beeinflusst, die Küstenstriche Schleswig-Holsteins gelangten ebenfalls zu Reichtum. Zahlreiche Bewohner der Küstengebiete heuerten auf niederländischen Schiffen an – besonders bekannt sind wohl die Walfahrer der nordfriesischen Inseln.

Englands Aufstieg zur beherrschenden Seemacht begann 1588, als der Invasionsversuch der spanischen Armada an einer Kombination von herausragenden englischen Seegefechten unter der Führung von Sir Francis Drake und dem schlechten Wetter scheiterte. Die erstarkende englische Marine lieferte sich mehrere Seekriege mit den auf der anderen Nordseeseite liegenden Niederlanden und konnte diese am Ende des 17. Jahrhunderts als weltumspannende Seemacht ablösen. Der Aufbau des Britischen Empires als Reich, „in dem die Sonne nie untergeht“, war nur möglich, weil die britische Marine die europäischen Gewässer und speziell die Nordsee uneingeschränkt beherrschte. Der einzig ernst zu nehmende Versuch, diese Vorherrschaft zu brechen wurde von Napoleon unternommen. Die von Admiral Horatio Nelson gewonnene Schlacht von Trafalgar, die die britische Vorherrschaft zur See für mehr als ein Jahrhundert sicherte, führte dann aber nur zur Kontinentalsperre, mit der Großbritannien von den Importen des europäischen Kontinents abgeschnitten werden sollte.

In diesem Krieg standen sich in der Nordsee hauptsächlich die Flotten der beiden Anrainer Deutschland (Kaiserliche Marine) und Großbritannien (Grand Fleet) gegenüber.

Auf Grund der Übermacht britischer Schiffe konnte die „Grand Fleet“ beinahe ungestört die Seeherrschaft über die Nordsee erlangen und eine Seeblockade einleiten. Das Ziel der Blockade war es, Deutschland von den Schifffahrtswegen zu trennen, um die Versorgung mit kriegswichtigen Importen zu verhindern und das ungestörte Übersetzen des britischen Expeditionskorps zu garantieren. Aufgrund der defensiven Ausstattung Helgolands mit einer starken Küstenverteidigung war für Deutschland nur die Deutsche Bucht gesichert, während die übrige Nordsee und der Ärmelkanal während des gesamten Krieges durch die Royal Navy kontrolliert wurde.
Das erste Seegefecht fand am 28. August 1914 vor Helgoland statt und endete mit einem klaren britischen Sieg. Da die Überwasser-Streitkräfte der kaiserlichen Marine auf offenem Wasser chancenlos waren, leiteten die Deutschen den U-Boot-Krieg ein. Nach anfänglichen Misserfolgen deutscher Unterseeboote gelang es "U 9" am 22. September 1914 drei britische Panzerkreuzer ca. 50 km nördlich von Hoek van Holland zu versenken.

Im November 1914 erklärte die britische Kriegsmarine die gesamte Nordsee zur Kriegszone, die daraufhin vermint wurde. Schiffe, die unter der Flagge neutraler Staaten fuhren, konnten in der Nordsee ohne Vorwarnung das Ziel britischer Angriffe werden.

Im Gefecht auf der Doggerbank erlitt Deutschland am 24. Januar 1915 eine weitere Niederlage gegen die Briten und in der Folgezeit schlugen sämtliche Versuche, die alliierte Nordseeblockade zu durchbrechen, fehl. Auf Grund dieser Fehlschläge erfolgte am 4. Februar der Beginn des uneingeschränkten U-Boot-Krieges, in dem neben alliierte auch neutrale Schiffe angegriffen werden konnten.

Am 31. Mai und 1. Juni 1916 kam es vor Jütland mit der Skagerrakschlacht zur größten Seeschlacht des Ersten Weltkriegs und gemessen an der Zahl und Größe der beteiligten Schiffe (258) zur wahrscheinlich größten Seeschlacht der Weltgeschichte. Das Ziel der Deutschen, die britische Marine entscheidend zu schwächen und damit die Aufhebung der Seeblockade zu erzwingen, wurde nicht erreicht. Letztlich endete die Schlacht ohne einen eindeutigen Sieger und Deutschland setzte wieder alle Hoffnungen auf den uneingeschränkten U-Boot-Krieg.

Als sich das Ende des Krieges anbahnte, sollte gegen den Willen der neuen deutschen Regierung am 28. Oktober 1918 noch einmal ein Großangriff auf die britische Marine stattfinden, worauf der Kieler Matrosenaufstand ausbrach und der Seekrieg sein Ende fand. Die Meuterei der Matrosen leitete auch die Entwicklung zur Novemberrevolution in Deutschland ein.

Auch der Zweite Weltkrieg war hinsichtlich des Seekrieges auf Seiten der Deutschen Marine vor allem ein U-Boot-Krieg, der allerdings kaum noch in der Nordsee, sondern vor allem im Atlantik ausgetragen wurde. Anders als im Ersten Weltkrieg war die Nordsee auch nicht mehr ausschließliches Hoheitsgebiet der Alliierten, sondern vor allem in den ersten Kriegsjahren Schauplatz einer intensiven Küstenkriegsführung mit kleinen Fahrzeugen wie U-Booten, Minensuchbooten und Schnellbooten. Doch trotz anfänglicher Erfolge, die Großbritannien zeitweise in eine Versorgungskrise brachten, gelang es nicht, den Widerstand entscheidend zu brechen. Wie im Ersten Weltkrieg beherrschten die Alliierten bald die See, speziell wegen der Luftüberlegenheit auch die Nordsee und schnitten Deutschland von überseeischer Versorgung ab. Der damit verbundene Mangel an Ressourcen für die Kriegführung war einer der Gründe dafür, dass der Krieg nicht zu gewinnen war.

Am 14. Oktober 1939 gelang es Kapitänleutnant Günther Prien mit dem Unterseeboot "U 47" in die Bucht von Scapa Flow einzudringen und das Kriegsschiff "HMS Royal Oak" mit 1400 Mann Besatzung zu versenken.

Am 9. April 1940 lief auf deutscher Seite die Operation Weserübung an, bei der fast die gesamte deutsche Flotte mobilisiert und in Richtung Skandinavien in Fahrt gesetzt wurde. Kurze Zeit später waren die militärischen Ziele der Invasion (Besetzung der norwegischen Häfen, Sicherstellung der Eisenerz-Versorgung, Verhinderung einer zweiten Front im Norden) erreicht und Norwegen und Dänemark besetzt. Diese Besatzung dauerte bis zum Ende des Krieges und während der gesamten Zeit diente der quer über die Nordsee laufende Shetland Bus als wichtiger Flucht- und Versorgungsweg von Norwegen nach Großbritannien. Zuerst von norwegischen Fischerbooten betrieben, wurden diese im Laufe des Krieges durch drei U-Boot-Jäger der Royal Navy ersetzt.

Auf Grund der Unterlegenheit bei den größeren Kampfschiffen, deutlich sichtbar durch die frühen Verluste ("Admiral Graf Spee" 1939, "Blücher" 1940 und "Bismarck" 1941), verlegte sich die Kriegsmarine mehr und mehr auf die Kriegsführung mit kleinen Einheiten und die verbliebenen Großkampfschiffe wie die "Tirpitz" ankerten nahezu untätig in Norwegens Fjorden.

In den letzten Kriegsjahren und den ersten Nachkriegsjahren unter alliierter Aufsicht wurden große Mengen Munition in der Nordsee verklappt. Während chemische Kampfstoffe vor allem in Skagerrak und Ostsee versenkt wurden, wurde konventionelle Munition (Granaten, Minen, Panzerfäuste, Patronen etc.) in der Deutschen Bucht versenkt. Die Zahlenschätzungen gehen hier weit auseinander, klar scheint jedoch zu sein, dass mehrere hunderttausend Tonnen Munition in der See versenkt wurden.

In der Zeit nach dem Zweiten Weltkrieg trat die Nutzung der Nordsee für friedliche Zwecke in den Vordergrund; denn während sich in der Ostsee die Gegner des Kalten Kriegs direkt gegenüberstanden und beäugten, war die Nordsee ein neben Schweden nur von NATO-Mitgliedsstaaten begrenztes Meer.

Ökonomische Bedeutung gewann die Nordsee in den 1960ern, als die Anrainerstaaten begannen, gefundenes Erdöl und -gas kommerziell zu nutzen. Die größte Katastrophe in der Geschichte der Öl- und Gasförderung in der Nordsee war der Untergang der Bohrinsel Piper Alpha 1988, bei dem 167 Menschen ums Leben kamen.

Im August 2011 erteilte die Deutsche Bundesregierung einen Auftrag zur systematischen archäologischen Prospektion der südlichen Nordsee, auch außerhalb der 12-Seemeilen-Zone, an das Deutsche Schiffahrtsmuseum in Bremerhaven, da der Bestand zahlreicher archäologischer Fundplätze durch geplante Bauvorhaben bedroht ist.




</doc>
<doc id="3506" url="https://de.wikipedia.org/wiki?curid=3506" title="Niue">
Niue

Niue (oder: Niuë) (niueanisch: Niuē; : []; alter Name "Savage Island") ist eine isolierte Koralleninsel im Südpazifik in der Nähe von Tonga, 2400 km nordöstlich von Neuseeland. Sie liegt südlich von Samoa und westlich der Cookinseln. Seit 1974 ist Niue durch einen Assoziierungsvertrag mit Neuseeland verbunden.

Niue ist die Spitze eines erloschenen Vulkans und mit 261,46 km² Fläche eines der größten gehobenen Atolle der Welt. Es erhebt sich etwa zwischen 4000 und 5000 Meter über den Meeresgrund. Das innere Plateau der Insel ist meist flach und reicht bis zu über den Meeresspiegel. Niue hat an seinem Rand ein bis zu 30 Meter steil aufragendes Riff aus Kalkstein. Von der Gesamtfläche können etwa 80 % für die Landwirtschaft genutzt werden, der Rest besteht aus meist tropischen Wäldern. Die Küste ist stark zerklüftet mit vielen Höhlen und Schluchten, zum Beispiel der Talava Cave an der Nordwestküste oder der Avaiki-Höhle im anstehenden Kalkstein an der Westküste. Nennenswerte Strände existieren nicht. Es gibt 14 Dörfer, das größte ist Alofi mit etwa 600 Einwohnern.

Die durchschnittliche Temperatur in Niue beträgt 26 °C, und im Schnitt fallen etwas mehr als 2000 mm Niederschlag. Da das Regenwasser schnell im porösen Korallenkalkstein versickert, bilden sich keine oberirdischen Gewässer. Man hat deshalb für die Wasserversorgung Zisternen gebaut.

Bei stabilen Hochdruck-Wetterlagen liegt Niue auf der Grenze zwischen tropischem und subtropischem Bereich mit südöstlichen Winden. Bei Tiefdruckgebieten dreht der Wind auf Nordwest bis West. Tropische Wirbelstürme kommen ebenfalls aus dieser Himmelsrichtung und haben in den Jahren 1959, 1960, 1970, 1989, 1990 und zuletzt 2004 die Insel heimgesucht. Die durch die Wirbelstürme verursachten hohen Wellenberge richten an Land zwar wegen der Steilküste praktisch keinen Schaden an, die Korallenbänke sind jedoch stark betroffen.

Am 6. Januar 2004 verwüstete der Zyklon Heta die Pazifikinsel schwer. Zwei Menschen verloren ihr Leben, über 200 Bewohner wurden obdachlos. Die durch den Zyklon bedingte Sturmflut richtete auf dem gesamten nordwestlichen Küstenabschnitt zwischen Liha Point im Nordosten und Halagigie Point im Westen schwere Verwüstungen an.

Nur vor und an der Küste Niues lebt die Niue-Plattschwanz-Seeschlange ("Laticauda schystorhyncha"). Sie wird von den Einheimischen „Katuali“ genannt, erreicht eine Länge von bis zu einem Meter, hat einen platten Schwanz und verbringt einen Teil ihres Lebens an Land. Die Schlange ist hochgiftig, soll sich aber friedliebend verhalten. Außerdem kommt auf Niue der Palmendieb ("Birgus latro", auch Kokosnussräuber) vor. Endemisch für die Insel ist "Nacaduba niueensis", eine Schmetterlingsart aus der Familie der Bläulinge. Der aufgrund seiner charakteristischen braunen Färbung und des komplizierten Musters auf der Unterseite der Flügel unverwechselbare Falter wird auf Sammlermünzen und Briefmarken abgebildet. Zudem lassen sich vor der Küste Niues Buckelwale antreffen. Niue ist einer der wenigen Plätze, wo man einige Monate im Jahr legal mit diesen baden darf.

Die Landwirtschaft wird durch den überwiegend fruchtbaren Boden begünstigt. Sie dient fast ausschließlich der Selbstversorgung. Angebaut werden vor allem Kokospalmen, Bananen, Passionsfrüchte, Limonen, Taro, Bataten, Maniok und Yams. Die Viehzucht umfasst vorwiegend Rinder, Schweine und Geflügel.

Verwaltungsmäßig wird Niue in 14 "Villages" (etwa: Gemeinden) gegliedert. Diese entsprechen den Dörfern mit dem jeweiligen Umland, wobei die Hauptstadt Alofi aus den Verwaltungseinheiten Alofi North und Alofi South besteht. Jede Gemeinde hat einen "Village Council" (Gemeinderat), der seinen eigenen "Village Council Chairman" wählt. Die Gemeinden entsprechen auch den Wahlbezirken. Jede Gemeinde entsendet einen "Assemblyman" (Vertreter) in das Parlament von Niue.

Niue hat 1607 Einwohner (2011). Eine starke Abwanderung nach Neuseeland hat die Einwohnerzahl seit 1966 um mehr als die Hälfte vermindert. Dies führte zu zahlreichen Wüstungen. In Neuseeland leben heute rund 24.000 Niueaner. Die Niueaner sprechen eine eigene polynesische Sprache, das Niueanische, dessen nächste verwandte Sprache Tongaisch ist. Daneben wird auch Englisch gesprochen.

Etwa zwei Drittel der Bevölkerung gehören der reformierten Ekalesia Niue an, daneben gibt es Katholiken, Mormonen, Zeugen Jehovas, Presbyterianer, Methodisten und Adventisten.

Niue wurde wahrscheinlich seit dem 10. Jahrhundert von Polynesiern aus Samoa besiedelt, weitere Siedler kamen im 16. Jahrhundert von Tonga. Niue hatte danach aber nur wenig Kontakt mit Nachbarinseln und entwickelte sich eigenständig, was sich in großen Unterschieden gegenüber Sprache und Kultur auf anderen Inseln der Region ausdrückt.

1774 entdeckte James Cook Niue für Europa. Er konnte dort wegen der feindlich gesinnten Bevölkerung nicht landen und gab der Insel den Namen Savage Island (Insel der Wilden). Missionare aus England bekehrten um 1846 die meisten Bewohner zum Christentum.

Vom 20. Oktober 1900 bis zum 11. Juni 1901 war Niue Teil der British Western Pacific Territories. Am 11. Juni 1901 wurde Niue von Neuseeland als Teil der Cookinseln annektiert. 1903 wurde Niue von den Cookinseln abgetrennt. 1974 wurde das Abkommen zur Assoziation mit Neuseeland geschlossen.

Der völkerrechtliche Status Niues ist von besonderer Natur. Niue ist ein autonomes Gebiet in „freier Assoziation mit Neuseeland“. Dieser Status wurde von Niue in einem Akt der Selbstbestimmung gewählt, der von den Vereinten Nationen gebilligt wurde und fortlaufend überwacht wird. Diesen Status besitzen auch die Cookinseln. Somit ist Neuseeland Suzerän Niues.

Niue ist ein sich selbst verwaltendes Territorium. Die gesetzgebende und ausführende Gewalt unterliegt keinen Beschränkungen durch Neuseeland. Die Einwohner des Landes sind Bürger Neuseelands. Auf Wunsch von Niue nimmt Neuseeland weiterhin bestimmte Zuständigkeiten des Landes im Bereich der Außen- und Sicherheitspolitik wahr.

Sowohl Niues abgeschiedene Lage als auch kulturelle und sprachliche Unterschiede zur übrigen polynesischen Bevölkerung der Cookinseln haben zu seiner Selbstverwaltung geführt.

Die starke Abwanderung, noch einmal intensiviert durch die Folgen der Sturmkatastrophe, führte 2004 zu Spekulationen neuseeländischer Politiker, dass der Assoziationsstatus in Zukunft zu Gunsten Neuseelands verändert werden könnte. Die vor allem vom neuseeländischen Außenminister Phil Goff vorgebrachten Überlegungen wurden vom niueischen Premierminister Young Vivian entschieden abgelehnt.

Niue ist assoziiertes Mitglied des Commonwealth of Nations und seit 1975 Mitglied des Pacific Islands Forum. Eine Mitgliedschaft bei den Vereinten Nationen bleibt dem Inselstaat aufgrund der Assoziierung und der geringen weltweiten Anerkennung bisher verwehrt. Obwohl Niue denselben Status besitzt wie die Cookinseln, hat auch Deutschland den Pazifikstaat bisher nicht anerkannt.

Niue ist stark von neuseeländischen Hilfszahlungen abhängig, im Budgetjahr 2015/16 belief sich die Hilfe Neuseelands auf 22,5 Mio. NZ$. Die Landwirtschaft dient vor allem der Selbstversorgung. Die Industrie verarbeitet hauptsächlich die erzeugten Nahrungsmittel. Der Verkauf von Briefmarken und selbstgemachtem Kunsthandwerk sind wichtige Einnahmequellen. Wie überall in Polynesien leidet die Wirtschaft unter dem Mangel an Ressourcen, der Abgeschiedenheit und der schrumpfenden Bevölkerung. Zudem erschütterte Anfang 2004 der Zyklon Heta die empfindliche Wirtschaft stark. Beim Wiederaufbau ist das Land auf ausländische Hilfe angewiesen. Die Regierung versucht, den Tourismus und das Finanzwesen zu fördern. Der Tourismus ist stark eingeschränkt, da es kaum Unterkünfte und keinen nennenswerten Strand gibt. Die schroffen Klippen verhindern fast überall den Zugang zum Meer. Murray McCully, Neuseelands Außenminister, beschloss ein staatliches Investment-Programm in Niues Tourismussektor im Wert von 7,5 Mio. NZ$. Der Tourismus, so McCully, sei der größte Antrieb für Niues Wirtschaftswachstum und könne die Eigenständigkeit Niues fördern.

Die Handelsbilanz Niues ist chronisch defizitär. Im Jahr 2008 standen Einfuhren im Wert von beinahe 11 Mio. NZ$ Ausfuhren von lediglich 27.000 NZ$ gegenüber.

Am 28. Januar 2016 legte die EU-Kommission ein Maßnahmenpaket zur Bekämpfung von Steuerflucht vor, bei dem unter anderem Niue auf der schwarzen Liste der Steueroasen auftaucht.

Im April 2016 wurde, basierend auf den an die Öffentlichkeit geratenen Panama Papers, Niue als Drehscheibe im Handel mit hochpreisigen Kunstwerken genannt.

Niue hat keinen Hafen. Alle Güter müssen von den vor Alofi auf Reede liegenden Schiffen auf kleinere Boote umgeschlagen werden, die dann in einem von zwei Richtfeuern markierten Fahrwasser durch eine Lücke im Korallenriff zur dortigen Anlegestelle gelangen können. Auf Niue gibt es mit dem Hanan International Airport einen Flughafen mit asphaltierter Landebahn. Bis Oktober 2005 wurde Niue einmal wöchentlich von Apia (Samoa) aus angeflogen. Jeden Freitag (ab November 2005) und seit Mai 2014 zusätzlich jeden Mittwoch fliegt die Air New Zealand ab Auckland mit einem Airbus A320 den Flughafen Niue an.

Insgesamt hat die Insel 120 Kilometer Straßen 1. Ordnung; davon sind 67 Kilometer Rundstraße. Es gibt keinen öffentlichen Nahverkehr.

In Niue gibt es einen Radio- und einen Fernsehsender sowie ein Internet-Café. Die Nachrichten werden vom neuseeländischen Fernsehen übernommen. Da Niue auf der anderen Seite der Datumslinie liegt, ist es einer der wenigen Orte der Erde, an denen man stets nur die Nachrichten „von morgen“ sehen kann.

Die Insel plant, ihren Strombedarf ab 2025 zu 80 % durch erneuerbare Energien (Windenergie und Solarenergie) zu decken.

Niue sorgte 2003 für Schlagzeilen, als die Internet Users Society Niue (IUSN) ein flächendeckendes, kostenloses WLAN auf der Insel einrichtete. Zunächst gab es Widerstand von der Regierung, die sich im Streit mit der IUSN befindet. Inzwischen sind jedoch bereits 18 Hotspots in Betrieb, ein weiterer ist in Vorbereitung (Stand September 2014). Somit wurde Niue die erste „Wi-Fi Nation“.

Auf der Insel stehen (2009) 1100 Internet-Usern 382.600 Domains (2008) gegenüber. Der Grund liegt in der internationalen Namenskennung. nu, das in einigen skandinavischen Sprachen und im Niederländischen „jetzt“ bedeutet und entsprechend gern verwendet wird, weil daraus interessante Namen gebildet werden können. Beispielsweise sicherte sich eine schwedische Internetfirma aus Kalmar die Domain "sverige.nu" und vertreibt Domains mit der .nu-Endung. Für die Zuweisung der internationalen Top-Level-Domain setzt die hierfür zuständige IANA einen gewissen Autonomiestatus des betreffenden Gebietes voraus.

Da „nu“ auf Französisch „nackt“ bedeutet, wird die Domain auch gern für französischsprachige Erotikseiten benutzt.

Der einzige Fernsehsender und der einzige Radiosender (sendet auf FM und AM) auf Niue werden von der Broadcasting Corporation of Niue betrieben, einer staatlichen Rundfunkanstalt. Als vom Staat unabhängiges Medium, das nicht internetbasiert ist, besteht nur die Zeitung Niue Star, die gleichzeitig die einzige Zeitung des Landes ist.

Der Fußballverband "Niue Island Soccer Association" ist nicht Mitglied des Weltfußballverbandes FIFA und lediglich assoziiertes Mitglied des Kontinentalverbandes OFC. Daher kann sich das Land nicht für Fußball-Weltmeisterschaften oder die Fußball-Ozeanienmeisterschaft qualifizieren. Bisher trat eine niueanische Fußballnationalmannschaft lediglich bei zwei Länderspielen bei Südpazifik-Spielen 1983 in Erscheinung. Die Spiele gegen Papua-Neuguinea (0:19) und Tahiti (0:14) gingen verloren.

Dagegen besitzt Niue eine relativ erfolgreiche Rugby-Nationalmannschaft, die an der Qualifikation zur WM 2007 teilnahm. In deren Rahmen wurde am 23. Juli 2005 ein Sieg gegen Tahiti (55:8) erreicht, dem jedoch zwei Wochen später eine Niederlage gegen die Cookinseln folgte (5:24), wodurch die Qualifikation für die Endrunde verpasst wurde. Derzeit (Oktober 2017) wird die Mannschaft in der Weltrangliste auf Rang 86 geführt. Besonderes Aufsehen erregten zwei Siege gegen das viel höher eingeschätzte Japan Anfang 2003.

Seit einiger Zeit gibt die Regierung von Niue Sammler- und Anlagemünzen heraus. Die Münzen gelten zwar per Beschluss als offizielles Zahlungsmittel, sind zu diesem Zweck auf Niue aber nicht in Gebrauch. Die Motive reichen von Tennisspielern bis zu Figuren aus Disney-Comics und den Science-Fiction-Serien Doctor Who und Star Wars.

Darüber hinaus gibt Niue auch bekanntere Münzen aus, wie die „Niue Lunar Serie“ oder die „Niue Turtle“. Beide Münzserien werden, wie viele andere von Niue herausgegebene Geldstücke, von der New Zealand Mint geprägt.




</doc>
<doc id="3508" url="https://de.wikipedia.org/wiki?curid=3508" title="Niklas Luhmann">
Niklas Luhmann

Niklas Luhmann (* 8. Dezember 1927 in Lüneburg; † 6. November 1998 in Oerlinghausen) war ein deutscher Soziologe und Gesellschaftstheoretiker. Als wichtigster deutschsprachiger Vertreter der soziologischen Systemtheorie und der Soziokybernetik zählt Luhmann zu den herausragenden Klassikern der Sozialwissenschaften im 20. Jahrhundert.

Luhmann wurde 1927 in die Familie eines Brauereibesitzers in Lüneburg geboren und besuchte das altsprachliche Johanneum. 1944 wurde er im Alter von 16 Jahren offiziell als Luftwaffenhelfer eingezogen, nachdem er schon seit dem 15. Lebensjahr dort tätig gewesen war. Von 1944 bis September 1945 war Luhmann in amerikanischer Kriegsgefangenschaft; seine Behandlung dort erschien ihm später als „ – gelinde gesagt – nicht nach den Regeln der internationalen Konventionen.“

Luhmann studierte von 1946 bis 1949 Rechtswissenschaft an der Albert-Ludwigs-Universität Freiburg mit einem Schwerpunkt auf römischem Recht. Es folgte bis 1953 eine Referendarausbildung in Lüneburg. 1954 bis 1962 war er Verwaltungsbeamter in Lüneburg, 1954 bis 1955 am Oberverwaltungsgericht Lüneburg Assistent des Präsidenten und wurde 1955 ins niedersächsische Kultusministerium abgeordnet. In dieser Zeit begann er auch mit dem Aufbau seiner Zettelkästen. 1960/61 erhielt Luhmann ein Fortbildungs-Stipendium für die Harvard-Universität, das er nach seiner Beurlaubung wahrnehmen konnte. Dort kam er in Kontakt mit Talcott Parsons und dessen strukturfunktionaler Systemtheorie.

Nach seiner Tätigkeit als Referent an der Deutschen Hochschule für Verwaltungswissenschaften Speyer von 1962 bis 1965 war er von 1965 bis 1968 Abteilungsleiter an der Sozialforschungsstelle an der Universität Münster in Dortmund. Nachdem er im Wintersemester 1965/66 für Soziologie an der Westfälischen Wilhelms-Universität in Münster eingeschrieben war, wurde er dort im Februar 1966 zum Dr. sc. pol. (Doktor der Sozialwissenschaften) promoviert mit seiner in Speyer entstandenen Schrift "Recht und Automation in der öffentlichen Verwaltung. Eine verwaltungswissenschaftliche Untersuchung" (Gutachter: Dieter Claessens und Helmut Schelsky). Fünf Monate später habilitierte er sich bei Dieter Claessens und Heinz Hartmann mit dem bereits 1964 erschienenen Buch "Funktionen und Folgen formaler Organisation". Seine Berufung 1968 an die Universität Bielefeld machte Luhmann nicht nur zum ersten Professor an dieser Neugründung, sondern ließ ihn in der Folge auch entscheidend am Aufbau der ersten soziologischen Fakultät im deutschsprachigen Raum mitwirken. Hier lehrte und forschte er bis zu seiner Emeritierung 1993.

Luhmann heiratete 1960 Ursula von Walter. Aus der Ehe gingen eine Tochter und zwei Söhne hervor. Seine Ehefrau starb 1977; danach zog er seine Kinder allein groß.Wie erst 2007 bekannt wurde, war Niklas Luhmann Mitglied der NSDAP. Allerdings ist die Aussagekraft der Mitgliederdatei allein geschichtswissenschaftlich umstritten: inwieweit die Aufnahme in die Partei z. B. im Rahmen der 1944/45 von der Parteiführung initiierten Sammelanmeldung insbesondere der HJ-Jahrgänge 1926/27 erfolgte, ist ungeklärt; auf dem bei der Aufnahme in den juristischen Referendariatsdienst in der Nachkriegszeit üblichen "Fragebogen für die politische Überprüfung" (sog. Entnazifizierung) hat Luhmann 1949 den Aufnahmeantrag auf das Frühjahr 1944 datiert, gleichzeitig aber angegeben, eine Mitgliedsnummer nie erhalten zu haben.

Luhmanns Kinder stritten jahrelang vor Gericht um die Eigentums- und Urheberrechte an seinem wissenschaftlichen Werk einschließlich des berühmten Zettelkastens, worauf das OLG Hamm im Jahr 2004 entschied, dass allein seiner Tochter die Rechte daran zustehen. Luhmann hatte ihr noch zu Lebzeiten 1995 in einem Vorausvermächtnis alle Urheberrechte zugewendet, da es sein Wunsch war, dass sein geistiges Erbe in einer Hand bleibt. Der Wert seines wissenschaftlichen Nachlasses wird auf einen siebenstelligen Euro-Betrag geschätzt.

Die Universität Bielefeld erwarb mit Unterstützung der Krupp-Stiftung seinen Nachlass 2011, um ein "Luhmann-Archiv" zu errichten. Wichtigster Teil des Nachlasses ist der ca. 90.000 Zettel umfassende Zettelkasten, den Luhmann seit den 1950er Jahren gepflegt hat und der die Grundlage des umfangreichen Werkes darstellt. In einem von der Nordrhein-westfälischen Akademie der Wissenschaften und der Künste geförderten Langzeitprojekt der Fakultät für Soziologie der Universität Bielefeld in Kooperation mit dem Cologne Center for eHumanities (CCeH) werden seit 2015 sowohl der Zettelkasten als auch die im Nachlass befindlichen Manuskripte – darunter allein vier verschiedene Versionen einer Gesellschaftstheorie aus den 1960er bis 1990er Jahren – digitalisiert und ediert. Der Nachlass befindet sich im Universitätsarchiv Bielefeld.

Anlässlich des 90. Geburtstags Niklas Luhmanns wurde im Dezember 2017 ein ca. 1000seitiges Manuskript zur Gesellschaftstheorie aus dem Nachlass veröffentlicht. Das unter dem Titel "Systemtheorie der Gesellschaft" erschienene Grundlagenwerk, das bereits weitgehend die Struktur der schließlich 1997 publizierten Gesellschaftstheorie vorwegnimmt, zusätzlich aber noch eine umfassende sozial- und differenzierungstheoretische Einleitung umfasst, hat Luhmann zwischen 1973 und 1975 verfasst, das Manuskript dann aber nicht publiziert.

Das Lebenswerk Luhmanns ist eine allgemeine und umfassende Theorie der Gesellschaft, die gleichermaßen Geltung in der wissenschaftlichen Untersuchung sozialer Mikrosysteme (Liebesbeziehungen) und Makrosysteme (Rechtssystem, Politik) beansprucht. Der Anspruch seiner Theorie auf besonders große Tragweite beruht darauf, dass seine Systemtheorie von der Kommunikation ausgeht und die Strukturen der Kommunikation in weitgehend allen sozialen Systemen vergleichbare Formen aufweisen. Luhmanns Systemtheorie kann als Fortsetzung des radikalen Konstruktivismus in der Soziologie verstanden werden. Er knüpft vor allem an die theoretischen Grundlagen Humberto Maturanas und dessen Theorie autopoietischer Systeme an. Ferner lieferten Edmund Husserl und Immanuel Kant wichtige Voraussetzungen, was den theoretischen Zeitbegriff anbelangt, sowie George Spencer-Brown, was den Form- und Sinnbegriff anlangt. Dem gegenüber bricht Luhmann mit theoretischen Grundannahmen der Soziologie und Philosophie, die in unlösbare Paradoxien hineinführen: z. B. ersetzt er Handlung durch Kommunikation als basalen soziologischen Operationstyp. Er bricht auch mit dem klassischen Subjekt-Objekt-Schema und ersetzt es durch die Leitdifferenz System und Umwelt.

Bereits 1970 lieferten sich Luhmann und der Soziologe Jürgen Habermas, als jüngster Vertreter der Kritischen Theorie, eine ausführliche Kontroverse zu ihren teils gegensätzlichen Theoriemodellen, die sie mit einer gemeinsamen Publikation „Theorie der Gesellschaft oder Sozialtechnologie“ dokumentierten. Der wohl wichtigste Streitpunkt dieser Kontroverse war, ob die Soziologie eine moralische Komponente oder eine soziale Utopie (Herrschaftsfreiheit) durchzutragen habe oder lediglich eine Beschreibung der Gesellschaft nach funktionaler Prämisse leisten müsse. Aus der Sicht Luhmanns fällt die Antwort so aus, dass das Erstere nur auf Kosten des Letzteren möglich ist. Wenn sich die Soziologie an der Kritik oder am Diskurs orientiert, so ist sie damit auch an bestimmte Ausgangslagen gebunden und kommt fatalerweise nur zu Aussagen von zeitlich begrenzter Gültigkeit. Um dem zu entgehen, muss Luhmann zufolge die Soziologie eine noch größere Abstraktion der sozialen Dynamik finden, die dafür eine längere Geltungsdauer beanspruchen kann. Die moralische Bewertung und Kritik des Zeitgeschehens werde dadurch keineswegs ausgeschlossen, im Gegenteil, sie werde lediglich aus der Funktion der Soziologie ausgelagert in andere Bereiche, nämlich Politik oder Ethik. Dieser Schritt sei besonders deshalb erforderlich, weil die Soziologie bis dato weder über einen allgemeinen Begriff noch über eine allgemeine Theorie der Gesellschaft verfügt. Für die Soziologie als Wissenschaft sei es notwendig, dass sie ihren Gegenstand in allgemeiner Weise bezeichnen kann.

Luhmanns Theorie der Gesellschaft geht davon aus, dass die „moderne“ Gesellschaft durch den Prozess der funktionalen Differenzierung gekennzeichnet ist. Die Gesellschaftsstruktur des alten Europa hat sich aufgrund der Komplexitätszunahme eigener Sinnressourcen von der segmentären zur stratifikatorisch-hierarchischen und weiter zur funktional differenzierten Ordnung umgeformt. In der Moderne lösen sich zunehmend Teilsysteme aus dem Gesamtkontext der Gesellschaft heraus und grenzen sich nach Maßgabe eigener funktionaler Prämissen vom Rest der Gesellschaft ab (Ausdifferenzierung). Die moderne Gesellschaft ist aufgelöst in eine wachsende Vielheit von Teilsystemen, die sich gegenseitig zur Umwelt haben und die strukturell mehr oder weniger fest aneinander gekoppelt sind. Die Gesellschaft überhaupt stellt für jedes einzelne Teilsystem (und für alle Teilsysteme zusammen) einen identischen Hintergrund dar, der funktional auf die Möglichkeit der Kommunikation hin entworfen werden kann.

Luhmann bietet erstmals in der relativ jungen Geschichte der Soziologie (ca. 150–200 Jahre, vergleiche die mindestens 2500 Jahre bestehende Tradition der Philosophie) nach Emil Durkheim, Max Weber und weiteren einen allgemein gültigen und zeitlich konsistenten Begriff der Gesellschaft an, der die grundlegende Paradoxie aufzulösen vermag, dass die Soziologie selbst ein Teil der Gesellschaft ist, also selbst ein Teil des Gegenstandes ist, den sie wissenschaftlich zu begreifen sucht, und dadurch die Unabhängigkeit und Unbedingtheit dessen, als was Gesellschaft bezeichnet wird, entscheidend beeinträchtigt werden. Schließlich wird alles, womit die Soziologie arbeitet – Sprache, Kommunikation, Buchdruck, Problemlagen, Forschungsziele, Geld usw., – von der Gesellschaft bereitgestellt.

Im Sinne der Wissenschaftslogik ist ein selbst entwickelter Gesellschaftsbegriff selbst-implikativ und ungültig. Das Betätigungsfeld der Soziologie muss nach Luhmann zu der Frage umgedreht werden, "wie es trotzdem möglich ist", dass Teilsysteme sich in der Gesellschaft orientieren können und dennoch relativ stabile Strukturen aufweisen und dass sich dauerhafte Institutionen in der Gesellschaft etabliert haben, die anscheinend (vielleicht aber auch nur scheinbar) die Lage beherrschen. Die Teilsysteme der Gesellschaft werden im Hinblick auf ihre evolutiven, selbst-stabilisierenden, autopoietischen Strukturen hin beobachtet und geben selbst die Antwort darauf, was Gesellschaft ist, indem sie zeigen, wie sie mit der Komplexität und Paradoxierung der Gesellschaft umgehen. Diesen Beobachtungen hat sich Luhmann zugewendet.

1984 erschien Luhmanns (chronologisch) erstes Hauptwerk „Soziale Systeme. Grundriß einer allgemeinen Theorie“. Mit dem von langer Hand vorbereiteten Entwurf gab Luhmann seiner Systemtheorie zum ersten Mal eine einheitliche Gestalt. Nach „Soziale Systeme“ folgten mehrere Bände über Teilsysteme der Gesellschaft („Die Wirtschaft der Gesellschaft“, „Die Wissenschaft der Gesellschaft“, „Die Kunst der Gesellschaft“) und weitere. In diesen Arbeiten wird mit ausreichend tiefem Rückgriff in die Geschichte des jeweiligen Teilsystems dessen Ausdifferenzierung und Evolution nachvollzogen. Es werden die funktionalen Prämissen, die symbiotischen Mechanismen, die symbolisch generalisierten Kommunikationsmedien sowie die operative Geschlossenheit und Autopoiesis des Teilsystems und dessen Verhältnis und strukturelle Kopplung zur Umwelt untersucht. Luhmann erbrachte hier eine interdisziplinäre Leistung, wie seine vielfältigen bibliographischen Querverweise belegen. Vollendet wurde dieser Entwurf 1997 durch das zweite Hauptwerk „Die Gesellschaft der Gesellschaft“ (2 Bde.), das die am weitesten ausgearbeitete Fassung seiner Systemtheorie und seines Gesellschaftsbegriffes darstellt.

Die Luhmannsche Systemtheorie (in Abgrenzung zur allgemeinen Systemtheorie von Ludwig von Bertalanffy u. a. sowie zur Theorie sozialer Systeme von Talcott Parsons) gilt derzeit als eine der erfolgreichsten und verbreitetsten Theorien im deutschen Sprachraum, nicht nur in der Soziologie, sondern auch in diversen Feldern wie der Psychologie, der Theorie des Managements oder der Literaturtheorie. Auch international beeinflusst sie den sozialphilosophischen Diskurs, wobei sich nennenswerte Luhmann-Strömungen in Deutschland, den USA, Japan, Italien und Skandinavien herausgebildet haben.

Das Fehlen eines primär normativen Elements in Luhmanns Systemtheorie hat eine teilweise heftige Debatte nicht nur in der Soziologie entfacht. Aus erkenntnistheoretischer Perspektive wird moniert, die Theorie laufe auf Grund ihres tautologischen, deskriptiven Ansatzes ins Leere und sage nicht mehr über die Welt, als was wir aufgrund fachwissenschaftlicher Erkenntnisse ohnehin schon über sie wissen oder wissen könnten. Genau dieser konstruktivistische Ansatz ist wiederum eine wesentliche Eigenschaft der Theorie: Als Beobachter der Welt können wir nach Luhmann nur das identifizieren und beschreiben, was für uns beobachtbar ist.










Zahlreiche Begriffe der Soziologie wurden von ihm geschaffen, mehr noch aufgegriffen und systemtheoretisch neu interpretiert. Im Einzelnen:

Als renommierte Wissenschaftler, die an Luhmanns Werk anknüpfen und sich ihm verbunden fühlen, gelten u. a. Dirk Baecker, Elena Esposito, Peter Fuchs, Andreas Göbel, André Kieserling, Armin Nassehi, Rudolf Stichweh, Gunther Teubner, Helmut Willke, Rudolf Schlögl und David Seidl.








</doc>
<doc id="3509" url="https://de.wikipedia.org/wiki?curid=3509" title="Nachhaltigkeit">
Nachhaltigkeit

Nachhaltigkeit ist ein Handlungsprinzip zur Ressourcen-Nutzung, bei dem die Bewahrung der wesentlichen Eigenschaften, der Stabilität und der natürlichen Regenerationsfähigkeit des jeweiligen Systems im Vordergrund steht.

Die Bezeichnung hat eine komplexe und facettenreiche Begriffsgeschichte. Das Wort "Nachhaltigkeit" stammt von dem Verb "nachhalten" mit der Bedeutung . Heutzutage sind im Wesentlichen drei Bedeutungen zu unterscheiden:

Eine erstmalige Verwendung der Bezeichnung "Nachhaltigkeit" in deutscher Sprache im Sinne eines langfristig angelegten verantwortungsbewussten Umgangs mit einer Ressource ist bei Hans Carl von Carlowitz 1713 in seinem Werk "Silvicultura oeconomica" nachgewiesen. Carlowitz fragte, .

Nachhaltigkeit gilt in einem Wörterbucheintrag von 1910 als Übersetzung von lateinisch "perpetuitas" und ist das Beständige und Unablässige wie auch das ununterbrochen Fortlaufende, das Wirksame und Nachdrückliche oder einfach der Erfolg oder die Wirksamkeit einer Sache. Vor 1860 war die Bezeichnung als Substantiv noch nicht lexikalisch erfasst, im Rechtschreibduden erstmals 1915 (anders das Adjektiv "nachhaltig"); bis in die 1980er Jahre hatte sie alltagssprachlich die Bedeutung von Dauerhaftigkeit und wurde nicht für einen Begriff politischen Sinnes verwendet. Beispielsweise taucht das Wort "nachhaltig" in Meyers Konversations-Lexikon von 1905 auf im Satz , und des Weiteren in der Aussage, dass ein Forst bereits die nachhaltige Form einer Waldwirtschaft darstellt.

Im derzeitigen Sprachgebrauch konkurrieren unterschiedliche Begriffsauffassungen:

Subsistenzorientierte, traditionelle Wirtschaftsformen, die noch weitgehend unverändert sind (wie Jagen und Sammeln, Feldbau (sofern die Naturgebiete noch ausreichend groß und dünn bevölkert sind) sowie die Fernweidewirtschaft) bilden stabile und dauerhafte – also im ursprünglichen Sinne nachhaltige – Wirtschaftssysteme, die in vielfältiger Weise mit den natürlichen Ökosystemen vernetzt sind. Sie sind gekennzeichnet durch effiziente, langsame und kontinuierliche Anpassung der Landnutzung an die jeweiligen Standortbedingungen seit Jahrhunderten.
Dieser Effekt kehrt sich allerdings mehr und mehr um, wenn ein rapider ökonomischer und sozialer Wandel Probleme schafft, deren Auswirkungen nicht vorhersehbar sind (siehe auch Kalte und heiße Kulturen oder Optionen).

Auch die Biodiversitätskonvention der UNO erkennt an, dass traditionelle Lebensweisen in besonderem Maße nachhaltig sind und die biologische Vielfalt nicht verringern. Im Gegensatz zu industrialisierten Gesellschaften, die nicht unmittelbar auf ein bestimmtes Gebiet angewiesen sind, haben solche Gemeinschaften ein direktes Interesse an der Aufrechterhaltung und dem Schutz dieser Ökosysteme, deren Stabilität sie nie gefährdet haben.

Die Ethnologie hat gezeigt, dass traditionell-nachhaltiges Wirtschaften in sehr vielen indigenen Kulturen "(vor dem Kontakt mit den Europäern)" als moralische Leitlinie einer „heiligen Erdverbundenheit“ im kulturellen Gedächtnis " über animistische Weltbilder, Mythen, Rituale und Tabus der Ethnischen Religionen" verankert war "(→ siehe auch: Wildes Denken)". Nach Odum und Cannon verfügen alle stabilen Systeme über Mechanismen, die ihren Gleichgewichtszustand möglichst konstant halten und dabei Schwankungen der Umwelt ausgleichen. Die Anthropologen Roy Rappaport, Gerardo Reichel-Dolmatoff und Thomas Harding haben unabhängig voneinander festgestellt, dass die Mythen und rituellen Zyklen der sogenannten „Naturvölker“ genau diese Aufgabe erfüllen und den Gemeinschaften ermöglichen, sich Veränderungen der Umwelt "so weit wie möglich" anzupassen und die Stabilität der Ökosysteme dabei "so wenig wie möglich" zu beeinträchtigen.

Die 1983 von den Vereinten Nationen eingesetzte Weltkommission für Umwelt und Entwicklung (Brundtland-Kommission) beeinflusste die internationale Debatte über Entwicklungs- und Umweltpolitik maßgeblich, ohne jedoch auf den Ursprung in der deutschen forstwirtschaftlichen Debatte Bezug zu nehmen. Die Kommission unter dem Vorsitz der ehemaligen norwegischen Ministerpräsidentin Gro Harlem Brundtland hatte den Auftrag, langfristige Perspektiven für eine Entwicklungspolitik aufzuzeigen, die zugleich umweltschonend ist. In ihrem auch als Brundtland-Bericht bekannt gewordenen Abschlussdokument "Unsere gemeinsame Zukunft" aus dem Jahre 1987 ist das von diesem Leitgedanken inspirierte Konzept der nachhaltigen Entwicklung definiert.

Nachhaltigkeit ist eine sowohl in der Wissenschaft als auch in der Politik und in der Unternehmenspraxis verwendete Bezeichnung. Er dient weniger als beschreibender, sondern in aller Regel als normativer Zielbegriff. Dabei werden die Ausdrücke „Nachhaltigkeit“ und „nachhaltige Entwicklung“ vielfach synonym verwendet. Es konkurrieren in der Debatte mindestens die folgenden drei Varianten:




Manche Autoren stellen fest, dass aufgrund der vielfältigen Definition "Nachhaltigkeit" zu einem „Gummiwort“ geworden sei. Zugleich wird aber betont, dass die Idee werde. Um die Unschärfeprobleme mit der Bezeichnung "Nachhaltigkeit" zu umgehen, wird bei Auseinandersetzungen um umweltverträgliche Formen der Zivilisation teilweise auf andere Bezeichnungen ausgewichen wie "Zivilisationsökologie" oder "Zukunftsverträglichkeit", die sich bislang jedoch nicht durchsetzen konnten.

Für viele Unternehmen ist das Attribut „nachhaltig“ zu einer inhaltlich schwer überprüfbaren Komponente ihrer PR-Strategie geworden. Demgegenüber stehen Konzepte des Nachhaltigkeitsmanagements, die unternehmerischen Erfolg mit der Berücksichtigung sozialer und ökologischer Aspekte verbinden. Demnach können sich Unternehmen durch besonders nachhaltiges Handeln einen Wettbewerbsvorteil verschaffen.

Die deutsche politische Debatte zu diesem Begriff ist auf kommunaler Ebene stark mit den von der UN-Konferenz angeregten Lokalen-Agenda-21-Prozessen verbunden. Auf Bundesebene wird die Bezeichnung seit der Enquete-Kommission „Schutz des Menschen und der Umwelt. Ziele und Rahmenbedingungen einer nachhaltig zukunftsverträglichen Entwicklung“ von 1995 stärker verwendet. Der 1. Zwischenbericht der Kommission diente schon auf der 1. Rio-Folgekonferenz in New York im Jahr 1997 („Rio Plus 5“) dazu, über die deutschen Beiträge zur Umsetzung des Nachhaltigkeitskonzeptes zu berichten.

In der bundesdeutschen politischen Debatte hatten Bündnis 90/Die Grünen den Begriff der Nachhaltigkeit bei der Bundestagswahl 1998 noch dominant besetzt, er fand aber auch Erwähnung bei allen anderen im Bundestag vertretenen Parteien. Zur Bundestagswahl 2002 benutzten dann mit Ausnahme der Grünen die anderen Parteien das Wort Nachhaltigkeit deutlich häufiger als noch vier Jahre zuvor.





Nachhaltigkeit enthält in seiner Grundidee einen Nutzen für alle Beteiligten. Wenn der Umstieg auf nachhaltige Wirtschaftsformen allerdings aus der Not heraus stattfindet, weil der Raubbau an den Ressourcen bereits sehr weit fortgeschritten ist, dann liegt darin durchaus auch Konfliktpotential. In der deutschen Holzwirtschaft des 18. und 19. Jahrhunderts – als es kaum noch Wälder gab – stellten die Menschen sich die Frage, wer von dieser neuen Forstwirtschaft profitieren würde und wer nicht. Dies ist besser zu verstehen, wenn man sich vor Augen führt, dass die Menschen in den Wintern (der damaligen „Kleinen Eiszeit“) auf jedes Klafter Brennholz angewiesen waren, um nicht zu erfrieren. Der Bedarf war unmittelbar da und viel zu groß, um gedeckt zu werden – es herrschte akute Holznot. Nachhaltigkeit in der Forstwirtschaft setzt jedoch voraus, dass genügend Bäume stehen bleiben, die zum Teil mit polizeilicher Gewalt vor dem Diebstahl durch verzweifelte Menschen geschützt werden mussten. Ähnliche „notgedrungene“ Interessenkonflikte sind auch heute noch in vielen Gebieten der Erde an der Tagesordnung, in denen Nachhaltigkeit einen fortgeschrittenen Raubbau ersetzen soll.




Seit 2001 hat sich an den Universitäten die Fachrichtung Nachhaltigkeitswissenschaft etabliert. Die Forschung außerhalb des universitären Umfelds wird meist eher unter dem Schlagwort sozial-ökologische Forschung zusammengefasst. 
Das Bundesministerium für Bildung und Forschung fördert im Rahmenprogramm „Forschung für Nachhaltige Entwicklung (FONA)“ Forschungsprojekte zum Klimawandel, zum Meere und Ozeanen, zu Städten der Zukunft und zur nachhaltigen Ressourcennutzung. Mit dem FONA-Rahmenprogramm setzt die Bundesregierung die nationale Nachhaltigkeitsstrategie um.
Anfang März 2015 startete das Leistungszentrum Nachhaltigkeit Freiburg, eine Kooperation zwischen der dortigen Albert-Ludwigs-Universität, den fünf Freiburger Instituten der Fraunhofer-Gesellschaft sowie Unternehmen aus der Industrie.

Seit 2008 verleiht die Stiftung Deutscher Nachhaltigkeitspreis jährlich den Deutschen Nachhaltigkeitspreis öffentlichkeitswirksam im Rahmen einer großen Gala.




Hörbuch





</doc>
<doc id="3511" url="https://de.wikipedia.org/wiki?curid=3511" title="Name">
Name

Ein Name ist, nach einer aktuellen wissenschaftlichen Definition, ein verbaler Zugriffsindex auf eine Informationsmenge über ein Individuum.
Namen sind somit einer Person, einem Gegenstand, einer organisatorischen Einheit (z. B. einem Betrieb) oder einem Begriff zugeordnete Informationen, die der Identifizierung und Individualisierung dienen sollen (Funktion der Namenklarheit).

Mit der wissenschaftlichen Erforschung von Namen beschäftigt sich die Onomastik. Sie unterteilt sich in die Namenkunde, die Geschichte, Gebrauch und Etymologie der Namen klärt, und die Theoretische Namenforschung, die sich mit der Frage beschäftigt: Was ist ein Name? Die Namentheorie beschäftigt sich demnach mit der Hauptbedeutung, der Denotation, der Namen, während sich die Namenkunde über die Denotation hinaus, mit der Konnotation der Namen beschäftigt. Siehe auch Artikel Theoretische Namenforschung und Begriff.

Das gemeingerm. Wort mhd. "name", ahd. "namo" beruht auf idg. "[e]nō̌mn̥-".

Die moderne Logik beziehungsweise theoretische Namenforschung sieht im "Namen" ("Nomen" im logischen Sinne, also wesentlich enger gefasst als im grammatischen Sinne) einen Spezialfall der "Bezeichnung". Man unterscheidet drei grundlegende Typen der Nomina:
Namen sind im Judentum mehr als Begriffe, das gilt auch für die Gottesnamen. Seine "Gattungsbezeichnung" ist "Elohim", sein „Eigename“ ist "JHWH" - auch Tetragramm genannt -, der mit "Jahwe" oder seltener und später mit Jehova vokalisiert wurde. Die hebräische Konsonantenschrift hatte keine Vokale, erst die Masoreten im Mittelalter fügten Vokale hinzu. So ist "JHWH" eigentlich nicht übersetzbar, weil es kein Name, kein Gedanke, keine Substanz oder Existenz ist. Die zentrale Bedeutung wird allgemein mit "Ich bin" oder "Ich bin da" wiedergegeben und weist auf die Unaussprechlichkeit, Unerklärlichkeit und Unergründlichkeit Gottes hin. JHWH wurde erstmals in Genesis 4,26 gebraucht, Mose beim brennenden Dornbusch in der Wüste offenbart (Exodus 3, 14) und kommt 6.823 mal in der hebräischen Bibel vor. Aus Scheu und Ehrfurcht vor JHWH, seiner Größe und Heiligkeit wurde JHWH von den Juden nach der babylonischen Gefangenschaft nicht mehr ausgesprochen, stattdessen wurden "adonay" (mein Herr) und heute "ha-schem" (der Name), "ha-makom" (der andere Ort oder die andere Seite) oder "adoschem" (Wortkombination von Herr und Name) gebraucht.

Die Personennamen umfassen die Bezeichnungen für Einzelwesen (Vornamen und Familiennamen).
Volksbezeichnungen und Stammesnamen sind keine Eigennamen, sondern Gattungsnamen.

Im deutschen Sprachraum hat sich seit dem 12. Jahrhundert ein zweigliedriges Namensystem mit einem Individualnamen (Vorname, Rufname, Nebenname) und Familiennamen (Beiname, Nachname, Zuname) entwickelt. In anderen Ländern gibt es verschiedene Namensysteme. So verwendet man beispielsweise in Russland ein dreigliedriges Namensystem mit einem Vatersnamen. Solche mehrgliedrigen Namensysteme sind seit der Antike bekannt. Die römischen Namen bestanden aus bis zu drei Elementen und gliederten sich in Vorname (Praenomen), Sippenname (Nomen Gentile) und Beiname (Agnomen, bzw. Cognomen). Letzterer hatte große Bedeutung, weil die antiken Römer nur sehr wenige Vornamen zur Auswahl hatten und Sippennamen ein Privileg der Oberschicht waren. Die verschiedenen Namensysteme und die dazu geltenden gesetzlichen Regelungen sind aufgelistet in: 

Ein auffälliges Merkmal, das speziell bei Familiennamen im deutschen Sprachraum zu beobachten ist, besteht darin, dass es eine Tendenz zur Variation (Diversifikation) ein und desselben Namens gibt. So existieren als sog. „adjektivische Übernamen“ neben „Lang“ auch die Formen „Lange“, „Langer“ und „Langen“, ein Phänomen, das bei vielen Namen auftritt. Bei Vornamen wiederum ist festzustellen, dass viele von ihnen Benennungsmoden unterliegen, die dafür sorgen, dass ein bestimmter Name häufiger wird, einen Höhepunkt der Beliebtheit erreicht und dann wieder an Häufigkeit verliert.

Siehe hierzu auch unter Namensrecht.

In der Kunst spielen Namen eine wichtige Rolle. Mit dem Eigen- oder Künstlernamen sich einen „Namen“ zu erarbeiten, ist eine der größten Motivationen der meisten Künstler. Mit der Bekanntheit des eigenen „Namens“ steigt häufig auch der Handelswert seiner angebotenen Kunst.

Salvador Dalí ging gegen Ende seiner Karriere spielerisch mit seinem Namen um. So signierte er weiße Blätter und verkaufte diese. Den Käufern stand es frei, ein Bild zu malen, das den Namen Dalí trug.

In literarischen und filmischen Werken tragen die handelnden Personen und die Orte, an denen die Handlung stattfindet, zuweilen fiktive sprechende Namen. Hierdurch sollen sie bereits durch ihre äußere Benennung ihrem inneren Wesen nach charakterisiert werden.

Das Stilmittel der sprechenden Namen ist bereits seit der Antike bekannt und findet sich ebenso in den mündlichen Überlieferungen von Mythen und Sagen in den verschiedensten Kulturkreisen.

In der klassischen allegorischen Literatur bedürfen die Namen meist keiner Interpretation seitens des Lesers. So trifft der Pilger in John Bunyans "The Pilgrim’s Progress" unter anderem den Riesen „Verzweiflung“, der Herr der „Burg des Zweifels“ ist. Ästhetisch reizvollere Codierungen erfolgen insbesondere durch Wortspiele, Buchstabendreher, Lautmalerei oder Akronyme.

Geographische Namen im eigentlichen Sinne sind immer Individualnamen. Geographische Klassennamen werden nicht zu den Geographika (Sg.: "Geographikum") gezählt.


Viele dieser Namen können aber auch zu den Mikrotyponymen gerechnet werden, wenn sie kleinere Objekte ansprechen.

Die Einteilung der Himmelskörper- und Sphärennamen erfolgt nach Planeten, Monden, Planetoiden, Sternen "(Astronyme)" und Sternbildern. Dabei wurden zunächst für jene Himmelskörper unseres Sternsystems Namen vergeben, die von der Erde aus sichtbar waren. Sie wurden durchweg nach Göttern aus Mythen benannt, wobei in der Antike jedes Volk seine eigenen Götternamen vergab. Heutzutage haben sich die Götternamen der Römer als Fachterminologie durchgesetzt. Die dazugehörigen Monde erhielten ebenfalls Bezeichnungen aus den Mythen. Diese wurden passend zu den Götternamen des Planeten, den sie umkreisten, vergeben.

Ab 1801 begannen sich die astronomischen Forschungen auszuweiten und die ersten Planetoiden wurden gesichtet. Diese erhielten zu Beginn die Namen der Göttinnen. Mit zunehmender Fülle der Kleinplaneten mussten jedoch irdische Frauennamen genutzt werden. Auch die Sternbilder haben in der Mehrzahl einen mythischen Hintergrund.

Anders hingegen verfuhr man bei Sternen. Bevor Koordinaten zu ihrer Unterscheidung genutzt wurden, erhielten sie meist arabische Namen nach ihren Entdeckern. Die Araber waren im 8. und 9. Jahrhundert führend in der Astronomie. Sternensysteme oder auch Nebel besitzen hingegen selten Eigennamen.

Nach Peter von Polenz unterteilen sich die Raumnamen in naturräumliche und politische Gebilde. Erstere umfassen Landschaften und Gebiete, letztere fest umgrenzte Räume (Bezirks- und Ländernamen). Bereits in frühester Zeit wurden Stammesbezeichnungen als Raumnamen verwendet. Erhaltene Beispiele sind etwa Niedersachsen (nach den Sachsen), Bayern (nach den Baiern), Thüringen (nach den Thüringern), Hessen (vermutlich nach den Chatten), Westfalen (nach den Westfalen) und Holstein (nach den Holsten). 

Die Gewässernamen sind die ältesten Zeugnisse unserer Sprache. Da insbesondere an den größten Flüssen die ersten Besiedlungen zu finden waren, wurden hier die ersten Gewässernamen vergeben. Daraus resultiert die Beobachtung, dass je größer ein Fluss, umso älter sein Name ist. Es wird zwischen stehenden und fließenden Gewässern unterschieden, wobei insbesondere die fließenden Gewässer im Mittelpunkt des Interesses der Forschung stehen.

In der Gewässernamenforschung haben Hans Krahe und dessen Schüler W. P. Schmidt die bis heute stark umstrittene Theorie der "Alteuropäischen Hydronymie" geprägt. Sie besagt, dass es Gewässernamen gibt, die außerhalb ihrer Einzelsprache in ganz Europa Entsprechungen besitzen, wodurch die Existenz einer voreinzelsprachlichen, indogermanischen Sprache belegt werden soll. Ein Beispiel hierfür ist die Isère in Südfrankreich – Iser/Jizera in Tschechien – Isar in Deutschland – IJssel in den Niederlanden.

Die Ortsnamenforschung beschäftigt sich im Besonderen mit den Strukturen, der Entstehung, dem Wandel und dem Verlust von Siedlungsbenennungen. Sie geht dabei diachronisch vor, indem sie von der heutigen Ortsnamenform ausgehend Belege sammelt und mit Hilfe derer versucht, den ursprünglichen Namen zu rekonstruieren. Anhand dieser Grundform kann die Etymologie des Namens bestimmt werden. Unterschieden wird dabei nach Bildungsweise der Orte und sprachlicher Zugehörigkeit.


Flurnamen sind sprachliche Zeichen, die der Orientierung im Raum, zur Identifizierung sowie Individualisierung von Objekten kleinerer landschaftlicher Einheiten dienen. Dazu gehören: Äcker, Wiesen, Spezialkulturen (Reben, Hanfgärten), Hecken, Wälder, Berge, Täler, Alpen, Felsen, Bäche, Flüsse, Seen, Quellen, Brunnen, Wege, Gassen, Grenzen und Gewerbeanlagen (Köhlereien, Mühlen, Stampfwerke). Flurnamen bezeichnen demnach die unbesiedelten Teile einer Landschaft. Aufgrund ihrer geringen kommunikativen Reichweite werden sie auch als Mikrotoponyme bezeichnet. In der Forschung ist umstritten, ob Objekte innerhalb von Siedlungen den Flurnamen zuzurechnen sind oder nicht. In Bezug auf Bildungsweise und sprachliche Zugehörigkeit sind sie den Ortsnamen vergleichbar.

Straßennamen (griech. "hodos" „Weg“) dienen der Orientierung in einer Stadt. Sie werden unterteilt in längliche Gebilde "(Prodonyme)" und Plätze "(Agoronyme)". Sie sind im Mittelalter entstanden. Seitdem werden sie oft zu politischen Zwecken genutzt, wobei man sich heute zunehmend um zeitlose Namen bemüht. Hierzu gibt es für die Kommunen Arbeitshilfen, vor allem, um Doppelbenennungen oder gleichklingende Namen zu vermeiden. Im Zuge dieser neuen Benennungssituation sind zunehmend hodonymische Felder entstanden. Das bedeutet, dass zusammenhängende Gebiete nach einheitlichen Gesichtspunkten benannt werden, bspw. nach Komponisten, Blumen oder Planeten.

Daneben gibt es auch Hodonyme, die zu den Makrotoponymen gezählt werden können, etwa "Via Appia" für die römische Fernverkehrsstraße oder "Brenner Autobahn", sowie für Eisenbahnstrecken wie "Transsibirische Eisenbahn".

Bevor im 18. Jahrhundert nach französischem Vorbild Hausnummern eingeführt wurden, waren die "Haus- und Hofnamen" ein wichtiges Unterscheidungsmerkmal in den Städten. Sie teilten die Städte in Viertel und trennten sie von den Vorstädten und Dörfern. Vor allem die öffentlichen Bauten erhielten Titel, sowie Kaufhäuser und Mietshäuser mit entsprechendem Standard. Nach dem Zweiten Weltkrieg ging die Verwendung von Haus- und Hofnamen stark zurück und ist nur noch in sehr ländlichen, wenig dicht besiedelten Gebieten üblich.

– immer Individualnamen –

– immer Individualnamen –

Bildungsstätten sind Prestigeobjekte, weswegen sie sich besonders zur Namenvergabe eignen. Meist sind die Namen aus einer Nominalgruppenstruktur gebildet, deren Kern ein appellativisches Nomen (Schule, Hochschule, Universität (→ "„Universitätsnamen“"), Institut) bildet. Durch Komposition werden diese mit Attribution oder asyndetischen Nachstellungen verbunden.

Die Namen der Bildungseinrichtungen beziehen sich meist auf den Gründer/Stifter, den (finanziellen) Förderer oder auf Patrone, das heißt Personen, die keine direkte Beziehung zu der Bildungseinrichtung haben, aber trotzdem ein besonderes Prestige besitzen. Ziel dieser Namengebung ist, die betreffende Persönlichkeit zu ehren, an sie zu erinnern oder ihre Vorbildfunktion zu betonen.

Demgegenüber gibt es Namen, die nur einen intensionalen Gehalt besitzen. Sie teilen lediglich sachliche Informationen mit, um entweder die Institution, die fachliche Spezifikation oder die Trägerschaft zu kennzeichnen. Dabei soll absichtlich auf (selbst)bewertende Informationen verzichtet werden.

In Deutschland muss der potentielle Prestigegehalt in sinnvollem Verhältnis zur Wichtigkeit der Institution stehen. Dementsprechend tragen Bildungseinrichtungen mit hohem Prestige keine Patronatsnamen, dagegen beispielsweise Grund- und Hauptschulen meist Namen mit regionaler Bedeutung.


– immer Gattungsnamen, erst die Seriennummer ist der Individualname –
Zu den Warennamen gehören Artikelnamen, Markennamen und Firmennamen. Sie müssen bestimmte Informationen an den Käufer übermitteln, sei es über den Hersteller, den Herstellungsort, den Stoff des Produkts, die Eigenschaften des Produkts, den Verwendungszweck oder die Wirkungsweise.

Sie sind insofern eine besondere Gruppe, als dass die Namen zu Gattungsbegriffen werden können. Als Beispiel sei hier das Taschentuch von Tempo genannt, das ursprünglich die herstellende Firma betitelte und nun im alltäglichen Sprachgebrauch das Papiertaschentuch betitelt.

Die Warennamen sind markenrechtlich geschützt. Die gesetzlichen Bestimmungen dazu, sind nachzulesen in:

Die Namen für die verschiedenen Medien sind durch gesetzliche Regeln definiert und geschützt. Mit der Firma wird ein bestimmtes Unternehmen namentlich gekennzeichnet und somit individualisiert.

Sie sind die Namen, die am stärksten Modeerscheinungen unterworfen sind. Die Marketinganforderungen an einen Mediennamen sind: 
Entsprechende gesetzliche Bestimmungen finden sich bei:





Tiere, zu denen Menschen eine besondere Beziehung aufbauen, erhalten sehr oft Namen, die menschlichen Namen entsprechen. In Märchen, aber auch in Erzählungen der Neuzeit, haben Tiere oft Namen.

Dagegen ist die schon seit der Antike nachweisbare von Carl von Linné in unserer heutigen Form eingeführte Nomenklatur nicht im engeren Sinn „Name“, sondern bezieht sich meist auf Eigenschaften, die in die Gattungsnamen eingearbeitet werden. Diese Eigenschaften können sowohl Organe, Aussehen als auch Fundorte beschreiben. Des Weiteren werden auch teilweise Personen gewürdigt, die die Art als erste beschrieben haben bzw. geehrt werden sollen. Deutsche Tier- und Pflanzennamen werden nach ähnlichen Gesichtspunkten gebildet, beruhen jedoch nicht grundsätzlich auf Binominalität.

Eine weitere Form der Tier- und Pflanzennamen sind Benennungen von Zuchtformen, also von willentlich erzeugten Kreuzungen bzw. Hybriden von Tieren und Pflanzen. Die Namensgebung erfolgt meist durch die Züchter, die ihre Erzeugnisse oft nach sich selbst oder dem Zuchtort benennen. Allerdings werden vor allem Zuchtpflanzen häufig nach berühmten Persönlichkeiten (z. B. Herrschern) benannt.

Die meteorologischen Namen wurden von Clement Lindley Wragge geprägt. Er vergab als erster weibliche Vornamen für tropische Wirbelwinde und männliche für außertropische Tiefdruckwirbel mit Sturm- und Orkanfeldern. Bekanntermaßen verwendete er mit Vorliebe die Namen ihm missfälliger Politiker.

Aber erst in den 1950er Jahren setzte sich die Benennung für meteorologische Erscheinungen durch. Dies resultierte daraus, dass für die Jagdflugzeuge des amerikanischen Militärs eine Umschreibung der Phänomene zu lange dauerte. Zuerst wurden dabei die Unterteilungen von Clement L. Wragge nach männlichen und weiblichen Vornamen beibehalten. Als aber wiederholte Beschwerden an die Öffentlichkeit kamen, beschloss man 1979, beide Geschlechter gleichmäßig mit den Namen der Wirbelstürme zu belasten. Dazu wurde 1979 eine offizielle Liste der IHC herausgegeben. Diese Namensliste ist bis heute Grundlage für die Bezeichnungen tropischer Wirbelstürme. In Deutschland werden seit den 1950er Jahren Hoch- und Tiefdruckgebiete benannt. Diese Liste wurde ursprünglich von Richard Scherhag geschrieben, heute werden die Namen im Rahmen der Aktion Wetterpate von der Technischen Universität in Berlin versteigert. Diese Namen werden inzwischen bei Starkwindereignissen von den Medien auch außerhalb Deutschlands verwendet. Eine Benennung von Tiefdruckgebieten erfolgt auch beim norwegischen Wetterdienst.

Auch Gegenstände hatten oder haben Eigennamen; dies ist in verschiedenen Kulturen üblich. So führten etwa bei den Germanen einzelne Schwerter (z. B. Siegfrieds Schwert „Balmung“) oder Helme Namen, heute noch Autos (z. B. die „Grüne Minna“), Schiffe, Eisenbahnzüge, Puppen oder Spieltiere.

In vielen Fällen und aus unterschiedlichen Motiven heraus werden die echten Namen unkenntlich gemacht. Dies kann dazu dienen, eine Person anonym zu lassen (Kryptonym, Pseudonym) – wie im Falle von „Peter Panter“ für „Kurt Tucholsky“ – oder auch eine erwünschte Wirkung, z. B. in der Werbung, zu erzielen, indem eine einprägsame Bezeichnung gebildet wird (z. B. „Haribo“ für „Hans Riegel, Bonn“). Kryptonyme werden auch genutzt, um Aktionen oder Projekte mit einem „verdeckten“, doch einprägsamen Wort zu benennen.






</doc>
<doc id="3512" url="https://de.wikipedia.org/wiki?curid=3512" title="Netz">
Netz

Netz steht für:


Jagd:

Verkehr:

Energie- und Wasserversorgung:

Kommunikationstechnik:

Informatik:

Mathematik:

Biologie und Anatomie:

Sport:

Schutznetze:

Sonstige Netze:

Orte:
Siehe auch:



</doc>
<doc id="3513" url="https://de.wikipedia.org/wiki?curid=3513" title="Nordrhein-Westfalen">
Nordrhein-Westfalen

Nordrhein-Westfalen (, , "NW", Abkürzung "NRW") ist ein Land im Westen der Bundesrepublik Deutschland. Die Landeshauptstadt ist Düsseldorf, die einwohnerreichste Stadt ist Köln.

Nordrhein-Westfalen grenzt im Norden und Nordosten an Niedersachsen, im Südosten an Hessen, im Süden an Rheinland-Pfalz sowie im Westen an die belgische Provinz Lüttich sowie an die niederländischen Provinzen Limburg, Gelderland und Overijssel. Mit rund Millionen Einwohnern ist Nordrhein-Westfalen das bevölkerungsreichste, flächenbezogen mit rund 34.100 Quadratkilometern das viertgrößte deutsche Land. 29 der 79 deutschen Großstädte liegen in seinem stark urbanisierten Gebiet. Die Metropolregion Rhein-Ruhr im Zentrum des Landes ist mit rund zehn Millionen Bewohnern einer der 30 größten Ballungsräume der Welt und zentraler Teil der am stärksten verdichteten Region Europas, der „Blauen Banane“.

Den nördlichen Teil des Ballungsraums Rhein-Ruhr bildet das stark verstädterte Ruhrgebiet mit den Zentren Dortmund, Essen, Duisburg und Bochum. Dessen wirtschaftlicher Aufstieg basierte im frühen 19. Jahrhundert auf der Industrialisierung und der Montanindustrie, insbesondere dem Erz- und Kohlebergbau. Seit dem Rückgang des Bergbaus ab den 1960er Jahren vollzieht sich dort ein bis heute andauernder Strukturwandel hin zu einer Dienstleistungs- und Technologiewirtschaft, welcher durch subventionierte Großprojekte wie RUHR.2010 – Kulturhauptstadt Europas begleitet wird. Bis heute ist die Wirtschaftsstruktur Nordrhein-Westfalens durch altindustrielle Schlüsselindustrien geprägt, jedoch existiert im Ganzen eine diversifizierte Wirtschaft. Mit einem Anteil von rund 22 Prozent am deutschen Bruttoinlandsprodukt ist Nordrhein-Westfalen das Land mit der höchsten Wirtschaftsleistung.

Das Land Nordrhein-Westfalen wurde 1946 von der britischen Besatzungsmacht aus der preußischen Provinz Westfalen und dem Nordteil der ebenfalls preußischen Rheinprovinz (Nordrhein) errichtet und 1947 um das Land Lippe erweitert. Seit 1949 ist es ein Land der Bundesrepublik Deutschland. Bis 1999 war Bonn der Regierungssitz der Bundesrepublik. Die Bundesministerien behielten Dienstsitze in Bonn, dem das Berlin/Bonn-Gesetz bedeutende Regierungsfunktionen als Bundesstadt garantiert.

Kulturell ist Nordrhein-Westfalen kein einheitlicher Raum; besonders zwischen dem Landesteil Rheinland einerseits und den Landesteilen Westfalen und Lippe andererseits bestehen deutliche Unterschiede. Bedeutende Bildungs- und Forschungsstätten im Land sind 14 öffentlich-rechtliche Universitäten, 16 öffentlich-rechtliche Fachhochschulen, sieben staatliche Kunst- und Musikhochschulen, 28 anerkannte private und kirchliche Hochschulen mit Hauptsitz in Nordrhein-Westfalen sowie fünf Verwaltungshochschulen, das Forschungszentrum Jülich, die NRW Graduate Schools und das DLR (Deutsches Zentrum für Luft- und Raumfahrt).

Nordrhein-Westfalen liegt im Westen der Bundesrepublik Deutschland und grenzt im Uhrzeigersinn an Niedersachsen, Hessen, Rheinland-Pfalz, Belgien und die Niederlande. Die Ausdehnung des Landes beträgt von Südwest bis Nordost rund 260 Kilometer. Der Norden des Landes liegt in der Norddeutschen Tiefebene und gliedert sich grob in die Westfälische Bucht, die von den Flüssen Lippe, Ems und Ruhr durchflossen wird, und das Niederrheinische Tiefland beidseitig des Rheins, der der größte Fluss des Landes (und zugleich Deutschlands) ist. Der tiefste Punkt liegt bei im Nordwesten des Landes. Die restlichen Gebiete des Landes haben Anteil an den deutschen Mittelgebirgsregionen. Das Weserbergland an der oberen Weser prägt den Osten des Landes. Die Gebirge des Rheinischen Schiefergebirges nehmen den Süden ein. Das Rheinische Schiefergebirge gliedert sich grob in die linksrheinische Eifel im Südwesten und Bergisches Land sowie Sauerland östlich des Rheins. Der Langenberg im zum Sauerland zählenden Rothaargebirge ist mit der höchste Berg des Landes. Der geographische Mittelpunkt des Landes liegt in Dortmund in der Aplerbecker Mark; bei Selfkant liegt der westlichste Punkt Nordrhein-Westfalens und gleichzeitig Deutschlands.

Das Klima Nordrhein-Westfalens weist relativ ausgeglichene Temperatur- und Niederschlagsgänge auf. Die mittleren Jahrestemperaturen betragen abhängig von der Höhenlage zwischen 5 °C und 10 °C. Die jährlichen Niederschläge liegen zwischen 600 Millimeter im Tiefland und 1400 Millimeter in den Mittelgebirgen.

Kraft Besatzungsrechts gründete die Besatzungsmacht Großbritannien das Land Nordrhein-Westfalen am 23. August 1946 aus dem Nordteil der preußischen Rheinprovinz (Provinz Nordrhein) sowie der ebenfalls preußischen Provinz Westfalen. Die politische Vorbereitung dafür lief unter dem Codewort "Operation Marriage". Nach der Eingliederung des jahrhundertelang selbstständigen Landes Lippe im Jahr 1947 war der heutige territoriale Zuschnitt erreicht.

Auf seinem Staatsgebiet ist Nordrhein-Westfalen in die Rechtsnachfolge des Freistaats Preußen und des Landes Lippe eingetreten. Im Gegensatz zu einigen anderen deutschen Ländern gab es für Nordrhein-Westfalen als Ganzes keinen stark identitätsstiftenden Vorgängerstaat. Bei der Staatsgründung Nordrhein-Westfalens stand auch nicht der Gedanke einer Zusammenführung homogener Gebiete, sondern der Wunsch der britischen Regierung im Vordergrund, das Ruhrgebiet und seine bedeutenden industriellen Ressourcen als Ganzes in ein Land einzubetten.

1949 gründete Nordrhein-Westfalen gemeinsam mit anderen Ländern die Bundesrepublik Deutschland. Größte Herausforderungen in der Nachkriegszeit waren der Wiederaufbau des kriegszerstörten Landes und der Aufbau eines demokratischen Staatswesens. Insbesondere als Folge des Niedergangs der Montanindustrie in Folge von Kohle- und Stahlkrisen sowie des Trends zur Tertiarisierung entwickelte sich die Gestaltung des notwendigen Strukturwandels zu einem zentralen Thema der Landespolitik.

Zwischen 1966 und 1975 kam es zu einer kommunalen Gebietsreform, wodurch die Anzahl der Kreisfreien Städte, Kreisangehörigen Gemeinden und Kreise verringert wurde. Die Anzahl der Regierungsbezirke wurde von 6 auf 5 Bezirke verringert. Der ehemalige Regierungsbezirk Aachen wurde mit dem Regierungsbezirk Köln zum neuen Regierungsbezirk Köln zusammengelegt.

Bonn war von 1949 bis 1990 Hauptstadt, noch bis 1999 Regierungssitz der Bundesrepublik Deutschland und beherbergt auch heute noch zahlreiche Bundesbehörden.

Nordrhein-Westfalen ist gemäß dem Grundgesetz für die Bundesrepublik Deutschland und der Verfassung für das Land Nordrhein-Westfalen ein Land der Bundesrepublik Deutschland. Gemäß seiner Verfassung ist das Land nach den Grundsätzen eines republikanischen, sozialen und demokratischen Rechtsstaats organisiert. Die Legislative liegt maßgeblich beim vom Wahlvolk für die Dauer von fünf Jahren gewählten Landtag. Elemente direkter Demokratie spielen in der Praxis kaum eine Rolle. Seit 2017 tragen CDU und FDP die Landesregierung. Die Exekutive, welche parlamentsgebunden ist, wird vom Ministerpräsidenten geleitet – seit dem 27. Juni 2017 von Armin Laschet (CDU). Die Landesverwaltung ist der Landesregierung nachgeordnet. Die Bezirksregierungen für die Regierungsbezirke Düsseldorf, Köln, Münster, Detmold und Arnsberg sind eine mittlere Ebene der Landesverwaltung. Die Landesverfassungsgerichtsbarkeit liegt beim Verfassungsgerichtshof für das Land Nordrhein-Westfalen. Aufgrund seiner Teilsouveränität kann Nordrhein-Westfalen als beschränktes staatliches Völkerrechtssubjekt auf bestimmten Gebieten völkerrechtliche Verträge abschließen. Über den Bundesrat wirkt das Land an der bundesstaatlichen Gesetzgebung und an Angelegenheiten der Europäischen Union mit. Bonn ist als Bundesstadt Sitz zahlreicher Einrichtungen des Bundes, außerdem Standort verschiedener Organisationen der Vereinten Nationen.

Die Städte und Gemeinden des Landes haben das Recht zur kommunalen Selbstverwaltung und übernehmen Aufgaben der Landesverwaltung im Auftrag des Landes oder im Wege der Organleihe. Auf kommunaler Ebene gliedert sich das Land Nordrhein-Westfalen in 30 Kreise und eine Städteregion. Die Kreise und die Städteregion umfassen 374 kreisangehörige (regionsangehörige) Gemeinden und Städte. Daneben bestehen in Nordrhein-Westfalen 22 kreisfreie Städte. Die Stadt Aachen ist sowohl kreisfrei als auch Teil der Städteregion Aachen. Insgesamt gliedert sich das Land damit in 396 Gemeinden. Das Land hat 29 Großstädte. Besondere Bedeutung im kulturellen und sozialen Bereich sowie für weitere Aufgaben der Kommunalverwaltung haben die beiden Landschaftsverbände in Nordrhein-Westfalen, sowie weitere Kommunalverbände wie etwa der Regionalverband Ruhr oder der Landesverband Lippe. Nachfolgend die Gliederung des Landes in Regierungsbezirke und zugehörige Kreise und kreisfreie Städte (kreisfreie Städte kursiv):

Das Land Nordrhein-Westfalen hat (Stand ) Einwohner und ist damit das bevölkerungsreichste deutsche Land. Zugleich ist das Gebiet mit einer Bevölkerungsdichte von /34088.31 round 0 Einwohnern pro Quadratkilometer unter den Flächenländern das mit Abstand am dichtesten besiedelte Land. Die Bevölkerung ist dabei jedoch recht ungleich verteilt. Eher dünn besiedelt sind das Münsterland, das Tecklenburger Land, die eher bergigen Regionen im Süden des Landes sowie Teile Ostwestfalen-Lippes. Die Metropolregion Rhein-Ruhr ist mit rund zehn Millionen Einwohnern eine der am dichtesten besiedelten und bevölkerungsreichsten Regionen Europas. Folgende Tabelle zeigt die Bevölkerungsentwicklung des Landes.

2006 beträgt der Anteil der Über-65-Jährigen rund 19,7 Prozent (1950: 8,8 Prozent). Der Anteil der Unter-15-Jährigen beträgt 2006 noch 14,7 Prozent (1950: 22,6 Prozent).

2006 gab es im Land rund 8,5 Millionen Haushalte. Die durchschnittliche Haushaltsgröße beträgt demnach 2,12 Personen. In 37 Prozent dieser Haushalte wohnt allerdings nur eine Person. Noch 1950 betrug die durchschnittliche Haushaltsgröße über 3 Personen. Der Anteil der Einpersonenhaushalte betrug 1950 nur 16,9 Prozent. Der Anteil der Alleinstehenden ist noch höher als der Anteil der Einpersonenhaushalte. 2006 waren rund 39 Prozent der Bevölkerung alleinstehend. In einer „klassischen“ Paarbeziehung mit Kindern leben nur rund ein Viertel der Nordrhein-Westfalen.

2006 hatte Nordrhein-Westfalen mit 1,36 Kindern je Frau die dritthöchste Fertilitätsrate unter den Ländern Deutschlands. 2007 stieg dieser Wert auf 1,39. Jedoch gibt es starke regionale Unterschiede. Vergleichsweise hohe Werte von über 1,5 finden sich im Norden und Osten des Landes. Den Spitzenplatz nimmt der Kreis Lippe mit 1,61 (2007) ein. Niedrige Werte finden sich im Ruhrgebiet, z. B. Bochum mit 1,15 oder Dortmund mit 1,33. Den höchsten Wert im Ruhrgebiet hat Duisburg mit 1,45. Remscheid ist die kreisfreie Stadt in Nordrhein-Westfalen mit der höchsten Fertilitätsrate, 1,56. Sehr niedrige Werte weisen die Landeshauptstadt Düsseldorf mit 1,29 und die bevölkerungsreichste Stadt Köln mit 1,26 auf.

2014 betrug der Anteil der Einwohner mit Migrationshintergrund (registrierte Einwohner mit nicht deutscher Staatsangehörigkeit oder außerhalb Deutschland geborene und seit 1950 zugewanderte Personen sowie deren Kinder) 23,6 Prozent. Der Anteil der Ausländer (registrierte Einwohner ohne deutsche Staatsangehörigkeit) an der Gesamtbevölkerung betrug am 31. Dezember 2006 10,6 Prozent.

Verschiedenen Prognosen zufolge wird die Bevölkerung Nordrhein-Westfalens in den kommenden Jahrzehnten deutlich schrumpfen. Die Bevölkerungsvorausberechnung der Statistikbehörde des Landes sagt eine Gesamtbevölkerung von nur noch 16,16 Millionen für das Jahr 2050 voraus. Der Anteil der Über-65-Jährigen an der Gesamtbevölkerung wird der Prognose nach bis 2050 auf fast 30 Prozent steigen. Das Statistische Bundesamt Deutschland (DESTATIS) prognostiziert für NRW einen Bevölkerungsrückgang von etwa einer Million Menschen alle 20 Jahre. Die Bertelsmann Stiftung hat im Jahr 2011 die Prognose „Wegweiser Kommune“ veröffentlicht. Bis 2030 wird ein Bevölkerungsrückgang um ca. eine Million Einwohner prognostiziert.

Lt. der Prognose von IT.NRW aus 3/2016 geht der Bevölkerungsrückgang einher mit dem Rückgang der Erwerbspersonen für den Zeitraum 2014 bis 2040. Man rechnet mit einem Rückgang um rd. 7,9 % oder 693.000 Erwerbspersonen (bei den Männern um 5,2 % oder 248.000 von 4.739.000 auf 4.491.000 bzw. bei den Frauen um 11,1 % oder 445.000 von 4.013.000 auf 3.568.000).

Die Ergebnisse dieser Bevölkerungsprognosen fassen folgende Tabellen zusammen:

Nordrhein-Westfalen wird als ein „Bindestrich-Land“ bezeichnet, weil es unter dem maßgeblichen Einfluss der Besatzungsmacht Großbritannien in den Jahren 1946 und 1947 aus drei historisch unterschiedlichen Landesteilen gebildet wurde und seither noch keine besonders starke Identifikation mit dem Land entstanden ist. Kulturelle Überlegungen traten bei der Staatsgründung hinter geopolitischen und wirtschaftlichen Überlegungen, das Ruhrgebiet als Ganzes einzubetten, zurück. Wirtschaftlich waren die Räume des Landes bereits früh, teilweise eng, miteinander verflochten.

Die Bewohner der drei Landesteile Westfalen, Lippe und Rheinland haben seit 1946 durchaus eine gewisse Identifikation mit dem Land entwickelt. Besonders seit den 1980er Jahren war ein Durchbruch zu einer deutlichen Landesidentität zu beobachten. Allerdings tritt dieses Bewusstsein häufig noch gegenüber dem Bewusstsein einer Zugehörigkeit zu einem Landesteil Nordrhein-Westfalens zurück. Trennend wirken vor allem die historischen, kulturellen und sprachlichen Unterschiede zwischen dem Rheinland und Westfalen-Lippe. Große Teile von Westfalen-Lippe, vor allem im Norden und Osten, fühlen sich kulturräumlich und historisch den angrenzenden Regionen Niedersachsens verbunden. Aber auch innerhalb der anderen Regionen Nordrhein-Westfalens sind weitere ausgeprägte Regionalidentitäten vorhanden, die sich oftmals auf die in Mittelalter und Neuzeit gebildeten Territorien, ihre Herrscherhäuser und die damit einhergehenden konfessionellen Bindungen zurückführen lassen. Die heute 586,9 km lange rheinisch-westfälische Binnengrenze zwischen den ehemaligen preußischen Provinzen innerhalb des heutigen Landes wird über die preußische Zeit hinweg als uralte Kulturgrenze angesehen, die sich bis auf die fränkisch-sächsische Stammesgrenze des 8. Jahrhunderts zurückverfolgen lässt. Das für so ein kleines Gebiet bemerkenswert ausgeprägte Regionalbewusstsein der Lipper speist sich vor allem aus seiner langen Eigenständigkeit, denn kulturräumlich ist das Lipperland kaum vom restlichen Westfalen, insbesondere dem angrenzenden Minden-Ravensberg abzugrenzen. Die Identität der Rheinländer greift in ihrem Vorstellungsraum ebenfalls über die Grenzen Nordrhein-Westfalens hinaus und bezieht sich entlang des Rheins meist auf einen historischen Kulturraum, der in etwa der ehemaligen Rheinprovinz entspricht, also auch Teile des heutigen Rheinland-Pfalz und Randgebiete Ostbelgiens einbezieht (→ "Rheinlande"). Kennzeichnend für das Selbstbild der Rheinländer ist ferner ihr Bewusstsein für die Verflechtung mit und die Nachbarschaft zu Westeuropa. Im 19. Jahrhundert pflegten viele Bewohner des späteren Nordrhein-Westfalen als „Musspreußen“ weitgehend ein eher distanziertes Verhältnis zu Preußen. Zum Ausdruck kam dies etwa während der Kölner Wirren, die in der Verhaftung des Kölner Erzbischofs Clemens August Droste zu Vischering gipfelten und die die Entstehung eines politischen Katholizismus im Rheinland und in Westfalen förderten, des Weiteren im Köln-Düsseldorfer Verbrüderungsfest im Sommer des Jahres 1843, als rheinische Bürger sich für die Beibehaltung ihres Rheinischen Rechts engagierten. Die Kluft vergrößerte sich für viele durch das Scheitern der Märzrevolution und durch den Kulturkampf, die repressiven Maßnahmen gegen die Bindung vieler Rheinländer und Westfalen an die römisch-katholische Kirche und an den Ultramontanismus. Durch die Gegensätze des im „fernen Berlin“ zentral gesteuerten preußischen Staats wurde die rheinische und westfälische Identitätsentwicklung gefördert. Großen Einfluss hatte ferner, dass Preußen die Rheinländer, die bis dahin auf verschiedene Herrschaftsgebiete verteilt gewesen waren, nicht nur in der Rheinprovinz vereinigte, sondern auch die für das rheinische Selbstbild prägende Rheinromantik förderte, vor allem im Bereich Mittelrhein. Entsprechendes lässt sich über die Westfalen sagen, die ebenfalls im 19. Jahrhundert durch Preußen in der Provinz Westfalen vereinigt wurden.

Bei den Landesbewohnern mit ausländischer Migrationsgeschichte, deren Anteil gerade in den verstädterten und industrialisierten Regionen nicht gering ist, kommt als identitätsbildender Faktor zur Geltung, dass ihre Kultur und Identität durch die Lebenswelt ihrer unterschiedlichen Herkunftskulturen mitgeprägt wird, etwa bei sogenannten "Ruhrpolen", Aussiedlern und Spätaussiedlern, Türkeistämmigen und ihren Nachfahren. Nicht gering ist zudem der Anteil jener Landesbewohner, die aus anderen Bundesländern oder als sogenannte "Heimatvertriebene" zugezogen sind und so eine in anderen deutschen oder vormals deutschen Landstrichen vorgeprägte Identität „mitgebracht“ haben. Das Merkmal der Mischung und Aufnahme verschiedener Menschen, Kulturen und Traditionen im Rheinland hatte den Schriftsteller Carl Zuckmayer 1946 – also schon lange vor der Arbeitsmigration der sogenannten "Gastarbeiter" – zu seiner Metapher vom Rhein als der „Völkermühle Europas“ geführt.

Die alten Gegensätze zwischen den Landesteilen verblassen nur langsam. Die beiden Landschaftsverbände für Westfalen-Lippe und das Rheinland sind mit der regionalen Kulturpflege beauftragt und institutionalisieren gewissermaßen so die kulturräumliche Trennung des Landes. Mit Ostwestfalen-Lippe und insbesondere dem industriekulturell geprägten Ruhrgebiet sind aber mittlerweile auch landesteilübergreifende und neue identitätsstiftende Regionen entstanden. Durch den Prozess der europäischen Integration hat sich außerdem als eine weitere Identitätsebene – oberhalb des vorherrschenden Selbstbildes der Nordrhein-Westfalen als Deutsche – eine europäische Identität stärker ausgeprägt.

Über 60 Jahre nach der Gründung Nordrhein-Westfalens ist insgesamt festzustellen, dass das Landesbewusstsein, gefördert durch Veröffentlichungen, Veranstaltungen (z. B. durch den Nordrhein-Westfalen-Tag), Auszeichnungen, Hoheitszeichen und Staatssymbole, Museen mit landesspezifischen Themen, den Bau und die Präsenz repräsentativer staatlicher Gebäude sowie durch Institutionen wie z. B. den Westdeutschen Rundfunk, sich entwickelt und dass die Identität der Landesbewohner sich räumlich zunehmend an den Landesgrenzen orientiert. In unterschiedlicher Weise und Intensität haben die jeweiligen Ministerpräsidenten und Landesregierungen die nordrhein-westfälische Identität der Landesbewohner zu fördern und zu nutzen versucht, etwa Franz Meyers (Gründung der Kunstsammlung Nordrhein-Westfalen), Johannes Rau (Wahlslogan "Wir in NRW", später Name eines Blogs) und Jürgen Rüttgers (Slogan "Wir im Westen", später Stiftung des Nordrhein-Westfalen-Tages). Von einem besonders starken allgemeinen Landesbewusstsein ist gleichwohl noch nicht zu sprechen.

Größte religiöse Gruppe sind die römisch-katholischen Christen mit rund 39,3 Prozent der Gesamtbevölkerung im Jahr 2015. Zweitgrößte Gruppe sind evangelische Gläubige mit 28,5 Prozent beim Zensus 2011 (heute vermutlich niedriger, aber keine neueren Daten bekannt). Der Anteil der Konfessionslosen beträgt in etwa 25 Prozent. Evangelische Freikirchen stellen 1,1 Prozent und Orthodoxe 1,5 Prozent der Einwohner. Die größte nichtchristliche Bevölkerungsgruppe mit einem religiösen Bekenntnis bilden heute Muslime, die überwiegend der sunnitischen Glaubensrichtung des Islam angehören. Daneben gibt es auch Aleviten und andere Gruppen. Juden machen etwa 0,1 Prozent aus. Die Landesverfassung erkennt den Kirchen eine besondere gesellschaftliche Rolle zu, insbesondere in Bereichen des Bildungswesens, und garantiert das Recht auf freie Ausübung einer Religion.

Umgangs- und Amtssprache ist Deutsch. Die Dialekte und Sprachen in Nordrhein-Westfalen variieren wegen der kulturräumlichen Uneinheitlichkeit des Landesgebiets. Im rheinischen Landesteil finden sich überwiegend fränkische Dialekte, in Westfalen und Lippe vor allem westfälische Regiolekte des Hochdeutschen, die ihre Wurzeln in westfälischen Dialekten des Niederdeutschen haben. Das unter dem Einfluss von Immigration entstandene Ruhrdeutsch ist ein Beispiel für einen Regiolekt. Durch Sprachunterricht und Einwanderung hat sich die Mehrsprachigkeit der Bewohner im Laufe der letzten Jahrzehnte beträchtlich erhöht.

Nordrhein-Westfalen bzw. das Gebiet des heutigen Landes hat zahlreiche bedeutende Persönlichkeiten hervorgebracht. Mit an der Spitze internationaler Bekanntheit dürften der Komponist Ludwig van Beethoven, der Maler Peter Paul Rubens und der Rennfahrer Michael Schumacher stehen. Allein fünf Bundespräsidenten wurden im späteren Nordrhein-Westfalen geboren: Gustav Heinemann, Heinrich Lübke, Johannes Rau, Walter Scheel, Frank-Walter Steinmeier. Mit Konrad Adenauer und Gerhard Schröder stammen auch zwei Bundeskanzler aus Nordrhein-Westfalen. Bekannte weitere Persönlichkeiten sind Otto III., Friedrich Engels, Joseph Beuys, Heinrich Böll, Annette von Droste-Hülshoff, Heinrich Heine, Wilhelm Conrad Röntgen und Alfred Krupp.

Das nordrhein-westfälische Schulsystem sieht nach einer vierjährigen Regelzeit auf der Grundschule den Besuch einer der weiterführenden Schulformen Hauptschule, Realschule, Gymnasium, Gesamtschule oder Sekundarschule vor. Höchste schulische Abschlussprüfung ist die Allgemeine Hochschulreife, die nach achtjähriger Regelschulzeit auf einer weiterführenden Schule als Zentralabitur abgelegt wird.

In den 1950er Jahren gab es in Nordrhein-Westfalen nur wenige Universitäten, darunter die Westfälische Wilhelms-Universität in Münster, die Universität zu Köln und die Rheinische Friedrich-Wilhelms-Universität Bonn. Seit den 1960er Jahren wurden vermehrt neue Hochschulen gegründet. An den nordrhein-westfälischen Hochschulen sind im Wintersemester 2014/2015 mehr als 700.000 Studenten eingeschrieben. Die Universitäten Bochum, Duisburg-Essen, Hagen, Köln und Münster sind Universitäten mit den größten Studierendenzahlen des Landes und gehören zu den zehn größten Universitäten in Deutschland. Insgesamt hat das Land (Stand 2015) inklusive der Fachhochschulen 30 öffentliche Hochschulen, sieben staatliche Kunst- und Musikhochschulen, 30 anerkannte private und kirchliche Hochschulen sowie fünf Fachhochschulen, die nicht der Dienst- und Fachaufsicht des Landes unterliegen.

Neben den sonstigen Einrichtungen der Helmholtz-Gemeinschaft, der Max-Planck-Gesellschaft und der Fraunhofer-Gesellschaft ist vor allem das Forschungszentrum Jülich als eine der größten Forschungseinrichtungen Europas bekannt. Die NRW Graduate Schools sind Einrichtungen der Spitzenforschung innerhalb der bestehenden Universitäten.

"Land von Kohle und Stahl" war in den 1950er und 1960er Jahren eine durchaus treffende Selbst- und Fremdbeschreibung für Nordrhein-Westfalen. Der montanindustriell geprägte Raum an Rhein und Ruhr war eine der wichtigsten Industrieregionen Europas und trug zum Wiederaufbau und zum Wirtschaftswunder nicht nur im Land, sondern in der gesamten Bundesrepublik entscheidend bei. Seine Zukunft war nach dem Zweiten Weltkrieg aufgrund der Ruhrfrage zunächst international politisch umstritten gewesen. Über das Ruhrstatut und den Schuman-Plan wurde ab Ende der 1940er Jahre der Weg hin zu einer europäischen Vergemeinschaftung des Montansektors in Gestalt der Europäischen Gemeinschaft für Kohle und Stahl beschritten. Spätestens seit den 1960er Jahren machte sich die starke Ausrichtung des Ruhrgebiets auf die Montanindustrie dann als Monostruktur negativ bemerkbar. Immer wiederkehrende Stahl- und Kohlekrisen ließen den montanindustriellen Sektor immer mehr zusammenschmelzen. Dagegen erlebten im produzierenden Sektor die Unternehmen jenseits des Ruhrgebiets, insbesondere im Maschinenbau sowie in der metall- und eisenverarbeitenden Industrie, einen erheblichen Aufschwung. Der Strukturwandel fiel in Nordrhein-Westfalen regional sehr unterschiedlich aus. Während immer noch Teile des alten Reviers hohe Arbeitslosenzahlen aufweisen, hat sich ein Strukturwandel von der überwiegend industriell geprägten Gesellschaft zur Wissensgesellschaft vollzogen.

Trotz der Strukturprobleme und des jahrelangen unterdurchschnittlichen Wirtschaftswachstums war das Land mit einem Bruttoinlandsprodukt (BIP) von 624,7 Milliarden Euro im Jahr 2014 das wirtschaftsstärkste Land Deutschlands und eines der wichtigsten Wirtschaftszentren der Welt. Das Land erwirtschaftete 2014 rund 21,5 Prozent der deutschen Wirtschaftsleistung. Bei der Betrachtung des Bruttoinlandsprodukts pro Einwohner liegt Nordrhein-Westfalen im Mittelfeld der Länder Westdeutschlands. Die Arbeitslosenquote beträgt . Dieser Wert liegt über der bundesdeutschen und deutlich über der westdeutschen Arbeitslosenquote. Die nordrhein-westfälische Arbeitslosenquote ist die höchste aller westdeutschen Flächenländer.

Nach Erhebungen des amerikanischen Wirtschaftsmagazins Fortune haben vier der hundert umsatzstärksten Unternehmen der Welt ihren Sitz in Nordrhein-Westfalen. Einer Erhebung des Wirtschaftsblattes zufolge waren 2009 in Nordrhein-Westfalen die zehn umsatzstärksten Unternehmen E.ON (Versorger), Metro (Einzelhandel), Deutsche Telekom (Telekommunikation), Aldi Nord und Aldi Süd (Einzelhandel), Rewe (Einzelhandel), RWE (Versorger), Deutsche Post AG (Logistik und Transport), ThyssenKrupp (Maschinen- und Anlagenbau), Deutsche BP (Versorger) und die Bayer AG (Pharma und Chemie). Deutschlandweit kann das Land mit rund 135 Milliarden Euro (rund 29 Prozent aller Direktinvestitionen in Deutschland, beide Stand Ende 2009) die höchsten ausländischen Direktinvestitionen aller deutschen Länder verbuchen.

Einhergehend mit der zentralen Lage im wichtigsten europäischen Wirtschaftsraum, der hohen Bevölkerungsdichte, der starken Urbanisierung und der zahlreichen Wirtschaftsstandorte weist Nordrhein-Westfalen eines der dichtesten Verkehrsnetze weltweit auf. 2015 entfiel jede dritte Staumeldung in Deutschland auf Nordrhein-Westfalen.

Der Bahnhof Köln Eifeltor ist Deutschlands größter Containerumschlagbahnhof für den kombinierten Frachtverkehr Schiene/Straße. Im Güterverkehr zählt der Bahnhof Hagen-Vorhalle zu den größten Rangierbahnhöfen Deutschlands.

Der Duisburger Hafen gilt als Verkehrsdrehscheibe der deutschen Binnenschifffahrt. Bedeutendste Binnenwasserstraße in Nordrhein-Westfalen ist der Rhein. Daneben spielen der Rhein-Herne-Kanal (RHK), der Wesel-Datteln-Kanal (WDK), der Datteln-Hamm-Kanal (DHK) und der Dortmund-Ems-Kanal (DEK) eine bedeutende Rolle für Binnenschifffahrt.

Die beiden wichtigsten Drehkreuze im internationalen Flugverkehr sind die Flughäfen Düsseldorf (drittgrößter deutscher Flughafen nach Passagierzahlen) und Köln/Bonn (drittgrößter deutscher Flughafen nach Frachtaufkommen). Weitere Flughäfen mit regelmäßigem Linien- und Charterverkehr sind der Flughafen Dortmund, der Flughafen Niederrhein, der Flughafen Münster/Osnabrück und der Flughafen Paderborn/Lippstadt.

Die Förderung von Kunst und Kultur in Nordrhein-Westfalen ist in der Landesverfassung als ein Staatsziel festgeschrieben. Nordrhein-Westfalen unterstützt eine Vielzahl an künstlerischen und kulturellen Projekten und Institutionen, fast ausschließlich im Lande, auf Bundesebene die Stiftung Preußischer Kulturbesitz, die Kulturstiftung der Länder, die Kunst- und Ausstellungshalle der Bundesrepublik Deutschland sowie das Goethe-Institut. Alleine durch die bloße Einwohnerzahl des Landes leben im Land zahlreiche Kulturschaffende, nach einer Schätzung etwa 30.000 Künstler. Eine bedeutende Rolle bei der nationalen und internationalen Förderung des Films und der Medien spielt die Film- und Medienstiftung Nordrhein-Westfalen. Vergleichbares leistet die Kunststiftung NRW für den Bereich Kunst und Kultur.

Kennzeichnend für die Kunst- und Kulturszene Nordrhein-Westfalens ist ihre Vielfältigkeit und ihr Polyzentrismus. Gründe dafür liegen unter anderem in den ausgeprägten regionalen Unterschieden des Landes in kulturräumlicher Sicht, deren auffälligste Kulturgrenze zwischen Westfalen-Lippe und dem Rheinland verläuft, und in der Landesgeschichte, die das Entstehen einer auch in kultureller Hinsicht dominierenden Metropole oder Residenzstadt nicht ermöglichte. Bedeutende Kultureinrichtungen finden sich daher über das Land verteilt. Impulse für die Kultur- und Kunstentwicklungen im heutigen Land gingen nur selten „von oben aus“. Besonders die Arbeiterkultur im Ruhrgebiet ist dafür ein Beispiel. Die Arbeiterkultur bildete eine der Wurzeln für den Wandel des Ruhrgebiets von einer industriell dominierten Region hin zu einer „Kulturmetropole“, die – wie andere Industrieregionen Nordrhein-Westfalens – ihre Industriekultur weiter als wichtigen Bestandteil ihrer kulturellen Identität sieht. 2010 war das Ruhrgebiet Kulturhauptstadt Europas.

Mit der "Art Cologne" ist in Nordrhein-Westfalen die größte Kunstmesse Deutschlands und die älteste der Welt beheimatet. Weitere bekannte Kunstmessen sind die "Art Düsseldorf", die "Große Kunstausstellung NRW Düsseldorf" und der "Rundgang" der Kunstakademie Düsseldorf. Düsseldorf und Köln sind bekannt als Zentren der Kunst und des Kunsthandels sowie Standorte bedeutender Kunstsammlungen und Ausstellungshäuser, darunter in Düsseldorf die Kunstsammlung Nordrhein-Westfalen, die Kunsthalle Düsseldorf, das Museum Kunstpalast und das NRW-Forum, in Köln die Gemäldegalerie Wallraf-Richartz-Museum & Fondation Corboud sowie das Museum Ludwig. In Dortmund befindet sich das Museum Ostwall. Essen ist Standort des Museums Folkwang und in Münster befindet sich das LWL-Museum für Kunst und Kultur. Auf der Museumsmeile in Bonn befinden sich mit dem Kunstmuseum Bonn und der Kunst- und Ausstellungshalle der Bundesrepublik Deutschland zwei der bedeutendsten Kunstmuseen des Landes. Zu den bedeutendsten Theaterbühnen Deutschlands zählen das Schauspielhaus Bochum und das Schauspiel Köln. Das einzige Staatstheater Nordrhein-Westfalens ist das Düsseldorfer Schauspielhaus. Die bekanntesten Opernhäuser sind die Oper Köln, das Opernhaus Dortmund, das Aalto-Theater in Essen sowie die Deutsche Oper am Rhein in Düsseldorf und Duisburg. Das Theater Dortmund stellt eines der großen Dreispartenhäuser in Deutschland dar. Das Konzerthaus Dortmund, die Tonhalle Düsseldorf, die Philharmonie Essen und die Kölner Philharmonie sind die bekanntesten Konzerthallen des Landes. Bekannteste Kunsthochschulen sind die Kunstakademie Düsseldorf, die Hochschule für Musik Detmold und die Folkwang Universität der Künste.

Die größte Touristenattraktion des Landes und zugleich der Bundesrepublik ist der Kölner Dom mit rund sechs Millionen Besuchern jährlich. Neben dem Kölner Dom zählen auch der Aachener Dom und die Schlösser Augustusburg und Falkenlust, seit 2001 die Zeche und Kokerei Zollverein sowie seit 2014 auch das Kloster Corvey zu den UNESCO-Welterbestätten des Landes. Die Zeche ist das herausragendste Beispiel zahlreicher Industriedenkmäler in den Städten an Rhein und Ruhr, deren historische Innenstadtbereiche dagegen im Zweiten Weltkrieg oder bereits während der Industrialisierung vielfach zerstört wurden. Als Staat drückt sich Nordrhein-Westfalen durch eine Vielzahl staatlicher Bauten aus, insbesondere durch den Landtag Nordrhein-Westfalen und die Gebäude der Landesregierung im Regierungsviertel von Düsseldorf. Ludwig Mies van der Rohe ist der weltweit wohl bekannteste Architekt aus dem Gebiet des heutigen Nordrhein-Westfalens.

Die Natur in Nordrhein-Westfalen ist typisch für den mitteleuropäischen Naturraum. Verbreitet sind in Nordrhein-Westfalen vor allem Arten, die an ausgeräumte Kulturlandschaften oder waldige Mittelgebirgslandschaften angepasst sind. Die ursprüngliche artenreiche Flora und Fauna ist jedoch aufgrund der hohen Siedlungsdichte, der großen Industriezentren des Landes, des Bergbaus, der fast flächendeckenden landwirtschaftlichen und forstwirtschaftlichen Nutzung sowie der fast völligen verkehrstechnischen Erschließung wie in kaum einer anderen Region Europas bedroht. Der Strukturwandel im Ruhrgebiet und Umweltschutzmaßnahmen führten insgesamt zu einer starken Verbesserung der Luft- und Wasserqualität in der Region Rhein-Ruhr. Nordrhein-Westfalen ist etwa zu 25 Prozent bewaldet. Für den Naturschutz und die Naherholung für die Bewohner der dichtbesiedelten Region Rhein-Ruhr weisen daher vor allem die bewaldeten Mittelgebirgsregionen des Landes eine bedeutende Rolle auf. Nordrhein-Westfalen hat Anteil an 14 Naturparks. Der größte dieser Parks ist der Naturpark Teutoburger Wald / Eggegebirge in Ostwestfalen-Lippe. Mit dem Nationalpark Eifel besitzt das Land seit 2004 einen Nationalpark.

Die Beherbergungsbetriebe des Landes hatten im Jahr 2012 etwa 20 Millionen Gäste, die insgesamt rund 45,4 Millionen Mal übernachteten. Die meisten Übernachtungen wurden 2012 im Reisegebiet Teutoburger Wald verzeichnet (6,5 Millionen), gefolgt vom Sauerland und dem Reisegebiet „Köln und Rhein-Erft-Kreis“ mit jeweils 6,2 Millionen Übernachtungen. Die Übernachtungszahl ausländischer Gäste lag 2012 bei über 9,2 Millionen. Dabei sind die Gäste aus den Niederlanden mit rund 25,2 Prozent der Übernachtungen die bedeutendste Gruppe. Mit deutlichem Abstand folgen Touristen aus dem Vereinigten Königreich (7,7 Prozent), Belgien (6,3 Prozent), den USA (5,2 Prozent) und Frankreich (4,1 Prozent).

In Bezug auf die nordrhein-westfälische Küche ist wieder die Zweiteilung in einen rheinischen und westfälisch-lippischen Teil zu erkennen. Zu der Rheinischen Küche zählen auch die Küchen des Niederrheins und des Bergischen Landes. Die Westfälische Küche und die hierzu zählende Lippische Küche sind im weiteren Sinne ein Teil der norddeutschen Küche. Die Westfälische Küche ist vor allem deftig, ein Beispiel ist Westfälischer Schinken auf Pumpernickel. Auch andere Fleisch-, Wurst- und Brotspezialitäten spielen dort eine große Rolle. Die Rheinische Küche ist ebenfalls bodenständig, erscheint im Vergleich zur Westfälischen Küche aber etwas raffinierter. Ein bekanntes Beispiel für Rheinische Küche ist der Rheinische Sauerbraten. Im Südwesten des Landes am Rhein wird Wein im Weinbaugebiet Mittelrhein (Großlage Petersberg) angebaut. Ansonsten ist Nordrhein-Westfalen ein „Bierland“. Neben dem vor allem in Westfalen verbreiteten Pils, das in den 1970er Jahren das bis dahin vorherrschende Exportbier zurückdrängte, gibt es mit Altbier und Kölsch zwei obergärige Biersorten, die vor allem im Rheinland verbreitet sind.

Der Sport in Nordrhein-Westfalen wird vor allem durch den Fußball geprägt. Nicht nur alleine durch seine Größe und Bevölkerungsdichte zu erklären, weist die Region Rhein-Ruhr eine besonders hohe Dichte erfolgreicher Fußballvereine auf. Zur Tradition der Bergarbeiter im Revier zählte immer auch der Fußball. In der Fußball-Bundesliga ist Nordrhein-Westfalen gemessen an der Einwohnerzahl meist überrepräsentiert. Bundesliga-Vereine sind zurzeit Borussia Dortmund, Borussia Mönchengladbach, der FC Schalke 04, der 1. FC Köln und Bayer 04 Leverkusen. Das größte Fußballstadion Deutschlands ist mit über 80.000 Plätzen der Signal Iduna Park (ehemaliges Westfalenstadion) in Dortmund. Neben dem Fußball werden in Nordrhein-Westfalen aber auch viele andere Sportarten betrieben. Besonders viele Zuschauer ziehen dabei Handball, Eishockey und Basketball an.

Neben den bundesweit gültigen Feiertagen Neujahr, Karfreitag, Ostern, Tag der Arbeit, Christi Himmelfahrt, Pfingsten, Tag der Deutschen Einheit und Weihnachten sind in Nordrhein-Westfalen Fronleichnam und Allerheiligen gesetzliche Feiertage.

Der Landtag hat am 24. Juni 2015 beschlossen, dass am 31. Oktober 2017 einmalig der Reformationstag als Feiertag begangen wird.

Das Land Nordrhein-Westfalen verleiht bzw. dotiert folgende Auszeichnungen:


Bekannte Preise, die von privaten Organisationen in Nordrhein-Westfalen verliehen werden, sind:





</doc>
<doc id="3514" url="https://de.wikipedia.org/wiki?curid=3514" title="Nouvelle Vague">
Nouvelle Vague

Nouvelle Vague ( für "Neue Welle") ist eine Stilrichtung des französischen Kinos, die in zwei Phasen verlief. Die erste, weniger beachtete fand bereits 1918 ihren Anfang und wurde maßgeblich von Marcel L’Herbier geprägt. Ihre Fortsetzung fand die Nouvelle Vague dann in ihrer bedeutenderen Phase in den späten 1950er Jahren.
Nach dem Ersten Weltkrieg formte sich in der Stummfilmzeit eine eher inhomogene avantgardistische Szene, deren berühmteste Vertreter Abel Gance, René Clair sowie Jean Cocteau waren. Die Entwicklung wurde nachhaltig durch den Zweiten Weltkrieg unterbrochen. Ende der 1950er Jahre entstand dann in Frankreich eine Bewegung unter jungen Cinéasten und den Pionieren der ersten Welle, die sich gegen die eingefahrene Bildsprache und den vorhersagbaren Erzählfluss des etablierten kommerziellen Kinos wandte. Sie griffen die Ideen und Vorstellungen der Vertreter dieser ersten Welle auf. Bekannte Regisseure der Nouvelle Vague wurden später Autoren der Cahiers du cinéma. In ihren Artikeln stellten sie sich gegen die Verbiederung und die Vorhersehbarkeit des französischen Qualitätskinos ("cinéma de qualité") und propagierten vor allem Filme von Regisseuren wie Alfred Hitchcock, Howard Hawks, Jean Renoir und Roberto Rossellini.

1954 veröffentlichte François Truffaut den Artikel "Eine gewisse Tendenz im französischen Film" ("Une certaine tendance du cinéma français"). Dieser Text gilt als erste eigene theoretische Grundlage der Nouvelle Vague und wendet sich vor allem gegen jene Drehbuchschreiber, die uninspiriert Romanvorlagen adaptieren, ohne selbst einen Bezug zum Kino zu haben. Die Forderung: "Männer des Kinos" sollten Kino machen und sich nicht von Schriftstellern vorschreiben lassen, was verfilmbar ist und was nicht.

Begründet auf den Schriften Alexandre Astrucs und unter Federführung von André Bazin, dem Chefredakteur und einem der Gründer der Cahiers, entwickelten sie die Politik der Autoren ("politique des auteurs"). Diese Politik forderte vom Regisseur, sich an allen Schritten der Filmproduktion zu beteiligen, um so einen eigenen persönlichen Stil entwickeln zu können. Mit dieser charakteristischen Handschrift des Regisseurs sollten die Filme persönlicher und individueller werden und aus dem Schattendasein der Literatur treten. Dabei werde nicht der einzelne Film eines Regisseurs bewertet, sondern immer sein Gesamtwerk. Was zähle, sei das Verhältnis eines Autors zu seinem Film, was sich in der Art seiner Umsetzung ausdrücke. Er unterscheide sich vom Regisseur ("réalisateur"), der stets nur die vom Drehbuchschreiber vorgeschriebene Geschichte umsetze. Autor ("auteur") sei daher, wer Beobachtetes "wieder"erschafft ("récréer"). Er könne insofern einem fremden Stoff durch Bearbeitung und Transzendierung seinen persönlichen Stempel aufdrücken. Die Politik der Autoren soll aber nicht mit dem Autorenfilm in Deutschland verwechselt werden. Auch eine Übersetzung mit "Auteur-Theorie" sei falsch, da sie die mit ihr verbundenen Forderungen an die Regisseure unterschlägt.

Michel Marie begreift die Nouvelle Vague als eine Kunst-Schule ("école artistique"). Die Politik der Autoren könne in diesem Sinne als ästhetisches Programm verstanden werden, wonach der Autor seine Weltanschauung ("vision du monde") dem Film einschreibt. Der feste Korpus von Debütfilmen, die sich auf ein gemeinsames Programm beziehen und als Nouvelle Vague wahrgenommen werden, spricht ebenfalls dafür, von einer Schule zu sprechen. Ein fester Gruppenzusammenhang wird durch die publizistische Unterstützung der Filmzeitschriften ("Cahiers du cinéma") gefördert und vor allem - Michel Marie betont dies ausdrücklich - existieren gemeinsame Feinde: die Autoren der Rive Gauche, versammelt um die wesentlich politisiertere Filmzeitschrift Positif.

Impulsgeber und Vorläufer der Nouvelle Vague sind im italienischen Neorealismus, in Dokumentarfilmen von Regisseuren wie Jean Rouch und in den US-amerikanischen B-Movies zu suchen.

Durch die Entwicklung neuer, leichterer Kameras und lichtempfindlicheren Filmmaterials war es den Filmemachern erstmals möglich, ohne künstliches Licht zu drehen und außerhalb der Filmstudios mit der Handkamera zu arbeiten. Die Fotografie des Kameramanns Raoul Coutard war dabei prägend für die visuelle Ästhetik.

Die Regisseure engagierten vor allem junge unbekannte Schauspieler und weniger die etablierten Filmstars. Musik spielte eine wichtige Rolle in den Filmen, ebenso waren neue Filmtechniken und außergewöhnliche Erzählstile charakteristisch. Es entstand der Essayfilm.

Die Blütezeit der Nouvelle Vague dauerte bis Mitte der 1960er Jahre an. Die entwickelten Effekte und Erzähltechniken werden noch heute, auch in kommerziellen Filmen und der Werbung, verwendet.










</doc>
<doc id="3516" url="https://de.wikipedia.org/wiki?curid=3516" title="Natrium">
Natrium

Natrium ist ein häufig vorkommendes chemisches Element mit dem Symbol Na und der Ordnungszahl 11. Im Periodensystem der Elemente steht es in der 3. Periode und als Alkalimetall in der 1. IUPAC-Gruppe bzw. 1. Hauptgruppe. Natrium ist ein Reinelement, dessen einziges stabiles Isotop Na ist.

Elementares Natrium wurde erstmals 1807 von Humphry Davy durch Schmelzflusselektrolyse aus Natriumhydroxid gewonnen und Sodium genannt. Diese Bezeichnung wird im englischen und französischen Sprachraum verwendet, Ableitungen hiervon in den romanischen und zum Teil auch in slawischen Sprachen. Der deutsche Name „Natrium“ ist über , „Natron“, vom ägyptischen "netjerj" abgeleitet. "Natrium" und Ableitungen hiervon sind außer in der deutschen Sprache auch in Skandinavien, im Niederländischen und einigen slawischen Sprachen in Verwendung. Im Japanischen hat Natrium den deutsch „klingenden“ Namen "Natoriumu".

Unter Normalbedingungen ist Natrium ein wachsweiches, silberglänzendes und hochreaktives Metall. Wegen seiner starken Reaktivität wird metallisches (elementares) Natrium unter inerten Bedingungen gelagert, meistens in Paraffinöl oder Petroleum, bei größeren Mengen in luftdicht verschlossenen Stahlfässern.

In der Erdhülle gehört Natrium zu den zehn häufigsten Elementen und kommt in zahlreichen Mineralen der Erdkruste vor. Auch in Meerwasser ist eine erhebliche Menge Natrium in Form von Natriumionen enthalten.

Die Ägypter prägten in der Antike für das aus Sodaseen gewonnene "Soda" den Begriff "netjerj" ("neter"). Die Griechen übernahmen dieses Wort als griech. νίτρον "nitron", die Römer als "nitrium", die Araber als "natrun".
Natriumverbindungen sind im Gegensatz zum elementaren Metall schon sehr lange bekannt und wurden seither aus Meerwasser oder Seen gewonnen, aus Erdlagerstätten abgebaut und gehandelt. Die wichtigste Natriumverbindung Kochsalz ("Natriumchlorid") wurde in Bergwerken oder durch Trocknen von Meerwasser oder salzhaltigem Quellwasser in Salinen gewonnen. Der Handel mit Salz war für viele Städte die Grundlage ihres Reichtums, und prägte teils sogar ihren Namen (Salzgitter, Salzburg). Auf den germanischen Namen für Saline (Hall) weisen Ortsnamen wie Hallstatt, Hallein, Halle (Saale), Bad Hall, Bad Reichenhall, Schwäbisch Hall, Schweizerhalle oder Hall in Tirol hin. Auch andere natürlich vorkommende Natriumverbindungen wie Natriumcarbonat ("Soda" oder "Natron") und Natronsalpeter wurden seit der Antike gewonnen und gehandelt.

Die Herstellung von elementarem Natrium gelang erst im Jahre 1807 Humphry Davy durch Elektrolyse von geschmolzenem Natriumhydroxid ("Ätznatron") unter Verwendung von Voltaschen Säulen als Stromquelle. Wie er am 19. November 1807 vor der "Royal Society" in London berichtete, gewann er zwei verschiedene Metalle: Das in Soda enthaltene Natrium nannte er "Sodium", was die noch heute gebräuchliche Bezeichnung des Metalls im französischen und englischsprachigen Raum ist; das andere Metall nannte er "Potassium" (Kalium). Berzelius schlug 1811 den heutigen Namen "Natrium" vor.

Im Universum steht Natrium in der Häufigkeit an 14. Stelle, vergleichbar mit Calcium und Nickel. Im ausgestrahlten Licht vieler Himmelskörper, auch dem der Sonne, kann die gelbe Natrium-D-Linie gut nachgewiesen werden.

Auf der Erde ist Natrium mit einem Anteil an der Erdkruste von 2,36 % das sechsthäufigste Element. Es kommt aufgrund seiner Reaktivität nicht elementar, sondern stets in Verbindungen, den Natrium-Salzen, vor. Ein großer Speicher von Natrium ist das Meerwasser. Ein Liter Meerwasser enthält durchschnittlich 11 Gramm Natriumionen.

Häufige Natriumminerale sind Albit, auch Natronfeldspat genannt, NaAlSiO und Oligoklas (Na,Ca)Al(Si,Al)O. Neben diesen gesteinsbildenden Mineralen, die zu den Feldspaten zählen, kommt Natrium in großen Salzlagerstätten vor. Es existieren vor allem große Lagerstätten an Halit (Natriumchlorid, umgangssprachlich häufig „Steinsalz“ genannt), die durch das Austrocknen von Meeresteilen entstanden sind. Diese stellen die wichtigste Quelle zur Gewinnung von Natrium und seinen Verbindungen dar. Bekannte deutsche Salzförderstätten sind unter anderem Salzgitter, Bad Reichenhall, Stade und Bad Friedrichshall.

Neben Natriumchlorid kommen auch andere Natriumsalze in der Natur vor. So ist Natriumnitrat oder Natronsalpeter (auch Chilesalpeter genannt) NaNO eines der wenigen natürlichen Nitratminerale. Es kommt wegen seiner guten Wasserlöslichkeit aber nur in besonders trockenen Gegenden, wie der Atacamawüste in Chile vor. Vor Erfindung des Haber-Bosch-Verfahrens war dies der wichtigste Rohstoff für viele Düngemittel und Sprengstoffe.

Natriumcarbonat NaCO wird ebenfalls natürlich in mehreren Mineralen gefunden. Das bekannteste Mineral ist Soda NaCO · 10 HO. Es wird in großen Mengen abgebaut und vor allem in der Glasherstellung verwendet.

Daneben existieren noch eine Vielzahl weiterer Natriummineralen (siehe auch: ). Ein bekanntes ist der Kryolith ("Eisstein", Na<nowiki>[AlF</nowiki><nowiki>]</nowiki>), der in geschmolzenem Zustand als Lösungsmittel für Aluminiumoxid bei der Aluminiumherstellung dient. Da das einzige bekannte Kryolith-Vorkommen in Grönland abgebaut ist, wird Kryolith heute künstlich hergestellt.

Natrium wird hauptsächlich aus Natriumchlorid gewonnen, welches meist bergmännisch oder durch Austrocknung salzhaltiger Lösungen wie dem Meerwasser erhalten wird. Nur ein kleiner Teil des Natriumchlorids wird zu elementarem Natrium weiterverarbeitet, der größte Teil wird als Speisesalz oder für die Herstellung anderer Natriumverbindungen verwendet.

Die großtechnische Herstellung von Natrium erfolgt heute durch Schmelzflusselektrolyse von trockenem Natriumchlorid in einer sogenannten Downs-Zelle (1924 patentiert von James C. Downs). Zur Schmelzpunkterniedrigung wird ein eutektisches Salzgemisch aus 60 % Calciumchlorid und 40 % Natriumchlorid eingesetzt, das bei 580 °C schmilzt. Auch Bariumchlorid ist als Zusatz möglich. Es wird eine Spannung von etwa sieben Volt angelegt. Für die Herstellung von einem Kilogramm Natrium werden während der Elektrolyse etwa 10 kWh Strom verbraucht, im gesamten Produktionsprozess etwa 12 kWh.

Die zylindrische Elektrolysezelle besteht aus einer mittigen Graphitanode und einem seitlichen Kathodenring aus Eisen. Oberhalb der Zelle ist eine Glocke, die das entstandene Chlor sammelt und abführt. Das Natrium sammelt sich oberhalb der Kathoden und wird durch ein gekühltes Steigrohr aus der Zelle entfernt. Ebenfalls entstandenes Calcium kristallisiert dort aus und fällt in die Schmelze zurück.

Die Elektrolyse von Natriumchlorid löste das "Castner-Verfahren" ab. Dabei wurde das Natrium durch Schmelzflusselektrolyse von Natriumhydroxid gewonnen. Dieses hatte zwar den Vorteil des geringeren Schmelzpunktes von Natriumhydroxid (318 °C), es wird aber mehr elektrische Energie benötigt. Seit Einführung der Chlor-Alkali-Schmelzflusselektrolyse hat sich der Preis für Natrium drastisch verringert. Damit ist Natrium volumenbezogen das preiswerteste Leichtmetall überhaupt. Der Preis hängt allerdings stark von den Stromkosten und dem Preis für das ebenfalls entstehende Chlor ab.

Natrium ist ein silberweißes, weiches Leichtmetall. In vielen Eigenschaften steht es zwischen Lithium und Kalium. So liegt der Schmelzpunkt mit 97,82 °C zwischen dem des Lithiums (180,54 °C) und dem des Kaliums (63,6 °C). Ähnlich ist dies beim Siedepunkt und der spezifischen Wärmekapazität. Mit einer Dichte von 0,968 g · cm ist Natrium eines der spezifisch leichtesten Elemente. Von den bei Raumtemperatur festen Elementen haben nur noch Lithium und Kalium eine geringere Dichte. Mit einer Mohshärte von 0,5 ist Natrium so weich, dass es mit dem Messer zu schneiden ist.

Natrium kristallisiert, wie die anderen Alkalimetalle, im kubischen Kristallsystem in einem raumzentrierten Gitter mit der und zwei Formeleinheiten pro Elementarzelle. Unterhalb von 51 K geht es in eine hexagonal dichteste Kugelpackung mit den Gitterparametern "a" = 376 pm und "c" = 615 pm über.

Natriumdampf besteht sowohl aus einzelnen Metallatomen, als auch aus Dimeren der Form Na. Am Siedepunkt liegen 16 % der Atome als Dimer vor. Der Dampf ist gelb und erscheint in der Durchsicht purpurfarben.

Mit Kalium werden in einem weiten Konzentrationsbereich bei Raumtemperatur flüssige Gemische gebildet. Das Phasendiagramm zeigt eine bei 7 °C inkongruent schmelzende Verbindung NaK und ein Eutektikum bei −12,6 °C mit einem Kaliumgehalt von 77 % (Massenanteil).

Wie die anderen Alkalimetalle ist Natrium ein sehr unedles Element (Normalpotential: −2,71 V) und reagiert leicht mit vielen anderen Elementen und zum Teil auch mit Verbindungen. Die Reaktionen sind vor allem mit Nichtmetallen, wie Chlor oder Schwefel, sehr heftig und laufen unter leuchtend gelber Flammenerscheinung ab.

Der ansonsten reaktive Sauerstoff stellt eine Besonderheit dar. Natrium und Sauerstoff reagieren ohne Anwesenheit von Wasser bei Raumtemperatur und auch beim Erwärmen nicht direkt miteinander. Unter einer vollkommen wasserfreien Sauerstoff-Atmosphäre kann Natrium sogar geschmolzen werden, ohne dass es zur Reaktion kommt. Sind dagegen Feuchtigkeitsspuren vorhanden, verbrennt es leicht zu Natriumperoxid.

Mit Wasser reagiert Natrium unter Bildung von Wasserstoff zu Natriumhydroxid. Hochgeschwindigkeitsaufnahmen der Reaktion von Alkalimetallen mit Wasser legen eine Coulomb-Explosion nahe. In Alkoholen setzt sich Natrium unter Bildung von Wasserstoff zu Natrium-Alkoholaten um. Durch die hohe Reaktionswärme schmilzt es häufig auf. Bei feiner Verteilung des Natriums und der damit einhergehenden großen Reaktionsoberfläche kann die Reaktion explosiv sein und den Wasserstoff entzünden. Kommt Natrium mit chlorierten Verbindungen wie Dichlormethan, Chloroform, Tetrachlormethan in Kontakt, kommt es unter Bildung von Natriumchlorid zu einer schnellen und exothermen Reaktion.

Natrium löst sich mit blauer Färbung in flüssigem Ammoniak. Die Farbe beruht auf freien Elektronen, die vom Natrium in die Lösung abgegeben werden. So leitet die Lösung auch elektrischen Strom und ist verdünnt paramagnetisch.
Auf ähnliche Art und Weise lässt sich auch das Anion des Natriums, das Natrid-Ion, zum Beispiel in der Form des Kalium(2.2.2-Kryptand)natrids (K(C222)Na) darstellen. Es ist ein sehr starkes Reduktionsmittel.

Von Natrium sind insgesamt 19 Isotope und 3 weitere Kernisomere von Na bis Na bekannt. Von diesen kommt nur eines, das Isotop Na natürlich vor. Damit ist Natrium eines von 22 Reinelementen. Die langlebigsten künstlichen Isotope sind Na, das mit einer Halbwertszeit von 2,602 Jahren unter Betazerfall in Ne übergeht und Na, das mit einer Halbwertszeit von 14,957 Stunden ebenfalls unter Betazerfall zu Mg zerfällt. Diese werden als Tracer in der Nuklearmedizin verwendet. Alle anderen Isotope und Isomere haben nur kurze Halbwertszeiten von Sekunden oder Millisekunden.

Es werden große Mengen Natriumchlorid und andere Natriumverbindungen, wie Natriumcarbonat, gefördert. Davon wird aber nur ein sehr kleiner Teil zu Natrium weiterverarbeitet. Der größte Teil wird direkt verwendet oder zu anderen Verbindungen umgesetzt. Über Verwendungsmöglichkeiten von Natriumverbindungen: siehe Abschnitt Verbindungen.

Natrium ist das meistgebrauchte Alkalimetall. Es wird für diverse Zwecke sowohl technisch als auch im Labor eingesetzt. Im Schulunterricht und bei Experimentalvorlesungen kann Natrium mit Hilfe eines Natriumlöffels und Wasser zur Wasserstoffherstellung verwendet werden. Aus einem Teil des Natriums wird eine Anzahl von Natriumverbindungen hergestellt. Dies sind beispielsweise das als Bleichmittel verwendete Natriumperoxid und die starke Base Natriumamid. Diese kommen nicht natürlich vor und können auch nicht direkt aus Natriumchlorid gewonnen werden. Auch Natriumcyanid und Natriumhydrid werden aus Natrium hergestellt. Da Natrium das Erstarrungsgefüge beeinflusst, kann es als Zusatz von Aluminium-Silicium-Legierungen verwendet werden (Veredelungsverfahren nach Aladár Pácz).

Natrium katalysiert die Polymerisation von 1,3-Butadien und Isopren. Daher wurde es für die Produktion von künstlichen Kautschuk eingesetzt. Mit Natrium als Katalysator gewonnener Kunststoff, der als Buna bezeichnet wurde, war das erste künstliche Gummi der Welt. Ab 1937 wurde er in den Buna-Werken (benannt nach Butadien und Natrium) in Schkopau produziert.

Da Natrium mit einer Wärmeleitfähigkeit von 140 W/(m·K), die weit über der von Stahl (15 bis 58 W/(m·K)) liegt, gute Wärmeübertragungseigenschaften und ebenso einen niedrigen Schmelzpunkt mit gleichzeitig großem flüssigen Bereich besitzt, wird es als Kühlmittel zur Kühlung der thermisch hoch beanspruchten Auslassventile in Verbrennungsmotoren verwendet. Hierzu werden die Ventilschäfte hohl ausgeführt und zum Teil mit Natrium gefüllt. Im Betrieb schmilzt das Natrium und schwappt zwischen der heißen und kalten Seite hin und her. Die Wärme wird dadurch vom rotglühend heißen Ventilteller abtransportiert.

Auch schnelle Brüter werden mit geschmolzenem Natrium gekühlt. In solchen Brutreaktoren dürfen die bei der Kernspaltung entstehenden schnellen Neutronen nicht wie in anderen Reaktortypen zwischen den Brennstäben abgebremst werden. Es darf daher zur Kühlung kein Wasser, das als Bremsmittel (Moderator) wirkt, eingesetzt werden. Die Wärme wird dann über einen Sekundärnatriumkreislauf an den Dampferzeuger für den Turbinenbetrieb weitergegeben.

Natriumdampflampen nutzen das charakteristische gelbe Licht aus, das Natriumdampf bei einer elektrischen Entladung aussendet. Sie werden aufgrund ihrer hohen Lichtausbeute häufig zur Straßenbeleuchtung eingesetzt.

Einige Metalle, wie Titan, Zirconium, Tantal oder Uran können nicht durch Reduktion mit Kohlenstoff gewonnen werden, weil dabei stabile und nicht abtrennbare Carbide entstehen. Neben einigen anderen Elementen, insbesondere Aluminium und Magnesium, wird daher Natrium als Reduktionsmittel eingesetzt. Ein weiteres Element, zu dessen Darstellung Natrium eingesetzt wird, ist Kalium. Da Kalium ein sehr unedles Element ist, kann es nicht durch Reduktion mit Kohlenstoff gewonnen werden. Eine theoretisch mögliche Herstellung durch Elektrolyse ist technisch auf Grund der guten Löslichkeit von Kalium in einer Kaliumchloridschmelze nicht möglich.

Natrium spielt eine wichtige Rolle als Reduktionsmittel in der organischen Synthese. Über lange Zeit war die technisch wichtigste Natriumanwendung die Herstellung von Tetraethylblei aus Chlorethan. Dieses war ein wichtiges Antiklopfmittel, das dem Benzin beigemischt wurde. Aus Umweltschutzgründen wurde die Verwendung von Tetraethylblei stark eingeschränkt oder ganz verboten. Daher ging auch der Verbrauch an Natrium zurück. Auch in anderen Reaktionen wie der Birch-Reduktion und der Pinakol-Kupplung wird Natrium verwendet. Diese sind jedoch eher im Labormaßstab von Interesse.

Da Natrium auch mit Spuren von Wasser reagiert, kann frisch gepresster Natriumdraht als Trocknungsmittel zur Trocknung organischer Lösungsmittel, wie Diethylether oder Toluol, genutzt werden. Für halogenhaltige Lösungsmittel (Beispiele: Methylenchlorid, Chloroform) ist diese Methode "nicht" geeignet, da Natrium mit diesen heftig reagiert.

Natrium-Kalium-Legierungen sind bei Raumtemperatur flüssig. Diese dienen zur Wärmeübertragung sowie zur Dehalogenierung in der organischen Synthese. Na-K eignet sich gut zum Trocknen einiger bereits gut vorgetrockneter Lösungsmittel, um besonders niedrige Rest-Wassergehalte zu erreichen.

Während der 1960er Jahre wurde mit Natriumkabeln mit Polyethylen-Umhüllung experimentiert. Wegen der geringeren Leitfähigkeit hätte ein hypothetisches Natriumkabel einen um 75 % größeren Durchmesser.

Der qualitative Nachweis und die quantitative Bestimmung erfolgen atomspektroskopisch durch die intensiv gelbe Flammenfärbung oder genauer über die Na-Doppellinie bei 588,99 nm und 589,59 nm.

Der Nachweis von Natrium auf rein chemischem Weg ist sehr schwierig. Da fast alle Natriumverbindungen gut wasserlöslich sind, sind klassische Fällungsreaktionen und gravimetrische Bestimmungen kaum möglich. Ausnahmen bilden das gelbe "Natriummagnesiumuranylacetat" NaMg(UO)(CHCOO)·9 HO und das farblose "Natriumhexahydroxoantimonat" Na[Sb(OH)], die beide schwerlöslich sind.
Eine Fällungsreaktion mit dem Sulfat-Bismut-Doppelsalz 3NaSO·2Bi(SO)·2HO ist auch möglich.

Auch Farbreaktionen sind schwierig, da Natriumionen in wässriger Lösung farblos sind. Von praktischer Bedeutung sind heute daher neben der Ionenchromatographie nur noch die spektroskopischen Methoden.

Natrium ist eines der Elemente, die für alle tierischen Organismen essentiell sind. Im tierischen Organismus ist Natrium – zusammen mit Chlor – das neunthäufigste Element und stellt – nach Calcium und Kalium – das dritthäufigste anorganische Ion. Damit zählt es physiologisch zu den Mengenelementen. Natrium liegt in Lebewesen in Form von Na-Ionen vor.

Im menschlichen Körper sind bei einem durchschnittlichen Körpergewicht von 70 kg etwa 100 g Natrium als Na-Ionen enthalten. Davon liegen zwei Drittel als NaCl und ein Drittel als NaHCO vor. Da es im menschlichen Körper 90 % der extrazellulären Elektrolyte ausmacht, bestimmt die Natriumkonzentration über das Gefäßvolumen das Volumen der interstitiellen Flüssigkeit.

Der Schätzwert für die minimale Zufuhr von Natrium liegt laut den D-A-CH-Referenzwerten bei 550 mg/Tag für Erwachsene. Von verschiedenen Organisationen gibt es jedoch insbesondere Empfehlungen für eine maximale Zufuhr von Natrium (WHO: 2 g/Tag; AHA: 1,5 g/Tag).

Die tatsächliche tägliche Natriumzufuhr liegt häufig über diesen Werten. Die Ursache dafür ist unser relativ hoher Salzkonsum (2,5 g Salz enthalten ca. 1 g Natrium). Die Nationale Verzehrsstudie II (NVS II) des Max Rubner-Instituts, bei der der Natriumkonsum anhand von Fragebögen ermittelt wurde, ergab im Median eine Aufnahme von 3,2 g/Tag (Männer) bzw. 2,4 g/Tag (Frauen). Vermutlich liegt die tatsächliche Natriumzufuhr aber noch höher, da die Erfassung über Fragebögen fehleranfällig ist. Als „Goldstandard“ für die Ermittlung der Natriumzufuhr dient die Bestimmung von Natrium im 24-Stunden-Urin. Einem Bericht der WHO zufolge lag in der INTERSALT-Studie die Natriumausscheidung in verschiedenen Orten Deutschlands bei 4,1-4,5 g/Tag (Männer) bzw. 2,7-3,5 g/Tag (Frauen).

Der Natriumgehalt wird streng kontrolliert und ist eng verbunden mit der "Regulation des Wasserhaushalts". Die normale Natriumkonzentration im Serum liegt bei etwa 135–145 mmol/l. Ist der Natriumspiegel geringer wird von einer Hyponatriämie gesprochen, bei der es zu einer Steigerung des Zellvolumens kommt. Bei einer Hypernatriämie dagegen ist der Natriumspiegel zu hoch und die Zellen schrumpfen. In beiden Fällen wird vor allem die Funktion des Gehirns beeinträchtigt. Es kann zu epileptischen Anfällen und Bewusstseinsstörungen bis hin zum Koma kommen. Eine wichtige Rolle für die Regulation spielen das Renin-Angiotensin-Aldosteron-System, das Adiuretin und Atriopeptin.

Schlüsselorgan bei der Regulation von Natrium ist die Niere. Diese ist dafür zuständig bei einem Natriumüberschuss Wasser zurückzuhalten, um das Natrium im Körper zu verdünnen, und Natrium selbst auszuscheiden. Bei einem Natriummangel wird vermehrt Wasser ausgeschieden und Natrium retiniert. Dabei gilt jedoch zu beachten, dass die Niere einige Zeit braucht, bis sie auf den veränderten Natriumbestand reagieren kann.

Im Organismus sind die Na-Ionen nicht gleichmäßig verteilt, vielmehr sind – wie bei den anderen Ionen auch – die Konzentrationen inner- und außerhalb der Zellen stark verschieden. Diese Konzentrationsgefälle von Na- und Cl- (überwiegend außen), K- sowie organischer Anionen (überwiegend innen) bedingen den Großteil des Membranpotentials lebender Zellen. Dieses Membranpotential und die Ionengradienten sind für die meisten Zellen überlebenswichtig. Da die kleinen anorganischen Ionen wegen der Konzentrationsunterschiede dauernd in den Nachbarbereich wandern, bedarf es eines aktiven Prozesses, der dem entgegensteuert. Die wichtigste Rolle spielt dabei die Natrium-Kalium-Pumpe, die unter Energieverbrauch Na- und K-Ionen immer wieder zurückpumpt.

Na-Ionen spielen eine wichtige Rolle bei der Entstehung und Weiterleitung von Erregungen in Nervenzellen (und Muskelfasern). An den Postsynapsen von Nervenzellen (und an der neuromuskulären Endplatte der Muskelfasern) befinden sich bestimmte Rezeptoren, die sich nach ihrer Aktivierung durch Überträgerstoffe (Neurotransmitter), die von der vorangehenden Nervenzelle bei deren Erregung ausgeschüttet werden, öffnen und für Natriumionen durchlässig werden. Durch Natriumeinstrom kommt es zu einer lokalen Änderung des im Grundzustand stabilen Membranpotentials der Zelle. Das Innere wird gegenüber dem Äußeren weniger negativ, man spricht von einer Depolarisation. Ist diese Depolarisation nach dem Weg bis zum Axon noch stark genug, kommt es zur Öffnung eines anderen Natriumkanaltyps. Dabei handelt es sich um die spannungsabhängigen Natriumkanäle des Axons, die die örtliche Depolarisation – gemeinsam mit anderen Ionenkanälen – durch einen bestimmten Öffnungs- und Schließrhythmus weiterleiten. An den Axonen der Nervenzellen entsteht so eine fortlaufende Spannungswelle, das Aktionspotential. Bei der Wiederherstellung des Grundzustandes spielt wiederum die Natrium-Kalium-Pumpe eine essentielle Rolle.

Bei Pflanzen spielt Natrium hingegen eine untergeordnete Rolle. Während Kalium für alle Pflanzen und die meisten Mikroorganismen essentiell ist, wird Natrium nur von einigen C4- und CAM-Pflanzen benötigt, von C3-Pflanzen in der Regel jedoch nicht.
Je nach Standort haben sich aber davon unabhängig Pflanzen entwickelt, die von einer Natriumaufnahme profitieren können. Diese Pflanzen, Halophyten genannt, kommen besonders in Küstenregionen oder anderen Gebieten vor, in denen der Boden eine hohe Natriumkonzentration aufweist.
Halophyten wie die Zuckerrübe, Kohl und viele C4-Gräser sind salztolerant, da sie das Natrium aus dem Zentralzylinder heraus in die Vakuolen der Blattzellen transportieren können, wo es als osmotisch wirksames Ion für eine Erhöhung des Turgors sorgt und dadurch statt des Kaliums die Zellstreckung und das Blattflächenwachstum positiv beeinflusst. Natrium substituiert damit zu einem Teil Kalium, zu einem anderen Teil wirkt es aber auch zusätzlich wachstumsfördernd.

Pflanzen, die Natrium nicht aus dem Zentralzylinder heraus in die Blattzellen transportieren können, akkumulieren es im Xylemparenchym. Zu diesen sogenannten natrophoben Pflanzen gehören u. a. Buschbohne und Mais. Das Natrium, gelangte es in die Blattzellen, könnte nicht in die Vakuolen transportiert werden, sondern verbliebe im Zellplasma (Cytosol) und würde dort das für die Bildung von Polymeren wichtige Kalium verdrängen (natriuminduzierter Kaliummangel). Dieses führte schließlich zu einer Hemmung der Photosynthese.
Die Akkumulation von Natrium im Zentralzylinder der Wurzel und im Stängelgewebe wirkt sich aber bei hoher Natriumkonzentration negativ für die Pflanze aus. Durch die Erhöhung des osmotischen Wertes wird sie bei der Wasseraufnahme und dem Wassertransport behindert. Es kommt zu einer Wasser- und Nährstoffunterversorgung der Blätter, was zu einer Verringerung der Photosyntheseleistung führt.

Da die meisten Pflanzen Natrium nur in geringen Mengen enthalten, müssen viele Pflanzenfresser zusätzliches Natriumchlorid aus natürlichen Salzvorkommen aufnehmen.

Kleinere Mengen Natrium werden unter Petroleum aufbewahrt. Für größere Mengen gibt es integrierte Handhabungssysteme mit Schutzgasatmosphäre. Das Natrium ist trotz Schutzgas oder Petroleum häufig von einer Schicht aus Natriumhydroxid und Natriumoxid überzogen.

Natriumbrände lassen sich mit Metallbrandpulver (Kochsalz), Kaliumchlorid, Grauguss-Spänen, behelfsweise mit Sand oder trockenem Zement löschen. Sand und Zement reagieren jedoch in gewissem Rahmen mit Natrium, was die Löschwirkung mindert. Keinesfalls dürfen Wasser, Schaum, Löschpulver, Kohlenstoffdioxid oder Halone verwendet werden. Diese Löschmittel reagieren mit Natrium zum Teil stark exotherm, was gegebenenfalls zu stärkeren Bränden und Explosionen führen kann.

In Verbindungen kommt das Natrium ausschließlich in der Oxidationsstufe +1 vor. Alle Verbindungen weisen einen stark ionischen Charakter auf, fast alle sind gut wasserlöslich. Natriumverbindungen zählen zu den wichtigsten Salzen vieler Säuren. Industriell werden meist Natriumsalze zur Gewinnung der entsprechenden Anionen verwendet, da deren Synthese kostengünstig ist.

Natriumchlorid (NaCl), häufig als Speisesalz oder Kochsalz bezeichnet, ist das wichtigste und bekannteste Natriumsalz. Da es in großen Mengen vorkommt, ist es der wichtigste Rohstoff für die Gewinnung von Natrium und anderen Natriumverbindungen. Natriumchlorid stellt auch für den Menschen die wichtigste Natriumquelle dar. Technisch wird es unter anderem zur Konservierung von Lebensmitteln und als Streusalz im Straßenverkehr genutzt. Es ist Namensgeber für die Natriumchlorid-Struktur, eine für viele Salze typische Kristallstruktur.

Daneben sind alle anderen möglichen Natriumhalogenide, also Natriumfluorid NaF, Natriumbromid NaBr und Natriumiodid NaI, bekannt und stabil.

Es sind insgesamt fünf Oxide des Natriums bekannt. Dies sind Natriumoxid NaO, Natriumperoxid
NaO, Natriumhyperoxid NaO, Dinatriumtrioxid NaO und Natriumtrioxid NaO. Natriumoxid ist in vielen Gläsern enthalten, es entsteht bei der Glasherstellung aus dem eingesetzten Natriumcarbonat. Bei der Verbrennung von Natrium entsteht es nur bei bestimmten Temperaturen (150–200 °C) und stöchiometrisch eingesetzten Mengen Natrium und Sauerstoff. Ist dies nicht der Fall, verbrennt Natrium zu Natriumperoxid. Dieses ist ein starkes Oxidationsmittel und das technisch wichtigste Natriumoxid. Es wird als Bleichmittel für Textilien und Papier, sowie als Sauerstoffquelle beim Tauchen und in U-Booten verwendet. Die anderen Oxide sind sehr instabil und zersetzen sich schnell.

Natriumhydroxid (NaOH) ist für die Industrie mit die wichtigste Base. Die wässrige Lösung von Natriumhydroxid wird Natronlauge genannt. Sie wird unter anderem für die Herstellung von Seife und Farbstoffen sowie zum Aufschluss von Bauxit bei der Aluminiumproduktion verwendet.

Mit Schwefelwasserstoff bildet Natrium zwei Salze, das Natriumsulfid NaS und das Natriumhydrogensulfid NaHS. Beide werden u. a. zur Schwermetallfällung verwendet.

Natriumsulfat NaSO, das Natriumsalz der Schwefelsäure, wird u. a. in Waschmitteln und in der Papierindustrie im Sulfatverfahren eingesetzt. Wie andere zweiwertige Anionen bildet Sulfat neben dem Natriumsulfat noch Natriumhydrogensulfat. Auch andere Schwefel-Sauerstoffsäuren bilden Natriumsalze. Ein Beispiel ist Natriumthiosulfat NaSO, das in der Analogfotografie als Fixiersalz verwendet wird.

In Natriumhydrid NaH und Natriumborhydrid NaBH liegt der Wasserstoff in der Oxidationsstufe −1 vor. Beide werden vorwiegend in der organischen Chemie verwendet. Natriumhydrid wird dabei im Wesentlichen als starke, wenig nucleophile Base zur Deprotonierung von Thiolen, Alkoholen, Amiden, C-H-aciden Verbindungen etc. eingesetzt, Natriumborhydrid hingegen zur Reduktion z. B. von Ketonen. Letztere Reaktion kann durch Gegenwart von Cer(III)verbindungen selektiv für Ketone durchgeführt werden (Luche-Reduktion). Kommen sie mit Wasser in Berührung, entsteht gasförmiger Wasserstoff H.

Natriumcarbonat NaCO und Natriumhydrogencarbonat NaHCO sind die Natriumsalze der Kohlensäure. Sie zählen, neben Natriumchlorid und Natriumhydroxid, zu den wichtigsten Natriumverbindungen. Natriumcarbonat (häufig mit dem Trivialnamen Soda bezeichnet) wird in großen Mengen bei der Glasherstellung benötigt. Natriumhydrogencarbonat wird als Backpulver verwendet. Es bildet beim Erhitzen mit Säuren Kohlenstoffdioxid und Wasser.

Natriumnitrat NaNO, das Natriumsalz der Salpetersäure, ist eine der seltenen natürlich vorkommenden Nitratverbindungen (Chilesalpeter). Natriumnitrat wird als Düngemittel und als Konservierungsmittel verwendet.

Organische Verbindungen des Natriums sind im Gegensatz zu denen des Lithiums sehr instabil. Sie sind äußerst reaktiv und können teilweise auch mit sonst unreaktiven aliphatischen Kohlenwasserstoffen reagieren. Ausreichend stabil für Anwendungen in Reaktionen sind nur Verbindungen mit aromatischen Resten, wie Cyclopentadien, die als Reduktionsmittel verwendet werden können.


</doc>
<doc id="3517" url="https://de.wikipedia.org/wiki?curid=3517" title="Neon">
Neon

Neon (griechisch "" „neu“) ist ein chemisches Element mit dem Symbol Ne und der Ordnungszahl 10.

Im Periodensystem steht es in der 8. Hauptgruppe, bzw. der 18. IUPAC-Gruppe und zählt daher zu den Edelgasen. Wie die anderen Edelgase ist es ein farbloses, äußerst reaktionsträges, einatomiges Gas. In vielen Eigenschaften wie Schmelz- und Siedepunkt oder Dichte steht es zwischen dem leichteren Helium und dem schwereren Argon.

Im Universum gehört Neon zu den häufigsten Elementen, auf der Erde ist es dagegen relativ selten, da wie bei Helium ein großer Teil des Gases in das Weltall entwichen ist. Vorwiegend ist es in der Erdatmosphäre zu finden, nur geringe Mengen sind in Gesteinen eingeschlossen.

Wie Krypton und Xenon wurde auch Neon 1898 von William Ramsay und Morris William Travers durch fraktionierte Destillation von flüssiger Luft entdeckt. Die bekannteste Anwendung sind die Leuchtröhren oder Neonlampen, in denen Neon durch Gasentladungen in einer typischen orangeroten Farbe zum Leuchten angeregt wird.

1894 war von Lord Rayleigh und William Ramsay als erstes Edelgas das Argon entdeckt worden. Ramsay isolierte 1895 auch das zuvor nur aus dem Sonnenspektrum bekannte Helium aus Uranerzen. Aus den Gesetzen des Periodensystems erkannte er, dass es zwischen Helium und Argon ein weiteres Element mit einer Atommasse von etwa 20 u geben müsste.

Er untersuchte daher ab 1896 zunächst verschiedene Minerale und Meteoriten und die von diesen beim Erhitzen oder Lösen abgegebenen Gase. Ramsay und sein Mitarbeiter Morris William Travers waren dabei jedoch nicht erfolgreich, es wurden Helium und seltener Argon gefunden. Auch die Untersuchung heißer Gase aus Cauterets in Frankreich und aus Island brachten keine Ergebnisse.

Schließlich begannen sie, 15 Liter Rohargon, das aus flüssiger Luft isoliert wurde, zu untersuchen und durch Verflüssigen und fraktionierte Destillation zu trennen. Das erste dadurch abgetrennte und am Flammenspektrum nachgewiesene Element war dabei das Krypton, am 13. Juni 1898 gelang schließlich die Isolierung eines leichteren Elementes aus der niedriger siedenden Fraktion des Rohargons. Dieses nannten Ramsay und Travers "Neon", nach dem griechischen "" „neu“. Kurze Zeit später konnten sie aus der Krypton enthaltenden Fraktion ein weiteres Element, das Xenon gewinnen.

Die erste Anwendung des neu entdeckten Gases war die 1910 von dem Franzosen Georges Claude entwickelte Neonlampe: In eine Glasröhre gefülltes Neon wird durch hohe Spannungen zum Leuchten angeregt.

Neon, vor allem das Isotop Ne, ist ein wichtiges Zwischenprodukt in der Nukleosynthese in Sternen, entsteht aber erst beim Kohlenstoffbrennen. Während des Heliumbrennens bei etwa 200 · 10 K entsteht Ne auf Grund des geringen Einfangquerschnitts von O für α-Teilchen nicht, lediglich die Isotope Ne und Ne können aus dem schwereren O entstehen. Steigen die Temperatur und die Dichte eines Sterns nach Verbrauch des Heliums deutlich an, so kommt es zum Kohlenstoffbrennen, bei dem zwei Kohlenstoffatome zu einem angeregten Magnesiumisotop Mg* fusionieren. Aus diesem bildet sich durch α-Zerfall Ne.

Bei weiterem Temperatur- und Druckanstieg kommt es zum Neonbrennen, bei dem Ne im α-Zerfall zu O reagiert beziehungsweise mit den entstandenen Heliumkernen zu Mg fusioniert.

Dieses findet auf Grund der höheren Empfindlichkeit von Ne im Vergleich zu O gegenüber Gammastrahlung vor den eigentlich zu erwartenden Reaktionen des leichteren Sauerstoffkerns statt. Erst nach Ende des Neonbrennens findet das Sauerstoffbrennen statt, bei dem aus O schwerere Elemente wie Silicium, Phosphor und Schwefel gebildet werden.

Neon zählt im Universum zu den häufigsten Elementen, lediglich Wasserstoff, Helium, Sauerstoff, Kohlenstoff und Stickstoff sind häufiger. Auf der Erde ist es dagegen – wie Helium – relativ selten, der Gesamtanteil an der Erdhülle beträgt etwa 0,005 ppm. Der Großteil des Neons befindet sich dabei in der Atmosphäre, mit einem durchschnittlichen Gehalt von 18,18 ppm ist es nach Argon das häufigste Edelgas. Aus der unterschiedlichen Verteilung der leichten und schweren Isotope des Neons auf der Erde und Sonne kann geschlossen werden, dass seit Entstehung der Erde ein Großteil des Neons aus der Atmosphäre entwichen ist und bevorzugt die schwereren Isotope Ne und Ne zurückgeblieben sind.

In kleinen Mengen kommt Neon auch in Gesteinen der Erde vor. Nachgewiesen wurde es in Granit, Basaltgesteinen, Diamanten und vulkanischen Gasen. Auf Grund verschiedener Isotopenzusammensetzungen wird vermutet, dass dieses Neon drei verschiedene Ursprünge hat: Primordiales Neon, dessen Zusammensetzung derjenigen der Sonne entspricht und das ohne Kontakt zur Atmosphäre in Diamanten oder im Erdmantel eingeschlossen ist; atmosphärisches Neon und durch Spallationsreaktionen mit kosmischer Strahlung entstandenes Neon.

Auf Gasplaneten wie Jupiter kann auf Grund der hohen Gravitation das Neon nicht entweichen, die Isotopenzusammensetzung entspricht daher derjenigen bei der Bildung des Planeten. Wie von der Raumsonde Galileo festgestellt, entspricht das Verhältnis von Ne zu Ne demjenigen der Sonne, was Rückschlüsse auf die Entstehungsbedingungen, etwa die Temperatur, bei der Bildung der Gasplaneten zulässt.

Neon lässt sich als Nebenprodukt bei der Luftzerlegung nach dem Linde-Verfahren gewinnen. Nach Abtrennung von Wasser, Kohlenstoffdioxid, Sauerstoff, den bei höheren Temperaturen siedenden Edelgasen und dem Großteil an Stickstoff bleibt ein Gasgemisch zurück, das zu 35 % aus Neon, daneben aus Helium, Wasserstoff und etwa 50 % Stickstoff besteht (jeweils Stoffmengenanteile). Dieses kann auf verschiedene Weisen getrennt werden, so dass am Ende die reinen Gase Neon und Helium gewonnen werden. Eine Möglichkeit ist es, die Gase über Kondensation bei unterschiedlichen Siedepunkten und die Ausnutzung des Joule-Thomson-Effektes zu trennen. Nach Abtrennung des Wasserstoffes über katalytische Reaktion mit zugegebenem Sauerstoff und Entfernung des Wassers wird dabei zunächst bei 30 bar und 66 K der Stickstoff verflüssigt und abgetrennt. Nach der Entfernung des restlichen Stickstoffes durch Adsorption an Silicagel bleibt ein Gasgemisch von etwa 76 % Neon und 24 % Helium zurück. Dieses wird bei Raumtemperatur zunächst auf 180 bar verdichtet und stufenweise auf 50 K abgekühlt. Bei der Expansion auf 25 bar und anschließend auf 1,5 bar kondensiert das Neon, während Helium gasförmig bleibt. Eine genauere Trennung erfolgt danach über eine Kolonne.

Eine Alternative ist die Adsorption. Dazu wird nach der Abtrennung des Stickstoffes das Neon bei 5 bar und 67 K an ein Trägermaterial adsorbiert. Dieses gibt bei 3 bar das Neon wieder ab, so dass es vom Helium getrennt werden kann. Um eine größere Reinheit zu erreichen, wird das Neon zweimal nacheinander adsorbiert.

Neon ist ein bei Normalbedingungen einatomiges, farbloses und geruchloses Gas, das bei 27 K (−246 °C) kondensiert und bei 24,57 K (−248,59 °C) erstarrt. Es besitzt damit den kleinsten Temperaturbereich aller Elemente, in dem es flüssig ist. Wie die anderen Edelgase außer dem Helium kristallisiert Neon in einer kubisch dichtesten Kugelpackung mit dem Gitterparameter "a" = 443 pm.

Wie alle Edelgase besitzt Neon nur abgeschlossene Schalen (Edelgaskonfiguration). Dadurch lässt sich erklären, dass das Gas stets einatomig vorliegt und die Reaktivität gering ist.

Mit einer Dichte von 0,9 kg/m bei 0 °C und 1013 hPa ist Neon etwas leichter als Luft, steigt also auf. Im Phasendiagramm liegt der Tripelpunkt bei 24,56 K und 43,37 kPa, der kritische Punkt bei 44,4 K, 265,4 kPa sowie einer kritischen Dichte von 0,483 g/cm.

In Wasser ist Neon schlecht löslich, in einem Liter Wasser können sich bei 20 °C maximal 10,5 ml Neon lösen.

Wie andere Edelgase zeigt Neon bei Gasentladungen ein charakteristisches Linienspektrum. Da die Linien im sichtbaren Spektralbereich vorwiegend im roten bis gelben Bereich sind, erscheint das Gas bei einer Entladung in einer typischen roten Farbe.

Als typisches Edelgas ist Neon äußerst reaktionsträge, es sind wie beim Helium bislang keine Verbindungen des Elementes bekannt. Sogar Clathrate, bei denen andere Edelgase in anderen Verbindungen physikalisch eingeschlossen sind, sind unbekannt. Nach theoretischen Berechnungen ist Neon das am wenigsten reaktive Element. So ist die berechnete Dissoziationsenthalpie für Verbindungen des Typs NgBeO (Ng: Edelgas) bei der Neonverbindung am geringsten. Es zeigte sich, dass selbst das Neonanalogon der einzig bekannten, nach Rechnungen stabilen Heliumverbindung HHeF nicht stabil sein sollte. Mögliche Erklärungen für diese Ergebnisse sind die größeren Fluor-Wasserstoff-Abstände und damit schwächere Anziehungskräfte im HNe-Ion im Vergleich zur Helium-Spezies oder abstoßende p-π-Wechselwirkungen in Neon-Kationen.

Es sind lediglich aus massenspektrometrischen Untersuchungen einige Ionen bekannt, in denen Neon beteiligt ist. Zu diesen zählen das Ne-Ion und einige Element-Neon-Ionen wie ArNe, HeNe und HNe.

Es sind insgesamt 19 Isotope des Neons zwischen Ne und Ne bekannt. Von diesen sind drei, Ne, Ne und Ne stabil und kommen auch in der Natur vor. Ne kommt mit 90,48 % Anteil mit Abstand am häufigsten vor. Ne ist mit 0,27 % Anteil auf der Erde am seltensten und Ne kommt mit einer Häufigkeit von 9,25 % in der natürlichen Isotopenverteilung auf der Erde vor. Alle anderen Isotope haben kurze Halbwertszeiten von maximal 3,38 Minuten bei Ne.

Bedingt durch den Verlust von Neon in das Weltall und die Bildung in Kernreaktionen ist das Verhältnis von Ne/Ne und Ne/Ne von Neon, das in Gesteinen eingeschlossen ist und keinen Kontakt zur Atmosphäre besitzt, nicht immer gleich. Daher lassen sich aus den Isotopenverhältnissen Rückschlüsse auf die Entstehung schließen. So ist in Gesteinen, in denen Neon durch Spallationsreaktionen entstanden ist, der Gehalt an Ne erhöht. Primordiales Neon, das vor dem Verlust eines großen Teils des Neons in Gesteinen und Diamanten eingeschlossen wurde, besitzt dagegen einen höheren Anteil an Ne.

"→ Liste der Neon-Isotope"

Wie die anderen Edelgase hat Neon auf Grund der Reaktionsträgheit keine biologische Bedeutung und ist auch nicht toxisch. In höheren Konzentrationen wirkt es durch Verdrängung des Sauerstoffs erstickend. Bei Drücken von mehr als 110 bar wirkt es narkotisierend.

Auf Grund der Seltenheit und komplizierten Herstellung und des damit einhergehenden höheren Preises im Vergleich zum ähnlichen Argon wird Neon nur in kleineren Mengen verwendet. Neon ist Füllgas von Leuchtröhren und Glimmlampen, in denen es durch Gasentladungen zum Leuchten in einer typischen orangeroten Farbe angeregt wird. Auch in Blitz- und Stroboskoplampen wird Neon als Füllgas genutzt.

Helium-Neon-Laser, in denen ein Gemisch an Helium und Neon eingesetzt wird, zählen zu den wichtigeren Lasern. Die notwendige Besetzungsinversion des Laser wird dabei durch die Anregung des Heliums und strahlungslosen Übergang von Elektronen zum Neon erreicht. Die stimulierte Emission erfolgt am Neon bei Wellenlängen von 632,8 nm (rot) sowie 1152,3 nm und 3391 nm (infrarot). Weitere Laserübergänge, etwa im grünen Spektralbereich bei 543,3 nm, sind möglich.

Flüssiges Neon kann als Kältemittel eingesetzt werden. Es hat eine 40-mal höhere Kühlleistung als flüssiges Helium und eine dreimal höhere als Wasserstoff.

Neon kann im Gemisch mit Sauerstoff als Atemgas für das Tauchen in großer Tiefe genutzt werden. Es wird jedoch nur selten eingesetzt, da es im Vergleich zum ähnlich verwendbaren Helium einen höheren Preis besitzt und auch einen größeren Atemwiderstand aufweist.



</doc>
<doc id="3518" url="https://de.wikipedia.org/wiki?curid=3518" title="Neodym">
Neodym

Neodym (Nomenklaturempfehlung war zeitweise "Neodymium") ist ein chemisches Element mit dem Elementsymbol Nd und der Ordnungszahl 60. Im Periodensystem steht es in der Gruppe der Lanthanoide und zählt damit auch zu den Metallen der Seltenen Erden. Die Elementbezeichnung leitet sich von den griechischen Worten νέος "neos" ‚neu‘ und δίδυμος "didymos" ‚Zwilling‘ (als Zwilling von Lanthan) ab. Das Metall wird in Form der Legierung Neodym-Eisen-Bor für starke Permanentmagnete verwendet.

1841 extrahierte Carl Gustav Mosander die Seltene Erde Didym aus Ceriterde. 1874 bemerkte Per Teodor Cleve, dass es sich bei Didym eigentlich um zwei Elemente handelte. Im Jahr 1879 isolierte Lecoq de Boisbaudran Samarium aus Didym, das er aus dem Mineral Samarskit gewann. 1885 gelang es Carl Auer von Welsbach, Didym in Praseodym und Neodym zu trennen, die Salze unterschiedlicher Farbe bilden. Reines metallisches Neodym wurde erst 1925 dargestellt.

Neodym kommt in natürlicher Form nur in chemischen Verbindungen vergesellschaftet mit anderen Lanthanoiden, vorzugsweise Mineralien, vor:

Der wichtigste Lieferant mit 91 % der Weltproduktion ist China. Das führt dort zu erheblichen Umweltproblemen. „Bei der Trennung des Neodyms vom geförderten Gestein entstehen giftige Abfallprodukte, außerdem wird radioaktives Uran und Thorium beim Abbauprozess freigesetzt. Diese Stoffe gelangen zumindest teilweise ins Grundwasser, kontaminieren so Fauna und Flora erheblich und werden für den Menschen als gesundheitsschädlich eingestuft“. Weitere wirtschaftlich verwertbare Vorkommen finden sich in Australien, hier vor allem im Northern Territory das von Arafura Resources ausgebeutet wird.

Wie bei allen Lanthanoiden werden zuerst die Erze durch Flotation angereichert, danach die Metalle in ihre Halogenide umgewandelt und durch fraktionierte Kristallisation, Ionenaustausch oder Extraktion getrennt.

Nach einer aufwendigen Abtrennung der Neodymbegleiter kann das Oxid mit Fluorwasserstoff zu Neodym(III)-fluorid umgesetzt und anschließend mit Calcium unter Bildung von Calciumfluorid zu Neodym reduziert werden. Calciumreste und Verunreinigungen trennt man durch Umschmelzen im Vakuum ab. 

Dieses Verfahren wurde in China seit etwa 1984 durch Schmelzfluss-Elektrolyse eines Gemisches aus Neodymfluorid, Neodymoxid und Lithiumfluorid abgelöst. Formal entspricht dieses Verfahren dem Hall-Héroult-Prozess, das elementare Neodym wird jedoch an einer senkrecht stehenden Wolfram-Kathode abgeschieden, welche von einer ringförmigen Graphit-Anode umgeben ist. Das schmelzflüssige Neodym wird in einer Molybdänwanne aufgefangen, aus der es manuell in eine Barrengussform gegossen wird. Jede solche Elektrolyse-Zelle wird als Einkammer-Elektrolysezelle betrieben. Über den tatsächlichen Betrieb der Zellen in den chinesischen Betrieben sind keine überprüfbaren Angaben bekannt. Die Produktion von metallischem Neodym erfolgt vermutlich nach einem Standard-Operationsprotokoll, ohne messtechnische Rückkopplung. Es steht zu vermuten, dass insbesondere die diskontinuierliche Zugabe von Neodymoxid manuell und ohne stetige Rückkopplung an die Zellspannung erfolgt. In diesem Regelbetrieb treten unvermeidlich Überspannungen an der Anode auf, was zu exorbitant hohen Emissionen der extrem potenten Treibhausgase Kohlenstofftetrafluorid und Hexafluorethan führt. Diese hoch flüchtigen, perfluorierten Kohlenstoffverbindungen (PFK´s) sind nicht durch bekannte Filterverfahren abtrennbar und sind nach der Emission als sehr langlebige, persistente Verbindungen mit Halbwertszeiten von ca. 10.000 respektive ca. 50.000 Jahren mit einem ca. 5.000-fach (Tetrafluormethan) und 9.000-fach (Hexafluorethan) stärkerem Treibhauspotenzial in der Erdatmosphäre aktiv. Die Weiterentwicklung hin zu einem PFK-freien Elektrolyseverfahren ist Gegenstand der elektrometallurgischen Prozessentwicklung in den USA.

Die chinesische Regierung hat angekündigt, schärfere Umweltauflagen einzuführen und stärker gegen illegale Minen vorzugehen. Anfang Juni 2011 scheint es zu einer ersten Umsetzung dieser Absicht gekommen zu sein. Laut Berichten der Financial Times erhält der staatseigene Produzent (Baotou Steel Rare Earth) das Monopol für den Abbau und die Aufbereitung der Seltenen Erden. 35 lizenzierte Betriebe werden geschlossen und entschädigt, neun weitere nicht lizenzierte Betriebe sollen geschlossen und nicht entschädigt werden. In den U.S.A. wird derzeit die Mine "Mountain Pass" in Kalifornien und in Australien die Mine "Mount Weld" reaktiviert. Beiden Minen werden vom Öko-Institut e. V. akzeptable Umweltschutzsysteme bescheinigt. Allerdings gibt es auch Vorhaben zum kombinierten Abbau Seltener Erden in Grönland, bei denen die giftigen Rückstände in Seen verklappt werden sollen.

Die Jahresproduktion wurde 2012 auf 21.000 t geschätzt, davon kommen 91 % aus China. Laut USGS lag der Preis für 1 kg Neodym 2001 unter 10 USD. Der Preis stieg bis 2010 auf 80 USD und erreichte 2011 den Höchststand mit 244 USD je kg. Danach ging er wieder zurück und lag 2013 bei 65 USD je kg.

Das silbrigweiß glänzende Metall gehört zu den Lanthanoiden und Metallen der Seltenen Erden. Es ist an der Luft etwas korrosionsbeständiger als Europium, Lanthan, Cer oder Praseodym, bildet aber leicht eine rosaviolette Oxidschicht aus, welche an der Luft abblättern kann.

Bei hohen Temperaturen verbrennt Neodym zum Sesquioxid NdO. Mit Wasser reagiert es unter Bildung von Wasserstoff zum Neodymhydroxid Nd(OH). Mit Wasserstoff setzt es sich zum Hydrid NdH um. Neben der Hauptwertigkeit/Oxidationszahl 3 kommen unter besonderen Bedingungen auch die Oxidationszahlen 2 und 4 vor.





Neodym-Eisen-Bor (NdFeB) ist der Werkstoff, aus dem derzeit die stärksten Dauermagnete hergestellt werden können. Sie erreichen eine Remanenz von bis zu 1,4 Tesla. Die Koerzitivfeldstärke H schwankt im Bereich von 870 bis 2750 kA/m.



</doc>
<doc id="3519" url="https://de.wikipedia.org/wiki?curid=3519" title="Neptunium">
Neptunium

Neptunium ist ein chemisches Element mit dem Elementsymbol Np und der Ordnungszahl 93. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block). Neptunium ist das erste der sogenannten Transurane, die auf der Erde, bis auf Spuren von Neptunium und Plutonium, nicht mehr natürlich vorkommen. Neptunium ist ein giftiges und radioaktives Schwermetall. Es wurde benannt nach dem Planeten Neptun, der auf den Planeten Uranus folgt. Neptunium folgt im Periodensystem auf Uran, dann folgt Plutonium, das schwerste auf der Erde natürlich vorkommende Element mit der Ordnungszahl 94.

Im Mai 1934 äußerte sich Ida Noddack zu den damals bestehenden Lücken im Periodensystem der Elemente und stellte am Ende ihrer Arbeit Überlegungen über die Möglichkeit von Transuranen an.

Wenige Wochen später veröffentlichte Enrico Fermi drei Arbeiten zu diesem Thema. Noddack setzte sich im September 1934 kritisch mit der vermeintlichen Entdeckung des Elements 93 durch Fermi auseinander. In ihren Ausführungen nahm sie u. a. die Entdeckung der neutroneninduzierten Kernspaltung vorweg:

Das radioaktive Element Neptunium synthetisierten Edwin M. McMillan und Philip H. Abelson erstmals 1940 durch Beschuss von Uran mit Neutronen.

Arthur C. Wahl und Glenn T. Seaborg entdeckten 1942 das Neptuniumisotop Np. Es entsteht aus U, das ein β-Strahler mit rund 7 Tagen Halbwertszeit ist, oder durch einen (n, 2n)-Prozess aus U. Np ist ein α-Strahler mit einer Halbwertszeit von 2,144 · 10 Jahren.

Im Jahr 1950 wurden aus U, U und U durch Beschuss mit Deuteronen die Neptuniumisotope Np, Np und Np erzeugt. Im Jahr 1958 wurden aus hochangereichertem U durch Beschuss mit Deuteronen die Neptuniumisotope Np, Np und Np erzeugt. Die 1-Stunden Neptunium-Aktivität, die zuvor dem Np zugewiesen worden ist, gehört hingegen zum Isotop Np.

Neptunium entsteht als Nebenprodukt der Energiegewinnung in Kernreaktoren. Eine Tonne abgebrannter Kernbrennstoff kann etwa 500 g Neptunium enthalten. So entstandenes Neptunium besteht fast ausschließlich aus dem Isotop Np. Es entsteht aus dem Uranisotop U durch zweifachen Neutroneneinfang und anschließenden β-Zerfall.

Metallisches Neptunium kann durch Reduktion aus seinen Verbindungen erhalten werden. Zuerst wurde Neptunium(III)-fluorid mit elementarem Barium oder Lithium bei 1200 °C zur Reaktion gebracht.

Neptuniummetall hat ein silbernes Aussehen, ist chemisch reaktiv und existiert in mindestens drei verschiedenen Modifikationen:

Neptunium besitzt eine der höchsten Dichten aller Elemente. Neben Rhenium, Osmium, Iridium und Platin ist es eines der wenigen Elemente, die eine höhere Dichte als 20 g/cm besitzen.

Neptunium bildet eine Reihe von Verbindungen, in denen es in den Oxidationsstufen +3 bis +7 vorliegen kann. Damit besitzt Neptunium zusammen mit Plutonium die höchste mögliche Oxidationsstufe aller Actinoiden. In wässriger Lösung haben die Neptuniumionen charakteristische Farben, so ist das Np-Ion purpurviolett, Np gelbgrün, NpO grün, NpO rosarot und NpO tiefgrün.

Eine biologische Funktion des Neptuniums ist nicht bekannt. Anaerobe Mikroorganismen können mittels Mn(II/III)- und Fe(II)-Spezies Np(V) zu Np(IV) reduzieren. Ferner wurden die Faktoren untersucht, die die Biosorption und Bioakkumulation des Neptuniums durch Bakterien beeinflussen.

Von Neptunium sind insgesamt 20 Isotope und 5 Kernisomere bekannt. Die langlebigsten Isotope sind Np mit 2,144 Mio. Jahren, Np mit 154.000 Jahren und Np mit 396,1 Tagen Halbwertszeit. Die restlichen Isotope und Kernisomere besitzen Halbwertszeiten zwischen 45 Nanosekunden (Np) und 4,4 Tagen (Np).




Wie bei allen Transuran-Nukliden ist auch bei den Np-Isotopen die neutroneninduzierte Kernspaltung möglich. Die Isotope mit ungerader Neutronenanzahl im Kern – von den langlebigen also Np – haben große Wirkungsquerschnitte für die Spaltung durch thermische Neutronen; beim Np beträgt er 2600 Barn, es ist also "leicht spaltbar".

Bei dem im Kernreaktorbrennstoff anfallenden Np beträgt dieser Wirkungsquerschnitt nur 20 Millibarn. Dieses Isotop ist jedoch aufgrund anderer kernphysikalischer Eigenschaften geeignet, mit der Spaltung durch "schnelle" Neutronen im reinen Material eine Kettenreaktion aufrechtzuerhalten. Im Los Alamos National Laboratory wurde seine kritische Masse experimentell zu etwa 60 kg bestimmt. Daher ist Np ein mögliches Material für Kernwaffen.

Das in Kernreaktoren aus U erbrütete Np kann zur Gewinnung von Pu zur Verwendung in Radionuklidbatterien genutzt werden. Dazu wird es (zusammen mit unwesentlichen Mengen anderer Neptuniumisotope) vom abgebrannten Reaktorbrennstoff abgetrennt und in Brennstäbe gefüllt, die nur Neptunium enthalten. Diese werden wieder in den Kernreaktor eingesetzt, wo sie erneut mit Neutronen bestrahlt werden; aus dem Np wird Pu erbrütet.

"→ Kategorie: "

Bekannt sind Oxide in den Stufen +4 bis +6: Neptunium(IV)-oxid (NpO), Neptunium(V)-oxid (NpO) und Neptunium(VI)-oxid (NpO · HO). Neptuniumdioxid (NpO) ist das chemisch stabilste Oxid des Neptuniums und findet Verwendung in Kernbrennstäben.

Für Neptunium sind Halogenide in den Oxidationsstufen +3 bis +6 bekannt.

Für die Stufe +3 sind sämtliche Verbindungen der vier Halogene Fluor, Chlor, Brom und Iod bekannt. Darüber hinaus bildet es Halogenide in den Stufen +4 bis +6.

In der Oxidationsstufe +6 ist das Neptuniumhexafluorid (NpF) von besonderer Bedeutung. Es ist ein orangefarbener Feststoff mit sehr hoher Flüchtigkeit, der schon bei 56 °C in den gasförmigen Zustand übergeht. In dieser Eigenschaft ähnelt es sehr dem Uranhexafluorid und Plutoniumhexafluorid, daher kann es genauso in der Anreicherung und Isotopentrennung verwendet werden.

Analog zu Uranocen, einer Organometallverbindung in der Uran von zwei Cyclooctatetraen-Liganden komplexiert ist, wurden die entsprechenden Komplexe von Thorium, Protactinium, Plutonium, Americium und auch des Neptuniums, (η-CH)Np, dargestellt.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen.





</doc>
<doc id="3520" url="https://de.wikipedia.org/wiki?curid=3520" title="Nobelium">
Nobelium

Nobelium ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol No und der Ordnungszahl 102. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Nobelium ist ein radioaktives Metall, welches aber aufgrund der geringen zur Verfügung stehenden Mengen bisher nicht als Metall dargestellt wurde. Es wurde 1957 entdeckt und Alfred Nobel zu Ehren benannt. Der Name wurde 1994 endgültig von der IUPAC bestätigt.

Über eine Entdeckung wurde erstmals 1957 von einer Wissenschaftler-Arbeitsgruppe aus den USA, Großbritannien und Schweden berichtet. 1958 meinten Albert Ghiorso, Torbjørn Sikkeland, John R. Walton und Glenn T. Seaborg in Berkeley das No entdeckt zu haben. Ferner meldete im gleichen Jahr eine sowjetische Gruppe um G. N. Flerov die Entdeckung von No. 1964 wurde aus Dubna die Herstellung von No gemeldet. Aber erst 1968 wurden in Berkeley aus Cf und C ca. 3000 Atome No erzeugt.

Im Periodensystem steht das Nobelium mit der Ordnungszahl 102 in der Reihe der Actinoide, sein Vorgänger ist das Mendelevium, das nachfolgende Element ist das Lawrencium. Sein Analogon in der Reihe der Lanthanoide ist das Ytterbium.

Nobelium ist ein radioaktives und sehr kurzlebiges Metall. In seinen Verbindungen tritt die Oxidationszahl +2 gegenüber +3 häufiger auf.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.




</doc>
<doc id="3521" url="https://de.wikipedia.org/wiki?curid=3521" title="Niob">
Niob

Niob [] (nach Niobe, der Tochter des Tantalos) ist ein chemisches Element mit dem Elementsymbol Nb und der Ordnungszahl 41. Es zählt zu den Übergangsmetallen, im Periodensystem steht es in der 5. Periode sowie der 5. Nebengruppe (Gruppe 5) oder Vanadiumgruppe.

Im angelsächsischen Sprachraum wird auch heute noch von vielen Metallurgen, Werkstoffanbietern und im privaten Umgang die schon länger veraltete Bezeichnung "Columbium" und das Kurzzeichen "Cb" verwendet.

Das selten vorkommende Schwermetall ist von grauer Farbe und gut schmiedbar. Niob kann aus den Erzen Columbit, Coltan (Columbit-Tantalit) und Loparit gewonnen werden. Es wird hauptsächlich in der Metallurgie verwendet, um Spezialstähle herzustellen und die Schweißbarkeit zu verbessern.

Niob wurde 1801 durch Charles Hatchett entdeckt. Er fand es in einer Probe von Columbit-Erz aus einem Flussbett in Massachusetts, das um 1743 nach England verschickt worden war. Hatchett benannte das Element "Columbium" (nach Columbia, der Personifikation der Vereinigten Staaten). Bis Mitte des 19. Jahrhunderts ging man davon aus, dass es sich bei Columbium und dem 1802 entdeckten Tantal um dasselbe Element handelt, da sie in Mineralen fast immer zusammen auftreten.

Erst 1844 zeigte der Berliner Professor Heinrich Rose, dass Niob- und Tantalsäure unterschiedliche Stoffe sind. Nicht um die Arbeiten Hatchetts und dessen Namensgebung wissend, nannte er das wiederentdeckte Element aufgrund dessen Ähnlichkeit mit Tantal nach Niobe, der Tochter des Tantalos.

Erst nach 100 Jahren Auseinandersetzung legte die International Union of Pure and Applied Chemistry (IUPAC) 1950 "Niob" als offizielle Bezeichnung des Elements fest.

1864 gelang Christian Wilhelm Blomstrand die Herstellung von metallischem Niob durch Reduktion von Niobchlorid mit Wasserstoff in der Hitze. 1866 bestätigte Charles Marignac Tantal als eigenständiges Element.

1907 stellte Werner von Bolton durch Reduktion eines Heptafluoroniobats mit Natrium sehr reines Niob her.

Niob ist ein seltenes Element mit einem Anteil an der Erdkruste von 1,8 · 10 %. Es kommt nicht gediegen vor. Aufgrund der ähnlichen Ionenradien kommen Niob und Tantal immer verschwistert vor. Die wichtigsten Minerale sind Columbit (Fe, Mn)(Nb, Ta)O, das je nach Gehalt an Niob oder Tantal auch als Niobit oder Tantalit bezeichnet wird, sowie Pyrochlor (NaCaNbOF). Etwa 85 - 90 % der Niob-Industrie verwendet Niob von Lagerstätten, die nicht mit dem Bergbau von Tantal-Erzen assoziiert sind.

Die wichtigsten erschlossenen Vorkommen von Pyrochlor liegen in Brasilien. Die Lagerstätten bei Araxá werden von der Companhia Brasileira de Metalurgia e Mineração (CBMM) bewirtschaftet und sollen mit geschätzten Reserven von 460 mio t ausreichen, um den Weltbedarf an Niob für 500 Jahre zu decken. Eine weitere brasilianische Lagerstätte mit ca. 18 mio t Niob-Reserven wird von Mineração Catalão de Goias geführt, die vermutlich größten Vorkommen bei São Gabriel da Cachoeira sind aufgrund ihrer Lage im Nationalpark Pico da Neblina und indigenen Reservaten bislang nicht erschlossen. Die drittgrößte ausgebeutete Lagerstätte der Welt ist die vom Konzern Iamgold betriebene Niobec-Mine in Kanada. Die drei genannten Unternehmen produzieren Ferro-Niob mit einem nominalen Nb-Oxid-Gehalt von 60 % und decken gemeinsam etwa 85 % des Niob-Weltbedarfs. Weitere bedeutende, zum Teil wenig erschlossene Niob-Lagerstätten befinden sich im Lowosero-Massiv in Russland und in der Lueshe-Mine in der Provinz Nord-Kivu (Demokratische Republik Kongo).

Einige Nioberze wie Tantalit und Coltan, sowie deren Derivate wurden 2012 von der US-amerikanischen Börsenaufsicht SEC als so genanntes "Konfliktmineral" eingestuft, dessen Verwendung für Unternehmen gegenüber der SEC berichtspflichtig ist. Als Grund hierfür werden die Produktionsorte im Osten des Kongo angeführt, die von Rebellen kontrolliert werden und so im Verdacht stehen, bewaffnete Konflikte mitzufinanzieren.

Weitere meist seltene Minerale sind:

Da Niob und Tantal immer zusammen vorkommen, werden Niob- und Tantalerze zunächst gemeinsam aufgeschlossen und anschließend durch fraktionierte Kristallisation oder unterschiedliche Löslichkeit in organischen Lösungsmitteln getrennt. Das erste industrielle Trennverfahren solcherart wurde 1866 von Charles Marignac entwickelt.

Zunächst werden die Erze einem Gemisch von konzentrierter Schwefel- und Flusssäure bei 50–80 °C ausgesetzt. Dabei bilden sich die komplexen Fluoride <nowiki>[</nowiki>NbF<nowiki>]</nowiki> und <nowiki>[</nowiki>TaF<nowiki>]</nowiki>, die leicht löslich sind.

Durch Überführung in eine wässrige Phase und Zugabe von Kaliumfluorid können die Dikalium-Salze dieser Fluoride (z. B. Kaliumheptafluoroniobat) gebildet werden. Dabei ist nur das Tantalfluorid in Wasser schwer löslich und fällt aus. Das leicht lösliche Niobfluorid kann so vom Tantal getrennt werden. Heutzutage ist aber eine Trennung durch Extraktion mit Methylisobutylketon üblich. Eine dritte Möglichkeit der Trennung ist die durch fraktionierte Destillation der Chloride NbCl und TaCl. Diese sind durch Reaktion von Erzen, Koks und Chlor bei hohen Temperaturen darstellbar.

Aus dem abgetrennten Niobfluorid wird durch Reaktion mit Sauerstoff zunächst Niobpentoxid hergestellt. Dieses wird entweder mit Kohlenstoff zunächst zu Niobcarbid umgesetzt und dann mit weiterem Niobpentoxid bei 2000 °C im Vakuum zum Metall reduziert oder direkt aluminothermisch gewonnen. Der größte Teil des Niob für die Stahlindustrie wird so produziert, dabei wird noch Eisenoxid zugesetzt, um eine Eisen-Niob-Legierung (60 % Niob) zu erhalten. Werden Halogenide als Ausgangsstoff für die Reduktion eingesetzt, geschieht dies mit Natrium als Reduktionsmittel.

Die Jahresproduktion lag 2006 bei fast 60.000 t, 90 % davon wurden in Brasilien gefördert. 2013 betrug die Gesamtmenge des produzierten Niobs 59.400 Tonnen; Hauptproduzenten waren Brasilien (53.100 t) und Kanada (5.260 t). Der USGS gibt als US-Importpreise für eine Eisen-Niob-Legierung ("Ferroniobium" mit einem Anteil von 65 % Niob) 37.781 USD je Tonne im Jahre 2010 und 43.415 USD je Tonne für 2013 an.

Niob ist ein grau glänzendes, duktiles Schwermetall. Bekannt sind die Oxidationsstufen −3, −1, 0, +1, +2, +3, +4, +5. Wie beim Vanadium, das im Periodensystem über dem Niob steht, ist die Stufe +5 am beständigsten. Das chemische Verhalten des Niobs ist fast identisch mit dem des Tantals, das im Periodensystem direkt unter Niob steht.

Infolge der Ausbildung einer Passivschicht (Schutzschicht) ist Niob an der Luft sehr beständig. Die meisten Säuren greifen es bei Raumtemperatur daher nicht an. Nur Flusssäure, vor allem im Gemisch mit Salpetersäure, sowie heiße konzentrierte Schwefelsäure korrodieren metallisches Niob rasch. In heißen Alkalien ist Niob ebenfalls unbeständig, da sie die Passivschicht auflösen. Bei Temperaturen oberhalb von 200 °C beginnt es in Gegenwart von Sauerstoff zu oxidieren. Eine schweißtechnische Bearbeitung von Niob muss wegen seiner Unbeständigkeit an der Luft unter Schutzgasatmosphäre ablaufen.

Der Zusatz von Wolfram und Molybdän zu Niob erhöht seine Hitzebeständigkeit, Aluminium seine Festigkeit.

Niob wird unterhalb von 9,26 K supraleitend und ist damit das supraleitende Element mit der höchsten Sprungtemperatur.

Niob nimmt leicht Gase auf, so kann ein Gramm Niob bei Raumtemperatur 100 cm³ Wasserstoff aufnehmen, was früher in der Vakuumröhrentechnik ausgenutzt wurde.

Niob wird als Legierungszusatz für rostfreie Stähle, Sonderedelstähle (z. B. Rohre für die Salzsäureproduktion) und Nichteisenlegierungen verwendet, da sich niob-legierte Werkstoffe durch eine erhöhte mechanische Festigkeit auszeichnen. Bereits in Konzentrationen von 0,01 bis 0,1 Massenprozent kann Niob in Kombination mit thermomechanischem Walzen die Festigkeit und Zähigkeit von Stahl wesentlich steigern.
Erste Versuche zur Verwendung von Niob als Legierungselement (Ersatz von Wolfram) fanden 1925 in den USA statt. Solcherart veredelte Stähle werden häufig im Rohrleitungsbau (Pipeline construction) eingesetzt. Als starker Karbidbildner wird Niob auch in Schweißzusatzwerkstoffen zum Abbinden von Kohlenstoff zulegiert.

Als weitere Verwendungen sind anzuführen:


Niob gilt zwar als nicht toxisch, jedoch irritiert metallischer Niobstaub Augen und Haut. Niobstaub ist leicht entzündlich.

Eine physiologische Wirkungsweise des Niobs ist unbekannt.




</doc>
<doc id="3522" url="https://de.wikipedia.org/wiki?curid=3522" title="Nickel">
Nickel

Nickel ist ein chemisches Element mit dem Elementsymbol Ni und der Ordnungszahl 28. Es zählt zu den Übergangsmetallen, im Periodensystem steht es nach der älteren Zählweise in der 8. Nebengruppe oder Eisen-Platin-Gruppe, nach der neueren in der Gruppe 10 oder Nickelgruppe.

Nickel wurde erstmals 1751 von Axel Frederic Cronstedt rein dargestellt und nach dem Mineral "Kupfernickel" (schwedisch "kopparnickel", heute Nickelin) benannt, in dem er das bis dahin unbekannte Metall fand.

Mit der Bezeichnung "Kupfernickel" belegten die mittelalterlichen Bergleute das Erz, das zwar aussah wie Kupfererz, aus dem sich aber kein Kupfer gewinnen ließ, als sei es von Berggeistern („Nickeln“) verhext. Eine ähnlich koboldhafte Etymologie findet sich bei Cobalt.

Die erste Münze aus reinem Nickel wurde 1881 geprägt.

Nickel kommt in der Erdkruste mit einem Gehalt von etwa 0,008 % vor. Aufgrund geophysikalischer und geochemischer Indizien geht man davon aus, dass sich der größte Teil des Nickels auf der Erde und anderen terrestrischen Planeten im Kern befindet, wo es eine Legierung mit Eisen und einigen leichten Elementen bildet. Im Erdkern beträgt sein Massenanteil nach neuesten Modellen etwa 5,2 %.

Gediegen, das heißt in elementarer Form kommt Nickel nur selten vor. Bisher sind lediglich rund 50 Fundorte für gediegen Nickel dokumentiert (Stand 2018), so unter anderem in Australien, China, Kanada, Russland und den Vereinigten Staaten von Amerika.

Traditionell wird der größte Teil der Nickelproduktion aus sulfidischen Erzen wie Pentlandit (ca. 34 % Nickel), Nickelmagnetkies (Verwachsungsaggregat aus Pyrrhotin und Pentlandit) sowie einigen weiteren Nickelmineralen wie Millerit (ca. 64–65 % Nickel) und Nickelin (ca. 44 % Nickel) gewonnen. Darüber hinaus werden auch lateritische Nickelerze, vor allem aus Garnierit, einem Gemenge aus Népouit (ca. 46 % Nickel) und Willemseit (ca. 29 % Nickel), als Rohstoffe zur Nickelproduktion abgebaut. Insgesamt sind bisher rund 200 Nickelminerale bekannt, und einige haben noch weit höhere Nickelgehalte als die bereits genannten, kommen jedoch im Gegensatz zu diesen viel seltener vor. So ist beispielsweise der sehr seltene Bunsenit das Mineral mit dem höchsten Nickelgehalt von bis zu 78,58 %. Die ebenfalls seltenen Minerale Heazlewoodit und Awaruit enthalten zwischen 72 und 73 % Nickel.

Die Gewinnung verschiebt sich aufgrund der Ausbeutung der klassischen sulfidischen Lagerstätten zunehmend zu lateritischen Nickelerzen. Diese müssen jedoch aufwändig per Hochdruck-Säurelaugung () gewonnen werden.

Um das Nickel wirtschaftlich abbauen zu können, muss der Nickelgehalt des Erzes mindestens 0,5 % betragen. Die wichtigsten Vorkommen finden sich in Kanada (Sudbury-Becken), Neukaledonien, Russland (Norilsk und Halbinsel Kola), Australien (Queensland) und Kuba (Moa Bay und Nicaro). Ein häufiger Begleiter des Nickels ist Cobalt.

Natürlich vorkommendes Nickel in seiner elementaren Form wurde erstmals 1967 durch Paul Ramdohr beschrieben und von der International Mineralogical Association (IMA) als eigenständige Mineralart anerkannt (Interne Eingangs-Nr. der IMA: "1966-039").

Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Nickel unter der System-Nr. "1.AA.05" (Elemente – Metalle und intermetallische Verbindungen – Kupfer-Cupalit-Familie – Kupfergruppe) beziehungsweise in der veralteten 8. Auflage unter "I/A.08" ("Nickel-Reihe") eingeordnet. Die vorwiegend im englischsprachigen Raum verwendete Systematik der Minerale nach Dana führt das Element-Mineral unter der System-Nr. "01.01.11.05" ("Eisen-Nickelgruppe").

Als Typlokalität gilt die Halbinsel Bogota nahe Canala in der Nordprovinz von Neukaledonien, wo sich gediegen Nickel in Form idiomorpher kubischer Körner oder eingewachsener Würfel bis etwa 0,1 mm als Einschlüsse in Heazlewoodit sowie als „spinnenartige“ unregelmäßige Masse zwischen den Heazlewooditkörnern fand. Als Begleitminerale können neben Heazlewoodit unter anderem noch Chalkopyrit, Chalkosin, Galenit, Godlevskit, gediegen Kupfer, Millerit, Orcelit, Pentlandit, Pyrit und Pyrrhotin auftreten.

Der überwiegende Teil des Nickels wird aus nickel- und kupferhaltigen Eisenerzen wie Nickelmagnetkies gewonnen. Um die Gewinnung wirtschaftlich zu machen, muss das Nickel zunächst durch Flotation auf etwa fünf Prozent Nickelgehalt angereichert werden. Danach wird das Erz ähnlich wie bei der Kupferherstellung geröstet. Dabei wird das Erz zunächst vorgeröstet, um einen Teil des Eisensulfids in Eisenoxid umzuwandeln. Anschließend werden Silikate und Koks dazugegeben, um das Eisenoxid als Eisensilikat zu verschlacken. Gleichzeitig bildet sich der Kupfer-Nickel-Rohstein aus Nickel-, Kupfer und Eisensulfid. Da dieser spezifisch schwerer als die Eisensilicat-Schlacke ist, können die beiden Phasen getrennt abgestochen werden.

Anschließend wird der Rohstein in einen Konverter gefüllt und Siliciumdioxid dazugegeben. Es wird Sauerstoff eingeblasen. Dadurch wird das restliche Eisensulfid zu Eisenoxid geröstet und danach verschlackt. Es entsteht der "Kupfer-Nickel-Feinstein", der zu etwa 80 % aus Kupfer und Nickel und zu etwa 20 % aus Schwefel besteht.

Zur Gewinnung des Rohnickels muss das Nickel vom Kupfer abgetrennt werden. Dazu verschmilzt man den Feinstein mit Natriumsulfid NaS. Dabei bildet sich nur zwischen Kupfer- und Natriumsulfid ein leicht schmelzendes Doppelsulfid. Es bilden sich zwei einfach zu trennende Phasen aus Kupfer-Natrium-Doppelsulfid (flüssig) und Nickelsulfid. Nach der Abtrennung wird das Nickelsulfid zu Nickeloxid geröstet und danach mit Koks zu Nickel reduziert.

Um Reinnickel zu gewinnen, wird das Rohnickel elektrolytisch raffiniert. Dazu wird in einer Elektrolysezelle das Rohnickel als Anode, ein Nickelfeinblech als Kathode geschaltet. Als Elektrolyt dient eine Nickelsalzlösung. Während der Elektrolyse gehen an der Anode Nickel und alle unedleren Bestandteile in Lösung. Alle edleren Bestandteile bleiben fest und fallen als Anodenschlamm unter die Elektrode. Dieser dient als wichtige Quelle für die Herstellung von Edelmetallen, wie Gold oder Platin. An der Kathode werden Nickelionen aus der Lösung zu Nickel reduziert, alle unedleren Bestandteile bleiben in Lösung. Die Reinheit von Elektrolytnickel beträgt rund 99,9 %.

Für die Gewinnung von Reinstnickel mit einer Reinheit von 99,99 % gibt es als Spezialverfahren das Mond-Verfahren, benannt nach Ludwig Mond, der 1890 Nickeltetracarbonyl entdeckte. Dieses Verfahren beruht auf der Bildung und Zersetzung des Nickeltetracarbonyls. Dazu wird feinverteiltes Rohnickelpulver bei 80 °C in einen Kohlenmonoxidstrom gebracht. Dabei bildet sich gasförmiges Nickeltetracarbonyl. Dieses wird von Flugstaub befreit und in eine 180 °C heiße Zersetzungskammer geleitet. Darin befinden sich kleine Nickelkugeln. An diesen zersetzt sich das Nickeltetracarbonyl wieder zu Nickel und Kohlenmonoxid. Es entsteht dadurch sehr reines Nickel.

Nickel ist ein silbrig-weißes Metall, das mit einer Dichte von 8,91 g/cm zu den Schwermetallen zählt. Es ist mittelhart (Mohs-Härte 3,8), schmiedbar, duktil und lässt sich ausgezeichnet polieren. Nickel ist wie Eisen und Cobalt ferromagnetisch, wobei die Curie-Temperatur 354 °C beträgt. Das Metall kristallisiert in einer kubisch-flächenzentrierten Kristallstruktur (Kupfer-Typ) in der mit dem Gitterparameter a = 352,4 pm sowie vier Formeleinheiten pro Elementarzelle. Diese Struktur behält es auch bei hohen Drücken bis mindestens 70 GPa bei. Eine weitere, metastabile Modifikation mit kubisch-raumzentrierter Kugelpackung konnte in dünnen Schichten auf Eisen oder Galliumarsenid gewonnen werden. Sie besitzt mit 183 °C eine deutlich geringere Curie-Temperatur.

Die Zugfestigkeit von weichgeglühtem Nickel liegt bei 400–450 MPa bei einer Bruchdehnung zwischen 30 und 45 %. Die Härtewerte liegen um die 80 HB. Kalt verfestigtes Nickel, dessen Bruchdehnung unter 2 % beträgt, erreicht Festigkeiten bis 750 MPa bei Härtewerten um 180 HB. Reinnickelhalbzeuge mit 99 % Ni-Gehalt können kalt hochverfestigt werden.

Das Isotop Ni hat die höchste Bindungsenergie je Nukleon aller Isotope aller Elemente.

Nickel ist bei Raumtemperatur gegen Luft, Wasser, Salzsäure und Laugen sehr beständig. Verdünnte Säuren greifen Nickel nur sehr langsam an. Gegenüber konzentrierten, oxidierenden Säuren (Salpetersäure) tritt analog zum rostfreien Stahl Passivierung ein. Löslich ist Nickel in verdünnter Salpetersäure (ca. 10- bis 15-prozentig). Auch eine halbkonzentrierte Salpetersäure (ca. 30-prozentig) bewirkt noch merkliche Passivierung. Der häufigste Oxidationszustand ist +II, seltener werden −I, 0, +I, +III und +IV beobachtet. Im Nickeltetracarbonyl hat Nickel die Oxidationszahl 0. Nickel(II)-Salze lösen sich in Wasser unter Bildung von Aquakomplexen mit grünlicher Farbe.

Fein verteiltes Nickel reagiert mit Kohlenmonoxid bei 50 bis 80 °C zu Nickeltetracarbonyl, Ni(CO), einer farblosen, sehr giftigen Flüssigkeit. Diese dient als Zwischenprodukt zur Herstellung von reinstem Nickel nach dem Mond-Verfahren. Bei 180 bis 200 °C zerfällt Nickeltetracarbonyl wieder in Nickel und Kohlenmonoxid.

Der umstrittenen Essenzialität von Nickel steht die Existenz mehrerer Enzyme gegenüber, die im Normalfall Nickel enthalten, darauf aber nicht angewiesen sind, da dessen Rolle als Kation von anderen zweiwertigen Kationen übernommen werden kann. Im Menschen handelt es sich dabei um drei Proteine, von denen bekannt ist, dass sie Nickel binden:

Für Pflanzen und verschiedene Mikroorganismen ist die Essenzialität von Nickel durch Isolierung mehrerer Enzyme (z. B. Urease, Co-F430), die Nickel im aktiven Zentrum enthalten, sowie durch Nachweis von Mangelerscheinungen in nickelarmer Umgebung, die sich durch Zusatz von Ni(II)-Salzen beheben lassen, gesichert.

In der Elektrophysiologie werden Nickel-Ionen dazu verwendet, spannungsaktivierte Calciumkanäle zu blockieren.
Nickel ist mit der Nickeldermatitis der häufigste Auslöser für Kontaktallergien: in Deutschland sind schätzungsweise 1,9 bis 4,5 Millionen Menschen gegen Nickel sensibilisiert. Deswegen werden Metalle und Legierungen, die mit der Haut in Kontakt kommen, zunehmend seltener vernickelt. Etwa 10 % aller Kinder sind gegenüber Nickel sensibilisiert. Bei erneutem Kontakt mit dem Allergen können diese mit einer Kontaktallergie reagieren.

Das Einatmen anorganischer Nickelverbindungen ist mit einem erhöhten Krebsrisiko für Plattenepithelkarzinome der Lunge und der oberen Luftwege verbunden. Derartige bösartige Neubildungen werden in Deutschland bei berufsbedingter Exposition als Berufskrankheiten anerkannt (BK 4109).
Außerdem ist ein erhöhter Nickelgehalt in der Atemluft und im Trinkwasser ein Risikofaktor für eine Sensibilisierung gegen Nickel bei Kindern.

Die Verwendung von Nickel in Bedarfsgegenständen (wie Armbanduhren, Spielzeug, Geräten zur Lebensmittelverarbeitung usw.) ist in der Europäischen Union per Verordnung limitiert. Diese ist in Deutschland durch die Bedarfsgegenständeverordnung umgesetzt, in welcher Grenzwerte zur Freisetzung festgelegt sind.

Nickel wird als Metall in geringen Mengen benötigt, der größte Teil der Produktion geht in die Produktion von nichtrostenden Stählen und Nickellegierungen. Die Reserven an nach heutigen Gesichtspunkten abbauwürdigen Nickelvorkommen liegen zwischen 70 und 170 Millionen Tonnen. Gegenwärtig werden weltweit jährlich weit mehr als eine Million Tonnen (2006: 1,340 Mio. Tonnen) gefördert. Der Preis für Nickel unterliegt wegen Finanzmarktspekulationen zeitweise sehr hohen Preisschwankungen.

Reines Nickelmetall wird in feinverteilter Form als Katalysator bei der Hydrierung ungesättigter Fettsäuren verwendet. Auf Grund seiner chemischen Beständigkeit wird Nickel für Apparate im chemischen Labor und der chemischen Industrie verwendet (z. B. Nickeltiegel für Aufschlüsse). Aus Nickelmetall werden Nickellegierungen, z. B. für Münzen, hergestellt.

Nickel dient als Überzugsmetall zum Korrosionsschutz („Vernickeln“) von Metallgegenständen: Wegen seiner vor Oxidation schützenden Eigenschaften werden Metalle (insbesondere Eisen) mittels galvanischer Technik für bestimmte technische Zwecke mit einer Nickelschicht überzogen.

Ebenfalls genutzt wurde das Metall früher zur Herstellung der Fassungen von Nickelbrillen.

Als Betastrahler wird das Nickelisotop Ni in Elektroneneinfangdetektoren in Gaschromatografen eingesetzt.

Nickel ist ein bedeutendes Legierungsmetall, das hauptsächlich zur Stahlveredelung verwendet wird. Der größte Teil des Nickels geht dorthin. Es macht Stahl korrosionsbeständig und erhöht seine Härte, Zähigkeit und Duktilität. Mit Nickel hochlegierte Stähle werden bei besonders korrosiven Umgebungen eingesetzt. Der Edelstahl V2A (der Name entstammt der "Versuchscharge 2 austenitisch" im Krupp-Stahlwerk, entspricht X12CrNi18-8) enthält 8 % Nickel neben 18 % Chrom, V4A (Markennamen Cromargan oder Nirosta) 11 % neben 18 % Chrom und 2 % Molybdän.

Etwa 20 % des Nickels werden (in Deutschland) zur Herstellung von weiteren Nickellegierungen verwendet:

Die Nachweisreaktion für die in Wasser meist mit grüner Farbe löslichen Nickel(II)-salze wird in der quantitativen Analytik gravimetrisch sowie qualitativ im Kationentrenngang mit Dimethylglyoxim-Lösung (Tschugajews Reagens) durchgeführt. Nickelsalze werden zuvor gegebenenfalls durch Ammoniumsulfid als grauschwarzes Nickel(II)-sulfid ausgefällt und in Salpetersäure gelöst. Der spezifische Nachweis ist dann durch Reaktion mit Dimethylglyoxim in ammoniakalischer Lösung möglich. Dabei fällt das himbeerrote Bis(dimethylglyoximato)nickel(II) als Komplex aus:

formula_2.

Da Nickel aus ammoniakalischer Lösung mit Dimethylglyoxim quantitativ ausfällt, ist dieser Nachweis auch für die quantitative gravimetrische Nickelanalyse verwendbar. Aus ammoniakalischer Lösung kann auch mittels Elektrogravimetrie an einer Platinnetzelektrode eine quantitative Bestimmung erfolgen. Ähnlich anderen Schwermetallen wird Nickel heute meist durch Atomspektroskopie oder Massenspektrometrie auch im Ultraspurenbereich quantitativ bestimmt. Äußerst empfindlich ist die inverse Voltammetrie mit adsorptiver Anreicherung des Ni-Dimethyglyoxim-Komplexes an hängenden Quecksilbertropfen oder Quecksilberfilmelektroden.

Nickel kommt in Verbindungen hauptsächlich in der Oxidationsstufe +II vor. Die Stufen 0, +I, +III und +IV sind selten und meist instabil. Nickel bildet eine Vielzahl meist farbiger Komplexe.


Nickeltetracarbonyl Ni(CO) ist eine farblose, sehr giftige Flüssigkeit. Sie ist ein wichtiges Zwischenprodukt im Mond-Verfahren. Nickeltetracarbonyl war die erste entdeckte Metallcarbonyl-Verbindung.

Nickel und dabei v. a. Nickel(II)-Ionen bildet viele, meist farbige Komplexe. Die Koordinationszahlen 6, 5 oder 4 sind am häufigsten. Bei schwachen, einzähnigen Liganden, beispielsweise Wasser, liegen sie meist als oktaedrische und paramagnetische High-spin-Komplexe mit Koordinationszahl 6 vor. Starke Liganden wie Cyanid bilden quadratisch-planare, diamagnetische Low-spin-Komplexe. Ebenfalls einen quadratisch-planaren Komplex bildet Dimethylglyoxim, da der Komplex zusätzlich durch Wasserstoffbrücken stabilisiert ist. Letzterer Bis(dimethylglyoximato)nickel(II)-Komplex ist für den nasschemischen Nickelnachweis von Bedeutung.
Anionische Nickelkomplexe enden auf "-niccolat".

Beispiele für Amminkomplexe sind der blaue Tetraamminnickel(II)- und violette Hexaamminnickel(II)-komplex. Beide Verbindungen erhält man durch Zugabe von Ammoniak zu Nickel(II)-salzlösungen:
Durch Zugabe von Kaliumcyanid zu Nickel(II)-salzlösungen entsteht zunächst Nickel(II)-cyanid, das sich im Überschuss von Kaliumcyanid zu gelbem Kaliumtetracyanoniccolat(II) auflöst:
Eine entsprechende Verbindung entsteht mit Kaliumthiocyanat. Eine sehr empfindliche Verbindung ist das Kaliumhexafluoroniccolat(IV) (K[NiF]).
Mit einem starken Reduktionsmittel lässt sich aus Kaliumtetracyanoniccolat(II) der zweikernige Komplex K[Ni(CN)] mit einwertigem Nickel herstellen.
Daneben existiert eine Vielzahl von Komplexen mit organischen Liganden wie beispielsweise Ethylendiamin oder Anionen von Carbonsäuren.




</doc>
<doc id="3523" url="https://de.wikipedia.org/wiki?curid=3523" title="Neutron">
Neutron

Das Neutron [] (Plural "Neutronen" []) ist ein elektrisch neutrales Teilchen mit dem Formelzeichen n. Es ist, neben dem Proton, Bestandteil der meisten Atomkerne und somit aller uns vertrauten Materie. Beide gehören zu den Hadronen und Nukleonen.

Neutronen existieren auch ohne Einbindung in einen Atomkern. In diesem Zustand werden sie als freie Neutronen bezeichnet und sind instabil.

Das Neutron trägt keine elektrische Ladung (daher der Name); es hat aber ein magnetisches Moment von −1,91 Kernmagnetonen. Es hat eine Masse von rund 1,675 · 10 kg (1,008 665 u). Es gehört zu den Baryonen und ist aus Quarks aufgebaut – zwei down-Quarks und einem up-Quark (Formel udd). Das Neutron hat den Spin 1/2 und ist damit ein Fermion. Als zusammengesetztes Teilchen ist es räumlich ausgedehnt und hat einen Durchmesser von circa 1,7 · 10 m, also 1,7 Femtometer (fm).

Das Antiteilchen zum Neutron ist das Antineutron, das erstmals 1956 von Bruce Cork am Bevatron bei Proton-Proton-Kollisionen nachgewiesen wurde.

Das Neutron unterliegt allen in der Physik bekannten vier Wechselwirkungen: der Gravitationskraft, der starken, der elektromagnetischen und der schwachen Wechselwirkung. 

Die starke Wechselwirkung – genauer: die „Kernkraft“, eine Art Restwechselwirkung der zwischen den Quarks wirkenden starken Wechselwirkung – ist dafür verantwortlich, dass Neutronen in Kernen gebunden sind, und bestimmt auch das Verhalten von freien Neutronen bei Stößen mit Atomkernen.

Das Neutron ist zwar elektrisch neutral und unterliegt damit nicht der elektrostatischen Anziehung oder Abstoßung, aber aufgrund seines magnetischen Moments trotzdem der elektromagnetischen Wechselwirkung. Diese Tatsache sowie die räumliche Ausdehnung sind klare Indizien dafür, dass das Neutron ein zusammengesetztes Teilchen ist. 

Die schwache Wechselwirkung ist verantwortlich für den Betazerfall des freien Neutrons in ein Proton, ein Elektron und ein Elektron-Antineutrino.

Das Neutron hat eine mit 939,6 MeV um etwa 1,3 MeV (0,14 %) größere Ruheenergie als das Proton. Das Neutron ist radioaktiv und zerfällt als Beta-Minus-Strahler (β-Strahler) in ein Proton, ein Elektron und ein Elektron-Antineutrino:

Die Lebensdauer beträgt ca. 880 Sekunden (knapp 15 Minuten); dies entspricht einer Halbwertszeit von etwa 610 Sekunden. Das ist die mit Abstand größte Halbwertszeit aller instabilen Hadronen. Sie ist schwierig zu messen, denn ein in normaler materieller Umgebung freigesetztes Neutron (auch in Luft) wird meist in Sekundenbruchteilen wieder von einem Atomkern absorbiert, „erlebt“ seinen Zerfall also nicht. Dementsprechend ist der Zerfall bei praktischen Anwendungen bedeutungslos, und das Neutron kann dafür als stabiles Teilchen angesehen werden. Grundlagenphysikalisch ist der Zerfall jedoch interessant. In einer frühen Phase des Universums machten freie Neutronen einen bedeutenden Teil der Materie aus; man kann die Entstehung besonders der leichten Elemente (und deren Isotopenverteilung) besser nachvollziehen, wenn die Lebensdauer des Neutrons genau bekannt ist. Außerdem erhofft man sich ein besseres Verständnis der schwachen Wechselwirkung.

Die Umkehrung des Neutronenzerfalls tritt auf, wenn ein protonenreicher Atomkern mit einem Elektron der Atomhülle reagiert (Elektroneneinfang), sowie unter den extremen Bedingungen bei der Entstehung eines Neutronensterns:

Mit Ausnahme des häufigsten Wasserstoffisotops (Protium, H), dessen Atomkern nur aus einem einzelnen Proton besteht und das den normalen Wasserstoff bildet, enthalten alle Atomkerne sowohl Protonen als auch Neutronen. Protonen und Neutronen werden zusammenfassend Nukleonen (von lateinisch "nucleus", Kern) genannt.

Die Anzahl der Protonen im Atomkern bestimmt die Kernladung, und damit die Zahl der Elektronen, die im neutralen Atom gebunden sind. Die Elektronenhülle wiederum bestimmt die chemischen Eigenschaften des Atoms. Deswegen gehören sogenannte Isotope, also die Atomsorten mit gleicher Protonenzahl Z aber unterschiedlicher Neutronenzahl N, zum gleichen chemischen Element. Ein Neutron und ein Proton haben jeweils fast die gleiche Masse, die rund 2000-mal höher als die Masse eines Elektrons ist. Die Nukleonenzahl A=Z+N, also die Gesamtzahl der Protonen und Neutronen eines Atomkerns, wird deswegen auch als Massezahl bezeichnet. Sie stimmt etwa mit der Gesamtmasse des Atoms in u überein. Wenn einzelne Isotope unterschieden werden sollen, wird die Massezahl meist als Hochzahl vor oder mit einem Bindestrich hinter das Elementsymbol geschrieben, z. B. Ca oder Ca-40 und Pb oder Pb-208. Um aus der Massezahl die Neutronenanzahl zu ermitteln, muss die Ordnungszahl (Protonenanzahl) des Elements abgezogen werden. Blei hat z. B. die Ordnungszahl 82; der Blei-208-Kern enthält also 208−82=126 Neutronen.

Wie stark ein Atomkern gebunden ist, hängt von der Zahl der Protonen und Neutronen, vor allem aber von dem Verhältnis dieser Zahlen ab. Bei leichteren Kernen ist die Bindung bei etwa gleicher Anzahl (N/Z≈1) am stärksten (z. B. ist Ca mit je 20 Protonen und Neutronen der stabilste Kern mit Massenzahl 40), bei sehr schweren Kernen verschiebt sich das Verhältnis bis hin zu N/Z≈1,5, z. B. in Pb, weil mit wachsendem Z die elektrische Abstoßung der Protonen zunehmend destabilisierend wirkt. Dieser Unterschied in der Bindungsenergie wirkt sich stärker aus als der eher geringe Massenunterschied von Proton und Neutron, so dass von Kernen gleicher Massenzahl diese jeweils am stabilsten sind. Ein Kern mit „zu vielen“ Neutronen kann sich – wie das freie Neutron – durch β-Zerfall in einen Kern gleicher Massenzahl umwandeln, der ein Neutron weniger und ein Proton mehr hat. Vereinfachend gesagt hat sich ein Neutron in ein Proton umgewandelt. Hat ein Kern „zu wenige“ Neutronen, kann er sich durch β-Zerfall in einen Kern umwandeln, der ein Neutron mehr und ein Proton weniger hat. Man kann dies als Umwandlung eines Protons in ein Neutron interpretieren, ein Vorgang, der nicht bei freien Protonen möglich ist. 

Es gibt viele verschiedene Arten von Neutronenquellen, in denen Neutronen aus Atomkernen freigesetzt werden.

Zur Untersuchung von kondensierter Materie durch elastische und inelastische Neutronenstreuung werden vor allem Neutronen aus Forschungsreaktoren genutzt. Dort werden die Neutronen bei der Kernspaltung frei. Diese schnellen Neutronen haben Energien im Bereich von einigen MeV und müssen für Materialuntersuchungen erst auf rund ein Millionstel ihrer Bewegungsenergie abgebremst werden. Eine neuere Alternative zu Forschungsreaktoren sind Spallationsquellen. 

Auch Gewitter können schnelle Neutronen mit Energien bis zu 30 MeV produzieren. Der Fluss dieser Neutronen liegt zwischen 10 und 10 pro ms und pro m je nach Detektionshöhe. Die Energie der meisten dieser Neutronen, selbst mit anfänglichen Energien von 20 MeV, sinkt binnen 1 ms in den keV-Bereich.

Die Abbremsung von Reaktorneutronen geschieht in einem den Reaktorkern umgebenden Wassertank (leichtes oder schweres Wasser als Moderator). Bei jedem Zusammenstoß eines schnellen Neutrons mit einem Atomkern des Wassers wird Energie an den getroffenen Kern abgegeben. Nach vielen solchen Stößen weisen die Neutronen ein der Wassertemperatur (etwa 300 K, „Zimmertemperatur“) entsprechendes Energiespektrum auf. Man spricht dann von thermischen Neutronen (siehe untenstehende Tabelle).

Mit zusätzlichen Moderatoren kann das Energiespektrum der Neutronen verschoben werden. Diese zusätzlichen Moderatoren an Forschungsreaktoren bezeichnet man auch als sekundäre Neutronenquellen. Zur Gewinnung von sogenannten kalten Neutronen kommt häufig flüssiges Deuterium mit einer Temperatur von etwa 20 K zum Einsatz. Sogenannte heiße Neutronen werden in der Regel mit Graphit-Moderatoren bei etwa 3000 K erzeugt. Kalte, thermische und heiße Neutronen weisen jeweils eine bestimmte, mehr oder weniger breite Energieverteilung und damit Wellenlängenverteilung auf. 

In jedem Fall werden die Neutronen durch Strahlrohre (Neutronenleiter) aus dem Moderatortank oder den sekundären Neutronenquellen zu den Experimenten geleitet. Allerdings müssen noch genügend viele Neutronen im Reaktorkern verbleiben oder dorthin zurück reflektiert werden, um die Kettenreaktion aufrechtzuerhalten.

"Ultrakalte" Neutronen (UCN) besitzen nur eine sehr geringe kinetische Energie und bewegen sich mit weniger als 5 m/s, so dass sie sich magnetisch, mechanisch oder gravitativ speichern lassen. Von Gefäßwänden aus Beryllium, Berylliumoxid oder Nickel werden sie reflektiert. Speicherexperimente ermöglichen minutenlange Beobachtungsdauern, viel länger als bei Experimenten an Neutronenstrahlen.

Für viele Experimente werden monoenergetische Neutronen, also Neutronen einheitlicher Energie, benötigt. Diese erhält man an Reaktoren z. B. durch den Einsatz eines Monochromators. Dies ist ein Einkristall oder Mosaik-Kristall aus beispielsweise Silizium, Germanium, Kupfer oder Graphit; durch Nutzung bestimmter Bragg-Reflexe und Monochromatorwinkel können verschiedene Wellenlängen (Energien) aus der Wellenlängenverteilung ausgewählt werden (siehe auch Neutronensuperspiegel).

Monochromatische Neutronen höherer Energien können an Beschleunigern aus geeigneten Kernreaktionen gewonnen werden.

Die Wirkungen freier Neutronen auf Materie sind je nach der kinetischen Energie der Neutronen ganz verschieden. Deshalb werden Klassen oder Bereiche der Neutronenenergie mit besonderen Bezeichnungen belegt. Die Bezeichnungen werden nicht ganz einheitlich verwendet. Ein Beispiel ist folgende Liste:


Die breiteren dieser Bereiche werden von Spezialisten noch weiter unterteilt, z. B. der Bereich unterhalb der thermischen Energie in "kalte", "sehr kalte" und "ultrakalte" Neutronen.

Da Neutronen keine elektrische Ladung tragen, können sie nicht direkt mit auf Ionisierung beruhenden Detektoren nachgewiesen werden. Der Nachweis von Neutronen geschieht mittels Neutronendetektoren. Bei niedrigen Neutronenenergien (unter etwa 100 keV) beruhen diese stets auf einer geeigneten Kernreaktion. Bei höheren Energien kann stattdessen auch der Rückstoß ausgenutzt werden, den ein geladenes Teilchen (meist Proton) bei der Streuung des Neutrons erfährt.

"Freie" (also nicht in einem Kern gebundene) Neutronen können an Atomkernen gestreut werden oder von ihnen absorbiert werden, was eine Kernreaktion darstellt.

Die Streuung kann elastisch oder inelastisch sein. Bei inelastischer Streuung verbleibt der Atomkern in einem angeregten Zustand, der dann (meist) durch Emission von Gammastrahlung zum Grundzustand zurückkehrt. Die elastische Streuung schneller Neutronen an leichten Atomkernen (Moderatoren) bewirkt ihre Abbremsung, bis sie zu thermischen Neutronen werden.

Insbesondere thermische Neutronen werden von vielen Atomkernen absorbiert. Wird danach nur Gammastrahlung, aber kein Teilchen mit Masse emittiert, heißt der Vorgang Neutroneneinfang. Der entstandene neue Atomkern ist ein um eine Einheit schwereres Isotop des ursprünglichen Kerns und kann radioaktiv sein (Neutronenaktivierung). Als Neutronenabsorber werden Nuklide mit besonders großen Wirkungsquerschnitten für die Absorption eines thermischen Neutrons bezeichnet. Die meist verwendeten sind Cadmium-113 und Bor-10; sie werden in Neutronenabschirmungen und zur Steuerung von Kernreaktoren verwendet. 

Bei einigen sehr schweren Nukliden kann die Absorption eines Neutrons zur Kernspaltung führen. Da diese wiederum Neutronen freisetzt, kann der Vorgang u. U. als Kettenreaktion mit Freisetzung großer Energiemengen verlaufen. Dies wird in Kernreaktoren und auch in Kernwaffen ausgenutzt.

Die Materialeigenschaften von Metallen und anderen Werkstoffen werden durch Neutronenbestrahlung verschlechtert. Dies begrenzt die Lebensdauer von Komponenten in z. B. Kernreaktoren. In eventuellen Kernfusionsreaktoren mit ihrer höheren Energie der Neutronen träte dieses Problem verstärkt auf.

Die Wirkung auf lebendes Gewebe ist ebenfalls schädlich. Sie beruht bei schnellen Neutronen größtenteils auf von diesen angestoßenen Protonen, die einer stark ionisierenden Strahlung entsprechen. Diese Schadwirkung ist gelegentlich als Strahlentherapie zur Bekämpfung von Krebszellen erprobt worden. Thermische Neutronen erzeugen durch Neutroneneinfang in Wasserstoff eine Gammastrahlung, die ihrerseits ionisiert.

Ernest Rutherford sagte im Jahr 1920 einen neutralen Kernbaustein voraus, bei dem es sich möglicherweise um eine Proton-Elektron-Kombination handele, er sprach von einem „kollabierten Wasserstoffatom“. William Draper Harkins bezeichnete dieses Teilchen 1921 als Neutron.

Die ersten Schritte zur Entdeckung des Neutrons wurden von Walther Bothe und seinem Studenten Herbert Becker getan. Sie beschrieben im Jahr 1930 einen ungewöhnlichen Typ von Strahlung, der entstand, wenn sie Beryllium mit Alphastrahlung aus dem radioaktiven Zerfall von Polonium beschossen. Ziel war es, Beobachtungen Ernest Rutherfords zu bestätigen, wonach bei diesem Vorgang eine sehr energiereiche Strahlung emittiert wurde. Dementsprechend hielten sie die durchdringende Strahlung, die sie bei diesen Versuchen mit Hilfe von elektrischen Zählmethoden feststellen konnten, anfänglich fälschlicherweise für Gammastrahlung. Die gleichen Versuche machten sie auch mit Lithium und Bor, und kamen schlussendlich zum Ergebnis, dass die beobachteten „Gammastrahlen“ mehr Energie besaßen als die Alphateilchen, mit denen sie die Atome beschossen hatten. Bei der Bestrahlung von Beryllium mit Alphateilchen entstand nicht – wie zuvor erwartet – Bor, sondern Kohlenstoff. In heutiger Schreibweise lautet die beobachtete Kernreaktion:

oder in Kurzform 

Die beobachtete, sehr energiereiche Strahlung hatte ein großes Durchdringungsvermögen durch Materie, zeigte jedoch sonst ein für Gammastrahlung ungewöhnliches Verhalten. Sie vermochte zum Beispiel leichte Atome in schnelle Bewegung zu versetzen. Eine genauere Analyse zeigte, dass die Energie dieser „Gammastrahlung“ so groß hätte sein müssen, dass sie alles bis dahin Bekannte weit übertroffen hätte. So kamen mehr und mehr Zweifel auf, ob es sich wirklich um Gammastrahlen handelte. Entsprechend dem durchgeführten Versuch nannte man die Strahlung inzwischen „Beryllium-Strahlung“.

1931 stellten Irène Joliot-Curie und ihr Ehemann Frédéric Joliot-Curie bei Experimenten mit der Beryllium-Strahlung folgende Tatsache fest: Lässt man die „Beryllium-Strahlung“ in eine Ionisationskammer treffen, so zeigt diese keinen nennenswerten Strom an. Bringt man jedoch vor die Ionisationskammer eine wasserstoffhaltige Materialschicht (zum Beispiel Paraffin), dann steigt der Strom in der Kammer stark an. Als Ursache vermutete das Ehepaar Joliot-Curie, dass die „Beryllium-Strahlung“ aus dem wasserstoffhaltigen Paraffin Protonen herauslöst, welche dann in der Ionisationskammer Ionisierung bewirken. Sie konnten ihre Vermutung durch den Nachweis solcher Rückstoß-Protonen in der Wilsonschen Nebelkammer belegen. Als Mechanismus vermuteten sie einen dem Compton-Effekt verwandten Vorgang. Die harte Gammastrahlung sollte den Protonen den notwendigen Impuls übertragen. Abschätzungen zeigten jedoch, dass zur Erzeugung eines Rückstoßprotons, dessen Spurlänge in der Nebelkammer etwa 26 cm betrug, eine unrealistisch hohe Gammaenergie von etwa 50 MeV notwendig wäre.

James Chadwick – ein Schüler Rutherfords, der wie er zunächst die Hypothese eines stark gebundenen Elektron-Proton-Zustands vertrat – glaubte wie dieser nicht an einen „Compton-Effekt beim Proton“ und nahm an, dass die „Beryllium-Strahlung“ aus Teilchen bestehen müsse. Als Irène und Frédéric Joliot-Curie ihre Versuchsergebnisse veröffentlichten, in denen sie zeigten, dass Bothes „Beryllium-Strahlung“ in der Lage war, aus Paraffin Protonen mit hoher Energie herauszuschlagen, war für Chadwick klar, dass es sich nicht um Gammastrahlung, sondern nur um Teilchen mit einer dem Proton vergleichbaren Masse handeln konnte. In den zahlreichen Versuchen wiederholte er die Experimente von Joliot-Curie und bestätigte deren Beobachtung. 1932 konnte er experimentell erhärten, dass es sich bei der „Beryllium-Strahlung“ nicht um Gammastrahlen, sondern um schnell bewegte Teilchen handelte, die ungefähr die Masse des Protons besitzen, jedoch elektrisch neutral sind; die Eigenschaften dieser Strahlung waren eher mit denen eines bereits zwölf Jahre zuvor von Ernest Rutherford als Kernbaustein vermuteten neutralen Teilchens in Einklang zu bringen. Da die nunmehr entdeckten Teilchen keine elektrische Ladung trugen, nannte er sie Neutronen. Chadwick veröffentlichte seine Entdeckung im Jahr 1932. Die Publikation erschien unter "Letters to the Editor", ist knapp eine Seite lang und trug ihm im Jahre 1935 den Nobelpreis für Physik ein. 

Dass gerade die Kombination von Beryllium als Target und Polonium als Alphateilchen-Quelle eine hohe Neutronenausbeute ergibt, erklärt sich nach heutigem Wissen daraus, dass der Energiegewinn (Q-Wert) der formula_5-Reaktion an formula_6 mit 5,7 MeV besonders hoch ist und dass Po mit 5,3 MeV eine der höchsten natürlichen Alphaenergien liefert.

Mit der Entdeckung des Neutrons konnte die Beschreibung des Atomaufbaus vorerst vollendet werden: Der Atomkern, bestehend aus Protonen und Neutronen, wird von einer Hülle aus Elektronen umgeben. Bei einem elektrisch neutralen Atom ist die Anzahl der negativ geladenen Elektronen gleich der der positiv geladenen Protonen im Atomkern, wohingegen die Anzahl der Neutronen im Kern variieren kann.

Im gleichen Jahr 1932 stellte Werner Heisenberg seine Nukleonentheorie auf.

Noch 1940 nahm man an, dass das Neutron eine Verbindung aus Proton und Elektron darstellt. So hätte man alle Atome auf diese zwei Bausteine zurückführen können. Erst mit der weiteren Entwicklung der Quantenmechanik und der Kernphysik wurde klar, dass es keine Elektronen als dauerhafte Bestandteile des Kerns geben kann.

„Neutron“ war ursprünglich Wolfgang Paulis Bezeichnung für das 1930 von ihm postulierte Auftreten eines (Anti-)Neutrinos beim Betazerfall gewesen. Die Bezeichnung Neutrino, vorgeschlagen von Enrico Fermi, etablierte sich erst später.





</doc>
<doc id="3525" url="https://de.wikipedia.org/wiki?curid=3525" title="Nupedia">
Nupedia

Nupedia war eine freie, kommerzielle Online-Enzyklopädie in größtenteils englischer Sprache und die Vorläuferin der Wikipedia. Die verwendeten Programme "NupeCode" und der Nachfolger "nunupedia" stehen wie die Wikipedia-Software MediaWiki unter der GNU General Public License.

Ins Leben gerufen wurde Nupedia im März 2000 von Jimmy Wales und Larry Sanger. Finanziert wurde das Projekt von Bomis, einem Online-Anbieter für Foren zu den Themen Unterhaltung, Sport, Science-Fiction, Erotik und Pornografie.

Nupedia hatte ein aufwendiges, siebenstufiges Peer-Review-Verfahren, das einen hohen Qualitätsstandard der Artikel sicherstellen sollte. Nupedia entwickelte sich sehr langsam. Der erste Eintrag im Artikelnamensraum wurde am 25. Juli 2000 veröffentlicht. Bis November 2000 kam lediglich ein weiterer Artikel in voller Länge hinzu, nach 18 Monaten waren es etwa 20. Als das Projekt im September 2003 eingestellt wurde, waren lediglich 25 Artikel fertiggestellt.

Für die Nupedia erfolgte die Erarbeitung der Artikel mit einem kleineren Kreis von Fachautoren, und vor der Veröffentlichung wurden alle Artikel einer ausführlichen Prüfung unterworfen. Die Artikel unterlagen anfangs der eigenen "Nupedia Open Content License"; im Januar 2001 wechselte man auf Drängen von Richard Stallman zur GNU-Lizenz für freie Dokumentation. Dieser startete aber zu gleicher Zeit das GNUPedia-Projekt, wodurch Befürchtungen einer möglichen Konkurrenz der beiden Projekte geweckt wurden. Da Nupedia einen sehr bürokratischen Arbeitsablauf hatte, waren die Artikel zwar von hoher Qualität, aber das Projekt wuchs nur langsam.

2001 startete Jimmy Wales das Wikipedia-Projekt. Wikipedia war ursprünglich nur als Vorstufe für Nupedia-Artikel gedacht, zog aber viele Akteure an und entwickelte eine große Eigendynamik. Im ersten Monat wurden 200 Artikel erstellt, im ersten Jahr waren es 18.000. Der Erfolg führte nicht nur zur Beendigung des GNUPedia-Projektes, sondern auch von Nupedia. Bomis beendete seine Unterstützung im Januar 2002 und Larry Sanger trat kurz darauf sowohl aus dem Nupedia- als auch aus dem Wikipedia-Projekt aus. In der Folgezeit wurden nur noch zwei Artikel fertiggestellt. Das Projekt wurde im September 2003 beendet. Ein Teil der Artikel wurde in die Wikipedia übernommen.

Die Domain nupedia.com, unter der das Projekt früher erreichbar war, wurde inzwischen abgeschaltet.




</doc>
<doc id="3527" url="https://de.wikipedia.org/wiki?curid=3527" title="Newton (Einheit)">
Newton (Einheit)

</math>
Das Newton [] (Einheitenzeichen N) ist die im internationalen Einheitensystem (SI) für die physikalische Größe Kraft verwendete Maßeinheit. Sie wurde im Jahre 1946 von der Generalkonferenz für Maß und Gewicht im heutigen Sinn festgelegt und 1948 von ihr nach dem englischen Wissenschaftler Isaac Newton benannt.

Das Newton ist eine abgeleitete Einheit, sie kann durch die Basiseinheiten Kilogramm (kg), Meter (m) und Sekunde (s) ausgedrückt werden:

Ein Newton ist die Größe der Kraft, die aufgebracht werden muss, um einen ruhenden Körper der Masse 1 kg innerhalb einer Sekunde gleichförmig auf die Geschwindigkeit formula_4 zu beschleunigen.

Oder:

Ein Newton ist die Größe der Kraft, die aufgebracht werden muss, um einem Körper der Masse 1 kg die Beschleunigung formula_5 zu erteilen.

Oder:

Ein Newton ist die Größe der Kraft, die aufgebracht werden muss, um bei einer geradlinigen Bewegung die Geschwindigkeit eines Körpers der Masse 1 kg jede Sekunde um formula_4 zu ändern.

Wenn man die Gewichtskraft eines Objektes in einem Schwerefeld angeben will (was üblicherweise in Newton erfolgt), ist das zu unterscheiden von der Masse des Objektes, die in Kilogramm angegeben wird. Als Faustregel gilt: 1 kg entspricht auf der Erdoberfläche etwa 10 N, diese Größen sind aber, so wie deren Einheiten kg und N, grundverschieden. Da die mittlere Erdbeschleunigung (der sog. Ortsfaktor) auf Meereshöhe formula_7 beträgt, erfährt ein Körper der Masse 1 kg dort eine Gewichtskraft von 9,81 N. Umgekehrt ist 1 Newton die Gewichtskraft, die auf einen Körper mit der Masse 102 Gramm wirkt.

Maximalbelastungen von Fußböden werden oft in Newton oder Newton pro Quadratmeter angegeben. Auch die Reißfestigkeit von Seilen (z. B. Abschleppseilen) oder die Belastungsgrenze von Kränen wird oft in Newton angegeben – oder sie wurde mittels obiger Faustformel in eine zulässige Masse umgerechnet: Man teilt die zur Krafteinheit Newton gehörende Maßzahl durch 10, um die zur Masseneinheit Kilogramm gehörende Maßzahl zu erhalten:

Das Einheitenzeichen kann mit den üblichen Vorsätzen für Maßeinheiten kombiniert werden. Gebräuchlich sind:



</doc>

<doc id="1796" url="https://de.wikipedia.org/wiki?curid=1796" title="Glaube">
Glaube

Das Wort Glaube (auch Glauben; lateinisch "fides"; indogermanisch "leubh" ‚begehren‘, ‚lieb haben‘, ‚für lieb erklären‘, ‚gutheißen‘, ‚loben‘) bezeichnet hier eine Grundhaltung des Vertrauens, v. a. im Kontext religiöser Überzeugungen.

Während der ähnliche Begriff „Religiosität“ die Ehrfurcht vor der Ordnung und Vielfalt in der Welt und die "allgemeine" Empfindung einer transzendenten (nicht erklär- oder beweisbaren) Wirklichkeit meint, beinhaltet „Glaube“ das Überzeugtsein von der Lehre einer "konkreten" Religion (oder Philosophie).

Das deutsche Wort "Glaube" wird in dem hier behandelten Sinn verwendet als Übersetzung des griechischen Substantivs πίστις "pistis" mit der Grundbedeutung „Treue, Vertrauen“. Das zugehörige Verb lautet πιστεύω "pisteúō" „ich bin treu, vertraue“ (πιστεύειν "pisteúein", „treu sein, vertrauen“). Ursprünglich gemeint war also: „Ich verlasse mich auf ..., ich binde meine Existenz an ..., ich bin treu zu ...“. Das Wort zielt demnach auf Vertrauen, Gehorsam (vergleiche: Gelöbnis, Verlöbnis), Treue.

Das lateinische Wort "credere" (vgl. Credo und Kreditor) – von "cor dare": „das Herz geben/schenken“ – ist direkt verwandt mit der altindischen Wurzel "sraddha-" („glauben“) und ist eine sehr alte (indogermanische) Verbalkomposition. Die Bestandteile bedeuten: „Herz“ und „setzen, stellen, legen“, zusammen also etwa „sein Herz (auf etwas) setzen“. Das unbestimmte „ich weiß nicht“ entspricht hingegen dem lateinischen Wort "putare" („glauben, dass“).

Im Hebräischen gibt es die Vokabel "aman": sich an etwas festmachen. Die Vokabel "aman" mit der Schreibung „"Aleph-Mem-Nun"“ wird nur in der Stammesmodifikation des "Hif'il" (Aussprache „hä'ämin“) mit dem Wort „glauben“ übersetzt. Diese Stammesmodifikation drückt im Allgemeinen einen kausativen Aspekt der Grundbedeutung aus. Die Grundbedeutung, die auch im ursprünglich hebräischen Wort Amen erscheint, ist „fest“ oder „unerschütterlich“, die Bedeutung im "Hif'il" ist also „jemanden fest sein lassen“.

Christlicher Glaube ist Hinwendung zum christlichen Gott und Abwendung von sich selbst. Er gilt darum als unvereinbar mit "Selbstruhm" und dem Vertrauen auf eigenes Tun (Röm 3,20–28). In dieser antwortenden Hinwendung des christlichen Gläubigen liegt zugleich ein aktives, nach außen und anderen Menschen zustrebendes Moment. Der christliche Glaube kann zur tätigen Liebe (Gal 5,6) bewegen. Der Glaubensbegriff wandelt sich in seiner Bedeutung innerhalb der christlichen Bibel. Eine mögliche Definition nimmt der neutestamentliche Autor des Hebräerbriefs vor:
Für gläubige Christen gilt "christlicher Glaube" als keine antike oder mittelalterliche Vorstufe vom Wissen, sondern etwas vom Wesen her anderes. Damit ist auch kein bloßes "Für-wahr-Halten", auch keine Vermutungsäußerung gemeint. Dann hieße es so viel wie: ‚Ich traue dir, ich vertraue dir, ich kann auf dich bauen. Ich habe eine Gewissheit, die weniger aus Berechnungen und Experimenten kommt.‘ Theologisch unterscheidet man den Glaubensakt, lateinisch „fides qua creditur“ (zu Deutsch; „der Glaube, mit dem geglaubt wird“) einerseits, den Glaubensinhalt, lat. ‚ der Glaube, der geglaubt wird andererseits.

Der Glaubensinhalt wird in den Christlichen Glaubensbekenntnissen zum Ausdruck gebracht und in der Dogmatik systematisch dargelegt und theologisch untersucht. Zentral geht es beim christlichen Glauben um eine Bejahung Gottes: „Es gehört gerade zur Wahrheit des Glaubens, Gott aufgrund seiner Selbstmitteilung so zu denken, wie er ist.“ Gemeinsam ist den meisten christlichen Strömungen der Glaube, dass alles Seiende durch Gott geschaffen wurde und im Dasein gehalten wird. Im Mittelpunkt dieser Schöpfung steht der Mensch, der aber nicht aus eigener Kraft zum Guten fähig ist (Erbsünde) und der Liebe Jesu Christi bedarf, um gerettet zu werden und ewiges Leben zu erlangen. Jesus Christus ist nach der christlichen Glaubenslehre der Mensch gewordene Sohn Gottes. Die drei Personen der christlichen Gottheit, Gott der Sohn, Gott der Vater und Gott der Heilige Geist, sind dreieinig. Grundlage des Glaubens ist die Heilige Schrift der Bibel, die als von Gott inspiriert angesehen wird. Biblische Texte sind interpretationsbedürftig. Zwischen vielen Stellen, die mehr implizit zur Deutung des Glaubensbegriffs verwendbar sind, wird folgende besonders explizite Formulierung häufig diskutiert: „Es ist aber der Glaube das feste Vertrauen auf das Erhoffte, ein Überzeugtsein von dem, was man nicht sieht.“ (Hebr 11,1) Das hier mit „Überzeugtsein“ wiedergegebene griechische ἔλεγχος "élegchos" (elenchos) bedeutet auch so viel wie Gegenbeweis, Widerlegung oder „Überführtsein“. In diesem Sinne wird hier wohl gesprochen von einem Überführtwerden wider äußeren Anscheins.

Ein wesentlicher Streitpunkt unter den christlichen Konfessionen ist seit der Reformation die Frage, ob der Mensch vor Gott durch seinen Glauben allein gerechtfertigt werde, wie Martin Luther es betont hat, oder ob dazu auch die guten Werke nötig seien, weil Glaube ohne Werke tot sei, wie es im Katholizismus unterstrichen wird. Nach allgemein christlicher Überzeugung ist der Glaube die persönliche Antwort auf Gottes bzw. Jesu Wort. Dabei geschieht diese Antwort immer in der Gemeinschaft aller Glaubenden und stellvertretend für alle Menschen. Uneinigkeit besteht in der Frage, ob die volle Wirklichkeit des Glaubens sich im Herzen des Einzelnen vollzieht (so die meisten protestantischen Denominationen) oder ob der Glaube der Kirche ontologische Priorität hat (so die katholische Lehre).

Die vom christlichen Glauben geprägte Lebensführung wird als Frömmigkeit bezeichnet.

Besonders in der christlich-protestantischen Theologie wird nach Karl Barth oft Glaube gegen Religion abgegrenzt. Barth sah Religion als eigenmächtigen Weg des Menschen zu Gott an und betonte, eine Erkenntnis des Willens Gottes gebe es nur im Glauben an Jesus Christus. Das Hören auf das Evangelium sprenge alle menschlichen Begriffe von Gott, alle ethischen Irrwege.

Dietrich Bonhoeffer übernahm diese Unterscheidung und radikalisierte sie in seiner Frage nach einem Christentum ohne Religion. Angesichts der grundsätzlich positiv gesehenen „mündig gewordenen Welt“, des Verlusts des „religiösen Apriori“, von Innerlichkeit, Gewissen und klassischer Metaphysik habe Barth
Bonhoeffers Ziel war es dagegen, den Kern der Glaubenshaltungen im Rahmen der kirchlichen Tradition herauszustellen, den er nicht in Aussagen über einen Jenseitsgott sieht, sondern in Praxis und deren Begründung in Ethik, alt- und neutestamentlicher Geschichte und Mythologie sowie mystischer Erfahrung (als ästhetisches Bewusstwerden von Grundeinstellungen, nicht übersinnliche Erfahrung).

Gerhard Ebeling betonte wie Barth die kritische Kraft des Glaubens gegen religiöse Festlegungen und Sicherheiten, sah aber Religion als Lebensbedingung des Glaubens an.






Nach Paulus von Tarsus ist Glaube (neben der Hoffnung und der Liebe) eine der drei christlichen Tugenden.

Das Christentum verehrt vor allem Abraham für seinen unerschütterlichen Glauben an Gott . Christen verstehen Abraham so, dass er damals den im ganzen Vorderen Orient bekannten Gott El verehrte, der als der Schöpfer des Alls, als der höchste Gott über allen Göttern galt und unter mancherlei Zunamen: als Höchster, als der Ewige, als der Mächtige, als der Allsehende an den verschiedensten Orten angebetet wurde. Er verehrte ihn auch als seinen Familiengott, als seinen persönlichen Gott, der so für seine Nachfahren zum Gott Abrahams und zum Gott Israels wurde und auch im Christentum eine neue Bedeutung gewann.

Laut Auslegung des Alten Testaments, ist von einem Glauben an das Jenseits bei Abraham jedoch noch nicht die Rede. Ebenfalls ist nicht anzunehmen, dass Abraham die Existenz anderer Götter bestritt. Von diesem Gott El wusste er sich ganz persönlich angerufen. Sein Glaube sah dahingehend aus, dass er mit einer Verheißung beschenkt wurde. El stellte ihm Nachkommenschaft und Land in Aussicht.

Ein Erklärungsansatz sieht so aus, dass der Halbnomade Abraham, nur „die Himmel“, als eine symbolische Entsprechung seines Gottes, der sich allenthalben über ihm wölbt, als seine ständige Begleitung ansah. Er vertraute sich nicht den Göttern irgendeines Landes an, sondern nur dem Gott, dem alle Lande gehören; nicht einem Ortsgott, sondern seinem Gott, der mit ihm geht und ihn persönlich kennt, ihm nahe ist von Ort zu Ort. Abraham wurde um der Zukunft willen, die ihm der Glaube verhieß, zum Heimatlosen, und fand seine Heimat gerade in der Treue zu seinem Gott.

Glaube selbst ist kein religiöses Konzept des Judentums.
Eine hebräische annähernde Entsprechung für "Glauben" im religiösen Sinn ist "Emuna" (auch: Emunah), was meist unzureichend gemeinhin mit „Glaube“, „Zuversicht“ oder „Vertrauen in Gott“ übersetzt wird. Emuna ('E-mu-na; hebräisch: אמונה) stammt von der hebräischen Wort-Wurzel אמן, von der "Amen" und die hebräischen Wörter für Treue, Verlässlichkeit, Übung, Künstler, Handwerker u. a. abgeleitet werden. Der deutsche Rabbiner Samson Raphael Hirsch übersetzte es mit „Vertrauensgrund“.

Im Judentum wird der positive Wert jener "Emuna" und der negative Status eines "Apikorus" (übersetzt mit „Gottesleugner“) beachtet.

Jüdischer Glaube bezieht sich auf die ganze jüdische religiöse Tradition. „Nicht Glauben hat der Ewige von Abraham gefordert.“ (Michael Holzman, in:, S. 157) Statt eines inhaltlich festgelegten religiösen Glaubens steht nach alter – schon weit vorchristlicher – Tradition Gerechtigkeit auf der Grundlage der universellen Nächstenliebe und Gleichheit aller Menschen im Mittelpunkt, was auch im liberalen Judentum bewahrt bleibt: „Das Judentum ist nicht nur ethisch, sondern die Ethik macht sein Prinzip, sein Wesen aus.“

Der jüdische Gelehrte Franz Rosenzweig drückte es sehr einfach aus:

In dieser Form ist der jüdische „Glaube“ ausgedrückt in: Gerechtigkeit und Liebe (Gottesliebe, Nächstenliebe, Feindesliebe), Tat und Erinnerung, in Freiheit zum Schutz des Lebens.

Das gegenwärtige Judentum, das diese Traditionen des ethischen Monotheismus bewahrt und anpasst, wird das rabbinische Judentum genannt. Dieses umfasst den weiten Raum der Traditionen in der Neuzeit und im Mittelalter, mit Bezug in die biblischen und vorbiblischen Zeiten, und betrifft das Mosaik der Traditionen des Judentums in der Vielfalt seiner Strömungen. Immer wieder wird in diesem Zusammenhang von "jüdischen Glaubensprinzipien" gesprochen, jedoch existiert im Judentum kein allgemeingültiger, zwingend geforderter Glaube, kein "Credo".

Das rabbinische Judentum hat den antiken Macht- und Hoheitszentralismus der Tempelpriesterschaft viel radikaler abgelegt als das in den christlichen Gemeinden und Kirchen der Fall ist, die „bei sich einen besonderen Priesterstand schufen, an die biblischen Vorschriften über die jüdischen Priester anknüpften“, wie sich auch in den verschiedenen christlichen Dogmatiken zeigt. Rabbiner sind keine Priester und jüdische Traditionen verwalten sich hauptsächlich in demokratischen lokalen Gemeinden. Im Gegensatz zum Christentum oder Islam kann im Judentum jeder persönliche Glaube an den ein-einzigen Gott, das ewige Wesen akzeptiert werden. In der Gegenwart werden gleichwohl verschiedene religiöse Strömungen des Judentums praktiziert, welche die Bedeutung von Überlieferungen unterschiedlich gewichten.
Obgleich sich die christlichen Religionen in allen ihren Konfessionen und Abspaltungen (siehe: Mormonen, Zeugen Jehovas etc.) einst vom Judentum und der mosaischen Tradition getrennt hatten und die jüdische Bibel als eigene Interpretation in griechischer und lateinischer Übersetzung in ihre heiligen Schriften integrierte, bleibt der christliche Glaube mit dem Judentum unvereinbar.

Yecheskel Kaufmann von der Hebräischen Universität in Jerusalem fasste dies so zusammen:


Der nordamerikanische Neurowissenschaftler Michael Persinger stimulierte durch magnetische Felder die Schläfenlappen seiner Probanden, und meinte dadurch religiöse Empfindungen (Gottesmodul) zu erzeugen. Er erklärte, dass diese Phänomene den Symptomen der Epilepsie gleichen. Scott Atran verfolgt dagegen in seinem Werk "In Gods We Trust" einen darwinistischen Ansatz. Die darwinistische Glaubensforschung sieht den Glauben nicht als anerzogen, sondern als im Bewusstsein des Menschen evolutionär verankert. Die Fähigkeit zu Religiosität und Glaube wird dabei beispielsweise als evolutionäres Nebenprodukt erklärt, es werden aber auch mögliche Selektionsvorteile untersucht. Justin Barrett dagegen sieht in einer evolutionspsychologischen Herangehensweise die Religiosität nicht als überlebenswichtige Strategie von Gemeinschaften, sondern als ein Entwicklungsstadium der menschlichen Psyche.





</doc>
<doc id="1798" url="https://de.wikipedia.org/wiki?curid=1798" title="Hermetic Order of the Golden Dawn">
Hermetic Order of the Golden Dawn

Der Order of the Golden Dawn, besser bekannt als Hermetic Order of the Golden Dawn (Hermetischer Orden der Goldenen Dämmerung, kurz: "Golden Dawn") war eine magische diskrete Gesellschaft. Sie wurde am 12. Februar 1888 von William Robert Woodman, Samuel Liddell MacGregor Mathers und William Wynn Westcott in England gegründet. Am 1. März 1888 erfolgte die offizielle Eröffnung des "Isis-Urania Tempels No. 3" in London. Der Orden bestand bis 1900/1903 und zerfiel dann wegen innerer Streitigkeiten in diverse Nachfolgeorganisationen.

Bekannte Mitglieder des Ordens waren unter anderem Aleister Crowley, Arthur Machen, Pamela Colman Smith, Arthur Edward Waite und William Butler Yeats.

Im Gegensatz zur Theosophischen Gesellschaft, die sich zunehmend an östlichen Weisheitslehren orientierte, verstand sich der Golden Dawn als Fortführung der westlichen Mysterien, insbesondere der Tradition der Rosenkreuzer. Als esoterisch arbeitender Orden wurde er zum Prototyp ähnlicher Organisationen. Viele der modernen magischen oder esoterischen Strömungen wie Wicca und Thelema können bis zu diesem Orden zurückverfolgt werden.

Alle drei Gründer – Westcott, Woodman und Mathers – waren sowohl Freimaurer als auch Rosenkreuzer der Societas Rosicruciana in Anglia. Westcott und Mathers gehörten außerdem der von Anna Kingsford gegründeten Hermetic Society in London an.

Die Grundlagen der Lehren des Golden Dawn entnahm man dem sogenannten "Cipher Manuscript," das der Ordenslegende nach unter obskuren Umständen von A.F.A Woodford entdeckt worden sein soll. Es war in einem Alphabet verfasst, das einer überlieferten Geheimschrift von Abt Trithemius ähnlich ist, und konnte daher von Westcott entschlüsselt werden. Das Manuskript enthält in skizzierter Form fünf Initiationsrituale, sowie entsprechendes Lehrmaterial, u. a. die bisher geheimen Zuordnungen des Tarots zu den hebräischen Buchstaben. Ferner soll sich auf einer der Seiten die Kontaktadresse einer gewissen "Soror Sapiens Dominabitur Astris" (Anna Sprengel), einer hohen Adeptin der Rosenkreuzer aus Deutschland, befunden haben. William Wynn Westcott will von ihr die Erlaubnis zur Gründung des "Golden Dawn in the Outer" (des äußeren Ordens der Goldenen Dämmerung) erhalten haben. Schließlich beauftragte Westcott Mathers, die Ritualskizzen zu arbeitsfähigen Ritualen auszuarbeiten.

Der Eröffnung des "Isis- Urania Tempels No.3" in London im März 1888 folgten noch im selben Jahr die Gründung des "Osiris Tempels No.4" in Weston-super-Mare (Somerset) und des "Horus Tempel No.5" in Bradford (Yorkshire). Erst 1893 erfolgte auf Antrag einiger schottischer Mitglieder die Eröffnung des "Amen-Ra Tempels No.6" in Edinburgh, gefolgt von der Einweihung des "Ahathoor Tempels No.7" in Paris im Januar 1894.

Zunächst wurden nur fünf Grade rituell bearbeitet, die in ihrer Gesamtheit den "äußeren oder ersten Orden" bildeten. Neben der rituellen Arbeit wurde nur theoretisches Grundlagenwissen über die Kabbala, Astrologie, Tarot und Alchemie vermittelt. Einzige Ausnahme war das "Kleine Pentagrammritual", das gelehrt wurde, damit sich der Schüler ein erstes Bild von den unsichtbaren Kräften und deren Lenkung machen konnte.

Bis Dezember 1891 existierte der "zweite oder innere Orden", der die Adeptengrade umfassen sollte, nur administrativ, da es noch keine entsprechenden Initiationsrituale gab. Es wurde lediglich der sogenannte "Portalgrad" geschaffen, der als Bindeglied zwischen äußeren und inneren Orden fungierte.

Es war Samuel Liddell MacGregor Mathers vorbehalten, diesen "zweiten oder inneren Orden", den eigentlichen magischen Orden, zu schaffen, den "Ordo Rosae Rubeae et Aureae Crucis" (Orden der Roten Rose und des goldenen Kreuzes), auch kurz "R.R. et A.C." genannt. Im Herbst 1891 erklärte Mathers, in Paris den Kontakt zu den "geheimen Oberen", den Mitgliedern des innersten oder dritten Ordens, hergestellt zu haben. Von einem Frater L.E.T "(Lux e tenebris)" will er weitere Instruktionen für den Aufbau des zweiten Ordens erhalten haben. Er schuf daraufhin das zentrale Initiationsritual des Ordens, das des "Adeptus Minor Grades". Dafür wird die in der Fama Fraternitatis geschilderte Wiederauffindung des verborgenen Grabes von Christian Rosencreutz rituell umgesetzt. Es wird eigens das "Gewölbe der Adepten" konstruiert, eine begehbare siebenseitige Initiationskammer. Dieses Gewölbe gilt als Widerspiegelung des Universums, unter anderem entsprechen seine farbig bemalten Seiten den sieben Planeten. Darin steht der sogenannte Pastos, ein Sarkophag, in dem der Chefadept während des Rituals liegen wird, um den Ordensgründer Christian Rosencreutz zu verkörpern. Es war Annie Horniman, die im Dezember 1891 als Erste das neue Adeptus Minor Ritual durchlief.

War Westcott vorwiegend für Lehrinhalte des äußeren Ordens verantwortlich, verfasste Mathers fast alle wichtigen Instruktionen und Lehrschriften des zweiten Ordens. Gleich nach der Initiation hatte sich der der "Neophyt Adeptus Minor" durch einen Katalog von Instruktionen und Lehrschriften zu arbeiten. Er hatte verschiedene magische Gegenstände herzustellen und zu weihen, unter anderem die vier „Elementarwaffen“ Stab, Kelch, Dolch und Pantakel. Eine andere häufig geübte Technik war das sogenannte "Reisen in der Geistvision (Travelling in the Spirit Vision)", einer der aktiven Imagination vergleichbaren Methode. Weiter fortgeschrittene Dokumente beschäftigen sich unter anderem mit dem frühneuzeitlichen System der henochischen Engelmagie von John Dee. Neben den offiziellen Lehrschriften zirkulierten zwischen den Mitgliedern des inneren Ordens auch noch die sogenannten "Flying Rolls", Abhandlungen und Erfahrungsberichte, die von fortgeschritteneren Mitgliedern geschrieben wurden.

Nach dem Tod von William Robert Woodman im Dezember 1891, dessen Funktion jedoch nicht ersetzt wurde, kam es zu einer unterschwelligen Rivalität zwischen Mathers und Westcott, der sich 1897 schließlich ganz von allen administrativen Führungspositionen im Orden zurückzog. Mathers, der seit Mai 1892 überwiegend in Paris lebte und den Orden aus der Distanz leitete, ernannte daraufhin Florence Farr zu seiner Repräsentantin in England. Dies provozierte einige Auseinandersetzungen, da sich einige männliche Mitglieder weigerten, sich einer weiblichen Führung unterzuordnen. Darüber hinaus führte der autoritäre und zugleich inkonsistente Führungsstil von Mathers zu vielfältigen Streitigkeiten. So wurde u. a. Annie Hornimann wegen ihrer anhaltenden Kritik von ihm im Dezember 1896 wegen mangelnder Unterordnung aus dem Orden ausgeschlossen.

Einen Höhepunkt erreichten die Streitigkeiten Anfang 1900, als sich die Londoner Adepten weigerten, den exzentrischen Aleister Crowley in den "inneren Orden" aufzunehmen. In einem persönlichen Brief an seine Freundin Isabella Augusta Gregory erwähnte William Butler Yeats die Gründe, aus denen Crowley abgelehnt wurde: Er meinte, eine mystische Gesellschaft sei schließlich keine Besserungsanstalt. Daraufhin reiste Crowley nach Paris, um sich von Mathers selbst in den Grad des "Adeptus Minor" einweihen zu lassen, was aber von den Londoner Adepten nicht anerkannt wurde. Florence Farr war Mathers’ Verhalten überdrüssig und bot ihm ihren Rücktritt an. Dieser wollte jedoch nicht darauf eingehen, da er dahinter eine Intrige vermutete, Westcott wieder an die Spitze zu bringen. In einem persönlichen Brief an Farr behauptete er:

Dies führte zum offenen Aufruhr der Londoner Adepten, die ein siebenköpfiges Komitee einsetzten, um diese Aussagen ihrem Wahrheitsgehalt nach zu untersuchen. Jedoch weigerte sich Westcott, Stellung zu den Vorwürfen zu beziehen. Nachdem auch Mathers nicht bereit dazu war, in London vor einem Komitee zu erscheinen, das er in keiner Weise billigte, wurde er von seinem Amt entbunden, was einer offenen Rebellion gleichkam.

Am 17. April brach Crowley, im Auftrag von Mathers, gewaltsam in die Räumlichkeiten des "zweiten Ordens" ein und ließ die Schlösser austauschen, um das "Gewölbe der Adepten" und weiteres Material in seinen Besitz zu bringen. Dies konnte jedoch von Yeats und andern Mitgliedern des "inneren Ordens" durch gerichtliche Schritte verhindert werden. Mathers und Crowley wurden aus dem Orden ausgeschlossen, wodurch die Spaltung endgültig wurde. Annie Hornimann wurde rehabilitiert und zur Sekretärin ernannt, um das gesamte Material des Ordens neu zu ordnen.

Wenige Monate später versammelte Edward Berridge die wenigen Mathers noch loyal gebliebenen Adepten und gründete den "Isis Tempel No.11" in London, der später zum " Alpha et Omega Tempel No.1" wurde.

Auf einer Zusammenkunft des "inneren Ordens" am 3. Mai 1902 wurden Percy Bullock, Robert William Felkin und John William Brodie-Innes für ein Jahr als leitende Triade bestätigt. In Folge wurde der äußere Orden in "Hermetic Society of the M.R." umbenannt, wobei M.R. für "Morgenröthe" steht. Jedoch scheiterte der Versuch, die Leitung dauerhaft auf eine demokratische Basis zu stellen, an den unterschiedlichen Interessen der Mitglieder. 1903 versuchte John William Brodie-Innes seinen Führungsanspruch durchzusetzen, scheiterte jedoch an Arthur Edward Waite und seiner Fraktion. Trotz mehrfacher Treffen konnte man sich nicht auf eine leitende Triade einigen.

In dieser Situation übernahmen Waite und seine Fraktion das "Gewölbe der Adepten " von "Isis- Urania No.3" und konstituierten am 7. November 1903 den "Independent and Rectified Order R.R. et A.C." als inneren Orden des "Independent and Rectified Rite of the Golden Dawn". Waite und seine Fraktion bevorzugten eher die mystische Ausrichtung des Ordens und unterzogen alle Rituale einer ausführlichen Revision. Aus dem "Independent and Rectified Rite" ging schließlich 1916 das "Fellowship of the Rosy Cross" hervor.

Der Großteil der verbleibenden, eher magisch orientierten, Mitglieder gründeten daraufhin unter Felkin und Brodie-Innes den "Amon Tempel", der zum Haupttempel der Stella Matutina wurde. Auch die Stella Matutina unterzog die Rituale des ursprünglichen Golden Dawn ausführlichen Überarbeitungen, die dadurch eine wesentliche Verbesserung und Erweiterung erfuhren.

In der Folgezeit bemühte sich Brodie-Innes darum, den eingeschlafenen "Amen Ra Tempel No.6" in Edinburgh wiederzubeleben. Er nahm heimlich Kontakt zu Mathers auf, mit dem er sich wieder versöhnte, und wechselte schließlich zum Alpha et Omega.

Die Lehrinhalte des Golden Dawn bestanden aus zahlreichen okkulten und mystischen Traditionen, wie Kabbala und Tarot, Astrologie, Alchemie und der henochischen Magie Dees sowie aus magischen Arbeiten mit den Göttern Altägyptens und Griechenlands. Elemente des Christentums und Judentums wurden synkretistisch integriert. Das Gradsystem gliederte sich anfangs in neun, später in elf Grade, die in Zusammenhang mit den kabbalistischen zehn Sephiroth stehen. Viele heutige okkulte Organisationen bedienen sich des Studienmaterials des Golden Dawn. Zum Beispiel in einigen Wicca-Kult-Gruppen werden seine Pentagrammrituale verwendet, die unverändert dem Golden Dawn entstammen.






In der Tradition des Golden Dawn arbeitende Gruppen


</doc>
<doc id="1800" url="https://de.wikipedia.org/wiki?curid=1800" title="Georges Méliès">
Georges Méliès

Georges Méliès (* 8. Dezember 1861 in Paris; † 21. Januar 1938 ebenda) war ein französischer Illusionist, Theaterbesitzer, Filmpionier und Filmregisseur. Méliès zählt zu den Pionieren der Filmgeschichte und gilt als Erfinder des „narrativen Films“ und der Stop-Motion-Filmtechnik.

Marie-Georges-Jean Méliès − dritter Sohn des wohlhabenden Schuhfabrikanten Jean-Louis-Stanislas Méliès und dessen Frau Johannah Catherine Schuering − wurde mit sieben Jahren im Lycée du Prince Impérial eingeschult. Während des Deutsch-Französischen-Krieges folgte 1870 eine Umschulung in das Lycée Louis-Le-Grand, wo er 1880 sein Baccalauréat erwarb. Bereits zu dieser Zeit zeigte sich sein künstlerisches Talent. Vom November 1881 bis November 1882 leistete Méliès Militärdienst. Er hoffte anschließend die École des Beaux-Arts besuchen zu dürfen, was ihm sein Vater jedoch untersagte. Um sich auf die Arbeit in der väterlichen Manufaktur vorzubereiten und seine Englischkenntnisse zu verbessern, wurde er 1884 für ein Jahr von seinen Eltern nach London geschickt, wo er für einen Geschäftsfreund seines Vaters arbeitete. Da seine Sprachkenntnisse noch nicht genügten, Vorstellungen im Sprechtheater zu verstehen, besuchte er hier regelmäßig die magischen Vorstellungen in der Egyptian Hall von John Nevil Maskelyne und Méliès entwickelte eine Passion für die Zauberkunst.

Nach der Rückkehr 1885 nach Paris trat Méliès als Aufseher über die Maschinen in den väterlichen Betrieb ein. Er entwickelte dabei technisches Geschick, das er neben seiner Arbeit beim Nachbau einiger Automaten des Zauberkünstlers Jean Eugène Robert-Houdin verfeinerte. Méliès begann zu dieser Zeit ebenfalls, magische Tricks einzuüben und absolvierte erste Auftritte im Musée Grévin sowie in der Galerie Vivienne. Im Jahr 1885 heirateten Georges Méliès und Eugénin Genin († 1913). Sein Vater zog sich 1888 aus der Arbeit in der Manufaktur zurück. Méliès verkaufte nun den Teil des möglichen Erbes an die beiden älteren Brüder. Vom Erlös erwarb er das "Théâtre Robert-Houdin".

Die Geschäfte am Theater gestalteten sich gerade zu Beginn schwierig: Bis 1898 erzielte Méliès keinen Gewinn. Das Programm bestand aus Zaubervorstellungen, kurzen Féerien, Pantomimen und der Vorführung einiger Automaten, die Méliès zusammen mit dem Theater erworben hatte. In den Vorstellungen traten auch bekanntere Zauberkünstler dieser Zeit auf, darunter Buatier de Kolta, der Zwerg La Fée Mab, Duppery und Legris. Zum festen Ensemble gehörte ebenfalls Jeanne d’Alcy, die später in vielen Filmen von Méliès spielte und die er in zweiter Ehe heiratete. 

Die Brüder Lumière hatten über dem Theater ein Atelier gemietet. Am 28. Dezember 1895 sah Méliès eine der ersten Vorführungen des Cinématographen. Den Verkauf des Apparates an Méliès verweigerten die Brüder Lumière mit der Begründung, der wirtschaftliche Erfolg dieser Erfindung sei unklar und die eigene Verwertung solle solange laufen, wie ein Interesse des Publikums bestehe. Méliès reiste daraufhin nach England, wo er von Robert William Paul einen Projektor, einige Filme von Edison und unbelichtete Negative kaufte. Am 4. April 1896 eröffnete Méliès sein "Théâtre Robert-Houdin" neu als Kino. Er kaufte bald einen weiteren Projektor und baute den von Paul in eine Kamera um. Zusammen mit den Geschäftspartnern Lucien Reulos und Lucien Korsten ließ Méliès im September 1896 einen Méliès-Reulos-Kinétographen patentieren.

Im Sommer 1896 drehte Méliès seinen ersten Film mit dem Titel "Une partie de cartes", der einem gleichnamigen Film der Lumières stark ähnelte. Anschließend filmte er zuerst nur kurze Aufnahmen von alltäglichen Szenen, ergänzte sein Repertoire aber bald auch um inszenierte Szenen, den sogenannten "scènes composées". Seine Filme zeigte er nicht nur im "Théâtre Robert-Houdin", sondern vertrieb sie auch an Jahrmarktskünstler, die sie zusammen mit anderen Attraktionen vorführten.

Méliès errichtete 1896 Frankreichs erstes Filmstudio – neben Edisons Black Maria eines der ersten der Welt – auf dem Grundstück seiner Familie in Montreuil in der Nähe von Paris. Das Studio wurde Anfang 1897 eröffnet. Hier entstanden von nun an fast alle seine Filme. Das Studio glich in seinen Maßen dem "Théâtre Robert-Houdin" und wie dessen Bühne war auch seine mit Falltüren und anderen Mechanismen ausgestattet, um Bühnenillusionen zu erzeugen. Die Wände und das Dach waren wie bei einem Fotoatelier aus Glas gefertigt, um eine ausreichende Beleuchtung der Bühne zu gewährleisten. Bewegliche Jalousien erlaubten es, den Lichteinfall zu beeinflussen. Im Jahre 1905 erweiterte Méliès das Filmstudio.
Im Jahr 1897 gründete Méliès die Produktionsfirma Star Film, mit der er bis 1913 über 500 Filme drehte. Zu Beginn war er mit ihnen wirtschaftlich relativ erfolgreich, doch schon bald wurde er von der Konkurrenz und Schwarzkopierern bedroht. Um in Amerika seine Filme besser vertreiben und seine Rechte wahren zu können, eröffnete die Star Film 1903 in New York ein Büro, das von Méliès’ Bruder Gaston Méliès geleitet wurde. In Frankreich erwuchs ihm insbesondere mit der Produktionsfirma Pathé ein starker Konkurrent. Als sich im Jahr 1909 deren Vertriebssystem durchsetzte, bei dem die Filme nicht mehr verkauft, sondern verliehen wurden, waren Méliès’ teuer produzierte Filme den günstigen, schnell und industriell gefertigten Filmen wirtschaftlich nicht mehr gewachsen. Ab 1911 konnte er eigene Filme nur noch im Auftrag der Pathé produzieren, 1913 kam seine Filmproduktion zum Erliegen.

Während des Ersten Weltkrieges trat Méliès als Varietékünstler auf; er verlor jedoch sein gesamtes Vermögen. In seiner zweiten Ehe heiratete Georges Méliès im Jahr 1925 Jeanne d’Alcy (Fanny Manieux alias Charlotte Faës). Mit seiner Ehefrau betrieb er bis 1932 einen Spielzeugladen in der Metrostation Montparnasse. Als Filmschaffender war er zu dieser Zeit vergessen, bis 1929 einige seiner Werke auftauchten und Filmjournalisten wieder über ihn berichteten. Ab 1932 ermöglichten andere Filmschaffende ihm und seiner Frau den Aufenthalt in einer Altersresidenz in Orly. Sein Grab liegt auf dem Pariser Friedhof Père Lachaise.

Im Jahr 1947 benannte die neu gegründete französische Filmkritikervereinigung Association Française de la Critique de Cinéma (AFCC) ihren jährlich vergebenen Preis für die beste französische Kinoproduktion nach Méliès.

Das Werk von Georges Méliès zeichnet sich durch eine große Vielfalt, Phantasie und technisches Geschick aus. Von der großen Zahl seiner Filme, die er zwischen 1896 und 1913 drehte, sind etwa 200 erhalten. Die Filme stammen aus verschiedenen Genres. Sein bekanntester Film ist wahrscheinlich "Le Voyage dans la Lune" "(Die Reise zum Mond)", mit dem er 1902 in Anlehnung an den gleichnamigen Roman von Jules Verne den ersten Science-Fiction-Film schuf. Neben diesen Féeries entstanden auch kurze Dokumentationen ähnlich denen der Brüder Lumière, nachgestellte Ereignisse der Zeitgeschichte und kurze Trickfilme.

Üblicherweise wird Méliès die Erfindung des Stopptricks zugesprochen. Dass er diesen entdeckte, als er während einer Aufnahme auf dem Place de l'Opera filmte und seine Kamera stockte, muss aber wahrscheinlich als Legende betrachtet werden. Mit Hilfe des Stopptricks drehte er viele Filme, die an Zauberkunststücke erinnern, wie sie im "Théâtre Robert-Houdin" gezeigt wurden. Neben diesem bediente er sich auch anderer filmischer Tricks wie Doppelbelichtungen und Modellaufnahmen.

Einen wichtigen Platz in seinem Schaffen nehmen sogenannte Aktualitätenfilme ein, in denen er Ereignisse der Zeitgeschichte nachstellte. Unter anderem filmte er eine zwölfteilige Reihe über die Dreyfus-Affäre ("L'Affaire Dreyfus", 1899) und inszenierte 1902 die Krönung Eduards VII., bevor diese überhaupt stattfand.

Weil Méliès massive finanzielle Probleme hatte und keine Geldgeber für seine Filmprojekte fand, musste er nach 1923 im Anschluss an den Bankrott 1200 Filme als Rohmaterial an die Schuhindustrie verkaufen.

Die filmgeschichtliche Position der Werke von Méliès zu bestimmen, gestaltet sich schwierig: Klassischerweise unterscheidet die Filmgeschichte Méliès’ inszenierte Stücke von den dokumentarischen Arbeiten der Lumières und sieht hierin den wichtigsten Gegensatz im frühen Film. Diese Unterscheidung wird heute jedoch nicht mehr so scharf gezogen, da auch Méliès zu Beginn hauptsächlich dokumentarisch filmte und die Brüder Lumière ebenfalls kurze Szenen inszenierten. Eine weitere Unterscheidung wird häufig zwischen den primitiven frühen Filmen, wie denen von Méliès, und dem späteren elaborierten Kino nach D. W. Griffith vorgenommen. Méliès’ Filme stehen als Beispiele für ein primitives Kino, das noch keine filmische Sprache mit Schnitten und diversen Aufnahmegrößen kannte. Auch diese Unterscheidung wurde in den letzten Jahren häufig abgelehnt und statt ihrer gefordert, das frühe Kino nicht mit den Begriffen des späteren narrativen zu analysieren, sondern es als ein Kino der Attraktionen zu betrachten, das eigenen ästhetischen Regeln folgte.

In dem Roman "Die Entdeckung des Hugo Cabret" (2007) beschreibt Brian Selznick die Wiederentdeckung der Filme von Georges Méliès durch den Protagonisten Hugo Cabret. Den Roman verfilmte Martin Scorsese im Jahre 2011 unter dem Titel "Hugo Cabret", wobei Georges Méliès durch Ben Kingsley dargestellt wurde.

Einige Ausschnitte der Filme "Le voyage dans la lune", "Le voyage à travers l'impossible" und "L’éclipse du soleil en pleine lune" sind im Musikvideo zu "Heaven for Everyone" von Queen aus dem Jahr 1995 eingearbeitet. Das Video der Smashing Pumpkins zu ihrem Titel "Tonight, tonight" (1996) wurde im Stil dieser Méliès-Filme gestaltet.







</doc>
<doc id="1801" url="https://de.wikipedia.org/wiki?curid=1801" title="Guy Ritchie">
Guy Ritchie

Guy Stuart Ritchie (* 10. September 1968 in Hatfield, Hertfordshire) ist ein britischer Drehbuchautor und Filmregisseur.

Ritchie begann 1995 als Regisseur für Werbefilme und Musikvideos. Im selben Jahr drehte er den Kurzfilm "The Hard Case", mit Hilfe dessen er seinen ersten Film "Bube, Dame, König, grAS" finanzieren konnte, der zum Kultfilm avancierte. Der darauf folgende Film "Snatch – Schweine und Diamanten" bot ein größeres Starensemble mit Schauspielern wie Brad Pitt und Benicio del Toro. Außerdem hatte Ritchie für "Snatch" ein wesentlich größeres Budget als bei seinem Debütfilm. Am 22. Dezember 2000 heiratete er die Sängerin Madonna, mit der er 2001 das kontroverse Musikvideo "What It Feels Like For A Girl" und den Kurzfilm "The Hire: Star" für BMW drehte. Sein Spielfilm "Stürmische Liebe – Swept Away" (2003) mit Madonna in der Hauptrolle fiel sowohl bei den Kritikern als auch kommerziell durch. Zwei Jahre darauf folgte mit "Revolver" der dritte Film Ritchies mit Jason Statham in der Hauptrolle. Kommerziell konnte der Film nicht an die Erfolge von "Bube, Dame, König, grAS" und "Snatch – Schweine und Diamanten" anknüpfen, jedoch ist es der Film, mit dem Ritchie bis dato am zufriedensten war.

Madonna und Ritchie haben einen gemeinsamen leiblichen und einen gemeinsam adoptierten Sohn. Die Ehe wurde am 21. November 2008 in London geschieden, er erhielt eine Abfindung in Höhe von 55,6 Millionen bis 66,7 Millionen Euro.

2008 führte Ritchie Regie beim Nike-Werbespot "Evolution", der unter anderem für die Euro 2008 gedreht wurde, außerdem drehte er den Film "Rock N Rolla" mit Gerard Butler sowie die Werbereihe "Nespresso – What else?" mit George Clooney.

2009 erschien Ritchies Film "Sherlock Holmes" mit Robert Downey junior als Sherlock Holmes und Jude Law als Dr. Watson in den Hauptrollen. Im Jahr 2011 wurde eine Fortsetzung mit dem Titel "" realisiert. 2015 erschien unter seiner Regie der Film "Codename U.N.C.L.E.", eine Kinoadaption der Serie "Solo für O.N.C.E.L.". "" (2017) basiert auf der Artussage.

2017 wurde Ritchie in die Academy of Motion Picture Arts and Sciences (AMPAS) aufgenommen, die jährlich die Oscars vergibt.




</doc>
<doc id="1802" url="https://de.wikipedia.org/wiki?curid=1802" title="Goldene Himbeere">
Goldene Himbeere

Die Goldene Himbeere () ist ein erstmals 1981 vergebener Negativ-Filmpreis. Der "Razzie" wurde als eine Art Anti-Oscar vom Cineasten John J. B. Wilson kreiert und wird in verschiedenen, teils absurden Kategorien für die jeweils schlechteste Leistung des Filmjahres traditionell am Abend vor der Oscar-Verleihung vergeben. Die materiell annähernd wertlose Sieger-Trophäe besteht aus einer Kunststoffhimbeere, die auf eine Super-8-Filmrolle aus Aluminium geklebt und mit Goldfarbe überzogen ist.

Namensgeber der Auszeichnung ist die englische Redewendung "to blow a raspberry" im Sinne von „ein Furzgeräusch mit dem Mund erzeugen“ (wörtlich „eine Himbeere blasen“). Diese stammt ursprünglich aus dem "Cockney rhyming slang," einer Londoner Varietät des Englischen, in der häufig vorkommende Wörter durch Reimwörter oder -phrasen ersetzt werden. So heißt es dort statt "fart" („Furz“) "raspberry tart" („Himbeertörtchen“), was wiederum verkürzt in "to blow a raspberry" auftaucht. Die Geste gilt als „Ausdruck der Missbilligung oder Verachtung“.

Hinter dem Preis steht die "Golden Raspberry Award Foundation" (G.R.A.F.), der 2007 mehr als 750 Filmkritiker, Journalisten und Filmschaffende aus 42 US-Staaten und mehr als zwölf Ländern angehörten. Aus Deutschland benennt die Münchner Filmwerkstatt 50 Personen, die an der Abstimmung teilnehmen.

Die 38. Golden Raspberry Awards wurden am 3. März 2018, dem Vorabend der Oscarverleihung, im kalifornischen Hollywood (Los Angeles) vergeben.




Rückblickend auf ein Jahrzehnt, werden an besonders schlechte Filmwerke und Filmschaffende nochmals Goldene Himbeeren verliehen, so geschehen für die 1980er, 1990er und 2000er Jahre. Während der Preisverleihung zur Goldenen Himbeere 2000 wurden für das 20. Jahrhundert die schlechteste Schauspielerin und der schlechteste Schauspieler des Jahrhunderts gekürt. Anlässlich des 25-jährigen Bestehens der Goldenen Himbeere wurden auf der Preisverleihung 2005 Trophäen für "das Schlechteste aus 25 Jahren" verliehen. Ab 2015 wird der "Razzie Redeemer Award" („Himbeere der Erlösung“) vergeben. Diesen erhält ein Künstler, der mit der Goldenen Himbeere ausgezeichnet oder dafür nominiert war und der später eine beachtenswerte künstlerische Leistung abgeliefert hat. Ob diese Kategorie dauerhaft eingerichtet werden wird, steht noch nicht fest.




</doc>
<doc id="1803" url="https://de.wikipedia.org/wiki?curid=1803" title="Goldene Himbeere/Schlechtester Film">
Goldene Himbeere/Schlechtester Film

Die Goldene Himbeere für den schlechtesten Film wird seit 1981 jährlich vergeben. Dabei bezieht sie sich auf Filme des vergangenen Jahres. So wurde beispielsweise der Preis des Jahres 2007 am 23. Februar 2008 verliehen.


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:

Die Stimmauszählung ergab ein Unentschieden:

Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:

Die Stimmauszählung ergab ein Unentschieden:

Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:


Außerdem nominiert:



Außerdem nominiert:


Außerdem nominiert:

Die Stimmauszählung ergab ein Unentschieden:

Außerdem nominiert:

Außerdem nominiert:

Außerdem nominiert: 


</doc>
<doc id="1806" url="https://de.wikipedia.org/wiki?curid=1806" title="Galileo">
Galileo

Galileo steht für:



Galileo steht für:

Siehe auch:


</doc>
<doc id="1807" url="https://de.wikipedia.org/wiki?curid=1807" title="Gnus">
Gnus

Die Gnus ("Connochaetes", das einzelne Tier das Gnu, entweder Bulle oder Kuh) sind eine Gattung afrikanischer Antilopen, die in großen Herden leben und zur Gruppe der Kuhantilopen gehören. Ursprünglich wurden als Arten innerhalb dieser Gattung nur das Weißschwanzgnu und das Streifengnu unterschieden. Mittlerweile wird den zuvor als Unterart des Streifengnus eingeordneten Östlichen Weißbartgnus, Weißbindengnus und Serengeti-Weißbartgnus ebenfalls ein Art-Status zugebilligt. 

Der Bestand betrug zu Beginn des 21. Jahrhunderts rund 1,5 Millionen Gnus. Die häufigste Art ist das Serengeti-Weißbartgnu mit 1,3 Millionen Individuen, der Verbreitungsschwerpunkt der Gattung liegt entsprechend im Osten Afrikas. Die seltenste Art ist das Östliche Weißbartgnu, dessen Bestand zwischen 6000 und 8000 Tieren beträgt. 

Der Kopf und die Hörner der Gnus haben rinderartige Merkmale. Die Hörner sind kurz, kräftig und bei beiden Geschlechtern vorhanden. Die Kopfrumpflänge beträgt etwa 2 m. Ausgewachsene Bullen des Streifengnus können eine Schulterhöhe von bis zu 156 cm erreichen, bei den ausgewachsenen männlichen Serengeti-Weißbartgnus liegt die Schulterhöhe dagegen zwischen 110 und 134 cm. Die kleinste Art ist das Weißschwanzgnu, dessen Bullen eine Schulterhöhe von 110 bis 120 Zentimeter erreichen. Die Kühe haben im Durchschnitt eine etwas geringere Schulterhöhe als die Bullen. 

Bullen der größeren Gnuarten können ein Körpergewicht von bis zu 250 Kilogramm erreichen, Bullen des Weißschwanzgnus als die kleinste Gnuart erreichen ein Körpergewicht von 180 Kilogramm. Das Körpergewicht der Weibchen beträgt beim Streifengnu zwischen 190 und 215 Kilogramm, bei den Serengeti-Weißbartgnus dagegen 140 bis 180 Kilogramm und beim Weißschwanzgnu durchschnittlich 155 Kilogramm. 

Die Fellfarbe ist je nach Art unterschiedlich. Streifengnus haben ein dunkel schiefergraues Fell mit auffälligen Längsstreifen, die hellste Art ist dagegen das Serengeti-Weißbartgnu mit einem rotbräunlichen Fell.

Der Sexualdimorphismus bei Gnus ist nur gering ausgeprägt. Diese Eigenschaft wird bei einer Reihe von afrikanischen Antilopenarten beobachtet, die in einem Herdenverband leben und häufig auch weite Wanderungen unternehmen. Vermutet wird, dass der geringe Geschlechterunterschied es männlichen Tieren erlaubt, in der Herde zu leben, ohne dass dies zu einer erhöhten Aggressivität mit anderen männlichen Tieren des Herdenverbands führt. Es ermöglicht insbesondere heranwachsenden männlichen Tieren, im Schutz der Herde zu leben.

Das Verbreitungsgebiet der Gnus erstreckt sich über den Südosten und Süden Afrikas. Das nördlichste Verbreitungsgebiet befindet sich knapp südlich vom Äquator in Zentralkenia und erstreckt sich, abgesehen von der Demokratischen Republik Kongos über alle Staaten des Subkontinents. Der Oranje und der Übergang von der Baumsavanne zum gemäßigten Klima des Highveld stellen die südliche Verbreitungsgrenze dar. Das auf der baumlosen Ebene des Highveld lebende Weißschwanz-Gnu ist die Gnu-Art mit dem südlichsten Verbreitungsgebiet. Trotz dieses sehr weiträumigen Verbreitungsgebietes liegt das Hauptvorkommen in der Serengeti. Dort leben heute rund 85 Prozent des weltweiten Gnu-Bestands, nachdem die Bestandszahlen in den anderen Regionen des Verbreitungsgebietes seit Beginn des 20. Jahrhunderts stark zurückgegangen sind. 

Der Verlust an Lebensraum seit der Kolonialisierung des afrikanischen Kontinents hat dazu geführt, dass in weiten Teilen des Verbreitungsgebietes anstelle größerer wandernder Herden heute überwiegend kleine, mehr sesshafte Populationen typisch sind, die in geschützten Gebieten leben. Lebensraumverluste sind vor allem auf eine landwirtschaftliche Nutzung früherer Weidegebiete sowie die Errichtung von weitläufigen Zäunen zurückzuführen, die die Übertragung von Tierkrankheiten verhindern sollen. Im Etosha-Nationalpark lebten beispielsweise bis in die 1960er Jahre in bestimmten Jahreszeiten bis zu 30.000 Streifengnus. Etwa 10.000 der Tiere waren ganzjährig in der Region des Nationalparks anzutreffen. 20.000 Gnus wanderten während der Regenzeit aus dem Ovamboland in den Nationalpark. Nachdem ein Zaun diese Wanderungsroute unterbrach, ging die Gnu-Population im Etosha-Nationalpark auf 2000 bis 3000 Individuen zurück und ist seitdem auf diesem Niveau verblieben. In Botswana wurde auf Regierungsbetreiben in den späten 1980er Jahren entlang der Nordgrenze des Ghanzi Districts ein Schutzzaun errichtet, der in dieser Region die Möglichkeiten zur Hausrindhaltung verbessern sollte. Er schnitt großräumig den Zugang von Wildtieren aus dem Zentralgebiet der Kalahari zu Wasserstellen ab und führte 1988 zum Tod von 50.000 Gnus innerhalb von vier Monaten.

Gnus wurden außerdem in Regionen außerhalb ihres historischen Verbreitungsgebietes angesiedelt. So sind sie heute auch im östlichen Hochland von Simbabwe und in privaten Wildreservaten in küstennahen Regionen Namibias anzutreffen.

Streifengnus, Östliche Weißbartgnus, Weißbindengnus und Serengeti-Weißbartgnus besiedeln die Savannenregionen mit Niederschlagsmengen zwischen 400 mm und 800 mm und kommen entsprechend in der Dornenstrauch- und Trockensavanne vor. Sie sind Arten der Ebenen und Weidegänger mit einer Präferenz für Flächen mit kurzwüchsigem Vegetationsbestand und am ehesten in solchen Regionen anzutreffen, die locker mit Akazien bestanden sind. Während der Regenzeit sind sie auch in arideren Regionen ihres Verbreitungsgebietes anzutreffen, während sie sich während der Trockenzeit in Regionen versammeln, in denen hoher Niederschlag dazu führt, dass permanent Wasserstellen zur Verfügung stehen. Oberhalb von 1800 bis 2100 Höhenmeter sind Gnus selten, jedoch überqueren sie während ihrer saisonalen Wanderbewegungen auch Grasland in Bergregionen oder Hügelland.

Das Weißschwanzgnu als die südlichste Art besiedelte ursprünglich Teile Südafrika, Swaziland und Lesotho und wurde in Swaziland und Lesotho bereits im 19. Jahrhundert durch starke Bejagung ausgerottet. Ursprünglich hielt es sich während der Trockenzeit im Grasland des klimatisch gemäßigten Highvelds auf und wanderte in der Regenzeit in die aridere Karoo. Diese Wanderungen finden heute nicht mehr statt - es kommt als sesshafte Art nur noch in Schutzgebieten vor, wurde mittlerweile jedoch in Lesotho und Swaziland wieder angesiedelt und erfolgreich in Namibia eingeführt.

Gnus sind vor allem für ihre Wanderungen bekannt. Allerdings wandern nicht alle Herden, da in vielen Regionen des Verbreitungsgebietes die Zersiedelung des Lebensraumes sowie weitläufig Zaunsysteme, die Hausrindherden schützen sollen, die Wanderungen unterbinden. 

Auch von den in der Serengeti lebenden Serengeti-Weißbartgnus wandern nicht alle Herden, es gibt sowohl im westlichen Korridor des Serengeti-Nationalpark, in der Masai Mara und im Ngorongoro-Krater sesshafte Herden. Die Wanderung der Serengeti-Weißbartgnus, der zahlenmäßig stärksten Art innerhalb der Gattung, ist jedoch unverändert einer der auffälligsten Tierzüge der Welt. Während der Regenzeit sind die Herden dieser Gnu-Art in den mineralstoffreichen Ebenen der südöstlichen Serengeti in Tansania zu finden. Gegen Ende Mai oder Anfang Juni endet die Regenzeit, und die Gräser verwelken. Die Gnus ziehen dann in einer Kolonne nordwärts über den Mara-Fluss in die Masai-Mara-Ebene in Südkenia. Dort finden sie aufgrund von Schauern vereinzelte Regionen mit starkem Graswachstum. Das Gras hier hat jedoch einen schweren Phosphormangel, so dass die Gnus mit dem Beginn der Regenzeit gegen Ende des Jahres in die Serengeti zurückkehren. Bei der dabei notwendigen Überquerung des Mara-Flusses werden die Gnus von Krokodilen erwartet, die hunderte von ihnen erbeuten.

Während in trockenen Habitaten Herden aus Gnus aller Altersgruppen und Geschlechter zusammengesetzt sein können, bilden Männchen und Weibchen für gewöhnlich jeweils getrennte Herden. Am ausgeprägtesten ist dies während der Zeit, in der die Kälber zur Welt kommen und sich innerhalb der Herden zahlreiche Subgroupierungen herausbilden. Tragende Kühe halten sich in dieser Zeit gewöhnlich in der Nähe anderer tragender Kühe an und schließen sich, sobald sie gekalbt haben, anderen Kühen an, die säugende Jungtiere führen. Nichttragende Weibchen dagegen sind häufig in Herden nicht territorialer Männchen zu finden. 

Wie bei zahlreichen anderen Antilopenarten liegen während des Tages ruhende Gnus in einer sternförmigen Anordnung, da sie sich beim Niederlegen gewöhnlich Rücken an Rücken legen. Das hat zur Folge, dass die Tiere in unterschiedliche Richtungen sehen und Prädatoren so schneller entdecken. Nachts suchen sie die am geringsten mit Bäumen und Sträuchern bestandene Flächen auf und legen sich dort in linearen Formationen, die nicht breiter sind als etwa ein dutzend Tiere und der Abstand zwischen den einzelnen Gnus so groß ist, dass andere Gnus die Formation durchqueren können. Nach Richard D. Este ist auch dies eine Anti-Prädatoren-Strategie. Im Falle eines Angriffs beispielsweise durch Tüpfelhyänen kann jedes Gnu sofort die Flucht ergreifen ohne durch ein anderes Tier behindert zu werden. 

Die Tragzeit der Gnus beträgt etwa neun Monate. Anschließend wird ein einziges Junges geboren, das für weitere neun Monate gesäugt wird. Das Geschlechterverhältnis ist zum Geburtszeitpunkt ausgeglichen: Es kommen etwa gleichviel männliche wie weibliche Kälber zur Welt. Ab dem Zeitpunkt, ab dem die Kälber sich von ihren Muttertieren absondern, verschiebt sich das Geschlechterverhältnis allmählich zu Gunsten der Weibchen. In sesshaften Populationen des Serengeti-Weißbartgnus kommen gelegentlich auf zwei Weibchen dann nur noch ein Bulle. Richard D. Estes führt dies darauf zurück, dass männliche Tiere in sesshaften Populationen einem höheren Stress ausgesetzt sind, da sie ständig um Reviere kämpfen und Junggesellengruppen sich tendenziell in den Randbereichen der Herde aufhalten, die schlechtere Lebensbedingungen bieten. Die Lebensdauer der Gnus beträgt bis zu zwanzig Jahre, allerdings werden die meisten lange vorher von Raubtieren gerissen.

In einem Alter von etwa neun Monaten verlassen die Kälber ihre Muttertiere und bilden unabhängig vom Geschlecht Jährlingsgruppen. Einige der weiblichen Jährlinge und eine geringe Zahl männlicher Jährlinge bleiben allerdings in den Kuhherden und folgen noch immer ihren jeweiligen Müttern. In ihrem zweiten Lebensjahr bilden diese Jungtiere zunehmend nach den Geschlechtern getrennte Gruppen. Ab dem Beginn des dritten Lebensjahr sind Weibchen äußerlich kaum noch von nichttragenden älteren Kühe zu unterscheiden. Beim Serengeti-Weißbartgnu werden rund 80 Prozent dieser weiblichen Jungtiere während der nächsten Brunft gedeckt, sie sind dann etwa 28 Monate alt. 

Junge Bullen schließen sich ab ihrem zweiten Lebensjahr separaten Junggesellenverbänden an. Diese sind altersmäßig durchmischt, die jungen Bullen können von den älteren jedoch noch in ihrem dritten Lebensjahr äußerlich unterschieden werden, da ihre Hörner weniger kräftig ausgebildet sind und ihr Körperbau schlanker ist. Ihre körperliche Entwicklung schließen junge Bullen im vierten oder fünften Lebensjahr ab. Verglichen mit den Junggesellenverbänden nah verwandter Antilopenarten wie der Nordafrikanischen Kuhantilope, der Leierantilope und der Grant-Gazelle, bei denen die Bullen häufig miteinander kämpfen, sind die Angehörigen eines solchen Gnu-Junggesellenverbandes vergleichsweise wenig aggressiv untereinander. Richard D. Estes führt dies unter anderem darauf zurück, dass auch der Abstand territorialer Bullen zueinander geringer ist als bei diesen Arten und männliche Gnus daher generell toleranter gegenüber Geschlechtsgenossen sind.

Bullen versuchen ein Territorium zu etablieren, da nur ein solches ihnen die Chance zur Fortpflanzung bietet. Dieses Revier wird gegen andere Männchen verteidigt; solche Zusammentreffen haben ritualisierte Drohgebärden und Kämpfe mit den Hörnern zur Folge. Betritt eine Weibchenherde einen solchen Eigenbezirk, übernimmt das Männchen die Kontrolle über sie, verteidigt sie und paart sich mit ihnen, bis sie das Revier wieder verlassen. In Regionen mit einem geringen Bestand an Weißschwanzgnus beträgt der Abstand der Bullen etwa einen Kilometer voneinander. Der minimale Revierabstand in Regionen mit einem hohen Bestand an Weißschwanzgnus beträgt dagegen etwa 180 Meter. Deutlich kleiner sind die Reviere des Serengeti-Weißbartgnus: Im Ngorongoro-Krater wurden pro Quadratkilometer zwischen 57 und 85 territoriale Bullen gezählt.

Zu den Fressfeinden der Gnus zählen Löwen, Leoparden, Hyänen und der Afrikanische Wildhund sowie Krokodile. Gesunde, ausgewachsene Gnus verfügen über beträchtliche Körperkraft und können Angreifern deshalb erhebliche Verletzungen zufügen. In der Regel sind es Jungtiere sowie kranke Gnus, die von Prädatoren geschlagen werden. Flucht ist jedoch das typische Verhalten bei angreifenden Fressfeinden. Fliehende Gnus erreichen eine Geschwindigkeit von bis zu 80 km/h.

Ziehende Herden werden von Geiern begleitet, für die die Kadaver von Gnus eine wesentliche Nahrungsquelle darstellen. Etwa 70 Prozent der Gnu-Kadaver werden von Geiern gefressen. Der zahlenmäßige Rückgang an ziehenden Gnus hatte einen negativen Effekt auch auf die Geierpopulation.

In der offenen Savanne vermischen sich Herden von Zebras und Gnus und sind auch während der Wanderungen vergesellschaftet. Häufig sind es Zebraherden, die den Gnuherden vorangehen. Es besteht dabei eine gewisse ökologische Abhängigkeit, da Zebras die hohen, nährstoffärmeren Grasstände vertilgen, die Gnus hingegen die mittleren und nährstoffreicheren. Diese Vergesellschaftung verringert außerdem das Risiko, dass Fressfeinde sich anschleichen können. Gnus reagieren auch auf die Alarmrufe anderer Tierarten. So reagieren Gnus beispielsweise auf die Alarmrufe von Pavianen und sind in der Lage, diese sehr genau von anderen, ähnlich klingenden Rufen zu unterscheiden.

Die Gnus bilden eine Gattung aus der Familie der Hornträger (Bovidae). Innerhalb der Hornträger werden sie zur Unterfamilie der Antilopinae und zur Tribus der Kuhantilope (Alcelaphini) gestellt. Die nächsten Verwandten der Gnus bilden dadurch die Eigentlichen Kuhantilopen ("Alcelaphus"), die Leierantilopen ("Damaliscus") und die Hunter-Antilope ("Beatragus"). Allgemein zeichnen sich Kuhantilopen durch ihren großen Körperbau mit charakteristisch hoher Lage der Schulter sowie abfallendem Rücken, durch die quer gerippten Hörnern und die tiefen Drüsengruben im Gesicht aus. Weitere Merkmale finden sich in dem langen Schädel und den großen Hohlräumen in der Stirn, die bis in die Ansätze der Hörner reichen. Molekulargenetischen Untersuchungen zufolge bilden die Gnus die Schwestergruppe zu allen anderen Kuhantilopen, sie trennten sich bereits im Oberen Miozän von den anderen Linien ab.

Ursprünglich wurden als Arten innerhalb dieser Gattung nur das Weißschwanzgnu und das Streifengnu unterschieden. Während einer Revision der Hornträger, die Colin Peter Groves und Peter Grubb im Jahr 2011 vorlegten, wurden einige Formen, die ursprünglich als Unterarten des Streifengnus galten, in den Artstatus erhoben. Folgende Arten werden dadurch heute anerkannt:

Die wissenschaftliche Erstbeschreibung der Gattung "Connochaetes" erfolgte durch Martin Lichtenstein im Jahr 1812. Lichtenstein versuchte sich in seiner Veröffentlichung an der Gliederung der Antilopen und stellte "Connochaetes" als langschwänzige Form mit Mähne und Tränensäcken heraus, bei der beide Geschlechter Hörner tragen.
Gnus wurden schon immer wegen ihres Fleisches und ihrer Haut gejagt; aus den Schwänzen pflegte man Fliegenwedel herzustellen. Mit der Ankunft weißer Siedler wurden die Tiere massenhaft abgeschossen, so dass die Herden kontinuierlich kleiner wurden. Insbesondere die Bestände des Weißschwanz-Gnus gingen so frühzeitig zurück, dass die Lebensweise und vor allem die Wanderbewegungen dieser Gnu-Art nie in freier Wildbahn studiert wurde. Es überlebte dank der Initiative einiger Farmer, die die Tiere auf ihrem Land schützten. 

Im Serengeti-Nationalpark haben sich die dortigen Serengeti-Weißbartgnus dank massiver Schutzbemühungen wieder stark vermehrt. Von 500.000 Tieren im Jahr 1970 stieg die Population wieder auf etwa 1,3 Millionen an. Allerdings ist es nicht in ganz Afrika so gut um die Gnus bestellt. In vielen Staaten sinken die Bestände weiterhin, so im südlichen Afrika, wo der Bestand des Streifengnus von 300.000 im Jahr 1970 auf 130.000 abfiel. Die Population des Weißbindengnus könnte sich auf etwa 70.000 bis 80.000 beziffern, während das Weißbartgnu möglicherweise nur noch rund 8000 Individuen umfasst.

Der Name "Gnu" ist der Sprache der Khoikhoi entnommen.




</doc>
<doc id="1809" url="https://de.wikipedia.org/wiki?curid=1809" title="Gold">
Gold

Gold (bereits althochdeutsch "gold", zu einer indogermanischen Wurzel "*ghel": glänzend, gelb) ist ein chemisches Element mit dem Elementsymbol Au () und der Ordnungszahl 79. Es ist ein Übergangsmetall und steht im Periodensystem in der 1. Nebengruppe (Gruppe 11), die auch als Kupfergruppe bezeichnet wird. Diese Gruppe enthält Kupfer und die Edelmetalle Silber und Gold. Die drei Metalle werden in der Chemie auch als „Münzmetalle“ bezeichnet. Des Weiteren enthält die Gruppe noch das künstlich erzeugte, radioaktive und extrem kurzlebige Roentgenium, welches bisher keine Anwendungen hat.

Gold wird seit Jahrtausenden für rituelle Gegenstände und Schmuck sowie seit dem 6. Jahrhundert v. Chr. in Form von Goldmünzen als Zahlungsmittel genutzt. Es wird auch als Lebensmittelzusatzstoff mit der E-Nummer "E175" verwendet.

Gold wurde in früheren Zeiten wegen seiner auffallend glänzenden gelben Farbe metallisch gediegen in der Natur gefunden. Es lässt sich sehr gut mechanisch bearbeiten und korrodiert nicht. Wegen der Beständigkeit seines Glanzes, seiner Seltenheit, seiner scheinbaren Unvergänglichkeit und seiner auffallenden Schwere wurde es in vielen Kulturen vor allem für herausgehobene rituelle Gegenstände und Schmuck verwendet.

Gold zählt zu den ersten Metallen, die von Menschen verarbeitet wurden. Die Goldgewinnung ist seit der frühen Kupferzeit nachgewiesen. Die leichte Legierbarkeit mit vielen Metallen, die moderate Schmelztemperatur und die günstigen Eigenschaften der Legierungen machten Gold als Werkstoff sehr attraktiv.

Die ältesten bislang bekannten Goldartefakte der Menschheit sind insgesamt etwa 3.000 goldene Objekte aus dem Gräberfeld von Warna (Bulgarien), die als Grabbeigaben niedergelegt wurden und zwischen 4600–4300 v. Chr. datiert werden. Mehr als 7.000 Goldobjekte sind aus dem 4. Jahrtausend v. Chr. aus Gräbern der osteuropäischen Maikop-Kultur bekannt. Der früheste Nachweis in Mitteleuropa liegt mit den beiden Goldscheiben im Depotfund von Stollhof (Niederösterreich) vor und stammt ebenfalls aus dem 4. Jahrtausend v. Chr. Seit dieser Zeit wurde Gold vereinzelt in Form von Schmuckgegenständen aus Südosteuropa importiert.

In Mittel- und Nordeuropa treten goldene Gegenstände vermehrt erst im dritten Jahrtausend v. Chr. als Grabbeigaben auf, vor allem in der endneolithischen Glockenbecherkultur. Beispiele sind die Ohrringe und die Haarspange beim Bogenschützen von Amesbury oder die 2013 gefundenen Goldringe eines Glockenbecher-Grabes aus Wustermark, Landkreis Havelland. Berühmte Beispiele aus der nachfolgenden Bronzezeit sind die Goldauflagen der Himmelsscheibe von Nebra (Frühbronzezeit) und die vier spätbronzezeitlichen Goldhüte.

Die Ägypter beuteten Vorkommen in Oberägypten und Nubien aus. So ist auf dem Turiner Papyrus u. a. auch die Lage einer Goldmine verzeichnet. Die Römer nutzten Fundstätten in Kleinasien, Spanien (Las Médulas), Rumänien und Germanien.

Die Sage von der Fahrt der Argonauten zum "Goldenen Vlies" nach Kolchis wurde anscheinend von den Seereisen griechischer Goldsucher angeregt.

In der Tora wird vom Goldenen Kalb erzählt, das sich die Israeliten als Götzenbild herstellten, während Moses die Zehn Gebote empfing, und vom Goldland Ophir. Das Neue Testament erwähnt Gold (neben Weihrauch und Myrrhe) als eines der Huldigungsgeschenke der Weisen aus dem Morgenland für den neugeborenen Jesus .

Auch in Südamerika und Mesoamerika wurde schon sehr früh Gold verarbeitet. So beherrschten beispielsweise die Mochica in Peru bereits Anfang des ersten Jahrtausends die Legierungsbildung (Tumbago) sowie die Vergoldung und stellten Gegenstände für rituelle Zwecke aus mehreren Kilogramm Gold her.

Die Goldgewinnung und -reinigung erfolgte durch Goldwäscherei, Amalgamation und Kupellation (Oxidieren unedlerer Metalle mit Blei, auch Läuterung genannt), auch in Kombination der Verfahren.

Die Gier nach Gold wurde mit der Vormachtstellung der europäischen Seemächte Spanien, Portugal, England und Italien zu einem maßgeblichen Grund für Kriege und Eroberungszüge der Neuzeit. Besonders der Goldreichtum der indigenen Völker in Mittel- und Südamerika lockte nach der Entdeckung Amerikas im Jahre 1492 europäische und insbesondere spanische Eroberer (Conquistadores) an, die Gold in Galeonen nach Europa brachten. Spanien wurde so eine Zeit lang zur reichsten Nation Europas; die indigenen Kulturen wurden durch die Eroberer bzw. durch eingeschleppte Krankheiten zerstört.

Immer wieder lockten Goldfunde große Scharen von Abenteurern an. Im 19. Jahrhundert kam es auf verschiedenen Kontinenten zu Goldrausch genannten Massenbewegungen von Goldsuchern in die Gebiete großer Goldvorkommen. Beispiele hierfür sind der kalifornische Goldrausch im Jahre 1849 und der Goldrausch des Jahres 1897 am Klondike River in Alaska. Auch in Australien (Bathurst, Temora, Teetulpa und Coolgardie) und Südafrika (Witwatersrand) kam es zum Goldrausch.

Der schwankende Goldpreis führt oft zu bedeutenden sozialen Veränderungen: So führte ein fallender Goldpreis in Südafrika zu einer starken Verarmung des von der Goldförderung lebenden Bevölkerungsteils. Im brasilianischen Amazonasraum ist der informelle Goldabbau durch Garimpeiros oft mit schwerwiegenden sozialen und ökologischen Folgen verbunden.

Der Goldanteil in der kontinentalen Erdkruste beträgt 0,004 ppm, also etwa 4 Gramm pro 1000 Tonnen Gestein. Der Anteil schwankt je nach Region – in Lagerstätten, die abgebaut werden, liegt der Goldanteil oft bei mehreren Gramm pro Tonne.

Gold kommt auf der Erde vorwiegend gediegen, das heißt in elementarer, metallischer Form vor. Es findet sich in primären Rohstoffvorkommen als goldhaltiges Gestein (Golderz) sowie in sekundären Vorkommen unter anderem in Seifen-Lagerstätten.

Etwa 44 % des 2015 geförderten Goldes stammen aus der Volksrepublik China, Australien, den Vereinigten Staaten, Russland und Kanada. Die tiefsten Goldbergwerke der Welt befinden sich in Südafrika. Dort wird Gold fast 4000 Meter unter der Erdoberfläche abgebaut. Anfang 2011 plante das Bergbauunternehmen AngloGold Ashanti bereits Schächte in bis zu 5000 Metern Tiefe.

2016 fielen rund 17% der geförderten Goldmengen als Nebenprodukt bei der Raffination anderer Metalle wie Kupfer, Nickel oder der anderen Edelmetalle an, sodass unter Umständen erst die Gewinnung dieses Nebenprodukts die Ausbeutung anderer Lagerstätten wirtschaftlich macht.

Insgesamt sind weltweit bisher (Stand 2017) fast 2700 Fundorte für gediegen Gold dokumentiert.

Die meisten Elemente bis zum Eisen, aber schwerer als Wasserstoff, sind unter Energieabgabe in unseren Sonnenvorgängern durch Kernfusionen entstanden (siehe auch Nukleosynthese). Das auf der Erde vorkommende Gold ist – wie alle Elemente, die schwerer sind als Eisen – bei dem Supernova-Kernkollaps unseres Sonnenvorgängers unter Energieaufnahme entstanden. 

1994 durchgeführte Computersimulationen sagten voraus, dass bei einer Kollision von zwei Neutronensternen aus dem dabei in den Weltraum herausgeschleuderten Material und den nachfolgenden Reaktionskaskaden nebst anderen schweren Elementen auch rund 30 Erdmassen Gold entstehen. Am 17. August 2017 erfassten die LIGO-Detektoren Gravitationswellen, die als Kollision von zwei Neutronensternen in einer Entfernung von 130 Millionen Lichtjahren gedeutet wurden. Die Reaktionen der herausgeschleuderten Materie konnten daraufhin mit optischen Teleskopen beobachtet werden. Die gemessenen Spektrallinien bestätigten die Voraussage, dass bei diesem Ereignis große Mengen an Gold und anderen schweren Elementen entstanden.

Solange die frühe Erde noch keine feste Kruste hatte, ist alles Gold aufgrund seiner hohen Dichte in den Erdkern gewandert. Wir finden nur noch Gold, das nach der Krustenbildung auf die Erde gelangt ist oder durch vulkanische Prozesse wieder an ihre Oberfläche kam.

Die folgenden Abschnitte führen einige der wichtigsten Typen primärer Goldlagerstätten auf:

Das Witwatersrand-Goldfeld in Südafrika ist mit Abstand das größte der Welt. Bislang hat diese Lagerstätte mehr als 40.000 t Gold geliefert. Die Erzkörper sind frühproterozoische (etwa 1,8 Milliarden Jahre alte) Paläo-Flussschotter, die gediegen Gold, Pyrit und lokal abbauwürdige Konzentrationen von Uranpechblende enthalten. Die genaue Genese der Lagerstätte ist umstritten. Klassisch wird die Lagerstätte als eine Paläo-Seifenlagerstätte interpretiert, womit sie unter die sekundären Lagerstätten fallen würde. Etwa 25 % des gefundenen Goldes weisen eine Form auf, die für einen Transport durch hydrothermale Lösungen typisch ist, während es sich bei 75 % des Goldes um die typischen Nuggets handelt, die für einen fluvialen Transport sprechen. Neuere Isotopenuntersuchungen legen allerdings eine sehr kleinräumige hydrothermale Mobilisation des Goldes von wenigen Millimetern bis Zentimetern nahe, sodass wahrscheinlich dieses Gold ursprünglich aus den Flussschottern stammt. Das Vorhandensein von gerundeten Pyrit- und Uranpechblende-Klasten zeigt aber auf jeden Fall an, dass diese zum ursprünglichen Bestand der Flussschotter gehörten. Sie zeigen damit an, dass die Erdatmosphäre zu diesem Zeitpunkt nur einen geringen Gehalt an Sauerstoff besessen haben kann, da diese Minerale unter oxidierenden Bedingungen nicht stabil sind.

Die Ressourcen der Lagerstätte liegen noch bei mehreren zehntausend Tonnen Gold, allerdings in erheblicher Tiefe. Hier befinden sich die tiefsten Bergwerke der Welt (nahezu 4000 m); ihr Abbau ist deshalb nur bei hohen Goldpreisen wirtschaftlich. Die Lagerstätte macht 40 % des weltweit bisher geförderten Goldes plus Ressourcen aus.


Es handelt sich meist um reine Goldlagerstätten ohne Gewinnungsmöglichkeit für andere Metalle. Einige wenige Lagerstätten enthalten allerdings solch hohe Gehalte an Arsen, dass sie zu den wichtigsten Vorkommen dieses Halbmetalls gehören.


Bedeutende Beispiele für diesen Lagerstättentyp gibt es unter anderem in Papua-Neuguinea, Neuseeland, Mexiko, Peru und Rumänien.

Bei diesem Typ handelt es sich um Lagerstätten in karbonatischen Gesteinen. Die bedeutendsten Vorkommen dieses Typs liegen in Utah und Nevada (USA). Die dortigen Lagerstätten bildeten sich in einem kurzen Intervall vor 42 bis 30 Millionen Jahren. Sie formten sich aus reduzierten, mäßig sauren Fluiden mit Temperaturen von 150 bis 250 °C in Tiefen über 2000 m. Die Erzkörper können wenige bis mehr als 100 Millionen Tonnen Erz enthalten bei Gehalten zwischen 1 und 10 g/t. Gold ist meist an feinverteilten arsenreichen Pyrit gebunden. Dadurch ist die Aufbereitung dieser Erze relativ aufwendig.

Diese Lagerstätten kommen in felsischen Magmatiten wie Graniten und Rhyolithen vor. Es handelt sich dabei um große hydrothermale Brekzienkörper mit hohen Gehalten an Eisen in Form von Hämatit und/oder Magnetit. Diese Lagerstätten entstanden vermutlich unter einem Vulkankomplex. Bei einem Ausbruch führten hydrothermale Fluide zur Bildung von Brekzien aus Magmatiten und setzten Eisenoxide, Kupfersulfide, gediegenes Gold sowie weitere Minerale ab. Die bedeutendsten Lagerstätten dieses Typs befinden sich in mesoproterozoischen Gesteinen Australiens wie Earnest Henry (Queensland), Prominent Hill und Olympic Dam (beide im Bundesstaat South Australia). Letztere stellt einen der größten Erzkörper der Erde dar mit derzeit vermuteten Ressourcen von 8,4 Milliarden Tonnen Erz. Die Erzgehalte liegen zwischen 0,5 und 2 % für Kupfer und 0,5 und 1,5 g/t Gold. In den meisten Lagerstätten dieses Typs befinden sich reine Kupfer- und Goldvorkommen, während Olympic Dam auch Uran und Silber enthält. Diese Lagerstätte stellt die größte bekannte Uranlagerstätte der Erde dar.

Solche Lagerstätten finden sich weltweit in jungen Gebirgskomplexen. Es handelt sich um große Erzkörper in intermediären bis sauren plutonischen Magmatiten. Die Erzminerale (Pyrit, Chalkopyrit, Bornit, Chalkosin, Molybdänit) kommen feinverteilt auf ein Netzwerk aus Klüften im Gestein vor. Die Erzkörper beinhalten einige 10 Millionen bis mehreren Milliarden Tonnen Erz. Die größte Lagerstätte dieses Typs ist Chuquicamata in Chile mit über 10 Milliarden Tonnen Erz. In den USA ist Bingham Canyon die bedeutendste Lagerstätte und einer der größten Goldproduzenten des Landes. Die Erzgehalte sind vergleichsweise gering mit 0,5 bis 1 % Kupfer und 0,1 bis 1 g/t Gold, aber die Größe der Erzkörper lässt eine wirtschaftliche Gewinnung zu. Oftmals sind diese Lagerstätten mit Skarnlagerstätten assoziiert und es finden sich epithermale Goldlagerstätten im weiteren Umfeld.

Diese Lagerstätten bilden sich im marinen Bereich. "Volcanic Hosted Massive Sulfides" (VHMS) sind an basische Magmatite (meist Basalte) gebunden, während "Sediment Hosted Massive Sulfides" (SHMS) in marinen Sedimentgesteinen vorkommen. Meist handelt es sich bei diesen Lagerstätten um reine Buntmetalllagerstätten (Blei, Zink, Kupfer), einige enthalten aber auch gewinnbare Beimengungen von Gold, Silber und anderen Elementen. Die devonische SHMS-Lagerstätte Rammelsberg bei Goslar im Harz stellt mit 28 Millionen Tonnen Erz und einem Goldgehalt von 1 g/t als Beimengung zu den extrem hohen Blei- und Zinkgehalten die bedeutendste deutsche Goldlagerstätte dar.

Fast alle europäischen Flüsse führen Spuren von Gold mit sich. Dieses Gold war zuvor in Form zumeist kleiner, dünner Blättchen in Gestein eingelagert. Durch Verwitterungsprozesse des umgebenden Gesteins wird es freigesetzt und gelangt so ins Flusswasser und wird als Fluss-Seife abgelagert.

Auf den Geröllbänken des Hoch- und Oberrheines wie bei Istein finden sich geringe Mengen, insbesondere Flitter. Diese, als Rheingold bezeichneten Sekundärablagerungen, wurden in den vergangenen Jahrhunderten und bislang, mit mäßigem Ertrag, ausgewaschen. Der einzige offizielle Goldproduzent Deutschlands, ein seit 2008 zur Holcim-Gruppe gehörendes Kieswerk bei Rheinzabern, nutzt ebenfalls diese Vorkommen.

Die Weltjahresförderung betrug 2008 noch 2260 Tonnen, 2011 bereits 2.700 Tonnen, etwa hundertmal mehr als im 19. Jahrhundert. Aktuell wird in zwei Jahren mehr Gold gefördert, als in den tausend Jahren des Mittelalters zusammen dokumentiert ist.

Das meiste Gold wurde lange Zeit in Südafrika gefördert. Im Jahr 2007 förderte Australien die größte Menge. Seit 2008 stammt die größte Fördermenge aus der Volksrepublik China, gefolgt von Australien. Seit 2008 fördern die USA mehr Gold als Südafrika, seit 2010 liegt die Fördermenge der Russischen Föderation über der von Südafrika.

Weltweit existieren nur wenige große Goldförderunternehmen, deren Aktien an den Börsen gehandelt werden. Dazu gehören etwa Agnico Eagle Mines, AngloGold Ashanti, Barrick Gold, Freeport-McMoRan Copper & Gold, Gold Fields Ltd., Goldcorp, Kinross Gold, Newmont Mining und Yamana Gold.

Die Förderung von Gold in Europa – am meisten in Finnland und Schweden – ist im internationalen Vergleich unbedeutend. Die rumänischen Golderzvorkommen sind wohl die größten in Europa. In Bulgarien finden in den stillgelegten Goldminen "Zlata" (aktiver Bergbau: 1939–1973) und "Krushov Dol" (aktiv: 1965–1974) wieder Erkundungen statt. In "Barsele" (in der Gemeinde Storuman) in Schweden wurde ein Vorkommen erkundet.

In der gesamten Geschichte der Menschheit wurden bisher schätzungsweise 170.000 Tonnen Gold geschürft (Stand 2011). Dies entspricht einem Würfel mit 20,65 Metern Kantenlänge (rund 8800 Kubikmetern) reinem Gold, und rund 24,3 g (also etwas mehr als ein Kubikzentimeter) pro Kopf der Weltbevölkerung.

Natürliche Vorkommen an gediegen Gold, das heißt in seiner elementaren Form waren bereits vor der Gründung der International Mineralogical Association (IMA) bekannt. Gold ist daher als sogenanntes "grandfathered" Mineral als eigenständige Mineralart anerkannt.

Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Gold unter der System-Nr. „1.AA.05“ (Elemente – Metalle und intermetallische Verbindungen – Kupfer-Cupalit-Familie – Kupfergruppe) beziehungsweise in der veralteten 8. Auflage unter "I/A.01" (Kupfer-Reihe) eingeordnet. Die vorwiegend im englischsprachigen Raum verwendete führt das Element-Mineral unter der System-Nr. „01.01.01.01“ (Goldgruppe).

In der Natur findet sich Gold üblicherweise in Form von abgerundeten Nuggets, als Schuppen oder Flocken sowie in dendritischen (baumartig) oder haar- bis drahtförmigen Aggregaten. Selten entwickelt Gold auch grobkristalline Stufen mit oktaedrischen, dodekaedrischen und würfeligen Kristallen. Es kann mit verschiedenen Mineralen vergesellschaftet sein wie unter anderem Altait, Ankerit, Arsenopyrit, Calaverit, Chalkopyrit, Krennerit, Pyrit, Pyrrhotin, Quarz, Scheelit, Sylvanit ("Schrifterz"), Tetradymit und Turmalin.

Da Gold ein relativ reaktionsträges Element ist, behält es gewöhnlich seinen Glanz und Farbe und ist daher in der Natur leicht zu erkennen. Dennoch wird es immer wieder mit farblich ähnlichen Mineralen wie Pyrit ("Katzengold", "Narrengold") und Chalkopyrit verwechselt. Gold ist zudem ein Bestandteil verschiedener Mineralarten. Beispiele für Minerale mit den höchsten Goldgehalten sind unter anderem Bezsmertnovit ((Au,Ag)Cu(Te,Pb); 78,56 % Au), Tetra-Auricuprid (CuAu; 75,61 % Au), Maldonit (AuBi; 65,34 % Au) und Yuanjiangit (AuSn; 62,40 % Au). Insgesamt sind bisher 33 bekannt (Stand 2017).

Im Gegensatz zu den meisten anderen Metallen kommt das chemisch inerte Gold meist gediegen vor und muss nicht durch Reduktion aus Erzen gewonnen werden, wie Eisen. Es wird nur mechanisch aus dem umgebenden Gestein gelöst. Da Gold chemisch wenig reaktiv und somit nur schwierig in lösliche Verbindungen überführt werden kann, werden spezielle Verfahren zur Goldgewinnung angewendet.

Ohne Lupe direkt sichtbares Gold, sogenanntes „Freigold“ in Form von Nuggets oder Goldstaub, ist eine Rarität. Das größte bekannte Goldnugget, „Welcome Stranger“ genannt, wurde 1869 in Australien gefunden und wog 2284 Feinunzen (rund 71 kg). Das meiste Gold in den Vorkommen liegt in kleinsten Partikelchen im umgebenden Gestein fein verteilt vor und entgeht somit den Versuchen, es mit einfachen Verfahren manuell zu sammeln.

In der Praxis werden mehrere Verfahren miteinander kombiniert, um die gewünschte hohe Ausbeute zu erhalten. Durch Fortschritte in den Gewinnungsmethoden, Vernachlässigung der Abfallproblematik und bei hohem Marktpreis lohnt sich sogar der Abbau von Erz, das nur ein Gramm Gold pro Tonne enthält. Alte Abraumhalden ehemaliger Goldvorkommen werden deshalb mittels verbesserter Technik nochmals aufgearbeitet.

Gold fällt als Nebenprodukt bei der Raffination anderer Metalle an und wird in großem Umfang wiedergewonnen. Mehr als zehn Prozent des weltweit abgebauten Goldes wird im Kleinbergbau gewonnen. Ein Teil davon kann als Konfliktrohstoff betrachtet werden, der eine negative Auswirkung auf die dort lebende Bevölkerung hat und zum sogenannten Ressourcenfluch führen kann.

Das sogenannte Goldwaschen als einfachstes Verfahren zur Goldgewinnung nutzt die hohe Dichte des Metalls. Dabei wird goldhaltiger Sand mit Wasser aufgeschlämmt. Da Gold schwerer ist als der umgebende Sand, setzt sich das Gold schneller am Boden ab und kann so abgetrennt werden. Gold aus Flussablagerungen wird so gewonnen. Die Hobby-Goldsucher wenden meist dieses Verfahren an. Dessen Nachteil besteht jedoch in der geringen Ausbeute bei großem Zeitaufwand des Suchenden. Der Vorteil dieser Methode ist die zuverlässige Ausbeute an groben Goldteilchen, die bei der Cyanidlaugerei nicht vollständig erfasst werden. Es lässt sich verbessern durch Einbringen von Fellen in die abströmende Flüssigkeit, in dem sich kleinste Goldpartikelchen in den Fellhaaren verfangen und die Ausbeute erhöhen.

Goldwaschen wird mitunter teilmechanisiert an Land durchgeführt oder mit Schwimmbaggern mit integrierter Wäsche direkt im Fluss. Minentechnisch gewonnenes Erz wird zuvor mechanisch auf geeignete Korngrößen zerkleinert und das zermahlene Gestein in ähnlicher Weise bearbeitet.

Dieses Verfahren geht der nachfolgend beschriebenen weiteren Ausnutzung der goldführenden Sande und Schlämme voraus.

Beim Amalgamverfahren wird die Legierungsbildung zwischen Gold und Quecksilber zu Amalgam genutzt. Zur Goldgewinnung und -reinigung werden goldhaltige Sande und Schlämme intensiv mit Quecksilber vermischt. Das Gold, aber auch eventuell andere vorhandene gediegene Metalle wie Silber lösen sich dabei im Quecksilber. Goldamalgam hat eine silberne Farbe; je nachdem, wie viel Quecksilber im Überschuss vorliegt, ist es flüssig bis pastös teigig und der Schmelzpunkt der Legierung ist geringer. Amalgam und Quecksilber sammeln sich wegen der hohen Dichte am Gefäßgrund, das Quecksilber fließt ab. Durch Erhitzen des Amalgams (wie bei Feuervergoldung detailliert beschrieben) verdampft das Quecksilber und zurück bleibt kompaktes Rohgold.

Die entstehenden Quecksilberdämpfe stellen bei unsachgemäßer Handhabung eine große gesundheitliche Gefahr dar (siehe Quecksilbervergiftung). Speziell einfache Goldschürfer praktizieren keine Wiedergewinnung des Quecksilbers (durch Destillation oder Absaugung und Abfilterung mit Aktivkohle). Stattdessen wird das Amalgam in offenen Blechgefäßen mithilfe von Lötlampen und sonstigen Gasbrennern erhitzt. Das Quecksilber (Siedepunkt 357 °C) dampft dabei in die Umgebungsluft ab und kondensiert alsbald in der Umwelt. Dadurch werden die Böden in der Umgebung und die Flüsse mit Quecksilber verseucht, die dort lebenden Menschen werden mit Quecksilber belastet. Durch das Minamata-Übereinkommen sollen Alternativen zum Amalgamverfahren gefördert werden.

Das Amalgamverfahren wurde bereits in der Antike angewendet. Es wurde geschätzt, dass 20 % bis 30 % des weltweit geförderten Goldes durch nicht industrielles Schürfen, also von Goldsuchern gewonnen wird.

Bei größeren Vorkommen, die eine industrielle Erschließung erlauben, wird seit Ende des 19. Jahrhunderts die "Cyanidlaugung" angewendet. Vor dem Hintergrund, dass sich Gold in sauerstoffhaltiger Natriumcyanid-Lösung (Natriumsalz der Blausäure HCN) als Komplexverbindung löst, werden die metallhaltigen Sande staubfein gemahlen, aufgeschichtet und im Rieselverfahren mit der Extraktionslösung unter freiem Luftzutritt versetzt. Die kleinsten Metallteilchen werden hierbei zuerst aufgelöst, weil sie die relativ größte Reaktionsoberfläche haben.

Das Edelmetall findet sich chemisch gebunden im hochgiftigen Sickerwasser. Nach Filtration und Ausfällung mit Zinkstaub wird es als brauner Schlamm erhalten, aus dem nach Waschen und Trocknen durch Reduktion Rohgold wird.

Hier schließt sich die Reinigung des Rohgoldes an. Raffiniert zu Feingold ist es dann standardisiert und marktreif. Die Cyanidlaugen werden in Kreislaufprozessen wiederverwendet. Dennoch entweichen Blausäure und ihre Salze (Cyanide) in die Umwelt, teilweise in größeren Mengen, etwa bei Unglücken, Fehlfunktionen der Anlage, Überschwemmungen. Alle diese Stoffe sind hochgiftig, allerdings leicht zersetzbar. Im Stoffkreislauf der Natur werden sie relativ schnell oxidativ abgebaut oder durch Hydrolyse zersetzt.

Diese Art der Goldgewinnung hinterlässt enorme Abraumhalden und Stäube mit Cyanidspuren. Umweltschäden entstehen dadurch, dass Schlamm in Ländern mit geringer Umweltüberwachung unkontrolliert in Flüsse abgeleitet wird oder Schlammabsetzbecken bersten, wie im Jahr 2000 im rumänischen Baia-Mare.

Ein umweltfreundlicheres Verfahren stellt die Goldextraktion und -reinigung mithilfe von Borax (Natriumborat) dar. Der Zusatz von Borax als schlackenbildendes Flussmittel beim Schmelzen von verunreinigtem Gold setzt Schmelzpunkt und Viskosität der Schmelze aus Oxiden und Silikaten der Begleitstoffe ("nicht des Goldes" wie es oft fälschlicherweise angegeben wird) herab. Dadurch kann das Schmelzen mit einfacheren, kostengünstigen Brennern erfolgen (mit Zusatz von Holzkohle und extra Luftzufuhr unter Verwendung eines Haartrockners oder Blasebalgs), wobei die Ausbeute der Extraktion erhöht wird. Das Gold (oder bei Anwesenheit von Silber eine Gold-Silber-Legierung) setzt sich dabei am Boden der Schmelzpfanne ab, die Oxide schwimmen auf. Gelegentlich werden andere Flussmittel zugesetzt (beispielsweise Calciumfluorid, Natriumcarbonat, Natriumnitrat oder Mangandioxid). Würden alle Goldschürfer auf der Welt dieses Verfahren anwenden, könnte die Emission von rund 1000 Tonnen Quecksilber vermieden werden, das sind etwa 30 % der weltweiten Quecksilber-Emissionen.

Gold wird häufig aus Anodenschlämmen gewonnen, die bei der Raffination anderer Metalle, vor allem von Kupfer, zurückbleiben. Während der Elektrolyse wird das edle Gold nicht oxidiert und nicht gelöst; es sammelt sich unter der Anode an. Neben Gold fallen dabei Silber und andere Edelmetalle an, die durch geeignete Verfahren voneinander getrennt werden.

Eine wichtige Quelle des Edelmetalls ist die Aufbereitung von Dental- und Schmuckverarbeitungsabfällen sowie alten edelmetallhaltigen Materialien, wie selektierter Elektronikschrott und Galvanikschlämme.

In den Klärschlämmen der Städte sind bemerkenswerte Goldspuren enthalten, die von der Nutzung, der Verarbeitung und dem Verschleiß von Goldlegierungen (Abrieb von Zahnfüllungen, Schmuckkettenglieder, Verlust usw.) stammen. Eine Untersuchung verschiedener Proben aus Arizona ergab beispielsweise neben verschiedenen anderen Edelmetallen einen Gehalt von durchschnittlich 0,3 Gramm Gold pro Tonne Klärschlamm. 2017 konnten in einer Schlackensortieranlage in der Schweiz 65 Kilogramm Gold im Wert von 2,1 Millionen Franken gewonnen werden.

Im September 2013 berieten Österreichs Krematorienbetreiber, wie mit dem Gold verbrannter Verstorbener rechtlich umgegangen werden soll, das bislang verklumpt mit Knochenasche in der Urne den Hinterbliebenen ausgefolgt wird.

Fritz Haber versuchte in den 1920er Jahren, Gold aus dem Meerwasser zu gewinnen, womit die deutschen Reparationen bezahlt werden sollten. Es wurde damals angenommen, dass Meerwasser zwischen 3 und 10 Milligramm Gold pro Tonne enthält. Der durchschnittliche Gehalt war aber mit 0,0044 Milligramm Gold pro Tonne Meerwasser etwa um den Faktor 1000 niedriger und für eine wirtschaftliche Verwertung deutlich zu gering. Durch moderne Messmethoden wurde festgestellt, dass der Atlantik und der nordöstliche Pazifik 50–150 Femtomol (fmol) Gold pro Liter Wasser beinhaltet. Das entspricht 0,010–0,030 µg/m³. Im Tiefenwasser des Mittelmeeres lassen sich eher höhere Werte von 100–150 fmol Gold pro Liter Meerwasser messen. Insgesamt ergibt das 15.000 Tonnen Gold in den Weltmeeren.

Die Hoffnung, Gold künstlich herstellen zu können, wurde von vielen Kulturen über Jahrhunderte gehegt. Dabei entstand unter anderem die Hypothese vom sogenannten "Stein der Weisen", der Gold aus unedlen Metallen entstehen lassen sollte. Die Alchemie wurde gelegentlich als „künstliche Darstellung von Silber und Gold“ oder schlicht als „Goldmacherei“ aufgefasst.

Beispielsweise wird in zwei ostmitteldeutschen Handschriften des 15. Jahrhunderts ein Nikolaus von Paris genannt, nach dessen alchemistischem Traktat "Von silber unde von golde" Gold herstellgestellt werden könne, indem Silber und „rotes Eisen“ mit Salmiak versetzt werden, diese Mischung eine Woche in heißem Pferdemist belassen wird, danach gefiltert und auf die Hälfte eingedampft wird und mit der dadurch entstandene Substanz Silber in 12karätiges Gold transmutiert werden könne. Wenn dann ein Teil dieses Goldes mit vier Teilen natürlichem Gold gemischt wird, solle 20karätiges Gold entstehen.

Gold entsteht allein bei verschiedenen kerntechnischen Prozessen (Kernfusion resp. Kernfission) in winzigen Mengen.

Beträchtliche Mengen von hochgiftigem Quecksilber, schon bei der Goldgewinnung mit ausgeschwemmt oder beim Verdampfen wissentlich in die Umwelt freigesetzt, vergiften große Gebiete und Flussläufe dauerhaft. Da Goldgewinnung oft improvisatorische Züge trägt und fernab von effektiver behördlicher Überwachung stattfindet, werden Umweltaspekte untergeordnet oder ignoriert.

Die negativen Umweltauswirkungen führen häufig zu Konflikten zwischen den Goldschürfern und der einheimischen Bevölkerung. Es gibt jedoch erste Projekte ökologischen Goldabbaus, wie das "Oro Verde" in Kolumbien. Für Barren, deren Gold aus dieser Mine stammt, wurde im Februar 2011 erstmals das Fair-Trade-Siegel vergeben. Europas erste Lieferanten für "Faires Gold" waren in Frankreich und Großbritannien, seit einiger Zeit ist es – neben nicht zertifiiertem – auch bei ÖGUSSA erhältlich, die dafür eine eigene Prozesslinie einrichtete. Der Wiener Juwelier Nikl sieht auch in Gold, das schon länger am Markt ist und als Altgold recycliert wird Faires Gold, zusätzlichen Bedarf von typisch 0,5 kg pro Jahr kauft er als "Fair Mined Gold", das aus kleinen südamerikanischen Minen stammt mit 4000 € pro kg Aufschlag auf den Tagespreis. Dieser feste Aufschlag betrug ehemals – bei seiner Einführung vor Jahren – +10 % auf den Goldpreis und liegt bei gesunkenem Goldkurs bei relativ +10...15 %. Etwa ein Fünftel der Kunden fragt aktiv danach. Kunden zahlen dafür etwas mehr und erkaufen sich damit ein besseres Gewissen gegenüber den unter sehr harten Bedingungen arbeitenden Minenbauern.

Gold besteht aus nur einem stabilen Isotop und gehört damit zu den 22 Reinelementen. Das Schwermetall ist unlegiert weich wie Zinn mit einer Mohshärte von 2,5 bis 3 (VHN = 30–34; silberhaltig 44–58). Gold lässt sich aufgrund seiner Duktilität zu Blattgold schlagen und zu besonders dünnen Folien von etwa 2000 Atomlagen ausrollen. Deshalb verwendete Ernest Rutherford Goldfolie für seinen Streuversuch. Weißes Licht schimmert grünlich durch Goldfolie. Darüber hinaus lässt es sich leicht mit vielen Metallen legieren.

Gold kristallisiert ausschließlich in einem kubisch flächenzentriertem Raumgitter und weist damit eine kubisch dichteste Kugelpackung mit der auf. Der Gitterparameter beträgt bei reinem Gold 0,4078 nm (entspricht 4,078 Å) bei 4 Formeleinheiten pro Elementarzelle.
Reines Gold hat eine metallisch-sattgelbe Farbe, die entsprechend als „goldgelb“ bekannt ist, und eine ebensolche Strichfarbe. In feiner Verteilung ist es je nach Korngröße gelblich, ockerbraun bis purpurviolett und wird dann als Goldpurpur bezeichnet. Mit zunehmender Temperatur verliert Feingold an Farbintensität und ist hellgelb glühend, bevor es schmilzt. Das geschmolzene Metall ist zitronengelb, leicht grünlich und erhält seine intensive gelborange Farbe erst wieder, wenn es vollständig abgekühlt ist. Vor dem Lötrohr ist Gold leicht schmelzbar zu einer vollkommenen Kugel.

Beimengungen von Kupfer lassen es rosa oder rötlich erscheinen, senken die Schmelztemperatur und steigern zugleich Härte, Festigkeit und Polierbarkeit beträchtlich. Steigende Silberanteile verändern die Farbe des reinen Goldes über hellgelb nach hellgrün und schließlich zu weiß; Schmelztemperatur und Härte verändern sich dabei nur sehr wenig. Die meisten Metalle, so auch die bekannten Platinmetalle, Quecksilber und die Eisenmetalle, führen als Beimischungen dagegen in steigenden Anteilen zu einer Entfärbung in Form einer eher schmutziggelbgrauen bis grauweißen Legierung. So variiert die Farbe von palladiumhaltigem Gold ("Porpezit") zwischen lohfarben und hellbraun.

Einige der ungewöhnlichen Eigenschaften wie die goldgelbe Farbe und hohe Duktilität werden aktuell mit dem Einfluss von Relativistischen Effekten auf die Elektronenorbitale erklärt. So entsteht die gelbliche Farbe durch Absorption im Frequenzbereich der Komplementärfarbe Blau. Ursache dafür ist die auf Grund relativistischer Effekte vergleichsweise kleine Bandlücke zwischen dem 6s- und den 5d-Orbitalen.

In der Oberflächenchemie werden verschiedene Flächen von Au-Einkristallen u. a. in der Rastertunnelmikroskopie eingesetzt (siehe Abbildung).

Die spezifische Verdampfungsenthalpie ΔH von Gold ist mit 1,70 kJ/g wesentlich geringer als beispielsweise diejenige von Wasser (mit 2,26 kJ/g) oder Eisen (6,26 kJ/g, alle für die Siedetemperatur bestimmt). Bei überhitzten Goldschmelzen können daher (wie auch bei anderen Schmelzemanipulationen etwa in der Stahlindustrie) beträchtliche Rauch- und Verdampfungsverluste auftreten, sofern der Schmelzvorgang ohne Abdichtung oder Absaugung und Abscheidung in Aktivkohle erfolgt.

Gold wird von gewöhnlichen (Mineral-)säuren nicht angegriffen. Lediglich einige stark oxidierende Säuren wie Königswasser (einem Gemisch aus Salzsäure und Salpetersäure) oder Selensäure lösen Gold. In Königswasser bildet sich Tetrachloridogoldsäure:

Die Halogene Chlor, Brom und Iod vermögen Gold zu lösen, letzteres sogar in alkoholischer Lösung. In wässrigen Cyanidlösungen ist Gold leicht unter Oxidation durch Sauerstoff als Kaliumdicyanidoaurat(I) löslich. In heißen, sauren hydrothermalen Lösungen ist Gold relativ gut physikalisch löslich. Demzufolge wird es oft in Quarzgesteinen mit vorgefunden. Es wurde beobachtet, dass einige Huminsäuren in der Lage sind, Gold anzulösen.

Rund die Hälfte des am Markt gehandelten Goldes wird zu Schmuck verarbeitet, etwa ein Drittel wird von institutionellen und privaten Investoren erworben (ohne Zentralbanken), 9 % werden in der Industrie einschließlich Zahntechnik verwendet (Durchschnittswerte für 2010–2014). Die Aufkäufe durch Zentralbanken haben stark zugenommen: von 2 % der weltweiten Nachfrage im Jahr 2010 auf 14 % im Jahr 2014. Siehe auch Tabelle zu Angebot und Nachfrage 1997–2011.

Der größte Teil des gewonnenen Goldes wird in der Schmuckindustrie verwendet. Goldschmiede verarbeiten Gold und andere Edelmetalle zu Ringen, Ketten, Armbändern und anderem Schmuck. Der Edelmetallgehalt wird durch die Repunze beglaubigt. Einige Orden sind aus Gold gefertigt (Kutusoworden). Indien und China sind die beiden größten Märkte für Goldschmuck, zusammen sorgen sie für über 50 % der Nachfrage nach Gold in diesem Bereich.

Goldfolie, auch Blattgold genannt, gibt nichtmetallischen Gegenständen, wie Bilderrahmen, Büchern (Goldschnitt), Mobiliar, Figuren, Architekturelementen, Stuck, Ikonen, das Aussehen von echtem Gold. Seit der Antike wird Blattgold von Goldschlägern aus hochgoldhaltigen Legierungen hergestellt. Dabei wird Gold dünner als die Wellenlänge des sichtbaren Lichtes gewalzt und geschlagen. Im Auflicht glänzt die Folie goldgelb, im Gegenlicht scheint grünlich-blau die Lichtquelle durch und bildet das Schlagmuster des Metalls ab. Der Vergolder präpariert die Unterlage zunächst mit einem Klebemittel und legt anschließend die Goldfolie auf. Mit 1 Gramm Blattgold kann ein halber Quadratmeter Fläche überzogen werden.

Dekorativ findet Gold vielfältige Anwendungen, zum Beispiel in galvanischen Beschichtungen von Metallen und Kunststoffen. Auf Porzellanglasuren, Zahnersatzkeramiken und Glas lassen sich Goldpigmente einbrennen. Historisch war die Feuervergoldung von Metallen mit Hilfe der Gold-Quecksilber-Legierungen, sogenannter Amalgame, nachweislich schon in der Antike die einzig brauchbare Methode, um dauerhafte Vergoldungen auf Silber, Bronze oder unedlen Metallen herzustellen. Mit der Entwicklung galvanischer Vergoldungsbäder im späten 19. Jahrhundert und 20. Jahrhundert wurde dieser Bereich in den Möglichkeiten qualitativ erweitert und ersetzt.

Goldpigmente wurden historisch in der Glasherstellung seit dem 16. Jahrhundert eingesetzt (Goldrubinglas), werden allerdings weitgehend durch preiswertere Verfahren ersetzt.

Im Speisenbereich wird Gold als Lebensmittelzusatzstoff E 175 verwendet. In Form von Blattgold und Blattgoldflocken dient es zum Vergolden von Speisen, zum Beispiel für Überzüge von Süßwaren und zur Verzierung von Pralinen. Der teuerste Burger der Welt (Stand 2017) wird mit Blattgold zubereitet. In Getränken wird es für Danziger Goldwasser und Schwabacher Goldwasser verwendet. Metallisches Gold gilt als ungiftig, reichert sich im Körper nicht an und wird mit dem Rest der verdauten Nahrung wieder ausgeschieden.

Gold dient in Form von Goldmünzen und Barrengold als Wertanlage und als internationales Zahlungsmittel. Gold wird von vielen Zentralbanken der Welt als Währungsreserve eingelagert, obwohl die Währungen nicht mehr durch Goldreserven gedeckt sind.

Private und institutionelle Anleger investieren in Gold und in Wertpapiere, die den Goldkurs abbilden. In Krisenzeiten (Inflation oder Wirtschaftskrise) wird Gold als stabile Wertanlage gesehen, die Wertsteigerungen relativ zu anderen Wertanlagen erfahren kann. Der intrinsische Wert von Gold wird durch seine relative Seltenheit und durch die durchschnittlich aufgebrachte Arbeitsleistung bei seiner Förderung bestimmt. Deswegen hat Gold kein Ausfallrisiko wie sonstige Papiergeldanlagen, wo die Zinsrate sich nach dem wahrgenommen Ausfallrisiko der Marktteilnehmer richtet. Bei dieser Betrachtung wird allerdings häufig ausgeblendet, dass der Goldpreis im Zeitablauf starken Schwankungen ausgesetzt ist.

Der Preis des Goldes wird auf dem offenen Markt bestimmt. Das geschieht seit dem 17. Jahrhundert am London Bullion Market. Seit dem 12. September 1919 treffen sich wichtige Goldhändler in einer Rothschild-Bank in London, um den Goldpreis formal zu fixieren (siehe Goldfixing). Seit 1968 gibt es ein weiteres tägliches Treffen in der Bank um 15 Uhr Londoner Zeit, um den Preis zur Öffnungszeit der US-Börsen erneut festzulegen. Für den standardisierten Goldhandel an Rohstoffbörsen wurde „XAU“ als eigenes Währungskürzel nach ISO 4217 vergeben. Es bezeichnet den Preis einer Feinunze Gold.

Am 17. März 1968 wurde der Goldpreis gespalten und ein zweigliedriges System eingeführt. Der eine Preis konnte sich frei dem Markt anpassen, der andere war fix. 1973 wurde der Goldpreis freigegeben, und der Besitz von Gold war in den USA wieder erlaubt. China hat den Privatbesitz von Gold 1983 wieder erlaubt (siehe Goldverbot).

Der Goldpreis ist unter anderem von den aktuellen Fördermengen, vom Ölpreis und vom Kurs des US-Dollars abhängig (Gold wird in US-Dollar gehandelt). Er kann von den Zentralbanken beeinflusst werden, die zusammen etwa 30.750 Tonnen Gold besitzen (Stand Dezember 2011), das sind knapp 19 % der weltweit vorhandenen Goldmenge von 170.000 Tonnen.

Historisch wurde Gold seit Jahrtausenden als Währung eingesetzt. Eine Geldeinheit entsprach einer bestimmten Menge Gold. In Deutschland war während des Deutschen Reichs von 1871 bis 1918 das gesetzliche Zahlungsmittel die Goldmark (siehe auch Kurantmünze), wobei 2,79 Goldmark einem Gramm Gold entsprachen und die Reichsbank gegen Vorlage einer Banknote die entsprechende Menge in physisches Gold eintauschte. Die Golddeckung wurde zu Beginn des Ersten Weltkrieges aufgehoben; und konnte danach nicht wieder eingeführt werden wegen der Reparationen, die die Goldreserven des Deutschen Reiches verschlangen, und wegen der Vervielfachung der in Umlauf gebrachten Papiergeldmenge. Diese faktische Umstellung auf nicht-goldgedecktes Geld (Vertrauenswährung oder Fiat Money) führte bereits während des Krieges zur Abwertung der Mark und ermöglichte die Hyperinflation der 1920er Jahre.

Lange Zeit entsprachen in den Vereinigten Staaten 20,67 US-Dollar einer Unze Gold. 1934 fand eine Abwertung des US-Dollars durch die Neufestlegung des Goldpreises auf 35 US-Dollar je Feinunze statt. Das neue Verhältnis wurde im Bretton-Woods-System von 1944 bestätigt.

Um Gold als Währungsalternative auszuschließen und um die Währungsreserven (Goldreserve) zu erhöhen, war der Goldbesitz in den USA zeitweise verboten. Von 1933 bis 1973 war Goldbesitz nur in Form von Schmuck und Münzsammlungen erlaubt. Präsident Franklin D. Roosevelt ließ Gold über die "Executive Order 6102" konfiszieren. Präsident Richard Nixon beendete 1971 das Bretton-Woods-System und schaffte dessen Versprechen ab, dass alle Nationalbanken eine Feinunze Gold für 35 US-Dollars von der US-Notenbank verlangen können.

Da der Goldstandard die herausgegebene Geldmenge und die Höhe der Staatsverschuldung beschränkt, waren die Regierungen daran interessiert, ihre Währungen vom Gold zu lösen. In beiden Weltkriegen wurde der Goldstandard aufgegeben, da die benötigten Geldmittel zur Kriegsproduktion nur per Inflation aufzubringen waren. Heutzutage sind sämtliche Währungen der Welt vom Gold losgelöst, und erst dadurch war die extreme Ausweitung der heutigen Geldmengen und Schulden möglich. Die vorhandene Goldmenge würde zu den aktuellen Kursen nicht als Wertdeckung für eine bedeutsame Währung ausreichen. Das im Januar 2006 vorhandene Gold entsprach einem Marktwert von 2,5 Billionen € und wäre hypothetisch somit gerade einmal geeignet gewesen, die damaligen Staatsschulden Deutschlands und Spaniens zu decken. Im Falle einer erneuten Deckung von bedeutenden Währungen müsste der Goldkurs auf ein Vielfaches ansteigen.

Die Elektronikindustrie verwendet Gold u. a. aufgrund der guten Verarbeitbarkeit und hervorragenden Kontaktgabe (hohe Korrosionsbeständigkeit, leichte Lötbarkeit):

Wegen seiner Korrosionsbeständigkeit und ästhetischen Qualitäten wird es in der Zahnprothetik als Füll- oder Ersatzmaterial für defekte oder fehlende Zähne eingesetzt. Dabei kommen Legierungen zum Einsatz, da reines Gold zu weich wäre. Diese bestehen meist zu rund 80 % aus Gold und zu 20 % aus Beimetallen wie etwa Platin. Die Popularität von Goldzähnen hat in westlichen Ländern zugunsten unauffälligerer Kunststoffimplantate abgenommen, während sie in vielen anderen Teilen der Erde nach wie vor häufig verwendet werden. 

Einige Goldsalze werden heilend zur Rheumatherapie eingesetzt. Die Goldsalze Natriumaurothiomalat und Auranofin werden als Basistherapie gegen rheumatoide Arthritis (chronische Polyarthritis) angewendet. Goldtherapien erreichen ihre volle Wirkung erst nach mehreren Monaten und sind mit Nebenwirkungen verbunden. Es kann zu allergischen Reaktionen und bei unsachgemäßer Anwendung zu einer Schädigung von Leber, Blut und Nieren kommen. Etwa 50 % der Therapien mit Goldsalzen werden aufgrund der unerwünschten Wirkungen abgebrochen. In neuerer Zeit verdrängen preisgünstigere Medikamente mit besserem Nebenwirkungsprofil die Behandlung mit goldhaltigen Therapeutika.

1913 ließ der Arzneimittelhersteller Madaus das homöopathische Präparat "Essentia Aurea: Goldtropfen" patentieren, das unter der Marke „Herzgold“ verkauft und gegen Herz- und allgemeine Schwächezustände angewandt wurde.

Gold reflektiert Infrarotlicht sehr gut (98 % bei Wellenlängen > 700 nm) sowie rotes und gelbes Licht besser als blaues und violettes, deshalb werden wärmereflektierende Beschichtungen auf Gläsern, Strahlteiler und Spiegel – auch Laserspiegel für Laser im mittleren Infrarot – aus Goldschichten hergestellt (Sputtern, Bedampfen, mit Schutzschicht).

Gold ist ein Dotand von Germanium (Germanium-Gold, kurz Ge:Au) – einem Halbleiter zum Nachweis von Infrarot von 1 bis etwa 8 µm Wellenlänge bei Kühlung auf 77 K nach dem Prinzip der Photoleitung.

Nanoskopisch vorliegende metallische Goldpartikel, also solche mit einer Größe im Nanometer-Maßstab, sind in jüngster Zeit Schwerpunkt intensiver Forschung geworden, weil ihre Verwendung als heterogene Katalysatoren in organisch-chemischen Reaktionen neue, Lösungsmittel-freie Verfahren zulässt. Dies ist Teil eines Prozesses der Umgestaltung der chemischen Produktionsweisen in Richtung einer grünen Chemie. Weiterhin werden Gold-Nanopartikel als inertes Trägermaterial mit verschiedenen Molekülen beschichtet, etwa zur Verwendung in einer Genkanone.

In diesem Zusammenhang wurde entdeckt, dass Gold-Nanopartikel nach Adsorption chiraler Substanzen selbst chirale Strukturen aufweisen können. Die Chiralität dieser Partikel kann durch die Enantiomere der Adsorbentien gesteuert werden, bleibt jedoch erhalten, wenn in achiraler (racemischer) Umgebung verfahren wird.

Die Reinheit von Gold wird historisch in Karat (abgekürzt kt) angegeben. 24 Karat entsprechen purem Gold (Feingold). Mit Einführung des metrischen Systems wurde die Umstellung auf Promille-Angaben vorgenommen. So bedeutet der Stempeleindruck „750“ in Goldware, dass das Metall von 1000 Gewichtsanteilen 750 Anteile (d. h. ¾) reines Gold enthält, entsprechend 18 Karat („585“ entspricht 14 Karat, „375“ entspricht 9 Karat und „333“ entspricht 8 Karat). Bullionmünzen haben entweder 916,6 Promille (Krugerrand, Britannia, American Eagle) oder 999,9 Promille Gold (Wiener Philharmoniker, Maple Leaf, Nugget, American Buffalo). Die Reinheit kann mit einer Dezimalzahl angegeben werden, zum Beispiel als 0,999 oder 1,000 (Feingold).

Hochwertiger Schmuck wird international üblicherweise aus Goldlegierungen mit einem Feingehalt von 750 oder höher angefertigt. Dabei ist Wahl des verwendeten Feingehaltes von regionalen und kulturellen Vorlieben beeinflusst. So werden auf dem amerikanischen Kontinent vor allem Legierungen mit 585 ‰ Goldanteil verwendet, während im Nahen Osten sattgelber Goldschmuck ab Feingehalten von etwa 20 bis 22 kt (833–916 ‰) aufwärts besonders geschätzt wird. In Südostasien und im chinesisch, thailändisch und malaiisch beeinflussten Kulturkreis geht dies traditionell sogar bis hin zum Schmuck aus reinem Feingold, der in der dortigen Kultur als besonders hochwertig betrachtet wird.

Die Anteile an eventuell enthaltenen anderen Edelmetallen (Silber, Palladium, Platin, Rhodium, Iridium u. a.) wird bei der Stempelung nicht berücksichtigt.

Vor allem aufgrund des hohen Preises von Gold wurden Legierungen aus unedlen Metallen entwickelt, die als Goldimitat benutzt werden oder als Unterlage bei der Herstellung von Doublé Verwendung finden. Dies sind in den meisten Fällen ungenormte Kupferlegierungen mit den verschiedensten Zusätzen. Eine Legierung aus mindestens 50 % Kupfer und Zink als Hauptlegierungsanteil (bis über 44 %) ist als Messing bekannt. Die Zugabe von Blei (bis zu 3 %) erhöht die Zerspanbarkeit des Messings. Wichtige Messingsorten sind Tombak (über 67 % Kupfer) und Sondermessing (enthält weitere Metalle).

Aus Edelmetallen werden Legierungen hergestellt, die wie Gold erscheinen können, ohne dass Gold in ihnen enthalten ist. Bei manchen Legierungen wird jedoch Gold selbst in geringen Anteilen hinzugegeben.

Die Prüfung von Gold auf dessen Echtheit und Reinheit und somit die Wertbestimmung erfolgt prinzipiell durch drei verschiedene Methoden:

Klassische Goldlegierungen für Schmuck gehören dem Dreistoffsystem Gold-Silber-Kupfer an. Ein Grund dafür ist, dass diese Metalle natürlich miteinander vorkommen und es bis ins 19. Jahrhundert in Europa verboten war, Gold mit anderen Metallen als Kupfer und Silber zu legieren. Das Farbspektrum derartiger Goldlegierungen reicht von sattgelb über hellgrün und lachsrosa bis hin zu silberweiß. Diese Legierungen sind leicht herstellbar und gut zu verarbeiten. Je nach Anforderung werden durch Zusatz weiterer Metalle die Legierungseigenschaften wie erwünscht beeinflusst. So senken beispielsweise kleinere Zusätze von Zink, Indium, Zinn, Cadmium oder Gallium die Schmelztemperaturen und die Oberflächenspannung der Metallschmelze bei nur geringfügiger Änderung der Farbe der Legierung. Dies ist eine Eigenschaft, die der Verwendung als Lotlegierungen für andere Goldwerkstoffe entgegenkommt. Andere Zusätze wie Platin, Nickel oder höhere Kupferanteile erhöhen beträchtlich die Härte der Metallmischung, verändern aber andererseits die schöne Farbigkeit des Goldes negativ. Zusätze wie Blei ("bleihaltiges Lötzinn"), Bismut und viele Leichtmetalle machen Goldlegierungen spröde, sodass diese nicht mehr verformbar sind.

Doch nicht nur die Art, sondern auch die Menge der zugesetzten Metalle verändert die Goldlegierungen in gewünschter Weise. Ist eine satte Eigenfarbe erwünscht, sind bei sehr edlen Goldlegierungen mindestens drei Viertel Massenteile Gold erforderlich. Höchste Festigkeit und Härte werden andererseits bei den eher blasseren Goldlegierungen mit einem Feingehalt um 585 erreicht, weshalb dieses empirisch gefundene Legierungsverhältnis seit langem verwendet wurde. Legierungen mit einem deutlich geringeren Feingehalt als diese sind hingegen aufgrund der unedlen Beimischungen durch langfristige Korrosionseffekte bedroht.

Weiterhin ist zu unterscheiden, ob die Legierungen als Gussmaterial verarbeitet werden sollen oder wie herkömmlich als Knetlegierungen, also schmiedbar, zur Kaltverformung geeignet sein müssen. Erstere beinhalten Kornfeinungszusätze im Zehntelpromillebereich, die beim langsamen Erstarren der Schmelze in der Gussform das Kristallwachstum günstig beeinflussen, Zusätze von etwas Silicium unterdrücken die Oberflächenoxidation beim Erhitzen in der Luft, verschlechtern aber die Kaltbearbeitungsfähigkeit und Lötbarkeit.

Legieren bedeutet in diesem Zusammenhang letztendlich ein „Verdünnen“ des reinen Goldes und es werden seine geschätzten Eigenschaften wie Farbe, Korrosionsfestigkeit, Preis, Dichte „verdünnt“, doch gewinnt andererseits mechanische Festigkeit und Polierfähigkeit hinzu.

Die gebrauchsfreundlichen Eigenschaften, das „Edle“ der Goldlegierungen, wird durch das Verhältnis von Edelmetallatomen zur Gesamtanzahl der Atome in der Legierung bestimmt. Deren Eigenschaften wie Korrosionsfestigkeit, Farbwirkung oder intermetallische Bindung werden durch dieses Stückzahlenverhältnis festgelegt. Die Stoffmenge, das Mol und die Stöchiometrie weisen darauf. Der Gewichtsanteil bestimmt nur indirekt die Eigenschaften und ist darüber hinaus sehr von den verwendeten Zusatzmetallen abhängig.

Gold mit der Atommasse 197 und Kupferatome mit der Massenzahl 63 (nur rund ein Drittel) bilden eine Legierung mit dem Atomverhältnis 1:1. Dieses Legierungsbeispiel zeigt ein Gewichtsanteil von 756 Teilen Feingold und suggeriert über das Gewicht einen hohen Edelmetallgehalt. Genau betrachtet jedoch beträgt dieser über den Anteil der Goldatome (die Stückzahl) nur 50 %. Empirisch wird jedoch eine Legierung unter 50 Atomprozent Gold von Säuren angreifbar. Je kleiner die Atommassen der Legierungszusätze, desto drastischer fällt dieser Effekt aus.

So betrachtet sind bei den üblicherweise verwendeten 750er-Goldlegierungen bereits nur ca. die Hälfte der Legierungsatome Gold. Extremes Beispiel ist eine 333er-Goldlegierung, hier kommen nur 2 Goldatome auf 9 Zusatzatome. Dies erklärt die sehr unedlen Eigenschaften dieses Materials, wie hohe Anlaufneigung, Korrosionsverhalten und geringe Farbtiefe. Viele Goldschmiede und Länder, wie die Schweiz, lehnen es ab, diese Legierung noch als „Gold“ aufzufassen.

Die Zahlangabe 750/ooo – egal ob bei Weißgold, Rotgold oder Gelbgold – besagt immer automatisch aus, dass dieselbe Menge Feingold in 
der jeweiligen Legierung enthalten ist. Kupfer, Silber oder Palladium und andere Legierbestandteile wechseln jedoch – je nach Farbe der Goldlegierung – in ihrer mengenmäßigen Zusammensetzung.

Rotgold ist eine Goldlegierung, bestehend aus Feingold, Kupfer und gegebenenfalls etwas Silber, um die mechanische Verarbeitbarkeit zu verbessern. Der relativ hohe Kupferanteil, der deutlich über dem des Silbers liegt, ist für die namensgebende „rote“ Färbung und Härte des Materials verantwortlich. Der Farbton ist kupferähnlich.

Regional sind bestimmte Goldfarbtönungen beliebt; so werden im Osten und Süden Europas eher dunklere und farbstarke rötlichere Goldlegierungen verwendet. Umgangssprachlich wurde Rotgold in der DDR als "Russengold" bezeichnet; teilweise ist in Süddeutschland noch der Begriff "Türkengold" gebräuchlich. Russengold hat den ungebräuchlichen Feingehalt von 583 und ist daran sehr gut zu erkennen. Die Färbung ist etwas heller als beim Rotgold.

Dabei handelt es sich um eine dem Feingold ähnelnde gelbe Goldlegierung aus Feingold mit Silber und Kupfer. Das Verhältnis beeinflusst die Farbe. Mit abnehmendem Goldgehalt reduziert sich die Tiefe des Gelbtons sehr schnell. Üblicherweise ist das Verhältnis der dem Gold zugesetzten Metalle untereinander ca. 1:1; die Tönungen und Farbintensität können stufenlos und beliebig gewählt werden. Die Farbe reicht von hellgelb mit deutlichem Silberanteil bis zu gelborange mit dem umgekehrten Verhältnis zum Kupferzusatz. Gelbgold ist durch ihren hohen Erkennungswert weltweit mit Abstand die beliebteste Goldfarbe.

Grüngold ist eine grünlichgelbe Goldlegierung ohne Kupferzusatz. Die Farbe entsteht durch Annäherung an das Atomverhältnis Gold:Silber 1:1, was im optimalen Fall einem Goldanteil von 646 entspricht, bei dem der deutlichste Grünton auftritt. Da in diesem Falle der Silberanteil schon über 40 % beträgt, ist der Farbton relativ hell. Bis zu einem Drittel des Silbers lässt sich durch Cadmium ersetzen, was den Grünton intensiviert, die günstigen Anlaufeigenschaften und die Schmelztemperatur allerdings reduziert. Die Legierungen sind sehr weich und wenig farbstark. Grüngold wird selten verwendet, üblicherweise zur Darstellung von Laubblättern oder ähnlichem.

Weißgold als Sammelbegriff bezeichnet Goldlegierungen, die durch Beimischung deutlich entfärbender Zusatzmetalle eine weiß-blassgetönte Goldlegierung ergeben. Als Legierungszusätze werden hauptsächlich das Platinnebenmetall Palladium, (früher sehr häufig) Nickel oder bei niedrigen Goldgehalten Silber verwendet. Die Entfärbung des von Natur aus gelben Goldes tritt kontinuierlich ein und setzt eine gewisse Menge des entfärbenden Zusatzes voraus; der Rest, der dann noch bis zum berechneten Gesamtvolumen fehlt, wird oft aus Kupfer/Silber gestellt.

Diese fast farblosen Werkstoffe wurden in den Jahren 1912/13 als kostengünstiger und punzierfähiger Platinersatz für Schmuckzwecke in Pforzheim entwickelt und werbewirksam als "Weißgold" eingeführt. Im frankophonen Sprachraum sind diese Werkstoffe treffender als „or gris“, "Graugold" bekannt. Ziel war ein gut zu verarbeitendes, anlaufbeständiges Material, in dem farblose Brillanten hervorragend ihre Wirkung zeigen konnten. Bis dato war Silber nötig, das nachdunkelt, oder Platin und das etwas dunklere und leichtere Palladium angewiesen. Folgerichtig existieren vor dieser Zeit keine Schmuckstücke mit Weißgold.

Viele Metalle bilden mit Gold „weiße“ Legierungen, so Quecksilber oder Eisen (durch die Legierung mit dem Edelmetall Gold wird Eisen nicht rostfrei). Platin bildet mit Gold eine schwere, teurere und sehr gut aushärtbare Legierung. Die in präkolumbischer Zeit in Südamerika hergestellten Platinobjekte bestehen aus diesem weißlich-beige bis schmutzig-grau aussehenden Material.

"Nickelhaltiges Weißgold" (eine Gold-Kupfer-Nickel-Zink-Legierung mit variablen 10–13 % Nickelanteil) kann als durch den Nickelzusatz entfärbte Rotgoldlegierung aufgefasst werden; demzufolge ist es relativ hart und kann bis zur Federhärte gewalzt, gezogen oder geschmiedet werden. Die hohe Grundfestigkeit ermöglicht beispielsweise geringere Wandstärken bei gleicher Stabilität. Weitere Eigenschaften wie hervorragende Zerspanbarkeit und Polierbarkeit sind von großem Vorteil. Dazu kommen noch der niedrige Schmelzpunkt und günstigere Preis, der wiederum daraus resultiert, dass keine weiteren Edelmetalle im Zusatz enthalten sind und die Dichte geringer ist als beim palladiumlegierten Pendant. Für mechanisch beanspruchte Teile wie Broschierungen, Nadeln, Scharniere und Verbindungsteile wird dieses Material von den Schmuckherstellern und Juwelieren aufgrund der Festigkeit sehr geschätzt. Nickelweißgold ist die Basis von weißgoldenen Lotlegierungen. Da jedoch der Nickelanteil auf der Haut allergische Reaktionen hervorrufen kann, wird es mittlerweile in fast allen modernen Schmucklegierungen weitestgehend vermieden.

Die edlere Alternative ist "palladiumhaltiges Weißgold", eigentlich treffender als Graugold zu bezeichnen. Es ist vergleichsweise weich, wobei es unterschiedliche Rezepturen von harten bis weichen Legierungen gibt. Es handelt sich um Mehrstofflegierungen mit bis zu sechs Komponenten. Der Grundfarbton der palladiumbasierten Goldmischungen ist Allgemein dunkler, eben „grauer“ als der des nickelbasierten Weißgoldes. Der Palladiumzusatz mit ca. 13–16 % muss höher als beim Nickelweißgold gewählt werden, um die Gesamtmischung vergleichbar zu entfärben. Üblicherweise werden diese Weiß-/Graugoldlegierungen meistens nach der Bearbeitung sowieso rhodiniert. Daher ist es weniger wichtig, dass die Legierung so ganz farbrein weiß oder hellgrau erscheint, und es wird bewusst am Palladiumzusatz gespart, der sehr den Preis auftreibt und zudem die Mischung nachteilig dunkler färbt. Nativ sehen diese Werkstoffe demzufolge oft leicht beige aus. Der Vergleich mit Platin oder Silber ist augenfällig. Die Verarbeitungseigenschaften, wie Zerspanbarkeit, die bei maschinellem Drehen beispielsweise von Trauringen gefordert ist, stellen andere Anforderungen an die Werkzeuge. Die Gießeigenschaften (höherer Schmelzpunkt und höhere Oberflächenspannung der Schmelze) unterscheiden sich vom nickelbasierten Pendant. Eine strukturelle Zähigkeit der Legierungen erhöht den Aufwand der Hochglanzpolitur in ungewohnter Weise. Nachteilig ist der erhöhte Preis durch den nicht unbeträchtlichen Palladiumanteil und die höhere Dichte des Materials. Positiv zeigen die Legierungen ihren hohen Anteil an Edelmetallen (Gold-Palladium-Silber) in deren Eigenschaften. Ein Schmuckstück in Palladiumweißgold war im Januar 2007 ca. 20 % teurer als das vergleichbare aus Gelbgold bei gleichem Feingehalt.

Anbieter von Goldlegierungen entwickeln immer wieder neuartige Werkstoffe. So gibt es Weißgoldlegierungen mit Cobalt, Chrom, Mangan-Germanium und anderen Metallen. Verarbeitungsprobleme, Preisentwicklungen oder mangelnde Akzeptanz der Kunden lassen solche neuen Goldlegierungen häufig schnell wieder vom Markt verschwinden.

Da sich „weißes“ Gold nicht elektrochemisch abscheiden lässt, werden Schmuckerzeugnisse aus Weißgold in der Regel auf galvanischem Wege rhodiniert. Dieser Überzug mit Rhodium, einem Platin-Nebenmetall, bewirkt eine Farbverbesserung hin zu einem reinen, silberartigen Weiß sowie eine verbesserte Kratzfestigkeit gegenüber der unbeschichteten Metalloberfläche aus reinem Weißgold. Dieser Rhodiumüberzug muss nicht explizit angegeben werden. Durch Abtragen dieses Überzuges kommt das eigentliche Weiß- oder Graugold wieder zum Vorschein, was bei Trauringen oft zu optischen Beeinträchtigungen führt. In den letzten Jahren werden daher Weißgoldringe bewusst in ihrer Naturfarbe verkauft, um Enttäuschungen beim Verbraucher zu vermeiden.

Eine aushärtbare Titan-Gold-Legierung mit 99 % Gold und 1 % Titan wird in der Trauringherstellung und Medizintechnik eingesetzt. Der hohe Edelmetallanteil in Verbindung mit hoher Festigkeit machen den Werkstoff interessant. Die gelbe Farbe ist vergleichbar mit der von 750 Gelbgold, jedoch „grauer“. Durch den Titanzusatz ist die Legierung beim Schmelzen sehr empfindlich und reagiert mit Sauerstoff und Stickstoff.

Gold kommt in seinen Verbindungen hauptsächlich in den Oxidationsstufen +1 und +3 vor. Daneben ist −1-, +2- und +5-wertiges Gold bekannt. Goldverbindungen sind sehr instabil und zersetzen sich bei Erwärmung leicht unter Entstehung von elementarem Gold.

Goldverbindungen können aufgrund der Giftigkeit des Verbindungspartners zum Teil sehr giftig sein, etwa Tetrachloridogoldsäure und die Goldcyanide.

Gold und Goldverbindungen sind für Lebewesen nicht essentiell. Da Gold in Magensäure unlöslich ist, ist beim Verzehr (etwa als Dekoration) von reinem, metallischem Gold keine Vergiftung zu befürchten. Reichern sich hingegen Gold-Ionen, zum Beispiel bei übermäßiger Aufnahme von Goldsalzen, im Körper an, kann es zu Symptomen einer Schwermetallvergiftung kommen. Die meisten Pflanzenwurzeln werden durch Gabe (hoher Mengen) an Goldsalzen geschädigt.

Es gibt Menschen, die allergisch auf Goldlegierungen reagieren (Nachweisversuche mittels Natriumthioaurosulfat sind schwierig und unsicher). Diese Goldallergie ist allerdings extrem selten und noch nicht ausreichend untersucht. Bei der Verwendung von Goldfüllungen und anderem goldhaltigen Zahnersatz ist zu beachten, dass Goldlegierungen andere Bestandteile enthalten und eine allergisierende Wirkung meist von anderen Bestandteilen, beispielsweise Zink, ausgelöst werden kann.

Mit Gold, das für wertvoll und kostbar steht, werden andere wertvolle Sachen ebenfalls bezeichnet. Meist wird ein Adjektiv hinzugesetzt, wie „Schwarzes Gold“ für Öl. Wörter und Redewendungen, in denen "Gold" vorkommt, sind zudem in ihrer Bedeutung meist positiv oder euphemistisch besetzt.

Beispiele:

Es zu gibt diesen positiv besetzten Ausdrücken auch Gegenbeispiele, so sind "goldene Wasserhähne" nicht nur Zeichen von großem Luxus, sondern Sinnbild für Dekadenz. Als „Blutgold“ wurden illegal ausgeführte Goldmengen während des Zweiten Kongokriegs bezeichnet, mit denen die beteiligten Milizen ihre Waffenkäufe finanzierten (vgl. auch →"Blutdiamanten").

Schrottsammler bezeichnen Kupfer mit „Gold“, da sie unter den gängigen Metallen für Kupfer den höchsten Preis erzielen.

In der Heraldik ist die Wappenfarbe Gold durch Gelb ersetzbar (und Silber durch Weiß). Die Farben Gold und Silber werden in der Heraldik als „Metalle“ bezeichnet (siehe Tingierung).





</doc>
<doc id="1810" url="https://de.wikipedia.org/wiki?curid=1810" title="Gallium">
Gallium

Gallium ist ein selten vorkommendes chemisches Element mit dem Elementsymbol Ga und der Ordnungszahl 31. Im Periodensystem steht es in der 4. Periode und ist das dritte Element der 3. Hauptgruppe, 13. IUPAC-Gruppe, oder Borgruppe. Es ist ein silberweißes, leicht zu verflüssigendes Metall. Gallium kristallisiert nicht in einer der sonst häufig bei Metallen anzutreffenden Kristallstrukturen, sondern in seiner stabilsten Modifikation in einer orthorhombischen Struktur mit Gallium-Dimeren. Daneben sind noch sechs weitere Modifikationen bekannt, die sich bei speziellen Kristallisationsbedingungen oder unter hohem Druck bilden. In seinen chemischen Eigenschaften ähnelt das Metall stark dem Aluminium.

In der Natur kommt Gallium nur in geringem Umfang und meist als Beimischung in Aluminium-, Zink- oder Germaniumerzen vor; Galliumminerale sind sehr selten. Gallium wird auch als Nebenprodukt bei der Produktion von Aluminium oder Zink gewonnen. Der größte Teil des Galliums wird zum Halbleiter Galliumarsenid weiterverarbeitet, der vor allem für Leuchtdioden verwendet wird.

Erstmals wurde ein dem späteren Gallium entsprechendes Element 1871 von Dmitri Mendelejew vorausgesagt. Er prognostizierte mit Hilfe des von ihm entwickelten Periodensystems ein neues, "Eka-Aluminium" genanntes Element und sagte auch einige Eigenschaften dieses Elementes (Atommasse, spezifisches Gewicht, Schmelzpunktlage und Art der Salze) voraus.

Der französische Chemiker Paul Émile Lecoq de Boisbaudran, der Mendelejews Voraussagen nicht kannte, hatte herausgefunden, dass in der Linienabfolge im Linienspektrum von Elementfamilien bestimmte Gesetze herrschen, und versuchte, diese für die Aluminiumfamilie zu bestätigen. Dabei erkannte er, dass zwischen Aluminium und Indium ein weiteres, noch unbekanntes Element stehen müsse. 1875 gelang es ihm schließlich, im Emissionsspektrum von Zinkblende-Erz, das er in Säure gelöst und mit metallischem Zink versetzt hatte, zwei violette Spektrallinien nachzuweisen, die er dem unbekannten Element zuordnete.

Anschließend konnte Lecoq de Boisbaudran aus einigen hundert Kilogramm Zinkblende eine größere Menge Galliumhydroxid gewinnen. Daraus stellte er durch Lösen in einer Kaliumcarbonatlösung und Elektrolyse erstmals elementares Gallium dar.

Lecoq de Boisbaudran benannte das Element nach Gallien, der lateinischen Bezeichnung seines Heimatlandes Frankreich. 

Nachdem die Eigenschaften des neuen Elementes bestimmt waren, erkannte Mendelejew schnell, dass es sich dabei um das von ihm vorausberechnete Eka-Aluminium handeln müsse. Viele Eigenschaften stimmten sehr genau mit den vorausberechneten Werten überein. So wich der theoretisch ermittelte Wert der Dichte von 5,9 nur sehr wenig vom experimentellen von 5,904 ab.

Gallium ist auf der Erde ein seltenes Element. Mit einem Gehalt von 19 ppm in der kontinentalen Erdkruste ist seine Häufigkeit vergleichbar mit derjenigen von Lithium und Blei. Es kommt nicht elementar, sondern nur gebunden vor, vorwiegend in Aluminium-, Zink- oder Germaniumerzen. Zu den galliumreichsten Erzen zählen Bauxite, Zinkblende-Erze und Germanit.

Die Galliumgehalte sind meist gering; so enthält der in Surinam gefundene Bauxit mit dem höchsten bekannten Gehalt nur 0,008 % Gallium. Die weltweit in Bauxit befindlichen Galliumreserven werden auf 1,6 · 10 Tonnen geschätzt. Höhere Gehalte mit bis zu 1 % Gallium kommen in Germanit vor. Lediglich in der Apex-Mine im US-Bundesstaat Utah kommen so hohe Gehalte in den Erzen vor, dass ein Abbau zur Galliumgewinnung versucht wurde. Dies scheiterte jedoch nach kurzer Zeit aus Rentabilitätsgründen.

Nur wenige Galliumminerale sind bekannt; zu diesen zählen die vorwiegend in Tsumeb in Namibia gefundenen Gallit (CuGaS), Söhngeit (Ga(OH)) und Tsumgallit (GaO(OH)).

Gallium wird als Nebenprodukt bei der Aluminiumherstellung aus Bauxit im Bayer-Verfahren gewonnen. Als Ausgangsprodukt dient das dabei in Natronlauge gelöste Gemisch von Natriumaluminat und Natriumgallat. Durch verschiedene Verfahren kann hieraus Gallium vom Aluminium getrennt werden. Eine Möglichkeit ist die fraktionierte Kristallisation mit Hilfe von Kohlenstoffdioxid, wobei zunächst bevorzugt Aluminiumhydroxid ausfällt, während sich das leichter lösliche Natriumgallat in der Natronlauge anreichert. Erst nach weiteren Prozessschritten wird Galliumhydroxid gefällt, vermischt mit Aluminiumhydroxid. Anschließend wird das Gemisch in Natronlauge gelöst und Gallium durch Elektrolyse gewonnen. Da dieses Verfahren energie- und arbeitsaufwändig ist, wird es nur in Ländern mit geringen Kosten dafür, etwa der Volksrepublik China, angewendet.

Gallium kann auch direkt durch Elektrolyse aus der Natronlauge gewonnen werden. Dazu werden Quecksilberkathoden eingesetzt, wobei sich bei der Elektrolyse ein Galliumamalgam bildet. Auch das Versetzen der Lösung mit Natriumamalgam ist möglich.

Mit Hilfe spezieller Hydroxychinoline als Chelatliganden ist es möglich, Gallium aus der Natronlauge mit Kerosin zu extrahieren und so vom Aluminium zu trennen. Weitere Elemente, die dabei ebenfalls extrahiert werden, können mit verdünnten Säuren abgetrennt werden. Anschließend wird die verbliebene Galliumverbindung in konzentrierter Salz- oder Schwefelsäure gelöst und elektrolytisch zum Metall reduziert.

Für viele technische Anwendungen wird sehr reines Gallium benötigt; für Halbleiter beispielsweise darf es mitunter nur ein Hundertmillionstel an Fremdstoffen enthalten. Mögliche Reinigungsverfahren sind Vakuumdestillation, fraktionierte Kristallisation oder Zonenschmelzen.

Die Menge an produziertem Gallium ist gering. 2008 betrug die Weltprimärproduktion 95 Tonnen. Eine weitere wichtige Quelle ist das Wiederaufbereiten von galliumhaltigen Abfällen, daraus wurden 2008 weitere 135 Tonnen Gallium gewonnen. Hauptproduktionsländer sind die Volksrepublik China, Deutschland, Kasachstan und die Ukraine, für das Galliumrecycling auch die Vereinigten Staaten, Japan und das Vereinigte Königreich.

Im Labormaßstab lässt sich Gallium durch Elektrolyse einer Lösung von Galliumhydroxid in Natronlauge an Platin- oder Wolfram-Elektroden darstellen.

Gallium ist ein silberweißes, weiches (Mohs-Härte: 1,5) Metall. Es hat einen für Metalle ungewöhnlich niedrigen Schmelzpunkt, der bei 29,76 °C liegt. Es ist damit nach Quecksilber und Caesium das Metall mit dem niedrigsten Schmelzpunkt, der auch deutlich unter denjenigen der benachbarten Elemente Aluminium und Indium liegt. Verantwortlich hierfür ist wahrscheinlich die ungewöhnliche Kristallstruktur, die im Gegensatz zu den Strukturen anderer Metalle keine hohe Symmetrie aufweist und daher nicht sehr stabil ist. Da der Siedepunkt mit 2400 °C vergleichsweise hoch liegt, besitzt Gallium einen ungewöhnlich großen Bereich, in dem es flüssig ist. Auf Grund der schwierigen Kristallisation lässt sich flüssiges Gallium leicht unter den Schmelzpunkt abkühlen (Unterkühlung) und kristallisiert bei der Bildung von Kristallisationskeimen schlagartig.

Gallium besitzt wie Silicium, einige andere Elemente und Wasser eine Dichteanomalie; seine Dichte ist im flüssigen Zustand um etwa 3,2 % höher als in fester Form. Dies ist typisch für Stoffe, die im festen Zustand molekulare Bindungen besitzen.

Gallium ist im festen Zustand diamagnetisch, wird jedoch im flüssigen Zustand paramagnetisch ("Χ" = 2,4 · 10 bei 40 °C)

Charakteristisch für seine Strukturen ist das Ausbilden von Gallium-Gallium-Bindungen. Es sind verschiedene Modifikationen bekannt, die sich bei unterschiedlichen Kristallisationsbedingungen (vier bekannte Modifikationen, α- bis δ-Gallium, unter Normaldruck) und unter Druck bilden (insgesamt drei weitere Hochdruckmodifikationen, Ga-II, Ga-III, Ga-IV). Die bei Raumtemperatur stabilste Modifikation ist das α-Gallium, das in einer orthorhombischen Schichtstruktur kristallisiert. Dabei bilden jeweils zwei über eine kovalente Bindung aneinander gebundene Atome ein Dimer. Jedes Galliumatom grenzt zusätzlich an sechs weitere Atome anderer Dimere. Zwischen den einzelnen Dimeren herrschen metallische Bindungen. Die Galliumdimere sind so stabil, dass sie auch beim Schmelzen zunächst erhalten bleiben und auch in der Gasphase nachweisbar sind.

Weitere Modifikationen bilden sich bei der Kristallisation von unterkühltem, flüssigem Gallium. Bei −16,3 °C bildet sich β-Gallium, das eine monokline Kristallstruktur besitzt. In der Struktur liegen parallel angeordnete Zickzackketten aus Galliumatomen vor. Tritt die Kristallisation bei einer Temperatur von −19,4 °C ein, bildet sich trigonales δ-Gallium, in dem vergleichbar mit α-Bor verzerrte Ikosaeder aus zwölf Galliumatomen vorliegen. Diese sind über einzelne Galliumatome miteinander verbunden. Bei −35,6 °C entsteht schließlich γ-Gallium. In dieser orthorhombischen Modifikation bilden sich Röhren aus miteinander verbundenen Ga-Ringen aus, in deren Mitte eine lineare Kette aus weiteren Galliumatomen liegt.

Wird Gallium bei Raumtemperatur unter hohen Druck gesetzt, so bilden sich bei Druckerhöhung nacheinander verschiedene Hochdruckmodifikationen. Ab 30 kbar ist die kubische Gallium-II-Modifikation stabil, bei der jedes Atom von jeweils acht weiteren umgeben ist. Wird der Druck auf 140 kbar erhöht, kristallisiert das Metall nun als tetragonales Gallium-III in einer Struktur, die derjenigen des Indiums entspricht. Wird der Druck weiter auf etwa 1200 kbar erhöht, bildet sich schließlich die kubisch-flächenzentrierte Struktur des Gallium-IV.

Die chemischen Eigenschaften von Gallium ähneln denen des Aluminiums. Wie dieses ist Gallium durch die Bildung einer dichten Oxidschicht an der Luft passiviert und reagiert nicht. Erst in reinem Sauerstoff bei hohem Druck verbrennt das Metall mit heller Flamme unter Bildung des Oxides. Ähnlich reagiert es auch nicht mit Wasser, da sich hierbei das unlösliche Galliumhydroxid bildet. Ist dagegen Gallium mit Aluminium legiert und durch die Schmelzpunkterniedrigung bei Raumtemperatur flüssig, so reagiert es sehr heftig mit Wasser. Auch mit Halogenen reagiert Gallium schnell unter Bildung der entsprechenden Salze GaX.

Gallium ist amphoter und sowohl in Säuren als auch in Basen unter Wasserstoffentwicklung löslich. In Säuren bilden sich analog zu Aluminium Salze mit Ga-Ionen, in Basen Gallate der Form [Ga(OH)]. In verdünnten Säuren löst es sich dabei langsam, in Königswasser und konzentrierter Natronlauge schnell. Durch Salpetersäure wird Gallium passiviert.

Die meisten Metalle werden von flüssigem Gallium angegriffen, so dass es nur in Behältern aus Quarz, Glas, Graphit, Aluminiumoxid, Wolfram bis 800 °C und Tantal bis 450 °C aufbewahrt werden kann.

Es sind insgesamt 31 Galliumisotope zwischen Ga und Ga und weitere elfKernisomere bekannt. Von diesen sind zwei, Ga und Ga stabil und kommen auch in der Natur vor. In der natürlichen Isotopenzusammensetzung überwiegt Ga mit 60,12 %, 39,88 % sind Ga. Von den instabilen Isotopen besitzt Ga mit 3,26 Tagen die längste Halbwertszeit, die übrigen Halbwertszeiten reichen von Sekunden bis maximal 14,1 Stunden bei Ga.

Zwei Galliumisotope, Ga und das mit 67,71 Minuten Halbwertszeit kurzlebige Ga werden in der Nuklearmedizin als Tracer für die Positronen-Emissions-Tomographie genutzt. Ga wird dabei in einem Cyclotron erzeugt, während bei der Erzeugung von Ga kein Cyclotron nötig ist. Stattdessen wird das längerlebige Germaniumisotop Ge durch Bestrahlung von Ga mit Protonen erzeugt. Dieses zerfällt zu Ga, wobei das entstandene Ga in einem Gallium-68-Generator extrahiert werden kann. Für Untersuchungen wird das Gallium in der Regel in einem Komplex mit einem stark chelatisierenden Liganden wie 1,4,7,10-Tetraazacyclododecan-1,4,7,10-tetraessigsäure (DOTA) gebunden.

"→ Liste der Gallium-Isotope"

Auf Grund der Seltenheit des Elementes wird Gallium nur in geringem Umfang verwendet. Aus dem größten Teil des produzierten Galliums werden verschiedene Galliumverbindungen hergestellt. Die mit Abstand ökonomisch wichtigsten sind die mit Elementen der 5. Hauptgruppe, vor allem Galliumarsenid, das unter anderem für Solarzellen und Leuchtdioden benötigt wird. Im Jahre 2003 wurden 95 % des produzierten Galliums hierzu verarbeitet. Daneben dient es auch als Material zur Dotierung von Silicium (p-Dotierung).

Der große Temperaturbereich, in dem das Element flüssig ist, und der gleichzeitig niedrige Dampfdruck werden für den Bau von Thermometern (als Bestandteil von Galinstan) ausgenutzt. Galliumthermometer lassen sich bis zu Temperaturen von 1200 °C einsetzen. Flüssiges Gallium kann als Sperrflüssigkeit zur Volumenmessung von Gasen bei höheren Temperaturen sowie als flüssiges Elektrodenmaterial bei der Gewinnung von Reinstmetallen wie Indium verwendet werden.

Gallium besitzt eine hohe Benetzbarkeit und ein gutes Reflexionsvermögen und wird darum als Beschichtung für Spiegel eingesetzt. Zudem wird es in Schmelzlegierungen, für Wärmetauscher in Kernreaktoren und als Ersatz für Quecksilber in Lampen eingesetzt.

Legierungen von Gallium mit anderen Metallen haben verschiedene Einsatzgebiete. Magnetische Werkstoffe entstehen durch Legieren mit Gadolinium, Eisen, Yttrium, Lithium und Magnesium. Die Legierung mit Vanadium in der Zusammensetzung VGa ist ein Supraleiter mit der vergleichsweise hohen Sprungtemperatur von 16,8 K. In Kernwaffen wird es mit Plutonium legiert, um Phasenumwandlungen zu verhindern. Viele Galliumlegierungen wie Galinstan sind bei Raumtemperatur flüssig und können das giftige Quecksilber oder die sehr reaktiven Natrium-Kalium-Legierungen ersetzen.

Aufgrund seines niedrigen Schwellenwerts für den Neutrinoeinfang von nur 233,2 keV eignet sich Gallium als Detektormaterial zum Nachweis solarer Neutrinos (vgl. Sonnenneutrinoexperiment GALLEX).

Gallium lässt sich mit verschiedenen typischen Farbreaktionen qualitativ nachweisen. Dazu zählen die Reaktion mit Rhodamin B in Benzol, das bei Zusatz von Gallium orangegelb bis rotviolett fluoresziert, Morin, das wie bei der Reaktion mit Aluminium eine grüne Fluoreszenz zeigt, und Kaliumhexacyanidoferrat(III), mit dem Gallium einen weißen Niederschlag aus Galliumhexacyanidoferrat(III) bildet. Zudem ist ein spektroskopischer Nachweis über die charakteristischen violetten Spektrallinien bei 417,1 und 403,1 nm möglich.

Quantitative Nachweise können über komplexometrische Titrationen, beispielsweise mit Ethylendiamintetraessigsäure oder über die Atomabsorptionsspektrometrie, erfolgen.

Für Galliummetall existieren keine toxikologischen Daten; es wirkt jedoch reizend auf Haut, Augen und Atemwege. Die Verbindungen Gallium(III)-nitrat Ga(NO) und Gallium(III)-oxid GaO besitzen orale LD-Werte im Grammbereich: 4,360 g/kg für das Nitrat und 10 g/kg für das Oxid. Gallium wird daher als gering toxisch angesehen und spielt, soweit bekannt, als Spurenelement keine Rolle für den Menschen.

Gallium kommt in Verbindungen fast ausschließlich in der Oxidationsstufe +3 vor. Daneben sind seltene und meist sehr instabile Gallium(I)-Verbindungen bekannt sowie solche, die sowohl ein- als auch dreiwertiges Gallium enthalten (formal Gallium(II)-Verbindungen).

Die technisch wichtigsten Verbindungen des Galliums sind diejenigen mit den Elementen der Stickstoffgruppe. Galliumnitrid, Galliumphosphid, Galliumarsenid und Galliumantimonid sind typische Halbleiter (III-V-Halbleiter) und werden für Transistoren, Dioden und andere Bauteile der Elektronik verwendet. Insbesondere Leuchtdioden verschiedener Farben werden aus Gallium-Stickstoffgruppen-Verbindungen hergestellt. Die von der Bandlücke abhängige Farbe kann dabei durch das unterschiedliche Verhältnis der Anionen oder auch durch das Ersetzen von Gallium durch Aluminium oder Indium eingestellt werden. Galliumarsenid wird darüber hinaus für Solarzellen verwendet. Insbesondere bei Satelliten werden diese eingesetzt, da Galliumarsenid widerstandsfähiger gegen ionisierende Strahlung als Silicium ist.

Galliumhalogenide der Form GaX ähneln in vielen Eigenschaften den entsprechenden Aluminiumverbindungen. Mit Ausnahme des Gallium(III)-fluorides kommen sie als Dimer in einer Aluminiumbromidstruktur vor. Als einziges Halogenid hat Gallium(III)-chlorid eine geringe wirtschaftliche Bedeutung. Es wird als Lewis-Säure in Friedel-Crafts-Reaktionen eingesetzt.

Gallium(III)-oxid ist wie Aluminiumoxid ein farbloser, hochschmelzender Feststoff. Es kommt in fünf verschiedenen Modifikationen vor, von denen die kubische β-Modifikation am stabilsten ist.

Organische Galliumverbindungen existieren als "Gallane" GaR, "Gallylene" GaR und als "höhere Gallane", die Gallium-Gallium-Bindungen enthalten. Sie sind wie viele andere metallorganische Verbindungen instabil gegen Luft und Hydrolyse. Eine der wenigen galliumorganischen Verbindungen mit wirtschaftlicher Bedeutung ist Trimethylgallium, das als Dotierungsreagenz und für die Erzeugung dünner Schichten an Galliumarsenid und Galliumnitrid in der metallorganischen Gasphasenepitaxie eingesetzt wird.

Einen Überblick über Galliumverbindungen gibt die .




</doc>
<doc id="1811" url="https://de.wikipedia.org/wiki?curid=1811" title="Gadolinium">
Gadolinium

Gadolinium ist ein chemisches Element mit dem Elementsymbol Gd und der Ordnungszahl 64. Im Periodensystem steht es in der Gruppe der Lanthanoide und zählt damit auch zu den Metallen der Seltenen Erden.

Das Element wurde erstmals 1880 vom Schweizer Chemiker Jean Charles Galissard de Marignac entdeckt. Er untersuchte dabei die Bestandteile von Samarskit und ihre unterschiedliche Löslichkeit in Kaliumsulfat-Lösungen. Es bildeten sich je nach Löslichkeit mehrere Fraktionen. In einer der Fraktionen fand er im Absorptionsspektrum die Spektrallinien eines unbekannten Elements. Dieses nannte er, da er nicht ausreichend Material für eine exakte Bestimmung erhalten konnte, Y. Daneben fand er in einer weiteren Fraktion das ihm ebenfalls unbekannte Y, hierbei stellte sich jedoch schnell heraus, dass es sich um das schon von Marc Delafontaine und Paul Émile Lecoq de Boisbaudran gefundene Samarium handelte. Nachdem die Existenz von Y von William Crookes und Paul Émile Lecoq de Boisbaudran bestätigt werden konnte, nannte Lecoq de Boisbaudran am 19. April 1886 das neue Element in Absprache mit Marignac "Gadolinium", zu Ehren des finnischen Chemikers Johan Gadolin, mit dem Symbol Gd.

Metallisches Gadolinium wurde erstmals 1935 von Félix Trombe gewonnen. Er nutzte dafür die elektrolytische Reduktion einer Schmelze aus Gadolinium(III)-chlorid, Kaliumchlorid und Lithiumchlorid bei 625–675 °C an Cadmiumelektroden. Kurze Zeit später entdeckte er zusammen mit Georges Urbain und Pierre-Ernest Weiss den Ferromagnetismus des Elements.

Gadolinium ist auf der Erde ein seltenes Element, sein Anteil an der kontinentalen Erdkruste beträgt etwa 6,2 ppm.

Das Element kommt in vielen Mineralen der Seltenerdmetalle in unterschiedlichen Gehalten vor. Besonders hoch ist der Gadoliniumgehalt in Mineralen der Yttererden wie Xenotim. In Xenotimvorkommen aus Malaysia beträgt der Gadoliniumanteil etwa 4 %. Aber auch Monazit enthält je nach Lagerstätte 1,5 bis 2 % des Elements, in Bastnäsit ist der Anteil mit 0,15 bis 0,7 % dagegen geringer. Es ist nur ein einziges Mineral bekannt, in dem Gadolinium das Seltenerdmetall mit dem höchsten Anteil ist. Dabei handelt es sich um das sehr seltene Uranylcarbonat Lepersonnit-(Gd) mit der chemischen Zusammensetzung Ca(Gd,Dy)(UO)(SiO)(CO)(OH) · 48HO.

Aufgrund der nur geringen Mengen des in den Erzen enthaltenen Gadoliniums und der Ähnlichkeit mit den anderen Lanthanoiden ist dessen Separierung schwierig. Nach dem Aufschluss der Ausgangsmaterialien wie Monazit oder Bastnäsit mit Schwefelsäure oder Natronlauge sind verschiedene Wege zur Abtrennung möglich. Neben Ionenaustausch ist besonders ein auf der Flüssig-Flüssig-Extraktion basierendes Verfahren wichtig. Dabei wird bei Bastnäsit als Ausgangsmaterial zunächst das Cer in Form von Cer(IV)-oxid abgetrennt und die verbleibenden Seltenen Erden in Salzsäure gelöst. Daraufhin werden mit Hilfe einer Mischung von DEHPA (Di(2-ethylhexyl)phosphorsäure) und Kerosin in Flüssig-Flüssig-Extraktion Europium, Gadolinium, Samarium und die schwereren Seltenerdmetalle von den leichten getrennt. Ersteres lässt sich chemisch durch Reduktion zu zweiwertigem Europium und Fällung als schwerlösliches Europium(II)-sulfat abtrennen. Für die Trennung von Gadolinium, Samarium und dem Rest wird wiederum die Flüssig-Flüssig-Extraktion genutzt. Die Mischung wird in verdünnter Salzsäure gelöst, mit einer Mischung von DEHPA und Trimethylbenzolen (Shellsol A) behandelt und in einer Mixer-Settler-Apparatur getrennt.

Die Gewinnung elementaren Gadoliniums ist über die Reduktion von Gadolinium(III)-fluorid mit Calcium möglich.

Gadolinium wird nur in geringerem Umfang produziert und benötigt. Wichtigster Produzent ist, wie bei allen Seltenerdmetallen, die Volksrepublik China.

Das silbrigweiß bis grauweiß glänzende Metall der Seltenen Erden ist duktil und schmiedbar. Es kristallisiert in einer hexagonal-dichtesten Kristallstruktur mit den Gitterparametern a = 363 pm und c = 578 pm. Oberhalb von 1262 °C geht die Struktur in eine kubisch-raumzentrierte Struktur über.

Neben dieser Hochtemperaturphase sind mehrere Hochdruckphasen bekannt. Die Abfolge der Phasen entspricht dabei der der anderen Lanthanoide (außer Europium und Ytterbium). Auf die hexagonale Struktur folgt (jeweils bei Raumtemperatur) bei Drücken über 1,5 GPa eine Struktur vom Samarium-Typ, oberhalb von 6,5 GPa ist eine doppelt-hexagonale Kristallstruktur stabil. Eine kubisch-flächenzentrierte Packung ist bei Drücken zwischen 26 und 33 GPa am stabilsten. Bei noch größeren Drücken sind noch eine doppelt-kubisch-flächenzentrierte Struktur sowie das monokline Gd-VIII bekannt.

Gadolinium ist neben Dysprosium, Holmium, Erbium, Terbium und Thulium eines der Lanthanoide, das Ferromagnetismus besitzt. Mit einer Curie-Temperatur von 292,5 K (19,3 °C) besitzt es die höchste Curie-Temperatur aller Lanthanoide, nur Eisen, Cobalt und Nickel besitzen höhere. Oberhalb dieser Temperatur ist es paramagnetisch mit einer magnetischen Suszeptibilität χ von 0,12.

Aufgrund dieser magnetischen Eigenschaften hat Gadolinium auch eine sehr stark temperaturabhängige Wärmekapazität. Bei tiefen Temperaturen (unter 4 K) dominiert zunächst, wie bei Metallen üblich, die elektronische Wärmekapazität "C" (wobei "C" = γ·"T" mit γ = 6,38 mJ·mol·K und "T" der Temperatur). Für höhere Temperaturen ist die Debyesche Wärmekapazität (mit der Debye-Temperatur Θ = 163,4 K) ausschlaggebend. Unterhalb der Curie-Temperatur nimmt die Wärmekapazität dann stark zu, was auf das Spinsystem zurückzuführen ist. Sie erreicht 56 J·mol·K bei 290 K, um bei höheren Temperaturen fast schlagartig auf unter 31 J·mol·K einzubrechen.

Gadolinium ist Bestandteil keramischer Hochtemperatur-Supraleiter des Typs BaGdCuO mit einer Sprungtemperatur von 94,5 K.
Das reine Element ist nicht supraleitfähig.

Gadolinium hat mit 49.000 barn wegen seines enthaltenen Isotops Gd-157 (mit 254.000 barn) den höchsten Einfangquerschnitt für thermische Neutronen aller bekannten stabilen Elemente (nur das instabile Xe-135 erreicht etwa das zehnfache von Gd-157). Die hohe Abbrandrate ("burn-out-rate") schränkt eine Verwendung als Steuerstab in Kernreaktoren stark ein.

In trockener Luft ist Gadolinium relativ beständig, in feuchter Luft bildet es eine nichtschützende, lose anhaftende und abblätternde Oxidschicht aus. Mit Wasser reagiert es langsam. In verdünnten Säuren löst es sich auf. Stäube von metallischem Gadolinium sind feuer- und explosionsgefährlich.
Gadolinium wird zur Herstellung von Gadolinium-Yttrium-Granat für Mikrowellenanwendungen verwendet. Oxysulfide dienen zur Herstellung von grünem Leuchtstoff für nachleuchtende Bildschirme (Radar).

Intravenös injizierte Gadolinium(III)-Verbindungen, wie zum Beispiel Gadopentetat-Dimeglumin, dienen als Kontrastmittel bei Untersuchungen im Kernspintomographen. Dazu werden wegen der hohen Giftigkeit von freien Gadolinium-Ionen Komplexierungsmittel mit hoher Komplexierungskonstante, wie beispielsweise die Chelate DTPA (Diethylentriaminpentaessigsäure) und DOTA (1,4,7,10-Tetraazacyclododecan-1,4,7,10-tetraessigsäure, mit Gd = Gadotersäure), verwendet. Durch die sieben ungepaarten Elektronen in der f-Schale ist Gadolinium stark paramagnetisch. Das Kontrastmittel ermöglicht so den umgebenden Protonen – im Wesentlichen Wasser – schneller zu relaxieren. Dies erhöht die Kontrastunterschiede zwischen verschiedenen Geweben in einer MRT-Aufnahme erheblich. Aber Gadolinium kann sich nach Angaben der US-amerikanischen Arzneimittelbehörde FDA im Gehirn ablagern. Der Bundesverband Deutscher Nuklearmediziner (BDN) rät, die Mittel vorerst nur bei unvermeidbaren Untersuchungen einzusetzen. Gadolinium wird intravenös verabfolgt, um bspw. Tumoren und entzündliche Veränderungen im Gehirn darzustellen.
Bei Störungen der Blut-Liquor-Schranke kommt es zu einer Anreicherung im verdächtigen Bereich und liefert somit wichtige diagnostische Informationen.

Gadolinium-Gallium-Granat wurde zur Herstellung von Magnetblasenspeichern genutzt. Auch in der Herstellung von wiederbeschreibbaren Compact Discs findet es Anwendung.

Zusätze von 1 % Gadolinium erhöhen die Bearbeitbarkeit und die Hochtemperatur- und Oxidationsbeständigkeit von Eisen- und Chromlegierungen. Entsprechende Gadolinium-Eisen-Kobalt-Legierungen können zur optomagnetischen Datenspeicherung eingesetzt werden.

Gadolinium könnte, da es einen Curie-Punkt nahe der Zimmertemperatur besitzt, in Kühlgeräten, die nach dem Prinzip der adiabatischen Magnetisierung funktionieren, Verwendung finden. Solche Kühlgeräte würden ohne Ozonschicht-schädigende Fluorchlorkohlenwasserstoffe (FCKW) auskommen und besäßen keine verschleißenden mechanischen Teile. Umgekehrt lässt sich ein Motor durch Versorgung mit warmen und kaltem Wasser betreiben. Dadurch ließe sich z. B. die Restenergie aus warmen industriellen Abwässern gewinnen.

Gadolinium wird in Form von Gadoliniumoxid in modernen Brennelementen als abbrennbares Absorbermaterial verwendet, das nach einem Brennelementewechsel zu Beginn des Betriebszyklus die durch einen Überschuss an Kernbrennstoff entstehende zu hohe Reaktivität des Reaktors begrenzt. Mit zunehmendem Abbrand der Brennelemente wird auch das Gadolinium abgebaut.

Mit Terbium dotiertes Gadolinium-Oxysulfid (GdOS:Tb) ist ein in der Röntgentechnik häufig eingesetzter Szintillator. GdOS:Tb emittiert Licht mit einer Wellenlänge von 545 nm.



Es ist keine biologische Funktion des Gadoliniums bekannt.

Freie Gadolinium-Ionen verhalten sich ähnlich wie Calcium-Ionen, das heißt, sie werden vorwiegend in der Leber und im Knochensystem eingebaut und können dort über Jahre verbleiben. Freies Gadolinium beeinflusst außerdem als Calciumantagonist – die Ionenradien von Calcium und Gadolinium sind nahezu gleich – die Kontraktilität des Myokards und hemmt das Gerinnungssystem.

Intravenös applizierte Lösungen von freien Gadolinium-Ionen wirken akut toxisch. Von der Toxizität betroffen sind unter anderem die glatte und die quergestreifte Muskulatur, die Funktion der Mitochondrien und die Blutgerinnung.

Die Toxizität von freiem Gadolinium ist als hoch einzustufen. In "komplexierter" Form, so wie das Gadolinium in den zugelassenen Kontrastmitteln vorliegt, ist es dagegen unter Berücksichtigung der Kontraindikationen im Allgemeinen gut verträglich. Seit 2006 gibt es zunehmend Berichte, dass es bei niereninsuffizienten Patienten nach Gabe verschiedener Chelate des Gadoliniums, insbesondere Gd-DTPA, zum Krankheitsbild der nephrogenen systemischen Fibrose kommen kann. Eine neue Studie liefert Hinweise darauf, dass Gadolinium in Kontrastmitteln nach mehrmaligen MRTs zu Ablagerungen und eventuell auch Strukturschädigungen im Gehirn führen könnten.<ref name="DOI10.1148/radiol.13131669">Tomonori Kanda, Kazunari Ishii, Hiroki Kawaguchi, Kazuhiro Kitajima, Daisuke Takenaka: "High Signal Intensity in the Dentate Nucleus and Globus Pallidus on Unenhanced T1-weighted MR Images: Relationship with Increasing Cumulative Dose of a Gadolinium-based Contrast Material." In: "Radiology." 2013, S. 131669, .</ref> Ob es wirklich zu einer Schädigung kommt, konnte jedoch noch nicht festgestellt werden.



</doc>
<doc id="1812" url="https://de.wikipedia.org/wiki?curid=1812" title="Germanium">
Germanium

Germanium (von ‚Deutschland‘, nach dem Land, in dem es zuerst gefunden wurde) ist ein chemisches Element mit dem Elementsymbol Ge und der Ordnungszahl 32. Im Periodensystem steht es in der 4. Periode und in der 4. Hauptgruppe (14. IUPAC-Gruppe, p-Block und Kohlenstoffgruppe). Es wurde am 6. Februar 1886 erstmals im Mineral Argyrodit nachgewiesen.<ref name="DOI10.1002/ciuz.200490078"></ref>

Als 1871 Dmitri Mendelejew das Periodensystem entwarf, stieß er auf eine Lücke unterhalb des Siliciums und postulierte ein bis dahin unbekanntes Element, das er als Eka-Silicium bezeichnete. Mendelejew machte Vorhersagen über die Eigenschaften des Eka-Siliciums und dessen Verbindungen, die von der Wissenschaft jedoch abgelehnt wurden.

1885 fand Clemens Winkler (1838–1904), ein Chemiker an der Bergakademie Freiberg, als er mit dem neu entdeckten Mineral Argyrodit arbeitete, dass dessen quantitative Analyse stets ein Fehlbetrag von rund sieben Prozent lieferte. So oft die Analyse wiederholt wurde, blieb der Fehlbetrag etwa konstant und Winkler vermutete schließlich, dass das Mineral ein bis dahin unbekanntes Element enthielt. Nach vier Monaten Arbeit gelang ihm schließlich am 6. Februar 1886 die Isolierung eines weißen Sulfid-Niederschlages, der sich im Wasserstoffstrom zu einem metallischen Pulver reduzieren ließ. In Anlehnung an den zuvor entdeckten Planeten Neptun wollte Winkler das neue Element zunächst Neptunium nennen. Da dieser Name jedoch bereits für ein anderes vermutetes Element verwendet worden war, nannte er es nach dem Entdeckungsort Germanium.<ref name="DOI10.1002/cber.18860190156"></ref><ref name="DOI10.1002/prac.18860340122"></ref> Winkler vermutete zunächst, dass es sich beim Germanium um das von Mendelejew postulierte Eka-Stibium handelte, während Mendelejew es zunächst eher als Eka-Cadmium und nicht als Eka-Silicium einstufen wollte. Nachdem weitere Eigenschaften ermittelt worden waren, bestätigte sich, dass es sich wohl um das vorhergesagte Element Eka-Silicium handelte. Mendelejew hatte dessen Eigenschaften aus seinem Periodensystem abgeleitet, so dass dieser Fund zur Anerkennung des Periodensystems beitrug:

Die Herkunft und Etymologie des Namens Germanium könnte auch aus einem semantischen Missverständnis in Zusammenhang mit seinem Vorgängerelement Gallium herrühren, denn für die Namensgebung von Gallium gibt es zwei Theorien. Nach der Ersten benannte der französische Chemiker Paul Émile Lecoq de Boisbaudran das Element nach Gallien, der lateinischen Bezeichnung seines Heimatlandes Frankreich. Die Zweite gibt das ebenfalls lateinische Wort ' (Hahn) als Quelle des Namens an, das im Französischen ' („der Hahn“) heißt. Paul Émile "Lecoq" de Boisbaudran hätte das neue Element demnach nach seinem eigenen Namen benannt. Winkler nahm an, dass das vorherige Element Gallium nach der Staatsangehörigkeit des französischen Entdeckers benannt wurde. So nannte er das neue chemische Element „Germanium“ zu Ehren seines Landes (lateinisch "" für Deutschland).

Germanium ist weit verbreitet, kommt aber nur in sehr geringen Konzentrationen vor; Clarke-Wert (= Durchschnittsgehalt in der Erdkruste): 1,5 g/t. Es wird als Begleiter in Kupfer- und Zinkerzen gefunden (Mansfelder Kupferschiefer). Die wichtigsten Minerale sind Argyrodit, Canfieldit, Germanit und Renierit. Einige Pflanzen reichern Germanium an. Diese Eigenschaft führt zu einigen sehr umstrittenen Thesen bezüglich der Physiologie von Pflanzen („pflanzlicher Abwehr-Stoff gegen Viren“), die letztlich auch zu Anwendungen in der Homöopathie führen.

Laut USGS lag die Jahresproduktion 2014 bei geschätzten 165 t, davon 120 t in China. Der Preis für 1 kg Germanium betrug 2014 ca. 1.900 USD. Laut EU lag der Preis 2003 bei 300 USD je kg und stieg bis 2009 auf 1.000 USD.

Germanium steht im Periodensystem in der Serie der Halbmetalle, wird aber nach neuerer Definition als Halbleiter klassifiziert. Elementares Germanium ist sehr spröde und an der Luft bei Raumtemperatur sehr beständig. Erst bei starkem Glühen in einer Sauerstoff-Atmosphäre wird es zu Germanium(IV)-oxid (GeO) oxidiert. In Pulverform ist es ein entzündbarer Feststoff und kann durch kurzzeitige Einwirkung einer Zündquelle leicht entzündet werden und brennt nach deren Entfernung weiter. Die Entzündungsgefahr ist umso größer, je feiner der Stoff verteilt ist. In kompakter Form ist es nicht brennbar. Germanium ist zwei- und vierwertig. Germanium(IV)-Verbindungen sind am beständigsten. Von Salzsäure, Kalilauge und verdünnter Schwefelsäure wird Germanium nicht angegriffen. In alkalischen Wasserstoffperoxid-Lösungen, konzentrierter heißer Schwefelsäure und konzentrierter Salpetersäure wird es dagegen unter Bildung von Germaniumdioxidhydrat aufgelöst. Gemäß seiner Stellung im Periodensystem steht es in seinen chemischen Eigenschaften zwischen Silicium und Zinn.

Germanium weist als einer von wenigen Stoffen die Eigenschaft der Dichteanomalie auf. Seine Dichte ist in festem Zustand niedriger als in flüssigem. Seine Bandlücke beträgt bei Zimmertemperatur ca. 0,67 eV.

Wafer aus Germanium sind erheblich zerbrechlicher als Wafer aus Silicium.

Als Halbleiter war es das führende Material in der Elektronik Mitte des 20. Jahrhunderts, vor allem zur Herstellung der ersten am Markt erhältlichen Germaniumdioden und Bipolartransistoren, bis es vom Silicium in diesem Bereichen verdrängt wurde. Anwendungen finden sich heute in der Hochfrequenztechnik (z. B. als Siliciumgermanium-Verbindungshalbleiter) und Detektortechnologie (z. B. als Röntgendetektor). Für Solarzellen aus Galliumarsenid (GaAs) werden zum Teil Wafer aus Germanium als Trägermaterial verwendet. Die Gitterkonstante von Germanium ist der von Galliumarsenid sehr ähnlich, so dass GaAs epitaktisch auf Germanium-Einkristallen aufwächst.

Seine zweite Hauptanwendung findet es in der Infrarotoptik in Form von Fenstern und Linsen-Systemen aus poly- oder monokristallinem Germanium sowie optischen Gläsern mit Infrarotdurchlässigkeit, so genannten Chalkogenidgläsern. Einsatzgebiete hierfür sind unter anderem militärische und zivile Nachtsichtgeräte sowie Wärmebildkameras.

Weitere wesentliche Verwendungen liegen in der Herstellung von Lichtwellenleitern und Polyesterfasern: In modernen Glasfasern für die Telekommunikation wird mit Hilfe von Germaniumtetrachlorid bei der chemischen Gasphasenabscheidung eine Anreicherung von Germaniumdioxid im inneren Faserkern erzeugt. Dadurch entsteht im Vergleich zum Fasermantel ein höherer Brechungsindex im Kern, wodurch die Führung der Lichtwellen gewährleistet wird. In der Polyesterchemie kommt Germaniumdioxid als Katalysator bei der Herstellung von bestimmten Polyesterfasern und -granulaten zum Einsatz, speziell für recyclingfähige PET-Flaschen (PET = Polyethylenterephthalat).

Ge wird beim Gallium-68-Generator als Mutternuklid zur Herstellung von Gallium-68 verwendet. Ebenso findet Ge als Quelle zur Detektorkalibration bei der Positronen-Emissions-Tomographie Anwendung.

Als hochreiner Einkristall wird Germanium als Strahlendetektor eingesetzt.

Die Substanz Bis(carboxyethyl)germaniumsesquioxid ("Ge-132") ist als Nahrungsergänzungsmittel zur Anwendung bei einer Reihe von Erkrankungen einschließlich Krebs, chronischem Müdigkeitssyndrom, Immunschwäche, AIDS, Bluthochdruck, Arthritis und Lebensmittelallergien angepriesen worden. Positive Wirkungen auf den Krankheitsverlauf wurden bisher wissenschaftlich nicht nachgewiesen.

Gemäß der europäischen "Richtlinie 2002/46/EG zur Angleichung der Rechtsvorschriften der Mitgliedstaaten über Nahrungsergänzungsmittel" soll Germanium nicht in Nahrungsergänzungsmitteln verwendet werden. In vielen Ländern der EU, die ihre nationalen Rechtsvorschriften bereits angeglichen haben, so auch Deutschland und Österreich, ist daher der Zusatz von Germanium als Mineralstoffquelle in Nahrungsergänzungsmitteln nicht erlaubt.

Die zuständigen Behörden warnen ausdrücklich vor dem Verzehr von "Ge-132", da schwere Gesundheitsschäden und Todesfälle nicht auszuschließen sind.

Eine therapeutische Wirksamkeit der antineoplastischen Substanz Spirogermanium bei Krebserkrankungen wurde nicht nachgewiesen. Zugelassene Fertigarzneimittel mit dem Wirkstoff Spirogermanium gibt es nicht. In Deutschland gelten germaniumhaltige Arzneimittelanfertigungen (Rezepturen), abgesehen von homöopathischen Verdünnungen ab D4, als bedenklich. Ihre Herstellung und ihre Abgabe sind daher verboten. "Germanium metallicum" gibt es in Form homöopathischer Arzneimittel. Als Bestandteil homöopathischer Zubereitungen wird "di"-Kalium-Germanium-citrat-lactat beschrieben.

Germanium und seine Verbindungen weisen eine relative geringe Toxizität auf. Spuren von Germanium sind in den folgenden Nahrungsmitteln enthalten: Bohnen, Tomatensaft, Austern, Thunfisch und Knoblauch. Es ist nach dem Stand der Wissenschaft kein essentielles Spurenelement. Es ist keine biologische Funktion für Germanium bekannt. Ein möglicher Einfluss auf den Kohlenhydrat-Metabolismus wurde diskutiert. Es sind keine Germanium-Mangelerkrankungen bekannt.

Vergiftungen mit Germanium bei Menschen traten bisher nur nach der Einnahme von anorganischen Germaniumverbindungen als Nahrungsergänzungsmittel auf. Erste Symptome sind dabei Appetitlosigkeit, Gewichtsverlust, Erschöpfungszustände und Muskelschwäche. Darauf folgen Funktionsstörungen der Niere, bis hin zum Nierenversagen, das für den Patienten letal sein kann. Periphere Neuropathie als Folgeerkrankung sind ebenfalls berichtet.

Vorübergehende neurotoxische Nebenwirkungen bei der Einnahme von Spirogermanium in klinischen Studien werden berichtet. Spirogermanium wurde in den 1980er Jahren als Cytostatikum getestet. Daten aus Studien an gesunden freiwilligen Probanden sind nicht verfügbar.

Aus Tierversuchen weiß man, dass Germanium eine geringe akute orale Toxizität hat. Die Symptome einer akuten Vergiftung mit großen Dosen von Germaniumverbindungen beinhalten:
Letztlich führt Atemlähmung zum Tod der Versuchstiere. Symptome einer chronischen bzw. subchronischen Vergiftung mit anorganischen Germaniumverbindungen sind:
Organische Germaniumverbindungen zeigten eine geringere Giftigkeit, führten jedoch bei den Versuchstieren zu Gewichtsverlust und einer Abnahme der Anzahl der roten Blutkörperchen. Über die fruchtschädigende Wirkung von Germanium liegen nur wenige Daten vor. Natriumgermanat wurde in Ratten als nicht krebserregend getestet.

Der Mechanismus der Toxizität von Germanium ist noch nicht vollständig geklärt. Spezifische pathologische Effekte an den Mitochondrien von Nieren- und Nervenzellen wurden jedoch beobachtet.

Es wird ebenfalls diskutiert, ob Germanium evtl. Wechselwirkungen mit Silicium im Knochen-Metabolismus zeigt. Es kann die Wirkung von Diuretika blockieren und die Aktivität einer Reihe von Enzymen herabsetzen bzw. blockieren, wie beispielsweise Dehydrogenasen. Im Tierversuch zeigen Mäuse eine erhöhte Hexabarbital-induzierte Schlafdauer, wenn sie zusätzlich mit Germaniumverbindungen behandelt wurden. Dies lässt darauf schließen, dass die Cytochrom-P450-Aktivität ebenfalls eingeschränkt wird. Es gibt Berichte über organische Germaniumverbindungen, welche das Entgiftungsenzym Glutathion-S-Transferase blockieren.

Germanium wird bei oraler Aufnahme sehr leicht vom Körper aufgenommen. Es verteilt sich dabei über das gesamte Körpergewebe, vornehmlich in den Nieren und der Schilddrüse. Organogermane akkumulieren dabei im Gegensatz zu anorganischen Germaniumverbindungen nicht im menschlichen Körper. Allerdings gibt es nur wenige Studien über den Germanium-Metabolismus.

Es wird im Wesentlichen über den Urin ausgeschieden. Ausscheidung über Galle und Fäzes findet ebenso statt.

Germanium bildet Ge(II)- u. beständigere Ge(IV)-Verbindungen, nur wenige besitzen technische Bedeutung.

Von den Germaniumhalogeniden sind ebenfalls Ge(II)- u. Ge(IV)-Vertreter bekannt. Germaniumtetrachlorid, (GeCl), eine Flüssigkeit mit einem Siedepunkt von 83 °C, bildet sich bei Einwirkung von Chlorwasserstoff auf Germaniumoxide und ist ein wichtiges Zwischenprodukt bei der Germanium-Gewinnung. Hochreines GeCl wird bei der Herstellung von Lichtwellenleitern aus Quarzglas eingesetzt um auf der Innenseite der Quarzfasern eine hochreine Germanium(IV)-oxid Schicht zu erzeugen. Zur Erzeugung von hochreinen Germaniumschichten kann auch die Disproportionierung von Germanium(II)-iodid unter Bildung von Germanium und Germanium(IV)-iodid eingesetzt werden:

Germanate sind Verbindungen des Germaniums, die sich von dessen Oxid ableiten. In fast allen Germanium-haltigen Mineralien liegt das Germanium als Germanat vor.

Germane werden die Wasserstoffverbindungen des Germaniums genannt, die eine homologe Reihe verschieden langer Kettenmoleküle bilden. Monogerman oder Germaniumhydrid (GeH) ist ein Gas und wird in der Halbleiterindustrie zur Epitaxie und zum Dotieren verwendet.




</doc>
<doc id="1813" url="https://de.wikipedia.org/wiki?curid=1813" title="Gramm">
Gramm

Ein Gramm ist eine physikalische Maßeinheit für die Masse, das Einheitenzeichen ist g. Ein Gramm beträgt ein Tausendstel eines Kilogramms (kg), der offiziellen SI-Basiseinheit für die Masse. In dieser Art, als 0,001 Kilogramm, ist das Gramm in nationalen Einheitengesetzen und in Normen definiert.

Der Name stammt von (), der Bezeichnung für ein kleines Gewicht (1/24 Unze, entsprechend dem Skrupel, ca. 1,25 g).

Unter Verwendung von Vorsätzen für Maßeinheiten kann man schreiben:
10 Mt = 10 Tg = 10 kt = 10 t = 10 kg = 1 g = 10 mg = 10 µg = 10 ng = 10 pg

Ausgeschrieben:

Im englischen Sprachraum verbreitete Einheiten:

1 pound = 16 ounces = 453,59238 g 

"oder"
2,2046 pound = 1000 g

Speziell in der Chemie und der Atomphysik wird auch die atomare Masseneinheit formula_1 (auch als "Dalton", Da, bezeichnet) verwendet. Sie ist von der Masse eines Protons abgeleitet, aber leicht abweichend definiert.

Eine erste Definition von Gramm und Kilogramm stammen aus der Zeit des revolutionären Frankreichs und beziehen sich auf Wasser, das in metrischen Volumeneinheiten zu messen ist (ein Liter für das Kilogramm). Präzise formuliert: Ein Gramm destilliertes Wasser nimmt bei einer Temperatur von 3,98 °C und einem Luftdruck von 101,325 kPa das Volumen eines Kubikzentimeters bzw. eines Milliliters ein. Seit 1889 ist der tausendste Teil des Urkilogramms, welches in Paris aufbewahrt wird, als ein Gramm festgelegt. 

Abweichend von den sonstigen Regeln im SI bildet man dezimale Teile und Vielfache von Masseeinheiten nicht, indem man von der Basiseinheit Kilogramm ausgeht, sondern durch Herleitung vom Gramm. Dabei hat das Megagramm den besonderen Namen Tonne (t).

Das Dekagramm hat das SI-konforme Einheitenzeichen „dag“. In Österreich ist das Dekagramm (= 10 Gramm) vor Entstehung des SI durch Gesetz vom 5. Juli 1950 als gesetzliches Maß für die Masse mit dem Zeichen „dkg“ eingeführt worden. 1973 wurde das Zeichen entsprechend den Regeln im SI zu „dag“ geändert. Das ursprüngliche Zeichen blieb bis zum 31. Dezember 1977 übergangsweise zulässig. In Österreich (und auch den anderen Nachfolgestaaten Österreich-Ungarns) wird vor allem beim Einkauf von Lebensmitteln und in Rezepten die Maßeinheit Dekagramm häufig verwendet und dabei umgangssprachlich oft zu „Deka“ verkürzt. In gewisser Weise setzt es das metrische Lot fort.

In Italien ist das Hektogramm ("ettogrammo," umgangssprachlich zu "etto," Plural "etti" verkürzt, = 100 Gramm) eine beim Einkaufen von Lebensmitteln übliche Maßeinheit.



</doc>
<doc id="1814" url="https://de.wikipedia.org/wiki?curid=1814" title="Grad Celsius">
Grad Celsius

Das Grad Celsius ist eine Maßeinheit der Temperatur, welche nach Anders Celsius benannt wurde.

Die Celsius-Temperatur ist zur thermodynamischen Temperatur äquivalent:
formula_1

Für die Zahlenwerte einer Celsius-Temperatur und der entsprechenden thermodynamischen Temperatur gilt folgende Beziehung:
formula_2

Die geschweiften Klammern bezeichnen dabei nur die Zahlenwerte, und zwar bei Verwendung der Einheiten Grad Celsius oder Kelvin.

Nach Regeln der Organe der internationalen Meterkonvention darf das Grad Celsius auch zusammen mit SI-Vorsätzen benutzt werden, nach deutschem Einheitenrecht jedoch nicht. Diese Regelung wurde nicht in die nationale deutsche Normung des Deutschen Instituts für Normung (DIN 1301-1, DIN 1345) übernommen.

Durch die Neudefinition der Celsius-Skala über die Kelvin-Skala liegen der Schmelz- und der Siedepunkt von Wasser nicht mehr genau bei 0 °C und 100 °C, sondern bei 0,002519 °C und 99,9839 °C (99,9743 °C nach ITS-90).

Die Celsius-Skala geht auf den schwedischen Astronomen Anders Celsius zurück, der 1742 eine hundertteilige Temperaturskala vorstellte. Als Fixpunkte nutzte er, wie die 1730 vorgestellte Réaumur-Skala, die Temperaturen von Gefrier- und Siedepunkt des Wassers bei Normaldruck, das heißt einem Luftdruck von 1013,25 Hektopascal oder 760 Millimeter Quecksilbersäule. Der Bereich zwischen diesen Fixpunkten, gemessen mit einem Quecksilberthermometer, ist in 100 gleich lange Abschnitte eingeteilt, die als Grad bezeichnet sind. Dies führte zu der historischen Bezeichnung des „hundertteiligen Thermometers“. Anders als bei der modernen Celsius-Skala ordnete Celsius jedoch dem Siedepunkt von Wasser den Wert 0° und dem Gefrierpunkt den Wert 100° zu. Somit nahm der Temperaturwert eines Körpers beim Erwärmen ab.

Die moderne Celsius-Skala, bei der dem Siedepunkt von Wasser der Wert 100° und dem Gefrierpunkt der Wert 0° zugeordnet wird, wurde durch Carl von Linné, einen Freund Celsius’, kurz nach dessen Tod im Jahr 1744 eingeführt.

1948, ca. 200 Jahre nach der Einführung der Skala, wurde zu Ehren Celsius’ der Skalenabstand bei einem Celsius-Thermometer von einem Zentigrad bzw. Zentesimalgrad durch die "9. internationale Generalkonferenz für Maß und Gewicht" offiziell in die Temperatureinheit Grad Celsius umbenannt.

Eine andere Definition liegt der Norm DIN 1345 (Ausgabe Dezember 1993) des DIN zu Grunde. Es wird eine besondere Größenbenennung „Celsius-Temperatur“ eingeführt. Dies ist die Differenz der jeweiligen thermodynamischen Temperatur und der festen Bezugstemperatur 273,15 Kelvin (Einheitenzeichen: K). Weil diese Norm für Temperaturdifferenzen die Verwendung des Kelvin empfiehlt, legt sie weiterhin fest: 

Das Symbol für die Maßeinheit ist eine Kombination aus dem Gradzeichen und dem Großbuchstaben „C“. Diese sind als Einheit zu betrachten und dürfen nicht getrennt werden. Der Zahlenwert steht davor, wie bei Maßeinheiten üblich getrennt durch ein Leerzeichen. Aus Gründen der Kompatibilität enthält der Unicode-Standard zusätzlich die Darstellung durch "ein" Zeichen ℃ (codice_1), das Unicode-Konsortium rät aber von der Verwendung ab.

Als Formelzeichen für die Celsius-Temperatur ist nach SI das kleine "t" normgerecht, alternativ ist auch das formula_3 (zur Unterscheidung auch formula_4) ("theta") üblich. Die Verwendung des großen "T" ist falsch, da "T" der absoluten Temperatur in "Kelvin" vorbehalten ist.

Die Temperaturdifferenz formula_5 ist der Unterschied in der Temperatur von zwei Messpunkten formula_6, die sich in der Zeit oder der räumlichen Position unterscheiden.

Als Einheit für Temperaturdifferenzen wird vom DIN in Anpassung an das Internationale Einheitensystem (SI) mit der Norm DIN 1345 (Ausgabe Dezember 1993) das Kelvin empfohlen. Die DIN ergänzt dazu: Im Sinne dieser Norm stellt die „Celsius-Temperatur“ die Differenz der jeweiligen thermodynamischen Temperatur und der festen Bezugstemperatur 273,15 K dar; bei Angabe der Celsius-Temperatur wird der Einheitenname Grad Celsius als besonderer Name für das Kelvin benutzt (denn für Temperaturdifferenzen empfiehlt die Norm ja an sich das Kelvin).

Die Zahlenwerte von Temperaturdifferenzen stimmen bei der Verwendung der Einheiten Kelvin und Grad Celsius überein:
<math>\left\{ \Delta t \right\}_\mathrm


</doc>
<doc id="1815" url="https://de.wikipedia.org/wiki?curid=1815" title="Gauß (Einheit)">
Gauß (Einheit)

</math>
Gauß (in der Schweiz oder unter englischsprachigem Einfluss auch Gauss; Einheitenzeichen: Gs, G; nach Carl Friedrich Gauß) ist die Einheit der magnetischen Flussdichte "B" im elektromagnetischen CGS-System und im Gaußschen CGS-System:

In Deutschland ist das Gauß seit 1970 "keine" gesetzliche Einheit im Messwesen, wird aber vor allem in der Astrophysik weiterhin verwendet. Stattdessen ist die gesetzliche Einheit der magnetischen Flussdichte in der EU und in der Schweiz das Tesla, die entsprechende Einheit im Internationalen Einheitensystem (SI):

Parallel zum Gauß existierte für die magnetische Flussdichte die Einheit Gamma:

Das Gauß wird oft mit der Gaußschen CGS-Einheit der magnetischen Feldstärke H verwechselt, dem Oersted, das sich "im Gaußschen Einheitensystem und in elektromagnetischen CGS-Systemen" ebenfalls darstellen lässt als formula_6. Der Grund für diese formale Gleichheit ist, dass magnetische Flussdichte und magnetische Feldstärke "in o.g. Einheitensystemen" von gleicher Dimension sind (anders als im SI, wo sich die beiden Größen stets um die magnetische Permeabilität formula_7 bzw. um die magnetische Feldkonstante formula_8 als Proportionalitätsfaktor unterscheiden).

„Gauß“ als Name der elektromagnetischen CGS-Einheit für die "magnetische Feldstärke" wurde im Jahre 1900 auf dem 5. Internationalen Elektrizitätskongress in Paris festgelegt. Infolge eines Missverständnisses nahmen die amerikanischen Delegierten jedoch an, „Gauß“ wäre als Name für die elektromagnetische CGS-Einheit der "magnetischen Flussdichte" vereinbart worden. Diese Mehrdeutigkeit wurde 1930 auf der IEC-Tagung in Stockholm und Oslo zugunsten der amerikanischen Auffassung bereinigt.

1933 legte das IEC auf einer Sitzung in Paris fest, dass 1 cmgs als elektromagnetische CGS-Einheit der magnetischen Feldstärke Oersted heißen soll.

Das Einheitenzeichen „Gs“ für das Gauß wurde vom IEC 1935 auf einer Konferenz in Scheveningen festgelegt.


</doc>
<doc id="1817" url="https://de.wikipedia.org/wiki?curid=1817" title="Grad Fahrenheit">
Grad Fahrenheit

Grad Fahrenheit ist eine Maßeinheit der Temperatur. Sie wurde nach Daniel Gabriel Fahrenheit benannt.

Fahrenheit entwickelte seine Temperaturskala nach einem Besuch bei dem dänischen Astronomen Ole Rømer in Kopenhagen. Rømer war der Erste, der ein Thermometer entwickelte, welches mit Hilfe zweier Fixpunkte kalibriert wurde. In der Rømer-Skala liegt der Gefrierpunkt des Wassers bei 7,5 Grad, der Siedepunkt bei 60 Grad und die durchschnittliche Körpertemperatur eines Menschen bei 26,9 Grad.

Fahrenheit verwendete demgegenüber als Nullpunkt seiner Skala die tiefste Temperatur, die er mit einer Mischung aus Eis, Wasser und Salmiak (= Ammoniumchlorid) oder Seesalz "(Kältemischung)" erzeugen konnte: −17,8 °C. Dadurch wollte er negative Temperaturen vermeiden, wie sie bei der Rømer-Skala schon im normalen Alltagsgebrauch auftraten.

Als zweiten und dritten Fixpunkt legte Fahrenheit 1714 den Gefrierpunkt des reinen Wassers (Eispunkt) bei 32 °F und die Körpertemperatur eines gesunden Menschen bei 96 °F fest. Allerdings entsprechen 96 °F rund 35,6 °C; dieser Wert liegt, verglichen mit heute üblichen Messmethoden, deutlich unterhalb des menschlichen Normaltemperaturbereichs.

Der Nachteil dieser Skala bestand darin, dass mit der verbesserten Genauigkeit von Messungen im 19. Jahrhundert insbesondere der untere und der obere Fixpunkt nicht hinreichend genau reproduzierbar waren. Es wurde damit eine Neudefinition der Skala nötig. Zur Definition einer Temperaturskala benötigt man zum einen nur zwei verschiedene, dafür aber möglichst genau reproduzierbare Temperaturen, zum anderen die willkürliche Festlegung der Einteilung der Temperaturdifferenz in Skalenteile und eines Skalennullpunkts.

Seit den 1860er Jahren, und gesetzlich eingeführt in den Vereinigten Staaten mit dem "Mendenhall Order" von 1893, versuchte man die hergebrachten Einheiten "(customary units)" des angloamerikanischen Maßsystems an die Definitionen des internationalen metrischen Systems anzubinden. Seit dieser Zeit war die Fahrenheit-Skala durch die Skala des hundertteiligen Thermometers definiert, und hatte damit also als Fixpunkte den Gefrierpunkt (gleich 32 °F) und den Siedepunkt des Wassers (gleich 212 °F). Seit 1948, als das hundertteilige Thermometer in Celsius-Skala umbenannt und neu definiert wurde, ist die Fahrenheitskala indirekt durch die Kelvin-Skala definiert.

Die Fahrenheit-Skala war im 18. und frühen 19. Jahrhundert auch in Europa verbreitet (neben anderen, nun gänzlich unüblichen Skalen); erst mit der Durchsetzung des metrischen Systems seit dem mittleren 19. Jahrhundert hat sich in Kontinentaleuropa die Celsius-Skala durchgesetzt. In Großbritannien blieb die Fahrenheit-Skala länger in Gebrauch. Dort wurde sie zudem auch für die Definition gewisser anderer imperialer Maßeinheiten verwendet; so war etwa im "Weights and Measures Act" von 1855 das Yard definiert anhand eines Norm-Yards aus Bronze und Gold bei einer Temperatur von 62 °F. Kopien dieses Norm-Yards, die herstellungsbedingt leichte Abweichungen aufwiesen, wurden durch die Angabe von leicht abweichenden Temperaturen geeicht (zwischen 61,94 und 62,16 °F).

Die offizielle Einführung des metrischen Systems "(metrication)" wurde in Großbritannien bereits ab 1818 diskutiert und zwischen 1965 und 1980 aktiv vorangetrieben, dann aber wieder aufgegeben. Zwingend wurde die Einführung erst mit der Mitgliedschaft in der Europäischen Wirtschaftsgemeinschaft ab 1995 umgesetzt. In inoffiziellen Publikationen, besonders in Wetterprognosen, wurde die Fahrenheit-Skala in Großbritannien auch nach 1995 verwendet, allerdings mit abnehmender Tendenz.

Offizielle Verwendung findet die Fahrenheit-Skala nur noch in den USA und ihren Außengebieten, in Belize sowie auf den Bahamas und den Cayman Islands.

Im Folgenden werden einige Umrechnungstabellen für verschiedene Temperaturwerte und -Einheiten angegeben.

Zum 300. Jahrestag der Einführung der Fahrenheitskala gab die Deutsche Post AG am 3. November 2014 ein Sonderpostwertzeichen im Wert von 60 Eurocent heraus. Der Markenentwurf stammt von den Grafikern Thomas und Martin Poschauko.



</doc>
<doc id="1819" url="https://de.wikipedia.org/wiki?curid=1819" title="Gallone">
Gallone

Die Gallone () ist eine Raumeinheit (Trocken-, Flüssigkeitsmaß, siehe Raummaß). Es gibt unterschiedliche Definitionen der Gallone in den Maßsystemen verschiedener Länder.
Einheitenzeichen: Imp.gal., US.liq.gal., US.dry.gal.

1 Imp.gal. = 4 Imp.qt. = 8 Imp.pt. = 16 Imp.cup = 32 Imp.gi. = 160 Imp.fl.oz. = 4,54609 Liter "(exakt per Definition)" =  inch³ ≈ 277,41945 inch³

1 Imp.gal. = US.liq.gal. ≈ 1,20095 US.liq.gal.

Die britische (imperiale) Gallone basiert auf einem mittelalterlichen englischen Biermaß. Im Jahr 1824 wurde sie neu definiert auf physikalischer Basis als das Volumen von 10 englischen Pfund destilliertem Wasser bei 62 °F (17 °C), gemessen mit Messinggewichten einer bestimmten Zusammensetzung bei festgelegtem Luftdruck, ebenso galt die Definition 1 Gallone = 277,42 Kubikzoll. Im Jahr 1985 wurde dann nach kanadischem Vorbild eine Neudefinition basierend auf dem metrischen Liter eingeführt, wodurch eine Gallone nun etwas weniger als 277,42 Kubikzoll umfasst. In den USA verwendet man dagegen eine aus dem Weinhandel stammende Definition (siehe unten).
1 US.liq.gal. = 4 liquid qt. = 8 US.liq.pt. = 16 US.cup = 32 US.liq.gi. = 128 US.fl.oz. = 231 inch³ = 3,785411784 Liter

Die US-amerikanische Gallone basiert auf einem mittelalterlichen englischen Weinmaß, das ursprünglich als ein Zylinder mit einer Höhe von 6 Zoll (engl. "inch") und einem Durchmesser von 7 Zoll definiert wurde. Die Kreiszahl π wurde damals üblicherweise mit 22/7 approximiert. Im Jahr 1706, während der Herrschaft von Königin Anne, wurde die Weingallone daher redefiniert als ein Quader mit einer Abmessung von 3 × 7 × 11 inch³, was dem alten Zylindervolumen, berechnet mit der erwähnten Näherung für π, entspricht. Diese Definition ist in den USA bis heute in Gebrauch, in Großbritannien selbst wurde sie jedoch durch die aus dem Bierhandel stammende Imperial Gallon (siehe oben) verdrängt. Da das Inch heute als dezimales Vielfaches des Meters definiert ist (1 inch = 25,4 mm), kann das Volumen der US-amerikanischen Gallone in Litern exakt angegeben werden.

Ferner ist "1 petroleum barrel = 42 US.liq.gal." = 9702 inch³ = 158,987294928 Liter
1 US.dry.gal. = 4 US.dry.qt = 8 US.dry.pt = 268,8025 inch³ = 4,40488377086 Liter

1 US.dry.gal. =  US.liq.gal. ≈ 1,1636 US.liq.gal.

"1 US.bushel" = 4 peck "= 8 US.dry.gal." = 2150,42 inch³ ≈ 35,239 Liter
1 metric gallon = 8 metric pint = 16 metric cup = 4 Liter


</doc>
<doc id="1820" url="https://de.wikipedia.org/wiki?curid=1820" title="Gill">
Gill

Gill war ein englisches Volumenmaß und als ein Trocken- und Flüssigkeitsmaß in Anwendung. Das Maß war mit Ausnahme von Bier in den britischen Kolonien und den USA etwa 6 Kubikzoll (Preuß.) oder Quart (Preuß.). Ein Gill liegt zwischen 0,11 und 0,14 Liter. 

In britischen Bars wurde der Schnaps früher in Mengen von gill verkauft, was in etwa 2,3 cl entsprach. Heute sind in Großbritannien 2,5 oder 3,5 cl üblich.




</doc>
<doc id="1821" url="https://de.wikipedia.org/wiki?curid=1821" title="Gran (Einheit)">
Gran (Einheit)

Das Gran (lat. ' „Korn“), auch Grän"', ist eine alte Maßeinheit der Masse. Das Einheitenzeichen ist "gr."

Das römische Gran – wohl ein Gerstenkorn – ist der (12 × 576 =) 6912. Teil der römischen Libra, bzw. der (16 × 576 =) 9216. Teil der Mina. Es wog also recht genau 47 mg.

Das Grain ist die kleinste klassische englische Masse-Einheit und allen drei in der Neuzeit verbreiteten englischen Systemen ("Avoirdupois, Troy" und "apothecaries' weight") gemein. So zählt das übliche Pfund 7000 "grains", das Apotheker- und Troy-Pfund genau 5760 "grains". Daneben wurden auch einige traditionelle Masse-Einheiten in englischen Kolonien anhand des "Grains" neu definiert, etwa das indische Tola zu 180 Grain.


Die Einheit wird zunehmend durch Karat bzw. Milligramm ersetzt. Die Masse von Hand- und Faustfeuerwaffengeschossen, von Pfeilspitzen sowie von Treibladungspulver bei Munition wird weiterhin allgemein in englischen "Grain" angegeben.

Das "französische Grain" war der 9216. Teil des französischen Pfundes. Im Jahre 1799 wurde das definitive, dezimale Kilogramm als 18.827,15 "französische Grain" festgestellt. Das bedeutet ein "französisches Grain" entspricht etwa 53,1148 mg (1 gr. / (18827,15 gr./kg) × 1000000 mg/kg).




</doc>
<doc id="1822" url="https://de.wikipedia.org/wiki?curid=1822" title="Goethe (Begriffsklärung)">
Goethe (Begriffsklärung)

Goethe oder Göthe ist der Familienname folgender Personen:



Goethe, Sonstiges:
Siehe auch:



</doc>
<doc id="1823" url="https://de.wikipedia.org/wiki?curid=1823" title="Giordano Bruno">
Giordano Bruno

Giordano Bruno, vor 1565 Filippo Bruno (* Januar 1548 in Nola; † 17. Februar 1600 in Rom), war ein italienischer Priester, Dichter, Philosoph und Astronom. Er wurde durch die Inquisition der Ketzerei und Magie für schuldig befunden und vom Gouverneur von Rom zum Tod auf dem Scheiterhaufen verurteilt. Am 12. März 2000 erklärte Papst Johannes Paul II. nach Beratung mit dem päpstlichen Kulturrat und einer theologischen Kommission, die Hinrichtung sei nunmehr auch aus kirchlicher Sicht als Unrecht zu betrachten.

Bruno postulierte die Unendlichkeit des Weltraums und die ewige Dauer des Universums. Damit stellte er sich der damals herrschenden Meinung einer in Sphären untergliederten geozentrischen Welt entgegen. Viel schwerer wog damals, dass seine pantheistischen Thesen von einer unendlichen materiellen Welt keinen Raum für ein Jenseits ließen, da zeitliche Anfangslosigkeit des Universums eine Schöpfung und dessen ewiger Bestand ein Jüngstes Gericht ausschlossen.

Giordano Bruno wurde im Jahre 1548 unter dem Namen Filippo Bruno in Nola bei Neapel geboren. Von seinem Heimatort ist seine spätere Selbstbezeichnung „Nolano“ (der Nolaner) abgeleitet. Sein Vater war Giovanni Bruno, ein Soldat, seine Mutter Fraulissa (Flaulisa?) Savolino.

Bruno studierte zunächst in Neapel und trat am 15. Juni 1565 in den Orden der Dominikaner ein, und zwar in das Kloster "San Domenico Maggiore", wo er den Taufnamen Filippo ablegte und den Ordensnamen Jordanus/Giordano (nach dem zweiten Ordensmeister Jordan von Sachsen) annahm. Bald darauf geriet er in Konflikt mit der Ordensleitung, da er sich der Marienverehrung verweigerte und alle Heiligenbilder aus seiner Klosterzelle entfernte. Doch wurde dies als jugendliche Verirrung aufgefasst und blieb zunächst folgenlos. 1572 empfing er die Priesterweihe.

1576 geriet er zum ersten Mal unter Verdacht der Ketzerei und musste Neapel verlassen. Er floh nach Rom, um sich dem Papst zu Füßen zu werfen. Als dort jedoch bekannt wurde, dass Bruno bei seiner Flucht aus dem Kloster Schriften des Kirchenvaters Hieronymus in die Latrine geworfen hatte, musste er auch aus Rom fliehen. Er trat aus dem Mönchsorden aus und reiste nach Noli und Savona (Ligurien), dann nach Turin, Venedig und Padua weiter. Brunos Leben wurde fortan zu einer Wanderschaft durch Europa.

Die wiederentdeckten Ideen der antiken Naturphilosophie übten große Anziehung auf ihn aus. Zu dieser Zeit begann sich das von Nikolaus Kopernikus postulierte heliozentrische Weltbild durchzusetzen. Hierdurch ermutigt, entwickelte Bruno im Laufe der folgenden Jahre seine eigene Philosophie.

Über Chambéry ging er im Spätherbst 1578 zunächst nach Genf, wo seit Johannes Calvins Tod Théodore de Bèze dessen Nachfolge angetreten hatte. Bruno trat der protestantischen Kirche bei und hoffte, so Schutz vor der römischen Inquisition zu finden. Infolge unüberbrückbarer theologischer Differenzen wurde Bruno im August 1579 für kurze Zeit inhaftiert und mit Maßnahmen der calvinistischen Kirchenzucht belegt. Um freizukommen, widerrief er. Ende August 1579 kam er frei und reiste sogleich nach Lyon weiter.

Bruno gelangte dann 1579 nach Toulouse und hatte dort kurz einen Lehrstuhl inne. Zu dieser Zeit begann sein phänomenales Gedächtnis Furore zu machen. Bruno arbeitete mit einer speziellen Mnemotechnik. Aber die Erklärung, dass er magische Fähigkeiten habe, schien manchen Zeitgenossen dann doch einleuchtender.

Die Hugenottenkriege trieben ihn dann zwei Jahre später nach Paris. Dort blieb er bis 1583 und wurde von König Heinrich III. gefördert.

Mit einem Empfehlungsschreiben Heinrichs III. ging er 1583 nach England, versuchte zunächst in Oxford zu lehren, verursachte mit seinen Angriffen auf Aristoteles und wegen eines Plagiatsvorwurfs jedoch einen Skandal und erhielt keinen Lehrstuhl. Bis Mitte 1585 lebte er dann im Haus seines Freundes und Förderers, des französischen Botschafters Michel de Castelnau Mauvissière (1517–1592), in London. Er machte Bekanntschaft mit Philip Sidney und mit Mitgliedern von John Dees hermetischem Zirkel. Ob Bruno John Dee persönlich begegnete, bleibt ungewiss. Seine Ansichten setzten in Oxford eine intensive Kontroverse in Gang, an der John Underhill, der Rektor des Lincoln College und spätere Bischof von Oxford, sowie George Abbot, der spätere Erzbischof von Canterbury, beteiligt waren.

Dort veröffentlichte er seine „italienischen Dialoge“, darunter "Cena de le Ceneri" (Das Aschermittwochsmahl) (1584), in dem er schonungslose Polemik gegen den Oxforder Gelehrtenstand übte und das Londoner Geistesleben heftig karikierte, sowie "De l’Infinito, Universo e Mondi" (Über die Unendlichkeit, das Universum und die Welten). In letzterem erklärte er die Sterne damit, dass sie wie unsere Sonne seien, dass das Universum unendlich sei, es eine unendliche Anzahl von Welten gebe und diese mit einer unendlichen Anzahl intelligenter Lebewesen bevölkert seien.

1585 ging er wieder nach Paris, die Stimmung dort war aber nicht so aufgeschlossen wie noch zwei Jahre zuvor. Nach Tumulten, die durch seine 120 Thesen gegen die aristotelische Naturlehre und ihre Vertreter entfacht wurden, und nach einer Schmähschrift gegen den Mathematiker Fabrizio Mordente musste er Paris verlassen.

Bruno reiste nach Deutschland weiter und versuchte, einen Lehrstuhl in Marburg zu erhalten. Im Sommer 1586 kam Bruno nach Wittenberg. Auf Fürsprache des Rechtsgelehrten Alberico Gentilis fand er Aufnahme als Extraordinarius an der Artistenfakultät der Universität Wittenberg. Er erhielt das Recht auf freie Vorträge über Philosophie. In seinen Vorlesungen behandelte er das Organon des Aristoteles, Mathematik, Logik, Physik und Metaphysik.

In Wittenberg entstanden 1587 zwei Bücher über Logik und Gedächtniskunst – ein Thema, das später Gottfried Wilhelm Leibniz fortsetzen sollte –, die Bruno dem Kanzler der Universität Georg Mylius widmete. Als 1588 in Wittenberg Streitigkeiten zwischen Gnesiolutheranern und Philippisten ausbrachen, verließ Bruno am 8. März die Stadt und ging für ein halbes Jahr nach Prag. Zwar gewann er die Gunst Kaiser Rudolfs II., erhielt aber keinen Lehrauftrag. Mit einer finanziellen Unterstützung von 300 Talern von Rudolf II. reiste er nach Helmstedt weiter, wo er eine Professur an der Academia Julia erhielt. Hier sammelte er, wie in Noli, in der Stille seine Kräfte und bereitete die "Frankfurter Schriften" vor, die sein philosophisches Vermächtnis werden sollten. Es hielt ihn nicht lange; nach den Calvinisten in Genf exkommunizierten ihn jetzt die Lutheraner.

Wo auch immer Bruno wirkte, versuchte er einen festen Lehrstuhl zu erhalten – erfolglos. Brunos Talent, sich in der Welt der komplizierten Machtverhältnisse der Renaissance zu behaupten, könnte zwiespältiger nicht interpretiert werden: Auf der einen Seite gelang es ihm immer wieder, mächtige Gönner auf seine Seite zu ziehen. Auf dem theologisch-philosophischen Kampfplatz schuf er sich Feinde mit rücksichtsloser Polemik, beißendem Spott und insbesondere mit der Ablehnung der Gottessohnschaft Christi und mit seiner kompromisslosen Gegnerschaft zu Aristoteles.

Ab Juli 1590 lebte er in Frankfurt am Main. In der Freien und Reichsstadt kam es aber zu Auseinandersetzungen u. a. mit Johannes Münzenberger, der seit 1574 Kustos und ab 1580 Prior am Karmeliterkloster Frankfurt war, aber auch mit den Stadtoberen aus dem Rat der Stadt, die ihn im Februar 1591 auswiesen. Er plante in der freien Reichsstadt zunächst bei dem Verleger und Drucker Johann Wechel († 1593) zu wohnen. Die Stadtherren lehnten Brunos Ansinnen ab. Johann Wechel fand für Bruno eine Unterkunft im Karmeliterkloster.

Es folgt ein Kurzaufenthalt in Zürich.

Während seiner Frankfurter Zeit erfasste ihn wohl Heimweh. In Italien war freilich die Inquisition mächtig, und die katholische Kirche kämpfte mit allen Mitteln gegen die Reformation. Schließlich waren es der Tod des konservativen Papstes Sixtus V. und die Vakanz eines Lehrstuhls für Mathematik an der Universität Padua, die den Ausschlag gaben, dass Bruno nach Italien zurückkehrte. Während eines Aufenthalts auf der Buchmesse in Frankfurt erreichte ihn eine Einladung von Mocenigo nach Venedig, die er jedoch ablehnte.

Er lehrte zunächst in Padua, doch wurde der Lehrstuhl bald an Galileo Galilei vergeben. Bruno nahm danach eine Einladung nach Venedig an. Sein Gastgeber, Zuane Mocenigo (1531–1598), "Provveditore Generale di Marano", wollte in die Gedächtniskunst eingeweiht werden; doch es ist viel wahrscheinlicher, dass er sich von Bruno Einblick in weit „magischere“ Künste erhoffte. Wohl aus Enttäuschung, dass diese Erwartungen nicht erfüllt wurden, kam es zu Streitigkeiten. Während Bruno noch überlegte, Venedig zu verlassen, wurde er von Mocenigo denunziert und am 22. Mai 1592 von der Inquisition verhaftet.

Im venezianischen Kerker widerrief er nach sieben Verhören. Die Macht der Inquisition traf in Venedig auf wenig Widerstand, da sich Venedig für Bruno als nicht zuständig erachtet haben dürfte. Einerseits war Venedig zuerst nicht geneigt, Bruno nach Rom auszuliefern, andererseits war er nach damaliger Rechtsauffassung ein geflohener Mönch, der ausgeliefert werden musste. Zudem wurde er ein Opfer der damaligen politischen Spiele.

Anfang 1593 wurde Giordano Bruno nach Rom gebracht und in der Engelsburg gefangengesetzt. In den folgenden sieben Jahren wurde der Prozess gegen ihn vorbereitet. Er versuchte vergeblich, eine Audienz bei Papst Clemens VIII. zu erreichen, und war sogar bereit, teilweise zu widerrufen. Doch dies genügte der Inquisition nicht. Als sie den vollständigen Widerruf forderte, reagierte Bruno hinhaltend und schließlich trotzig: An der Ablehnung der Gottessohnschaft Christi, des Jüngsten Gerichts und der Behauptung vieler ‚Welten‘ hielt er fest.

Am 8. Februar 1600 wurde das Urteil des Heiligen Offiziums verlesen: Giordano Bruno wurde wegen Ketzerei und Magie aus dem Orden der Dominikaner und aus der Kirche ausgestoßen und dem weltlichen Gericht des Gouverneurs in Rom überstellt, mit der herkömmlichen Bitte, dieser möge die Strenge des Gesetzes mildern und keine Strafen gegen Leib oder Leben verhängen. Außerdem wurden alle seine Schriften verboten, seine Werke sollten öffentlich zerrissen und verbrannt werden. Bruno reagierte auf das Urteil mit seinem berühmt gewordenen Satz: „Mit größerer Furcht verkündet ihr vielleicht das Urteil gegen mich, als ich es entgegennehme“ („Maiori forsan cum timore sententiam in me fertis quam ego accipiam“).

Von dem weltlichen Gericht des römischen Gouverneurs wurde Bruno anschließend zum Tod auf dem Scheiterhaufen verurteilt. Von fast achtjähriger Kerkerhaft körperlich gebrochen, wurde der 52-jährige Giordano Bruno am 17. Februar 1600 auf dem Campo de’ Fiori auf dem Scheiterhaufen hingerichtet. Vor der Hinrichtung wurde Giordano Bruno angeblich die Zunge festgebunden, damit er nicht zum anwesenden Volk sprechen konnte.

Seine Bücher wurden auf den Index der verbotenen Schriften gesetzt, wo sie bis zu dessen Abschaffung 1966 im Zuge des Zweiten Vatikanischen Konzils blieben.

Im Jahr 2000 erklärte Papst Johannes Paul II. nach Beratung mit dem päpstlichen Kulturrat und einer theologischen Kommission die Hinrichtung Giordano Brunos für Unrecht: Selbst Männer der Kirche seien im Namen des Glaubens und der Sittenlehre mitunter Wege gegangen, „die nicht im Einklang mit den Evangelien stehen“. Eine vollständige Rehabilitierung des Gelehrten Giordano Bruno durch die katholische Kirche fand aber nicht statt, da der Pantheismus nicht mit der katholischen Lehre vereinbar sei.

Bruno beeinflusste eine Reihe von Philosophen und Schriftstellern stark, unter anderem Pierre Gassendi, Baruch de Spinoza, Lucilio Vanini, Friedrich Schelling, Galileo Galilei, Johann Wolfgang von Goethe und Friedrich Nietzsche. Gottfried Wilhelm Leibniz übernahm von ihm den Begriff der Monade.

Aus der großen Zahl von literarischen Verarbeitungen des Lebens Brunos zwei Beispiele: 1841 veröffentlichte Leopold Schefer die Novelle "Göttliche Komödie in Rom" über den Prozess und die Hinrichtung Giordano Brunos. Bertolt Brecht schrieb die Erzählung "Der Mantel des Ketzers". Gian Maria Volonté verkörperte 1973 Giordano Bruno in dem Spielfilm "Der Mönch von San Dominico (Giordano Bruno)" von Giuliano Montaldo.

Auf dem Campo de’ Fiori in Rom erinnert ein Denkmal der Freimaurer des Grande Oriente d’Italia, das von der laizistisch regierten Stadtgemeinde 1889 gegen den Willen des damaligen Papstes Leo XIII. (1878–1903) errichtet wurde, an Giordano Bruno.

Nach Giordano Bruno ist der Asteroid (5148) Giordano und ein 22 km durchmessender Mondkrater benannt, 103° östl. Länge, 36° nördl. Breite. Im deutschsprachigen Raum trägt seinen Namen die 2004 gegründete Giordano-Bruno-Stiftung, die sich dem evolutionären Humanismus und der Förderung der Religionskritik widmet und insbesondere den Religionskritiker Karlheinz Deschner förderte. Außerdem ist die Giordano-Bruno-Gesamtschule in Helmstedt nach ihm benannt. Seit 2008 gibt es am Potsdamer Platz in Berlin ein Giordano-Bruno-Denkmal.

Für Bruno stammte alles aus der Natur von der göttlichen Einheit von Materie und Dunkelheit ab. Zum einen trennte er Gott von der Welt, und zum anderen tendierte er zu einem dazu entgegengesetzten Pantheismus. Bruno verband die These, dass Gott allem innewohne, mit dem Glauben, dass die Realität der Vorstellung entspringe. Damit nahm er die Gedanken von Gottfried Wilhelm Leibniz und Baruch de Spinoza vorweg. Er stellte sich gegen das geozentrische Weltbild, nahm stattdessen an, dass die Welt und die Menschen ein einmaliger "Unfall" einer einzelnen lebenden "Weltsubstanz" seien, und bekannte sich zur kopernikanischen Theorie. Weiterhin postulierte er die Monade, die als eine unteilbare Einheit ein Element des Weltaufbaus darstellt. Der Begriff Monade wurde von Gottfried Wilhelm Leibniz übernommen. Bruno ist einer der wichtigsten Vertreter einer panpsychistischen Weltanschauung, der zufolge überall im Kosmos geistige Eigenschaften vorhanden sind.

Von den christlichen Kirchen wurde Atheismus und Pantheismus lange Zeit gleichgesetzt. Die Vorstellungen Giordano Brunos stehen im Gegensatz zum materialistischen Weltbild. Sie stehen in der Tradition des neuplatonischen Idealismus sowie der Mystik, die er vor allem durch die Werke von Avicenna, Averroes, Nikolaus von Kues rezipiert hat. Zwar hat Bruno viele Erkenntnisse der modernen Naturwissenschaften vorweggenommen. Dies verdankt sich jedoch eher einem „naturphilosophischen Ganzheitsdenken“ als einem physikalisch-analytischen Zugang, wie er etwa für seinen Zeitgenossen Galileo Galilei kennzeichnend war. Dies wird besonders klar in Brunos Erkenntnistheorie, verdeutlicht etwa in seiner Interpretation des Aktaion-Mythos in den "Heroischen Leidenschaften". Mit dem nach Wahrheit Suchenden verhält es sich laut Bruno wie mit dem griechischen Jäger Aktaion. Dieser hatte auf der Jagd die nackte Göttin Diana beim Bad überrascht und wird in einen Hirsch verwandelt, der von seinen eigenen Hunden gejagt und zerrissen wird. Diana ist hier ein Sinnbild für die Natur, deren Erkenntnis sich dem Menschen entziehen will. Bruno schreibt, es sei „das letzte Ziel und das Ende dieser Jagd [nach der Wahrheit] […], in den Besitz jener flüchtigen und scheuen Beute zu gelangen, durch die der Beutemacher zur Beute, der Jäger zum Gejagten wird.“ Das Göttliche wird im Pantheismus Brunos nicht etwa in die Natur hineingelegt, die dann ein vom Erkenntnissubjekt unabhängiger, objektiver Forschungsgegenstand wäre. Vielmehr wird auch das Erkenntnissubjekt als Teil des Kosmos begriffen. Es löst sich in seiner Individualität auf, sobald es die Erfahrung der pantheistischen Einheit macht, die bei Bruno mystischen, übersinnlichen Charakter hat. So heißt es in Brunos Interpretation des Aktaion-Mythos:

„So verschlingen die Hunde, die Gedanken an göttliche Dinge, diesen Aktaion, so dass er nun für das Volk, die Menge tot ist, gelöst aus den Verstrickungen der verwirrten Sinne, frei vom fleischlichen Gefängnis der Materie. Deshalb braucht er seine Diana nun nicht mehr gleichsam durch Ritzen und Fenster zu betrachten, sondern ist nach dem Niederreißen der Mauern ganz Auge mit dem gesamten Horizont im Blick.“

Den Prinzipien seiner Naturphilosophie folgend, glaubte Bruno nicht nur, dass das Weltall unendlich ist, sondern dass es auch unendlich viele Lebewesen auf anderen Planeten im Universum gibt. Diese Schlussfolgerungen zog er aus dem Gedanken, dass einer allmächtigen und unendlichen Gottheit auch nur ein unendliches Universum entsprechen kann, denn alles andere wäre einer unendlichen Gottheit nicht würdig. Giordano Bruno kann in seiner Philosophie aber nicht einfach „hinter“ Kopernikus oder Galilei eingereiht werden. Er teilte deren in erster Linie auf der Beobachtung der Natur basierende Überlegungen nicht. Er zweifelte an der Kompetenz der Mathematik und setzte an deren Stelle seine spezifische naturphilosophische Betrachtungsweise. In seiner Gesamtheit kann Brunos Denken in die Philosophia perennis eingeordnet werden, der er einen neuen naturphilosophischen Zugang sowie revolutionären und kämpferischen Aspekt hinzufügte.

Zwar übernahm Bruno zunächst von Aristoteles die Vorstellung, die riesigen Räume zwischen den unendlich vielen Sonnensystemen seien mit Äther erfüllt, weil leerer Raum nicht existieren könne, doch entwickelte er schließlich in "De immenso" die Konzeption eines Vakuums. Zudem brach er mit der bis dahin gängigen Auffassung des Aristoteles von der Zweigeteiltheit der Welt in den translunaren und den sublunaren Bereich. Der Bereich über der Mondsphäre galt als der heilige Bereich, von dem allein ein verlässliches Zeitmaß abgenommen werden konnte. Dies galt aber nicht für den Bereich unterhalb der Mondsphäre, den sublunaren Bereich, in dem sich die Erde befand, so dass es vor Giordano Bruno nicht denkbar war, ein irdisches Zeitmaß anzugeben. Durch die Aufhebung dieser Grenze zwischen sublunarem und translunarem Bereich durch Giordano Bruno wurde die Erde in den göttlichen Bereich einbezogen, so dass auch auf der Erde gültige Zeitmaßstäbe denkbar wurden.

In "De immenso" entwarf er eine erste Raumfahrtidee. „Mit den Flügeln des Geistes“ unternahm er Reisen zum Mond und anderen Gestirnen, führte Gedankenexperimente zur planetaren Perspektive durch und fragte nach den Gründen für die Fähigkeit des Menschen, begrenzte Horizonte überwinden zu können.

Sein Denken wurde von Platon, Epikur, Lukrez, Thomas von Aquin, Johannes Scotus Eriugena, Nikolaus von Kues und Ramon Llull beeinflusst. Er war ein ausgeprägter Kritiker der Lehren des Aristoteles. Untersuchungsergebnissen der Kulturhistorikerin Frances Yates zufolge war Bruno auch von Marsilio Ficino und der hermetischen Literatur beeinflusst. In England vertrat vor ihm auch schon der frühe Kopernikaner Thomas Digges die These der Unendlichkeit des Weltraums und veröffentlichte das, was Bruno beeinflusst haben könnte.

Werkausgaben




</doc>
<doc id="1827" url="https://de.wikipedia.org/wiki?curid=1827" title="Gregor Gysi">
Gregor Gysi

Gregor Florian Gysi (Aussprache []) (* 16. Januar 1948 in Berlin) ist ein deutscher Jurist, Rechtsanwalt und Politiker (SED, PDS, Die Linke). Er ist seit 2005 Mitglied des Deutschen Bundestages, dem er schon zwischen 1990 und 2000 angehört hatte. Von 2005 bis 2015 war er Fraktionsvorsitzender der Linksfraktion im Deutschen Bundestag. Mit dem Amtsantritt des dritten Kabinetts Merkel am 17. Dezember 2013 wurde Gysi zusätzlich Oppositionsführer im Deutschen Bundestag. Am 17. Dezember 2016 wählte die Europäische Linke Gysi auf einem Parteitag in Berlin zum Präsidenten.

Von Ende 1989 bis 1993 war Gysi letzter Vorsitzender der "SED-PDS" und ihrer Nachfolgepartei PDS. Nach der erstmals freien Volkskammerwahl 1990 führte er ihre Fraktion in der Volkskammer der DDR. Von 1990 bis 1998 war er Vorsitzender der Bundestagsgruppe der PDS und von 1998 bis 2000 Vorsitzender der PDS-Bundestagsfraktion. Im Jahr 2002 war er fünf Monate Bürgermeister und Senator für Wirtschaft, Arbeit und Frauen des Landes Berlin.

Gysi war und ist eine zentrale und prominente Figur der PDS bzw. der Partei Die Linke und wirkte dabei prägend auf das politische Geschehen in der Bundespolitik seit 1990. Zu seinen politischen Erfolgen zählt der Formationsprozess der SED, der er ab 1967 angehörte, zur PDS und die Schaffung einer bundesdeutschen linken Partei.

Gregor Gysi stammt aus einer Berliner Familie, deren Stammvater, der Seidenfärber Samuel Gysin (* 1681), im frühen 18. Jahrhundert aus Läufelfingen (Schweiz) eingewandert war. Einer seiner Vorfahren väterlicherseits war der Begründer der deutschen Rassegeflügelzucht, Robert Oettel. Gysi hat auch jüdische Vorfahren, so einen jüdischen Urgroßvater mütterlicherseits und eine jüdische Großmutter väterlicherseits. Gregor Gysis Vater Klaus Gysi (1912–1999) war somit nach der Halacha jüdisch, Gregor Gysi selbst hingegen nicht.

Klaus Gysi, studierter Ökonom, trat 1931 der KPD bei und arbeitete nach dem Zweiten Weltkrieg in der DDR. Er war u. a. als Geschäftsleiter des Aufbau Verlags tätig, später als Botschafter in Italien, als DDR-Kulturminister und Staatssekretär für Kirchenfragen. Er war auch für die Staatssicherheit als "IM Kurt" tätig.

Mütterlicherseits stammen Gregor Gysis Vorfahren aus der jüdischen Kaufmannsfamilie Lessing, die aus der Nähe von Bamberg kam und zeitweise in Sankt Petersburg lebte und tätig war. Sein Urgroßvater war der nach St. Petersburg ausgewanderte Industrielle Anton Lessing, sein Urgroßonkel der Gründer der Bamberger Hofbräu AG Simon Lessing. Gysis Großvater mütterlicherseits Gottfried Lessing, Anton Lessings Sohn, ein in Russland lebender Hütteningenieur, heiratete die deutsch-russische Adelige Tatjana von Schwanebach. Dieser Ehe entstammten zwei Kinder: Gregor Gysis Mutter Irene (1912–2007) und Gottfried Lessing (1914–1979), der zweite Ehemann der späteren Literaturnobelpreisträgerin Doris Lessing. Nach Ausbruch des Ersten Weltkriegs wurde die Familie aufgrund ihrer deutschen Herkunft nach Deutschland ausgewiesen.

Die Eltern von Gregor Gysi hielten sich während des Zweiten Weltkriegs in Deutschland auf. Das Paar war im Auftrag der KPD im Widerstand gegen den Nationalsozialismus und heiratete 1945; die Ehe wurde nach 14 Jahren geschieden.

Irene Gysi war im Kulturministerium der DDR für den Austausch mit dem Ausland zuständig und leitete später die ostdeutsche Filiale des Internationalen Theaterinstituts.

Gysi wuchs in Ost-Berlin auf. Dort besuchte er von 1954 bis 1962 eine Polytechnische Oberschule, von 1962 bis 1966 die Erweiterte Oberschule „Heinrich Hertz“ (ab 1965 Schule mit mathematischem Schwerpunkt) im Stadtteil Adlershof. Hier erwarb er 1966 das Abitur und legte gleichzeitig den Lehrabschluss zum Facharbeiter für Rinderzucht (im VEG Blankenfelde) ab. Anschließend absolvierte Gysi ein Studium der Rechtswissenschaft an der Humboldt-Universität zu Berlin, das er 1970 als Diplom-Jurist beendete.

Ab 1971 war Gysi einer der wenigen freien Rechtsanwälte in der DDR. In dieser Funktion verteidigte er auch Systemkritiker und Ausreisewillige, darunter bekannte Personen wie Robert Havemann, Rudolf Bahro, Jürgen Fuchs, Bärbel Bohley und Ulrike Poppe. 1976 erfolgte seine Promotion zum Dr. jur. mit der Arbeit "Zur Vervollkommnung des sozialistischen Rechtes im Rechtsverwirklichungsprozeß".

Von 1988 bis 1989 war er Vorsitzender des Kollegiums der Rechtsanwälte in Ost-Berlin und gleichzeitig Vorsitzender der 15 Kollegien der Rechtsanwälte in der DDR. Am 12. September 1989 war er zusammen mit dem Ost-Berliner Rechtsanwalt Wolfgang Vogel in Prag, um die DDR-Flüchtlinge in der deutschen Botschaft zur Rückkehr in die DDR aufzufordern. Im Herbst 1989, vor der politischen Wende in der DDR, setzte Gysi sich als Anwalt für die Zulassung des oppositionellen Neuen Forums ein. Von August 2002 bis zu seiner Wiederwahl als Abgeordneter des Bundestages im Jahre 2005 sowie nach seinem Rücktritt als Fraktionsvorsitzender der Linkspartei im Deutschen Bundestag 2016 war bzw. ist Gysi wieder als Rechtsanwalt in Berlin tätig.

Seit 1967 war Gysi Mitglied der SED. Als er 1989 in den Blickpunkt der Öffentlichkeit trat, arbeitete er an einem Reisegesetz mit. Am 4. November 1989 sprach Gysi vor 500.000 Menschen auf der Massenkundgebung auf dem Berliner Alexanderplatz und forderte ein neues Wahlrecht sowie ein Verfassungsgericht. Seine Eloquenz und rhetorische Begabung ließen ihn schnell zu einem der Medienstars des Herbstes werden. Ab dem 3. Dezember 1989 gehörte er dem Arbeitsausschuss zur Vorbereitung des außerordentlichen Parteitages der SED an und war Vorsitzender eines parteiinternen Untersuchungsausschusses.

Auf dem Sonderparteitag am 9. Dezember 1989 wurde Gysi mit 95,3 Prozent der Delegiertenstimmen zum Vorsitzenden der SED gewählt. Am 16. Dezember 1989 sprach er sich auf dem Sonderparteitag der SED-PDS für eine Zusammenarbeit beider deutscher Staaten bei voller Wahrung ihrer Souveränität aus. Im Winter 1989/90 war Gysi als Parteivorsitzender der damaligen SED-PDS daran beteiligt, dass die Partei nicht aufgelöst wurde und das Parteivermögen sowie Arbeitsplätze innerhalb der Partei erhalten blieben. Den Parteivorsitz der PDS hatte Gysi bis zum 31. Januar 1993 inne. Danach wirkte er zunächst als stellvertretender Parteivorsitzender, dann als Mitglied im Parteivorstand weiter mit, bis er im Januar 1997 endgültig aus dem Parteivorstand ausschied.

Am 23. Dezember 2005 wurde er auch Mitglied der WASG, ebenso wie Oskar Lafontaine auch Mitglied in der Linkspartei PDS wurde. Damit machten beide demonstrativ von der Möglichkeit einer Doppelmitgliedschaft in der Linkspartei und in der WASG Gebrauch. Seit dem 16. Juni 2007 ist Gysi Mitglied der Partei Die Linke; ebenso ist er Mitglied der parteinahen Rosa-Luxemburg-Stiftung. Im Dezember 2016 wurde er zum Vorsitzenden der Europäischen Linken gewählt.

1990–2002: Mitglied der Volkskammer der DDR und des Deutschen Bundestags:Von März bis Oktober 1990 war Gysi Abgeordneter der ersten frei gewählten Volkskammer der DDR, dort Fraktionsvorsitzender der PDS. Als solcher wurde er am 3. Oktober 1990 Mitglied des Deutschen Bundestages, aus dem er am 1. Februar 2002 ausschied, um das Amt des Wirtschaftssenators in Berlin anzutreten. Er war von 1990 bis 1998 Vorsitzender der PDS-Bundestagsgruppe, dann bis zum 2. Oktober 2000 Vorsitzender der PDS-Bundestagsfraktion.

2001−2002 Abgeordneter und Stadtsenator in Berlin: 2001 wurde Gysi Mitglied des Abgeordnetenhauses von Berlin. Am 17. Januar 2002 wurde er Bürgermeister und Senator für Wirtschaft, Arbeit und Frauen des Landes Berlin in dem vom Regierenden Bürgermeister Klaus Wowereit geführten Senat Wowereit II. Am 31. Juli 2002 trat er im Rahmen der Bonusmeilen-Affäre von allen Ämtern zurück.

Seit 2005 Abgeordneter zum Deutschen Bundestag: Für die Bundestagswahl 2005 kehrte Gysi als Spitzenkandidat der Linkspartei in die Bundespolitik zurück. Er war Direktkandidat für den Wahlkreis 85 Treptow-Köpenick und führte die Landesliste der Linkspartei Berlin an. Bei der Wahl konnte er sich gegen seinen Konkurrenten Siegfried Scheffler von der SPD durchsetzen und zog mit 40,4 Prozent der abgegebenen Erststimmen direkt in den Bundestag ein. Gemeinsam mit Oskar Lafontaine wurde er zum Fraktionsvorsitzenden der Linksfraktion gewählt.

Auch bei der Bundestagswahl 2009 trat er als Spitzenkandidat der Berliner Landesliste an. Sein Erststimmen-Ergebnis in seinem Wahlkreis Berlin-Treptow-Köpenick konnte er jedoch auf 44,4 Prozent verbessern und zog somit erneut per Direktmandat in den Bundestag ein. Nach dem Verzicht Oskar Lafontaines wurde Gysi am 9. Oktober 2009 mit 94,7 Prozent zum alleinigen Fraktionsvorsitzenden der Bundestagsfraktion der Linken bestimmt und 2011 mit 81,3 Prozent im Amt bestätigt.

Bei der Bundestagswahl 2013 gelang es Gysi – wiederum Spitzenkandidat der Berliner Landesliste – trotz leichter Einbußen von 2,2 Prozentpunkten sein Direktmandat mit 42,2 Prozent erneut zu verteidigen.
Wie schon 2011 wies er Sahra Wagenknechts Ambitionen auf eine Doppelspitze in der Fraktion erfolgreich zurück und wurde am 9. Oktober 2013 auf einer Fraktionsklausur im brandenburgischen Bersteland erneut zum alleinigen Fraktionsvorsitzenden gewählt. Aufgrund der regierenden Großen Koalition war er damit Oppositionsführer.

Am 7. Juni 2015 gab er bekannt, dass er nicht erneut für den Fraktionsvorsitz der Linken kandidieren werde. Entsprechend schied er am 12. Oktober 2015 aus beiden Ämtern aus. Seine Nachfolge im Fraktionsvorsitz und damit auch in der Oppositionsführung wurden Dietmar Bartsch und Sahra Wagenknecht.

Bei der Bundestagswahl 2017 gewann Gysi sein Mandat als Direktkandidat im Bundestagswahlkreis Berlin-Treptow – Köpenick. Er ließ sich nicht über die Landesliste absichern.

Auf dem Sonderparteitag der SED vom 8./9. und 16./17. Dezember 1989 unterstützte Gregor Gysi den Fortbestand der SED unter neuem Namen („SED-PDS“) unter anderem mit dem Argument, eine Auflösung und Neugründung würde juristische Auseinandersetzungen um das Parteivermögen nach sich ziehen und sei eine ernste wirtschaftliche Bedrohung für die Partei.
Später wurde ihm seitens der Unabhängigen Kommission zur Überprüfung des Vermögens der Parteien und Massenorganisationen der DDR vorgeworfen, er sei aktiv an der Verschleierung des SED-Parteienvermögens beteiligt gewesen und habe im Putnik-Deal versucht, mit Hilfe der KPdSU SED-Gelder ins Ausland zu verschieben, um sie vor dem Zugriff staatlicher Stellen zu sichern. Der Untersuchungsausschuss des Deutschen Bundestages 1998 zum Verbleib des SED-Parteienvermögens gab an, dass Gysi bei seiner Befragung geschwiegen und damit zusammen mit weiteren PDS-Funktionären die Arbeit des Ausschusses behindert habe.

Im Januar 2012 wurde bekannt, dass Gregor Gysi als einer von 27 Bundestagsabgeordneten der Linken unter Beobachtung durch das Bundesamt für Verfassungsschutz steht,
was von Politikern aller Fraktionen kritisiert wurde.

Nachdem diese Überwachung Anfang 2014 eingestellt worden war, stellte das Verwaltungsgericht Köln in einem Anerkenntnisurteil im September 2014 fest, dass die Personenakte Gysis zu vernichten sei.

Bericht des Immunitätsausschusses 1998: Laut Abschlussbericht des Immunitätsausschusses des Deutschen Bundestages soll Gysi zwischen 1975 und 1986 für das Ministerium für Staatssicherheit der DDR unter verschiedenen Decknamen, dabei hauptsächlich als "IM Notar" gearbeitet haben, nachdem in einer früheren Version des Abschlussberichtes noch davon die Rede war, dass ein solcher Nachweis aufgrund der vorhandenen Unterlagen nicht erfolgen kann.

Im Abschlussbericht heißt es unter anderem, Gysi habe
Gregor Gysi bezog zu diesen Aussagen im Abschlussbericht wie folgt Stellung:
Die Feststellungen des Immunitätsausschusses hatten aber keine Auswirkungen auf Gysis Arbeit als Abgeordneter, der im Abschlussbericht selbst der Beschuldigung widersprach und auf „wesentliche Mängel und Fehler“ im Verfahren hinwies. Die PDS und die FDP stimmten dem Papier nicht zu.

Gysi legte erneut Klage gegen die Feststellung ein. Er bekannte sich zur Kooperation mit der Staatsanwaltschaft und dem Zentralkomitee der SED „im Interesse und mit Wissen seiner Klienten“ und ging mehrmals erfolgreich, gerichtlich gegen die Verbreitung der Behauptung, er wäre "IM Gregor" / "IM Notar" gewesen, vor. 1998 untersagte das Landgericht Hamburg dem Magazin "Der Spiegel", weiterhin zu behaupten, Gregor Gysi habe für die Stasi-Spionageabteilung gearbeitet und dort den Decknamen "IM Notar" geführt, weil der Spiegel seine Behauptungen nicht habe beweisen können.

Klage Gysis gegen Aktenveröffentlichungen 2008: Nachdem das ZDF am 27. Mai 2008 ein Interview mit Marianne Birthler ausgestrahlt hatte, in dem sie Gysi eine Stasi-Tätigkeit vorwarf, ging Gysi mit einem Unterlassungsbegehren gegen den Sender vor.

Im Mai 2008 unterlag Gysi vor dem Berliner Verwaltungsgericht mit einer Klage gegen die Veröffentlichung mehrerer Protokolle über Robert Havemann und den – laut diesen Berichten – zur DDR-Führung „negativ eingestellt[en]“ Thomas Klingenstein, geb. Erwin. In einem der Protokolle ist die Rede von einer Autofahrt eines „IM“ mit „Erwin“. Das Protokoll wird von der BStU und von Klingenstein selbst auf die Rückfahrt von einem Besuch bei Havemann am 3. Oktober 1979 bezogen. ("„Der IM nahm ‚Erwin‘ mit in die Stadt und erfuhr zur Person folgendes …“").

Die Bundesbeauftragte für die Stasiunterlagen, Marianne Birthler, erklärte hierzu, es gäbe in ihrem Haus keine Zweifel daran, dass der IM nach Aktenlage „nur Gregor Gysi gewesen sein“ könne.
Der ARD sagte sie, es gebe Erkenntnisse, dass Gysi „wissentlich und willentlich“ die Stasi unterrichtet habe. Klingenstein erklärte, er sei mit niemandem außer Gysi auf der Rückfahrt zusammen gewesen, der Text könne sich daher nur auf Gysi beziehen.

Die erfolglose Klage richtete sich ferner gegen die Freigabe von Protokollen, ausweislich derer DDR-Staatschef Erich Honecker Gysi über dessen Vater ausrichten ließ, dieser solle im Rahmen der „juristisch konsequente[n] Verteidigung“ Havemanns als dessen Rechtsanwalt „ein Vertrauensverhältnis zu Havemann her[zu]stellen mit dem Ziel, dass dieser seine Außenpropaganda einstellt“. Dem liegt ein Tonbandbericht in Ich-Form über ein Gespräch bei, das Gysi 1979 mit Havemann führte. („Ich schlug ihm noch einmal vor, jegliche Veröffentlichungen im Westen zu unterlassen und sich allein auf die DDR zu beschränken.“) Die zunächst mit seiner anwaltlichen Schweigepflicht begründete Berufung zog Gysi später zurück.

Öffentliche Stellungnahmen Gysis zur Causa im Jahr 2008: Gysi bestreitet nach wie vor, als IM tätig gewesen zu sein: Er sei erstmals 1980 von der Stasi wegen der Möglichkeit einer inoffiziellen Mitarbeit überprüft und 1986 abschließend „zur Aufklärung und Bekämpfung politischer Untergrundtätigkeit nicht geeignet“ befunden worden.
„Im September 1980 legte die Stasi einen Vorlauf an, um zu prüfen, ob ich als IM infrage käme. Wozu einen solchen Vorlauf im Jahr 1980, wenn ich angeblich 1979 bereits IM war?“
Eine „inhaltliche Weitergabe des Gesprächs mit Thomas Erwin, allerdings nicht an die Stasi, sondern an das ZK der SED“, schließt Gysi aber nicht mehr aus.
Er habe außerdem „erhebliche Verbesserungen für Havemann wie die Aufhebung des Hausarrestes oder die Verhinderung weiterer Anklagen erreicht“.

Havemanns Sohn Florian hat Gysi in der Angelegenheit ausdrücklich verteidigt. Am 28. Mai 2008 erklärte er in einem Interview: „Unabhängig von der Frage, ob Herr Gysi IM war, was ich nicht beurteilen kann, hat er im Sinne unseres Vaters gehandelt.“
Hingegen stellt Havemanns Frau Katja anhand der Stasi-Unterlagen Gysis Rolle ins andere Licht – und spricht dabei auch über ihre Gewissheit, dass er sich eindeutig hinter "IM Gregor" und "IM Notar" verbirgt.

Gysi hinterfragte die Glaubwürdigkeit der Akten: Die Bundesbeauftragte habe in einem anderen Fall erklärt, „dass sie die Diskrepanzen zwischen dem Akteninhalt und tatsächlichen Begebenheiten nicht untersuchen dürfe. Die Behörde sei auch nicht befugt, Unterlagen zu bewerten und auch nicht, Wahrheitsfeststellungen zu treffen.“

Debatte im Deutschen Bundestag zur Causa Gysi: Am 28. Mai 2008 befasste sich der Bundestag auf Verlangen von CDU/CSU und SPD in der Aktuellen Stunde mit dem „Bericht aus den Unterlagen der Bundesbeauftragten für die Stasi-Unterlagen, Marianne Birthler, über vertrauliche Gespräche, die Gregor Gysi 1979/1980 als DDR-Rechtsanwalt mit Mandanten geführt hat“. In der Debatte forderten Abgeordnete der CDU, SPD, Grüne und FDP sowohl Konsequenzen in Form einer Entschuldigung bei den Opfern als auch den Ämterverzicht Gysis.

Der Vorsitzende der Linksfraktion, Oskar Lafontaine forderte als Konsequenz aus den Äußerungen von Marianne Birthler deren Entlassung.

Birthler bekräftigte dagegen, dass die Aktenlage zweifelsfrei zeige, dass Gysi wissentlich und willentlich Informationen an die Stasi geliefert habe. Dies sei gemäß Stasi-Unterlagengesetz entscheidend, als Stasi-Spitzel zu gelten, „unabhängig davon, ob eine Verpflichtungserklärung existiere oder nicht“.

Gysi erwirkt Sendeverbot eines ZDF-Beitrags zu den Stasi-Vorwürfen: Gegen den entsprechenden ZDF-Beitrag setzte sich Gysi beim Landgericht Hamburg mit einer einstweiligen Verfügung auf Unterlassung und Gegendarstellung zur Wehr. Nachdem das Hamburger Landgericht in erster Instanz gegen Gysi entschied,
hob das Hanseatische Oberlandesgericht den Entscheid der Vorinstanz auf. Begründet wurde dies mit einer unzulässigen Verdachtsberichterstattung und unzureichenden Recherchen im Vorfeld. Gegen dieses Urteil wurden durch das ZDF Rechtsmittel eingelegt. Am 4. September 2009 fällte das Landgericht Hamburg im Hauptsacheverfahren das Urteil, das dem ZDF untersagt, durch die im „heute-journal“ vom 22. Mai 2008 erfolgte Berichterstattung den Verdacht zu erwecken, Gysi habe „wissentlich und willentlich an die Stasi berichtet“.
Damit hat das Landgericht Hamburg kein grundsätzliches Verbreitungsverbot im Hinblick auf die streitige Äußerung von Frau Birthler verhängt, sondern den Verbotstenor ausschließlich auf die konkrete Darstellungsform in der Sendung „heute-journal“ vom 22. Mai 2008 beschränkt.
Im Berufungsverfahren zum weitergehenden Antrag Gysis auf einstweilige Verfügung bestätigte das Oberlandesgericht Hamburg am 8. September 2009 sein Urteil, in welchem dem ZDF überhaupt verboten wird, die Äußerungen Birthlers bestätigend zu verbreiten.
In der darauf folgenden Berufungsverhandlung wurde im Urteil vom 23. März 2010 vom OLG Hamburg dieses Verbot bestätigt und eine Revision nicht zugelassen.
Eine Beschwerde des ZDF gegen die Nichtzulassung dieser Revision wurde am 20. September 2011 vom Bundesgerichtshof zurückgewiesen.

Ausweitung der Ermittlungen der Staatsanwaltschaft im Jahr 2013: Wegen neuer Hinweise hat die Staatsanwaltschaft Hamburg ihre Ermittlungen gegen Gysi ausgeweitet. Ermittelt wird wegen einer möglicherweise falschen eidesstattlichen Versicherung. Gysi hatte erklärt, „zu keinem Zeitpunkt über Mandanten oder sonst jemanden wissentlich und willentlich an die Staatssicherheit berichtet zu haben“.

Der Rechtsstreit um den ersten Film "Die Akte Gysi", der im Januar 2011 in der ARD ausgestrahlt wurde, endete mit einem Vergleich, in dem sich der NDR verpflichtete, ihn nicht mehr zu zeigen. Die NDR-Autoren Hans-Jürgen Börner und Silke König setzten ihre Recherchen fort und legen weitere Einzelheiten zu Gysis DDR-Vergangenheit vor – auch (u. a.) zu Gysis Rolle in Fällen von Rudolf Bahro, Robert Havemann, Thomas Klingenstein, Rolf Henrich – ihren zweiten Film "Gysi und die Stasi" strahlte die ARD im Dezember 2013 aus.

Justiz-Eklat 2015: Staatsanwalt verweigert Anklageerhebung gegen Gysi: Nach Recherchen von NDR, WDR und „Süddeutscher Zeitung“ erteilte der Hamburger Generalstaatsanwalt Lutz von Selle die Weisung, Gregor Gysi anzuklagen. Anklagepunkt war der Verdacht, die von ihm am 18. Januar 2011 abgegebene eidesstattliche Versicherung sei unwahr. Die Ermittlungen hatten Anfang 2013 begonnen. Ausgangspunkt waren Anzeigen Vera Lengsfelds und eines ehemaligen Richters. Der zuständige ermittelnde Staatsanwalt weigerte sich, Anklage zu erheben, weil kein hinreichender Tatverdacht vorliege und damit die Weisung unrechtmäßig sei. Die Hamburger Justizbehörde unter Justizsenator Till Steffen, bei der sich der Staatsanwalt beschwert hatte, hob die Weisung des Generalstaatsanwalts auf. Georg Mascolo und Hans Leyendecker von der "Süddeutschen Zeitung" und andere Journalisten bezeichneten die Vorgänge als in der Justizgeschichte einmaligen Eklat.

Einstellung des Verfahrens: Für eine Stasi-Mitarbeit Gysis keine eindeutig belastenden Beweise vorhanden: Im Juni 2016 wurde das Verfahren gegen Gysi von der Hamburger Staatsanwaltschaft eingestellt. Die eidesstattliche Erklärung, in der er die Mitarbeit bei der Stasi bestritt, ließ sich durch die Staatsanwaltschaft nicht widerlegen. Zwar konnten in den Unterlagen Hinweise auf eine mögliche Tätigkeit für die Stasi unter den Decknamen "Gregor" oder "Notar" gefunden werden, doch ließen diese unterschiedliche, teilweise widersprüchliche Interpretationen zu und auch durch Zeugenaussagen sowie die Aufzeichnungen des Generalbundesanwaltes wurde kein eindeutig belastendes Material gefunden.

Im Wahlkampf 2013 behauptete Gysi, in Deutschland gelte noch immer das Besatzungsstatut. So forderte Gysi im Interview mit dem Deutschlandfunk ein Ende der Besatzung Deutschlands und die Aufhebung des Besatzungsstatuts, damit Deutschland endlich als Land souverän werden könne. Gysi wiederholte diese Forderungen in Interviews mit dem Tagesspiegel, bei Phoenix und bei TV Berlin. Im Jahr 2015 antwortete er auf die Frage, ob Deutschland noch besetzt sei, mit „nein“ und äußerte, dass die Bundesrepublik Deutschland ein souveräner Staat sei, sich aber nicht so benehme; nahm in diesen Zusammenhängen aber nicht zum Besatzungsstatut Stellung.

Gysi kritisierte 2013 die Bundesregierung scharf, dass sie nichts dagegen unternehme, dass die Five Eyes in Deutschland Bürger, Politiker und Unternehmen ausspähten. Edward Snowden habe den Friedensnobelpreis verdient. Diese Rede im Bundestag wurde von der Universität Tübingen als „Rede des Jahres 2013“ ausgezeichnet.

Als Kind wurde Gregor Gysi zeitweise als Synchronsprecher eingesetzt. Seine Schwester Gabriele Gysi (* 1946) ist Schauspielerin. Sie kehrte bereits 1985 der DDR per Ausreiseantrag den Rücken und lebte seither in der Bundesrepublik.

Gysis erste Ehe endete mit einer Trennung Anfang der 1970er Jahre. In zweiter Ehe war Gysi seit 1996 mit der Rechtsanwältin und Politikerin Andrea Gysi verheiratet, von der er seit November 2010 getrennt lebte und 2013 geschieden wurde.
Er hat drei Kinder; einen Sohn aus erster Ehe, eine Tochter aus zweiter Ehe und einen Adoptivsohn.

Nachdem Gregor Gysi im Jahr 2004 bereits zwei Herzinfarkte erlitten hatte, musste er sich im November 2004 wegen eines Hirnaneurysmas einer Operation unterziehen. Infolge dieses Eingriffs erlitt er einen dritten Herzinfarkt.

Gysi bezeichnet sich als nicht gläubig und ist konfessionslos.

Gregor Gysi ist Mitglied des 1. FC Union Berlin.







</doc>
<doc id="1828" url="https://de.wikipedia.org/wiki?curid=1828" title="Gabi Zimmer">
Gabi Zimmer

Gabriele „Gabi“ Zimmer (* 7. Mai 1955 in Ost-Berlin) ist eine deutsche Politikerin (Die Linke). Von 2000 bis 2003 war sie Bundesvorsitzende der PDS, zuvor war sie bereits von 1990 bis 1998 Landesvorsitzende der PDS Thüringen und von 1999 bis 2000 PDS-Fraktionsvorsitzende im Thüringer Landtag. Sie ist seit 2004 Mitglied des Europäischen Parlamentes und wurde 2009 und 2014 wiedergewählt. Seit März 2012 ist sie zudem Vorsitzende der Fraktion der Vereinten Europäischen Linken/Nordische Grüne Linke.

Nach dem Abitur absolvierte Gabi Zimmer ab 1973 ein Studium in der Sektion "Theoretische und Angewandte Sprachwissenschaften" an der Karl-Marx-Universität Leipzig in der russischen und der französischen Sprache, welches sie 1977 als Diplom-Sprachmittlerin beendete. Danach begann sie eine Tätigkeit als Sachbearbeiterin beim VEB Jagd- und Sportwaffenwerk "Ernst Thälmann" in Suhl. Von 1981 bis 1987 war sie hier Redakteurin der betriebseigenen Zeitung und gehörte dann von 1987 bis 1989 der Parteileitung der SED dieses Betriebes an.

Gabi Zimmer ist konfessionslos, verheiratet und hat zwei Kinder.

Gabi Zimmer wurde 1981 Mitglied der SED. Im November 1989 wurde sie zur SED-Parteisekretärin ihres Betriebes gewählt. Im Februar 1990 wurde sie zur PDS-Bezirksvorsitzenden in Suhl gewählt und war von Juli 1990 bis Dezember 1998 PDS-Landesvorsitzende in Thüringen.

Von 1997 bis 2000 war sie stellvertretende Parteivorsitzende. Am 14. Oktober 2000 wurde sie als Nachfolgerin von Lothar Bisky Bundesvorsitzende der PDS. Nach dem Scheitern der Partei an der Fünf-Prozent-Hürde bei der Bundestagswahl 2002 und den anschließenden Flügelkämpfen zwischen Parteilinken und Parteirechten kündigte sie am 7. Mai 2003 auf einem Sonderparteitag der PDS an, nicht erneut zu kandidieren. Am 28. Juni 2003 wurde schließlich Lothar Bisky wieder zum Parteivorsitzenden gewählt.

Von 1990 bis 2004 war sie Mitglied des Thüringer Landtags. Sie war Spitzenkandidatin ihrer Partei bei den Landtagswahlen 1994 und 1999. Nach der Landtagswahl 1999 wurde sie Vorsitzende der PDS-Landtagsfraktion, gab dieses Amt aber nach ihrer Wahl zur PDS-Bundesvorsitzenden am 1. November 2000 an Werner Buse ab.

Bei den Europawahlen 2004, 2009 und 2014 wurde sie zum Mitglied des Europäischen Parlaments gewählt. Dort gehört sie der Fraktion GUE/NGL an und ist Stellvertretendes Mitglied im Ausschuss für Beschäftigung und soziale Angelegenheiten sowie Mitglied in der Konferenz der Präsidenten und in der Delegation in der Paritätischen Parlamentarischen Versammlung AKP-EU.
Seit März 2012 ist sie zudem Fraktionsvorsitzende.




</doc>
<doc id="1829" url="https://de.wikipedia.org/wiki?curid=1829" title="Greenpeace">
Greenpeace

Greenpeace [] ist eine 1971 von Friedensaktivisten in Vancouver, Kanada, gegründete transnationale politische Non-Profit-Organisation, die den Umweltschutz zum Thema hat. Sie wurde vor allem durch Kampagnen gegen Kernwaffentests und Aktionen gegen den Walfang bekannt. Später konzentrierte sich die Organisation darüber hinaus auf weitere Themen wie Überfischung, die globale Erwärmung, die Zerstörung von Urwäldern, die Atomenergie und die Gentechnik. Zudem weist Greenpeace auch auf Alternativen durch technische Innovationen hin.

Greenpeace hatte nach eigenen Angaben im Jahr 2015 weltweit rund drei Millionen Fördermitglieder und beschäftigte rund 2.400 Mitarbeiter. Greenpeace Deutschland hat mehr als 580.000 Fördermitglieder (2015). Es gibt in über 45 Ländern weltweit Greenpeace-Büros und 28 regionale Büros.

Die Organisation entstand Anfang der siebziger Jahre in Vancouver (Kanada) aus der Formation "Don’t Make a Wave Committee", die von US-amerikanischen und kanadischen Atomkraftgegnern und Pazifisten gegründet worden war. Dieses Komitee kam in der Absicht zusammen, eine Serie von Atombombentests zu verhindern.

Am 16. Oktober 1970 fand ein Benefizkonzert von Joni Mitchell, James Taylor und Phil Ochs im Pacific Coliseum in Vancouver statt, dessen Erlöse (Eintritt: 3 Dollar pro Person) einer kleinen Gruppe friedensbewegter Menschen zugutekam, die den Plan hatten, mit einem Schiff vor der Küste Alaskas gegen den anstehenden Atomtest auf Amchitka zu protestieren. Der Name der geplanten Aktion lautete "Greenpeace".

Irving Stowe organisierte das Konzert und wurde von Joan Baez unterstützt; Baez konnte damals nicht am Konzert teilnehmen, stellte aber die Verbindung zwischen Stowe und Mitchell her und Mitchell lud Ihren damaligen Freund James Taylor zum Konzert ein.

Das Konzert wurde 1970 auf Band aufgezeichnet und die Bänder von Familie Stowe verwahrt. Die Stowes hatten nie das Geld, aber immer die Hoffnung gehabt, das Konzert irgendwann veröffentlichen zu können. Somit konnte die Familie die erforderlichen Restaurierungen nicht selbst durchführen und wusste auch nicht, wie man sich die Rechte am Mitschnitt sichern könnte.

Das änderte sich 2006, als John Timmins, Bruder eines Cowboy-Junkies-Mitgliedes, als „Foundation Officer“ zu Greenpeace kam. Als Timmins von den Bändern erfuhr, besuchte er Barbara Stowe, Irvings Tochter, die ihm diese Geschichte erzählte. Timmins gelang es danach, Joni Mitchell und James Taylors Vertreter zu kontaktieren, die erforderliche Erlaubnis zur Restaurierung der Bänder zu erhalten und die Rechte zur Veröffentlichung der Aufnahmen zu sichern. Das Benefizkonzert wurde dann im November 2009 durch Greenpeace als CD und Download-Album mit dem Namen "Amchitka, the 1970 concert that launched Greenpeace" veröffentlicht und wird seither über eine eigene Website vertrieben.

Die Aktivisten des "Don’t Make a Wave Committee" charterten im September 1971 den von John Cormack befehligten Fischkutter „Phyllis Cormack“ mit der Absicht, den angesetzten zweiten Atomtest zu stören und die Zündung der Bomben zu verhindern. Das Schiff wurde in "Greenpeace" umbenannt und setzte die Segel in Richtung des Testgeländes nach Amchitka. Doch die US Coast Guard fing die "Phyllis Cormack" mit ihrem Küstenwachschiff "Confidence" ab und zwang sie, zum Hafen zurückzukehren.
Auf ihrer Rückkehr nach Alaska erfuhr die Mannschaft, dass in allen größeren Städten Kanadas Proteste stattgefunden und die USA den zweiten unterirdischen Test auf den November verschoben hatten. Die Versuche, mit einem zweiten gecharterten Schiff in die Testzone zu fahren, schlugen zwar fehl, dennoch fanden bei Amchitka keine weiteren Atomtests mehr statt.
Später änderte auch die Organisation ihren Namen in „Greenpeace“.

Im Mai 1972 veröffentlichte die neu gegründete "Greenpeace-Stiftung" einen Appell an verständnisvolle Kapitäne, um ihnen beim Protest gegen die Atomtests der französischen Regierung im Pazifik-Atoll Mururoa zu helfen. Eine Antwort kam hierbei von David McTaggart, einem Kanadier und früheren Unternehmer, der zu diesem Zeitpunkt in Neuseeland lebte.

McTaggart verkaufte seine Geschäftsinteressen und zog in den Südpazifik. Sein Handeln war eine Reaktion auf eine Gasexplosion, die einen Angestellten in einer seiner Skihütten ernsthaft verletzt hatte. Entrüstet darüber, dass jede Regierung ihn von jedem Teil des Pazifiks ausschließen könnte, stellte er aus dem Grund seine Jacht, die "Vega", zur Verfügung und machte sich daran, eine Mannschaft zusammenzustellen.

1973 fuhr McTaggart die "Vega" in die Ausschlusszone um Mururoa, damit sein Schiff von der französischen Marine gerammt wurde. Als er den Protest im Folgejahr wiederholte, bestiegen französische Seeleute die Vega und schlugen ihn zusammen.

Später veröffentlichte die Marine organisierte Fotos, wie sich McTaggart mit oberen Marineoffizieren eine Schlägerei liefert, und verlangte von den beiden gegnerischen Parteien mehr Zurückhaltung. In einem anderen Licht erschien der Sachverhalt, als in den Medien Fotos erschienen, die McTaggart während der Schlägerei zeigen, die das Mannschaftsmitglied Anne-Marie Horne aufnahm und aus der Jacht schmuggelte.

Die Kampagne zeigte Wirkung, als die französische Regierung eine Unterbrechung der oberirdischen Tests bekannt gab, wenngleich sie diese fortan unterirdisch durchführte. Auch in der Folgezeit veranstaltete Greenpeace Kampagnen gegen die Tests im Pazifik, bis die Franzosen ihr Testprogramm 1995 zu Ende brachten.

1975 gab es 15-20 verschiedene Gruppen mit dem Namen Greenpeace, erst später wurden sie in einer Organisation zusammengefasst, die am 14. Oktober 1979 unter dem Namen "Greenpeace International" gegründet wurde. Prominente Gründungsmitglieder waren unter anderem David McTaggart, Robert Hunter und Patrick Moore.

Auf Initiative McTaggarts wurde das Hauptquartier nach Europa verlegt, wo es gut organisierte Ableger gab, welche finanziell besser aufgestellt waren als die kanadische Organisation.

1985 sollte die "Rainbow Warrior" zum Mururoa-Atoll in Französisch-Polynesien fahren, um gegen die dort stattfindenden französischen Atomtests zu protestieren.

Im Hafen von Auckland vor Anker liegend wurde sie am 10. Juli 1985 durch Agenten des französischen Auslands-Nachrichtendienstes (DGSE) versenkt. Dabei ertrank der niederländisch-portugiesische Greenpeace-Fotograf Fernando Pereira.

Ein zentrales Element der Greenpeace-Arbeit ist die Aktion. Hierbei begeben sich Aktivisten an einen Ort, der ihrer Auffassung nach symbolisch für Umweltzerstörung steht und protestieren dort meistens mit Transparenten. Mittels oft spektakulärer Auftritte direkt am Ort des Geschehens versucht die Organisation, die Öffentlichkeit aufmerksam zu machen, um meist große Industriekonzerne oder Regierungen durch öffentlichen Druck zum Einlenken zu bewegen. Diese Art des Auftretens machte die Organisation in den achtziger Jahren bekannt.

Neben eher konventionellen Methoden von Umweltorganisationen wie Beeinflussung von Politikern und Anwesenheit bei internationalen Organisationen verfolgt Greenpeace noch die ausdrückliche Methodik direkter Aktionen ohne Gewaltanwendung.

Die Methode, die Aufmerksamkeit der Öffentlichkeit auf sich zu lenken, wurde vom „Bearing Witness“ (Zeugnis ablegen) der Quäker abgeleitet. Dabei geht es Greenpeace nach eigenen Angaben darum, „Zeugnis abzulegen“ über Unrecht, das der Meinung der Organisation nach geschieht. So positionieren sich beispielsweise Mitglieder öffentlichkeitswirksam zwischen der Harpune der Walfänger und deren Beute oder dringen in Atomkraftwerke ein. Um ihre Standpunkte wissenschaftlich belegen beziehungsweise neue Standpunkte entwickeln zu können, beauftragt Greenpeace wie andere Umweltschutzorganisationen Wissenschaftler mit dem Anfertigen von Studien. Außerdem ist Greenpeace in vielen internationalen Gremien beratend tätig.

Von anderen Umweltschutzorganisationen grenzt sich Greenpeace unter anderem durch die Beschränkung auf bestimmte, meist weltweit verfolgte, öffentlichkeitswirksame Themengebiete ab wie zum Beispiel Atomkraft, Globale Erwärmung, Biodiversität und Artenschutz, Grüne Gentechnik, Biopatente und Chemie. Bereits seit längerem plädiert die Umweltschutzorganisation gegen den Import von Atomstrom. Eine weitere Kampagne der Organisation richtet sich beispielsweise gegen H&M. Der Modehersteller verwende zu viele chemische Zusätze in Kleidungstücken. Nach langem Hin und Her beugte sich H&M und sicherte Greenpeace die Verringerung der chemischen Zusätze zu. Themen wie Verkehr oder Hausmüll spielen höchstens eine untergeordnete Rolle in einigen Greenpeace-Länderbüros. Auch ist Greenpeace entgegen weitläufigen Annahmen keine Tierschutzorganisation.

Die Umwandlung von Greenpeace von einem losen Netzwerk hin zu einer weltweiten Organisation geht hauptsächlich auf Ideen von McTaggart zurück. Dieser fasste seine Vorstöße in einer Mitteilung von 1994 folgendermaßen zusammen:

Im Jahr 1995 erreichte Greenpeace durch die Besetzung des schwimmenden Öltanks Brent Spar, dass die Betreiberfirmen Shell und Exxon von der geplanten Versenkung im Nordatlantik Abstand nahmen und die Anlage statt dessen an Land entsorgen ließen. Die Kampagne führte zu einem Verbot der Versenkung von Ölplattformen im Nordatlantik. Im Zuge der Kampagne hatte Greenpeace grob falsche Schätzungen zur Menge der Ölrückstände auf der Plattform veröffentlicht. Die Organisation hat sich für die falschen Zahlen bei Shell und der Öffentlichkeit entschuldigt.

2001 organisierte Greenpeace eine Aktion gegen die US-Importe von brasilianischem Mahagoni-Holz im Wert von zehn Millionen US-Dollar, nachdem die brasilianische Regierung eine Wartefrist für Exporte von Mahagoni-Holz verhängt hatte. Am 12. April 2002 enterten zwei Vertreter von Greenpeace das Schiff und trugen das Mahagoni-Holz, um ein Transparent mit der Aufschrift „Präsident Bush, stoppen Sie die illegale Abholzung“ aufzuhängen. Die beiden Vertreter wurden zusammen mit vier anderen, die ihnen halfen, verhaftet. Nachdem sie sich schuldig bekannt und eine Ordnungsgeld gezahlt hatten, wurden sie zu einem Wochenende Gefängnis verurteilt.

Am 18. Juli 2003 verwendete das Justizministerium der USA den Vorfall dazu, die gesamte Organisation Greenpeace an sich unter das 1872 verabschiedete und relativ unbekannte „Sailormongering-Gesetz“ zu stellen, das 1890 zuletzt angewendet worden war. Die Berufung auf dieses Gesetz, was den Zweck hatte, gewaltlose Demonstranten kriminell einstufen und strafrechtlich verfolgen zu können, löste auf der ganzen Welt Proteste aus. Zu den Kritikern dieser strafrechtlichen Verfolgung gehörten Al Gore, Patrick Leahy, die National Association for the Advancement of Colored People, die "ACLU of Florida" und die "People For the American Way". Das Ministerium stellte dies später beim Bundesgericht in Miami am 14. November 2003 auf eine überarbeitete Anklageschrift um, indem es die Aussage, dass Greenpeace fälschlicherweise behauptet hätte, dass das Mahagoni-Holz auf dem betroffenen Schiff Schmuggelware sei, fallen ließ.

Am 16. Mai 2004 entschied der zuständige Richter Adalberto Jordan zugunsten von Greenpeace und kam zu dem Ergebnis, dass „die Anklage eine seltene – und wohl auch beispiellose – strafrechtliche Verfolgung einer rechtlich legalen Gruppe“ darstellt, deren Verhalten zur freien Meinungsäußerung gehöre.

In den letzten Jahren haben es sich Greenpeace-Aktivisten zur Aufgabe gemacht, Hersteller der IT-Industrie durch Kampagnen zu umweltbewussterem Handeln im Sinne von Green IT zu bewegen.

Zum Beispiel blockierten 2004 niederländische Greenpeace-Aktivisten in diesem Zusammenhang das Utrechter Büro der Firma Hewlett-Packard und im Mai 2005 wurde vor der Genfer Zentrale eine LKW-Ladung Elektronikschrott abgeworfen. Im Dezember 2005 demonstrierten erneut Aktivisten vor dem Hauptquartier in Palo Alto, weil die Firma als führendes Unternehmen der Branche weitaus mehr gefährliche Stoffe bei der Produktion verwendet als die Konkurrenzunternehmen.

2006 rief Greenpeace im Internet im Rahmen einer Mitmachkampagne die Apple-Nutzer zur kreativen Beteiligung auf, um den Hersteller dazu zu bewegen, weniger giftige Chemikalien bei der Herstellung seiner Geräte zu verwenden. Ferner wurde hierbei das eingeschränkte Rücknahme- und Recyclingprogramm für Altgeräte von Apple kritisiert. Für diese Aktion wurde Greenpeace 2007 von der International Academy of Digital Arts and Sciences bei der 11. Verleihung des Webby Awards mit einem Preis in der Sparte „Aktivismus“ ausgezeichnet.

Zuletzt im Herbst 2014 machte Greenpeace durch eine Aktion mit einem Schaufelrad auf der SPD-Zentrale in Berlin auf die Kohlepolitik der Bundesregierung aufmerksam, verbunden mit der Botschaft an den Bundeswirtschaftsminister Sigmar Gabriel "„Kohle zerstört das Klima!“".

Anfang Mai 2016 gelangte Greenpeace in Besitz einer großen Menge Abschriften von geheimen Verhandlungsdokumenten zum geplanten Freihandelsabkommen zwischen den USA und der Europäischen Union. Die Dokumente wurden vom NDR und der Süddeutschen Zeitung verifiziert und veröffentlicht. Greenpeace stellte in den Folgetagen vor dem Brandenburger Tor in unmittelbarer Nähe zum Reichstagsgebäude in Berlin einen auf einen LKW aufgebockten gläsernen Lesesaal auf, in dem die Dokumente frei einsehbar waren. Nach einer repräsentativen Umfrage von „gut eintausend“ Bürgern durch ARD-Deutschlandtrend kurz nach den Veröffentlichungen äußerten 79 % der Befragten Zweifel am Erhalt des Verbraucherschutzes und an der Geheimniskrämerei der Verhandlungen. Kurz nach der Veröffentlichung äußerte der französische Präsident François Hollande, er werde ein Freihandelsabkommen „im derzeitigen Zustand“ ablehnen. Alle 28 EU-Mitgliedstaaten und das Europäische Parlament müssen dem Abkommen zustimmen.

Zu den Erfolgen der Organisation zählen unter anderem das Ende des kommerziellen Walfangs seit 2002, die Einrichtung eines Schutzgebietes in der Antarktis und der vorzeitige Stopp vieler Atombombentestreihen.

Zu den bedeutendsten Erfolgen, denen die Organisation ihrem Einfluss zuschreibt, gehören unter anderem die Einstellung von Atomtests auf Amchitka in Alaska (1972) und die Verlängerung des Antarktisvertrages (1991), der die Besitznahme der Antarktis durch andere Länder oder aus kommerziellen Interessen verbietet.

Um letzteres zu sichern, wurde in der Antarktis die "World Park Base" errichtet, die von 1987 bis 1991 in Betrieb war. Bereits 1983 war ein "Weltpark Antarktis" gefordert worden. 
Nach 6 Jahren Verhandlungen unterzeichnete Japan als letzter von 26 Vertragsstaaten das Protokoll des Umweltschutzabkommens, welches somit ab 14. Januar 1998 in Kraft trat.

In einem Patentstreit mit dem Neurobiologen Oliver Brüstle erwirkte Greenpeace 2011 durch den Europäischen Gerichtshof (EuGH) in Luxemburg ein Urteil, das das Patentieren menschlicher embryonaler Stammzellen verbietet. Der Bonner Forscher hatte 1997 ein Patent für nervliche Vorläuferzellen angemeldet, die er aus menschlichen embryonalen Stammzellen herstellte. Greenpeace machte ethische Bedenken gegen die Patente des Wissenschaftlers geltend. Das Bundespatentamt erklärte das Patent daraufhin für nichtig und verwies auf den Schutz der Menschenwürde und des menschlichen Lebens. Als nächste Instanz war der Bundesgerichtshof mit der Sache befasst. Dieser verwies die Frage an den EuGH.

Greenpeace Deutschland ist ein als gemeinnützig anerkannter, eingetragener Verein. Die Mitgliederversammlung, das oberste Beschlussgremium, besteht laut Satzung aus 40 stimmberechtigten Mitgliedern – zusammengesetzt aus 10 Mitarbeitern von Greenpeace Deutschland, 10 Mitarbeitern von ausländischen Greenpeace-Büros, 10 Personen aus dem öffentlichen Leben sowie 10 ehrenamtlichen Mitgliedern. Diese 40 Mitglieder werden nicht von den Fördermitgliedern gewählt, was Greenpeace schon den Vorwurf eingebracht hat, eine undemokratische Organisationsstruktur zu haben. Die Organisation verweist hingegen auf die höhere Effizienz, Schnelligkeit und Unabhängigkeit dieser Organisationsform. Bei basisdemokratischen Mitgliederversammlungen sei meistens nur ein Bruchteil der Wahlberechtigten anwesend, was es Splittergruppen oder Industrielobbyisten leicht machen würde, ihre Stimmmacht zu missbrauchen.

Die Handlungsfähigkeit der Organisation beruht im Wesentlichen auf den rund 589.000 Fördermitgliedern (Stand: 2014), die Greenpeace zur größten deutschen Umweltschutzorganisation machen. Als finanzstärkstes Länderbüro schultert Greenpeace Deutschland schon seit Jahren den Großteil der Ausgaben für internationale Greenpeace-Kampagnen. Ende Oktober 2013 bezog das Länderbüro Deutschland seinen neuen Sitz in der Hamburger HafenCity.

Im Jahr 2016 erhielt Greenpeace Deutschland Spenden in der Höhe von rund EUR 58,5 Mio. Im Vergleich wurden 2010 EUR 47,7 Mio. und 2012 EUR 53,5 Mio. eingenommen. 

Für Kampagnen wurden davon mit EUR 37,9 Mio. lediglich 65 % des Budgets ausgegeben. Der verbleibende Rest wurde in mittelbare und unmittelbare Spendenwerbung sowie die Verwaltung investiert, wobei alleine die unmittelbare Spendenwerbung (Werbekosten, Fördererbetreuung) ca. 15 % der Einnahmen kostete.

Der deutsche Kommunikationsdirektor der Umweltschutzorganisation Michael Pauli bestätigte, dass die Organisation insgesamt rund 90 Millionen Euro für Fundraising ausgebe - „um 300 Millionen Euro Einnahmen weltweit zu erhalten. Wir sehen das positiv.“

1980 in Bielefeld von William Parkinson, Gerhard Dunkel, Dirk Rehrmann u. a. gegründet, erlangte die heute in Hamburg ansässige Organisation in Deutschland eine große Bekanntheit mit ihren Protesten gegen die Dünnsäureverklappung in der Nordsee und gegen die Luftverschmutzung mit Dioxinen durch den Chemiekonzern Boehringer. Greenpeace erreichte, dass Boehringer sein Werk 1984 in Hamburg schließen musste. Ebenso gaben die Hersteller von Titandioxid ihr Einverständnis, die Dünnsäureverklappung zu beenden. 1995 konnte Greenpeace den Konzern Shell dazu bewegen, auf die Versenkung des schwimmenden Öltanks Brent Spar im Atlantik zu verzichten und dass im Jahr darauf international ein Versenkungsverbot für Ölplattformen im Nordatlantik festgeschrieben wurde.
Greenpeace hat im Laufe dieser Kampagne eine stark überhöhte Angabe zur Ölmenge an Bord des Tanks gemacht und dies später der Öffentlichkeit gegenüber auf einen Messfehler zurückgeführt.

Seit 1995 gibt es Jugendaktionsgruppen (JAG) der Greenpeace-Jugend (Alter 14 bis 19 Jahre). In rund 40 Städten sind über 700 Jugendliche auf diese Art und Weise aktiv und richten sich mit ihren Aktionen auch an die Öffentlichkeit und die Politik. Themenschwerpunkte sind dieselben wie die der Greenpeace-Mutterorganisation.
Für Kinder zwischen 10 und 14 Jahren gibt es bei Greenpeace die Möglichkeit so genannte "Greenteams" zu gründen. Hier können sich die Kinder – mit ein wenig Hilfe von Erwachsenen – auch schon für den Umweltschutz engagieren und eigene Aktionen gestalten.

Greenpeace arbeitet mit der Stralsunder Stiftung Deutsches Meeresmuseum zusammen. Die Organisation gestaltet eine Ausstellung im Ozeaneum Stralsund mit, in der unter dem Titel "1:1 Riesen der Meere" in einer 18 Meter hohen Halle lebensgroße Modelle verschiedener Walarten präsentiert werden. 

Ein weiteres wichtiges Standbein sind die über 100 Ortsgruppen mit ihren über 4.900 ehrenamtlichen Mitarbeitern. Über 4.000 Aktivitäten
sind 2014 mit Hilfe von Ehrenamtlichen in Deutschland unterstützt worden.

Neben dem Verein gibt es noch andere Organisationen mit dem Namen Greenpeace in Deutschland:
Die Greenpeace Stiftung ist verbunden mit dem Bergwaldprojekt, das 1987 durch Greenpeace Schweiz gegründet wurde, und betreut den Tierpark Arche Warder.

Außerdem gibt es das Greenpeace-Magazin, das von der Greenpeace Media GmbH herausgegeben wird und neben Umweltschutz ebenso soziale und wirtschaftliche Themen behandelt. Die Redaktion ist unabhängig vom Verein Greenpeace.

Im Laufe der Jahre versuchte die Organisation verstärkt, Alternativen aufzuzeigen und Antworten auf Umweltfragen vorzustellen:


Greenpeace arbeitet mit 27 nationalen und 15 regionalen Büros. Die Arbeiten der einzelnen Greenpeace-Sektionen sind untereinander koordiniert, die internationalen Kampagnen und Arbeitsgebiete werden unter Federführung von Greenpeace International entwickelt und für alle Länderbüros vorgeschlagen. Seit 1. April 2016 sind Jennifer Morgan und Bunny McDiarmid neue Geschäftsführerinnen von Greenpeace International.

Die internationale Greenpeace-Organisation ist die "Stiftung Greenpeace Council", eine Stiftung nach niederländischem Recht mit Sitz in Amsterdam. Sie ist unter der Nummer 41200415 der Handelskammer Amsterdam registriert. Die Greenpeace-Organisationen sind in den verschiedenen Ländern in unterschiedlichen Rechtsformen organisiert: In Deutschland als eingetragener Verein, in der Schweiz als eine Stiftung. Der Schweizer Ableger von Greenpeace wurde im November 1984 in Zürich gegründet. In Österreich gründeten Umweltschützer zwei Jahre zuvor die Vereinigung „Freunde von Greenpeace“.

Seit Greenpeace gegründet wurde, spielen Hochseeschiffe in den jeweiligen Kampagnen eine sehr große Rolle.

1978 stellte Greenpeace die "Rainbow Warrior" in Dienst, einen 40 Meter langen früheren Fisch-Trawler. Einer der ersten Einsätze der "Rainbow Warrior" wandte sich gegen isländischen Walfang. Zwischen 1978 und 1985 engagierten sich Mitglieder der Mannschaft direkt bei friedlichen Aktionen gegen das Abladen von giftigem und radioaktivem Müll in Ozeanen, gegen die Jagd auf die Kegelrobbe auf den Orkney-Inseln und gegen Atomtests im Pazifik.

1985 sollte "Rainbow Warrior" in den Gewässern um das Mururoa-Atoll demonstrieren, wo Frankreich gerade Atomtests durchführte. Bei der Versenkung des Schiffes (siehe Versenkung der Rainbow Warrior) mit zwei Bomben durch den französischen Geheimdienst, kam auch der Fotograf Fernando Pereira ums Leben.

1989 gab Greenpeace den Auftrag, ein Ersatzschiff zu beschaffen, das "Rainbow Warrior" genannt wurde. Es war das Flaggschiff der Greenpeace-Flotte, bis es am 16. August 2011 in Singapur der NGO Friendship übergeben wurde.

Am 4. Juli 2011 setzte sich durch den Stapellauf der "Rainbow Warrior" die Namenstradition fort. Das neue Schiff hat Platz für 32 Besatzungsmitglieder und einen Helipad. Vornehmlich als Segler konzipiert, hat es dennoch einen effizienten Dieselmotor mit Katalysator, was bei Schiffsmotoren unüblich ist. Der Preis für das werftneue Schiff betrug 23 Millionen Euro.

Weitere Schiffe, die sich im Besitz von Greenpeace befinden, sind die "Sirius" (seit 1981), die "Arctic Sunrise" (seit 1996), die "Esperanza" (seit 2002) und die "Beluga II" (seit 2004).

Im Jahre 1995 erregte das gecharterte Greenpeace-Schiff "Altair" erhebliches Aufsehen in den Medien, erst als es am 30. April die Tank- und Verladeplattform Brent Spar in der Nordsee besetzte, um deren Versenkung im Atlantik zu verhindern, und dann ein zweites Mal, als es am 25. Oktober im italienischen Brindisi von der Besatzung einer französischen Fregatte geentert und beschädigt wurde.

Am 19. September 2013 wurde die Arctic Sunrise von Beamten des russischen Grenzschutzes gestürmt. Greenpeace-Aktivisten hatten am Vortag versucht, die Ölplattform Priraslomnaja des russischen Staatskonzerns Gazprom in der Petschorasee zu besetzen.

Greenpeace wurde bei den Save The World Awards 2009, die in Zwentendorf/NÖ überreicht wurden, ein Preis für sein weltweites Engagement zum Schutz des Weltklimas verliehen.

Frank Zelko sieht vor dem Hintergrund eines auf Eigeninteressen ausgerichteten Wirtschaftssystems die globale Bürgerpolitik von Greenpeace, das heißt die Erzeugung ökologischer Sensibilität durch gewaltfreie direkte Aktion als das bedeutendste Vermächtnis der Organisation. 

Einige Kritiker stellen deren steuerliche Begünstigung in Frage, in den USA etwa die von ExxonMobil finanzierte und ausschließlich zur Beobachtung von Greenpeace gegründete "Public Interest Watch" (PIW). Im März 2006 bestätigte die Steuerbehörde IRS, dass Greenpeace USA zu Recht die Steuervorteile einer Non-Profit-Organisation besitzt.

In Deutschland gab es mehrere Versuche, die Gemeinnützigkeit abzuerkennen, die jedoch bislang nicht erfolgreich waren. Im Dezember 2004 untersuchte das Finanzamt Hamburg, ob Greenpeace gegen Gesetze verstoßen hat und dem Verein deshalb der Status der Gemeinnützigkeit für 2003 aberkannt werden soll, weil die Organisation sich auf ihrer Internetseite nicht von bestimmten rechtswidrigen Aktionen distanziert habe.

1989 wurde in Kanada das Steuerrecht für gemeinnützige Organisationen geändert. Greenpeace und andere Organisationen verloren dadurch den Status der Gemeinnützigkeit.

Am 6. Mai 2011 wurde die Berufung im Zuge eines abgewiesenen Antrags von Greenpeace of New Zealand Inc. auf Gemeinnützigkeit durch den High Court of New Zealand abgelehnt, da die Organisation durch ihr Lobbying zu politisch orientiert sei. In diesem Zusammenhang wurden auch potentiell illegale Aktivitäten durch Greenpeace als Begründung genannt.

Undemokratische Strukturen werden der Organisation immer wieder vorgeworfen. Im Gegensatz zu den meisten anderen großen Umweltorganisationen haben die Basis-Aktivisten und Förderer bei Greenpeace nur wenige bzw. keine Mitbestimmungsrechte, so spricht der Spiegel von einem „nicht eben demokratische[n] Verbandsaufbau“. Bereits in den 1980er Jahren spaltete sich die Organisation Robin Wood „unter anderem aus Protest gegen den als undemokratisch empfundenen ‚Öko-Multi‘ Greenpeace“ ab. Greenpeace argumentiert, dass eine international handlungsfähige Organisation nicht jede einzelne Entscheidung basisdemokratisch treffen könne und verweist auf die höhere Effizienz, Schnelligkeit und Unabhängigkeit seiner Organisationsform.

Der Umweltschützer Patrick Moore, Gründungsmitglied und ehemaliger Präsident von Greenpeace International, hat sich inzwischen von der Organisation abgewandt. Er wirft der Organisation ideologische Verblendung vor, die in einer rigorosen Protesthaltung gipfelte und bei der sich die Organisation weigere, Konsens in Bezug auf Ökologie zu schaffen. So sagte der heute für die Forstwirtschaft arbeitende Moore in einem Interview:

Zelko kommt zum Ergebnis, dass Greenpeace allenfalls vorgeworfen werden kann, sich auf wissenschaftliche Studien zu beziehen, die den eigenen Interessen am besten dienen. Dies gelte jedoch auch für den Kritiker Patrick Moore.

Ein konkretes Beispiel für den Vorwurf der Unwissenschaftlichkeit ist die Ablehnung des Goldenen Reis. Moore warf der Organisation im Zusammenhang mit deren Lobby-Tätigkeit gegen die Zulassung von Goldenem Reis die Mitschuld am Tod von Kindern in Entwicklungsländern und Verbrechen gegen die Menschlichkeit vor.
Laut Ingo Potrykus, dem Mitbegründer des Projekts "Goldener Reis" hätte Greenpeace Probleme, wenn die Menschen erkennen würden, dass Angst unbegründet ist und dass die Technologie zur Rettung von Menschenleben eingesetzt werden könnte.

Paul Watson war 1972 eines der ersten Mitglieder von Greenpeace und hatte die Organisation schon vor deren offizieller Gründung unterstützt. Im Jahre 1977 verließ Watson Greenpeace im Streit und gründete die Sea Shepherd Conservation Society. Ihm war die Organisation Greenpeace zu passiv und zu ineffizient.

Nach seinen Worten hat sich Greenpeace zur größten „Wohlfühlorganisation“ der Welt entwickelt. Er sagt, dass Menschen Greenpeace beitreten, um sich gut zu fühlen. Sie wollen sich als Teil der Lösung fühlen und nicht als Teil des Problems. Nach der Meinung von Paul Watson ist Greenpeace ein Geschäft. Dieses Geschäft verkaufe den Menschen ein gutes Gewissen.

Im Januar 2016 kritisierte Watson den Arktis-Experten Jon Burgwald und forderte dessen Rücktritt, nachdem dieser in einem Interview die indigene Robbenjagd als „ethisch“ und „nachhaltig“ bezeichnete.

Im Juni 2014 wurde bekannt, dass durch Devisentermingeschäfte eines Mitarbeiters der Greenpeace-Zentrale in Amsterdam zur Absicherung von Währungsschwankungen Verluste in Höhe von insgesamt 3,8 Millionen Euro entstanden. Das Geld sei ursprünglich für den Aufbau neuer Länderorganisationen gedacht gewesen, der Verlust sei laut einem Greenpeace-Sprecher „gravierend, aber nicht existenzbedrohend“, aktuelle Kampagnen seien nicht gefährdet. Greenpeace International hat nach eigenen Angaben allein 2012 rund 270 Millionen Euro eingenommen, die Gelder stammten wie auch hier, größten Teils aus Spenden.
Vorsatz und persönliche Motive konnten laut Greenpeace ausgeschlossen werden. Der mittlerweile entlassene Finanzmitarbeiter für den internationalen Bereich hatte offenbar eine Firma damit betraut, Organisations-Gelder anzulegen. Dass Makler auf dem Finanzspekulationsmarkt, mit fortwährenden globalen Schäden, mit Greenpeace-Geldern „zocken“, wird kritisiert und stattdessen mehr Geldanlage in ökologische Unternehmen gefordert. Nach dem Bekanntwerden der Millionenverluste hat Greenpeace in Österreich rund 200 und in Deutschland rund 700 Förderer verloren. Ein Sprecher von Greenpeace Österreich sagte es habe, gerade in dieser schwierigen Situation, auch viele zusätzliche Spenden gegeben - aus Solidarität.

Im Dezember 2014 platzierten Greenpeace-Aktivisten während der Weltklimakonferenz in Lima Stofftücher in unmittelbarer Nähe der Kolibri-Figur der Nazca-Linien, welche den Schriftzug „Time for change! The future is renewable!“ sowie das Greenpeace-Logo darstellten. Die peruanische Regierung verurteilte die Aktion an der empfindlichen archäologischen Stätte und ersuchte um die Festnahme der 20 Beteiligten, was jedoch von einem lokalen Gericht zurückgewiesen wurde. Nach Ana María Cogorno, der Vorsitzenden der Nazcalinien-Schutzorganisation Asociación María Reiche, sind die durch Greenpeace am UNESCO-Weltkulturerbe verursachten Schäden irreparabel.

Im April 2017 wurde in Großbritannien ein Bußgeld von 30.000 britischen Pfund (ca. 36.000 €) gegen Greenpeace verhängt, weil die Organisation sich vor der Unterhauswahl 2015 bewusst nicht als „drittbeteiligte Kampagnenorganisation“ registriert hatte. Damit ist Greenpeace die erste Organisation, die nach dem 2014 in Kraft getretenen britischen Lobbygesetz ("Transparency of Lobbying, Non-Party Campaigning and Trade Union Administration Act") sanktioniert wurde, welches die Einflussnahme von Dritten auf den Wahlausgang beschränken soll.

Ende Juni 2016 haben mehr als ein Drittel der weltweit lebenden Nobelpreisträger die Mainauer Deklaration unterzeichnet, in der Greenpeace in scharfen Worten dazu aufgerufen wird, die Ablehnung der grünen Gentechnik zu überdenken.

Manche kritisieren die Vorgehensweise der Organisation an sich, die sie für zu direkt und teilweise illegal halten: Gerade bei Protesten auf oder direkt neben dem Gelände von Unternehmen reagieren die Betreiber oft mit Anzeigen wegen Hausfriedensbruch und Nötigung und versuchen, einstweilige Verfügungen gegen den Verein durchzusetzen. Greenpeace-Aktivisten gehen das Risiko einer Verurteilung ein, weil sie durch ihr Auftreten die Aufmerksamkeit der Öffentlichkeit auf sich ziehen können, unter anderem, um deren Spendenbereitschaft zu erhöhen und sie damit höhere Rechtsgüter durchzusetzen hoffen, die ihrer Ansicht nach anders nicht durchzusetzen wären.

Der Verein versendet Newsletter zu aktuellen Kampagnen und Aktionen und viermal pro Jahr kostenfrei die Zeitschrift "Greenpeace Nachrichten" () an seine Förderer.





</doc>
<doc id="1830" url="https://de.wikipedia.org/wiki?curid=1830" title="Graz">
Graz

Graz ist die Landeshauptstadt der Steiermark und mit 289.440 Einwohnern (Stand: 1. Jänner 2018) die zweitgrößte Stadt der Republik Österreich. Die Stadt liegt an beiden Seiten der Mur im Grazer Becken. Die Metropolregion Graz ist mit 622.259 Einwohnern (Stand 2016) nach den Metropolregionen Wien und Linz die drittgrößte Metropolregion Österreichs. Der Großraum Graz war in den letzten zehn Jahren der am schnellsten wachsende Ballungsraum Österreichs.

Das Grazer Feld war in der römischen Kaiserzeit eine dicht besiedelte Agrarlandschaft. Im 6. Jahrhundert wurde hier eine Burg errichtet, von der sich der Name Graz ableitet (slowenisch "gradec" bedeutet "kleine Burg"). Das Stadtwappen erhielt Graz 1245, von 1379 bis 1619 war es habsburgische Residenzstadt und widerstand in diesem Zeitraum mehreren osmanischen Angriffen. 2003 war Graz die Kulturhauptstadt Europas; 2015 wurde es Reformationsstadt Europas.

Graz hat sich zu einer Universitätsstadt mit insgesamt fast 60.000 Studierenden (Stand: 2. Jänner 2017) entwickelt. Sie wurde zur Menschenrechtsstadt erkoren und ist Trägerin des Europapreises. Die Altstadt von Graz und das Schloss Eggenberg gehören seit 1999 bzw. 2010 zum UNESCO-Weltkulturerbe. Graz ist Bischofssitz der Diözese Graz-Seckau. Seit März 2011 ist Graz als "UNESCO City of Design" Teil des Creative Cities Network.

Graz liegt rund 150 km südwestlich von Wien, an beiden Seiten der Mur, wo diese ihren Durchbruch durch das Grazer Bergland beendet und in das Grazer Becken eintritt. Die Stadt füllt den nördlichen Teil des Grazer Beckens von Westen bis Osten fast vollständig aus und ist an drei Seiten von Bergen umschlossen, die das bebaute Stadtgebiet um bis zu 400 m überragen. Nach Süden öffnet sich das Stadtgebiet ins Grazer Feld.

Der höchste Punkt in Graz ist der Plabutsch mit 754 m im Nordwesten der Stadt, die tiefste Stelle, mit etwa 330 m, befindet sich dort, wo die Mur die Stadt im Süden verlässt. Innerhalb von Graz gibt es zwei markante Erhebungen, den Schloßberg und den Austein mit dem Kalvarienberg.

Die nächste Stadt mit überregionaler Bedeutung ist Marburg an der Drau (Maribor) in Slowenien, rund 60 km südlich von Graz gelegen. Die beiden Städte verbinden immer enger werdende kulturelle und wirtschaftliche Beziehungen. Die Europaregion Graz-Maribor ist ein Beispiel dafür.

Das Stadtgebiet von Graz ist in 17 Stadtbezirke gegliedert. Rund um den ersten Bezirk, die Innere Stadt (I.), reihen sich St. Leonhard (II.), Geidorf (III.), Lend (IV.), Gries (V.) und Jakomini (VI.). Abgesehen von der Altstadt sind fünf der sechs Kernbezirke historisch gewachsen.

Innerstädtische Zentren, von denen das Wachstum ausging, waren der Murplatz, heute Südtiroler Platz, in der ehemaligen Murvorstadt, der Jakominiplatz, die ehemals eigenständige Vorstadt Geidorf, die sich um die Leechkirche gebildet hatte und der Guntarn-Hof, ein historischer Hof auf dem Areal der Leonhardkirche, der als zweites Grazer Siedlungsgebiet gilt. In der Murvorstadt entwickelten sich die Bezirke Gries um den Griesplatz und Lend um den Lendplatz.

Die restlichen Bezirke bilden den äußeren Ring von Graz: Liebenau, St. Peter, Waltendorf, Ries, Mariatrost, Andritz, Gösting, Eggenberg, Wetzelsdorf, Straßgang und Puntigam. Der gesamte äußere Ring der Vorstadtgemeinden wurde 1938 in Stadtbezirke umgewandelt, es kam zur Bildung von „Groß-Graz“ und zu einem dadurch bedingten Bevölkerungsanstieg; die Abspaltung Puntigams von Straßgang erfolgte 1988.

Graz ist in 28 Katastralgemeinden aufgeteilt: Die Bezirke Innere Stadt, St. Leonhard, Geidorf, Lend, Gries, Jakomini, Wetzelsdorf, Gösting sowie Waltendorf bilden je eine Katastralgemeinde.

Einige äußere Bezirke bestehen aus Katastralgemeinden, die mit dem jeweiligen Bezirk nicht deckungsgleich sind. Diese sind Engelsdorf, Messendorf (teilweise), Thondorf, Liebenau, Murfeld und Neudorf in Liebenau; Stifting und Ragnitz in Ries; Wenisbuch und Fölling in Mariatrost; Andritz, St. Veit ob Graz und Weinitzen in Andritz; Algersdorf und Baierdorf in Eggenberg; Straßgang (teilweise) und Webling (teilweise) in Straßgang; sowie Gries (teilweise), Rudersdorf, Straßgang (teilweise) und Webling (teilweise) in Puntigam.

Bei einigen Katastralgemeindenamen ist die alte, dörfliche Struktur erhalten geblieben. Drei Beispiele: Algersdorf war ein eigenständiges Dorf außerhalb des Stadtgebietes, wie Thondorf im heutigen Liebenau oder Wenisbuch in Mariatrost. Andere Dörfer und Ortschaften wie St. Johann oder Kroisbach im Bezirk Mariatrost, die vor der Eingemeindung einen geschlossenen Siedlungskern bildeten, werden nicht als Katastralgemeinden geführt.

Graz ist umgeben vom politischen Bezirk Graz Umgebung, in dem sich alle benachbarten Gemeinden befinden:

Graz liegt im Bereich der illyrischen Klimazone. Durch die Lage am südöstlichen Alpenrand ergibt sich eine gute Abschirmung gegenüber den in Mitteleuropa vorherrschenden Westwetterlagen. Größere Niederschlagsmengen dringen daher vorwiegend aus dem mediterranen Bereich ein.

Die Durchschnittstemperaturen betragen am Flughafen Graz-Thalerhof 8,7 °C und bei der Universität Graz 9,4 °C. Der mittlere Jahresniederschlag ergibt bei durchschnittlich 92,1 Niederschlagstagen (Messpunkt Universität Graz) eine Gesamtmenge von 818,9 mm.

Durch die geschützte Lage herrscht mildes Klima, so dass in den Parkanlagen und auf dem Schloßberg Pflanzenarten gedeihen, die üblicherweise erst in Südeuropa anzutreffen sind. Der mediterrane Einfluss zeigt sich in über 2100 jährlichen Sonnenstunden sowie einer durchschnittlichen Julitemperatur von 21 °C im zehnjährigen Mittel. Die Beckenlage hat vor allem in den Wintermonaten klimatische Nachteile: Im Winter entsteht gelegentlich eine Inversionswetterlage, die einen Luftaustausch im Grazer Becken erschwert und zu Überschreitungen des Grenzwerts für Feinstaub führen kann.

Das Gebirgsland nördlich von Graz auf beiden Seiten des engen Murtals besteht aus Kalk, Kalifeldspat und Schiefer aus dem Paläozoikum.

Das Stadtgebiet von Graz wird von der Mur durchflossen. Neben diesem gibt es noch eine Reihe von Fließgewässern. Siehe Liste der Fließgewässer in Graz.

Die Bevölkerungszahl von Graz überschritt etwa im Jahr 1870, in der sogenannten Gründerzeit, die 100.000er-Marke. In der Folge stieg die Einwohnerzahl bis in die 1970er Jahre stetig an – teilweise durch natürlichen Zuwachs und Zuwanderung, teilweise durch Eingemeindung von Nachbarorten im Jahre 1938 nach dem Anschluss Österreichs durch die Nationalsozialisten. Von Ende der 1970er Jahre bis 2001 verringerte sich die Zahl wieder, da viele Grazer in die Umlandgemeinden zogen.

Obwohl in diesen Jahren die Zahl der Bewohner mit Hauptwohnsitz abnahm, gab es zeitgleich eine Zunahme an Bewohnern mit Zweitwohnsitz und seit 2001 wieder eine Zunahme von Einwohnern mit Hauptwohnsitz. Hinzu kommen in Graz wohnende jüngere Werktätige, die ihren Hauptwohnsitz bei den Eltern außerhalb von Graz haben. Das stellt Graz vor finanzielle Probleme, da die Stadt die Infrastruktur für alle in und um Graz wohnenden Menschen bereitstellen muss, vom Bund jedoch nur Geld für die Bewohner mit Hauptwohnsitz erhält. Andererseits profitieren der Wirtschaftsstandort und die Bauwirtschaft von den meist jüngeren Leuten, die in Graz ihren Zweitwohnsitz haben. Graz ist somit die am schnellsten wachsende Stadt Österreichs. Es gibt zirka 110.000 Haushalte mit einem Frauenanteil von 52 Prozent.

Am 1. Jänner 2017 hatten 286.686 Personen ihren Hauptwohnsitz in Graz. Inklusive Nebenwohnsitze, kommt man auf 320.587 Einwohner. Es gibt 428 gemeldete Obdachlose in der Stadt.

Am 1. Jänner 2015 waren:

223.071 der Menschen (83,56 %) mit Hauptwohnsitz in Graz sind österreichische Staatsbürger, 16.548 sonstige EU-Bürger (6,20 %) und 27.346 Nicht-EU-Bürger (10,24 %). Von den Nicht-EU-Bürgern stammen 2.525 aus Afrika, 1.096 aus Nord- oder Südamerika, 8.655 aus Asien, 68 aus Ozeanien, 1.510 aus Russland und insgesamt 10.610 aus Nicht-EU-Staaten Europas. 371 Grazer sind staatenlos.

Staaten, aus denen mindestens 500 Menschen in Graz leben: Bosnien und Herzegowina (5.575), Kroatien (4.912), Deutschland (5.372), Türkei (4.688), Rumänien (4.584), Ungarn (2.012), Russland (1.510), Slowenien (1.386), Italien (1.031), Nigeria (911), Ägypten (716), Mazedonien (675), Polen (639), Volksrepublik und Republik China (Taiwan) (615), Slowakei (612), Bulgarien (591), Serbien und Montenegro (550).

Graz ist eine Statutarstadt. Das bedeutet, dass die Gemeindeorganisation durch ein eigenes Landesgesetz ("Statut der Landeshauptstadt Graz" aus dem Jahr 1967) geregelt wird und die Gemeindeorgane (insbesondere der Magistrat) neben den üblichen Aufgaben einer Gemeinde die der Bezirksverwaltungsbehörde innehaben. Die Grazer Gemeinderatswahlen finden nicht gleichzeitig mit den steirischen Gemeinderatswahlen statt. Graz ist Sitz des Landtages Steiermark (im Landhaus), der Steiermärkischen Landesregierung und aller Landesbehörden, der Bezirkshauptmannschaft Graz-Umgebung, der Wirtschafts-, Landwirtschafts- und Arbeiterkammer Steiermark, der Landespolizeidirektion Steiermark, der Landesgerichte für ZRS und Strafsachen und des Oberlandesgerichtes für Steiermark und Kärnten, des Bundesfinanzgerichtes (Außenstelle Graz) und des Arbeitsmarktservices.

Ab den 1970er Jahren kam es innerhalb der Grazer Kommunalpolitik zu einigen Besonderheiten: Das in Graz traditionell politisch starke deutschnationale Lager, vertreten durch die FPÖ, erhielt überdurchschnittlich viele Wählerstimmen und stellte zwischen 1973 und 1983 mit Alexander Götz den Bürgermeister. Danach fiel die FPÖ auf wenige Mandate zurück. Zur gleichen Zeit war Graz die erste Großstadt in Österreich, in der die Grünen – als Alternative Liste Graz (ALG) – in den Gemeinderat einzogen (1983). Nachdem in der folgenden Legislaturperiode die Mehrheitsverhältnisse nicht eindeutig waren, teilten sich zwei Bürgermeister die Amtszeit, zuerst Franz Hasiba (ÖVP) und anschließend Alfred Stingl (SPÖ), der nach der Wahl 1988 bis 2003 Bürgermeister blieb. In der folgenden Amtsperiode wurde die KPÖ mit über 20 % der Stimmen drittstärkste politische Kraft. Dieser Erfolg wird dem sozialen Engagement des damaligen KPÖ-Spitzenkandidaten und Gemeinderates Ernest Kaltenegger zugeschrieben. Nachdem Kaltenegger für die KPÖ-Steiermark bei den Landtagswahlen (Oktober 2005) antrat und in den Landtag einziehen konnte, verließ er den Grazer Gemeinderat. Seine Nachfolgerin Elke Kahr setzte dieses Engagement fort.
Gemeinderatswahl 2008

Gemeinderatswahl 2012

Gemeinderatswahl 2017

Der Grazer Stadtrat umfasst sieben Mitglieder, die gemäß den Gemeinderatswahlergebnissen proportional auf die sechs vertretenen Parteien aufgeteilt werden. Aufgrund der geringen Anzahl der Stimmen gehen dabei die SPÖ und die NEOS leer aus.

Der erste gewählte Grazer Bürgermeister in der Nachkriegszeit war Eduard Speck (SPÖ), welcher sein Amt bis 1960 ausübte. Er wurde von seinem Parteikollegen Gustav Scherbaum (SPÖ) abgelöst, dieser regierte bis 1973. Nach dem Erstarken des rechtsnationalen Lagers unter der FPÖ wurde 1973 Alexander Götz Bürgermeister; er übte das Amt bis 1983 aus. Auf ihn folgten Franz Hasiba (ÖVP) und Alfred Stingl (SPÖ), welche sich zunächst eine fünfjährige Amtsperiode aufteilten, Stingl behielt das Bürgermeisteramt jedoch bis 2003. Auf ihn folgte Siegfried Nagl (ÖVP), der bei den Gemeinderatswahlen 2008 und 2012 nochmals als Bürgermeister bestätigt wurde.

Auf der Plattform graz.at können Behördenwege mit Hilfe von Online-Verfahren von zuhause erledigt werden. Bei einem Teil dieser Formulare, wie zum Beispiel dem Förderungsantrag auf Fernwärme oder auch dem Kirchenaustritt ist eine elektronische Signatur mit der Bürgerkarte notwendig.

Graz unterhält Städtepartnerschaften mit folgenden Städten:

Den meisten Städten, mit denen Städtepartnerschaften eingegangen werden, sind im Stadtgebiet einige Straßen- und Wegnamen gewidmet: Am häufigsten vertreten sind Alleebezeichnungen: Coventryallee, Dubrovnikallee, Montclairallee, Pécsallee sind Wege im Grazer Stadtpark, der Groningenplatz ist eine Freifläche im Bereich der Burgringkurve, die St.-Petersburg-Allee befindet sich im Augarten.

Die Darmstadtgasse und die Trondheimgasse sind Wohn- und Parallelstraßen im Bezirk Lend. Der Pula-Kai ist ein Murkai zwischen Augartenbrücke und Berta-von-Suttner-Friedensbrücke in Jakomini, der Marburger Kai in der Innenstadt, sowie die Marburger Straße beim ORF-Landesstudio. Die Triester Straße ist eine der Grazer Fernstraßen. Keine Entsprechung im Straßennetz haben die Städte Temeswar und Ljubljana.

Das Grazer Stadtwappen ähnelt dem steirischen Landeswappen und geht diesem in seiner Darstellung voraus. Es zeigt im grünen Feld einen aufrecht seitwärts schreitenden, silbernen, goldgewaffneten Panther. Er ist gekrönt mit einer goldenen, dreiblättrigen Laubkrone im Gegensatz zum steirischen Wappen, bei dem der Panther behörnt ist und der Schild den Herzogshut der Steiermark trägt. Aus den Leibesöffnungen schlagen rote Flammenzungen. Das Stadtwappen ist jenem der oberösterreichischen Stadt Steyr ähnlich.

Graz ist eine Stadt mit langer Zeitungstradition. Gab es zu Zeit Kaiser Josefs II. fünf Zeitungen in der Stadt, reduzierte sich deren Anzahl unter Erzherzog Johann auf eine, die
„Grätzer Zeitung“, die einer strengen Kontrolle unterlag. Ein wichtiger Grazer Medienkonzern ist die Styria, die neben der regionalen „Kleinen Zeitung“, mit einer Reichweite von 50 % in den Bundesländern Steiermark und Kärnten, die „Presse“ und andere Zeitschriften und Zeitungen in Österreich, Slowenien und Kroatien herausgibt. Historisch erschienen die „Neue Zeit“, eine sozialdemokratische Tageszeitung und die „Südost Tagespost“ eine Tageszeitung der steirischen ÖVP.

Jede österreichweit erscheinende Tageszeitung verfügt über eine eigene Redaktion in Graz. Die „Kronen Zeitung“ produziert in Graz eine eigene Steiermark-Ausgabe.

Kostenlos wöchentlich verteilt werden an die Wohnungstür: die „Woche“ und der „(Neue) Grazer“ und seit Mitte 2014 „(Kleine Zeitung) Wohin“ mit Veranstaltungsprogramm (als Nachfolger von „G7“ nun über Entnahmefächer an Straßenbahnhaltestellen). Monatlich erscheint „BIG“ (Bürgerinformation Graz der Regierenden der Stadt), „Grazer Stadtblatt“ der KPÖ, ein Gratisblatt der FPÖ und anderes. Die „Woche“ betreibt eine Online-Ausgabe mit Beiträgen von „Regionauten“ aus dem Publikum. Seit etwa 2015 gibt Styria auch die sich an Studenten richtende Gratis-Wochenzeitung "Futter," erhältlich an Straßenbahnhaltestellen-Säulen, heraus.

Seit Oktober 1995 erscheint monatlich die Straßenzeitung „Megaphon“ mit den Schwerpunkten Sozialpolitik und Integration im Straßenverkauf. Die Wochenzeitung „Falter“ betrieb von März 2005 bis etwa 2010 eine Steiermark-Redaktion.

In Graz gibt es, wie in jeder Landeshauptstadt, ein ORF-Landesstudio. Von dort aus wird das 24-Stunden-Vollprogramm Radio Steiermark gesendet. Die tagesaktuelle TV-Sendung "Steiermark heute" wird im Landesstudio produziert und im Vorabendprogramm auf ORF 2 ausgestrahlt. Bevor das Landesstudio in der Marburger Straße errichtet wurde, sendete der ORF sein Programm aus der Villa Ferry in der Zusertalgasse im Bezirk Geidorf. Mit Steiermark1 und MemaTV, einem Sender der nur gelegentlich Programm sendete, gab es bis 2013 in Graz zwei weitere TV-Anstalten.

Der erste Radiosender, der sich neben den ORF-Radios etablieren konnte, ist die Antenne Steiermark. Sie war 1995 der erste Privatsender Österreichs und hatte ihren Sitz am Sender Dobl bei Graz und ist Ende 2014 ins neue Styria-Gebäude in der Conrad-von-Hötzendorf-Straße übersiedelt. Mit Radio Graz, Radio Soundportal und dem freien Radio Helsinki können in Graz drei weitere Radiokanäle empfangen werden. Das österreichweit gesendete Radio KroneHit hat eine Zweigstelle in Graz. Am Grazer Hausberg, dem Schöckl, steht die gleichnamige Rundfunk-Sendeanlage, die Graz, die gesamte Südsteiermark, das Südburgenland und andere Gebiete mit Radio- und Fernsehprogrammen versorgt.

Der deutsche Reiseschriftsteller Johann Gottfried Seume gelangte auf seiner berühmten "Reise nach Syrakus im Jahre 1802" unter anderem nach Graz. Er schrieb:
Heute setzt Graz im Tourismus vor allem auf die historische Substanz der Altstadt und auf das südliche Flair. Die offiziellen Auszeichnungen der Stadt als Kulturhauptstadt 2003, als Weltkulturerbe unterstreichen diese Positionierung. 2012 erhob die Agrarmarkt Austria Marketing GesmbH Graz zur „Genuss-Hauptstadt“ Österreichs. Grund dafür war der „Grazer Krauthäuptel“, eine besondere Form des Kopfsalats, die Anfang des 20. Jahrhunderts aus Krain in die Steiermark gekommen war.

Große Bedeutung kommt Graz auch als Kongressstadt zu. Graz verfügt aktuell über rund 5000 Gästebetten im gewerblichen Bereich, dazu kommen weitere circa 1.000 Gästebetten im nicht gewerblichen Bereich (inkl. Camping, Jugendherbergen und Privatzimmer). Rund 50 % aller Nächtigungen sind dem Segment der Geschäftsreisenden zuzurechnen. Etwa 13 % entfallen auf das Segment Kongress- und Seminartourismus. 37 % entfallen auf das Segment des klassischen Städte- und Kulturtouristen. Dieses Segment weist in den letzten Jahren die stärkste Wachstumsrate auf. So wurden im Jahre 2014 mehr als 1 Million Nächtigungen verzeichnet. Das Krisenjahr 2009 konnte Graz mit rund 788.000 Nächtigungen abschließen, was gegenüber dem Jahr zuvor einen Rückgang von 1 % bedeutet. Die Gästestruktur weist einen hohen Anteil an Inlandsgästen (rund 47 %) aus, gleichzeitig einen stark internationalen Mix.

Die historische Grazer Altstadt und die Dachlandschaft wurden 1999 wegen ihres sehr guten Erhaltungszustandes und der Sichtbarkeit der baugeschichtlichen Entwicklung im Altstadtbild zum UNESCO-Weltkulturerbe erklärt und 2010 auf „Stadt Graz – Historisches Zentrum und Schloss Eggenberg“ erweitert. Diese Auszeichnung ist verbunden mit der Verpflichtung, das historische Erbe mit seinem seit der Gotik errichteten Bauensemble zu erhalten und neue Architektur harmonisch einzufügen. Die meisten Grazer Sehenswürdigkeiten befinden sich in der Altstadt. Diese erstreckt sich über den gesamten Bezirk Innere Stadt. Auch außerhalb der Altstadt gibt es viele historische Gebäude, vor allem in den Bezirken St. Leonhard (II.) und Geidorf (III.).

Im geographischen Zentrum der Stadt liegt der Schloßberg, der zwischen 1125 und 1809 als Festung diente. Nach der erfolgreichen Verteidigung der Anlage gegen die Truppen Napoleons unter dem Kommando von Genieoberst Franz Xaver Hackher zu Hart und dem Friedensschluss erfolgte die Sprengung der Schloßbergfestung. Die Grazer Bürger kauften den Uhrturm und den Glockenturm frei, so dass beide bis in die Gegenwart erhalten sind. Ab 1839 begann der Ausbau des kahlen Felsens zur Parkanlage. Neben den beiden Türmen sind noch einige Festungsreste und denkmalgeschützte Bauwerke aus der Zeit erhalten geblieben, unter ihnen die Reste der Thomaskapelle, Bürger- und Stallbastei und die Kasematten (ehemaliges Verlies). Zu erreichen ist das Schloßbergplateau mit seinen Denkmälern, von denen nicht alle freigelegt sind, über den Kriegssteig, die Schloßbergbahn, den Lift und über einige Fußwege. Im Berg selbst befinden sich einige Kilometer lange Stollensysteme, die im Zweiten Weltkrieg Schutzbunker bei Luft- und Bombenangriffen waren. In der Gegenwart wird ein Teil davon für Veranstaltungen („Dom im Berg“) oder als Märchenbahn („Die Grazer Märchenbahn“) genutzt. Das Montan- und Werksbahnmuseum im Berg ist für die Öffentlichkeit nicht zugänglich.

Der Schloßberg ist über ein Weg- und Straßennetz umrund- und erschließbar. Der Rundgang beginnt mit dem äußeren Paulustor in der gleichnamigen Paulustorgasse, dem einzigen erhaltenen Stadttor des Renaissance-Befestigungsgürtels, der von Domenico dell’Allio ausgeführt wurde. Das nächste Bauwerk stadteinwärts ist die Palmburg mit ihrer mächtigen Auffahrtsrampe. Unmittelbar daneben befindet sich das Volkskundemuseum und die unscheinbare Antoniuskirche. Die Bauzeit des Sakralbaus war zwischen 1600 und 1602, nachdem im Jahr 1600 über 10.000 protestantische Bücher an ihrem Standort verbrannt wurden. Dieser Bereich heißt Paulustorvorstadt.

Den Eingang zur Sporgasse, einem steil abfallenden und engen Gassenzug, dominiert das Palais Saurau mit seinem wuchtigen Portal und der Halbfigur eines Türken unterhalb der Dachkante. Es folgen das ehemalige Gasthaus „Zur goldenen Pastete“ mit seinem in Graz einmaligen Runderker, das ehemalige Augustinereremitenkloster und die Stiegenkirche, das Deutschritterordenshaus und einige repräsentative Bürgerhäuser mit Geschäftslokalen, bevor die Gasse in den Hauptplatz mündet. Die Stiegenkirche, die über einen Stiegenaufgang erreichbar ist, war Teil der Paulsburg, dem ältesten Teil der Grazer Stadtbefestigung.

Vom Hauptplatz führt die Sackstraße in Richtung Norden. Zu Beginn des Straßenzugs steht das „Hotel Erzherzog Johann“, danach folgen das Warenhaus Kastner & Öhler, die Landschaftsapotheke, die älteste Grazer Apotheke, das Gasthaus „Zum Roten Krebsen“, das Palais Kellersberg, das Witwenpalais und die Palais Attems, Herberstein und Khuenburg. Das Palais Attems mit seiner Prunkfassade ist „das bedeutendste Adelspalais der Steiermark“, an seiner Rückseite angebaut ist der Admonterhof. Das Palais Herberstein beherbergt das „Museum im Palais“; im Palais Khuenburg, dem Geburtsort des 1914 in Sarajewo ermordeten Habsburger Thronfolgers Franz Ferdinand, sind das Grazer Stadtmuseum und das Apothekermuseum untergebracht. Unmittelbar daneben steht der Reinerhof, das älteste urkundlich erwähnte Grazer Gebäude.

Vom Schloßbergplatz aus sind der Kriegssteig und der Schloßbergstollen, eine direkte Verbindung zum Karmeliterplatz, zu sehen. Der Platzanlage gegenüber liegt die Dreifaltigkeitskirche. Sie zählt zum Gebäudekomplex der Schulschwesternschule. Die Sackstraße mündet in den Kaiser-Franz-Josef-Kai', ein Straßenzug der anstelle einer abgebrochenen Häuserzeile das Murufer säumt. Die Grazer Sackstraße bestand ursprünglich aus drei „Säcken“, also abgeschlossenen Verbauungszonen. Mit der Zeit durchbrachen insgesamt drei, nicht mehr bestehende Sacktore die Mauern, um Wohnraum zu gewinnen. Dieses Gebiet galt lange Zeit als das am dichtesten besiedelte in Graz. Am Kai, der in die stark befahrene Wickenburggasse mündet, liegen die Talstation der Schloßbergbahn und Bürgerhäuser. Es sind noch Reste der alten Bastei des dritten Sacktores erkennbar.

Am Fuße des Schloßberges liegt die „Grazer Stadtkrone“. Sie besteht aus vier Monumentalbauten: dem gotischen Dom (Domkirche St. Ägidius), dem bedeutenden manieristischen Bau des Mausoleums mit integrierter Katharinenkirche aus dem 17. Jahrhundert, der alten Jesuiten-Universität und der Grazer Burg.

Der Grazer Dom ist seit 1786 Domkirche der Diözese Graz-Seckau und Pfarrkirche der Grazer Dompfarre. Der außen unscheinbar anmutende Sakralbau mit schlichtem Dachreiter diente von 1577 bis 1773 den Grazer Jesuiten als Ordenskirche. Er ist das kunst- und kulturhistorisch bedeutendste innerstädtische Sakralbauwerk, wurde unter Friedrich III. im 15. Jahrhundert errichtet und war Hofkirche der römisch-deutschen Kaiser. Der Dom war einst mit einem Verbindungsgang mit der Grazer Burg verbunden. Der Hauptaltar ist in seiner Gestaltung ein barockes Gesamtkunstwerk. In den beiden Seitenschiffen befinden sich die Brauttruhen der Paola Gonzaga aus dem Herrschergeschlecht von Mantua – geschaffen von Andrea Mantegna in der italienischen Frührenaissance. An der Außenwand des Doms ist ein Fresko, das sogenannte Landplagenbild, angebracht.

Unmittelbar neben dem Dom steht die Katharinenkirche mit dem Mausoleum, ein Gebäude im Stil des Manierismus. Es ist die Grabstätte von Kaiser Ferdinand II. (1578–1637) und der größte Mausoleumsbau der Habsburger. Zwischen Domkirche und Mausoleum steht auf einem Sockel eine Bronzeskulptur des Kirchenpatrons Ägidius, die nach der Vorlage des Grazer Künstlers Erwin Huber gestaltet wurde.

Graz war von 1379 bis 1619 Residenzstadt der Habsburger. Untrennbar mit der Grazer Geschichte verbunden ist der Jesuitenorden. Gegenüber der alten und ersten Grazer Universität bewohnten die Mönche den Domherrenhof. Die Grazer Burg mit dem Burggarten ist der Sitz der steirischen Landesregierung. Ihr Bau wurde 1438 unter Herzog Friedrich V. begonnen und unter Erzherzog Karl II. und seinem Sohn, Kaiser Ferdinand II., weitergeführt. Ein Relikt aus der ersten, gotischen Bauphase ist die Doppelwendeltreppe von 1499. In das Gebäude integriert ist das Burgtor, neben dem äußeren Paulustor das letzte erhaltene Stadttor von Graz.

Das Grazer Zentrum besteht in großen Teilen aus dem Hauptplatz, der Herrengasse, Färberplatz und Mehlplatz, Teilen der Burggasse und Bürgergasse, der Schmiedgasse, Raubergasse, Neutorgasse, dem Marburger Kai, Andreas-Hofer-Platz und dem Franziskanerviertel mit den jeweiligen Nebengassen.

Der Grazer Hauptplatz ist eine unregelmäßige und historisch gewachsene Platzanlage mit Marktfunktion, die sich früher nach Süden bis zur Landhausgasse erstreckte. Er wird von allen Straßenbahnlinien der Stadt frequentiert. An der Südseite steht das im neuklassischen Stil gehaltene Grazer Rathaus, zwischen 1889 und 1895 nach Plänen der Architekten Alexander Wielemans und Theodor Reuter über einem älteren Rathaus aus dem Jahr 1807 errichtet, das heute den Grazer Gemeinderat beherbergt. Zentral in der Mitte des Platzes dominiert der Erzherzog Johann-Brunnen, ein 1878 enthüllte Werk des Wiener Kunstgießers Franz Pönninger. Das überlebensgroße Bronze-Standbild des Erzherzogs Johann überragt einen imposanten metallernen Sockel, der an den Enden vier weibliche Statuen aufweist, welche die Flüsse Mur, Enns, Drau und Sann allegorisch repräsentieren. Den Hauptplatz säumen Bürgerhäuser und Stadtpalais: das Weißsche Haus, die Adler-Apotheke, die beiden stuckverzierten Luegg-Häuser an der Ecke zur Sporgasse, das Weikhard-Haus mit der gleichnamigen Standuhr und das Palais Stürgkh.

Zwischen dem Hauptplatz und dem zweiten zentralen Platz, dem Jakominiplatz, verläuft die Herrengasse, eine barocke Prunkstraße. Hier stehen das Landhaus mit seinem Renaissancearkadenhof, das Landeszeughaus mit der größten frühneuzeitlichen Waffensammlung der Welt, das sogenannte „Gemalte Haus“ und die Stadtpfarrkirche, die Grazer Hauptpfarrkirche. Bevor die Herrengasse in die Ringstraße mündet, durchquert sie den Platz Am Eisernen Tor mit seinem Brunnen und der Mariensäule. Das namensgebende Eiserne Tor war bis ins 19. Jahrhundert ein Stadttor des Renaissance-Befestigungsgürtels. Im südlichen Teil der Herrengasse befand sich, geographisch von Kaiserfeldgasse und Schmiedgasse begrenzt, bis 1439 das Grazer Judenghetto.

Von der Stadtkrone erreicht man das 1776 eröffnete Grazer Schauspielhaus am Freiheitsplatz und über Hofgasse und Bürgergasse das Gassensystem um den Glockenspielplatz mit dem Glockenspielhaus. Am Ende der Engen Gasse kann man via Stempfergasse, eine Einkaufsstraße, in die Herrengasse gelangen oder beim Bischofplatz das Bischöflichen Palais sehen.

Die Franziskanerkirche, die zweitgrößte Grazer Kirche, ist am Ostufer der Mur zu finden und Mittelpunkt des Franziskanerviertels. Wegen ihrer einstigen Insellage ist die erste Grazer Klosterniederlassung schräg gestellt. Der Verlauf der sogenannten „Kot(h)mur“, ein Abwasserkanal, trennte das Areal vom Rest der Innenstadt.

Vom Franziskanerviertel ist das Joanneumviertel, zwischen Raubergasse, Landhausgasse und Andreas-Hofer-Platz gelegen, erreichbar. Es besteht aus zwei Monumentalbauten und beherbergt das Haupthaus des größten steirischen Museums, dem Joanneum. Im neu gestalteten Innenhof befand sich einst der alte botanische Garten, der nach Geidorf ausweichen musste. Anstelle des neuen Joanneums und des Postamtsgebäudes von Friedrich Setz stand bis 1884 das abgebrochene Neutor. Das Magistratsgebäude der Stadt Graz dominiert das Straßenbild der Schmiedgasse. Am Südende des Marburger Kais steht das Oberlandesgericht Graz.

An wenigen oberirdischen Stellen ist die alte Stadtmauer sichtbar: die Glacisstraße und der Stadtpark erinnern namentlich und räumlich an die Freifläche vor der Stadtfestung; im Pfauengarten und im Stadtpark ist je ein Mauerrest erhalten geblieben. Die Anlage der Ringstraße folgt ungefähr dem Verlauf des ehemaligen Wassergrabens; an einigen Ecken ist aus der Luft die Lage der ehemaligen Basteien zu sehen. Der Grazer Stadtpark, der den Großteil des alten Glacis bedeckt, ist der größte innerstädtische Grünraum. Neben zahlreichen Denkmälern befinden sich in ihm das Forum Stadtpark, das Künstlerhaus - Halle für Kunst und Medien, der Musikpavillon, der Stadtparkbrunnen und einige Naturdenkmäler. Im Jahr 1869 begann die Anlage des Parks, 1873 eröffnete sie Bürgermeister Moritz Ritter von Franck. Außerhalb des Stadtparks und auf dem Gebiet der Innenstadt wurde das 1899 eröffnete Opernhaus Graz errichtet.

Die Beschreibung der Grazer Brücken und Stege beschränkt sich auf jene, die den Murfluss überqueren oder in einem Fall auch nur begleitet. (Zu anderen Gewässern siehe auch: Grazer Mühlgang.) Zwei Seilfähren, zuletzt 1958, sind Geschichte; Personenschifffahrt gab es nur von 1888 bis zum Zerbrechen des Dampfschiffs "Styria" (vormals "Kühbeck") 1889 an der Radetzkybrücke. Seit dem Kulturhauptstadtjahr 2003 ist eine bei niedrigem Wasserstand am Grund aufsitzende „Murinsel“, eigentlich ein Ponton-Steg, eine technische Kuriosität. Am linken Ufer etwa 70 m oberhalb der Keplerbrücke wurde Dezember 2016 der Pegel Graz mit einem interaktiven Bildschirm ausgestattet. Die Stadt veröffentlicht Pegelstände der Grazer Bäche insbesondere zur Beobachtung von Hochwassergefahr. Historisch und aktuell gab bzw. gibt es die Idee, entlang und über dem Fluss eine Seilbahn für Personentransport zu bauen.

An der nördlichen Gemeindegrenze zu Gratkorn überquert die A9 Pyhrnautobahn die Mur.
Etwa 2,5 Kilometer flussabwärts befindet sich die 1922 eröffnete und unter Denkmalschutz stehende, schmale Weinzöttlbrücke – aus Beton, mit einem Industrie-Anschlussgleis, ursprünglich mit Gasleuchten. Es folgen der Pongratz-Moore-Steg und die 1989/90 erbaute Kalvarienbrücke mit einem aufragenden, blauen Dreiecksrahmen aus Stahl zwischen den Tragwerken.

Rechtsufrig an der Kalvarienbrücke am Haus Kalvariengürtel 1 findet sich an der westseitigen Fassade die Putzmalerei "Floßlend um 1870" von Toni Hafner, darstellend ein Floß aus mit Ketten verzurrten Holzstämmen mit einem Seil zum Steg am Ufer, dahinter (flussaufwärts) eine frühere Holzbrücke aus zumindest 5 Bögen.

Linksufrig 700 m oberhalb der Kalvarienbrücke (nur wenig unterhalb der (rechtsufrigen) Kalvarienkirche) 1 m östlich des uferbegleitenden Rad-Gehwegs () weist eine Stele (zwischen hohen Büschen) auf die ehemals hier 1934–1958 betriebene hölzerne Seilfähre „Überfuhr“ hin. An der linken Uferböschung sind hier noch 6 Betonstufen mit 4 Eisenverankerungen als Relikte zu sehen, am gegenüberliegenden rechten Ufer weniger. Laut Beschriftung hat zuvor von „1864 bis zum Hochwasser 1873/74“ etwa 200 m unterhalb eine andere Überfuhr bestanden.

Die nordwestlich des Schlossbergs gelegene Ferdinandsbrücke, benannt nach Kaiser Ferdinand I., war die erste Kettenbrücke der Steiermark und die größte Österreichs, ab Herbst 1833 vom Pächter der Überfuhr, Franz Strohmeyer, erbaut nach den Plänen des Wiener Architekten Johann Jäckl, eingeweiht am 19. April 1836 von Fürstbischof Roman Sebastian Zängerle. Sie wurde 1920 in Keplerbrücke umbenannt, zu Ehren von Johannes Kepler, der hier sechs Jahre lebte und forschte. 1963 erfolgte ein Brückenneubau mit Stahlträgern, dessen unterwasserseitiger Gehsteig um 1993 für Radfahrer geöffnet und auf Kosten der Fahrbahn verbreitert wurde. 2006 wurde die stark genutzte Route für Rad- und Fußverkehr entlang des linken Ufers hier kreuzungsfrei durch das längste Grazer Brückenbauwerk unterführt und nach einer der ersten Grazer Radfahrerinnen Elise-Steininger-Steg benannt. Die Stahlträgerelemente sind mit Epoxidharz-Quarzsand rutschfest beschichtet, eines der Südrampe ist rollstuhlgerecht waagrecht, sie weisen jedoch bis zu 30 mm breite Dehnfugen auf. Die in der unübersichtlichen Nord-Kurve noch breiter geratene Fuge wurde durch ein rutschig glattes NiRo-Blech abgedeckt, später durch ein geriffeltes ersetzt, der stoßende Buckel bleibt. Das schmuck designte NiRo-Geländer mit senkrechte Streben weist einen Handlauf mit laserscharf geschnittenen Halterungen auf, die die Haut von Fingern zerschneiden.

Flussabwärts liegen die Murinsel. Die als Sehenswürdigkeit beliebte und teilweise überdachte „Insel“ ist eigentlich eine Pontonbrücke aus Stahl, die aus einem großen ovalen Schwimmkörper, der bei Niedrigwasser auf Kufen am Grund aufsitzt. Damit auch in diesem Fall die „Insel“ ganz von Wasser umgeben ist, wurde etwas unterhalb eine kleine Sohlschwelle quer über den ganzen Fluss errichtet. Vom rechten Ufer führt ein geschwungener Steg zum Bug des Schwimmkörpers, der – länglich-oval, in Flussmitte – oben an einem Stahlseil hängt, das im Fluss verankert ist und etwa jährlich von einem Taucher überprüft wird. Vom Heck führt ein gegenläufig geschwungener Steg zum linken Ufer. In Flussachse gesehen laufen die Stege bei Niedrigwasser V-förmig zur Flussmitte nach unten. Der Schwimmkörper hebt sich mit Hochwasser, im Extremfall höher als die Brückenköpfe der Stege an den Ufern. Der Querschub der Stege verdreht dabei die Insel etwas um die Hochachse. Die Stege sind per Treppen, links auch per Lift, von rechts auch per Rampe erreichbar und als Fußweg beschildert. Knapp danach – in der Verlängerung des Schlossbergplatzes – folgt der Erich-Edegger-Steg. Dieser Steg wurde 2003 nach dem Kommunalpolitiker und Kämpfer für sanfte Mobilität benannt.

Dann kommt die Erzherzog-Johann-Brücke. Die ehemalige Furt hier war die erste und über 400 Jahre einzige Grazer Murbrücke. 1843 errichtete die Stadt eine Kettenbrücke, die 1892 durch eine Eisenkonstruktion ersetzt wurde. 1918 wurde die Franz-Carl-Kettenbrücke in Hauptbrücke umbenannt. Ein schlichter Neubau der Hauptbrücke mit breiter Fahrbahn erfolgte 1964 noch mit der Absicht, eine Häuserzeile der engen Murgasse abzureißen und Kfz-Verkehr über den Hauptplatz zu führen. Die Figuren der Austria und Styria, die auf der Brücke standen, befinden sich im Stadtpark, Bronze-Verzierungen gelangten in Privathand und wurden um 2003 und 2014 wiederum der Stadt zum Kauf angeboten. Ein Bronze-Schmuckelement ist seit etwa 2003 am linken Ufer unter der Brücke ausgestellt, sowie eines aus Stein, das aus der Mur geborgen wurde. Hier wurde um 2002 eine Terrasse betoniert, von der unterwasserseitig eine markante Treppe bis nahe zum linken Brückenkopf hinaufführt, flussabwärts führt die als Gehweg mit Steinbänken ausgebaute Murpromenade. Die moderne Hauptbrücke wurde um 2006 generalsaniert: Die Geländer wurde aus NiRo-Stahl als sanft gewölbt profilierte Reling plus Drahtseilnetz ausgeführt, das mittlerweile voll von „Liebesschlössern“ hängt. Die Gehsteige wurden verbreitert, die Stufe des oberwasserseitigen auf 3 cm abgesenkt und die Straßenbahntrasse radfreundlicher entlang der südlichen Gehsteigkante parallel geführt. Erst um 2013 wurde die fast autofreie Brücke auf Erzherzog-Johann-Brücke umbenannt. An einer unterwasserseitig zwischen Mittelpfeiler und linkem Ufer durch Steinschlichtung erzeugten breiten Wasserwalze fand 2003 die Paddel-Rodeo-WM statt.

Seit Anfang 2018 schreitet der gemeinsame Bau des Murkraftwerks Graz in Puntigam und des Zentralen Speicherkanals (links) längs im Bett der Mur mit mehreren Speisekanälen quer unter der Mursohle etwa bis hinauf zur Tegetthoffbrücke voran. Während des Baus ist hier Bootfahren und das Betreten der Murufer jahrelang verboten, der Flusseinschnitt wird letztlich hinauf bis zur Erzherzog-Johann-Brücke zum Staubecken werden und damit von der Paddel- zur Ruderstrecke gewandelt.

Flussabwärts folgt die 1975 fertiggestellte Tegetthoffbrücke und die denkmalgeschützte Radetzkybrücke. Mit Unterstützung eines von der Gehradweg-(GRW)-Brücke abgehängten Seils wird seit etwa 2000 in der linken Flusshälfte bei passender Wasserführung in einer Walze sowohl auf Brettern gesurft als auch Playboating betrieben. Das 2017 baubegonnenes Murkraftwerk Puntigam wird diese Walze, wie auch die Stelle an der Erzherzog Johann-Brücke weiter oben stillstauen, schon Mitte Dezember 2017 wurde mit dem Errichten der Baustraße am linken Flussufer für den Zentralen Speicherkanal (ZSK) hinauf bis etwa zur Radetzkybrücke ein Bootsfahrverbot bis über das Bootshaus am Marburgerkai ausgedehnt, sodass das Weihnachtspaddeln des Kanu Clubs Graz (KCG) am 16. Dezember 2017 ab Kraftwerk Weinzödl erstmals schon vor der Erzherzog-Johann Brücke per Ausstieg am Murbeach endete.

Darunter folgt die Augartenbrücke als letzte bestehende Betonbogenbrücke und der besonders aufwendig konstruierte Augartensteg für Fuß- und Radverkehr, der zur Kosteneinsparung 2003 ohne das von den Architekten für das linke Ufer geplante Anschlussbauwerk errichtet wurde. Das Tragwerk wurde als Ganzes am rechten Ufer auf Rollen gelagert, durch Kranzug unterstützt vorne von einem Schreitwerk über den Fluss gebracht. Nach Auflagerung auf die Brückenköpfe wurden die vier etwas schräg gestellten Bögen mittels zwei über der Flussmitte und jeweils rechts und links etwas unter der Fahrbahn liegenden auf stählerne Druckstreben wirkenden Hydraulikzylindern, die zuletzt per Schweißnaht fixiert wurden, in sich verspannt. Erst um 2010 wurde dieser Steg um eine Schotterwegachse Richtung Osten durch den Park ergänzt, auf der sogar Radverkehr erlaubt wurde.

Nach dem auf den Augarten folgenden Augartenbad führt die erst (um) 1984 so benannte Berta-von-Suttner-Friedensbrücke über die Mur. Sie ist vierspurig, stark mit Kfz befahren und rechtsufrig mit einer Schleife und Unterführung kreuzungsfrei angeschlossen. Unterwasserseitig weist sie einen Geh- und Radweg auf, der sich am linken Ufer durch unfallträchtig schlechte Sichtbeziehung auszeichnet und am rechten Ufer durch mangelnden Radverkehrsanschluss flussaufwärts.

Nur eine Schrottplatzlänge (nahe dem linken Ufer) weiter und nach der Schneesturzstelle rechts quert die Eisenbahnbrücke der steirischen Ostbahn. Für Bootfahrer und Schwimmer – die Mur hat seit Jahrzehnten schon gute Badequalität – gefährlich im Bereich eines ehemaligen Mittelpfeilers aufragende Stahlprofile wurden um 2009 von der Feuerwehr entfernt.

Etwa 1 km flussabwärts führt der für die Öffentlichkeit gesperrte "Rohrsteg" der Steweag/Steg zwei isolierte Fernwärmerohre plus Hochspannungskabel vom 1963 in Betrieb gegangenen Fernheizkraftwerk (Puchstraße) die Mur.

Fast 1 km weiter und etwas südlich der Sturzgasse läuft der Puchsteg über den Fluss. Mit Trägern und Geländer aus Stahl, beides holzbeplankt, wurde er 1949 für Mitarbeiter des am rechten Ufer liegenden Puchwerks errichtet und um 2013 als Fußgänger- und Radfahrerbrücke saniert. Der Steg ist – Stand 2017 – etwa 75 m lang, die Mur hier bei Mittelwasser etwa 45 m breit, die Brückenköpfe werden durch in der Draufsicht U- bis trapezförmige Eisenbetonfundamentmauern und kurze Straßenrampen mit 10–15 % Steigung gebildet. Die zwei Pfeiler aus lackierten Stahlgitterstützen stehen im Wasser, sind fast zweidimensional-trapezförmig – aus Sicht der Wasseranströmung wenige Dezimeter schmal, aus Sicht der Stegachse oben so breit wie der Steg – etwa 2,5 m, unten im Wasser etwa 10 m breit. An der oberwasserseitigen, schrägen Stirnkante und einem Teil der Seiten tragen die Pfeiler Holzverplankungen als Schutz vor direktem Stoß und Verhaken von eventuellem Treibgut, Eis im Winter oder Bäume bei Hochwasser.

2017 wurden bis 15. März der Baumbewuchs beider Ufer auf einer Strecke um den Puchsteg gerodet um ein weiteres Murkraftwerk auf Höhe Olympiawiese zu errichten. Eine von etwa 14.000 Unterschreibenden geforderte Volksbefragung als Entscheid zur Kraftwerkserrichtung wurde nicht durchgeführt. Bis Dezember 2017 wurde am linken Ufer flussaufwärts bis etwa zur Radetzkybrücke die Baustraße für den Zentralen Speicherkanal im Wasser errichtet, dazu auch eine mit 90 t belastbare Brücke bis in Flussmitte etwas unterhalb der Radetzkybrücke.

Das Kraftwerk soll ab etwa 2021 einen weiteren Übergang für Fußgänger und Radfahrer bieten, am linken Ufer die Radroute jedoch auf einen Umweg legen. Gegenüber dem neuen, jungen Obmann Marcel Bloder des Grazer Kajakvereins KCG wurde seitens des Kraftwerksbetreibers Energie Steiermark eine Trainingsstrecke mit natürlichem Gefälle für verschiedene Arten von Wassersport unmittelbar links des Kraftwerks zugesichert – stundenweise und gegen Entgelt. Der oberhalb des geplanten Kraftwerkorts liegende Puchsteg liegt für eine geforderte Unterfahrbarkeit mit Feuerwehrbooten in Bezug auf die Stauhöhe zu niedrig, wird also in der derzeitigen Form nicht bestehen bleiben. Das Verschieben des Puchstegs um etwa 220 m nach Norden bis in die Flucht der Sturzgasse und sein Heben ist geplant, wodurch in Verbindung mit dem Murkraftwerk die flußquerenden Netzmaschen für Radverkehr in Zukunft in diesem Bereich etwas verdichtet werden.

Es folgt die Puntigamer Brücke, eine vierspurige Straßenbrücke, die 1995/96 neu errichtet wurde und mit dem Südgürtel (Baustart 2014, Eröffnung am 19. Mai 2017) noch mehr Autoverkehr tragen wird. Südseitig ist sie mit Geh- und Radweg auf Niveau der Fahrbahn ausgestattet, getrennt von dieser durch eine abgestufte, orange verflieste Betonmauer. Bruckstücke des Vorgängerbaus liegen flussabwärts der Brücke, wo wilderes Wasser, Schwälle und an einer Stelle etwas rechts der Flussmitte sogar ein Kehrwasser auftritt. (Stand 2017, mit einer Räumung und Eintiefung für das Unterwasser des kommenden Kraftwerks ist zu rechnen.)

Etwa 500 m südlich auf Höhe Auer-von-Welsbach-Gasse, an der ein Stadtgaswerk lag, führt der eiserne ebenfalls holzbeplankte Gasrohrsteg (errichtet 1951) mit zwei Pfeilern für Radfahrer und Fußgänger über den Fluss, linksufrig mit einer rechtwinkelig flussaufwärts abgeknickten Rampe an den Uferweg (GRW nur flussaufwärts) und die parallele Murfelderstraße angebunden. Am rechten Ufer endet hier die legale Möglichkeit zu Skaten, eigentlich wäre ab hier nur "Rollsteigen" am Rand der gehsteiglosen Fahrbahn erlaubt. Hier liegt in etwa die Stauwurzel des erst 2012 in Betrieb gegangene Murkraftwerks Gössendorf. Die bald folgende Autobahnbrücke liegt schon knapp südlich der Stadtgrenze in Gössendorf bzw. Feldkirchen bei Graz, genau 1 km weiter das genannte Kraftwerk.

Von den insgesamt elf Grazer Stadttoren sind zwei erhalten geblieben: das Burgtor als Abschnitt der Grazer Burg und das äußere Paulustor am Ende der Paulustorgasse. Während das Paulustor das einzig erhalten gebliebene Walltor des historischen Spätrenaissance-Befestigungsgürtels ist, der von Festungsbaumeister Domenico dell’Allio geplant worden war, ist das Burgtor weder der mittelalterlichen, noch der neuzeitlichen Stadtbefestigung zuzuordnen.

Von der mittelalterlichen Mauer ist nichts mehr erhalten. Die beiden Murtore in der Murgasse wurden 1837 abgetragen, 1846 folgte das innere Paulustor in der Sporgasse, die drei Sacktore in der Sackstraße sind seit dem 19. Jahrhundert nicht mehr vorhanden. Am Platz zum Eisernen Tor stand das gleichnamige Stadttor der Renaissance-Mauer. Es ähnelte wie das Neutor, 1884 als letztes der alten Tore abgebrochen, dem äußeren Paulustor. Am kürzesten bestand das Franzenstor an der Mündung der Burggasse in den Roseggergarten. Der 1835 errichtete Zierbogen wurde 1856 wieder entfernt. Die Abrisse der Grazer Stadttore wurden durch das erhöhte Verkehrsaufkommen und durch die gestiegene Bautätigkeit legitimiert. Nach der Schleifung der Festungswerke Mitte des 19. Jahrhunderts verloren sie ihre Schutzfunktion.

Bis zum Anschluss Österreichs im März 1938 wurde an den Grazer Stadttoren ein Pflasterzoll erhoben (diese Maut war im 19. Jahrhundert fast überall in Europa abgeschafft worden).

Das Schloss Eggenberg und der Schlosspark sind mit jährlich mehr als einer Million Besuchern die meistfrequentierte Sehenswürdigkeit außerhalb der Grazer Innenstadt. Es zählt seit 2010 mit der Altstadt zum UNESCO-Welterbe.

Das Schloss gilt als die bedeutende barocke Schlossanlage der Steiermark. Seine Geschichte reicht bis ins Mittelalter zurück. Ab 1625 wurde es im Auftrag Hans Ulrichs von Eggenberg (1568–1634) und unter der Leitung des Hofarchitekten Giovanni Pietro de Pomis zur repräsentativen Vierflügelanlage ausgebaut. Das Schloss war direkt über die "Eggenberger Allee" mit der Innenstadt verbunden; von der Prachtstraße sind nur mehr ein kleiner Abschnitt mit Alleebestand unmittelbar in Schlossnähe und ein Straßenname übrig geblieben.

Schloss Eggenberg ist nach einer kosmischen Zahlensymbolik ausgerichtet. Die vier Ecktürme stehen für die vier Himmelsrichtungen und die vier Elemente. Die Anlage besitzt 365 Außenfenster für die Tage eines Jahres. Im zweiten Stock, der Beletage, befinden sich 52 Außenfenster für die Wochen eines Jahres. Jedes Stockwerk im Haus hat genau 31 Räume für die maximale Anzahl der Tage eines Monats. Im zweiten Stock sind außen, ringförmig 24 Prunkräume angeordnet, die die Stunden eines Tages symbolisieren. Das Konzept soll an die Gregorianische Kalenderreform von 1582 erinnern.

Ein Zyklus von 24 Prunkräumen mit originaler Ausstattung aus dem 17. und 18. Jahrhundert gehört zu den bedeutendsten Ensembles historischer Innenräume Österreichs. Selten hat sich eine Raumausstattung von vergleichbarer Qualität so vollständig erhalten. Ihr Mittelpunkt ist der stuckverzierte Planetensaal mit dem Gemäldezyklus von Hans Adam Weissenkircher.

Der öffentlich zugängliche Schlosspark wurde zusammen mit dem Schloss konzipiert. Im Verlauf der Geschichte wechselte er häufig sein Aussehen. Er ist einer der wenigen historischen Gärten Österreichs, die unter Denkmalschutz stehen. An der Nordecke befindet sich der seit 2000 neu angelegte Planetengarten. Im Schlosspark leben frei laufende Pfauen.

In Graz gibt es, wie in den meisten Städten im katholisch geprägten Österreich, zahlreiche Sakralbauten. Zu den ältesten Kirchen der Stadt zählen die Leechkirche nahe der Universität Graz, deren Ursprünge in das Jahr 1202 zurückgehen, die Stiegenkirche als Teil der historischen Paulsburg in der Sporgasse und die Rupertikirche in Straßgang. Die höchsten Kirchen sind die im neugotischen Backsteinstil erbaute Herz Jesu Kirche und die Franziskanerkirche – deren Turmunterteil einst Teil der Grazer Stadtmauer war. Die Herz-Jesu-Kirche ist mit 109,6 m die dritthöchste Kirche Österreichs und das höchste Gebäude von Graz. Im selben Bezirk befindet sich gegenüber dem LKH Graz die 1361 erstmals urkundlich erwähnte Pfarrkirche St. Leonhard. An dieser Stelle stand mit dem Meierhof Guntarn im Mittelalter die erste Grazer Siedlung außerhalb der Innenstadt.

In Graz existieren zahlreiche Ordensniederlassungen, von denen viele im Zuge der Josephinischen Reformen von 1783 aufgelöst wurden. Es blieben hauptsächlich Klöster bestehen, die sich der Krankenpflege und der Bildung widmeten. In der Innenstadt hat sich seit dem 13. Jahrhundert ein Franziskanerkloster erhalten, am gegenüberliegenden Murufer liegt das Minoritenkloster mit der barocken Mariahilferkirche. In der Sackstraße, direkt gegenüber dem Schloßbergsteig, ist die Dreifaltigkeitskirche zu sehen, die bis 1900 die Kirche des ehemaligen Ursulinenkonvents war. In der Paulustorgasse steht neben dem Volkskundemuseum die Antoniuskirche.

Zu den größeren Anlagen auf Grazer Stadtgebiet zählen (in Auswahl) der Dominikanerkonvent in der Münzgrabenstraße, das Lazaristenkloster in der Mariengasse, unmittelbar daneben steht das Kloster der Barmherzigen Schwestern. Die Barmherzigen Brüder unterhalten in Graz zwei Krankenhäuser: eines in der Marschallgasse und eines in Eggenberg. Frauenorden, die sich der Krankenpflege widmen, sind die Elisabethinen im Bezirk Gries und die Kreuzschwestern mit Konvent und Privatklinik in Geidorf. Im Schulbetrieb tätig sind die Ursulinen in der Leonhardstraße, die Schulschwestern am Fuße des Schloßberges und in Eggenberg, sowie Schule und Kloster des Sacré Coeur Graz.

In den Grazer Kernbezirken St. Leonhard und Geidorf befinden sich die Leonhardkirche, die neugotische Herz-Jesu-Kirche, die Leechkirche, die Grabenkirche als Klosterkirche der Ordensniederlassung der Kapuziner, die Erlöserkirche am Gelände des LKH Graz und die Kirche Maria Schnee als Teil des Karmelitenklosters in der Grabenstraße. Neben dem Männerkloster steht das weibliche Pendant, das Karmelitinnenkloster mit der Kirche zum Hl. Josef, deren erstes Konvent am ehemaligen Fischplatz (heute Andreas-Hofer-Platz) 1782 aufgelöst und das Gebäude 1934 abgebrochen wurde. Neben der Kreuzschwesternkirche mit der Privatklinik ist in Geidorf mit der Salvatorkirche ein moderner Sakralbau erhalten.

Der Grazer Kalvarienberg befindet sich im vierten Bezirk Lend. Die Anlage auf dem Austein wurde im 16. Jahrhundert gegründet und von den Jesuiten verwaltet. Besonders sehenswert ist die hochbarocke Kalvarienbergkirche mit der "Heiligen Stiege" und der "Ecce-Homo-Bühne" und die zahlreichen Kapellen. Im selben Bezirk sind außerdem die Barmherzigenkirche und die Marienkirche in der Nähe des Grazer Hauptbahnhofs erwähnenswert.

In Gries stehen mit der Kirche St. Andrä, der Welschen Kirche am Griesplatz und der Bürgerspitalkirche bedeutende gotische und barocke Kirchenbauten. Neben der barocken Karlauerkirche und der Zentralfriedhofskirche im neugotischen Backsteinstil gibt es einige moderne Kirchenbauten: St. Lukas mit seiner ungewöhnlichen Innenausstattung, St. Johannes als Teil der Triestersiedlung, sowie Kirche und Pfarrzentrum Don Bosco, in deren Gebäude eine Mautstelle und ein Pulvermagazin untergebracht waren. Der Bezirk Jakomini ist vor allem durch den modernen Kirchenbau der Münzgrabenkirche und durch die Josefkirche geprägt.

Ein sakrales Zentrum in Graz ist der Außenbezirk Mariatrost. Die Basilika Mariatrost, eine überregional bekannte und barocke Wallfahrtskirche auf dem Purberg ist offiziell seit 1714 Ziel großer Pilgerströme. Verehrt wird eine wundertätige Marienstatue aus dem Stift Rein in Eisbach. Die Errichtung des Sakralbaus dauerte von 1714 bis 1779. Nach den Josephinischen Reformen wurden die Klostertrakte als Stallungen zweckentfremdet. Von der Mitte des 19. Jahrhunderts nahmen die Franziskaner die Wallfahrtstradition bis 1996 wieder auf. Im selben Jahr ernannte Papst Johannes Paul II. Mariatrost zur Basilica minor. 1968 kam es in der Wallfahrtskirche zur Veröffentlichung der Mariatroster Erklärung.

Neben der Basilika Mariatrost befinden sich zwei weitere Sakralbauten im Stadtbezirk: zum einen die Mariagrüner Kirche, die als bedeutendste kirchliche Stiftung eines Bürgers der Stadt Graz gilt. Louis Bonaparte, ein Bruder Napoleons, besuchte auf seinen Spaziergängen oftmals den Kirchenbau. 1873 heiratete der steirische Schriftsteller Peter Rosegger seine erste Frau in der Mariagrüner Kirche. Zum anderen die moderne Maria Verkündigungskirche in Kroisbach, die, in eine Wohnsiedlung integriert, 1974 geweiht wurde.

Im Stadtbezirk St. Peter steht weithin sichtbar die Pfarrkirche St. Peter. Im Nordosten von Graz kann man die Kirche St. Ulrich samt dazugehörigem Quellheiligtum, die Pfarrkirche St. Veit und die Kirche Heilige Familie in Andritz besichtigen. In zwei Stadtbezirken gibt es jeweils einen Sakralbau: St. Paul in der Eisteichsiedlung in Waltendorf und die Bruder-Klaus-Kirche nahe der Satelliten-Stadt am Berliner Ring in der Ragnitz.

Am rechten Murufer sind in den Bezirken Gösting und Eggenberg die Kirche St. Anna, die Schlosskirche des Schlosses Eggenberg, der Vierzehn-Nothelfer-Kirche, die moderne Schutzengelkirche, die Vinzenzkirche und auf einem Bergkamm in Wetzelsdorf die Kirche St. Johann und Paul zu sehen. Im südlichen Stadtteil Straßgang befinden sich mit der Kirche Maria im Elend, der Schlosskirche St. Martin, der etwas entlegenen Florianikirche auf dem Florianiberg, der Elisabethkirche und der erwähnten Rupertikirche weitere Grazer Sakralbauten. Im jüngsten Grazer Bezirk Puntigam gibt es die 1967 erbaute Pfarrkirche St. Leopold und die Anstaltskirche des LSF Graz.

Die Heilandskirche, in der Nähe des Grazer Opernhauses gelegen, ist die größte evangelische Kirche der Stadt Graz. Der heute bestehende Bau wurde im historistischen Stil ab 1853 errichtet, nachdem sich an dieser Stelle seit 1824 ein evangelisches Bethaus befunden hatte. Er ist Teil eines Gebäudekomplexes mit Pfarrgebäuden und dem Martin-Luther-Haus. Zur evangelischen Konfession zählen auch die Kreuzkirche am Rande des Volksgartens, deren Pfarrheim das Mühl-Schlössl ist, die Christuskirche in Eggenberg, die Evangelische Johanneskirche in Andritz und die Erlöserkirche in Liebenau.

Neben den katholischen und evangelischen Kirchenbauten findet man in Graz in der Kernstockgasse die altkatholische Kirche, in der Wiener Straße die koptische Kirche und über das Stadtgebiet verstreut Zentren diverser Konfessionen, darunter in Eggenberg ein Gemeindehaus der Kirche Jesu Christi der Heiligen der Letzten Tage (Mormonen).

Graz besitzt auch eine Synagoge. Die alte Grazer Synagoge wurde im Jahr 1892 errichtet und gehörte zur jüdischen Gemeinde mit ihren 2500 Mitgliedern. Sie war ein Nachfolgebau der Synagoge im ehemaligen Judenviertel in der Grazer Innenstadt. In der sogenannten Reichskristallnacht vom 9. zum 10. November 1938 wurde das Gebetshaus niedergebrannt und das gesamte Areal eingeebnet, um die Erinnerung an die Synagoge auszulöschen. Sämtliche Grazer Juden wurden nach Wien deportiert und Graz zur ersten „judenfreien“ Großstadt der Ostmark erklärt. Bis ins Jahr 1998 befand sich an der Stelle der zerstörten Synagoge nur eine Rasenfläche mit einem Gedenkstein. Unter Verwendung des alten Ziegelwerks wurde die neue Grazer Synagoge 1998 erbaut.

Am nördlichen Grazer Stadtrand befindet sich die Burgruine Gösting, eine Ruine mit sehr gutem Überblick, von der aus das Murtal nördlich von Graz einst kontrolliert wurde. An jener Stelle befand sich im 10. und 11. Jahrhundert eine Vorgängeranlage auf dem Frauenkogel, deren Reste wie die Burg denkmalgeschützt sind. Die Burg Gösting selbst ist im 12. Jahrhundert datiert und war Teil eines Kreidfeuer-Warnsystems, das die Bevölkerung vor Bedrohungen warnen sollte. 1723 wurde die Burg durch Blitzschlag zerstört, heute ist die Ruine ein beliebtes Ausflugsziel. Nach der Zerstörung erbauten die Grafen Attems als neuen Sitz das Barockschloss Gösting.

Der größte Jugendstilbaukomplex Österreichs liegt im Osten der Stadt, das LKH-Universitätsklinikum Graz. Es „galt damals als das modernste Krankenhaus des Kontinents und wurde vielfach sogar als Weltwunder bezeichnet“. Die relativ weite Entfernung vom Stadtzentrum erregte jedoch den Unmut der Grazer Bürgerschaft. Der Gebäudekomplex ist durch ein unterirdisches Tunnelsystem verbunden. Jede medizinische Abteilung besitzt ihr eigenes Gebäude. Im Laufe der Zeit wurden Modernisierungsmaßnahmen ausgeführt. In der Nähe des Krankenhauses findet man auch die Rettenbachklamm, eine ganzjährig begehbare Klamm im Stadtgebiet, das Naherholungsgebiet Leechwald, den künstlich angelegten Hilmteich mit einem Schlössl und die Hilmwarte.

Außerhalb der Innenstadt sind relativ wenige Palais vorhanden, die meisten sind Eigentum der öffentlichen Hand. In St. Leonhard steht am Stadtparkrand das Palais Kees, ein Bauwerk des Spätklassizismus. Es beherbergte unter anderem das k.u.k. Korpskommando. Seit einer Renovierung zu Beginn des 21. Jahrhunderts wird es als Studentenheim genutzt. Das Palais Meran, der ehemalige Stadtsitz Erzherzog Johanns, findet seit 1963 als Hauptgebäude der Grazer Kunstuniversität Verwendung. Die Elisabethstraße ist mit zahlreichen Palais und Villenanlagen eine der Grazer Prachtstraßen aus der Gründerzeit. Zu erwähnen sind die Palais Apfaltrern, Auersperg, Kottulinsky, Kübeck und Prokesch-Osten und die Meranhäuser. In den Räumlichkeiten des Palais Mayr-Melnhof ist das Grazer Literaturhaus untergebracht. Etwas stadtauswärts liegen (in Auswahl) die Villen Kollmann und Lazarini. Das Geidorfer Villenviertel erstreckt sich von der Elisabethstraße über die Leechgasse bis zur Schubertstraße.

Das Palais Thinnfeld im Bezirk Lend ist direkt das Grazer Kunsthaus angeschlossen. Eine Besonderheit am Palais Wertl von Wertlsberg ist sein schlossähnlicher Charakter, der durch zwei polygonale Ecktürme und einen Eck-Erker erzielt wird. Am Eingang zur Dominikanergasse steht mit dem Palais Gleispach das einzige Palais im Bezirk Gries. In allen anderen Grazer Stadtbezirken existieren keine Palais, sondern Schlösser und Edelhöfe.

Da Graz während der Monarchie ein beliebter Sitz von Adeligen und höheren Beamten war, findet man auf dem Stadtgebiet viele Schlösser und Palais. Neben den zahlreichen Innenstadtpalais sind es vor allem Schlösser und Edelhöfe die das Grazer Erscheinungsbild in den Randbereichen prägen. Neben dem Schloss Eggenberg mit seiner Parkanlage, dem Barockschloss Gösting und der Grazer Burg sind einige Bauten erwähnenswert. Inmitten von Geidorf steht das Meerscheinschlössl, ein barocker Bau mit ehemals weitläufigem Park. Das Hallerschloss und das Schloss Lustbühel mit integriertem Kindergarten liegen in Waltendorf. Im ehemaligen Schloss Liebenau war eine Kadettenschule untergebracht, seit den 1970er Jahren befindet sich in den Räumlichkeiten das HIB Liebenau.

Zu den innerstädtischen Schlössern zählen das Messe-Schlössl, das Metahof-Schlössl in Bahnhofsnähe, das Mühl-Schlössl beim Volksgarten oder das Tupay-Schlössl in der ehemaligen Schönau. In den Randbezirken stehen an prominenter Lage Schloss St. Martin und das St. Veiter Schlössl, etwas verborgen Schloss Algersdorf, Schloss Reinthal, Schloss Moosbrunn oder Schloss Kroisbach. Neben den Grazer Schlössern gibt es einige erhaltene Edelhöfe, also Ansitze, die häufig von Steuern befreit waren und als Gutsbetrieb geführt wurden. Ein Beispiel ist der Weisseneggerhof am Esperantoplatz. Die meisten Anlagen befinden sich in Privatbesitz.

Eine Besonderheit ist das ehemalige Jagdschloss Karlau. Es war in das ursprüngliche Gebiet der Karl-Au eingebettet und von einem Tiergarten und dem kaiserlichen Jagdgebiet umgeben. Im Tiergarten wurden neben Wassergeflügel und Rotwild auch Falken, Reiher und Fasanen gezüchtet, in den Mur-Auen ausgesetzt und bejagt. Noch heute erinnern etliche Straßennamen (Tiergartenweg, Rebhuhnweg, Reiherstadlgasse, Falkenturmgasse, Fasangartengasse, Auf der Tändelwiese) in der Umgebung an Tiergarten und Jagdgebiet. „Tändel“ zum Beispiel ist eine alte Bezeichnung für Rotwild. Im Laufe seiner Geschichte nutzte man das Schloss als Kriegsgefangenenhaus, ab 1769 als Arbeitshaus, bis 1803 die Einrichtung zum Provinzialstrafhaus erfolgte. Nach vielen Aus- und Umbauten ist vom Schloss nur mehr der Kern der Justizanstalt Graz-Karlau erhalten geblieben.

Graz besitzt eine Fülle an Denkmälern. Zu den prominentesten zählen (in Auswahl) die Mariensäulen (1666–1670) am Eisernen Tor, am Karlauplatz (1762) und am Marienplatz (1680), die Pestsäulen am Karmeliterplatz (1680), Lendplatz (1680) und Griesplatz (1680), die als Votivsäulen nach Pestepidemien oder Feindinvasionen von der Bürgerschaft gestiftet wurden. Dazu zählt auch das Pestdenkmal „Am Damm“, ein Denkmal in Kapellenform von 1680. Die verstärkte Errichtung solcher Pestsäulen und -denkmäler um das Jahr 1680 entsprang der Dankbarkeit wegen des Endes einer Pestepidemie in Graz, die mit über 3.500 Todesopfern ungefähr ein Fünftel der Stadtbevölkerung das Leben kostete.

Der Erzherzog-Johann-Brunnen (1878) am Hauptplatz mit einem überlebensgroßen Bronze-Standbild des Erzherzogs Johann und den allegorischen Darstellungen der vier Flüsse Mur, Enns, Drau und Sann wurde von Franz Pönninger entworfen und am 8. September 1878 enthüllt. An den vier Ecken sind Brunnenschalen eingefasst. Die Sockel sind mit allegorischen Bronzereliefs verziert. Ursprünglich sollte der Brunnen im Joanneumsgarten oder am Eisernen Tor aufgestellt werden. Das Erscheinungsbild einiger Grazer Plätze und Parks ist durch Brunnen geprägt. Das Major-Hackher-Denkmal („Hackher-Löwe“, 1909) am Schloßberg ist dem gleichnamigen Oberst gewidmet, der 1809 den Schloßberg erfolgreich gegen die Truppen Napoleons verteidigte. 1909 schuf Otto Jarl zum hundertjährigen Gedenken die erste Löwenplastik, die nach ihrer Einschmelzung 1943 erst 1966 durch eine Bronzeplastik Wilhelm Gössers ersetzt wurde.

Einige Plätze der Grazer Innenstadt sind mit exponierten Persönlichkeitsdenkmälern versehen. Das überlebensgroße Persönlichkeitsdenkmal Kaiser Franz I. (1838/41) steht am Freiheitsplatz, eine Bronzebüste Josephs II. (1887) am Opernring, das Persönlichkeitsdenkmal Peter Roseggers von Wilhelm Gösser und der Rosariumbrunnen befinden sich im Roseggergarten. Im Stadtpark, auf dem Schloßberg und in Opernnähe sind zahlreiche Denkmäler und Büsten aufgestellt, wie das Welden-Denkmal. In der Nähe des Stadtparkbrunnens (1873), der für die Wiener Weltausstellung gefertigt wurde, am "Platz der Menschenrechte" stehen die Bronzefiguren der Austria und Styria von Hans Brandstätter, die sich auf der ehemaligen Hauptbrücke (gegenwärtig: Erzherzog-Johann-Brücke) befanden. Ebenfalls im Stadtpark steht das Moritz-Ritter-von-Franck-Denkmal. Das Persönlichkeitsdenkmal des Admirals Wilhelm von Tegetthoff dominiert den Tegetthoffplatz, das Maria-Grüner-Denkmal, eine Säule mit bekrönender Terrakotta-Vase und Versen von Louis Bonaparte, Castelli und Anastasius Grün. befindet sich in unmittelbarer Nähe zur Mariagrüner Kirche.

Auf Teilen des ehemaligen Glacis wurde durch den Grazer Bürgermeister Moritz Ritter von Franck ein großer Park begründet, der heute den Stadtpark bildet. In der Nähe des Stadtparks liegt das Opernhaus Graz, das zweitgrößte Opernhaus Österreichs, welches Ende des 19. Jahrhunderts von dem Wiener Architekturbüro Büro Fellner & Helmer erbaut wurde. Direkt neben dem Opernhaus steht eine moderne Stahlskulptur, das „Lichtschwert“. Am Westufer der Mur befinden sich die modernen Gebäude, das Kunsthaus und dazwischen die Murinsel. Weiterhin sind die Hauptgebäude der Karl-Franzens-Universität, der Technischen Universität und der Universität für Musik und darstellende Kunst, sowie die Leechkirche, die älteste Kirche in Graz (1202), in den an die Altstadt angrenzenden Bezirken zu finden. Unweit der Karl-Franzens-Universität liegt der Botanische Garten.

Das Universalmuseum Joanneum in der Steiermark ist nicht nur das älteste und – nach dem Kunsthistorischen Museum in Wien – das zweitgrößte Museum Österreichs, sondern seiner Vielfalt und des Umfanges der Sammlungsbestände wegen auch das bedeutendste unter den österreichischen Landesmuseen. Namensgeber des Museums ist Erzherzog Johann, der im Jahr 1811 seine privaten Sammlungen stiftete mit dem Auftrag, „das Lernen zu erleichtern und die Wissbegierde zu reizen“. Der Erzherzog legte besonderes Gewicht auf Technik und Naturwissenschaften.

Die Idee zu einer naturwissenschaftlichen Lehranstalt stammte aus 1775 vom ehemaligen Jesuiten Leopold Biwald. Neben dem Unterhalt eines Lyzeums und den Ankauf des Lesliehofes, das fortan als Altes Joanneum bekannt ist, war die Gründung eines Landesarchives Primärziel von Erzherzog Johann. Seine Sammlertätigkeit ermöglichte dessen Eröffnung und der erste Joanneumsarchivar Josef Wartinger konnte eine erste "Kurzgefasste Geschichte der Steiermark" verfassen. Den Gründungsstatuten des Erzherzogs zufolge erfüllt das Universalmuseum Joanneum – gemäß der Idee des Sammelns, Forschens, Bewahrens und Vermittelns – nach wie vor die Aufgabe, ein umfassendes Bild der Entwicklungen von Natur, Geschichte, Kunst und Kultur in der Steiermark zu zeigen.
Die meisten der sechzehn Ausstellungsorte liegen in Graz: Die Alte Galerie im Schloss Eggenberg verfügt über bedeutende Bestände europäischer Kunst; von Romanik und Gotik über die deutsche und italienische Renaissance bis zu reich bestückten Kennerkabinetten des Barock. Die Neue Galerie umfasst bedeutende Sammlungen bildender Kunst des 19. und 20. Jahrhunderts und der Gegenwart. Sie ist seit 2011 im Joanneumsviertel untergebracht, das mit den Multimedialen Sammlungen sowie dem im März 2013 neu eröffneten Naturkundemuseum zwei weitere Museen beheimatet.

Schloss Eggenberg, die bedeutendste barocke Schlossanlage der Steiermark, zählt mit seiner erhaltenen Ausstattung, dem weitläufigen Landschaftsgarten sowie mit den im Schloss untergebrachten Sammlungen (Alte Galerie, Münzkabinett und Archäologiemuseum) zu den wertvollsten Kulturgütern Österreichs. Außerhalb des renovierten Joanneumviertels befindet sich das als „Friendly Alien“ bekannte Kunsthaus. Die Paulustorgasse beherbergt das Volkskundemuseum die älteste und umfangreichste volkskundliche Sammlung der Steiermark. Das Landeszeughaus in der Herrengasse ist ein Museum für Rüstungen und Waffen und mit zirka 32.000 Einzelstücken (im Originalzustand) die größte historische Sammlung der Welt.



Ursprünglich wurden die Friedhöfe rund um die Kirchen angelegt. Durch das Bevölkerungswachstum wurden ab dem 16. Jahrhundert auch außerhalb der Stadtmauern, in den Vorstädten, Friedhöfe angelegt. Kaiser Joseph II. erließ im Rahmen der Sanitätsreform 1782 ein generelles Verbot für innerstädtische Beisetzungen. In der Folge wurden die innerstädtischen Friedhöfe aufgelassen und neue außerhalb der Stadt angelegt. Die Grazer Friedhöfe sind alle im kirchlichen Besitz, ausgenommen der Urnenfriedhof, welcher der Stadt Graz (Grazer Bestattung) gehört.

Mit einem Alter von rund eintausend Jahren ist der an der südlichen Stadtgrenze gelegene Friedhof Feldkirchen bei Graz der älteste bestehende Friedhof, welcher von der „Grazer Bestattung“ zu den Grazer Friedhöfen gezählt wird. Er besitzt auch ein eigenes Beinhaus mit den Gebeinen von 1.767 Menschen aus Galizien und der Bukowina, welche 1936 nach Schleifung des Internierungslagers und des dazugehörenden Friedhofs im Bereich des heutigen Flughafens Graz-Thalerhof, hierher überführt worden sind.
Das Stadtbild der inneren 6 Stadtbezirke ist, wie für eine mitteleuropäische Stadt typisch, vor allem durch eher niedrige, gleichmäßige Verbauung sowie durch zahlreiche Sakralbauten geprägt. Die restlichen Bezirke von Graz sind geprägt von den Baustilen der zweiten Hälfte des 20. Jahrhunderts. Abgesehen von der Altstadt lässt sich an den vorherrschenden Baustilen der Stadtteile die Epoche, in denen sie ihren größten Ausbau und Bevölkerungszuwachs erfuhren, erkennen. So werden die direkt an die Altstadt angrenzenden inneren sechs Stadtbezirke vom Baustil der Gründerzeit, dem Historismus, geprägt. Ganze, bisher vorstädtisch geprägte Stadtviertel, wurden mit mehrgeschoßigen Zinshäusern, die reichen Fassadenstuck aufweisen, verbaut.

Für die neu entstandene Klasse der Großindustriellen entstanden auch mehrere vornehme Villenviertel. In der Zwischenkriegszeit war die Bautätigkeit aufgrund der schwierigen wirtschaftlichen Lage gedämpft. Trotzdem gelang es der Stadt Graz einige Wohnsiedlungen und öffentliche Gebäude zu bauen, ansonsten aber wurde in diesen Jahren wenig errichtet. Die stärkste Veränderung des Stadtbildes wurde in der Zeit zwischen 1950 und 1980 vollzogen, da die vielen kriegszerstörten Häuser oft mit Hochhäusern ersetzt wurden und zugleich die Wohnungsnot mit dem Bau von großen Hochhaussiedlungen in den Außenbezirken bekämpft wurde. Außerdem wurden auch weite Teile der Außenbezirke von Graz mit einem „Teppich“ aus Einfamilienhäusern verbaut. Das Bauerbe des Historismus wurde in der Nachkriegszeit als geschmacklos empfunden und bei vielen Häusern wurden die Stuckfassaden abgeschlagen, auch wenn sie den Krieg unbeschädigt überstanden hatten. Das geschah vor allem in jenen Stadtteilen, die der Bombenkrieg stark in Mitleidenschaft gezogen hatte.

In den Stadtteilen Geidorf und St. Leonhard, die den Bombenkrieg fast unbeschädigt überstanden hatten, gibt es noch ganze Viertel mit Häusern deren Fassadenstuck intakt ist. 1972 wurde die Altstadt unter Schutz gestellt, um den geplanten Abriss von ganzen Häuserzeilen zu verhindern. 1974 wurde ein Hochhausbauverbot für die gesamte Stadt verordnet, als Reaktion auf den oft unsensiblen Umgang der Investoren mit dem Stadtbild. Weiterhin wurden auch Teile der Außenbezirke als Grüngürtel unter Schutz gestellt, und die Bebauungsdichten im gesamten Stadtgebiet wurden drastisch gesenkt. Während die Unterschutzstellung der Altstadt und des Grüngürtels heutzutage als großer Erfolg gewertet werden, wurden das Hochhausverbot und die niedrigen Bebauungsdichten inzwischen teilweise revidiert. Die Stadtplaner hatten erkannt, dass Zersiedlung ein großes Problem darstellt; daher ist der Bau von Hochhäusern in mehreren Gebieten außerhalb der Altstadt und der Gründerzeitviertel wieder erlaubt.

Mit dem Bau des Südgürtels (2014 bis 2017) ist mehrere Jahre viel Geld (> 100 Mio. Euro) in Struktur für Kfz-Verkehr geflossen. Das relativ zentrumsnahe große Gelände der ehemaligen Brauerei Reininghaus im XIV. Bezirk Eggenberg wird von Investoren in den nächsten Jahren bebaut werden. Großer Bedarf an Wohnungen führt derzeit zur Bebauung von Baulücken und Ausbauten von bestehenden Häusern. Die Ansiedlung von jungen Wirtschaftstreibenden auch in Form von "Co-Working-Space" findet seit einigen Jahren insbesondere um Mariahilfergasse und Lendplatz statt.

1965 entstand die „Grazer Schule“. Ihr zuzurechnen ist eine Reihe von Gebäuden im Bereich der Grazer Universitäten. Dazu zählen z. B. die Gewächshäuser von Volker Giencke, oder das RESOWI-Zentrum von Günther Domenig. 2003 bestärkte Graz als Kulturhauptstadt Europas seinen Ruf mit mehreren neuen Bauten, darunter Stadthalle, Kindermuseum, Helmut-List-Halle, Kunsthaus und Murinsel. Letztere knüpfen, obwohl nicht von heimischen Architekten und Künstlern, in ihrer Formgebung an die „Grazer Schule“ an. Ein Wohnbauprojekt der letzten Jahre ist das Rondo. Seit 2009 besitzt die Kunstuniversität mit dem sogenannten "Mumuth" von Ben van Berkel einen modernen Veranstaltungsort.

70 % der Stadtfläche von Graz werden von Grünflächen eingenommen, wobei einen großen Teil dieser Flächen die Gärten der zahlreichen Einfamilienhäuser ausmachen. Der Grüngürtel, der unter besonderem Schutz steht, nimmt den ganzen westlichen, nördlichen und östlichen Stadtrand ein. Sehr beliebt als Ausflugsziele im Grüngürtel sind die Platte, der Leechwald und der Plabutsch. Es gibt zahlreiche Parkanlagen in Graz. Neben dem Stadtpark, dem größten Park in Graz, sind auch noch Volksgarten, Augarten, Schlosspark Eggenberg, Eustacchio Naturpark und Burggarten nennenswert. Auch der Schloßberg wurde nach der Schleifung der Burg im 19. Jahrhundert begrünt und dient heute als Erholungsraum.

Ab der Mitte des 19. Jahrhunderts – der Gründerzeit – wurden in den damals ländlich geprägten Vororten Geidorf, Jakomini und St. Leonhard nach einem die Urbanität fördernden Gestaltungskonzept bei der Errichtung von Wohnhäusern Vorgärten als Bindeglied zwischen Haus und öffentlichem Raum angelegt. Schmiedeeiserne Zäune und Zierpflanzen, wie Flieder, Magnolien, Rosen und Hortensien zählten zu den wesentlichen Merkmalen dieser „Visitenkarte“ der Hausbesitzer. Vorgärten besitzen einen historischen, kulturellen und ästhetischen Wert, sie vermitteln eine städtebaulich-räumliche Qualität und erfüllen auch eine nicht unwesentliche ökologische Funktion. Die im Auftrag der Grazer Stadtplanung vom Naturschutzbund Steiermark im Jahre 2003 erstellte Fotodokumentation ergibt die Anzahl von 800 gründerzeitlichen Vorgärten, die seit dem Jahre 2008, wie die Fassade des Gebäudes und der Innenhof, nach dem Grazer Altstadterhaltungsgesetz geschützt sind.

Die Berge, die das Grazer Becken von Westen bis Nordosten umschließen (Buchkogel, Plabutsch, Hohe Rannach, Leber, Platte, Lustbühel), laden zu kurzen Spaziergängen oder ausgedehnten Wanderungen ein und bieten schöne Ausblicke auf die Stadt; zudem sind sie vom Zentrum aus leicht mit öffentlichen Verkehrsmitteln erreichbar. Darüber hinaus gibt es ein immer dichter werdendes Netz von Mountainbike-Routen mit den Hauptgebieten Schöckl und Plabutsch. Das nordöstlich anschließende Grazer Bergland – das sich vom Grazer Hausberg Schöckl (1445 m) bis hin zum Hochlantsch (1720 m) erstreckt – erweitert diese Möglichkeiten nochmals um Klammen und Höhlen wie die Bärenschützklamm, die Kesselfallklamm oder die Lurgrotte.

Für historisch Interessierte bieten sich Tagesausflüge zum Österreichischen Freilichtmuseum im rund 20 Kilometer nördlich der Stadt gelegenen Stübing oder beispielsweise zum Bundesgestüt Piber an, in dem die Lipizzaner für die Spanische Hofreitschule in Wien gezüchtet und dressiert werden.

Im Großraum Graz finden sich auch Kirchen, die von österreichischen Künstlern neu gestaltet wurden. In Bärnbach die Hundertwasserkirche (umgestaltet von Friedensreich Hundertwasser) und im Vorort Thal bei Graz die St. Jakob Kirche mit dem Zubau und der Ausstattung von Ernst Fuchs. Auch in Bärnbach wurde ein Brunnen von Ernst Fuchs gestaltet.

Die Stadt Graz hat wegen ihrer Lage am Schnittpunkt europäischer Kulturen eine jahrhundertealte Tradition als internationales Kulturzentrum. Durch die Funktion als Hauptstadt Innerösterreichs ab 1379 gewann Graz größeren Einfluss im Alpen-Adria-Raum. Die romanischen und slawischen Einflüsse sind bis in die Gegenwart bemerk- und vor allem durch die Bauwerke der Altstadt sichtbar. 1993 fand in Graz der „Europäische Kulturmonat“ statt. Am 1. Dezember 1999 wurde Graz für seine Altstadt von der UNESCO in die Liste der Weltkulturerbe aufgenommen. 2003 war Graz Kulturhauptstadt Europas. Seit 2010 zählt das Barockschloss Eggenberg zum Weltkulturerbe.

Zu den wichtigsten Grazer Veranstaltungsorten zählen seit 2002 die Stadthalle Graz auf dem Messegelände, die bis zu 11.030 Menschen Platz bietet, der Stephaniensaal im „Congress Graz“, die Helmut-List-Halle mit Platz für bis zu 2.400 Besucher, das Veranstaltungszentrum Seifenfabrik, ein 2003 eröffnetes Zentrum, das in einer ehemaligen Seifenfabrik etabliert wurde.

Eine der ältesten Spielstätten auf Grazer Stadtgebiet ist das Grazer Orpheum. Es zählt neben der Kasemattenbühne auf dem Schloßberg und dem Dom im Berg zu den sogenannten „Grazer Spielstätten“. Das Orpheum, die Geburtsstätte des Circus Roncalli, ist der 1950 errichtete Nachfolgebau des alten Grazer Varieté Orpheum, das zwischen 1899 und 1936 bestand. Der „Dom im Berg“ wurde für die Landesausstellung 2000 in den Schloßbergstollen errichtet und bietet bis zu 600 Personen Platz. Die Kasematten-Bühne auf dem Schloßberg ist eine überdachte Freilichtbühne, die zur Bestandszeit der Festung als Vorratskeller oder Kerker diente. Seit 2005 steht für Veranstaltungen die Alte Universität Graz in der Hofgasse zur Verfügung.

Der 1967 gegründete „Steirische Herbst“ ist ein internationales Mehrsparten-Festival für zeitgenössische Kunst; die „styriarte“ ist ein Musikfestival für Klassik und Barock, das „springfestival“ eine Veranstaltung für elektronische Kunst und Musik und das „Aufsteirern“ ein Fest der Volkskultur. 

Zu den wichtigsten Grazer Veranstaltungen zählen die Diagonale, ein jährlich stattfindendes Filmfestival, das Elevate Festival mit Schwerpunkt zeitgenössischer Musik, Kunst und politischen Diskurs, der Jazz-Sommer Graz, sowie "La Strada," eine internationale Veranstaltungsreihe für Straßen- und Figurentheater. Seit 1987 wird in Graz der Grazer Kleinkunstvogel vergeben, ein Wettbewerb, der als ältestes deutschsprachiges „Sprungbrett“ für den Kabarett- und Kleinkunst-Nachwuchs gilt. Seit 1986 findet das Berg- und Abenteuerfilmfestival Graz statt. Einen gesellschaftlichen Höhepunkt des Jahres bildet, ähnlich wie in Wien der Opernball, seit 1999 die Opernredoute im Grazer Opernhaus.

Die österreichische Filmproduktion ist auf Wien zentriert.

In Graz entstanden 1919 die Kurzstummfilme (600 bis 800 Meter) "Der Sprung in die Ehe" mit Ernst Arnold als Hauptdarsteller und "Die Zwangsjacke" mit Sängern des Opernhauses Graz als Darsteller. Beide stammten von der Grazer „Alpin-Film“. Ebenfalls in Graz produzierte man die Filme "Czaty, Die schöne Müllerin" und "Schwarze Augen". Alle drei Filme inszenierte Ludwig Loibner und wurden von der Mitropa-Musikfilm produziert. Besonderheit dieser Stummfilme war, dass es keine Zwischentitel gab, da stattdessen Sänger und Orchester den Film begleiteten, wozu Adolf Peter Balladen von Carl Loewe und Lieder von Franz Schubert bearbeitete. Problematisch war natürlich die Abstimmung von Orchester und Sänger auf die Geschwindigkeit des Films, weshalb abgesehen von der Premiere der Filme am 19. September 1921 keine weiteren Aufführungen belegt sind.

Ebenfalls in der Steiermark stellte der Dokumentarfilmpionier Bruno Lötsch, Vater von Umweltschützer und Museumsdirektor Bernd Lötsch, seine ersten Aufnahmen für das ab 1920 erschienene „Steiermärkische Filmjournal“ her, eine Wochenschau im Grazer Kinovorprogramm.

Im März 2004 wurde die CINESTYRIA laut Eigendefinition als eine regionale, nationale und internationale Schnittstelle für Filmförderung, Information, Service und Support steiermarkrelevanter Film- und TV-Projekte eingesetzt. Die verbesserte Kunst- und Nachwuchsförderung führte zu neuen Impulsen in der lokalen Filmszene.

Die Nachwuchsfilmgruppe LOOM drehte 2005 in Graz ihren Kinofilm "Jenseits" (2006, Regie Stefan Müller, u. a. mit Andreas Vitásek), u. a. in den Bezirken Mariatrost, Liebenau und St. Leonhard.

Zwei jüngere Fernsehproduktionen, die in Graz spielen und gedreht wurden, sind: "Die Liebe hat das letzte Wort" (2004, Regie Ariane Zeller, u. a. mit Günther Maria Halmer und Ruth Maria Kubitschek), sowie "Die Ohrfeige" (2005, Regie Johannes Fabrick, u. a. mit Alexander Lutz und Julia Stemberger).

Auch der Handlungsort des dreifachen Gewinners des Österreichischen Filmpreises 2011 "Die unabsichtliche Entführung der Frau Elfriede Ott" ist Graz.

Im Jahr 2014 wurde in Graz nach der Buchvorlage von Wolf Haas "Das ewige Leben" (Regie: Wolfgang Murnberger) gedreht.

Die bekannteste Grazer Band ist Opus mit dem Welthit "Live Is Life" aus dem Jahr 1985. Wilfried (Wilfried Scheutz) vertrat Österreich beim Eurovision Song Contest 1988 mit "Lisa Mona Lisa". Mit dem bekanntesten österreichischen Popsänger Falco ist Graz durch dessen Ex-Ehefrau verbunden.

In den Jahren 2004 und 2005 konnten die Bands Shiver und Rising Girl, deren Bandmitglieder aus Graz kommen, in der österreichischen Hitparade Platzierungen landen. Weitere Bands, die im regionalen Bereich sowie teilweise österreichweit und auch in Deutschland Beachtung finden sind Binder & Krieglstein, Jerx, Antimaniax, The Staggers, Facelift und Red Lights Flash.

Im Jahr 2002 formierte sich ein Orchester neu: Das Recreation – Großes Orchester Graz, welches oft auch in kleiner Besetzung auftritt. Auch die "World Choir Games" fanden einmal in Graz statt.

Die Literaturzeitschrift "manuskripte" erscheint seit 1960 in Graz, die "Perspektive" seit 1977 und die "Lichtungen" seit 1979. Wichtige Örtlichkeiten der Literaturszene sind das Forum Stadtpark und ist das Grazer Literaturhaus in der Elisabethstraße. Mit Graz verbunden sind der Schriftstellerverbände der Grazer Autorinnen Autorenversammlung und der Grazer Gruppe.

Seit 2013 wird das Künstlerhaus - Halle für Kunst und Medien im Stadtpark vom unabhängig agierenden und gemeinnützigen Verein "Kunstverein Medienturm im Künstlerhaus" geleitet, mit dem Ziel, zeitgenössische internationale Kunst, sowie herausragende lokale Künstler einem breiten Publikum präsentieren zu können.

Mit dem SK Sturm und dem GAK, die sich über Jahrzehnte auf Augenhöhe duelliert haben, stellt die Stadt zwei der großen Traditionsvereine des österreichischen Fußballs. Während Sturm aktuell in der Fußball-Bundesliga spielt, musste der GAK 2012 Konkurs anmelden; der Nachfolgeverein GAK 1902 startete neu in der untersten Liga.

Durch die EC Graz 99ers ist Graz in der höchsten Spielklasse der österreichischen Eishockeyliga vertreten, durch den ATSE Graz zudem auch in der steirischen Eliteliga.

Im American Football sind die Graz Giants in der Austrian Football League aktiv.

Auch der Laufsport ist unter den Bürgern der steirischen Landeshauptstadt sehr beliebt. Die Stadt bzw. die nähere Umgebung bieten eine Vielzahl an Trainingsmöglichkeiten. So bieten der Murradweg und die Naherholungszentren Leechwald (21,5 km Laufwege) und Platte beschilderte und vermessene Laufwege. Diese Wege sind auch unter Mountainbikern und Nordic-Walkern beliebt. Höhepunkte der Laufsaison sind der Graz-Marathon (Ende Oktober), der Grazer Volkslauf, welcher am 17. April 1983 erstmals ausgetragen wurde und somit der älteste Volkslauf Österreichs ist, weiterhin der Business-Lauf und der Frauenlauf und schließlich als Jahresabschluss der Grazer Silvesterlauf. Bekannt ist auch der sogenannte USI-Lauf oder Kleeblatt-Lauf, der einmal jährlich vom Sportinstitut der Grazer Universität abgehalten wird. Er wird jedes Jahr begleitet vom USI-Fest, das stets abends auf den Kleeblattlauf folgt und mit bis zu 25.000 Besuchern als das größte Studentenfest Europas gilt. Graz ist auch Zentrum des Orientierungslaufs, mit drei ansässigen Klubs (Sportunion Schöckl Graz, OLC Graz und HSV Graz), die regelmäßig nationale, aber auch internationale Wettkämpfe veranstalten. Mit der Sportunion Triathlonverein Steiermark ist Österreichs größter Triathlonverein in Graz beheimatet.

Internationale Aufmerksamkeit brachte das 1984–2007 insgesamt 24 Mal am Dienstag nach der Tour de France durchgeführte „Grazer Altstadtkriterium“, ein Radrennen mit kurviger Streckenführung durch die engen, auch steilen und gepflasterten Gassen der Grazer Altstadt, an dem internationale Spitzenradsportler wie Lance Armstrong oder Jan Ullrich teilnahmen. Das in Graz für indoors entwickelte Altbau(rad)kriterium führt um einen sehr beengten Rundkurs, typisch in einer Wohnung. Juli 2018 wird ein MTB-Rennen ab Freiheitsplatz starten. Einradfahren und Artistik wird von vielen Kindern und Jugendlichen sommers in Kursen der Zirkusschulen gelernt. Als Spezialität wird auch Municycling und Rad-Trial betrieben. Von etwa 1964 bis etwa 1989 und etwa 2005 wurden Bergsprint-Radrennen auf den Grazer Schloßberg gefahren; seit 2015 wird hier ein ähnliches Bergeinzelzeitfahren, der Schlossbergman veranstaltet. Seit 2001 findet auch in Graz etwa im Sommerhalbjahr Freitag abends, wenn trocken, ein 20-km-Cityskating statt, bis 2016 ab Tummelplatz, seit 2017 ab OBI Baumarktparkplatz, Conrad-von-Hötzendorfstraße.

Im Sommer bietet die Stadt zahlreiche Bade- und Schwimmmöglichkeiten. Die Freibäder der Grazer Freizeitbetriebe Augarten (Jakomini), Eggenberg, Margarethen (Geidorf), Stukitz (Andritz) und Straßgang werden jeden Sommer von Badegästen besucht. Auch die in Graz-Umgebung gelegenen Badeseen in Kumberg (Well-Welt), das Schwarzl-Freizeitzentrum in Premstätten und die Copacabana in Kalsdorf bei Graz ziehen jedes Jahr hauptsächlich Grazer Gäste an. In den Freizeitzentren, aber auch in den Freibädern, gibt es ein reichhaltiges Sportangebot (Beachvolleyball, -soccer, Paddle, Minigolf und so weiter).

2003 fand an einer Walze unter der Hauptbrücke die "Paddel Rodeo WM" statt. Mit dem Kraftwerksbau Graz Puntigam ab 2017 wird die Mur hinauf bis fast zur Murinsel aufgestaut und 3 dieser mit natürlichem Wasserstrom über mit Baggern bei Niedrigwqasser eingelegte Steinblöcke funktionierenden Wasserwalzen stillgelegt.

Wärmer werdende Winter ließen in den letzten Jahren kaum mehr Natureis-Eislaufen auf dem Hilmteich, dem Teich im Volksgarten oder im Schatten des Kirchbergs in Mariatrost zu. Die einzige Kunsteisbahn befindet sich in der Liebenauer Eishalle. Als kostenlose Attraktion wenige Wochen um den Jahreswechsel wird in den letzten Jahren eine Kunsteisbahn, nun am Karmeliterplatz aufgebaut.

Eislaufen und Eiskunstlaufen haben in Graz und in der Steiermark eine längere Tradition. So wurde der "Steirische Eislaufverband" mit dessen ersten Präsidenten Leo Scheu bereits 1923 gegründet. Zu Ehren von Scheu wird in Graz jährlich eine große Eislaufveranstaltung, die "Icechallenge" (das "Leo Scheu Memorial") veranstaltet. Diese Veranstaltung mit jährlich bis zu 150 Sportlern wurde seit dem Jahr 1971 insgesamt bereits 35 Mal in der Liebenauer Eishalle durchgeführt.

Der Steirische Eislaufverband konnte sportlich einige Erfolge verbuchen. In den letzten fünf Jahren wurde bei den Damen durch Karin Brandtstätter 2005, Kathrin Freudelsperger 2007 (beide vom Grazer Eislaufverein) und durch Denise Kögl 2008 (Eissportclub) insgesamt drei "Österreichische Staatmeistertitel" gewonnen. Zudem wurden mit Kathrin Freudelsperger 2007 und Denise Kögl 2008 erstmals steirische Einzelsportler im Eiskunstlaufen zu Weltmeisterschaften entsandt.

Auch Ultimate Frisbee wird in Graz professionell gespielt. Drei österreichische Nationalspieler trainieren in Graz. Das österreichische Nationalteam wurde im Sommer 2004 in Portugal Weltmeister.

Als bekannte Sportler, die aus Graz stammen, sind an oberster Stelle die Medaillengewinner bei Olympischen Spielen zu nennen: Harald Winkler (Gold, Viererbob 1992), Franz Brunner und Walter Reisp (Silber, Handball 1936), und Ine Schäffer (Bronze, Leichtathletik 1948) sowie Marion Kreiner (Bronze, Snowboard 2010).

Graz hat durch seine günstige Lage im Südosten Österreichs eine wichtige Standort-Funktion für internationale wie nationale Unternehmen. Der Zentralraum Graz erwirtschaftet mehr als ein Drittel der industriellen Wertschöpfung des Bundeslandes Steiermark und bietet mehr als 40 % der steirischen Arbeitnehmer einen Arbeitsplatz. Graz und die Steiermark sind Österreichs Innovationszentrum und Technologiefabrik, jede dritte High-Tech-Innovation in Österreich kommt aus dieser Region.

2003 arbeiteten in Graz 184.135 Personen in 10.692 Arbeitsstätten, rund 70 % davon im Dienstleistungssektor (besonders öffentlicher Dienst, Handel, Geld- und Versicherungswesen). Zum Vergleich: Im Jahr 2001 waren es noch 158.268 Personen. 2003 wurden 996 neue Grazer Unternehmen gegründet. Seit 1906 finden in Graz jährlich die Grazer Herbstmesse und zahlreiche Fachmessen im Messecenter Graz statt, bei denen häufig mehr als 200.000 Besucher registriert werden. Aufgrund der großen wirtschaftlichen Anziehungskraft der Stadt sind mehr als 75.000 der Arbeitnehmer Einpendler. Mehr als 40 % der gesamten steirischen Wirtschaftsleistung werden im Zentralraum Graz erwirtschaftet.

Graz ist Sitz bedeutender, global wie national agierender Unternehmen und wichtigster Wirtschaftsstandort der Region und Südösterreichs. Zu den großen und bekannten Arbeitgebern zählen der Anlagenbauer Andritz AG, der Automobilhersteller Magna Steyr, ein vom Austro-Kanadier Frank Stronach gegründeter und international tätiger Konzern, der sich auf dem Gelände des ehemaligen Eurostar Automobilwerkes befindet. Das Vorgängerunternehmen war Steyr Daimler Puch. Die Fabriken der Puch-Werke in Thondorf wurden von Steyr adaptiert. Der Grazer Unternehmer Johann Puch hatte seine Fabrik 1899 in der Grazer Strauchergasse gegründet; das Werk wurde während des Zweiten Weltkrieges nach Thondorf verlegt, um für die Rüstungsindustrie produzieren zu können.

Graz ist weltweit bekannt für hochspezialisierte, insbesondere im KMU-Bereich angesiedelte Unternehmen des Maschinenbaus und der Umwelttechnik. Das Schuhhandelshaus Stiefelkönig wurde 1919 in Graz gegründet. Die AVL List unter der Leitung von Helmut List sowie Anton Paar haben ihren Sitz in Graz, ebenso wie der Versicherungskonzern der Grazer Wechselseitigen und die Merkur Versicherung, zahlreiche Banken, sowie verschiedenste Mittel-, Klein- und Kleinstbetriebe aus Gewerbe und Industrie. In Puntigam befindet sich die gleichnamige Brauerei, die mittlerweile Teil des Heineken-Konzerns ist. Bis zu ihrer Stilllegung im Jahr 1947 gab es in Eggenberg die Brauerei Reininghaus. Das Reininghaus-Bier wird in Puntigam abgefüllt.

Im Laufe der langen Geschichte entstanden viele historisch interessante Unternehmen auf dem Grazer Stadtgebiet. Ehemalige Grazer Unternehmen sind, in Auswahl, die Maschinen- und Motorenfabrikanten Simmering-Graz-Pauker, der Automobilhersteller Ditmar & Urban, der von 1924 bis 1925 bestand und nur ein Modell herstellte, die 1825 gegründete und Ende des 19. Jahrhunderts geschlossene Grazer Zuckerfabrik, die erste steirische Sektkellerei und Weingroßhandlung der Brüder Kleinoscheg oder die Hutfabrik Josef Pichler & Söhne.

Der stark wachsende Autocluster Steiermark (oder „ACstyria“) ist ein Zusammenschluss von mehr als 180 steirischen Unternehmen, die in der Autozulieferindustrie tätig sind. Das Zentrum des Autoclusters ist Graz. Größtes Unternehmen und Leitbetrieb ist der Magna-Konzern. Im Autocluster arbeiteten im Jahr 2006 zirka 44.000 Menschen, die einen Umsatz von 9,6 Mrd. € und eine Wertschöpfung von 1,6 Mrd. € erwirtschafteten. KTM fertigt seinen Sportwagen X-Bow im neu erbauten Werk in Graz (Bezirk St. Peter).
Darüber hinaus entwickeln sich innerhalb der Stadtgrenzen Branchen wie Nano- und Biotechnologie, Umwelttechnologie, Medizintechnik und Flugzeugbau in rasantem Tempo.

Graz ist eine überregionale Einkaufsstadt, deren Einzugsgebiet sich weit über die Stadtgrenzen und das Umland hinaus bis ins südliche Burgenland, nach Slowenien, Ungarn und Kroatien erstreckt.

Eine beliebte Einkaufsstraße ist die Herrengasse in der Inneren Stadt. Die Annenstraße, welche vom Hauptplatz nach Westen Richtung Hauptbahnhof führt, war früher eine sehr belebte Einkaufsstraße. Sie hat mittlerweile, trotz einiger Revitalisierungsversuche, viel von ihrer einstigen Bedeutung verloren. Weitere Einkaufsstraßen sind die Sackstraße, wo viele kleine Galerien und Kunstgewerbegeschäfte zu finden sind, die Sporgasse sowie die Murgasse.

Das Groß- bzw. „Alpenlandkaufhaus“ Kastner & Öhler, das älteste Grazer Kaufhaus, steht in der Sackstraße. In und um Graz gibt es eine Reihe von Einkaufszentren: Das „Shopping-Center West“ am Weblingergürtel, das Einkaufszentrum Murpark an der Liebenauer Tangente, den „Citypark“ am Lazarettgürtel sowie das Einkaufszentrum „Shopping Nord“ in Gösting, an der Kreuzung Wiener Straße – Autobahnzubringer Nord.

In der Grazer Nachbargemeinde Seiersberg liegt das größte Einkaufszentrum, die Shopping City Seiersberg. In Planung befindet sich ein Outletcenter in Puntigam. Seit der Eröffnung des „Shopping Nord“ im März 2008 weist Graz die höchste Dichte an Einkaufszentren in Österreich auf. Damit kommt auf jeden Einwohner der Stadt mindestens ein Quadratmeter Einkaufszentrum.

Graz ist die größte Landwirtschaftsgemeinde der Steiermark. Etwa 7.600 Rinder, Schweine, Schafe, Hühner und sonstiges Geflügel sowie Ziegen und Zuchtwild werden in etwa 340 Betrieben im Stadtgebiet gehalten. Auf 14 verschiedenen Bauernmärkten bieten die Landwirte das ganze Jahr über Kulinarisches aus Eigenproduktion an. Die Märkte am Kaiser-Josef-Platz und am Lendplatz zählen zu den größten und ältesten Grazer Märkten. Von einem reichhaltigen Angebot an frischen Lebensmitteln aus biologischem Anbau profitiert auch die berühmte und stark expandierende Spitzengastronomie der Stadt.

Graz besitzt eine ausgeprägte Fernwärmeversorgung mit einer Anschlussleistung von mehr als 500 MW. Im Winter wird die Wärme überwiegend aus Abwärme der Stromerzeugung genutzt, im Sommer stammt die Energie teils aus industrieller Abwärme und Gaskesseln. Graz beschreitet technologisch neue Wege: Thermische Solaranlagen mit mehreren Tausend Quadratmeter Kollektorfläche liefern mehrere Megawatt Wärme: auf dem Dach der Trainingshalle des Eisstadions Graz-Liebenau (direkt neben der Merkur Arena) mit 700 kW Leistung, auf der Siedlung Berliner Ring (1300 kW), am Fernheizkraftwerk und auf den Dächern der städtischen AEVG (Abfall-Entsorgungs- und Verwertungs-GmbH, 3000 kW) und beim Wasserwerk der Graz AG (2000 kW).

Derzeit (Stand 2016) plant die Energie Steiermark mit weiteren Unternehmen, denn Anteil an solarer Fernwärme deutlich aufzustocken. Dazu soll ein aus Solarkollektoren bestehender Solarpark mit einer Größe zwischen 15 und 45 ha sowie ein dazugehörender saisonaler Wärmespeicher errichtet werden. Bei Realisierung der 45-ha-Variante soll die Anlage circa 230 GWh Wärmeenergie liefern, was etwa 20 % des Grazer Fernwärmebedarfes entspricht. Der zugehörige Speicher zum jahreszeitlichen Ausgleich hätte ein Fassungsvermögen von circa 1,8 Mio. Kubikmetern, die Kosten dieser Variante werden auf etwa 200 Mio. Euro geschätzt.

Die Wasserversorgung in Graz stellt die Grazer Stadtwerke bereit. Das Wasser stammt ausschließlich vom Grundwasser der quartären Schotterfüllungen des Murtales. Die Quellen liegen in Friesach, im Stadtbezirk Andritz und in St. Ilgen am Hochschwab. Das Verteilsystem in Graz hat eine Länge von 835 km; inklusive der etwa 30.000 Hausanschlüssen 1.273 km. Die Grazer Stadtwerke verfügen über 23 Trinkwasserhochbehälter mit einem Gesamtspeichervolumen von 34.742 m³.

Die Abfallentsorgung in Graz wird seit 1984 von der AEVG wahrgenommen. Sie ist ein Unternehmen der Grazer Stadtwerke und der Stadt Graz. Jährlich entsorgt das Unternehmen zirka 135.000 t Müll, davon landen etwa 20.000 t auf einer Deponie. Der Betrieb trägt das Emas-Gütesiegel für geprüftes Umweltmanagement.

In Graz decken sieben Krankenhäuser, mehrere Privatkliniken/Sanatorien sowie über 40 Apotheken und zahlreiche niedergelassene Ärzte die Gesundheitsversorgung der Bevölkerung ab. Das LKH-Universitätsklinikum Graz ist ein Krankenhaus mit Maximalversorgung, über 1500 Betten und 7190 Mitarbeitern. Es deckt den Grazer Osten ab und dient außerdem als Schwerpunkt-Klinik für Patienten aus der gesamten Steiermark. Für den Westen sind das LKH Graz-West in Eggenberg mit 280 Betten und rund 500 Mitarbeitern, sowie das Unfallkrankenhaus der AUVA mit 180 Betten und rund 440 Mitarbeitern zuständig.

In der Landesnervenklinik Sigmund Freud (LSF), einer öffentlichen Sonderkrankenanstalt in Straßgang, werden Patienten mit psychischen, psychosomatischen und neurologischen Erkrankungen ambulant und stationär betreut. Für diese stehen 780 Betten zur Verfügung.

Weiters bestehen im Grazer Westen das geriatrische Krankenhaus Albert-Schweitzer-Klinik in Gries mit über 300 Betten, das Krankenhaus der Barmherzigen Brüder I in Lend mit etwa 220 Betten, das Krankenhaus der Barmherzigen Brüder II in Eggenberg mit 260 Betten und das Krankenhaus der Elisabethinen in Gries mit rund 180 Betten. Es gibt in Graz einige Privatkliniken: die Privatklinik Kastanienhof, die Privatklinik Leech, die Privatklinik der Kreuzschwestern, das Sanatorium St. Leonhard, das Sanatorium Hansa und die Privatklinik Graz-Ragnitz.

Seit 2017 besteht das VinziDorf-Hospiz für Obdachlose.

Den Rettungsdienst deckt in Graz das Österreichische Rote Kreuz mit zwei Notarzteinsatzfahrzeugen, zwei Notfallwagen (Jumbo) und mehr als 30 Rettungswagen (RTW) ab. Die „Jumbo“ genannten Notfallwagen sind eine Grazer Besonderheit und im österreichischen Rettungsdienst einmalig, da sie besetzt mit sogenannten Rettungsmedizinern des Medizinercorps (Ärzten oder Medizinstudenten kurz vor Studienabschluss) zwischen Rettungs- und Notarztwagen einzuordnen sind. Weiteres ist Graz der Standort der Landesleitstelle des steirischen Roten Kreuzes.

Neben dem Roten Kreuz sorgen der Arbeiter-Samariter-Bund (ASB), die Malteser und das Grüne Kreuz für den Krankentransport. Zusätzlich ist am Flughafen Graz-Thalerhof der Christophorus Notarzthubschrauber C12 stationiert.

In Graz gibt es einen Ärztenotdienst, der außerhalb der Praxisöffnungszeiten eine allgemeinmedizinische Versorgung in und um Graz bietet.

Als Sicherheitsbehörde für die Stadt fungiert die Landespolizeidirektion Steiermark mit Sitz in der Straßganger Straße. Ihr beigegeben als Dienststelle des Wachkörpers für das Stadtgebiet, mit 13 Polizeiinspektionen und einem Polizeianhaltezentrum, ist das Stadtpolizeikommando Graz.

Als Ergänzung zur Polizei dient die städtische Ordnungswache. Sie dient der Überwachung der örtlichen Sicherheit, darf aber keine gerichtlich strafbaren Taten verfolgen und führt auch keine Überwachungstätigkeiten im Gebiet des Straßenverkehrs durch.

Wie fast alle anderen großen österreichischen Städte verfügt auch Graz über eine hauptberufliche Feuerwehr. Mit insgesamt drei Wachen in den Bezirken Lend, St. Leonhard und Puntigam wird die Stadt im Regeldienst durch die Berufsfeuerwehr abgedeckt, zusätzlich gibt es noch eine Feuerwache der Freiwilligen Feuerwehr Graz im Bezirk Mariatrost. Die Freiwillige Feuerwehr fungiert als Ergänzung zur Berufsfeuerwehr, die im Bedarfsfall alarmiert wird. Der Großteil der Fahrzeuge der Freiwilligen Feuerwehr ist im Sinne der Nutzung von Synergieeffekten in der Feuerwache Süd der Berufsfeuerwehr stationiert. Eine Besonderheit bildet die Feuerwache Mariatrost, die von der Berufsfeuerwehr aufgelassen und an die Freiwillige Feuerwehr übergeben wurde. Seitdem wird diese Wache ausschließlich von der Freiwilligen Feuerwehr besetzt, eine Lösung die steiermarkweit einmalig ist.

Der Binnenverkehr in Graz wird vor allem durch den motorisierten Individualverkehr geprägt, durch den rund 45 % der Wege zurückgelegt werden. Der öffentliche Personennahverkehr erreicht rund 20 %, etwa 16 % werden mit dem Fahrrad zurückgelegt und rund 19 % zu Fuß.

Die Grazer Innenstadt ist von großflächigen Fußgängerzonen geprägt. Die Erweiterung wird von der Stadtplanung aktiv vorangetrieben. Dennoch ist ein Anstieg des motorisierten Individualverkehrs zu verzeichnen.

Für die positive Entwicklung des Grazer Radfahrnetzes war das Engagement Erich Edeggers entscheidend. 1980 markierten Aktivisten einen Radfahrstreifen mit einem Radfahrsymbol. Sie wurden polizeilich abgestraft; jedoch kaufte Vizebürgermeister Edegger die Schablone des Symbols auf. Die Markierungsart wurde übernommen, auch das Befahren der Fußgängerzone in der Schmiedgasse und das Fahren gegen die Einbahn wurden gesetzlich geregelt. Nach Edeggers Tod 1992 stockten seine begonnenen Initiativen. Der Fußgänger- und Radfahrsteg zwischen Schloßbergplatz und Mariahilferplatz ist nach dem Stadtpolitiker benannt.

Graz ist eine relativ radfahrerfreundliche Stadt, auf deren Gebiet rund 120 km Radverkehrsanlagen angelegt sind. Erklärtes Ziel der städtischen Verkehrsplaner ist es, den Radverkehrsanteil von 14 % (2007) zu steigern. Ein Beschluss aus dem Jahr 1980, ein Netz aus 190 km Radverkehrsanlagen zu errichten, dürfte erst 2035 realisiert sein. Neben dem Bau dreier Stege über die Mur gibt es an beiden Ufern Rad- und Fußwege. Befragungen zum Fahrradverhalten der Bürger führt die Stadt in regelmäßigen Abständen mit einem Fahrradklimatest durch.

Die 365 km lange „Murradweg“, eine touristische Radroute und nach dem „Donauradweg“ der zweithäufigst frequentierte Radweg Österreichs, sowie die Mountainbike-Route „Alpentour“ führen durch Graz. Die nähere Umgebung kann man über die gebirgige Radroute „Rund um Graz“ erkunden.

In der aus dem Verkehrsclub Steiermark um 1995 hervorgegangene "Radlobby ARGUS Steiermark" engagieren sich ehrenamtliche Aktivisten seit den 2000er Jahren um Berücksichtigung und Förderung von Radverkehr. In Kooperation mit Fahrradküche und "Critical Mass" werden seit etwa 2013 "Ghost Bikes" als weiße Mahnmale für getötete Radfahrer aufgestellt. Als April 2015 zwei Radfahrer an geöffneten Autotüren zu Tode kamen, wurde die Kampagne für ausreichenden Abstand intensiviert. Radfahrer sollen demnach 1,20 m Abstand von Autotüren einhalten, überholende Autofahrer 1,50 m von Radfahrern.

Seit 2007 fordert die Protestinitiative critical mass mehrfach mehr Platz für Radfahren in der Stadt. Seit die grüne Vizebürgermeisterin Lisa Rücker das Verkehrsresort übernommen hatte, ist eine Forcierung des Radnetzausbaus zu beobachten. Dazu zählen eine Liberalisierung von Radfahren im Zentrum (Fußgängerzonen, Parks, Einbahnen), die Qualitätshebung von Radwegen, das Werben für Radfahren als gesunde Bewegungsform, sowie die Berücksichtigung von Wünschen der Nutzer (Aktion Radfalle), neben einer deutlichen Reduktion des Autoverkehrsanteils.

Graz besitzt ein Straßennetz von rund 1000 km. Als eine der ersten österreichischen Städte begann am 1. September 1992 ein Modellversuch „Tempo 30“, es kam zu einer deutlichen Reduktion der Unfallzahlen, die jedoch durch gleichzeitig durchgeführte gestalterische und bauliche Maßnahmen und einer intensivierten polizeilichen Überwachung des Verkehrs hervorgerufe wurde. Dennoch wurde der Versuch bis heute nicht beendet.

Nach massivem Bau von Tiefgaragen in der Innenstadt, unter anderem aufwändig unter dem historischen Gebäude des Kaufhauses Kastner & Öhler, wurden bis 2007 bei Kaufhäusern, Großbetrieben und am Stadtrand Auto-Parkflächen ausgebaut. Mit der zweiten Röhre im Plabutschtunnel, der Nordspange (Gürtel-Unterführung) und Ausbauten am Südgürtel wurden großräumig wirksame Kapazitäten geschaffen.

Ein Großteil des Stadtgebiets sind gebührenpflichtige Kurzparkzonen, diese werden in blaue (teure 1,80 €/h in Schritten zu 0,10 € mindestens eine halbe Stunde zu 0,90 €, maximal 3 Stunden zu 5,40 €) und grüne (billigere 1,2 €/h oder 7 € für ein Tagesticket) Zonen unterteilt. Die Abrechnung erfolgt mittels Parkscheinautomaten, an denen innerhalb von 10 Minuten ein Parkticket gelöst dann zum Auto gebracht und gut sichtbar hinter der Windschutzscheibe platziert werden muss. Eine Überzahlung wird nicht refundiert das Ticket darf auch nicht weitergegeben werden. Das System wird durch private Überwachungsorgane sehr dicht überwacht.

Es wird neuerdings ein Park&Ride System, mit Parkhäuern an wichtigen Einfallsstraßen sowie Autobahnzubringern, propagiert. Im Tarif ist eine Tageskarte für die öffentlichen Verkehrsmittel mandatorisch inkludiert ein Tages-Kombiticket kostet 8 €.

Die ersatzlose Schließung eines kleineren niveaugleichen Bahnübergangs in Gösting erfolgte um 2013 trotz Protests von Anrainern. Für das Stadtentwicklungsgebiet Reininghaus wird eine zusätzliche Bahnunterführung in westlicher Verlängerung der Josef-Hubergasse in den nächsten Jahren gebaut.

Graz verfügt über ein relativ gut ausgebautes öffentliches Verkehrsnetz, das Teil des Steirischen Verkehrsverbunds ist. Sechs reguläre Straßenbahnlinien und viele Buslinien durchziehen das Stadtgebiet. Das Netz ist dicht ausgebaut und wird von vielen Grazern frequentiert (66.4 km Straßenbahn und 250 km Bus). Die Graz Linien betreiben mit der Grazer Schloßbergbahn eine Standseilbahn, die mit gewöhnlichen Fahrscheinen zu benutzen ist, und einen kostenpflichtigen Aufzug auf den Schloßberg. In den Nächten von Freitag auf Samstag und von Samstag auf Sonntag, sowie in den Nächten vor Feiertagen verkehren Nachtbuslinien.

Die ursprünglichen Pferdebahnen (1878–1895) wurden durch elektrische Garnituren ersetzt. Die erste Grazer Straßenbahnlinie führte vom alten Südbahnhof (heute Hauptbahnhof) über eine Strecke von 2,2 km zum Jakominiplatz. Nach stetigem Ausbau des Liniennetzes bis nach dem Zweiten Weltkrieg fiel ein Großteil der Anlagen in den 1950er- und 1960er-Jahren dem verstärkten Aufkommen des Individualverkehrs zum Opfer. Betroffen war die nicht mehr existierende Ringlinie 2. Sie wurde ersatzlos gestrichen und wird in der Liniennummerierung nicht geführt. Exponate, die an die historische Entwicklung der Grazer Straßenbahn erinnern, sind im Tramway-Museum bei der Mariatroster Endhaltestelle ausgestellt.

Den wichtigsten Knotenpunkt des innerstädtischen öffentlichen Verkehrs bilden der Jakominiplatz, an dem alle Straßenbahnlinien, zehn Buslinien und alle Nachtbuslinien zusammentreffen, und der Europaplatz vor dem Grazer Hauptbahnhof, der als Busbahnhof für den Regionalverkehr dient. Zwischen 2005 und 2007 wurden die Straßenbahnlinien 4, 5 und 6 verlängert – die erste nennenswerte Erweiterung (insgesamt 3,5 km), seit in der Nachkriegszeit fast die Hälfte des Straßenbahnnetzes eingestellt worden war. Der Ausbau des Straßenbahnnetzes Richtung Südwesten zum Nahverkehrsknoten Don Bosco und zum Stadtentwicklungsgebiet Reininghaus, sowie der Bau einer Innenstadt-Entflechtungstrecke wurden vom Gemeinderat beschlossen. Weitere Projekte, etwa eine Nordwestlinie oder die Anbindung der Karl-Franzens-Universität, wurden aus finanziellen Gründen vorerst aufgeschoben. In der jüngsten Gegenwart sorgt die hohe Lautstärke (Luft- und Bodenschall) der neuen – schwereren und etwas breiteren – Variobahn-Triebwagen für Aufregung.

Der Grazer Hauptbahnhof, für seine funktionale Innenarchitektur mit dem Brunel Award ausgezeichnet und bei VCÖ-Passagier-Umfragen 2003 und 2004 als schönster Bahnhof Österreichs bewertet, liegt an der Südbahn. Er ist der Ausgangspunkt der steirischen Ostbahn und der Graz-Köflacher Eisenbahn (GKB). Von ihm fahren S-Bahnen in alle Teile der Steiermark sowie direkte Intercity-Züge nach Wien, Salzburg und Innsbruck ab. EuroCity-Züge verbinden Graz direkt mit Marburg an der Drau und Laibach in Slowenien, Zagreb in Kroatien, Brünn und Prag in Tschechien, Zürich in der Schweiz, sowie Frankfurt am Main und Saarbrücken in Deutschland. Zürich ist auch per EuroNight direkt erreichbar, Budapest in Ungarn mit einer täglichen Kurswagen-Verbindung. Die im Bau befindliche Koralmbahn soll die historisch und geographisch bedingte schlechte Anbindung von Graz an das europäische Eisenbahnnetz deutlich verbessern und Graz direkt mit Italien verbinden.

Im Grazer Stadtgebiet gibt es sechs Bahnhöfe (Stand 2016), sowie weitere (Bedarfs-)Haltestellen für den S-Bahn-Betrieb. Neben dem Hauptbahnhof Graz, der im Zweiten Weltkrieg durch Luftangriffe zerstört und danach wieder aufgebaut wurde, sind das der Ostbahnhof, ein im Jahr 1873 eröffneter Backsteinbau, der nun unter Denkmalschutz steht, und der Köflacher-Bahnhof, sowie drei weitere Bahnhöfe ("Don Bosco," "Murpark," "Puntigam"), die im Zuge des Ausbaus der S-Bahn-Steiermark an der Südbahn und an der in Bau befindlichen Koralmbahn, sowie an der Steirischen Ostbahn errichtet wurden und als Netzknoten der Verbundlinien (Bahn, Bus, Straßenbahn) fungieren.

Ein Jahr nach Inbetriebnahme der Koralmbahn mit dem Koralmtunnel soll auch der Semmeringbasistunnel, der sich seit 2016 in allen Bauabschnitten in Vortrieb befindet, fertiggestellt sein. Voraussichtlich ab 2023 könnte die Fahrzeit auf der Schiene zwischen Graz und Wien von 2,5 auf 2 Stunden verkürzt werden. Von Graz nach Klagenfurt würde sich die Fahrzeit auf 1 Stunde reduzieren. Für den Güterverkehr entfallen mit Inbetriebnahme des Semmeringbasistunnels die auf der Bestandstrecke der Semmeringbahn zu bewältigenden betrieblichen Einschränkungen und Erschwernisse.

Die S-Bahn Steiermark wurde im Dezember 2007 mit sechs Linien eröffnet und ist in der Ausbauphase. An diesem Projekt wird seit 1998 gearbeitet. Die Teilinbetriebnahme erfolgte am 9. Dezember 2007. Mit 11. Dezember 2016 gingen weitere zwei S-Bahn-Linien in Betrieb. Das S-Bahn-Netz soll mit der Inbetriebnahme der neuen Koralmbahn und des Koralmtunnels in Vollbetrieb sein und der Großraum Graz von neun S-Bahn-Linien in einem verdichteten Taktverkehr erschlossen werden. Die S-Bahn bietet derzeit stündlich mehrere Verbindungen. Im Vollbetrieb sollen die Taktintervalle auf allen Linien je nach Infrastruktur verdichtet werden. Die S-Bahn ist eine Kooperation zwischen den Bahngesellschaften ÖBB, STLB und GKB.

Der Neu- und Umbau zu einem Nahverkehrsknoten Hauptbahnhof wurde 2016 abgeschlossen. Die Umstiegsmöglichkeiten zwischen den Zügen und den städtischen Verkehrsmitteln wurden adaptiert und die Straßenbahn mit einer Unterführung des "Eggenbergergürtels" und einer Unterflurtrasse mit Doppelhaltestellen in Tieflage an den Hauptbahnhof angebunden.

Die meisten Fernbuslinien starten mehrmals täglich und am Hauptbahnhof Graz, vielfach in Stockbussen, mit WC und WLAN. Die ÖBB bieten den Intercitybus (Option: 1. Klasse) nach (Wolfsberg und) Klagenfurt (Fahrtzeit 2:00; Anschluss nach Venedig) an, Westbus/Blaguss bedient über St. Michael Wien (2:45) und Klagenfurt (3:00). Mit Flixbus – u. a. ab Girardigasse 1 – erreicht man Linz, Wien, Triest, Maribor und Ljubljana. Nur Dr. Richard/MeinFernbus.de fährt (seit 26. November 2014) ab Jakominiplatz und Murpark in knapp 2:30 nach Wien. Am längsten, doch teilweise nur zum Wochenende, bestehen Busverbindungen auf den Gastarbeiterstrecken: Über Varaždin täglich nach Zagreb (in 4:15; von Wien) durch Blaguss/Eurolines/AP-Varaždin mit Abfahrt am Hauptbahnhof jedoch vor dem alten Postamt. Fahrten in 22 bis 30 Stunden von Wien nach Istanbul bieten Bosfor (mit Ulusoy über Budapest, Belgrad und Sofia) und Imperial Reisen.

Etwa 10 km südlich vom Stadtzentrum befindet sich im Gemeindegebiet von Feldkirchen und Kalsdorf der Flughafen Graz. Er ist über Bus- und Bahnverbindungen von Graz erreichbar. Der Flughafen ist in Bezug auf Passagierzahlen im Linienverkehr nach dem Flughafen Wien-Schwechat und dem Flughafen Salzburg der drittgrößte österreichische Flughafen. Auch in Bezug auf Frachtaufkommen ist er nach den Flughäfen Wien-Schwechat und Linz der drittgrößte österreichische Flughafen. Am Flughafen befindet sich seit 1981 das österreichische Luftfahrtmuseum. Im Jahr 2017 nutzten 959.166 Passagiere den Flughafen.

Im Linienverkehr bieten Austrian Airlines Verbindungen nach Wien, Düsseldorf, Frankfurt und Stuttgart an, KLM nach Amsterdam, Lufthansa nach München, SkyWork Airlines nach Berlin, Swiss nach Zürich und Turkish Airlines nach Istanbul. Ziele des Charterverkehrs sind vor allem Mittelmeer-Destinationen.

Graz liegt am Kreuzungspunkt der Pyhrn Autobahn A 9 und der Süd Autobahn A 2, die sich beim Knoten Graz-West schneiden. Die A 9 verläuft auf Stadtgebiet beinahe komplett unterirdisch durch den zehn Kilometer langen Plabutschtunnel im Grazer Westen. Pläne, eine Stadtautobahn durch besiedeltes Gebiet zu führen, wurden nach Bürgerprotesten aufgegeben. Die A 2 ist von Graz aus über den Autobahnzubringer Graz-Ost erreichbar.

Weiterhin quert die Grazer Straße B 67 im Westen die Stadt von Nord nach Süd; sie ist durchgehend vierspurig ausgebaut und eine wichtige innerstädtische Transitstraße. Von ihr zweigen drei Teilstücke ab, die B 67a, die B 67b und die B 67c. Diese stellen Verbindungen zu drei Landesstraßen (ehemalige Bundesstraßen) her: die Gleisdorfer Straße B 65 (beginnend an der Kreuzung Elisabethstraße/Merangasse), die Weizer Straße B 72 (beginnend am Geidorfplatz) und die Kirchbacher Straße B 73 (beginnend am Übergang Münzgrabenstraße/Liebenauer Hauptstraße). Außerdem zweigt bei der Gürtelturmkreuzung die Packer Straße B 70 ab.

Die Landesstraße B 67a (Grazer Ringstraße) führt von Andritz über den Geidorfplatz und die Plüddemanngasse nach Graz-Messendorf und dann als Südgürtel weiter über Puntigam zum Verteilerkreis Webling. In diesem Abschnitt wurde am 19. März 2017 die Unterflurtrasse „Südgürtel“ nach langer Planungs- und Bauzeit für den Verkehr freigegeben. Damit ist der südliche Teil der B 67a durchgehend vierspurig befahrbar.

Die Schreibweise der Namen der Grazer Verkehrsflächen folgt den Grundsätzen der Wiener Nomenklaturkommission. Karl A. Kubinzky publiziert exemplarisch in Zeitungen aus seinem Werk über die Straßennamen.

Im Juli 2014 beauftragte der Gemeinderat die Prüfung sämtlicher Grazer Straßennamen. Eine 14-köpfige Kommission unter Leitung von Stefan Karner untersuchte 1630 Grazer Straßen und Plätze hinsichtlich ihres Namens. Im März 2018 wurde der rund 1000 Seiten umfassende Abschlussbericht präsentiert. Darin wurden 82 Straßennamen als historisch „kritisch“ eingestuft und 20 als „höchst bedenklich“.

Die Stadt ist in erster Linie für die Pflichtschulen verantwortlich, für die sie die Infrastruktur zur Verfügung stellt. Es gibt 51 Volksschulen und 21 Hauptschulen in Graz.

Daneben arbeitet sie mit den vom Bund betriebenen Allgemein- und Berufsbildenden Höheren Schulen zusammen. In Graz existieren 23 Bundesgymnasien (zum Beispiel das BRG Kepler), davon ein eigenständiges internationales Gymnasium (Graz International Bilingual School) und vier katholische Privatschulen, darunter das Bischöfliche Gymnasium.

In Graz gibt es acht höhere technische Lehranstalten (HTL), vier Handelsakademien/Handelsschulen (HAK/HASCH) sowie acht Schulen für wirtschaftliche Berufe (HBLA). Das Schulzentrum St. Peter umfasst neben zwei Gymnasien sechs Landesberufsschulen.

Graz ist mit etwa 60.000 Studierenden, vier Universitäten, zwei pädagogischen Hochschulen und zwei Fachhochschulen nach Wien der zweitgrößte Hochschulstandort Österreichs. Etwa jeder sechste Einwohner ist ein Student.

Die im Bezirk Geidorf gelegene Karl-Franzens-Universität (Carola-Franciscea) wurde 1585 gegründet und ist somit nach der Universität Wien die zweitälteste Universität Österreichs. Mit 32.000 Studenten ist die Universität Graz auch die zweitgrößte Universität des Landes und bietet eine Vielzahl an Studienrichtungen und -fächern an. 16.000 Studenten besuchen die Technische Universität (Erzherzog-Johann-Universität) und weitere 4.300 sind an der Medizinischen Universität (Leopold-Auenbrugger-Universität) immatrikuliert. Auch diese beiden Universitäten sind damit die zweitgrößten des jeweiligen Fachbereiches in Österreich. Ergänzt wird die Reihe der Grazer Universitäten durch die Universität für Musik und darstellende Kunst mit 2.300 Studenten (einschließlich Mitbelegern). Auch die Universität Klagenfurt unterhält im Rahmen der Fakultät für Interdisziplinäre Forschung und Fortbildung einen Standort in Graz.

Mit über 40 Studentenverbindungen zählt Graz zu den „Verbindungshochburgen“. 

Graz ist das größte österreichische Fachhochschulzentrum mit der Fachhochschule Joanneum, welche mit 3.400 Studenten die zweitgrößte Fachhochschule in Österreich darstellt, und der Fachhochschule Campus 02, welche 1.000 Studenten ausbildet. Neben diesen sind die pädagogischen Hochschulen (vor dem Wintersemester 2007 noch Akademien) Pädagogische Hochschule Steiermark sowie die Kirchliche Pädagogische Hochschule Graz in Graz ansässig.

Für die Musikausbildung unterhalb des Hochschulniveaus ist das Johann-Joseph-Fux-Konservatorium des Landes Steiermark zuständig. Weiters existiert das Konservatorium für Kirchenmusik der Diözese Graz-Seckau.

Darüber hinaus gibt es in Graz auch eine Schule für allgemeine Gesundheits- und Krankenpflege.

Anbieter berufsorientierter Weiterbildung sind das Wirtschaftsförderungsinstitut (WIFI), das Berufsförderungsinstitut (BFI), das Volksbildungshaus „Urania“, das Berufsförderungsinstitut Steiermark, die Volkshochschule Steiermark, das Gymnasium für Berufstätige Graz, die BHAKB Bundeshandelsakademie für Berufstätige, die Bulme Höhere Technische Bundeslehranstalt, FH Johanneum und FH Campus 02.

Neben der Steiermärkischen Landesbibliothek stehen der breiten Öffentlichkeit die Stadtbibliothek Graz mit sechs Zweigstellen, einer Mediathek, einem Bücherbus und dem Zustellservice in alle Grazer Postfilialen sowie die Bibliothek der Arbeiterkammer Graz zur Verfügung. Seit 1998 besteht die HörBibliothek Mariahilf. Auf dem wissenschaftlichen Sektor sind jene Bibliotheken zu nennen, die an allen Hochschulen, Universitäten und Fachhochschulen eingerichtet sind. Die älteste, bedeutendste und umfangreichste ist die 1573 gegründete Bibliothek der Universität Graz mit mehr als 3 Millionen Medien.

Die Wissenschaftsstadt Graz ist in hohem Maß von der Lehre und Forschung an ihren vier Universitäten geprägt. Neben den akademischen Bildungsstätten gibt es eine große Zahl von wissenschaftlichen Projekten und Institutionen. Dazu zählt in erster Linie die Joanneum Research GmbH, die mit circa 20 Instituten und 400 Mitarbeitern die zweitgrößte außeruniversitäre Forschungseinrichtung Österreichs darstellt und ihre Zentrale sowie einige Institute in Graz hat. Ebenfalls vertreten ist die Österreichische Akademie der Wissenschaften mit ihrem Institut für Weltraumforschung und circa 85 Mitarbeitern.

Weitere außeruniversitäre Einrichtungen sind:

Der Strafrechtler und Kriminologie Hans Gross lebte und wirkte in Graz. Er gilt als Begründer der Kriminalistik, der Lehre von den Mitteln und Methoden der Bekämpfung einzelner Straftaten und des Verbrechertums. Ihm und seiner Arbeit ist im Keller des Hauptgebäudes der Karl-Franzens-Universität ein eigenes Museum gewidmet.

Zu den bekanntesten in Graz geborenen Personen zählen die beiden Kaiser Ferdinand II. und Ferdinand III., der Barockbaumeister Johann Bernhard Fischer von Erlach, der Orientalist Joseph von Hammer-Purgstall (1774–1856), der 1914 in Sarajevo ermordete Erzherzog Franz Ferdinand und der Adelige Roman von Ungern-Sternberg. In der jüngeren Geschichte haben Graz als Geburtsort: der Chasan Leo Roth, Autor Gerhard Roth, Komponist und Musikpädagoge sowie Inhaber einer Musikschule Jakob Stolz, Komponist Robert Stolz, Komponist und Dirigent Leopold Stolz, Dirigent Karl Böhm, Schauspieler Rudolf Lenz, der ehemalige Rennfahrer und jetzige Motorsportberater von Red Bull Racing Helmut Marko, Zeitungsherausgeber Hans Dichand, der Architekt Friedrich St. Florian sowie der ehemalige österreichische Bundespräsident Heinz Fischer.


Von der Stadt Graz zu Ehrenbürgern und Ehrenringträgern ernannt wurden unter anderem: Karlheinz Böhm, Nikolaus Harnoncourt, David Herzog, Helmut List, Fritz Popelka, Arnold Schwarzenegger – er gab den Ehrenring am 19. Dezember 2005 zurück – und Heinz Fischer.

Der Hauptgürtelasteroid (2806) Graz wurde nach der steirischen Landeshauptstadt benannt.




</doc>
<doc id="1831" url="https://de.wikipedia.org/wiki?curid=1831" title="Gottfried Wilhelm Leibniz">
Gottfried Wilhelm Leibniz

Gottfried Wilhelm Leibniz (* in Leipzig; † 14. November 1716 in Hannover) war ein deutscher Philosoph, Mathematiker, Diplomat, Historiker und politischer Berater der frühen Aufklärung. Er gilt als der universale Geist seiner Zeit und war einer der bedeutendsten Philosophen des ausgehenden 17. und beginnenden 18. Jahrhunderts sowie einer der wichtigsten Vordenker der Aufklärung. Im 18. Jahrhundert wird er vielfach als Freiherr bezeichnet; doch bislang fehlt eine Beurkundung über eine Nobilitierung.

In frühen Schriften anderer Autoren wurde sein Nachname – analog zu demjenigen seines Vaters, Friedrich Leibnütz, und dessen väterlichen Vorfahren – auch „Leibnütz“, teils auch „Leibnitz“ (franz. "Godefroi Guillaume Leibnitz") geschrieben. Ab 1671 wählte er die Schreibweise „Leibniz“ für seinen Familiennamen.

Leibniz wurde nach dem damals in den protestantischen Territorien des Heiligen Römischen Reiches gültigen Julianischen Kalender am 21. Juni 1646 in Leipzig geboren und am 23. Juni in der Leipziger Nikolaikirche getauft. Seine Eltern, insbesondere sein Vater, der aus Altenberg im Erzgebirge stammende Rechtsgelehrte Friedrich Leibnütz (1597–1652), weckten früh ein Interesse an juristischen und philosophischen Problemen. Sein Vater war Jurist, Notar und Professor für Moralphilosophie (Ethik) und seine Mutter Catharina war die Tochter des Leipziger Professors und Rechtswissenschaftlers Wilhelm Schmuck. Der achtjährige Leibniz erlernte anhand der umfangreichen väterlichen Bibliothek autodidaktisch die lateinische und die griechische Sprache. Mit zwölf Jahren entwickelte er beim Durchdenken logischer Fragestellungen die Anfänge einer mathematischen Zeichensprache.

Leibniz besuchte von 1655 bis 1661 die Nikolaischule in Leipzig, die im Original erhalten und saniert ist. 1661 immatrikulierte er sich an der Leipziger Universität und betrieb philosophische Studien beim Theologen Johann Adam Schertzer und dem Philosophietheoretiker Jakob Thomasius.
1663 wechselte er an die Universität von Jena, um sich dort unter Anleitung des Mathematikers, Physikers und Astronomen Erhard Weigel pythagoreischen Gedanken zu öffnen.

1666, noch im Alter von 19 Jahren, veröffentlichte Leibniz sein erstes Buch "De Arte Combinatoria" (Über die Kunst der Kombinatorik). Mit dem ersten Teil dieses Buches "Disputatio arithmetica de complexionibus" wurde er im März des Jahres von "Jakob Thomasius" und "Erhard Weigel" in Philosophie promoviert.

Später im selben Jahr, mit 20 Jahren wollte sich Leibniz zum Doktor der Rechte promovieren lassen, doch die Leipziger Professoren lehnten ihn als zu jung ab. So ging er nach Nürnberg, um dort an der Universität Altdorf das Verwehrte nachzuholen. Vorübergehend stand er in Verbindung zu einer dortigen alchimistischen Geheimgesellschaft, deren Experimente er jedoch schon bald verspottete. Anschließend stand er bis 1672 im Dienst des Mainzer Erzbischofs Johann Philipp von Schönborn. Er lebte während seiner Mainzer Zeit im Boyneburger Hof, der Wohnstätte des kurmainzischen Oberhofmarschalls Johann Christian von Boyneburg, der ihm eine Stelle als Mitarbeiter des Hofrats Hermann Andreas Lasser verschafft hatte. Mit Lasser arbeitete er im Auftrag des Kurfürsten an einer Reform des römischen Rechts ("Corpus juris reconcinnatum"). Sein Werk "Nova methodus discendae docendaeque jurisprudentiae" („Eine neue Methode, die Jurisprudenz zu lernen und zu lehren“) erlangte in einschlägigen Kreisen starke Rezeption. Im Jahre 1670 stieg Leibniz trotz seiner lutherischen Konfession zum Rat am kurfürstlichen Oberrevisionsgericht auf.

1672 reiste Leibniz als Diplomat nach Paris. Dort unterbreitete er dem „Sonnenkönig“ Ludwig XIV. einen Plan für einen kreuzzugähnlichen Eroberungsfeldzug gegen Ägypten, um ihn von den geplanten Eroberungskriegen in Europa abzubringen. Der König lehnte diesen Plan ab; über einhundert Jahre später jedoch setzte Napoléon Bonaparte ihn in der Ägyptischen Expedition um.

1672/73 vollendete Leibniz Arbeiten an seiner Rechenmaschine mit Staffelwalze für die vier Grundrechenarten, führte diese vor der Royal Society in London vor und wurde auswärtiges Mitglied dieser berühmten Gelehrtengesellschaft. Das von Leibniz weiterentwickelte duale Zahlensystem legte den Grundstein zur rechnergestützten Informationstechnologie des 20. Jahrhunderts.

Schon Jahre zuvor, ab 1668, hatte sich unterdessen der welfische Herzog Johann Friedrich bemüht, Leibniz als Bibliothekar an seine Residenzstadt Hannover zu berufen. Doch erst nach mehreren Absagen sagte Leibniz dem Herzog schließlich im Jahr 1676 zu und wurde rund zwei Jahre später auch zu Johann Friedrichs Hofrat ernannt. Unter Ernst August wurde Leibniz 1691 auch Bibliothekar der Herzog August Bibliothek in Wolfenbüttel, mit Kurfürstin Sophie von der Pfalz stand er in regem Gedankenaustausch.

1682–1686 beschäftigte sich Leibniz mit technischen Problemen der Bergwerke im Oberharz. Er hielt sich häufig in Clausthal auf und machte zahlreiche Vorschläge zur Verbesserung des Oberharzer Bergbaus.

Ab 1685 reiste Leibniz im Auftrag des Welfenhauses durch Europa, um eine Geschichte der Welfen zu schreiben. Dadurch hatte er 1688 die Gelegenheit zu einer Audienz bei Kaiser Leopold I. in Wien. Dabei trug Leibniz seine Pläne für eine Münzreform, zum Geld-, Handels- und Manufakturwesen, zur Finanzierung der Türkenkriege, zum Aufbau eines Reichsarchives und vieles andere vor. Doch es wurde ihm nur wohlwollende Aufmerksamkeit zuteil.

1698 bezog Leibniz ein heute nach ihm benanntes Leibnizhaus in Hannover. Hier ließ Leibniz bald darauf für Jahre seinen Schüler und Sekretär, den späteren Gelehrten Rafael Levi, ebenfalls wohnen. Im Zweiten Weltkrieg wurde das Leibnizhaus zerstört und 1983 an anderer Stelle mit rekonstruierter Fassade neu gebaut.

1700 wurden nach Verhandlungen mit dem brandenburgischen Kurfürsten Friedrich III., dem späteren König Friedrich I., Pläne für eine Königlich-Preußische Akademie der Wissenschaften nach englischem und französischem Vorbild in die Tat umgesetzt. Mit Unterstützung von Friedrichs Gattin Sophie Charlotte, an deren Hof im Schloss Lietzenburg Leibniz häufig zu Gast war, wurde die Akademie in Berlin gegründet und Leibniz wurde ihr erster Präsident. Um diesen Erfolg auszudehnen, führte er 1704 in Dresden Verhandlungen über die Gründung einer sächsischen Akademie. Er gründete insgesamt drei Akademien, die bis heute Bestand haben: die Brandenburgische Sozietät der Wissenschaften (heute weitergeführt als Berlin-Brandenburgische Akademie der Wissenschaften) sowie die Akademien in Wien und St. Petersburg. Leibniz hat auf diese Weise zusammen mit seinen eigenen mathematischen und philosophischen Leistungen die Herausbildung von eigenständigen Wissenschaften sehr befördert und hiermit historisch bleibende Bedeutung erlangt.

Gottfried Wilhelm Leibniz wurde angeblich Ende 1711 von Kaiser Karl VI. geadelt und in den Freiherrenstand erhoben; es fehlt allerdings die entsprechende Urkunde.

Kurz vor seinem Tod kühlten die Beziehungen zum Haus Hannover ab, das nun unter der Leitung von Georg I. Ludwig stand. Leibniz starb vereinsamt am 14. November 1716 im Alter von 70 Jahren in Hannover und wurde dort in der Neustädter Hof- und Stadtkirche St. Johannis beigesetzt. Umstritten ist der Rahmen der Begräbnisfeier. Vielfach wird behauptet, nur sein Sekretär sei beim Begräbnis anwesend gewesen und kein Geistlicher habe die Beisetzung begleitet. Dagegen berichten Johann Georg von Eckhart (Leibniz’ langjähriger Sekretär und Mitarbeiter) und Johann Hermann Vogler (sein letzter Assistent und Amanuensis), die Beisetzung habe am 14. Dezember 1716 durch Oberhofprediger David Rupert Erythropel stattgefunden. Eckhart, der wenige Tage nach Leibniz’ Tod zum Hofrat und dessen Nachfolger als Bibliothekar und Historiograph des Hauses Hannover ernannt worden war, berichtet aber auch, dass alle Kollegen, die Beamten des Hofes, zum Begräbnis eingeladen worden seien, aber nur Hofrat von Eckhart selbst als einziger „von Stand“ erschienen sei.

Auf dem Sarg ließ Hofrat von Eckhart ein Ornament anbringen, das eine Eins innerhalb einer Null zeigte, mit der Inschrift "OMNIA AD UNUM" (deutsch: „Alles (bezieht sich) auf das Eine“), als Hinweis auf das von Leibniz entwickelte binäre Zahlensystem.

Im Auftrag von Friedrich Simon Löffler, dem Neffen und Erben von Gottfried Wilhelm Leibniz, erstellte der Bibliothekar Daniel Eberhard Baring ein Verzeichnis der von dem Universalgelehrten privat aufgebauten „Leibniz-Bibliothek“.

Leibniz sagte über sich selbst: „Beim Erwachen hatte ich schon so viele Einfälle, dass der Tag nicht ausreichte, um sie niederzuschreiben.“ Er zählt zur Frühaufklärung und wird oft als letzter Universalgelehrter bezeichnet. Er hatte einen starken Einfluss auf die nachfolgenden Aufklärer, die klassische deutsche Philosophie, den deutschen Idealismus und die Literatur der Klassik. Seine Entdeckungen in den Naturwissenschaften und seine philosophischen und historischen Schriften werden bis heute von Gelehrten in aller Welt zu Rate gezogen. Er repräsentierte als letzter großer Denker die vor dem 18. Jahrhundert praktizierte Wissenschaft der vielfältigen Verknüpfung und des Analysierens der Zusammenhänge.

Einige seiner Forschungsergebnisse und Initiativen waren:

Leibniz betrachtete die Wissenschaft als eine Einheit. Seine Erkenntnisse in der Integralrechnung, die Theorie der unendlichen Reihen, seine neuartige Geometrie, die Theorien der Kombinatorik, die Vorstellung über die Grundlagen der Mathematik und die Wahrscheinlichkeitsrechnung entwickelten sich in enger Verbindung mit seinen philosophischen Ansichten. Das gleiche trifft auf seine Erkenntnisse der Dynamik, auf die biologischen und geologischen Konzeptionen sowie auf die Forschungen im Bereich der praktischen Politik und der theoretischen Geschichtswissenschaft zu.

Das philosophische Schaffen von Leibniz gruppiert sich um drei große Problemkreise: die Monadentheorie, die Determinationskonzeption und die erkenntnistheoretisch-logischen Ansichten.

Leibniz hat sein Denken kontinuierlich revidiert. Eine komprimierte Darstellung wichtiger Ideen zur Metaphysik findet sich in seiner "Monadologie" (1714) – eine Monadentheorie.

Auch das Problem der „Essai de Théodicée“ (1710) erscheint bei Leibniz gelöst. Unsere Welt ist die beste aller möglichen Welten, sie besitzt einen maximalen Reichtum von Momenten und in diesem Sinne die größtmögliche Mannigfaltigkeit.

In seiner Begriffslehre geht Leibniz davon aus, dass sich alle Begriffe auf einfache, atomare Konzepte zurückführen lassen. Er beschäftigte sich damit, wie man diesen Konzepten Zeichen zuordnen könnte und so wiederum daraus alle Begriffe ableiten könnte.
So ließe sich eine ideale Sprache aufbauen. Neben anderen haben die Philosophen Russell und Wittgenstein diese Idee aufgegriffen und weitergeführt.
Mit der "Ars combinatoria" (1666) versuchte Leibniz eine Wiederaufnahme des Projektes der Heuristik.

Text des Leibniz-Zitates am Historischen Museum Hannover:
Der berühmte Satz von der „besten aller möglichen Welten“ ist oft missverstanden worden, unter anderem hat ihn Voltaire in seinem Roman "Candide" parodiert. Die Idee der „besten aller möglichen Welten“ soll nicht in naiver Weise tatsächliches und großes Übel in der Welt leugnen oder schönreden. Vielmehr weist Leibniz auf einen notwendigen Zusammenhang zwischen Gutem und Üblem hin: Es gebe nämlich Gutes, das nur zum Preis der Existenz von Übel zu haben ist. Die wirkliche Welt ist die beste u. a. in dem Sinne, dass das Gute in ihr auch von Gott nicht mit einem geringeren Maß an Übel verwirklicht werden kann. Außerdem ist die "„beste aller möglichen Welten“" dynamisch gedacht: Nicht der derzeitige Zustand der Welt ist der bestmögliche, sondern die Welt mit ihrem Entwicklungspotential ist die beste aller möglichen Welten.

Gerade dieses Entwicklungspotential ermöglicht es, den derzeitigen Zustand zu verbessern, nicht hin auf einen utopischen Endpunkt, sondern immer weiter, in einem nicht endenden Prozess der ständigen sich überbietenden Entwicklung.

Leibniz argumentiert einerseits, dass einige der Übel nur scheinbar sind, bzw. dass weniger Übel an einer Stelle ein mehr an anderer Stelle notwendig machen würde. Auch führt er zum Beispiel die Vielfalt an, die die Qualität der Welt ausmache. Es gibt aber auch einen logischen Grund, warum diese die beste aller möglichen Welten sein muss. Wenn nämlich Gott eine Welt aus dem Möglichen ins Wirkliche überführen möchte, so braucht er einen zureichenden Grund, da er nicht willkürlich wählen kann. Das einzige Kriterium, das eine Welt aber qualitativ von allen anderen unterscheidet, ist, die beste zu sein. Im Gegensatz etwa zu Descartes vertritt Leibniz die Ansicht, dass Gott logische Wahrheiten nicht schaffen oder ändern kann. Die Summe aller möglichen Welten findet Gott ebenso vor wie mathematische Sätze. Er hat darum auf den Zustand und die Geschehnisse innerhalb einer Welt keinen Einfluss. Selbst wenn er – Naturgesetze außer Kraft setzend – ein Wunder wirkt, so ist dieses Wunder mit der Auswahl der möglichen Welt schon ein für allemal festgelegt.

Ein Teilaspekt davon ist: Gott hat unter allen möglichen Welten die beste geschaffen. Da er allmächtig, allwissend und allgütig ist, musste er das auch. Die in der Welt vorkommenden Übel stehen dem nicht entgegen. Leibniz unterscheidet sie nach drei Typen:

Nach Leibniz gibt es keinen Widerspruch zwischen Determinismus und Freiheit. Obwohl mit der Wahl der Welt jede Handlung eines Menschen zum Beispiel vollständig unverrückbar festliegt, so ist die Tatsache, dass sich ein Mensch in einer Situation so und nicht anders verhält, völlig frei (im Sinne von unvorhersehbar). Dass sich ein Mensch so verhält (so verhalten würde), ist gerade der Grund, warum die Welt gewählt wurde. Ein anderes Verhalten wäre entweder logisch nicht möglich (nicht kompossibel mit dem Rest der Welt) oder würde eine moralisch schlechtere Welt bedingen.

Die Ausführungen über die beste aller möglichen Welten können als Antizipation moderner Modallogiken (z. B. die von Saul Aaron Kripke oder David Kellogg Lewis) gesehen werden.

Leibniz formuliert früh die Maxime der Verstandesmäßigkeit der Aufklärung. Zitat: „Jeder Mensch besitzt Fähigkeiten zur vernünftigen Lebensführung.“ Wenn Religion und Vernunft übereinstimmen, entstünde eine wahrhafte Religion. Leibniz postulierte, alle Gaben können den Menschen verderben, nur die echte Vernunft sei ihm unbedingt heilsam, aber an ihr werde erst dann kein Zweifel mehr haften, wenn sie sich überall gleich klar und gewiss, wie die Arithmetik, erweisen könne. Der Mathematiker Leibniz war im Gefolge des Pythagoras der Auffassung, dass sich in den Zahlen die tiefsten Geheimnisse verbergen. Das heißt, wenn man Vernunft mit Zahlen ausdrücken könnte, wäre der Einwand widerlegt: „Woher weißt du, dass deine Vernunft besser ist als meine? Welches Kriterium hast du für die Wahrheit?“

Harmonie ist ein prägender Begriff von Leibniz’ Philosophie. Er beschreibt Harmonie als Summe von unendlich vielen, unendlich kleinen Krafteinheiten, sogenannten Monaden, den Urbestandteilen der Weltsubstanz, die durch Gott vereint wurden und so die Welt zusammenhalten.

Leibniz geht davon aus, dass Gott alles aus dem Nichts geschaffen hat (creatio ex nihilo) und alles, was Gott geschaffen hat, gut ist. Daraus ergibt sich die Schlussfolgerung, dass überall eine wunderbare Ordnung zu finden ist. Als Beispiel nennt er die Zahlen, da dort keine Veränderungen vorgenommen wurden.

Dieses Sinnbild des christlichen Glaubens wollte Leibniz sogar zur Heidenbekehrung einsetzen. Andererseits meint Leibniz auch: „Alles weltliche Übel entsteht aus dem endlichen Wesen der Natur“. Allerdings sei die Unvollkommenheit ein notwendiges Teilübel. Letztlich sei die aktuale Welt die „bestmögliche aller Welten“. Leibniz’ populäre Darstellung vieler seiner Grundgedanken unter dem Titel „Theodizee“ behandelt u. a. diese Ausräumung von vermeintlich an Gott zu richtenden Einwendungen wegen der Unvollkommenheit der Welt und der erfahrenen Leiden.

Leibniz entwickelte die Monadentheorie als Gegenentwurf zu den zeitgenössischen Strömungen. Die Philosophen des 17. Jahrhunderts arbeiteten in der Regel entweder eine neue Substanztheorie aus oder sie entwickelten die Atomtheorie nach neuzeitlichen Maßstäben weiter. Leibniz befriedigte keine dieser Auffassungen. Er nennt die Philosophie der Atomisten eine „faule“ Philosophie, da diese Auffassung, welche die Atome als letzte Bausteine ansieht, die lebendige, sich verändernde Welt nicht tiefgründig genug analysiere. Entgegen atomistischen Zeit- und Raumauffassungen, die diese Existenzformen der Materie mit einem leeren Gefäß vergleichen, vertritt Leibniz eine dialektische Konzeption, in der Raum und Zeit Ordnungsbeziehungen in der materiellen Welt sind. Der Raum ist die Ordnung der zur gleichen Zeit existierenden Dinge, die Zeit die Ordnung ihrer kontinuierlichen Veränderungen.

Den Monadenbegriff greift er aus der neuplatonischen Tradition auf. Der Begriff "Monade", „Einheit“, stammt aus der "Stoicheiosis theologike" des spätantiken Philosophen Proklos.

Eine Monade – der zentrale Begriff der Leibniz’schen Welterklärung – ist eine einfache, nicht ausgedehnte und daher unteilbare Substanz, die äußeren mechanischen Einwirkungen unzugänglich ist.

Das gesamte Universum bildet sich in den von den Monaden spontan gebildeten Wahrnehmungen (Perzeptionen) ab. Sie sind eine Art spirituelle Atome, ewig, unzerlegbar, einzigartig. Leibniz vertritt somit eine panpsychistische Weltanschauung. Die Idee der Monade löst das Problem der Wechselwirkung von Geist und Materie, welches dem System René Descartes’ entspringt. Ebenso löst sie das Problem der Vereinzelung, welches im System Baruch Spinozas problematisch erscheint. Dort werden einzelne Lebewesen als bloß zufällige Veränderungen der einzigen Substanz beschrieben. Ein Beispiel: Eine Substanz kann ohne Denken existieren, aber das Denken nicht ohne Substanz.

Da Leibniz die Grundfrage der Philosophie idealistisch löst und die Materie für ihn nur ein „Anderssein der Seele“ ist, verwirft er den absoluten Charakter von Raum und Zeit. Raum und Zeit werden in der Leibniz’schen Metaphysik als Ordnungsbeziehungen zwischen Entitäten der materiellen Welt verstanden. Die Theorie der Substanz von Leibniz schließt die Möglichkeiten der allseitigen Entwicklungen ein. Obwohl die Monaden in ihren Keimen identisch sind, entwickeln sie sich verschieden. Entwicklung bedeutet nach Leibniz nicht das Entstehen von grundsätzlich Neuem, sondern nur die Entfaltung des Vorhandenen. Leib, Seele und Geist sind nicht grundsätzlich verschieden, sie sind bloß unterschiedlich entwickelt. Leibniz löst das Problem der Verbindung von Körper und Seele, indem er darlegt, dass alle Monaden, obwohl sie keinen gegenseitigen Einfluss auf ihre innere Struktur ausüben, koordiniert wirken. Er behauptet, dass Gott beim Schaffen der Monaden ihre Einheit und koordinierte Wirkung gesichert habe. Er kennzeichnet diesen Zustand mit dem Begriff der „prästabilierten Harmonie“. Trotz des idealistisch-teleologischen Wesens dieser Anschauung ist das Bemühen zu spüren, die Einheit der Welt nachzuweisen und die in ihr wirkenden Gesetzmäßigkeiten aufzudecken.

1667 veröffentlichte Leibniz eine Schrift zur Reform des Rechtswesens. Darin fordert er eine Vereinheitlichung der Gesetzeswerke der christlichen Nationen. Er versuchte, in jeder Religion etwas Wahres zu finden und dies in eine große "Harmonie", in eine allumfassende allgemeine Religion einzuordnen. Mit diesen Bemühungen begab er sich auf die Ebene eines Erasmus von Rotterdam, der ein ähnliches Ziel hatte, nämlich eine Gelehrtenrepublik zu erschaffen, in der antike und christliche Elemente verbunden werden, um Toleranz und Humanität zu fördern.
Leibniz bemühte sich zeit seines Lebens um den Frieden. Er versuchte 1670 zu einer Reunion von Katholiken und Protestanten beizutragen. Zwischen 1679 und 1702 führte er Verhandlungen mit den Bischöfen Spinola und Bossuet. Bis 1706 bemühte er sich ergebnislos um einen Zusammenschluss wenigstens der evangelischen Konfessionen. Diesen Bemühungen lag seine Ansicht zu Grunde, dass die Glaubensgemeinschaft eine unerlässliche Voraussetzung für die Bewahrung der abendländischen Kultur sei. Alle seine Anstrengungen konnten den Eigensinn der tief voneinander getrennten Länder nicht überwinden. Daran scheiterte Leibniz’ Streben nach Synthese und Harmonie.

Für Leibniz galt die Devise: „Ohne Gott ist nichts.“ Deshalb setzte er für Gott die Eins und für das Nichts die Null.
Gleichzeitig untersuchte er die Sprache und stellte fest, dass sie ständig Fehler zulässt. Dadurch entstehen enorme Verständigungsprobleme, die über kurz oder lang zu Konflikten führen. Leibniz setzte als Ziel seiner Forschungen die Lösung dieser Konflikte. Er meinte erkannt zu haben, dass unser Denken eigentlich ein Rechenvorgang sei, womit sich der Kreis zur Religiosität und jener von Gott und Nichts, von 1 und 0, schließt. Konsequenterweise versuchte er eine sichere logische Symbolsprache zu entwickeln (mathesis universalis). Dafür diskutierte er das Dualsystem entsprechend aus: es bildet die operationale Grundlage der modernen Computertechnik.
Außerdem erkannte Leibniz, dass man jedem Gegenstand eine charakteristische Zahl beilegen kann, ähnlich den arithmetischen Zeichen für die natürlichen Zahlen. Damit, so Leibniz, wollte Gott uns zeigen, dass unser Verstand noch ein weit tieferes Geheimnis birgt, von dem die Arithmetik nur ein Schattenbild ist.

Leibniz begründete eine Mathematik räumlicher Lage- und Ortsbeziehungen, die nicht wie die Algebra auf Zahlen und Größen basieren sollte, sondern auf rein qualitativen Eigenschaften. Er nannte seine neue Wissenschaft "Analysis situs", verwendete aber auch andere Bezeichnungen wie "Geometria situs", "Calculus situs", "Nouvelle characteristique" oder "Analyse géometrique". Daraus entstand später die Topologie, die von Johann Benedict Listing in Auseinandersetzung mit der Leibnizschen Analysis Situs entwickelt wurde.

Leibniz befasste sich intensiv mit Logik und propagierte erstmals eine symbolische Logik in Kalkülform. Seine Logikkalkül-Skizzen veröffentlichte er allerdings nicht; erst sehr verspätet (1840, 1890, 1903) wurden sie publiziert. Seine charakteristischen Zahlen aus dem Jahr 1679 sind ein arithmetisches Modell der Logik des Aristoteles. Seinen Hauptkalkül entwickelte er in den "Generales Inquisitiones" von 1686. Er entwarf dort die erste Gleichungslogik und leitete in ihr fast zwei Jahrhunderte vor der Boole-Schule die Gesetze der booleschen Verbandsordnung ab. Innerhalb dieses Kalküls formulierte er die traditionelle Begriffslogik bzw. Syllogistik auf gleichungslogischer Grundlage. Er erfand die Mengendiagramme lange vor Leonhard Euler und John Venn und stellte mit ihnen die Syllogistik dar.

Das Leibniz’sche Gesetz geht auf ihn zurück.

Während eines Parisaufenthalts in den Jahren 1672 bis 1676 trat Leibniz in Kontakt zu führenden Mathematikern seiner Zeit. Ohne sichere theoretische Grundlage lernte man damals, unendliche Folgen und Reihen aufzusummieren. Leibniz fand ein Kriterium zur Konvergenz alternierender Reihen (Leibniz-Kriterium), aus dem insbesondere die Konvergenz der sogenannten Leibniz-Reihe
folgt. Mittels geometrischer Überlegungen fand er auch deren Grenzwert formula_2. Durch Summation von Reihen gelangte Leibniz 1675 zur Integral- und von dort zur Differentialrechnung; er dokumentierte seine Betrachtungen 1684 mit einer Publikation in den "acta eruditorum". Nach heutigen Maßstäben (Priorität der Erstveröffentlichung) würde er als alleiniger Urheber der Infinitesimalrechnung gelten; diese Interpretation ist jedoch anachronistisch, da wissenschaftliche Kommunikation im 17. Jahrhundert primär mündlich, über den Zugang zu Manuskripten und per Briefwechsel erfolgte. Bleibendes Verdienst von Leibniz ist insbesondere die heute noch übliche Notation von Differentialen (mit einem Buchstaben "d" von lat. "differentia"), Differentialquotienten formula_3 und Integralen (formula_4; das Integralzeichen ist abgeleitet aus dem Buchstaben "S" von lat. "summa").

Der englische Naturwissenschaftler Sir Isaac Newton hatte die Grundzüge der Infinitesimalrechnung bereits 1666 entwickelt. Jedoch veröffentlichte er seine Ergebnisse erst 1687. Daraus entstand Jahrzehnte später der vielleicht berühmteste Prioritätsstreit der Wissenschaftsgeschichte. Die ersten Pamphlete, in denen Leibniz beziehungsweise Newton beschuldigt wurden, den jeweils anderen plagiiert zu haben, erschienen 1699 und 1704. Im Jahr 1711 brach der Streit in voller Schärfe aus. Die Royal Society verabschiedete 1712 einen Untersuchungsbericht, der von Newton selbst fabriziert worden war; Johann Bernoulli antwortete 1713 mit einem persönlichen Angriff auf Newton. Der Streit wurde über Leibniz’ Tod hinaus fortgeführt und vergiftete die Beziehungen zwischen englischen und kontinentalen Mathematikern über mehrere Generationen hinweg. Schaden nahm vor allem die Entwicklung der Mathematik in England, die lange an den technisch unterlegenen newtonschen Notationen festhielt. Heute ist sich die Forschung einig, dass Leibniz und Newton ihre Kalküle unabhängig voneinander entwickelt haben.

Bei der Beschäftigung mit der Matrizen-Rechnung fand der Mathematiker die so genannte Leibniz-Formel zur Berechnung der Determinante für eine allgemeine formula_5-Matrix:

Er entwickelte auch die Dyadik (Dualsystem) mit den Ziffern 0 und 1 (Dualzahlen), die für die moderne Computertechnik von grundlegender Bedeutung ist.

Viele bedeutende Erfindungen stammen von Leibniz, zum Beispiel eine Rechenmaschine sowie Erfindungen zur Nutzung des Windes bei der Grubenentwässerung im Oberharzer Bergbau.
Leibnizens Rechenmaschine (von der es fünf aufeinanderfolgende Versionen gibt) war ein historischer Meilenstein im Bau von mechanischen Rechenmaschinen. Das von ihm erfundene Staffelwalzenprinzip, mit dem Multiplikationen auf mechanische Weise realisiert werden konnten, hielt sich über 200 Jahre als unverzichtbare Basistechnik. Die feinmechanischen Probleme, die es beim Bau einer solchen Maschine zu überwinden galt, waren jedoch so immens, dass berechtigte Zweifel daran bestehen, ob zu Leibnizens Lebzeiten jemals eine fehlerfrei arbeitende Maschine realisiert werden konnte. Eine fehlerfrei arbeitende Replik nach Leibnizens Konstruktionsplan konnte erst 1990 durch Nikolaus Joachim Lehmann (Dresden) realisiert werden.
Zitat von Leibniz:

Im weiteren Sinne war Leibniz wegbereitend für die Rechenmaschine im heutigen Sinne, den Computer. Er entdeckte, dass sich Rechenprozesse viel einfacher mit einer binären Zahlencodierung durchführen lassen, und ferner, dass sich mittels des binären Zahlencodes die Prinzipien der Arithmetik mit den Prinzipien der Logik verknüpfen lassen (siehe "De progressione Dyadica", 1679; oder "Explication de l’Arithmetique Binaire", 1703). Die hier erforschten Prinzipien wurden erst 230 Jahre später in der Konstruktion von Rechenmaschinen eingesetzt (z. B. bei der Zuse Z1).
Leibniz hatte beim Bau einer Rechenmaschine, anders als frühere Erfinder, eher philosophische Motive. Mit dem viel bemühten Zitat, es sei „ausgezeichneter Menschen unwürdig, gleich Sklaven Stunden zu verlieren mit Berechnungen“, wird eine Grenze zwischen Mensch und Maschine gezogen. Dem Erfindergeist (Freiheit, Spontaneität und Vernunft) als das spezifisch Menschliche wird das Mechanische der technisch-natürlichen Kausalität gegenübergestellt. Leibniz Erfindung sollte daher eng im Zusammenhang mit den etwa zeitgleich erschienenen Arbeiten zur Monadologie gesehen werden, statt in Verbindung mit praktischen, d. h. kaufmännischen, technischen und mathematischen Interessen.

Eine weitere „Rechenmaschine“ von Leibniz blieb Konzept: die "Machina deciphratoria", ein kryptologisches Gerät. Bereits in den späten 1670er-Jahren hat er die Chiffriermaschine erfunden, allerdings erst 1688 in einem Schriftsatz für eine Audienz bei Kaiser Leopold I. in Wien beschrieben. „Damit nahm er um reichlich 200 Jahre das Prinzip der Rotor-Schlüsselmaschine von Arvid Damm (1869–1927) vorweg, nach dem die erste Generation der mechanischen Chiffriermaschinen (ab 1918) funktionierte.“

In den Jahren 2010–2011 hat Nicholas Rescher das Prinzip aus Leibnizens Aufzeichnungen rekonstruiert und Klaus Badur den Entwurf in Detailkonstruktionen umgesetzt, aufgrund der das funktionierende Gerät 2014 von der Firma G. Rottstedt in Garbsen gebaut wurde. Kaiser Leopold hat Leibnizens Angebot nicht weiter erwogen, da seine Berater ihre damaligen Verfahren (fälschlich) für sicher hielten.

Zur Funktionsweise:
„Für die leibnizsche Maschine besteht der Schlüssel aus a) einem Sortiment von sechs Chiffrieralphabeten, die mitsamt den zugehörigen Dechiffrieralphabeten auf die Trommel aufzubringen sind; b) der Angabe, welches von zwölf möglichen Lückenzahnrädern zum Einsatz kommt; c) der Anfangsposition dieses Lückenzahnrads. Für die sechs Chiffrieralphabete hat man im Prinzip die Auswahl aus 26! = 1 × 2 × … × 26 ≈ 4 × 10 Möglichkeiten. Realistischerweise hätte man dem Diplomaten wohl kaum mehr als 50 Alphabetpaare in den Geheimkoffer gegeben. Aber solange der Spion nicht an den Koffer kommt, muss er das komplette Sortiment der Möglichkeiten in Betracht ziehen. Und selbst mit 50 Alphabetpaaren bleiben 50!/(50 – 6)! = 11.441.304.000 Möglichkeiten, sie auf der Trommel zu montieren – die Reihenfolge der Streifen mitgerechnet.“

Leibniz verwendete noch nicht das Wort psychologia, das von Johann Thomas Freigius eingeführt wurde, sondern schrieb hauptsächlich in seiner "Monadologie" und in seinen "Neuen Essays" über psychologische Themen. Innerhalb der philosophischen Fachliteratur gibt es jedoch kaum Bezüge zu diesem Interessengebiet von Leibniz.

Mit den Begriffen Perzeption und Apperzeption bezeichnete Leibniz den Übergang von einer unbemerkten Sinnesempfindung zur bewussten Wahrnehmung und deren Aufnahme in das individuelle (Selbst-)Bewusstsein. Dies entspricht seiner Lehre vom Kontinuitätsgesetz der minimalen Veränderungen in der Physik. Die Annahme eines Kontinuums mit bestimmten Sinnesschwellen wurde ein Leitgedanke der experimentellen Psychologie und Psychophysik von Gustav Theodor Fechner und Wilhelm Wundt. Zum Verhältnis von Seelischem und Körperlichem in ihrem parallelen Ablauf (Psychophysischer Parallelismus) postulierte Leibniz, dass seelische Vorgänge dem Zweckprinzip (Teleologie), körperliche Vorgänge dem Kausalprinzip folgen. Er schrieb: „Die Seelen handeln gemäß den Gesetzen der Zweckursachen durch Strebungen, Ziele und Mittel. Die Körper handeln gemäß den Gesetzen der Wirkursachen oder der Bewegungen. Und die zwei Reiche, das der Wirkursachen und das der Zweckursachen, stehen miteinander in Harmonie.“ 

Leibniz‘ Prinzipienlehre und seine Lehre von der Willenstätigkeit (Streben, Verlangen) hatten, abgelöst von den theologischen Letztbegründungen, einen grundlegenden Einfluss auf das Denken und auf die Forschung Wilhelm Wundts, der international als der wichtigste Begründer der modernen Psychologie gilt. Leibniz prägte den Begriff der Perspektive (Perspektivismus) am Beispiel verschiedener Ansichten derselben Stadt. Die Bereitschaft und Fähigkeit zum Perspektivenwechsel, d. h. einander wechselseitig ergänzender Betrachtung, bilden einen Grundgedanken von Wundts Wissenschaftstheorie der Psychologie.

Im Streit um das historische Ausgangsgebiet der germanischen Sprachen bzw. Völker vertraten schwedische Forscher wie Olof Rudbeck d. Ä. bereits im 17. Jahrhundert die Theorie, Skandinavien sei die „Urheimat“ der Germanen ("Gothizismus"). Leibniz widersprach dieser Theorie im Jahre 1696 in seiner "Dissertatio de origine Germanorum", wobei er – seiner Zeit weit voraus – mit dem Befund der Gewässernamen (Hydronymie) argumentierte. In dem bis heute nicht abschließend entschiedenen Gelehrtenstreit neigt seit einiger Zeit (wieder) eine wachsende Zahl von Prähistorikern (u. a. Rolf Hachmann) und Linguisten (u. a. Jürgen Udolph, Wolfram Euler) der von Leibniz vertretenen Position zu.

Im Zusammenhang mit der Auseinandersetzung über die Herkunft des Germanischen widersprach Leibniz zudem der von zeitgenössischen schwedischen Gelehrten vertretene Ansicht, ein archaisches Schwedisch sei die Urform der germanischen Sprachen. Ebenso wies er die damals noch weit verbreitete Ansicht zurück, Hebräisch sei die erste Sprache der gesamten Menschheit. Im Zuge seiner intensiven philologischen Studien arbeitete er über die Frage der Ursprünge der slawischen Sprachen, erkannte die linguistische Bedeutung des Sanskrit und war vom klassischen Chinesisch fasziniert.

Als im Juni 1692 in einem Steinbruch bei Thiede, heute ein Stadtteil von Salzgitter, ein riesiges prähistorisches Skelett freigelegt wurde, wies Leibniz anhand eines Zahnes nach, dass man nicht die Überreste eines „Riesen“, sondern das Knochengerüst eines Mammuts oder See-Elefanten gefunden habe.

In seinem 1749 posthum veröffentlichten Werk „Protogaea“ gilt Leibniz als Pionier der Höhlenkunde und als Mitbegründer der Paläontologie, da er darin Fossilien nicht als Naturspiele betrachtete, sondern als Versteinerungen früherer Organismen, die durch große Umwälzungen in anderen Teilen der Erde verschwunden seien oder verändert wurden. Leibniz hatte auch erste Vorstellungen zu einem evolutiven Artenwandel und vermutete beispielsweise, dass die verschiedenen Raubkatzenarten von einer gemeinsamen ursprünglichen Katzenart abstammen könnten. Einige Stücke seiner Sammlungen haben sich bis heute an der Universität Göttingen erhalten.

Leibniz war einer der wichtigsten interdisziplinären Gelehrten seiner Epoche. Ein großer Teil seines Wirkens ist in Briefen dokumentiert. Aus der Zeit zwischen 1663 und 1716 sind über 20.000 Briefe an Leibniz überliefert, die er von rund 1.100 Korrespondenten aus 16 Ländern erhalten hat. Im Leibniz-Archiv sind rund 15.000 Briefe dokumentiert. Zu seinen Korrespondenten zählen die Naturwissenschaftler Christiaan Huygens und Lambert van Velthuysen, Juristen und Staatsmänner wie Samuel von Pufendorf, Magnus von Wedderkop oder Veit Ludwig von Seckendorff, die Philosophen Thomas Hobbes, Baruch de Spinoza und Jakob Thomasius und der Theologe Antoine Arnauld. Er war ein „homo societatis“. Leibniz erhielt nach der Erfindung seiner Rechenmaschine eine Zeichnung von dem Jesuiten Joachim Bouvet, der sich damals in China unter anderem mit dem Studium des I Ging beschäftigte, und auf welcher die Anordnung der Hexagramme nach Fu Hi (mythische Figur vor der Zeitrechnung) gezeigt ist. Auf dieser Zeichnung trug Leibniz handschriftlich die numerischen Zahlen korrespondierend zu dem abgebildeten Binärcode ein.

Zu den aufschlussreichsten Quellen zur Arbeit, den persönlichen und finanziellen Verhältnissen Leibniz’ sowie der Situation am kurfürstlichen Hof von Hannover gehört der intensive Briefwechsel mit seinem Amanuensis Johann Friedrich Hodann (1674–1745), dem in den Zeiten der oft jahrelangen Abwesenheit von Hannover die Aufsicht und Verwaltung des Hauses in der Schmiedestraße und der Gärten übertragen wurde.


Das Deutsche Nominierungskomitee hat den in der Gottfried Wilhelm Leibniz Bibliothek in Hannover aufbewahrten Briefwechsel von "Gottfried Wilhelm Leibniz" 2006 für das UNESCO-Programm Weltdokumentenerbe (Memory of the World) vorgeschlagen. Im Herbst 2007 entschied der Generaldirektor der UNESCO abschließend über den Neueintrag in das Memory of the World-Register. Damit erklärte die UNESCO den Briefwechsel als Bestandteil des Weltgedächtnisses und damit als besonders schützenswert.

Der Briefwechsel enthält rund 15.000 Briefe mit 1.100 Korrespondenten. Er ist Bestandteil des in Hannover aufbewahrten Leibniz-Nachlasses mit ca. 50.000 Nummern mit rund 200.000 Blättern. Zum Nachlass gehören auch die Bibliothek von Leibniz und das einzig erhaltene Exemplar der von ihm konstruierten Vier-Spezies-Rechenmaschine.

Das Leibnizdenkmal im Innenhof der Universität Leipzig zeigt Leibniz als überlebensgroße Statue.

In Hannover erinnern zwei Denkmäler im öffentlichen Raum an Leibniz. Der Leibniztempel im Georgengarten wurde 1790 geweiht; es handelt sich um das erste einem Nichtadeligen gewidmete Bauwerk in Deutschland. Am Opernplatz befindet sich ein von Stefan Schwerdtfeger geschaffener bronzener Schattenriss von Leibniz’ Kopf, der am 27. November 2008 eingeweiht wurde. Die 2,5 m hohe Skulptur auf einem Granitsockel wurde von 10 Sponsoren für 110.000 Euro gestiftet. Eine Seite bildet das Leibniz-Zitat "Unitas in multitudine" (Einheit der Vielfalt) ab, die andere Seite zeigt das von Leibniz entwickelte binäre Zahlensystem.

















Werke


</doc>
<doc id="1832" url="https://de.wikipedia.org/wiki?curid=1832" title="Garbage Collection">
Garbage Collection

Die Garbage Collection, kurz GC ( für "Müllabfuhr", auch automatische Speicherbereinigung oder Freispeichersammlung genannt) bezeichnet in der Software- und Informationstechnik eine automatische Speicherverwaltung, die den Speicherbedarf eines Computerprogramms minimiert. Dabei wird zur Laufzeit versucht, nicht länger benötigte Speicherbereiche automatisch zu identifizieren, um diese dann freizugeben. Manche automatische Speicherbereinigungen führen darüber hinaus die noch verwendeten Speicherbereiche zusammen ("Defragmentierung"). Wesentlicher Vorteil von GC gegenüber manueller Speicherverwaltung ist die Vermeidung von Speicherproblemen, der Vorteil wird mit einem erhöhten Ressourcenverbrauch erkauft.

In vielen Softwaresystemen wird benötigter (Arbeits-)Speicher dynamisch (bei Bedarf) reserviert. Wird er nach Abarbeitung eines Programmteils nicht weiter verwendet, so sollte der Speicher wieder freigegeben werden, um eine Wiederverwendung dieser Ressource zu ermöglichen. Bei einer expliziten, manuellen Speicherverwaltung geschieht dies durch Festlegen der Speicherreservierung und -freigabe im Programm durch den Programmierer, ein schnell komplex und damit potenziell fehlerträchtig werdendes Vorgehen. Neben dem Vergessen einer Freigabe, das längerfristig zu Speicherknappheit führen kann, führt das zu frühe Freigeben von (anderswo) noch benötigtem Speicher meist schnell zum Programmabsturz. Vergessene Speicherfreigaben führen oft nicht sofort zu Auffälligkeiten im Programmablauf - zumindest nicht während der typischerweise nur kurzen Programmläufe während der Entwicklung, sondern erst, wenn das fertige Programm vom Endanwender oft über Stunden und Tage ununterbrochen betrieben wird.

Bei manueller Speicherverwaltung ist es oft nicht möglich oder sehr aufwendig, den Speicher zu defragmentieren. Stark fragmentierter Speicher kann dazu führen, dass eine Speicherreservierung des Programms fehlschlägt, da kein ausreichend großer zusammenhängender Bereich verfügbar ist.

Bei der automatischen Speicherbereinigung ist die Idee, diese Aufgabe durch eine "Garbage Collector" genannte Routine automatisch erledigen zu lassen, ohne Zutun des Programmierers. D. h. das Speichermanagement wird von einer expliziten Festlegung zur Programmerstellungszeit ("Compile-Zeit") zu einer dynamischen Analyse des Speicherbedarfs zur "Laufzeit" des Programms verschoben.

Üblicherweise läuft eine solche automatische Speicherbereinigung im Hintergrund (bzw. nebenläufig) in mehr oder minder regelmäßigen Zeitabständen (z. B. während Pausen im Programmablauf) und wird nicht explizit durch das Programm ausgelöst. GC kann jedoch häufig auch zusätzlich direkt ausgelöst werden, um dem Programm etwas Kontrolle über die Bereinigung zu geben, z. B. in einer Situation von Speichermangel ("Out-Of-Memory").

Es gibt verschiedene Ansätze, um eine automatische Speicherbereinigung zu implementieren. Gewünschte Anforderungen können ein möglichst geringer Speicherverschnitt, eine maximale Allozierungsgeschwindigkeit, eine Reduktion der Speicherfragmentierung und viele weitere mehr sein, die sich durchaus auch widersprechen und zu Zielkonflikten führen können. D. h. je nach Anwendungsfall kann eine automatische Speicherbereinigung sehr unterschiedlich aussehen und sicher viele Anforderungen erfüllen, manche aber auch nicht.

Typischerweise werden jedoch alle diese Varianten zwei Grundtypen von Speicherbereinigungen zugeordnet: Konservative und nicht-konservative Speicherbereinigung.

Unter einer konservativen automatischen Speicherbereinigung versteht man eine, die "nicht" zuverlässig alle nicht-referenzierten Objekte erkennen kann. Diese hat meistens keine Informationen darüber, wo sich im Speicher Referenzen auf andere Objekte befinden. Zur Speicherbereinigung muss sie den Speicher auf "mögliche" Referenzen durchsuchen. Jede Bitfolge, die eine gültige Referenz in den Speicher sein könnte, wird als Referenz angenommen. Es kann dabei nicht festgestellt werden, ob es sich dabei nicht doch um ein Zufallsmuster handelt. Daher erkennen konservative Kollektoren gelegentlich Objekte als referenziert, obwohl sie es eigentlich nicht sind. Da eine automatische Speicherbereinigung niemals Objekte entfernen darf, die noch gebraucht werden "könnten", muss sie konservativ annehmen, dass es sich bei der erkannten Bitfolge um eine Referenz handelt.

Insbesondere wenn eine automatische Speicherbereinigung auch dringlichere Ressourcen als Speicher freigeben muss (siehe Finalisierung), kann ein konservativer Kollektor ein Risiko darstellen. Im Allgemeinen findet man konservative GCs dort, wo interne Pointer (also Pointer auf unterschiedliche Teile eines Objektes) erlaubt sind, was eine Implementierung der automatischen Speicherverwaltung erschwert. Beispiele dafür sind die Sprachen C und C++. Hier sei anzumerken, dass dies nicht für die "„verwalteten Typen“" in C++/CLI gilt, da dort eigene Referenztypen für die automatische Speicherbereinigung eingeführt wurden, die es nicht erlauben, direkt die Adresse eines Objekts auszulesen.

Unter einer nicht-konservativen automatischen Speicherbereinigung (manchmal auch als "„exakte Speicherbereinigung“" bezeichnet) versteht man eine, der Metadaten vorliegen, anhand derer sie alle Referenzen innerhalb von Objekten und Stackframes auffinden kann. Bei nicht-konservativer Speicherbereinigung wird zwischen Verfolgung ("tracing garbage collectors") und Referenzzählung unterschieden.

Bei diesem Verfahren der Speicherbereinigung wird von bekanntermaßen noch benutzten Objekten ausgehend allen Verweisen auf andere Objekte gefolgt. Jedes so erreichte Objekt wird markiert. Anschließend werden alle nicht markierten Objekte zur Wiederverwendung freigegeben.

Die Freigabe kann zur Speicherfragmentierung führen. Das Problem ist hierbei jedoch etwas geringer als bei manueller Speicherverwaltung. Während bei manueller Speicherverwaltung die Deallozierung immer sofort erfolgt, werden bei Mark-and-Sweep fast immer mehrere Objekte auf einmal beseitigt, wodurch größere zusammenhängende Speicherbereiche frei werden können.

Der "Mark-and-Compact-Algorithmus" benutzt ebenso wie "Mark-and-Sweep" das Prinzip der Erreichbarkeit in Graphen, um noch referenzierte Objekte zu erkennen. Diese kopiert er an eine andere Stelle im Speicher. Der ganze Bereich, aus dem die noch referenzierten (man spricht hier auch von „lebenden“) Objekte herauskopiert wurden, wird nun als freier Speicherbereich betrachtet.

Nachteil dieser Methode ist das Verschieben der „lebenden“ Objekte selber, denn Zeiger auf diese werden ungültig und müssen angepasst werden. Hierzu gibt es grundsätzlich wenigstens zwei Verfahren:

Das Verschieben der Objekte hat allerdings den Vorteil, dass jene, die die Bereinigung „überlebt“ haben, nun alle kompaktiert zusammenliegen und der Speicher damit praktisch defragmentiert ist. Auch ist es möglich, sehr schnell zu allozieren, weil freier Speicherplatz nicht aufwändig gesucht wird. Anschaulich: Werden die referenzierten Objekte an den „Anfang“ des Speichers verschoben, kann neuer Speicher einfach am „Ende“, hinter dem letzten lebenden Objekt, alloziert werden. Das Allozieren funktioniert damit vergleichsweise einfach, ähnlich wie beim Stack.

Generationelle GCs verkürzen die Laufzeit der Speicherfreigabe. Dazu wird die Situation ausgenutzt, dass in der Praxis die Lebensdauer von Objekten meist sehr unterschiedlich ist: Auf der einen Seite existieren Objekte, die die gesamte Laufzeit der Applikation überleben. Auf der anderen Seite gibt es eine große Menge von Objekten, die nur temporär für die Durchführung einer einzelnen Aufgabe benötigt werden. Der Speicher wird bei generationellen GCs in mehrere Teilbereiche (Generationen) aufgeteilt. Die Langlebigkeit wird durch einen Zähler quantifiziert, welcher bei jeder Garbage-Collection inkrementiert wird. Mit jeder Anwendung des Freigabe-Algorithmus (zum Beispiel Mark-and-Compact oder Stop-And-Copy) werden langlebige Objekte in eine höhere Generation verschoben. Der Vorteil liegt darin, dass die Speicherbereinigung für niedrige Generationen häufiger und schneller durchgeführt werden kann, da nur ein Teil der Objekte verschoben und deren Zeiger verändert werden müssen. Höhere Generationen enthalten mit hoher Wahrscheinlichkeit nur lebende (bzw. sehr wenige tote) Objekte und müssen deshalb seltener bereinigt werden.

Die Anzahl der Generationen wird heuristisch festgelegt (zum Beispiel drei in .NET, zwei für junge Objekte (auch Young-Generation genannt) und einer für alte Objekte (Tenured-Generation) in der Java-VM von Sun).
Zudem können für jede Generation unterschiedliche Algorithmen verwendet werden. In Java beispielsweise wird für die niedrigste Generation ein modifizierter Stop-And-Copy-Algorithmus angewandt, für die höhere Mark-And-Compact.

Bei diesem Verfahren führt jedes Objekt einen Zähler mit der Anzahl aller Referenzen, die auf dieses Objekt zeigen. Fällt der Referenzzähler eines Objektes auf null, so kann es freigegeben werden.

Ein besonderes Problem der Freispeichersammlung mit Referenzzählung liegt in so genannten "zyklischen Referenzen", bei denen Objekte Referenzen aufeinander halten, aber sonst von keinem Konsumenten im System mehr verwendet werden. Nehmen wir beispielsweise an, Objekt A halte eine Referenz auf Objekt B und umgekehrt, während der Rest des Systems ihre Dienste nicht mehr benötigt. Somit verweisen beide Objekte gegenseitig "(zyklisch)" aufeinander, weshalb die automatische Speicherbereinigung nicht ohne weiteres erkennen kann, dass sie nicht mehr benutzt werden. Die Folge hiervon ist, dass der Speicher somit für die Dauer der Programmausführung belegt bleibt. Es gibt unterschiedliche Algorithmen, die solche Situationen erkennen und auflösen können, zumeist nach dem Prinzip der "Erreichbarkeit in Graphen".

Mit einer Garbage Collection können einige häufig auftretende Programmierfehler, die beim Umgang mit dynamischer Speicherverwaltung oft gemacht werden, ganz oder zumindest teilweise vermieden werden. Besonders zu erwähnen sind hierbei Speicherlecks, die doppelte Freigabe von Ressourcen und die Dereferenzierung von versehentlich zu früh freigegebenen Ressourcen (Hängende Zeiger). Eine Freigabe noch referenzierter Objekte führt zu hängenden Zeigern, welche oft zu Programmabstürzen und undeterministischem Verhalten führen.

Als Folge des Satzes von Rice kann nicht festgestellt werden, ob noch referenzierte Objekte jemals wieder benutzt werden. Darum gibt eine automatische Speicherbereinigung nur vom Programm nicht mehr referenzierte Objekte frei; sie verhindert keine „Speicherlecks“ der Sorte, dass das Programm auf den Speicherbereich noch eine Referenz hält, den Inhalt jedoch nie wieder nutzt. Derartige Speicherlecks stellen normalerweise Logische Fehler oder Designfehler (Fehler im Grundkonzept, falsche Anforderungen an die Software, Softwaredesign-Fehler) dar und können auch bei nicht-automatischer Speicherverwaltung entstehen.

Zusätzlich behebt Garbage Collection das Problem der Speicherfragmentierung, das kein Programmierfehler im eigentlichen Sinne ist, jedoch auf ungünstigem Programmdesign basieren kann. Dieses Problem kann zu nur schwer reproduzierbaren Programmabstürzen führen.
Das Problem der Speicherfragmentierung wird von explizitem/manuellem Speichermanagement im Allgemeinen nicht gelöst.

Ob eine automatische Speicherbereinigung Programme insgesamt beschleunigt oder ausbremst, ist umstritten. In einigen Kontexten, wie z. B. wenn Speicher erst dann freigegeben wird, wenn die Systemanforderungen gerade niedrig sind oder wenn die Speicherverwaltung des Systems durch Defragmentierung entlastet wird, kann sie zu Leistungssteigerungen führen. Es existieren Microbenchmarks, welche belegen, dass bei Programmiersprachen mit automatischer Speicherbereinigung die Anlage/Freigabe von Objekten in Summe schneller vonstattengeht als ohne, jedoch auch Microbenchmarks, die insgesamt einen überwiegend negativen Einfluss auf die Leistungsfähigkeit sehen. Eine Veröffentlichung von 2005 gibt an, dass die Leistungsfähigkeit von Garbage Collection nur dann gleich gut wie oder leicht besser als beim expliziten Speichermanagement sei, wenn der Garbage Collection fünfmal so viel Speicher zusteht, wie tatsächlich benötigt wird. Bei dreimal so viel Speicher liefe Garbage Collection im Schnitt 17 % langsamer, bei doppelt so viel Speicher 70 % langsamer als bei explizitem Speichermanagement.

Beim Speicherverbrauch führt eine automatische Speicherverwaltung und -bereinigung zu einem Overhead gegenüber einem expliziten, händischen Speichermanagement aufgrund der zeitverzögerten Bereinigung. Eine wissenschaftliche Veröffentlichung von 1993 schätzt den Overhead bei "konservativer" Speicherbereinigung auf typischerweise 30–150 %. Andererseits ist eine korrekte Implementierung manueller Speicherfreigabe in nicht trivialen Programmen komplex umzusetzen, was Fehlerquellen für Speicherlecks bei manueller Speicherfreigabe schafft. Beispielsweise kann die oft angewandte Methode der Referenzzählung keine zyklischen Referenzen erkennen und führt ohne Ergänzung durch komplexe Algorithmen zu Speicherlecks.

Indem der Programmierer die Entscheidung über den Freigabezeitpunkt nicht explizit festlegt, gibt er auch einen Teil der Kontrolle über den Programmfluss auf. Da die automatische Speicherbereinigung i. d. R. nebenläufig stattfindet, hat das Programm selbst keine Information darüber, wann Speicherbereiche wirklich freigegeben bzw. Objekte finalisiert werden. Dadurch ist der Programmfluss potentiell nicht mehr deterministisch.

Konkret können folgende Formen nicht-deterministischen Verhaltens auftreten:

Mittels kompaktierender Algorithmen kann Garbage Collection eine Fragmentierung des Speichers verhindern. Siehe dazu Mark and Compact. Damit werden Lücken im Speicher vermieden, die aufgrund zu großer neuer Objekte nicht aufgefüllt werden könnten. Defragmentierung führt zwar zu einer längeren Verzögerung beim Freigeben von Speicher, reduziert allerdings die Allozierungsdauer. Um die Speicherfreigabe möglichst schnell durchführen zu können, wird darauf geachtet, dass möglichst selten große Speicherbereiche aufgeräumt werden müssen. Deshalb werden diese Algorithmen bevorzugt in Kombination mit generationellen Verfahren eingesetzt.

Defragmentierung des Speichers führt zu folgenden Vorteilen:

Als "Finalisierung" () bezeichnet man in objekt-orientierten Programmiersprachen eine spezielle Methode, die aufgerufen wird, wenn ein Objekt durch den "Garbage Collector" freigegeben wird.

Anders als bei Destruktoren sind Finalisierungsmethoden nicht deterministisch: Ein Destruktor wird aufgerufen, wenn ein Objekt explizit durch das Programm freigegeben wird. Die Finalisierungsmethode wird jedoch erst aufgerufen, wenn der Garbage Collector entscheidet, das Objekt freizugeben. Abhängig vom Garbage Collector kann dies zu einem beliebigen Zeitpunkt geschehen, wenn festgestellt wird, dass das Programm das Objekt nicht mehr verwendet – möglicherweise auch nie bzw. am Ende der Laufzeit (siehe auch Abschnitt #Determinismus).

Die Finalisierung kann in der Praxis zu Problemen führen, wenn sie für die Freigabe von Ressourcen verantwortlich ist:

In der Programmiersprache Java verfügen Objekte über eine spezielle Methode namens finalize(), die für diesen Zweck überschrieben werden kann. Aus den oben genannten Gründen wird für Java empfohlen, komplett auf Finalisierung zu verzichten und stattdessen eine explizite Terminierungsmethode zu verwenden. Der automatischen Speicherbereinigung fällt dann also ausschließlich die Aufgabe der Speicherverwaltung zu.

Einige ältere (APL, LISP, BASIC) und viele neuere Programmiersprachen verfügen über eine integrierte automatische Speicherbereinigung.

Für Programmiersprachen wie C, bei denen die Programmierer die Speicherverwaltung von Hand erledigen müssen, gibt es teilweise Bibliotheken, die eine automatische Speicherbereinigung zur Verfügung stellen, was bei der Programmierung aber leicht umgangen werden kann, beziehungsweise bei systemnaher Programmierung sogar umgangen werden muss. Aus diesem Grund können in einigen Programmiersprachen systemnah programmierte Module von der automatischen Speicherbereinigung ausgenommen werden, indem sie explizit gekennzeichnet werden (zum Beispiel in C# mit der Option "/unsafe" oder in Component Pascal mit der obligatorischen Anweisung "IMPORT SYSTEM").

Weitere Beispiele für Programmiersprachen mit einer automatischen Speicherverwaltung sind Smalltalk, Haskell, Oberon, Python, Ruby, OCaml, Perl, Visual Objects, ABAP, Objective-C (ab Version 2.0), D sowie alle Sprachen, die auf der Java Virtual Machine (JVM) ablaufen (Java, Groovy, Clojure, Scala, …) sowie die für die Common Language Runtime von .NET entwickelt wurden (zum Beispiel C# oder VB.NET).

Apple führte 2007 mit der Veröffentlichung von Mac OS X Leopard (10.5) Garbage Collection als die „wichtigste Veränderung“ für Objective-C 2.0 ein, die gemäß Apple „Objective-C dieselbe Leichtigkeit der Speicherverwaltung wie bei anderen modernen Sprachen“ brachte. 2012 mit OS X Mountain Lion (10.8) wurde allerdings Garbage Collection als veraltet deklariert und die Verwendung des mit Mac OS X Lion (10.7) eingeführten automatischen Referenzzählungsmechanismus (engl. Automatic reference counting, ARC) zur Kompilierungszeit auf Basis des gerade eingeführten CLANG/LLVM 3.0 Compilers forciert. Bei dieser automatisierten Referenzzählung wird durch den Compiler Code zum Erkennen und Entfernen nicht mehr benötigter Objekten mittels Referenzzählung an geeigneten Stellen eingebaut. Im Gegensatz zu GCs mit Referenzzählung läuft die automatisierte Referenzzählung seriell und an zur Compilezeit festgelegten Zeitpunkten und damit deterministisch. Allerdings enthält ARC keine Möglichkeit, zyklische Referenzen zu erkennen; Programmierer müssen daher die Lebensdauer ihrer Objekte explizit managen und Zyklen manuell auflösen oder mit schwachen oder unsicheren Referenzen arbeiten.

Laut Apple haben Mobil-Apps ohne GC eine bessere und vorhersagbarere Leistungsfähigkeit. Das GC-freie iOS als Basis ermöglicht Apple, mobile Geräte mit weniger Speicher als die GC-basierende Konkurrenz zu bauen, welche trotzdem eine gleiche oder bessere Leistungsfähigkeit und Akku-Laufzeit aufweisen; ein Ansatz, der auch in der Fachpresse als architektonischer Vorteil beschrieben wurde.




</doc>
<doc id="1833" url="https://de.wikipedia.org/wiki?curid=1833" title="Galileo Galilei">
Galileo Galilei

<onlyinclude>

Galileo Galilei (* 15. Februar 1564 in Pisa; † in Arcetri bei Florenz) war ein italienischer Universalgelehrter. Er war Philosoph, Mathematiker, Ingenieur, Physiker, Astronom und Kosmologe. Viele seiner Entdeckungen, vor allem in der Mechanik und der Astronomie, gelten als bahnbrechend. Er entwickelte die Methode, die Natur durch die Kombination von Experimenten, Messungen und mathematischen Analysen zu erforschen und wurde damit einer der wichtigsten Begründer der neuzeitlichen exakten Naturwissenschaften. Berühmt wurde er auch dadurch, dass die katholische Kirche ihn verurteilte, was sie erst 1992 widerrief.</onlyinclude>

Galileo Galilei stammte aus einer verarmten Florentiner Patrizierfamilie. Sein Familienzweig hatte den Namen eines bedeutenden Vorfahren angenommen, des Arztes Galileo Bonaiuti (15. Jahrhundert). Galileis Vater Vincenzo war vorübergehend nach der Heirat mit Giulia Ammannati (Pisa, 1562) Tuchhändler, ansonsten aber Musiker, Komponist und Musiktheoretiker und hatte mathematische Kenntnisse und Interessen; er lebte ab den 1570er Jahren ständig in Florenz und untersuchte den Zusammenhang zwischen Saitenspannung und Tonhöhe und erkannte dabei eine nichtlineare Beziehung der Physik.

Galilei wurde als Novize im Kloster der Vallombrosaner erzogen und zeigte Neigung, in den Benediktinerorden einzutreten, wurde aber von seinem Vater nach Hause geholt und 1580 zum Medizinstudium nach Pisa geschickt, wo sich Galileo 1581 einschrieb.

Nach vier Jahren brach er sein Studium ab und ging nach Florenz, um bei Ostilio Ricci, einem Gelehrten aus der Schule von Nicolo Tartaglia, Mathematik zu studieren. Er bestritt seinen Lebensunterhalt mit Privatunterricht, beschäftigte sich mit angewandter Mathematik, Mechanik und Hydraulik und begann, in den gebildeten Kreisen der Stadt mit Vorträgen und Manuskripten auf sich aufmerksam zu machen. Vor der Accademia Fiorentina glänzte er mit einem geometrisch-philologischen Referat über die Topografie von Dantes Hölle ("Due lezioni all’Accademia fiorentina circa la figura, sito e grandezza dell’Inferno di Dante," 1588). 1585/86 veröffentlichte er erste Ergebnisse zur Schwere fester Körper "(Theoremata circa centrum gravitatis solidorum)" (in der Tradition von Archimedes Schrift darüber) und löste ein antikes Problem (Heron) durch Konstruktion einer hydrostatischen Waage zur Bestimmung des spezifischen Gewichts ("La bilancetta," Manuskript). Seine 1587 erfolgte Bewerbung um eine Professorenstelle für Mathematik an der Päpstlichen Universität von Bologna hatte keinen Erfolg, obwohl er sich in der Bewerbung drei Jahre älter machte. Man zog den älteren Giovanni Antonio Magini vor, der außerdem dort studiert hatte. Die Gutachter vermuteten auch einen Fehler in den von Galilei der Bewerbung beigegebenen mathematischen Schriften. Danach schuf er sich aber einen Ruf als Mathematiker in Florenz unter anderem durch öffentliche Vorlesungen in der Akademie über die Architektur-Maße der Hölle (1588) und durch ein Manuskript über die Theorie der Schwerpunkte in der Tradition von Archimedes (1587), das er zirkulieren ließ.

Im Jahr 1589 erhielt Galilei für drei Jahre eine Stelle als Hochschullehrer und Inhaber des Lehrstuhls für Mathematik an der Universität Pisa. Er unterrichtete Euklids Elemente und elementare Astronomie sowie Astrologie für Mediziner. Die Bezahlung war allerdings gering; dennoch gelang es ihm, vorzügliche Instrumente zu bauen und zu verkaufen. Auch entwickelte er ein – noch sehr ungenau arbeitendes – Thermometer. Er untersuchte die Pendelbewegung und fand, dass die Periode nicht von der Auslenkung oder dem Gewicht des Pendels, sondern von dessen Länge abhängt. Bis in seine letzten Lebensjahre beschäftigte ihn das Problem, wie man diese Entdeckung zur Konstruktion einer Pendeluhr nutzen kann.

Ausgehend von der Bewegung des Pendels führte Galilei zur Untersuchung der Fallgesetze die schiefe Ebene mit anschließender horizontaler Bahn als Versuchsanordnung ein. Er experimentierte daran mit Kugeln aus verschiedenen Materialien. Das erlaubte es erstmals, den langsam anrollenden Kugeln eine bestimmte Geschwindigkeit zu erteilen und diese zu messen. So entdeckte er die Beschleunigung und die Tatsache, dass diese etwas von der Geschwindigkeit völlig verschiedenes ist. Dies wiederum ließ sich am besten in der Formelsprache der Mathematik darstellen. Am deutlichsten formulierte Galilei diese neue Einstellung zur Physik 1623 im "Saggiatore:"

Galileis Schüler und erster Biograf Vincenzo Viviani behauptete, Galilei habe in Pisa auch Fallversuche vom Schiefen Turm unternommen. In Galileis eigenen Schriften und Aufzeichnungen findet sich jedoch kein Hinweis auf solche Versuche. Davon zu unterscheiden ist das Turmargument als Gedankenexperiment, auf das Galilei in seinem Hauptwerk "Dialogo" eingeht.

Galilei fasste die Ergebnisse seiner mechanischen Untersuchungen in einem Manuskript zusammen, das heute als "De motu antiquiora" zitiert wird und erst 1890 gedruckt wurde. Darin enthaltene Angriffe auf Aristoteles nahmen seine aristotelisch geprägten Kollegen in Pisa unfreundlich auf. Galileis Anstellung wurde 1592 nicht verlängert. Seine materielle Situation war zusätzlich dadurch verschärft, dass 1591 sein Vater gestorben war und er nun als ältester Sohn auch für seine Geschwister (einen Bruder und drei Schwestern) und Mutter Verantwortung übernehmen musste.

Dank guter Protektion wurde Galilei 1592 auf den Lehrstuhl für Mathematik an der Universität Padua berufen, auf den sich auch Giordano Bruno Hoffnungen gemacht hatte. In Padua, das zur reichen und liberalen Republik Venedig gehörte, blieb Galilei 18 Jahre lang.

Obwohl seine Stelle wesentlich besser dotiert war als die vorige in Pisa, besserte Galilei sein Salär auf, indem er neben seinen akademischen Vorlesungen vornehmen Schülern Privatunterricht erteilte, darunter zwei späteren Kardinälen. Ferner vertrieb Galilei ab 1597 einen "Proportionszirkel." Für die Fertigung dieses Vorläufers des Rechenschiebers, der "Compasso" genannt wurde und dessen Konstruktion er erheblich verbessert hatte, beschäftigte er einen eigenen Mechaniker. Bereits in diesem Jahr ließ er in einem Brief an Johannes Kepler deutlich erkennen, dass er das heliozentrische Weltsystem gegenüber dem vorherrschenden Glauben an das geozentrische Weltbild favorisierte: „… unser Lehrer Copernicus, der verlacht wurde“.

Die heute nach Kepler benannte Supernova von 1604 veranlasste ihn zu drei öffentlichen Vorträgen, in denen er die aristotelische Astronomie und Naturphilosophie angriff. Aus der Tatsache, dass keine Parallaxe festgestellt werden konnte, schloss Galilei wie bereits 1572 Tycho Brahe, dass der neue Stern weit von der Erde entfernt sei und sich deshalb in der Fixsternsphäre befinden müsse. Nach herrschender Lehre wurde diese Sphäre für unveränderlich gehalten und Galilei vertrat damit ein weiteres Argument gegen die Anschauungen der Peripatetiker, wie man die Aristoteles-Schüler auch nannte. Seine Untersuchungen zu den Bewegungsgesetzen setzte er in diesen Jahren fort.

1609 erfuhr Galilei von dem im Jahr zuvor in Holland von Jan Lippershey erfundenen Fernrohr. Er baute aus käuflichen Linsen ein Gerät mit ungefähr vierfacher Vergrößerung, lernte dann selbst Linsen zu schleifen und erreichte bald eine acht- bis neunfache, in späteren Jahren bis zu 33-fache Vergrößerung. Aus dieser Zeit stammt auch ein in der Nationalbibliothek von Florenz entdeckter Einkaufszettel, der Einblick gibt, wie Galilei seine diesbezüglichen Erkenntnisse in die Praxis umsetzte.

Am 25. August 1609 führte Galilei sein Instrument, dessen militärischer Nutzen auf der Hand lag und das im Gegensatz zum wenig später entwickelten Keplerschen Fernrohr eine aufrecht stehende Abbildung lieferte, der venezianischen Regierung – der Signoria – vor. Das Instrument machte einen tiefen Eindruck und Galilei überließ der Signoria das völlig illusorische alleinige Recht zur Herstellung solcher Instrumente, woraufhin sein Gehalt erhöht wurde. Verschiedentlich wurde behauptet, Galilei habe die Erfindung des Fernrohrs wider besseres Wissen für sich beansprucht, so durch Brecht im Drama "Leben des Galilei" und durch Hans Conrad Zander, der sich auf das Galilei-Zitat zu einem „neulich von ihm erfundenen Fernrohr“ aus dem Sidereus Nuncius beruft. Dagegen hat Galilei die Grundidee des Teleskops wohl nicht als seine eigene Erfindung ausgegeben, eine Gehaltskürzung (-suspension) im folgenden Jahr deutet aber an, dass sich die Signoria durchaus hinters Licht geführt fühlte.

Als einer der ersten Menschen nutzte Galilei ein Fernrohr zur Himmelsbeobachtung. Dies bedeutete eine Revolution in der Astronomie, denn bis dahin waren die Menschen auf Beobachtungen mit dem bloßen Auge angewiesen. Er stellte fest, dass die Oberfläche des Mondes rau und uneben ist, mit Erhebungen, Klüften und Kratern. Er erkannte zudem, dass die dunkle Partie der Mondoberfläche von der Erde aufgehellt wird (sog. Erdschein) und dass die Planeten – im Gegensatz zu den Fixsternen – als Scheiben zu sehen sind. Er entdeckte die vier größten Monde des Jupiter, die er in Vorbereitung seines Wechsels an den Medici-Hof die "Mediceischen Gestirne" nannte und die heute als die "Galileischen Monde" bezeichnet werden. Er beobachtete, dass es sich bei der Milchstraße nicht um ein nebliges Gebilde (wie es dem bloßen Auge vorkommt), sondern um "„nihil aliud quam innumerarum Stellarum coacervatim consitarum congeries" (nichts anderes als eine Anhäufung zahlloser Sterne)“ handelt. Diese Entdeckungen und seine Federzeichnung der Mondoberfläche wurden im "Sidereus Nuncius" (Sternenbote bzw. Nachricht von den Sternen) von 1610 veröffentlicht und machten Galilei auf einen Schlag berühmt. Obwohl Galilei darin die Abbildung eines unübersehbar nichtexistenten großen Mondkraters am Terminator publizierte, war der "Sidereus Nuncius" innerhalb weniger Tage vergriffen.

Im Herbst 1610 ernannte der Großherzog der Toskana und ehemalige Schüler Galileis Cosimo II. de’ Medici ihn zum Hofmathematiker, Hofphilosophen und zum ersten Mathematikprofessor in Pisa ohne jede Lehrverpflichtung. Galilei bekam damit volle Freiheit, sich ganz seinen Forschungen zu widmen. Bereits 1605 war Galilei zum Mitglied der Florentiner Accademia della Crusca gewählt worden, nach seiner Übersiedlung übernahm er in ihr auch Führungsaufgaben. 1658 beschloss die Akademie, seine "Opere" in der nächsten Ausgabe des "Vocabolario" (1691 veröffentlicht) als eine der Textgrundlagen für mathematische und philosophische Terminologie zu benutzen.

Spätestens bei der Umsiedlung nach Florenz trennte sich Galilei von Marina Gamba, seiner Haushälterin, mit der er drei Kinder hatte: Virginia (Ordensname: Maria Celeste; 1600–1634), Livia (Ordensname: Arcangela; 1601–1659) und Vincenzio (1606–1669). Mit Hilfe eines Bewunderers, des Kardinals Maffeo Barberini und späteren Papstes Urban VIII., brachte Galilei seine Töchter noch vor Erreichen des Mindestalters in einem Kloster unter, denn sie hatten als uneheliche Kinder kaum Aussichten auf eine standesgemäße Heirat. Der Sohn wurde 1613 zu seinem Vater nach Florenz geschickt, nachdem Marina Gamba einen Mann namens Giovanni Bartoluzzi geheiratet hatte. Galilei legitimierte ihn später.

Galilei setzte seine astronomischen Beobachtungen fort und fand heraus, dass der Planet Venus Phasengestalten wie der Mond zeigt. Die Venussichel und die volleren Phasen interpretierte er derart, dass die Venus zeitweise zwischen Sonne und Erde steht, zu anderen Zeiten aber jenseits der Sonne. Darüber korrespondierte er mit den römischen Jesuiten um Christophorus Clavius (mit diesem hatte er bereits 1587 eine kontroverse Diskussion geführt), welche die Phasengestalt der Venus bereits unabhängig von ihm entdeckt hatten. Über die kosmologischen Konsequenzen und darüber, dass das ptolemäische Weltbild nicht mehr länger haltbar war, waren sich der jesuitische Mathematiker und der Astronom mehr oder weniger im Klaren.

In seiner Begeisterung über seine wissenschaftlichen Erkenntnisse sandte er in seiner Werkstatt gefertigte Fernrohre an Freunde und andere Wissenschaftler. Jedoch erreichten nur wenige Exemplare das gewünschte Auflösungsvermögen. So konnte es geschehen, dass manche die Jupitermonde und andere seiner Entdeckungen nicht erkennen konnten und ihm Täuschungsabsichten unterstellten.

Im Jahr 1611 besuchte Galilei Rom. Er wurde für seine Entdeckungen hoch geehrt und machte mittels seines Teleskops seinen Freunden – darunter auch Jesuiten – unverzüglich „le cose nuove del cielo“ (die neu entdeckten Gegenstände am Himmel) zugänglich: den Jupiter mit seinen vier Begleitern, den gebirgigen, zerklüfteten Mond, die „gehörnte“, d. h. sichelförmige Venus und den „dreifachen“ Saturn. Er wurde daraufhin zum sechsten Mitglied der Accademia dei Lincei ernannt. Diese Ehre war ihm so wichtig, dass er sich fortan "Galileo Galilei Linceo" nannte.

Bei diesem Aufenthalt hatte er eine Audienz bei Papst Paul V. und traf seinen alten Bewunderer Maffeo Barberini. Ein Jahr später war Barberini dabei, als Galilei eine weitere, unhaltbare Behauptung des Aristoteles mit einem simplen, aber überzeugenden Experiment widerlegte: Eis schwimmt auf Wasser nicht deswegen, weil es zwar schwerer, aber flach ist, sondern weil es leichter ist.

Zwischen Ende 1610 und Mitte 1611 beobachtete Galilei erstmals mit dem Teleskop dunkle Flecken auf der Sonnenscheibe. Diese Entdeckung der Sonnenflecken verwickelte ihn in eine Auseinandersetzung mit dem Jesuiten Christoph Scheiner: Man stritt sowohl um die Priorität als auch um die Deutung. Um die Vollkommenheit der Sonne zu retten, nahm Scheiner an, dass die Flecken Satelliten seien, wogegen Galilei die Beobachtung anführte, dass Sonnenflecken entstehen und vergehen. Er veröffentlichte diese Erkenntnis 1613 in "Lettere solari," einem der ersten wissenschaftlichen Werke, die nicht in lateinischer Sprache, sondern in Umgangssprache verfasst wurden.

Für Galilei war es offensichtlich, dass seine astronomischen Beobachtungen das heliozentrische Weltbild des Nikolaus Kopernikus stützten, aber keinen zwingenden Beweis lieferten: Sämtliche Beobachtungen wie etwa die Venusphasen waren auch mit dem Weltmodell des Tycho Brahe vereinbar, wonach sich Sonne und Mond um die Erde, die übrigen Planeten aber um die Sonne drehen. Tatsächlich gelang es erst James Bradley im Jahre 1729, mit der stellaren Aberration die Eigenbewegung der Erde gegenüber der Fixsternsphäre nachzuweisen.

Galilei hielt sich bei der Interpretation seiner astronomischen Beobachtungen zunächst zurück. Jedoch war ihm wohl schon in seiner Zeit in Pisa der Gedanke gekommen, die Drehungen "(revolutiones)" der Erde um ihre Achse und um die Sonne seien die Ursache für die Gezeiten: „die Gewässer würden dabei beschleunigt und hin- und herbewegt“. Damit glaubte er, einen Beweis für das kopernikanische Weltbild in Händen zu haben. Erst Isaac Newton konnte jedoch im Jahre 1687 beweisen, dass neben der Zentrifugalkraft auch die Anziehungskräfte der Massen von Mond und Sonne für Ebbe und Flut ursächlich sind.

Kontroverse Diskussionen am Florentiner Hof veranlassten Galilei zu erklären, dass eine mit dem kopernikanischen System verträgliche Bibelauslegung möglich sei (Brief an seinen Schüler und Nachfolger in Pisa, Benedetto Castelli, 21. Dezember 1613; Brief an die Großherzogin-Mutter Christine von Lothringen, 1615, jedoch erst 1636 veröffentlicht). Der Brief an Castelli wurde in fehlerhafter Abschrift der Inquisition zugespielt, was Galilei veranlasste, eine korrekte Abschrift hinterherzusenden und selbst nach Rom zu reisen, um seinen Standpunkt zu vertreten.

Im März 1614 gelang es Galilei, das spezifische Gewicht der Luft als ein 660stel des Gewichts des Wassers zu bestimmen – herrschende Meinung war bis zu diesem Zeitpunkt, dass Luft keinerlei Gewicht hat. Dies war eine weitere Widerlegung aristotelischer Anschauungen. In dieser Zeit war er häufig als Gutachter für den Großherzog in technisch-physikalischen Fragen tätig. Als Forscher beschäftigte er sich insbesondere mit Hydrodynamik, Lichtbrechung in Glas und Wasser sowie Mechanik mit der mathematischen Beschreibung der Beschleunigung beliebiger Körper.

In den Jahren 1610–1614 hielt er sich häufig auf dem Landgut seines Freundes Filippo Salviati auf, um seine seit Jahren angeschlagene Gesundheit wiederherzustellen.

Im Jahr 1615 veröffentlichte der Kleriker Paolo Antonio Foscarini (circa 1565–1616) ein Buch, das beweisen sollte, dass die kopernikanische Astronomie nicht der Heiligen Schrift widersprach. Daraufhin eröffnete die Römische Inquisition nach Vorarbeit des bedeutenden Kirchenlehrers Kardinal Robert Bellarmin, einer zentralen Persönlichkeit der Kurie und der Inquisition, ein Untersuchungsverfahren. 1616 wurde Foscarinis Buch gebannt. Zugleich wurden einige nichttheologische Schriften über kopernikanische Astronomie, darunter auch ein Werk von Johannes Kepler, auf den Index Librorum Prohibitorum gesetzt. Das Hauptwerk des Copernicus, "De revolutionibus orbium coelestium," in dessen Todesjahr 1543 erschienen, wurde nicht verboten, sondern „suspendiert“: Es durfte fortan bis 1822 im Einflussbereich der Römischen Inquisition nur noch in Bearbeitungen erscheinen, die betonten, dass das heliozentrische System ein bloßes mathematisches Modell sei.

An diesem Verfahren, das nicht zu den Inquisitionsprozessen gezählt werden kann, war Galilei offiziell nicht beteiligt. Seine Haltung war jedoch ein offenes Geheimnis, auch wenn das Schreiben an die Großherzogin-Mutter noch nicht veröffentlicht war. Wenige Tage nach der förmlichen Index-Beschlussfassung schrieb Bellarmin an Galilei einen Brief mit der Versicherung, Galilei habe keiner Lehre abschwören müssen; gleichzeitig jedoch enthielt dieses Schreiben die nachdrückliche Ermahnung, das kopernikanische System in keiner Weise als Tatsache zu verteidigen, sondern allenfalls als Hypothese zu diskutieren. Dieser Brief wurde im Prozess von 1632/33 als Beweis für Galileis Ungehorsam zitiert. Allerdings gab es in den Akten zwei verschiedene Fassungen, von denen nur eine korrekt unterschrieben und zugestellt war, weshalb im 19. und 20. Jahrhundert einige Historiker annahmen, die Inquisitionsbehörde habe 1632 zu Ungunsten Galileis einen Beweis gefälscht.

Galilei hielt sich von nun an mit Äußerungen in der Öffentlichkeit zum kopernikanischen System zurück. Ab 1616 beschäftigte er sich intensiv mit der Möglichkeit, die Bewegungen der Jupitermonde als Zeitmesser zu nutzen, um das Längengradproblem zu lösen. Allerdings blieb er damit erfolglos. Auch veränderte er erstmals ein Teleskop in ein Mikroskop, ohne jedoch die damit möglichen Entdeckungen ernsthaft weiter zu verfolgen.

1623 wurde Galileis alter Förderer, Kardinal Maffeo Barberini, zum Papst gewählt (Urban VIII.). Galilei widmete ihm sogleich seine Schrift "Saggiatore" (italienisch = die Goldwaage), eine Polemik gegen den Jesuitenpater Orazio Grassi über die Kometenerscheinungen von 1618–1619, über atomistische und methodologische Fragen. In diesem Buch, an dem er seit 1620 gearbeitet hatte, äußerte Galilei seine berühmt gewordene Überzeugung, die Philosophie (nach dem Sprachgebrauch der Zeit ist damit die Naturwissenschaft gemeint) stehe in dem Buch der Natur, und dieses Buch sei in mathematischer Sprache geschrieben: Ohne Geometrie zu beherrschen, verstehe man kein einziges Wort. Seither gilt Galilei als Begründer der modernen, mathematisch orientierten Naturwissenschaften, gleichzeitig enthielt dies eine klare Absage an Alchemie und Astrologie.

Im "Saggiatore" griff er auf eine Theorie des Aristoteles über Meteore zurück und interpretierte die Kometen als erdnahe optische Effekte, vergleichbar den Phänomenen wie Regenbogen oder Polarlicht. Zur Zeit der Kometenerscheinungen war Galilei allerdings aus gesundheitlichen Gründen nicht in der Lage, selbst Beobachtungen anzustellen. Seine empirisch nicht fundierte Polemik gegen die Theorie der Kometen, die Tycho Brahe und Orazio Grassi vertraten, ist als indirekte Verteidigung des kopernikanischen Systems zu verstehen, das durch die Annahme sich nicht auf Kreisbahnen bewegender Himmelskörper bedroht gewesen wäre.

Das "Saggiatore" wurde anonym wegen Atomismus und damit eines Verstoßes gegen die die Eucharistie betreffenden Dogmen des tridentinischen Konzils angezeigt. Unter Zuhilfenahme eines Gefälligkeitsgutachtens Pater Giovanni Guevaras ließen die Gönner Galileis im Vatikan diese Anzeige versanden. Der Wissenschaftshistoriker Pietro Redondi vermutet deshalb, dass auch dem Prozess 1633 eine Anzeige wegen Atomismus und damit häretischer Ansichten bezüglich des Abendmahls zugrunde liegt, die jedoch durch Intervention der eigens geschaffenen päpstlichen Untersuchungskommission auf die weit weniger brisante Frage des Kopernikanismus bzw. des Ungehorsams abgelenkt wurde.

1624 reiste Galilei nach Rom und wurde sechs Mal von Papst Urban VIII. empfangen, der ihn ermutigte, über das kopernikanische System zu publizieren, solange er dieses als "Hypothese" behandle; den Brief von Bellarmin an Galilei aus dem Jahr 1616 kannte Urban VIII. damals nicht.

Nach langen Vorarbeiten und wieder unterbrochen durch Krankheiten, vollendete Galilei 1630 den "Dialogo di Galileo Galilei sopra i due Massimi Sistemi del Mondo Tolemaico e Copernicano" (Dialog von Galileo Galilei über die zwei wichtigsten Weltsysteme, das ptolemäische und das kopernikanische). In diesem Buch erklärte Galilei unter anderem sein Relativitätsprinzip und seinen Vorschlag zur Bestimmung der Lichtgeschwindigkeit. Die erste präzise Messung der Lichtgeschwindigkeit auf der Erde gelang erst 1849 Fizeau. Als vermeintlich stärkstes Argument für das kopernikanische System diente Galilei seine – irrige – Theorie der Gezeiten.

Im Mai 1630 reiste Galilei erneut nach Rom, um bei Papst Urban VIII. und dem für die Zensur verantwortlichen Inquisitor Niccolò Riccardi ein Imprimatur zu erwirken. Er erhielt daraufhin eine vorläufige Druckerlaubnis. Zurück in Florenz entschied Galilei aus verschiedenen Gründen, sich mit dem Imprimatur durch den Florentiner Inquisitor zu begnügen und das Werk in Florenz drucken zu lassen. Zwei dieser Gründe waren der Tod des Herausgebers Fürst Cesi, Gründers der Accademia dei Lincei, und eine Pestepidemie. Aufgrund verschiedener Schwierigkeiten, ausgelöst durch Riccardi, konnte der Druck aber erst im Juli 1631 beginnen. Im Februar 1632 erschien der "Dialogo." Das Buch widmete Galileo Galilei dem Großherzog Ferdinando II. de’ Medici und händigte ihm das erste gedruckte Exemplar am 22. Februar aus.

In zweierlei Hinsicht setzte der "Dialogo" im aktuellen, astronomischen und eben auch weltanschaulich-theologischen Diskurs neue Akzente: 1. An die Stelle der Wissenschaftssprache Latein trat die Volkssprache Italienisch, denn die Diskussionen sollten gezielt über die Kreise der Wissenschaft hinausgetragen werden. 2. Er verschwieg bewusst das von den Jesuiten – u. a. Clavius, Giovanni Riccioli, Grimaldi – favorisierte Tychonische Planetenmodell. Es hätte analog zu Kopernikus’ Modell einige Phänomene wie die zeitweise Venussichel und die veränderliche Größe der Planetenscheibchen erklärt. Im Kampf um die Deutungshoheit des astronomischen Weltbildes bekämpfte Galilei den Konkurrenten Tycho Brahe mit Totschweigen.

Der Zensurauflage, das Werk mit einer Schlussrede zugunsten des ptolemäischen Systems zu beschließen, meinte Galilei nachzukommen, indem er diese Rede in den Mund des offensichtlichen Dummkopfs "Simplicio" legte. Überdies beging er den Fehler, sich über einen Lieblingsgedanken Barberinis (Urban VIII.) lustig zu machen: dass man eine Theorie niemals über die von ihr vorhergesagten Effekte prüfen könne, da Gott diese Effekte jederzeit auch auf anderem Wege hervorbringen könne. Damit hatte Galilei den Bogen überspannt und die Protektion des Papstes verspielt.

Im Juli 1632 wies Riccardi den Inquisitor von Florenz an, er solle die Verbreitung des "Dialogo" verhindern. Im September bestellte der Papst Galilei nach Rom ein. Mit Bitte um Aufschub, ärztlichen Attesten, langwieriger Anreise und obendrein Quarantäne infolge der Pestepidemie verging jedoch der gesamte Winter.

In Rom wohnte Galilei in der Residenz des toskanischen Botschafters. Anfang April 1633 wurde er offiziell vernommen und musste für 22 Tage eine Unterkunft der Inquisition beziehen. Am 30. April bekannte er in einer zweiten Anhörung, in seinem Buch geirrt zu haben, und durfte wieder in die toskanische Botschaft zurückkehren.

Am 10. Mai reichte er seine schriftliche Verteidigung ein, eine Bitte um Gnade. Am 22. Juni 1633 fand der Prozess in der Basilika Santa Maria sopra Minerva statt. Zunächst leugnete Galilei, auf die Dialogform seines Werkes verweisend, das kopernikanische System gelehrt zu haben.

Ihm wurde der Bellarminbrief (welche Fassung, ist nicht bekannt) vorgehalten, und man beschuldigte ihn des Ungehorsams. Nachdem er seinen Fehlern abgeschworen, sie verflucht und verabscheut hatte, wurde er zu lebenslanger Kerkerhaft verurteilt und war somit der Hinrichtung auf dem Scheiterhaufen entkommen.

Dass Galilei überhaupt verurteilt wurde, war unter den zuständigen zehn Kardinälen durchaus strittig; drei von ihnen (darunter Francesco Barberini, der Neffe des Papstes) unterschrieben das Urteil nicht.

Galilei selbst hielt an seiner Überzeugung fest. Die Behauptung, der zufolge er beim Verlassen des Gerichtssaals gemurmelt haben soll, „Eppur si muove“ (und sie [die Erde] bewegt sich doch), gilt vielfach als nachträgliche Erfindung. Sie wurde jedoch schon zu seinen Lebzeiten verbreitet, wie ein spanisches Gemälde von circa 1643/45 zeigt, das 1911 entdeckt wurde und diese Worte enthält.

Galilei sah zeitlebens die Kreisbahnen als zentralen Bestandteil des kopernikanischen Systems an und lehnte elliptische Bahnen aus diesem Grund ab. Kepler, mit dem er in Briefkontakt stand, hatte mit seinem Modell der Ellipsenbahnen praktisch alle Ungereimtheiten zwischen Beobachtung und dem heliozentrischen Weltbild beseitigt. Zur Rettung seines Konzepts der Kreisbahnen nahm Galilei in Kauf, dass es die beobachtete Position des Planeten Mars wesentlich schlechter voraussagte als die geozentrischen Modelle von Ptolemaios oder Brahe.

Dass Galilei die Kometen zu atmosphärischen Erscheinungen uminterpretierte, weil die alternative Erklärung von sich im Sonnensystem umherbewegenden Objekten sein Weltbild gefährdet hätte, dürfte der Glaubwürdigkeit seines Modells ebenfalls eher abträglich gewesen sein. Bei den nur unter großen Gefahren für das Augenlicht beobachtbaren Sonnenflecken kam hinzu, dass deren Zahl nach 1610 abfiel und sie von 1645 an sogar für fast 75 Jahre nahezu völlig ausblieben (sog. Maunderminimum).

Schließlich diskutierte Galilei in seinem Dialog wohlweislich nur die beiden Weltsysteme von Copernicus und Ptolemaios. Letzteres hatte er anhand der Venusphasen empirisch widerlegt, nicht jedoch das geozentrische Modell von Brahe, das sich mit seinen Beobachtungen ebenfalls vertrug.

Galilei blieb nach dem Urteil unter Arrest in der Botschaft des Herzogtums Toskana in Rom. Nach wenigen Wochen wurde er unter die Aufsicht des Erzbischofs von Siena gestellt, der allerdings sein glühender Bewunderer war und ihn nach Kräften unterstützte. In Siena konnte er seine tiefe Niedergeschlagenheit über den Prozess und seinen Ausgang überwinden.

Nach fünf Monaten, im Dezember 1633, durfte er in seine Villa "Gioiella" in Arcetri zurückkehren, blieb jedoch unter Hausarrest, verbunden mit dem Verbot jeglicher Lehrtätigkeit. Als er wegen eines schmerzhaften Leistenbruchs um Erlaubnis bat, Ärzte in Florenz aufsuchen zu dürfen, wurde sein Gesuch abgelehnt mit der Warnung, weitere solche Anfragen würden zu Aufhebung des Hausarrestes und Einkerkerung führen.

Gemäß dem Urteil hatte er über drei Jahre lang wöchentlich die sieben Bußpsalmen zu beten; diese Verpflichtung übernahm – solange sie noch lebte – seine Tochter Suor Celeste. Zudem wurden seine sozialen Kontakte stark eingeschränkt. Immerhin war es ihm gestattet, mit seinen weniger kontroversen Forschungen fortzufahren und seine Töchter im Kloster San Matteo zu besuchen. Sämtliche Veröffentlichungen waren ihm verboten, jedoch führte er einen ausgedehnten Briefwechsel mit Freunden und Gelehrten im In- und Ausland und konnte später zeitweilig Besucher empfangen, darunter Thomas Hobbes und John Milton, ab 1641 seinen ehemaligen Schüler Benedetto Castelli.

Galilei hatte seit längerem Probleme mit seinen Augen; 1638 erblindete er vollständig – sei es als Folge seiner anfangs ohne ausreichenden Schutz unternommenen Sonnenbeobachtungen oder aufgrund einer genetisch bedingten Veranlagung. Jedoch entdeckte er noch kurz vor dem völligen Verlust seiner Sehkraft die Libration des Mondes. Ein Gnadengesuch auf Freilassung wurde abgelehnt. Seine letzten Jahre verbrachte er in seinem Landhaus in Arcetri.

Ab dem Juli 1633 – noch in Siena – hatte Galilei an seinem physikalischen Hauptwerk "Discorsi e Dimostrazioni Matematiche intorno a due nuove scienze" gearbeitet. Obwohl das Inquisitionsurteil kein explizites Publikationsverbot enthielt, stellte sich eine Veröffentlichung im Einflussbereich der katholischen Kirche als unmöglich heraus. So geschah es, dass die Welt zuerst durch Matthias Berneggers lateinische Übersetzung von Galileis Werk Kenntnis erhielt, erschienen unter dem Titel "Systema cosmicum" im Verlag Elsevier und gedruckt 1635 in Straßburg bei David Hautt. Ein Druck des italienischen Texts der "Discorsi" erschien im Jahr 1638 bei Elsevier in Leiden.

Inhaltlich griff Galilei in den Discorsi Ansätze und Ergebnisse aus seinen frühen Jahren wieder auf. Die beiden "neuen Wissenschaften," die Galilei darin begründet, sind in moderner Sprache Festigkeitslehre und Kinematik. Er wies unter anderem nach, dass die bogenförmige Bewegung eines Geschosses aus zwei Komponenten besteht: Die horizontale mit konstanter Geschwindigkeit in Folge der Trägheit, die nach unten gerichtete mit zeitproportional zunehmender Geschwindigkeit durch konstante Beschleunigung. Das Zusammenwirken beider führt zu einer parabelförmigen Flugbahn. In dem Buch findet sich auch ein Paradoxon über das Unendliche (Galileis Paradoxon), dessen zugrundeliegende Ideen erst viel später im 19. Jahrhundert von Georg Cantor ausgebaut wurden.

Im Spätherbst 1641 löste Evangelista Torricelli seinen seit 1637 für ihn tätigen Begleiter Vincenzo Viviani als Assistent und Privatsekretär ab, doch war bereits klar, dass Galilei nicht mehr lang zu leben hatte. Er starb am 8. Januar 1642 in Arcetri. Ein feierliches Begräbnis in einem prunkvollen Grab, das der Großherzog vorgesehen hatte, wurde unterbunden. Er wurde zunächst anonym in Santa Croce in Florenz beigesetzt. Erst ungefähr 30 Jahre später erfolgte die Kennzeichnung des Grabes mit einer Inschrift. Die heute vorhandene repräsentative Grabstätte in Santa Croce wurde 1737 fertiggestellt. Sie wurde durch eine Stiftung des Galilei-Assistenten Vincenzo Viviani finanziert.

Nachdem es den Päpsten und Kardinälen gerade erst gelungen war, mithilfe der Dominikaner- und Jesuitenorden ihren Einfluss in Italien im Kampf gegen die Reformation wieder zu festigen, deuteten sie die Förderung der Wissenschaften in Großbritannien, Holland und Deutschland als fortdauernde Angriffe auf die Erklärungshoheit ihrer Institutionen – des dekretierten Consensus patrum. Sie sahen sich zum Beharren auf dem Althergebrachten gezwungen. Gleichzeitig gab es mächtige kirchliche Stimmen, die eine wörtliche Auslegung der Heiligen Schrift ablehnten und die Argumentation, Glauben und Wissenschaft seien getrennte Sphären, offensiv vertraten. So schrieb Kardinal Bellarmin, dass man, "läge ein wirklicher Beweis für das heliozentrische System vor, bei der Auslegung der heiligen Schrift in der Tat vorsichtig vorgehen" müsse. Ausdruck der kirchlichen Ambivalenz ihm gegenüber ist die recht milde Ermahnung von 1616, Galilei sei im „Irrtum des Glaubens“ und möge darum „von einer Verbreitung des kopernikanischen Weltbildes absehen“.

Erst nachdem Galilei 1632 mit dem "Dialogo" wieder für das kopernikanische Weltbild eintrat und die ersten Exemplare sogar an seine erklärten Gegner wie z. B. den Inquisitor Serristori schickte, wurde ein formales Verfahren gegen ihn eröffnet. Auch jetzt noch war das Klima, verglichen mit anderen Häresieprozessen, freundlich und das Urteil milde. Nachdem Galilei geschworen hatte, „stets geglaubt zu haben, gegenwärtig zu glauben und in Zukunft mit Gottes Hilfe glauben zu wollen alles das, was die katholische und apostolische Kirche für wahr hält, predigt und lehret“, erhielt er lediglich Kerkerhaft, die bereits nach wenigen Wochen in Hausarrest umgewandelt wurde. In einem Kerker hat Galilei nie eingesessen.

Die Tragik von Galileis Wirken liegt darin, dass er als ein zeitlebens tiefgläubiges Mitglied der Kirche den Versuch unternahm, ebendiese Kirche vor einem verhängnisvollen Irrtum zu bewahren. Seine Intention war es nicht, die Kirche zu widerlegen oder zu spalten, vielmehr war ihm an einer Reform der Weltsicht der Kirche gelegen. Seine verschiedenen Aufenthalte in Rom bis zum Jahr 1616 hatten auch den Zweck, Kirchenmänner wie Bellarmin davon zu überzeugen, dass die Peripatetiker nicht unfehlbar waren und die Heilige Schrift nicht immer buchstabengetreu gelesen werden müsse. Auch war Galilei davon überzeugt, die Werke Gottes durch Experiment und Logik früher oder später vollständig klären zu können. Papst Urban VIII. dagegen vertrat die Auffassung, dass sich die vielfältigen, von Gott bewirkten Naturerscheinungen dem beschränkten Verstand der Menschen für immer entzögen.

Der Inquisitionsprozess gegen Galilei hat zu endlosen historischen Kontroversen und zahlreichen literarischen Bearbeitungen angeregt; unter anderem in Bertolt Brechts "Leben des Galilei."

1741 gewährte die römische Inquisition auf Bitte Benedikts XIV. das Imprimatur auf die erste Gesamtausgabe der Werke Galileis. Unter Pius VII. wurde 1822 erstmals ein Imprimatur auf ein Buch erteilt, das das kopernikanische System als physikalische Realität behandelte. Der Autor, ein gewisser Settele, war Kanoniker. Für Nicht-Kleriker war das Interdikt wohl längst belanglos geworden.

1979 beauftragte Johannes Paul II. die Päpstliche Akademie der Wissenschaften, den berühmten Fall aufzuarbeiten. Am 31. Oktober 1992 wurde der Kommissionsbericht übergeben, und Johannes Paul II. hielt eine Rede, in der er seine Sicht des Verhältnisses von kirchlicher Lehre und Wissenschaft darstellte. Am 2. November 1992 wurde Galileo Galilei von der römisch-katholischen Kirche formal rehabilitiert. Er soll eine Statue im Vatikan erhalten. Im November 2008 distanzierte sich der Vatikan erneut von der Verurteilung Galileis durch die päpstliche Inquisition. Der damalige Papst Urban VIII. habe das Urteil gegen Galilei nicht unterzeichnet, Papst und Kurie hätten nicht geschlossen hinter der Inquisition gestanden.

Galilei gilt als wesentlicher Begründer der modernen Naturwissenschaften. Zum einen entwickelte er maßgeblich die für sie grundlegende Methode, bestehend aus der Kombination von eigener Beobachtung, gegebenenfalls anhand von geplanten Experimenten, mit möglichst genauer quantitativer Messung der beobachtbaren Größen und der Analyse der Messergebnisse mit den Mitteln der Mathematik. Zum anderen forderte er, den so gewonnenen Ergebnissen eine Vorrangstellung vor rein philosophisch oder theologisch begründeten Aussagen über die Natur zuzuerkennen.

Es blieb nicht aus, dass Galilei als dem wesentlichen Begründer der experimentellen Methodik vorgeworfen wurde, einige der von ihm beschriebenen und als Beleg für die Korrektheit seiner Theorien ausgegebenen Experimente niemals selbst durchgeführt zu haben. Das gilt in wesentlichen Punkten als widerlegt (siehe Betrug und Fälschung in der Wissenschaft).

Zu den großen begrifflichen Errungenschaften Galileis zählt die Widerlegung der Bewegungslehre des Aristoteles, insbesondere der darin formulierten prinzipiellen Gegensätze zwischen Ruhe und Bewegung sowie zwischen natürlicher und unnatürlicher (oder erzwungener) Bewegung. Galilei hatte an der schiefen Ebene erstmals die Zunahme der Fallgeschwindigkeit nachgemessen und gefunden, dass sie nicht in diskreten Graden und nicht in Proportion zur durchlaufenen Strecke zunimmt, sondern dass sie in Proportion zur verstrichenen Zeit vom Wert null an stetig anwächst und bis zum Erreichen der Endgeschwindigkeit alle dazwischen liegenden Werte durchläuft. Die von Johannes Buridan und Francis Bacon beobachtete Tatsache, dass die rein mechanischen Vorgänge wie Fall und Stoß auf einem gleichmäßig bewegten Schiff genau so ablaufen wie an Land, verallgemeinerte Galilei zu einem neuen Relativitätsprinzip: Danach gibt es bei den beobachtbaren Vorgängen keinen absoluten Unterschied zwischen Ruhe und (gleichförmiger) Bewegung. Das führte ihn weiter zur Aufstellung des Trägheitsprinzips, denn wenn die gleichförmige Mitbewegung eines Körpers mit einem Schiff von einem Mitfahrer des Schiffs genau so gut auch als Ruhe angesehen werden kann, dann erfordert die Aufrechterhaltung dieser Bewegung offenbar keine dauernd wirkende äußere Kraft.

Die gleichmäßig beschleunigte Bewegung beschäftigte Galilei über vierzig Jahre lang. Seine experimentelle Innovation bestand in der Verwendung einer Fallrinne als schiefe Ebene, mit der er die Fallgesetze auf einer verlangsamten Zeitskala studieren konnte. Die Beschleunigung bestimmte er über seinen Puls, mit Wasseruhren oder dadurch, dass der Körper ein rhythmisches Signal auslöst, wenn der Auslöser in geeigneten Abständen platziert ist. Für die Entwicklung der physikalischen Methode ebenso bedeutsam war Galileis Schritt, die aus Experimenten gewonnenen Kenntnisse dazu zu nutzen, weiterführende Experimente zu planen und durchzuführen: Er präparierte mithilfe der schiefen Ebene Körper, die eine definierte horizontale Geschwindigkeit besaßen, und konnte mit diesen die Experimente zum horizontalen Wurf anstellen.

Die verbreitete Geschichte über Galileis eigenhändig durchgeführte Fallversuche vom Schiefen Turm in Pisa sind als Legende einzustufen, denn es gibt keinen verlässlichen Beleg dafür. Ebenso wurde und wird vereinzelt immer noch bezweifelt, dass Galilei die Versuche zur beschleunigten Bewegung auf der schiefen Ebene wirklich durchgeführt hat. Die Begründung beruhte ursprünglich darauf, dass im gesamten Nachlass Galileis, der Anfang des 20. Jahrhunderts publiziert worden war, fast keine Aufzeichnungen zu durchgeführten Messungen zu finden waren. Jedoch fand in den 1960er Jahren Stillman Drake, nachdem er selber in Florenz in das Archiv hinuntergestiegen war, zahlreiche Blätter von Galileis Hand, die in der Gesamtausgabe fortgelassen worden waren. Es waren die Protokolle der Messungen, die bei der Zusammenstellung der Gesamtausgabe für unwichtig gehalten worden waren, weil auf ihnen nur wenig oder gar kein Text zu sehen war, dafür aber Skizzen und Zahlen.

Wie aus dem Titel der "Discorsi" hervorgeht, veröffentlichte Galilei seine Ergebnisse über die Festigkeit eines Balkens mit dem vollen Bewusstsein, damit eine neue Wissenschaft zu begründen. Die weitere Entwicklung hat ihm recht gegeben; sein Beitrag kann tatsächlich als Begründung der Festigkeitslehre gelten.

Galilei stellte fest, dass die Tragfähigkeit eines Balkens größer ist, wenn man ihn hochkant, nicht flachkant stellt. Er setzte als Erster die äußere Belastung in Relation zu den inneren Spannungen. Eine quantitative Theorie konnte er allerdings noch nicht aufstellen. Den heute "Neutralfläche" genannten Bereich verschwindender Zug- bzw. Druckspannung ordnete er am unteren Rand des eingespannten Balkens statt in der Mitte des Balkenquerschnittes an. Korrekturen dieses Irrtums konnten sich im 17. und 18. Jahrhundert nicht durchsetzen; erst Anfang des 19. Jahrhunderts sorgte Navier erfolgreich für eine Richtigstellung.

Galileis astronomische Entdeckungen sind im biografischen Teil bereits aufgeführt. Einige davon zogen bahnbrechende Erkenntnisse nach sich:

Galileis Thermoskop aus dem Jahr 1592 ist das erste nachweisbare Temperaturmessgerät. Es wurde von Santorius mit Skalenstrichen versehen und schließlich von Fahrenheit 1714 entscheidend verbessert.

Christiaan Huygens entwickelte später Galileis Idee, eine mechanische Uhr durch ein Pendel zu steuern, zur Praxisreife.

Siehe auch .

Nach Galilei benannt sind:





Galilei veröffentlichte seine wissenschaftlichen Erkenntnisse in den folgenden Hauptwerken:


Neuere Ausgaben sind:









</doc>
<doc id="1834" url="https://de.wikipedia.org/wiki?curid=1834" title="Magenband">
Magenband

Ein Magenband (englisch "gastric band") dient einer medizinischen Behandlungsmethode bei krankhafter Adipositas, die durch Diäten und konservative Behandlungsmaßnahmen nicht behandelbar ist. Zusammen mit anderen Methoden gehört das Magenband zum Spektrum der Adipositaschirurgie, der chirurgischen Therapie des krankhaften Übergewichts. Sie ist das letzte Mittel der Wahl, wenn konservative Methoden zur Gewichtsreduktion versagt haben. Die Indikation zur Magenzügelung ist gegeben ab einem Body-Mass-Index von mindestens 35–40.

Durch Einengen des Magen<nowiki></nowiki>durchmessers im Eingangsbereich kann eine dauerhafte deutliche Gewichtsreduktion erreicht werden. Hierzu wird in einer laparoskopischen Operation ein verstellbares Silikonband um den Magenfundus gelegt. Der Durchmesser der Öffnung ist durch Auffüllen des Bandes mit Flüssigkeit veränderbar, ein entsprechender Zugang (Portkammer) wird in der Bauchwand oder vor dem Brustbein platziert.

In Deutschland übernehmen die Krankenkassen die Kosten für das Magenband, wenn die medizinische Notwendigkeit belegt wurde. Dies ist in der Regel der Fall, wenn der Patient einen BMI > 40 oder einen BMI > 35 mit schwerwiegenden Begleiterscheinungen (z. B. Diabetes, Gelenkerkrankungen, etc.) hat, alle konservativen Behandlungsmöglichkeiten ausgeschöpft sind (ultima ratio) und der Patient aufgeklärt und motiviert ist. Außerdem darf das OP-Risiko nicht das Risiko bei anderen möglichen Eingriffen übersteigen.

Laut verschiedener Studien werden folgende Wirkungen des Magenbandes auf den Patienten angenommen:






</doc>
<doc id="1835" url="https://de.wikipedia.org/wiki?curid=1835" title="Gefäßchirurgie">
Gefäßchirurgie

Die Gefäßchirurgie ist ein Teilgebiet der Chirurgie, das sich mit der Erkennung und operativen Behandlung sowie der Nachsorge und Rehabilitation von Erkrankungen des Gefäßsystems einschließlich der Verletzungen und Fehlbildungen der Blutgefäße befasst. Zu den Methoden der Gefäßchirurgie gehört die operative Behandlung von Gefäßkrankheiten und -verletzungen durch plastische Verfahren, vor allem als chirurgische Therapie der großen Hauptschlagadern, auch der Bauchaorta, sowie der Durchblutungsstörungen der peripheren Schlagadern, aber auch die Chirurgie der peripheren und zentralen Venen.

Die Chirurgie der Herzkranzgefäße gehört hingegen traditionell in die Herzchirurgie.

Typische chirurgische Eingriffe in der Gefäßchirurgie sind das Anlegen von Gefäßbypässen bei peripherer arterieller Verschlusskrankheit oder Protheseneinlage bei außerhalb des Gehirns lokalisierten Aneurysmen. Zur Gefäßchirurgie gehört auch die operative Behandlung der das Gehirn versorgenden Gefäße, etwa der Halsschlagader, bei Arteriosklerose zur Schlaganfallprophylaxe. Daneben werden in der Gefäßchirurgie unter anderem Dialyse-Shunts (Kurzschlussverbindungen zwischen Arterie und Vene, über die eine Dialyse erfolgt) angelegt.

Nach der Musterweiterbildungsordnung der Bundesärztekammer müssen Fachärzte für Gefäßchirurgie auch Operationen am Venensystem beherrschen. Die Phlebologie befasst sich als Zusatz-Weiterbildung mit den Beinvenen.

Bis in die Neuzeit bestand die (offene) chirurgische Behandlung von Blutgefäßen vor allem in blutstillenden Maßnahmen, etwa der bereits seit der Antike bekannten Gefäßligatur.
1897 führte der amerikanische Chirurg John Benjamin Murphy erstmals erfolgreich eine zirkuläre Naht an der Oberschenkelarterie durch.
Als Pionier der modernen arteriellen Gefäßchirurgie gilt der französische Mediziner Alexis Carrel mit seiner 1902 beschriebenen Gefäßanastomose. Die Einfügung (Interposition) von Venenabschnitten zur Behandlung von Arterienverletzungen führte der deutsche Chirurg Erich Lexer 1907 ein.




</doc>
<doc id="1837" url="https://de.wikipedia.org/wiki?curid=1837" title="Geschichte">
Geschichte

Unter Geschichte versteht man im Allgemeinen diejenigen Aspekte der Vergangenheit, derer Menschen gedenken und die sie deuten, um sich über den Charakter zeitlichen Wandels und dessen Auswirkungen auf die eigene Gegenwart und Zukunft zu orientieren.

Im engeren Sinne ist "Geschichte" die Entwicklung der Menschheit, weshalb auch von Menschheitsgeschichte gesprochen wird (im Unterschied etwa zur Naturgeschichte). In diesem Zusammenhang wird "Geschichte" gelegentlich synonym mit Vergangenheit gebraucht. Daneben bedeutet "Geschichte" aber auch die Betrachtung der Vergangenheit im Gedenken, im Erzählen und in der Geschichtsschreibung. Forscher, die sich der Geschichtswissenschaft widmen, nennt man Historiker.

Schließlich bezeichnet man mit "Geschichte" auch das Schulfach Geschichte, das über den Ablauf der Vergangenheit informiert und einen Überblick über Ereignisse der Welt-, Landes-, Regional-, Personen-, Politik-, Religions- und Kulturgeschichte gibt.

Wenn man Geschichte als Vergangenheit betrachtet, lassen sich folgende Bereiche unterscheiden:

Nur die Geschichte in diesem dritten, engsten Bereich ist Gegenstand der Geschichtswissenschaft mit ihrer spezifischen Methode, denn erst über Schriftzeugnisse wird das fassbar, was der Mensch aus dem Erfahrenen als seine Geschichte versteht und wie er sich diese angeeignet hat. Im Mittelpunkt der Beschäftigung mit Geschichte, der Erkundung (griechisch: "Historie") der Vergangenheit, stehen dabei die "Quellen", d. h. zeitnahe schriftliche Aufzeichnungen und Dokumente.

Dabei ist zu unterscheiden zwischen Geschichte als Geschehen und dem Geschichtsbewusstsein, dem Bild des Gewesenen, das sich einerseits im Selbstverständnis der historischen Personen widerspiegelt, andererseits sich bei der Erforschung und Darstellung aufgrund der vorhandenen Überlieferungen für den Betrachter ergibt, der das Geschehen zu erfassen versucht (vgl. Geschichtsschreibung und Geschichte der Geschichtsschreibung). Diese nachträgliche Geschichtserkenntnis gründet sich auf Überreste und Tradition. Solche Erkenntnis ist allerdings nie völlig objektiv, sondern abhängig von der historischen Situation, der Perspektive des Betrachters und den verfügbaren Quellen. Eine bestimmte Perspektive gegen andere Perspektiven durchzusetzen (aber auch der Versuch, Multiperspektivität zu ermöglichen) ist Sache der Geschichtspolitik.

Dagegen hat sich Geschichtsdidaktik die Aufgabe gestellt, den Zugang zu den wichtigsten Bereichen von Geschichte zu erleichtern und ein mehrdimensionales Geschichtsbewusstsein zu ermöglichen. Der Geschichtsunterricht ist der Versuch der praktischen Umsetzung von Geschichtsdidaktik. Im Idealfall sollen inhaltlich nicht nur die bisherigen Erkenntnisse der Geschichtswissenschaft, sondern zumindest in Ansätzen auch historisch-kritische Methodenkenntnisse vermittelt werden - dies umso mehr, als das in der Schule vermittelte Geschichtswissen an sich stets nur eine Rekonstruktion ist, die keinen Wahrheitsanspruch erheben kann.

Einen Zugriff auf Geschichte von den verschiedenen Geschichtsräumen und Nationalgeschichten aus verschafft das . Den Zugriff auf spezifische Sachbereiche der Geschichte ermöglichen die verschiedenen Disziplinen der Geschichtswissenschaft.

Gordon A. Craig sagte 1981 in Münster:

Geschichte kann als Resultat wissenschaftlicher Forschung gesehen werden. Die Darstellung der Ergebnisse und Zusammenhänge als eine künstlerische Tätigkeit zu betrachten, wird mitunter vorgeschlagen. Der Historiker soll dem Leser auf eine nachvollziehbare, objektive und überzeugende Weise den Gang der Ereignisse sowie deren Ursachen und Wirkungen, ein alltagsweltliches Geschichtsbewusstsein präsentieren. Die Geschichtsphilosophie versucht, den Gang der Handlungen in einen übergeordneten Zusammenhang, ein Geschichtsbild, zu bringen. Dies ist vorwiegend der Ansatz der Geisteswissenschaften. Wesentliche Ordnungskriterien- und Hilfsmittel sind hierbei die Chronologie und Periodisierung.

Die Erforschung der Geschichte der Natur oder die Erdgeschichte wird selbstverständlich auch von der Naturwissenschaft betrieben. Hierbei geht es hauptsächlich darum, die Naturgeschichte beziehungsweise die Geschichte der Erde nachvollziehbar zu machen auf der Grundlage naturwissenschaftlicher Erkenntnismethoden.

Einen weiteren Ansatz verfolgt die Historio-Metrie, welche zeitliche Geschehnisse in einer metrischen Übersicht darzustellen versucht.

Die Geschichtswissenschaft diskutiert auch die Frage, wie weit das von ihr entworfene Bild von der Vergangenheit überhaupt in der Lage ist, die tatsächliche Vergangenheit abzubilden. Das bezieht sich nicht allein auf die Unmöglichkeit, historische Situationen und Prozesse in ihrer Gesamtheit oder Totalität abzubilden, sondern hängt auch mit Zweifeln in ihre Quellen (ganz abgesehen von den Fälschungen) zusammen. Während man im 19. Jahrhundert bemüht war, gegensätzliche Aussagen in verschiedenen Quellen weitestgehend zu harmonisieren, findet man sich heute eher damit ab, dass der vergangene Sachverhalt häufig endgültig und unrekonstruierbar verschwunden ist. Bekanntes Beispiel für diesen Wandel ist die Darstellung der Krönung Karls des Großen in Rom zum Kaiser, die in den päpstlichen Quellen anders geschildert wird als in den Quellen, die nördlich der Alpen entstanden sind. Während in diesem Falle die Nichtrekonstruierbarkeit angesichts sich widersprechender Quellen heute allgemein akzeptiert wird, ist es bei Quellen, denen keine abweichende oder von ihr unabhängige Darstellung gegenübersteht, eine viel diskutierte Frage, ob das Bild, das auf Grund dieser Quellen von der Vergangenheit gezeichnet wird, nicht eine Konstruktion ist, die mit den wirklichen Geschehnissen wenig oder möglicherweise nichts zu tun hat. Hier können der Prozess Jesu oder die Hintergründe der konstantinischen Wende als Beispiel dienen. Dabei wird zum einen die Frage diskutiert, ob der Versuch einer Rekonstruktion in derartigen Fällen nicht ebenfalls unterbleiben sollte, und zum anderen, ob eine solche Unterscheidung zwischen „wirklicher“ und „rekonstruierter“ Wirklichkeit überhaupt einen Sinn hat und ob nicht die Maxime genügt, dass die rekonstruierte Geschichte so lange als Wirklichkeit gilt, bis neue Erkenntnisse eine Korrektur erfordern.

Das Bemühen um wissenschaftliche Rekonstruktion von Geschichte kommt – schon allein wegen der sprachlichen Bestimmtheit ihrer Vermittlung – nicht ohne konstruierende Anteile aus. Der Rohstoff der Geschichte, die Gesamtheit des Vergangenen, kann erst durch Benennung, Bewertung und geistige Ordnung im Medium der Sprache sichtbar bzw. begreiflich gemacht werden. Demnach ist Geschichte (auch) das Erzeugnis der Historiker und der sich auf die Vergangenheit besinnenden Menschen. „Nur soweit diese Besinnung stattfindet und sich artikuliert, gibt es Geschichte. Außerhalb dieses Bereichs ist nur noch Gegenwart ohne Tiefendimension und totes Material.“

Indem die Darstellung der Geschichte auch als eine künstlerische Aufgabe betrachtet werden kann, kommt es umgekehrt zur künstlerischen bzw. literarischen Verarbeitung von geschichtlichen Themen, ohne dass vorrangig ein wissenschaftliches Erkenntnisinteresse besteht. So sind zum Beispiel das Drama "Wallenstein" von Friedrich von Schiller oder der "Torquato Tasso" von Johann Wolfgang von Goethe literarische Werke auf der Grundlage des historischen Stoffes, ohne zugleich als Geschichtswerke im wissenschaftlichen Sinn verstanden werden zu können.

Neben den literarischen Betrachtungsweisen gibt es Formen künstlerischer Auseinandersetzung mit Geschichte auch in der bildenden Kunst, speziell in der Historienmalerei, mitunter auch in monumentalen Formen wie im Falle des Bauernkriegspanoramas.

Die gezielt von politischen Interessen geleitete Darstellung von Geschichte ist Gegenstand der Geschichtspolitik, die auch von manchen Historikern aktiv mit betrieben wird. Geschichtspolitik dient der Einflussnahme auf die allgemeine Meinungsbildung in der Gesellschaft, insbesondere in totalitären Systemen. Sie hat in Abhängigkeit vom politischen System zeittypische Auswirkungen auf Geschichtsdidaktik und der Geschichtspädagogik.

Die Mittel von Geschichtspolitik sind vielfältig. Zu den diesbezüglichen Begriffen gehören: Geschichtlichkeit, Geschichtsbewusstsein, Geschichtsraum, Geschichtsperspektive, Historisierung, Erinnerungskultur, Glorifizierung beziehungsweise Geschichtsfälschung.

Daneben gibt es Formen geschichtlicher Wissensvermittlung durch Museumspädagogik und durch Unterhaltungsmedien bis hin zum Histotainment (wie zum Beispiel Mittelaltermärkte), ein Spektrum, das von didaktischer Wissensvermittlung bis zur bloßen Unterhaltung reicht und auch in mancherlei Kombinationen anzutreffen ist.




Wege zu Geschichtsdarstellungen für Laien und für schulische Zwecke


</doc>
<doc id="1839" url="https://de.wikipedia.org/wiki?curid=1839" title="Gruppentheorie">
Gruppentheorie

Die Gruppentheorie als mathematische Disziplin untersucht die algebraische Struktur von Gruppen.

Anschaulich besteht eine Gruppe aus den Symmetrien eines Objekts oder einer Konfiguration zusammen mit jener Verknüpfung, die durch das Hintereinanderausführen dieser Symmetrien gegeben ist. So bilden beispielsweise die Drehungen eines regelmäßigen formula_1-Ecks in der Ebene, mit denen die Figur auf sich selbst abgebildet werden kann, eine Gruppe mit formula_1 Elementen. Um dieses Konzept allgemein zu fassen, hat sich eine knappe und mächtige Definition herausgebildet: Demnach ist eine Gruppe eine Menge zusammen mit einer zweistelligen inneren Verknüpfung (durch die jedem geordneten Paar von Elementen eindeutig ein Element dieser Menge als Resultat zugeordnet wird), wenn diese Verknüpfung assoziativ ist und es ein neutrales Element gibt sowie zu jedem Element ein Inverses. So bildet zum Beispiel auch die Menge der ganzen Zahlen zusammen mit der Addition eine Gruppe.

Die systematische Untersuchung von Gruppen begann im 19. Jahrhundert und wurde durch konkrete Probleme ausgelöst, zunächst durch die Frage nach der Lösbarkeit von algebraischen Gleichungen, später durch die Untersuchung geometrischer Symmetrien. Dementsprechend stand zunächst die Untersuchung konkreter Gruppen im Vordergrund; erst gegen Ende des 19. Jahrhunderts wurden verstärkt abstrakte Fragestellungen untersucht. Wichtige Beiträge stammen unter anderem von Évariste Galois und Niels Henrik Abel in der Algebra sowie Felix Klein und Sophus Lie in der Geometrie. Eine der herausragenden mathematischen Leistungen des 20. Jahrhunderts ist die Klassifikation aller endlichen einfachen Gruppen, also der unzerlegbaren Bausteine aller endlichen Gruppen.

Die große Bedeutung der Gruppentheorie für viele Gebiete der Mathematik und ihrer Anwendungen resultiert aus ihrer Allgemeinheit, denn sie umfasst in einer einheitlichen Sprache sowohl geometrische Sachverhalte (Bewegungen des Raumes, Symmetrien etc.) als auch arithmetische Regeln (Rechnen mit Zahlen, Matrizen etc.). Vor allem in der Algebra ist der Begriff der Gruppe von grundlegender Bedeutung: Ringe, Körper, Moduln und Vektorräume sind Gruppen mit zusätzlichen Strukturen und Eigenschaften. Methoden und Sprechweise der Gruppentheorie durchziehen daher viele Gebiete der Mathematik. In Physik und Chemie treten Gruppen überall dort auf, wo Symmetrien eine Rolle spielen (z. B. Invarianz physikalischer Gesetze, Symmetrie von Molekülen und Kristallen). Zur Untersuchung solcher Phänomene liefern die Gruppentheorie und die eng verwandte Darstellungstheorie die theoretischen Grundlagen und eröffnen wichtige Anwendungen.

Gruppen werden in der Mathematik verwendet, um das Rechnen mit Zahlen zu verallgemeinern. Entsprechend besteht eine Gruppe aus einer Menge von Dingen (z. B. Zahlen, Symbolen, Objekten, Bewegungen) und einer Rechenvorschrift (eine Verknüpfung, in diesem Absatz als formula_3 dargestellt), die angibt, wie mit diesen Dingen umzugehen ist. Diese Rechenvorschrift muss dabei bestimmten Regeln genügen, den sogenannten Gruppenaxiomen, die im Folgenden erklärt werden.

Von einer Gruppe spricht man, falls für eine Menge zusammen mit einer Verknüpfung je zweier Elemente dieser Menge, hier geschrieben als formula_4, die folgenden Anforderungen erfüllt sind:

Man beachte: Falls auf der Menge von mehreren Verknüpfungen die Rede ist, etwa formula_3 und formula_17, dann gibt es mehrere neutrale und inverse Elemente, jeweils passend zur Verknüpfung. Wenn aus dem Kontext klar ist, dass nur eine bestimmte Verknüpfung gemeint ist, dann spricht man kurz von "dem" neutralen Element formula_7 und "dem" inversen Element formula_13 zu formula_10 ohne die Verknüpfung nochmals explizit zu erwähnen.


Beispiele für abelsche Gruppen sind

Die sehr allgemeine Definition von Gruppen ermöglicht es, nicht nur Mengen von Zahlen mit entsprechenden Operationen als Gruppen aufzufassen, sondern auch andere mathematische Objekte mit geeigneten Verknüpfungen, die die obigen Anforderungen erfüllen. Ein solches Beispiel ist die Menge der Drehungen und Spiegelungen (Symmetrietransformationen), durch die ein regelmäßiges n-Eck auf sich selbst abgebildet wird, mit der Hintereinanderausführung der Transformationen als Verknüpfung (Diedergruppe).

Eine Gruppe ist ein Paar formula_26. Dabei ist formula_27 eine Menge und formula_28 eine zweistellige Verknüpfung bezüglich formula_27. Das heißt, dadurch wird die Abbildung formula_30 beschrieben. Zudem müssen die folgenden Axiome für die Verknüpfung erfüllt sein, damit formula_26 als Gruppe bezeichnet werden kann:


Eine Gruppe formula_26 heißt "abelsch" oder "kommutativ," wenn zusätzlich das folgende Axiom erfüllt ist:


Andernfalls, d. h., wenn es Gruppenelemente formula_46 gibt, für die formula_47 ist, heißt die Gruppe formula_26 nichtabelsch.

Bekannte Beispiele für Gruppen sind:
Eine ausführlichere Aufzählung finden Sie in der Liste kleiner Gruppen.

Die Mächtigkeit (Kardinalität) formula_49 der Trägermenge der Gruppe nennt man "Ordnung" der Gruppe oder kurz "Gruppenordnung." Für endliche Mengen ist dies einfach die Anzahl der Elemente.

Ist formula_50 eine Teilmenge der Trägermenge formula_27 einer Gruppe formula_26 und ist formula_53 selbst eine Gruppe, so nennt man formula_50 eine "Untergruppe" von formula_27, Bezeichnung formula_56.

Hierzu ein wichtiger Satz (Satz von Lagrange): Die Ordnung jeder Untergruppe formula_50 einer endlichen Gruppe formula_27 ist ein Teiler der Ordnung der Gruppe formula_27. Ist speziell formula_49 eine Primzahl, dann hat formula_27 nur die (trivialen) Untergruppen formula_62 (bestehend aus dem neutralen Element) und formula_27 selbst.

Gibt es in formula_27 ein Element formula_10 so, dass man jedes Element als Potenz formula_66 (mit einer ganzen Zahl formula_1, die auch negativ sein darf) schreiben kann, so nennt man formula_27 eine "zyklische Gruppe" und formula_10 "erzeugendes Element."

Ergibt ein Element formula_10 der Gruppe, endlich viele Male (formula_1-mal) mit sich selbst verknüpft, das neutrale Element 1, d. h., gibt es ein formula_72 mit formula_73, so nennt man das kleinste derartige formula_74 die "Ordnung" des Elements formula_10. In diesem Fall spricht man von einem Element endlicher Ordnung oder "Torsionselement". Falls kein solches formula_1 existiert, sagt man, dass formula_10 "unendliche Ordnung" hat. In beiden Fällen entspricht die Ordnung des Elements der Ordnung der von ihm erzeugten Untergruppe.

Wegen des Satzes von Lagrange folgt: In einer endlichen Gruppe ist die Ordnung jedes Elements endlich, und ein Teiler der Gruppenordnung.

Die kleinste positive Zahl formula_1, mit der formula_79 für jedes Gruppenelement formula_10 gilt, wird Gruppenexponent genannt.

Definiert man auf der Gruppe formula_26 mit einer Untergruppe formula_53 die Relation formula_83 durch

erhält man eine Äquivalenzrelation auf formula_27. Die Äquivalenzklasse zu einem Element formula_86 (d. h. die Menge aller Elemente formula_33, die zu formula_10 in der Relation formula_83 stehen), ist die Menge

Für diese Menge schreibt man formula_91 oder formula_92. Da diese Menge alle Elemente von formula_27 enthält, die dadurch entstehen, dass das Element formula_10 mit allen Elementen aus formula_50 verknüpft wird, heißt sie die Linksnebenklasse, Alternativbezeichnung Linksrestklasse, von formula_50 nach dem Element formula_10.

Wenn man andererseits eine Relation formula_98 durch

definiert, dann ist dies im Allgemeinen eine andere Äquivalenzrelation und die Menge der zu formula_10 äquivalenten Elemente in formula_27 jetzt

die durch "Rechts"verknüpfung der Elemente aus formula_50 mit dem Element formula_10 entsteht. Sie wird mit formula_105 oder formula_106 bezeichnet und Rechtsnebenklasse, Alternativbezeichnung Rechtsrestklasse, von formula_50 nach dem Element formula_10 genannt.

Nebenklassen werden benutzt, um den Satz von Lagrange zu beweisen, um die Begriffe "Normalteiler" und "Faktorgruppe" zu erklären und um Gruppenoperationen zu studieren.

Sind zwei Untergruppen formula_109 und formula_50 gegeben, so erhält man eine Äquivalenzrelation durch
Die Äquivalenzklasse zu formula_37 ist
Für diese Menge schreibt man formula_114 oder formula_115 und nennt sie die formula_116-Doppelnebenklasse zu formula_10.

Ist für jedes Element formula_86 die linke Nebenklasse von formula_50 gleich der rechten, d. h. formula_120, so nennt man formula_50 einen "Normalteiler" von formula_27, Bezeichnung formula_123.

In einer abelschen Gruppe ist jede Untergruppe ein Normalteiler. Der Kern jedes Gruppenhomomorphismus ist ein Normalteiler.

Die Linksnebenklassen (oder auch die Rechtsnebenklassen) bezüglich einer Untergruppe teilen die Gruppe (als Menge angesehen) in disjunkte Teilmengen auf. Ist die Untergruppe sogar ein Normalteiler, so ist jede Linksnebenklasse zugleich eine Rechtsnebenklasse und wird ab jetzt nur "Nebenklasse" genannt.

Ist formula_50 ein Normalteiler von formula_27, dann kann man auf der Menge formula_126 der Nebenklassen eine Verknüpfung definieren:
Die Verknüpfung ist wohldefiniert, d. h., sie ist nicht abhängig von der Wahl der Repräsentanten formula_128 und formula_129 in ihrer Nebenklasse. (Ist formula_50 kein Normalteiler, dann gibt es Nebenklassen mit Repräsentanten, die verschiedene Ergebnisse produzieren.)

Zusammen mit dieser induzierten Verknüpfung bildet die Menge der Nebenklassen eine Gruppe, die Faktorgruppe formula_126. Die Faktorgruppe ist eine Art vergröbertes Abbild der originalen Gruppe.

Eine nicht-triviale Gruppe heißt "einfach," wenn sie keine Normalteiler außer der trivialen Gruppe und sich selbst hat. Beispielsweise sind alle Gruppen von Primzahlordnung einfach. Die einfachen Gruppen spielen eine wichtige Rolle als „Grundbausteine“ von Gruppen. Seit 1982 sind die endlichen einfachen Gruppen vollständig klassifiziert. Jede gehört entweder zu einer der 18 "Familien endlicher einfacher Gruppen" oder ist eine der 26 Ausnahmegruppen, die auch als sporadische Gruppen bezeichnet werden.

Manche Eigenschaften endlicher Gruppen lassen sich mit dem Zauberwürfel veranschaulichen, der seit seiner Erfindung vielfach im akademischen Unterricht eingesetzt wurde, weil die Permutationen der Ecken- und Kantenelemente des Würfels ein sichtbares und handgreifliches Beispiel einer Gruppe darstellen.

Die Menge der möglichen Positionen der Atome der Moleküle in ihrer Gleichgewichtskonformation lässt sich mit Hilfe von Symmetrieoperationen (Spiegelung, Drehung, Inversion, Drehspiegelung) auf sich selbst abbilden. Die Symmetrieoperationen lassen sich zu Gruppen, den sogenannten Punktgruppen zusammenfassen.





In der Quantenmechanik sind Symmetriegruppen als Gruppen von unitären oder antiunitären Operatoren realisiert. Die Eigenvektoren einer maximalen abelschen Untergruppe dieser Operatoren zeichnet eine physikalisch wichtige Basis aus, die zu Zuständen mit wohldefinierter Energie oder Impuls oder Drehimpuls oder Ladung gehört. Beispielsweise bilden in der Festkörperphysik die Zustände in einem Kristall mit einer fest gewählten Energie einen Darstellungsraum der Symmetriegruppe des Kristalls.

Die Entdeckung der Gruppentheorie wird Évariste Galois zugeschrieben, der die Lösbarkeit algebraischer Gleichungen durch Radikale (in heutiger Terminologie) auf die Auflösbarkeit ihrer Galois-Gruppe zurückführte. Galois’ Arbeit wurde erst 1846 postum veröffentlicht. Implizit spielte das Konzept einer Gruppe aber bereits bei Lagrange ("Réflexions sur la résolution algébrique," 1771) und Gauß ("Disquisitiones Arithmeticae," 1801) eine Rolle.

Im letzten Viertel des 19. Jahrhunderts wurde die Gruppentheorie vor allem durch Felix Kleins Erlanger Programm und die von Sophus Lie entwickelte Theorie der kontinuierlichen Transformationsgruppen sowie auch Poincarés und Kleins Arbeiten über automorphe Funktionen zu einem zentralen Bestandteil der Mathematik. Aus dem Jahr 1881 stammt Poincarés bekanntes Zitat „Les mathématiques ne sont qu’une histoire des groupes.“ (Die Mathematik ist nur eine Geschichte der Gruppen.)

Eine abstrakte Definition von Gruppen findet sich erstmals 1854 bei Arthur Cayley:
Erst ab 1878 erschienen die ersten Arbeiten zur abstrakten Gruppentheorie. Cayley bewies, dass jede endliche Gruppe isomorph zu einer Gruppe von Permutationen ist und bemerkte in derselben Arbeit, dass es einfacher sei, Gruppen als abstrakte Gruppen statt als Gruppen von Permutationen zu betrachten. 1882 definierte Dyck erstmals Gruppen mittels Erzeugern und Relationen.




</doc>
<doc id="1841" url="https://de.wikipedia.org/wiki?curid=1841" title="Globale Erwärmung">
Globale Erwärmung

Als globale Erwärmung bezeichnet man den Anstieg der Durchschnittstemperatur der erdnahen Atmosphäre und der Meere seit der Industrialisierung in den letzten 150 Jahren. Der berechnete Erwärmungstrend über die letzten 50 Jahre (1956 bis 2005) in Höhe von 0,13 °C ± 0,03 °C pro Jahrzehnt ist fast zweimal so groß wie derjenige über die letzten 100 Jahre (1906 bis 2005) in Höhe von vs. 0,07 °C ± 0,02 °C pro Jahrzehnt. Dieser Prozess verläuft erheblich schneller als alle bisher bekannten Erwärmungsphasen der jüngeren Erdgeschichte, das heißt während des 66 Millionen Jahre umfassenden Känozoikums (Erdneuzeit).<ref name="10.1038/ngeo2681"></ref> So erwärmte sich die Erde beim Übergang von Eiszeit in eine Zwischeneiszeit binnen ca. 10.000 Jahren etwa um 4 bis 5 °C. Bei der menschengemachten globalen Erwärmung wird jedoch eine Temperaturerhöhung von 4 bis 5 °C binnen 100 Jahren erwartet; die Erwärmungsgeschwindigkeit ist also etwa 100 mal größer als bei historischen natürlichen Klimaveränderungen.

2016 war das wärmste Jahr seit Beginn der systematischen Messungen im Jahr 1880. Es war ca. 1,1 °C wärmer als in vorindustrieller Zeit. So warm war es nach aktuellen Forschungsergebnissen zuletzt am Ende der Eem-Warmzeit vor 115.000 Jahren. 16 der 17 wärmsten jemals gemessenen Jahre traten im 21. Jahrhundert auf, die vier wärmsten Jahre waren in absteigender Reihenfolge 2016, 2017, 2015 und 2014. Der Temperaturanstieg zwischen 1880 und 2012 betrug nach Angaben des Weltklimarates (IPCC) 0,85 K. 2015 stieg die von der Messstation Mauna Loa gemessene mittlere Kohlenstoffdioxidkonzentration in der Erdatmosphäre, die der wichtigste Faktor für die Erwärmung ist, erstmals auf über 400 ppm.<ref name="Blunden/Arndt"></ref>

Im Gegensatz zum Wetter, das kurzfristig-aktuelle Zustände der Atmosphäre beschreibt, werden hinsichtlich des Klimas Mittelwerte über längere Zeiträume erhoben. Üblicherweise werden dabei Normalperioden von jeweils 30 Jahren betrachtet.
Oft werden die Bezeichnungen „Klimawandel“ und „globale Erwärmung“ synonym verwendet, obwohl die Gleichsetzung missverständlich ist: Der natürliche Klimawandel wird mittlerweile vom anthropogenen (menschengemachten) Einfluss deutlich überlagert. Der IPCC schreibt in seinem 2013 erschienenen fünften Sachstandsbericht, dass es extrem wahrscheinlich ist, dass die Menschen mehr als 50 % der 1951–2010 beobachteten Erwärmung verursacht haben. Nach der besten Schätzung stimmt der menschliche Einfluss auf die Erwärmung in etwa mit der insgesamt beobachteten Erwärmung während dieses Zeitraums überein.

Die fortdauernde anthropogene Anreicherung der Erdatmosphäre mit Treibhausgasen (Kohlenstoffdioxid (CO), Methan und Distickstoffmonoxid), die vor allem durch die Nutzung fossiler Energie (Brennstoffe), durch weltumfassende Entwaldung sowie Land- und insbesondere Viehwirtschaft freigesetzt werden, erhöht das Rückhaltevermögen für infrarote Wärmestrahlung in der Troposphäre. Nach Modellrechnungen trägt Kohlenstoffdioxid am meisten zur globalen Erwärmung bei.

Die ersten wissenschaftlichen Erkenntnisse zum anthropogenen Treibhauseffekt stammen aus der Mitte des 19. Jahrhunderts. Etwa ab den 1960er Jahren gab es auf internationaler Ebene erste Gespräche zu dem Thema. Seit etwa Anfang der 1990er Jahre besteht ein wissenschaftlicher Konsens, dass die gegenwärtige globale Erwärmung vom Menschen verursacht wird.

Zu den laut Klimaforschung erwarteten und teils bereits beobachtbaren Folgen der globalen Erwärmung gehören je nach Erdregion: Meereis- und Gletscherschmelze, ein Meeresspiegelanstieg, das Auftauen von Permafrostböden, wachsende Dürrezonen und zunehmende Wetter-Extreme mit entsprechenden Rückwirkungen auf die Lebens- und Überlebenssituation von Menschen und Tieren (Artensterben). Nationale und internationale Klimapolitik zielt sowohl auf das Stoppen des Klimawandels wie auch auf eine Anpassung an die zu erwartende Erwärmung. Um die menschengemachte globale Erwärmung aufhalten zu können, müssen weitere Treibhausgasemissionen langfristig vollständig vermieden werden. Mit Stand 2016 sind bereits ca. der maximal möglichen Emissionen für das im Übereinkommen von Paris vereinbarten Zwei-Grad-Ziel aufgebraucht, sodass die weltweiten Emissionen schnell gesenkt werden müssen, wenn das Ziel noch erreicht werden soll.

Seit der Industriellen Revolution verstärkt der Mensch den natürlichen Treibhauseffekt durch den Ausstoß von Treibhausgasen, wie messtechnisch belegt werden konnte. In der Klimatologie ist es heute Konsens, dass die gestiegene Konzentration der vom Menschen in die Erdatmosphäre freigesetzten Treibhausgase mit hoher Wahrscheinlichkeit die wichtigste Ursache der globalen Erwärmung ist, da ohne sie die gemessenen Temperaturen nicht zu erklären sind.

Treibhausgase lassen die von der Sonne kommende kurzwellige Strahlung weitgehend ungehindert auf die Erde durch, absorbieren aber einen Großteil der von der Erde ausgestrahlten Infrarotstrahlung. Dadurch erwärmen sie sich und emittieren selbst Strahlung im längerwelligen Bereich (vgl. Kirchhoffsches Strahlungsgesetz). Der in Richtung der Erdoberfläche gerichtete Strahlungsanteil wird als atmosphärische Gegenstrahlung bezeichnet. Im isotropen Fall wird die absorbierte Energie je zur Hälfte in Richtung Erde und Weltall abgestrahlt. Hierdurch erwärmt sich die Erdoberfläche stärker, als wenn allein die kurzwellige Strahlung der Sonne sie erwärmen würde. Das IPCC schätzt den Grad des wissenschaftlichen Verständnisses über die Wirkung von Treibhausgasen als „hoch“ ein.

Das Treibhausgas Wasserdampf (HO) trägt mit 36 bis 66 %, Kohlenstoffdioxid (CO) mit 9 bis 26 % und Methan mit 4 bis 9 % zum natürlichen Treibhauseffekt bei. Die große Bandbreite erklärt sich folgendermaßen: Einerseits gibt es sowohl örtlich wie auch zeitlich große Schwankungen in der Konzentration dieser Gase. Zum anderen überlappen sich deren Absorptionsspektren. Beispiel: Strahlung, die von Wasserdampf bereits absorbiert wurde, kann von Kohlenstoffdioxid nicht mehr absorbiert werden. Das bedeutet, dass in einer (Eis-)Wüste, in der Wasserdampf nur wenig zum Treibhauseffekt beiträgt, die übrigen Treibhausgase mehr zum Gesamttreibhauseffekt beitragen als in den feuchten Tropen.

Da die genannten Treibhausgase natürliche Bestandteile der Atmosphäre sind, wird die von ihnen verursachte Temperaturerhöhung als "natürlicher Treibhauseffekt" bezeichnet. Der natürliche Treibhauseffekt führt dazu, dass die Durchschnittstemperatur der Erde bei etwa +14 °C liegt. Ohne den natürlichen Treibhauseffekt läge sie bei etwa −18 °C. Hierbei handelt es sich um rechnerisch bestimmte Werte (siehe auch Idealisiertes Treibhausmodell). In der Literatur können diese Werte gegebenenfalls leicht abweichen, je nach Rechenansatz und der zu Grunde gelegten Annahmen, zum Beispiel dem Reflexionsverhalten "(Albedo)" der Erde. Diese Werte dienen als Nachweis, dass es einen natürlichen Treibhauseffekt gibt, da ohne ihn die Temperatur entsprechend deutlich geringer sein müsste und sich die höhere Temperatur mit dem Treibhauseffekt erklären lässt. Abweichungen von wenigen Grad Celsius spielen bei diesem Nachweis zunächst keine wesentliche Rolle.

Hauptursache für die globale Erwärmung ist die durch menschliche Aktivitäten steigende Treibhausgaskonzentration in der Erdatmosphäre. Im Fünften Sachstandsbericht des IPCC wird der daraus resultierende zusätzliche Strahlungsantrieb im Zeitraum 1750 bis 2011 netto (das heißt nach Abzug ebenfalls kühlender Effekte zum Beispiel durch Aerosole) mit 2,3 W/m² beziffert. Brutto verursachten alle langlebigen Treibhausgase einen Strahlungsantrieb von 2,83 W/m². Bedeutendstes Treibhausgas war CO mit 1,82 W/m², gefolgt von Methan mit 0,48 W/m². Halogenwasserstoffe verursachten einen Strahlungsantrieb von 0,36 W/m², Lachgas 0,17 W/m². Ebenfalls verantwortlich für einen positiven Strahlungsantrieb waren Fluorkohlenwasserstoffe, deren Beitrag nach einem massiven Rückgang der weltweiten Produktion aber inzwischen abnimmt. Von den kurzlebigen Treibhausgasen hat Ozon, dessen Entstehung durch Stickoxide, Kohlenmonoxid oder Kohlenwasserstoffe angeregt wird, mit 0,4 W/m² den höchsten Strahlungsantrieb. Einen negativen (das heißt kühlenden) Strahlungsantrieb in Höhe von –0,9 W/m² verursachen Aerosole. 

Hingegen sind Veränderungen der natürlichen Sonnenaktivität ein unbedeutender Faktor bei der gegenwärtig beobachteten Erderwärmung. Die Sonnenaktivität machte im gleichen Zeitraum einen Strahlungsantrieb von nur 0,1 W/m² aus; seit Mitte des 20. Jahrhunderts ging die Sonnenaktivität sogar zurück.

Der Anteil aller vier Bestandteile des natürlichen Treibhauseffekts in der Atmosphäre ist seit dem Beginn der industriellen Revolution gestiegen. Die Geschwindigkeit des Konzentrationsanstiegs ist die schnellste der letzten 22.000 Jahre.

Die Konzentration von Kohlenstoffdioxid in der Erdatmosphäre ist vor allem durch die Nutzung fossiler Energie, durch die Zementindustrie und großflächige Entwaldung seit Beginn der Industrialisierung von ca. 280 ppmV um 40 % auf ca. 400 ppmV (parts per million, Teile pro Million Volumenanteil) im Jahr 2015 gestiegen. Während der letzten 14 Millionen Jahre (seit dem Mittleren Miozän) existierten keine signifikant höheren CO-Werte als gegenwärtig. Nach Messungen aus Eisbohrkernen betrug die CO-Konzentration in den letzten 800.000 Jahren nie mehr als 300 ppmV.

Der Volumenanteil von Methan stieg von 730 ppbV im Jahr 1750 auf 1.800 ppbV (parts per billion, Teile pro Milliarde Volumenanteil) im Jahr 2011 an. Dies ist ein Anstieg um 150 % und wie bei Kohlenstoffdioxid der höchste Stand seit mindestens 800.000 Jahren. Als Hauptursache hierfür gilt derzeit die Viehhaltung, gefolgt von weiteren landwirtschaftlichen Aktivitäten wie dem Anbau von Reis. Das Treibhauspotenzial von 1 kg Methan ist, auf einen Zeitraum von 100 Jahren betrachtet, 25mal höher als das von 1 kg Kohlenstoffdioxid. Nach einer neueren Untersuchung beträgt dieser Faktor sogar 33, wenn Wechselwirkungen mit atmosphärischen Aerosolen berücksichtigt werden. In einer sauerstoffhaltigen Atmosphäre wird Methan jedoch oxidiert, meist durch Hydroxyl-Radikale. Ein einmal in die Atmosphäre gelangtes Methan-Molekül hat dort eine durchschnittliche Verweilzeit von zwölf Jahren.

Im Unterschied dazu liegt die Verweildauer von Kohlenstoffdioxid teilweise im Bereich von Jahrhunderten. Die Ozeane nehmen atmosphärisches Kohlenstoffdioxid zwar sehr rasch auf: Ein CO-Molekül wird nach durchschnittlich fünf Jahren in den Ozeanen gelöst. Diese geben es aber auch wieder an die Atmosphäre ab, so dass ein Teil des vom Menschen emittierten Kohlenstoffdioxids letztlich für mehrere Jahrhunderte (ca. 30 %) und ein weiterer Teil (ca. 20 %) sogar für Jahrtausende im Kohlenstoffkreislauf von Hydrosphäre und Atmosphäre verbleibt.

Der Volumenanteil von Lachgas stieg von vorindustriell 270 ppbV auf mittlerweile 323 ppbV. Durch sein Absorptionsspektrum trägt es dazu bei, ein sonst zum Weltall hin offenes Strahlungsfenster zu schließen. Trotz seiner sehr geringen Konzentration in der Atmosphäre trägt es zum anthropogenen Treibhauseffekt etwa 6 % bei, da seine Wirkung als Treibhausgas 298mal stärker ist als die von Kohlenstoffdioxid; daneben hat es auch eine recht hohe atmosphärische Verweilzeit von 114 Jahren.

Die Wasserdampfkonzentration der Atmosphäre wird durch anthropogene Wasserdampfemissionen nicht signifikant verändert, da zusätzlich in die Atmosphäre eingebrachtes Wasser innerhalb weniger Tage auskondensiert. Steigende globale Durchschnittstemperaturen führen jedoch zu einem höheren Dampfdruck, das heißt einer stärkeren Verdunstung. Der damit global ansteigende Wasserdampfgehalt der Atmosphäre treibt die globale Erwärmung zusätzlich an. Wasserdampf wirkt somit im Wesentlichen als Rückkopplungsglied. Diese Wasserdampf-Rückkopplung ist neben der Eis-Albedo-Rückkopplung die stärkste, positiv wirkende Rückkopplung im globalen Klimageschehen.

Neben Treibhausgasen beeinflussen auch die Sonnenaktivität sowie Aerosole das Erdklima.
Aerosole liefern von allen festgestellten Beiträgen zum Strahlungsantrieb die größte Unsicherheit, und das Verständnis über sie wird vom IPCC als „gering“ bezeichnet. Die Wirkung eines Aerosols auf die Lufttemperatur ist abhängig von seiner Flughöhe in der Atmosphäre. In der untersten Atmosphärenschicht, der Troposphäre, sorgen Rußpartikel für einen Temperaturanstieg, da sie das Sonnenlicht absorbieren und anschließend Wärmestrahlung abgeben. Die verringerte Reflektivität (Albedo) von Schnee- und Eisflächen und anschließend darauf niedergegangenen Rußpartikeln wirkt ebenfalls erwärmend. In höheren Luftschichten hingegen sorgen Mineralpartikel durch ihre abschirmende Wirkung dafür, dass es an der Erdoberfläche kühler wird.

Einen großen Unsicherheitsfaktor bei der Bemessung der Klimawirkung von Aerosolen stellt ihr Einfluss auf die ebenfalls nicht vollständig verstandene Wolkenbildung dar. Trotz der Unsicherheiten wird Aerosolen insgesamt eine deutlich abkühlende Wirkung zugemessen.

Der zwischen den 1940er bis Mitte der 1970er Jahre beobachtete Rückgang der globalen Durchschnittstemperaturen sowie die zeitweise Stagnation der globalen Durchschnittstemperaturen nach dem Jahr 2000 wird zum großen Teil der kühlenden Wirkung von Sulfataerosolen zugeschrieben, die im ersten Fall in Europa und den USA und im letzten Fall in der Volksrepublik China und Indien zu verorten waren.

Eine Reihe von Faktoren beeinflussen das globale Klimasystem. In der Diskussion um die Ursachen der globalen Erwärmung werden oft Faktoren genannt, die nachrangig sind oder sogar kühlend auf das Klimasystem wirken.

So ist eine veränderte kosmische Strahlung nicht für die gegenwärtig beobachtete Erwärmung verantwortlich.

Die Erde befindet sich seit ca. 1850, also etwa seit dem Beginn der industriellen Revolution, in einer Phase der Wiedererwärmung aus der kleinen Eiszeit. Ohne die Eingriffe des Menschen in den natürlichen Klimaverlauf würde sich der seit 6000 Jahren bestehende Abkühlungstrend von 0,10 bis 0,15 °C pro Jahrtausend fortsetzen und – je nach Literaturquelle – in 20.000 bis 50.000 Jahren in eine neue Kaltzeit führen.

Die Annahme, das Ozonloch sei eine wesentliche Ursache der globalen Erwärmung, ist ebenso falsch, denn der Ozonabbau wärmt nicht das Klima der Erde, sondern hat einen leicht kühlenden Effekt. Der Ozonabbau wirkt hierbei auf zweierlei Arten: Die verringerte Ozonkonzentration kühlt die Stratosphäre, da die UV-Strahlung dort nicht mehr absorbiert wird, wärmt hingegen die Troposphäre, wo sie absorbiert wird. Die kältere Stratosphäre schickt weniger wärmende Infrarotstrahlung nach unten und kühlt damit die Troposphäre. Insgesamt dominiert der Kühlungseffekt, so dass das IPCC folgert, dass der beobachtete Ozonschwund im Verlauf der letzten beiden Dekaden zu einem negativen Strahlungsantrieb auf das Klimasystem geführt hat, der sich auf etwa −0,15 ± 0,10 Watt pro Quadratmeter (W/m²) beziffern lässt.

Veränderungen in der Sonne wird ein geringer Einfluss auf die gemessene globale Erwärmung zugesprochen. Die seit 1978 direkt vom Orbit aus gemessene Änderung der Sonnenaktivität ist bei weitem zu klein, um als Hauptursache für die seither beobachtete Temperaturentwicklung in Frage zu kommen. Seit den 1960er Jahren ist der Verlauf der globalen Durchschnittstemperatur von der Sonnenaktivität entkoppelt.

Das IPCC schätzt den zusätzlichen Strahlungsantrieb durch die Sonne seit Beginn der Industrialisierung auf etwa 0,12 Watt pro Quadratmeter. Das 90-Prozent-Konfidenzintervall für diese Schätzung wird mit 0,06 bis 0,30 W/m² angegeben; im Vergleich dazu tragen die anthropogenen Treibhausgase mit 2,63 (± 0,26) W/m² zur Erwärmung bei. Das IPCC schreibt, dass der Grad des wissenschaftlichen Verständnisses bezüglich des Einflusses solarer Variabilität (siehe auch Streuung) vom Dritten zum Vierten Sachstandsbericht von „sehr gering“ auf „gering“ zugenommen hat.

Bei fast allen Prozessen entsteht Wärme, so bei der Produktion von elektrischem Strom, bei der Nutzung von Verbrennungsmotoren (siehe Wirkungsgrad) oder beim Betrieb von Computern. In den USA und Westeuropa trugen Gebäudeheizung, industrielle Prozesse und Verbrennungsmotoren im Jahr 2008 mit 0,39 W/m² bzw. 0,68 W/m² zur Erwärmung bei und haben damit einen gewissen Einfluss auf das regionale Klimageschehen. Weltweit gesehen betrug dieser Wert 0,028 W/m² (also nur etwa 1 % der globalen Erwärmung). Merkliche Beiträge zur Erwärmung sind für den Fall des weiteren ungebremsten Anstiegs der Energieerzeugung (wie in den vergangenen Jahrzehnten) ab dem Ende unseres Jahrhunderts zu erwarten.

Als Hauptanzeichen für die derzeitige globale Erwärmung gelten die seit etwa 1860 vorliegenden weltweiten Temperaturmessungen sowie die Auswertungen verschiedener Klimaarchive. Verglichen mit den Schwankungen der Jahreszeiten sowie beim Wechsel von Tag und Nacht erscheinen die im Folgenden genannten Zahlen klein; als globale Änderung des Klimas bedeuten sie jedoch sehr viel, wenn man die um nur etwa 6 K niedriger liegende Durchschnittstemperatur auf der Erde während der letzten Eiszeit bedenkt.

Im Jahr 2005 wurde u. a. aufgrund der gemessenen Temperaturzunahme der Meere über eine Dekade errechnet, dass die Erde 0,85 Watt pro Quadratmeter mehr Leistung aufnimmt als sie ins All abstrahlt.<ref name="DOI10.1126/science.1110252">J. Hansen: "Earth’s Energy Imbalance: Confirmation and Implications." In: "Science." 308, 2005, S. 1431, .</ref>

Laut einer im Jahr 2016 erschienenen Publikation begann die globale Durchschnittstemperatur bereits seit dem Jahr 1830 aufgrund menschlicher Aktivitäten zu steigen. Dies wurde im Rahmen einer breit angelegten Studie gefunden, bei der eine große Zahl weltweit verteilter, paläoklimatologischer Anzeiger vergangener Zeiten (sogenannte Klimaproxys) ausgewertet wurden. Zu dieser Zeit gab es noch kein dichtes Netz von Temperaturmessstationen.<ref name="DOI10.1038/nature19082">

</ref> Eine deutliche Erwärmungsphase war zwischen 1910 und 1945 zu beobachten, in der aufgrund der noch vergleichsweise geringen Konzentration von Treibhausgasen auch natürliche Schwankungen einen deutlichen Einfluss hatten. Am ausgeprägtesten ist die Erwärmung von 1975 bis heute. Global war das Jahr 2015 vor dem Jahr 2014 das mit Abstand heißeste jemals gemessene Jahr. Die Oberflächentemperatur lag um 0,76 °C über dem Durchschnitt der Jahre 1961–1990 und ca. 1 °C über dem Durchschnitt der Jahre 1850-1900.

2016 war das wärmste Jahr seit Beginn der Messungen im Jahr 1880. Es war ca. 1,1 °C wärmer als in vorindustrieller Zeit. 2017 war das bisher wärmste Nicht-El-Niño-Jahr und zugleich das zweitwärmste Jahr seit Beginn der Messungen. 16 der 17 wärmsten jemals gemessenen Jahre traten im 21. Jahrhundert auf, die drei wärmsten Jahre waren in absteigender Reihenfolge 2016, 2015 und 2014. Nach Zahlen von Copernicus-Programms lag die Erwärmung sogar um 1,3 °C oberhalb des Niveaus der vorindustriellen Zeit, womit die politisch anvisierte Grenze von 1,5 °C zeitweise nahezu erreicht war. Gegenüber dem Jahr 2015 hat die zusätzliche Erwärmung 0,2 °C betragen.

Zwischen 1880 und 2012 nahmen die global gemittelten, bodennahen Lufttemperaturen um 0,85 °C zu. Insbesondere bei kurzen Zeitreihen ist zu berücksichtigen, dass Anfangs- und Endjahr starken Einfluss auf den Trend haben können und somit nicht zwingend langfristige Trends widerspiegeln müssen. Ein Beispiel für eine solche Abweichung ist der Zeitraum zwischen 1998 und 2012, der mit einem starken El Niño und damit außergewöhnlich heißen Jahr begann, weshalb der Erwärmungstrend mit 0,05 °C pro Jahrzehnt in diesem Zeitraum deutlich unter dem langfristigen Trend von 0,12 °C pro Jahrzehnt im Zeitraum 1951 bis 2012 zurückblieb. Trotzdem waren die 30 Jahre von 1983 bis 2012 auf der Nordhalbkugel die wärmste Normalperiode seit 1400 Jahren.

In einer 2007 erschienenen Studie konnte der natürliche Anteil der Erwärmung des 20. Jahrhunderts auf unter 0,2 K eingegrenzt werden. 

Neben der Luft haben sich auch die Ozeane erwärmt, die über 90 % der Wärmeenergie aufgenommen haben. Während sich die Weltmeere seit 1955 aufgrund ihres enormen Volumens und ihrer großen Temperaturträgheit insgesamt nur um 0,04 K aufgeheizt haben, erhöhte sich ihre Oberflächentemperatur im selben Zeitraum um 0,6 K. Im Bereich zwischen der Meeresoberfläche bis zu einer Tiefe von 75 Metern stieg die Temperatur von 1971 bis 2010 um durchschnittlich 0,11 K pro Jahrzehnt an.

Der Energieinhalt der Weltmeere nahm zwischen Mitte der 1950er Jahre bis 1998 um ca. 14,5 × 10 Joule zu, was einer Heizleistung von 0,2 Watt pro m² der gesamten Erdoberfläche entspricht.<ref name="DOI10.1029/2004GL021592">S. Levitus: "Warming of the world ocean, 1955–2003." In: "Geophysical Research Letters." 32, 2005, .</ref> Die Energiezunahme der Weltmeere in Höhe von 14,5 × 10 Joule entspricht der Energie von 100 Millionen Hiroshima-Atombomben; diese Energiemenge würde die unteren 10 Kilometer der Atmosphäre um 22 K erwärmen. Über den Zeitraum 1971 und 2016 lag die gemittelte Wärmeaufnahme der Ozeane bei einer Leistung von etwa 200 Terawatt und damit mehr als 10 Mal so hoch wie der komplette Weltenergieverbrauch der Menschheit.

Seit dem Jahr 2000 wird der Wärmeinhalt der Ozeane mit Hilfe des Argo-Programms vermessen, wodurch seit dieser Zeit erheblich genauere Daten über den Zustand wie auch die Veränderung von klimatologisch relevanten Messwerten (z. B. Wärmeinhalt, Salinität, Tiefenprofil) verfügbar sind.

Die 10 wärmsten Jahre im Zeitraum von 1880 bis 2016 – Abweichung von der langjährigen Durchschnitts-Temperatur in °C

Luft über Landflächen erwärmt sich allgemein stärker als über Wasserflächen, was in der zweiten Abbildung dieses Artikels erkennbar ist. Die Erwärmung der Landflächen zwischen 1970 und 2014 lag im Mittel bei 0,26 K und damit mehr als doppelt so hoch wie über dem Meer, das sich im selben Zeitraum um 0,12 K erwärmte. Dementsprechend stiegen die Temperaturen auf der Nordhalbkugel, auf der sich der Großteil der Landflächen befindet, in den vergangenen 100 Jahren stärker an als auf der Südhalbkugel, wie auch die nebenstehende Grafik zeigt.

Die Nacht- und Wintertemperaturen stiegen etwas stärker an als die Tages- und Sommertemperaturen. Aufgeteilt nach Jahreszeiten wurde die größte Erwärmung während der Wintermonate gemessen, und dabei besonders stark über dem westlichen Nordamerika, Skandinavien und Sibirien. Im Frühling stiegen die Temperaturen am stärksten in Europa sowie in Nord- und Ostasien an. Im Sommer waren Europa und Nordafrika am stärksten betroffen, und im Herbst entfiel die größte Steigerung auf den Norden Nordamerikas, Grönland und Ostasien. Besonders markant fiel die Erwärmung in der Arktis aus, wo sie im jährlichen Mittel etwa doppelt so hoch ist wie im globalen Durchschnitt. Mit Ausnahme weniger Regionen ist die Erwärmung seit 1979 weltweit nachweisbar.

Für die verschiedenen Luftschichten der Erdatmosphäre wird theoretisch eine unterschiedliche Erwärmung erwartet und faktisch auch gemessen. Während sich die Erdoberfläche und die niedrige bis mittlere Troposphäre erwärmen sollten, lassen Modelle für die höher gelegene Stratosphäre eine Abkühlung vermuten. Tatsächlich wurde genau dieses Muster in Messungen gefunden. Die Satellitendaten zeigen eine Abnahme der Temperatur der unteren Stratosphäre von 0,314 K pro Jahrzehnt während der letzten 30 Jahre. Diese Abkühlung wird zum einen durch den verstärkten Treibhauseffekt und zum anderen durch Ozonschwund durch FCKWs in der Stratosphäre verursacht, siehe auch Montrealer Protokoll zum Schutz der Ozonschicht. Wäre die Sonne maßgebliche Ursache, hätten sich sowohl die oberflächennahen Schichten, die niedere bis mittlere Troposphäre wie auch die Stratosphäre erwärmen müssen. Nach dem gegenwärtigen Verständnis heißt dies, dass der überwiegende Teil der beobachteten Erwärmung durch menschliche Aktivitäten verursacht sein muss.

Auch bei Annahme einer Erwärmung um 4 K bis zum Ende des 21. Jahrhunderts wird es im Verlauf immer wieder Phasen der Stagnation oder sogar der Abkühlung geben. Diese Phasen können bis zu ca. 15 Jahre andauern. Ursachen sind der elfjährige Sonnenfleckenzyklus, kühlende starke Vulkanausbrüche sowie die natürliche Eigenschaft des Weltklimas, einen schwingenden Temperaturverlauf zu zeigen (AMO, PDO, ENSO). So kann beispielsweise das Auftreten von El-Niño- bzw. La-Niña-Ereignissen die globale Durchschnittstemperatur von einem Jahr auf das andere um 0,2 K erhöhen bzw. absenken und für wenige Jahre den jährlichen Erwärmungstrend von ca. 0,02 K überdecken, aber auch verstärken.<ref name="DOI10.1002/2013EF000165"></ref>

Das globale Klimasystem ist von Rückkopplungen geprägt, die Temperaturveränderungen verstärken oder abschwächen. Eine die Ursache verstärkende Rückkopplung wird als positive Rückkopplung bezeichnet. Bei bestimmten Zuständen des globalen Klimageschehens sind nach heutigem Kenntnisstand die positiven Rückkopplungen deutlich stärker als die negativen Rückkopplungen, so dass das Klimasystem in einen anderen Zustand kippen kann.

Die beiden stärksten, positiv wirkenden Rückkopplungsprozesse sind die Eis-Albedo-Rückkopplung und die Wasserdampf-Rückkopplung. Ein Abschmelzen der Polkappen bewirkt durch verminderte Reflexion einen zusätzlichen Energieeintrag über die Eis-Albedo-Rückkopplung. Die Wasserdampfrückkopplung entsteht dadurch, dass die Atmosphäre einer wärmeren Welt auch mehr Wasserdampf enthält. Da Wasserdampf das mit Abstand mächtigste Treibhausgas ist, wird dadurch ein eingeleiteter Erwärmungsprozess weiter verstärkt – unabhängig davon, was diese Erwärmung letztlich ausgelöst hat. Gleiches gilt auch bei einer Abkühlung, die durch dieselben Prozesse weiter verstärkt wird. Zur quantitativen Beschreibung der Reaktion des Klimas auf Veränderungen der Strahlungsbilanz wurde der Begriff der Klimasensitivität etabliert. Mit ihr lassen sich unterschiedliche Einflussgrößen gut miteinander vergleichen.

Eine weitere positive Rückkopplung erfolgt durch das CO selbst. Mit zunehmender Erderwärmung wird auch das Wasser in den Ozeanen wärmer und kann dadurch weniger CO aufnehmen. Als Folge davon gelangt vermehrt CO in die Atmosphäre, was den Treibhauseffekt zusätzlich verstärkt.

Neben diesen drei physikalisch gut verstandenen Rückkopplungen existieren jedoch noch weitere Rückkopplungsfaktoren, deren Wirken weit schwieriger abschätzbar ist:

Wolken beeinflussen das Klima der Erde maßgeblich, indem sie einen Teil der einfallenden Strahlung reflektieren. Strahlung, die von der Sonne kommt, wird zurück ins All, Strahlung darunter liegender Atmosphärenschichten in Richtung Boden reflektiert. Die Helligkeit der Wolken stammt von kurzwelliger Strahlung im sichtbaren Wellenlängenbereich.

Eine größere optische Dicke niedriger Wolken bewirkt, dass mehr Energie ins All zurückgestrahlt wird; die Temperatur der Erde sinkt. Umgekehrt lassen weniger dichte Wolken mehr Sonnenstrahlung passieren, was darunter liegende Atmosphärenschichten wärmt. Niedrige Wolken sind oft dicht und reflektieren viel Sonnenlicht zurück in den Weltraum. Sie liegen auch niedriger in der Atmosphäre, wo Temperaturen höher sind, und strahlen deshalb mehr Wärme ab. Die Tendenz niedriger Wolken ist daher, die Erde zu kühlen.

Hohe Wolken sind meist dünn und nicht sehr reflektierend. Sie lassen einen Großteil der Sonnenwärme durch und da sie sehr hoch liegen, wo die Lufttemperatur sehr niedrig ist, strahlen diese Wolken nicht viel Wärme ab. Die Tendenz hoher Wolken ist, die Erde zu erwärmen.

Die Vegetation und die Beschaffenheit des Bodens und insbesondere seine Versiegelung, Entwaldung oder landwirtschaftliche Nutzung haben maßgeblichen Einfluss auf die Verdunstung und somit auf die Wolkenbildung und das Klima. Nachgewiesen wurde ebenfalls eine Verminderung der Wolkenbildung durch Pflanzen, welche bei einem Kohlenstoffdioxid-Anstieg bis zu 15 Prozent weniger Wasserdampf freigeben. Das wiederum reduziert die Wolkenbildung.

Insgesamt wird die globale Erwärmung durch Wolken-Rückkopplungen wahrscheinlich noch verstärkt.

Vegetation und Bodenbeschaffenheit reflektieren je nach Beschaffenheit das einfallende Sonnenlicht unterschiedlich. Reflektiertes Sonnenlicht wird als kurzwellige Sonnenstrahlung in den Weltraum zurückgeworfen (ansonsten wäre die Erdoberfläche aus Sicht des Weltalls ohne Infrarotkamera schwarz). Albedo ist ein Maß für das Rückstrahlvermögen von diffus reflektierenden (reemittierenden), also nicht spiegelnden und nicht selbst leuchtenden Oberflächen.

Nicht nur der Verbrauch von fossilen Energieträgern führt zu einer Freisetzung von Treibhausgasen. Die intensive Bestellung von Ackerland und die Entwaldung sind ebenfalls eine bedeutende Treibhausgasquelle. Die Vegetation benötigt für den Prozess der Photosynthese Kohlenstoffdioxid zum Wachsen. Bäume benötigen CO in größeren Mengen als Getreide. Der Boden ist eine wichtige Senke, da er organisches, kohlenstoffhaltiges Material enthält. Durch ackerbauliche Tätigkeiten wird dieser gespeicherte Kohlenstoff in Form von Kohlenstoffdioxid jedoch teilweise freigesetzt.

Im Permafrost Westsibiriens lagern 70 Milliarden Tonnen Methan, in der Tiefsee ungleich größere Mengen Gashydratvorkommen. Durch lokale Klimaveränderungen (aktuell: +3 K innerhalb von 40 Jahren in Westsibirien) könnten auch bei geringer globaler Erwärmung regional kritische Temperaturen erreicht werden; es besteht die Gefahr der Freisetzung der dort gespeicherten Methanressourcen in die Atmosphäre.

Eine Berechnung unter Annahme derartiger Rückkopplungen wurde von Wissenschaftlern der University of California, Berkeley erstellt, die annahmen, dass der Kohlenstoffdioxidgehalt der Atmosphäre sich von den derzeitigen etwa 390 ppmV bis 2100 auf etwa 550 ppmV erhöhen wird. Dies sei allein der von der Menschheit bewirkte anthropogene Zuwachs. Die erhöhte Temperatur führt zu zusätzlicher Freisetzung von Treibhausgasen, insbesondere Kohlenstoffdioxid und Methan. Bei ansteigender Temperatur erfolgt eine erhöhte Freisetzung von Kohlenstoffdioxid aus den Weltmeeren und die beschleunigte Verrottung von Biomasse, was zusätzliches Methan und Kohlenstoffdioxid freisetzt. Durch diese positive Rückkopplung könnte die globale Erwärmung um 2 K stärker ausfallen als gegenwärtig angenommen wird. Aus diesem und anderen Gründen schätzt Barrie Pittock in "Eos", der Publikation der American Geophysical Union, dass die zukünftige Erwärmung über die vom IPCC genannten Bandbreiten hinausgehen könnte. Er nennt acht Gründe für seine Vermutung, darunter unter anderem auch den Rückgang der globalen Verdunkelung und Rückkopplungseffekte durch Biomasse.

Bei einer Verdoppelung der CO-Konzentration in der Atmosphäre gehen Klimaforscher davon aus, dass die Erhöhung der Erdmitteltemperatur innerhalb von 1,5 bis 4,5 K liegen wird. Dieser Wert ist auch als Klimasensitivität bekannt und ist auf das vorindustrielle Niveau (von 1750) bezogen, ebenso wie der dafür maßgebende Strahlungsantrieb; mit dieser Größe werden alle bekannten, die Strahlungsbilanz der Erde beeinflussenden Faktoren vom IPCC quantitativ beschrieben und vergleichbar gemacht. Das IPCC rechnet, abhängig von den Zuwachsraten aller Treibhausgase und dem angewandten Modell, bis 2100 mit einer Zunahme der globalen Durchschnittstemperatur um 0,9 bis 5,4 K. Zum Vergleich: Die schnellste Erwärmung im Verlauf von der letzten Eiszeit zur heutigen Warmzeit war eine Erwärmung um etwa ein Grad pro 1000 Jahre.

Nach einer Studie an der Carnegie Institution for Science, in der die Ergebnisse eines Kohlenstoff-Zyklus-Modells mit Daten aus Vergleichsuntersuchungen zwischen Klimamodellen des fünften IPCC-Sachstandsberichts ausgewertet wurden, reagiert das globale Klimasystem auf einen CO-Eintrag mit einer zeitlichen Verzögerung von etwa 10 Jahren mit einer Sprungfunktion; das bedeutet, dass die Erwärmung nach etwa 10 Jahren ihr Maximum erreicht und dann für sehr lange Zeiträume dort verharrt.

Der Climate Action Tracker gibt die wahrscheinlichste, bis zum Ende dieses Jahrhunderts zu erwartende Erderwärmung an. Demnach ist die Welt aktuell (2016) auf dem Weg zu einer Erwärmung um 3,6 °C im Vergleich zur vorindustriellen globalen Durchschnittstemperatur. Zur Berechnung dieses Wertes werden die Selbstverpflichtungen der wichtigsten Emittenten, die Treibhausgasemissionen zu verringern, in ein Klimamodell eingespeist.

Der dabei maßgebliche, allerdings auch der mit der größten Unsicherheit behaftete Parameter ist die Prognose über die künftige Entwicklung der Weltwirtschaft. Da das Wirtschaftswachstum der Welt in der Vergangenheit stark mit dem Verbrauch an fossilen Energieträgern korrelierte und dies auch in der näheren Zukunft erwartet werden kann, erklärt sich hieraus die relativ große Bandbreite der von den Klimatologen projizierten globalen Erwärmung.

Ein weiterer wahrscheinlicher Einfluss ist ein Rückgang der Förderung konventionellen Erdöls aufgrund des Eintretens des globalen Erdölfördermaximums (des sogenannten „Peak Oil“), das von vielen Experten bis etwa 2030, möglicherweise jedoch auch deutlich früher, erwartet wird. Wird das dann fehlende Öl durch nicht-konventionelles Erdöl wie z. B. Ölsande ausgeglichen, so kann sich die Menge an Treibhausgasen bis zu einem Faktor von 2,5 vergrößern und Anstrengungen zur Reduktion von Emissionen zunichtemachen.

Nach einer im Jahr 2009 erschienenen Studie wird die gegenwärtig bereits angestoßene Erwärmung noch für mindestens 1000 Jahre irreversibel sein, selbst wenn heute alle Treibhausgasemissionen vollständig gestoppt würden. In weiteren Szenarien wurden die Emissionen schrittweise bis zum Ende unseres Jahrhunderts fortgesetzt und dann ebenfalls abrupt beendet. Dabei wurden wesentliche Annahmen und Aussagen, die im 4. IPCC-Bericht über die folgenden 1000 Jahre gemacht wurden,<ref name="DOI10.1175/2007JCLI1905.1">G.-K. Plattner, Reto Knutti u. a.: "Long-Term Climate Commitments Projected with Climate–Carbon Cycle Models." In: "Journal of Climate." 21, 2008, S. 2721, .</ref> bestätigt und verfeinert. Langfristige Klimasimulationen deuten darauf hin, dass sich die von einer erhöhten Kohlenstoffdioxidkonzentration aufgeheizte Erde nur um ca. ein Grad pro 12.000 Jahre abkühlen wird.

Ein komplettes Verbrennen der fossilen Energieressourcen, die konservativ auf 5 Billionen Tonnen Kohlenstoff geschätzt werden, würde hingegen zu einem weltweiten Temperaturanstieg von ca. 6,4 bis 9,5 °C führen, was sehr starke negative Auswirkungen auf Ökosysteme, menschliche Gesundheit, Landwirtschaft, die Wirtschaft usw. hätte. Würden neben konventionellen auch unkonventionelle Ressourcen verbrannt, könnte die Kohlendioxidkonzentration in der Erdatmosphäre bis auf ca. 5000 ppm bis zum Jahr 2400 ansteigen. Neben einer enormen Temperaturerhöhung würde hierbei der Antarktische Eisschild fast vollständig abschmelzen, womit der Meeresspiegel auch ohne Einberechnung des grönländischen Eisschildes um ca. 58 m steigen würde.

Aufbauend auf der Entdeckung des Treibhauseffektes durch Jean Baptiste Joseph Fourier im Jahr 1824 identifizierte John Tyndall 1862 einige der für diesen Effekt verantwortlichen Gase, allen voran Wasserdampf und Kohlenstoffdioxid. Hieran anknüpfend veröffentlichte Svante Arrhenius 1896 als Erster die Hypothese, dass die anthropogene CO-Anreicherung in der Atmosphäre die Erdtemperatur erhöhen könne, womit die „Wissenschaft von der globalen Erwärmung“ im engeren Sinne begann.

Im Jahr 1908 publizierte der britische Meteorologe und spätere Präsident der Royal Meteorological Society Ernest Gold ein Paper zur Stratosphäre. Er schrieb darin, dass die Temperatur der Tropopause mit steigender CO-Konzentration steigt. Es ist dies ein Kennzeichen der globalen Erwärmung, das fast ein Jahrhundert später auch gemessen werden konnte.

In den späten 1950er Jahren wurde erstmals nachgewiesen, dass der Kohlenstoffdioxidgehalt der Atmosphäre ansteigt. Auf Initiative von Roger Revelle startete Charles David Keeling 1958 auf dem Berg Mauna Loa (Hawaii, Big Island) regelmäßige Messungen des CO-Gehalts der Atmosphäre (Keeling-Kurve). Gilbert Plass nutzte 1956 erstmals Computer und erheblich genauere Absorptionsspektren des CO zur Berechnung der zu erwartenden Erwärmung. Er erhielt 3,6 K (3,6 °C) als Wert für die Klimasensitivität.

Die ersten Computerprogramme zur Modellierung des Weltklimas wurden Ende der 1960er Jahre geschrieben.

1979 schrieb die National Academy of Sciences der USA im sog. Charney-Report, dass ein Anstieg der Kohlenstoffdioxidkonzentration ohne Zweifel mit einer signifikanten Klimaerwärmung verknüpft sei. Deutliche Effekte seien aufgrund der Trägheit des Klimasystems jedoch erst in einigen Jahrzehnten zu erwarten.

Mit der Aussage von James E. Hansen vor dem "Energy and Natural Resources Committee" des US-Senats am 23. Juni 1988 hat zum ersten Mal ein Wissenschaftler vor einem politischen Gremium geäußert, er sei zu 99 Prozent davon überzeugt, dass die Jahresrekordtemperatur nicht das Resultat natürlicher Schwankungen ist. Bereits in dieser Sitzung wurden Forderungen nach politischen Maßnahmen gestellt, um die globale Erwärmung zu verlangsamen. Dazu gehörte die Schaffung des Weltklimarats (IPCC), der den politischen Entscheidungsträgern und Regierungen zuarbeiten soll. Im IPCC wird der wissenschaftliche Erkenntnisstand zur globalen Erwärmung und zum anthropogenen Anteil daran diskutiert und in Berichten zusammengefasst.

Die Erforschung von Ursachen und Folgen der globalen Erwärmung ist seit ihrem Beginn eng mit der Analyse der klimatischen Bedingungen vergangener Zeiten verknüpft. Svante Arrhenius, der als Erster darauf hinwies, dass der Mensch durch die Emission von CO die Erde erwärmt, erkannte bei der Suche nach den Ursachen der Eiszeiten den klimatischen Einfluss wechselnder Konzentrationen von Kohlenstoffdioxid in der Erdatmosphäre.

So wie Erdbeben und Vulkanausbrüche sind auch Klimawandel etwas Natürliches. Seit der Entstehung der Erde hat sich das irdische Klima ständig verändert und es wird sich auch künftig ändern. In erster Linie verantwortlich dafür waren eine wechselnde Konzentration und Zusammensetzung der Treibhausgase in der Atmosphäre durch die unterschiedliche Intensität von Vulkanismus und Erosion. Weitere klimawirksame Faktoren sind die variable Sonneneinstrahlung, unter anderem auf Grund der Milanković-Zyklen, sowie eine durch die Plattentektonik verursachte permanente Umgestaltung und Verschiebung der Kontinente mit einer daraus resultierenden Verlagerung großer Meeresströmungen. Landmassen an den Polen förderten die Bildung von Eiskappen, und veränderte ozeanische Strömungen lenkten Wärme entweder von den Polen weg oder zu diesen hin und beeinflussten auf diese Weise die Stärke der sehr mächtigen Eis-Albedo-Rückkopplung.

Obwohl Leuchtkraft und Strahlungsleistung der Sonne am Beginn der Erdgeschichte etwa 30 Prozent unter den heutigen Werten lagen, herrschten in der gesamten Zeit Bedingungen, unter denen flüssiges Wasser existieren konnte. Dieses Paradoxon der schwachen, jungen Sonne genannte Phänomen führte in den 1980er Jahren zur Hypothese eines „CO-Thermostats“. Er hielt die Temperaturen der Erde über Jahrmilliarden konstant in Bereichen, die Leben auf unserem Planeten ermöglichten.

Wenn Vulkane vermehrt CO ausstießen, so dass die Temperaturen anstiegen, erhöhte sich der Grad der Verwitterung, wodurch mehr CO gebunden wurde. War die Erde kalt und die Konzentration des Treibhausgases gering, wurde die Verwitterung durch die Vereisung weiter Landflächen stark verringert. Das durch den Vulkanismus weiter in die Atmosphäre strömende Treibhausgas reicherte sich dort bis zu einem gewissen Kipppunkt an, um schließlich ein globales Tauwetter auszulösen. Der Nachteil dieses Mechanismus besteht darin, dass er mehrere Jahrtausende für die Korrektur von Treibhausgaskonzentrationen und Temperaturen benötigt, und es sind mehrere Fälle bekannt, bei denen er versagte.

Im Verlauf – vermutlich mehrerer – Schneeball-Erde-Ereignisse während des Neoproterozoikums vor rund 750 bis 600 Millionen Jahren fror die Erdoberfläche fast vollkommen zu, und zur Zeit des wahrscheinlich größten Massenaussterbens vor 250 Millionen Jahren war der Planet ein Supertreibhaus mit drastisch höheren Temperaturen als heute.

Man nimmt an, dass die große Sauerstoffkatastrophe vor 2,3 Milliarden Jahren einen Zusammenbruch der Methankonzentration in der Atmosphäre bewirkte. Dies verminderte den Treibhauseffekt so stark, dass daraus eine großflächige und lang andauernde Vereisung der Erde während der huronischen Eiszeit resultierte. Das letzte derartige Ereignis fand unmittelbar vor der kambrischen Explosion vor 635 Millionen Jahren statt und wird Marinoische Eiszeit genannt. Die helle Oberfläche der fast vollständig gefrorenen Erde reflektierte nahezu die gesamte einfallende Sonnenenergie zurück ins All und hielt die Erde so im Eiszeitzustand gefangen; dies änderte sich erst, als die Konzentration von Kohlenstoffdioxid in der Erdatmosphäre, bedingt durch den unter dem Eis fortdauernden Vulkanismus, auf extrem hohe Werte gestiegen war. Da das CO-Thermostat auf Veränderungen nur träge reagiert, taute die Erde nicht nur auf, sondern stürzte in der Folge für einige Jahrzehntausende in das andere Extrem eines Supertreibhauses. Das Ausmaß der Vereisung ist jedoch in der Wissenschaft umstritten, weil Klimadaten aus dieser Zeit ungenau und lückenhaft sind.
Das Supertreibhaus, das vor 250 Millionen Jahren an der Perm-Trias-Grenze fast alles Leben auf der Erde auslöschte, wurde sehr wahrscheinlich von einer lang andauernden intensiven Vulkantätigkeit verursacht, die zur Entstehung des sibirischen Trapp führte. Aktuelle Isotopenuntersuchungen deuten darauf hin, dass sich die damaligen Meere innerhalb eines relativ kurzen Zeitraums um bis zu 8 K erwärmten und parallel dazu stark versauerten. Während dieser und anderer Phasen extrem hoher Temperaturen enthielten die Ozeane zu großen Teilen keinen Sauerstoff. Derartige ozeanische anoxische Ereignisse wiederholten sich in der Erdgeschichte mehrfach.

Man weiß heute, dass sowohl Phasen starker Abkühlung, wie sie beispielsweise während der grande Coupure stattfand, als auch rapide Erwärmungen von Massenaussterben begleitet wurden. Der Paläontologe Peter Ward behauptet sogar, dass alle bekannten Massenaussterben der Erdgeschichte mit Ausnahme des KT-Impakt durch Klimakrisen ausgelöst wurden.

Das Klima der letzten 10.000 Jahre war im Vergleich zu den häufigen und starken Schwankungen der vorangegangenen Jahrhunderttausende ungewöhnlich stabil. Diese Stabilität gilt als Grundvoraussetzung für die Entwicklung und den Fortbestand der menschlichen Zivilisation.<ref name="DOI10.1126/science.1228026"></ref>

Zuletzt kam es während des Paläozän/Eozän-Temperaturmaximum und beim Eocene Thermal Maximum 2 zu einer schnellen und starken globalen Erwärmung, die von einem massiven Eintrag von Kohlenstoff (CO und/oder Methan) in die Atmosphäre verursacht wurde. Diese Epochen sind daher Gegenstand intensiver Forschungen, um daraus Erkenntnisse über mögliche Auswirkungen der laufenden menschengemachten Erwärmung zu gewinnen.

Der laufende und für die kommenden Jahre erwartete Klimawandel hat möglicherweise das Ausmaß großer Klimawandel der Erdgeschichte, die vorhergesagte kommende Temperaturänderung läuft aber mindestens um einen Faktor 20 schneller ab als in allen globalen Klimawandeln der letzten 65 Millionen Jahre. Betrachtet man die Geschwindigkeit der Erwärmungsphasen von Eiszeiten zu Zwischeneiszeiten, wie sie in den letzten ca. 500.000 Jahren fünf mal vorkamen, so kam es dort jeweils zu Phasen der schnellen Erwärmung. Diese Phasen dauerten jeweils ca. 10.000 Jahre an und waren durch einen Temperaturanstieg von insgesamt ca. 4 bis 5 °C gekennzeichnet. Die Temperatur stieg also um ca. 4 bis 5 Hundertstel Grad pro Jahrhundert an. Diese historischen Temperaturanstiege verliefen damit um ca. Faktor 100 langsamer als die derzeit beobachtete menschengemachte Erwärmung, bei der mit Temperaturanstiegen von 4 bis 5 °C binnen eines Jahrhunderts gerechnet wird, sofern keine erheblichen Klimaschutzmaßnahmen getroffen werden.

Anhand der bald zweihundert Jahre umfassenden Datenlage und Forschung kann man davon ausgehen, dass die Epoche des Pliozäns ein analoges Beispiel für die Zukunft unseres Planeten sein kann. Der Kohlenstoffdioxid-Gehalt der Atmosphäre im mittleren Pliozän wurde mit Hilfe der Isotopenuntersuchung von Δ13C ermittelt und lag damals im Bereich von 400 ppm, das entspricht der Konzentration des Jahres 2015. Mit Hilfe von Klimaproxies sind Temperatur und Meeresspiegel der Zeit vor 5 Millionen Jahren rekonstruierbar. Zum Beginn des Pliozäns lag die globale Durchschnittstemperatur um 2 K höher als im Holozän; die globale Jahresdurchschnittstemperatur reagiert aufgrund der enormen Wärmekapazität der Weltmeere sehr träge auf Änderungen des Strahlungsantriebs und so ist sie seit Beginn der industriellen Revolution erst um 0,9 K angestiegen.

Die Erwärmung führt unter anderem zu einem Meeresspiegelanstieg. Der Meeresspiegel lag in der Mitte des Pliozäns um rund 20 Meter höher als heute.

Der Intergovernmental Panel on Climate Change (IPCC) wurde 1988 vom Umweltprogramm der Vereinten Nationen (UNEP) gemeinsam mit der Weltorganisation für Meteorologie (WMO) eingerichtet und ist der 1992 abgeschlossenen Klimarahmenkonvention beigeordnet. Der IPCC fasst für seine im Abstand von etwa sechs Jahren erscheinenden Berichte die weltweiten Forschungsergebnisse auf dem Gebiet der Klimaveränderung zusammen und bildet damit den aktuellen Stand des Wissens in der Klimatologie ab.

Die Organisation wurde 2007, gemeinsam mit dem ehemaligen US-Vizepräsidenten Al Gore, mit dem Friedensnobelpreis ausgezeichnet. Der Fünfte Sachstandsbericht ist im September 2013 erschienen.

Seit der Entdeckung des Treibhauseffektes 1824 durch Jean Baptiste Joseph Fourier und 1862 von Wasserdampf und Kohlenstoffdioxid durch John Tyndall gilt die Erforschung zum Erd-Klimasystem als eine der best erforschten Wissenschaften in der Geschichte der Menschheit. Seit 150 Jahren ist die wärmende Wirkung von Treibhausgasen bekannt, deren Konzentrationsanstieg in der Erdatmosphäre dann Mitte der 50er Jahre des vorigen Jahrhunderts sicher nachgewiesen werden konnte. Die seit Mitte der 1970er Jahre festgestellte, ausgeprägte und bis heute ununterbrochene Klimaerwärmung kann mit Hilfe der seitdem deutlich verbesserten Messtechnik nicht primär auf solare Einflüsse oder andere natürliche Faktoren zurückgeführt werden, da sich diese seit dieser Zeit nur minimal veränderten. Grundlegende Forschungen zur Auswirkung der Treibhausgase stammen vom Ozeanographen Veerabhadran Ramanathan aus der Mitte der 1970er Jahre. Viele tausende Studien wurden seitdem veröffentlicht, von denen die große Mehrheit (etwa 97 %) den „wissenschaftlichen Konsens zum Klimawandel“ stützt. Projektionen und Berechnungen, die vor Jahrzehnten getätigt wurden, streuten noch recht groß, haben insgesamt den Trend aber überraschend gut getroffen. Werden die Modelle mit aktuelleren Messungen, vor allem der Strahlungsbilanz zwischen oberer Atmosphäre und dem Weltraum, dann sinkt die Streuung zwischen den Modellen und der Mittelwert für die Erwärmung zum Ende des Jahrhunderts steigt etwas.

Man unterscheidet in der Klimaforschung zwischen Trend und Zeitpunkt und berechnet dafür die Eintrittswahrscheinlichkeiten. Im Themenumfeld der globalen Erwärmung ist beispielsweise Folgendes nicht genau bekannt: Mehrere Ereignis-Zeitpunkte, darunter der Zeitpunkt, an dem die Arktis im 21. Jahrhundert im Sommer eisfrei sein wird; Der exakte Meeresspiegelanstieg bis zum Ende des 21. Jahrhunderts ist ebenfalls nicht bekannt. Unsicherheiten bestehen in der genauen Art, Form, Ort und der Verteilung von globalen Kipppunkten im Klimasystem und damit auch verbunden in der Kenntnis der genauen regionalen Auswirkungen der globalen Erwärmung. Die Mehrzahl der relevanten wissenschaftlichen Grundlagen gelten als sehr gut verstanden.

Unter Fachwissenschaftlern existiert ein Konsens bezüglich der menschengemachten globalen Erwärmung, der seit spätestens Anfang der 1990er Jahre besteht. Keine wissenschaftliche Einrichtung auf nationaler oder internationaler Ebene hat Abweichungen zu den Konsensergebnissen des Klimawandels feststellen können. Der in den IPCC-Berichten zum Ausdruck gebrachte wissenschaftliche Konsens wird von den nationalen und internationalen Wissenschaftsakademien und allen G8-Ländern ausdrücklich unterstützt.

Der wissenschaftliche Konsens zum Klimawandel besteht in der Feststellung, dass sich das Erd-Klimasystem erwärmt und weiter erwärmen wird. Dies wird anhand von Beobachtungen der steigenden Durchschnittstemperatur der Luft und Ozeane, großflächigem Abschmelzen von Schnee- und Eisflächen und dem Meeresspiegelanstieg ermittelt. Mit 90 % Sicherheit wird dies durch Treibhausgase, Rodungen und das Verbrennen von fossilem Treibstoff verursacht. Die American Association for the Advancement of Science – die weltweit größte wissenschaftliche Gesellschaft – stellt dar, dass sich 97 % aller Klimatologen darüber einig sind, dass ein vom Menschen verursachter Klimawandel stattfindet, und betont den zu vielen Aspekten der Klimatologie herrschenden Konsens. Spätestens seit der Jahrtausendwende wird der Wissensstand um die mit dem Klimawandel verbundenen Folgen als ausreichend sicher angesehen, um umfangreiche Klimaschutzmaßnahmen zu rechtfertigen.

Laut einer 2014 veröffentlichten Studie bestand unter der Annahme keines anthropogenen Treibhauseffekts nur eine Wahrscheinlichkeit von 0,001 % für das tatsächlich eingetretene Ereignis von mindestens 304 Monaten in Folge (von März bis zum Stand der Analyse Juni 2010) mit einem Monatsmittel der globalen Temperatur über dem Mittelwert für das 20. Jahrhundert.

Der Themenkomplex der globalen Erwärmung war seit jeher Gegenstand kontroverser Diskussionen mit wechselnden Schwerpunkten. Anfang des 20. Jahrhunderts überwog die Unsicherheit, ob die theoretisch vorhergesagte Erwärmung messtechnisch überhaupt nachweisbar sein würde. Als in den USA während der 1930er Jahre erstmals ein signifikanter Temperaturanstieg in einigen Regionen registriert wurde, galt dies zwar als ein starkes Indiz für eine zunehmende Erderwärmung, gleichzeitig wurde jedoch bezweifelt, ob dieser Prozess tatsächlich auf menschlichen Einflüssen beruhte. Diese Zweifel werden von manchen klimaskeptischen Gruppierungen bis heute geäußert, und gelegentlich wird sogar in den Medien eine globale Abkühlung für die kommenden Jahrzehnte vorausgesagt, was von Klimaforschern zurückgewiesen wird.

Deutlich ausgeprägt ist die Ablehnung des wissenschaftlichen Konsenses insbesondere in Staaten, in denen mit großem finanziellen Einsatz durch Unternehmen, v. a. aus der Branche der fossilen Energien, eine einflussreiche Kontrabewegung geschaffen wurde, deren Ziel es ist, die Existenz des wissenschaftlichen Konsenses durch bewusstes Säen von Zweifeln zu untergraben. Besonders erfolgreich waren diese Aktionen unter konservativen Bevölkerungsteilen in den USA. Eine wichtige Rolle bei der Verschleierung des Standes der Wissenschaft spielen konservative Denkfabriken, deren Rolle in der öffentlichen und politischen Klimadebatte mittlerweile immer stärker in den Fokus der Forschung rückt.

Zu den wichtigsten Kräften der organisierten Klimaleugnerbewegung, die die Existenz der menschengemachten Globalen Erwärmung durch gezielte Attacken auf die Klimaforschung abstreiten, zählen das Cato Institute, das Competitive Enterprise Institute, das George C. Marshall Institute sowie das Heartland Institute, allesamt konservativ ausgerichtete Think Tanks. Ihr Ziel war und ist es, mittels der Strategie Fear, Uncertainty and Doubt in der Bevölkerung Unsicherheit und Zweifel an der Existenz der Globalen Erwärmung zu schaffen, um anschließend zu argumentieren, dass es nicht genügend Belege dafür gebe, konkrete Klimaschutzmaßnahmen zu ergreifen. Insgesamt stehen der US-Klima-Contrarian-Bewegung rund 900 Millionen Dollar pro Jahr für Kampagnenzwecke zur Verfügung. Die überwältigende Mehrheit der Mittel stammt von politisch konservativen Organisationen, wobei die Finanzierung zunehmend über Donors-Trust-Organisationen verschleiert wird.

Wegen der Auswirkungen auf menschliche Sicherheit, Gesundheit, Wirtschaft und Umwelt ist die globale Erwärmung mit Risiken behaftet. Einige schon heute wahrnehmbare Veränderungen wie die verringerte Schneebedeckung, der steigende Meeresspiegel oder die Gletscherschmelze gelten neben den Temperaturmessungen auch als Belege für den Klimawandel. Konsequenzen der globalen Erwärmung wirken sowohl direkt auf den Menschen als auch auf Ökosysteme. Dazu verstärkt der Klimawandel viele andere gravierenden Probleme wie z.B. den Artenschwund oder die Bodendegradation, sodass die Bekämpfung des Klimawandels zugleich eine Schlüsselmaßnahme für das Lösen anderer dringender Probleme auf dem Weg hin zu einer nachhaltigen Lebensweise ist. Um die vielfältigen Auswirkungen der globalen Erwärmung quantitativ erfassen zu können, wurde der sogenannte Klimawandelindex geschaffen.

Wissenschaftler projizieren verschiedene direkte und indirekte Auswirkungen auf Hydrosphäre, Atmosphäre und Biosphäre. Im Bericht des Weltklimarats (IPCC) werden diesen Projektionen jeweils Wahrscheinlichkeiten zugeordnet. Zu den Folgen zählen Hitzewellen, besonders in den Tropen, ein Hunderte Millionen Menschen betreffender Anstieg des Meeresspiegels, und Missernten, welche die globale Ernährungssicherheit gefährden. Eine sich stark erwärmende Welt ist, so ein Weltbank-Bericht, mit erheblichen Beeinträchtigungen für den Menschen verbunden.

Man unterscheidet mindestens zwei Arten unerwarteter Effekte: Kombinierte Effekte, bei denen mehrere Extremereignisse zusammen wirken und ihre Wirkung gegenseitig verstärken (beispielsweise Dürren und Großbrände), und Kippelemente. Bedingt durch die vielfachen Rückkopplungen im Erdsystem reagiert dieses auf Einflüsse oftmals nichtlinear, das heißt, Veränderungen vollziehen sich in diesen Fällen nicht kontinuierlich, sondern sprunghaft. Es gibt eine Reihe von Kippelementen, die bei fortschreitender Erwärmung wahrscheinlich abrupt einen neuen Zustand einnehmen werden, der ab einem gewissen Punkt (Tipping Point) schwer oder gar nicht umkehrbar sein wird. Beispiele für Kippelemente sind das Abschmelzen der arktischen Eisdecke oder eine Verlangsamung der thermohalinen Zirkulation. Da die Möglichkeit, das Klima in Klimamodellen abzubilden, nie vollständig der Realität entsprechen wird, das Klimasystem aufgrund seiner chaotischen Natur grundsätzlich nicht im Detail vorhersagbar ist und sich die Welt zudem zunehmend außerhalb des Bereichs bewegt, für den verlässliche Klimadaten der Vergangenheit vorliegen, können weder Art, Ausmaß noch Zeitpunkt solcher Ereignisse vorhergesagt werden.

Obwohl die Erwärmung der Erde durch einen kontinuierlich stattfindenden Energiezustrom verursacht wird, zeigen sich die Folgen dieses Erwärmungsprozesses nicht in kontinuierlichen Effekten, sondern meist in abrupten Ereignissen: Wie in vielen anderen Bereichen, so führt auch der menschengemachte Klimawandel zu einer Reihe von plötzlich auftretenden Veränderungen, die oft unvorhergesehen eintreten. Beispiele sind: plötzliches Aussterben einer Art, die – womöglich durch andere Umweltfaktoren vorbelastet – durch ein klimatisches Extremereignis eliminiert wird. Ein anderes Beispiel ist die Wirkung steigender Meeresspiegel. Diese führen nicht unmittelbar zu Überschwemmungen, sondern erst wenn im Rahmen von z. B. Sturmfluten ein vormals ausreichender Damm überschwemmt wird. Und auch der Meeresspiegelanstieg kann sich durch nichtlineare Effekte in sehr kurzer Zeit rasch beschleunigen, wie dies in der Klimageschichte beispielsweise beim Schmelzwasserpuls 1A der Fall war.

Auch Untersuchungen von klimatischen Veränderungen in der Erdgeschichte zeigen, dass Klimawandel in der Vergangenheit nicht nur graduell und langsam abliefen, sondern bisweilen auch sehr rasch. Die Durchschnittstemperaturen veränderten sich bei diesen plötzlichen Klimaveränderungen mit einer Geschwindigkeit von zehn Jahren oder weniger. So war am Ende der jüngeren Dryas und während der Dansgaard-Oeschger-Ereignisse während der letzten Kaltzeit regional eine Erwärmung um 8 °C in etwa 10 Jahren zu beobachten. Nach heutigem Kenntnisstand erscheint es wahrscheinlich, dass diese schnellen Sprünge im Klimasystem auch künftig stattfinden werden, wenn bestimmte Kipppunkte überschritten werden. Das gegenwärtige Verständnis der zugrunde liegenden Prozesse reicht jedoch nicht aus, diese Ereignisse vorherzusagen. Sollte es in den kommenden Jahren oder Jahrzehnten dazu kommen, wird dies somit unerwartet und überraschend erfolgen.

Die Risiken für Ökosysteme auf einer sich erwärmenden Erde wachsen mit jedem Grad des Temperaturanstiegs. Die Risiken unterhalb einer Erwärmung von 1 K gegenüber dem vorindustriellen Wert sind vergleichsweise gering. Zwischen 1 und 2 K Erwärmung liegen auf regionaler Ebene mitunter substanzielle Risiken vor. Eine Erwärmung oberhalb von 2 K birgt erhöhte Risiken für das Aussterben zahlreicher Tier- und Pflanzenarten, deren Lebensräume nicht länger ihren Anforderungen entsprechen. Bei über 2 K Temperaturanstieg drohen der Kollaps von Ökosystemen und signifikante Auswirkungen auf Wasser sowie Nahrungsmittelvorräte durch Ernteausfall.




Das Weltwirtschaftsforum Davos stuft in seinem Bericht "Global Risks 2013" den Klimawandel als eines der wichtigsten globalen Risiken ein: Das Wechselspiel zwischen der Belastung der wirtschaftlichen und ökologischen Systeme werde unvorhersehbare Herausforderungen für globale und nationale Widerstandsfähigkeiten darstellen.

Verschiedene Militärstrategen und Sicherheitsexperten befürchten geopolitische Verwerfungen infolge von Klimaveränderungen, die sicherheitspolitische Risiken für die Stabilität der Weltordnung und den „Weltfrieden“ bergen, auch der UN-Sicherheitsrat gab 2011 auf Initiative Deutschlands eine entsprechende Erklärung ab. Der ehemalige deutsche Außenminister Frank-Walter Steinmeier bewertete im April 2015 nach Erscheinen einer zum „G7“-Außenminister-Treffen in Lübeck verfassten europäischen Studie den Klimawandel ebenfalls als „eine wachsende Herausforderung für Frieden und Stabilität“. Die Studie empfiehlt u. a. die Einrichtung einer G7-Taskforce.

Die wirtschaftlichen Folgen der globalen Klimaerwärmung sind nach gegenwärtigen Schätzungen beträchtlich: Das Deutsche Institut für Wirtschaftsforschung schätzte 2004/5, dass ohne zügig umgesetzten Klimaschutz der Klimawandel bis zum Jahr 2050 bis zu 200.000 Milliarden US-Dollar volkswirtschaftliche Kosten verursachen könnte (wobei diese Schätzung mit großen Unsicherheiten behaftet ist).<ref name="Kemfert/Praetorius 2005">Claudia Kemfert, Barbara Praetorius: "Die ökonomischen Kosten des Klimawandels und der Klimapolitik." In: "DIW, Vierteljahreshefte zur Wirtschaftsforschung." 74, 2/2005, S. 133–136 Online</ref> Der Stern-Report (er wurde Mitte 2005 von der damaligen britischen Regierung in Auftrag gegeben) schätzte die durch den Klimawandel bis zum Jahr 2100 zu erwartenden Schäden auf 5 bis 20 Prozent der globalen Wirtschaftsleistung.

Nach einem im Vorfeld der im November 2017 in Bonn stattfindenden 23. UN-Klimakonferenz („COP 23“) veröffentlichten "Lancet-Report" hat sich die Zahl der wetterbedingten Naturkatastrophen seit 2000 um 46 % erhöht; allein 2016 sei dadurch ein ökonomischer Schaden von 126 Mrd. Dollar entstanden.

"Siehe auch „Klimafinanzierung“, Klimaversicherung, Loss and Damage, Unternehmerische Klimarisiken"

In einem komplexen Themenkomplex wie der globalen Erwärmung wird es ein vollständiges Verständnis aller Teilaspekte voraussichtlich niemals geben, ebenso wenig wie es bei ähnlich komplexen Themen ein vollständiges Verständnis jedes Details gibt. Die Grundlagen der den anthropogenen Klimawandel auslösenden Mechanismen gelten jedoch als verstanden.

Die Entscheidung für oder gegen Klimaschutzmaßnahmen basiert nicht auf einem „Beweis“, dass der anthropogene Klimawandel gefährliche Ausmaße annehmen wird. Vielmehr liegt ihr eine Risikoabschätzung zugrunde. Der Umweltbiologe Stephen Schneider vergleicht die Probleme einer Beweisführung für die Schädlichkeit der globalen Erwärmung mit der, die sich bei gewohnheitsmäßigem Rauchen von Tabak ergibt. So sei es bis heute unbewiesen, dass Rauchen Krebs erzeugt, auch kenne man die zugrunde liegenden Zusammenhänge nicht in allen Details. Dennoch deuteten die statistischen Befunde, also die Epidemiologie, klar auf einen engen kausalen Zusammenhang zwischen Krebs und Rauchen hin. Schneider erwähnt des Weiteren den Fall eines Patienten, bei dem im Rahmen einer Routine-Röntgenaufnahme ein verdächtiger Schatten auf der Lunge entdeckt wurde. Um herauszufinden, ob dieser Schatten der Hinweis auf einen bösartigen Tumor ist, besteht die Möglichkeit einer schmerzhaften, risikobehafteten und teuren Gewebeprobennahme (Biopsie). Alternativ könne der Patient auch warten, ob der Schatten auf dem Bild mit der Zeit größer wird, was als Beleg dafür gilt, dass ein Tumor vorliegt. Dann besteht aber die Gefahr einer Metastasierung, die die Heilungschancen drastisch verringert. Die Entscheidung für oder gegen eine Biopsie basiere wie die Entscheidung für oder gegen Klimaschutzmaßnahmen auf einer Risikobewertung. Schneider betont, dass bei einer Entscheidungsfindung, die sich auf eine Risikobetrachtung stützt, ein Beweis nicht nötig sei.

Das Ausmaß der möglichen Konsequenzen der globalen Erwärmung führt zur Frage, wie diese politisch verhindert oder wie ihre Folgen zumindest gemildert werden können. Die Emissionsminderung aller Treibhausgase ist Hauptgegenstand der umfassenden Klimarahmenkonvention (UNFCCC) der Vereinten Nationen als der völkerrechtlich verbindlichen Regelung zum Klimaschutz. Sie wurde 1992 in New York City verabschiedet und im gleichen Jahr auf der UN-Konferenz für Umwelt und Entwicklung (UNCED) in Rio de Janeiro von den meisten Staaten unterschrieben. Mit der Rahmenkonvention geht als neu entstandenes Prinzip der Staatengemeinschaft einher, dass auf eine solche massive Bedrohung der globalen Umwelt auch ohne genaue Kenntnis des letztlichen tatsächlichen Ausmaßes reagiert werden soll. Auf der Rio-Konferenz wurde auch die Agenda 21 verabschiedet, die seitdem Grundlage für viele lokale Schutzmaßnahmen ist.

Die derzeit 195 Vertragsstaaten der Rahmenkonvention treffen sich jährlich zu UN-Klimakonferenzen. Die bekanntesten dieser Konferenzen waren 1997 im japanischen Kyōto, die als Ergebnis das Kyoto-Protokoll hervorbrachte, 2009 in Kopenhagen und 2015 in Paris. Dort wurde von allen 195 Vertragsstaaten vereinbart, die globale Erwärmung auf deutlich unter 2 °C gegenüber vorindustrieller Zeit zu begrenzen. Angestrebt werden soll eine Begrenzung auf 1,5 °C.

In der sozialwissenschaftlichen Literatur werden unterschiedliche politische Instrumente zur Senkung von Treibhausgasemissionen empfohlen und z. T. kontrovers diskutiert. Manche Wissenschaftler wie z. B. Joachim Weimann empfehlen einen globalen Emissionsrechtehandel als allein ausreichendes, da effizientestes Instrument. Andere Ökonomen wie z. B. der britische Energiewissenschaftler Dieter Helm erachten dagegen eine CO-Steuer für geeigneter, da stabiler als die schwankenden CO-Preise eines Emissionshandels, welche für Unternehmen zu schwierig kalkulierbar seien. Andere wiederum (z. B. der US-amerikanische Politökonom Scott Barrett) argumentieren, dass staatlich vorgeschriebene technische Standards (bestimmte CO-arme oder CO-freie Produktionstechnologien bzw. Konsumgüter wie z. B. Pkw) wie beim Montreal-Protokoll zum Schutz der Ozonschicht sich in der internationalen Politik weit besser politisch durchsetzen ließen als ein globaler Emissionsrechtehandel oder eine CO-Steuer. Der Sozialwissenschaftler Anthony Patt sieht einen Emissionshandel in der realen Politik ebenfalls als zu wenig wirkmächtig an, da der politische Widerstand gegen genügend (d. h. ausreichend für die Dekarbonisierung) stark steigende bzw. hohe CO-Preise v. a. seitens der energieintensiven Industrien zu groß sei. Die CO-Preise würden daher – wie beim EU-Emissionshandel – nur auf niedrigem Niveau schwanken, sodass sich (bei einem alleinigen Emissionshandel) für potentielle Öko-Investoren kapitalintensive, langfristig ausgerichtete Zukunftsinvestitionen in CO-freie Technologien nicht lohnen würden. Dafür bräuchten sie vielmehr die sichere Erwartung, dass die CO-Preise in Zukunft steigen und hoch bleiben, damit sie sich gegen Konkurrenten, die mit CO-intensiven Technologien wirtschaften, auf dem Wettbewerbsmarkt absehbar durchsetzen können. Das politische System kann sich jedoch nicht verlässlich auf einen künftig verlässlich steigenden, hohen CO-Preis verpflichten, da derartige politische Entscheidungen in einer Demokratie immer reversibel sind bzw. wären (so wurde z. B. in Australien eine CO-Steuer erst eingeführt und nach zwei Jahren von einer neuen, konservativen Regierung wieder abgeschafft). Dies wird auch als „Commitment Problem“ der Klimapolitik bezeichnet. 

Deshalb befürwortet Anthony Patt Gesetze zur Subventionierung CO-freier Technologien wie z. B. das Erneuerbare-Energien-Gesetz (EEG) in Deutschland, die genau diese benötigte Erwartungssicherheit für potentielle Investoren in CO-freie Technologien herstellen: Das EEG garantiert(e) (zumindest bis zur EEG-Novelle 2016) einem Produzenten von Strom aus erneuerbaren Energien für einen langen Zeitraum (20 Jahre) einen bestimmten Verkaufspreis, der über dem Marktniveau liegt. Diese Garantie unterliegt dem verfassungsrechtlichen Vertrauensschutz. Auf diese Weise abgesichert, gelang es den Investoren in erneuerbare Energien in den letzten beiden Jahrzehnten, durch den Ausbau die Kosten für die Erzeugung von Strom aus erneuerbaren Energien durch "Learning by doing" (Erfahrungskurve) sehr stark zu senken und allmählich gegenüber Strom aus fossilen Energiequellen und Atomstrom wettbewerbsfähig zu werden. Ähnliche Argumentationen, die die Notwendigkeit einer Flankierung des Emissionshandels durch Gesetze wie das EEG betonen, finden sich im Sondergutachten 2011 des Sachverständigenrates Umwelt (Seite 240 ff.) und dem Energieökonomen Erik Gawel. Befürworter des Emissionshandels halten dem entgegen, dass der Staat dadurch zu stark in den Markt eingreife und im Gegensatz zu diesem übermäßig teure Technologien für die Subventionierung auswählen würde, da er im Gegensatz zu den Marktakteuren nicht über das Wissen verfüge, welches die effizientesten Technologien seien. Dadurch würden volkswirtschaftliche Ressourcen verschwendet, sodass sich die Gesellschaft weniger Klimaschutz leisten könne als eigentlich (d. h. mit einem idealen Emissionshandel) möglich.

Als Grenze von tolerablem zu „gefährlichem“ Klimawandel wird in der Klimapolitik gemeinhin eine durchschnittliche Erwärmung um 2 K gegenüber dem vorindustriellen Niveau angenommen. Das 2-Kelvin-Ziel („2-Grad-Ziel“) basiert auf der Grafik "burning embers" im IPCC 2001, überarbeitet 2009. Da 0,7 K bereits erreicht sind, verbleiben damit noch 1,3 K. Das 2-Kelvin-Ziel wurde etwa beim G8-Gipfel im Juli 2009 anerkannt. Es ist auch Teil des Copenhagen Accord. Einzelne Staaten, besonders Mitglieder der Europäischen Union, hatten sich diesem Ziel bereits länger verschrieben. In Deutschland empfiehlt der Wissenschaftliche Beirat der Bundesregierung Globale Umweltveränderungen (WBGU) bereits seit 1994, die mittlere Erwärmung auf höchstens 2 K zu begrenzen. International wurde das 2-Grad-Ziel 2015 in völkerrechtlich bindender Form verabschiedet, das sog. Übereinkommen von Paris trat im November 2016 in Kraft.

Die bisherigen Pläne der einzelnen Staaten reichen nicht aus, um das Zwei-Grad-Ziel zu erreichen. Sollten die Staaten ihre gemachten Zusagen erfüllen, ergibt sich eine globale Erwärmung von 2,6 bis 3,1 °C bis 2100 sowie ein weiterer Temperaturanstieg nach 2100. Für die Einhaltung der Zwei-Grad-Grenze sind demnach eine nachträgliche Verschärfung der Zusagen oder eine Übererfüllung der Ziele zwingend notwendig. Für die Begrenzung der Erdtemperatur auf einem bestimmten Niveau müssen die Treibhausgasemissionen netto auf Null zurückgefahren werden, da für eine bestimmte Temperatur nur ein begrenztes Kohlenstoffbudget zur Verfügung steht, das emittiert werden kann. Im Umkehrschluss bedeutet, dass die Erderwärmung so lange weiter angeht, solange weitere Emissionen produziert werden, die die Gesamtmenge an Treibhausgasen in der Atmosphäre erhöhen. Emissionen nur zu reduzieren führt also nicht zu einem Stoppen der globalen Erwärmung, sondern nur zu einem langsameren Anstieg der Temperaturen.

Der Anstieg des Meeresspiegels wäre mit der 2-Kelvin-Begrenzung nicht gestoppt. Die teilweise deutlich stärkere Erwärmung über den Landflächen bringt weitere Probleme. Besonders stark zunehmende Temperaturen werden über der Arktis erwartet. Beispielsweise erklärten Indigene Völker das 2-Kelvin-Ziel für zu schwach, weil es ihre Kultur und ihre Lebensweise immer noch zerstören würde, sei es in arktischen Regionen, in kleinen Inselstaaten sowie in Wald- oder Trockengebieten.

Nach einer im Jahr 2012 veröffentlichten Studie im Auftrag der Weltbank wäre eine Erwärmung um vier Grad, wie derzeit befürchtet, mit verheerenden Folgen verbunden. In den Tropen könnten Ende des Jahrhunderts die kühlsten Monate deutlich wärmer sein als die heißesten Monate der Gegenwart. Der Meeresspiegel kann bei 4 Kelvin globaler Erwärmung in diesem Jahrhundert 50 bis 100 Zentimeter steigen, und danach noch deutlich höher. Dabei ist dieser Anstieg regional unterschiedlich stark, dies hängt von Meeresströmungen und anderen Faktoren ab. Am höchsten wird das Meer den Projektionen zufolge an den Küsten von Ländern wie den Philippinen, Mexiko, Indien steigen. In der Landwirtschaft könnte dies zu großflächigen Ernteausfällen führen. Veränderungen im Wasserkreislauf können hierbei erschwerend hinzukommen, etwa wenn Dürren vorherrschen oder landwirtschaftliche Flächen überflutet werden. Betroffen seien vor allem die Armen dieser Welt, für die Entwicklung ohne Klimaschutz nach Lage der Fakten kaum möglich sei.

Eine zentrale Empfehlung des IPCC ist ein globaler Kohleausstieg verbunden mit einem schnellen und grundlegenden Umbau der weltweiten Energieversorgung.

Politische Vorgaben zum Klimaschutz müssen durch entsprechende Maßnahmen umgesetzt werden. Auf der technischen Seite existiert eine Vielzahl von Optionen zur Verminderung von Treibhausgasemissionen. Nach einer 2004 erschienenen Studie ließ sich selbst mit den damals vorhandenen Mittel bereits ein effektiver Klimaschutz realisieren.<ref name="Pacala/Solow 2004">Stephen Pacala, Robert Socolow: "Stabilization Wedges: Solving the Climate Problem for the Next 50 Years with Current Technologies." In: "Science." 305, 14. August 2004, S. 968–972 (PDF; 181 kB)</ref> Vor allem die Kosten einer solchen Vermeidungsstrategie hemmen bislang die notwendigen Investitionen in Klimaschutztechnik, auch wenn wie oben beschrieben diese Kosten teilweise deutlich niedriger geschätzt werden, verglichen mit den ansonsten eintretenden Schäden durch den Klimawandel.

Der Umbau des Energiesystems von fossilen auf erneuerbare Energiequellen, die sog. Energiewende, wird als ein weiterer unverzichtbarer Bestandteil effektiver Klimaschutzpolitik angesehen. Die globalen Potenziale sind im IPCC-Bericht dargestellt. Im Gegensatz zu fossilen Energieträgern wird bei der Nutzung der erneuerbaren Energien mit Ausnahme der Bioenergie kein Kohlenstoffdioxid ausgestoßen, und auch diese ist weitgehend CO-neutral. Der Einsatz erneuerbarer Energien bietet sowohl ökologisch als auch ökonomisch großes Potenzial, vor allem durch das weitgehende Vermeiden der mit anderen Energieformen verbundenen Folgeschäden, die als sog. externe Kosten hohe volkswirtschaftliche Schäden verursachen.

Grundsätzlich lässt sich festhalten, dass erneuerbare Energien verglichen mit konventionellen Energienutzungsformen eine bessere Umweltbilanz aufweisen. Zwar liegt der Materialbedarf für diese Technologien höher als beim Bau von Wärmekraftwerken, die Umweltbelastung durch den höheren Materialbedarf ist jedoch gering verglichen mit den brennstoffbedingten direkten Emissionen von fossil befeuerten Kraftwerken. Durch Umstellung der Energieversorgung auf ein regeneratives Energiesystem lässt sich somit die durch den Energiesektor verursachte Umweltbelastung reduzieren. Ob die erhofften ökologischen Vorteile im Einzelfall realistisch sind, kann durch eine Ökobilanz festgestellt werden. So müssen bei der Biomasse-Nutzung zum Beispiel Landverbrauch, chemischer Pflanzenschutz und Reduzierung der Artenvielfalt der erwünschten CO-Reduzierung gegenübergestellt werden.

Eine verbesserte Energieeffizienz ist ein zentrales Element technischer Klimaschutzlösungen. Nimmt die Energieeffizienz zu, kann eine Dienstleistung oder ein Produkt mit weniger Energieverbrauch als zuvor angeboten oder hergestellt werden. Das heißt beispielsweise, dass in einer Wohnung weniger geheizt werden muss, ein Kühlschrank weniger Strom benötigt oder ein Auto einen geringeren Benzinverbrauch hat. In all diesen Fällen führt die zunehmende Effizienz zu einem abnehmenden Energieverbrauch und damit zu einem verringerten Treibhausgas-Ausstoß. McKinsey berechnete zudem, dass zahlreiche Energieeffizienz-Maßnahmen gleichzeitig einen volkswirtschaftlichen Gewinn abwerfen.

In einer globalen Bilanz betrachtet muss jedoch ebenfalls der Rebound-Effekt berücksichtigt werden, der dazu führt, dass eine gesteigerte Energie- bzw. Ressourceneffizienz durch eine Mehrproduktion an Produkten oder Dienstleistungen teilweise wieder ausgeglichen wird. Es wird davon ausgegangen, dass die Energieeinsparung durch Energieeffizienzmaßnahmen durch Rebound-Effekt im Schnitt um 10 % gemildert wird, wobei Werte einzelner Studien zwischen 0 und 30 % schwanken.

Für den Betrieb von "fossilen Kraftwerken" wird eine CO-Abscheidung und -Speicherung (CCS) angestrebt. Zumindest für Länder wie Deutschland mit seiner begrenzten geologischen Endlagerkapazität für CO dürfte es sich auch bei CCS nur um eine Übergangslösung für wenige Jahrzehnte handeln.

Pflanzenkohle (Biokohle, englisch "biochar") besteht zu überwiegendem Anteil aus reinem Kohlenstoff und kann mit pyrolytischer Verkohlung hergestellt werden. Biokohle eingebracht ins Erdreich kann dort über Jahrtausende überdauern.
Man geht davon aus, dass mit nachhaltiger Pflanzenkohleerzeugung, (CO)-, Methan (CH)- und Distickstoffmonoxid (NO)-Emissionen in einer Höhe von 1,8 Gigatonnen CO-Äquivalent (= COe), das heißt 12 % der jährlichen, anthropogenen Treibhausemissionen kompensiert werden können. Im Verlauf eines Jahrhunderts kann eine Menge Pflanzenkohle hergestellt werden, die Gesamtemissionen in Höhe von 130 Gigatonnen COe entsprechen, ohne dabei Lebensmittel- und Naturschutzsicherheit zu gefährden.<ref name="DOI10.1038/ncomms1053"></ref>

"Geoengineering" umfasst bisher nicht eingesetzte technische Eingriffe in die Umwelt, um die Erwärmung abzumildern, darunter die Eisendüngung im Meer, um das Algenwachstum anzuregen und auf diese Weise CO zu binden, und das Einbringen von Sulfataerosol in die Stratosphäre zur Reflexion von Sonnenstrahlen. Beide Maßnahmen gelten mittlerweile als unbrauchbar.

Neben politischen Weichenstellungen wie dem Einsatz für eine Energiewende und den Kohleausstieg gehören auch wirtschaftliche Maßnahmen zum Repertoire klimaschützenden Vorgehens, z. B. der Rückzug von Investoren wie Versicherungen, Kreditinstituten und Banken aus Geldanlagen in fossil geprägte Industriebereiche und Unternehmen („Desinvestition“) oder Investitionen umzuleiten in Wirtschaftssektoren wie Nachhaltigkeit und Erneuerbare Energien. So hat z. B. die Weltbank auf dem "One Planet Summit" Anfang Dezember 2017 in Paris angekündigt, ab 2019 keine Projekte zur Erschließung von Erdöl und Erdgas mehr zu finanzieren. Der Versicherungskonzern "Axa" teilte dort mit, in Zukunft keine Neubauten von Kohlekraftwerken mehr zu versichern und bis 2020 zwölf Mrd. Euro in „grüne“ Projekte investieren zu wollen. Umweltschutzorganisationen wie "Urgewald" legen hier den Schwerpunkt ihrer Aktivitäten.

Individuelle Möglichkeiten für Beiträge zum Klimaschutz bestehen in Verhaltensumstellungen und verändertem Konsum mit Energieeinsparungen.

Es gibt zahlreiche Maßnahmen zur CO-Reduktion. Hierzu gehören unter anderem:


Schätzungen des IPCC (2007) zufolge gehen 10 bis 12 Prozent der globalen Emissionen von Treibhausgasen auf die Landwirtschaft zurück. Nicht berücksichtigt wurden hier jedoch unter anderem die Folgen der Abholzung größerer Flächen (u. a. Regenwald) für landwirtschaftliche Zwecke. Eine Studie im Auftrag von Greenpeace geht daher von einem agrarischen Anteil von 17 bis 32 Prozent an den von Menschen verursachten Treibhausgasen aus. In Großbritannien stehen etwa 19 Prozent der Treibhausgasemissionen im Zusammenhang mit Nahrungsmitteln (Landwirtschaft, Verarbeitung, Transport, Einzelhandel, Konsum, Abfall). Etwa 50 Prozent davon gehen diesen Schätzungen zufolge auf Fleisch und Milchprodukte zurück. Das Food Climate Research Network empfiehlt daher unter anderem marktorientierte und regulative Maßnahmen zu nachhaltigerer Produktion bzw. nachhaltigerem Konsum von Lebensmitteln (z. B. CO-emissionsabhängige Preise/Steuern).

Würde der globale Fleischkonsum ab 2015 innerhalb von 40 Jahren auf weniger als ein Drittel reduziert, sänken einer Modellsimulation zufolge die Lachgas- und Methanemissionen der Landwirtschaft unter das Niveau von 1995.

Zur Reduzierung der nahrungsmittelbezogenen Emissionen verbreitet empfohlen wird der Konsum lokal produzierter Lebensmittel. Einer US-amerikanischen Ökobilanz von Weber und Matthews (2008) zufolge liegt der Beitrag des Transports zu den Emissionen der Lebensmittelversorgung in den USA bei 11 Prozent. Der Hauptanteil (83 Prozent) entstehe bei der Produktion, weswegen die Art der konsumierten Lebensmittel den größten Einfluss habe. Besonders kritisch bezüglich der Produktion von Treibhausgasen wird der Konsum von rotem Fleisch gesehen; stattdessen sollte eher auf Milchprodukte, Geflügel, Fisch, Eier oder Gemüse zurückgegriffen werden.

Parallel zu vorbeugendem Klimaschutz in Fom von Vermeidungsstrategien sind Anpassungen an bereits eingetretene bzw. künftig zu erwartende Auswirkungen des menschengemachten Klimawandels notwendig: Die mit der Erderwärmung verbundenen negativen Folgen sollen so weit möglich gemindert und möglichst verträglich gestaltet werden; gleichzeitig wird die Nutzung regional möglicherweise positiver Folgen geprüft. Die Anpassungsfähigkeit variiert in Abhängigkeit von verschiedensten Parametern, darunter bestehende Kenntnisse zu örtlichen Klimaveränderungen oder z. B. der Entwicklungsstand und die ökonomische Leistungsfähigkeit eines Landes oder einer Gesellschaft. Insgesamt wird speziell in sozio-ökonomischer Hinsicht die Fähigkeit zur Anpassung stark durch die Vulnerabilität geprägt. Der "Intergovernmental Panel on Climate Change" (IPCC) zählt die am wenigsten fortgeschrittenen „Entwicklungsländer“ zu den Ländern und Regionen mit besonders hoher Vulnerabilität. 

Die Anpassung an die Folgen der Erderwärmung hat vor allem kurz- bis mittelfristige Wirkung. Da die Anpassungsfähigkeit von Gesellschaften jedoch begrenzt ist und eine starke Erderwärmung bereits getätigte Anpassungsmaßnahmen wieder zunichte machen kann, kann Anpassung keine Alternative zum vorbeugenden Klimaschutz sein, sondern nur eine Ergänzung dazu.

Die Palette potenzieller Anpassungsmaßnahmen reicht von rein technologischen Maßnahmen (z. B. Küstenschutz) über Verhaltensänderungen (z. B. Ernährungsverhalten, Wahl der Urlaubsziele) und betriebswirtschaftlichen Entscheidungen (z. B. veränderte Landbewirtschaftung) bis zu politischen Entscheidungen (z. B. Planungsvorschriften, Emissionsminderungsziele). Angesichts der Tatsache, dass der Klimawandel sich auf viele Sektoren einer Volkswirtschaft auswirkt, ist die Integration von Anpassung z. B. in nationale Entwicklungspläne, Armutsbekämpfungsstrategien oder sektorale Planungsprozesse eine zentrale Herausforderung; viele Staaten haben daher Anpassungsstrategien entwickelt. 

In der im Jahr 1992 verabschiedeten Klimarahmenkonvention (UNFCCC), die mittlerweile von 192 Staaten ratifiziert worden ist, spielte das Thema Anpassung noch kaum eine Rolle gegenüber der Vermeidung eines gefährlichen Klimawandels (Artikel 2 der UNFCCC). Für das Kyoto-Protokoll, das 1997 vereinbart wurde und 2005 in Kraft trat, gilt das zwar ähnlich, doch wurde dort grundsätzlich der Beschluss zur Einrichtung eines speziellen UN-Anpassungsfonds („Adaptation Fund“) gefasst, um die besonders betroffenen Entwicklungsländer bei der Finanzierung von Anpassungsmaßnahmen zu unterstützen. Dazu soll auch der Green Climate Fund der Vereinten Nationen beitragen, der während der Klimakonferenz 2010 in Cancún eingerichtet wurde. Für den Fonds stellen Industrienationen Gelder bereit, damit sich Entwicklungsländer besser an den Klimawandel anpassen können.

Spätestens mit dem 3. Sachstandsbericht des IPCC, der 2001 veröffentlicht wurde, hat das Verständnis für die Notwendigkeit von Anpassungsstrategien zugenommen. Betreffs der wissenschaftlichen Unterstützung für Regierungen war insbesondere das im Jahr 2006 beschlossene Nairobi-Arbeitsprogramm zu Adaptation und Vulnerabilität ein wichtiger Schritt. Der Bali-Aktionsplan (Fahrplan von Bali) von 2007 behandelte Anpassung erstmals gleichgewichtig mit der Vermeidung von Emissionen und diente als Rahmen für die anschließenden Verhandlungen zu einem neuen, umfassenden internationalen Klimaabkommen.

Am 16. April 2013 wurde von der EU-Kommission eine EU-Strategie zur Anpassung an den Klimawandel vorgestellt. Bis zu diesem Datum hatten 15 EU-Mitgliedstaaten eine eigene Anpassungsstrategie erarbeitet.

Die globale Erwärmung ist zunehmend auch ein Thema in Kunst, Literatur und Film.

Dargestellt wird das Thema zum Beispiel in den Katastrophenfilmen ', ' oder "Welt in Angst".

Zudem gibt es verschiedene Dokumentarfilme zu dem Thema. "Eine unbequeme Wahrheit" gilt mit als Kernbotschaft von Nobelpreisträger Al Gore zum Klimawandel. Dokumentarischen Anspruch und teilweise polemische Inhalte hat der britische Film "". Auch der schwedische Dokumentarfilm "Unser Planet" befasst sich unter anderem mit dem Klimawandel und beinhaltet Interviews mit verschiedenen Klimaforschern. Der US-amerikanische Dokumentarfilm "Chasing Ice" hat den Gletscherschwund als Folge der globalen Erwärmung zum Inhalt und porträtiert das "Extreme-Ice-Survey"-Projekt des Naturfotografen James Balog.

Literarisch wird das Thema u. a. in den 2010 erschienenen Romanen des britischen Schriftstellers Ian McEwan "(Solar)" oder des Autorengespanns Ann-Monika Pleitgen und Ilja Bohnet "(Kein Durchkommen)" verarbeitet; zur Bewältigung des Klimawandels ist 2013 auch der Comic "Die Große Transformation. Klima – Kriegen wir die Kurve?" erschienen.
"Cape Farewell" ist ein internationales gemeinnütziges Projekt des britischen Künstlers David Buckland. Ziel ist die Zusammenarbeit von Künstlern, Wissenschaftlern und „Kommunikatoren“ (u. a. Medienvertretern) zum Thema Klimawandel. Im Rahmen des Projekts wurden verschiedene Expeditionen zur Arktis und in die Anden durchgeführt, die u. a. filmisch, fotografisch, literarisch und musikalisch verarbeitet wurden (u. a. in den Filmen "Art from the Arctic" und "Burning Ice").





</doc>
<doc id="1845" url="https://de.wikipedia.org/wiki?curid=1845" title="Gene Hackman">
Gene Hackman

Hackman wuchs bei seiner Großmutter in Danville, Illinois auf. Seine Eltern ließen sich scheiden, als er noch ein Kind war. Seine Schulbildung brach Hackman 1947 ab und trat – trotz seiner Minderjährigkeit – dem Militär bei. Beim Marine Corps diente er mehrere Jahre lang als Funker.

Nach seiner Entlassung studierte Hackman in New York Journalistik und arbeitete bei zahlreichen Rundfunkstationen im ganzen Land. Anfang der 1950er Jahre wollte er Schauspieler werden. In Los Angeles besuchte er die "Playhouse Acting School" und gab in "The Curious Miss Caraway" sein Bühnendebüt. Er brach die Ausbildung dort nach drei Monaten ab.

Zurück in New York teilte sich Hackman eine Ein-Zimmer-Wohnung mit seinen früheren Kommilitonen Dustin Hoffman und Robert Duvall, mit denen er auch später befreundet blieb. Alle drei Schauspieler lebten jahrelang in sehr bescheidenen Verhältnissen. Erst in den späten 1960er und frühen 1970er Jahren gelang den drei Schauspielern der Durchbruch in Hollywood.

Nach mehreren Auftritten am Broadway (u. a. in "Any Wednesday") und im Fernsehen "(The United States Steel Hour)" spielte Gene Hackman 1961 in seinem ersten Kinofilm, "Der Tollwütige", mit. 1964 lernte er bei den Dreharbeiten zu "Lilith" Warren Beatty kennen, der ihm drei Jahre später eine Rolle in dem Verbrecher-Drama "Bonnie und Clyde" verschaffte. Der große Erfolg des Films brachte Hackmans Karriere weiter voran. Für seine Darstellung des Gangsters Buck Barrow wurde er außerdem als Bester Nebendarsteller für einen Oscar nominiert. 1970 erhielt er für seine Rolle in dem Familiendrama "Kein Lied für meinen Vater" erneut eine Nominierung.

Gene Hackman war bereits über 40 Jahre alt, als ihm 1971 mit dem New-Hollywood-Klassiker "French Connection – Brennpunkt Brooklyn" der internationale Durchbruch gelang. Der mit einem geringen Budget von 1,8 Millionen Dollar und ohne Stars realisierte Film wurde zu einem großen Kassenerfolg und spielte mehr als 50 Millionen Dollar ein. Hackman (der die Rolle bekommen hatte, nachdem sie von zahlreichen anderen Schauspielern abgelehnt worden war) trat als fanatischer Drogenfahnder „Popeye“ Doyle in Erscheinung und wurde für seine Darstellung mit einem Oscar ausgezeichnet.

Hackman hatte sich in Hollywood nun endgültig als einer der führenden Charakterdarsteller etabliert und konnte diesen Status gut drei Jahrzehnte lang halten. Seine Gage stieg im Lauf der 1970er Jahre von 100.000 Dollar "(French Connection)" auf zwei Millionen "(Superman)".

Nach "French Connection" folgten Auftritte in "Die Höllenfahrt der Poseidon", "Asphalt-Blüten" mit Al Pacino und "Der Dialog" von Francis Ford Coppola. 1975 spielte er unter der Regie von Arthur Penn in einem der wichtigsten Neo-Noir-Filme, "Die heiße Spur".
1978, 1980 und 1987 spielte er in den "Superman"-Filmen mit Christopher Reeve den Bösewicht und Superman-Gegenspieler Lex Luthor. 1983 spielte er in Ted Kotcheff's Kriegsfilm "Die verwegenen Sieben" zusammen mit Fred Ward die Hauptrolle. Für seine Rolle als FBI-Agent Anderson in dem Südstaaten-Drama "Mississippi Burning – Die Wurzel des Hasses" wurde er 1988 abermals für einen Oscar als Bester Hauptdarsteller nominiert.

In den späten 1980er und frühen 1990er Jahren schien Hackman bei der Rollenauswahl weniger anspruchsvoll zu sein und spielte, offensichtlich aus rein finanziellen Erwägungen, in zahlreichen Filmen mit, die bei Kritik und Publikum nur wenig Resonanz fanden. Allein zwischen 1988 und 1991 war der Darsteller in elf Filmen zu sehen. Er lehnte das Angebot ab, als Regisseur und Darsteller (des Hannibal Lecter) den Psychothriller Das Schweigen der Lämmer zu realisieren, da ihm der düstere Stoff nicht zusagte.

Gene Hackman gelang es ab 1992, seine Karriere durch eine sorgfältige Rollenauswahl wieder zu stabilisieren. Er reduzierte sein Arbeitspensum und drehte in der Regel noch ein bis zwei Filme pro Jahr. Einige seiner größten kommerziellen Erfolge konnte er in den 1990er Jahren feiern. Unter anderem wirkte er in Clint Eastwoods Spätwestern "Erbarmungslos", wofür er einen Oscar als Bester Nebendarsteller erhielt. Darüber hinaus war er in der Grisham-Verfilmung "Die Firma", in dem Western "Schneller als der Tod" und als U-Boot-Kommandant in dem Meuterei-Film "Crimson Tide" zu sehen. In "Der Staatsfeind Nr. 1" nahm er das Rollenprofil eines Abhörspezialisten wieder auf, was als Hommage an die von ihm dargestellte Figur in dem Psychodrama "Der Dialog" gedacht war. In den Filmkomödien "Schnappt Shorty", "The Birdcage – Ein Paradies für schrille Vögel", "Die Royal Tenenbaums" und "" stellte er sein komisches Talent unter Beweis.

2003 stand er in dem Gerichtsdrama "Das Urteil", wieder nach einem Roman von John Grisham, das einzige Mal mit seinem Freund Dustin Hoffman vor der Kamera. Sein bisher letzter Film war die Komödie "Willkommen in Mooseport" (2004) an der Seite von Ray Romano. Bis 2004 wirkte er in über 80 Filmproduktionen mit.

Am 7. Juli 2004 gab Hackman in einem Interview mit dem Talkmaster Larry King bekannt, dass er keine weiteren Filmangebote habe und er davon ausgehe, dass seine Filmkarriere zu Ende sei. Tatsächlich ist Hackman seitdem in keinem Film mehr aufgetreten.

Gene Hackman betätigte sich neben seiner Schauspielkarriere auch als Schriftsteller. Er schrieb in Zusammenarbeit mit Daniel Lenihan u. a. die Romane "Wake of the Perdido Star", "Payback at Morning Peak" und (dt.) Jacks Rache.

Hackman ist seit 1991 mit Betsy Arakawa verheiratet und lebt in Santa Fe, New Mexico. Aus seiner ersten Ehe mit Faye Maltese hat er drei Kinder.

Seine Synchronsprecher wechselten im Laufe der Jahrzehnte. Am häufigsten wurde Hackman von Horst Niendorf synchronisiert, der ihn zwischen 1971 und 1990 neunzehnmal sprach. Seit 1993 lieh ihm Klaus Sonnenschein elfmal seine Stimme. Hackman wurde außerdem von Hartmut Neugebauer, Engelbert von Nordhausen, Rolf Schult, Michael Chevalier oder Wolfgang Kieling synchronisiert.

Gene Hackman war zwischen 1978 und 1984 auch einige Jahre als Amateur-Rennfahrer bei Sportwagenrennen aktiv. Er ging unter anderem beim 24-Stunden-Rennen von Daytona und dem 12-Stunden-Rennen von Sebring an den Start.









</doc>
<doc id="1846" url="https://de.wikipedia.org/wiki?curid=1846" title="Gwyneth Paltrow">
Gwyneth Paltrow

Gwyneth Katherine Paltrow (* 27. September 1972 in Los Angeles, Kalifornien) ist eine US-amerikanische Schauspielerin, Unternehmerin, Model und Sängerin. Sie ist Golden-Globe- sowie Oscar-Preisträgerin.

Paltrow wurde als Tochter des Regisseurs und Fernsehproduzenten Bruce Paltrow und der Schauspielerin Blythe Danner geboren. Ihr Vater war polnisch-jüdischer und weißrussisch-jüdischer Herkunft, während ihre Mutter eine Quäkerin der Pennsylvania Dutch mit entfernteren englischen Vorfahren aus Barbados ist. Paltrows Ururgroßvater väterlicherseits hieß ursprünglich Paltrowicz und war ein Rabbiner in Nowogród (Polen). Sie hat einen Bruder namens Jake Paltrow, der ebenfalls in der Filmbranche tätig ist, sowie eine Cousine namens Katherine Sian Moennig, ebenfalls Schauspielerin. Als Paltrow 15 Jahre alt war, verbrachte sie ein Jahr in Spanien. Seitdem spricht sie fließend Spanisch, am 12. April 2003 wurde sie zur Ehrenbürgerin ihrer Austauschstadt Talavera de la Reina ernannt. Sie absolvierte die Highschool und besuchte anschließend die University of California, Santa Barbara, wo sie Kunstgeschichte studierte. Sie brach das Studium ab und widmete sich der Schauspielerei; sie hatte ihr Talent schon früher bewiesen, als sie zusammen mit ihrer Mutter in Theaterstücken mitwirkte.

1990 gab Paltrow ihr professionelles Theaterdebüt. Ihre erste Filmrolle spielte sie in "Shout" (1991) an der Seite von John Travolta. Im selben Jahr spielte sie unter der Regie ihres Patenonkels Steven Spielberg die junge Wendy in dem Fantasyfilm "Hook". Kleinere Rollen hatte sie in den Thrillern "Flesh And Bone – Ein blutiges Erbe" (1993) und "Malice – Eine Intrige" (1993) mit Nicole Kidman.

Größere Bekanntheit erlangte sie 1995 an der Seite von Brad Pitt und Morgan Freeman mit einer Nebenrolle in dem Film "Sieben", der ein internationaler Kinoerfolg war und Paltrow eine Nominierung für den Satellite Award einbrachte. 1996 spielte sie die Titelrolle in der Verfilmung von Jane Austens "Emma", wofür sie positive Kritiken erhielt. 1997 war Paltrow im Gespräch für die Rolle der Rose in dem Drama "Titanic" von James Cameron, die dann aber mit Kate Winslet besetzt wurde.

1998 gelang Paltrow der internationale Durchbruch mit der Hauptrolle der Viola De Lesseps in der Liebeskomödie "Shakespeare in Love", in der sie die fiktive Freundin von William Shakespeare (Joseph Fiennes) spielte. Der Film war bei Kritikern wie beim Kinopublikum ein Erfolg, und Paltrow wurde für ihre Darbietung mit zahlreichen Filmpreisen ausgezeichnet. Sie erhielt unter anderem den Oscar als "Beste Hauptdarstellerin", einen Golden Globe als "Beste Hauptdarstellerin – Musical oder Komödie" und den Screen Actors Guild Award für die "Beste weibliche Hauptrolle". 1999 war sie an der Seite von Jude Law, Matt Damon und Cate Blanchett in dem Drama "Der talentierte Mr. Ripley" zu sehen. 2000 spielte sie neben Ben Affleck in dem Liebesfilm "Bounce – Eine Chance für die Liebe". 2001 war sie als Margot Tenenbaum in dem Ensemblefilm "Die Royal Tenenbaums" zu sehen und in der Hauptrolle der Komödie "Schwer verliebt" an der Seite von Jack Black, wofür sie teilweise einen Fettanzug tragen musste. Sie spielte auch die Hauptrolle in der erfolglosen Komödie "Flight Girls" (2003), für die sie 10 Millionen US-Dollar Gage erhielt und war neben Angelina Jolie in "Sky Captain and the World of Tomorrow" (2004) zu sehen.

2006 erhielt Paltrow für das Drama "Der Beweis – Liebe zwischen Genie und Wahnsinn" (2005) erneut eine Nominierung für den Golden Globe. 2007 spielte sie die Hauptrolle in "The Good Night" unter der Regie ihres Bruders Jake. Das US-amerikanische Forbes Magazine zählte sie 2008 zu den bestbezahlten Schauspielerinnen Hollywoods. Zwischen Juni 2007 und Juni 2008 erhielt sie Gagen in Höhe von 25 Mio. US-Dollar, womit sie zusammen mit Reese Witherspoon hinter Cameron Diaz, Keira Knightley und Jennifer Aniston auf Platz vier rangierte.

2008 gründete Paltrow die Webseite "Goop" auf Basis eines Newsletters mit persönlichen Lebensstil-Tipps von ihr und einem angeschlossenen Onlineshop mit entsprechenden Produkten dazu. Im selben Jahr trat sie als Pepper Potts in dem Actionfilm "Iron Man" mit Robert Downey Jr. auf. Der Film spielte weltweit über 500 Millionen US-Dollar ein und Paltrow wurde für einen Teen Choice Award in der Kategorie "Choice Movie Actress" nominiert. Wegen des großen Erfolges zog der Film die Fortsetzung "Iron Man 2" nach sich, in dem sie ihre Rolle wiederholte und erneut für einen Teen-Choice-Award und einen Scream-Award als "Beste Action-Darstellerin" nominiert war. Am 13. Dezember 2010 wurde Paltrow mit einem Stern (Nr. 2427) auf dem Walk of Fame in Hollywood geehrt.

Bis Juli 2017 spielte sie in drei weiteren Produktionen aus dem Marvel Cinematic Universe die Rolle der Pepper Potts. Daneben spielte sie seit "Iron Man 2" in fünf anderen Kinofilmen mit, außerdem war sie in den Serien "Glee" (2010, 2011 und 2014) und "Web Therapy" (2014) zu sehen.

Im März 2011 erreichte sie mit dem Lied "Do You Wanna Touch Me? (Oh Yeah!)", einer Coverversion des gleichnamigen Songs von Gary Glitter (1973), Platz 1 der australischen Charts. Im selben Jahr erhielt sie für ihren Gastauftritt als Holly Holliday in der US-amerikanischen Fernsehserie "Glee" einen Emmy.

Paltrows deutsche Standard-Synchronsprecherin ist Katrin Fröhlich.

Nach Beziehungen mit Brad Pitt (von 1995 bis 1997 verlobt) und Ben Affleck (von 1998 bis 2000 liiert) heiratete Paltrow am 5. Dezember 2003 in Kalifornien Chris Martin, den Sänger der britischen Band Coldplay. Am 14. Mai 2004 wurde die gemeinsame Tochter geboren. Am 9. April 2006 brachte Paltrow in New York ihr zweites Kind zur Welt. Paltrow und ihr Mann waren früher Vegetarier. In ihrem Goop-Newsletter vom 4. Februar 2010 schrieb sie jedoch unter der Überschrift „Meatless Monday“: „I am not a Vegetarian“. Diese Aussage wurde 2011 in einem mit ihr gedrehten Werbefilm der Supermarktkette Spar verwendet.

Am 25. März 2014 gab Paltrow auf ihrer Internetseite mit ihrem Ehemann Chris Martin zusammen die Trennung bekannt. Seit Juli 2016 sind Paltrow und Martin offiziell geschieden.

Im Jahr 2014 konvertierte sie zum Judentum.

Im Januar 2018 gab sie ihre Verlobung mit dem TV-Produzenten Brad Falchuk bekannt.

Weitere Songs:


Oscar

Bambi

British Academy Film Award

Blockbuster Entertainment Award

Emmy

Golden Globe

Goldene Himbeere

Goldene Kamera

Independent Spirit Award

MTV Movie Award

Satellite Award

Screen Actors Guild Award

Teen Choice Award




</doc>
<doc id="1848" url="https://de.wikipedia.org/wiki?curid=1848" title="George W. Bush">
George W. Bush

George Walker Bush , meist abgekürzt George W. Bush [] (* 6. Juli 1946 in New Haven, Connecticut), ist ein US-amerikanischer Politiker der Republikanischen Partei und war von 2001 bis 2009 der 43. Präsident der Vereinigten Staaten.

Der Sohn einer einflussreichen Familie und des 41. US-Präsidenten George H. W. Bush bekleidete nach Unternehmertätigkeit in der Ölindustrie von 1995 bis 2000 das Amt des Gouverneurs von Texas. Bei der US-Präsidentschaftswahl 2000 gewann er gegen den Demokraten und damals amtierenden Vizepräsidenten Al Gore und wurde 2004 wiedergewählt.

Als Reaktion auf die Terroranschläge am 11. September 2001 erklärte Bush 2001 den Afghanistankrieg und 2003 den völkerrechtswidrigen Irakkrieg. Im Rahmen eines umfassend konzipierten „Krieges gegen den Terror“ ließ er zusätzlich – unter weltweiter Kritik – Bürgerrechte im USA PATRIOT Act einschränken und rechtsstaatliche Grundsätze in Ermittlungsverfahren aussetzen (siehe Guantanamo Bay). Im Sinne einer neokonservativen Außenpolitik identifizierte Bush eine „Achse des Bösen“ von „Schurkenstaaten“, gegen die die Vereinigten Staaten als hegemoniale Weltmacht das westliche, wirtschaftlich und politisch liberale Modell ausbreiten sollten bis hin zu einem militärischen Interventionsrecht (Bush-Doktrin). Bush steigerte die Militärausgaben und das Staatsdefizit erheblich; seine innenpolitische Ausgangsidee des „mitfühlenden Konservatismus“ sorgte für Initiativen wie die No-Child-Left-Behind-Politik, während er mit Steuersenkungen und Deregulierungsmaßnahmen eine angebotsorientierte Wirtschaftspolitik verfolgte. Bushs Ansehen sank nach hohen Kriegsverlusten, dem Hurrikan Katrina und der Finanzkrise ab 2007.

Bush ist Angehöriger einer wohlhabenden und einflussreichen Familie. Laut dem Genealogen Gary Boyd Roberts führt eine Abstammungslinie von George W. Bush zu den Pilgervätern der Mayflower. Einer seiner Vorfahren heiratete eine Urenkelin von Pocahontas. Zudem bestehen verwandtschaftliche Beziehungen zu 16 früheren US-Präsidenten. Sein Großvater war der Unternehmer und Senator Prescott Bush. Der als Sohn von Barbara Bush (1925–2018) und George H. W. Bush geborene George W. wuchs zusammen mit seinen vier jüngeren Geschwistern Jeb, Neil, Marvin und Dorothy in Midland und Houston auf. Eine weitere jüngere Schwester, Robin, verstarb 1953 dreijährig an Leukämie. Sein Bruder Jeb war von 1999 bis 2007 Gouverneur von Florida und 2016 einer der schließlich unterlegenen Präsidentschaftskandidaten in der parteiinternen Vorwahl der Republikaner. George W. Bushs von Freunden wie Gegnern zuweilen verwendeter Spitzname "Dubya" ist abgeleitet von der in den Südstaaten üblichen verkürzten Aussprache des Buchstabens W (eigentlich "Double U"). Bushs Vorname und sein Mittelname wurden angelehnt an die entsprechenden Namen seines Vaters (George Herbert Walker Bush) und seines Urgroßvaters (George Herbert Walker).
Bush besuchte von 1961 bis 1964 die Phillips Academy, die auch sein Vater besucht hatte. Anschließend studierte er von September 1964 bis Mai 1968 Geschichte an der Yale University, wo er, ebenfalls wie sein Vater, Mitglied der Vereinigung Skull & Bones und des Studentenbundes Delta Kappa Epsilon war. Im Oktober 1965 wurde er dessen Präsident, wie zuvor ebenfalls sein Vater. 1968 schloss er sein Studium an der Yale-Universität als Bachelor in Geschichte ab. Von 1972 bis 1975 besuchte George W. Bush die Harvard Business School der Harvard University, an der er den Master of Business Administration erwarb. 1977 heiratete er Laura Welch. Ihre Zwillingstöchter Jenna und Barbara wurden am 25. November 1981 geboren.

Im Jahr 1976 wurde Bush wegen Trunkenheit am Steuer in Maine befristet der Führerschein entzogen; wegen einer Alkholabhängigkeit musste er sich einem strengen Entzug unterziehen. Zehn Jahre später konvertierte er von den Anglikanern zu den Methodisten, um fortan als wiedergeborener Christ völlig auf Alkohol zu verzichten.

Bush ist seit 1999 Eigentümer der "Prairie Chapel Ranch" in der Nähe von Crawford im McLennan County in Texas, auf der er seine Ferien verbrachte und auch Staatsgäste empfing. Er hält sich außerdem oft im Sommersitz seines Vaters "Walker’s Point" bei Kennebunkport in Maine auf, wo ebenfalls Staatsgäste empfangen wurden.

Bush verpflichtete sich 1968 für sechs Jahre bei der Nationalgarde. Bei der Air National Guard in Texas wurde er Leutnant, Pilot und Führer einer Staffel F-102 "Delta Daggers". Da die Nationalgarde zu dieser Zeit vorwiegend im Inland eingesetzt wurde, sah sich Bush später mit dem Vorwurf konfrontiert, sich damit einem Einsatz im Vietnamkrieg entzogen zu haben und wurde daher "draft dodger" gescholten („ein sich der Einberufung Entziehender“), bei patriotischen Amerikanern alles andere als ein Ehrentitel. Dies war jedoch damals nicht nur unter Politikersöhnen eine durchaus verbreitete Praxis (sein Vater war damals Kongressabgeordneter im Repräsentantenhaus). Der Verdacht, Bush habe außerdem seine sich aus dem Dienst in der Nationalgarde ergebenden Pflichten nicht gewissenhaft erfüllt, sprach deshalb starke politische Empfindlichkeiten an. Auf Druck der Öffentlichkeit ließ er daher im Februar 2004 die Akten über diese Zeit für Untersuchungen freigeben.

Seine Unternehmertätigkeit begann Bush im Jahr 1978 in der Erdölförderindustrie mit der Gründung von "Arbusto Energy" (span. für "Busch"), später in "Bush Exploration" umbenannt. Als Anfang der 1980er-Jahre die Ölpreise einbrachen, geriet das Unternehmen in Schwierigkeiten und musste im Jahr 1984 mit dem Ölunternehmen "Spectrum 7 Energy Corp." fusionieren. Bush wurde Leiter des Unternehmens. Als im Jahr 1986 die Ölpreise erneut einbrachen, wurde es zahlungsunfähig und von "Harken Energy Corp." aufgekauft. Bush wurde dadurch einer der Direktoren bei Harken. 1988 erwarb er 5 % des Baseballteam Texas Rangers. Er war bis zu seiner Wahl zum Gouverneur von Texas 1994 der "Managing Partner" des Teams und veräußerte seinen Anteil 1998 für 15 Millionen US-Dollar.

1978 kandidierte Bush für das Repräsentantenhaus. Er verlor die Wahl mit 46,8 % gegen den Demokraten Kent Hance. 1988 war Bush Mitglied im Wahlkampfteam seines Vaters bei dessen schließlich erfolgreicher Präsidentschaftswahl.

Die texanische Gouverneurswahl am 8. November 1994 konnte Bush mit 53,5 % gegen die demokratische Amtsinhaberin Ann Richards, die 45,9 % erhielt, für sich entscheiden. Sein Wahlkampf wurde wie alle nachfolgenden von Karl Rove geplant. Um seine Position auch unter den demokratischen Wählern zu festigen, setzte Bush in seiner ersten Amtszeit auf weitgehende Kooperation mit dem politischen Gegner; so ernannte er einen Demokraten zu seinem Stellvertreter und stärkte diesem den Rücken, indem er sich beispielsweise für Gesetzesvorhaben der gegnerischen Fraktion im texanischen Parlament einsetzte.

1998 wurde er als texanischer Gouverneur mit 68,2 % wiedergewählt, sein Gegenkandidat Garry Mauro kam auf 31,2 %.

Als Gouverneur war Bush ein entschiedener Befürworter der Todesstrafe: 154 zum Tode Verurteilte wurden während seiner sechsjährigen Amtszeit hingerichtet. Dies entsprach 64 % der Exekutionen in Texas von 1976 bis 2000. Dieser Umstand führte bei den Gegnern der Todesstrafe auch international wiederholt zu scharfer Kritik. Die Zahl der Hinrichtungen stieg ab 1997; die meisten Hinrichtungen gab es 2000, während er für das Präsidentenamt kandidierte.

Im Jahr 2000 wurde Bush zum Präsidentschaftskandidaten der Republikaner nominiert und trat unter dem Motto des „mitfühlenden Konservatismus“ ("compassionate conservatism") mit Dick Cheney gegen den Kandidaten der Demokraten und damaligen Vizepräsidenten Al Gore sowie gegen den von den Grünen nominierten (international als Verbraucherschutzanwalt bekannten) Ralph Nader an. Während des Wahlkampfes identifizierte er sich mit den Werten der als "Reagan Revolution" bezeichneten konservativen Wende in Amerika, die vor allem mit dem Ausgang der Präsidentschaftswahl in den Vereinigten Staaten 1980 verbunden ist. Außenpolitisch kritisierte er Clinton und Gore unter anderem für die Interventionen im Balkan, die nicht im nationalen Interesse seien. Andererseits trat er für härtere Positionen gegenüber Irak und Nordkorea sowie eine Erhöhung des Haushaltes des Pentagon ein. Die Wahl wurde ein hartes Kopf-an-Kopf-Rennen zwischen Bush und Gore, so dass der Ausgang schließlich nur noch vom Ergebnis im Bundesstaat Florida abhing. Die dortige Auszählung, ein Vorsprung von 537 Stimmen, war jedoch stark umstritten. Die von Al Gore verlangte, von Floridas Staatsgericht eingeleitete Neuauszählung wurde durch ein Urteil des Supreme Court mit der Mehrheit von fünf zu vier Richterstimmen für verfassungswidrig erklärt, weil unordentlich und in den verschiedenen Distrikten des Bundesstaates uneinheitlich organisiert gezählt worden sei. Eine verfassungsgemäße Neuauszählung sei nicht mehr innerhalb der vorgesehenen Frist zu gewährleisten, deshalb sei jede Neuauszählung zu stoppen.

Damit wurde die erste Stimmenzählung, bei der Bush knapp geführt hatte, automatisch bestätigt. Bush erhielt zwar etwa 500.000 Wählerstimmen weniger als Gore, konnte aber mit den Wahlmännern Floridas insgesamt mehr Wahlmännerstimmen auf sich vereinigen.

Am 20. Januar 2001 wurde Bush vereidigt. Er war nach John Quincy Adams der zweite US-Präsident, dessen Vater ebenfalls US-Präsident gewesen war, und der erste Präsident mit einem MBA-Abschluss. Mit Bush zogen viele Republikaner wieder ins Weiße Haus ein, die schon unter seinem Vater wichtige Ämter innehatten, darunter Dick Cheney (damals Verteidigungsminister) als Vizepräsident. Außenminister wurde der frühere Golfkriegsgeneral Colin Powell, Donald Rumsfeld – der bereits von 1975 bis 1977 als solcher amtiert hatte – Verteidigungsminister. Die wichtigsten Berater waren Karl Rove und Sicherheitsberaterin Condoleezza Rice. Viele der neuen Mitarbeiter der Regierung George W. Bush hatten vorher der neokonservativen Denkfabrik "Project for the New American Century" angehört, etwa Richard Perle, Richard Armitage, Paul Wolfowitz und Lewis Libby.

Wie schon als Gouverneur kündigte Bush anfangs ein möglichst einvernehmliches, zumindest abgestimmtes Handeln mit dem politischen Gegner an. Als Schwerpunkte benannte er unter anderem Rechenschaftspflichten politischer Akteure gegenüber dem Volk, Stärkung des Militärs und Schaffung von Möglichkeiten für Arbeiter, Teile der Sozialversicherungsbeiträge privat zu investieren. Wichtiger Partner bei der Arbeit mit dem Senat wurde für Bush der Demokrat Edward Kennedy.

Im Sommer 2001 verloren die Republikaner durch den Wechsel eines republikanischen Senators zu den Demokraten ihre bisherige Mehrheit im Senat. Dies sahen viele Republikaner als Vertrauensbruch, was die parteiübergreifende Zusammenarbeit erschwerte.

Wichtigste Gesetzesprojekte vor dem 11. September 2001 waren ein Programm zur massiven Senkung der Steuern und die Reform des Bildungswesens. Beide nahm der US-Kongress an.
Im März 2001 wurde der endgültige Ausstieg der USA aus dem Kyoto-Abkommen zur Reduzierung der Treibhausgase verkündet. Dieser Schritt stieß im In- und Ausland auf scharfe Kritik. In der Kontroverse um Ölbohrungen im Arctic National Wildlife Refuge drängte die Regierung Bush auf eine Aufhebung des dort bestehenden Förderverbotes für Erdöl. In der Stammzellenforschung nahm Bush im August 2001 restriktive Positionen ein, auch um den rechten Flügel der Partei zu beruhigen, während er zur Mitte hin mit demokratischen Senatoren den No Child Left Behind Act ausarbeitete. Außenpolitisch baute er zwar nach dem Absturz eines US-Spionageflugzeugs mitsamt der Piloten beim Zwischenfall bei Hainan am 1. April 2001 eine Drohkulisse gegenüber China auf, verfolgte eine militärische Konfrontation aber nicht weiter.

Die Terroranschläge am 11. September 2001 veränderten die Politik Bushs tiefgreifend. Im Vorfeld hatten US-Nachrichtendienste ihn mehrmals vor Anschlägen der Terrororganisation al-Qaida in den USA gewarnt, zuletzt durch ein Memorandum Richard Clarkes vom 6. August 2001.

Am 20. September 2001 machte Bush den Gründer und Anführer al-Qaidas Osama bin Laden für die Anschläge verantwortlich und forderte vom Regime der Taliban seine Auslieferung aus Afghanistan binnen 14 Tagen. Dann rief er einen "Krieg gegen den Terrorismus" aus. Mit breiter internationaler Unterstützung, darunter der deutschen Bundesregierung und der Organisation für Islamische Zusammenarbeit, führten die USA und Großbritannien im Rahmen der Operation Enduring Freedom zunächst ab dem 7. Oktober Krieg in Afghanistan, wo al-Qaida ihre Basis hatte. Als Legitimation diente die Resolution 1368 des UN-Sicherheitsrates, welche am 12. September verabschiedet worden war. Ein weiteres Ziel neben der Terrorismusbekämpfung war es, das islamistische Regime der Taliban zu stürzen. Die Zahl der getöteten Zivilisten in diesem Krieg schätzte die Frankfurter Konferenz der europäischen Exil-Afghanen im Dezember 2001 auf etwa 18.000.
Am 15. Juni 2002 hielt Bush eine Rede zum Nahostkonflikt, die neben der Forderung nach einer neuen palästinensischen Führung auch den Grundstein für den späteren gemeinsamen Friedensplan der Vereinigten Staaten, Russlands, der Europäischen Union und der Vereinten Nationen, die sogenannte "Roadmap", enthielt. Dies war sein erstes wahrnehmbares Engagement in dem Konflikt.

Im Juli 2002 entschied sich Bush, eine vom US-Kongress bewilligte Finanzhilfe in Höhe von 34 Mio. USD für den Bevölkerungsfonds der Vereinten Nationen (UNFPA) zurückzuhalten, da dieser seiner Meinung nach in der Volksrepublik China Zwangsabtreibungen und ‑sterilisationen finanziere.

Im August 2002 versuchte Bush, den neu errichteten Internationalen Strafgerichtshof zu schwächen. Bilaterale Abkommen mit anderen Staaten sollten Auslieferungen von US-Bürgern nach Den Haag, dem Sitz des Gerichtshofs, verhindern. Der "American Servicemembers’ Protection Act" erlaubte es dem US-Präsidenten stattdessen, deren gewaltsame Befreiung anzuordnen.

Einen Monat später wurde die neue Nationale Sicherheitsstrategie veröffentlicht, die auch als Bush-Doktrin bekannt wurde. Sie ließ explizit Präventivschläge bei Bedrohung der USA durch Massenvernichtungswaffen zu.

Auf Initiative von Bush wurde das AIDS Relief Program (PEPFAR) gestartet. Die US-Regierung gab seit 2003 44 Milliarden US-Dollar für den Kampf gegen AIDS, Tuberkulose und Malaria in Afrika aus mit dem Ziel, fünf Millionen Menschen zu retten. Das unter Bush gestartete Malaria-Programm hat laut New York Times mittlerweile 1,7 Millionen Babies und Kleinkindern das Leben gerettet.

In Folge des 11. September 2001 drängten vor allem Dick Cheney und Paul Wolfowitz den Präsidenten mit später als fehlerhaft erkannten Geheimdienstberichten zu biologischen und Chemiewaffen im Irak, Saddam Hussein endgültig auszuschalten. Mit seiner State of the Union Address am 29. Januar 2002 beginnend sprach Bush bis zum März des Folgejahres 164 Mal öffentlich zum Irak und setzte ihn auf eine Achse des Bösen mit Iran und Nordkorea. In diesen Reden warf er Saddam Hussein stets vor, zu versuchen, Massenvernichtungswaffen zu erlangen, Terrorismus aktiv zu unterstützen, die eigene Bevölkerung zu unterdrücken, auch mit Giftgasangriffen, und die Region insgesamt zu destabilisieren. Neben diesen Gründen für eine Invasion des Iraks führte er an, dass eine Demokratisierung des Irak sich positiv auf den ganzen Nahen Osten inklusive Israel und Palästina auswirke. Am 11. Oktober 2002 erreichte er eine breite Zustimmung im Kongress zu einer Invasion des Irak, wobei auch die Demokraten mehrheitlich dafür votierten. Der UN-Sicherheitsrat verabschiedete am 8. November 2002 einstimmig auf amerikanische Initiative die Resolution 1441, welche dem Irak ein letztes Ultimatum stellte, Waffeninspekteuren der Internationalen Atomenergieorganisation unbeschränkten Zugang zu allen Anlagen zu geben. Der Vorwurf wurde schon damals skeptisch aufgenommen und ließ sich später nicht beweisen. Sein stetig wachsender Druck auf den Irak gipfelte schließlich im März 2003 im Irakkrieg mit der Invasion des Iraks durch britische und amerikanische Truppenverbände. Die Truppen des Irak wurden innerhalb weniger Wochen besiegt und das Land besetzt mit dem erklärten Ziel, dort Voraussetzungen für eine demokratische Regierung zu schaffen.

Da wegen der starken Opposition im UN-Sicherheitsrat das eigentlich gewünschte ausdrückliche UNO-Mandat nicht zu bekommen war, stützte er sich am Ende lediglich auf eine so genannte "Koalition der Willigen" aus Großbritannien, Spanien, Italien, Polen, Australien und etwa 30 weiteren Staaten. Die Gegner des Irakkrieges, darunter die Regierungen Frankreichs, Russlands, Deutschlands und Österreichs, sahen in einer Fortführung der Waffeninspektionen durch die Internationale Atomenergieorganisation (IAEO) ein angemesseneres, vorläufig ausreichendes Mittel für eine Abrüstung des Irak.

Weltweit beteiligten sich im Februar und März 2003 Millionen von Menschen an Antikriegsdemonstrationen der Friedensbewegung, auch in solchen Ländern, deren Regierungen sich hinter Bush gestellt hatten. Unter anderem wurde ihm vorgeworfen, der eigentliche Kriegsgrund seien der wirtschaftspolitische Zugriff auf die irakischen Erdölquellen und geostrategische Interessen der USA.
Während der Krieg in Afghanistan im Allgemeinen durch das Recht auf Selbstverteidigung als gedeckt gilt, war die völkerrechtliche Legitimation des Irakkrieges von Anfang an stark umstritten. Im Irak wurden keine Massenvernichtungswaffen gefunden. Auch die Zahl der Terroranschläge mit islamistischem Hintergrund und deren Opferzahlen nahmen nicht ab. Am 1. Mai 2003 verkündete Bush das Kriegsende ("mission accomplished!"). Doch der Irak blieb ein Unruheherd, in dem sich Anschläge gegen die Besatzungstruppen, gegen andere Ausländer und gegen mit ihnen zusammenarbeitende, aber auch völlig unbeteiligte irakische und arabische Zivilisten häufen.

Im Mai 2004 drangen zunehmend Informationen über Praktiken teils systematischer Folter und Misshandlung irakischer Gefangener durch Angehörige amerikanischen Militärs im Bagdader Abu-Ghuraib-Gefängnis an die Öffentlichkeit. Die Folterungen wurden durch Fotos und Videos belegt. Bei den Misshandlungen hatte es auch schon mehrere Todesopfer gegeben. Bald wurden ähnliche Vorfälle auch in anderen von Amerikanern geführten Militärgefängnissen im Irak und in Afghanistan bekannt. Schon zuvor war die Bush-Regierung wegen der Behandlung der Gefangenen im Gefangenenlager Guantanamo in Kuba in die Kritik geraten. Mit dem Bekanntwerden der Vorfälle in Abu Ghuraib weitete sich die Angelegenheit zu einem Skandal aus, der die moralische Glaubwürdigkeit der Bush-Regierung untergrub.

Im Dezember 2008 besuchte Bush den Irak und wurde vom Journalisten Muntazer al-Zaidi mit zwei Schuhen beworfen und verbal beleidigt.

Der "No Child Left Behind Act" („Kein Kind soll zurückbleiben“), der die Qualität des öffentlichen Schulwesens, jedoch auch den Zugriff des Militärs auf Personalien der Schüler zum Zweck einer Rekrutierung verbessern sollte, trat im Januar 2002 in Kraft.

Innenpolitisch konnte sich Bush infolge des allgemeinen Schocks durch die Anschläge zunächst auf eine breite Unterstützung im Kampf gegen den Terrorismus verlassen: Maßnahmen wie strengere Sicherheitskontrollen und Einreisebedingungen und Einschränkungen der Bürgerrechte durch den "Patriot Act I" konnten zügig und praktisch ohne öffentliche Auseinandersetzung verabschiedet werden. George W. Bush konnte in Umfragen die höchsten Zustimmungswerte verzeichnen, die jemals in den Vereinigten Staaten gemessen wurden. Erst später wurde Kritik hauptsächlich an den Plänen zu einem "Patriot Act II" laut. Das Informationsfreiheitsgesetz wurde stark beschnitten; immer mehr Regierungsakten werden mit dem Vermerk „Geheim“ oder „Nur für den Dienstgebrauch“ versehen und damit der Transparenz entzogen. Unter Verweis auf die Terrorismusbekämpfung wurden diverse Institutionen wie der Zoll (CBP und ICE), die Küstenwache und die Katastrophenschutzbehörde Federal Emergency Management Agency in einem neuen "Ministerium für Innere Sicherheit" mit zusammen 180.000 Mitarbeitern zusammengefasst.

Ein US-Bundesgericht in Detroit erklärte am 17. August 2006 das umstrittene Abhörprogramm der Regierung für verfassungswidrig. Bush hatte das Abhörprogramm nach den Anschlägen vom 11. September 2001 unter Ausschluss der Öffentlichkeit zur Terrorbekämpfung genehmigt. Es erlaubt den Behörden, internationale Telefongespräche amerikanischer Bürger mitzuhören und auch E-Mails abzufangen, ohne dafür eine richterliche Genehmigung beantragen zu müssen. Das geheime Programm war 2005 aufgedeckt und daraufhin heftig kritisiert worden.

Im März 2002 entschied Präsident Bush, Einfuhrzölle auf Stahlprodukte zu erheben, um amerikanische Firmen vor ausländischer Konkurrenz zu schützen. In der Folge verhängte die Europäische Union, gebilligt von der WTO, Strafzölle gegen die Vereinigten Staaten. Im November 2002 gewann die republikanische Partei die Senatswahl und konnte, entgegen dem normalen Trend, dass die Regierungspartei in den Halbzeitwahlen Stimmen verliert, ihre Mehrheit im Kongress ausbauen.

Eine Gesetzesinitiative des Präsidenten, die durch Steuererleichterungen die Konjunktur ankurbeln sollte, trat im Mai 2003 in Kraft. Im November 2003 gelang es Präsident Bush, eine Mehrheit für eine umfangreiche Gesundheitsreform mit Einführung staatlicher Zuschüsse für Medikamente im Rahmen der Medicare-Versicherung zu gewinnen.

Im Januar 2004 kündigte Präsident Bush an, der Weltraumbehörde NASA Mittel für eine bemannte Station auf dem Mond zur Verfügung zu stellen, die als Bahnhof und Testlabor für spätere Flüge zum Mars dienen soll.

Bush gewann die Präsidentschaftswahl im November 2004 knapp gegen seinen Herausforderer von den Demokraten, John Kerry; der Bundesstaat Ohio erwies sich als wahlentscheidend. Bush erhielt – aufgrund der für amerikanische Verhältnisse hohen Wahlbeteiligung – in absoluten Zahlen mehr Stimmen als jeder andere Präsident zuvor. Zum ersten Mal seit 1988 erhielt der Wahlsieger nicht nur die absolute Mehrheit der Wahlmännerstimmen, sondern auch die der abgegebenen Wählerstimmen. Im Kabinett wurde etwa die Hälfte der Minister ausgetauscht. Außenminister Colin Powell, der schon im November 2004 seinen Rücktritt erklärt hatte, wurde durch Condoleezza Rice ersetzt. In der Ansprache seiner zweiten Amtseinführung im Januar 2005 erklärte Bush, dass es nun die Politik der Vereinigten Staaten sei, Tyranneien auf der Welt zu beenden; das Überleben der amerikanischen Freiheit hänge von der Freiheit jedes anderen Landes ab.

Anfang 2005 setzte Bush den stellvertretenden Verteidigungsminister Paul Wolfowitz als Präsidenten der Weltbank durch. Am 1. August 2005 bestimmte er per Dekret den UNO-Kritiker John R. Bolton zum neuen UNO-Botschafter der Vereinigten Staaten. (Die Besetzung dieses Postens ist eigentlich zustimmungspflichtig durch den Senat, in der Sommerpause kann der Präsident dies umgehen.) Bolton war von der demokratischen Opposition sowie von Teilen der Republikaner abgelehnt worden.

Am 8. August 2005 setzte Bush mit seiner Unterschrift den "Energy Policy Act" in Kraft, der steuerliche Anreize für fossile Energieträger bietet, um deren Produktion zu fördern. Des Weiteren wurde, um auch die Zustimmung der Demokraten aus dem Mittleren Westen in Senat und Repräsentantenhaus zu erhalten, die Produktion von Biokraftstoff mit nachwachsenden Rohstoffen wie Mais gesetzlich gefördert. Insgesamt wurde der "Energy Policy Act" trotz moderater Umweltschutzmaßnahmen vor allem als ein Gunstbeweis an die Energieindustrie gesehen.

Am 29. August 2005 traf der Hurrikan Katrina auf die amerikanische Südküste und löste eine der verheerendsten Naturkatastrophen in der Geschichte der Vereinigten Staaten aus. Die Stadt New Orleans, die nur mangelhaft auf den Hurrikan vorbereitet war, wurde überflutet. Etwa 1800 Menschen kamen ums Leben, der Sturm richtete Sachschäden im Wert von 81 Milliarden Dollar an. Bush erfuhr heftige Kritik an seinem Vorgehen vor und nach der Katastrophe durch die amerikanischen Medien und viele Betroffene. Insbesondere wurde die Notfallplanung kritisiert, da es keine nationalen Evakuierungspläne gab und Bush eine Kongressstudie für den Schutz von Städten vor Hurrikanen der Stärke 5 auf Eis gelegt hatte. Die Haushaltsmittel für Katastrophenschutz und Innere Sicherheit waren stattdessen teilweise für den Irakkrieg genutzt worden. Ebenso wurde ihm vorgeworfen, rechtzeitige Mahnungen zur Verstärkung der Deiche missachtet zu haben: Bush hatte den Chef der für den Deichbau zuständigen Behörde 2002 entlassen, nachdem dieser dem Kongress Pläne für ein 188 Millionen Dollar teures Flutungsprojekt am unteren Mississippi vorgelegt hatte. In Interviews Anfang September behauptete Bush hingegen: „Niemand konnte den Bruch der Deiche voraussehen.“

Bush hat 2006 eine Gruppe von zehn Inseln und acht Atollen im Nordwesten von Hawaii zum größten Meeresschutzgebiet der Welt erklärt. Die Inselgruppe sei Heimat eines sehr seltenen und faszinierenden Meereslebens. Die unbewohnten Inseln erstrecken sich über eine Länge von 2250 Kilometern. Die Fläche entspricht ungefähr der von Deutschland. Zum Meeresschutzgebiet gehört auch das mit rund 11 700 Quadratkilometern größte, abgeschieden liegende Korallenriff-System der Welt. Außerdem legen in dem Gebiet 90 Prozent der gefährdeten Grünen Meeresschildkröten von Hawaii ihre Eier ab. Das Reservat löst das australische Great Barrier Reef als größtes maritimes Schutzgebiet der Welt ab.

Die Regierung Bush trieb die Unterdrückung von Umweltberichten voran. Auf kritische Berichte über den Klimawandel wurde Einfluss genommen, etwa indem Wissenschaftler zu einer Vermeidung der kritischen Passagen aufgefordert wurden. Auch Kontakte zu Medien wurden reglementiert. Darüber beklagt hatte sich die Hälfte von 300 zu diesem Thema befragten Wissenschaftlern. Im Juni 2006 bezeichnete Bush den Klimawandel jedoch als „ernsthaftes Problem“.

Zu Bushs innenpolitischen Erfolgen in der zweiten Amtszeit gehört die Neubesetzung zweier Sitze im Supreme Court. Am 5. September 2005 nominierte er John G. Roberts junior zum Nachfolger als Chief Justice für den verstorbenen William H. Rehnquist. Nach dessen Bestätigung durch den Senat nominierte Bush am 3. Oktober 2005 seine Rechtsberaterin Harriet Miers als Nachfolgerin der zurückgetretenen Richterin Sandra Day O’Connor, zog dies aber nach starker Kritik aus allen politischen Lagern am 27. Oktober 2005 auf Miers’ Bitte hin zurück und nominierte stattdessen Samuel Alito, der am 31. Januar 2006 mit knapper Mehrheit vom Senat bestätigt wurde.
Ebenfalls im Oktober spitzte sich die sogenannte Plame-Affäre zu, bei der hochrangige Mitglieder der Bush-Regierung beschuldigt wurden, die Identität der CIA-Agentin und Frau des Bush-kritischen Diplomaten Joseph Wilson, Valery Plame, an die Presse weitergegeben zu haben. Lewis Libby, Stabschef von Vizepräsident Dick Cheney, wurde unter anderem wegen Meineids und Behinderung der Justiz verhaftet. Weitere Beschuldigte waren Dick Cheney und Karl Rove, einer der wichtigsten Berater Bushs. Im April 2006 wurden dessen Aufgabenbereich als Berater verändert und ihm die Planung des Wahlkampfes der Kongresswahlen im November übertragen.

In der ab 2006 geführten Debatte um das Aufenthaltsrecht illegal eingewanderter Hispanics nahm Bush eine eher liberale Haltung ein und befürwortete erleichterte Aufenthaltsbedingungen für bereits im Inland Befindliche, da die Vereinigten Staaten ein traditionelles Einwanderungsland seien. Seine liberale Haltung brachten Medien oft mit seiner Herkunft aus Texas, dem am stärksten mexikanisch geprägten Bundesstaat, in Zusammenhang. Bei Neokonservativen stieß diese Haltung auf bisweilen harsche Kritik. Gleichwohl unterzeichnete Bush Ende Oktober 2006 ein Gesetz zum Ausbau der Grenze zu Mexiko, was die mexikanische Regierung massiv kritisierte. Bei den Halbzeitwahlen 2006 verloren die Republikaner die Mehrheit in beiden Kongresskammern (Repräsentantenhaus und Senat). Das bestimmende Thema der Wahlen war der Irakkrieg. Nach der Wahl gab Bush den schon länger geplanten Rücktritt des Verteidigungsministers Rumsfeld bekannt und ernannte Robert Gates zu dessen Nachfolger.

Im Sommer 2007 zogen sich mehrere hochrangige Mitarbeiter der Bush-Regierung zurück, Pressesprecher Tony Snow aus persönlichen Gründen, Karl Rove und der Justizminister Alberto R. Gonzales nach einer Affäre um die Entlassung zahlreicher Bundesanwälte, möglicherweise aus politischen Gründen. Im März 2008 legte Bush sein Veto gegen ein Gesetz ein, welches unter anderem die Anwendung der als Waterboarding bekannten Foltermethode durch die CIA verhindern sollte. Bush erklärte, dass die auf solche Weise erzielten Erfolge diese Art von Folter rechtfertigten. Gegen die Anwendung der Methode hatten sich zuvor auch ranghohe Militärs der Vereinigten Staaten gewandt. Am 16. April 2008 feierte der Präsident zusammen mit Papst Benedikt XVI., der im Rahmen einer apostolischen Reise die Vereinigten Staaten besuchte, dessen 81. Geburtstag zusammen mit 9000 Gästen im Weißen Haus.

Im Namen des Ministeriums für Innere Sicherheit unterzeichnete George W. Bush im Mai 2007 die "National Security Presidential Directive 51" (NSPD 51), auch als "Homeland Security Presidential Directive 20" (HSPD 20) bekannt, die im Falle einer nationalen Katastrophe oder Notfalls die Fortdauer der konstitutionellen Regierungsarbeit („Enduring Constitutional Government“) sicherstellen soll, indem der Präsident die Kooperation zwischen der Exekutive, der Legislative, und der Judikative koordiniert.

Am 28. Juli 2008 stimmte Bush als erster Präsident nach 51 Jahren der Todesstrafe für einen verurteilten amerikanischen Militärangehörigen zu.

Die letzten Monate von Bushs Amtszeit waren durch die internationale Finanzkrise geprägt, als viele Banken des Landes in Schieflage gerieten und einige Insolvenz anmelden mussten. Durch Bushs geschwächte Position im Kongress musste er bei dem ersten Rettungspaket nicht nur um die Zustimmung der Demokraten, sondern auch bei seinen eigenen Parteifreunden um Billigung werben, die sich zunehmend wegen seiner Unpopularität von ihm distanzierten. Am 29. September 2008 scheiterte ein von Bush eingebrachtes Paket im Kongress, nicht zuletzt weil ihm auch einige Parteifreunde die Gefolgschaft versagten.

Ein bestimmendes Thema der zweiten Amtszeit Bushs blieb der Irak, in dem 130.000 amerikanische und 20.000 britische Soldaten stationiert waren. Mit der einstimmig vom UN-Sicherheitsrat verabschiedeten Resolution 1483 vom 22. Mai 2003 wurden die USA und Großbritannien zu Besatzungsmächten erklärt. In dieser Resolution wurden die Bestrebungen begrüßt, eine irakische Übergangsverfassung zu verabschieden, deren Ziel es war, einen demokratischen Bundesstaat zu bilden, in dem Kurden, Sunniten und Schiiten gemeinsam leben könnten. Zugleich wurde darin gefordert, dass das frühere irakische Regime für die von ihm begangenen Verbrechen und Gräueltaten zur Rechenschaft gezogen werden muss. Anfang Januar 2005 wählten die Iraker eine Übergangsregierung, im Oktober stimmten sie über eine neue Verfassung ab.

Seit dem erklärten Ende der Kampfhandlungen kam es im Irak zu ständigen terroristischen Angriffen und ab 2006 zu bürgerkriegsähnlichen Zuständen mit Kämpfen zwischen Schiiten und Sunniten. Seit Kriegsende sind zehntausende Iraker und über 1000 amerikanische Soldaten gewaltsam ums Leben gekommen. Bushs Politik änderte sich dadurch jedoch nicht. Die Opposition der Demokraten konnte sich nicht auf einen gemeinsamen Alternativvorschlag einigen, allerdings wuchs in Anbetracht der wachsenden Ausgaben für den Krieg, der getöteten Amerikaner und der dadurch erzwungenen militärischen Handlungsunfähigkeit in anderen Konflikten wie mit Iran oder Nordkorea, die Kritik.

Im März 2006 setzte der Kongress die Baker-Kommission ein, eine zehnköpfige Gruppe um den ehemaligen Außenminister James Baker, um eine neue Strategie für den Irak zu erarbeiten. Diese legte im Dezember ihre Vorschläge vor, u. a. einen Abzug aller Kampftruppen aus dem Irak bis 2008. Die Demokraten forderten, nachdem sie die Halbzeitwahlen gewonnen hatten, eine Änderung der Strategie mit Abzug der amerikanischen Truppen. Am 11. Januar 2007 stellte Bush dann seine neue Irak-Strategie vor, den sogenannten „Surge“ („Erhebung“). Er ließ die Vorschläge der Kommission und die Forderungen der Opposition unberücksichtigt und 21.000 weitere Soldaten entsenden, um die Situation zu befrieden. Ein Abzug sollte sich anschließen, sobald die irakische Regierung alleine in der Lage sei, für Stabilität zu sorgen.

Das Verhältnis der meisten europäischen Regierungen zur Regierung Bush war, teils infolge des neokonservativen Paradigmas seiner Politik und seines Werdegangs, belastet. Neben der spanischen unter Zapatero und der italienischen unter Romano Prodi galt dies ebenso für die französische Regierung unter Chirac. So zogen die Wahlsieger Zapatero und Prodi die Irak-Kontingente ihrer Länder in Ablehnung des Irakkrieges unmittelbar nach ihrem jeweiligen Regierungswechsel aus dem arabischen Land ab. Auch das Verhältnis Bushs zum deutschen Bundeskanzler Gerhard Schröder war wegen dessen ablehnender Haltung zum Irak-Engagement beeinträchtigt. Schröders Nachfolgerin Angela Merkel hat sich um ein besseres Verhältnis bemüht; gleichwohl stand Mitte 2006 die Mehrheit der deutschen Bevölkerung der Person Bush und dessen Regierung unverändert negativ gegenüber. Im Vergleich zu Bush genießt der vorherige Präsident Clinton nach wie vor wesentlich höhere Sympathiewerte in Europa.

Die Bush-Administration gab im Oktober 2002 bekannt, dass Nordkorea entgegen dem Rahmenabkommen sein Urananreicherungsprogramm wieder aufgenommen habe. Im November beschloss sie in der KEDO (Korean Peninsula Energy Development Organization), die Schweröllieferung an die DVRK auszusetzen. Diese Maßnahme war eine offizielle Aussetzung des Rahmenabkommens durch die US-amerikanische Seite. Als Reaktion Nordkoreas wurde der Forschungsreaktor in Yongbjon wieder mit Brennstäben beladen, die von der IAEA (International Atomic Energy Agency) installierten Videokameras wurden entfernt und die beiden IAEA-Inspekteure im Dezember des Landes verwiesen. Danach trat Nordkorea aus dem NPT (Non-Proliferation Treaty) aus. 
Im April 2008 einigten sich die USA und Nordkorea bei einem bilateralen Treffen in Singapur auf eine konkrete Vereinbarung. Pjöngjang sollte eine Auflistung seines Atomprogramms abliefern und seine Nuklearanlagen schließen. Im Gegenzug sollte Washington die DVRK aus dem Feindstaatenhandelsgesetz und von der Liste der terrorismusfördernden Staaten streichen. Ende Juni sprengte Nordkorea einen Kühlturm in Yongbjon und übergab die vereinbarte Auflistung an China und die USA. US-Präsident Bush kündigte daraufhin eine Aufhebung der Handelssanktionen an und stellte eine Streichung der DVRK von der Terrorliste in Aussicht. 
Die Streichung Nordkoreas von der Terrorliste verzögerte sich, weil die Bush-Regierung sie an einen überprüfbaren Abbau des Atomprogramms knüpfte. Ende August teilte Nordkorea daraufhin mit, die Abbrucharbeiten am Nuklearreaktor vorläufig einzustellen und die schon abgeschaltete Atomanlage wieder in Betrieb zu nehmen. Im Oktober besuchte der US-Chefunterhändler Christopher Hill Nordkorea, um Pjöngjang zur Weiterführung des
Abbauprozesses zu bewegen. Das nordkoreanische Regime wollte jedoch nur angekündigte Überprüfungen seiner Atomanlagen akzeptieren, während Washington eine unangemeldete Überprüfung aller Anlagen verlangte. Kurz darauf wurde Nordkorea von der seit 20 Jahren bestehenden Terrorliste gestrichen.

Die Politik der amerikanischen Regierung unter George W. Bushs Präsidentschaft führte zu einer starken Polarisierung zwischen Befürwortern und Kritikern. Persönlichkeit und Eigenschaften Bushs wurden von Anhängern und Gegnern meist sehr gegensätzlich eingeschätzt. Während die einen seinen Glauben an Gott und die Führungsrolle der Vereinigten Staaten als Stärken schätzten, wiesen die anderen auf seine denkwürdige Biographie, unter anderem mit Hinweis auf seine früheren Alkoholprobleme, seine selbsterklärte „Wiedergeburt“ als Christ und Anti-Alkoholiker und gewisse sprachliche Unsicherheiten hin (siehe die Bushisms).

Während ein Großteil der Mainstream-Medien nach den Attentaten vom 11. September 2001 die Regierungspolitik unterstützte, riefen die umstrittene Wahl im Jahr 2000 und die Verschärfung der unilateralistischen Politik der Vereinigten Staaten nach diesen Terroranschlägen zahlreiche Kritiker auf den Plan. Der Dokumentarfilmer Michael Moore wurde mit seinen Büchern "Stupid White Men" und "Volle Deckung, Mr. Bush" sowie mit seinem Film "Fahrenheit 9/11" als Bush-Kritiker bekannt. Der damalige argentinische Präsident Néstor Kirchner sagte, Bush habe in einem Gespräch mit ihm über weltwirtschaftliche Probleme auf seinen Vorschlag der Auflage eines neuen Marshallplans ärgerlich reagiert und geantwortet, das beste Mittel, die Wirtschaft wiederzubeleben, sei der Krieg, und dass die Vereinigten Staaten durch Krieg stärker geworden seien. Bush wurde vom früheren deutschen Regierungssprecher Uwe-Karsten Heye als „intellektuell äußerst niederschwellig“ eingeschätzt, wie dieser 2010 in einem Interview erklärte.

George W. Bush war der erste amerikanische Präsident, dessen Handeln die Produktion und Veröffentlichung eines großen Kinofilms ("Fahrenheit 9/11"), den Start eines gesamten Rundfunknetzes ("Air America Radio") und die Premiere einer Fernsehserie ("The Al Franken Show") bewirkte, deren gemeinsames Ziel es war, seine Wiederwahl zu verhindern. Für "Fahrenheit 9/11" erhielt Bush als erster Präsident den Filmpreis Goldene Himbeere als schlechtester Hauptdarsteller; da er in dem Film nur in Archivaufnahmen gezeigt wurde und nicht auftrat, ist anzunehmen, dass die Verleihung als politische Stellungnahme der Jury zu betrachten ist. Auch weitere Auszeichnungen sind als Kritik zu verstehen; 2005 wurde der Schwammkugelkäfer "Agathidium bushi", der sich von Schleimpilzen ernährt, nach Bush benannt.

2001 verlieh die "United States Sports Academy" in Daphne Bush die Ehrendoktorwürde. Im Dezember 2004 wählte ihn das Time Magazine zur "Person des Jahres" 2004 „for sharpening the debate until the choices bled, for reframing reality to match his design, for gambling his fortunes – and ours – on his faith in the power of leadership“ (frei übersetzt: Für das Zuspitzen der Debatte bis zum Ausbluten von Alternativen, für das Umformen der Wirklichkeit zur Übereinstimmung mit seiner Vorstellung, für das Aufsspielsetzen seines – und unseres – Geschicks aufgrund seines Glaubens in die Kraft der Führerschaft).

Staatsbesuche Bushs in anderen Ländern führten immer wieder zu Demonstrationen gegen seine Außenpolitik, insbesondere den Irakkrieg und das Internierungslager Guantánamo. So kam es Anfang Juni 2004 bei einer Europareise Bushs anlässlich des 60. Jahrestags der Invasion der Alliierten in der Normandie beim Staatsbesuch in Italien zu massiven Straßenprotesten in Rom gegen Bush. Dort wurde der Präsident auch von Papst Johannes Paul II. zu einer Audienz empfangen. Der Papst, ein entschiedener Gegner des Irakkrieges, kritisierte Bushs Irak-Politik deutlich und forderte ihn zu einer Änderung auf.

Nach der Amtseinführung von Barack Obama am 20. Januar 2009 kündigte Bush an, sich mit seiner Frau Laura in Preston Hollow in der Nähe von Dallas niederzulassen. Er verfasste seine Memoiren unter dem Titel "Decision Points", die am 9. November 2010 auf Englisch erschienen und am gleichen Tag 220.000 mal verkauft wurden.

Am 4. Februar 2011 reichte Amnesty International bei der Schweizer Bundesanwaltschaft sowie bei der Genfer Staatsanwaltschaft Anzeige wegen des Verstoßes der Anti-Folter-Konvention gegen Bush ein. In seinen Memoiren hatte Bush unter anderem zugegeben, persönlich angeordnet zu haben, den mutmaßlichen 9/11-Drahtzieher Chalid Scheich Mohammed dem Waterboarding zu unterziehen. Die Reise nach Genf, welche Bush am 12. Februar 2011 antreten wollte, wurde daraufhin wegen angekündigter Proteste abgesagt. Im November 2011 wurde Bush von der Kuala Lumpur War Crimes Commission der Verbrechen gegen den Frieden schuldig gesprochen wegen der rechtswidrigen Invasion im Irak.

Als Bush von seinem Nachfolger Obama telefonisch über die Tötung Osama Bin Ladens am 2. Mai 2011 informiert wurde, bezeichnete er dies als „guten Anruf“; jedoch sei er „nicht außer sich vor Freude“ gewesen. Er lobte die Arbeit der Geheimdienste in dieser Sache. Obamas Einladung zu einem gemeinsamen Auftritt bei der zu diesem Anlass abgehaltenen Feier am Ground Zero lehnte er ab.

Bush tritt – wie auch sein Vorgänger Bill Clinton – regelmäßig als Redner auf. Nach Berechnungen des "Center for Public Integrity" von 2011 hat er damit seit 2009 mindestens 15 Millionen Dollar verdient. Seit dem Ruhestand ist Bush als Maler tätig und wurde von seiner Kunstlehrerin als erfolgreich beschrieben.

Am 25. April 2013 eröffnete George W. Bush wie alle Vorgänger seit Herbert Hoover eine Präsidentenbibliothek. An der Eröffnungsfeier in Dallas nahmen Barack Obama, Bill Clinton und Jimmy Carter sowie sein Vater teil. Bush erklärte dabei: „Der politische Wind weht von rechts oder links, Umfragewerte steigen und fallen, Unterstützer kommen und gehen, aber am Ende zeichnen sich politische Führer durch ihre Überzeugungen aus. Meine tiefste Überzeugung und die Richtschnur meiner Regierung war, dass die USA sich für die Ausweitung der Freiheit einsetzen müssen.“

Aufgrund einer Arterienverstopfung wurde Bush im August 2013 am Herz operiert. Dabei wurde ihm ein Stent eingesetzt.

Im November 2014 veröffentlichte Bush eine Biografie über seinen Vater mit dem Titel "41: A Portrait of my father."

Im Vorwahlkampf der Republikaner für die Präsidentschaftswahl 2016 unterstützte und beriet Bush seinen Bruder Jeb, hielt sich jedoch lange im Hintergrund. Im Februar 2016 beteiligte er sich an Wahlkampfveranstaltungen seines Bruders in South Carolina. Er äußerte sich ablehnend zum schließlich als Parteikandidat nominierten Unternehmer Donald Trump und hat ihm – wie sein Vater George H. W. Bush und sein Bruder – die Unterstützung für die Hauptwahl im November verweigert.

Am 20. Januar 2017 nahm er an der Amtseinführung Trumps teil. Fünf Wochen später nahm Bush in einem Fernsehinterview die freie Presse vor Trumps Anfeindungen in Schutz und äußerte, es sei Aufgabe der Medien, die politische Führung zu kontrollieren.
Im Oktober 2017 hielt er eine vielbeachtete Trump-kritische Rede.

Laut Julian E. Zelizer verfolgte die Regierung Bush konstant vier wesentliche Ziele. 

Erstens setzte Bush die Politik seiner Vorgänger der letzten Jahrzehnte fort, Dienstleistungen, Hochtechnologie und die Erdölproduktion sowie ‑verarbeitung im Sun Belt zu fördern, um somit auch dem demographischen Wandel zuungunsten des Mittleren Westen und des Nordosten Rechnung zu tragen. Zudem stammte ein bedeutender Teil der Wählerbasis der Republikaner aus dieser Region. 

Zweitens ging es Bush darum, die Industrie zu deregulieren und zeitgleich die Steuern zu senken. Während der Präsidentschaft gab es daher kaum Initiativen im Bereich Arbeits- und Umweltschutz. Ein weiteres Mittel, das bereits Richard Nixon und Reagan genutzt hatten, war die Besetzung von Behörden mit Leitern, die deren Auftrag politisch ablehnend gegenüberstanden. Steuersenkungen wurden vor allem für die Wohlhabenden und die obere Mittelschicht durchgesetzt. 

Ein drittes, seit dem Vietnamkrieg von den Konservativen favorisiertes Leitmotiv der Präsidentschaft war, die exekutive Vollmacht in der Sicherheitspolitik mit aller Gewalt zu stärken, insbesondere nach 9-11. Dazu gehörte eine erhebliche Ausdehnung von Geheimdienstoperationen unter diesem Mandat und eine beim Verhör bis zur Folter reichende Verschärfung ihrer Methoden. Die präsidiale Exekutivmacht durch Bush war mitbestimmend beim Nationbuilding nach dem Irakkrieg. Wie auch Reagan und sein eigener Vater sah er andererseits weitgehende und für die USA verlustreiche Kriege wie Vietnam als warnendes Beispiel für die Grenzen eigenen Engagements. 

Das vierte und am schwersten zu erreichende Ziel war, eine exekutive und legislative Parteienmacht herzustellen, wie dies zuletzt den Demokraten nach der Präsidentschaftswahl in den Vereinigten Staaten 1932 gelungen war. Dazu wurden einerseits dezidiert konservative Positionen eingenommen, um mit der höheren Mobilisierbarkeit dieser Klientel in polarisierenden Fragen knappe Mehrheiten gewinnen zu können, andererseits konnten mit den Reaktionen auf 9-11 oder Initiativen wie dem "No Child Left Behind Act" breite politische Mehrheiten im Sinne des "compassionate conservatism" gewonnen werden.

Der Regisseur Oliver Stone und der Drehbuchautor Stanley Weiser erarbeiteten eine Filmbiografie von Präsident Bush: "W. – Ein missverstandenes Leben" hatte am 17. Oktober 2008 in den Vereinigten Staaten Premiere. George W. Bush wird von Josh Brolin und Laura Bush von Elizabeth Banks gespielt.

2008 erschien mit George Walker Bush in „Being W.“ zudem ein satirischer Dokumentarfilm.






</doc>
<doc id="1849" url="https://de.wikipedia.org/wiki?curid=1849" title="Gobi">
Gobi

Die Wüste Gobi, oder kurz die Gobi (, ), ist ein weiträumiges Trockengebiet in Zentralasien, in der Mongolei und China. Sie besteht aus zusammenhängenden, vielgestaltigen Wüsten- und Halbwüstenlandschaften. Charakteristisch sind insbesondere die extrem wasserarmen, meist steinigen und seltener sandigen Wüsten in China, sowie die lebensfreundlicheren, weiten Steppen in der Mongolei. Mit 2,3 Millionen km² ist sie eine der größten nichtpolaren Wüstenregionen der Erde.

Die Wüste Gobi ist ein riesiges Trockengebiet, das in mittleren Breitengraden auf dem Mongolischen Plateau im Innern des asiatischen Kontinents gelegen ist. Zu ihr gehören in Nordwest-China die zusammenhängenden Wüstenareale von "Junggar Gobi", "Gaxun Gobi", "Alashan Shamo" und "Erdos Shamo", in der Süd-Mongolei die "Trans-Altai-Gobi", sowie in der Südost- und Ost-Mongolei die "Ost-Gobi" bzw. "Mongolische Gobi". Innerhalb dieser Wüstenregionen werden noch zahlreiche weitere Teilwüsten unterschieden. Grob umrissen wird sie von großen Gebirgsketten begrenzt. Das sind im Norden die Altai- und Changai-Gebirge, im Osten der Westrand der Da Hinggan-Gebirgskette, im Süden Qilian Shan und Bei Shan, im Westen die östlichen Ausläufer des Tian Shan. Nicht zur Wüste Gobi gehören die im Westen im Tarimbecken gelegenen Sandwüsten Kumtag, Lop Nor und Taklamakan. Diese sind zwar durch den Hexikorridor mit ihr verbunden, aber sonst durch die Bergketten von Kuruktagh und Beishan von ihr getrennt.

Die Begrenzung der Wüste Gobi war lange Zeit nur vage definiert. Das Gebiet "Xamo Desertum" wurde zwar schon in einigen Karten aus dem 17. Jahrhundert als Streifen dargestellt, der klar begrenzt ist und von einer Reihe von Gebirgen umgeben wird, aber eine systematische Vermessung erfolgte erst zu Beginn des 21. Jahrhunderts. Unter Anwendung moderner naturwissenschaftlicher Kriterien wie des NDVI-Vegetationsindex können in Satellitenaufnahmen Wüsten identifiziert werden. So wurde festgestellt, dass die Grenze der Wüste Gobi unter dem Einfluss von sich verändernden anthropogenen und klimatischen Faktoren jedes Jahr variiert. Für das Jahr 1982 ergab sich eine Fläche von 1,16 Millionen km², die in den folgenden Jahre leicht abnahm. Für 1988 wurde ein plötzlicher Anstieg auf 1,57 Millionen km² festgestellt. Danach nahm die Fläche wieder deutlich ab, jedoch ohne den alten Wert zu erreichen. Es wird angenommen, dass dies die Folge des El Niño von 1982–1983 sowie eines besonders warmen Winters und Frühlings in 1989/1990 war. Eine neuere Untersuchung mit Satellitenaufnahmen, die auch einen Ariditätsindex zur Definition des Wüstengebiets heranzieht, ergab einen wesentlich größeren Wert für die Fläche. Für den Zeitraum von 2000 bis 2012 betrug die durchschnittliche Fläche 2,35 Millionen km². Dabei variierte der Wert zwischen 2,08 Millionen km² im Jahr 2009 und 2,47 Millionen km² im Jahr 2012. Damit ist die Wüste Gobi nach der Sahara und den Arabischen Wüsten eine der größten nichtpolaren Wüsten der Erde.

Die "Bejschan" (auch "Beishan") ist eine vor allem gebirgige Wüste, sie wird daher manchmal auch als Gebirge eingeordnet. Sie stellt eine Verknüpfung zwischen der Steppe Gobi im Osten und dem Tarimbecken im Westen dar.

Monod klassifizierte Bejschan als eigene Wüste, da hier bestimmte Tier- und Pflanzenarten vorkommen, und sie somit ein eigenes Ökosystem bildet. Dazu kommt die eigene geologische Struktur. Abgegrenzt wird das Gebiet im Süden durch das Nanschan-Gebirge, im Norden durch die mongolische Grenze, im Osten durch den Flusslauf des Edsin Gol, dahinter schließt sich die Alashan an.

Durch die Wüste verläuft heute eine Schnellstraße als Verbindung von Xinjiang mit dem östlichen China.

Die "Alashan" (auch "Alaschan", mongolisch "Alxa") ist eine Sandwüste südwestlich der Gobi, die in China mit den Teilwüsten-Namen "Tengger" (Tenggeli Shamo) und "Badain Jaran" (Badanjilin Shamo/Badan Jilin Shamo) bezeichnet wird. Sie wird im Süden durch die Verlängerung des Nanschan-Gebirges abgegrenzt, im Südosten durch den Gelben Fluss, im Osten durch das Alxa- bzw. Helan-Shan-Gebirge, nördlich davon wiederum durch den Gelben Fluss. Im Norden reicht sie bis an die mongolische Grenze, im Westen schließt sich die Bejschan an. Die Landschaft hat eine allgemeine Höhe von 1000 bis 1500 m. Im Badain Jaran liegt der Biluthu, der höchste Sandberg der Welt mit 1610 Metern Höhe.

Die Namensgebung ist umstritten, da manche Wissenschaftler den Begriff „Alashan-Gobi“ verwenden und sie als Teil der Gobi ansehen. Dabei soll „Gobi“ allerdings für die Form der Fels- und Geröllwüste stehen. Teilweise wird der Begriff "Alashan" selbst vermieden. Stattdessen wird die Region weiter in die Wüsten Badain Jaran und Tengger (mongolisch: „weiter Himmel“) unterteilt.

Laut dem Forscher Prschewalski ist die Alashan eine Ebene, die wahrscheinlich einmal das Bett eines großen Sees oder Meeres bildete. Er folgerte dies aus der ebenen Form der Region insgesamt und den Salztonebenen samt Salzseen in den tiefsten Teilen. Insbesondere in der Tengger-Wüste ist über hunderte Kilometer nichts als bloßer Sand zu sehen; daher auch die mongolische Bezeichnung „Tengger“ für weiter Himmel. Es gibt in der Alashan fast keine Oasen. Nahe den angrenzenden Gebirgen ist anstelle des gelben Sandes auch Kies zu finden.

Im westlichen Teil der Alashan gibt es Dünen, die bis zu 520 m hoch ("Biluthu") sind und somit die höchsten Dünen der Erde darstellen. Von den 140 Salzseen, die zwischen den Dünen zu finden sind, gelten einige den Mongolen als heilig. Daher befinden sich hier auch lamaistische Klöster. Die Einwohner sind heute vor allem Han, Mongolen und Hui.

Das Klima der Gobi ist kontinental, also mit extrem niedrigen Temperaturen im Winter und heißen Sommern. Wegen fehlender Gewässer und geringer Vegetation sind die Temperaturunterschiede zwischen Tag und Nacht das ganze Jahr über stark schwankend. In den Wintermonaten gibt es Tage, an denen das Thermometer auf unter −65 °C fällt.

Obwohl im Südosten Ausläufer des Monsuns die Steppe erreichen, ist das Gebiet mit 30 bis 200 mm Niederschlag pro Jahr durch Trockenheit ausgezeichnet. Besonders die kalten Winter sind gleichzeitig sehr trocken. Im Frühling und frühen Sommer kann es allerdings zu eisigen Sand- und Schneestürmen kommen. Damit ist das Gebiet im Gegensatz zur Taklamakan-Wüste nicht hyperarid.

Die Bezeichnung Gobi wird mehrdeutig verwendet. Sie steht zum einen für das geographische Objekt Wüste Gobi, das heißt für ein konkretes Trockengebiet in Zentralasien, das eine irgendwie definierte Begrenzung hat und den Namen Gobi trägt. Zum anderen wird das Wort Gobi auch als geomorphologisches Synonym für Steinwüste benutzt, die ein beherrschendes Landschaftselement in der Wüste Gobi darstellt. Die zeitgenössische Geographie unterscheidet dabei in der Wüste Gobi neben Gobi für die steinigen Landschaften auch noch Shamo für die sandigen Landschaften.

Ursprünglich erhielten die innerasiatischen Wüsten ihre Namen durch die lokalen Minderheiten. In China wurden diese Bezeichnungen später aus den lokalen Sprachen in die chinesische Schrift transliteriert, oft unter Veränderung ihrer Bedeutung. Der chinesisch-mongolische Wüstengürtel war für frühe europäische Handelsreisende völlig unüberschaubar und schier endlos. Um von Vorderasien nach China zu gelangen, nahm die schwierige und gefahrvolle Durchquerung der sandigen Ebenen und kahlen Berge der „großen Wüste“, wie sie im 13. Jahrhundert von Marco Polo bezeichnet wurde, an ihrer schmalsten Stelle einen ganzen Monat in Anspruch. Die Mongolen nannten die Wüste Govi oder Kébé, was „wasserloser Ort“ bedeutete und der modernen Definition von Wüste nahekommt. Im 17. Jahrhundert fand die Bezeichnung "Gobee" erstmals in europäischen Landkarten Erwähnung. Gleichzeitig, so nahmen es die Europäer wahr und notierten es in ihren Landkarten, wurde in China die Wüste Gobi auch als Sha-mo () bezeichnet, was eigentlich Sandwüste bedeutet.

Allerdings wurde und wird in China und der Mongolei die Wüste Gobi stärker geographisch und geomorphologisch nach Teilwüsten differenziert und es werden gegebenenfalls andere Bezeichnungen benutzt. So wird, wenn es um die geomorphologische Einteilung geht, von "gobi" nur dann gesprochen, wenn mehr als 50 % der Fläche von losem unsortiertem Geröll und Kies mit 4 bis 64 mm Durchmesser bedeckt sind. Ein Beispiel für eine solche regionale geomorphologische Einheit ist die Junggar Gobi () am nord-westlichen Extrem der Wüste Gobi in China. Etwa 43,5 % aller chinesischen Wüstenflächen sind nach dieser Definition Gobi-Wüsten, also sinngemäß Steinwüsten.
Die Junggar Gobi ist gleichzeitig ein Beispiel dafür, dass auch heute noch historisch bedingt verschiedene Namen in uneinheitlicher Schreibweise für dasselbe Gebiet verwendet werden. Sie ist auch bekannt als Gurbantünggüt-, Gurban Tunggut-, Dsungarische- oder Dzoosotoyn-Elisen-Wüste.

Handelt es sich um vom Wind angewehten Sand der ausgedehnte Flächen oder kleine Dünen bildet, dann wird in China von "shazhi huangmo" ( „Sandland“) gesprochen, wie zum Beispiel Hunshandake Shazhi Huangmo (, englisch „Hunshandake Sandy Land“, deutsch „Hunshandake-Sandland“) am östlichen Extrem der Wüste Gobi in China. 45,3 % aller chinesischen Wüstenflächen sind solche Sandwüsten. In der Mongolei heißen die Sandwüsten Els, wie z. B. das Dünenfeld Khongoryn Els.

Die verbleibenden Trockengebiete sind Steppen mit semistabilen oder stabilen Dünen und Sandflächen die in China mit "shadi" () bzw. Sandland bezeichnet werden, wie zum Beispiel bei Horqin Shadi (), ebenfalls am östlichen Extrem der Wüste Gobi gelegen. 11,2 % aller chinesischen Wüstenflächen sind Steppen.

In der Gobi wurden viele bedeutende Fossilien, darunter auch Versteinerungen, aus unterschiedlichen geologischen Epochen gefunden. Die meisten Funde von Sauriereiern und ganzen Nestern stammen von hier. In vergangenen Abschnitten der Erdgeschichte herrschten andere klimatische Verhältnisse mit üppiger Vegetation, die entsprechend gute Lebensbedingungen für Saurier boten.

Die Gobi ist in der Geschichte vor allem als Teil des Mongolischen Reichs bekannt. Außerdem liegen einige wichtige Handelsstädte der Seidenstraße am Südrand der Gobi.

In der Wüste Gobi und den umliegenden Regionen sind zahlreiche Tierarten anzutreffen, darunter Wölfe, Dschiggetai, Kropfgazelle, Gerbil und Steppeniltis. Es gibt auch noch einige Schneeleoparden.

Die Wüste beherbergt einige an Trockenheit angepasste Sträucher und Gräser. Das Gebiet ist allerdings gegen Beweidung und Fahrzeuge sehr anfällig. Menschliche Einflüsse sind vor allem im östlichen Teil größer, wo auch mehr Regen fällt und sich Viehhaltung lohnt. In der Mongolei wurde das Grasland in letzter Zeit durch den größeren Anteil von Ziegen in den Viehherden zurückgedrängt. Die Ziegen liefern wertvolle Kaschmirwolle, reißen aber im Gegensatz zu Schafen das Gras mit der Wurzel aus. Die qualitativ hochwertige Kaschmirwolle ist ein Hauptexportgut der Mongolei und wird fast zu 100 % von großen chinesischen Textilunternehmen abgenommen. Da diese Produktionssteigerung nicht nachhaltig vorangetrieben wurde, gilt das Ökosystem als extrem gefährdet. Seit die Viehherden in den 1990er Jahren privatisiert wurden, gibt es auch die staatliche Kontrolle der Weideflächen nicht mehr.

In der Volksrepublik China stellt Desertifikation ebenfalls ein gravierendes Problem dar, nicht zuletzt, da es in Form von Sandstürmen auch dichter besiedelte Gebiete wie Peking betrifft. In der Inneren Mongolei versucht man, gegen die Überweidung durch großflächige Absperrungen von Weideland und Umsiedlungen vorzugehen. Außerdem werden vor allem entlang von Verkehrslinien breite Schutzpflanzungen (Chinas Grüne Mauer) angelegt, um die Auswirkungen von Sandstürmen zu begrenzen.





</doc>
<doc id="1850" url="https://de.wikipedia.org/wiki?curid=1850" title="Gott">
Gott

Als Gott (je nach Zusammenhang auch "Göttin, Gottheit") wird meist ein übernatürliches Wesen bzw. eine unvorstellbare Macht bezeichnet. Im Verständnis verschiedener Mythologien, Religionen und Glaubensüberzeugungen sowie in der Metaphysik werden einem Gott oder mehreren Göttern besondere Verehrung zuteil und besondere Eigenschaften zugeschrieben; unter anderem erster Ursprung bzw. Schöpfer oder Gestalter der Wirklichkeit zu sein.

Mit "Gott" ohne weitere Bestimmung wird meist ein allumfassender Gott bezeichnet.

Der Wortstamm von "Gott" ist alt, doch nur im germanischen Sprachraum anzutreffen und außerhalb unbekannt. Bezeichnungen sind alt- und mittelhochdeutsch "got," altsächsisch, altfriesisch, mittelniederdeutsch und englisch "god", gotisch "guþ," altnordisch "gođ" sowie schwedisch und dänisch "gud".

Die Germanen verehrten den urgermanischen Himmelsgott "Tiwaz", der durch sprachliche Evidenz als indogermanisches Erbe erwiesen ist. In den verschiedenen Dialektgruppen des Germanischen erscheint er beispielsweise als althochdeutscher "Ziu" und altnordischer "Tyr". Der Name des Gottes geht zurück auf das indogermanische Wort für den „Gott“, "*deiwos". Hierbei handelt es sich um eine bereits urindogermanische Vrddhi-Ableitung zum Wort "*djews" „Himmel“. Die Personifizierung "*djeus phtēr" „Vater Himmel“ findet sich wieder im griechischen "Zeus" ("Zeu páter", Vok. zu , Gen. ), dem römischen "Jupiter" (vom Vokativ *"Dioupater" zum Nominativ "Diēspiter"), dem vedisch-altindischen "Dyaus Pita" und dem illyrischen ("Dei-pátyros" „himmlischer Vater“). Alle diese Formen können auf die Wurzel "*djew-" zurückführen, das als „Erstrahlen, Erscheinen“ übersetzt wird. Dieses Wort liegt wiederum mit seiner Ableitung *"deiwos" dem altindischen "deva" und dem lateinischen "deus" als Begriffe für Gott zugrunde. 

Für die Herkunft des germanischen Wortes "Gott" wird davon ausgegangen, dass der Begriff aus dem substantivierten zweiten Partizip des indogermanischen "*ghuto-m" der Verbalwurzel "*gheu-" „rufen, anrufen“ entstanden ist. Danach wären die Götter die (etwa durch Zauberwort) angerufenen Wesen. Alternativ könnte das Wort auch auf die indogermanische Verbalwurzel "*gheu-" „gießen“ zurückgeführt werden, wonach der Gott als „das, dem (mit) Trankopfer geopfert wird“ zu verstehen wäre. Das griechische "theói" steht ebenfalls etymologisch mit dem Verb "thýein" „opfern“ zusammen, wie das Simplex "theós" „Gott“ durch Entsprechungen im anatolischen Wortschatz das Votivobjekt des Altars etymologisch bezeichnet. Das Standardnachschlagewerk, Kluges "Etymologisches Wörterbuch der deutschen Sprache," bestärkt die Vermutung einer Ableitung von „gießen“ oder Trankopfer durch Vergleich mit dem Avestischen und Altindischen. Wolfgang Meid fügt hierzu an: „Dies ist aber grammatisch unplausibel, denn ‚gegossen‘ wird der Trank, nicht der Gott“.

Die germanische Bezeichnung "*guda-" „Gott“ war ursprünglich ein grammatisches Neutrum, ebenso wie andere germanische Bezeichnungen für Götter. Bei der Übertragung auf den christlichen Gott wurde das Wort zur Zeit der arianischen Christianisierung der Goten im 3. bis 4. Jahrhundert im oströmischen Wirkungskreis und in der fränkisch-angelsächsischen römisch-katholischen Mission unter den Merowingern und Karolingern zum Maskulinum. Im Gotischen blieb das Wort allerdings als Bezeichnung der heidnischen Götter – wegen der christlichen Ablehnung dieser Götter – geschlechtslos. Der Übergang vom Neutrum zum Maskulinum vollzog sich im westgermanischen Bereich etwa vom beginnenden 6. Jahrhundert bis zum ausgehenden 8. Jahrhundert. Im skandinavisch-nordgermanischen Bereich hielt sich das Neutrum länger, da dort das Wort für den persönlichen Gott Ase ("óss") lebendig blieb.

Wie die anderen Wörter, beziehungsweise Begriffe für „Gott“ wurde es oft in der Mehrzahl verwendet, um eine nicht näher umschriebene Gruppe göttlicher Wesen zu beschreiben. Aufgrund der Abstammung des Wortes wird davon ausgegangen, dass es die höheren Mächte (Numen) als passive Wesen bezeichnet, die verehrt wurden, und nicht als aktive Wesen, die das irdische Geschehen instand hielten. Andererseits waren andere Wörter für „Gott“ zur Bezeichnung eines aktiven Wesens ebenfalls geschlechtslos. Daraus ergibt sich mit hoher Wahrscheinlichkeit, dass solche Wörter im Plural die Götter als Ganzheit bezeichneten ("tívar" = altnordischer Plural „die Götter“, ursprünglich zu Týr). Viele Geschehnisse waren nicht einem bestimmten „Gott“, sondern ganz allgemein „den Göttern“ zuzuschreiben. Daraus erklärt sich, dass die Singularform des ursprünglichen "*deiwos-Teiwaz" nur noch appellativ in Namenskompositen erscheint, beispielsweise bei Odin, der den Beinamen "Fimbultýr" („großer, gewaltiger Gott“) trägt. Neben den einzelnen Göttergestalten, die durch einen eigenen Namen, eigene Mythen und einen festen Kult in den Vordergrund traten und leicht zu erkennen waren, gab es die unabsehbare göttliche Masse, aus der beispielsweise Mythendichter neue Figuren hervorheben konnten.

Einen transzendenten Gottesbegriff entwickelten die Germanen nie, oder nur im Norden und erst sehr spät. Erst bei Snorri Sturluson im 13. Jahrhundert ist Odin der "Alfaþir" („Allvater“). In der Übergangszeit der Christianisierung, verbunden mit Formen von Synkretismus wurden Odin, Thor und Balder in den isländisch-nordischen Texten zu allmächtigen oder vollkommenen Göttern erklärt, um der auftauchenden Gestalt Christi entgegentreten zu können. Der begriffliche Gegensatz zwischen „Göttern“ und „Menschen“ ("*teiwoz" – "*gumanez"), den die Germanen von altersher kannten, wurde ersetzt durch die neue Dichotomie "*guda" – "*gumanez." Indem diese Verbindung stabreimend wirkt, fand sie in diverser Dichtung, insbesondere der altnordischen, Eingang und somit auch Wirkung. Die ehemals geschlechtsneutrale Begrifflichkeit „Gott“ wurde schließlich männlich, sobald sie den christlichen Gott bezeichnete. So trat infolge der Christianisierung der heute bestehende Bedeutungswandel ein, in dem das Wort umgedeutet und auf den − meist als männlich empfundenen − jüdisch-christlichen Gott JHWH () angewendet wurde.

Der erst in karolingischer Zeit belegte Begriff "Gottheit" (für gr. θεότης, lat. "divinitas") ist mehrdeutig und kann zum einen als Substanzbegriff im Sinne von „göttliche Natur“ verwendet werden oder das Innere, Passive der Göttlichkeit betonen, zum anderen ausschließlich auf außerchristliche Götter angewandt werden. Letztere Bedeutung ist erst seit der Mitte des 18. Jahrhunderts gebräuchlich.

Die Frage, unter welchen Umständen eine Entität als Gott eingeordnet werden kann, hat bislang in der Religionswissenschaft kaum Beachtung gefunden, zumal die jüdisch-christliche Tradition stets eine implizite Vorlage für den Gottesbegriff lieferte. Dies ist neben der Einschränkung auf einen Kulturraum insofern problematisch, als es bereits in diesen Religionen eine Vielzahl unterschiedlicher Gottesvorstellungen gibt. H. P. Owen stellt in der "Encyclopedia of Philosophy" fest, dass es „sehr schwierig und vielleicht unmöglich“ sei, eine Definition von „Gott“ aufzustellen, die alle Verwendungen des Wortes und entsprechender Wörter in anderen Sprachen abdeckt. Die 2. Ausgabe des "Dictionnaire de la langue philosophique" gibt als allgemeine Definition an: „Übernatürliches Wesen, das die Menschen ehren sollen.“ Der christliche Philosoph Brian Leftow legt in der "Routledge Encyclopedia of Philosophy" folgende restriktivere Definition zugrunde: „Die höchste Wirklichkeit, die Quelle oder der Grund alles anderen, perfekt und der Anbetung würdig.“

Nicht alle Kulturen unterscheiden eindeutig zwischen Göttern, Geistern, Engeln, Dämonen und anderen übernatürlichen Wesen; gelegentlich wird der entsprechende Begriff in anderen Sprachen recht weit gefasst. So etwa können die Orishas der Yoruba sowohl als Ahnengeister als auch als dem höchsten Gott Olorun untergeordnete Götter betrachtet werden. Das Wort "vodon" (vgl. „Voodoo“) in der Fon-Sprache wird sowohl mit „Gott“ als auch mit „Geist“ übersetzt, ebenso wie das japanische Wort "Kami." Die buddhistischen Devas, meist als „Götter“ übersetzt, sind übernatürliche Wesen mit eigener Persönlichkeit, gelten aber nicht als perfekt, unsterblich, allmächtig oder allwissend. Einige neuplatonische Denker bezeichneten mit dem Wort "θεός" (theós) eine Vielzahl spiritueller Entitäten, darunter die menschliche Seele. Die Frage nach einer angemessenen Definition von „Gott“ wird noch dadurch verkompliziert, dass Philosophen und Theologen Gottesbegriffe entwickelt haben, die sich von der religiösen Praxis wesentlich unterscheiden (siehe Abschnitte zu metaphysischen und populären Vorstellungen).

In der kognitiven Religionswissenschaft werden Götter zu den übernatürlichen Akteuren gezählt. Als Akteur wird in der Philosophie und Psychologie ein Wesen mit geistigen Fähigkeiten bezeichnet, dem bewusste Ansichten und Wünsche zugesprochen werden, oder dessen Verhalten durch mentale Zustände hervorgerufen wird. Aus natürlichen Konzepten können übernatürliche gebildet werden, indem intuitive, alltägliche Auffassungen der ihnen zugehörigen ontologischen Kategorien verletzt werden. Beispiele für solche Konzepte sind Bäume, die sich nirgendwo befinden, Steine, die Gefühle empfinden, und eben auch Wesen, die unsichtbar sind. Die geistigen Fähigkeiten des Akteurs sind die einzige anthropomorphe Eigenschaft, die von Gläubigen und Theologen gleichermaßen akzeptiert wird.

Oft wird zwischen polytheistischen Religionen, die mehrere Götter kennen, und monotheistischen Religionen mit nur einem Gott unterschieden. In der Kosmologie monotheistischer Religionen werden die polytheistischen Götter mit ihren unterschiedlichen Funktionen teils zu Attributen des einzigen Gottes zusammengefasst, teils tiefergestellten übernatürlichen Wesen wie Engeln und Heiligen übertragen.

In vielen polytheistischen Religionen sind die Götter als Pantheon organisiert. In dieser heiligen Gemeinschaft gibt es eine Hierarchie, die sich aus den unterschiedlichen Funktionen der einzelnen Götter ergibt. Teilweise gibt es einen Herrscher über das Pantheon, wie zum Beispiel einen Vater aller Götter (so etwa El bei den Kanaanäern) oder eine Göttin mit Vormachtstellung (etwa Amaterasu im frühen Shintō). Religionen mit einem Hauptgott werden henotheistisch genannt. Philosophen wie Plato und die Stoiker sprachen gelegentlich von „Gott“ und „den Göttern“ unterschiedslos im selben Absatz.

Die Abgrenzung zwischen Mono- und Polytheismus ist nicht immer objektiv eindeutig, denn in manchen Religionen existiert ein Gott in mehreren Formen, beziehungsweise Hypostasen (Trimurti im Hinduismus, Dreifaltigkeit im Christentum, „Gott oben/unten“ bei den Bari, „Vater, Mutter, Sohn“ bei den Ndebele). Darüber hinaus können übernatürliche Personen wie Maria (die Mutter Jesu) oder Siddhartha Gautama zumindest im Rahmen der vergleichenden Religionswissenschaft oder aus dem Blickwinkel anderer Religionen als zusätzliche Götter betrachtet werden. Auch kann eine Religion insofern mono- und polytheistische Aspekte vereinen, als je nach Konfession und selbst je nach Anhänger unterschiedliche Gottesvorstellungen anzutreffen sind. Frühe Christen glaubten beispielsweise je nach Gruppierung an einen, zwei, 30 oder 365 verschiedene Götter, und Dreifaltigkeitslehren reichen vom Glauben an drei Götter (Tritheismus) bis zur Vorstellung, dass die drei nur verschiedene Aspekte eines Gottes sind (Modalismus). Alle drei abrahamitischen Religionen sind heute ausdrücklich monotheistisch.

Die Götter monotheistischer Religionen, die höchstrangigen, mächtigsten Gottheiten in polytheistischen Religionen "(siehe auch: Henotheismus)", aber auch Vorstellungen einer höchsten übernatürlichen Kraft in einigen ethnischen Religionen – etwa Kitchi Manitu der Algonkin – werden in Religionswissenschaft und Ethnologie häufig als "Hochgott" oder "Höchstes Wesen" bezeichnet. Bis zum Beginn des 20. Jahrhunderts setzten eurozentrisch denkende Völkerkundler und Missionare viele Hochgottvorstellungen vorschnell und undifferenziert mit der christlichen Gottesvorstellung gleich (etwa bei afrikanischen, australischen oder nordamerikanischen Göttern bzw. göttlichen Kräften). Die ethnographische Literatur ist voller Beispiele dafür. Häufig gilt der Hochgott als Schöpfer, der jedoch "nicht" angebetet wird, da er anschließend keinen Einfluss mehr auf das menschliche Leben nahm.

Tatsächlich werden die Hochgötter der verschiedenen Kulturen ausgesprochen unterschiedlich beschrieben. Dazu im Folgenden exemplarisch eine Tabelle nach vier Kriterien (in Prozentwerte umgerechnet aus dem "Handbook of Living Religions", Middlesex 1984):

Eine in verschiedenen Kulturen verbreitete Vorstellung des Ursprungs der Welt stellt das urtümliche Universum als Ei dar, das in seiner Schale die Fähigkeit zur Erschaffung aller Dinge enthält. Üblicherweise findet dann ein Ereignis statt, das Veränderungen und Entwicklungen bewirkt. Bei den westafrikanischen Dogon erschütterte der Schöpfergott Amma das kosmische Ei und ließ so Götter der Ordnung und des Chaos frei. Die Vorstellung eines göttlichen Handwerkers oder Zimmerers ist in Afrika weit verbreitet.

In mehreren Kulturen haben Elternwesen die Welt erschaffen. Im Schöpfungsmythos der Maori zum Beispiel begann die Welt, als der Himmelsvater und die Erdmutter Rangi und Papa durch ihre Söhne getrennt wurden. Bei den Azteken bestand die Schöpfung darin, dass Ometecutli sich in seine männlichen und weiblichen Teile, Ometeotl und Omecihuatl, trennte. Eine Variante des dualen Schöpfungsmythos findet sich im antiken Griechenland; hier waren die Erdmutter Gaia und der männliche Himmelsgott Uranos die ersten beiden Götter. Der Schöpfungsmythos eines ersten Götterpaares fand sich außerdem in der japanischen Mythologie mit der Überlieferung von Izanagi und Izanami, sowie in allen Kulturen von Ozeanien. In einigen Vorstellungen wurde die Welt – teils auch die Götter – geschaffen, indem ein Lebewesen geopfert wurde. In der nordischen Religion etwa schlachteten die drei Schöpfergötter den Urzeitriesen Ymir, dessen Organe zu Teilen der Welt wurden. Ähnliches wird in einer vedischen Hymne von Purusha und in der chinesischen Mythologie von Pangu berichtet.

Aristoteles erwähnt im Buch XII seiner Metaphysik einen immateriellen „unbewegten Beweger“ (griech. ού κινούμενον κινεῖ) als erste Ursache, die der bereits vorhandenen Materie Struktur verliehen hat. Er verneinte jedoch eine Schöpfung, denn die Materie sei ewig und unerschaffen. In seinem "Timaios" vertrat Platon die Auffassung, dass ein Schöpfergott (Demiurg) der ungeordneten Ur-Materie eine Form verliehen haben muss, um daraus ein vernünftiges Ganzes zu schaffen. Einige Götter haben sich „selbst erschaffen“, wie zum Beispiel Ometecuhtli bei den Azteken oder der Aborigine-Gott Baiame. In anderen Kulturen wie dem Christentum wird eine Schöpfung aus dem Nichts (Creatio ex nihilo) vertreten, bei der Gott ohne jegliche Voraussetzungen auskommt. Nicht alle Schöpfergötter haben alles erschaffen. Der Gott Karei oder Ta Pedn der Semang zum Beispiel hat alles außer der Erde und den Menschen geschaffen; diese sind das Werk des untergeordneten Gottes Ple.

In vielen Kulturen haben Schöpfergötter für die Menschen eine untergeordnete Rolle. Ein Beispiel ist Bunjil aus der Religion der Aborigines, der nach der Schöpfung der Erde, Bäume, Tiere und Menschen seinen beiden Kindern die Macht über Himmel und Erde übergeben hat. Seitdem hat er sich von der Welt zurückgezogen und schwebt über den Wolken.

Manche Religionen kennen einen Zyklus aus Schöpfung und Vernichtung. Eine der kompliziertesten Varianten findet sich im Hinduismus. Hier entspringt dem Nabel von Vishnu eine Lotusblume, die den Schöpfergott Brahma freigibt. Die von Brahma erschaffene Welt besteht eine sehr lange Zeit, bis sie sich in Chaos auflöst und der Zyklus von neuem beginnt. Weitere zyklische Vorstellungen der Welt finden sich unter anderem bei den Hopi-Indianern und den Azteken.

Götter, die sich im Himmel offenbaren, wurden und werden sehr oft als die höchsten Götter betrachtet; typische Beispiele sind der frühvedische Gott Varuna und der iranische Gott Ahura Mazda. Der Glaube an Himmelsgötter als höchste Wesen, die die Welt erschaffen haben, ist bis zu einem gewissen Grad in allen Ethnien anzutreffen. Solche Götter gelten jedoch meist als passiv, sodass ihnen in der religiösen Praxis eine unerhebliche Rolle zukommt. Bedeutender ist der Glaube an heilige Kräfte und Wesen, die dem Alltagsleben des Menschen näherkommen und die ihm zweckmäßiger erscheinen. Diese heiligen Kräfte nehmen unterschiedliche Formen an und reichen von Totemismus und Ahnenkulten bis hin zu Totengeistern und Sonnengöttern. Laut Mircea Eliade standen Himmelsgötter vormals oft im Zentrum des religiösen Lebens, wurden aber mit der Zeit durch zugänglichere Formen ersetzt. Beispiele für Himmelsgötter, die nach wie vor kultisch verehrt werden, sind der Zuñi-Gott Awonawilona und der Schöpfergott der San, Cagn.

In Kulturen mit differenzierten polytheistischen Vorstellungen gehen Himmelsgötter über meteorologisch-astronomische Phänomene hinaus. Oft wird ihnen eine außerordentliche Macht zugesprochen; der höchste Gott der arktischen Völker zum Beispiel ist ein allmächtiger Herrscher über die Welt. Im Gegensatz dazu ist der Himmelsgott einiger sibirischer und zentralasiatischer Völker so weit von der Welt entfernt, dass er sich nicht um menschliche Belange kümmert.

Der Donner war immer ein wichtiges Kennzeichen von Himmelsgöttern. Indianerstämme aus Kansas behaupteten, dass sie ihren Gott Wakan nie zu Gesicht bekommen, aber oft seine Stimme als Donner vernommen hätten. Die Spezialisierung von Himmelsgöttern zu Sturm- und Regengöttern erklärt sich laut Eliade durch ihre Passivität, die im Gegensatz zum direkten Einfluss der Sturmgötter auf die Landwirtschaft steht. Das vedische Ashvamedha-Opfer war zunächst dem Himmelsgott Varuna gewidmet, dessen Stelle wurde aber später vom Sturmgott Prajapati und manchmal auch Indra eingenommen. Weitere bekannte Beispiele für Sturmgötter sind Zeus, Min, Rudra, Adad, Iupiter Dolichenus und Thor. Häufig wiederkehrende Themen bei Sturmgöttern sind neben Regen und Donner die Heirat mit einer Erdmutter sowie eine rituelle und mythologische Beziehung zu Stieren. Min, Baal und Adad zählen zu den Göttern, die als Stiere repräsentiert werden und die nicht aufgrund ihrer himmelsartigen Attribute verehrt werden, sondern durch ihre Heirat mit der Erdmutter und den daraus erwachsenden lebensspendenden Funktionen. Demgegenüber bewahrten Zeus, Jupiter und El aufgrund ihrer Rolle als Weltherrscher eine gewisse Autonomie und Vormachtstellung im Pantheon.

Die Sonnenanbetung war vor allem in Ägypten, Asien und im primitiven Europa vorherrschend. In Afrika wurde der höchste Gott recht häufig mit der Zeit in einen Sonnengott umgewandelt; zahlreiche afrikanische Völker geben ihrem höchsten Gott den Namen „Sonne“. Bei den Kavirondo ist die Sonne der höchste Gott, und die Kaffa nennen ihr höchstes Wesen Abo, was sowohl für „Vater“ als auch für „Sonne“ steht. Ähnlich wie Himmelsgötter sind auch Sonnengötter in Afrika selten zentraler Gegenstand der Verehrung.

Ebenso waren die Sonnengötter Atum-Re im alten Ägypten, Huitzilopochtli in Mexiko, Amaterasu in Japan und die Sonnengötter diverser Indianerstämme die höchsten Götter. Sonnengötter können auch Zerstörungen hervorrufen, insbesondere bei Wüstenvölkern. In Ägypten führte Re die toten Seelen durch die Unterwelt. Auch der sumerische Gott Utu stand mit der Unterwelt in Beziehung, wo er über die Seelen richtete.

Da die Mondphasen mit den Gezeiten zusammenhängen, werden Mondgötter oft mit dem Wasser in Beziehung gesetzt. Der sumerische Gott Nanna etwa herrschte über die Wasser, und Ardvisura Anahita, die iranische Göttin des Wassers, war zugleich ein Mondwesen. Ähnliche Verbindungen bestanden bei den Irokesen und mexikanischen Kulturen. Ein zentralbrasilianisches Volk nennt die Tochter des Mondgottes „Mutter des Wassers“. Eine große Zahl von Fruchtbarkeitsgöttern stehen außerdem mit dem Mond in Verbindung, so etwa Ištar in Mesopotamien, Anaitis im Iran und Selene in Griechenland. Mondgötter wie Thot in Ägypten oder Aningaaq bei den Inuit messen die Zeit und regeln Naturerscheinungen. Götter, die mit den Sternen und Planeten in Verbindung gebracht werden, gelten gelegentlich als die Augen des Himmelsgottes, weshalb ihnen oftmals Allwissenheit zugesprochen wird.

Eine der ersten Theophanien der Erde und des Bodens war die einer Mutter, die mit Fruchtbarkeit in Verbindung gebracht wurde. Obwohl viele Erd- und einige Fruchtbarkeitsgötter als androgyn beschrieben werden, ist die Vorstellung einer personifizierten Erde oder Erdmutter weit verbreitet. Gaia wurde in Griechenland recht häufig verehrt. Laut Hesiods "Theogonie" entstand aus ihrem Schoße Uranos, mit dem sie zusammen in einer Form der Hierogamie eine ganze Familie von Göttern gebar. Die Entwicklung der Landwirtschaft führte dazu, dass die Erdmutter zugunsten einer Göttin der Vegetation und Ernte in Vergessenheit geriet; in Griechenland beispielsweise nahm Demeter den Platz von Gaia ein. Diese Entwicklung verlieh männlichen, befruchtenden Göttern wieder neues Gewicht. Derartige landwirtschaftliche Kulte waren sehr beständig und reichen in einigen Fällen von prähistorischen Zeiten bis in die Gegenwart.

Fluss- und Wassergötter wurden in mehreren Kulturen verehrt, so etwa Anahita im Zoroastrismus und Sarasvati im Hinduismus. Ein sehr bekannter Flussgott der Griechen, Acheloos, wurde von Homer nicht nur mit dem gleichnamigen Fluss in Verbindung gebracht, sondern als Gott aller Flüsse, Seen und Quellen zu den großen Göttern gezählt. Über allen kleineren Wassergöttern stand Poseidon, der Gott des Meeres. In der nordischen Religion personifiziert Ägir den endlosen Ozean. Für Hindus ist Ganga (der Fluss Ganges) eine mächtige Göttin, die das Land versorgt und zwischen der irdischen und der göttlichen Welt vermittelt. Sedna, die Meeresgöttin der Inuit, ist die Mutter aller Wassertiere, sorgt aber auch für Hunger und Verwüstung, wenn Menschen Tabus verletzen.

Georges Dumézil stellte drei hauptsächliche soziale Funktionen bei Göttern der proto-indoeuropäischen Kultur fest: die Funktion eines Herrschers mit magischen und rechtsprechenden Aspekten, eine physische Macht- und Mutfunktion, insbesondere in Kriegszeiten, sowie eine Fruchtbarkeits- und Wohlstandsfunktion. Auf andere Kulturen ist dieses Schema nur bedingt anwendbar. So etwa kombinieren viele Götter des Nahen Ostens und in Afrika die Funktionen eines Herrschers und eines Kriegsherren, während andere Kulturen nicht klar zwischen den Ernte- und den Kriegsfunktionen trennen.
Die höchsten Götter sind oft zugleich Hüter der gesellschaftlichen Ordnung und der Moral. Derartige Götter ziehen Menschen zur Verantwortung, richten über sie und bestrafen sie, entweder direkt oder indirekt durch andere Götter. Im vedischen Verständnis gilt Varuna als Schützer des kosmisch-moralischen Gesetzes "(rta)." Der jüdisch-christliche Gott JHWH ist der Urheber des Gesetzes. In der römischen Religion war Jupiter der Hüter des Eides, der Verträge und der moralischen Pflichten. In Babylon wachte die Versammlung der großen Götter über die Gesellschaft und bestimmte über die menschlichen Schicksale.

Diejenigen Götter, die ihre physische Macht einsetzen, fungieren oftmals gleichzeitig als Kriegsgötter. Diese Rolle kommt besonders kosmischen Sturmgöttern zu, zum Beispiel Indra in den Veden, Thor in der nordischen Religion, Marduk bei den Babyloniern oder JHWH bei den Israeliten. Ein klassischer Kriegsgott ist Mars, der den römischen Staat gegen die Feinde verteidigte, aber auch Felder und Herden vor Unglücken schützte. Für die Yoruba ist Ogún der Gott der Jagd, der Eisenherstellung und des Krieges. Viele Göttinnen werden ebenfalls als göttliche Kämpferinnen und Beschützerinnen verehrt, so etwa Anat bei den Kanaanäern, Athene bei den Griechen oder Durga in der hinduistischen Tradition. Göttliche Beschützer sind sehr vielfältig und reichen von Castor und Pollux, den Beschützern der römischen Soldaten, bis hin zu den Straßen-Kami in Japan.

Fruchtbarkeitsgötter bilden eine sehr große und vielfältige Kategorie. In Griechenland war Hera, die Gattin des Zeus, Göttin der Heirat, und Aphrodite sowie Eros sind Liebesgötter. In Skandinavien war Freya die Göttin der Liebe und der Ehe. Die aztekische Göttin Xochiquetzal war eine beliebte Göttin der Künste, Liebe und Liebeslust. Volkstümliche mexikanische Darstellungen identifizieren die Jungfrau Maria mit einer indigenen Fruchtbarkeitsgöttin, die vor dem Eintreffen der Europäer das Land beherrschte.
Hestia war die griechische Göttin des Familienherdes, ebenso wie Vesta bei den Römern, wo sie eine besondere staatliche Kultstellung einnahm. In der vedischen Zeit herrschte Agni, Gott des Feuers, gleichzeitig über den Familienherd, ebenso wie Zao Jun in der chinesischen Volksreligion. Im alten Ägypten war Neith die Göttin des häuslichen Handwerks, ähnlich Athene bei den Griechen. Für die Ainu Nordjapans war die Feuergöttin Iresu-Huchi gleichzeitig die Göttin des Haushalts, dem sie Frieden und Wohlstand schenkte. Traditionelle japanische Haushalte zeigen Bildnisse von Daikoku und Ebisu als Schützer des Haushaltes.

Auch Dörfer haben nicht selten eigene Götter, die ihnen Schutz und Gedeihen sichern. Der chinesische Erdgott Tudi Gong wird in vielen Dörfern Ostasiens verehrt. In Indien haben die meisten traditionellen Dörfer ihre eigenen Götter, häufig weibliche Gottheiten, "(Gramadevata)", die in Festen als Dorfgründer und Beschützer, aber auch als gelegentliche Ursache von Krankheiten und Katastrophen gedacht werden.

Während einige Götter Krankheit und Tod bringen, heilen andere Kranke und beschützen die Toten, und andere Götter wiederum vereinen diese beiden Funktionen. Bekannt ist der griechische Gott Asklepios der Medizin und Heilkunst. In China wurde der Arzt Baosheng Dadi nach seinem Tod zum Gott der Medizin erhoben. Zu den Göttern, die Krankheiten herbeiführen, zählen Pakoro Kamui bei den Ainu sowie Lugal-Irra und Namtar in Mesopotamien. Letzterem wurde nachgesagt, 60 verschiedene Krankheiten verursachen zu können. In den Veden bringt Rudra oft Krankheit und Verwüstung, wird aber auch als Heiler verehrt. Die Eigenschaften, die Totengöttern zugeschrieben werden, hängen von den religiös-kulturellen Vorstellungen des Geschehens nach dem Tode ab. Der ägyptische Gott Hathor behütet die Toten, und im Hinduismus richtet Yama über die Toten.
Die Götter, die in Verbindung mit dem kulturellen Leben stehen, sind recht vielfältig. In mehreren Religionen gilt die Kultur als gottgegeben; Dichter, Maler, Bildhauer und Tänzer wurden durch Götter zu kreativen Leistungen inspiriert. Im Hinduismus ist laut dem Ramayana Rama der Überbringer der Kultur. Sarasvati, die Göttin des Lernens, der Kunst und Musik, wird sehr oft in Schulfeiern verehrt, und Shiva trägt den Beinamen „König des Tanzes“. In Ägypten war Thot der Erfinder aller Künste und Wissenschaften, von der Arithmetik bis zur Hieroglyphenschrift.

Für fast jeden Beruf und jedes Handwerk gibt es einen Gott. Njörðr war in der nordischen Religion der Schutzherr der Schiffsbauer und Seeleute. In Griechenland wurden Herakles und Hermes vor allem mit dem Handel, Athene mit Handwerkerinnen, und Hephaistos mit der Schmiedekunst in Verbindung gebracht. Bei den Yoruba sorgt Ogún für Wohlstand bei all jenen, die im Beruf mit Metall in Berührung kommen, zum Beispiel Goldschmiede, Barbiere, Mechaniker und Taxifahrer.

Göttern wird mit anthropomorphen Begriffen oft eine spezifische Persönlichkeit zugeschrieben, die gütige und zornige Eigenschaften einschließt. Sehr grausam sind die Muttergöttinnen der Azteken wie etwa Coatlicue, die mit einer Bluse aus menschlichen Händen und Herzen dargestellt wird. Sie gebar den Kriegsgott Huitzilopochtli, der seine vierhundert Geschwister tötete. JHWH wird in der Tora sowohl milde als auch grimmig dargestellt. In Indien besitzen die wichtigsten Götter eine „sanftmütige“ und eine „furchtbare“ Form. Obwohl Kali für Tod und Verwüstung steht und ihre Kinder frisst, wird sie von vielen Hindus als liebevolle Mutter verehrt. Die Göttin Hina der Hawaiier ist ein weiteres Beispiel für einen Gott, der das Gedeihen fördert, aber auch Tod und Verwüstung über die Menschen bringt. Vor der christlichen Missionierung glaubten die Kikuyu, dass ihr Gott ein Gott der Liebe sei, er aber jene, die ihm nicht gehorchen, mit Hunger, Krankheit und Tod bestraft.

Andere Götter werden als vollkommen gütig betrachtet. Für Platon war Gott das moralisch Beste und Vollkommene, und für manche christliche Theologen ist Gott allgütig. Im Gegensatz dazu waren die Götter des griechischen Pantheons für ihr oftmals unmoralisches Tun bekannt. Das Volk der Chagga kennt den Schöpfergott Ruwa, der zugleich Hüter der Moral ist. Dieser Gott ist allgütig, sodass die Menschen keine Angst vor ihm haben müssen; gefürchtet sind allein die Totengeister. Der Gott Buga der Ewenken sitzt auf einem weißen Marmorthron und herrscht über alle Dinge, tut aber nur Gutes und bestraft nicht.

Götter können nicht nur mit Anthropomorphismen beschrieben werden, sondern auch ein unverblümt menschliches oder menschenartiges Wesen besitzen. Hierzu zählen Halbgötter wie Perseus in der griechischen Mythologie oder Māui in der Religion der Maori. Diese Halbgötter sind gegenüber echten Göttern meist in ihrer Macht eingeschränkt. Ein Beispiel für einen Menschen, der zum Kriegsgott erklärt wurde, ist der chinesische General Guan Yu. Das chinesische Mädchen Mazu wurde als Göttin in den Himmel aufgenommen und wird seitdem als „Himmelskönigin“ und Schützerin der Seeleute verehrt. Umgekehrt können einige Götter in menschlicher Form erscheinen, so etwa Jesus im christlichen Dogma der Menschwerdung sowie die Avatara von Vishnu. Die Apotheose ist die Vergöttlichung eines als heldenhaft angesehenen Menschen, der als Gottkönig verehrt wird. Beispiele dafür sind Alexander der Große und Gaius Iulius Caesar, der im Römischen Reich als Divus Iulius verehrt wurde.

Die übernatürlichen Eigenschaften, die Göttern zugesprochen werden, variieren. Einige Götter sind allwissend, allmächtig und allgegenwärtig, während andere nur beschränkten Wissenszugang besitzen oder nur in bestimmter Hinsicht mächtig sind. In der Philosophie der Antike sind systematische Betrachtungen zu Gott oder den Göttern häufig anzutreffen. Auch in der hinduistischen Philosophie, der Theologie der abrahamitischen Religionen und der modernen abendländischen Philosophie finden sich rationale Überlegungen zu den metaphysischen Eigenschaften des Göttlichen. Nicht immer wird dabei das Wort „Gott“ verwendet. Verschiedene griechische Philosophen sprachen von „dem Einen“, und Georg Wilhelm Friedrich Hegel verwendete Synonyme wie das „unendliche Leben“, „das Absolute“, der „Begriff“, die „Idee“, der „absolute Geist“ oder die „einzige absolute Wirklichkeit“.

Ein tendenziell abstraktes Gottesbild entsteht aus dem Anspruch der Desillusionierung mythologisch-religiöser Gottesvorstellungen durch rationale Erwägungen. Zwar unterscheidet sich ein derartiger, in Blaise Pascals "Mémorial" so genannter „Gott der Philosophen und Gelehrten“ in mancherlei Hinsicht von einem Gott der Mythologie und Offenbarung, häufig gehen Philosophen und Theologen aber davon aus, dass es sich bei beiden lediglich um unterschiedliche Beschreibungen derselben Realität handelt.

Je nach metaphysischer Weltanschauung wird das Verhältnis zwischen den Göttern und der Welt unterschiedlich dargestellt. In einigen Vorstellungen sind Gott oder die Götter völlig von der Welt getrennt, in anderen schließt ein Gott die Welt teilweise oder ganz ein.

Der Theismus kann zunächst – so etwa bei Richard Swinburne oder John Leslie Mackie – als Gegensatz zum Atheismus, dem Nichtglauben an Götter, betrachtet werden. Hier bezeichnet der Begriff jegliche Weltanschauung, die die Existenz einer göttlichen Instanz annimmt. Im engeren Sinne bezeichnet klassischer Theismus den Glauben an einen oder mehrere Götter, die mit der Welt nicht identisch sind, diese aber lenken und in sie eingreifen, und die eventuell auch ewig und unveränderlich sind.

Das Wort „Deismus“ hat die gleiche Wortherkunft wie „Theismus“, wurde aber bereits bei seiner ersten bekannten Verwendung in der zweiten Hälfte des 16. Jahrhunderts mit abweichender Bedeutung verwendet. Von verschiedenen Denkern wurde die Bezeichnung unterschiedlich verwendet, sie hatte aber in jedem Fall einen unorthodoxen Beiklang, der sich von der etablierten Religion abgrenzte. Deisten vertraten im Allgemeinen einen undogmatischen Monotheismus und wiesen übernatürliche Offenbarungen zurück. Der Deismus hatte seine Blütezeit während der Aufklärung und war besonders im angloamerikanischen Raum verbreitet, wo Anthony Collins und Thomas Paine als bekannte Verfechter hervortraten. Im späten 18. und frühen 19. Jahrhundert etablierte sich eine weitere Bedeutung von Deismus als Glaube an einen Gott, der sich nach der Schöpfung zurückgezogen hat und seitdem nicht mehr in die Welt eingreift.

Gemäß dem Emanationismus ist alles aus einem Urprinzip (Gott) durch Emanation, einen Prozess ähnlich dem Ausfließen oder Ausstrahlen, hervorgegangen. Mit zunehmender Emanation werden die Produkte immer weniger perfekt; die transzendente Quelle – von Plotin „das Eine“ genannt – bleibt davon unberührt. Der Emanationismus findet sich in gnostischen Lehren, wie zum Beispiel in der "Pistis Sophia" und einigen Schriften des Valentinus. Die kabbalistische Philosophie, Theosophie und der Bahaismus wurden ebenfalls durch den Emanationismus beeinflusst. Im Gegensatz zum Pantheismus ist das göttliche Urprinzip transzendent und nicht immanent. Einige Philosophen betrachten den Emanationismus als Form des Panentheismus.

Das Wort „Panentheismus“ wurde 1828 von Karl Christian Friedrich Krause geprägt. Nach panentheistischer Auffassung ist die Welt Teil eines einzigen Gottes, aber nicht mit diesem identisch. Der Panentheismus stellt insofern einen Mittelweg zwischen klassischem Theismus und Pantheismus dar, als er einerseits einen Gott mit Verstand und Willen annimmt, andererseits aber die enge Verbindung zwischen Gott und dem Universum herausstellt. Für Gustav Theodor Fechner beispielsweise gehörte die Welt zu Gott, ähnlich wie der Körper nur ein Teil des Menschen ist, wobei dessen Geist den anderen Teil darstellt. Die Prozesstheologie vertritt ebenfalls eine panentheistische Sicht. Der Begriff lässt sich auch weiter fassen; in diesem Sinn kann unterschieden werden zwischen individuellem Panentheismus („Gott existiert in meinem tiefsten Inneren“), ontologischem Panentheismus („Gott ist der Grund aller Existenz“), sozialem Panentheismus („Gott existiert in unserer Beziehung zu anderen Menschen“) und kosmischem Panentheismus („Gott findet sich in der Natur oder in der Schönheit“).

Der erst im frühen 18. Jahrhundert so genannte Pantheismus bezeichnet die Auffassung, dass alles Existierende göttlich ist. Pantheisten wenden sich gegen die Auffassung, dass Gott und das Universum verschiedene Dinge sind. Eine Denkrichtung, die dem Pantheismus zumindest nahekommt, ist der Advaita-Vedanta. Diese von Shankara vertretene Philosophie beschreibt Brahman (nicht zu verwechseln mit dem Schöpfergott Brahma) als allwissend, allmächtig und Grund alles Seienden. Für Shankara gab es letztendlich keinen Unterschied zwischen der Welt, der individuellen Seele (Atman), und der „Weltseele“ Brahman. Im 16. Jahrhundert stellte Giordano Bruno die These auf, dass Gott sich in allen Dingen manifestiere, die ein ineinander greifendes Ganzes bilden. Für Baruch Spinoza gab es nur eine einheitliche Substanz, nämlich Gott. Paul Harrison, der Begründer des World Pantheist Movement, unterscheidet zwischen wissenschaftlichem, idealistischem und dualistischem Pantheismus; letzterer behauptet die Existenz eines immateriellen Geistes.

Der religiöse oder spirituelle Naturalismus – ein spätestens seit den 1940er Jahren in der US-amerikanischen Theologie anzutreffender Begriff – geht davon aus, dass alles Existierende im Prinzip naturwissenschaftlich erklärbar ist. Zugleich wird eine religiöse Haltung gegenüber der Welt oder Teilen der Welt eingenommen, ohne eine höhere, ontologisch getrennte Realität anzunehmen. Sofern das Objekt der religiösen Orientierung als Gott bezeichnet wird, kann diese Haltung als „naturalistischer Theismus“ bezeichnet werden. Gott ist hier entweder der kreative Prozess innerhalb des Universums (so bei Shailer Mathews und Henry Nelson Wieman) oder die Gesamtheit des Universums. Zumindest der „wissenschaftliche“ Pantheismus ist demnach eine Form des naturalistischen Theismus.

Der jüdisch-christliche Gott wird von den meisten Theologen als transzendent betrachtet, das heißt, er ist „außerhalb“ der Welt, die er erschaffen hat. Gleichzeitig ist er bis zu einem gewissen Grad auch immanent, also Teil der Welt – zum Beispiel durch seine Anwesenheit in den religiösen Gefühlen Gläubiger. Auch im Hinduismus wurde Gott gelegentlich als transzendent beschrieben, so etwa vom Hymnendichter Nammalvar. Ramanuja schrieb einerseits, dass Gott den Menschen nicht durch Meditation oder Gebete zugänglich sei, sich aber andererseits denen, die ihn anbeten, in menschlicher Form gezeigt habe. Im Islam gilt Gott sowohl als transzendent als auch als immanent. Bei den Lugbara, einem in der Grenzregion Uganda/Demokratische Republik Kongo ansässigen Volk, wird eine transzendente "(Adroa)" und eine immanente "(Adro)" Form von Gott unterschieden. In seiner immanenten Form lebt er zeitweise auf der Erde in Flüssen, Bäumen, Dickichten und Bergen.

Die Vorstellung eines allwissenden (omniszienten) Gottes ist in vielen Kulturen verbreitet und spätestens im 6. Jahrhundert v. Chr. bei Xenophanes nachweisbar. Die großen monotheistischen Religionen vertreten eine omnisziente Gottesvorstellung; bereits im Tanach wird JHWH als allwissend beschrieben. Im Hinduismus gilt Varuna als allwissend. Die meisten der allwissenden Götter sind Himmelsgötter, so etwa Tororut bei den Pokot in Kenia, Ngai bei den Massai oder Tengri bei den Altaiern. Meist sind es böse Taten, die die Aufmerksamkeit der allwissenden Götter auf sich ziehen.

Das Konzept der Allmacht (Omnipotenz) wird von allen abrahamitischen Religionen vertreten, findet sich aber häufig auch außerhalb, wie etwa bei Alhou, dem höchsten Wesen der Sema-Naga, oder beim Gott Karai Kasang der Jingpo. Bei den Azteken war Tezcatlipoca allmächtig „in Erde und Himmel“. In jedem Fall werden Götter häufig als mächtig dargestellt, und göttliche Beinamen wie „der Allmächtige“ sind weit verbreitet. Manche Völker bringen die göttliche Macht vor allem mit der Natur, andere eher mit menschlichen Belangen in Verbindung. Andererseits wurde der kanaanäische Gott El manchmal alt und kraftlos dargestellt, als er durch Baal ersetzt wurde. Zu beobachten ist in verschiedenen Kulturen eine Tendenz zur Vereinigung lokaler Götter zu großen Göttern, die alle vorherigen Machtattribute übernehmen.

Die Allgegenwart (Ubiquität) ist ebenfalls eine weitverbreitete Eigenschaft von Göttern. Unter den antiken Philosophen wurde sie von Sokrates und Epiktet vertreten. Amun, der ägyptische Wind- und Fruchtbarkeitsgott, wurde als „der, der allen Dingen innewohnt“ bezeichnet. Die Bena in Tansania glauben, dass ihr Gott „überall gleichzeitig ist“. Oft kombinieren Götter Allwissen und Allgegenwart; so etwa wurde in Flores, Indonesien, vom Gott Dua Nggae behauptet, er sehe alles, wisse alles und sei überall. In manchen Völkern werden Götter, wenn auch allgegenwärtig, mit bestimmten Plätzen in Verbindung gebracht. Die Langi glauben zum Beispiel, dass Hügel mit Gott in Verbindung stehen, und dass es deshalb gefährlich ist, in deren Nähe Häuser zu bauen. Im antiken Griechenland weilten die wichtigsten Götter im Himmel oder auf dem Olymp.

In der westlichen Philosophie und Theologie wurde Gott fast immer als persönliches Wesen betrachtet, so etwa bei Platon, Aristoteles und Spinoza. Einige Philosophen wie Hegel sahen in persönlichen Beschreibungen Gottes eine unvollkommene Vorstellung des Absoluten. Auch hinduistische Texte wie die Bhagavad Gita beschreiben Gott als persönliches Wesen, während Shankara unpersönliche Auffassungen des Brahman vertrat.

Philosophen, die die Welt als Teil Gottes oder als Verkörperung seines Wesens sehen, halten Gott zumindest teilweise für materiell. Eine solche Auffassung wurde beispielsweise von den Stoikern vertreten, die ihn mit den Grundelementen Luft und Feuer gleichsetzten. Auch der christliche Schriftsteller Tertullian nannte Gott noch „corporalis“. Von platonisch beeinflussten Denkern wurde hingegen gelehrt, dass ein materielles Wesen Gottes Perfektion widersprechen würde. Auch afrikanische Völker halten Gott im Allgemeinen für ein körperloses, immaterielles Geistwesen, wenngleich er in anthropomorphen Metaphern beschrieben wird.

Als übernatürliche Geistwesen sind Götter zumindest zeitweise unsichtbar. In einigen Völkern gilt Gott als unsichtbar, während seine Wirkungen physisch spürbar sind, zum Beispiel als Wind. Andere Kulturen halten natürliche Phänomene und Objekte – den Himmel, Gestirne oder Donner – für Erscheinungsformen von Göttern. Einige Götter sind jedoch zum Teil sichtbar. In dem Tora-Bericht des brennenden Dornbuschs verhüllt sich Mose das Gesicht aus Furcht, Gott anzusehen. Der Himmelsgott der San ist üblicherweise unsichtbar, zieht aber manchmal mit hellem Licht vorüber, und seine Stimme ist als Donner zu vernehmen.

Die christliche Theologie unterscheidet drei Arten, etwas über Gott zu erfahren: Vernunft, Offenbarung und religiöse Erfahrung. In der natürlichen Theologie wird versucht, durch Vernunft und Beobachtung Aussagen über Gott zu treffen. Im Allgemeinen werden jedoch Götter zumindest teilweise als unergründlich betrachtet. Die Alur halten ihren Gott für „praktisch unerkennbar“, und die Lugbara geben zu, nicht viel über das Wesen ihres Gottes zu wissen, da er sich der menschlichen Vorstellung entzieht. Ähnliches wird auch im Islam behauptet: Der Mensch als begrenztes Wesen kann Gott, der frei von „Grenzen und Dimensionen“ ist, nicht wie andere Dinge begreifen. Søren Kierkegaard ging soweit, Gott als das Unergründliche zu definieren.

In vielen Völkern finden sich Beinamen für Götter wie „der Immerwährende“, „der Ewige“ oder „der, der immer da ist“; oft wird gleichzeitig die Unveränderlichkeit betont. Christliche Denker wie Boëthius, die Gott als perfektes Wesen betrachteten, waren zugleich von dessen Ewigkeit überzeugt. Dass Gottes Natur unveränderlich ist, wurde von Platon, wie auch jüdischen und christlichen Theologen behauptet, insbesondere von Augustinus von Hippo. Im Gegensatz dazu steht ein Gott, der in die Zeit eingebunden ist und mit seinen Geschöpfen interagiert. Ein solches „relationales“ Gottesbild wird zum Beispiel von Richard Swinburne und William Lane Craig vertreten. Im alten Ägypten konnten Götter sterben; so etwa wurde Osiris von seinem Widersacher Seth ermordet. Dies bedeutete aber wegen der kosmologischen Lehre von der zyklischen Wiederkehr nicht unbedingt die Beendigung der Existenz. Radikale Vertreter der Gott-ist-tot-Theologie der 1960er Jahre waren der Auffassung, dass Gott buchstäblich gestorben sei.

Charles Hartshorne und William Reese schlugen eine Klassifikation von Vorstellungen des „Höchsten“ nach metaphysischen Attributen vor. Sie identifizierten folgende fünf grundsätzliche Eigenschaften, die in verschiedenen Vorstellungen auftreten:

Aus der Kombination dieser Eigenschaften ergibt sich nach Hartshorne und Reese folgende Einteilung:

Bei der Darstellung von Göttern kann zunächst grob unterschieden werden zwischen Buchreligionen, die eine kanonisierte Heilige Schrift kennen, Kultreligionen, die von vor dem Bild des Gottes ausgeführten Kulthandlungen bestimmt werden, und „mystischen“ Religionen, die Wort und Bild letztlich als unangemessene Form der Aussage über das Göttliche betrachten.

Zwar besaßen die Ägypter der Antike zahlreiche heilige Schriften, fassten diese aber nicht zu einer kanonischen Norm zusammen. Die Götter erschienen eher in ihrem Bild als in ihrem Wort, weshalb die ägyptische Religion zu den Kultreligionen gezählt wird. Auch im antiken Griechenland spielte die Schrift neben der Bilderverehrung eine untergeordnete Rolle. Im Judentum hingegen offenbart sich Gott im Wort; bildliche Darstellungen werden daher verworfen. Ähnliches gilt für den Zoroastrismus. Im Christentum kam es über die Frage der Ikonenverehrung zum byzantinischen Bilderstreit. Auch wenn das Bilderverbot im Christentum oft nicht eingehalten wurde, lehnt die Theologie anthropomorphe Beschreibungen grundsätzlich ab, da Gott nicht auf eine Stufe mit profanen menschlichen Zügen gestellt werden soll. Das Bilderverbot im Islam wird vergleichsweise konsequent beachtet, weshalb einzig die Kalligrafie als schmückendes Element hervortritt.

In einigen Kultreligionen wurden Götter als tierähnliche Wesen dargestellt, so etwa in Ägypten und in den meso- und südamerikanischen Hochkulturen. Diese Bildnisse bedeuten nicht, dass man sich die angebeteten Götter genauso vorstellte. Vielmehr sollten sie die Andersartigkeit des nicht darstellbaren bekunden. Auch Darstellungen von Göttern mit spezifischen Attributen, wie zum Beispiel Sonnengöttern, sind nicht als Erscheinungsformen jener Götter zu deuten, sondern sollen lediglich wesentliche Aspekte bildlich zum Ausdruck bringen.

Mythologische Götter werden häufig in Filmen dargestellt (z. B. Thor). Der einzige Gott im Sinne monotheistischer Religionen wird nur selten in Filmen dargestellt. Beispiele sind

In der sumerischen Religion wurde das Numinose als unsichtbare Kraft oder „Élan vital“ betrachtet, die den Dingen innewohnt. Die sumerische Sprache bezeichnet beispielsweise mit "Nanna" sowohl den Mond als auch die in ihm verborgene Kraft, den Mondgott. Eine ähnliche Gleichsetzung von Objekt und Gott findet sich im Gilgamesch-Epos. Im vierten Jahrtausend v. Chr. wurden vor allem die Kräfte der Natur verehrt, besonders jene, die für das menschliche Überleben wichtig waren. Aus dem menschlichen Bedürfnis, mit den Göttern eine sinnvolle Verbindung aufzubauen, wurden anthropomorphe Götter bevorzugt. Die vorherrschende Form war die des Sohnes und Versorgers, dessen Lebensgeschichte den jährlichen Erntezyklus widerspiegelte, zum Beispiel Dumuzi.

Inmitten der kriegsähnlichen Zustände zu Beginn des dritten Jahrtausends entwickelte sich die Vorstellung eines mächtigen göttlichen Herrschers und Kriegers. In den überlieferten Gebeten Gudeas zu Ningirsu, dem Hauptgott von Lagaš, wird dieser mit „Meister“, „Herr“ und „Krieger“ angeredet. Die neue Rolle der Götter als Beschützer und Militärchefs erforderte es, ihren Willen zu ergründen. Dies konnte in Traumvisionen oder durch Wahrsagen geschehen. Die Götter wurden außerdem als Verwalter ihres Gutes betrachtet. Anstatt alleine zu agieren, wurden sie durch höhere Götter oder durch die Götterversammlung mit besonderen Aufgaben betreut. Die Götterversammlung hatte im Wesentlichen die Aufgabe, über Übeltäter zu richten und hochrangige Amtsträger zu ernennen oder abzusetzen, und zwar sowohl Menschen als auch Götter. In dieser Beziehung wurden die Götter recht menschlich dargestellt; so etwa stärkten sie sich vor der Versammlung durch Speisen und Getränke.

Im zweiten Jahrtausend entwickelte sich zunehmend eine „persönliche“ Religion, in der Gott sich um den Anbeter sorgt. Zum einen legte der Gläubige sein Vertrauen in das Mitgefühl des Gottes, zum anderen erwartete er Bestrafung für Sünden. Persönliches Glück wurde oft mit göttlicher Belohnung in Verbindung gebracht; in der akkadischen Sprache lautete die Bezeichnung für „Glück haben“ wörtlich übersetzt „einen Gott bekommen“. Deutlich wird die bescheidene Haltung und Selbsterniedrigung in den überlieferten Bußpsalmen und „Briefen zu Gott“. Die Vorstellung eines persönlichen Gottes beeinflusste ebenfalls die ägyptische Religion zu dieser Zeit und später die israelitische Religion.

Der babylonische Schöpfungsmythos Enûma elîsch nennt etwa 300 Götter des Himmels und 300 Götter der Unterwelt. In Rykle Borgers assyrisch-babylonischer Zeichenliste lassen sich etwa 130 Götternamen belegen, wobei einige Beinamen oder Erscheinungsformen anderer Götter sind, und rund 25 als große Götter gelten können.

Wie auch andere prähistorische Völker scheinen die Ägypter ihre Ehrfurcht vor den Mächten der natürlichen Welt bekundet zu haben. Archäologische Funde deuten auf Götter in Tiergestalt wie Kühe oder Falken hin, die Aspekte des Kosmos repräsentierten. Zu Beginn der historischen Zeit gab es Götter wie Min und Neith, die in menschlicher Gestalt verehrt wurden. Das ägyptische Wort "netjer" umfasste sowohl als Götter verehrte Menschen als auch Geister und Dämonen, und selbst die Hieroglyphen wurden manchmal als Götter bezeichnet.

Ägypten entwickelte mehrere Schöpfungsmythen, die nie zu einem Mythos vereinheitlicht wurden, aber einige gemeinsame Züge aufweisen. Laut der Achtheit von Hermopolis wurde die Welt durch vier Götterpaare erschaffen, die männliche und weibliche Aspekte des vorweltlichen Zustands verkörperten (Urgewässer, Endlosigkeit, Finsternis, Unsichtbarkeit). Ein anderer Mythos, die Neunheit von Heliopolis, beschreibt den Sonnengott Atum als Allerzeuger und Vater der Götter, aus dessen Körperflüssigkeiten weitere Götter hervorgingen. Nach der memphitischen Theologie erschuf der androgyne Gott der Metallarbeiter, Handwerker und Baumeister, Ptah, Atum und alle anderen Götter durch „Herz und Zunge“. Dies ist die früheste bekannte Variante der Logos-Vorstellung, in der die Welt durch kreative Rede eines Gottes Gestalt annimmt.

Die Charaktereigenschaften der Götter waren sehr unterschiedlich. Einige Götter waren besonders hilfreich für den Menschen, wie beispielsweise Thot, Horus und Isis wegen ihrer heilenden Kräfte, während andere der Menschheit feindlich gesinnt waren. Andere Götter wiederum wiesen ambivalente Züge auf; Hathor etwa wurde als Göttin der Liebe, Musik und Feier verehrt, galt aber auch als rasende Zerstörerin der Menschheit. Viele Kulte der Hauptgötter bildeten mit der Zeit Familientriaden aus Vater, Mutter und Sohn, wie zum Beispiel Amun, Mut und Chons in Theben. Daneben bildeten sich Gruppen aus vier, fünf oder mehr Göttern, ohne dass ein klares Schema ersichtlich ist. Insbesondere während des Neuen Reiches war die persönliche Frömmigkeit weitverbreitet. Erhaltene Bittschriften zeugen davon, dass Götter menschliche Sünden vergeben konnten.

Viele Götter änderten im Laufe der Zeit ihre regionalen Zugehörigkeiten, während andere zu regionalen oder landesweiten Göttern aufstiegen und umgekehrt. Auch der Charakter von Göttern konnte sich ändern; so etwa war Seths Natur, Beliebtheit und Bedeutung starken Schwankungen ausgesetzt. Osiris übernahm im Laufe der Zeit viele Beinamen und Eigenschaften anderer Götter. Eine ägyptische Besonderheit bestand in der Kombination unterschiedlicher Götter, indem man ihre Namen verknüpfte (beispielsweise Atum-Chepre und Amun-Re) und ihre Gestalt neu zusammensetzte. Aus der altägyptischen Zeit sind 1500 Götter namentlich bekannt, wobei nur von einer kleineren Zahl Genaueres bekannt ist. Isis gehörte zu den letzten ägyptischen Göttern, die überdauerten; noch aus dem Jahr 452 n. Chr. ist überliefert, dass Pilger im Tempel von Philae ihre Statue besuchten.

Die ältesten hinduistischen Schriften, die Veden, reichen bis in die Mitte des 1. Jahrtausends v. Chr. zurück. Ein wichtiger Begriff der hinduistischen Philosophie ist Brahman, eine nicht wahrnehmbare Abstraktion. Brahman lässt sich nicht definieren; es ist nach einem Ausspruch der Brihadaranyaka-Upanishad "neti neti" (nicht so, nicht so!). Die Götter, Ishvara und die Devas sind demnach symbolische Entitäten, die aus dem Brahman hervorgingen und die die leitenden Kräfte der Welt repräsentieren. Nach dem Brihadaranyaka-Upanishad ist der Lebenshauch (Prana) die Seele der Götter und einziges höchstes Wesen.

Im Brihadaranyaka-Upanishad ist von 33 Göttern die Rede: acht Existenzsphären (Vasus), elf Lebensprinzipien (Rudras), zwölf Herrscherprinzipien (Adityas), einem Himmelsherrscher (Indra) und einem Erzeuger (Prajapati), die jeweils in verschiedenen Entwicklungssphären (Mahiman) auftreten. Diese Zahlen variieren jedoch je nach Text. Indra wird als allgegenwärtig beschrieben, und er ist in der Lage, jede beliebige Form anzunehmen. Laut dem Avyakta-Upanishad verkörpert er die Eigenschaften aller Götter und ist daher der wichtigste unter ihnen. Die Adityas personifizieren die Gesetze, die das Universum und die menschliche Gesellschaft beherrschen. Zu ihnen gehören Mitra (die Freundschaft), Aryaman (die Ehre) oder Varuna (der Verbindende). Hinzu kommen untergeordnete Götter wie die Söhne von Shiva, darunter Ganapati. Daneben werden weitere Götter beschrieben, wie die Ashvins, die Yakshas oder der Totengott Yama. Die Götter der Veden bilden nur einen kleinen Teil des hinduistischen Pantheons, und viele werden heute nicht mehr verehrt.

Die Trimurti aus Brahma, Vishnu und Shiva repräsentiert die drei kosmischen Funktionen des Universums. Vishnu kann in beliebigen Avataras erscheinen. Shiva ging vermutlich aus seiner vedischen Entsprechung Rudra hervor. Während Rudra als aggressiv, aktiv und zerstörerisch beschrieben wurde, gilt Shiva auch als friedlich. Dennoch ist sein Charakter ambivalent; er besitzt furchtbare und sanftmütige Formen. Brahma ist die personifizierte, männliche Form des Brahman. Er gilt als erste Ursache alles Seienden, und wird in verschiedenen Schöpfungsmythen beschrieben. Jeder der männlichen Götter besitzt eine weibliche Form, die Shakti, die dessen Kraft und Macht ist und durch die er handelt. Die Götter werden also als sowohl männlich als auch weiblich angesehen.

Üblicherweise verehren Gläubige einen bevorzugten Gott, ohne dessen aus dem Brahman hervorgehende Wesensart zu leugnen. Aus hinduistischer Sicht ist der Monotheismus nur die Verherrlichung eines bevorzugten Gottes; in der Bhagavad Gita erklärt Krishna, die Verehrung anderer Götter sei nur die Verehrung seiner selbst. Oft wurde versucht, Entsprechungen zu Göttern anderer Religionen und Glaubensrichtungen herzustellen; so wurde der vedische Rudra mit dem dravidischen Shiva, dem griechischen Dionysos und dem ägyptischen Osiris identifiziert. Einigen Hindus, die mit der christlichen Religion vertraut sind, gilt Jesus als Avatar Vishnus, denn Vishnu wird nicht als persönlicher Gott einer bestimmten Religion, sondern als universelles Prinzip betrachtet. Die Bezeichnung „Hinduismus“ entstand erst spät und fasst recht unterschiedliche Kulte zusammen. Weit verbreitet ist heute die Verehrung Shivas und Vishnus; im Shivaismus bzw. Vishnuismus werden sie als Hauptgötter oder höchstes Brahman angenommen. Hinzu kommt der Shaktismus, der Shakti, Devi oder eine der vielen anderen Göttinnen als Hauptgöttin verehrt.

Neben dem Hinduismus liegt in Indien der Jainismus als atheistische Religion vor und der Sikhismus als monotheistische Religion.

Der chinesische Daoismus gilt in seiner Frühform als atheistisch, später entwickelte er jedoch ein großes, polytheistisches Pantheon; siehe dazu Pantheon des Daoismus.

Vor allem in älterer westlicher Literatur und oft auch heute wird die Meinung vertreten, dass der im Pali-Kanon beschriebene „ursprüngliche“ Buddhismus des historischen Buddha, Siddhartha Gautama, eine atheistische „Lebensphilosophie“ und keine Religion sei. Dies ist bestenfalls eine grobe Vereinfachung, die nicht der religiösen Praxis in allen buddhistisch geprägten Ländern entspricht.

Nach dem Anguttara-Nikaya antwortete Siddhartha Gautama auf die Frage, ob er ein Mensch oder ein Gott (Deva) sei, dass er kein Gott, Gandharva oder Mensch sei, sondern ein Buddha. In Mahayana-Texten wird der Dharma-Körper (Dharma-kāya) eines Buddhas mit der absoluten Realität gleichgesetzt, die bis an die Grenzen der Welt reicht und alles durchdringt. Der Dharma-Körper ist auch insofern omniszient, als die gesamte Welt sich direkt in seinem Geist widerspiegelt. Der Manifestationskörper (Nirmāṇa-kāya) des Buddhas kann in jeder Form erscheinen; seine Handlungen sind allerdings keine Folge willentlicher Entscheidungen. Nach der formellen Lehre des Theravada-Buddhismus ist der Buddha tot und greift nicht mehr in die Welt ein; dennoch wird er wie ein Gott verehrt und auch von manchen Gläubigen angebetet. Obwohl sich Buddhas und Bodhisattvas in mancher Hinsicht von Göttern unterscheiden, werden sie zum Teil dennoch zu den göttlichen Wesen gezählt.

Die im frühen Buddhismus formulierte Lehre des bedingten Entstehens postuliert Nichtwissen als Ursache der Kette der Wiedergeburten. Einer Interpretation zufolge handelt es sich dabei um eine Kritik des brahmanischen Schöpfungsmythos des Rigveda. Insofern entwickelte der Buddhismus eine nichtteleologische Kausalitätslehre, die ohne einen Schöpfergott auskommt. Das Lebensrad, das im Mahayana-Buddhismus die sechs Daseinsbereiche beschreibt, enthält den Bereich der Götter (Devas) und den Bereich der „eifersüchtigen Götter“ (Asuras), die im Theravada zu den Devas gezählt werden. Buddhistische Gläubige beten viele der Hindu-Götter an, was insofern keinen Synkretismus darstellt, als diese Götter von Anfang an Teil des Buddhismus waren. Ihre Existenz wurde nie bestritten, wenngleich sie im Buddhismus als entbehrlich gelten. Zwischen dem Glauben an Buddha und an die Götter besteht insofern ein Gleichgewicht, als Götter in weltlichen Angelegenheiten helfen können, aber nur der Buddha den Weg zur Erlösung zeigen kann.

Da die Topografie des antiken Griechenlands Kommunikation über Land- und Seewege erschwerte und es sprachliche und ethnische Unterschiede gab, variierten die mythologischen Inhalte und Kulte. Homers Werke "Ilias" und "Odyssee" führten zu einer teilweisen Stabilisierung dieser Mythen und übten auf nachfolgende griechische und römische Autoren einen wesentlichen Einfluss aus. Die Griechen und Römer kannten zahlreiche Schöpfungsmythen, die viele Parallelen zu den Mythen der Ägypter, Sumerer, Babylonier und Hebräer aufweisen. Laut Homer waren die Titanen Okeanos und Tethys für den Ursprung der Götter verantwortlich. Okeanos repräsentierte dabei den ringförmigen Ozean, der die scheibenförmige Erde umschloss. Hesiod gab in seiner "Theogonie" (um 700 v. Chr.) die erste bekannte vollständige Beschreibung der Schöpfung. Aus dem Chaos entstand Gaia, die Uranos hervorbrachte. Beide zeugten sechs weibliche und sechs männliche Kinder, die Titanen, die ebenfalls Kinder hatten. Die Titanen waren im Wesentlichen Personifikationen verschiedener Aspekte der Natur. Nach dem Sturz der Titanen traten Zeus und die anderen Götter des Olymp die Weltherrschaft an.

Die Götter bildeten ein hierarchisch organisiertes Pantheon. Im Allgemeinen galten sie als menschenähnlich und fühlend, wenngleich ihr Aussehen und ihre Handlungen bis zu einem bestimmten Grad idealisiert wurden. Andererseits konnten sie die körperlichen und geistigen Schwächen der Menschen widerspiegeln. Die Götter lebten in Häusern auf dem Olymp oder im Himmel; ein wichtiger Unterschied bestand jedoch zwischen den Göttern der Luft und der Oberwelt, und den chthonischen Göttern, die in der Erdtiefe walten. Götter konnten sich mit großer Geschwindigkeit fortbewegen, plötzlich verschwinden und erscheinen, und beliebige Formen annehmen – menschlich, tierisch und göttlich. Obwohl ihre Macht größer war als die der Menschen, waren sie kaum allmächtig, außer womöglich Zeus, und selbst seine Handlungen waren dem Schicksal unterworfen. Die Eigenschaft, die die griechischen Götter am offensichtlichsten von den Menschen abhob, war ihre Unsterblichkeit.

Wenngleich einige Götter nur in bestimmten Plätzen besonders verehrt wurden – so etwa Athene in Athen und Hera in Argos – wurden die wichtigsten Götter in der gesamten griechischen Welt anerkannt. An der Spitze stand Zeus, der Vater aller Götter und Menschen. Er verteidigte, teils zusammen mit anderen Göttern, die höchsten moralischen Werte, und beschützte die Familie und den Staat. Zeus konnte als Gott ohne Namensnennung genannt werden. Xenophanes griff die üblichen anthropomorphen Vorstellungen scharf an und behauptete, dass es einen einzigen nichtanthropomorphen Gott gab.

Die Römische Religion hatte ihre Wurzeln in den religiösen Vorstellungen vorrömischer italienischer Völker wie den Sabinern und den Etruskern. Im Allgemeinen wiesen die römischen Götter, die ursprünglich eher im Kult als im Mythos verankert waren, weniger anthropomorphe Züge auf als die griechischen Götter. Als im 3. Jahrhundert v. Chr. die ersten Historiker und Epiker auf Latein schrieben, war der Einfluss der griechischen Literatur bereits vorherrschend. Viele Autoren waren selbst Griechen, sodass die römischen Legenden aus den griechischen adaptiert wurden. Die ursprünglichen italienischen Götter wurden mit den griechischen gleichgesetzt, zum Beispiel Saturnus mit Kronos oder der große Himmelsgott Jupiter mit Zeus.

Die Hauptquelle für die israelitische und später die jüdische Religion ist die kanonisierte Bibel, der Tanach. Die Religion Israels war ursprünglich henotheistisch. Als die Israeliten zur Richterzeit (1250 bis 1000 v. Chr.) in Kanaan sesshaft wurden, griffen sie die dortigen religiösen Vorstellungen auf, wenngleich die Kanaanäer in der Bibel negativ beschrieben werden. Die weitgehende Übereinstimmung zwischen den Attributen JHWHs, des einzigen Gottes Israels, und des kanaanäisch-ugaritischen Gottes El deutet darauf hin, dass JHWH aus El entstanden ist und sich allmählich vom henotheistischen Kult entfernte. Diese Annahme wird dadurch gestützt, dass gegen El, anders als die anderen biblischen Götter, nicht polemisiert wurde, und dass er seine Funktion als Urvater der Götterversammlung behielt.

Der Hochgott El nahm in der ugaritischen Religion den Vorsitz der Götterversammlung ein und wurde als Schöpfer der Götter und Geschöpfe genannt. Neben ihm stand der junge Fruchtbarkeitsgott Baal, Erzeuger des Gewitters und Spender des Regens. Oft wurde er zusammen mit seiner Geliebten Anat als kriegerischer Gott dargestellt, der seine Gegner tötete. Anat selbst tritt als Kämpferin und Liebende hervor, und sie scheut sich nicht, selbst dem obersten Gott El zu drohen. Unter den Göttinnen von Ugarit nahm Athirat als Gemahlin Els den höchsten Rang ein. Astarte oder Aschera, die Himmelskönigin, wurde mit der babylonischen Kriegs- und Liebesgöttin Ištar gleichgesetzt.

Das 1. Buch Mose nennt JHWH als Schöpfer des Himmels und der Erde. Da sein Gottesname nicht ausgesprochen wurde, trat an seine Stelle oft die Bezeichnung "Adonai" („Herr“). Im Deboralied, einem der ältesten Texte der Bibel, wird JHWH als Gott Israels beschrieben, der zugunsten seines Volks eingreift. Hier überwiegt die atmosphärische Beschreibung JHWHs, vor dem die Erde bebt, die Wolken von Wasser triefen und die Berge schwanken. Andere Stellen bekunden, dass er im Himmel wohnt. Weitere Texte heben die kriegerischen Züge JHWHs hervor; das Buch der Richter betont vor allem seine Hilfe in Israels Kriegen gegen die Feinde. Außerhalb Jerusalems wurden weiterhin Baal und die Göttinnen verehrt. Sowohl JHWH als auch Baal waren Himmelsgötter, die mit Blitz und Donner in Verbindung standen. Ein Hinweis darauf, dass beide während der Richterzeit nicht immer getrennt wurden, ist der Namensbestandteil "Baal", der auch in den Eigennamen streng jahwistischer Familien vorkommt. Erst später wurde Baal als Erzfeind JHWHs beschrieben.

JHWH gilt nach dem 5. Buch Mose als einziger Gott Israels. Er wird als eifersüchtiger Gott beschrieben, der keinen anderen Gott an seiner Seite duldet. Als „großer und furchtbarer Gott“, der sein Volk Israel aus Liebe auserwählt hat, fordert er Ehrfurcht und Liebe von seinen Anhängern. JHWHs Charakter ist ambivalent, denn er bringt sowohl Gutes als auch Böses. Das Gesetz genießt als Wort Gottes göttliche Autorität, und auch die Zehn Gebote sind Ausdruck des göttlichen Willens. Zwar finden sich vor allem in den älteren Texten der Bibel deutliche Anthropomorphismen, doch das jüdische Bilderverbot drückt klar aus, dass JHWH nicht in menschlichen Zügen gedacht werden könne.

Das Quellenmaterial zur altsüdarabischen Religion besteht im Wesentlichen aus Inschriften in Denkmälern, die eine große Zahl von Göttern und deren Beinamen nennen. In allen altsüdarabischen Reichen war Athtar der Hauptgott, dem der Planet Venus zugeordnet wurde. Neben seiner überlebenswichtigen Bewässerungs- und Fruchtbarkeitsfunktion war er auch als Kriegsgott tätig. Der sabäische Staatsgott war Almaqah, der mit dem Mond in Verbindung stand und zusammen mit dem König und dem Reichsvolk den Staat repräsentierte. Der Sonnengott hatte zwei weibliche Erscheinungsformen, nämlich dat-Himyam und dat-Baʿdan. Zusammen mit Athtar und Almaqah bildeten sie die offizielle Götterdreiheit Sabas, und auch in anderen südarabischen Staaten wurden sie am häufigsten genannt. Daneben gab es weitere regionale Götter wie Sama, vermutlich ein Mondgott, und Taʿlab. In der späteren Königszeit (ab 40 n. Chr.) kam es, bedingt durch schwere innere Machtkämpfe verschiedener Stämme, zu einer Differenzierung in weitere Erscheinungsformen und Einzelgötter. Eine Darstellung von Göttern in menschlicher Gestalt fand nicht statt; stattdessen wurden oft symbolhafte Zeichen und Tiere verwendet.

In Zentral- und Nordarabien lebte die Bevölkerung nicht wie im Süden in hochentwickelten Staaten, sondern führte – mit Ausnahme von Lihyan – ein Nomadendasein. Die Quellenlage in Zentralarabien ist zwar wesentlich schlechter als im Süden, doch liefern spätere Texte wie der gegen das Heidentum polemisierende Koran oder das Götzenbuch des Ibn al-Kalbī Hinweise zu den altzentralarabischen Göttern. Wie alle Nomadenvölker beteten auch die Beduinen Arabiens mit Allah einen höchsten Himmelsgott an, der die Welt erschaffen hat und Regen spendet. Andere Götter genossen nicht denselben hohen Rang und bildeten auch kein hierarchisch geordnetes Pantheon. Neben Allah wurden die drei Göttinnen Manat, Al-Lāt und Al-ʿUzzā, auch „Töchter Allahs“ genannt, in ganz Arabien hoch verehrt. Al-Lat wurde von Herodot mit Urania, der Himmelsgöttin, gleichgesetzt; vermutlich hatte sie ursprünglich eine ähnlich überragende Bedeutung wie Allah. Eine untergeordnete Rolle spielten die im Götzenbuch genannten drei Dutzend lokalen Götter, die oft bestimmten Stämmen zugeordnet waren.

Obwohl es im rabbinischen Judentum keine systematischen Betrachtungen zu den Attributen Gottes gab, bestand in einigen wesentlichen Punkten Einigkeit. Alle Rabbiner waren von der Einheit Gottes, des Schöpfers von Himmel und Erde, überzeugt. Gott belohnt diejenigen, die seinem Willen gehorchen, und bestraft die anderen, und er wählte unter allen Völkern das jüdische aus, um ihm die Tora zu offenbaren. Das Tetragramm JHWH wurde aus Ehrfurcht nie ausgesprochen, und stattdessen andere Namen oder Umschreibungen wie Adonai („Herr“) oder der Heilige verwendet. Obwohl Gott direkt im Gebet angesprochen werden kann, ist seine wahre Natur unergründlich, und er unterscheidet sich gänzlich von seinen Geschöpfen. Dennoch sorgten sich die Autoren des Talmud wenig um anthropomorphe Beschreibungen. Oft wurde Gott mit einem König verglichen, der auf dem Thron des Urteils oder dem Thron der Vergebung sitzt. Besonders nach dem Jüdischen Krieg wurde die Vorstellung vertieft, dass Gott menschliches Leid fühlt und mit den Opfern der Verfolgung trauert. Bilderverehrung und dualistische Vorstellungen wiesen die Rabbiner strikt zurück.

Im Mittelalter kam es unter dem Einfluss der griechischen Philosophie zu einer Verfeinerung der Attribute Gottes. Die mittelalterlichen Theologen wiesen darauf hin, dass alle anthropomorphen Beschreibungen Gottes in der Bibel nicht wörtlich zu verstehen seien. Zu Maimonides’ 13 Prinzipien des jüdischen Glaubens zählt die Auffassung, dass Gott körperlos und immateriell ist. Gott war sowohl allwissend als auch allmächtig. Wie auch die Autoren der Bibel und die Rabbiner vertraten die mittelalterlichen jüdischen Denker einen fürsorglichen Gott, wobei sich diese nach Maimonides und Levi ben Gershon nur auf die Menschen und nicht auf alle Geschöpfe erstreckt.

Die Kabbalisten akzeptierten die abstrakten Beschreibungen der mittelalterlichen Philosophen, verspürten aber als Mystiker den Wunsch, eine lebendigere Verbindung zu Gott aufzubauen. In der Kabbala wurde zwischen Gott selbst – dem unergründlichen En Sof – und seinen Erscheinungsformen unterschieden. Die Sephiroth waren die zehn Emanationen, die aus Gott selbst, zu dem überhaupt nichts gesagt werden kann, entspringen. Sie repräsentieren verschiedene Aspekte Gottes wie Weisheit, Stärke oder Pracht. Der Chassidismus des 18. Jahrhunderts, besonders die Chabad-Bewegung, tendierte zu einem panentheistischen Verständnis des Tzimtzum-Begriffs: Ohne Gott gäbe es kein Universum, aber ohne das Universum wäre Gott immer noch das gleiche.

Unter den jüdischen Denkern des 20. Jahrhunderts vertrat Mordechai M. Kaplan am vehementesten eine naturalistische Weltsicht. Für ihn war Gott kein übernatürliches, persönliches Wesen, sondern die universelle Kraft, die zu Gerechtigkeit führt. Martin Buber hingegen betonte den persönlichen Aspekt Gottes. Mehr als jedes andere Ereignis führte der Holocaust zu einer Neueinschätzung der mittelalterlichen Aussagen zum Verhältnis von Gott und den Menschen sowie dem Theodizeeproblem.

Da die Christen ursprünglich eine jüdische Gruppierung waren, wurden ihre Gottesvorstellungen stark von jüdischen Traditionen geprägt. Daneben beeinflusste die griechische Philosophie, insbesondere der antike Platonismus, die christlichen Gottesvorstellungen maßgeblich. Frühe Versuche, eine christliche Theologie auszuarbeiten, etwa bei Klemens von Alexandrien, Justin dem Märtyrer, Irenäus von Lyon, Athenagoras und Theophilus nehmen nicht nur Bezug auf biblische Überlieferung, inzwischen ausgebildete Bekenntnisformeln und liturgische Redeweisen, sondern in Terminologie, Inhalten und Werkkonzeptionen auch in unterschiedlichem Ausmaß Anleihen bei jüdischen Theologen und philosophischen Traditionen. Gott wird vielfach als transzendent und ewig, frei von zeitlichen oder räumlichen Grenzen und mit höchster übernatürlicher Macht und Ehre ausgestattet beschrieben. Wegen der Unergründlichkeit seines Wesens wird er oftmals nur in symbolischen Ausdrucksweisen, in seinen Wirkungen und ansonsten in verneinenden Eigenschaften wie „unendlich“, „unergründlich“ oder „unsichtbar“ benannt. Redeweisen in Bibel, Liturgie, Gebetsformularen und dergleichen, die dazu führen könnten, Gott körperlich und insbesondere anthropomorph vorzustellen, werden dabei vielfach, vor allem bei Theologen in der Schultradition der alexandrinischen Theologie (einschließlich etwa Philon von Alexandria und Origenes), als uneigentliche Aussageweisen interpretiert. Andere Theologen sind zurückhaltender oder ablehnender gegenüber Kultur, Terminologie und Konzepten griechischer Traditionen und beziehen sich direkter auf jüdisch-christliche Überlieferungen.

Das im Jahr 325 formulierte Bekenntnis von Nicäa, das heute von allen großen christlichen Kirchen anerkannt wird, nennt Jesus Christus göttlich und wesenseins mit Gottvater und erwähnt außerdem kurz den Heiligen Geist. Die Vorstellung, dass Jesus gleichzeitig Mensch und Gott war, wurde im späteren christologischen Bekenntnis des Konzils von Chalcedon bestätigt. Spätere Debatten und Festlegungen wenden sich von der Christologie stärker der Trinitätstheologie zu. Es wird versucht, die Annahme dreier Götter bzw. voneinander unabhängiger Modalitäten, die durch Vater, Sohn und Geist verkörpert werden, zu vermeiden bzw. als Irrlehre darzustellen. Sie werden als der Substanz nach identisch, jedoch der Relation nach verschieden bestimmt; davon abweichende Lehren und Lehrer werden als häretisch abgegrenzt.

Die christliche Theologie des Mittelalters arbeitet in der Rezeption weiterer antiker Konzepte und teils auch der Debatten in jüdischer und islamischer Theologie die Gotteslehre in unterschiedlichen, teils gegensätzlichen Akzenten weiter aus. Dabei war vielfach umstritten, wie stark Anleihen an griechische philosophische Terminologie gehen können und sollen und an nicht bereits ein Wissen aus Offenbarung veranschlagende philosophische Konzeptualisierungen "(natürliche Vernunft)". Eine Kompromissformel des vierten Laterankonzils (can. 806) ist etwa, dass Gott jeweils in größerem Maße unähnlich bleibe auch bei allen durchaus möglichen Aufweisen von Ähnlichkeiten mit Geschaffenem.

Die Reformation forderte eine stärkere Rückbesinnung auf biblische Texte. Natürlicher Vernunft und zwischenzeitlicher Tradition wird weniger Erkenntniswert zugeschrieben. Während u. a. im 19. Jahrhundert einige Theologen auf Herausforderungen u. a. durch Aufklärung und moderne Vernunft- und Offenbarungskritik mit einer konstruktiven Rezeption u. a. transzendentalphilosophischer Ideen reagieren, weisen andere dies zurück. Die Reichweite „natürlicher Vernunft“ wird dann geringer, „Übernatürliches“ höher veranschlagt; so in unterschiedlichsten Ausprägungen etwa vonseiten der meisten katholischen Revitalisierungsversuche der Systematisierungen des Thomas von Aquin, anders etwa Anfang des 20. Jahrhunderts bei protestantischen Theologen wie Karl Barth, die sich stärker auf biblische Offenbarung beziehen. In jüngeren theologischen Debatten wurden auch zuvor weitgehend unstrittige Aspekte der Gottesvorstellung kritisch diskutiert, etwa die Allmacht Gottes.

Der Islam, der auf der arabischen Halbinsel entstand, hat seinen Gottesbegriff vor allem in Auseinandersetzung mit der altarabischen Religion, die verschiedene lokale Gottheiten kannte, entwickelt. Bereits der Koran betont die Einheit und Einzigkeit Gottes und bedient sich einer Reihe von Argumenten, um zu zeigen, dass Gott als Schöpfer der Welt den Glauben an andere göttliche Wesen und Mächte überflüssig macht. Kontinuität im Gottesbild bestand dagegen zur monotheistischen Religion im altsüdarabischen Reich von Himyar. Aus diesem Umfeld wurden auch die beiden Gottesnamen Allah und Rahman in den Islam übernommen, die miteinander verbunden zum Beispiel in der Basmala-Formel erscheinen. Darüber hinaus hat sich der frühe Islam auch mit dem christlichen Gottesbild auseinandergesetzt. Die Formel in Sure 112, dass Gott weder zeugend noch gezeugt ist, lässt sich als eine direkte Zurückweisung der Aussage im nizänischen Glaubensbekenntnis verstehen, wonach Jesus von Gott „gezeugt, nicht geschaffen“ ist.

Theologische Debatten, die um die Mitte des 8. Jahrhunderts einsetzten, kreisten um die Frage, wie die verschiedenen Aussagen über Gott im Koran, die Körperlichkeit oder Menschenähnlichkeit implizieren, zu interpretieren sind. Während einige theologische Schulen diese Aussagen wörtlich nahmen und zu einem anthropomorphistischen Gottesbild neigten (so z. B. al-Mughīra ibn Saʿīd und Muqātil ibn Sulaimān), vertraten andere eine sehr radikale Transzendenz Gottes (so z. B. Dschahm ibn Safwān). Ende des 8. Jahrhunderts entwickelten sich Zwischenpositionen. Der schiitische Gelehrte Hischām ibn al-Hakam (gest. nach 795) definierte Gott als einen dreidimensionalen, massiven Lichtkörper, wobei er sich an den Aussagen über Gott in Sure 112 und im Lichtvers orientierte. Die Anhänger der Muʿtazila betonten, dass Gottes Wesen unbeschreibbar ist; anthropomorphe Zuschreibungen im Koran mussten ihrer Auffassung nach metaphorisch ausgelegt werden.

Anlass zu theologischen Spekulationen haben daneben auch die zahlreichen im Koran genannten Namen und Eigenschaften Gottes gegeben. Es stellte sich die Frage, wie diese sich zu Gottes eigenem Wesen verhalten. Während die Muʿtaziliten im Zuge ihrer strengen Betonung der Einheit Gottes (Tauhīd) meinten, dass Gott Qualifikationen wie „wissend“ ("ʿālim"), „mächtig“ ("qādir"), „lebendig“ ("ḥaiy") allein durch sich selbst ("bi-nafsihī") habe, wurde in der sunnitischen Theologie angenommen, dass diese Eigenschaften auf korrelierende Substantive verweisen, nämlich „Wissen“ ("ʿilm"), „Macht“ ("qudra"), "Leben" ("ḥayāt"), denen eine reale Existenz zukommt. Um nicht das Prinzip der Einheit Gottes zu verletzen, konnten sie allerdings nicht so weit gehen, diese Attribute als verschieden von Gott zu bezeichnen. Der Theologe Ibn Kullāb entwickelte darum die Formel, dass die Attribute Gottes „weder identisch mit Gott noch nicht-identisch mit ihm“ seien. Diese Formel wurde später auch in die aschʿaritische Theologie übernommen. Die Attribute Gottes erhielten damit eine Position, die derjenigen der Hypostasen in der christlichen Theologie ähnelte.

Manche Gruppen wie die frühen Hanbaliten lehnten es aber auch ganz ab, das Wesen Gottes zum Gegenstand rationaler Spekulation zu machen. In ihrer Tradition stehen die heutigen Wahhabiten. In der sufischen Tradition zog man der Spekulation über Gott eine unmittelbare, mystische Gotteserfahrung in Form des „Entwerdens in Gott“ ("fanā fī Llāh") vor. Bei verschiedenen Gruppen der schiitischen Ghulāt-Tradition gab es schließlich die Tendenz, den eigenen Imam als Gott anzusehen.

Im Voodoo wird Bondyè als einziger Gott verehrt. Da er sich in unerreichbarer Ferne befinden soll, werden Gebete und Opfer ausschließlich an die Loa als vermittelnde Geistwesen gerichtet.

Bestrebungen, die Existenz Gottes oder der Götter schlüssig abzuleiten, finden sich bereits in der griechischen Philosophie. In der jüdischen und frühchristlichen Apologetik, und später in der jüdischen, christlichen und arabischen Scholastik wurden weitere formale Gottesbeweise aufgestellt. Einige moderne Apologeten legen die Existenz Gottes ebenfalls anhand logischer Argumente dar.

Die folgende Liste nennt bedeutende Argumente für die Existenz Gottes, sowie einige namhafte Vertreter.

Anstatt Beweise für die Existenz eines Gottes zu führen, kann gezeigt werden, dass der Glaube an dessen Existenz vorteilhaft ist. Immanuel Kant und Johann Gottlieb Fichte waren beispielsweise der Auffassung, dass der Gottglaube moralisch notwendig ist. Der Pascalschen Wette zufolge ist es vernünftig, sicherheitshalber an Gott zu glauben, da dieser gegebenenfalls den Glauben belohnt und den Nichtglauben bestraft.

Für Friedrich Wilhelm Joseph Schelling war Philosophie nur dann wirkliche Philosophie, wenn sich durch sie über „Dasein und Nichtdasein Gottes etwas wissenschaftlich ausmachen lasse.“ Auch für Georg Wilhelm Friedrich Hegel hat die Philosophie den Zweck, Gott zu erkennen, da ihr Gegenstand, die Wahrheit, nichts anderes sei als die Auseinandersetzung mit Gott. Kant hingegen kritisierte klassische Gottesbeweise und hielt die objektive Realität Gottes weder für beweis- noch widerlegbar. Friedrich Nietzsche war gegenüber metaphysischen Konzepten skeptisch; er lehnte den Versuch ab, auf eine unbedingte, widerspruchslose Welt zu schließen und nur durch Negationen Gott einzuführen.

Die Ansicht, dass keine vernünftige Diskussion über die Existenz von Göttern möglich ist, wird üblicherweise damit begründet, dass die menschliche Vernunft hierzu nicht in der Lage sei (Irrationalismus und Fideismus), oder dass alle Wahrheitsaussagen letztendlich willkürlich seien (erkenntnistheoretischer Relativismus). Der starke Agnostizismus vertritt die Auffassung, dass niemand wissen kann, ob es einen Gott gibt, und dass es nicht möglich sein wird, diese Frage je zu beantworten.

Der Nichtglaube an Götter wird häufig mit einem Mangel an Beweisen für deren Existenz begründet. Russells Teekanne ist ein Beispiel, das die philosophische Beweislast für die Behauptung eines Gottes aufzeigen soll. Eine ähnliche Haltung wird im Rahmen von Religionsparodien beansprucht, bei denen übernatürliche Wesen wie das „unsichtbare rosafarbene Einhorn“ oder das „fliegende Spaghettimonster“ erfunden werden. Neben logischen Argumenten gegen bestimmte Gottesvorstellungen wie dem Allmachtsparadoxon und dem Theodizeeproblem gibt es Versuche, die Existenz von Göttern empirisch zu widerlegen. So würden naturwissenschaftliche Erklärungen zur Entstehung des Lebens und des Universums sowie statistische Untersuchungen zur Unwirksamkeit von Gebeten zeigen, dass das Universum sich genau so verhält, wie in Abwesenheit eines Gottes zu erwarten sei.

In einer 1998 durchgeführten Umfrage unter 1000 US-Amerikanern wurden als Hauptgründe für den Glauben an Gott die Schönheit, Perfektion oder Komplexität der Welt (29 % der Befragten, die an Gott glauben) sowie die persönliche Gotteserfahrung (21 %) genannt. Eine Umfrage unter Mitgliedern der Skeptics Society ergab als Hauptgrund für den Nichtglauben an Gott den Mangel an Beweisen für dessen Existenz (38 % der Befragten, die an keinen Gott glauben).

Eine Zusammenfassung von Umfrageergebnissen aus verschiedenen Staaten ergab im Jahr 2007, dass es weltweit zwischen 505 und 749 Millionen Atheisten und Agnostiker gibt. Laut der Encyclopædia Britannica gab es 2009 weltweit 640 Millionen (9,4 %) Nichtreligiöse und Agnostiker, und weitere 139 Millionen (2,0 %) Atheisten, hauptsächlich in China.

Bei einer Eurobarometer-Umfrage im Jahr 2005 wurde festgestellt, dass 52 % der damaligen EU-Bevölkerung glaubt, dass es einen Gott gibt. Eine vagere Frage nach dem Glauben an „eine andere spirituelle Kraft oder Lebenskraft“ wurde von weiteren 27 % positiv beantwortet. Bezüglich der Gottgläubigkeit bestanden große Unterschiede zwischen den einzelnen europäischen Staaten. Die Umfrage ergab, dass der Glaube an Gott in Staaten mit starkem kirchlichen Einfluss am stärksten verbreitet ist, dass mehr Frauen (58 %) als Männer (45 %) an einen Gott glauben, und dass der Gottglaube mit höherem Alter, geringerer Bildung und politisch rechtsgerichteten Ansichten korreliert.

Bei empirischen Untersuchungen wurde immer wieder festgestellt, dass die unter Gläubigen verbreiteten Gottesvorstellungen auch innerhalb derselben Religion sehr vielfältig sind. Ähnlichkeitsstruktur- und Faktorenanalysen ergaben verschiedene Dimensionen, aus denen ein Gottesbild aufgebaut sein kann. So können göttliche Eigenschaften beispielsweise entlang der Dimensionen richtend-kümmernd, kontrollierend-rettend oder konkret-abstrakt variieren.

Justin Barrett kam bei Untersuchungen unter US-amerikanischen und indischen Gläubigen zum Ergebnis, dass Personen intuitiv zu personenähnlichen Gottesvorstellungen tendieren, die der theologischen Lehre zuwiderlaufen. Beispielsweise besteht die Tendenz, zu denken, dass Gott beziehungsweise die Götter sich bewegen, Sinneseindrücke verarbeiten oder nur eine Aufgabe auf einmal erledigen können. Andererseits werden in abstrakteren Situationen theologische Attribute wie Allgegenwart oder Allmacht zur Beschreibung von Gott übernommen. Die ontologische Diskrepanz zwischen Menschen und dem Übernatürlichen wird demnach zumindest in kognitiv relevanten, alltäglichen Situationen wie dem Gebet überbrückt, indem die Unterschiede zwischen beiden Bereichen ignoriert werden.

In der Psychoanalyse wird der Gottglaube als eine Form des Wunschdenkens betrachtet. Für Sigmund Freud war Gott die Projektion einer perfekten, schützenden Vaterfigur, die das Gefühl einer idealisierten Kindheit vermitteln soll. Für Carl Gustav Jung ist Gott eine Erfahrung, die in seelischen Tiefenschichten bereit liegt. Das innerseelische Gottesbild entspricht dem Archetypus des Selbst und repräsentiert psychische Ganzheit. Über die metaphysische Wirklichkeit Gottes ist damit nichts ausgesagt. Andere Psychoanalytiker sahen Gott nicht als tröstlichen Traum, sondern als Projektion des neurotischen Selbsthasses. Ludwig Feuerbach, der ebenfalls religionskritische Thesen vertrat, sah im Gottglauben den „Spiegel des Menschen“, der Rückschlüsse auf das menschliche Wesen erlaube.

Die kognitive Religionswissenschaft geht davon aus, dass Menschen aufgrund ihrer Veranlagung dazu tendieren, Vorstellungen von übernatürlichen Akteuren zu verfestigen. Die Standardtheorie begründet dies im Wesentlichen durch zwei mentale Module bei Menschen, dem Theory of Mind Mechanism (ToMM) und der Agency Detection Device (ADD). Durch das ToMM sind Menschen in der Lage, bei anderen Akteuren Gefühle und Absichten zu vermuten. Die ADD ermöglicht es, aufgrund sensorischer Reize schnell die Anwesenheit von Akteuren in der Umgebung wahrzunehmen. Sie diente beim Frühmenschen dazu, Prädatoren rechtzeitig zu erkennen und zu meiden, wird aber auch heute noch aktiv, sodass selbst hinter natürlichen Ereignissen oftmals ein Akteur vermutet wird. Dieses Erklärungsmodell bezieht sich nicht nur auf Götter, sondern auf alle übernatürlichen Akteure.

Ein verwandter Forschungsgegenstand ist die Frage, welche kognitiven Fähigkeiten in Bezug auf den Gottglauben angeboren sind. Die Anthropomorphismus-Hypothese geht davon aus, dass Kinder einen Gott anfänglich als „großen Supermenschen im Himmel“ betrachten, und erst später die Vorstellung eines transzendenten, körperlosen Wesens entwickeln. Demgegenüber besagt die Preparedness-Hypothese, dass Kinder derartige metaphysische Eigenschaften problemlos akzeptieren, da sie von Beginn an kognitiv in der Lage sind, sich allgemeine übernatürliche Akteure vorzustellen.

Nachschlagewerke und Überblicksdarstellungen zur Mythologie:

Vergleichende Religionsphilosophie:

Vergleichende Religionswissenschaft:




</doc>
<doc id="1851" url="https://de.wikipedia.org/wiki?curid=1851" title="Genotyp">
Genotyp

Der Genotyp ( "génos" „Gattung, Geschlecht“ und "týpos" „Gestalt, Abbild, Muster“) ist die Gesamtheit der Gene eines Organismus oder auch das Erbbild dieses Lebewesens. Er repräsentiert dessen exakte genetische Ausstattung, die sämtliche in diesem Individuum vorhandenen Erbanlagen umfasst. 

Zum Genotyp zählen bei Eukaryoten sowohl die Gene in den Chromosomen der Zellkerne als auch extrachromosomale DNA wie die Gene der mitochondrialen DNA und bei Pflanzen zusätzlich die DNA der Chloroplasten. Bei den Prokaryoten liegen die Gene in einem Nukleoid oder Plasmid. 

Der individuelle Genotyp bildet die genetische Grundlage für die Ausbildung der morphologischen und physiologischen Merkmale des Individuums durch Genexpression, die den Phänotyp bestimmen. Bei den diploiden Organismen, die durch sexuelle Fortpflanzung entstehen, gehören zum Genotyp aber auch Erbanlagen, die rezessiv sind und weniger bis keinen Einfluss auf den Phänotyp haben, wenn sie heterozygot vorliegen. Auch die auf Introns liegende Erbinformation gehört zum Genotyp, ohne dass sie sich auf den Phänotyp auswirken kann, denn nur die Erbinformation auf den Exons kann aktiviert werden und bei der Proteinbiosynthese abgelesen werden. Der Genotyp als Gesamtheit der genetischen Ausstattung ist auch insofern bedeutsam, als ein Gen allein nicht für die Ausbildung eines Merkmals sorgen kann, sondern das Zusammenwirken vieler Gene in einer zeitlichen Abfolge. Hierbei kann das Vorhandensein oder das Fehlen eines Allels jedoch durchaus eine entscheidende Rolle spielen, ob ein Merkmal ausgebildet werden kann oder nicht.

Der Begriff "Genotyp" wurde 1909 von dem dänischen Genetiker Wilhelm Johannsen geprägt. Er wird heute vor allem im Zusammenhang mit der Mendelschen Vererbungslehre verwendet sowie bei der Auswertung der Ergebnisse einer DNA-Analyse, hier allerdings - nicht ganz der ursprünglichen Definition entsprechend - unter Bezugnahme auf nur diejenigen Erbanlagen, die bei der Überprüfung von Interesse sind. 

Die genetische Information der gesamten Zelle wird als Idiotyp (Idiotypus) bezeichnet. Zwei Organismen, deren Gene sich auch nur an einem Genlocus (= einer Position in ihrem Genom) unterscheiden, haben einen unterschiedlichen Genotyp. Der Begriff „Genotyp“ bezieht sich also auf die vollständige Kombination aller Allele / aller Loci eines Organismus. Unter Phänotyp versteht man die Summe aller beobachtbaren Merkmale des Organismus (z. B. Größe, Blütenfarbe, Schnabelform), die sich als Ergebnis der Interaktion des Genotyps mit der Umwelt entwickelt haben. Der Genotyp ändert sich zu Lebzeiten eines Organismus nicht, ausgenommen durch extreme Einflüsse wie z. B. mutagene Strahlung oder durch Temperaturschocks.

Auch Organismen identischen Genotyps unterscheiden sich gewöhnlich in ihrem Phänotyp. Verantwortlich dafür sind epigenetische Mechanismen, d. h. identische Gene können in verschiedenen Organismen verschieden exprimiert werden (siehe auch Modifikation). 

Ein alltägliches Beispiel sind monozygotische (eineiige) Zwillinge. Eineiige Zwillinge haben den gleichen Genotyp, da sie das gleiche Genom in sich tragen. Doch können eineiige Zwillinge anhand ihrer Fingerabdrücke identifiziert werden, die niemals vollständig gleich sind. Im Laufe des Lebens können sie einen unterschiedlichen Phänotyp entwickeln.

Das Konzept der phänotypischen Plastizität beschreibt das Maß, in dem der Phänotyp eines Organismus durch seinen Genotyp vorherbestimmt ist. Ein hoher Wert der Plastizität bedeutet: Umwelteinflüsse haben einen starken Einfluss auf den sich individuell entwickelnden Phänotyp. Bei geringer Plastizität kann der Phänotyp aus dem Genotyp zuverlässig vorhergesagt werden, unabhängig von besonderen Umweltverhältnissen während der Entwicklung. Hohe Plastizität lässt sich am Beispiel der Larven des Wassermolchs beobachten: Wenn diese Larven die Anwesenheit von Räubern wie Libellen wahrnehmen, vergrößern sich Kopf und Schwanz im Verhältnis zum Körper und die Haut wird dunkler pigmentiert. Larven mit diesen Merkmalen haben bessere Überlebenschancen gegenüber Räubern, wachsen aber langsamer als andere Phänotypen.


</doc>
<doc id="1852" url="https://de.wikipedia.org/wiki?curid=1852" title="Graphentheorie">
Graphentheorie

Die Graphentheorie (seltener auch Grafentheorie) ist ein Teilgebiet der Mathematik, das die Eigenschaften von Graphen und ihre Beziehungen zueinander untersucht.

Dadurch, dass einerseits viele algorithmische Probleme auf Graphen zurückgeführt werden können und andererseits die Lösung graphentheoretischer Probleme oft auf Algorithmen basiert, ist die Graphentheorie auch in der Informatik, insbesondere der Komplexitätstheorie, von großer Bedeutung. Die Untersuchung von Graphen ist auch Inhalt der Netzwerktheorie. Zudem lassen sich zahlreiche Alltagsprobleme mit Hilfe von Graphen modellieren.

In der Graphentheorie bezeichnet ein Graph eine Menge von Knoten (auch Ecken oder Punkte genannt) zusammen mit einer Menge von Kanten. Eine Kante ist hierbei eine Menge von genau zwei Knoten. Ist die Menge der Knoten endlich, spricht man von "endlichen Graphen", ansonsten von "unendlichen Graphen". Wenn die Kanten statt durch Mengen durch geordnete Paare von Knoten angegeben sind, spricht man von "gerichteten Graphen". In diesem Falle unterscheidet man zwischen der Kante ("a","b") – als Kante von Knoten "a" zu Knoten "b" – und der Kante ("b","a") – als Kante von Knoten "b" zu Knoten "a".

Komplexere Graphentypen sind:

Knoten und Kanten können auch mit Farben (formal mit natürlichen Zahlen) oder Gewichten (d. h. rationalen- oder reellen Zahlen) versehen werden. Man spricht dann von knoten- bzw. kantengefärbten oder -gewichteten Graphen.

Die Graphentheorie definiert eine Vielzahl von grundlegenden Begriffen sehr intuitiv einleuchtend:

Weitere grundlegende Begriffe sind:

Graphen können verschiedene Eigenschaften haben. So kann ein Graph zusammenhängend, bipartit, planar, eulersch oder hamiltonisch sein. Es kann nach der Existenz spezieller Teilgraphen gefragt werden oder bestimmte Parameter untersucht werden, wie zum Beispiel Knotenzahl, Kantenzahl, Minimalgrad, Maximalgrad, Taillenweite, Durchmesser, Knotenzusammenhangszahl, Kantenzusammenhangszahl, Bogenzusammenhangszahl, chromatische Zahl, Stabilitätszahl oder Cliquenzahl.

Die verschiedenen Eigenschaften können zueinander in Beziehung stehen. Die Beziehungen zu untersuchen ist eine Aufgabe der Graphentheorie. Beispielsweise ist die Knotenzusammenhangszahl nie größer als die Kantenzusammenhangszahl, welche wiederum nie größer als der Minimalgrad des betrachteten Graphen ist. In ebenen Graphen ist die Färbungszahl immer kleiner als 5. Diese Aussage ist auch als der Vier-Farben-Satz bekannt.

Einige der aufgezählten Grapheneigenschaften sind relativ schnell algorithmisch bestimmbar, etwa wenn der Aufwand höchstens mit dem Quadrat der Größe der Graphen wächst. Andere Eigenschaften praktisch angewandter Graphen sind innerhalb der Lebensdauer heutiger Computer nicht exakt zu bestimmen. Allerdings kann in diesen Fällen der Einsatz von heuristischen Verfahren zu sinnvollen Näherungslösungen führen.

Die wichtigsten Probleme und Ergebnisse der Graphentheorie werden in folgenden Artikeln dargestellt:

Ein von der Graphentheorie unabhängiger Vorläufer in der Antike war die Methode Dihairesis, mit deren Hilfe man (nur teilweise graphisch) zoologische, musikwissenschaftliche und andere Begriffe hierarchisierte. Es entstanden so frühe Systematiken innerhalb verschiedener Wissensgebiete.

Die Anfänge der Graphentheorie gehen bis in das Jahr 1736 zurück. Damals publizierte Leonhard Euler eine Lösung für das Königsberger Brückenproblem. Die Frage war, ob es einen Rundgang durch die Stadt Königsberg (Preußen) gibt, der jede der sieben Brücken über den Fluss Pregel genau einmal benutzt. Euler konnte eine für dieses Problem nicht erfüllbare notwendige Bedingung angeben und so die Existenz eines solchen Rundganges verneinen. 

Der Begriff "Graph" wurde in Anlehnung an graphischen Notationen chemischer Strukturen erstmals 1878 von dem Mathematiker James Joseph Sylvester verwendet. Als weiterer Begründer der frühen Graphentheorie gilt Arthur Cayley. Eine der ersten Anwendungen waren chemische Konstitutionsformeln. (Siehe auch Chemische Graphentheorie und Literatur: Bonchev/Rouvray, 1990). Das erste Lehrbuch zur Graphentheorie erschien 1936 von Dénes Kőnig.

In der zweiten Hälfte des 20. Jahrhunderts hat William Thomas Tutte maßgeblich an der Weiterentwicklung der Graphentheorie gearbeitet und dieses Teilgebiet der Mathematik stark geprägt.

Eine wichtige Anwendung der algorithmischen Graphentheorie ist die Suche nach einer kürzesten Route zwischen zwei Orten in einem Straßennetz. Dieses Problem kann man als Graphenproblem modellieren. Hierzu abstrahiert man das Straßennetz, in dem man alle Orte als Knoten aufnimmt, und eine Kante hinzufügt, wenn es eine Verbindung zwischen diesen Orten gibt. Die Kanten dieses Graphen werden je nach Anwendung gewichtet, zum Beispiel mit der Länge der assoziierten Verbindung. Mit Hilfe eines Algorithmus zum Finden von kürzesten Pfaden (beispielsweise mit dem Algorithmus von Dijkstra) kann eine kürzeste Verbindung effizient gefunden werden.

Algorithmisch deutlich schwieriger ist die Bestimmung einer kürzesten Rundreise (siehe Problem des Handlungsreisenden), bei der alle Orte eines Straßennetzes genau einmal besucht werden müssen. Da die Zahl der möglichen Rundreisen faktoriell mit der Zahl der Orte wächst, ist der naive Algorithmus, alle Rundreisen auszuprobieren und die kürzeste auszuwählen, für praktische Anwendungen nur für sehr kleine Netzwerke praktikabel. Es existieren für dieses Problem eine Reihe von Approximationsalgorithmen, die eine gute aber nicht eine optimale Rundreise finden. So liefert die Christofides-Heuristik eine Rundreise die maximal 1,5-mal so lang ist wie die bestmögliche. Unter der Annahme, dass "P" ≠ "NP" ("siehe" P-NP-Problem), existiert kein effizienter Algorithmus für die Bestimmung einer optimalen Lösung, da dieses Problem "NP"-schwer ist.

Ein weiteres bekanntes Problem fragt, wie viele Farben man braucht um die Länder einer Landkarte einzufärben, sodass keine zwei benachbarten Länder die gleiche Farbe zugewiesen bekommen. Die Nachbarschaftsbeziehung der Länder kann man als planaren Graph repräsentieren. Das Problem kann nun als Knoten-Färbungsproblem modelliert werden. Nach dem Vier-Farben-Satz braucht man maximal 4 verschiedene Farben. Ob sich im Allgemeinen ein Graph mit weniger Farben einfärben lässt, kann man nach heutigem Wissensstand nicht effizient entscheiden. Das Problem gilt als eines der schwierigsten Probleme in der Klasse der "NP"-vollständigen Probleme. Unter der Voraussetzung, dass "P"≠ "NP", ist selbst eine bis auf einen konstanten Faktor angenäherte Lösung nicht effizient möglich.

Hauptsächlich in der Informatik wird die Transformation von Graphen anhand von regelbasierten Graphersetzungssystemen untersucht. Graphersetzungssysteme, die dem Aufzählen aller Graphen einer Graphsprache dienen, werden auch Graphgrammatik genannt.

Im Bereich der Computergrafik ist die Visualisierung von Graphen (‚Graphzeichnen‘, ) eine Herausforderung. Besonders komplexe Netze werden erst durch ausgefeilte Autolayout-Algorithmen übersichtlich (siehe hierzu: Layout #Graphentheorie).


Ungerichtete Graphen besitzen symmetrische Adjazenzmatrizen und deshalb reelle Eigenwerte. Alle Eigenwerte zusammengenommen werden als Spektrum des Graphen bezeichnet. Während die Adjazenzmatrix von der Knotensortierung abhängig ist, ist das Spektrum davon unabhängig.





</doc>
<doc id="1854" url="https://de.wikipedia.org/wiki?curid=1854" title="Gentherapie">
Gentherapie

Als Gentherapie bezeichnet man das Einfügen von Nukleinsäuren wie DNA oder RNA in die Körperzellen eines Individuums, um beispielsweise eine Krankheit zu behandeln. Klassischerweise soll ein intaktes Gen in das Genom der Zielzelle eingefügt werden, um ein defektes Gen zu ersetzen, das ursächlich für die Entstehung der Krankheit ist. Beim Menschen wurden und werden Gentherapien teilweise erfolgreich z. B. im Rahmen von klinischen Studien durchgeführt. Als weltweit erstes Fertigarzneimittel wurde 2003 in China "Gendicine" (rAD-p53) eingeführt.

Innerhalb Europas gehören Gentherapeutika zur Gruppe der Arzneimittel für neuartige Therapien. 

Üblicherweise werden dem Körper einige Zellen entnommen, um diesen im Labor ("ex vivo)" die entsprechenden Nukleinsäuren einzufügen. Anschließend können die Zellen zum Beispiel vermehrt werden, um dann wieder in den Körper eingebracht zu werden. Eine Gentherapie kann auch direkt im Körper ("in vivo") erfolgen. Je nach Art der Gentherapie und der verwendeten Technik kann die Nukleinsäure in das Zellgenom integriert werden oder lediglich zeitweise in der Zelle verbleiben. Entsprechend kann der therapeutische Effekt dauerhaft oder zeitlich beschränkt bestehen.

Für den Transfer gibt es verschiedene Methoden, um eine therapeutische Nukleinsäure in eine Zelle zu transportieren:

Das Ersetzen und dauerhafte Einfügen eines intakten Gens in Form von DNA hat nur bei sogenannten monogenetischen Erkrankungen Aussicht auf Erfolg. Erkrankungen, die durch komplexere genetische Schäden ausgelöst werden, wie zum Beispiel Krebs, können mit Gentherapie nicht ursächlich behandelt werden. Eine Gentherapie darf in Deutschland und einigen anderen Ländern nur in den somatischen (nicht die Keimbahn betreffenden) Zellen durchgeführt werden, damit die neue genetische Information nicht an die Kinder der behandelten Person weitervererbt werden kann. Diese gesetzliche Beschränkung basiert auf ethischen und auf die Sicherheit betreffenden Gesichtspunkten (siehe auch Keimbahntherapie). Das größtmögliche Risiko einer "somatischen" Gentherapie ist eine ungerichtete Integration der Spender-DNA an unpassender Stelle innerhalb des Genoms der Wirtszelle. Da die Integrationsstelle bisher nicht vorhersehbar ist, können andere vorher intakte Gene in ihrer Funktion gestört werden. Im schlimmsten Fall könnte der therapeutische Nutzen des neuen Gens durch eine neue evtl. schwerere Krankheit, bedingt durch die Störung eines vorher intakten Gens, aufgehoben werden.

Derzeit beschränken sich gentherapeutische Ansätze in der Praxis auf zwei verschiedene Zelltypen: zugängliche Stammzellen und langlebige, ausdifferenzierte, postmitotische Zellen. Je nach Zelltyp kommen verschiedene Methoden der Gentherapie zum Einsatz.

Körperzellen, die für eine Gentherapie mit Retroviren als Vektor in Frage kommen, müssen bestimmte Anforderungen erfüllen:

Folgende Zelltypen haben sich als geeignet erwiesen:

In der von "The Journal of Gene Medicine" bereitgestellten Datenbank "Gene Therapy Clinical Trials Worldwide" werden über 1500 klinische Studien gelistet, die bisher genehmigt wurden (Stand 2010).

Die Aufreinigung und molekulare Charakterisierung von Zytokinen wurde vom Team um Roland Mertelsmann erforscht. Sowohl Interleukin-2 wie auch G-CSF wurden homogen dargestellt, erste translationale und klinische Studien zu Zytokinen folgten. Da Experimente in murinen Modellen demonstrierten, dass die lokale Sekretion von immunitätstimulierenden Zytokinen wie Interleukin-2 zur starken Aktivierung des Immunsystem mit therapeutischer Wirksamkeit führte, wurde dieser Ansatz auch im Menschen verfolgt. Diese Untersuchungen lieferten eine der Grundlagen für die späteren, klinisch erfolgreichen Gentherapiestrategien.

Am 14. September 1990 wurde von Ärzten des US-amerikanischen Bundesgesundheitsinstituts an einem vierjährigen Mädchen die weltweit erste gentherapeutische Behandlung durchgeführt. Die Patientin Ashanti DeSilva litt an einem schweren kombinierten Immundefekt (SCID), einer sehr seltenen Krankheit (Inzidenz 1:100.000), verursacht durch einen schweren Defekt sowohl des T- als auch des B-Lymphozytensystems. Bei von diesem Defekt betroffenen Patienten ist das Immunsystem in seiner Funktion erheblich bis vollständig beeinträchtigt, d. h., es gibt wenig oder gar keine Immunantwort – schon eine Erkältungskrankheit kann für die Kinder den Tod bedeuten. Die Gentherapie, die aufgrund der begrenzten Lebensdauer der Leukozyten mehrmals im Jahr wiederholt werden muss, ermöglicht den Patienten ein Leben ohne strikte Quarantäne. Der Gentherapie an Ashanti DeSilva ging ein dreijähriges Genehmigungsverfahren voraus.

Die von den Symptomen her identische Erkrankung X-SCID, die auf Grund von Mutationen in der gemeinsamen Kette einiger Interleukinrezeptoren auftritt (γc, CD132), wurde ebenfalls mit einem gentherapeutischen Ansatz von Alain Fischer in Paris behandelt. Nachdem die Behandlung zunächst größtenteils erfolgreich verlief, traten bei einigen Patienten nach einiger Zeit Leukämien auf (Näheres siehe X-SCID).

1999 erfuhr die Gentherapieforschung einen Rückschlag. Bei einer von der Universität von Pennsylvania durchgeführten, von James M. Wilson geleiteten Versuchsreihe kam es zu schwerwiegenden Komplikationen.

Der 18-jährige Jesse Gelsinger litt am angeborenen Ornithin-Transcarbamylase-Defizit.
Er nahm an der letzten von sechs Versuchsstufen als Proband teil. Die am 13. September bei ihm injizierte Dosis der als Träger verwendeten Adenoviren lag bei 38 Billionen Partikeln. Das ist bedeutend mehr, als bei einer natürlichen Infektion übertragen werden. Der Zustand Gelsingers verschlechterte sich nach der Injektion sehr schnell, so dass er infolgedessen am 17. September an einem Multiorganversagen verstarb. Jesse Gelsinger war zu diesem Zeitpunkt der weltweit sechste, offiziell gemeldete Tote, der durch eine bei Gentherapie-Versuchen angewandte, künstliche Infektion mit Adenoviren verstorben ist. In jedem dieser sechs Fälle soll nach Einschätzung der entsprechenden Versuchsleitung die Grunderkrankung die Ursache für den eingetretenen Tod gewesen sein.

Dem leitenden Arzt James M. Wilson wurde im Anschluss dieses Versuches von der Gesundheitsbehörde der USA jede weitere Forschung am Menschen untersagt. Unter anderem, weil Voraussetzungen, die für einen Versuch am Menschen vorgegeben waren, bewusst nicht eingehalten wurden. So war der Gesundheitszustand von Jesse Gelsinger zu Beginn des Versuches zwar stabil, allerdings überschritten seine Leberwerte die von der Gesundheitsbehörde vorgegebenen Höchstgrenze.

Durch teilweise Entfernung des Corezeptors CCR5 aus dem Genom der CD4-positiven T-Zellen mit einem adoptivem Zelltransfer und "Genome Editing" kann das Absinken der Konzentration der CD4-positiven T-Zellen bei einer HIV-Infektion verlangsamt werden.

Im Oktober 2012 erfolgte als erste Zulassung eines Gentherapeutikums in der westlichen Welt die Zulassung von "Glybera" (Alipogentiparvovec) durch die Europäische Kommission zur Behandlung des seltenen Leidens der familiären Lipoproteinlipasedefizienz (LPLD) bei Erwachsenen. Im November 2015 haben sich der GKV-Spitzenverband und der pharmazeutische Unternehmer Chiesi GmbH (als Lizenznehmer des niederländischen Pharmaunternehmens uniQure) auf einen Erstattungsbetrag für das Arzneimittel geeinigt. Im April 2017 teilte uniQure mit, dass man keine Verlängerung der europäischen Zulassung beantragen wird. Die Zulassung endete am 25. Oktober 2017.

Für die USA war die Zulassung von "Glybera" vergeblich beantragt worden.

Talimogen laherparepvec (Handelsname: "Imlygic", Hersteller: Amgen) und die Autologe CD34+-angereicherte Zellfraktion, die CD34+-Zellen enthält, die mit retroviralem Vektor transduziert wurden, der für die humane ADA-cDNA-Sequenz codiert (Handelsname: "Strimvelis", Hersteller: GSK), sind aktuell in der EU zugelassene Gentherapeutika.

Tisagenlecleucel (CTL019) (Handelsname: "Kymriah"; Hersteller Novartis) ist der erste CAR-T-Zell-Therapie-Wirkstoff, der in den USA von der FDA zu einer "adoptiven" Immuntherapie, konkret zur Behandlung einer bestimmten Form der akuten lymphatischen Leukämie (ALL), zugelassen wurde. Dadurch, dass es sich um den ersten Vertreter eines vollkommen neuen Therapieansatzes handelt, spricht man von "first in class". Es ist die erste Gentherapie, die in den USA zugelassen wurde. Die Zulassung in Europa steht noch aus.

Der Preis für die Kymriah-Behandlung wurde von Novartis auf 475.000 Dollar festgelegt.

Voretigen neparvovec (Handelsname: "Luxturna", Hersteller: Spark Therapeutics) erhielt im Oktober 2017 für die USA eine einstimmige Zulassungsempfehlung seitens des FDA Advisory Committees zur Behandlung von Patienten mit einer erblichen Netzhauterkrankung.

Die Kosten für die Luxturna-Behandlung werden auf 1 Mio. US-Dollar geschätzt.





</doc>
<doc id="1855" url="https://de.wikipedia.org/wiki?curid=1855" title="Genre-Theorie">
Genre-Theorie

Die Genre-Theorie (von frz. "genre" für "Gattung" bzw. "Stil") klassifiziert typische Merkmale erzählerischer literarischer oder filmischer Werke. Explizit dienen die Form der Erzählung, ihre Grundstimmung, ihr Thema und ihre Handlung sowie historische oder räumliche Bezüge der Geschichte als Mittel der Unterscheidung. Den Genres übergeordnet ist der allgemeinere Begriff der Gattung.

Der Begriff Genre als Unterbegriff der Gattung stammt aus der Literaturwissenschaft. Seit den 1970er Jahren hat er sich, von den USA ausgehend, in der Filmwissenschaft etabliert, wird aber auch in der Musikwissenschaft und in der Spielwissenschaft verwendet. Aufgrund der unterschiedlichen Ansätze ist der Begriff vielgestaltig und unterscheidet sich nur bedingt von anderen Allgemeinbegriffen wie Gattung, Format oder Textsorte. 

Der Philosoph Aristoteles teilte in seiner "Poetik" die Dichtung in Epik, Lyrik und Drama ein und definierte damit die ersten Gattungen. Im Deutschen ergibt sich ein Verständnisproblem aus der Tatsache, dass die Begriffe "Genre" und "Gattung" gleichermaßen verwendet werden, je nach Kontext aber unterschiedliche Bedeutungen besitzen. Die Gattung gliedert der Tendenz nach die „hohe“ und das Genre die „niedere“ Kunst, ähnlich wie bei der Unterscheidung zwischen der „niederen“ Genremalerei und der „hohen“ Historienmalerei. Außerdem scheint die Gattung größere Werkgruppen zu bezeichnen als das Genre.

Das englische "genre" wird im Deutschen zwar mit "Gattung" übersetzt und umgekehrt; je nach Kontext und Autor können die beiden Begriffe aber divergierende Bedeutungen annehmen. In der Filmwissenschaft spricht man in der Regel von Genres und meint damit thematisch verbundene Filmgruppen wie Western oder Thriller. In der Literaturwissenschaft ist dagegen meist von Gattungen die Rede, wenn es sich um Bildungsroman oder bürgerliches Trauerspiel handelt. Das Genre ist aus dieser Sicht eher eine Mode, während die Gattung als Ausführung eines theoretischen Programms erscheint. Genretheorie ist den Genres, die sie beschreibt, nicht vorausgesetzt, wie eine Poetik es für die Gattungen zu sein versucht.
So ist es in der deutschsprachigen Filmwissenschaft durchaus üblich, ‚große‘ Gruppen wie Spiel- oder Dokumentarfilm als Gattung zu bezeichnen; in der Germanistik wiederum kommt auch der Ausdruck Genre zum Einsatz, meist als Bezeichnung für besonders stark typisierte, kommerziell ausgerichtete Literaturgattungen außerhalb oder am Rande des Literaturkanons. Der Arztroman ist beispielsweise typische Genreliteratur, der Roman wird dagegen als Gattung bezeichnet.

Auch hier gilt, dass diese Unterscheidung keineswegs überall gemacht wird. Es ist durchaus zulässig, den Kriminalroman als Gattung zu bezeichnen; den Roman als Genre zu betiteln, ist dagegen unüblich. Im Englischen wiederum besitzt "genre" eine ähnlich breite Bedeutung wie "Gattung" im Deutschen: "Gothic Novel", "romance" oder "poetry" sind „genres“. Der unterschiedliche Sprachgebrauch führt nicht selten zu Verwirrung. Für Englischsprachige stellt die Behauptung, der "nonfiction film" sei ein "genre", kein Problem dar, im deutschsprachigen Kontext führt diese Feststellung dagegen meist zu heftigen Diskussionen, weil Genre hier normalerweise als kleinere Einheit verstanden wird.

Wenn die Merkmale eines Genres nicht durch eine Poetik definiert sind wie beim Regeldrama der französischen Klassik, stellen sich bei jeder Zuordnung die Probleme der Induktion: Ein Film wird im Vergleich mit anderen Filmen dem Genre des Westerns oder des Horrorfilms zugeordnet. Es gibt keine vorgegebene Norm als feste Bezugsgröße. Wie soll man jedoch feststellen, zu welchem Genre ein Werk gehört, wenn ein Genre lediglich ein Kanon von Werken ist, den man mehr oder weniger willkürlich festlegen muss, da eine vollständige Beschreibung aller relevanten Merkmale nicht oder nur aus den Werken selbst hergeleitet werden kann? 

Janet Staiger erklärt, es gebe vier Arten, Genres festzulegen, ohne der Wissenschaft, der Kritik, dem Publikum oder dem Markt den Vorzug zu geben: 1. Die Bestimmung eines idealen Vorbilds, mit dem die übrigen Exponenten des Genres verglichen werden. "(idealist method)" 2. Der Versuch, auf inhaltliche Gemeinsamkeiten zu schließen. "(empiricist method)" 3. Die schlichte Festlegung, was zu einem Genre gehören soll "(a priori method)". 4. Die Untersuchung gesellschaftlicher Erwartungen, die zu Genres führen "(social convention method)". Tzvetan Todorov hat 1970 festgestellt, dass historisch gewachsene und theoretisch modellierte Genres selten identisch sind und sich die Vorstellungen der Wissenschaft daher von den Konventionen des Marktes unterscheiden. Seither hat sich auch die Wissenschaft vermehrt mit den Marktstrukturen auseinandergesetzt, aus denen Genre-Begriffe hervorgegangen sind, statt auf eigenen Einteilungen zu beharren.

Erschwerend kommt hinzu, dass jedes Werk sich mit den Genre-Traditionen auseinandersetzt und neue Elemente einführt, beziehungsweise alte weglässt oder uminterpretiert. Sonst wäre es zu vorhersehbar und klischeehaft. Da das Publikum mit einem Genre gewisse Erwartungen verknüpft, ist das Werk stereotyp, wenn sie alle erfüllt, und unverständlich, wenn sie alle enttäuscht werden.

Wenn ein neues Werk unweigerlich neue Elemente einbringen muss, verändert sich die Definition eines Genres im Laufe der Zeit. Genres sind demnach historischen Prozessen unterworfen und beziehen sich immer auf bereits vorhandene Werke, neue können sie nicht vollständig beschreiben. In diesen Prozessen spielen auch gesellschaftliche Veränderungen eine wichtige Rolle, da der Konsens über ein Genre letztlich ein gesellschaftlicher Konsens ist.

Verschiedene Parteien tragen also zur Bildung und Weiterentwicklung von Genres bei: Einmal die Produzenten und Autoren im Versuch, die Publikumsreaktionen zu kalkulieren, zum anderen das Publikum selbst, das eine Erwartungshaltung aufgebaut hat, und nicht zuletzt die Kritiker, die als Motor der Entwicklung dienen und den analytischen Hintergrund liefern.

Die Genre-Theorie beschäftigt sich mit all diesen Fragen und hat komplexe Erklärungssysteme entworfen. Trotzdem bleiben viele Fragen offen, zum Beispiel zum Verhältnis von Autor und Genre.




</doc>
<doc id="1856" url="https://de.wikipedia.org/wiki?curid=1856" title="Goldbachsche Vermutung">
Goldbachsche Vermutung

Die Goldbachsche Vermutung, benannt nach dem Mathematiker Christian Goldbach, ist eine unbewiesene Aussage aus dem Bereich der Zahlentheorie. Sie gehört als eines der Hilbertschen Probleme zu den bekanntesten ungelösten Problemen der Mathematik.

Die starke (oder binäre) Goldbachsche Vermutung lautet wie folgt:

Mit dieser Vermutung befassten sich bis in die heutige Zeit viele Zahlentheoretiker, ohne sie bisher bewiesen oder widerlegt zu haben.

Tomás Oliveira e Silva zeigte mittels eines Volunteer-Computing-Projekts mittlerweile (Stand April 2012) die Gültigkeit der Vermutung für alle Zahlen bis 4·10. Ein Beweis dafür, dass sie für "jede" beliebig große gerade Zahl gilt, ist dies nicht.

Nachdem der britische Verlag Faber & Faber im Jahr 2000 ein Preisgeld von einer Million Dollar für den Beweis der Vermutung ausgelobt hatte, wuchs auch das öffentliche Interesse an dieser Frage. Das Preisgeld wurde nicht ausgezahlt, da bis April 2002 kein Beweis eingegangen war.

Die schwächere Vermutung

ist als "ternäre" oder "schwache" Goldbachsche Vermutung bekannt. Sie ist teilweise gelöst: Denn einerseits gilt sie, wenn die verallgemeinerte Riemannsche Vermutung richtig ist, und andererseits ist gezeigt, dass sie für alle genügend großen Zahlen gilt (Satz von Winogradow, siehe Verwandte Resultate).

Am 13. Mai 2013 kündigte der peruanische Mathematiker Harald Helfgott einen mutmaßlichen Beweis der ternären Goldbachschen Vermutung für alle Zahlen an, die größer als 10 sind. Die Gültigkeit für sämtliche Zahlen unterhalb 8,875·10 ist bereits mit Computerhilfe überprüft worden.

Aus der starken Goldbachschen Vermutung folgt die schwache Goldbachsche Vermutung, denn jede ungerade Zahl formula_1 kann als Summe formula_2 geschrieben werden. Der erste Summand formula_3 ist nach der starken Goldbachschen Vermutung Summe zweier Primzahlen (formula_4), womit eine Darstellung formula_5 von formula_1 als Summe von drei Primzahlen gefunden ist.

Als Goldbach-Zerlegung wird die Darstellung einer geraden Zahl als Summe zweier Primzahlen bezeichnet, beispielsweise ist formula_7 eine Goldbach-Zerlegung der 8. Die Zerlegungen sind nicht eindeutig, wie man an formula_8 ersehen kann. Für größere gerade Zahlen gibt es eine tendenziell wachsende Anzahl von Goldbach-Zerlegungen („mehrfache Goldbachzahlen“). Die Anzahl der Goldbach-Zerlegungen lässt sich mit Computerunterstützung leicht berechnen, siehe Abbildung.

Um die starke Goldbachsche Vermutung zu verletzen, müsste ein Datenpunkt irgendwann auf die Nulllinie fallen.

Die Forderung an eine gerade Zahl formula_9, dass für jede Primzahl formula_10 mit formula_11 auch formula_12 eine Primzahl und somit formula_13 eine Goldbach-Zerlegung ist (die Zahl formula_9 also die maximale Anzahl an Goldbach-Zerlegungen besitzt), erfüllen genau die vier Zahlen 10, 16, 36 und 210. Auch die schwächere Forderung, dass für jede Primzahl formula_10 mit formula_16 auch formula_12 eine Primzahl ist, erfüllt keine Zahl formula_18.

Die Goldbachsche Vermutung zieht immer wieder Amateurmathematiker an. Gelegentlich erhalten solche Beweisversuche auch mediale Aufmerksamkeit.





</doc>
<doc id="1857" url="https://de.wikipedia.org/wiki?curid=1857" title="Daniel Gabriel Fahrenheit">
Daniel Gabriel Fahrenheit

Daniel Gabriel Fahrenheit (* 24. Mai 1686 in Danzig; † 16. September 1736 in Den Haag) war ein deutscher Physiker und Erfinder von Messinstrumenten. Nach ihm wurde die Temperatureinheit Grad Fahrenheit (°F) benannt.

Seine in Danzig wohnenden Eltern waren Daniel (1656–1701) und Concordia Fahrenheit (1657–1701) (geb. Schumann, verw. Runge). Die Mutter kam aus einer bekannten Danziger Kaufmannsfamilie und war die Tochter des Großhändlers Michael Schumann (1624–1673). Daniel war das älteste von fünf Kindern (zwei Söhne, drei Töchter) und überlebte als einziges die ersten Lebensjahre in der Danziger Hundegasse (nach 1945 "Ulica Ogarna 95"). Sein Großvater Reinhold Fahrenheit war von Kneiphof/Königsberg (Preußen) nach Danzig gezogen und hatte sich dort als Kaufmann etabliert. Die Familie stammte vermutlich aus Hildesheim, Daniels Urgroßvater hatte aber in Rostock gelebt, bevor er nach Königsberg gezogen war.

Seine Eltern waren früh verstorben, vermutlich am Verzehr giftiger Pilze. Danach war Fahrenheit gezwungen, eine Kaufmannslehrstelle in Amsterdam anzunehmen. Er unternahm dann weite Reisen und ließ sich 1717 in der niederländischen Stadt Den Haag als Glasbläser nieder, um sich vor allem mit dem Bau von Barometern, Höhenmessern und Thermometern zu befassen. 1718 hielt er in Amsterdam Vorlesungen über Chemie. Am 7. Mai 1724 wurde er zum Fellow der Royal Society gewählt. 

Fahrenheit entwickelte präzise Thermometer mit 3-Punkte-Eichung (Fahrenheit-Skala) und begründete hiermit die Thermometrie. Zunächst verwendete er als Thermometersubstanz Weingeist, ab etwa 1714 auch Quecksilber. Er kam auf den Gedanken, als er eine Arbeit von Guillaume Amontons las, in der die Änderung der Anzeige von Quecksilberbarometern mit der Temperatur beschrieben wurde. Er erfand somit das Quecksilberthermometer; dieses gab es bereits vorher, aber nur durch seine Kalibrierung und seine Herstellungsverfahren wurde es auch allgemein anwendbar. Als Nullpunkt seiner Skala verwendete er die tiefste Temperatur, die er mit einer Eis-Salz-Kältemischung erzeugen konnte: −17,8 °C. 

1721 entdeckte er, dass Wasser erheblich unter seinen Gefrierpunkt abgekühlt werden kann, ohne zu gefrieren. Man spricht dabei von unterkühltem Wasser.

Fahrenheit konstruierte außerdem ein Aräometer, ein Pyknometer und ein Hypsobarometer. Eine Zeit lang war die Fahrenheit-Skala in Deutschland in Gebrauch. In den USA wird heute noch nach Fahrenheit gemessen. 

Autographen von ihm werden unter anderem in der Gottfried Wilhelm Leibniz Bibliothek in Hannover aufbewahrt.





</doc>
<doc id="1858" url="https://de.wikipedia.org/wiki?curid=1858" title="Germanische Sprachen">
Germanische Sprachen

Die germanischen Sprachen sind ein Zweig der indogermanischen Sprachfamilie. Sie umfassen etwa 15 Sprachen mit rund 500 Millionen Muttersprachlern, fast 800 Millionen einschließlich der Zweitsprecher. Ein charakteristisches Phänomen aller germanischen Sprachen gegenüber den anderen indogermanischen Sprachen sind die Veränderungen im Konsonantismus durch die Germanische Lautverschiebung.

Dieser Artikel dient der Gesamtdarstellung der germanischen Sprachen. Auf Untergruppen und einzelne Sprachen und ihre Dialekte wird verwiesen. Die urgermanische Sprache wird in einem separaten Artikel behandelt.
Insgesamt zehn germanische Sprachen besitzen jeweils mehr als eine Million Sprecher.


Die germanischen Sprachen werden in der Regel in "West-, Nord-" und "Ostgermanisch" eingeteilt (siehe unten die ausführliche Klassifikation). Die Sprachgrenze zwischen Nord- und Westgermanisch wird heute durch die deutsch-dänische Grenze markiert und lag früher etwas weiter südlich an der Eider. Innerhalb der beiden großen Sprachgruppen gibt es fließende Übergänge durch lokale Dialekte.

Zu den westgermanischen Sprachen gehören: Englisch, Deutsch, Niederländisch, Afrikaans, Niederdeutsch, Jiddisch, Luxemburgisch, Friesisch und Pennsylvania Dutch.
Dazu gehören: Schwedisch, Dänisch, Norwegisch, Färöisch und Isländisch.
Alle ostgermanischen Sprachen sind ausgestorben. Die bestüberlieferte ostgermanische Sprache ist Gotisch.

Der germanische Zweig des Indogermanischen umfasst heute 15 Sprachen mit zusammen rund 500 Millionen Sprechern. Einige dieser Sprachen werden von manchen Forschern nur als Dialekte betrachtet (siehe unten). Diese 15 Sprachen können nach dem Grad ihrer Verwandtschaft wie folgt klassifiziert werden (die Sprecherzahlen beziehen sich auf Muttersprachler):

Germanisch (15 Sprachen mit insgesamt 490 Millionen Sprechern):

Die Grundlage dieser Klassifikation ist der Weblink „Klassifikation der indogermanischen Sprachen“, der für das Germanische vor allem auf Robinson 1992 basiert. Die aktuellen Sprecherzahlen entstammen Ethnologue 2005 und offiziellen Länderstatistiken.

Da die Grenzen zwischen Sprachen und Dialekten fließend sind, werden z. B. "Luxemburgisch, Plautdietsch, Pennsylvanisch" und "Niederdeutsch" nicht von allen Forschern als "Sprachen" betrachtet, "Schwyzerdütsch" und "Schottisch (Scots)" dagegen von anderen als weitere eigenständige westgermanische Sprachen angesehen. Ein weiteres Beispiel: Die beiden Varianten des "Norwegischen" ("Bokmål" und "Nynorsk") werden von einigen Skandinavisten als separate Sprachen betrachtet, wobei dann Bokmål in die Nähe des Dänischen, Nynorsk in die Nähe des Isländisch-Faröischen rückt.

Während die obige Klassifikation lediglich eine Gliederung der heute existierenden germanischen Sprachen bietet, soll folgende Darstellung einen historischen Einblick vermitteln, da auch die ausgestorbenen germanischen Sprachen aufgeführt werden. Nicht belegte, aber erschließbare Zwischenglieder sind durch * gekennzeichnet. Insbesondere über die historische Gliederung der westgermanischen Sprachen gibt es bisher keinen vollständigen Konsens, die folgende historisch orientierte Darstellung (nach Maurer 1942, Wiesinger 1983, dtv-Atlas Deutsche Sprache 2001) gibt aber die mehrheitlich vertretene Forschungsrichtung wieder. Dabei wird das Westgermanische nicht als ursprüngliche genetische Einheit aufgefasst, es hat sich erst später aus seinen Komponenten Nordseegermanisch, Weser-Rhein-Germanisch und Elbgermanisch durch Konvergenz herausgebildet. Aus dieser Darstellung wird auch klar, dass die Dialekte des Deutschen verschiedenen Zweigen des „Westgermanischen“ angehören, "Deutsch" also nur in Form seiner Dialekte in einen historischen germanischen Stammbaum integrierbar ist.


Die drei historischen Stadien des Hochdeutschen – Althochdeutsch, Mittelhochdeutsch und Neuhochdeutsch – sind nur als Vereinigung von Dialekten greifbar, die verschiedenen Zweigen der obigen Klassifikation angehören.

So ist Althochdeutsch eine Zusammenfassung altmitteldeutscher und altoberdeutscher Dialekte und Dialektgruppen:


Mittelhochdeutsch setzt sich ebenfalls aus mittel- und oberdeutschen Dialekten zusammen:


Das Neuhochdeutsche entwickelte sich aus mittel- und oberdeutschen Dialekten; Details im Artikel Deutsche Sprache.

Seit ungefähr dem 2. Jahrhundert n. Chr. haben die germanischen Stämme eigene Schriftzeichen verwendet, die Runen. Es entstand das sogenannte ältere Futhark, eine frühe Form der Runenreihe, die bis ca. 750 n. Chr. in Gebrauch war. Die überlieferte Gotische Bibel des 4. Jahrhunderts hat ihre eigene Schrift, nämlich das vom Bischof Wulfila entwickelte Gotische Alphabet. Später wurden die germanischen Sprachen mit lateinischen Buchstaben geschrieben. Beispiele von modifizierten Buchstaben sind das Yogh (ȝ) und die latinisierten Runen Thorn (þ) und Wunjo (ƿ).

Die folgenden Tabellen stellen einige Wortgleichungen aus den Bereichen "Verwandtschaftsbezeichnungen", "Körperteile", "Tiernamen", "Umweltbegriffe", "Pronomina", "Verben" und "Zahlwörter" für einige alt- und neugermanische Sprachen zusammen. Man erkennt den hohen Grad der Verwandtschaft der germanischen Sprachen insgesamt, die besondere Ähnlichkeit der westgermanischen und nordgermanischen Sprachen untereinander, die stärkere Abweichung des Gotischen von beiden Gruppen und letztlich die Beziehung des Germanischen zum Indogermanischen (letzte Spalte, hier sind die Abweichungen natürlich größer). Hier können auch die Gesetze der germanischen (ersten) und hochdeutschen (zweiten) Lautverschiebung überprüft werden (ausführliche Behandlung im nächsten Abschnitt). Da die germanischen und indogermanischen Formen nur rekonstruiert sind, sind sie mit einem * versehen.

Folgende Nomina sind in fast allen germanischen Sprachen vertreten und können auch für das Urindogermanische rekonstruiert werden:

<nowiki>**</nowiki> Aue = Mutterschaf (veraltend, landschaftlich)

Es gibt jedoch auch einige germanische Nomina, welche nicht aus dem Urindogermanischen ererbt zu sein scheinen, da sich keine verwandten Wörter in außergermanischen Sprachen finden lassen:

<nowiki>**</nowiki>verwandt ist das Deutsche Verb "gebären"

Fast alle germanischen Zahlwörter sind aus dem Urindogermanischen ererbt:

Quelle dieser Tabellen ist der Weblink „Germanische Wortgleichungen“, der wiederum auf der Basis mehrerer etymologischer Wörterbücher zusammengestellt wurde, darunter Kluge 2002, Onions 1966 und Pokorny 1959.

Die germanischen Sprachen unterscheiden sich von anderen indogermanischen Sprachen durch eine charakteristische, eben die "germanische" Konsonantenverschiebung, die in der Germanistik als "erste" von einer folgenden "zweiten" Lautverschiebung unterschieden wird. Die folgende Tabelle bringt Wortgleichungen, die diesen Übergang von den indogermanischen zu den entsprechenden protogermanischen Konsonanten belegen. Da auch die hochdeutschen Parallelen angegeben sind, belegt die Tabelle auch die Zweite Lautverschiebung vom (Ur-)Germanischen zum Hochdeutschen. Rekonstruierte protogermanische und indogermanische Formen sind durch * gekennzeichnet, entsprechende Konsonanten durch Fettdruck hervorgehoben.

Während z. B. das Lateinische und Griechische die „indogermanischen“ Konsonanten weitgehend erhalten, erfährt das Germanische einen lautgesetzlichen Wandel der Tenues /p, t, k/, Mediae /b, d, g/ und Mediae-Aspiratae /bh, dh, gh/. Das Englische und das Niederdeutsche konservieren bis heute diese „germanischen“ Konsonanten, dagegen erfolgt beim Übergang zum Hochdeutschen eine zweite Lautverschiebung dieser Konsonantengruppe. Insgesamt ergeben sich folgende Lautgesetze:

Germanische und hochdeutsche Lautverschiebung

Einige Forscher vermuten, dass das Protogermanische mit den Vorläufern der baltischen und slawischen Sprachen eine Dialektgruppe innerhalb der west-indogermanischen Sprachen bildete. Diese Annahme wird nicht zuletzt durch eine neuere lexikostatistische Arbeit gestützt. Diese Vorformen des Germanischen könnten bereits im späten 3. und frühen 2. Jahrtausend v. Chr. entsprechend ihrer geographischen Lage eine Zwischenstellung zwischen dem Keltisch-Italischen im Südwesten und dem Baltoslawischen im Südosten eingenommen haben.

Das Protogermanische habe sich dann aus dieser Gruppe gelöst, wonach es deutliche Wechselwirkungen mit frühfinnischen Sprachen zeigt.

Bezüglich einer sogenannten germanischen „Urheimat“ bringt der Onomast Jürgen Udolph das Argument, dass sich germanische Orts- und Gewässernamen mit Schwerpunkt im weiteren Umkreis des Harzes nachweisen lassen. Diese Beobachtung belegt jedoch im Grunde nur eine seit der Benennung ungestörte germanische Besiedlung, nicht deren Zeitrahmen. Einen Zeitrahmen bieten dagegen archäologische Funde auf Grund gleichartiger, ungebrochener Traditionen im Raum zwischen dem von Udolph vorgeschlagenen Harzumland bis Südskandinavien seit etwa dem 12. Jahrhundert v. Chr.

Die protogermanische Sprache (auch „Urgermanisch“ oder „Gemeingermanisch“) konnte durch sprachwissenschaftliche Vergleiche weitgehend rekonstruiert werden. Diese erschlossene Vorform soll bis etwa 100 v. Chr., in der sogenannten "gemeingermanischen" Sprachperiode relativ einheitlich geblieben sein.
Als Eigenheit fällt auf, dass das Germanische einige indogermanische Erbwörter recht eigenwillig verwendet (Beispiel: "sehen" = „[mit den Augen] folgen“, vgl. Lateinisch "sequi").
Nach Euler (2009) spaltete sich als erste Sprache das ausgestorbene, fast nur durch das Gotische überlieferte Ostgermanische ab. Im 1. Jahrhundert n. Chr. hätten sich dann die westgermanischen von den nordgermanischen Sprachen getrennt.

Der protogermanische Wortschatz enthält eine Reihe von Lehnwörtern nicht-germanischen Ursprungs.
Auffallend sind z. B. Entlehnungen im Bereich von Schiffbau und Navigation aus einer bisher unbekannten Substratsprache, vermutlich im westlichen Ostseeraum. Diese germanische Substrathypothese wird allerdings inzwischen stark bestritten.
Dagegen werden Entlehnungen im Bereich sozialer Organisation vor allem keltischem Einfluss zugeschrieben. Diese Beobachtungen legen eine Entstehung des Germanischen als Einwanderersprache nahe.
Wertvolle Hinweise sowohl auf die germanischen Lautformen als auch vorgeschichtliche Nachbarschaftsverhältnisse geben noch heute in ostsee-finnischen Sprachen erhaltene Entlehnungen aus dem Germanischen, wie z. B. finnisch "kuningas" (König) aus Germanisch: *"kuningaz", "rengas" (Ring) aus Germanisch: *"hrengaz" (/z/ steht für stimmhaftes /s/).

Das Germanische kannte ursprünglich weder den bestimmten noch den unbestimmten Artikel, ebenso wie das Lateinische und die meisten slawischen und baltischen Sprachen. Das Westgermanische bildete dann die bestimmten Artikel „der“, „die“ und „das“ aus den Demonstrativpronomen. Die unbestimmten Artikel wurden in den westgermanischen und in den meisten nordgermanischen Sprachen (wie in den romanischen Sprachen) aus dem Zahlwort für „1“ gebildet. Das moderne Isländisch hat keinen unbestimmten Artikel entwickelt.





</doc>
<doc id="1860" url="https://de.wikipedia.org/wiki?curid=1860" title="Goldener Schnitt">
Goldener Schnitt

Als Goldener Schnitt (lateinisch: "sectio aurea", "proportio divina") wird das Teilungsverhältnis einer Strecke oder anderen Größe bezeichnet, bei dem das Verhältnis des Ganzen zu seinem größeren Teil (auch "Major" genannt) dem Verhältnis des größeren zum kleineren Teil (dem "Minor") entspricht. Als Formel ausgedrückt (mit formula_1 als Major und formula_2 als Minor) gilt:

Das mittels Division dieser Größen als Zahl berechnete Teilungsverhältnis des Goldenen Schnittes ist eine irrationale Zahl, das heißt eine Zahl, die sich nicht als Bruch ganzer Zahlen darstellen lässt. Diese Zahl wird ebenfalls als Goldener Schnitt oder auch als "Goldene Zahl" bezeichnet. Als mathematisches Symbol für diese Zahl wird meist der griechische Buchstabe Phi (formula_5, formula_6 oder formula_7), seltener auch Tau (formula_8, formula_9) oder "g" verwendet:

Die Kenntnis des Goldenen Schnittes ist in der mathematischen Literatur seit der Zeit der griechischen Antike (Euklid von Alexandria) nachgewiesen. Vereinzelt schon im Spätmittelalter (Campanus von Novara) und besonders dann in der Renaissance (Luca Pacioli, Johannes Kepler) wurde er auch in philosophische und theologische Zusammenhänge gestellt. Seit dem 19. Jahrhundert wurde er zunächst in der ästhetischen Theorie (Adolf Zeising) und dann auch in künstlerischer, architektonischer und kunsthandwerklicher Praxis als ein ideales Prinzip ästhetischer Proportionierung bewertet. Die Nachweisbarkeit einer derart besonderen ästhetischen Wirkung ist in der Forschung allerdings umstritten, desgleichen die historische Frage, ob der Goldene Schnitt auch schon bei der Proportionierung von Kunst- und Bauwerken älterer Epochen eine Rolle gespielt hat.

Das Verhältnis des goldenen Schnitts ist nicht nur in Mathematik, Kunst oder Architektur von Bedeutung, sondern findet sich auch in der Natur, beispielsweise bei der Anordnung von Blättern und in Blütenständen mancher Pflanzen.

Eine Strecke formula_11 der Länge formula_12 wird durch einen inneren Punkt formula_13 so geteilt, dass das Verhältnis der Länge formula_1 des größeren Teilabschnitts formula_15 zur der Länge formula_2 des kleineren Teilabschnitts formula_17 dem Verhältnis der gesamten Streckenlänge formula_18 zur Länge formula_1 des größeren Teilabschnitts entspricht. Es gilt somit formula_20 beziehungsweise formula_21. Diese Teilung heißt "Goldener Schnitt" der Strecke formula_11. Man spricht dann davon, dass der Punkt formula_13 die Strecke formula_11 im "Goldenen Schnitt" teilt oder auch von der "stetigen Teilung" der Stecke formula_11 durch den Punkt formula_13. Das Verhältnis formula_27 der Streckenabschnitte formula_11 und formula_17 wird "Goldene Zahl" genannt.

Eine einfache Rechnung zeigt:

Wird eine Strecke formula_12 im Goldenen Schnitt geteilt, so gilt für den längeren Abschnitt
und für den kürzeren

Die Goldene Zahl ist eine irrationale Zahl, das heißt, sie lässt sich nicht als Bruch zweier ganzer Zahlen darstellen.
Sie ist jedoch eine algebraische Zahl vom Grad 2, insbesondere kann sie mit Zirkel und Lineal konstruiert werden.

Ferner ist sie besonders schlecht durch Brüche approximierbar. Aus diesem Grund wird sie in der Literatur gelegentlich auch als "irrationalste Zahl" bezeichnet. Diese Eigenschaft wird im Abschnitt Approximationseigenschaften der Goldenen Zahl genauer erläutert.

Als Konstruktionsverfahren werden nach den Postulaten des Euklid nur diejenigen Verfahren akzeptiert, die sich auf die Verwendung von Zirkel und Lineal (ohne Skala) beschränken. Für die Teilung einer Strecke im Verhältnis des Goldenen Schnittes gibt es eine Fülle derartiger Verfahren, von denen im Folgenden exemplarisch nur einige erwähnt werden. Unterschieden wird dabei eine "innere" und "äußere" Teilung. Bei der "äußeren Teilung" wird der in der Verlängerung der Ausgangsstrecke außen liegende Punkt gesucht, der die vorhandene Strecke zum (größeren) Teil des Goldenen Schnittes macht. Der Goldene Schnitt stellt dabei einen Spezialfall der harmonischen Teilung dar. Aufgeführt werden im Folgenden auch zwei moderne, von Künstlern gefundene Konstruktionen.

Anstatt stets neu konstruieren zu müssen, wurde im 19. Jahrhundert von Künstlern und Handwerkern ein "Goldener Zirkel" – ein auf das Goldene Verhältnis eingestellter Reduktionszirkel – benutzt. Insbesondere im Schreinerhandwerk wurde ein ähnliches Instrument in Form eines Storchschnabels benutzt.

Regelmäßiges Fünfeck und Pentagramm bilden jeweils eine Grundfigur, in der das Verhältnis des Goldenen Schnittes wiederholt auftritt. Die Seite eines regelmäßigen Fünfecks z. B. befindet sich im Goldenen Schnitt zu seinen Diagonalen. Die Diagonalen untereinander wiederum teilen sich ebenfalls im goldenen Verhältnis, d. h. formula_35 verhält sich zu formula_36 wie formula_36 zu formula_38. Der Beweis dazu nutzt die Ähnlichkeit geeignet gewählter Dreiecke.

Das Pentagramm, eines der ältesten magischen Symbole der Kulturgeschichte, steht in einer besonders engen Beziehung zum Goldenen Schnitt. Zu jeder Strecke und Teilstrecke im Pentagramm findet sich ein Partner, der mit ihr im Verhältnis des Goldenen Schnittes steht. In der Abbildung sind alle drei möglichen Streckenpaare jeweils blau (längere Strecke) und orange (kürzere Strecke) markiert. Sie lassen sich über das oben beschriebene Verfahren der stetigen Teilung nacheinander erzeugen. Im Prinzip ist es damit in das verkleinerte Pentagramm fortsetzbar, das in das innere Fünfeck gezeichnet werden könnte, und damit in alle weiteren. Stünden die beiden Strecken in einem Verhältnis ganzer Zahlen, müsste dieses Verfahren der fortgesetzten Subtraktion irgendwann Null ergeben und damit abbrechen. Die Betrachtung des Pentagramms zeigt aber anschaulich, dass das nicht der Fall ist. Eine Weiterentwicklung dieser Geometrie findet sich bei der Penrose-Parkettierung.

Für den Beweis, dass es sich um den Goldenen Schnitt handelt, beachte man, dass neben den vielen Strecken, die aus offensichtlichen Symmetriegründen gleich lang sind, auch formula_39 gilt. Ursache ist, dass das Dreieck formula_40 zwei gleiche Winkel besitzt, wie durch Parallelverschiebung der Strecke formula_41 erkannt werden kann, und daher gleichschenklig ist. Nach dem Strahlensatz gilt:

Wird formula_43 ersetzt und die Gleichheit der auftretenden Teilstücke beachtet, so wird genau die obige Definitionsgleichung für den Goldenen Schnitt erhalten.

Ein Rechteck, dessen Seitenverhältnis dem Goldenen Schnitt entspricht, wird als Goldenes Rechteck benannt und ebenso ein gleichschenkliges Dreieck, bei dem zwei Seiten in diesem Verhältnis stehen, als Goldenes Dreieck.


Der Goldene Winkel wird erhalten, wenn der Vollwinkel im Goldenen Schnitt geteilt wird. Dies führt auf den überstumpfen Winkel formula_44
Gewöhnlich wird aber seine Ergänzung zum Vollwinkel, formula_45 als Goldener Winkel bezeichnet. Dies ist dadurch gerechtfertigt, dass Drehungen um ±2π keine Rolle spielen und das Vorzeichen nur den Drehsinn des Winkels bezeichnet.

Durch wiederholte Drehung um den Goldenen Winkel entstehen immer wieder neue Positionen, etwa für die Blattansätze im Bild. Wie bei jeder irrationalen Zahl werden dabei nie exakte Überdeckungen entstehen. Weil die Goldene Zahl im unten beschriebenen Sinn die „irrationalste“ Zahl darstellt, wird dabei erreicht, dass die Überdeckung der Blätter, welche die Photosynthese behindert, in der Summe minimiert wird.

Dabei zerlegen die ersten formula_46 Positionen den Kreis in formula_46 Abschnitte. Diese formula_46 Abschnitte haben höchstens drei verschiedene Winkel.
Im Fall einer Fibonacci-Zahl formula_49 treten nur zwei Winkel formula_50,formula_51 auf.
Für formula_52 tritt der Winkel formula_53 hinzu.

Betrachtet man für wachsendes formula_46 fortfolgend die sich verfeinernden Zerlegungen des Kreises, so teilt die formula_55-te Position stets einen der verbliebenen größten Abschnitte, und zwar immer den im Verlauf der Teilungen zuerst entstandenen, d. h. den "ältesten" Abschnitt. Diese Teilung erfolgt im goldenen Verhältnis, so dass, im Uhrzeigersinn gesehen, ein Winkel formula_56 mit geradem formula_57 vor einem Winkel formula_58 mit ungeradem formula_59 liegt.

Wenn wir den Abschnitt mit dem Winkel formula_51 mit formula_61 bezeichnen, so erhalten wir so nacheinander die Kreiszerlegungen
formula_62
formula_63
formula_64
formula_65
formula_66
formula_67
formula_68
formula_69
formula_70
formula_71
usw.

Die Goldene Spirale ist ein Sonderfall der logarithmischen Spirale. Diese Spirale lässt sich mittels rekursiver Teilung eines Goldenen Rechtecks in je ein Quadrat und ein weiteres, kleineres Goldenes Rechteck konstruieren (siehe nebenstehendes Bild). Sie wird oft durch eine Folge von Viertelkreisen approximiert. Ihr Radius ändert sich bei jeder 90°-Drehung um den Faktor formula_5.

Es gilt: formula_73 mit der Steigung formula_74, wobei formula_75 hierbei der Zahlenwert für den rechten Winkel, also 90° bzw. formula_76 ist, also formula_77 mit der Goldenen Zahl formula_78

Mithin gilt für die Steigung: formula_79.

Die Goldene Spirale ist unter den logarithmischen Spiralen durch die folgende Eigenschaft ausgezeichnet. Seien formula_80 vier auf der Spirale aufeinanderfolgende Schnittpunkte mit einer Geraden durch das Zentrum. Dann sind die beiden Punktepaare formula_81 und formula_82 harmonisch konjugiert, d.h., für ihr Doppelverhältnis gilt 

Die zwölf Ecken des Ikosaeders bilden die Ecken von drei gleich großen, senkrecht aufeinanderstehenden Rechtecken mit gemeinsamem Mittelpunkt und mit den Seitenverhältnissen des Goldenen Schnittes. Diese Anordnung der drei Rechtecke wird auch Goldener-Schnitt-Stuhl genannt.
Weil der Ikosaeder zum Pentagon-Dodekaeder dual ist, bilden die zwölf Mittelpunkte der Fünfecke ebenfalls die Ecken eines Goldener-Schnitt-Stuhls.

Die in der Einleitung angegebene Definition

lautet mit aufgelöster rechter Seite und nach Umstellung

beziehungsweise mit   formula_86   wie folgt:

Multiplikation mit   formula_5   ergibt die quadratische Gleichung

mit den beiden Lösungen   formula_90   und   formula_91

Da von diesen beiden Werten nur der positive für die Goldene Zahl in Frage kommt, folgt

Der Ansatz ist die in der Einleitung angegebene Definition
mit einem "Major" formula_94.

Um den Zahlenwert von formula_5 geometrisch zu finden, stellt man sich die Konstruktion mit einer oben gezeigten Konstruktionsmethode auf einem Zahlenstrahl vor. Wie in der nebenstehenden Darstellung erkennbar, kann hierfür auch das klassische Verfahren mit äußerer Teilung genutzt werden. Die Darstellung zeigt die Zahlenwerte der Schnittpunkte auf dem Zahlenstrahl, die sich im Laufe der Konstruktion ergeben. Weiters zeigt sie das rechtwinklige Dreieck formula_96 mit den Katheten formula_97 und formula_98 sowie die mit Hilfe des Satzes des Pythagoras bestimmte Hypotenuse formula_99.

Der Zahlenwert von formula_5 ist somit auf dem Zahlenstrahl direkt ablesbar:
Zusammengefasst ergibt es ebenfalls

Zu einer gegebenen Zahl formula_103 lässt sich eine Folge formula_104 für formula_105 konstruieren. Diese Folge hat die Eigenschaft, dass je drei aufeinanderfolgende Glieder formula_106 einen Goldenen Schnitt bilden, das heißt, es gilt

Diese Folge spielt in der Proportionslehre in Kunst und Architektur eine wichtige Rolle, weil sich zu einer gegebenen Länge formula_103 weitere dazu harmonisch wirkende Längen erzeugen lassen. Dadurch lassen sich auch Objekte sehr unterschiedlicher Abmessungen, wie Fenster- und Raumbreite, mittels des Goldenen Schnitts in Bezug setzen und ganze Serien untereinander harmonischer Maße erstellen.

In einem engen Zusammenhang zum Goldenen Schnitt steht die unendliche Zahlenfolge der Fibonacci-Zahlen (siehe unten die Abschnitte Mittelalter und Renaissance):

Die jeweils nächste Zahl in dieser Folge wird als Summe der beiden vorangehenden erhalten. Das Verhältnis zweier aufeinanderfolgender Zahlen der Fibonacci-Folge strebt gegen den Goldenen Schnitt (siehe Tabelle). Das rekursive Bildungsgesetz formula_111 bedeutet nämlich

Sofern dieses Verhältnis gegen einen Grenzwert formula_5 konvergiert, muss für diesen gelten

Diese Argumentation gilt auch für verallgemeinerte Fibonacci-Folgen mit zwei beliebigen Anfangsgliedern.

Die Glieder der Fibonacci-Folge formula_115 lassen sich für alle formula_116 über die Formel von Binet berechnen:

mit formula_118

Diese Formel liefert die richtigen Anfangswerte formula_119 und formula_120 und erfüllt die rekursive Gleichung formula_111 für alle formula_46.

Wie weiter oben schon angegeben, ist die Goldene Zahl formula_5 eine irrationale Zahl, das heißt, sie lässt sich nicht als Bruch zweier ganzer Zahlen darstellen. Sie wird manchmal die „irrationalste“ aller Zahlen genannt, weil sie sich (in einem speziellen zahlentheoretischen Sinn) besonders schlecht durch rationale Zahlen approximieren lässt (diophantische Approximation). Dies soll im Folgenden durch einen Vergleich mit der ebenfalls irrationalen Kreiszahl formula_124 illustriert werden. Letztere ist wesentlich besser approximierbar als formula_5, zum Beispiel lässt formula_124 sich durch den Bruch formula_127 mit einer Abweichung von nur zirka 0,00126 approximieren. Ein derartig geringer Fehler wäre im Allgemeinen erst bei einem sehr viel größeren Nenner zu erwarten.
Die Goldene Zahl lässt sich direkt aus der Forderung nach möglichst schlechter Approximierbarkeit durch rationale Zahlen konstruieren. Um das zu verstehen, ist das folgende Verfahren zur Approximation beliebiger Zahlen durch einen Bruch am Beispiel der Zahl formula_124 zu bedenken. Zunächst wird diese Zahl in ihren ganzzahligen Anteil und einen Rest zerlegt, der kleiner als formula_129 ist: formula_130. Der Kehrwert dieses Restes ist eine Zahl, die größer als formula_129 ist. Sie lässt sich daher wiederum zerlegen in einen ganzzahligen Anteil und einen Rest kleiner formula_129: formula_133. Wird mit diesem Rest und allen folgenden ebenso verfahren, dann folgt die unendliche Kettenbruchentwicklung der Zahl formula_124

Wird diese Kettenbruchentwicklung nach endlich vielen Schritten abgebrochen, dann werden formula_124 die bekannten Näherungen formula_137, formula_127, formula_139, formula_140, … erhalten, die rasch gegen formula_124 streben. Für jeden einzelnen dieser Brüche gilt, dass es keinen Bruch mit einem kleineren Nenner gibt, der formula_124 besser approximiert. Dies gilt ganz allgemein:

Im obigen Kettenbruch erscheint vor jedem Pluszeichen eine ganze Zahl. Je größer diese Zahl ist, umso kleiner ist der Bruch, in dessen Nenner sie steht, und umso kleiner ist daher auch der Fehler, der entsteht, wenn der unendliche Kettenbruch vor diesem Bruch abgebrochen wird. Die größte Zahl im obigen Abschnitt des Kettenbruchs ist die formula_147. Das ist der Grund, warum formula_127 eine derart gute Approximation für formula_124 darstellt.

In Umkehrung dieser Argumentation folgt nun, dass die Approximation besonders schlecht ist, wenn die Zahl vor dem Pluszeichen besonders klein ist. Die kleinste zulässige Zahl dort ist aber die formula_129. Der Kettenbruch, der ausschließlich Einsen enthält, lässt sich daher besonders schlecht durch rationale Zahlen approximieren und ist in diesem Sinn die „irrationalste aller Zahlen“.

Für die Goldene Zahl gilt nun aber formula_151 (siehe oben), woraus sich durch wiederholte Anwendung ergibt

Da die Kettenbruchentwicklung der Goldenen Zahl formula_5 also nur Einsen enthält, gehört sie zu den Zahlen, die besonders schlecht rational approximierbar sind. Bricht ihre Kettenbruchentwicklung an irgendeiner Stelle ab, so wird stets ein Bruch aus zwei aufeinanderfolgenden Fibonacci-Zahlen erhalten.

Eine weitere kuriose Bezeichnung ist die folgende: In der Theorie der dynamischen Systeme werden Zahlen, deren unendliche Kettenbruchdarstellung ab irgendeiner Stelle nur noch Einsen enthält, als „noble Zahlen“ bezeichnet. Da die Goldene Zahl nur Einsen in ihrem Kettenbruch hat, kann sie (scherzhaft) als „nobelste aller Zahlen“ bezeichnet werden.





Wird die Strecke in ihrer Länge formula_171 als reelle Zahl formula_1 interpretiert, und die Teilung durch den Goldenen Schnitt im Punkt T in die beiden Teilstrecken und als Zerlegung dieser Zahl formula_1 in zwei Summanden formula_143 und formula_175, so ist formula_143 das geometrische Mittel der Zahlen formula_1 und formula_175. Das folgt aus der allgemeinen Definition des geometrischen Mittels formula_179, hier: formula_180. Des Weiteren folgt daraus unmittelbar, dass formula_1 wiederum das geometrische Mittel von formula_182 und formula_183 ist.

Für jedes beliebige reelle formula_1 lässt sich daher sowohl eine mathematische Folge aufsteigend wie absteigend angeben. Die aufsteigende, wie die absteigende Folge ist jeweils rekursiv definiert.

Für die aufsteigende Folge gilt:
formula_185 mit dem Anfangspunkt formula_186.

Für die absteigende Folge gilt:
formula_187 mit dem Anfangspunkt formula_186.

Die geometrische Verallgemeinerung des Goldenen Schnittes durch seine mehrfache Anwendung ist die stetige Teilung einer Strecke formula_11. Dabei wird die Strecke formula_11 zunächst in eine kleinere Strecke formula_191 und eine größere formula_192 zerlegt. Die Strecke formula_192 (d. h. der größere der entstandenen Streckenabschnitte) wird nunmehr erneut einem Goldenen Schnitt unterzogen, wobei formula_194 als (neuer) größerer Streckenabschnitt und formula_195 als kleinerer verbleiben. Dieser Schritt kann nun unendlich oft wiederholt werden, da auf Grund der mathematischen Eigenschaften des Goldenen Schnittes trotz der fortschreitenden Teilung es keinen Punkt formula_196 geben wird, der mit dem ursprünglichen Punkt formula_197 zusammenfällt.

Dieses allgemeingültige Vorgehen kann aber auch dadurch erreicht werden, dass im Punkt B nach der Konstruktion von A' die Strecke formula_191 abgetragen wird: Der auf diese Weise entstehende Punkt formula_199 ist der gleiche, wie der soeben in der (allgemeinen) Zerlegung beschriebene Punkt formula_199.

Diese Schrittfolge wird als stetige Teilung einer Strecke formula_11 bezeichnet.

Analytisch ist damit die stetige Teilung als Verallgemeinerung des Goldenen Schnittes ein Beispiel von Selbstähnlichkeit: Wird wiederum die entstandenen Längen der Strecken als reelle Zahlen interpretiert, so gilt: Wird die kürzere der beiden Strecken von der längeren subtrahiert, so ist eine noch kürzere Strecke formula_202, zu der die mittlere Strecke formula_2 wiederum im Verhältnis des Goldenen Schnittes, also

Diese Aussage ist analytisch wiederum identisch zu der absteigenden geometrischen Folge des vorangegangenen Abschnittes. Für die Verlängerung einer gegebenen Strecke gilt demzufolge die gleiche Aussage, sie führt zur aufsteigenden geometrischen Folge.

Aus dieser Aussage heraus gilt aber auch: Ein Rechteck mit den Seiten formula_1 und formula_2 entspricht genau dann dem Goldenen Schnitt, wenn das auch für das Rechteck mit den Seiten formula_207 und formula_1 der Fall ist. Ein Goldenes Rechteck lässt sich daher stets in ein kleineres Goldenes Rechteck und ein Quadrat zerlegen. Diese Verallgemeinerung ist wiederum Grundlage für die Konstruktion der (unendlichen) Goldenen Spirale, wie oben beschrieben.

Der Begriff "Goldener Schnitt" wurde erst ab der ersten Hälfte des 19. Jahrhunderts populär, obwohl die mathematischen Prinzipien schon seit der Antike bekannt waren. Auch der Begriff "Goldene Zahl" stammt aus dieser Zeit, noch 1819 wird dieser Begriff mit dem Meton-Zyklus in einem der griechischen Kalendersysteme in Verbindung gebracht.

Die erste erhalten gebliebene genaue Beschreibung des Goldenen Schnittes findet sich im zweiten Buch der "Elemente" des Euklid (um 300 v. Chr.), der darauf über seine Untersuchungen an den platonischen Körpern und dem Fünfeck beziehungsweise dem Pentagramm stieß. Seine Bezeichnung für dieses Teilungsverhältnis wurde später ins Lateinische als „proportio habens medium et duo extrema“ übersetzt, was heute als „Teilung im inneren und äußeren Verhältnis“ bezeichnet wird.

Als historisch gesichert kann heute gelten, dass der Goldene Schnitt bereits vor Euklid bekannt war. Umstritten ist, ob die Entdeckung auf Hippasos von Metapont (spätes 6. Jahrhundert v. Chr.) oder auf Eudoxos von Knidos (um 370 v. Chr.) zurückgeht.

In seinem Rechenbuch "Liber abbaci" (nicht erhaltene Erstfassung 1202, erhaltene 2. Fassung nicht vor 1220), einem umfangreichen arithmetischen und algebraischen Lehrwerk über das Rechnen mit den indo-arabischen Ziffern, kommt der italienische Mathematiker Leonardo da Pisa, genannt „Fibonacci“, kurz auch auf die später nach ihm benannte Fibonacci-Folge zu sprechen, und zwar im Zusammenhang mit der sogenannten Kaninchen-Aufgabe, in der zu errechnen ist, wie viele Kaninchenpaare bei einer Fortpflanzungsrate von einem Paar Jungkaninchen pro Elternpaar und Monat nach Ablauf eines Jahres insgesamt vorhanden sind, wenn ein erstes Paar bereits im ersten Monat und dessen Nachwuchs jeweils ab seinem zweiten Lebensmonat Junge wirft. Leonardo führt die Zahlenfolge für jeden Monat vor (2, 3, 5, 8 … bis 377) und weist darauf hin, dass sich jedes Glied der Reihe (ab dem dritten) durch Summierung der beiden vorhergehenden Reihenglieder errechnen lässt. Eine weitere Beschäftigung mit dieser Folge findet sich bei ihm nicht, d. h. der Zusammenhang zum Goldenen Schnitt wird von ihm nicht dargestellt.

Dass ihm allerdings der (erst später so genannte) Goldene Schnitt bekannt und in der Tradition Euklids ein Begriff war, zeigt sich gegen Ende seines Werks bei einer algebraischen Aufgabe, in der es darum geht (in moderner Formulierung wiedergegeben) "a" und "b" zu finden mit formula_209 und formula_210.

Hierzu weist Leonardo darauf hin, dass im Fall von formula_211 die Proportion formula_212 gilt, 10 also von "a" und "b" im Verhältnis des Goldenen Schnittes (ohne diesen Begriff zu gebrauchen) geteilt wird ("et scis, secundum hanc diuisionem, 10 diuisa esse media et extrema proportione; quia est sicut 10 ad maiorem partem, ita maior pars ad minorem").

Einen Zusammenhang zwischen Fibonacci-Folge und Goldenem Schnitt stellte Leonardo jedoch noch nicht her: Die Entdeckung, dass sich bei Teilung eines Gliedes der Fibonacci-Folge durch das vorhergehende Reihenglied als Näherungswert Φ ergibt, wurde lange Zeit Johannes Kepler zugeschrieben, konnte jedoch in jüngerer Zeit auch schon in einer handschriftlichen Anmerkung nachgewiesen werden, mit der ein mutmaßlich aus Italien stammender Leser in der ersten Hälfte des 16. Jahrhunderts Euklids Theorem II.11 in der Euklid-Ausgabe Paciolis von 1509 kommentierte:

Auch der Herausgeber dieser Euklid-Ausgabe, der Franziskaner Luca Pacioli di Borgo San Sepolcro (1445–1514), der an der Universität von Perugia Mathematik lehrte, hatte sich intensiv mit dem Goldenen Schnitt befasst. Er nannte diese Streckenteilung „göttliche Teilung“, was sich auf Platons Identifizierung der Schöpfung mit den fünf platonischen Körpern bezog, zu deren Konstruktion der Goldene Schnitt ein wichtiges Hilfsmittel darstellt. Sein gleichnamiges Werk "De divina proportione" von 1509 besteht aus drei unabhängigen Büchern. Bei dem ersten handelt es sich um eine rein mathematische Abhandlung, die jedoch keinerlei Bezug zur Kunst und Architektur herstellt. Das zweite ist ein kurzer Traktat über die Schriften des Römers Vitruv aus dem 1. Jahrhundert v. Chr. zur Architektur, in denen Vitruv die Proportionen des menschlichen Körpers als Vorlage für Architektur darstellt. Dieses Buch enthält eine Studie von Leonardo da Vinci (1452–1519) über den vitruvianischen Menschen. Das Verhältnis von Seite des den Menschen umgebenden Quadrats zu Radius des umgebenden Kreises – nicht das Verhältnis der Proportionen des Menschen selbst – in diesem berühmten Bild entspricht mit einer Abweichung von 1,7 % dem Goldenen Schnitt, der jedoch im zugehörigen Buch gar nicht erwähnt wird. Darüber hinaus würde diese Abweichung bei einem konstruktiven Verfahren nicht zu erwarten sein.

Im Oktober 1597 stellte Johannes Kepler in einem Brief an seinen früheren Tübinger Professor Michael Maestlin die Frage, warum es nur eine einzige mögliche Lösung gebe für die Aufgabe, ein rechtwinkliges Dreieck zu konstruieren, bei dem das Verhältnis der kürzeren zur längeren Seite dem der längeren zur Hypotenuse entspricht. Auf das Original dieses Briefes notierte Maestlin eine Berechnung, die die Hypotenuse einmal mit 10 und einmal mit 10.000.000, und für den letzteren Fall dann die längere Seite mit 7.861.514 und die kürzeste Seite mit 6.180.340 beziffert. Das entspricht einer bis auf die sechste Nachkommastelle genauen (und bis zur fünften korrekten) Angabe des Goldenen Schnittes und ist nach den älteren sexagesimalen Berechnungen der Antike die erste bekannte dezimale Angabe dieser Art.

In Abhandlungen verschiedener Autoren im 19. Jahrhundert, insbesondere von dem Philosophen Adolf Zeising, wurden die beiden Schriften von Pacioli und da Vinci zu der These kombiniert, Pacioli habe in der „De Divina Proportione“ in Zusammenarbeit mit Leonardo da Vinci einen Zusammenhang zwischen Kunst und Goldenem Schnitt hergestellt und damit seine Wiederentdeckung für die Malerei der Renaissance begründet. Zeising war überdies von der Existenz eines Naturgesetzes der Ästhetik überzeugt, dessen Basis der Goldene Schnitt sein müsse. Er suchte und fand den Goldenen Schnitt überall. Seine Schriften verbreiteten sich rasch und begründeten eine wahre Euphorie bezüglich des Goldenen Schnittes. Andererseits zeigt eine Untersuchung der Literatur, dass vor Zeising niemand in den Werken der Antike oder Renaissance den Goldenen Schnitt zu erkennen glaubte. Entsprechende Funde sind daher heute unter Kunsthistorikern eher umstritten, wie "Neveux" 1995 nachwies.

Eine der ersten gesicherten Verwendungen der Bezeichnung "Goldener Schnitt" wurde 1835 von Martin Ohm (1792–1872; Bruder von Georg Simon Ohm) in einem Lehrbuch der Mathematik verwendet. Auch die Bezeichnung "sectio aurea" entstand erst in dieser Zeit.

Gustav Theodor Fechner, ein Begründer der experimentellen Psychologie, stellte 1876 bei Untersuchungen mit Versuchspersonen anhand von Rechtecken in der Tat eine Präferenz für den Goldenen Schnitt fest. Die Ergebnisse bei der Streckenteilung und bei Ellipsen fielen jedoch anders aus. Neuzeitliche Untersuchungen zeigen, dass das Ergebnis solcher Experimente stark vom Kontext der Darbietung abhängt. Fechner fand ferner bei Vermessungen von Bildern in verschiedenen Museen Europas, dass die Seitenverhältnisse im Hochformat im Mittel etwa 4:5 und im Querformat etwa 4:3 betragen und sich damit deutlich vom Goldenen Schnitt unterscheiden.

Ende des 20. Jahrhunderts suchte die Kunsthistorikerin Marguerite Neveux mit röntgenanalytischen Verfahren unter der Farbe von Originalgemälden, die angeblich den Goldenen Schnitt enthalten würden, vergeblich nach entsprechenden Markierungen oder Konstruktionsspuren.

Das spektakulärste Beispiel für Verhältnisse des Goldenen Schnittes in der Natur findet sich bei der Anordnung von Blättern (Phyllotaxis) und in Blütenständen mancher Pflanzen.
Bei diesen Pflanzen teilt der Winkel zwischen zwei aufeinander folgenden Blättern den Vollkreis von 360° im Verhältnis des Goldenen Schnittes, wenn die beiden Blattansätze durch eine Parallelverschiebung eines der Blätter entlang der Pflanzenachse zur Deckung gebracht wird. Es handelt sich um den "Goldenen Winkel" von etwa 137,5°.

Die daraus entstehenden Strukturen werden auch als selbstähnlich bezeichnet: Auf diese Weise findet sich ein Muster einer tieferen Strukturebene in höheren Ebenen wieder.
Beispiele sind die Sonnenblume, Kohlarten, Kiefernnadeln an jungen Ästen, Zapfen, Agaven, viele Palmen- und Yuccaarten und die Blütenblätter der Rose, um nur einige zu nennen.

Ursache ist das Bestreben dieser Pflanzen, ihre Blätter auf Abstand zu halten. Es wird vermutet, dass sie dazu an jedem Blattansatz einen besonderen Wachstumshemmer (Inhibitor) erzeugen, der im Pflanzenstamm – vor allem nach oben, in geringerem Umfang aber auch in seitlicher Richtung – diffundiert. Dabei bilden sich in verschiedene Richtungen bestimmte Konzentrationsgefälle aus. Das nächste Blatt entwickelt sich an einer Stelle des Umfangs, wo die Konzentration minimal ist. Dabei stellt sich ein bestimmter Winkel zum Vorgänger ein. Würde dieser Winkel den Vollkreis im Verhältnis einer rationalen Zahl formula_213 teilen, dann würde dieses Blatt genau in die gleiche Richtung wachsen wie dasjenige "n" Blätter zuvor. Der Beitrag dieses Blattes zur Konzentration des Inhibitors ist aber an dieser Stelle gerade maximal. Daher stellt sich ein Winkel mit einem Verhältnis ein, das alle rationalen Zahlen meidet. Die Zahl ist nun aber gerade die Goldene Zahl (siehe oben). Da bisher kein solcher Inhibitor isoliert werden konnte, werden auch andere Hypothesen diskutiert, wie die Steuerung dieser Vorgänge in analoger Weise durch Konzentrationsverteilungen von Nährstoffen.

Der Nutzen für die Pflanze könnte darin bestehen, dass auf diese Weise von oben einfallendes Sonnenlicht (bzw. Wasser und Luft) optimal genutzt wird, eine Vermutung, die bereits Leonardo da Vinci äußerte, oder auch im effizienteren Transport der durch Photosynthese entstandenen Kohlenhydrate im Phloemteil der Leitbündel nach unten. Die Wurzeln von Pflanzen weisen den Goldenen Winkel weniger deutlich auf. Bei anderen Pflanzen wiederum treten Blattspiralen mit anderen Stellungswinkeln zutage. So wird bei manchen Kakteenarten ein Winkel von 99,5° beobachtet, der mit der Variante der Fibonacci-Folge 1, 3, 4, 7, 11, … korrespondiert. In Computersimulationen des Pflanzenwachstums lassen sich diese verschiedenen Verhaltensweisen durch geeignete Wahl der Diffusionskoeffizienten des Inhibitors provozieren.
Bei vielen nach dem Goldenen Schnitt organisierten Pflanzen bilden sich in diesem Zusammenhang so genannte Fibonacci-Spiralen aus. Spiralen dieser Art sind besonders gut zu erkennen, wenn der Blattabstand im Vergleich zum Umfang der Pflanzenachse besonders klein ist. Sie werden nicht von aufeinander folgenden Blättern gebildet, sondern von solchen im Abstand "n", wobei "n" eine Fibonacci-Zahl ist. Solche Blätter befinden sich in enger Nachbarschaft, denn das "n"-fache des Goldenen Winkels formula_214 ist ungefähr ein Vielfaches von 360° wegen

wobei "m" die nächstkleinere Fibonacci-Zahl zu "n" ist. Da jedes der Blätter zwischen diesen beiden zu einer anderen Spirale gehört, sind "n" Spiralen zu sehen. Ist formula_216 größer als formula_5, so ist das Verhältnis der beiden nächsten Fibonacci-Zahlen kleiner und umgekehrt. Daher sind in beide Richtungen Spiralen zu aufeinander folgenden Fibonaccizahlen zu sehen. Der Drehsinn der beiden Spiralentypen ist dem Zufall überlassen, sodass beide Möglichkeiten gleich häufig auftreten.

Besonders beeindruckend sind Fibonacci-Spiralen (die damit wiederum dem Goldenen Schnitt zugeordnet sind) in Blütenständen, wie bei Sonnenblumen. Dort sitzen Blüten, aus denen später Früchte entstehen, auf der stark gestauchten, scheibenförmigen Blütenstandsachse dicht nebeneinander, wobei jede einzelne Blüte einem eigenen Kreis um den Mittelpunkt des Blütenstandes zugeordnet werden kann. Wachstumstechnisch aufeinander folgende Früchte liegen daher räumlich weit auseinander, während direkte Nachbarn wieder einen Abstand entsprechend einer Fibonacci-Zahl haben. Im äußeren Bereich von Sonnenblumen werden 34 und 55 Spiralen gezählt, bei größeren Exemplaren 55 und 89 oder sogar 89 und 144. Die Abweichung vom mathematischen Goldenen Winkel, die in diesem Fall nicht überschritten wird, beträgt weniger als 0,01 %.

Der Goldene Schnitt ist außerdem in radiärsymmetrischen fünfzähligen Blüten erkennbar wie bei der Glockenblume, der Akelei und der (wilden) Hecken-Rose. Der Abstand der Spitzen von Blütenblättern nächster Nachbarn zu dem der übernächsten steht wie beim regelmäßigen Fünfeck üblich in seinem Verhältnis. Das betrifft ebenso Seesterne und andere Tiere mit fünfzähliger Symmetrie.
Darüber hinaus wird der Goldene Schnitt auch im Verhältnis der Längen aufeinander folgender Stängelabschnitte mancher Pflanzen vermutet wie bei der Pappel. Auch im Efeublatt stehen die Blattachsen "a" und "b" (siehe Abbildung) ungefähr im Verhältnis des Goldenen Schnittes. Diese Beispiele sind jedoch umstritten.

Noch im 19. Jahrhundert war die Ansicht weit verbreitet, dass der Goldene Schnitt ein göttliches Naturgesetz sei und in vielfacher Weise auch in den Proportionen des menschlichen Körpers realisiert wäre. So nahm Adolf Zeising in seinem Buch über die Proportionen des menschlichen Körpers an, dass der Nabel die Körpergröße im Verhältnis des Goldenen Schnittes teile, und der untere Abschnitt werde durch das Knie wiederum so geteilt. Ferner scheinen die Verhältnisse benachbarter Teile der Gliedmaßen wie bei Ober- und Unterarm sowie bei den Fingerknochen ungefähr in diesem Verhältnis zu stehen. Eine genaue Überprüfung ergibt jedoch Streuungen der Verhältnisse im 20-%-Bereich. Oft enthält auch die Definition, wie die Länge eines Körperteils exakt zu bestimmen sei, eine gewisse Portion Willkür. Ferner fehlt dieser These bis heute eine wissenschaftliche Grundlage. Es dominiert daher weitgehend die Ansicht, dass diese Beobachtungen lediglich die Folge gezielter Selektion von benachbarten Paaren aus einer Menge von beliebigen Größen sind.

Seit langem ist bekannt, dass die Umlaufzeiten mancher Planeten und Monde in Verhältnis kleiner ganzer Zahlen stehen wie Jupiter und Saturn mit 2:5 oder die Jupitermonde Io, Ganymed und Europa mit 1:2:4. Derartige Bahnresonanzen stabilisieren die Bahnen der Himmelskörper langfristig gegen kleinere Störungen. Erst 1964 wurde entdeckt, dass auch hinreichend irrationale Verhältnisse, wie sie im Fall formula_218 vorliegen würden, stabilisierend wirken können. Derartige Bahnen werden KAM-Bahnen (siehe Kolmogorow-Arnold-Moser-Theorem) genannt, wobei die drei Buchstaben für die Namen der Entdecker Andrei Kolmogorow, V. I. Arnold und Jürgen Moser stehen.

Kontrahierbare kosmische Objekte ohne feste Oberfläche, wie Schwarze Löcher oder auch die Sonne, haben aufgrund ihrer Eigengravitation die paradoxe Eigenschaft, heißer zu werden, wenn sie Wärme abstrahlen (negative Wärmekapazität). Bei rotierenden Schwarzen Löchern findet ab einem kritischen Drehimpuls ein Umschlag von negativer zu positiver Wärmekapazität statt, wobei dieser Tipping-Point von der Masse des Schwarzen Loches abhängt. In einer formula_219-dimensionalen Raumzeit kommt dabei eine Metrik formula_220 ins Spiel, deren Eigenwerte formula_5 für formula_222 sich als Nullstellen des charakteristischen Polynoms
ergeben.

Der Goldene Schnitt tritt auch bei den Quasikristallen der Festkörperphysik in Erscheinung, die 1984 von Dan Shechtman und seinen Kollegen entdeckt wurden. Dabei handelt es sich um Strukturen mit fünfzähliger Symmetrie, aus denen sich aber, wie bereits Kepler erkannte, keine streng periodischen Kristallgitter aufbauen lassen, wie dies bei Kristallen üblich ist. Entsprechend groß war die Überraschung, als bei Röntgenstrukturanalysen Beugungsbilder mit fünfzähliger Symmetrie gefunden wurden. Diese Quasikristalle bestehen strukturell aus zwei verschiedenen rhomboedrischen Grundbausteinen, mit denen der Raum zwar lückenlos, jedoch ohne globale Periodizität gefüllt werden kann (Penrose-Parkettierung). Beide Rhomboeder setzten sich aus den gleichen rautenförmigen Seitenflächen zusammen, die jedoch unterschiedlich orientiert sind. Die Form dieser Rauten lässt sich nun dadurch definieren, dass ihre Diagonalen im Verhältnis des Goldenen Schnittes stehen. Für die Entdeckung von Quasikristallen wurde Shechtman 2011 der Nobelpreis für Chemie verliehen.

Im Buchdruck wurde gelegentlich die Nutzfläche einer Seite, der sogenannte Satzspiegel, so positioniert, dass das Verhältnis von Bundsteg zu Kopfsteg zu Außensteg zu Fußsteg sich wie 2:3:5:8 verhielt. Diese Wahl von Fibonacci-Zahlen approximiert den Goldenen Schnitt. Eine solche Gestaltung wird auch weiterhin in Teilen der Fachliteratur zum Buchdruck empfohlen.

Die folgende Abbildung zeigt im Vergleich verschiedene Rechtecke mit prominenten Seitenverhältnissen in der Umgebung von  Φ=1,618… . Angegeben ist jeweils das Verhältnis von Höhe zu Breite und der entsprechende Zahlenfaktor:


Frühe Hinweise auf eine Verwendung des Goldenen Schnittes stammen aus der Architektur. Die Schriften des griechischen Geschichtsschreibers Herodot zur Cheops-Pyramide werden gelegentlich dahingehend ausgelegt, dass die Höhe der Seitenfläche zur Hälfte der Basiskante im Verhältnis des Goldenen Schnittes stünde. Die entsprechende Textstelle ist allerdings interpretierbar. Andererseits wird auch die These vertreten, dass das Verhältnis formula_224 für Pyramidenhöhe zu Basiskante die tatsächlichen Maße noch besser widerspiegele. Der Unterschied zwischen beiden vertretenen Thesen beträgt zwar lediglich 3,0 %, ein absoluter Beweis zugunsten der einen oder anderen These ist demzufolge damit aber nicht verbunden.

Viele Werke der griechischen Antike werden als Beispiele für die Verwendung des Goldenen Schnittes angesehen wie die Vorderfront des 447–432 v. Chr. unter Perikles erbauten Parthenon-Tempels auf der Athener Akropolis. Da zu diesen Werken keine Pläne überliefert sind, ist nicht bekannt, ob diese Proportionen bewusst oder intuitiv gewählt wurden. Auch in späteren Epochen sind mögliche Beispiele für den Goldenen Schnitt, wie der Dom von Florenz, Notre Dame in Paris oder die Torhalle in Lorsch (770 n. Chr.) zu finden. Auch in diesen Fällen ist die bewusste Anwendung des Goldenen Schnittes anhand der historischen Quellen nicht nachweisbar.

Es gibt demzufolge keinen empirisch gesicherten Nachweis für eine signifikant größere Häufigkeit des Goldenen Schnittes in diesen Epochen im Vergleich zu anderen Teilungsverhältnissen. Ebenso fehlen historische Belege für eine absichtliche Verwendung des Goldenen Schnittes.
Als ein Beispiel für eine Umsetzung des Goldenen Schnittes wird immer wieder das Alte Rathaus in Leipzig, ein Renaissancebau aus den Jahren 1556/57, genannt. Der aus der Mittelachse gerückte Rathausturm wird, so wird behauptet, als architektonische Avantgardeleistung der damaligen Zeit angesehen und er stünde mit dem dadurch verursachten Aufruhr für das städtische Selbstbewusstsein der Stadt. Gleichwohl gibt es bei genauer historischer Quellenforschung keinen Beleg dafür. Insbesondere gibt es keinen Beleg dafür, dass Hieronymus Lotter als der damalige Baumeister den Goldenen Schnitt bewusst als Konstruktionsprinzip verwendet hat: Alle originären Quellen verweisen lediglich auf einen gotischen Vorgängerbau, auf dessen Grundmauern Lotter das Rathaus errichtet hat. Dass der Goldene Schnitt hier eine Rolle gespielt habe, ist quellenhistorisch nicht belegbar.

Die erste quellenhistorisch gesicherte Verwendung des Goldenen Schnittes in der Architektur stammt aus dem 20. Jahrhundert: Der Architekt und Maler Le Corbusier (1887–1965) entwickelte ab 1940 ein einheitliches Maßsystem basierend auf den menschlichen Maßen und dem Goldenen Schnitt. Er veröffentlichte dieses 1949 in seiner Schrift "Der Modulor", die zu den bedeutendsten Schriften der Architekturgeschichte und -theorie gezählt wird. Bereits 1934 wurde ihm für die Anwendung mathematischer Ordnungsprinzipien von der Universität Zürich der Titel "doctor honoris causa" der mathematischen Wissenschaften verliehen. Für eine frühere Verwendung dieses Systems ist dies jedoch aus den aufgezeigten Gründen kein Beleg.

Inwieweit die Verwendung des Goldenen Schnittes in der Kunst zu besonders ästhetischen Ergebnissen führt, ist letztlich eine Frage der jeweils herrschenden Kunstauffassung. Für die generelle These, dass diese Proportion als besonders ansprechend und harmonisch empfunden wird, gibt es keine gesicherten Belege. Viele Künstler setzten den Goldenen Schnitt bewusst ein, bei vielen Werken wurden Kunsthistoriker erst im Nachhinein fündig. Diese Befunde sind jedoch angesichts der Fülle von möglichen Strukturen, wie sie in einem reich strukturierten Gemälde zu finden sind, oft umstritten.

So werden zahlreichen Skulpturen griechischer Bildhauer, wie der Apollo von Belvedere, der Leochares (um 325 v. Chr.) zugeschrieben wird, oder Werke von Phidias (5. Jahrhundert v. Chr.) als Beispiele für die Verwendung des Goldenen Schnittes angesehen. Auf letzteren bezieht sich auch die heute oft übliche Bezeichnung formula_5 für den Goldenen Schnitt, die von dem amerikanischen Mathematiker Mark Barr eingeführt wurde. Die ebenfalls gelegentlich verwendete Bezeichnung formula_9 bezieht sich dagegen auf das griechische Wort "τομή" für „Schnitt“.

Der Goldene Schnitt wird auch in vielen Werken der Renaissance-Künstler vermutet, unter anderem bei Raffael, Leonardo da Vinci und Albrecht Dürer, bei Dürers Werken insbesondere in seinem Selbstbildnis von 1500 und seinem Kupferstich Melencolia I von 1514.

Auch in der Fotografie wird der Goldene Schnitt zur Bildgestaltung eingesetzt. Als Faustformel wird die Drittel-Regel verwendet.

In der zeitgenössischen bildenden Kunst wird der Goldene Schnitt nicht nur als Gestaltungsmerkmal verwendet, sondern ist in manchen Arbeiten selbst Thema oder zentraler Bildinhalt. Der Künstler Jo Niemeyer verwendet den Goldenen Schnitt als grundlegendes Gestaltungsprinzip in seinen Werken, die der konkreten Kunst zugeordnet werden. Die Künstlerin Martina Schettina thematisiert den Goldenen Schnitt in ihren Arbeiten zum Fünfeck, bei welchem die Diagonalen einander im Goldenen Schnitt teilen. Auch visualisiert sie Konstruktionsmethode und Formeln zum Goldenen Schnitt.

In der Musik werden Töne als konsonant empfunden, wenn das Verhältnis ihrer Schwingungsfrequenzen ein Bruch aus kleinen ganzen Zahlen ist. Dass eine Annäherung dieses Verhältnisses zum Goldenen Schnitt hin nicht unbedingt zu einem wohlklingenden Intervall führt, lässt sich daran erkennen, dass unter den Tonintervallen, deren Schwingungsverhältnis aufeinanderfolgenden Fibonacci-Zahlen entspricht, höchstens die Quinte mit einem Schwingungsverhältnis von 3:2 herausragt. Die große Terz mit einem Schwingungsverhältnis von 5:4 wird schon als harmonischer empfunden als die große Sexte mit 5:3 und die kleine Sexte mit 8:5. Da ein Tonintervall im Goldenen Schnitt mit etwa 833,09 Cent nur etwa 19 Cent größer ist als eine kleine Sexte, ist es für ein wenig geschultes Ohr nur schwer von dieser zu unterscheiden ().

Der Goldene Schnitt wird gelegentlich auch in Strukturkonzepten von Musikstücken vermutet. So hat der ungarische Musikwissenschaftler Ernö Lendvai versucht, den Goldenen Schnitt als wesentliches Gestaltungsprinzip der Werke Béla Bartóks nachzuweisen. Seiner Ansicht nach hat Bartók den Aufbau seiner Kompositionen so gestaltet, dass die Anzahl der Takte in einzelnen Formabschnitten Verhältnisse bilden, die den Goldenen Schnitt approximieren würden. Allerdings sind seine Berechnungen umstritten.

In der Musik nach 1945 finden sich Beispiele für die bewusste Proportionierung nach den Zahlen der Fibonacci-Folge, etwa im Klavierstück IX von Karlheinz Stockhausen oder in der Spektralmusik von Gérard Grisey.

Der Goldene Schnitt wird gelegentlich im Musikinstrumentenbau verwendet. Insbesondere beim Geigenbau soll er für besonders klangschöne Instrumente bürgen. So wird auch behauptet, dass der berühmte Geigenbauer Stradivari den Goldenen Schnitt verwendete, um die klanglich optimale Position der F-Löcher für seine Violinen zu berechnen. Diese Behauptungen basieren jedoch lediglich auf nachträglichen numerischen Analysen von Stradivaris Instrumenten. Ein Nachweis, dass Stradivari bewusst den Goldenen Schnitt zur Bestimmung ihrer Proportionen angewandt habe, existiert jedoch nicht.

In der Informatik werden Daten in Hashtabellen gespeichert, um darauf schnell zuzugreifen. Die Position formula_227, an der ein Datensatz formula_228 in der Tabelle gespeichert wird, berechnet sich durch eine Hashfunktion formula_229. Für einen effizienten Zugriff müssen die Datensätze möglichst gleichmäßig verteilt in die Tabelle geschrieben werden. Eine Variante für die Hashfunktion ist die multiplikative Methode, bei der die Hashwerte für eine Tabelle der Größe formula_230 nach der folgenden Formel berechnet werden:

Dabei stellen formula_232 Gaußklammern dar, die den Klammerinhalt auf die nächste ganze Zahl abrunden. Der angesehene Informatiker Donald E. Knuth schlägt für die frei wählbare Konstante formula_233 vor, um eine gute Verteilung der Datensätze zu erhalten.

Das Verfahren des Goldenen Schnittes (auch: "Goldener-Schnitt-Verfahren", "Methode des Goldenen Schnittes" oder "Suchverfahren Goldener Schnitt") ist ein Verfahren der mathematischen nichtlinearen Optimierung, genauer berechnet es algorithmisch eine numerische Näherung für eine Extremstelle (Minimum oder Maximum) einer reellen Funktion einer Variablen in einem Suchintervall formula_234. Es basiert auf der analytischen Anwendung der ursprünglich geometrisch definierten stetigen Teilung. Im Gegensatz zum Intervallhalbierungsverfahren wird dabei das Suchintervall nicht bei jedem Schritt halbiert, sondern nach dem Prinzip des Goldenen Schnittes verkleinert.

Der verwendete Parameter formula_9 ("tau") hat dabei nicht, wie bei dem Bisektionsverfahren, den Wert formula_236, sondern es wird formula_237 gewählt, sodass sich zwei Punkte formula_238 und formula_239 für das Optimierungsverfahren ergeben, die das Suchintervall im Goldenen Schnitt teilen.

Wird angenommen, dass jeder Punkt in jedem Intervall mit gleicher Wahrscheinlichkeit Extrempunkt sein kann, führt dies bei Unbestimmtheitsintervallen dazu, dass das Verfahren des Goldenen Schnittes z. B. um 14 % effektiver ist als die Intervallhalbierungsmethode. Im Vergleich zu diesem und weiteren sequentiellen Verfahren ist es – mathematisch gesehen – das für allgemeine Funktionen effektivste Verfahren; nur im Fall differenzierbarer Funktionen ist es der direkten mathematischen Lösung unterlegen. Dass sich dieses Verfahren in der manuellen Rechnung nicht durchgesetzt hat, liegt vor allem an den notwendigen Wurzelberechnungen für die einzelnen Zwischenschritte.

Eine weitere Verbindung zwischen der Informationstheorie und dem Goldenen Schnitt wurde durch Helmar Frank mit der Definition der Auffälligkeit hergestellt. Er konnte zeigen, dass der mathematische Wert des Maximums der Auffälligkeit sehr nah an das Verhältnis des Goldenen Schnitts herankommt.




Deutsch


Englisch


</doc>
<doc id="1862" url="https://de.wikipedia.org/wiki?curid=1862" title="Grunge">
Grunge

Grunge (deutsch ‚Schmuddel‘, ‚Dreck‘) ist ein vor allem in den 1990er-Jahren populärer Musikstil, dessen Ursprünge und Anfänge in der US-amerikanischen Undergroundbewegung lagen. Grunge wurde auch als "Seattle-Sound" bezeichnet und wird oft als eine Vermischung von Punkrock und Heavy Metal angesehen.

Der „Grunge-Sound“ beruht hauptsächlich auf dem Gitarrensound des Hard Rock der 1970er-Jahre (deutlicher Einfluss zum Beispiel bei Pearl Jam) und der Ästhetik und den Lyrics des Punkrock. Während sich einige der Bands mehr in Richtung Metal (Soundgarden, Alice in Chains) bewegten und andere sich mehr dem Punkrock-Einfluss (Nirvana, Mudhoney, 7 Year Bitch) verschrieben hatten, so war tatsächlich bei allen Grunge-Bands aus Seattle der charakteristische „Seattle-Sound“ aufzufinden.

Der Begriff "Grunge" kam als Umschreibung für einen Musiktyp erstmals in den 1960er- und 1970er-Jahren auf, um den Stil der Musik einiger Bands zu beschreiben. Damals wurde Grunge weniger als "Subgenre" gesehen, sondern als besonders markantes aber passendes Eigenschaftswort für einen rau und „dreckig“ wirkenden Klang. Neil Young (& Crazy Horse), The Stooges und The Velvet Underground beispielsweise fielen zu dieser Zeit im Vergleich zu anderen Bands des Rock-Genres besonders auf, da sie ziemlich experimentierfreudig waren, was "Feedback"-Effekte anging – akustische Rückkopplungen, besonders der E-Gitarre. Zudem wirkte ihr Gitarrenspiel oft weniger „sauber“ und „glatt“ als das der musikalischen Referenzen dieser Jahrzehnte. Vielmehr sollte der Klang roh und ungeschliffen wirken, was durch Einsatz von Verzerreffekten hervorgehoben wurde. Auch wurden die Aufnahmen in der Regel wenig bis gar nicht im Studio bearbeitet. Dadurch wirkte die Musik generell „unkonventionell“ und „unabhängig“. Dieser Stil prägte den Sound der Musikszene aus Seattle, welche Ende der 1980er- und Anfang der 1990er-Jahre aufkam. Erst seit dieser Zeit und durch kontinuierlich steigende Popularität von Bands wie Nirvana, Pearl Jam, Alice in Chains oder Soundgarden wurde „Grunge“ auch als Begriff für ein Subgenre der Rockmusik verwendet.
Zu den ersten Bands, die in diesem Bereich experimentierten, gehörten Wipers und Mission of Burma, deren Stil in den späten 1970ern und frühen 1980ern dem vorherrschenden Punk-Publikum zu rockig oder, im Falle von Mission of Burma, zu komplex war. Später folgte Hüsker Dü, deren Ursprünge in der Punkszene lagen. Sie kombinierten die Energie des Punk-Rock mit dem komplexeren Songwriting des Rock und erreichten damit Mitte der 1980er-Jahre ein größeres Publikum. Einen Schritt weiter bewegten sich Dinosaur Jr., die mit ihrer Nähe zum klassischen Rock im Stil von Neil Young und einem extrem übersteuerten Gitarrensound mit Wah-Wah-Effekten, im Stil des Garagenrock der 1960er-Jahre auffielen. Weitere Einflüsse waren Sonic Youth, Big Black, Butthole Surfers und andere Vertreter des Noise-Rock. Die Veröffentlichungen ganzer Labels wie zum Beispiel Homestead Records, SST Records oder Amphetamine Reptile waren ausschlaggebend. Prägend für die Szene waren College-Rundfunksender, die diese Independent-Musik oft spielten, sowie der eher provinzielle Charakter der Region um Seattle, einer Gegend, in der nur unbekannte Musiker eine Auftrittsmöglichkeit suchten.

Mitte der 1980er bildete sich ein Kern in der Szene Seattles, zu dem neben Green River, Soundgarden und The U-Men auch The Melvins gehörten. Zu diesem Zeitpunkt entstand auch die Bezeichnung "Grunge" als Genrebegriff.
1988 wurde in Seattle der "Sub Pop Singles Club" gegründet – ein Plattenlabel, auf dem die ersten Aufnahmen lokaler Bands wie Tad, Mudhoney, Nirvana und Soundgarden erschienen. Ein großer Teil der Aufnahmen wurde von Jack Endino produziert, und es kristallisierte sich ein Klang heraus, den man für geeignet hielt, als "„Seattle-Sound“" vermarktet zu werden. Ein Jahr später wurde ein Rockjournalist des britischen "Melody-Maker"-Magazins auf diese Produktionen aufmerksam, worauf im März 1989 der Artikel "„Seattle, Rock City“" erschien. In Seattle sorgte dieser Artikel für große Aufregung. Der Rest der Welt zeigte zunächst nur mäßiges Interesse.

Das änderte sich schlagartig, als im September 1991 das Album Nevermind von Nirvana erschien. Auslöser des "Grunge"-Hypes war der Song "Smells Like Teen Spirit", der auf ebendiesem Album erschienen war. Dank des häufigen Einsatzes des Musik-Videos beim Musik-Fernsehkanal MTV avancierte er zum Hit. Die Musikindustrie und die Medien entwickelten fortan ein ausgeprägtes Interesse für die Musikszene in Seattle. Das Magazin "Spin" beschrieb es in der Dezember-Ausgabe von 1992 mit den Worten: "„Seattle ist momentan für die Rockwelt, was Bethlehem für das Christentum ist.“" Bands ohne Plattenvertrag wurden plötzlich unter Vertrag genommen. Andere Bands, die schon bei einem "Independent-Label" unter Vertrag waren, wurden per Vertriebsverträge an die großen Plattenfirmen weitergereicht. Die Kommerzialisierung der Szene ging schnell vonstatten. Markante Bekleidungsstücke wie das Flanellhemd wurden als neue Mode verkauft. Dabei waren Flanellhemden im Alltag der Region von Seattle seit Jahrzehnten ein beliebtes Kleidungsstück bei Jung und Alt. So wurde ein banaler Alltagsgegenstand zum "Dresscode" der neuen "Grunge"-Szene.

Nicht nur durch den Tod des Sängers Kurt Cobain von Nirvana im April 1994 fand "Grunge" ein jähes Ende. Ursache waren auch die immer größer werdenden persönlichen oder künstlerischen Differenzen innerhalb einiger Bands, die durch den Druck von außen (Presse, Medien) zusätzlich angeheizt wurden. Überdies hatte eine ungewöhnlich hohe Zahl an Grunge-Musikern Drogenprobleme, was Bands wie Alice in Chains oder die Stone Temple Pilots erheblich belastete.

Der "Grunge"-Hype war bei den Kritikern ein beliebtes Angriffsziel. Die Entwicklung zeigt, wie die Musikindustrie mit einer Handvoll Bands und deren Auftreten in Bezug auf Aussehen und Attitüde einen "Hype" produzierte, von dem sie noch heute zehrt. Doch neben dem erwünschten Aspekt, dem Eintreten in den Fokus der öffentlichen Wahrnehmung, kamen auch negative Aspekte hinzu. So kam es, dass sich nach dem Tode Cobains die meisten Grunge-Bands auflösten oder aus dem Blickfeld der breiteren Masse verschwanden.

Sämtliche Metal-Genres hatten mit dem Aufstieg des Grunge an Popularität verloren. Auf Nachfrage äußerte sich 1994 Joey DeMaio von Manowar über die Grunge-Szene: „Gibt es ein Wort für etwas, das rangmäßig noch unter Scheiße steht?“ Ob daraus der Frust über eigenen Popularitätsverlust spricht oder ob es sich dabei um eine für die Band typische derbe Wahrung ihrer True-Metal-Attitüde handelt, sei dahingestellt.

Dass Grunge so einen Rundumschlag bewirkte und den Massengeschmack so sehr veränderte, kam also auch für viele Musiker damals überraschend. Neben Frust und negativer Kritik gab es jedoch auch positive Stimmen dazu, etwa von John Such, dem ehemaligen Bassisten von Bon Jovi, der Grunge als „erfrischend“ lobte, oder auch von Sebastian Bach, der den neuen, anderen Klang begrüßte. Dass der Grunge etwa 1997 aus ähnlichen Gründen zu Grunde ging wie zuvor der Glam Metal, ist wohl Ironie. Der Metal Hammer-Redakteur und Autor Frank Thiessies fügt dazu süffisant an, dass heute Grunge, wenn auch als Alternative Rock betitelt, und Sleaze Rock einträchtig nebeneinander existieren, wenn auch nicht wie zu den jeweiligen Glanzzeiten und wirft in den Raum, wieso denn niemand die Frage stellt, ob der Nu Metal dem Rock und Metal nicht noch viel schwerer zu schaffen gemacht hätte. Bedenklich erscheint aber die bereits Mitte der 1990er-Jahre große Distanz der Hörer zum Glam Metal. Viele schienen nun nicht mehr zugeben zu wollen, dass ihnen diese „stumpfe, klischeesatte, aber einfach umwerfende Räubermusik“ noch einige Jahre zuvor so wichtig gewesen war.


Bekannte Filme, die etwas mit der Grunge-Musik aus Seattle direkt zu tun haben oder beinhalten, sind z. B.:



</doc>
<doc id="1863" url="https://de.wikipedia.org/wiki?curid=1863" title="Generation X (Roman)">
Generation X (Roman)

Generation X ist der Titel des 1991 erschienenen Episodenromanes des kanadischen Schriftstellers und Künstlers Douglas Coupland. Übersetzer ist Harald Riemann.

Die Protagonisten des Romans, Andrew Palmer – er hat an der Universität Japanisch studiert –, Dagmar Bellinghausen, ehemaliger Marketingexperte, und Claire Baxter, Tochter aus reichem Hause, wohnen in Mietbungalows in der kalifornischen Stadt Palm Springs. Sie schlagen sich mit schlecht bezahlten Jobs durch, für die sie alle drei überqualifiziert sind: einer arbeitet als Parfümverkäufer in einem Warenhaus, die beiden anderen sind Barkeeper. Die „Mitglieder des Armut-Jet-Sets“, wie sie sich nennen, unterhalten sich, indem sie sich Geschichten erzählen: Geschichten aus ihrem Leben, Familiengeschichten, aber auch Phantasiegeschichten und Lebensträume, denen sie nachhängen und Schreckensvisionen von einem möglichen Nuklearschlag, von denen die amerikanische Gesellschaft dieser Zeit umgetrieben wird. Und sie verbringen viel Zeit vor dem Fernsehapparat.
Am Ende des Romans sind die Drei auf dem Weg nach Mexiko, um dort ein Hotel zu eröffnen.

Der Text gliedert sich in drei Teile mit insgesamt 32 kurzen Kapiteln. Das letzte Kapitel "Zahlen" gehört nicht mehr zur Romanhandlung, sondern enthält statistische Daten zu Bevölkerung und Umwelt.

Nach Couplands Einschätzung ist für diese Generation charakteristisch, dass sie sich erstmals ohne Kriegseinwirkung mit weniger Wohlstand und ökonomischer Sicherheit begnügen muss als die Elterngenerationen, aber andererseits für deren ökonomische und ökologische Sünden büßt. Der Roman erzählt „Geschichten von der Katerstimmung im Amerika nach der auf Pump veranstalteten letzten großen Sause unter Reagan und Bush“ "(Deutschlandfunk)" über eine Generation mit „zu vielen Fernsehern und zu wenig Arbeit“ (Newsweek). Coupland kritisiert mit seinem Schlüsselroman die Wohlstandsgesellschaft der Vorgänger-Generation, die „mit 30 stirbt, um mit 70 begraben zu werden“. Ursprünglich sollte der Begriff "Generation X" andeuten, dass sich diese Generation bislang erfolgreich der Benennungswut von Werbeindustrie und journalistischem Gewerbe entzogen hat. Couplands Buch erreichte die Bestsellerlisten und der Titel wurde zum Schlagwort für die bis dahin unbenannte Generation.

Aus Couplands Erzählstil gingen neben dem Titel noch weitere Vokabeln in den allgemeinen Sprachgebrauch über. Eigentlich hätte Coupland ein Lifestyle-Lexikon über die „Twentysomethings“ schreiben sollen. Der ehemalige Kunststudent kam jedoch von der Idee eines unterhaltsamen Sachbuchs ab und legte stattdessen einen anekdotenhaft erzählten Roman vor, dessen Helden sich weigern "„kleine Monster so scharf auf einen Hamburger [zu] machen, dass ihre Begeisterung auch über ihr Kotzen hinaus anhält“". Erste Skizzen aus dem ursprünglichen Projekt wurden als Marginalien in den Roman eingearbeitet, die an passender Stelle Couplands Wortschöpfungen in einem „Lexikon der nicht funktionierenden Kultur“ erklären sollen. So prägte Coupland auch den Begriff McJob, im Roman definiert als "„ein niedrig dotierter Job im Dienstleistungsbereich mit wenig Prestige, wenig Würde, wenig Nutzen und ohne Zukunft. Oftmals als befriedigende Karriere bezeichnet von Leuten, die niemals eine gemacht haben“".

Coupland stellt dem eingeschliffenen Lebensstil aus gesellschaftlichen und ökonomischen Zwängen eine "„Lessness“" genannte Philosophie gegenüber, die den Wert des Lebens nicht an der Anhäufung von Statussymbolen misst. Das 'neue' Wertsystem wird auch ironisch als „Exhibitionistische Bescheidenheit“ bezeichnet. Aufgrund dieses Lebensgefühls der Konsumverweigerung würde Couplands "Generation X" (z. B. von der "Seattle Times") in Anlehnung an Gertrude Stein auch als „Lost Generation der Neunziger“ bezeichnet. Coupland stützt seine Beobachtungen am Ende des Buches mit einigen Statistiken und Zitaten aus verschiedenen Zeitschriften.

Seit den frühen 1950er Jahren wurde der Begriff "Generation X" verschiedentlich in Zusammenhängen mit der jeweils zeitgenössischen Jugend gebraucht; der Begriff lag also sozusagen „in der Luft“. Siehe dazu den Abschnitt "Begriffsgeschichte" im Artikel zum gleichnamigen soziologischen Begriff.

Coupland selbst lieferte unterschiedliche Erklärungen, wer ihn zum Titel seines Buchs angeregt hat. Coupland, damals Kunststudent, sollte eigentlich im Auftrag eines kanadischen Verlages ein Sachbuch über die amerikanische Jugend der Zeit schreiben, lieferte aber stattdessen einen Roman ab. Allerdings ergänzte er ihn um ein Glossar der Neologismen der beschriebenen „Szene“. Das Buch wurde vom Verlag abgelehnt, dann aber von Simon & Schuster publiziert, entwickelte sich nach zögerlichem Start zu einem Bestseller, und der Autor avancierte in der öffentlichen Wahrnehmung zu einem Sprecher der Befindlichkeiten seiner Generation und Diagnostiker des Zeitgeists.

Einige wenige aus der großen Anzahl von Definitionen im Buch. Diese sind nicht in den allgemeinen Sprachgebrauch übergegangen, beschreiben aber das Lebensgefühl der Generation X, so wie Coupland es sieht:

Couplands Wortneuschöpfungen wurden von den Medien und Marketingfirmen der USA begierig aufgegriffen und als „Leitfaden für die Kategorisierungen und das Verständnis der schwer zu fassenden Alterskohorte genommen“. Noch heute dient das Schlagwort dem Verlag selbst als Marketingmittel, wenn er sein Marketingbuch über die sogenannten Baby-Boomer (geboren zwischen 1943 und 1960) unter dem Titel "Marketing to Generation X. Strategies for a new aera" publiziert.

Der Spiegel bewertete die literarische Qualität des Romans eher zurückhaltend, hält den Autor aber für einen „intelligenten Beobachter des Alltags“ und „Meister darin, prägnante Etikette für aktuelle Phänomene zu vergeben“. Ohne Glossar hätte nach Meinung des Spiegels das Buch nicht zu einen weltweit gelesenen und zitierten Kultbuch werden können. Der Roman wurde in 16 Sprachen übersetzt, mehreren Auflagen gedruckt und allein in den USA bis 1994 über 300 000 Mal verkauft.



Der Begriff Generation X wird auch in der Soziologie oder im Marketing für eine Bevölkerungskohorte bezeichnet. Daraus abgeleitet wurde die Generation Y.


</doc>
<doc id="1864" url="https://de.wikipedia.org/wiki?curid=1864" title="Allgemeines Zoll- und Handelsabkommen">
Allgemeines Zoll- und Handelsabkommen

Das Allgemeine Zoll- und Handelsabkommen (, GATT) wurde am 30. Oktober 1947 abgeschlossen, als der Plan für eine Internationale Handelsorganisation (ITO) nicht verwirklicht werden konnte. Das Abkommen trat am 1. Januar 1948 in Kraft.

Das GATT von 1947 begründete keine Internationale Organisation, sondern war ein völkerrechtlicher Vertrag, weshalb seine 23 Gründungsmitglieder (Australien, Belgien, Brasilien, Burma, Kanada, Ceylon, Chile, Republik China, Kuba, Frankreich, Indien, Libanon, Luxemburg, Neuseeland, Niederlande, Norwegen, Pakistan, Südrhodesien, Südafrikanische Union, Syrien, Tschechoslowakei, Vereinigtes Königreich sowie USA) auch als „Vertragsparteien“ und nicht als Mitgliedsstaaten bezeichnet wurden. Die Bundesrepublik Deutschland trat am 1. Oktober 1951 diesem Vertragssystem bei. Österreich gehört dem GATT seit 19. Oktober 1951 an. Die Schweiz trat 1966 als Vollmitglied bei. Alle Mitglieder der Welthandelsorganisation (WTO) sind auch Vertragspartner des GATT. Sitz des GATT-Sekretariats war, bis zu seiner Ablösung durch die WTO 1995, Genf. Die WTO als Dachorganisation des GATT hat auch heute noch ihren Hauptsitz dort.

Es stellt eine internationale Vereinbarung über den Welthandel dar. Bis 1994 wurden in acht Verhandlungsrunden Zölle und andere Handelshemmnisse Schritt für Schritt abgebaut. Durch das GATT ist im Verlauf der Geschichte der Grundstein zur Gründung der Welthandelsorganisation (WTO 1995) gelegt worden, in die es heute noch eingegliedert ist. Damals gehörten dem Abkommen 123 gleichberechtigte Mitgliedsländer an. Zur Unterscheidung zwischen dem ursprünglichen und dem heutigen Übereinkommen im Rahmen der WTO wird in der Regel die Jahreszahl 1947 bzw. 1994 hinzugefügt.

Der Beginn des GATT liegt 1944 in den USA, als die Bretton-Woods-Konferenz stattfand, an der 44 Staaten teilnahmen. Diese ist für die Einrichtung eines festen Wechselkurssystems verantwortlich, gründete den Internationalen Währungsfonds (IWF) sowie die Weltbank. In einem Punkt jedoch konnte keine Einigung erzielt werden: Bei der Gründung einer Welthandelsorganisation. Stattdessen entwickelte die Bretton-Woods-Konferenz ein Vertragswerk, das 1948 in Kraft trat: Das "Allgemeine Zoll- und Handelsabkommen (GATT)".

Durch das GATT wurde festgelegt, dass Zölle, Abgaben und andere Hemmnisse im internationalen Handel abgebaut werden müssen. Dadurch sollten Welthandel und Weltwirtschaft gefördert werden.
Hieraus resultierten zwei Prinzipien: Erstens die Meistbegünstigungsklausel (Prinzip der Gleichbehandlung), bei der allen Handelspartnern eines Landes gleiche Zollvergünstigungen gewährt werden. Zweitens das Verbot der Diskriminierung, bei dem erlassene Ausnahmen vom Verbot mengenmäßiger Beschränkungen für alle gelten. Des Weiteren sollte durch GATT ein Prozess zur Lösung von internationalen Handelskonflikten etabliert werden. Die Maßnahmen konzentrierten sich vornehmlich auf den Güterhandel. Ausnahmen von den GATT-Prinzipien sind auch möglich, wie zum Beispiel vom Meistbegünstigungsprinzip innerhalb einer Zollunion oder Freihandelszone, wie etwa der Europäischen Union. Auch Nachbarländern und Entwicklungsländern können besondere Handelspräferenzen eingeräumt werden. 

Benachteiligungen beim Handel sollen im Wesentlichen durch drei Prinzipien verhindert werden:

Art. XIV erlaubt Ausnahmen vom Meistbegünstigungsprinzip.

Art. XII erlaubt Beschränkungen zum Schutz der Zahlungsbilanz.

Art. XIX erlaubt Notstandsmaßnahmen bei der Einfuhr bestimmter Waren, um zu verhindern, dass inländischen Erzeugern ernsthafter Schaden zugefügt wird. Diese Ausnahme wurde unter GATT 1947 häufig angewandt, ist in GATT 1994 jedoch durch ein zusätzliches Übereinkommen stärker reglementiert.

Art. XXV:5 erlaubt unter außergewöhnlichen, nicht vorgesehenen Umständen, dass eine Vertragspartei von einer Verpflichtung befreit wird. Über eine solche Ausnahme entscheiden die Vertragsparteien mit Zweidrittelmehrheit.

Art. XX regelt allgemeine Ausnahmen. Unter dem Vorbehalt, dass es nicht willkürlich stattfindet oder zu einer verschleierten Beschränkung des internationalen Handels führt, dürfen die Vertragsparteien unter anderem folgende Maßnahmen durchführen:

Und unter bestimmten Bedingungen:

Art. XXIV regelt Ausnahmen von Freihandelszonen und Zollunionen.

Von 1948 bis 1994 wurden durch das GATT die Regeln für einen Großteil des Welthandels festgelegt. In dieser Zeit gab es acht mehrjährige Verhandlungsrunden (u. a. in Frankreich, Großbritannien, Belgien und Marokko). Diese kontinentübergreifenden Treffen nahmen 1948 in Havanna (Kuba) ihren Anfang. Es stellte sich heraus, dass GATT das einzige multilaterale Instrument war, um den internationalen Handel kontrollieren zu können. In den Anfangsjahren konzentrierten sich die Handelsrunden auf Senkung der Zölle. Die sogenannte "Kennedy-Runde" (1964–1967) ergab ein Anti-Dumping-Abkommen zur Vermeidung von Preisverfall und eine Lektion zur Entwicklung. 
In den 70ern stellte die "Tokio-Runde" den ersten bedeutenden Versuch dar, sich internationalen Handelsbarrieren zu widersetzen. Als letzte und umfangreichste Verhandlungsrunde führte die "Uruguay-Runde" (1986–1994) zur Gründung der WTO und einem neuen Katalog von Vereinbarungen.

Die "Tokio-Runde" (1973–1979) stellte einen ersten Versuch dar, das internationale Handelssystem zu reformieren. An diesen Verhandlungen beteiligten sich 102 Länder. Es wurden Bemühungen fortgesetzt die Handelszölle weiter zu senken. Ein Resultat war die durchschnittliche Senkung der Zollgebühren für industriell gefertigte Produkte auf rund 4,7 %. Dies geschah nach dem Prinzip der Verhältnismäßigkeit: je höher der Zoll, umso höher die Kürzung. 

In anderen Verhandlungsschwerpunkten traten Unstimmigkeiten zwischen den Teilnehmern auf. Ein Streitpunkt war die Reformierung des landwirtschaftlichen Handels. Dennoch einigte man sich über eine Reihe von Abkommen, die zollfreie Staatsgrenzen betrafen. In vielen Fällen unterzeichnete nur eine geringe Zahl von GATT-Mitgliedern diese Vereinbarungen und Absprachen. Diese nicht akzeptierten Abkommen waren nicht multilateral und wurden inoffiziell als "Codes" bezeichnet. Mehrere Codes wurden in der "Uruguay-Runde" überarbeitet und in multilaterale Verpflichtungen umgewandelt.

Der Grundstein zur "Uruguay-Runde" wurde im November 1982 auf einem Treffen der Abgesandten in Genf gelegt. Das verabschiedete Arbeitsprogramm wurde zur Basis der in Uruguay verhandelten Agenda. Im September 1986 begannen in Punta del Este (Uruguay) die Verhandlungen. Sie beinhalteten ausstehende handelspolitische Probleme. Das Handelssystem wurde auf mehrere neue Bereiche ausgedehnt, um den Handel in den sensiblen Bereichen Landwirtschaft und Textilindustrie zu verbessern.
Neben diesen Punkten wurden alle ursprünglichen GATT-Texte besprochen. Dieser bis dahin größte Verhandlungsauftrag sollte über einen Zeitraum von vier Jahren vervollständigt werden. Nach der Hälfte der Zeit fand ein Treffen der Beauftragten in Montreal (Kanada, 1988) statt, um die Fortschritte des Auftrages zu bewerten. In diesen Gesprächen konnte jedoch keine Einigung erzielt werden. Aus diesem Grund fand im folgenden April ein erneutes Treffen der Offiziellen in Genf statt.
Am Ende der Verhandlungen stand ein Paket von Beschlüssen fest. 

Um die Entwicklungsländer zu fördern, wurden Zugeständnisse in der Markttransparenz für tropische Produkte gemacht. Zur schnellen Beseitigung von Streitigkeiten unter den Handelspartnern wurden ein Konfliktsystem sowie ein Prüfmechanismus für die Handelspolitik eingeführt. In landwirtschaftlichen Fragen wurden dagegen kaum Ergebnisse erzielt, und man beschloss, die Gespräche später fortzusetzen. Letztendlich entstand ein erster Entwurf der endgültigen rechtlichen Übereinkunft, der „Final Act“. Dieser wurde von dem späteren Generaldirektor der GATT Arthur Dunkel übersetzt und im Dezember 1991 in Genf vorgelegt. Der Text erfüllte alle Aspekte der "Uruguay-Runde" mit einer Ausnahme: Die Liste der Verpflichtungen zum Beschneiden der Importsteuern und Öffnung ihrer Dienstleistungsmärkte (siehe auch GATS, engl. "General Agreement on Trade in Services"). Der Entwurf wurde zur Basis für den endgültigen Beschluss.

Im Juli 1993 gaben die USA, Japan, die EU und Kanada bekannt, dass wichtige Fortschritte in den Zollverhandlungen ähnlicher Bereiche (Marktzugang) erzielt wurden. Es dauerte bis zum 15. Dezember 1993, bis jedes Problem gelöst wurde und die Verhandlungen über den Marktzugang von Gütern und Dienstleistungen endeten. Das Abkommen wurde am 15. April 1994 in Marrakesch (Marokko) von den Abgeordneten der 123 teilnehmenden Staaten unterzeichnet.

Die im "Allgemeinen Zoll- und Handelsabkommen" beschlossenen Aktionen stellten lediglich ein Provisorium dar. Dennoch konnte 47 Jahre lang die Liberalisierung des Welthandels gefördert und gesichert werden. Die fortwährende Minimierung der Zölle regte zwischen den 50er und 60er Jahren sehr hohe Wachstumsraten des Welthandels an. Infolgedessen konnte ein durchschnittliches Wachstum von 8 % pro Jahr erreicht werden. Während der GATT-Ära überstieg das Handelswachstum durch die Handelsliberalisierung das Produktionswachstum. Die Stabilität dieses Handelssystems verursachte einen Anstieg der Mitgliederzahl seit der "Uruguay-Runde". Die Reformen bewirkten eine nachhaltige Verbesserung und Entwicklung der internationalen Wirtschafts- und Handelsbeziehungen.

Für die Gründung der WTO wurden die Errungenschaften der GATT-Verhandlungen als Dachvertrag genutzt und bestehen noch heute. Die GATT-Regelungen setzen sich aus den ursprünglichen Verträgen von 1947 (GATT 1947), den aktualisierten Satzungen der "Uruguay-Runde" und den abschließenden Änderungen von 1994 ((GATT 1994), Marrakesch) zusammen. Heutzutage kontrolliert die WTO den internationalen Güterhandel.




</doc>
<doc id="1866" url="https://de.wikipedia.org/wiki?curid=1866" title="Gibraltar">
Gibraltar

Gibraltar ( [], []) ist ein britisches Überseegebiet an der Südspitze der Iberischen Halbinsel. Es steht seit 1704 unter der Souveränität des Vereinigten Königreichs und wurde 1713 von Spanien offiziell im Frieden von Utrecht abgetreten, jedoch seitdem von Spanien beansprucht.

Gibraltar ist eine Halbinsel, die die Bucht von Algeciras östlich begrenzt, und liegt an der Nordseite der Meerenge Straße von Gibraltar, an der Europa und Afrika sich am nächsten sind. Das Territorium umfasst eine Landfläche von 6,5 km, wobei die Grenze zwischen Gibraltar und Spanien nur 1,2 Kilometer lang ist. Auf der spanischen Seite der Grenze liegt die Stadt La Línea de la Concepción. Die von Gibraltar beanspruchte Meeresfläche reicht bis zu drei Seemeilen vor die Küste.

Gibraltar besteht aus einem flachen, größtenteils sandigen Gebiet und dem Felsen von Gibraltar. Der an der Ostseite spektakulär steil aus dem Meer aufragende Kalksteinfelsen "(engl. Upper Rock, span. Peñón)" fällt schon von weitem über der Bucht von Algeciras ins Auge. Er ist von Nord nach Süd etwa vier Kilometer lang und bis zu 1,2 Kilometer breit. Die Spitze des Felsens erreicht eine Höhe von 426 m. Er besteht hauptsächlich aus dem im Jura gebildeten Kalkstein und ist damit älter als die benachbarten südspanischen Felsen. Der flache Teil Gibraltars konnte durch Landgewinnung etwas vergrößert werden. Das Material stammt zum großen Teil aus dem Inneren des Felsens, wo es beim Bau der insgesamt etwa 50 Kilometer Tunnel anfiel. Neben den künstlichen Hohlräumen besitzt der Felsen eine ganze Reihe von natürlich entstandenen Höhlen.

Das Wetter in Gibraltar wird wesentlich durch den Levante (Ostwind) und den Poniente (Westwind) bestimmt. Diese lokalen Winde entstehen durch das Atlas-Gebirge im Süden und die Sierra Nevada im Norden.
Gibraltar gliedert sich in das Naturschutzgebiet "Upper Rock," das Stadtgebiet, die Ostseite und den zu Gibraltar gehörenden Teil des Mittelmeeres, insbesondere der Bucht von Gibraltar.

Das Naturschutzgebiet wurde am 1. April 1993 gegründet und ist heute für Touristen gegen Gebühr zu besichtigen.

Die Stadt Gibraltar erstreckt sich auf dem schmalen Streifen der Westseite, auf der der Felsen flacher zum Meer abfällt. Während die Westseite stark bevölkert ist, leben auf der Ostseite nur wenige Menschen in den beiden Dörfern Catalan Bay und Sandy Bay. Im Norden der Halbinsel befinden sich an der Grenze zu Spanien der Flughafen, einige militärische Einrichtungen und ein Friedhof für Gefallene aus den Weltkriegen. Im Nordwesten ist ein modernes, mit Hochhäusern bebautes Viertel entstanden, in dem auch eine Marina und Terminals für Fähren gebaut wurden. Südlich davon findet sich am Ufer der Militärhafen und ein Industriegebiet, in dem zum Beispiel einige Trockendocks vorzufinden sind. Das touristische Zentrum im Westen ist die "Main Street" und die umliegenden Straßen und Plätze, die teilweise autofrei sind.

Da es keine natürlichen Süßwasservorkommen gibt, wurde lange Zeit Regenwasser aufgefangen und, wo möglich, Salzwasser verwendet. So entstand beispielsweise 1908 ein 130.000 m² großes Auffangbecken für Regenwasser auf der Ostseite der Halbinsel, das allerdings inzwischen abgebaut wurde. Heute wird das benötigte Süßwasser durch Meerwasserentsalzung produziert.

Neben dem Naturschutzgebiet "Upper Rock" ist auch das gesamte Meeresgebiet von Gibraltar seit dem 1. Januar 1996 unter Schutz gestellt.

Gibraltar ist der einzige Ort in Europa, an dem Affen (Tierart: Berberaffe oder Magot, "Macaca silvanus") freilebend vorkommen. Deswegen nennt man Gibraltar auch den „Affenfelsen“. Die Affen werden zwar allgemein als freilebend bezeichnet, führen aber eher eine Art von Leben im Wildpark, bei dem sie regelmäßig von Menschen gefüttert werden.


Gibraltar ist eines der am dichtesten besiedelten Gebiete der Erde. 32.577 Personen wohnen in Gibraltar. Die Bevölkerungsdichte beträgt 5012 Einwohner pro Quadratkilometer (2012), die unbesiedelten Gebiete von Upper Rock mitgerechnet. Mittels Landgewinnung wird versucht, der Platznot Herr zu werden.

Überalterung ist seit den 1990er-Jahren ein immer größer werdendes Problem. Die Lebenserwartung der Bewohner liegt bei 78,5 Jahren für Männer, und 83,3 Jahren für Frauen. Die Geburtenrate liegt bei jährlich 10,67 Geburten pro 1000 Einwohner. Auf eine Frau kommen im Schnitt 1,65 Neugeborene. Die Kindersterblichkeit liegt bei 0,483 %. Das Bevölkerungswachstum ist mit 0,11 % pro Jahr sehr niedrig.

Quelle: UN

Die meisten Einwohner Gibraltars sind britischer, spanischer, italienischer oder portugiesischer Herkunft. Alle Gibraltarer haben einen britischen Pass. Die Ausländerbehörde stellt Einwanderern zusätzlich zu ihrer alten Staatsbürgerschaft einen britischen Pass für Gibraltar aus. Gemäß einer Analyse der Familiennamen im Wählerregister von 1995 waren 27 % britischer, 26 % spanischer (meist andalusischer, jedoch 2 % menorquinischer), 19 % italienischer, 11 % portugiesischer, 8 % maltesischer, 3 % israelischer Herkunft. Weitere 4 % kamen aus anderen Staaten, während bei 2 % die Herkunft nicht eruierbar war.

Der Großteil der Bevölkerung ist mit über 78 Prozent katholisch. Das Gebiet Gibraltars bildet das Bistum Gibraltar; als Nationalheiligtümer werden die Kathedrale St. Mary the Crowned und das Heiligtum Unserer Lieben Frau von Europa angesehen. An zweiter Stelle folgt konfessionell die Anglikanische Kirche mit rund sieben Prozent der Bevölkerung. Die Cathedral of the Holy Trinity ist Bischofskirche der Diözese in Europa der Church of England für ganz Kontinentaleuropa. Für die vier Prozent Muslime steht mit der Ibrahim-al-Ibrahim-Moschee eine der größten Moscheen Europas als Versammlungsraum zur Verfügung. Weiter wohnen in Gibraltar auch Angehörige weiterer christlicher Konfessionen (3 %), Juden (2 %), Hindus (2 %) und Anhänger mehrerer anderer Religionen.

Einzige Amtssprache Gibraltars ist Englisch, die meisten Einwohner sprechen daneben auch Spanisch. Obgleich nur Englisch offiziellen Charakter besitzt, sind viele Verkehrs-, Straßen- und Hinweisschilder zusätzlich in spanischer Sprache beschriftet. Darüber hinaus sprechen viele Einwohner als Umgangssprache Llanito, einen Dialekt, der größtenteils auf andalusischem Spanisch basiert, jedoch auch einige Elemente des Englischen und verschiedener südeuropäischer Sprachen enthält.

Natürliche Höhlen im Felsen von Gibraltar gelten als die letzten Rückzugsgebiete der Neandertaler in Europa. Gesicherte Spuren weisen auf eine Besiedlung der Gorham-Höhle noch vor etwa 28.000 Jahren hin.

Im Altertum galt Gibraltar als eine der Säulen des Herakles. Römische Spuren in Gibraltar (lat. „Mons Calpe“) sind nicht bekannt. Den Römern folgten die Westgoten, die sich der Iberischen Halbinsel bemächtigten.

711 wurde Gibraltar von den muslimischen Arabern und Berbern eingenommen. Der Name "Gibraltar" stammt aus dem Arabischen ( "Dschabal Ṭāriq", „Berg des Tarik“), nach Tāriq ibn Ziyād, einem maurischen Feldherrn, der die strategische Bedeutung Gibraltars für die Eroberung Spaniens erkannte und als erster Muslim ein Stück Spaniens eroberte. Um etwa 1160 entstand eine erste Festung in Gibraltar, die in den kommenden Jahrhunderten ausgebaut wurde und heute als "Moorish Castle" bekannt ist. Die Muslime beherrschten Gibraltar bis zur Reconquista 1462 (von 1309 bis 1333 erstmals kastilisch durch Ferdinand IV.).

Am 25. April 1607 fand während des Achtzigjährigen Krieges die Schlacht bei Gibraltar statt. Dabei überraschte eine niederländische Flotte eine in der Bucht von Gibraltar ankernde spanische Flotte und vernichtete sie.

Nachdem die spanischen Habsburger die Vorherrschaft in Europa am Ende des Dreißigjährigen Krieges 1648 verloren hatten, kämpften Niederländer und Engländer um die Kontrolle der Ozeane. Dies war die Zeit der vier Englisch-Niederländischen Seekriege, die in der Zeit zwischen 1652 und 1784 stattfanden. So wurde beispielsweise der Zweite Englisch-Niederländische Seekrieg dadurch ausgelöst, dass ein niederländischer Geleitzug im Dezember 1664 in der Straße von Gibraltar von den Engländern überfallen wurde. Zwischen diesen Auseinandersetzungen kam es immer wieder zu Friedensschlüssen und gemeinsamen Aktionen gegen Dritte. Eine dieser gemeinsamen Aktionen war die Eroberung Gibraltars am 4. August 1704 durch Prinz Georg von Hessen-Darmstadt im Spanischen Erbfolgekrieg an Bord der englisch-holländischen Flotte unter Admiral Sir George Rooke. Die spanische Besatzung wurde dabei in Abwandlung militärischer Taktik nicht im Morgengrauen, sondern während der Siesta am Nachmittag überrascht. Die anschließende Belagerung Gibraltars durch Spanien blieb erfolglos. 1713 wurde das Gebiet im Vertrag von Utrecht formell den Briten zugesprochen und ist seit 1830 britische Kronkolonie. Während des Englisch-Spanischen Krieges von 1727–1729 belagerten Truppen von Philipp V. vergeblich Gibraltar. Zwischen 1779 und 1783 versuchten spanische und französische Truppen erneut, die Festung zu erobern (Great Siege). In dieser Zeit wurden die ersten Tunnel, die sogenannten "Great Siege Tunnels", gegraben.

Während des Zweiten Weltkrieges wurde die Zivilbevölkerung Gibraltars umgesiedelt. In dieser Zeit wurde der Felsen in eine unterirdische Festung für bis zu 15.000 Soldaten umgewandelt. Die Tunnel, die sogenannten "World War II Tunnels", können heute in Teilen besichtigt werden. Ziel dieser Befestigung war es, einem möglichen Angriff der deutschen Wehrmacht begegnen zu können. Diese hatte auch mit einem ersten Operationsentwurf vom 20. August 1940 die Eroberung des Stützpunktes geplant. Das Unternehmen Felix wurde jedoch nie durchgeführt, da Spanien neutral blieb. Bei einem Vergeltungsschlag für die britische Operation Catapult bombardierten Luftstreitkräfte des restfranzösischen Vichy-Regimes Gibraltar am 24. und 25. September 1940 und versenkten dabei einen Hilfskreuzer im Hafen. Vor Beginn der anglo-amerikanischen Invasion Französisch-Nordafrikas, der Operation Torch, schlug der US-amerikanische General Eisenhower sein Hauptquartier am 5. November 1942 in Gibraltar auf. Drei Tage später begann die Invasion Marokkos mit 300.000 Soldaten. Letzten Endes blieb Gibraltar der einzige Teil des nichtneutralen westeuropäischen Festlands, der zu keiner Zeit von Deutschland oder seinen Verbündeten besetzt war.

Vor Gibraltar kam der Premierminister der polnischen Exilregierung, General Sikorski, bei einem Flugzeugunglück am 4. Juli 1943 ums Leben.

Die Straße von Gibraltar, die das Mittelmeer mit dem Atlantik verbindet, ist für das Militär von großer Bedeutung. Das Vereinigte Königreich unterhält in Gibraltar einen Flottenstützpunkt.

Seit langem kommt es zu Spannungen zwischen dem Vereinigten Königreich und Spanien, weil Spanien die Hoheit über Gibraltar wiedererlangen möchte. Die Grenze nach Spanien war von 1969 bis 1985 geschlossen. Seit 1946 steht das Territorium auf der UN-Liste der Hoheitsgebiete ohne Selbstregierung. Bei einem Referendum am 7. November 2002 (Wahlbeteiligung: fast 90 %) stimmten 99 % der Abstimmenden für einen Verbleib unter britischer Herrschaft. Nur 187 Bewohner waren für eine geteilte Souveränität.

Am 18. September 2006 schlossen der Außenminister Spaniens und der Europaminister des Vereinigten Königreichs sowie der Chief Minister of Gibraltar Peter Caruana in Córdoba einen Vertrag zur Zusammenarbeit. Darin wird festgelegt, dass ein neues Terminal für den Flughafen Gibraltar gebaut wird, sodass der Flughafen auch von spanischer Seite aus genutzt werden kann. Außenminister Spaniens war damals Miguel Ángel Moratinos (Kabinett Zapatero I), Europaminister des Vereinigten Königreichs war Geoff Hoon (Kabinett Blair III). Ab dem 16. Dezember 2006 gab es (zum ersten Mal seit Jahrzehnten) einen Linienflug von Spanien nach Gibraltar (Näheres hier). Außerdem wurden Regelungen über das Telefonnetz, die Entschädigung von spanischen Arbeitern, die nach der Schließung der Grenze 1969 ihre Arbeit verloren hatten, und eine Erleichterung der Grenzkontrollen auf der Landseite getroffen. Weiterhin soll eine Dépendance des Instituto Cervantes in Gibraltar eröffnet werden. Am 21. Juli 2009 kam Außenminister Moratinos als erster Vertreter der spanischen Regierung seit Beginn der britischen Souveränität über Gibraltar zu einem offiziellen Besuch nach Gibraltar.

Gibraltar ist ein Überseeterritorium des Vereinigten Königreichs. Es hat eine eigene Regierung, die die Aufgaben der Selbstverwaltung erfüllt. Sie umfasst alle Bereiche außer Verteidigung, Außenpolitik und innere Sicherheit, die vom Vereinigten Königreich übernommen werden. Staatsoberhaupt ist die britische Königin; sie wird in Gibraltar durch einen Gouverneur repräsentiert. Der Gouverneur ist gleichzeitig der Oberbefehlshaber der Armee und der Polizei. Der amtierende Gouverneur Edward Davis wurde im Januar 2016 ernannt.

Im November 2006 stimmten über 60 Prozent der gibraltarischen Bevölkerung für eine neue Verfassung, die größere Eigenständigkeit vorsieht, insbesondere im Justizwesen.

Seit 1704, als die Englische Krone die Herrschaft über die Halbinsel erlangte und im Vertrag von Utrecht 1713 zugesichert bekam, versucht Spanien, die britische Kolonie zurückzuerlangen. Im 18. Jahrhundert wurde dies mit militärischen Mitteln versucht, nämlich in den drei Belagerungen von 1704, 1727 und 1779–1783, allesamt erfolglos. Im 19. Jahrhundert waren weitere militärische Aktionen gegen Großbritannien infolge seiner weltweiten politischen und militärischen Dominanz aussichtslos und unterblieben daher. Erst in den 1950er Jahren unternahm der spanische Diktator Francisco Franco neue Versuche, Gibraltar zu annektieren, wobei er auch den Exilpräsidenten Spaniens, Claudio Sánchez Albornoz, auf seiner Seite hatte. Seither fanden mehrere Verhandlungsrunden statt, die aber zu keiner abschließenden Lösung führten. In zwei Volksabstimmungen, in denen Gibraltar über einen Wechsel zu Spanien entschied, wurden die Vorschläge überaus deutlich verworfen: Am 10. September 1967 mit 12.138 zu 44 Stimmen und am 7. November 2002 mit 17.900 zu 187 Stimmen. 2002 war nur über eine gemeinsame britisch-spanische Ausübung der Souveränitätsrechte über Gibraltar abgestimmt worden. Durch verschiedene Repressionen hatte sich Spanien bei den Einwohnern Gibraltars unbeliebt gemacht, darunter die jahrelange komplette Schließung der Grenze (vom 9. Juni 1969 bis zum 4. Februar 1985), danach oft lange Wartezeiten am Grenzübergang, Beschränkungen beim Zugang zu Telekommunikationsmitteln oder Versuche, Gibraltars Bevölkerung von der Teilnahme an internationalen Sportanlässen auszuschließen. Zwischen 2009 und 2011 gab es auch kleinere Grenzzwischenfälle in den Hoheitsgewässern.

Spanien war zwar von den Vereinten Nationen in dem Bestreben, die Souveränität über Gibraltar zu erlangen, unterstützt worden, da Gibraltar offiziell noch eine aufzulösende Kolonie ist, nach diesen Abstimmungen stellte Jim Murphy, britischer Minister, klar, dass das Vereinigte Königreich nichts ohne die explizite Zustimmung der Gibraltarer tun würde. Außerdem sei der Status Gibraltars umstritten und somit auch sein Status als Kolonie. Inzwischen sieht die UNO Gibraltar als ein bilaterales Problem zwischen Großbritannien und Spanien an und überlässt es diesen Staaten, eine Lösung zu finden.

Trotz der verbesserten Zusammenarbeit zwischen Spanien und dem britischen Überseegebiet herrscht bis in die heutige Zeit Uneinigkeit über die jeweiligen Hoheitsrechte vor der Küste Gibraltars. Spanien erkennt nur eine kleine Zone rund um den Hafen als britisch an und beruft sich dabei auf den Vertrag von Utrecht, das Vereinigte Königreich hingegen beansprucht unter Berufung auf dieselbe Urkunde eine Drei-Meilen-Zone, was wiederholt zu Zusammenstößen zwischen der spanischen Guardia Civil und britischen Patrouillenbooten führte. Am 18. November 2009 beobachtete die Guardia Civil, wie ein Schnellboot der britischen Marine sieben Seemeilen südlich von Gibraltar Schießübungen auf eine Boje mit der spanischen Fahne durchführte; der britische Botschafter Giles Paxman entschuldigte sich wenig später für „mangelndes Urteilsvermögen und fehlende Sensibilität“ der Schiffsbesatzung. Am 7. Dezember 2009 fuhr ein Boot der Guardia Civil bei der Verfolgung mutmaßlicher Drogenschmuggler bis in den Hafen von Gibraltar. Die spanischen Sicherheitskräfte machten dort zwar die zwei Insassen des flüchtenden Schnellbootes dingfest, wurden aber ihrerseits von der Gibraltar Squadron festgenommen. Der spanische Innenminister Alfredo Pérez Rubalcaba entschuldigte sich wenig später bei Gibraltars Chief Minister Peter Caruana für das „nicht korrekte Verhalten“ seiner Beamten. Diese wurden noch am selben Tag wieder auf freien Fuß gesetzt.

Ende Juli 2013 ließ die Regierung von Gibraltar 70 je drei Tonnen schwere eisenbewehrte Betonklötze im Meer versenken. Fischer protestierten gegen die Klötze. Premierminister David Cameron bat etwa drei Wochen später in einem Telefongespräch mit EU-Kommissionspräsident José Manuel Barroso „dringend“ um die Entsendung von EU-Beobachtern an die Grenze.

Juristisch umstritten ist auch der Verlauf der Landgrenze. "De facto" bildet der 1909 von Großbritannien errichtete Zaun, der etwa 800 Meter nördlich des Nordhangs des Felsens von Gibraltar verläuft, die Grenze. Dieser Grenzverlauf wird offiziell von Spanien nicht anerkannt, da nach dem Vertrag von Utrecht nur „die Stadt und die Burg von Gibraltar nebst dem zugehörigen Hafen und den zugehörigen Verteidigungsanlagen und Befestigungen“ abgetreten wurden. Das umstrittene Gebiet wird heute zum größten Teil vom Flughafen von Gibraltar eingenommen. Anders als die Frage der Küstengewässer hat die Landgrenze jedoch in den letzten Jahrzehnten keinen Anlass von tatsächlichen Auseinandersetzungen gegeben. Spanischerseits wird die de-facto-Grenze jedoch nicht als „Grenze“, sondern als "la verja" („der Zaun“) bezeichnet.

Die Bevölkerung Gibraltars wählt das siebzehnköpfige Gibraltar Parliament. Jeder Wähler hat zehn Stimmen. Eine Einteilung in Wahlkreise gibt es nicht. Da eine Personenwahl stattfindet, ist die Vertretung der Parteien nicht notwendigerweise proportional. Derzeit sind drei Parteien im Parlament vertreten.

Der von einer Mehrheit unterstützte Kandidat wird vom Gouverneur zum Regierungschef (Chief Minister) ernannt. Außer dem Chief Minister besteht die Exekutive noch aus dem Finanzminister und dem Justizminister.

Bei den Parlamentswahlen am 8. Dezember 2011 erhielt die Gibraltar Socialist Labour Party (GSLP) sieben Mandate, die Gibraltar Social Democrats (GSD) des früheren Chief Minister Peter Caruana trotz erheblich höherer Stimmenzahl ebenso nur sieben Sitze. Die Gibraltar Liberal Party (Libs) ist mit drei Sitzen vertreten und bildet mit der GSLP eine Koalition, sodass die GSD derzeit in der Opposition ist.

Die GSLP stellt mit Fabian Picardo den Chief Minister. Alle Parteien sind für Gibraltars Selbstregierung. Sowohl GSD als auch GSLP weigern sich, Vereinbarungen mit Spanien zu treffen, wobei die GSLP traditionell radikaler ist.

Bis 2006 trug das Gremium den Namen "House of Assembly". Die Namensänderung im Zuge der neuen Verfassung sollte auch das höhere Maß an Selbständigkeit ausdrücken, da House of Assembly ein wiederholt verwendeter Name in britischen Kolonien war. Es hatte auch 17 Mitglieder, aber nur 15 wurden von der Bevölkerung gewählt. Jeder Wähler hatte acht Stimmen, was oft dazu führte, dass die Parteien acht Kandidaten aufstellten mit der Bitte, alle zu wählen. Hierdurch erhielt die stärkste Fraktion in der Regel acht Sitze, die unterlegene Partei sieben Sitze.

Im Gegensatz zu allen anderen Britischen Überseegebieten ist Gibraltar Teil der Europäischen Union.

Aus Sicht der Europäischen Union sind die Einwohner nicht Staatsangehörige des Vereinigten Königreichs (EuGH C-145/04, 2006). Es gibt einige spezielle Regelungen:
Im Jahr 2003 erhielten die Bewohner Gibraltars durch den "European Parliament (Representation) Act 2003" das Wahlrecht für das Europäische Parlament, obwohl die Bürger keine Unionsbürger im Sinne des Art. 20 AEUV sind. Dies begründet der EuGH mit der engen Verbindung von Gibraltar zum Vereinigten Königreich. Die Konsequenzen für andere EU-Mitgliedstaaten (diese müssten die Bürger von Gibraltar dann auch zu Europawahlen zulassen) sind noch ungeklärt. Bei der Europawahl gehört Gibraltar zum Europawahl-Wahlkreis "South West England", der sieben Vertreter stellt (er besteht aus der Region South West England und Gibraltar). Bei der Europawahl 2004 nutzten 57,5 % der Wahlberechtigten Gibraltars ihr neues Recht. Damit lag die Wahlbeteiligung 18,6 Prozentpunkte über dem britischen Durchschnitt.

Beim Referendum über den Verbleib des Vereinigten Königreichs in der Europäischen Union 2016 stimmten 95,9 % für den Verbleib in der Europäischen Union (19.322 Stimmen), 4,1 % für den Brexit (823 Stimmen) – bei einer Wahlbeteiligung von 83,5 %. Gibraltar war damit der Stimmbezirk mit dem höchsten Stimmanteil für einen Verbleib in der EU.

Im Bezug auf dieses Thema schlug der spanische Außenminister José Manuel García-Margallo „eine britisch-spanische Ko-Souveränität“ für die Halbinsel vor. Dieser Status sollte für einen begrenzten Zeitraum gelten, der dann zur Rückgabe des britischen Gebiets an Spanien führen solle. In dieser Übergangszeit könnten die Einwohner Gibraltars britische Staatsbürger bleiben und eine besondere Steuerregelung erhalten. Die Idee einer geteilten Souveränität wurde bereits bei Verhandlungen zwischen London und Madrid in den Jahren 2001 und 2002 geprüft. Bei einem Referendum wurde sie jedoch von den Bürgern Gibraltars abgelehnt. Joseph García, stellvertretender Chefminister Gibraltars, gab an, dass sich die Position Gibraltars bezüglich Spaniens auch nach dem EU-Referendum nicht geändert hätte. Man sehe sich als Briten, die Zukunft Gibraltars liege aber in der EU.

Die Wirtschaft Gibraltars wird vor allem vom Tourismus bestimmt. Neben vielen Tagestouristen übernachten auch immer mehr Touristen in den zahlreichen Hotels. Daneben tragen das Offshore-Finanzwesen sowie Schiffbau und Schiffsreparatur mit jeweils etwa 25 Prozent zum Bruttoinlandsprodukt bei. An vierter Stelle steht der Telekommunikationsbereich, der zu etwa 10 Prozent am BIP beteiligt ist. Einen immer größeren Teil der Wirtschaft stellt die wachsende Zahl in Gibraltar angesiedelter internationaler Anbieter von Online-Sportwetten sowie -Casinos dar. Im Haushaltsjahr 2011/2012 betrug das Bruttoinlandsprodukt 1.169,37 Mio. £ , das entsprach Mitte 2012 etwa 1,837 Mrd. US$.

Im Staatshaushalt wurden seit 2004 regelmäßig Überschüsse erwirtschaftet, die jeweils zwischen 1,1 % und 4,1 % des BIP betrugen. So standen im Haushaltsjahr 2011/2012 Einnahmen von 454,6 Mio. £ lediglich Ausgaben von 420,3 Mio. £ gegenüber, was einem Haushaltsüberschuss von 2,93 % des BIP entsprach.

Das Gibraltar-Pfund ist formal eine eigene Währung, ist aber in einem Verhältnis von 1:1 fest an das britische Pfund gebunden. Deswegen wird auch oft mit britischen Pfund bezahlt und weniger mit dem Euro.

Für Postdienstleistungen war in Gibraltar seit dem Jahr 1886 das „Gibraltar Post Office“ zuständig. Im Jahre 2005 wurde ihm von der britischen Königin Elisabeth II. der Titel „Royal“ verliehen, sodass das Postwesen in Gibraltar nun in der Hand des „Royal Gibraltar Post Office“ liegt. Damit ist die Postgesellschaft von Gibraltar das einzige Postunternehmen außerhalb des britischen Mutterlandes, dem der Titel „Royal“ zuerkannt wurde.

Das Royal Gibraltar Post Office gibt eigene Briefmarken heraus, deren Nominalwert in Gibraltar-Pfund (GIP) ausgewiesen werden. Aufgrund der geographischen Limitierung sind die Briefmarken des „Royal Gibraltar Post Office“ bei Touristen und Sammlern sehr beliebt. Die Briefmarken weisen häufig das Motiv der britischen Königin Elisabeth II. auf.

Postsendungen von Gibraltar in das Ausland (Ausnahme Spanien) werden zunächst nach London geflogen und von dort aus in ihre Bestimmungsstaaten weitertransportiert. Postsendungen für Spanien werden hingegen an der Landesgrenze der spanischen Post übergeben. Die gleiche Vorgehensweise gilt ebenfalls in umgekehrter Richtung für Postsendungen aus dem Ausland nach Gibraltar. Das Hauptpostamt ist in der Main-Street 104 zu finden.

Gibraltar verfügt über einen eigenen Flughafen, den Flughafen Gibraltar. Es handelt sich dabei um den weltweit einzigen Flughafen, dessen Landebahn eine vierspurige Straße kreuzt.

Gibraltar ist einer der bedeutendsten, in jedem Fall aber der umsatzstärkste Nachschubplatz für Schiffsdiesel im Mittelmeer. Im Jahre 2005 liefen 6662 hochseetaugliche Schiffe den Hafen an, 90 % von ihnen zum Tanken.

In Gibraltar verkehren insgesamt sieben Buslinien (1 bis 4, 7 bis 9, Stand: Februar 2014), wobei es die Linie 1 noch als verkürzte Variante 1B gibt, die aber selten verkehrt. Die Beförderung ist kostenfrei für Inhaber bestimmter Ausweise, andere zahlen 2,25 Gibraltar-Pfund bzw. 3 Euro für ein Tagesticket.

Obgleich Gibraltar unter britischer Oberhoheit steht, gilt hier, wegen der geringen Größe und seiner Nähe zu Spanien, seit dem Jahr 1929 der Rechtsverkehr.

Von der Innenstadt zum Felsen (Upper Rock) gibt es eine regelmäßig verkehrende Seilbahn mit einer Zwischenstation.

Die Winston Churchill Avenue ist die einzige Verbindung nach Spanien.

Die gibraltarische Kultur ist stark beeinflusst durch die britische, die spanische und auch die marokkanische Kultur.

Musikbands aus Gibraltar sind zum Beispiel Breed 77, The SoulMates und "No Direction".

Der gibraltarische Nationalfeiertag ist der 10. September. Viele Häuser werden mit der Flagge Gibraltars und rot-weißen Luftballons verziert. Von 1992 bis 2015 wurde an diesem Tag für jeden Bürger ein Ballon „in die Luft entlassen“, dieses Ritual ist 2016 wegen Naturschutzbedenken verboten worden.


Die Fußballnationalmannschaft von Gibraltar existiert seit 1895 und belegte unter anderem beim FIFI Wild Cup den dritten Platz. Es wird jährlich eine nationale Meisterschaft ausgespielt.

Gibraltar hat ein eigenes Fußballstadion, in dem sämtliche Ligaspiele sowie Länderspiele ausgetragen werden. Am 8. Dezember 2006 wurde der Fußballverband von Gibraltar vorläufig als UEFA-Mitglied aufgenommen. Eine definitive Abstimmung erfolgte am 26. Januar 2007 in Düsseldorf – dort wurde der Antrag Gibraltars zur UEFA-Mitgliedschaft abgelehnt. Nach einem Urteil des internationalen Sportgerichtshof CAS vom August 2011 musste die Entscheidung revidiert und Gibraltar ab 1. Oktober 2012 erneut als vorläufiges Mitglied aufgenommen werden. Daraufhin wurde Gibraltar als eigenständiger Nationalverband bei den Auslosungen zur EM der U-17, der U-19 und zum UEFA Futsal Cup berücksichtigt. Die endgültige Aufnahme wurde im Rahmen des 37. UEFA-Kongresses am 24. Mai 2013 in London beschlossen. Per Exekutiv-Entscheid ist auch als Vollmitglied ein Aufeinandertreffen von Mannschaften aus Spanien und Gibraltar in Gruppenspielen nicht zulässig. Am 19. November 2013 bestritt die Nationalmannschaft im portugiesischen Faro ihr erstes offizielles Länderspiel gegen ein anderes UEFA-Mitglied. Die Partie gegen die Slowakei endete 0:0. Die Qualifikation zur Fußball-Europameisterschaft 2016 war das erste Turnier, bei dem Gibraltar um die Teilnahme spielte. Das erste Spiel gegen Deutschland fand am 14. November 2014 in Nürnberg statt und endete 4:0 für Deutschland.

Ähnlich der Entwicklung zur UEFA-Aufnahme Gibraltars hatte die CAS über die Aufnahme Gibraltars bei der FIFA zu entscheiden. Vor allem Spanien wehrte sich, wie ehedem bei der Aufnahme in die UEFA, gegen eine Mitgliedschaft Gibraltars. Zugleich wurde argumentiert, dass Gibraltar kein in sich freies Land wäre und damit die Voraussetzungen für eine Aufnahme in die FIFA nicht erfüllt wären. Die CAS urteilte, dass die FIFA baldmöglichst alle Voraussetzungen zu schaffen hätte, Gibraltar als Vollmitglied aufnehmen zu können.

Wie auch in anderen Sportarten kämpfen die Nationalmannschaften Gibraltars um internationale Anerkennung. Rugby und Cricket haben sich auf Grund der Historie Gibraltars etabliert. In Cricket wird am europäischen Wettbewerb teilgenommen.

Die Gibraltar Broadcasting Corporation (GBC) betreibt einen eigenen Radio- und Fernsehsender für Gibraltar. Die örtliche Variante des britischen Soldatensenders British Forces Broadcasting Service ist sowohl online als auch über Eutelsat 10A zu empfangen. Außerdem gibt es verschiedene Tageszeitungen in englischer und spanischer Sprache. Die wichtigsten gibraltarischen Tageszeitungen sind der "Gibraltar Chronicle" und "Panorama".

Seit 1995 existiert die länderspezifische Top-Level-Domain ".gi".

Gibraltar unterhält derzeit zwei Städtepartnerschaften:




</doc>
<doc id="1868" url="https://de.wikipedia.org/wiki?curid=1868" title="Große Syrte">
Große Syrte

Die Große Syrte (Dschûnel Kebrit, auch Golf von Sidra) ist eine weite Bucht des Mittelmeeres an der Nordküste Libyens. Sie gehört zum Libyschen Meer und ist der südlichste Teil des Mittelmeeres.

Die Große Syrte erstreckt sich zwischen der Landspitze von Cephalae (Ras Kasr Hamet) bei Misurata in Tripolitanien (Westen) und der Landspitze von Boreum Ras Teyonas bei Bengasi in der Kyrenaika mit dem Plateau von Barka im Osten und weist eine Länge von 439 Kilometern auf, sowie eine maximale Breite von 180 Kilometern (Einbuchtung nach Süden). Die Flächenausdehnung beträgt rund 57.000 km². 

An der Großen Syrte liegen die Städte Sirte, Ras Lanuf und Marsa el-Brega (al-Burayqah). Sie sind Umschlagplätze für den Erdölexport Libyens und über Pipelines mit dem Landesinneren verbunden.

Die Reichsteilung von 395, die das Römische Reich in West- und Ostrom teilte, teilte die Küste der großen Syrte unter den beiden Reichen auf. Während des Zweiten Weltkrieges fanden zwei Gefechte zwischen Briten und Italienern im Golf von Syrte, das Erste und das 
Zweite Seegefecht im Golf von Syrte statt.
Unter Berufung auf eigene Auslegungen des Seerechtsübereinkommens erklärte Libyen 1973 einen Großteil der Großen Syrte, nämlich den gesamten Bereich südlich des 32. Breitengrades (genauer: 32° 30' N; von Libyens ehemaligen Staatschef Muammar al-Gaddafi „Todeslinie“ genannt), zu seinen Hoheitsgewässern. Diese von den meisten nichtarabischen Staaten der Welt nicht anerkannte Ausbreitung in als international angesehene Gewässer führte wiederholt zu militärischen Auseinandersetzungen, obwohl außer von und nach Libyen gehenden Verbindungen keine internationalen Schifffahrtswege durch die Syrte verlaufen. Die im Mittelmeer stationierte 6. US-Flotte führte dort wiederholt Manöver ("freedom of navigation exercises") durch, die Libyen in die Schranken weisen sollten. 1981 und 1989 wurden jeweils zwei libysche Kampfjets von im Mittelmeer stationierten Streitkräften der USA abgeschossen. 1986 wurden während der Operation "Attain Document III / Prairie Fire" in der Großen Syrte zwei libysche Kriegsschiffe durch US-Streitkräfte versenkt.



</doc>
<doc id="1870" url="https://de.wikipedia.org/wiki?curid=1870" title="Genetik">
Genetik

Die Genetik (moderne Wortschöpfung zu griechisch γενεά "geneá" ‚Abstammung‘, γένεσις "génesis" ‚Ursprung‘) oder Vererbungslehre ist die Wissenschaft von der Vererbung und ein Teilgebiet der Biologie. Sie befasst sich mit den Gesetzmäßigkeiten und materiellen Grundlagen der Ausbildung von erblichen Merkmalen und der Weitergabe von Erbanlagen (Genen) an die nächste Generation.

Das Wissen, dass individuelle Merkmale über mehrere Generationen hinweg weitergegeben werden, ist relativ jung; Vorstellungen von solchen natürlichen Vererbungsprozessen prägten sich erst im 18. und frühen 19. Jahrhundert aus. Als Begründer der Genetik (zunächst als einen quantifizierbaren Zusammenhang zwischen den Generationen herstellende Transmissionsgenetik) gilt der Augustinermönch und Hilfslehrer Gregor Mendel, der in den Jahren 1856 bis 1865 im Garten seines Klosters systematisch Kreuzungsexperimente mit Erbsen durchführte und diese statistisch auswertete. So entdeckte er die später nach ihm benannten Mendelschen Regeln, die in der Wissenschaft allerdings erst im Jahr 1900 rezipiert und bestätigt wurden. Der heute weitaus bedeutendste Teilbereich der Genetik ist die Molekulargenetik, die sich mit den molekularen Grundlagen der Vererbung befasst. Aus ihr ging die Gentechnik hervor, in der die Erkenntnisse der Molekulargenetik praktisch angewendet werden.

Das Adjektiv „genetisch“ wurde schon um 1800 von Johann Wolfgang von Goethe in dessen Arbeiten zur Morphologie der Pflanzen und in der Folgezeit häufig in der romantischen Naturphilosophie sowie in der deskriptiven Embryologie verwendet. Anders als heute meinte man damit eine „Methode'“ („genetische Methode“) der Untersuchung und Beschreibung der Individualentwicklung (Ontogenese) von Organismen. Das Substantiv „Genetik“ gebrauchte erstmals William Bateson 1905 zur Bezeichnung der neuen Forschungsdisziplin.

In Deutschland wurde bis in die zweite Hälfte des 20. Jahrhunderts der Ausdruck „Erbbiologie“ bedeutungsgleich gebraucht, zumeist zur Unterscheidung der „Erbbiologie des Menschen“ (Humangenetik) von der allgemeinen Genetik. Die Bezeichnung „Humangenetik“ war dabei in Deutschland bereits um 1940 etabliert. Damit wurde ein Rückzug auf wissenschaftlich gebotene Grundlagenforschung angezeigt, während „Rassenhygiene“ angewandte Wissenschaft darstellte. Nach 1945 verschwanden die Bezeichnungen „Erbbiologie“ sowie „Rassenhygiene“ allmählich aus dem wissenschaftlichen Sprachgebrauch.



Schon in der Antike versuchten Menschen die Gesetzmäßigkeiten der Zeugung und die Ähnlichkeiten zwischen Verwandten zu erklären, und einige der im antiken Griechenland entwickelten Konzepte blieben bis in die Neuzeit gültig oder wurden in der Neuzeit wieder aufgegriffen. So lehrte der griechische Philosoph Alkmaion um 500 v. Chr., dass die Zeugung der Nachkommen durch die Zusammenwirkung des männlichen und des weiblichen „Samens“ geschehe. Sein Postulat eines weiblichen Samens fand in der damaligen Naturphilosophie und später auch in der hippokratischen Medizin allgemeine Anerkennung. Davon abweichend behaupteten Hippon und Anaxagoras, dass nur der Mann zeugungsfähigen Samen bilde und dass der weibliche Organismus den Keim nur ernähre. Die Bildung des Samens erfolgte laut Alkmaion im Gehirn, von wo aus er durch die Adern in den Hoden gelange. Demgegenüber erklärten Anaxagoras und Demokrit, dass der gesamte Organismus zur Bildung des Samens beitrage, – eine Ansicht, die als Pangenesistheorie über 2000 Jahre später von Charles Darwin erneut vertreten wurde. Auch die Überlegungen des Anaxagoras, wonach alle Körperteile des Kindes bereits im Samen (Sperma) vorgebildet seien, traten als Präformationslehre in der Neuzeit wieder auf. In der Antike wurden diese frühen Lehren weitgehend abgelöst durch die Ansichten des Aristoteles (De generatione animalium), wonach das Sperma aus dem Blut entsteht und bei der Zeugung nur immateriell wirkt, indem es Form und Bewegung auf die durch den weiblichen Organismus bereitgestellte flüssige Materie überträgt. Die Entwicklung des Keims beschrieb Aristoteles als Epigenese, wonach im Gegensatz zur Präformation die verschiedenen Organe nacheinander durch die Einwirkung des väterlichen Formprinzips ausgebildet werden. Neben der geschlechtlichen Zeugung kannte Aristoteles auch die Parthenogenese (Jungfernzeugung) sowie die (vermeintliche) Urzeugung von Insekten aus faulenden Stoffen.

Der Aristoteles-Schüler Theophrastus postulierte eine "transmutatio frumentorum" und nahm an, dass sich Getreidearten zu ihrer Wildform zurückverwandeln können. Zudem unterschied er männliche und weibliche Pflanzen bei der Dattelpalme.

Vererbung war bis in das 18. Jahrhundert ein juristischer Begriff und fand für natürliche Vorgänge keine Anwendung. Denn Ähnlichkeiten zwischen Verwandten wurden ausreichend über jeweils spezifische lokale Faktoren und die Lebensweise des Individuums erklärt: über das Klima, die Ernährung, die Art der Betätigungen usw. Wie gewisse Merkmale unter Nachkommen blieben auch diese Faktoren für die Nachkommen in der Regel konstant. Irreguläre Merkmale konnten dann entsprechend auf irreguläre Einflüsse bei der Zeugung oder der Entwicklung des Individuums zurückgeführt werden. Erst mit dem zunehmenden internationalen Verkehr und zum Beispiel der Anlage von exotischen Gärten wurde ein Wahrnehmungsraum dafür geschaffen, dass es vom Individuum und seinem jeweiligen Ort ablösbare, natürliche Gesetze geben müsse, die sowohl die Weitergabe von regulären als auch zuweilen eine Weitergabe von neu erworbenen Eigenschaften regeln.
Der Begriff der Fortpflanzung oder Reproduktion, in dessen Kontext von Vererbung im biologischen Sinn gesprochen werden kann, kam erst gegen Ende des 18. Jahrhunderts auf. In früheren Jahrhunderten galt die „Zeugung“ eines Lebewesens als ein Schöpfungsakt, der grundsätzlich eines göttlichen Eingriffs bedurfte und im Rahmen des Präformismus vielfach als Teilaspekt der Erschaffung der Welt betrachtet wurde. Dabei unterschied man die Zeugung durch den Samen (Sperma) im Mutterleib von der Urzeugung, durch welche niedere Tiere (etwa Würmer, Insekten, Schlangen und Mäuse) aus toter Materie hervorzugehen schienen. Die „Samenzeugung“ betrachtete man als Eigenheit des Menschen und der höheren Tiere, welche zu ihrer Ausbildung eines Mutterleibs bedürfen. Erst gegen Ende des 17. Jahrhunderts setzte sich, vor allem aufgrund der Experimente Francesco Redis, die Einsicht durch, dass Würmer, Insekten und andere niedere Tiere nicht aus toter Materie entstehen, sondern von gleichartigen Tieren gezeugt werden. Nun betrachtete man die Zeugung nicht mehr als Schöpfungsakt, sondern verlegte diesen in die Zeit der Erschaffung der Welt, bei der, wie man annahm, alle zukünftigen Generationen von Lebewesen zugleich ineinandergeschachtelt erschaffen wurden. Die Zeugung war somit nur noch eine Aktivierung des längst vorhandenen Keims, der sich dann zu einem voll ausgebildeten Organismus entfaltete. Strittig war dabei, ob die Keime durch das weibliche oder durch das männliche Geschlecht weitergegeben werden, ob sie also im Ei oder im „Samentierchen“ eingeschachtelt sind. Beide Ansichten hatten ihre Anhänger (Ovisten und Animalkulisten), bis die Entdeckung der Jungfernzeugung bei der Blattlaus durch Charles Bonnet 1740 den Streit zugunsten der Ovisten entschied.

Neben der sehr populären Präformationslehre, die 1625 durch Giuseppe degli Aromatari (1587–1660) ins Spiel gebracht worden war, gab es im 17. Jahrhundert auch renommierte Anhänger der an Aristoteles anknüpfenden Epigenesislehre, namentlich William Harvey und René Descartes. Deren Ansichten galten jedoch als antiquiert und wurden als unwissenschaftlich verworfen, da sie immaterielle Wirkprinzipien voraussetzten, während der Präformismus rein mechanistisch gedacht werden konnte und zudem durch die Einführung des Mikroskops einen starken Auftrieb erfuhr.

Die Vorstellung der Präformation herrschte bis in das 19. Jahrhundert hinein vor, obwohl es durchaus Forschungsergebnisse gab, die nicht mit ihr in Einklang gebracht werden konnten. Großes Erstaunen riefen die Versuche zur Regeneration bei Salamandern, Süßwasserpolypen und anderen Tieren hervor. Polypen kann man fein zerhacken, und jedes Teilstück entwickelt sich, wie Abraham Trembley 1744 beschrieb, innerhalb von zwei bis drei Wochen zu einem kompletten Tier. In den Jahren 1744 bis 1754 veröffentlichte Pierre-Louis Moreau de Maupertuis mehrere Schriften, in denen er aufgrund von Beobachtungen bei Tieren und Menschen, wonach beide Eltern Merkmale an ihre Nachkommen weitergeben können, die Präformationslehre kritisierte und ablehnte. Entsprechende Beobachtungen publizierte auch Joseph Gottlieb Kölreuter (1761), der als Erster Kreuzungen verschiedener Pflanzenarten studierte. Und Caspar Friedrich Wolff beschrieb 1759 minutiös die Entwicklung des Embryos im Hühnerei aus völlig undifferenzierter Materie. Trotz der Probleme, die derartige Forschungen aufwarfen, geriet die Präformationslehre jedoch erst im frühen 19. Jahrhundert durch die embryologischen Untersuchungen von Christian Heinrich Pander (1817) und Karl Ernst von Baer (1828) ins Wanken, bei denen diese die Bedeutung der Keimblätter aufklärten und allgemein gültige Gesetzmäßigkeiten der Embryogenese der Tiere aufzeigten.

Mit der Etablierung der von Matthias Jacob Schleiden (1838), Theodor Schwann (1839) und Rudolf Virchow (1858) entwickelten Allgemeinen Zelltheorie wurde deutlich, dass die Gründe für die Ähnlichkeit von Eltern und Nachkommen in der Zelle lokalisiert sein müssen. Alle Organismen bestehen aus Zellen, Wachstum beruht auf der Vermehrung der Zellen durch Teilung, und bei der geschlechtlichen Fortpflanzung, die bei Vielzellern der Normalfall ist, vereinigen sich je eine Keimzelle beiderlei Geschlechts zu einer Zygote, aus welcher durch fortwährende Teilung und Differenzierung der neue Organismus hervorgeht.

Die Gesetzmäßigkeiten der Vererbung blieben lange im Unklaren. Schon in den Jahren 1799 bis 1823 führte Thomas Andrew Knight – wie einige Jahrzehnte später Gregor Mendel – Kreuzungsexperimente mit Erbsen durch, bei denen er bereits die Erscheinungen der Dominanz und der Aufspaltung von Merkmalen beobachtete. 1863 publizierte Charles Naudin (1815–1899) die Ergebnisse seiner Kreuzungsexperimente mit zahlreichen Pflanzengattungen, wobei er das sehr gleichartige Aussehen aller Pflanzen der ersten Tochtergeneration und die „extreme Verschiedenartigkeit der Formen“ in den folgenden Generationen konstatierte und damit weitere bedeutende Aspekte der fast zeitgleichen Erkenntnisse Mendels vorwegnahm, aber im Unterschied zu Mendel keine statistische Auswertung durchführte.
Der entscheidende Durchbruch gelang dann Mendel mit seinen 1856 begonnenen Kreuzungsversuchen, bei denen er sich auf "einzelne Merkmale" konzentrierte und die erhaltenen Daten "statistisch auswertete". So konnte er die grundlegenden Gesetzmäßigkeiten bei der Verteilung von Erbanlagen auf die Nachkommen ermitteln, die heute als Mendelsche Regeln bezeichnet werden. Diese Entdeckungen, die er 1866 publizierte, blieben jedoch zunächst in der Fachwelt fast unbeachtet und wurden erst im Jahr 1900 von Hugo de Vries, Carl Correns und Erich Tschermak wiederentdeckt und aufgrund eigener Versuche bestätigt.

Einen radikalen Umbruch der Vorstellungen von der Vererbung brachte die Keimbahn- oder Keimplasmatheorie mit sich, die August Weismann in den 1880er Jahren entwickelte. Schon seit dem Altertum galt es als selbstverständlich, dass Merkmale, welche die Eltern während ihres Lebens erworben haben, auf die Nachkommen übertragen werden können. Nach Jean-Baptiste de Lamarck, in dessen Evolutionstheorie sie eine bedeutende Rolle spielte, wird diese Ansicht heute als Lamarckismus bezeichnet. Doch auch Charles Darwin postulierte in seiner Pangenesistheorie, dass der ganze elterliche Organismus auf die Keimzellen einwirke – unter anderem sogar indirekt durch Telegonie. Weismann unterschied nun zwischen der Keimbahn, auf der die Keimzellen eines Organismus sich von der Zygote herleiten, und dem Soma als der Gesamtheit aller übrigen Zellen, aus denen keine Keimzellen hervorgehen können und von denen auch keine Einwirkungen auf die Keimbahn ausgehen. Diese Theorie war allerdings anfangs sehr umstritten.
Mit seinem zweibändigen Werk "Die Mutationstheorie" (1901/03) führte de Vries den bis dahin in der Paläontologie gebräuchlichen Begriff „Mutation“ in die Vererbungslehre ein. Nach seiner Auffassung handelte es sich bei Mutationen um umfassende, sprunghafte Veränderungen, durch welche eine neue Art entstehe. Dabei stützte er sich auf seine Studien an Nachtkerzen, bei denen eine „in allen ihren Organen“ stark veränderte Pflanze aufgetreten war, deren Merkmale sich als erbkonstant erwiesen und die er daher als neue Art ("Oenothera gigas") beschrieb. (Später stellte sich heraus, dass "„Oe. gigas“" im Unterschied zu den diploiden Ausgangspflanzen tetraploid war und somit – aus heutiger Sicht – der Sonderfall einer Genommutation (Autopolyploidie) vorlag.) Dieser Befund stand im Widerspruch zu der an Charles Darwin anschließenden Evolutionstheorie, die das Auftreten geringfügiger Veränderungen voraussetzte, und das war einer der Gründe, warum der „Mendelismus“ sich zeitweilig im Widerstreit mit dem damals noch nicht allgemein akzeptierten Darwinismus befand.

In den Jahren um die Jahrhundertwende untersuchten etliche Forscher die unterschiedlichen Formen der Chromosomen und deren Verhalten bei Zellteilungen. Aufgrund der Beobachtung, dass gleich aussehende Chromosomen paarweise auftreten, äußerte Walter Sutton 1902 als erster die Vermutung, dass dies etwas mit den ebenfalls gepaarten Merkmalen und deren „Spaltung“ in den Untersuchungen von Mendel und seinen Wiederentdeckern zu tun haben könne. Im Anschluss daran formulierte Theodor Boveri 1904 die Chromosomentheorie der Vererbung, wonach die Erbanlagen an die Chromosomen gebunden sind und deren Verhalten bei der Meiose und Befruchtung den Mendelschen Regeln entspricht.
Eine sehr folgenreiche Entscheidung war die Wahl von Taufliegen als Versuchsobjekt durch die Arbeitsgruppe um Thomas Hunt Morgan im Jahre 1907, vor allem weil diese in großer Zahl auf kleinem Raum gehalten werden können und sich sehr viel schneller vermehren als die bis dahin verwendeten Pflanzen. So stellte sich bald heraus, dass es auch geringfügige Mutationen gibt, auf deren Grundlage allmähliche Veränderungen innerhalb von Populationen möglich sind (Morgan: "For Darwin", 1909). Eine weitere wichtige Entdeckung machte Morgans Team etwa 1911, als man die schon 1900 von Correns publizierte Beobachtung, dass manche Merkmale meist zusammen vererbt werden (Genkopplung), mit Untersuchungen der Chromosomen verband und so zu dem Schluss kam, dass es sich bei den Koppelungsgruppen um Gruppen von Genen handelt, welche auf demselben Chromosom liegen. Wie sich weiter herausstellte, kann es zu einem Austausch von Genen zwischen homologen Chromosomen kommen (Crossing-over), und aufgrund der relativen Häufigkeiten dieser intrachromosomalen Rekombinationen konnte man eine lineare Anordnung der Gene auf einem Chromosom ableiten (Genkarte). Diese Erkenntnisse fasste Morgan 1921 in "The Physical Basis of Heredity" und 1926 programmatisch in "The Theory of the Gene" zusammen, worin er die Chromosomentheorie zur Gentheorie weiterentwickelte.

Diese Theorie war schon während ihrer allmählichen Herausbildung sehr umstritten. Ein zentraler Streitpunkt war die Frage, ob die Erbanlagen sich ausschließlich im Zellkern oder auch im Zytoplasma befinden. Vertreter der letzteren Ansicht waren u. a. Boveri, Correns, Hans Driesch, Jacques Loeb und Richard Goldschmidt. Sie postulierten, dass im Kern nur relativ geringfügige Erbfaktoren bis hin zu Artmerkmalen lokalisiert seien, während Merkmale höherer systematischer Kategorien (Gattung, Familie usw.) durch das Plasma vererbt würden. Der entschiedenste Vertreter der Gegenseite war Morgans ehemaliger Mitarbeiter Hermann Joseph Muller, der in "The Gene as the Basis of Life" (1929) die im Kern lokalisierten Gene als die Grundlage des Lebens überhaupt bezeichnete und die Bedeutung des Plasmas als sekundär einstufte.

Muller war es auch, der 1927 erstmals von der Erzeugung von Mutationen durch Röntgenstrahlung berichtete, wodurch die genetische Forschung nicht mehr darauf angewiesen war, auf spontan auftretende Mutationen zu warten. Der von de Vries, Morgan, Muller und Anderen vertretenen Ansicht der Zufälligkeit der Mutationen stand das u. a. von Paul Kammerer und Trofim Denissowitsch Lyssenko verfochtene Postulat gegenüber, dass Mutationen „gerichtet“ und qualitativ durch Umwelteinflüsse bestimmt seien.

Nach dem allgemeinen Bekanntwerden von Mendels mathematisch exakter Beschreibung des dominant-rezessiven Erbgangs im Jahr 1900 wurde die Frage diskutiert, ob rezessive Merkmale in natürlichen Populationen allmählich verschwinden oder auf Dauer erhalten bleiben. Hierzu fanden der deutsche Arzt Wilhelm Weinberg und der britische Mathematiker Godfrey Harold Hardy 1908 fast gleichzeitig eine Formel, die das Gleichgewicht dominanter und rezessiver Merkmale in Populationen beschreibt. Diese Entdeckung wurde jedoch unter Genetikern zunächst kaum beachtet. Erst 1917 führte Reginald Punnett das von ihm so genannte „Hardy-Gesetz“ in die Populationsforschung ein, was ein wichtiger Beitrag zur Begründung der Populationsgenetik als eigenständigem Forschungszweig in den 1920er Jahren war. Weinbergs Beitrag wurde sogar erst 1943 von Curt Stern wiederentdeckt, der die Formel daraufhin in „Hardy-Weinberg-Gesetz“ umbenannte.

Die Grundlagen der Populationsgenetik wurden parallel von Sewall Wright, Ronald A. Fisher und J. B. S. Haldane entwickelt. Sie erkannten, dass Vererbungsvorgänge in der Natur sinnvollerweise auf der Ebene von Populationen zu betrachten sind, und formulierten dafür die theoretischen Grundlagen (Haldane: "A Mathematical Theory of Natural and Artificial Selection", 1924–1932; Fisher: "The Genetical Theory of Natural Selection", 1930; Wright: "Evolution in Mendelian Populations", 1931).

Seit 1889 (Richard Altmann) war bekannt, dass Chromosomen aus „Nucleinsäure“ und basischem Protein bestehen. Über deren Aufbau und Funktion konnte jedoch lange Zeit nur spekuliert werden. 1902 postulierten Emil Fischer und Franz Hofmeister, dass Proteine Polypeptide seien, also lange Ketten von Aminosäuren. Das war zu diesem Zeitpunkt allerdings noch sehr spekulativ. Als 1905 die ersten Analysen der Aminosäuren-Zusammensetzung von Proteinen publiziert wurden, erfassten diese lediglich ein Fünftel des untersuchten Proteins, und die Identifikation aller 20 proteinogenen Aminosäuren zog sich bis 1935 hin. Dagegen war bei der Nukleinsäure schon 1903 klar (Albrecht Kossel), dass sie neben Zucker und Phosphat lediglich fünf verschiedene Nukleinbasen enthält. Erste Analysen der Basenzusammensetzung durch Hermann Steudel ergaben 1906, dass die vier hauptsächlich vorhandenen Basen zu annähernd gleichen Anteilen enthalten sind. Daraus schloss Steudel (1907), dass die Nukleinsäure „ein relativ einfach gebauter Körper sei“, dem man keine anspruchsvollen Funktionen beimessen könne. Dies etablierte sich als Lehrmeinung, die bis in die 1930er Jahre gültig blieb, und auf dieser Grundlage betrachtete man nicht die Nukleinsäure(n), sondern die Proteine als „Erbsubstanz“.

Zu der Einsicht, dass es sich gerade umgekehrt verhält und die Nukleinsäure DNA als Erbsubstanz angesehen werden muss, führten die Experimente der Arbeitsgruppe von Oswald Avery zur Transformation von Pneumokokken (1944) und das Hershey-Chase-Experiment von 1952 mit Bakteriophagen. Außerdem zeigte Erwin Chargaff 1950, dass die vier Nukleotide, aus denen die DNA besteht, nicht zu gleichen, sondern zu "paarweise" gleichen Anteilen enthalten sind. Zusammen mit Röntgenstrukturanalyse-Daten von Rosalind Franklin war das die Grundlage für die Entwicklung des Doppelhelix-Strukturmodells der DNA durch James Watson und Francis Crick 1953.




</doc>
<doc id="1872" url="https://de.wikipedia.org/wiki?curid=1872" title="Großbritannien (Insel)">
Großbritannien (Insel)

Die Insel Großbritannien liegt im Atlantischen Ozean, zwischen der Irischen See und dem Nordatlantik im Westen, der Nordsee im Osten und dem Ärmelkanal im Südosten, an der nordwestlichen Küste des europäischen Kontinents.

Mit einer Fläche von 219.331 km² ist die Hauptinsel die neuntgrößte Insel der Welt sowie die größte Insel Europas und der Britischen Inseln, zu denen unter anderem auch noch Irland und die Isle of Man gehören. England und Wales bildeten im Altertum die römische Provinz "".

Ein alter Name für die Insel Großbritannien ist „Albion“. Dieser Name, von Alfred Holder ("Alt-Keltischer Sprachschatz", 1896) als „Weißland“ übersetzt, könnte sich auf die weißen Kreideklippen von Dover beziehen, die man normalerweise als Erstes sieht, wenn man mit dem Schiff über den Ärmelkanal nach Großbritannien fährt. Ein historischer Name im Gälischen für Schottland ist ".

Politisch ist Großbritannien seit dem " eine Einheit, die aus den Staaten England, Wales (seit 1542 in dem Königreich England) und Schottland gebildet wurde. Die Hebriden, die Orkney und die Shetlandinseln werden als Teil Schottlands ebenfalls politisch zu Großbritannien gerechnet. Die Länder Großbritanniens und der angesprochenen Nebeninseln bilden zusammen mit der Provinz Nordirland das Vereinigte Königreich Großbritannien und Nordirland, das im Deutschen ebenfalls "Großbritannien" genannt wird, auch wenn es vom Staatsgebiet wesentlich größer als die geographische Insel Großbritannien ist.

Die direkt der britischen Krone unterstehenden Kanalinseln und die Insel Man sind kein Teil des Vereinigten Königreichs Großbritannien und Nordirland; Man und auch ganz Irland gehören jedoch geographisch zu den Britischen Inseln, die Kanalinseln dagegen nicht.

Historisch gab es zwei Britannien: die Insel "[Groß]Britannien" und die Halbinsel "Kleinbritannien" im Nordwesten Frankreichs (heute Bretagne genannt). Im Englischen werden sie als ' und ' bezeichnet; im Französischen werden ähnlich ' und ' unterschieden.

Der Begriff Großbritannien fand erstmals weite Verbreitung während der Regierung des Königs Jakob VI. von Schottland, der als Jakob I. auch England regierte; er bezeichnete die von einem Monarchen regierte Insel, die aus zwei Staaten mit eigenen Parlamenten bestand. Nach der Vereinigung von England und Schottland war die Bezeichnung Königreich Großbritannien von 1707 bis 1800 gebräuchlich.

Mit dem " fand eine erneute Umgestaltung statt: Das vom englischen Königshaus regierte Irland wurde mit dem „Königreich Großbritannien“ zum „Vereinigten Königreich von Großbritannien und Irland“ vereint. Nachdem 26 der 32 irischen Grafschaften den Irischen Freistaat gebildet hatten, entstand im Jahre 1922 das „Vereinigte Königreich von Großbritannien und Nordirland“.

Vereinfachenderweise wird die Bezeichnung „Großbritannien“ häufig als Synonym für das Vereinigte Königreich verwendet, insbesondere auch seitens der Medien und bei nicht-offiziellen Äußerungen von Politikern. Die Regierungsorgane des Vereinigten Königreichs werden als „britisch“ bezeichnet: „Der britische Premierminister“ oder „das Britische Königshaus“. Hierbei steht „Großbritannien“ auch für Nordirland, das geographisch natürlich kein Teil der Insel ist. Die häufig verwendete Bezeichnung „englisch“ anstatt „britisch“ an gleicher Stelle ist noch ungenauer, da England wiederum nur ein Teil von Großbritannien ist.

Die Insel Großbritannien besteht aus den folgenden drei Nationen (Landesteilen), die politisch nur sehr eingeschränkt eigenständig sind:

Seit England, Wales und Schottland im Jahre 1707 vereint wurden, hat England, welches die weitaus größte Bevölkerung hat, immer die dominierende Rolle in allen nachfolgenden Staaten gespielt. Das zeigt sich auch, indem die Krönung des britischen Monarchen das englische Ritual befolgt, oder das britische Parlament der Struktur des englischen Parlaments folgt und mit dem " das ursprünglich englische Parlamentsgebäude nutzt.

Die äußersten Punkte der Insel sind Land’s End in der Grafschaft Cornwall im Südwesten und John o’ Groats in Caithness im Norden Schottlands.

England ist überwiegend hügelig. Süd- und Ostengland sind eine Schichtstufenlandschaft und abgesehen von kleinen Küstenebenen der geologisch jüngste Teil der Insel. Die einzige ausgedehnte Ebene sind die Fens zwischen Cambridge und dem Wash an der Grenze zwischen Ost- und Mittelengland. Die Penninen (Pennine Range) im Norden Englands gehen fast bruchlos in die Uplands Südschottlands über. Östlich davon erstrecken sich wiederum Hügelländer. Die Mitte Schottlands besteht aus dem Firth of Clyde und Firth of Forth verbindenden Graben der Central Lowlands und einem nordöstlich anschließenden Hügelland. Zum nördlich anschließenden Schottischen Hochland werden außer der rauen Gebirgsregion auch die westlich und nördlich anschließenden, ebenfalls rauen, aber nicht immer hohen über 80 Inseln gezählt, weswegen man auch von Highlands and Islands spricht. Wales, durch den Bristolkanal genannten Meeresarm von Südwestengland getrennt, ist ebenfalls ein Bergland.

Industrielle Ballungsräume sind Greater London, die Umgebung von Birmingham, die alten Steinkohlegebiete die sich U-förmig um die Penninen gruppieren, Südwales und das Quertal in der Mitte Schottlands. Die Hügelländer mit ihren von Hecken gesäumten Ländereien und oft kleinen Landstraßen sind von einer Geschichte intensiver Nutzung geprägt, die allerdings im 19. Jahrhundert mit der Möglichkeit Grundnahrungsmittel in großem Maße zu importieren, vielerorts einer Grünlandwirtschaft wich. Die Bergländer sind dünn besiedelt und werden extensiv beweidet. Im Norden und Westen Schottlands wurde für die extensive Schafhaltung ein großer Teil der eingesessenen Landbevölkerung im 18. und frühen 19. Jahrhundert vertrieben (Highland Clearances). Jahrhundertelanger Raubbau an den Wäldern ist erst zum geringen Teil in den letzten Jahrzehnten durch Aufforstung behoben worden.


</doc>
<doc id="1873" url="https://de.wikipedia.org/wiki?curid=1873" title="Gewerkschaft">
Gewerkschaft

Eine Gewerkschaft ist eine Vereinigung von in der Regel abhängig Beschäftigten zur Vertretung ihrer wirtschaftlichen, sozialen und kulturellen Interessen.

Die Gewerkschaften sind zumeist aus der europäischen Arbeiterbewegung hervorgegangen und setzen sich seit ihrem Bestehen für höhere Löhne, bessere Arbeitsbedingungen, mehr Mitbestimmung, für Arbeitszeitverkürzungen und teilweise auch für weitergehende Gesellschaftsveränderung ein. Sie schließen als Verhandlungspartner von Arbeitgeberverbänden beispielsweise überbetriebliche Tarifverträge ab und führen dazu Lohnkämpfe, gegebenenfalls auch mit Hilfe von Streiks und Boykotts. Die Gewerkschaften versuchen, in Vertretung der Interessen ihrer Mitglieder, einen möglichst großen Teil der Unternehmensgewinne als Lohn und zur Verbesserung der Arbeitsbedingungen an die Belegschaft zu verteilen. Dagegen vertritt die Unternehmensführung die Interessen der Unternehmensinhaber und Aktionäre, die möglichst hohe Gewinne erwirtschaften will, als ausgeschüttete Dividende und neue betriebliche Investitionen. Da es keine "richtige" oder "optimale" Aufteilung der Gewinne gibt, ist die Gewinnverteilung eine Machtfrage, die von beiden Parteien entsprechend ihrer Interessenlage unterschiedlich beurteilt wird. Gewerkschaften, die eine positive Entwicklung des Betriebes ebenfalls im Auge haben müssen, sind deshalb auch für neue betriebliche Investitionen, ohne die der Betrieb wirtschaftlich ins Hintertreffen geriete. Sie waren wegen dieser engen Verknüpfung mit den Unternehmensinteressen nie so radikal wie die Arbeiterparteien im 19. Jahrhundert. Richtig ist, dass Gewerkschaften – häufig begrenzt auf fachliche Sektoren – für den Erhalt ihres Sektors kämpfen, auch wenn dieser ökonomisch nicht mehr wettbewerbsfähig ist. Gewerkschaftsvertreter sind sehr oft in den von Arbeitgebern finanzierten Aufsichtsräten an der Kontrolle der Betriebsleitung beteiligt. "Leitende Angestellte" werden mit steigender Hierarchieebene der Unternehmensleitung zugehörig betrachtet und sind entsprechend selten Gewerkschaftsmitglied.

Gewerkschaften weisen oft darauf hin, dass ihre Lohnforderungen für eine Umverteilung mindestens des Produktivitätsfortschritts sorgen und so insbesondere die Massenkaufkraft, Voraussetzung für einen stabilen (Binnen-)Konsum, erhalten bleibt. So argumentieren Gewerkschaften auf der Grundlage nachfrageorientierter Wachstumsmodelle für ihre Positionen.

Insbesondere neoklassisch orientierte Ökonomen fordern ein flexibles Arbeitszeitmodell; Gewerkschaften stehen jedoch häufig für andere Regelungen ein. Kritiker werfen Gewerkschaften vor, dadurch den heimischen Standort zu schwächen. Für die Ökonomen der Gewerkschaften – traditionell eher Anhänger des Keynesianismus – geht die Krise auf dem Arbeitsmarkt v. a. auf die Produktivitätszuwächse zurück, die gesellschaftlich ungleich verteilt sind und der Markt deshalb nicht das erhöhte Produktionspotential aufnehmen kann (Binnennachfrage). Die Gewerkschaften behaupten, nicht die Lohnkosten seien zu hoch, sondern die Löhne seien zu niedrig.

Gegner dieser Auffassung sagen, dass gerade für Unternehmen, die dazu in der Lage sind, flexibel den Standort in Niedriglohnländer zu verlagern, hohe Stundenlöhne jedoch abschreckend seien. Andererseits können sich auch die Verhältnisse in Niedriglohnländern schnell ändern. In China steigen die Löhne derzeit um bis zu zehn Prozent pro Jahr. Die Chinesen verlagern ihre Produktionen nach Vietnam und Myanmar.

Es scheint, dass in Osteuropa in den letzten Jahren zahlreiche neue Werke der Automobilindustrie entstanden und in Deutschland Arbeitsplätze verloren gegangen seien. In Ländern mit hoher Produktivität und niedrigeren Lohnkosten als in Deutschland, etwa Schweden, blieben Arbeitsplätze hingegen erhalten. Gerade in der Industrie seien von Arbeitsplatzabbau auch zuliefernde Unternehmen und damit weitere Stellen betroffen. Tatsächlich haben sich aber die durchschnittlichen Lohnstückkosten in Schweden in den letzten zehn Jahren um das Vierfache mehr erhöht als in Deutschland. Auch ist die Zahl der in der deutschen Automobilbranche Beschäftigten in Deutschland sogar gestiegen.

In globalisiertem Kontext aufgeführte keynesianische Argumente zu Nachfragestärkung würden nach den Kritikern angeblich damit überlagert, da ohne Arbeit auch keine Nachfrage möglich ist und weil sich Investitionen ungehinderter im globalen Markt bewegen können als Menschen.

Zur Kaufkrafttheorie der Löhne gibt es unterschiedliche Ansichten. Während der Kaufkrafttheorie kritisch gegenüberstehende Ökonomen meinen, dass diese Theorie die Verhältnisse zu sehr vereinfache, meinen die Befürworter dieser Theorie, dass die Gewinntheorie die Verhältnisse zu sehr vereinfache.

Durch die Senkung von Konfliktkosten tragen die Gewerkschaften in Deutschland zu einer stabilen Grundlage der Wirtschaft bei. Im Vergleich zu anderen industriell entwickelten Rechtsstaaten wird in Deutschland nur selten gestreikt. Als nach dem Prinzip der Gewaltenteilung wirkende Gegenkraft ermöglichen sie es den Arbeitgebern, sich klar auf ihre Standpunkte zu konzentrieren. Dem stehen auch "Konsenskosten" entgegen. In Rechtsstaaten sind diese vorwiegend finanzieller Natur. Sie unterscheiden sich somit von den menschlichen Kosten, die durch erzwungenen Konsens in autoritären Staaten entstehen.

In den vergangenen Jahren nahm der Druck auf die Gewerkschaften zu. Staaten in Mittel- und Osteuropa sowie in Asien gelang es, ein hohes Bildungs-, Produktivitäts- und Infrastrukturniveau aufzubauen. In Staaten wie China erfolgt der Druck auf Gewerkschaften durch Kriminalisierung der Gründer unabhängiger Gewerkschaften. Weiterhin existiert als Kennzeichen für fehlende Rechtsstaatlichkeit ein Widerspruch zwischen gesetzlichen Regelungen und der Einklagbarkeit von Rechten.

Die Folge der Konkurrenz aus Gebieten mit geringerer Rechtsstaatlichkeit und der Unterdrückung von Gewerkschaften war zum Teil die Abwanderung von Arbeitsplätzen aus Westeuropa. Trotz der hohen Arbeitslosigkeit und der (umstrittenen) These, ein Industrieland wie Deutschland sei international nicht mehr wettbewerbsfähig, halten die Gewerkschaften an Lohnforderungen fest, die zumindest die Inflation ausgleichen, aber auch teilweise höher sind als das wirtschaftliche Wachstum, wenn in einer Branche besonders hohe Produktivitätszuwächse zu verzeichnen sind.

In Deutschland wird auf die im internationalen Vergleich wenigsten Streiktage verwiesen. Streiks sind für alle Gewerkschaften mit hohen Kosten verbunden und für Arbeitgeber neben kurzfristigen Produktionsausfällen langfristig ein Standortnachteil. So ist es im Sinne beider Parteien, Streiks zu vermeiden.

Die meisten Gewerkschaften halten Strategien von Lohnsenkung, um gegen Maschinen zu konkurrieren oder um arbeitsintensive Produktionen zu halten, langfristig für verfehlt, auch wenn sie in Einzelfällen entsprechenden Abmachungen zustimmen. Eine wirtschaftstheoretische Grundlage für solche Lohnsenkungen hierfür gibt es jedoch nicht.
Gewerkschaften zielen bei ihren Aktivitäten auf die Schaffung neuer Massennachfrage, die die Binnenkonjunktur anregen soll. So wird die Abkopplung Deutschlands von der anziehenden Weltkonjunktur zum Teil auf die schwache Binnennachfrage zurückgeführt. Einige Wirtschaftsexperten kritisieren jedoch, dass dabei der doppelte Nachfrageeffekt von den Gewerkschaften keine Berücksichtigung findet. Nachfrage entstehe auch dann, wenn man es Unternehmen erleichtert, Investitionen zu tätigen. Jedoch ist die Wirkung der Investition der eines vorweggenommenen zukünftigen Konsums gleich, denn investiert wird nur dort, wo später Konsum erwartet wird.

Langfristig sei der Konsum der Zukunft durch die Kredite für die Investitionen in der Vergangenheit bereits gebunden. Somit könne man sich langfristig auf die Betrachtung des Konsums zurückziehen und deswegen den Effekt der doppelten Nachfrage ignorieren. Allerdings haben die letzten Jahre gezeigt, dass beispielsweise Großunternehmen verstärkt nicht mehr im Inland, sondern auf den Kapitalmärkten oder in Fusionen mit ausländischen Unternehmen investieren. Auch Exportrekorde der deutschen Wirtschaft – die der These mangelnder internationaler Wettbewerbsfähigkeit widersprechen – können die Binnennachfrage nicht ausreichend stützen.

Bedeutender ist beim doppelten Nachfrageeffekt jedoch die Nachfrage im Inland. Diese ist naturgemäß hoch, wenn es heimischen Unternehmen gut geht. Denn nicht nur private Haushalte, sondern insbesondere auch heimische Unternehmen konsumieren im Inland, etwa über Zulieferungen. Hohe Löhne oder hohe Abgaben jedoch wirkten diesem Konsum entgegen und verlagerten ihn ins Ausland. Dieser These wird aber mit dem Argument widersprochen, die hohen Löhne an ortsansässige Mitarbeiter würden diesen erst ermöglichen, auch in der Region ihr erarbeitetes Geld auszugeben, sodass sich bei Lohnerhöhungen allenfalls eine Substitution von Zahlungen an regionale Zulieferer zu Zahlungen an regionale Mitarbeiter ergebe. Ähnlich sehe es mit Staatsabgaben aus, die auch vom jeweiligen Staat zur Bezahlung seiner Ausgaben in bevorzugt seinem Staatsgebiet verwendet würden.

Den Gewerkschaften gelang es im Laufe der Nachkriegszeit immer deutlicher, auch als allgemeiner gesellschaftlicher Vertreter der Interessen der arbeitenden Bevölkerung politisch und institutionell anerkannt zu werden. Hierbei übernahmen sie außerhalb des eigentlichen Koalitionszwecks, wie der Wahrnehmung der Arbeitnehmerinteressen beim Abschluss von Tarifverträgen, umfangreiche Aufgaben. Dies gelang vor allem im politischen Raum umso mehr, als sie in allen Parlamenten durch eine große Anzahl von ihnen als Mitglieder angehörenden Abgeordneten Unterstützung fanden. So waren in den Bundestagen von 1965–1987 sowie von 1998 (gemeint ist jeweils das Wahljahr) zwischen 50 % und 60 % der Abgeordneten Mitglieder von Gewerkschaften, 2002 waren es 47 %, 2005 sind es unter 40 % (36 % bei den DGB-Gewerkschaften). Im Rahmen der Mitbestimmung der Arbeitnehmer bei den sie beschäftigenden Betrieben erhielten die Gewerkschaften, soweit sie dort Mitglieder besitzen, selbständige Antrags- und Beteiligungsrechte, wie auch grundsätzliche Zugangsrechte zu diesen Betrieben. Bei Unternehmen, die mehr als 2000 Beschäftigte haben, haben sie das Recht zwei oder drei der den dort Beschäftigten zustehenden Aufsichtsräte (zwischen sechs und zehn je nach Betriebsgröße) direkt zu stellen. Zwar werden auch die Gewerkschaftsvertreter von den stimmberechtigten Mitarbeitern des Betriebes oder deren Delegierten gewählt, jedoch steht das Vorschlagsrecht hierzu allein der Gewerkschaft zu. So saßen nach einer Ermittlung des Instituts der deutschen Wirtschaft aus dem Jahre 2006 in den Aufsichtsräten der mitbestimmungspflichtigen Unternehmen ca. 1.700, zum teil hochrangige, Vertreter der Gewerkschaften. An der Sozial- und Arbeitsverwaltung nehmen die Gewerkschaften teilweise durch Entsendung von Mitgliedern teil und treten immer dort als Vertreter der Arbeitnehmer auf, wo die Arbeitgeber sich von ihren Verbänden vertreten lassen. Aufgrund ihrer Stellung entsenden sie auch ihre Vertreter in allgemeine Einrichtungen, wie etwa den bei den öffentlich-rechtlichen Rundfunkanstalten gebildeten Rundfunkräten.

Ein wichtiger Maßstab für die Durchsetzungskraft einer Gewerkschaft oder eines Gewerkschaftsverbandes
ist der Organisationsgrad. Der (Netto-)Organisationsgrad bezeichnet dabei den Anteil der in einer Branche oder einem Organisationsbereich beschäftigten Gewerkschaftsmitglieder an allen in dieser Branche oder diesem Bereich Beschäftigten.

Manche Gewerkschaften in manchen Ländern haben wie andere gesellschaftliche Großorganisationen in den letzten Jahrzehnten einen erheblichen Mitgliederschwund gehabt. Das Gesamtbild ist jedoch nicht einheitlich. Häufig genannte Gründe für einen Rückgang sind eine gesellschaftliche Tendenz zur Individualisierung, kleiner werdende Betriebsstrukturen, Verringerung von Arbeitsplätzen in der Industrie zu Gunsten des Dienstleistungsbereichs, aber auch der Führungsstil der Gewerkschaften, Korruptionsaffären und Unzufriedenheit der Mitglieder mit den Ergebnissen bei der Durchsetzung von Entgelterhöhungen.
Einzelheiten sind der folgenden Tabelle zu entnehmen. In Deutschland verzeichnen manche Gewerkschaften inzwischen wieder Mitgliederzuwächse.

Organisationsgrad in %: aktive Mitglieder (ohne Rentner) zu abhängig Beschäftigte zzgl. Arbeitslose; Länder mit *: Mitglieder zu abhängig Beschäftigte.

Die erste Gewerkschaft („Vakbond“) wurde in Belgien von den Schriftsetzern in Brüssel 1842 gegründet. Belgien hat mit ca. 80 % der Beschäftigten einen der höchsten gewerkschaftlichen Organisationsgrade in Europa. In Belgien gibt es verschiedene Gewerkschaftsrichtungen, unter anderem „Freie Gewerkschaften“.

Die Gewerkschaften übernehmen in Belgien auch Aufgaben als Arbeitslosenkassen.

In Belgien und damit auch in der Deutschsprachigen Gemeinschaft sind die wichtigsten Organisationen:

Neben diesen drei Organisationen sind folgende Gewerkschaften (Vakbonden) im flämischen Teil Belgiens registriert:

In Frankreich gibt es verschiedene vorwiegend politisch ausgerichtete Gewerkschaften:

Die Confédération française démocratique du travail (CFDT, "Französischer Demokratischer Gewerkschaftsbund") ist mit etwa 800.000 Mitgliedern der größte Gewerkschaftsbund Frankreichs.
Gegründet wurde die CFDT 1964, als die Mitglieder des christlichen Gewerkschaftsbunds "Confédération française des travailleurs chrétiens" (CFTC) sich mehrheitlich für eine Säkularisierung und die Umbenennung zur CFDT entschieden. Etwa ein Zehntel ihrer Mitglieder verließ allerdings in der Folge den Verband und gründete die CFTC wieder neu.

In ihren frühen Jahren stand die CFDT politisch der Parti socialiste unifié (PSU) nahe, ab 1974 dann der "Parti socialiste" (PS) unter François Mitterrand. Inzwischen ist die CFDT allerdings politisch weitgehend ungebunden und unterstützte beispielsweise 1995 den konservativen Premierminister Alain Juppé bei der Durchsetzung heftig umstrittener Sozialstaatsreformen.

Die Confédération générale du travail (CGT, "Allgemeiner Gewerkschaftsbund") ist ein französischer Gewerkschaftsbund, der traditionell der Kommunistischen Partei Frankreichs nahesteht. Seit einigen Jahren ist allerdings zu beobachten, dass diese Verbindung zunehmend schwächer wird.

Gegründet wurde die CGT 1895 auf einem Kongress in Limoges durch den Zusammenschluss der "Fédération des bourses du travail" und der "Fédération nationale des syndicats".

Heute ist die CGT mit etwa 700.000 Mitgliedern der zweitgrößte Gewerkschaftsbund Frankreichs. Geographische Schwerpunkte sind das Département Ariège im Südwesten des Landes (an der Grenze zu Spanien und Andorra) und die Region Limousin.

Die Confédération générale du travail-Force ouvrière (CGT-FO; sinngemäß "Allgemeiner Gewerkschaftsbund-Arbeitermacht"), heute in aller Regel "Force ouvrière (FO)" genannt, ist einer der vier bedeutenden Gewerkschaftsbünde in Frankreich. Sie ist traditionell der gemäßigten Linken zuzuordnen.

Ihre "Raison d’Être" (Daseinszweck) ist die Dominanz der KPF im Allgemeinen Gewerkschaftsbund (CGT) nach dem Zweiten Weltkrieg. Die Gründung erfolgte 1948. Sie sieht ihr Ziel in der Verteidigung der Ideale der Republik: (Freiheit, Gleichheit, Brüderlichkeit, Trennung des Staates von der Religion). Die Besonderheit der CGT-FO liegt in der parteipolitischen Profillosigkeit.

Die Confédération paysanne (auch als die "conf’" bekannt) ist eine französische Bauerngewerkschaft. Sie ist Mitglied in der Europäischen Bauern Koordination und der Via Campesina.

Sie wurde 1987 gegründet. Sie entstand aus der Vereinigung zweier kleinerer Gewerkschaften, der FNSP und der CNSTP. 2001 erhielt sie 28 % der Stimmen bei den Wahlen zur Landwirtschaftskammer, dies machte sie zur zweitgrößten landwirtschaftlichen Gewerkschaft in Frankreich. Sie ist in allen französischen Départements und Übersee-Départements aktiv.

Sie kämpft für eine kleinräumige Landwirtschaft, für den Umweltschutz und für die Qualität der erzeugten Produkte. Ihre öffentlichkeitswirksamen Aktionen gegen die WTO im August 1999 und gegen die Nutzung von gentechnisch veränderten Organismen in der Landwirtschaft sowie die Skandale um dioxinverseuchtes Hühnerfleisch und BSE in der Massentierhaltung ließen ihre Popularität in Frankreich steigen.

Die Confédération française des travailleurs chrétiens (CFTC, "Französischer Bund christlicher Arbeiter") ist ein französischer Gewerkschaftsbund mit derzeit etwa 130.000 Mitgliedern.

Gegründet wurde der Verband 1919 durch den Zusammenschluss von 321 Gewerkschaften. Die Programmatik lehnte sich an die Enzyklika Rerum Novarum von Papst Leo XIII. an. Daher wurde eine Revolution, als deren Instrument sich der kommunistische Konkurrenzverband CGT sah, von der CFTC stets zugunsten von Reformen abgelehnt. 1964 entschied sich eine Mehrheit der Mitglieder der CFTC für die Säkularisierung und die Umbenennung in "Confédération française démocratique du travail (CFDT)". Etwa zehn Prozent der Mitglieder verließen daraufhin die Organisation und gründeten die CFTC neu.

Die CFTC steht politisch weiter rechts als die CFDT. Sie ist der einzige Gewerkschaftsbund Frankreichs, dessen Mitglieder bei der französischen Präsidentschaftswahl 2002 überdurchschnittlich oft für den rechtsextremen Kandidaten Jean-Marie Le Pen stimmten.

Die Coordination Française Nationale des Travailleurs ist der "Gewerkschaftsbund" der rechtsextremen Partei Front national (FN). Eigentlich sollte man vielleicht eher als einen Korportationsbund ansehen, da dieser Bund mehrere Minigewerkschaften (Polizei, Gefängnispersonal, Verkehrsmittel, Post, Bildung), Arbeitsgeberorganisationen, Mietervereine vereint. 1997 nahm er an den Sozialwahlen teil, was ihn nicht annähernd auf fünf Prozent kommen ließ. Dies sollte durch Klagen von Gewerkschaften verhindert werden. Einer der Gründe, warum er sehr hinter den Wahlergebnissen der FN zurückbleibt, ist die Tatsache, dass er nicht allgemein als Gewerkschaftsorganisation anerkannt ist, und dass einige Teilgewerkschaften nur auf dem Papier existieren.

In Frankreich tragen drei (oder vier) Gewerkschaften den Namen Confédération nationale du travail (CNT):

Die "Confédération Nationale du Travail-Vignoles (CNT-F)" ist eine revolutionär-syndikalistische Gewerkschaftsföderation.

Die "Confédération Nationale du Travail – AIT (CNT-IAA) (auch CNT-F genannt innerlich der IAA)" ist der anarchosyndikalistischen Internationalen ArbeiterInnen-Assoziation (IAA) angeschlossen. Sie ist eine dezentral aufgebaute anarchistische Gewerkschaftskonföderation und besteht aus lokalen und betrieblichen Gruppen (Syndikaten) in 17 französischen Städten. Ihre Mitgliederzahl liegt nach eigenen Angaben derzeit im dreistelligen Bereich.

Die "Confédération Nationale du Travail – Solidarité Ouvrière (CNT-SO)" ist eine Spaltung von der französischen CNT-AIT, die ein Bedürfnis nach der Nützung von Gewerkschaftsfunktionären anerkennt.

Ihren Ursprung haben alle "Mini"-gewerkschaften in einer Gruppe, die 1946 von im französischen Exil lebenden spanischen Anarchosyndikalisten gegründet wurde, die in Paris noch einige Mitglieder in die "Confédération Nationale du Travail – Tour d'Auvergne" hat, auch wenn diese keine Aktion heutzutage zu haben scheinen.

Die Union syndicale Solidaires ist ein linker basisdemokratischer Gewerkschaftsverband. Sie gilt als eine Abspaltung der gemäßigten CFDT, die die Selbstorganisation weiterfördern wollte. Eigentlich fundierten es zehn unabhängige Gewerkschaften als "Groupe des 10 (G10)". In 89 integrierte der Bund die Gewerkschaft Sud-PTT, eine Spaltung der CFDT in dem französischen Postdienst. Ein paar Jahren später folgten andere CFDT Teilgewerkschaften unter die Name Sud. Die Einzelgewerkschaften gehören der "G10 Solidaires" an, auch wenn die Bevölkerung oft die Name "Sud" besser als Solidaires kennt. Nach einigen Politiker, sollte die trotzkistische "Ligue communiste révolutionnaire" versuchen, sich durch Einflussnahme in der SUD zu etablieren. Laut andern sollte es den reformistischen Linksbund "Ensemble !" sein, und andere noch behaupten, es seien eher die Anarchisten von "Alternative Libertaire". G10 hat auch an die Gründung der linksalternativen Think Tank "Attac" teilgenommen.

1993 geboren aus der Spaltung der FEN, und der Vereinigung mit der FAT, der FMC, der FGSOA und der FGAF.

Schon im ausgehenden 18. Jahrhundert begann in Großbritannien die Entwicklung der Industrialisierung; der bisherige Agrarstaat wandelte sich. Noch waren die Handwerker in „Guilds“ – also Gilden – organisiert. Die dann zunehmende Industrialisierung war der Kern der Entwicklung zu „trade unions“ oder „labour unions“, also zu den Gewerkschaften. Ein wichtiger Schritt war die formale staatliche Anerkennung der Gewerkschaften im Jahr 1872, die einerseits einen Meilenstein der Demokratisierung darstellte, andererseits die britischen Gewerkschaften auch davon abhielt, sich in einer Partei zu organisieren, um grundlegende politische Ziele durchzusetzen, wie es etwa bei der deutlich früheren Entwicklung von Arbeiterparteien in anderen, stärker repressiven Staaten der Fall war.

Großbritannien hat als früher Industriestaat die bedeutendste historische Entwicklung in der Gewerkschaftsbewegung. Davon zeugen die rund 108 historischen Gewerkschaften. Auch der Trade-Unionismus hat seinen Ursprung in den industriellen Zentren Englands.
Traditionell arbeiteten die Gewerkschaften politisch mit der Labour Party zusammen, welche in ihren Anfangsjahren keine Individualmitgliedschaft kannte. Mitglieder der Labour Party waren automatisch auch Gewerkschaftsmitglieder, sofern sich die entsprechenden Gewerkschaften für einen Beitritt zur Labour Party entschlossen haben. Vor der Zusammenarbeit mit Labour bestand jedoch ein enges Verhältnis der Trade Unions zur Liberalen Partei, welche über Jahrzehnte die parlamentarische Interessenvertretung der Gewerkschaften war ("Lib Lab"). Labour konnte anfangs kaum Anklang bei der Mehrheit der Gewerkschafter finden. Auch frühere Gründungen von sozialdemokratischen Parteien blieben erfolglos, so die Independent Labour Party und die Social Democratic Federation, die sich später der Labour Party anschlossen. Britische Gewerkschaften waren vor allem durch ein hohes Maß an Entpolitisierung und Misstrauen gegenüber sozialistischen Ideen geprägt. Revolutionäre Umsturzpläne waren unter Gewerkschaftsmitgliedern praktisch nicht zu finden. Eine Folge der langen Tradition von Gewerkschaften in Großbritannien und dem Ausbleiben allzu großer Repressionen gegen die Arbeitervertretung, etwa im Gegensatz zum bismarckschen Deutschland.

In Großbritannien herrschte bis 1980 das Closed-Shop-Modell (dt. hier in etwa: in sich geschlossener Betrieb), das heißt, alle Mitarbeiter eines Betriebes mussten pflichtmäßig der Gewerkschaft angehören.
Schon der Winter of Discontent 1978/1979, in dem es zu ausgedehnten Streiks der Gewerkschaften mit dem Ziel signifikanter Lohnsteigerungen kam, endete mit einer erheblichen Schwächung der Gewerkschaftsbewegung. In der Folge kam Margaret Thatcher bei den Parlamentswahlen am 4. Mai 1979 an die Regierungsmacht.

Im Britischen Bergarbeiterstreik 1984/1985 verlor die National Union of Mineworkers (NUM) immer mehr den Rückhalt der britischen Bevölkerung und führte zu einer zunehmenden Radikalität des Bergarbeiterführers Arthur Scargill. Nach über einem Jahr des Arbeitskampfes gewann die Regierung unter Margaret Thatcher die Oberhand über die Bergarbeitergewerkschaft.

Es folgte die Abschaffung des Closed Shop (Pflichtmitgliedschaft in Gewerkschaften für Arbeiter zahlreicher Unternehmen) und dem Verbot der so genannten Flying Pickets (Streikposten, die nicht dem bestreikten Betrieb angehören). In der Wirtschaft wurden daraufhin einige von den Gewerkschaften zuvor bekämpften technischen Innovationen nachgeholt. So konnten beispielsweise Ende der Achtziger die britischen Zeitungen vom Bleisatz auf den in anderen Ländern schon seit langem üblichen Fotosatz umgestellt werden, was die Gewerkschaften bis dahin immer verhindert hatten.

Die Bergarbeitergewerkschaft National Union of Mineworkers (NUM) verlor von 253.000 Mitgliedern im Jahre 1979 auf nunmehr unter 5.000 Mitglieder im Jahre 2000. Die "Transport and General Workers Union (TGWU)" hatte im Jahre 1979 noch 2.086.000 Mitglieder und kam im Jahre 2000 noch gerade auf 858.000 Mitglieder.


Wegen ihrer Größe oder Bedeutung sind hervorzuheben:

1908 wurde in Irland die "Irish Transport and General Workers’ Union" (ITGWU; Irische Transport- und Arbeiterunion) als eine irische Gewerkschaftsunion von James Larkin gegründet. Anfangs bezog die ITGWU ihre Mitglieder hauptsächlich von der in Liverpool ansässigen "National Union of Dock Labourers" (Nationale Union der Hafenarbeiter), aus der Larkin 1908 ausgeschlossen worden war. Später umfasste die Union Mitglieder verschiedenster Industriezweige.

Die ITGWU war der Dreh- und Angelpunkt während des "Dublin Lockout" im Jahr 1913 – einem monatelangen Generalstreik, der die ITGWU und die Arbeiterbewegung nachhaltig beeinflusste. Nach der (für die ITGWU fehlgeschlagenen) Aussperrung wanderte Larkin 1914 nach Amerika aus, und William X. O’Brien wurde der neue Anführer und diente später für viele Jahre als Generalsekretär.

Larkin kehrte 1923 nach Irland zurück und traf sich mit Mitgliedern der "Trade Union", um ein Ende des Irischen Bürgerkriegs zu bewirken. Trotz aller Bemühungen befand sich Larkin im Widerspruch zu William O’Brien, der in dessen Abwesenheit zur tragenden Figur der ITGWU, der "Irish Labour Party" und dem "Trade Union Congress" aufgestiegen war. Der bittere Streit zwischen den beiden sollte über zwanzig Jahre andauern.

1924 gründete Larkins Bruder Peter eine neue Union, die "Workers’ Union of Ireland" (WUI), zu der viele ITGWU-Mitglieder aus Dublin wechselten. Doch trotz des Mitgliederschwunds blieb die ITGWU die dominante Kraft unter den Gewerkschaftsverbänden, vor allem außerhalb der Hauptstadt Dublin.

1945 verließ die ITGWU den "Irish Congress of Trade Unions", als der "Congress" die Mitgliedschaft der WUI akzeptierte, und begründete den rivalisierenden "Congress of Irish Unions".

1990 schloss sich die ITGWU letztendlich mit der WUI zur neuen Gewerkschaft, der "Services, Industrial, Professional and Technical Union" (SIPTU) zusammen.

Italien hat – wie in Frankreich – politisch orientierte Richtunggewerkschaften:

Die Confederazione Generale Italiana del Lavoro (CGIL) ist ein nationaler Gewerkschaftsbund in Italien. Sie wurde im Juni 1944 gegründet durch die Einigung von Sozialisten, Kommunisten und Christdemokraten, niedergelegt im so genannten "Vertrag von Rom". Sie hat mehr als 5 Mio. Mitglieder, von denen ungefähr 2,3 Mio. aktiv sind. Die CGIL ist Mitglied im Internationalen Bund Freier Gewerkschaften und dem Europäischen Gewerkschaftsbund sowie des "Trade Union Advisory Committee", einem gewerkschaftlichen Beratungskomitee der OECD.

Die Unione Sindacale Italiana (USI; Union der italienischen Syndikalisten) war der Dachverband der italienischen Syndikalisten, der im Zuge des "Biennio rosso" ca. 1 Mio. Mitglieder zählte. Die USI wurde 1912 gegründet. Sie sagte sich vom Reformismus los und orientierte sich an den radikalen Grundsätzen der Ersten Internationale. Sie schloss sich später der anarcho-syndikalistischen "International Workers Association" (IWA) an.

Nach Kriegsende erreichte die Mitgliedszahl im "Biennio rosso", den beiden „roten“ Jahren 1919 und 1920, einen Höhepunkt (ca. 1.000.000). Während dieser Zeit schloss sich der "International Workers Association" (IWA; "Associazione Internazionale dei Lavoratori", AIT; "Asociación Internacional de los Trabajadores") an. Sie nannte sich auch USI-AIT und wurde zum Hauptgegner Mussolinis in den Straßenschlachten des "Biennio rosso" und "Biennio nero". Die USI-AIT wurde 1926 von Mussolini verboten, setzte jedoch ihre Tätigkeit im Untergrund und im Exil fort.

Nach dem Zweiten Weltkrieg folgten die verbliebenen Mitglieder dem Ruf der "Federazione Anarchica Italiana" die zur Teilnahme an einer Einheitsgewerkschaft aufrief und fusionierte mit der "Confederazione Generale Italiana del Lavoro" (CGIL). Als sich die CGIL 1950 teilte, gründeten einige Aktivisten die USI-AIT erneut, konnten aber auch nicht annähernd an frühere Erfolge anschließen. Bis in die 1960er Jahre war sie in einigen Regionen vertreten, die syndikalistische Botschaft hielt sie bis zuletzt aufrecht.

Die Confederazione Generale dei Sindacati Autonomi dei Lavoratori (CONF.S.A.L.) ist ein 1979 gegründeter italienischer Gewerkschaftsbund. Er entstand aus einer Fusion der seit vielen Jahren existierenden SNALS und UNSA. Er gehört dem europäischen Gewerkschaftsbund CESI an, dessen Mitgliedsorganisationen nicht sozialistisch sind.

Die Confederazione Italiana Sindacati Autonomi Lavoratori (CISAL) ist ein kleinerer Gewerkschaftsbund in Italien. Er wurde 1957 gegründet. Als einziger Gewerkschaftsbund weist er eine Präferenz für die von relativ vielen Arbeitern gewählten Partei "Forza Italia" des früheren Ministerpräsidenten Silvio Berlusconi auf. Die CISAL zählt 1.700.000 Mitglieder.

Der Autonome Südtiroler Gewerkschaftsbund (ASGB) ist die gewerkschaftliche Organisation der deutschen und ladinischen Arbeiterschaft in Südtirol. Er wurde 1964 von ehemaligen Mitgliedern der mehrheitlich italienischen Gewerkschaft CISL gegründet, um den Arbeitern der deutschen und ladinischen Volksgruppe eine eigenständige Gewerkschaftsvertretung zu ermöglichen. Heute ist der ASGB mit 27.000 Mitgliedern (2006) die stärkste Gewerkschaftsorganisation in Südtirol.

Die Confederazione Italiana Lavoratori Liberi (CONF.ILL; Italienischer Bund freier Arbeiter) ist eine Mitgliedsorganisation des Bundes europäischer nicht-sozialistischer Gewerkschaftsbünde CESI. Die CONF.ILL ist kein Gewerkschaftsbund im juristischen Sinne, hat aber in Italien etwa 200.000 Mitglieder.

Die Unione Generale del Lavoro (UGL) ist ein unbedeutender italienischer Gewerkschaftsbund. Er steht der postfaschistischen Partei Alleanza Nazionale nahe. Die deutschen Gewerkschaften kooperieren nicht mit der UGL. 1950 wurde die UGL als CISNAL gegründet und trägt seit 1996 den jetzigen Namen.

Das Sindacato Padano (SINPA, sinngemäß "Oberitalienische Gewerkschaft") ist eine sehr kleine Gewerkschaft in Norditalien. Sie steht der norditalienischen Befreiungsbewegung "Lega Nord" nahe und besitzt nur eine eingeschränkte Bedeutung. Sie wurde 1996 als "Sindacato Autonomo di Lavoratori Padani" (Autonome Gewerkschaft der oberitalienischen Arbeiter) gegründet.

Die erste Gewerkschaft (vakbond) in den Niederlanden war der "Algemene Nederlandse Grafische Bond", die 1866 gegründet wurde.

Es gibt folgende Gewerkschaften (Vakbonden) in den Niederlanden:


Der Österreichische Gewerkschaftsbund ist eine 1945 gegründete überparteiliche Interessenvertretung für Arbeitnehmer. Traditionell wird er aber von den Sozialdemokraten dominiert. Er ist als Verein konstituiert und gliedert sich intern in sieben (Stand 2011) Teilgewerkschaften. Die Zentrale befindet sich in Wien (Laurenzerberg). Seit 2009/2010 befindet sich der ÖGB aber mit den Fachgewerkschaften PRO-GE, GBH, vida, GPF, GdG-KMSfB und dem Verband Österreichischer Gewerkschaftlicher Bildung (VÖGB), dem ÖGB-Verlag und dem Reiseunternehmen Sotour Austria in einem gemeinsamen Haus. Es befindet sich am Handelskai im 2. Wiener Gemeindebezirk (Johann-Böhm-Platz 1, 1020 Wien).

Die sieben Teilgewerkschaften sind:

Weiters sind die Gewerkschaften GBH, KMSfB, GPF und vida als "infra – Die Allianz der Infrastrukturgewerkschaften" gemeinsam tätig.

Die Gewerkschaftsbewegung war ein integraler Bestandteil der Arbeiterbewegung in Schweden. Die ersten Gewerkschaftsvereine wurden in den 1870er Jahren nach britischem und deutschem Vorbild gebildet. Der Durchbruch gelang infolge der großen Streikwelle in Norrland um 1880. Diese Streiks, die vom Militär niedergeschlagen worden waren, wiesen auf die Notwendigkeit einer einheitlichen Organisation hin. In den folgenden Jahren entstand eine Reihe von Gewerkschaften, die sich 1898 in einem Dachverband, der Landesorganisation LO zusammenschlossen.

Die positive Entwicklung wurde durch den Generalstreik von 1909 unterbrochen, weil dieser nach einigen Wochen zusammenbrach. Viele Mitglieder verließen die Landesorganisation und schlossen sich einer neugegründeten syndikalistischen Bewegung nach französischem Vorbild, "Sveriges Arbetares Centralorganisation" (SAC), an. Zunächst war die Konkurrenz zwischen diesen beiden Gewerkschaften stark, doch verlor die syndikalistische Gewerkschaft nach dem Ersten Weltkrieg schnell an Bedeutung.

Die Landesorganisation ging schon früh eine enge Verbindung mit dem politischen Zweig der Arbeiterbewegung, der Sozialdemokratischen Arbeiterpartei (SAP), ein. Mit dem Erstarken der SAP bekam auch LO eine stärkere Position bei der Durchsetzung gewerkschaftlicher Fragen.

Gewerkschaften und Arbeitgeber bekämpften gleichermaßen Versuche des Staates, den Arbeitsmarkt durch Gesetzgebung zu regeln. 1938 wurde das historisch bedeutsame "Abkommen von Saltsjöbaden" geschlossen: LO und die Arbeitgeberorganisation "Svenska Arbetsgivareföreningen " (SAF) legten Rahmenbedingungen für die Sozialpartnerschaft fest, die bis in die 1960er Jahre Gültigkeit behielten. Die Vertragspartner einigten sich über eine Verhandlungsordnung und stellten Regeln für den Einsatz von Kampfmaßnahmen auf.

In der Zwischenkriegszeit entstanden auch die ersten Angestelltengewerkschaften. Die soziale und berufliche Situation der Angestellten war besser als die der Arbeiter, und sie standen den Arbeitgebern näher. Diese Angestelltengewerkschaften traten nicht dem SAP-nahen Dachverband LO bei, sondern bildeten 1944 einen eigenen Dachverband, "Tjänstemännens Centralorganisation" (TCO). 1947 entstand die letzte der drei Dachorganisationen, die Akademiker-Gewerkschaft SACO.

In den 1960er und 1970er Jahren wuchsen die Gewerkschaften stark. Der Organisationsgrad betrug Mitte der 1980er Jahre 85 %. Danach stagnierte zwar die Mitgliederzahl, aber der heutige Organisationsgrad von 69 % (2016) ist einer der höchsten in allen Industriestaaten. Der Frauenanteil gleicht in skandinavischen Gewerkschaften dem Männeranteil und ist somit deutlich höher als im übrigen Europa.

Die veränderten Bedingungen auf dem Arbeitsmarkt haben zu einer verstärkten Zusammenarbeit der Gewerkschaftsverbände seit den 1990er Jahren geführt.


In der Schweiz ist heute gut jeder vierte Beschäftigte in einer Gewerkschaft oder einem gewerkschaftsähnlichen Verband organisiert. Im westeuropäischen Vergleich ist dies eher wenig.

Der Schweizerische Gewerkschaftsbund (SGB) ist die größte Arbeitnehmerorganisation der Schweiz. In ihm sind sechzehn Einzelgewerkschaften zusammengeschlossen, die insgesamt rund 380.000 Mitglieder vertreten. Die Schweizer Gewerkschaften waren von Anfang an um die Schaffung eigener Selbsthilfe- und Sozialeinrichtungen bemüht: Zunächst Arbeitslosen- und Krankenkassen, Alters- und Unterstützungskassen, später Schulungs- und Ferien- und Sozialeinrichtungen. Von jeher gewährten die Gewerkschaften ihren Mitgliedern auch Rechtsschutz.

2002 wurde in Bern unter dem Namen Travail.Suisse eine neue Dachorganisation der Arbeitnehmenden gegründet. Urheber dieser Gründung waren die Verbände und Gewerkschaften, die vorher dem Christlichnationalen Gewerkschaftsbund der Schweiz (CNG) und der Vereinigung schweizerischer Angestelltenverbände (VSA) angeschlossen waren. Diese Verbände vertreten rund 170.000 Mitglieder.

Mit dem Zusammenschluss der GBI, SMUV, VHTL, unia und actions zur Unia ist im Herbst 2004 die größte Gewerkschaft der Schweiz mit rund 200.000 Mitgliedern und fast 100 Sekretariaten entstanden. Die Anstellungsbedingungen von gegen einer Million Menschen sind in Gesamtarbeitsverträgen geregelt, die von der Unia ausgehandelt werden.


In Spanien gibt es verschiedene Gewerkschaftsbünde. Zu den größten gehören die "Comisiones Obreras", die "Confederación General del Trabajo" und die "Union Sindical Obrera". Regionale Organisationen sind die "Confederación Intersindical Galego" in Galicien, die baskische demokratische ELA-STV und die baskische nationalistische "Langile Abertzalen Batzordeak". Des Weiteren ist die anarcho-syndikalistische Gewerkschaft "Confederación Nacional del Trabajo" (CNT) eine einflussreiche Gewerkschaft, v. a. im katalanischen Raum um Barcelona.

In den USA entstanden die Gewerkschaften nicht wie die europäischen Gewerkschaften aus klassenkämpferischen Motiven, sondern sie waren überwiegend eine Schutzgemeinschaft gegenüber den Einwanderern und den Nichtorganisierten. Die US-Gewerkschaften des späten 19. Jahrhunderts verstanden sich als Gegenentwurf zu den Unternehmenskartellen gewissermaßen als „Lohnkartelle“, die im kapitalistischen System die Rolle des Anbieters von Arbeitskraft einnahmen und diese im Interesse ihrer Mitglieder möglichst teuer verkaufen wollten. In der Anfangszeit wurden Arbeitgeber oft erpresst und mit Gewalt gezwungen, nur Gewerkschaftsmitglieder einzustellen. Umgekehrt war die Mitgliedschaft in einer US-Gewerkschaft üblicherweise an hohe Eintrittsgebühren oder Abgabe eines Teils des Lohnes sowie Aufnahmeprüfungen gebunden.

Als erste US-Gewerkschaft wurde die National Labor Union (NLU) am 20. August 1866 in Baltimore gegründet. Ihr wichtigstes Ziel war die Einführung des Achtstundentages. Schwarze oder Chinesen wurden in dieser "Trade Union" anfangs nicht als Mitglied aufgenommen und gründeten eigene kleine Gewerkschaften. In ihrer Grundsatztagung von 1869 erklärte die NLU, Arbeiterrechte unabhängig von Geschlecht und Hautfarbe zu vertreten, und öffnete sich ab nun auch Frauen und Schwarzen. Die NLU beschränkte sich jedoch zunehmend auf die Arbeit im Parlament (Einbringen von Gesetzesinitiativen); ihre Bedeutung nahm ab und sie löste sich 1873 auf.

Schon 1869 wurden die Knights of Labor gegründet und wurden bald zur führenden Gewerkschaft in den USA. Sie hatte bis zu 700.000 Mitglieder im 19. Jahrhundert. Ihre Ziele waren neben der Erhöhung der Löhne die Abschaffung der Kinderarbeit und die Einführung des Achtstundentages. Sie forderten „a proper share of the wealth they create, […] more free time, and generally more benefits of society“, kurz also eine gerechtere Vermögensverteilung, mehr Freizeit und eine sozialere Gesellschaft. Bekannt wurde aus den Reihen dieser Gewerkschaft Thomas Mooney (1882–1942), der ein Arbeiteraktivist in San Francisco war und der in den Bombenanschlag beim "Preparedness Day" 1916 verwickelt gewesen sein soll.
Eine Unterstützerin der Gewerkschaftsforderungen war ab 1870 Victoria Woodhull, die zeitweise dem amerikanischen Zweig der Ersten Internationale angehörte.

1886 gründete Samuel Gompers als Reorganisation der Vorgängerorganisation "Federation of Organized Trades and Labor Unions" in Columbus (Ohio) die "American Federation of Labor" (AFL, heute AFL-CIO), die US-weit schon bald 1,4 Millionen Mitglieder aufweisen sollte. Sie war einer der ersten Facharbeitergewerkschaftsbünde in den USA. Gompers war bis zu seinem Tode im Jahr 1924 Präsident der AFL.

1886 schlossen sich viele Einzelgewerkschaften zum Dachverband AFL zusammen, der rund die Hälfte der amerikanischen Arbeiter vertrat. Nach außen setzte der Verband auf die Abriegelung vor allem gegenüber der wegen der Fließbandfertigung wachsenden Schicht der ungelernten Arbeiter. Die AFL konzentrierte sich auf die direkten, aktuellen Forderungen der Arbeiter und stellte die Rechte der Eigentümer der Produktionsmittel im Kapitalismus nicht in Frage. Sie unterstützte einzelne Politiker, welche die Interessen der Arbeiter vertrat, nicht jedoch einzelne Parteien. Sie stand im Gegensatz zu den radikaleren "Industrial Workers of the World" (IWW), die sich als Klassenkampforganisation verstand und alle Arbeiter und Arbeiterinnen unabhängig von ihrem beruflichen Status organisierte.

1938 spalteten sich zehn Gewerkschaften von der AFL ab und gründeten den neuen Dachverband CIO, der sich gezielt Ungelernten öffnete. Beide Verbände standen sich in den folgenden Jahrzehnten verfeindet gegenüber. Nach dem Zweiten Weltkrieg gerieten die Gewerkschaften unter Druck durch die republikanische Mehrheit im Kongress und den Präsidenten Eisenhower. 1955 vereinigte sie sich deshalb wieder mit dem CIO und existiert heute als AFL-CIO. Die Mitgliederschaft nimmt kontinuierlich ab. 1955 waren 34 % der Arbeiter in den USA in dem neuen Dachverband organisiert. 2005 waren es 12 %. Für 2008 wird der gewerkschaftliche Organisationsgrad in der privaten Wirtschaft nur noch mit 7,5 % angegeben.

2005 traten zahlreiche Gewerkschaften (darunter mit der Dienstleistungsgewerkschaft SEIU die größte Einzelgewerkschaft) aus und formierten sich unter dem neuen Dachverband "Change to Win".

Seit dem späten 20. Jahrhundert wandeln sich die US-Gewerkschaften programmatisch immer mehr zu einer ganzheitlichen Vertretung aller Arbeitnehmer. Ihr Einfluss ist seit 1980 stetig zurückgegangen.

Die "United Steel, Paper and Forestry, Rubber, Manufacturing, Energy, Allied Industrial and Service Workers International Union" (USW) ist mit über 1,2 Millionen aktiven und ehemaligen Mitgliedern die größte Industriegewerkschaft Nordamerikas. Sie vertritt Arbeiter in den USA und in Kanada. Hauptsitz der Gewerkschaft ist Pittsburgh (Pennsylvania).

Die USW wurde als USWA am 22. Mai 1942 von Mitgliedern der "Amalgamated Association of Iron, Steel, and Tin Workers" und des "Steel Workers Organizing Committee" gegründet. Zuvor war es zu häufigen, teilweise gewaltsamen Auseinandersetzungen von streikenden Arbeitern und Streikbrechern oder der Polizei gekommen. Erster Präsident der Gewerkschaft war Mitbegründer Philip Murray.

Die "American Railway Union" (ARU) war die größte Gewerkschaft der 1890er Jahre in Amerika und die erste Industriegewerkschaft in den Vereinigten Staaten. Sie wurde am 20. Juni 1893 von Bahnarbeitern in Chicago, Illinois, unter der Führung von Eugene V. Debs (einem Lokomotivheizer) gegründet. Die ARU verkörperte in ihrer Politik, anders als die anderen Gewerkschaften, eine Vertretung aller Bahnmitarbeiter, unabhängig davon, ob sie nun als Handwerker oder im Dienst am Kunden bei einer Eisenbahngesellschaft eingesetzt waren. Innerhalb eines Jahres hatte die ARU Hunderte von Ortsgruppen und über 140.000 Mitglieder im ganzen Land.

1893 kürzte die Eisenbahngesellschaft "Great Northern Railway" die Löhne ihrer Beschäftigten. Bis zum April entschied sich die ARU für einen Streik und legte damit die Eisenbahn für 18 Tage lahm. Sie erzwang dadurch von der Gesellschaft die Rücknahme der Lohnkürzungen bei ihren Arbeitern. Dies war der erste und einzige Sieg der Gewerkschaft.

In ähnlicher Weise kürzte die "Pullman Palace Car Company" ihre Löhne fünfmal – um 30 bis 70 Prozent – zwischen September 1893 und März 1894. Viele Pullman-Arbeiter waren inzwischen in die Eisenbahnergewerkschaft eingetreten. Eine ARU-Versammlung sich aus Solidarität den Streikenden anzuschließen und boykottierte Pullman-Waggons. Der Boykott wurde ein großer Erfolg. Als Antwort darauf gab das Pullman-Management die Order aus, Pullman-Waggons an die Postzüge anzuhängen, um sich so eine Unterstützung ihres Standpunktes über den Postdienst zu verschaffen und die Bundesregierung zu interessieren.

Mit Hilfe des "Sherman Antitrust Acts" von 1890, der geregelt hatte, es sei für Firmenzusammenschlüsse illegal, Handelsbewegungen oder den Handel einzuschränken, wurde ein gerichtliches Verbot am 2. Juli erwirkt. Es untersagte der ARU-Führung, durch „Zwingen oder Herbeiführen von Drohungen, Einschüchterung, Überredung, Gewalt oder Tätlichkeiten, Bahnangestellten zu verwehren oder sie zu hindern, ihre Aufgaben durchzuführen.“ Am nächsten Tag befahl US-Präsident Grover Cleveland 20.000 Mann Bundestruppen, den Streik zu zerschlagen und für den Zuglauf zu sorgen. Bis zum 7. Juli wurden Debs und sieben andere ARU-Funktionäre festgenommen, angeklagt und wegen „Verschwörung zur Störung des freien Postverkehrs“ verurteilt. Der Streik wurde endgültig zerschlagen. Die ARU löste sich schließlich auf. Die "Pullman Company" öffnete ihren Betrieb ohne die entlassenen Gewerkschaftsführer wieder.

Die "International Brotherhood of Teamsters" (IBT) (sinngemäß: Internationale Bruderschaft der Fuhrleute), kurz nur "Teamsters" genannt, ist die Gewerkschaft der Transportarbeiter und damit die größte Einzelgewerkschaft der USA und ist seit 1992 auch in Kanada als "Teamsters Canada" tätig.

Die IBT, vormals auch bekannt als "International Brotherhood of Teamsters, Chauffeurs, Warehousemen and Helpers of America" verfügt über etwa 1,4 Mio. beitragszahlende Mitglieder und 400.000 Rentner (Stand 2004) und gehört damit auch zu den größten Einzelgewerkschaften weltweit.

Anfänglich waren die "Teamsters" nur eine Gewerkschaft für Lastkraftwagenfahrer, expandierten aber zur allgemeinen Transportarbeitergewerkschaft und reichen heute bis hinein in die Lebensmittelbranche. Sie sind somit auch die zuständige Gewerkschaft beim Logistikriesen UPS. Die Gewerkschaft gehört heute zur Gewerkschaftsgruppe "Change to win", nachdem sie mit einigen anderen Gewerkschaften 2005 den ehemaligen Dachverband AFL-CIO verließ.

Die "Writers Guild of America" (WGA) ist die gemeinsame Gewerkschaft der Autoren in der Film- und Fernsehindustrie der USA. Sie teilt sich in einen westlichen und östlichen Bereich. Ab 2003 zählte sie landesweit über 11.000 Mitglieder. Die Gewerkschaft sorgt auch für die Gesundheits- und Pensionsleistungen der Mitglieder. Ebenso kontrolliert sie die Einhaltung des Urheberrechtes.

Die "National Hockey League Players’ Association" (NHLPA) ist eine Gewerkschaft der Profi-Eishockey-Spieler der NHL in Nordamerika. Der Hauptsitz der Spielervereinigung NHLPA ist Toronto. Sie wurde im Juni 1967 von den Spielern aus den Eishockeyclubs der "Original Six" gegründet. Das "Collective Bargaining Agreement" (CBA; Gesamtarbeitsvertrag) stellt den Tarifvertrag zwischen der NHL (Vereine) und der Spielergewerkschaft NHLPA dar.






</doc>
<doc id="1874" url="https://de.wikipedia.org/wiki?curid=1874" title="Gernika">
Gernika

Gernika [] (offiziell baskisch "Gernika-Lumo", kastilisch "Guernica y Luno") ist eine Stadt in der spanischen autonomen Region Baskenland (baskisch Euskadi) mit Einwohnern (Stand: ).

Am 26. April 1937, während des spanischen Bürgerkriegs, flog die Legion Condor den Luftangriff auf Gernika. Bei den darauf folgenden Großfeuern kamen mehrere hundert Menschen, nahezu ausschließlich Zivilisten, ums Leben, die Stadt wurde großflächig zerstört. Laut Welt-Redakteur Berthold Seewald gehört der Name Gernika zu den "Sinnbildern des modernen Krieges und der totalitären Ideologien, die ihn befördern." 

Gernika liegt nordöstlich von Bilbao (Bilbo) und gehört zur Provinz Bizkaia. Das Municipio "Gernika-Lumo" besteht aus der Stadt "Gernika" und dem 1,5 km entfernten Vorort "San Pedro de Luno." Sie liegt am Fluss Oka.

Gernika wurde am 28. April 1366 gegründet und wird seither von Basken als heilige Stadt verehrt, da sie das Symbol des Kampfes um Unabhängigkeit ist. Seit dem Mittelalter fanden hier Ratsversammlungen statt. Die umliegenden Könige von Navarra und Kastilien schworen unter der Eiche von Gernika, die besonderen Autonomiegesetze der Bizkaia, die Fueros, zu achten und zu wahren.

In Gernika befindet sich oberhalb der Stadtmitte das heilige Nationalsymbol der Basken, eine Eiche (Gernikako Arbola), unter der bis 1876 die Ältestenräte aus dem ganzen Baskenland jährlich zusammenkamen, um eine Form von direkter Demokratie auszuüben. Der Baum wird jeweils, wenn er abstirbt, durch eine Neupflanzung aus den Früchten des alten Baumes ersetzt.

Im spanischen Bürgerkrieg wurde die Stadt beim Luftangriff auf Guernica am 26. April 1937 durch Flugzeuge der deutschen Legion Condor und der italienischen Corpo Truppe Volontarie, die auf Seiten Francos kämpften, bombardiert und kurze Zeit später durch nationalistische Truppen eingenommen. Pablo Picasso verarbeitete die Schrecken dieses Angriffs in seinem Bild "Guernica".

1991 wurde ein "Park der Europäischen Völker" mit mehreren Skulpturen eröffnet. "Gure aitaren etxea" (Das Haus unseres Vaters) von Eduardo Chillida und "Large figure in a shelter" (Große Figur in einer Schutzhütte) von Henry Moore sind die prominentesten.


Gernika ist seit 1989 Partnerstadt der baden-württembergischen Stadt Pforzheim. Bei der Städtepartnerschaft spielt die Tatsache eine Rolle, dass Pforzheim am 23. Februar 1945 Ziel eines Flächenbombardements der Royal Air Force und nach Dresden die am stärksten zerstörte Stadt des damaligen Deutschlands war.
Gernika verfügt über ein "Museum des Friedens", das an das 1987 gegründete Friedensforschungszentrum "Gernika Gogoratuz" (baskisch für „Gernika erinnern“) angeschlossen ist.
Die UNESCO hat Gernika als internationale Friedensstadt ausgezeichnet.



</doc>
<doc id="1877" url="https://de.wikipedia.org/wiki?curid=1877" title="Galba">
Galba

Lucius Livius Ocella Servius Sulpicius Galba (* 24. Dezember 3 v. Chr. bei Tarracina; † 15. Januar 69 n. Chr. in Rom) war vom 8. Juni 68 bis 15. Januar 69 n. Chr. römischer Kaiser. Im Jahr seines Todes wurden drei weitere Kaiser ernannt, weswegen es als Vierkaiserjahr bezeichnet wird. Galba war der erste Kaiser, der nicht aus dem julisch-claudischen Kaiserhaus stammte.

Galba wurde am 24. Dezember 3 v. Chr. in einer Villa bei Tarracina geboren. Er stammte aus einer alten senatorischen Familie und galt charakterlich als untadelig. Sein Vater war Gaius Sulpicius Galba (Suffektkonsul 5 v. Chr.), seine Mutter Mummia Achaica. Über seine Mutter hatte er verwandtschaftliche Verbindungen zu Quintus Lutatius Catulus (Konsul 78 v. Chr.), einem großen Politiker der späten Republik, der Galba ein Vorbild war. Nach deren Tod adoptierte ihn Livia Ocellina, die zweite Frau seines Vaters, wodurch er deren großen Besitz bei Tarracina erbte. Fortan nannte er sich Lucius Livius Ocella Servius Sulpicius Galba, wodurch eine nähere Verbindung zu Livia entstand, der Frau des Augustus und Mutter des Tiberius, die ihn in seiner Karriere förderte. Zudem vermachte Livia Ocellina ihm eine beachtliche Menge an Reichtum. Galba hatte einen älteren Bruder, Gaius, der 22 n.Chr. das Amt des Konsuls erreichte, jedoch sein Vermögen verschleuderte und sich aus Rom zurückzog. Nachdem er bei Tiberius in Ungnade gefallen war, beging Gaius 36 n. Chr. Selbstmord. Galba heiratete um 20 n. Chr. Aemilia Lepida, mit der er zwei Söhne hatte. Sowohl seine Frau als auch die beiden Kinder verstarben jedoch, woraufhin Galba als Witwer lebte.

Galba begann die senatorische Ämterlaufbahn "(cursus honorum)" unter der Herrschaft des Tiberius. Als Prätor ließ er bei den "ludi florales", den Spielen zu Ehren der Göttin Flora, seiltanzende Elefanten auftreten. Noch vor seinem ersten ordentlichen "Consulat" 33 n. Chr. war Galba für ein Jahr praetorischer Statthalter von Aquitanien. Nachdem im Sommer 39 n. Chr. eine Verschwörung gegen Caligula aufgedeckt wurde, bei der Gnaeus Cornelius Lentulus Gaetulicus (Konsul 26), der Kommandant des obergermanischen Heeres, eine tragende Rolle spielte, wurde Galba dessen Nachfolger. Mit harten Maßnahmen stellte er die Disziplin des Heeres wieder her. Etwa um diese Zeit, vielleicht noch unter seinem Vorgänger, war ein Einfall der Chatten über den Rhein erfolgt, weshalb Galba 39/40 n. Chr. einen Vorstoß in deren Gebiet unternahm. Dieser Feldzug wurde im Frühjahr oder Sommer 40 n.Chr. unter dem Oberbefehl Caligulas auf rechtsrheinischem Gebiet fortgesetzt. Als jedoch Caligula im Spätsommer 40 n. Chr. nach Rom abreiste, kam es zu einem erneuten Vorstoß der Chatten. Dieser wurde im folgenden Jahr, unter der Herrschaft des Claudius, durch Galba zurückgeschlagen, wobei er auch in das Gebiet der Chatten eindrang. Die Siege Galbas sorgten dafür, dass in den folgenden zehn Jahren die Chatten den Frieden hielten. 43 n. Chr. begleitete er Claudius, der ihn sehr schätzte, als comes auf dessen Britannienreise und war von 44–46 Prokonsul von Africa. Dort sollte Galba die Ordnung in der Provinz wiederherstellen, die sowohl durch innere Streitigkeiten als auch rebellierende Stämme bedroht war. Hier besiegte er 45 n. Chr. die Musulamier. Auch sein Beharren auf Disziplin und sein Gerechtigkeitssinn, gerade auch in unbedeutenden Angelegenheiten, halfen ihm bei der Ausführung seiner Aufgabe. Für seine Dienste in Africa und Germanien bekam Galba die Auszeichnungen eines Triumphators und drei Priesterämter verliehen. In der folgenden Zeit zog sich Galba ins Privatleben zurück, wobei wohl die Missgunst Agrippinas, der Schwester des Caligula und Mutter Neros, die Claudius 49 n. Chr. geheiratet hatte, eine Rolle spielte. Nach deren Ermordung im März 59 n. Chr. wurde Galba 59/60 n. Chr. wieder rehabilitiert, da er immer noch als tüchtig und loyal galt. Von Nero wurde Galba 60 n. Chr. als Statthalter in die Provinz Hispania Tarraconensis gesandt.

Bereits nach der Ermordung Caligulas durch die Prätorianer am 24. Januar 41 n. Chr. soll Galba ein erstes Mal von seinen Freunden dazu gedrängt worden sein, nach der Macht zu greifen, was er jedoch ablehnte. Im Winter 67/68 n. Chr. begann nun unter der Initiative von Gaius Iulius Vindex, dem Statthalter der unbewaffneten Provinz Gallia Lugdunensis, eine Aufstandsbewegung gegen Nero. Galba verhielt sich auf dessen Briefe mit der Bitte um Unterstützung zuerst neutral, da er sich weder anschloss, noch die Nachricht von der Rebellion nach Rom übersandte, wie dies andere Kommandeure taten. Dies scheint daran gelegen zu haben, dass Galba den Briefen kein Vertrauen schenkte. Erst Anfang April 68 n. Chr. schloss sich Galba der Aufstandsbewegung an und wurde am 3. April in Carthago Nova durch Soldaten und Provinziale zum Kaiser ausgerufen. Er nannte sich jedoch zunächst "legatus Senatus Populique Romani", da die letztendliche Entscheidung über den Nachfolger Neros dem Senat überlassen werden sollte. Galba sandte zudem Briefe an die anderen Provinzstatthalter, in denen er um ihre Unterstützung bat. Diesen Bitten kamen Aulus Caecina Alienus, der Quaestor von Baetica, und Marcus Salvius Otho, der Statthalter von Lusitania, nach, die sich ihm anschlossen. Gerade Othos finanzielle Unterstützung brauchte Galba, um seine Truppen zu bezahlen. Zu Beginn verfügte er nur über eine Legion, die "Legio VI Victrix", drei Auxiliarkohorten und zwei Reiterschwadronen. Galba ordnete deshalb Aushebungen an und stellte somit eine zweite Legion, die "Legio VII Galbiana", und zwei weitere Auxiliarkohorten aus dem Gebiet der Vasconen auf, wodurch sich seine Streitkraft fast verdoppelte. Unterdessen hatte Vindex die Belagerung von Lugdunum aufgenommen, der Hauptstadt der Provinz Gallia Lugdunensis. Als Lucius Verginius Rufus, der Statthalter in Obergermanien, davon erfuhr, zog er los, um den Aufstand niederzuschlagen. Hierzu führte er seine zwei Legionen aus Mogontiacum, die "Legio IV Macedonica" und die "Legio XXII Primigenia", sowie Teile weiterer vier Legionen aus Niedergermanien inklusive Auxiliartruppen ins Feld. Vindex soll zu diesem Zeitpunkt etwa 100.000 Mann unter Waffen gehabt haben, die jedoch hauptsächlich schlecht ausgerüstet waren. Als er davon erfuhr, dass Rufus Vesontio belagerte, das ihn nicht mit offenen Toren empfangen hatte, eilte Vindex der Stadt zur Hilfe und schlug sein Lager in der Nähe auf. Die beiden Befehlshaber schickten untereinander Nachrichten und trafen sich letztendlich sogar persönlich, wobei sie sich gegen Nero aussprachen. Dennoch kam es danach unter ungeklärten Umständen zur Schlacht zwischen den beiden Heeren, die wohl durch die Initiative der Soldaten zustande kam und bei der Vindex, nach dem Verlust von 20.000 Soldaten, Selbstmord beging. Als die Legionäre Rufus nun zum Kaiser ausrufen wollten, lehnte dieser jedoch ab, weshalb seine Truppen weiterhin zu Nero standen. Nachdem Galba die Nachricht von der Niederlage des Vindex bei Vesontio erhielt, schrieb er sofort an Rufus und bot eine Kooperation an, bekam jedoch keine Antwort und zog sich daraufhin nach Clunia zurück. Im Juni 68 n. Chr. änderte sich die Lage für Galba jedoch, da der Prätorianerpräfekt Gaius Nymphidius Sabinus die Prätorianer mit dem Versprechen eines gewaltigen Geldgeschenkes von 7.500 "Denarii" (30.000 Sesterzen) zum Abfall von Nero bewegte. Daraufhin bestätigte der Senat am 8. Juni 68 n. Chr. Galba als neuen Kaiser und erklärte Nero zum Staatsfeind, woraufhin dieser am 9. Juni Selbstmord beging.

Die Nachricht von seiner Bestätigung als Kaiser erreichte Galba wohl am 16. oder 18. Juni, als dessen Freigelassener Icelus in Clunia eintraf. Zwei Tage später folgte eine Nachricht mit dem exakten Text des Senatsbeschlusses, die der Senator Titus Vinius überbrachte. Galba übernahm nun den Namen Servius Galba Imperator Caesar Augustus und die "tribunicia potestas". Vor seinem Aufbruch nach Italien regelte er jedoch zuerst die Verhältnisse in Hispania und übertrug die Kontrolle über die drei Provinzen Tarraconensis, Lusitania und Baetica Cluvius Rufus. Vermutlich am 16. Juli brach Galba von Tarraco nach Italien auf, wobei er die "Legio VII Galbiana" mit sich führte. Auf seinem Weg nach Rom bestrafte Galba die Städte Spaniens und Galliens, die gezögert hatten, sich ihm anzuschließen, durch höhere Steuerabgaben sowie die Schleifung der Stadtmauern in einigen Fällen. Bei der Stadt Narbo traf er eine Abordnung des Senats, die ihn bat die Reise zu beschleunigen. Dieses Treffen lässt sich etwa auf den 5. August datieren. Galba setzte seinen Weg in Gallien fort und setzte Iunius Blaesus als Statthalter der Provinz Lugdunensis ein, in deren Hauptstadt er die "Legio I Italica" und die "ala Tauriana" stationierte. Die Stämme der Haeduer, Arverner und Sequaner, die Vindex unterstützt hatten, wurden mit zusätzlichen Gebiet und der Befreiung von Tributen belohnt, während die Treverer und Lingonen, die mit den Rheinlegionen kooperiert hatten, bestraft wurden. Dieses Verhalten der Belohnung von Unterstützern und Bestrafung von Zauderern oder Widersachern wandte Galba auch in weiteren Fällen an. Den Statthalter der Provinz Aquitania, Betuus Cilo, ließ Galba hinrichten, wohl weil dieser um Unterstützung angefragt hatte, als der Aufstand des Vindex ausgebrochen war. Auch in Germanien griff Galba ein und ließ Verginius Rufus durch Marcus Hordeonius Flaccus ablösen. Rufus schloss sich daraufhin dem Zug des Kaisers nach Rom an. Aulus Caecina Alienus, der Galba schon früh unterstützt hatte, wurde mit einem Legionskommando in Obergermanien betraut. In Niedergermanien war der Befehlshaber Fonteius Capito unter unklaren Umständen umgekommen. Er wurde durch Aulus Vitellius ersetzt. In der Provinz Belgica wurde Pompeius Propinquuus als Prokurator eingesetzt. In Mauretania wurde Lucceius Albinus, dem prokuratorischen Statthalter von Mauretania Caesariensis, durch Galba ebenfalls die Verwaltung von Mauretania Tingitana übertragen. In Pannonia und Dalmatia wurde Cornelius Fuscus zum Prokurator ernannt, wohl um die beiden Statthalter der Provinzen zu überwachen. Ebenso wurde die Legio VII Galbiana im November 68 unter dem Befehl des Marcus Antonius Primus nach Carnuntum in Pannonia verlegt. Galatia und Pamphylia wurden durch den neu eingesetzten Calpurnius Asprenas verwaltet. Nach dem Tode Lucius Clodius Macers in der Provinz Africa hatte Gaius Vipstanus Apronianus das Amt des Prokonsuls auf Galbas Geheiß inne. Auch in Rom selbst griff der Kaiser zu Gunsten seiner Anhänger ein und ernannte Cornelius Laco zum Prätorianerpräfekten. Ebenso erhielt Aulus Ducenius Geminus das Amt des "praefectus urbi" und Plotius Firmus wurde zum "praefectus vigilum" ernannt.

Bei der Behandlung der Soldaten zeigte Galba kein besonderes Geschick. Die Legionen Germaniens erhielten von ihm keine Anerkennung für ihren Einsatz zur Aufstandsniederschlagung des Vindex und fürchteten eine Bestrafung, da sie in dieser Angelegenheit auf der scheinbar falschen Seite gekämpft hatten. Weitere Feindschaft erregten die Absetzung des Verginius Rufus und der mysteriöse Tod des Fonteius Capito. Weiteren Hass unter den Soldaten erzeugte Galbas Vorgehen gegen die unter Nero ausgehobene "Legio I Adiutrix" aus Marinesoldaten, die vor Rom ihr Lager aufgeschlagen hatte. Diese wollten von Galba eine Bestätigung ihres Status erhalten und versammelten sich dazu an der Milvischen Brücke. Als ihre Forderungen jedoch zurückgewiesen worden, versuchten die Soldaten durch einen Aufruhr der Sache Nachdruck zu verleihen. Galba ließ daraufhin seine Infanterie und Kavallerie gegen die Aufständischen vorgehen, wobei viele getötet wurden. Die Überlebenden wurden dezimiert. Die darauf folgende Erfüllung der Forderungen und offizielle Anerkennung als "Legio I Adiutrix" änderte wenig an dem Schaden, den Galbas Ansehen bei den Soldaten durch diese Tat erhalten hatte. Auch sein Verhalten gegenüber den Prätorianern nahm ihm den Rückhalt der Soldaten, da er gar nicht daran dachte, das von Gaius Nymphidius Sabinus versprochene Donativ zu zahlen. Dies wäre umso ratsamer gewesen, da durch die Prätorianer unter Führung des Militärtribunen Antonius Honoratus der Versuch des Sabinus, die Herrschaft zu übernehmen, niedergeschlagen und dieser daraufhin getötet worden war.

Ein politisches Ziel Galbas war die Sanierung des Staatshaushaltes, der unter den immensen Ausgaben Neros gelitten hatte. 2,2 Milliarden Sesterzen, die unter Nero verschenkt worden waren, sollten wieder eingetrieben werden, nur ein Zehntel der Schenkung sollte den betroffenen Personen überlassen werden. Für die Eintreibung der Summe wurden 30 Ritter eingesetzt. Da das Geld größtenteils schon in Güter umgesetzt worden war, wurden diese zwangsversteigert. Der größte Erfolg bei diesem Vorhaben scheint die Eintreibung von 40.000 Sesterzen gewesen zu sein, die Nero dem Orakel von Delphi geschenkt hatte.

Durch eine Inschrift vom Herbst 68 ist ebenso bekannt, dass Galba Reparaturen an der "Horrea Sulpicia", dem größten Getreidespeicher Roms vornehmen ließ. Eine Wiederherstellung der "frumentationes", der kostenlosen Getreideabgaben an die männlichen Bewohner Roms, nahm er jedoch nicht vor, wohl aus finanziellen Gründen.

Als größtes Problem Galbas erwies sich jedoch die Abhängigkeit von seinen Beratern Titus Vinius, Cornelius Laco und Icelus, durch die er sich isolierte. All diese Männer hatten laut den antiken Quellen charakterliche Mängel und verfolgten eigene Interessen. Vinius’ Karriere war von Skandalen geprägt und er häufte sich ein Vermögen an. Laco hatte keine militärische oder administrative Erfahrung und soll arrogant und faul gewesen sein. Icelus, der erst kurz zuvor in den Ritterstand erhobene Freigelassene Galbas, raffte sich ebenso wie Vinius ein Vermögen zusammen.

Zur Krise für Galbas Herrschaft kam es am 1. Januar 69 n. Chr., als die Legionen in Obergermanien sich weigerten, ihren Eid auf den Kaiser zu erneuern und dessen Standbilder umstießen. Die Nachricht von diesem Ereignis erreichte Galba wenige Tage später durch einen Brief des Prokurators der Provinz Belgica, Pompeius Propinquus.

Die Krise in Germanien beschleunigte nun die Wahl eines Nachfolgers. Da seine beiden Söhne sowie seine Frau vor ihm gestorben waren, musste Galba nach einem anderen potentiellen Kandidaten Ausschau halten. Hierfür kamen mehrere Männer in Frage. Eine Option war Marcus Salvius Otho, der Galba schon seit Spanien und dem Aufstand des Vindex als einer der frühesten Gefolgsleute begleitete. Als Alternative wurde Gnaeus Cornelius Dolabella gesehen, ein entfernter Verwandter und vermutlich dessen Großneffe. Nachdem nun am 9. Januar der Brief des Prokurators der Provinz Belgica, Pompeius Propinquus, eintraf, der Galba über die Krise bei den germanischen Legionen informierte, sah sich der Kaiser gezwungen eine Entscheidung in der Nachfolgerwahl zu treffen. Deshalb rief er am 10. Januar seine engsten Berater zu einem Treffen und erklärte, dass er Lucius Calpurnius Piso Frugi Licinianus auserkoren habe ihm nachzufolgen. Die Adoption Pisos wurde zuerst den Prätorianern bekannt gemacht, jedoch versäumte es der Kaiser auch bei dieser Gelegenheit die Soldaten an sich oder seinen Nachfolger zu binden. Auch in dieser Situation versprach er kein Geldgeschenk, weder das von Nymphidius Sabinus versprochene Geld noch eine Summe zur Feier der Ereignisse wollte Galba zahlen. Danach suchte man den Senat auf, um diesem die Adoption Pisos zu verkünden. Viele Senatoren nahmen die Nachricht mit Freude auf oder täuschten dies vor, wenn sie Piso nicht wohlgesinnt waren, die neutrale Mehrheit duldete die Entscheidung.

Die Entscheidung Galbas musste Otho brüskieren, da dieser sich selbst Hoffnungen auf die Nachfolge gemacht hatte. Er plante nun eine Verschwörung gegen Galba, um sich den Kaisertitel zu sichern. Hilfreich war hierbei, dass Otho auch schon früher sich um die Soldaten bemüht und diesen Geldgeschenke oder Gunstbeweise zukommen lassen hatte. Er soll unter anderem je 100 Sesterze an jeden Mann der wachhabenden Kohorte verteilt haben, wenn Galba bei ihm speiste. Otho überließ nun die Planung des Putsches seinem Freigelassenen Onomastus, der den Ordonnanzoffizier Barbius Proculus, der für die Parole der kaiserlichen Leibgarde zuständig war, und den Optio Veturius mit Geld und Versprechungen bestach, damit diese weitere Soldaten für den Putsch gewinnen. Es wurde nun zuerst der Plan gefasst, Otho in der Nacht des 14. Januar zu ergreifen und zum Prätorianerlager zu bringen, jedoch dann verworfen, da er zu viele Risiken barg. Die Ausführung des Putsches wurde auf den Morgen des 15. Januar verschoben.

An diesem Tag opferte Galba morgens vor dem Tempel des Apoll. Zuerst war auch Otho bei der Zeremonie anwesend, entschuldigte sich jedoch später unter dem Vorwand eines Hauskaufes, nachdem Onomastus ihm die Nachricht überbracht hatte, dass man bereit zum Putsch sei, und eilte zum "Miliarium Aureum". Dort traf er jedoch nur 23 Leibgardisten an, die ihn als Kaiser anriefen, weshalb man sich schleunigst auf den Weg zum Prätorianerlager machte, wobei sich die Zahl der Putschisten etwa verdoppelte. Als Galba über die Ereignisse informiert wurde, befahl er zuerst die Loyalität der Palastwache zu überprüfen, weshalb Piso dorthin geschickt wurde. Unterdessen wurden auch Gesandte zu den anderen in Rom verbliebenen Truppen gesandt, jedoch ohne großen Erfolg. Die Prätorianer bedrohten zwei gesandte Tribunen, Cetrius Severus und Subrius Dexter, den dritten, Pompeius Longinus, setzten sie fest. Die Legion der Marinesoldaten, die von Galba bestraft worden waren, hatte sich auf die Seite der meuternden Prätorianer geschlagen. Die illyrischen Truppenteile vertrieben den Gesandten Celsus Marius und die germanischen Truppen zögerten, obwohl sie Galba wohlgesinnt waren. Der Kaiser musste sich nun entscheiden, ob er sich im Palast verschanzen oder den Putschisten aktiv entgegentreten wolle, bevor sie sich organisierten. Galba entschied sich dafür, den Putsch zu zerschlagen und machte sich in Begleitung einer ihm wohlgesinnten Menschenmenge, die sich vor dem Palast versammelt hatte, auf zum Forum. Unterdessen hatte Otho die Soldaten im Prätorianerlager für sich gewonnen und bewaffnen lassen. Da die Nachricht umging, dass Galba den Pöbel bewaffnen lasse, ließ er die Soldaten überstürzt aufbrechen, die daraufhin auf das Forum stürmten und die Menge auseinandertrieben. Als diese in Sichtweite kamen, desertierten auch die verbliebenen Soldaten der Palastwache, lediglich der Prätorianerzenturio Sempronius Densus verteidigte Galba, starb jedoch dabei. Der Kaiser, der im Chaos aus seiner Sänfte geschleudert worden war, wurde in der Nähe des Lacus Curtius getötet. Sein verstümmelter Leichnam wurde von Gaius Helvidius Priscus mit Erlaubnis Othos geborgen und vom Rechnungsführer Argius in seinen Privatgärten an der Via Aurelia bestattet. Über ihn wurde zuerst die Damnatio memoriae verhängt, jedoch wurde am 1. Januar 70 n.Chr. Galbas Ansehen durch Vespasian wiederhergestellt.

Die wichtigsten antiken Quellen für Leben und Herrschaft Galbas sind Suetons Kaiserviten, die Historien des Tacitus (1,1–49), die Galba-Biografie des Plutarch und das Geschichtswerk Cassius Dios (63,22–64,7). Vermutlich gab es auch von Cluvius Rufus, Fabius Rusticus and Plinius dem Älteren Werke über Galba, diese sind jedoch nicht erhalten.





</doc>
<doc id="1879" url="https://de.wikipedia.org/wiki?curid=1879" title="Gen">
Gen

Als Gen wird meist ein Abschnitt auf der DNA bezeichnet, der Grundinformationen für die Entwicklung von Eigenschaften eines Individuums und zur Herstellung einer biologisch aktiven RNA enthält. Bei diesem Prozess der Transkription wird eine komplementäre Kopie des codogenen Strangabschnitts in Form einer RNA hergestellt.

Es gibt verschiedene Arten der RNA. Bei der Translation, einem Teilvorgang der Proteinbiosynthese, wird die Aminosäuresequenz der Proteine von der mRNA abgelesen. Die Proteine übernehmen im Körper jeweils spezifische Funktionen, mit denen sich Merkmale ausprägen können. Der Aktivitätszustand eines Gens bzw. dessen Ausprägung, seine Expression, kann in einzelnen Zellen verschieden reguliert werden.

Allgemein werden die nur elektronenmikroskopisch sichtbaren Gene auch als Erbanlage oder in den Chromosomen, auf spezifischen Plätzen lokalisierte Erbfaktoren bezeichnet, da sie die Träger von Erbinformation sind, die durch Reproduktion an Nachkommen weitergegeben wird. Die Erforschung des Aufbaus, der Funktion und Vererbung von Genen ist Gegenstand der Genetik. Die gesamte Erbinformation einer Zelle wird Genom genannt.

1854 begann Johann Gregor Mendel, die Vererbung von Merkmalen bei Erbsen zu untersuchen. Er schlug als erster die Existenz von Faktoren vor, die von Eltern auf die Nachkommen übertragen werden. Bei seinen Kreuzungsversuchen beschrieb er, dass Merkmale voneinander unabhängig vererbt werden können, sowie dominante und rezessive Merkmale. Er entwickelte die Hypothese, dass es homo- und heterozygote Zustände geben kann, und legte damit die Grundlage für die Unterscheidung zwischen Genotyp und Phänotyp.

1900 gilt als das Jahr der „Wiederentdeckung“ der mendelschen Regeln, da die Botaniker Hugo de Vries, Erich Tschermak und Carl Correns aufgriffen, dass es quantifizierbare Regeln gibt, nach denen die Faktoren, die für die Ausprägung von Merkmalen verantwortlich waren, an die Nachkommen weitergegeben werden. Correns prägte dabei den Begriff "Anlage" bzw. "Erbanlage". William Bateson erinnerte 1902 in daran, dass es zwei Varianten der Erbfaktoren in jeder Zelle gibt. Er nannte das zweite Element "Allelomorph" nach dem griechischen Wort für ‚Andere‘ und prägte damit den Begriff des Allels. Archibald Garrod, ein britischer Arzt, hatte sich mit Stoffwechselerkrankungen beschäftigt und stellte fest, dass diese in Familien vererbt wurden. Garrod erkannte, dass die Gesetze also auch bei Menschen gültig waren, und vermutete, die Erbanlagen seien die Basis für die "Chemische Individualität" von Menschen. Dass die von Mendel „Elemente“ genannten Faktoren auf den Chromosomen zu finden sind, wurde 1902 von Walter Sutton vermutet.

August Weismann stellte in seinen "Vorträgen zur Deszendenztheorie" 1904 die Entdeckung vor, dass es einen Unterschied zwischen Körperzellen und Keimzellen gibt, und dass nur letztere in der Lage sind, neue Organismen hervorzubringen. Keimzellen sollten eine „Vererbungssubstanz“ enthalten, die sich aus einzelnen Elementen zusammensetzten, die er "Determinanten" nannte. Diese Determinanten sollten für die sichtbare Ausprägung beispielsweise der Gliedmaßen verantwortlich sein.

Die Bezeichnung „Gen“ wurde erst 1909 von dem Dänen Wilhelm Johannsen geprägt. Er benannte die Objekte, mit denen sich die Vererbungslehre beschäftigt, nach dem griechischen Substantiv für ‚Nachkommenschaft‘. Für ihn waren sie jedoch nur eine Rechnungseinheit. Bereits drei Jahre zuvor hatte William Bateson die Wissenschaft von der Vererbung als "Genetik" bezeichnet, nach dem griechischen Adjektiv . Zu diesem Zeitpunkt war die chemische Natur der Gene immer noch vollkommen unklar.

In den ersten Jahren des 20. Jahrhunderts nahmen sich die Genetiker nach verschiedenen Pflanzen auch Insekten und später Vögel vor, um die Vererbungsgesetze zu testen. In Kombination mit den 1842 entdeckten und 1888 benannten Chromosomen entstand so die Chromosomentheorie der Vererbung. Es war durch verbesserte Färbetechniken beobachtet worden, dass sich Chromosomen erst verdoppeln und sich dann mit den Zellen teilen. Daher waren sie als Träger der Erbanlagen in Frage gekommen. Während dieser Zeit herrschte eine Kontroverse zwischen den Vertretern der Hypothese von Johannsen und Mendel, dass Gene etwas Materielles sind, und deren Kritikern, die eine Verbindung von Genen und Chromosomen als „Physikalismus“ und „Mendelismus“ abtaten und Gene weiterhin als abstrakte Einheiten betrachteten.

Thomas Hunt Morgan war ebenfalls überzeugt, dass es nicht physikalische Einheiten sein konnten, die für die verschiedenen Merkmale verantwortlich waren, und versuchte, den Mendelismus zu widerlegen. Er begann 1910 mit Kreuzungsversuchen an Schwarzbäuchigen Taufliegen. Seine Arbeiten erbrachten jedoch das Gegenteil: Den endgültigen Beweis, dass Gene auf Chromosomen liegen und damit materiellen Ursprungs sind. Zusammen mit seinen Mitarbeitern, darunter Calvin Bridges, Alfred Sturtevant und Hermann Muller, fand er viele natürliche Mutationen und untersuchte in unzähligen Kreuzungen die Wahrscheinlichkeit, dass zwei Merkmale gemeinsam vererbt werden. Sie konnten so zeigen, dass Gene an bestimmten Stellen auf den Chromosomen liegen und hintereinander aufgereiht sind. Gemeinsam erstellte die Gruppe in jahrelanger Arbeit die erste Genkarte. Da unter dem Mikroskop auch das Crossing over beobachtet werden konnte, war bekannt, dass Chromosomen Abschnitte austauschen können. Je näher zwei Gene auf dem Chromosom beieinander liegen, desto größer die Wahrscheinlichkeit, dass sie gemeinsam vererbt und nicht durch ein Crossing-over-Ereignis getrennt werden. Dadurch konnten Angaben über die Entfernung zweier Gene gemacht werden, die nach Morgan in "centiMorgan" angegeben werden.

Hermann Muller begann einige Zeit später, mit Röntgenstrahlen zu experimentieren, und konnte zeigen, dass die Bestrahlung von Fliegen deren Mutationsrate stark erhöht. Diese Erkenntnis aus dem Jahr 1927 war eine Sensation, da dadurch zum ersten Mal tatsächlich gezeigt wurde, dass Gene physikalische Objekte sind, die sich von außerhalb beeinflussen lassen.

1928 wies Frederick Griffith in dem als „Griffiths Experiment“ bekannt gewordenen Versuch zum ersten Mal nach, dass Gene von Organismen auf andere übertragen werden können. Der von ihm nachgewiesene Vorgang war die Transformation.
1941 zeigten George Wells Beadle und Edward Lawrie Tatum, dass Mutationen in Genen für Defekte in Stoffwechselwegen verantwortlich sind, was zeigte, dass spezifische Gene spezifische Proteine codieren. Diese Erkenntnisse führten zur „Ein-Gen-ein-Enzym-Hypothese“, die später zur „Ein-Gen-ein-Polypeptid-Hypothese“ präzisiert wurde. Oswald Avery, Colin MacLeod und Maclyn McCarty zeigten 1944, dass die DNA die genetische Information enthält. 1953 wurde die Struktur der DNA von James D. Watson and Francis Crick, basierend auf den Arbeiten von Rosalind Franklin und Erwin Chargaff, entschlüsselt und das Modell der DNA-Doppelhelix entworfen.
1969 gelang Jonathan Beckwith als erstem die Isolierung eines einzelnen Gens.

Die Definition, was ein Gen genau ist, hat sich ständig verändert und wurde an neue Erkenntnisse angepasst. Für den Versuch einer aktuellen Definition benötigten 25 Wissenschaftler des der Universität Berkeley Anfang 2006 zwei Tage, bis sie eine Version erreichten, mit der alle leben konnten. Ein Gen ist demnach .

Und auch diese Definition ist nicht endgültig. Durch das ENCODE ()-Projekt, bei dem die Transkriptionsaktivität des Genoms gemappt wurde, wurden neue komplexe Regulationsmuster gefunden. Dabei wurde festgestellt, dass die Transkription nichtcodierender RNA viel verbreiteter ist als bislang angenommen. Die Definition lautet daher: .

Auf molekularer Ebene besteht ein Gen aus zwei unterschiedlichen Bereichen:


Es gibt verschiedene Besonderheiten im Aufbau von Genen verschiedener Lebewesen. In der Zeichnung wird der Aufbau eines typischen eukaryotischen Gens dargestellt, das ein Protein codiert.

Vor der Transkriptionseinheit oder auch innerhalb der Exons (hellblau und dunkelblau) und Introns(rosa und rot) liegen regulatorische Elemente, wie zum Beispiel Enhancer oder Promotor. An diese binden, abhängig von der Sequenz, verschiedene Proteine, wie beispielsweise die Transkriptionsfaktoren und die RNA-Polymerase. Die prä-mRNA (unreife mRNA), die im Zellkern bei der Transkription zunächst entsteht, wird in dem Reifungsprozess zur reifen mRNA modifiziert. Die mRNA enthält neben dem direkt proteincodierenden Offenen Leserahmen noch untranslatierte, also nichtcodierende Bereiche, den 5' untranslatierten Bereich (5' UTR) und den 3' untranslatierten Bereich (3' UTR). Diese Bereiche dienen zur Regulation der Translationsinitiation und zur Regulation der Aktivität der Ribonukleasen, die die RNA wieder abbauen.

Die Gene der Prokaryoten unterscheiden sich im Aufbau von eukaryotischen Genen dadurch, dass sie keine Introns besitzen. Zudem können mehrere unterschiedliche RNA-bildende Genabschnitte sehr nah hintereinander geschaltet sein (man spricht dann von polycistronischen Genen) und in ihrer Aktivität von einem gemeinsamen regulatorischen Element geregelt werden. Diese Gencluster werden gemeinsam transkribiert, aber in verschiedene Proteine translatiert. Diese Einheit aus Regulationselement und polycistronischen Genen nennt man Operon. Operons sind typisch für Prokaryoten.

Gene codieren nicht nur die mRNA, aus der dann die Proteine translatiert werden, sondern auch die rRNA und die tRNA sowie weitere Ribonukleinsäuren, die andere Aufgaben in der Zelle haben, beispielsweise bei der Proteinbiosynthese oder der Genregulation. Ein Gen, das ein Protein codiert, enthält eine Beschreibung der Aminosäure-Sequenz dieses Proteins. Diese Beschreibung liegt in einer chemischen Sprache vor, nämlich im genetischen Code in Form der Nukleotid-Sequenz der DNA. Die einzelnen „Kettenglieder“ (Nukleotide) der DNA stellen – in Dreiergruppen (Tripletts, Codon) zusammengefasst – die „Buchstaben“ des genetischen Codes dar. Der codierende Bereich, also alle Nukleotide, die direkt an der Beschreibung der Aminosäuresequenz beteiligt sind, wird als offener Leserahmen bezeichnet. Ein Nukleotid besteht aus einem Teil Phosphat, einem Teil Desoxyribose (Zucker) und einer Base. Eine Base ist entweder Adenin, Thymin, Guanin oder Cytosin.

Gene können mutieren, sich also spontan oder durch Einwirkung von außen (beispielsweise durch Radioaktivität) verändern. Diese Veränderungen können an verschiedenen Stellen im Gen erfolgen. Demzufolge kann ein Gen nach einer Reihe von Mutationen in verschiedenen Zustandsformen vorliegen, die man Allele nennt. Eine DNA-Sequenz kann auch mehrere überlappende Gene enthalten. Durch Genduplikation verdoppelte Gene können sequenzidentisch sein, dennoch aber unterschiedlich reguliert werden und damit zu unterschiedlichen Aminosäuresequenzen führen, ohne dass sie Allele sind.

Generell schwankt das Verhältnis zwischen Introns und Exons von Gen zu Gen sehr stark. So gibt es einige Gene ohne Introns, während andere zu über 95 % aus Introns bestehen. Beim Dystrophin-Gen – mit 2,5 Millionen Basenpaaren das größte menschliche Gen  – besteht das daraus codierte Protein aus 3685 Aminosäuren. Der Anteil der codierenden Basenpaare beträgt somit 0,44 %.

In der nachfolgenden Tabelle sind einige Proteine und das jeweils codierende Gen aufgeführt.

Gene sind dann „aktiv“, wenn ihre Information in RNA umgeschrieben wird, das heißt, die Transkription stattfindet. Je nach Funktion des Gens entsteht also mRNA, tRNA oder rRNA. In der Folge kann also, muss aber nicht zwingend, bei mRNA aus dieser Aktivität auch ein Protein translatiert werden. Eine Übersicht über die Vorgänge bieten die Artikel Genexpression und Proteinbiosynthese.

Die Aktivität einzelner Gene wird über eine Vielzahl von Mechanismen gesteuert und kontrolliert. Ein Weg ist die Steuerung über die Rate ihrer Transkription in hnRNA. Ein anderer Weg ist der Abbau der mRNA, bevor sie beispielsweise über siRNA translatiert wird. Kurzfristig erfolgt die Genregulation durch Bindung und Ablösung von Proteinen, sogenannten Transkriptionsfaktoren, an spezifische Bereiche der DNA, die sogenannten „regulatorischen Elemente“. Langfristig wird dies über Methylierung oder das „Verpacken“ von DNA-Abschnitten in Histon­komplexe erreicht. Auch die regulatorischen Elemente der DNA unterliegen der Variation. Der Einfluss von Änderungen in der Genregulation einschließlich der Steuerung des alternativen Splicings dürfte vergleichbar mit dem Einfluss von Mutationen proteincodierender Sequenzen sein. Mit klassischen genetischen Methoden – durch Analyse von Erbgängen und Phänotypen – sind diese Effekte in der Vererbung normalerweise nicht voneinander zu trennen. Lediglich die Molekularbiologie kann hier Hinweise geben. Eine Übersicht über die Regulationsvorgänge von Genen wird im Artikel Genregulation dargestellt.

Bei allen Lebewesen codiert nur ein Teil der DNA für definierte RNAs. Die übrigen Teile der DNA werden als nichtcodierende DNA bezeichnet. Sie hat Funktionen in der Genregulation, beispielsweise für die Regulation des alternativen Splicings, und hat Einfluss auf die Architektur der Chromosomen.

Der Ort auf einem Chromosom, an dem sich das Gen befindet, wird als Genort bezeichnet. Gene sind darüber hinaus nicht gleichmäßig auf den Chromosomen verteilt, sondern kommen zum Teil in sogenannten Clustern vor. Gencluster können dabei aus zufällig in räumlicher Nähe zueinander liegenden Genen bestehen, oder es handelt sich um Gruppen von Genen, die für Proteine codieren, die in einem funktionellen Zusammenhang stehen. Gene, deren Proteine ähnliche Funktion haben, können aber auch auf verschiedenen Chromosomen liegen.

Es gibt Abschnitte auf der DNA, die für mehrere verschiedene Proteine codieren. Der Grund dafür sind überlappende offene Leserahmen.

Als "genetische Variation" wird das Auftreten von genetischen Varianten (Allele, Gene oder Genotypen) bei individuellen Lebewesen bezeichnet.
Sie entsteht durch Mutationen, aber auch durch Vorgänge bei der Meiose („Crossing over“), durch die Erbanlagen der Großeltern unterschiedlich auf die Geschlechtszellen verteilt werden. Für die Entstehung neuer Gene können ebenfalls Mutationen oder De-novo-Entstehung ursächlich sein.

Genetische Variabilität ist dagegen die Fähigkeit einer gesamten Population, Individuen mit unterschiedlichem Erbgut hervorzubringen. Hierbei spielen nicht nur genetische Vorgänge, sondern auch Mechanismen der Partnerwahl eine Rolle.
Die genetische Variabilität spielt eine entscheidende Rolle für die Fähigkeit einer Population, unter veränderten Umweltbedingungen zu überleben, und stellt einen wichtigen Faktor der Evolution dar.

Obwohl bei allen zellbasierten Lebensformen Gene als DNA-Abschnitte vorliegen, gibt es einige Viren, deren genetische Information in Form von RNA vorliegt.
RNA-Viren befallen eine Zelle, die dann sofort mit der Produktion von Proteinen direkt nach Anleitung der RNA beginnt; eine Transkription von DNA nach RNA entfällt.
Retroviren hingegen übersetzen ihre RNA bei der Infektion in DNA, und zwar unter Mitwirkung des Enzyms Reverse Transkriptase.

Als Gen im engeren Sinne bezeichnet man in der Regel eine Nukleotidsequenz, die die Information für ein Protein enthält, das unmittelbar funktionsfähig ist. Pseudogene stellen dagegen Genkopien dar, die kein funktionelles Protein in voller Länge codieren. Oftmals sind diese durch Genduplikationen entstanden und/oder durch Mutationen, die sich in der Folge ohne Selektion auch im Pseudogen akkumulieren (anhäufen), und ihre ursprüngliche Funktion verloren haben. Einige scheinen dennoch eine Rolle bei der Regulierung der Genaktivität zu spielen. Das menschliche Genom enthält etwa 20.000 Pseudogene. Das Humangenomprojekt wurde mit dem Ziel gegründet, das Genom des Menschen vollständig zu entschlüsseln.

Sie werden auch als Transposons bezeichnet und sind mobile Erbgutabschnitte, die sich innerhalb der DNA einer Zelle frei bewegen können. Aus ihrem angestammten Ort im Erbgut schneiden sie sich selbst aus und fügen sich an einer beliebig anderen Stelle wieder ein. Biologen um Fred Gage vom Salk Institute for Biological Studies in La Jolla (USA) haben nachgewiesen, dass diese springenden Gene nicht nur wie bislang angenommen in den Zellen der Keimbahn vorkommen, sondern auch in Nerven-Vorläuferzellen aktiv sind. Forschungsergebnisse von Eric Lander et al. (2007) zeigen, dass Transposons eine wichtige Funktion haben, indem sie als "kreativer Faktor" im Genom wichtige genetische Innovationen rasch im Erbgut verbreiten können.




</doc>
<doc id="1880" url="https://de.wikipedia.org/wiki?curid=1880" title="Gordian III.">
Gordian III.

Marcus Antonius Gordianus (* 20. Januar 225; † 244), auch bekannt als Gordian III., war von 238 bis 244 römischer Kaiser.

Marcus Antonius Gordianus wurde am 20. Januar 225 in Rom geboren. Die Namen seiner Eltern in der spätantiken "Historia Augusta" sind fiktiv. Als sehr wahrscheinlich gilt, dass seine Mutter Antonia Gordiana eine Tochter von Gordian I. und damit Schwester des Gordian II. war. Wohl im Sommer 241 heiratete er Furia Sabinia Tranquillina, Tochter seines späteren Prätorianerpräfekten Gaius Furius Sabinus Aquila Timesitheus. Anlässlich dieser Hochzeit wurden Denare mit der Diana Lucifera auf der Rückseite geprägt. Die Ehe blieb kinderlos.

Nach dem Tod der beiden Gordiane im Januar oder März 238 übernahm der Senat notgedrungen den Widerstand gegen den amtierenden Kaiser Maximinus Thrax, den man zuvor zum Staatsfeind erklärt hatte, und ernannte zwei gleichrangige "Augusti" aus den eigenen Reihen, Balbinus und Pupienus. Die stadtrömische Bevölkerung setzte mit Hilfe der Prätorianer, die sich durch das Vorgehen des Senates in der Kaiserwahl übergangen fühlten, die Ernennung des 13-jährigen Gordian (III.) zum "Caesar" und "princeps iuventutis" durch. Nach Maximinus’ gewaltsamen Tod bei Aquileia (zwischen April und Juni 238) traten offensichtliche Differenzen zwischen Balbinus und Pupienus hervor. Beide strebten die alleinige Herrschaft an. In dieser Situation ergriffen die Prätorianer die Initiative: Sie töteten die beiden "Augusti" im Kaiserpalast und zwangen den Senat zur Anerkennung von Gordians Herrschaftsanspruch. Gordian wurde daraufhin (wohl im Juli/August 238) zum alleinigen Kaiser proklamiert.

Zu Beginn seiner Herrschaft stand Gordian III. unter dem Einfluss des Senats, der den Kaiser durch einen Beraterstab lenkte. Innenpolitisch galt es zunächst, die Lage nach den chaotischen Verhältnissen der ersten Monate des Jahres 238 zu stabilisieren. Das bedeutete, die Autorität der Regierung auf Basis der Akzeptanz durch Heer und Senat (bzw. formal auch der stadtrömischen Bevölkerung) wiederherzustellen.

Als Zeichen des Konsenses wurde zunächst die "Legio III Augusta", die unter dem mauretanischen Statthalter Capelianus die Erhebung der Gordiane in der Provinz "Africa" niedergeschlagen hatte, aufgelöst (was bald zu militärischen Problemen im nun fast wehrlosen Nordafrika führen sollte). Neuer Prätorianerpräfekt wurde der Senator Aedinius Julianus – eine ungewöhnliche Entscheidung, da der Posten eigentlich Rittern vorbehalten war. Der Provinzstatthalter von "Hispania Citerior", Quintus Decius Valerinus, der Gordian III. die Anerkennung verweigert hatte, wurde gegen Ende 238 abgelöst. Die stadtrömische "plebs" wurde durch Zuwendungen und die Ausrichtung von Spielen besänftigt, und dem Heer wurden die bei Herrscherwechseln üblichen Geldzahlungen (Donative) versprochen. Der Steuerdruck auf die Reichsbevölkerung wurde leicht gemindert und die Selbstverwaltung der Städte gestärkt. Zeugnis der vielfältigen Maßnahmen ist eine spürbare Steigerung der Rechtsaktivitäten zu Beginn der Herrschaft Gordians III.

Die enorme finanzielle Belastung führte allerdings zu einer Geldentwertung mit spürbaren Auswirkungen auf Preise und Löhne. In den Jahren 239 und 241 bekleidete Gordian selbst das Konsulat. Im Jahr 240 wurde Sabinianus in Karthago zum Kaiser ausgerufen. Die Usurpation wurde aber durch den Statthalter von Mauretanien noch im selben Jahr niedergeschlagen.
Im gleichen Jahr fiel die wichtige, mit Rom verbündete Grenzstadt Hatra in Mesopotamien an das erstarkende neupersische Sassanidenreich. Die Perser überschritten den Euphrat und bedrohten Antiochia und die Provinz "Syria". Die persische Bedrohung sollte die letzten Regierungsjahre Gordians III. prägen (siehe auch "Römisch-Persische Kriege"). Die Römer reagierten mit den Vorbereitungen zu einem Perserfeldzug, den wohl bereits Pupienus und Balbinus beschlossen hatten. Mit dieser Aufgabe wurde der Ritter Gaius Furius Sabinus Aquila Timesitheus beauftragt. Timesitheus wurde Anfang 241 zum Prätorianerpräfekten ernannt und führte von nun an die Regierungsgeschäfte für den Kaiser, auf den er großen Einfluss gewann. Die Vorbereitungen waren Anfang 242 abgeschlossen. Nach der rituellen Öffnung der Tore des Janustempels (wohl zum letzten Mal in der Antike) begab sich der Kaiser mit dem Heer in den Osten. Im Herbst 242 erreichte das Heer Antiochia und konnte die Sicherheit in der Provinz wiederherstellen. Ende 242 oder Anfang 243 starb der loyale Timesitheus unter unbekannten Umständen, ihm folgte sein Stellvertreter Philippus Arabs im Amt des Prätorianerpräfekten nach.

Zu Beginn des Jahres 243 rückten die römischen Legionen in die Provinz Mesopotamien ein und schlugen die Armee des Perserkönigs Schapur I., der inzwischen die Nachfolge seines Vaters Ardaschir I. angetreten hatte, in der Schlacht bei Resaina (vermutlich noch unter der Führung des Timesitheus). Damit war die Kontrolle über die Provinz Mesopotamien wiederhergestellt. Gegen Herbst 243 stieß das römische Heer wahrscheinlich gegen die persische Hauptresidenz Ktesiphon vor, doch erlitten die Römer Anfang 244 bei Mesiche eine schwere Niederlage.

Gordian III. starb wohl im Februar 244. Sowohl über den Ort als auch über die genauen Todesumstände liegen uns in den Quellen unterschiedliche Versionen vor. Möglicherweise befand er sich schon wieder auf dem Rückweg außerhalb des persischen Gebiets, als er einer Meuterei der Soldaten zum Opfer fiel. Es wird oft angenommen, dass Philippus Arabs aus eigenem Machtstreben zumindest die Diskreditierung Gordians III. beim Heer betrieb, wenn nicht gar für seine Ermordung direkt verantwortlich war, doch ist dies umstritten. Die persische Darstellung, nach der Gordian infolge der Schlacht von Mesiche fiel, wird aber von mehreren Forschern als durchaus glaubwürdig angesehen, zumal auch spätere byzantinische Quellen (etwa Johannes Zonaras), die auf älteres Material zurückgreifen konnten, nicht auf eine Ermordung des Kaisers hinweisen.

Die Quellen zeichnen ein recht positives Bild von ihm – nur in der "Historia Augusta" erscheint er im Angesicht des Todes recht jämmerlich –, können aber nicht verleugnen, dass er wohl alles in allem aufgrund seiner Jugend ein schwacher und von seiner Umgebung abhängiger Herrscher war.

Gordian III. gehörte zu den sogenannten Kinderkaisern Roms. Trotz der relativ kurzen Dauer seines Wirkens konnte die Forschung fünf Bildnistypen ausmachen.

a) Typ 1 (Archetypus, vor Regierungsantritt)
Die Bildnisse zeigen wenig differenziert ausgearbeitete, symmetrische Gesichtszüge. Die Physiognomie Gordians III. ist dennoch erkennbar, wenn auch die für Gordian typischen Gesichtsfalten noch fehlen.
Ein Beispiel für diesen Typ findet sich im Konservatorenpalast in Rom (Inv. 995).

b) Typ 2 (ca. 238 bis 239)

Dieser Bildnistyp stellt Gordian III. mit zwei Querfalten auf der Stirn und zwei senkrechten Falten über der Nasenwurzel dar. Der Kaiser blickt äußerst konzentriert. Das Stilmittel der Querfalten, die sich in späteren Bildnistypen kaum mehr finden, sowie des wachen Blicks sind offensichtlich zur Darstellung von Herrscherwürde und Reife angewandt worden.
Das Leitstück für diesen Typ stellt die Trabeabüste im Pergamonmuseum in Berlin dar.
c) Typ 3 (ca. 238 bis 239)

Das Bestimmungsmerkmal dieses Typs, der offenbar zeitgleich mit Typ 2 kursierte, ist die gewollte Ähnlichkeit mit Severus Alexander. Dazu wurden andere Stilmittel als bei Typ 2 verwendet. Die Konturen sind nicht so hart ausgeführt und die Querfalten auf der Stirn fehlen. Dadurch strahlen die Bildnisse mehr Ruhe und Milde aus.
Das Leitstück für diesen Typ stellt eine Büste im Louvre in Paris dar (Inv. 2331).

d) Typ 4 (ca. 240 bis 242)

Dieser Bildnistyp verzichtet sowohl auf die Strenge des Typs 2 als auch auf die Nachahmung der Physiognomie von Severus Alexander aus Typ 3. Durch ein markanteres und annähernd rechteckiges Schädelprofil, durch angedeuteten Bartwuchs und die Führung der Augenbrauen wirkt der Kaiser reifer und strahlt Optimismus aus.
Das Leitstück für diesen Typ stellt eine Büste im Konservatorenpalast in Rom dar (Inv. 479).

e) Typ 5 (ca. 242 bis 244)
Diese letzten Bildnisse Gordians III. sind in den Proportionen des Typs 4 in der Länge gestreckt, vielleicht um den Herrscher im Zuge des Perserkrieges heroisch entrückt darzustellen.
Das Leitstück für diesen Typ stellt eine Panzerbüste im Louvre in Paris dar (Inv. 1063).

Gordian III. ist in einer ganzen Reihe von Quellen belegt, darunter in
Auch wenn keine dieser Quellen als besonders zuverlässig gilt und die späteren vielfach auf den früheren beruhen, so ergeben sie doch in ihrer Gesamtheit ein relativ klares Bild vom Leben Gordians, schildern ihn aber vielleicht insgesamt zu positiv.




</doc>
<doc id="1881" url="https://de.wikipedia.org/wiki?curid=1881" title="Geisteswissenschaft">
Geisteswissenschaft

Der Begriff Geisteswissenschaft(en) ist in der deutschsprachigen Denktradition eine Sammelbezeichnung für aktuell rund 40 unterschiedliche Einzelwissenschaften („Disziplinen“), die mit unterschiedlichen Methoden Gegenstandsbereiche, welche mit kulturellen, geistigen, medialen, teils auch sozialen bzw. soziologischen, historischen, politischen und religiösen Phänomenen zusammenhängen, untersuchen. Die meisten Geisteswissenschaften betreiben dabei also auch in einem gewissen Maße Anthropologie, da in allen Disziplinen der Mensch und seine Werke im Mittelpunkt stehen (→ Anthropologie). Eine einheitliche Begründung der Geisteswissenschaften wurde von Wilhelm Dilthey auf der Basis einer philosophischen Lehre vom Sinn und Verstehen von Lebensäußerungen (Hermeneutik) angestrebt.

Das Wort „Geisteswissenschaft“ ist schon in einer 1787 anonym verfassten Schrift mit dem Titel "Wer sind die Aufklärer?" belegt. Dort steht: „Wenn sage ich, Geistliche, die doch in der Gottesgelehrtheit und Geisteswissenschaft sorgfältigst sind unterrichtet worden …“ Der Autor bezieht sich also noch auf eine Theorie der „Pneumatologie des Geistes“. Damit ist eine Wissenschaft gemeint, welche Erklärungen gibt, die sich nicht auf natürliche, sondern „geistige“ Ursachen beziehen. In diesem Sinne redet auch z. B. Gottsched von einer „Geisterlehre“.

Fritz van Calker und Friedrich Schlegel verwenden „Geisteswissenschaft“ als Synonym für Philosophie überhaupt.

Näher am heutigen Wortsinn ist, was David Hume mit „moral philosophy“ meint, was Jeremy Bentham als „Pneumatologie“ von „Somatologie“ abgrenzt und was Ampère „Noologie“ im Gegensatz zur „Kosmologie“ nennt. John Stuart Mill bezeichnet in seinem System der deduktiven und induktiven Logik von 1843 mit „moral sciences“ die Disziplinen Psychologie, Ethologie und Soziologie. Mill bezieht dabei die induktive Logik auf die Datenbeschaffung aus geschichtlichen und gesellschaftlichen Phänomenen, weshalb die moral sciences so ungenau seien wie z. B. die Meteorologie. Jacob Heinrich Wilhelm Schiel hatte in einer ersten Übersetzung (in der zweiten nicht mehr) für moral sciences „Geisteswissenschaft“ gesetzt. Diese Verwendung dürfte zwar einflussreich gewesen sein, aber der deutsche Ausdruck wurde "nicht", wie früher oft angenommen, als Lehnübersetzung aus Mill geprägt, sondern ist, wie angezeigt, schon früher zu finden.

Georg Wilhelm Friedrich Hegel, Franz Hillebrand und andere deutsche Autoren sprechen von einer Geisteslehre oder Geisteswissenschaft. Hegels Geist-Begriff bezieht sich dabei nicht nur auf Individuen, sondern auch Gruppen und als objektiver Geist auf die Welt überhaupt.

Etwa im heutigen Sinne tritt das Wort „Geisteswissenschaft“ bei dem sonst unbekannten Ernst Adolf Eduard Calinich (* 25. März 1806 in Bautzen – 1824 stud.phil. in Leipzig, Mitglied in der Lausitzer Predigergesellschaft – 1844 Vizedirektor am Seminar in Dresden) auf, der 1847 zwischen der „naturwissenschaftlichen und der geisteswissenschaftlichen Methode“ unterscheidet, eine Zweiheit, von der unspezifisch auch schon 1824 bei W. J. A. Werber die Rede ist.

Der Ausdruck „Geisteswissenschaft“ bekommt seine Prägnanz wesentlich durch Wilhelm Dilthey ("Einleitung in die Geisteswissenschaften", 1883) und ist eng mit den politischen und universitären Voraussetzungen im deutschen Sprachgebiet verbunden.

Prägend ist dabei u. a. die Ausbildung der historischen Schule im Gefolge u. a. von Friedrich Carl von Savigny, Leopold von Ranke und Johann Gustav Droysen, die in kritischer Absetzung u. a. zu Hegel ein Ideal für Näherbestimmungen des methodischen Propriums von „Geisteswissenschaften“ vorgibt.

Dilthey definierte die Geisteswissenschaften in scharfer Entgegensetzung zu den Naturwissenschaften durch die ihnen eigene Methode des Verstehens, wie sie als Hermeneutik seit Friedrich Schleiermacher auch außerhalb der Philologie gebräuchlich geworden war. Dilthey suchte sie als „Erfahrungswissenschaft der geistigen Erscheinungen“ beziehungsweise als „Wissenschaft der geistigen Welt“ zu begründen. Sie sollte eine ursprünglich konzipierte „Kritik der historischen Vernunft“ empirisch erweitern.

Dilthey griff zur Wortbildung „Geisteswissenschaften“ Hegels Begriff des Geistes auf. Hegel bezog den Begriff „Geist“ auf das „Geistesleben“ einer Gruppe, eines Volkes oder einer Kultur. Der Begriff ist daher stark an die deutsche idealistische Tradition und Hegels Konzept des objektiv-objektivierten Geistes gebunden. Dies ist bis heute Grund dafür, dass er sich kaum übersetzen lässt. Übliche Analoga sind "humanities", "(liberal) arts" und "human studies". Das französische Analogon ist meist "sciences humaines".

Wichtig für die frühe Konzeption der Geisteswissenschaften waren die Gegensatzpaare Geist–Natur, Geschichte–Naturwissenschaft, Verstehen-Erklären. Während die Naturwissenschaft versuchte, die Natur aufgrund ewiger Gesetze zu "erklären", sah man es als Aufgabe einer historisch ausgerichteten Geisteswissenschaft, das Geistesleben vergangener Völker in ihrer Einmaligkeit zu "verstehen".

Zu Mitte und Ende des 19. Jahrhunderts orientieren sich außerdem viele Autoren an kantischer Erkenntnistheorie und v. a. am sogenannten Psychologismus. So definiert etwa Wilhelm Wundt, dass die Geisteswissenschaften ansetzen, „wo der Mensch als wollendes und denkendes Subject ein wesentlicher Faktor der Erscheinungen ist“. Theodor Lipps definierte - bezogen auf das Individuum - die „Geisteswissenschaft“‘ als „Wissenschaft der inneren Erfahrung“. Er hielt die individuelle „innere Erfahrung“ für den grundlegenden Maßstab von Erkenntnistheorie, Logik, Psychologie und Wahrnehmung. Ähnlich die „Südwestdeutsche Schule“ des Neukantianismus (Wilhelm Windelband, Heinrich Rickert). Im Sinne von Psychologismus und historischer Schule wird hier postuliert: Geisteswissenschaften sind ideographisch, nicht nomothetisch (Windelband); sie sind individualisierend und wertbezogen, nicht generalisierend (H. Rickert), „auf historische Einmaligkeiten und nicht nur auf Gesetzmäßigkeiten gerichtet“. Rickert nennt die Geisteswissenschaften, da er sie auf Kulturwerte bezieht, auch „Kulturwissenschaften“. Auch Max Weber und Ernst Troeltsch stehen dieser Wertphilosophie nahe.

Die Marburger Schule (Hermann Cohen u. a.) dagegen sieht die Logik der Geisteswissenschaften in der Rechtswissenschaft.

Um 1900 ist dann der lebensphilosophische Geistbegriff u. a. Diltheys weithin prägend, so etwa bei Philosophen und Pädagogen wie Nicolai Hartmann, Otto Friedrich Bollnow, Eduard Spranger, Theodor Litt, Herman Nohl, Georg Misch, Hans Freyer und Erich Rothacker.

Im Gefolge des Linkshegelianismus wird die nach Hegel und der Lebensphilosophie übliche Rede von „Geisteswissenschaften“ Mitte des 20. Jahrhunderts im marxistischen Sprachgebrauch weitgehend ersetzt durch „Sozialwissenschaften“ oder „Gesellschaftswissenschaften“.

Der Begriff umfasst im „deutschen Sprachgebrauch sämtliche Wissenschaften, die nicht Naturwissenschaften sind (mit Ausnahme der Mathematik), also alle, die in der theologischen, juristischen und philosophischen (d. i. philologisch-historischen) Fakultät gepflegt werden“.

Auch wenn in den Geisteswissenschaften heute noch davon ausgegangen wird, dass sich kulturelle Bedeutungszusammenhänge, Sinnstrukturen, Verstehens- und Wahrnehmungsweisen nicht allein im Rahmen einer naturwissenschaftlichen Betrachtungsweise behandeln lassen, so ist die starke Opposition zwischen den Disziplinen inzwischen verschwunden, und es wird versucht, durch interdisziplinäre Ansätze beide Zugangsweisen zu kombinieren.

Über diese erkenntnistheoretischen Erörterungen hinaus führten jedoch auch politische und soziale Absichten zu solchen Schlüssen: Die Nützlichkeit technischer Neuerungen täuschte nach der Julirevolution von 1830 und der Märzrevolution von 1848 über den gescheiterten gesellschaftlichen Konsens hinweg. Die aufstrebenden Natur- und Ingenieurwissenschaften stützten mindestens vordergründig die restaurative Macht des Spätabsolutismus. Hermeneutik hat dagegen mit einem stets neu zu findenden und zu erhaltenden Konsens von Beobachtern zu tun und entzieht sich der empirischen Nachweisbarkeit in Spurensicherung oder Experiment, die mit Erfolg gegen ältere wissenschaftliche Methoden ausgespielt wurden. Um dem gewachsenen Anspruch auf Wertfreiheit und Objektivität zu genügen, musste sich allerdings auch die Hermeneutik vermehrt der Spurensicherung bedienen. Dieses Konzept einer Wissenschaft erschien Dilthey verteidigenswert.

Der Aufschwung der Naturwissenschaften seit Anfang des 19. Jahrhunderts war einhergegangen mit der Herausbildung neuartiger Disziplinen im Rahmen der alten Philosophischen Fakultät, die sich durch rigorose Methodik auszeichneten; die alte Einheit war unwiederbringlich verloren. Damit war ein Großteil der alten Fächer in Frage gestellt. Das Konzept der Geisteswissenschaften half diesen, sich zu behaupten und zu modernisieren. So haben sich die alten Fakultätswissenschaften Theologie und Rechtswissenschaft erfolgreich als Geisteswissenschaften neu definiert.

Eine ähnliche und parallel laufende Unterscheidung ist die zwischen nomothetischen („regelsetzenden“) und idiographischen („beschreibenden“) Wissenschaften, die manchmal dazu dient, die Sozialwissenschaften als nomothetisch abzugrenzen. Sie geht auf Wilhelm Windelband zurück.

Ein weiterer wichtiger Faktor für die Entstehung der Geisteswissenschaften war das Verhältnis zwischen Universität und Staat: Im 19. Jahrhundert hatten sich die bürgerlichen Gelehrten, Künstler und Literaten einen Geistesadel und eine Hochkultur geschaffen, und diesen „Geist“ galt es nicht zuletzt gegenüber der führenden Oberschicht zu behaupten. Der Adel dagegen benötigte keine Reputation durch künstlerische oder wissenschaftliche Betätigung. Er zog sich zurück und tendierte eher zur populären Unterhaltung.

Ob eine Geschichtlichkeit von „Seelenvorgängen“ (Dilthey) etwas Kollektives sein kann, war nicht zuletzt eine politische Haltung. Georg Friedrich Hegel betrachtete den Geist als etwas Überindividuelles, nicht bloß Subjektives. Dies traf in einer Zeit der fehlenden staatlichen Einheit und der missglückten Emanzipation des Bürgertums von partikularisierenden Interessen des Adels auf breite Zustimmung. Mehr als in anderen Sprachgebieten ist im deutschen das Wollen und Handeln („Wirken“) eines gemeinschaftlichen Geistes behauptet worden. Aus dieser Tradition heraus entstanden Allgemeinbegriffe wie Zeitgeist, „Geist einer Nation“, „Geist einer Epoche“. Max Weber sprach von einem „Geist“ des Kapitalismus ("Die protestantische Ethik und der „Geist“ des Kapitalismus", 1904/05).

Dieser Begriff des Geistes, der Institutionen, Strukturen und Erklärungsmuster zu etwas von sich aus Lebendigem macht, blieb nicht unumstritten. So gab es immer den Vorwurf, dass die traditionellen Autoritäten de facto durch technische und bürokratische Apparate ersetzt worden seien, die die Willensfreiheit zum Sachzwang machten. Eine ähnliche Ansicht hat Friedrich Kittler mit seiner Forderung einer „Austreibung des Geistes aus den Geisteswissenschaften“ vertreten.

In der interdisziplinär angelegten Aktion Ritterbusch wurden Geisteswissenschaften in die völkische Ideologie des Nationalsozialismus und die Verherrlichung des Krieges eingebunden.

Als Gegenbewegung nach dem Zweiten Weltkrieg erfolgte eine starke Individualisierung. Die wissenschaftliche Würdigung großer Persönlichkeiten und ihrer Werke blendete mitunter ihre geschichtlichen Bedingtheiten aus. In der Literaturwissenschaft wurde die werkimmanente Interpretation üblich.

Der Titel der 1959 erschienenen These der Zwei Kulturen von C. P. Snow wurde zum Schlagwort: Geisteswissenschaften (englisch "humanities") und Naturwissenschaften trennen unvereinbare Wissenschaftskulturen, die sich derart diametral gegenüberstehen, dass eine Kommunikation unmöglich scheint. Als Reaktion auf diese stark rezipierte Studie erschien im Jahr 1995 John Brockmans "Die dritte Kultur" als Vision einer Vermittlung zwischen den Wissenschaften.

Wie die Begriffsgeschichte illustriert, hat der Ausdruck „Geisteswissenschaft“ eine wechselhafte Verwendung erfahren. Bis in die Gegenwart hat die Vielfalt unterschiedlicher Einzelwissenschaften weiter zugenommen, wobei unterschiedliche institutionelle Systematiken entstanden, etwa was die unterschiedliche verwaltungsmäßige Zusammenlegung zu universitären Fachbereichen und Fakultäten betrifft.

Unter die verschiedenen heute gebrauchten Sammelbegriffe zählen beispielsweise neben „Geisteswissenschaften“ Bezeichnungen wie Sozialwissenschaften, Naturwissenschaften, Humanwissenschaften (Wissenschaften, die irgendeinen Aspekt der Menschen zum Untersuchungsgegenstand haben, wie neben Geistes- und Sozialwissenschaften Humanbiologie, Medizin u. a.), Kulturwissenschaften, Lebenswissenschaften usw. Auch hier besteht im Detail und in Grenzfällen, v. a. was neuere interdisziplinäre Fächer und Studiengänge betrifft, kein Konsens über Begriffsbestimmung oder Begriffsumfang, also insbesondere darüber, welche faktischen Studiengänge aus welchen kriteriologischen Gründen unter welchen dieser Sammelbegriffe gehören.

Der Theologe Arno Anzenbacher schlug 1981 beispielsweise folgende Wissenschaftsgliederung vor:
Zahlreiche Theoretiker sowie eine Vielzahl der Institute rechnen weder die Sozial- und Wirtschaftswissenschaften noch die Humanwissenschaften (im engeren Sinne) zu den Geisteswissenschaften.

Odo Marquard vertrat 1986 die These, es sei die Aufgabe der Geisteswissenschaften bei fortlaufender Umwälzung und Modernisierung der Lebensverhältnisse in der technisch-zivilisatorischen Gesellschaft, ein Asyl für Kultur und Tradition zu bieten und so die Modernisierung erträglich zu machen:
„Die Geisteswissenschaften helfen den Traditionen, damit die Menschen die Modernisierung aushalten können; sie sind […] nicht modernisierungsfeindlich, sondern – als Kompensation der Modernisierungsschäden – gerade modernisierungsermöglichend. Dafür brauchen sie die Kunst der Wiedervertrautmachung fremd gewordener Herkunftswelten.“
Prominente Wissenschaftler wie Wolfgang Frühwald, Hans Robert Jauß und Reinhart Koselleck forderten Anfang der 1990er Jahre eine verstärkte Umorientierung der Geisteswissenschaften hin zu Kulturwissenschaften. In ihrer Denkschrift „Geisteswissenschaften heute“ als Ergebnis eines Forschungsprojektes des Wissenschaftsrates und der Westdeutschen Rektorenkonferenz bestimmen sie 1991 die Aufgabe und Zukunft der Geisteswissenschaften wie folgt:
„Die Geisteswissenschaften sind der ‹Ort›, an dem sich moderne Gesellschaften ein Wissen von sich selbst in Wissenschaftsform verschaffen. […] es ist ihre Aufgabe, dies in der Weise zu tun, daß ihre Optik auf das kulturelle Ganze, auf Kultur als Inbegriff aller menschlichen Arbeit und Lebensformen, auf die kulturelle Form der Welt geht, die Naturwissenschaften und sie selbst eingeschlossen.“
Auf die Frage nach der Zukunft der Geisteswissenschaften in einer zunehmend technisierten Umwelt antwortete Norbert Schneider, seinerzeit (2009) Vorsteher des von der Schließung bedrohten Instituts für Kunstgeschichte der Universität Karlsruhe:
„Jedenfalls gab und gibt es eine große Fraktion innerhalb der technischen und naturwissenschaftlichen Disziplinen, die […] die eminent wichtige Funktion der Geisteswissenschaften [übersieht], die zu großen Teilen das historisch-kulturelle Erbe bewahren, auch das von technisch-naturwissenschaftlichen Errungenschaften, z. B. in der Wissenschaftsgeschichte, an der unter anderem auch die Kunstgeschichte maßgeblich beteiligt ist. Darüber hinaus halten die Geisteswissenschaften institutionell auch eine Reflexion über die Selbstverständigung der Gesellschaft lebendig in Gang, die über reines Effizienzdenken hinausgeht.“
Hans Albert hat den methodologischen Autonomieanspruch der Geisteswissenschaft als solchen kritisiert. Er vertritt demgegenüber die Ansicht, dass es für Wissenschaft grundsätzlich gesehen nur eine einheitliche Methode gebe. Damit leugnet er aber keineswegs, dass das (Sinn-)Verstehen eine für die Geisteswissenschaften spezifische Funktion hat; nur ist dies nach Albert keine methodologische, sondern eine der Rolle der Wahrnehmung in den Naturwissenschaften vergleichbare Funktion, ein „Sonderfall der Wahrnehmung“.






</doc>
<doc id="1884" url="https://de.wikipedia.org/wiki?curid=1884" title="Republikaner">
Republikaner

Republikaner bezeichnet:

im Einzelnen
Schweizer Tageszeitungen
Siehe auch:



</doc>

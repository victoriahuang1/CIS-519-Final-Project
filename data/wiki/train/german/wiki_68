<doc id="10068" url="https://de.wikipedia.org/wiki?curid=10068" title="Max Ernst">
Max Ernst

Max Ernst (* 2. April 1891 in Brühl (Rheinland); † 1. April 1976 in Paris, Frankreich), eigentlich "Maximilian Maria Ernst", war ein bedeutender Maler, Grafiker und Bildhauer deutscher Herkunft. Nach dem Militärdienst im Ersten Weltkrieg gründete er 1919 zusammen mit Johannes Baargeld und Hans Arp die Kölner Dada-Gruppe. 1922 ließ er seine Familie zurück und zog nach Paris, wo er sich dem Kreis der Surrealisten um André Breton anschloss und zu einem der wichtigsten Mitglieder im künstlerischen Bereich der surrealistischen Bewegung wurde. Zu Beginn des Zweiten Weltkriegs wurde er ab 1939 mehrfach in Frankreich interniert, konnte zusammen mit der Kunstmäzenin Peggy Guggenheim, seiner späteren dritten Ehefrau, fliehen und wählte, wie viele andere europäische Künstler, 1941 als Exil die USA. 1953 kehrte er mit seiner vierten Ehefrau, der Malerin Dorothea Tanning, nach Frankreich zurück.

Mit seinen Gemälden, Collagen und Skulpturen schuf der Künstler rätselhafte Bildkombinationen, bizarre Wesen, die häufig Vögel darstellen, und phantastische Landschaften. Neben seinem künstlerischen Werk verfasste Max Ernst Gedichte und autobiografische sowie kunsttheoretische Schriften. Künstlerbücher nehmen einen breiten Raum in seinem Schaffen ein. Die Techniken Frottage, Grattage und Drip Painting, als "Oszillation" entwickelt, gehen auf ihn zurück. Angewendet durch Jackson Pollock wurde Drip Painting zu einem Bestandteil des amerikanischen abstrakten Expressionismus.

Max Ernst wurde als drittes von insgesamt neun Kindern des Taubstummenlehrers und Laienmalers Philipp Ernst (1862–1942) und seiner Frau Luise, geb. Kopp (1865–1949) geboren. Ersten Kontakt mit Malerei bekam er durch seinen Vater. Dieser malte seinen fünf Jahre alten Sohn 1896 als Jesuskind.

Max Ernst veröffentlichte 1942 im amerikanischen Exil im Kunstmagazin "View" einen autobiografischen Text, in dem er seine magische Beziehung zu Vögeln beschreibt: seine Geburt aus einem Vogelei, das seine Mutter in ein Adlernest gelegt hatte, und die Verbindung zwischen dem Tod seines Lieblingsvogels, des rosa Kakadus Hornebom, und der als gleichzeitig erlebten Geburt seiner jüngsten Schwester Apollonia (genannt Loni) im Jahr 1906. Max Ernsts Schilderung des Zusammentreffens beider Ereignisse ist fiktiv, der Kakadu lebte nach der Geburt der Schwester einige Zeit weiter. Max Ernst entwickelte, von diesen Vorstellungen geprägt, in seinem Werk als Alter Ego den „Vogelobren Hornebom“ und „Loplop“. Im Spätwerk taucht der Künstler als „Schnabelmax“ auf. Vogelähnliche Geschöpfe erscheinen in vielen seiner Bilder, besonders in der Werkreihe "Loplop präsentiert" aus den 1930er Jahren.

Nach dem Abitur in Brühl studierte er von 1910 bis 1914 an der Universität Bonn zunächst Altphilologie, Philosophie, Psychologie und Kunstgeschichte. Philosophen, die der Student Ernst schätzte, waren Novalis, Hegel und die „großen Kritiker des eindimensional gewordenen Denkens des 19. Jahrhunderts, Max Stirner und Friedrich Nietzsche“. Durch das Studium der Psychologie kam er mit den Schriften Sigmund Freuds in Berührung und beschäftigte sich mit der Kunst der Geisteskranken.

Max Ernst besuchte oft das Museum in Köln und interessierte sich für die flämischen Meister Hieronymus Bosch und Pieter Bruegel sowie für die deutsche Romantik, besonders für Caspar David Friedrich. Aus dem Jahr 1909 sind zwei Landschaften erhalten, die seine Bewunderung für Vincent van Gogh zeigen. 1911 befreundete er sich mit August Macke und beschloss im folgenden Jahr, ermutigt durch die Anerkennung und Förderung durch Mackes Freundeskreis, als Autodidakt Maler zu werden.

Seine erste Ausstellung hatte Max Ernst 1912 in der Galerie Feldmann in Köln. Auf der Internationalen Kunstausstellung des Sonderbundes Westdeutscher Kunstfreunde und Künstler zu Cöln im selben Jahr sah er Werke bedeutender Künstler wie die von Paul Cézanne, Pablo Picasso, Henri Matisse und Edvard Munch.

1912/1913 schrieb Max Ernst Kunst- und Theaterkritiken für den Bonner "Volksmund" und beteiligte sich 1913 an der Ausstellung Rheinische Expressionisten in Bonn. Während einer Reise nach Paris im selben Jahr lernte er Guillaume Apollinaire und Robert Delaunay kennen sowie 1914 Hans Arp, mit dem ihn eine lebenslange Freundschaft verband. Ebenfalls 1913 war Max Ernst mit zwei Werken (#129: "Sturm"; #130: "Promenade") am Ersten Deutschen Herbstsalon in der Berliner Galerie „Der Sturm“ von Herwarth Walden vertreten, in dessen Galerie im März 1916 eine zweite Ausstellung seiner Werke folgte, und 1917 in der Zürcher „Galerie Dada“, einer weiteren von Walden organisierten Ausstellung.

Von August 1914 bis November 1918 war Ernst im Ersten Weltkrieg in Frankreich und Polen im Einsatz. 1918 wurde er gegen seinen Willen zum Leutnant befördert. Am 7. Oktober 1918, kurz vor dem Ende des Kriegs, heiratete er in einer Kriegstrauung seine Studienfreundin, die promovierte Kunsthistorikerin Luise Straus, die Tochter des jüdischen Hutfabrikanten Jacob Straus, in Köln. Der gemeinsame Sohn Hans-Ulrich, der später unter dem Namen Jimmy Ernst in den USA als surrealistischer Maler bekannt wurde, kam 1920 zur Welt.
Die Rückkehr aus dem Krieg empfand er als Wiedergeburt:

Angeekelt von der „großen Schweinerei dieses blödsinnigen Krieges“, gründete Max Ernst 1919 mit dem selbst erfundenen Beinamen „minimax dadamax“ gemeinsam mit Johannes Theodor Baargeld, dem „Zentrodada“, und Hans Arp die Kölner Dada-Gruppe, an der seine Ehefrau Luise Straus-Ernst beteiligt war. Bereits im Februar und März des Jahres waren nach einer Doppelnummer fünf weitere Ausgaben der Wochenschrift "Der Ventilator", herausgegeben von Baargeld unter Mitarbeit von Max Ernst, erschienen. Weitere Ausgaben wurden verboten.

Im Sommer 1919 fuhr Ernst zusammen mit Baargeld nach München und besuchte dort Paul Klee. In der Münchner Buch- und Kunsthandlung von Hans Goltz entdeckte er in der Zeitschrift "Valori Plastici" Werke von Giorgio de Chirico, die ihn so sehr beeindruckten, dass er die Grafik-Mappe "Fiat modes – pereat ars" mit den Mitteln einer finanziellen Unterstützung für arbeitslose Künstler in Heinrich Hoerles Schloemilch-Verlag herausbrachte. Ebenfalls 1919 wurde in Düsseldorf die Künstlervereinigung Das Junge Rheinland gegründet, zu deren Mitgliedern er zählte. Ihr Mittelpunkt war die Galerie von Johanna Ey, die das Werk von Ernst durch Ausstellungen unterstützte. Im November des Jahres fand eine Ausstellung der Dadaisten in den Räumen des Kölnischen Kunstvereins statt, die von der britischen Militärregierung, unter der das Rheinland nach dem Ersten Weltkrieg stand, geschlossen wurde.

1920 wurde die „Zentrale W/3“ gegründet, die „Weststupidien 3“ waren Arp, Baargeld und Ernst, die im Februar die Zeitschrift "die schammade.(dilettanten erhebt euch!)" veröffentlichten. Zu den französischen Mitarbeitern gehörten André Breton, Paul Éluard und Louis Aragon. Die zweite Kölner Dada-Ausstellung wurde im April 1920 unter dem Titel "Dada-Vorfrühling" im Brauhaus Winter gezeigt. Gemeinsam mit Hans Arp schuf er hierfür die "Collages collectifs", scherzhaft "Fatagaga" (Fabrication de tableaux garantis gazometriques) genannt. Die Ausstellung erregte Unmut in der Öffentlichkeit, wurde zeitweise von der Polizei geschlossen und führte zum Bruch mit Ernsts Vater. Ab Ende Juni 1920 war der Künstler wie auch Baargeld mit Werken an der Ersten Internationalen Dada-Messe in Berlin vertreten. Ein Jahr später traf er sich während eines Urlaubs in Tarrenz, Tirol, unter anderem mit Tristan Tzara, Hans Arp, Sophie Taeuber und André Breton. Auf Einladung Bretons hatte er im Mai/Juni 1921 eine erste Ausstellung in der Pariser Galerie Au Sans Pareil, auf der er jedoch nicht anwesend war. Der Katalog verzeichnete unter anderem die Collage "Der Hut macht den Mann". Im Herbst 1921 waren der Schriftsteller Paul Éluard und seine Frau Gala für eine Woche bei Max Ernst in Köln zu Gast. Éluard erwarb von ihm spontan die frühsurrealistischen Gemälde "Celebes" und "Oedipus Rex".

Im August 1922 zog der Künstler nach einem zweiten Aufenthalt in Tarrenz endgültig nach Paris und verließ seine Familie. Éluard hatte ihm seinen eigenen Pass geschickt, da Ernst kein Visum bekommen hatte, und Max Ernst wohnte beim Ehepaar Éluard. Fast zwei Jahre lang musste er Gelegenheitsarbeiten verrichten, da er von seiner Malerei nicht leben konnte. Sein Werk "Das Rendezvous der Freunde" aus dem Jahr 1922 wurde im folgenden Jahr im „Salon des Indépendants“ präsentiert. 1923 zog das Ehepaar Éluard in ein Haus in Eaubonne nahe Paris, wo Ernst einen Zyklus von 15 Bildern mit grotesken Fabelwesen und paradiesischen Gärten an die Türen malte. Sie wurden von den Nachbewohnern überklebt, 1969 wiederentdeckt, vom Künstler selbst restauriert und auf Leinwand übertragen. Darunter befand sich das Wandbild "Beim ersten klaren Wort", das heute zur Kunstsammlung Nordrhein-Westfalen gehört. Ernst hatte sich in Gala verliebt, und Éluard, der die Liaison zunächst geduldet hatte, entfloh der ménage à trois 1924 nach Saigon. Gala und Max Ernst folgten ihm. Das Geld für die Reise stellte er durch Verkäufe seiner Pariser Bilder an Johanna Ey sicher. Während die Éluards nach Paris zurückkehrten, trat Ernst erst drei Monate später die Rückreise an. Gala heiratete später Salvador Dalí.
Ab 1924 war Ernst bereits eines der wichtigsten Mitglieder der surrealistischen Gruppe um André Breton. In diesem Jahr verfasste Breton das "Manifeste du Surréalisme", in dem er Surrealismus als einen „reinen psychischen Automatismus“ definierte. Organ der Gruppe war die Zeitschrift "La Révolution surréaliste", in die Werke von Ernst aufgenommen wurden.

1925 bezog Ernst sein erstes Atelier in Les Fusains in der 22, rue Tourlaque, Paris. Ein in diesem Jahr abgeschlossener Vertrag mit dem Sammler Jacques Viot sicherte ihm ein regelmäßiges Einkommen, Ausstellungen in Pariser Galerien folgten. Ab 1925 fertigte er seine ersten Werke in der von ihm erfundenen Frottagetechnik an, die in seinem Buch "Histoire naturelle" 1926 veröffentlicht wurden, und entwickelte die Maltechnik der Grattage. Diese Techniken bestimmten nun den für ihn typischen Stil. Er arbeitete zusammen mit Joan Miró am Bühnenbild und an den Kostümen für Djagilews Ballett "Romeo and Juliet", Musik von Constant Lambert, das von den Ballets Russes aufgeführt wurde. Diese Mitarbeit rief den Protest der Surrealistengruppe hervor. Auf starken Widerstand, dieses Mal bei klerikalen Kreisen, stieß sein 1926 entstandenes Gemälde "", das im Salon des Indépendants und darauf im Kölnischen Kunstverein ausgestellt wurde, wo es entfernt werden sollte.

Nach der Scheidung im Jahr 1926 von Luise Straus-Ernst heiratete Ernst im folgenden Jahr die wesentlich jüngere Marie-Berthe Aurenche, die Schwester des Drehbuchautors Jean Aurenche. Ab den frühen 1930er Jahren bezog das Paar eine Wohnung im Quartier Montparnasse in der 26, rue des Plantes, wo sie Nachbarn von Alberto Giacometti waren, der gelegentlich Skulpturen auf der Terrasse der Ernsts lagern durfte. 1930 übernahm Max Ernst die Rolle eines Räuberhauptmanns in dem umstrittenen surrealistischen Film "Das goldene Zeitalter (L’Âge d’Or)" von Luis Buñuel und Salvador Dalí. Im Herbst 1933 lernte er die zwanzigjährige Künstlerin Meret Oppenheim kennen, mit der er eine bis ins nächste Jahr andauernde Liebesbeziehung hatte.

Den Sommer 1934 verbrachte Max Ernst bei Alberto Giacometti in dessen Sommeratelier in Maloja; beide Künstler transportierten in einem Flussbett vorgefundene, vom Wasser rundgewaschene Granitblöcke vor Giacomettis Haus. Ernst bearbeitete diese Blöcke als erste bildhauerische Versuche, schuf auf ihnen flache Reliefs, die Darstellungen von Blumen, Vögeln und Figuren boten, deren Form er später jedoch nicht fortsetzte. 1936 erfolgte die Scheidung von Marie-Berthe Aurenche.

Unter dem NS-Regime in Deutschland wurden Max Ernsts Werke diffamiert, er selbst wurde geächtet und seine Kunst 1933 als „entartet“ eingestuft. 1937 wurden zwei seiner Bilder in der Münchner Ausstellung "Entartete Kunst" gezeigt. Das Gemälde "Die Schöne Gärtnerin" (1923), in einem Raum unter dem Motto „Verhöhnung der deutschen Frau“ ausgestellt, gilt seitdem als verschollen. In diesem Jahr entstand unter dem Eindruck des Spanischen Bürgerkriegs das Gemälde "L’Ange du foyer" "(Der Hausengel)" in drei Variationen, auf dem ein Ungeheuer mit gefletschten Zähnen und ausgefahrenen Krallen die Erde bedroht. Ebenfalls in diesem Jahr entstanden Bühnenbilder zu Alfred Jarrys Drama "Ubu enchainé", das 30 Jahre nach dem Tod des Autors in der Comédie des Champs-Elysées am 22. September 1937 erstmals aufgeführt wurde.

1938 war er Teilnehmer an der richtungweisenden Exposition Internationale du Surréalisme in der Galerie Beaux-Arts in Paris; zusammen mit Salvador Dalí war er als technischer Beirat tätig und stellte 14 Gemälde aus, dazu eine Puppe als Kunstobjekt. Die Ausstellung erwies sich jedoch als Abschlussmanifestation der surrealistischen Bewegung. Die politischen Umstände sowie die persönlichen, politisch motivierten Differenzen – zwischen André Breton und Paul Éluard, der sich dem Stalinismus angenähert hatte – veranlassten Éluard, im selben Jahr aus der Surrealistengruppe auszutreten. Max Ernst und Man Ray schlossen sich ihm solidarisch an.

Der Zweite Weltkrieg beeinflusste das Leben des Künstlers nachhaltig: Ernst wurde 1939 als „verfeindeter Deutscher“ in Frankreich zunächst im ehemaligen Gefängnis von Largentière, anschließend im Lager Les Milles interniert, wo er auf Hans Bellmer traf. Dort entstand das gemeinsame Wandbild "Schöpfungen, die Geschöpfe der Einbildungskraft", und Bellmer schuf ein Porträt Max Ernsts, das aus gemalten Ziegelsteinen zusammengesetzt war als Hinweis auf die Mauern des Lagers. Durch Vermittlung Éluards wurde er zu Weihnachten wieder freigelassen. Im darauffolgenden Jahr wurde er erneut inhaftiert, dieses Mal von der Gestapo, doch ihm gelang die Flucht. In seinem Haus konnte er nicht bleiben, denn seine Geliebte Leonora Carrington, mit der er seit 1938 in Saint-Martin-d’Ardèche zusammengelebt hatte, war geflüchtet und hatte es für eine Flasche Cognac an einen Nachbarn „verkauft“, um es vor der Beschlagnahme zu retten.

Ernst floh mit der Kunstsammlerin Peggy Guggenheim, die ihn finanziell unterstützte, 1941 über Spanien und Portugal in die USA. Fluchthelfer war Varian Fry, der Leiter des Emergency Rescue Committee. Sie erreichten am 14. Juli 1941 den New Yorker Flughafen La Guardia, wo sie von Jimmy Ernst und alten Bekannten begrüßt wurden. Max Ernst wurde zunächst als „feindlicher Ausländer“ von Beamten der Einwanderungsbehörde in der Festung auf Ellis Island interniert, aber nach drei Tagen entlassen. Im Dezember des Jahres – nach dem Eintritt der Vereinigten Staaten in den Krieg – heirateten Max Ernst und Peggy Guggenheim in New York.
Im März des Jahres 1942 nahm Ernst an der Gemeinschaftsausstellung "Artists in Exile" in der Gallery Pierre Matisse teil. Ein Foto zeigt ihn neben 13 weiteren Künstlern wie Marc Chagall, Fernand Léger, Roberto Matta, Piet Mondrian, Kurt Seligmann, Yves Tanguy und dem Schriftsteller André Breton. Im Mai erschien der Katalog mit einem Umschlagbild von Ernst für Peggy Guggenheims Galerie Art of This Century, die im Oktober 1942 eröffnet wurde. Im selben Jahr wurde er mit David Hare, Marcel Duchamp und André Breton Herausgeber der Zeitschrift "VVV", die eine wichtige Rolle bei der Verbreitung des Surrealismus in den Vereinigten Staaten spielte. Ebenfalls im Jahr 1942 nahm Ernst an der Ausstellung First Papers of Surrealism in New York teil und begann verstärkt als Bildhauer zu arbeiten.

In der New Yorker Buchhandlung Wakefield zeigte Betty Parsons in einer Gruppenausstellung ein in neuer Technik gemaltes Bild Ernsts, damals nannte er es "abstrakte Kunst, konkrete Kunst", später "Junger Mann, neugierig den Flug einer nicht-euklidischen Fliege beobachtend". Die Technik – Drip Painting oder Oszillation genannt – erregte die Aufmerksamkeit einiger junger amerikanischer Maler wie Jackson Pollock.

Ernst lernte Ende 1942 die junge amerikanische Malerin Dorothea Tanning kennen, die in Guggenheims Galerie Art of This Century in der Ausstellung "Exhibition by 31 Women" vertreten war, und trennte sich daraufhin von Peggy Guggenheim. 1945/46 fand die Ausschreibung zum Bel-Ami-Wettbewerb statt, den Ernst mit seinem Gemälde "Die Versuchung des heiligen Antonius" gewann.

1946 wurde Doppelhochzeit gefeiert: Max Ernst und Dorothea Tanning sowie Man Ray und Juliet Browner ließen sich in Beverly Hills trauen. Anschließend brach das Ehepaar nach Sedona in der Wüste von Arizona auf und erbaute dort ein kleines, einsam gelegenes Haus, das Capricorn Hill genannt wurde. Dort entstand 1948 die Zementplastik "Capricorn". Im selben Jahr erlangte Max Ernst die amerikanische Staatsbürgerschaft. Er kam in Kontakt mit den Ureinwohnern, den Hopi, und befasste sich mit ihrer Kunst. Sein Interesse galt besonders den Kachinapuppen und den Zeremonienmasken. Die geometrisch stilisierten Formen und die Bemalung der Kachinas spiegeln sich in seinem Werk wider.

Anlässlich des 60. Geburtstags von Ernst veranstaltete seine Heimatstadt Brühl 1951 eine erste große deutsche Retrospektive im Brühler Schloss Augustusburg, die gut besucht war, jedoch mit einem finanziellen Defizit endete. Die Kölner Galerie Der Spiegel beteiligte sich an der Ausstellung, woraus sich ab 1953 eine lebenslange Geschäfts- und Freundschaftbeziehung zwischen der Galerie und Max Ernst sowie Dorothea Tanning entwickelte. So bat Ernst seinen Kölner Galeristen Hein Stünke, den Inhaber der Galerie Der Spiegel, um Mithilfe bei der Auffindung seiner während der Wirren des Zweiten Weltkriegs verloren gegangenen Scheidungspapiere seiner zweiten Ehe mit Marie-Berthe Aurenche, die den Künstler durch ihre Pariser Anwälte aufgefordert hatte, den ehelichen Verkehr und Unterhalt wieder aufzunehmen. Hein Stünke beauftragte den Kunstsammler und Juristen Josef Haubrich mit der Recherche, der darin erfolgreich war. 1952 wurde Ernst Mitglied des Collège de ’Pataphysique, das nach dem Zweiten Weltkrieg zu Ehren von Alfred Jarry und Jarrys ’Pataphysik, der Wissenschaft von den imaginären Lösungen, gegründet worden war. Im Sommer des Jahres hielt er 30 Gastvorlesungen zum Thema „Fünfzig Jahre moderne Kunst“ an der Universität von Hawaii in Honolulu.

1953 kehrten Max Ernst und Dorothea Tanning nach Paris zurück, wo sie ihre zwei Mansardenzimmer am Quai St. Michel wieder bezogen. Er arbeitete als Nachbar von Constantin Brâncuși in einem Atelier auf der Impasse Ronsin Nr. 11, das ihm der US-amerikanische Maler William Copley zur Verfügung gestellt hatte. Das Collège de ’Pataphysique in Paris verlieh ihm im selben Jahr den Titel „Satrap“.

Auf der 27. Biennale von Venedig 1954 wurde Max Ernst mit dem großen Preis für Malerei geehrt, was zum endgültigen Ausschluss aus der Surrealistengruppe führte. Den großen Preis für Grafik in diesem Jahr erhielt Joan Miró, den für Plastik Hans Arp. Die Preisverleihung führte zu internationaler Bekanntheit und weiteren Ehrungen.

1955 zog das Ehepaar nach Huismes und wohnte im Haus „Le pin perdu“, das seit 2009 als „Maison Max Ernst“ zu besichtigen ist. Er behielt diesen Landsitz sowie die Wohnung in Paris, hielt sich jedoch seit 1964 aus gesundheitlichen Gründen mit seiner Frau oft im Süden Frankreichs in Seillans im Département Var in der Provence auf. 1958 wurde er französischer Staatsbürger.

Die Ehrenbürgerschaft, die Ernsts Geburtsstadt Brühl ihm im Jahr 1966 zum 75. Lebensjahr anbot, lehnte er nachtragend ab, da die Stadt sein geschenktes Bild "Die Geburt der Komödie" (1947) im Jahr 1951 für 800 Dollar verkauft hatte. Zudem spielte die schlechte Behandlung von Karl Seibt, dem städtischen Mitveranstalter der finanziell defizitären Max-Ernst-Ausstellung von 1951, durch die Stadt eine Rolle. Im Zusammenhang mit seiner Ablehnung machten ihm der "Kölner Stadt-Anzeiger" sowie private Bekannte den Vorwurf, er hätte seine erste Frau, Luise Straus-Ernst, die Jüdin war und 1944 in Auschwitz ermordet wurde, retten können; Ernst habe es aber abgelehnt, sie ins amerikanische Exil mitzunehmen. Der Künstler bezog daraufhin im "Stadtanzeiger" mit einem Brief Stellung und stellte klar, dass seine erste Frau die ihr angebotene Wiederheirat abgelehnt habe, obwohl sie die Möglichkeit eröffnet hätte, sie mitzunehmen.

1967 schuf er einen Brunnen für die Stadt Amboise, der 1968 als „Fontaine Max Ernst“ eingeweiht wurde. Der Streit zwischen dem Künstler und seiner Heimatstadt Brühl wurde 1971 beendet. Die Stadt vergibt seit diesem Jahr den Kunstpreis „Max-Ernst-Stipendium“ für junge, noch in der Ausbildung befindliche Künstler, und Ernst stiftete die Figuren für den Brunnen, der vor dem Brühler Rathaus steht.

Im Jahr 1975 hielt Max Ernst sich nach einem Schlaganfall wieder in Paris auf. Er starb einen Tag vor seinem 85. Geburtstag, am 1. April 1976 in Paris und wurde nach seiner Einäscherung im Kolumbarium des Friedhofs Père Lachaise begraben. Seine Frau Dorothea Tanning überlebte ihn um mehr als drei Jahrzehnte. Sie starb im Januar 2012 im Alter von 101 Jahren in New York. Der Sohn aus erster Ehe, Jimmy Ernst, starb am 6. Februar 1984 in New York. Seine Memoiren, "A Not-So-Still Life" (deutsch: "Nicht gerade ein Stilleben. Erinnerungen an meinen Vater Max Ernst"), wurden kurz vor seinem Tod veröffentlicht.

1946 gewann Max Ernst den Bel-Ami-Wettbewerb mit seinem Gemälde "Die Versuchung des Heiligen Antonius", und 1954 auf der 27. Biennale von Venedig erhielt er den großen Preis für Malerei. 1958 wurde er Mitglied der Akademie der Künste in Berlin, und 1961 veranstaltete das New Yorker Museum of Modern Art eine Max-Ernst-Retrospektive. Im selben Jahr erhielt er die Stefan-Lochner-Medaille der Stadt Köln. 1964 verlieh die Regierung Nordrhein-Westfalens ihm die Ehrenprofessur, und in Hamburg wurde er mit dem Lichtwark-Preis geehrt.

1966, im 75. Lebensjahr, folgte die Ernennung zum Offizier der Ehrenlegion. 1972 wurde ihm die Ehrendoktorwürde der Universität Bonn verliehen. Im Alter von 84 Jahren reiste Ernst 1975 zur Eröffnung der Max-Ernst-Retrospektive im Solomon R. Guggenheim Museum in New York. Im selben Jahr öffnete die Retrospektive im Grand Palais in Paris ihre Türen. 1976 wurde ihm postum der Goslarer Kaiserring verliehen, einer der international bedeutendsten Kunstpreise.

In der Bundesrepublik Deutschland gibt es einige Straßen, die nach Max Ernst benannt sind, beispielsweise in Köln und Bonn. Seine Heimatstadt Brühl hat ihm die Max-Ernst-Allee gewidmet. Die Stadt Paris benannte im Jahr 1990 die rue Max Ernst nach ihm, sie liegt im 20. Arrondissement im Quartier du Père-Lachaise.

Max Ernst erneuerte die Sprache der Malerei des 20. Jahrhunderts auf revolutionäre Weise. Als Autodidakt ohne akademische Ausbildung wandte er sich gegen die alte Auffassung von „Talent“ und lobte den Surrealismus, der das „Märchen vom Schöpfertum“ angegriffen und vernichtet habe. Der von ihm sehr geschätzte Maler der Romantik, Caspar David Friedrich, hatte seine malerischen Zeitgenossen aufgerufen: „Schließe dein leibliches Auge, damit du mit dem geistigen Auge zuerst sehest dein Bild, Dann fördere zutage, was du im Dunkeln gesehen, dass es zurückwirke auf Andere, von außen nach innen“. Max Ernst formulierte es ähnlich: „Die Rolle des Malers besteht darin, einzukreisen und zu projizieren, was er in sich selbst sieht“.

Ernsts Frühwerk ist durch vielfältige Einflüsse geprägt. So orientierte er sich bereits 1909 an Vincent van Gogh (Beispiel: "Landschaft mit Sonne") und war beeindruckt von Georges Seurat, Henri Matisse, August Macke, Wassily Kandinsky sowie vom Futurismus. Kriegserfahrungen verarbeitete er in starkfarbigen Aquarellen. Das Ziel des stilistischen Wechselspiels war die bewusste Synthese der unterschiedlichen Gattungen, Richtungen und Epochen: Kombinatorik wurde zu seinem Gestaltungsprinzip. Doch ließ er sich nicht nur von Werken der Avantgarde und alter Meister, sondern auch von Volkskunst und Bauernmalerei inspirieren.

Mit der Verwendung der Collage schuf Ernst bereits in seiner dadaistischen Phase nach dem Ersten Weltkrieg von 1919 bis 1921 neben einigen Materialcollagen eine große Anzahl von Werken aus Druckmaterialien, in denen er fremdes Material, beispielsweise Zeichnungen aus Katalogen, aus Lehrbüchern aller Wissenschaftsbereiche oder Häkelvorlagen ausschnitt, verwirrend kombinierte und neu zusammenklebte oder übermalte.

Der Künstler übernahm von Giorgio de Chirico, dem Hauptvertreter der Pittura metafisica, frühzeitig die Idee, in der Malerei Bildmotive in rätselhaften Kombinationen zusammenzustellen; diese Idee wurde später von den Surrealisten übernommen. Sie entsprach der freien Assoziation, die Sigmund Freud anwendete, um das Unbewusste in den Gedanken seiner Patienten, das sich in Träumen äußerte, freizulegen. 1919 entstand, angelehnt an die Pittura metafisica, das Gemälde "Aquis submersus". Der Titel orientiert sich an Theodor Storms gleichnamiger Novelle. Eine der bekanntesten Collagen aus der Kölner Zeit ist "Der Hut macht den Mann" aus dem Jahr 1920, bei dem es sich ursprünglich um eine Druckvorlage für einen Werbekatalog seines Schwiegervaters handelte, der eine Hutfabrik besaß. "Celebes", geschaffen 1921 in Köln, gehört als erstes Werk zur Gruppe der Gemälde, die Max Ernst zwischen 1921 und 1924 in der Zeit des Übergangs zwischen Dada und Surrealismus malte. Das Gruppenporträt "Das Rendezvous der Freunde" entstand 1922 nach Ernsts Übersiedlung nach Paris

André Breton veröffentlichte 1924 sein erstes surrealistisches Manifest in Paris. Es galt als Beginn der Bewegung. Der von den Surrealisten oft zitierte Satz „Schön wie die zufällige Begegnung einer Nähmaschine und eines Regenschirms auf dem Seziertisch“ entstammt den "Gesängen des Maldoror" (1868/69) von Lautréamont. Max Ernst variierte ihn:

Die provokante Darstellung des Gemäldes "" aus dem Jahr 1926 soll durch André Breton inspiriert worden sein. Ab 1925 begann Max Ernst mit den künstlerischen Techniken der Frottage für Grafiken und der Grattage für Gemälde zu arbeiten. Die Techniken und Werkbeispiele werden in dem Abschnitt „Max Ernsts Erfindungen“ näher beschrieben. Ab 1927 entstanden erste Bildserien seiner Vogel- und Waldmotive, die sein ganzes Werk durchziehen. Sein erster Collageroman "La femme 100 têtes" nach Illustrationen aus Trivialromanen des 19. Jahrhunderts mit einer Einleitung von Breton erschien im Jahr 1929 bei den Éditions du Carrefour in Paris.

Ernsts Alter Ego, der Vogel "Loplop", zieht sich in unterschiedlichen Techniken und Varianten durch sein Werk. Er stellte sich selbst in verschiedenen anthropomorphen Formationen dar, immer auf zwei Beinen stehend, und der Rumpf dient wie eine Staffelei als Präsentationsfläche für die Darstellung unterschiedlicher Motive. Er kombiniert verschiedene malerische Techniken mit der Collage. Ein Beispiel ist "Loplop stellt Loplop vor" aus dem Jahr 1930. In der Reihe "Loplop présente …" ab den 1930er Jahren wie "Loplop présente la belle saison", "Loplop présente deux fleurs" oder "Loplop présente le portrait de Marie-Berthe" entstanden überwiegend Bleistiftfrottagen.

Ab 1935 entstand ein Hauptwerk in der Technik der Grattage, "Die ganze Stadt": Es zeigt rätselhafte, ruinenartige, an Festungen oder Tempelanlagen erinnernde Gebäude als versteinerte Stadt, in der kein Leben mehr zu erkennen ist. Der Künstler begann ab 1937 mit der Technik der Décalcomanie zu arbeiten. "Die Einkleidung der Braut" aus dem Jahr 1940 zeigt eine halbnackte Frau mit Vogelkopf in einem roten bodenlangen Umhang. Der Vogel zur Linken, eine Speerspitze haltend, kann als Verkörperung des Künstlers gesehen werden, während die Braut seine Geliebte Leonora Carrington darstellen könnte. 1940/41 entstand ebenfalls "Marlene"; die abgebildete Frau, umgeben von Vogelwesen, hat die Gesichtszüge von Marlene Dietrich, die 1937 in die USA ausgewandert war, und als Sujet vermutlich auf seine eigene Flucht hinweist. Das Bild "Europa nach dem Regen II" als ein Höhepunkt des Arbeitens in dieser Technik begann er 1940 in Frankreich und vollendete es nach der Emigration 1942 in New York. Es zeigt eine Landschaft, die von Verwüstung und Tod geprägt ist und dessen Titel Gedanken an die materiellen und geistigen Vernichtungen des Krieges hervorruft.

1942 entwickelte er die Technik des Drip Painting oder Oszillation, mit der er nur wenige Bilder wie beispielsweise "Der verwirrte Planet" schuf. Der Automatismus, den die Oszillation bot, regte junge amerikanische Künstler zur Nachahmung an, und besonders Jackson Pollock widmete sich dem Ausbau und der Verfeinerung dieser Technik. Ernst erklärte, sie sei ein Kinderspiel:

Die erneuerte Verbindung zur surrealistischen Gruppe dokumentiert eines seiner Hauptwerke, das Gemälde "Le Surréalisme et la peinture" ("Der Surrealismus und die Malerei", 1942), dessen Titel von Bretons gleichnamigem Essay aus dem Jahr 1928 stammt. Eine Art Vogelgestalt, dargestellt mit ihrer Brut, malt an einem auf der Staffelei stehenden Bild, das in Oszillationstechnik angelegt ist.

Im Jahr 1943 schuf Ernst "Vox Angelica"; das Gemälde ist 152 × 205 cm groß und aus 51 größeren und kleineren Einzelbildern zusammengesetzt. Er malte es in den verschiedenen von ihm benutzten Techniken. Viele Experten sehen in diesem Werk einen Wendepunkt in seinem Schaffen, von dem aus sich eine hellere und einfachere Bildsprache durchsetzte. 1945 entstand als Höhepunkt der New Yorker Zeit "Die Versuchung des heiligen Antonius" sowie ein mathematisches Thema, das Porträt des "Euklid". 1948 schuf er das "Festmahl der Götter", und um 1948 malte er "Arizona desert after Rain", das die Umgebung des Hauses in Sedona zeigt, wo er seine Großplastik "Capricorn" herstellte.

1953 kehrte Max Ernst aus den USA nach Frankreich zurück. Die Rückkehr markiert zugleich den Anstieg seiner grafischen Produktion. Sie wurde in einer Ausstellung im Sprengel Museum, Hannover im Jahr 2012 unter dem Titel "Max Ernst. Lorbeeren und Erdbeeren" gezeigt. Illustrationen aus den 1960er Jahren folgten Plakate aus den 1970er Jahren. Ausschnitte aus seinem Collagenroman "Lieux communs: Onze poèmes et douze collages" aus dem Jahr 1971 waren als Siebdrucke präsent. Die asiatisch anmutende Lithografie "Grand Palais" entstand 1975, ein Jahr vor Ernsts Tod.

An Gemälden entstanden etwa "Das Floß der Medusa" (1955), "Der Garten Frankreichs" (1962) und "Rückkehr der schönen Gärtnerin" (1967), letzteres als Reminiszenz an das verschollene Bild "Die schöne Gärtnerin" von 1923, sowie "Nichts geht mehr" (1973). Im Gemälde "Der Garten Frankreichs" "(Le jardin de la France)" hatte der Künstler eine Kopie der "Geburt der Venus" von Alexandre Cabanel so übermalt, dass die erotischen Zonen des Körpers einer kopflosen Frau – zwischen zwei Flussläufen dargestellt – dominieren.

Ernsts Werk widmete sich auch in den späten Jahren nicht der Gegenstandslosigkeit, wie der Informellen Kunst, die in dieser Zeit häufig vertreten war. Er kommentierte diese Tendenzen mit: „Totale Ablehnung, wie ein Tachist zu leben.“

Max Ernst erfand und benutzte verschiedene halbautomatische Techniken, mit denen er Strukturen erzeugte, die seine Imaginationskraft beflügelten.

Selbst erfundene Techniken:

Weitere benutzte Techniken:

Zu jedem Geburtstag seiner vierten Ehefrau Dorothea Tanning hatte Max Ernst eine Liebeserklärung geschaffen, um seine Verbundenheit mit ihr zu bezeugen. Die 36 "D-Paintings" zu Geburtstagen und zusätzlich zur Hochzeit und Silberhochzeit sind seit 2005 Bestandteil des Max-Ernst-Museums in Brühl. In jedem Werk ist der Buchstabe „D“ enthalten.

Max Ernst illustrierte einige Werke Lewis Carrolls: "La chasse au snark" (1950, 8 Illustrationen), "Logique sans peine" (1966), "The Hunting of the Snark (Die Jagd nach dem Snark)" (1968, 32 Farblithografien) und "Lewis Carrolls Wunderhorn" (1970, eine Anthologie, ausgewählt von Max Ernst und Werner Spies mit 36 Farblithografien). Unter den Surrealisten fühlten sich besonders Ernst und Salvador Dalí durch Carrolls Kinderbuch "Alice im Wunderland" in ihrer Suche nach dem Phantastischen bestärkt. Die Hamburger Kunsthalle zeigte ab Juni 2012 die Ausstellung "Alice im Wunderland der Kunst". Ernsts Gemälde "Alice in 1941" war Bestandteil der Ausstellung.

Max Ernst schuf drei sogenannte „Collageromane“. Der erste war "La femme 100 têtes," erschienen Ende 1929 mit einem Vorwort von André Breton. 1930 folgte "Rêve d’une petite fille qui voulut entrer au Carmel" und 1934 "Une semaine de bonté." Die Collageromane, die teilweise mit Textteilen versehen sind, basieren auf Holzstichen des 19. Jahrhunderts, unter anderem von Gustave Doré. Vor allem die beiden ersten Romane sind vielfach interpretierbar, sie suggerieren durch Bildfolge und -legenden einen zusammenhanglosen Handlungsablauf. Inhaltlich wird durch die Wiederholung bestimmter Motive in Text und Bild die durch die Visionen des Surrealismus ermöglichte Befreiung aus gesellschaftlicher, staatlicher oder kirchlicher Unterdrückung angedeutet. Die Collagenbilder aus "Une semaine de bonté" wurden erstmals nach 1936 im Jahr 2008 in der Albertina in Wien gezeigt. Die Ausstellung wurde anschließend vom Max-Ernst-Museum in Brühl und der Hamburger Kunsthalle übernommen.

1922 erschien das Malerbuch "Les malheurs des immortels" in der Librairie Six (Paris) mit Gedichten von Paul Éluard und Max Ernst sowie 21 Illustrationen Ernsts. 1926 folgte "Histoire naturelle" mit 34 Frottagen und einem Vorwort von Hans Arp, veröffentlicht in der Galerie Jeanne Bucher, Paris. Es enthielt die ersten Frottagen des Künstlers. Ernst berief sich zu ihrer Entstehung ausdrücklich auf Leonardo da Vinci, in dessen "Traktat über die Malerei" stand, man „könne den Geist zu mannigfaltigen Erfindungen anregen, indem man sich in fleckiges Gemäuer hineinsähe, es ließen sich darin womöglich Landschaften, Schlachten und seltsame Gestalten ausmachen“. Ernst bemerkte dazu:

1953 erschien "Das Schnabelpaar. Acht Radierungen in Farben und ein Gedicht von Max Ernst" im Verlag Ernst Beyeler. Originalausgabe als Kassette, gedruckt bei Lacourière in Paris, Umschlag aus bedrucktem Pergament.

Das Buch aus dem Jahr 1964, "Maximiliana ou l’exercice illégal de l’Astronomie," gilt als Hauptwerk der späten druckgrafischen Arbeiten des Künstlers. Der Name geht auf den Lithographen und lange verkannten autodidaktischen Astronomen Ernst Leberecht Tempel (1821–1889) zurück, der 1861 den Asteroiden (65) Cybele entdeckte und ihn "Maximiliana" nannte. Ernsts und Tempels Lebenswege zeigen viele Gemeinsamkeiten auf, so wie Ernst besaß der Hobby-Astronom „kein Diplom“. Der Vorname des Astronomen war wie der Nachname des Künstlers „Ernst“, und „Maximilian“ war Ernsts Taufname. Das Werk enthält lose Doppelblätter mit 34 Radierungen und 14 „Schrift-Blätter“ von Max Ernst, teilweise farbig und mit Aquatinta ausgeführt. Die Radierungen sind mit typographisch gestaltetem Text nach Ernst Leberecht Tempel und der „Geheimschrift“ von Ernst ergänzt. Das Format beträgt 43 × 32 × 3 cm. Die Typographie stammt von Iliazd, der das Buch in Paris verlegte. 1967 folgte ein Film über das Thema von Peter Schamoni unter Mitwirkung von Ernst (Siehe Rezeptionsteil).

Max Ernst erläuterte sein Verhältnis zur Bildhauerei gegenüber seinem frühen Biografen Patrick Waldberg:
Max Ernst begann 1934 anlässlich seines Besuchs bei Alberto Giacometti in Maloja, sich mit Bildhauerei zu befassen. Doch blieb er vor dem Zweiten Weltkrieg lange der einzige surrealistische Bildhauer, da sich Giacometti bereits ein Jahr später zur Rückkehr zum Modell entschieden hatte und Hans Arp trotz seiner Beteiligung an Dada zur Abstraktion und den Ideen Brâncușis neigte. Ernst benutzte wie bei der Malerei improvisierte Techniken: "Oedipus" (1934 in zwei Fassungen) entstand zum Beispiel mit seinen zwei phallischen Formen aus Gussformen von Sandeimern, die in verschiedenen Kombinationen übereinander platziert wurden. Ein weiteres frühes Werk ist "Habakuk" (1934), das in mehreren Bronzegüssen existiert; einer steht vor der Kunsthalle Düsseldorf. Sein Hauptwerk ist die Zementplastik "Capricorn", geschaffen 1948 in Sedona, von der nur noch Fragmente vorhanden sind. Das Gipsmodell ist in der Nationalgalerie Berlin zu sehen, mehrere Bronzegüsse erfolgten ab den 1960er Jahren. Neben Skulpturengruppen wie "Corps enseignant pour une école de tueurs" (1967) fassen die Figurenkonstellationen für die Brunnen in Amboise (1968) und Brühl (1971) das plastische Spätwerk zusammen. Eine Besonderheit entstand 1966: Der passionierte Schachspieler schuf ein Schachspiel aus Glas, das er "Immortel" nannte. 

Reliefs und Plastiken entstanden an seinen Häusern in Saint-Martin-d’Ardèche, Sedona, Huismes und Seillans.

Neben kunsttheoretischen Schriften wie "Was ist Surrealismus" aus dem Jahr 1934 und "Au-delà de la peinture" von 1937 verfasste Max Ernst auch autobiografische Texte. 1948 erschien "Some Data on the Youth of Max Ernst" in dem Sammelband "Beyond Painting", 1962 seine Autobiografie "Biographische Notizen (Wahrheitsgewebe und Lügengewebe)" als Bestandteil des Ausstellungskatalogs der Retrospektive im Kölner Wallraf-Richartz-Museum. Der Untertitel „Wahrheitsgewebe und Lügengewebe“ ist offensichtlich eine Anspielung auf Goethes Autobiografie "Dichtung und Wahrheit". Ernst variierte und ergänzte den Text später mehrfach. Er verfasste seinen Lebenslauf, in dem er von sich in der 3. Person spricht, nach surrealistischen Maßstäben und lenkt die Aufmerksamkeit auf seine frühe Kindheit, Mythen und Märchen, religiöse Begegnungen und psychische Besonderheiten. Mit dem Untertitel „Lügengewebe“ verweist der Künstler darauf, gelegentlich poetische Auslegungen und Koppelungen von Ereignissen mit seinen Bildern hinzugefügt zu haben, die auch chronologisch nicht stimmen. Möglicherweise wollte er mit seinen Aufzeichnungen die Selbstdarstellungen großer Männer ironisieren oder das Publikum, das sich durch seine Bilder beunruhigt fühlte, besänftigen.

André Breton schrieb 1928 in "Le Surréalisme et la Peinture", Max Ernst sei „nie abgewichen von dem gebieterischen Bedürfnis, wie es Baudelaire forderte: unaufhörlich ‚Neues zu finden‘. Von ständig wachsender Kraft in diesen letzten zwanzig Jahren, findet sein Werk, was einen solchen Willen anlangt, nicht seinesgleichen“.

Max Ernsts Freund, der Schriftsteller Paul Éluard, wies 1948 in "A L’Intérieur de la vue" darauf hin, dass der Künstler sich selbst identifiziert, mit dem, was er zeigt. „Indem er seine Vision über diese grobe, gefühllose Wirklichkeit, die wir gelassen hinnehmen sollen, hinausprojiziert, führt er uns in eine befreite Welt, wo wir allem zustimmen, wo nichts mehr unverständlich ist.“ In Bezug auf Ernsts Alter Ego schrieb er poetisch: „Von Federn verschlungen und dem Meer überlassen, hat er seinen Schatten in Flug übersetzt, in den Flug der Vögel der Freiheit.“

In den 1940er Jahren kam es in den Vereinigten Staaten zu einem künstlerischen Umschwung, der nach Jahren des sozialkritischen Amerikanischen Realismus zu einem Neubeginn – „dem Wunsch, bei Null anzufangen“ (Barnett Newman) – führte, dem abstrakten Expressionismus. Künstler wie Barnett Newman, Jackson Pollock, Mark Rothko, Clyfford Still und Franz Kline fanden ihre Vorbilder in Miró, Masson und Max Ernst, in deren kontrolliertem psychischen Automatismus, beim gelenkten Zufall, nicht unbedingt in deren Techniken. Pollock übernahm von Max Ernst die Technik der „Oszillation“ als Drip Painting, baute sie aus und wurde durch diese Technik bekannt. Er erhielt daraufhin den Spitznamen „Jack the Dripper“.

Der Kunsthistoriker Werner Spies, der Max Ernst 1966 kennengelernt hatte, zahlreiche Schriften über ihn verfasste und den Werkkatalog herausgab, äußerte 2005 in einem Interview: „Man kann sagen, dass es den Surrealismus ohne Max Ernst in dieser Form überhaupt nicht gegeben hätte. Und der ganze poetische und ikonografische Fundus, den das Werk von Max Ernst beinhaltete, basierte sehr stark auf der deutschen Romantik, sowohl was die literarische Tradition angeht, als auch die der bildenden Kunst“. Er fuhr fort, indem er an die Feindseligkeit erinnerte, die nach dem Ersten Weltkrieg zwischen Deutschen und Franzosen entstanden war. In dieser Situation habe Max Ernst das Glück gehabt, in Paris in den 1920er Jahren mit Bréton, Éluard, Aragon und Tzara Künstler getroffen zu haben, die sich nicht für die nationale Herkunft eines Menschen interessiert hätten. Éluard etwa habe zu jenen gehört, die den Deutschen noch kurz zuvor im Schützengraben von Verdun gegenübergelegen hätten. Die Pariser Künstler hätten in Max Ernst einen wirklichen Geistesverwandten entdeckt, vereint durch die Poesie, die Sucht nach der Überwindung der Kausalität und des Pragmatischen.

Max Ernst war in seinem Leben häufig mit Filmen befasst, so 1930 als Darsteller in "L'Âge d'Or"; weitere Filme sind unter Filmografie aufgelistet. Der Regisseur Peter Schamoni drehte 1963 in Huismes, wo Ernst seit 1955 lebte, einen Film zu Ehren des Künstlers mit dem Titel "Max Ernst – Entdeckungsfahrten ins Unbewusste". 1967 folgte ein weiterer Kurzfilm von Schamoni, der in Seillans gedreht wurde. Er trägt den Titel "Die widerrechtliche Ausübung der Astronomie", ist Max Ernst gewidmet, der nicht nur das Drehbuch über das Schicksal des Amateurastronomen Ernst Wilhelm Leberecht Tempel (1821–1889) mitschrieb, sondern auch als Sprecher im Film fungierte. „Er hatte Genie, aber kein Diplom“, so fasste es Ernst zusammen, eine Affinität zu seinem eigenen Leben. Zum 100. Geburtstag des Künstlers drehte Schamoni 1991 einen weiteren Film, "", der sich mit Ernsts Leben und Werk befasst.

Schon seit 1971, fünf Jahre vor seinem Tod, vergibt die Stadt Brühl mit damaliger Zustimmung des Künstlers, das Max-Ernst-Stipendium zur Förderung junger Künstlerinnen und Künstler. 1981, fünf Jahre nach seinem Tod, wurde das Städtische Gymnasium in seiner Geburtsstadt Brühl in Max-Ernst-Gymnasium umbenannt. In Köln trägt eine Gesamtschule seinen Namen. Im Jahr 2000 wurde die Max Ernst Gesellschaft gegründet. Ihrem großen "Sohn der Stadt" zu Ehren eröffnete die Stadt Brühl 2005 das neue Max Ernst Museum. Vorsitzender des Stiftungsrates und des Kuratoriums der Stiftung Max Ernst war Werner Spies. In Ernsts nahe gelegenen Geburtshaus, das 2009 erworben wurde, eröffnete die Stadt am 1. Juli 2012 ein dem Museum angeschlossenes "Fantasie Labor" mit einem angrenzenden Erweiterungsbau, das der Allgemeinheit und Schulen zur Verfügung steht.

Die Verbreitung des Werks von Max Ernst in den USA wurde maßgeblich von dem Galeristen Alexander Iolas gefördert, der ab den 1930er/1940er Jahren auf surrealistische Kunst setzte und neben Ernst auch René Magritte führte.

Das Auktionshaus Christie’s in New York erzielte im November 2011 einen Weltrekord für ein Werk Max Ernsts. Statt geschätzter vier bis sechs Millionen Dollar fiel der Hammer für "The Stolen Mirror" aus dem Jahr 1941 bei 16,3 Millionen Dollar. Ein anonymer Käufer aus Europa ersteigerte das nur 65 × 81 cm große Ölgemälde. Es kombiniert eine wie im Traum gemalte Landschaft mit statuenhaften Frauenakten, umhüllt von Tieren und Pflanzen. "The Stolen Mirror" gehörte zunächst dem Surrealismus-Sammler Edward James. Dann kaufte es Ernsts Sohn Jimmy Ernst zurück, und es verblieb in der Familie. Die gesicherte Provenienz ist bedeutungsvoll, da es um den ebenfalls Max Ernst betreffenden Fälschungsskandal der fingierten Sammlung Werner Jägers negative Berichte gab. Zwischen 1999 und 2004 wurden sieben von Werner Spies für echt erklärte Bilder im Jahr 2011 in einem Prozess als Fälschungen Wolfgang Beltracchis entlarvt, darunter "La Mer" und "La Forêt II". Möglicherweise gibt es noch mehr gefälschte Werke von Max Ernst.







"Kataloge und Werkbeschreibungen"

"Darstellungen"

"Belletristik"



</doc>
<doc id="10072" url="https://de.wikipedia.org/wiki?curid=10072" title="Bundesrat (Schweiz)">
Bundesrat (Schweiz)

Der Bundesrat (, , ) ist die Bundesregierung der Schweizerischen Eidgenossenschaft und gemäss der Bundesverfassung die "". 

Die einzelnen Mitglieder des Rates werden ebenfalls "Bundesrat" genannt; falls es aus sprachlichen Gründen nötig ist, zwischen der Behörde und dem Ratsmitglied zu unterscheiden, heisst erstere auch «Gesamtbundesrat».
Als weibliche Form ist heute «Bundesrätin» üblich.

Als Kollegialbehörde ( BV) unterscheidet sich der Bundesrat teils erheblich von den Regierungen anderer demokratischer Staaten. Er besteht aus sieben gleichberechtigten, von der Vereinigten Bundesversammlung fest auf vier Jahre gewählten Mitgliedern. Der Bundesrat als Ganzes (und nicht der Bundespräsident) übt auch die protokollarischen Funktionen aus, die in anderen Ländern dem Staatsoberhaupt obliegen (ein solches Amt sieht die Bundesverfassung nicht vor).

Bei der Schaffung des Bundesrates dienten das französische Direktorium der Revolutionszeit und antike griechische Behörden (Archonten) als Vorbild. Die Schweiz ist das einzige Land der Welt, welches dieses System angepasst und anstelle eines Westminster-Systems oder eines präsidentiellen Regierungssystems als Regierungsform übernommen hat. Es stellt somit politikwissenschaftlich ein Direktorialsystem dar.

Die sieben Mitglieder des Bundesrats werden durch den Bundeskanzler oder die Bundeskanzlerin unterstützt. Dieser bzw. diese leitet die Schweizerische Bundeskanzlei (BK), die Stabsstelle des Bundesrates.

Nach der Wahl von Simonetta Sommaruga in den Ersatzwahlen 2010 erhielt der Bundesrat erstmals eine Frauenmehrheit, die er mit der Wahl Alain Bersets als Nachfolger der nicht mehr kandidierenden Micheline Calmy-Rey auf 1. Januar 2012 wieder verlor.
Die Mitglieder des Bundesrates werden von der Vereinigten Bundesversammlung mit absolutem Mehr gewählt. Verschiedene Versuche zur Einführung der Volkswahl des Bundesrates blieben bisher erfolglos. Jeweils in der ersten Session des neu gewählten Nationalrates, also zu Beginn seiner vierjährigen Legislaturperiode, findet eine Gesamterneuerungswahl des Bundesrates statt. Dazwischen werden jährlich von der Vereinigten Bundesversammlung aus den Bundesratsmitgliedern der Präsident und der Vizepräsident des Bundesrates für das kommende Jahr bestimmt. Falls ein einzelner Bundesrat vor Ablauf der Amtszeit zurücktritt, wird ein Nachfolger gewählt, der aber nur bis zur nächsten Gesamterneuerungswahl gewählt ist. In der zweiten Hälfte des 19. Jahrhunderts war die Komplimentswahl üblich: Amtierende Bundesräte kandidierten als Nationalräte, um sich ihre Legitimation als Regierungsmitglieder durch die Stimmberechtigten bestätigen zu lassen; erst danach folgte die Wiederwahl durch die Bundesversammlung.

Wählbar ist grundsätzlich jeder stimmberechtigte Schweizer Bürger. Bei jeder Wahl melden sich einige Bewerber aus dem «gewöhnlichen Volk». Im Laufe der Geschichte hat sich jedoch ein nicht leicht darzustellendes Wahlverfahren mit zahlreichen geschriebenen und ungeschriebenen Regeln entwickelt, dessen Ziel eine möglichst «gerechte», ausgewogene Vertretung der Bevölkerung im Sinne der schweizerischen Konkordanzdemokratie ist.

Das Verfahren richtet sich nach BV und , , , und ParlG.

Da ein parlamentarisches Misstrauensvotum in der Verfassung nicht vorgesehen ist, können Bundesräte während der Legislaturperiode nicht abgesetzt werden. Auch eine Nichtwiederwahl eines amtierenden Bundesrates ist nicht üblich und geschah seit 1848 erst viermal, in jüngster Zeit wurden am 10. Dezember 2003 Bundesrätin Ruth Metzler-Arnold und am 12. Dezember 2007 Bundesrat Christoph Blocher nicht wiedergewählt. Daraus ergibt sich auch eine sehr lange Amtsdauer der Bundesräte (rund zehn Jahre im Durchschnitt). Der längstdienende Bundesrat war Karl Schenk von 1864 bis 1895; die längstdienenden Bundesräte im 20. Jahrhundert waren Giuseppe Motta von 1911 bis 1940 und Philipp Etter von 1934 bis 1959.

Bis 2009 war auch das Vorgehen im Falle einer dauernden Handlungsunfähigkeit eines Bundesrates nicht geregelt. Als Bundesrat Jean Bourgknecht im Mai 1962 einen Schlaganfall erlitt, wurde das damit entstandene Problem der Amtsunfähigkeit eines Mitgliedes des Bundesrates ad hoc gelöst, wenn auch aus heutiger Sicht auf rechtlich problematische Art und Weise, indem drei Familienangehörige des Bundesrates am 3. September 1962 in seinem Namen den Rücktritt erklärten. Diese Lücke wurde erst nach einer parlamentarischen Initiative von 2005 mit der Revision des Parlamentsgesetzes vom 3. Oktober 2008 (Inkrafttreten am 2. März 2009) geschlossen. Dessen legt nun fest, dass im Falle einer voraussichtlich langandauernden Amtsunfähigkeit eines Mitglieds des Bundesrates infolge schwerwiegender gesundheitlicher Probleme oder Einwirkungen, die ihn daran hindern, an seinen Arbeitsplatz zurückzukehren, die Vereinigte Bundesversammlung auf Antrag des Büros derselben oder des Bundesrates die Amtsunfähigkeit feststellt.

Gemäss dem Parlamentsgesetz kann ein Bundesrat nicht zugleich Mitglied des Parlaments sein. Trotzdem pflegen die Bundesräte üblicherweise einen regen Kontakt mit der Fraktion ihrer Partei und nehmen an den Fraktionssitzungen mit beratender Stimme teil, dürfen aber – im Gegensatz zu Fraktionsmitgliedern – weder Anträge stellen noch abstimmen.

Die sieben Bundesräte beschliessen eigentlich mit Mehrheitsentscheiden gemeinsam über alle Geschäfte, aber in der Praxis stehen sie als "«Departementsvorsteher»" je einem Bereich (Departement) der Bundesverwaltung vor (Departementalprinzip). Insofern sind sie vergleichbar mit Ministern anderer Länder; umgangssprachlich beziehungsweise in den Medien sind für die jeweils betroffenen Departementsvorsteher je nach Thema auch Bezeichnungen wie «Energieministerin X» oder «Sportminister Y» üblich. Einen Regierungschef mit Richtlinienkompetenz gibt es aber nicht. Da sich die Zuständigkeit des Gesamtbundesrates auf sämtliche Geschäfte aller Departemente erstreckt, hat jeder Bundesrat über sein Departement hinaus erhebliche Mitsprache- und Einflussmöglichkeiten.

Die Verteilung der Departemente wird jeweils nach der Bundesratswahl durch die Bundesräte selber vorgenommen, es gibt kein Mitwirkungsrecht des Parlaments. Dabei wird nach dem "«Anciennitätsprinzip»" vorgegangen: Der amtsälteste Bundesrat nennt zuerst sein Wunsch-Departement, anschliessend der zweitälteste und so weiter. Neu gewählten Mitgliedern des Bundesrates wird das verbleibende Departement angeboten. Falls sich die Mitglieder des Bundesrates nicht auf eine Zuteilung der Departemente einigen können, kann eine Abstimmung den Entscheid herbeiführen.

Die sieben Mitglieder des Bundesrats werden durch den Bundeskanzler oder die Bundeskanzlerin unterstützt. Dieser bzw. diese leitet die Schweizerische Bundeskanzlei (BK), die Stabsstelle des Bundesrates.

Der Bundesrat setzt sich zurzeit wie folgt zusammen:

Ein bedeutsamer Unterschied zwischen Regierungsmitgliedern anderer Länder und den Schweizer Bundesräten besteht darin, dass ein Bundesrat auch Teil des kollektiven Staatsoberhauptes ist und dass es keinen Regierungschef mit Weisungsbefugnis oder Richtlinienkompetenz gibt. Dazu kommt die Tatsache, dass ein Bundesrat auf eine Periode von vier Jahren fest gewählt ist. Der Bundespräsident hat den übrigen Bundesräten selbst im äussersten Fall nur den Stichentscheid bei einer sonst unentschiedenen Abstimmung im Gesamtbundesrat voraus.

Die vom Kollegium mit Mehrheitsentscheid gefassten Bundesratsbeschlüsse müssen in der Folge vom zuständigen Departementsvorsteher vor Parlament und Öffentlichkeit auch dann vertreten werden, wenn dieser den getroffenen Entscheid eigentlich ablehnt "(Kollegialitätsprinzip)". Dabei regelt die Bundesverfassung nur die Form der Entscheidungsfindung ( BV: ""), ohne sich zur Handhabung des Prinzips sonst, insbesondere zum Verhalten der Mitglieder des Bundesrats nach getroffenen Entscheiden, weiter zu äussern. Von alters her ist es als ausnahmsweise zulässig betrachtet worden, dass ein Bundesrat eine von einem Bundesratsbeschluss abweichende Meinung öffentlich kundtut, wenn er sich auf Gewissensgründe beruft und die Entscheidung nicht unter die Bearbeitung des eigenen Departements fällt. In neuerer Zeit ist jedoch immer öfter zu beobachten, dass einzelne Bundesräte Entscheide des Kollegiums mehr oder weniger offen zu desavouieren versuchen. So werden Sinn und Unsinn des Kollegialitätsprinzips auch immer wieder in den Medien und in politischen Gremien thematisiert.

Die Vereinigte Bundesversammlung wählt jedes Jahr aus den sieben Bundesräten den Bundespräsidenten sowie den Vizepräsidenten des Bundesrates. Gemäss Tradition werden diese Positionen der Reihe nach allen Mitgliedern des Bundesrates übertragen. Ein neues Bundesratsmitglied wird üblicherweise erst zum Vizepräsidenten und anschliessend zum Bundespräsidenten gewählt, nachdem es unter dem Präsidium aller amtsälteren Kollegen gewirkt hat. Der Bundespräsident kann nicht als Staatsoberhaupt oder als Regierungschef der Schweiz bezeichnet werden, da er als erster unter Gleichen (→ "primus inter pares") keine erweiterten Rechte hat. Ihm werden Repräsentationsaufgaben als Stellvertreter des Gesamtbundesrates übergeben, und er leitet die Bundesratssitzungen.

Weil die Schweiz kein Staatsoberhaupt hat, pflegt sie auch keine Staatsbesuche abzustatten. Wenn sich der Bundespräsident ins Ausland begibt, dann tut er dies nur als zuständiger Departementsvorsteher. Jedoch gelten hier auch Ausnahmen. So vertritt der Bundespräsident die Schweiz an Versammlungen von Staatsoberhäuptern (beispielsweise an der Generalversammlung der Vereinten Nationen).

Gemäss der protokollarischen Rangordnung in der Schweiz ist der Bundespräsident der höchste Schweizer. 
In der Schweiz herrscht eine Konkordanzdemokratie. Unter der Konkordanz wird der Wille verstanden, möglichst viele verschiedene Parteien, Minderheiten und gesellschaftliche Gruppen in einen Prozess einzubeziehen und Entscheidungen durch Herbeiführung eines Konsenses zu treffen.

Diese Konkordanz wird bei der Zusammenstellung des Bundesrats vom Parlament berücksichtigt. Die Konkordanz in der Schweiz ist jedoch nicht, wie die Berücksichtigung der verschiedenen Landesgegenden und Sprachregionen, von der Verfassung aufgetragen ( BV: ""), sondern wurde vielmehr während Jahrzehnten zu einer Tradition. Aus dieser Konkordanz hat sich 1959 auch eine sogenannte Zauberformel gebildet. Nach dieser Formel durften die drei wählerstärksten Parteien, SP, FDP und CVP, je zwei und die viertstärkste, SVP, einen Sitz im Bundesrat beanspruchen. Diese Zusammensetzung blieb bis 2003 unverändert. Danach wechselte ein Sitz von der CVP zur SVP, nachdem diese zur wählerstärksten Partei aufgestiegen war. Die neue Zusammensetzung der 2:2:2:1-Formel wurde teilweise «neue Zauberformel» genannt oder weiterhin einfach «Zauberformel». 2008 bis 2015 wich die Zusammensetzung des Bundesrates von der «Zauberformel» ab: Nachdem ihre beiden Bundesratsmitglieder aus der SVP ausgetreten waren, war die SVP, obwohl stimmenstärkste Partei, 2008 zunächst mit keinem, ab Ende 2008 nur mit einem Bundesrat vertreten.

Ein Schweizer Bundesrat erhält ein jährliches Bruttoeinkommen von 445'163 Schweizer Franken (1. Januar 2017) sowie jährliche (nicht steuerpflichtige) Repräsentationszulagen von 30'000 Franken (1. Januar 2017, nicht indexiert). Das Bruttojahreseinkommen wird der Teuerung angepasst, jedoch gibt es keine Reallohnerhöhung. Der Bundespräsident erhält zusätzlich zu den genannten Beträgen eine Entschädigung in Höhe von 12'000 Franken während des einjährigen Präsidialjahres. Entstehende Kosten für Telekommunikation (Festnetz, Mobiltelefon, PC) werden von der Bundesverwaltung übernommen.

Überdies haben die Mitglieder der Landesregierung jeweils Anspruch auf ein Repräsentations- und ein Dienstfahrzeug. Bei Zeitdruck kann für Dienstreisen ein Hubschrauber oder Jet der Schweizer Luftwaffe beansprucht werden. Ein Bundesrat erhält darüber hinaus ein Generalabonnement (GA) der Schweizerischen Bundesbahnen der ersten Klasse und ein GA für die Seilbahnen in der Schweiz. Nach der Tätigkeit als Bundesrat erhält der ehemalige Magistrat eine jährliche Pension von 222'359 Franken (fünfzig Prozent des Einkommens eines amtierenden Bundesrates). Lohn aus einer allfälligen beruflichen Tätigkeit wird dabei angerechnet, sofern sich aus der Summe von Lohn und Rente sonst ein Einkommen ergäbe, das höher ist als das eines amtierenden Bundesrates.

Der Gesamtbundesrat begann im Jahr 2010, manche seiner Sitzungen ausserhalb des Bundeshauses («extra muros») abzuhalten, um die Verbundenheit mit den diversen Landesteilen auszudrücken. So tagte er seither in den Kantonen Tessin, Jura (2010), Uri, Wallis, Basel-Stadt (2011) und Schaffhausen (2012). Per 24. April 2013 besuchte der Bundesrat das Schloss in Prangins im Kanton Waadt; unter anderem fand ein Zusammentreffen mit den Einwohnern von Nyon statt. Am 16. April 2014 hielt der Bundesrat zum achten Mal eine Sitzung «extra muros», nunmehr im Kanton Schwyz, wo ein Treffen mit der Bevölkerung im Mythenforum stattfand. Am 25. April 2015 besuchte der Bundesrat die Bevölkerung des Kantons Freiburg auf dem Rathausplatz. Der Bundesrat führte seine externe Sitzung am 25. April 2016 im Kanton Waadt durch und begegnete der Bevölkerung auf der Place Saint-François. Der Rathausplatz im Kanton Glarus war Ziel des Bundesrates für seine Sitzung am 31. August 2016. Die auswärtige Bundesratssitzung 2017 fand am 29. März im Kanton Solothurn statt, ein Treffen mit der Bevölkerung erfolgte auf dem Kronenplatz.




</doc>
<doc id="10075" url="https://de.wikipedia.org/wiki?curid=10075" title="Raubtiere">
Raubtiere

Die Raubtiere (Carnivora) sind eine Ordnung der Säugetiere (Mammalia), zu der die Hundeartigen (Canoidea) und die Katzenartigen (Feloidea) gehören.

Die 16 rezenten Familien der Raubtiere sind mit etwa 270 Arten in 110 Gattungen nahezu weltweit verbreitet und ernähren sich als typische Prädatoren (Beutegreifer) überwiegend von Wirbeltieren.

Die wissenschaftliche Bezeichnung „Carnivora“ setzt sich aus den lateinischen Begriffen "caro, carnis" „Fleisch“ und "vorare" „verschlingen“ zusammen. Doch ernähren sich viele Raubtiere nicht ausschließlich von Fleisch; so sind etwa die Bären (Ursidae) opportunistische Allesfresser, und einige Arten, darunter der Große Panda, haben sich auf Pflanzennahrung spezialisiert.

Die Vertreter der Raubtiere sind sowohl hinsichtlich ihrer äußerlichen Attribute als auch ihrer Habitate sehr verschiedenartig. Diese Vielfalt wird im Vergleich zwischen dem kleinsten Raubtier der Erde, dem Mauswiesel, dessen Weibchen nicht einmal 50 g wiegen, und dem größten Raubtier der Erde, dem Südlichen Seeelefanten, dessen Bullen bis zu 6,5 Meter lang und mehr als 3,5 Tonnen schwer werden können, besonders hervorgehoben. Sofern als Referenz alle beutegreifenden Wirbeltiere herangezogen werden, so wird dieser Vergleich mit dem Pottwal, welcher bis zu 20 Meter lang und 50 Tonnen schwer werden kann, als größtes Raubtier der Erde umso imposanter.

Das Gebiss der landbewohnenden Raubtierfamilien baut auf folgender Zahnformel auf: Incisivi (Schneidezähne) 3/3, Canini (Eckzähne) 1/1, Prämolare (Vorbackenzähne) 4/4, Molare (Backenzähne) 3/3. Je nach Raubtierart sind die Ausprägungen unterschiedlich, wobei die Eckzähne, die so genannten Fangzähne, in der Regel extrem verlängert sind. Fast alle Arten besitzen je sechs kleine Schneidezähne im Ober- und Unterkiefer. Die wenigen Ausnahmen sind der Lippenbär, der in jeder Oberkieferhälfte nur zwei Schneidezähne besitzt, um durch die entstandene Lücke Insekten aufsaugen zu können, und der Seeotter, der im Unterkiefer insgesamt nur vier Schneidezähne trägt.

Alle Landraubtiere tragen darüber hinaus ein charakteristisches Gebissmerkmal: die sogenannte P4/M1-Brechschere, die sich jeweils aus zwei scharfen Reißzähnen zusammensetzt und hervorragend geeignet ist, um Fleisch zu zerschneiden. In jeder Kieferhälfte bilden zwei Reißzähne eine solche Funktionseinheit. Der obere Reißzahn ist der letzte Vorbackenzahn im Oberkiefer (Prämolar 4 = P4), der untere ist der erste Backenzahn (Molar 1 = M1) im Unterkiefer. Bei Hyänen sind sie besonders kräftig und eignen sich sogar zum Aufbrechen großer Knochen, bei Allesfressern wie Bären und Kleinbären sind sie weniger ausgeprägt. Die restlichen Backenzähne der Raubtiere sind im Gegensatz zu den auffälligen Reißzähnen in der Regel eher klein. Die Zahl der Backenzähne ist bei einigen Gruppen reduziert, so beispielsweise bei den Katzen.

Das Gebiss der Robben unterscheidet sich deutlich von dem der landlebenden Raubtiere. Es ist darauf spezialisiert, schlüpfrige Fische festzuhalten, und besteht aus einem oder zwei Paaren unterer Schneidezähne, relativ unauffälligen Eckzähnen und 12 bis 24 kegelförmigen, homodonten Backenzähnen. Extreme Abwandlungen sind die Stoßzähne des Walrosses oder die modifizierten Backenzähne der Krabbenfresser.

Charakteristisch für Raubtierschädel sind ausladende Jochbögen, eine große Schläfengrube als Ursprung für den kräftigen, zum Zubeißen wichtigen Schläfenmuskel, sowie die Verbindung von Augenhöhle und Schläfenfenster.
Der Unterkiefer ist so im Oberkiefer verankert, dass er nur auf- und abwärts bewegt werden kann; Seitwärtsbewegungen wie etwa beim Kauen sind nicht möglich.

Raubtiere besitzen vier oder fünf Zehen an jedem Fuß. Der Daumen kann den anderen Zehen nicht gegenübergestellt werden und ist bei einigen Arten zurückgebildet oder reduziert. Die Handwurzelknochen sind in der Regel verwachsen, wodurch das Handgelenk gefestigt wird. Das Schlüsselbein ist sowohl bei Robben als auch bei den anderen Gruppen reduziert oder ganz verschwunden. Es dient bei anderen Säugern dazu, Seitwärtsbewegungen der Gliedmaßen zu ermöglichen. Raubtiere, die vor allem darauf ausgerichtet sind, Beute zu verfolgen, bewegen ihre Beine jedoch hauptsächlich vor und zurück. Einige Raubtiere wie Katzen und Hunde gehen auf den Zehen, während andere wie die Bären Sohlengänger sind. Bei einigen, etwa Katzen und Schleichkatzen, findet man als Besonderheit einziehbare Krallen. Die Gliedmaßen der Robben sind stark modifiziert und zu Flossen umgebildet, bei denen die Zehen durch Schwimmhäute verbunden sind.

Wegen der meist geringen Spezialisierung bei der Nahrungsaufnahme ist, wie das Gebiss, auch der Verdauungstrakt im Vergleich zu vielen Pflanzenfressern recht ursprünglich und bietet dadurch eine höhere Anpassungsfähigkeit. Er besteht aus dem Magen und einem relativ kurzen Darm.

Weibliche Raubtiere verfügen über eine zweihörnige Gebärmutter. Sie haben bauchständige Milchdrüsen. Männliche Raubtiere (mit Ausnahme der Hyänen) haben einen Penisknochen (Baculum), die Hoden liegen außen.

Das relativ große Gehirn ist stark gefurcht.

Mit etwa 270 Arten sind die Raubtiere eine der artenreicheren Ordnungen der Säugetiere. Sie kommen auf allen Kontinenten vor, wobei sie in der Antarktis nur an den Küsten anzutreffen sind. Mit Ausnahme von Ohrenrobbenkolonien an der Südküste war Australien früher raubtierfrei, in historischer Zeit wurde jedoch der australische Dingo und in der Neuzeit Rotfuchs und Hauskatze durch den Menschen eingeführt.

Alle Familien der Katzenartigen sind, mit Ausnahme der Katzen selbst, die auch in Nord- und Südamerika vorkommen, natürlicherweise auf die alte Welt beschränkt. Zwei Familien der Katzenartigen, die Madagassischen Raubtiere und die Pardelroller, haben recht kleine Verbreitungsgebiete und kommen ausschließlich auf Madagaskar beziehungsweise in Zentralafrika vor. Die übrigen drei, Hyänen, Mangusten und Schleichkatzen, sind jeweils in Afrika, Asien und in Randgebieten in Europa verbreitet. Unter den Hundeartigen sind die Hunde, Bären und Marder fast weltweit verbreitet und fehlen ursprünglich nur in Australien und der Antarktis. Die Bären sind in Afrika allerdings mit dem Atlasbären im Holozän ausgestorben. Die Skunks sind in Südostasien und Amerika verbreitet, die Katzenbären mit einer Art auf Asien beschränkt und die Kleinbären leben ausschließlich in Amerika. In drei Familien bewohnen die Wasserraubtiere die Küstengewässer aller Kontinente, sowie einige wenige Süßwasserseen.

Die Lebensräume der Raubtiere sind vielseitig, und es gibt nur wenige Habitate, die sie nicht bevölkern. So findet man sie vom Packeisgürtel bis in tropische Regenwälder und von Küstenmeeren bis in trockene Wüsten.

Die Bandbreite des Sozialverhaltens ist nicht nur unter den Raubtieren an sich groß, sondern variiert auch deutlich innerhalb der einzelnen Tiergruppen. Oft steht die Gesellschaftsform in engem Zusammenhang mit der Jagdweise und Ernährung der jeweiligen Art. So leben einige Arten in Rudeln (Wölfe, Löwen) oder Kolonien (Seelöwen), andere als Einzelgänger (Leopard, Braunbär) oder in Familiengruppen (Schakale).

Die meisten Raubtiere sind Fleischfresser. Ihren Fleischbedarf decken sie durch Jagd oder das Fressen von Aas. Ein großer Teil der Carnivora ist jedoch "omnivor", also allesfressend, das heißt, sie nehmen neben Fleisch auch andere Nahrung wie Beeren oder Gräser zu sich. Viele kleinere Raubtiere wie Mangusten, aber auch einige größere Arten wie Löffelhund, Erdwolf und Lippenbär ernähren sich zu großen Teilen von Wirbellosen, vornehmlich Insekten. Einige Raubtierarten, darunter der Große Panda, der Pardelroller oder der Wickelbär, sind sogar vorrangig oder fast ausschließlich Pflanzenfresser. Dennoch findet man zahlreiche hochspezialisierte Beutegreifer innerhalb dieser Ordnung.

Die Art und Weise, wie Raubtiere ihre Opfer erlegen, ist sehr vielseitig. Einige Arten, etwa die Wildhunde, hetzen ihre Beute bis zur Erschöpfung, andere schleichen sich nah an ihre Beute heran und überraschen sie mit einem schnellen Angriff, so beispielsweise die Katzen. Marder sind fähig, schnell kletternden Eichhörnchen in Bäumen nachzustellen, Wiesel verfolgen Nagetiere in ihre Gänge und Robben jagen Fische. Große Robben wie Seeelefanten erreichen dabei Tiefen von über 1000 Metern. Einige Raubtiere sind in der Lage, Tiere zu erlegen, die um einiges größer sind als sie selbst. Zum Beispiel können Tiger Gaure (große Rinder aus Südostasien) erlegen, und das Hermelin kann ein Kaninchen töten, das ein Vielfaches seines Körpergewichtes wiegt. Einige Arten setzen vor allem auf Gruppenjagd, während andere im Alleingang jagen.

Die meisten Raubtierarten werfen etwa einmal pro Jahr, kleinere Arten auch mehrmals. Bei großen Arten wie den Großkatzen und Bären vergehen meist zwei bis drei Jahre zwischen zwei Würfen. Die Tragzeit schwankt zwischen 50 und 115 Tagen. Die Jungen kommen in der Regel klein, blind und unfähig zum eigenständigen Überleben zur Welt.

Bei einigen Marderartigen und Bären tritt eine verzögerte Entwicklung des Embryos auf. Dieser als Keimruhe bezeichnete Mechanismus verlängert die Tragzeit und stellt sicher, dass die Jungen zu einer möglichst günstigen Jahreszeit geboren werden.

Die Raubtiere werden heute aufgrund molekulargenetischer Befunde zur großen Säugerlinie der Laurasiatheria gezählt, zu denen auch die Insektenfresser, Fledertiere, Unpaarhufer, Cetartiodactyla (Wale und Paarhufer) und Schuppentiere gehören.
Innerhalb der Laurasiatheria werden die Raubtiere heute meist zusammen mit den Schuppentieren und den ausgestorbenen Creodonten in eine gesonderte Gruppe, die Ferae, gestellt. Deren Schwestergruppe wären nach dieser Auffassung die Unpaarhufer. Ein mögliches Kladogramm der Laurasiatheria sieht folgendermaßen aus:
Während die Ferae heute als relativ sicher angesehen werden, gibt es bzgl. der Systematik innerhalb der Laurasiatheria noch regelmäßige Diskussionen.

Die klassische Einteilung der Raubtiere sah zwei Unterordnungen vor, die Landraubtiere und die Wasserraubtiere; letztere waren dabei die Robben, erstere alle landbewohnenden Raubtiere. Nach heutigem Stand ist diese Unterteilung überholt, da Robben sich aus hundeartigen Raubtieren entwickelten und daher diesen zuzuordnen sind. Diese Zugehörigkeit basiert sowohl auf morphologischen als auch auf molekulargenetischen Untersuchungen. Demnach spalteten sich die frühen Raubtiere nicht zuerst in Land- und Wasserraubtiere auf, sondern in Hundeartige (Canoidea) und Katzenartige (Feloidea). Schon lange unterscheidet man diese zwei Stammlinien, wobei früher allerdings die Robben nicht zu den Hundeartigen gerechnet wurden.

Unbestritten ist heute, dass die drei Familien der Robben ein monophyletisches Taxon sind, also alle auf einen gemeinsamen Vorfahren zurückgehen. Auch die Monophylie der traditionell unterschiedenen Landraubtier-Familien bestätigte sich größtenteils. Dies gilt insbesondere für Hunde, Bären, Hyänen und Katzen. Die anderen herkömmlichen Familien, die Marderartigen, Kleinbären, Schleichkatzen und Mangusten, haben noch immer Gültigkeit, mussten jedoch teilweise etwas umstrukturiert werden, um den neuen taxonomischen Befunden gerecht zu werden. So bilden die Skunks zusammen mit den Stinkdachsen Südostasiens eine eigene Familie, der Kleine Panda ist in einer eigenen Familie Ailuridae abgetrennt und die Madagassischen Raubtiere bilden ein monophyletisches Taxon. Der Pardelroller und die Linsangs stellen nach molekulargenetischen Untersuchungen ebenfalls jeweils eigenständige Familien dar.



Die genaueren verwandtschaftlichen Verhältnisse der Raubtierfamilien untereinander konnten jüngst durch molekulargenetische Analysen etwas aufgehellt werden, wenn auch einzelne Fragen noch immer ungeklärt sind. Als sicher gilt heute, dass innerhalb der Hundeartigen die Caniden (Hunde) allen anderen heutigen Gruppen, die als Arctoidea zusammengefasst werden, gegenüberstehen. Die Arctoidea selbst gliedern sich in die drei Hauptkladen Bären, Robben und Musteloidea. Zu letzteren zählen die Katzenbären, Skunks, Kleinbären und Marder. Die Systematik unter den Musteloidea ist nicht ganz gesichert, doch Marder und die Kleinbären scheinen eine Einheit zu bilden, deren Schwestergruppe die Skunks sind. Diesen drei Gruppen stünden demnach die Katzenbären als gesonderte Gruppe innerhalb der Musteloidea gegenüber. Zu den Katzenbären gehört nach heutigen Erkenntnissen nur der Kleine Panda. Der Große Panda hingegen wird heute den Großbären (Ursidae) zugeteilt. Unklar bleibt unter den Hundeartigen die Stellung der ausgestorbenen Amphicyonidae.

Innerhalb der Katzenartigen stellten sich überraschend die Pardelroller, die gegenwärtig mit nur einer Art in Zentralafrika vertreten sind, als eigene Familie heraus, die allen anderen überlebenden Familien aus dem Katzenzweig gegenüberstehen. Des Weiteren bilden alle Madagassischen Raubtiere eine monophyletische Gruppe, die nahe mit den Mangusten verwandt ist. Die Schwestergruppe dieser beiden Gruppen dürften die Hyänen sein. Bei den Katzenartigen wären demnach neben den Pardelrollern drei Hauptlinien zu unterscheiden: (1) Die Katzen und die Linsangs, (2) die Schleichkatzen (ohne Pardelroller und Madagassische Raubtiere) und (3) eine Gruppe, die sich aus Hyänen, Mangusten und Madagassischen Raubtieren zusammensetzt. Unklar ist, ob die Katzen oder die Schleichkatzen dem Hyänen-Mangusten-Zweig näher stehen. Ungeklärt ist auch die genaue Stellung der ausgestorbenen Nimravidae, Barbourofelidae sowie der hyänenähnlichen Stenoplesictidae und Percrocutidae.

Ein mögliches Kladogramm der Raubtierfamilien könnte folgendermaßen aussehen:
Nach der herkömmlichen Meinung hatten sich die Raubtiere schon im frühen Paläozän in die beiden großen Raubtier-Stammlinien, die Katzenartigen (Feliformia) und die Hundeartigen (Caniformia), aufgespalten. Als älteste Gruppe der ersteren galt bisher die ausgestorbene Familie Viverravidae, die bereits im Paläozän nachgewiesen ist. Einer ihrer Vertreter war beispielsweise die zierliche, baumlebende Gattung "Protictis". Als früheste Gruppe der Hundeartigen wurden meist die Miacidae angesehen, die etwa ab dem späten Paläozän auftreten. (Ursprünglich wurden die Miaciden sogar als Vorläufer aller Raubtiere angesehen.) Ihre Pfoten waren flexibel, was auf Kletterfähigkeiten hinweist, und sie besaßen ein vollständiges Gebiss mit 44 Zähnen. Auch die Brechschere war bereits entwickelt.

Neuere Studien weisen allerdings darauf hin, dass die Miaciden und Viverraviden nicht die direkten Vorfahren der beiden Raubtierlinien sind, sondern sogar ganz außerhalb der Carnivora anzusiedeln sind. Die Miaciden scheinen nicht einmal eine monophyletische Gruppe zu sein. Viverraviden und Miaciden werden aber auch nach diesen Befunden mit den Raubtieren zu einem Taxon, den Carnivoramorpha, zusammengefasst. Diesen Ergebnissen zufolge hätten sich die beiden Hauptlinien der Carnivora erst vor etwa 43 Millionen Jahren auseinanderentwickelt.

Eine der ältesten Familien aus dem Katzenzweig sind die Nimravidae, die sehr stark an Katzen (Felidae) erinnern, aber als separate Familie angesehen werden. Sie traten erstmals im späten Eozän Nordamerikas und Eurasiens auf. Eine weitere Familie, die Barbourofelidae, wurden ursprünglich als Unterfamilie der Nimravidae angesehen, doch gelten sie heute als eigene Familie. Die Barbourofelidae starben erst im späten Miozän mit der nordamerikanischen Gattung "Barbourofelis" aus.

Der erste Vertreter der Katzen selbst war "Proailurus" aus dem Oligozän und Miozän Europas. Er war etwa so groß wie ein Ozelot. Im Miozän wanderten die Katzen erstmals nach Nordamerika ein und verdrängten schnell die dort lebenden Nimraviden. Im Pliozän wanderten sie auch nach Südamerika ein. Die anderen Familien der Katzenartigen blieben, abgesehen von einer nordamerikanischen Hyänengattung des Pliozäns, auf die Alte Welt beschränkt und erreichten nie den amerikanischen Kontinent.

Die zweite Linie der Raubtiere sind die Hundeartigen. Ihre namensgebende Familie, die Hunde (Canidae), ist entwicklungsgeschichtlich vor allem in Nordamerika vertreten und war ursprünglich auf diesen Kontinent beschränkt. Die Gattung "Hesperocyon" aus dem mittleren Eozän war der erste bekannte Vertreter dieser Familie. Die Hunde erreichten Europa im Miozän, Afrika, Asien und Südamerika nicht vor dem Pliozän.

Auch die zweite Familie, die Bären, tauchte zuerst in Nordamerika auf und erreichte Eurasien und Afrika im Miozän. Im Gegensatz zu den heutigen Formen war der erste Bär noch ziemlich klein. "Parictis" aus dem späten Eozän hatte einen nur 7 cm langen Schädel. Die anderen Familien der Hundeartigen, zu denen neben den heute noch existierenden auch die ausgestorbene Familie der Amphicyonidae gehörte, sind spätestens ab dem frühen Oligozän sowohl in Nordamerika als auch in der Alten Welt nachweisbar. Südamerika erreichten die Hundeartigen genau wie die Katzen erst im Pliozän, nach der Entstehung der mittelamerikanischen Landbrücke. Lediglich die Kleinbären sind schon ab dem späten Miozän auf diesem Kontinent nachgewiesen.

Die Robben, die innerhalb der Hundeartigen zur Gruppe der Arctoidea zählen, sind erst aus dem Oligozän bekannt. "Enaliarctos" etwa hatte bereits Flossen und lebte im späten Oligozän Kaliforniens. Die Backenzähne dieser frühen Gattung waren noch kaum modifizierte Reißzähne, wie sie für Landraubtiere typisch sind. Vertreter, die den drei Robbenfamilien zugeordnet werden können, sind aus dem Miozän bekannt. Ohrenrobben und Walrosse waren damals an den Küsten des Nordpazifik verbreitet, während die Hundsrobben im Nordatlantik lebten.

Bevor sich die Carnivora zu den Gipfelräubern entwickelten, wurde diese Nische von zwei anderen Säugerordnungen ausgefüllt, die bereits vor langer Zeit ausstarben. Die ersten waren die Mesonychia, fleischfressende Huftiere, die im Paläozän und Eozän verbreitet waren. Sie brachten die ersten großen Fleischfresser unter den höheren Säugetieren hervor. Eine zweite Gruppe räuberischer Säugetiere waren die Creodonten („Scheinraubtiere“). Im frühen Paläogen waren die eigentlichen Raubtiere noch verhältnismäßig klein, die Creodonten jedoch waren mit einer beachtlichen Formenfülle großer Fleischfresser vertreten. Ebenso wie die Carnivora hatten die Creodonten ein Brechscherengebiss entwickelt. Die Brechscheren von Raubtieren und Creodonten bestanden jedoch jeweils aus unterschiedlichen Backenzähnen, ein Beleg dafür, dass beide Gruppen sich unabhängig voneinander, also konvergent, entwickelt haben.

Auf den Kontinenten Australien und Südamerika, wo lange Zeit keine modernen Raubtiere lebten, wurde die Rolle größerer Fleischfresser ursprünglich von verschiedenen Beuteltierarten ausgefüllt. In Südamerika lebten bis in das Pliozän fleischfressende Beuteltiere der Ordnung Sparassodonta, zu denen auch die den säbelzahnkatzenähnliche Gattung "Thylacosmilus" gehörte. Mit der Bildung der mittelamerikanischen Landbrücke und dem großen amerikanischen Faunenaustausch am Ende des Pliozäns wanderten Carnivora aus Nordamerika ein und verdrängten ihre südamerikanischen Konkurrenten. Unter den speziellen Bedingungen Australiens konnten sich unter den Beutelsäugern einige mittelgroße Fleischfresser entwickeln, wie etwa die Beutelmarder und der Beutelteufel, die zu den Raubbeutlern gehören. Einer anderen Familie der Raubbeutlerartigen gehört der Beutelwolf an, der im 20. Jahrhundert verschwand. Fleischfresser außerhalb der Raubbeutlerartigen waren z. B. die Beutellöwen, die im Pleistozän ausstarben, oder fleischfressende Känguruarten der Gattung "Ekaltadeta", die bereits im Obermiozän ausstarben.

Seit Urzeiten sind große Raubtiere die Nahrungskonkurrenten des Menschen. Viele Raubtiere wurden als Feinde der Nutztiere des Menschen verfolgt und verloren einen Großteil ihres Lebensraumes durch die Ausbreitung und Konkurrenz des Menschen. Auch die Jagd auf Wildtiere verübelte ihnen der Mensch und dezimierte sie aus diesem Grund. So wurden Großraubtiere im Yellowstone-Nationalpark selbst nach der Nationalpark-Gründung verfolgt und der Wolf dabei ausgerottet. Viele Raubtiere wurden oder werden auch wegen ihres Fells zur Herstellung von Kleidung und als Jagdtrophäen bejagt. Heute sind etliche Arten vom Aussterben bedroht und besonders die Bestände der großen Raubtiere sind vielfach bis auf kleine Reliktpopulationen zusammengeschmolzen.

Stellenweise ist heute allerdings ein Umdenken zu erkennen. Vor allem in Europa und Nordamerika scheinen einige Großraubtiere wieder etwas an verlorenem Boden gutmachen zu können. So wurden Wölfe im Yellowstone-Nationalpark wiedereingeführt und in Mitteleuropa etablieren sich zunehmend Bären, Wölfe und Luchse. Einige anpassungsfähige Arten, wie etwa Rotfuchs oder Marder, dringen immer mehr in menschliche Siedlungen vor und finden selbst in modernen Großstädten ein Auskommen.

Besonders die großen Arten wie Löwe, Tiger, Bär und Wolf haben mythische Bedeutung erlangt und Eingang in zahlreiche Sagen gefunden.

Einige Arten (vor allem Haushund und Hauskatze) werden vom Menschen auch als Haustiere gehalten. Verschiedene Marder werden wegen ihres Felles oder im Falle des Frettchens zur Kaninchen- und Hasenjagd gezüchtet.

Mehrere Raubtierarten, wie etwa der Rotfuchs, sind Überträger gefährlicher Seuchen wie der Tollwut.




</doc>
<doc id="10076" url="https://de.wikipedia.org/wiki?curid=10076" title="Raubtier">
Raubtier

Raubtier bezeichnet 


Siehe auch:


</doc>
<doc id="10077" url="https://de.wikipedia.org/wiki?curid=10077" title="Zahnarme">
Zahnarme

Die Zahnarmen (Pilosa) sind eine Ordnung der Säugetiere. In dieser Gruppe werden die Ameisenbären und die Faultiere zusammengefasst. Zusammen mit den Gepanzerten Nebengelenktieren bilden sie die Überordnung der Nebengelenktiere (Xenarthra).

Faultiere und Ameisenbären sind stark an die jeweilige Lebensweise angepasst und unterscheiden sich äußerlich deutlich voneinander. Ameisenbären sind langschnäuzige, zahnlose Tiere, die am Boden oder in den Bäumen leben und sich von Insekten ernähren. Die lebenden Faultiere sind kurzschnäuzige, pflanzenfressende Tiere mit bis zu 20 homodonten Zähnen. Sie leben auf Bäumen, die bodenbewohnenden Riesenfaultiere sind ausgestorben. Von den Gepanzerten Nebengelenktieren, die heute nur mehr in Form der Gürteltiere vorkommen, unterscheiden sie sich im Fehlen der Hautpanzerung. Während die Monophylie der Nebengelenktiere als ganzer außer Zweifel steht, sind die Zahnarmen ein umstrittenes Taxon.

Schon Ende des 18. Jahrhunderts fassten Félix Vicq d’Azyr und Frédéric Cuvier mehrere Tiergruppen als Zahnlose (Edentata) zusammen. Diese schlossen neben den heute zu den Nebengelenktieren gerechneten Säugetieren auch noch die Ameisenigel, Schuppentiere und Erdferkel ein. Heute gelten diese Säugetiergruppen nur als oberflächlich ähnlich und nicht näher miteinander verwandt.



</doc>
<doc id="10079" url="https://de.wikipedia.org/wiki?curid=10079" title="Insektarium">
Insektarium

Als Insektarium bezeichnet man eine Sonderform des Terrariums zur Haltung von Insekten oder Spinnentieren. Auch die Abteilung eines Zoos, in der Insekten gezeigt werden, wird so genannt.

Behältnisse zur Haltung von Insekten sind meist an mindestens einer Seite einsehbar. Da Wirbellose bei den notwendigen Umweltbedingungen meist flexibler sind als Reptilien und insbesondere Amphibien, ist die Kontrolle von Temperatur und Luftfeuchtigkeit oft nicht so wichtig. Man verwendet wegen der geringen Größe der Tiere meistens kleinere Behälter. 

In Insektarien werden unter anderem folgende Tiere gehalten:



</doc>
<doc id="10081" url="https://de.wikipedia.org/wiki?curid=10081" title="Aquarium">
Aquarium

Das Aquarium (von „zum Wasser gehörig“; substantiviertes Neutrum "aquarium" speziell „Wasserbehälter“) ist die am weitesten verbreitete Art des Vivariums. Meist handelt es sich bei Aquarien um Gefäße aus Glas oder durchsichtigem Kunststoff, die mit Wasser befüllt werden. Unter Verwendung von Fischen und wirbellosen Tieren wie Weichtieren oder auch Krebsen sowie Wasserpflanzen und Bodenmaterialien, meist Kies oder Sand, stellt der Aquarianer eine Unterwasserwelt her und erhält sie am Leben. Auf Wassertiere spezialisierte Zoos (auch Aquazoos genannt) bezeichnen sich ebenfalls als Aquarien.

Früher nannte man den Kellerraum in Apotheken, der zur Aufbewahrung flüssiger Arzneistoffe in Flaschen, Fässern usw. bestimmt war, „Aquarium“. In England wurde der Begriff „Aquarium“ dann verwendet, um die in den Glashäusern zur Pflege von Wasserpflanzen (aber nicht Wassertieren) aufgestellten Bassins zu bezeichnen. Der moderne Begriff des Aquariums wurde im 19. Jahrhundert üblich.

Die Haltung von Fischen in einer künstlichen Umgebung hat eine weit zurückreichende Vergangenheit. Bereits die Sumerer hielten gefangene Fische in Teichen, bevor sie sie für Mahlzeiten zubereiteten. Ähnliches ist aus dem alten Ägypten bekannt. Für alle Tempelgärten von der frühdynastischen Zeit bis zum Neuen Reich gilt, dass sie mit rechteckigen Wasserbassins ausgestattet waren, die als Einkonsonantenzeichen auch in die Hieroglyphenschrift eingingen. Auch in den häufig relativ kleinen ägyptischen Hausgärten hatten künstlich angelegte Teiche und Becken eine zentrale Rolle inne. Auf einem altägyptischen Gartenmodell, das sich heute im Metropolitan Museum in New York befindet, nimmt das von Maulbeerfeigen umrahmte Wasserbecken fast die ganze Gartenfläche ein. Aufgrund von Abbildungen, die man in der Ausgrabungsstätte Oxyrhynchus gefunden hat, weiß man, dass in solchen Becken Fische gehalten wurden.

Ähnlich alt ist die Haltung von Fischen in China. Die gezielte Züchtung von Karpfen begann vermutlich vor rund 2500 Jahren; das älteste Buch zur Fischzucht datiert in die Zeit von 770 bis 476 v. Chr. Während der Song-Dynastie (960–1216 n. Chr.) begann die Domestikation des Goldfisches aus der Silberkarausche, ab dem frühen 16. Jahrhundert ist belegt, dass Goldfische in großen Keramikgefäßen auch in Häusern gehalten wurden. 1596 erschien das erste Buch zum Thema Aquarium von Chang Chi'en-te mit dem Titel „Chu sha yü p'u“, auf Deutsch „Traktat über die Goldfische“.

Die ersten Hinweise auf die Haltung von Nutzfischen im antiken Rom stammen aus dem 2. Jahrhundert v. Chr. Zunächst waren es Süßwasserbecken, in denen Bauern Fische zum Verzehr zogen. Zu Anfang des 1. Jahrhunderts v. Chr. begann die kommerzielle Zucht von marinen Fischen in Meerwasserbecken von Angehörigen der Oberschicht. Schon bald wurden Fische darin vor allem zur Zierde gehalten. Die Meerwasserbecken erreichten in der Folge immer größere Ausmaße und nahmen im 1. Jahrhundert n. Chr. weite Abschnitte der mittelitalienischen Westküste ein. Ihr Betrieb entwickelte sich seit der Mitte des 1. Jahrhunderts v. Chr. zu einem Statussymbol einer elitären Oberschicht. Für etwa einhundert Jahre wurde Fischhaltung ein integraler Bestandteil gehobener römischer Lebensweise, nicht selten mit Hang zur Verschwendung. In der zweiten Hälfte des 1. Jahrhunderts n. Chr. forderte die neue flavische Dynastie Bescheidenheit und Sparsamkeit. Nachfolgend verlor die Fischhaltung als Mode rasch an Bedeutung.

Der genaue Zeitpunkt, zu dem erstmals Lebewesen in durchsichtigen Behältern gepflegt wurden, lässt sich nicht genau bestimmen. Der englische Tagebuchschreiber Samuel Pepys notierte 1665 in seinen Aufzeichnungen, dass er in London Fische gesehen habe, die in einem Wasserglas am Leben gehalten wurden. Am wahrscheinlichsten handelte es sich um Goldfische. Genannt werden auch Paradiesfische, die durch die Handelsbeziehungen der East India Company aus Kanton, wo sie in Gartenteichen gehalten wurden, nach London gelangten. Zunächst waren es vor allem naturwissenschaftliche Forscher, die Lebewesen in Behältern hielten, um daran ihre Untersuchungen vorzunehmen. Der britische Chemiker Joseph Priestley beispielsweise, der 1774 gleichzeitig mit dem in Schweden arbeitenden Stralsunder Carl Wilhelm Scheele den Sauerstoff entdeckte, nahm weitergehende Untersuchungen zum Sauerstoff an Wasserpflanzen vor, die er in seinem Labor hielt.

Auf der Weltausstellung 1851 in London wurden erstmals Aquarien einer breiten Öffentlichkeit gezeigt. Die Scheiben dieser Aquarien wurden wie bei einem Ward’schen Kasten von einem gusseisernen Rahmen zusammengehalten.

Den Begriff "„Aquarium“" prägte allerdings der englische Naturforscher Philip Henry Gosse in seiner 1853 erschienenen Veröffentlichung "„A Naturalist’s Rambles on the Devonshire Coast“". Das Buch fand in der britischen Öffentlichkeit sehr großen Anklang. Ein Jahr später erschien sein Buch "„The Aquarium: an Unveiling of the Wonders of the Deep Sea“". Es kam in Mode, Lebewesen zumindest kurzfristig in kleinen Glasbehältern zu pflegen, um sie besser studieren zu können. In Deutschland war es vor allem der Naturforscher, Pädagoge und Schriftsteller Emil Adolf Roßmäßler, der mit verschiedenen Artikeln dieses Hobby in Deutschland populär machte. Er wird daher scherzhaft auch als „Vater der deutschen Aquaristik“ bezeichnet. 1854 erschien von ihm in der damals populären Familienzeitschrift Die Gartenlaube der Artikel "Der Ocean auf dem Tisch". Ähnlich wie Gosse stellte er hier die Pflege von Seewassertieren vor. Roßmäßler verfolgte mit der Veröffentlichung das Ziel, die Naturwissenschaft im Volk bekannt und populär zu machen. Ihm wurde allerdings bald klar, dass dies durch ein Süßwasseraquarium einfacher zu erzielen war. Deswegen folgte in der Gartenlaube sehr bald der Artikel "Der See im Glase", der zu so viel Rückfragen zu dieser Form der Tierhaltung führte, dass er 1857 sein Buch "Das Süßwasseraquarium" veröffentlichte. Roßmäßler gab darin konkrete Hinweise, wie ein solches Aquarium einzurichten und zu pflegen sei. Neben dem Goldfisch empfahl er vor allem die Elritze und den Schlammpeitzger.

Roßmäßlers Veröffentlichungen folgten eine Vielzahl von weiteren Aquarienbüchern und -zeitschriften. Aquarienvereine wurden in ganz Deutschland gegründet. Es wurde modern, in den Wintergärten der Gründerzeitvillen ein sogenanntes Salonaquarium oder ein Goldfischglas aufzustellen.

Aus heutiger Sicht ist diese euphorische Sicht nicht zutreffend. Man hatte zur damaligen Zeit wenig Wissen über die Bedürfnisse der gehälterten Lebewesen oder die im Aquarium ablaufenden Prozesse. Aquarien wurden damals teilweise noch mit Kerzen geheizt, die unter dem Aquarium aufgestellt wurden. Im Herbst starben die Pfleglinge sehr häufig, weil es dem Wasser an Sauerstoff mangelte und man nicht in der Lage war, für eine ausreichende Beleuchtung der Aquarien zu sorgen.

Kontroverse gesellschaftliche Diskussionen entstanden bezüglich der Fragen, ob innerhalb der Aquaristik auch Frauen Fische züchten dürften und ob nur einheimische Arten oder auch „Exoten“ gehalten werden sollten.

Die Weiterentwicklung der Aquaristik während der letzten einhundert Jahre ist vor allem auf das Verständnis zurückzuführen, dass ein Aquarium in der Regel ohne entsprechende technische und chemische Unterstützung keinen Lebensraum für Fische und Pflanzen bieten kann. Der Schwerpunkt der technischen Weiterentwicklung lag dabei vor allem in der zweiten Hälfte des 20. Jahrhunderts. Ausschlaggebend war neben einem zunehmenden Wissen über die biologischen und chemischen Vorgänge in einem Aquarium die Verfügbarkeit von Materialien für den Bau immer ausgefeilterer Aquarienfilter, verbesserter Beleuchtungssysteme und kleinerer Pumpen sowie eine ausreichende Kaufkraft eines genügend großen Anteils der Bevölkerung, der bereit war, in dieses Hobby zu investieren und solche Produkte nachzufragen. Auch in den Communitys findet das Thema zusehends seine Verbreitung.

Aquarien können aus unterschiedlichen Materialien gefertigt sein. Bis Ende der 1960er Jahre waren die meisten Aquarien Rahmenaquarien, bei denen Glasscheiben in Metallrahmen eingekittet und z. B. mit Bitumen abgedichtet wurden, oder Vollglasaquarien bis etwa 20 Liter Fassungsvermögen. Eine entscheidende Konstruktionsänderung kam erst mit der Entwicklung der Silikonkleber, welche die früheren Metallrahmen überflüssig machten. Damit waren rahmenlose Aquarien in den verschiedensten Größen und Formen möglich. Die typischen Aquarien der 1960er und 1970er Jahre waren meist an den Seiten mit Holz verkleidet. Damit sollte sichergestellt werden, dass sich das Aquarium harmonisch in die Wohnung einfügte. In dieser Zeit wurde zunehmend auch Wert auf eine harmonische Pflanzenvielfalt im Becken gelegt, so dass das holländische Pflanzenaquarium entstand.

Für die Aquarien gibt es verschiedene Möglichkeiten und Methoden zur Temperatureinstellung. Gebräuchlich sind Kontaktthermometer, elektronische Regeleinrichtungen und manuelle Temperatureinstellung. Heute haben die in Aquarien verwendeten Stabheizungen Messsonden, die eine gradgenaue Einstellung ermöglichen, ohne dass Zusatzgeräte erforderlich sind. Elektronische Einrichtungen erlauben teilweise auch eine zeit- oder tageslichtgesteuerte Temperaturführung entsprechend den täglichen Temperaturschwankungen in den Ursprungsgebieten der Fische. Eine allmähliche Erhöhung der Wassertemperatur, wie sie notwendig ist, um bei einigen Labyrinthfischen die Laichbereitschaft auszulösen, ist mit elektronischen Reglern relativ einfach zu erzielen. Anstelle von in das Becken gehängten Stabheizungen gibt es auch verschiedene Arten von Bodenheizungen. In großen Aquarien findet man oft eine direkte Erwärmung über Heizspiralen, die über die Zentralheizung gesteuert werden.

Auch die Meerwasseraquaristik, die lange Zeit nur von erfahrenen Aquarianern betrieben wurde, ist heute einfacher zu realisieren. Sie gilt jedoch immer noch als anspruchsvoller und auch kostenintensiver als die Süßwasseraquaristik. In der Praxis haben sich nationale Vorlieben herausgebildet. In Deutschland und wohl auch in vielen anderen Ländern werden Riffaquarien vor allem nach dem Berliner System betrieben; viele französische Meerwasseraquarianer nutzen das Jaubert-System und aus Nordamerika kommt neuerdings die Deep Sand Methode, die ein enormes Wachstum von Steinkorallen bei geringem Technikeinsatz möglich macht. Zeovith-Methode oder Algenrefugium sind weitere Pflegemöglichkeiten in der Meerwasseraquaristik. Es lässt sich allerdings keines von ihnen als das „beste“ System bezeichnen. Klassische Filter wie in der Süßwasseraquaristik haben sich nicht durchgesetzt.
In der Süßwasseraquaristik galt Takashi Amano als einer der einflussreichsten Aquarianer. Er machte das sogenannte Naturaquarium populär, bei dem Landschaftsbilder der Natur nachgebildet werden und das von der japanischen Gartenkunst maßgeblich beeinflusst ist. Es handelt sich dabei keineswegs um Biotop-Aquarien, bei denen ein Lebensraum exakt nachgebildet wird. Ziel ist es vielmehr, ästhetische Landschaften als Kontemplationsobjekte mit den Mitteln der Aquaristik zu gestalten. Vorbilder aus Natur und Phantasie werden dabei ins Aquarium übertragen. Dabei kombinierte Amano Pflanzen, die aus unterschiedlichen Kontinenten stammen, und vergesellschaftete Lebewesen, deren Lebensräume sich in der freien Natur nicht überschneiden.

In eine ähnliche Richtung geht auch ein neuerer Trend der Gestaltung von Aquarien, der sich Aquascaping nennt. Hier steht die ansprechende Gestaltung einer Wasserwelt, etwa mit Pflanzen, im Vordergrund. Häufig werden nur sehr kleine Fische oder Wirbellose eingesetzt, zum Teil wird auch vollständig auf die Verwendung von Tieren verzichtet.

Zunehmend findet sich heutzutage eine Vielfalt von sogenannten „Designaquarien“. Diese Becken haben Säulen- oder Pyramidenform, oder es werden Wandaquarien in Form eines Bildes oder einer Halbkugel mit zu geringem Volumen angeboten, die zur Haltung von Fischen nicht geeignet sind (siehe Goldfischglas). Designer erfinden Beckeneinrichtungen, wie z. B. aus Chromelementen oder Plastikschläuchen, die ebenso unter den Aspekt der Tierquälerei fallen. Die neueste Entwicklung der Aquarienform ist das Bonsai-Aquarium. Hier wird ähnlich einem Paludarium Wert auf einen Landteil gelegt, der mit Wurzeln, Wasserfällen, Moos und kleinen Solitärpflanzen gestaltet ist. Das Becken dieses Aquarientyps hat eine halbierte Front- und abgeschrägte Seitenscheiben.

Süßwasseraquarien können nach einer Reihe sehr unterschiedlicher Kriterien klassifiziert werden. Ein wichtiges Gliederungsmerkmal ist heutzutage die Klassifizierung des Wassers anhand der darin gelösten Inhaltsstoffe. Sie sind entscheidend dafür, welchen Lebewesen im Aquarium geeignete Bedingungen geboten werden können. Früher wurde mehr Wert auf die Unterscheidung Kalt- oder Warmwasseraquarium gelegt.

Aquarien werden zuerst vor allem nach dem Salzgehalt des Wassers unterschieden. Meereswasseraquarien haben den höchsten Anteil an gelöstem Salz im Wasser. In ihnen werden Lebensbedingungen simuliert, wie sie in Ozeanen vorkommen. Der Salzgehalt liegt bei 3,4 Prozent (34 g/l). Bei Süßwasseraquarien beträgt der Anteil des Salzes im Wasser weniger als 0,6 Prozent. Nachgeahmt werden die Lebensbedingungen in einem See oder Fluss. Dieser Typus von Aquarium ist der in der Aquaristik am meisten verbreitete. Brackwasseraquarien sind dagegen ein verhältnismäßig wenig verbreiteter Aquarientyp. Sie bilden die Lebensbedingungen der Mündungsgebiete großer Flüsse oder Mangrovenküsten nach. Der Salzgehalt liegt zwischen den Werten für ein Salz- und Süßwasseraquarium.

Viele tropische Gewässer, aus denen im Aquarium gepflegte Lebewesen stammen, haben salzarmes und sehr weiches Wasser. Bezeichnungen wie Amazonasbecken oder Schwarzwasseraquarium weisen auf Süßwasseraquarien hin, die diese Lebensbedingungen simulieren; während in einem Schwarzwasserbecken jedoch Arten unterschiedlichster Herkunft gepflegt werden, die diese Haltungsbedingungen fordern, werden in einem Amazonasbecken gezielt nur solche Pflanzen, Fische und Wirbellose gehalten, die ihr ursprüngliches Verbreitungsgebiet im Amazonas haben. Sogenannte Malawibecken oder Ostafrikaaquarien beherbergen meist Fische der ostafrikanischen Grabenseen mit höherem Salzgehalt. Aquarien für heimische Kaltwasserfische weisen ebenfalls oft härteres Wasser als diejenigen für tropische Fische auf; der Begriff „Hartwasserbecken“ ist jedoch unüblich.

Einen spezifischen Typ des Aquariums stellt das sogenannte Altwasseraquarium dar. Hier wird auf den regelmäßigen Teilwasserwechsel verzichtet und lediglich das verdunstete Wasser mit Regen- oder destilliertem Wasser aufgefüllt. Notwendig dafür ist ein nur sehr geringer Fischbesatz, damit Filter und Pflanzen die Möglichkeit haben, die anfallenden Stoffwechselprodukte umzuwandeln; diese Form ist in der Aquaristik jedoch umstritten.

Bei den meisten Aquarien handelt es sich um tropische Süßwasseraquarien, bei denen eine Wassertemperatur zwischen 22 und 28 °C konstant gehalten wird. Temperaturen über 28 °C sind meist nur in Aquarien mit extrem wärmeliebenden Fischen wie etwa Diskusfischen oder Harnischwelsen vorzufinden. Temperaturschwankungen beschränken sich (wenn überhaupt) auf einen Tag-Nacht-Rhythmus oder einen geringfügigen Anstieg oder Abfall der Temperatur zur Laichzeit. Es gibt keine genaue Grenze, ab welcher Wassertemperatur ein Aquarium zu den Kaltwasseraquarien zählt. Allgemein werden so oft Aquarien bezeichnet, die nicht über eine Heizung verfügen und Zimmertemperatur aufweisen. Je nach Standort des Aquariums kann es sogar notwendig sein, mit einem Kühlaggregat oder Raumkühlung dafür zu sorgen, dass an heißen Sommertagen die Wassertemperatur nicht über ein für die gepflegten Lebewesen erträgliches Maß hinaus steigt. Größere Schwankungen der Temperatur im Jahreslauf sind charakteristisch für klassische Kaltwasseraquarien. In ihnen werden häufig Fischarten gehalten, die auch in den Gewässern Mitteleuropas heimisch sind. Dies sind beispielsweise Orfen, Moderlieschen, Steinbeißer, Mühlkoppe und der Dreistachlige Stichling. Auch der Goldfisch und der aus Nordamerika stammende Scheibenbarsch zählen zu den Fischen, die sich bei nicht zu hohen Temperaturen wohlfühlen. In den letzten Jahren haben vermehrt Fische aus den Subtropen und topographisch hochgelegenen Regionen der Tropen Einzug in die Kaltwasseraquaristik gefunden, z. B. verschiedene Grundeln und Hochlandkärpflinge.

Die meisten Aquarianer beginnen mit einem sogenannten Gesellschaftsaquarium, in dem mehrere Fisch- und wirbellose Arten gepflegt werden, die dieselben Bedingungen an die Wasserwerte stellen. Bei einigen Fischarten spielen nicht nur die Wasserwerte eine Rolle. So benötigen die als Mbuna bezeichneten endemischen Buntbarscharten des Malawisees Felsenaufbauten, die ihnen Höhlen und Rückzugsmöglichkeiten bieten. Die meisten Pflanzen dagegen werden von diesen Fischen gefressen. Die Felsenbecken, in denen diese Fischarten gepflegt werden, benötigen in der Regel eine besonders gute Filterung, da hier die Stickstoffumwandlung nicht durch Aquarienpflanzen unterstützt wird. Ähnlich wie im Felsenbecken ist auch das Strömungsbecken ein Aquarientyp, in dem spezifische Umweltbedingungen vorherrschen. Hier werden Fische vergesellschaftet, die für ihr Wohlbefinden eine hohe Fließgeschwindigkeit des Wassers benötigen. Erzielt wird dies, indem eine starke Kreiselpumpe ihre Ausströmungsöffnung auf der einen Seite des Beckens hat, ihre Einsaugöffnung aber auf die andere Seite des Beckens verlegt ist. Strömungsbecken, in denen Bachläufe simuliert werden und in denen unter anderem Bachforellen gezeigt werden, sind regelmäßig in großen öffentlichen Schauaquarien zu sehen.

Artaquarien sind vor allem bei erfahreneren Aquarianern verbreitet, die gezielt eine Art pflegen und gegebenenfalls züchten möchten, die besondere Halteanforderungen stellt. Ein Beispiel für eine solche Fischart ist die Australische Wüstengrundel, die sich am wohlsten fühlt, wenn während der Nacht die Wassertemperatur im Aquarium stark abfällt. Nur wenige andere Fischarten kämen über längere Zeit mit diesen starken Temperaturschwankungen zurecht. Andere Arten eignen sich wegen ihrer Aggressivität gegenüber anderen Fischen nicht für eine Haltung im Gesellschaftsbecken. So neigen die im Süßwasser- beziehungsweise im Brackwasseraquarium haltbaren Vertreter der Kugelfische wie Assel-, Zwerg- und Palembang-Kugelfisch dazu, die Flossen anderer Fische zu zupfen. Zumindest die ersten zwei Arten lassen sich unter Umständen gemeinsam mit sehr wendigen Fischarten halten, empfehlenswerter für sie ist jedoch ein Artaquarium.

Biotop-Aquarien sind Aquarien, in denen ein in der freien Natur vorkommender Lebensraum möglichst exakt nachgebildet wird. In solchen Aquarien werden nur Fische aus derselben Region vergesellschaftet. Klassischer Weise nachgebildete Biotope sind ein Bachlauf im Amazonasbecken, ein Sauerwasserteich im Amazonasbecken, ein mittelamerikanischer Küstenbach, ein westafrikanischer Flusslauf, ein Tanganjikasee-Biotop, eine Felsküste im Malawisee, ein südostasiatischer Bachlauf, ein südostasiatischer Bewässerungskanal, eine südostasiatische Flussmündung (Brackwasser) sowie ein Flusslauf in Australien/Neuguinea.

Im Naturaquarium und im Holländischen Pflanzenaquarium haben Aquarienpflanzen gegenüber den Fischarten eine mindestens ebenbürtige Rolle inne. Gelegentlich wird in Holländischen Pflanzenaquarien völlig auf die Haltung von Fischen verzichtet. Für beide Aquarientypen ist eine Düngung mit CO die Regel, um ein optimales Pflanzenwachstum sicherzustellen.

Die kleinsten im Handel erhältlichen Aquarien haben einen Wasserinhalt von 4,5 Liter. Aquarien mit einer Füllmenge von unter 54 l werden auch als Nano-Aquarien bezeichnet. Sie sind erst ab ca. 30 l in ganz speziellen Fällen für bestimmte Fische geeignet. In ihnen werden allerdings meistens Garnelen oder andere Wirbellose gepflegt und – falls es sich dabei um Meerwasser handelt – auch Weichkorallen, Krustenanemonen, winzige Krebstiere oder Stachelhäuter. Bei solch kleinen Aquarien ist eine sehr genaue Kontrolle der Wasserwerte und eine sehr sorgfältige Konzeption wichtig.

Noch kleiner sind die sogenannten Ecospheres, die jedoch nach Ansicht vieler Aquarianer nicht unter den Begriff Aquarium fallen. Bei ihnen handelt es sich um einen geschlossenen Glaskörper, in dem eine tote Koralle, Grünalge und die Garnelenart "Halocaridina rubra" vergesellschaftet sind. Diese Garnele ist so genügsam, dass sie in diesen Kugeln ohne zusätzliche Fütterung bis zu zwei Jahre überlebt (normale Lebenserwartung 10–20 Jahre). Ein Eingreifen oder eine Fütterung durch den Aquarianer ist nicht möglich. Aus diesem Grund lehnen viele Aquarianer diese Form der Aquaristik ab.

Die am meisten verkauften Aquarien mit den Maßen 60 × 30 × 30 cm fassen dagegen 54 Liter Wasser. Diese Aquarien gelten gleichfalls als kleine Aquarien, wenn auch darin eine Reihe von Fischarten gepflegt werden kann. In Aquarien dieser geringen Größe wirken sich Pflegefehler (wie insbesondere zu hoher Besatz, aber auch ausbleibender Teilwasserwechsel oder falsche Fütterung) sehr viel schneller und drastischer aus als in größeren Aquarien. Eine durch Pflegefehler verursachte schnelle Änderung des pH-Werts kann zum Tod der darin gepflegten Lebewesen führen. Aquarien, die mehr als 100 Liter fassen, gelten daher als weniger anspruchsvoll in der Pflege und sind geeigneter für die in der Aquaristik gängigen Fischarten. Daher sind sie für weniger erfahrene Aquarianer eine sinnvolle Größe. Vorgefertigte Aquarien werden vom Fachhandel bis zu etwa 700 Litern Volumen angeboten. Maßgefertigte Aquarien können auch bei Privathaltern diese Größe deutlich übersteigen. Dabei muss jedoch das erhebliche Gewicht eines solchen Aquariums und seine Auswirkung auf die Statik eines Hauses berücksichtigt werden.

Die größten Aquarien finden sich in öffentlichen Schauaquarien. Mehrere Schauaquarien, wie beispielsweise das Shedd Aquarium, das Monterey Bay Aquarium und das Okinawa Churaumi Aquarium haben Aquarien, die 7,5 Mio. Liter Wasser fassen.

Das größte mobile Aquarium Europas fasst etwa 60.000 Liter Wasser und wird im Rahmen von Kongressen oder Ausstellungen gezeigt, kann aber auch von privaten Veranstaltern gemietet werden. Mobile Aquarien sind nicht nur wegen der wechselnden Wasserqualität an den einzelnen Einsatzorten schwierig zu versorgen, sondern auch, weil jeweils ein absolut planer und tragfähiger Untergrund vorhanden sein muss, damit die zentimeterdicken Scheiben nicht reißen.

Ein einfacher Glaskasten reicht als Behausung für aquatische Lebewesen in den seltensten Fällen aus. Um die Umwelt am Herkunftsort der Organismen zumindest so gut nachzubilden, dass sie gut gedeihen und sich eventuell sogar fortpflanzen, wird in der Aquaristik heute in großem Umfang Aquarientechnik eingesetzt.

Für die Aufrechterhaltung geeigneter Lebensbedingungen sind ein Filter, bei der Haltung der meisten Fischarten auch eine Heizung und eine Beleuchtung, nötig. Zusätzlich sind Teilwasserwechsel nötig.

Filter haben im Aquarium die Funktion, Schwebeteilchen, Futter- und Pflanzenreste sowie gelöste Verbindungen aus dem Wasser zu entfernen oder in ungiftige Stoffe umzubauen. Der Aquariumfilter in Süßwasseraquarien kann dabei als Innen- oder Außenfilter ausgeführt sein. Innenfilter sind die am häufigsten verwendete Filterform, ihr Einsatz ist jedoch meist auf kleinere Aquarien beschränkt. Eine der einfachsten Filterformen ist dabei der Hamburger Mattenfilter. Außenfilter werden heute meist als sogenannte Topffilter betrieben; daneben existieren auch Sandfilter oder Rieselfilter, die aber viel Raum beanspruchen. Eine spezielle Art eines Innenfilters ist der Bodenfilter. Dabei wird der Kies vom Bodengrund als Filter verwendet. Solche Filtersysteme sind in Deutschland unüblich und vor allem in Großbritannien und Nordamerika anzutreffen.

Der Abbau von Schadstoffen erfolgt durch Mikroorganismen nach dem Prinzip der Nitrifikation. Die Mikroorganismen sind in einem neuen Filter allerdings noch nicht in ausreichender Anzahl am Filtersubstrat vorhanden. Neu eingerichtete Aquarien werden daher über mehrere Wochen ohne Fischbesatz betrieben, damit sich diese Mikroorganismen etablieren können. Dieser Prozess kann durch den Einsatz sog. Filterstarter beschleunigt werden. Aktivkohlefilter hingegen entziehen dem Wasser auf physikalische Weise (Adsorption) Schadstoffe; ihre Aufnahmekapazität ist begrenzt, weswegen die Aktivkohle regelmäßig erneuert werden muss.

In einem Meerwasseraquarium wird die Filterung über einen Eiweißabschäumer erzielt. Dies ist möglich, weil Meerwasser eine andere Oberflächenspannung hat als Süßwasser. Organisch gelöste Verbindungen, hauptsächlich Stickstoffverbindungen (Eiweiße), aber auch Fette und Öle, sammeln sich an der Grenzfläche zwischen Luft und Wasser an. Durch die Erzeugung von feinen Blasen entsteht ein mehr oder weniger feiner Schaum, der zusammen mit den Verschmutzungen aus dem Aquarium entfernt werden kann. Dieser Vorgang lässt sich auch an der Küste beobachten, wenn die Meeresbrandung Schaum produziert, der sich in Form von Flocken am Strand sammelt.

Bei der Heizung ist heute eine Stabheizung mit Thermostatregelung am gebräuchlichsten. Eine Bodenheizung, bei der entweder eine Heizmatte auf die Bodenscheibe gelegt oder ein Heizkabel im Bodengrund untergebracht wird, ist bei der Aufstellung in beheizten Räumen nicht nötig. Ein Vorteil der Bodenheizung soll eine Zirkulation des Wassers durch den Bodengrund sein. Ein Nachteil ist, neben dem höheren Anschaffungspreis, dass (anders als bei der Stabheizung) ein einfacher Austausch bei einem Defekt nicht möglich ist. Früher wurden Heizmatten auch häufig unterhalb des Aquariums zwischen Bodenscheibe und Schrankplatte verlegt. Diese Art der Bodenheizung hat den Vorteil, dass keinerlei stromführende Teile innerhalb des Aquariums liegen und den Nachteil, dass ein starker Wärmeverlust auftritt, da die Wärme der Heizmatte erst durch die isolierende Bodenscheibe gelangen muss. Die Gefahr eines elektrischen Schlages wird jedoch auch bei den internen Heizmatten bzw. -kabel gebannt, indem die gängigen Heizkabelsysteme mit ungefährlicher Niederspannung arbeiten.

Einige Außenfilter haben auch eine integrierte Heizung, so dass erwärmtes Wasser in das Becken fließt; eine bessere Temperierung bis in die letzten Winkel des Aquariums ist die Folge. Auch hier gilt als Nachteil, dass bei einem solchen Kombigerät das gesamte Gerät ausgetauscht werden muss, wenn entweder die Heizung oder der Filter defekt ist. Bei einem Ausfall des Filters mit integrierter Heizung überleben die Aquarienfische normalerweise die sich verschlechternde Wasserqualität mehrere Tage, aber nicht unbedingt den Temperatursturz.

Zur Haltung von Tieren aus sehr kühlen Gewässern gibt es kompakte Kühlgeräte, die in den Außenfilter-Kreislauf eingebunden werden oder eine eigene Pumpe besitzen.

Bei der Beleuchtung werden häufig Leuchtstoffröhren eingesetzt. Die Verbreitung von Quecksilberdampflampen, auch „HQL-Lampen“ genannt, nimmt heute ab, da Halogenmetalldampflampen, auch „HQI-Brenner“ genannt, bei gleicher Leistung eine höhere Lichtausbeute bieten. Für Becken über 50 cm Höhe sind HQI-Brenner empfohlen, da die Lichtstärke mit zunehmender Tiefe schnell abnimmt. Einen besonderen Reiz versprühen diese beiden letzteren Leuchtmittel durch ihren punktförmigen Lichtaustritt (anders als bei der langgestreckten Leuchtstoffröhre). Dies führt insbesondere bei einer stärkeren Oberflächenbewegung des Wassers zur sogenannten Kaustik, wie man sie vom Tauchen kennt.

Speziell auf die Aquaristik abgestimmte Leuchtstoffröhren ermöglichen die Simulation verschiedener Lichtverhältnisse (z. B. natürliches Tageslicht, Tropensonne, Beleuchtung mit erhöhtem Blauanteil zur Simulation einer Meereswassertiefe). Durch die Auswahl der Beleuchtungs-Spektralfarben wird sowohl das Wachstum von Wasserpflanzen (erwünscht) und Algen (meist unerwünscht) beeinflusst. Röhren mit erhöhtem Rot- und Blauanteil werden gezielt zur Verstärkung der Sichtbarkeit der roten, orangen und blauen Farbtöne von Fischen und Korallen eingesetzt, fördern aber auch das Algenwachstum. Am häufigsten werden Beleuchtungen mit Vollspektrum eingesetzt, die dem Sonnenlicht ähnliche Farbverhältnisse schaffen.

Der Einsatz von elektronischen Beleuchtungssteuerungen ermöglicht die zeitgesteuerte Einstellung verschiedener Lichtstimmungen (Farbe, Helligkeit) und Simulation von Sonnenauf- und -untergängen. Im Regelfall ist aber die Steuerung der Beleuchtung über eine einfache Zeitschaltuhr ausreichend. Wenn Fische gehalten werden, ist lediglich ein Tag-Nacht-Wechsel notwendig. Wasserpflanzen benötigen eine Beleuchtungsdauer von 8 bis 12 Stunden, die durch eine Beleuchtungspause unterbrochen werden kann.

Die Aquarienbeleuchtung wird häufig in die Aquarienabdeckung integriert. Sie muss den dort herrschenden Feuchtraumbedingungen widerstehen und eine Gefährdung durch elektrischen Stromschlag bei Arbeiten am Aquarium ausschließen. Um unnötige und erhebliche Energieverluste durch Verdunstung zu vermeiden, sollte in jedem Fall eine Aquarienabdeckung verwendet werden.

Aus ästhetischen Gesichtspunkten, aber auch zur Beobachtung des Verhaltens nachtaktiver Tiere setzen sich zunehmend Moonlight-Beleuchtungen durch, die nach Abschaltung der Hauptbeleuchtung ein schwaches bläuliches Mondlicht simulieren. Diese Effektbeleuchtungen werden mit LEDs oder Kaltkathodenröhren geringer Leistung realisiert. Die Wirkung auf das nächtliche Verhalten der Fische ist umstritten. Einerseits ist eine absolute Finsternis unnatürlich und kann Stress bei den Tieren auslösen, andererseits kann zu helles „Mondlicht“ verhindern, dass die Tiere zur Ruhe kommen. Vereinzelt wird von verbessertem Laichverhalten unter Moonlight-Beleuchtung berichtet.

Mittlerweile können LED-Röhren T8-Leuchtstoffröhren einigermaßen gut ersetzen. Bei T8-Leuchtstoffröhren ist ein Austausch durch T5-LED-Röhren nur bedingt empfehlenswert, da der Röhrendurchmesser gegenüber T8 entscheidend geringer ist und dadurch nicht sehr viel Fläche für Kühlkörper innerhalb der LED-Röhre zur Verfügung steht, sodass die Gesamtleistung der T5-Röhren stark begrenzt ist. Außerdem muss man beachten, dass es LED-Austauschröhren gibt, die weiterhin mit dem Trafo der Leuchtstofflampe betrieben werden, bei denen nur der Starter durch eine Brücke ersetzt werden muss. Bei diesen Retrofit-Röhren muss man bedenken, dass die Effizienz nicht an die Röhren herankommt, die man direkt mit 230 V betreibt, also an der Fassung vorher die Verkabelung geändert werden muss, bei der der alte Trafo komplett aus dem Stromkreislauf ausgeschlossen wird. Vermehrt tauchen aus diesem Grund fertig verkabelte LED-Leuchtbalken auf, die mit dem ursprünglichen Leuchtbalken & Leuchtstoffröhrendesign nicht mehr viel gemeinsam haben, sich dafür aber gut in Abdeckungen integrieren lassen ohne dabei elektrische Änderungsarbeiten vorzunehmen.

Wasserpflanzen benötigen für ihre Photosynthese zwar das gesamte Lichtspektrum, verwerten aber den roten (um 700 nm) und den blauen (um 450 nm) Spektralbereich. Weiße Leuchtdioden emittieren auch in diesem Spektralbereich.

Ein Thermometer ist notwendig, um die Wassertemperatur gelegentlich zu überprüfen. Ein einfaches, schwimmfähiges Flüssigkeitsthermometer ist dabei ausreichend, auch wenn mittlerweile digitale Aquarienthermometer angeboten werden. Erhältlich sind auch Thermofolien, die von außen an einer Stelle des Aquarienglases aufgeklebt werden. Diese Thermometer, die die Temperatur durch eine unterschiedliche Färbung der Schrift anzeigen, werden gelegentlich als nicht hinreichend genau kritisiert.

Mit einem Kescher oder einer Fischfangglocke werden Fische oder auch frei schwimmende Pflanzenbestandteile aus dem Wasser herausgefischt. Eine Fischfangglocke ist dabei besonders geeignet, wenn die Fische sehr empfindlich sind oder man Jungfischschwärme fangen möchte. Schlauch und Eimer sind nötig, um den Wasserwechsel vornehmen zu können. Ein Mulmsauger ist behilflich, um Mulm aus dem Aquarium zu entfernen. Sinnvoll bei längerer Abwesenheit ist ein Futterautomat, der über einen längeren Zeitraum regelmäßig Futter in das Aquarium abgibt. Mit ihm wird die Versorgung der Fische während eines Urlaubs sichergestellt. Aquarianer, die besonders Wert auf die Pflege ihrer Aquarienpflanzen legen, verwenden außerdem eine langstielige Pflanz-Pinzette und -Schere.
Um lebendgebärende Fische im Aquarium nach dem Ablaichen daran zu hindern, ihren Nachwuchs zu fressen, wird häufig ein Ablaichkasten verwendet.

Für die Bestimmung der Wasserwerte werden zum Teil elektronische Geräte angeboten. Es gibt aber auch einfach anzuwendende Tauch- und Tropftests. Bei Tauchtests werden Teststicks kurz ins Aquariumwasser getaucht. Anhand einer Farbskala kann dann der zu bestimmende Wert gemessen werden. Im Handel erhältlich sind dabei Tauchtests, die gleichzeitig Nitrit, Nitrat, die Gesamthärte, die Karbonathärte und den pH-Wert bestimmen. Wegen ihrer oft sehr hohen Ungenauigkeit werden sie von fortgeschrittenen Aquarianern oft heftig kritisiert. Das im Wasser gelöste Kohlenstoffdioxid lässt sich am einfachsten mit einem CO-Dauertest messen. Auch die Tropftests messen die im Wasser enthaltenen Stoffe mit einer Farbskala. Dabei werden eine bestimmte Anzahl einer Testlösung auf meist fünf Milliliter Aquariumwasser gegeben. Auch sie bestimmen die Wasserwerte nicht so exakt wie eine Laboruntersuchung. Für die meisten Aquarianer sind die Tropftests jedoch hinreichend genau.

Hobbyaquarianer entscheiden sich meist für einen Besatz mit Lebewesen, die mit den Wasserbedingungen zurechtkommen, die das jeweilige Leitungswasser bietet. Sofern die Wasserwerte des Aquariums auch beim Leitungswasser vorliegen, kann dann ein Aquarium nach einem Teilwasserwechsel direkt mit entsprechend temperiertem Leitungswasser aufgefüllt werden. Viele Aquarianer behandeln das Wasser jedoch mit einem Wasseraufbereiter, um die darin befindlichen Schwermetalle zu binden und eventuell vorhandenes Chlor zu neutralisieren. Nur bei sehr stark mit Schwermetallen oder Herbiziden belastetem Wasser ist eine Filterung mit einem Carbonit-Filter notwendig. Die beim biologischen Stickstoffkreislauf entstehenden Nitrate können mit einem Nitratfilter entzogen werden.

Generell lässt sich Leitungswasser so verändern, dass jeder gewünschte Wasserwert erreicht werden kann. So kann sehr hartes Leitungswasser durch eine Umkehrosmose oder durch einen Mischbettfilter auf den gewünschten niedrigeren Härtegrad gebracht werden. Brackwasser- oder Salzwasserbedingungen werden durch die Hinzufügung von speziellen Salzmischungen erzielt.

Die meisten Aquarien haben einen Bodengrund, welcher in der Regel aus Kies oder Sand besteht.

Kies wird am häufigsten als Bodengrund verwendet. Ihn gibt es in unterschiedlichen Korngrößen. Empfehlenswert sind rund geschliffene Steine, damit Fische mit empfindlichen Barteln sich daran nicht verletzen können. Auch empfehlenswert ist, wenn die Kieshöhe vorne geringer ist als hinten. So rutschen Pflanzenreste etc. nach vorne. Dies ist ein Vorteil, da man die Pflanzenreste leicht entfernen kann. Die Kiesgröße ist abhängig von den zu pflegenden Lebewesen. Generell besteht die Gefahr, dass bei zu großer Kiesgröße Futterreste in die Steinlücken fallen, dort für die Fische nicht erreichbar sind und in den Lücken verfaulen, was die Wasserwerte negativ beeinflusst. Kleine Kiesgrößen mit einem Durchmesser von etwa einem Millimeter sind beispielsweise bei Süßwassergarnelen notwendig. Die Tiere sind dann in der Lage, die einzelnen Steine mit ihren Beinen umzudrehen und nach Algenaufwuchs abzusuchen. Aquarianer, die Wert auf gutes Pflanzenwachstum legen, bringen unterhalb der Kiesschicht meist noch eine spezielle Aquarienpflanzerde ein, um die Pflanzen ausreichend mit Nährstoffen zu versorgen. Andere Aquarianer verzichten darauf, da bodenwühlende Schnecken diese Pflanzerde nach oben wühlen können. Stattdessen bringen sie Düngekugeln direkt unterhalb der Pflanzen in den Kies ein. Die Färbung des Kieses ist zu einem großen Teil eine ästhetische Entscheidung. Fische, die in Aquarien mit sehr hellem Kies als Bodengrund gehalten werden, wirken jedoch blasser. Bei Fischarten, die aus schattigen Lebensräumen stammen, kann ein zu heller Bodengrund ein scheueres Verhalten auslösen.

Sand ist bei einigen wenigen Fischarten als Bodengrund notwendig, um ihrer wühlenden Versteckweise entgegenzukommen. Geeignet für Aquarien ist allerdings nur Sand, der keine Einflüsse auf die Wasserqualität hat: Kalkhaltiger Sand härtet zum Beispiel das Wasser auf. Reiner Quarzsand ist am geeignetsten. Sinnvollerweise sollte er keinen zu hohen Feinstanteil aufweisen, um Wassertrübungen und Verbacken zu verhindern. Eine einkörnige Sieblinie ist hilfreich. Damit sich bodenlebende und gründelnde Fische nicht verletzen, sollten die Sandkörner abgerundet und nicht scharfkantig sein.

Aquarien werden häufig mit Steinen, leeren Muschelschalen und Moorkienwurzeln dekoriert. Die Verwendung dieses Dekorationsmaterials dient nicht nur der ästhetischen Befriedigung des Aquarienbesitzers, sondern ist für die im Aquarium gepflegten Lebewesen häufig auch notwendig. So sind beispielsweise viele Cichliden Höhlenbrüter und legen ihren Laich nur an der Oberdecke einer Steinhöhle ab. Welse raspeln das Holz der Moorkienwurzeln ab und Garnelen der Gattung "Caridina" suchen bevorzugt darauf nach kleinem Algenaufwuchs. Einige Fischarten wie der Tanganjika-Schneckenbarsch sind darauf angewiesen, dass ihr Aquarium ihnen leere Schnecken- oder Muschelschalen als Rückzugsmöglichkeit anbietet. Und letztlich bilden, ähnlich wie Wasserpflanzen, solche Dekorationsgegenstände die Markierung von Reviergrenzen für territoriale Arten.

Bei der Auswahl des Dekorationsmaterials ist unbedingt darauf zu achten, dass es auch bei langer Wässerung keine giftigen Stoffe an das Aquarienwasser abgibt oder die Wasserwerte für die gepflegten Arten ungünstig verändert. Ungeeignete Steine können z. B. Mineralsalze, Eisen oder Kalk abgeben.

Energie wird in erster Linie für das Beheizen, die Beleuchtung und die Wasseraufbereitung aufgebracht.

In der folgenden Energiebetrachtung soll ein Standardaquarium (200 Liter) von 100 cm × 40 cm × 50 cm (L × B × H) mit Abdeckung dienen.
In Summe macht das einen Jahresverbrauch von 430 kWh, was etwa ein Viertel eines Singlehaushalts (etwa 1800 kWh) ausmacht. Überschlägig kann man von etwa 2 kWh pro Jahr pro Liter ausgehen.

Wärmeverlust

Mit 47 % ist die Heizung des Aquariums der energieaufwändigste Verbraucher. Dabei sind zwei wesentliche Faktoren ausschlaggebend:

Wichtig bei dieser Betrachtung sind die Oberfläche, die zum Wärmeaustausch zur Verfügung steht, die Temperaturdifferenz des Wassers zur umgebenden Raumluft und der Wärmedurchgangskoeffizient oder U-Wert. Der Wärmedurchgang findet annäherungsweise am Deckel und den Seitenwänden statt, der Boden ist in der Regel schon etwas gedämmt und soll in der Betrachtung vernachlässigt werden. Eine Temperaturdifferenz von 5 K (25 °C Wassertemperatur und 20 °C Raumtemperatur) wird angenommen.

Eine hohe Temperaturdifferenz wie auch eine große Wärmeaustauschfläche sorgen für hohe Energieverluste.

Verdunstung, auch Evaporation genannt, entzieht dem Wasser permanent Energie, die wie bei dem oben genannten Wärmedurchgang durch Heizen nachgeführt werden muss. Nun kann durch Anbringen einer Aquarienabdeckung der Luft- und damit Feuchtigkeitsaustausch deutlich reduziert werden. Die mit Wasser gesättigte Luft unterhalb der Abdeckung kann keine weitere Feuchtigkeit aufnehmen und unterdrückt damit die Verdunstung. Allerdings wird und soll eine Abdeckung nicht luftdicht sein, weshalb nach wie vor Verdunstung stattfinden wird, aber in deutlich eingeschränktem Umfang.

Folgende Beispielrechnung macht das deutlich. Pro Quadratmeter Wasseroberfläche kann unter der Annahme von natürlich konvektivem Luftaustausch ohne Aquarienabdeckung ein Wasserverlust durch Verdunstung von etwa 5–6 Litern pro Tag angenommen werden. Mit Deckel reduziert sich das auf 1–2 Liter pro Tag. Bei einer Wassertemperatur von 25 °C gehen etwa 2441 kJ pro kg Wasser verloren (entspricht 0,684 kWh pro Tag pro Liter Wasser).

Lichtausbeute

Anhängig vom verwendeten Leuchtmittel ist die Energieeffizienz entsprechend gut (s. Beleuchtung). In dem betrachteten Fall macht das Licht einen Energieanteil von 38 % aus. Der Einsatz von LED-Leuchten würde den Energiebedarf bei gleicher Lichtleistung auf etwa 90 % reduzieren (ca. 20 Watt). Es ist auch zu berücksichtigen, dass bauartbedingt die Wärmequelle fehlt.

Gesamtenergieverluste

In der Praxis kommt es sehr stark auf die Größe des Aquariums und die verwendete Technik an. Der Hauptanteil der Energieverluste entsteht durch die auszugleichenden Wärmeverluste und die Beleuchtung. Bei einem Aquarium mit 500 Litern Inhalt (brutto) werden Energieverbrauchswerte von ca. 2–3 kWh/Tag gemessen.

Bei den im Aquarium kultivierten Pflanzen handelt es sich entweder um Wasserpflanzen, die stets submers wachsen oder um auch emers gedeihende Sumpfpflanzen. Nach Schätzungen der Aquarienpflanzenexpertin Christel Kasselmann werden zwischen 100 und 150 Pflanzenarten als Aquarienpflanzen angeboten. Lediglich 30 Arten davon sind echte Wasserpflanzen.

Auch wenn ein Aquarium wie etwa ein Felsenbecken ohne Aquarienpflanzen betrieben werden kann, spielen sie in den meisten Aquarien eine wichtige Rolle. Als „Abfallprodukt“ der Photosynthese produzieren sie während der Beleuchtungsphase des Aquariums Sauerstoff, der von den anderen im Aquarium gehaltenen Lebewesen zur Atmung benötigt wird. Sie reduzieren außerdem das im Aquarium unerwünschte Algenwachstum, indem sie den Algen bei gutem Wuchs Nährstoffe entziehen. Schadstoffe wie Phosphat, Nitrat und Ammonium werden gleichfalls durch Pflanzen abgebaut. Darüber hinaus bieten sie den Tieren Verstecke und Reviergrenzen.

Die für Holländische Pflanzenaquarien typischen Pflanzenstraßen werden mit Pflanzen wie Bachburgel, Kardinalslobelie und Mooskugeln erzielt. Besitzer von diesen Aquarien bemühen sich häufig um die Kultur besonders anspruchsvoller Aquarienpflanzen wie Zungenblatt, Cognacpflanze, Wasserhaar und Tausendblatt. Im Japanischen Naturaquarium spielen besonders häufig kleinwüchsige und zierliche Arten eine Rolle, die im Vordergrund gepflanzt werden. Dazu zählen das Teichlebermoos, das Zungenblatt und die Nadelsimse. Häufig werden in diesen Aquarientypen auch Pflanzen wie beispielsweise der Javafarn auf Steine oder Wurzeln aufgebunden. In Aquarien mit Fischen, die zarte Pflanzen fressen, hat sich insbesondere das Zwergspeerblatt bewährt, dessen harte Blätter von kaum einer Fischart geschädigt werden.

Kohlendioxid (CO), das die Pflanzen für ihr Wachstum benötigen, ist im Wasser nur in geringem Maße gelöst. Der Bedarf an Kohlendioxid ist je nach Pflanzenart unterschiedlich. Es gibt eine Reihe von Aquarienpflanzen, wie beispielsweise die Dichtblättrige Wasserpest, der Indische Wasserfreund und der Indische Wasserstern, deren CO-Bedarf ohne zusätzlich Zufuhr sichergestellt werden kann. Alle drei Pflanzenarten werden für ein neu eingerichtetes Aquarium besonders häufig empfohlen. Da sie gut anwachsen und schnellwüchsig sind, entziehen sie in der Anfangsphase eines Aquariums, in der die Gefahr eines übermäßigen Algenwachstums besonders hoch ist, den Algen effektiv die Nahrungsgrundlage. Wegen ihrer einfachen Kultivierung werden sie außerdem regelmäßig als preisgünstige Bundware angeboten.

Sehr viele Aquarienpflanzen benötigen für ein gutes Gedeihen einen höheren Kohlendioxidgehalt im Wasser. Diese Pflanzen sind auf eine Kohlenstoffdioxid-Düngung angewiesen. Es gibt eine Vielzahl unterschiedlicher Möglichkeiten, diese Düngung zu erzielen. Dies reicht von einer auf der alkoholischen Gärung basierenden Gärungsanlage bis zum (ggf. computergesteuerten) Druckgasflaschensystem.

Starke Oberflächenbewegung des Wassers und die Verwendung von Ausströmersteinen (Sprudelsteinen) treibt in starkem Maße das im Wasser gelöste Kohlenstoffdioxid aus. Im Glauben, mehr Sauerstoff für die Fische ins Wasser zu bringen, wird also durch Membranpumpen und plätschernde Filtereinläufe das Gegenteil erreicht: Das im Wasser gelöste Kohlenstoffdioxid wird verstärkt über die Wasseroberfläche an die Luft abgegeben und den Wasserpflanzen als Lebensgrundlage entzogen, die darauf mit vermindertem Wachstum reagieren, weniger Sauerstoff für die Fische erzeugen und den Algen mehr Nährstoffe überlassen. Kohlenstoffdioxid-reiches Aquarienwasser ist deshalb entgegen landläufiger Anfängermeinung nicht sauerstoffärmer, sondern sauerstoffreicher.

Nahezu alle im Handel angebotenen Aquarienpflanzen stammen aus Gärtnereien, die sich auf die Zucht von Aquarienpflanzen spezialisiert haben. Vor allem aus der Gattung der Schwertpflanzen werden immer wieder neue Varietäten herangezogen, die sich vor allem durch interessante rot bis rotbräunlich gesprenkelte Blattfarben auszeichnen.

Die ersten in Aquarien gehaltenen Fischarten stammten meist aus heimischen Küsten- und Binnengewässern. Zu den wenigen fremdländischen Aquarienfischen, die bereits sehr frühzeitig in der aquaristischen Literatur genannt wurden, zählt der Goldfisch. Einer der ersten auffällig gefärbten Tropenfische, die gezielt und regelmäßig importiert wurden, war der Paradiesfisch. Für ihn ist belegt, dass er 1876 erstmals in Deutschland gehalten wurde. Diese Art zählte zu den wenigen Fischen, die den langen Transport aus Asien überstehen konnten. Paradiesfische sind nicht nur auf die Kiemenatmung angewiesen, sondern sind als Labyrinthfische auch in der Lage, über ihr sogenanntes Labyrinthorgan atmosphärischen Sauerstoff zu atmen.

Aufgrund der Nachfrage in Europa folgten sehr schnell weitere Fischarten, die aus Regionen wie Manaus in Brasilien, Bangkok in Thailand oder Jakarta importiert wurden. Meist handelte es sich wie beim Paradiesfisch um Labyrinthfische. Aufgrund des langwierigen Transports setzten sich jedoch in der Aquaristik anfangs nur solche Fischarten durch, die nachgezüchtet werden konnten. Die Mehrzahl der heute im Handel erhältlichen Fische stammt aus Nachzuchten. Wildfänge werden jedoch nach wie vor regelmäßig angeboten, da einige Fischarten nach wie vor in Gefangenschaft nicht nachzüchtbar sind oder ihr Fang wirtschaftlicher ist als die Zucht. Über Wildfänge werden außerdem bis heute neue Arten in die Aquaristik eingeführt. Das gilt insbesondere für Panzerwelse, Buntbarsche und Salmler.

Wie viele andere Hobbys unterliegt auch die Beschäftigung mit der Haltung von Lebewesen in Aquarien Trends und Moden. In der Süßwasseraquaristik gilt der Diskus vielen Aquarianern immer noch als der „König der Fische“. Ebenfalls sehr beliebt ist die Haltung von Skalaren und der sogenannten Mbunas. Viele Aquarianer halten außerdem Welse, wobei eine starke Nachfrage insbesondere nach solchen Arten besteht, die wie der Großkopf-Bratpfannenwels oder der Blauer Antennenwels eine ausgefallene Körperform haben. Bei einigen neu eingeführten Welsarten wie dem sehr auffallend gefärbten Zebrawels drückt sich diese Nachfrage auch in einem im Vergleich zu anderen Fischarten hohen Preis aus.

Nach ihrer Nahrungsweise können Fische in fischfressende Piscivoren, insektenfressende Insektivoren, pflanzenfressende Herbivoren und Planktonfresser unterteilt werden. Meist lässt schon die Stellung des Fischmauls darauf schließen, welche Nahrungsnische die jeweilige Art nutzt. Fische mit oberständigem Maul sind in der Regel an der Wasseroberfläche fressende Fische. Ein unterständiges Maul ist meist bei Bodenfischen zu finden, die in Mulm oder auf steinigem Untergrund nach Nahrung suchen oder Algenaufwuchs abraspeln.

Zur Fütterung der Fische bietet die Tierfutterindustrie heute ein großes Spektrum an Zierfischfutter an. Trockenfutter, das in Flocken-, Granulat- oder als Futtertabletten angeboten wird, gibt es in unterschiedlichen Zusammensetzungen, um den unterschiedlichen Nahrungsanforderungen der einzelnen Fischarten gerecht zu werden. Nicht alle Fischarten nehmen jedoch Trockenfutter an. Wasserflöhe, Mückenlarven, Bachflohkrebse und andere aquatische Kleintiere dienen vor allem der Fütterung von überwiegend räuberisch lebenden Fischarten. Diese Futtertiere werden lebend, getrocknet, gefriergetrocknet oder tiefgefroren („Frostfutter“) angeboten. Arten wie z. B. Schützenfische sind zwingend auf Lebendfutter angewiesen. Lebendfutter wie Grindalwürmer und Salinenkrebse können selbst herangezogen werden. Der Handel bietet aber eine Reihe von Futtertieren an.

Vor allem in den letzten Jahren hat in der Süßwasseraquaristik die gezielte Haltung von Wirbellosen zugenommen. In der Meerwasseraquaristik wird sie schon länger praktiziert.

Turmdeckel- und Posthornschnecken sind schon seit längerer Zeit in Aquarien zu finden. Insbesondere die grabenden Turmdeckelschnecken belüften den Bodengrund und verwerten organische Pflanzenstoffe. Obwohl sie damit ein gesundes Pflanzenwachstum sicherstellen, waren sie in der Aquaristik immer eher „Beitiere“. Der Handel bietet daneben immer mehr an auffallend gefärbten oder geformten Schnecken wie die unterschiedlichen Arten der Apfelschnecken oder die Zebrarennschnecke an. Wegen ihres hohen Stoffwechsels müssen sie bei der maximal im Aquarium haltbaren Tierzahl mit eingerechnet werden. Die Mindestkantenlänge für eine Apfelschnecke beträgt 60 cm.

Seit einigen Jahren werden auch vermehrt Süßwasser- und Felsengarnelen im Süßwasseraquarium gehalten. Ihre Popularität ist vor allem auf den japanischen Fotografen und Aquarianer Takashi Amano zurückzuführen, der in seinen Aquarien Yamatonuma-Garnelen zur Algenkontrolle einsetzte. Dies hat zu einer vermehrten Einführung anderer Garnelenarten geführt.

Im Handel angeboten werden außerdem einige Krebsarten. In der Süßwasseraquaristik sind es vor allem Flusskrebsarten, die zur Fauna Australiens oder Nordamerikas gehören. Diese in ihrem Habitus einem Hummer ähnlichen Krebsarten werden vorwiegend in Farmen für Speisezwecke gezüchtet. In den Zoofachhandel gelangen vor allem blau gefärbte Exemplare wie der Yabbi, der Marron und der Red Claw Hummer. Alle drei Arten benötigen sehr große Aquarien, können nur mit großen Fischen vergesellschaftet werden und vergreifen sich auch an den Aquarienpflanzen. Beliebt ist deshalb auch der im Vergleich dazu winzige Orange Zwergflusskrebs. Krabben dagegen werden selten in reinen Aquarien gepflegt; sie benötigen bis auf wenige Ausnahmen für ihr Wohlbefinden auch immer einen ausreichend großen Landteil und werden daher in Aquaterrarien gehalten.

Seltene Pfleglinge im Aquarium sind Amphibien. Zu den häufiger im Aquarium gepflegten Arten zählen Krallenfrösche, Zwergkrallenfrösche, der Japanische Feuerbauchmolch, der Axolotl sowie die Schwimmwühle.

Algensporen sind nur wenige Mikrometer groß. Sie werden beim Um- und Einsetzen von Fischen und Pflanzen sowie beim Wasserwechsel mit eingeschleppt. Vermehren sie sich explosionsartig, ist dies ein Hinweis darauf, dass die Stickstoffumwandlung im Aquarium nicht ausreichend gut funktioniert.

Besonders in der Startphase eines Aquariums können sich Blaualgen (Cyanobakterien) stark vermehren, die einen meist blaugrünen, gelegentlich auch schwärzlich-purpurnen oder bräunlichen Überzug über Pflanzen und Steinen bilden. Blaualgen wachsen sehr schnell und können Pflanzen, die noch nicht gut angewachsen sind, unter sich ersticken. Eine mechanische Entfernung ist zwar sinnvoll, allerdings können selbst aus kleinsten Rückständen wieder große Algenmengen nachwachsen.

Zu den Rotalgen zählen die sogenannten Bart-, Pinsel- und Pelzalgen. Sie sind von einer schmutzig grünen bis schwärzlichen Farbe. Die namensgebende Rotfärbung ist dann sichtbar, wenn man die Algenfäden in Alkohol legt. Rotalgen werden allerdings im Gegensatz zu den Blaualgen von einer Reihe von Fischarten gefressen. Kieselalgen treten dann auf, wenn ein Aquarium nur unzureichend beleuchtet ist und der Sauerstoffwert wegen der dann geringen Assimilationsleistung der Aquarienpflanzen zu niedrig ist. Diese Algen, die einen schmierig-bräunlichen Belag auf Steinen und Wurzeln bilden, verschwinden, wenn die Lichtverhältnisse im Aquarium verbessert werden. Grünalgen stellen dagegen ähnlich hohe Ansprüche an das Wasser wie die Aquarienpflanzen. Sie sind von grüner bis hellgrüner Farbe und wachsen je nach Art watteartig, fadenförmig oder büschelartig und lassen sich einfach mit der Hand oder einer Pinzette entfernen.

Einem übermäßigen Wachstum von Algen in der Einlaufphase eines Aquariums kann vorgebeugt werden, indem man mehrere Wochen wartet, bevor das Aquarium das erste Mal mit Fischen und Wirbellosen besetzt wird. In dieser Zeit wird das Aquarium etwa zehn Stunden am Tag beleuchtet, und auch der Filter läuft bereits ständig. Gleichzeitig wird das Aquarium sehr dicht mit schnellwüchsigen Pflanzen bepflanzt. Die Pflanzen entziehen den Algen die für ihr explosionsartiges Wachstum nötigen Nährstoffe, und im Filter können sich in dieser Zeit die Mikroorganismen ansiedeln, die für den Umbau von Ammonium, Phosphat und Nitrit notwendig sind. Neue Filter können mit speziellen Bakterienkulturen (Fachhandel) geimpft werden, um die biologische Aktivität anzuregen. Bei eingefahrenen Filtern sollten niemals alle Filtermedien gleichzeitig gewechselt/gereinigt werden, damit die angesiedelten Bakterienkulturen nicht vollständig vernichtet werden. Wenn man bereits ein Aquarium hat, kann man einen Teil des Frischwassers im neuen Aquarium durch Aquarienwasser ersetzen und etwas Filterschlamm des eingefahrenen Filters in den Filter des neuen Beckens geben. Das hilft in der Einlaufphase, schneller ein neues Gleichgewicht zu bilden. Besonders Anfänger verwechseln gutgemeinte Sterilität mit der biologischen Sauberkeit, die weniger durch mechanische Filterung und „frisches“ Wasser, als durch aktive Mikroorganismen im Bodengrund und den grobporigen Filtermedien erzeugt wird.

Die Mehrzahl der Aquarienpflanzen sind Sumpfpflanzen, die in Wassergärtnereien emers herangezogen werden. Sie brauchen daher einige Tage, um sich auf das Aquarienmilieu umzustellen. Zu den empfohlenen Pflanzen, die auch bei vorheriger emerser Kultur sehr schnell anwachsen, zählen Indischer Wasserfreund und Wasserstern. Dichtblättrige Wasserpest ist ebenfalls sehr gut geeignet; sie wird submers kultiviert, benötigt aber eine starke Beleuchtung. Sie kann sowohl flutend als auch verwurzelt wachsen; fast immer flutend sind die einheimischen Hornblätter, die auch zu einer Nährstoffreduktion im Aquarium beitragen. Gleiches gilt für Schwimmpflanzen wie die Muschelblumen sowie Wasserlinsen.

Eine Reihe von Fischarten frisst Algen. Als hervorragender Algenvertilger gelten die Siamesische Rüsselbarbe und die Schönflossige Rüsselbarbe. Auch Ohrgitterwelse und Antennen-Harnischwelse werden unter anderem wegen dieser Eigenschaft im Aquarium gepflegt. Die auch von Anfängern häufig gehaltenen Guppys sowie Spitzmaulkärpflinge, zu dem die Zuchtform Black Molly gehört, sind Fische, die bedingt Algenaufwuchs fressen. Eine bestehende Algenplage lässt sich durch Algen fressende Fische kaum bekämpfen, da diese Tiere vorwiegend junge, zarte Algen abweiden. Schnecken sind trotz ihres Rufes als Algenvernichter wenig geeignet, da sie sich selbst durch starke Vermehrung zur Plage entwickeln können und durch ihre Stoffwechselprodukte selbst zur Düngung beitragen. Darüber hinaus sind es Süßwassergarnelen, die wirkungsvoll gegen den Algenaufwuchs vorgehen. Mittlerweile bietet der Handel auch chemische Mittel gegen Algenwuchs an. Diese Mittel können allerdings auch die Pflanzen schädigen.

Gegen Schwebealgen können UV-Lampen in den Wasserkreislauf des Außenfilters installiert werden.

Obwohl Schnecken eine wichtige Funktion im Aquarium innehaben, können sich einige Arten sehr schnell vermehren und im Aquarium überhandnehmen. Zur Plage werden sie oft dann, wenn Fische zu reichlich gefüttert werden, so dass die Schnecken ein sehr großes Nahrungsangebot finden. Sie können über spezielle Schneckenfallen entfernt werden. Auch einige Fischarten fressen junge Schnecken, wobei man jedoch anmerken muss, dass die oft empfohlenen schneckenfressenden Fische besondere Anforderungen stellen, die in vielen Aquarien nicht zu verwirklichen sind (so können z. B. Prachtschmerlen durchaus bis zu 30 cm lang werden, bedürfen mehrerer Artgenossen und benötigen ein Aquarium mit einer Mindestkantenlänge von 150 cm).

Planarien treten gleichfalls gelegentlich in Aquarien auf. Es handelt sich um bis zu mehrere Millimeter große Würmchen, die auf Scheiben und Einrichtungsgegenständen herumkriechen. Während sie erwachsenen Fischen nicht gefährlich werden, können sie jedoch den Fischlaich fressen. Planarien vermehren sich vor allem dann stark, wenn im Aquarium Futterreste und organische Abfallstoffe reichlich vorhanden sind. Chemische Mittel, die Planarien behandeln, lassen auch Schnecken und gegebenenfalls Pflanzen eingehen. Hilfreich kann es sein, wenn das Aquarium für mehrere Tage eine Wassertemperatur von mindestens 35 Grad Celsius aufweist. In dieser Zeit müssen die Fische und „erwünschte“ Wirbellose in einem Ersatzaquarium gepflegt werden.

In einer Folge der Sendung „Abenteuer Diagnose“ wird darauf hingewiesen, dass Aquarien Reservoirs für gefährliche Bakterien sein können.

Der pH-Wert gilt in der Aquaristik als wichtiger „Wasserwert“. Er drückt das Verhältnis zwischen sauren und alkalischen Inhaltsstoffen des Wassers aus; genauer gesagt das Verhältnis von Wasserstoffionen und Hydroxidionen. Herrscht eine Überzahl an Wasserstoffionen, so spricht man von einer Säure (sauer, pH-Wert < 7), überwiegen die Hydroxidionen, so spricht man von einer Base (alkalisch, pH-Wert > 7). Lebewesen haben einen unterschiedlich großen Toleranzbereich für den pH-Wert und können außerhalb von diesem nicht überleben. Die Überprüfung, ob eine Tierart in einem Aquarium geeignete Überlebensbedingungen findet, orientiert sich unter anderem am pH-Wert. Der Toleranzbereich von Pflanzen ist meist etwas größer als der von Tieren.

Der pH-Wert wird auf einer logarithmischen Skala mit Werten zwischen 1 und 14 gemessen. Wasserwerte im Aquarium liegen zwischen 4,5 und 9,5, wobei beides Extremwerte darstellen. Ein pH-Wert von sieben gilt als „neutral“, d. h. weder als alkalisch noch als sauer. Logarithmisch bedeutet dabei, dass Wasser mit einem pH-Wert von sechs bereits 10 mal mehr Säuren als solches mit einem pH-Wert von 7 enthält. Wasser mit einem pH-Wert von fünf enthält bereits 100 mal mehr Säuren als solches mit einem Wert von 7. Sogenannte Schwarzwasseraquarien, in denen südamerikanische Fische wie etwa Skalare oder der häufig gehaltene Rote Neon gut gedeihen, haben beispielsweise pH-Werte im Bereich von 5,5 bis 6,5.

Die meisten Wasserpflanzen gedeihen am besten in Wasser mit einem pH-Wert zwischen 6,2 und 7,2 da bei diesen Werten in aller Regel auch der für das Pflanzenwachstum wichtige Kohlensäurewert höher ist. In Wasser mit einem pH-Wert über 7, das als „alkalisch“ bezeichnet wird, fühlen sich vor allem ostafrikanische Buntbarsche wohl.

Um den pH-Wert zu messen gibt es verschiedene Möglichkeiten:

1. pH-Teststreifen oder Indikatorpapier: Die zu messende Lösung wird auf einen solchen Teststreifen bzw. das Indikatorpapier getropft und die Verfärbung mit einer Tabelle verglichen und somit der pH-Wert ermittelt. Dies ist eine einfache und schnelle Möglichkeit den pH-Wert zu messen, sie ist jedoch relativ ungenau.

2. pH-Tropfentest: Bei dieser Messmethode werden in eine bestimmte Menge einer zu messenden Lösung Tropfen einer Reagenz gegeben, die einen Indikator enthält, und die Verfärbung wird ebenfalls mit einer Tabelle verglichen. Diese Messmethode nimmt etwas mehr Zeit in Anspruch und ist in der Regel auch etwas teurer, aber dafür auch wesentlich genauer als der Nachweis mit Indikatorpapier.

3. elektronisches Messgerät: Hier wird in die zu messende Lösung eine Elektrode getaucht und ein elektronisches Anzeigegerät ermittelt aus verschiedenen Faktoren einen pH-Wert. Der Vorteil dieser Messmethode ist, dass die Lösung auch verfärbt oder anderweitig verunreinigt sein kann.

Die Carbonathärte (KH) bezeichnet die Menge an Carbonat- und Hydrogencarbonat-Ionen. Damit diese Anionen als Carbonathärtebildner fungieren können, muss eine entsprechende Anzahl von Calcium- und Magnesium-Kationen vorhanden sein. Die Carbonathärte stellt im Wasser einen Säurepuffer dar, der eine starke und schnelle Veränderung des pH-Wertes verhindert. Wasser mit einem niedrigen KH-Wert hat im Allgemeinen einen niedrigen pH-Wert, während Wasser mit einem hohen KH-Wert normalerweise alkalisch ist.

Die Gesamthärte (GH) bezeichnet die Summe aller im Wasser gelösten Ionen der Erdalkalimetalle. Zu diesen zählen neben Magnesium und Calcium (ca. 80 %) unter anderem Strontium und Barium. In der Aquaristik spielt die Gesamthärte meist eine geringere Rolle als die Carbonathärte. Normalerweise ist die Karbonathärte geringer als die Gesamthärte.

Mit dem Leitwert wird die Summe aller gelösten Salze im Wasser bezeichnet. Je mehr Salze im Wasser gelöst sind, desto besser ist die Leitfähigkeit. Vollentsalztes Wasser leitet kaum noch Strom (eine geringe Leitfähigkeit ist durch die Eigendissoziation immer gegeben).

Der Leitwert spielt insbesondere dann eine Rolle, wenn Weichwasserfische gezüchtet werden sollen.

Eine Bestimmung des Salzgehaltes und des Redoxpotentials wird nur für Brackwasser- und Meereswasseraquarien benötigt.

Die Dichte des Salzgehaltes wird mit einem Aräometer gemessen. Süßwasser hat bei einer Temperatur von 4 Grad Celsius ein spezifisches Gewicht von 1,0. Tropisches Meereswasser, also das Meerwasser, das normalerweise in einem Meerwasseraquarium nachgebildet wird, hat dagegen bei dieser Temperatur wegen der gelösten Salze und Spurenelemente ein spezifisches Gewicht zwischen 1,020 und 1,027. Brackwasser liegt zwischen diesen beiden Werten. Die in der Aquaristik verwendeten Aräometer sind auf 24 Grad Celsius geeicht.

Das Redoxpotential (rH-Wert) sagt aus, wie hoch das Wasser mit organischen Substanzen belastet ist und welchen Gehalt an Sauerstoff es aufweist. Hohe Werte deuten auf einen hohen Sauerstoffgehalt und einen geringen Gehalt an organischen Stoffen hin. Gemessen wird der rH-Wert mit elektronischen Messgeräten. Bei der Ermittlung der Werte müssen auch der pH-Wert sowie die Wassertemperatur berücksichtigt werden.

Die Stickstoffumwandlung, die durch Pflanzen und Mikroorganismen (einschl. Filter) im Aquarium stattfindet, wird gelegentlich auch als Stickstoffkreislauf bezeichnet. Bei den wenigsten Aquarien liegt jedoch tatsächlich ein Kreislauf vor, bei dem keine Eingriffe mehr notwendig sind. Die Lebewesen im Aquarium sind in der Regel auf Futter angewiesen und zur Nitratreduzierung ist ein regelmäßiger Teilwasserwechsel notwendig.

Über die Kiemen und aus dem Urin der im Aquarium gepflegten Lebewesen sowie dem unverbrauchten Futter wird zunächst das giftige Ammoniak freigesetzt. In nicht zu alkalischem Wasser liegt diese Stickstoffverbindung überwiegend als Ammonium vor. Dieses ist für die Fische und Wirbellosen des Aquariums weniger schädlich. Für Pflanzen stellt es sogar einen wertvollen Dünger dar. Lediglich bei hohen pH-Werten und Temperaturen überwiegt Ammoniak. Für Fische stellt Ammoniak ein starkes Gift dar. Ist Ammoniak im Wasser hoch konzentriert, sind diese nicht mehr in der Lage, über die Kiemen Ammoniak abzuatmen. Sie vergiften sich dann letztlich selbst.

Das Ammonium, das von den Pflanzen nicht als Nährstoff verbraucht wird, wird von Mikroorganismen wie z. B. "Nitrosomonas"-Bakterien, die sowohl im Aquarium als auch im Filter vorhanden sind, in Nitrit abgebaut. Nitrit ist wie Ammoniak für Fische giftig und bereits ab Werten von 1 Milligramm pro Liter tödlich. Ein plötzliches Hin- und Herschießen der Fische im Aquarium, apathisches Verhalten oder hektisches Atmen weisen auf eine mögliche Vergiftung durch Nitrit hin. Nitrit wird allerdings durch z. B. Nitrobacter-Bakterien in der nächsten Stufe zu Nitrat umgewandelt. Dieser Prozess, bei dem unter Verbrauch von Sauerstoff Ammonium zu Nitrit zu Nitrat umgewandelt wird, wird als Nitrifikation bezeichnet. Nitrat ist für Fische nicht giftig. Er sollte jedoch gering gehalten werden, um eine (teilweise) Denitrifikation (s. u.) im Aquarium oder Körper des Fisches zu vermeiden, die letztlich wieder Nitrit freisetzen würde. Nitrat wird daher durch den regelmäßigen Teilwasserwechsel aus dem Aquarienwasser entfernt. Neben der Nitrifikation findet noch ein weiter Prozess statt, bei dem von Mikroorganismen Nitrat als Ersatz für Sauerstoff veratmet wird. Diesen anaerob ablaufenden Prozess bezeichnet man als Denitrifikation. Er findet in den meisten Aquarien kaum statt. Für die Entfernung von Nitrat kann bei Bedarf z. B. ein spezieller Nitratfilter eingesetzt werden.

Die meisten regelmäßig im Zoofachhandel angebotenen Süßwasserfische stammen aus Nachzuchten. Geschätzt wird, dass es jährlich 300 Millionen Zierfische sind, die für den Aquarienhandel herangezogen werden.

Gezüchtet werden die Fische vor allem in Südostasien. Zuchtstationen befinden sich vor allem im Umland von Hongkong, Singapur und Bangkok sowie in geringerem Umfang auf Sri Lanka, Japan und Taiwan. Darüber hinaus gibt es große Zierfischzuchten in Nord- und Südamerika, in einigen südafrikanischen Ländern sowie Israel und Tschechien. Zuchtanlagen für Meerwasserfische sind vor allem seit der Mitte der 1990er Jahre entstanden.

Für den Flugzeugtransport in die Importländer werden die Fische in der Regel in Beutel verpackt, die zu einem Drittel mit Wasser und zu zwei Drittel mit Luft oder Sauerstoff gefüllt sind. Luft wird bei Arten wie Labyrinthfischen und Panzerwelsen verwendet, da diese auch atmosphärische Luft atmen. Für sie wäre reiner Sauerstoff tödlich. Gelegentlich wird dem Wasser auch ein Betäubungsmittel beigegeben, damit der Stoffwechsel der Fische nicht zu hoch ist und das Wasser nicht zu sehr belastet wird. Zierfischexporteure lassen außerdem ihre Fische vor dem Transport einige Tage fasten, um die Wasserbelastung niedrig zu halten. Die Beutel werden dann in Styroporboxen verpackt und an ihre Bestimmungsorte geflogen. Einer der großen Frachtflughäfen, an denen viele Zierfische ankommen, ist der Frankfurter Flughafen. Importeure sind in der Regel Großhändler, die die Fische zunächst in Quarantänebecken halten und in der Regel erst nach sieben bis 14 Tagen in den Verkauf geben.

Nach wie vor werden außerdem weltweit Fische, Pflanzen und Wirbellose aus der freien Natur für den Tier- und Pflanzenexport entnommen. Vor allem in Ländern mit einem niedrigen Pro-Kopf-Einkommen kann dies in ländlichen Regionen für Bevölkerungsteile eine wesentliche Einkommensquelle darstellen. Regelmäßig importiert werden vor allem Fische, die gar nicht oder nur sehr schwer nachzuzüchten sind. Darüber hinaus besteht eine große Nachfrage nach aquaristischen Neuheiten. Die Zahl der Wildfänge wird auf 30 Millionen pro Jahr geschätzt.

Die negativen Auswirkungen der Sammlung von Fischen und Pflanzen werden immer wieder kritisch diskutiert. Als Kritikpunkte werden unter anderem angeführt, dass beim Wildfang von Riff-Fischen Korallenriffe zerstört, sehr viele andere Tierarten als Beifang entnommen und Artbestände so stark geplündert werden, dass Populationen im ursprünglichen Verbreitungsgebiet zusammenbrechen. Sammelexpeditionen sind langwierig, kostspielig und nicht immer erfolgreich. Der Transport zu den Exportzentren kann für die Tiere sehr belastend sein. Viele Aquarianer halten deshalb nur Fische aus Nachzuchten. Unter US-Aquarianern, die sich auf die Meerwasseraquaristik spezialisiert hatten, gaben in einer 1997 durchgeführten Umfrage zwei Drittel der Befragten an, dass sie lieber Fische aus Nachzuchten kaufen würden als Wildfänge. 80 Prozent der Befragten gaben außerdem an, dass nur solche Fische in den Handel gelangen sollten, bei denen entweder durch Fangquoten der Fortbestand der Art vor Ort sichergestellt ist oder die direkt aus Nachzuchten stammten.

Eine artgerechte Haltung von Fischarten im Aquarium ist möglich. Sie setzt wie bei jeder Haltung von Lebewesen ein ausreichendes Wissen voraus. Viele Aquarianer investieren viel Zeit und Geld, um ihren Fischen oder Wirbellosen artgerechte Bedingungen zu bieten. Eine Reihe von Aquarianern halten auch bewusst nur solche Tiere, die nachzüchtbar sind, und unternehmen zum Teil große Anstrengungen, um seltenere Fischarten nachzuzüchten.

Ist kein ausreichendes Wissen vorhanden, können in einem Aquarium sehr schnell Bedingungen entstehen, die als Tierquälerei angesehen werden können. Typische Haltefehler sind:

Im besten Fall reagieren die Fische auf diese Haltefehler mit verzögertem Wachstum und verringerter Farbpracht. Häufiger gehen die Fische allerdings zugrunde, wobei sich das Sterben über mehrere Monate hinziehen kann.

Die Qualität der Beratung in Zoofachhandlungen ist stark personenabhängig und reicht von hoher fachlicher Kompetenz bis hin zum reinen Verkaufsverhalten. Bei einigen Arten werden Zuchtformen angeboten, die aus Sicht einer sehr großen Zahl von Aquarianern Qualzuchten darstellen. Dies sind beispielsweise Goldfische mit teleskopartig vergrößerten Augen oder so stark vergrößerten Flossen, dass ein artgerechtes Verhalten den Tieren nicht mehr möglich ist. Regelmäßig werden außerdem Arten angeboten, die sehr spezielle Halteanforderungen stellen, die die wenigsten Aquarianer erfüllen können. So wird beispielsweise das Silberflossenblatt regelmäßig als Süßwasserfisch angeboten, obwohl der Fisch im ausgewachsenen Zustand ein 1.400-Liter-Aquarium mit Brackwasserbedingungen benötigt.

Eine Reihe von Fischarten, Wirbellosen als auch Wasserpflanzen sind vom Aussterben bedroht oder gelten als in der freien Natur bereits ausgestorben. Tropische Lebewesen haben zum Teil nur sehr kleine Verbreitungsgebiete. So ist der südamerikanische Villavicencio-Zwergbuntbarsch nur in wenigen Gewässern in der Nähe der kolumbianischen Stadt Villavicencio am Fuße der Anden gefunden worden und ist durch Biotopzerstörung bedroht. Endlers Guppy aus der Familie der Lebendgebärenden Zahnkarpfen stammt aus einer einzigen Süßwasser-Lagune im Nordosten Venezuelas. Er gilt in der freien Natur als ausgestorben; als Aquarienfisch wird er jedoch sowohl von Privatpersonen als auch öffentlichen Zoos wie beispielsweise dem Kölner Zoo gezüchtet. Dies gilt auch für eine ganze Reihe anderer Arten. Die Fachzeitschrift "Aquarium Live" veröffentlichte beispielsweise in ihrer Ausgabe vom April/Mai 2006 einen Aufruf von Peter Finke, sich an der Erhaltungszucht der Gattung "Parosphromenus" zu beteiligen. Diese Fische gehören zu den am meisten bedrohten Labyrinthfischen, da ihr Lebensraum durch Waldabholzung bedroht ist. Die in sehr sauren, fast mineralstofffreien Urwaldbächen Südostasiens lebenden Fische sind allerdings in ihrer Haltung sehr anspruchsvoll und werden äußerst selten im Zoofachhandel angeboten. Es existiert aber ein Arbeitskreis, der sich zum Ziel gesetzt hat, das Aussterben dieser Fischgattung zumindest in Aquarien aufzuhalten.

Erhaltungszuchten sind jedoch nicht einfach durchzuführen. In Gefangenschaft gezüchtete Fische können sich sehr schnell in Körperform und Färbung deutlich von denen in freier Natur lebenden Artgenossen unterscheiden.

In Aquarien gehaltene Lebewesen haben immer wieder eine Rolle in den Naturwissenschaften gespielt. Das gilt von den Sauerstoffforschungen von Priestley anhand von Wasserpflanzen im 18. Jahrhundert über die Beobachtungen von Konrad Lorenz über Territorialverhalten am Beispiel des Stichlings im frühen 20. Jahrhundert bis zu den Untersuchungen über Auswirkungen von in Wasser gelösten Schadstoffen heute. Die Aquaristik führt auch dazu, dass Tier- und Pflanzenarten eingeführt werden, die bislang noch nicht wissenschaftlich beschrieben wurden. Für einige Fischfamilien, bei denen die Anzahl der neu entdeckten Arten die Geschwindigkeit übersteigt, mit der diese eingeordnet werden, behelfen sich Aquarianer mit Code-Systemen. So werden beispielsweise noch nicht klassifizierte Harnisch-Welse mit L-Nummern bezeichnet. Die Rückwirkung des mit der Aquaristik verbundenen kommerziellen Interesse auf die Forschung lässt sich auch an einer gänzlich anderen Familie zeigen: Wissenschaftliche Untersuchungen bei im Süßwasser lebenden Garnelen konzentrierten sich vor wenigen Jahren noch auf die für den menschlichen Verzehr in Shrimp-Farmen gezüchteten Felsengarnelen. Erst seitdem es eine Nachfrage für in Aquarien haltbare Arten gibt, hat sich hier die Forschung auch auf die zahlreichen anderen Garnelenarten ausgedehnt.

Geschätzt wird, dass von einhundert deutschen Haushalten etwa vier bis sechs mindestens ein Aquarium pflegen. Etwa 40 Prozent der Aquarianer besitzen mehr als ein Aquarium. Der Industrieverband Heimtierbedarf veranschlagte den Umsatz mit Aquarienbedarf für 2003 mit 189 Millionen Euro. Damit geben Aquarienbesitzer für ihr Hobby mehr aus als Hunde- oder Katzenbesitzer; nicht in dieser Zahl enthalten ist der Umsatz von Wassertieren und -pflanzen. 2003 wurden für mehr als 20 Millionen Euro Zierfische von Großhändlern importiert. Da die Einzelverkaufspreise im Zoofachhandel deutlich über den Importpreisen im Großhandel liegen, wird der jährliche Umsatz auf einen hohen zweistelligen Millionenbetrag geschätzt.

Viele Zoos zeigen auf ihrem Gelände auch Aquarien, wie das Aquarium im Zoologischen Garten Berlin, der eines der ältesten Aquarien in Deutschland besitzt. Der Aquazoo Düsseldorf, der gleichzeitig Naturkundemuseum ist, bemüht sich in seiner Ausstellung, die Adaption von Tieren an ihren Lebensraum deutlich zu machen. Die Sammlung ist nach pädagogischen Aspekten aufgebaut.

Zu den zahlreichen öffentlichen Schauaquarien gehören auch das Deutsche Meeresmuseum in Stralsund, das Ozeaneum Stralsund und das Seewasseraquarium in Wilhelmshaven. Weitere Beispiele sind das Oceanário de Lisboa in Lissabon, das anlässlich der Weltausstellung 1998 errichtet wurde und das als das zweitgrößte Meerwasseraquarium der Welt gilt, oder das 2001 eröffnete "Oceanium" im Zoo von Rotterdam, Diergaarde Blijdorp.

Am 29. März 2014 wurde das mit 48750 m³ Gesamtinhalt größte Aquarium der Welt, das Chimelong Ocean Kingdom eröffnet. Es liegt auf der chinesischen Insel Hengqin unmittelbar westlich von Macau und 70 km südwestlich von Hongkong in der Provinz Guangdong. Es enthält 19 Becken, wurde mit 1,5 Mrd. € Kosten errichtet und setzt 5 Weltrekorde: Größte Acrylglasscheibe (B × H 39,6 m × 8,3 m) als größte Aquariumsscheibe, größte Aquariums-Unterwasserkuppel, größtes Einzelbecken.

Das mit Stand von 2008 noch größte Aquarium der Welt, das Georgia Aquarium, befindet sich in Atlanta, USA. Die Becken fassen mehr als 30 Millionen Liter Wasser. Über 500 verschiedene Spezies, gesamthaft rund 120000 Meeresbewohner, finden Platz in diesem künstlichen Riff. Als Hauptattraktion gelten zwei Walhaie.
Wegen der Haltung solcher Meerestiere werden diese Aquarien häufig kritisiert. Besonders häufig steht die US-amerikanische Sea-World-Kette mit ihren Delphin- und Orca-Shows in der Kritik, da bezweifelt wird, dass derart großen Meerestieren eine artgerechte Haltung geboten werden kann. Die ebenfalls kommerzielle britische Kette Sea Life Centre verfolgte zumindest in Deutschland ein anderes Konzept und arbeitete bis 2006 mit Greenpeace zusammen. Greenpeace nutzte die Räumlichkeiten für eigene Ausstellungen. Die Sea Life Centre zeigten dafür keine Hochseehaie, tropische Fischarten und bedrohte Arten. Diese Verpflichtung verhinderte allerdings auch, dass sich die Sea Life Centre an Erhaltungszuchten beteiligen.

Schauaquarien in kleinerem Maßstab betreiben auch einige Aquarienvereine, die in Deutschland überwiegend im Verband Deutscher Vereine für Aquarien- und Terrarienkunde e. V. organisiert sind. Andere europäische Vereine haben sich überwiegend in der European Aquarium and Terrarium Association zusammengeschlossen.

Geschichte des Aquariums

Aquarien einrichten

Lebewesen im Aquarium

Lexika




</doc>
<doc id="10083" url="https://de.wikipedia.org/wiki?curid=10083" title="Hobby">
Hobby

Ein Hobby (deutscher Plural: "Hobbys") ist eine Freizeitbeschäftigung, die der Ausübende freiwillig und regelmäßig betreibt, die dem eigenen Vergnügen oder der Entspannung dient und zum eigenen Selbstbild beiträgt, also einen Teil seiner Identität darstellt. Das Wort „Hobby“ ist vom englischen "hobby horse" abgeleitet, das mit „Steckenpferd“ in beiden Bedeutungen – Kinderspielzeug und Freizeitbeschäftigung – übersetzt wird. Das hölzerne Steckenpferd trägt seinen Reiter nirgendwohin, weil es in den Händen gehalten wird, entsprechend erwirtschaftet das Hobby kein Einkommen und ist kein Beruf.

Beispiele für verbreitete Hobbys sind Aktivitäten wie Sammeln, Handarbeiten, Basteln, Modellbau, Sport, Kunst oder Weiterbildung.

Der Begriff Hobby hat eine leichte Nähe zum Begriff Spiel. Als Hobby wird eine Tätigkeit indes nur bezeichnet, wenn man für diese Tätigkeit eine im Vergleich zu anderen Freizeitgestaltungen besondere Vorliebe (Präferenz) hat und sie regelmäßig ausübt.

Ein wichtiges Kriterium für die Unterscheidung, was als Hobby oder als Arbeit gilt, ist häufig, aber nicht notgedrungen, ob es als Quelle für den Lebensunterhalt dient. Im allgemeinen Sprachgebrauch werden diejenigen, die sich in ihrer Freizeit mit einem Fachgebiet befassen, Amateure bzw. Ehrenamtliche genannt – als Gegenstück zum beruflichen Fachmann, Profi. Eine ehrenamtliche Tätigkeit muss allerdings kein Hobby sein, sondern kann auch dem Notwendigkeitsdenken entspringen.

Eine Sache als Hobby zu betreiben, kann in gesteigerter Form auch den Charakter haben, Fan einer Sache zu sein, wenn es sich um eine Art Verehrung der Sache handelt.

Das Betreiben eines Hobbys hat oft entspannende oder sonstige nützliche therapeutische Nebenwirkungen. In einigen Fällen allerdings (beispielsweise beim Sammeln) können die Grenzen zwischen Beruf, Hobby und Sucht zu verschwimmen beginnen. Manche Sammlungen können durchaus als Geldanlage gelten, zum Beispiel Kunstsammlungen. Darüber hinaus gibt es Amateure, die auf ihrem jeweiligen Fachgebiet Wissen und Fähigkeiten erwerben, in denen sie professionellen Fachleuten nicht nachstehen und auch einen entsprechenden Ruf genießen.

Nach den Forschungsergebnissen der Stiftung für Zukunftsfragen – eine Initiative von British American Tobacco im Freizeitmonitor 2013 prägen vor allem Medien und Erholung den Freizeitalltag der Bundesbürger. Demnach bleibt Fernsehen die häufigste Freizeitbeschäftigung. Bundesbürger haben durchschnittlich 3 Stunden und 49 Minuten Freizeit pro Werktag. Das sind 14 Minuten weniger als noch 2010.

Der Begriff leitet sich vom englischen "hobby-horse" ab, das auch im Englischen heute meist zu "hobby" (Plural: "hobbies") verkürzt wird, und bezeichnete ursprünglich ein kleines Pferd oder auch ein Pony, was sich auf hobyn/hoby zurückführen lässt, aber auch ein Kinderspielzeug, das Steckenpferd. Später hat auch dieser deutsche Begriff die Zweitbedeutung „Freizeitbeschäftigung“ angenommen.



</doc>
<doc id="10085" url="https://de.wikipedia.org/wiki?curid=10085" title="Franka Potente">
Franka Potente

Franka Potente (* 22. Juli 1974 in Münster) ist eine deutsche Schauspielerin und Schriftstellerin.

Franka Potente wuchs als das ältere von zwei Kindern eines Lehrers und einer medizinisch-technischen Assistentin in Dülmen auf. Sie ging dort auf das Clemens-Brentano-Gymnasium. Ihr italienischer Nachname geht auf den sizilianischen Urgroßvater zurück, einen Dachdecker, der im 19. Jahrhundert nach Deutschland gekommen war.

Mit 17 Jahren war sie als Austauschschülerin in Texas. 1994 machte sie in Dülmen das Abitur und begann in München an der Otto-Falckenberg-Schule eine Schauspielerausbildung, die sie jedoch nach zwei Jahren abbrach. Sie ging nach New York und belegte einen Kurs am Lee Strasberg Theatre and Film Institute. In einer Münchner Bar gab ihr eine Casterin ihre Karte. Potente rief sie eine Woche später an, was zu ihrer ersten Hauptrolle in Hans-Christian Schmids Film "Nach Fünf im Urwald" führte. Der französische Regisseur Henri Helman holte sie 1997 für eine Hauptrolle in "Une vie pour une autre" an der Seite von Line Renaud nach Frankreich.

Den Durchbruch zum Star bedeutete 1998 ihre Titelrolle in Tom Tykwers Film "Lola rennt", der international Aufsehen erregte. Als künstlerisch ergiebig erwies sich 2000 eine weitere Zusammenarbeit mit Tykwer in "Der Krieger und die Kaiserin". Es folgten einige Auftritte in Hollywood-Filmen, wie etwa 2001 "Blow" mit Johnny Depp und 2002 "Die Bourne Identität" mit Matt Damon. Potente, die wegen ihrer US-Karriere nach Los Angeles gezogen war, war jedoch mit der Qualität der ihr dort angebotenen Rollen nicht zufrieden.

2004 zog sie zurück nach Berlin. Im selben Jahr beteiligte sie sich mit dem Punkmusiker Bela B. der Band "Die Ärzte" an der Anti-Pelz-Kampagne der Tierrechtsorganisation PETA. Basierend auf ihrem Briefwechsel mit dem Kollegen Max Urlacher während der Zeit in Los Angeles erschien im September 2005 ein Buch unter dem Titel "Los Angeles – Berlin. Ein Jahr." Für den Kurzfilm "Der die Tollkirsche ausgräbt" schrieb sie innerhalb von drei Tagen ein Drehbuch und führte später auch Regie. Der 43-minütige Film handelt von einer komödiantischen Liebesgeschichte am Tag des Waffenstillstands 1918 und verwendet Erzählweisen des frühen Stummfilmkinos. Die Uraufführung fand auf der Berlinale 2006 statt. Im selben Jahr war sie in Oskar Roehlers Michel-Houellebecq-Verfilmung "Elementarteilchen" zu sehen und drehte an der Seite von Eric Bana den Film "Unter der Sonne Australiens", das Regiedebüt des australischen Schauspielers Richard Roxburgh. Für den Film wurde sie 2007 für den AFI-Award des Australian Film Institute als beste Hauptdarstellerin nominiert.

Ab Mitte der sechsten Staffel der US-amerikanischen Serie "The Shield" spielte sie die Tochter eines armenischen Mafioso, die in dessen dunkle Machenschaften verwickelt ist. 2008 war sie in Steven Soderberghs Film "Che – Guerrilla" über das Leben Ernesto Che Guevaras die deutschstämmige Guerillakämpferin Tamara Bunke. In der von Bernd Eichinger geplanten Verfilmung des erfolgreichen Historienromans "Die Päpstin" war Potente ursprünglich für die Titelrolle der Päpstin Johanna vorgesehen, solange Volker Schlöndorff Regisseur war. Im Mai 2008 wurde jedoch bekanntgegeben, dass die Rolle an ihre Kollegin Johanna Wokalek ging.

2008 stand Potente für die von Wolfgang Panzer inszenierte Fernsehadaption "Die Brücke" des gleichnamigen Antikriegsromans "Die Brücke" von Manfred Gregor vor der Kamera, einer Neuverfilmung des erfolgreichen Kinofilms "Die Brücke" von Bernhard Wicki aus dem Jahr 1959. Ebenfalls 2008 stellte das Filmfest München den Film "Die Hetzjagd" vor, in dem sie die Nazijägerin Beate Klarsfeld spielt.

Im Mai 2009 veröffentlichte sie zusammen mit ihrem Personal Trainer Karsten Schellenberg ihr zweites Buch "Kick Ass – Das alternative Workout". Im September 2009 war Potente zum Auftakt der 6. Staffel der Serie "Dr. House" als Gaststar der Doppelfolge „Broken“ in einer größeren Rolle zu sehen. Im August 2010 erschien ihr erster Erzählband „Zehn“. Im September 2010 spielte sie als Gaststar in der US-Serie "Psych" in der Folge „One, Maybe Two, Ways Out“ die Rolle der US-Spionin Nadia. 2012 übernahm sie die Rolle der Anne Frank in der Serie "American Horror Story". Im Jahr 2014 spielt sie die Eleanore Nacht an der Seite von Diane Kruger und Demián Bichir in der US-Serie "The Bridge – America".

Franka Potente war von 1998 bis 2002 mit dem deutschen Filmregisseur Tom Tykwer liiert, später mit dem US-amerikanischen Schauspieler Elijah Wood, mit dem sie den Film "All I Want" drehte, sowie mit dem amerikanischen Geschäftsmann Dio Hauser. Seit 2012 ist Potente mit dem US-amerikanischen Schauspieler Derek Richardson verheiratet, mit dem sie in Los Angeles lebt. Sie haben zwei gemeinsame Töchter, die 2011 und 2013 geboren wurden.

Schauspielerin
Regisseurin

Autorin

Singles






</doc>
<doc id="10086" url="https://de.wikipedia.org/wiki?curid=10086" title="Einheit">
Einheit

Einheit bedeutet:

Organisatorische Zusammenfassungen von Personal und Material:

in der Philosophie:

in der Politik:

Zeitschriften:

Zusammenschlüsse:
Siehe auch:


</doc>
<doc id="10088" url="https://de.wikipedia.org/wiki?curid=10088" title="Immunologie">
Immunologie

Die Immunologie oder Immunbiologie ist die Lehre von den biologischen und biochemischen Grundlagen der körperlichen Abwehr von Krankheitserregern wie Bakterien, Viren und Pilzen sowie anderen körperfremden Stoffen wie beispielsweise biologischen Toxinen und Umweltgiften, und darüber hinaus von Störungen und Fehlfunktionen dieser Abwehrmechanismen. Sie ist damit eine Teildisziplin der Biologie. Forschungsgegenstand ist das Immunsystem, ein System von zellulären und molekularen Prozessen, welche die Erkennung und Inaktivierung von Krankheitserregern und körperfremden Substanzen realisieren. Diese Prozesse werden unter dem Begriff Immunantwort zusammengefasst. Aufgrund der zentralen Rolle des menschlichen Immunsystems bei einer Vielzahl von Erkrankungen ist die Immunologie in der Medizin für das Verständnis, die Prävention, die Diagnostik und die Therapie von Krankheiten von großer Bedeutung. 

Es gibt verschiedene Teilgebiete der Immunologie. Die Immunchemie untersucht die Struktur von Antigenen, Antikörpern und die chemischen Grundlagen der Immunreaktionen. Die Immungenetik untersucht die genetische Variabilität von Immunreaktionen, bzw. die Mechanismen der Erzeugung von Antikörpern, T-Zell-Rezeptoren und antigenpräsentierenden Komplexen. Die Immunpathologie und die klinische Immunologie untersuchen Störungen des Immunsystems, die beispielsweise im Falle von Allergien, bei der Bildung von Tumoren und bei Autoimmunkrankheiten auftreten.

Die ältesten bekannten Aufzeichnungen, die Hinweise auf immunologisch relevante Phänomene enthalten, stammen aus dem Jahr 430 vor Christus. Der Geschichtsschreiber Thukydides stellte damals während der sogenannten Attischen Seuche in Athen zur Zeit des Peloponnesischen Krieges fest, dass nur Menschen für die Versorgung der Erkrankten in Frage kamen, welche die Krankheit selbst bereits durchgestanden und überlebt hatten. Aus der Zeit um das Jahr 100 vor Christus sind erste Berichte aus China zu einer gezielten Übertragung der Pocken auf gesunde Menschen zum Zweck der Vorbeugung bekannt. Weite Verbreitung erlangte dieses Verfahren, bei dem Eiter von leicht Erkrankten mit einer Nadel auf Gesunde übertragen wurde, unter der Bezeichnung „Variolation“ seit dem 15. Jahrhundert vor allem in China, Indien und der Türkei. Durch die Ehefrau des britischen Botschafters in Konstantinopel, die ihren Sohn auf diese Weise impfen ließ, gelangte die Variolation ab etwa 1722 nach England und verbreitete sich in den folgenden Jahren auch im Rest Europas.

Zur gleichen Zeit stellte der englische Landarzt Edward Jenner fest, dass Melkerinnen, die sich mit den für Menschen harmlos verlaufenden Kuhpocken infiziert hatten, bei den damals häufig auftretenden Pockenepidemien verschont blieben oder nur leichte Krankheitsverläufe zeigten. Nach intensiver Beobachtung dieses Phänomens impfte er am 14. Mai 1796 einen gesunden achtjährigen Jungen mit Gewebsflüssigkeit, die er einer Pustel von einer mit Kuhpocken infizierten Milchmagd entnommen hatte. Nachdem der Junge den leichten Verlauf der Kuhpocken überstanden hatte, infizierte ihn Jenner mit echten Pocken. Auch diese Infektion überstand der Junge ohne schwerwiegende Symptome. Im Vergleich zur Variolation bot Jenners Verfahren einige entscheidende Vorteile: Die mit Kuhpocken geimpften Personen wiesen nicht die für Pocken typischen Pusteln und die daraus resultierenden Narben auf, es gab keinen tödlichen Verlauf der Impfung und die geimpften Personen stellten selbst kein Ansteckungsrisiko dar. Edward Jenner gilt deshalb heute als Begründer der Immunologie.

Ein Meilenstein in der Entwicklung der Immunologie, der den Beginn der gezielten Forschung markierte, war die Entwicklung eines Impfstoffes gegen die Tollwut im Jahr 1885 durch Louis Pasteur. Am 6. Juli 1885 impfte er damit den neunjährigen Joseph Meister, der zwei Tage zuvor von einem tollwütigen Hund gebissen worden war. Joseph Meister wurde damit der erste Mensch in der Geschichte der Medizin, der eine Tollwutinfektion überlebte. Innerhalb eines Jahres wurde diese Impfung bei 350 weiteren infizierten Personen angewendet, von denen keiner an Tollwut verstarb. Bereits drei Jahre vorher entdeckte Robert Koch den Erreger der Tuberkulose und kurze Zeit später die Tuberkulin-Reaktion, die auf der Basis der Immunantwort den Nachweis einer Tuberkulose-Infektion ermöglichte. 

1888 entdeckten Pierre Paul Émile Roux und Alexandre Émile Jean Yersin das Diphtherie-Toxin. Zwei Jahre später konnten Emil Adolf von Behring und Shibasaburo Kitasato sogenannte Antitoxine im Serum von Patienten nachweisen, welche die Diphtherie überstanden hatten. Emil Adolf von Behring begann auch damit, diese Antiseren zur Behandlung von Diphtherie einzusetzen. Er erhielt für seine Forschungsergebnisse den 1901 erstmals verliehenen Nobelpreis für Physiologie oder Medizin. Der belgische Bakteriologe Jules Baptiste Vincent Bordet entdeckte 1898, dass eine Erhitzung des Serums auf 55 Grad Celsius zwar kaum Auswirkungen auf die Eigenschaft des Serums hatte, an bestimmte chemische Stoffe zu binden, die bakterienzerstörende Wirkung des Serums ging jedoch verloren. Er postulierte aufgrund dieser Entdeckung die Existenz einer hitzeempfindlichen Komponente im Serum, die für die Wirkung des Serums auf Bakterien notwendig war, und nannte diese Komponente „Alexin“. Paul Ehrlich beschäftigte sich in den folgenden Jahren mit der Untersuchung dieser Komponente und führte den noch heute verwendeten Begriff „Komplement“ ein.

Zum Beginn des 20. Jahrhunderts teilte sich die immunologische Forschung in zwei Betrachtungsweisen. Die Humoralimmunologen, die prominentesten von ihnen Paul Ehrlich und Emil Adolf von Behring, vertraten die Ansicht, dass die Grundlagen der Infektionsabwehr in Substanzen im Blutserum, also den Antitoxinen zu suchen seien. Diese Theorie war um 1900 und in den folgenden Jahrzehnten die vorherrschende Auffassung. Daneben entwickelte sich die Ansicht der Zellularimmunologen, insbesondere basierend auf den Arbeiten von George Nuttall sowie Ilja Iljitsch Metschnikow ab etwa 1883/1884. Metschnikow konnte anhand von Untersuchungen zur Wirkung von weißen Blutkörperchen auf Bakterien die Bedeutung körpereigener zellulärer Prozesse für die Abwehr von Krankheitserregern nachweisen. Wie sich später zeigen sollte, sind beide Aspekte gleichermaßen am Wirken des Immunsystems und an der Immunantwort beteiligt. Es dauerte allerdings bis etwa 1940, bis die Auffassungen der Zellularimmunologen allgemeine Anerkennung fanden und die Annahme, dass Antikörper der Hauptmechanismus der Immunabwehr wären, aufgegeben wurde.

Im Jahr 1901 entdeckte Karl Landsteiner das AB0-Blutgruppensystem und leistete damit einen weiteren wichtigen Beitrag zum Verständnis des Immunsystems. Clemens Peter Freiherr von Pirquet stellte 1906 fest, dass Patienten bei einer wiederholten Gabe von Pferdeserum eine heftige Reaktion auf die zweite Behandlung zeigten. Er prägte für diese Überempfindlichkeitsreaktion den Begriff „Allergie“. Emil von Dungern und Ludwik Hirszfeld veröffentlichten 1910 ihre Ergebnisse zur Vererbung der Blutgruppen und damit erstmals Ergebnisse zur Genetik von Komponenten des Immunsystems. In dieser Arbeit schlugen die beiden auch die Bezeichnung „AB0“ als neue Nomenklatur vor – international verbindlich wurde diese jedoch erst 1928 eingeführt. 1917 beschrieb Karl Landsteiner erstmals das Konzept der Haptene, kleiner Moleküle, die bei Kopplung an ein Protein eine Immunreaktion mit Bildung spezifischer Antikörper auslösen können. Lloyd Felton gelang 1926 die Aufreinigung von Antikörpern aus Serum. In den 1930er Jahren konnte dann Michael Heidelberger zeigen, dass es sich bei Antikörpern hinsichtlich ihrer chemischen Natur um Proteine handelt. Darüber hinaus gelang ihm gemeinsam mit Elvin A. Kabat der Nachweis, dass Antikörper der Gamma-Fraktion der im Serum vorhandenen Globuline entsprechen. Im gleichen Zeitraum entwickelte John Marrack erstmals eine Theorie zur spezifischen Erkennung von Antigenen durch Antikörper.

Peter Alfred Gorer entdeckte in den 1930er Jahren bei Studien mit Mäusen zur Abstoßung von transplantierten Tumoren die H-2-Antigene der Maus und damit den ersten Haupthistokompatibilitätskomplex (MHC). Ebenfalls durch Untersuchungen zur Transplantatabstoßung konnten Peter Medawar und Thomas Gibson wichtige Funktionen von Immunzellen aufklären. Damit begann die endgültige Anerkennung der zellulären Immunologie. Im Jahr 1948 fand Astrid Fagraeus heraus, dass Antikörper durch die B-Zellen im Plasma produziert werden. Ein Jahr später veröffentlichten Frank Macfarlane Burnet und Frank Fenner ihre Hypothese der immunologischen Toleranz, die wenige Jahre später von Jacques Miller mit der Entdeckung der Elimination autoreaktiver T-Zellklone im Thymus bewiesen wurde. 1957 beschrieb Frank Macfarlane Burnet die Klon-Selektionstheorie als das zentrale Prinzip der adaptiven Immunität. 

Der Brite Alick Isaacs und der Schweizer Jean Lindenmann entdeckten 1957 bei der Untersuchung der Auswirkungen von Virusinfektionen auf Zellkulturen, dass die Zellen für die Dauer einer Virusinfektion weitestgehend resistent gegenüber einer zweiten Infektion durch ein anderes Virus waren. Sie isolierten aus den infizierten Zellkulturen ein Protein, das sie Interferon (IFN) nannten. Zum Ende der 1960er und zum Beginn der 1970er Jahre entdeckten dann John David und Barry Bloom unabhängig voneinander den Makrophagen migrationsinhibierenden Faktor ("Macrophage migration inhibitory factor", MIF) und eine Reihe weiterer Substanzen, die von Lymphozyten abgegeben werden. Dudley Dumonde prägte für diese Substanzen den Begriff „Lymphokine“. Stanley Cohen, der 1986 für seine Entdeckung der Wachstumsfaktoren NGF und EGF den Nobelpreis für Physiologie oder Medizin bekam, begann in den frühen 1970er Jahren zusammen mit Takeshi Yoshida, die Funktionen der als Lymphokine bezeichneten Faktoren zu untersuchen. Sie erkannten dabei, dass diese Substanzen zu einer Gruppe von hormon-ähnlichen Botenstoffen gehören, die von vielen verschiedenen Zellen des Immunsystems gebildet werden. Stanley Cohen schlug deshalb 1974 den Begriff „Zytokine“ vor, der sich mit der Entdeckung weiterer dieser Stoffe schnell durchsetzte. Mittlerweile sind neben den genannten Faktoren über 100 weitere Zytokine bekannt und in ihrer Struktur und Funktion detailliert untersucht.

Die Zeit um 1960 wird allgemein als Beginn der modernen Immunologie angesehen. Rodney Porter gelang es zwischen 1959 und 1961, die Struktur von Antikörpern aufzuklären. Zur gleichen Zeit entdeckte Jean Dausset den Haupthistokompatibilitätskomplex des Menschen, den so genannten „Human Leukocyte Antigen“-Komplex (HLA-Komplex). Ab etwa 1960 wurden von einer Reihe von Wissenschaftlern auch die Grundlagen der zellulären Immunologie aufgeklärt, was unter anderem zur Differenzierung und Beschreibung der B- und T-Lymphozyten und der Entdeckung ihrer jeweiligen Funktionen durch Jacques Miller führte. Damit setzte sich die Einteilung der Immunabwehr in einen humoralen und einen zellulären Bereich durch. In den folgenden Jahrzehnten wurden unter anderem die verschiedenen Antikörper-Subtypen entdeckt und hinsichtlich ihrer Funktion untersucht. 1975 beschrieben Georges Köhler und César Milstein die Gewinnung monoklonaler Antikörper. Aufgrund der weitreichenden Folgen dieser Entdeckung für die Grundlagenforschung sowie die Diagnostik und Therapie von Erkrankungen erhielten sie 1984 den Nobelpreis für Physiologie oder Medizin. Weitere wichtige Erkenntnisse betrafen die genetischen Grundlagen der Immunologie wie die Beschreibung der MHC-Restriktion durch Rolf Zinkernagel im Jahr 1974, die Identifizierung von Immunglobulin-Genen 1985 durch Susumu Tonegawa und von T-Zell-Rezeptor-Genen durch Leroy Hood ebenfalls ab etwa 1985.

Zentraler Forschungsgegenstand der Immunologie ist das Immunsystem der Säugetiere. Dabei handelt es sich um ein komplexes System von Molekülen und Zellen, durch das die Erkennung und Inaktivierung von körperfremden Strukturen realisiert wird. Die Reaktionen dieses Systems auf solche Strukturen werden unter dem Begriff Immunantwort zusammengefasst. Die Organe des Körpers, die für die Immunantwort zuständig sind, werden zusammen mit den Lymphgefäßen als lymphatisches System bezeichnet. Für das Funktionieren der Immunantwort ist darüber hinaus der Blutkreislauf von entscheidender Bedeutung.

Die Forschung in der Immunologie befasst sich vorrangig mit medizinischen und klinischen Aspekten der Immunantwort, also beispielsweise ihrer Fehlregulation bei bestimmten Erkrankungen sowie ihrer gezielten Beeinflussung zur Behandlung von Krankheiten. Ein weiteres wichtiges Forschungsgebiet ist die Anwendung von immunologischen Methoden für analytische und diagnostische Zwecke. Die Immunologie lässt sich nach dem untersuchten Teilaspekt, der verwendeten Methodik und der Betrachtungsebene in verschiedene Teildisziplinen untergliedern.

Die zelluläre Immunologie befasst sich mit den Zellen des Immunsystems und den von ihnen ausgehenden Reaktionen. Zu den Zellen des angeborenen Immunsystems gehören beispielsweise die Neutrophilen Granulozyten, die auch als Fresszellen bezeichneten Makrophagen, sowie die natürlichen Killerzellen (NK-Zellen). Das adaptive Immunsystem umfasst auf zellulärer Ebene die B-Lymphozyten und die T-Lymphozyten. Im Gegensatz zum angeborenen Immunsystem kann das adaptive Immunsystem eine spezifische Reaktion gegen bestimmte körperfremde Strukturen ausbilden, allerdings erst nach einem erstmaligen Kontakt. Für das angeborene Immunsystem ist ein solcher Erstkontakt nicht notwendig.

Die humorale Immunologie beschäftigt sich mit den auf Proteinen basierenden Prozessen des Immunsystems. Zu diesen gehört, im Rahmen der angeborenen Immunantwort, das Komplementsystem. Im adaptiven Teil des Immunsystems sind Antikörper für die humorale Immunantwort zuständig. Ein weiteres wichtiges Forschungsthema der humoralen Immunologie sind die Zytokine. Dabei handelt es sich um Proteine, die die Regulation des Immunsystems und die Kommunikation seiner verschiedenen Komponenten steuern.

Die Immunchemie untersucht die Struktur und Eigenschaften von Antigenen und Antikörpern sowie die chemischen Grundlagen der Immunantwort. Eine wichtige Anwendung der Immunchemie sind diagnostische und analytische Verfahren auf der Basis der Antigen-Antikörper-Reaktion, wie zum Beispiel die Immunhistochemie. Die Immungenetik beschäftigt sich mit den genetischen Grundlagen des Immunsystems, also beispielsweise der genetisch bedingten Variabilität von Immunreaktionen sowie den Mechanismen der Erzeugung von Antikörpern, T-Zell-Rezeptoren und antigenpräsentierenden Komplexen. Die Immunpathologie und die klinische Immunologie widmen sich den medizinischen Aspekten der Immunologie.

Aus historischen Gründen beschäftigt sich die Immunologie hauptsächlich mit dem Immunsystem von Wirbeltieren (Vertebraten), insbesondere dem der Säugetiere. Dies liegt vor allem an den medizinischen Ursprüngen der Immunologie und hat dazu geführt, dass auch in Lehrbüchern und anderen Veröffentlichungen die Immunologie oft nur mit der Immunabwehr bei Säugetieren als Forschungsgegenstand dargestellt wird. Ein Teilbereich der immunologischen Forschung widmet sich jedoch auch dem Immunsystem von wirbellosen Tieren (Invertebraten). Dieses ist im Vergleich zum Immunsystem der Wirbeltiere gekennzeichnet durch das Fehlen eines adaptiven Immunsystems und damit durch weitestgehend unspezifische Abwehrvorgänge, durch das Vorhandensein von differenzierten biochemischen Abwehrmechanismen in Form von antimikrobiellen Faktoren sowie durch ausgeprägte anatomische Strukturen zur mechanischen Verhinderung des Eindringens von Krankheitserregern und körperfremden Substanzen. Innerhalb des zellulären Immunsystems der wirbellosen Tiere nehmen phagozytierende Zellen eine zentrale Rolle ein. 

Ziel dieser Forschung ist es zum einen, die Evolution des Immunsystems und damit auch seine Funktionen besser zu verstehen. Durch den Vergleich der Abwehrmechanismen verschiedener Tiere ist es möglich zu erkennen, welche Teilaspekte ihnen gemeinsam sind und wie sich diese entwickelt haben. Man spricht deshalb auch von "vergleichender Immunologie". Weitere Bereiche, auf die sich die Forschung zur Immunologie der Invertebraten auswirkt, sind die Ökotoxikologie sowie die Schädlingsbekämpfung und Hygiene. Innerhalb der biomedizinischen Forschung ermöglicht das Verständnis der Immunabwehr von wirbellosen Tieren, diese in Teilbereichen als Modellorganismen zu nutzen. Einzelne biochemische Komponenten des Immunsystems von Invertebraten lassen sich möglicherweise auch zu therapeutischen und diagnostischen Zwecken einsetzen.

Das Immunsystem ist an einer Vielzahl von Krankheiten und anderen klinisch bedeutsamen Vorgängen direkt oder indirekt beteiligt. Diese lassen sich anhand der zugrundeliegenden Mechanismen unterscheiden.

Bei Infektionen mit Bakterien, Viren, Protozoen oder Pilzen erfolgt im Normalfall eine Abwehr des Eindringens und der Ausbreitung der Krankheitserreger durch das Immunsystem. Unter bestimmten Bedingungen kann die Immunreaktion jedoch versagen oder nur ungenügend sein, so dass sich eine Infektion ausbreitet und vom Immunsystem nicht mehr angemessen kontrolliert wird. Dies kann dazu führen, dass eine Infektion chronisch wird, die Krankheitserreger also ständig im Körper verbleiben und dauerhaft oder schubweise entsprechende Symptome verursachen. Eine schwere generalisierte Infektion, also die Ausbreitung von einem lokalen Infektionsort über die Blutbahn im gesamten Körper, wird als Sepsis bezeichnet. Aufgrund massiver Reaktionen des Körpers verläuft diese oft tödlich.

Den so genannten Autoimmunerkrankungen liegt eine fehlgeleitete Reaktion des Immunsystems gegen körpereigene Strukturen zugrunde. Diese Reaktionen können entweder zur irreversiblen Zerstörung von körpereigenem Gewebe führen oder körpereigene Moleküle wie zum Beispiel Rezeptoren und Hormone in ihrer Funktion beeinträchtigen. Zu den Autoimmunerkrankungen zählen beispielsweise der Diabetes mellitus Typ 1, die Hashimoto-Thyreoiditis, die Myasthenia gravis, der Morbus Basedow sowie die meisten entzündlich-rheumatischen Krankheiten, unter anderem die Rheumatoide Arthritis.

Bei Allergien, auch als Überempfindlichkeitsreaktion bezeichnet, kommt es zu einer überschießenden Reaktion des Immunsystems auf bestimmte körperfremde Strukturen. Voraussetzung für die Entstehung einer Allergie ist ein harmlos verlaufender Erstkontakt mit dem als Allergen bezeichneten Fremdstoff. Durch diesen Erstkontakt kommt es zur so genannten Sensibilisierung, das heißt der Ausprägung einer spezifischen Immunantwort. Jeder erneute Kontakt mit dem Allergen kann dann zu einer übermäßig starken Reaktion des Immunsystems führen. Allergien sind besonders häufig gegen pflanzliche Pollen, Tierhaare, Lebensmittelbestandteile und Medikamente. Eine Mischform aus Allergie und Autoimmunerkrankung ist die Zöliakie, bei der es zu einer Kreuzreaktion auf das in den meisten Getreidesorten enthaltene Kleber-Eiweiß Gluten und bestimmte Strukturen im Dünndarmgewebe kommt.

Zu den Erkrankungen, die durch eine ungenügende Immunabwehr (Immuninsuffizienz) gekennzeichnet sind, zählen beispielsweise das erworbene Immunschwäche-Syndrom AIDS ("Acquired Immunodeficiency Syndrome"), das durch eine Infektion mit dem HI-Virus ausgelöst wird. Schwere angeborene Immunschwächeerkrankungen, bei denen gleichzeitig („kombiniert“) der humorale und der zelluläre Teil des adaptiven Immunsystems betroffen sind, werden unter der Bezeichnung Severe Combined Immunodeficiency (SCID) zusammengefasst. Patienten mit einer angeborenen oder erworbenen Immunschwäche besitzen eine hohe Anfälligkeit für Infektionserkrankungen, die mit fortschreitender Immunschwäche in der Regel auch zum Tod führen.

Auch bei Krebserkrankungen spielt das Immunsystem eine wichtige Rolle. Patienten mit einer Immunschwäche, zum Beispiel durch eine immunsuppressive Behandlung nach einer Organtransplantation oder durch eine HIV-Infektion, zeigen eine deutlich erhöhte Häufigkeit bestimmter Krebserkrankungen. Das Immunsystem ist dabei für die Kontrolle entarteter Zellen verantwortlich, so dass diese inaktiviert werden, bevor ein manifester Tumor entstehen kann. Das Teilgebiet der Immunologie, das sich mit den immunologischen Vorgängen bei der Entstehung, dem Verlauf und der Bekämpfung von Tumoren befasst, ist die Tumorimmunologie. Die Krebsimmuntherapie umfasst eine Reihe immunologischer Therapieansätze.

Von entscheidender Relevanz sind immunologische Prozesse bei der Transplantation von Spenderorganen. Da transplantierte Organe vom Immunsystem als körperfremd erkannt werden, kommt es zu einer entsprechenden Immunantwort. Unbehandelt führt diese zur Abstoßung und damit dem Funktionsverlust des betreffenden Organs. Umgekehrt können aber auch, zum Beispiel bei einer Stammzelltransplantation, in einem Transplantat enthaltene Immunzellen eine Immunreaktion gegen den Empfängerorganismus verursachen, man spricht dann von der Graft-versus-Host-Reaktion. In der Folge ist zum Erhalt des Organs eine lebenslange Behandlung der betroffenen Patienten mit so genannten Immunsuppressiva notwendig, also Medikamenten, welche die kurz- und langfristig vorhandenen Immunreaktionen unterdrücken.

Ähnlich wie bei der Transplantation von fremden Organen oder Geweben ist das Immunsystem auch an der Reaktion des Körpers gegen Implantate entscheidend beteiligt. Implantate bestehen beispielsweise aus Metallen oder Kunststoffen und werden für vielfältige Aufgaben eingesetzt, unter anderem zum vorübergehenden oder dauerhaften Ersatz von Knochen oder Blutgefäßen, als plastische Implantate zur Ausformung bestimmter Körperstrukturen und zum Zahnersatz, sowie zum Ersatz oder zur Unterstützung von körpereigenen Organen bei ihrer Funktion, wie zum Beispiel Cochleaimplantate oder Herzschrittmacher. Da Implantate aus körperfremdem Material bestehen, sind sie vielfältigen Prozessen der Immunabwehr ausgesetzt, insbesondere einer chronisch vorhandenen Entzündungsreaktion. Die immunologische Verträglichkeit dieser Materialien ist damit ein wichtiger Aspekt ihrer Biokompatibilität und trägt entscheidend zur dauerhaften Funktion des Implantats bei.

Eine Reihe von therapeutischen Anwendungen, die auf Erkenntnissen und Prinzipien der Immunologie beruhen, lassen sich unter dem Begriff Immunmodulation zusammenfassen. Dies betrifft alle Therapieansätze, die auf einer gezielten Beeinflussung von bestimmten Prozessen oder Komponenten des Immunsystems beruhen.

Weit verbreitet sind beispielsweise Impfungen, bei denen durch die Gabe von Antigenen das Immunsystem zur Ausbildung einer Immunantwort gegen diese Antigene angeregt wird. Impfungen spielen eine entscheidende Rolle bei der Prävention von Infektionskrankheiten. Darüber hinaus gibt es erste Erfolge hinsichtlich einer Impfung gegen krebsassoziierte Viren wie beispielsweise das humane Papillomvirus. Auf dem gleichen Prinzip wie Impfungen beruht die als Krebsimmuntherapie bezeichnete Sensibilisierung des Immunsystems auf tumorspezifische Strukturen bei Krebserkrankungen.

Ein weiterer Ansatz aus dem Bereich der Immunmodulation wird mit den Begriffen Hyposensibilisierung beziehungsweise „Spezifische Immuntherapie (SIT)“ bezeichnet. Ziel dabei ist, eine so genannte Immuntoleranz des Körpers gegen bestimmte Antigene zu erreichen. Das bedeutet, dass vorhandene Abwehrreaktionen des Körpers gegen diese Antigene verringert werden. Erreicht werden soll dies durch die wiederholte Gabe der entsprechenden Antigene mit schrittweiser Steigerung der Dosis. Von therapeutischer Relevanz ist die Hyposensibilisierung bei allergischen Erkrankungen. Darüber hinaus gibt es Studien zur Anwendung bei Autoimmunkrankheiten.

Unter dem Begriff Immunsuppression werden Therapien zusammengefasst, deren Ziel die Unterdrückung von unerwünschten immunologischen Prozessen ist. Möglich ist dies durch Medikamente, die in verschiedene Prozesse der Immunabwehr eingreifen. Angewandt werden diese Medikamente vor allem zur Verhinderung der Abstoßung von transplantierten Organen. Darüber werden immunsuppressive Therapien auch bei Autoimmunerkrankungen getestet.

Eine Immunstimulation, also die Anregung des Immunsystems und die Verstärkung der Immunantwort, ist ebenfalls möglich. Dazu können beispielsweise bestimmte körpereigene Proteine therapeutisch eingesetzt werden, die eine Rolle bei der Regulation des Immunsystems spielen. Am häufigsten werden hierzu bestimmte Zytokine verwendet. Von Relevanz sind entsprechende Therapien insbesondere bei Virusinfektionen.

Eine weitere wichtige Anwendung immunologischer Prinzipien zur Behandlung von Krankheiten sind therapeutische Antikörper. Dabei handelt es sich um Antikörper, also Globulin-Proteine des Immunsystems, die biotechnologisch hergestellt werden und gezielt gegen bestimmte Strukturen im Körper gerichtet sind. Diese Strukturen, für die vorher eine Relevanz bei bestimmten Erkrankungen nachgewiesen wurde, werden durch die therapeutischen Antikörper in ihrer Wirkung blockiert oder neutralisiert. Oft handelt es sich bei diesen Zielstrukturen um Proteine auf der Oberfläche von Zellen, wie zum Beispiel Transportproteine, Signalproteine oder Rezeptoren, aber auch um lösliche Proteine im Serum wie Zytokine oder Hormone. Therapeutische Antikörper sind mittlerweile unter anderem zugelassen zur Behandlung von verschiedenen Krebserkrankungen, von Autoimmunerkrankungen, von Allergien sowie zur Verhinderung der Abstoßung von Transplantaten.

Antikörper werden darüber hinaus auch als Antiserum gegen bestimmte Giftstoffe eingesetzt. Zur Gewinnung dieser Antiseren werden Tieren wie beispielsweise Pferden kleine Mengen der entsprechenden Gifte injiziert. Diese Tiere entwickeln daraufhin spezifische Antikörper in ihrem Blut, welche die Giftstoffe in ihrer Wirkung neutralisieren. Nach der Gewinnung und Reinigung der entsprechenden Antikörper aus dem Blut dieser Tiere können diese zur akuten Behandlung von Vergiftungen, beispielsweise nach Schlangenbissen, eingesetzt werden. Entsprechend gewonnene Antiseren werden darüber hinaus auch zur sogenannten passiven Immunisierung gegen bestimmte Infektionskrankheiten verwendet, wenn für eine aktive Immunisierung durch eine reguläre Impfung nicht ausreichend Zeit zur Verfügung steht oder kein Impfstoff für eine aktive Immunisierung verfügbar ist. Tierische Antiseren rufen jedoch bei wiederholter Anwendung selbst eine Immunreaktion hervor. Aus diesem Grund wird in der Regel eine aktive Immunisierung bevorzugt, wenn diese möglich ist. Als Notfallmaßnahme erfolgt eine passive Immunisierung bei Verdacht auf eine Tollwutinfektion.

Immunologische Labormethoden spielen eine große Rolle bei der Diagnostik von Erkrankungen und in der biomedizinischen Grundlagenforschung. Als Immunassays werden alle Verfahren bezeichnet, die zum qualitativen oder quantitativen Nachweis von bestimmten Strukturen in Flüssigkeiten die spezifische Erkennung von Antigenen durch Antikörper nutzen. Immunassays werden zur Identifikation von Krankheitserregern ebenso genutzt wie zur Untersuchung von Körperflüssigkeiten auf das Vorhandensein von bestimmten körpereigenen Proteinen, die bei Krankheiten als spezifische Biomarker gelten. Für eine Reihe von Erkrankungen, insbesondere Allergien, Autoimmunerkrankungen und Infektionen, ist als Teil der Diagnose und zur Verlaufskontrolle der Nachweis von spezifischen Antikörpern möglich. Immunassays werden aber beispielsweise auch als Schwangerschaftstests verwendet. Weitere Anwendungen in der Medizin sind die Identifizierung von Giftstoffen und Rauschdrogen, die Überwachung von Arzneistoffen im Körper (Drug monitoring), oder der Nachweis bestimmter Dopingsubstanzen in der Sportmedizin. Außerhalb der medizinischen Diagnostik werden Immunassays beispielsweise in der Umwelt-, Lebensmittel- und Agraranalytik eingesetzt, unter anderem zum Nachweis von Umweltgiften, von Allergenen in Lebensmitteln oder von genetisch veränderten Organismen.

Bei Organtransplantationen, bei der Übertragung von Knochenmark und bei Blutspenden wird durch die molekulargenetische Charakterisierung bestimmter Histokompatibilitätsmarker eine möglichst große Übereinstimmung zwischen Spender und Empfänger sichergestellt. Die Immunhistochemie nutzt Antikörper zum Anfärben spezifischer Strukturen in mikroskopischen Präparaten und ist damit eine wichtige Anwendung immunologischer Prinzipien in der pathologischen Diagnostik. Bei der Durchflusszytometrie und der Magnetic Cell Separation (MACS) werden Antikörper verwendet, um auf Zellen bestimmte Oberflächenstrukturen nachzuweisen und dadurch Zellgemische aufzutrennen oder hinsichtlich ihrer Zusammensetzung zu analysieren. Für die klinische Diagnostik ist dies beispielsweise in der Hämatologie für die Untersuchung der Zellverteilung im Blut von Bedeutung.




</doc>
<doc id="10090" url="https://de.wikipedia.org/wiki?curid=10090" title="Devisen">
Devisen

Devisen (meist in der Mehrzahl gebraucht, selten auch "Devise") ist ein Begriff aus der Volkswirtschaft und dem Finanzwesen für allgemein auf Fremdwährung lautende ausländische Zahlungsmittel (außer Sorten). Speziell im Bankwesen (hier als Pluraletantum, also stets in der Mehrzahl gebraucht) werden hierunter auf fremde Währung lautende, im Ausland zahlbare Forderungen verstanden, meist in Form von Guthaben bei ausländischen Banken, Schecks und Wechseln. Im Devisenhandel der Kreditinstitute wird der Begriff am engsten interpretiert, denn hier reduziert sich der Devisenbegriff auf bei ausländischen Kreditinstituten gehaltene Bankguthaben.

Als Fachbegriff des Finanzwesens begegnet „Devisen“ erst gegen Mitte des 19. Jahrhunderts, zunächst als Bezeichnung für kurzfristig fällige Wechsel mit auswärtigem Zahlungsort, insbesondere in fremder Währung zahlbare Auslandswechsel. Seine Herkunft ist nicht sicher geklärt; offenkundig handelt sich um eine etymologische Dublette des im 16. Jahrhundert aus dem Französischen ins Deutsche entlehnten Wortes "devise", das eigentlich in der Fachsprache der Heraldik einen Wappenspruch bzw. das dafür abgeteilte Wappenfeld (vgl. "diviser", „einteilen“) bezeichnet und heute allgemein im Sinne von „Wahlspruch, Motto, Losung“ gebraucht wird. Die neuerliche Bedeutungserweiterung zu „Wechselbrief“ ist semasiologisch schwieriger zu deuten. Dem "Larousse universel" von 1922 zufolge pflegten Bankhäuser die Auslandswechsel in ihrem Portfolio nach Zahlungsorten zu sortieren: „"on place les effets par devises: ceux sur Londres, sur Bruxelles, sur Rome, etc."“. Der Gebrauch von "devise" entspricht hier dem von „Auszahlung“ im deutschen Börsenjargon („Auszahlung London,“ heute leicht antiquiert), möglicherweise versteht sich das Wort aber zugleich im Sinne von „Kennwort,“ vielleicht aber auch im älteren Wortsinn von „Einteilung.“ Sicher scheint indes, dass dieser neue Wortgebrauch zuerst in Deutschland aufkam und später ins Französische rückentlehnt wurde. Als Bezeichnung für Wechsel ist „Devise“ zumindest zuerst in August Schiebes "Kaufmännischem Handwörterbuch" von 1833 gebucht („ein gezierter und übel angewandter Ausdruck, dessen man sich oft in Cursberichten bedient, wo er so viel als Wechsel bedeutet; z. B. ‚diese Devise (es sei nun Pariser Papier oder anderes) ist gesucht.‘“) und im deutschen Schrifttum der folgenden Jahrzehnte vielfach bezeugt, in Frankreich hingegen auffälligerweise nur in deutsch-französischen Wörterbüchern. Für einen rechtsrheinischen Ursprung spricht auch die eigentümliche Wortwahl im Larousse: dass Devisen hier „"sur"“ London usw. platziert werden, legt eine Übertragung aus dem Deutschen nahe (Wechsel werden „auf“ einen Bankplatz, also etwa „auf London“ gezogen).

Bis zum Ersten Weltkrieg blieb der Begriff fast ausschließlich auf handelbare kurzfristige Auslandswechsel beschränkt, diese nennt etwa Arthur Nussbaum noch 1925 „Devisen im engsten Sinne.“ Die heutige Bedeutung, die auch und insbesondere ausländische Bankeinlagen und andere in Fremdwährung notierte Werte umfasst, verfestigte sich erst in der Zeit der Weimarer Republik und des Nationalsozialismus, einhergehend mit dem Bedeutungszuwachs der staatlichen Devisenbewirtschaftung in der Außen- und Wirtschaftspolitik. In dieser weiter gefassten Bedeutung oder schlicht im Sinne von „Fremdwährung“ ist der Begriff heute auch im Französischen geläufig ("devises"), außerdem im Spanischen ("divisas"), Niederländischen ("deviezen"), Polnischen ("dewizy") und einigen weiteren Sprachen. Im Englischen fehlt eine exakte Entsprechung; die gängige Übersetzung "foreign exchange" oder "forex" bezeichnet streng genommen nicht die Devisen selbst, sondern den Handel damit.

Je nach Untersuchungszweck wird der Devisenbegriff weiter oder enger verwendet. Alle Definitionen erfassen Zahlungsmittel in fremder Währung, die im Ausland zahlbar sind. Zahlungsmittel wiederum sind geldähnliche Forderungsrechte im Rahmen des unbaren Zahlungsverkehrs (Wechsel, Schecks und Zahlungsanweisungen wie Reiseschecks oder Kreditbriefe). Devisen ermöglichen die sofortige (oder zumindest sehr kurzfristige) Geldzahlung in einer fremden Währung und verkörpern damit sofortige Kaufkraft in fremder Währung (oder zumindest Kaufkraft in Form kurzfristig fälliger Forderungen, also mit einer kurzen Selbstliquidationsfrist). Bei Kreditinstituten wird der Devisenbegriff technisch auf „Sichteinlagen in fremder Währung auf den Girokonten ausländischer und inländischer Banken“ reduziert, manchmal sogar auf Sichteinlagen im Emissionsland der Währung eingeengt: „Im Zentrum des Devisenhandels stehen die auf fremde Währung lautenden Sichtguthaben, die bei Banken im Emissionsland der Fremdwährung gehalten werden“
Darüber hinaus werden kurzfristige Termineinlagen und Geldmarktpapiere hinzugerechnet.

Nicht zu den Devisen gehören Sorten, weil sie als Bargeld zu den gesetzlichen Zahlungsmitteln gehören. Sie können jedoch durch Einzahlung auf ein Konto in gleicher Landeswährung in Devisen umgewandelt werden. Weniger liquide Forderungsrechte wie Anleihen in ausländischer Währung werden ebenfalls nicht zu den Devisen gerechnet, insbesondere nicht Anteilswerte wie Aktien.

Ein freier Devisenverkehr ist nur möglich, wenn die betroffenen Währungen einer vollen Konvertierbarkeit unterliegen. Dann hat jeder Besitzer von Devisen das Recht, diese unbeschränkt, insbesondere ohne Rücksicht auf den Verwendungszweck, zum Paritätskurs gegen inländische oder andere ausländische Währungen einzutauschen. Ist jedoch eine Währung lediglich beschränkt konvertierbar, so unterscheidet man folgende Teilaspekte:


Die beschränkte oder völlig fehlende Konvertierbarkeit behindert einen freien Devisen- und Kapitalverkehr, insbesondere den Handel mit Devisen über Börsen oder zwischen Kreditinstituten. Volle Konvertierbarkeit ist ein wichtiges Indiz für so genannte „Hartwährungsländer“.

Devisen – hier begrenzt auf Guthaben bei ausländischen Banken – sind in standardisierten Mengen handelbar. Dies kann über Devisenbörsen, also staatlich genehmigte und organisierte Institutionen, bei denen durch einen amtlichen Makler für notierte Währungen ein amtlich ermittelter Kassakurs als Mittelkurs festgelegt wird, geschehen. Leitbörse in Deutschland war bis zum 31. Dezember 1998 Frankfurt am Main, die börsentäglich gegen 13 Uhr etwaige offene Devisenspitzen unter Mitwirkung der Bundesbank im so genannten "Fixing" ausglich und sodann den amtlichen Devisenkurs festgestellt hatte. Alle Börsenteilnehmer verrechneten zum amtlichen Mittelkurs, während für die Bankkunden eine über dem Mittelkurs liegende Spanne („Spread“) als Briefkurs und eine entsprechend darunter liegende Spanne als Geldkurs ermittelt wurde. Zum Geldkurs kaufen die Banken Devisen ihrer Kunden an, zum Briefkurs werden sie den Kunden verkauft. Damit war der amtliche Devisenkurs der wichtigste Referenzwert für Devisengeschäfte.

Seit der Schließung der deutschen Devisenbörsen durch Einführung des Euro werden als Referenzwerte für Devisen vielfach die so genannten „EuroFX“, bei denen insgesamt 17 Institute gegen 13 Uhr die Mittelwerte ihrer Wechselkurse festlegen und bei REUTERS veröffentlichen, herangezogen. Börsen wie die "NYSE Euronext" greifen bei der Umrechnung von Fremdwährungskontrakten auf die EuroFX-Kurse zu, Kreditkartengesellschaften rechnen Auslandsumsätze auf dieser Basis um, und der Endverbraucher kann mit Hilfe der Veröffentlichung in der Tagespresse seine Abrechnungen kontrollieren. EuroFX hat jedoch zum 1. April 2015 die Ermittlung von Devisenkursen eingestellt.

Nach einer Telefonkonferenz mit den Zentralbanken der Euro-Teilnehmerländer stellt die Europäische Zentralbank (EZB) täglich kurz nach 14 Uhr Wechselkurse, so genannte Referenzkurse, fest. Sie werden jedoch – aufgrund ihrer größeren Marktferne – weniger für Devisengeschäfte als für Statistiken, Bilanzen von Großunternehmen und allgemeine Orientierungshilfe genutzt.

Der größte Teil des weltweit gehandelten Devisenvolumens wurde und wird weiterhin über den Devisenhandel im Rahmen des Interbankenhandels abgewickelt. Hierbei bemühen die Banken nicht Devisenbörsen, sondern schließen bilaterale Devisengeschäfte direkt mit ihren Partnerbanken („Kontrahenten“) ab. Dabei kommt es zu einer individuellen Kursbildung für jedes einzelne Geschäft, die völlig unabhängig vom EuroFX-Kurs auf Grundlage der aktuellen Marktkurse erfolgt. Der EuroFX-Kurs ist allenfalls ein Zwischenergebnis und reflektiert den Durchschnittswert des Interbankenhandels zu einem bestimmten Zeitpunkt.

→ "Hauptartikel: Devisenmarkt"
Durch den Devisenmarkt wird der Tausch inländischen Geldes in ausländisches und umgekehrt ermöglicht und dadurch Kaufkraft von Inlandswährung in Auslandswährung umgewandelt. Der Devisenmarkt ist ein ökonomischer Ort, an dem Devisenangebot und Devisennachfrage aufeinandertreffen. Dies geschieht institutionalisiert durch den Devisenhandel, der Devisengeschäfte als Handelsobjekt zum Gegenstand hat. Hierzu gehören als Grundformen Devisenkassa- und Devisentermingeschäfte. Der Devisenhandel bei Kreditinstituten sorgt im Kundengeschäft für den Ausgleich der Devisenüberschüsse oder -defizite und für die Absicherung vor Währungsrisiken, die aus dem Halten offener Devisenpositionen resultieren können.

Devisengeschäfte, bei denen die Währungen zwei Werktage nach dem Zeitpunkt des Geschäftsabschlusses getauscht werden, heißen „Devisenkassageschäfte“. Hingegen ist ein „Devisentermingeschäft“ ein Devisengeschäft, bei dem der Tausch von Währungen zu einem bestimmten zukünftigen Termin (1, 3, 6, 12 Monate oder mehr) und zu einem bereits bei Geschäftsabschluss vereinbarten Devisenkurs (Terminkurs) vereinbart wird. Devisentermingeschäfte eignen sich zur Sicherung gegen Wechselkursschwankungen.

Zwischen dem Devisenkassa- und dem Devisenterminkurs besteht eine Differenz, die als „Swapsatz“ bezeichnet wird; diese wird in Prozent des Devisenkassakurses ausgedrückt. Liegt der Terminkurs über dem Kassakurs, spricht man von einem „Report“, liegt er darunter, von einem „Deport“. Ausgangspunkt ist das Zinsniveau im In- bzw. Ausland (bei Mengennotierung):

Bei Devisenoptionsgeschäften erwirbt der Käufer das Recht, an einem bestimmten Tag oder auch innerhalb einer bestimmten Frist einen bestimmten Betrag in einer Währung zu einem festgelegten Preis zu kaufen bzw. zu verkaufen. Je nach Inhalt des Geschäfts unterscheidet man Kauf- und Verkaufsoptionen. Der Geschäftspartner des Erwerbers einer Option wird als Stillhalter bezeichnet. Bei einem Devisen-Futures-Kontrakt verpflichtet sich der Erwerber, zu einem bestimmten Betrag Währungen zu kaufen bzw. zu verkaufen.

Kassadevisen stehen dem Käufer sofort zur Verfügung (in der Praxis zwei Werktage nach dem Kauf). Termindevisen dagegen stehen dem Käufer erst zu einem späteren Zeitpunkt zur Verfügung.

Devisen sind als Vermögenswerte auch Objekt für Spekulation und Arbitrage. Während die Spekulation versucht, Kursunterschiede innerhalb eines bestimmten Zeitraums auszunutzen, ist die Arbitrage auf die Ausnutzung von Kursunterschieden zu einem bestimmten Zeitpunkt an verschiedenen Orten angelegt. Devisenspekulation und -arbitrage gehören zum Eigenhandel der Banken, sind also nicht kundengetrieben. Auch Nichtbanken nehmen an Devisenspekulation und -arbitrage in erheblichem Umfang teil, indem (international tätige) Unternehmen insbesondere ihre Deviseneinnahmen oder -ausgaben nicht taggleich glattstellen oder sichern („hedgen“), sondern im Bestand halten, ohne dass es hierfür operative Gründe gibt.

Devisenspekulation ist letztlich der Kauf oder Verkauf von Devisenkassa- oder –termingeschäften in der Erwartung, dass bei Vornahme der entgegengesetzten Transaktion (Glattstellung) in der Zukunft durch die Devisenkursentwicklung ein Gewinn entstehen wird. Damit erfordert Devisenspekulation offene Fremdwährungsforderungen („open long position“) und/oder offene Fremdwährungsverbindlichkeiten („open short position“), die nicht durch entsprechende Gegenpositionen kongruent (also mit identischer Fälligkeit und in derselben Währung) abgesichert sind.

Devisenarbitrage liegt vor, wenn Devisenkursunterschiede zu einem bestimmten Zeitpunkt ausgenutzt werden. Arbitrage erfordert mithin den gleichzeitigen Kauf und Verkauf einer Währung auf zwei unterschiedlichen Märkten, um den zwischen diesen Märkten bestehenden Kursunterschied gewinnbringend zu nutzen. Durch diese Zeitpunktbezogenheit ist die Arbitrage völlig risikolos, während mit der Spekulation auch eine Verlustgefahr verbunden ist.

Aus volkswirtschaftlicher Sicht entstehen Devisenbestände in einem Staat insbesondere durch permanente Überschüsse in der Handelsbilanz (wenn von anderen Einflüssen abstrahiert wird). Dann nämlich exportiert ein Staat mehr, als er importiert, wodurch er mehr Devisen einnimmt, als er durch Importe wieder ausgeben muss. Die auf diese Weise entstehenden Devisenbestände eines Staates sind nicht nur ein international anerkanntes Statussymbol für die Wirtschaftskraft des Staates, sondern sind Teil wichtiger Kennzahlen beim Rating von Staaten durch Ratingagenturen (siehe Länderrisiko). Diese berechnen etwa die Kennzahl der Importdeckung, indem sie das Importvolumen eines Staates den Devisenbeständen in einem bestimmten Zeitraum gegenüberstellen und dadurch ermitteln können, wie lange das Importvolumen aus vorhandenen Devisenbeständen ohne Kreditaufnahme bezahlt werden kann. Darüber hinaus bilden derartige Devisenbestände auch das Zahlungsmittel für die Tilgung und Zinszahlung von Staatsschulden bei ausländischen Gläubigern. Temporäre Leistungsbilanzdefizite können bei hohen Devisenreserven bedenkenlos über die Aufnahme von Auslandskrediten finanziert werden. Ein Staat mit hohen Devisenbeständen besitzt mithin vergleichsweise höhere Importfähigkeiten und Schuldenbedienungspotenziale als Staaten mit sehr geringen oder keinen Devisenbeständen. Staaten mit sehr geringen oder keinen Devisenbeständen können Ziel internationaler Spekulation werden. Dann müssen die betroffenen Länder Kursstützungsmaßnahmen ergreifen, die absehbar durch Devisenmarktinterventionen zu einem Verbrauch der Währungsreserven führen und auf diese Weise ein internationales Moratorium eines Staates verursachen können. Damit sind Devisenbestände auch eine wesentliche Bestandsgröße innerhalb der Währungsreserven eines Staates.

Die von Staaten gehaltenen Devisenreserven betrugen 1999 nach Angaben des Internationalen Währungsfonds knapp 2 Billionen US-Dollar. Im Jahr 2008 war dieser Betrag in der Spitze auf 7 Billionen Dollar angestiegen, davon entfielen etwa 2 1/2 Bio. Dollar auf fortgeschrittene Industrieländer, während 4 1/2 Bio. Dollar von Schwellenländern gehalten wurden. 2009 war dieser weltweite Devisenbestand rückläufig. Er liegt jetzt bei 6 1/2 Bio. Dollar.

Erste Formen von Devisen und ihrem Handel sind bereits im alten Griechenland zu verzeichnen. In den verschiedenen Kleinstaaten gab es unterschiedliche Währungen, was den Handel zwischen ihnen sehr erschwerte. Deshalb fungierten Privatbankiers als Geldwechsler, welche das Wechselgeschäft mit den verschiedenen Währungen und Münzen besorgten. Außerdem beschäftigten sie sich mit Anleihen, Pfandleihen und Investitionstätigkeiten.

Auch im Römischen Reich entwickelte sich eine Art Wechselkurs bzw. Währungstausch. Die sogenannten Argentarii (Wechsler, Bankiers) bestimmten den Wert verschiedener Münzen und Währungen auf ihren Wechseltischen und tauschten diese gegen die Römische Währung ein. Dabei wurden viele andere Währungen und Münzen dem römischen Münzsystem angepasst. Dadurch entstanden auch erste Formen von Schecks und Wechseln.
Ein weiterer Fortschritt entstand durch die florentinische Familie der Medici, welche sich im Zuge ihrer regen Handelstätigkeiten ein Nostrobuch bzw. Nostrokonto anlegten, in dem sie zahlreiche ausländische und inländische Währungen und deren entsprechenden Werte auflisteten.
Im späten Mittelalter setzte sich der Goldstandard durch. Die Nationalstaaten begannen Rohstoffe wie Gold und Silber anzuhäufen, um die eigene Währung sicherzustellen. Dies bedeutete, dass eine bestimmte Menge Gold einer bestimmten Menge Geld entsprach. Da Geld überall weltweit ungefähr gleich viel wert war, entstand ein mehr oder weniger konstanter Wechselkurs.
1944 brach das Gold-System aber zusammen und mit dem Bretton-Woods-Abkommen nahm der US-Dollar sozusagen seine Rolle ein. Dies bedeutete, dass Währungen jederzeit in fester Relation zum US-Dollar getauscht werden konnten. Diese wiederum konnten jederzeit in Goldbestände umgetauscht werden. Dieses System brach aber in den 1970er Jahren aufgrund von Zahlungsdefiziten der USA zusammen, in Folge welcher die Wechselkurse mehrmals angepasst werden mussten.
Seitdem herrscht das System der freien und schwankenden Wechselkurses.
1982 löste Mexiko mit dem Auflösen seines Devisenhandels die Schuldenkrise aus, was zeigte, dass Devisen und deren Handel von externen Einflüssen wie Politik und Wirtschaft sehr beeinflusst werden können.



</doc>
<doc id="10092" url="https://de.wikipedia.org/wiki?curid=10092" title="Einheit (Mathematik)">
Einheit (Mathematik)

In der Algebra, einem Teilgebiet der Mathematik, wird ein invertierbares Element eines Monoids als Einheit bezeichnet. Einheiten werden vor allem in unitären Ringen betrachtet.

Sei formula_1 ein Monoid, wobei mit formula_2 das neutrale Element bezeichnet wird. 
Dann heißt ein Element formula_3 eine Einheit, wenn es invertierbar ist, also wenn es ein formula_4 gibt mit
Das Element formula_6 mit dieser Eigenschaft ist eindeutig bestimmt und wird als das "inverse Element" von formula_7 bezeichnet und oft als formula_8 notiert.

Elemente, die keine Einheiten sind, werden oft als "Nichteinheiten" bezeichnet.

Die Menge formula_9 aller Einheiten eines Monoids, also
bildet eine Gruppe, die "Einheitengruppe" von formula_11. Eine weitere übliche Bezeichnung für die Einheitengruppe ist formula_12.

Sei formula_13 ein unitärer Ring, also ein Ring mit einem neutralen Element bezüglich der Multiplikation, das mit formula_2 bezeichnet wird. Dann ist formula_15 ein Monoid und damit ist der Begriff der "Einheit" für einen unitären Ring definiert und ist gerade die Menge der invertierbaren Elemente.



Ist das Monoid formula_11 nicht kommutativ, so können auch einseitige Einheiten betrachtet werden
Ein Element formula_3 ist genau dann eine Einheit, wenn es gleichzeitig eine Linkseinheit und eine Rechtseinheit ist. In einem kommutativen Monoid stimmen die drei Begriffe überein. formula_2 bleibt auch im nicht-kommutativen Fall eine beidseitige Einheit.

Es gibt den folgenden Ring formula_22, in dem es eine Linkseinheit formula_55 gibt, die keine Rechtseinheit ist, und eine Rechtseinheit formula_56, die keine Linkseinheit ist. Außerdem sind formula_55 und formula_56 noch einseitige Nullteiler.

formula_22 bestehe aus allen Matrizen der Größe „abzählbar-mal-abzählbar“ mit Komponenten in den reellen Zahlen, bei denen in jeder Zeile und in jeder Spalte nur endlich viele Nicht-Nullen stehen (insgesamt dürfen dabei unendlich viele Nicht-Nullen enthalten sein). formula_22 ist ein Ring mit der gewöhnlichen Matrizenaddition und Matrizenmultiplikation. Die Einheitsmatrix formula_61 hat nur Einsen auf der Hauptdiagonalen und sonst Nullen, sie ist das Einselement von formula_22 (das neutrale Element der Multiplikation).

formula_55 sei die Matrix in formula_22, die in der ersten oberen Nebendiagonalen nur Einsen hat und sonst nur Nullen:

formula_56 sei die Transponierte formula_67 von formula_55, also die Matrix, die in der ersten Diagonalen unterhalb der Hauptdiagonalen nur Einsen hat, und sonst nur Nullen.

Es ist formula_69, also ist formula_55 eine Linkseinheit und formula_56 eine Rechtseinheit.
Für jedes Element formula_72 von formula_22 hat aber das Produkt formula_74 in der ersten Spalte nur Nullen, und das Produkt formula_75 in der ersten Zeile nur Nullen. Damit kann formula_55 keine Rechtseinheit und formula_56 keine Linkseinheit sein.
Mit der Matrix formula_78, die nur in der Komponente formula_79 eine Eins und sonst nur Nullen enthält, ist formula_80 und formula_81, also ist formula_55 ein Linksnullteiler und formula_56 ein Rechtsnullteiler.

Eine funktionalanalytische Variante dieses Beispiels ist der unilaterale Shiftoperator.



</doc>
<doc id="10094" url="https://de.wikipedia.org/wiki?curid=10094" title="Primelement">
Primelement

Der Begriff Primelement ist in der kommutativen Algebra eine Verallgemeinerung des Begriffs der Primzahl auf "kommutative unitäre Ringe".

Ein Element formula_1 eines kommutativen unitären Ringes formula_2 heißt Primelement, falls formula_1 weder 0 noch eine Einheit ist und für alle formula_4 gilt: Teilt formula_1 das Produkt formula_6, dann teilt formula_1 auch formula_8 oder formula_9.

In Symbolnotation: formula_10

Primelemente sind also diejenigen Elemente abgesehen von 0 und Einheiten, die, wenn sie in irgendeinem Produkt vorkommen, auch in mindestens einem der Faktoren vorkommen.

Eine andere Verallgemeinerung des Primzahlbegriffs sind irreduzible Elemente, die dadurch definiert sind, dass sie keine Einheiten sind und nicht als Produkt von zwei Nicht-Einheiten dargestellt werden können. Im Allgemeinen ist weder jedes Primelement irreduzibel noch jedes irreduzible Element prim (siehe Beispiele). Aber in einem Integritätsring ist jedes Primelement irreduzibel, und in einem faktoriellen Ring ist auch umgekehrt jedes irreduzible Element prim.




</doc>
<doc id="10099" url="https://de.wikipedia.org/wiki?curid=10099" title="Konjunktion (Astronomie)">
Konjunktion (Astronomie)

Als Konjunktion (Astronomisches Symbol: ☌) wird die scheinbare Begegnung zweier Planeten oder eines Planeten mit Sonne oder Mond bezeichnet. Sie tritt ein, wenn die Himmelskörper am Sternhimmel den gleichen Koordinatenwert der Rektaszension erreichen. Manchmal wird sie auch auf die Ekliptikale Länge bezogen ("Konjunktion in Länge"). Je geringer dann die Koordinatendifferenz in Deklination ist, desto näher stehen die Himmelskörper aneinander.

Wenn sich bei sehr engen Begegnungen einer der Körper vor den anderen schiebt, spricht man von Berührung bzw. Bedeckung. Als Kontakt werden die Momente bezeichnet, wo sich die sichtbaren Scheibchen von außen oder innen berühren.

"Konjunktion in Rektaszension", "in Deklination" und "Berührung" liegen im Allgemeinen zeitlich auseinander. Es ist möglich, dass zwei Himmelskörper in Konjunktion in ekliptikaler Länge kommen (aber nicht in Rektaszension), und umgekehrt. Unterschreitet der Winkelabstand der beiden Himmelskörper die Summe der scheinbaren Radien, kommt es zu einer "Berührung". Je nachdem, ob der kleinere oder der größere Körper näher am Beobachter ist, also welcher der beiden sich „vorn“ befindet, spricht man dabei von "Durchgang (Transit)" oder "Okkultation (Bedeckung)". 

Während die Konjunktionen von Planeten jede synodische Periode stattfinden, sind Berührungen bzw. Bedeckungen zwischen ihnen äußerst selten. Planetenbedeckungen durch den Mond kommen hingegen wegen dessen scheinbarer Größe häufiger vor - im Durchschnitt mehrere im Jahrzehnt. Hingegen treten freiäugig sichtbare Sternbedeckungen etwa wöchentlich auf.

Am Sonnen- bzw. Mondrand bezeichnet man dann vier ausgewählte Zeitpunkte als "1. – 4. Kontakt":
Siehe auch Sonnenfinsternis, Sonnenrand, Sternbedeckung.

Alle drei Phänomene sind im Allgemeinen auf einen Beobachter auf der Erde bezogen, könnten aber auch für beliebige andere Standpunkte definiert werden (topozentrisches Problem). Dann finden diese Ereignisse zu anderen Zeiten als für die Erde statt.

Konjunktionen zwischen den Planeten untereinander, den Planeten und hellen Fixsternen, dem Mond und Planeten, sowie dem Mond und hellen Sternen erzeugen oft interessante Anblicke. Sie sind daher meist in astronomischen Jahrbüchern angeführt. Manchmal bilden Planeten für einige Tage bis Wochen auch Dreiecke oder Vierecke mit hellen Sternen, was allerdings nicht extra vorausberechnet wird. Am Frühlingshimmel 2014 gab es sogar zwei solcher Konstellationen: Jupiter mit den Zwillingssternen Castor und Pollux, sowie Mars, Saturn, Arktur und Spica in der Jungfrau. 

Besonders spektakulär ist das Zusammentreffen von Jupiter und Saturn am Himmel, was man als Große Konjunktion bezeichnet. Sie tritt etwa alle 20 Jahre ein. Weil die beiden hellen Planeten wegen ihrer großen Entfernung nur langsam über den Sternhimmel wandern, stehen sie wochenlang nahe beisammen. Wenn sie in dieser Zeit ihre Opposition haben, können sich ihre jährlichen Planetenschleifen fast decken, und sie begegnen sich während einiger Monate sogar "dreimal". Diesen seltenen Sonderfall nennt man Größte Konjunktion; sein Auftreten im Jahr 6 v. Chr. gilt als die schlüssigste Erklärung des im Matthäus-Evangelium beschriebenen "Stern von Betlehem".

Konjunktionen von Kleinplaneten (Asteroiden) oder der lichtschwachen Planeten Uranus und Neptun mit hellen Planeten oder Fixsternen ermöglichen auch ungeübten Beobachtern diese Objekte aufzusuchen, da der helle Planet oder Stern als Leitstern dienen kann.

Interessante Konjunktionen ergeben sich auch aus Durchgängen von Satelliten, erfordern aber sehr exakte Vorausberechnung und spezielle Methoden der Beobachtung.

Sternbedeckungen durch Asteroiden erlauben es, deren Größe und Form sehr präzise zu vermessen, indem an verschiedenen Orten auf der Erde die Okkultationsdauer und Zeit gemessen wird.

Stehen zwei Himmelskörper fast zur gleichen Zeit in Opposition zur Sonne, so kann es wegen der scheinbaren Schleifenbahnen der beteiligten Himmelskörper zu drei Konjunktionen innerhalb weniger Monate kommen. Man spricht hierbei von einer "dreifachen Konjunktion". Solche Ereignisse sind zwischen den Planeten sehr selten. Die letzte derartige dreifache Konjunktion zwischen Mars und Jupiter fand 1980, zwischen Jupiter und Saturn 1981 und zwischen Mars und Saturn 1945/46 statt, die nächsten derartigen Ereignisse treten erst wieder 2123 (dreifache Konjunktion Mars - Jupiter), 2238/39 (dreifache Konjunktion Jupiter - Saturn) und 2148 (dreifache Konjunktion Mars - Saturn) ein. Dreifache Konjunktionen zwischen Jupiter und Uranus und Jupiter und Neptun sind häufiger.

Für die "unteren Planeten" Merkur und Venus wird im Zusammenhang mit Konjunktionen mit der Sonne der Begriff "obere Konjunktion" und "untere Konjunktion" verwendet. Stehen Merkur und Venus – von der Erde aus gesehen – hinter der Sonne, so befinden sie sich in oberer Konjunktion, stehen sie zwischen Sonne und Erde, so befinden sie sich in unterer Konjunktion. Die beiden Planeten haben in oberer Konjunktion den größten Erdabstand, in unterer Konjunktion haben sie den kleinsten Erdabstand, sind aber nur in Ausnahmefällen (Merkur und Venus während eines Durchgangs oder Venus bei großem nördlichen oder südlichen Sonnenabstand) sichtbar.

Für die "oberen Planeten" gibt es nur eine Art der Konjunktion mit der Sonne (hinter dieser, sie können nicht zwischen Erde und Sonne stehen). Steht ein oberer Planet in Konjunktion, so hat er den maximalen Erdabstand und ist von der Erde aus unsichtbar. Die oberen Planeten erreichen ihre beste Sichtbarkeit zum Zeitpunkt der Opposition.

Steht der Mond in Konjunktion mit der Sonne, so haben wir Neumond. Wie Merkur und Venus zieht der Mond meist nördlich oder südlich von der Sonne vorbei. Nur wenn er in der Nähe seiner Bahnknoten steht, wandert er für Beobachter in manchen Gebieten der Erde direkt vor der Sonne vorbei. Dort findet eine partielle oder totale Sonnenfinsternis statt.





</doc>
<doc id="10101" url="https://de.wikipedia.org/wiki?curid=10101" title="Opposition (Astronomie)">
Opposition (Astronomie)

Als Opposition (Astronomisches Symbol: ☍) bezeichnet man in der Astronomie die Konstellation, bei der sich zwei Himmelskörper von der Erde aus betrachtet im Winkelabstand (Elongation) von 180 Grad zueinander befinden.
Im Regelfall interessiert dabei nur die Opposition eines Himmelskörpers gegenüber der Sonne (als zweitem Himmelskörper), weil er erst dann die ganze Nacht über am Himmel zu sehen ist und um Mitternacht am höchsten über dem Horizont steht.

Steht etwa der Mond in Opposition zur Sonne, so haben wir Vollmond. Himmelskörper, die wie die unteren Planeten Merkur und Venus innerhalb der Erdbahn die Sonne umlaufen, können dagegen nie in Opposition zur Sonne kommen und sind daher stets nur am Abend- "oder" Morgenhimmel, nicht aber während der ganzen Nacht zu sehen.

Zeitnah zur Opposition eines Himmelskörpers in Bezug auf die Sonne wird regelmäßig der geringste Erdabstand erreicht, was für erdgebundene Beobachtungen natürlich von Vorteil ist. Wegen der Exzentrizität der Planetenbahnen allerdings wird dieser geringste Erdabstand häufig nicht exakt zum Oppositionszeitpunkt erreicht, die auftretende Differenz von wenigen Tagen ist aber im Regelfall ohne praktische Bedeutung.

Stehen zwei Himmelskörper fast zur gleichen Zeit in Opposition zur Sonne, so kann es wegen der scheinbaren Schleifenbahnen ("Oppositionsschleife") der beteiligten Himmelskörper zu einer so genannten dreifachen Konjunktion kommen. Hierbei begegnen sich die beiden Himmelskörper dreimal im zeitlichen Abstand von nur wenigen Monaten.

Bei (ausreichend) exakter Opposition, wenn also Sonne, Erde und der dritte Himmelskörper – in dieser Reihenfolge – auf einer Linie stehen, kommt es, wenn die Größen- und Abstandsverhältnisse passen, zu einer Abschattung des dritten Körpers. Im Fall des kleinen und nahen Mondes führt eine ausreichend perfekte Opposition zu einer totalen oder ringförmigen Mondfinsternis, während eine weniger genaue Opposition eine asymmetrische Teil-Verfinsterung bewirkt.

Sonderformen der Opposition



</doc>
<doc id="10102" url="https://de.wikipedia.org/wiki?curid=10102" title="Obere Konjunktion">
Obere Konjunktion

Als Obere Konjunktion bezeichnet man das Himmelsereignis, wenn ein innerer Planet (bezüglich eines Beobachters auf der Erde also Merkur oder Venus) genau hinter der Sonne steht. Er zeigt uns in diesem Fall seine voll beleuchtete Seite (ähnlich wie beim Vollmond) und zieht unbeobachtbar meist nördlich oder südlich der Sonne vorbei.

Findet eine Obere Konjunktion in der Nähe der Bahnknoten der Merkur- oder Venusbahn statt, so können diese Planeten von der Sonnenscheibe bedeckt werden, was aber wegen des enormen Helligkeitsunterschieds nicht beobachtet werden kann.



</doc>
<doc id="10103" url="https://de.wikipedia.org/wiki?curid=10103" title="Untere Konjunktion">
Untere Konjunktion

Als Untere Konjunktion bezeichnet man jenes Himmelsereignis, bei dem sich ein innerer Planet im Laufe seiner Bahnbewegung zwischen dem Beobachter und der Sonne durchbewegt. Er zeigt dem Beobachter dann seine unbeleuchtete Seite (ähnlich wie bei Neumond) und zieht, meist unbeobachtbar, nördlich oder südlich der Sonne vorbei. 

Bezüglich der Erde gibt es nur "zwei" innere Planeten - den Merkur und die Venus. Wegen unterschiedlicher Bahnneigungen zur Erde kommt es nur selten zu einem direkten Vorübergang des Planeten vor der Sonnenscheibe: Merkurdurchgänge etwa jedes Jahrzehnt, Venusdurchgänge maximal 2 pro Jahrhundert.

Die Astronomen definieren als "Untere Konjunktion" jenen Zeitpunkt, in dem der Planet dieselbe ekliptikale Länge wie die Sonne hat, was etwa dem Moment der geringsten Winkeldistanz entspricht. 

Bei unserem direkten Nachbarplaneten Venus kann der Vorbeizug bis zu 9,5 Grad nördlich oder südlich der Sonne erfolgen. Theoretisch zeigt der Planet dann eine äußerst schmale Sichelform, die aber durch die Lichtbrechung in der dicken Venusatmosphäre verbreitert wird (siehe auch Dichotomie). Weil daher die zur Erde gewandte Seite der Venus nicht völlig unbeleuchtet ist, kann sie in einem guten Teleskop für einige Tage sowohl am Morgen- als auch am Abendhimmel zu sehen sein.

Findet eine "Untere Konjunktion" in der Nähe der Bahnknoten der Merkur- oder Venusbahn statt, so können bei diesem Ereignis Merkur oder Venus vor der Sonnenscheibe vorbeiziehen; es kommt zu einem so genannten Merkur- bzw. Venusdurchgang. Hierbei ist es möglich, sie mit geeigneten Sonnenbeobachtungs-Instrumenten als kleine, tiefschwarze Scheibchen zu beobachten, die von Ost nach West vor der Sonne vorbeiwandern.




</doc>
<doc id="10104" url="https://de.wikipedia.org/wiki?curid=10104" title="Deutsches Institut für Normung">
Deutsches Institut für Normung

Das Deutsche Institut für Normung e. V. (DIN) ist die bedeutendste nationale Normungsorganisation in der Bundesrepublik Deutschland. Sie wurde am 22. Dezember 1917 unter dem Namen „Normenausschuß der deutschen Industrie“ gegründet. Eine erste Umbenennung erfolgte 1926 zu „Deutscher Normenausschuß“, um auszudrücken, dass sich das Arbeitsgebiet nicht mehr auf die Industrie beschränkte. Der heutige Name „DIN Deutsches Institut für Normung e. V.“ wurde 1975 im Zusammenhang mit dem zwischen der Organisation und der Bundesrepublik Deutschland abgeschlossenen "Normenvertrag" gewählt. Die unter der Leitung von Arbeitsausschüssen dieser Normungsorganisation erarbeiteten Normen werden als "DIN-Normen" bezeichnet.

Das Deutsche Institut für Normung ist ein eingetragener Verein, wird privatwirtschaftlich getragen und bei seinen europäischen und internationalen Normungsaktivitäten von der Bundesrepublik Deutschland als einzige nationale Normungsorganisation unterstützt. Es bietet den sogenannten „interessierten Kreisen“ (Hersteller, Handel, Industrie, Wissenschaft, Verbraucher, Prüfinstitute und Behörden) ein Forum, im Konsensverfahren Normen zu erarbeiten. Der interessierte Kreis der Verbraucher wird durch den Verbraucherrat des DIN vertreten. Das DIN ist Mitglied der Europäischen Bewegung Deutschlands.

Die Grundprinzipien der Arbeit des DIN sind in DIN 820 festgeschrieben:

Aufgabe des DIN ist es, zum Nutzen der Allgemeinheit unter Wahrung des öffentlichen Interesses die Normung anzuregen, zu organisieren, zu steuern und zu moderieren. Die Arbeitsergebnisse dienen der Innovation, der Rationalisierung, Verständigung in Wirtschaft, Wissenschaft, Verwaltung und Öffentlichkeit, der Sicherung von Gebrauchstauglichkeit, Qualitätssicherung, Kompatibilität, Austauschbarkeit, Gesundheit, Sicherheit, dem Verbraucherschutz, Arbeitsschutz und dem Umweltschutz. Bei ihrer Erstellung wird angestrebt, dass die allgemein anerkannten Regeln der Technik eingehalten werden und der aktuelle Stand der Technik berücksichtigt wird.

Die elektrotechnischen Themen werden von DIN und dem deutschen Verband der Elektrotechnik (VDE) gemeinsam in der DKE Deutsche Kommission Elektrotechnik Elektronik Informationstechnik im DIN und VDE bearbeitet.

Das DIN arbeitet in den internationalen und europäischen Normengremien "ISO" und "CEN" und in den elektrotechnischen Organisationen "IEC" und "CENELEC" mit, um die deutschen Interessen zu vertreten und den internationalen freien Warenverkehr zu fördern. Es organisiert die Eingliederung internationaler Normen in das deutsche Normenwerk.

Die DIN-Normen werden über den Beuth Verlag, ein Tochterunternehmen der DIN-Gruppe, in Papierform und als Download kostenpflichtig vertrieben. Der Verlag vertreibt auch Normdokumente anderer und ausländischer Normungsstellen.

In der Schweiz leistet die Schweizerische Normen-Vereinigung (SNV) und in Österreich das Austrian Standards Institute ÖNORM vergleichbare Arbeit.

Das DIN ist ein eingetragener Verein, dessen Mitglieder juristische Personen sind. Die Mitgliederversammlung wählt das Präsidium, das aus Vertretern aller beteiligten interessierten Kreise (sämtliche Wirtschaftssektoren, die Verbraucher, die Wissenschaft und der Staat) besteht. Präsident seit 2015 ist Albert Dürr.

Das DIN wird von einer Geschäftsleitung geführt, welcher der Vorsitzende des Vorstandes vorsteht. Der Vorsitzende des Vorstandes ist auch Mitglied des Präsidiums. Die fest angestellten Mitarbeiter des DIN sorgen als Sekretäre dafür, dass die Grundprinzipien des DIN eingehalten werden, d. h. dass zum Beispiel kein interessierter Kreis unberücksichtigt bleibt. Sie organisieren die Arbeit in den Gremien (auch in internationalen), stellen das Arbeitsprogramm und den Haushaltsplan der Normenausschüsse auf und stimmen beides mit dem Lenkungsgremium ab, das aus Vertretern der interessierten Kreise besteht. Das DIN stellt die elektronische Infrastruktur für die Normenentwicklung zur Verfügung.

Die ergebnisorientierten Aktivitäten (zum Beispiel der Vertrieb der Norm-Dokumente durch den Beuth-Verlag) erfolgen in GmbHs als Tochter- und Beteiligungsgesellschaften. Sie tragen zur Kostendeckung der gemeinnützigen Normungsaktivitäten bei.

Die fachliche Arbeit der Normung wird in Arbeitsausschüssen beziehungsweise Komitees durchgeführt. Für eine bestimmte Normungsaufgabe ist jeweils nur ein Arbeitsausschuss beziehungsweise ein Technisches Komitee zuständig. Diese Ausschüsse bzw. Komitees vertreten ihre Aufgabe zugleich in den regionalen und internationalen Normungsorganisationen. Im Regelfall sind mehrere Arbeitsausschüsse zu einem Normenausschuss im DIN zusammengefasst.

Einige Normenausschüsse führen den Namen „Normenstelle“, die Groß/Kleinschreibung innerhalb der Abkürzungen der Normenausschüsse ist unsystematisch, viele, aber nicht alle Abkürzungen für Normenausschüsse beginnen mit „N“. Die Namen einiger Normenausschüsse sind irreführend, z. B. vertritt der Normenausschuss Automobiltechnik das gesamte Gebiet des Kraftfahrzeugwesens. Zurzeit (Ende Mai 2013) gibt es ca. 70 Normenausschüsse. Eine vollständige Liste findet man über die Homepage des DIN e. V.


Das Budget des DIN und damit die Finanzierung der Normungsarbeit wird aus drei Quellen gespeist:


Die Vorarbeiten zur Rationalisierung der Rüstungsproduktion im Januar 1917 führten zu der Erkenntnis, dass ganz Deutschland zu einer Produktionsgemeinschaft für einen Abnehmer, die Streitkräfte, werden musste, und dass hierfür grundlegende Normen, insbesondere zur Zusammenarbeit im Maschinenbauwesen, notwendig waren. Die zur folgenden Gründung des DIN führende Initiative ging deshalb vom „Königlichen Fabrikationsbüro für Artillerie (Fabo-A)“ in Berlin aus.

Das DIN im Deutschen Reich wurde im Mai 1917 als „Normalienauschuß für den Maschinenbau“ gegründet mit der Aufgabe, die wichtigsten Maschinenelemente zu vereinheitlichen. Am 22. Dezember 1917 erfolgte die Umbenennung zum „Normenausschuß der deutschen Industrie“ (NADI). Die Arbeitsergebnisse des NADI waren die „Deutschen Industrie-Normen“ (zuerst als „DI-Norm,“ aber bald als „DIN“ abgekürzt). Die erste Norm (DI-Norm 1 Kegelstifte) erschien im März 1918. Seit 1920 ist das DIN ein eingetragener Verein und 1922 wurde die für den Verbraucher wohl bekannteste Norm DIN 476 Papierformate (zum Beispiel DIN A 4) veröffentlicht. Im deutschen Alltag vertraut ist ebenfalls die Normschrift auf den Verkehrsschildern DIN 1451, die als Schrift umgangssprachlich kurz "die DIN" genannt wird.

1926 wurde das DIN von „Normenausschuß der deutschen Industrie“ in „Deutscher Normenausschuß“ (DNA) umbenannt, denn bereits in den 1920er-Jahren hatte die Normung im Reich das engere Gebiet der Industrie überschritten. Aus demselben Grund versuchte der DNA, die Abkürzung „DIN“ mit „Das Ist Norm“ zu belegen, um „Deutsche Industrie-Norm“ abzulösen. Allerdings konnte sich dieser Begriff in der Öffentlichkeit nicht durchsetzen.

Nach dem Zweiten Weltkrieg genehmigte der Alliierte Kontrollrat 1946 dem DIN die Wiederaufnahme seiner Tätigkeit. Das DIN wurde 1951 Mitglied in der Internationalen Organisation für Standardisierungen (ISO) mit dem Anspruch, den deutschen Sprachraum zu vertreten.

Im Mai 1975 (kurz vor Schließen des "Normenvertrages", siehe unten) änderten sich der Name der Organisation und der ihrer Arbeitsergebnisse erneut. Seitdem heißt die Organisation „DIN Deutsches Institut für Normung e. V.“, und die Arbeitsergebnisse sind die „Deutschen Normen“ oder „DIN-Normen“.

Am 5. Juni 1975 unterzeichneten das DIN Deutsches Institut für Normung e. V. und die Bundesrepublik Deutschland den "Normenvertrag" Dadurch wurde dem DIN eine erhebliche öffentliche Anerkennung zuteil, denn die Bundesrepublik verpflichtete sich, sich bei einschlägigen Fragen und Aufgaben, die vom Staat gestellt werden, ausschließlich an ihn zu wenden. Ebenso ausschließlich empfiehlt die Bundesrepublik für internationale Normungsarbeit nur das DIN. Im Gegenzug machte das DIN seine bisher nach innen geltenden Grundprinzipien (DIN 820) öffentlich verbindlich und verpflichtete sich, vom Staat angeregte Normungsaufgaben nicht nur aufzugreifen, sondern sie bevorzugt zu behandeln. Infolge des mit dem Vertrag eingebrachten öffentlichen Interesses entstanden beim DIN die Kommissionen für Sicherheitstechnik und für Umweltschutz und der Verbraucherrat. Entgegen einer verbreiteten Auffassung blieb das DIN eine unabhängige nicht-staatliche Organisation. Die Bundesrepublik hat kein Weisungsrecht zur Arbeit des DIN erhalten, hat aber auch keinen Teil ihrer eigenen Hoheit an das DIN abgegeben.

Das Gegenstück zur DIN-Norm war in der DDR die TGL, die anfänglich weitgehend auf den DIN-Normen beruhte, später verstärkt RGW-Standards berücksichtigte. Die ost-/westdeutsche Zusammenarbeit auf dem Gebiet der Normung ließ stark nach, nachdem die DDR-Regierung die DIN-Geschäftsstellen in Ost-Berlin, Jena und Ilmenau 1961 geschlossen hatte. Seit der 1990 erfolgten Auflösung des Amtes für Standardisierung, Messwesen und Warenprüfung (ASMW) der DDR ist das DIN wieder für die Normungsarbeit in ganz Deutschland zuständig.

Heute ist die Normungsarbeit zunehmend europäisch und international geprägt: Nur noch 15 % aller Normungsprojekte sind rein nationaler Natur. Das DIN führte im Jahr 2015 17 % aller Sekretariate in ISO- und 29 % aller Sekretariate in CEN-Arbeitsgremien.

2007 ist DIN durch kontroverse Entscheidungen für das groß geschriebene ß (Versal-ß) sowie für den von Microsoft eingeführten Dokumentenstandard Office Open XML aufgefallen.

1988 wurde die DIN Software GmbH mit dem Zweck der Beschaffung, Erstellung und Vertrieb von zertifizierten Dateien und Programmen auf maschinenlesbaren Datenträgern im Bereich Technik, insbesondere zur Herstellung DIN-normgerechter Erzeugnisse und Anwendungen DIN-genormter Verfahren gegründet. Die Gründung wurde von vier Gesellschaftern (DIN, Verband der Automobilindustrie, VDMA und ZVEI) getragen. Seit 1993 ist die DIN Software GmbH ein 100-prozentiges Tochterunternehmen von DIN. Anfänglich bestanden die Hauptprodukte aus CAD-Normteiledaten zur Anwendung in Konstruktionsprogrammen. Die Daten der Normteile wurden aus Normen entnommen und um Metadaten der Deutsches Informationszentrum für technische Regeln-Datenbank erweitert. Ebenfalls seit Beginn der DIN Software GmbH wurden EDIFACT-Normdateien angeboten.

Die bisher hierarchische Struktur der Deutsches Informationszentrum für technische Regeln-Datenbank wurde 1996 durch eine relationale Datenbank abgelöst. Die DITR-Datenbank wurde ab dem Jahr 2000 zudem als Masterdatenbank von DIN, Beuth Verlag und DIN Software GmbH für Norm- und Dokumentennachweise benutzt.
Im Jahr 1989 wurde der Benutzerfachausschuss des Deutsches Informationszentrum für technische Regeln (heute der DIN Software GmbH) gegründet. Er war und ist ein Bindeglied zwischen den Nutzern von Informationsprodukten oder -dienstleistungen aus der DITR-Datenbank und dem Hersteller.
2003 ging das von DIN betriebene Deutsches Informationszentrum für technische Regeln mit seiner Datenbank in der DIN Software GmbH auf.

2003 übernahm die DIN Software GmbH von DIN das Deutsches Informationszentrum für technische Regeln und die DITR-Datenbank inklusive der elektronischen Volltextarchive für das Technische Recht.

DIN-Normen bilden einen Maßstab für einwandfreies technisches Verhalten und sind daher im Rahmen der Rechtsordnung von Bedeutung. Grundsätzlich haben DIN-Normen den Charakter von Empfehlungen. Ihre Anwendung steht jedem frei, d. h., man kann sie anwenden, muss es aber nicht. Verbindlich werden Normen dann, wenn in privaten Verträgen oder in Gesetzen und Verordnungen auf sie Bezug genommen und dort deren Anwendung festgelegt wird. Weil Normen eindeutige Aussagen sind, lassen sich durch ihre einzelvertraglich vereinbarte Verbindlichkeit Rechtsstreitigkeiten vermeiden. Die Bezugnahme in Gesetzen und Verordnungen entlastet den Staat und die Bürger von rechtlichen Detailregelungen.

Auch in den Fällen, in denen DIN-Normen von Vertragsparteien nicht zum Inhalt eines Vertrages gemacht worden sind, dienen sie im Streitfall wegen Sachmängeln (Kauf- und Werkvertragsrecht) als Entscheidungshilfe. Hierbei besteht grundsätzlich die Vermutung, dass die DIN-Normen den anerkannten Regeln der Technik entsprechen. Eine solche Vermutung kann dennoch erschüttert oder widerlegt werden, etwa wenn erst ein unfertiger Normentwurf besteht oder durch ein Sachverständigengutachten.<ref name="Az. VII ZR 184/97">BGH, Urteil vom 14. Mai 1998, Az. VII ZR 184/97, Volltext = BGHZ 139, 16.</ref>

Urheberrecht
DIN-Normen sind schöpferische Leistungen und genießen als Sprachwerke den Schutz durch das Urheberrechtsgesetz. Dieser Grundsatz wurde auch vom Bundesgerichtshof mehrfach bestätigt. DIN-Normen dürfen deshalb nicht ohne Zustimmung des Deutschen Instituts für Normung e. V. als Nutzungsrechtsinhaber vervielfältigt und verbreitet oder im Internet öffentlich zugänglich gemacht werden. Die Schranken des Urheberrechts gelten aber auch für die Normen. So dürfen z. B. einzelne Norm-Blätter für private Zwecke auf Papier kopiert werden ( UrhG).

Kein Urheberrecht für vom Gesetzgeber abgedruckte Normen (amtliche Werke)

Für amtlich in Bezug genommene und in Gesetzen abgedruckte technische Regeln gilt nach Abs. 3 UrhG folgende Ausnahme:

(1) Gesetze, Verordnungen, amtliche Erlasse und Bekanntmachungen sowie Entscheidungen und amtlich verfasste Leitsätze zu Entscheidungen genießen keinen urheberrechtlichen Schutz."
(2) Das Gleiche gilt für andere amtliche Werke, die im amtlichen Interesse zur allgemeinen Kenntnisnahme veröffentlicht worden sind, mit der Einschränkung, dass die Bestimmungen über Änderungsverbot und Quellenangabe in Abs. 1 bis 3 und Abs. 1 und 2 entsprechend anzuwenden sind."
(3) Das Urheberrecht an privaten Normwerken wird durch die Absätze 1 und 2 nicht berührt, wenn Gesetze, Verordnungen, Erlasse oder amtliche Bekanntmachungen auf sie verweisen, ohne ihren Wortlaut wiederzugeben. In diesem Fall ist der Urheber verpflichtet, jedem Verleger zu angemessenen Bedingungen ein Recht zur Vervielfältigung und Verbreitung einzuräumen. Ist ein Dritter Inhaber des ausschließlichen Rechts zur Vervielfältigung und Verbreitung, so ist dieser zur Einräumung des Nutzungsrechts nach Satz 2 verpflichtet.

Die "Initiative gegen die Direktgeltung privater Normen im Bauwesen" hat im Jahr 2003 vergeblich versucht, die Einfügung des Absatzes 3 zu verhindern. Kritisiert wurde, dass vom Staat für verbindlich erklärte Normen nicht gemeinfrei sind (außer sie sind als Volltext in einer amtlichen Bekanntmachung enthalten), also gekauft werden müssen.

DIN verleiht jährlich DIN-Preise für Wettbewerbe in verschiedenen Kategorien. Die Preise sind mit bis zu 10.000 Euro dotiert.

Der Waldemar-Hellmich-Kreis ist der Ehrensenat des DIN Deutsches Institut für Normung. Anlässlich seines 40-jährigen Bestehens hat das DIN 1957 zur Erinnerung an seinen Begründer Waldemar Hellmich und zur Ehrung von Persönlichkeiten, die sich in ihrer beruflichen Tätigkeit auf dem Gebiet der Normung verdient gemacht haben, den Waldemar-Hellmich-Kreis gegründet. Er soll die Tradition des DIN pflegen und durch Empfehlungen zur lebendigen Weiterentwicklung der Normungsarbeit beitragen. Die Mitglieder des Kreises werden vom Präsidium des DIN berufen. Als sichtbares Zeichen der Zugehörigkeit zum Waldemar-Hellmich-Kreis dient eine Anstecknadel, die die Papierformate darstellt. Die Mitgliederzahl des Kreises wird auf 50 beschränkt.





</doc>
<doc id="10106" url="https://de.wikipedia.org/wiki?curid=10106" title="Kanarische Inseln">
Kanarische Inseln

Die Kanarischen Inseln () oder Kanaren sind eine politisch zu Spanien, geographisch zu Afrika und biogeografisch zu Makaronesien gehörende Inselgruppe.

Die Kanarischen Inseln befinden sich im östlichen Zentralatlantik, etwa 100 bis 500 Kilometer westlich der marokkanischen Küste.

Die Autonome Gemeinschaft "Canarias", die alle Kanarischen Inseln umfasst, ist eine der 17 Autonomen Gemeinschaften Spaniens.

Die Kanarischen Inseln liegen im Atlantik in einer geographischen Region, die als Makaronesien bezeichnet wird. Dazu zählen außerdem die Kapverden, die Azoren, der Madeira-Archipel und die Ilhas Selvagens. Zwischen rund 27° 38' und 29° 30' nördlicher Breite sowie 13° 22' und 18° 11' westlicher Länge befinden sich die Kanaren zwischen 1028 und 1483 Kilometer vom Mutterland Spanien (Kap Trafalgar) entfernt, auf gleicher Breite mit beispielsweise der Sahara, Kuwait und Florida. Es gilt, im Gegensatz zum spanischen Festland, die westeuropäische Zeit.

Die Kanaren bestehen aus sieben Hauptinseln und sechs Nebeninseln:
Einige kleinere unbewohnte Felsinseln sind Anaga, Salmor und Garachico. Der höchste Berg der Kanarischen Inseln ist der 3718 Meter hohe Pico del Teide. Er liegt auf der Insel Teneriffa und ist gleichzeitig Spaniens höchste Erhebung.

Die Inselgruppe der Kanaren gehört geologisch zu Afrika. Sie befindet sich im Atlantischen Ozean am Ostrand des "Kanarischen Beckens", das bis in 6501 Meter Tiefe abfällt. Dieses Seebecken besteht aus dem kleineren Nordbecken und dem größeren Südbecken, die durch die "Kanarenschwelle" getrennt sind, an deren östlichem Ende sich die Kanaren erheben. Die Grenzen des gesamten Beckens bilden im Norden die "Azorenschwelle", im Süden die "Kapverdenschwelle" und im Westen der Nordatlantische Rücken.

Die Kanarischen Inseln sind ein Archipel, der seine Entstehung dem Intraplattenvulkanismus und einem unter ihm liegenden Hotspot verdankt. Sowohl seismische Tomographie als auch geochemische Analysen deuten auf eine tief im Erdmantel liegende Anomalie als Quelle der Magmen hin. Die unstete zeitliche Entwicklung der Inseln kann am besten mit einer sogenannten Edge-Driven Convection (Plattenrandkonvektion) erklärt werden, welche einen aufsteigenden Plume durcheinanderwirbelt.

Geologisch sind die Inseln bis noch in jüngerer Zeit aktiv gewesen, bzw. zum Teil sind sie noch vulkanisch aktiv. Die östlichen Inseln Fuerteventura und Lanzarote sind mit 22 Mio. Jahren bzw. 15,5 Mio. Jahren die ältesten. Gran Canaria entstand vor etwa 14,5 Mio. Jahren, La Gomera vor etwa 11 Mio. und Teneriffa vor etwa 12 Mio. Jahren. La Palma und El Hierro sind mit 2 bzw. 1,2 Mio. Jahren die jüngsten Inseln des Archipels.

Die Hotspot-Wanderung wird als die schlüssigste Erklärung für die Entstehung der Kanarischen Inselkette angesehen: Über dem ortsfesten Hotspot wandert die Afrikanische Platte mit einer Driftgeschwindigkeit von 1,2 cm/Jahr nordostwärts. Der Wert leitet sich aus dem Entstehungszeitraum der Kanarischen Inseln bis zur jüngsten Insel El Hierro und dem Abstand zwischen den Inseln ab. Die im GPS ermittelte Driftgeschwindigkeit am Standort La Palma im Zeitabschnitt 2008 bis 2012 beträgt 1,7 cm/Jahr und 1,6 cm/Jahr (gemessen über die Verschiebung auf dem Breiten- und dem Längengrad).

Jede Insel weist eine individuelle Entstehungsgeschichte auf, außer Lanzarote und Fuerteventura, die eine ähnliche geologische Geschichte durchlaufen haben. Die beiden Inseln und die dazwischen liegende Nebeninsel Lobos waren über Jahrtausende hinweg während des niedrigen Wasserstands in den Kaltzeiten zu einer einzigen Insel verbunden. Auch heute trennt sie nur eine etwa 40 Meter tiefe und etwa 10 Kilometer breite Meerenge.

Allgemein gehen Geologen von einer dreiphasigen Entstehungsgeschichte aus, die mit Hilfe der Argon-Isotop-Datierung der vulkanischen Gesteine belegt ist: Sie begann vor mindestens 142 Millionen Jahren mit unterseeischen Eruptionen, wobei sich aus Kissenlaven, Hyaloklastiten und Intrusionen zunächst unterseeische Berge aufbauten. Nach etlichen Millionen Jahren ragten diese über die Meeresoberfläche hinaus und es bauten sich in anhaltenden Eruptionsserien Schildvulkane auf (vgl. Hawaii oder Surtsey). Danach gab es auf den ältesten Vulkaninseln Fuerteventura, Lanzarote, Gran Canaria und La Gomera eine mehrere Millionen Jahre andauernde eruptive Pause, die durch starke Erosion oberhalb des Meeresspiegels nachweisbar ist. Späterer erneuter Vulkanismus im Pliozän und Quartär formte die Inseln. Auch hier sind Ruhephasen mit Erosionsschichten nachweisbar.

In der Folge kam es auf allen Inseln wiederholt zum Flankenkollaps und zu Trümmerlawinen, deren Schuttfächer bis weit ins Meer hinaus noch heute nachweisbar sind. Zusammen mit großen explosiven Ausbrüchen bildeten sich dabei auch die großen Calderen auf Gran Canaria, Teneriffa und La Palma.

Die vulkanischen Aktivitäten halten mit großen Ausbrüchen im 18. Jahrhundert auf Lanzarote und dem letzten Ausbruch zu Lande auf La Palma 1971 bis in die heutige Zeit an: Ab dem 10. Oktober 2011 entstand, etwa 270 Meter unter dem Meeresspiegel und wenige Kilometer südlich der Küste vor der Insel El Hierro, ein neuer Vulkan, dessen Auswurfprodukte auch die Meeresoberfläche erreichten; die unterseeischen Eruptionen endeten Anfang März 2012.

Das subtropische Klima der Kanaren ist aufgrund seiner Nähe zum nördlichen Wendekreis zwischen dem 27. und 29. Breitengrad das ganze Jahr über angenehm, was dem Archipel den Beinamen "Inseln des ewigen Frühlings" eingebracht hat. Der gleichbleibend kühle Kanarenstrom, ein Teil des Golfstroms, gleicht die Temperaturen aus, und die Passatwinde halten die heißen Luftmassen aus der nahen Sahara meist fern. Eine Ausnahme bildet die mit Calima bezeichnete Wetterlage, die bei Ostwind trockene, warme Luft mit feinem Saharasand auf die Inseln bringt. Grundsätzlich kann zwischen einer Trockenzeit im Sommer und einer regenreicheren Zeit im Winter unterschieden werden. In den Küstenregionen liegen die Durchschnittstemperaturen im Sommer kaum höher als 25 °C, im Winter um 17 °C.

Hinzu kommt eine wesentliche Abhängigkeit des Klimas von der Topografie der Inseln. Der Nordostpassat hat dabei wesentlichen Einfluss auf die hohen westlichen Inseln, deren Nordosten durch starke Wolkenbildung an den Gebirgen deutlich feuchter und kühler ist als deren Süden. Da auf diesen Inseln die Höhenunterschiede im Vergleich zur Fläche sehr groß sind, gibt es deutlich zu differenzierende vertikale Klimazonen. Diese reichen von den trocken-heißen Küstenregionen über die feucht-kühle und deshalb oft bewaldete Zone bis hin zu kühl-trockenen Zonen mit teilweisem Hochgebirgsklima. Wenig Einfluss hat der Passat auf die flachen östlichen Inseln Lanzarote und Fuerteventura, auf denen durchweg ein arides (trockenes) Klima herrscht. Außerdem existieren auf allen Inseln in Bereichen tiefer Schluchten und hoher Felswände Mikroklimata mit großer Pflanzenvielfalt.

Die Flora der Kanarischen Inseln zeichnet sich sowohl durch eine hohe Artenvielfalt als auch durch einen hohen Anteil an ortsspezifischen Pflanzenarten aus. Nach aktuellen Schätzungen gibt es auf den Kanaren rund 2000 Pflanzenarten, von denen 514 kanarische Endemiten sind, wovon wiederum 57 Prozent ausschließlich auf einer der Inseln vorkommen. Auf dem Archipel ist die Pflanzenwelt stark von der Höhenlage, der Regenmenge und der Bodenbeschaffenheit abhängig. Sie ist daher von Insel zu Insel äußerst unterschiedlich. In der Vegetationskunde werden die Kanaren zusammen mit Madeira der makaronesischen Region zugerechnet. Ganz allgemein betrachtet bestehen heute zur nordafrikanischen und mediterranen Flora enge verwandtschaftliche Beziehungen. Eine Besonderheit stellt die endemische Kanarische Kiefer dar, da bei Bränden ihre Rinde verkrustet und sie nachfolgend wieder Triebe ausbilden kann.
Die Tierwelt wird auf den Kanaren hauptsächlich von Reptilien und Vögeln bestimmt.

Die Eidechsengattung "Gallotia" ist auf der Inselgruppe endemisch. Sie umfasst insgesamt acht Arten (Ostkanareneidechse ("G. atlantica"), La-Palma-Rieseneidechse ("G. auaritae"), La-Gomera-Rieseneidechse ("G. bravoana"), Kleine Kanareneidechse ("G. caesaris"), Kanareneidechse ("G. galloti"), Teneriffa-Rieseneidechse ("G. intermedia"), El-Hierro-Rieseneidechse ("G. simonyi"), Gran-Canaria-Rieseneidechse ("G. stehlini")). Darunter sind einige sehr groß werdende Formen bis 80 Zentimeter Gesamtlänge, daher werden die Tiere auch als Rieseneidechsen bezeichnet.

Weitere dort vorkommende Reptiliengattungen sind:


Schlangen lebten ursprünglich nicht auf den Inseln, mittlerweile wurden jedoch auf Gran Canaria durch den Menschen ungiftige Kettennattern eingeführt, die sich stark vermehrt haben.
Ein Projekt mit einem Budget von etwa einer Million Euro und einer Laufzeit vom 1. September 2011 bis 1. September 2015 soll ihre Population deutlich und nachhaltig reduzieren.

Die größten Reptilien der Kanaren sind die in Küstennähe lebenden Meeresschildkröten (Cheloniidae). Überhaupt ist die Meeresfauna mit fast 550 Fischarten artenreich.
Erwähnenswert sind unter anderem einige Rochenarten, zahlreiche Haiarten wie Engelhaie, mehrere Hammerhaiarten, Makohaie, Weiße Haie, aber auch Schwertfische und Barrakudas sowie große Thunfische, Meerbrassen, Papageifische, Zacken- und Ziegelbarsche, Flügelbutte und der Pollack. Zudem konnten bisher 28 Wal- und Delfinarten im Archipel nachgewiesen werden, darunter auch das größte lebende Raubtier – der Pottwal.

In den dauerfeuchten Nebelwäldern haben sich auch Amphibien angesiedelt (Mittelmeer-Laubfrosch ("Hyla meridionalis"), Iberischer Wasserfrosch ("Pelophylax perezi")).

Die Vogelwelt der Kanaren setzt sich aus Endemiten der Kanaren und Madeiras, typischen Arten des Mittelmeerraums und Nordafrikas sowie paläarktischen Kosmopoliten zusammen. Von den Letzteren haben sich hier bei zahlreichen Arten endemische Unterarten herausgebildet.

Zu den endemischen Arten, die auch auf Madeira heimisch sind, zählen der Einfarbsegler, der Kanarenpieper und der Kanarengirlitz, die wilde Stammform des Kanarienvogels. Der Kanaren-Zilpzalp ("Phylloscopus canariensis") brütet auf allen Kanareninseln, das Kanarengoldhähnchen lediglich auf den westlich gelegenen. Die Lorbeertaube und Bolles Lorbeertaube kommen nur auf La Palma, Teneriffa und Gomera, die letztere Art zudem auf El Hierro vor. Der Teydefink ist nur auf Teneriffa und Gran Canaria zu finden, der Kanarenschmätzer ("Saxicola dacotiae") brütet mit etwa 1000 Paaren lediglich auf Fuerteventura.

Beispiele für typische Vogelarten aus dem Mittelmeerraum sind der Eleonorenfalke, die Mittelmeermöwe (ssp. "atlantis"), Samtkopf-Grasmücke (ssp. "leucogastra") und Brillengrasmücke (ssp. "orbitalis"), der Weidensperling und der Südliche Raubwürger (ssp. "koeningi"). Beispiele für Vertreter der nordafrikanischen Avifauna sind das Felsenhuhn, der Wüstenfalke, die Kragentrappe, das Sandflughuhn, der Rennvogel und der Wüstengimpel. Bei den hier endemischen Unterarten europäisch verbreiteter Vögel sind vor allem die des Buchfinken ("F. c. canariensis" und "palmae") und der Blaumeise ("C. c. teneriffae") zu nennen, die von den Nominatformen deutlich abweichen. Weitere endemische Unterarten gibt es von Turmfalke, Schleier- und Waldohreule, Buntspecht, Gebirgsstelze, Amsel, Mönchsgrasmücke und Rotkehlchen.

Mit mehreren tausend Arten ist die Insektenwelt vertreten. Dazu gehören zahlreiche Schmetterlinge mit endemischen Arten wie der Kanaren-Weißling ("Pieris cheiranthi"), der Kanarische Admiral ("Vanessa vulcania") und das Kanaren-Waldbrettspiel ("Pararge xiphioides"). Häufig anzutreffen sind Libellen. Auch Heuschrecken sind häufig. Lästig werden konnten aus Afrika kommende gefräßige Heuschreckenschwärme bis in die Mitte des zwanzigsten Jahrhunderts. Heute werden diese schon draußen auf dem Meer mit Insektiziden so traktiert, dass es diese Bedrohung praktisch nicht mehr gibt.

Aus wirtschaftlichen Gründen wurde aus Mittelamerika die Cochenilleschildlaus nebst ihrer Wirtspflanze, der Opuntie, eingeführt. Aus den Läusen wurde ein karminroter Farbstoff gewonnen, der mittlerweile aber synthetisch hergestellt wird.

Die wenigen freilebenden Säugetiere sind mit Ausnahme der meisten Fledermäuse (wie der Madeira-Fledermaus, der Alpenfledermaus oder der Weißrandfledermaus) erst nach der Eroberung ausgesetzt worden oder als Mitreisende eingewandert. Erst in den 1980er Jahren wurde auf Fuerteventura und nachfolgend auch auf Lanzarote eine endemische Spitzmausart entdeckt und als eigenständige Art erfasst. Insbesondere die Wildziege, der Europäische Mufflon, der Mähnenspringer und das Wildkaninchen richten an der endemischen Pflanzenwelt der Kanaren schwere Schäden an. Verwilderte Hauskatzen werden dafür verantwortlich gemacht, dass die großen Echsen der westlichen Inseln heute vom Aussterben bedroht sind.
Als Haustiere gehalten werden: Ziegen, Schafe, Pferde, Esel, Mulis, Schweine, Kühe, Hunde, Katzen, Dromedare (als Lasttier gehalten auf Lanzarote, Fuerteventura, Gran Canaria), Hühner und Puten.

Insgesamt gibt es auf dem Archipel 146 Naturschutzgebiete, die sich auf 301.335 Hektar ausdehnen. Darunter sind elf Naturparks mit einer Gesamtfläche von 111.022 Hektar sowie 131 verschiedene Naturreservate, Naturdenkmäler, Landschaftsschutzgebiete, Orte von wissenschaftlichem Interesse und ländliche Parks. Ebenfalls dazugehörig, haben die Kanaren in ihrer autonomen Gemeinschaft die meisten Nationalparks in Spanien, vier von insgesamt 13, mit einer Gesamtfläche von 32.681 Hektar:

Seit Dezember 2006 gehören die Kanaren zur Gruppe der besonders schutzbedürftigen Meeresgebiete der Internationalen Seeschifffahrts-Organisation IMO. Das bedeutet, dass ein Umkreis von zwölf Seemeilen (rund 22,2 Kilometer) für den Transit von Schiffen mit gefährlicher Fracht komplett gesperrt ist.

Bis ins 15. Jahrhundert war der Archipel von den einzelnen Inselbevölkerungen, die keinen Kontakt zueinander hatten, bewohnt. Obwohl sie sich in Sprache und Kultur unterschieden, wird in der Populärwissenschaftlichen Literatur häufig für alle Insulaner der Name der größten Gruppe "Guanchen", der Ureinwohner der Insel Teneriffa, verwendet.
Durch die spanische Eroberung wurde zwar deren Kultur nahezu vernichtet, jedoch vermischten sich viele dieser Ureinwohner mit den neuen Siedlern. Etwa ein Drittel der heutigen Bevölkerung der Inseln stammt in weiblicher Linie von den Altkanariern ab. Die neuen Siedler, die im 16. Jahrhundert auf die Inseln kamen, wanderten aus dem Süden der spanischen Halbinsel und aus dem heutigen Portugal ein.

Die Bevölkerung der Kanarischen Inseln ist im vergangenen Jahrhundert stetig angestiegen. Der Trend verstärkt sich weiter, sodass laut spanischem Statistikamt im Mai 2006 die Zwei-Millionen-Einwohner-Marke überschritten worden ist. Die Entwicklung der Bevölkerung nach dem Einwohnermelderegister "(Padrón Municipal de habitantes)" ab 2003 stellt sich wie folgt dar:<ref name="CeC2007/08/6"></ref>

Das starke Bevölkerungswachstum in der ersten Dekade des Jahrtausends resultiert hauptsächlich aus Einwanderungen, angeführt von Bürgern aus der Europäischen Union (ohne Spanien: 129.039) und den Ländern Lateinamerikas (77.502).<ref name="CeC2007/08/9"></ref>

Insgesamt leben im Jahr 2007 (Stand: 1. Januar) 250.736 behördlich registrierte Bürger ohne spanische Staatsangehörigkeit auf den Kanaren, das entspricht einem Anteil von 12,36 %, der größte Teil davon (114.500) lebt auf Teneriffa und Gran Canaria (64.662).
Hinzu kommen noch 25.928 Bürger aus anderen autonomen Gemeinschaften Spaniens (Stand: 2006).

Die folgende Liste enthält die 25 größten Gemeinden der Kanarischen Inseln (Stand: 1. Januar 2012).
Die jeweils größte Gemeinde einer Insel ist in Fettschrift. Zum Vergleich wurden die beiden größten Gemeinden von La Gomera und El Hierro in die Liste aufgenommen.

Der größte Teil der kanarischen Bevölkerung ist römisch-katholisch getauft. Über die Mitgliederzahlen der verschiedenen Religionsgemeinschaften gibt es keine verlässlichen Zahlen. Die Anzahl der Kultstätten wurde 2012 in einer Untersuchung festgestellt.
Evangelische Kirchen 155, Zeugen Jehovas 45, Muslime 29, Buddhisten 14, Adventisten 10, Mormonen 9, Bahá'í 8, Anglikaner 5, Juden 5.

Die römisch-katholische Kirche auf den Kanaren besteht seit der Aufteilung eines bis dahin gemeinsamen Bistums im Jahre 1819 aus zwei dem Erzbistum Sevilla als Suffraganbistümer unterstellten Diözesen:

Die Diözese der Kanarischen Inseln ("Diócesis de Canarias") erstreckt sich über die Provinz Las Palmas und ist damit für die östlichen Inseln Lanzarote, Fuerteventura und Gran Canaria zuständig. Bischofskirche ist die Kathedrale Santa Ana. Sitz des Bischofs ist der Bischofspalast ("Palacio Episcopal") am "Plaza de Santa Ana" gegenüber der Kathedrale.

Die Diözese San Cristóbal de La Laguna ("Diócesis Nivariense" oder "Diócesis de Tenerife") wurde am 1. Februar 1819 von Papst Pius VII. errichtet und ist mit dem Gebiet der Provinz Santa Cruz de Tenerife deckungsgleich. Sie hat ihren Sitz in San Cristóbal de La Laguna auf Teneriffa und ist für die westlichen Inseln Teneriffa, La Palma, La Gomera und El Hierro zuständig. Die Kathedrale Santa María de los Remedios war vor der Aufteilung des Archipels auf zwei Diözesen eine Gemeindekirche. Der Wohnsitz des Bischofs und Sitz des Ordinariates ist der "Palacio Salazar".

Die Jungfrau von Candelaria ist die Schutzheilige der Kanarischen Inseln. Ihr zu Ehren wurde in der Gemeinde Candelaria die Basilika von Candelaria als Wallfahrtskirche gebaut. 

Die beiden von der katholischen Kirche heiliggesprochenen Missionare in Brasilien und Guatemala José de Anchieta (1534–1597) und Peter von Betancurt (1626–1667) waren auf Teneriffa geboren und werden an ihren Geburtsorten verehrt.

Die offizielle Landessprache auf den Kanarischen Inseln ist heute Spanisch. Die auf der kanarischen Inselgruppe gesprochene Art der spanischen Sprache bildet zusammen mit dem in Andalusien und Mittel- und Südamerika gesprochenen Spanisch eine Sprachgruppe die mit dem Begriff "Atlantisches Spanisch" (español atlántico) oder "Südliches Spanisch" (español meridional) bezeichnet wird.
Typische Besonderheiten sind das häufige Fehlen des Buchstabens "s" in der Aussprache besonders am Wortende und der Ersatz der 2. Person Plural durch die 3. Person Plural. Es gibt auch einige Wörter die auf dem Festland nicht gebräuchlich sind. (Beispiel: "guagua" statt "autobús").

Vor der spanischen Eroberung wurde auf jeder Kanareninsel eine eigene Variante der Ursprache Guanche gesprochen. Durch die systematische Vernichtung der Kultur der Altkanarier durch die Eroberer im 15. und 16. Jahrhundert sind nur noch wenige Sprachstücke überliefert. Dabei handelt es sich in erster Linie um Ortsbezeichnungen und Namen von auf den Inseln endemischen Pflanzen. Es wurden auch in Stein geritzte Zeichen gefunden die große Ähnlichkeit
mit dem Libysch-Berberischen haben. Es handelt sich dabei aber nicht um zusammenhängende Texte. Auf der Insel La Gomera hat sich eine Kommunikationsform der Ureinwohner erhalten bei der die akustischen Merkmale einer gesprochenen Sprache durch Pfiffe dargestellt werden. Die Basissprache des Silbo Gomero ist heute nicht mehr Guanche, sondern Spanisch.

Die Herkunft der Ureinwohner der Kanaren ist für die Forschung im Detail noch ungeklärt. Skelettfunde, aber vor allem überlieferte Ortsnamensbezeichnungen verweisen darauf, dass die Großzahl der eingewanderten oder dorthin verschleppten Ureinwohner aus Nordafrika kamen und von dem dort siedelnden Volk der Berber abstammen. Die Bewohner der einzelnen, in Sichtweite zueinander gelegenen Inseln hatten keinen oder nur sehr seltenen Kontakt zueinander. Diese Erkenntnis basiert auf dem derzeitigen Forschungsstand, dem zufolge die Ureinwohner der Kanaren über keine hochseetauglichen Boote verfügten.

Die größeren Inseln (wie Teneriffa) waren in mehrere Stammesgebiete unterteilt, die jeweils von einer Adelskaste regiert worden waren. Auf Teneriffa befanden sich die Gebiete der einzelnen Stämme entlang der Küste. Das dauerhaft fruchtbare Hochplateau, aus dem der Kegel des Vulkans Teide aufragt, war in vorkolonialer Zeit neutrales Land, das von allen Stämmen gemeinsam als Weideland in den Sommermonaten genutzt worden war. Der Adelsstand wurde nicht vererbt, sondern durch eine Auswahl bereits gewählter Adeliger erworben.

Den nur spärlich erhaltenen Überlieferungen von Chronisten folgend gab es auch auf den kleineren Inseln wie La Gomera und El Hierro eine solche Adelskaste. Die Bevölkerungsdichte war auf allen Inseln gering. Auf Lanzarote, die als erste der Kanarischen Inseln erobert wurde, zählte man nur 400 Ureinwohner.

Die Urbevölkerung wird als großgewachsen und hellhäutig beschrieben. Skelettfunde belegen, dass viele Urkanarier blonde oder rötliche Haare gehabt haben. Da es auf den Vulkaninseln keinerlei Metalle gibt, fertigten sie ihre Waffen aus Holz, Steinen, Obsidian und Knochen. Pfeil und Bogen, das Rad sowie eine allgemein gebräuchliche Schrift waren den Ureinwohnern unbekannt. In Steinritzungen enthaltene seltene Inschriften verweisen auf die Berberabstammung, sind aber nur als Ausnahme feststellbar.

Die vorkoloniale Bevölkerung der Kanaren erwirtschaftete die Grundlage ihrer Existenz zuvorderst als Hirten und Ackerbauern – seltene Netzfunde belegen aber auch, dass zumindest auch Strandfischerei betrieben worden ist. Für den Fischfang wurden auch Euphorbiengewächse eingesetzt, die in flache Lagunen geworfen wurden und deren Wolfsmilchsaft die Fische durch Vergiftung kurzzeitig betäubte. Des Weiteren belegen Muschelschalenfunde in von Archäologen entdeckten Wohnstätten eine diesbezügliche Sammleraktivität der Ureinwohner. Auch die Jagd ist als Nahrungsergänzung nachgewiesen.

Weitere archäologische Funde beweisen auch, dass natürliche Höhlen des Vulkangesteins von den Ureinwohnern oft als Wohnstatt genutzt worden sind; aber auch, dass einfache Hütten sowie (selten) ohne Mörtel erbaute Steinhäuser zu Wohnzwecken, aber auch für Versammlungen erbaut worden sind. Das in vorkolonialer Zeit bereits existente Maurerhandwerk – das ohne die Verwendung eines Mörtels stabile Mauern errichten konnte (Verkeiltechnik) - ist auf allen Kanarischen Inseln bis in die Neuzeit hinein in meisterlicher Perfektion erhalten geblieben. Von Archäologen entdeckte Feuerstellen in ausgegrabenen Hütten, Häusern oder auch als Wohnstatt genutzten Höhlen wurden mit der Radiokarbonmethode untersucht – keine Feuerstelle war dem Untersuchungsergebnis zufolge älter als 3000 Jahre. Nur wenig ist über die soziale Kultur der Ureinwohner überliefert worden. Neben dem schon erwähnten Kastenwesen wurde berichtet, dass die Frauen ihre Sexualität polygam ausgerichtet haben, dies vermutlich wegen der Inzuchtgefahr.

Frauen standen allgemein in hohem Ansehen und waren in der Adelsschicht häufig vertreten. Auf Gran Canaria war zum Zeitpunkt der Eroberung das Inseloberhaupt eine Guanchin. Auch der Priesterstand rekrutierte sich überwiegend aus Frauen, die auch medizinische Operationen durchgeführt haben. Im archäologischen Museum von Santa Cruz sind Schädel ausgestellt, an denen Hirnoperationen vorgenommen worden sind. Die Verheilung der Knocheneinschnitte beweist hier, dass diese Operationen auch überlebt worden sind.

Zu Beginn des 14. Jahrhunderts waren die europäischen Seefahrer durch Verbesserungen der nautischen Instrumente und Aufzeichnungen über die Strömungs- und Windverhältnisse in der Lage, die Inseln im Atlantik nicht nur zu besuchen, sondern auch wieder sicher zurück zu kommen. Die Besuche, von denen offenbar nur wenige dokumentiert sind, ergaben aber auch, dass sich eine Reise mit dem Ziel, die Produkte der Inseln, hauptsächlich Ziegenfelle und Roccella, einzutauschen, nicht rentierte. Zweck der Reisen auf die Kanarischen Inseln war es, dort eine Basisstation für die Erforschung der Küste Afrikas zu errichten, um in Afrika an das Gold zu kommen, das in Europa immer knapper wurde.

Über den ersten Unternehmer, von dem bekannt ist, dass er zu Beginn des 14. Jahrhunderts eine solche Basisstation auf der Afrika am Nächsten gelegenen Insel anlegte, den Genuesen Lancelotto Malocello, gibt es kaum gesichertes Wissen. Ein Eintrag in einer Seekarte des mallorquinischen Kartografen Angelino Dulcert aus dem Jahr 1339 bezeichnet diese Insel als Insel des Genuesen Lancelotto Malocello. Sie trägt seither den Namen Lanzarote.

Ein Teilnehmer der 1341 von dem portugiesischen König Alfons IV. ausgerüsteten Forschungsreise zu den Kanarischen Inseln, der Genuese Niccoloso da Recco, schrieb einen ausführlichen Bericht über die Inseln, in dem er auch Angaben über die Bevölkerung machte.

Im Jahr 1344 gründete Papst Clemens VI. das Fürstentum der Glücklichen Inseln als Lehen des Heiligen Stuhls. Er ernannte Luis de la Cerda, einen Adeligen, der den Königsfamilien Kastiliens und Frankreichs entstammte, zum Fürsten und übergab ihm eine goldene Krone und ein Zepter. Luis de la Cerda verpflichtete sich, seine Untertanen zum wahren Glauben zu bekehren. Nachdem er 1346 oder 1348 gestorben war, ohne sein Fürstentum gesehen zu haben, wurde dieses in keinem päpstlichen Dokument mehr genannt.

Als mallorquinische Gläubige einen Neubeginn der Christianisierung der Kanarischen Inseln mit friedlichen Mitteln planten, gab Papst Clemens VI. dazu seinen Segen und gründete 1351 das "Bistum der Glücklichen Inseln" (Fortunatae Insulae), das später den Namen Bistum Telde erhielt. Die friedliche Missionsarbeit durch Mallorquiner und Katalanen endete vermutlich 1393, als Einheimische als Reaktion auf räuberische Überfälle durch westeuropäische Piraten die Missionare umbrachten.

Zu Beginn des 15. Jahrhunderts erwirkte Robert de Braquemont, der Vertreter des französischen Königs, bei Heinrich III. von Kastilien für seinen Neffen Jean de Béthencourt die Erlaubnis die Kanarischen Inseln zu erobern und als kastilisches Lehen zu regieren. Im Jahr 1402 errichteten Jean de Béthencourt auf der Insel Lanzarote einen Stützpunkt. Im Verlauf der nächsten Jahre gelang es ihm, anfangs meist nahezu kampflos, die Oberhoheit der Krone von Kastilien auch auf den Inseln Fuerteventura und El Hierro durchzusetzen. Die Inbesitznahme der Inseln La Gomera, Gran Canaria, Teneriffa und La Palma scheiterte am Widerstand der Einwohner. Über den Ablauf der Eroberung gibt es einen Bericht mit dem Titel Le Canarien der in zwei Versionen überliefert ist.

Am 7. Juli 1404 gründete Papst Benedikt XIII. ein neues Bistum für die Kanarischen Inseln auf der Insel Lanzarote, das Bistum San Marcial del Rubicón, ohne dabei einen Bezug auf das untergegangene Bistum Telde herzustellen.

Im Jahr 1418 verkaufte Maciot de Béthencourt, der Neffe des Eroberers, die Ansprüche auf die Herrschaft über die Inseln. Im Verlauf der nächsten Jahre gingen die Rechte mehrfach auf andere Personen über. Im Jahr 1445 erhielt Hernán Peraza (der Ältere, 1390–1452) die Regierungsgewalt über die eroberten Inseln und den Anspruch auf die noch zu erobernden Inseln. Er stellte die Oberhoheit der Krone von Kastilien auf den Inseln außer Gran Canaria, Teneriffa und La Palma sicher. Seine Versuche auch diese Inseln zu erobern blieben erfolglos. Am Ende seines Lebens musste Hernán Peraza einsehen, dass das Unternehmen, die großen Kanarischen Inseln zu erobern, seine wirtschaftlichen Möglichkeiten überstieg. Seine Tochter Inés de Peraza erbte 1454 die Herrschaft. Im Jahr 1477 übernahmen die Katholischen Könige das Recht zur Eroberung der drei großen Inseln Gran Canaria, La Palma und Teneriffa wieder für die Krone von Kastilien. Sie entschädigten die Familie Peraza-Herrera mit fünf Millionen Maravedíes und dem Recht sich Grafen von Gomera und El Hierro zu nennen.
Im Verlauf des Kastilischen Erbfolgekrieges versuchten die Katholischen Könige ihre Ansprüche auf ein Recht des Handels mit Afrika zu bekräftigen. Sie schickten Handelsflotten, die aus Schiffen von Händlern aus den Ländern der Krone von Kastilien bestanden, auf Fahrten, die über die Kanarischen Inseln an die Küste Afrikas gingen, um dort Waren gegen Gold und Sklaven einzutauschen. Alle Versuche Handelsbeziehungen zwischen Afrika und Kastilien aufzubauen wurden aber von Portugal mit militärischer Gewalt verhindert. Die Katholischen Könige mussten einsehen, dass sie das portugiesische Monopol auf den Afrikahandel nicht durchbrechen konnten. Daher verzichtete Kastilien in den Verträgen von Alcáçovas auf die Seefahrt südlich des Kaps Bojador und bekam dafür die Garantie auf die Herrschaft über die Kanarischen Inseln und einen Streifen der Afrikanischen Küste zwischen Kap Nun und Kap Bojador zugesprochen. Südlich des Kaps Bojador war kastilischen Schiffen jegliche Zufahrt verboten. Die erste Reise des Christoph Kolumbus verlief daher auch genau auf dieser Grenze nach Westen.

Während die Kanarischen Inseln im 14. bis 16. Jahrhundert hauptsächlich als Ausgangspunkt eines Handels mit Afrika gesehen wurde, bekamen sie nach 1492 eine Bedeutung als Ausgangspunkt für die Erschließung Amerikas. Der Hauptgrund dafür war, dass von den Inseln aus Wind und Strömung die Überfahrt nach Amerika leichter machten als von Europa aus. Auch zur Zeit der Dampfschifffahrt liefen die Verkehrswege über die Kanarischen Inseln, auf denen vor der Überfahrt nach Amerika Wasser und Kohle, die aus England importiert war, gebunkert werden konnte.

1478 begannen die Katholischen Könige damit, ihre Herrschaft auch über die Inseln Gran Canaria, La Palma und Teneriffa durchzusetzen. Die Kosten für die Eroberung wurden nicht durch die Krone, sondern durch die Konquistadoren bzw. durch private Geldgebern aufgebracht. Diese sollten aus den zukünftigen Gewinnen der eroberten Territorien entschädigt werden. Nach einigen Schwierigkeiten gelang es bis 1483 die Insel Gran Canaria zu unterwerfen. Im Jahr 1493 eroberte Alonso Fernández de Lugo die Insel La Palma. Bei der Unterwerfung der Einwohner der Insel Teneriffa wurden die kastilischen Truppen 1494 nahe dem heutigen Ort La Matanza de Acentejo von den Guanchen geschlagen. Bei einem neuen Angriff 1495 siegten die Truppen der Invasoren in der Schlacht von Aguere bei der heutigen Stadt La Laguna und der zweiten Schlacht von Acentejo bei der heutigen Stadt La Victoria. Die Guanchen ergaben sich 1496. Bei beiden Eroberungsfeldzügen kämpften auf der Seite der Kastilier auch Altkanarier von anderen Kanarischen Inseln und Mitglieder der Guanchenstämme aus dem Süden der Insel Teneriffa.

In der folgenden Zeit entwickelten sich die Kanarischen Inseln zu einem Teil des Königreiches Kastiliens bzw. zu einem gesonderten Königreich der Krone von Kastilien. Im Jahr 1494 erhielt Las Palmas de Gran Canaria Stadtrechte wie sie bei Städten in Kastilien üblich waren. Die städtischen Verwaltungen wurden nach dem Muster von Sevilla organisiert. Es galt kastilisches Recht. Die Altkanarier traten nicht, wie später die Ureinwohner Amerikas, als zu berücksichtigende ethnische Minderheit auf. Von den auf dem Festland abweichenden Rechten wie z.B. das zeitweise gewährte Recht des direkten Handels mit Amerika wurden mit der geografischen Lage gerechtfertigt.

Während des Spanischen Erbfolgekrieges ließen sich die Einwohner auch durch den Angriff des englischen Admirals John Jennings auf Santa Cruz de Tenerife nicht von ihrer Unterstützung des Königs Philipp V. abbringen. Von den Napoleonischen Kriegen auf der Iberischen Halbinsel gegen eine französische Machtübernahme in Spanien war die Inselgruppe aufgrund ihrer Lage kaum betroffen. In La Laguna bildete sich die „Junta Suprema de Canarias“ die auch in der Junta Suprema Central auf der spanischen Halbinsel vertreten war. In den verfassungsgebenden Cortes von Cádiz waren die Kanarischen Inseln wie andere Teile Festlandspaniens vertreten. Bei der Neueinteilung des Königreiches Spanien in 49 Provinzen im Jahr 1833 stand die Provinz Santa Cruz de Tenerife, die damals noch alle Kanarischen Inseln umfasste, in einer Reihe mit den Provinzen der Halbinsel. Die Provinz Santa Cruz de Tenerife war, bei den Wahlen im 19. und zu Beginn des 20. Jahrhunderts, abgesehen von dem geografisch bedingten abweichenden Wahltermin, ein Wahlkreis wie alle anderen Provinzen.

Während der Zeit der Zweiten Republik wurden verschiedene Autonomiestatute für die Kanarischen Inseln entworfen von denen aber keines bis 1937 in Kraft treten konnte. Während des Spanischen Bürgerkriegs kam es auf den Kanarischen Inseln nicht zu kriegerischen Auseinandersetzungen. Sehr viele Anhänger der gewählten Regierung und Gegner der franquistischen Regierung wurden in besonders hierfür eingerichteten Lagern interniert. Viele wurden ermordet bzw. verschwanden. Während des Zweiten Weltkrieges waren die Inseln von nahezu allen wirtschaftlichen Beziehungen abgeschnitten. Sowohl die deutsche Wehrmacht als auch die Alliierten hatten Pläne die Inseln als Stützpunkt zu besetzen. Nach dem Tod Francisco Francos beschloss das Spanische Parlament 1982 ein Autonomiestatut (ähnlich einer deutschen Landesverfassung) für die Kanarischen Inseln. Die „Comunidad Autónoma de Canarias“ ist heute eine der 17 Autonomen Gemeinschaften Spaniens. Der Día de Canarias am 30. Mai ist der Feiertag der Kanarischen Inseln. Am 30. Mai 1983 ist das erste Parlament der "Comunidad Autónoma de Canarias" zusammengetreten.

Die Inseln werden von Flüchtlingen aus Afrika unter sehr schwierigen Bedingungen, nämlich der Überquerung des Kanarenstroms, mit kleinen Booten angesteuert. Bisheriges Rekordjahr war 2002 mit 9.929 Flüchtlingen. 2005 waren es nach Angaben des spanischen Innenministeriums noch 4.751. Im Jahr 2006 gab es einen Anstieg auf etwa 31.000 Migranten, wobei im September erstmals auch asiatische Flüchtlinge aufgegriffen wurden. Dieser enorme Aufwärtstrend setzte sich mit etwa 10.000 Einwanderern im Jahr 2007 nicht fort (2008 etwa 8.000 illegale Einwanderer). Die trotzdem hohe Zahl wird mit der verstärkten Bewachung der Straße von Gibraltar und der Ausstellung von Aufenthalts- und Arbeitserlaubnissen für illegale Einwanderer in der Vergangenheit durch die spanische Regierung in Verbindung gebracht. Von denen, die im Jahre 2006 die Kanaren erreicht haben, konnten nur knapp 10 Prozent in ihre Heimat zurückgebracht werden. Gründe sind das Schweigen der Flüchtlinge zu ihrer Herkunft sowie noch nicht existierende Rückführungsabkommen mit den afrikanischen Ländern. Im November 2006 fand erstmals die „EU-Afrika-Konferenz für Migration und Entwicklung“ statt, die sich ebenfalls mit der illegalen Einwanderung beschäftigte. Der Einsatz der Frontex soll nach Verdoppelung der EU-Gelder ab 2008 permanenten Charakter bekommen. Die Grenzkontrollmission versucht mit Hilfe verschiedener europäischer Patrouillenboote, unterstützt von Aufklärungsflugzeugen, das Ablegen von Flüchtlingsbooten an der afrikanischen Küste zu verhindern. Hier, insbesondere an der senegalesischen Küste, startet etwa die Hälfte aller Flüchtlingstransporte. Es gibt eine Schätzung der Guardia Civil, dass zwischen dem 1. Januar 2006 und dem 21. August 2007 mindestens 1.260 Bootsflüchtlinge bei der Überfahrt von Afrika auf die Kanaren ums Leben gekommen seien.
Andere vermuten eine mehrfach höhere Opferzahl.

Das Wappen zeigt im blauen Wappenschild zweimal drei pfahlweise gestellte silberne Dreiecke und ein Dreieck im Schildfuß, die die sieben Hauptinseln symbolisieren. Eine goldene Krone ruht auf dem Schild, darüber im schwebenden silbernen Band die Devise in Majuskeln ‚Océano‘.

Schildhalter sind zwei braune Hunde mit blauem Halsband. Dem Wappen fehlt das Postament.

Im Jahr 1991 hat die Regierung der Kanarischen Inseln den Kanarengirlitz ("Serinus canaria") und die Kanarische Dattelpalme ("Phoenix canariensis") zu Symbolen der Kanarischen Inseln erklärt.

Das gesamtspanische Parlament in Madrid (Cortes Generales) besteht aus zwei Kammern: dem Abgeordnetenhaus (350 Sitze) und dem Senat (266 Sitze).

Die Bevölkerung der Kanarischen Inseln ist im Abgeordnetenhaus (Congreso de los Diputados) in Madrid durch 15 Abgeordnete vertreten. Im Wahlkreis, der die Provinz Las Palmas de Gran Canaria umfasst, werden acht Abgeordnete, im Wahlkreis, der die Provinz Santa Cruz de Tenerife umfasst, werden sieben Abgeordnete in einer Listenwahl nach dem Verhältniswahlrecht gewählt.

Bei den Wahlen zum Senat wählen die Bewohner der Inseln Gran Canaria und Teneriffa je drei Senatoren, die von Fuerteventura, La Gomera, El Hierro, Lanzarote und La Palma je einen Senator in einer direkten Verhältniswahl. Drei weitere Senatoren werden vom Parlament der Autonomen Gemeinschaft der Kanarischen Inseln bestimmt.

Die Kanarischen Inseln haben ein Einkammerparlament. Wahlberechtigt sind alle volljährigen Spanier, die auf dem Gebiet der Kanarischen Inseln mit Erstwohnsitz gemeldet sind. Das Parlament wird in allgemeinen, direkten, gleichen, freien und geheimen Wahlen beschickt. Wahlkreise sind die einzelnen Inseln. Innerhalb dieser Wahlkreise werden die Abgeordneten in einer Verhältniswahl bestimmt. Nach dem Autonomiestatut muss das Parlament 50 bis 70 Abgeordnete haben.
Die Inseln entsenden entsprechend ihrer Einwohnerzahl unterschiedlich viele Abgeordnete in das Parlament:
Bei der Sitzverteilung werden nur die Parteien berücksichtigt, auf die auf der jeweiligen Insel mindestens 30 % oder aber auf den Kanaren insgesamt mindestens 6 % der Stimmen entfallen sind.

Der Sitz des Parlaments ist in Santa Cruz de Tenerife.

Bei der Wahl vom 24. Mai 2015 haben 931.876 Wähler von 1.661.272 Wahlberechtigten ihre Stimme abgegeben, was einer Wahlbeteiligung von 56,1 % entspricht. Es formte sich eine Regierungskoalition aus Coalición Canaria – Partido Nacionalista Canario (CC-PNC) und PSOE die aus folgender Sitzverteilung resultiert:

Die Regierung der Kanarischen Inseln besteht aus dem Präsidenten (Presidente del Gobierno), dem Vizepräsidenten und den Ministern (Consejeros).

Der Präsident wird vom Parlament gewählt. Er muss Mitglied des Parlaments sein und wird nach der Wahl vom König ernannt. Der Präsident ernennt und entlässt den Vizepräsidenten und die übrigen Regierungsmitglieder. Der Vizepräsident muss Mitglied des Parlamentes sein. Die Zahl der Regierungsmitglieder ist auf elf begrenzt.

Der Sitz des Präsidenten wechselt mit jeder Legislaturperiode des Parlamentes. In der Zeit von 2015 bis 2019 ist der Sitz Las Palmas de Gran Canaria, der des Vizepräsidenten Santa Cruz de Tenerife.
Präsident der Autonomen Gemeinschaft der Kanarischen Inseln ist seit 2015 Fernando Clavijo Batlle von der regionalen Partei Coalición Canaria (CC). Nach den Wahlen am 24. Mai 2015 bildete die Coalición Canaria – Partido Nacionalista Canario mit der PSOE eine Koalitionsregierung:
Im Jahr 1927 wurden die seit 1833 bestehende Provinz Santa Cruz de Tenerife (Provinzen wurden üblicherweise nach der Provinzhauptstadt benannt) aufgeteilt und damit eine neue Provinz Las Palmas de Gran Canaria geschaffen. Über diesen beiden Provinzen gab es auf den Kanarischen Inseln keine übergeordnete Verwaltungseinheit. Die Regierung in Madrid war in beiden Provinzen durch einen Zivilgouverneur vertreten.

Die Verfassung des Königreichs Spanien von 1978 sieht in Artikel 141 Provinzen grundsätzlich als Verwaltungseinheiten mit einem Provinzialrat und eigener Provinzverwaltung vor. Im Absatz 4 des Artikels wird allerdings für die „archipiélagos“, also für die Balearischen Inseln und die Kanarischen Inseln, die Bildung von Inselregierungen („Consejos“ bzw. „Cabildos“) bestimmt. Daraus ergibt sich, dass die Einteilung der Kanarischen Inseln in Provinzen nahezu bedeutungslos ist, da es keine Provinzverwaltungen gibt. Die Provinzen sind weiterhin Wahlkreise bei der Wahl zum gesamtspanischen Parlament ("Cortes Generales"). Die Regierung in Madrid ist anstelle des früheren Zivilgouverneurs durch eine Subdelegación del Gobierno in beiden Provinzhauptstädten vertreten.

Die autonome Gemeinschaft der Kanaren besteht aus zwei Provinzen:

Die Verfassung von 1978 sieht für die Kanarischen Inseln Cabildos Insulares als auf die einzelnen Inseln begrenzte Regierungs-, Verwaltungs- und Repräsentationsorgane vor. Die Cabildos werden in allgemeinen Wahlen gewählt.

Die Verfassung von 1978 garantiert die Selbständigkeit der Städte. Das Rathaus (Ayuntamiento) besteht aus der Ratsversammlung (Pleno) und der Stadtregierung (Junta de Gobierno Local).

Die Ratsversammlung wird in allgemeiner, gleicher, freier, direkter und geheimer Wahl beschickt. Bei dieser Wahl sind auch Bürger der Europäischen Union wahlberechtigt, wenn sie mit ihrem ersten Wohnsitz in der Gemeinde gemeldet sind (sogenannte Residentes). Der Bürgermeister ist Vorsitzender der Ratsversammlung (Presidente del Ayuntamiento).

Die Stadtregierung besteht aus dem Bürgermeister und seinen Stellvertretern, die Ratsmitglieder sein müssen, und weiteren Mitgliedern. Die Stadtregierung (Junta de Gobierno) leitet die Stadtverwaltung.

Die Städte der Kanarischen Inseln sind sehr unterschiedlich groß: Von 8,73 km² (Puerto de la Cruz) bis zu 383,52 km² (Pájara) und von 770 Einwohnern (Betancuria) bis zu 381.271 Einwohnern (Las Palmas de Gran Canaria). Daraus ergibt sich eine sehr unterschiedlich Bedeutung der Aktivitäten der Ayuntamientos.

Alle wichtigen Länder der Welt sind mit Konsulaten vertreten, von denen einige darüber hinaus ein Wirtschafts- und Handelsbüro unterhalten.

Das Bruttoinlandsprodukt (BIP) der kanarischen Wirtschaft betrug im Jahr 2007 rund 42,4 Mrd. Euro. Rund 71 % davon wurden im Dienstleistungssektor (inklusive Tourismus) erwirtschaftet, gefolgt vom Baugewerbe (4,7 Mrd.), Industrie und Handel (1,7 Mrd.), Energie (1,1 Mrd.) sowie Landwirtschaft, Viehwirtschaft und Fischerei (0,5 Mrd.).<ref name="CeC2007/08/50"></ref>

Insgesamt wuchs das kanarische BIP von 2006 auf 2007 um 7,17 %.<ref name="CeC2007/08/51"></ref>

Auf den Inseln gibt es rund 1.250 Zweigstellen der wichtigsten spanischen und vieler ausländischer Banken.

Personen, auch EU-Ausländer, die auf den Kanaren mit Erstwohnsitz gemeldet sind, erhalten mit ihrer Meldebescheinigung Vergünstigungen, wie beispielsweise Ermäßigungen auf interinsuläre Flüge und Fähren sowie für Flüge auf das spanische Festland und die Balearen.

Vor dem Aufkommen des Massentourismus in den 1960er Jahren lebten die Kanarios vor allem von Landwirtschaft, Viehzucht und Fischerei. Dieser Wirtschaftszweig spielt heute, gemessen am BIP, eine vergleichsweise geringe Rolle.

Der Großteil der landwirtschaftlichen Produktion wird durch den Anbau von Bananen erzielt; weitere nennenswerte Erzeugnisse sind Tomaten, Gurken, Schnittblumen, Kartoffeln und Wein. Es werden aber auch andere Südfrüchte wie Clementinen, Zitronen, Papaya und Mango angebaut. Die gesamte landwirtschaftliche Nutzfläche im Jahr 2006 belief sich auf 51.867 Hektar.

Der mit Abstand wichtigste Wirtschaftszweig ist der Tourismus. Die wichtigsten touristischen Zentren befinden sich auf Teneriffa, Gran Canaria, Fuerteventura und Lanzarote. Im Jahr 2007 wurden rund 14,2 Mrd. Euro, also 31,09 % des BIP in der Tourismusbranche umgesetzt (2006: 13,5 Mrd.).
Die Zahl der Gäste ging von 2006 auf 2007 um 6,6 % auf rund 9,33 Millionen zurück. Die weitaus meisten Touristen kommen aus Großbritannien, gefolgt von Deutschland, den Niederlanden und Irland; davon reisten 3.410.165 nach Teneriffa, 2.713.728 nach Gran Canaria, 1.471.979 nach Fuerteventura und 1.618.335 nach Lanzarote.

Im Jahr 2012 sank die Zahl der Besucher vom spanischen Festland aufgrund der Wirtschaftskrise um rund 2,1 %. Insgesamt waren es mit 10.101.493 Touristen jedoch nur 0,4 % weniger Gäste. Im Jahr 2014 entspannte sich diese Lage wieder. Doch noch immer werden die Kanaren von 0,7 % weniger Festland-Spaniern besucht als vor der Krise. Bei Kreuzfahrern ist der Trend deutlich positiver: Allein Teneriffa verzeichnete mit 301 Schiffen und 545.000 Kreuzfahrern 2014 erneut einen Rekord. Insgesamt wurden die Kanarischen Inseln im Jahr 2014 von 11,4 Millionen ausländischen Besuchern besucht. Dies entspricht einem Plus von 8 % im Vergleich zum Vorjahr.

Die Industrie konzentriert sich hauptsächlich auf die Energie- und Wasserwirtschaft, aber auch auf Lebensmittel-, Tabak- und andere Leichtindustrien. Insgesamt ging die Industrieproduktion im Jahr 2007 um 0,3 % zurück (2006: −1,48 %).
Das Baugewerbe steht mit einem Anteil von 11,2 % am BIP an zweiter Stelle nach dem Dienstleistungssektor. Die Anzahl der Baugenehmigungen ist von 2006 auf 2007 von 5.053 auf 4.012 zurückgegangen.

Der wichtigste Handelspartner der Kanaren ist die Europäische Union, und hier besonders Spanien. Das Außenhandelsdefizit der Kanaren betrug im Jahr 2007 rund 14 Mrd. Euro; der größte Teil davon, rund 10 Mrd., ist auf Importe aus Spanien zurückzuführen.

Die Arbeitslosenquote lag im Oktober 2014 bei 33,36 %, womit sie sich seit 2007 verdreifacht hatte, aber im Vergleich zum Vorjahr um vier Prozent gefallen war.
Zum Ende des Jahres 2013 stieg die Anzahl der Beschäftigten im Vergleich zum Vorjahr um 3,81 % und 10.862 Personen, was die Arbeitslosenquote erstmals seit 1997 reduzierte.

Die Jugendarbeitslosigkeit ist auf den Kanaren die höchste in Europa. Sie lag 2013 bei den 16- bis 24-jährigen bei rund 70 Prozent. Dabei ließen sich etwa 44.000 Jugendliche in den Arbeitsämtern der beiden Provinzen aufgrund der Perspektivlosigkeit gar nicht mehr registrieren. Viele versuchen im Ausland, hauptsächlich in Deutschland und Südamerika, Fuß zu fassen.

Laut Nationalem Statistikamt (INE) betrug das Brutto-Monatsgehalt eines Arbeitnehmers auf den Kanaren im Jahr 2006 durchschnittlich 1300 Euro und ist damit das zweitniedrigste spanienweit. Hinzu kommt, dass ein kanarischer Arbeitnehmer mit durchschnittlich 146,1 Monatsstunden die drittlängste Arbeitszeit des Landes hat. 2005 hatte die autonome Region Kanarische Inseln den zweithöchsten Anstieg der Arbeitslosenquote Spaniens.

Seit Januar 2000 existiert die Sonderzone ZEC ("Zona Especial Canaria"), die zunächst bis 31. Dezember 2008 von der Europäischen Union genehmigt war, und im Januar 2007 von der EU bis ins Jahr 2019 verlängert wurde. Diese von der spanischen Zentralregierung und der Regionalregierung gegründete und dem spanischen Wirtschaftsministerium angegliederte Organisation hat zur Aufgabe, die wirtschaftliche und soziale Entwicklung des Archipels zu fördern und zu erweitern, damit sie nicht nur von den vorherrschenden Wirtschaftszweigen Tourismus und Bau abhängt. Deshalb gibt es sogenannte ZEC-Unternehmen, die sich zu bestimmten Investitionen und zur Schaffung von Arbeitsplätzen verpflichten, und somit beispielsweise von einem reduzierten Satz der spanischen Körperschaftsteuer von nur vier Prozent profitieren können (normal in Spanien 30 %). Auf den Inseln Gran Canaria und Teneriffa ist eine Mindestinvestition von 100.000 Euro und die Schaffung von fünf Arbeitsplätzen Voraussetzung, auf den anderen Inseln mit geringerer Wirtschaft sind es 50.000 Euro und drei Neueinstellungen. Eine Hauptaufgabe der ZEC ist es, ausländisches Kapital auf die Kanaren zu holen. Bereits über 78 % der Investitionen im Niedrigsteuergebiet stammen aus dem Ausland, davon über 13 % aus Deutschland, was damit nach Spanien zum größten Investor unter den zugelassenen ZEC-Unternehmen wurde.

Im Gesetz über die wirtschaftlichen und steuerlichen Regelungen der Kanaren vom 22. Juli 1972 wurde eine vom Festland abweichende Mehrwertsteuer eingeführt. Diese abweichende Steuer wurde auch nach dem Beitritt Spaniens zur EU beibehalten. Nach der Änderung im Jahr 2012 gibt es sieben verschiedene Steuersätze:


Darüber hinaus gelten auf den Kanaren andere Steuersätze u. a. bei der Mineralölsteuer, der Tabaksteuer, Brandweinsteuer usw. Die Erbschaftssteuer ist in allen Autonomen Gemeinschaften unterschiedlich geregelt.

Der Archipel ist rund zwei Flugstunden von der iberischen Halbinsel und etwa vier Flugstunden von Mitteleuropa entfernt. Es gibt Direktflüge in die wichtigsten Städte Spaniens, Europas und Lateinamerikas. Jede Insel besitzt inzwischen einen eigenen Flughafen:

Während die Flughäfen von La Gomera und El Hierro nur Flüge innerhalb der Kanarischen Inseln abwickeln, handelt es sich bei den anderen um internationale Flughäfen. Dabei zählen die Flughäfen Teneriffa Süd und Gran Canaria zu den verkehrsreichsten Spaniens. Untereinander sind die Inseln durch zahlreiche Fluglinien verbunden. Diese werden hauptsächlich von der in Telde auf Gran Canaria ansässigen kanarischen Fluggesellschaft Binter Canarias übernommen. Eine weitere, im Jahre 2001 gegründete, kanarische Fluggesellschaft war Islas Airways, die ihren Flugbetrieb 2012 eingestellt hat.

Zwischen den Inseln verkehren Fähren, unter anderem der Reedereien Fred. Olsen Express, "Compañía Trasmediterránea" und Naviera Armas.

Bereits von 1904 bis 1959 fuhr auf Teneriffa eine Straßenbahn. Seit März 2007 fährt als einzig schienengebundenes Verkehrsmittel der Kanaren wieder eine Straßenbahn, die Tranvia Tenerife zwischen der Hauptstadt Santa Cruz und der Universitätsstadt La Laguna. Zudem befindet sich eine Eisenbahnstrecke zwischen Santa Cruz und Adeje in Planung.

Darüber hinaus ist eine Eisenbahnstrecke auf Gran Canaria geplant. Sie soll die Inselhauptstadt Las Palmas mit der im Süden gelegenen Massentourismusregion Maspalomas verbinden.

Durch den jahrhundertelangen Kulturaustausch ist der Archipel geprägt durch eine Vermischung der Kulturen der Guanchen, Berbergruppen, europäischen Kolonialherren und der Gebräuche, die durch Handelsschifffahrt, hauptsächlich vom amerikanischen Kontinent, auf die Inseln gebracht wurden. Es gibt zahlreiche archäologische Fundstätten, deren Funde in ethnographischen und anthropologischen Museen zu sehen sind. Einzigartig ist die Pfeifsprache El Silbo, die die Ureinwohner der Kanaren entwickelten, und die heute wieder in Schulen auf La Gomera gelehrt wird.

Historische und künstlerische Monumente bringen durch ihre Architektur, Bildhauerei und Malerei die kanarische Identität zum Ausdruck. Hier sind insbesondere die Werke des Künstlers und Naturschützers César Manrique von der Insel Lanzarote zu erwähnen. Zu den Traditionen gehören Feste mit typischen Trachten und kanarischer Folklore in den einzelnen Dörfern, bei denen das typische Saiteninstrument der Kanaren, die Timple, wichtig ist. Religion und heidnische Riten der Ureinwohner vermischen sich. Dazu gehören der lateinamerikanisch inspirierte Karneval mit Samba-Rhythmen und vielen Farben genauso wie Hahnenkämpfe und der Ringkampf Lucha Canaria.
Küche

Die kanarische Küche ist stark an die spanische angelehnt mit verschiedenen Einflüssen. Als Grundlage der häufig einfachen Gerichte dienen Kartoffeln und Hülsenfrüchte.

Es gibt eine eigene öffentlich-rechtliche Radio- und Fernsehanstalt, TV Canaria, mit Sitz in Las Palmas de Gran Canaria, außerdem den Radiosender "Radio ECCA" und die Tageszeitungen "Canarias7", "Diario de Avisos", "El Día", "La Opinión de Tenerife" und "La Provincia" (in alphabetischer Reihenfolge).

Die Kanaren sind ein wichtiger Kommunikationsknoten zwischen Europa, Afrika und Amerika – dort findet sich die höchste Dichte an Überseekabeln weltweit.

Die Kanaren verfügen über zwei staatliche Universitäten, die Universität La Laguna und die Universität Las Palmas de Gran Canaria, denen verschiedene Fernstudienzentren angeschlossen sind. Die Universität Las Palmas de Gran Canaria wurde 1989 aus Teilen der von 1979 bis 1989 existierenden "Universidad Politécnica de Canarias" und einigen Standorten der Universität La Laguna gebildet; sie ist überwiegend auf Technik, Wirtschaft und Verwaltung ausgerichtet, während die naturwissenschaftlichen Fächer in La Laguna konzentriert sind.

Es gibt auch zwei private Universitäten: die betriebswirtschaftlich orientierte Universität Europea de Canarias (Teneriffa) und die Universität Fernando Pessoa-Canarias (Gran Canaria) u. a. für Medien- und Gesundheitsberufe.


Allgemeines

Geschichte
Flora und Fauna
Geologie
Kultur



</doc>
<doc id="10108" url="https://de.wikipedia.org/wiki?curid=10108" title="Europa">
Europa

Europa (, "") ist ein Erdteil, der sich über das westliche Fünftel der eurasischen Landmasse erstreckt. Obwohl es geographisch gesehen ein Subkontinent ist, der mit Asien zusammen den Kontinent Eurasien bildet, wird es historisch und kulturell begründet meist als eigenständiger Kontinent betrachtet. Dies verweist darauf, dass sich der Begriff „Europa“ nicht in der geographischen Definition erschöpft, sondern sich auch auf historische, kulturelle, politische, wirtschaftliche, rechtliche, ideelle Aspekte bezieht.

Die Einwohner Europas werden als Europäer bezeichnet. Mit über 700 Millionen Einwohnern, die auf einer Fläche von etwa zehn Millionen Quadratkilometern leben, gehört Europa zu den dichter besiedelten Teilen der Erde. Europa ist stark urbanisiert, besonders im Bereich der Hauptstädte und der Megalopolis „Blaue Banane“. Bedeutende politische Bündnisse Europas sind vor allem der Europarat und die Europäische Union.

Der Name „Europa“ lässt sich in Europa selbst am weitesten in Form der griechischen zurückverfolgen: Hier wurde ' meist als Kompositum aus altgriechisch , ', „weit“ und , ', „Sicht“, „Gesicht“ aufgefasst, daher ', „die [Frau] mit der weiten Sicht“.

Nach der griechischen Mythologie war dies der Name einer phönizischen Königstochter, altgriechisch "Εὐρώπη", die Zeus in Stiergestalt schwimmend nach Kreta entführte und dort verführte. Dieser Name stammt nach Auffassung einiger Etymologen aus einer semitischen Sprache und wurde dann gräzisiert, wohl aus phönizisch "" („dunkel“, „Abend“), das die Richtung des Sonnenuntergangs, den „Westen“, bezeichnet (vgl. Abendland).

Im 5. Jahrhundert vor Christus bezog der griechische Schriftsteller und Geograph Herodot den Begriff „Europa“, der sich als geographischer Terminus seinerzeit nur auf den Peloponnes bezogen hatte, auf die Landmassen nördlich des Mittelmeers sowie des Schwarzen Meers und unterschied sie so von den Landmassen Asiens („Asia“) und Afrikas („Libya“).

Europa selbst ist Namensgeber für das 1890 entdeckte chemische Element Europium.

Europa hat im Osten gegenüber Asien keine eindeutige geographische oder geologische Grenze. Deshalb sind die ‚Grenzen Europas‘ eine Frage gesellschaftlicher Übereinkunft. Eine geographische Definition Europas ist immer willkürlich. Nach einer bekannten Formulierung von Bernard-Henri Lévy ist Europa „kein Ort, sondern eine Idee“. Heute folgt man bei der Grenzziehung zwischen Europa und Asien üblicherweise weitgehend der Definition von Philip Johan von Strahlenberg. Danach bilden Uralgebirge und -fluss die Ostgrenze Europas. Zwischen dem Kaspischen Meer und dem Schwarzen Meer verläuft die Grenzlinie durch die Manytschniederung nördlich des Kaukasusgebirges, da an ihrer Stelle einst eine Meeresstraße das Kaspische Meer mit dem Schwarzen Meer verband. Die oben gezeigte Weltkugel zieht – davon leicht abweichend – die Grenze entlang des Kaukasus-Hauptkammes; diese Grenzziehung wird im englisch- und französischsprachigen Raum bevorzugt (→ Innereurasische Grenze).

Insgesamt hat Europa eine Fläche von etwa 10,5 Millionen Quadratkilometern und ist damit nach Australien der zweitkleinste Kontinent. Der nördlichste Punkt des europäischen Festlands ist Kinnarodden auf der Nordkinnhalbinsel in Norwegen, der südlichste ist die Punta de Tarifa in Spanien, der westlichste das Cabo da Roca in Portugal. Die Nord-Süd-Ausdehnung beträgt etwa 3800 km. In Ost-West-Richtung misst das europäische Festland etwa 6000 km, vom Uralgebirge in Russland bis zur Atlantikküste Portugals.

Der Kontinent liegt hauptsächlich in den gemäßigten Breiten. Das Klima ist in Europa im Vergleich mit Gebieten gleicher geographischer Breite in anderen Erdteilen wegen des Einflusses des relativ warmen Golfstroms milder. Die durchschnittliche Jahrestemperatur von Bordeaux beträgt beispielsweise 12,8 °C, während das auf nahezu gleicher Breite liegende kanadische Halifax (Nova Scotia) nur 6,3 °C erreicht.

In großen Teilen Westeuropas stehen milden Wintern kühle Sommer gegenüber. In Gebieten, die weit vom Meer, also auch von dessen klimatisch ausgleichendem Einfluss entfernt liegen, sind die Temperaturunterschiede in verschiedenen Jahreszeiten stärker. In den größten Teilen Osteuropas ist das Klima durch das Fehlen des Meeres in unmittelbarer Nähe vorwiegend kontinental geprägt. Kalte Winter und heiße Sommer sind in diesen Regionen vorherrschend.

Durch die Erdrotation und die damit verbundene ablenkende Kraft kommen Winde hauptsächlich aus westlicher Richtung. Da in den Küstengebieten Westeuropas deswegen Winde vornehmlich vom Atlantischen Ozean wehen, kommt es fast ganzjährig zu Regen. In Ost- und Mitteleuropa ist die Niederschlagsmenge dagegen vergleichsweise gering, da wiederum der Einfluss des Meeres auf diese Regionen zu gering ausfällt. Gebirge beeinflussen die Niederschlagsmenge ebenfalls stark. So zählen Teile der nordwestlichen Alpen zu den niederschlagsreichsten Gebieten Europas, während zentrale Alpentäler vergleichsweise trocken sind. Die Alpen wirken in diesem Fall als Barriere für Regenfronten.

In Gebieten nahe dem Mittelmeer herrscht überwiegend mediterranes Klima, das heißt trockene und heiße Sommer, während es im Winter mild und feucht ist. Im Sommer gibt es vorwiegend Winde aus nördlicher Richtung, im Winter bringen westliche Winde aus Richtung des Atlantischen Ozeans dagegen häufigen Niederschlag mit. Im Norden Europas herrscht arktisches Klima, im Südosten an der Wolga dagegen Kontinentalklima mit heißen Sommern und geringem Niederschlag. So kann es z. B. auch sein, dass in Lappland noch Schnee liegt, während in Sizilien über Sommerhitze geklagt wird, es auf den Shetland­inseln andauernd regnet und Andalusien von jahrelanger Trockenheit geplagt wird.

Die Vegetation in Europa lässt sich anhand des Klimas grob in vier Zonen einteilen: arktisch, boreal, gemäßigt und mediterran. Durch jahrtausendelange Nutzung ist die ursprüngliche Vegetation Europas in der gemäßigten und mediterranen Zone bis auf winzige Reste vernichtet. Im Norden Europas sind in der arktischen Tundra nur Sträucher, Moose und Flechten zu finden. Weiter südlich schließen sich in Skandinavien und dem nördlichen Russland boreale Nadelwälder an (vorwiegend aus Fichten und Kiefern), in die ausgedehnte Moore eingebettet sind. In der gemäßigten Zone würden zwischen dem Atlantischen Ozean und den Karpaten unter natürlichen Bedingungen vor allem durch Rotbuchen geprägte Laubwälder wachsen, in den wärmsten Tieflagen auch solche mit sommergrünen Eichen und Hainbuchen. Östlich vom Verbreitungsgebiet der Rotbuche, etwa ab der Weichsel und dem Dnjestr bis zum südlichen Ural, wären Mischwälder ausgebildet, in denen Stieleiche, Winter-Linde, Waldkiefer und – im Übergang zur nördlich anschließenden Nadelwaldzone – die Fichte die wichtigste Rolle spielen. Nach Süden schließt an diese Mischwaldzone nördlich vom Schwarzen Meer eine Region an, in der unter natürlichen Bedingungen Steppen­vegetation ausgebildet wäre.

Auch der Mittelmeerraum war ursprünglich weitgehend bewaldet. Dort wäre die Vegetation ohne menschliche Einwirkung von mediterranem Hartlaubwald geprägt, der von immergrünen Eichenarten (v. a. Steineiche) beherrscht wird, in den südlichsten und wärmsten Lagen auch mit starker Beteiligung von Olivenbäumen, Pistazien und Kiefern. Durch den anthropogenen Einfluss aber wachsen heutzutage vor allem Macchie- oder Garigue-Gewächse. In den kühleren Lagen des Mittelmeergebiets spielen auch verschiedene sommergrüne Eichenarten, etwa die Flaumeiche, eine große Rolle. In den mittleren und höheren Lagen der Gebirge ist die ursprüngliche Vegetation besser erhalten als in den Tieflagen. Hier sind auf relativ kleinem Raum mehrere einander überlagernde Höhenstufen ausgebildet, wobei in Abhängigkeit vom Klima die einzelnen Komponenten einer solchen Höhenstufenzonierung sehr unterschiedlich sein können. In den Bergwäldern der mittleren oberen Höhenlagen sind neben der Rotbuche vor allem Tannen- und Kiefernarten, in den nördlicheren Gebirgen auch Fichte und Lärche von Bedeutung. Oberhalb der Waldgrenze sind in den Gebirgen Zwergstrauchheiden, alpine Rasen, und im Mittelmeerraum auch Dornpolstervegetation zu finden. Nach oben zu wird die geschlossene Vegetation von einzeln wachsenden Fels- und Schuttpflanzen abgelöst.Heute ist der größte Teil Europas durch agrarische Nutzung geprägt, die verbliebenen Wälder werden intensiv forstlich genutzt. Vom Menschen weniger beeinträchtigte Bereiche gibt es vor allem in den höheren Lagen der Gebirge.

Große Raubtiere wie Bär, Wolf oder Luchs sind nur noch im Norden oder Osten in größeren Beständen zu finden. Im äußersten Norden und Osten Europas kommen Rentiere und Elche noch relativ häufig vor. Nach seiner Ausrottung wurde der Wisent aus Zoobeständen wieder in Osteuropa ausgewildert. Rothirsch, Reh und Wildschwein findet man in den meisten Regionen Europas. Eine typisch europäische Großtierart ist die Gämse, die außerhalb der europäischen Gebirge nur in Kleinasien und den asiatischen Teilen des Kaukasus verbreitet ist. In Alpen, Pyrenäen und im Kaukasus leben verschiedene Arten des Steinbocks. Die Saigaantilope war noch im Mittelalter bis in die Ukraine verbreitet, heute ist sie in Europa auf Gebiete am Unterlauf der Wolga beschränkt. Viele Arten sind schon vor Jahrhunderten ausgerottet worden (Ur, Tarpan). Auch der Bestand an Greifvögeln, wie Steinadler, Seeadler und Gänsegeier ist in den letzten Jahrhunderten stark dezimiert worden. Nur in Südeuropa gibt es aufgrund des warmen Klimas eine größere Artenzahl an Eidechsen, Schlangen und Schildkröten. An der Küste leben Seehunde und andere Robbenarten. In den nördlichen Meeren sind die bekanntesten Wale Schwertwal und Schweinswal, im Mittelmeer verschiedene Delfinarten. Die bekanntesten Salzwasserfische sind Heringe und verschiedene Thunfischarten.

Die Städte und Gemeinden bilden die kleinsten administrativen Einheiten im Verwaltungsaufbau der Staaten. Lissabon war eine der ersten kosmopolitischen Metropolen der Neuzeit, da sie im Zeitalter des Imperialismus Zentrum eines großen Kolonialreiches war. Noch 1950 befanden sich vier Städte in der heutigen Europäischen Union unter den 20 größten der Welt, seitdem wachsen die europäischen Städte kaum noch, verlieren teilweise an Einwohnerzahlen. Diese vier größten heutigen EU-Städte waren London auf dem zweiten Platz hinter New York, Paris auf dem vierten Platz hinter Tokio sowie Mailand und Neapel, die Platz 13 und 19 belegten.

Doch in der zweiten Hälfte des 20. Jahrhunderts holten die Städte der restlichen Welt, besonders die der Entwicklungsländer, z. B. Mexiko-Stadt, Manila oder São Paulo, bei den Einwohnerzahlen auf und führen jetzt die Tabelle der Welt an. Trotz Stagnation in älteren Städten gibt es zahlreiche andere Prozesse in der Raumentwicklung des 21. Jahrhunderts, wie z. B. die Gentrifizierung der Innenstädte.

Problematisch beim Vergleich der Metropolen ist, dass es auch oft polyzentrische Ballungsräume gibt, in denen die Kernstädte im Vergleich zum Ballungsraum sehr klein sind (z. B. Rhein-Ruhr, das oberschlesische Industriegebiet oder Mittelengland). Daher tauchen sie, auch wenn sie teilweise sehr groß sind, nicht in der Tabelle unten auf.

Städte mit mehr als 1,5 Mio. Einwohnern in Europa sind:

Die Staaten Vereinigtes Königreich, Irland, Portugal und Island haben sich der westeuropäischen Zeit (WEZ/WET) zugeordnet, die identisch mit der koordinierten Weltzeit (UTC) ist. Sie entspricht der mittleren Sonnenzeit am Nullmeridian, der durch London und Ostengland verläuft. 

Alle mittel- und einige west- und osteuropäischen Staaten nutzen die Mitteleuropäische Zeit (UTC + eine Stunde), dies sind Spanien, Frankreich, Andorra, Monaco, Die Niederlande, Belgien, Luxemburg, Dänemark, Schweden, Norwegen, Deutschland, Polen, Österreich, die Schweiz, Liechtenstein, Italien, San Marino, der Vatikanstaat, Polen, Tschechien, die Slowakei, Ungarn, Slowenien, Kroatien, Bosnien-Herzegowina, Serbien, Montenegro, Mazedonien, der Kosovo, Malta und Albanien.

Die osteuropäische Zeit (UTC + 2 Stunden) nutzen Finnland, Estland, Lettland, Litauen, die Ukraine, Rumänien, Moldau, Bulgarien und Griechenland.

Die Moskauer Zeit (UST +3 Std.) nutzen Weißrussland und ein Großteil des europäischen Teils Russlands und die Türkei.

Die meisten Staaten verwenden von Ende März bis Ende Oktober eine Sommerzeit, bei der die Uhren über diesen Zeitraum hinweg eine Stunde vorgestellt werden. Dies gilt für alle EU-Staaten, auch die meisten europäischen nicht-EU-Staaten folgen diesem Beispiel, jedoch nicht Island, Russland, Weißrussland und die Türkei.

Älteste Nachweise von Vertretern der Gattung "Homo" stammen derzeit aus der Sierra de Atapuerca in Spanien und sind bis zu 1,2 Mio. Jahre alt. Noch ältere Fossilfunde aus Georgien (außerhalb der heute gültigen Grenzen Europas) sind 1,8 Mio. Jahre alt und werden als „Homo erectus ergaster georgicus“ bezeichnet. Im nordalpinen Europa beginnt der älteste Besiedlungshorizont mit "Homo heidelbergensis" vor ca. 600.000 Jahren.

Erst vor ca. 40.000 Jahren gelangte "Homo sapiens" nach Europa (vgl. Ausbreitung des Menschen) und ersetzte nach und nach den Neandertaler. Mit der Jungsteinzeit und der Bronzezeit begann in Europa eine lange Geschichte großer kultureller und wirtschaftlicher Errungenschaften, zunächst im Mittelmeerraum, dann auch im Norden und Osten.

Besonders die griechische Kultur, das Römische Reich und das Christentum hinterließen bis heute ihre Spuren. In der Antike vereinigte das Römische Reich zur Zeit des Augustus erstmals das gesamte südliche Europa zusammen mit den anderen Küstenländern des Mittelmeerraums in einem Großreich. Im Römischen Reich konnte sich die neue Religion des Christentums schnell ausbreiten. Trotz aller Verfolgungen wurde das Christentum in der Spätantike von Konstantin dem Großen gefördert (konstantinische Wende) und unter Kaiser Theodosius I. zur Staatsreligion erhoben – eine Entwicklung, die bis heute fortwirkt. Mit dem Ende der Antike brach im Westen das Imperium Romanum zusammen, bestand im Osten jedoch als (immer mehr griechisch geprägtes) Byzantinisches Reich bis 1453 fort. In dieser Zeit drängten mit der Völkerwanderung eine Vielzahl von meist germanischen Stämmen (Angelsachsen, Franken, Goten u. a.) in das westliche Europa und bildeten den Grundstein für zukünftige Nationen (England, Frankreich, Spanien).

Im Frühmittelalter erklärte das Paderborner Epos den Herrscher des Frankenreichs, Karl den Großen, zum „Vater Europas“ ("pater Europæ"). Das Mittelalter war unter anderem geprägt von der Konkurrenz zwischen dem neuen römischen Kaiser im Westen (→ Römisch-deutscher Kaiser) und dem byzantinischen Kaiser in Konstantinopel (→ Zweikaiserproblem), auf deren beider Einflusssphären die später weiter vertiefte Spaltung in ein westliches und östliches Europa zurückgeht (→ Morgenländisches Schisma). Missionare verbreiteten das Christentum seit dem Frühmittelalter über Nord- und Osteuropa, so dass ganz Europa im Spätmittelalter christlich wurde. Im westlichen Europa gab es jedoch seit dem Investiturstreit im 11. Jahrhundert Auseinandersetzungen zwischen Kaiser und Papst um die Vorherrschaft. Im Zeitalter der Renaissance entdeckte man im lateinischen Westen das „Wissen der Antike“ wieder, was teils einen kulturellen Aufschwung bewirkte. Die Reformation im 16. Jahrhundert spaltete die westliche Kirche (mit der orthodoxen Kirche war es bereits im Jahr 1054 zum Bruch gekommen) in einen katholischen und evangelischen Teil. Religionskriege waren die Folge. 1618 bis 1648 verwüstete der Dreißigjährige Krieg weite Teile Mitteleuropas.

Seit dem 15. Jahrhundert bauten europäische Nationen (besonders Spanien, Portugal, Russland, die Niederlande, Frankreich und das Vereinigte Königreich) koloniale Imperien mit großen Besitztümern auf allen anderen Kontinenten auf. Europa ist der Kontinent, der die anderen Erdteile am meisten beeinflusst hat (zum Beispiel durch christliche Missionierung, Kolonien, Sklavenhandel, Warenaustausch und Kultur).

Im 18. Jahrhundert setzte die Bewegung der Aufklärung neue Akzente und forderte Toleranz, die Achtung der Menschenwürde, Gleichheit und Freiheit. In Frankreich kam 1789 durch die Französische Revolution das Bürgertum zur Macht. Im frühen 19. Jahrhundert musste sich halb Europa nach dem Willen des nach der Revolutionszeit zur Macht gekommenen französischen Kaisers Napoleon richten, bis er 1812 in Russland ein Fiasko erlebte. Die konservativen Siegermächte versuchten darauf im Rahmen des Wiener Kongresses die vorrevolutionären Zustände wiederherzustellen, was nur vorübergehend gelang. Die Industrialisierung begann in Teilen Europas im 18. Jahrhundert und veränderte rasant den Alltag breiter Bevölkerungsschichten. Als Folge der Verarmung der Arbeiter entstand im 19. Jahrhundert die kommunistische Bewegung. Daneben war das 19. Jahrhundert stark bestimmt von der Verbreitung demokratischer Ideen und Systeme, der konservativen Reaktion darauf und dem mit ungezügeltem Nationalismus einhergehenden Imperialismus der Großmächte. Sowohl der Erste Weltkrieg (1914 bis 1918) als auch der Zweite Weltkrieg (1939 bis 1945) brachen in Europa aus und richteten riesige Zerstörungen an. Rund 60 Millionen Menschen verloren in Letzterem ihr Leben; geschätzte sechs Millionen davon waren Juden, welche die Nationalsozialisten in ihrem Rassenwahn, wie auch Homosexuelle und „Zigeuner“, auszurotten versuchten.

Nach dem Zweiten Weltkrieg und im Kalten Krieg war Europa in zwei große politisch-ökonomische Blöcke geteilt: sozialistische Nationen in Osteuropa und kapitalistische Nationen in Westeuropa. Man sprach auch vom Eisernen Vorhang, der die Staaten Europas voneinander trennte. Dazwischen gab es einige neutrale Staaten. Erst Perestroika und Glasnost führten in der Sowjetunion Mitte der 1980er Jahre zu einem politischen Kurswechsel. 1989 brach der Ostblock auf, die Berliner Mauer fiel, die Sowjetunion und der Warschauer Pakt lösten sich auf.

Seit den letzten Jahrzehnten des 20. Jahrhunderts wachsen die Kulturen Europas verstärkt zusammen, was sich zum einen durch Institutionen wie die EU, aber auch durch die Bevölkerungs- und Wirtschaftsschwerpunkte zeigt.

Zieht man die Grenze Europas gemäß der Definition Strahlenbergs ("siehe: Abschnitt Geographie") gibt es zurzeit 46 souveräne Staaten (ohne den Kosovo), die ganz oder teilweise in Europa liegen. 



Daneben gibt es noch kleinere Territorien, die nicht integrierter Bestandteil eines Staates, aber auch nicht selbständig sind:


Diese drei Gebiete sind Kronbesitzungen der britischen Krone. Sie sind kein Bestandteil des Vereinigten Königreichs Großbritannien und Nordirland.

Gibraltar ist ein Britisches Überseegebiet. Als solches steht es zwar unter der Souveränität des Vereinigten Königreichs, ist aber ebenfalls kein Teil davon.


Die Färöer bilden zusammen mit Grönland und dem dänischen Kernland das Königreich Dänemark.


Diese zwei Gebiete sind Teil des Königreichs Norwegen und werden deshalb gewöhnlich zu Europa gezählt. Aufgrund ihrer Lage zwischen Skandinavien und Grönland ("Jan Mayen") bzw. zwischen Skandinavien und dem Nordpol ("Svalbard") ist die geographische Zuordnung zum Kontinent nicht eindeutig.


Die Inselgruppen sind zwei autonome Regionen Portugals. Ihre Zuordnung zu Europa folgt aufgrund politischer, kultureller und historischer Gleichartigkeit. Geographisch liegt etwa Madeira näher der afrikanischen als der europäischen Küste.



In der Mitte des 20. Jahrhunderts wurde als erste gemeinsame europäische Institution der Europarat gegründet, der nach Ende des Ost-West-Konfliktes 47 Staaten umfasst. Ebenfalls von großer Bedeutung ist die Organisation für Sicherheit und Zusammenarbeit in Europa (OSZE), die 1975 als Konferenz für Sicherheit und Zusammenarbeit in Europa (KSZE) damals blockübergreifend gegründet wurde, und der neben europäischen Ländern auch die USA und Kanada angehören.

1951 schlossen sich Belgien, die Niederlande, Luxemburg, Deutschland, Italien und Frankreich zur Montanunion bzw. EGKS (Europäische Gemeinschaft für Kohle und Stahl) zusammen. Der Versuch, eine Europäische Verteidigungsgemeinschaft (EVG) sowie eine Europäische Politische Gemeinschaft (EPG) zu gründen, scheiterte 1954 an der französischen Nationalversammlung. Daraufhin wurden 1957 mit den Römischen Verträgen die Europäische Atomgemeinschaft (Euratom) sowie die Europäische Wirtschaftsgemeinschaft (EWG) gegründet. Ab dem EG-Fusionsvertrag 1967 teilten sich die drei Europäische Gemeinschaften (EGKS, Euratom und EWG) die gemeinsamen Institutionen Kommission, Rat, Parlament und Gerichtshof. Mit dem Vertrag von Maastricht 1993 wurde die EWG in Europäische Gemeinschaft (EG) umbenannt und außerdem die Europäische Union gegründet, die die drei Gemeinschaften umfasste sowie um zwei intergouvernementale Politikbereiche die Gemeinsame Außen- und Sicherheitspolitik und die Zusammenarbeit im Bereich Justiz und Inneres erweiterte. 2002 wurde die EGKS aufgelöst und ihre Funktionen von der EG übernommen. Nach der Ost-Erweiterung in den Jahren 2004, 2007 und 2013 zählt die EU gegenwärtig 28 Mitgliedsstaaten. Durch den Vertrag von Lissabon ging die EG zum 1. Dezember 2009 vollständig in der EU auf. Heute sind 27 europäische Länder Mitglied der Europäischen Union. Der 28. Mitgliedstaat der EU, die Republik Zypern, zählt geographisch zu Asien.

Ein Großteil der Mitgliedstaaten der ebenfalls in den 1950er Jahren gegründeten Europäischen Freihandelsassoziation (EFTA) ist mittlerweile in die EU übergetreten, nur noch Island, Norwegen, Schweiz und Liechtenstein sind Mitglieder. Die EFTA ist anders als die EU eine reine Freihandelszone ohne supranationale Entscheidungsbefugnisse.

In militärischer Hinsicht ist die NATO (Nordatlantikvertrag-Organisation) in Europa von großer Bedeutung. Sie wurde 1949 aufgrund der sich abzeichnenden Differenzen nach dem Ende des Zweiten Weltkriegs zwischen den Westalliierten und der UdSSR gegründet. Neben den 23 europäischen Mitgliedern sind auch die USA, Kanada und die Türkei Mitglieder der NATO.

Einige Staaten gehören geographisch nicht zu Europa, sind aber Mitglieder europäischer Organisationen


Mit über 700 Millionen Einwohnern gehört Europa zu den dichter besiedelten Teilen der Erde. Die durchschnittliche Bevölkerungsdichte beträgt etwa 65 Einwohner pro km². Vor allem in West-, Mittel- und Südeuropa ist die Bevölkerungsdichte relativ hoch, während sie weiter nach Nord- und Osteuropa stark abnimmt. Die zentral gelegene Bevölkerungskonzentration in West-, Mittel- und Südeuropa, die sich bandförmig zwischen der Irischen See und dem Mittelmeer erstreckt, wird unter der Bezeichnung „Blaue Banane“ als wirtschafts- und siedlungsgeographisch bedeutende Megalopolis klassifiziert.

Mehr als 90 Prozent der Einwohner Europas sprechen indogermanische Sprachen. Am weitesten verbreitet sind slawische, germanische und romanische Sprachen. Auch Griechisch, Albanisch, die baltischen und keltischen Sprachen sowie das Romani zählen zu den indogermanischen Sprachen.

Die uralischen Sprachen stellen die zweitgrößte Sprachfamilie Europas dar. Sie unterteilen sich weiter in die samojedischen Sprachen, die von einigen wenigen Tausend Menschen im äußersten Nordosten Europas gesprochen werden, und in die finno-ugrischen Sprachen. Hierzu zählen vor allem Finnisch, Ungarisch, Estnisch als Amtssprachen, ferner die in Lappland gesprochenen samischen Sprachen und einige Minderheitensprachen, vor allem in Russland.

Im europäischen Teil der Türkei ist mit Türkisch eine Turksprache Amts- und Titularsprache, ebenso Kasachisch im europäischen Teil Kasachstans. Andere Turksprachen kommen als Minderheitensprachen in Ost- und Südosteuropa vor, so etwa das Gagausische und das Tatarische. Mit Kalmückisch wird am Ostrand des Kontinents auch ein Vertreter der mongolischen Sprachfamilie in Europa gesprochen.

Mit Maltesisch ist auf der Insel Malta zudem eine Sprache des semitischen Zweigs der afroasiatischen Sprachen vertreten. Die in Spanien und Frankreich gesprochene baskische Sprache wird keiner größeren Sprachfamilie zugerechnet, ihr Ursprung konnte durch die moderne Sprachwissenschaft nicht rekonstruiert werden und ist nach wie vor unbekannt. Darüber hinaus werden heute in Europa zahlreiche weitere Sprachen aus anderen Sprachfamilien gesprochen, die durch Einwanderer in jüngerer Zeit hierher gelangt sind.

Betrachtet man den Kaukasus-Hauptkamm als Südostgrenze Europas, zählen auch zahlreiche Kaukasische Sprachen, darunter Adygeisch, Kabardinisch, Abasinisch, sowie die verschiedenen Nachisch-Dagestanischen Sprachen zu den Sprachen, die geschlossene Sprachgebiete in Europa haben. Prinzipiell südlich des Hauptkammes angesiedelt sind Georgisch und Swanisch, die jedoch den Hauptkamm nach Norden stellenweise geringfügig überschreiten. Turksprachen, die in Europa nur an der Nordflanke des Kaukasus gesprochen werden, sind Aserbaidschanisch, Karatschai-Balkarisch, Kumykisch und Nogaisch. Eine Iranische Sprachinsel im zentralen Nordkaukasus bildet weiterhin das Ossetische, im Ostkaukasus bis ins 19. Jahrhundert regional auch Tatisch, deren Sprecheranteil aber seitdem sehr zurückgeht.

Bezogen auf die Zahl der Bewohner ist in Europa das lateinische Schriftsystem am gebräuchlichsten, gefolgt von dem kyrillischen Alphabet (in Russland, Weißrussland, der Ukraine, Bulgarien, Serbien, Montenegro, Mazedonien und Teilen Bosniens und der Herzegowina) und dem griechischen Alphabet. Die lateinische Sprache hat durch die römisch-katholische Kirche bis in die Neuzeit als Schriftsprache des Kontinents überleben können.

Das Christentum und der Islam sind die am weitesten in Europa verbreiteten Religionen.

Etwa 75 % der Europäer sind Christen (vor allem katholisch, protestantisch, orthodox).

Zwischen 42 und 53 Millionen, also 6–8 % sind Muslime, wobei die meisten Muslime in den europäischen Teilen Russlands (13–20 Millionen) leben. Ca. 16 Millionen sind muslimische Einwanderer und deren Nachkommen in der Europäischen Union. 9,5 Millionen leben im europäischen Gebiet der Türkei, 2,2 Millionen in Bosnien und Herzegowina und 1,4–2,5 Millionen in Albanien.

Knapp 2 Millionen (ca. 0,3 %) der europäischen Bevölkerung sind Juden, die meisten davon in Frankreich (ca. 520.000), dem Vereinigten Königreich (ca. 270.000), Russland (ca. 260.000) und Deutschland (ca. 200.000). Andere Religionen (Hinduismus, Buddhismus etc.) sind ebenfalls mit weniger als 0,3 % vertreten.

Etwa 17 % der Europäer sind konfessionslos, besonders in Estland, Tschechien, den Niederlanden, Russland und Ostdeutschland, sonst v. a. in den Städten.

Die alleinige Konfessionszugehörigkeit sagt jedoch wenig über den tatsächlichen Grad der Religiosität in einem Land aus. Nach der European Values Study bezeichneten sich ca. ein Drittel der Europäer als unreligiös, 5 % als überzeugte Atheisten.


Das Christentum erreichte Europa erstmals im 1. Jahrhundert nach Chr. Der Islam breitete sich im 8. Jahrhundert auf der Iberischen Halbinsel aus, wurde aber im Zuge der „Reconquista“ vom 13. bis zum 15. Jahrhundert wieder verdrängt. Europäer verbreiteten das Christentum durch Einwanderung und Mission in Amerika, Australien und in geringerem Maße auch auf anderen Kontinenten (Teile Südostasiens, Afrikas und Ozeaniens). Heute ist Europa weitgehend säkularisiert.

Die römisch-katholische Kirche hat seit 1964 sechs Heilige zu Patronen Europas ernannt.

Im 19. Jahrhundert war Europa durch die in England beginnende und auf den ganzen Kontinent übergreifende Industrielle Revolution konkurrenzlos die führende Wirtschaftskraft. Später brachten verschiedene internationale Einrichtungen und Organisationen, wie die EFTA (Europäische Freihandelszone) und die Europäische Gemeinschaft – der heutigen EU – einen Wachstumsschub, der in vielen Teilen Europas bis in die 1970er und teilweise in die 1980er anhielt. Die Versorgung der Bevölkerung Europas konnte durch die Verringerung von Handelsbeschränkungen ebenfalls weiter ausgebaut werden. Momentan erlebt Osteuropa einen Wirtschaftsboom und schließt zu den westeuropäischen Staaten auf. Besonders hoch ist das Wachstum zurzeit in den EU-Staaten Lettland, Estland, Litauen und der Slowakei, mit zum Teil zweistelligen Wachstumsraten.

Mit dem Vertrag von Maastricht bildeten zunächst 12 Staaten der EU ab 1999 eine Währungsunion. Ab dem 1. Januar 2002 wurden Euro-Geldscheine und -münzen eingeführt. Heute ist der Euro gesetzliches Zahlungsmittel in 18 EU-Staaten und sechs weiteren europäischen Staaten.

Heute ist Europa ein wohlhabender Kontinent mit großen Industriemetropolen, ertragreicher Landwirtschaft und einem wachsenden Dienstleistungssektor. Dennoch ist seit den 1970er Jahren Arbeitslosigkeit ein weit verbreitetes Problem in vielen europäischen Staaten. Die Industrie und die Dienstleistung konzentrieren sich vor allem auf die Ballungsgebiete. In den meisten Staaten Europas ist das Problem nicht mehr der Mangel an Nahrungsmitteln, sondern die Überproduktion und die Fettleibigkeit. Ausfuhrgüter sind vor allem Maschinen, Stahl, Computer­bedarf und Autos. Einfuhrgüter sind unter anderem Kakao, Tee, Kautschuk, Erdöl, Erdgas und Erze.

Bildhauerei, Malerei, Literatur, Architektur und Musik haben in Europa eine lange Tradition. Viele Städte, wie beispielsweise Paris, Wien, Rom, Berlin und Moskau werden heute als kulturelle Zentren betrachtet. Außerdem besitzen viele Städte wichtige Theater, Museen, Orchester und weitere bedeutende Einrichtungen.

In allen Staaten Europas gibt es eine Schulpflicht oder mindestens eine Bildungspflicht, die oft vom 6. oder 7. bis zum 15. oder 16. Lebensjahr dauert. Die Grundschulzeit beträgt in den meisten Ländern vier bis fünf – in wenigen Staaten auch sieben oder acht Jahre. Danach folgt meist eine weiterführende Schule, die auch verschiedene Schultypen und -arten umfassen kann. Die Analphabetenrate ist in fast allen Ländern durch verschiedene Förderungen des Bildungssystems sehr niedrig. In vielen Ländern gibt es außerdem renommierte Hochschulen und Universitäten, teilweise auch mit bestimmten fachlichen Schwerpunkten.

Der Sport hat für die Vorstellung eines einheitlichen europäischen Raumes eine besondere Rolle gespielt, da es seit Ende des 19. Jahrhunderts "Europameisterschaften" (zuerst 1891 im Eiskunstlauf) gibt. Da die Europäische Union auf eine eigene Gestaltung des Sports weitgehend verzichtet hat, in den Fußball-Ligen nationale Monopole gepflegt werden, die sich auch vom europäischen Ausland strikt abgrenzen, ist das Bild des Europas des Berufssports in der öffentlichen Wahrnehmung viel präsenter als das des politischen Europas.





</doc>
<doc id="10110" url="https://de.wikipedia.org/wiki?curid=10110" title="Einkommensteuer (Deutschland)">
Einkommensteuer (Deutschland)

Die Einkommensteuer in Deutschland (Abkürzung: ESt) ist eine Gemeinschaftssteuer, die auf das Einkommen natürlicher Personen erhoben wird.

Rechtsgrundlage für die Berechnung und Erhebung der Einkommensteuer ist – neben weiteren Gesetzen – das Einkommensteuergesetz (EStG). Der Einkommensteuertarif regelt die Berechnungsvorschriften. Bemessungsgrundlage ist das zu versteuernde Einkommen.

Im Jahr 2016 nahm der deutsche Staat über 239 Milliarden Euro Einkommensteuer ein. Das entspricht knapp einem Drittel der gesamten Steuereinnahmen Deutschlands.

Der ebenfalls vorkommende Ausdruck "Einkommenssteuer" mit Fugen-s wird in der offiziellen Rechtssprache nicht verwendet.

Erhebungsformen der Einkommensteuer sind die Lohnsteuer, die Kapitalertragsteuer, die Bauabzugsteuer und die Aufsichtsratsteuer. Sie werden auch als Quellensteuern bezeichnet, da sie direkt an der Quelle abgezogen werden. Die Abgeltungsteuer dient seit 2009 als bestimmte Anwendung der Kapitalertragsteuer. Nach dem Welteinkommensprinzip sind die in Deutschland Steuerpflichtigen mit ihrem weltweiten Einkommen steuerpflichtig.

Personengesellschaften (zum Beispiel die OHG, KG oder GbR) sind nicht Besteuerungssubjekt der Einkommensteuer, jedoch unterliegen die Gesellschafter einer Personengesellschaft mit ihrem Gewinnanteil der Einkommensteuer ( Abs. 1 Satz 1 Nr. 2 EStG). Kapitalgesellschaften unterliegen nicht der Einkommensteuer, sondern der Körperschaftsteuer.

Die Einkommensteuer ist eine direkte Steuer, weil Steuerschuldner und Steuerträger identisch sind. Sie ist eine Gemeinschaftssteuer, da sie Bund, Ländern und Gemeinden zusteht. Zudem ist sie eine Personensteuer, eine Subjektsteuer (anders als die Gewerbesteuer, die das Objekt Gewerbebetrieb besteuert) und eine Ertragsteuer.

Die im Mittelalter erhobenen kirchlichen Personalzehnten "(decimae personales)" waren erste Ansätze zur Personalbesteuerung. Im 17. Jahrhundert wurde der preußische Kopfschoß eingeführt. Die erste Einkommensteuer moderner Art wurde auf deutschem Gebiet 1811 bis 1813 in Ostpreußen erhoben. Sie war schon 1808 von Freiherr vom Stein in Anlehnung an die englische "income tax" von 1799 ursprünglich als Kriegsabgabe empfohlen worden.

1820 führte Preußen unter Karl August Fürst von Hardenberg eine Klassensteuer ein. Die Steuerstaffelung orientierte sich dabei nach der Gruppierung der Stände. Diese Steuer wurde 1851 für die höheren Einkommen von einer klassifizierten Einkommensteuer abgelöst und 1891 unter Finanzminister Johannes von Miquel durch eine Einheits-Einkommensteuer mit Erklärungspflicht und Progression ersetzt, wobei der Steuersatz der Einkommensteuer von rund 0,6 % (für Jahreseinkommen von 900 bis 1050 Mark) bis auf 4 % (für Jahreseinkommen über 100.000 Mark) stieg.

Diesem Vorbild folgten bis zum Ersten Weltkrieg alle deutschen Bundesstaaten, nachdem Bremen bereits im Jahr 1848, Hessen 1869 und Sachsen 1874 zu einer allgemeinen Einkommensteuer übergegangen waren. Im Zuge der Finanzreform von Matthias Erzberger zu Beginn der Weimarer Republik entstand durch Gesetz vom 29. März 1920 eine einheitliche Reichseinkommensteuer, die bei den Steuerreformen von 1925 und 1934 fortentwickelt wurde. 1925 wurde unter anderem die Lohnsteuerkarte eingeführt, 1934 die Lohnsteuerklassen I bis IV.

Im Kontrollratsgesetz Nr. 3 vom 20. Oktober 1945 sowie dem Kontrollratsgesetz Nr. 12 vom 11. Februar 1946 wurden durch die alliierten Besatzungsmächte starke Anhebungen der Steuertarife verfügt, bis hin zu einem Spitzensteuersatz von 95 %. Durch die folgende Währungsreform und mehrere Steuergesetze, die Tarifsenkungen bzw. Steuervergünstigungen vorsahen, wurden diese Anhebungen der Tarife jedoch teilweise wieder abgemildert bzw. durch Anhebung von Verbrauchssteuersätzen kompensiert. 

Das Grundgesetz von 1949 legte fest, dass die Erträge aus der Einkommensteuer grundsätzlich den Ländern zustehen, der Bund jedoch hieran partizipieren kann. Erst durch das Verfassungsänderungsgesetz von 1955 wurde die Einkommensteuer zur Gemeinschaftssteuer von Bund und Ländern nach Abs. 3 GG erklärt.

Eine bedeutende Änderung hat die Einkommensteuer in Deutschland durch das Einkommensteuerreformgesetz vom 5. August 1974 erlebt, mit dem der Sonderausgabenabzug grundlegend neu geregelt wurde. Von großer Bedeutung war auch die Einführung eines linear-progressiven Tarifs im Jahre 1990, die den seit den 1960er Jahren vorhandenen "Mittelstandsbauch" abschaffte. Weitere Meilensteine waren die Anhebung des Grundfreibetrages auf das Existenzminimum und die Einführung der Günstigerprüfung zwischen Kinderfreibetrag und Kindergeld im Jahre 1996.

In neuerer Zeit ist ein deutlicher Trend zur Verminderung der Einkommensteuer und damit einhergehender Erhöhung der Verbrauchssteuern erkennbar.

Folgende Tabelle zeigt die historische Entwicklung der Eckwerte sowie der Eingangs- und Spitzensteuersätze im deutschen Einkommensteuertarif. Diese Angaben sind jedoch alleine noch nicht ausreichend für ein ganzheitliches Verständnis der Steuerbelastung der verschiedenen Einkommensgruppen. Dazu muss neben dem Grundfreibetrag zusätzlich der Tarifverlauf der Grenz- und Durchschnittssteuersätze betrachtet werden, wie das in den obigen Grafiken für ausgewählte Jahre dargestellt ist. 

Der steilste Anstieg des Grenzsteuersatzes findet seit dem Jahr 2004 im unteren Einkommensbereich statt. Daher wird vielfach beklagt, dass kleine und mittlere Einkommensgruppen von der kalten Progression besonders betroffen seien. Die Progressionsbreite, also das Verhältnis zwischen dem obersten Einkommenseckwert zum Grundfreibetrag, ist in den letzten Jahren ständig gesunken. So hat der Progressionsbereich des Grenzsteuersatzes heute nur noch weniger als ein Zehntel seiner Breite von 1958. Allerdings wurde der Grundfreibetrag 1996 mehr als verdoppelt und der oberste Grenzsteuersatz danach schrittweise gesenkt. Im Jahr 2007 wurde ein zusätzlicher gestufter Spitzensteuersatz eingeführt (Reichensteuer), der jedoch die linearen Progressionszonen unverändert lässt. Die im Jahre 1990 eingeführte durchgehend lineare Progression ist inzwischen durch einen Tarif mit unterschiedlich steilen Zonen und Sprüngen ersetzt worden (siehe auch Grafiken oben).

Folgende Prinzipien prägen das Einkommensteuerrecht:

Rechtsgrundlagen sind das Einkommensteuergesetz (EStG) vom 16. Oktober 1934 und die am 21. Dezember 1955 in Kraft getretene Einkommensteuer-Durchführungsverordnung (EStDV). Aber auch andere Steuergesetze wie zum Beispiel das Außensteuergesetz (AStG) vom 8. September 1972 oder das Umwandlungssteuergesetz (UmwStG) vom 28. Oktober 1994 enthalten materiell-rechtliche Regelungen für die Einkommensbesteuerung. Um eine einheitliche Anwendung des Einkommensteuerrechts durch die Finanzverwaltung zu gewährleisten, wurden von der Bundesregierung am 16. Dezember 2005 Einkommensteuerrichtlinien erlassen (EStR), die allerdings nur die Finanzverwaltung, nicht jedoch die Finanzgerichte oder den Steuerpflichtigen binden. Ergänzend werden regelmäßig Erlasse vom Bundesministerium der Finanzen (BMF) veröffentlicht, in denen spezielle Fragen zur Rechtsanwendung geklärt werden.

Nach Abs. 3 und 5 GG stehen die Einnahmen aus der Einkommensteuer dem Bund, den Bundesländern und den Gemeinden gemeinsam zu. Welche Gebietskörperschaft Anspruch auf die vereinnahmten Steuern erheben kann, regelt das Zerlegungsgesetz. Die Gemeinden erhalten seit 1970 einen Anteil an der Einkommensteuer. Ursprünglich betrug er 14, aktuell beträgt er 15 Prozent des Aufkommens (§ 1 Gemeindefinanzreformgesetz). Die übrigen 85 Prozent teilen sich dann hälftig (also je 42,5 %) auf den Bund und die Länder nach Abs. 1 GG. Die Steuerertragshoheit liegt dabei jeweils bei der Gemeinde und dem Land, in dem der Steuerpflichtige seinen Wohnsitz hat. 

Bei der Berechnung des Verteilungsschlüssels zwischen den Gemeinden werden allerdings nur zu versteuernde Einkommen bis zu einem Höchstwert von 35.000 Euro pro Person berücksichtigt. Von den hohen Einkommen ihrer Bürger profitieren die Gemeinden somit nicht. Damit wird erreicht, dass im Bundesdurchschnitt rund 60 Prozent des örtlichen Aufkommens in die Ermittlung des Verteilungsschlüssels einfließen und das Steuerkraftgefälle zwischen großen und kleinen Gemeinden gewahrt wird.

Der gemeindliche Anteil an der Einkommensteuer belief sich 2014 auf 30,3 Milliarden Euro. Die Kommunen decken damit im Bundesdurchschnitt 2014 rund 14 % ihrer bereinigten Ausgaben. Für Bund und Länder ist die Bedeutung größer. Sie decken jeweils ein knappes Drittel ihrer Ausgaben über diese Steuer. Die Einnahmen der Gemeinden resultieren direkt aus dem örtlichen Aufkommen an Löhnen und Gehältern. Jenes ist abhängig vom Lohnniveau und Arbeitslosigkeit. Entsprechend groß sind die Aufkommensunterschiede pro Einwohner bundesweit. Im Jahr 2014 vereinnahmte der ärmste Landkreis Mansfeld-Südharz 176 Euro und der reichste Landkreis Starnberg 721 Euro je Einwohner.

Die Einkommensteuer wird sowohl auf das Welteinkommen natürlicher Personen mit Wohnsitz oder gewöhnlichem Aufenthalt im Inland (unbeschränkte Einkommensteuerpflicht gemäß ) als auch auf das inländische Einkommen (im Sinne des ) von Personen, die weder ihren Wohnsitz noch ihren gewöhnlichem Aufenthalt im Inland haben (beschränkte Einkommensteuerpflicht gemäß ), erhoben.

Die Einkommensteuer beruht auf dem Prinzip der Abschnittsbesteuerung (). Veranlagungszeitraum ist das Kalenderjahr (). Bei Gewerbebetrieben kann das Wirtschaftsjahr vom Kalenderjahr abweichen. Der Gewinn gilt in dem Veranlagungszeitraum als bezogen, in dem das Wirtschaftsjahr endet. Auch bei Land- und Forstwirten gibt es wegen der unterschiedlichen Erntezeiten und Ernteerträge Abweichungen (). Hier wird der Gewinn zeitanteilig auf die jeweiligen Veranlagungszeiträume aufgeteilt.

Die Veranlagung zur Einkommensteuer erfolgt in der Regel auf Grund der vom Steuerpflichtigen eingereichten Einkommensteuererklärung. Die Pflicht zur Abgabe einer Steuererklärung ergibt sich aus den steuergesetzlichen Regelungen ( und ). Weiterhin kann die Finanzbehörde den Steuerpflichtigen zur Abgabe einer Steuererklärung auffordern ().

Eine Einkommensteuererklärung ist – soweit eine Verpflichtung zur Abgabe besteht – regelmäßig bis spätestens 31. Juli (bis 2016 bis spätestens 31. Mai) des dem Veranlagungszeitraum folgenden Kalenderjahres bei dem Finanzamt einzureichen (). Diese Frist kann auf Antrag verlängert werden.
Für Veranlagungen auf Antrag bei Bezug von Einkünften aus nichtselbstständiger Arbeit () gilt eine vierjährige Frist nach Ablauf des jeweiligen Veranlagungszeitraums. Der Antrag auf Veranlagung wird durch die Abgabe einer Einkommensteuererklärung gestellt, was zur Geltendmachung zusätzlicher Steuerermäßigungen sinnvoll sein kann. Dies wird umgangssprachlich oft als „Lohnsteuerjahresausgleich“ bezeichnet, was aber rechtlich nicht zutreffend ist.

Zuständig für die Festsetzung und Erhebung der Einkommensteuer ist nach Abgabenordnung regelmäßig das Finanzamt, in dessen Bezirk der Steuerpflichtige seinen Wohnsitz hat.

Der Steuerpflichtige wird durch den Einkommensteuerbescheid (Verwaltungsakt) über die für den Veranlagungszeitraum festgesetzte Steuer unterrichtet. Regelmäßig erfolgt mit der Festsetzung der Einkommensteuer auch die Festsetzung der Kirchensteuer (soweit der Steuerpflichtige einer hebeberechtigten Konfession angehört, in Bayern abweichend Festsetzung durch die Kirchensteuerämter), des Solidaritätszuschlags und ggf. der Arbeitnehmersparzulage.

Die Steuerfestsetzung kann innerhalb eines Monats nach Bekanntgabe des Bescheides mit dem Einspruch angegriffen werden. Die Besteuerungsgrundlagen selbst sind nicht angreifbar (). Ob es aufgrund der Steuerfestsetzung zu einer Erstattung oder Nachzahlung für den Steuerpflichtigen kommt, regelt der Abrechnungsteil des Steuerbescheids, der neben der Steuerfestsetzung einen selbstständigen – ebenfalls angreifbaren – Verwaltungsakt darstellt.

Da die Einkommensteuer erst mit Ablauf eines Kalenderjahres entsteht und erst nach ihrer Festsetzung fällig wird, hat der Gesetzgeber die Entrichtung von Vorauszahlungen vorgeschrieben (). Die Vorauszahlungen werden vom Finanzamt durch einen Vorauszahlungsbescheid festgesetzt.

Das Einkommensteuerrecht unterscheidet zwischen Einnahmen, Einkünften, Einkommen und zu versteuerndem Einkommen. Einkünfte sind die um Werbungskosten oder Betriebsausgaben verminderten Einnahmen. Die Berechnung des Einkommens und des zu versteuernden Einkommens ist weiter unten dargestellt. 

Die zu zahlende Einkommensteuer ergibt sich durch Anwendung des Einkommensteuertarifs auf das zu versteuernde Einkommen als Bemessungsgrundlage, das folgendermaßen ermittelt wird:

Die Ermittlung der "Einkünfte" erfolgt für den Veranlagungszeitraum getrennt nach den Einkunftsarten ( Abs. 1 EStG). Hierbei sind Werbungskosten bzw. Betriebsausgaben von den "Einnahmen" bereits abgezogen.

+ Einkünfte aus Land- und Forstwirtschaft ()
+ Einkünfte aus Gewerbebetrieb ()
+ Einkünfte aus selbständiger Arbeit ()
+ Einkünfte aus nichtselbständiger Arbeit ()
+ Einkünfte aus Kapitalvermögen ()
+ Einkünfte aus Vermietung und Verpachtung ()
+ Sonstige Einkünfte ()
= Summe der Einkünfte (bei Zusammenveranlagung: jedes einzelnen Ehegatten)
− Altersentlastungsbetrag für Personen im Kalenderjahr nach Vollendung des 64. Lebensjahres (§ 24a EStG)
− Entlastungsbetrag für Alleinerziehende, 1908 €
− Freibetrag für Land- und Forstwirte, 900 €/1800 €
= Gesamtbetrag der Einkünfte (GdE, bei Zusammenveranlagung: beider Ehegatten)
− Verlustabzug (Höchstbetrag beachten!)
− Sonderausgaben, die keine Vorsorgeaufwendungen sind (), einschließlich Kirchensteuer
− Vorsorgeaufwendungen, einschließlich Altersvorsorge ()
− außergewöhnliche Belastungen allgemeiner Art, falls über der zumutbaren Belastung
− Unterhalt an bedürftige Personen (außergewöhnliche Belastungen)
− Ausbildungsfreibetrag (außergewöhnliche Belastungen)
− Beschäftigung einer Hilfe im Haushalt (außergewöhnliche Belastungen)
− Behindertenpauschbetrag (außergewöhnliche Belastungen)
− Hinterbliebenen-Pauschbetrag (außergewöhnliche Belastungen)
− Pflege-Pauschbetrag (außergewöhnliche Belastungen)
− Kinderbetreuungskosten (Sonderausgaben)
− Abzugsbetrag für die Förderung des Wohneigentums
+ hinzuzurechnende Einkünfte nach dem Außensteuergesetz
= Einkommen
− Freibeträge für Kinder, falls günstiger als das Kindergeld
− Härteausgleich
= zu versteuerndes Einkommen (zvE)
Jahressteuer nach Grund-/Splittingtabelle – siehe auch Einkommensteuertarif
= Tarifliche Einkommensteuer
+ Jahressteuer nach Sonderberechnung – siehe z. B. Progressionsvorbehalt
− Steuerermäßigungen
+ Hinzurechnungen (z. B. Kindergeld, falls Ansatz der Kinderfreibeträge; Altersvorsorgezulage, falls günstiger als Sonderausgaben)
= Festzusetzende Jahressteuer
− geleistete Vorauszahlungen
− anzurechnende Kapitalertragsteuer (falls Abgeltungsteuer ungünstiger)
− anzurechnende Lohnsteuer
= Einkommensteuernachzahlung/-erstattung

Die sieben Einkunftsarten stehen nicht gleichrangig nebeneinander.

Wie viel Steuer auf das jährlich zu versteuernde Einkommen (zvE) gezahlt werden muss, ergibt sich aus dem Einkommensteuertarif. Danach gelten die nachstehend dargestellten Tarifzonen.

Zunächst wird die für das zvE zutreffende gesetzliche Formel ausgewählt. Der individuelle Steuerbetrag berechnet sich anhand der ausgewählten Formel, indem das "ganze" Einkommen (also ohne Abzug des Grundfreibetrags) auf volle Euro abgerundet eingesetzt wird. Damit wird der gesamte Steuerbetrag berechnet und ebenfalls auf volle Euro abgerundet ().

Bei Verheirateten gilt das Ehegattensplitting. Um die Formeln für "verheiratete Steuerpflichtige" verwenden zu können, wird das gemeinsame zu versteuernde Einkommen halbiert, das Ergebnis in die zutreffende Formel als zvE eingesetzt und der danach errechnete Steuerbetrag verdoppelt.

Diese Formeln gelten prinzipiell auch für die Lohnsteuer (StKl 1 bis 4), wobei sich das zvE durch Abzug der pauschalisierten Freibeträge vom Bruttoeinkommen ergibt.

Formel zur Berechnung der Einkommensteuer:

formula_1

Nachfolgend werden die im Gesetzestext angegebenen Formeln näher erläutert. Mit "S" = Steuerbetrag, "zvE" = jährlich zu versteuerndes Einkommen und den weiter unten definierten Faktoren "y" und "z" gilt für "alleinstehende Steuerpflichtige":

Bis zu einem zvE von 9000 € fällt keine Steuer an.

formula_2

Dabei ist "y" ein Zehntausendstel von (zvE – 9000):

formula_3

Dadurch wird vom zvE der Eckwert, bei dem die Zone beginnt, abgezogen, weil die Berechnung nur den darüber hinausgehenden zvE-Teil betrifft. Die Division durch 10.000 dient lediglich dazu, zu viele Nachkommastellen bei den Faktoren zu vermeiden.

Die Zahl 1400 geteilt durch 10.000 entspricht dem Eingangssteuersatz von 14 %.

Die Zahl 997,8 bestimmt den linearen Progressionsfaktor. Sie bewirkt, dass der Grenzsteuersatz linear ansteigt und so sprunghafte Übergänge zwischen den Tarifzonen vermieden werden (siehe Linear-progressiver Tarif).

In dieser Zone fallen maximal 948,49 € Einkommenssteuer an.

formula_4

Dabei ist z ein Zehntausendstel von (zvE – 13.996 €):

formula_5

Auch hier wird vom zvE der Eckwert, bei dem die Zone beginnt, abgezogen.

Die Zahl 2397 geteilt durch 10.000 entspricht dem Eingangssteuersatz dieser Zone von 23,97 %. Die Zahl 220,13 bestimmt hier den linearen Progressionsfaktor.

Die Zahl 948,49 entspricht dem Steuerbetrag der zweiten Zone, der zur Belastung dieser Zone hinzu addiert wird.

Am Ende dieser Zone fallen zusammen mit den unteren Zonen maximal 14.456,83 € Einkommensteuer an.

Das zvE wird mit dem Grenzsteuersatz von 42 % multipliziert und es wird 8.621,75 subtrahiert.

formula_6

Die Zahl 8.621,75 kann einer steuerlichen Kenngröße nicht direkt zugeordnet werden, denn sie ergibt sich durch Umstellen der Ursprungsformel:

formula_7

Man erkennt hier den Eckwert von 54.949 Euro und die Summe der Steuerbeträge aus Zone zwei und drei mit 14.456,83 Euro.

Am Ende dieser Zone fallen zusammen mit den unteren Zonen maximal 100.801,70 € Einkommensteuer an.

Das zvE wird mit dem Grenzsteuersatz von 45 % multipliziert und es wird 16.437,70 € subtrahiert.

formula_8

Die gleichwertige Formel lautet:

formula_9

Die Erläuterungen zur vierten Zone gelten hier sinngemäß.

Bei der Ermittlung des Einkommensteuertarifs bedient man sich der linearen Progression. Das heißt, mit steigendem Einkommen steigt auch der Grenzsteuersatz linear an. Das gilt jedoch nur in den Progressionszonen. In der Proportionalzone ist der Grenzsteuersatz konstant und entspricht dem Spitzensteuersatz. Beim Verlauf des Durchschnittssteuersatzes erfolgt eine asymptotische Annäherung an diesen Spitzensteuersatz.

Mathematisch kann dies wie folgt ausgedrückt werden:
Ist "S" der Steuerbetrag und "B" die Bemessungsgrundlage, so ergibt sich der Steuerbetrag als Funktion in Abhängigkeit von "B":

wobei "a, b, c" per Gesetz festgelegte Parameter sind. Der Faktor "b" ist der anfängliche Grenzsteuersatz an der Tarifgrenze (Eckwert) und der Faktor "a" definiert den linearen Progressionsfaktor. Er berechnet sich aus der Differenz des anfänglichen Grenzsteuersatzes der betrachteten Tarifzone zu dem der nächsten Zone und der Differenz der entsprechenden Eckwerte. Der Faktor 2 im Nenner ist für die korrekte Ermittlung der Grenzsteuersatzfunktion (Ableitungsregel) erforderlich.

In den "Progressionszonen" ist der Parameter "a" größer als null, in der "Proportionalzone" ist er gleich null, dann ergibt sich die Geradengleichung

Im deutschen EStG ist die Konstante "c" unterschiedlich definiert. In den "Progressionszonen" ist "c" der Steuerteilbetrag, der sich aus der vorhergehenden Tarifzone ergibt. Daher gilt hier für "B = zvE − Eckwert". In den "Proportionalzonen" ist der Betrag von "c" jedoch der Steuerteilbetrag, der in allen vorhergehenden Tarifzonen zu viel berechnet wird, indem zunächst der Grenzsteuersatz "b" mit "B = zvE" auf das gesamte zvE angewandt wird. Dies wird daraufhin durch Subtraktion des Betrages von "c" korrigiert.

Die Parameter entsprechen den in der folgenden Tabelle wiedergegebenen Werten (links die Werte laut Gesetz und rechts mathematisch gleichwertig mit c als Steuerbetrag der jeweils niedrigeren Tarifzone). So wird klar, dass der Grenzsteuersatz in den beiden oberen Zonen nur für den Anteil des Einkommens gilt, der den jeweiligen Eckwert übersteigt.

Das Verhältnis von Steuerbetrag "S" zum zu versteuernden Einkommen "zvE" ergibt den Durchschnittssteuersatz formula_13:

Der Grenzsteuersatz formula_15 ist der augenblickliche Anstieg des Steuerbetrags, d. h. die Ableitung (Differentialquotient) formula_16 des Steuerbetrags nach dem zu versteuernden Einkommen. Für den Grenzsteuersatz gilt daher in den Progressionszonen:

sowie in der Proportionalzone

mit denselben Werten der Parameter "a" und "b" wie oben.

Die genauesten statistischen Daten liefert die dreijährliche Lohn- und Einkommensteuerstatistik des Statistischen Bundesamtes (Fachserie 14 Reihe 7.1) als Vollerhebung. Die neueste Ausgabe enthält die Daten aus dem Jahre 2010 und ist am 28. November 2014 erschienen. Da in Deutschland eine Veranlagung nach Grund- oder Splittingtarif erfolgt, sind in dieser Statistik zwei getrennte Auswertungen vorhanden (Tabellen 2.1 und 2.2).

Die beiden oberen Grafiken auf der rechten Seite veranschaulichen die Aufteilung der Summen aller Einkünfte bzw. der Summen aller zu versteuernden Einkommen (zvE) in Bezug zur Größenklasse des zvE. Außerdem sind die mittleren Steuersätze, die tatsächlich ausgewiesen sind (rote Linie, durchgezogen), und die nach § 32a EStG aus dem Jahr 2010 zu erwartenden Durchschnittssteuersätze (rote Linie, gestrichelt) eingetragen. Es ist ersichtlich, dass auch bei Unterschreiten des Grundfreibetrags Fälle ausgewiesen sind, für die Einkommensteuer anfällt. Gründe dafür können sein:

Weiterhin ist ersichtlich, dass bei höheren Einkommen die tatsächlichen Steueranteile deutlich unter den nach § 32a EStG zu erwartenden Durchschnittsteuersätzen liegen. Ursächlich dafür ist die Anwendung des Rechenschemas zur Ermittlung des Einkommensteuerbetrages (weiter oben in diesem Artikel). Hier gibt es noch einige Hinzurechnungs- und Abzugspositionen (Steuerermäßigungen), die diesen Effekt erklären.
Die Aufteilung der Anzahl der steuerpflichtigen Einzelpersonen und Ehepaare nach der Größenklasse des zvE zeigen die beiden unteren Bilder.

Auch hier fällt ein signifikant hoher Anteil an Steuerpflichtigen mit zvE unter dem Grundfreibetrag auf (grüne und blaue Balken), insbesondere bei den nach Grundtarif besteuerten Personen. Diese zahlen aber, wie zu erwarten ist, überwiegend gar keine Steuern (rote Balken).

Die meisten Steuerzahler liegen im Bereich des zvE von etwa 15.000 bis 35.000 Euro (Grundtarif) bzw. 30.000 bis 70.000 Euro (Splittingtarif) jährlich.
Infolge der Ungleichheit der Bruttoeinkommen und des progressiv ausgestalteten Einkommensteuertarifs wird der größte Teil der Einkommen- und Lohnsteuer von den in Dezilen (in prozentuale Zehnergruppen zerlegtes Gesamtaufkommen) erfassten oberen Einkommensgruppen erbracht, etwa die Hälfte allein von dem obersten Dezil, auf das knapp 25 % des Gesamteinkommens entfällt, während die unteren fünf Dezile (also die Hälfte der Steuerpflichtigen), auf die knapp 30 % des steuerpflichtigen Gesamteinkommen entfällt, nur mit etwa 6,5 % am Steueraufkommen beteiligt sind.

Die Progressivität der Einkommensteuer ist auch aus der nachfolgenden Tabelle zu entnehmen. Diese Tabelle aus dem Jahr 2007 ist eine Teilwiedergabe einer amtlichen Tabelle aus dem dritten Armuts- und Reichtumsbericht der Bundesregierung und basiert auf der Einkommensverteilung des Jahres 2005. Betrachtet man jedoch die Wirkung aller Steuern auf die Einkommensverteilung, so schwächt die regressive Wirkung der Verbrauchsteuern (etwa Mehrwertsteuer, Mineralöl- und Kraftfahrzeugsteuer) die progressive Wirkung der Einkommensteuer insgesamt ab.

Im internationalen Vergleich liegt der "Spitzensteuersatz" der deutschen Einkommensteuer in der oberen Hälfte, der "Eingangssteuersatz" im unteren Viertel. Eine ausführliche Tabelle dazu befindet sich im Artikel Einkommensteuertarif. Die Angaben zum Eingangs- und Spitzensteuersatz sagen jedoch alleine noch nichts über den Tarifverlauf, die Eckwerte und die Belastung der einzelnen Einkommensgruppen aus.

Das deutsche Einkommensteuerrecht steht seit Jahren in der Kritik: Zahlreiche Ausnahmen und Sonderregelungen führten zu Intransparenz. Ein wichtiger Eckpunkt aller Steuerreform-Konzepte ist die Steuervereinfachung. Ausnahmen und Sonderregelungen sollen eingeschränkt oder abgeschafft werden, um mit den dadurch freiwerdenden Mitteln die Steuersätze zu senken, was allerdings nicht ohne Weiteres bedeutet, dass die Einkommensteuer dadurch allgemein sinken muss.

Kritisiert wird auch die schleichende Erhöhung der Steuerlast durch die kalte Progression. Der Steuerzahlerbund hat errechnet, dass im Jahr 1958 der Spitzensteuersatz von 53 % bei dem 20-fachen des damaligen Durchschnittseinkommens zu Grunde gelegt wurde und derzeit (Stand 2017) bereits bei dem 1,3-fachen Durchschnittseinkommen. Jedoch ist dabei nicht berücksichtigt, dass der zum Vergleich herangezogene Grenzsteuersatz hier auf 42 % gesenkt wurde. Vom Jahr 1995 bis 2017 sei die Zahl der Steuerpflichtigen, die das Ende der Progressionszone (bei etwa 55.000 Euro mit 42 %) erreichen, von ca. 0,5 Millionen auf ca. 3,7 Millionen gestiegen.

Der Bundesrechnungshof kam in einem Bericht für den Finanzausschuss des Bundestages vom 17. Januar 2012 zu dem Ergebnis, dass der gesetzmäßige Vollzug der Steuergesetze bei der Veranlagung der Arbeitnehmer weiterhin nicht gewährleistet ist.

Den Bewohnern der Gemeinde Büsingen am Hochrhein wird aus Billigkeitsgründen ein zusätzlicher Freibetrag zur Abgeltung der höheren Schweizer Lebenshaltungskosten gewährt. Dieser beträgt 30 % des zu versteuernden Einkommens, jedoch nicht mehr als 30 % von jährlich 15.338 € bei Ledigen und 30.675 € bei Verheirateten. Für jedes Kind erhöht sich diese Bemessungsgrundlage um 7.670 €.




</doc>
<doc id="10112" url="https://de.wikipedia.org/wiki?curid=10112" title="Tritium">
Tritium

Tritium (von "trítos" ‚der Dritte‘) ist ein in der Natur in Spuren vorkommendes Isotop des Wasserstoffs. Sein Atomkern wird auch Triton genannt, er besteht aus einem Proton und zwei Neutronen. Tritium (H) wird aufgrund seiner Masse auch als "„überschwerer“" oder "„superschwerer“ Wasserstoff" bezeichnet. Tritium ist ein radioaktiver Betastrahler; es zerfällt mit einer Halbwertszeit von 12,32 Jahren.

Wasserstoff hat noch zwei weitere leichtere Isotope, das häufige Protium (H) und das seltenere Deuterium (H).

Eigenständige Namen und Symbole für Isotope eines Elements gibt es nur bei Deuterium (Symbol D anstatt H) und Tritium (Symbol T anstatt H), weil das Massenverhältnis zwischen Protium und seinen Isotopen verhältnismäßig groß ist (Deuterium 1:2 und Tritium 1:3) und sich daraus merkliche Unterschiede im chemischen Verhalten ergeben. (Zum Vergleich: Bei dem nächstgrößeren Isotopenpaar He und He ist es 1:1,33; bei U und U nur noch 1:1,013).

Tritium entsteht auf natürliche Weise vor allem in der Stratosphäre. Schnelle Protonen der kosmischen Strahlung bilden durch Spallation teilweise direkt Tritium, vor allem aber sekundäre Neutronen, die in der Stratosphäre und oberen Troposphäre in Reaktionen mit Stickstoff Tritium bilden:

oder auch

Tritium bildet zunächst HT (Tritiumwasserstoff), diffundiert (sofern in der Stratosphäre entstanden) zur Tropopause, oxidiert in der Troposphäre mit einer Zeitkonstanten von 6,5 Jahren durch photochemische Reaktionen zu HTO (T-haltiges Wasser) und regnet dann vergleichsweise schnell aus. Aus Bildung und radioaktivem Zerfall stellt sich ein Fließgleichgewicht ein, wodurch sich in der Biosphäre ständig ca. 3,5 kg Tritium aus natürlicher Produktion befinden, zu 99 % in oberflächennahen Schichten der Ozeane.

In mit schwerem Wasser moderierten Reaktoren (siehe z. B. CANDU) fällt Tritium in einer Menge von rund 1 kg pro 5 GWa (Gigawattjahre) – das sind ca. 158,4 PJ erzeugter elektrischer Energie – als unvermeidliches Nebenprodukt an. Die Extraktion aus dem Kühlwasser ist aufwändig, da hierzu eine Isotopentrennung erforderlich ist.

Auch im Kühlmittel des Primärkreises vieler Druckwasserreaktoren bildet sich Tritium, da dem Wasser eine gewisse Menge an Borsäure zur Steuerung der Reaktivität im Reaktorkern beigesetzt wird. Die erwünschte Reaktion ist dabei, dass Bor-10 ein Neutron absorbiert, und dann sofort in ein Alpha-Teilchen und Lithium-7 zerfällt. Eine unerwünschte Nebenreaktion ist jedoch, dass Bor-11 ein Neutron absorbiert und dann in Tritium und Beryllium-9 zerfällt.

Tritium ist außerdem ein weniger häufiges Nebenprodukt bei der Kernspaltung von spaltbaren Atomkernen wie U, Pu oder U und entsteht dabei mit einer Häufigkeit von ungefähr 1 Tritiumkern pro 10 Spaltungen. Das Tritium entsteht bei 7 % der ternären Zerfälle, also wenn das Ausgangsnuklid in drei anstatt zwei Bruchstücke gespalten wird. In der Regel verbleibt dieses Tritium zusammen mit den anderen Spaltprodukten in den Brennelementen. Bei einem Unfall mit Kernschmelze oder bei der Wiederaufarbeitung von Kernbrennstoffen kann es aber freigesetzt werden.

Tritium kann durch Reaktion von Li mit Neutronen hergestellt werden:

In Wasserstoffbomben wird das zur Fusion nötige Tritium überwiegend erst während der Explosion der Bombe auf diesem Weg hergestellt. Die dazu benötigten Neutronen stammen anfangs aus der Spaltung von Uran oder Plutonium, später auch aus der Fusionsreaktion selbst.

Für Kernfusionsreaktoren ist die Produktion von Tritium aus Lithium auf demselben Weg vorgesehen: Anfangs, insbesondere für den Betrieb von Forschungsreaktoren, soll Tritium mittels des Neutronenflusses in Kernreaktoren erbrütet werden. Später soll das nötige Tritium im Blanket der Kernfusionsreaktoren hergestellt werden.

Während sich im Atomkern des Wasserstoffatoms (H) neben dem Proton kein Neutron befindet und im Deuterium (H oder D) ein Neutron, sind es im Tritium zwei. Im Gegensatz zu H und H ist der Tritiumkern instabil und zerfällt mit einer Halbwertszeit von 12,32 Jahren unter Emission eines Elektrons und eines Antineutrinos in das Heliumisotop He (Beta-Minus-Zerfall):
Bei diesem Zerfall wird insgesamt eine Energie von 18,6 keV frei:

formula_5.

Davon erhält das Elektron im Mittel 5,7 keV kinetische Energie.
Im Vergleich zu anderen Betastrahlern ist die Strahlung sehr weich. In Wasser wird sie nach wenigen Mikrometern gestoppt; sie kann auch die oberen Hautschichten nicht durchdringen. Die Radioaktivität von Tritium ist daher vor allem beim Verschlucken oder beim Einatmen gefährlich.

Das Symbol ist H; aus Gründen der Vereinfachung in der Formelschreibweise wird häufig auch T verwendet.

Tritiumoxid (überschweres Wasser) TO hat eine Siedetemperatur von 101,51 °C und eine Schmelztemperatur von 4,48 °C.

Unter anderem in der Biologie, Chemie und Medizin wird Tritium als sog. Tracer zur Markierung bestimmter Substanzen verwendet.

In Tritiumgaslichtquellen (langlebige Leuchtmittel) wird gasförmiges Tritium zusammen mit einem Leuchtstoff in versiegelten Borsilikatglasröhrchen verwendet. Die Betastrahlung des Tritiums regt die Leuchtstoff-Beschichtung innen auf dem Glasröhrchen zu einem schwachen Leuchten (Fluoreszenz) an. Diese »kalten Leuchten« haben eine theoretische Lebensdauer von mehreren Jahrzehnten und sind in verschiedenen Farben zu erhalten.

Die vorgenannten Tritiumgaslichtquellen kommen auch als Lichtquelle auf Uhrenzifferblättern und -zeigern bestimmter Uhrenmodelle zur Anwendung. In Kompassen, wie sie die US Army in den 1980er Jahren benutzte, verwendete man entsprechend 120 mCi Tritium in der Farbe für Markierungen der Himmelsrichtungen. Auch auf Visiere von Waffen wurde tritiumhaltige Leuchtfarbe aufgebracht. 

Ionisationsrauchmelder arbeiten teilweise mit einer Tritiumgas-Ampulle als Ionisator. Bei der Herstellung und Lagerung größerer Mengen bestehen wegen der Radioaktivität allerdings gesundheitliche Risiken. Daher wird es durch phosphoreszierende Leuchtmittel, wie z. B. Superluminova ersetzt. In Rauchmeldern kann statt Tritium auch Am (Americium) verwendet werden.

Ein 1:1-Gemisch von Deuterium und Tritium (D-T) hat die günstigsten Eigenschaften als Brennstoff für die Fusionsenergie: Eine hohe Energieausbeute, einen relativ großen Wirkungsquerschnitt, die kleinstmögliche dafür zu überwindende Coulombkraft (nur eine elektrische Ladung je Atom) und dadurch eine vergleichsweise niedrige Fusionstemperatur. Sie beträgt in Fusionsreaktoren etwa 100 Millionen Kelvin, gegenüber 400 Millionen Kelvin bei der in dieser Hinsicht nächstgeeigneten Deuterium-Deuterium-Reaktion. Deshalb kommt für zukünftige Fusionskraftwerke nur ein D-T-Gemisch in Betracht. Für deren Dauerbetrieb wären jedoch ausreichende Mengen Tritium nur durch Erbrüten aus Lithium-6 im Reaktor selbst herstellbar. Erste Experimente, bei denen D-T-Fusionen nachgewiesen wurden, fanden bisher an den Test-Anlagen Joint European Torus (JET) in Culham, England sowie am Tokamak Fusion Test Reactor (TFTR) in Princeton statt. D-T-Experimente in größerem Maßstab sind für die zweite Experimentphase des Projekts ITER vorgesehen. Anlagen zur Erforschung der Grundlagen eines Fusionskraftwerks wie etwa in Deutschland der Tokamak ASDEX Upgrade in Garching oder der Stellarator Wendelstein 7-X in Greifswald verwenden dagegen nur Deuterium- oder Wasserstoff-Plasmen, weil es erst einmal nur darum ging und geht, ein stabiles Plasma zu erzeugen. Damit besteht unmittelbar nach jedem Experiment Zugang zu der Anlage und den Messgeräten und der Strahlenschutz-Aufwand kann geringer gehalten werden (der ist auch bei einem Deuterium-Plasma notwendig, weil darin auch bei 100 Millionen Kelvin schon zahlreiche Deuterium-Deuterium-Fusionen stattfinden).

Tritium ist auch ein wesentlicher Bestandteil bestimmter Kernwaffen. Bereits wenige Gramm eines gasförmigen Deuterium-Tritium-Gemischs können in Kernspaltungs-Waffen deren Sprengstoffwirkung um den Faktor 2 verstärken, auch „boosting“ genannt. Für Neutronenbomben ist Tritium sogar essentiell zur Funktion notwendig; hier werden bis zu 20 Gramm Tritium pro Sprengkopf benötigt. In Wasserstoffbomben wird Tritium nur als Booster und zur Einstellung der Sprengkraft in der Fissionstufe verwendet, in der Fusionstufe kommt hingegen Lithiumdeuterid zum Einsatz, aus dem erst unter Neutronenbeschuss Tritium gebildet wird.

Wegen seiner relativ kurzen Halbwertzeit von 12,3 Jahren wird Tritium zur Altersbestimmung von oberflächennahem Grundwasser oder zur Untersuchung von hydrologischen Fließbedingungen verwendet. Ausgangspunkt der Berechnungen ist der Eintrag des Tritium in den 1950er- und frühen 1960er-Jahren in das Grundwasser. Ursachen des Eintrags waren die zahlreichen Atomtests in der Atmosphäre, die erhebliche Mengen an Tritium freisetzten.

Durch Betazerfall von Tritium entsteht nichtradioaktives Helium-3. Auf Grund seiner extremen Seltenheit in natürlichen Heliumquellen stellt dies derzeit die am wenigsten preisintensive Quelle für Helium-3 dar. Es wird in der Grundlagenforschung benötigt.

Die von Tritium ausgehenden chemischen Gefahren sind zwar mit denen von Wasserstoff identisch, aber vergleichsweise zu vernachlässigen gegenüber den radioaktiven Gefahren als gasförmiger Betastrahler, die auch völlig andere Handhabungsvorschriften erfordern. Die Kennzeichnung für Wasserstoff gemäß Anhang VI der Verordnung (EG) Nr. 1272/2008 (CLP), die sich nur mit den von der Chemie ausgehenden Gefahren befassen, würden hier eher verharmlosend wirken und wurden deshalb weggelassen, zumal Tritium nur in dafür fachlich qualifizierten Laboratorien und nur in geringen Mengen gehandhabt wird.

Tritium ist nicht stark radiotoxisch, kann jedoch in Form von Wasser im Körper gespeichert und umgesetzt werden. Eine französisch-belgische Studie von 2008 kommt zum Schluss, dass seine radiologischen Wirkungen bisher unterschätzt wurden: Es kann sich z. B. in die DNS (Erbsubstanz) einlagern, was vor allem bei einer Schwangerschaft problematisch sein kann. Eine andere Studie kommt sogar zu dem Schluss, die Wirkung könnte bisher um den Faktor 1000–5000 unterschätzt worden sein.

Der Nachweis von Tritium erfolgt unter anderem anhand der Wirkungen der Radioaktivität mittels Flüssigszintillationszählern oder offenen Ionisationskammern. Ebenso können Massenspektrometer zum Nachweis verwendet werden.



</doc>
<doc id="10114" url="https://de.wikipedia.org/wiki?curid=10114" title="Chuck Palahniuk">
Chuck Palahniuk

Chuck Palahniuk [], bürgerlich "Charles Michael Palahniuk" (* 21. Februar 1962 in Pasco, Washington), ist ein US-amerikanischer Schriftsteller und Journalist. Er wohnt derzeit in Vancouver und ist vor allem durch sein Erstlingswerk "Fight Club" (1996) bekannt geworden, das von David Fincher mit Brad Pitt und Edward Norton verfilmt wurde (siehe auch "Fight Club"). Sein Roman "Choke" (2008) wurde von Clark Gregg mit Anjelica Huston und Sam Rockwell ebenfalls verfilmt.

Vor seiner Karriere als Schriftsteller besuchte Palahniuk die "School of Journalism" an der University of Oregon, arbeitete als freier Journalist sowie Nutzfahrzeugmechaniker und war ehrenamtlich in Obdachlosenheimen und Jugendherbergen tätig.

Auf seiner Internetpräsenz bietet Palahniuk „Premiumnutzern“ unter anderem Schreibseminare an und beantwortet einen großen Teil der Leseranfragen persönlich. Seine Internetpräsenz gilt als eine der größten zentralen Anlaufstellen für einen einzelnen Autor.

Palahniuk ist Mitglied der "Cacophony Society", einer anarchistischen Gesellschaft, die dem "Suicide Club of San Francisco" entsprungen ist. Das sogenannte "Project Mayhem" aus seinem Roman "Fight Club" basiert auf den Grundgedanken der "Cacophony Society".

Palahniuks Idee zu "Fight Club" entstand, als er eines Morgens von einer Rauferei gezeichnet zur Arbeit erschien und keiner seiner Kollegen ihn darauf ansprach. Er überlegte, ob es möglich sei, solch ein Doppelleben zu führen, ohne dass von der Gesellschaft unangenehme Fragen gestellt werden. Obwohl "Fight Club" Palahniuks erste Veröffentlichung ist, hat er zuvor bereits "Invisible Monsters" und "Insomnia: If You Lived Here, You'd Be Home Already" geschrieben. "Invisible Monsters" fand zunächst aufgrund seines „verstörenden Inhalts“ keinen Verleger, und mit "Insomnia: If You Lived Here, You'd Be Home Already" war Palahniuk nicht zufrieden genug, um es veröffentlichen zu lassen; es bildete jedoch die Grundlage für "Fight Club".

"Die Kolonie" ist ein aus 24 Kurzgeschichten in einer Rahmenhandlung zusammengesetzter Roman. Die Kurzgeschichte "Guts" (deutscher Titel: "Vorfall") sorgte auf Palahniuks Lesungen für Aufregung, da bei diesen insgesamt 73 Leute ohnmächtig wurden. Die Geschichte ist als Sonderbeilage zum Time Magazine erschienen.

Die 2003 produzierte Dokumentation "Postcards from the Future" befasst sich mit Palahniuk und seinem Status als Kultautor in den USA.





</doc>
<doc id="10116" url="https://de.wikipedia.org/wiki?curid=10116" title="Gemeiner Flugdrache">
Gemeiner Flugdrache

Der Gemeine Flugdrache ("Draco volans") ist ein Schuppenkriechtier aus der Familie der Agame in den tropischen Regenwäldern Südostasiens. Diese Art der Flugdrachen ("Draco") ist verbreitet in Hinterindien, auf Sumatra, Java und Borneo. Gelegentlich ist sie auch in Sekundärwäldern und Plantagen anzutreffen.

Gemeine Flugdrachen werden 20 Zentimeter lang, wovon der dünne Schwanz mehr als die Hälfte ausmacht. Männchen haben einen flachen Rückenkamm, und eine große, orangefarbene Kehlfalte, die bei den Weibchen kleiner und von blauer Farbe ist. Die von den ausgebreiteten verlängerten freien Rippen gestützte, orange-schwarz gestreifte und als Flughaut genutzte Flankenhaut ermöglicht es den Tieren zwischen Bäumen zu gleiten. Sie wird auch beim Imponierverhalten gespreizt.

Der Gemeine Flugdrache hält sich auf Bäumen in Höhen von einem bis 20 Metern auf und geht nur zur Eiablage auf den Boden. Er ernährt sich vor allem von baumbewohnenden Ameisen. Seine Gleitflüge haben eine durchschnittliche Weite von 20 bis 30 Metern, dabei beträgt der Höhenverlust 5 bis 8 Meter. Die Maximalweite soll 60 Meter betragen. Der Gleitflug wird durch Drehungen des Schwanzes stabilisiert. Durch Schwanzbewegungen und eine Änderung der Flughautstellung ist auch das Ansteuern von Zielen und das Ausweichen bei Hindernissen möglich.

Flugdrachen legen ihre Eier in den Bodengrund. Im Tierpark Berlin entnahm man einem gerade verendeten Weibchen drei Eier. Die frisch geschlüpften Jungtiere hatten eine Kopf-Rumpf-Länge von 2,5 bis 2,6 und eine Schwanzlänge von 4 bis 4,2 Zentimeter. Sie waren rindenfarben, hatten einen braunschwarzen Kopf und geringelte Schwänze.



</doc>
<doc id="10118" url="https://de.wikipedia.org/wiki?curid=10118" title="Drache">
Drache

Drache (lateinisch "draco", altgriechisch "drákōn" „Schlange“, von "dérkesthai" „sehen“) steht für:


Drache ist der Familienname von:



Siehe auch:



</doc>
<doc id="10119" url="https://de.wikipedia.org/wiki?curid=10119" title="Gmunden">
Gmunden

Gmunden ist eine Stadt im oberösterreichischen Salzkammergut.

Die Stadt liegt am Nordufer des Traunsees und hat Einwohner (Stand ). Gmunden ist Sitz der Bezirkshauptmannschaft des Bezirks Gmunden. Als Zentrum des Gerichtsbezirks Gmunden beherbergt es das Bezirksgericht. Die Stadt ist bekannt als Herkunftsort der Gmundner Keramik mit ihrem charakteristischen weiß-grün geflammten Dekor. Während der k. u. k. Monarchie galt Gmunden als renommierte Sommerfrische, wovon heute noch die zahlreichen Villen künden. Bekannt wurde die Stadt in den 1990er-Jahren unter anderem durch die Fernsehserie "Schlosshotel Orth".

Gmunden liegt auf 425 m Höhe im Traunviertel. Die Ausdehnung beträgt von Nord nach Süd 9,4 km, von West nach Ost 13,5 km. Die Gesamtfläche beträgt 63,49 km². Das Gemeindegebiet umfasst das Nordufer sowie einen Großteil des Ostufers des Traunsees.

Die Stadtgemeinde besteht aus den "Katastralgemeinden:"
Durch die Lage im Alpenvorland ist das Gemeindegebiet Gmundens von zahlreichen Bergen umgeben. Die höchste Erhebung ist mit 1.691 m der Traunstein. Dieser gilt als Hausberg der Gmundner und ist ein beliebter Wander- und Kletterberg. Darüber hinaus ist er der Garant für genügend Trinkwasser.

Der Grünberg ist ebenfalls ein beliebtes Ausflugsziel. Dieser ist über markierte Wanderwege zu Fuß erreichbar. Seit Sommer 2014 ist er wieder über die Grünbergseilbahn erreichbar.

Der Traunsee nimmt einen wesentlichen Teil des Gemeindegebietes ein. Vor dem Jahr 1900 verursachte der See immer wieder Hochwasser. Mehrere Male war der ganze Rathausplatz überschwemmt. Die Wassermarken mit den jeweiligen Jahreszahlen sind an manchen Hauswänden und am Schloss Ort zu sehen. Bei Ebensee fließt die Traun in den Traunsee, bei Gmunden verlässt sie den See wieder. Die Katastralgemeinde Traundorf ist durch den Fluss vom Stadtzentrum getrennt.

Das Wasserwerk der Stadt Gmunden gewinnt sein Trinkwasser aus zwei großen Wasserwerken "Traunstein West" und "Auwald". Um den steigenden Bedarf an qualitativ hochwertigem Trinkwasser bewältigen zu können, haben sich elf Gemeinden der Bezirke Gmunden und Kirchdorf zu einem Wasserverband zusammengeschlossen.

Das Gebiet Gmunden zählt zu den ältesten Ablagerungen des rhenodanubischen Flysches. Im Gebiet des Gschliefgrabens, der Grenze zwischen Grünberg und Traunstein, tritt das Helvetikum hervor, wo seit 1984 systematisch nach Fossilien gesucht wird. Hier wurde erstmals die noch unbekannte Art "Hoplitoplacenticeras preyi" entdeckt. Insgesamt wurden 35 Arten von Ammoniten nachgewiesen.

Südlich des Gschliefgrabens schließt sich die "Langbathscholle" an, die aus Hauptdolomit besteht. Besonders in der Kaltenbachwildnis treten die bizarren Verwitterungsformen dieses Gesteins eindrucksvoll zu Tage. Unmittelbar südlich des "Zierler Berges" beginnt der Traunstein, der als Teil der Höllengebirgsdecke aus Wettersteinkalk besteht.

Im Gschliefgraben kommt es immer wieder zu Rutschungen. Eine konkrete Lösung für dieses Problem hat man noch nicht gefunden. Im Dezember 2007 mussten die Bewohner der Gebäude in der Traunsteinstraße Nr. 239 bis 303 wegen der Gefahr eines Hangrutsches aus dem Gschliefgraben evakuiert werden.

Das ursprüngliche Zentrum der Stadt war der Marktplatz. Dieser verlor jedoch in der Blütezeit des Salzhandels an Bedeutung. In der Folge wurde der Rathausplatz zum Stadtzentrum.

Wegen der gebirgigen Umgebung steht nur wenig Fläche als Baugrund zur Verfügung.

Der Großteil des zur Verfügung stehenden Baugebiets wird für Wohngebiete genutzt, deren Gesamtfläche 220,9 ha beträgt. 11,6 ha sind Dorfgebiet, 26,4 ha Kerngebiet, 56,5 ha gemischtes Baugebiet und 21,1 ha Sonderbaugebiet. Für industrielle und gewerbliche Bauten stehen insgesamt 68,8 ha zur Verfügung.

Die erste Besiedlung des heutigen Stadtgebietes geht bis ins 5. Jahrhundert zurück. Wann Gmunden von einem befestigten Markt zur Stadt wurde, ist nicht genau bekannt. Gmunden feierte die Stadterhebung im Jahr 1278. Im Mittelalter war Gmunden schwer befestigt. Erst um 1300 erhielt Gmunden eine eigene Kirche. Ein Landesgericht (heute Bezirksgericht) gab es bereits im Jahre 1217. Wirtschaftliches und politisches Oberhaupt war der Salzamtmann bzw. der Stadtrichter.

Die Lebensgrundlage der Stadt blieb auch in der frühen Neuzeit der Salzhandel. Die wirtschaftliche Situation Gmundens verschlechterte sich im Zuge des oberösterreichischen Bauernkrieges immens, der Salzhandel konnte kaum aufrechterhalten werden und Plünderungen standen an der Tagesordnung. Nachdem ein Waffenstillstand ausgehandelt wurde, rückten Musketiere ein.

Während des 17. Jahrhunderts wuchs die wirtschaftliche Bedeutung von Gmunden als Lieferant von Kriegsschiffen. Wegen der zweiten Wiener Türkenbelagerung mussten Tausende Flüchtlinge aufgenommen und verpflegt werden, weshalb die bestehende Türkensteuer beträchtlich erhöht wurde.
Während des österreichischen Erbfolgekrieges glichen Gmunden und die nähere Umgebung einem großen Feldlager, obwohl es nie zu kriegerischen Auseinandersetzungen kam. Die Quartierlasten zuerst für die Bayern, dann für die Franzosen und nach dem Einmarsch für die kaiserlichen Truppen in Oberösterreich waren enorm.

Anfang des 19. Jahrhunderts zeichnete sich das Ende der Blüte des Salzhandels ab, weshalb es wichtig war, neue Einnahmequellen zu finden. Die Entwicklung Gmundens zur Kurstadt und die daraus hervorgehende legendäre Konkurrenz mit Bad Ischl begann. 1862 wurde Gmunden zur Kurstadt ernannt.

Im September 1914 übernahm Gmunden die Aufgabe einer Lazarettstadt. 190 verwundete Soldaten kamen an und wurden in die dafür eingerichteten Spitäler gebracht. In den folgenden Monaten trafen hunderte Verwundete ein. Im Jahre 1916 kam es zu einer rapiden Verschlechterung der Versorgung der Bevölkerung, die vorerst nicht verbessert werden konnte.

Mit 1. Januar 1939 mussten Teile von Eck und Ort sowie Traunleithen und Theresienthal von der Gemeinde Altmünster an Gmunden abgetreten werden. Somit war auch der Bahnhof in das Gemeindegebiet von Gmunden eingegliedert. Bis in den 1950er-Jahren versuchte die Gemeinde Altmünster die abgetretenen Gebiete zurückzubekommen.

1942 musste Gmunden hunderte Flüchtlinge aufnehmen. Ein Jahr vor Kriegsende wurde Gmunden zur Flüchtlingsstadt, die Bevölkerung wuchs auf 30.000 Menschen an, die Gemeinde konnte diese Menschenmengen nicht bewältigen. Obwohl die Stadt von Bombenangriffen verschont blieb, war die Wirtschaft gegen Ende des Krieges völlig am Boden. Im Zweiten Weltkrieg wurde sie ebenfalls als Lazarettstadt genutzt. Von den über 600 eingezogenen Gmundnern überlebten nicht einmal 13 Prozent den Krieg. Während der Besatzungszeit wurde von den US-Truppen ein Spionagezentrum errichtet. Dabei wurden auch viele ehemalige SS-Leute angeheuert, da man Kenntnisse über die sowjetischen Verhältnisse vermutete. Von dem amerikanischen Wiederaufbauprogramm profitierte auch die Stadt Gmunden.

Im Jahr 2008 fand die dezentrale Landesausstellung unter dem Titel „Das Salzkammergut“ statt. Die Leit- und Überblicksausstellung war im Schloss Ort in Gmunden. Neben Gmunden beteiligten sich elf weitere Gemeinden des Salzkammerguts. Für diesen Zweck wurde das Kammerhofmuseum aus- bzw. umgebaut.

Wegen zu geringer Schülerzahl wurden die bisher eigenständigen Hauptschulen (Hebbel- und Habertschule) ab dem Schuljahr 2007/2008 zusammengelegt.

Für das Jahr 2007 gab es einen Maßnahmenkatalog der Stadtgemeinde. Zu den wichtigen Vorhaben zählten der Ausbau des Stadtzentrums und der Bau des Seehotels Lacus Felix. Mit dem Bau des Hotels sollte im Jahr 2007 begonnen werden, nach der Liquidierung der Hotelgesellschaft wegen finanzieller Probleme wurde das Bauvorhaben jedoch 2014 aufgegeben. 2009 sollte mit der Umsetzung des so genannten „Bahnhofpakets“ begonnen werden, das folgende Vorhaben enthielt: Da der Seebahnhof durch das Hotelprojekt abgerissen wird, soll eine passende Lösung gefunden werden – die wahrscheinlichste Option ist die Errichtung eines neuen Gebäudes im Klosterareal. Der Bahnhof Gmunden soll zudem einen modernen Mittelbahnsteig erhalten, das Bahnhofsgebäude soll renoviert werden. Die Schrankenanlage beim Bahnübergang in der Nähe des Bahnhofs soll durch eine Unterführung entfallen.

Die berühmtesten Sehenswürdigkeiten der Stadt Gmunden sind das Landschloss und Seeschloss Ort. Letzteres war auch Drehort der Fernsehserie "Schlosshotel Orth". Das Seeschloss zählt zu den ältesten Gebäuden des Salzkammergutes (im 10. Jahrhundert erbaut) und wurde 909 und später 1053 erstmals urkundlich erwähnt.

Nicht weit vom Seeschloss entfernt befindet sich die Villa Toscana. Dieses Gebäude wurde zwischen 1870 und 1877 inmitten eines 88.000 m² großen Parks als Domizil der Großherzogin der Toskana, Maria Antonie von Neapel-Sizilien errichtet. Weitere Schlösser in der näheren Umgebung sind das Schloss Cumberland aus dem 19. Jahrhundert und das Schloss Weyer, welches eine Dauerausstellung über Meißner Porzellan beherbergt.

Auf dem Rinnholzplatz steht der "Salzträgerbrunnen", der einzige Keramikbrunnen Österreichs, aus dem Trinkwasser vom „Heiligen Bründl“ fließt.

In der Stadtpfarrkirche steht der Dreikönigsaltar, der vom bayerisch-österreichischen Bildhauer Thomas Schwanthaler um 1678 geschaffen wurde. In der 1636 erbauten Kapuzinerkirche ist das Hochaltarbild "Maria Heimsuchung", 1753 von dem Tiroler Barockmaler Philipp Haller geschaffen, besonders beachtenswert.

In deren Nähe befindet sich auch der "Gmundner Marktplatz". Neben alten Häusern, wie das erste Gmundner Rathaus, welches bis ins Jahr 1301 bestand, befindet sich hier auch der Stadtbrunnen mit dem Stadtwappen. Das Rathaus wurde 1574 von einem italienischen Baumeister erbaut, mit einem Keramikglockenspiel aus dem 16. Jahrhundert, und 1925 einer umfangreichen Renovierung unterzogen. Zur Gründungszeit der Stadt war der Marktplatz das wirtschaftliche und politische Zentrum. In einer vom Marktplatz wegführenden Gasse befinden sich neben der ersten Apotheke des Salzkammergutes auch das Pepöckhaus, in dem sich seit 1988 das Klo & So Sanitärmuseum befindet.

Der 1450 erbaute "Kammerhof" – seinerzeit Sitz der habsburgischen Salzkammer und Repräsentanz des Kaisers: Gmunden war über einige Jahrhunderte „Hauptstadt“ des Salzkammerguts – bietet architektonisch sowohl historische als auch moderne Elemente. Im Kammerhof befindet sich neben dem "Stadtmuseum" auch das "Brahmsmuseum": Johannes Brahms verbrachte die Sommermonate öfters bei dem Wiener Industriellen Viktor von Miller zu Aichholz in dessen Gmundner Villa, nach dem Tode von Brahms im Jahre 1897 gründete jener im Jahre 1900 in Gmunden das erste Brahmsmuseum der Welt.

Neben dem Kammerhof befindet sich eine "Schiffsanlegestelle" der Traunseeschiffahrt. Hier ist der berühmte Raddampfer "Gisela" (Baujahr 1871) abgestellt. Das Schiff wurde nach der Tochter Kaiser Franz Josephs, Gisela Louise Marie von Österreich, benannt.

Eine weitere Sehenswürdigkeit ist die von Stern & Hafferl erbaute Gmundner Straßenbahn. Sie verbindet das Stadtzentrum mit dem Hauptbahnhof und ist die älteste, kürzeste und – mit einer Steigung von 10 % – die steilste Straßenbahn Österreichs.

In der Gmundner-Keramik-Manufaktur kann bei Werksführungen zugesehen werden, wie das typische „Grüngeflammte“ entsteht. Der Einfluss der Gmundner-Keramik ist so stark, dass sich Gmunden „Keramikstadt“ nennt.

Freisitz Weinberg im Stadtteil Traundorf.

Die Stadt ist heute Mitglied im Verband "Kleine historische Städte".

Zur Förderung der Koordination im Kulturbereich wurde im Mai 1973 ein eigenes Kulturamt in Gmunden gegründet. Die Stadt leistet jährlich Beiträge zur Förderung von Musik- und Denkmalpflege, Theaterwesen und an das Museum.

Gmunden hat eine vielfältige Infrastruktur für kulturelle Angebote. Hier zu nennen sind:





Im Gemeindegebiet befindet sich seit 1963 ein Naturschutzgebiet, das etwa 145 Hektar umfasst und sich im Besitz der Österreichischen Bundesforste befindet. Das Gebiet erstreckt sich vom Gschliefgraben im Norden über die Hohe Scharte im Osten bis zum Lainautal im Süden. Im Westen ist das Gebiet durch den Traunsee begrenzt. Hier im Übergangsbereich von Kalkalpen und Alpenvorland sowie durch die großen Höhenunterschiede von Traunsee mit 430 Meter bis zum Gipfel des Traunstein mit 1691 Meter, herrscht eine große Vielfalt an Biotopen. So finden sich hier Fichten-Tannen-Buchenwälder, Krummholzgürtel, Alpine Rasen sowie Schuttgesellschaften. Im Bereich des Laudachsees existiert ein Latschenhochmoor sowie ein Verlandungsmoor mit einem bekannten Vorkommen an Feuer- und Alpensalamander.

Im 19. und 20. Jahrhundert pflegten viele Komponisten und Schriftsteller rege Beziehungen zur Stadt Gmunden. Die berühmtesten waren die Komponisten Johannes Brahms und Anton Bruckner.
Zu den Ehrenbürgern der Stadt zählen neben Feldherrn und Politikern auch Unternehmer.



Die offiziellen Gemeindenfarben sind "Rot-Blau-Gelb".
Blasonierung: „Viermal geteilt, in Rot drei goldene Küflen, die mittlere größer, in Silber eine nach links gerichtete beladene goldene Salzzille, in Blau ein nach links schwimmender silberner Fisch, in Gold drei fächerförmig angeordnete grüne Seeblätter und in Rot ein felsiger silberner Dreiberg.“

Im Jahre 1593 verlieh Kaiser Rudolf II. der Stadt Gmunden ein „vermehrtes und gebessertes Wappen“, das bis heute nicht verändert wurde. Die Küflen und Salzzille weisen auf die ehemalige Abfertigung und Verfrachtung von Salzes hin, der schwimmende Fisch auf blauem Untergrund stellt einen Saibling dar, der auf das Fischereigewerbe hindeutet. Die Seeblätter und der schroffe Berg charakterisieren die Lage der Stadt am See und dem nahen Gebirge.

Der Gemeinderat besteht aus 37 Mitgliedern. Bei der letzten Gemeinderatswahl im September 2015 ergab sich folgende Mandatsaufteilung:

Bürgermeister ist Stefan Krapf von der ÖVP. Der Stadtrat besteht einschließlich des Bürgermeisters aus neun Mitgliedern, von denen sechs die ÖVP stellt und jeweils einen Stadtrat die SPÖ, die FPÖ und die BIG.

Die Stadt Gmunden unterhält enge partnerschaftliche Beziehungen zu der norddeutschen Stadt Tornesch in Schleswig-Holstein und der norditalienischen Stadt Faenza in der Provinz Ravenna.

Seit 27. März 2006 ist Gmunden eine offizielle Klimabündnisgemeinde. Im kommenden Jahr werden Sitzungen stattfinden, wo sich die verschiedenen Spezialisten der Stadt über das weitere Vorgehen zur Einhaltung dieser Ziele beraten werden.

Die Stadt Gmunden besitzt seit 17. Mai 1862 den Status eines Luftkurortes. Um diesen beibehalten zu können, werden alle zehn Jahre Kontrollen durchgeführt (Luftverschmutzung, Sonneneinwirkung, Niederschlag usw.).

Die Einwohner Gmundens sind gegenüber dem Bundesländerschnitt deutlich älter. So sind in Gmunden 15,6 % der Einwohner jünger als 15 Jahre ("Oberösterreich: 18,2 %") und 59,6 % zwischen 15 und 59 Jahre alt ("OÖ: 61,6 %"). Der Anteil der Einwohner mit mehr als 59 Jahren zeigt mit 24,8 % die stärkste Abweichung ("OÖ: 20,2 %").

88,4 % der Bevölkerung besitzen die österreichische Staatsbürgerschaft, 1,5 % sind weitere EU-Staatsbürger, 10,2 % andere Ausländer. Den stärksten Ausländeranteil stellten im Jahr 2001 Bürger von Bosnien-Herzegowina (3,6 %) und der ehemaligen Bundesrepublik Jugoslawien (2,7 %), gefolgt von Türken (1,2 %) und Deutschen (1,1 %).

Zur römisch-katholischen Kirche bekannten sich 2001 69,3 % der Einwohner. Des Weiteren befindet sich in Gmunden eine größere Gemeinde der evangelischen Kirche A.B., der sich 7,3 % der Bevölkerung zugehörig fühlen, sowie eine Mennonitische Freikirche mit etwa 100 wöchentlichen Gottesdienstbesuchern (www.mf-gmunden.at). 5,9 % der Bevölkerung sind islamischen Glaubens und 3,3 % orthodox. 10,3 % sind ohne religiöses Bekenntnis.


Im Laufe der Geschichte hat sich das wirtschaftliche Zentrum erst vom Markt- zum Rathausplatz, dann zum Rande der Stadt, dem Salzkammergut Einkaufspark (SEP), verlagert. Auf Grund der mangelnden Fläche haben sich nur wenige größere Unternehmen in der Stadt angesiedelt. Die allmähliche Ansiedlung von Geschäften der großen Handelsketten führte zum Aussterben aller Krämerläden in der Innenstadt. Der Bau des Salzkammergut Einkaufsparks im Jahre 1975 und dessen weiterer Ausbau im Jahre 2005 führte zu weiteren Geschäftsschließungen. Die Stadtpolitik versucht durch Parkgebührenbefreiung und das Ambiente, die Kaufmannschaft durch gemeinsame Veranstaltungen die Innenstadt wiederzubeleben. Jetzt gibt es Pläne, die Häuser der Innenstadt zu renovieren, und der Bau eines Hotels ist geplant.

Zu den wichtigsten Gmundner Unternehmen gehören die Stern & Hafferl Verkehrsgesellschaft m.b.H., welche Bahnen, Busse und das Reisebüro Stern Reisen betreibt, die Gmundner Zementwerke, die Energie-AG Oberösterreich, die Gmundner-Keramik-Manufaktur, GEG Elektrobau, Laufen Austria, die Gmundner Molkerei und die Stern & Hafferl Baugesellschaft, ein Spezialist für Renovierungen alter Häuser.



1993 wurde der „Verkehrsverbund Gmunden“ (VVG) gegründet, zu dem neben der Straßenbahn Gmunden die City-Buslinien 1 und 2 gehören. Später schloss sich dieser „kleine“ Verkehrsverbund dem OÖVV (Oberösterreichischer Verkehrsverbund) an. Dadurch wurden weitere Möglichkeiten der Förderung durch das Land Oberösterreich gefunden und die City-Buslinie 3 eingerichtet. Außerdem wird die Straßenbahn Gmunden von 2014 bis 2017 verlängert und mit der Traunseebahn verbunden, so hält Gmunden an der Straßenbahn fest. Aufgrund der Straßenbahnverlängerung wird auch der Gmundner Bahnhof neu gebaut.

Der City-Busverkehr wird von Stern & Hafferl Verkehrs-GmbH und der ÖBB-Postbus GmbH gemeinsam betrieben. Des Weiteren gibt es von Stern & Hafferl, Postbus und Sklona einige Regio-Linien, die Gmunden mit seinen Nachbargemeinden und auch darüber hinaus verbinden. Gmunden verfügt über einen eigenen Bahnhof, der von ÖBB-Personenzügen bedient wird. Außerdem spielt der Güterverkehr mit der Firma Hatschek eine bedeutende Rolle, da diese eines der größten Zementwerke in der Region besitzt.

Zudem besteht über die Traunseebahn eine Verbindung nach Vorchdorf und weiter mit der Lokalbahn Lambach-Vorchdorf nach Lambach mit Anschluss an die ÖBB.

Im historischen Längsschnitt betrachtet, beherbergte Gmunden fünf Bahnhöfe. Heute bestehen neben dem Hauptbahnhof noch der Personenbahnhof, der Bahnhof der Pferdeeisenbahn in der Anna-Straße (heute Wohnhaus), der Seebahnhof und der Bahnhof Engelhof. Der „Traundorfer-Bahnhof“ in der Schlagenstraße wurde Ende der 1970er Jahre abgerissen, 2013 der historische Seebahnhof. Dafür soll Gmunden als Visionärsstadt die Durchbindung von der Straßenbahn zur Regionalbahn bekommen, um ein weiteres attraktives Angebot für Fahrgäste zu schaffen. Das bedeutet eine direkte Linie von Vorchdorf über das Gmundner Zentrum zum Bahnhof.

Am 12. März 2016 wurde die RegioTram Vorchdorf–Gmunden samt neuer Remise und Werkstätte in Vorchdorf mit einem Fest eröffnet.

Alle "Parkplätze" in der Innenstadt sind gebührenpflichtig, außerhalb des Zentrums nicht. Wegen des herrschenden Parkplatzmangels wurde eine Tiefgarage errichtet. Die Stadtgemeinde möchte diese, wie Ende September bekannt wurde, kaufen, um für die Landesausstellung 2008 gewappnet zu sein.

Im Zuge des Abrisses und Neubaus der Traunbrücke (nächst Ausmündung aus dem Traunsee) wurde zur vorübergehenden Nutzung eine Ersatzbrücke wenige Meter daneben errichtet, diese muss ab 13. November 2017 für eine Woche gesperrt werden, wodurch großräumige Umfahrungen notwendig werden.




</doc>
<doc id="10121" url="https://de.wikipedia.org/wiki?curid=10121" title="Heldenplatz">
Heldenplatz

Der Heldenplatz ist ein historischer Platz in Wien, der zum Gelände der Hofburg gehört. Der Bundespräsident residiert im angrenzenden Teil der Hofburg, der Bundeskanzler am mit dem Heldenplatz verbundenen "Ballhausplatz".

An der Stelle des heutigen Heldenplatzes befand sich ursprünglich die Burgbastei als Teil der Wiener Stadtmauern. Im Rahmen des Fünften Koalitionskriegs besetzten 1809 die Truppen Napoleons die Stadt Wien und sprengten in der Folge Teile der Stadtbefestigung, darunter die Burgbastei. Die Reste wurden erst 1817–1819 entfernt, wodurch der „äußere Burgplatz“ entstand. (Im Rahmen von Bauarbeiten wurden 2016 die Fundamente der Burgbastei ergraben.) Der neu entstandene Platz hieß damals volkstümlich "Promenadeplatz" und offiziell "Neuer Paradeplatz".

Im Zuge der Errichtung der Ringstraßenzone wurde der der „äußere Burgplatz“ unter Kaiser Franz Joseph I. als Teil des "Kaiserforums" konzipiert, das jedoch nie vollendet wurde. Geplant war, einen rechteckigen Platz zu schaffen, der durch den Leopoldinischen Trakt, die Neue Hofburg, die Habsburgischen Sammlungen (heutiges Kunsthistorisches und Naturhistorisches Museum) sowie von einem nie gebauten Pendant der Neuen Burg an der Stelle des Volksgartens eingerahmt werden sollte. Der Platz zwischen den Museen ist heute als "Maria-Theresien-Platz" bekannt, weil dort das Maria-Theresien-Denkmal von Kaspar von Zumbusch steht. Die unverbaut gebliebene Nordwestseite bietet einen umfassenden Rundblick über die westliche Ringstraße mit Parlament, Wiener Rathaus und Burgtheater.

Der Name "Heldenplatz" wurde 1878 festgelegt; er kommt von den beiden Reiterdenkmälern, die beide von Anton Dominik Fernkorn (mit den Aufbauten von Eduard van der Nüll) stammen. Das ältere ist dasjenige Erzherzog Karls, das 1860 enthüllt wurde. Es diente zur militärischen Glorifizierung der Dynastie, zumal einer ihrer Angehörigen hier als Feldherr gefeiert wird. Kurz vor der Enthüllung des Denkmals unterlag Österreich Italien allerdings in der Schlacht von Solferino. Das Denkmal ist insofern bemerkenswert, als das Pferd im Aufspringen dargestellt ist, so dass nur die beiden Hinterbeine den Sockel berühren. Inspiriert ist das Denkmal vom bekannten Schlachtenbild Johann Peter Kraffts. 1865 wurde auch das Pendant fertiggestellt, die Reiterstatue Prinz Eugens, die kurz vor der Niederlage von Königgrätz enthüllt wurde. Nach Schlaganfällen Fernkorns wurde sie von Schülern vollendet. Die technische Perfektion des Erzherzog-Karl-Denkmals wurde nicht mehr erreicht, hier berührt auch der Schweif des Pferdes den Sockel.

Den Abschluss des Heldenplatzes bildet das "Äußere Burgtor", das 1824 von Peter von Nobile gestaltet wurde und als Denkmal für die Soldaten der Napoleonischen Kriege gedacht war. Heute dient es allgemein als Denkmal für Kriegsgefallene. Das Innere des Baus ist nur im Rahmen staatlicher Zeremonien zugänglich. Vom Heldenplatz aus gesehen auf der rechten Seite des Burgtors befindet sich das Denkmal der Exekutive, ein Denkmal für die im Dienst getöteten Polizisten und Gendarmen Österreichs. Die künstlerische Gestaltung übernahm Florian Schaumberger, das Denkmal wurde am 2. Mai 2002 enthüllt.

Mit dem Heldenplatz ist der "Ballhausplatz" verbunden, er befindet sich zwischen Leopoldinischem Trakt und Bundeskanzleramt. Letzteres stammt von Johann Lucas von Hildebrandt und ist seit dem späten 18. Jahrhundert ein zentraler politischer Ort.

Am 15. März 1938 verkündete Adolf Hitler vom Balkon der Neuen Burg aus den versammelten Massen auf dem Heldenplatz den „Anschluss Österreichs“ an das Deutsche Reich. Der Titel des Theaterstücks "Heldenplatz" von Thomas Bernhard bezieht sich auf dieses Ereignis. Ebenso das berühmte Gedicht "" des Wiener Lyrikers Ernst Jandl, das die Stimmung während der Hitler-Rede am Heldenplatz einzufangen versucht.

Seit Jahrzehnten findet auf dem Heldenplatz anlässlich des österreichischen Nationalfeiertags am 25. und 26. Oktober eine Veranstaltung des Bundesheers statt. Am 26. Oktober findet die traditionelle Angelobung unter Führung des Bundespräsidenten statt. 

Nachdem 2012 im Denkmal des toten Soldaten ein verstecktes NS-Pamphlet gefunden und Namen von SS-Leuten in den Totenbüchern entdeckt wurden, ließ das zuständige Bundesheer die Gedenkräume leeren.

Am Jahrestag der Wiedererrichtung der Republik am 12. März legte Verteidigungsminister Gerald Klug erstmals 2013 zum Gedenken an den Einmarsch Hitlers und den Untergang Österreichs einen Kranz nieder. 

Anlässlich des 68. Jahrestags der Befreiung vom Nationalsozialismus hielt am 8. Mai 2013 das Bundesheer erstmals eine Mahnwache für die Opfer des Nationalsozialismus auf dem Heldenplatz ab. Zusätzlich fand, initiiert vom Mauthausen Komitee Österreich und den Wiener Symphonikern, erstmals ein Fest der Freude mit einem großen Festkonzert der Wiener Symphoniker bei freiem Eintritt auf dem Heldenplatz statt. Diese Veranstaltung findet seither alljährlich statt, wird von rund 15.000 Menschen besucht und seit 2015 auch im Fernsehen übertragen.

Da das Parlamentsgebäude von 2017 bis 2020 renoviert wird, wurden am Heldenplatz zwei provisorische Pavillons errichtet; sie bieten Raum für Büros und Öffentlichkeitsarbeit. Die Aushubarbeiten wurden von Archäologen begleitet, wobei im Sommer 2016 Reste des Burgtores ergraben wurden. Die Gebäude werden 2020 wieder entfernt werden.

Am Heldenplatz fanden in den vergangenen Jahren wiederholt große Veranstaltungen statt. Am 5. Mai 2003 der Massenstart von zehntausenden weißen Luftballons mit Briefen an Opfer des Nationalsozialismus durch A Letter To The Stars, die Leistungsschau des Bundesheers und von Einsatzorganisationen, die Mitmach-Präsentation am Tag des Sports. Im Sommerhalbjahr startet und endet das Friday Night Skating am befestigten Teil des Platzes. In Erinnerung an den Anschluss Österreichs 1938 richtete das Haus der Geschichte Österreich am Heldenplatz die Klanginstallation "The Voices" ein, die von der schottischen Künstlerin Susan Philipsz erarbeitet wurde.




</doc>
<doc id="10123" url="https://de.wikipedia.org/wiki?curid=10123" title="Jahreszeit">
Jahreszeit

Jahreszeiten unterteilen das Jahr in verschiedene Perioden, die durch astronomische Daten – so auch kalendarisch – oder durch charakteristische klimatische Eigenschaften abgegrenzt werden. Im alltäglichen Sprachgebrauch sind damit hauptsächlich meteorologisch deutlich voneinander unterscheidbare Jahresabschnitte gemeint. In gemäßigten Breiten sind dies die vier Jahreszeiten Frühling, Sommer, Herbst und Winter; in den Tropen sind es Trockenzeit und Regenzeit. Die Beschreibung der Jahreszeiten bezieht sich in diesem Artikel auf die Nordhalbkugel der Erde, auf der Südhalbkugel sind sie kalendarisch um ein halbes Jahr versetzt.
"Sommer" und "Winter" können je auch als Hälften eines Jahres verstanden werden, beispielsweise als das Nordsommerhalbjahr beziehungsweise das Südwinterhalbjahr.

Verschiedene Völker unterteilen bzw. unterteilten das Jahr in andere Jahreszeiten. So kennen die Samen in Skandinavien 8 Jahreszeiten, australische Aborigines in Arnhem Land, die Gagudju sprechen, 6 Jahreszeiten. In Russland kennt man die Rasputiza, zwei Schlammzeiten jeweils während der Schneeschmelze im Frühjahr und während der Herbstregenfälle.

Im Laufe eines Jahres ändern sich an einem bestimmten geographischen Ort Intensität, Dauer und Winkel des Sonnenlichteinfalls. Diese Veränderungen wiederholen sich nach einem Umlauf der Erde um die Sonne als jahreszeitliche Schwankungen. Für die auf der Erde erlebbaren Jahreszeiten bestimmend ist die Lage der Rotationsachse der Erde relativ zur Ebene ihrer Umlaufbahn.

Wie bei einem Kreisel behält die Erdachse ihre Orientierung im Raum bei und steht so wegen der Drehimpulserhaltung (nahezu) raumfest in einem bestimmten Winkel zur Ekliptikebene. Diese Neigung der Erdachse ist nicht rechtwinklig, sondern beträgt (derzeit) etwa 66,6°, sodass die Äquatorebene um etwa 23,4° (23° 26') geneigt ist gegenüber der Bahnebene. Deshalb ändert sich während eines Erdumlaufs um die Sonne der Einstrahlwinkel des Sonnenlichts (Sonnenhöhe zu Mittag) im Jahreslauf. Zusätzlich ändert sich durch die Rotation der Erde um sich selbst auch die Dauer des Tageslichts (lichter Tag) als Tageslänge, in den polfernen Regionen außerhalb der Polarkreise je von einer Erdumdrehung zur nächsten. Je länger und je steiler Sonnenlicht auf die Oberfläche trifft, desto mehr kann diese Region dadurch erwärmt werden.

Für die Entstehung von Jahreszeiten ist zunächst entscheidend, wie stark der jeweilige Anteil der Strahlungsleistung der Sonne, den eine bestimmte geografische Region empfängt, im Jahreslauf schwankt. Die auf das Relief der Oberfläche bezogene Bestrahlungsstärke hängt ab vom Einstrahlwinkel und von der Bestrahlungsdauer. Der Einstrahlwinkel erreicht mittags sein tägliches Maxium und diese Sonnenhöhe zu Mittag schwankt für alle Orte außerhalb der Polarregionen im Jahreslauf um ± 23,4°, wobei der durchschnittliche Höhenwinkel zu den Polen hin flacher wird. Die mögliche tägliche Bestrahlungsdauer, der lichte Tag, ist dagegen durchschnittlich gleich lang, doch wächst die jährliche Schwankungsbreite der Tageslänge mit zunehmender geografischer Breite. Da beide Faktoren, Winkel und Dauer der Sonneneinstrahlung, über den Tagbogen zusammenhängen – höchster Sonnenstand und längster Tag fallen zusammen – und ihre Schwankungen sich summieren, hängt die Ausbildung von Jahreszeiten somit vorrangig von der geografischen Breite einer Region ab.

Allein nach dem Breitenkreis lassen sich daher auf den Einfallswinkel des Sonnenlichts bezogene solare Klimazonen unterscheiden. So werden die äquatornahen Tropen als "tropische Zonen" zwischen den Wendekreisen (23,4° Breite) abgegrenzt gegenüber den "ektropischen Zonen" – zu denen dann die Subtropen bzw. Mittelbreiten und (ab etwa 66,6° Breite) die Polarzonen gehören – mit äquatorfern zunehmend stärker ausgeprägten jahreszeitlichen Unterschieden.

Während der Zeit zwischen den Tag-und-Nacht-Gleichen im März und im September ist die Nordhalbkugel stärker der Sonne zugeneigt, so dass die Sonne für einen dort befindlichen Beobachter einen hohen Bogen durchläuft. Bei hochstehender Sonne trifft die Sonnenstrahlung steil auf die Erdoberfläche und liefert so einen relativ hohen Energieeintrag pro Fläche. Weiterhin liegt der größere Teil der täglich durchlaufenen scheinbaren Sonnenbahn um die Erde als Tagesbogen oberhalb des Horizonts, sodass die Tage lang sind und viel Zeit für den Energieeintrag zur Verfügung steht. Der somit erhöhte Energieeintrag bewirkt in diesem Zeitraum eine Erwärmung der nördlichen Hemisphäre (Nordhalbkugel).

Befindet sich die Erde ein halbes Jahr später am gegenüberliegenden Punkt ihrer Bahn, so ist die Nordhalbkugel der Sonne abgeneigt – infolge der abgesehen von Präzession und Nutation relativ raumfesten Lage der Erdachse. Für einen Beobachter auf der Nordhalbkugel ergibt sich dann eine niedrig verlaufende tägliche Sonnenbahn. Steht die Sonne tief, so trifft die Sonnenstrahlung flacher auf die Erdoberfläche, so dass sie sich auf eine größere Fläche verteilt und weniger Energie einträgt. Außerdem liegt nur der kleinere Teil der täglichen Sonnenbahn oberhalb des Horizonts, so dass der Energieeintrag nur für eine kurze Zeitspanne erfolgen kann. Die Folge ist eine Abkühlung der nördlichen Erdhalbkugel.

Erwärmung und Abkühlung zeigen sich zunächst in den Lufttemperaturen (siehe Abbildung); wegen der thermischen Trägheit folgen die Bodentemperaturen den Höchst- und Tiefstständen der Sonne mit einer gewissen Verzögerung. Die Unterschiede im Tagesbogen des Sonnenlaufes nehmen mit höherer geografischer Breite zu und haben zunehmend stärkere Auswirkungen (bis hin zur Polarnacht), zum Äquator zu werden die jahreszeitlichen Schwankungen geringer.

Frühlings-, Sommer-, Herbst- und Winterlandschaft in der Eifel bei Monschau:

Auf der Südhalbkugel der Erde herrschen jeweils denen der Nordhalbkugel entgegengesetzte Jahreszeiten: Ist es im Süden Sommer, so im Norden Winter, und umgekehrt. In tropischen und subtropischen Gebieten unterscheidet man stattdessen zwischen Regen- und Trockenzeiten. In den Tropen gibt es zwei Regenzeiten pro Jahr, die sich jedoch mit zunehmender geografischer Breite zu einer einzelnen zweigipfeligen und schließlich zu einer eingipfeligen Regenzeit verbinden.

Zwar läuft die Erde auf einer Bahn um die Sonne, die elliptisch und nicht kreisförmig ist, sodass die Distanz zur Sonne variiert, doch betreffen die dadurch entstehenden Unterschiede allein die Lichtintensität und sind wegen der geringen Bahnexzentrizität der Erdbahn nicht groß. Den sonnenfernsten Bahnpunkt durchläuft die Erde in der ersten Juliwoche, im Sommer auf der Nordhalbkugel. Die im Jahresverlauf wegen des exzentrischen Umlaufes leicht veränderte Entfernung der Erde von der Sonne ist also nicht die Ursache für die Jahreszeiten. Diese Änderung zwischen Perihel- und Apheldurchgang macht lediglich gegenwärtig die Südwinter etwas strenger und die Nordwinter etwas milder (kürzer und sonnennäher) als sie es bei einer kreisförmigen Erdbahn wären. Unter den derzeitigen Umständen ist die Erde im Nordwinter an ihrem sonnennächsten Punkt (Perihel) – um den 3. Januar, rund 147,1 Mio. km; im Südwinter ist sie am weitesten von der Sonne entfernt (Aphel) – um den 5. Juli, rund 152,1 Mio. km. Der Grund für die Jahreszeiten auf der Erde liegt – wie oben bereits ausgeführt – im Winkel und der Dauer der Sonneneinstrahlung. Für Mitteleuropa betragen die Extrema der Winkel im Sommer 60° bis 65° und die der möglichen Sonnenscheindauer in Mitteldeutschland 16–17 Stunden, im Winter sind es hier hingegen 7–8 Stunden beziehungsweise Winkel von nur 13° bis 18°.

Aufgrund der gravitativen Einwirkung hauptsächlich von Mond und Sonne auf den rotierenden Erdkörper führt die Erdachse eine Präzessionsbewegung aus, so dass sich die Lage der Bezugspunkte von Sonnenwenden und Tagundnachtgleichen allmählich verschiebt und in etwa 26.000 Jahren einmal rückläufig (retrograd) rund um die Erdbahn wandert (Zyklus der Präzession). Das auf den Frühlingspunkt bezogene tropische Jahr dauert daher rund 20 Minuten kürzer als ein auf den Fixsternhintergrund bezogener vollständiger Umlauf der Erde um die Sonne, ein siderisches Jahr. An der Länge des tropischen Jahrs orientiert sich das heutige bürgerliche Jahr der kalendarischen Zeitrechnung. Der tropischen Jahreslänge wird das Kalenderjahr im gregorianischen Kalender durch Einfügen von Schalttagen angenähert, womit sich für kalendarische Angaben des Anfangs von Jahreszeiten typische Verschiebungen ergeben, beispielsweise für den Herbstanfang.

Außerdem dreht sich infolge von Bahnstörungen durch andere Planeten die Apsidenlinie (Gerade durch Aphel und Perihel) in gut 111.000 Jahren einmal rechtläufig (prograd). Wegen dieser einander gegenläufigen Bewegungen läuft das Perihel in rund 21.000 Jahren einmal durch alle Jahreszeiten. In etwa zehntausend Jahren wird der sonnennächste Bahnpunkt mit der nördlichen Sommersonnenwende zusammenfallen. Die winterlichen Jahreszeiten der Nordhalbkugel werden dann länger und sonnenferner stattfinden als heute. Im Gegenzug wird die Südhalbkugel kürzere und sonnennähere Winter bekommen.

Astronomisch werden die Jahreszeiten nach der scheinbaren geozentrischen ekliptikalen Länge des Sonnenstandes bestimmt. Unter Berücksichtigung von Aberration und Nutation wird dafür die scheinbare jährliche Bahn der Sonne von einem hypothetischen Beobachtungsort im Erdmittelpunkt aus betrachtet und in vier Abschnitte unterteilt. Jeder der Bahnabschnitte ist durch je einen Äquinoktial-Punkt (von Tagundnachtgleiche, bei 0° bzw. bei 180°) und je einen Solstitial-Punkt (von Sonnenwende, bei 90° bzw. bei 270°) begrenzt.

Die astronomischen Jahreszeiten sind definiert als jene Zeitspannen, die während des Durchlaufens eines bestimmten der vier Abschnitte jeweils vergehen und dauern wegen der unterschiedlichen Winkelgeschwindigkeit nicht gleich lange. Aufgrund der geozentrischen, auf den Erdmittelpunkt bezogenen Definition beginnt oder endet eine astronomische Jahreszeit standortunabhängig weltweit zum selben Zeitpunkt (dem aber in verschiedenen Zeitzonen unterschiedliche Uhrzeiten entsprechen).


Die Jahreszeitenanfänge sind nicht exakt identisch mit dem Überschreiten des Himmelsäquators oder dem Erreichen der größten Deklination, weil es eigentlich der Schwerpunkt des Erde-Mond-Systems ist, der sich gleichmäßig in der „Erdbahnebene“ um die Sonne bewegt, während die Erde selbst diesen Schwerpunkt umkreist und sich in der Regel etwas oberhalb oder unterhalb dieser Ebene befindet. Vom geozentrischen Beobachter aus gesehen läuft die Sonne daher nicht exakt auf der Ekliptik (sie hat eine ekliptikale Breite ungleich Null). Sie passiert deshalb zum einen nicht exakt durch Frühlings- und Herbstpunkt, zum anderen führt ihre veränderliche ekliptikale Breite dazu, dass die maximale Deklination in der Regel nicht genau an den Sonnwendpunkten angenommen wird.

Die Tabelle listet die astronomischen Jahreszeitenanfänge bis 2025 für die Mitteleuropäische Zeitzone (UTC+1) auf (Schaltjahre hervorgehoben):

Zwischen zwei Frühlingsanfängen liegt im Mittel ein Zeitraum von 365 Tagen 5 Stunden und 49 Minuten (siehe tropisches Jahr). Jeder Frühlingsanfang fällt daher auf eine um knapp 6 Stunden spätere Uhrzeit als der vorhergehende. Diese systematische Drift zeigt sich auch in der Tabelle beim Vergleich der Zeitangaben für aufeinanderfolgende Jahre. Abweichungen der einzelnen Zeitabstände vom Mittelwert werden durch die von anderen Planeten bewirkten Bahnstörungen sowie den bereits erwähnten Unterschied zwischen Erdmittelpunkt und Erde-Mond-Schwerpunkt verursacht (diese Einflüsse werden im Artikel Erdbahn näher erläutert).

Nach vier Jahren hat sich der Frühlingsanfang um knapp 24 Stunden zu späteren Uhrzeiten verschoben. Der julianische Kalender führte nun einen Schalttag ein (in der Tabelle: 2012, 2016, 2020), um den Frühlingsanfang wieder um 24 Stunden auf frühere Zeitpunkte zu verschieben. Da die Korrektur durch den Schalttag 24 Stunden beträgt, der Frühlingsanfang aber erst um "knapp" 24 Stunden verschoben ist (nämlich im Mittel um 4 · 5 h 49 m = 23 h 16 m), hat der Schalttag eine Überkompensation zur Folge, so dass der Frühlingsanfang nach einem Schaltjahrzyklus von vier Jahren im Mittel um etwa 44 Minuten zu "früheren" Zeitpunkten verschoben ist. Auch dies zeigt sich in der Tabelle beim Vergleich zweier Frühlingsanfänge, die um vier Jahre auseinanderliegen. Diese Überkompensation wird im gregorianischen Kalender dadurch korrigiert, dass in drei von vier Hunderterjahren (Säkular-Jahre) der Schalttag ausfällt. Dies war in der Vergangenheit in den Jahren 1700, 1800 und 1900 der Fall.

Da die Schaltregel erst eine gewisse Verschiebung des Frühlingsanfangs auflaufen lässt, bevor sie sie durch Einlegen eines Schalttages wieder korrigiert, schwankt die Uhrzeit des Frühlingsanfangs (und entsprechend die aller anderen Jahreszeitenanfänge) in einem Bereich von etwa 18 Stunden. Meist liegt eine Mitternacht in diesem Bereich, so dass der betreffende Jahreszeitenbeginn im Laufe der Jahre an zwei unterschiedlichen Kalendertagen stattfinden kann. So fällt gegenwärtig der Herbstbeginn in der Mitteleuropäischen Zeitzone auf den 22. oder 23. September. Falls der Schwankungsbereich sich nicht über eine Mitternacht hinweg erstreckt, finden die betreffenden Jahreszeitenanfänge stets am selben Kalenderdatum statt. So fällt gegenwärtig der Sommeranfang in der Mitteleuropäischen Zeitzone (aber nicht in anderen Zeitzonen) stets auf den 21. Juni.

Diese Verhältnisse bleiben aber nicht konstant, da jeder Schalttag eine Überkompensation bewirkt und sich, wie oben erwähnt, die Jahreszeitenanfänge längerfristig langsam zu früheren Kalenderzeitpunkten hin verschieben, bis diese Verschiebung durch die Schaltregel für Hunderterjahre wieder korrigiert wird:


Die astronomischen Jahreszeiten entsprechen jeweils bestimmten Abschnitten der Erdbahn. Da die Erdbahn leicht elliptisch ist, durchläuft die Erde diese Abschnitte mit variabler Geschwindigkeit, so dass die Jahreszeiten nicht alle gleich lang sind.

Gegenwärtig befindet sich die Erde bei Winterbeginn in der Nähe des Perihels und durchläuft daher Herbst und Winter schneller als Frühling und Sommer. Da das Perihel wegen der Präzession und gravitativer Bahnstörungen langsam durch die Jahreszeiten wandert, ändern sich auch die Geschwindigkeiten, mit denen die jeweiligen Jahreszeiten durchlaufen werden.

Die Tabelle zeigt die mittlere Dauer der einzelnen Jahreszeiten in Tagen:

Im Jahr 1246 fielen Perihel und Wintersonnenwende zusammen, der Winter hatte also dieselbe Länge wie der Herbst und der Sommer dieselbe Länge wie der Frühling. Seither ist der Winter die kürzeste Jahreszeit. Er wird seine geringste Länge (88,71 Tage) um das Jahr 3500 erreichen und dann wieder länger werden. Er bleibt die kürzeste Jahreszeit, bis etwa im Jahr 6430 das Perihel mit der Frühjahrs-Tag-und-Nacht-Gleiche zusammenfällt.

Aufgrund der gravitativen Störungen durch die Planeten und den Mond führt die Erdachse eine Präzessionsbewegung aus: ihre Neigung bleibt (im Wesentlichen) konstant; die Richtung, in die sie geneigt ist, schwenkt aber im Verlauf von etwa 26.000 Jahren einmal um 360° herum. Damit ändert sich nichts an der Abfolge der Jahreszeiten, lediglich der Bahnabschnitt, in dem die jeweilige Jahreszeit auftritt, verschiebt sich: die Sommersonnenwende beispielsweise tritt immer dann ein, wenn das nördliche Ende der Erdachse der Sonne exakt zugeneigt ist. Der Einfluss dieser Verschiebung auf Dauer und Strenge der Jahreszeiten wurde bereits erläutert.

Für einen Zeitpunkt in 13.000 Jahren wäre die Erdachse auf obiger Abbildung in allen gezeigten Positionen nach links statt nach rechts geneigt zu zeichnen. Die Erdkugel in der rechten Position hätte dann die "Nord"halbkugel der Sonne zugewandt, es wäre der Zeitpunkt der Sommer- statt der Wintersonnenwende. Der Gregorianische Kalender ist so eingerichtet, dass er diese Verschiebung mitmacht: die mittlere Länge seines Kalenderjahres (365,2425 Tage) entspricht ungefähr der Länge des tropischen Jahres (365,2422 Tage), so dass das Kalenderdatum 21. März stets in der Nähe des astronomischen Frühlingsanfangs fixiert bleibt und auch die anderen Jahreszeitenanfänge entsprechend mitwandern. In jener Bahnposition wäre dann also Juni statt Dezember, wie es für einen Sommeranfang zu erwarten ist.

In der Richtung, in welche die Nachtseite dieser Erdkugel zeigt, liegen das Sternbild Orion und andere charakteristische Wintersternbilder. In 13.000 Jahren wird in diesem Bahnabschnitt Sommer herrschen, und Orion wird ein Sommersternbild sein, allerdings auf der Nordhalbkugel dann deutlich niedriger stehen.

Die meteorologischen Jahreszeiten sind nach Kalendermonaten eingeteilt und umfassen jeweils immer drei komplette Monate. Sie werden damit insgesamt etwa drei Wochen früher angesetzt als die astronomischen Jahreszeiten. Mit der meteorologischen Definition fallen die auf der Nordhalbkugel wie in Deutschland durchschnittlich wärmsten Monate Juni, Juli und August in den Sommer und die kältesten Monate Dezember, Januar und Februar in den Winter (siehe Abbildung oben, jahreszeitlicher Temperaturverlauf (NH)). Hinzu kommt, dass zumindest in der Zeit vor Erfindung des Computers statistische Auswertungen so einfacher durchzuführen waren.

Um den im Jahresablauf zu beobachtenden saisonalen Entwicklungsstand der Natur durch Jahreszeiten zu beschreiben, ist die Unterteilung in vier Jahreszeiten im Allgemeinen zu grob. In der Phänologie kennt man daher bis zu zehn Jahreszeiten, deren Beginn lokal verschieden durch das Eintreten verschiedener Naturereignisse (zum Beispiel Apfelblüte) gegeben ist.

Der prägende Einfluss, den der Ablauf der Jahreszeiten auf den Lebensrhythmus der Menschen hat, schlägt sich auch sprachlich nieder (Jugend entspricht Frühling, Blüte des Lebens entspricht Sommer, Alter entspricht Herbst).

Im Deutschen nennt man einen Zeitraum, in dem der Lebensrhythmus in einer Gegend erheblich vom Normalen abweicht, die fünfte Jahreszeit.

In osteuropäischen Ländern, in denen das Kontinentalklima herrscht, sind die zwei Jahreszeiten Frühling und Herbst extrem kurz und dafür der Sommer und Winter jeweils wesentlich heißer bzw. kälter.



</doc>
<doc id="10124" url="https://de.wikipedia.org/wiki?curid=10124" title="Saiteninstrument">
Saiteninstrument

Ein Saiteninstrument, auch Chordophon („Saitenklinger“; von griechisch "chordē" „Saite“, "phōnē" „Stimme“, „Klang“), ist ein Musikinstrument, das zur Tonerzeugung eine oder mehrere Saiten verwendet, die zwischen zwei Punkten gespannt sind. In den meisten Fällen wird die Schwingungsenergie auf einen Resonanzkörper übertragen und dort in Schallenergie umgewandelt. Verschiedene Tonhöhen werden entweder durch Abteilen der Saiten oder dadurch erreicht, dass für jeden Ton (mindestens) eine eigene Saite vorhanden ist. In jedem Fall gilt: Je kürzer, je straffer gespannt und je dünner die Saite ist, desto höher wird der Ton. Zu den Saiteninstrumenten gehören Lauteninstrumente (mit Stiellauten und Leiern), Zithern und Harfen.

Idiophone, etwa in Gestalt zusammengeschlagener Steine, gelten als die ältesten Musikinstrumente, die es gibt, seit Menschen ein Gespür für Rhythmus entwickelten. Ihnen folgten Flöten, die vermutlich ab dem Aurignacien (40.000–31.000 BP) gespielt wurden. Saiteninstrumente sind deutlich jünger, wenn als deren Entstehungszeit das Vorkommen von Bögen mit einer Saite ab dem späten Jungpaläolithikum oder dem Mesolithikum angesetzt wird. Ob diese Bögen zuerst als Jagdbögen oder Musikbögen verwendet wurden, ist nicht bekannt. Die ältesten Darstellungen eines Musikbogens finden sich auf 15.000 Jahre alten Höhlenzeichnungen. Der einfache Musikbogen mit angelegtem Resonanzkörper oder der durch den Mundraum des Spielers verstärkte Mundbogen gilt als Urform aller Chordophone. Er besteht aus einem biegsamen Holzstab, um dessen Enden ein Stück Darm oder eine Schnur gezogen ist, die so den Stab zum Bogen spannt. Mit einem kleinen Stab angeschlagen oder mit den Fingern gezupft, werden mehrere Grund- und Obertöne hervorgebracht, die durch einen am Bogen angebrachten Resonanzkörper, etwa eine Kalebasse, verstärkt werden können. Eine konsequente Weiterentwicklung führt von Musikbögen zu Harfen und Leiern mit ebenfalls frei gespannten Saiten. Für Zithern, deren Saiten parallel zu einem Saitenträger verlaufen, ist auch eine Herkunft von idiochorden (Saite und Saitenträger bestehen aus demselben Material) Bambusröhrenzithern denkbar (heutige Beispiele "valiha" und "chigring").

Das älteste erhaltene Saiteninstrument ist mehrsaitig: die 4500 Jahre alte Harfe der Königin Puabi von Ur. Die Sumerer und Ägypter spielten auf Harfen, die Griechen des Altertums kannten mehrere Leiern, die bekanntesten waren Kithara und Lyra. Bogenharfen sind im ägyptischen Alten Reich ab etwa 2500 v. Chr. auf Wandmalereien dargestellt und werden in abgewandelten Formen noch in Afrika mit Schwerpunkt Zentralafrika ("kundi") und Uganda ("ennanga") gespielt. Dagegen sind Bogenharfen in Asien bis auf die burmesische "saung gauk", die ostafghanische "waji" und die zentralindische "bin-baja" praktisch verschwunden. In Indien entwickelten sich aus den Bogenharfen die "vina" genannten Stabzithern und Lauteninstrumente. Die im Mittleren Reich ab dem 16. Jahrhundert v. Chr. hinzugekommene Winkelharfe ist in Afrika bis auf die mauretanische "ardin" verschwunden. Im Vorderen Orient entstanden aus den mesopotamischen Harfen die im Mittelalter bis ins 17. Jahrhundert verbreiteten vertikalen Winkelharfen "tschang" und in Zentralasien die kleineren Harfen mit horizontalem Resonanzkörper („Steppenharfen“).

Die Weiterentwicklung vom einfachen Bogen zum komplexen modernen Saiteninstrument hat die unterschiedlichsten Typen hervorgebracht. Die meisten waren aber nur über eine begrenzte Zeitspanne hinweg in Gebrauch, wie etwa die in der Antike überaus beliebten Leiern. Schon Homer beschreibt die viersaitige Leier Phorminx. Dieses Saiteninstrument bestand aus einem Resonanzkörper aus Holz, Schildkrötenpanzer oder später auch aus Metall, über den mehrere Saiten aufgespannt waren. Die Saiten wurden über Stege (Joche) geführt, wovon das obere Querjoch die Saiten hielt und spannte. Die Saiten verliefen parallel zur Resonanzdecke. Gespielt wurde das Instrument mit den Fingern oder dem Plektrum durch Zupfen oder Anreißen. Mit der zweiten Hand wurden die Saiten gedämpft oder verkürzt und somit die Tonhöhe bestimmt. 

Die historischen Leiern, auch Jochlauten genannt, gehören mit zahlreichen antiken Funden zu den bestdokumentierten Instrumenten des Altertums. Die ältesten Funde weisen nach Mesopotamien ins 3. Jahrtausend v. Chr. Im Verlauf der Geschichte wurde die Leier den Erfordernissen angepasst und unterlag vielfältiger Wandlung. Dabei entstanden Formen mit bis zu 15 Saiten. Etwa seit dem 8. Jahrhundert v. Chr. ist sie auch auf der iberischen Halbinsel bekannt, später ist bei den Kelten eine siebensaitige Leier nachweisbar. Heutige arabische Leiern sind in Ägypten die "tanbura", in Nubien die "kisir", am Roten Meer die "simsimiyya". Zu den Leiern in Äthiopien gehören die "krar" und die "beganna". Die mechanisch betriebene Drehleier hat nur den Namen von den alten Jochlauten, ihre Saitenanordnung entspricht einer Violine.

Saiteninstrumente lassen sich auf verschiedene Weisen systematisch einordnen. Eine gebräuchliche Einteilung erfolgt nach der Hornbostel-Sachs-Systematik. 

Ein weiterer Ansatz ist die Klassifizierung nach der Methode, mit der die Saiten zur Schwingung angeregt werden:





Der Instrumentenbauer unterscheidet unabhängig von der Spielweise zwischen drei Grundformen von Saiteninstrumenten und geht dabei von der Art der Schwingungsübertragung auf den Resonanzkörper aus, was unmittelbare Auswirkungen auf Klangfarbe und -entfaltung hat.




Der Kistenbass gehört nach dieser Einteilung zur Klasse der Harfen, das Hackbrett wird den Zithern zugeordnet, und die Geige ist ein Instrument aus der Gruppe der Lauten.

Diese rein bautechnische Klassifizierung eignet sich nur sehr bedingt für die allgemeine Beschreibung einzelner Instrumente, weil sie dem normalen Verständnis zu widersprechen scheint. Zudem gibt es eine große Anzahl von Mischformen, das Klavier ist also eine Zither ohne Resonanzdecke und die "japanische Harfe" (Koto) bautechnisch gesehen eine Zither, deren Schwingungsübertragung jedoch über bewegliche Stege, aber ohne Hals erfolgt.

Zur Bildung des spezifischen Klangs des Instruments sind wie bei allen akustischen Instrumenten bei den Saiteninstrumenten alle fest verbauten Teile und deren spezifischen Materialeigenschaften (Holzmaserung und Dichte, Legierung der Metallteile, Guss oder geschmiedet, Saitenstärke) in ihrer Gesamtheit beteiligt. Bei Instrumenten, die keinen Resonanzkörper besitzen wie E-Gitarren ohne Resonanzkörper (engl.: solidbody guitars), wird nur ein geringer Teil der Schwingungsenergie in Schall umgewandelt. Damit man in diesen Fällen die Schwingung der Saiten lauter hören kann, werden sie üblicherweise über einen Tonabnehmer abgenommen und elektrisch verstärkt. 




</doc>
<doc id="10125" url="https://de.wikipedia.org/wiki?curid=10125" title="Kabarett">
Kabarett

Das Kabarett (Deutschland: [], []; Österreich: []; Schweiz: []) ist eine Form der Kleinkunst, in der darstellende Kunst (schauspielerische Szenen, Monologe, Dialoge, Pantomime), Lyrik (Gedichte, Balladen) oder Musik oft in Form der Satire oder Polemik miteinander verbunden werden. Kabarett ist in seiner Motivation gesellschaftskritisch, komisch-unterhaltend und/oder künstlerisch-ästhetisch.

Das Wort "Kabarett" stammt vom französischen "cabaret" (Schänke oder Kneipe) und wurde später zu "Kabarett" [] eingedeutscht. Das Wort "Cabaret" selbst wird international und historisch oft gleichbedeutend mit Revue verwendet. In der Schweiz und in Österreich hat sich wie bei vergleichbaren Begriffen die französische Aussprache [] erhalten. Zuweilen wird auf die Bedeutung des Wortes "Kabarett" als einer drehbaren Speiseplatte mit kleinen Fächern oder Schüsselchen hingewiesen.

Die Grenzen zwischen Kabarett zu Comedy und Stand-up-Comedy sind bisweilen fließend; so können Künstler wie zum Beispiel Michael Mittermeier, Django Asül, Josef Hader oder Alfred Dorfer immer wieder nur schwer eindeutig dem Kabarett oder der Comedy zugeordnet werden. Grundsätzlich steht im Kabarett eher eine pointierte Kritik öffentlicher Ereignisse oder Personen aus Politik und Gesellschaft durch den Kabarettisten im Mittelpunkt, während bei Comedy und Stand-up-Comedy oft eher die komische Schilderung von Konflikten mit der eigenen Umwelt im Vordergrund steht.

Die Verbindung vom Schauspiel mit seinen epischen und dramatischen Elementen, Lyrik und Musik kann sehr unterschiedliche Kabarettformen hervorbringen. Häufige Stilelemente sind die Satire und Parodie, die sich dann in einem Sketch ausdrücken, sowie Sarkasmus und Ironie.

Eine Kabarettveranstaltung ist zuweilen in „Nummern“ (Sketches, Lieder, Parodien, Schmähreden) aufgeteilt, die durch Conférencen verbunden werden. Dieses „Nummernkabarett“ wurde in Österreich u. a. von Josef Hader und Alfred Dorfer zu einer zwanglosen poetischen Erzählung weiterentwickelt. Eine Sonderform stellt das „Vortragskabarett“ dar, wobei hier alle Stilelemente eines konventionellen Bühnenvortrags (Beamer, Overheadprojektor, Flipchart usw.) mit schauspielerischen Elementen (Kostüm, Maske, Gesang usw.) verbunden wird.

Kabarett findet traditionell auf einer Bühne vor Publikum und seit den 1960er Jahren in Hörfunk und Fernsehen statt. Beispiele aus dem öffentlich-rechtlichen Rundfunk sind Scheibenwischer, Mitternachtsspitzen und Neues aus der Anstalt sowie dessen Nachfolge-Sendung Die Anstalt.

Anfang der 1880er Jahre wurde in Paris mit dem "cabaret artistique" von Rodolphe Salis das erste Kabarett eröffnet. Am 18. November 1881 erhielt es den Namen "Le Chat Noir" und es sollte Es wurde schnell auch zu einer Stätte, in der die Artisten, damit waren im Paris des 19. Jahrhunderts sämtliche Künstler gemeint, ihre Nummern ausprobierten und sich gegenseitig vorführten, bevor sie dem Publikum vorgestellt wurden.

Zwanzig Jahre später mietete Ernst von Wolzogen in Berlin in der Alexanderstraße 4 die Sezessionsbühne und eröffnete am 18. Januar 1901 die Kleinkunstbühne „Überbrettl“, die jedoch bereits mit Ende der Spielzeit 1902/03 in Liquidation ging.

Etwa zur selben Zeit bildete sich in München die Gruppe Elf Scharfrichter, die ihr Kabarett nach einer Europatournee und anschließender Auflösung der Gruppe 1904 nach Wien brachten. Hier gab es allerdings schon seit 1901 das Kabarett von Felix Salten.

Einer der ersten Kabarettstars in Deutschland war Otto Reutter, dessen Couplets inzwischen 100 Jahre überdauert haben. Die strenge Theaterzensur im Kaiserreich sorgte dafür, dass im Kabarett des beginnenden 20. Jahrhunderts jegliche Form der öffentlichen Kritik verboten war. Mit dem Ende des Ersten Weltkrieges wurde die Zensur auf Theater und Kabarettprogramme aufgehoben, und die Kabarettisten konnten ab 1919 auf die aktuellen politischen Entwicklungen und die soziale Situation der Menschen eingehen. In dieser Zeit blühte das deutsche Kabarett erstmals auf und brachte neben Otto Reutter, der bis 1931 sein Alterswerk schuf, so unterschiedliche Künstler wie Claire Waldoff, Werner Finck (1929–1935 beim Kabarett Die Katakombe), Hans Deppe, Isa Vermehren, Rudolf Platte, Ivo Veit oder Karl Valentin (auch Direktor des Münchener Kabaretts „Wien-München“) hervor. Für das Kabarett schrieben damals angesehene Autoren wie Kurt Tucholsky und Erich Kästner. Klaus Mann verfasste für das Kabarett seiner Schwester Erika Mann, „Die Pfeffermühle“, Couplets und Texte.
Ab der Machtübernahme der NSDAP wurden diese geistvolle Zeitkritik bekämpft und die Akteure verfolgt, mit schwerwiegenden Folgen für das Kabarett in Deutschland: Finck zum Beispiel wurde 1935 verhaftet und im KZ Esterwegen interniert, Tucholsky starb Ende desselben Jahres an einer Überdosis Tabletten. Max Ehrlich wurde am 1. Oktober 1944 in Auschwitz ermordet. Viele der deutschsprachigen Kabarettisten begaben sich ins Exil in die Schweiz, nach Frankreich, Skandinavien oder in die USA. Die Folge war, dass es in Deutschland selbst nur noch das staatlich kontrollierte Kabarett gab, das immer stärker zu einer Bühne für volksdeutsche Witzeerzähler verkam oder das Publikum zum Durchhalten aufforderte. Zeit- oder Systemkritik selbst starb jedoch nicht aus, konnte aber nur noch „unter der Hand“ in privatem Kreis erfolgen.
Ab 1945 sorgten die Besatzungsmächte dafür, den Deutschen die Gräuel der nationalsozialistischen Herrschaft näher zu bringen. Zur „Umerziehung“ gehörte auch das kulturelle Leben. So halfen die Kulturoffiziere der Militärregierungen dabei, Theater und Kabarett in Gang, neue und bisher verbotene Stücke auf die Bühne zu bringen. Fast vergessen ist, dass es unmittelbar nach 1945 auch in der sowjetischen Besatzungszone eine relativ freie Kabarettszene gegeben hat, so in Leipzig das von Ferdinand May 1945 gegründete „Literarische Kabarett“ (ab 1947 „Die Rampe“). Erst Mitte der 1950er Jahre wurden die freien Kabaretts durch das propagandistische Kabarett ersetzt. Im Westen prangerten schnell die „Tol(l)eranten“ in Mainz (mit Hanns Dieter Hüsch), das „Kom(m)ödchen“ in Düsseldorf (mit Kay und Lore Lorentz), „Die Schaubude“ in München (1945–1948 mit Ursula Herking, Bum Krüger, Hellmuth Krüger, Monika Greving, Karl Schönböck, Bruno Hübner) - der ein paar Jahre nach der Schließung die "Kleine Freiheit" und noch später die "„Münchner Lach- und Schießgesellschaft“" (mit Dieter Hildebrandt, Klaus Havenstein, Achim Strietzel, Ursula Herking, Hans Jürgen Diedrich und Sammy Drechsel) folgten - sowie „Die Stachelschweine“ in Berlin (mit Rolf Ulrich, Inge Wolffberg, Günter Pfitzmann, Jo Herbst, Wolfgang Gruner, Achim Strietzel) Themen wie die Bundesregierung des damaligen Kanzlers Konrad Adenauer, den Kalten Krieg und später die Auswüchse des Wirtschaftswunders an.

Die vielfältigen Probleme der ständig gefährdeten „Insel der Freiheit“ West-Berlin wurden vorwiegend in Günter Neumanns (1913–1972) Kabarett „Die Insulaner“ glossiert.

Die Programme dieser Kabarettisten der 1950er Jahre wurden durch die Entdeckung der Satire als kabarettistisches Stilmittel erstmals große Publikumserfolge, die sich auch im jungen deutschen Fernsehen fortsetzten. 1953 wurde in Berlin-Ost „Die Distel“ als erstes staatliches Kabarett der DDR eröffnet – zensiert und ohne staatskritische Themen. Weitere ostdeutsche Kabaretts wie die „Kneifzange“ und die „Leipziger Pfeffermühle“ folgten, hatten aber beim Wortwitz stets auf die besonderen Gäste im Publikum zu achten, die man, so Peter Ensikat, sofort daran erkannte, Das DDR-Kabarett war alsbald in die staatlichen Theater-Strukturen eingebunden und musste den „scharfen Grat zwischen Anpassung und verschlüsselter Kritik“ begehen, wo es „zuweilen virtuos die Quadratur des Kreises seilzutanzen“ hatte, wie Volker Kühn als „graue Eminenz des Kabaretts“ und ausgewiesener Kenner seiner Geschichte feststellte.

In den 1960er Jahren waren es in Westdeutschland vor allem Kabarettisten wie Wolfgang Neuss („Das jüngste Gerücht“, „Neuss Deutschland“ (eine Zeitung), „Die Villon Show“, „Asyl im Domizil“), Heinz Erhardt („Noch'n Gedicht“) oder Werner Finck (Kabarett „Nebelhorn“ in Zürich), die neben den großen Kabarett-Ensembles aus Düsseldorf, München und Berlin dem Zeitgeist ihren Wortwitz entgegensetzten. Ende der 1960er Jahre spaltete die Studentenbewegung Teile des Kabaretts in Deutschland. Künstler wie Hanns Dieter Hüsch wurden ausgepfiffen, weil die Studenten in ihnen Teile des Establishments sahen. Die erstmals 1966 bei den Wühlmäusen zu sehende kabarettistische Großform (Autor dieses Programms war Heio Müller) wurde von weiteren Kabaretts aufgegriffen.

In den 1970er Jahren entwickelten sich weitere neue Formen des Kabaretts wie Dieter Hildebrandts kabarettistische TV-Sendung „Notizen aus der Provinz“ und ab 1977 das Szenekabarett „Die 3 Tornados“. Noch in den ausgehenden 1980er Jahren war politisches Kabarett in der Bundesrepublik angesehener Teil der Gesellschaftskritik und gewann nach der Vereinigung von Bundesrepublik und DDR nochmals kurz an Bedeutung. Neue Künstler wie der Frankfurter Matthias Beltz („Vorläufiges Frankfurter Fronttheater“) oder Mathias Richling setzten Zeichen. In den 1990er Jahren wurde das Kabarett aber gleich von mehreren Seiten verdrängt. Der Comedy-Boom (Comedy ist mit dem Kabarett verwandt, aber meistens nicht politisch), das Privatfernsehen und die damit verbundene Prioritätensetzung der öffentlich-rechtlichen Anstalten sowie ein geringer werdendes Interesse des Publikums sorgten für einen Rückgang von Kabarettprogrammen. In der ARD verblieb einzig die Sendung „Scheibenwischer“ (mit Bruno Jonas und Mathias Richling) im Programm bzw. die Nachfolge-Sendung unter dem Titel „Satire Gipfel“ mit Mathias Richling, bzw. seit 2011 mit Dieter Nuhr unter dem Namen „nuhr im Ersten“. Das ZDF zeigte von Januar 2007 bis Oktober 2013 die Sendung „Neues aus der Anstalt“ mit Urban Priol, Georg Schramm (Folgen 1–36) und Frank-Markus Barwasser (Folgen 37–62). Seit Februar 2014 präsentiert das ZDF die Nachfolge-Sendung „Die Anstalt“ mit Max Uthoff und Claus von Wagner. Seit September 2015 läuft im ZDF zudem die Kabarett-Late-Night-Show „Mann, Sieber!“ mit Christoph Sieber und Tobias Mann. Die dritten Programme senden regelmäßig Kabarett („Quer“, bis 2012 „Ottis Schlachthof“ und seit März 2013 schlachthof im BR, „Mitternachtsspitzen“ im WDR, „Extra 3“ im NDR oder „Richling – Zwerch trifft Fell“ sowie „Spätschicht – Die Comedy Bühne“ im SWR). Im 3sat lief von 2004 bis 2007 die Sendung „alles muss raus“ mit Urban Priol. Aktuelle Ereignisse im Kabarett (News, TV-Sendungen, Radio-Sendungen, Premieren, Veranstaltungsorte, Veranstaltungen…) werden seit dem Jahr 2000 auf der Webseite Kabarett-News.de präsentiert.

Die von der Bundesrepublik Deutschland geförderte Stiftung „Deutsches Kabarettarchiv“ hat ihren Standort in Mainz im historischen „Proviant-Magazin“. Zur Neueröffnung wurde zugleich ein „Walk of Fame des Kabaretts“ zwischen dem Mainzer Forum-Theater „unterhaus“ und dem „Deutschen Kabarettarchiv“ eröffnet: Bronzetafeln mit einem Edelstahlstern, der die Gravur des Namenszuges einer aus der Kabarettgeschichte herausragenden Persönlichkeit enthält. Zu den ersten gehörten Werner Finck, Lore Lorentz, Erich Kästner, Kurt Tucholsky und Klabund.

Zeit-Autor Felix Dachsel hat 2017 kritisiert, dass das Kabarett in Deutschland in jüngerer Zeit stark zum Moralisieren und zur Belehrung neige, und es anders als Late-Night-Shows in den Vereinigten Staaten (Jimmy Kimmel, Stephen Colbert, Jon Stewart) empfindlich an Witz vermissen lasse.

Die Geschichte des Kabaretts in Österreich reicht zurück bis in die letzten Jahrzehnte der Habsburgermonarchie, als Komiker und „Possenreißer“ mit ihren Erzählungen das Publikum zum Lachen brachten. Das erste Kabarett wurde am 16. November 1901 in Wien von Felix Salten eröffnet und hieß „Jung-Wiener Theater zum lieben Augustin“ und war im Theater an der Wien eingerichtet. Es gab allerdings nur sieben Vorstellungen. Erst ab 1906 entstand eine nachhaltige Kabarettszene. In jenem Jahr wurde – abermals im Theater an der Wien – das Kabarett Hölle eröffnet sowie in der Ballgasse das Kabarett Nachtlicht, das 1907 schloss und als Cabaret Fledermaus neu eröffnet wurde. In der „Hölle“ begann auch Fritz Grünbaums Karriere als philosophierender Conférencier. 1912 eröffnete das noch heute bestehende Kabarett Simpl, das als Kellertheater viele Stars dieser Kunst hervorbrachte.

Bis 1938 gedieh diese untrennbar mit der zahlreichen jüdischen Bevölkerung Wiens verbundene Kunstform in hervorragender Weise auf dem vorwiegend bürgerlich-liberalen Publikum.

Dass das österreichische Kabarett nach Nationalsozialismus und Zweitem Weltkrieg und der Vertreibung und Ermordung der Juden wiederauferstehen konnte, lag daran, dass einige der vertriebenen Kabarettisten zurückkehrten, so Karl Farkas 1946, Hermann Leopoldi 1947, Gerhard Bronner 1948, Armin Berg 1949 und Georg Kreisler 1955. Berg und Farkas verhalfen dem Traditions-Kabarett Simpl ab 1949 zu erneutem Erfolg, Bronner und Kreisler gründeten 1955 unter anderem mit Helmut Qualtinger das später so genannte „Namenlose Ensemble“ und Leopoldi tourte durch den gesamten deutschsprachigen Raum. Weitere Kabarettgrößen vor dem Zweiten Weltkrieg waren Heinrich Eisenbach, Alexander Roda Roda, Fritz Grünbaum und Egon Friedell, die bis auf Eisenbach entweder in der Emigration oder in der Zeit des Nationalsozialismus ums Leben kamen.

Eine neue Generation des Kabaretts entstand in den 1970er Jahren aus dem Studentenprotest und der alternativen Szene heraus, darunter Lukas Resetarits und Erwin Steinhauer. Ihnen folgte ab Ende der 1980er Jahre eine Welle neuer Kabarettisten und Kabarettgruppen, die dem Kabarett bis zum heutigen Tage in Österreich einen Stellenwert als Massenmedium zukommen lassen. Zu den bekanntesten Vertretern des österreichischen Kabaretts der Gegenwart zählen Roland Düringer, der als Hauptdarsteller der erfolgreichsten Kabarettfilme agierte und mit seinen Shows auch schon die Wiener Stadthalle füllen konnte, Alfred Dorfer, der vor allem politisches Kabarett betreibt und 2004 bis 2011 die TV-Satireshow Dorfers Donnerstalk leitete, sowie Josef Hader, dessen tragisch-komischer Humor im gesamten deutschsprachigen Raum Kultstatus genießt.

Ebenfalls zählen seit Ende der 1980er Jahre Andreas Vitasek, Reinhard Nowak und Andrea Händler zu den Fixgrößen des österreichischen Kabaretts, die mit Dorfer vor allem in den populären Kabarettfilmen der 1990er Jahre stets Hauptrollen besetzten. Deren gemeinsamer Ursprung ist die Kabarettgruppe Schlabarett. Weitere seit vielen Jahren populäre Kabarettisten und Kabarettgruppen sind Die Hektiker, Gunkl, Stermann & Grissemann, Florian Scheuba, Thomas Maurer, Michael Niavarani, Viktor Gernot, Thomas Stipsits oder seit Ende der 1990er Jahre Alf Poier und seit der Jahrtausendwende maschek., Nadja Maleh, Hosea Ratschiller, Monica Weinzettl und die Science Busters. Unter Newcomer zu reihen sind die Gruppe Vetophil, RaDeschnig, Flüsterzweieck, Aschenbrenner.wunderl sowie Paul Pizzera und Otto Jaus, Nina Hartmann und Lisa Eckhart. Das Kabarett ist heute mehr als je zuvor wesentlicher Bestandteil der österreichischen Kulturlandschaft.


Einige der einflussreichsten und bekanntesten Künstler waren oder sind:

Willy Astor, Django Asül, Frank-Markus Barwasser, Jürgen Becker, Matthias Beltz, Jan Böhmermann, Jochen Busse, HG. Butzko, Matthias Deutschmann, Alfred Dorfer, Matthias Egersdörfer, Werner Finck, Ottfried Fischer, Lisa Fitz, Thomas Freitag, Rainald Grebe, Monika Gruber, Günter Grünwald, Josef Hader, Dieter Hallervorden, Anny Hartmann, Dieter Hildebrandt, Rüdiger Hoffmann, Jörg Hube, Hanns Dieter Hüsch, Gerburg Jahnke, Bruno Jonas, Luise Kinseher, Marc-Uwe Kling, Georg Kreisler, Hellmuth Krüger, Frank Lüdecke, Uwe Lyko, Jochen Malmsheimer, Manfred Maurenbrecher, Rolf Miller, Wolfgang Neuss, Dieter Nuhr, Sissi Perlinger, Volker Pispers, Lisa Politt, Gerhard Polt, Urban Priol, Annette Postel, Sebastian Pufpaff, Arnulf Rating, Andreas Rebers, Hagen Rether, Mathias Richling, Richard Rogler, Helmut Ruge, Helmut Schleich, Wilfried Schmickler, Harald Schmidt, Werner Schneyder, Georg Schramm, Martina Schwarzmann, Horst Schroth, Olaf Schubert, Christoph Sieber, Simone Solga, Uwe Steimle, Ludger Stratmann, Dieter Süverkrüp, Mathias Tretter, Max Uthoff, Henning Venske, Claus von Wagner, Bodo Wartke, Philipp Weber, Sigi Zimmerschied.

Noch umfangreichere Listen finden sich in den Kategorien , und .









</doc>
<doc id="10130" url="https://de.wikipedia.org/wiki?curid=10130" title="Komodowaran">
Komodowaran

Der Komodowaran oder Komododrache ("Varanus komodoensis") ist eine Echse aus der Gattung der Warane ("Varanus"), deren Verbreitungsgebiet auf einige der Kleinen Sundainseln von Indonesien beschränkt ist. Mit Maximallängen von drei Metern bei einem Gewicht von über 70 kg ist er die größte gegenwärtig lebende Echse. Das Beutespektrum der tagaktiven Tiere verändert sich mit dem Alter und zunehmender Körpergröße und reicht von Insekten bis hin zu Säugetieren wie Mähnenhirschen und Wildschweinen. Er ist der einzige Waran, der regelmäßig Beutetiere dieser Größe schlägt. Die Jagd auf große Säugetiere wird durch ein in spezialisierten Drüsen im Unterkiefer produziertes Gift unterstützt, welches unter anderem die Blutgerinnung verringert und einen Schock verursacht. Entflohene Beute kann an diesem Gift auch noch nach Tagen zugrunde gehen. Auch Aas ist ein wichtiger Bestandteil des Nahrungsspektrums.

Die Komodowarane sind in ihrem Verbreitungsgebiet die wichtigste Attraktion für den Tourismus, doch teils unbeliebt bei der ansässigen Bevölkerung, da sie gelegentlich Nutztiere reißen. Der Bestand des Komodowarans wird meist mit etwa 4000 Exemplaren angegeben, könnte mittlerweile jedoch auf unter 3000 gesunken sein. Die Rote Liste gefährdeter Arten der IUCN stuft den Komodowaran als "vulnerable" (gefährdet) ein.

Adulte Komodowarane besitzen eine nahezu einheitlich erdbraune Körperoberseite, eine gräuliche Bauchpartie und hellgelbe Augenlider. Weibchen können an der Schnauze zusätzlich gelbgrün, grünbraun, rosa oder violett gepunktet sein. Jungtiere zeigen hingegen ein braunes Muster mit großen gelben oder orangen Flecken am Rücken und auf der Schnauze. Die Schläfenregion hat bei Jungtieren eine gräuliche, mit weißen Flecken durchsetze Färbung. Die Vorderbeine sind braun mit weißen Flecken, der Bauch ist hellgelb mit einigen großen dunklen Flecken. Die Juvenilzeichnung verliert sich nach und nach mit dem Alter: Subadulte Exemplare haben immer noch eine hellere Färbung auf der Schnauze. Dies gilt besonders für die Weibchen, deren Körper jedoch im subadulten Alter bereits einheitlich braun sind. Die Iris ist bei Jungtieren hellbraun, später braun. Die Zunge hat, unabhängig vom Alter des Tieres, eine gelbe Färbung.

Aufgrund der Größe, des kräftigeren Körperbaus und der verschiedenen Färbung ist die Verwechslung mit einer sympatrisch vorkommenden Unterart des Bindenwarans ("Varanus salvator") ausgeschlossen.

Ausgewachsene Komodowarane sind massig gebaute, große Echsen mit einer arttypisch breiten Schnauze. Jungtiere sind noch recht zierlich gebaut, werden im Verlaufe ihrer Entwicklung aber immer gedrungener, die Hinterbeine stämmiger und kürzer und der schlanke Schwanz proportional zunehmend kürzer und dicker, bis er nur noch etwa die Hälfte der Körperlänge ausmacht. Die Zehen sind mit relativ kurzen, sehr scharfen, nach hinten gekrümmten Krallen versehen. Weitere besondere anatomische Details sind Hautknochenplatten unter den Schuppen der Vorderseite der Beine, am Schwanzansatz, um den Hals und dorsal auf dem Schädel.

Das Gebiss zeigt innerhalb der Art eine recht konstante Anzahl leicht nach hinten gekrümmter, spitz zulaufender Zähne, die bei sehr großen Individuen bis zu 2 cm lang sein können. Der Oberkiefer trägt am vorne liegenden Zwischenkieferbein ("Incisivum") sieben und der nach hinten anschließende paarige Oberkieferknochen ("Maxillare") je 13 Zähne. Der Unterkiefer ("Mandibula") besitzt beidseits zwölf Zähne. Charakteristischerweise weisen die Zähne des Maxillare an ihrem Hinterrand eine Linie mit 14 bis 55 feinen Widerhaken auf.

Die Art zeigt einen geringen Geschlechtsdimorphismus: Männchen werden in der Regel etwas größer und massiger als Weibchen und weisen oft in der zweiten und dritten Schuppenreihe vor der Kloake zwei Einsenkungen auf, die rosettenartig von kleinen Schuppen umgeben sind.

Wildlebende Komodowarane erreichen die Geschlechtsreife geschlechtsunabhängig bei einer Gesamtlänge von etwa 150 cm. Adulte Tiere weisen auf Komodo näherungsweise eine Durchschnitts-Kopf-Rumpf-Länge von 92 cm bei einer Gesamtlänge von zirka 196 cm und einem Durchschnittsgewicht von 23 kg auf, wie aus einer Studie an den größten 15 % von 226 Individuen hervorgeht. Auf beutereichen Inseln erreichen dominante Individuen, darunter insbesondere Männchen, auch regelmäßig Gesamtlängen zwischen 225 und 260 cm. Sporadisch werden von letzteren sogar Gesamtlängen von gegen 300 cm erzielt. Das größte bisher seriös vermessene Männchen stammt von Loh Liang auf Komodo und wies eine Kopf-Rumpf-Länge von 154 cm bei einer Gesamtlänge von 304 cm und ein Gewicht von 81,5 kg auf. Das größte Weibchen stammt aus dem Wae Wuul Reservat auf Flores und besaß eine Kopf-Rumpf-Länge von 135 cm bei einer Gesamtlänge von 267 cm und einem Gewicht von 42 kg. Das längste vermessene Exemplar des Bindenwarans ("Varanus salvator") übertrifft mit 3,21 m die Länge des Komodowarans, jedoch wird der deutlich schwerere und kräftigere Komodowaran allgemein als größte rezente Echse bezeichnet.

Der Komodowaran lebt in Indonesien auf den Kleinen Sundainseln: Komodo, Rinca, Gili Dasami, Gili Motang und in küstennahen Bereichen im Norden und Westen des Westteils der Insel Flores. Laut ansässiger Bevölkerung leben Komodowarane auch im Nordwesten von West-Flores und auch auf Ost-Flores, diese Vorkommen sind jedoch nicht bestätigt.

Auf den gebirgigen Inseln Komodo und Rinca finden sich Komodowarane nahezu überall. Sie bewohnen hier bevorzugt Savannen und saisonal geprägte Monsunwälder, daneben auch Grasland, nahezu immergrüne Wälder und vegetationsreiche Strandabschnitte. Die Populationsdichte ist in den Niederungen am höchsten und wird gegen Meereshöhe deutlich geringer. Vereinzelte Individuen konnten auf Komodo sogar bis in Meereshöhe nachgewiesen werden. Auf Gili Dasami wird die Art vornehmlich in den dort vorherrschenden, saisonal grünen Monsunwäldern gefunden. Auf Gili Motang dominieren ebenfalls Monsunwälder die Landschaft, in den küstennahen Bereichen im Norden und Südwesten lebt der Komodowaran jedoch auch in Savannen. Auf Flores bewohnt die Art ebenfalls verschiedenste Vegetationsformen, darunter insbesondere Savannen, Grasland, verschiedene trockene bis feuchte Monsunwaldtypen und Mangrovenwälder. Der auf dieser Insel ab zirka 250 bis Meereshöhe örtlich auftretende Nebelwald stellt eine natürliche Barriere für den Komodowaran dar. Generell sind für Komodowarane Übergangszonen zwischen dichteren und offeneren Landschaftsformen für eine optimale Anpassung an tägliche Klimaschwankungen und die durch die lange Trocken- und die kurze Regenperiode begründeten saisonalen Klimaschwankungen wichtig. Allzu offene, dichte und feuchte Gebiete werden gemieden.

Junge Komodowarane sind hervorragende Kletterer und halten sich nahezu ausschließlich auf Bäumen auf. Hauptgründe sind die Futtersuche, gute Verstecke, Sonnungsplätze und Schutz vor kannibalischen großen Artgenossen. Mit zunehmender Größe wird das Klettern immer langsamer und bedächtiger, jedoch sind Tiere unter 150 cm immer noch fähig, Stämme vertikal bis in 10 m Höhe zu erklimmen. Mit einer Größe um die 100 cm breiten sie ihren Aktivitätsraum immer mehr auch auf den Boden aus und gehen ab einer Gesamtlänge von zirka 150 cm zu einer bodenbewohnenden Lebensweise über. Ausgewachsene Komodowarane sind ausschließlich bodenbewohnend, da ihre Körpergröße und Masse es ihnen nicht mehr erlaubt, zu klettern. Komodowarane sind sehr aktiv. Abgesehen von Verdauungspausen legen große Individuen täglich durchschnittlich etwa 450 bis 960 m, teilweise sogar bis zu 5,5 km in einem etwa 4,8 km/h schnellen Trott zurück. Bewegungen des Komodowarans werden stark von der Suche nach Partnern in der Paarungszeit sowie von Aas beeinflusst. Bei Gefahr können sie zudem ziemlich unabhängig von ihrer Körpergröße mit 14 bis 18,5 km/h schnell rennen und diese Geschwindigkeit im Notfall für mehr als einen halben Kilometer aufrechterhalten. Sie sind mittelmäßige Schwimmer, die sich mit angeschmiegten Beinen unter Wasser mit wellenförmigen Körperbewegungen fortbewegen und teilweise bis 4 m tief tauchen können. Anders als etliche Warane schwimmen sie jedoch nur selten, meist nur über kurze Distanzen, maximal etwa 450 m weit zu dem Festland vorgelagerten kleinen Inseln.

Die Art ist von Sonnenaufgang (um 6 Uhr) bis Sonnenuntergang (um 19 Uhr) tagaktiv. Dieses Aktivitätsmuster bleibt in seinen Grundzügen das ganze Jahr über gleich, zumal Komodowarane keine saisonale Ruheperiode halten. In der Regenzeit von Januar bis März konzentriert sich die Aktivität auf die wärmsten Tagesstunden um die Mittagszeit. Im trockeneren Rest des Jahres wärmen sich die Tiere morgens in offenem Gelände von etwa 28 °C auf 40 °C auf, ruhen um die heiße Mittagszeit in kühlen Verstecken und zeigen ab 13:30 Uhr verstärkte Bewegungsaktivität in bewaldeten Gebieten, gegen 17 Uhr auch wieder in offenerem Gelände. Als Unterschlupf während der heißesten Tagesstunden und in der Nacht nutzen adulte Komodowarane bevorzugt einen ihrer zahlreichen, 0,75 bis 3,2 m langen und mit den Vorderbeinen selbstgegrabenen Bauten, daneben auch Höhlen von Säugetieren oder dichte Vegetation. Jungtiere verstecken sich in Astlöchern, Baumhöhlen oder unter loser Rinde.

Komodowarane sind opportunistische Jäger, die ihre Beute züngelnd orten und sich aus der Nähe auf ihren Sehsinn verlassen. Jungtiere ernähren sich primär von kleinen Echsen und Insekten, welche sie aktiv unter Baumrinde, seltener auch im Gras aufspüren. Komodowarane bis zu einer Gesamtlänge von etwa 100 cm sind ebenfalls noch aktive Jäger, die ihre Beute teilweise für bis zu 15 Sekunden verfolgen. Sie fressen bevorzugt Nagetiere, welche sie aus deren Gangsystemen ausgraben, sowie bodenbewohnende Vögel und deren Eier. Adulte Individuen erbeuten hauptsächlich große Wirbeltiere, die bevorzugt halb bis gleich viel, regelmäßig aber auch doppelt so viel wiegen wie der Waran selbst. Die häufigsten Beutetiere sind junge Mähnenhirsche und kleine Wildschweine, daneben auch Makaken, verwilderte Hunde, domestizierte Ziegen, diverse giftige und ungiftige Schlangen, Meeresschildkröten und deren Eier, bodenbewohnende Vögel und deren Eier, Kälber von Wasserbüffeln und Sambar sowie Fohlen verwilderter Hauspferde. Selten werden sogar Beutetiere über 100 kg attackiert, in ungewöhnlichen Fällen sogar bis zu 320 kg schwere Wasserbüffel überwältigt. Des Weiteren werden besonders an Orten mit hoher Populationsdichte auch junge Artgenossen (Kannibalismus) gefressen.

Adulte Komodowarane jagen aus dem Hinterhalt, indem sie sich am Rand von Wildwechseln auf die Lauer legen, die Beute beim Schlafen überraschen oder sich langsam an sie anpirschen. Aus etwa 1 m Distanz wird die Beute durch plötzliches Vorpreschen gepackt. Kleinere Beutetiere werden sodann mit der Schnauze auf dem Boden totgedrückt, totgeschüttelt oder schlicht durch einen einfachen Biss getötet. Große Beute wird am Hinterbein, Gesäß oder Genick gepackt, durch heftiges Ziehen und Schütteln zu Boden gerungen, fixiert, weitere tiefe Wunden gebissen und die Eingeweide herausgerissen, was zum Verbluten führt. Vom Angriff bis zum Tod vergehen dabei meist 2,5 bis 4 Minuten. Einem sehr großen Beutetier werden die Achillessehnen durchgebissen und dieses durch weiteres Beißen und Ziehen nach und nach zu Fall gebracht und überwältigt. Gelegentlich missglücken solche Attacken, sodass die verletzte Beute flieht und teilweise erst Tage später an der Vergiftung stirbt.

Darüber hinaus wird von semiadulten und adulten Komodowaranen wann immer möglich Aas gefressen. Dieses können sie durch Züngeln bereits auf etwa 3 km, unter optimalen Bedingungen sogar schon aus 11 km Entfernung wahrnehmen. Der Waran folgt dann der Geruchsspur bis zum Kadaver. An größerer Beute sammeln sich in seltenen Fällen bis zu 17 Individuen. Während einer solchen Futteraggregation warten große Individuen in einem Radius von etwa 1,5 m um die Nahrungsressource, bis das in der Hierarchie über ihnen stehende Tier gesättigt den Platz verlässt. Kleinere Individuen müssen warten, bis sie zum Zug kommen können.

Wie bei allen Waranen wird die Beute ganz oder in großen Brocken verschluckt. Durch Hin- und Herschleudern und auf den Boden Schlagen werden Knochen disartikuliert, und mit den gesägten Zähnen wird das Fleisch durch ruckartige Körperbewegungen zerrissen. Die Beute wird nahezu vollständig gefressen, bei größeren Kadavern können nur etwa 8 %, bei sehr großen maximal 30 % nicht verwertet werden. Bei einer Mahlzeit können Komodowarane bis über 70 % ihres eigenen Körpergewichtes an Nahrung aufnehmen. Ein 42 kg schweres Tier ist beispielsweise fähig, ein 30 kg schweres Wildschwein innerhalb von 17 Minuten zu zerlegen und nahezu komplett zu fressen. Die Verdauung dauert drei bis sechs Tage, schlecht verdauliche Teile wie Haare, Federn, Klauen, Hufe, Zähne und größere Knochen werden bereits im Vorfeld wieder ausgewürgt. Die Erfolgsquote größerer Komodowarane bei aktiver Jagd und Aassuche ist recht niedrig, weswegen sie nur etwa einmal im Monat zu einer größeren Beute kommen. Jüngere Komodowarane hingegen fressen noch häufiger und in regelmäßigeren Abständen kleinere Mahlzeiten.

Der Komodowaran besitzt im Unterkiefer Giftdrüsen, die aus einem klar abgegrenzten Hohlraum (Lumen) sowie Gängen bestehen, die sich zwischen den Zähnen des Unterkiefers in die Mundhöhle öffnen. Die wirksamen Bestandteile des Gifts sind wie bei allen Reptiliengiften eine Mischung verschiedener Proteine, beim Komodowaran Kallikrein, Phospholipase A (PLA), natriuretische Peptide, Proteine der AVIT-Familie sowie CRISP-Proteine. Das Gift bewirkt Bewusstlosigkeit durch schnellen Blutdruckabfall (arterielle Hypotonie) und übermäßige Schmerzempfindlichkeit (Hyperalgesie) sowie Hemmung der Blutgerinnung (Koagulopathie) mit beschleunigtem Verbluten unter Volumenmangelschock. Das Gift optimiert die Jagd, womöglich können große Beutetiere nur deswegen überwältigt werden. Erste biomechanische Studien am Schädel des Komodowarans deuten nämlich darauf hin, dass der Schädel im Wesentlichen nur auf Zug von vorne gut ausgerichtet ist. Daneben hat der Kiefer mit 39 Newton (N) eine nur geringe Beißkraft (Leistenkrokodil mit vergleichbarer Körpergröße: 252 N). Der initiale giftige Biss kann folglich die fehlende Belastbarkeit des Schädels kompensieren und das Niederringen des Beutetieres vereinfachen. Im Vergleich dazu haben andere große Raubtiere wie Großkatzen für den Angriff auf große Beutetiere sehr viel belastbarere Schädel.

Früher wurde vermutet, dass größere, gebissene Beute nach einer misslungenen Attacke Tage später an einer bakteriellen Sepsis (Blutvergiftung) stirbt. Tatsächlich finden sich im Speichel des Komodowarans verschiedene pathogene Bakterien, die nach einiger Zeit den Tod durch Sepsis verursachen können. Es wird jedoch davon ausgegangen, dass diese Bakterien in der Regel beim Verenden entflohener Beutetiere eine untergeordnete Rolle spielen und der Hauptgrund für das doch eher rasche Sterben der Wirkung des applizierten Gifts zuzuschreiben ist.

Giftdrüsen sind bei den „Echsen“ (Schuppenkriechtiere, exklusive Schlangen (Serpentes)) nicht auf den Komodowaran beschränkt, sondern kommen wahrscheinlich bei allen Waranen sowie allen Mitgliedern des Taxons Toxicofera vor, dem auch die Warane angehören. Wie bei allen Waranen scheint es auch beim Komodowaran wahrscheinlich, dass das Gift mit Bestandteilen wie PLA auch die Verdauung unterstützt.

Komodowarane sind primär Einzelgänger. Dominante Individuen beanspruchen 258 bis 529 Hektar große, über Jahre beibehaltene Territorien, die sie mit Exkrementen markieren. Die Reviere der Warane können in ein kleines Kernareal und ein großes Streifgebiet unterteilt werden. Im Kernareal mit den präferierten Sonnungsstellen, Versteckplätzen und den bedeutendsten Beutepfaden finden etwa die Hälfte aller Aktivitäten statt. Im ebenfalls sehr gut vertrauten Streifgebiet werden in regelmäßigen Abständen auf zahlreichen Routen Brutstellen, Wildtierpfade und Schlafstellen nach potentieller Beute abgesucht. Daneben besitzen dominante Komodowarane noch ein viel größeres, schwieriger zu quantifizierendes Areal, welches sie zum Auffinden von Tierkadavern beanspruchen. Im Gegensatz zu Kernarealen werden Streifgebiete nicht speziell verteidigt und überschneiden sich bei dominanten Männchen im Schnitt zu 35 % mit den Streifgebieten anderer Männchen und bis zu 99 % mit denen von Weibchen. Junge und jüngere adulte Tiere sind Durchzügler, bewegen sich nach eher zufälligen Mustern sehr vorsichtig über weitere Areale und scheinen kein eigenes Revier zu haben. Diese Bewegungen könnten eine wichtige Rolle in der Populationsgenetik spielen.

Die innerartliche Kommunikation der Komodowarane ist für Echsen sehr weit entwickelt. Kommen mehrere Individuen an einer Nahrungsressource zusammen, begegnen sie sich zufällig oder konkurrieren während der Paarungszeit um Fortpflanzungspartner, zeigen sie verschiedenartiges Ausdrucksverhalten, um ein hierarchisches Verhältnis herzustellen. Hierzu dienen vordergründig visuelle Signale. Drohgebärden beinhalten in ansteigenden Stufen Kopfhochstellen, senkrechtes Kehlespreizen, Zischen, Schwanz in Peitschposition bringen, Maulöffnen, auf den Gegner zurennen und Beißen. Unterwerfung wird durch Kopfsenken und hochbeinig in steifem Schritt Weglaufen, Körper auf den Boden pressen und Beine von sich strecken oder durch Wegrennen signalisiert. In der Regel lässt das dominante Tier dem Unterlegenen die Chance zu fliehen. Bei der Versammlung mehrerer Tiere um eine Nahrungsressource werden Artgenossen toleriert, wenn sie sich im Hierarchiesystem einordnen.

Die Balz- und Paarungszeit des Komodowarans reicht von Mai bis August, gelegentlich werden Paarungen aber auch außerhalb der eigentlichen Saison beobachtet. Oft treffen sich die Geschlechter zufällig an einem Kadaver. Die Männchen legen auf der Partnersuche aber auch häufig große Strecken zurück. Ist beim Zusammentreffen mehrerer paarungsbereiter Warane die Hierarchie unter den Männchen noch nicht geregelt, liefern sich diese ritualisierte Kämpfe um die Weibchen, sogenannte „Kommentkämpfe“. Diese verlaufen beim Komodowaran „warantypisch“: Die Kontrahenten richten sich einander zugewandt auf ihren Hinterbeinen auf, stützen sich mit ihrem Schwanz ab, greifen den Oberkörper des Gegners mit den Vorderbeinen und versuchen, den Gegner zu Fall zu bringen. Der Gewinner erhält Zutritt zu einem oder mehreren Weibchen. Weibchen werden umworben, indem sie zuerst von der Schnauzenspitze bis zur Kloake bezüngelt werden. Dann reibt das Männchen seinen Kopf am Weibchen, drückt seine Schnauze auf ihre Schwanzwurzel, kratzt sie mit den Vorderbeinen am Rücken, und schließlich steigt das Männchen zur Paarung auf ihren Rücken. Weibchen zeigen sich während der ersten Phase der Balz oft abwehrend, drohen oder beißen und reißen sich los, weswegen die Paarung meist erst nach mehreren Anläufen gelingt.

Die Weibchen legen in der Trockenzeit, meist im September, Gelege von maximal 33, im Schnitt 18 ledrigen Eiern, die im Schnitt 87 × 56 mm groß sind und 125 g wiegen. Sie werden bevorzugt in bereits von Großfußhühnern angelegte, bis zu 1,5 m hohe Bruthügel vergraben. Daneben werden unter anderem auch selbst ausgehobene Nistmulden verwendet. Häufig werden die Eier eines Geleges auf mehrere Nester verteilt, um die Überlebenschancen vor Nesträubern zu steigern. Weiter wurden Weibchen beobachtet, die in regelmäßigen Abständen ihr Gelege besuchten oder dieses beinahe permanent über drei Monate hinweg bewachten. Die Jungtiere schlüpfen schließlich nach einer sehr langen Inkubationsperiode von etwa acht Monaten am Ende der Regenzeit von März bis April. Sie sind beim Schlupf im Schnitt 42 cm lang und wiegen 100 g. Im ersten Lebensjahr wachsen die Jungtiere beinahe auf das Doppelte ihrer Gesamtlänge heran, danach nimmt die Wachstumsgeschwindigkeit stetig ab. Im Vergleich zu anderen Waranen werden Komodowarane verhältnismäßig spät geschlechtsreif. In Gefangenschaft erreichen sowohl männliche als auch weibliche Tiere die Geschlechtsreife frühestens mit fünf Jahren. Für freilebende Weibchen wird das jeweilige Alter auf etwa neun Jahre und für Männchen auf zehn Jahre geschätzt. Es wird davon ausgegangen, dass ein Tier gewöhnlich mehr als elf Jahre benötigt, um eine Länge über 2 m zu erlangen.

Aus Zoos sind von Komodowaranen bisher mindestens zwei bestätigte Fälle von Parthenogenese bekannt. Als Parthenogenese bezeichnet man die Fähigkeit eines weiblichen Tieres, lebensfähige Junge ohne vorherige Befruchtung durch ein Männchen zur Welt zu bringen. Bei Komodowaranen und anderen Waranen kommt noch die Besonderheit hinzu, dass sich in parthenogenetisch gezeugten Gelegen nur Männchen anstelle der üblichen Weibchen finden. Dies hängt mit dem ZW-System bei Reptilien zusammen: Wenn das Weibchen zwei Z-Chromosomen weitergibt, entwickeln sich Männchen, ein Jungtier mit zwei W-Chromosomen ist nicht entwicklungsfähig.

Größere Komodowarane haben keine Fressfeinde, Jungtiere hingegen werden außer von ihren großen Artgenossen auch von verwilderten Hunden, Zibetkatzen, Wildschweinen, Greifvögeln und Schlangen gejagt. Wildlebende Komodowarane werden vor allem von der Zecke "Amblyomma robinosoni", aber auch von "Aponomma komodoense" und "Amblyomma helvolum" parasitiert. Die Zecken finden sich meist am Rücken und an den Flanken, vor allem über der Lateralfalte und an den Ansätzen der Beine. Die Anzahl von Zecken ist während des Höhepunkts der Trockenzeit am größten und nimmt nach Beginn der Regenzeit stark ab. Als Endoparasiten gelten Amöben der Gattung "Endolimax" und Bandwürmer der Gattungen "Duthiersa" und "Acanthotaenia".

Angaben zum Durchschnitts- und Maximalalter freilebender Individuen sind unbekannt. Die maximale Lebenserwartung wird auf mindestens 30 Jahre geschätzt. Im Taronga Zoo in Sydney wurde ein Exemplar 24 Jahre alt.

Die Erstbeschreibung erfolgte 1912 durch Peter Ouwens, damals Direktor der zoologischen Sammlung von Bogor (Java). Ouwens verwendete die Haut eines adulten Exemplares als Holotypus und veröffentlichte als zusätzlichen Beweis Fotografien. Anhand seiner Hemipenismorphologie wird der Komodowaran innerhalb der Gattung "Varanus" in die Untergattung "Varanus" gestellt. Laut DNA-Analysen ist "Varanus varius" die Schwesterart von "Varanus komodoensis". Die zwei Arten bilden zusammen das Schwestertaxon von "Varanus salvadorii" und alle drei Arten zusammen die "V. varius"-Gruppe innerhalb der Indo-Australischen Warane. Schwestergruppe der "V. varius"-Gruppe sind die kleinwüchsigen Warane der Untergattung "Odatria". Obwohl im Verbreitungsgebiet des Komodowarans merkliche genetische Unterschiede festgestellt wurden, wird aktuell sowohl auf morphologischer als auch auf genetischer Basis keine Unterart anerkannt.

Anfänglich wurde angenommen, die enorme Körpergröße des Komodowarans hätte sich nach Einwanderung einer kleineren Waranart auf die Kleinen Sundainseln aufgrund von Inselgigantismus, vielleicht auch als Anpassung an die Jagd auf die heute ausgestorbenen Zwergelefanten der Gattung "Stegodon", entwickelt. Nach heutigen molekularbiologischen und paläobiogeographischen Erkenntnissen ist der Komodowaran Teil eines Indo-Australischen Kladus großer Warane, der seine enorme Größe bereits im Pliozän von Australien entwickelte und unter anderem auch den größten Waran überhaupt enthielt ("Varanus priscus", starb nach dem Pleistozän aus). Der Komodowaran entwickelte sich im frühen Pliozän in Australien. Dass während der letzten Eiszeiten oft der Meeresspiegel sank, hätte eine weitere Ausbreitung nach Südwesten im indonesischen Archipel begünstigt. Der Komodowaran erreichte vor etwa 900.000 Jahren Flores, Vorkommen zu dieser Zeit werden durch fossile Zähne belegt. Java besiedelte er wahrscheinlich vor 800.000 bis 700.000 Jahren. Er starb schließlich, bis auf den Bereich seiner heutigen Verbreitung, aus. Der Komodowaran ist somit als letzter Überlebender einer australischen Radiation großer Warane anzusehen.

Die IUCN stuft den Komodowaran seit 1996 in der Roten Liste gefährdeter Arten als gefährdet ("vulnerable") ein, 2004 wurde jedoch schon der Vorschlag gemacht, ihn als stark gefährdet ("endangered") auszuweisen. Bestandsschätzungen von 2002 gehen von rund 2000 Exemplaren auf Flores, 1700 Exemplaren auf Komodo, 1300 Exemplaren auf Rinca und je 100 auf Gili Motang und Gili Dasami aus, der Bestand auf Flores könnte jedoch mittlerweile auf 500 Exemplare gesunken sein. Als Hauptbedrohungen für den Komodowaran gelten aktuell Habitatfragmentierung und vor allem der Rückgang der Populationen von Mähnenhirschen, Wildschweinen und Wasserbüffeln, den wichtigsten Beutetieren für große Warane. Verantwortlich für den Rückgang sind Wilderei, Brände (teils durch Menschen verursacht) und Rodung, letztere beide vor allem, um neues Ackerland zu gewinnen. Auf Gili Motang wurden die Auswirkungen des Beutetierrückgangs genauer untersucht: Es konnte festgestellt werden, dass dort die Komodowarane wegen Beutemangel im Vergleich zu anderen Populationen deutlich kleiner blieben und 2004 die Fangrate von Waranen in Fallen mit Ködern 63,56 % niedriger war als 1994. Das Aussterben des Komodowarans auf Padar (ebenfalls Kleine Sundainseln) in den späten 1970ern ist wahrscheinlich auch auf Mangel an größeren Huftieren zurückzuführen. Daneben müssen Komodowarane oft mit verwilderten Haushunden um Aas konkurrieren.

1980 wurde zum Schutz des Komodowarans der Nationalpark Komodo gegründet, später wurden noch im Westen von Flores das Wae Wuul Reserve und im Norden das Wolo Tado Reserve gegründet. Die Tiere sind in Indonesien streng geschützt. Das Washingtoner Artenschutzübereinkommen listet den Komodowaran in Anhang I, somit ist ohne Sondergenehmigungen jeglicher Handel mit lebendigen Komodowaranen oder Körperteilen von ihnen (z. B. Häute) verboten. Im Nationalpark Komodo versuchen Ranger die für den Rückgang von Beutetieren mitverantwortliche Wilderei zu unterbinden. Erhaltungszuchten in Gefangenschaft können ebenfalls dem Erhalt der Art dienen. Die erste Nachzucht in menschlicher Obhut außerhalb des Heimatlandes Indonesien erfolgte 1992 im Smithsonian National Zoological Park. Die erste europäische Nachzucht glückte 2004 in einem Zoo auf Gran Canaria.

Komodowarane meiden die Konfrontation mit Menschen. Jungtiere sind scheu und fliehen bei Annäherung eines Menschen teilweise schon auf 100 m Distanz schnell in ein Versteck. Ältere Tiere ziehen sich erst bei geringerer Distanz in einem langsamen Trott zurück. Werden Komodowarane jedoch in die Enge getrieben, reagieren sie aggressiv, sperren das Maul auf, zischen warnend und bringen ihren Schwanz in Peitschposition. Lässt der Angreifer nicht ab, gehen sie zum Gegenangriff über, indem sie auf den Gegner zulaufen und ihn schließlich zu beißen versuchen.
Es existieren einige Berichte, wonach Komodowarane angeblich Menschen attackiert oder in Einzelfällen sogar getötet und gefressen haben sollen. Von den wenigen seriös nachprüfbaren Berichten haben sich die meisten Attacken als Verteidigungsbisse bei Bedrohung durch den Menschen herausgestellt. Nur wenige Vorfälle sind nachweislich unprovoziert geschehen. Auffenberg (1981) ist der Überzeugung, dass diese Attacken nur auf wenige abnormale Individuen zurückzuführen sind, welche die Scheu vor Menschen verloren haben und sich ungewöhnlich aggressiv verhalten.

In Gefangenschaft können Komodowarane zahm werden, erkennen ihre Pfleger und zeigen zuweilen Spielverhalten. Sie gelten wie alle Warane als vergleichsweise intelligent.

Komodowarane sind bei der einheimischen Bevölkerung teils sehr unbeliebt, weil sie gelegentlich Nutztiere (speziell Ziegen) reißen und zum Trocknen aufgehängten oder ausgelegten Fisch fressen. Komodowarane sollen auch frisch bestattete Tote ausgegraben und deren Leichen gefressen haben. Andererseits sind Komodowarane als große Touristenattraktion eine wichtige Einnahmequelle für die Bewohner der Kleinen Sundainseln. 1995/96 gaben Waran-Touristen geschätzte 1.100.000 $ teils im Nationalpark Komodo selbst, zum allergrößten Teil jedoch in den beiden Orten aus, von denen aus er erreicht werden kann; die Einnahmen durch den Tourismus haben bei der Bevölkerung einen hohen Stellenwert.




</doc>
<doc id="10131" url="https://de.wikipedia.org/wiki?curid=10131" title="Spule">
Spule

Spule (von althochdeutsch "spuola" „abgespaltenes Holzstück“, zum Aufwickeln von Fäden) steht für:



</doc>
<doc id="10133" url="https://de.wikipedia.org/wiki?curid=10133" title="Paradox (Begriffsklärung)">
Paradox (Begriffsklärung)

Paradox steht für:
Siehe auch:


</doc>
<doc id="10134" url="https://de.wikipedia.org/wiki?curid=10134" title="Struktur">
Struktur

Struktur (von lateinisch "strūctūra" ‚Zusammenfügung‘, ‚Bauart‘, ‚Sinngefüge‘) steht für:

in zusammengesetzter Form:

Siehe auch:



</doc>
<doc id="10150" url="https://de.wikipedia.org/wiki?curid=10150" title="Noradrenalin">
Noradrenalin

Noradrenalin oder Norepinephrin (INN) ist ein körpereigener Botenstoff, der als Stresshormon und Neurotransmitter wirkt. Als Körperhormon wird die Substanz im Nebennierenmark gebildet; als Neurotransmitter dagegen im Nervensystem produziert (im Locus caeruleus).

Noradrenalin ist ein Katecholamin und eng mit Adrenalin verwandt. Durch Verengung von Blutgefäßen erhöht es den Blutdruck. Wie die Vorsilbe Nor- anzeigt, trägt Noradrenalin im Vergleich zum Adrenalin keine Methylgruppe (-CH) an seiner Aminogruppe. Daher zeigen Noradrenalin und Adrenalin zum Teil physiologisch unterschiedliche Wirkungen.

Das Noradrenalin wurde 1948 von Peter Holtz entdeckt, der es zu dieser Zeit noch Norepinephrin nannte. Durch diese Entdeckung konnte wenige Zeit später die physiologische Wirkung der beiden Nebennierenhormone (Noradrenalin und Adrenalin) geklärt werden. 1949 führte M. Goldenberg das Noradrenalin zur Therapie bei schwerem Schock ein.
Noradrenalin wird neben dem Adrenalin als Hormon in den Nebennieren produziert und ins Blut abgegeben (Fluchtreflex). Es wirkt vorwiegend an den Arteriolen und führt über Aktivierung von Adrenozeptoren zu einer Engstellung dieser Gefäße und damit zu einer Blutdrucksteigerung.

Die wichtigste Funktion von Noradrenalin ist seine Rolle als Neurotransmitter im Zentralnervensystem und dem sympathischen Nervensystem. Damit unterscheidet sich Noradrenalin vom Adrenalin, welches nur eine untergeordnete Neurotransmitterrolle besitzt.

Noradrenalin wird im peripheren Nervensystem von sympathischen Nervenfasern ausgeschüttet. Es ist eine Überträgersubstanz (Neurotransmitter) der postganglionären Synapsen des sympathischen Nervensystems und entfaltet dort weitgehend die gleiche Wirkung wie Adrenalin. Die Eliminierung des Noradrenalins aus dem synaptischen Spalt erfolgt hauptsächlich durch Wiederaufnahme in die präsynaptische Zelle über den Transporter, kann aber auch enzymatisch inaktiviert werden. Noradrenalin-Wiederaufnahmehemmer führen zu einer Erhöhung der Noradrenalin-Konzentration und somit zu einer Erhöhung des Sympathikotonus.

Im "Locus caeruleus", einer relativ kleinen, dunkelfarbigen Zellgruppe in der vorderen Rautengrube, einem Teil der Brücke (Pons), wird ein Großteil des Noradrenalins des ZNS produziert. Benzodiazepine vermindern die Aktivität des "Locus caeruleus" und reduzieren damit den Transport von Noradrenalin zum Vorderhirn.

Noradrenalin gehört, wie beispielsweise auch Adrenalin und Dopamin, zur Gruppe der Katecholamine. Sein natürliches Stereoisomer ist L-(−)-Noradrenalin [Synonym: (S)-Noradrenalin], und dessen Enantiomer -(+)-Noradrenalin [Synonym: (R)-Noradrenalin] ist physiologisch unbedeutend.

Die Produktion von Noradrenalin erfolgt in den Nebennieren und im Nervensystem aus Dopamin mittels des Enzyms Dopaminhydroxylase. Als Kofaktor und Elektronendonator spielt Vitamin C eine Rolle.

Eine pathologisch erhöhte Konzentration an Noradrenalin im Blut findet sich beim Krankheitsbild der Herzinsuffizienz.

Noradrenalin wird als Notfall-Arzneimittel in der Intensivmedizin (bei Erwachsenen in einer Dosierung von 2–16 µg/min) verwendet. Es leistet gute Dienste bei der Behandlung von folgenden Krankheitsbildern:

Es wird dabei intravenös meistens mittels Spritzenpumpe verabreicht. Noradrenalin sollte so niedrig wie möglich dosiert werden, da es dem Herzen die Pumparbeit erschwert. Hauptzielparameter der Dosierung ist eine ausreichende Nierenausscheidung. Meist wird Noradrenalin mit einer relativ hohen Flüssigkeitsgabe kombiniert, um das intravasale Volumen aufzufüllen.

Noradrenalin darf nicht oder nur sehr vorsichtig bei folgenden Zuständen angewandt werden:

Monopräparate: Arterenol (D), sowie als Generikum (CH)
Kombinationspräparate: Scandonest (CH)



</doc>
<doc id="10151" url="https://de.wikipedia.org/wiki?curid=10151" title="Neurotransmitter">
Neurotransmitter

Neurotransmitter sind Botenstoffe, die an chemischen Synapsen die Erregung von einer Nervenzelle auf andere Zellen übertragen ("synaptische Transmission").

Sie werden im Zellkörper oder in der Endigung des Axons vom sendenden Neuron produziert und in Quanten freigesetzt.

Der Ausdruck "Neurotransmitter" ist abgeleitet von „Sehne, Nerv“ und „hinüber schicken, übertragen“.

Neurotransmitter sind Botenstoffe von Nervenzellen, mit denen die (präsynaptischen) elektrischen Signale eines Neurons an einer Synapse in chemische Signale umgebildet werden, die bei der nachgeordneten Zelle wieder (postsynaptische) elektrische Signale hervorrufen können.

In die präsynaptische Membranregion des Neurons fortgeleitete elektrische Impulse, Aktionspotentiale, veranlassen über kurzzeitigen Calciumeinstrom die Ausschüttung der Botenstoffe aus Vorratsspeichern, den synaptischen Vesikeln. Dieser Vorgang ist eine Exozytose: Durch Fusion der Vesikelmembranen mit der präsynaptischen Membran wird das je enthaltene Quantum an Transmittermolekülen in den (extrazellulären) synaptischen Spalt freigesetzt und gelangt per Diffusion zu den Rezeptoren auf der postsynaptischen Membran der nachgeschalteten Zelle.

Diese Membranproteine der subsynaptischen Region erkennen den jeweiligen Transmitter spezifisch an seiner molekularen räumlichen Struktur und Ladungsverteilung durch komplementäre Strukturen. Die Bindung eines Transmittermoleküls führt zur strukturellen Veränderung des Rezeptorproteins, wodurch direkt ("ionotrop") oder mittelbar ("metabotrop") bestimmte Ionenkanäle in dieser Region vorübergehend geöffnet werden.

Abhängig von der Zahl an Rezeptoren mit gebundenem Transmitter entstehen so Ionenströme verschiedener Stärke mit entsprechenden postsynaptischen Potentialdifferenzen (PSP). Diese sind nun – festgelegt über die Zuordnung von Rezeptoren in der Membran zu Ionenkanälen bestimmter Ionensorte – entweder depolarisierend, so dass sie als exzitatorisches postsynaptisches Potential ("EPSP") eine Erregung der nachgeschalteten Zelle "fördern" bzw. zur Bildung eines Aktionspotentials führen, oder aber so, dass sie als inhibitorisches postsynaptisches Potential ("IPSP") jene "hemmen" bzw. eine Erregung verhindern. Damit wird zwischen exzitatorischen und inhibitorischen Synapsen unterschieden.

Neben dem eigentlichen Neurotransmitter werden nicht selten noch Kotransmitter ausgeschüttet ("Kotransmission"), welche die Erregungsübertragung auf verschiedene Weise als Neuromodulatoren beeinflussen können. Die Bindung von Transmittern an Rezeptormoleküle ist in der Regel reversibel, nach Ablösung somit erneut möglich. Begrenzt wird ihre Wirkung nicht allein durch Diffusion, sondern durch enzymatische Spaltung (z. B. Cholinesterasen), Aufnahme in Gliazellen, präsynaptische Wiederaufnahme in das Neuron oder auch eine postsynaptische Internalisation samt Rezeptor (als Endozytose). Daneben ist postsynaptisch die prompte Inaktivation von Ionenkanälen (Desensitivierung) möglich. Weiterhin können präsynaptisch gelegene Autorezeptoren für den Transmitter dessen Freisetzung negativ rückgekoppelt beschränken. Darüber hinaus sind zahlreiche weitere präsynaptische Rezeptoren bekannt, überwiegend metabotrop G-Protein-gekoppelte Rezeptoren, womit sich vielfältige Modifikationen synaptischer Übertragung ergeben.

Für die Wirkung einer synaptischen Transmission ist nicht die präsynaptisch als Transmitter ausgeschüttete chemische Substanz entscheidend, sondern die postsynaptisch ausgebildete Empfänglichkeit der nachgeordneten Zelle. Beispielsweise ruft der gleiche Transmitter Acetylcholin im Skelettmuskel – vermittelt über ionotrope nikotinische N-Cholinozeptoren – eine Depolarisation hervor, jedoch im Herzmuskel – vermittelt über metabotrope muskarinische M-Cholinozeptoren – eine Hyperpolarisation. Im einen Fall führt dies zu einer Erregung von Skelettmuskelfasern, im anderen Fall zu einer Abnahme der Erregbarkeit von Herzmuskelzellen.

Der wichtigste Transmitter im peripheren Nervensystem ist Acetylcholin, so nicht nur an der motorischen Endplatte von Muskelfasern, sondern auch im parasympathischen Teil des vegetativen Nervensystems sowie präganglionär im sympathischen Teil, postganglionär wird hier meist Noradrenalin ausgeschüttet (doch sind z. B. die Schweißdrüsen cholinerg innerviert).

Der wichtigste Neurotransmitter im zentralen Nervensystem (ZNS) ist Glutamat, mit erregender Wirkung; die wichtigsten Transmitter inhibitorischer Synapsen sind Gamma-Aminobuttersäure (GABA) und Glycin. Andere häufige Neurotransmitter sind Dopamin und Serotonin neben Acetylcholin und Noradrenalin, auch bei Synapsen im ZNS.

Biochemisch betrachtet sind die meisten bekannten Neurotransmitter neben Acetylcholin (aus Cholin, "cholinerge Übertragung") entweder

Daneben fungieren Phosphosester von Purinen wie Adenosinmonophosphat (AMP), Adenosindiphosphat (ADP), Adenosintriphosphat (ATP) sowie Uridindiphosphat (UDP) und Uridintriphosphat (UTP) auch an Synapsen als (Ko-)Transmitter.

Neurotransmitter können zunächst nach Stoffklassen eingeteilt werden.









</doc>
<doc id="10152" url="https://de.wikipedia.org/wiki?curid=10152" title="Drachen">
Drachen

Ein Drachen (nach chinesischen, als Drachenbilder ausgeführten Drachen), Fesseldrachen oder auch Kite (englisch) ist ein Spiel- und Sportgerät, das mit Wind betrieben wird. Er besteht in der einfachsten Ausführung aus einem Segel, das in der Regel durch ein Gestänge aufgespannt wird und einer am Gestänge befestigten Leine, die vom Drachensteigenden (im Drachensport "Pilot") gehalten wird. Der Drachen wird so in den Wind gestellt, dass durch die Anströmung der Luft gegen das Drachensegel dynamischer Auftrieb entsteht und der Drachen nach oben steigt.

Es gibt auch ein Drachenfliegen "ohne Wind", bei dem die Luftanströmung nur durch Fortbewegung des Piloten erzielt wird, etwa durch Gehen oder Laufen. Auf diese Weise kann bei Windstille oder in geschlossenen Räumen geflogen werden.

Ein Drachen bezieht seine Auftriebsenergie aus dem Wind, der über die Segelfläche gelenkt wird. Er fängt den Wind nicht ein wie ein Fallschirm, sondern er leitet den Wind über seine Auftriebsflächen und legt sich auf die Strömung oder lässt sich vom Unterdruck über der Segelfläche emporziehen. Dazu muss ein Drachen eine Strömungsrichtung definieren und sich in die geeignete Richtung in den Wind drehen.
Kastendrachen erreichen dies mit den senkrechten Flächen des Kastens, die den Drachen in die Richtung mit dem kleinsten Widerstand rücken.
Flachdrachen leiten die Windströmung von der Mitte nach außen ab, indem sie sich im Winddruck beugen, also die äußeren Segelspitzen nach hinten biegen. Um auch bei geringen Windgeschwindigkeiten stabile Fluglagen zu erzielen, ist in der Regel das Quergestänge bereits gewinkelt angebracht.

Der Ursprung der Drachen ist heute nur noch sehr schwer feststellbar. Es gibt Hinweise, dass die Drachen bereits im 6. Jahrhundert v. Chr. erfunden wurden.
Die erste geschichtliche Erwähnung von Drachen als Flugobjekt stammt aus dem China des 5. Jahrhunderts v. Chr. Jüngste Funde im indonesischen Raum lassen jedoch die Möglichkeit offen, dass Drachen als Flugobjekt noch wesentlich älter sein könnten.

Die ersten Drachen, die in China im 5. Jahrhundert v. Chr. auftauchten, bestanden hauptsächlich aus Bambusstäben und Seide. Weil Letztere teuer war, verbreitete sich der Drachen nur langsam. Das änderte sich, als das erste Papier entwickelt wurde. Man vermutet, dass die Drachen durch buddhistische Missionare und über den Handel mit Papier im 2. Jahrhundert v. Chr. schließlich auch Japan und Korea erreichten.

Der im Bild zu sehende Centipede-Drachen war ursprünglich ein traditioneller Drachen, der nur für den Kaiser aus Seide und Bambus gebaut wurde, und den nur er fliegen durfte. Er galt als ein Glückssymbol und wurde in der Qing-Dynastie (1644–1911) so hoch wie möglich geflogen, um dann die Leine zu kappen und den Drachen dem Wind zu überlassen. Die Chinesen glaubten, dass damit auch alle Sorgen und Gefahren davonfliegen.

In Japan sind über 300 verschiedene Drachentypen bekannt. Dabei unterscheiden sich die verschiedenen Modelle oft je nach Geographie und Materialverfügbarkeit. Seit Anfang des 19. Jahrhunderts haben sich die Bemalung und Bauart wenig verändert.

So entstanden im Laufe der Zeit viele Volksfeste, bei denen Drachen eine zentrale Bedeutung erhielten. Ein Beispiel ist hier das japanische Neujahrsfest.

Auch in Afghanistan spielen Drachen insbesondere im Herbst und in den Wintermonaten eine große Rolle. Doch der Höhepunkt des Drachensteigens ist während des traditionellen Neujahrfestes, Nouruz, das seit jeher am Frühlingsanfang zelebriert wird.

Die Römer ließen zu besonderen Anlässen wie militärischen Siegen oder Volksfesten bunt verzierte Windsäcke fliegen. Das Militär verwendete ab 300 n. Chr. als Feldzeichen die Dracostandarte, die aus einem stilisierten Drachenkopf und einer im Wind flatternden Tuchröhre bestand. Hier liegen auch die etymologischen Wurzeln für den mit einem langen Schweif versehenen (Spielzeug-)Drachen.

Die echten Drachen kamen aber erst im 16. Jahrhundert über holländische, portugiesische und englische Kaufleute nach Europa, die Handel mit Fernost betrieben. Giambattista della Porta beschreibt 1589 in Band 20 seines Werkes „Magia naturalis“ den Bau eines Fesseldrachens.

Zu Beginn des 18. Jahrhunderts waren die Drachen schließlich als Kinderspielzeug im ganzen Westen verbreitet. Jetzt erst erkannte die Wissenschaft, welche forschungstechnischen Möglichkeiten der Drachen bot. Die Experimente konzentrierten sich zu Beginn auf meteorologische Untersuchungen. So untersuchten zum Beispiel Thomas Melvill (1726–1753) und Alexander Wilson mit einem Drachen Temperaturunterschiede zwischen den einzelnen Höhenlagen. Ein bekanntes Experiment dieser Zeit wurde von dem amerikanischen Diplomaten und Wissenschaftler Benjamin Franklin durchgeführt. Franklin untersuchte mit Drachen die Wirkung elektrischer Blitze. Der am Himmel schwebende Drachen trug in dieser Zeit stark zum beständigen Wunsch der Menschheit bei, fliegen zu können, und beeinflusste so maßgeblich die Entwicklung der ersten Flugmaschine.

Im heutigen Griechenland ist das Fliegenlassen von selbstgebauten Papierdrachen fester Bestandteil des Kathara Deftera, dem Montag, an dem die Fastenzeit beginnt und die Karnevalszeit endet. An diesem gesetzlichen Feiertag stehen bei geeignetem Wind Hunderttausende von Drachen über ganz Griechenland, wobei laut dem Volksglauben eine große erreichte Höhe des Drachens Glück für das kommende Jahr verheißt.

Der Drachen fand auch Anwendung im militärischen Bereich. Aus der Frühzeit und dem Mittelalter sind die folgenden Verfahren überliefert:
Im alten Japan verwendeten Heerführer Drachen zur psychologischen Kriegführung. Sie befestigten Apparaturen zur Geräuscherzeugung an den Drachen und ließen sie nachts über den feindlichen Truppen steigen. Die Krieger glaubten so, sie würden von bösen Geistern attackiert. Es gibt Aufzeichnungen über riesige Drachen, die Bogenschützen trugen.

In Korea wurden mittelalterliche Schlachten mit Signaldrachen koordiniert, deren Zeichnung direkte Befehle an die Truppen darstellten. Die Drachen waren auch aus bewaldeten Stellungen gut auszumachen, so dass die verbildlichten Anweisungen schnell die verschiedenen Truppen erreichen konnten.

Im europäischen Raum wurde bei Belagerungen mit Drachen die Entfernung zu den feindlichen Stellungen ausgemessen. Dabei ließ man den Drachen abstürzen und maß die Länge der ausgegebenen Schnur.

Zu Beginn des 20. Jahrhunderts experimentierte der Amerikaner Samuel Franklin Cody mit seinem "man lifting system" – bekannt geworden und 1903 patentiert als "Cody’s Mankite" – für die britische Armee. Ziel der Experimente war es, einen Drachen zu konstruieren, der einen Menschen in die Lüfte heben konnte.

Während der Weltkriege fanden Drachen hauptsächlich in Bereichen der Luftüberwachung und Schützentraining ihren Einsatz, wurden jedoch bald durch neuere Technologien ersetzt. Die Firma Steiff überstand den Krieg durch ihren erfolgreichen Roloplan-Drachen, der vom Militär als Flakziel geordert wurde. Auch zur Abwehr von Tieffliegerangriffen wurden Drachen vielfach eingesetzt "(Sauls Luftabwehrdrachen)". Das Ziel dabei war die Störung der Flugbahn der Angreifer.

Im Zweiten Weltkrieg kam der Drachen "Gibson Girl" zum Einsatz. Er war Teil der Seenotrettungsausrüstung für Flugbesatzungen, die notwassern mussten. Der Drachen trug eine dünne Notantenne in den Himmel, die mit einem Funkgerät verbunden war. Ein handbetriebener Generator erlaubte der Flugbesatzung, SOS zu funken.

Neben der Verwendung als Kinderspielzeug und Sportgerät können Drachen auch zum Transport verwendet werden. Ein Beispiel hierfür ist Skysails, ein computergesteuertes Drachensystem zur Senkung des Kraftstoffverbrauchs von Schiffen. Dabei wird ein Lenkdrachen mit einer Fläche von bis zu 600 m² an einem Schiff befestigt und dient so als zusätzlicher Antrieb.

Drachen können auch als Antennenträger in großer Höhe dienen. So ist es möglich, die Drachenschnur in Form eines Kabels auszuführen, das als Antenne für Längstwelle, Langwelle oder Mittelwelle dient.

Seit ca. 100 Jahren bekannt ist eine Art der Fotografie, bei der die Zugkraft eines Drachens genutzt wird, um eine Kamera zu heben. Dies wird international als Kite Aerial Photography (kurz KAP) bezeichnet.

Viele Jahre wurden Drachen auch für die meteorologische Untersuchung der freien Atmosphäre genutzt. Am Aeronautischen Observatorium in Lindenberg wurde am 1. August 1919 der noch heute gültige Höhenweltrekord für Drachen von 9.740 m unter dem Aufstiegsleiter Georg Stüve (1888–1935) erreicht. Dabei wurde eine Drachenkette aus acht Schirmdrachen verwendet.

Außerdem wurden sämtliche Konstruktionsprinzipien der Luftfahrtpioniere direkt von Erfahrungen aus dem Drachenbau abgeleitet. Otto Lilienthal, Gustav Weißkopf, Alexander Graham Bell und die Brüder Wright arbeiteten sich mit Drachen an ihre Konstruktionen heran.

Anfang der 1920er Jahre konstruierte Hans Seehase in Berlin einen Drachenfallschirm, den er am 23. April 1923 erstmals auf dem Tempelhofer Feld erprobte. Weiterhin experimentierte er mit personentragenden, zusammenlegbaren Drachen, die beispielsweise durch Boote geschleppt auch militärische Verwendung finden sollten.

Da in großer Höhe bessere Windverhältnisse für Windgeneratoren zur Stromerzeugung herrschen, hat es Versuche gegeben, diese mit Drachen auf Höhe zu bringen und dort zu halten.

Einen weiteren Prototyp baute der Holländer Wubbo Ockels 2011. Durch eine Computersteuerung bewegt sich ein Lenkdrachen (TubeKite) in liegenden Achten am Himmel. Durch ständiges Verändern der Leinenlänge (Jo-Jo Effekt) kann eine am Boden befindliche Batterie aufgeladen werden. Der Vorteil dieses Systems besteht aus den geringen Kosten. Um aber mehrere Kites auf einer Farm betreiben zu können, müssen die Kites untereinander durch eine komplizierte Steuerung kontrolliert werden, um Höhenabfall und Kollisionen zu vermeiden.

Ein Drachen besteht meist aus einem "Gestänge", das mit Tuch bespannt ist; über die "Waage" sind die "Leinen" daran befestigt, welche zum Piloten führen.

Als Material für das Gestänge können Stäbe zum Beispiel aus Bambus, Holz, Aluminium, glasfaser- oder kohlenstofffaserverstärktem Kunststoff in unterschiedlichen Profilformen als Baumaterial verwendet werden. Zur Verbindung der Stäbe werden oft kurze Stücke von Kunststoffschläuchen als Muffen verwendet; industriell hergestellte Drachen verwenden meist passend angefertigte Kunststoffteile.

Als Bespannung können Papier, Segeltuch (aus Ripstop-Nylon oder Polyester), Baumwoll-/Hanftuch, Tyvek (auch Papierstoff genannt), Seide, Naturmaterialien (Blätter, Leder), Folien aus Polyester oder Aluminium u. a. dienen.

Die Drachenwaage, kurz auch Waage genannt, verbindet Drachen und Leine miteinander. Die Waage ist für die Stellung des Drachens zum Wind verantwortlich und bestimmt die Flugeigenschaften des Drachen. Es gibt verschiedene Waagetypen, die sich durch unterschiedliche Verstellmöglichkeiten und Anwendungszwecke auszeichnen. Grundsätzlich wird mit der Waage das Verhältnis von Zugkraft zu Auftriebskraft beim Aufteilen der Windkraft bestimmt (Anstellwinkel). Je steiler der Drachen steht, umso mehr wird in Zugkraft verwandelt, je flacher, umso mehr in Auftriebskraft.

Über die Leine ist der Drachen mit dem Piloten verbunden. Eine Leine endet meist in einer Haspel, auf der sie aufgewickelt werden kann. Haspeln für Einleiner haben manchmal zwei Griffe, damit sie mit beiden Händen gehalten und vor allem aufgewickelt werden können. Bei Lenkdrachen werden die Leinen dagegen auf Griffe (Haspeln) oder Ringspulen aufgewickelt, die mit je einer Hand gehalten werden können. Alternativ werden Lenkdrachen auch mit Lenkschlaufen gelenkt (welche um das Handgelenk gelegt werden) oder mit einer Lenkstange; bei sehr zugstarken Lenkdrachen kann der Pilot auch ein "Trapez" tragen, das die Zugkraft aufnimmt; der Pilot muss dann mit den Armen nur noch steuern, ohne mit ihnen die Zugkraft kompensieren zu müssen. Einleiner werden oft auch nicht direkt vom Piloten gehalten, sondern an einem "Bodenanker" befestigt. Falls der Drachen sehr hoch steigen soll, verwendet man eine Umlenkrolle, Winsch und Seilwinde. Drachenleinen von Lenkdrachen werden heute üblicherweise aus Dyneema, Polyethylen oder Kevlar hergestellt. Im Gegensatz zu den Leinen ungelenkter Drachen wird bei Lenkdrachen kein Reck verwendet. Daher wird bei Lenkdrachen kein Nylon verwendet, weil sich dieses unter Zug längt, was den Drachen schwerer steuerbar machen würde.

Für die Leinen von einleinigen, ungelenkten Drachen verwendet man das elastischere Nylon oder Polyester. Es längt sich unter Zug, was dazu führt, dass die Leine Windschwankungen ausgleichen kann. Traditionell fanden Flachs- und Hanfkordeln Verwendung.

Drachenleinen können durch ihren geringen Durchmesser, verbunden mit der starken auf ihr lastenden Zugkraft, schneidend wirken. Deswegen dürfen sich keine Zuschauer in der Viertelkugel (auch: "Windfenster"), die Drachen und Piloten zusammen mit den Leinen beschreiben können, aufhalten; selbst die Drachenlenker vermeiden es mit bloßen Händen unter Zug stehende Leinen zu halten. Die Leinen werden üblicherweise mittels Spleiß und in einem Schutzmantel mit den Halteösen verbunden.

Einige Drachen besitzen einen "Schwanz"; dieser kann aus Stoffbändern (Streamer), einem Schlauch oder beispielsweise einem kleinen Windsack bestehen. Bei vielen Drachen dient der Schwanz zur Stabilisierung (ähnlich wie bei einem Schiff ein Treibanker) – ohne ihn verhält sich der Drachen sehr nervös und bricht ständig zur Seite aus.

Einleinerdrachen sind die Grundform der Drachen. Sie sind üblicherweise ungelenkt. In Europa sind deltoide Flachdrachen der verbreitetste Typ. In der Konstruktion sind die einleinigen Drachen recht anspruchsvoll, da während des Flugs kaum Einfluss auf das Verhalten des Drachen genommen werden kann.

Eine Ausnahme bilden hier die sogenannten "Kampfdrachen", bei welchen es sich um einleinige Lenkdrachen handelt. Wird die Leine locker gelassen, bricht der Drachen seitlich aus. Durch Ziehen an der Leine stabilisiert er seine Flugbahn in einen Geradeausflug. Durch die Gestängekonstruktion (Querstrebe in der Mitte dicker als außen) werden die Segelflächen durch den höheren Winddruck nach hinten gebogen, so dass durch die starre Mittelstrebe ein sogenannter Kiel entsteht, welcher den Drachen stabilisiert.

In asiatischen Ländern sind Drachenkämpfe seit vielen Jahrhunderten Tradition. Sieger beim Drachenkampf ist der, dessen Drachen als letzter noch am Himmel steht, während die gegnerischen Drachen zu Boden gedrängt oder abgeschnitten wurden. In neuester Zeit haben Drachenkämpfe auch auf europäischen Drachenfesten Einzug gefunden. Hier ist der Rokkaku das meistverwendete Kampfdrachen-Modell.

Weitere Einleinerdrachen sind z. B. Kastendrachen, Roloplan, Gleitdrachen, Cody Warkite, Sauls-Naval-Barrage-Drachen und Markoni.
Mehrleinige Drachen sind in der Regel lenkbar. Sie haben gewöhnlich zwei Leinen, an denen unterschiedlich stark gezogen werden kann. Jeweils eine Leine ist mit einem Ende des Drachens verbunden. Meist gibt es ein linkes und ein rechtes Ende, an dem jeweils eine Leine befestigt ist.

Da Drachen die Eigenschaft haben, bei genügend Wind von allein aufzusteigen, kann, wenn z. B. an der rechten Leine stärker gezogen wird, in diesem Fall der linke Teil des Drachens schneller aufsteigen. Folglich dreht er sich in diesem Fall um seinen rechten Teil, vom Piloten aus gesehen im Uhrzeigersinn. Dabei stehen sich der Vektor der Windkraft und der Vektor der Zugkraft nicht genau gegenüber. Folglich verdreht sich der linke Teil des Drachens leicht nach hinten (nach Lee), so dass der Drachen aus Sicht des Piloten weiter nach rechts zeigt. Das hat zur Folge, dass der Drachen auch weiter nach rechts fliegt. Mit dem Zug an der linken Leine verhält es sich spiegelsymmetrisch.

Piloten können mit Zug an den Leinen den Lenkdrachen Loopings, Schrauben, Winkel und andere Figuren fliegen lassen. Je nach Leinen sind bis zu 15 Loopings und somit Verdrehungen der Steuerleinen nach einer Seite möglich. Das funktioniert auch als Mannschaftssport, üblicherweise vier Piloten bilden eine Crew und fliegen in Wettkämpfen gemeinsame Figuren.

Der Deltadrachen ist der klassische Lenkdrachen. Er wurde neben dem Rhombusdrachen als erster als Lenkdrachen eingesetzt.

Der Delta besteht aus einem V-förmigen Segel, das durch ein etwas schmaleres V in der Stabkonstruktion aufgespannt wird. Der Wind wölbt die Segelflächen nach oben und bildet das Profil aus. Bei etwa einem Drittel der Stablänge des V und bei rund zwei Dritteln wird je eine Waagenleine befestigt. Eine dritte Leine wird an das untere Ende der Mittelstrebe geführt (Dreipunktaufhängung). Damit erhält man zwei Befestigungspunkte für die Steuerleinen.

Deltadrachen gibt es in vielen Ausführungsvarianten. Beim rechts abgebildeten Exemplar wird die Segelfläche durch kleine Ständer, so genannte "Stand-Offs", vorgespannt.

Grundsätzlich gilt, dass kleine Drachen schneller in ihren Bewegungen sind, große dagegen langsamer. Große Drachen entwickeln deutlich höhere Zugkräfte als kleine.

Es können mehrere Lenkdrachen hintereinander gekoppelt werden. Der aktuelle Weltrekord liegt bei 301 Drachen (21. August 2010 in St. Peter-Ording).

Mattendrachen (auch: "Airfoils", "Parafoils", "Soft Kite", "Lenkmatte" oder einfach "Matte" genannt) sind stablose Drachen, die bei Anströmung ein Flügelprofil ähnlich einem Gleitschirm entwickeln. Da sie vollständig aus Gewebe und Leinen bestehen, können sie platzsparend gefaltet und ohne Bruchgefahr für Stangen transportiert werden.

Die Matten bestehen aus Ober- und Untersegel, denen durch verbindende Stege ein Flügelprofil gegeben wird. Die entstehenden Kammern sind alle oder teilweise an der Anströmungsseite geöffnet. Die Kammern sind untereinander verbunden, damit Druckunterschiede im Innern ausgeglichen werden können.

Bei Anströmung auf die Vorderkante füllen sich die Kammern mit dem Staudruck, der höher ist als die Drücke auf der umströmten Außenfläche. Dadurch wird das Profil der Matte stabilisiert. Der durch den Unterdruck auf der Oberseite entstehende Auftrieb wird durch eine komplexe vielleinige Waage von der Unterseite auf die zwei Zugleinen abgeleitet.

Matten werden durch unterschiedliche Längen der beiden Zugleinen gelenkt. Dreileiner-Matten besitzen eine weitere Leine, mit der das Starten vereinfacht, die Zugkraft reguliert und der Flug jederzeit kontrolliert beendet werden kann. Vierleiner-Matten besitzen zusätzlich zu den zwei "Lenkleinen" zwei weitere Leinen – "Bremsleinen" –, die auf die hintere Kante des Profils wirken. Bremsleinen unterstützen die Lenkbarkeit. Durch gleichzeitigen Zug an beiden Bremsleinen kann der Auftrieb der bewegten Matte erhöht oder auch die Matte abgebremst und gelandet werden. Vierleiner können auch rückwärts vom Boden starten. Der Einsatz aller vier Leinen wird im sportlichen Einsatz wegen der exakteren Lenkbarkeit und Dosierbarkeit der Zugkräfte bevorzugt. Manche Lenkmatten können wahlweise als Zweileiner oder Vierleiner geflogen werden.

Eine Matte muss immer mit Zug an den Leinen geflogen werden. Wird sie nicht ausreichend angeströmt, so wird sie drucklos und klappt zusammen. Eine hybride Bauform findet man als Flexifoils und ihren Nachfolgern, den Airfoils. Anstatt des Rohres ist ein sehr biegsamer GFK-Stab verbaut, der die Matte selbst bei geringer Anströmung aufspannt.
Ein Vorteil der Flexifoils/Airfoils ist, dass diese Schirme problemlos aneinandergehängt werden können.
Mit dem sogenannten Stacker6 der Firma Flexifoil wurde der Geschwindigkeitsrekord mit erstaunlichen 180 km/h aufgestellt.

Einsatzgebiete: Kitebuggy fahren, Kitesurfen, Snowkiting und Kitelandboarding

Die Steigerung des Lenkdrachens findet sich im "Vierleinerdrachen" wieder.
Beim Vierleiner werden die beiden üblichen Steuerleinen durch zwei weitere an der hinteren Segelkante angebrachte Bremsleinen ergänzt.
Durch diese beiden Leinen lässt sich der Anstellwinkel der beiden Segelflächen variabel einstellen. Damit lassen sich die Drachen auch rückwärts fliegen, auf der Stelle stehen oder drehen (Propeller Spin) und sanft auf den Boden aufsetzen.

Zugdrachen sind in der Regel vierleinige Mattendrachen (s. o.) oder mehrleinige sogenannte Tubekites (Kitesurfen), Ausnahmen sind hier zweileinige, große Deltadrachen und als Zugdrachen verwendete zweileinige Mattendrachen. Moderne Zugdrachen werden mit hohem technischen Aufwand gefertigt, für das Design und die Konstruktion fließen Erkenntnisse aus dem Paragliding mit ein.

Zugdrachen werden für verschiedene Drachensportarten verwendet, zum Beispiel beim Kitesailing. Kitesailing ist der Oberbegriff für Sportarten wie: Kitesurfen, Snowkiting, Kitebuggyfahren oder Powerkiting.

Im Bereich der Schifffahrt gibt es Systeme, um Zugdrachen als Antriebsunterstützung einzusetzen ("Skysails"), diese haben allerdings noch keine weite Verbreitung erlangt.


Meist wird der Drachen einfach nur steigen gelassen. Als Wort für das "Drachen steigen lassen" hat sich der Anglizismus "kiting" verbreitet, insbesondere für die sportliche Variante.
Je nach Wind können Drachen solche Kraft entfalten, dass sie den Piloten mitschleifen oder sogar in die Luft ziehen können. Das bewusste Spiel mit den Kräften des Drachens wird Powerkiting genannt.

Eine weitere Form des sportlichen Drachenfliegens ist das Kitesailing. Kitesailing beinhaltet das Fahren mit einem Kitebuggy (Buggykiting), das Kitesurfen und das Snowkiting. Hierbei lässt man sich in der Regel von großen zugstarken Matten ziehen.

Weit verbreitet ist der Trickflug oder Ballettflug. Mit Deltadrachen werden in einer Choreographie bestimmte Tricks, die von dem Schwierigkeitsgrad bestimmt werden, nachgeflogen.
Zurzeit gibt es ca. 155 offizielle Figuren, die bei Wettbewerben geflogen werden. Die Wettbewerbsfelder haben definierte Größen, die sich wie folgt darstellen:
Die Fläche zwischen den markierten Grenzen soll mindestens den unten wiedergegebenen Abmessungen entsprechen:

Bekannte Tricks sind zum Beispiel die Lazy Susan, bei der sich der Drachen einmal auf dem Rücken um 360° dreht. Oder der Backflip, wobei sich der Drachen auf den Rücken legt.

Indoor-Drachen sind ultraleichte Lenkdrachen, die keinen Wind benötigen. Die Rückwärtsbewegung des Piloten reicht aus, um diese Drachen zu fliegen. Diese Fähigkeit wird durch extrem leichte Bauweise unter Verwendung von Carbongestänge und leichtem Segeltuch erreicht.

Das so genannte "Indoor-Kiting" findet oftmals in Sporthallen oder leerstehenden Fabrikhallen statt. Beim Fliegen von Lenkdrachen im Freien kommt der Wind von hinten, und der Drachen kann in einem Bereich von ca. 120 Grad von links nach rechts und zurück bewegt werden. In einer Halle, wo nur durch die individuelle Rückwärtsbewegung Wind in das Segel kommt, ergibt sich hingegen ein Windfenster von vollen 360 Grad.

Auch beim Indoor-Kiting gibt es verschiedene Arten von Lenkdrachen, die in der Regel durch die Anzahl ihrer Steuerleinen klassifiziert sind. Es wird zwischen Ein-, Zwei- und Vierleiner-Drachen unterschieden.

Große Drachenflugfestivals finden regelmäßig in der Drachenhauptstadt der Welt Weifang (China), Pasir Gudang (Malaysia), Berck und Fréjus (Frankreich), Insel Fanø (Dänemark) sowie in Lünen, Travemünde und Kamen (Deutschland) statt. Weitere Veranstaltungen gibt es auf Rømø (Dänemark) sowie in Cuxhaven und in St. Peter-Ording (Deutschland). Im deutschsprachigen Raum fanden im Jahr 2015 über 150 Drachenflugveranstaltungen statt.

In Deutschland, aber unter anderem auch in Österreich, der Schweiz, dem Vereinigten Königreich von Großbritannien und Irland sowie von der EASA – European Aviation Safety Authority – wurden Drachen bisher unabhängig von der Länge der Flugleinen als Luftfahrzeuge eingestuft.
Damit besteht teilweise abhängig von den jeweiligen nationalen Regelungen einerseits eine Gefährdungshaftung für den Halter des Drachens, mit der andererseits vielfach eine gesetzliche Versicherungspflicht nach dem Luftverkehrsgesetz besteht.
In Deutschland bestand eine Halterhaftung und gesetzliche Versicherungspflicht schon vor der letzten Änderung des Luftverkehrsgesetzes (LuftVG) im Jahr 2005. Über die üblichen Privathaftpflichtversicherungen waren daher Drachen in aller Regel nicht oder nur sehr eingeschränkt versichert. Insbesondere stellten die Versicherungsklauseln oft auf ein bestimmtes Höchstgewicht und eine bestimmte Flughöhe von 30 Metern ab. Gleichzeitig wurde in den Bedingungen aber eine Haftung für versicherungspflichtige Drachen generell ausgeschlossen, sodass im Schadensfall bei solchen Policen immer mit einer Ablehnung der Schadensübernahme gerechnet werden musste.
Diese Haftungsfragen wurden zwischen dem Gesamtverband der Deutschen Versicherungswirtschaft, verschiedenen Versicherern, dem Luftfahrt-Bundesamt und dem Bundesministerium für Verkehr, Bau und Stadtentwicklung gleichwohl kontrovers diskutiert. Im Ende 2011 wurde sodann der bisher in der Diskussion maßgebliche "§ 1 Abs. 2 Nr. 7 LuftVG" ersatzlos aus dem Luftverkehrsgesetz gestrichen. Der Deutsche Gesetzgeber begründet das damit, dass ein Drachen eher als Hindernis für die Luftfahrt, denn als Luftfahrzeug anzusehen sei. Der Deutsche Gesetzgeber hat insofern mit einer ergänzenden Änderung des "§ 31 LuftVG" klargestellt, dass er Drachen als Gerät ansieht, welches mit besonderen Gefahren für die Luftfahrt verbunden ist, ohne Luftfahrzeug zu sein. Seither kann man wohl davon ausgehen, dass (Lenk-)Drachen in Deutschland nicht mehr als Luftfahrzeuge gelten. Wobei nach der amtlichen Gesetzesbegründung der Gesetzgeber scheinbar davon ausgeht, dass ein Drachen weiterhin versicherungspflichtig sein soll, was etwas verwirrend ist, denn eine Versicherungspflicht besteht nach "§ 43 Abs. 2 LuftVG" ausdrücklich nur für Luftfahrzeuge. Möglicherweise handelt es sich hier jedoch lediglich um eine Fehlinterpretation des Gesetzgebers. Auch die "VO 785/2004 (EG)" sieht ja eine Versicherungspflicht für Drachen gerade nicht vor.





</doc>
<doc id="10155" url="https://de.wikipedia.org/wiki?curid=10155" title="Tauschkreis">
Tauschkreis

In einem Tauschkreis oder Tauschring (auch Tauschzirkel, Zeittauschbörse, Nachbarschaftshilfeverein, LETS, Talentemarkt, Tauschnetz) werden vorrangig Dienstleistungen, gelegentlich auch Waren, ohne Einsatz gesetzlicher Zahlungsmittel zwischen den Teilnehmern getauscht.

Neben einigen Versuchen, die bis ins 19., teilweise bis ins 17. Jahrhundert zurückreichen, ist wohl Silvio Gesells „Die natürliche Wirtschaftsordnung“ (1911) zu nennen, mit seiner Freigeld-Theorie.
Auf diese Freigeldtheorie bezogen sich dann einige Experimente im Zuge von Weltwirtschaftskrise und Inflation.
Hans Timm und Helmut Rödinger gründeten Ende der 1920er Jahre in Erfurt die Wära. Es folgten weitere Wära-Experimente in Schwanenkirchen, die Gemeinde Wörgl, Tirol beschloss 1931 ein sog. Notgeld einzuführen (Wörgler Geldexperiment).

Nachdem die Gemeinden, in denen mit Freigeld oder Notgeld gehandelt wurde, wirtschaftlich innerhalb kürzester Zeit aufblühten, kam sehr schnell das endgültige Aus: 1931 in Deutschland durch die brüningschen Notgeldverordnungen und 1933 durch die österreichische Notenbank, die einen Gerichtsbeschluss erwirkte, der das Notgeld verbot.

1931 versuchte man durch Gründung von sog. Ausgleichskassen der Situation zu begegnen, im Gegensatz zu Wära und Notgeld wurde hier bargeldlos verrechnet. Aber am Ende wurden auch sie verboten.

Vancouver Island, Kanada, 1983: Als nach Abwanderung von Luftwaffe und Industrie die Arbeitslosigkeit stark anstieg, florierte der Tauschhandel. Für die Erfindung des „Green Dollars“ und die Einführung einer Tauschzentrale, die die Konten der Mitglieder verwaltete sowie die Angebote und Nachfragen sammelte und veröffentlichte, kann Michael Linton als Gründer der modernen Tauschsysteme gesehen werden.

Michael Linton zufolge müssen die folgenden 7 Kriterien erfüllt sein, damit man ein Netzwerk als LETS bezeichnen kann:

Nicht alle sieben Punkte werden heute von allen LETS-Netzwerken gleich gesehen. Manche geben als Anreiz anfänglich ein Guthaben – bei Austritt muss das Konto wieder dieses Guthaben aufweisen. Und gerade die wertmäßige Koppelung an die Landeswährung wird häufig abgelehnt.


Am weitesten verbreitet ist eine Praxis, die im Wesentlichen der Zeitbörse entspricht. Allerdings ist es auch üblich, darüber hinaus relativ frei zu verhandeln, insbesondere um einzelnen Tauschpartnern, die eher Vertreter des Leistungsprinzips sind, etwas entgegenzukommen. Sehr strenge Vorgaben im Sinne einer Überreglementierung sind allgemein äußerst unbeliebt. Leistungsbörsen haben bisher größere Chancen, mehr gewerbliche Teilnehmer zu gewinnen.

Nach Auffassung der Tauschringbefürworter kann die selbstorganisierte Form des Wirtschaftens in Tauschringen die Bedürfnisse der Teilnehmer oft besser erfüllen als das gewöhnliche geldförmige Wirtschaftssystem. Sowohl Befürworter als auch Kritiker von Tauschringen weisen darauf hin, dass wichtige Lebensbereiche wie Wohnen und Erwerbsarbeit von Tauschringen praktisch nicht erfasst werden und die Ziele alternativen Wirtschaftens nur innerhalb eines ökonomischen und gesellschaftlichen Gesamtkonzeptes zu verwirklichen seien. Ein Diskussionsthema ist in diesem Zusammenhang, inwieweit Tauschwährungen etwa durch Zinsfreiheit und lokale Gebundenheit Vorteile gegenüber gewöhnlichem Geld haben.

Grundlegend für die Verrechnung in Tauschringen mit Zeitwährung ist allein die aufgewendete Zeit der Teilnehmer. Tauschringbefürworter sehen in der Gleichbewertung aller Tätigkeiten einen Vorteil gegenüber dem üblichen geldförmigen Wirtschaften. Kritiker behaupten, dass in Zeittauschkreisen die Eigenschaften einer Marktstruktur von Angebot und Nachfrage weitgehend reproduziert werde, es somit nicht zwangsläufig eine Gleichstellung der Teilnehmer gebe. Kritisiert wird auch, dass die meisten Tauschringe einen Äquivalenztausch organisieren würden, der zum Beispiel vermehrt Hilfsbedürftigen nicht gerecht werde und keine grundlegende Alternative zum kapitalistischen Wirtschaftssystem darstelle (siehe auch Kapitalismuskritik).

Üblicherweise erstellen Tauschkreise ein nach Rubriken geordnetes Verzeichnis mit allen Angeboten und Gesuchen der Mitglieder, das regelmäßig aktualisiert wird. Wann, wo und auf welche Weise eine angebotene Tätigkeit ausgeführt wird, vereinbaren die jeweils Beteiligten. In der Organisationsform, die am verbreitetsten ist, werden Leistungen und Gegenleistungen in eigenen Währungen verrechnet. Für jedes Mitglied wird ein fiktives "Konto" geführt. Nimmt das Mitglied eine Leistung in Anspruch, so verringert sich der Kontostand, wird eine Leistung erbracht, erhöht er sich. Negative Kontostände sind in der Regel zulässig. Die Zentrale der Tauschbank bildet hierbei die zentrale Verrechnungsstelle aller Soll und Haben. Dies wird häufig durch eine eigenverantwortliche Führung von sog. Tauschheften ersetzt, in denen die Aktivitäten festgehalten werden. Die komplementäre Währung arbeitet gänzlich zinsfrei und ist nicht mit der Arbeit einer Zentralbank zu verwechseln. Die Währungen der Tauschkreise sind kein einfaches Abbild offizieller Landeswährungen, indem Eigenschaften wie Inflation, Verschuldung und Deflation allein durch administrative Beschlüsse reguliert werden. In einer Minderheit dieser Tauschkreise wird durch einen negativen Zins oder eine Gebühr auf Guthaben eine Anregung des Tauschens angestrebt (siehe Umlaufsicherung).

Viele Tauschkreise verwenden Währungen, in denen geleistete Dienste analog zur aufgebrachten Zeit in Einheiten verbucht werden. Dies dient in erster Linie dazu, Missbrauch zu verhindern und weniger einer "Buchhaltung". Einige Tauschringe stellen es den jeweils an einem Tausch Beteiligten frei, ob und nach welchem Maßstab eine Verrechnung in der Tauschwährung geschieht; jedoch wird Verkaufstätigkeit bzw. ein Anbieten von Leistungen gegen Geldzahlung abgelehnt. In Zusammenhang mit der Diskussion über eine Umsonstökonomie werden seit einiger Zeit Nutzergemeinschaften propagiert. In einigen Praxismodellen sind diese ähnlich zu den schon länger bekannten Freiwilligenagenturen organisiert. Sie bilden zusammen mit Formen organisierter Nachbarschaftshilfe eine Gruppe von Tauschkreisen, in denen keine Verrechnung und kein Verbuchen von Leistungen und Gegenleistungen in einer Tauschwährung stattfindet.

Die meisten Tauschkreise sind lokal gebunden. Neben Dienstleistungen wie „Nachhilfe geben“ oder „Babysitten“ werden gelegentlich auch Waren (etwa gebrauchte Kinderkleidung) gegen die Umrechnungseinheit getauscht. In ländlichen Tauschringen kann der Warentausch eine größere Bedeutung bekommen. Nur wenige Tauschkreise beschränken den Tausch ausschließlich auf Dienstleistungen.


Tauschringe in der Form von Vereinen können grundsätzlich nicht als steuerbegünstigte Körperschaft i. S. d. ff. Abgabenordnung anerkannt werden, weil regelmäßig durch die gegenseitige Unterstützung – unabhängig von Alter oder Krankheit – in erster Linie eigenwirtschaftliche Interessen der Mitglieder gefördert werden und damit gegen den Grundsatz der Selbstlosigkeit ( Abs. 1 AO) verstoßen wird.

Sofern der Verein lediglich Zeitkonten seiner Mitglieder verwaltet und Dienstleistungen vermittelt, erfüllt er zudem nicht die Voraussetzung der Unmittelbarkeit ( Abs. 1 Satz 1 AO).

Beschränkt sich dagegen der Zweck des Vereins nach Satzung und tatsächlicher Geschäftsführung auf die Förderung der Jugend- und Altenhilfe sowie die Förderung mildtätiger Zwecke, kann er als steuerbegünstigte Körperschaft anerkannt werden. In diesen Fällen kann die Selbstlosigkeit i. S. d. Abs. 1 Satz 1 AO unbeschadet des Entgelts für die aktiven Mitglieder erhalten bleiben, da diese Vorschrift nicht voraussetzt, dass der Verein und seine Mitglieder für erbrachte Dienstleistungen im Rahmen der satzungsmäßigen Zwecke auf angemessene materielle Vorteile verzichten. Es reicht aus, wenn die eigene Opferwilligkeit nicht zugunsten eigennütziger Interessen in den Hintergrund gedrängt wird. Um die Voraussetzungen der Unmittelbarkeit ( Abs. 1 AO) zu erfüllen, müssen die aktiven Mitglieder ihre Dienstleistungen als Hilfspersonen des Vereins i. S. d. Abs. 1 Satz 2 AO ausüben.

Rechtliche Aspekte der Tauschring-Mitglieder, wie Produkthaftung, Versicherungspflicht und Steuerpflicht, ergeben sich auf Basis der Freundschafts- und Nachbarschaftshilfe oder aus der rechtlichen Situation eines Tauschhandels.

Für Unternehmer stellt der Tausch ein umsatzsteuerrelevantes Rechtsgeschäft dar, das nach handels- und steuerrechtlichen Grundsätzen (Bruttoprinzip, Einzelbewertung, Vollständigkeit) zu erfassen ist.

Tauschringe können mittels Verrechnungsstellen auch überregional tauschen. In Deutschland existieren zwei bekannte Verrechnungsstellen. Der Ressourcentauschring (RTR)
und die Verrechnungsstelle für Tauschringe (VeSTa).(Website beendet 31. Dezember 2012)

In solchen Verrechnungsstellen sind Tauschringe als Mitglieder angemeldet
und ermöglichen so ihren Mitgliedern, Tauschaktivitäten,
die im eigenen Tauschring nicht zu finden sind oder nicht angeboten werden, in Anspruch zu nehmen.

Zudem gibt es seit 2008 das 3Länder Clearing von zart. Damit können angeschlossene Tauschkreise aus Deutschland, Österreich und der Schweiz untereinander tauschen. Als Basis dazu dient die Software Cyclos.

Lokales und überregionales Tauschen ermöglicht auch das Community Exchange System seit 2003 (seit 2010 auch in deutsch).

Zu dem steht seit 2010 die Online-Plattform AcrossLETS zur Verfügung. Diese dient wie RTR oder VeSTa der Verrechnung von Tauschleistungen, nutzt dazu aber eine eigene Verrechnungseinheit, welche es ermöglicht auch einzelne Mitglieder von Tauschringen oder auch Personen, die gar keinem lokalen Tauschring angeschlossen sind, in überregionales Tauschen einzubeziehen.

Die überregionalen Tauschaktivitäten bieten sich für Tätigkeiten (wie z. B. Korrekturlesen, Übersetzungen u. ä.) an, die keinen räumlichen Bezug benötigen. Für Übernachtungen oder Umzüge von Tauschringmitgliedern können sie ebenfalls genutzt werden.

Obwohl für die Verwaltung von Mitgliederstammdaten und -anzeigen sowie für die Verrechnung der jeweiligen Tauschwährung zwischen den Mitgliedern eines Tauschrings keine spezielle Software notwendig ist, erleichtert sie die Verwaltungsaufgaben eines Tauschrings jedoch und sorgt für eine bessere Übersicht.

Neben Standard-Buchhaltungsprogrammen gibt es extra für Tauschringe entwickelte oder angepasste Programme. In Deutschland werden vor allem die folgenden genutzt:







</doc>
<doc id="10160" url="https://de.wikipedia.org/wiki?curid=10160" title="Religionskritik">
Religionskritik

Religionskritik stellt Religiosität und Religionen, ihre Glaubensaussagen, Konzepte, Institutionen und Erscheinungsformen rational beziehungsweise moralisch-ethisch in Frage. Sie begleitet die Religionen durch ihre ganze Geschichte.

Die Kritik an anderen Religionen ist eine verbreitete Form religiöser Selbstdarstellung, besonders dort, wo eine Religion einen Absolutheitsanspruch für den eigenen Glauben erhebt. Im Monotheismus, wo ein höchster Gott zugleich als einziger Gott gilt, werden andere Götter bzw. Gottesbilder und ihnen zugeordnete Offenbarungen in der Regel am Maßstab des eigenen Gottesbildes kritisiert. Diese Form der Kritik hat die Religionsgeschichte wesentlich mitbestimmt.

Eine seit der Antike bekannte immanente Kritik misst empirische Erscheinungen von Religion am normativen Begriff einer absoluten Wahrheit, um falsche Gottesvorstellungen und Religionspraktiken abzuweisen. Die Christliche Theologie unterschied seit etwa 400 die „wahre“ von der „falschen“ Religion innerhalb und außerhalb des Christentums. Diese Unterscheidung wurde besonders in der Reformationszeit als reflektierte theologische Selbstkritik ausformuliert, um die christliche Religionsausübung zu reformieren.

Die Philosophie der Neuzeit bildete im Zeitalter der Aufklärung einen Allgemeinbegriff von Religion, um die verfeindeten christlichen Konfessionen, ihren Dogmatismus und ihre wechselseitigen Absolutheitsansprüche zu kritisieren. Dieser Allgemeinbegriff umfasste tendenziell auch außerchristliche Religionen und religionsähnliche Weltanschauungen, ordnet also Vorstellungen von „so etwas wie Gott“ in ein gemeinsames Genus ein. Die menschliche Religiosität wurde auf eine natürliche Fähigkeit des Menschen zum Erleben, Erfragen und Begreifen eines Sinnganzen zurückgeführt, die sich zur humanen Vernunft fortentwickeln könne und werde. Diese Vernunftreligion sollte die widerstreitenden partikularen Glaubensbekenntnisse rationalen Zwecken dienstbar machen, überwinden, auflösen oder in ein höheres, nun ganz auf sich selbst gestelltes Selbstbewusstsein „aufheben.“ Damit wurde zunehmend Religion überhaupt Gegenstand kritischen Denkens.

Der Theologe Johann Heinrich Tieftrunk verwendete die Begriffe „Kritik der Religion“ (1790) bzw. „Religionskritik“ (1791 ff.) erstmals als Titel. Er verstand sie wie Immanuel Kant ("Religion in den Grenzen der praktischen Vernunft", 1793) als kritische Prüfung nicht nur bestimmter Religionsinhalte, sondern des religiösen Bewusstseins als solchem, das zu kritisieren sei, soweit es der Autonomie der Vernunft widerspreche.

Seit den Enzyklopädisten des 17. und 18. Jahrhunderts wurde eine dezidierte Religionskritik zunehmend als spezielle philosophische Teildisziplin entfaltet. Daraus entstanden vor allem im 19. und frühen 20. Jahrhundert ausformulierte Gegenpositionen zu Religion überhaupt. Als in diesem Sinn „klassische“ Religionskritiker gelten heute vor allem Auguste Comte, Ludwig Feuerbach, Karl Marx, Friedrich Nietzsche, Sigmund Freud, Bertrand Russell, Albert Camus und Jean-Paul Sartre. Mit ihren Theorien schufen sie atheistische Weltbilder oder trugen zu deren Entwicklung bei. In den neuzeitlichen Naturwissenschaften setzte sich der methodische Grundsatz durch, Erkenntnisse unter prinzipiellem Verzicht auf transzendent begründete Dogmen oder Hypothesen zu gewinnen.

Die verschiedenartige Religionskritik des Empirismus, Materialismus, Rationalismus, Positivismus, Marxismus, der Psychoanalyse und des Existentialismus zielt auf Aufklärung, Destruktion und/oder Ersetzung vorhandener Religion, indem sie deren Entstehung aus nichtreligiösen Faktoren aufzuzeigen versucht. Auch diese Entwürfe bezogen sich historisch vor allem auf vom Judentum und Christentum überlieferte theistische Gottesbilder und Dogmen. Sie kritisieren aber darüber hinaus jede Religion und auch die philosophische Metaphysik, die ihren reflektierten Gottesbegriff ihrerseits gegen personale, mythische und naive Gottesbilder abgrenzte.

Stand in der Frühen Neuzeit der kirchliche Anspruch auf abschließende Welterklärung, im 18. Jahrhundert das christliche Monopol auf ethische Lebensführung im Feuer der aufklärenden Kritik, so rückte im 19. Jahrhundert die soziale Funktion der (noch immer vor allem christlichen) Religion in den Vordergrund des kritischen Interesses. Sie wurde nun immer stärker als Sammlung von Methoden der Selbstberuhigung, Fremdbestimmung und Herrschaftssicherung angesehen, die es zu überwinden und abzuschaffen gelte.

Die Griechische Philosophie der Antike wies aller abendländischen Philosophie den Weg, indem sie das Konzept der „Vernunft“ (griech. λόγος, "logos") ins Zentrum ihrer Reflexion rückte. Die „Warum“-Frage, aus dem Staunen über den Kosmos geboren, nach seinem Grund und Sinn suchend, ist der Beginn dieser philosophischen Haltung. Damit begann „das Sterben der Götter“: In allen Varianten griechischen Geistes war eine Kritik an überkommener Religion, am Mythos der Götterwelt, am Schein oder am falschen Sein des allzu selbstverständlich Gegebenen, an der Unvernunft möglich, angelegt und großenteils auch ausformuliert. Wissen stand tendenziell von vornherein gegen Glauben. Jedoch verstand die frühe griechische Philosophie sich nicht primär als Religionskritik: Obwohl viele ihrer Denker die Götter und ihre Mythen als Illusion sahen und beschrieben, bekämpften sie die praktische Religionsausübung kaum. Auch für Skeptiker, kritische Empiristen und Materialisten war die metaphysische Frage nach einem Weltgrund, Weltganzen und Sinn des Seins nicht erledigt und beschäftigte viele von ihnen zentral.

Die Vorsokratiker suchten den Urgrund aller Dinge (griech. ἀρχή, Arché) nicht jenseits der Welt, sondern in ihr. Damit entmythologisierten sie tendenziell die Griechische Mythologie.

Die Mythen Homers hatten „Okeanos“, die Theogonie Hesiods das Chaos als Ursprung allen Lebens, auch dessen der Götter, dargestellt. Dieser Mythos steht aus Sicht einiger Religionskritiker auch hinter der biblischen „Urflut“ (Gen 1,2). Thales von Milet (um 630–560 v. Chr.) machte daraus eine empirisch überprüfbare Aussage: Er sieht das Wasser als einheitlichen Urstoff, aus dem alle übrige Stoffe hervorgingen.

Sein Schüler Anaximander (um 610–547 v. Chr.) versucht, aus dem damals zugänglichen Wissen erstmals ein konsistentes Weltmodell abzuleiten. Er kommt vom Gedanken der unendlichen Zeit (mythisch im Gott Kronos symbolisiert), vom ewigen Werden und Vergehen, zum negativen Grenzbegriff des Grenzenlosen (griech. ἄπειρον, "apeiron"): Der Urgrund könne kein bekannter Stoff sein, da alle Stoffe zeitlicher Veränderung unterlägen. Er müsse in allem enthalten sein, ohne je wahrnehmbar und bestimmbar zu werden. Das schließe alle positiven Aussagemöglichkeiten über ihn aus. Dies nahm die seit dem Neuplatonismus verbreitete Negative Theologie vorweg.

Für Anaximenes (um 585–524 v. Chr.) dagegen muss die grenzenlose Ursubstanz bestimmbar sein, da sonst aus ihr keine konkreten Dinge entstehen könnten. Er findet sie in der „Luft“, die alle Substanzen durchdringe und als ständige Bewegung ihre Qualitätsänderungen bewirke.

Pythagoras (ca. 580–500 v. Chr.) führt die Veränderungen der Dinge nicht auf einen Urstoff, sondern auf mathematisch berechenbare Gesetzmäßigkeiten zurück. Diese seien dem Menschen erkennbar, weil seinem Geist das Zahlensystem innewohne. Damit nahm er Platons Ideenlehre vorweg. Er bekämpfte die Göttermythen Homers und lehrte eine unpersönliche Gottheit ohne menschliche Eigenschaften. Aber er glaubte auch an die zyklische Seelenwanderung und übernahm Rituale aus dem Apollos- und Orpheuskult.

Xenophanes aus Kolophon (570–475 v. Chr.) gilt als erster antiker Religionskritiker. Aus sehr verschiedenen Gottesvorstellungen, die ihm bei Auslandsreisen begegneten, folgert er, diese müssten von den jeweiligen Gläubigen geprägt sein (Fragment 27): „Die Äthiopier behaupten, ihre Götter seien stumpfnasig und schwarz, die Thraker, blauäugig und blond.“ Jedes Volk stelle sich Gott also analog zum eigenen Aussehen vor: Damit nahm er Feuerbachs Projektionsverdacht im Kern vorweg. Er kritisiert den Anthropomorphismus der Mythen Homers und Hesiods, der den Göttern unsittliches Verhalten wie Ehebruch, Eifersucht, Betrug usw. zutraue (Fragment 26).

Dabei argumentiert er nicht atheistisch, sondern ethisch gegen falsche Gottesbilder und die Vielheit der Götter. Die Naturereignisse seien nicht göttlichen Ursprungs. Aber „in“, „hinter“ oder „über“ allen Gottesbildern sei das Göttliche als vollkommenes Wesen zu erahnen (Fragment 34): bei aller unbewussten Evidenz unsagbar und unbeschreibbar. Dieses einheitsstiftende Urprinzip müsse ein einziges, umfassendes, alle Vorstellungen übersteigendes reines Geistwesen (griech. νοῦς, "nous") sein: darin der Kugelform ähnlich (Fragment 37). Absolutes Wissen darüber sei aber in der Welt der ständig wandelbaren Dinge unmöglich (Fragment 38): „Denn sogar wenn es einem in außerordentlichem Maße gelungen wäre, Vollkommenes zu sagen, würde er sich dessen trotzdem nicht bewusst sein: bei allen Dingen gibt es nur Annahmen.“ Gott werde durch menschliches Reden über Gott unweigerlich begrenzt.

Sein Schüler Parmenides von Elea (geb. um 540, Todesjahr unbekannt) stellt den Begriff des Seins (ὤν) ins Zentrum seiner Reflexion und gibt der abendländischen Philosophie damit jahrhundertelang ihr Thema vor. Er geht vom Denken aus und schließt in einem klassischen Syllogismus das Nichtsein als undenkbar aus: Denken bedeutet Seiendes denken und ist nur als logisches Urteilen in Form des Aussagesatzes (Subjekt – Prädikat) möglich. Das „ist“ im Urteilssatz beweist das Dasein des gedachten Gegenstandes. Das „Sein“ ist nicht nur Objekt, sondern auch Mittel des Denkens, ja es denkt selbst. Damit nimmt Parmenides den ontologischen Gottesbeweis vorweg.

Empedokles (um 483–423 v. Chr.) erkennt nur dem Stoff Sein zu, das bleibt. Werden ist Bewegung, die als Kraft auf quantitativ beständigen Stoff wirkt: Das begründete die mechanische Physik. Aber die Vielfalt des Werdens lasse sich unmöglich aus einem einzigen Urstoff erklären. So lehrt er die vier Elemente Feuer-Wasser-Erde-Luft, die sich ständig neu verbinden und trennen und so Werden und Vergehen erzeugen, ohne je das Gesetz der Stofferhaltung zu brechen: Das begründete die Chemie und verweist auf den Massenerhaltungssatz. Doch auch er hielt die Idee einer nichtstofflichen Geisterwelt fest und glaubte an die Seelenwanderung als Strafe des Schicksals für in diesem Leben begangene Verbrechen.

Anaxagoras (um 500–428 v. Chr.) fragt nach dem wahren „ersten Bewegenden“ des mechanischen Prozesses. Er lehrt feste Elementarteilchen (σπέρματα, "spermata"), aus denen sich auch Feuer und Luft zusammensetzen. Alles entstehe aus allem, indem es sich neu mische und scheide; Eigenschaften seien nur Mischungsverhältnisse. Umso mehr frage sich, was zur ständigen Neuordnung der Teilchen den Anstoß gebe: Es könne nicht in der Materie liegen, sondern müsse Geist (νοῦς) sein, der alle Dinge sinnvoll und zweckmäßig ordne. Er sah diese einfache, mächtige und wissende Essenz aber nicht als Gottheit, sondern als feinsten aller Stoffe, der so von allen übrigen Substanzen geschieden sei und sie doch alle umgebe, durchflute und umherwirble. Nur der Mensch habe Anteil an diesem Wesen; darum könne er es erkennen und die Welt der Dinge, Pflanzen und Tiere beherrschen. Anaxagoras wurde als „Atheist“ angeklagt und verließ deshalb Athen.

Demokrit (460–390 v. Chr.) lehrte erstmals eine konsistente materialistische Weltanschauung mit vier Grundaussagen:
Darauf baut er sein Weltbild auf, das etwa moderne Theorien der Planetenentstehung und den biologischen "survival of the fittest" (das Überleben der am besten Angepassten) schon erstaunlich genau vordachte. Für Götter und Geister war nun kein Raum mehr: Auch die Seele sei feinstofflich und zerstreue sich nach dem Tod des Einzellebens.

Epikur (341–270 v. Chr.) gibt erstmals eine rationale Erklärung für das Entstehen der Religion: Ihre Lehren seien nur ein Abbild menschlicher Ideen, die keine äußeren Einwirkungen zu ihrer Erklärung benötigen. Die Götter der griechischen Mythologie erwiesen sich durch ihre anthropomorphen (menschenähnlichen) Züge als Wunschgebilde. Diese Kritik trifft teilweise und indirekt – da Epikur sie nicht ausdrücklich darauf bezog – auch einzelne Gottesbilder des Alten Testaments, die den personalen Schöpfergott mit menschlichen Eigenschaften ausstatten und in bewusst menschlicher Sprache auch vom „eifersüchtigen“, „zornigen“, „reuigen“ und „liebenden“ Gott sprechen.

Lactantius überliefert ein prägnantes Argument eines unbekannten Skeptikers gegen die Theodizee, das er irrtümlich Epikur zuschrieb: Gott sei entweder nicht allmächtig oder nicht wohlwollend, da sonst die Übel in der Welt nicht bestehen könnten.

Der römische Dichterphilosoph Lukrez (ca. 98–55 v. Chr.), ein Anhänger Epikurs, führt in seinem Werk "Über die Natur der Welt" 28 Beweise für die Nichtexistenz der Götter aus. Er führt Religion auf menschliche Furcht zurück, die nur durch „des Geistes lebendige Kraft“ besiegt werden könne.

Die Sophisten betrieben eine Religionskritik durch aufklärende Rhetorik. Oft waren sie geschulte Anwälte vor Gericht oder zogen als Wanderlehrer umher, um die Bevölkerung öffentlich zu bilden.

Protagoras (481–411) wollte nach eigener Aussage „das Starke schwach und das Schwache stark“ machen. Er vertrat eine subjektivistische Erkenntnistheorie, die bereits sehr modern anmutet. „Wahrheit“ hänge immer vom Betrachter ab (Fragment 1): „Wie alles einzelne mir erscheint, so ist es für mich, wie dir, so für dich … Der Mensch ist das Maß aller Dinge, der seienden, dass sie sind, der nichtseienden, dass sie nicht sind.“
Darum bestritt er über Xenophanes hinaus auch die Notwendigkeit eines Gottwesens hinter allen Göttern. Menschen könnten überhaupt keine Aussagen über Götter machen, weil sie in der veränderlichen Welt keine allgemeinen dauerhaften Erkenntniskriterien dafür besäßen (Fragment 4):
Damit ließ er offen, ob es Götter gebe oder nicht, weil wir sie nicht erkennen können.

Man warf Protagoras schon zu Lebzeiten vor, dass er diese Not des Nichtwissens zur Tugend des rein subjektiven Behauptens transformiere und über alles Wissen allein entscheide, indem er sich selbst zum einzigen Maßstab allen Erkennens erhebe. Andere deuten seine Aussagen nicht als überhebliche Anmaßung, sondern als Hinweis auf einen Zwang: Der Mensch müsse sich zum Maß seines Wissens und Handelns machen, da er zunächst gar keinen anderen Maßstab habe. Damit wurde Protagoras auch für die subjektive Begründung von positiver, nicht absoluter Religion herangezogen. Denn auch jede „Offenbarung“ ereigne sich innerhalb des menschlichen Wahrnehmungsbereichs und sei nur individuell erfahrbar.

Platon wirft den Sophisten vor, Seelen nicht zu führen, sondern zu fangen, lediglich um recht zu haben und davon zu leben. Die Platoniker kritisieren also das Betreiben von Religionskritik als Gewerbe. Sie selbst gehen von ewigen, unveränderlichen, dem menschlichen Bewusstsein inhärenten bzw. angeborenen Ideen aus, die sich auch kritisch gegen falschen Schein und zu Mythen verdinglichte Wahrheit richten.

Aristoteles geht zwar gegen Platon wie Materialisten und Sophisten von der empirischen Wahrnehmung aus, schreitet aber zur metaphysischen Frage nach der "prima causa" (ersten Ursache) alles Seins fort. Von da aus kritisiert er sowohl die gewöhnliche Naturreligion, die an eine Vielzahl menschenähnlicher Götter glaubt, als auch das mechanistische und atomistische Weltbild, das der Vielfalt der Erscheinungen nicht gerecht werde. Sein Begriff des notwendigen, unpersönlichen, transzendenten „unbewegten Bewegers“ als Weltgrund kritisiert alle Ursprungsideen, die das Göttliche als Teil der Welt denken.

Platon und Aristoteles gelten nicht als Vorläufer der modernen Religionskritik, da diese die metaphysischen Voraussetzungen ihrer Kritik an konkreten Erscheinungsformen von Religion ihrerseits kritisiert.

Die Stoa kritisiert mit ihrer aus Naturbeobachtung gewonnenen Idee der Vorsehung "(providentia dei)" jene Gottesvorstellungen, die einen Weltgrund von der Welt getrennt denken, als rationale Erfindung.

Poseidonios von Apameia (um 135–50 v. Chr.) gilt als Begründer der mittleren Stoa, der ältere stoische Lehren kritisch revidierte. Sein nur bruchstückhaft von Schülern und Nachfolgern überliefertes Werk "Über die Götter" legte Cicero seinem Werk "De natura deorum" („Vom Wesen der Götter“) zugrunde. Danach unterschied Poseidonios streng zwischen einer angeborenen natürlichen Religiosität aller Menschen, die die Vorstellung von etwas Göttlichem in der Vernunft begründe, und den historisch und sozial erworbenen religiösen Vorstellungen konkret bestehender Kulte, die er ablehnte.

Diese Form der immanenten Religionskritik an empirischer Religionsausübung durch Rückführung auf natürliche Einsicht in den erfahrbaren Zusammenhang alles Weltgeschehens war im griechisch-römischen Hellenismus die Regel und bestimmte auch die spätere „Vernunftreligion“ der Aufklärung mit.

Der Skeptizismus kritisiert die metaphysische Kosmologie wie die empirische Teleologie (Zielgerichtetheit) als menschliche Konstrukte, die an der widersprüchlichen Naturerfahrung zerbrechen. Er bestreitet die Möglichkeit eines metaphysischen Rückschlussverfahrens zum Erweis eines Weltgrundes oder der Sinnhaftigkeit der Welt.

Die Zielrichtung skeptischer Kritik ist also divergent: Sie kann den Gottes"begriff" (als Reflexion auf den Weltgrund) ebenso bestreiten wie die Gottes"erfahrung" (als Reflexion auf das eigene Welterleben). Sie bestreitet in jedem Fall die Notwendigkeit eines – wie auch immer gearteten – Weltgrundes (genannt „Gott“) für die Welt und den Menschen. Dabei ist der Ansatz dieser Kritik seinerseits empirisch:



Im Ergebnis kommt diese philosophische Kritik jedoch nicht über die allgemeine Skepsis an allen positiven Glaubensaussagen hinaus: Religion als Begegnung des Menschen mit einer existierenden oder gedachten Transzendenz sei philosophisch weder zu beweisen noch zu widerlegen, vgl. Skeptizismus.

Die christliche Theologie versuchte seit der Apologetik (2. Jahrhundert), christliche Glaubenssätze mit einem empirisch-metaphysischen Weltbild auszugleichen. Dabei formulierte sie mehr oder weniger konsistente Grundannahmen über Natur, Welt und Mensch als Ganzes, über ihre Entstehung und Zukunft („Erlösung“). Sofern sie damit der allgemeinen Erfahrung zugängliche Wahrheit beanspruchte, geriet diese auf der Ebene von Tatsachenprüfung unvermeidbar in die Kritik. So kritisierten bereits Kelsos und Porphyrios das Christentum für aus ihrer Sicht widernatürliche und absurde Behauptungen über Gott, Welt und Mensch mit dem Ziel einer vernünftigeren, den gebildeten Römern eher akzeptablen Religion.

Thomas von Aquin integriert Offenbarung und rationale Welterkenntnis in ein gemeinsames umfassendes Lehrsystem: Natürliches Fragen nach dem Weltgrund komme bereits zur allgemeinen Erkenntnis eines höchsten Wesens (vgl. Natürliche Theologie), christlicher Glaube ergänze diese durch das geoffenbarte Wissen, wer und was dieses Wesen ist und will. Dabei legte die thomistische Scholastik sich auf das geozentrische Weltbild fest, das seit Pythagoras und Aristoteles als „bewiesen“ galt.

Mit Nicolaus Copernicus, Johannes Kepler, Giordano Bruno, Galileo Galilei und Isaac Newton zerbrachen große Teile dieses Weltbildes und begann die Emanzipation der experimentellen Naturwissenschaften vom Wahrheitsmonopol der mittelalterlichen katholischen Kirche. Hinzu kam der allmähliche Wandel der ständischen Gesellschaftsordnung, in der Kleriker und Laien auseinandertraten und letztere allmählich einen höheren Grad an Allgemeinbildung erwarben.

Jean Bodin (1530–1596) ließ in seinem fiktiven "Colloqium heptaplomeres" (1593) je einen Juden, Muslim, Katholiken, Lutheraner und Calvinisten in einen Dialog mit einem Anhänger der natürlichen Religion treten. Dabei kritisiert dieser die christlichen Dogmen der Erbsünde, Trinitätslehre und Inkarnation. Als positive Inhalte dieser von ihm bevorzugten "religio naturalis" bestimmt Bodin das Dasein eines Gottes, Willensfreiheit, Vergeltung der Taten jedes Vernunftwesens nach seinem Tod und Erkennbarkeit von Naturgesetzen.

Eine ähnliche Kritik verfasste Edward Herbert (1581–1648) in zwei Traktaten 1624 und 1645, in denen er eine natürliche Religion ausschließlich aus natürlicher Wahrheitserkenntnis ableitet, nicht aus Offenbarungsurkunden und religiöser Überlieferung. Die angeborene Fähigkeit zu Allgemeinbegriffen ergebe fünf Wahrheiten: Eine höchste Gottheit existiere, sie müsse verehrt werden, dazu gehörten immer Tugend und Frömmigkeit, Laster und Verbrechen seien durch Reue zu sühnen, es gebe Lohn oder Strafe nach diesem Leben.

Mit René Descartes (1596–1650) gewinnt die Spannung zwischen Philosophie und Theologie an Schärfe: Diese war seit der Reformation gegeben, da Martin Luther den Christusglauben gegen allgemeines Weltwissen, abstraktes Gotteswissen und zweckgebundenes Herrschaftswissen stellte. Nun brach Descartes die scholastische Synthese von natürlicher Theologie (bzw. philosophischer Metaphysik) und spezieller (christlicher) Offenbarung auch von der Seite des nicht von vornherein gläubigen Denkens her. Erstmals begründet das denkende Subjekt Selbstbewusstsein autonom. Von der intuitiven Erfahrung des "Cogito ergo sum" („Ich denke, also bin ich.“) aus stütze der Begriff Gottes nur noch sekundär menschliche Selbstgewissheit.

Stärker als die meisten englisch- und deutschsprachigen Aufklärer strebte die Mehrheit der französischen Enzyklopädisten des 18. Jahrhunderts nicht bloß die Überwindung konfessioneller Streitereien, sondern die Destruktion aller bestehenden Religionen zugunsten des erklärten Atheismus an. Sie orientierten sich dabei am Materialismus Demokrits und Epikurs. Vertreter dieser Denkrichtung waren unter anderen Jean Meslier (1664–1729), Julien Offray de La Mettrie (1709–1751), Denis Diderot (1713–1784); Claude Adrien Helvétius (1715–1771) und Paul Henri Thiry d’Holbach (1723–1789).

Einer der schärfsten Kirchenkritiker damals war François-Marie Arouet (1694–1778), der sich "Voltaire" nannte. Er bekämpfte vor allem den Machtanspruch der katholischen Kirche und das Bündnis des Klerus mit Adel und Absolutismus, rief zur Zerstörung des Papsttums auf („Écrasez l’infâme!“) und trat gegen religiöse Indoktrination für Glaubens- und Gewissensfreiheit ein. Voltaire bekannte sich jedoch nicht zum Atheismus, sondern zum Deismus, weil er den Glauben an einen strafenden Gott als beste Basis für ein soziales Leben nach moralischen Grundsätzen hielt („Wenn Gott nicht existierte, müsste man ihn erfinden.“). Im Jahr 1762 ließ Voltaire, ohne sich als Herausgeber zu erkennen zu geben, Auszüge aus Jean Mesliers atheistischen „Testament“ publizieren. Diese Passagen sind im Original so voll ätzender Kritik, dass Voltaire sie umschrieb und abmilderte, was ihren ursprünglichen Inhalt zum Teil entstellte.

David Hume (1711–1776) begründete im Gefolge von Roger Bacon (1214–1294) und Francis Bacon (1561–1626) den strengen rationalen Empirismus, der sich auch gegen die englischen Deisten seiner Zeit wandte. Sein Hauptwerk "Inquiry concerning human understanding" (zwei Teile, erschienen 1748 und 1751) war eine radikale Erkenntniskritik an allen rationalen Begründungsversuchen der Religion. Es fand in England wenig Beachtung, beeinflusste aber Immanuel Kant stark.

In seiner 1777 erschienenen Schrift "Essay on the Immortality of the Soul" (Unsterblichkeit der Seele) resümierte Hume seine Hauptauffassungen:

Ebenso wenig wie zwischen Materie und Geist könne zuverlässig zwischen Ursache und Wirkung einer Sache unterschieden werden. Da die sinnliche Erfahrung die einzige Erkenntnisquelle des Menschen sei, sei nicht auszuschließen, „ob nicht die Materie durch ihre Struktur oder Anordnung die Ursache des Gedankens sein kann.“ Damit erklärte Hume mit einem Schlag den lange behaupteten fundamentalen Unterschied zwischen Materialismus und Idealismus für aus sich heraus hinfällig.

Er versuchte, die ethische Begründung von Religion zu zerschlagen:

Der Deismus, der Gott als Anstoß zum in sich nahezu mechanisch abrollenden Weltgeschehen auffasste, verliere damit seinen Anspruch auf ethische Lebensführung, weil nichts, was geschieht, von Gottes ursprünglichem Wollen unabhängig sein könne:

Während also Gottes Absichten mit dem Menschen – ein Endgericht als traditionelle religiöse Vorstellung vorausgesetzt – verborgen, sinnlos und unmenschlich erscheinen, folgerte Hume im Blick auf die Natur:

Kant (1724–1804) ist kein reiner Religionskritiker. Seine „Kritik der reinen Vernunft“ (KrV) war viel umfassender: Alle metaphysischen Gottesbeweise überschritten unzulässig die kategorialen Grenzen menschlicher Vernunft. Er legt vor allem die Unmöglichkeit des ontologischen Rückschlusses von der Essenz zur Existenz Gottes (Anselm von Canterbury) dar, auf den er die übrigen Gottesbeweise zurückführt. Diese Rückführung ist umstritten. Doch seither ist die moderne Philosophie von deutlicher Distanz zu jeder Art von Metaphysik geprägt und sieht religiöse Deutungsmuster der Wirklichkeit unter dem Vorzeichen des Irrealen und Irrationalen (vgl. auch Kritizismus).

Im Blick auf die Moral, die Kant allein durch die Vernunft zu begründen versucht, billigt er der Religion jedoch eine mündiges Menschsein fördernde Rolle zu: Denn „es ist notwendig, dass unser ganzer Lebenswandel sittlichen Maximen untergeordnet werde“. Dabei benötige die Eigenart des menschlichen Denkens eine „wirkende Ursache“ sowie einen „entsprechenden Ausgang, es sei in diesem, oder einem anderen Leben“ (Kant 1787, B 840–841). „Ohne also einen Gott und eine für uns jetzt nicht sichtbare, aber gehoffte Welt, sind die herrlichen Ideen der Sittlichkeit zwar Gegenstände des Beifalls und der Bewunderung, aber nicht Triebfedern des Vorsatzes und der Ausübung“ (Kant 1787, B 841). Die Idee eines Gottes hat für Kant also im Hinblick auf Moral eine motivierende, nicht aber begründende Funktion. Gott ist für Kant in der Kritik der praktischen Vernunft ein notwendiges „Postulat“ der Vernunft, ohne dass ihr deswegen auch objektive Realität zukäme. Kants abstrakter, philosophischer Gottesbegriff ist jedoch nicht identisch mit Vorstellungen beispielsweise eines persönlichen Gottes oder eines Gottes, der in die Welt eingreifen würde.

Die Moralgesetze und das gute Handeln hier in der Welt, nicht übernatürliche Aspekte, sind für Kant der eigentliche und einzige Sinn und Zweck der Religion. Diese Gesetze waren es, „deren innere praktische Notwendigkeit uns zu der Voraussetzung einer selbständigen Ursache, oder eines weisen Weltregierers führte, um jenen Gesetzen Effekt zu geben“ (KrV, B 846). Der vorausgesetzte Gott dürfe darum nicht als ein neuer Gegenstand oder ein reales Sein angesehen werden, von dem umgekehrt dann die moralischen Gesetze abgeleitet werden. Das wäre nach Kant „schwärmerisch oder wohl gar frevelhaft“ und würde „die letzten Zwecke der Vernunft verkehren und vereiteln“ (Kant, KrV, B 841). In dieser fatalen Verdrehung der tatsächlichen Verhältnisse werde das Gottesbild vom Hilfsmittel zum eigentlichen Zweck und das gute Handeln zum bloßen Hilfsmittel der Gottesverehrung. Die Religion kann nach Kant ihren Zweck in der Welt in Übereinstimmung mit der Vernunft nur erfüllen, wenn gilt (KrV, B 847):
Das Absolute ist für Kant wie in der negativen Theologie unbestimmbar. Wo es zu definieren versucht werde, komme es zwangsläufig zu Streit, Widersprüchen und Spaltungen darüber, welche der vielen verschiedenen und sich widersprechenden Bilder und Bestimmungen des Absoluten die einzig wahren und realen sind. Diese Auseinandersetzungen liefen dann dem moralisch guten Handeln in der Welt als Hauptthema und eigentlichem Sinn der Religion entgegen. Nur wenn deshalb restlos alle Gottesbilder zu den bloßen und austauschbaren Hilfsvorstellungen relativiert werden, die sie nach Kant sind, würden die religiösen Auseinandersetzungen beseitigt und damit der eigentliche Sinn und Zweck der heutigen Religion in der Welt erfüllt: das gute und sittliche Handeln auch auf der Ebene des interreligiösen Dialoges, statt eines Kampfes der Kulturen um die einzig wahre und reale Gottesvorstellung.

In seiner religionsphilosophischen Schrift Die Religion innerhalb der Grenzen der bloßen Vernunft forderte Kant eine Vernunftreligion, deren Grundsätze prinzipiell allein auf Vernunft und nicht auf „statutarischem“, also auf bloßen Dogmen gegründeten Glauben beruhen:

Ebenso wandte er sich in scharfer Form gegen verschiedene Arten des religiösen Kultes wie etwa Gebete, Beichten oder Gottesdienste. Die einzige Funktion einer Religion sah Kant in der Gewährleistung eines (durch Vernunft geprüften) moralischen Lebenswandels:

Eine Religion, die sich auf Offenbarung gründet, lehnte Kant ab.

→ "siehe Hauptartikel: Die Religion innerhalb der Grenzen der bloßen Vernunft"

Gotthold Ephraim Lessing (1729–1781) betrachtet Religion in Gestalt von Judentum, Christentum und Islam einerseits als historischen Ursprung, andererseits als zu überwindende Vorstufe einer selbständigen Vernunftreligion. Dazu veröffentlichte er 1777 auch die "Apologie oder Schutzschrift für die vernünftigen Verehrer Gottes" von Hermann Samuel Reimarus (1699–1768) als "Fragmente eines Ungenannten".

Der Orientalist Reimarus hatte das Christentum zu Lebzeiten zwar als Vernunftreligion gegen die radikaleren französischen Aufklärer verteidigt, in dieser posthum veröffentlichten Spätschrift aber begonnen, mithilfe der Bibelkritik christliche Theologie und Dogmen als Priesterbetrug zur Unterdrückung des armen Volkes zu bekämpfen. Er griff darin vor allem den Glauben an die Wunder Jesu, die Auferstehung Jesu Christi und die damals auch von Protestanten dogmatisierte Lehre der Verbalinspiration als frommen Betrug der Apostel an.

Obwohl Lessing selbst keine prinzipiell christentumsfeindliche Haltung vertrat, löste seine Veröffentlichung den jahrelangen Fragmentenstreit mit Vertretern der Lutherischen Orthodoxie aus, in dessen Verlauf Lessing ein Publikationsverbot erhielt. Er verfasste daraufhin 1779 das Drama Nathan der Weise, in dem er Toleranz und gegenseitige Achtung von den drei monotheistischen Religionen fordert und seinem Freund, dem jüdischen Religionsphilosophen Moses Mendelssohn, ein Denkmal setzte. Nach der Ringparabel ist von Menschen nicht zu entscheiden, wer Gott in der besten Form verehrt.

Andererseits fordert Lessing die Aufklärung des in Religionssystemen gefesselten Kinderglaubens zu Gunsten eines zukünftigen sittlichen Humanismus ohne spezifisch biblische Gottesoffenbarung („Die Erziehung des Menschengeschlechts“ 1780).

Friedrich Schleiermacher (1768–1834) versucht – in der romantischen Gegenbewegung zum Rationalismus der Aufklärer –, das religiöse „Gefühl der schlechthinnigen Abhängigkeit“ den Gebildeten wieder nahezubringen. Er sieht das subjektive, nicht begrifflich fassbare Erleben der Unendlichkeit als rein rezeptive, passive Form des Selbstbewusstseins, die sich jedem aktiven kritischen Zugriff des Verstandes entziehe. Damit greift er in gewisser Weise die mittelalterliche Mystik mit ihrer Kritik an veräußerlichten Religionsformen wieder auf. Er kritisiert von da aus den Dogmatismus und Konfessionalismus der protestantischen Staatskirchen, verlangt aber keine institutionelle Trennung von Kirche und Staat.

Johann Gottlieb Fichte (1762–1814) gilt als Begründer des deutschen Idealismus. Er schreibt 1792 einen "Versuch einer Critik aller Offenbarung", der die Prinzipien Kants konsequent durchführt, um daraufhin das Ich-Bewusstsein spekulativ zu begründen. Auch in dieser Reflexion des Menschen auf den Grund seines Selbstbewusstseins kommt die Religion zuletzt wieder in Betracht: In der unausdenkbaren und unaufgebbaren Idee des Absoluten (des Unbedingten) findet die idealistische Vernunft ihren letzten Grund.

Georg Wilhelm Friedrich Hegel (1770–1831) versucht, diesen Grund auch für das Begreifen der Weltgeschichte geltend zu machen und das begrenzte subjektive Selbstbewusstsein – den religiösen Glauben – als Teilmoment der Selbstentfaltung des zu sich kommenden Weltgeistes dialektisch „aufzuheben“ "(Phänomenologie des Geistes)". Damit macht er gegen die Romantiker die Arbeit des Begreifens, den Anspruch der Wahrheit auf das Ganze – die Totalität der erfahrbaren Dinge inklusive der menschlichen Geschichte – wieder geltend.

Während Hegel die notwendige Kritik der partikularen Religion mit ihrem vernünftig zu begreifenden Sinn konstruktiv vermitteln und so bewahren wollte, traten beide Seiten bei einigen seiner Schüler bald auseinander.

Auguste Comte (1798–1857) begründete nach seinem Bruch mit dem Frühsozialismus Henri de Saint-Simons mit seinem 1842 veröffentlichten Hauptwerk "Cours de philosophie positive" den religionskritischen Positivismus.

Die Grundidee Comtes ist, dass Religion nur in ihren Phänomenen existiere und das Wissen darüber nur relativ, nie absolut sein könne. Die Frage nach dem „Wesen“ der Religion und ihrer letzten „Ursache“ sei darum sinnlos. Man könne nur nach den Beziehungen zu anderen wahrnehmbaren religiösen Erscheinungen fragen, ihre historische Aufeinanderfolge und ihre Ähnlichkeiten feststellen (Religionsgeschichte). Sofern darin Gesetzmäßigkeiten erkennbar würden, scheide die Religionswissenschaft alle theologischen und metaphysischen Begriffe aus und werde positiv.

Comte sieht diese ausgeschiedenen Begriffe als Vorstadien der wissenschaftlichen Beschreibung der Religion im Sinne einer notwendigen Entwicklung: Der Fetischismus der primitiven „Naturreligionen“ halte Einzelobjekte für lebendig, der Polytheismus nehme eine Vielzahl unsichtbarer Wesen als Ursache der Naturerscheinungen an, der Theismus wie die Metaphysik reduziere dieses Wesen weiter auf abstrakte Kräfte, Urprinzipien, Natureigenschaften und der Monotheismus führe diese auf Willensakte eines einzigen unsichtbaren göttlichen Wesens zurück.

Der Positivismus erkenne darin bloße Scheinbarkeiten, die er auf strenge Gesetzmäßigkeiten zurückführe. Von der Religion selbst her führe ein notwendiger Erkenntnisweg zur reinen Wissenschaft der Phänomene. Jede Einzelwissenschaft mache diese Entwicklung vom naiv-theologischen zum reflektiert-metaphysischen zum positiv-beschreibenden Stadium durch.

Der „Linkshegelianer“ Ludwig Feuerbach (1804–1872) wendet den zu-sich-selbst-kommenden Begriff in seinem Werk Das Wesen des Christentums 1841 kritisch gegen die Religion und will sie als Projektion entlarven: „Gott“ sei nur der an den Himmel projizierte Selbstausdruck des endlichen Selbstbewusstseins, das sich Unendlichkeit ersehne. Mit der Vorstellung Gottes stelle der Mensch sich sein eigenes Wesen gegenüber, mache es sich als Objekt seiner Sehnsucht gegenständlich anschaulich:
Feuerbach entfaltete diese Kritik in den weiteren Auflagen des Werks (1843, 1849) vor allem an Zentralgedanken der Theologie Martin Luthers: Die Inkarnation – „Gott wird endlicher Mensch“ – sei eigentlich „nicht anderes als“ der verkehrte Wunsch des Menschen, unendlich und unsterblich – wie Gott – zu werden. Er griff dabei ausdrücklich die Kritik Epikurs am Anthropomorphismus der Religion wie auch das Drei-Stadien-Gesetz von Lessing und Comte (Religion als „Kindheitsstadium“ der Menschheitsentwicklung) auf.

Indem der Mensch in Gott sich selbst wiedererkenne, werde er sich seiner religiösen Sehnsucht als Entfremdung gewahr. Indem er sich als den Produzenten Gottes entdecke, könne seine in der Religion fehlgeleitete Vernunft zur Humanisierung freigesetzt werden: In der zwischenmenschlichen Liebe finde der Mensch seine wahre Erfüllung. Damit lehnt Feuerbach das religiöse Element des menschlichen Selbstbewusstseins nicht "per se" ab, will es aber „übersetzen“ und einsetzen für die Gestaltung eines humanen Zusammenlebens.

Religionskritik ist für Feuerbach also notwendig, um dem religiösen Bewusstsein die Hingabe an ein fremdes Scheinwesen als von ihm produzierten Verblendungszusammenhang aufzudecken. Dann werde Religion durch sinnlich-irdische Liebe zu den Mitmenschen ersetzbar und tendenziell überflüssig. Sie könne und müsse ebenso vergehen wie der an der Unendlichkeit des eigenen Selbst hängende Egoismus, der in der Vorstellung Gottes einsame Selbstbefriedigung suche und finde.

Anders als Hegel zielt Feuerbach also nicht auf die Erkenntnis eines absoluten Geistes, der als an-und-für-sich-seiende oder -werdende Weltvernunft gedacht wird und überindividuell selbsttätig sein und bleiben soll, sondern auf das endgültige Verschwinden der Religion im humanen Fortschritt der Menschheit. Diese, nicht der Einzelne, ist für ihn in Wahrheit unendlich. Nur durch Liebe zur Menschheit kann das Individuum die religiöse Selbstentzweiung aufheben; nur durch Anerkennung seiner Endlichkeit – denn die Sterblichkeit ist das, was alle Menschen zu einer Gattung verbindet – wird er zur Menschlichkeit fähig.

Karl Marx (1818–1883) begreift im Anschluss an Feuerbach Religionskritik als die Voraussetzung aller Kritik. In seinen Frühschriften (MEW I, S. 378) weist er auf die Doppelnatur von Religion hin:
Die Ambivalenz religiösen Bewusstseins ist für Marx – wie für Feuerbach – Ausdruck eines grundlegenden Mangels im sozialen Leben und kann sich sowohl als Protest gegen das Elend wie als Flucht aus dem Elend in einen illusionären Rausch äußern. In beidem verbirgt sich jedoch eine fundamentale Unfähigkeit, dessen wahre Ursachen aufzudecken und sie praktisch zu bewältigen. Religion ist für Marx ebenso wie andere Ideologien ein „verkehrtes Bewusstsein“, das die gesellschaftlichen Verhältnisse erzeugen, die ihnen aber nur das abstrakte Gegenbild einer irrealen besseren Welt gegenüberstellen.

Dieses Bewusstsein kann jedoch nur mit der praktischen Umwälzung jener Verhältnisse, die immer neue Illusionen über sich produzieren, um fortzubestehen, aufgehoben und durch ein wahrhaft menschliches Bewusstsein der Realität abgelöst werden (MEW I, S. 379):
Darum kritisiert Marx auch Feuerbachs und Hegels rein individualistischen, dem Idealismus verhafteten Ansatz und stellt ihnen seine berühmten 11 „Thesen über Feuerbach“ entgegen, die in der 11. These gipfeln:
Von da aus geht Marx nun zur Kritik der politischen Ökonomie, also zur Analyse der auf gesetzmäßiger Ausbeutung gegründeten Klassengesellschaft über. Er kritisiert jene Religionskritiker, die diesen Sprung nicht mitvollziehen und sich an der äußeren Erscheinung der Religion abarbeiten. Mit der Überwindung des Kapitalismus, so erwartet er, wird auch die Religion ihre scheinhafte Notwendigkeit verlieren und – wie der Staat, dessen soziales Ferment sie ja ist – in der klassenlosen Gesellschaft „absterben“.

Die marxsche Religionskritik ist also kein Selbstzweck, sondern dient dem revolutionären Humanismus (MEW I, S. 385):

Max Weber (1864–1920) antwortete auf Marx mit einem eher geisteswissenschaftlichen und historischen Ansatz: Er sieht Religion in Gestalt des europäischen Protestantismus als Wegbereiter der modernen kapitalistischen Industriegesellschaft. Die „Lohnethik“ Johannes Calvins habe zu einer asketischen Verzichtshaltung und zum Aufschub unmittelbarer Bedürfnisbefriedigung beigetragen. Dies habe die Einführung von industriellen Fertigungsmethoden, Produktion von Überschüssen, Realisierung von Mehrwert in der neuen Massenproduktion ermöglicht. Anders als Marx sieht er darin nicht nur ein negatives Element von Klassenherrschaft, sondern auch ein Element des Fortschritts und größerer geistiger Freiheit des Individuums.

Friedrich Nietzsche (1844–1900) greift im Rahmen einer umfassenden Kulturphilosophie das von der Religion geprägte Menschenbild an, um dem Menschen einen Raum für neue Selbstbestimmung zu eröffnen. Er versucht, die Funktion religiöser Riten, Glaubensinhalte und Wertsetzungen sowohl auf individualpsychologischer als auch auf gesellschaftlicher Ebene zu bestimmen. Das beinhaltet oft Kritik an religiösen Werten und priesterlichen Institutionen. Nietzsche sieht es als Aufgabe der zukünftigen Wissenschaft, die bisherigen Religionen, Moralen und Weltanschauungen zu analysieren; die zukünftige Philosophie müsse vor diesem Hintergrund neue Werte setzen.

Den christlichen Glauben sieht Nietzsche in Europa im Niedergang („Gott ist tot“). Die christliche Moral hebe sich (auch in seiner eigenen Philosophie) selbst auf, mit dem Glauben an einen Gott würden auch alle bisher geglaubten Werte sich entwerten. In der dekadenten Moderne enthülle sich die christlich-abendländische Tradition als im Kern nihilistisch. Auf diesen nun bevorstehenden „europäischen Nihilismus“, in dem er eine „Selbstverkleinerung des Menschen“ fürchtet, sucht Nietzsche eine Antwort. Seine vor allem in "Also sprach Zarathustra" gegebenen Hinweise auf neue Wertsetzungen („Wille zur Macht“, „Ewige Wiederkunft“, „Übermensch“, Wiederherstellung des Dionysoskults) bleiben allerdings vergleichsweise unklar.

In seiner Spätzeit spitzt Nietzsche seine Kritik auf den Kern der christlichen Botschaft zu ("Der Antichrist"): Er sieht im Christentum eine barbarische Schwächung aller edlen Eigenschaften des Menschen. Das Christentum habe beginnend mit Paulus eine lebensfeindliche Sklavenmoral gepredigt, sodass Nietzsche sich den Aufschwung zu einem höheren Menschsein nur als totales Abstreifen des abendländischen Christentums mit einer „Umwertung aller Werte“ vorstellen kann. Als Beispiel für seine ethische Umorientierung kann folgender Satz am Beginn seines Antichristen dienen: "Die Schwachen und Mißrathnen sollen zu Grunde gehn: erster Satz unsrer Menschenliebe. Und man soll ihnen noch dazu helfen."

David Friedrich Strauß war einer der ersten, der öffentlichkeitswirksam die These vertrat, dass in der Figur Jesu nicht ein historisches Geschehen mythisch übermalt worden sei, wovon die liberale Leben-Jesu-Forschung ausging, sondern viel radikaler meinte er, sei einem Mythos das Gewand der Historie übergeworfen worden. Laut Strauß seien Überlieferungen wie die Jungfrauengeburt, die sich nur bei Matthäus und Lukas findet, eine mythisch-poetische Legende. Die gesamte Darstellung der Evangelisten sei von Sagen und Legenden durchsetzt und damit weithin unhistorisch. Während andere Leben-Jesu Forscher mittels der „Abzugsmethode“ versuchten, durch Streichung legendarischer Züge einen "historischen Kern" aus den Evangelien extrahieren zu können, waren es vor allem Martin Kähler, William Wrede und Rudolf Bultmann, die den Evangelien ihre Tauglichkeit als Quellen für historische Zusammenhänge komplett absprachen.

Offen blieb aber die Frage, die Entstehung des Neuen Testaments und des Christentums aus dem „Christusmythos“ heraus zu erklären. In der Folge wurde von einigen Jesus-Mythos-Vertretern genauer untersucht, aus welchen vorausgehenden Mythologien das Urchristentum entstanden sein könnte. Als erste formulierten Charles François Dupuis und Constantin François Volney die These, Jesus von Nazaret sei keine historische Person, sondern ein Symbol für den Sonnenmythos und sein Erdendasein bilde die Winterphase im Sonnenzyklus ab. Ferner zeigten sie enge Parallelen zum Hinduismus und zur persischen Religion auf.

Mit der Entwicklung der historisch-kritischen Methode in den Geschichtswissenschaften und ihrer Anwendung auf die Bibel wurden eine tiefgehende Textanalyse möglich und es wurde herausgearbeitet, dass viele Bibelfragmente u. a. altägyptischen Quellen nahezu wörtlich glichen. Auch die klassische dogmatische Zuordnung der Autorschaft und der zeitliche Rahmen der Niederschrift wurde mittels historisch-kritischer Methoden hinterfragt. So kam die Tübinger Schule unter Ferdinand Christian Baur zu dem Schluss, dass 10 der Paulus zugeschriebenen 14 Paulusbriefe offenbar Pseudoepigraphien seien. Bruno Bauer, Arthur Drews sowie Anhänger der holländischen Schule der Radikalkritik folgerten gar, dass alle Paulusbriefe unecht seien und möglicherweise auch Paulus selbst nicht historisch sei.

Sigmund Freud gründete um 1900 die Psychoanalyse mit Anspruch auf wissenschaftliche Methodik. Er sieht religiöse Vorstellungen primär als Ausdruck unbewusster Prozesse und erklärt sie aus infantiler Abhängigkeit. Der religiöse Mensch sehe Gott als Vaterfigur, die er brauche, um die Verantwortung für ein selbstbestimmtes Leben abzugeben. Gottesglaube sei eine illusionäre Befriedigung des regressiven kindlichen Wunsches nach Geborgenheit, Sicherheit und Autorität.

Freud identifiziert dieses Gottesbild mit dem Über-Ich als jenem Teil der Psyche, der die normative Unterdrückung der Triebe, vor allem des Sexualtriebes leiste. Es kann als internalisierte Moral Schuldgefühle erzeugen und zu neurotischer Selbstentzweiung führen. Die Psychoanalyse versucht deshalb, dem Individuum seine eigenen verborgenen Wünsche aufzudecken und einen Teil der in frühkindlicher Sozialisation erworbenen Schuldgefühle zu nehmen.

Die Sublimation von Triebenergie sieht Freud nicht nur negativ, sondern als Antrieb für bedeutende Kulturleistungen des Menschen. Er betrachtete Kultur skeptisch (Das Unbehagen in der Kultur) und erwartete nicht, dass Religion sich aufheben lasse (Die Zukunft einer Illusion). Eine argumentative Widerlegung Gottes und aktive Bekämpfung religiöser Ausdrucksformen war nicht sein Anliegen, sondern die individuelle Integration von Über-Ich, Ich und Es in eine reife erwachsene Selbstannahme, die eine freie Entscheidungsfähigkeit in allen Lebensbereichen ermöglicht.

Freuds Schüler Wilhelm Reich versuchte, Psychoanalyse und Marxismus miteinander zu verbinden (Freudomarxismus) und damit das soziale Bedürfnis nach Religion besser zu verstehen. Er sieht die modernen Sexualneurosen als Ergebnis eines jahrtausendealten kulturellen Masochismus, der in Form von Religionen und anderen Leidensideologien die menschliche Bereitschaft zur Unterwerfung unter gesellschaftliche Macht- und Gewaltstrukturen prägt. Die mögliche Überwindung dieser Zwangsneurose sieht er in der freien Entfaltung der natürlichen Sexualität als wesentlichem Teil der Persönlichkeitsentwicklung. Durch seine enge Freundschaft mit Alexander Sutherland Neill ist Reich in der 68er-Bewegung zu einem beliebten Vorbild für die antiautoritäre Erziehung geworden.

Einen konsequenten naturwissenschaftlich begründeten Rationalismus vertritt der Mathematiker und Philosoph Bertrand Russell in seinem berühmten Essay "Why I Am Not A Christian" (1927). Die Grundlage der Religion sei die Angst – vor dem Mysteriösen, vor der Niederlage, vor dem Tod. Angst sei der Vater der Grausamkeit und so nehme es nicht Wunder, dass Grausamkeit und Religion historisch Hand in Hand gegangen seien. Die Konzeption Gottes entspringe einem altertümlichen orientalischen Despotismus, die des freien Menschen unwürdig sei. Die Welt brauche keine Religion, sondern eine furchtlose Perspektive und freie Intelligenz.

Autoren, die sich der Tradition des Logischen Empirismus verpflichtet fühlen (der frühe Ludwig Wittgenstein, Rudolf Carnap, Alfred Jules Ayer und andere), üben Kritik an der religiösen Sprache, deren Sätze für sie zu großen Teilen sinnlos sind. Sinnvolle Sätze seien entweder rein analytische Sätze und damit Tautologien oder empirisch-synthetische Sätze, die sich durch Erfahrung prinzipiell verifizieren lassen. Gehört ein Satz keiner dieser beiden Klassen an, so sei er ein Scheinsatz, d. h. weder wahr noch falsch, sondern sinnlos. Da die Sätze der Religion, insofern sie Ausdrücke wie „das Absolute“, „der absolute Geist“ oder „Gott“ verwenden, weder tautologisch noch verifizierbar sind, müsse auch ihnen jeglicher Sinn abgesprochen werden.

Von den Vertretern des Logischen Empirismus wird dabei nicht geleugnet, dass die Suche nach einem letzten Grund der Welt und des Lebens emotional verständlich sein mag. Der Rückgriff auf eine Gottheit erkläre aber nichts, da er nicht zu Hypothesen führt, die sich erfolgreich auf die Tatsachen anwenden lassen.

Karl Popper, der Begründer des Kritischen Rationalismus, betrachtete die Wirkung der christlichen Religion prinzipiell positiv: Der Mensch verdanke „zahlreiche Ziele und Ideale [seiner] abendländischen Kultur, wie die Freiheit und Gleichheit, dem Einflusse des Christentums“.

Hans Albert, der im Gegensatz zum Agnostiker Popper Atheist ist, sieht ein generelles Problem in dem seiner Ansicht nach grundsätzlich „dogmatischen“ Charakter von Religionen. Religiöse Aussagen erheben demnach einen Letztbegründungsanspruch, der sich auf bestimmte „Einsichten“ und „Offenbarungen“ beruft. Albert lehnt dies als einen willkürlichen Abbruch des Begründungsverfahrens ab, der dazu diene, „die betreffende Überzeugung gegen alle möglichen Einwände zu immunisieren“. Er setzt dagegen das „Prinzip der kritischen Prüfung“; mit diesem habe man „die Aussicht, durch Versuch und Irrtum – durch versuchsweise Konstruktion prüfbarer Theorien und ihre kritische Diskussion anhand relevanter Gesichtspunkte – der Wahrheit näher zu kommen ohne allerdings jemals Gewißheit zu erreichen“. Dies bedeutet einen „Fallibilismus in bezug auf jedwede mögliche Instanz“, d. h. man könne von keiner Instanz wie etwa „der Vernunft, der Intuition oder der Erfahrung, dem Gewissen, dem Gefühl, einer Person, einer Gruppe oder einer Klasse von Personen, etwa von Amtsträgern“ ausschließen, dass sie sich irrt. Diese „Einsicht, daß alle Gewißheit in der Erkenntnis selbstfabriziert […] und damit für die Erfassung der Wirklichkeit ohne Bedeutung“ sei, stelle „den Erkenntniswert jedes Dogmas“ in Frage.

Jean-Paul Sartre vertritt einen „atheistischen Existentialismus“. Für ihn ist Gott nichts als eine Bedrohung der menschlichen Freiheit. Der erste Schritt des Existenzialismus sei es, jeden Menschen in Besitz dessen, was er ist, zu bringen und auf ihm die gänzliche Verantwortung für seine Existenz ruhen zu lassen. In L’existentialisme est un humanisme formuliert er:

Der deutsche Existenzphilosoph Karl Jaspers vertritt dagegen eine „existenziale Interpretation“ der Religion, d. h. eine auf den einzelnen Menschen bezogene Auseinandersetzung mit dem Transzendenten. Er bezieht sich auf die „maßgebenden Menschen“ nach der Reihenfolge ihrer Bedeutung: Sokrates, Buddha, Konfuzius und Jesus. Offenbarungsglauben kritisiert er zugunsten eines philosophischen Glaubens, den das Individuum entwickeln muss und der keine Verheißung, sondern lediglich Selbstverantwortung mit sich bringt.

Ernst Bloch kritisiert den dogmatischen Marxismus in seinem Versuch, die Religion durch Revolution abzuschaffen. Er stellt dagegen das Moment der Utopie, das jede erstarrte Herrschaftsform transzendiert. Dieses unabgegoltene Hoffnungspotential findet er gerade auch in der Religion wieder ("Atheismus im Christentum", "Das Prinzip Hoffnung").

Auch die Philosophen der Frankfurter Schule sehen den vulgärmarxistischen Rationalismus kritisch als eine Art „Religion“, die ein absolutes Wissen über das Ziel der menschlichen Gesellschaft vorgibt und damit nur neue Eindimensionalität und Herrschaft etabliere (Herbert Marcuse: "Der eindimensionale Mensch").

Eine heute weitverbreitete Haltung sieht die Existenz eines „Gottes“ als weder beweisbar noch widerlegbar an (Agnostizismus). Sie sieht in der Tradition Kants metaphysische Fragen, die auf eine transzendente Realität zielen, als sinnlose Fragen an, da die Antworten jenseits des menschlichen Erkenntnisvermögens lägen: so zum Beispiel Emil Heinrich du Bois-Reymonds „Ignoramus et ignorabimus“ (lat. „Wir wissen es nicht und wir werden es niemals wissen“).

Ebenso verbreitet ist ein postmoderner Relativismus, der jedem Menschen seine individuelle Form von Religiosität zugesteht und auf die Wahrheitsfrage weitgehend verzichtet. Diesem entspricht – ähnlich wie im Hellenismus um die Zeitenwende – ein neues Aufleben religiöser Strömungen, die sich nicht mehr von den großen Weltreligionen, Kirchen und Glaubensrichtungen her definieren, sondern Elemente daraus auswählen (Eklektizismus) und mit paganen Motiven zu einem Synkretismus und Pluralismus auch im Blick auf die Gottheit verbinden.

Dies findet man heute vor allem in der Esoterik, aber auch in eher nichtreligiösen Richtungen. Ihnen ist die Abgrenzung von den traditionell monotheistischen Religionen gemeinsam, die mit dem Glauben an einen einzigen universalen Gott oft einen Absolutheitsanspruch ihrer Lehre verbinden. So stimmt etwa der Philosoph Odo Marquard ein „Lob des Polytheismus“ an (in: "Abschied vom Prinzipiellen," 1981), in dem er den Monomythos des Christentums als ersten Geschichtsunfall bezeichnet. Dem setzt er die segnende Wirkung des religiösen Pluralismus entgegen (vgl. auch Jan Assmann).

Die Auffassung, dass Religion und Wissenschaft miteinander harmonieren, wird oftmals von gläubigen Wissenschaftlern vertreten, darunter Arthur Peacocke, John Polkinghorne oder Francis Collins. Laut Stephen Jay Gould, der sich selbst als Agnostiker bezeichnete, widersprechen sich Religion und Wissenschaft nicht, da beide unterschiedliche Bereiche abdecken („Nonoverlapping Magisteria“).

Im Gegenzug dazu halten Wissenschaftler wie Steven Weinberg, Richard Dawkins und Norman Levitt (1943–2009) Theismus und Wissenschaft für grundsätzlich unvereinbar, da sie völlig unterschiedliche Aussagen über das Universum machen würden. Es sei nicht möglich, gleichzeitig ein theistisches und ein wissenschaftlich orientiertes Weltbild zu haben, ohne Unstimmigkeiten zu verdrängen. Die konziliante Haltung von Institutionen wie der US-amerikanischen National Academy of Sciences sei im Wesentlichen strategisch motiviert, um gemäßigte Gläubige zur Akzeptanz von wissenschaftlichen Erkenntnissen wie der Evolutionstheorie zu bewegen. Der US-amerikanische Physiker Victor Stenger ist der Auffassung, dass religiöse Glaubensvorstellungen wie das ewige Leben, Reinkarnation, die Wirksamkeit von Gebeten, Körper-Seele-Dualismus, Wunder und Schöpfung nicht nur empirisch unbegründet sind, sondern durch die Wissenschaft widerlegt wurden.

Für Alan Sokal sind die meisten Religionen als Pseudowissenschaften einzuordnen, ähnlich der Astrologie und der Homöopathie.

Seit 1945 griffen Autoren aus verschiedenen Wissenschaftsbereichen bestehende religionskritische Ansätze auf und vertieften oder aktualisierten sie, manche auch in persönlicher Form.

Der Theologe und Philosoph Joachim Kahl plädierte 1968 in seinem Buch "Das Elend des Christentums" für eine „Humanität ohne Gott“ (Untertitel) im Kontext der existenzialistischen „Gott ist tot“-Theologie der 1960er Jahre.

Der Psychoanalytiker Tilmann Moser beschrieb 1976 in dem Buch "Gottesvergiftung" seine religiöse Sozialisation, kam aber im Nachfolgewerk „zu einem erträglichen Gottesbild“.

Uta Ranke-Heinemanns Ablehnung der biologisch gedeuteten Jungfrauengeburt wurde im Jahr 1987 in der Presse diskutiert. Ihr wurde von Franz Hengsbach am 15. Juni 1987 wegen ihrer Berufung auf ein Ratzinger-Zitat der theologische Lehrstuhl entzogen. Ihre Kritik an der katholischen Lehre ging über diese Thematik hinaus.

Herbert Schnädelbach löste am 11. Mai 2000 mit seiner Kritik an der "mea culpa"-Erklärung von Johannes Paul II. eine Debatte aus. In folgenden Beiträgen verteidigte er als „frommer Atheist“ die Aufklärung der Theologie und stellte den „neuen“ Atheismus wegen seiner naturwissenschaftlichen Engführung als konfessionelle Gefahr dar.
Der Psychologe Franz Buggle beschrieb 2003 in seinem Buch "Denn sie wissen nicht, was sie glauben" die Bibel wegen fragwürdiger und widersprüchlicher Gebote als ungeeignete Basis für ethische Orientierung und kritisierte auch einige neuere Theologen wie Hans Küng.

Karlheinz Deschner hat eine umfassende "Kriminalgeschichte des Christentums" verfasst, die zahlreiche Verbrechen von Kirchenvertretern auflistet, um die inhumanen Wirkungen kirchlicher Machtpolitik und Heuchelei von Christen aller Epochen bis hin zum Klerikalfaschismus aufzudecken.

„Neue Atheisten“ wie Sam Harris ("Das Ende des Glaubens", 2004), Richard Dawkins ("Der Gotteswahn", 2006), Daniel Dennett ("Den Bann brechen: Religion als natürliches Phänomen", 2006), Christopher Hitchens ("Der Herr ist kein Hirte – Wie Religion die Welt vergiftet", 2007) und Michel Onfray ("Wir brauchen keinen Gott", 2006/2007) kritisieren jede Form von Religion als irrationalen Aberglauben und setzen sich für eine von Vernunft und Verstand dominierte Welt ein.

Der Völkerkundler Pascal Boyer versuchte 2004 ("Und Mensch schuf Gott"), Feuerbachs Projektionsthese hirnphysiologisch zu untermauern: Ein bestimmtes Modul, das Sinneseindrücke verarbeite, führe Veränderungen in der Umwelt leicht auf Lebewesen zurück und lasse aus unklaren Wahrnehmungen Vorstellungen von übernatürlichen Akteuren, wie zum Beispiel Göttern oder Geistern, entstehen.

Andreas Kilian deutete Religion 2009 als biologisch selektierte nicht-logische Argumentationsebene, um den individuellen Egoismus gegenüber anderen besser rechtfertigen und durchsetzen zu können.

Thomas Grüter weist in seinem Buch „Magisches Denken“ auf konstituierende Elemente magischen Denkens in Religionen hin.

Das menschliche Streben, sich mit „höheren Mächten“ in Einklang zu bringen, diese zu Gottesbildern zu verdichten, sich als Gegenstände der Anbetung gegenüberstellen, dafür kollektive Verehrung zu beanspruchen und Herrschaftsverhältnisse damit abzusichern, unterliegt im Tanach, der hebräischen Bibel, scharfer Kritik.

Das Bilderverbot als Kehrseite des 1. Gebots (siehe Zehn Gebote) verbietet dem Volk Israel jegliche Gottesbilder und deren Anbetung. Dieses richtet sich nicht nur gegen fremde Götter, sondern vor allem gegen die Tendenz, über JHWHs Wesen zu verfügen und ihn für menschliche Zwecke zu benutzen. Die Prophetie Israels kritisiert dieses Streben seit ihren Anfängen besonders im Blick auf die religiösen und politischen Führer des Gottesvolkes. Sie bezieht die Kritik vor allem auf den Wunsch nach einem König wie bei anderen antiken Völkern (Samuel, ), Synkretismus (Elija, ), Ausbeutung durch Priester und Königshof (Amos), Opferkult und Rechtlosigkeit im Namen des JHWH-Glaubens (, ), Bündnis- und Rüstungspolitik mit Berufung auf Gott (Jesaja), die am Tempel angestellten willfährigen Heilspropheten (Jeremia) usw.

Seit dem Untergang der beiden Teilreiche (586 v. Chr.) erinnert die biblische Geschichtsschreibung an das immer wiederkehrende Versagen des Gottesvolkes und seiner religiösen Führer, die nicht auf Gottes Selbstmitteilung gewartet, sondern eigenmächtige Gottesbilder geschaffen und damit Unheil für alle heraufbeschworen hätten: etwa in der Geschichte vom Goldenen Kalb ().

In der im babylonischen Exil entworfenen Darstellung der Weltschöpfung schlägt sich die Erinnerung an den Exodus aus der Sklaverei, die mit der Gottkönigsideologie begründet wurde, nieder: Die erfahrbare Welt wird entgöttert, die babylonischen Astralgottheiten sind zu „Lampen“ und Wegmarken für den Menschen depotenziert ().

Seit der Reformation sieht die christliche Theologie die Kritik an der eigenen Religion, dem Christentum, als eine ihrer Hauptaufgaben. Martin Luther stellte als Maßstab der Kritik die Selbstmitteilung Gottes in der Person Jesu Christi, wie er vor allem in den Schriften des Paulus als Leidender und Auferstandener (gem. Römer-, Galater- und Epheserbrief) bezeugt ist und deshalb nicht von Menschen zu erfinden und festzustellen sei, auf: allen historisch gewachsenen religiösen Traditionen, dem gesamten Kirchenapparat, der scholastischen Synthese von Glauben und Wissen und der „Hure Vernunft“, die sich für unterschiedlichste Zwecke missbrauchen lasse, gegenüber. Dies galt ihm nicht bloß für den Katholizismus, sondern als ständige Überprüfung der gesamtchristlichen Theorie und Praxis und Ansporn zu Kirchenreformen mit gesellschaftlicher Außenwirkung "(ecclesia semper reformanda)".

Der Kant-Schüler Johann Heinrich Tieftrunk (1759–1837), rationalistischer evangelischer Theologe, antwortete auf Kants Fundamentalkritik am metaphysischen Denken mit eigenen Entwürfen einer Religionskritik am Christentum, besonders am Protestantismus. Maßstab war für ihn wie für Kant die vernünftige Selbstbestimmung: Religion wird Objekt der Kritik, sofern sie gegen dieses Postulat verstößt. Kritisiert wird das religiöse Bewusstsein überhaupt, um es über sich selbst aufzuklären und auf seine praktische Vernunft – den Beitrag für menschliches Zusammenleben – hin zu überprüfen. Dabei wird religiöse Fremdbestimmung als Selbstbetrug der Vernunft – kein „Priesterbetrug“ – beschrieben, die sich damit eigentlich einem eigenen als einem fremden Maßstab unterwerfe. Im Vollzug dieser Selbstaufklärung soll das der Vernunft gemäße Wertvolle der Religion bewusst angeeignet und so bewahrt werden.

Nachdem der Neuprotestantismus sich im 19. Jahrhundert auf die empirische Religiosität, die subjektive Gotteserfahrung und den durch Sittlichkeit zu veredelnden zivilisatorischen Fortschritt zurückgezogen hatte, erneuerte Karl Barth den reformatorischen Ansatz nach 1918, griff die Religions- und Ideologiekritik von Feuerbach und Marx positiv auf und führte sie christologisch durch (Kirchliche Dogmatik I/2, §17: "Gottes Offenbarung als Aufhebung der Religion"). Gott offenbare sich in Jesus Christus im völligen Gegensatz zu menschlicher Religion. Im Kreuzestod Jesu zeige Gott sein wahres Wesen: Damit decke er alles eigenmächtige Streben nach einer Synthese zwischen Gott und Mensch als Sünde auf. Religion erscheint in diesem Spiegel als nie zu Gott führendes Menschenwerk, als Eigenmacht und Verleugnung des wahren, zu Leiden und Tod für den Menschen fähigen Gottes.

Barth bezog diese Kritik 1938 besonders auf den Protestantismus seiner Gegenwart, der sich mit weltlichen Mächten von Nation, Rasse, Staat verbündete und konfessionellem Sonderbesitz anhing, dabei aber den mit den Juden leidenden und sterbenden Gott verleugnete und übersah.

Das Denken und Handeln von Dietrich Bonhoeffer kreiste um das Leitmotiv: „Wer ist Jesus Christus für uns heute?“ Die Antworten, die er fand und vorlebte, stellten das herkömmliche Christentum, seine religiösen Ausdrucksformen, seine anachronistische Apologetik und politische Weltfremdheit immer stärker in Frage und ließen es schließlich ganz zurück.

Als Pazifist kritisierte Bonhoeffer die Unverbindlichkeit der Ökumene und die nationalistischen Bindungen ihrer Mitgliedskirchen, die zu keinem schlichten, gemeinsamen, leidensbereiten, nur so von den Nationen unüberhörbaren Friedenszeugnis gegen den drohenden Weltkrieg fähig waren (Rede in Fanö 1937). Als Teilnehmer am konspirativen Widerstand gegen den Nationalsozialismus übte er schärfste Selbstkritik stellvertretend für die mutlose, mit ihrer eigenen Existenzerhaltung beschäftigte, gegenüber den Opfern des NS-Staates versagende Bekennende Kirche (Schuldbekenntnis 1941). In seinen letzten Lebensmonaten verabschiedete er sich vom abendländischen Modell des Christentums als einer Religion, die die „mündig gewordene Welt“ einfach nicht brauche.

Jede dieser Kritiken war für ihn eine schmerzhaft entdeckte, unvermeidbare Antwort auf die Herausforderung zur Nachfolge Jesu mitten in der Gegenwart. Sein Buch "Nachfolge" beginnt mit dem Satz: „Billige Gnade ist der Todfeind unserer Kirche.“ Billig war für ihn eine auf Predigt und Sakramente reduziertes Heilsangebot, das als Ausrede für Nichtstun und Gleichgültigkeit gegenüber der Not des Nächsten diente: ein „Glaube“ ohne Selbstkritik (Buße), ohne entsprechendes Handeln, ohne Bereitschaft, mitzuleiden und Verantwortung zu übernehmen für andere.

Seine "Ethik" (1940–1943) war eine umfassende Absage an jede von zeitlosen Idealen, Normen und Prinzipien ausgehende Tugendethik und an die lutherische Zwei-Reiche-Lehre: Das traditionelle Denken in zwei Räumen – hier die dem Bösen verfallene Welt, dort Gottes unerreichbare jenseitige Gegenwelt, hier der faule Kompromiss mit dem Diesseits, dort der ebenso faule pseudorevolutionäre Hass auf das Bestehende – verfehle die Wirklichkeit, in die hinein das konkrete Gebot Gottes den Christen hier und heute stelle. Das schlichte Dasein Jesu Christi im leidenden Nächsten decke die falschen Fronten der Gegenwart auf: „Schlimmer als die böse Tat ist das Bösesein. Schlimmer ist es, wenn ein Lügner die Wahrheit sagt, als wenn ein Liebhaber der Wahrheit lügt; schlimmer, wenn ein Menschenhasser Bruderliebe übt, als wenn ein Liebhaber der Menschen einmal vom Hass überwältigt wird.“ Um den Nächsten zu retten und das Menschenrecht angesichts der totalen Herrschaft des Bösen (in Gestalt des NS-Staates) zu bezeugen, sei der Christ unter Umständen zum Bruch aller Zehn Gebote genötigt.

Seine "Gefängnisbriefe" (April bis August 1944) an Eberhard Bethge enthielten offene Fragen, Skizzen, Aphorismen und Visionen, die auf eine umfassende Abkehr von allen religiösen Formen des christlichen Glaubens und Hinwendung zu einem „religionslosen Christentum“ zielten:
Jedes Stichwort stand für eine Variante der protestantischen Theologie: Wortmitteilung für die lutherische Orthodoxie, Innerlichkeit für Schleiermachers romantisches „Gefühl der schlechthinnigen Abhängigkeit“, Gewissen für Albrecht Ritschls und Wilhelm Hermanns neokantianischen Moralismus. Demgegenüber sah Bonhoeffer eine Zukunft ohne Religion voraus:
Das „religiöse Landen“ meinte die herkömmliche apologetische Methodik, einen Grenzbezirk im menschlichen Denken und Fühlen aufzuweisen, um dort „Gott“ zu vermitteln und einleuchtend zu machen:
Die religiöse Art des Redens von Gott sei „einerseits metaphysisch, andererseits individualistisch“: Dies bezog sich auf den katholischen Thomismus ebenso wie auf die lutherische Rechtfertigungslehre. Beide verfehlten die Situation des heutigen Menschen, der seine Welt nicht mehr unter religiösen Vorzeichen sehe und gestalte. Der Prozess der Aufklärung und Säkularisierung sei unumkehrbar:
Diese Einsicht fand Bonhoeffer gerade im Evangelium:
Gerade weil Bonhoeffer an die Gegenwart Christi in dieser Welt glaubte, war „Religion“ für ihn Flucht aus der Realität in ein imaginäres Jenseits. Dies zielte nicht auf eine neue „modernere“ Auslegung der Bibel für Atheisten, wie es später die Gott-ist-tot-Theologie unternahm, sondern auf eine ganz andere christliche Existenzweise:
Damit wagte er eine erste Antwort, wie ein künftiges Christuszeugnis in einer faktisch religions- und gottlosen Welt aussehen könne. Realitätsgerecht sei allein das Leiden für Andere unter Verzicht auf jeden religiösen Egoismus und jede heimliche Missionsabsicht. Die Kirche habe sich in der Zeit des Nationalsozialismus als unfähig erwiesen,
Die künftige Kirche müsse gleich zu Beginn alles Eigentum den Notleidenden geben. Die Pfarrer müssten ausschließlich von den freiwilligen Gaben der Gemeinden leben, eventuell von einem gewöhnlichen Beruf. Sie müssten ständig und ganz an den gesellschaftlichen Aufgaben teilnehmen, „nicht herrschend, sondern helfend und dienend“. Nur so könnten Christen den Menschen zeigen, was ein Leben in Christus heiße und als Vorbilder wirken.

Unter diesem Schlagwort haben v. a. auch Theologen religionskritische Überlegungen angestellt, zum Beispiel Dorothee Sölle. Auch das Theodizee-Problem hat durch Auschwitz ("pars pro toto" für die Shoa) eine Verschärfung erfahren, so bei Günther Anders, für den Gott stets einer ist, der Auschwitz zugelassen hat.

Innerhalb der katholischen Kirche hat der Theologe Karl Rahner ein "anonymes Christentum" angedacht, also ein aus der Legitimität des Zweifels geborenes Abfallen von der Religion, das entgegen der Formel "extra ecclesiam nulla salus" doch "heilig" sein könne (vgl. auch Zweites Vatikanisches Konzil).

"Siehe auch:" Hans Jonas















</doc>
<doc id="10164" url="https://de.wikipedia.org/wiki?curid=10164" title="Kiel (Begriffsklärung)">
Kiel (Begriffsklärung)

Kiel bezeichnet:


Geographische Objekte:

Schiffe:
Kiel ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="10167" url="https://de.wikipedia.org/wiki?curid=10167" title="Saturn">
Saturn

Saturn steht für:

"Technik"

"Transporttechnik"

" Medien"

"Geographie"

"Unternehmen":
Saturn als Familienname:
Siehe auch:



</doc>
<doc id="10169" url="https://de.wikipedia.org/wiki?curid=10169" title="Griechen">
Griechen

Die Griechen (von lateinisch ", der Bezeichnung für die Griechisch sprechenden Völker der Antike, etymologische Herkunft nicht sicher geklärt; ) sind ein indogermanisches Volk, dessen sprachliche Wurzeln sich bis ins zweite vorchristliche Jahrtausend zurückverfolgen lassen. Heute leben über 10,5 Millionen Griechen in Griechenland und auf Zypern; ca. 7 Mio. Menschen außerhalb Griechenlands und Zyperns bezeichnen sich ebenfalls als Griechen.

Bei Homer werden die gegen Troja ziehenden Griechen nach dem auf der Peloponnes siedelnden Volksstamm der Achaier (, '), nach Danaos, dem Stammvater von Menelaos und Agamemnon als Danaer (, ') oder als Argiver ( ") bezeichnet. Der seit der Dorischen Wanderung das antike Griechenland prägende Volksstamm der Dorer wurde für die Gesamtbezeichnung der Griechen nie nachweisbar herangezogen.

Der Terminus "Hellenen" () – ursprünglich der Name eines thessalischen Stammes nach dessen mythischem Stammvater Hellen – für die Griechen ist belegt bei Pausanias, Herodot und Thukydides und wurde im klassischen Griechenland der Begriff für die Gesamtheit der Griechisch sprechenden Völker verwendet (Gegenbegriff: "Barbaren" - ).

Der Begriff "Hellenen" wurde im spätantiken Oströmischen Reich zunächst nur noch für die Anhänger der alten griechischen Kulte, später für alle Nichtchristen gebraucht, fand dann gegen Ende des Reichs jedoch in gebildeten Kreisen auch wieder für die Griechisch Sprechenden Verwendung (Plethon 1418: „Wir sind… der Abstammung nach Hellenen. Dafür zeugt sowohl die Sprache als auch die von den Vätern ererbte Bildung“). Bis ins 18./19. Jahrhundert hinein blieb die Selbstbezeichnung vieler Griechen allerdings Ῥωμαῖοι ("Römer" bzw. Rhomäer), da der Bezug auf das christliche Byzantinische Reich noch während der osmanischen Herrschaft große Bedeutung besaß und die Eigenwahrnehmung breiter Bevölkerungskreise prägte. Erst als es im frühen 19. Jahrhundert, befeuert durch die Griechenlandbegeisterung in England, Frankreich und Deutschland, zu einer Rückbesinnung auf die vorchristliche Zivilisation der Antike kam, änderte sich dies grundlegend.

Die Griechen der Gegenwart verwenden in Anknüpfung an ihre Sprache und die historische Bezeichnung des Landes, in dem sie leben (antikes Griechenland), wieder den Begriff ("" ‚Hellenen‘). Im deutschen Sprachgebrauch wird der Begriff Hellas eher literarisch für Griechenland verwendet, er findet sich auch in Begriffen wie dem Hellenismus als nachklassischer Epoche des antiken Griechenland, den Hellenisten als griechischsprachigen Juden, den Philhellenen als Freunden Griechenlands sowie dem Panhellenismus als politischem Modell.

Die lateinische Bezeichnung ' geht auf die Griechen zurück, die im 8. vorchristlichen Jahrhundert in Italien, der späteren , siedelten und sich selbst als ' oder ähnlich bezeichneten. Bei Homer ist der Name einer böotischen Stadt namens Graia () belegt, Pausanias erwähnt Graia als alten Namen der böotischen Stadt Tanagra.

Bei Aristoteles ("Metaphysik", 1.352) findet sich die älteste Quelle für die griechische Bezeichnung " (). Er erwähnt die Einwohner des zentralen Epirus, die ursprünglich ‚Griechen‘ () geheißen hätten und erst später Hellenen genannt worden seien. Diese Ansicht bestätigen weitere Quellen, in der Parischen Chronik wird gar das Jahr 1521 v. Chr. für den Zeitpunkt der Umbenennung der Griechen in Hellenen angegeben.

Der lateinische Begriff " wurde schließlich etymologisch zur Grundlage der Bezeichnung des Volkes in fast allen Sprachen, wenn auch Übersetzungen des Begriffs Hellenen meist ebenfalls existieren.
Der neugriechische Aufklärer Adamantios Korais schlug vor, den Begriff anstelle des seinerzeit verwendeten "" wieder einzuführen.

Im Byzantinischen bzw. Oströmischen Reich (griech. ‚Kaiserreich der Römer‘) bezeichneten sich die Einwohner auch nach dem Ende der Antike weiterhin als ‚Römer‘ (später und neugriechisch auch ' ; siehe auch Rhomäer) – nach dem Schisma 1054 in Abgrenzung zur Römischen Kirche häufiger wieder als Griechen (' ). Noch heute wird der Begriff von Griechen gebraucht, wenn die orthodoxe, byzantinische Tradition des Volkes betont werden soll. Die Griechen des Mittelalters werden seit dem 19. Jhdt von Historikern als ‚Byzantiner‘ bezeichnet.

Auch im Türkischen und Arabischen wurde der Begriff "Rumi" für die Griechen gebraucht, beispielsweise im Koran.

Östlich Griechenlands wurde das Volk der Ionier namensgebend für die Griechen. Im Hebräischen ist schon seit biblischer Zeit "" () der Begriff für die Griechen, das Land heißt im modernen Hebräisch und die Bewohner יוונים "Jevanim".

Die Perser bezeichneten Griechenland als ', und der Begriff drang in alle Sprachen des Perserreichs. Von den Persern entlehnt ist die Sanskrit-Bezeichnung ' und das Pali-Wort "Yona". So verbreitete sich die Bezeichnung letztlich in der ganzen muslimischen und weit in der indisch beeinflussten Welt, Beispiele sind , türkisch ' und indonesisch '.

Die verschiedenen Völker der Griechen definierten die Zugehörigkeit zu den "Hellenen" über die verschiedenen Varianten der griechischen Sprache und über den olympischen Kult in der Religion. Religiöse Feste wie die Mysterien von Eleusis, zu denen sich Einwohner aller griechischen Völker versammelten, bildeten eine Einheit stiftende, quasi nationale Manifestation in der politisch zersplitterten und oft durch gegenseitige Konkurrenz oder Krieg geprägten griechischen Welt. Auch die verhältnismäßig einheitliche Tempel-Architektur im gesamten griechischen Raum ist ein Beispiel für die Rolle der Religion für die gesamtgriechische Kultur. Die panhellenischen Olympischen Spiele, ein kultischer Wettkampf auf dem heiligen Hain am Zeusheiligtum von Olympia, waren nur freien Bürgern ebendieser griechischen Welt offen. Inwieweit die Makedonen eine mit dem Griechischen verwandte Sprache oder einen Dialekt des Griechischen sprachen, ist bis heute umstritten, offenbar wurde auch ihre Zugehörigkeit zu den Hellenen in der Antike - besonders von Athen - bezweifelt. Ab 408 v. Chr. waren sie jedoch nachweislich zu den Olympischen Spielen zugelassen, waren also als Hellenen anerkannt.

Nichtgriechen bezeichnete man onomatopoetisch als Barbaren (), ein Wort, das das ‚Stammeln‘ - "bar bar" – der unverständlichen Fremdsprache wiedergibt. Später wurde das Wort Synonym für ungeschliffenes, unzivilisiertes und kulturloses Verhalten schlechthin, siehe auch "Barbarei".

Etwa ab 800 v. Chr. gründeten zahlreiche griechische Poleis Kolonien im gesamten Mittelmeerraum, einschließlich des Schwarzen Meeres. Meist waren diese Kolonien der Mutterstadt (Metropolis) freundschaftlich verbundene, doch politisch selbständige Stadtstaaten. Griechische Gründungen sind z. B. Massilia (Marseille), Nikaia (Nizza), Neapolis (Neapel), Syrakusai (Syrakus), Taras (Tarent), Byzantion (ab ca. 337 Konstantinopel/ seit 1930 Istanbul), Dioskurias (Sochumi), Kerkinitis (Jewpatorija), Odessos (Warna) und Trapezus (Trabzon).

Mit dem Reich Alexanders des Großen wurde Griechisch Staatssprache eines riesigen Großreiches. Griechisch wurde die "lingua franca" des Vorderen Orients und blieb dies auch, als der östliche Mittelmeerraum unter römische Herrschaft geriet. Zwar kam es immer wieder zu Freiheitserklärungen für einige oder gar alle griechischen "" - etwa durch Kaiser Nero -, doch faktisch war Griechenland über Jahrhunderte Teil des "Imperium Romanum", dessen Eliten in der Regel neben Latein auch Griechisch sprachen. Östlich der Grenzen des Römischen Reiches verlor unter der Herrschaft der Parther das Griechische bis zur Herrschaft der Sassaniden seine Bedeutung. Im östlichen Iran verschwand das Griechische unter den Kuschana von Münzen und Inschriften, im heutigen Afghanistan blieb lediglich ein griechisches Alphabet mit Zusatzzeichen bis zur islamischen Eroberung im 7. Jahrhundert für die einheimische Sprache in Gebrauch. Im dritten und vierten nachchristlichen Jahrhundert gewann das Lateinische auf Kosten des Griechischen auch im Ostteil des Römischen Reiches Bedeutung. Schriftsteller wie der aus dem syrischen Antiochia stammende Ammianus Marcellinus verfassten ihre Werke in lateinischer Sprache. Erst nach der Regierungszeit Kaiser Justinians, als dem Oströmischen Reich die lateinisch sprechenden Provinzen verloren gingen oder durch Kriege nachhaltig verwüstet wurden, wurde Griechisch dann zur zweiten Amtssprache des Oströmischen, später Byzantinischen Reiches – im 7. Jahrhundert nach der Regierungszeit Kaiser Herakleios löste es Latein in dieser Hinsicht dann ganz ab. In dieser Zeit verlor das Griechische im Orient weiter an Bedeutung, in Ägypten zugunsten des Koptischen und in Syrien an die Syrische Sprache. Später setzte sich dort nach der islamischen Expansion (seit 632) das Arabische durch.

Im Zuge der spätantiken Völkerwanderung fielen seit 250–396 zunächst in Thrakien und dann südwärts bis in den Peloponnes Westgoten ein. Im Jahre 378 kam es zur Schlacht von Adrianopel (378) später wurde das Heiligtum von Epidauros von den Goten geplündert. Im 5. und 6. Jahrhundert fielen einmal mehr Ostgoten und Hunnen in das Gebiet des heutigen Griechenland ein. Während diese Völker noch weiterzogen, begann im frühen 7. Jahrhundert eine nachhaltige Landnahme der Slawen auf dem Balkan, die sich aber im Wesentlichen auf das Hinterland konzentrierte, während die (teilweise) fortifizierten Städte entlang der Küstenregionen ununterbrochen in griechischer Hand blieben. Hierzu konstatiert Konstantin Porphyrogennetos im 10. Jahrhundert: „Das ganze Land wurde slawisiert und barbarisch.“

Erst im frühen 9. Jahrhundert konnte Byzanz seine Herrschaft über Griechenland wieder sichern, man begann, griechischsprachige Einwohner des östlichen Reiches nach Europa umzusiedeln und die slawischen Einwanderer gezielt zu gräzisieren. Die griechisch-orthodoxe Kultur gelangte in Griechenland wieder zu einer gewissen Blüte (siehe auch Mystras), die architekturgeschichtlich insofern interessant ist, als sie auch antike Bauteile in die Kirchenbauten integrierte und so erstmals wieder einen Bezug zur antiken griechischen Kultur herstellte, gleichzeitig aber auch ihre endgültige Überwindung vollzog.
Mit der Eroberung Konstantinopels während des Vierten Kreuzzuges im Jahr 1204 kamen neue Herrscher nach Griechenland: Fränkische Ritter und vor allem die Seemacht Venedig sicherten sich wichtige Handelsposten für den Orienthandel in Griechenland und bedrohten die byzantinisch-ostkirchliche Kultur der Griechen nachhaltig.

Im westlichen Griechenland und dem heutigen Südalbanien bildete sich aber mit dem Despotat Epiros ein griechischer Nachfolgestaat des Byzantinischen Reiches, während im westlichen Kleinasien die Reiche von Nikaia (heute Iznik) und an der südöstlichen Schwarzmeerküste mit dem Kaiserreich Trapezunt weitere griechische Staaten in byzantinischer Nachfolge entstanden. Nikaia gelang dann 1261 die Wiedereinnahme Konstantinopels.

Mit der Eroberung Konstantinopels 1453 durch die Osmanen begann die türkische Herrschaft über Griechenland. Das osmanische Regime vollzog seine Herrschaft in einem Tributsystem, duldete jedoch die Kirche weitgehend und überließ die lokale Verwaltung und Jurisdiktion den Einheimischen. Vom 16. bis 18. Jahrhundert bildete sich so wieder eine einheimische Bürgerschicht heraus, die jedoch nicht ausschließlich griechische, sondern ebenso albanische, sephardisch-jüdische, slawische und türkische Bevölkerungsteile mit einschloss. Der Begriff ‚Griechen‘ war im osmanischen Reich ein Synonym für die Angehörigen der Griechisch-Orthodoxen Kirche, so wie ‚Türken‘ ein Synonym für die Anhänger des Islams war.

Einige griechische Historiker (Paparrigopoulos, Vakalopoulos) sehen in der Rückbesinnung auf die antiken "Hellenen" durch spätbyzantinische Autoren (Plethon, s. o.) nach dem Vierten Kreuzzug 1204 den Ursprung des modernen hellenischen Bewusstseins. Dies wird von griechischen marxistischen Historikern (Zevgos, Rousos) stark relativiert; sie betonen den neuzeitlichen Charakter der Gestaltung der griechischen Nation. Wohl sicher ist, dass die Angehörigen der orthodoxen Kirche, zu denen auch die Griechen gehörten, im Osmanischen Reich grundsätzlich diskriminiert waren und die Herrschaft der Türken weitgehend als Fremdherrschaft („Türkenherrschaft“, , "") empfanden.

Der Aufstand gegen die osmanische Herrschaft im 19. Jahrhundert ging von griechisch sprechenden Christen aus, die aus einer verhältnismäßig gebildeten, bürgerlichen Schicht kamen, die einen beträchtlichen Teil des Handels im Osmanischen Reich bestimmte. Bereits im ausgehenden 18. Jahrhundert hatten sie, auch in Abgrenzung zur im Reich institutionalisierten Orthodoxen Kirche, begonnen, die griechische Antike als Vorbild eines nicht-kirchlichen, griechischen Nationengefühls, wieder anzunehmen. Die Unterstützung durch die west- und mitteleuropäischen Philhellenen, die in den Freiheitskämpfern Nachfahren der antiken Griechen sahen und von einer Wiederkehr des alten Hellas träumten, gaben zusätzliche Impulse für die Rückbesinnung auf das antike Griechenland.

So bezog sich der 1822 geschaffene griechische Staat, der durch das Londoner Protokoll 1830 ebenso wie die nationale griechische Idee sanktioniert wurde, auf das antike Griechenland. Geografische Bezeichnungen beispielsweise wurden weitgehend regräzisiert. Mit der Katharevousa (‚Reinsprache‘) wurde eine dem Altgriechischen nahestehende Nationalsprache künstlich geschaffen, die bis 1976 Amtssprache in Griechenland blieb und erst infolge der Überwindung der Obristendiktatur als solche abgeschafft wurde.

So ist auch zu verstehen, dass die 1830 publizierte These des deutschen Orientalisten Fallmerayer, dass die antiken Griechen im Mittelalter ausgestorben seien und durch hellenisierte Slawen und Albaner verdrängt worden wären, von der sich bildenden griechischen Elite auf das Heftigste bekämpft wurde. Der Argumentation Fallmerayers, die von einem antiken „Geschlecht der Hellenen“ ausgeht und konstatiert, kein „Tropfen edlen und ungemischten Hellenenblutes“ fließe mehr in den Adern der modernen Griechen, wurde auch wissenschaftlich bald widersprochen (heute gilt sie als widerlegt, während der albanisch- und slawischstämmige Bevölkerungsanteil am modernen griechischen Staatsvolk nicht mehr bestritten wird); gleichwohl bestärkte Fallmerayer unfreiwillig die griechischen Nationalisten in deren Betonung einer kulturellen Kontinuität. Der Klassiker der griechischen Geschichtsschreibung, Konstantinos Paparrigopoulos’ "Geschichte der hellenischen Nation von den frühesten bis zu den neueren Zeiten", hat das Selbstverständnis der Griechen als Nachfolger der antiken Hellenen grundlegend geprägt.

Im neu geschaffenen Staat auf dem Territorium des heutigen Mittel- und Südgriechenland lebte nur etwa ein Drittel der Griechen des Osmanischen Reiches, die wichtigsten griechischen Handelszentren wie z. B. Smyrna oder Konstantinopel befanden sich weiter in türkischer Hand. Gleichzeitig befanden sich noch Angehörige slawischer Völker, Albaner und Türken im griechischen Staat.

Bis zum Ende des 19. Jahrhunderts hatten griechische Historiker die Wiederentdeckung und Rehabilitierung der byzantinischen Vergangenheit vervollständigt. Der Ruhm und Glanz des byzantinischen Reiches ließ in ihren Augen zeitweise die klassische Antike verblassen und lieferte ihnen zudem das theoretische Grundgerüst für die "Megali Idea" (μεγάλη ιδέα ‚große Idee‘), der Vision des nach Freiheit strebenden griechischen Volkes. Diese Vision, die die Vereinigung aller Gebiete griechischer Besiedlung vom Balkan bis zu Kleinasien innerhalb der Grenzen eines einzigen Staates mit der Hauptstadt Konstantinopel anstrebte, beherrschte den unabhängigen Staat während des ersten Jahrhunderts seiner Existenz.

Dem griechischen Staat gelang bis 1920 eine territoriale Erweiterung auf (mit Ausnahme des Dodekanes) das heutige Staatsgebiet. Weitere Versuche der Erweiterung wurden durch die sogenannte Kleinasiatische Katastrophe gestoppt: Im Vertrag von Lausanne wurden die (noch heute geltenden) territorialen Grenzen gezogen und ein umfangreicher „Bevölkerungsaustausch“ zwischen den Staaten verfügt – also die gezielte Vertreibung der jeweiligen nationalen Minderheiten. Das heißt, die in Kleinasien ansässigen Griechen (etwa 1,5 Mio.) wurden gezwungen, nach Griechenland auszuwandern, die in dem nun Griechenland zugefallenen Gebiet beheimateten Türken (ca. 0,5 Mio.) wurden gezwungen, in die Türkei auszuwandern.

Gleichzeitig erhielten jedoch auch Bewohner anderer östlicher Gemeinden eine Möglichkeit, in das neu geschaffene Griechenland einzuwandern. Zur selben Zeit zogen zahlreiche Slawen und Albaner in die entstehenden Nationalstaaten des Balkans.


Migration ist ein beinahe kontinuierlicher Bestandteil der Geschichte der Griechen: Man kann vier Phasen griechischer Auswanderung aus dem Stammland definieren:

Die moderne Auswanderung beginnt etwa um die Hälfte des 19. Jahrhunderts. Nach Angaben des griechischen Nationalen Statistischen Dienstes sind zwischen 1850 und 1940 rund 511.000 Menschen aus Griechenland ausgewandert, allein 463.000 von ihnen in die USA. Die höchsten Auswandererzahlen finden sich hierbei in den Jahren 1906–1915. Nach dem Zweiten Weltkrieg setzt um die Mitte der 1950er Jahre wieder eine stärkere Emigration aus Griechenland ein, mit jährlichen Zahlen bis 1975 von 12.000 bis 30.000, wobei die USA zugunsten Westeuropas immer stärker in den Hintergrund treten. Auch eine Rückwanderung nach Griechenland findet statt, ist jedoch wesentlich geringer als die Auswanderung.

Aus den Siedlungsgebieten außerhalb des griechischen Staatsgebiets emigrierten zu Beginn des 20. Jahrhunderts mehr Griechen nach Übersee als in den griechischen Staat selbst.

Ähnlich wie die antiken Kolonisten haben viele der modernen Auslandsgriechen den Kontakt zum Mutterland stets weiter gepflegt und Sprache, Religion und Bräuche auch in der neuen Heimat erhalten. Das Selbstbewusstsein als Griechen bzw. griechische Diaspora hat sich so bis auf den heutigen Tag unter vielen der bis zu 4 Mio. Auslandsgriechen erhalten, oft auch nach Annehmen der Nationalität des neuen Heimatlands.

„Es ist in der Tat verblüffend, wie viele Aspekte des heutigen politischen Lebens Griechenlands – vor allem Athens – antike Parallelen haben,“ schreibt Heinz A. Richter in seinem Werk "Griechenland im 20. Jahrhundert," führt dabei Beispiele wie „den leidenschaftlichen Anteil am Leben der Politeia, deren Ereignisse eifrig diskutiert werden“ auf und geht bis hin zu charakterlichen Gemeinsamkeiten zwischen den heutigen und antiken Griechen.

Als ‚direkte Nachfahren der antiken Hellenen‘ legen die Griechen großen Wert auf die Kenntnis der Antike. Bereits in der Grundschule steht Geschichte auf dem Lehrplan, Altgriechisch ist Pflichtfach. Antike Gelehrte und deren Schriften wie Homer, Platon und Sokrates sind wichtig, die kritische Auseinandersetzung mit dem Erbe der Antike spielt kulturell häufig eine große Rolle. Im Namensstreit um den Staatsnamen Mazedoniens verweist der griechische Staat auf die Nachfolge der Nordgriechen aus den antiken Makedonen und sieht die Vereinnahmung Alexanders des Großen durch Mazedonien sehr kritisch.

Gleichzeitig empfinden sich viele Griechen, auch außerhalb des heutigen griechischen Staatsgebiets aufgewachsene, immer noch als "Romii" (‚Römer‘, vgl. Romiosini). Diese starke Identifikation mit Byzanz erklärt sich nicht zuletzt durch den traditionell großen, im Grunde identitätsstiftenden Einfluss der griechisch-orthodoxen Kirche auf Griechenland. Das byzantinische Erbe geht aber über die Religion hinaus, es spiegelt sich auch im Volksglauben, in Sitten, Gebräuchen, Musik etc. wider. Byzantinische Legenden wie z. B. die vom „zu Marmor versteinerten Kaiser“ (der letzte byzantinische Kaiser Konstantinos Palaiologos), der eines Tages wiederauferstehen und die Romaii von der osmanischen Fremdherrschaft befreien würde, leben bis heute als Volksglaube fort.

Auch prägt diese Identifikation der Griechen mit ihrem mittelalterlichen Großreich das bis heute anhaltende Misstrauen gegenüber dem – fränkischen i. e. katholischen – Westen, der sie in ihren Augen im Kampf gegen die Osmanen aus Gründen der religiösen Machtkämpfe und Einflussnahme (vgl. Schisma) allein gelassen und verraten habe.

Gegen diese tief verwurzelte Identifikation des Volkes mit Byzanz konnten auch griechische Gelehrte der Neuzeit wie Adamantios Korais, der Byzanz als priesterbeherrschten Obskurantismus zutiefst verachtete und sich ausschließlich mit der Antike identifizierte, nichts ausrichten.

Die Griechen bilden das Staatsvolk in Griechenland; ihre Zahl beträgt rund 11 Millionen. Da die griechische Verfassung die Orthodoxe Kirche als Staatskirche definiert, gelten Angehörige anderer, im griechischen Sprachgebrauch „fremder Konfessionen“ ("xena dogmata" ξένα δόγματα) oft nicht als Griechen im eigentlichen Sinne. Eine rechtliche Anerkennung besteht nur für die muslimische Minderheit (gebildet von Türken und Pomaken), andere Minderheitensprachen wie Albanisch, Aromunisch und Ägäis-Mazedonisch haben keinen offiziellen Status in Griechenland. Die etwa 50.000 Angehörigen der Griechischen Katholischen Kirche wie auch jüngerer christlicher Kirchen werden statistisch als Griechen fremder Religion geführt.

Etwa 721.000 Griechen (2004) bilden rund 78 Prozent der Bevölkerung auf Zypern. Sie entstanden aus einer Vermischung der antiken griechischen Inselbevölkerung mit im Mittelalter vom Festland zugezogenen Griechen. Bedingt durch die lange politische und räumliche Isolation im Mittelalter und in der Neuzeit konnten sich bis heute einige sprachliche Archaismen aus dem Mittelalter halten. Dadurch weicht das zypriotische Griechisch, die Umgangssprache der Zyperngriechen merklich von der griechischen Hochsprache ab. Letztere wird trotzdem in allen formellen Zusammenhängen (Bildungswesen, Ämter, Medien) und in Schriftform benutzt. Religiös sind die Zyperngriechen, früher auch in Abgrenzung zu den Zyperntürken als "Zyprioten", seit 431 autokephal (Kirche von Zypern). Dennoch ist die kulturelle Verbindung zum griechischen Mutterland stets sehr stark gewesen, so dass die griechischen Zyprer nach eigenem Selbstverständnis zwar sich nach wie vor Griechen verstehen, sich aber gegenüber diesen auch abgrenzen. Seit der türkischen Invasion 1974 leben fast alle griechischen Zyprer (bis auf eine kleine Minderheit von etwa 500 Personen) auf dem verbliebenen Territorium der Republik Zypern.

Die Sprachen der griechischen Enklaven in Italien werden unter der Bezeichnung Griko zusammengefasst. Verschiedenen Theorien zufolge sind die Griko sprechenden Italiener entweder Nachfahren griechischer Kolonisten im "Großgriechenland" (Magna Graecia) der Antike oder Nachfahren von Byzantinern, die im 9. Jahrhundert in Süditalien ansässig wurden. Die Sprecherzahl wird auf ca. 70.000 geschätzt.
Die Sprachinseln konzentrieren sich auf je neun Dörfer in zwei Regionen, Grecìa Salentina auf der Halbinsel Salento und Bovesìa (griechisch-kalabrischer Dialekt) im südlichen Kalabrien. Das Griko hat in Italien den Status einer Minderheitensprache.

Der zu Albanien gehörende nördliche Teil der Region Epirus ("Ípiros" ) ist auch heute noch griechisch besiedelt. Die Region um die Stadt Argyrókastro (), auf albanisch Gjirokastër, wurde von mehr als 100.000 Griechen bewohnt. Über die heutige Zahl existieren recht unterschiedliche Angaben. Nach albanischen Angaben beläuft sich ihre Zahl auf etwa 66.000 Menschen. Auch in den albanischen Städten Vlora und der Hauptstadt Tirana leben einige tausend Griechen, deren Familien aber ursprünglich allesamt aus dem Nordepirus stammen. Viele dürften nach Öffnung der Grenze aufgrund der schlechten Wirtschaftslage Albaniens nach Griechenland eingewandert sein. Kulturelle und politische Rechte für Minderheiten werden in der Verfassung Albaniens in den Artikeln 3 und 20 garantiert. 

Die Pontier ("Póndii" ) sind die größte griechische Gruppe, die um das Schwarze Meer ansässig war. Ihr Siedlungsgebiet reichte von der Stadt Sinop (gr. "Sinópi" ) im Westen bis kurz vor Batumi im Osten. Größte Stadt der Region war Trabzon (gr. "Trapezous" ). Viele Städte in der heute türkischen Region waren bis 1922 nahezu ausschließlich von Griechen bevölkert, doch nach der Kleinasiatischen Katastrophe 1922 mussten nahezu alle Griechen das Land verlassen. Die meisten siedelten sich in Gebieten Nordgriechenlands an, aus denen viele nicht griechischsprachige Einwohner nach Bulgarien und in die Türkei ausgewandert waren. Ihr Dialekt, das Pontische, wird dort bis heute gepflegt.

An der georgischen Schwarzmeerküste ließen sich Griechen aus dem Pontos ("Póndos" ) im Mittelalter ebenso nieder wie die Urumer in Abchasien. Viele dieser Familien wurden aber von den Einheimischen assimiliert, die anderen sind nach dem Fall des Eisernen Vorhangs meist nach Griechenland eingewandert.

Daneben siedelten Griechen bis ins 20. Jahrhundert an der bulgarischen Schwarzmeerküste um die Stadt Burgas sowie in Ostthrakien. In den Städten Constanța, Plowdiw (gr. "Philippópoli" ), Warna und Odessa bildeten sie große Gemeinden. In der Ukraine, in Teilen der Krim und um die Stadt Mariupol leben bis heute beträchtliche griechische Minderheiten, die ebenfalls eine Variante des Pontischen sprechen.

Die "Rum" sind Nachfahren der griechischen Byzantiner. Nach dem Fall Konstantinopels 1453 konvertierten viele der im Schwarzmeerraum verbliebenen griechischen Christen zum Islam. Ihre muslimischen Nachkommen sprechen ebenfalls Pontisch (türk. "Rumca").

Außer den bereits angesprochenen Pontiern lebten bis 1922 auch in anderen Regionen Kleinasiens Griechen. Die größte griechische Stadt in dieser Zeit war Smyrna ("Smyrni" ), heute İzmir. Fast die gesamte heute türkische Ägäisküste war von Griechen besiedelt, da dort bereits in der vorchristlichen Antike griechische Kolonien gegründet worden waren. In einigen Regionen stellten sie die überwiegende Bevölkerungsmehrheit, insgesamt rund zehn Prozent der Bevölkerung. Alle außer den griechischen Bewohnern Konstantinopels mussten im Zuge des Bevölkerungsaustauschs nach 1922 ins griechische Staatsgebiet umsiedeln. Nach dem Pogrom von Istanbul im Jahre 1955 verließen auch die meisten in Istanbul verbliebenen Griechen ihre Heimat. Heute leben außer auf den türkischen Ägäisinseln Gökçeada (gr. "Imbros" ) und Bozcaada (gr. "Tenedos" ) sowie in Istanbul (gr. "Konstantinoúpolis" Κωνσταντινούπολις) keine Griechen mehr in der Türkei. Davon wohnten 2006 noch 1.650 in Istanbul.

Auch an der Südküste, in der heutigen türkischen Provinz Hatay, lebten viele Griechen. Im Sandschak Alexandrette lebten antiochenische Griechen; die Zahl ging von 50.000 im Jahre 1895 auf rund 30.000 in den 1930er Jahren zurück, und 1995 wurde die dortige Bevölkerung an griechischstämmigen Türken auf 10.000 geschätzt. Die verbliebenen Griechen in der Provinz Hatay mussten zwangsweise Türkisch sprechen, so dass sie schnell assimiliert wurden. Im Jahr 1999 lebten noch 2.500 Griechen in der Türkei.
Während noch zu Beginn des 20. Jahrhunderts fast 500.000 Griechen in Ägypten in und um die Stadt Alexandria lebten, waren es 1950 nur mehr noch knapp 100.000 und im Jahr 2000 kaum mehr als 800. Daneben gab und gibt es auch noch heute einige kleinere griechische Gemeinden im Irak und im Libanon.

Seit den 1930er Jahren und nach dem Holocaust begann eine Emigration griechischer Juden nach Israel, die heute weitgehend in die israelische Gesellschaft assimiliert sind.

Die Griechen in den Vereinigten Arabischen Emiraten und in Bahrain sind als Fachkräfte oder Geschäftsleute in den letzten Jahrzehnten dorthin abgewandert.

Griechenland war wie andere europäische Länder im späten 19. Jahrhundert von einer Auswanderungswelle nach Nordamerika und Australien betroffen. Mitunter kamen auch politische Gründe hinzu.

"Hauptartikel: Griechen in Deutschland"

Seit 1700 emigrierten vor allem griechische Kaufleute nach Deutschland, sie waren im Pelzhandel, im Tabak- und Südfrüchtehandel tätig.

Etwa 1.000.000 Griechen waren im Laufe der Gastarbeiterzeit in der Bundesrepublik Deutschland. Da aber eine dauernde Fluktuation herrschte, erreichte die Wohnbevölkerung mit über 400.000 Griechen in den Jahren 1973 und 1974 ihren Höchststand. Sie ging nach dem Sturz der griechischen Militärdiktatur 1974 bis 1976 um ein Achtel zurück. Heute leben etwa 300.000 in Deutschland; die Verteilung ist allerdings regional sehr unterschiedlich. Es existiert ein starkes Süd-Nord-Gefälle. Außerdem leben mehr Griechen in städtischen Gebieten als auf dem Land.

Während und nach dem griechischen Bürgerkrieg emigrierten viele griechische Kommunisten aus politischen Gründen in die DDR oder schickten ihre Kinder in dortige Kinderheime. Diese Welle endete erst mit dem Ende der Militärdiktatur.

Die Entwicklung der griechischen Wohnbevölkerung in Deutschland (seit 1967)

"Hauptartikel: Griechen in Österreich"

Seit dem 17. Jahrhundert kamen griechische Kaufleute und Unternehmer nach Österreich. Sie waren im Handel und im Bankenwesen tätig, Mitte des 20. Jahrhunderts kamen auch viele Studenten. Im Gegensatz zu den Griechen in Deutschland zeichnet sich diese Auslandsgemeinde durch eine größere Homogenität und eine geringere Fluktuation während der verschiedenen Jahrzehnte aus.

Hier leben etwa 212.000 Griechen. Gerade in London leben sehr viele griechisch-zypriotische Einwanderer, was damit zusammenhängt, dass Zypern von 1878 bis 1960 unter britischer Herrschaft stand.

In Frankreich leben etwa 35.000 Griechen. Viele bekannte griechische Persönlichkeiten waren während der griechischen Militärdiktatur im französischen Exil.

Als Teil der europäischen Einwanderungswellen im 19. und zu Beginn des 20. Jahrhunderts wanderten auch viele Griechen in die USA und nach Kanada aus. Viele von ihnen bewahrten ihre kulturelle Identität. Die griechische Botschaft in den USA schätzt die dortige Zahl der Griechen auf 2.000.000. Nochmals etwa 350.000 leben in Kanada.
In und um Chicago leben etwa 200.000 Griechen, in und um New York weitere 200.000. Die US-Gemeinde mit dem höchsten griechischstämmigen Bevölkerungsanteil (9,3 %) ist Tarpon Springs in Florida. In Montréal und Toronto in Kanada schätzt man die Zahl der griechischen Einwohner auf jeweils 120.000. Straßen sind in diesen Wohngebieten in Nordamerika oftmals auch griechisch beschildert.

Im Jahre 2000 lebten 1.153.295 Menschen griechischer Abstammung in den USA, davon beherrschten noch 365.435 ihre griechische Muttersprache.
2012 waren 133.917 Einwohner der USA in Griechenland geboren.

Einwanderung von Griechen in die USA:

Während der Auswanderungswelle nach Nordamerika verschlug es auch etwa 50.000 Griechen nach Südamerika, vor allem nach Brasilien, wo alleine in São Paulo 20.000 Griechen leben.

Auch diese Griechen sind Auswanderer und deren Nachkommen. 75 Prozent der etwa 700.000 Griechen in Australien leben in Sydney und Melbourne. Mittlerweile ist Melbourne die drittgrößte von Griechen bewohnte Stadt der Welt und die größte außerhalb Griechenlands.

Nach einer Hypothese ist das (offensichtlich indoeuropäische) Volk der Dayuan, das um 130 v. Chr. in chinesischen Quellen beschrieben wird, aus Nachfahren griechischer Siedler aus der Zeit Alexanders des Großen hervorgegangen. So wird z. B. spekuliert, dass der Namensbestandteil Yuan eine Transliteration der Wörter "Yona" oder "Yavana" ist, die in Pali das Wort ‚Ionier‘ umschreiben. Demnach würde "Dayuan" (wörtlich: ‚Große Yuan‘) eigentlich ‚Große Ionier‘ bedeuten. Der Kontakt der Dayuan mit den Chinesen gilt als historisches Schlüsselereignis, da er den ersten Kontakt zwischen einer indoeuropäischen und der chinesischen Kultur darstellte. Diese Begegnung legte den Grundstein für die Entstehung der Seidenstraße, die die zentrale Verbindung zwischen Ost und West, sowohl zum Austausch von Waren als auch von kultureller Identität bildete, und vom 1. Jahrhundert v. Chr. bis zum 15. Jahrhundert Bestand hatte.

Das Volk der Chitral Kalasha oder "Schwarzen Kafiri" ist eine ethnische Minderheit der Nordwestprovinz Pakistans. Sie lebt in einer abgeschiedenen Bergregion Chitrals, den Tälern "Bumburiet, Birir" und "Rumbur", und sieht sich als direkte Nachfahren der Makedonen aus der Zeit Alexanders des Großen. Allerdings werden diese Annahmen, da es Hinweise auf ein deutlich früheres Bestehen lange vor Alexanders Invasion in Persien gibt, in neuerer Zeit stark bezweifelt. Die Chitral Kalasha sprechen "Kalasha-mun", auch "Kalasha" genannt, eine vom Aussterben bedrohte indoiranische bzw. dardische Sprache. Etwa dreitausend Angehörige dieser Ethnie haben, als einziges Volk in der Gegenwart, eine polytheistische Religion mit vermuteten Bezügen zu jener der antiken Griechen bzw. der frühen Proto-Indoeuropäer bewahrt. Die teils deutlichen europäischen Züge in ihrer Kultur sowie in ihren physischen Merkmalen haben zu verschiedenen Hypothesen, beispielsweise einer unmittelbaren Abstammung von den antiken Griechen oder den Proto-Indoeuropäern, geführt.

Die Urum (Eigenbezeichnung: "Urum", Pl. "Urumları") sind eine kleine turksprachige Minderheit vorwiegend im Kaukasus, der Südwestukraine, der Krim und dem Balkan. Als Alternativbezeichnung ist aus der deutschen Turkologie auch der Begriff "Graeko-Tataren" bekannt. Die Angehörigen dieser Volksgruppe sind aus ethnischer Sicht als Griechen (türkisch "Rum" ‚Grieche‘) anzusehen, deren Vorfahren (rund 9.600 Menschen) um das Jahr 1780 die tatarische Sprache annahmen. Die Volksgruppe der Urum umfasst heute rund 13.000 Menschen. Die Urum sind griechisch-orthodoxe Christen. Bei Volkszählungen werden die Urum in Georgien aufgrund ihres Glaubens als „Griechen“ und nicht als Turkvolk aufgeführt.



</doc>
<doc id="10173" url="https://de.wikipedia.org/wiki?curid=10173" title="Panama">
Panama

Panama () ist ein Staat in Mittelamerika, der an Costa Rica im Westen und Kolumbien im Osten grenzt. Der das Land durchquerende Panamakanal verbindet die Karibik im Norden und den Pazifischen Ozean im Süden.

Panama erhielt 1903 seine Unabhängigkeit von Kolumbien. Danach stand das Land lange unter dem Einfluss der USA. Im Jahre 1999 gaben die USA schließlich die volle Kontrolle des Panamakanals an den Staat Panama ab. Dank den Einnahmen des Kanals zählt Panama heute zu den reichsten Ländern in Lateinamerika. Im Index der menschlichen Entwicklung belegte das Land 2016 Platz 60 von 188 Ländern.

Der Staat Panama hat eine Fläche von 75.517 km², davon sind 210 km² Binnenwasserflächen. Seine Küste ist 1.915 km lang.

Panama nimmt den schmalsten Teil der mittelamerikanischen Landbrücke ein, die hier vom Panamakanal durchbrochen wird, der Isthmus von Panama ist nur 60 km breit. Der höchste Berg des Landes ist der Volcán Barú mit 3477 m. Er ist Teil der mittelamerikanischen Kordillere, die Panama parallel zu den Küsten durchzieht.

Die am dichtesten besiedelten Gebiete liegen an der pazifischen Küste und um die Kanalzone. Über die Hälfte der Bevölkerung lebt in Städten.

Panama grenzt im Westen an Costa Rica und im Osten an Kolumbien mit Grenzlängen von 330 bzw. 225 Kilometern.

An der nördlichen, der karibischen Küste Panamas ist von Januar bis März/April Trockenzeit, ab Ende Juli bis September kleine Trockenzeit. Die Inselgruppe Bocas del Toro im äußersten Nordwesten Panamas beherbergt einen Nationalpark und ist mit ihren weißen Stränden eines der beliebtesten Touristenziele von Panama. Die Flora und Fauna ist durch den Tourismus zunehmend bedroht. Die nord-östliche Karibikküste ist Heimat der Kuna Yala-Indianer (auch "San Blas"), die anerkannte Autonomie innerhalb der Region besitzen.

Die Halbinsel Azuero liegt etwa auf halber Strecke zwischen Panama-Stadt und David. Auf der gesamten Fahrt via Chitré und Pedasi überwiegen Weideflächen, die nur gelegentlich von Reis- und Maisanbauten unterbrochen werden. Nach und nach geht die Strecke in das Canajagua-Bergland über, in dem die Erosion eigenartige, kegelartige Hügel hinterlassen hat. Im südwestlichen Teil, dem trockensten Gebiet Panamas, liegt der etwa 8.000 ha große, halbwüstenartige Nationalpark Parque Nacional Sarigua.

Die Provinz Darién ist großteils durch tropischen Regenwald gekennzeichnet. Für Touristen sind der Osten und der Süden der Provinz gefährlich. Insbesondere im Grenzgebiet zu Kolumbien bestehen hohe Sicherheitsrisiken, da hier Schmuggler und Drogenhändler aus Kolumbien sowie Widerstandskämpfer die staatliche Ordnung und Sicherheit weitgehend ausgeschaltet haben.

Panama ist eine natürliche Landbrücke und verbindet Nord- und Zentralamerika mit Südamerika. So kommen hier einige Tierarten aus beiden Teilen vor, wie beispielsweise die Agutis, Tapire oder auch die Harpyie. Geschätzt leben in Panama rund 300.000 Insektenarten. Mehr als 29 % der Landfläche ist in 15 Naturparks aufgeteilt, die dem Schutz und Arterhalt der heimischen Tier- und Pflanzenwelt dienen.

Gemäß dem letzten Census vom 19. Mai 2010 hatte Panama rund 3,406 Mio. Einwohner.
Die amtliche Bezeichnung der Einwohner lautet Panamaer bzw. Panamaerin. Umgangssprachlich wird auch oft die Bezeichnung Panamenier abgeleitet von span. Panameños oder Englisch Panamanians benutzt. Die Lebenserwartung beträgt 2016 für Männer 75,8 und für Frauen 81,6 Jahre, sie ist damit eine der höchsten in Lateinamerika. In einer Umfrage des Meinungsforschungsinstitutes Gallup vom Dezember 2012 zählten die Einwohner des Landes mit zu den glücklichsten Menschen auf der Erde.

Quelle: UN

Beinahe zwei Drittel (rund 60 %) der Panamaer sind Mestizen, stammen also sowohl von Indianern als auch von Europäern ab. Sie leben vor allem in den im Westen des Landes gelegenen Provinzen Coclé, Herrera und Veraguas, wo sie bis zu 90 % der dortigen Bevölkerung stellen. 

Die zweitstärkste Gruppe bilden die Schwarzen und Mulatten mit insgesamt ca. 15 % (in den Provinzen Darién und Bocas del Toro jeweils um 50 %). Sie lassen sich in zwei Gruppen einteilen, die jeweils die Hälfte aller Afropanamaer ausmachen. Sie stammen meist entweder von durch Spanier aus Afrika verschleppten Sklaven ab und sprechen daher spanisch, oder von Einwanderern von den westindischen Inseln, welche ein mit westafrikanischen und auch französischen Elementen durchsetztes kreolisches Englisch sprechen. So stammt das Wort für Maismehl "kukúu" aus der Sprache der Ewe, für essen "yampí" aus der der Wolof.

Auf die Afropanamaer folgen mit 13 % die Nachkommen der europäischen Siedler (in den Provinzen Panamá, Los Santos, und Chiriquí über 15 %). Die Indigenen haben einen Anteil von 8,3 % an der Bevölkerung (größte Gruppen: Guaymí und Cuna). Asiaten machen einen Anteil von rund 4 % an der Bevölkerung aus.

Die Bevölkerung Panamas ist überwiegend christlich; 86 % (nach anderen Angaben 75 bis 80 %) sind Katholiken und 10 % (nach anderen Angaben 15 bis 25 %) Protestanten (meist Evangelikale). Es gibt jeweils ungefähr 1 % Juden und Muslime. Unter den Ngäbe (Ngöbe) gibt es viele Anhänger der Bahai-Religion. Auch Mormonen sind stark vertreten. Einige Gruppen der indigenen Kuna, Ngäbe, Buglé und Embera sind nach wie vor Anhänger ihrer traditionellen mesoamerikanischen Religionen.

Im Jahr 1821 spaltete sich Panama von Spanien ab und wurde Teil von Großkolumbien unter Simón Bolívar. In den 1830er und 1840er Jahren wurde mehrmals die Unabhängigkeit von der Provinz Granada (ehemals Kolumbien) erklärt, aber schnell rückgängig gemacht.

Infolge einer militärischen Intervention der USA spaltete sich das heutige Panama nach dem "Panamakonflikt" am 3. November 1903 von Kolumbien ab. Vorausgegangen war der 1000-Tage-Krieg, in dem der Indigene Victoriano Lorenzo für die Selbstbestimmung der Indianer kämpfte, was er mit seinem Leben bezahlte: er wurde erschossen. Damit verloren die Indianer die Chance auf Landbesitz und politische Repräsentation in dem neuen Staat.

Gleich darauf unterzeichnete Philippe Bunau-Varilla als panamaischer Unterhändler den Hay-Bunau-Varilla-Vertrag, der den USA erlaubte, den Panamakanal zu bauen und gleichzeitig die Hoheitsrechte für das Gebiet um den Kanal für sich zu beanspruchen, die so bezeichnete "Panamakanalzone". Zwischen 1904 und 1914 errichteten Ingenieure der US Army den Panamakanal. Erste Arbeiten an dem Kanal hatten allerdings schon unter französischer Führung im Jahre 1881 begonnen.

Von 1959 bis 1962 wurde die "Puente de las Américas", eine den Kanal überspannende Brücke, unter den US-Amerikanern erbaut, die über 40 Jahre die einzige Landverbindung zwischen Nord- und Südamerika bleiben sollte. Am 15. August 2004 wurde eine zweite Brücke namens "Puente del Centenario" unter der scheidenden Präsidentin Mireya Moscoso eröffnet. Sie wurde allerdings erst 2005 in Betrieb genommen. Derzeit (2018) ist eine dritte Brücke bei Colón in Bau.

1964 kam es in der Panamakanalzone zum Flaggenstreit zwischen den USA und Panama.

Am 7. September 1977 wurden die Torrijos-Carter-Verträge unterzeichnet (dem Vertrag den Namen gegeben haben der damalige De-facto-Präsident Panamas, Omar Torrijos, und der damalige US-Präsident Jimmy Carter), der die Übertragung der gesamten Kanalzone von den USA an Panama bis Ende 1999 zusicherte.

Im Dezember 1989 wurde der Diktator Manuel Noriega, der Panama nach dem mysteriösen Tod von Omar Torrijos 1981 mit Hilfe des Militärs regierte, durch die USA (unter George H. W. Bush) in der Operation Just Cause gestürzt. Bis zu seinem Sturz war Noriega einer der wichtigsten Verbündeten der Vereinigten Staaten in Mittelamerika. Der General wird oft fälschlicherweise als Präsident von 1983 bis 1989 angesehen. Er war jedoch lediglich der Chef der Nationalgarde, die später in die "Fuerzas de Defensa de Panamá" umgewandelt wurde. Fünf Tage vor seiner Festnahme erhob ihn die Nationalversammlung zum Chef der Regierung mit außerordentlichen und zeitlich unbeschränkten Rechten.

Am 31. Dezember 1999 um 12:00 Uhr wurden gemäß dem Torrijos-Carter-Vertrag das gesamte US-Gebiet entlang des Kanals sowie alle US-amerikanischen Militärbasen offiziell an Panama übergeben. Der Betrieb und die Verwaltung des Kanals wurden von der panamaischen Panamakanal-Behörde (Autoridad del Canal de Panamá – ACP) übernommen. Nach Angaben der Regierung ist aber geplant, dass die USA zwei Marinestützpunkte in Panama errichten werden, in Bahia Piña und Punta Coca.

Das Parlament in Panama ist die Nationalversammlung. Seit Juli 2014 ist Juan Carlos Varela von der Partido Panameñista (PP), der Ricardo Martinelli von der Cambio Democrático (CD) nach der Präsidentschaftswahl 2014 ablöste, Präsident Panamas.

Im Demokratieindex 2016 belegt Panama Platz 45 von 167 Ländern, womit das Land als eine „fehlerhafte Demokratie“ gilt. Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Panama 2016 von 176 Ländern zusammen mit Mongolei und Sambia auf dem 87. Platz, mit 32 von maximal 100 Punkten.

Am 22. Oktober 2006 wurde in einer Volksabstimmung für den Ausbau des Panamakanals entschieden. 78 % stimmten dabei für den Ausbau (bei einer Wahlbeteiligung von 44 %). Der Kanal, eine der Haupteinnahmequellen des Landes, hatte seine Kapazitätsgrenze von 14.000 Schiffen pro Jahr erreicht.
Der neu ausgebaute Kanal wurde am 26. Juni 2016 feierlich in Betrieb genommen.

Panama ist seit der Bildung von Panamá Oeste 2014 verwaltungsmäßig in zehn Provinzen (provincias) und fünf Territorien (comarcas) eingeteilt. In den autonomen Territorien gibt es eine Selbstverwaltung der indigenen Volksgruppen Kuna, Ngöbe, Buglé, Embera und Wounaan.

Provinzen (in Klammer die Provinzhauptstadt):

Autonome indigene Territorien mit Provinzstatus:

Comarcas ohne vollständigen Provinzstatus

Die Hauptverbindungsstraße ist die Panamericana, die durch ganz Panama führt. Die Panamericana (engl. "Pan American Highway") verbindet eigentlich Nordamerika mit Südamerika und führt von Alaska bis Süd-Chile, doch im Grenzgebiet zwischen Panama und Kolumbien, im Urwaldgebiet des Darién, befindet sich eine Lücke, der so genannte "Tapón del Darién" (auch unter dem englischen Namen "Darien Gap" bekannt), die bisher noch nicht geschlossen wurde. Die Ursache dafür liegt vor allem darin begründet, dass das Gebiet von Sümpfen durchzogen ist und darüber hinaus von kolumbianischen Guerrilleros gehalten wird, die dort Schmuggel und Drogenhandel betreiben, so dass der Aufwand für die Sicherheit beim Bau der Straße sehr hoch wäre. Zudem sind auch Bedenken von Umweltschützern laut geworden, so dass die panamerikanische Nord-Süd-Verbindung wohl in naher Zukunft nicht fertiggestellt werden wird.

Die Nord-Süd-Verbindung zwischen Atlantik und Panama-Stadt am Pazifik ist als Transistmica bekannt. Heute verläuft neben dem Panamakanal die Bahnstrecke (vor kurzem renoviert) sowie in einiger Entfernung eine Straßenverbindung zwischen Panama-Stadt und Colón. Auf ihr passieren viele Unfälle (durch hohes Verkehrsaufkommen und schlechten Zustand). Von 2007 bis 2009 wurde parallel zur Transistmica eine 42 km lange neue Autobahn durch den brasilianischen Odebrecht-Konzern gebaut, um die Fahrt zwischen Colón und Panama-Stadt zu verkürzen und sicherer zu machen.

2005 wurden 5,8 Terawattstunden elektrischer Strom erzeugt, davon 36 Prozent in Wärmekraftwerken und der Rest fast ausschließlich in Wasserkraftwerken.

2016 nutzten 45,2 % der Bevölkerung das Internet.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Panama Platz 50 von 137 Ländern (Stand 2017–2018). Zusätzlich hat Panama die globalisierteste Wirtschaft der gesamten Region. Ein Hauptgrund dafür ist die Drehscheibenfunktion des Landes dank des 2016 für größere Schiffe erweiterten Panama-Kanals und eines damit verbundenen Netzes von in der Region führenden, hochmodernen und  effizienten Häfen - sowohl am Atlantik als auch am Pazifik - sowie zahlreicher Freihandels- und Sonderwirtschaftszonen, die von überregionaler Bedeutung sind. Panamas BIP pro Kopf betrug 2016 23.034 US-Dollar (KKB), damit lag Wohlstandsniveau ungefähr auf dem Level von Kroatien. Das Land ist damit eines der reichsten in Lateinamerikas. Dank dem steigenden Handel expandierte die Wirtschaft in den letzten Jahren mit jährlich 5 bis 6 Prozent.

Wichtigste Einkunftsquellen der panamaischen Volkswirtschaft sind der Panamakanal, in dessen Verwaltung, Betrieb und Instandhaltung rund 8000 Menschen arbeiten, sowie die Registrierung von Schiffen. In Panama sind weltweit die meisten Schiffe registriert, fast jedes fünfte. Der Grund dafür sind das vergleichsweise unkomplizierte Verfahren und die geringen Steuern.

Die größte Einkommensquelle Panamas hängt mit dem Betrieb des Panamakanals zusammen, der lange Zeit unter der gemeinsamen Verwaltung Panamas und der Vereinigten Staaten stand und seit dem 31. Dezember 1999 der alleinigen Verfügungsgewalt Panamas unterliegt. Für die Wirtschaft, die bereits unter den Sanktionen der Vereinigten Staaten litt, bedeutete der amerikanische Einmarsch vom Dezember 1989 einen weiteren Rückschlag. Das Bruttoinlandsprodukt (BIP) beträgt 13.733 Millionen US-Dollar (2004; Dienstleistungen: 74 Prozent, Industrie: 18 Prozent, Landwirtschaft: 8 Prozent); daraus ergibt sich ein BIP pro Kopf von 4.210 US-Dollar.

Die Gesamtzahl der Arbeitskräfte Panamas, die einer regulär bezahlten Arbeit nachgehen, liegt bei weniger als einem Drittel der Gesamtbevölkerung. Der Rest arbeitet für den Eigenbedarf, lebt von der Schwarzarbeit oder ist arbeitslos (2005: 9,6 Prozent). 19 Prozent der regulär beschäftigten Erwerbstätigen arbeiten in der Land- und Forstwirtschaft sowie der Fischerei, 62 Prozent sind im Handels-, Finanz- und Dienstleistungsbereich beschäftigt, 19 Prozent in der Industrie. Knapp 17 Prozent aller Lohnarbeiter sind gewerkschaftlich organisiert.

Am 28. Januar 2016 legte die EU-Kommission ein Maßnahmenpaket "zur Bekämpfung von Steuerflucht" vor, bei dem unter anderem Panama auf der "schwarzen Liste" der Steueroasen auftaucht. 
Nach den Enthüllungen der Panama Papers versprach Präsident Varela mehr Steuertransparenz und Offenheit.

8,8 Prozent der Bodenfläche Panamas werden landwirtschaftlich genutzt. Hauptsächlich werden Bananen, Plátanos, Zuckerrohr, Reis, Ananas, Mais und Kaffee angebaut. Zum Tierbestand gehören Rinder, Schweine und Hühner.

Wälder bedecken etwa 45 Prozent der Landesfläche. Eine nationale Forstbehörde verwaltet den Großteil der Waldflächen – etwa zwei Drittel stehen unter Schutz, das verbleibende Drittel wird als Wirtschaftswald genutzt.

Die Fischerei ist ein bedeutsamer Wirtschaftszweig. Zu den wichtigsten Fangprodukten gehören Garnelen und Krabben.

In Panama werden geringe Mengen Gold und Silber abgebaut Salz wird an der Pazifikküste gewonnen. Die meisten Erzeugnisse der Fertigungsindustrie wie Zement, Zigaretten, Schuhe, Bekleidung, Seife, verarbeitete Nahrungsmittel und alkoholische Getränke sind für den einheimischen Markt bestimmt. Erdölraffinerien produzieren hauptsächlich für die Ausfuhr.

Panama hat seit den 1980er Jahren eines der größten Bankwesen in Lateinamerika und zog darum auch deutsche Vermögende an. Insbesondere Offshore-Geschäfte werden im großen Stil angeboten, unter anderem von Großkanzleien wie die durch die Panama Papers bekannt gewordene Mossack Fonseca.

Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Panama 2017 von 180 Ländern zusammen mit Brasilien, Kolumbien, Indonesien, Peru, Thailand und Sambia auf dem 96. Platz, mit 37 von maximal 100 Punkten.

Die Währungseinheit von Panama ist der Balboa (PAB), dabei ist 1 PAB in 100 Centésimos unterteilt. Der US-Dollar ist ebenfalls offizielles Zahlungsmittel. Der Balboa ist 1 zu 1 an den US-Dollar gekoppelt. Neben US-Cent-Münzen sind auch Balboa-Münzen im Umlauf. Ab 1 PAB/USD wird nur noch mit US-Dollar-Scheinen bezahlt. Die Nationalbank von Panama ist die 1904 gegründete Staatsbank.

Die wichtigsten Exportgüter des Landes sind Bananen, Ananas, Erdölerzeugnisse, Garnelen, Rohzucker und Kaffee. Über 60 Prozent davon gehen in die Vereinigten Staaten. Die Importe stammen hauptsächlich aus den Vereinigten Staaten und China. Weitere Hauptlieferanten Panamas sind Mexiko und Japan. Eingeführt werden in erster Linie fossile Brennstoffe, Textilien, Konserven, Maschinen, Chemikalien, Fördereinrichtungen und Rohstoffe. Die Handelsbilanz ist negativ.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 12,4 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 11,7 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 1,2 % des BIP.
Die Staatsverschuldung betrug 2016 39,7 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Panamas Straßennetz ist einschließlich eines Abschnitts der Panamericana 11.400 Kilometer lang.

Das Land verfügte einst über rund 450 Schienenkilometer, wovon rund 380 km Schmalspurbahnen waren, die letzten wurden 2008/2009 abgebaut. Die etwa 75 km lange Hauptstrecke der Eisenbahn verläuft quer durch den Isthmus von Panama und verbindet die Stadt Colón an der Karibik mit Panama-Stadt am Pazifik, im Wesentlichen dem Kanal entlang. Die Panama Canal Railroad, errichtet worden 1850–1855, Streckenumlegung 1904–1912, besaß die Spurweite von 1524 mm, sie wurde in den 1990er Jahren privatisiert und von der Kansas Southern Industries aus den USA übernommen, modernisiert und 2001 auf Normalspur 1435 mm umgespurt. Es findet Güterverkehr, seit 2001 auch mit Doppelstock-Containertragwagen, statt, zudem verkehrt an Werktagen ein Personenzug in jede Richtung und für die Reisenden von Kreuzfahrtschiffen werden Charterzüge gefahren.

Der Panamakanal verbindet das Karibische Meer mit dem Pazifischen Ozean. Die größten Häfen des Landes sind Balboa, Cristóbal, Bocas del Toro, Almirante und Puerto Armuelles. Die Handelsflotte von Panama ist mit 6184 Schiffen z. Zt. die größte der Welt, was allerdings auf die Praxis der Ausflaggung zurückzuführen ist: fast alle hier registrierten Schiffe befinden sich in ausländischem Besitz und sind mit ausländischen Mannschaften besetzt.

In der Nähe der Stadt Panama liegt der internationale Flughafen Aeropuerto Internacional de Tocumen (PTY) nahe Tocumen.
Außerdem befindet sich in der ehemaligen Kanalzone in unmittelbarer Nähe des zentralen Busterminals der Regionalflughafen Marco A. Gelabert. Von dort aus erreicht man Provinzhauptstädte wie David (Chiriqui) oder Changinola und Isla Colon (Bocas del Toro). Die Inlandfluggesellschaften sind Aeroperlas Regional (zur Grupo TACA gehörend) und die private Air Panama.
Die Fluggesellschaft Aeroperlas Regional hat Ende Februar 2012 Konkurs angemeldet und seitdem den regionalen Flugverkehr komplett eingestellt.

Nationalfeiertag ist der 3. November, an dem der Unabhängigkeit von Kolumbien gedacht wird.

Panamas Volksbrauchtum ist durch spanische, afrikanische und indigene Einflüsse geprägt. Ihren Ausdruck findet es vor allem in Musik, Tanz und Festen wie z. B. im Karneval. Historische sowie moderne Tänze aus dem Nachbarland Kolumbien und der Karibik formen, zusammen mit den typischen lateinamerikanischen Tänzen, eine große Vielfalt. Bekannte Tänze sind der aus dem 17. Jahrhundert stammende "Tamborito" und der von der Guineaküste stammende Cumbia im Zweiviertel- und Viervierteltakt, der auch in Kolumbien und El Salvador verbreitet ist. Trommeln und andere Perkussionsinstrumente spielen bei diesen Tänzen eine bedeutende Rolle.

Zu nennen sind weiterhin:

Einer der bekanntesten Salsa-Musiker der Welt stammt aus Panama: Rubén Blades, er war von 2004 bis Juni 2009 Tourismusminister seines Landes.

Die meisten Tänze werden nach wie vor von breiten Teilen der Bevölkerung beherrscht.

Das erste in der Kolonialzeit Panamas veröffentlichte Werk ist das "Llanto de Panama con la muerte de Don Enrique Enriquez" aus dem 17. Jahrhundert. Schon vor der Unabhängigkeit im Jahre 1903 gab es im heutigen Panama Vertreter einer romantischen Literatur, in deren Arbeit sich die liberalen und nationalen Strömungen der Zeit und vor allem Kolumbiens spiegelten. Mit der Ankunft des Modernismo in der Lyrik radikalisierte sich die nationale Bewegung. Zu den ersten Modernisten zählte der von Mallarmé beeinflusste Diplomat Darío Herrera (1870–1914), ein Freund Rubén Daríos.

Der bekannteste Autor des 20. Jahrhunderts ist Ricardo Miro (1883–1940), der aus politischen Gründen das Studium der Malerei in Bogotá aufgeben musste. Nach der Unabhängigkeit verfasste er das Gedicht "Patria" (1904), das eine Art informeller Nationalhymne darstellt. Von ihm stammen auch die Romane „Las noches de Babel“ und "Flor de María". Nach ihm ist ein Literaturpreis benannt. Keiner Schule zuordnen lässt sich die naturverbundene Lyrik von María Olimpia de Obaldía (1891–1985), die sich für die Frauen des indigenen Volkes der Ngäbe einsetzte.

Panama hat in der zweiten Hälfte des 20. Jahrhunderts zahlreiche weitere Schriftsteller hervorgebracht, deren Werke allerdings in der Regel bislang nicht ins Deutsche übersetzt wurden. Beeinflusst wurden sie unter anderem von kolumbianischen, nicaraguanischen und guatemaltekischen Autoren. Dennoch blieben lokale Themen wie der Kanal und das Leben auf dem Lande wichtig.

Internationale Anerkennung fanden die Werke der in Venezuela als Tochter panamesisch-nikaraguanischer Eltern geborene, zeitweise in Kolumbien lebende Literaturwissenschaftlerin, Essayistin und Erzählerin Gloria Guardia (* 1940), deren Werk durch die Aufhebung der Grenzen zwischen den Genres und eine postmoderne Collagetechnik gekennzeichnet ist. Zu den bekannteren Autoren gehören auch Joaquín Beleño (1922–1988), dessen Erfahrungen mit der US-amerikanischen Verwaltung der Kanalzone sich in seinen Romanen niederschlugen, der realistische Erzähler und Lyriker Tristán Solarte (* 1924), der 1984 beinahe einem Attentat von Paramilitärs zum Opfer gefallen wäre, sowie die vielseitige Autorin und Ärztin Rosa María Britton (* 1936) und der Erzähler, Lyriker und Essayist Enrique Jaramillo Levi (* 1944), der mehr als 50 Bücher verfasst oder herausgegeben hat.

Das Themenspektrum der Autoren der jüngeren Generation hat sich erheblich erweitert; sie schreiben über das urbane Chaos oder auch über phantastische Themen. Unter ihnen sticht die von karibischen Vorfahren abstammende Musikerin und Musiktherapeutin Melanie Taylor (* 1972) hervor, die sich von der Lyrik Guatemalas inspirieren lässt und auch Mikrogeschichten ("Minificción") verfasst. Elektronische Minitexte, Fotoessays und Erzählungen verfasst der Ingenieur, Computerwissenschaftler und Kulturmanager José Luis Rodríguez Pittí (* 1971), der den Blog "minitextos.org" herausgibt. Dadurch hat sich die mediale Reichweite der Autoren des kleinen Landes stark vergrößert. Über aktuelle literarische Ereignisse informiert die Website "escritorespanama.com".

Panama hat einen schwach entwickelten Buchmarkt und importiert viele Bücher aus Kolumbien, Mexiko oder Spanien. Literaturpreise verleihen u. a. das "Instituto Nacional de Cultura", die Technische Universität Panama (UTP) und die "Fundación Cultural Signos".

Die bekanntesten panamaischen Maler sind Hernando de la Cruz (1592-1646), Manuel Encarnacion Amador (1869-1952), Alberto Dutary (1928-1997), Etanislao Arias Peña (1952-2003), Adriano Herrerabarría, Roberto Lewis, Pablo Runyan (1925-2002),oder Rodolfo Antonio Méndez Vargas.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Panama Platz 96 von 180 Ländern. Bei der Situation der Pressefreiheit im Land gibt es laut der Nichtregierungsorganisation "Erkennbare Probleme".

Zu den beliebtesten Sportarten in Panama gehören Baseball und Fußball. Der größte Erfolg der Fußballnationalmannschaft war der zweite Platz beim CONCACAF Gold Cup 2013 und 2005 und die Qualifikation für die Fußball-Weltmeisterschaft 2018 in Russland.

Die Küche Panamas ist eine Mischung aus spanischen und karibischen bis hin zu amerikanischen und deutschen Gerichten und Zutaten. Auch der Einfluss der indigenen Bevölkerung hat Panamas Speisekammer maßgeblich beeinflusst. Zu den bekanntesten Gerichten des Landes gehören heute der Sancocho, ein Eintopf aus Huhn und Gemüse, die sogenannten Tamales, eine Art Maisteig mit verschiedenen Zutaten wie Fleisch oder Rosinen und Yuca al Mojo, gekochte Maniokstücke mit Olivenöl.






</doc>
<doc id="10174" url="https://de.wikipedia.org/wiki?curid=10174" title="Costa Rica">
Costa Rica

Costa Rica ([], spanisch für „reiche Küste“), früher auch "Kostarika", ist ein Staat in Zentralamerika, der im Norden an Nicaragua und im Süden an Panama grenzt. Im Osten ist er durch die Karibik und im Westen durch den Pazifik begrenzt. Das Land gilt als eines der fortschrittlichsten Lateinamerikas. So wurde die Armee bereits 1948 zugunsten der Förderung von Bildungs- und Gesundheitsprogrammen abgeschafft, das Land gewinnt knapp 100 % seines Strombedarfs aus regenerativen Quellen und der Ökotourismus wird stark gefördert. Rund 27 % der Landesfläche stehen unter Naturschutz.

Im Gegensatz zu vielen anderen Ländern der Region ist es seit den 1950er Jahren eine stabile Demokratie und blieb durch frühzeitige sozialpolitische Maßnahmen von der in Lateinamerika des 20. Jahrhunderts weit verbreiteten Problematik der sozialen Unruhen, Bürgerkriege und Diktaturen verschont. Angesichts damaliger bewaffneter Konflikte in benachbarten Ländern erklärte es 1983 seine „dauerhafte und aktive unbewaffnete Neutralität“ und wird auch als „die Schweiz Zentralamerikas“ bezeichnet.

Topographisch lässt sich Costa Rica in fünf Großräume gliedern.


In den Cordilleras gibt es eine Vielzahl noch aktiver und auch erloschener Vulkane, darunter der Turrialba. Die drei am meisten besuchten sind der Volcán Poás (2704 Meter), der Arenal (1633 Meter) und der Irazú (3432 Meter). Der Chirripó Grande (3820 Meter) ist der höchste Berg des Landes.

Geologisch ist Costa Rica vor relativ kurzer Zeit geformt worden. Durch die Subduktion "(Unterschiebung)" der Cocos-Platte unter die Karibische Platte am Mittelamerikanischen Graben formte sich vor etwa 140 bis 65 Millionen Jahren eine Kette von Vulkaninseln. Diese Inseln verbanden sich allmählich durch Anhebungen des Meeresbodens, durch Abtragung der Vulkanhänge und die Anschwemmungen der Flüsse untereinander. Diese Bewegungen halten bis heute an, so dass es fast täglich zu leichten Erdbeben kommt.

Parallel zum Mittelamerikanischen Graben gliedert sich Costa Rica in mehrere geologische Einheiten:

Aufgrund seiner Lage zwischen 8° und 11° nördlicher Breite befindet sich Costa Rica in den Tropen. Allerdings differieren die Niederschläge erheblich: In San José fallen im Jahr 1867 mm, in Puerto Limón an der Karibikküste mit 3518 mm fast doppelt so viel. In Costa Rica sind zwei der tropischen Klimatypen anzutreffen. Die Abgrenzung der beiden Typen wird durch die von Nordwest nach Südost verlaufende Gebirgskette bewirkt. Das wechselfeuchte Klima der Pazifikküste ist durch zwei Jahreszeiten gekennzeichnet: eine Regen- und eine Trockenzeit. Die Regenzeit erstreckt sich im Zentraltal und im nordwestlichen Landesteil von Mai bis November, die Trockenzeit von Dezember bis April. Im niederschlagsreicheren Süden und der zentralen Pazifikküste des Landes setzt die Trockenzeit ein bis zwei Monate später ein und endet ebenfalls im April. Der im zentralen Norden (östlich der großen Gebirgskette) liegende Landesteil liegt in einer klimatischen Übergangszone, in der die Niederschläge während der Trockenzeit der Pazifikregionen Costa Ricas etwas zurückgehen, aber nicht völlig ausbleiben. An der Karibikküste herrscht durch die vorwiegend aus Nordost wehenden Winde, die über der karibischen See Feuchtigkeit aufnehmen, ein äquatoriales Klima mit Niederschlägen zu jeder Jahreszeit vor, wobei die Monate Februar und März sowie September und Oktober trockener ausfallen.

Costa Rica weist eine sehr hohe biologische Vielfalt an Pflanzen und Tieren sowie Landschaftsformen auf. Das Land liegt in den Neotropen. Mit einer Landfläche von rund 51.100 Quadratkilometern und einem Gebiet von 589.000 Quadratkilometern an territorialen Gewässern gehört Costa Rica zu den 20 an Biodiversität reichsten Ländern der Erde.

Die beiden Küsten und die Bergregionen weisen eine Vielzahl verschiedener Mikroklimata auf, die mit ein Grund für die Fülle verschiedener Ökosysteme des Landes sind. Die nachgewiesenen 500.000 Arten des Staates sind vier Prozent der weltweit angenommenen Arten. 300.000 der insgesamt 500.000 Arten sind Insekten.

Klimaschutz, Naturschutz und Waldschutz werden als wichtiger Bestandteil der staatlichen Umweltpolitik angesehen und werden konsequent umgesetzt. Hatten Holzfäller in den 1970er und 1980er Jahren rund 80 % des Regenwaldes gerodet, sind heute wieder mehr als 50 % des Landes von Wald bewachsen. Zum Schutz des Waldes setzt Costa Rica erfolgreich auf den Ökotourismus, von dem sowohl Einheimische als auch umweltbewusste Reisende profitieren. So geben rund 1,5 Millionen Touristen jährlich fast 1,5 Milliarden Dollar für einen Besuch der Regenwälder und anderer ökologischer Ziele Costa Ricas aus.

Rund 27 % der Fläche Costa Ricas stehen unter Naturschutz. 2012 gab es 160 Schutzgebiete wie biologische Reservate, Nationalparks und Naturschutzgebiete.

Es gibt 26 Nationalparks über das Land verstreut mit ganz unterschiedlichen charakteristischen Merkmalen. In dem 1969 erlassenen Gesetz zur Erhaltung des Waldes wurde "Santa Rosa" im Nordwesten des Landes an der Pazifikküste als erster Nationalpark eingerichtet. Gleichzeitig wurde eine Nationalparkverwaltung geschaffen, die aber zunächst weder genügend finanzielle Mittel noch Personal besaß, um den Park wirksam vor Bauern und Neusiedlern zu schützen. Dank der Privatinitiative "Regenwald der Österreicher" ist inzwischen die Situation deutlich besser geworden.

Die zu Costa Rica gehörende unbewohnte Cocos-Insel ("Isla del Coco") liegt 500 Kilometer vor der Küste im Pazifischen Ozean und darf nur mit besonderer Genehmigung betreten werden. Sie ist wie auch die Nationalparks La Amistad und "Guanacaste" von der UNESCO zum Weltkulturerbe erklärt worden.

Vor Puerto Viejo ("Puerto Viejo de Talamanca") befindet sich der Nationalpark Cahuita mit Vögeln, Schlangen, Echsen und Schmetterlingen. In den Baumwipfeln findet man Mantelbrüllaffen. Auf der küstennahen Strecke in Richtung Manzanillo herrscht in den meisten Abschnitten dichter Wald. Im Nationalen Tierschutzgebiet Gandoca-Manzanillo sind Pflanzen, Meer und Tiere geschützt. Auf der Strecke bewegen sich Tukane, Faultiere und Herden von Affen mit kräftigem Greifschwanz, die sich von Blättern ernähren. Ebenfalls häufig in diesem Teil Costa Ricas sind Weißschulterkapuzineraffen.
Die seltenen Rotrücken-Totenkopfäffchen hingegen können nur noch im südpazifischen Raum Costa Ricas, beispielsweise in den Nationalparks Manuel Antonio und Corcovado, beobachtet werden. Die vierte Affenart Costa Ricas, der Geoffroy-Klammeraffe, bevorzugt dichte Waldgebiete und ernährt sich von Insekten und kleinen Früchten.

Eine wichtige Grundlage der staatlichen Umweltpolitik war die „Initiative Frieden mit der Natur“ des Präsidenten Oscar Arias, mit der er der Umweltzerstörung ein radikales Ende setzen wollte. Im Dezember 2007 wurde das Gesetz verabschiedet. Arias erklärte 2007, dass bis zum Jahr 2021, Costa Ricas 200. Geburtstag, das Land als erster Staat der Erde eine ausgeglichene Bilanz beim Ausstoß des Treibhausgases Kohlendioxid (CO) erreichen wolle.

2011 bezeichnete der ehemalige deutsche Bundespräsident Christian Wulff bei seinem Staatsbesuch in Costa Rica das Land als „ökologisches Vorbild“. 

Die Energieversorgung Costa Ricas wird fast ausschließlich aus erneuerbaren Energien gesichert.

Die Bevölkerung setzt sich aus 94 % Mestizen („Mischlingen“) und Weißen, 3 % Schwarzen, 1 % Indios, 1 % Chinesen und 1 % Sonstigen zusammen. Umgangssprachlich werden die Einwohner "Ticos" und "Ticas" genannt.

Unter den Costa-Ricanern mit afrikanischer Abstammung an der Atlantikküste sind viele Nachkommen eingewanderter Arbeiter von den westindischen Inseln, die daher zumeist ein englisches Kreol sprechen.

Costa Rica hat den geringsten Anteil rein indigener Bevölkerung unter den mittelamerikanischen Staaten. Der größte Teil der costaricanischen Guaymí lebt heute zurückgezogen und in Restgruppen in der Cordillera de Talamanca, die aber dennoch einem starken Assimilationsdruck ausgesetzt sind.

Etwa 60 % der Costa-Ricaner leben in Städten. Zwei Drittel der etwa fünf Millionen Einwohner des Landes leben im klimatisch begünstigten Hochland, Siedlungsschwerpunkt ist das "Valle Central", in dem die bedeutenden Städte San José, Heredia, Cartago und Alajuela liegen. Wichtige Küstenstädte sind Puerto Limón (Karibikküste) und Puntarenas (Pazifikküste). Liberia ist das wirtschaftliche Zentrum der Nordwestregion Guanacaste und verfügt neben San José als einzige weitere Stadt Costa Ricas über einen internationalen Flughafen.

Mindestens 25 % der Einwohner sind unter 25 Jahre alt. Die Arbeitslosigkeit ist seit 2007 stark gestiegen. Die Arbeitslosenquote beträgt offiziell ca. 8 %, liegt bei Jugendlichen jedoch weitaus höher. Viele Kinder und Jugendliche der Armenviertel in San José sind in kriminellen Straßenbanden organisiert (sog. "chapulines", „Kapuzenträger“). Raub und Drogenhandel spielen eine große Rolle. Durch die Deportation jugendlicher Straftäter aus den USA hat sich die Situation verschlechtert.

In einer Umfrage des Meinungsforschungsinstitutes Gallup vom Dezember 2012 zählen die Einwohner des Landes dennoch zu den glücklichsten Menschen auf der Erde.

Quelle: UN

Die Bevölkerung ist nach einer 2013 geführten Umfrage überwiegend christlich (62 % römisch-katholisch, 21 % Protestanten, inkl. evangelikaler Gruppierungen). Es bestehen auch Gruppen wie die Zeugen Jehovas mit etwa 1,3 % und Mormonen. Rund 9 % bekannten sich bei dieser Umfrage mit der Überschrift „Religion in Zeiten von Papst Franziskus“ als religionsfrei. Die römisch-katholische Konfession ist Staatsreligion des Landes. Die Indigenen – insbesondere in abgelegenen Regionen – pflegen noch heute ihre traditionellen mesoamerikanischen Religionen, häufig synkretistisch vermischt mit christlichen Elementen.

Die offizielle Landessprache ist Spanisch mit einigen costa-ricanischen Eigenarten. An der Atlantikküste sind auch Englisch und eine englisch-basierte Kreolsprache weit verbreitet, die dem Jamaika-Patois ähnlich ist.

Das in Costa Rica gesprochene Spanisch weist einige Unterschiede zum Standard-Spanisch auf.


Schon während des Zweiten Weltkrieges war in Costa Rica ein kostenloses Gesundheitssystem eingerichtet worden, zusammen mit einem Rentensystem. Das System geriet nach 75 Jahren seines Bestehens in den 2010er-Jahren in finanzielle Schieflage, auch wegen der gestiegenen Lebenserwartung. Private Krankenhäuser machen der Gesundheitsversorgung in lukrativen Bereichen Konkurrenz, sind allerdings aufgrund der hohen Kosten für viele nicht bezahlbar. In öffentlichen Krankenhäusern war darum meist mit langen Wartezeiten zu rechnen.

In Costa Rica gab es im Jahr 2005 etwa 6800 Ärzte, das waren etwa 1,69 pro 1000 Einwohner.
Für Gesundheitsaufgaben gab das Land etwa 689 Mio. $ jährlich aus. Die tägliche Nahrungsaufnahme pro Kopf betrug etwa 2610 kcal. Die durchschnittliche Lebenserwartung lag 2015 bei 79,2 Jahren und war damit im Durchschnitt höher als in den USA. Die Bewohner der Halbinsel Nicoya zählen sogar zu den Langlebigsten der Welt.

Quelle: UN

Die erste Universität wurde 1843 gegründet, wegen ihrer engen Bindung zur katholischen Kirche aber 1888 wieder geschlossen. Die heute größte und renommierteste Hochschule ist die Universität von Costa Rica. Seit Abschaffung der Armee im Jahr 1948 nutzt Costa Rica einen Großteil des dafür angelegten Budgets für die Weiterentwicklung des Ausbildungssektors. Mit den fünf öffentlichen und mehr als 50 privaten Universitäten soll sichergestellt werden, dass sich die Zahl der Studienabgänger im Bereich Technologie und Wissenschaft erhöht. Viele Absolventen sprechen ein ausgezeichnetes Englisch. 

Das Land hat ein hohes Bildungsniveau, die Analphabetenquote ist mit 4,2 Prozent nach Kuba (mit 3 %) die zweitniedrigste Mittelamerikas und eine der niedrigsten in Lateinamerika und sowohl in den Industrie- und Entwicklungsländern. Es gilt eine sechsjährige Schulpflicht. Bereits 1940 wurde die erste Behindertenschule von Fernando Centeno Güell eröffnet.

Die ältesten archäologischen Funde, die eine menschliche Besiedlung Costa Ricas dokumentieren, werden der Zeit zwischen 12.000 und 8.000 vor Christus zugerechnet. Bis zur Ankunft der Spanier im 16. Jahrhundert n. Chr. lebten vermutlich 400.000 Menschen im heutigen Staatsgebiet.

1502 landete Christoph Kolumbus als erster Europäer an der Atlantikküste des heutigen Costa Rica. Die ersten Erkundungen der Küstenregion und des Inlands fanden erst 1510 unter Diego de Nicuesa statt. Zwischen 1519 und 1523 wurde dann der Großteil des heutigen Costa Ricas im Namen der spanischen Krone erobert. Die von Kolumbus vorgenommene Bezeichnung "Costa Rica y Castillo de Oro" (dt.: Reiche Küste und Goldene Burg) sollte sich als Wunschvorstellung herausstellen: Das Land ist arm an mineralischen Rohstoffen und Edelmetallen. Erst ab 1560 wurde Costa Rica systematisch kolonisiert; spanische Konquistadoren gründeten 1563 Cartago, das bis 1823 Hauptstadt Costa Ricas war. Im 17. Jahrhundert festigten die Spanier ihre Herrschaft in Costa Rica, das jedoch wegen seiner Rohstoffarmut und unbedeutenden strategischen Lage eine nachrangige und unterentwickelte Kolonie blieb.

Am 15. September 1821 erhielt Costa Rica die Unabhängigkeit von der ehemaligen Kolonialmacht Spanien. Die Geschichte Costa Ricas ist für mittelamerikanische Verhältnisse eine Erfolgsgeschichte. Seit dem späten 19. Jahrhundert gab es nur zwei kurze Perioden der Gewalt, die die Demokratisierung des Landes beeinträchtigten.
In der Nationalkampagne von 1856/57, die gegen den US-amerikanischen Filibuster William Walker gerichtet war, kamen durch das Einschleppen der Cholera aus dem Kriegsgebiet Nicaragua nach der Zweiten Schlacht von Rivas angeblich bis zu 10 % der Bevölkerung ums Leben. Zum Andenken an die Vertreibung Walkers wurde am 15. September 1895 das Nationalmonument von Costa Rica eingeweiht, das sich im Stadtzentrum von San José befindet.

Von 1917 bis 1919 übte Federico Alberto Tinoco Granados eine Militärdiktatur aus, die jedoch von den Vereinigten Staaten nicht als legitime Regierung anerkannt wurde, aber die Unterstützung der in Costa Rica einflussreichen United Fruit Company besaß. Letztlich sah sich Tinoco gezwungen, ins Exil nach Frankreich zu gehen.
Im Februar/März 1921 eskalierte ein seit langem andauernender Grenzkonflikt mit Panama im so genannten "Guerra de Coto", in dem costa-ricanische Truppen die Grenze überschritten. Daraufhin wurden panamische Polizisten - die Republik Panama besaß zu diesem Zeitpunkt keine Streitkräfte - gegen die Invasoren eingesetzt. Nach einigen Scharmützeln wurde der Konflikt durch das Erscheinen des amerikanischen Schlachtschiffs USS Pennsylvania im Rahmen der Kanonenbootpolitik beendet und der Status quo ante wiederhergestellt.

Von 1936 bis 1940 amtierte in Costa Rica der autoritäre Präsident León Cortés, dem Sympathien für den Nationalsozialismus nachgesagt wurden. Umstritten ist dabei insbesondere die Rolle des deutschen Einwanderers Max Effinger, der unter Cortés das Amt des Direktors für öffentliche Arbeiten ausübte. Unter seinem Nachfolger, Rafael Calderón Guardia, vollzog das Land einen außenpolitischen Kurswechsel und orientierte sich nun stärker an den USA. Im Dezember 1941, nach dem Angriff auf Pearl Harbor, trat Costa Rica als erstes zentralamerikanisches Land in den Zweiten Weltkrieg ein. Deutsche, italienische und japanische Staatsbürger wurden nun überwacht und ihr Vermögen unter staatliche Kontrolle gestellt.

Von direkten Kriegshandlungen blieb Costa Rica praktisch verschont. Die einzige Ausnahme bildete die Versenkung des panamaischen Frachters "San Pablo" am 3. Juli 1942 vor dem Hafen von Puerto Limón durch das deutsche U-Boot "U 161". Die Versenkung löste in Costa Rica Empörung aus und führte zu Krawallen, die vorzugsweise gegen die Geschäfte von italienischen und deutschen Residenten bzw. Einwanderern gerichtet waren.

Costa Rica ist eine Präsidialrepublik. Der Präsident wird alle vier Jahre vom Volk direkt gewählt und kann nach einer Legislaturperiode zunächst nicht wiedergewählt werden. Bis zu den Wahlen im Februar 2010 hatte Óscar Arias Sánchez dieses Amt inne, der bereits zwischen 1986 und 1990 Präsident Costa Ricas war. Bei den Wahlen am 7. Februar 2010 wurde zum ersten Mal in der Geschichte Costa Ricas eine Frau als Staatsoberhaupt gewählt. Laura Chinchilla Miranda von der sozialdemokratisch-liberalen Regierungspartei Partido Liberación Nacional (PLN) wurde mit 46,78 Prozent der Stimmen eindeutig an die Spitze des Landes gewählt. Nachdem sein Gegenkandidat vor der Stichwahl der Präsidentschaftswahl 2014 zurückgezogen hatte, wurde Luis Guillermo Solís am 6. April 2014 mit 77,8 % der abgegebenen Stimmen zum neuen Präsidenten gewählt. Bei der Stichwahl der Präsidentschaftswahl am 1. April 2018 gewann Carlos Alvarado; sein Amtsantritt ist für den 8. Mai 2018 terminiert.

Das Einkammer-Parlament besteht aus 57 Abgeordneten und wird alle vier Jahre nach Verhältniswahl gewählt.

Das Auswärtige Amt schreibt: „Costa Rica ist – was politische Stabilität und sozialen Frieden angeht – im zentralamerikanischen Kontext ein Musterland, das verfassungsmäßig und institutionell teilweise dem nordamerikanischen Muster (Präsidialsystem) folgt. Die Gewaltenteilung ist streng […].“
Präsident José Figueres Ferrer (1948–1949, 1953–1958, 1970–1974) ließ nach dem Beitritt zum TIAR-Pakt die Armee am 8. Mai 1949 per Verfassung abschaffen und investierte die hierdurch freigesetzten Gelder in den Aufbau des Bildungs- und Gesundheitswesens. Grenzschutzaufgaben hat die Polizei übernommen. Allerdings existierte von 1949 bis 1996 eine paramilitärische Polizeitruppe, die "Guardia Civil de Costa Rica" mit einer Stärke von zeitweise bis zu 8400 Mann. 1983 verkündete Präsident Luis Alberto Monge angesichts der Verschärfung der Bürgerkriege in Zentralamerika (v. a. in Nicaragua) die dauernde, aktive und unbewaffnete Neutralität des Landes. Unabhängig davon bildete die "Guardia Civil" 1985/86 aufgrund des Contra-Kriegs eine Spezialeinheit, das "Batallon Relámpago" („Blitz-Bataillon“) zur Sicherung der Grenze nach Nicaragua.

Präsident Óscar Arias Sánchez erhielt 1987 für sein Engagement für die friedliche Beilegung von Konflikten in Mittelamerika den Friedensnobelpreis.
Costa Rica wurde auch die „Schweiz Mittelamerikas“ genannt, wegen der bergigen Landschaft und auch wegen des relativen Wohlstands und der oben genannten Neutralität, die sich im Gegensatz zur Schweiz bislang auch durch die Abwesenheit einer Armee ausdrückte. Am 1. Juli 2010 genehmigte das Parlament Costa Ricas auf Antrag der Präsidentin des Landes, Laura Chinchilla, die Stationierung von bis zu 46 Kriegsschiffen und 7000 Soldaten der USA in Costa Rica. Diese sollen den von Kolumbien ausgehenden Drogenschmuggel bekämpfen. Die Opposition sah durch „das kriegerische Potenzial“ die nationale Souveränität des Landes verletzt und reichte Klage beim Verfassungsgericht ein. Die Stationierung wurde zunächst auf einen Zeitraum von sechs Monaten bis zum 31. Dezember 2010 befristet. Seither wird sie um jeweils sechs Monate verlängert. Die Journalistin Eva Golinger wies darauf hin, dass in einem offiziellen Dokument, das die amerikanische Botschaft an das Ministerium für Sicherheit in Costa Rica geschickt hat, die uneingeschränkte Straflosigkeit von US-Soldaten zu einer Voraussetzung des Einsatzes gemacht wird, wie das bei allen Auslandseinsätzen des US-Militärs üblich ist: 

Seit 1996 ist statt der "Guardia Civil" die "Fuerza Pública de la República de Costa Rica" für die innere Sicherheit zuständig. Seit 2007 gibt es in San José eine spezielle Touristenpolizei (Notruf 911).

Das Auswärtige Amt warnt vor Kriminalität, insbesondere Gewaltkriminalität in San José (dort insbesondere in der Umgebung der Busbahnhöfe) sowie zwischen Jacó und Quepos, an den Stränden in der Provinz Guanacaste am Pazifik und u. a. in Cahuita und Puerto Viejo südlich von Limón. Auch vor Entführungen wird gewarnt.

Am 7. Oktober 2007 fand in Costa Rica eine Volksabstimmung (Referendum) über ein Freihandelsabkommen mit den Vereinigten Staaten von Amerika, Zentralamerika und der Dominikanischen Republik statt (Tratado de Libre Comercio con los Estados Unidos, Centroamérica y República Dominicana, kurz TLC), die mit einer knappen Mehrheit von 51,6 % der abgegebenen Stimmen akzeptiert wurde. Vor dem Referendum über das TLC fanden zahlreiche Diskussionen unter breiter Anteilnahme der Bevölkerung im Fernsehen und im Radio, aber auch in Kirchen, Schulen, Universitäten und auf öffentlichen Plätzen statt. Die Kampagne für das TLC wurde vor allem vom damaligen Präsidenten und Friedensnobelpreisträger Óscar Arias Sánchez und seiner Partei PLN getragen, aber auch von Intellektuellen, Politikern und Wirtschaftsvertretern unterstützt (so von dem Ex-Wirtschaftsminister Alfredo Volio). Die Kampagnen gegen das TLC wurden von der Oppositionspartei Partido Acción Ciudadana (PAC) und deren Vertretern Ottón Solís und José Miguel Corrales unterstützt. Es bildeten sich auch verschiedene private Initiativen gegen das TLC, die unter dem Slogan „¡Mi corazón dice no!“ (Mein Herz sagt nein!) zusammengefasst waren. Nach dem Referendum kam es zu keinen nennenswerten Krawallen oder Zerstörungen, obwohl die Debatten zuvor teilweise recht emotional waren.
Costa Rica ist Mitglied der CELAC. Die OECD kündigte im Mai 2013 an, dass 2015 Beitrittsgespräche mit Costa Rica beginnen sollen. 

Indigene Gruppen wehrten sich seit den 1990er Jahren gegen den geplanten Staudamm im bewaldeten Térraba-Tal im Süden Costa Ricas. 2006 wurde das Projekt ohne ihre Zustimmung beschlossen. Das El-Diquis-Wasserkraftwerk soll 680 Megawatt Strom produzieren, wovon 90 Prozent für den Export bestimmt sind. Für den Stausee wurden 2200 Hektar Waldfläche in technische Nutzfläche umgewandelt, wobei das geschlagene Tropenholz zusätzlich zur weiteren wirtschaftlichen Prosperität des Landes beitrug. Der restliche Wald im Térraba-Tal ist das geschützte Reservat der Teribe, einer indigenen Gruppe von noch 750 Personen (Stand 2011). Die Teribe siedeln seit Jahrhunderten in dem Gebiet, das einst 9000 Hektar umfasste. Dem Stamm sind noch 10 Prozent geblieben. Auf dem Rest siedelten nach costa-ricanischem Recht illegal nicht-indigene Siedler. Der costa-ricanische Staat ließ sie gewähren.

Prostitution ist nach costa-ricanischem Gesetz geduldet. In größeren Städten wie San José sind Prostituierte in einigen Zonen auf der Straße anzutreffen und Sextourismus insbesondere durch nordamerikanische Touristen ist üblich. Prostitution ist in einigen Touristenhotels geduldet. Kinderprostitution wird seit Jahren von der Regierung bekämpft. Die Kinderschutzorganisation ECPAT versucht mit einem "Code of Conduct" für Angestellte von Tourismusunternehmen, Hotels, Autoverleihen, Taxi-Firmen, Schiffs-Betreibern und anderen im Tourismusgeschäft Tätigen ebenso die Aufmerksamkeit für das Problem zu erhöhen.

In der Rangliste der Pressefreiheit 2017, herausgegeben von Reporter ohne Grenzen, belegte Costa Rica Platz 6 von 180 Ländern. Die Presse des Landes zählt damit zu den freiesten der Welt und ist die freieste des amerikanischen Kontinents.

Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Costa Rica 2016 von 176 Ländern zusammen mit Brunei und Spanien auf dem 41. Platz, mit 58 von maximal 100 Punkten.

Die Provinzen sind in Kantone ("cantones") untergliedert, von denen jeder wiederum in eine unterschiedliche Anzahl Distrikte ("distritos") aufgeteilt wird. Genauso wie die Provinzen sind alle Kantone und Distrikte innerhalb ihrer nächsthöheren Verwaltungseinheit durchnummeriert. Insgesamt gibt es 81 Kantone und 470 Distrikte.

Costa Rica ist außerdem in sechs Regionen ("regiones") unterteilt: Central, Chorotega, Pacífico Central, Brunca (auch Pacífico Sur), Huetar Atlántica und Huetar Norte. Diese Regionen sind im Gegensatz zu den Provinzen keine Verwaltungseinheiten, sondern die Unterteilung wurde zum Zweck sozioökonomischer Untersuchungen vorgenommen. Jede Region umfasst mehrere Kantone verschiedener benachbarter Provinzen. Die Namen Chorotega, Brunca und Huetar sind abgeleitet von den gleichnamigen indigenen Völkern.

2016 lebten 77,7 % der Bevölkerung in Städten oder städtischen Räumen. Die 5 größten Städte sind (Stand 2017):

Die Handelsbilanz Costa Ricas ist traditionell negativ.

Obwohl Costa Rica immer noch stark landwirtschaftlich geprägt ist, konnten auch andere Wirtschaftssektoren ausgebaut werden. Wichtigster Devisenbringer des Landes ist inzwischen der Tourismus, der 2009 mit 1,9 Millionen Besuchern 6,6 % des BIP ausmachte. Eine wichtige Rolle nimmt hierbei der Ökotourismus ein. Daneben wurde auch der High-Tech-Sektor ausgebaut; zweitwichtigster Devisenbringer des Landes ist eine Chipfabrik der Firma Intel. Der Anteil der Landwirtschaft am Bruttoinlandsprodukt betrug im Jahre 2009 8,8 % (1965 noch 23,5 %) während die Industrie 26 % und die Dienstleistungen 61,1 % zum Bruttoinlandsprodukt beitrugen. Die meisten Industriebetriebe konzentrieren sich auf der zentralen Hochebene um die Hauptstadt San José. Der Bananenanbau in ausgedehnten Plantagen befindet sich traditionell an der Karibikküste um den Hafen Puerto Limón und neuerdings auch an der Pazifikküste um die Hafenstadt Golfito.

Am 7. Oktober 2007 hat Costa Rica in einem Referendum dem Freihandelsvertrag Cafta mit den USA zugestimmt. 51,6 % der Wähler stimmten mit Ja. Die Wahlbeteiligung lag bei rund 60 % von insgesamt 2,6 Millionen Wahlberechtigten. Der costa-ricanische Präsident Óscar Arias Sánchez ging als Sieger aus der Abstimmung hervor. Seine Hoffnungen waren, dass mit dem Freihandelsvertrag die staatlichen Monopole fallen und Investitionen angezogen werden könnten, wodurch Arbeitsstellen geschaffen würden. Dem Referendum waren heftige Kontroversen vorausgegangen. Die Gegner des Freihandels argumentierten, das Land sei nicht gewappnet und noch nicht reif für die Beseitigung der Handelsschranken, da dies zur Folge habe, dass die USA Costa Rica mit Waren überschwemmten, was zu Arbeitsplatzverlusten führte. Die Zustimmung zum Referendum sei auch als Verrat am eigenen Land zu werten, gehe doch die nationale Souveränität dadurch verloren.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Costa Rica Platz 47 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegte Costa Rica 2017 Platz 63 von 180 Ländern. 

Costa Rica importierte im Jahr 2014 für 16,4 Mrd. US$ Waren und exportierte gleichzeitig Waren für 11,1 Mrd. US$, hatte also eine negative Handelsbilanz. Wichtigste Importgüter im Jahr 2011 waren Rohstoffe (55 %, vor allem Erdölprodukte), Konsumgüter (19 %) und Investitionsgüter (17 %, vor allem Elektronik). Hauptlieferländer 2014 waren die USA mit 47,4 %, China mit 9,8 %, Mexiko mit 6,6 % und Japan mit 2,7 %. Deutschland folgt mit 2,0 % auf Rang 7. Die wichtigsten Abnehmerländer für Exporte waren die USA mit 37,3 %, die Niederlande mit 6,1 % und Panama mit 5,3 %. Zu den Exportprodukten gehören vor allem integrierte Schaltungen, elektronische Geräte, Bananen und tropische Früchte.

Das BIP betrug 2016 nominal ca. 58,1 Mrd. US$. Das durchschnittliche BIP/Kopf betrug im Jahre 2016 11.835 US$. Costa Rica ist damit, nach Panama, das zweitwohlhabendeste Land Mittelamerikas.

Die Stromerzeugung Costa Ricas wird fast ausschließlich durch erneuerbare Energien gedeckt, während fossile Energien unbedeutend sind. 2016 stammten 98,1 % des Stroms aus erneuerbaren Quellen, 2015 98,9 % und 2014 94 %. Im Jahr 2013 lieferten Wasserkraftwerke ca. 68 % des Stroms, Geothermiekraftwerke knapp 15 % und Windkraftanlagen rund 5 %, zudem wurden in kleinem Umfang auch weitere erneuerbare Energien genutzt. Fossile Energieträger deckten knapp 12 % des Strombedarfs. Durch diese Struktur können erneuerbare Energien Costa Rica immer wieder monatelang alleine mit Strom versorgen. Ziel von Costa Rica ist es, bis 2021 vollständig auf fossile Energien verzichten zu können. Im September 2016 wurde hierzu u. a. mit der Talsperre Reventazón ein neues Wasserkraftwerk mit einer Leistung von 305 MW in Betrieb genommen, das bei Inbetriebnahme das größte Wasserkraftwerk Mittelamerikas war.

Costa Rica ist der zweitgrößte Bananenexporteur der Welt. In der Produktion von Bananen liegt Costa Rica mit 2,7 Millionen Tonnen jedoch weltweit nur auf dem 7. Platz hinter Indien mit 15,1 Millionen Tonnen, Ecuador mit 7,56 Millionen Tonnen, Brasilien mit 5,5 Millionen Tonnen, China mit 5,2 Millionen Tonnen, Philippinen mit 4,5 Millionen Tonnen und Indonesien mit 3,6 Millionen Tonnen (die Angaben beziehen sich auf die Produktion im Jahr 2001).

Neben Bananen wurde in den letzten Jahren zunehmend die Ananas zu einem wichtigen Exportgut. Auf Kosten der Umwelt und der kleinbäuerlichen Landwirtschaft erschlossen die wichtigsten internationalen Fruchthersteller, darunter Chiquita, Dole und Del Monte, in den vergangenen Jahrzehnten immer neue Flächen und machten Costa Rica zum weltweit führenden Ananasexporteur. Die über 27.000 im Ananasanbau arbeitenden Menschen haben zumeist keinen gewerkschaftlichen Schutz und arbeiten unter katastrophalen Gesundheitsbedingungen.

Ein weiteres Exportprodukt ist Kaffee, der hauptsächlich im Valle Central angebaut wird. Bis Ende der 1980er Jahre war Kaffee das wichtigste Exportprodukt. Weitere Agrarprodukte, die exportiert werden, sind Papaya, Melonen, Macadamianüsse und Zierpflanzen. Die wirtschaftliche Bedeutung von Rinderzucht und Zuckerrohr nimmt weiterhin ab.

Digitale Produkte und Services machen bereits 24 Prozent des gesamten Landesexportes aus. Mehr als die Hälfte der costa-ricanischen Internet-Firmen besitzen Standorte in mehreren Ländern Lateinamerikas, USA, Kanada und teilweise bereits auch in Europa, Asien und Afrika. In den Bereichen Mobile und Web Development gibt es eine Vielzahl von kleinen und mittelgroßen Internet-Firmen, die sich im Raum San José angesiedelt haben und Apps für Smartphones und Tablets als Outsourcing-Anbieter für verschiedene Branchen entwickeln. 
Digital Animation Services in 2D/3D oder E-Learning Anbieter werden aufgrund der exzellenten Qualität und überschaubarer Preise von Firmen wie Coca Cola, Walt Disney, Autokonzernen und anderen seit langer Zeit als etablierter Serviceprovider genutzt.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 11,310 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 8,115 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 5,5 % des BIP.Die Staatsverschuldung betrug 2016 26,7 Mrd. US-Dollar oder 46 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Die erste Eisenbahnlinie führte in Kapspur (1067 mm) von San José zum Atlantik, von Alajuela nach San José und später weiter bis Cartago. Aus der anderen Richtung führte die Eisenbahn von Limón bis nach Matina, doch dann ging das Geld aus. Erst Jahrzehnte später konnte das fehlende Zwischenstück gebaut werden, und die Strecke bis Limón konnte erst im Dezember 1890 eröffnet werden. Die erste formelle Eisenbahngesellschaft war die Costa Rica Railway Co. Ltd. Weitere Eisenbahnstrecken wurden von der United Fruit Company, heute Chiquita Brands International, im Bananen-Anbaugebiet am Atlantik zum Abtransport der Früchte gebaut. Später folgte die Northern Railway of Costa Rica, welche die Strecken der Costa Rica Railway übernahm. Die Strecke vom Pazifik nach San José, die Ferrocarril al Pacífico, wurde, nach Überwindung zahlreicher baulicher Schwierigkeiten, im Juli 1910 ebenfalls mit einer Spurweite von 1067 mm (Kapspur) eröffnet. Jede dieser Linien hat in San José eine eigene Endstation, Estación al Atlántico und Estación al Pacífico. Die weiter unten erwähnte Stadtbahn verkehrt unter anderem zwischen diesen beiden Stationen als eine Art Straßenbahn. Im Jahre 1930 beschloss man die Elektrifizierung der Pazifikstrecke, die Elektrifizierung erfolgte durch die deutsche Firma AEG. Die Strecke ist sehr kurvenreich und hat eine Länge von 130 Kilometer. Als erste Bahn in einem Tropenland wählte man Einphasen-Wechselstrom von 15 kV, 20 Hertz und benötigte deshalb kein Unterwerk. Im Jahr 1977 wurden alle noch existierenden Eisenbahnstrecken unter der Staatsbahn FECOSA (Ferrocarriles de Costa Rica) zusammengefasst. 1980 wurden dann auch die insgesamt 106 Kilometer langen, am Atlantik gelegenen Strecken für den Bananentransport von Rio Frio nach Siquirres und Puerto Limón elektrifiziert. Die damals als FECOSA benannte Staatsbahn, welche diese Strecken übernommen hatte, betrieb die Strecken bis dahin mit Dieselloks. Da die Belastung mit über einer Million Tonnen pro Jahr außergewöhnlich hoch war, beschloss man die Strecke grundlegend zu erneuern und zu elektrifizieren. 1985 wurde die FECOSA durch die neue Staatsbahn INCOFER (Instituto Costarricense de Ferrocarriles) ersetzt. Alle nichtelektrischen Eisenbahnlinien werden heute mit Dieseltraktion betrieben, anfangs wurde auf allen Strecken mit Dampfloks gefahren. Durch starke Erdrutsche wurde die Strecke zwischen Cartago und Turrialba im Jahre 1988 zerstört und das Eisenbahnnetz somit in zwei Teile geteilt. Ein starkes Erdbeben beschädigte dann im Jahre 1991 die Strecken entlang der atlantischen Küste, die jedoch innerhalb von zwei Wochen wieder instand gesetzt wurden. Allerdings war die atlantische Bananentransportlinie schon früher wegen der Erschöpfung der Bananenfelder und der hohen Stromtarife nicht mehr konkurrenzfähig. Die Bananen werden am Abend transportiert, wenn die Tarife am höchsten sind, und die Elektrizitätswerke waren nicht zu Konzessionen zu bewegen. Weitere Gründe waren die hohen Personalkosten (Überbemannung), und so wurden die Transporte auf LKWs umgestellt. Als Folge davon wurde der Betrieb im Jahr 1995 eingestellt.

In den letzten Jahren wurden diverse Routen im Zentraltal reaktiviert. Auf den meisten verkehren ausrangierte Apolo Züge aus Spanien. Während im Jahr 2016 auf den bedienten Routen etwa 1,7 Millionen Passagiere befördert wurden, ging die Zahl im Jahr 2017 zurück, obwohl die zusätzliche Strecke Heredia - Alajuela aktiviert wurde. So wurden im Jahr 2017 nur noch 1,4 Million Passagiere transportiert. Zurückgeführt wird dies auf den Ausfall diverser Triebwagen durch Unfälle.
Ein Wochenendpersonenzug soll noch existieren, dieser wird von "America Travel" organisiert und verkehrt von der Hauptstadt San José bis hinunter zum Pazifik nach Caldera. Von dort kann man mit dem Bus weiter nach Puntarenas fahren. Dieses eine Zugpaar verkehrt nur an Sonntagen und befördert die Ausflügler aus dem Ballungsraum San José an den Pazifik, was immer noch ein beliebter Familienausflug ist, für welchen die Eisenbahn unersetzlich ist. Diese Fahrt dauert insgesamt rund vier Stunden, morgens hin zum Pazifik, abends zurück nach San José. Diese Fahrten müssen vorab gebucht werden. Außerdem wurden im mittleren Teil des Landes zusätzlich einige wichtige Strecken für den Güterverkehr reaktiviert.

Gesicherte Bahnübergänge gibt es selbst in San José nicht, anfangs gab es viele Kollisionen mit Autofahrern. Diese haben mittlerweile aber stark abgenommen.

Costa Rica besitzt eine vielseitige Musikkultur von traditioneller einheimischer Musik über karibische Musik und allgemein lateinamerikanische Musik bis hin zu weltweiten Strömungen und Stilen. Neben den internationalen Charts haben vor allem lateinamerikanische Tanzmusik – von Salsa bis Reggaeton – aber auch andere Stile und Gruppen aus Lateinamerika großen Einfluss. Gleichzeitig gibt es eine reiche Kultur an aktiven Musikern und zahlreiche private und öffentliche Musiker-Gruppen (zum Beispiel Universitätsorchester). Zu den typischen Elementen traditioneller Musik zählen unter anderen Gesang, Marimba und Gitarre.

Beispiele für traditionelle Musik

Zu den Hauptbestandteilen der sehr vielfältigen costa-ricanischen Küche zählen Reis, Bohnen und häufig auch Kochbananen. Diese Grundzutaten werden zu den verschiedensten Gerichten verarbeitet. An den Karibik-Küstenregionen locken schmackhafte exotische Speisen, die unter anderem durch die Zubereitung mit Kokosmilch ihre Raffinesse erhalten. Vor allem in San José haben sich in den letzten Jahren aber auch viele Fastfood-Ketten wie McDonald’s, Pizza Hut, Burger King und Taco Bell etabliert.

Das typische Frühstück und zugleich das Nationalgericht Costa Ricas ist "Gallo Pinto": ein Gericht aus gebratenem Reis mit schwarzen Bohnen und Zwiebeln. Dazu werden häufig Spiegel- oder Rühreier sowie Tortillas und Sauerrahm serviert. Zu Mittag werden in der Regel sogenannte "Casados" gegessen, bei denen ebenfalls Reis und Bohnen die Basis bilden. Dazu werden Kochbananen und Fleisch, Hähnchen oder Fisch sowie ein wenig Salat und gedünstetes Gemüse gereicht. In den zahlreichen kleinen "Sodas" erhält man die Mittagsgerichte zum günstigen Preis.

Auch " Tamales" gelten als typisch für Costa Rica. Diese gefüllten, in Bananenblätter gewickelten und anschließend gekochten Maistaschen bereiten Costaricaner traditionell zu Weihnachten, aber mittlerweile auch während des Jahres gerne zu. Sie werden häufig zum Frühstück gegessen, oftmals auch mit "salsa italiana".
Zum Nachtisch werden häufig frische Früchte oder auch Süßspeisen wie Kuchen, Torten oder Schokolade serviert.

Die beliebtesten Getränke stellen neben Wasser, Limonaden und Kaffee Erfrischungsgetränke wie "Batidos" (mit Wasser, Milch, Joghurt oder Eis zubereitete Getränke aus frischen Früchten) und "pipa" (direkt aus der Kokosnuss getrunkenes Kokoswasser) dar. Allgemein kann gesagt werden, dass bei Getränken sehr viel Zucker verwendet wird, auch wenn die Früchte in den "Batidos" viel Fruchtzucker besitzen.
Bei alkoholischen Getränken dominiert der Bier- und Schnapskonsum, Wein wird nur selten getrunken. Die am häufigsten getrunkenen Biersorten sind die costa-ricanischen Biere "Imperial", "Pilsen" und "Bavaria".

Der Fußball hat in Costa Rica einen hohen Stellenwert. Die costa-ricanische Fußballnationalmannschaft konnte sich bisher für die vier Fußball-Weltmeisterschaften 1990, 2002, 2006 und 2014 qualifizieren. 2006 bestritt sie mit Deutschland das Eröffnungsspiel in München, das Costa Rica mit 2:4 verlor. Das zweite Spiel verlor man mit 0:3 gegen Ecuador, was gleichzeitig das WM-Aus bedeutete. Das letzte Spiel wurde mit 1:2 gegen Polen verloren. 1990 erreichte die Nationalmannschaft in einer Vorrundengruppe als Zweiter hinter Brasilien das Achtelfinale. Bei der Fußball-Weltmeisterschaft 2002, ebenfalls in einer Gruppe mit Brasilien, schied sie in der Vorrunde als Dritter nur aufgrund der schlechteren Tordifferenz gegenüber der Türkei aus. 2014 gelang es der Auswahl, sich durch Siege über die favorisierten Mannschaften aus Uruguay und Italien wie bereits 1990 erneut für das Achtelfinale und durch einen Sieg nach Elfmeterschießen gegen Griechenland zum ersten Mal auch für das Viertelfinale zu qualifizieren. Dort schied man nach Elfmeterschießen gegen die Niederlande aus.

Die nationale Fußball-Liga Primera División de Costa Rica wird von zwei Klubs dominiert, zum einen vom Deportivo Saprissa aus der Hauptstadt San José, zum anderen von dem Verein LD Alajuelense aus Alajuela.





</doc>
<doc id="10175" url="https://de.wikipedia.org/wiki?curid=10175" title="Britische Jungferninseln">
Britische Jungferninseln

Die Britischen Jungferninseln (, offiziell nur "Virgin Islands") sind ein britisches Überseegebiet in der Karibik. Sie umfassen über 60 Inseln und Riffs im nordöstlichen Teil der Inselgruppe der Jungferninseln am nördlichen Ende des Inselbogens der Kleinen Antillen. Die insbesondere durch Zuwanderung schnell wachsende Bevölkerung (Wanderungssaldoziffer 2012: 18,56 pro 1000 Einwohner, weltweit dritthöchster Wert) bewohnt 16 der etwa 100 km östlich von Puerto Rico gelegenen Inseln.

Die Wirtschaft ist eng verknüpft mit den bevölkerungsstärkeren Amerikanischen Jungferninseln, die offizielle Währung ist daher der US-Dollar. Dominierend ist der Dienstleistungssektor, darin insbesondere die Tourismus- und die Offshore-Finanzbranche.

Im 1. Jahrhundert v. Chr. besiedelten Arawak die Inseln, wurden aber im 15. Jahrhundert von Kariben unterworfen. 1493 entdeckte Christoph Kolumbus die Jungferninseln für die Europäer. 1555 besiegten spanische Truppen die indigene Bevölkerung und rotteten sie in den kommenden Jahrzehnten aus.

Bis zum Beginn des 17. Jahrhunderts wurden die Inseln zwar mehrfach von spanischen und englischen Schiffen auf deren Expeditionen nach Süd- und Mittelamerika besucht, eine Kolonisierung unterblieb aber zunächst. Erst 1615 erscheint in spanischen Aufzeichnungen eine dauerhafte Siedlung des holländischen Freibeuters Joost van Dyk auf Tortola. Bald darauf erkannte die Niederländische Westindien-Kompanie die strategische Bedeutung der Jungferninseln. Durch holländische Siedler wurden in der Folgezeit mehrere Schanzen und kleine Forts errichtet; es kam zu zahlreichen kriegerischen Auseinandersetzungen mit spanischen Truppen.

Nach dem Ausbruch des Dritten Englisch-Niederländischen Krieges wurde das Gebiet 1672 von England annektiert und in die Kolonie der "British Leeward Islands" eingegliedert. Die Briten legten Plantagen zum Anbau von Zuckerrohr an, wofür sie eine wachsende Anzahl aus Afrika deportierter Sklaven benötigten. In der Mitte des 18. Jahrhunderts gab es hier bereits über 6000 Sklaven. Heute sind daher rund 75 % der Bevölkerung schwarzafrikanischer Abstammung. Zu Beginn des 19. Jahrhunderts kam es zu mehreren Sklavenaufständen; 1834 wurde die Sklaverei offiziell abgeschafft.

Die 1871 gegründete "Leewards Islands Federation" wurde 1956 aufgelöst. Die Britischen Jungferninseln bildeten ab 1960 eine eigene Kronkolonie und erlangten mit der Verfassung von 1967 eine stärkere innere Autonomie. Seit 1946 steht das Territorium auf der UN-Liste der Hoheitsgebiete ohne Selbstregierung.

Die Britischen Jungferninseln haben 28.054 Einwohner (Stand 2010). Die Mehrheit der Bevölkerung ist ganz oder zum Teil afrikanischer Abstammung. 17,6 % sind Methodisten, 10,4 % Mitglieder der Gemeinde Gottes, 9,5 % Anglikaner, 9 % Adventisten des siebenten Tages, 8,9 % Katholiken, 8,2 % Pfingstler, 7,4 % Baptisten, 6,9 % gehören der New Testament Church of God an, 2,5 % sind Zeugen Jehovas, 1,9 % Hindus, 17,7 % bekennen sich zu einer anderen oder keiner Religion (alle Zahlen von 2010).

Das Gebiet hat eine Größe von 150 km² und besteht aus 16 bewohnten und über 20 unbewohnten Inseln. Die vier Hauptinseln sind Anegada, Jost Van Dyke, Tortola und Virgin Gorda. Gesamtliste:

Die Insel Anegada besteht aus Korallenriffen und Kalkstein und ist relativ flach; die anderen Inseln sind vulkanischen Ursprungs und bergig bzw. hügelig. Der höchste Punkt der Inseln ist der Mount Sage mit 523 m auf Tortola.

Das Klima ist tropisch und feucht, die Temperaturen sind durch Winde gemäßigt. Hurrikane und tropische Stürme gibt es von Juli bis Oktober. Die natürlichen Süßwasservorkommen sind begrenzt. Abgesehen von einigen Quellen und periodischen Wasserläufen auf Tortola, die im Sommer austrocknen, beruht die Wasserversorgung der Inseln auf Brunnen und Regenwasserzisternen.

Die Britischen Jungferninseln sind assoziiertes Mitglied der Karibischen Gemeinschaft und der Organisation Ostkaribischer Staaten. Sie gehören zu den Trägern der University of the West Indies.

Die oberste exekutive Autorität der Britischen Jungferninseln hat Königin Elisabeth II. inne. In dieser Funktion wird sie vom Gouverneur vertreten, der von ihr auf Vorschlag der britischen Regierung ernannt wird. Für die Außen- und Verteidigungspolitik ist das Vereinigte Königreich zuständig.

Die Verfassung der Inselgruppe wurde zuletzt 2007 neu erlassen. Regierungschef ist der Premier, bis 2007 Chief Minister, der zusammen mit den vier anderen Regierungsmitgliedern aus den Mitgliedern des Legislativrates vom Gouverneur ernannt wird. Der Legislativrat "(House of Assembly)" besteht aus 13 vom Volk gewählten Abgeordneten, dem Speaker sowie dem nicht stimmberechtigten Attorney General.

Seit 2017 hat Augustus Jaspert das Amt des Gouverneurs inne. Amtierender Premier ist seit November 2011 Orlando Smith, National Democratic Party (NDP), der Ralph Telford O’Neal, Virgin Islands Party (VIP), dessen Vorgänger er bereits von 2003 bis 2007 war, ablöste.

Das Straßennetz umfasst 200 km befestigte Straßen (Stand 2007), es herrscht Linksverkehr. In Road Town gibt es einen Hafen.

Auf dem Gebiet der Britischen Jungferninseln gibt es fünf Flughäfen: Auguste George Airport auf der Insel Anegada (IATA-Flughafencode: NGD), Terrance B. Lettsome International Airport auf der Insel Beef Island (EIS), Virgin Gorda Airport auf der Insel Virgin Gorda (VIJ) sowie die beiden Wasserflugplätze "North Sound Water Aerodrome" nördlich der Insel Virgin Gorda (NSX) und "West End Seaplane Base" nördlich der Insel Tortola (TOV).

Die Wirtschaft, eine der stabilsten und florierendsten in der Karibik, ist überwiegend vom Tourismus abhängig. Der direkte Beitrag des Tourismus zum Bruttoinlandsprodukt lag 2016 bei etwa 34 %; der Gesamtbeitrag zur Wirtschaftskraft des Landes, der auch die indirekten Auswirkungen auf andere Wirtschaftszweige berücksichtigt, übersteigt 95 %. Circa 825.000 Touristen, darunter mehr als 440.000 Kreuzfahrtpassagiere hauptsächlich aus den USA, besuchten 2006 die Inseln. Haupteinnahmequelle der örtlichen Tourismusindustrie ist das Verchartern von Yachten.

Von vergleichsweise untergeordneter Bedeutung ist die Landwirtschaft, deren Hauptaktivität in der Viehzucht liegt. Die schlechte Bodenqualität ermöglicht nur eine begrenzte Versorgung der heimischen Bevölkerung.

Die Wirtschaft ist eng an die der Amerikanischen Jungferninseln gebunden. Seit 1959 ist der US-Dollar die offizielle Währung.

Seit Mitte der 1980er Jahre bietet die Regierung Unternehmen die Möglichkeit, auf den Inseln mit einer Briefkastenfirma ansässig zu sein. Die Gebühren für die Gründung solcher Gesellschaften machen inzwischen mehr als 50 % der Staatseinnahmen aus. Etwa 450.000 Briefkastenfirmen waren 2012 auf den Inseln registriert. Bis 2015 waren es bereits 800.000 Unternehmen.




</doc>
<doc id="10178" url="https://de.wikipedia.org/wiki?curid=10178" title="METAFONT">
METAFONT

METAFONT ist eine abstrakte Beschreibungssprache zur Definition von Vektorschriften. Es ist auch der Name des zugehörigen Interpreters, der den Metafont-Code ausführt und Bitmap-Schriften bestimmter Auflösung erzeugt. Es wurde von Donald E. Knuth als Begleiter von TeX entwickelt, um den zweiten Band von "The Art of Computer Programming" zu setzen, da er mit dem Resultat des Fotosatzes des ersten Bandes unzufrieden war. METAFONT ist speziell entworfen, um TeX zu unterstützen, und ist deshalb auch Teil vieler TeX-Distributionen.

Knuth begann seine Arbeit an der Fonterstellungssoftware 1977 und brachte 1979 die erste Version von METAFONT heraus. Wegen Unzulänglichkeiten der originalen METAFONT-Sprache entwickelte Knuth 1984 ein komplett neues METAFONT-System, dieses überarbeitete System ist heute in Benutzung. Die Versionsnummer nähert sich in Analogie zu TeX der Eulerschen Zahl an und wird nach Knuths Tod zu e geändert werden.

Die Form der Buchstaben wird in METAFONT über geometrische Gleichungen definiert, die durchaus komplex sein können. Anders als die verbreiteteren Outline-Schriften (wie TrueType oder PostScript-Type-1-Fonts), besteht ein METAFONT-Font hauptsächlich aus Strichen von „Stiften“ bestimmter Breite, zusammen mit gefüllten Flächen. Daher beschreibt eine METAFONT-Datei nicht die Umrisse der Zeichen, sondern den Weg des Stiftes und die jeweilige Breite des Striches beim Zeichnen. Ein schönes Beispiel dazu findet sich im u. g. METAFONT-Tutorial in Kapitel 0.5.

Einige einfachere METAFONT-Fonts, wie z. B. die kalligraphischen Mathematikfonts der Computer-Modern-Familie, verwenden einen einzigen Strich eines relativ großen Stifts, um jeden sichtbaren „Strich“ eines Zeichens zu definieren. Komplexere Schriften wie die Roman-Textschriften der Computer-Modern-Familie verwenden in der Regel einen schmalen Stift, um die Umrisse der sichtbaren „Striche“ zu zeichnen, die dann gefüllt werden. Bei einem Standard-Outline-Font dagegen sind die Umrisse stets dimensionslos.

Das Verwenden von nicht-dimensionslosen Stiften zur Umrandung von Flächen führt in der Regel zu rationalen Kurven höherer Ordnung für die tatsächliche Umrisslinie, wodurch eine Konversion von METAFONT-Schriften nach TrueType oder PostScript oftmals äußerst schwierig ist und in der Regel nur angenähert werden kann – TrueType verwendet nur Kurven stückweise zweiten Grades, PostScript nur Kurven dritten Grades.

METAFONT unterscheidet sich von anderen Systemen durch die Variabilität der Fonts, die durch wenige Parameter wie Seitenverhältnis, Neigung, Strichstärke oder Serifengröße spezifiziert werden (deswegen auch das "Meta" im Namen); so kann durch Veränderung von relativ abstrakten Parametern an einer Stelle in der METAFONT-Datei eine in der ganzen Schrift konsistente Änderung im Aussehen erreicht werden. Über „Breite eines schmalen Striches“, „Größe von Serifen“, „Neigung der Ellipse in Rundungen“ kann z. B. eine klassizistische Antiqua zu einer modernen Grotesk-Schrift geändert werden.

Computer Modern Roman illustriert viele Verwendungen dieser Möglichkeiten; eine typische TeX-Installation beinhaltet für einen bestimmten Font Schriftschnitte in Größen von 5 bis 17 Punkt, mit der gleichen (statt durch die Hochskalierung zunehmenden) Strichstärke in allen Größen und größenabhängigen Zeichenbreiten für bessere Lesbarkeit. Andere Fonts von Computer Modern, im Speziellen Typewriter und die Sans-Serif-Schnitte, sind, wie schon weiter oben erwähnt, im Wesentlichen über die gleiche METAFONT-Datei definiert, jedoch mit unterschiedlichen globalen Parametern.

Andere Parameter wie die O-Korrektur (der Betrag, den eine Rundung größer gezeichnet wird) oder der Blackness-Parameter (Linien werden etwas dicker) dienen zur Anpassung an verschiedene Ausgabegeräte. Daneben sind in METAFONT die Definition von Ligaturen und die Beeinflussung des Kernings möglich.

Vor einiger Zeit ist ein Abkömmling von METAFONT entstanden, der eine PostScript-Datei ausgibt: MetaPost. Aus diesem wurde später METATYPE1 entwickelt, das einen Postscript-Type-1-Font mit den für TeX nötigen Metrikdateien erzeugt. Beide können jedoch keine Stifte verwenden, sondern beschreiben den Umriss des Bildes bzw. Zeichens.

Die wichtigste in METAFONT entwickelte Schrift ist die "Computer Modern" von Knuth, die von TeX als Standardschrift benutzt wird.

Der METAFONT-Interpreter kann auch interaktiv ausgeführt werden und kennt Kommandos, um die produzierten Bilder am Bildschirm auszugeben. Knuth sagte, er benutzt METAFONT als eine Art Tischrechner zur Lösung komplizierter Gleichungen, obwohl er mittlerweile MetaPost für mathematische Illustrationen verwendet.

Bei Metafont-Schriften muss für jedes Gerät ein angepasster Satz an Bitmap-Schriften erstellt werden. Die Idee von Metafont entspricht der von DVI-Dateien, bei denen keine Schriften, sondern nur Verweise darauf eingebunden werden. Nicht der Dokumentersteller liefert die Schriften, sondern derjenige, der das Dokument verarbeitet (liest bzw. ausdruckt), indem der DVI-Treiber die für dieses Gerät optimal erstellten Bitmap-Fonts lädt. Allerdings müssen auf dem Gerät die entsprechenden Schriften überhaupt verfügbar sein.

Hingegen verfolgt man bei PDF-Dateien den Ansatz, dass alle Schriften in die PDF-Datei eingebunden werden. Zwar kann man verschiedene Bitmap-Fonts einbinden, jedoch werden diese für unterschiedlichste Geräte nicht optimal angepasst sein. Somit werden für portable PDF-Dateien ausschließlich Vektorschriften (z. B. Type 1 Fonts) empfohlen. Beim Drucken hingegen liefern Metafont-Schriften, die auf den speziellen Drucker angepasst sind, Qualitätsvorteile gegenüber Type-1-Schriften.

Hier ein kleines Beispiel aus dem Metafont-Logo-font, einer sehr einfachen Schriftart. Jeder Punkt hat eine X- und eine Y-Koordinate, die mit x1, y1, x2, etc. bezeichnet werden. Der Ausdruck z4 ist eine Abkürzung für (x4,y4).

Die Zeile 1 definiert ein neues Zeichen mit dem Namen F und dem Zeichenkode 14.

In Zeile 2 werden die x-Positionen der Punkte 1, 2 und 3 auf leftstemloc gesetzt, einen Wert, der die Position des linken Striches angibt.

Die Zeile 3 definiert x4 so, dass er, abgesehen vom "over-shot" ho, den gleichen Abstand vom rechten Rand (spezifiziert durch w) hat, wie ihn Punkt 1 vom linken Rand hat.

In Zeilen 10 bis 14 werden letztendlich die Punkte 1, 3 und 4 durch gerade Linien (der linke und obere Strich vom F) verbunden und der kleine waagerechte Strich in der Mitte durch Verbinden der Punkte 2 und 5 gezeichnet.





</doc>
<doc id="10185" url="https://de.wikipedia.org/wiki?curid=10185" title="Ligatur (Typografie)">
Ligatur (Typografie)

Eine Ligatur (auch Buchstabenverbund, mittellateinisch "ligatura" „Verbindung“, nach lateinisch "ligatur" (3. Pers. Singular, Präsens Passiv) „es ist verbunden“, von "ligare" – „binden, verbinden“) bezeichnet in der Typografie die Verschmelzung zweier oder mehrerer Buchstaben einer Satzschrift zu einer Glyphe. Auch in handschriftlichen Kurrentschriften kommen Ligaturen vor. Dort entstanden diese entweder durch schnelle Schreibweise häufig genutzter Zeichenkombinationen oder zur optischen Korrektur.

Im Satz werden Ligaturen heute vor allem verwendet, wenn zwei Buchstaben mit Oberlängen (z. B. "f, i, l, t") aufeinander folgen, da ohne Ligatur eine Lücke zwischen den Buchstaben entstehen würde oder es bei Anwendung der Unterschneidung zu unschönen Verbindungen der Oberlängen käme. Im Bleisatz sind Ligaturen zur Ermöglichung von Unterschneidungen zwingend nötig. Menge, Art und Gebrauch der Ligaturen unterscheiden sich je nach Sprache und Schriftsystem.

Ligaturen vermeiden optische Lücken, die das Erscheinungsbild und die Lesbarkeit eines Texts stören. Sie werden vor allem im professionellen Satz verwendet, wurden aber aus zeitlichen und ökonomischen Gründen besonders im Zeitungssatz weggelassen. Ligaturen werden vor allem nach ästhetischen Gesichtspunkten gestaltet und nicht nur durch einfache Verringerung der Laufweite gebildet (siehe Abbildungen).

In deutschsprachigen Texten sind die Ligaturen "ﬀ, ﬁ, ﬂ, ft" sowie deren Kombinationen ("ﬃ, ﬄ" und so weiter) geläufig, weniger üblich sind Ligaturen etwa von "fk, fj, fh, fb, fz, ll, st, ch, ck, ct, th, tt, tz, kk, Qu, ſi, ſſ, ﬅ, ſch". Je nach Schriftart sind diese zudem selten Ligaturen im engeren Sinne, da die einzelnen Buchstaben nur zur Unterschneidung näher aneinander gerückt sind, aber keine tatsächliche Verbindung eingehen. Die Anzahl der Ligaturen ist bei verschiedenen Schriftarten unterschiedlich.

Im Deutschen werden Ligaturen nur gesetzt, wenn die zu verbindenden Buchstaben im gleichen Morphem liegen, beispielsweise im Wortstamm. Ligaturen werden in der Regel nicht gesetzt, wenn die Buchstaben über eine grammatikalische Fuge (z. B. eine Wortfuge) reichen. „Kaufläche“ "(Kau-fläche)" wird daher mit ﬂ-Ligatur geschrieben; „Kauf‌leute“ hingegen nicht, weil die Buchstaben "f" und "l" verschiedenen Wortteilen "(Kauf-leute)" angehören. Eine Ausnahme bilden Nachsilben, die mit "i" beginnen "(-ig, -in, -ich, -isch)". Hier werden auch über die grammatikalische Fuge hinweg Ligaturen gesetzt. So wird beispielsweise „häufig“ trotz der Fuge "(häuf-ig)" mit ﬁ-Ligatur geschrieben. Im Zweifel wird der Gliederung des Wortes nach Sprechsilben gefolgt und entsprechend die Ligatur gesetzt .
Die Anwendung von Ligaturen ist nicht verbindlich geregelt, generell folgt man dem Grundsatz: Getrennt gesprochene Buchstaben werden nicht in Ligatur gesetzt.

Die Verwendung von Ligaturen hat im Bleisatz neben den ästhetischen auch technische Gründe. Ohne Ligaturen hätte man beim Buchstaben "f" nur die Wahl, ihn bündig auf den Kegel zu stellen, was ein den Lesefluss störendes ‚Loch‘ im Satzbild zur Folge hätte, oder ihn rechts frei über den Kegel hinausgehenzulassen, damit er zum Teil über den Kegel des nachfolgenden Buchstabens rage. Der so freigestellte Teil des "f" würde jedoch ohne den schützenden Kegel leicht abbrechen. Aus diesem Grund werden die betreffenden Kombinationen direkt zusammen auf einen Kegel gegossen (vgl. Logotype).

Der Computersatz erlaubt heute eine fast beliebige Positionierung der Buchstaben. Deshalb ist es möglich, den Abstand zwischen zwei Zeichen einzustellen und in vielen Fällen auf Ligaturen zu verzichten. Viele Schriftarten bieten spezielle Ligaturen an. Andere Ligaturen werden durch entsprechende Positionierung der typografischen Zeichen emuliert. Die "ß"-Ligatur wird als ein Buchstabe behandelt und standardmäßig verwendet.

Neue Schrifttechniken wie OpenType, Graphite von SIL oder Apples wenig verbreitetes AAT erlauben den manuellen oder auch automatischen Einsatz von Ligaturen, ohne den zugrundeliegenden Code zu verändern, sofern dies auch explizit in einer Schrift vorgesehen ist. Einige Programme wie z. B. QuarkXPress (ab Version 7) oder InDesign bieten diese Möglichkeit mit OpenType sowohl unter Mac OS X als auch unter Windows.

Bei den meisten Microsoft-Programmen wie Word (bis Version 2007) müssen Ligaturen noch als Sonderzeichen eingefügt werden und verwirren daher die Rechtschreibprüfung. Das Emulieren mancher Ligaturen aus mehreren Einzelzeichen gelingt besser, wenn das Schriftdesign des Fonts darauf bereits angepasst ist. Microsoft Word 2010 ist die erste Version von Word, die OpenType-basierte Ligaturen offiziell unterstützt.
Schwierigkeiten können sich bei der Rechtschreibprüfung ergeben. Manche Satzprogramme unterstützen die Verwendung von Ligaturen nur in recht aufwändiger Weise. Einige Satzprogramme (zum Beispiel TeX) verlagern die Verwendung von Ligaturen daher in den Ausgabeprozess, was das Problem nur teilweise löst, da es kontextabhängig ist, ob ein Zeichen als Ligatur gesetzt werden muss oder nicht. Der Setzer muss explizit angeben können, ob eine Ligatur gesetzt werden soll oder nicht (z. B. in LaTeX codice_1 oder HTML codice_2 zur Unterbindung der automatischen Ligatur). Die automatisierte Setzung bzw. Unterdrückung von Ligaturen anhand der Silbentrennungswörterbücher wird bisher ausschließlich vom Satzprogramm LuaLaTeX mithilfe des Pakets "selnolig" geleistet.

Ungewollte „Ligaturen“ können entstehen, wenn Buchstaben zu eng zusammengerückt werden. Viele Schriftarten enthalten entsprechende Informationen zur Optimierung der Abstände von Buchstabenpaaren, sogenannte Unterschneidungspaare.

Der universelle Zeichensatz – Unicode – bietet nur sehr beschränkte Unterstützung für Ligaturen. Nur einige wenige Ligaturen wie „ﬁ“ haben aus Kompatibilitätsgründen mit bestehenden Zeichensätzen/-kodierungen (u. a. Macintosh Roman) ihren Platz in Unicode. Das Unicode-Konsortium lehnt es grundsätzlich ab, weitere Ligaturen in Unicode aufzunehmen und begründet dies damit, dass Ligaturen ein Problem der Zeichen"darstellung", nicht der Zeichen"kodierung" seien. Eine Steuerung der somit notwendigen automatischen Ligation soll mit dem Bindehemmer U+200C "(Verbindung unterdrücken)" und dem breitenlosen Verbinder U+200D "(Verbindung erzwingen)" geschehen, die in HTML als Zeichenentitäten codice_3 "(zero width non-joiner)" bzw. codice_4 "(zero width joiner)" aufgenommen wurden.

Im deutschsprachigen Fraktursatz gibt es eine Reihe von Ligaturen. Auch im Fraktursatz gilt in jedem Fall die Regel, dass eine Ligatur nicht über eine Wortfuge hinweg gesetzt werden darf (Beispiel: Ta-tz-e = tz-Ligatur, aber Lu-ft-z-ug = ft-Ligatur + z). Dies betrifft auch Familiennamen slawischer Herkunft auf -cky (z. B. "Ranicky"), die – entsprechend der getrennten Aussprache – nicht mit ck-Ligatur, sondern getrennten Lettern c und k geschrieben werden.

Im Sperrsatz werden die Ligaturen "ch, ck, ﬅ" und "tz" nicht gesperrt. Alle anderen üblichen Ligaturen "(ﬀ, ﬁ, ﬂ, ft, ll, ſi, ſſ, ﬅ, tt," seit Anfang des 20. Jahrhunderts auch "ſch)" werden gesperrt, also im Sperrsatz in Einzelbuchstaben aufgelöst. Bei "ﬅ" bestehe auch die Möglichkeit, dass es im Sperrsatz zwar nicht gesperrt, aber trotzdem aufgelöst werde. Das "ß" wurde schon im 19. Jahrhundert nicht mehr als Ligatur, sondern als Einzelbuchstabe begriffen und folglich auch nicht aufgelöst.

Die reformierte deutsche Rechtschreibung belebt im Bereich der s-Schreibung eine Idee aus dem frühen 19. Jahrhundert wieder, die sogenannte Heysesche s-Schreibung. Im Heyseschen Fremdwörterbuch taucht für Doppel-s am Wortende eine eigens geschaffene Ligatur "ſs" auf.

In den europäischen Schriften ist eine Ligatur ein Element der grafischen Oberflächenstruktur der geschriebenen Sprache, das heißt, dass sie nicht notwendiger Bestandteil des Schriftsystems (der Orthografie) sind. Ihre Verwendung folgt allein typografischen Regeln, die der optischen bzw. ästhetischen Gestaltung dienen, ohne für die Bedeutungsunterscheidung von Wörtern "notwendig" zu sein. Für das Funktionieren der deutschen Orthografie ist es zum Beispiel nicht notwendig, „knifflig“ mit der Ligatur für "ffl" zu schreiben. Die Verwendung von Ligaturen ist in keiner europäischen Sprache orthografisch vorgeschrieben, sie ist ein reines Stilmittel und variiert von Schriftart zu Schriftart. Ligaturen sind demnach keine Grapheme eines Schriftsystems.

Von diesen echten, synchron als solche verwendeten Ligaturen sind Einheiten eines Schriftsystems zu unterscheiden, die ursprünglich (diachron) aus Ligaturen entstanden sind (vgl. auch die Entwicklung des lateinischen Schriftsystems). Zu diesen zählen Buchstaben wie "w, ß, æ, œ" und Zeichen wie "&" und "%" (vgl. u.). Die Verwendung solcher Buchstaben unterliegt heute (synchron) "orthografischen" Regelungen, sie sind "kleinste bedeutungsunterscheidende Einheiten" eines Schriftsystems und damit Grapheme wie andere Buchstaben auch.

Der Buchstabe "w" hat sich als Ligatur aus zwei "u" bzw. "v" entwickelt. Bis ins Mittelalter wurde zwischen den Buchstaben "u" und "v" nur ästhetisch, nicht dem Lautwert nach unterschieden. In manchen Sprachen ist dieser Ursprung noch an der Buchstabenbezeichnung nachzuvollziehen (engl. "double u" „Doppel-u“, span. "doble uve" oder "doble v" „Doppel-v“, frz. "double v" „Doppel-v“).

Im Druck wurde das kleine w bei nicht vorhandener Letter auf verschiedene Weise realisiert, im Fraktursatz beispielsweise mit rundem r als „rv“.

Historisch gesehen geht das ß in der deutschen Sprache auf eine Ligatur aus "ſ" („langes s“, ursprünglich ein weiterer Buchstabe des deutschen Alphabets) und "z" in den gebrochenen Schriften zurück. Bedeutsam für die Form des ß in den heutzutage üblichen Antiqua-Schriftarten war jedoch auch eine Ligatur aus langem "ſ" und "s". Diese Variante des Doppel-"s" geriet in den genannten Schriften im 18. Jahrhundert gleichzeitig mit dem langen "ſ" außer Gebrauch. In der zweiten Hälfte des 19. Jahrhunderts setzte sich der Buchstabe "ß" – nach dem Vorbild des Fraktursatzes – allmählich auch im Antiquasatz durch. Mit Annahme der Beschlüsse der orthographischen Konferenz von 1901 wurde das "ß" auch im Antiquasatz zur offiziellen Regel.

Während die aktuelle deutsche Rechtschreibung das ß auf die Verwendung nach langen Vokalen und Diphthongen beschränkt und damit nur den ß-Buchstaben anerkennt, benutzen Autoren und Verlage wie Diogenes, die weiterhin die Rechtschreibung des 20. Jahrhunderts benutzen, auch die ß-Ligatur nach Adelung. Beide Varianten werden aus linguistischer Sicht meist als gleichwertig betrachtet, während über die typographische Ebenbürtigkeit nach wie vor Uneinigkeit herrscht. Der „Erfinder“ der reformierten ß/ss-Schreibung jedenfalls, Johann Christian August Heyse, hatte, nachdem heftige Kritik an seiner Schreibvariante geäußert worden war, 1826 vorgeschlagen, „ein neues Zeichen zu creiren [und] ein ſ zu verbinden mit einem s“. Diese neue Ligatur entsprach damit prinzipiell dem im 20. Jahrhundert aus ſ und s neugeschaffenen ß für die Antiqua.

Das &-Zeichen (Kaufmanns-Und) ist eine Ligatur aus den ursprünglichen Einzelbuchstaben „e“ und „t“ (, „und“). In der Schreibschrift hat es sich zunächst in eine Darstellung entwickelt, in der der Bogen vom kleinen "e" in den Stamm des kleinen "t" übergeht. Wird das "E" großgeschrieben, ergibt sich in vielen kursiven Schriften die nebenstehende Darstellung. Daraus hat sich durch weitere Vereinfachung das heute bekannte Kaufmanns-Und () gebildet.

Das Prozentzeichen % ist eine Ligatur von „per cento“ (ital. „von hundert“), das @-Zeichen wird gemeinhin als eine Ligatur von „ad“ oder „at“ angesehen. Die Herkunft ist noch nicht geklärt.

In skandinavischen Texten sind die Ligaturen von "ſk", "ſl" und "ſþ" üblich. In der französischen Sprache unterscheidet man ästhetische und orthographische Ligaturen. Die orthographischen Ligaturen sind bindend, dürfen nicht getrennt geschrieben werden und gelten als eigene Buchstaben ("Æ" und "Œ", l’e-dans-l’a und l’e-dans-l’o). Umstritten ist die Stellung der Ligatur "IJ" als eigener Buchstabe im Alphabet des Niederländischen.

Nicht nur in der lateinischen Schrift gibt es Ligaturen, auch in vielen anderen sind sie vorhanden. Im griechischen Alphabet findet sich als Zahlzeichen für die Ziffer 6 noch der alte Buchstabe Ϛ (Stigma), eine Ligatur aus Sigma und Tau. Schon seit byzantinischer Zeit wird gelegentlich für den Digraphen ΟΥ (Omikron-Ypsilon, gesprochen ), die Ligatur Ȣ verwendet.

Der kyrillische Buchstabe Ю (Ju) geht auf eine Ligatur der griechischen Buchstaben Iota und Omikron zurück. Das Serbische verwendet die Ligaturen Љ (Lje) und Њ (Nje), zusammengesetzt aus Л bzw. Н und dem Weichheitszeichen Ь für die palatalisierten Laute // und //.

Einige Schriften werden grundsätzlich nur wortweise verbunden geschrieben und gedruckt, wie z. B. Mongolisch und Arabisch. In der arabischen Schrift ist die Form des Zeichens vom Kontext abhängig. Es gibt bis zu vier verschiedene Formen je Buchstaben: allein stehend, initial, medial und final. Die einzige Zwangsligatur im Arabischen ist das Lam-Alif (), das bei der Verbindung der Buchstaben  (Lam) und  (Alif) entsteht. Bei bestimmten Schriftarten kommt eine große Zahl von weiteren Ligaturen hinzu, deren Verwendung aber nicht obligatorisch ist.
In fast allen indischen Schriften spielen Ligaturen eine wichtige Rolle. Hier sind sie nicht nur typografische Varianten, sondern haben einen graphematischen Status, ihr Gebrauch ist also bedeutungsunterscheidend. In den indischen Schriften trägt jeder Buchstabe einen inhärenten Vokal (meist "a"). Wenn zwei Konsonanten unmittelbar ohne Vokal aufeinandertreffen, werden sie zu einer Ligatur verbunden. Vor allem in Sanskrit-Texten kommen teils sehr komplizierte Ligaturen mit drei oder mehr Bestandteilen vor. Manche Ligaturen sind in ihrer Bildungsweise einfach, z. B. ergeben in der Devanagari-Schrift स (sa) und न (na) die Ligatur स्न (sna). Bei anderen Verbindungen wie क्ष (kṣa) – aus क (ka) und ष (ṣa) – sind die Einzelbestandteile dagegen nicht mehr ohne weiteres zu erkennen. Allein die tamilische und die singhalesische Schrift verwenden keine Ligaturen, sondern ein spezielles diakritisches Zeichen, welches das Fehlen des inhärenten Vokals anzeigt.

Das Japanische kennt die Katakana-Ligatur ヿ (koto) sowie die Hiragana-Ligatur ゟ (yori).




</doc>
<doc id="10187" url="https://de.wikipedia.org/wiki?curid=10187" title="Unterschneidung (Typografie)">
Unterschneidung (Typografie)

Unterschneidung bezeichnet in der Typografie den Vorgang, den horizontalen Abstand (den "Weißraum") zwischen mehreren Buchstaben (Standarddickte) durch optischen Ausgleich so zu verringern, dass er gleichmäßig erscheint und so vom Betrachter als angenehmer empfunden wird.

Die Zeichenfolge VA verdeutlicht die Unterschneidung:
Weitere Beispiele für Unterschneidungspaare:

Av, AV, Aw, AW, LT, LV, Ly, Ta, Te, To, Ty, T., Va, Vo, V., Ya, Yo, Y.

Zu Zeiten des Bleisatzes war die Zeichenbreite (Dickte) einschließlich des nicht bedruckten Abstandes zu den benachbarten Buchstaben links und rechts (Fleisch) normalerweise durch die physische Breite einer einzelnen Letter bestimmt. Unterschneidung war für längere Texte kaum möglich, da diese zu aufwendig gewesen wäre. Sie kam daher nur bei relativ großen Buchstaben zur Anwendung. Dabei wurde der Kegel (Schaft) so geschnitten, dass der Buchstabe teilweise in den Bereich des benachbarten Buchstabens hineinragte, also vor dessen Kegel lag. Vom sogenannten Fleisch unter dem Buchstaben wurde etwas weggeschnitten, sodass die Buchstaben näher zusammengerückt werden konnten. Für häufig vorkommende Buchstabenkombinationen wurden Unterschneidungspaare auch als Ligaturen in Blei gegossen.

Dementsprechend sind auch bei den meisten TrueType-, OpenType- und PostScript-Schriftarten für den Computersatz Informationen zur Definition von Unterschneidungspaaren (Kerning-Paaren) hinterlegt. Diese werden auch von vielen landläufigen Textverarbeitungsprogrammen berücksichtigt, wobei das oftmals erst explizit eingestellt werden muss. Bei Metafont geschieht die Vorgabe beispielsweise mit der ligtable-Definition, bei Microsoft Word über "Zeichenformat – Zeichenabstand – Unterschneidung ab Schriftgröße x". Zusätzlich ermöglichen die meisten DTP-Programme die Definition von Unterschneidungen per Hand.

Für besondere Zwecke gibt es auch Schriftarten ohne Unterschneidung, zumeist sind dies nicht-proportionale, d. h. dicktengleiche Schriften, wie beispielsweise die Schreibmaschinenschrift Courier.

Zwar handelt es sich bei einer Überschneidung wie auch bei der Unterschneidung ebenfalls um eine Überlappung von nebeneinander liegenden Zeichen. Die Überschneidung ist aber dadurch gekennzeichnet, dass sie nicht umgebungsabhängig ist: 

Das kleine z der Garamond-Kursiven ragt beispielsweise mit seinem geschwungenen unteren Strich immer in den Bereich des rechts nachfolgenden Zeichens hinein.





</doc>
<doc id="10192" url="https://de.wikipedia.org/wiki?curid=10192" title="Pentagramm">
Pentagramm

Pentagramm ( "pénte" ,fünf‘, πεντάγραμμος "pentágrammos" ‚mit fünf Linien‘) ist unter anderem Bezeichnung für eine Form des fünfzackigen Sterns (Fünfsterns), die sich ergibt, wenn die Diagonalen eines regelmäßigen Fünfecks (Pentagon) nachgezogen werden. Verbreitet sind auch die Bezeichnungen Drudenfuß bzw. Drudenstern, Pentakel sowie Pentalpha, da es sich durch fünf ineinander stehende Alphas („Α“) bilden lässt. Da das Pentagramm fünf Spitzen hat, gibt es zwei grundsätzliche Arten seiner Ausrichtung: mit einer Spitze nach oben (dann zeigen zwei Spitzen nach unten und zwei zur Seite), oder mit einer Spitze nach unten (dann zeigen zwei Spitzen nach oben und zwei zur Seite). Letzteres wird auch als „umgekehrtes“ oder „invertiertes“ Pentagramm bezeichnet.

Das Pentagramm ist im Unicodeblock Verschiedene Symbole aufgenommen. Am Codepunkt U+26E4 (9956) ⛤ unter dem Namen "PENTAGRAM" und am Codepunkt U+26E7 (9959) ⛧ als "INVERTED PENTAGRAM".

Aus fünf gleichmäßig, also in Abständen von 72°, auf einem Kreis verteilten Punkten lassen sich mittels Sehnen zwei fünfachsig symmetrische Figuren erstellen (Konstruktion der Punkte siehe Konstruktion eines regelmäßigen Fünfecks mit Zirkel und Lineal):

Die inneren Abschnitte der Sehnen des Pentagramms bilden wiederum ein regelmäßiges Fünfeck. Gegenüber dem äußeren ist es um 36° gedreht. Die Zacken des Pentagramms sind gleichschenklige Dreiecke. Die Winkel zwischen Basis und Schenkeln dieser Dreiecke betragen 72°.

Das innere Fünfeck bildet zusammen mit je zwei nicht benachbarten Zacken ein gleichschenkliges Dreieck mit stumpfer Spitze, dem schon erwähnten 108°-Winkel.

Zeichnet man in das innere Fünfeck wieder ein Pentagramm, so bilden dessen Sehnen mit Teilen der Sehnen des äußeren Pentagramms ebenfalls gleichschenklige Dreiecke mit stumpfer Spitze von 108°. Deren Mittelsenkrechten liegen parallel zu denen der Zackendreiecke, die ihrerseits die Symmetrieachsen aller Pentagramme und Fünfecke bilden.

Äußeres Fünfeck, Pentagramm und inneres Fünfeck haben denselben Mittelpunkt. 
Jede Sehne des inneren Pentagramms verläuft parallel zu einer jenseits des Mittelpunktes gelegenen Sehne des äußeren Pentagramms.

Alle Winkel zwischen den Kanten des Pentagramms und des umschließenden Fünfecks betragen also 36°, 72° oder 108°. Die fünf Symmetrieachsen haben zu den Kanten Winkel von 18°, 54° und 90°.

Bezeichnet man den Abstand von zwei benachbarten Spitzen mit formula_1 und den Umkreisradius mit formula_2, so ergibt sich die Sehnenlänge formula_3 des Pentagramms zu
bzw.

Die Länge einer der zehn Außenkanten des Pentagramms (der Abstand von einer Zackenspitze bis zum ersten Schnittpunkt mit einer Sehne) lässt sich folgendermaßen berechnen:

Alle Sehnen und durch Schnittpunkte begrenzte Sehnenteile eines Pentagramms samt äußerem und innerem Fünfeck haben nur vier verschiedene Längen. Davon stehen jeweils aufeinander folgende zueinander im Verhältnis des Goldenen Schnitts, d. h. die Verhältnisse: 

Denn es gilt z. B. für das erste Verhältnis:

Aufgrund des Strahlensatzes gilt:

Betrachten wir das Verhältnis
Dann ist formula_13 und entspricht somit der Definition des Goldenen Schnitts.

Berechnung des Flächeninhaltes eines Pentagramms mit der Seite formula_14 (Seite des Fünfecks in dem Pentagramm):

... oder in Zahlen:

Wenn statt der Seitenlänge des inneren Fünfecks formula_1 die des äußeren Fünfecks
formula_18 mit formula_19 bekannt ist, dann ist folgende Formel anwendbar:

in Zahlen:

Das Fünfeck taucht in verschiedenen Zusammenhängen in der Natur auf, so ist es ein Kennzeichen der Gewöhnlichen Stockrose.

Das Pentagramm war das Symbol der Venus, sowohl des Planeten als auch der Göttin. Es wurde auf einem Krug aus der mesopotamischen Djemdet-Nasr-Zeit, d. h. um 3000 v. Chr., als Ideogramm der sumerischen Göttin Inanna/Ištar gefunden.

Pythagoras kannte es als Symbol für Gesundheit. Ihn interessierte daran besonders der mathematische Aspekt des Goldenen Schnitts. Da man es in einem Zug zeichnen kann und am Schluss wieder zum Anfang gelangt, galt es auch als Zeichen für den Kreislauf des Lebens. Abraxas, Gott der Gnostiker, wurde ebenfalls durch ein Pentagramm symbolisiert, weil er fünf Urkräfte in sich vereint.

Auch in der Heraldik, zum Beispiel auf Wappen und Flaggen, wird der Drudenfuß als gemeine Figur verwendet. Marokko und Äthiopien führen den Drudenfuß in ihren Flaggen, die Kommunen Knielingen, Giebenach und Schlotheim in ihren Wappen.

Das Pentagramm mit seinem Goldenen Schnitt prägte manchen Kirchenbau.
Pentagramme sind noch heute an vielen Fensterrosetten gotischer Kirchengebäude zu sehen. An der Ostseite des Turms der Marktkirche Hannover ist ebenfalls ein Drudenfuß zu sehen. Eine christliche Deutung für die fünf Ecken sind die fünf Wunden Jesu Christi.

Von den Dombauhütten kam das Pentagramm zu den Freimaurern und wurde das übergeordnete Symbol auf ihren Arbeitsteppichen. Seine fünf Spitzen weisen auf die Tugenden der Klugheit, der Gerechtigkeit, der Stärke, der Mäßigung und des Fleißes hin. 

Weitere symbolische Deutungen der fünf Ecken des Pentagramms sind der Geist und die vier Elemente Feuer, Wasser, Erde und Luft; oder auch Äther und die vier Himmelsrichtungen Norden, Süden, Westen und Osten.

Der "Flammende Stern" ist ein freimaurerisches Pentagramm, das von einer Gloriole umgeben ist und zentral den Buchstaben "G" enthält (identische Bedeutung).

Als Symbol auf Münzen ist das Pentagramm bereits seit dem Frühmittelalter auf merowingischen Münzen nachweisbar. Auch auf den Münzen und Geldscheinen Französisch-Marokkos ist es vielfach zu finden. Beispiele:

Das Pentagramm, insbesondere wenn es von einem Kreis umschlossen ist, wird auch als Symbol für Amulette verwendet und "Pentakel" genannt. Das Wort kann aber auch allgemein Amulette zur magischen Beschwörung bezeichnen, auch wenn ein anderes Symbol darauf ist.

In mittelalterlicher und nachmittelalterlicher Zeit galt das Pentagramm als Bannzeichen gegen das Böse sowie als Zauber- und Abwehrzeichen gegen Dämonen. Der Name "Drudenfuß" wird zum einen damit erklärt, dass das Zeichen als Schutzzeichen gegen nächtliche Spukgeister, die Druden, angesehen wurde. Es gab aber auch den Glauben, dass Druden selbst einen vogelartigen Fußabdruck hinterlassen, der in etwa dem Pentagramm gleicht.

In Goethes "Faust I" (Vers 1395 f.) hindert das Zeichen den Teufel Mephistopheles daran, Fausts Studierzimmer zu verlassen:

Seit dem französischen Okkultisten Éliphas Lévi wird insbesondere das „umgekehrte“ Pentagramm/Pentakel mit Okkultismus und Satanismus in Verbindung gebracht. In Lévis "Dogme et rituel de la haute magie" aus der Mitte des 19. Jahrhunderts wird es als „Zeichen der Ziege des Sabbaths“ bezeichnet, wobei die Zacken jeweils für die Hörner, die Ohren und den Bart der Ziege stehen. Umgekehrt wurde das auf zwei Spitzen stehende Pentagramm mit Jesus Christus identifiziert, wobei Jesu eigentlicher Name יְהוֹשׁוּעַ (Jehoschua) beziehungsweise יֵשׁוּעַ (Jeschua) vom Tetragrammaton יהוה (JHWH) ausgehend mit יהשוה wiedergegeben und als "Pentagrammaton" bezeichnet wurde.

Darauf bauten zahlreiche spätere Okkultisten auf. Mit einem Ziegenkopf im von verzerrten hebräischen Zeichen (LVYThN לִוְיָתָן) umringten, invertierten Pentagramm erschien es 1961 auf dem Cover von Maurice Bessys "Histoire en 1000 Images de la Magie"; Anton Szandor LaVey übernahm dieses unter der Bezeichnung "Sigil of Baphomet" als Zeichen seiner 1966 gegründeten Church of Satan. LaVey benutzte das Siegel des Baphomet auch für die Cover der Schallplatte "The Satanic Mass" (1968) und der "Satanischen Bibel" (1969).

In den 1980er Jahren wurde das invertierte Pentagramm in der Metal-Szene, insbesondere im Black- und Death Metal, als Symbol für das Böse oder den Satanismus beliebt, das Siegel des Baphomet erschien z. B. auf Schallplatten wie Jamras "The Second Coming" (1972) und Venoms "Welcome to Hell" (1982). Das Pentakel (Pentagramm mit Kreis) ist auch im Neuheidentum, insbesondere in der neureligiösen Bewegung Wicca, ein wichtiges Symbol.




</doc>
<doc id="10193" url="https://de.wikipedia.org/wiki?curid=10193" title="High Performance File System">
High Performance File System

HPFS ist das Dateisystem von OS/2. 

Es wurde 1989 mit der OS/2-Version 1.2 als erste Implementierung eines installierbaren Dateisystems (IFS) eingeführt. In Gegensatz zum direkt im Systemkern integrierten FAT16-Dateisystem unterstützt es Partitionen bis zu einer theoretischen Größe von 2 Tebibyte (in der Praxis bis zu 64 Gibibyte) und lange Dateinamen mit bis zu 255 Zeichen, sowie sogenannte erweiterte Attribute, die es erlauben, beliebige Metainformationen mit einer Größe bis zu 64 kB je Datei, an eine Datei zu binden, ohne deren Inhalt zu verändern. So lässt sich z. B. eindeutig hinterlegen, mit welcher Anwendung eine Datei bearbeitet werden kann. Hierdurch entfällt gleichzeitig der Zwang, einer Datei eine bestimmte Dateiendung geben zu müssen (wie z. B. unter Windows).

Das fehlende Journaling führt dazu, dass das Dateisystem durch Abstürze verwundbar ist. Aufgrund dessen und der Größenbeschränkungen für Dateien und Partitionen wurde von IBM später das Journaled File System entwickelt, welches seit 2000 von OS/2 unterstützt wird.

HPFS wurde in OS/2 in zwei Varianten implementiert. Die Standard-Version lag dem Betriebssystem bei, daneben gab es eine 32-Bit-Variante namens "HPFS386", die unter anderem Bestandteil des LAN Server war.

Es sind verschiedene Programme zum Lesen von HPFS-Dateisystemen verfügbar. Einige bieten auch Schreibmöglichkeiten an.

Die ersten Versionen von Windows NT unterstützten neben dem damals neuen Dateisystem NTFS auch HPFS und konnten bei Bedarf auf HPFS-formatierte IFS-Partitionen installiert werden. Windows NT 4.0 unterstützt standardmäßig kein HPFS mehr, es sei denn, es wird eine Update-Installation von Windows NT 4.0 von einer bestehenden Windows NT 3.51-Installation vorgenommen. Der Treiber kann aber auch manuell von der Windows NT 3.51-CD kopiert und installiert werden. Bei Windows 2000 ist der nötige Treiber auf der Windows 2000-CD vorhanden und kann bei Bedarf nachinstalliert werden. In allen Fällen unterstützt der mitgelieferte Treiber lediglich Partitionen bis zu einer Größe von 4 GiB, größere Partitionen werden nicht unterstützt und beim Zugriff zerstört. In Windows XP wurde die HPFS-Unterstützung entfernt – auch ein auf einer HPFS-Partition installiertes Windows-NT-Betriebssystem lässt sich fortan nicht mehr starten.

Ein Kernelmodul sorgte ursprünglich nur für den lesenden Zugriff. In späteren Versionen wurde die Schreibfunktionalität hinzugefügt und sonstige Erweiterungen vorgenommen.




</doc>
<doc id="10195" url="https://de.wikipedia.org/wiki?curid=10195" title="Obelisk">
Obelisk

Ein Obelisk (Plural "Obelisken"; über von , dem Deminutiv von ) ist ein freistehender, hoher, sich nach oben verjüngender monolithischer Steinpfeiler (Stele), der eine pyramidenförmige Spitze hat, das Pyramidion. Ägyptologen verwenden meist den Ausdruck Tehen-Pfeiler für Obelisk.

Der Obelisk stellte im alten Ägypten wie die Pyramide die steingewordenen Strahlen des Sonnengottes dar und ist die Verbindung zwischen der hiesigen und der Götterwelt. Obelisken standen in der Regel paarweise vor Pyramiden oder Tempeln.

Zunächst wurden Obelisken vor dem Tempel des Sonnengottes Re in Heliopolis, dem alten "On", aufgestellt. Im Alten Reich waren Obelisken zunächst schmucklos glatt — lediglich die pyramidenförmige Spitze war vergoldet bzw. mit Elektron überzogen und spiegelte so den Glanz der Sonne und damit die Macht des Sonnengottes wider. Ihr Schattenumlauf war Anzeiger und Zeichen für die tägliche Umfahrt des Re auf der Sonnenbarke von Osten nach Westen (nachts zurück durch die Unterwelt) und wurde von den Menschen beobachtet. Sein genau bestimmter Auf- und Untergang und der unbeeinflussbare Jahresablauf machten Re zum Sinnbild der Weltordnung, Hüter des Rechts und der zwischenmenschlichen Beziehungen. Begleitet wird Re von seiner Tochter Maat, der Göttin der guten Ordnung, der Gerechtigkeit und der Wahrheit. Re löste den Urgott Atum ab und galt als Vater des Pharao, der deshalb den Königstitel "Sohn des Re" trug. Daher wurden die Obelisken als Symbole des Sonnengottes, der göttlichen Weltordnung und der Verbindung Re-Pharao so wichtig, so verbreitet und wurden auch immer größer ausgeführt.

Mit der Verschmelzung Res mit dem ursprünglichen Fruchtbarkeitsgott Amun aus Theben zum Reichsgott Amun-Re stieg die Bedeutung und Verbreitung des baulichen Zeichens noch weiter. Im Neuen Reich wurden die Obelisken schließlich ganz mit Hieroglyphen beschriftet. Die größten Obelisken ließ die Pharaonin Hatschepsut (18. Dynastie) herstellen. Der bis heute im Amun-Tempel in Karnak bei Luxor stehende ist 32 m hoch. Seine Spitze war ebenfalls mit Elektron überzogen.

Das Gewicht eines großen Obelisken beträgt über 200 Tonnen; ein unvollendeter Obelisk in Assuan gilt als größter mit etwa 1100 Tonnen. Im alten Ägypten war das Ziehen und Aufrichten heiliger Pfeiler ein wichtiger Teil des rituellen Geschehens.

Die zunächst schmucklosen Obelisken erhielten später Inschriften. Die Anordnung der Hieroglyphen-Inschriften folgte dabei festen Regeln. Die Zeilen der Hieroglyphen sind nach rechts oder links orientiert. Die Menschen- und Tiersymbole der Schriftzeichen auf der Vorder- und Rückseite der paarig vor den Tempeln aufgestellten Obelisken blickten immer zu dem zwischen den beiden Tehen-Pfeilern verlaufenden Weg zum Tempeleingang, die auf den Seitenflächen zum Tempel. So kann noch heute bei den nach Rom, Paris oder Istanbul verbrachten Obelisken die ursprüngliche Orientierung zum Tempel bestimmt werden. Diese Anordnung von Bildsymbolen ist uralt, so blicken bereits die Tierfiguren von Stier, Fuchs oder Kranich auf den Längsseiten der T-förmigen Pfeiler der ältesten bekannten Tempelanlage in Göbekli Tepe auf den Weg, der ins Zentrum der jeweiligen Anlage führt.
Römische Kaiser brachten 13 Obelisken als Siegestrophäen aus Ägypten nach Rom. Einige wurden im frühen Mittelalter wegen ihres „heidnischen“ Ursprungs zerstört oder zerfielen.

Große altägyptische Obelisken stehen heute außer in Luxor (4) und Rom (13) noch je einer in Heliopolis, Kairo, Istanbul, Paris (Place de la Concorde), London (St. George’s Circus), New York (Central Park), Wimborne Minster (Kingston Lacy), Urbino, Florenz, München, Catania und Caesarea Maritima. Die Obelisken in London und New York, „Cleopatra’s Needles“ genannt, standen gemeinsam vor einem Tempel. Der seit 1836 in Paris aufgestellte Obelisk von Luxor ist 23 Meter hoch, wiegt 258 Tonnen und stand zuvor am Eingang des Luxor-Tempels.
Bis zum Ende des Neuen Reiches wurden alle Obelisken aus dem typischen rötlichen Assuangranit in den Granitsteinbrüchen südöstlich von Assuan hergestellt. Dazu wurde ein Monolith in der erforderlichen Größe geschnitten und i. d. R. noch im Steinbruch in die Grobform gehauen. Anschließend konnte die Verschiffung über den Nil zum jeweiligen Zielort erfolgen.

Nach dem Niedergang des Alten Ägypten und dem Verlust ihrer ursprünglichen religiösen Bedeutung wurden Obelisken auch aus anderen Materialien gefertigt, zunächst aus anderen Natursteinsorten, in der Neuzeit sogar aus Metall. Die meisten neuzeitlichen Obelisken sind zudem keine Monolithen, sondern aus mehreren Teilen zusammengesetzt, oder sie bestehen aus Mauerwerk, das mit Naturstein verkleidet wurde.

Einen großen Obelisken nur mit Menschenkraft, Pferden, Holz und Seilen aufzurichten, war selbst noch in der Renaissance kein leichtes Unterfangen. Dies verdeutlicht ein Kupferstich von 1743, der die Aufrichtung des Vatikanischen Obelisken auf dem Petersplatz in Rom am 10. September 1586 zeigt: mit 907 Menschen, 75 Pferden, 40 Spillen und Flaschenzügen sowie 5 großen Hebeln unter der Leitung von Domenico Fontana im Auftrag von Papst Sixtus V.

Die Ägypter hatten die Probleme von Herstellung, Transport und Aufrichtung unter ungeheurem Einsatz mit einfacheren Techniken gelöst. Ein Obelisk war jeweils an einem Stück aus dem Rosengranitfelsen von Assuan zu schlagen. Dafür wurden mit Schlagwerkzeugen zunächst zwei begehbare Gräben von z. B. 32 Meter Länge in den Fels getrieben, ebenso zwei kurze an der künftigen Grundfläche und der Spitze.

Die Form perfektionierte man weitgehend schon vor Ort, um Gewicht beim Transport zu sparen. Am Grund der Gräben wurden seitliche Schlitze zur Mitte des teilfreigelegten Blocks vorgetrieben. Wie genau das Ablösen des Obelisken vom Felsuntergrund geschah, ist anhand der archäologischen Befunde nicht sicher zu klären. Vielleicht durch Holzkeile, die man mit Wasser begoss, so dass sie aufquollen. Oder man unterhöhlte den Obelisken komplett und mauerte nach und nach den Hohlraum mit Ziegelsteinen aus, um ein Absacken zu vermeiden. Nach dem Ablösen wurde der Block mit Hebeln angehoben und mit Holz unterfüttert. Dann wurde er weiter angehoben und wieder unterfüttert.

Am Block wurden Kufen angebracht, auf denen er mit Hilfe von vielen hundert Menschen, die großen mit weit über tausend Arbeitskräften, fortgezogen werden konnte. Die Hauptstrecke wurde auf einem großen Nilschiff zurückgelegt, von Assuan bis Heliopolis beträgt die Entfernung 1500 km. Am Aufstellungsort wurde der Obelisk auf den Kufen eine Sandaufschüttung hinauf gezogen oder gehebelt, deren Höhe über der Basis des vorgesehenen Ortes der halben Länge des Obelisken entsprach. Die Mitte des Obelisken wurde über dem Aufstellungspunkt platziert. Zusätzlich hatten die Arbeiter eine schräge Mauer errichtet, die vom Baugrund bis zur Obeliskenbasis reichend als Führung diente. Unter der Mitte des Obelisken lag quer ein sehr starker Rundbalken. Nun wurde der Sand unter der unteren Hälfte langsam weggegraben. So senkte sich die untere Hälfte des Obelisken ab auf sein Fundament und die obere hob sich am Drehpunkt in die Höhe. Wegen der Mauer konnte der Stein nicht wegrutschen. Der Sand wurde abgegraben bis der Obelisk ganz heruntergeglitten war und senkrecht stand. Anschließend wurden die Mauern, die ja eine bloße Montierhilfe waren, der Restsand und die Kufen entfernt.

Die Römer brachten, wie Sueton berichtet, erbeutete Obelisken mit hierfür verstärkten Schiffen nach Rom. Da sie auch die ägyptischen Steinbrüche weiterbetrieben und z. B. hunderte von Granitsäulen für die kaiserlichen Bauten mit bis zu 240 Tonnen Gewicht vom Mons Claudianus in der östlichen ägyptischen Wüste nach Rom transportierten, ist zu vermuten, dass sie sich auch der ägyptischen Techniken bedienten.

Die assyrischen Obelisken war Inschriften- und Bildträger. Im Gegensatz zu den ägyptischen Obelisken, deren geradkantige Spitze in ihrer Form an Pyramiden erinnert, besaßen assyrische Obelisken eine gestufte Bekrönung, die in ihrer Form an eine Zikkurat erinnert. Wegen ihrer im Wesentlichen nur einseitigen Bearbeitung werden sie oft auch als Stelen bezeichnet.

Die bekanntesten Stücke sind der "Schwarze Obelisk" des Salmanassar III. (859–824 v. Chr.), der "Broken Obelisk" von Aššur-bel-kala (1073–1056 v. Chr.) und der "Weiße Obelisk" des Königs Aššur-nâṣir-apli I. (1050–1031 v. Chr) oder II. (883–859 v. Chr). 

Auf Letzterem befinden sich verschiedene Darstellungen in acht Registern, unter anderem Kriegs-, Tributs- und Jagdszenen. Der "Rassam Obelisk" (benannt nach Hormuzd Rassam) ist ein sehr fragmentarisch erhaltenes Exemplar, das aufgrund seiner Inschrift dem Aššur-nâṣir-apli II. (883–859 v. Chr.) zugeschrieben werden kann.

Die zu Beginn des 1. Jahrtausends n. Chr. errichteten Stelen in der äthiopischen Stadt Aksum werden manchmal ebenfalls als Obelisken bezeichnet.

Neuzeitliche Obelisken sind zumeist aus mehreren Steinen zusammengesetzt und gemauert.

Im Jahre 1775 errichtete Markgraf Carl Friedrich von Baden einen Obelisk in Linkenheim. In dieser Zeit entstanden weitere Obelisken in der Region Karlsruhe, Würzburg und auch bei Wiesbaden.

Der Obelisk aus Blaustein, nach dem Entwurf des "Capitaine" Boucher, wurde 1807 zur Erinnerung an den Beginn der Landvermessung durch den napoleonischen Geographen Jean Joseph Tranchot auf dem Lousberg in Aachen errichtet. Von ihm aus wurden mit dem Triangulationsverfahren weitere Punkte im Gelände bestimmt, mit deren Hilfe man schließlich das gesamte Gelände kartografisch darstellen konnte. Mit der Absetzung Napoleons am 2. April 1814 wurde das Denkmal zerstört. Am 15. Mai 1815 wurde der Obelisk auf Anordnung des preußischen Freiherrn Karl von Müffling wiedererrichtet, der die Vermessungsarbeiten im Auftrage des Königreiches Preußen fortsetzte. Als Hilfspunkt für die Triangulation genutzt, befindet sich in Sichtweite des Lousberg-Obelisken im Paulinenwäldchen bei Aachen ein weiterer Blaustein-Obelisk.

Von Fritz Gerth stammt das 1909 Herzog Adolph gewidmete Landesdenkmal in Wiesbaden. Er platzierte ein vier Meter hohes klassizistisches Bronzestandbild vor einem massiven Steinobelisken von acht Metern Höhe. 

Im Jahr 1823 wurde auf dem Braunschweiger Löwenwall ein Obelisk nach einem Entwurf von Peter Joseph Krahe zu Ehren der in den Napoleonischen Kriegen gefallenen Braunschweigischen Herzöge Karl Wilhelm Ferdinand und Friedrich Wilhelm errichtet.

Das höchste Bauwerk in der Form eines Obelisken, das Washington Monument in Washington, D.C. mit seinen 169,30 m Höhe, wurde 1884 fertiggestellt, ist jedoch eigentlich nur ein Turm. Es war zu seiner Zeit (bis zur Fertigstellung des Eiffelturms 1889) das höchste Bauwerk der Erde und löste damit den Kölner Dom ab.

Seit der Renaissance wurden vielerorts in Europa kleine – meist gemauerte – Obelisken als hoheitliche Zeichen auf Portalen, Giebeln und in den Ecken von Kirchen und Palästen aufgesetzt, die dadurch in ihrer Bedeutung aber auch rein optisch aufgewertet wurden.




</doc>
<doc id="10197" url="https://de.wikipedia.org/wiki?curid=10197" title="OS/2">
OS/2

OS/2 (Operating System/2 – anfangs bei IBM Deutschland auch "BS/2" für "Betriebssystem/2") ist ein multitaskingfähiges Betriebssystem für den PC.
Es wurde ursprünglich als Nachfolger für DOS von IBM und Microsoft gemeinsam entwickelt. Nachdem Microsoft 1991 die Kooperation beendete, um sich stattdessen der Windows-Weiterentwicklung zu widmen, entwickelte IBM OS/2 allein weiter. 2005 wurden der Vertrieb und die Basisentwicklung von IBM eingestellt, unter der Markenbezeichnung eComStation wird das Betriebssystem seitdem von Dritten weiterentwickelt.
Als Nachfolger von eComStation und OS/2 ist seit 2017 „ArcaOS“ des Unternehmens Arca Noae erhältlich.

Im Januar 1983 begann Microsoft die Entwicklung an einer Version von MS-DOS, die nicht nur ein Programm, sondern mehrere Programme gleichzeitig ausführen konnte. Dieses wurde später unter der Bezeichnung MS-DOS 4.0 veröffentlicht. Allerdings hatte diese Version von MS-DOS einige Einschränkungen. Die Multitaskingfähigkeiten beschränkten sich auf Anwendungen, die speziell für diese Version von MS-DOS entwickelt wurden; ansonsten konnte nur eine herkömmliche MS-DOS-Anwendung ausgeführt werden. Zudem mussten, da das Betriebssystem komplett im Real Mode lief, alle Anwendungen sich den konventionellen Speicher von 640 KB teilen. Aufgrund dieser Einschränkungen erschien diese Version von MS-DOS nicht im Handel, sondern wurde ausschließlich an bestimmte OEMs lizenziert.

Microsoft entwickelte das Betriebssystem anschließend weiter. Um die Einschränkungen der vorherigen Version zu beheben, sollte der Schutzmodus des 80286-Prozessors ausgenutzt werden. Microsoft unterzeichnete dazu einen Vertrag mit IBM, in dem beide Unternehmen sich einigten, das Betriebssystem, nun unter dem Namen CP/DOS bekannt, gemeinsam weiterzuentwickeln. Zum Ende der Entwicklung hin erfuhr das Betriebssystem eine erneute Namensänderung und hieß fortan OS/2.

Der 80286 hatte ein großes Problem: zwar konnte der Prozessor vom Real Mode in den Schutzmodus wechseln, der umgekehrte Weg, also vom Schutzmodus wieder zurück in den Real Mode zu wechseln, war jedoch nicht vorgesehen. Dies war allerdings notwendig, um weiterhin die Kompatibilität zu DOS-Anwendungen aufrechterhalten zu können, die nur im Real Mode lauffähig sind. Dieses Problem konnte erst gelöst werden, als ein Weg gefunden wurde, den Prozessor so zurückzusetzen, dass er sich wieder im Real Mode befand.

Am 2. April 1987 kündigten Microsoft und IBM das neue Betriebssystem offiziell an. Die erste Version sollte wie DOS textbasiert sein, eine spätere Version sollte dann eine von Windows abgeleitete grafische Benutzeroberfläche namens Presentation Manager beinhalten. Gleichzeitig kündigte Microsoft ein Entwicklungskit für das neue Betriebssystem an, welches für 3.000 US-Dollar erhältlich sein sollte. Dieses Entwicklungskit erschien zwei Monate später.

Früher als zunächst angekündigt veröffentlichte IBM die erste Version von OS/2 am 4. Dezember 1987 zum Preis von 325 US-Dollar. Knapp zwei Wochen später erschien auch Microsofts Version von OS/2. Während IBM OS/2 direkt vertrieb, lizenzierte Microsoft das Betriebssystem an OEMs weiter, die es mit ihren Computern auslieferten und entsprechend anpassten. Die Systemanforderungen des auf vier Disketten erschienenen Betriebssystems waren ein 80286-Prozessor mit mindestens 2 MB Arbeitsspeicher und 5 MB Festplattenspeicher. Die erste Version sah aufgrund der fehlenden grafischen Benutzeroberfläche DOS sehr ähnlich, darunter verbarg sich jedoch ein multitaskingfähiges Betriebssystem, das bis zu 12 OS/2-Sitzungen zur selben Zeit verwalten kann und bis zu 16 MB Arbeitsspeicher unterstützt. Daneben bot OS/2 eine weitere Sitzung, in der die meisten DOS-Anwendungen ausgeführt werden konnten. Im Juli 1988 brachte IBM die OS/2 Extended Edition zum Preis von 795 US-Dollar heraus, die zusätzliche Datenbank- und Kommunikationsprogramme enthielt.

Allerdings gab es auch Kritik. So sei es zwar angesichts der großen Zahl an Computern mit 80286-Prozessoren verständlich, dass OS/2 für diesen Prozessor entwickelt wurde, allerdings war seitdem bereits der 80386 erschienen, der zahlreiche Vorteile gegenüber dem 80286 bot. Außerdem fehle dem Betriebssystem ein leistungsfähiges Dateisystem - OS/2 benutzte weiterhin das aus DOS übernommene FAT16-Dateisystem, das keine langen Dateinamen unterstützte und in der damaligen Version auf eine maximale Partitionsgröße von 32 MB beschränkt war. Generell hieß es, dass OS/2 gegenüber DOS keine Vorteile böte, solange es keine Software für das neue Betriebssystem gäbe.

Am 31. Oktober 1988 erschien die Version 1.1 von OS/2, die erstmals die langersehnte grafische Benutzeroberfläche enthielt. Diese war jedoch nicht die einzige Neuheit: OS/2 unterstützte nun Partitionen, die größer als 32 MB sind. Zu den ersten Anwendungen für die neue Benutzeroberfläche zählte Borlands Softwarepaket SideKick, die von November an dem Betriebssystem beilag. Das nunmehr auf fünf Disketten angewachsene OS/2 1.1 hatte ähnliche Systemvoraussetzungen wie die vorherige Version, benötigte aber 8 MB an Festplattenspeicher. Einer flächendeckenden Verbreitung des Betriebssystems standen zu dieser Zeit hauptsächlich fehlende Treiber im Weg, vor allem Druckertreiber lagen dem Betriebssystem nur sehr spärlich bei.

OS/2 1.2 folgte im September 1989. Diese Version bot erstmals die Möglichkeit der sogenannten installierbaren Dateisysteme; die ähnlich wie Gerätetreiber beim Systemstart geladen werden und im Prinzip den Zugriff auf beliebige Dateisysteme ermöglichen können. Eines dieser installierbaren Dateisysteme, HPFS, ist im Lieferumfang von OS/2 1.2 enthalten: HPFS ist effizienter als das bisherige FAT-Dateisystem, zudem unterstützt es lange Dateinamen und Partitionen, die größer als 2 Gigabyte sind. Außerdem können beliebige Daten, sogenannte erweiterte Attribute, an Dateien angehängt werden. Allerdings gibt es keinerlei Abwärtskompatibilität; DOS kann auf HPFS-Partitionen nicht zugreifen und für DOS- und ältere OS/2-Anwendungen sind Dateien, die lange Dateinamen benutzen, nicht sichtbar. Außerdem enthält OS/2 1.2 eine überarbeitete Benutzeroberfläche sowie einige zusätzliche Anwendungen.

Die Version 1.3 von OS/2 kam im November 1990 heraus. Diese enthielt kaum Neuerungen, sondern war lediglich eine verbesserte Version von OS/2 1.2, die geringere Systemvoraussetzungen hatte und mehr Treiber beinhaltete. Die Resonanz dieser Version war jedoch gering.

Schon von Anfang an war die Beziehung zwischen den beiden Unternehmen angespannt. IBM und Microsoft waren zwei verschiedene Firmen, die unterschiedliche Ziele verfolgten. Microsoft wollte OS/2, ähnlich wie DOS und auch Windows, als Softwareplattform für viele verschiedene PCs verfügbar machen, IBM hingegen wollte mit OS/2 den Verkauf der eigenen Rechner, allen voran die PS/2-Produktreihe, ankurbeln. Auch die Entwickler beider Firmen waren häufig in gegenseitigen Konflikten verwickelt. IBM nutzte Lines of Code als Maßeinheit für die Produktivität von Entwicklern; Microsofts Entwickler schrieben hingegen kurzen und effizienten Code und galten dadurch aus IBMs Sicht als unproduktiv. Microsofts Entwickler wiederum beschwerten sich über die Bürokratie von Seiten IBMs. Microsoft wollte OS/2 zügig an den 80386-Prozessor anpassen, aber IBM hielt aufgrund von Kundenversprechen dagegen und propagierte weiterhin die 80286-Version.

Die Spannungen entluden sich schließlich, als Windows 3.0 erschien und ein großer Erfolg wurde. Innerhalb von vier Jahren verkaufte sich OS/2 nur 300.000 Mal, Windows 3.0 hingegen erreichte nach einem Jahr bereits 3 Millionen Kopien. Zunächst hieß es, dass IBM die alleinige Federführung an der nächsten Version von OS/2, die Version 2.0, erhalten sollte, während Microsoft die übernächste Version 3.0 entwickelte. Schließlich gab Microsoft die Entwicklung an OS/2 völlig auf und nutzte die Ressourcen zur Entwicklung eines neuen Betriebssystems mit der Bezeichnung Windows NT.

Begleitet von einer großen Werbekampagne veröffentlichte das nun auf sich alleine gestellte IBM OS/2 2.0 am 1. April 1992. Das Unternehmen bezeichnete OS/2 als . OS/2 war nun ein 32-Bit-Betriebssystem, das die Fähigkeiten des 80386 ausnutzte, wenngleich das Betriebssystem weiterhin große Teile an 16-Bit-Code enthielt. Das Betriebssystem konnte erstmals mehrere DOS-Anwendungen parallel ausführen und bot zudem die Möglichkeit, Windows-Anwendungen in einer speziellen "Win-OS/2-Umgebung" zu starten, die entweder direkt als Fenster in die OS/2-Umgebung eingebettet oder im Vollbild-Modus ausgeführt werden konnte. Der Vollbild-Modus war zwar schneller, aber wegen der fehlenden Unterstützung für Grafikkartentreiber auf 640×480 und 16 Farben beschränkt. Zudem unterstützte die Win-OS/2-Umgebung nur den Standard-Modus, was Anwendungen, die den 386-erweiterten Modus benötigten, ausschloss.

OS/2 2.0 ersetzte den Präsentations-Manager durch die neue Workplace Shell. Das Konzept der Workplace Shell folgt dem Paradigma, dass alles ein Objekt ist, wie etwa Icons, Fenster oder auch der Desktophintergrund, und durch Verfahren wie Drag and Drop angepasst werden kann. Die Benutzeroberfläche galt als äußerst flexibel, gleichzeitig aber als schwierig zu erlernen. Die Workplace Shell merkte sich zudem die offenen Anwendungen beim Herunterfahren und startete diese wieder, sobald das Betriebssystem wieder hochfuhr.

Am 18. Mai 1993 folgte die Version 2.1 von OS/2. Diese Version bot erstmals eine Multimedia-Unterstützung sowie zahlreiche Treiber vor allem für SCSI-Geräte, Grafikkarten, CD-ROM-Laufwerke und PCMCIA-Geräte. Außerdem wurde die Win-OS/2-Umgebung auf den Stand von Windows 3.1 gebracht und unterstützt im Gegensatz zu OS/2 2.0 den 386-erweiterten Modus. Durch verschiedene Optimierungen, allen voran die Portierung der Grafikfunktionen auf 32 Bit, konnte die Leistung im Vergleich zur Version 2.0 deutlich gesteigert werden. Im Januar 1994 folgte "OS/2 for Windows", eine Version von OS/2, der die Win-OS/2-Umgebung fehlte und die stattdessen ein bereits auf dem Rechner installiertes Windows 3.1 benutzte, um Windowsprogramme innerhalb von OS/2 auszuführen. Diese Version war mit 149 US-Dollar günstiger als das reguläre OS/2, das 249 US-Dollar kostete, und war prinzipiell auch ohne Windows verwendbar – dann jedoch ohne Unterstützung für Windowsprogramme. Im Juli 1994 kam "OS/2 2.11 SMP" heraus, die erste Version von OS/2 mit Unterstützung für Mehrprozessorsysteme mit bis zu 16 Prozessoren.

Die Veröffentlichung von OS/2 2.1 stand ganz im Zeichen der wachsenden Konkurrenz vor allem durch das sich in Entwicklung befindliche Windows NT des Konkurrenten Microsoft, aber auch durch grafische Unix-Systeme. OS/2 2.x hatte den Zeitvorteil, da es bereits ein Jahr lang auf dem Markt war und dadurch bereits mit 32-Bit-Anwendungen aufwarten konnte. Allerdings lief das Betriebssystem nur auf dem x86-Prozessor und zeigte erhebliche Defizite sowohl im Sicherheits- als auch im Netzwerkbereich.

IBM kündigte an, an einer neuen Version von OS/2 unter dem Namen "Personal OS/2" zu arbeiten. Diese sollte einen geringeren Speicherverbrauch aufweisen und so direkt mit Windows 3.1 konkurrieren. Aus Personal OS/2 wurde zunächst OS/2 3.0 und im Oktober 1994 erschien das Betriebssystem unter der Bezeichnung "OS/2 Warp". OS/2 Warp erschien zunächst, ähnlich wie OS/2 2.1 zuvor, in zwei Versionen, die aufgrund ihrer Verpackung auch "Blue Spine" und "Red Spine" genannt wurden: "Blue Spine" enthielt wie in vorherigen Versionen die Win-OS/2-Umgebung, während "Red Spine" ein bereits installiertes Windows verwendete.

OS/2 Warp enthielt hauptsächlich einige Verbesserungen der grafischen Benutzeroberfläche, unter anderem das "LaunchPad" zum schnelleren Start von Programmen. Die größte Neuerung war jedoch das beiliegende "BonusPak", mit dem IBM dem Mangel an Anwendungen für OS/2 begegnete. Darunter befanden sich etwa "IBM Works", eine Sammlung von Büroanwendungen ähnlich dem Konkurrenzprodukt Microsoft Works sowie eine Sammlung an Internetprogrammen, die eine einfache Möglichkeit boten, sich über ein Modem ins Internet einzuwählen und es mittels der beiliegenden Programme wie eines Webbrowsers zu benutzen. Im Mai 1995 folgte OS/2 Warp Connect, welches ähnlich wie die normale Version von OS/2 Warp jeweils als "Blue Spine" und "Red Spine" erschien. Diese Version von OS/2 bot gegenüber der normalen Version von OS/2 zusätzliche Netzwerkfunktionen, wie TCP/IP-Unterstützung, einen Netware-Client, eine Datei- und Druckerfreigabe sowie einen Remoteclient ähnlich dem Remote Access Service unter Windows.

Am 26. Februar 1996 erschien OS/2 Warp Server 4.0 – trotz des Namens eine Version von OS/2 Warp 3.0 – und trat die Nachfolge des vormals separaten Produkts LAN Server an. Dieser bietet unter anderem einen DHCP-Server und dynamisches DNS. Zudem integrierte IBM zahlreiche Produkte, die zuvor separat bezogen werden mussten, in das Betriebssystem. Dazu zählt etwa das Systemverwaltungsprogramm "SystemView", mit dem sowohl der Server als auch die angeschlossenen Clients verwaltet werden können, eine Backuplösung namens "Personally Safe and Sound", sowie einen Replikationsdienst für Laptops. OS/2 Warp Server 4.0 erschien sowohl in einer normalen Version zum Preis von 629 US-Dollar als auch in der Advanced-Version für 1299 US-Dollar; letztere unterstützte mehr Clients, das Dateisystem HPFS386, softwareseitiges RAID sowie Disk Quotas. Im September 1996 folgte eine Version von OS/2 Warp Server 4.0 Advanced mit SMP-Unterstützung.

Im Jahr 1995 verkaufte IBM 5,4 Millionen Kopien von OS/2, davon 2,1 Millionen zusammen mit IBM-Computern, 2,2 Millionen im Handel und 1,1 Millionen über OEMs.

Im September 1995 kündigte IBM eine neue Version von OS/2 mit dem Codenamen "Merlin" an, die im März 1996 erscheinen sollte. Dieser Termin verzögerte sich jedoch und so startete IBM erst am 13. Juni den Betatest von Merlin, bei dem eine Vorversion des Betriebssystems an 10.000 Tester herausgegeben wurde. Zeitweise plante IBM, Unterstützung für 32-Bit-Windowsanwendungen in Merlin zu implementieren, sah aber von diesem Vorhaben ab. Nach einem weiteren Betatest im August stellte IBM OS/2 Warp 4.0 am 25. September 1996 vor. OS/2 Warp 4.0 enthielt eine komplett überarbeitete Benutzeroberfläche. Über das "WarpCenter", das das "LaunchPad" ersetzt, können Programme über ein Drop-Down-Menü gestartet werden, ähnlich wie beim Startmenü unter Windows 95. Mit der Funktion "VoiceType" kann das Betriebssystem über eine Spracheingabe gesteuert werden. Außerdem enthielt das Betriebssystem eine Java-Runtime sowie das zugehörige Java Development Kit. OS/2 Warp 4.0 implementierte erstmals OpenGL und enthielt zudem "Open32", das die Portierung von 32-Bit-Windowsanwendungen auf OS/2 vereinfachen sollte. Im Gegensatz zur vorherigen Version gab es keine unterschiedlichen Varianten von OS/2 Warp 4.0; das Betriebssystem enthielt sowohl die Netzwerkfunktionen als auch die Win-OS/2-Umgebung.

Vor allem Firmen kritisierten diese Version von OS/2. Die neuen Funktionen dieser Version von OS/2 seien hauptsächlich für Heimanwender interessant, für Unternehmen bedeuteten sie lediglich zusätzliche Schulungskosten. Als OS/2 Warp 4 erschien, benutzten die meisten Unternehmen noch die Version 2.11, einige hatten erst die Migration auf den Vorgänger OS/2 Warp abgeschlossen. Gleichzeitig mehrten sich die Anzeichen, dass IBM sein Engagement an OS/2 aufgeben würde, auch wenn das Unternehmen zusicherte, das Betriebssystem noch mindestens zehn Jahre zu unterstützen. Im November 1997 erschien mit "WorkSpace on-demand" eine OS/2-basierte Thin-Client-Lösung.

Im Mai 1999 veröffentlichte IBM "OS/2 Warp Server for e-business". Diese Version von OS/2, die wie die vorherige Version symmetrisches Multiprocessing mit bis zu vier Prozessoren unterstützt, enthält ein neues, stabileres Dateisystem namens Journaled File System, Unterstützung für das Jahr 2000 und das Eurozeichen. Netzwerkmäßig bietet das Betriebssystem eine Portierung des Apache Web Servers unter dem Namen "Domino Go" sowie "WebSphere", ein Anwendungsserver mit Unterstützung für JavaServer Pages. Mit einem Preis von 1699 US-Dollar galt diese Version von OS/2 als stark überteuert und allerhöchstens für bestehende OS/2-Nutzer interessant.

Im Jahr 2000 gab IBM bekannt, ein "Convenience Pack" jeweils für Warp 4 und Warp Server zu veröffentlichen. Dieses Convenience Pack enthielt eine bootfähige Installations-CD mit allen bisher erschienen Aktualisierungen. Dies sollte die Installation von OS/2 vor allem auf neueren Systemen vereinfachen, für das bisher eine zeitraubende Installation von Fixpacks, Gerätetreibern und aktueller Software notwendig war. Das Convenience Pack war lediglich über ein Abonnement bei IBM erhältlich. Gleichzeitig arbeitete das Unternehmen "Serenity Systems" an einer Weiterentwicklung von OS/2 Warp 4 unter dem Namen eComStation, dessen erste Version 2001 erhältlich war.

Schließlich kündigte IBM im Jahr 2005 an, die Unterstützung von OS/2 zum Ende des Jahres 2006 einzustellen. Den Kunden empfahl das Unternehmen einen Umstieg auf Linux. Das unter der Federführung von Serenity Systems entwickelte eComStation hingegen wird bis heute unterstützt. Es finden nach wie vor Veranstaltungen für Entwickler und Anwender statt, wie zum Beispiel die Warpstock 2014 im Oktober 2014 in St. Louis (USA), ein österreichisches Unternehmen stellte im Januar 2015 eine Portierung von OpenOffice auf OS/2 vor.

1991 plante IBM ein neues Projekt unter dem Namen "Workplace OS". Basierend auf dem Mach-Kernel sollte das Workplace OS in der Lage sein, auf Basis des PowerPC-Prozessors mehrere Betriebssysteme, wie OS/2, AIX und Mac OS, auszuführen, und das auf verschiedenen Geräten, wie Desktop-PCs oder PDAs. Doch Apple stieg aus der Entwicklung aus und die Portierung von AIX auf Workplace OS stellte sich als unpraktikabel heraus, da das Betriebssystem aufgrund von Leistungseinbußen hinter den Konkurrenzprodukten SunOS und HP-UX zurückfallen würde. Zwar arbeitete IBM weiter an Workplace OS, doch im Oktober 1995 stellte das Unternehmen das Produkt ein, nachdem schwere Unzulänglichkeiten im "PowerPC 620"-Prozessor bekannt wurden.

Stattdessen begann IBM unabhängig von Workplace OS die Portierung von OS/2 auf den PowerPC-Prozessor. 1994 kündigte das Unternehmen an, OS/2 für PowerPC bis zum Jahresende fertigzustellen. Die PowerPC-Version würde wie die x86-Version von OS/2 DOS- und Windowsprogramme unterstützen, dafür aber nicht mit alten 16-Bit-OS/2-Anwendungen kompatibel sein. Ende des Jahres veröffentlichte IBM eine erste Vorversion des Produkts. Der Veröffentlichungstermin musste mehrmals verschoben werden, und erst zwei Jahre später, im Januar 1996, bot IBM OS/2 für PowerPC an, allerdings nur auf Anfrage für Kunden, die das Produkt vor seiner Veröffentlichung testen wollten. Das Produkt war sehr unvollständig; obwohl es offiziell als "OS/2 Warp Connect, PowerPC edition" bezeichnet wurde, gab es keinerlei Netzwerkfunktionalität. Nur einen Monat später entschied IBM, die Entwicklung von OS/2 für PowerPC einzustellen und das Produkt nicht im Handel anzubieten.

Mittlerweile wird OS/2 im Heim-Bereich wegen des geringeren Angebots aktueller Software kaum noch eingesetzt, und auch bei Banken, Versicherungen und Fluggesellschaften ist die Nutzung rückläufig. Neue Installationen werden meistens mit der eComStation-Distribution realisiert. Daneben findet es noch im Bereich der Haustechnik und der Sicherheitstechnik Verwendung. Außerdem füllt es eine Nische in der Fertigungsindustrie aus.

Bisher blieben OS/2 und die eComStation (eCS) praktisch von Gefahren wie Viren, Trojanern und Würmern verschont. Dies ist allerdings weniger auf die Systemarchitektur als mehr auf den geringen Marktanteil zurückzuführen.

OS/2 konnte neben speziell für OS/2 entwickelten Programmen auch in mehreren virtuellen Maschinen MS-DOS-Programme und mittels WinOS/2 Windows-3.1-Programme ausführen. Mit Win32s und vor allem dem Projekt Odin ist es möglich, einige Win32-Programme innerhalb der OS/2-Umgebung zu nutzen. Über Virtual PC, das vor der Übernahme von Connectix durch Microsoft auch für OS/2 verfügbar war, oder Bochs lassen sich auch komplette Win32-Umgebungen starten.

Für die einfache Überführung von Windows in OS/2-Anwendungen existiert die Schnittstelle "Open32". Zudem gibt es Bibliotheken und Entwicklungswerkzeuge, welche die Portierung von Unix-Anwendungen unterstützen.

Das Betriebssystem scheiterte am PC-Markt und brachte nicht nur IBM große wirtschaftliche Verluste ein, sondern auch den meisten Unternehmen, die IBM unterstützten und Anwendungen für OS/2 entwickelten.

Letzten Endes verlor IBM unter anderem durch den Misserfolg von OS/2 seine Vormachtstellung am PC-Markt und musste diesen Markt, einst von dem Unternehmen begründet, weitestgehend an die Konkurrenz abtreten.

Trotz des großen Interesses an dem neuen Betriebssystem sollte sich OS/2 zunächst als Fehlschlag erweisen.

Ein großer Fehler von IBM war die zunächst fehlende grafische Benutzeroberfläche. Seit dem Erfolg des Macintosh im Jahr 1984 stieg das Interesse an grafischen Benutzeroberflächen stark an; kommandozeilenbasierte Betriebssysteme galten als nicht mehr zeitgemäß. Bereits zuvor scheiterte IBM mit TopView am Versuch, ein kommandozeilenbasiertes Betriebssystem auf dem Markt zu etablieren. Die Entwicklung an der grafischen Oberfläche von OS/2 verzögerte sich, da IBM sich entschied, die Systems Application Architecture zu implementieren, um die Entwicklung von Anwendungen für das gesamte Portfolio von IBM-Rechnern, vom Großrechner bis zum PC, zu vereinheitlichen. Das Konzept scheiterte letztendlich und die Verzögerungen sorgten dafür, dass die grafische Benutzeroberfläche erst zu einem späteren Zeitpunkt fertiggestellt werden konnte und das Betriebssystem vorerst nur mit einer kommandozeilenbasierten Oberfläche ausgestattet wurde. Somit wiederholte das Unternehmen genau denselben Fehler.

Der Name „OS/2“ stellte sich als äußerst ungünstig heraus, da viele potentielle Kunden dachten, das Betriebssystem würde nur auf PS/2-Rechnern laufen. Zudem schreckte die Preispolitik von IBM potentielle Kunden ab. OS/2 war weit teurer als DOS und auch Windows, sodass nur wenige das Betriebssystem im Handel kauften. Der Preis für das Entwicklungskit von 3000 US-Dollar war für viele kleinere Entwicklungsunternehmen zu teuer und stand in keinem Verhältnis zu vergleichbaren Entwicklungskits für Windows oder den Macintosh.

Auf einen weiteren Faktor hatte IBM keinen Einfluss: Genau zur Veröffentlichung von OS/2 kam es aufgrund von Produktionsschwierigkeiten zu einem Lieferengpass bei Speicherbausteinen, der den Preis von Arbeitsspeicher auf das Vierfache ansteigen ließ. Zwei Jahre lang hielt sich dieses Preisniveau. Dies machte das Aufrüsten eines PCs für OS/2 zu einem äußerst kostspieligen Unterfangen.

Als sich herauskristallisierte, dass Windows zu einem großen Erfolg werden würde, gab es einige Pläne von IBM-Entwicklern, um Microsoft doch noch aufzuhalten. Einer dieser Pläne war, die grafische Benutzeroberfläche GEOS der Firma GeoWorks aufzukaufen. GEOS war sehr ressourcenschonend und lief selbst auf dem originalen IBM-PC in einer akzeptablen Geschwindigkeit. Ein anderer Plan war, den Presentation Manager auf DOS zu portieren und so auf den OS/2-Unterbau zu verzichten. Das Unternehmen entschied sich jedoch, diese Pläne abzuweisen und stattdessen auf vollen Konfrontationskurs mit Microsoft zu gehen, und drohte damit, Windows nicht länger zu vermarkten und zu unterstützen, sollte Microsoft nicht seine Rechte an Windows an IBM abgeben. Bill Gates entschied sich, nicht auf die Drohung einzugehen, und so verlor IBM seinen letzten Einfluss auf Microsoft.

Mit OS/2 2.0 versuchte IBM nun alleine, gegen die Übermacht von Microsoft und seinem Betriebssystem Windows anzukämpfen. Das Betriebssystem litt weiterhin an einem Mangel an Anwendungsprogrammen, da sich IBM nicht bemühte, Entwickler anzuwerben oder sie zu unterstützen. Eine in diesem Licht getroffene Entscheidung war Win-OS/2, mit der Windows-Anwendungen auch auf OS/2 ausgeführt werden konnten. Zwar erhielt OS/2 dadurch kurzfristig einen Vorteil, langfristig jedoch sank dadurch der Anreiz für Entwickler, spezielle OS/2-Anwendungen zu entwickeln, da Windows-Anwendungen ohnehin auch auf OS/2 lauffähig waren.

Eines der größten Probleme war, dass IBM nun versuchen musste, das Betriebssystem selber zu vermarkten. Als ein typisches Großunternehmen vermarktete IBM seine Produkte zu einem großen Teil an andere Unternehmen und wusste so im B2B-Bereich zu bestehen. IBM hatte jedoch keinerlei Erfahrung damit, Produkte an Endanwender zu verkaufen, und beging dadurch zahlreiche große Fehler. So sponserte IBM jahrelang den Fiesta Bowl des amerikanischen College Footballs mit dem Großteil der für OS/2 veranschlagten Werbekosten, ohne dass es einen erkennbaren Zusammenhang zwischen College-Football-Fans und dem Betriebssystem gab.

Mit OS/2 Warp versuchte sich IBM an einer neuen Marketingkampagne. Schon zuvor nutzte das Unternehmen intern Begriffe aus dem Star-Trek-Universum als Codenamen und so wollte IBM das Betriebssystem mithilfe von Darstellern aus der Serie offiziell veröffentlichen. Jedoch vergaß das Unternehmen dabei, sich die notwendigen Rechte von Paramount Pictures zu sichern. Paramount drohte mit einer Klage und so musste IBM die geplante Werbekampagne fallen lassen. Den Begriff "Warp" konnte das Unternehmen zwar weiter nutzen, aber nur in einer seiner anderen Bedeutungen, die ein eher schlechtes Licht auf das Produkt warfen.

Als IBM bemerkte, dass erste Entwickler Spiele für das Betriebssystem entwickelten und diese eine große Resonanz entwickelten, fasste das Unternehmen den Beschluss, Warp für jugendliche Computerfreaks zu bewerben. Damit stand IBM jedoch im Konflikt zu den bisherigen Kunden des Betriebssystems, die größtenteils Unternehmen waren und ganz andere Anforderungen an das Betriebssystem stellten. Dazu kamen die vor allem in den USA ausgestrahlten und von allen Seiten stark kritisierten Werbevideos, die die Stärken des Betriebssystems überhaupt nicht darstellten, dazu zählte etwa ein Werbevideo, das Nonnen in einem tschechischen Kloster zeigt. Vor allem der technische Support war auf den Ansturm von Endkunden nicht vorbereitet, sodass IBM große Summen an Geld ausgeben musste, um triviale Probleme von Endkunden zu lösen, und das trotz der Marge des Betriebssystems, die vor allem aufgrund von Lizenzzahlungen an Microsoft gering ausfiel.

Innerhalb des Entwicklerteams bei IBM herrschte eine hohe Fluktuation, sodass es zuletzt nur noch wenige Entwickler gab, die sich mit dem Quellcode von OS/2, der zu großen Teilen noch in Assemblersprache geschrieben war, auskannten. Dadurch blieben zahlreiche Fehler des Betriebssystems bis zuletzt ungelöst. Am bekanntesten ist dabei ein konzeptuelles Problem des Presentation Managers, der nur eine einzige synchrone Eingabewarteschlange ("Synchronous Input Queue") besaß. Dadurch konnte ein fehlerhaftes Programm die gesamte grafische Benutzeroberfläche blockieren und so das Betriebssystem unbenutzbar machen.

Schließlich trug auch das Scheitern von OS/2 für PowerPC zum Untergang des Betriebssystems bei. Der Erfolg des Projekts hing die ganze Zeit von den PowerPC-Prozessoren ab, und nachdem diese nicht die erhoffte Leistung zeigten und kaum besser waren als vergleichbare Intel-Prozessoren, war OS/2 für PowerPC zum Scheitern verurteilt. Dabei band das Projekt wertvolle Ressourcen, die vor allem im Angesicht des sich in Entwicklung befindlichen Windows 95 besser dazu hätten genutzt werden können, die x86-Version zu verbessern. Eine viel fatalere Folge war jedoch, dass IBM das Vertrauen in das OS/2-Entwicklerteam verlor und ihm deshalb weitere Ressourcen entzog, weil es nicht in der Lage war, OS/2 für PowerPC rechtzeitig fertigzustellen.

Aber auch Microsoft unternahm Versuche, die Verbreitung von OS/2 zu verhindern. Als die deutschen Computerhändler Vobis und Escom ankündigten, auf ihren PCs zukünftig OS/2 vorzuinstallieren und Windows nur noch gegen Aufpreis anzubieten, übte Microsoft massiven Druck auf die beiden Computerhändler aus. So schloss Microsoft Vobis vom Beta-Programm von Windows 95 aus, bot für die Zukunft Windows-Lizenzen nur zu wesentlich schlechteren Bedingungen an und versuchte, Vobis dazu zu zwingen, eine Verschwiegenheitserklärung zu unterzeichnen. Ebenso weigerte sich Microsoft zunächst, Windows 95 an IBM zu lizenzieren, und verlangte von dem Unternehmen, OS/2 sofort einzustellen, um Lizenzen für Windows 95 zu erhalten. Erst am Tag der Veröffentlichung von Windows 95 erhielt IBM die Lizenzen von Microsoft.

Der endgültige Schlag traf OS/2, als Lou Gerstner, der CEO von IBM, in einem Interview bekanntgab, dass OS/2 seinen letzten Kampf führe und dass es zu spät für IBM sei. Dies interpretieren viele Entwickler dahingehend, dass IBM das Betriebssystem OS/2 aufgeben würde. Später gab ein IBM-Pressesprecher bekannt, dass er zuhause Windows 95 installieren werde und OS/2 in diesem Zusammenhang mit Sonys Betamax verglich, was die öffentliche Meinung von OS/2 zusätzlich traf. IBM unternahm nichts, um diese Aussagen richtigzustellen und so wendeten sich zahlreiche Entwickler und Kunden von OS/2 ab. In dem Moment galt OS/2 als gescheitert und auch IBM hatte keinen klaren Plan für OS/2 Warp 4, sodass diese Version kaum Marktanteile erzielen konnte.

Ende 1996 schloss IBM schließlich das Entwicklungsstudio in Boca Raton in Florida, das für die Entwicklung von OS/2 verantwortlich war.







</doc>
<doc id="10198" url="https://de.wikipedia.org/wiki?curid=10198" title="Dokumenttypdefinition">
Dokumenttypdefinition

Eine Dokumenttypdefinition (), abgekürzt DTD, ist ein Satz an Regeln, der benutzt wird, um Dokumente eines bestimmten Typs zu deklarieren. Ein Dokumenttyp ist dabei eine Klasse ähnlicher Dokumente, wie beispielsweise Telefonbücher oder Inventurdatensätze. Die Dokumenttypdefinition besteht dabei aus Elementtypen, Attributen von Elementen, Entitäten und Notationen. Konkret heißt das, dass in einer DTD die Reihenfolge, die Verschachtelung der Elemente und die Art des Inhalts von Attributen festgelegt wird, also die Struktur des Dokuments.

Eine DTD spezifiziert die Syntax einer Anwendung von SGML oder XML, wie beispielsweise die von ihnen abgeleiteten Sprachen HTML oder XHTML. Diese Syntax ist normalerweise in einer weniger generellen Form gehalten als die SGML- oder XML-Syntax.

Die Syntax und Semantik einer DTD ist Bestandteil der SGML- und XML-Spezifikation. Die DTD wird bereits beim Lesen des Dokumentes geprüft. In SGML muss jedes Dokument zwingend einer DTD zugeordnet sein. In XML ist diese Zuordnung optional. Ein Dokument auf Basis von DTD (codice_1) gewährleistet die Richtigkeit der angewendeten DTD, also der vorgesehenen Syntax und Semantik. Die Korrektheit der Daten steht hier im Vordergrund. Ein Dokument ohne extern referenzierte DTD oder eingeschlossene DTD wird beim Lesen nur nach der Wohlgeformtheit geprüft. Die schnelle Lesbarkeit steht hier im Vordergrund. Der Inhalt kann von der gewünschten Syntax und Semantik abweichen. Für beide Fälle kann mit zusätzlichen Prozessen die Qualität der Daten nachträglich geprüft werden.

Hinweis: Im Folgenden werden die Beispiele jeweils in XML-Syntax angegeben.

Eine Dokumenttypdeklaration stellt die Verbindung zwischen einem Dokument und der DTD her. Die Dokumenttypdeklaration wird am Beginn eines Dokumentes vor dem Wurzelelement angegeben. Die DTD kann als externe Datei referenziert werden (externe DTD) oder aber auch direkt im Dokument eingebunden werden (interne DTD).

Die Syntax für eine Dokumenttypdeklaration lautet in SGML und XML:
<!DOCTYPE Wurzelelement SYSTEM "datei.dtd">
<!DOCTYPE Wurzelelement SYSTEM "datei.dtd" [ … ]>
<!DOCTYPE Wurzelelement PUBLIC "Public Identifier" "datei.dtd">
<!DOCTYPE Wurzelelement PUBLIC "Public Identifier" "datei.dtd" [ … ]>
<!DOCTYPE Wurzelelement [ … ]>
In SGML sind zusätzlich auch folgende Varianten ohne System-Identifikator zulässig:
<!DOCTYPE Wurzelelement PUBLIC "Public Identifier">
<!DOCTYPE Wurzelelement PUBLIC "Public Identifier" [ … ]>
In HTML5 gibt es zwar keine DTD mehr, die Dokumenttypdeklaration existiert aber in einer verkürzten Form weiter:
<!DOCTYPE html>
Der System-Identifikator (SYSTEM), im Beispiel der Eintrag codice_2, enthält den Dateinamen der externe DTD. Der Dateiname kann als beliebiger URI angegeben werden.

Der Public-Identifikator (PUBLIC) enthält einen öffentlich bekannten Identifikator für die DTD. Zum Beispiel wird mit dem Identifikator codice_3 die DTD für XHTML eindeutig deklariert. Wenn der Identifikator dem System bekannt ist, verwendet das System die damit assoziierte DTD und lädt nicht die direkt angegebene DTD aus dem System-Identifikator. In Web-Browsern wird so das wiederholte Laden der DTD vermieden.

Der Eintrag codice_4 kennzeichnet eine interne DTD oder Ergänzungen zu einer DTD.

Innerhalb einer DTD kann die Dokumentstruktur mit Deklarationen von "Elementtypen", "Attributlisten", "Entities" und "Notationen" und Textblöcken definiert werden. Dabei können spezielle "Parameter-Entities" benutzt werden, die DTD-Teile enthalten und nur innerhalb der DTD erlaubt sind.

Textblöcke sind entweder CDATA oder PCDATA.

Die Strukturelemente "(Building Blocks)" werden über Attribut-Zuordnungen definiert:

CDATA (englisch "Character Data") kennzeichnet einen ungeparsten Textblock. Die Syntax für einen CDATA-Bereich lautet:
<![CDATA[Zeichendaten]]>
Im Bereich Zeichendaten sind alle Zeichen erlaubt, ausgenommen das Erkennungsmuster für das Ende codice_5 des CDATA-Bereiches. Beispiel:
<![CDATA[<Diese><Daten>werden<nicht><geparst>.]]>
Innerhalb einer Entitätendefinition in XML lautet die Syntax:
<!ENTITY amp "Zeichendaten">
In SGML muss das Schlüsselwort CDATA explizit angegeben werden:
<!ENTITY amp CDATA "Zeichendaten">
Im Bereich Zeichendaten sind alle Zeichen erlaubt, ausgenommen der Endekennung der Zeichenkette codice_6 bei codice_7-Zeichenketten oder codice_8 bei codice_9-Zeichenketten. Beispiel:
<!ENTITY amp CDATA "&#38;">
Die Zeichendaten codice_10 werden vom Parser nicht analysiert.

Für PCDATA (englisch "Parsed Character Data") wird das Schlüsselwort #PCDATA verwendet. Damit wird ein Textblock, der auch weitere Anweisungen an den Parser enthalten kann gekennzeichnet. Der Inhalt dieses Textblocks wird syntaktisch vom Parser analysiert. Im Gegensatz zu CDATA dürfen nur Zeichen enthalten sein, die keine Tags, Deklarationen oder Verarbeitungsanweisungen einleitet. Verboten sind hier beispielsweise codice_11 (z. B. Startzeichen eines Tags, z. B. codice_12). 

Mit einer Elementtyp-Deklaration wird ein Element und sein möglicher Inhalt definiert. In einem validen Dokument dürfen nur Elemente vorkommen, die in der DTD definiert sind.

Der Inhalt eines Elementes kann durch die Angabe anderer Elementnamen und durch einige Schlüsselwörter und Zeichen angegeben werden.

<!ELEMENT html (head, body)>
<!ELEMENT hr EMPTY>
<!ELEMENT div (#PCDATA | p | ul | ol | dl | table | pre | hr |
<!ELEMENT dl (dt|dd)+>
Attribute werden innerhalb einer Attributliste codice_22 definiert. Die Attributliste enthält durch Leerzeichen oder Zeilenumbrüche getrennt jeweils den "Attributnamen", den "Typ" und "Vorgaben" der einzelnen Attribute.

Beispiele für Elemente:

Mit den "Attribut-Vorgaben" kann angegeben werden, ob ein Attribut vorkommen muss (codice_23) oder nicht (codice_24) oder einen festen Wert enthält (codice_25) und welcher Wert als Standardwert benutzt wird, falls das Attribut bei einem Tag nicht angegeben wird.
Beispiel einer Attribut-Deklaration:
<!ATTLIST img

Eine "Entität" ist eine benannte Abkürzung für eine Zeichenkette oder ein externes Dokument, die innerhalb der DTD oder des Dokumentes, verwendet werden kann. Eine "Entität" der Form codice_26 wird dabei durch den deklarierten Inhalt der "Entity" ersetzt. "(Zur allgemeinen Verwendung siehe Entität (Auszeichnungssprache).)"

"Entitäten" bestehen aus Zeichenketten. Diese können selber wieder "Entitäten" und wohlgeformtes Markup enthalten:

<!ENTITY name "Benedikt">
<!ENTITY papst "&name;, der XVI.">
<!ENTITY wplink "<a href='http://de.wikipedia.org'>Wikipedia</a>">
"Entitäten" können auch für Inhalt einer Datei definiert werden. Dabei wird ein "Public"- oder "System-Identifier" benutzt.

<!ENTITY kapitel1 SYSTEM "kapitel1.xml">
<!ENTITY wichtig PUBLIC "-//privat//WICHTIG//" "wichtig.xml">
Bei externen "Entities" kann zusätzlich angegeben werden, dass es sich um eine "nicht analysierte Entität" (NDATA, Nicht-XML/SGML-Daten) handelt. In diesem Fall muss eine "Notation" angegeben werden (hier "gif").
<!ENTITY bild SYSTEM "../grafiken/bild.gif" NDATA gif>
Notationen sind Hinweise zur Interpretation von externen Daten, die nicht direkt vom Parser verarbeitet werden. Notationen können sich beispielsweise auf ein Dateiformat für Bilder beziehen.
<!NOTATION Datentyp SYSTEM "URL">
<!NOTATION Datentyp PUBLIC "Identifikator">
NMTOKEN ("name token") ist mit einem Bezeichner verwandt, geht jedoch freizügiger mit den Regeln zur Namensgebung um. So sind bei einem NMTOKEN Bezeichner mit führender Ziffer oder führendem Punkt erlaubt, wohingegen bei einem Bezeichner nur Buchstaben, Ideographen und "underlines" an erster Stelle erlaubt sind. Somit ist jeder Bezeichner auch ein NMTOKEN, jedoch nicht umgekehrt.
Beispiele für NMTOKEN:

12alpha
.crc

Deklarations-Beispiel:

<!ATTLIST birthdate

Parameter-Entitäten enthalten eine benannte Zeichenkette, die mittels codice_27 an fast allen Stellen innerhalb einer DTD eingesetzt werden kann. Auf diese Weise lassen sich beispielsweise externe Dateien in eine DTD einbinden und mehrfach vorkommende Bestandteile abkürzen. Parameter-Entitäten werden wie normale Entities deklariert, wobei vor dem Elementnamen ein einzelnes Prozentzeichen steht. Beispiel:
<!ENTITY % datei SYSTEM "andere-datei.ent">
%datei;

<!ENTITY % foo.inhalt "(bar|doz)*">
<!ELEMENT foo %foo.inhalt;>
Ein bedingter Abschnitt ist ein Konstrukt, um Deklarationen ein- oder auszuschalten. Beispiel:
<![INCLUDE[

Schaltet die Deklaration von codice_28 ein. Entsprechend gilt:
<![IGNORE[

um codice_28 auszuschalten.

Man verwendet bedingte Abschnitte wie oben jedoch nicht allein, sondern meist im Zusammenspiel mit Parameter-Entities:
<!ENTITY % weiche "INCLUDE">
<![%weiche;[

Das Parameter-Entity codice_30 ist durch eine der möglichen Schlüsselwörter codice_31 oder codice_32 belegt. Je nach Belegung wird das Entity codice_28 deklariert oder nicht.

Durch diese Art der Schreibweise kann ein bedingter Abschnitt durch Überschreibung von Parameter-Entities angepasst werden.

Beispiel eines kurzen Dokumentes mit Verweis auf eine externe DTD:

<?xml version="1.0"?>
<!DOCTYPE hallo SYSTEM "hallo.dtd">
<hallo>Hallo Welt!</hallo>
In der XML-Deklaration kann im Beispiel auch das Pseudoattribut codice_34 angegeben werden (d. h., es wird eine externe DTD benötigt):

<?xml version="1.0" standalone="no"?>
<!DOCTYPE hallo SYSTEM "hallo.dtd">
<hallo>Hallo Welt!</hallo>
Der Inhalt von codice_35:

<!ELEMENT hallo (#PCDATA)>
Kurzes Dokument mit interner DTD:

<?xml version="1.0"?>
<!DOCTYPE hallo [<!ELEMENT hallo (#PCDATA)>]>
<hallo>Hallo Welt!</hallo>
In der XML-Deklaration kann im Beispiel auch das Pseudoattribut codice_36 angegeben werden (d. h., es wird keine externe DTD benötigt):
<?xml version="1.0" standalone="yes"?>
<!DOCTYPE hallo [<!ELEMENT hallo (#PCDATA)>]>
<hallo>Hallo Welt!</hallo>



</doc>
<doc id="10201" url="https://de.wikipedia.org/wiki?curid=10201" title="Jupiter (Mythologie)">
Jupiter (Mythologie)

Jupiter (lateinisch Iuppiter, seltener "Iupiter" oder "Juppiter"; Genitiv: "Iovis", seltener "Jovis") ist der Name der obersten Gottheit der römischen Religion. Eine ältere Namensform ist "Diēspiter". Er wurde oft als Iuppiter Optimus Maximus bezeichnet („bester und größter Jupiter“), in Inschriften meist abgekürzt zu IOM. Jupiter ist die römische Form des griechischen Zeus, des „Himmelsvaters“.

Der ältere Name "Diēspiter" setzt sich aus "dieis" (lateinisch "dies" „Tag“) und "pater" (lateinisch „Vater“) zusammen (vgl. auch altindisch "Dyaus pitar") und bedeutet demnach „Himmelsvater“. Das "iu" in "Iuppiter" und der Name "Zeus" gehen auf dieselbe indoeuropäische Wurzel "*diu" für „hell“ zurück, welche die Haupteigenschaft Jupiters (bzw. Zeus’) als alter Himmels- und Wettergott bezeichnet, der auch als Lichtbringer verstanden wurde. Entsprechend ist eine Nebenbedeutung von "Iuppiter" auch einfach „Himmel“ oder „Luft“; "sub Iove" bedeutet dementsprechend „unter offenem Himmel“ oder „im Freien“.

Die Aussprache wandelte sich um Christi Geburt von [] zu []. Der Name kommt auf römischen Inschriften hauptsächlich abgekürzt vor; wurde er ausgeschrieben, so schrieb man in der Antike, je nach Aussprache, ivpiter oder ivppiter – die Römer unterschieden in der Schreibung nicht zwischen /i/ und /j/ bzw. /u/ und /v/. In der Neuzeit wurde der Name meist „Jupiter“ geschrieben, seltener „Juppiter“ (so z. B. das Zedlersche Universallexikon, aber auch noch Georg Wissowa in seinem Werk "Religion und Kultus der Römer").

In der deutschen Altphilologie verbreitete sich im Laufe des 19. Jahrhunderts auch die originalsprachliche Schreibung mit der Folge, dass man in wissenschaftlichen Texten neben „Jupiter“ auch zunehmend „Iuppiter“ schrieb.
Die Änderung der Terminologie wurde von der germanischen Altertumswissenschaft nicht nachvollzogen; daher wird beispielsweise in deutschsprachigen vorgeschichtlichen Arbeiten weiterhin vorwiegend „Jupiter“ geschrieben.

In der Lehre, in Lateinwörterbüchern, allgemeinen Lexika und mythologischer Fachliteratur, in populären Texten etc. wird im Deutschen weiterhin „Jupiter“ geschrieben; dies ist auch die beispielsweise vom Duden vorgeschlagene Schreibweise.

Wie viele andere antike Götter wurde Jupiter mit verschiedenen Beinamen verehrt, die jeweils bestimmte Aspekte betonten oder mit einzelnen Örtlichkeiten verbunden waren, bzw. örtliche Götter vereinnahmten. Als "Jupiter Latiaris" wurde er von den Latinern als Schutzgottheit ihres erst später von Rom dominierten Städtebundes verehrt, sein Tempel befand sich daher außerhalb Roms in den Albaner Bergen.
Als Staatsgott war er "Jupiter Optimus Maximus", als oberster Gott in der im Tempel auf dem Kapitol verehrten Kapitolinischen Trias war er "Jupiter Capitolinus".

Weitere Beinamen gehen auf alte Kulte zurück, wie "Iupiter Feretrius" („der Edelbeuteträger“) oder "Iupiter Stator" („der die Feinde zum Stehen bringt“). Andere erlangten erst in der Kaiserzeit Bedeutung, so der Kult des "Jupiter Tonans" („der Donnerer“), der eigentlich eine Übertragung des griechischen "Zeus Bronton" ist. Als "Jupiter Pluvius" („der Regnende“) wurde er zur Beendigung sommerlicher Dürren angerufen.

Die Iden jedes Monats, die ursprünglich dem Vollmond entsprachen, also solche Tage, an denen es keine völlige Dunkelheit gab, waren "feriae Iovis" „Festtage des Jupiter“. An diesem Tag wurde ein ihm geweihtes weißes Schaf in feierlicher Prozession über die Via Sacra auf das Kapitol geführt und dort geopfert.
Auch die Stiftungstage der Jupitertempel fielen auf die Iden:
Die dem Jupiter geweihten großen Festmähler, die "epula Iovis," fanden ebenfalls an den Iden statt, eines am 13. September und ein anderes am 13. November.

Weiter dem Jupiter geweihte Feste waren die Weinfeste, die beiden Vinalia (Vinalia Priora am 23. April und "Vinalia Rustica" am 19. August) und vermutlich auch die Meditrinalia am 11. Oktober. Auch die Weinlese, die je nach Reife der Trauben zu wechselnder Zeit begann, wurde mit der Opferung eines Lammes durch den Flamen Dialis, den Staatspriester des Jupiter, eröffnet.

Nach Jupiter wurde der fünfte (heute vierte) Wochentag "Iovis dies" genannt, daher italienisch "giovedi," spanisch "jueves" und französisch "jeudi". Die Germanen setzten ihn mit Donar, skandinavisch Thor, gleich, daher deutsch "Donnerstag" und englisch "Thursday".

Außerdem entsprach ihm der Planet Jupiter.

Das zweifellos bedeutendste Heiligtum des Jupiter und der Sitz des Staatskultes befand sich auf dem Kapitol. Der nördliche, höhere Gipfel des Kapitols hieß "arx" („Burg“). Hier endete die Prozessionsstraße "Via sacra" und hier befand sich die Beobachtungsstätte der Auguren, von der aus sie den Flug der Vögel verfolgten.

Auf dem Südgipfel befand sich der Tempel des "Jupiter Feretrius", das älteste Heiligtum des Gottes in der Stadt, der Sage nach von Romulus selbst gestiftet.
Der Tempel enthielt kein Kultbild, hier wurde aber ein heiliger Stein aufbewahrt, der sogenannte "silex" oder "lapis". Beide Worte bedeuten „Stein“, "silex" bezeichnet eher den harten Stein, "lapis" eher den größeren Stein oder Felsbrocken.
Falls es sich bei dem "silex" um ein zu Opferzwecken verwendetes Steinmesser gehandelt hat, kommen als Material eigentlich nur Feuerstein und Obsidian in Frage. Über die Rolle des "silex" beim Opfer herrscht jedoch keine Klarheit und seine Beschaffenheit ist strittig.
Nach diesem Stein wurde der Gott des Heiligtums auch "Jupiter Lapis" („Jupiter vom Stein“) genannt.
Ein Schwur bei diesem Gott war besonders feierlich und wurde bei völkerrechtlichen Vereinbarungen gebraucht.

Der Name "Jupiter Feretrius" („der Edelbeuteträger“) aber geht darauf zurück, dass in diesem Tempel die Spolia opima, die „reiche Beute“, dem Gott geweiht wurde. Die "Spolia opima" war die Rüstung eines feindlichen Heerführers, die ihm in der Schlacht von einem römischen Heerführer abgenommen wurde. Der beste Teil dieser reichen Beute, die "prima spolia", wurde dem "Jupiter Feretrius" geweiht. Es ist klar, dass aufgrund der hohen Anforderungen an den Erwerb der Weihgeschenke es im Lauf der römischen Geschichte nur wenige Male zu einer solchen Weihung kam, das erste Mal der Sage nach durch den Stifter des Heiligtums, Romulus selbst.

Außer dem altehrwürdigen Tempel des "Jupiter Feretrius" befand sich der eigentliche Haupttempel Jupiters in seiner Eigenschaft als römischer Staatsgott auf dem Südgipfel, genauer der Tempel der Kapitolinischen Trias, bestehend aus "Jupiter Optimus Maximus", Juno und Minerva.
Dieser Tempel, das Capitolium von Rom, war der Sage nach noch weitgehend von Lucius Tarquinius Superbus, dem letzten König Roms, errichtet worden, eingeweiht wurde es aber im ersten Jahr der römischen Republik am 13. September 509 v. Chr.

Im Tempel befanden sich die Kultbilder der Trias jeweils in einer eigenen Cella. Die Cella des Jupiter Optimus Maximus befand sich in der Mitte, Juno auf der linken und Minerva auf der rechten Seite.
Darüber hinaus gab es weitere Kultbilder, vor allem solche mit Jupiter eng verbundener Götter, so z. B. des Summanus. Auf dem Dach befand sich als Akroterion ein Bildnis von Jupiter, der eine Quadriga lenkt.

Das Capitolium war im Lauf der Geschichte mehrfach abgebrannt bzw. zerstört worden, wurde aber stets wieder auf den gleichen Fundamenten errichtet.

Ein weiterer Kult galt dem "Jupiter Fulgur" („Jupiter Blitz“) mit Heiligtum auf dem Campus Martius, bei dem Jupiter in Gestalt des Blitzes verehrt wurde. Ihm waren die "Bidentalia" heilig, die Orte, an denen ein Blitz in die Erde schlug (Blitzmale). Geschah das auf öffentlichem Boden, so wurde an der Stelle ein Sühneopfer vollzogen und die Stelle mit einem sogenannten Puteal eingegrenzt, einer kreisförmigen Ummauerung, die verhindern sollte, dass der vom Blitz versengte und dadurch geheiligte Boden berührt oder betreten wurde. Dabei wurde unterschieden zwischen Blitzen, die bei Tag einschlugen und solchen, die bei Nacht einschlugen. Nur die tagsüber erfolgten Blitzschläge wurden Jupiter selbst zugeordnet, die nächtlichen einer ihm eng verbundenen Gottheit namens Summanus. Die Inschriften der Blitzgräber lauteten entsprechend "fulgur Dium conditum" bzw. "fulgur Summanum conditum" (etwa „hier hat Jupiter /Summanus eingeschlagen“).

Mit "Jupiter Fulgur" verknüpft ist die Sage von "Jupiter Elicius" („herabgezogener Jupiter“). Demnach gelang es König Numa Pompilius mit Unterstützung der Nymphe Egeria und der Götter Picus und Faunus, Jupiter aus dem Himmel „herabzuziehen“ (d. h. einen magischen Gotteszwang zu bewirken) und ihn zu veranlassen, Numa die Mittel und Riten der Blitzsühnung zu verraten. Zudem erhielt Numa von Jupiter damals die Ancile, altertümliche Schilde und Symbole römischen Machtanspruchs. Das Heiligtum des "Jupiter Elicius" befand sich auf dem Aventin, der auch Schauplatz der Sage ist.

Von "Jupiter Fulgur" zu unterscheiden ist "Jupiter Tonans" („donnernder Jupiter“). Der Kult stammt aus augusteischer Zeit und geht auf ein Ereignis zurück, bei dem Augustus während eines Feldzuges gegen die Kantabrer beinahe vom Blitz erschlagen worden wäre. Zum Dank gelobte er dem Jupiter einen Tempel auf dem Kapitol.
Dieser Tempel auf dem Südhügel des Kapitols wurde besonders aufwändig gestaltet mit Wänden aus Marmorquadern und reichem Bildschmuck. Das Heiligtum wurde am 1. September 22 v. Chr. geweiht.

Die Ausstattung soll so prachtvoll gewesen sein, dass "Jupiter Capitolinus" eines Nachts dem Augustus im Traum erschien und sich über Zurücksetzung und Vernachlässigung beklagte. Darauf versicherte Augustus dem obersten Jupiter, dass der Gott des neuen Tempels nur Pförtner des Heiligtums sei. Um dieses Verhältnis kenntlich zu machen, ließ Augustus dann Klingelschellen ("tintinnabula") am Dach des Tempels des "Jupiter tonans" anbringen.

Jupiter in seiner kriegerischen Rolle war der Kult des "Jupiter Stator" („der Flucht hemmende Jupiter“ bzw. „Jupiter der Erhalter“) gewidmet.
Dieser Gott besaß in Rom zwei Tempel.
Der eine befand sich bei der Porta Mugionia an der Nordseite des Palatin.
Der Sage nach ging auch er auf Romulus zurück, tatsächlich wurde er 294 v. Chr. von Marcus Atilius Regulus im dritten Samnitenkrieg gelobt und wenig später erbaut.
Ein anderer Tempel des "Jupiter Stator" wurde von Quintus Caecilius Metellus Macedonicus nach seinem Triumph 146 v. Chr. in der Nähe des Circus Flaminius erbaut.

In der Funktion dem "Jupiter Stator" ähnlich ist "Jupiter Victor" („der Sieger“), dessen Tempel von Quintus Fabius Maximus Rullianus ebenfalls im Samnitenkrieg votiert wurde.
Der Tempel befand sich vermutlich auf dem Quirinal.
Weitere militärisch orientierte Jupiter-Kulte waren dem "Jupiter Invictus" („unbesiegter Jupiter“) und dem "Jupiter Propugnator" („der Streiter“) gewidmet.

Im auch von Kelten besiedelten römischen Obergermanien pflegte man den Kult des Jupiter mit den sogenannten Jupitergigantensäulen, wobei der keltische Himmelsgott Taranis ebenfalls verehrt wurde.

Auf dem heutigen Gellértberg in Budapest (Aquincum), den die Römer wahrscheinlich "Mons Teutanus" nannten, lag ein spätkeltisches Oppidum der Eravisker, die dort Teutates, eine keltische Hauptgottheit, unter der Namensvariation "Teutanus" verehrten. Der Kult wurde später von der Aquincumer Bevölkerung übernommen. Vom 2. bis in das 3. Jahrhundert n. Chr. ließen die Duoviri der Colonia jährlich am 11. Juni einen Altarstein für Teutanus errichten, der mit dem Iuppiter Optimus Maximus identifiziert wurde.

Der Jupiterkult wurde als Teil des Vielgötterglaubens unter Kaiser Theodosius I. Ende des 4. Jahrhunderts n. Chr. als Staatsreligion des Römischen Reiches vom Christentum abgelöst.

Die Götter der römischen Mythologie wurden bis zur Entfaltung der Interpretatio Romana vor allem als Personifikationen von Naturereignissen verstanden, mit denen kaum mythische Erzählungen verbunden waren. Erst mit der Gleichsetzung der griechischen mit den römischen Göttern wurden auch die Erzählungen der griechischen Mythologie übertragen, wobei Jupiter mit Zeus gleichgesetzt wurde. Im Zuge des Aufblühens der lateinischen Literatur ab dem 3. Jahrhundert v. Chr. wurden sukzessiv Mythen und Eigenschaften des Zeus auf Jupiter übertragen, weiterentwickelt und an die sich entwickelnde römische Kultur angepasst.

In der stoizistischen Deutung des Mythos verliert Jupiter seine seit dem homerischen Zeus fortgeschriebenen individuellen Züge, woraus sich eine physikalisch-allegorische Deutung entwickelt, die oberflächlich besehen wieder an den homerischen Zeus anschließt.

Der Vater des Jupiter ist Saturnus, der mit dem griechischen Vater des Zeus identifizierte Kronos, der seine Nachkommen verschlingt und durch eine List einen Stein anstatt seines jüngsten Kindes verspeist. Der Geburtsort des Jupiter wird wie bei Zeus als Höhle am Berg Dikte oder am Ida angegeben, wo er in beiden Fällen von der Milch Amaltheas aufgezogen wird. Beim Vatikanischen Mythographen wird Jupiter zunächst in die Obhut einer Wölfin gegeben, die jedoch nicht genug Milch hat, womit der Jupiter-Mythos an den Mythos von Romulus und Remus gekoppelt wird. Nach anderen Autoren wird er von Bienen mit Honig ernährt, wofür er diesen später die Fertigkeit verliehen haben soll, ohne Beischlaf Kinder zu zeugen. Der humanistische Gelehrte Natale Conti gibt an, er sei von Bärinnen versorgt worden.

Im von den Griechen bereits aus altorientalischen Erzählungen übernommenen Sukzessionsmythos, in der die Herrschaftsabfolge der Göttergenerationen erklärt wird und der die Vorherrschaft des Jupiter unter den Göttern begründet, besiegen die olympischen Götter unter seiner Führung die Titanen und setzen sich erfolgreich gegen die Machtansprüche der Giganten zur Wehr. In der lateinischen Überlieferung vermischen sich die einzelnen griechischen Mythen um die verschiedenen Machtkämpfe zunehmend, was sich am augenfälligsten an den Namen seiner Gegner zeigt, wenn einerseits in Beschreibungen der Gigantomachie Titanennamen unter den Giganten genannt, andererseits zunehmend ursprünglich eigenständige Gestalten wie die Aloaden oder Typhon den Giganten zugeschlagen werden.

Daneben erscheinen Teile des Sukzessionskampfes mit anderen Kontexten verknüpft, etwa wenn der bereits als Götterkönig herrschende Jupiter von den Titanen angegriffen wird, weil seine eifersüchtige Gattin Juno diese dazu angestiftet hat oder wenn ihm im Kampf gegen den Typhon, bei dessen Angriff die übrigen Götter als Tiere verwandelt nach Ägypten geflohen waren, die Göttin Minerva zur Seite gestellt wird. Beim Vatikanischen Mythographen sind Titanomachie, Gigantomachie und Typhonomachie schließlich zu einer einzelnen Erzählung verschmolzen, in der die Giganten und Titanen vom Geschrei der Esel der zur Hilfe kommenden Satyrn vertrieben werden. Beim Anblick des Typhon flüchten auch hier alle Götter bis auf Jupiter. Er besiegt die Angreifer schließlich mit der Hilfe eines Adlers, der seine Blitze zu den Gegnern trägt.

Wie der griechische Zeus hat Jupiter neben seiner Ehe mit der eifersüchtigen Juno zahlreiche Liebschaften, wofür er zumeist die Gestalt wechselt und sich in Tiere, Menschen, Götter oder auch Dinge verwandelt. Europa entführt er beispielsweise in Gestalt eines wunderschönen Stieres, Leda nähert er sich in Gestalt eines Schwans, Antiope tritt er als Satyr oder Stier gegenüber, und Callisto, einer Jungfrau aus dem Gefolge der Diana, zeigt er sich als ihre Herrin. Für Alcmena verwandelt er sich in ihren Gatten Amphitryon, für Danae in Goldregen, und Aegina zeigt er sich in Flammengestalt. Auch liebt er den schönen Jüngling Ganymedes, den er in Adlergestalt von der Erde weg raubt und als Mundschenk auf den Olymp versetzt.

Jupiters Verwandlungskünste beschränken sich jedoch nicht auf ihn selbst, er verwandelt beispielsweise den Kureten Celmis in Stahl, um ihm Unsterblichkeit zu verleihen, seinem Sohn Aeacus zum Schutz gegen Juno ein Ameisenvolk zum Volk der Myrmidonen oder Io in eine Kuh.

Zu den wenigen nicht aus der griechischen Mythologie abgeleiteten Mythen gehören die in der römischen Mythologie deutlich seltener verbreiteten Ortssagen. Zum Beispiel wurde das Wasser einer wegen seiner Heilkraft gerühmten Quelle für viele Opfer verwendet; im Mythos wird die Quellnymphe Iuturna zu Jupiters Geliebten, der ihr dafür Unsterblichkeit verleiht, damit sie auch weiterhin über die Quelle wachen kann.

Anhand von schriftlichen Quellen lässt sich schon kaum ein Bild des Zeus machen. Die spärlichen Beschreibungen der äußeren Erscheinung wurden nur zum Teil von römischen Schriftstellern zur Beschreibung Jupiters übernommen, erfuhren jedoch auch Abwandlungen. Während er in der rechten Hand die Blitze hält und schleudert, trägt er in der Linken ein elfenbeinernes Zepter, sein Haar ist so gewaltig, dass er mit dem bloßen Schütteln desselben Erde, Meer und Sternenhimmel erschüttert. Er trägt einen Bart, aus dem nach dem Vatikanischen Mythographen die Minerva geboren wird. Als Kleidung trägt er eine goldene Tunica, die das Vorbild für die Tunicae römischer Triumphatoren ist.

In der stoischen Deutung verliert Jupiter seine aus der griechischen Mythologie übernommenen individuellen Züge. Jupiter wird als Gottheit verstanden, die sich in allen Teilen der Welt gleichermaßen manifestiert und je nach Manifestation nur unterschiedlich bezeichnet wird, die übrigen Götter sind demnach nur ihren jeweiligen Aufgaben angepasste Teile des Jupiter. So repräsentiert etwa Juno die Luft, Diana die Erde und der als „Jupiter“ bezeichnete Teil den Äther.

Seine Attribute sind ein Bündel von Blitzen in der Hand und der ihn begleitende Adler als Zeichen der Macht; oft wird er thronend dargestellt. Sein heiliger Baum ist die Eiche, daher wird Jupiter gelegentlich auch mit einem Eichenkranz abgebildet. In der Kunst des 16. bis 18. Jahrhunderts symbolisiert er in der Gruppe der vier Elemente das Feuer.




</doc>
<doc id="10202" url="https://de.wikipedia.org/wiki?curid=10202" title="Resource Description Framework">
Resource Description Framework

Das Resource Description Framework (RDF, engl. sinngemäß „System zur Beschreibung von Ressourcen“) bezeichnet eine technische Herangehensweise im Internet zur Formulierung logischer Aussagen über beliebige Dinge (Ressourcen). Ursprünglich wurde RDF vom World Wide Web Consortium (W3C) als Standard zur Beschreibung von Metadaten konzipiert. Mittlerweile gilt RDF als ein grundlegender Baustein des Semantischen Webs. RDF ähnelt den klassischen Methoden zur Modellierung von Konzepten wie UML-Klassendiagramme und Entity-Relationship-Modell. Im RDF-Modell besteht jede Aussage aus den drei Einheiten Subjekt, Prädikat und Objekt, wobei eine Ressource als Subjekt mit einer anderen Ressource oder lediglich einem Wert (Literal) als Objekt näher beschrieben wird. Mit einer weiteren Ressource als Prädikat bilden diese drei Einheiten ein "Tripel" („3-Tupel“). Um global eindeutige Bezeichner für Ressourcen zu haben, werden diese dafür nach Konvention analog zu URL geformt. URL für allgemein häufig benutzte Beschreibungen, wie z. B. für Metadaten, sind RDF-Entwicklern bekannt, und können so weltweit für den gleichen Zweck verwendet werden, was u. a. Programmen ermöglicht, die Daten wiederum für den Menschen sinnvoll darzustellen.

Das RDF-Modell ist ein Datenmodell mit einer wohldefinierten formalen Semantik, das auf gerichteten Graphen basiert. Daten in RDF sind Aussagen über Ressourcen. Diese Aussagen werden dabei als Tripel modelliert. Die Menge der Tripel bildet einen (mathematischen) Graphen, und wird als RDF-Modell bezeichnet. Ein Tripel ist eine Elementaraussage, die aus Subjekt, Prädikat und Objekt besteht.

Das Tripel stellt eine Behauptung dar, in der Subjekt und Objekt miteinander in Beziehung (Relation) gesetzt sind. Beziehungen sind vom Subjekt zum Objekt gerichtet und mit dem Prädikat benannt. Tripel, die sich auf dieselben Subjekte bzw. Objekte beziehen, bilden ein semantisches Netz, das häufig tabellarisch oder graphisch dargestellt wird. Umgangssprachlich gesehen ist jede Aussage in RDF ein einfacher Satz. Etwa:

Übertragen auf die Modellierung mittels RDF:

In folgender Beispieltabelle (ergänzt um weitere Aussagen) bildet jede Zeile ein Tripel:

Eine Ressource ist etwas, das eindeutig bezeichnet ist und worüber man etwas aussagen möchte. Subjekt und Prädikat sind immer Ressourcen. Das Objekt kann entweder eine Ressource oder nur ein Literal sein. Literale sind dabei Zeichenketten, die unter Umständen noch anhand eines angegebenen Datentyps interpretiert werden. Als Literale lassen sich z. B. Wahrheitswerte, Zahlen oder Datumsangaben spezifizieren. RDF-Ressourcen werden durch eindeutige Bezeichner (URIs) identifiziert. Die URIs erlauben es, Aussagen aus verschiedenen Quellen zu verbinden. Meist werden die Ressourcen mit einer URI bezeichnet, die von der Form her einer URL ähnelt. URLs sind spezielle URIs, die dazu dienen, Webseiten eindeutig zu bezeichnen. URIs müssen nicht zwangsläufig im Netzwerk erreichbar sein.

Beispiele:

Über die als Prädikat verwendeten Ressourcen lassen sich wiederum in RDF Aussagen treffen und als Metadatenformat ablegen. Andere RDF-Autoren können diese Vokabulare durch Referenzierung weiterverwenden. Ein prominentes Beispiel dafür ist die Repräsentation von Dublin Core in RDF. Zum anderen bilden RDF-Statements selbst Ressourcen, auf die mit weiteren Aussagen verwiesen werden kann. Diese Technik der Aussagen über Aussagen wird als Reifikation bezeichnet.

Zusätzlich hält RDF vordefinierte Datentypen für Listen und Mengen bereit, um Gruppen von Ressourcen zusammenzufassen. Ressourcen, die keinen expliziten URI haben, sondern nur zur Gruppierung von anderen Objekten dienen, werden in der Regel durch sogenannte „blank nodes“ modelliert. Ein Beispiel dafür ist die Zuweisung eines Namens, der aus separaten Zeichenketten für Vor- und Nachnamen besteht.

RDF ist unabhängig von einer speziellen (textuellen) Repräsentation. Üblich ist XML sowie eine kürzere Syntax namens "Notation 3" (N3). Das W3C hat im Jahr 2011 zusätzlich die Sprache Turtle definiert, die eine reduzierte Teilmenge von N3 darstellt und damit zu einer größeren Verbreitung beitragen soll.

Für die Speicherung von RDF in Datenbanken und Datenstrukturen gibt es verschiedene Konzepte ("Triplestore"), da ein reines Ablegen der Tripel in einer relationalen Tabelle nicht sehr effizient für viele Abfragen ist.

RDF-Tripel werden auch graphisch dargestellt: Nach Konvention werden Ressourcen, die Subjekt oder Objekt eines Tripels sind, durch Ellipsen und Literale durch Rechtecke symbolisiert. Die Verbindung zwischen einem Subjekt und einem Objekt wird durch eine gerichtete Kante, die mit dem Prädikat beschriftet ist, dargestellt. Die folgende Abbildung folgt dieser Konvention und zeigt „<nowiki>http://de.wikipedia.org/wiki/Resource_Description_Framework</nowiki>“. Der abgebildete RDF-Graph sagt aus, dass die Ressource – in diesem Fall der vorliegende Artikel – einen Titel namens „Resource Description Framework“ hat und einen Publisher, die „Wikipedia“. Diese ist in dem Beispiel lediglich als Literal modelliert und kann somit nicht weiter spezifiziert werden.

Zur Suche in RDF-Daten wurden verschiedene Abfragesprachen entworfen. Die RDF Query Language (RDQL), erinnert der Form nach stark an SQL. Im Januar 2008 hat das W3C SPARQL als "W3C Recommendation" verabschiedet und damit quasi zum Standard für RDF Anfragesprachen gemacht, weshalb es für SPARQL auch viele Implementierungen gibt.

Als Beispiel sei folgende Beschreibung über den aktuellen Artikel gegeben, wobei "Titel" und "Herausgeber" nach Dublin Core definiert sind: <nowiki>‚http://de.wikipedia.org/wiki/Resource_Description_Framework</nowiki>‘ hat den Titel ‚Resource Description Framework‘ sowie den Herausgeber ‚Wikipedia – Die freie Enzyklopädie‘. In RDF (N3) wird dies mittels zwei Tripeln ausgedrückt (das codice_1 dient hier nur der besseren Lesbarkeit):

Eine Anfrage, die herausfindet, wie der "Titel" einer vom "Herausgeber „Wikipedia – Die freie Enzyklopädie“" definierten Ressource lautet, könnte in SPARQL wie folgt aussehen:

PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dc: <http://purl.org/dc/elements/1.1/>
SELECT ?title

Das Ergebnis ist eine Tabelle mit genau einem Eintrag (Bindung der Variable "?title") mit dem Wert Resource Description Framework.

Als Vorläufer von RDF kann das Meta Content Framework (MCF) in XML gelten, eine Sprache, die 1995–1997 von Ramanathan V. Guha entwickelt und nach seinem Wechsel zu Netscape im Juni 1997 beim W3C eingereicht wurde. Im Rahmen der Browserkriege war MCF auch eine Reaktion auf das Channel Definition Format von Microsoft. Anstatt MCF den Vorzug zu geben, entschloss man sich beim W3C, eine allgemeine Sprache zur Formulierung von Metadaten zu entwickeln, die den Namen RDF tragen sollte. Der erste RDF-Standard wurde im August 1997 als Entwurf vorgelegt und im Februar 1999 als Empfehlung veröffentlicht. Ab 1999 wurde mit der Entwicklung von RDF-Schema begonnen.




</doc>
<doc id="10206" url="https://de.wikipedia.org/wiki?curid=10206" title="Washington">
Washington

Washington ist ein männlicher Vor- und Familienname:


Washington heißen folgende geographische Objekte:

Verwaltungseinheiten:

Orte:

in den Vereinigten Staaten:
historisch:
sowie:
Washington, Weiteres:

Siehe auch:


</doc>
<doc id="10207" url="https://de.wikipedia.org/wiki?curid=10207" title="Weißes Haus">
Weißes Haus

Das Weiße Haus () in Washington, D.C. ist Amts- und offizieller Regierungssitz des Präsidenten der Vereinigten Staaten. Als Metonym ist es namensgebend für den Mitarbeiterstab des US-Präsidenten, das Executive Office of the President of the United States, ebenfalls meist als „Weißes Haus“ bezeichnet. Mitunter ist auch die ganze US-Regierung gemeint, vergleichbar den Begriffen "10 Downing Street", "Quai d’Orsay" oder historisch die "Wilhelmstraße".

Das Weiße Haus liegt an der Pennsylvania Avenue und hat die Hausnummer 1600. Seinen Namen erhielt es offiziell 1901 von Theodore Roosevelt aufgrund seines weißen Außenanstrichs, es dürfte aber schon zuvor umgangssprachlich als weißes Haus bezeichnet worden sein. Die zumeist in den Medien abgebildete weiße Villa stellt eigentlich nur den mittleren Teil des Gebäudekomplexes White House Complex dar, der außer dem Haupthaus aus zwei Nebengebäuden (dem Ost- und dem Westflügel, "East Wing" bzw. "West Wing"), den sie verbindenden Galerien und dem Eisenhower Executive Office Building besteht.

Die Lage des Weißen Hauses wurde von Präsident George Washington und dem Stadtplaner Pierre L’Enfant ausgesucht. Der Architekt war der irische Baumeister James Hoban, der sich das Leinster House (1745–1748) in Dublin, heute Sitz des irischen Parlaments, zum Vorbild nahm. Der Grundstein für den Erstbau wurde am 13. Oktober 1792 gelegt. Der Bau dauerte acht Jahre und kostete $232'371,83 (in heutiger Kaufkraft: rund $3,35 Millionen). Ab dem 1. November 1800 wurde das Weiße Haus erstmals genutzt.

Beim französischen Château de Rastignac (1811–1817) streiten sich die Historiker darüber, ob es als Vorbild für die späteren Umbauten am Weißen Haus Pate stand oder andersherum. Ein Indiz für ersteres könnte sein, dass Thomas Jefferson, der den Bau des Weißen Hauses fortführte, einige Zeit als amerikanischer Botschafter in Frankreich tätig war und sich seither bei seinen Bauaufgaben von dem französischen Architekten Charles-Louis Clérisseau beraten ließ.

1814 wurde es von britischen Truppen im sogenannten Krieg von 1812 niedergebrannt. Der Wiederaufbau im klassizistischen Stil begann 1819 und wurde erneut von James Hoban geleitet. Vorhandene Rauchschäden wurden weiß übertüncht. Ab 1824 wurden an der Vorderseite große Kalksteinsäulen errichtet. Diese stammen von der kroatischen Insel Brač und sind für ihre weiße Färbung bekannt. 1901 erfolgte unter Theodore Roosevelt eine Sanierung und der Anbau des Westflügels mit Bürotrakt; Roosevelt war es auch, der dem Gebäude offiziell den Namen „Weißes Haus“ gab. Das präsidiale Arbeitszimmer "Oval Office" entstand 1909 auf Initiative von William Howard Taft. Am 24. Dezember 1929 wurde der Westflügel durch ein Feuer zerstört.
Nach dem Zweiten Weltkrieg war das Weiße Haus in einem schlechten Zustand. Es war jahrelang mangelhaft unterhalten worden und hatte erhebliche Schäden. Ein Teil der Fundamente wurde 1948 als unterdimensioniert bewertet.
Unter Präsident Harry S. Truman wurde es von 1949 bis 1952 kernsaniert ("Truman reconstruction").
Es wurde vollständig entkernt; danach wurde die Innenstruktur (nunmehr in Stahlbeton) neu errichtet. Während der Bauarbeiten befand sich der Arbeitssitz der Regierung im Blair House;
am 27. März 1952 wurde er in das Weiße Haus zurückverlegt.

Im Weißen Haus hat, bis auf George Washington, jeder US-Präsident gewohnt. Am 2. Juni 1886 heiratete als erster und bisher einziger Präsident Grover Cleveland seine 28 Jahre jüngere Frau Frances Folsom im Weißen Haus. Zwischen 1812 und 1994 wurden insgesamt 17 Ehen dort geschlossen, oftmals von Töchtern amtierender Präsidenten, unter anderem Tricia Nixon Cox, Lynda Johnson Robb und Alice Roosevelt Longworth.

Die zweite Tochter der Clevelands, Esther (1893–1980), ist das einzige bislang im Weißen Haus zur Welt gekommene Kind.

Seit dem 19. Dezember 1960 ist es ein National Historic Landmark.

Das Anwesen verfügt über 132 Räume, 35 Badezimmer, 412 Türen, 147 Fenster, 8 Treppenhäuser, 3 Aufzüge, einen Swimmingpool, einen Tennisplatz, einen Kinosaal sowie eine unter Präsident Richard Nixon eingerichtete Bowlingbahn. Nach den Vorstellungen Barack Obamas wurde ein Basketballfeld errichtet.

Zum Anwesen des Weißen Hauses gehören heute die sogenannte Executive Mansion (Hauptgebäude) sowie der West Wing (Westflügel) und der East Wing (Ostflügel), die optisch unauffällig an die weiße Villa angebaut wurden. Die Executive Mansion beherbergt in der ersten Etage die repräsentativen Staatsräume. Weithin bekannt ist der East Room, größter Saal im Weißen Haus, wo beispielsweise Empfänge, Pressekonferenzen, Konzerte und Bälle stattfinden. Staatsbanketts werden zumeist im State Dining Room veranstaltet. Die Privatwohnung der Präsidentenfamilie befindet sich im zweiten Stock. In den angebauten Nebengebäuden befinden sich die Büros des Präsidenten, der First Lady und ihrer Mitarbeiter.

Das Weiße Haus wird von mehreren Gärten umgeben, darunter:

Seit 1923 wird jährlich in der Adventszeit am Weißen Haus der National Christmas Tree (Nationaler Weihnachtsbaum der USA) festlich geschmückt. Die Arbeiten, die unter der Oberleitung der jeweiligen First Lady stehen, werden von einer monatelangen Festlichkeit begleitet, dem sogenannten "Pageant of Peace" (etwa: Schaulaufen des Friedens). Das Einschalten der Beleuchtung wird vom Fernsehen übertragen. Kleinere Weihnachtsbäume in der Nähe und andere Dekorationen, die zum National Christmas Tree hinführen, werden als "Pathway to Peace" (Pfad zum Frieden) bezeichnet.

Die Nordfront des Weißen Hauses hat drei Etagen. Das Erdgeschoss ist allerdings durch den höheren Boden und eine Brüstung verborgen. Vor den zentralen drei Fenstern befindet sich ein Prostylos als Vorhalle, welcher um 1830 zusätzlich angebaut wurde. Die Fenster der ersten Etage besitzen abwechselnd spitze und halbrunde Giebel. Der Haupteingang in der Mitte der Säulenhalle wird von einem lünettenartigen Fenster und einem bogenförmigen Blumenrelief überwunden. Die Dachlinie ist durch die den moderneren dritten Stock umgebenden Balustrade verdeckt.

Die Südfassade ist eine Kombination aus Palladianismus und Neoklassizismus. Hier sind alle drei Etagen der Fassade sichtbar. Das Mauerwerk im Erdgeschoss ist rustiziert. In der Mitte der Fassade befindet sich ein neoklassizistischer vorspringender Bogen mit drei Fenstern. Der Bogen wird von fünf Fenstern flankiert, die in der ersten Etage wie die der Nordfassade abwechselnd halbrunde und spitze Giebel besitzen. Vom Erdgeschoss des Bogens führen zwei Treppen zu einer ionischen Säulenloggia mit dem "Truman Balcony" in der zweiten Etage.

Das Weiße Haus ist neben dem Kapitol einer der wichtigsten Blickpunkte diagonal verlaufender Avenuen und großer Grünflächen, die Washington als Sichtachsen durchziehen. Das städtebauliche Konzept für diese Anordnung beruht auf dem 1792 veröffentlichten "Plan of the City of Washington" des Stadtplaners Pierre L’Enfant. Als eine Inspirationsquelle für diese städtebauliche Planung gilt der Fächergrundriss der Stadt Karlsruhe.

Der East Room (Ostsaal) ist der größte repräsentative Saal des Präsidentensitzes. Er wird für verschiedenste Veranstaltungen genutzt, so beispielsweise für Pressekonferenzen, Empfänge, Bälle oder große Staatsessen. Der East Room ist aus Fernsehen und Medien der wohl bekannteste Raum im Weißen Haus, neben dem Oval Office. Im East Room hängt eine Kopie des Lansdowne Portrait, welches den ersten Präsidenten der USA, George Washington, zeigt. Der Raum befindet sich – wie der Name bereits sagt – auf der Ostseite des sogenannten State Floor im Erdgeschoss, in welchem sich die repräsentativen Staatsräume befinden.
Der East Room war in seiner Geschichte Schauplatz privater und hoch offizieller Ereignisse. So fanden beispielsweise schon Hochzeiten von Kindern der Präsidentenfamilien hier statt. Präsident Gerald Ford wurde hier 1974 nach dem Rücktritt Richard Nixons als Präsident vereidigt. Alle im Amt verstorbenen Präsidenten, wie beispielsweise Abraham Lincoln 1865 und John F. Kennedy 1963, wurden hier aufgebahrt.
Das Weiße Haus und das umliegende Gelände ist eine Hochsicherheitszone, die mit den modernsten Sicherheitsanlagen ausgestattet ist. Folgende Maßnahmen bzw. Einrichtungen schützen das Weiße Haus:




</doc>
<doc id="10209" url="https://de.wikipedia.org/wiki?curid=10209" title="Mozilla Application Suite">
Mozilla Application Suite

Mozilla Application Suite [] (auch Mozilla Suite oder einfach Mozilla, siehe auch allgemeiner Begriff Mozilla) war ein freies Programmpaket bestehend aus Webbrowser, E-Mail-Programm und weiteren Programmen. Mittlerweile hat die Mozilla Foundation die offizielle Entwicklung zugunsten des Webbrowsers Firefox und des E-Mail-Programms Thunderbird eingestellt. Die Suite wird seitdem unter dem neuen Namen SeaMonkey von einer kleinen Entwicklergemeinde weitergepflegt. Die Mozilla Suite steht unter mehreren freien Lizenzen wie der hauseigenen Mozilla Public License (MPL), sowie der GPL und LGPL.

Mozilla basierte auf dem 1998 von der Netscape Corporation freigegebenen Quellcode des Netscape Communicators; von dieser Codebasis enthielt Mozilla allerdings nicht mehr viel. Mozillas (neugeschriebene) Layout-Engine heißt Gecko.

Ursprünglich war das Wort "Mozilla" nur die firmeninterne Bezeichnung für den Netscape-Webbrowser. Zum genauen Ursprung des Namens, der sich auf den ersten populären Webbrowser Mosaic bezieht, gibt es mehrere Erklärungen. Eine besagt, dass er sich aus "Mosaic Killa" zusammensetzt, laut der anderen aus "Mosaic" und "Godzilla". Das offizielle Netscape-Maskottchen, eine grüne Eidechse, und der bei Mozilla verwendete Dinosaurier, die beide den Namen Mozilla tragen, deuten auf letztere Erklärung; es ist allerdings denkbar und vielleicht wahrscheinlicher, dass das Maskottchen wegen der Namensähnlichkeit von "Mozilla" mit der bekannten Figur "Godzilla" gewählt wurde. Eine der beiden Deutungen der Bedeutung von "Mozilla" ist also ein Backronym.

"Mozilla 1.0" erschien am 5. Juni 2002, die letzte stabile Version "1.7.13" am 22. April 2006. Danach wurde die Weiterentwicklung der Mozilla-Suite seitens der Mozilla Corporation eingestellt. Das Projekt wird jetzt von einer unabhängigen Programmierergruppe unter dem Namen SeaMonkey weitergeführt. Erscheinungsdaten der einzelnen Versionen der Suite sind auf der folgenden Zeittafel angeordnet.

Mozilla lief auf vielen verschiedenen Betriebssystemen ("Plattformen"). Dies wurde dadurch ermöglicht, dass große Teile von Mozilla "plattformunabhängig" waren. Die Hauptarbeit bei der Portierung entfiel auf die "Netscape Portable Runtime", die eine allgemeine Schnittstelle für z. B. Dateizugriff und Speicherverwaltung definierte.

Die Programmoberfläche wurde in der eigens entwickelten, ebenfalls plattformunabhängigen Sprache XUL, die auf XML basiert, geschrieben. Die Elemente dieser Oberfläche wurden, ähnlich wie Webseiten, durch Gecko dargestellt und nicht vom Betriebssystem. Dies hat den Vorteil, dass ein Entwickler für Änderungen an der Oberfläche keine Kenntnisse mehrerer Betriebssysteme benötigt. XUL ermöglichte auch, in einfacher Weise durch sogenannte "Extensions" die Mozilla-Oberfläche anzupassen oder zu erweitern. XUL unterstützt auch sogenannte Themes, die auf Basis von Webstandards wie Document Object Model und Cascading Style Sheets erstellt werden und mit denen das Aussehen von Mozilla dem eigenen Geschmack angepasst werden konnte. Ein Nachteil war jedoch, dass eine XUL-basierte Oberfläche auf älteren Rechnern langsamer läuft.

Mozilla war auch in der Lage Stylesheets auf HTML- und XML-Seiten anzuwenden. Dazu wurden die Standards CSS 1 bis 3 (CSS 1 vollständig, CSS 2 nahezu vollständig, CSS 3 teilweise) und XSLT unterstützt. Daneben gab es einen Pop-up-Blocker und die Funktion des Tabbed Browsing, sowie des Caret Browsing.

Erfahrenen Benutzern war es möglich, das Erscheinungsbild und das Verhalten von Mozilla durch das Bearbeiten der Konfiguration, in Mozilla-Browsern erreichbar über die Eingabe von "about:config" in der Adresszeile, und das Anlegen eines sogenannten "User-Stylesheets" zu beeinflussen.

Neben dem Browser beinhaltete Mozilla auch ein vollständiges E-Mail-Programm inklusive Adressbuch. Eines der wichtigsten Features war der ab Version 1.3 enthaltene adaptive Spam-Filter auf Basis der Bayesschen Wahrscheinlichkeitstheorie. Nach einer Lernphase, in welcher der Benutzer E-Mails als „Junk“ und „Nicht Junk“ klassifizierte, wies das Programm sehr gute Trefferquoten auf.
Das Mail-Programm unterstützte beliebig viele POP3- und IMAP-Postfächer pro Benutzer. Es erlaubte zudem das Anlegen von Filter-Regeln, durch die E-Mails automatisch in beliebige Ordner vorsortiert werden konnten, und verfügte über sehr viele Möglichkeiten, das Programm an die Vorlieben des Benutzers anzupassen.
Verschlüsselung und Signierung von E-Mails wurden mit dem S/MIME-Standard transparent und unkompliziert für den Benutzer durchgeführt. Mit Hilfe der Extension Enigmail war die verschlüsselte E-Mail-Kommunikation auch via PGP möglich. Außerdem war das Abonnieren von Newsgroups und das Verfassen von Beiträgen für diese möglich.

Enthalten war auch das Programm-Modul "Composer" zum Bearbeiten und Erstellen von HTML-Seiten; es ermöglichte die Erstellung von Webseiten nach dem WYSIWYG-Prinzip, doch auch die direkte Bearbeitung des Quellcodes war möglich.

Weiterhin verfügte die Mozilla-Suite über ein Modul namens "ChatZilla" zum Chatten in IRC-Netzwerken.

Mozilla bot einige für Webseiten- und JavaScript-Entwickler wichtige Funktionen zur Fehlersuche in Form der integrierten "Web-Development-Tools". Dazu gehörten die JavaScript-Konsole, die die Direkteingabe von JavaScript-Kommandos erlaubte, der JavaScript-Debugger "Venkman" und das Tool "DOM-Inspector", das es erlaubte, die baumartige Struktur von Webseiten zu inspizieren und zu analysieren und somit mögliche Fehler im Aufbau zu entdecken.

"Venkman" ist die JavaScript-Debugger-Komponente der Mozilla Application Suite für Gecko-basierte Browser wie Firefox 3.x, Netscape 7.x und 9.x und Seamonkey >1.x. Da sie nicht in Firefox als eingebautes Feature existiert, ist Venkman auch als Erweiterung für Mozilla Firefox erhältlich. Venkman ist nach "Dr. Peter Venkman", gespielt von Bill Murray, aus den Filmen "Ghostbusters – Die Geisterjäger" und "Ghostbusters II" benannt.

Bei vielen heutigen Browsern beginnt die Identifikationszeichenkette mit "Mozilla", das liegt daran, dass zu der Zeit, als Netscape Marktführer bei Browsern war, viele Websites diesen zum Anzeigen voraussetzten. Deswegen fingen die anderen Browserhersteller an, sich für den Netscape Browser auszugeben, indem sie ebenfalls "Mozilla" am Anfang ihrer Identifikationszeichenkette verwendeten.

Der Netscape Communicator Version 6.x und 7.x basieren auf dem Mozilla-Code.

Da der Code von Mozilla oft als zu groß und unübersichtlich kritisiert wurde, bildeten sich verschiedene Projekte, die auf Gecko basierend einen schnelleren, schlankeren und leichter zu wartenden Browser kreieren wollen. Diese sind unter anderem:

Die Roadmap von Mozilla sah vor, dass sich das Projekt mehr auf Einzelprodukte konzentriert und die Suite nur noch im stabilen Zweig 1.7 mit Sicherheitsupdates versorgte, von den späteren Milestones dagegen keine finalen, offiziellen Versionen mehr herausgab, Entwicklern jedoch Hilfestellung gibt, wenn sie auf Basis des Mozilla-Codes eigene Versionen der Suite herausbringen wollen.
Das Mozilla-Projekt selbst will sich ganz auf die Weiterentwicklung von Mozilla Firefox (Browser) und von Mozilla Thunderbird (E-Mail-Programm) konzentrieren. Der Hauptgrund für diese Änderung ist, dass die Akzeptanz von Einzelapplikationen bei den Anwendern als höher angenommen wurde als die einer kompletten Applikationssuite, angeregt durch die Popularität von Firefox. Die Weiterentwicklung des Editors (Composer) als Einzelanwendung „Nvu“ als externes Projekt wurde nach der Version 1.0 von Linspire eingestellt. Eine weiter fortgeschrittene Version des HTML-Editors bietet eine Abspaltung von NVU mit dem Namen KompoZer, welche von Fabien Cazenave entwickelt wird.

Nachdem die Mozilla Foundation bekannt gab, dass die Unterstützung der Suite eingestellt wird, haben sich einige ehemalige Mozilla-Entwickler zusammengefunden, um die Mozilla-Suite weiterzuentwickeln und so das Programm unter dem Namen SeaMonkey in Form einer Internet-Suite weiterzuführen. Nachdem die Mozilla Foundation den Namen SeaMonkey als Trademark registriert hatte, erfolgte am 2. Juli 2005 die offizielle Ankündigung des Projektes. Am 30. Januar 2006 erschien die erste stabile Version 1.0.




</doc>
<doc id="10212" url="https://de.wikipedia.org/wiki?curid=10212" title="Netscape Navigator">
Netscape Navigator

Netscape Navigator ist ein Webbrowser, der von dem Unternehmen Netscape Communications entwickelt wurde. Der heutige Eigentümer und Inhaber der Marke „Netscape“ ist AOL. Am 1. März 2008 wurden Weiterentwicklung und Support für Netscape eingestellt.

Der Netscape Browser wurde Anfang der 1990er-Jahre von Marc Andreessen, dem Mitbegründer der Netscape Communications (gegründet als "Mosaic Communications Corporation") und vormaligem Entwickler von NCSA Mosaic, entwickelt.

Der Codename für die Entwicklung war „Mozilla“, ein Kofferwort für „Mosaic Killer“ und "Godzilla". Der Netscape Navigator lizenzierte teilweise Code des grafischen Browsers Mosaic. Die Veröffentlichung erfolgte im Oktober 1994 noch unter dem Namen "Mosaic Netscape 0.9".

Als wesentliche Neuerung brachte der Netscape-Browser unter anderem die Unterstützung von HTML-Frames. Bis etwa 1996 war Netscape Navigator der führende Webbrowser für die 16-Bit-Betriebssysteme von Microsoft (bis Windows 95) und Mac OS von Apple Computer.

Netscape verlor im sogenannten Browserkrieg zusehends Marktanteile an den Internet Explorer, da dieser mit dem Betriebssystem Microsoft Windows mitgeliefert wurde. Das ließ dessen Benutzeranteil deutlich ansteigen und Netscapes Marktanteil schnell auf eine verschwindend geringe Größe abstürzen. Bereits Ende 2002 hatte Netscape Navigator keinen nennenswerten Marktanteil mehr. Außerdem wurde die Weiterentwicklung ab Ende 1998 nur noch zögerlich fortgesetzt, was dazu beitrug, dass der Browser stetig Marktanteile verlor. Besonders die Verarbeitung von Cascading Style Sheets (CSS) bereitete Netscape in diesen Versionen häufig Probleme. Microsoft setzte in jener Zeit zudem auf eine selbstentwickelte Laufzeitumgebung für Java-Anwendungen, die mit Suns Java-Plugin nicht voll kompatibel war. Netscape hatte daher mit Microsofts Java ebenfalls Darstellungsschwierigkeiten. Analog ging Microsoft mit JavaScript vor, das Microsoft als JScript mit eigenem Code erweiterte, den Netscape im Gegensatz zum Internet Explorer nicht interpretieren konnte. Netscape konnte daher für Internet Explorer optimierte Websites oftmals nur unzulänglich bis praktisch überhaupt nicht darstellen.

Für die Versionen nach 4.x stellte Netscape den Programmcode seines Browsers unter eine Open-Source-Lizenz und rief zur Koordination der Entwicklung das Mozilla-Projekt ins Leben. Die neueren Versionen des Netscape Navigators ab Version 6.0 beruhen daher auf dem Code des Mozilla-Projektes und sind integrierter Bestandteil von „Netscape Communicator“. Seither gibt es außerdem die abgewandelte Variante Netscape Compact, bei der sich einfach nutzerspezifische Änderungen vornehmen lassen.
Die letzte deutsche Version für die Macintosh-Plattform war Netscape 7.02 für Mac OS 9. Die letzte offiziell in deutscher Sprache verfügbare Version war Netscape 7.1, die auf Mozilla 1.4 basierte. Netscape 7.2 (basierend auf Mozilla 1.7.2) konnte mit einem inoffiziellen Sprachpaket, das auf Mozilla-Sprachpaketen basierte, nachgerüstet werden. Sowohl Netscape 7.1 und 7.2 gelten als veraltet; bezüglich Sicherheitslücken gibt es keinerlei Unterstützung mehr; deswegen ist vom Einsatz im Allgemeinen dringend abzuraten.

Version 8.0 wurde am 19. Mai 2005 veröffentlicht. Sie basierte auf Mozilla Firefox, konnte aber trotzdem mit der Internet Explorer Engine rendern, und trug den Namen "Netscape Browser" – der Namensbestandteil "Navigator" wurde nicht mehr verwendet. Version 8 war auch die erste Netscape-Version, welche nicht mehr für die Macintosh-Plattform mitentwickelt wurde. Bereits einen Tag später erschien wegen Sicherheitslücken ein Update auf Version 8.0.1. Der neugestaltete Browser wartete mit zahlreichen bereits integrierten Features und Gimmicks auf, u. a. einem integrierten Instant Messenger und die Unterstützung von Mycroft-Plugins. Bei der Darstellung von Webseiten konnte zwischen der Gecko-Engine und der Trident-Rendering-Engine des Internet Explorer gewechselt werden. Webseiten, die ausschließlich für den Internet Explorer entwickelt wurden, zeigte der Browser automatisch mit der Rendering-Engine des Internet Explorer an. AOL entwickelte den neuen Browser in Zusammenarbeit mit Mercurial Communications.

Das letzte Major Release von Netscape Browser war Version 9. Das Programm wurde AOL-intern aus Mozilla Firefox 2.x weiterentwickelt und mit speziellen Plugins ergänzt (einige davon waren in Firefox nachrüstbar). Die Zusammenarbeit mit dem kanadischen Software-Unternehmen Mercurial endete, und die Möglichkeit die Rendering Engine des Internet Explorer zu nutzen wurde wieder entfernt. Man führte die Bezeichnung „Navigator“ wieder ein, da Netscape Navigator 9 ein Stand-Alone-Browser, ohne Mail-, Usenet- oder Editorfunktion war, der nun auch wieder auf Windows, Mac OS X und Linux lauffähig war. Parallel arbeitete man bei Netscape auch an einem E-Mail- bzw. News-Programm, das auf den Namen Netscape Messenger hören sollte.

Am 28. Dezember 2007 gab AOL bekannt, dass der Support für sämtliche Netscape-Browser am 1. Februar 2008 eingestellt werde. Als Begründung wurde angegeben, dass die Investitionen für eine ausreichende Weiterentwicklung in der Unternehmensausrichtung von AOL keinen Raum fänden. Es wurde den Nutzern empfohlen, auf Firefox umzusteigen und die von Netscape bereitgestellten Erweiterungen zu verwenden, um Firefox in Aussehen und Bedienung Netscape anzupassen. Der offizielle Support für Netscape-Produkte wurde von der AOL-Tochter bis 1. März 2008 verlängert.

Der Netscape Navigator identifiziert sich bei Webservern mit dem User Agent String „Mozilla/"Version" ("Betriebssystem")“, was den Codenamen für die ursprüngliche Browser-Entwicklung noch widerspiegelt. Spätere Browser übernahmen und variierten den User Agent String des Netscape Navigators oder einer seiner „Varianten“, um zu diesem kompatibel zu sein. Daher identifizieren sich die meisten heute gängigen Browser (einschließlich Microsoft Internet Explorer, Mozilla Firefox und Google Chrome) heute als Variante eines „Mozilla-Browsers“.

Der Export von Programmen aus den USA, die über sichere Mechanismen der Verschlüsselung verfügen, war lange wegen der CoCom-Liste verboten. Daher haben australische Programmierer 1997 das Programm „Fortify for Netscape“ entwickelt. Dieses aktiviert in einer einmaligen Aktion in einem schon installierten Netscape-Browser wieder die starke Verschlüsselung. Anfang 2000 wurden diese Bestimmungen gelockert, ab Version 4.73 konnte Netscape 128-Bit-Keys verwenden, wodurch Fortify unnötig wurde.




</doc>
<doc id="10213" url="https://de.wikipedia.org/wiki?curid=10213" title="Staatspräsident">
Staatspräsident

Ein Staatspräsident, oft auch nur Präsident (v. ‚den Vorsitz haben‘) genannt, ist in der Regel das Staatsoberhaupt einer Republik. Je nach Staatssystem wird das Amt durch eine (allgemeine) Volkswahl, durch ein "Wahlkomitee", durch die Legislative oder durch andere Verfahren vergeben. Zum Teil werden auch in Diktaturen Präsidenten ernannt. Der Begriff existiert auch in manchen nicht souveränen Verwaltungseinheiten, insbesondere solchen mit einer gewissen Autonomie, die eine Verfassung haben, man spricht dann von Landespräsident oder Regionspräsident, Provinzpräsident und Ähnlichem.

Die deutsche Übersetzung als „Vorsitzender“ kann täuschen, da beispielsweise in Deutschland und Österreich die Bundespräsidenten keinem Gremium vorsitzen, sondern als Einzelpersonen Organwalter eines Staatsorgans sind. In der Schweiz hingegen ist der Bundespräsident ein echter Vorsitzender des Regierungsgremiums.

Die Stellung und die Befugnisse des Präsidenten werden meist durch eine geschriebene Verfassung festgelegt. Sie ist eingebunden in ein System der Gewaltenteilung oder zumindest in ein System der strikten Machtkontrolle durch die Verfassung.

Hier vereinigt der Präsident die Funktion des Staatsoberhaupts und die des Regierungschefs. Er besitzt große Machtbefugnisse. Der Präsident wird dann meist direkt vom Volk gewählt, was seine starke Position legitimiert.

Typische präsidentielle Regierungssysteme sind die der USA (wobei der Präsident hier indirekt über sogenannte Wahlmänner gewählt wird) und die meisten lateinamerikanischen Staaten.

Hier ist der Präsident nur Staatsoberhaupt. Er wird, je nach Staat, durch die Legislative (das Parlament) oder direkt durch das Volk gewählt und hat hauptsächlich repräsentative Funktionen. Die eigentliche Führung der Exekutive (Regierung) ist die Aufgabe eines je nach Land Premierminister, Ministerpräsident, Bundeskanzler, Regierungspräsident oder ähnlich genannten Regierungschefs, dessen Regierung vom Vertrauen des Parlaments abhängig ist.

Typische parlamentarische Regierungssysteme sind die Deutschlands, Italiens, Polens, Tschechiens und der Türkei (doch wird die Türkei gemäß dem Ergebnis der verfassungsändernden Volksabstimmung für eine Verfassungsreferendum in der Türkei 2017 nach der nächsten Wahl – voraussichtlich im November 2019 – eine Präsidialdemokratie sein).

Eine Sonderform bilden zum Beispiel die Systeme von Südafrika, Botswana, den Föderierten Staaten von Mikronesien oder auch Nauru. Hier ist die Exekutivgewalt an das Parlament gebunden, das heißt, der "vom Parlament gewählte" Regierungschef ist zugleich Staatsoberhaupt. Da sich eine eindeutige Zuordnung zum präsidentiellen oder parlamentarischen System somit sehr schwierig gestaltet, wird es in der Regel als eigenständiges Regierungssystem angesehen.

Hier konkurrieren Präsident und Parlament um die Macht. Wird der Präsident durch die Parlamentsmehrheit gestützt, hat er weitreichende Kompetenzen. Gehören Parlamentsmehrheit und Präsident unterschiedlichen Parteien an, herrscht Kohabitation () und die Macht des Präsidenten ist eingeschränkt.

Typische semipräsidentielle Regierungssysteme sind die Portugals, Österreichs, São Tomé und Príncipes, der Ukraine, Frankreichs und Russlands – wobei die letzteren beiden jedoch in der politischen Praxis eher dem präsidentiellen System zuneigen. In Österreich wirkt der Bundespräsident primär als Repräsentant. Lediglich in Krisenzeiten greift er umfassender in die Alltagspolitik ein.

Einige wenige Republiken kennen kein Staatsoberhaupt. Dazu zählt beispielsweise die Schweiz. In der Schweiz übt die Landesregierung die Funktion des Staatsoberhauptes als Kollektiv aus. Es gibt zwar einen Bundespräsidenten, der gleichzeitig Mitglied des Bundesrats ist. Er wird vom Parlament jeweils auf ein Jahr gewählt. Allerdings ist er nur "primus inter pares" (erster unter Gleichen), der nur auf internationaler Ebene die repräsentative Funktion des Staatsoberhauptes ausübt.

Die ehemaligen deutschen Länder Württemberg-Hohenzollern und Baden hatten einen Regierungschef mit der Amtsbezeichnung "Staatspräsident", was jedoch in diesen beiden Fällen nicht darüber hinwegtäuschen darf, dass das Amt und die Funktion voll und ganz dem eines Ministerpräsidenten der heutigen Länder Deutschlands entsprach. Genauso verhält es sich bei den als "Staatspräsident" bezeichneten Regierungschefs einiger Länder der Weimarer Republik, wie zum Beispiel die Regierungschefs in Baden, Hessen und Württemberg. In Bayern wollte man Anfang der 1950er-Jahre einen Staatspräsidenten einführen, entschied sich aber, die Rolle des Staatsoberhauptes dem Ministerpräsidenten des Freistaates zu überlassen, der fortan wie auch andere deutsche Landesregierungschefs seinen Amtssitz in der Staatskanzlei hat.

Die historischen Kronländer wurden ursprünglich von "Statthaltern" des Kaisers geleitet, die im Zuge der Staatsreformen von 1848/49 und 1867 teils die Bezeichnung "Landespräsident" erhielten. Sie waren aber die Regierungschefs der formal souveränen Königreiche, Herzogtümer und Fürstentümer, mit dem Kaiser als jeweiligem Landesherrn, sodass man allgemein von "Landeschefs" sprach. Daraus entwickelte sich nach 1918 der "Landeshauptmann". Der Landeshauptmann vereint die Funktion des Landesoberhauptes und des Regierungschefs. Der Landeshauptmann hat mit seiner Parteimehrheit im Landesrat und Landtag oft weitgehend alleinige Macht im Rahmen der Landeskompetenzen. Daher spricht man besonders bei den langgedienten Landeshauptleuten, die ihre Länder tiefgreifend prägen, ironischerweise von „Landeskaiser“: Sosehr die Republik nach den Erfahrungen der Monarchie – und auch des Austrofaschismus und der NS-Zeit – einen „starken Mann“ an der Spitze zu verhindern suchte, so autokratische Verhältnisse herrschen auf Landesebene.

In Diktaturen wird der Titel eines Präsidenten häufig von selbsternannten Anführern angenommen – ein Beispiel dafür ist Ugandas Expräsident Idi Amin. Einige staatssozialistische Regime kennen oder kannten ebenfalls Präsidenten, allerdings liegt die Macht beim Zentralkomitee der Partei und insbesondere ihrem Vorsitzenden ("siehe Vorsitzender des Staatsrats").

Die Islamische Republik Iran ist zwar Republik, das Staatsoberhaupt ist aber der Oberste Rechtsgelehrte, der "Präsident" ist lediglich das Regierungsoberhaupt. Es handelt sich um ein theokratisch geprägtes System.


"Präsident der Republik" ist bzw. war der offizielle Titel des Staatsoberhaupts in:
historisch:




</doc>
<doc id="10215" url="https://de.wikipedia.org/wiki?curid=10215" title="IRC">
IRC

IRC steht als Abkürzung für:



</doc>
<doc id="10216" url="https://de.wikipedia.org/wiki?curid=10216" title="Wireless Application Protocol">
Wireless Application Protocol

Das Wireless Application Protocol (WAP) bezeichnet eine Sammlung von Techniken und Protokollen, deren Ziel es ist, Internetinhalte für die langsamere Übertragungsrate und die längeren Antwortzeiten im Mobilfunk sowie für die kleinen Displays von Mobiltelefonen verfügbar zu machen. Verschiedene WAP-Implementierungen standen damit in direktem Wettbewerb zum i-mode-Dienst. WAP fand hauptsächlich in den 2000er Jahren eine gewisse Verbreitung. Aufgrund seiner Komplexität, mittlerweile schnellerer Verbindungen und HTML-fähiger Mobilgeräte ist es heute weitgehend überholt.

Die primäre Aufgabe bei WAP besteht darin, wegen der geringen Displaykapazitäten und Rechenleistung von WAP-Clients die Menge der zu übertragenden Daten zu reduzieren und zugleich bei der Kodierung der Internetinhalte die offene Struktur und Lesbarkeit einer Auszeichnungssprache (engl. "markup language") beizubehalten. Diese beiden Forderungen stehen zunächst im Widerspruch zueinander:

Die Lösung des Problems besteht darin, dass bei WAP zwar die offene Form einer Auszeichnungssprache (bei WAP ist das die Wireless Markup Language, WML) beibehalten wird, diese jedoch nicht als Text, sondern in kompilierter Form zum WAP-Client übertragen wird. Dazu erfolgt die Kommunikation zwischen WAP-Client und Webserver über einen Proxy, das so genannte WAP-Gateway. Dieser übersetzt die binär vom WAP-Client eintreffenden Anfragen in Klartext an den Web-Server. Die Antworten des Servers werden im WAP-Gateway kompiliert im MIME-Typ WMLC (Wireless Markup Language Compiled) an den Client übertragen. Dazu übernimmt das Gateway Aufgaben (wie syntaktische Analyse der WML-Seiten), die im Web der Browser ausführt.

Die Kommunikation zwischen Server und WAP-Gateway verwendet das im Web etablierte Protokoll HTTP. Die Kommunikation zwischen Gateway und WAP-Client erfolgt (bis WAP 1.2) via WSP. Bezüglich der Verwendung des Trägers auf der Funkstrecke ist WAP flexibel, möglich sind zum Beispiel Circuit Switched Data (CSD), High Speed Circuit Switched Data (HSCSD), General Packet Radio Service (GPRS), aber auch Universal Mobile Telecommunications System (UMTS) und High Speed Downlink Packet Access (HSDPA). Auch der GSM-spezifische USSD-Übermittlungsdienst kann genutzt werden.

WAP wird als Basistechnologie für MMS genutzt. Sowohl das Senden als auch das Empfangen einer Multimedia-Nachricht basiert auf WAP. Die Information, dass eine solche Nachricht zum Herunterladen bereitsteht, wird als WAP-Push an das Mobiltelefon gesendet.

Der WAP-Standard (ab WAP 1.2) unterstützt einen Push-Dienst, der es erlaubt, Nachrichten inklusive einer URI ohne Anforderung an den WAP-Client zu senden. Für den Nutzer sieht der WAP-Push-Dienst einer SMS oder MMS recht ähnlich. Technisch verbirgt sich dahinter eine Signalisierung per SMS oder – mit GPRS oder UMTS – auch per Service Indication (WAP-167-ServiceInd) (SI) oder Service Loading (WAP-168-ServiceLoad) (SL).

Das WAP-Forum verabschiedete 1997 den WAP 1.0-Standard. WAP 1.0 basiert auf der Handheld Device Markup Language (HDML), das von der Firma "Unwired Planet" (heute Openwave) entwickelt wurde. Der Standard war aber nicht ausgereift und es fehlte an entsprechender Browser-Software und WAP-fähigen Endgeräten. So konnte er sich nicht kommerziell durchsetzen und blieb praktisch ohne Bedeutung.

1999 wurde der 1.1-Standard veröffentlicht, der insbesondere XHTML-Konventionen übernahm. Er war nicht mit WAP 1.0 kompatibel. WAP 1.1 konnte sich auch auf dem Endgerätemarkt durchsetzen, so dass bald die meisten Mobiltelefone mit einem WAP-Browser ausgerüstet wurden. Das Ende 1999 vorgestellte WAP 1.2 stellt in erster Linie eine Verbesserung von WAP 1.1 dar. Die wichtigsten Erweiterungen sind der Push-Service und so genannte User Agent-Profile für WAP-Browser, die es erlauben, übertragene WAP-Seiten in ihrer Formatierung komfortabel an die spezifisch verwendete Browser-Software anzupassen.

Der Internetzugang über WAP 1.x wird von Mobilfunkbetreibern meist getrennt verrechnet und nicht als direkter Internetzugriff (Web) gewertet. Der Datenverkehr läuft über eigene WAP-APN.

Bei der Spezifikation von WAP 2.0 hat man auf Mobilfunk-Spezifika weitgehend verzichtet und die ursprünglichen WAP-Protokolle WSP, WTP und WTLS durch HTTP und SSL ersetzt. Damit fällt der Übergang zum Internet wesentlich leichter. WAP 2.0 hat aber bei CSD und GPRS mit Geschwindigkeitsproblemen zu kämpfen. Über WAP 2.0 sind in der Regel alle normalen Internetseiten erreichbar; aufgrund der niedrigen Geschwindigkeit von GPRS sind viele allerdings erst mit EDGE oder UMTS sinnvoll nutzbar.

Mit WAP 2.0 wurde auch das Proxy-Konzept aufgeweicht. Der Standard sieht nun auch die Möglichkeit vor, dass der Client unter Umgehung des Gateways direkt mit dem Web-Server kommuniziert. Es kann allerdings auch ein vom Mobilfunkbetreiber angebotener Proxy verwendet werden. Die Unabhängigkeit von einem Proxy beseitigt zwar die Abhängigkeit vom einwandfreien Funktionieren des WAP-Gateways, hat aber den Nachteil, dass es bereits WAP 2.0-Clients gibt, die WSP nicht mehr unterstützen. Diese können dann die verfügbaren WAP-Proxys nicht mehr verwenden.

Die Verwendung des Internetzugangs via WAP 2.0 wird von den Mobilfunkprovidern im Normalfall als direkter Internetzugang verrechnet. Bei GPRS oder UMTS fallen dann üblicherweise für jede Seite Kosten abhängig vom exakten Datenvolumen an, während es bei WAP 1.x üblich ist, nur pro Seite (d. h. pro jeweils angeklicktem Link) abzurechnen. In manchen Tarifen können dadurch die Kosten pro Seitenaufruf bei datenintensiveren HTML-Seiten wesentlich höher als bei WAP 1.x sein; sehr kleine Seiten können dagegen auch günstiger sein.

Einige Mobiltelefon-Hersteller schreiben in den Angaben zu ihren Geräten, dass es WAP-2.0-fähige Geräte wären, meinen dabei jedoch nicht die Übertragungstechnologie, sondern nur die Tatsache, dass die Geräte auch XHTML-Seiten anstatt nur WML-Seiten anzeigen können. In den WAP-Einstellungen sind jedoch weiterhin die WAP-1.x-Einstellungen einzutragen.

Über WAP lassen sich Bezahldienste abwickeln. Da WAP-Seiten in der Regel nicht von anderen Internetseiten unterscheidbar sind, kann so ungewollt und unerwartet die Inanspruchnahme kostenpflichtiger Dienstleistungen erfolgen, verbunden mit einer Übermittlung der Rufnummer an den Dienstleister oder ein dazwischengeschaltetes Inkassounternehmen.

Ein ähnlicher Dienst ist i-mode. Während WAP eine standardisierte Technologie ist, ist i-mode eine proprietäre Entwicklung von NTT Docomo.

Da Mobilfunkkunden nach Einführung der WAP-Technologie zunächst kaum auf schnellere Datenübertragungsstandards wie GPRS oder HSCSD zurückgreifen konnten und die Abrechnung im Datenverkehr normalerweise auf Basis von Zeit erfolgte, erklärten Spötter die Abkürzung gerne scherzhaft mit „"W"ait "A"nd "P"ay“, englisch für „warte und bezahle“. Während die anfänglichen Geschwindigkeitsprobleme mittlerweile nicht mehr bestehen, bleibt WAP trotz eingeführter Pauschaltarife bei normaler Abrechnung pro Klick bzw. pro Datenpaket im Vergleich zu den generell deutlich gesunkenen Internetgebühren über mobile Zugänge für den angebotenen Inhalt unverhältnismäßig teuer. Das ist auch ein Grund dafür, weshalb sich die WAP-Technologie trotz teilweise nützlicher Angebote nur sehr bedingt durchgesetzt hat.





</doc>
<doc id="10218" url="https://de.wikipedia.org/wiki?curid=10218" title="Extensible Hypertext Markup Language">
Extensible Hypertext Markup Language

Der W3C-Standard Extensible Hypertext Markup Language (erweiterbare HTML; Abkürzung XHTML) ist eine textbasierte Auszeichnungssprache zur Strukturierung und semantischen Auszeichnung von Inhalten wie Texten, Bildern und Hyperlinks in Dokumenten. Es ist eine Neuformulierung von HTML 4.01 in XML: Im Gegensatz zu HTML, welche mittels SGML definiert wurde, verwendet XHTML die strengere und einfacher zu parsende SGML-Teilmenge XML als Sprachgrundlage. XHTML-Dokumente genügen also den Syntaxregeln von XML.

XHTML 1.0 enthält alle Elemente von HTML 4.01, sodass eine Umformung von HTML-4.01-konformen Seiten zu XHTML 1.0 leicht möglich ist. Ein nicht XHTML-fähiger Webbrowser kann XHTML-Dokumente unter bestimmten Bedingungen trotzdem richtig darstellen (siehe MIME-Typen und HTML-Kompatibilität): er verarbeitet sie als normales HTML. Dabei wird ausgenutzt, dass die HTML-Parser der verbreiteten Browser tolerant gegenüber Syntaxfehlern sind. Diese Fehlertoleranz entstand als Antwort darauf, dass zahlreiche HTML-Dokumente im World Wide Web nicht dem formalen Standard entsprachen und Anwender Browsermeldungen über HTML-Syntax-Fehler als lästig empfinden. Für XHTML hingegen gilt die XML-Grundidee des unkomplizierten Datenaustauschs und der problemlosen automatisierten Verarbeitung. Daher sind Programme, die XHTML verarbeiten, nicht mehr derartig tolerant.

Neuere XHTML-Dokumenttypen enthalten keine Layout-Auszeichnungen mehr. XHTML Transitional 1.0 ist der letzte Dokumenttyp, welcher noch Layout-Elemente enthält, wie etwa "<font>" oder "<b>". In moderneren Dokumenttypen wie XHTML Strict 1.0 sind zwar noch wenige Layout-Elemente enthalten, allerdings nur noch aus Gründen der Rückwärtskompatibilität zu den "Transitional"-Dokumenttypen. In XHTML Basic oder XHTML 2 schließlich sind Layout-Elemente gar nicht mehr enthalten. Für die visuelle Gestaltung von XHTML-Elementen soll nur noch auf externe CSS-Regeln verwiesen werden.

Um die Entwicklung von Sprachen zu ermöglichen, die auf XHTML basieren, wurden verwandte und zusammengehörige Elemente in XHTML 1.1 in sogenannten Modulen zusammengefasst. Basierend auf diesen in DTDs und zukünftig in XML Schema abgefassten Modulen kann man eigene XHTML-Dokumenttypen nach dem Baukastenprinzip zusammenstellen und mit anderen XML-basierten Sprachen mischen. Beispielanwendungen der XHTML-Modularisierung sind XHTML 1.1, XHTML Basic sowie die Mischungen mit SMIL (Multimedia), SVG (Vektorgrafik) und MathML (mathematischer Formelsatz). Zum Einbinden allgemeiner Objekte wie zum Beispiel Multimedia-Plug-ins gibt es das Objektmodul.

XHTML fasst als Begriff die verschiedenen XHTML-Versionen zusammen:


Außerdem:

Dies ist der Quelltext eines standardkonformen HTML-Dokuments. Das Beispiel ist bewusst so kurz wie möglich gehalten und soll Unterschiede bei der erlaubten Syntax aufzeigen. Auch in HTML empfiehlt es sich, alle notwendigen Elemente vollständig zu notieren.

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<head>

<h1>Beispielseite</h1>
<p>Ein Absatz
<P>Noch ein<br>
Absatz
<ol>
</ol>
<p><img src=bild.gif alt="Bildmotiv">
</body>

Das codice_1-Element wurde ganz weggelassen, für das codice_18- bzw. codice_19-Element fehlen End- bzw. Starttag, codice_20- und codice_21-Elemente wurden nicht geschlossen. Das codice_22-Attribut des Bildes ist ohne Anführungszeichen angegeben.

Das gleiche Dokument als gültiges XHTML 1.1 könnte folgendermaßen aussehen:

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="de">
</html>
Die XML-Deklaration codice_23 ist optional, wird aber vom W3C empfohlen, da sie XML-Parsern die Zeichenkodierung des Dokuments mitteilt. Die Zeichenkodierung UTF-8 sollte in Großbuchstaben geschrieben werden. Dies widerspricht auf den ersten Blick dem XHTML-Grundsatz, alle Elemente und Attribute klein zu schreiben. Jedoch handelt es sich um den offiziellen Namen, vergeben von der IANA, bei dessen Interpretation die XML-Parser die Groß-/Kleinschreibung ignorieren können. Wenn die Kodierungsangabe fehlt und keine Kodierung im HTTP-Header gesendet wurde, kann der Browser gemäß dem XML-Standard die Kodierung UTF-8 oder UTF-16 verwenden.

Die Angabe der XML-Deklaration führt dazu, dass der Internet Explorer 6 und Opera 7.0 bis 7.03 in den sogenannten Quirks-Modus springen, was zu Besonderheiten bei der Verarbeitung der Stylesheets und JavaScript führt. Aus diesem Grund wird die XML-Deklaration oft weggelassen, wenn das Dokument als codice_24 an diesen Browser ausgeliefert wird (siehe den folgenden Abschnitt über MIME-Typen).

Beim Übertragen von HTML- und XHTML-Dokumenten finden bestimmte MIME-Typen Verwendung, z. B. in der codice_25-Kopfzeile bei E-Mail und vor allem bei HTTP:


Vom MIME-Typ hängt ab, wie die Browser das Dokument verarbeiten. Erst wenn ein XHTML-Dokument etwa mit dem Inhaltstyp codice_27 deklariert wird, setzen XHTML-fähige Browser ihre XML-Parser ein, durch welche die Vorteile des strengen XHTML-Codes, wie etwa die einfache Verarbeitbarkeit, ausgenutzt werden. Wenn dies der Fall ist, kann das Dokument nur dann dargestellt werden, wenn es sich um wohlgeformtes XML handelt. Viele aktuelle Browser, darunter Mozilla, Mozilla Firefox, Google Chrome, Opera und Safari, unterstützen den MIME-Typ codice_27. Der weit verbreitete Internet Explorer kann mit diesem MIME-Typ jedoch erst ab Version 7.0 etwas anfangen: Ältere Versionen öffnen einen Download-Dialog, anstatt das Dokument darzustellen. Daher sollte codice_24 verwendet werden, wenn der Browser nicht ausdrücklich in der codice_33-Kopfzeile der Anfrage übermittelt hat, dass er codice_27 unterstützt. Dies kann serverseitig ermittelt werden, um den passenden MIME-Typ zu senden – wobei man für den Internet Explorer ab Version 7.0 eine Versionsprüfung durchführen müsste, da der IE nach wie vor codice_35 als MIME-Typ sendet.

Mit HTML 4 begann das W3C damit, diejenigen Elemente und Attribute, die direkt für die Präsentation des Dokuments zuständig waren und keine ausgabe-unabhängige Strukturierung ausdrückten, schrittweise aus HTML auszuschließen. Zwar enthält XHTML 1.0 genauso wie HTML 4 eine Transitional-Variante mit diesen veralteten Sprachbestandteilen. Es hat sich jedoch im modernen Webdesign durchgesetzt, dass die Strict-Variante verwendet wird und die Dokumente konsequent mit CSS formatiert werden. Der strukturierte Inhalt und das jeweilige Layout können dadurch getrennt definiert werden. Mit XHTML 1.1 und dem geplanten XHTML 2.0 wollte das W3C diese Entwicklung endgültig abschließen, indem nur noch ausgabe-unabhängige Textauszeichnung erlaubt und das Layout zwangsläufig mit CSS oder ähnlichen Sprachen realisiert werden sollte.

HTML-Seiten besitzen als SGML-Sprache zwar eine genau definierte Struktur, die in der Dokumenttypdefinition (DTD) festgelegt ist. Ohne Kenntnis der DTD kann aber die hierarchische Baumstruktur eines Dokuments nicht zweifelsfrei ermittelt werden. Manche Elemente haben keinen End-Tag (wie z. B. codice_36 für einen Zeilenumbruch) oder ein optionales End-Tag (wie codice_37 für einen Textabsatz). Nur die DTD bestimmt, welche Elemente dies sind. Kennt der Parser sie nicht, so ist die Dokumenthierarchie mehrdeutig. XHTML als XML-Sprache behebt diesen Mangel.

HTML ist eigentlich nicht erweiterbar, übliche Browser gehen aber bei der HTML-Verarbeitung folgendermaßen vor:


Damit ist die Verarbeitung von verschiedenen HTML-Versionen möglich. Führt eine Version ein neues Element ein, wird dieses von älteren Browsern einfach ignoriert. Gleiches gilt für Attribute. Kennt ein HTML-3.2-fähiger Browser beispielsweise das in HTML 4.0 eingeführte codice_38-Element für Abkürzungen nicht, wird es überlesen und die Abkürzung erscheint in normaler Textformatierung. Dasselbe gilt für browserspezifische Erweiterungen. Das codice_39-Element ist beispielsweise in keinem HTML-Standard enthalten. Einige Browser, ursprünglich nur der Netscape Navigator, stellen den Text dann blinkend dar. Andere Browser stellen den Text wiederum normal dar.

Im Gegensatz zu HTML wurde XHTML mit Blick auf die Möglichkeit der Erweiterbarkeit geschaffen. XHTML nutzt dazu das Namensraum-Konzept von XML. Eine XHTML-Version bildet dabei einen solchen Namensraum. Andere XML-Sprachen wie MathML, SVG und RDF stellen weitere Namensräume dar. In einem XHTML-Dokument können nun Elemente aus fremden Namensräumen verwendet werden, indem Sie über das Attribut codice_40 eine entsprechende Namensraum-Angabe erhalten.
Dazu muss ein spezieller Doctype verwendet werden, welcher die Elemente definiert - bei Verwendung von MathML ist es:
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN"
"http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
Ein Beispiel für die Nutzung des Namensraum-Konzeptes zur Erweiterung von XHTML ist die Einbettung von MathML:
<p>Dies ist noch ganz normales XHTML</p>
formula_1
<p>...und hier geht XHTML weiter</p>
Ein MathML-fähiger Browser könnte diesen Dokumentausschnitt folgendermaßen darstellen:

Erweiterungen sind also durch Bildung von neuen Namensräumen möglich, ohne dass die XHTML-Standards selbst geändert werden müssen. Durch die Verwendung von Namensräumen ist ein Konflikt von Elementen gleichen Namens in verschiedenen Erweiterungen ausgeschlossen. Diese können immer eindeutig zugeordnet und zum Beispiel über das DOM mit der Kennung des Namensraums angesprochen werden. Die infolge der XHTML-Modularisierung entstandenen erweiterten XHTML-Versionen bauen auf diesem Konzept auf.

Durch das Entstehen solcher Erweiterungen tritt eine ähnliche Situation wie bei HTML-Erweiterungen ein, denn nicht alle Browser unterstützen die eingebundenen Erweiterungen wie zum Beispiel SVG. Der Browser hat folgende Möglichkeiten, mit Elementen aus unbekannten Namensräumen umzugehen:




</doc>
<doc id="10220" url="https://de.wikipedia.org/wiki?curid=10220" title="Dateiname">
Dateiname

Ein Dateiname identifiziert eine Datei in einem Dateisystem auf einem Datenträger oder bei einer Datenübertragung. Meist wird eine Datei zusätzlich durch einen Verzeichnisnamen charakterisiert, sodass ein vollständiger Pfadname entsteht. Erst diese Kombination zu einem vollständigen Pfadnamen ist in der Regel eindeutig. Pro Verzeichnis (Ordner) ist eine identische Benennung von zwei Dateien nicht möglich; unter einigen Betriebssystemen, z. B. Unix, qualifiziert auf einem entsprechenden Dateisystem eine veränderte Groß- und Kleinschreibung einen anderen Dateinamen, so dass z. B. codice_1 und codice_2 zwei unterschiedliche – und somit nicht identische – Benennungen sind.

Die optionale Dateinamenserweiterung ist Teil des Dateinamens.

Ein Dateiname kann – abhängig vom jeweiligen Betriebssystem – aus mehreren Teilen bestehen. Die einzelnen Teile sind durch bestimmte Zeichen, die in der Regel nicht Teil des Dateinamens sein können, getrennt; die Liste von Dateinamenserweiterungen verschafft einen Überblick.

Einige Betriebssysteme machen die Behandlung der Dateien allein von der jeweiligen Dateinamenserweiterung abhängig. Andere Betriebssysteme erkennen den Dateityp anhand des Inhalts – beispielsweise anhand einer sogenannten magischen Zahl, mit deren Hilfe sich ein bestimmter Dateityp relativ zuverlässig bestimmen lässt, oder mittels im zur Datei zugehörigen alternativen Datenstrom gespeicherten Daten, etwa zum Dateityp und zum erstellenden Programm. Doch auch auf Systemen, die nicht von einer Dateinamenserweiterung den Dateityp ableiten, werden Dateinamen damit versehen – u. A. da es den Datenaustausch vereinfacht.

Die maximale Länge eines Dateinamens wird sowohl durch das Betriebssystem als auch durch das Dateisystem des Datenträgers begrenzt. So können etwa auf einer CD-ROM bei Verwendung des Joliet-Dateisystems maximal 64 Zeichen genutzt werden. Eine indirekte Begrenzung kann zudem durch eine maximale Länge von Pfadnamen im Betriebssystem entstehen.

Ein Unterschied zwischen Windows und Unix (wie auch Linux) besteht darin, dass Windows bei Dateinamen nicht zwischen Groß- und Kleinschreibung unterscheidet (), während Unix dies tut (). So sind unter Unix in der Regel die Dateien codice_3 und codice_4 zwei unterschiedliche Dateien, während dies unter Windows nicht möglich ist.

Dateisysteme haben bestimmte interne Strukturen, die meist der des Referenzsystems entsprechen, für das das Dateisystem entwickelt wurde. So besitzt ein Dateisystem meist keine Rechteverwaltung, wenn das Betriebssystem diese ebenfalls nicht kennt. Ein Beispiel dafür ist das Dateisystem FAT von PC DOS bzw. MS-DOS: da DOS selbst kein Mehrbenutzersystem ist speichert es auch keine Zugriffsrechte für Dateien und Verzeichnisse. Ebenso verhält es sich mit Erstellungs- und Zugriffszeiten von Dateien, die in der systemüblichen Weise entweder als Lokalzeit oder als Universalzeit abgespeichert werden, oder bei der Konvention von Groß- und Kleinschreibung.

Im Englischen wird die Unterscheidung zwischen Groß- und Kleinbuchstaben „“ bezeichnet. Ein Betriebssystem, das diese Unterscheidung macht, sieht codice_1 (mit groß geschriebenem Anfangsbuchstaben) als anderen Dateinamen als codice_2 (alles in Kleinbuchstaben), was wiederum eine andere Datei darstellt als codice_7 (alles in Großbuchstaben). Unix-Betriebssysteme sind traditionell „,“ das heißt, sie unterscheiden zwischen großen und kleinen Buchstaben.

Viele weitere Betriebssysteme, wie etwa CP/M, PC DOS und MS-DOS sowie kompatible DOS-Betriebssysteme, aber auch Mac OS, Mac OS X und AmigaOS, unterscheiden nicht zwischen Groß- und Kleinbuchstaben, was Auswirkungen auf das verwendete Dateisystem hat. Unter MS-DOS sind daher die Dateinamen codice_1 und codice_2 ein und dieselbe Datei. Das FAT-Dateisystem speichert die Datei zudem immer in Großbuchstaben, also codice_7, ab. Mit VFAT hingegen wird die Datei in der bei der Erstellung verwendeten Schreibweise gespeichert, beim Zugriff hingegen wird nicht unterschieden. Ebenso geht das Amiga-„“ vor. Das (HFS) von Apple kann zwar „“ sein, ist dies aber aufgrund der Kompatibilität zu bestehenden Anwendungen nicht. Es hätte bei der Entwicklung von HFS+ einen Bruch mit bestehenden Macintosh-Applikationen bedeutet, weshalb auch unter OS X noch immer die codice_11-Variante von HFS+ als Standard-Dateisystem fungiert, die zwischen Groß- und Kleinbuchstaben nicht unterscheidet („“). Unter iOS ist das HFS+-Dateisystem hingegen standardmäßig „“ (codice_12).

Das Dateisystem FAT (in den Varianten FAT12, FAT16 und FAT32) ist auf fast allen Betriebssystemen implementiert. Weil das Dateisystem aber bestimmte Annahmen bezüglich des zugrunde liegenden Systems macht – und Dateinamen nur in Großbuchstaben speichert, werden diese z. B. auf unixoiden Betriebssystemen vom Dateisystemtreiber standardmäßig in Kleinbuchstaben umgewandelt. Mit der VFAT-Erweiterung wird diese Umwandlung nur dann vollzogen, wenn der Dateiname in der 8.3-Konvention und in Großbuchstaben im FAT vorliegt. Andere Erweiterungen für FAT, wie UMSDOS, implementieren eigenständig eine solche Umwandlung, wenn die Datei nicht auf Unix gespeichert wurde und im UMSDOS-Format vorgehalten wird. Da Unix/Linux allerdings zwischen Groß- und Kleinbuchstaben unterscheidet, wird eine Datei nur in der umgewandelten oder in der im VFAT gespeicherten Schreibweise erkannt.

Ein Beispiel: Unter Windows (mit VFAT, also ab Windows 95) wird eine Datei mit dem Namen codice_13 gespeichert. Diese wird unter Windows, weil Windows „“ ist, auch in einer anderen Schreibweise erkannt, beispielsweise in der Eingabeaufforderung mit codice_14 gelöscht. Unter Linux wird genau diese Datei jedoch nur in der genauen Schreibweise codice_13 erkannt. Will man beispielsweise die Datei mit codice_16 anzeigen und vertippt sich z. B. bei der Dateinamenserweiterung: codice_17, so gibt Linux die Fehlermeldung aus, dass die Datei nicht existiert.

Es kommt also nicht nur auf das Dateisystem an, sondern auch auf das Betriebssystem, und wie es mit den im Dateisystem vorgehaltenen Informationen (Dateiname, Rechte, Datum) umgeht. Unter Umständen kann z. B. ein Dateisystemtreiber zwar auf die Eigenheiten des Dateisystems eingehen, die Betriebssystemumgebung verhindert dies jedoch. Ein Beispiel ist der Umgang einer Unix-Shell mit Wildcards. Unter Linux ist im Dateisystemtreiber für AFFS die „case-"in"sensitivity“ eingebaut, nicht jedoch in der Shell. Ist auf einem Amiga-„“ beispielsweise der Dateiname codice_18 (mit teilweise Großbuchstaben) gespeichert, so kann dieser in der Unix-Shell mit codice_19 (alles in Kleinbuchstaben) gelöscht werden, weil der Dateisystemtreiber die Umwandlung vollzieht. Gibt man jedoch codice_20 ein, wird codice_18 nicht gelöscht, weil die Unix-Shell die Suche nach dem Dateinamen vollzieht – da diese „“ ist wird keine Übereinstimmung gefunden, weil die Shell strikt zwischen Groß- und Kleinbuchstaben unterscheidet.

Auch die Zugriffsrechte und Besitzer von Dateien können auf unterschiedlichen Betriebssystemen entweder übernommen oder vollkommen ignoriert werden. Der FUSE-Dateisystemtreiber NTFS-3G beispielsweise unterstützt die Zugriffsbeschränkung, wenn die betreffende Windows-Benutzer-SID zuvor einem Unix-Benutzer zugeordnet wurde ().

Unix- und Unix-ähnliche Betriebssysteme wie zum Beispiel Solaris oder Linux betrachten Dateinamen als Ganzes. Eine Datei kann mehrere Namen haben und sich in mehreren Verzeichnissen befinden („“ oder „ “). Alle Zeichen außer dem Schrägstrich und dem Nullzeichen sind erlaubt. Frühe Versionen hatten 1 bis 14 Zeichen lange Dateinamen. Die BSD-Varianten führten bis zu 255 Zeichen lange Namen ein.

Ein relativer Dateipfad kann aus mehreren Segmenten bestehen und beginnt mit einem Segment. Jedes Segment unterliegt den Regeln des Dateinamens, kann also 14 bzw. 255 Zeichen lang sein. Die Segmente der Dateipfade werden durch das Zeichen getrennt. Das letzte Segment kennzeichnet die eigentliche Datei. Die vorhergehenden Segmente sind entweder Verzeichnisnamen oder symbolische Verweise (englisch „symbolic links“) auf Verzeichnisnamen. Ein relativer Dateipfad geht vom aktuellen Arbeitsverzeichnis aus, das jeder Prozess individuell setzen kann.
Ein absoluter Dateipfad beginnt hingegen bereits mit und ist unabhängig vom aktuellen Arbeitsverzeichnis. Er geht vom Wurzelverzeichnis aus. Über das Wurzelverzeichnis sind alle Dateien eines Systems erreichbar.

Beim Zugriff wird zwischen Groß- und Kleinschreibung unterschieden.

Beispiele:

Der Dateiname codice_22 (Punkt) bezeichnet das aktuelle Arbeitsverzeichnis. Der Name codice_23 verweist auf das übergeordnete Verzeichnis.

Auch das Leerzeichen, der Zeilentrenner oder die sogenannten Wildcards codice_24 und codice_25 können Teil eines Pfadnamens sein. Solche Zeichen bringen allerdings manchmal später Probleme mit sich, da zum Beispiel schlecht programmierte Skripte damit nicht umgehen können.
Weiterhin kann es Probleme mit Dateinamen geben, die Zeichen enthalten, die im aktuell verwendeten Zeichensatz eines Programms nicht vorkommen (zum Beispiel japanische Zeichen auf einem amerikanisch eingerichteten System). Die nicht darstellbaren Zeichen werden dann oft als Fragezeichen oder kleine Kästchen angezeigt, was den Zugriff auf die Daten sehr schwierig macht. Diese Dateien können dann oft nur bearbeitet werden, nachdem sie auf einer niedrigen Dateisystem-Abstraktionsebene umbenannt wurden (zum Beispiel durch Angabe der sogenannten inode statt des Dateinamens mit codice_26 und codice_27).

Ein Unix-System verwendet keine speziellen Erweiterungen, wie codice_28 oder codice_29. Es hat sich allerdings eingebürgert, Dateien eines bestimmten Types, wie in anderen Betriebssystemen, auch mit einem Punkt und einer entsprechenden Erweiterung zu versehen, um die Übersichtlichkeit zu erhöhen. Beispielsweise wird die Endung codice_30 für C-Quellprogramme verwendet. Ausführbare Dateien, also Programme und Skripte, erhalten keine Endung. Dateitypen können ansonsten mit dem einfachen Programm codice_31, unabhängig von einer eventuell vorhandenen Erweiterung ermittelt werden.

Dateien oder Verzeichnisse, deren Namen mit einem Punkt beginnen, werden üblicherweise als „versteckte“ Dateien behandelt und nur angezeigt, wenn der Benutzer dies explizit angibt (zum Beispiel mit codice_32).

Ähnliches gilt für Verzeichnispfade.

Dateinamen bestehen unter CP/M sowie den verschiedenen PC-kompatiblen DOS-Versionen inkl. MS-Windows bis zur Version 3.11 aus einem maximal acht Zeichen umfassenden eigentlichen „Namen“ sowie optional einem Punkt und einer maximal drei Zeichen umfassenden „Erweiterung“ (), die auch den Typ der betreffenden Datei angibt (siehe 8.3). Erweiterungen werden oft von Programmen vergeben bzw. für Programme reserviert, zum Beispiel die Erweiterung codice_33 für Textdateien. Auch die Betriebssysteme selbst verwenden spezielle Erweiterungen wie beispielsweise codice_34 für Skriptdateien, codice_35 für Treiberdateien oder codice_28 und codice_37 für ausführbare Dateien.

Ein Dateiname inklusive Erweiterung darf aus folgenden Zeichen bestehen:

Die folgenden Zeichen sind dabei, da sie in den genannten Systemen syntaktische Funktionen erfüllen, in Dateinamen und Erweiterungen nicht erlaubt:

Außerdem sind einige Worte reserviert und dürfen nicht als Dateiname benutzt werden, da sie als Gerätenamen verwendet werden:

Dadurch kann man unter klassischem DOS zum Beispiel die folgenden Dateinamen, die unter anderen Betriebssystemen zulässig sein können, nicht benutzen: codice_40, codice_41, codice_42.

Verzeichnisnamen werden unter den genannten Betriebssystemen wie normale Dateinamen gehandhabt. Sie haben üblicherweise keine Erweiterung, können jedoch mit einer solchen versehen werden. Diese hat dann in der Regel, anders als beim Namen von sonstigen Dateien, keine Funktion. Jede Datei und jedes Verzeichnis befindet sich auf einem Laufwerk, welches durch einen Buchstaben und einen Doppelpunkt gekennzeichnet wird. Ein vollständiger Name besteht aus dem Laufwerk, optional einem oder mehreren Verzeichnisnamen und dem eigentlichen Dateinamen. Die genannten Bestandteile werden durch das Verzeichnistrennsymbol voneinander getrennt.

Da nur acht Zeichen zur Verfügung stehen, werden die Bezeichnungen oft verstümmelt. Die Namen codice_22 und codice_23 sind wie unter Unix für das aktuelle Verzeichnis und das übergeordnete Verzeichnis reserviert.

Beim Zugriff wird "nicht" zwischen Groß- und Kleinschreibung unterschieden.

Unter Windows (Windows 95, 98, ME, NT, 2000, XP, Vista, 7, 8) besteht ein Dateiname aus dem Namen, einem Punkt und einer Erweiterung, die den Dateityp festlegt. Es können auch mehrere Punkte in einem Dateinamen angegeben werden, der letzte Punkt dient dann zur Trennung von Name und Erweiterung.

Normalerweise ist die Pfadlänge unter Windows auf 260 Zeichen beschränkt, d. h. drei Zeichen für die Laufwerksangabe, 256 Zeichen für den Pfad innerhalb des Laufwerks und ein nicht sichtbares String-Terminierungszeichen. Längere Pfade bis zu 32.767 Zeichen, wie sie von NTFS unterstützt werden, sind mittels UNC (Uniform Naming Convention) möglich, d. h. codice_45 muss vorangestellt werden.

Zur Wahrung der Kompatibilität mit alten MS-DOS-Programmen kann der Dateiname auch in der 8.3-Notation angegeben werden, wenn dies in Windows nicht deaktiviert wurde. Dabei wird der Dateiname eindeutig mit acht Zeichen für den Namen, einem Punkt und bis zu drei Zeichen für die Dateierweiterung dargestellt, welche in jedem Verzeichnis neu generiert werden. Wenn Dateien ihren langen Dateinamen verloren haben, sie also nur diesen spezifischen Kurznamen haben, kann es zu Konflikten mit schon existierenden Dateien mit langem Dateinamen kommen, deren Dateiname auf denselben Namen verkürzt wurde, auch wenn sie vorher in einem anderen Verzeichnis problemlos koexistierten. (→8.3)

In Dateinamen und Erweiterungen sind, wie schon unter DOS und Windows bis Version 3.11, folgende Zeichen nicht erlaubt:
Ebenfalls unzulässig sind folgende, wie schon zuvor als Gerätenamen reservierte Dateinamen:

Dadurch kann man auch unter den neueren Windows-Versionen zum Beispiel folgende Dateinamen, die unter anderen Betriebssystemen erlaubt sein können, nicht benutzen: codice_40, codice_41, codice_42.

Problematisch sind außerdem Dateinamen, die das eigentlich erlaubte enthalten, das von der DOS-Umgebung unter Windows jedoch als Trennzeichen einzeiliger Befehlsketten verwendet wird, so dass alles auf ein -Zeichen Folgende als eine weitere DOS-Befehlszeile interpretiert wird. In der Konsequenz gibt die Windows-Eingabeaufforderung daher in diesem Fall eine Fehlermeldung aus, dass sie einen Befehl nicht finden oder ausführen konnte, dessen Name der Rest des eingegebenen Dateinamens nach dem -Zeichen ist, ganz zu schweigen davon, dass die fragliche Datei selbst natürlich auch nicht geöffnet oder bearbeitet werden konnte.

Zusätzlich sind auch Dateinamen problematisch, die "am Ende ein Leerzeichen" haben. Diese kann man unter Windows nicht anlegen; werden sie unter anderen Betriebssystemen erstellt, kann man auf sie unter Windows nicht zugreifen, da Windows die Leerzeichen am Ende einfach abschneidet. Autoren von Schadprogrammen haben dies bereits ausgenutzt, da dadurch Anti-Virenprogramme nur durch besondere Maßnahmen auf solche Dateien zugreifen können.

Ansonsten können alle im Unicode-Standard definierten Zeichen benutzt werden, wobei in der Praxis ältere Applikationen oft mit Zeichen Schwierigkeiten haben, deren Code nicht im Windows-1252-Zeichensatz enthalten ist.

Unter VMS (Virtual Memory System) besteht ein Dateiname aus dem Namen, einem Punkt, einer Erweiterung, einem Semikolon und einer Versionsnummer. Die Versionsnummer wird bei jeder Neuanlage einer gleichnamigen Datei (mit Erweiterung) automatisch um Eins erhöht. Dadurch kann man mehrere Versionen (Anzahl ist einstellbar, maximal 32.767) derselben Datei gleichzeitig halten. Die folgenden Angaben gelten für ODS-2 ():

Dateinamen können maximal 39 Zeichen lang sein, wobei nur bestimmte Zeichen (Buchstaben, Ziffern, Unterstrich, Dollarzeichen) erlaubt sind. Es wird nicht zwischen Groß- und Kleinschreibung unterschieden. Die Erweiterung kann ebenfalls 39 Byte lang sein, wird durch einen Punkt getrennt und ist nicht Teil des Dateinamens. Außer bei Verzeichnissen, wo die Erweiterung immer codice_49 lautet, hat sie aber keine Bedeutung für die mögliche Verwendung der Datei (es gibt aber Standards, die bei einigen Dateitypen üblicherweise eingehalten werden).

Die Gesamt-Pfadlänge (also Disk, Verzeichnisbaum, Dateiname, Erweiterung und Version) darf 255 Bytes nicht überschreiten.

Die Übertragung von Dateien im World Wide Web ist durch den HTTP-Standard geregelt. Enthält ein Dateiname Zeichen außerhalb der ASCII-Buchstaben und -Ziffern, so werden diese in der URL in einer %-Darstellung codiert mit einem Prozentzeichen, gefolgt von einem Zwei-Zeichen-Code in hexadezimaler Form, etwa codice_50 statt codice_51. Um den Codewert ermitteln zu können, ist die Kenntnis der Zeichencodierung (zum Beispiel UTF-8 oder ISO 8859-1) des Dateinamens nötig.

Der FTP-Standard sieht nur ASCII-Zeichen als zwingend unterstützt vor. Oft wird ein Dateidownload allerdings auch unter Benutzung von HTTP durchgeführt.

Die Übertragung von Dateianhängen (und damit auch die dort zulässigen Dateinamen) ist in den Standards SMTP und MIME geregelt.


</doc>
<doc id="10223" url="https://de.wikipedia.org/wiki?curid=10223" title="Galeon">
Galeon

Galeon ist ein freier Webbrowser der primär für die Integration in die Desktop-Umgebung Gnome gedacht war. Die Entwicklung wurde 2008 eingestellt.

Zum Anzeigen der Webseiten verwendet er Gecko, die HTML-Rendering-Engine von Mozilla. Ein Unterschied im Vergleich zu offiziellen Mozilla-Browsern besteht darin, dass anstatt der XUL- eine reine GTK+-2-Oberfläche eingesetzt wird. Die Weiterentwicklung ist seit September 2008 endgültig eingestellt. Zielgruppe von Galeon waren vor allem Benutzer, die einen in Gnome integrierten und im Vergleich zum offiziellen Gnome-Browser Epiphany funktional vielfältigeren Browser bevorzugten. Im Unterschied zu Epiphany, der seinerseits eine Abspaltung von Galeon ist, richtet sich Galeon daher weniger an den Gnome Human Interface Guidelines (HIG) aus und bietet zahlreiche Einstellmöglichkeiten und Funktionen.

Die erste stabile Version von Galeon, Galeon 0.6, wurde im Juni 2000 veröffentlicht. Im November 2001 folgt die Version Galeon 1.0. Mit Galeon 1.3 wird im Oktober 2002 die erste Gnome-2-Version veröffentlicht.

Nach Meinungsverschiedenheiten über die Designziele des Projekts und die Zukunft des Browsers verlässt der führende Entwickler Marco Pesenti Gritti im November 2002 das Galeon-Team und fängt mit Epiphany ein neues Projekt an. Drei Jahre später, im Oktober 2005, wird beschlossen, dass Galeon nicht mehr eigenständig weiterentwickelt wird, sondern die besonderen Galeon-Funktionen sollen bei Epiphany als Erweiterungen eingebracht werden.

Im November 2005 erscheint Galeon in Version 2.0.0., im September 2006 folgt die Version 2.0.2 und im September 2008 erscheint die finale Version 2.0.7.




</doc>
<doc id="10226" url="https://de.wikipedia.org/wiki?curid=10226" title="Netiquette">
Netiquette

Unter der Netiquette (auch Netikette geschrieben; ein Kofferwort aus dem englischen ' für das „Netz“ und dem französischen ' für die „Verhaltensregeln“) versteht man das gute oder angemessene und achtende "(respektvolle)" Benehmen in der technischen (elektronischen) Kommunikation. Der Begriff beschrieb ursprünglich Verhaltensempfehlungen im Usenet, er wird aber mittlerweile für alle Bereiche in Datennetzen verwendet. Wenn auch von vielen Netzteilnehmern als sinnvoll erachtet, hat die Netiquette meist keinerlei rechtliche Relevanz. Teilaspekte der Netiquette werden häufig kontrovers diskutiert. Was im Netz als guter Umgang miteinander (noch) akzeptiert wird, ist sehr unterschiedlich und hängt von den Teilnehmern innerhalb des Kommunikationssystems ab, wobei es in der Hand des jeweiligen Betreibers/Verantwortlichen liegt, Art und Ausmaß der Netiquette vorzugeben, deren Einhaltung zu kontrollieren und Verstöße ggf. durch Ausschluss von Teilnehmern negativ zu sanktionieren.
Es gibt keinen einheitlichen Netiquettetext, sondern eine Vielzahl von Texten und Empfehlungen.

Ziel der Netiquette ist eine möglichst für alle Teilnehmer angenehme Art der Kommunikation. Ein im Netz weitgehend anerkanntes Dokument hierzu ist etwa RFC 1855. Einige Beispiele für weit verbreitete Regeln:


Die erste und grundlegende Empfehlung der Usenet-Netiquette ist:
Einzelne Empfehlungen der Netiquette werden manchmal kritisiert, etwa die Forderung nach einem Realnamen, nach der es im deutschsprachigen Usenet als unhöflich galt, unter einem falschen Namen (Codename beziehungsweise Pseudonym) zu posten. In vielen Foren und zum Teil auch im Usenet hat diese Empfehlung seit etwa Anfang 2000 an Bedeutung verloren. Seither wird die anonyme Teilnahme an einem Forum zunehmend akzeptiert, ganz besonders dann, wenn sie aufgrund des Themas oder der Art der Diskussion wünschenswert oder notwendig erscheint. In vielen Foren, beispielsweise in größeren Mailingslisten, wird man aber immer noch dem Realnamen den Vorzug geben.

Vorsicht ist bei der Verwendung von Crosspostings angeraten. Gänzlich verpönt sind Multipostings.

Personen, die sich – manchmal im übertriebenen Maße – freiwillig der Kontrolle der Netiquette-Einhaltung widmen, heißen oft abwertend Netcops.

In den meisten deutschsprachigen Foren hat sich außerdem das Duzen als Form der Ansprache durchgesetzt. Siezt man, kann das als Ausdruck von Distanz verstanden werden. Zum Vergleich: In Frankreich etwa wird durchgehend gesiezt. In manchen anderen Sprachen, wie zum Beispiel dem Englischen, existiert dieses Problem nicht oder nur bedingt. Mit der Anrede „you“ wird zwar nicht direkt zwischen „du“ und „Sie“ unterschieden, sehr wohl aber durch eine Anrede per Vor- oder Nachnamen bzw. Mr./Mrs., akademischem Titel usw.

Im Chat wird Netiquette zu "Chatiquette". Die Anonymität eines Chats verleitet immer wieder Teilnehmer zu Äußerungen, die sie in nichtelektronischen Kommunikationsformen unterlassen würden. Diese reichen von penetranten Flirtversuchen und Unfreundlichkeiten über Pöbeleien bis zu Beleidigungen. Um Chattern Anhaltspunkte für das angemessene Verhalten in einem Chat zu geben, wurden viele verschiedene Chatiquetten geschrieben, die sich in den wichtigsten Punkten jedoch alle ähneln: Beleidigungen, rassistische Äußerungen und ständige Pöbeleien gelten beispielsweise als unerwünscht. Ebenfalls nicht gerne gesehen ist in Großbuchstaben oder auffällig vergrößerter Schrift zu schreiben, da dies als Schreien verstanden wird.

Chatbetreiber achten meist auf diese Punkte und ahnden Verstöße auch, zum Beispiel mit der Sperrung des Teilnehmers.

Im Jahr 2010 veröffentlichte der "Deutsche Knigge-Rat", ein privater Kreis, der in Anlehnung an den klassisch gewordenen Ratgeber Über den Umgang mit Menschen von Adolph Freiherr Knigge gegründet wurde, Höflichkeitsregeln für den Umgang in Sozialen Netzwerken. 

Das zwölf Punkte umfassende Programm des „Social-Media-Knigge“ zeigt, das die Sozialen Medien eine andere Form von Vernetzung hervorbringen als Communities in den bis dahin üblichen Newsgroups, Mailinglisten oder Webforen, in denen der Schwerpunkt auf der Diskussion von Themen lag, während es in Sozialen Medien vorwiegend um die Verknüpfungen geht, die einen persönlichen Kontakt virtuell herstellen, erhalten und ausfüllen. Dementsprechend steht bei der Netiquette für diesen Bereich der gesellschaftliche Umgang miteinander im Vordergrund und weniger die rein technische Bedienung der Software, beispielsweise des Newsreaders, um einen möglichst leicht lesbaren Beitrag zu erstellen. Die Bedienoberfläche ist ohnehin für alle Benutzer gleich und nicht zu ändern.

Man empfahl vor allem die sorgfältige Auswahl der Netzwerke, die man nutzt, warnte aber auch vor allzu „plumpen Vertraulichkeiten“ und riet zur Distanz bei Freundesanfragen: „Ihre Kunden sind nicht unbedingt Ihre ‚Freunde‘ und empfinden diese Bezeichnung vielleicht als unpassend oder zu intim.“ Unerwünschte Anfragen sollte man ablehnen, Belästigungen vermeiden und Trolle ausschließen, insgesamt solle man versuchen, lebendig und humorvoll zu bleiben. Bei allem, was man auf Sozialen Netzwerken tue, möge man sich fragen: „Möchte ich, dass meine Meldung auch in zwei Jahren gefunden und gelesen werden kann?“

Zwei Jahre später, im Jahr 2012, folgten Empfehlungen zur Wahrung der Privatsphäre auf Sozialen Netzwerken in einem sogenannten „Privacy-Knigge“. Im Vordergrund standen dabei Ratschläge zur Kontrolle der virtuellen Privatsphäre durch die Konfiguration der Einstellungen des eigenen Kontos bei dem Sozialen Netzwerk. Man möge sich genau überlegen, „ob man seine politischen Ansichten, sexuelle Orientierung oder seinen Familienstand für jedermann öffentlich machen möchte“ und man möge die Prangerwirkung öffentlicher Äußerungen im Internet bedenken.




</doc>
<doc id="10228" url="https://de.wikipedia.org/wiki?curid=10228" title="Samuel Hahnemann">
Samuel Hahnemann

Christian Friedrich Samuel Hahnemann (* 10. April 1755 in Meißen; † 2. Juli 1843 in Paris) war ein deutscher Arzt, medizinischer Schriftsteller und Übersetzer. Er ist der Begründer der Homöopathie.

Hahnemann wurde als drittes Kind von Christian Gottfried Hahnemann und seiner zweiten Frau Johanna Christiane Spieß in der Meißner Triebischvorstadt geboren. Sein Vater war ein Porzellanmaler in der berühmten Meißener Porzellanmanufaktur.

Samuel Hahnemann besuchte die Meißener Stadtschule, wobei der verarmten Familie des begabten Schülers das Schulgeld erlassen wurde, und bekam danach ein Stipendium an der Fürstenschule St. Afra in Meißen, wo er ab 1770 zur Schule ging. Nach dem Schulabschluss begann er 1775 ein Medizinstudium in Leipzig. Seinen Lebensunterhalt verdiente er in dieser Zeit mit Sprachunterricht und mit Übersetzungen physiologischer und medizinischer Werke ins Deutsche. 1777 wechselte Hahnemann für ein Dreivierteljahr an die Wiener Universität, wo er bei dem Medizinprofessor und ärztlichen Direktor des Spitals der Barmherzigen Brüder Joseph Freiherr von Quarin, der auch als Leibarzt von Maria Theresia tätig war, Unterricht am Krankenbett und bei Visiten erhielt, bis ihm das Geld ausging.
Im Oktober 1777 bot ihm, empfohlen durch Quarin, Freiherr Samuel von Brukenthal, eben von Kaiserin Maria Theresia zu ihrem Statthalter in Siebenbürgen ernannt, eine Stelle als Bibliothekar, Hauslehrer und Leibarzt an. Hahnemann begleitete ihn nach Hermannstadt und blieb dort knapp zwei Jahre lang. Er hat dort offenbar zahlreiche Fälle von Wechselfieber (Malaria) gesehen, es gibt auch Hinweise darauf, dass er selbst daran erkrankt ist (was für seinen Chinarindenversuch von Bedeutung ist). 1777 wurde er in die Freimaurerloge "Zu den drei Seeblättern" Hermannstadt aufgenommen. Hahnemann sortierte und erfasste auch die umfangreiche Münzsammlung Samuel von Brukenthals. Zudem verdingte er sich erneut als Hauslehrer, um den Lebensunterhalt sicherzustellen. Da es in Hermannstadt keine Universität gab, schloss Hahnemann im August 1779 sein Medizinstudium an der Friedrichs-Universität Erlangen mit der Promotion ab.

In den Folgejahren praktizierte er als Arzt, Chemiker, Übersetzer und Schriftsteller in vielen nord- und mitteldeutschen Städten – mit wechselndem Erfolg. Zeitweise gab er seine ärztliche Praxis völlig auf, „weil sie mir mehr Aufwand gekostet, als Einnahme gebracht, und gewöhnlich mich mit Undank belohnt hat“ (Brief vom 29. August 1791, zit. nach Jütte, S. 48), und widmete sich ganz chemischen Versuchen, Übersetzungen und Publikationen. Zu anderen Zeiten wiederum hatte er so viele Patienten, dass er kaum mit der Versorgung nachkam: „Fast hätte ich nicht schreiben können, weil ich in den wenigen Wochen, die ich in Eilenburg wohne, schon so mit Kunden gesegnet bin, dass ich oft nicht essen kann“ (Brief vom 18. September 1801, zit. nach Jütte, S. 74). Seine psychotherapeutischen, chemischen und schriftstellerischen Aktivitäten brachten ebenfalls recht wechselhafte Resultate, wie unten deutlich wird.

Die Gründe für Hahnemanns unstetes Umherziehen dürften vielfältig sein. Anthony Campbell fasst sie so zusammen: „… wurde weitergetrieben durch seinen ruhelosen Geist und die Notwendigkeit, einen Lebensunterhalt zu erwirtschaften“. Es war für einen unbemittelten, freischaffenden Geistesarbeiter wie Hahnemann nicht leicht, sich und bald auch seine schnell wachsende Familie zu ernähren; andererseits gehörte ein ausgeprägter Ehrgeiz, welcher ihn zu diversen Experimenten trieb, zu Hahnemanns Charaktereigenschaften, wie schon Zeitgenossen bemerkten. Schließlich gab es oftmals Streit, besonders häufig mit Apothekern, wegen Hahnemanns „interdisziplinärer“ Tätigkeit als Chemiker bzw. Pharmazeut und Arzt (der Leipziger Dispensierstreit, s. u., ist nur eines von vielen Beispielen).

Ein Beispiel für die ökonomischen Probleme und den Ehrgeiz des Schriftstellers, Übersetzers und Chemikers, die häufige Ortswechsel begünstigten:

Zunächst ließ sich Hahnemann in Hettstedt, dann in Dessau nieder, wo er 1782 die Apothekerstochter Johanna Leopoldine Henriette Küchler (1764–1830) heiratete; die beiden hatten zusammen insgesamt elf Kinder. Die nächste Station war Gommern bei Magdeburg, 1785 begann Hahnemann in Dresden zu praktizieren. Neben einer ganzen Reihe von Übersetzungen aus dem Englischen und Französischen (hauptsächlich medizinischer Fachbücher, aber auch eines umfangreichen belletristischen Werkes) begann er dort regelmäßig wissenschaftliche Beiträge zu veröffentlichen, so die Schrift „Über die Weinprobe auf Eisen und Blei“ (1788), die es ermöglichte, die Verfälschung von Wein mit giftigem Bleizucker nachzuweisen. Die "hahnemannsche Weinprobe" machte seinen Namen bekannt; sie wurde von der preußischen Regierung für die Weinhändler der Stadt Berlin vorgeschrieben. In Dresden fungierte Hahnemann zeitweise als Vertreter des Stadtphysicus und bekam so Einblick in die Gerichtsmedizin, was sich u. a. in einer Schrift über die Arsenikvergiftung niederschlug.

1789 zog Hahnemann mit Frau und mittlerweile drei Kindern nach Lockwitz, später nach Leipzig und dann in den Leipziger Vorort Stötteritz. Dort übersetzte er 1790 die zweibändige Arzneimittellehre des Schotten William Cullen, eines damals sehr bekannten Mediziners, des Lehrers von John Brown, dem Begründer des populären Brownianismus. Diese Übersetzung enthält eine Fußnote, die als die erste Spur der Homöopathie betrachtet werden kann. Hahnemann kritisierte darin, dass Cullen die bekannte Wirkung der Chinarinde bei Malaria („Wechselfieber“) auf deren magenstärkende Eigenschaften zurückführte. Und er gab einen Bericht über einen von ihm vorgenommenen wiederholten Selbstversuch mit der Chinarinde, die nach seinen Angaben bei ihm „alle mir sonst beim Wechselfieber gewöhnlichen Symptomen“ hervorrief – „doch ohne eigentlichen Fieberschauder“. Er formulierte, zunächst sehr vorsichtig, die Vermutung, dass diese Fähigkeit, vergleichbare Symptome hervorzurufen, für die Heilwirkung der Chinarinde bei Malaria verantwortlich sein könnte.

Im Jahre 1791 wurde Hahnemann in die renommierte „Churfürstlich Mayntzische Academie nützlicher Wissenschaften“ zu Erfurt aufgenommen. Im Jahr 1793 wurde er zudem zum Mitglied der Gelehrtenakademie Leopoldina gewählt.

In Gotha, wohin Hahnemann 1792 umgezogen war, kündigte der mit ihm befreundete Verleger Rudolph Zacharias Becker die Gründung einer „Genesungs-Anstalt für etwa 4 irrsinnige Personen aus vermögenden Häusern“ an, die ein „menschenfreundlicher Arzt“ (nämlich Hahnemann) leiten sollte. Es sollte auf Züchtigung, Fixierung und andere Disziplinierungsmaßnahmen verzichtet werden; die Therapie bestand im Wesentlichen offenbar in Gesprächen, möglicherweise auch bereits homöopathischen Medikamentengaben. Es gab jedoch nur einen Patienten, der das Honorar aufbringen konnte, den hannoverschen Autor und Beamten Friedrich Arnold Klockenbring, der offenbar an einer Art manisch-depressiver Gemütskrankheit litt. Im Frühjahr 1793 entließ Hahnemann Klockenbring als geheilt, musste danach allerdings seine Anstalt mangels weiterer Patienten schließen. In seinem Artikel „Striche zur Schilderung Klockenbrings während seines Trübsinns“, veröffentlicht in der „Deutschen Monatsschrift“ 1796, berichtet Hahnemann über seinen psychotherapeutischen Versuch.
Weitere Stationen Hahnemanns waren Molschleben, Göttingen, Pyrmont (1794), Wolfenbüttel, Braunschweig (1795) und Königslutter (1796–1799). Er publizierte weiterhin eifrig, insbesondere chemische und pharmazeutische Übersetzungen und eigene Schriften, u. a. zur Herstellung eines löslichen Quecksilberoxids und zur Entdeckung eines Mittels gegen Milchschorf der Kinder (Kalkschwefelleber, noch heute als homöopathisches Mittel Hepar sulphuris benutzt), aber auch ein zweiteiliges, viel gelesenes „Apothekerlexikon“. Während dieser Zeit hat Hahnemann weitere Experimente mit Arzneistoffen an sich und anderen vorgenommen. Sie gingen ein in den ersten Aufsatz über das Heilprinzip, das später das homöopathische heißen sollte. Dieser Aufsatz erschien 1796 in Hufelands „Journal der practischen Arzneykunde und Wundarzneykunst“ unter dem Titel „Versuch über ein neues Princip zur Auffindung der Heilkräfte der Arzneysubstanzen, nebst einigen Blicken auf die bisherigen“. Hier formulierte Hahnemann das Prinzip, „Ähnliches mit Ähnlichem“ zu heilen ("similia similibus curentur"), und versuchte es mit einer ganzen Reihe empirischer Beobachtungen abzustützen, u. a. mit weiteren Selbstversuchen, Vergiftungsberichten, Lesefrüchten und eigenen und fremden Heilungsgeschichten, die durch das Simileprinzip erklärt werden.

1799 zog Hahnemann mit seiner Familie, die inzwischen acht Kinder umfasste, nach Altona. Seine Versuche, die dortigen hohen Lebenshaltungskosten zu erwirtschaften, ließen sich nicht gut an: Die Kur des psychisch kranken Dichters Johann Karl Wezel, die er nach dem Muster der früheren Klockenbring-Behandlung übernahm, schlug fehl, da er mit dem aggressiven Patienten nicht zurechtkam, und seine im „Reichsanzeiger“ beworbene Schrift „Heilung und Verhütung des Scharlach-Fiebers“ nebst einem „Pülverchen“, das gegen Ansteckung mit Scharlach schützen sollte, erwies sich ebenfalls als erfolglos. Aufgrund seiner prekären ökonomischen Lage folgte 1800 ein weiterer Ortswechsel nach Mölln. In dieser Zeit bewarb und verkaufte er ein selbst hergestelltes „neues Laugensalz“, das sich aber bald als altbekanntes Borax erwies. Er gestand seinen Fehler ein und gab das eingenommene Geld zurück.

Hahnemann wechselte nun erneut den Ort, zunächst nach Machern, dann nach Eilenburg (1801–1803) und schließlich nach Schildau. In dieser Zeit sind ausweislich des Hahnemannschen Krankenjournals erstmals zweifelsfrei Behandlungen nach dem homöopathischen Ähnlichkeitsgesetz nachzuweisen. Hahnemann begann nun auch immer kleinere Dosen zu verwenden, was er in einem Aufsatz „Ueber die Kraft kleiner Gaben der Arzneien überhaupt und der Belladonna insbesondere“ in Hufelands „Journal der practischen Arzneykunde“ begründete. 1805 ließ er sich für einige Jahre in Torgau nieder. In diese Zeit fällt ein Aufsatz von Hahnemann, ebenfalls in Hufelands Zeitschrift, der erstmals den Begriff „homöopathisch“ einführt: „Fingerzeige auf den homöopathischen Gebrauch der Arzneien in der bisherigen Praxis“. Vor allem aber erschienen dort zwei Schriften, die deutlich machten, dass er seine Selbst- und wahrscheinlich auch Fremdversuchspraxis in den letzten Jahren konsequent fortgesetzt hatte: die lateinische Arzneimittellehre „Fragmenta de viribus medicamentorum positivis sive in sano corpore observatis“ (Fragmente zu den gesicherten Arzneikräften oder auch denjenigen, die am gesunden Körper beobachtet wurden) und das Buch „Heilkunde der Erfahrung“, die erste Gesamtdarstellung des neuen Heilprinzips. 1810 schließlich veröffentlichte Hahnemann die erste Auflage seines Grundlagenwerks zur Homöopathie, damals noch unter dem Titel „Organon der rationellen Heilkunde“ (spätere Auflagen tragen den Titel „Organon der Heilkunst“). Dieses Werk enthielt bereits alle Wesenszüge der Homöopathie, wurde aber in den kommenden Jahren noch erheblich überarbeitet und ergänzt. Es ist bis heute das theoretische Werk der Homöopathie geblieben.

Das „Organon“ wirkte, auch wegen der in ihm enthaltenen scharfen Polemik, sofort deutlich polarisierend. Hahnemann wurde nun als Haupt einer neuen Schule angesehen. 1811 folgte das erste große Werk, das sein Programm der Prüfung von Arzneimitteln an Gesunden umsetzte, nämlich der erste Band der „Reinen Arzneimittellehre“ (die es später auf sechs Bände bringen sollte) mit im Experiment an Gesunden (vor allem an sich selbst, seiner Familie und seinen Schülern) herausgefundenen „reinen Arzneiwirkungen“ sowie zahlreichen Literaturzitaten. Ein solches experimentell begründetes Werk kann als ein für die damalige Zeit einzigartiger Beitrag zur Pharmakologie gelten; nichts Vergleichbares war bis dahin versucht worden, wie Anthony Campbell schreibt.

1811 zog Hahnemann nach Leipzig um. Dort gelang es ihm 1812 mit einem weiteren wissenschaftlichen Werk („De Helleborismo veterum“, d. h. über den Gebrauch der Nieswurz bei den Alten = antiken Autoren), die Lehrbefugnis an der Universität zu erlangen (Habilitation). Hahnemann wurde nun tatsächlich zum Begründer einer heilkundlichen Richtung, sammelte Schüler um sich, vermochte sein Arzneiprüfungsprogramm mit neuen Kräften (insbesondere seinen Studenten und seinem ältesten Sohn) fortzusetzen und Vorlesungen über die Homöopathie zu halten – und wurde in ausgedehnte akademische Fehden verwickelt, da er unter den Medizinprofessoren auch erbitterte Gegner hatte. So gab es eine Auseinandersetzung mit Karl Heinrich Dzondi von der Universität Halle über die Frage, ob bei Verbrennungen möglichst kaltes oder eher warmes Wasser angewandt werden solle (als Exemplifikation des „Contraria“- gegen das Ähnlichkeitsprinzip). 1816 erschien der zweite Band seiner „Reinen Arzneimittellehre“, 1819 die zweite Auflage des „Organon der Heilkunst“, nun mit dem aufklärerischen Motto „aude sapere“ („Wage zu wissen“) von Horaz, das durch Kants Aufklärungsdefinition so bekannt geworden war. Hahnemann hielt auch Vorlesungen über die Geschichte der Medizin, die im Unterschied zu den Vorlesungen über Homöopathie unproblematisch verliefen.

In seiner Leipziger Zeit unterhielt Hahnemann auch eine ausgedehnte Praxis. Sein bekanntester Patient war, neben Friedrich Wieck, dem Vater von Clara Wieck (spätere Clara Schumann), Karl Philipp Fürst zu Schwarzenberg, österreichischer Generalfeldmarschall, der als Sieger von Leipzig sehr populär war (selbst Goethe kommentierte diese Entscheidung Schwarzenbergs interessiert). Der schwerkranke Fürst, der bereits alle möglichen anderen Ärzte in Anspruch genommen hatte, verstarb schließlich aber trotz aller Anstrengungen im Oktober 1820.

In diese Zeit fiel auch der „Leipziger Dispensierstreit“: Hahnemann, der über chemische und pharmazeutische Kenntnisse verfügte, bestand darauf, seine homöopathischen Arzneien selbst zubereiten zu dürfen, worauf ihn drei Leipziger Apotheker verklagten, da einzig die Apotheker das Privileg der Arzneiherstellung besaßen. Der Streit endete 1820 mit einem Kompromiss: Die Apotheker behielten ihr hergebrachtes Dispensierrecht, Hahnemann durfte jedoch in Notfällen, insbesondere auf dem Land, auch selbst Medikamente zubereiten. Ebenfalls in die Leipziger Zeit fällt Hahnemanns Annahme in die Freimaurerloge Minerva zu den drei Palmen im Jahr 1817.

Nach dieser Teilniederlage entschloss er sich zu einem weiteren Umzug und ging 1821 als Herzoglicher Leibarzt nach Köthen, wo ihm Herzog Friedrich Ferdinand von Anhalt-Köthen das Recht auf Verfertigung eigener Arzneien und Selbstdispensierung derselben nach Intervention des österreichischen Politikers Adam von Müller, eines einflussreichen Förderers der Homöopathie, schriftlich garantierte. 1822 wurde er zum Hofrat ernannt. 1829 beging er in Köthen feierlich das 50. Jubiläum seiner Erlanger Promotion.

In Köthen erschien 1828–1830 die erste Auflage der „Chronischen Krankheiten“, die einen erheblichen Wandel in der Doktrin der Homöopathie bedeuteten und auch bei Hahnemanns Anhängern nicht nur auf Begeisterung stießen. Während er an dem Ähnlichkeitsprinzip festhielt, stellte Hahnemann fest, dass bei gewissen chronischen Krankheiten eine einfache homöopathische Medikation nicht half. Er teilte sie in drei Typen ein: die Geschlechtskrankheiten Syphilis und „Sykosis“ ("Feigwarzenkrankheit") sowie die Mangelkrankheit „Psora“, unter die er eine Vielzahl unterschiedlicher Krankheitsmanifestationen rechnete und die er mit Hauterscheinungen vom Typ der Krätzebläschen in Verbindung brachte. Das Ähnlichkeitsprinzip sollte in diesen Fällen entsprechend nicht in erster Linie auf die gegenwärtigen Krankheitssymptome, sondern auf die „Ur-Übel“ angewandt werden, die das chronische Kranksein hervorgebracht hatten und in Form eines „Miasmas“, sozusagen einer dauerhaften Prägung, weiterhin das Leben der Patienten beeinträchtigten. Die von ihm in den nach und nach erscheinenden fünf Bänden der „"Chronischen Krankheiten"“ vorgestellten und an Gesunden experimentell geprüften Arzneien sollten auch imstande sein, das psorische Miasma zu bekämpfen. Ein weiterer, später noch schärfer formulierter Stein des Anstoßes war die neue Lehre von der Arzneipotenzierung: Hahnemann verkündete nun, dass die Methode der Verdünnung mit gleichzeitiger mechanischer Bearbeitung (Schütteln, Reiben) nicht nur nebenwirkungsärmere Arzneien hervorbringe, ja dass diese spezielle Methode ihre Heilkräfte erst wahrhaft aufschließe. Auch in dieser Lehre folgten ihm seine Schüler keineswegs bedingungslos.

Zugleich begann Hahnemann, der zuvor auf eine theoretische Begründung seines Ähnlichkeitsprinzips verzichtet und es als rein empirisches Gesetz bezeichnet hatte, sich der Lehre des Vitalismus anzunähern, was ihm unter anderem Beifall von Seiten des berühmten Hufeland einbrachte. In der vierten (1829) und vor allem der fünften Auflage des "Organon" (1833) räumte er einer immateriellen Lebenskraft des Organismus, die in den ersten "Organon"-Ausgaben keine Rolle gespielt hatte, einen prominenten Platz bei der Argumentation für das Ähnlichkeitsprinzip ein, freilich in Form einer Hypothese:

Am 31. März 1830 starb Hahnemanns erste Frau in Köthen nach 48-jähriger Ehe; vier seiner Töchter unterstützten ihn nun in seiner ausgedehnten Praxis. Eine große Rolle für die weitere Durchsetzung der Homöopathie spielten seine Stellungnahmen zu den großen Choleraepidemien der Jahre 1830 und 1831. Hahnemann hat zwar selbst nie einen Cholerakranken zu Gesicht bekommen, veröffentlichte aber in Köthen vier einflussreiche Texte zum Wesen der Cholera und zur Cholerabehandlung, die insbesondere in Wien auf großes Interesse stießen und – verglichen mit der damals herrschenden Medizin – mit recht gutem Erfolg angewendet wurden. Bemerkenswert ist insbesondere, dass Hahnemann die Cholera zu Recht für eine von „feinsten Thieren niederer Ordnung“ übertragene Infektionskrankheit hielt und dafür zumindest in der Anfangsphase eine Art antiseptische Therapie in Form einer Kampfer-Anwendung empfahl; zum relativen Erfolg der Homöopathie bei Cholera dürfte beigetragen haben, dass Hahnemann allen schwächenden Maßnahmen strikt widersprach und vor allem das von anderen Ärzten empfohlene Trinkverbot für widersinnig erklärte.

In die 1830er-Jahre fielen auch erbitterte Kämpfe um die Reinheit der neuen Lehre, die besonders in den Auseinandersetzungen um das erste homöopathische Krankenhaus in Leipzig, das 1833 begründet wurde, Gestalt annahmen. Hahnemann nahm in schärfster Form Stellung gegen jeden Versuch, die Homöopathie mit herkömmlichen, insbesondere schwächenden Mitteln wie Aderlass oder Abführmittel zu verbinden. Dies richtete sich speziell gegen den Leiter dieses Krankenhauses, Moritz Müller, der ein eklektisches Vorgehen bevorzugte. Die Konflikte mit den „Halbhomöopathen“ rissen danach nicht mehr ab.

Ende 1834 suchte die 34-jährige französische Malerin Mélanie d’Hervilly Hahnemann in Köthen als Patientin auf. Es kam zu einer stürmischen Liebesgeschichte mit dem mittlerweile neunundsiebzigjährigen Arzt, die zu Hahnemanns zweiter Ehe führte. Die Hochzeit, die ohne kirchlichen Segen am 18. Januar 1835 in seinem Haus in der Köthener Wallstraße stattfand, erregte gewaltiges Aufsehen. Zusammen mit seiner 45 Jahre jüngeren Gattin zog er bald darauf nach Paris um.

In Paris verbrachte Hahnemann seine letzten acht Jahre als angesehener und vielbeschäftigter Arzt. Ein prominenter Patient war bereits 1837 der Geiger Niccolò Paganini, der offenbar unter Priapismus, Harnverhaltung und Husten litt. Er brach die Behandlung ab, nachdem sein Annäherungsversuch an Hahnemanns Frau von dieser brüsk zurückgewiesen worden war. Ende 1838 behandelte er die Tochter des in der Pariser Gesellschaft sehr bekannten Literaten Ernest Legouvé, die, von den Ärzten aufgegeben, im Sterben lag. Der Maler Amaury Duval wurde an das Krankenbett gerufen, um sie noch zu porträtieren, und riet, Hahnemann zu rufen. Diesem gelang es, sie zu heilen. Daraufhin breitete sich der Ruf Hahnemanns in der Pariser Gesellschaft rasch aus und führte ihm weitere prominente Patienten zu, etwa den Schriftsteller Eugène Sue oder die Mutter von Victor Schœlcher. Die noch vorhandenen Krankengeschichten der Pariser Zeit sind erst zum Teil ausgewertet und dürften noch weitere Überraschungen bieten. 

In den letzten Jahren arbeitete Hahnemann an einer 6. Auflage seines "Organon", die neue Vorschriften zur Arzneibereitung („Potenzierung“) enthielt, insbesondere zu den später so genannten Q-Potenzen mit besonders großen Verdünnungsschritten von 1:50.000, über die es fast ein Jahrhundert lang eine virulente Gerüchteküche gab. Sie wurde, hauptsächlich aufgrund von Querelen zwischen Mélanie Hahnemann und verschiedenen Hahnemann-Schülern, erst 1921 von Richard Haehl aus dem Nachlass veröffentlicht. Eine nach Hahnemanns Tod von Arthur Lutze veröffentlichte Version gilt als nicht authentisch.

Hahnemann starb am 2. Juli 1843 in Paris, vermutlich an einer Lungenentzündung. Er wurde zunächst auf dem Friedhof Montmartre beigesetzt, später (1898) zusammen mit seiner zweiten Frau auf dem Friedhof Père Lachaise begraben. Hahnemanns Grabstein trägt auf seinen persönlichen Wunsch die Inschrift "„Non inutilis vixi“" (dt.: „Ich habe nicht unnütz gelebt“). Die zweite Frau Hahnemanns bezeichnete sich nach dessen Tod selbst als „"Doktorin der Homöopathie"“ und wurde deshalb 1847 beschuldigt, die Heilkunst ohne Erlaubnis auszuüben. Es kam zu einem Prozess.

Aufgrund seiner Verdienste wurde er zum Ehrenbürger von Meißen ernannt.

Am 10. August 1851 wurde im Rahmen einer Tagung des Homöopathischen Central-Vereins ein
Denkmal für Hahnemann in Leipzig enthüllt. Die Inschrift lautet:
"DEM // GRÜNDER DER HOMÖOPATHIE // SAM. HAHNEMANN // GEB. ZU MEISSEN D. 10. APRIL 1755 //
GEST. ZU PARIS D. 2. JULI 1843 // VON // SEINEN DANKBAREN SCHÜLERN // UND VEREHRERN"

Im Jahre 1900 wurde in Washington, D.C. ein Hahnemann-Denkmal errichtet und am 21. Juni feierlich eingeweiht. In den USA war damals die Homöopathie sehr weit verbreitet, und US-amerikanische Homöopathen hatten über 75.000 $ für das Monument gesammelt. Es trägt die Inschrift "Similia similibus curentur", die kürzeste Zusammenfassung des homöopathischen Prinzips: Ähnliches soll mit Ähnlichem geheilt werden.

In Hahnemanns längstem Wirkungsort Köthen wurde ihm 1897 ein Denkmal gesetzt, das zugleich auch dem Gründer der Köthener homöopathischen Klinik Arthur Lutze (1813–1870) gewidmet ist. Es stammt von dem Bildhauer Heinrich Pohlmann. Das Hahnemann-Lutze-Denkmal steht gegenüber dem neugotischen Prachtbau der Lutze-Klinik, der heute ebenso wie Hahnemanns Köthener Wohnhaus und Praxisräume wieder für Besucher zugänglich ist.

Seit 1906 trägt eine Straße im Leipziger Stadtteil Lindenau den Namen "Hahnemannstraße". Straßen mit diesem Namen gibt es auch in Stuttgart, München, Hamburg, Karlsruhe, Dresden und Erlangen.

Im Stadtzentrum von Meißen sind der Hahnemannsplatz und die Hahnemann-Apotheke nach ihm benannt.

In Köthen gibt es in seinem früheren Wohnhaus als Museum das Hahnemannhaus in der Wallstraße 47 (mit Gedenktafel) und in der Wallstraße 48 die europäische Fachbibliothek zur Homöopathie sowie im Historischen Museum im Schloss eine Dauerausstellung zur Homöopathie. 
In Hahnemanns Geburtsstadt Meißen findet sich in der Klosterruine Heilig Kreuz das Hahnemannzentrum e. V.

Nahezu der gesamte handschriftliche Nachlass Hahnemanns und alle Erstausgaben werden im Institut für Geschichte der Medizin der Robert Bosch Stiftung in Stuttgart aufbewahrt.






</doc>
<doc id="10229" url="https://de.wikipedia.org/wiki?curid=10229" title="Deimos (Mythologie)">
Deimos (Mythologie)

Deimos ( „Schrecken“, ) ist ein Daimon der griechischen Mythologie.

Deimos tritt in der Regel zusammen mit Phobos, „Furcht“, auf. Bereits in Homers "Ilias" verbreiten beide als Gefolge des Ares Furcht und Schrecken unter den Kämpfern vor Troja. Zusammen mit Phobos dient er dem Ares, dem sie die Pferde vor den Kampfwagen spannen. Da Homer Phobos zum Sohn des Ares macht, ist zu vermuten, dass für Homer auch Deimos Sohn des Ares war. Für Hesiod sind sie gemeinsam Söhne des Ares und der Aphrodite, die er Kythereia nennt. In den Dionysiaka des Nonnos von Panopolis sind beides Söhne des Enyalios, einer auch als eigenständige Gottheit verehrten Epiklesis des Ares. Sie treten hier nicht nur als Wagendiener und Begleiter des Ares im Kampf gegen Dionysos auf, sondern auch als Gehilfen des Zeus, der sie in seinem zweiten Kampf gegen Typhon antreten lässt, wobei er Phobos mit dem Blitzbündel, Deimos mit dem Donnerkeil ausstattet. Die Suda, ein vermutlich im 10. Jahrhundert entstandenes byzantinisches Lexikon, nennt Deimos und Phobos zusammen mit Kydoimos, dem Daimon des Nahkampfes, Söhne des Polemos, das heißt des Krieges, und Begleiter des Ares. Laut Suda erlitten sie das gleiche Schicksal wie Ares, der Hephaistos mit Gewalt in den Olymp zurückholen wollte, damit er Hera aus ihrer Fesselung auf dem Thron befreie. Unbeeindruckt jagte Hephaistos den Kriegsgott mit Feuerbränden davon. Laut Semos von Delos war Deimos der Vater der Skylla, die er mit der Krataiis, einer der Hekate verwandten, urgewaltigen dämonischen Macht, zeugte. 

Der um 400 v. Chr. wirkende Dichter Antimachos von Kolophon zählte Deimos wie auch Phobos zu den vier feuerschnaubenden Pferden des Ares, wohl als Folge einer Fehlinterpretation der Wagenszene bei Homer. Auch in der "Argonautika" des Valerius Flaccus sind Terror und Pavor, die römischen Übersetzungen von Deimos und Phobos, die Pferde des römischen Kriegsgottes Mars "(Martis equi)".

In der "Ilias" war Deimos zusammen mit Phobos auf dem Schild des Agamemnon dargestellt und auch der Schild des Herakles zeigte beide neben Ares auf dem Wagen stehend. Bei Quintus von Smyrna schließlich schmücken sie neben Enyo („Nahkampf“) und Eris („Streit“) den Schild des Achilleus. Sicher als Deimos zu identifizierende Darstellungen sind im Gegensatz zu solchen des Phobos aus der antiken Kunst nicht bekannt.

Nach den Begleitern des Ares, dem in der römischen Mythologie der Gott Mars entspricht, erhielten die Monde des Planeten Mars die Namen Deimos und Phobos.




</doc>
<doc id="10230" url="https://de.wikipedia.org/wiki?curid=10230" title="Phobos (Mythologie)">
Phobos (Mythologie)

Phobos ( „Furcht, panische Angst“, ) ist ein Daimon und Gott der griechischen Mythologie.

Phobos tritt in der Regel zusammen mit Deimos, „Schrecken“, auf. Bereits in Homers "Ilias" verbreiten beide als Gefolge des Ares Furcht und Schrecken unter den Kämpfern vor Troja. Zusammen mit Deimos dient er dem Ares, dem sie die Pferde vor den Kampfwagen spannen. Auch retten sie den von Herakles verwundeten Kriegsgott aus dem Kampfgeschehen und bringen ihn im Wagen zum Olymp. Bei Homer ist Phobos der geliebte Sohn des Ares und vermutlich war auch Deimos für Homer Sohn des Ares. Für Hesiod sind sie gemeinsam Söhne des Ares und der Aphrodite, die er Kythereia nennt. In den Dionysiaka des Nonnos von Panopolis sind beides Söhne des Enyalios, einer auch als eigenständige Gottheit verehrten Epiklesis des Ares. Sie treten hier nicht nur als Wagendiener und Begleiter des Ares im Kampf gegen Dionysos auf, sondern auch als Gehilfen des Zeus, der sie in seinem zweiten Kampf gegen Typhon antreten lässt, wobei er Phobos mit dem Blitzbündel, Deimos mit dem Donnerkeil ausstattet. Die Suda, ein vermutlich im 10. Jahrhundert entstandenes byzantinisches Lexikon, nennt Phobos und Deimos zusammen mit Kydoimos, dem Daimon des Nahkampfes, Söhne des Polemos, das heißt des Krieges, und Begleiter des Ares. Laut Suda erlitten sie das gleiche Schicksal wie Ares, der Hephaistos mit Gewalt in den Olymp zurückholen wollte, damit er Hera aus ihrer Fesselung auf dem Thron befreie. Unbeeindruckt jagte Hephaistos den Kriegsgott mit Feuerbränden davon. 

Der um 400 v. Chr. wirkende Dichter Antimachos von Kolophon zählte Phobos wie auch Deimos zu den vier feuerschnaubenden Pferden des Ares, wohl als Folge einer Fehlinterpretation der Wagenszene bei Homer. Auch in der "Argonautika" des Valerius Flaccus sind Pavor und Terror, die römischen Übersetzungen von Phobos und Deimos, die Pferde des römischen Kriegsgottes Mars "(Martis equi)".

In der "Ilias" war Phobos zusammen mit Deimos auf dem Schild des Agamemnon dargestellt und auch der auf der Kypseloslade im olympischen Heraion dargestellte Schild des Agamemnon zeigte als zentrales Motiv Phobos, laut Pausanias mit löwengestaltigem Kopf. Eine Beischrift beschrieb ihn: „Dies ist der Phobos der Sterblichen, der, der ihn trägt, ist Agamemnon.“ Der Schild des Herakles zeigte in den Randszenen beide neben Ares auf dem Wagen stehend. Darüber hinaus schmückte das Antlitz des Phobos das zentrale Bildfeld des Schildes. Es war schrecklich anzusehen, rückwärts blickend und mit feurig glühenden Augen. Seine weißen Zähne leuchteten schrecklich und Angst einflößend. Über seinen grimmigen Brauen schwebte Eris („Streit“). Bei Quintus von Smyrna schließlich schmücken Phobos und Deimos neben Enyo („Nahkampf“) und Eris den Schild des Achilleus. Doch auch auf der Ägis der Athena war Phobos neben dem Haupt der Gorgo Medusa laut Homer zu sehen. Allerdings wird die Echtheit der Stelle angezweifelt und als nachträglicher Einschub beurteilt.

In Sparta gab es ein Heiligtum mit einem kleinen Tempel des Phobos, das laut Plutarch nahe dem Syssition der Ephoren lag. Plutarch gibt als Begründung für die Verehrung des Phobos an, dass die Spartaner glaubten, der Staat werde durch Furcht zusammengehalten, nämlich der Furcht vor Zurechtweisung und Demütigung. Dem Wesen nach ist dieser spartanische Phobos der athenischen Aidos, der „Scheu, Sittsamkeit“, nah verwandt und unterscheidet sich deutlich von dem Sohn des Ares und dessen rein kriegbezogenen Wesen.

Nach den Begleitern des Ares, dem in der römischen Mythologie der Gott Mars entspricht, erhielten die Monde des Planeten Mars die Namen Phobos für den größeren und Deimos für den kleineren der Trabanten.




</doc>
<doc id="10232" url="https://de.wikipedia.org/wiki?curid=10232" title="Permakultur">
Permakultur

Permakultur ist ein Konzept, das auf die Schaffung von dauerhaft funktionierenden nachhaltigen und naturnahen Kreisläufen zielt. Ursprünglich für die Landwirtschaft entwickelt, ist sie inzwischen ein Denkprinzip, das auch Bereiche wie Energieversorgung, Landschaftsplanung und die Gestaltung sozialer Infrastrukturen umfasst.

Grundprinzip ist ein ökologisch, ökonomisch und sozial nachhaltiges Wirtschaften mit allen Ressourcen.

Permakulturell gestaltete Lebensräume werden als Systeme aufgefasst, in denen das Zusammenleben von Menschen, Tieren und Pflanzen so miteinander kombiniert wird, dass die Systeme zeitlich unbegrenzt funktionieren und die Bedürfnisse aller Elemente so weit wie möglich erfüllt werden. Bei der Gestaltung solcher Systeme werden auch integrative Denkansätze und Erkenntnisse aus Systemtheorie, Biokybernetik und Tiefenökologie angewandt. Dabei richtet sich die Aufmerksamkeit nicht nur auf die einzelnen Bestandteile eines Systems, sondern insbesondere auch auf die Beziehungen zwischen diesen und ihre optimale Nutzung für den Aufbau produktiver Systeme.

Ziel einer permakulturellen Planung ist die Erhaltung und schrittweise Optimierung, um ein sich selbst regulierendes System zu schaffen, das höchstens minimaler Eingriffe bedarf, um dauerhaft in einem dynamischen Gleichgewicht zu bleiben. Dabei stehen sich die Befriedigung kurzfristiger Bedürfnisse und die nachfolgender Generationen gleichwertig gegenüber. Das System soll stets produktiv und anpassbar bleiben. Vorbild sind dabei meist beobachtbare Selbstregulations­prozesse in Ökosystemen wie etwa Wäldern, Seen und Ozeanen.

Mitte der 1970er Jahre entwickelten die beiden Australier Bill Mollison und David Holmgren Ideen zum Aufbau langfristig ertragreicher landwirtschaftlicher Systeme als nachhaltigen Gegenentwurf zum vorherrschenden industriellen Agrarsystem. Im Prinzip „entdeckten“ sie die Kreisläufe des in Europa bereits bekannten Biolandbaus für sich und ihren Kontinent neu. Sie beobachteten, dass die industrielle Landwirtschaft durch ihre Präferenz für Monokulturen und den massiven Einsatz von Pestiziden Böden und Wasser verschmutze, die Biodiversität reduziere und ehemals fruchtbaren Boden der Erosion ausliefere. Heute werden solche Beobachtungen weltweit bestätigt und die Zustände in der Agrarindustrie zunehmend kritisiert.

Mollison und Holmgren prägten für ihren neuen Denkansatz den Begriff "Permakultur". Er entstand aus der Verknüpfung der Begriffe permanent agriculture (dt. 'dauerhafte Landwirtschaft'). Der Terminus "permanent agriculture" wurde bereits 1911 vom amerikanischen Agrarwissenschaftler Franklin Hiram King in einem ähnlichen Sinne verwendet, um die nachhaltigen Anbaumethoden in China, Korea und Japan zu beschreiben. 1978 veröffentlichte Mollison sein erstes Buch über dieses Konzept ("Permaculture One"). Drei Jahre später wurde er für die Erforschung und Beschreibung der Prinzipien dieser Form der naturnahen Landwirtschaft mit dem Alternativen Nobelpreis ausgezeichnet.

Mollison und Holmgren definierten Permakultur zunächst als Planung, Entwicklung und Bewirtschaftung integrierter, sich selbst entwickelnder Systeme aus mehrjährigen und sich selbst vermehrenden einjährigen Pflanzen und Tierarten, die im Einklang mit den jeweiligen Umweltbedingungen und den Bedürfnissen ihrer Nutzer stehen. Im Laufe der Jahre erweiterten und verfeinerten sie ihre Designprinzipien, indem sie diese in Hunderten von Projekten erprobten. Dabei wurde ihnen die Notwendigkeit der Einbeziehung sozialer Aspekte immer mehr bewusst.

So wurde während der 1980er Jahre aus dem ursprünglich landwirtschaftlichen Konzept ein holistisch-integrativer Denkansatz zur Gestaltung sozialer Siedlungsräume in Harmonie mit natürlich gewachsenen Habitaten im Sinne einer "permanent culture". Mit der Zeit erfuhr permakulturelles Denken und Handeln über die weltweit vernetzten neueren sozialen Bewegungen eine rasante Ausbreitung. Heute werden Permakulturprinzipien auch in vielen weiteren Gebieten umgesetzt, wie etwa der Architektur, der Stadt- und Regionalplanung sowie in der kooperativen Ökonomie und Software­entwicklung.

Die von Mollison und Holmgren entwickelte Gestaltung mit Hilfe von Mustern weist Parallelen zum 1977 von Christopher Alexander vorgestellten Prinzip der Pattern Language auf (siehe unten, Gestaltprinzipien).

Da die Begriffe „Permakultur“ und „Permaculture“ markenrechtlich nicht geschützt sind und es in Deutschland keinen staatlich anerkannten Ausbildungsberuf zum Erlernen von Permakultur gibt, hat sich unter Permakultur-Anhängern ein eigenes weltweit anerkanntes Ausbildungssystem etabliert. Die Basis-Ausbildung findet weltweit in Form sogenannter „Permaculture Design Certificate“ Kurse (PDK oder englisch PDC) statt. In mindestens 72 Unterrichtsstunden werden dort Grundlagen der Permakultur vermittelt. Die Kurse bauen auf dem Buch "Permaculture Designer's Manual" von Bill Mollison und David Holmgren auf und werden von zahlreichen Einrichtungen angeboten. Sie richten sich bewusst an jedermann, auch ohne Vorkenntnisse.

Aufbauend auf diesen Kurs bietet in Deutschland u. a. die Permakultur Akademie deutschlandweit eine Ausbildung zum Diplom-Permakultur-Designer an. Diese dauert zwei bis drei Jahre und schließt mit dem ebenfalls in internationalen Permakultur-Netzwerken anerkannten „Diploma of Applied Permaculture“ ab. Beide Abschlüsse sind in Deutschland nicht staatlich anerkannt. Die Akademie ist jedoch seit 2006 ein von der UN-Dekade „Bildung für nachhaltige Entwicklung“ ausgezeichnetes Projekt. Durch die Professionalisierung des Ausbildungskonzeptes wird die Stärkung eines innovativen und zukunftsfähigen Berufsbildes angestrebt. Seit 2013 bietet der Permakultur-Campus (Hamburg) auch im norddeutschen Raum eine anderthalb- bis zweijährige Ausbildung zum Diplom-Permakultur-Designer an.

Das Bildungskonzept empfiehlt den Beginn mit kleinen überschaubaren Systemen ("small scale design"). Bevorzugte Lernmethode ist das Action Learning, Denken und Handeln sollen sich abwechseln.

Die Anwendung von Permakulturprinzipien im Sinne einer integrativen, zukunftsfähigen Gestaltung unserer Lebensräume hat von Beginn an zur Formulierung ethischer Grundgedanken geführt. Auch diese wurden und werden ständig weiterentwickelt und bilden die Grundhaltung permakulturellen Denkens und Handelns. Sie sollen als Richtlinie für jegliches Permakultur-Design aufgefasst werden, sei es ein Garten-, Landwirtschafts- oder Forstprojekt, sei es der Bau eines Hauses oder einer ganzen Siedlung.

Diese ethischen Grundwerte decken die oben erwähnten ökologischen, ökonomischen und sozialen Komponenten ab und lassen sich mit folgenden drei Termini zusammenfassen

Als eine nachhaltige Bewirtschaftungsform zielt Permakultur darauf ab, Erträge langfristig in ausreichender Höhe sicherzustellen und dabei den Arbeitsaufwand (Energieverbrauch) zu minimieren.

Permakultursysteme zeigen, wie sich Einzelne und Gemeinschaften mit einem geringen Ressourcen-, Platz- und Zeitaufwand und einem Verständnis für natürliche Kreisläufe weitgehend selbst versorgen können. Permakultur-Projekte nutzen dabei u. a. die Speicherung von Regenwasser und Sonnenenergie, verwenden sie effizient, verbessern die Bodenfruchtbarkeit und praktizieren eine naturnahe Abfallvermeidung, bei der der Output des einen Systemelements als Input für die anderen genutzt wird.

Permakultur sieht sich ethisch verpflichtet, nachfolgenden Generationen einen größtmöglichen Gestaltungsspielraum zu gewährleisten. Boden, Wasser und alle anderen lebenserhaltenden Ressourcen sollen für langfristige Nutzung bewirtschaftet und so bewahrt werden.

Die internationale Permakulturbewegung unterstützt und praktiziert den Aufbau von produktiven Strukturen und Systemen, die allen Menschen ein gesundes, selbstbestimmtes und friedliches Leben ermöglichen.

Die Gestaltung und Bewahrung von Vielfalt ist ein zentrales Anliegen von Permakultur. Natürlich gewachsene Ökosysteme sind Vorbild. Kulturell geschaffene Systeme seien gesünder, produktiver und nachhaltiger, wenn sie ebenso vielfältig sind. Mischkulturen statt Monokulturen werden als Beispiel genannt.

Für ein permakulturelles Design sind vier Aspekte von Vielfalt bedeutsam:


Die oben erwähnte Übertragung der Nischenstrategie auf die Landwirtschaft veranschaulicht diesen Leitsatz. Anstatt die Weideflächen zu vergrößern oder Monokulturen anzubauen, um kurzfristig wirtschaftlich effizienter zu sein, ermöglicht ein Einsatz von Vielfalt (mehrere Nutztiere, Mischkulturen, ...), die Fläche langfristig bzw. nachhaltig effizient zu nutzen, das System klein zu halten und die Produktivität insgesamt zu erhöhen. Permakulturelle Ziele werden dadurch besser erreicht.

Eine nachhaltig effiziente Gestaltung nutzt die vorhandenen Ressourcen besser. Diesen Vorteil von nachhaltiger gegenüber kurzfristiger Effizienz zeigen uns die abfallfreien Nährstoffkreisläufe in der Natur. Pflanzen und Tiere produzieren keinen 'Abfall', weil sie Teil eines nachhaltigen Systems sind, das die Überreste des einen als Nahrung für die anderen wiederverwendet, zum Beispiel als Futter oder Dünger. Je höher also die Vielfalt in einem System, desto nachhaltig effizienter werden die vorhandenen Ressourcen genutzt. Ein rein auf kurzfristige Effizienz ausgelegtes System würde nur darauf achten, eine einzige Ressource bestmöglich zu nutzen, bis sie schließlich aufgebraucht ist; die anderen Ressourcen bleiben ungenutzt und verkümmern. Darum sind auf bloß kurzfristige Effizienz ausgelegte Systeme langfristig unproduktiver als nachhaltig effizient genutzte.

Das Foto zeigt, wie Laufenten, Hühner und Schafe ungestört ihre jeweiligen Bedürfnisse befriedigen. Gleichzeitig werden die vorhandenen Ressourcen nachhaltig effizient genutzt; was die einen nicht mögen, essen die anderen. Die unterschiedlichen Nischen ermöglichen eine Kooperation auf relativ kleinem Raum. Permakulturell gestaltete Systeme nutzen diese erfolgreiche ökologische Strategie zum Aufbau und Erhalt integrierter Lebensräume von Menschen, Tieren und Pflanzen.

Das Verständnis von Ökosystemen und der Leitsatz "nachhaltige Effizienz statt bloß kurzfristiger Effizienz" führt unmittelbar zur Einsicht, selbst gestaltete Systeme vorrangig durch Optimierung klein zu halten, statt sie zu vergrößern, um die Erträge zu maximieren. Das wäre langfristig gesehen eine Energieverschwendung, denn je höher die genutzte Vielfalt und deren produktives Umsatzvermögen, desto weniger Energie muss in das System hineingesteckt werden. Nebenbei erhöht die Vielfalt die Ausfallsicherheit des Systems.

Aus diesem Grund wird bei einem permakulturellen Design mehr auf die Beziehungen zwischen den Elementen geachtet, als nur auf die Elemente an sich. Außerdem sind kleine Systeme prinzipiell überschaubarer als große, denn wir Menschen haben ein begrenztes Auffassungsvermögen im Hinblick auf komplexe Vorgänge. Systemisches Denken erfordert komplexes Denken, was aber nicht kompliziert heißen muss, solange das System klein und die Menge der Elemente adäquat bleibt.

Ein Beispiel für intelligent genutzte Kleinräumigkeit "(small scale design)" ist die Kräuterspirale. Das Foto zeigt, wie durch die Nutzung verschiedener Dimensionen und Ebenen mit unterschiedlichem Bodenprofil die benötigte Anbaufläche klein gehalten werden kann. Insbesondere in dicht besiedelten Gebieten mit wenig verfügbarer Anbaufläche ist diese Strategie eine adäquate und hilfreiche Lösung.

Das Design größerer Systeme hingegen geschieht am besten in Form eines Mosaiks aus Subsystemen. Die Bildung von Subsystemen setzt in der Natur bei Erreichen einer kritischen Größe ein, dient dem Systemerhalt (Überleben) und kann als eine Strategie zur Optimierung (statt Maximierung) verstanden werden. So gibt es für alle Systeme eine optimale Größe, deren Überschreitung existenzgefährdende Nachteile mit sich bringen würde:


Die optimale Größe betrifft sowohl das räumliche Ausmaß als auch die Wachstumsdynamik
der Systemelemente: kurze Wege und dichte Kreisläufe sind kurz- oder langfristig effizienter als
großräumige Strukturen; Vielfalt von Beziehungen (Multifunktionalität) und begrenztes
Wachstum (Sättigung) der Elemente gewährleisten Flexibilität, Dauerhaftigkeit und Selbstregulation von Systemen.

Um z. B. einen Garten, der uns ernähren soll, mit geringst möglichem Energieaufwand lange produktiv zu halten, brauchen wir Strategien, mit denen wir ihn weitgehend sich selbst überlassen können. Dazu gehört auch die Nutzung kooperativer Strukturen, wie etwa eine biologische Schädlingsregulation. Mit hohem Energieaufwand hergestellte Pestizide vertreiben nicht nur die 'Schädlinge', sondern auch die 'Nützlinge', die uns viel Arbeit abnehmen können. Sobald nämlich die 'Schädlinge' wieder einwandern fehlen die 'Nützlinge', weil sie lange keine Nahrung fanden. Nun wird der Schaden erst richtig groß, weil die Population der 'Schädlinge' außer Kontrolle gerät, was den neuerlichen Energieaufwand verstärkt.

Solche selbst verursachten destruktiven Rückkopplungen entwickeln die oben erwähnte Eigendynamik und gefährden das System bis hin zum Kollaps. Statt also mit verschwenderischem Einsatz von Pestiziden zu versuchen, mit den 'Schädlingen' zu konkurrieren, hilft die Nutzung kooperativer Selbstregulation die Produktivität mit minimalem Aufwand zu sichern.

Das Foto zeigt, wie Laufenten und Gänse den gärtnernden Menschen als kooperative Gartenhelfer zur Seite stehen. Die Laufenten erledigen so manches Schneckenproblem und halten gemeinsam mit den Gänsen das Gras auf den Wegen kurz. Dadurch hat der Mensch energie- und kostensparende Vorteile: weniger Pflegeaufwand bei gleichzeitiger Erhöhung des Gesamtertrages. Auf den Einsatz von Pestiziden und/oder Herbiziden kann durch eine geschickt gewählte Pflanzen- und Tierkombination verzichtet werden. Bei einem hohen Anspruch an Selbstversorgung hat diese Strategie einen entsprechend hohen Stellenwert.

Ein vollständiger Gestaltungsprozess umfasst einen sich permanent wiederholenden Kreislauf aus Planung, Errichtung und Erhaltung der Gestaltung mit dem Ziel einer sukzessiven Optimierung. Genutzt werden dabei die Beobachtungen und Reflexionen aus dem "Action-Learning-Prozess". Die folgende Auflistung enthält eine (unvollständige) Auswahl an Planungshilfen, Gestaltungsprinzipien und Überlegungen zur Erhaltung einer Gestaltung.


Permakultur orientiert sich vorwiegend an Erkenntnissen aus ökologischen und sozialen Systemen. Die Beobachtung und Reflexion systemischer Abläufe soll im Sinne des Systemdenkens zu einem ganzheitlichen Verständnis führen. Um dieses Verständnis auf jedes Permakultur-Konzept anwenden zu können, wurden frei kombinierbare Prinzipien entwickelt.

Aus der Beobachtung von Ökosystemen leitete Bill Mollison folgende Gestaltungsgrundsätze ab:

In seinem 2002 erschienenen Buch "Permaculture. Principles and Pathways Beyond Sustainability" geht David Holmgren insbesondere auf die kommenden Herausforderungen in Bezug auf die Energiesicherheit künftiger Generationen ein. Rund 25 Jahre nach Bill Mollisons "Permaculture One" sieht er in der Anwendung von Permakultur ein hilfreiches Instrumentarium für einen sanften und gleichzeitig produktiven Übergang von einer destruktiven High-Energy-Industriegesellschaft hin zu einer nachhaltigen und lebensfreundlichen Low-Energy-Kultur.

Holmgren definiert folgende 12 Gestaltungsprinzipien:

Die Erhaltung eines permakulturell gestalteten Systems zielt auf eine Optimierung im Sinne einer langfristigen Produktivität. Die Erweiterungen, bzw. Verfeinerungen der implementierten Gestaltungslösungen erfolgt durch kontinuierliche Beobachtung und Evaluation. Ziel ist eine bestmögliche Selbstregulation durch 

Die oben beschriebenen Prinzipien treten in der praktischen Anwendung nicht als getrennte Phänomene auf, sondern sind auf vielfältige Art miteinander verknüpft. So bilden zum Beispiel die durch Zonierung optimierten Randzonen selbstorganisierende Muster aus, die wiederum mit anderen Elementen des Systems in nutzbringende Interaktion treten können. Im Folgenden sollen hier einige Designprinzipien erläutert werden, um die damit verbundenen Ideen zu verdeutlichen.

Eine der oben beschriebenen Möglichkeiten, mit Permakultur zu planen, ist die Zonierung. Sie dient unter anderem der Energieeffizienz, zum Beispiel der Optimierung von zurückzulegenden Wegen. Für eine Selbstversorger-Landwirtschaft könnte eine Zonierung zum Beispiel so aussehen:

Dieses Zonensystem ist als Hilfsmittel anzusehen. Es handelt sich hierbei nicht um harte Grenzen. Auch die Zonierung als solche ist den Erfordernissen eines Systems frei anpassbar und folgt keinem starren Konzept. Üblicherweise werden jedoch, wie oben dargestellt fünf Zonen verwendet, oft erweitert durch eine sogenannte Zone 0. Sie ist eher philosophischer Natur und ermöglicht es dem Betrachter einer solchen Zonierung sich selbst zum Objekt seiner Betrachtungen zu machen.

Unter "Randzone" versteht man den Übergangsbereich zwischen verschiedenen Elementen eines Systems. Sie sind die Bereiche von Wechselwirkung (Beziehung und Austausch) zwischen den einzelnen Systemkomponenten. Durch das Aufeinandertreffen zweier unterschiedlicher Zustände kommt es im Bereich der Ränder zu zahlreichen Interaktionen. Je nach gewünschtem Effekt kann es sinnvoll sein, die Randzonen in einem System zu vergrößern oder zu verkleinern.

Der Übergang von einem Wald zur freien Landschaft (Waldrand) ist durch das Aufeinandertreffen zweier Ökosysteme eine besonders artenreiche und produktive Zone. Analog dazu könnte in einer Permakulturplanung z. B. eine Hecke oder ein Beetrand in geschwungenen Linien angelegt sein um die Randzone gegenüber einer linearen Anordnung zu vergrößern.

Umgekehrt wird beim energieeffizienten Hausbau argumentiert: durch die Verringerung von Randzonen, etwa der Außenwände, versucht man den Wärmeverlust an die Umgebung während der Heizperiode zu minimieren, da Vorsprünge, Erker und andere Anbauten wie Kühlrippen wirken.

In einem Fischteich zum Beispiel wird nicht nur eine Fischart aufgezogen, sondern mehrere Fischarten. Der Teich weist Zonen unterschiedlicher Tiefe und auch unterschiedlicher Bepflanzung auf. Die Raubfische werden nicht gefüttert, sondern ernähren sich von anderen Fischen. Diese finden aber für sich genügend günstige Nischen vor, sodass sie nicht aussterben. Das System reguliert sich als Ganzes weitgehend selbst. Der Mensch fischt „überschüssige“ Fische ab.

Gleichzeitig können essbare Pflanzen am Rand des Fischteichs angebaut werden. Weitere (und zum Teil dieselben) Pflanzen können ihrerseits den Pflegeaufwand des Teiches verringern (Reinigungspflanzen). Die Erhöhung der ökologischen Vielfalt sorgt für ein dynamisches Gleichgewicht, erhöht die Flexibilität und sichert kontinuierliche Erträge.

Aufbau und Pflege von Waldgärten "(engl.: forest garden)" in Europa basieren auf einem Konzept des Engländers Robert Hart. Im indischen Bundesstaat Kerala gibt es bereits eine jahrhundertealte Tradition, ebenso bei den Chagga am Kilimandscharo (insbesondere vor der Zeit des ausgeprägten Kolonialismus). Dabei werden naturnahe Waldwirtschaft und umfangreiche Nahrungsproduktion miteinander kombiniert.

Ein gesundes Ökosystem Mischwald besteht aus sich permanent regenerierenden Bäumen, Sträuchern, Büschen, Kräutern, Bodendeckern und Rankepflanzen. Diese Vielfalt an Vegetationsschichten unterschiedlicher Höhe gewährleistet eine optimale Lichtausbeute und stellt zahlreiche Nischen für eine Vielfalt dort lebender Tierarten bereit; hohe kontinuierliche Produktivität auf relativ kleinem Raum. Diese Beobachtung und die praktischen Erfahrungen aus Indien waren ausschlaggebend für die Entwicklung eines Waldgartenkonzeptes für das gemäßigte Klima in Mitteleuropa.

Die Skizze zeigt, wie sich ein solches Ökosystem prinzipiell mit einem gezielten Anbau unterschiedlicher essbarer Pflanzen kultivieren lässt: Walnuss- und Obstbäume in den hochgelegenen Schichten, darunter Beerensträucher und fruchttragende Büsche, und in Bodennähe unterschiedliche Kräuter bis hinab zu den Bodendeckern. In der Humusschicht lassen sich z. B. rhizombildende Gemüse anbauen. Durch eine intelligente Auswahl an kooperierenden Pflanzengesellschaften kann über mehrere Jahre hinweg ein sich tendenziell selbst erhaltender Waldgarten zur Nahrungsversorgung aufgebaut werden. Die ökologische Vielfalt sorgt für Flexibilität und Stabilität.

Waldgärten sind Beispiele für Agroforstsysteme.

→ "Hauptartikel: Transition Towns"
"Transition Town" bedeutet „Stadt im Übergang“ bzw. „Stadt im Wandel“.

Louise Rooney und Rob Hopkins transferierten 2004 die Permakulturidee, die bisher eher im ländlichen Raum Anwendung fand, in die Stadt. Bei ihren Überlegungen fließen zwei weitere Aspekte ein: der Klimawandel und das Erdölfördermaximum Peak Oil. Sie gehen davon aus, dass die Nutzung fossiler Energieträger wie Öl, Kohle und Gas zwar vielen Menschen einen hohen Lebensstandard ermöglicht, aber auch die meisten sozialen und ökologischen Probleme mitverursacht hat.

Mit "Energiewende. Das Handbuch. Anleitung für zukunftsfähige Lebensweisen" veröffentlichte Hopkins 2008 eine Sammlung von Ideen, Erfahrungen, Herangehensweisen, Beispielen und praktischen Werkzeugen, um in der eigenen Kleinstadt oder im Stadtteil gemeinsam einen lokalen Plan B auf den Weg zu bringen. Die Rückbesinnung auf lokale und regionale Wirtschaftskreisläufe, so Hopkins, werde eine entscheidende Rolle spielen, wenn in wenigen Jahren die Schere zwischen Erdölfördermenge und Nachfrage auseinandergehen und der Ölpreis rapide ansteigen werde.

Mithilfe von Gründüngung wird der Boden aufgelockert und die Bodengare verbessert. Pflanzenteile von stickstoffsammelnden (siehe Knöllchenbakterien) Leguminosen wie Süßlupinen (deren Samen als eiweissreiches Nahrungs- oder Futtermittel dienen können), Klee oder Robinien (deren Äste zudem gutes Holz für Pflanzenstützen ergeben). Akazienzweige in Afrika sowie die krautigen Stängel von ausdauernden (beispielsweise von Topinambur) oder eiweisshaltigen Pflanzen (beispielsweise Beinwell) werden im Ganzen oder gehäckselt als stickstoffhaltiges aufdüngendes Mulchmaterial verwendet. Wege zwischen Beeten oder Ackerfurchen werden mit Klee besamt, der (vor der Samenbildung gemäht) gleich daneben mehrmals im Jahr als Mulch zur Verfügung steht. Wegen der Mulchnutzung kommen neue Methoden wie Direktsaat oder Mulchsaat zur Anwendung.




</doc>
<doc id="10240" url="https://de.wikipedia.org/wiki?curid=10240" title="Bauhaus Dessau">
Bauhaus Dessau

Das Bauhaus Dessau, auch Bauhausgebäude Dessau, ist ein Gebäudekomplex in Dessau-Roßlau. Das Bauhausgebäude entstand 1925 bis 1926 nach Plänen von Walter Gropius als Schulgebäude für die Kunst-, Design- und Architekturschule Bauhaus. Kriegszerstörte und baulich veränderte Partien wurden ab 1965 weitgehend im Sinne des Originals rekonstruiert. Das Gebäude wurde 1976 restauriert und teilweise modernisiert. Zwischen 1996 und 2006 fand eine erneute Restaurierung und Instandsetzung nach denkmalpflegerischen Prinzipien statt.
Die Bauhausstätten in Dessau und Weimar gehören seit 1996 zum UNESCO-Welterbe.

Das im internationalen Stil errichtete Gebäude besteht aus fünf additiv in Flügelform angeordneten, funktional gegliederten Teilen: Einem Flügelbau der „Kunstgewerbe- und Handwerkerschule“ (spätere Technische Lehranstalten), dem Werkstättentrakt mit der markanten Glasvorhangfassade ("Curtain Wall") und dem Atelierhaus. Im Atelierhaus sind die Wohnateliers für die Studenten untergebracht. Der Nordflügel der Handwerkerschule und der Werkstättentrakt sind durch eine zweigeschossige "Brücke" verbunden. Diese Brücke war für Verwaltungsräume und das Baubüro von Gropius (später die Architekturabteilung des Bauhauses) gedacht. In einem flachen Bau zwischen Werkstättentrakt und Atelierhaus befinden sich Aula und Bühne sowie die Mensa des Bauhauses.
Das Besondere am ganzen Komplex ist neben der damals neuartigen funktionalen Trennung durch einzelne, zu einem Organismus gefügte Einzelbaukörper, die völlig in Glas aufgelöste Wand des Werkstättentraktes, die in der damaligen Zeit für große Aufregung sorgte. Die Stützen des Gebäudes sind komplett von der Glasfassade zurückgesetzt, sodass die Glasschürze über alle drei Geschosse und die gesamte Gebäudelänge reicht und nicht unterbrochen wird. Es herrscht der Eindruck von Transparenz, Leichtigkeit und Flächigkeit. Diese neuartige, transparente Monumentalität hatte alle herrschenden Vorstellungen von Ästhetik überwunden.

Ornamentlosigkeit bestimmt dabei konsequent den gesamten Komplex. Durch die „offene“ Fassade entsteht eine neue, auch pädagogisch wirksame Beziehung zwischen außen und innen, vermittelt den Eindruck von Freiheit und Übersichtlichkeit. Die feingliedrige Glasfassade (Curtain Wall) in Stahl verursachte jedoch große Probleme bezüglich Sonnenschutz und Gebäudeklimatisierung. Im Sommer heizte sich das Gebäude infolge der direkten Sonneneinstrahlung enorm auf. Ein notwendiges Sonnenschutzsystem aus Vorhängen wiederum zerstörte die beabsichtigte Transparenz. Im Winter kühlte das Gebäude aufgrund der Einfachverglasung sehr schnell aus und musste stark beheizt werden. Die Be- und Entlüftung erfolgt über mechanisch gesteuerte, im Detail sehr ausgearbeitete Lamellenflügel.

Bereits 1911 entwarfen Gropius und Meyer gemeinsam die Fagus-Werke in Alfeld (Leine). Dabei verwendeten sie Elemente, die später charakteristisch und stilbestimmend werden sollten: Der gesamte Baukörper war funktional gegliedert. Sein Verwendungszweck bestimmte die Form; seine Ästhetik ergab sich aus seiner Funktionalität. Auch die Fassadengestaltung ließ die zukünftige Nutzung erahnen. Die revolutionäre Stahlskelettbauweise – tragende Stahlelemente mit Ziegelausfachung bedeckt von einem Flachdach – ermöglichte den Verzicht auf statische Verstärkung der Gebäudeecken. Diese „offenen Ecken“ wurden ersetzt durch kantenumgreifende Verglasung sowie Balkone und vermittelten so einen Eindruck von Leichtigkeit. Dabei trug die vorgehängte Glasfassade (Curtain Wall) selbst keine Last, zeigte aber die tragenden Elemente, die so selbst gestaltende Teile wurden. Auch in der Farbgebung ging man neue Wege. Die äußeren Wände wurden in neutralem, schlichten Weiß gehalten, innen differenzierten Farben zwischen tragenden und verkleidenden Elementen. Jede der dreizehn Werkstätten leistete ihren speziellen Beitrag, als Kunst "und" Handwerk zugleich wirkten u. a. Metallwerkstatt, Tischlerei, Glasmalerei, Weberei, Wandmalerei, Harmonisierungslehre, vereint von der von Gropius, Meyer und später van der Rohe geleiteten Architekturwerkstatt.

Das Bauhaus wurde bereits 1919 unter der neuen Leitung von Walter Gropius, der Henry van de Velde ablöste, als Nachfolge-Institution der 1906 vom Großherzog von Sachsen-Weimar gegründeten Kunstschule bzw. Kunstgewerbeschule Weimar umbenannt und reorganisiert. Ihr Leiter war von 1919 bis 1928 der Architekt Walter Gropius. Nachdem das Verhältnis zum zunehmend von rechten politischen Kräften dominierten thüringischen Staat immer kritischer geworden war, musste das Bauhaus 1925 auf politischen Druck hin aufgelöst werden. Die Auflösungserklärung wurde bereits am 29. Dezember 1924 in zahlreichen Tageszeitungen veröffentlicht. Sie wurde aber erst mit Ablauf der Verträge, die bis 31. März 1925 Gültigkeit hatten, rechtskräftig. Der Bürgermeister von Dessau, Fritz Hesse, und sein Kulturreferent Ludwig Grote ermöglichten Gropius die Verlagerung der Schule nach Dessau, wo das Bauhaus in den Jahren 1925 bis 1926 nach Entwürfen von Gropius neu errichtet und 1926 als Staatliche Hochschule von Anhalt anerkannt wurde.

Im März 1925 erhielt das Büro Gropius von der Stadt Dessau den Auftrag, das Gemeinschaftsgebäude der Kunstgewerbe- und Handwerkerschule Dessau (ab 1926 Technische Lehranstalten) und des Bauhauses zu entwerfen. Im September 1925 war der Baubeginn für das gemeinsame Schulgebäude. Am 21. März 1926 war das Richtfest und am 4. Dezember 1926 die Einweihung.
Meister und Bauhaus-Werkstätten hatten große Teile der Einrichtung selbst geplant und durchgeführt: Möbel und Einbauten stammten aus der Tischlerei (Bestuhlung der Aula von Marcel Breuer). (Die Lehrräume im Brückentrakt sowie die Werkstätten wurden mit Hockern der Firma Rowac, Chemnitz, ausgestattet.) Die Lampen wurden in der Metallwerkstatt hauptsächlich von Marianne Brandt entworfen (Leuchtkörper in der Aula von Max Krajewsky,) Möbelstoffe und Vorhangstoffe entstanden in der eigenen Weberei unter Gunta Stölzl. Die Beschriftungen kamen aus der Reklamewerkstatt und die Farbgestaltung aus der Werkstatt für Wandmalerei.

Mit der Gründung 1926 wurde erstmals auch eine Architekturabteilung aufgebaut, deren Leitung 1927 der Schweizer Hannes Meyer übernahm. 1928 trat Gropius von der Leitung zurück. Der politisch stark engagierte Hannes Meyer übernahm am 1. April 1928 seine Nachfolge und baute die Architekturabteilung weiter aus, wurde aber ebenfalls aus politischen Gründen am 1. August 1930 entlassen. Ihm folgte Ludwig Mies van der Rohe, dem es trotz fachlich-wissenschaftlicher Stärkung der Schule nicht gelang, das Bauhaus aus den politischen Wirren herauszuhalten.

1931, also gut ein Jahr vor Hitlers Machtergreifung, gewann die NSDAP bei den Gemeinderatswahlen in Dessau 15 der 36 Sitze und war damit stärkste Fraktion. In ihrem Flugblatt zu den Wahlen am 25. Oktober 1931 forderten die Nationalsozialisten als ersten von acht Punkten:

Die NSDAP verlangte in der Gemeinderatssitzung am 21. Januar 1932 den Abbruch des Gebäudes. Dies und der Beschluss auf Streichung der Gelder konnten noch knapp verhindert werden. Am 8. Juli 1932 besichtigten der zum Ministerpräsidenten des Freistaats Anhalt gewählte Nationalsozialist Alfred Freyberg und der nationalsozialistische Kunsttheoretiker und Architekt Paul Schultze-Naumburg das "Bauhaus Dessau". Da sich inzwischen die Stimmverhältnisse im Gemeinderat geändert hatten, erfolgte am 22. August 1932 auf Antrag der NSDAP-Fraktion der Beschluss zur Schließung. Mies van der Rohe versuchte noch die Fortführung als Privatinstitut in Berlin-Lankwitz; aber schon kurze Zeit später (1933) wurde die Institution von den Nationalsozialisten endgültig zur Selbstauflösung gezwungen. Das Gebäude diente als Gauführerschule im Gau Magdeburg-Anhalt.

1945 brannte das Gebäude nach dem schweren Luftangriff auf Dessau teilweise aus, auch die Glasfassade des Werkstattflügels war zerstört. Es wurde vereinfacht wieder aufgebaut (die Glasvorhangfassade wurde nicht rekonstruiert), und u. a. als Berufsschule genutzt.

1976 gab es einen ersten Versuch der Rückführung auf das originale Erscheinungsbild, bei dem auch die zerstörte Glasvorhangfassade anhand eines erhaltenen Reststückes rekonstruiert wurde. Dabei wurde aus Gründen der Wartungsfreundlichkeit Aluminium statt Stahl eingesetzt. Das Bauhaus wurde als Bildungszentrum vom Amt für Industrielle Formgestaltung genutzt, dessen Direktor Martin Kelm sich stark für den Erhalt und Wiederaufbau eingesetzt hatte.

Das Haus des Bauhausdirektors (Burgkühnauer Allee 1-6, jetzige Ebertallee) wurde durch Luftangriff zerstört, die Meisterhäuser beschädigt.

Seit 1994 ist das Gebäude in Dessau Sitz der Stiftung Bauhaus Dessau, die „das Erbe des historischen Bauhauses zu bewahren und der Öffentlichkeit zu vermitteln“ und „angesichts dieses Erbes Beiträge zur Gestaltung der heutigen Lebensumwelt zu leisten“ verpflichtet ist. Seit 1996 ist der Gebäudekomplex in der Liste des Weltkulturerbes der UNESCO eingetragen.

Zwischen 1996 und 2006 wurde das Bauhaus Dessau für 17 Millionen Euro nach den Plänen der 1920er Jahre und Befunden instand gesetzt und restauriert.

Das Bauhaus Dessau wurde 2001 in das Blaubuch aufgenommen. Das Blaubuch ist eine Liste national bedeutsamer Kultureinrichtungen in Ostdeutschland und umfasst zurzeit 20 sogenannte kulturelle Leuchttürme.

Nachdem die letzten Instandsetzungsarbeiten 2009 abgeschlossen wurden, konnte das Gebäude wieder annähernd so besichtigt werden, wie es ursprünglich geplant und gebaut wurde. Dennoch gibt es Unterschiede zur originalen Bausubstanz, die aufgrund der wechselvollen Geschichte des Gebäudes, modernen Notwendigkeiten und denkmalpflegerischen Überlegungen nicht aufzulösen sind. Diese umfassen unter anderem:


Heute wird der Großteil des Gebäudekomplexes von der Stiftung Bauhaus Dessau für Büroräume genutzt. Das Bauhaus verfügt zudem über eine eigene Bibliothek. Die ehemaligen Studentenwohnungen auf der Balkonseite des Ostflügels werden als Appartements vermietet. Des Weiteren besteht ein Pachtvertrag mit der Hochschule Anhalt. Derzeit werden sechs Räume im Parterrebereich des Nordflügels für Lehrtätigkeiten genutzt.

Im Werkstättenflügel sind Lehrräume des "Bauhaus Kollegs", des Postgraduiertenprogramms der Stiftung Bauhaus Dessau, untergebracht. Interdisziplinäre, internationale Studiengruppen entwickeln hier Lösungen für Probleme der Architektur und Urbanistik. Zudem ist die Koordinierungsstelle der Internationalen Bauausstellung Stadtumbau Sachsen-Anhalt 2010 untergebracht, die Konzepte für städtebauliche Problemstellungen in Sachsen-Anhalt erarbeitet.
Zum 100-jährigen Bauhausjubiläum 2019 wird in der Dessauer Innenstadt ein neu gebautes Museum eröffnet.

In der Nähe des Bauhauses (Ebertallee 65–71, ) baute Walter Gropius die "Meisterhäuser" als Unterkunft für die Meister des Bauhauses. Es waren gleichzeitig Musterhäuser für modernes Wohnen. Bauherr war die Stadt Dessau, die Bauhausmeister wohnten zur Miete.

Von Ost nach West waren es das Einzelhaus Gropius, sowie jeweils die Doppelhäuser Moholy-Nagy/Feininger, Muche/Schlemmer und Kandinsky/Klee. Die drei Doppelhäuser wiesen identische Grundrisse auf, wobei die eine Hälfte jeweils fast das um 90 Grad gedrehte Spiegelbild der anderen war.

Kennzeichnend für die Architektur der Häuser sind die kubische Form mit Flachdach, große, einfarbige Flächen und große Fenster, die eine Verbindung von Innen und Außen herstellen. Diese Verbindung wird auch durch die großen Terrassen und Balkone sowie die zahlreichen Türen thematisiert: Von nahezu jedem Raum aus ist es möglich, durch eine Tür nach draußen zu treten. Stark von außen sichtbare Elemente sind auch die Heizkörper der Zentralheizung, mit denen das „Moderne“ nach außen für jedermann sichtbar transportiert werden sollte. Dies führte sogar dazu, dass z. B. in den Badezimmern die Heizkörper an thermisch ungeeigneten Stellen angebracht, dafür aber von außen gut durch die Fenster sichtbar waren.

Die großen Atelier-Fenster der Häuser spiegeln den Baumbestand vor den Häusern wider und lassen ihn mit den gleichen hinter den Häusern stehenden Bäumen verschmelzen. Das führt dazu, dass diese Teile der Häuser in gewissen Sinne unsichtbar werden oder durchsichtig wirken. Es lässt sich nicht sagen, ob dieser Effekt der Leichtigkeit bzw. Offenheit bereits von den Bauherren beabsichtigt war, da der damalige Baumbestand im Detail nicht überliefert ist.

Die Meisterhäuser von Gropius und Moholy-Nagy wurden durch ein Bombardement 1945 vernichtet. In den 1950er Jahren wurde auf den Fundamenten des zerstörten Hauses Gropius ein Wohnhaus in traditioneller Satteldach-Bauweise errichtet (Haus Emmer). Die zerbombte Haushälfte von Moholy-Nagy wurde abgetragen und eine Freifläche geschaffen, sodass das Haus Feininger allein stand (gegenwärtig wird es vom Kurt-Weill-Zentrum genutzt).

Die noch bestehenden Häuser wurden nach 1990 z. T. mit privaten Mitteln aufwändig restauriert. Dabei wurde auch versucht, die ursprüngliche farbliche Gestaltung der Innenräume, die auf die Farbenlehre des Bauhauses zurückging, wiederherzustellen. Da die farbliche Gestaltung des Innenraums auch vom jeweiligen Bewohner abhing, findet man in den Räumen heute beispielhafte Farbgebungen, die jeweils nur den Zustand eines Raumes zu einer bestimmten Zeit wiederzugeben versuchen.

Auf den wenigen vorhandenen historischen Aufnahmen der Inneneinrichtungen ist zu sehen, dass die Bewohner der Meisterhäuser die Innenraumgestaltung sehr dem damaligen Zeitgeist anpassten, ganz entgegengesetzt dem äußeren Erscheinungsbild. Nur Moholy-Nagy richtete sein Haus nach den Ergebnissen, Vorgaben und Produkten des Bauhauses ein. Im Haus Kandinsky ist heute eine Wand originalgetreu mit Blattgold belegt rekonstruiert.

Inzwischen sind die im Krieg zerstörten Meisterhäuser Gropius und Moholy-Nagy auf Anregung des britischen Architekten David Chipperfield hin unter Leitung des Berliner Büros Bruno-Fioretti-Marquez als abstrakte Neuinterpretationen der ursprünglichen Architektur wieder aufgebaut worden. Die Innenwände wurden von dem Konzeptkünstler Olaf Nicolai mit unterschiedlichen Verputzarten und Weißtönen gestaltet, was je nach Lichteinfall einen wechselnden Eindruck ergibt. Die offizielle Wiedereröffnung der Meisterhäuser erfolgte am 16. Mai 2014 durch Bundespräsident Gauck. Die frühere Diskussion, ob die Häuser originalgetreu rekonstruiert werden sollen, ist damit überholt.

Eine in der Nachbarschaft zur Meisterhaussiedlung nach Plänen von Ludwig Mies van der Rohe erbaute Trinkhalle wurde in den 1970er Jahren abgebrochen. 2013 wurde die Trinkhalle im Rahmen der städtebaulichen Reparatur der Meisterhaussiedlung wiedererrichtet.

Weiterhin entstanden in Törten im heutigen Dessau-Süd 1926/1928 eine Siedlung mit Reihenhäusern (Großring, Mittelring, Kleinring) sowie 1930 die "Laubenganghäuser" (Mittelbreite, Peterholzstr.) in industrieller Bauweise. Hier wurden wesentliche Auffassungen der Architekten des Bauhauses wie Sonneneinstrahlung zu verschiedenen Tages- und Jahreszeiten, die Lebensabläufe in einem Wohnhaus sowie die Nutzung der anliegenden Gärten für die eigene Versorgung und die Erholung berücksichtigt.
Hier wurde umgesetzt, was Walter Gropius bereits formuliert hatte: Bauen ist das Gestalten von Lebensvorgängen.
Durch die industrielle Bauweise wurden günstige Preise erreicht, die für die Käufer einen Ausweg aus den steigenden Mieten durch Kauf eines Eigenheimes ermöglichen sollten. Wegen der Flachdachbauweise wurden die Häuser von Konservativen stark kritisiert.

Die Siedlung hat zahlreiche nachträgliche Änderungen erfahren. Insbesondere die Fensterfronten sind fast durchweg verändert. Zahlreiche individuelle Fassadengestaltungen haben den ursprünglichen einheitlichen Eindruck der Siedlung, die trotz dieser Umbauten noch gut erhalten ist, aufgeweicht. Das "Haus Anton" am Mittelring 35 wurde ab 1992 als erstes originalgetreu wiederhergestellt und ist im Rahmen einer Führung zu besichtigen. Haus 38 wird heute von der Moses-Mendelssohn-Gesellschaft genutzt.

Das 1928 nach einem Entwurf von Walter Gropius entstandene "Konsumgebäude" (eine Art umbauter Kaufhalle) wurde zu einem Zentrum der Törtener Siedlung. Es besteht aus zwei ineinandergeschobenen Kuben, einem horizontalen Ladenteil und einem vertikalen dreigeschossigen Wohnteil. So wird es heute noch genutzt. In dem ehemaligen Ladenteil befindet sich heute ein Informationszentrum zur Siedlung Törten, das täglich Führungen anbietet.

Nahe der Siedlung, in der Südstraße, steht das "Haus Fieger". Das im Sommer 1927 gebaute Haus ist der einzige umgesetzte Entwurf von Carl Fieger aus einer Reihe von Plänen für Kleinhäuser, die in rationeller Bauweise mit wandlungsfähigen Räumen entstehen sollten. Als privat genutztes Wohnhaus ist es nicht zu besichtigen.
Das so genannte "Stahlhaus" wurde in den Jahren 1926/1927 errichtet und war ein Gemeinschaftswerk von Richard Paulick und dem Bauhaus-Meister Georg Muche. Sie wollten die Rationalisierungsbestrebungen von Walter Gropius (Vorfertigung von Betonteilen) fortsetzen, indem sie vorgefertigte Stahlplatten im Trockenmontageverfahren benutzten. Das Stahlhaus blieb jedoch ein Experiment, weil es wegen der Eigenschaften des Werkstoffs sehr mit dem „Warm-Kalt-Problem“ zu kämpfen hatte. Nach Restaurierung befand sich darin bis Juni 2011 ein Informationszentrum zur o. g. Siedlung Törten. Heute kann es im Rahmen von täglich stattfindenden Führungen besichtigt werden.
Das erste städtische "Arbeitsamt" (heute Amt für Ordnung und Verkehr der Stadt Dessau-Roßlau) entstand 1928/1929 nach Entwürfen von Walter Gropius. Das private Baubüro von Gropius führte auch den Bau aus. Richard Paulick war maßgeblich am Bau des Arbeitsamtes beteiligt, dessen äußerer Eindruck allerdings durch nachträglich eingesetzte Holzfenster massiv verändert wurde.

Die Ausflugsgaststätte "Kornhaus" wurde 1929/1930 im Auftrag der Stadt Dessau und der Schultheiss-Patzenhofer-Brauerei direkt auf dem Elbdeich nach Plänen von Carl Fieger errichtet. Der Name erinnert an einen alten Getreidespeicher, der hier unmittelbar an der Elbe von Mitte des 18. Jahrhunderts bis in die 1870er Jahre stand. Das Gebäude wird auch heute noch als Gaststätte genutzt.
Die Bauhaustour ist ein 17 km langer Radwanderweg. Der ausgeschilderte Rundweg verbindet alle Baudenkmale der Bauhausarchitektur in Dessau. Karte




Artikel


</doc>
<doc id="10241" url="https://de.wikipedia.org/wiki?curid=10241" title="Bauhaus">
Bauhaus

Das Staatliche Bauhaus wurde 1919 von Walter Gropius in Weimar als Kunstschule gegründet. Nach Art und Konzeption war es damals etwas völlig Neues, da das Bauhaus eine Zusammenführung von Kunst und Handwerk darstellte. Das historische Bauhaus stellt heute die einflussreichste Bildungsstätte im Bereich der Architektur, der Kunst und des Designs im 20. Jahrhundert dar. Das Bauhaus bestand von 1919 bis 1933 und gilt heute weltweit als Heimstätte der Avantgarde der Klassischen Moderne auf allen Gebieten der freien und angewandten Kunst. Die Resonanz des Bauhauses hält bis heute an und prägt wesentlich das Bild modernistischer Strömungen.

Das Bauhaus entstand in Weimar durch die Vereinigung der Großherzoglich-Sächsischen Kunstschule Weimar mit der 1907 von Henry van de Velde gegründeten Großherzoglich-Sächsischen Kunstgewerbeschule Weimar. Sie wurde zum direkten Vorläufer des Bauhauses, das dann in van de Veldes Schulgebäuden seine Arbeit aufnahm. 1925 erfolgte der Umzug nach Dessau – ab 1926 im Gebäude des Bauhauses Dessau. 1932 musste das Bauhaus nach Berlin umziehen und wurde 1933 geschlossen.

Der Einfluss des Bauhauses war so bedeutend, dass umgangssprachlich der Begriff "Bauhaus" oft auch mit der Moderne in Architektur und Design gleichgesetzt wird. Kunstgeschichtlich ist es jedoch problematisch, den sogenannten Bauhausstil und die Entwicklungen in Deutschland isoliert zu betrachten und "Bauhaus" als Stilbegriff, als Architekturstil oder Möbelstil, zu verwenden. Die Entwürfe und Arbeiten von Lehrern und Schülern am Bauhaus werden daher als Teil von länderübergreifenden, längerfristigen Strömungen gesehen und unter Begriffen wie Funktionalismus, Klassische Moderne, Neue Sachlichkeit, Internationaler Stil, Neues Bauen eingeordnet.

Im Bauhaus wurden die traditionell getrennten Bereiche der bildenden Kunst, der angewandten Kunst und der darstellenden Kunst auf der Grundlage des Konzeptes miteinander verbunden, was wiederum starke Ausstrahlung auf Malerei, darstellende Kunst und Musik hatte.

Die ursprünglichen Intentionen von Henry van de Velde und Walter Gropius waren, die Kunst von der Industrialisierung zu emanzipieren und das Kunsthandwerk wiederzubeleben. Damit bildeten sie einen Gegenentwurf zur Ästhetik des Historismus, in der kunsthandwerklich entwickelte Ornamente durch industrielle Massenproduktion seriell kopiert wurden. Mit dem Begriff „Kunst“ wurde nicht die damalige Avantgarde bezeichnet, sondern die Formensprache der zeitgenössischen Entwerfer für die Produktion im Stil vergangener Epochen. Mit der Rückbesinnung auf das Handwerk war die gestalterische Intention damit verbunden, experimentell und manuell eine neue Formensprache zu entwickeln, die dem industriellen Herstellungsprozess gerecht wird.

Ein Leitbild des Bauhauses war, die Architektur als Gesamtkunstwerk mit den anderen Künsten zu verbinden. Deshalb verkündete das Bauhaus im Gründungsmanifest von 1919 auch: „Das Endziel aller bildnerischen Tätigkeit ist der Bau“. Im Laufe der Entwicklung resultiert jedoch besonders das heutige Industrie- und Grafikdesign aus diesen Ideen. In der Architektur hat sich das modulare Bauen nicht nur bei Industrieanlagen, sondern auch bei der Schaffung günstigen Wohnraums zum Beispiel in Satellitenstädten von Megametropolen durchgesetzt.

Das „Staatliche Bauhaus“ war vom Gründer Walter Gropius als eine Arbeitsgemeinschaft gedacht, in der die Unterscheidung zwischen Künstler und Handwerker aufgehoben werden sollte. Durch ihr Schaffen wollten die Mitarbeiter des Bauhauses gesellschaftliche Unterschiede beseitigen und zum Verständnis zwischen den Völkern beitragen. In Intention und Ergebnissen bestanden damit vielfältige Ähnlichkeiten und Verbindungen mit dem 1907 gegründeten Deutschen Werkbund, dessen Mitglied Walter Gropius bis 1933 war.
Vorgängerorganisation für das Bauhaus war das im Jahre 1902 von Henry van de Velde gegründete „Kunstgewerbliche Seminar“ und das etwas später als Lehranstalt konzipierte „Kunstgewerbliche Institut“, das seinen Lehrbetrieb im Jahr 1907 aufnahm. Unstrittig ist, dass bereits van de Velde eine erhebliche Anzahl jener Methoden und Prinzipien etablierte, für die das spätere Bauhaus berühmt geworden ist. Der belgische Architekt ließ sich dabei unter anderem von japanischer Innenarchitektur der Edo-Zeit inspirieren.

Das "Staatliche Bauhaus in Weimar" entstand am 12. April 1919 aus der Vereinigung der Großherzoglich-Sächsischen Hochschule für Bildende Kunst in Weimar und der 1915 aufgelösten Kunstgewerbeschule Weimar. Auf Vorschlag Henry van de Veldes, des bisherigen Direktors der Großherzoglich-Sächsischen Hochschule, wurde als dessen Nachfolger Walter Gropius ernannt. Dieser gab der neuen Schule ihren Namen.

Als Lehrer konnte Gropius bedeutende Künstler wie Lyonel Feininger, Johannes Itten, Josef Albers, Paul Klee (ab 1921), Wassily Kandinsky (ab 1922) und Oskar Schlemmer (ab 1921) für das Bauhaus gewinnen. Die Lehre am Bauhaus bestand aus dem Vorkurs und der Arbeit in den Werkstätten. Die Werkstätten wurden von den Künstlern, die sich Meister der Form nannten und den Handwerksmeistern geleitet. Anfangs mischten sich romantische Rückwendung in vorindustrielle Produktionsweisen mit modernen gestalterischen Ansätzen. Projekte des Bauhauses, wie das „Haus Sommerfeld“ waren in dieser Phase noch sehr expressionistisch geprägt. Großen Einfluss auf die Lehrer und Schüler des Bauhauses übte von 1921 bis 1922 der Niederländer Theo van Doesburg, Gründungsmitglied der De Stijl-Bewegung, aus. Er war nicht offiziell am Bauhaus tätig – eine Festanstellung Doesburgs als Meister hatte Gropius abgelehnt –, sondern gab in Weimar Privatkurse über architektonische Gestaltung. Insbesondere die Rückbesinnung auf einfache kubische Formen ist seinem Wirken zuzuschreiben.

1923 kam der Konstruktivist László Moholy-Nagy als Nachfolger des Malers Johannes Itten, der für ganzheitliche lebensreformerische Ideen stand. Beispielhaft, und für die Ausbildung an Kunst- und Designschulen in aller Welt bis heute prägend, ist der gemeinsame Vorkurs, bei dem Wert auf eine vielseitige und umfassende Ausbildung der Schüler gelegt wurde. 1920 wurde von Adolf Meyer eine Architekturabteilung ins Leben gerufen, eine systematische Architektenausbildung gab es am Bauhaus jedoch noch nicht. Das Musterhaus „Am Horn“ in Weimar wurde 1923 das erste Projekt, das konsequent in Architektur und Einrichtung von der Neuen Sachlichkeit, wie sie insbesondere die niederländische Richtung „De Stijl“ vorgab, geprägt war. In der Öffentlichkeit galten diese Bauten als „kalt“, „karg“ und „maschinell“.

Ab 1922 wurde auf Drängen des thüringischen Landesparlaments eine Ausstellung konzipiert, die die ersten Ergebnisse der neuen Hochschule zeigen sollte. Das Bauhaus selbst hielt diesen Zeitpunkt für zu verfrüht, um schon etwas mehr als Konturen des neuen und zu diesem Zeitpunkt noch weltweit einzigartigem pädagogischen Systems vorzuzeigen. Trotzdem konzentrierte Walter Gropius die Kräfte der Hochschule auf die Ausrichtung dieser ersten Ausstellung, die vom 15. August bis 30. September 1923 zu sehen war.

Eingeleitet wurde die Ausstellung von der so genannten "Bauhauswoche". Diese fand beim Publikum und der Presse sehr großen Anklang. Gropius eröffnete die Bauhauswoche mit einem Vortrag über "Kunst und Technik – eine neue Einheit". Es folgten Vorträge von Kandinsky ("Über synthetische Kunst") und dem holländischen Architekten Oud über die Entwicklung der modernen niederländischen Baukunst. Im Deutschen Nationaltheater wurde Oskar Schlemmers "Triadisches Ballett" aufgeführt sowie ein Konzert mit sechs Klavierstücken (darunter vier Uraufführungen) von Busoni und der Erstaufführung der "Marienlieder" von Hindemith; im Jenaer Theater war Kurt Schmidts "Mechanisches Ballett" zu sehen. Eine Matinee brachte am Abschlusstag unter der Leitung von Hermann Scherchen die Wiederholung der Erstaufführung von Strawinskis "Geschichte vom Soldaten". Der Tag klang mit Lampionfest, Feuerwerk, Tanz und der Vorführung der "Reflektarischen Lichtspiele" von Hirschfeld-Mack aus. Begleitet wurde die Ausstellung durch die Herausgabe des Manifestes "STAATLICHES BAUHAUS IN WEIMAR 1919-1923" in einer Auflage von 2600 Exemplaren, 2000 in deutscher, 300 in englischer und 300 in russischer Sprache hergestellt. Der Einbandentwurf stammte von Herbert Bayer.

Einer der wichtigsten Bestandteile der Ausstellung war das Musterhaus „Am Horn“, ein Versuchshaus nach einem Entwurf Georg Muches, das von Walter March und Adolf Meyer aus Gropius’ Architekturbüro gebaut wurde. Die Ausstattung war ein Gemeinschaftswerk aller Werkstätten des Bauhauses.

In der Zeit der Weimarer Republik galten Lehrer, Schüler und Bewunderer des Bauhauses als „links“ und „internationalistisch“. Politisch rechte Parteien lehnten das Bauhaus von Anfang an ab. Nachdem sich die Machtverhältnisse nach der Landtagswahl in Thüringen im Februar 1924 geändert hatten, kürzte die Regierung unter Richard Leutheußer (DVP) den Etat um 50 %. Daraufhin boten sich andere Städte den Lehrern und Schülern als neue Standorte an (z. B. der Kölner Oberbürgermeister Konrad Adenauer, der dann aber die Kölner Werkschulen gründete). Finanziell und politisch von der Thüringer Regierung unter Druck gesetzt, beschloss der Meisterrat 1925 den Umzug nach Dessau. Dort bot der Flugzeugbauer Hugo Junkers eine Förderung, zudem herrschte in dieser Industriestadt eine stabile sozialdemokratisch und liberal orientierte Mehrheit. Die Nutzung des Namens Bauhaus am neuen Standort erstritt sich Gropius vor Gericht, wer nach 1925 in Weimar blieb, durfte nicht mehr den Namen der Institution nutzen. Zahlreiche Lehrkräfte verließen daraufhin das Bauhaus, darunter der Architekt Adolf Meyer, welcher zum Projekt Neues Frankfurt ging, und der Grafiker Karl Peter Röhl, der an die Frankfurter Städelschule wechselte.

1925 erfolgte der Umzug nach Dessau. Dort entstanden die ersten Möbel aus dem neuartigen Material Stahlrohr und – von Marcel Breuer (der die Rechte am „Freischwinger“ besitzt), Mart Stam und Ludwig Mies van der Rohe entworfen – die ersten Freischwinger. Die Zusammenarbeit mit der Industrie begann. Um die am Bauhaus geleistete Arbeit und wichtige Tendenzen der Moderne darzustellen, wurde von 1925 bis 1930 die Buchreihe Bauhausbücher herausgegeben. Außerdem erschien seit Dezember 1926 vierteljährlich die Zeitschrift "bauhaus".

Am 4. Dezember 1926 wurde das neue, von Walter Gropius entworfene Bauhausgebäude eingeweiht. Der vollständig verglaste Werkstattflügel zur Straßenseite beeindruckte besonders, ebenso die gleichzeitig errichteten und ebenfalls von Gropius entworfenen „Meisterhäuser“, die als Wohnhäuser fungierten und wie das Bauhausgebäude konsequent und mustergültig die entwickelten Vorstellungen von Wohnen und Arbeiten vereinten. Die Lampen für das neue Gebäude wurden hauptsächlich von Marianne Brandt entworfen.

Am 1. April 1928 trat Gropius als Direktor zurück. Auf seinen Vorschlag wurde der Schweizer Architekt Hannes Meyer neuer Direktor, der für das Bauhaus nicht nur die Devise „Volksbedarf statt Luxusbedarf“ ausgab, sondern auch die Zusammenarbeit mit der Industrie intensivierte und eine Konzentration auf das Fach Architektur bewirkte. Meyer, der linkssozialistische Positionen vertrat, blieb bis zu seiner fristlosen Entlassung durch den Oberbürgermeister von Dessau am 1. August 1930 Direktor.

Ab 1930 leitete der Architekt Ludwig Mies van der Rohe das Bauhaus in Dessau. 1931 gewann die NSDAP die Gemeindewahl in Dessau und setzte 1932 die Schließung des staatlichen Bauhauses durch. Die KPD stimmte dagegen, die SPD enthielt sich. Ludwig Mies versuchte das Bauhaus durch Umzug nach Berlin als private Einrichtung fortzuführen.

1945 wurde das Gebäude des Bauhauses in Dessau teilweise zerstört. Es wurde erst 1976 rekonstruiert.

1932 wurde das Bauhaus als private Einrichtung nach Berlin-Lankwitz verlegt; aber schon kurze Zeit später, 1933, wurde die Institution von den Nationalsozialisten durch Repressalien wie Hausdurchsuchungen, Versiegelung der Räume und Verhaftung von Studenten endgültig zur Selbstauflösung gezwungen.

Viele Bauhausmitglieder emigrierten und trugen so zur internationalen Verbreitung der Ideen des Bauhauses bei.

Ab den 1930er-Jahren errichteten emigrierte jüdische Bauhaus-Architekten in Tel Aviv mehr als 4.000 Gebäude. Diese Weiße Stadt besitzt seit 2003 den Status eines UNESCO-Weltkulturerbes. Tel Aviv hat weltweit die größte Ansammlung von Gebäuden in Bauhaus-Stil.

Die Nationalsozialisten waren zwar gegen den Dessauer Modernismus, aber über den früheren Bauhausschüler Fritz Ertl trat nach Einschätzung von Cohen der nüchterne Funktionalismus des Bauhaus in der Architektur des Konzentrationslagers Auschwitz hervor.

Einige der Protagonisten des Bauhauses, wie Josef Albers, Walter Gropius, László Moholy-Nagy, Herbert Bayer und Ludwig Mies van der Rohe, emigrierten in der Folgezeit in die USA, wo – mit einem besonderen Schwerpunkt beim Black Mountain College – bald schon der Einfluss der Bauhaus-Lehr- und Entwurfskonzeption deutlich wurde. Besonders in der Architektur, aber auch im Produkt- und Kommunikationsdesign, setzten sich Methoden und Lehrsätze des Bauhauses rasch durch.

László Moholy-Nagy selbst führte das Konzept des Bauhauses ab 1937 im amerikanischen Exil als New Bauhaus in Chicago weiter (heute IIT Institute of Design). Ursprünglich sollte Walter Gropius dessen Direktor werden, dieser war jedoch einem Ruf als Professor für „Graduate School of Design“ der Harvard University gefolgt und empfahl daraufhin Moholy-Nagy.

Nach Ende des Zweiten Weltkriegs entstand 1953 in Ulm die "Hochschule für Gestaltung" (HfG Ulm), die zunächst von dem Bauhausabsolventen Max Bill geleitet und nach dem Vorbild des Bauhauses konzipiert wurde, einen ähnlichen sachlichen Stil weiterführte und unter anderem die neuen Einflüsse der Schweizer Typografie in ihre Akzidenzen und Grafiken einband, jedoch auf Kunst bewusst verzichtete.

Die Kunsthalle Darmstadt zeigte 1961 die Ausstellung „Bauhaus“. Darmstadt war zugleich Sitz des Bauhaus-Archivs; an der Mathildenhöhe sollte auch das neue Archivgebäude nach Plänen von Gropius entstehen, bevor dieses nach Berlin abgeworben wurde.

Anfang der 1970er-Jahre kam eine Reihe von Möbeln und Gebrauchsobjekten als lizenzierte Reeditionen auf den Markt, die bis heute die Vorstellung eines einheitlichen Bauhausstils prägen. Charakteristisch für diese Möbel ist der Verzicht auf die einstige Buntheit zugunsten der Farben schwarz, weiß, chrom.

1996 wurde das Bauhaus-Gebäude in Dessau, das seit 1986 vom „Bauhaus Dessau – Zentrum für Gestaltung“ genutzt wurde, in das UNESCO-Weltkulturerbe aufgenommen. Es ist heute Sitz der 1994 gegründeten Stiftung Bauhaus Dessau.

Arbeiten und Dokumente aus dem Bauhaus sowie einschlägige Literatur werden im Bauhaus-Archiv (Berlin) und im Bauhaus-Museum (Weimar) gesammelt und der Öffentlichkeit zugänglich gemacht.

2008 wurde in Tel Aviv in der Weißen Stadt ein Bauhaus-Museum errichtet.

Das Bauhaus nahm in der Geschichte von Kunst, Design und Architektur eine besondere Rolle ein. Die Schule hatte bei ihrer Gründung 1919 zunächst nicht viel mit Fotografie zu tun. 1923 eröffnete Walter Gropius die Bauhaus-Ausstellung in Weimar mit einem Vortrag namens „Kunst und Technik – eine neue Einheit“. Dies zeigt, dass sich innerhalb der Schule ein Wandel vollzogen hatte. Nicht allein der Bau war nun das Schaffensziel der Schule, die Möglichkeiten der industriellen Produktion sollten vermehrt genutzt werden. In den Werkstätten der Schule entstehen nun Objekte, wie Lampen, Stühle und ganze Kücheneinrichtungen, die für die Massenfertigung bestimmt sind und den bloßen Bau ergänzen sollen.

Diese Hinwendung zum Industriedesign und die Gestaltung von Gegenständen war es, die der Fotografie am Bauhaus den entscheidenden Aufwind verlieh. Die Ausstellung von 1923 wurde von einem umfangreichen Katalog begleitet, der neben Abbildungen von Architektur auch einige Abbildungen der am Bauhaus entworfenen und gebauten Gegenstände enthielt. Es wurde schnell klar, dass das Medium der Fotografie Vorteile für den Auftritt der Schule bot. Dank der Fotografie war es nun möglich, die geschaffenen Objekte einer breiteren Öffentlichkeit zu präsentieren. Bisher wurden zum Fotografieren der gewünschten Dinge jedoch noch externe Fotografen beauftragt, die das ohnehin schon relativ schmale Budget der Schule weiter belasteten.

Lucia Moholy gab schließlich den Impuls zur Verlagerung der Fotografietätigkeiten ans Bauhaus. Ihr Mann Lászlo Moholy-Nagy war bereits für seine fotografischen Experimente am Bauhaus bekannt. Darüber hinaus war sie es, die zusammen mit Walter Gropius den bereits erwähnten Ausstellungskalender gestaltete. Im Sommer 1923 absolvierte Lucia Moholy eine Lehre als Reproduktionsfotografin beim Weimarer Fotografiemeister Otto Eckner. In den darauf folgenden Jahren zwischen 1924 und 1928 entstand ein umfassender Bestand von Produktaufnahmen und Architekturbildern, die das Bild der Schule bis heute prägen. Der Stil von Lucia Moholy war dabei technisch, dokumentarisch und klar.

Dies liegt möglicherweise daran, dass sie ihr Handwerk erlernte und es sich nicht autodidaktisch beibrachte. Ihr Ziel war weniger das Experiment mit dem Medium der Fotografie an sich. Vielmehr versuchte sie, das Abzubildende klar, realistisch und sachlich zu zeigen. Lászlo und seine Frau waren bald nicht länger die einzigen am Bauhaus, die sich dem Fotografieren widmeten. Besonders unter dem Einfluss der experimentellen Fotografie von Lászlo Moholy-Nagy entwickelte sich um das Jahr 1927 am Bauhaus eine lebendige Fotografieszene, die neben Architektur und Objektaufnahmen zum Beispiel auch Szenen des täglichen Lebens und Porträts festhielt. Hinzu kamen Fotocollagen und -montagen.

Obwohl man beim neuen Sehen oft von "Bauhausfotografie" spricht, ist dies eigentlich nicht ganz korrekt. Bis ins Jahr 1929 gab es zum Beispiel keinen festen Fotografiekurs im Lehrplan der Schule. Erst im Jahr 1929 wurde die Fotoabteilung gegründet, deren Leitung Walter Peterhans übernahm. Hier lernten die Studenten nun die fotografische Theorie und auch das präzise Sehen. Der Begriff der "Bauhausfotografie" als Benennung einer Stilrichtung ist auch deshalb nicht ganz zutreffend, weil sich die vielen verschiedenen Stile der Schüler und Lehrer kaum unter einem Namen zusammenfassen lassen.

Das Studium am Bauhaus war aufgeteilt in drei Abschnitte. Die "Vorlehre" bestand aus einem halben Jahr Formunterricht und Materialübungen. Danach erfolgte die Aufnahme in die "Werklehre". Dabei konnte zwischen verschiedenen Lehrwerkstätten gewählt werden. Der dritte Abschnitt bestand aus der "Baulehre". Diese bestand aus der Mitarbeit am Bau mit bedingungsabhängiger Dauer. Als Abschluss wurde ein Meisterbrief der Handwerkskammer und bei besonderer Begabung auch des Bauhauses vergeben. Einige der arbeiteten nach ihrer Berufsausbildung als Meister am Bauhaus weiter.

Die "Werklehre" fand in den Werkstätten statt. Hier wurden die Schüler mit den grundlegenden Materialeigenschaften und wichtigen Prinzipien der Produktgestaltung vertraut gemacht.

Die Erziehung zum Künstler sollte nicht mehr in Klassen von Professoren (wie an Akademien) erfolgen, sondern im handwerklichen Umgang mit den Objekten.
Die leitenden Lehrer in den Werkstätten wurden nicht „Professoren“ genannt, sondern „Formmeister“. Ihnen stand jeweils ein Werkmeister unterstützend zur Verfügung, der die Grundlagen des Handwerks beherrschte.






</doc>
<doc id="10245" url="https://de.wikipedia.org/wiki?curid=10245" title="Wörlitz">
Wörlitz

Wörlitz ist ein Ortsteil der Stadt Oranienbaum-Wörlitz im Landkreis Wittenberg in Sachsen-Anhalt. Er ist durch den zum Weltkulturerbe gehörenden Wörlitzer Park bekannt und ein staatlich anerkannter Erholungsort.

Wörlitz liegt südlich der Elbe im Biosphärenreservat Mittelelbe zwischen Dessau-Roßlau und Wittenberg. Das Gebiet um Wörlitz ist flach und wird von zahlreichen Altwassern der Elbe sowie Gräben und Bächen durchzogen. Reste von Sanddünen („Wilde Berge“) aus der letzten Eiszeit sind die einzigen, nur wenige Meter hohen Erhebungen. Größtes Altwasser ist der Wörlitzer See unmittelbar nördlich der Stadt.


Wörlitz liegt an der Oranier-Route, einer deutsch-niederländischen Ferienstraße.

Am 13. November 1004 wurde Wörlitz erstmals urkundlich erwähnt. Schon 1440 wurde der Ort als Stadt bezeichnet.

Wörlitz ist mit dem Wörlitzer Park der Höhepunkt des Dessau-Wörlitzer Gartenreiches, das in der zweiten Hälfte des 18. Jahrhunderts unter der Regentschaft von Fürst Leopold III. Friedrich Franz von Anhalt-Dessau (1740–1817) geschaffen wurde. Er holte bedeutende Persönlichkeiten wie Erdmannsdorff, Rust und Basedow an seinen Hof. Kontakte und Verbindungen gab es bis nach England, Frankreich (Rousseau) und Italien. Historische Gäste in dieser Zeit in Wörlitz waren Alexander von Humboldt, Lavater, Georg Forster, Goethe, Jean Paul sowie Hölderlin. Gleichzeitig mit dem Park entstand Schloss Wörlitz. 

Im Jahr 2000 wurde die Anlage von der UNESCO in das Verzeichnis des Weltkulturerbes aufgenommen.

"Übergeordnete Verwaltungseinheiten:"

Am 1. Juli 2007 wurde die Gemeinde Wörlitz aufgrund einer Kreisgebietsreform vom ehemaligen Landkreis Anhalt-Zerbst in den Landkreis Wittenberg eingegliedert. Bei einer Bürgerbefragung am 14. Mai 2006 hatten die Einwohner der Stadt Wörlitz mit etwa 59 % der Stimmen für eine Eingemeindung nach Dessau gestimmt. Endgültig entschied aber der Stadtrat von Wörlitz, wobei das Ergebnis des Bürgerbefragung nicht bindend war. Außerdem hätten die Stadt Dessau, der Landkreis Wittenberg sowie das Land Sachsen-Anhalt zustimmen müssen. 

Seit dem 1. Januar 2011 ist Wörlitz Teil der neu gebildeten Stadt Oranienbaum-Wörlitz.

Blasonierung: „In Gold neben einem links aus natürlichem Rasen hervorwachsender grüner Eichenbaum ein springender schwarzer Eber.“

Das Wappen wurde am 10. März 1994 durch das Regierungspräsidium Dessau genehmigt und im Landeshauptarchiv Magdeburg unter der Wappenrollennummer 9/1994 registriert. Die Farben der Ortsfahne sind Grün-Weiß.


Die Kulturdenkmale des Ortes sind im örtlichen Denkmalverzeichnis eingetragen, darunter auch die Domäne Wörlitz und der Christliche Friedhof.








</doc>
<doc id="10254" url="https://de.wikipedia.org/wiki?curid=10254" title="Drachenviereck">
Drachenviereck

Ein Drachenviereck (oder Deltoid) ist ein ebenes Viereck,
oder (äquivalent)

Oft wird nur die konvexe Form des Deltoids als Drachenviereck bezeichnet und die nicht-konvexe Form als Pfeilviereck. (Die Bezeichnung „Drachenviereck“ verweist auf die Form vieler Flugdrachen.) 

Ein spezielles Drachenviereck ist der Rhombus (auch die Raute): Es ist ein "gleichseitiges Deltoid". 

Eine Verallgemeinerung des Drachenvierecks ist der schiefe (schräge) Drachen, bei dem nur verlangt wird, dass eine Diagonale durch die andere halbiert wird. Das Deltoid ist dann ein gerader Drachen.

Für jedes Deltoid gilt mit den Bezeichnungen aus der Grafik:
Für jedes konvexe Deltoid gilt:

Die Diagonale formula_3 ist Symmetrieachse und halbiert die Diagonale formula_4.
Sie teilt das Viereck formula_13 in zwei kongruente spiegelsymmetrische Dreiecke 
(formula_14 und formula_15).
Die Diagonale formula_4 teilt das Viereck in zwei gleichschenklige Dreiecke (formula_17 und formula_18).
Die Innenwinkel bei formula_5 und bei formula_6 sind gleich groß.
Die Winkel bei formula_7 und bei formula_8 werden von der Diagonale halbiert.

Der Flächeninhalt formula_7 eines Drachenvierecks lässt sich leicht aus den Längen der Diagonalen formula_24 und formula_25 bestimmen:

Der Umfang berechnet sich zu:

Der Inkreisradius:


</doc>
<doc id="10255" url="https://de.wikipedia.org/wiki?curid=10255" title="PPP">
PPP

PPP steht für:

"Chemie"

"Informatik"

"Medizin"

"Politik"

"Wirtschaft"

"Sonstiges"

ppp:

.ppp

Siehe auch:


</doc>
<doc id="10256" url="https://de.wikipedia.org/wiki?curid=10256" title="Sauce">
Sauce

Sauce oder Soße (von , auch "Tunke", ‚Brühe‘; aus ‚gesalzene Brühe‘) ist eine flüssig bis sämig gebundene, würzende Beigabe zu warmen und kalten Speisen, Salaten und Desserts.

Bereits im Altertum gab es Würzsaucen wie Garum und dessen Vorläufer, die aufwendig zubereitet und für viele Gerichte verwendet wurden. Auch die Rezeptsammlung Maestro Martinos aus dem 15. Jahrhundert, eine wichtige Quelle für die Esskultur im Mittelalter, beschreibt vor allem Saucen. Die heutige Systematik der Saucen und ihre Verwendung in der Haute Cuisine geht auf die französische Küche zurück. In der modernen Küche ergänzen Saucen Speisen wie Fisch, Fleisch oder Teigwaren, um den Geschmack abzurunden oder zu kontrastieren und den Genuss zu steigern. Bei Ragouts und manchen Salaten sind sie das verbindende Element verschiedener fester Zutaten. In vielen Fällen ist das jeweilige Gericht auch schöner anzusehen. Des Weiteren wirken sie mitunter appetitanregend und verdauungsfördernd.

Saucen basieren grundsätzlich auf aromatischen Flüssigkeiten wie Fonds, Weinen, Ölen oder Milchprodukten, die durch verschiedene Verfahren gebunden bzw. angedickt werden (siehe auch Abbinden). Hierfür benutzt man typischerweise Mehl (Mehlschwitze), Stärke, Ei oder kalte Butter. Durch die veränderte Konsistenz werden die anderen Speisebestandteile verbunden und Inhaltsstoffe direkt mit ihnen kombiniert.

Da die Sauce durch die Zusammenstellung der enthaltenen Zutaten und Konzentration der Aromen den Charakter und Geschmack eines Gerichts maßgeblich mitbestimmt, wird in der Küchenbrigade die Position des Sauciers meist mit dem erfahrensten Koch besetzt.

Der Duden empfiehlt die Schreibweise "Soße", während in der deutschsprachigen Fachliteratur die Verwendung von "Sauce" überwiegt.

Grundsaucen dienen als Basis für zahlreiche Ableitungen, die durch die Zugabe von Würzmitteln, geschmacksgebenden Zutaten und anderen Lebensmitteln entstehen. Sie werden nach Färbung, Rohstoff und Herstellungsmethode unterschieden. Das Konzept der Grundsaucen entstand in der klassischen Küche Frankreichs und gehört bis heute zum Standard in der internationalen Gastronomie. Hauptzutaten von Grundsaucen sind je nachdem Knochen und Parüren, Brühen oder Fonds, Wurzelgemüse, Tomaten, Tomatenmark, Mehl, Butter, Milch und Sahne.

Grundsätzlich unterscheidet man warme und kalte Grundsaucen, wobei zu letzterer nur die Mayonnaise gezählt wird. Warme Grundsaucen wiederum unterteilt man in Weiße und Braune Grundsaucen sowie die aufgeschlagene (warme) Grundsauce.

Als Weiße Grundsauce (') bezeichnet man helle Grundsaucen, die mit weißer Mehlschwitze (') gebunden werden. Unterschieden wird nach der verwendeten namens- und geschmackgebenen Brühe in: Kalbsvelouté, Fischvelouté, Gemüsevelouté und Geflügelvelouté. Aus dieser bereitet man durch die Zugabe von Rahm und anderen den namensgebenden Zutaten eine Sauce, die als Grundsauce angesehen wird. Für die Deutsche Sauce wird eine Kalbsvelouté mit einer Legierung aus Champignonfond und Eigelb gebunden. Durch die Verwendung von Milch statt Fond bzw. Brühe entsteht die Béchamelsauce, die beim Einsatz von Rahm auch entsprechend Rahmsauce genannt wird.

Als Braune Grundsauce (teilweise ' genannt) bezeichnet man dunkle Grundsaucen, die mit Braunem Fond zubereitet werden. Viele Begriffe werden je nach Epoche und Tradition synonym oder mit anderer Bedeutung verwendet. Teilweise verwendet man den Begriff Braune Kraftsauce, insbesondere wenn dafür ein Großer Brauner Fond (') verwendet wird. Früher wurde damit die Spanische Sauce ("") bezeichnet. Alternativ verwendet man auch andere gebundene dunkle Fonds.

Von den warm aufgeschlagenen Saucen wird heute nur die "Sauce hollandaise" (auch Holländische Sauce) als Grundsauce betrachtet, während die eigentlich selbständige "Sauce béarnaise" (Béarner Sauce) als Ableitung behandelt wird.

Auch die Tomatensauce wird oft zu den Grundsaucen gezählt, während andere Systematiken sie als selbständige Sauce ansehen.

Bei den sogenannten selbständigen Saucen sind nur geringe Abwandlungen möglich. Dazu gehören: Tomatensauce, Paprikasauce und warme englische Saucen.

Kalte Saucen werden als Bestandteil von Vorspeisen, Salaten, Zwischengerichten und Desserts betrachtet. Besondere Bedeutung haben sie bei der individuellen Gestaltung von Buffets, wo sie das Angebot durch Geschmack- und Farbkombinationen bereichern. Viele klassische Saucen werden von der Lebensmittelindustrie als verzehrfertiges Convenience-Produkt angeboten, einige sind sogar nur so verfügbar (etwa Würzsaucen wie die Worcestershiresauce). Als "Dips" bezeichnet man angedickte kalte Saucen, Mayonnaisemischungen und Käsecremes. Diese dienen zum Eintunken (engl. "to dip") kleiner Imbisse und Vorspeisen.
Dessertsaucen werden zu Süßspeisen, wie Pudding, Flammeri, Mehlspeisen, süßen Aufläufen, pochierten Früchten, roter Grütze oder Eis serviert. Die Grundlage bilden oft frische oder gefrorene Früchte, Konfitüren, Fruchtsäfte und Schokolade. Häufig finden außerdem Sahne und Milch, Eier und Läuterzucker Verwendung. Liköre und Spirituosen dienen der Aromatisierung. Dessertsaucen können kalt oder warm serviert werden.





</doc>
<doc id="10260" url="https://de.wikipedia.org/wiki?curid=10260" title="Wolfgang Thierse">
Wolfgang Thierse

Wolfgang Thierse (* 22. Oktober 1943 in Breslau) ist ein deutscher Politiker (SPD). Von 1998 bis 2005 war er Präsident des Deutschen Bundestages und von 2005 bis 2013 dessen Vizepräsident.

Wolfgang Thierse wurde am 22. Oktober 1943 als Sohn eines Rechtsanwaltes und Mitglieds der katholischen Zentrumspartei (später CDU-Kreistagsabgeordneter) in Breslau geboren. Nach der Vertreibung aus Breslau siedelte sich die Familie im thüringischen Eisfeld an, dort besuchte Thierse die Oberschule. Er war auch Mitglied der FDJ. Nach dem Abitur an der Erweiterten Oberschule im südthüringischen Hildburghausen erlernte er den Beruf des Schriftsetzers beim "Thüringer Tageblatt" in Weimar.

Thierse begann 1964 ein Studium der Germanistik und der Kulturwissenschaft an der Humboldt-Universität zu Berlin, das er 1969 mit dem Diplom beendete. Anschließend war er an der Sektion Kulturwissenschaften/Ästhetik der Humboldt-Universität wissenschaftlicher Assistent von Wolfgang Heise. Das Ministerium für Kultur der DDR, wo er ab 1975 in der Abteilung Bildende Kunst tätig war, entließ ihn, nachdem er sich geweigert hatte, eine Erklärung zu unterzeichnen, mit der er die Ausbürgerung von Wolf Biermann befürworten sollte.

1977 ging er als wissenschaftlicher Mitarbeiter an das Zentralinstitut für Literaturgeschichte der Akademie der Wissenschaften der DDR. Dort arbeitete er u. a. als Mitverfasser des "Historischen Wörterbuches ästhetischer Grundbegriffe". Zwischen 1970 und 1987 wirkte Thierse an den Drehbüchern für sieben DEFA-Dokumentarfilme mit und verfasste die Kommentartexte. Im Jahr 2003 erschien ein von ihm vorgetragenes Hörbuch mit einer der bekanntesten Erzählungen von Charles Dickens, "Eine Weihnachtsgeschichte".

Thierse war bis zur Wende und friedlichen Revolution parteilos und trat im Oktober 1989 dem Neuen Forum, einer der Bürgerbewegungen der Liste Bündnis 90, bei. Anfang Januar 1990 wurde er dann Mitglied der Sozialdemokratischen Partei der DDR (SDP). Nach dem Rücktritt von Ibrahim Böhme wurde Thierse am 9. Juni 1990 auf einem Sonderparteitag zum Vorsitzenden der SPD der DDR gewählt.

Auf dem Vereinigungsparteitag der SPD wurde er am 27. September 1990 zu einem der stellvertretenden Parteivorsitzenden gewählt. Aus diesem Amt schied er im November 2005 aus, gehörte aber weiterhin dem Parteivorstand an. Auf dem Bundesparteitag der SPD in Dresden im November 2009 kandidierte er auch nicht mehr für den Parteivorstand.

Thierse ist Sprecher des Arbeitskreises „Christen in der SPD“. Er war von 1991 bis 2013 Vorsitzender der Grundwertekommission der SPD und ist dort aktuell beratendes Mitglied.

Von März bis Oktober 1990 gehörte Thierse der ersten frei gewählten Volkskammer der DDR an. Hier war er zunächst stellvertretender Vorsitzender und ab dem 21. August 1990 Vorsitzender der SPD-Volkskammerfraktion.

Thierse zählte zu den 144 von der Volkskammer gewählten Abgeordneten, die am 3. Oktober 1990 Mitglied des Deutschen Bundestages wurden. Am 4. Oktober 1990 wurde er zum stellvertretenden Vorsitzenden der SPD-Bundestagsfraktion gewählt. Von Dezember 1990 bis Dezember 1991 war er außerdem Vorsitzender des Fraktionsarbeitskreises "Neue Länder/Deutschlandpolitik".

Am 26. Oktober 1998 wurde Thierse mit 512:109:45 Stimmen zum Präsidenten des Deutschen Bundestages gewählt und am 17. Oktober 2002 mit 357:219:20 Stimmen im Amt bestätigt. Thierse erhielt nur 59,9 Prozent der Stimmen, weil ihm aus dem Lager der Unionsparteien vorgeworfen wurde, sein Amt in der letzten Legislaturperiode zu parteiisch geführt zu haben.

In seine erste Amtszeit fiel auch die CDU-Spendenaffäre, die ihn als Parlamentspräsidenten insoweit betraf, als er "qua Amt" für die Überwachung der Einhaltung des Parteiengesetzes und die Ahndung eventueller Verstöße verantwortlich war. Er verhängte eine Strafe in Höhe von 7,8 Millionen D-Mark gegen die CDU und ließ die staatlichen Zuschüsse an die CDU um insgesamt 41 Millionen D-Mark kürzen. Über diese Geschehnisse sagte er selbst, es wäre ihm lieber gewesen, er hätte sich nicht mit ihnen beschäftigen müssen. Von Seiten der Union wurde Thierse wegen der verhängten Strafzahlung mehrfach angegriffen und seine Überparteilichkeit in Frage gestellt – dies unter anderem auch deshalb, weil Thierse, als Bundestagspräsident zu strenger Überparteilichkeit verpflichtet, weiterhin als Stellvertretender SPD-Parteivorsitzender amtierte. Das Bundesverfassungsgericht allerdings bestätigte die Rechtmäßigkeit dieses im Parteiengesetz ausdrücklich vorgesehenen Vorgehens.

Da die CDU/CSU-Bundestagsfraktion als stärkste Fraktion aus der Bundestagswahl 2005 hervorging, war seine Amtszeit am 18. Oktober 2005 beendet. Er wurde mit 417:136:52 Stimmen zum Vizepräsidenten des Bundestags gewählt. Am 27. Oktober 2009 wurde er dies erneut mit 371:170:65 Stimmen, dem schlechtesten Ergebnis aller fünf gewählten Stellvertreter.

Im Frühjahr 2005 besuchte eine von dem damaligen Bundestagspräsidenten Thierse angeführte Delegation das Mausoleum des verstorbenen chinesischen Diktators Mao Zedong.

Wolfgang Thierse zog 1994, 1998 und 2009 über die Landesliste Berlin, in den anderen Fällen als direkt gewählter Abgeordneter des Wahlkreises Berlin-Mitte – Prenzlauer Berg – Weißensee I (1990) bzw. Berlin-Pankow (seit 2002) in den Bundestag ein. Bei der Bundestagswahl 2005 erreichte er 41,1 % der Erststimmen. 2009 verlor er sein Direktmandat an Stefan Liebich von der Partei Die Linke. Am 28. August 2012 kündigte Thierse an, bei der Bundestagswahl 2013 nicht mehr zu kandidieren.

Thierse ist langjähriges persönlich hinzugewähltes Mitglied des Zentralkomitees der deutschen Katholiken (ZdK), Ehrenpräsident der Europäischen Bewegung Deutschland, deren Präsident er von 1998 bis 2000 war., Schirmherr der Georg-Elser-Initiative Berlin und Mitglied des Kuratoriums von Aktion Deutschland Hilft e. V., dem Bündnis der Hilfsorganisationen und Mitglied des Beirats des Cusanuswerks. Seit Oktober 2000 ist Thierse Kuratoriumsvorsitzender der Bundeskanzler-Willy-Brandt-Stiftung, die 1994 durch den Deutschen Bundestag errichtet wurde. Ferner ist er Vorsitzender des Vorstands des Kulturforums der Sozialdemokratie mit den Themenschwerpunkten Künstlerische und kulturpolitische Grundsatzfragen, Kulturpolitische Programmatik, Erinnerungskultur, Berlin-Kultur sowie Werte und Religion. Er ist Mitglied im Ehrenrat von AMCHA Deutschland, der zentralen Organisation für die psychosoziale Hilfe von Überlebenden des Holocaust und ihren Nachkommen in Israel. Bis 2013 übernahm Thierse die Schirmherrschaft des Behandlungszentrums für Folteropfer.

Thierse ist verheiratet, hat zwei Kinder und ist katholisch. Er wohnt im Berliner Ortsteil Prenzlauer Berg.

Wolfgang Thierse engagiert sich intensiv gegen Rechtsextremismus, u. a. als Schirmherr der Amadeu Antonio Stiftung. So nimmt er regelmäßig an einschlägigen Demonstrationen teil. Er geriet in die Kritik, da er auch bei Blockadeaktionen gegen nicht verbotene und ordentlich angemeldete rechtsextreme Demonstrationen mitmacht, zum Beispiel am 1. Mai 2010 in Berlin. Ein gegen ihn in diesem Zusammenhang eröffnetes Strafverfahren, u. a. wegen Nötigung, wurde aber eingestellt.

Am 19. Februar 2011, nach einer Aktion gegen die an diesem Tag geplanten rechtsextremen Demonstrationen in Dresden, erklärte Wolfgang Thierse in einem Interview mit dem MDR: „Die Polizei ist eben vollauf damit beschäftigt, die Neonazis zu schützen. […] Das ist sächsische Demokratie.“ Weil er damit nach Ansicht der Gewerkschaft der Polizei und von Politikern der sächsischen Regierungskoalition Prinzipien der Gewaltenteilung und Grundrechte, wie Versammlungs- und Demonstrationsfreiheit in Frage stelle sowie im Umgang mit politischen Gegnern Faustrecht billige, wurden diese Äußerungen kritisiert. Holger Zastrow, Fraktionsvorsitzender der sächsischen FDP, nannte ihn als „Bundestagsvizepräsident untragbar“. Andreas Arnold, ein ranghoher Polizeibeamter, zeigte Thierse wegen Beleidigung an. Anfang März 2011 wurde das daraufhin gegen ihn eingeleitete Ermittlungsverfahren eingestellt.

In zwei Interviews mit der "Berliner Morgenpost" und dem "Spiegel" zum Jahreswechsel 2012/2013 monierte Thierse, dass die Kultur in Berlin-Prenzlauer Berg unter dem Zuzug von Schwaben und anderen wohlhabenderen Deutschen leide, insofern sich diese Zugezogenen zu wenig anpassten. Nach 3000 zum Teil von Hass erfüllten Protestmails hieß Thierse die Schwaben in Berlin willkommen.

Wolfgang Thierse wurde 1993 mit dem Bundesverdienstkreuz 1. Klasse und 1999 mit dem Großkreuz des Bundesverdienstkreuzes auszeichnet.

2001 erhielt er von der Theodor-Heuss-Stiftung den Theodor-Heuss-Preis sowie den Ignatz-Bubis-Preis für Verständigung der Stadt Frankfurt am Main.

Am 26. Februar 2004 erhielt Thierse die Ehrendoktorwürde der Philosophischen Fakultät der Westfälischen Wilhelms-Universität Münster. Gewürdigt werden mit dieser Auszeichnung "seine besonderen Verdienste um die Verständigung zwischen Ost- und Westdeutschland, die Stärkung des demokratischen Bewusstseins in den neuen Bundesländern und die Zurückweisung radikaler Strömungen in der Gesellschaft."

Am 8. November 2005 erhielt er den Preis der Deutschen Gesellschaft e. V. "für Verdienste um die deutsche und europäische Verständigung."

Thierse wurde vom Deutschen Kulturrat, dem Spitzenverband der Bundeskulturverbände, der Kulturgroschen 2016 zuerkannt, der ihm am 15. März 2016 in Berlin übergeben wurde. Der Präsident des Deutschen Kulturrates Christian Höppner in seiner Begründung: "Wolfgang Thierse hat sich als Kulturpolitiker intensiv mit der deutschen Geschichte und der Rolle von Kunst sowie von Künstlern in der Gesellschaft auseinandersetzt. Er hat sich als Mitglied des Deutschen Bundestags stets für die Kultur stark gemacht und war dabei eine moralische Instanz - insbesondere in der Form des adäquaten Erinnerns an die deutsche Geschichte. Er ist einer der profiliertesten Verteidiger der kulturellen Institutionen und der Kulturlandschaft vor Abbau in Deutschland. Für diese außergewöhnliche kulturpolitische Lebensleistung erhielt er den Kulturgroschen, die höchste Auszeichnung des Deutschen Kulturrates."






</doc>
<doc id="10262" url="https://de.wikipedia.org/wiki?curid=10262" title="Genesis">
Genesis

Genesis ([], altgriechisch "Schöpfung", "Entstehung", "Geburt") bezeichnet:

Technik:

Kultur:

Fahrzeuge:
Siehe auch:


</doc>
<doc id="10263" url="https://de.wikipedia.org/wiki?curid=10263" title="Bereschit">
Bereschit

Bereschit (hebr. "Im Anfang") ist Bezeichnung für:



</doc>
<doc id="10265" url="https://de.wikipedia.org/wiki?curid=10265" title="Schemot">
Schemot

Schemot (hebr. "Namen") ist die Bezeichnung für:



</doc>

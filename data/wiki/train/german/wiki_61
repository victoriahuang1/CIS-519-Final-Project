<doc id="8884" url="https://de.wikipedia.org/wiki?curid=8884" title="Masturbation">
Masturbation

Unter Masturbation wird eine – überwiegend manuelle – Stimulation der eigenen Geschlechtsorgane verstanden, die in der Regel zum Orgasmus führt. Dabei können auch verschiedene Hilfsmittel zum Einsatz kommen.

Im häufigsten Fall handelt es sich bei der Masturbation um eine geschlechtliche "Selbstbefriedigung", also eine Form der "Autosexualität". Eine Masturbation kann jedoch auch an anderen und durch andere Personen ausgeführt werden. Neben der häufigsten Form der Masturbation durch die Benutzung der Hand als Stimulationswerkzeug gibt es auch verschiedene Sexspielzeuge und Masturbationshilfen, die zur Unterstützung der Masturbation eingesetzt werden können.

Die Masturbation stellt gemeinsam mit dem Geschlechtsverkehr die häufigste Form sexueller Aktivität dar. Auch als Sexpraktik gemeinsam mit dem Partner ist sie beliebt, da bei vielen Menschen durch die Beobachtung des masturbierenden Partners die sexuelle Erregung gesteigert wird. Sie ist oft Teil des Pettings und stellt eine Möglichkeit des „Safer Sex“ dar.

Die Wortherkunft ist nicht sicher geklärt. 
Die zuverlässigsten etymologischen Quellen leiten "masturbieren" mit einiger Wahrscheinlichkeit vom lat. "masturbari" (inf. Passiv mit medialer Bedeutung) ab, einer Zusammenziehung von "manibus turbari" (sich mit den Händen reizen), nach anderen von "manu turbari" (Ablativ im Singular statt Plural). So zum Beispiel: 
Analoges gilt im Italienischen: 
Andere etymologische Deutungsversuche bewegen sich eher im Bereich der Volksetymologie: 
Plausibel scheint manchen Linguisten die lateinische Herkunft aus der Vorsilbe „mas-“ („männlich“) mit dem Stammwort „turbare“ („stören, heftig bewegen“); eine moralisierende, aber unetymologische Deutung ergibt sich mit der aus dem Mittellatein schwach belegbaren Ableitung von lateinisch "manustupratio" (von "manus" „Hand“ und "stuprum" „Unzucht“, siehe oben). In dieser Form wird der Begriff häufig als „Befleckung, Unzucht mit der Hand“ übersetzt. 

Weitere Bezeichnungen vor allem für die Masturbation als "Selbst"-Befriedigung sind:

Masturbation ist nicht mit gesundheitlichen Schäden verbunden. In der Vergangenheit wurde Masturbation besonders gegenüber Kindern und Heranwachsenden moralisch geächtet, verdammt und mit Angst auslösenden Behauptungen, beispielsweise dass man durch Masturbation krank werde, tabuisiert. Eine „Masturbationsfurcht“ gab es in der ersten Hälfte des 19. Jahrhunderts jedoch vor allem im bürgerlichen Umfeld, aber nur selten in unteren Bevölkerungsschichten und adeligen Kreisen. Die Medizin, insbesondere des 19. Jahrhunderts, unterstützte diese Vorstellungen mit der Deutung verschiedener körperlicher Befindlichkeitsstörungen als Folge von zu häufiger Masturbation. Heute ist es selbstverständlich geworden, Masturbation anzuwenden.

Vereinzelt wird sie aber als störend oder sogar krankhaft gewertet, wenn sie öffentlich oder zwanghaft ausgeübt und zur Sucht wird. Vom psychologischen Standpunkt aus ist Suchtverhalten in jedem Lebensbereich mit Risiken und möglichen Gefährdungen der eigenen Person oder anderer verbunden, daher auch im Hinblick auf die Selbstbefriedigung.

Einige Studien an Männern haben Belege gefunden, dass regelmäßige Ejakulationen Prostatabeschwerden vorbeugen können. Dieser Zusammenhang wird in anderen Studien jedoch nicht bestätigt.

Selten leiden Männer nach jedem Samenerguss, egal ob durch Masturbation oder Geschlechtsverkehr hervorgerufen, unter Symptomen des Postorgasmic Illness Syndroms. Betroffene versuchen entsprechend, Ejakulationen zu vermeiden, um den Symptomen auszuweichen.

Der Anteil masturbierender Männer beträgt ca. 94 %, der von Frauen ca. 80 %. Viele entdecken die Masturbation bereits in der frühen Pubertät, einige erst später und manche schon als Kleinkind. Männer masturbieren gemäß mehreren Studien mit ungefähr zwölf Jahren zum ersten Mal und ab dann zumeist mindestens zweimal wöchentlich; Frauen masturbieren meist erst ab den späten Teenagerjahren regelmäßig.

Masturbiert wird auf ganz unterschiedliche Weise. Mit den Händen und Fingern oder mit Hilfsmitteln werden die eigenen erogenen Zonen des Körpers stimuliert. Dabei können Gleitmittel die Reizung verbessern.
Seit der Erfindung des Vibrators gegen Ende des 19. Jahrhunderts entwickelte sich ein großer Markt für Masturbationshilfmittel, wie Vibratoren und Dildos, Masturbatoren und Sex-Puppen.
Durch optische und akustische Reize, wie das Betrachten erotischer oder pornographischer Fotos oder Filme, können die bei der Selbstbefriedigung empfundenen Lustgefühle gesteigert werden.

Männer masturbieren üblicherweise durch Stimulation des Penis durch Bewegung der Vorhaut über den Penis und insbesondere die Eichel oder durch Streichen der Hand über das Glied. Wesentlich seltener ist die Selbstbefriedigung durch das Reiben des Penis gegen einen weichen Gegenstand, etwa ein Bett oder Kissen. Auch anale und urethrale Selbstbefriedigung sind selten.
Gelenkigen Männern ist auch die orale Selbstbefriedigung – Autofellatio genannt – möglich. Der Kinsey-Report nennt einen Anteil von 0,2 bis 0,3 % der männlichen Bevölkerung, der dazu in der Lage ist. Hingegen ist die Zahl der Männer, die dies wenigstens ausprobiert haben, wesentlich höher; auch ist dies ein nicht seltener Bestandteil masturbatorischer Traum-Phantasien. Kinsey vermutete hier einen animalischen Hintergrund, denn bei Primaten ist Autofellatio eine normale Form der sexuellen Betätigung.

Die Masturbation beim Mann endet fast immer mit dem Samenerguss. Die meisten Männer erreichen ihn dabei nach weniger als zwei Minuten – teils sogar nach zehn bis zwanzig Sekunden –, einige zögern den Samenerguss zur Steigerung der Lust deutlich, in Extremfällen bis zu über eine Stunde, hinaus.

Üblicherweise erfolgt bei Männern die Masturbation unter starker Beteiligung der Phantasie.

Die gebräuchlichste Form der Masturbation bei Frauen ist die Stimulation von Klitoris und Vulva mit der Hand, wobei die meisten Frauen auf dem Rücken liegen. Häufig wird zur Stimulation der Klitoris auch ein Vibrator verwendet. Nur relativ wenige Frauen führen beim Masturbieren ihre Finger oder Gegenstände wie Dildos in die Vagina ein, die Angaben dazu liegen in verschiedenen Studien bei unter 20 Prozent. Laut Hite praktizieren etwa zwölf Prozent der Frauen zumindest manchmal vaginale Masturbation in Kombination mit klitorialer Stimulation und etwa zwei Prozent manchmal auch rein vaginale Masturbation.

Eine schwedische Studie aus dem Jahr 2006 besagt, dass 69 Prozent der schwedischen Frauen sich bevorzugt klitorial befriedigen, 28 Prozent bevorzugen eine Kombination aus klitorialer und vaginaler Masturbation und weniger als drei Prozent ausschließlich vaginale Masturbation. Weitere weniger verbreitete Formen der Masturbation bei Frauen sind das Reiben der Vulva an weichen Gegenständen, das rhythmische Zusammenpressen der Oberschenkel und die Stimulation von Klitoris und Vulva mit Hilfe von Wasser. Anale Stimulation wird von manchen Frauen zusätzlich zur klitorialen oder vaginalen Masturbation ausgeübt. Die orale Selbstbefriedigung – genannt Autocunnilingus – ist bei Frauen mit extrem wenigen Ausnahmen unmöglich.

Während erotische Phantasien bei Männern sehr verbreitet sind, haben laut Kinsey nur die Hälfte der Frauen bei der Masturbation regelmäßig sexuelle Phantasien, für mehr als ein Drittel der Befragten ist Masturbation rein körperlich, laut Hite genießen sogar die meisten Frauen die Masturbation meist zwar physisch, nicht jedoch psychisch. Die Phantasien der Frauen sind stärker als bei Männern auf die bereits erlebten Arten sexueller Handlungen beschränkt. Im Gegensatz zum Geschlechtsverkehr erreichen bei der Masturbation sehr viele Frauen einen Orgasmus. In Kinseys Studien gaben 45 Prozent der Frauen an, durch Masturbation üblicherweise innerhalb von drei Minuten einen Orgasmus zu erreichen, weitere 25 Prozent in vier bis fünf Minuten, wobei viele der Frauen ihn mit Absicht hinauszögern. Viele Frauen geben darüber hinaus an, Masturbation bis zum Orgasmus zu nutzen, um die ablenkende sexuelle Erregung möglichst schnell abzubauen. Da Masturbation die einfachste und zuverlässigste Methode darstellt, um einen weiblichen Orgasmus herbeizuführen, ist sie für die Erforschung weiblicher Orgasmen von großer Bedeutung.

Die verbreitete Ansicht, die Sexualität würde erst mit der Pubertät entstehen, ist falsch. Laut Kinsey haben etwa 19 % der Mädchen bereits vor der Pubertät masturbiert, teils schon im ersten Lebensjahr. Sie erreichen dabei häufiger als Jungen entsprechenden Alters einen Orgasmus. Ältere Frauen masturbieren häufiger als junge, am meisten ist die Masturbation bei Frauen Mitte 40 verbreitet. Erst bei den über 50-Jährigen ist eine Abnahme der Verbreitung festzustellen. Die mittlere Häufigkeit der Masturbation ist über alle Altersstufen von 20 bis 55 Jahren fast konstant. Kinsey gibt den Median für masturbierende Singlefrauen mit einmal alle zwei bis drei Wochen an. Die Streuung ist dabei – wie bei den meisten Sexualgrößen – bei Frauen wesentlich größer als bei Männern, so dass die durchschnittliche Häufigkeit vermutlich bei etwa einmal pro Woche liegt.

Auch im antiken Griechenland mit seiner moralischen Freizügigkeit war Sexualität durchaus kein tabuloses Thema. Dennoch gab es Vertreter des Kynismus, die das Ziel hatten, zum Naturzustand zurückzukehren, „den sie in tierischen und kindlichen Verhaltensweisen gegenüber der Kultur, dem Anerzogenen (Paidéia) sahen.“ Einer ihrer Vertreter, Diogenes von Sinope, galt als Meister der Provokation und sagte, als er sich öffentlich auf dem Marktplatz befriedigte: „Könnte man doch auch den Bauch ebenso reiben, um den Hunger loszuwerden“. Viele andere griechische Philosophen standen der Masturbation jedoch kritischer gegenüber als Diogenes.

Ab dem späten Mittelalter wurde sie genauso wie alle anderen Formen der Sexualität, die nicht ausschließlich der Fortpflanzung dienten, von der römisch-katholischen Kirche als Sünde betrachtet und teilweise als widernatürliche Unzucht. In der 1768 eingeführten und bis 1787 gültigen Constitutio Criminalis Theresiana wird sie im selben Paragrafen wie die anderen „Unkeuschheiten wider die Natur“ abgehandelt und es war mindestens eine angemessene Leibesstrafe vorgesehen. Über den späteren französischen König Ludwig XIII. (1601–1643) ist dagegen bekannt, dass dessen Leibarzt schrieb, Kindermädchen sollten zur „abendlichen Beruhigung“ Jungen im „Kitzeln des Penis“ unterweisen.

In der Aufklärung erfuhr sie eine Brandmarkung als „soziale Gefahr“ und „unnatürliches Verhalten“ jenseits der rein religiösen Verurteilung.

Im Jahr 1712 erschien in England das vermutlich von dem geschäftstüchtigen Quacksalber und Schriftsteller John Marten geschriebene und anonym veröffentlichte Pamphlet "Onania: or, the Heinous Sin of Self-Pollution" („Onanie oder die abscheuliche Sünde der Selbstbeschmutzung“), das nach und nach in alle europäischen Sprachen übersetzt wurde und große Verbreitung erfuhr. Darin wurde behauptet, dass exzessive Masturbation vielfältige Krankheiten wie Pocken und Tuberkulose verursachen könne. Bezeichnend ist, dass John Marten gleichzeitig zahlreiche kleinere softpornografische Schriften veröffentlichte und in "Onania" eine von ihm erfundene „Medizin“ gegen die angeblich aus der Masturbation resultierenden Krankheiten anbot. Selbst die großen Aufklärer der Zeit glaubten dem anonym veröffentlichten Werk. Denis Diderot nahm die fragwürdigen Thesen unter dem Artikeltitel "Manstupration ou Manustupration" sogar in seine "Encyclopédie" auf.

Im 18. und 19. Jahrhundert fand in der Folge in ganz Europa geradezu ein „Feldzug gegen die Masturbation“ statt. Es erschienen unzählige wissenschaftliche und populärwissenschaftliche Veröffentlichungen, die die angeblichen Gefahren der Masturbation anprangerten und Methoden zu ihrer Verhinderung anboten. Als Standardwerk kann die ab 1760 in unzähligen Auflagen verbreitete Schrift "L’Onanisme. Dissertation sur les maladies produits par la masturbation" ("Die Onanie. Abhandlung über Krankheiten durch Masturbation") des Lausanner Arztes Samuel Auguste Tissot gelten.

Erst von jener Zeit an wurde die betreffende Bibelstelle über Onan nicht mehr als Coitus interruptus begriffen.

Falsche Vorstellungen kursierten über Jahrhunderte, dass „Selbstbefleckung“ die gesunde geschlechtliche Entwicklung eines Knaben behindere und zur Gehirnerweichung und zum Rückenmarksschwund führe. Auch Krebs, Lepra und Wahnsinn sowie weitere psychiatrische Krankheitsbilder sollten angeblich die Folge der Masturbation sein. Erst nachdem Robert Koch 1882 den Tuberkelbazillus entdeckte, behaupten die Mediziner nicht mehr, dass Masturbieren Tuberkulose hervorrufe.

Neben gesundheitlichen Gefahren wurden auch moralische Argumente gegen die Masturbation vorgebracht: sie sei egoistisch, verleite zur Disziplinlosigkeit, stelle ein „nutzloses Vergnügen“ dar und wurde mitunter als „sexueller Missbrauch“ bezeichnet. Die Masturbation fördere die Abkapselung des Masturbators von der Gesellschaft, da er zu seiner sexuellen Befriedigung keinen Partner benötigt.
Sigmund Freud befasste sich eingehend mit der Masturbation als Ursache neurotischer Erkrankungen, insbesondere der Neurasthenie als sogenannter Aktualneurose. Kindliche Masturbation sah er je nach Stand seiner Theorieentwicklung als Ausdruck einer vorhergehenden Verführung des Kindes oder im Rahmen der Theorie der infantilen Sexualität als spontanes, entwicklungsbedingtes Geschehen an. Gelegentlich bezeichnete er die Masturbation als die "Ursucht", an deren Stelle später andere, erwachsenentypische Süchte wie das Rauchen etwa träten. Als suchthaftes Verhalten aber spiele sie auch eine "ungeheure Rolle" im Verständnis der (als Psychoneurose beurteilten) Hysterie. Die Frage der Schädlichkeit der Onanie war um 1912 Gegenstand einer Debatte der Wiener Psychoanalytischen Vereinigung; Freud wendete sich resümierend gegen eine grundsätzliche Verharmlosung: In der Neurasthenie als direkte Folge, aber auch durch Verminderung der Potenz, Verweichlichung des Charakters durch Fixierung auf phantasierte Befriedigung statt realer Anstrengung und Stagnation der allgemeinen psychosexuellen Entwicklung disponiere die Selbstbefriedigung zur Neurose.

Bis weit ins 20. Jahrhundert hinein war der Glaube weit verbreitet, dass Akne durch Masturbation hervorgerufen werde. Die Hypothese konnte sich wohl deshalb so lange halten, weil Jugendliche in der Pubertät fast immer unter Akne leiden und gleichzeitig in der Pubertät auch häufig masturbieren (siehe auch Cum hoc ergo propter hoc). Bis in die 1980er Jahre wurde Masturbation auch in medizinischen Kreisen gelegentlich als unreife, im Erwachsenenalter als pathologische Form der Sexualität betrachtet.

Die Pädagogen der Aufklärung griffen im 18. Jahrhundert die medizinischen Argumente auf und verarbeiteten sie methodisch in ihren Lehrgeschichten. Namentlich aus den Reihen der Philanthropen (Villaume, Salzmann u. a.) kamen zahlreiche Monografien, die neben den vermeintlichen körperlichen Schäden auch die seelischen Verwüstungen darstellten, die die „Selbstschändung“ hervorrufe. Die Ursachen sahen die Pädagogen in einer nach ihrer Einschätzung verbreiteten verzärtelnden Erziehung und besonders in mangelhafter Hygiene, in zu weichen Betten, in falscher Ernährung, im Bewegungsmangel und in zu enger und zu warmer Kleidung.

Zu den Fehlern der häuslichen Erziehung kämen die falschen Lehrinhalte in den Schulen. Ein Hauptübel seien die Literatur und die sogenannten „schönen Künste“. Das permanente Schmachten, das ständige Verliebtsein und Sehnen nach dem Glück setze falsche Akzente. Die Literatur des „Sturm und Drang“ wurde besonders geächtet. Aber auch die alten Griechen blieben nicht verschont.

Die größte Gefahr freilich sahen die Philanthropen im sozialen Umgang der Kinder. Die Ammen, die die Kleinen in der Frühzeit betreuten, legten oft das Fundament für eine dauerhafte Verführung. Kinderwärterinnen, Gouvernanten, Bedienstete, Knechte, Mägde, Friseure, Schneider und Tanzlehrer setzten die Fehlleitung der Kinder und Jugendlichen systematisch fort. Nicht ungenannt blieben auch die Lehrer im Haus und in der Schule. Als Mittel der Gegenwirkung empfahlen die Philanthropen indirekte und direkte Maßnahmen. Zu den indirekten zählte die allgemeine Korrektur der Erziehung. Dazu gehörten Selbstzucht und Askese als Leitprinzipien, die Mäßigung im Essen, Trinken und Schlafen. Abhärtung und hygienische Maßnahmen sowie eine allgemeine Erziehung zur Schamhaftigkeit und der Erzeugung von Ekel bei geschlechtlichen Dingen.

Egal nun aber, wie das pädagogische Feld bestellt sei: oberstes Prinzip der Erzieher müsse es sein, den Zögling ständig zu überwachen und zu kontrollieren. „Lasst ihn weder Tag noch Nacht allein; schlaft wenigstens in seinem Zimmer“, hatte Rousseau in seinem "Emile" empfohlen. Zu den direkten Maßnahmen zählten die sogenannten „wahren Geschichten“ aus dem Leben, in denen die Pädagogen die zahllosen leib-seelischen Gebrechen anschaulich an den Lebensläufen unglücklicher Jungen und Mädchen darstellten, die der Masturbation verfallen waren. Langes Siechtum und Tod waren nicht selten der Ausgang der Horrorberichte, die die Jugendlichen wieder auf den Pfad der Tugend führen sollten.

Blieben diese Mittel ohne Wirkung, so empfahlen die Pädagogen das Anlegen von Fesselbändern, Gürteln und Leibchen. Als drastischste Maßnahme in der Pädagogik muss die Infibulation bezeichnet werden. Darunter verstand man einen Draht, der durch die Vorhaut über die Eichel angelegt wurde. Joachim Heinrich Campe, bedeutender Pädagoge und Verleger der deutschen Aufklärung, propagierte diese Methode nachhaltig und konnte nur bedauern, dass die Infibulation „nur bei der einen Hälfte unserer Jugend“ anwendbar sei.

Zudem hat es Überlegungen gegeben, die weiblichen Geschlechtsorgane operativ zu manipulieren. Die Maßnahmen reichen vom Vernähen der Vagina bis zu Klitorisbeschneidungen. In die Pädagogik wurden solche Empfehlungen jedoch nicht aufgenommen. Konsens bestand unter den Erziehern, dass dem „Erkennen des Masturbanten“ große Bedeutung zukomme. Hierfür entwarfen sie einen systematischen Beobachtungsplan, der Kriterien auflistete, die den Sünder überführen sollten.

In "" rekonstruiert der US-Anthropologe Weston La Barre die Ursachen des weltweiten Aberglaubens, männliche Masturbation führe zur Minderung von Nervensubstanz, und entdeckt sie in einer primitiven Fehleinschätzung des Wesens menschlicher Hirnmasse, die nicht als Substrat für Informationsverarbeitung, sondern Kraft"stoff" vorgestellt werde.

Der Philosoph der Aufklärung Immanuel Kant sah Selbstbefriedigung als eine sittliche Verfehlung. Für ihn ist der natürliche Zweck des Sexualtriebs, dem nicht zuwidergehandelt werden dürfe, die Fortpflanzung. In seiner "Metaphysik der Sitten" legt er dar, dass die „wohllüstige Selbstschändung“ (d. h. die Masturbation) eine Verletzung der Pflicht des Menschen gegen sich selbst sei, weil er seine eigene Persönlichkeit aufgebe, indem er sich selbst als reines Mittel zur Befriedigung seiner Triebe gebrauche. Diese Selbstaufgabe erfordere nicht einmal Mut, sondern nur ein Nachgeben gegenüber dem Trieb und wird deshalb von Kant als noch schlimmeres moralisches Vergehen bewertet als der Suizid.

Für das Judentum gelten die Gesetze und Regeln der Thora, der fünf Bücher Mose (Pentateuch), mitsamt ihrer Auslegung durch den Talmud.

Die jüdischen Schriften äußern sich nicht eindeutig zur Masturbation. Grundsätzlich ist anzumerken, dass Gott in der Bibel dem Volk Israel detaillierte Vorschriften zum Sexualverhalten macht ( und ), dabei die Masturbation jedoch nicht ausdrücklich erwähnt. Es gibt rituelle Unreinheit, die den Menschen von der Begegnung mit Gott (z. B. im Gottesdienst) ausschließt. Samenerguss nach gilt im Judentum (wie auch Eiterfluss, krankhafte Blutung oder die weibliche Menstruation) als Verlust von Lebenskeimen bzw. Lebenskraft und verunreinigt so den Körper. Von der Sünde unterscheidet sich diese Unreinheit dadurch, dass Sünde nur durch Opfer beseitigt werden kann, Unreinheit erfordert demgegenüber rituelle Waschungen (Mikwe) und eine Wartezeit (meist bis zum nächsten Abend).

Der Kizzur Schulchan Aruch (ein populäres halachisches Kompendium von 1834, das sich u. a. durch die ausschließliche Behandlung von häufig auftretenden rechtlichen Fragen des täglichen Lebens auszeichnet) sagt:

Formal gelten die jüdischen Schriften (das Alte Testament, d. h. ohne die Auslegung durch den Talmud) für die Christen unverändert fort, denn Jesus (und später Paulus) erklären selbst, das Alte Testament sei nicht durch die Lehre Christi ab- oder aufgelöst, sondern lediglich durch die Gottes- und Nächstenliebe erfüllt worden (). Viele jüdische Lehren haben dadurch einen anderen Stellenwert erhalten. Rituelle Waschungen sind im Christentum beispielsweise weitgehend unbekannt.

Jesus spricht sich in zahlreichen Situationen gegen das starre Befolgen von Gesetzen aus und stärkt im Gegenzug die Gewissensentscheidung des Einzelnen. Die Kirchen ordnen die Masturbation unterschiedlich ein. Es gibt Bibelstellen, die mit der Masturbation in Zusammenhang gebracht werden. Die Interpretation ist umstritten.

Meist wird die Bibelstelle als Lehrmeinung gegen Masturbation zitiert. Allerdings bestraft Gott Onan (daher der Begriff "Onanie") nicht wegen Masturbation, auch nicht wegen des Coitus interruptus, sondern wegen des Nichtvollzugs des im Judentum vorgeschriebenen Levirats (Heirat der Witwe seines verstorbenen Bruders, um ihr Nachkommen zu gewähren, die sie im Alter versorgen und den Namen des Bruders weiterbestehen lassen).

Andere Bibelstellen befassen sich mit übersteigerten Grundbedürfnissen und kritisieren beispielsweise in Sucht und suchtähnliche Gewohnheiten. Hier geht es auch um die Frage, inwiefern die betroffene Person noch über ihre eigenen Gefühle herrscht. Außerdem gibt es teilweise die Auffassung, dass Sexualität generell in die Ehe gehöre.

Nach rabbinischer Auslegung lag das heiratsfähige Alter bei 12 (Mädchen) beziehungsweise 13 (Jungen) Jahren, was nach Fritz Rienecker auf die frühere Pubertät in südlichen Ländern zurückzuführen sei. Deshalb geht die Bibel nicht auf vorehelichen Geschlechtsverkehr ein und bezeichnet lediglich Untreue beziehungsweise Ehebruch als Sünde. Weiterhin warnt die Bibel in vor sexuellen Fantasien mit einem anderen als dem eigenen Ehepartner. Über innereheliche Selbstbefriedigung dagegen trifft die Bibel keine klare Aussage, jedoch sind die Eheleute nach aufgefordert, sich einander nicht zu entziehen.

Nach Auffassung der römisch-katholischen Kirche stellt Selbstbefriedigung als „absichtliche Erregung der Geschlechtsorgane, mit dem Ziel, geschlechtliche Lust hervorzurufen“ (KKK Nr. 2352), wie auch jeglicher freiwillige, außereheliche „Gebrauch der Geschlechtskraft“ eine „in sich schwere ordnungswidrige Handlung“ dar. Sie gehört neben Pornographie und homosexuellen Praktiken zu den Sünden, die schwer gegen die Keuschheit verstoßen (KKK 2396). Allerdings werden in der Seelsorge Faktoren wie „affektive Unreife, die Macht eingefleischter Gewohnheiten, Angstzustände und weitere psychische oder gesellschaftliche Faktoren“ berücksichtigt, „welche die moralische Schuld vermindern oder sogar auf ein Mimimum beschränken können.“

Im Koran findet sich folgendes:
Sunnitische Rechtsgelehrte beziehen diese Koranstelle nicht auf Masturbation, sondern auf die zeitlich begrenzte "Genuss-Ehe", die sogenannte Mutʿa-Ehe, die bei zwölfer-schiitischen Muslimen zulässig ist. In der Sure 24 lautet Vers 33:

Keuschheit ("keusch" aus ) wird im Wortsinne als „Mäßigung im Umgang mit Sexualität“ verstanden, in anderen Koranstellen (Sure 17:32 zu Ehebruch; 24:30 und 33:35 zu Keuschheit) jedoch stets auf das ethische Handeln "zwischen" den Geschlechtern angewandt. Die traditionelle Exegese bezieht jede andere Form der Sexualität mit ein. Es gibt eine Reihe von Hadithen, von denen sich einige auch zur Masturbation ablehnend äußern. Unter anderem wird Fasten zur Vermeidung von Sünde empfohlen.

Im Unterschied zu vielen anderen Religionen sieht der Daoismus in der Masturbation keine „Sünde“, betrachtet aber die zum Samenerguss führende männliche Masturbation kritisch, weil sie durch die Verschwendung des Samens einen Verlust an Qi verursache und zu Schwächung und Krankheit des Körpers führe. Die daoistisch korrekt – also ohne Ejakulation – ausgeführte Masturbation dagegen wird nicht nur toleriert, sondern sogar als für den Körper gesund angesehen. ("Siehe auch:" Daoistische Sexualpraktiken, Abschnitt zur männlichen Ejakulation)

Masturbation wurde bei zahlreichen Säugetierarten (beispielsweise Hunden, zahlreichen Affenarten, Kühen, Pferden, Walen, Fledermäusen, Schafen) aber auch bei Vögeln, und bei Reptilien (beispielsweise Schildkröten) beobachtet. Dies umfasst, anders als früher behauptet, nicht nur domestizierte und in Gefangenschaft lebende Tiere, sondern auch wilde Tiere in freier Natur beider Geschlechter.
Bei Bären wurde beobachtet, dass sie masturbieren, während sie anderen Bären bei der Paarung zusehen.

Die dabei verwendeten Techniken sind vielfältig und umfassen beispielsweise die manuelle Stimulation mit Hand, Pfoten, Füßen oder Schwanz, Autofellatio, Reiben des Penis gegen den Bauch oder Gegenstände. Auch das Herstellen von Werkzeugen, die der Masturbation dienen, ist bei einigen Arten bekannt. Bei einigen Arten wurde eine spontane Ejakulation ohne vorhergehende körperliche Stimulation beobachtet. Auch die Stimulation weiterer erogener Zonen wie die Zitzen oder das Geweih verschiedener Hirscharten kann beobachtet werden. Bei weiblichen Säugetieren umfasst die Masturbation häufig die direkte oder indirekte Stimulation der bei allen Säugetieren vorhandenen Klitoris.





</doc>
<doc id="8887" url="https://de.wikipedia.org/wiki?curid=8887" title="Scheide">
Scheide

Scheide steht für:

Scheide ist der Familienname folgender Personen:


Siehe auch



</doc>
<doc id="8888" url="https://de.wikipedia.org/wiki?curid=8888" title="Kunstgeschichte">
Kunstgeschichte

Die Kunstgeschichte, veraltet auch Kunsthistorik, oder Kunstwissenschaft, ist die Wissenschaft von der historischen Entwicklung der bildenden Künste und ihrer ikonographischen, ikonologischen wie auch materiellen Bestimmung. Sie untersucht und beschreibt ebenso die kulturelle Funktion der Kunst hinsichtlich ihrer künstlerisch-anschaulichen Gegebenheiten, wie auch den Schaffensprozess von Künstlern.

Die Geschichte der Bildenden Kunst vollzieht sich durch die Veränderung der gesellschaftlichen Funktion und Stellung der Kunst, der theoretischen Auffassung über sie sowie durch die Entwicklung der Kunstformen und Stilrichtungen. Ziel des Faches Kunstgeschichte ist es, die künstlerischen Objekte nach ihren Inhalten zu befragen (Ikonographie), ihre formale Gestaltung zu bestimmen, die Werke in Raum und Zeit einzuordnen und ihrer Rezeption nachzugehen; dabei werden einerseits stilistische Zusammenhänge besprochen, andererseits wird versucht, den historischen Kontext als Voraussetzung eines Kunstwerks zu verstehen oder ihn zum Verständnis des Werks miteinzubeziehen.

Im Gegensatz zur Kunstkritik wählt sich die Kunstgeschichte in der Regel historische Gegenstände oder versucht zumindest sich zeitgenössischen Themen mit einer wissenschaftlich abgesicherten, methodisch definierten Herangehensweise zu nähern. Dabei wird anerkannt, dass (wissenschaftliche) Rezeption und Interpretation selbst zeitgebundene Handlungen sind.

Die klassischen Untersuchungsobjekte der Kunstgeschichte sind europäische und vorderasiatische Werke der Malerei und Grafik, Bildhauerei und Baukunst in der Zeit vom frühen Mittelalter bis zur Gegenwart. Seit ungefähr der zweiten Hälfte des 19. Jahrhunderts werden auch Gegenstände aus den Kirchenschätzen, die sog. Kleinkunst, analysiert. Die Vor- und Frühgeschichte behandelt (auch) die künstlerische Entwicklung vor dem Auftauchen der Schrift. Die Archäologie und die Ägyptologie behandeln (auch) die künstlerische Entwicklung der frühen Hochkulturen des Mittelmeerraumes. Die Kunstgeschichte widmet sich der Erforschung der historischen Entwicklung der europäischen Kunst ab dem Zeitpunkt, an dem das Christentum im 4. Jahrhundert im Römischen Reich Staatsreligion wird. In der Gegenwart erweitert sich das untersuchte Gebiet auf die kulturellen Einflusszonen der sogenannten westlichen Hemisphäre, also etwa auch Amerika oder die zeitgenössischen Künstler weltweit, die am Kunstmarkt teilnehmen. Die Architekturgeschichte wird nicht selten von der Kunstgeschichte berührt, obwohl sie im Kern heute zu den Kulturwissenschaften zu zählen ist. Jedoch kommt kaum eine allgemeine Kunstgeschichtsschreibung ohne die Erwähnung der Architekturgeschichte aus.

Die Kunst nichteuropäischer Kulturen und Länder wird außerhalb dieser Länder in den jeweiligen Landeskunden (Sinologie, Arabistik, Afrikanistik etc.) miterforscht oder in übergreifenden Disziplinen wie der Ethnologie. Die Kunstgeschichte öffnete sich seit der ersten Hälfte des 20. Jahrhunderts (siehe Carl Einstein, Leo Frobenius) auch anderen Kulturkreisen, etwa der Afrikanischen oder Asiatischen Kunstgeschichte. Darüber hinaus werden neue Darstellungsformen wie Fotografie, Medienkunst und Gattungen, Kunstgewerbe, Design untersucht. Jüngste Entwicklungen sehen in der Kunstgeschichte auch eine Bildwissenschaft, die – unabhängig vom Kunstcharakter eines Bildes – Funktionen und Entwicklungen analysiert (vgl. z. B. auch Game Studies).

Die Begriffe "Kunstgeschichte" oder "Kunstwissenschaft" sind eine Schöpfung des 19. Jahrhunderts und gehen auf Johann Joachim Winckelmann (1717–1768) zurück, der in seinen Werken zur Kunst der Antike erstmals genauere stilgeschichtliche Untersuchungen unternommen hat. Als erster deutscher Kunsthistoriker, der auch gemalt hat, kann wohl Joachim von Sandrart genannt werden, der in seinem 1679 erschienenen theoretischen Hauptwerk über die "Teutsche Academie der Edlen Bau- Bild und Mahlerey-Künste" erstmals über deutsche Künstler und Kunststile geschrieben hat. Im ausgehenden 18. Jahrhundert legte Fiorillo an der Universität Göttingen die Grundlagen für die Kunstgeschichte als akademisches Fach. Die zweite Anregung brachte die Kunsttheorie, allen voran Friedrich Wilhelm Joseph Schelling und Johann Gottfried Herder sowie die Romantik.

Grundlagen für die Kunstwissenschaft legten Karl Friedrich von Rumohr und Gustav Friedrich Waagen. Als Fachwissenschaft begründet wurde sie zum einen von Historikern wie Jacob Burckhardt, Herman Grimm und Carl Justi, die die Kunst in den allgemeinen Rahmen der Kulturgeschichte einbezogen. Zum zweiten durch die beginnende antiquarische Sichtung und Ordnung der überlieferten Kunstwerke, die eng mit dem Kunstsammeln verbunden war. Aus ihr entstanden die positivistische und kennerschaftliche Kunstgeschichte (Giovanni Morelli, Gottfried Semper). Eine dritte Wurzel der Kunstgeschichte kam aus der Philosophie und Ästhetik, vertreten durch Heinrich Gustav Hotho, Karl Schnaase; auch viele spätere Kunsthistoriker studierten Kunstgeschichte und Philosophie (Heinrich Wölfflin).

Von einer eigenständigen Disziplin Kunstgeschichte lässt sich erst seit dem 19. Jahrhundert sprechen. Bei vorhergehenden Schriften handelte es sich meist um Kunstbetrachtung und biographische Beschreibungen. Die Entwicklung dorthin bereiteten Traktate verfassende und schriftstellernde Künstler, Kunstschriftsteller, Philosophen und Kunstkritiker vor. Bereits in der Antike entstehen Texte über Kunst, die allerdings wie bei Lukian Kunstwerke in synästhetischer Form von Ekphrasen beschreiben, oder wie bei Plinius dem Ältern Kunstgeschichte als Teil eines allgemeinen Gesamtwerkes (Plinius d. Ä.:"Naturalis historia" (Geschichte der Natur)) abhandeln.

In ähnlicher Weise als Teil eines größeren Werkes über ein anderes Thema (Architektur) beschäftigte sich Vitruv in seinen zwischen 33 und 22 v. Chr. entstandenen "10 Büchern über die Architektur" mit der Kunst, in denen er zum Schluss kommt, dass die Architektur ein Primat über die Gattungen der bildenden Kunst habe.

Während das Werk zu seiner Zeit und in den folgenden Jahrhunderten wenig Aufsehen erregte, änderte sich das in der Renaissance, in denen Vitruvs Theorien bedeutende Künstler wie Albrecht Dürer und Leonardo Da Vinci zu Skizzen inspirierten. Dessen Illustration „Der vitruvianische Mensch“ gilt als eines der bekanntesten Werke der Kunst und begründete Vitruvs späten Ruhm.

Diese Praxis wird erst wieder in der Renaissance von einem Autor aufgegriffen, der mit Leonardo da Vinci eine außerordentliche Breite des wissenschaftlichen und künstlerischen Betätigungsfeldes gemein hatte: Giorgio Vasari. Der 1511 geborene († 1574) Architekt, Hofmaler der Medici und gleichzeitig als Biograf zeitgenössischer florentinischer Künstler tätige Schriftsteller war einer der ersten systematisch vorgehenden Kunsthistoriker. Die Kreativität seiner anderen Berufe bewies er auch als Autor: Mit der „Gotik“, die er als Anhänger der antiken Kunst als barbarisch (ital.: "gotico") empfand und der „Renaissance“ erfand er Schlüsselbegriffe, die bis heute die Kunstgeschichte prägen. Sein Werk "„Le Vite de’ più eccellenti Architetti, Pittori et Scultori italiani, da Cimabue insino a’ tempi nostri“" ("Die Lebensbeschreibungen der hervorragendsten italienischen Architekten, Maler und Bildhauer, von Cimabue bis in unsere Zeit") erschien 1550. Eine zweite, stark veränderte Auflage mit weiteren Künstlerbeschreibungen von Leon Battista Alberti, Albrecht Dürer, Andrea Palladio u. a. erschien im Jahre 1568. In diesen Auflagen wurden erstmals die wichtigsten Künstler einer Epoche in einem Werk zusammengefasst, – zum Teil vergleichend – beschrieben und in ihrer Bedeutung eingeordnet.

Im 1604 erschienenen Werk "Schilder-Boeck" führte der Holländer Karel van Mander diese Tradition weiter. Auch van Mander war von Haus aus Maler und Lehrer so bekannter Künstler wie Frans Hals, bevor er sich als Verfasser kunsthistorischer Schriften betätigte. Sein dreiteiliges „Maler-Buch“ war die erste nördlich der Alpen erschienene kunsttheoretische Schrift und befasste sich im ersten Teil mit den Grundlagen der Kunst, im zweiten mit Biographien verschiedener antiker Maler und bekannter italienischer Maler und im dritten mit in der niederländischen Malerei verwandten mythologischen Textquellen.

1755 veröffentlichte Johann Joachim Winckelmann, der später als erster Ausländer die Oberaufsicht über die Antiken in Rom haben sollte, in Dresden seine erste Schrift: "Gedanken über die Nachahmung der Griechischen Werke in der Malerei und Bildhauer-Kunst", der er in späteren Neuauflagen weitere Texte beigab. Darin sind bereits die neuen, folgenschweren Gedanken enthalten, die er in seinem 1764 erschienenen Hauptwerk, "Die Geschichte der Kunst des Altertums" in 2 Bänden, ausführlich darlegte. Winckelmann beschreibt darin nicht nur den chronologischen Ablauf einer Kunstgeschichte der Antike, sondern ein System der griechischen Kunst. Er entwickelt Kriterien einer Ästhetik des Schönen und identifiziert einen klassischen Stil, den er zum Maßstab seiner Beurteilung erhebt. Zwar steht diese Suche nach dem Schönen noch im Mittelpunkt, doch der Versuch einer Stilgeschichte gibt dem Idealen, der "edlen Einfalt" und "stillen Größe" einen ersten Kontext. Winckelmann stand im Austausch mit zeitgenössischen Künstlern (Anton Raphael Mengs) und stellte ständig Bezüge von der künstlerischen Vergangenheit in die damalige Gegenwart her. Zum „ersten Kunsthistoriker“ macht ihn unter anderem, dass er als Archäologe und Grabungsleiter von der materiellen Kenntnis seiner Forschungsobjekte ausging; dass er präzise Beschreibungen als Erkenntnismethode verwandte; dass er sich für die Systematisierung seiner Forschungsgegenstände interessierte.

In Göttingen wurde 1799 die erste Professur für Kunstgeschichte eingerichtet. Der Zeichenlehrer Johann Dominik Fiorillo betreute die Kunstsammlung und unterrichtete die ersten Studenten.

Die Entwicklung der wissenschaftlichen Disziplin Kunstgeschichte hat durch den Diskurs über exemplarische Fallbeispiele des Fachs im 19. Jahrhundert immer wieder Fortschritte gemacht. Eine besondere Rolle spielten dabei u. a. die Laokoon-Gruppe oder der Dresdner Holbeinstreit. Der Basler Historiker Jacob Burckhardt (1818–1897) widmete sich erstmals der Betrachtung einer ganzen Kulturlandschaft unter dem Aspekt ihrer künstlerischen Produktion zu Zeiten einer bestimmten Epoche. Von grundlegender Bedeutung für die universitäre Kunstgeschichte war zu diesem Zeitpunkt die "Stilgeschichte", also die stilistische Analyse von Kunstwerken, die Frage nach dem künstlerischen "Wie", zu der als weitere Erkenntnismittel die historische und hilfswissenschaftliche Erforschung eines Kunstwerks – also die Frage nach dem "Was" – kam. Später sollte sich dieses Verhältnis umkehren.

Wie kaum eine andere Wissenschaft wurde die Kunstgeschichte bis zur Machtergreifung 1933 von deutschen Gelehrten und der Lehre an deutschen Universitäten geprägt. Wichtige deutschsprachige Schulen der Kunstgeschichte vor dem Zweiten Weltkrieg waren:

Mit Karl Friedrich von Rumohr, Franz Theodor Kugler, Gustav Friedrich Waagen, Heinrich Gustav Hotho, Heinrich Wölfflin und Carl Schnaase.

Zur Wiener Schule gehören ihre Vertreter Franz Wickhoff, Alois Riegl, Julius von Schlosser, Moritz Thausing, Rudolf Eitelberger, Max Dvořák, Otto Pächt und Hans Sedlmayr. Fritz Saxl, Ernst Kris, Ernst Gombrich. Sie wurden alle in Wien ausgebildet. Max Dvořák prägte den Begriff der "Kunstgeschichte als Geistesgeschichte", Alois Riegl forschte über das Kunstwollen und prägte den Begriff der "spätrömischen Kunstindustrie".

Mit Heinrich Wölfflin, Hans Jantzen, Wilhelm Pinder.
Wölfflin prägte in der ersten Hälfte des 20. Jahrhunderts die Kunstgeschichte durch seinen formalistischen Stilbegriff.

Aby Warburg, Gertrud Bing, Fritz Saxl, Erwin Panofsky, William S. Heckscher, Edgar Wind, nach Warburgs Tod Ernst H. Gombrich. Der älteste Warburg-Bruder gründete in Hamburg seine Bibliothek zur Erforschung des Nachlebens der Antike in der Neuzeit, mit Saxl und Bing als engen Mitarbeitern. Panofsky und Wind studierten in Hamburg bei Ernst Cassirer, der die Kulturwissenschaftliche Bibliothek Warburg für seine Erforschung symbolischer Formen nutzte. Panofsky begründete den kunsthistorischen Forschungszweig der Ikonologie im nichtchristlichen Verständnis.

Auf Grund des Gesetzes zur Wiederherstellung des Berufsbeamtentums verloren viele jüdische Kunsthistoriker nach 1933 ihre Lehrberechtigung und wurden zur Emigration gezwungen. Zu den bekanntesten gehörten Erwin Panofsky (Princeton), Walter Friedlaender (New York University), Julius Held, Ernst H. Gombrich, in London am Warburg Institute, dessen Direktor er von 1959 bis 1976 war, Ernst Kris, Nikolaus Pevsner und Ernst Cohn-Wiener.

Ihre Stellen wurden auch von Kunsthistorikern besetzt, die den Zielen des Nationalsozialismus emphatisch zustimmten. Unter dem Aspekt ihres Verhältnisses zum „Dritten Reich“ waren bereits selbst Gegenstände der kunsthistorischen Forschung: u. a. Wilhelm Pinder (München, Berlin), Hans Sedlmayr (Wien) und Percy Ernst Schramm (Göttingen).

Durch die Vertreibung bedeutender Wissenschaftler durch die Nationalsozialisten entstanden im Ausland wichtige Zentren der kunsthistorischen Forschung: in Großbritannien das Warburg Institute, Courtauld Institute und Oxford, und in den Vereinigten Staaten an den Universitäten Princeton, Columbia, Berkeley und Stanford.

Die wichtigsten Vertreter des Faches waren in:

Heute wird das Fach Kunstgeschichte weniger von Schulen, denn von herausragenden Persönlichkeiten und bestimmten Forschungsschwerpunkten geprägt. Eine genaue Abgrenzung der beiden Begriffe lässt sich schwer ziehen, letztere bezieht aber generell Nachbardisziplinen aus der Psychologie, Soziologie oder Kulturgeschichte u. a., die sich mit Kunst auseinandersetzen, mit ein.

Die wichtigsten Forschungsfelder der jüngeren Kunstgeschichte werden von einigen Kunsthistorikern nicht mehr in der Bestandssicherung, der Datierung und Zuordnung einzelner Kunstwerke, sondern in der Untersuchung von Funktionen, Strukturen und soziologischer Bedeutung von Kunstwerken und Kunst im Allgemeinen gesehen. Diese Kunsthistoriker greifen damit die Entwicklungen anderer geisteswissenschaftlicher Disziplinen auf. Trotzdem wird auch die Bauforschung und daran angeschlossen eine objektbezogene Vorgehensweise der Kunstgeschichte weiterhin nicht zu vernachlässigen sein, da auch theorielastige Arbeiten nur dann glaubwürdig erscheinen, wenn sie – wie es oftmals nicht der Fall ist − auf konkret nachweisbaren Befunden fußen können.









Die folgenden Fachverbände vertreten die Interessen von Kunsthistorikern:







</doc>
<doc id="8889" url="https://de.wikipedia.org/wiki?curid=8889" title="Karolinger">
Karolinger

Karolinger ist der auf Karl Martell zurückgehende Hausname des Herrschergeschlechts der westgermanischen Franken, das ab 751 im Frankenreich die Königswürde innehatte. Sein berühmtester Vertreter war Karl der Große, von dem die späteren karolingischen Herrscher abstammten. Nach der Teilung des Karolingerreichs im Jahr 843 regierten die Karolinger im Ostfrankenreich bis zu ihrem dortigen Aussterben im Jahr 911, im Westfrankenreich mit zwei Unterbrechungen bis zum Dynastiewechsel von 987, der die Kapetinger an die Macht brachte. Im Herzogtum Niederlothringen herrschte noch bis ins frühe 11. Jahrhundert ein Seitenzweig der westfränkischen Karolinger. Mit diesem starb das Karolingergeschlecht im Mannesstamm aus (wenn man nur die nachfolgeberechtigten ehelichen Söhne berücksichtigt). Nicht thronfähige Nachkommen unehelicher Kinder der karolingischen Herrscher sowie Nachkommen Karls des Großen in weiblicher Linie existieren jedoch in großer Zahl.

Als die Karolinger regierten, wurden sie in den Quellen oft als Nachkommen Karls des Großen bezeichnet, aber den Begriff "Karolinger" als Bezeichnung für einen Abstammungsverband gab es noch nicht. Diese Bezeichnung wurde erst nachträglich eingeführt, wobei man offenbar auf die Rolle Karls des Großen als Urahn der späteren Karolinger und auf den Leitnamen "Karl" Bezug nahm. Im Ostfrankenreich tauchte erst in der zweiten Hälfte des 10. Jahrhunderts bei dem Geschichtsschreiber Widukind von Corvey die Bezeichnung "Karoli" „Karle“ auf; er nannte um 965 den 911 gestorbenen letzten ostfränkischen karolingischen König, Ludwig das Kind, "ultimus Karolorum apud orientales Francos", also „letzter der Karle bei den Ostfranken“. In Frankreich, wo die Herrschaft der Karolinger 987 geendet hatte, bezeichnete bald darauf der Geschichtsschreiber Richer von Reims die Könige der Vergangenheit als „Karle“. Daraus wurden später die lateinischen Namensformen "Karlenses", "Karlingi", "Karolini" und schließlich auch "Karolingi", woraus dann das deutsche Wort "Karolinger" abgeleitet wurde.

Ahnherren der Karolinger waren Arnulf von Metz aus dem Geschlecht der Arnulfinger und Pippin der Ältere aus dem Geschlecht der Pippiniden. Die Karolinger herrschten bereits ab 639 mit Unterbrechungen im Frankenreich, jedoch nicht als Könige, sondern nur als Hausmeier der Merowinger, deren alleiniger Anspruch auf die Königswürde weiterhin respektiert wurde. Bis zur Mitte des achten Jahrhunderts konnten die Karolinger ihre Macht so weit ausbauen, dass sie sich schließlich des nominellen merowingischen Königtums entledigen konnten. Pippin der Jüngere wurde 751 mit päpstlicher Unterstützung von den fränkischen Adligen zum König der Franken akklamiert, der letzte Merowinger wurde abgesetzt. Man nahm ihm symbolisch das Königsheil, indem seine Haare geschoren und er in ein Kloster gesteckt wurde. Dieser Dynastiewechsel bedeutete eine mit dem Gedanken des Gottesgnadentums begründete Abweichung von der Vorstellung des Geblütsrechts, die aber weiterhin lebendig blieb und nun auf die neue Dynastie übertragen wurde. Der bekannteste Karolinger, Karl der Große, begründete ein neues römisches Kaisertum, als er am 25. Dezember 800 in Rom von Papst Leo III. zum Kaiser gesalbt und gekrönt wurde.
Auf Karl den Großen folgte 814 Ludwig der Fromme als Kaiser, da Karls ältere Söhne Karl der Jüngere und Pippin der Bucklige bereits vor Karl verstarben. Noch zu Lebzeiten Ludwigs des Frommen erhoben sich seine Söhne Lothar I., Ludwig II., Pippin und Karl II. der Kahle in verschiedenen Koalitionen gegen ihren Vater und bekämpften sich gegenseitig (siehe: Innerdynastische Kämpfe der Karolinger 830–842), wobei der Rang des Kaisers, den Lothar seit 817 als Mitkaiser seines Vaters innehatte, nie in Frage gestellt wurde. 843 einigten sich die drei übrig gebliebenen Brüder Lothar I., Ludwig II. und Karl II. im Vertrag von Verdun auf eine Teilung der Herrschaft, jedoch nicht des Reiches. Nachdem Lothars Söhne ohne Erben verstorben waren, wurde die Herrschaft im Vertrag von Meerssen 870 zwischen Karl II. und Ludwig II. erneut geteilt. Karl III. der Dicke, König von Ostfranken, einte 885 kurzfristig beide Teile Frankens. Als Kaiser folgten ihm Arnulf von Kärnten und Ludwig III. der Blinde. Danach erloschen die Karolinger als Kaisergeschlecht.

Das Geschlecht der Karolinger beherrschte in unterschiedlichen Konstellationen bis 987 Mitteleuropa und prägte die frühmittelalterliche Welt entscheidend. Ausgehend vom fränkischen Erbschaftsrecht, welches keinen automatischen Primat des Erstgeborenen vorsah, wurde das Frankenreich nach dem Tode Karls des Großen mehrfach geteilt. Der Vertrag von Verdun 843 kann als der Ausgangspunkt der Länderentstehung Frankreichs und Deutschlands angesehen werden. Mit den Teilungen und dem Vertrag von Coulaines im westlichen Frankenreich ging ein Machtzuwachs des fränkischen Adels einher, der stets versuchte, Teilungspläne zu seinen Gunsten zu beeinflussen. Waren es die Karolinger, die das Angesicht Europas so prägten, dass ihr Einfluss noch auf über 1200 Jahre hinaus sichtbar sein sollte, so liegt in ihrer Herrschaft auch der Ursprung der föderativen und dezentralen Verfassung und Herrschaftsausübung auf dem Gebiet des heutigen Deutschland begründet.

Als die ostfränkische Linie 911 mit Ludwig dem Kind ausstarb, folgten erst mit dem Frankenherzog Konrad I. ein Konradiner und ab 919 mit dem Herzog der Sachsen Heinrich I. die Liudolfinger nach, die später Ottonen genannt wurden. Im Westfrankenreich waren noch bis 987 Karolinger an der Macht, dann wurden sie von den Kapetingern verdrängt.

Der Stil der Baukunst dieser Zeit, eine Form der Vorromanik, wird „karolingischer Baustil“ genannt. Die zur Zeit Karls des Großen entstandene Buch- und Verwaltungsschrift wird als karolingische Minuskel bezeichnet.

Wichtige erzählende Quellen stellen unter anderem Einhards Biographie Karls des Großen, diverse Annalen (unter anderem die sogenannten Reichsannalen, die Metzer Annalen, die Annalen von St. Bertin und die Annales Fuldenses), mehrere Chroniken (wie die des Regino von Prüm) und Nithards "Historien" dar.












Während die herrschenden Linien der Karolinger erloschen, überlebte ein Zweig der Karolinger – die Grafen von Vermandois – selbst das Haus der Liudolfinger (Ottonen), die die Karolinger in der Herrschaft über das Ostfrankenreich abgelöst hatten. Diese Linie der Karolinger leitet sich von Bernhard, (Unter-)König von Italien (812–818), ab. Dieser war ein außerehelicher Sohn des Pippin, der von 781 bis 810 (Unter-)König von Italien war, und ein Enkel Karls des Großen.

Der letzte männliche Nachkomme dieses Hauses – und damit der Karolinger – Graf Eudes, starb nach 1085. Seine Schwester, Adelaide Gräfin von Vermandois und Valois († 1120/24) brachte diese Grafschaften in ihre Ehe mit Hugo von Frankreich (* 1057; † 1101) ein, der das zweite Haus Vermandois begründete.





</doc>
<doc id="8892" url="https://de.wikipedia.org/wiki?curid=8892" title="Feuer">
Feuer

Das Feuer (von althochdeutsch: "fiur") bezeichnet die Flammen­bildung bei der Verbrennung unter Abgabe von Wärme und Licht. Voraussetzungen für die Entstehung und Aufrechterhaltung eines Feuers sind ein Brennstoff, ein Oxidationsmittel, wie etwa Sauerstoff aus der Luft, sowie die Überschreitung der Zündtemperatur des Brennstoffs.

Die Erzeugung von Feuer zählt zu den Kulturtechniken. Die Nutzung und zunehmende Beherrschung des Feuers war ein wichtiger Faktor der Menschwerdung und ist mindestens seit dem Jungpaläolithikum ein Bestandteil aller Zivilisationen.

Chemisch gesehen ist Feuer eine Oxidationsreaktion mit Flammenerscheinung. Dies ist eine exotherme Reaktion, das heißt, dass mit Feuererscheinung verlaufende Reaktionen mehr Energie in Form von Wärme an die Umgebung abgeben als zum Entzünden benötigt wird. Feuer ist heiß, weil die Umwandlung der schwachen Doppelbindung im Sauerstoffmolekül, O, in die stärkeren Bindungen in den Verbrennungsprodukten (Kohlendioxid und Wasser) Energie freisetzt (418 kJ pro 32 g O); die Bindungsenergien im Brennstoff spielen nur eine untergeordnete Rolle.

Zur Entfachung bzw. Aufrechterhaltung eines Feuers werden allgemein ein brennbarer Stoff, ein Oxidator und Zündenergie (Wärme, mechanische Funken, Elektrizität) benötigt. Dieser Zusammenhang kann in einem Verbrennungsdreieck anschaulich dargestellt werden. Mangelt es an einer der drei Komponenten, erlischt das Feuer. Dies kann man sich zur Brandbekämpfung zunutze machen.

Bei der Verbrennung von organischen Materialien werden beispielsweise Kohlenwasserstoffe mit dem Oxidationsmittel Sauerstoff aus der Luft bei einer vollständigen Verbrennung zu Kohlenstoffdioxid und Wasser umgesetzt. Auch partielle Verbrennungen sind möglich, wobei Kohlenstoffmonoxid und andere, nur teilweise oxidierte Stoffe entstehen und nicht oxidierte Stoffe wie Ruß zurückbleiben können. Das Oxidationsmittel kann jedoch in manchen Fällen auch bereits dem Brennstoff beigemischt sein, beispielsweise in Form von Salpeter.

Da die entstehenden Verbrennungsgase aufgrund ihrer hohen Temperatur eine geringere Dichte haben als die umgebende Luft, steigen sie bei einer frei brennenden Flamme durch natürliche Konvektion nach oben (Kamineffekt). Der entstehende Unterdruck saugt von unten und von der Seite Frischluft an. Der darin enthaltene Sauerstoff erhält die weitere Verbrennung aufrecht. Bei extrem großen Feuern kann der so entstehende Luftzug Orkan­stärke erreichen – man spricht dann von einem Feuersturm.

Da in der Schwerelosigkeit die Dichteunterschiede keine Konvektion verursachen, ist die Zufuhr von neuem Sauerstoff gestört und nur durch Diffusion möglich, weshalb sich beispielsweise bei einer in einem Raumschiff brennenden Kerze nur eine relativ schwache und annähernd kugelförmige Flamme ausbildet.

Ist die natürliche Konvektion für den gewünschten Zweck nicht ausreichend, können bei technischen Anwendungen sowohl die Luftzufuhr als auch die Abfuhr der Verbrennungsgase auch künstlich erfolgen, beispielsweise mit Hilfe von Gebläsen (siehe auch Saugzug).

Das Licht des Feuers ist eine physikalische Erscheinung. Elektronen der erhitzten Teilchen erlangen kurzzeitig ein höheres Energieniveau und fallen nach kurzer Zeit unter Abgabe (spontaner Emission) von Energie in Form eines Lichtquants (Photons) auf ihre ursprünglichen Energieniveaus zurück. Nicht jede solche Emission ist für das menschliche Auge sichtbar, es entsteht auch infrarote Strahlung (siehe Flammenfärbung).

Mit den chemischen und physikalischen Vorgängen in einem Feuer befasst sich die Verbrennungslehre.

Das neuhochdeutsche Wort "Feuer" lässt sich – über mittelhochdeutsch "viur", althochdeutsch "fiur" und westgermanisch "fewur" – zurückverfolgen bis zum uralten, bedeutungsidentischen Indogermanischen "peu̯ōr", "pū̌r", Genitiv "punés" (vergleiche auch altgriechisch πῦρ/"pyr", armenisch "hur", hethitisch "pahhur", gotisch "fon" und umbrisch "pir").

Die Zähmung von Wildfeuern (beispielsweise aus Blitzschlägen oder Erdbränden) und später die Kunstfertigkeit, Feuer zu entfachen, waren wichtige Schritte der Menschwerdung. Bei Zunahme der karnivoren Diät, die für "Homo habilis", mehr noch für "Homo rudolfensis" mit Veränderungen an Gebiss und Gehirn belegt ist, war die Verwertung dieser Nahrung durch Garen wesentlich effizienter. Erhitzung – durch Braten über offenem Feuer oder Kochen in heißen Quellen – erleichtert den enzymatischen Aufschluss der Nahrung und entlastet damit den Verdauungstrakt. Außerdem konnte Nahrung durch Räuchern länger haltbar gemacht werden (was anhand von Tierleichen nach einem Buschbrand oder sonstigem verbranntem Fleisch erlernt werden konnte). Das Erhitzen verringerte ferner die Belastung der Nahrung durch krankmachende Parasiten, Bakterien und Viren.

Feuer bot zugleich Wärme, Licht und Schutz vor Raubtieren und Insekten. Feuer ermöglichte die Härtung von Holz und Stein und später (im Neolithikum) von Ton oder Lehm zu Keramik und (noch später) zur Schmelze von Erzen.

Sehr frühe archäologische Belege der Feuernutzung durch Australopithecinen (vor 4–1,5 Millionen Jahren) ebenso wie durch "Homo habilis" (vor 2,5–2 Millionen Jahren) sind bis heute umstritten. Prominente Beispiele solch zweifelhafter Belege sind Koobi Fora am Turkanasee (Kenia), Swartkrans (Südafrika), Yuanmou (China), Gongwangling-Stätte (China; vgl. Lantian-Mensch) und Pandalja 1 bei Pula (Kroatien). Die Indizien von Feuerstellen in Swartkrans bestehen im Grad der Erhitzung des Sediments, indem mittels Elektronenspinresonanz belegt wird, dass die Brenntemperatur in der Feuerstelle höher war als bei einem natürlichen Grasbrand. Ein weiterer umstrittener Fundplatz liegt im kenianischen Chesowanja, nahe dem Baringosee. Dort wurden Tierknochen und Oldowan-Werkzeuge neben über fünfzig verbrannten Lehmbrocken sowie eine feuerstellenähnliche Anordnung von Steinen gefunden.

Die ältesten gesicherten Feuerstellen, die zweifelsfrei durch Menschen angelegt wurden, stammen aus der Wonderwerk-Höhle in Südafrika und sind etwa 1,7 Millionen Jahre alt. Als Indiz dienen verbrannte Knochensplitter und Pflanzenreste im tiefen Inneren der Höhle. Eine Feuerstelle mit verbrannten menschlichen Nahrungsresten liegt auch von Gesher Benot Ya’aqov im Norden Israels vor, die mit Werkzeugen von "Homo erectus" in Verbindung steht und etwa 790.000 Jahre alt ist. Neben kleinformatigen gebrannten Steinartefakten, deren räumliche Verteilung auf Feuerstellen schließen lässt, wurden hier auch verbrannte Reste essbarer Pflanzen gefunden: Wilde Gerste ("Hordeum spontaneum") sowie Holz Wilder Olivenbäume ("Olea europaea" subsp. "oleaster") und Wilder Weinreben "(Vitis sylvestris)." Viele Forscher gehen davon aus, dass die Besiedelung Ostasiens durch "Homo erectus" bzw. des nordalpinen Europa durch "Homo heidelbergensis" (synonym für den späten "Homo erectus" in Europa) vor etwa 600.000 Jahren nur mit Hilfe von Feuernutzung möglich war. Dennoch sind einige früher für "Homo erectus" angeführte Belege heute widerlegt, wie in der Höhle von Zhoukoudian (China), wo die Laminierung der Sedimentschichten mit Schluffen, organischen Partikeln und Holzkohlen stattdessen deren natürlichen Eintrag beweist.

Als älteste gesicherte Nachweise Europas gelten rund 400.000 Jahre alte Feuerstellen aus der englischen Beeches Pit, Terra Amata bei Nizza und Vértesszőlős in Ungarn. Die Fundplätze werden in mittelpleistozäne Interglaziale datiert, die mit den marinen OIS 9, 11 oder 13 gleichgesetzt werden. In denselben Zeithorizont sind Feuerstellen in der Qesem-Höhle in Israel zu stellen, sowie ein 350.000 Jahre alter Befund aus der Tabun-Höhle.

Umstritten sind dagegen die Befunde vom thüringischen Fundplatz Bilzingsleben, wo „Holzkohlefeuer“ und erhitzte Travertinbrocken als Beleg eines "Living floors" beschrieben wurden. Andere Forscher gehen von umgelagerten Hölzern aus, die durch Waldbrände verkohlt wurden. Auch Manganausfällungen können infolge der Schwarzfärbung von Gesteinen wie Travertin die Existenz von Feuerstellen vorspiegeln. Kontrovers wird auch die Feuernutzung im niedersächsischen Schöningen diskutiert. Ein als „Bratspieß“ bezeichneter Fichtenholzstab im Umfeld der Schöninger Speere wurde möglicherweise bewusst im Feuer gehärtet, von anderen Autoren wird der kontrollierte Umgang mit Feuer an diesem etwa 300.000 Jahre alten Fundplatz jedoch bezweifelt.<ref name="DOI10.1016/j.jhevol.2015.04.004">Mareike C. Stahlschmidt, Christopher E. Miller, Bertrand Ligouis, Ulrich Hambach, Paul Goldberg, Francesco Berna, Daniel Richter, Brigitte Urban, Jordi Serangeli, Nicholas J. Conard: "On the evidence for human use and control of fire at Schöningen." In: "Journal of Human Evolution." 89, 2015, S. 181, .</ref> Die vermutete Feuerhärtung von Hölzern wird auch für die etwa gleich alte Lanzenspitze von Clacton-on-Sea und die eemzeitliche Lanze von Lehringen in Frage gestellt. Beispiele aus dem Zeithorizont der „klassischen“ Neandertaler der Würm-Eiszeit liegen mit der Grotte XVI, dem Abric Romaní, dem Roc de Marsal und aus Italien vor.

Feuerhärtung dominiert bei "Homo sapiens" seit 72.000 Jahren bei Steingeräten aus Hornstein wie Feuerstein, sie tritt seit 164.000 Jahren (Fundort Pinnacle Point in Südafrika) auf (Tempern von Feuerstein). Die älteste Pyrit­knolle als Teil eines steinzeitlichen Feuerbestecks wurde aus einer Brandschicht der württembergischen Vogelherdhöhle beschrieben, die der archäologischen Kultur des typischen Aurignacien zugeordnet wird und auf rd. 32.000 Jahre datiert wird. Diese Knolle, deren Schichtzugehörigkeit wegen der ungenauen Ausgrabung im Jahre 1931 nicht zweifelsfrei erwiesen ist, wäre der mit Abstand älteste Beweis für das „Feuerschlagen“ und damit eines Feuerzeugs. Dazu gehört neben einer Pyrit- oder Markasitknolle im weiteren ein Schlagstein (meist Feuerstein) und ein Stück Zunderschwamm ("Fomes fomentarius") oder anderer Baumschwamm (zum Beispiel Birkenporling). Bei der Mehrzahl der archäologischen Funde ist jedoch unklar, ob es sich um "Pyrit" oder "Markasit" handelt, daher sollte der neutrale Begriff "Schwefelkies" verwendet werden. Weitere altsteinzeitliche Belege angeschlagener Schwefelkiesknollen gibt es aus Laussel (Schichtzuordnung unklar, Solutréen?) und aus dem belgischen Chaleux (Magdalénien). Solche „Feuerschlag-Sets“ sind im Mesolithikum und der jüngeren Vorgeschichte dann gehäuft gefunden worden. Gut datierte Belege aus dem Frühmesolithikum liegen vom englischen Fundplatz Star Carr vor, wo sowohl "Fomes fomentarius" als auch Markasit-Stücke gefunden wurden. Der Nachweis konnte auch durch Rückstände (Residuen) von Pyrit an Schlagsteinen nachgewiesen werden, wie in den spätmesolithischen Fundplätzen Henauhof-Nord bei Bad Buchau und am Ullafelsen im Fotschertal.

Feuer wurden vermutlich bei der Treibjagd auf flüchtiges Wild eingesetzt. Archäologische Indizien dafür gibt es allerdings nicht. Jedoch verwendeten die steinzeitlichen Ureinwohner Nordamerikas wie auch Australiens vor Einflussnahme durch Europäer Feuer zur nicht-agrarwirtschaftlichen Landnutzung. Henry T. Lewis zählte etwa siebzig verschiedene Gründe für die Brandsetzung durch Indianer auf. Treibjagden auf größeres Wild scheinen durch Feuersetzungen nicht zu profitieren, dagegen können nach einem Flächenbrand zahlreiche Kleintiere (hauptsächlich durch Frauen) eingesammelt werden. Feuer wurde später (vermutlich ab Neolithikum) gezielt zu Rodungszwecken eingesetzt, um Agrarflächen zu schaffen.

Im Zuge der Neolithisierung bildete das Feuer die Basis wichtiger Kulturtechniken, wie das Brennen von Keramik (Töpferei) und die Metall­schmelze (seit der Kupfersteinzeit). Die Standardmethode des Neolithikums ist das „Schwefelkies-Feuerzeug“, wie an diversen Funden der Bandkeramischen Kultur belegt werden kann. „Markasit-Feuerzeuge“ sind auch während der Bronzezeit nachgewiesen.

Ab der Eisenzeit ersetzt nach und nach der Feuerstahl die Schwefelkiesknolle.
Im Gräberfeld von Bescheid wurde im Hügel 78/2 ein Roteisenstein und eine Feuersteinklinge gefunden, was in dieser Kombination als Feuerzeug gedeutet wird.

Ahlenförmige Feuerstähle wurden unter anderem auf dem Nydam-Schiff gefunden. In Norddeutschland sind während der Eisenzeit schiffchenförmige Quarzitobjekte bekannt, die ebenso wie Feuerstein zur Funkenproduktion dienen.

Die Funkenerzeugung in Verbindung von Eisen und Feuerstein bleibt bis in die Neuzeit die am weitesten verbreitete Art des Feuermachens in Europa. Dieses Prinzip wird auch beim Steinschloss-Prinzip der Flinten angewandt.

Soll ein Feuer entfacht werden, muss neben dem Vorhandensein von Brennstoff und Sauerstoff dafür gesorgt werden, dass genügend Sauerstoff an den Brennstoff gelangt und die Verbrennungsprodukte abziehen können (Kamin­wirkung). Für das Entfachen ist eine "Initialzündung" notwendig, um die Zündtemperatur zu erreichen, wofür vorindustrielle Völker verschiedene Methoden kannten:





Geübte Menschen können ein Feuer mit solchen Methoden in etwa einer Minute entfachen; siehe dazu auch Survival. Heutzutage werden Feuer meist mit dem Feuerzeug oder mit Streichhölzern entfacht. Gegebenenfalls wird ein Fidibus verwendet, um unzugängliche Stellen zu entzünden.

Im antiken Griechenland wurde dem Element Feuer das Tetraeder als einer der fünf Platonischen Körper zugeordnet. Feuer ist eines der Elemente sowohl der klassischen Vier-Elemente-Lehre als auch der chinesisch-japanischen Fünf-Elemente-Lehre.

Die Bedeutung des Feuers spiegelt sich in zahlreichen Mythen wider, etwa dem der Feuerbringer Prometheus und Huschang oder des Vogels Phönix.

Die alte Religion des persischen Religionsstifters Zarathustra wirkte nachhaltig in die dortige Volkskultur hinein. Auch heute noch lebt diese Religion als Parsismus bzw. Zoroastrismus fort. Viele persische Vornamen nehmen auf das Feuer Bezug.

Die Römer verehrten Vesta, die Göttin und Hüterin des Herdfeuers, mit einem eigenen Frauenkult (den Vestalinnen).

Judentum / Christentum: Im Alten Testament der Bibel sind Feuer, Rauch und Beben Begleiterscheinungen einer Theophanie (Gotteserscheinung, vgl. z. B. 2. Buch Mose, Kapitel 3). Nach dem Zeugnis der Apostelgeschichte zeigte sich der Heilige Geist „"in Zungen wie von Feuer"“(vgl. Apg. 2 Pfingsten). In der Osternacht wird am Osterfeuer die Osterkerze, Sinnbild der Auferstehung Jesu Christi, entzündet. Der Brauch des Osterfeuers hat vermutlich vorchristliche Wurzeln. Bei Johannes nennt sich Jesus selbst "das Licht der Welt". Dem Feuer wird außerdem reinigende Wirkung zugesprochen ("siehe auch: Fegefeuer)". So wurden in der frühen Neuzeit angebliche Hexen auf dem Scheiterhaufen verbrannt, "um ihre sündigen Seelen reinigen zu lassen."

Im Hinduismus ist Agni, das Feuer, die Verkörperung Gottes, der auf der Erde in Flammenform erscheint. Feuer spielt im Gottesdienst sowie in allen anderen religiösen Riten eine herausragende Rolle: Die populärste tägliche Zeremonie ist das "Arati", wo man ein Butterlicht vor dem Altar schwenkt. Das Feueropfer, "Yaggya" (auch "Yajna") genannt, war ursprünglich wahrscheinlich das wichtigste Opferritual, bei dem die Opfergaben in das heilige Feuer geworfen wurden.

Auch heute noch spielt das Feuer im Glaubensleben der Hindus eine wichtige Rolle: Zu bestimmten Anlässen, ganz besonders wenn es um Reinigungszeremonien wie Einweihung von Wohnungen, Geschäften oder dergleichen geht, entzündet der Priester unter Gebeten rituell das heilige Feuer. Im Feueropfer, heute auch "Homa" oder "Havan" genannt, verehrt er Agni. Bei einer Wohnungseinweihung etwa trägt der Priester oder der Besitzer anschließend die Schüssel mit dem glimmenden Feuer segnend durch die Räume. Besonders bei allen hinduistischen Sakramenten ist immer die lebendige Anwesenheit des Göttlichen in seiner Flammenform notwendig: Ein hinduistisches Paar schließt die Ehe, indem es gemeinsam siebenmal um das Feuer herumgeht.

In manchen ethnischen Religionen gibt es einen oder mehrere Feuergeist(er). Im finnischen Epos Kalevala spielt der Raub der Feuermühle "Sampo" aus dem „Nordort“ (Pohjola) eine bedeutende Rolle, nach der auch die finnische Streichholz-Marke „Sampo“ benannt ist.
"Zweckfeuer" ist – im Gegensatz zu Schadfeuer – das beabsichtigte und kontrollierte Feuer, das zum Erwärmen oder Verbrennen von Gegenständen oder anderem gedacht ist, zum Beispiel das Kamin­feuer, Lagerfeuer, Grillfeuer und Schwedenfeuer.

Der Mensch hat schon sehr lange gelernt, das Feuer zu beherrschen und nutzt es bis heute, zum Teil indirekt in Form des elektrischen Stroms. Aber auch in damit betriebenen Anlagen wird der Begriff "Feuer" verwendet, z. B. in "Befeuerung" und "Leuchtfeuer". In der Technik bezeichnet man eine technische Vorrichtung, die mit Hilfe von Feuer Wärme erzeugen soll, als Feuerung. Bei flüssigen oder gasförmigen Brennstoffen kommt meist ein Brenner zum Einsatz.

Das "Schadfeuer" – auch Brand genannt – ist ein zerstörerisches, meist unbeabsichtigtes Feuer. Es verbrennt ungewollt Gegenstände und ist erst kontrollierbar, nachdem es eingedämmt wurde. Brandbekämpfung von Schadfeuern ist die originäre Aufgabe der Feuerwehren.

Versicherungen definieren den Begriff "Brand" üblicherweise als "Feuer, welches ohne einen bestimmungsgemäßen Herd entstanden ist oder diesen verlassen hat und sich selbständig auszubreiten vermag". Bestimmungsgemäßer Herd kann hier jedes Objekt sein, welches dafür bestimmt ist, Hitze (Backofen, Bügeleisen) oder Feuer zu erzeugen. Absichtliche Schadfeuer können durch Pyromanie entstehen.

Mit Hilfe von Brandwaffen (nicht zu verwechseln mit Feuerwaffen) kann Feuer im Kampf auch zur gezielten Schädigung eines Gegners genutzt werden.





</doc>
<doc id="8893" url="https://de.wikipedia.org/wiki?curid=8893" title="Ludwig van Beethoven">
Ludwig van Beethoven

Ludwig van Beethoven (getauft 17. Dezember 1770 in Bonn, Kurköln; † 26. März 1827 in Wien, Kaisertum Österreich) war ein deutscher Komponist und Pianist. Er führte die Wiener Klassik zu ihrer höchsten Entwicklung und bereitete der Musik der Romantik den Weg.

Zu Beginn seiner musikalischen Karriere machte sich Beethoven zunächst als Klaviervirtuose einen Namen. Zu seinen Stärken gehörte das freie Improvisieren und Fantasieren auf dem Instrument. Nach dem Umzug von Bonn nach Wien führte ihn sein Talent bald in die höchsten gesellschaftlichen Kreise der habsburgischen Metropole. Ein Gehörleiden, das sich im Laufe der Zeit zur völligen Taubheit verschlimmerte, setzte seiner Karriere als Pianist jedoch ein vorzeitiges Ende. Die Krankheit löste eine Krise aus, über die Beethoven 1802 in seinem Heiligenstädter Testament Zeugnis ablegte. Beethoven pflegte vielerlei Kontakte zu Frauen aus seinem Umfeld. Berühmt ist sein 1812 geschriebener Brief an die unsterbliche Geliebte, deren Identität bis heute nicht zweifelsfrei geklärt ist. Eine Kandidatin ist Josephine Brunsvik, deren Tochter Minona möglicherweise aus der Affäre mit Beethoven hervorging.

Mit der Verschlechterung seines Gehörs konzentrierte sich Beethoven mehr und mehr aufs Komponieren. Während andere Komponisten ihre Werke oft schnell zu Papier brachten, rang Beethoven um jede Note. Immer wieder wurde nachgearbeitet und verbessert. Aus seinem umfangreichen konzertanten Werk stechen insbesondere seine neun Sinfonien hervor. Daneben schuf er u. a. fünf Klavierkonzerte, ein Violinkonzert, mehrere Ouvertüren, die Oper Fidelio, die Missa solemnis sowie eine Vielzahl kammermusikalischer Werke. Beethoven ist seinem Anspruch, ein bleibendes musikalisches Werk für die Nachwelt zu hinterlassen, gerecht geworden. Seine Popularität ist ungebrochen, und heute gehört er zu den meistgespielten Komponisten der Welt.

Ludwig van Beethovens väterliche Vorfahren stammten aus Mechelen im flämischen Brabant. Mit seinem Großvater Ludwig van Beethoven (1712–1773) ging erstmals ein Musiker aus der Familie hervor. Dieser wurde 1733 als Basssänger an den Kurfürstlichen Hof nach Bonn berufen, 1761 ernannte ihn der neue Kurfürst Maximilian Friedrich zum kurkölnischen Hofkapellmeister. Dessen Sohn Johann (* 14. November 1740, † 18. Dezember 1792) wurde Tenorsänger an der Hofkapelle und erwarb sich darüber hinaus Ansehen als Musiklehrer. Am 12. November 1767 heiratete Johann die früh verwitwete Maria Magdalena Leym geb. Keverich (* 19. Dezember 1746). Aus der Ehe gingen sieben Kinder hervor, von denen nur drei das Säuglingsalter überlebten: Ludwig, Kaspar Karl (getauft 8. April 1774) und Nikolaus Johann (getauft 2. Oktober 1776). Die Geburt eines Bruders gleichen Namens Anfang April 1769 trug später zu Ludwig van Beethovens Verunsicherung über sein tatsächliches Alter bei.

Als zweites Kind wurde Ludwig van Beethoven am 17. Dezember 1770 in der damaligen St.-Remigius-Kirche getauft; die Kirche brannte 1800 ab und ist nicht mit der heutigen St.-Remigius-Kirche an anderer Stelle zu verwechseln. Geboren wurde er wahrscheinlich am 16. Dezember in der Wohnung der Familie im Haus Nr. 515 (heute Haus Nr. 20) in der Bonngasse.

Obwohl Beethoven erst drei Jahre alt war, als sein Großvater am 24. Dezember 1773 starb, verklärte er diesen zur Identifikationsfigur der Familie.

Johann van Beethoven erkannte früh die außerordentliche Begabung seines Sohnes und sorgte für eine solide Musikausbildung, an der auch Kollegen der Hofkapelle mitwirkten: der Hoforganist Gilles van den Eeden, der Sänger Tobias Pfeifer, die Violinisten Franz Georg Rovantini und Franz Ries und andere. Über Johann van Beethovens Unterricht sind gewalttätige Übergriffe auf seinen Sohn überliefert. Ob diese Berichte regelmäßige oder vereinzelte Vorfälle schildern, ist unklar. Im Alter von sieben Jahren trat Beethoven zum ersten Mal öffentlich als Pianist auf.

1782 trat der Komponist und Kapellmeister Christian Gottlob Neefe die Nachfolge van den Eedens als Hoforganist an. Neefe erteilte Beethoven zeitweise Klavier- und Kompositionsunterricht und vermittelte die Veröffentlichung erster Klavierkompositionen: der "Variationen über einen Marsch von Dressler" WoO 63 und der sogenannten Kurfürstensonaten WoO 47. Ob ihm allerdings die herausragende Rolle als Lehrer Beethovens zukommt, die ihm in der Literatur zugeschrieben wurde, ist zweifelhaft. 1782 wurde Beethoven Stellvertreter Neefes an der Orgel, zwei Jahre später erhielt er eine feste Anstellung als Organist. Darüber hinaus wirkte er als Cembalist und Bratschist in der Hofkapelle. Im Zusammenhang mit einer Neuorganisation der Hofmusik durch Maximilian Franz, den Nachfolger des verstorbenen Kurfürsten Max Friedrich, kam es 1784 zum Bruch zwischen Beethoven und Neefe.

Beethovens Schulbildung ging über Grundlegendes wie Lesen, Schreiben und Rechnen kaum hinaus. Zusätzlich erhielt er aber zeitweise Privatunterricht in Latein, Französisch und Italienisch. Geistige Anregung erhielt er darüber hinaus von Freunden aus Bonner Bürgerkreisen, besonders von dem Medizinstudenten und späteren Arzt Franz Gerhard Wegeler sowie von der Familie von Breuning, zu der Beethoven eine geradezu familiäre Beziehung pflegte. Die Freundschaft zu Wegeler und zu Stephan von Breuning dauerte trotz gelegentlicher Krisen lebenslang.

Am Hof des Kurfürsten Maximilian Franz herrschte ein liberales Klima. Aufklärerisches Gedankengut wurde u. a. in den Kreisen des Illuminatenordens, einer den Freimaurern nahestehenden Geheimgesellschaft, gepflegt. Zahlreiche Hofmusiker waren Mitglied der Bonner Loge, Neefe stand ihr vor. Nach dem Verbot der Illuminaten 1785 sammelten sich ihre Bonner Mitglieder in der 1787 gegründeten Lese- und Erholungsgesellschaft. Durch den an der Bonner Universität lehrenden Eulogius Schneider kam Beethoven auch früh mit den Ideen der Französischen Revolution in Berührung.

1784 schrieb Neefe über Beethoven, er werde „gewiß ein zweyter Wolfgang Amadeus Mozart werden, wenn er so fortschritte, wie er angefangen“. Bei Maximilian Franz, Kurfürst seit 1784 und erklärter Liebhaber der Musik Mozarts, traf diese Einschätzung auf fruchtbaren Boden. Ende Dezember 1786 brach Beethoven zu einer von Max Franz geförderten Reise nach Wien auf, um Kompositionsschüler Mozarts zu werden. Als Beethoven nach etwa dreimonatigem Aufenthalt die Rückreise antrat, unterbrach er diese mehrfach, um in Regensburg, München und Augsburg Station zu machen. Im Mai 1787 kehrte er nach Bonn zurück. Es ist nicht bekannt, ob es überhaupt zu einer Begegnung mit Mozart kam; für einen Unterricht durch das Vorbild fehlt jeder Beleg, und der Kurfürst zeigte sich von den Ergebnissen der Reise entsprechend enttäuscht. Der Grund für das Scheitern des Plans ist unklar.

Zurück in Bonn traf Beethoven auf eine dramatisch veränderte familiäre Situation. Der Gesundheitszustand der an der „Schwindsucht“ erkrankten Mutter hatte sich in kritischer Weise verschlechtert. Sie verstarb am 17. Juli 1787. Der Vater verlor zunehmend die Kontrolle über seinen ohnehin hohen Alkoholkonsum, sodass er schließlich nicht mehr in der Lage war, für seine drei Söhne zu sorgen. 1789 wurde er vom Dienst suspendiert, und Ludwig als ältestem Sohn wurde die Verfügungsgewalt über die Hälfte der Pension des Vaters erteilt, wodurch ihm faktisch die Rolle des Familienoberhauptes zufiel.

Mitte September 1791 kam Beethoven als Organist und Bratschist der Bonner Hofkapelle zu einem Generalkapitel des Deutschen Ordens nach Mergentheim. Die Reise der kurkölnischen Hofkapelle ging auf zwei Schiffen vonstatten, die über Rhein und Main bis Miltenberg fuhren. Beethoven, von seinen Freunden seines bräunlichen Teints und der schwarzen Augen wegen "Spagnol" genannt, agierte hierbei als Küchenjunge. Von Miltenberg ging es mit der Kutsche weiter nach Mergentheim, wo sich Beethoven bis Ende Oktober 1791 aufhielt.

Durch seinen Eintritt in den Deutschen Orden kam der aus Wien stammende Graf Ferdinand Ernst von Waldstein nach Bonn. Er wurde Beethovens erster adeliger Förderer, regte ihn zu Kompositionen an, so zur "Musik zu einem Ritterballett" WoO 1 und zu den "Variationen über ein Thema von Graf Waldstein" WoO 67, und nutzte seinen Einfluss auf den Kurfürsten, um ihn zur Fortsetzung der Förderung Beethovens zu bewegen.

Als im Juli 1792 Joseph Haydn auf dem Rückweg einer Englandreise in Bonn Station machte, wurde ein zweiter Studienaufenthalt Beethovens in Wien vereinbart. Nachdem Mozart bereits verstorben war, sollte er nun – nach einem Stammbucheintrag Waldsteins – „Mozart’s Geist aus Haydens Händen“ erhalten. Noch im November desselben Jahres brach Beethoven nach Wien auf.

Eine Folge von Ereignissen bewirkte, dass aus Beethovens Studienreise nach Wien ein dauerhafter und endgültiger Aufenthalt wurde. Kurz nach Beethovens Ankunft, am 18. Dezember 1792, starb sein Vater. 1794 besetzten französische Truppen das Rheinland, und der kurfürstliche Hof musste fliehen. Damit war Beethoven nicht nur der Boden für die Rückkehr nach Bonn entzogen, auch die Gehaltszahlungen des Kurfürsten blieben nun aus. Im Frühjahr 1794 schließlich übersiedelte sein Bruder Kaspar Karl nach Wien, im Dezember 1795 folgte auch Bruder Johann.

In Wien fand Beethoven bald die Unterstützung adeliger Musikliebhaber, die ihm halfen, in der neuen Heimat Fuß zu fassen, darunter Fürst Franz Joseph Maximilian von Lobkowitz und Gottfried Freiherr van Swieten. Eine besondere Rolle spielte Fürst Karl Lichnowsky; in seinem Haus knüpfte Beethoven Kontakte zu Wiener Musikerkreisen und lernte den Geiger Ignaz Schuppanzigh kennen, der als Interpret wesentlich zur Verbreitung seiner Werke beitragen sollte. Lichnowsky stellte Beethoven zeitweise eine Wohnung in seinem Haus zur Verfügung.

Ab 1800 zahlte Lichnowsky an Beethoven ein jährliches Gehalt in Höhe von 600 Gulden und schuf damit für die folgenden Jahre die Grundlage für eine unabhängige künstlerische Existenz.

Wie vereinbart nahm Beethoven bei Haydn Kompositionsunterricht, der von Beethovens Ankunft in Wien (November 1792) bis kurz vor Haydns Abreise nach England (19. Januar 1794) gedauert hat. Das Verhältnis zwischen dem renommierten Lehrer und dem eigenwilligen, selbstbewussten Schüler blieb nicht frei von Meinungsverschiedenheiten und Konflikten; so, als Haydn Bedenken gegen Beethovens Klaviertrio op. 1 Nr. 3 äußerte, da er es für zu schwer verständlich hielt. Auch wenn Beethoven einmal über seinen Lehrer gesagt haben soll, „nie etwas von ihm gelernt“ zu haben, so prägten Haydns Werke doch Beethovens Entwicklung als Komponist nachhaltig, vor allem auf den Gebieten der Sinfonie und der Kammermusik.

Allerdings scheint Beethoven mit Haydn als Lehrer unzufrieden gewesen zu sein. Heimlich nahm er Unterricht bei Johann Baptist Schenk. Ab 1794 studierte er Kontrapunkt bei Johann Georg Albrechtsberger, und von Antonio Salieri ließ er sich in der Gesangskomposition unterweisen.

Beethovens Erfolge als Komponist hängen anfangs eng mit seiner Karriere als Klaviervirtuose zusammen. In den ersten zehn Jahren in Wien entstanden allein 20 seiner 32 Klaviersonaten, darunter die "Grande Sonate pathétique" op. 13 in c-Moll und die beiden Sonaten op. 27, deren zweite unter der (nicht von Beethoven stammenden) Bezeichnung „Mondscheinsonate“ bekannt wurde; der Titelzusatz „quasi una fantasia“ deutet an, dass die Improvisation am Klavier eine wichtige Inspirationsquelle für den Komponisten war.

Am 29. März 1795 trat Beethoven mit seinem Klavierkonzert B-Dur op. 19 erstmals als Pianist an die Wiener Öffentlichkeit. Besonderes Aufsehen erregte er auch durch seine herausragende Fähigkeit zum freien Fantasieren. 1798 unternahm der junge Virtuose eine Konzertreise unter anderem nach Prag, Dresden und Berlin, die ein großer künstlerischer und finanzieller Erfolg wurde. Die von Lichnowsky initiierte Tournee folgte der Route der Reise, die der Fürst 1789 mit Mozart unternommen hatte.

Die ersten Kompositionen, die Beethoven drucken ließ, waren die drei 1794/95 entstandenen Klaviertrios, die er mit der Opusnummer 1 versah. In den folgenden Jahren setzte sich Beethoven mit zwei weiteren zentralen Gattungen der Klassik auseinander: dem Streichquartett und der Sinfonie. Zwischen 1798 und 1800 komponierte er, nach intensivem Studium der Quartette Haydns und Mozarts, eine erste Serie von sechs Quartetten, die er als op. 18 dem Fürsten Lobkowitz widmete. Kurz darauf, 1800 und 1802, präsentierte sich Beethoven als Sinfoniker. Die Widmung der 1. Sinfonie op. 21 in C-Dur ging an van Swieten, die der 2. Sinfonie op. 36 in D-Dur an den Fürsten Lichnowsky.

Beethovens wachsender Erfolg als Pianist und Komponist wurde von einer schwerwiegenden Beeinträchtigung überschattet: Etwa um 1798 zeigten sich erste Symptome jenes Gehörleidens, das schließlich zur Taubheit führen sollte. Nach Beethovens eigenem Bericht aus dem Jahr 1801 verschlimmerte sich das Leiden innerhalb weniger Jahre; es scheint jedoch in den Folgejahren einige Zeit stagniert zu haben.

Die Ursache der Erkrankung ist Gegenstand zahlreicher Untersuchungen. Zu den möglichen Ursachen zählen eine Atrophie der Gehörnerven oder eine Otosklerose.

Beethovens Gehörleiden stellte nicht nur eine ernste Bedrohung seiner Laufbahn als Musiker dar; es beeinträchtigte auch seinen gesellschaftlichen Umgang. Die Krankheit stürzte Beethoven in eine schwere persönliche Krise, die ihn zeitweilig sogar an Selbstmord denken ließ. Beethoven offenbarte seinen Seelenzustand im sogenannten Heiligenstädter Testament, einem Schriftstück, das er im Oktober 1802 am Ende einer Kur in Heiligenstadt verfasste, nachdem auch diese ohne den erhofften Erfolg geblieben war.

Die mittleren Wiener Jahre, vom Beethoven-Biographen Maynard Solomon als die „heroische Periode“ bezeichnet, sind, der Beeinträchtigung durch das Gehörleiden zum Trotz, die produktivste Phase in Beethovens Schaffensbiographie. Beethoven hatte zu dieser Zeit einen eigenen unverwechselbaren Stil entwickelt.

Sechs der neun Sinfonien komponierte Beethoven allein zwischen Herbst 1802 und 1812, darunter so bekannte Werke wie die 3. Sinfonie "Eroica", die 5. Sinfonie und die 6. Sinfonie "Pastorale". Darüber hinaus entstanden das 4. und 5. Klavierkonzert sowie die Endfassung des 3. Klavierkonzerts, das Violinkonzert op. 61 und die fünf „mittleren“ Streichquartette op. 59 Nr. 1–3, op. 74 und op. 95.

Auch die erste Fassung seiner einzigen Oper "Fidelio" komponierte Beethoven in dieser Zeit. Am 20. November 1805 wurde sie unter dem ursprünglichen Titel "Leonore" zum ersten Mal aufgeführt, in der Folge aber noch zweimal überarbeitet.

Seinen bis dahin größten Erfolg erzielte Beethoven 1813/1814 mit den Aufführungen eines eigens aus Anlass des Wiener Kongresses komponierten Werkes, "Wellingtons Sieg oder Die Schlacht bei Vittoria" op. 91, das den entscheidenden Sieg der Engländer über die napoleonischen Truppen schildert und damit beim Publikum den Geist der Zeit traf.

Die Französische Revolution und Napoleon Bonaparte hatten auf Beethoven einen starken Eindruck gemacht und wirkten sich nachweislich auf sein Schaffen aus.

So hatte Beethoven die 3. Sinfonie "Eroica" ursprünglich mit dem Zusatz „intitulata Bonaparte“ oder „geschrieben auf Bonaparte“ versehen wollen. Eine Anekdote berichtet, Beethoven habe den Titelzusatz wütend entfernt, nachdem Napoléon im Dezember 1804 sich selbst zum Kaiser gekrönt hatte. Wahrscheinlich hängt die Änderung des ursprünglichen Titels eher mit einer geplanten, aber letztlich nicht durchgeführten Reise nach Paris zusammen.

Auf eine französische Revolutionsoper, "Léonore ou L’amour conjugal" (Leonore oder Die eheliche Liebe) von Jean Nicolas Bouilly, geht der Stoff zurück, den Beethoven in seiner Oper "Fidelio" verarbeitete, und in seiner 5. Sinfonie in c-Moll op. 67 griff er Elemente der sogenannten Revolutionsmusik auf; ein Stil, den französischen Komponisten wie André-Ernest-Modeste Grétry, Étienne-Nicolas Méhul und Luigi Cherubini Ende des 18. Jahrhunderts geprägt hatten.

Im Verhältnis zwischen Beethoven und seinem bis dahin wichtigsten Mäzen, dem Fürsten Lichnowsky, kam es im Laufe der Jahre zu einer zunehmenden Entfremdung. Die Spannungen eskalierten im Herbst 1806 bei einem Aufenthalt Beethovens auf Schloss Grätz (tschechisch "Hradec") bei Troppau (tschechisch "Opava"), dem Sitz des Fürsten, in einer ernsten Auseinandersetzung. Etwa zur gleichen Zeit, 1806 oder 1807, stellte Lichnowsky, der in jenen Jahren außerordentlich hohe finanzielle Verpflichtungen zu erfüllen hatte, die jährlichen Gehaltszahlungen an den Komponisten ein.

Zwar bezog Beethoven neben dem fürstlichen Gehalt nicht unerhebliche Einkünfte aus Verlagsverträgen und Konzerteinnahmen, doch garantierten diese keine dauerhafte finanzielle Absicherung. Daher bewarb sich Beethoven im Dezember 1807 – vergeblich – bei der k.k. Hoftheaterdirektion um eine Anstellung und erwog darüber hinaus, Wien zu verlassen. Eine entsprechende Gelegenheit bot sich, als ihn Friedrich Ludwig III. Graf Truchsess zu Waldburg im November 1808 als Kapellmeister an den Hof Jérôme Bonapartes nach Kassel berief.

Durch eine Initiative Ignaz von Gleichensteins und der Gräfin Marie Erdődy, die zu Beethovens engstem Freundeskreis gehörten, gelang es, Beethoven in Wien zu halten. Am 1. März 1809 sicherten Erzherzog Rudolph, Franz Joseph Fürst Lobkowitz und Ferdinand Fürst Kinsky dem Komponisten per Dekret ein festes jährliches Gehalt zu unter der einzigen Bedingung, dass Beethoven in Wien wohnen bliebe (der sogenannte Rentenvertrag). Die Hoffnung Beethovens auf finanzielle Unabhängigkeit erhielt jedoch nach kurzer Zeit gleich mehrere Rückschläge: die Geldentwertung durch das sogenannte Finanzpatent im Frühjahr 1811, der Tod des Fürsten Kinsky im folgenden Jahr und der drohende Bankrott des Fürsten Lobkowitz 1813. Dadurch war Beethoven gezwungen, die Fortsetzung der Zahlungen gerichtlich einzuklagen.

Beethovens Wertschätzung Johann Wolfgang von Goethes begann sich seit den 1790er Jahren vor allem in seinen Liedkompositionen niederzuschlagen. 1809/1810 kumulierte die kompositorische Beschäftigung mit dem Dichter in den Liederzyklen op. 75 und op. 83 sowie der Schauspielmusik zu "Egmont" op. 84.

Während ihres Wienaufenthalts im Frühjahr-Sommer 1810 lernte Beethoven Ende Mai Bettina Brentano kennen, die Schwester des Dichters Clemens Brentano. Sie gewann sein Vertrauen und nutzte ihre Freundschaft zu Goethe, ein Treffen der beiden Künstlerpersönlichkeiten anzuregen. Durch die literarisch stark überformten Darstellungen ihrer Beziehung zu Beethoven hat Bettina Brentano später das romantische Beethoven-Bild maßgeblich mit geprägt.

Zur lange angebahnten Zusammenkunft zwischen Beethoven und Goethe kam es im Juli 1812 (19., 20., 21. und 23.), als sich beide im böhmischen Kurbad Teplitz aufhielten. Das Ergebnis war eher durchwachsen: Am 19. Juli schrieb Goethe an seine Frau: „Zusammengefaßter, energischer, inniger habe ich noch keinen Künstler gesehen. Ich begreife recht gut, wie er gegen die Welt wunderlich stehen muß.“ Und am 12. September 1812 schrieb Goethe aus Karlsbad an seinen Freund Carl Friedrich Zelter, den Leiter der Berliner Sing-Akademie: „Beethoven habe ich in Teplitz kennengelernt. Sein Talent hat mich in Erstaunen gesetzt; allein er ist leider eine ganz ungebändigte Persönlichkeit, die zwar gar nicht unrecht hat, wenn sie die Welt detestabel findet, aber sie freilich dadurch weder für sich noch für andre genußreicher macht. Sehr zu entschuldigen ist er hingegen und sehr zu bedauern, da ihn sein Gehör verläßt, was vielleicht dem musikalischen Teil seines Wesens weniger als dem geselligen schadet. Er, der ohnehin lakonischer Natur ist, wird es nun doppelt durch diesen Mangel.“ Dagegen schrieb Beethoven lakonisch am 9. August von Franzensbad an seinen Verleger Härtel: „Göthe behagt die Hofluft sehr, mehr als einem Dichter ziemt. Es ist nicht vielmehr über die Lächerlichkeiten der Virtuosen hier zu reden, wenn Dichter, die als die ersten Lehrer der Nation angesehen sein sollten, über diesem Schimmer alles andere vergessen können.“

Zwar war Beethoven nach einer Aussage von Franz Gerhard Wegeler „sehr häufig verliebt“, doch bezog sich dies nur auf eine kurze Periode in den 1790er Jahren, als Wegeler in Wien war. Frauen spielten jedoch in vieler Hinsicht eine große Rolle in Beethovens Leben: als Freundinnen und Vertraute, als Interpretinnen oder als Widmungsempfängerinnen.

Beethovens erste große Liebe galt Johanna von Honrath. Sein Jugendfreund Franz Gerhard Wegeler schreibt über die Bonner Jugendzeit des Komponisten: „Seine und Stephan von Breuning’s erste Liebe war Fräulein Jeanette d’Honrath aus Köln, Neumarkt Nro. 19. (jetziges Wohnhaus des Baumeisters Herrn Biercher), die oft einige Wochen in der von Breuning’schen Familie in Bonn zubrachte. Sie war eine schöne, lebhafte Blondine, von gefälliger Bildung und freundlicher Gesinnung, welche viele Freude an der Musik und eine angenehme Stimme hatte.“

Maria Anna Wilhelmine von und zu Westerholt-Gysenberg war wohl ebenfalls eine Jugendliebe Beethovens. Er selbst spricht sie in seinem Brief als „ma très cher amie“ (deutsch „meine sehr liebe Freundin“') an, was eher für eine innige Freundschaft als für Liebe spricht. Sein Jugendfreund Franz Gerhard Wegeler schreibt dagegen einige Jahre später über Beethovens erste Liebe von einem Fräulein v. W. – wobei er unbekannt lässt, welche Person er damit meinte:

Zu Beethovens längsten und treuesten Freundinnen gehörte die Gräfin Marie von Erdődy (1779–1837). Mehrere seiner Werke sind ihr gewidmet. Sie besaß außer ihrer Stadtwohnung ein Haus in Jedlesee, in dem Beethoven 1815 gewohnt haben soll. In der älteren Literatur wird verschiedentlich vermutet, dass es eine kurzzeitige Liebesbeziehung zwischen Beethoven und der Gräfin gab, was jedoch reine Spekulation ist.

Besonders freundschaftlich wurde Beethoven von der ungarischen Familie Brunsvik aufgenommen. Die Schwestern Therese, Josephine, Charlotte und ihr Bruder Franz begegneten dem Komponisten erstmals 1799. Als die Familie nach 18 Tagen wieder in ihre ungarische Heimat zurückkehrte, schrieb Beethoven in Josephines und Thereses Album einen Auszug von Goethes Gedicht „Ich denke dein“.

Auf Drängen der Mutter, die ihre schöne Tochter mit einem wohlhabenden Adligen vermählen wollte, heiratete Josephine Brunsvik noch im Sommer 1799 den Grafen Joseph von Deym, zog zu ihm nach Wien und gebar in ihrer kurzen Ehe vier Kinder. In dieser Zeit war Beethoven Josephines Klavierlehrer und regelmäßiger „standhafter Besucher der jungen Gräfin“. Nachdem Graf Deym Anfang 1804 unerwartet gestorben war, entwickelte sich zwischen Beethoven und Josephine eine Liebesbeziehung. Zwischen 1804 und 1809 schrieb Beethoven Josephine mindestens vierzehn teils leidenschaftliche Liebesbriefe, in denen er sie unter anderem als „Engel“, „mein Alles“ und als seine „einzig Geliebte“ bezeichnete und ihr „ewige Treue“ schwor. Der Briefwechsel, soweit erhalten, dokumentiert aber auch die seelischen Konflikte des Paares, die aus dem Widerspruch zwischen ihren persönlichen Gefühlen und den Zwängen der Realität resultierten: Josephine hatte vier Kinder zu versorgen, und im Falle einer Heirat mit dem nichtadligen Beethoven hätte sie die Vormundschaft für sie verloren. Im Herbst 1807 zog sich Josephine schließlich auf Druck ihrer Familie von Beethoven zurück. Bereits 1805 hatte Therese voller Besorgnis an Charlotte geschrieben: „Aber sage mir, Pepi und Beethoven, was soll daraus werden? Sie soll auf ihrer Hut sein! … Ihr Herz muß die Kraft haben nein zu sagen, eine traurige Pflicht, wenn nicht die traurigste aller.“

1810 ging Josephine eine zweite Ehe mit dem estnischen Baron Christoph von Stackelberg ein, die sich für sie äußerst unglücklich entwickelte. Ende Juni/Anfang Juli 1812 verließ Stackelberg sie. In ihrem Tagebucheintrag vom 8. Juni 1812 heißt es: „Ich habe heute einen schweren Tag. – Die Hand des Schicksals ruht düster auf mir – Ich sah nebst meinem tiefen Kummer auch noch die Entartung meiner Kinder und – fast – aller Muth wich von mir –!!!“ Kurz darauf notierte sie in ihrem Tagebuch, dass sie beabsichtigte, nach Prag zu reisen: „St. will daß ich mir selbst sitzen soll [er hat mich sitzen gelassen]. er ist gefühllos für bittende in der Noth. [...] Ich will Liebert in Prague [!] sprechen. ich will die Kinder nie von mir lassen. [...] Ich habe Stackb zu liebe [mich] physisch zugrunde gerichtet indem ich [...] noch so viele Kummer und Krankheit durch ihn zugezogen habe.“ In Prag traf Beethoven am 3. Juli seine „Unsterbliche Geliebte“.

1817 notierte Therese, die weiterhin mit Beethoven in Verbindung blieb, in ihrem Tagebuch über ihre kranke Schwester: „Ob Josephine nicht Strafe leidet wegen Luigi’s Weh? Seine Gattin – was hätte sie nicht aus dem Heros gemacht!“ Eine Tagebuchnotiz Thereses von 1848 lautet: „Ich Glückliche hatte Beethovens intimen, geistigen Umgang so viele Jahre! Josephinens Haus- und Herzensfreund! Sie waren füreinander geboren und lebten beide noch, hätten sie sich vereint.“

Zahlreiche Beethoven-Forschende, vor allem im deutschsprachigen Raum, darunter La Mara, Siegmund Kaznelson, Harry Goldschmidt, Brigitte und Jean Massin, Marie-Elisabeth Tellenbach, Carl Dahlhaus und Rita Steblin, halten Josephine für Beethovens „Unsterbliche Geliebte“.

Über die Schwestern Brunsvik lernte Beethoven um 1801/02 auch deren Cousine Gräfin Giulietta Guicciardi (1782–1856) kennen und verliebte sich kurzzeitig in sie. Er war sich jedoch darüber im klaren, dass eine Heirat wegen des Standesunterschiedes nicht in Frage kam. Außerdem war sie bereits mit dem Grafen Wenzel von Gallenberg verlobt, den sie 1803 heiratete. Beethoven widmete ihr 1802 die als „Mondscheinsonate“ bekannte "Sonata quasi una Fantasia", op. 27 Nr. 2.

Eine weitere mutmaßliche Geliebte Beethovens war Therese von Zandt, die zur Zeit ihrer siebenmonatigen Liaison mit dem Komponisten Stiftsdame im freiweltlichen Damenstift Asbeck war. Therese von Zandt veröffentlichte ab 1798 unter dem Kürzel "Z." als erste Frau Beiträge in der Leipziger "Allgemeinen musikalischen Zeitung". Möglicherweise war sie es, die Beethoven den Stoff zu seiner einzigen Oper Fidelio empfahl, als sie im Auftrag der Zeitung vom 5. Dezember 1803 bis zum 5. Juli 1804 nach Wien reiste. Jean-Nicolas Bouillys Libretto zur Oper "Léonore ou L'amour conjugaldes Fidelio", auf dem der Fidelio-Stoff basiert, wurde damals von Friedrich Rochlitz, Begründer und Redakteur der "Allgemeinen musikalischen Zeitung", erstmals aus dem Französischen ins Deutsche übersetzt. Ein Porträt aus Beethovens Besitz, von dem man 200 Jahre lang annahm, es zeige Giulietta Guicciardi, ist nach Forschungen von Klaus Martin Kopitz vermutlich Therese von Zandt zuzuschreiben.

Mit der französischen Pianistin Marie Bigot verkehrte Beethoven insbesondere im Jahre 1807. Die Freundschaft kam wahrscheinlich zustande, da Maries Gatte Paul Bigot bei Beethovens Gönner, dem Grafen Andrej Rasumowsky, als Bibliothekar tätig war. Von der Beziehung zeugen mehrere Briefe Beethovens. Er schenkte Marie auch das Autograph seiner berühmten "Appassionata", das sich heute in der Bibliothèque nationale de France in Paris befindet. Anfang März 1807 lud Beethoven Marie zu einer Spazierfahrt ein, als ihr Mann abwesend war. Dessen offensichtlich eifersüchtige Reaktion veranlasste Beethoven, kurz darauf einen Entschuldigungsbrief an das Ehepaar zu schreiben, in dem er betonte: „[...] ohnedem ist es einer meiner ersten Grundsätze nie in einem andern als Freundschaftlichen Verhältniß mit der Gattin eines andern zu stehen.“

Etwa im Frühjahr 1808 begegnete Beethoven erstmals der damals 15-jährigen Sängerin Elisabeth Röckel, der Schwester des Tenors Joseph August Röckel, der in den "Fidelio"-Aufführungen von 1806 die Partie des Florestan übernommen hatte. Zusammen mit ihrem Bruder wohnte sie in einer Dienstwohnung des Theaters an der Wien, wo sie als „Elis. [!] Rökel“ verzeichnet wurde und sich mit der gleichfalls dort lebenden Sängerin Anna Milder-Hauptmann befreundete, von der sie in einem Brief tatsächlich mit „Elise“ [!] angeredet wurde. Nach einer zweifelhaften Aussage von Anton Schindler gegenüber Gerhard von Breuning wollte Beethoven sie heiraten. Sie selbst hat das später bestritten, berichtete aber mehrfach, dass Beethoven ihr sehr zugetan war. In ihrem offiziellen Nekrolog heißt es: „Zu ihren Verehrern gehörte auch Beethoven.“ 2010 stellte Klaus Martin Kopitz die These auf, Beethoven habe für sie am 27. April 1810 sein berühmtes Albumblatt „Für Elise“ komponiert. 2015 veröffentlichte er weitere, bislang unbekannte Quellen über ihre Beziehung zu Beethoven. Das Autograph des Albumblattes war später im Besitz von Therese Malfatti, aus deren Nachlass es nach München zu der Lehrerin Babette Bredl gelangte, der Mutter von Malfattis Hausfreund und Erbe Rudolph Schachner. Bredl lieh das Autograph Ludwig Nohl, der es abschrieb und publizierte. Obwohl er ausdrücklich erklärte, es sei „nicht für Therese geschrieben“, sind mehrere Beethoven-Forscher der Ansicht, sie sei dennoch als Widmungsempfängerin anzusehen.

Elisabeth Röckel heiratete 1813 Johann Nepomuk Hummel und zog mit ihm nach Weimar, kam aber im März 1827 noch einmal nach Wien. Auf Wunsch des sterbenden Beethoven besuchte sie diesen mehrfach und erhielt zum Andenken eine Locke des Komponisten und dessen letzte Schreibfeder. Die Reliquien sind seit 2012 im Besitz des Beethoven Center der San José State University. Kurz nach dem Tode des Komponisten gestand sie Schindler, „welch’ tiefe Wurzeln ihre einstige Liebe zu Beeth. geschlagen u noch immer in ihr lebe.“

Der Musikwissenschaftler Michael Lorenz bezweifelte 2011, dass Elisabeth sich „Elise“ nannte, da dies nur durch wenige Quellen zu belegen ist. Wie er aber gleichfalls bemerkt, „wurde im Wien des Vormärz zwischen den Namen Elisabeth und Elise nicht mehr unterschieden, sie waren austauschbar und quasi identisch“.

Eine weitere Frau in Beethovens Leben war Therese Malfatti. Beethoven lernte sie 1809 durch seinen Freund Ignaz von Gleichenstein kennen, der 1811 Thereses Schwester Anna heiratete. Im Frühjahr 1810, angesichts Josephine Brunsviks Wiederverheiratung, plante Beethoven offenbar, Therese Malfatti einen Heiratsantrag zu machen, und ließ sich dafür von seinem Freund Franz Gerhard Wegeler in Bonn eine Abschrift seines Taufscheins besorgen. Als dann Therese von Malfatti seinen Antrag aber abwies – ihre Familie war aus Standesrücksichten ebenfalls dagegen –, überwand Beethoven diese Ablehnung vergleichsweise leicht. Therese blieb danach freundschaftlich mit ihm verbunden.

Ende Mai 1810 lernte Beethoven durch Bettina Brentano deren Schwägerin Antonie Brentano kennen, die von 1809 bis 1812 in Wien lebte, um den umfangreichen Nachlass ihres verstorbenen Vaters Johann Melchior Edler von Birkenstock zu verkaufen. Sie schrieb im März 1811 in einem Brief an Bettina, Beethoven sei ihr „einer der liebsten Menschen“ geworden und besuche sie „beinahe täglich“. Zwischen dem Ehepaar Franz und Antonie Brentano und Beethoven entwickelte sich eine freundschaftliche Beziehung, die Antonie in ihrem Tagebuch als „Wahlverwandtschaft“ bezeichnete. Sie besaß auch das Autograph von Beethovens Lied "An die Geliebte", WoO 140, auf dem von ihrer Hand vermerkt ist: „den 2n März 1812 mir vom Author erbethen“. Dieses Lied hatte Beethoven kurz zuvor der bayerischen Sängerin Regina Lang ins Stammbuch geschrieben. 1972 stellte Maynard Solomon die Hypothese auf, Beethovens Brief an die Unsterbliche Geliebte vom 6./7. Juli 1812 sei an Antonie Brentano gerichtet. Antonie reiste mit ihrer Familie von Prag nach Karlsbad weiter. Obwohl sich nicht beweisen lässt, dass Beethoven in Prag mit Antonie zusammentraf, und die „Unsterbliche Geliebte“ möglicherweise doch nicht nach „K“ ging, haben sich mittlerweile zahlreiche Beethoven-Forscher dieser Hypothese angeschlossen, darunter Yayoi Aoki, Barry Cooper, William Kinderman, Klaus Martin Kopitz, Lewis Lockwood und Susan Lund.

Beethovens Brief an die Unsterbliche Geliebte, den er am 6./7. Juli 1812 in Teplitz während einer Reise in die böhmischen Kurbäder verfasste, ist neben dem Heiligenstädter Testament das bedeutendste Selbstzeugnis des Komponisten. Er richtet sich an eine namentlich nicht genannte Frau, mit der es kurz zuvor, am 3. Juli in Prag, zu einer für die Zukunft der Beziehung entscheidenden Begegnung gekommen war. Aus dem Brief geht unter anderem die gegenseitig eingestandene Liebe hervor und die Hoffnung auf eine dauerhafte Verbindung der Liebenden, der aber offenbar große Hindernisse entgegenstehen. Die Identität der „Unsterblichen Geliebten“ ist unter Beethoven-Forschern umstritten.

Vom Jahr 1812 an begann sich Beethovens Lebenssituation deutlich zum Schlechteren zu verändern. Zu den schicksalhaften Ereignissen um die „Unsterbliche Geliebte“ kamen materielle Sorgen im Zusammenhang mit dem Rentenvertrag und eine Verschlimmerung des Gehörleidens bis hin zur völligen Taubheit. Von etwa 1813 an verwendete Beethoven Hörrohre, um mit seiner Umgebung zu kommunizieren, ab 1818 ist der Gebrauch sogenannter Konversationshefte nachzuweisen, worin die Gesprächspartner ihre Äußerungen notierten. Aufgrund seiner fortgeschrittenen Schwerhörigkeit war es ihm nicht mehr möglich, als Pianist aufzutreten.

Am 15. November 1815 starb Beethovens Bruder Kaspar Karl und hinterließ einen neun Jahre alten Sohn. Beethoven verstrickte sich in einen über Jahre andauernden, zermürbenden Rechtsstreit mit seiner Schwägerin Johanna um die Vormundschaft über seinen Neffen Karl, in dessen Verlauf ihm diese wechselweise zu- und wieder abgesprochen wurde. In seiner Funktion als Ersatzvater scheiterte Beethoven mit dem Versuch, den Schützling seinen moralisch überzogenen Erziehungszielen zu unterwerfen.

Gleichzeitig mit Beethovens persönlicher Krise vollzog sich ein Wandel seines kompositorischen Stils. 1813 bis 1814 war er zunächst mit der Komposition von "Wellingtons Sieg" sowie einer gründlichen Revision seiner Oper "Leonore" zu "Fidelio" beschäftigt. In den Folgejahren wandte sich Beethoven noch einmal intensiv der Klaviersonate zu. Es entstanden die Sonaten op. 90 (1814), op. 101 (1815–1817) und op. 106 („Hammerklavier-Sonate“, 1817–1818). Gleichzeitig schuf Beethoven die beiden Cellosonaten op. 102 (1815), den Liederkreis "An die ferne Geliebte" op. 98 (1816) sowie die Vertonung von Goethes "Meeres Stille und glückliche Fahrt" für Chor und Orchester op. 112.

Hatte sich Beethoven einige Jahre fast ausschließlich Werken für kleinere Besetzungen gewidmet, so bot sich 1819 ein Anlass, wieder ein größeres Werk in Angriff zu nehmen. Sein langjähriger Mäzen und Klavierschüler, der Erzherzog Rudolph, sollte am 20. März 1820 als Erzbischof von Olmütz (tschechisch "Olomouc") inthronisiert werden. Beethoven wurde mit der Komposition einer großen feierlichen Messe beauftragt. Doch der Kompositionsprozess der "Missa solemnis" op. 123 begann sich zu verselbständigen, sodass Beethoven das Werk erst Ende 1822 / Anfang 1823 vollendete.

Gleichzeitig mit der Messe arbeitete Beethoven an den "33 Veränderungen über einen Walzer von Anton Diabelli" op. 120, einem Variationenzyklus für Klavier, der auf einen Aufruf des Musikverlegers und Komponisten Diabelli zurückging. Dieser hatte seinen Walzer an zahlreiche Komponisten geschickt mit der Bitte, je eine Variation zu einer geplanten Sammelausgabe beizusteuern. Während der Arbeit an der "Missa solemnis" und den Diabelli-Variationen setzte Beethoven mit op. 109, 110 und 111 die Serie seiner letzten Klaviersonaten fort.

Nach mehr als zehnjähriger Pause wandte sich Beethoven auch wieder der Gattung Sinfonie zu. Die Uraufführung der 9. Sinfonie op. 125 am 7. Mai 1824 wurde vom Publikum begeistert aufgenommen. Die Aufführung leitete der Kapellmeister Michael Umlauf, Beethoven stand mit ihm zur Unterstützung am Dirigentenpult.

Schließlich entstand zwischen Frühjahr 1824 und Herbst 1826 beginnend mit dem Streichquartett op. 127 eine letzte Gruppe von fünf Streichquartetten. Angestoßen hatte die Quartettproduktion ein Kompositionsauftrag des russischen Musikliebhabers Nikolai Borisowitsch Fürst Galitzin. Zusätzliche Motivation erhielt Beethoven durch die Rückkehr des Geigers Ignaz Schuppanzigh nach Wien, der seit 1816 auf Reisen gewesen war und dessen Ensemble fast alle seine früheren Streichquartette uraufgeführt hatte. Das Streichquartett in F-Dur op. 135 war das letzte Werk, das Beethoven vollendete.

Durch die Taubheit war Beethoven in den letzten Jahren zunehmend auf die Unterstützung durch Freunde und Bekannte angewiesen. Zwar hatte Beethoven Hauspersonal (Köchin und Haushälterin), doch führten heftige Auseinandersetzungen mit den Angestellten mehrfach zu Kündigungen von der einen oder anderen Seite.

Schon früher hatte Beethoven Personen aus dem Freundeskreis zur Erledigung von Besorgungen und anderen Diensten genutzt, so etwa 1817/18 die Klavierbauerin Nanette Streicher geb. Stein. Die freundschaftliche Verbindung Beethovens zur Klavierbauerfamilie Stein geht schon auf seinen frühen Aufenthalt in Augsburg 1787 zurück. Nanette Streicher kümmerte sich um die Haushaltsführung und vermittelte oft auch zwischen dem Komponisten und seinem Hauspersonal.

1822 tauchte erstmals Anton Schindler in Beethovens Bekanntenkreis auf. Schindler suchte Beethovens Nähe und diente sich ihm als Faktotum an. Seine Mischung aus Servilität und Eigenmächtigkeit war immer wieder Auslöser für dessen Unmut und Verachtung. Nach Beethovens Tod brachte Schindler Dokumente aus dessen Nachlass, so einen Teil der Konversationshefte, in seinen Besitz. Schindler verfasste eine der frühesten Beethoven-Biografien, doch ist die Glaubwürdigkeit vieler seiner Angaben zweifelhaft, da er zur Untermauerung seiner Behauptungen auch vor Fälschungen nicht zurückschreckte.

Im Sommer 1825, nach dem Bruch mit Schindler, übernahm Karl Holz, der zweite Geiger aus dem Schuppanzigh-Quartett, die Funktion des persönlichen Sekretärs und Beraters.

Gerade in den letzten Monaten seines Lebens gewann die Freundschaft mit Beethovens Jugendfreund Stephan von Breuning, der 1801 nach Wien gekommen war, wieder an Bedeutung. Breuning wurde im September 1826 Mitvormund des Neffen Karl und kümmerte sich um Beethoven in den Monaten seiner Todeskrankheit.

Ludwig van Beethovens Bruder Johann hatte es als Apotheker in Wien zu einigem Wohlstand gebracht. Der nie besonders enge Kontakt der Brüder intensivierte sich, als Beethoven sich von Johann 1822 eine größere Summe lieh. In den folgenden Jahren zog der Komponist den erfolgreichen Geschäftsmann immer wieder als Berater in Geldangelegenheiten heran.

Die Entscheidung des Appellationsgerichts am 8. April 1820, die Beethoven endgültig zum Vormund seines Neffen Karl bestimmte unter der Bedingung, dass ein Mitvormund ihn unterstützte, konnte die fortgesetzten Spannungen zwischen Onkel und Neffe nicht beenden. Am 6. August 1826 unternahm Karl einen Selbstmordversuch, der zum Rücktritt Beethovens von der Vormundschaft führte.

Schon seit etwa seinem 30. Lebensjahr litt Beethoven häufig an Krankheiten. Es sind Schilderungen unterschiedlicher Symptome wie Durchfall, Leibschmerzen, Koliken, Fieberzustände oder Entzündungen überliefert. Als Ursachen kommen zum einen akute Erkrankungen in Betracht, zum anderen werden eine oder mehrere chronische Erkrankungen als Hauptursache genannt. Unter anderem werden eine Bleivergiftung, Brucellose und übermäßiger Alkoholgenuss vermutet. Ob nur eine einzige oder mehrere verschiedene Ursachen für Beethovens gesundheitliche Probleme verantwortlich waren, ließ sich bis in die heutige Zeit nicht zweifelsfrei feststellen. Beethovens Biografen haben festgehalten, dass der Künstler regelmäßig billigen Weißwein trank, der von den Winzern damals mit Bleizucker statt mit teurem Rohrzucker gesüßt wurde. Die Knochen und auch das Haar von Beethoven enthalten Blei und zwar in einer Konzentration, die selten gemessen wurde: "„Wir haben mehr als 20.000 Patienten untersucht und bei allen den Bleigehalt im Blut und in den Haaren gemessen. Darunter waren nur acht Menschen die vergleichbaren Bleiwerte hatten. Alle acht sind schwer krank und ihre Symptome ähneln denen von Beethoven. Das Blei muss nicht die einzige Ursache für Beethovens Krankheit und frühen Tod sein, aber mit Sicherheit hat das giftige Metall seine Beschwerden verstärkt.“"

Mit zunehmendem Alter mehrten sich Häufigkeit und Intensität der Krankheitszustände. Im Sommer 1821 kündigte sich durch eine schwere Gelbsucht eine Leberzirrhose an. Beethoven suchte Linderung der Beschwerden in Bäder- und Landaufenthalten. Sein letzter führte ihn am 29. September 1826 – zusammen mit seinem Neffen – auf das Landgut seines Bruders Johann nach Gneixendorf. Auf der Rückreise nach Wien, die Anfang Dezember bei nasskaltem Wetter im offenen Wagen stattfand, zog sich Beethoven eine Lungenentzündung zu. Kurz nach der Genesung zeigten sich mit Wasseransammlungen in Beinen und Unterleib sowie einer Gelbsucht schwere Symptome der Leberzirrhose, so dass Beethoven das Krankenbett nicht mehr verlassen konnte. Nach mehreren Punktionen und erfolglosen Behandlungsversuchen verschiedener Ärzte starb Beethoven am 26. März 1827.

Die Beisetzung auf dem Währinger Ortsfriedhof fand am 29. März unter großer Anteilnahme der Wiener Bevölkerung statt. Ungefähr 20.000 Personen sollen am Trauerzug teilgenommen haben. Die von Franz Grillparzer verfasste Grabrede sprach der Schauspieler Heinrich Anschütz. Franz Schubert, der Beethoven nur ein Jahr später ins Grab folgen sollte, erwies ihm neben Grillparzer als einer der 36 Fackelträger die letzte Ehre.

Beethovens Leichnam wurde zweimal exhumiert: 1863, um die Gebeine zu vermessen und den Schädel zu fotografieren; 1888, um sein Skelett – erneut unter großer öffentlicher Anteilnahme – in den Ehrenhain auf dem Wiener Zentralfriedhof umzubetten.

Beethoven gilt heute als der Vollender der Wiener Klassik und Wegbereiter der Romantik. Insbesondere in den für die Epoche der Wiener Klassik grundlegenden Formen der Sinfonie, der Klaviersonate und des Streichquartetts hat er Werke geschaffen, deren musikgeschichtlicher Einfluss kaum zu übertreffen ist. So legte Beethoven beispielsweise wichtige Grundsteine für die weitgehende sinfonische Durchdringung der Solokonzerte in der weiteren Musikgeschichte.

Die Sonatensatzform, und darin namentlich die Durchführung, entwickelte er fast zu Vollkommenheit.
Auch eine immer weiterführende Konzentration von Sinfonien und Solokonzerten, sich von Quantität zu Qualität bewegend, kann Beethoven zugesprochen werden.

In der ersten Periode seines Schaffens schloss er sich noch aufs Engste an Haydn und Mozart an, bis er zu seinem ganz eigenen Stil fand. Was ihn nun vor diesen Vorgängern auszeichnet, die ja ihrerseits schon die Instrumentalstimmen stark individualisiert hatten, ist zunächst die weitere Ausgestaltung der übernommenen Formen zu größeren, den neuen Ideen angemessenen, Dimensionen. Unter seinen Händen erweiterte sich das bisher höfisch-aristokratische Menuett zum lebhaften Scherzo; das Finale, vor Beethoven meist nur ein heiter und lebhaft sich verlaufender Ausgang, wurde bei ihm zum Gipfelpunkt der Entwicklung des ganzen Werks und übertrifft an Wucht und Breite nicht selten den ersten Satz. Das andere Neue war die überall erkennbare Einheit eines zusammenfassenden Gedankens. Was er in einzelnen Werken (z. B. in den Klaviersonaten "Pathétique" und "Appassionata," "Les Adieux", in der "Eroica" und in der "Pastoral-Sinfonie") schon durch den Titel deutlich machte, lässt sich auf die Mehrzahl seiner Instrumentalwerke anwenden: dass die in den einzelnen Teilen dargestellten Seelenzustände in einer inneren Beziehung zueinander stehen.

Seine Skizzenbücher zeigen, mit wie viel unermüdlicher Arbeit und wiederholten Versuchen er seinen Werken die Gestalt zu geben suchte, in der sie ihn schließlich befriedigten. Man staunt, wie O. Jahn schrieb:
Die frühen, noch in der Bonner Zeit entstandenen Werke Beethovens umfassen zehn heute bekannte Kompositionen aus dem Zeitraum 1782–1785, die im Bemühen, ihn zu einem Wunderkind zu stilisieren, fast alle veröffentlicht wurden. Außerdem sind etwa dreißig Werke aus den Jahren 1787–1792 bekannt, von denen damals jedoch nur eines veröffentlicht wurde. Viele davon arbeitete Beethoven in spätere Werke ein. Die Stücke des ersten Zeitraums waren noch stark vom Stil Neefes und Sterkels geprägt und orientierten sich am Vorbild Mozarts. Die späteren Bonner Jahre brachten eigenständigere Lieder, Kantaten, Arien und Variationen, die in manchen Fällen bereits den analytisch arbeitenden Komponisten späterer Zeit erahnen ließen. Die Werke im Sonatenstil sind dagegen wenig eindrucksvoll und blieben in großen Teilen Fragment; diese für den späteren Beethoven so wichtige Form hat er sich offenbar erst in der Wiener Zeit angeeignet.

Sie begann mit der Herausgabe der ersten drei Klaviertrios op. 1 (1795) und endete etwa mit den Jahren 1800–1802.

Die 1. Sinfonie gehört dazu. Im Alter von 29 Jahren nahm Beethoven sie in Angriff und vollendete sie am Anfang des darauffolgenden Jahres. Sie wurde mit großem Erfolg am 2. April 1800 uraufgeführt. Bahnbrechend in dieser Zeit waren die Klavierkompositionen, sowohl in der Form des Konzerts als auch der Sonate und Variation, nicht nur in der Technik, sondern auch im Zuschnitt der Sätze und des Ganzen.

Sie begann etwa in den Jahren 1800–1802 und dauerte bis 1814. Hierher gehören vor allem Sinfonien und Klaviersonaten.

Mit der 3. Sinfonie fand Beethoven zur Form einer eher monumentalen und heroischen Sinfonie. Ursprünglich trug sie den Titel „Sinfonia grande, intitolata Bonaparte“ (benannt nach Napoleon). Nachdem er jedoch erfuhr, dass Napoleon sich am 18. Mai 1804 zum französischen Kaiser gemacht hatte, radierte er in großer Wut den Namen aus dem Titelblatt. Seine neue Überschrift hieß nun „Heroische Sinfonie, komponiert um das Andenken eines großen Mannes zu feiern“, heute wird sie meistens mit ihrem italienischen Titel "Eroica" genannt. Uraufgeführt wurde sie im August 1804 im Wiener Palais des Fürsten Franz Joseph Maximilian von Lobkowitz, dem sie nun auch gewidmet war.

1805 wurde Beethovens einzige Oper uraufgeführt. Sie hatte, wie auch viele andere Werke, etliche Überarbeitungen erlebt und hieß ursprünglich „Leonore“. In der dritten Fassung als "Fidelio" (mit der E-Dur-Ouvertüre) kam sie 1814 auf die Bühne. Beethoven hat mit ihr keine neuen Wege dramatischer Gestaltung beschritten. Ihre besondere Stellung hat sie nach wie vor durch den reichen und tiefen, menschlich interessanten und berührenden Stoff und die Qualität ihrer Musik.

Beethovens 5. Sinfonie wird auch „Schicksalssinfonie“ genannt. Sie entstand in einer schweren Lebensphase des Komponisten. Über die markanten vier Anfangstöne soll Beethoven gesagt haben: „So pocht das Schicksal an die Pforte“. Allerdings wurde dies von Musikhistorikern als spätere legendarische Zuschreibung verworfen. Am 22. Dezember 1808 wurde sie zusammen mit der 6. Sinfonie, dem vierten Klavierkonzert und Teilen der C-Dur-Messe uraufgeführt.

Die Pastorale ist nicht im eigentlichen Sinne Programmmusik, wie sie fälschlich oft bezeichnet wird, sondern nach Beethovens eigener Aussage „mehr Ausdruck der Empfindung als Malerei“. Damit befindet sich die Sinfonie immer noch auf dem Boden der Klassik. Franz Liszt, der die ersten sinfonischen Dichtungen schrieb, bezog sich bei der Entwicklung dieses neuen Genres vielmehr auf die Ouvertüren Beethovens, wie etwa "Coriolan" oder "König Stephan." Allerdings stellte die naturbezogene Thematik der Pastorale ein Grundelement der folgenden Epoche der Romantik dar, der Epoche sinfonischer Dichtungen schlechthin. Ebenso legte die Kunst der Romantik den Akzent auf das Innerste des Menschen, also sein Empfinden und seine Gesinnung. Unter dem Gesichtspunkt, den wahrscheinlich auch Beethoven für seine Pastorale eingenommen hatte, könnte diese sehr wohl als Romantikvorläuferin betrachtet werden. Dafür spricht die Formerweiterung auf fünf Sätze ebenso wie der Einbezug von Instrumenten (z. B. Piccoloflöte), die im klassischen Sinfonieorchester nicht heimisch sind. Dies sind deutlich auf die Romantik hinweisende Neuerungen, welche in der triumphalen 9. Sinfonie noch wesentlich deutlicher werden. Außerdem sind auch einige der in der Pastorale verwendeten „programmmusikalischen“ Topoi („Sturm“ etc.) in der Romantik gerne aufgegriffen worden.

In der 1812 beendeten 7. Sinfonie A-Dur op. 92 nimmt Beethoven musikalisch bereits die späteren antinapoleonischen Befreiungskriege vorweg. Sie entfachte bei der Uraufführung im Dezember 1813 eine riesige Euphorie und Begeisterung unter den Zuhörern. Diese Sinfonie zeichnet sich durch einen gewissen Patriotismus aus und deutet nicht so deutlich auf die kommende Epoche der Romantik hin wie die Vorgänger-Sinfonien. Dennoch sind harmonische und polyphone Neuerungen in dem Werk deutlich zu spüren. Zentraler Satz des Werkes ist unüblicherweise der zweite, das "Allegretto". Es ist von einem feierlich schreitenden Rhythmus geprägt, wie ein endloser Trauerzug von Millionen. Er ist nach den Worten Beethovens denjenigen gewidmet, „die uns so viel geopfert haben“.

In den Jahren 1814 bis 1818 erlahmte Beethovens Produktion vorübergehend. In diesem kurzen Zeitraum traten nur ganz vereinzelt größere Kompositionen hervor, z. B. die Sonate in A (1815) und der „Liederkreis“. Krankheit und bitteres häusliches Leid hemmten seine Phantasie. Nach Überwindung dieser Zeit der Entmutigung war er in mancher Beziehung verändert. Sein Empfinden war bei völliger Abgeschlossenheit gegen die Außenwelt noch mehr verinnerlicht, infolgedessen der Ausdruck häufig noch ergreifender und unmittelbarer als früher, dagegen die Einheit von Inhalt und Form mitunter nicht so vollendet wie sonst, sondern von einem subjektiven Moment beeinflusst.

Die Hauptwerke dieser dritten Epoche sind die "Missa solemnis" (1818–23), die Beethoven selbst für sein vollendetstes Werk hielt, und die neunte Sinfonie in d-Moll (1823–24). Außerdem gehören zu dieser Zeit: die Ouvertüre „Zur Weihe des Hauses“, Op. 124 (1822), die Klaviersonaten Op. 106 in B-Dur (1818), Op. 109 in E-Dur, Op. 110 in As-Dur (1821) und Op. 111 in c-Moll (1822), die Diabelli-Variationen, mehrere kleinere Stücke für Klavier und Gesang und schließlich die letzten großen Streichquartette Op. 127 in Es-Dur (1824), Op. 130 in B-Dur und Op. 132 in a-Moll (1825), Op. 131 in cis-Moll und Op. 135 in F-Dur (1826). Die späten Streichquartette gehen auf Beethovens Beschäftigung mit den Kompositionstechniken Johann Sebastian Bachs zurück, speziell seine Fugentechniken. Trotzdem weisen diese Stücke nicht in die Vergangenheit, sondern weit in die Zukunft. Speziell sein Op. 133 („Große Fuge“) war für seine Zeit außerordentlich modern und wurde im gesamten 19. Jahrhundert kaum verstanden.

Viele Entwürfe, darunter der zu einer zehnten Sinfonie in c-Moll und zu einem Streichquintett in C-Dur, befanden sich im Nachlass des Komponisten.

1824 vollendete Beethoven die 9. Sinfonie. Der letzte Satz mit dem Chorfinale zu Schillers Gedicht "An die Freude" ist besonders bekannt. Das Thema dieses Satzes ist die Grundlage der Europahymne. Die Sinfonie drückt das Ringen eines Menschenherzens aus, das sich aus Mühen und Leiden nach dem Tag reiner Freude sehnt, der ihm doch in voller Klarheit und Reinheit nicht beschieden ist. Die ersten drei Sätze mit ihrer grandiosen Architektur, Instrumentierung und Themenverarbeitung wurden richtungsweisend für die Sinfoniker der Romantik bis hin zu Gustav Mahler. Die Uraufführung der 9. Sinfonie erfolgte zusammen mit Teilen der "Missa solemnis" am 7. Mai 1824. Beethoven wollte, obwohl er die Aufführung nicht mehr selbst leiten konnte, den Dirigenten durch Angabe der Tempi unterstützen. Das Orchester war allerdings angewiesen worden, dies nicht zu beachten.

Beethoven hat noch kurz vor seinem Tod an seiner 10. Sinfonie gearbeitet, diese aber nicht vollendet. Vom ersten Satz existieren jedoch viele Skizzen und Notizen. Barry Cooper hat diese Skizzen zu einem ersten Satz ausgearbeitet. Auch zu einem dritten Satz, einem mit „Presto“ betitelten kraftvollen Scherzo, existieren Skizzen aus dem Jahre 1825.


Sinfonien:


Klavierkonzerte:


Weitere konzertante Werke:

Ouvertüren:

Oper:


Ballette:

Bühnenmusiken:




Im Mittelpunkt seines Schaffens stehen die 32 Sonaten für Klavier. Außerdem schrieb er Variationen über fremde und eigene Themen, von denen die Diabelli-Variationen zu den bedeutendsten Klavierwerken überhaupt zählen.

Unter den zahlreichen Klavierstücken sind die 24 "Bagatellen" in op. 33, op. 119 und op. 126 große Kostbarkeiten. Populär sind vor allem das Albumblatt "Für Elise" und das Rondo "Die Wut über den verlorenen Groschen" (op. 129).













Bearbeitungen von Beethovens Hand:



















Biographisches

Aktuelles 

Fremdsprachige Websites

Noten und Liedtexte

Aufnahmen 

Gesamtaufführung


</doc>
<doc id="8894" url="https://de.wikipedia.org/wiki?curid=8894" title="Luftfeuchtigkeit">
Luftfeuchtigkeit

<onlyinclude>Die Luftfeuchtigkeit – oder kurz Luftfeuchte – bezeichnet den Anteil des Wasserdampfs am Gasgemisch der Luft. Flüssiges Wasser (zum Beispiel Regentropfen, Nebeltröpfchen) oder Eis (zum Beispiel Schneekristalle) werden der Luftfeuchtigkeit folglich nicht zugerechnet. Die Luftfeuchtigkeit ist eine wichtige Kenngröße für zahlreiche technische und meteorologische Vorgänge, für viele Lebensvorgänge bei Lebewesen sowie für Gesundheit und Behaglichkeit der Menschen.
</onlyinclude>
In Abhängigkeit von Temperatur und Druck kann ein gegebenes Luftvolumen nur eine gewisse Höchstmenge Wasserdampf enthalten. Die "relative Luftfeuchtigkeit", die das geläufigste Maß für die Luftfeuchtigkeit ist, beträgt dann 100 %. Allgemein gibt die relative Luftfeuchtigkeit, ausgedrückt in Prozent (%), das Gewichtsverhältnis des momentanen Wasserdampfgehalts zu dem Wasserdampfgehalt an, der für die aktuelle Temperatur und den aktuellen Druck maximal möglich ist. Durch die Aufnahme von Wasserdampf wird die Luftdichte verringert, da bei gleich bleibendem Gesamtdruck eine hinzugefügte Anzahl von HO-Molekülen dieselbe Anzahl von schwereren N- und O-Molekülen verdrängt.




Ein wasserdampffreies Luftgemisch bezeichnet man als trockene Luft. Tabellen zur Zusammensetzung der Luft beziehen sich in der Regel auf trockene Luft, da der Wasserdampfanteil feuchter Luft mit 0 bis 4 Volumenprozent vergleichsweise sehr stark schwankt. Beeinflusst wird die Luftfeuchtigkeit vor allem durch die Verfügbarkeit von Wasser, die Temperatur und den Grad der Durchmischung der Atmosphäre. Höhere Lufttemperaturen ermöglichen eine höhere Wasserdampfkonzentration in der Luft. Bei sehr geringen Konzentrationen von Wasserdampf in der Luft bezeichnet man die Luftfeuchtigkeit auch als Spurenfeuchtigkeit bzw. Spurenfeuchte.

An einer freien Wasseroberfläche, die flüssiges Wasser vom darüberliegenden Luftvolumen trennt, treten stets einzelne Wassermoleküle vom Wasservolumen in das Luftvolumen über. Im flüssigen Wasser sind die Wassermoleküle durch molekulare Kräfte, vor allem durch die Wasserstoffbrückenbindungen, vergleichsweise stark aneinander gebunden, wodurch sich der zusammenhängende Flüssigkeitsverbund erst ausbilden kann. Infolge ihrer thermischen Bewegung tragen die Wassermoleküle jedoch jeweils gewisse Beträge an kinetischer Energie, die um einen temperaturabhängigen Mittelwert herum streuen (Maxwell-Boltzmann-Verteilung). Ein kleiner Anteil von Wassermolekülen hat daher stets genügend thermische Energie, um die Bindungskräfte der umgebenden Moleküle zu überwinden, die Wasseroberfläche zu verlassen und in das Luftvolumen überzugehen, also zu verdunsten. Die Verdunstungsrate, das ist die Menge an verdunstendem Wasser je Zeiteinheit, hängt vom Anteil derjenigen Moleküle ab, deren kinetische Energie die Bindungsenergie des Flüssigkeitsverbundes überschreitet, und wird unter anderem von der herrschenden Temperatur bestimmt.

Umgekehrt treffen verdunstete Wassermoleküle aus der Luft auch wieder auf die Wasseroberfläche und können dort je nach ihrer kinetischen Energie mit einer gewissen Wahrscheinlichkeit vom Molekülverbund eingefangen werden, also kondensieren. Die Kondensationsrate ist nur vom Partialdruck des Wasserdampfes in der Luft abhängig, nicht aber von dem Anteil des Luftdrucks, den die anderen Bestandteile der Luft liefern.

Vier Größen beeinflussen die Menge dieses Stoffaustauschs:

Betrachtet man einen Verdunstungsvorgang bei konstanter Temperatur und anfangs trockener Luft, so stellt sich die der Temperatur entsprechende Verdunstungsrate ein, während die Kondensationsrate mangels Wassermolekülen in der Luft zunächst gleich null ist. Die Verdunstungsrate ist also größer als die Kondensationsrate, und die Anzahl von Wassermolekülen in der Luft steigt daher an. Damit wächst auch die Kondensationsrate, und die Nettoverdunstung (Verdunstungsrate minus Kondensationsrate) beginnt zu sinken. Die Dichte der Wassermoleküle in der Luft und damit die Kondensationsrate steigen so lange an, bis Kondensationsrate und Verdunstungsrate gleich sind, pro Zeiteinheit also ebenso viele Wassermoleküle vom Wasser in die Luft übertreten wie von der Luft ins Wasser. Dann ist der Gleichgewichtszustand erreicht, in dem die Nettoverdunstung null ist, obwohl ein ständiger Teilchenaustausch zwischen Luft und Wasser stattfindet.

Die im Gleichgewichtszustand vorliegende Konzentration von Wassermolekülen in der Luft ist die "Sättigungs­konzentration". Steigt die Temperatur, wird sich auch eine höhere Sättigungskonzentration einstellen, da die nun ebenfalls erhöhte Verdunstungsrate zur Erreichung eines neuen Gleichgewichts durch eine höhere Kondensationsrate wieder kompensiert werden muss, was eine höhere Teilchendichte in der Luft voraussetzt. Die Höhe der Sättigungskonzentration hängt also von der Temperatur ab.

Die Sättigungskonzentration wird fast allein durch die Eigenschaften der Wassermoleküle und ihre Wechselwirkung mit der Wasseroberfläche bestimmt, es besteht keine wesentliche Wechselwirkung mit den anderen Atmosphärengasen. Wären jene Gase nicht vorhanden, so würde sich über dem Wasser praktisch dieselbe Sättigungskonzentration einstellen. Die umgangssprachlich gebräuchliche und wegen der Einfachheit auch in Fachkreisen weit verbreitete Ausdrucksweise, "die Luft" könne bei gegebener Temperatur maximal eine bestimmte Menge an Wasserdampf "aufnehmen", ist irreführend. Die Luft nimmt die Feuchtigkeit nicht analog zu einem Schwamm auf, und auch der Begriff der Sättigung darf hier nicht analog zur Sättigung einer Lösung verstanden werden. Die Luft besteht aus selbstständig agierenden Gasteilchen, die im Wesentlichen nur über Stöße wechselwirken. Weder ist also Sauerstoff im Stickstoff, noch Wasserdampf in den anderen Luftbestandteilen gelöst.
Die Sättigungskonzentration ist somit von der kinetischen Energie der Wasserteilchen abhängig.

Aus demselben Grund wird die Sättigungskonzentration nicht von der Temperatur der Luft bestimmt, sondern von der Temperatur der verdunstenden Oberfläche. Der Bezug auf die Temperatur der Luft ist in der Alltagspraxis oft gerechtfertigt, da verdunstende Flächen geringer thermischer Trägheit meist näherungsweise Lufttemperatur annehmen (zum Beispiel an der Luft trocknende Wäsche). Ist jedoch die verdunstende Oberfläche deutlich wärmer als die Luft, so verdunsten die Wassermoleküle mit einer der Oberflächentemperatur entsprechenden Verdunstungsrate in die kühlere Luft hinein (heiße Herdplatte), auch wenn deren Sättigungskonzentration dabei überschritten wird. Ein Teil der Feuchtigkeit kondensiert dann in der Luft an den kühleren Aerosolen, welche Lufttemperatur angenommen haben, und wird als Dampf- oder Nebelschwaden sichtbar (zum Beispiel Dunstschwaden über einem herbstlichen See). Ist die Oberfläche kühler als die Luft, so kann unter Umständen auch der Feuchtigkeitsgehalt teilgesättigter Luft zu Übersättigung und Kondensation an der Oberfläche führen (zum Beispiel beschlagene Fenster in Küche oder Bad). Genauer gesagt, kondensiert der Wasserdampf zu Wasser (zu Tau, wenn die Oberflächentemperatur unter dem Taupunkt, oder zu Reif, wenn sie unter dem Reifpunkt liegt, siehe dazu auch unten).

Erhöht man durch eine Zufuhr von Wassermolekülen deren Konzentration über die Sättigungskonzentration (Übersättigung), so steigt wegen der größeren Dichte an Wassermolekülen in der Luft die Kondensationsrate vorübergehend über die Verdunstungsrate hinaus an und die Konzentration an Wassermolekülen sinkt daher wieder auf den Gleichgewichtswert.

Auch hier ist zu beachten, dass es sich nicht etwa um ein "Unvermögen der Luft" handelt, den überschüssigen Wasserdampf "zu halten". Vielmehr nutzt der Wasserdampf unter diesen Bedingungen eine sich darbietende Kondensationsfläche, um seine Konzentration durch heterogene Kondensation auf die Sättigungskonzentration zu senken. Fehlen solche Kondensationsflächen oder Kondensationskerne, so kann die Luft dauerhaft erhebliche Mengen von Wasserdampf aufnehmen, bis es schließlich zu einer spontanen Entstehung von Wassertröpfchen (homogene Kondensation) kommt; siehe dazu auch den Abschnitt "Oberflächenkrümmung des Wassers." Dies ist zum Beispiel in großen Volumina möglichst reiner Luft, also bei einer geringen Aerosolkonzentration, und bei großer Entfernung von etwaigen Umschließungsflächen der Fall (siehe Nebelkammer). Spontane Kondensation von Wasserdampf zu Wassertröpfchen findet ohne Kondensationskeime erst bei extremer Übersättigung von mehreren hundert Prozent relativer Feuchtigkeit statt. In der Praxis ist jedoch fast immer eine ausreichend große Menge von Aerosolen in der Luft vorhanden, so dass es in der Atmosphäre kaum zu Übersättigungen von mehreren Prozentpunkten kommt.

Die Verdunstungsrate des Wassers kann bestimmte Maximalwerte nicht überschreiten. Es dauert daher längere Zeit, bis sich das Gleichgewicht nach einer Störung wieder eingestellt hat. Wurde zum Beispiel durch nächtliche Abkühlung ein Teil des Feuchtigkeitsgehalts auskondensiert, so ist die Luft nach einer Erwärmung zunächst ungesättigt und kann den Sättigungszustand nur langsam wieder erreichen. Diese Teilsättigung ist für unsere Atmosphäre wegen der häufigen Temperaturschwankungen der Normalfall. Es ist für zahlreiche Vorgänge von großer Bedeutung, wie weit die Luft vom Sättigungszustand entfernt ist. Verschiedene Feuchtigkeitsmaße dienen dazu, diesen Zustand quantitativ zu beschreiben.

Bei Erhöhung der Temperatur nimmt der Anteil an Wassermolekülen zu, welche genügend kinetische Energie besitzen, um die Wasseroberfläche zu verlassen. Es stellt sich also eine höhere Verdunstungsrate ein, welche zur Wiederherstellung des Gleichgewichts durch eine höhere Kondensationsrate kompensiert werden muss, was aber eine höhere Konzentration von Wassermolekülen in der Luft voraussetzt.

Die Sättigungskonzentration des Wasserdampfs nimmt daher, wie in der Abbildung 1 dargestellt, mit steigender Temperatur exponentiell zu. Der Wasserdampf hat für jede Temperatur (und fast unabhängig vom Umgebungsdruck) eine eindeutig bestimmte Sättigungskonzentration. Bei atmosphärischem Normaldruck von 1013,25 hPa kann ein Kubikmeter Luft bei 10 °C maximal 9,41 g Wasser aufnehmen. Die gleiche Luftmenge nimmt bei 30 °C schon 30,38 g Wasser auf und bei 60 °C sind es schon über 100 g Wasser. Man bezeichnet diese Sättigungskonzentration als maximale Feuchtigkeit, die im Artikel Sättigung tabelliert ist. Hierbei sind auch Mollier-Diagramme nach Richard Mollier (1923) zur Darstellung der Luftfeuchtigkeit weit verbreitet.
Eine andere Möglichkeit zur Darstellung des Zusammenhangs von Luftfeuchtigkeit, Temperatur und Höhenlage ist das Emagramm.

Wie oben erwähnt, ist die Sättigungskonzentration des Wasserdampfs bei gegebener Temperatur praktisch unabhängig von der Anwesenheit der übrigen Atmosphärengase und damit auch fast unabhängig vom Umgebungsdruck. Eine "geringfügige" Abhängigkeit vom Umgebungsdruck ergibt sich jedoch aus drei Gründen:

Diese schwache Druckabhängigkeit kann bei Bedarf durch einen Korrekturfaktor berücksichtigt werden. Er ist von Temperatur und Druck abhängig und bewegt sich bei atmosphärischen Bedingungen im Bereich von 0,5 % (Näheres im Artikel Sättigungsdampfdruck).

Betrachtet man statt einer flüssigen Wasseroberfläche eine Eisoberfläche, so gelten dieselben Überlegungen auch für Sublimation und Resublimation der Wassermoleküle. Das Eis kühlt die direkt darüber liegende Luftschicht stark ab, diese hat dadurch eine geringere Sättigungskonzentration für Wassermoleküle. Sublimierte Wasserteilchen und die Umgebungsluftfeuchte führen deshalb zur Kondens- bzw. Nebelbildung im Nahbereich von Eisoberflächen.

Im Eiskristallverband unterliegen die Wassermoleküle jedoch stärkeren Bindungskräften als in flüssigem Wasser, so dass die Sättigungskonzentration über einer Eisoberfläche geringer ist als über einer Oberfläche flüssigen (unterkühlten) Wassers derselben Temperatur. Dieser Umstand spielt eine wichtige Rolle bei der Bildung von Regentropfen in Wolken (Bergeron-Findeisen-Prozess).

Sind im Wasser andere Stoffe gelöst, so erschweren sie den Wassermolekülen das Verlassen der Wasseroberfläche, wodurch die Verdunstungsrate sinkt und sich eine geringere Sättigungskonzentration einstellt (sog. Lösungseffekt).
In der Luft über gesättigten Salzlösungen stellen sich beispielsweise die in der Tabelle aufgeführten relativen Feuchtigkeiten ein.

Obwohl die Luft über den Lösungen mit Feuchtigkeit gesättigt ist, betragen die betreffenden relativen Feuchtigkeiten nicht 100 %, da die relative Feuchtigkeit stets auf die Sättigungskonzentration über einer ebenen und "reinen" Wasseroberfläche bezogen wird (siehe unten). Unterschreitet die Luft über der Salzlösung die betreffende Sättigungsfeuchtigkeit, so verdunstet Wasser aus der Lösung, um den Sättigungszustand wiederherzustellen. Überschreitet die Luft die Sättigungsfeuchtigkeit, so kondensiert ein Teil der Luftfeuchtigkeit an der Salzlösung. Diese wird dadurch verdünnt; soll sie zur Einhaltung definierter Verhältnisse salzgesättigt bleiben, so muss sie einen ausreichenden Bodensatz an ungelöstem Salz enthalten.

Der Lösungseffekt verdeutlicht nochmals, dass die Sättigungskonzentration in der Luft nicht von der Luft selbst, sondern von der verdunstenden Oberfläche bestimmt wird.

Ist die Wasseroberfläche wie zum Beispiel bei einem Tropfen konvex (nach außen gekrümmt), so sind die Wassermoleküle an der Oberfläche weniger stark gebunden und können die Oberfläche leichter verlassen. Dieser Krümmungseffekt bedingt daher, dass die Verdunstungsrate steigt. Wenn gesättigte Luft mit kleinen Nebeltröpfchen im Gleichgewicht steht, beträgt ihre relative Feuchtigkeit daher etwas "über" 100 %. Der gleiche Effekt führt auch dazu, dass ohne Kondensationskeime eine starke Übersättigung möglich ist, ohne dass es zu homogener Kondensation kommt; je nach Stärke der Übersättigung gibt es einen gewissen Mindestradius der Tröpfchen, unterhalb dem sie nicht stabil sind, da mit geringerem Radius die Verdunstungsrate steigt, durch die Verdunstung aber der Radius abnimmt (siehe Abschnitt "kritischer Radius" unter "Kelvingleichung").

Ist die Wasseroberfläche nach innen gekrümmt (wie zum Beispiel beim Meniskus in einer teilweise wassergefüllten Kapillare), so sind die Wassermoleküle an der Oberfläche stärker gebunden und können die Oberfläche weniger leicht verlassen – die Verdunstungsrate sinkt. Wenn gesättigte Luft in einem wasserhaltigen porösen Material mit den Menisken im Gleichgewicht steht, beträgt ihre relative Feuchtigkeit "weniger" als 100 %.

Der Wassergehalt der Luft kann durch verschiedene so genannte Feuchtigkeitsmaße angegeben werden. Synonym verwendbare Bezeichnungen werden durch einen Schrägstrich verdeutlicht, zusammengehörige Feuchtigkeitsmaße stehen in der gleichen Zeile.


Die absolute Luftfeuchtigkeit, auch Wasserdampfdichte oder kurz Dampfdichte (Formelzeichen: "ρ", "ρ", "d" oder "a"; nicht verbindlich festgelegt), ist die Masse des Wasserdampfs in einem bestimmten Luftvolumen, also dessen Dichte bzw. Konzentration. Sie wird üblicherweise in g Wasser pro Kubikmeter Luft angegeben. Nach oben begrenzt wird sie durch die maximale Feuchtigkeit "ρ", die während einer Sättigung herrscht (zugehörige Formeln und Werte siehe dort).

Die absolute Luftfeuchtigkeit ist ein direktes Maß für die in einem gegebenen Luftvolumen enthaltene Wasserdampfmenge. Sie lässt unmittelbar erkennen, wie viel Kondensat maximal ausfallen kann oder wie viel Wasser verdunstet werden muss, um eine gewünschte Luftfeuchtigkeit zu erhalten.

Die absolute Luftfeuchtigkeit ändert sich bei einer Volumenänderung des betrachteten Luftpakets, auch ohne dass der Luft Wasserdampf hinzugefügt oder entzogen wird. Bei einer Kompression des Luftpakets werden die darin enthaltenen Wassermoleküle auf einen geringeren Raum konzentriert, ihre Anzahl pro Kubikmeter nimmt zu, die absolute Feuchtigkeit steigt; das Umgekehrte gilt bei einer Expansion des Luftpakets. Die Volumenänderung des Luftpakets kann durch Änderung seiner Temperatur oder seines Druckes verursacht werden. Beim Vergleich der Feuchtigkeitsgehalte zweier Luftpakete sind daher gegebenenfalls ihre Temperatur- und Druckunterschiede zu berücksichtigen. Ein in der Atmosphäre aufgrund der Thermik aufsteigendes Luftpaket verringert beim Aufsteigen seine absolute Feuchtigkeit, auch wenn es dabei keinerlei Wasserdampf verliert, da es wegen der Abnahme des Luftdrucks mit der Höhe sein Volumen vergrößert. Die absolute Feuchtigkeit des Luftpakets ändert sich daher allein durch Auf- und Abwärtsbewegungen. Man bezeichnet dies auch als Verschiebungsvarianz oder Instationarität. Da die absolute Luftfeuchtigkeit zudem schwer zu messen ist, wird sie nur selten verwendet.

Die absolute Luftfeuchtigkeit "ρ" kann mittels folgender Formeln berechnet werden, wobei sich der erste Term durch die Umstellung der Zustandsgleichung idealer Gase ergibt:

Die einzelnen Formelzeichen stehen für folgende Größen:


Tabellenwerte siehe unter "Sättigung".

Die relative Luftfeuchtigkeit (Formelzeichen: "φ", "f", "U", "RH", "H" oder "rF"; nicht verbindlich festgelegt) ist das prozentuale Verhältnis zwischen dem momentanen Dampfdruck des Wassers und dem Sättigungsdampfdruck desselben (bei der Lufttemperatur) über einer reinen und ebenen Wasseroberfläche. Bei einer nichtprozentualen Angabe, also im Wertebereich 0 bis 1, spricht man auch vom "Sättigungsverhältnis".

Die relative Feuchtigkeit lässt unmittelbar erkennen, in welchem Grade die Luft mit Wasserdampf gesättigt ist:
Anhand der relativen Feuchtigkeit lässt sich daher leicht abschätzen, wie rasch Verdunstungsvorgänge ablaufen werden oder wie groß die Wahrscheinlichkeit von Tauwasserbildung ist. Da die Verdunstung von Feuchtigkeit durch die Haut stark von der relativen Feuchtigkeit der Umgebungsluft bestimmt wird, stellt die relative Feuchtigkeit eine wichtige Kenngröße für das Behaglichkeitsempfinden dar (siehe unten).
Ein zweiter Grund für die Bedeutung der relativen Feuchtigkeit liegt darin, dass sie den Ausgleichswassergehalt hygroskopischer Materialien bestimmt. Hygroskopische Materialien, insbesondere poröse Materialien wie Holz, Ziegel, Gipsputz, Textilien usw., nehmen beim Kontakt mit Luft Feuchtigkeit auf und binden die Wassermoleküle durch Adsorption an ihren Porenwänden. Die Menge der gebundenen Moleküle wird bestimmt durch die absolute Luftfeuchtigkeit einerseits (eine größere Wasserdampfkonzentration führt wegen der größeren Auftreffrate auf die Porenwandungen zu einer größeren Adsorptionsrate) und die Temperatur andererseits (eine höhere Temperatur führt zu einer größeren Desorptionsrate). Die Kombination dieser beiden einander entgegengerichteten Einflussgrößen führt dazu, dass der sich einstellende Ausgleichswassergehalt im Wesentlichen von der relativen Feuchtigkeit der Luft bestimmt wird. Die Feuchtigkeitsspeicherfunktion eines Materials gibt an, welchen Wassergehalt das Material bei einer gegebenen relativen Luftfeuchtigkeit annimmt; sie ist nur wenig von der Temperatur abhängig. Zur Messung des Feuchtigkeitsgehalts der Luft werden meist Materialien verwendet, deren zur Messung benutzte physikalische Eigenschaft von ihrem Wassergehalt abhängt (Längenänderung wegen Quellen und Schwinden, Kapazitätsänderung eines hygroskopischen Dielektrikums usw.). Da dieser Wassergehalt wiederum von der relativen Feuchtigkeit der Umgebungsluft bestimmt wird, messen solche Instrumente daher letztlich diese relative Feuchtigkeit, welche deshalb ein besonders leicht zu messendes und häufig benutztes Feuchtigkeitsmaß ist.

Mit steigender Temperatur nimmt die Wasserdampfmenge, die zur Sättigung benötigt würde, zu. Das hat zur Folge, dass die relative Luftfeuchtigkeit eines gegebenen Luftpakets bei Erwärmung abnimmt. Die Angabe der Temperatur ist für die Vergleichbarkeit der Werte daher zwingend notwendig. So sind beispielsweise in einer als trocken erscheinenden Wüste mit einer Lufttemperatur von 34,4 °C und einer relativen Luftfeuchtigkeit von 20 % insgesamt 7,6 g Wasserdampf in einem Kubikmeter Luft enthalten, was bei einer Lufttemperatur von 6,8 °C einer relativen Luftfeuchtigkeit von 100 % entspräche und somit zur Kondensation führen würde. Daher sind Phänomene wie Dunst oder Nebel ein Signal für eine hohe relative Luftfeuchtigkeit und gleichzeitig für tiefe Temperaturen. Die Wahrnehmung der Luft als trocken oder feucht liegt also eher an der Temperatur als an der tatsächlich in ihr enthaltenen Wassermenge.

Man kann die relative Luftfeuchtigkeit mit folgenden Formeln berechnen:

Die einzelnen Formelzeichen stehen für folgende Größen:


Deliqueszenz bzw. Deliqueszenzfeuchte beschreibt das für einen Stoff (meist Salze) spezifische Vermögen, die relative Luftfeuchte der umgebenden Luft zu beeinflussen.

Die spezifische Luftfeuchtigkeit (Formelzeichen: "s", "q" oder "x") gibt die Masse des Wassers an, die sich in einer bestimmten Masse feuchter Luft befindet. Der Zahlenwertbereich geht von formula_5, wobei für trockene Luft formula_6 ist und für luftfreien Dampf bzw. flüssiges Wasser formula_7 ist.

Diese Größe bleibt im Unterschied zu den vorherigen Feuchtigkeitsmaßen bei Volumenänderungen des betrachteten Luftpakets unverändert, solange keine Feuchtigkeit zu- oder abgeführt wird. Nimmt z. B. das Volumen des Luftpakets zu, so verteilen sich sowohl die (unveränderte) Masse der feuchten Luft als auch die (unveränderte) Masse des Wasserdampfs auf ein größeres Volumen, das Verhältnis der beiden Massen im Luftpaket zueinander bleibt aber dasselbe. Die spezifische Luftfeuchtigkeit behält beispielsweise entlang eines kondensationsfreien Belüftungsrohres einen konstanten Wert, auch wenn die feuchte Luft dabei durch Rohrabschnitte unterschiedlicher Temperatur läuft oder auf ihrem Weg zum Beispiel wegen eines Drosselventils Druckänderungen erfährt. Auch ein in der Atmosphäre aufsteigendes Luftpaket behält den Zahlenwert seiner spezifischen Feuchtigkeit bei, solange keine Feuchtigkeit (etwa durch Verdunstung von Regentropfen) zugeführt oder (durch Kondensation des Wasserdampfes) abgeführt wird. Diesem Vorteil steht allerdings die schwierige Messung der spezifischen Luftfeuchtigkeit entgegen, die im Regelfall einem Labor vorbehalten bleibt.

Die maximale spezifische Luftfeuchtigkeit im Sättigungszustand, die sogenannte Sättigungsfeuchtigkeit, hat das Formelzeichen "S" (auch "q").

Die spezifische Luftfeuchtigkeit "s" kann mit folgenden Formeln berechnet werden, wobei die jeweilige Größe über den ersten Term definiert ist und alle nachfolgenden Terme Äquivalente oder Näherungen hierzu darstellen (fL – Feuchtigkeit Luft; tL – trockene Luft; W – Wasserdampf bzw. Wasser). Von praktischer Bedeutung sind nur die letztgenannten Terme, alle anderen dienen der Herleitung und Nachvollziehbarkeit.

damit:

wobei gilt:

Die Sättigungsfeuchtigkeit errechnet sich dementsprechend nach:

Die einzelnen Formelzeichen stehen für folgende Größen:


Das Mischungsverhältnis (Formelzeichen: "μ", "x", "m"), auch Feuchtigkeitsgrad oder Wasserdampfgehalt genannt, gibt die Masse des Wassers an, die sich in einer bestimmten Masse trockener Luft befindet. In ihren Eigenschaften sind Mischungsverhältnis und spezifische Luftfeuchtigkeit identisch. Im Regelfall unterscheidet sich auch der Zahlenwert nicht sehr stark, weshalb man beide Größen genähert gleichsetzen kann.

Das Mischungsverhältnis kann mit folgenden Formeln berechnet werden, wobei es über den ersten Term definiert ist und alle nachfolgenden Terme Äquivalente oder Näherungen hierzu darstellen (fL – feuchte Luft; tL – trockene Luft; W – Wasserdampf bzw. Wasser):

Die einzelnen Formelzeichen stehen für folgende Größen:


Als Taupunkt oder Taupunkttemperatur bezeichnet man die Temperatur, bei der sich auf einem Gegenstand (bei vorhandener Feuchtigkeit) ein Gleichgewichtszustand von kondensierendem und verdunstendem Wasser einstellt, mit anderen Worten die Temperatur, bei deren Unterschreitung Kondensatbildung gerade einsetzt. Sie wird mit einem Taupunktspiegelhygrometer gemessen. Der Taupunkt einer Probe ist lediglich vom Druck abhängig, wohingegen die relative Feuchtigkeit eine von Druck und Temperatur abhängige Größe ist. Die Taupunktkurve gibt bei gegebenem atmosphärischen Druck für die jeweilige Temperatur den Maximalwert von Feuchtigkeit an, die Luft aufnehmen kann (= 100 % relative Feuchtigkeit). Abkühlung der Luft unter die Taupunkttemperatur führt zu Kondensation, Erwärmung zu neuer Wasserdampfaufnahmefähigkeit.

Die Feuchttemperatur ist jene Temperatur, die ein Luftpaket haben würde, wenn es adiabatisch bei konstantem Druck durch Verdunsten von Wasser in das Paket, bis zur Sättigung gekühlt, und dabei die benötigte latente Wärme dem Paket entzogen werden würde. Gemessen wird sie mit Hilfe eines Psychrometers (zum Beispiel Aßmannsches Aspirationspsychrometer).
Bei Kenntnis von Temperatur und Luftfeuchtigkeit kann man die Feuchttemperatur aus einer sogenannten Psychrometertabelle ablesen.
Die Formel für die Feuchttemperatur lautet:

wobei

In der praktischen Anwendung wurden zahlreiche empirische Formeln entwickelt, die aber meist nur in einem bestimmten Temperatur- und Druckbereich gut funktionieren.

In der angewandten Meteorologie wird sie oft zur Unterscheidung der Niederschlagsart (Schnee/Regen) an unbemannten Wetterstationen eingesetzt. Als Richtwert gilt, dass Niederschlag bei einer Feuchttemperatur größer oder gleich 1,2 °C als Regen, bei Tf kleiner oder gleich 1,2 °C als Schnee fällt. Allerdings lassen sich damit nur grobe Abschätzungen vornehmen.
Jüngste Untersuchungen für die Station Wien Hohe Warte (WMO: 11035) haben gezeigt, dass Niederschlag bei Tf unter 1,1 bzw. über 1,4 °C in 2/3 der Fälle in fester bzw. flüssiger Form auftritt. Im Wesentlichen konnte der Richtwert von 1,2 °C Feuchttemperatur also bestätigt werden.

Geräte zur Messung der Luftfeuchtigkeit werden als Hygrometer bezeichnet. Arten sind zum Beispiel Absorptionshygrometer (Haarhygrometer), Psychrometer und Taupunktspiegelhygrometer.

Feuchtigkeitsensoren liefern ein elektrisches Signal, Absorptionssensoren beruhen auf einer sich bei unterschiedlicher Wasseraufnahme ändernden elektrischen Eigenschaft bestimmter Materialien und Materialaufbauten. Beispiele für elektrische Sensoren sind unter anderem Impedanz-Sensoren, hier ist es die elektrische Leitfähigkeit, die sich ändert. Bei kapazitiven Sensoren wirkt die Feuchtigkeit auf das Dielektrikum und ändert so die Kapazität des Sensors, bei schwingquarzbasierten Feuchtigkeitsensoren verändert sich durch die Feuchtigkeit die Resonanzfrequenz des Quarzes.

In den weltweiten offiziellen Wetterstationen werden zur Messung der Luftfeuchtigkeit verschiedene Messgeräte benutzt. Eine Methode ist ein in der Klimahütte montiertes Aspirationspsychrometer, welches aus einem trockenen und einem feuchten Thermometer besteht.
Aus den Werten beider Thermometer kann man anhand einer Tabelle dann die aktuelle relative Luftfeuchtigkeit in Prozent und den Taupunkt ermitteln.
Weiterhin gibt es separate Messfühler für den Taupunkt, welche aus einem Sensor über einer Lithiumchloridlösung bestehen.

Feuchtigkeitsindikatoren bestehen zum Beispiel aus mit Kobaltchlorid versetztem Silicagel (Blaugel) und führen bei bestimmten Feuchtigkeitswerten einen Farbwechsel aus. Sie dienen dazu, feuchtigkeitsempfindlichen Gütern beigelegt zu werden, um insbesondere in tropischen Gegenden und bei starken Temperaturunterschieden deren Transportbedingungen hinsichtlich der relativen Luftfeuchtigkeit kontrollieren zu können. Blaugel (oder das kobaltfreie Orangegel) wird auch in hermetisch verschlossenen Baugruppen hinter Sichtfenstern untergebracht, um die Luftfeuchtigkeit im Inneren kontrollieren zu können.

Die Luftfeuchtigkeit zeigt einen typischen Tagesgang, der zwar je nach Umgebungsbedingungen sehr unterschiedlich sein kann und auch nicht immer einem bestimmten Muster folgen muss, es aber im Regelfall tut. So zeigt sich für das sommerliche Berlin ungefähr der folgende Verlauf: um 7 Uhr Ortszeit liegt die absolute Luftfeuchtigkeit im Mittel bei etwa 10,6 g/m³, um 14 Uhr bei 10,0 g/m³ und schließlich um 21 Uhr wieder bei 10,6 g/m³. Im Winter belaufen sich die Werte auf morgens 4,5 g/m³, mittags 4,6 g/m³ und abends wiederum 4,5 g/m³. Die Luftfeuchtigkeit steigt also im Winter nach Sonnenaufgang und sinkt nach Sonnenuntergang mit dem Tagesgang der Lufttemperatur und so, wie man es aufgrund der erhöhten Verdunstung erwarten kann. Im Sommer kommt der Einfluss der Konvektion hinzu, da aufsteigende Luftpakete das Eindringen trockenerer Luftmassen aus der Höhe bedingen und daher zu einem mittäglichen bis nachmittäglichen Minimum führen. In den Abendstunden steigt die absolute Luftfeuchtigkeit mit nachlassender Konvektion wieder an. Im Sommer ergeben sich daher zwei Dampfdruckmaxima, eines um etwa 8 Uhr und eines um ungefähr 23 Uhr.

Der Verlauf der relativen Luftfeuchtigkeit erreicht nachts (insbesondere bei fehlender Bewölkung) in Bodennähe oft 100 %, da die Temperatur der bodennahen Luftschichten durch Kontakt mit dem sich durch Abstrahlung in den Weltraum abkühlenden Erdboden unter den Taupunkt fällt. An windstillen Tagen wird schon kurze Zeit (ab 20 min) nach Sonnenuntergang der Taupunkt an isolierten horizontalen Flächen (Autodach, Flachdach) unterschritten. Bei senkrechten Flächen (Autofenster, Verkehrsschilder) dauert es etwas länger. Die Folge sind Tau bzw. Reif.

Im Jahresgang, basierend auf entweder Tages- oder Monatsmitteln als langjährigen Durchschnittswerten, zeigen sich Maxima der relativen Luftfeuchtigkeit im Spätherbst und Frühwinter, also im Zeitraum der größten Nebelbildung. Demgegenüber stehen Minimalwerte im Frühjahr und Frühsommer. Der Dampfdruck ist im Winter am geringsten und im Sommer am höchsten. Die bestimmenden Einflüsse sind dabei Verdunstung und Advektion von Wasserdampf, die einen sehr starken regionalen bzw. lokalen Bezug aufweisen.

Der Wasserdampfdruck nimmt mit zunehmender Höhe und damit abnehmender Lufttemperatur zunächst sehr rasch und dann ab drei Kilometern nur noch langsam ab. In zehn Kilometern Höhe beträgt er dann nur noch etwa ein Prozent des Bodenwertes. Die relative Luftfeuchtigkeit zeigt keinen derart eindeutigen Trend, ist in der Tropopause, in Mitteleuropa etwa ab 11 Kilometern Höhe, jedoch meist sehr gering. Sie beträgt hier im Normalfall etwa 20 % und sinkt mit zunehmender Höhe weiter ab, was auch der Grund dafür ist, dass die Wolkenbildung fast ausschließlich auf die Troposphäre begrenzt ist.

Die Luftfeuchtigkeit ist in einer Vielzahl von Anwendungen von Bedeutung, wobei hier die Meteorologie und Klimatologie zwar deren theoretisches, nicht aber deren anwendungsorientiertes Zentrum bilden. Die Rolle des Wasserdampfes, dessen Eigenschaften und insbesondere seine technischen Anwendungen außerhalb der atmosphärischen Bedingungen werden dort erläutert. Die allgemeinen Eigenschaften des Wassers und dessen natürliche Verbreitung können gesondert nachgelesen werden.

Im Alltag lassen sich zahlreiche Phänomene auf die Luftfeuchtigkeit zurückführen, von denen einige hier exemplarisch vorgestellt werden sollen.

Beobachtet man nasse Gegenstände oder offene Wasserflächen über einen längeren Zeitraum, ohne dass diesen von außen weiteres Wasser zugeführt wird, so nimmt deren Nässe ab bzw. die Wasserfläche trocknet aus. Wäsche wird mit der Zeit trocken, Pfützen verschwinden, Lebensmittel werden hart und ungenießbar. Es kommt zur Verdunstung. Diese ist jedoch nur so lange möglich, wie die Luft ungesättigt ist, die relative Luftfeuchtigkeit also unter 100 % liegt.
Betritt man aus der kühleren Umgebung kommend einen geheizten Raum, so stellt man oft fest, dass Brillengläser beschlagen. Gleiches gilt auch für Fensterscheiben. Sind die Scheiben kälter als der Innenraum, so beschlagen sie. Zum Beispiel auch bei Kraftfahrzeugen wird dadurch das Sichtfeld eingeschränkt. Der gleiche Effekt tritt in Bädern und Saunen auf, hier beschlagen oft auch Spiegel und andere kältere Gegenstände. Grund für all diese Effekte sind die kalten Oberflächen, die die Luft in ihrer unmittelbaren Umgebung abkühlen: je höher die relative Luftfeuchtigkeit der Luft ist, desto schneller erreicht sie beim Abkühlen den Taupunkt und Wasser kondensiert. Je höher der Temperaturunterschied zwischen den Oberflächen und der Umgebungsluft ist, desto stärker ist die Neigung zur Betauung bzw. zum Beschlagen. Aus diesem Grunde zeigen sich die beschriebenen Fälle vor allem im Winter, in feuchten Räumen, an Außenwänden und im Freien nachts bei unbedecktem Himmel (Abkühlung der Erdoberfläche durch Abstrahlung in den Weltraum). Sinken die Temperaturen der Oberflächen unter 0 °C, bilden sich Eisblumen oder Reif.
Gegenmaßnahmen gegen Betauung und Bereifung:

Der Effekt führt auch zum Vereisen von Gefrierfächern bzw. des Verdampfers in Kühlschränken und Gefriertruhen bei gleichzeitiger Austrocknung unverpackter Kühlware. Deren Wasser verdunstet bzw. sublimiert zunächst, um dann an kalten Oberflächen zu kondensieren bzw. zu Eis zu resublimieren. Technische Verwendung findet dieser Effekt bei der Gefriertrocknung.

Die Vereisung von Vergasern von Ottomotoren (zum Beispiel in Kraftfahrzeugen oder kleinen Flugzeugen) führt zum Motorausfall. Sie beruht im Wesentlichen auf der Abkühlung der Luft aufgrund der Verdunstungskälte des Benzins, teilweise auch aufgrund des Unterdruckes, der die Luft zusätzlich abkühlt.
Die Unterschreitung des Taupunktes kann man auch bei Flugzeugen oder schnellen Rennautos beobachten. Die Randwirbel an den Enden der Tragflächen oder eines Spoilers führen zu einem lokalen Absinken des Luftdruckes und nach dem 2. Gesetz von Gay-Lussac zu lokaler Abkühlung der Luft. Der Taupunkt wird lokal unterschritten und dort entsteht Nebel. Ist die Luftfeuchtigkeit bei Temperaturen unter null besonders hoch, kommt es bei Flugzeugen zur gefürchteten Tragflächenvereisung – dann reicht bereits der Unterdruck oberhalb und hinter den Tragflächen und Leitwerken, um eine Bereifung auszulösen.

Die Ausatemluft ist beim Menschen und homoiothermen Tieren wesentlich feuchtigkeitsreicher und wärmer als die Einatemluft. Dies erkennt man am zu sichtbaren Nebelschwaden kondensierenden Wasserdampf der Ausatemluft im Winter bzw. bei niedrigen Temperaturen und hoher Luftfeuchtigkeit. Die warme und feuchtigkeitsreiche Ausatemluft kühlt sich unter den Taupunkt ab und es kommt zur Entstehung von Wassertröpfchen. Gleiches gilt auch für die Abgase von Fahrzeugen, Flugzeugen und Kraftwerken, deren Wolkenbildung bzw. Kondensstreifen oft mit deren Schadstoffemission verwechselt werden.

Wird mit Wasserdampf gesättigte Luft unter den Taupunkt abgekühlt, so scheidet sich flüssiges Wasser durch Kondensation aus der Luft ab, falls die hierfür notwendigen Kondensationskerne (Aerosole) vorhanden sind. Diese liegen jedoch unter natürlichen Bedingungen fast immer in ausreichender Konzentration vor, so dass es nur in Ausnahmefällen zu markanten Übersättigungen von mehreren Prozentpunkten kommt. Die Kondensation und ab Temperaturen unter 0 °C auch Resublimation des Wasserdampfs führen unter anderem zur Wolken-, Schnee-, Nebel-, Tau- und Reifbildung. Wasserdampf ist daher kein permanentes Gas der Atmosphäre und weist mit einer statistischen Verweildauer von etwa zehn Tagen eine hohe Mobilität auf.

Obwohl der Wasserdampf nur mit relativ geringen Konzentrationen in der Atmosphäre vertreten ist, trägt er bedingt durch seine hohe Mobilität und den damit verbundenen Stoffumsatz einen großen Anteil am globalen Wasserkreislauf und spielt daher in der Wasserbilanz eine wichtige Rolle. Hierbei ist die Luftfeuchtigkeit auch eine wichtige Eingangsgröße zur Niederschlagsbildung bzw. deren Berechnung und auch zur Bestimmung der Verdunstung bzw. der Evaporation, Transpiration und Interzeptionsverdunstung. Dies spielt im Rahmen der klimatischen Wasserbilanz wiederum eine wesentliche Rolle für verschiedene Klimaklassifikationen.

Aus der Luftfeuchtigkeit lassen sich zudem wichtige meteorologische Größen ableiten, wie zum Beispiel das Kondensationsniveau und die virtuelle Temperatur. Auch ist die Luftfeuchtigkeit bzw. der Wasserdampf wesentlich am Strahlungshaushalt der Atmosphäre beteiligt – Wasserdampf ist das bedeutendste Treibhausgas. Wasserdampf, insbesondere jedoch Wolken verhindern stark die nächtliche Abkühlung der Erdoberfläche, da sie durch Absorption und Re-Emission einen Ausgleich der Strahlungsbilanz der Wärmeabstrahlung der Erdoberfläche herstellen.

Die im flüssigen Aggregatzustand des Wassers gespeicherte latente Wärme bedingt den Unterschied zwischen feucht- und trockenadiabatischem Temperaturgradienten – eine der Voraussetzungen für die Entstehung von Föhn.

Luft von niedriger relativer Luftfeuchtigkeit ist ein häufig im Alltag angewandtes Trocknungsmittel, z. B. bei der Trocknung von Textilien auf der Wäscheleine. Bei der Trocknung von Materialien durch Verdunstung ist entscheidend, dass die Luftfeuchtigkeit hinreichend niedrig ist. Bei einer relativen Luftfeuchtigkeit von 100 % kann das Trockengut nicht weiter trocknen, es stellt sich ein Gleichgewicht ein. Bei Trocknungsverfahren, zum Beispiel in Trocknern, auch Wäschetrocknern, versucht man daher, die relative Feuchtigkeit der Umgebung zu senken. Das kann durch Temperaturerhöhung, Luftaustausch (Fön, Ablufttrockner), durch Adsorption des Wassers (Adsorptionstrockner) oder durch Auskondensation des Wassers (Kondenstrockner) erfolgen.

In anderen Fällen wird hingegen in der Regel auf die Wirkung des Windes vertraut, der ständig neue Luft von niedriger relativer Luftfeuchtigkeit heranweht und so beispielsweise Heu, frisch geschlagenem Holz, Mörtel, aufgehängter Wäsche, Tabakblättern, Kaffee- oder Kakaobohnen das Wasser entzieht.

In der Biologie und hier besonders der Ökologie ist die Luftfeuchtigkeit von großer Bedeutung. Sie bedingt nicht nur das Auftreten von Klimazonen oder bestimmten Ökosystemen, sondern spielt auch bei der Transpiration über die Spaltöffnungen der Blätter und in deren Interzellularraum (Interzellulare) eine große Rolle (Wasserdampfpartialdruck). Die Luftfeuchtigkeit ist daher ein wichtiger Parameter für den Wasserhaushalt von Pflanzen, Tieren und Menschen (Schwitzen, Atmen, Pilzbefall). Eine besondere Rolle spielt die Luftfeuchtigkeit zudem für jene Tiere, die hauptsächlich über die Haut atmen. Hierzu zählen viele Schnecken und andere Weichtiere, die in der Folge auch eine geringe Toleranz gegen Austrocknung besitzen.

Für Wohn- und Büroräume wird eine relative Luftfeuchtigkeit von 40 bis 50 % empfohlen. In kühlen Bereichen ist eine höhere Luftfeuchtigkeit erträglicher als in besonders warmen Bereichen (unterhalb 20 °C können auch über 70 % noch als behaglich empfunden werden). Unbehaglich sind generell Luftfeuchtigkeiten über 95 % und unter 23 %. Bei üblichen Bedingungen kann in beheizten Räumen (im Winter, besonders bei tiefer Außentemperatur) die Luft ohne aktive Luftbefeuchtung zu trocken werden. Andererseits sollte die Luftfeuchtigkeit im Schlafzimmer bei geschlossenen Fenstern generell etwas niedriger sein, da durch die Ausatmung die Luftfeuchtigkeit weiter ansteigt und bei einer Ausgangs-Feuchtigkeit von 60 % die Schwelle zur Schimmelbildung überschritten werden kann. Es empfiehlt sich, in den Wohnräumen ein Hygrometer aufzustellen, um die aktuelle Luftfeuchtigkeit zu messen und gegebenenfalls mittels regelmäßigem Stoßlüften oder Luftentfeuchtern entgegenzuwirken.

Vor allem in geschlossenen, stark belüfteten und gut beheizten Räumen werden die empfohlenen Werte oft unterschritten, was zu einer verminderten Atemleistung und einer Beeinträchtigung der Haut bzw. Schleimhaut führen kann. Dies ist besonders im Winter der Fall, da die kalte Außenluft dann nur eine geringe absolute Luftfeuchtigkeit besitzt und durch das Erwärmen auf Zimmertemperatur die relative Luftfeuchtigkeit sehr stark absinkt. Bei zu stark sinkender Luftfeuchtigkeit kann durch eine Reduzierung von Undichtigkeiten der ungewollte Luftaustausch verringert werden. Die Luftfeuchtigkeit sollte jedoch auch im Bereich der kältesten Stellen des Raumes (Außenwände hinter Möbeln) nicht über 80 % ansteigen, da bei höheren Werten Schimmelwachstum nicht auszuschließen ist. Je nach Nutzung und Wärmedämmung der Räume ergeben sich zur Vermeidung von Schimmelwachstum oft Werte der Luftfeuchtigkeit, die deutlich unter den medizinisch empfohlenen liegen.

In sehr kalten Gebieten oder auch kalten Jahreszeiten bzw. in der Nacht zeigt sich oft ein erhöhter Flüssigkeitsverbrauch des menschlichen Organismus, obwohl aufgrund des fehlenden Flüssigkeitsverlustes durch Schwitzen eher das Gegenteil angenommen werden müsste. Begründet liegt dies in der Befeuchtung der trockenen Einatemluft und dem damit verbundenen Wasserverlust. Wird die kalte Außenluft beim Einatmen erwärmt, so steigt deren Wasserdampfkapazität und senkt damit auch die relative Luftfeuchtigkeit. Im Gegensatz hierzu steigt das Sättigungsdefizit an und die Neigung des flüssigen Lungengewebswassers, in den gasförmigen Aggregatzustand überzugehen, nimmt zu. Im Sommer bzw. bei warmer Umgebungsluft wird die Einatemluft kaum noch zusätzlich erwärmt und behält daher ihre meist hohe relative Luftfeuchtigkeit. Sind die zusätzlichen Wasserverluste durch Schwitzen hier nicht allzu groß, ist der Wasserbedarf des Körpers daher bei kalten Umgebungsbedingungen höher.

Eine zu niedrige Luftfeuchtigkeit ist für die Atmung nicht förderlich, da der Sauerstoff über die Alveolen dann schlechter in die Blutbahn gelangt. Die Haut benötigt eine hohe Luftfeuchtigkeit, um nicht auszutrocknen, da diese eng mit der Hautfeuchtigkeit gekoppelt ist. Besonders Schleimhäute sind für Austrocknen anfällig, da sie nur über einen geringen Verdunstungsschutz verfügen und auf ihre hohe Feuchtigkeit zur Erhaltung ihrer Funktionen angewiesen sind. So kann eine geringe Feuchtigkeit der Nasenschleimhaut ein erhöhtes Auftreten von Nasenbluten zur Folge haben. Generell wird dabei auch die Immunabwehr der Haut geschwächt (erhöhtes Erkältungsrisiko) und deren Fähigkeit zum Stoffaustausch herabgesetzt, wovon besonders die Mundschleimhaut betroffen ist. Auch die Anfälligkeit für Hautreizungen bzw. -rötungen oder gar Hautentzündungen wird durch eine geringe Luftfeuchtigkeit erhöht. Wenn diese Entzündungen nur in bestimmten Räumen oder Gebäuden auftreten ist dies in der Regel auf eine zusätzliche Belastung der Raumluft mit Schadstoffen (z. B. Feinstaub, Lösungsmittel, Formaldehyd usw.) zurückzuführen.

Bei der Durchführung von Inhalationsnarkosen ist die Anfeuchtung des inhalierten Gasgemisches sehr wichtig, da die zur Anwendung kommenden medizinischen Gase wasserfrei gelagert werden und andernfalls die auftretenden Verdunstungseffekte in der Lunge des Patienten Auskühlungserscheinungen (Verdunstungskälte) und eine gewisse Austrocknung bewirken würden.

Eine hohe relative Luftfeuchtigkeit behindert hingegen die Regulation der Körpertemperatur durch das Schwitzen und wird daher schnell als schwül empfunden. Trotz höherer Temperaturen können daher sehr heiße Wüsten oft wesentlich leichter durch den Organismus verkraftet werden (vorausgesetzt er leidet nicht unter Austrocknung) als Regenwälder mit einer hohen Luftfeuchtigkeit und vergleichsweise gemäßigten Temperaturen. Die Auswirkung der Luftfeuchtigkeit auf die gefühlte Temperatur wird durch den Humidex beschrieben, wobei der grundsätzliche Zusammenhang zwischen einer steigenden Luftfeuchtigkeit und einer steigenden gefühlten Temperatur auch für niedrige Werte der Luftfeuchtigkeit gilt und somit beispielsweise zur Reduzierung der Zimmertemperatur und damit des Heizaufwandes herangezogen werden kann.

In der Landwirtschaft besteht bei einer zu niedrigen Luftfeuchtigkeit die Gefahr einer Austrocknung der Felder und der angebauten Pflanzen und damit einer Missernte. Durch die Erhöhung des Dampfdruckgradienten zwischen Blattoberfläche und Atmosphäre wird den Pflanzen dabei Feuchtigkeit entzogen (siehe Abschnitt Biologie), insbesondere wenn ihre Spaltöffnungen am Tag geöffnet sind und sie nur über einen geringen Verdunstungsschutz verfügen, was bei vielen heimischen Pflanzen (C-3-Pflanzen), der Fall ist. Die Pflanzen erhöhen dadurch die Austrocknung des Bodens, andererseits schützen sie ihn vor direkter Sonneneinstrahlung und Erwärmung und fördern durch ihre Wurzeln Wasser aus tieferen Schichten an die Oberfläche. Viele Moor- und Sumpfpflanzen verfügen über einen Regelmechanismus, der die Verdunstungsrate bei beginnender Austrocknung senkt.

Die Wasserbilanz wird beim Freilandanbau wesentlich auch durch nächtlichen Tau verbessert – Pflanzen betauen eher als unbedeckter Erdboden, da sie sich nachts durch Wärmeabstrahlung schneller abkühlen als unbedeckter Boden mit seiner höheren Wärmekapazität.

Doch auch in der Forstwirtschaft und der holzverarbeitenden Industrie spielt die Luftfeuchtigkeit eine Rolle. Frisch geschlagenes Holz verfügt über eine hohe Eigenfeuchtigkeit, sie ist bei im Winter geschlagenem Holz geringer. Diese Holzfeuchtigkeit sinkt in der Zeit der Ablagerung ab und gleicht sich an die Luftfeuchtigkeit an. Wird zu frisches Holz verarbeitet, schwindet und verzieht es sich. Die Änderung der Holzfeuchtigkeit aufgrund wechselnder Luftfeuchtigkeit führt auch bei abgelagertem Holz zu sich ändernden Maßen des Holzes quer zur Faser und ist von großer Wichtigkeit für alle holzverarbeitenden Gewerbe und Industrien. Bei der Lagerung frischen Holzes in Sägewerken werden oft Sprinkleranlagen eingesetzt, um das Holz langsamer zu trocknen und so Schwindungsrisse zu vermeiden.

Auch abgelagertes Holz (Bretter, Kanthölzer und Balken) wird so gelagert, dass es von Luft umströmt wird und durch sein Eigengewicht parallel fixiert ist. Das soll garantieren, dass sich das Holz nicht verzieht oder gar fault.
Beim Verlegen von Dielen- und Parkettfußböden muss beachtet werden, dass sich das Holz aufgrund seiner Hygroskopizität der Umgebungsfeuchtigkeit anpasst. Unterhalb des Fasersättigungsbereiches führt dies zur Quellung oder Schwindung des Holzes. Aus diesem Grund werden auch Holzfässer bei Nichtbenutzung undicht.

In der Lagerhaltung von Lebensmitteln ist die Luftfeuchtigkeit sehr wichtig zur Steuerung der Genussreife, vor allem bei Lagerobst. Auch Korrosion kann durch eine hohe Luftfeuchtigkeit begünstigt werden, besonders über den indirekten Effekt der gesteigerten Taubildung, und muss daher bei Lagerung und Transport feuchtigkeitsempfindlicher Güter berücksichtigt werden. Beispiele, die bestimmte Luftfeuchtigkeit erfordern, sind Chemikalien, Zigarren (Humidor), Wein (Korken), Salami, Holz, Kunstwerke, Bücher und optische oder elektronische Baugruppen und Bauteile, zum Beispiel integrierte Schaltkreise. Die Luftfeuchtigkeit muss zur Einhaltung bestimmter Raumklimata in Lagerräumen, Museen, Archiven, Büchereien, Laboren, Rechenzentren und industriellen Produktionsanlagen (Mikroelektronik-Fertigung) überwacht oder gesteuert werden.

Beim Gütertransport in wetterisolierten Containern oder auch verschweißten Kunststoffbeuteln kann sich Kondenswasser und Betauung bilden, wenn die Luft im Inneren beim Sinken der Temperatur unter den Taupunkt gelangt, zum Beispiel beim Transport aus tropischen in kältere Gebiete. In Folienverpackungen feuchtigkeitsempfindlicher Güter werden daher Beutel mit Silicagel oder Zeolithe gegeben, die die Feuchtigkeit puffern. Feuchtigkeitsindikatoren dienen dazu, die Feuchtigkeitswerte in den Verpackungen während des Transports zu kontrollieren.
Feuchtigkeitsempfindliche Geräte wie z. B. in der Elektronik und Optik müssen nach Lagerung bei geringen Temperaturen zunächst temperieren, bevor deren Verpackung geöffnet wird. Ansonsten bildet sich an und in den Geräten Kondenswasser, was insbesondere beim sofortigen Betreiben der betauten Geräte zum Ausfall führen kann.

In der Bauphysik spielt der Taupunkt in Form der Taupunktebene eine wichtige Rolle. Unter dieser versteht man diejenige Fläche innerhalb des Mauerwerks oder der Wärmedämmung an der Außenwand eines Gebäudes, ab welcher es zur Kondensation kommen kann. Hintergrund ist, dass warme Luft mehr Feuchtigkeit aufnehmen kann als kalte Luft. Bewegt sich warme und mit Feuchtigkeit angereicherte Luft durch Diffusion oder Konvektion innerhalb der Außenwand oder Dämmschicht vom wärmeren zum kälteren Ort (im Winter meist von innen nach außen), so kommt es zur Bildung flüssigen Wassers, sobald der Taupunkt unterschritten wird. Hieraus ergeben sich Gefahren gesundheitsgefährdender Schimmelbildung oder die Dämmschichten versagen aufgrund der Wasseraufnahme.
Gegenmaßnahmen bestehen folglich darin, eine Taupunktunterschreitung durch geeignete Baumaterialien oder andere Maßnahmen zu vermeiden. Die Wärmedämmung sollte daher möglichst an der Außenseite der Wand angebracht werden und ihrerseits nach außen diffusionsoffen sein, sodass sie Wasser an die trockene Außenluft abgeben kann. Ist dies nicht möglich (zum Beispiel bei Innendämmung), muss die Wärmedämmschicht nach innen mit einer Dampfsperre (geschlossene Folie, keine Wasserdiffusion möglich) oder Dampfbremse (Wasserdiffusion ist eingeschränkt möglich) versehen sein, um das Eindringen feuchter Raumluft in die Wärmedämmschicht zu verhindern. Das ist insbesondere dann wichtig, wenn das Mauerwerk, zum Beispiel durch einen Außenanstrich, ein geringes Diffusionsvermögen aufweist.

In der Winterperiode – in diesem Zusammenhang oft als Tauperiode bezeichnet – sind die Temperatur und der Wasserdampfdruck im Inneren höher als außen. Die Außenwand weist daher für beide Werte ein Gefälle nach außen auf. Dieses ist jedoch selbst bei einer homogenen Außenwand nicht gleich, da deren zeitabhängige Speicherwirkung für Wärme und Wasserdampf unterschiedlich ist und sich auch die Temperaturen und Dampfdrücke im Zeitablauf unterschiedlich ändern. Bei inhomogenen Wänden kommt hinzu, dass das Gefälle in den einzelnen Materialien unterschiedlich ist. So hat eine Dampfsperrfolie zum Beispiel ein großes Dampfdruckgefälle, hingegen kaum ein Temperaturgefälle. Bei Dämmstoffen ist es oft umgekehrt, hier ist das Gefälle des Wasserdampfdrucks klein, aber das Temperaturgefälle hoch. Kondensation tritt immer dann ein, wenn die relative Luftfeuchtigkeit örtlich vorübergehend oder (zum Beispiel im Winter) dauernd 100 % überschreitet.

Die Kondenswasserbildung kann auch durch Baustoffe mit hoher Wasserdampfdurchlässigkeit und/oder einem hohen Wasseraufnahmevermögen (Pufferung) bei gleichzeitig geringer Wärmeleitfähigkeit verhindert werden. Beispiele sind Stroh/Lehm oder Holz. Hierbei kann oft auf Dampfsperren verzichtet werden.

Das sachgemäße Belüften von Wohnräumen (insbesondere bei Sanierungen mit Außenanstrich, unsachgemäß angebrachten Dampfsperren und abgedichteten Fenstern) hat einen großen Einfluss auf die Vermeidung von Schimmelbildung.

"Siehe auch:" Niedrigenergiehaus, Baubiologie.

In der Luftfahrt besteht die Gefahr des Vereisens von Tragflächen und Leitwerk durch die Resublimation des in der Luft enthaltenen Wasserdampfes. Dieser Effekt kann die Flugfähigkeit binnen kürzester Zeit sehr stark einschränken und ist für zahlreiche Unfälle verantwortlich. Entgegengewirkt wird diesem Vorgang durch Enteisungsanlagen, welche die kritischen Bereiche (zum Beispiel Tragflächenvorderkante) beheizen, um Eisansatz zu verhindern.

Eine preisgünstigere Methode besteht darin, die Tragflächenvorderkante mit einer Haut aus Gummi zu überziehen und stoßweise Druckluft zwischen die Gummihaut und die Tragfläche zu pressen. Die Haut wölbt sich und durch die Verformung wird das starre Eis abgesprengt. Diese Methode birgt allerdings ein gewisses Risiko. Ist der entstandene Eispanzer zum Zeitpunkt der Auslösung der Druckluft-Enteisung noch dünn, wird er durch die Gummihaut lediglich gewölbt, aber nicht gesprengt. In der Folge lagert sich weiteres Eis an, die erneute Auslösung der Enteisung bleibt ergebnislos. Um diesem Risiko entgegenzuwirken warten Piloten oft mit der Betätigung der Enteisung, bis sie der Ansicht sind, dass diese auch den tatsächlich gewünschten Effekt zu erzielen vermag.

In der Raumfahrt kommt es bei Raketenstarts zu ähnlichen durch niedrige Außentemperaturen bedingten Problemen. Startfenster werden daher auch nach meteorologischen Gesichtspunkten gewählt und Starts notfalls abgebrochen. Die Nichtbeachtung dieses Grundsatzes kann zum Absturz führen.

Die Luftfeuchte ist eine wichtige Kenngröße beim Füllen von Druckluftflaschen von z. B. Pressluftatmern. Dafür wird die Luftfeuchtigkeit nach DIN EN 12021 „Druckluft für Atemschutzgeräte“ als maximaler Wassergehalt der in Druckluftflaschen gelagerten Luft und der am Ausgang des Kompressors gemessenen Luft, also die absolute Luftfeuchtigkeit a, d oder f, vorgegeben.

Nach DIN EN 12021 Druckluft für Atemschutzgeräte darf der Wassergehalt in Druckluftflaschen maximal betragen:

Die absolute Luftfeuchtigkeit der vom Kompressor gelieferten Luft zum Füllen von 200-bar- oder 300-bar-Druckluftflaschen sollte 25 mg/m nicht überschreiten.
Die Luftfeuchte wird im Atemschutz mit Prüfröhrchenmessgeräten gemessen. Die Maßeinheit bezieht sich jeweils auf auf Atmosphärendruck entspannte Luft.

An Wärmetauschern und kalten Rohrleitungen, die kälter als die Umgebungsluft sind, kann Kondensation von Luftfeuchtigkeit und bei Unterschreiten des Gefrierpunkts auch Vereisung auftreten.

Im Inneren eines Kühlschranks, der daher in der Regel knapp über dem Gefrierpunkt betrieben wird, tritt daher Kondenswasser auf. Ehemals (um 1960/1975) bildete die – einzige – Kühlfläche als horizontale Ebene aus eloxiertem Aluminium den Boden des Gefrierfachs und lag so etwas abgeschirmt über dem Kühlraum. Die Kühlfläche vereiste mit der aus der Raumluft, und aus wasserhältigen Nahrungsmitteln kommenden Luftfeuchtigkeit und musste daher etwa wöchentlich "abgetaut" werden. Das Eis schmolz dann und tropfte entweder in eine ständig im Kühlschrank eingeschobene Vorrichtung aus dach- und kanalförmigen Stegen in eine Auffangwanne, die händisch herauszuziehen und zu leeren war. Spätere, nicht mehr mit Glaswolle, sondern durch Ausschäumen besser isolierte Geräte, hatten eine durchgehende Wanne aus Kunststoff mit einem im Kühlraum hintenliegenden Ablaufstutzen, dessen Stoppel zum Abtauen geöffnet wird, um das Tauwasser in ein daruntergestelles Gefäß anzulassen. Seit etwa 1980 bildet die rückseitige nahtlos aus geblasenem Kunststoff gebildete Rückwand die Kühlfläche des Kühlraums. Hier kondensiertes Wasser – eventuell während einer Kühlphase vorübergehend gefroren – rinnt nach unten ab in eine eingeformte Rille und weiter durch einen stets offenen Auslass, in eine Kunststofftasse außen am warmen Kühlaggregat und verdunstet dort. Solche Kühlschränke sind selbstabtauend. Das über mit Magnetleisten gefüllte Kunststoffwulste weitgehend luftdicht und damit fast wasserdampfdicht geschlossene Gefrierfach wird nur selten geöffnet und baut deshalb nur wenig Eis auf einer eigenen Kühlfläche auf, das manuell abgetaut werden muss.

Wenn sommers der Taupunkt von Luft in Kellern von Häusern steigt, kondensiert Luftfeuchte auf dem Rohr einer durchflossenen Trinkwasserleitung.

Eine Reihe von Gasen (Propan, Butan, CO, Lachgas) wird unter Druck verflüssigt in Druckflaschen, Kartuschen oder kleinen Patronen aus Metall vorrätig gehalten. Aus der Gasphase mit ausreichend großer Rate entnommene Mengen werden durch Verdunsten oder Sieden aus der Flüssigphase nachgeliefert, wodurch sich diese abkühlt, was an der Außenseite der aufrecht stehenden Flasche zu flüssiger Kondensation von Luftfeuchte und bei ausreichend niedriger Umgebungstemperatur zu Reifbildung führt, die sichtbar die Spiegelhöhe des Flüssigphase des Inhalts abzeichnet.

Wird nicht speziell entfeuchtete Druckluft aus einem Kessel rasch entlassen, kühl sich die Luft im Strahl beim Entspannen so weit ab, dass mitgerissene Umgebungsluft unter ihren Taupunkt abgekühlt werden kann, sodass sich temporär und lokal ein wenig Nebel bildet. Ein ähnlicher Effekt tritt beim raschen Öffnen eines aufrechten Gefässes eines Getränks auf, das unter einem gewissen Druck Kohlenstoffdioxid enthält. Wenn das Getränk nicht herausschäumt ist kurz eine kleine Nebelschwade über der Öffnung der Flasche oder Dose sichtbar.

In Trinkgläser kalt eingeschenkte Getränke lassen außen Luftfeuchte kondensieren. Um Tische zu schonen werden Bierdeckel untergelegt. Stielgläser behalten zumeist den Stiel trocken, solange sich der Belag aus feinen Tropfen nicht zu größeren zusammengeballt hat, die abrinnen. Über Stiele von Pilstulpen werden oft Pilsdeckchen gestülpt, die abrinnenden Schaum und Kondenswasser aufsaugen sollen.

An Außenwänden montierte Klimaanlagen lassen im gekühlten Luftstrom Wasser auskondensieren. Geringe Mengen flüssigen Wassers werden so mitunter über kleine Rohre auf den vor einem Geschäftslokal liegenden Gehsteig geleitet.

Entfeuchter bis hinunter zu reisetaschenkleinen Geräten funktionieren durch Abkühlen durchgeblasener Luft bis unter den Taupunkt, Abrinnen des auf den Kühlflächen kondensierten Wassers in ein Sammelgefäß und mehr als Wiedererwärmen der Luft. Typisch wird die Kompressorkältemaschine von einem Elektromotor angetrieben.

Der Einsatz hygroskopischer Stoffe (fest, selten flüssig) empfiehlt sich nur für kleine Luftvolumina. Elektronikgeräten aber auch schimmelanfälligen Lederwaren werden kleine Papiersäckchen von getrocknetem Silicagel beigepackt um Feuchte die beim Seetransport in Containern durch Kartonverpackungen diffundiert und durch Abkühlen kondensieren kann bis zu einer gewissen Menge zu binden. Zwischen wasserdampfdichte Lagen von Glas oder Kunststofffolie und ähnlichem wird häufig Seidenpapier oder ähnliches als Zwischenlage gepackt, um den Feuchtigkeitsaustausch zu fördern, um flüssiges Kondenswasser und damit einhergehende Transportvorgänge und Kapillareffekte zu vermeiden.

Im Chemielabor werden Stoffe oft wasserfrei benötigt, um sie ohne Wassergehalt zu verwiegen oder wasserfrei zu verarbeiten. Die Trocknung erfolgt grob an Luft, mehr oder weniger scharf durch Erwärmen eventuell bis zum Glühen. Luftfeuchte bewirkt beim Abkühlen das Wiederaufnehmen von Wasser. Deshalb werden Stoffe in Schalen im Exsikkator neben oder über Trocknungsmitteln gelagert. Der zu trocknende Stoff setzt – bei Raumtemperatur – Wasserdampf als Luftfeuchte frei und z. B. Silikagel, Kalziumchlorid oder konzentrierte Schwefelsäure nimmt den Wasserdampf aufgrund höherer Hygroskopizität auf. Das Absaugen von Luft aus dem Exsikkator erfolgt zumeist mit der Wasserstrahlpumpe, dadurch wird das Austreten von Wasserdampf (und anderer Dämpfe) aus der Probe und das Diffundieren des Wasserdampfs zum Trocknungsmittel hin erleichtert. Durch das Erzeugen eines Vakuums von hinunter bis zu etwa 1/100 bar steigt die absolute Luftfeuchtigkeit auf bis zum Hunderfachen an. Wenn nun beispielsweise Wasser mit Umgebungstemperatur (z. B. 20 °C) im Exsikkator als Wasserdampfquelle vorliegt, verändert sich die relative Luftfeuchtigkeit nach Gleichgewichsteinstellung nicht. Denn der Wasserdampfdruck bei 20 °C bewirkt (ideal betrachtet) unabhängig von nebenbei im selben Volumen vorhandener Luftmoleküle stets eine Sättigung mit Wasserdampf, also 100 % relative Feuchte. 

Eine Wasserstrahlpumpe wird zweckmässig mit kaltem Wasser betrieben, da sie in Richtung Vakuum eine Wasserdampfquelle der Temperatur der Pumpe darstellt. Am Exsikkator wird sie zum Absaugen organischer Dämpfe (z. B. von Lösemitteln) eher nur intermittiert und nicht langdauernd eingesetzt.

Beim Gefriertrocknen wird Gefrorenes, oft Lebensmittel schonend, weil ohne Erhitzung, im Vakuum getrocknet. Dabei wird verdunstender Wasserdampf im Vakuum angesaugt, Aromastoffe die weniger flüchtig als Wasser sind oder stärker am Stoff anhaften bleiben diesem erhalten.




</doc>
<doc id="8895" url="https://de.wikipedia.org/wiki?curid=8895" title="Homosexualität">
Homosexualität

Homosexualität bezeichnet je nach Verwendung sowohl gleichgeschlechtliches sexuelles Verhalten, erotisches und romantisches Begehren gegenüber Personen des eigenen Geschlechts als auch darauf aufbauende Identitäten – etwa sich selbst als lesbisch oder schwul zu definieren.
Homosexuelles Verhalten, homosexuelles Begehren und die "Sexuelle Identität" fallen nicht zwingend zusammen und werden deshalb in der Forschung unterschieden. In der Umgangssprache werden diese Aspekte jedoch häufig vermischt oder miteinander gleichgesetzt. Sexuelle Handlungen zwischen Männern und zwischen Frauen wurden in verschiedenen Epochen und Kulturen ganz unterschiedlich behandelt: teils befürwortet und toleriert, teils untersagt und verfolgt. Eine besondere Rolle spielen dabei die drei Abrahamitischen Weltreligionen, deren Schriftgelehrte den sexuellen Verkehr zwischen Männern auf der Basis von Bibel, Tora und Koran in der Regel als Sünde betrachteten, auch wenn liberale Strömungen mit dieser exegetischen Tradition heute zunehmend brechen.

Gleichgeschlechtliche Liebe und Lust sind in allen Gesellschaften und historischen Epochen durch entsprechende Quellen nachweisbar. Dagegen gilt die Entstehung der sexuellen Identität – im Sinne einer klaren Festlegung des Individuums auf eine bestimmte sexuelle Orientierung – heute als das Resultat von Entwicklungen der modernen Gesellschaft. Diese setzten ungefähr im 18. Jahrhundert unserer Zeitrechnung ein und umfassen Aspekte wie das Städtewachstum, die Bürokratisierung und die kapitalistische Versachlichung sozialer Beziehungen. Parallel zur Herausbildung heterosexistischer Normen in der Mehrheitsgesellschaft entstanden nach und nach in fast allen europäischen Metropolen abgegrenzte „schwule“ Subkulturen, deren Angehörige schon bald zum Gegenstand polizeilicher Überwachung, staatlicher Verfolgung, krimineller Erpressung und teilweise auch gewaltsamer Übergriffe wurden.

Die erste fundierte Verteidigung der Homosexualität schrieb mit "Eros. Die Männerliebe der Griechen" (1. Band 1836, 2. Band 1838) der Schweizer Modist und Tuchhändler Heinrich Hössli. Er begründete diese mit seiner Überzeugung, dass diese Veranlagung angeboren sei. Der Begriff Homosexualität wurde 1869 durch den österreichisch-ungarischen Schriftsteller Karl Maria Benkert (Pseudonym: "Karl Maria Kertbeny") erfunden. Zuvor hatte Karl Heinrich Ulrichs (* 1825) die Begriffe "Uranismus" (bzw. "Urning" für männliche Homosexueller, "Urninde" für weibliche Homosexuelle) verwendet und bekannt gemacht. Ulrichs forderte 1867 erstmals öffentlich – auf dem deutschen Juristentag in München vor 500 Mitgliedern – die Straflosigkeit homosexueller Handlungen. Es gab tumultartige Szenen, in denen seine Rede unterging.

Gegen Ende des 19. Jahrhunderts prägten Autoren aus dem Umfeld der modernen Sexualwissenschaft unsere heutigen Begriffe für Homo- und Heterosexualität, für die es, genau wie für den Begriff Sexualität selbst, in keiner Sprache bis dahin eine vergleichbare Entsprechung gab. Das internationale Vokabular zu diesem Thema stammt daher fast überall aus Wortneuschöpfungen und Lehnübersetzungen des letzten und vorletzten Jahrhunderts.

Die wissenschaftliche Erkenntnis, dass die damit verbundene Einteilung von Menschen in Homo- und Heterosexuelle den vielfältigen Schattierungen menschlichen Begehrens nicht wirklich gerecht wird, hat man seit 1900 mit der Residualkategorie der Bisexualität aufzufangen versucht. Jedoch ist dieses Konzept seinerseits zur Basis einer selbst gewählten Identität geworden und produziert daher neue begriffliche Unklarheiten, wie etwa die Existenz von Menschen, die sich in Umfragen weder als homo- noch bisexuell einstufen, sich aber trotzdem vom eigenen Geschlecht in unterschiedlichem Grade erotisch angezogen fühlen.

Mit der Konstruktion homosexuellen Begehrens als Abweichung von einer unterstellten „heterosexuellen Norm“ war von Anfang an auch der Versuch einer ätiologischen (medizinischen/psychologischen) Erklärung verknüpft. Nach 150 Jahren Forschung gibt es unter Sexualwissenschaftlern immer noch keinen Konsens, welche Faktoren für die Ausbildung sexueller Präferenzen ursächlich sind. Genannt wurden unter anderem genetische, endokrinologische (hormonelle) und psychoanalytische Erklärungsmodelle, die meist wenig miteinander vereinbar sind und somit in Konkurrenz zueinander stehen. In der Forschung hat sich heute weitgehend eine Deutung durchgesetzt, die auf der gesicherten Beobachtung aufbaut, dass homosexuelles Verhalten eines Teils von Populationen in der höheren Tierwelt sehr weit verbreitet ist. Solchem Verhalten wird demgemäß eine mögliche evolutionäre Funktion für den Abbau von Aggressionen und die soziale Integration bei komplexen, hochentwickelten Wirbeltiergesellschaften beigemessen. Homosexuelles Verhalten von Teilen einer Population hochentwickelter Lebewesen ist demnach ein durch die natürliche Evolution entstandenes, in der belebten Natur weit verbreitetes und sinnvolle Funktionen erfüllendes Phänomen.

Die Bezeichnung „Homosexualität“ ist eine hybride Wortneubildung aus dem Jahre 1868, geprägt vom Schriftsteller Karl Maria Kertbeny (1824–1882, bürgerlich: Karl Maria Benkert) von griech. "homόs" „gleich“ und lat. "sexus" „Geschlecht“. Gleichzeitig prägte er als Antonym die Bezeichnung „Heterosexualität“. Richard von Krafft-Ebing sorgte ab 1886 mit seinem Werk "Psychopathia sexualis" für eine weite Verbreitung der Neubildungen.

Hintergrund für diese und andere Wortbildungen war, dass es in der Neuzeit bis Mitte des 19. Jahrhunderts keinen überlieferten Begriff für gleichgeschlechtliches Empfinden gab. Vier Jahre vor Kertbeny führte Karl Heinrich Ulrichs 1864 die Begriffe „Uranismus“, „Urning“ (männlich) und „Urninde“ (weiblich) ein. Zwei Jahre nach Kertbeny und noch vor dem Erscheinen der "Psychopathia sexualis" prägte Carl Westphal 1870 den Begriff der „conträren Sexualempfindung“. Alle drei Begriffe wurden je nach Vorliebe verwendet. Magnus Hirschfeld berichtet 1914, dass sich der Begriff „Homosexualität“ durchgesetzt hat.

Als problematisch empfand Hirschfeld dabei, dass unter dem Eindruck der Endung "-sexuell" das Wort vielfach nicht im Sinne gleichgeschlechtlicher Artung erfasst und gebraucht wird, sondern im Sinne einer sexuellen Handlung. Tatsächlich handelt es sich hierbei um eine bis heute bestehende Polysemie (Mehrdeutigkeit). So wies Ernest Bornemann 1990 auf öffentliche Umfragen hin, nach denen die Mehrzahl der Deutschen den Begriff so versteht, dass Homosexualität weniger eine Orientierung als vielmehr den „Geschlechtsverkehr unter Männern“ bezeichnet.
Der Umstand, dass "homo" im Lateinischen „Mann“ (aber auch „Mensch“) und nicht, wie im Griechischen, „gleich“ bedeutet, führt in Verbindung mit der Tatsache, dass der zweite Teil des Wortes aus dem Lateinischen stammt, häufig zu einer irrtümlichen Verengung der Wortbedeutung auf „männliche Homosexualität“. Dies hat bisweilen skurrile Wendungen zur Folge, wie etwa die redundante Formulierung „Homosexuelle und Lesben“.

Die Verengung auf mann-männliche Sexualität trifft vor allem auf das Substantiv „Homosexueller“ zu, dessen weibliches Gegenstück „Homosexuelle“ kaum im Gebrauch ist. Homosexuelle Frauen werden stattdessen als "Lesben" (nach der griechischen Insel Lesbos, Heimat der Frauen liebenden Dichterin Sappho) oder, veraltet, als "Lesbierinnen" bezeichnet. Analog existiert für homosexuelle Männer auch das Wort "Schwuler" (von schwül – „drückend heiß“, in dieser Bedeutung seit dem 18. Jahrhundert, „schwül“ als Parallelbildung zu „kühl“, oder von „Schwulität“ – „Schwierigkeit, Bedrängnis, peinliche Lage“).

Ursprünglich abwertend oder nur im Rahmen der eigenen Subkultur verwendet, wurde die Bezeichnung „schwul“ in den 1970er Jahren von der Homosexuellenbewegung als Kampfbegriff eingesetzt und dadurch so weit gesellschaftsfähig gemacht, dass der Begriff heute sogar im Sprachgebrauch der Gesetzgebung auftaucht. Durch seinen trotzigen Gebrauch als Selbstbezeichnung sollte der abwertende Charakter des Wortes zurückgedrängt und den Homosexuellenfeinden enteignet werden. Dies ist jedoch nur zum Teil gelungen. In der Jugendsprache findet das Wort „schwul“ nach wie vor (oder sogar verstärkt) als Schimpfwort Verwendung und wird dort häufig auch als Synonym für „langweilig“, „weichlich“ beziehungsweise „enervierend“ benutzt. Synonym werden häufig „Tunte“ und „Schwuchtel“ verwendet.

Um das Missverständnis zu vermeiden, „Homosexualität“ bezöge sich begrifflich nur auf Männer, wurde seit 1900 als Alternative die deutsche Übersetzung Gleichgeschlechtlichkeit ins Spiel gebracht, und zwar vor allem als Adjektiv "(gleichgeschlechtlich)," weniger als Substantiv oder gar als Personenbezeichnung "(Gleichgeschlechtlicher)". Der Begriff verhindert eine Verwechslung mit dem Begriff Mann und setzt den semantischen Fokus von der Sexualität weg auf das Geschlecht. Auch in Gesetzesentwürfen und Gesetzen wird er neuerdings wegen der juristischen Klarheit bevorzugt. Häufigen Gebrauch findet er darüber hinaus bei der Beschreibung anderer Kulturen, da das Wort "homosexuell" gedanklich mit vielen sozialen Eigenheiten und Identitätsbeschreibungen der westlichen industrialisierten Welt verknüpft ist.

Sprachlich überholt ist die Bezeichnung Homosexualismus, die ebenfalls von Kertbeny eingeführt, aber seit jeher nur vereinzelt verwendet wurde. In neuerer Zeit wird „Homosexualismus“ in meist abwertender Weise von wertkonservativen, oft gläubigen Menschen gebraucht. Zum Teil wird es dabei als bloßes Synonym für Homosexualität benutzt, zum Teil aber auch, um die Überzeugung auszudrücken, dass über Straffreiheit und körperliche Unversehrtheit hinausgehende Forderungen eine „Ideologie“ darstellten, die den eigenen Anschauungen widerspricht. Darüber hinaus findet auch Homotropie als Bezeichnung für das sexuelle, erotische und partnerschaftliche Hingezogensein zum eigenen Geschlecht (selten) Verwendung (siehe auch Antonius M. J. M. Herman van de Spijker).

Im englischsprachigen Raum hat die Lesben- und Schwulenbewegung dagegen das Wort "gay" (im nachträglichen Rückgriff auf seine ursprüngliche Bedeutung „fröhlich“ und „bunt“, die zwischenzeitlich vom 17. bis ins 20. Jahrhundert hinein allerdings von der Bedeutung „ausschweifend, unmoralisch“ verdrängt worden war) als Selbstbezeichnung durchgesetzt, um sich von dem damals noch abwertend gebrauchten Ausdruck "queer" („seltsam, komisch“) zu distanzieren.

Ursprünglich eine geschlechtsneutrale Bezeichnung, hat sich der Begriff – ähnlich wie das deutsche Wort "schwul" – in den 1970er Jahren auf Männer verengt, während sich gleichgeschlechtlich liebende Frauen im Zuge des lesbisch-feministischen Separatismus zunehmend als "lesbians" und "dykes" bezeichneten. Der Begriff "gay" hat sich auch in anderen Sprachen wie dem Französischen "(gai)" eingebürgert und findet als Lehnwort auch in Deutschland neuerdings wieder zunehmend Verwendung.

Anfang der 1990er Jahre kam es innerhalb radikalerer politischer Kreise zu einer Wiederaneignung des Wortes "queer" als Überbegriff für Lesben und Schwule, was dann meist Transgender mit einschließt. Dieser Begriff hat die Wörter "gay" und "lesbian" jedoch nicht verdrängt, sondern nur partiell ersetzen können. Durch "Queer-Theorie" erfuhr er eine ähnliche Internationalisierung wie vorher der Begriff "gay".

"Chichi man" oder "Battyman" sind stark abwertende Begriffe, die aus dem Jamaika-Kreolischen kommen und für die Battyman-Tunes namensgebend sind.

In der Volksrepublik China hat die Sexualmedizin zunächst die Begriffe "tongxing'ai" („Homoerotik“) und "tongxinglian" („Homosexualität“) durchsetzen können. Es handelte sich um Lehnübersetzungen aus dem Japanischen, wo das entsprechende Wort "dōseiai" („gleichgeschlechtliche Liebe“) kurz zuvor in Anlehnung an das Deutsche geprägt worden war. Traditionelle Ausdrücke wie "fentao" („den Pfirsich teilen“) und "duanxiu" („den Ärmel abschneiden“), die anekdotisch auf Geschichten gleichgeschlechtlicher Liebe unter den chinesischen Kaisern anspielten, gingen in der Sprache urbaner Regionen verloren. Als Verben hatten sie eine Beziehung oder einen Akt, aber keine Persönlichkeitseigenschaft bezeichnet.

In den 1990er Jahren ersetzten homosexuelle Aktivisten "tongxinglian" ausgehend von Hongkong und Taiwan zunehmend durch den Terminus "tongzhi" („Genosse“, „Kamerad“). Nicht nur, weil das Wort die Silbe "tong" („gleich“) enthielt, sondern auch, weil es als Anspielung auf ein bekanntes Zitat von Sun Yat-sen verstanden werden konnte: "„Die Revolution hat noch nicht gesiegt, Genossen, lasst uns zusammen kämpfen“". "Tongzhi" ist bis heute zugleich die offizielle Anredeform innerhalb der Kommunistischen Partei Chinas, was zu einer Reihe von Konflikten beim Import der neuen Bedeutung auf das Festland führte.

In Taiwan hat sich während der zweiten Hälfte der 1990er Jahre in intellektuellen Kreisen unterdessen auch der Terminus "ku'er" als lautmalerische Anlehnung an das englische "queer" verbreiten können. Genau wie in den USA stellt er aber lediglich eine Ergänzung zum dominierenden Begriff – in diesem Fall "tongzhi" – dar.

Da es sich bei der Idee, gleichgeschlechtliche Liebe und Sexualität seien an einen bestimmten Personentypus gekoppelt oder auf diesen beschränkt, um eine moderne, westlich geprägte Vorstellung handelt, fehlen in fast allen Sprachen indigene Ausdrücke für homosexuelle Personen. Dies war früher auch im Westen so. John Henry Mackay veröffentlichte unter seinem Pseudonym "Sagitta" bereits 1906 die Bücher der „namenlosen Liebe“. Im ersten Band erklärt Mackay, dass es für diese Liebe immer noch keinen adäquaten Namen gibt, so dass er sie die „Namenlose“ nennen muss. Er legt dar, dass diese Liebe weder eine Angelegenheit der Kirche (Begriffe wie Sodomie, Unkeuschheit) noch des Staates, noch der Medizin (Homosexualität) sei, sondern allein der Natur und deshalb auch nur den Gesetzen der Natur unterstehe.

Auch heute noch gibt es beispielsweise im Arabischen keinen feststehenden Begriff für Lesben und Schwule. Der religiöse Begriff "luti" (, abgeleitet von der biblischen Figur Lots) entspricht etwa dem christlichen Terminus Sodomit und bezeichnet jemanden, der die vom Islam verbotene Handlung des Analverkehrs praktiziert. Er wird jedoch nicht im westlichen Sinn als Name für eine identitär fixierte Minderheit gebraucht. In Ägypten werden Beteiligte der in den 1990er Jahren entstandenen Homosexuellenszene von den Medien stattdessen als "schaddh" (, wörtlich „anormal“, „unregelmäßig“ oder „unnatürlich“; auch ) bezeichnet und diffamiert. Es gibt jedoch auch wertfreie Begriffe, die sich vom arabischen Wort ableiten – für Schwule und für Lesben –, wobei auch „Homosexualität“ an sich bedeutet.

In Ungarn setzt sich zurzeit das Schimpfwort "buzi" als Selbstbenennung von Angehörigen der schwulen Szene durch, obwohl es an sich gar keine Bedeutung hat. Es wird überall dort gebraucht, wo man seinem Ärger darüber Luft machen möchte, dass etwas schiefgelaufen ist. Aufgrund seiner spielerischen Konnotationen wird es analog zum englischen Begriff "queer" verwandt.

In Simbabwe benutzt die 1990 gegründete Organisation GALZ "(Gays and Lesbians of Zimbabwe)" englische Termini, da die Differenz zwischen einem afrikanischen Konzept gleichgeschlechtlicher Beziehungen und einer westlichen Identität als Lesbe oder Schwuler von den damaligen Gründern, die mehrheitlich weiß und wenig politisiert waren, nicht verstanden wurde und die einzige Alternative in der Landessprache Shona der beleidigende Ausdruck "ngochani" gewesen wäre. Der Name blieb jedoch auch später erhalten, da internationale Menschenrechte auf der Basis einer sexuellen Identität leichter einzuklagen schienen.

In der afroamerikanischen Bevölkerung der USA hat sich während der 1990er Jahre in Abgrenzung von einer weißen Gay-Identität der Begriff "Down-Low" oder DL herausgebildet. Er leitet sich von der Wendung "to be on the down low" („es nicht an die große Glocke hängen“) ab. Um auch gleichgeschlechtlich liebende Männer ohne schwule Identität durch HIV-Präventionskampagnen zu erreichen, benutzen Aids-Organisationen mittlerweile den neutralen Terminus „Men who have Sex with Men“ (MSM). Diese kultur- und kontextsensitive Strategie hat sich mittlerweile auch auf internationalen Konferenzen durchgesetzt.

Schätzungen über die Häufigkeit von Homosexualität variieren beträchtlich und werden durch unterschiedliche, voneinander abweichende Definitionen des Gegenstands zusätzlich verkompliziert. Darüber hinaus ist anzunehmen, dass Umfragen durch die soziale Stigmatisierung der Homosexualität und die damit einhergehende Tendenz zum Verschweigen eher nach unten als nach oben verfälscht sind. So schätzten sich etwa in einer repräsentativen Emnid-Umfrage aus dem Jahr 2000 nur 1,3 bzw. 0,6 Prozent der in Deutschland lebenden Befragten als schwul bzw. lesbisch sowie 2,8 bzw. 2,5 Prozent als bisexuell ein. Gleichzeitig gaben aber 9,4 Prozent der Männer und 19,5 Prozent der Frauen an, sich vom eigenen Geschlecht erotisch angezogen zu fühlen. Bei einer im Jahr 2003 in Australien durchgeführten Umfrage bezeichneten sich 1,6 Prozent der Männer als homosexuell und 0,9 Prozent als bisexuell; 0,8 bzw. 1,4 Prozent der befragten Frauen gaben an, lesbisch bzw. bisexuell zu sein. In Kanada stuften sich bei einer 2003 durchgeführten Umfrage unter Männern und Frauen im Alter zwischen 18 und 59 Jahren 1,0 Prozent als homosexuell und 0,7 Prozent als bisexuell ein. In Großbritannien ergab eine Umfrage des Office for National Statistics aus dem Jahr 2011/2012, dass sich 1,1 Prozent aller befragten Personen als schwul oder lesbisch einschätzten, 0,4 Prozent bezeichneten sich als bisexuell, weitere 3,6 Prozent waren sich in Bezug auf ihre Orientierung unsicher. Laut einer repräsentativen Untersuchung des Center for Disease Control and Prevention (CDC) vom März 2011 bezeichnen sich 1,7 Prozent der amerikanischen Männer zwischen 15 und 44 Jahren als homosexuell. Gary J. Gates von der Universität Kalifornien untersuchte elf US-amerikanische und internationale Studien aus den letzten Jahren; danach ist der Anteil der sich als homosexuell und bisexuell identifizierenden Frauen und Männer in den USA 2004–2009 angestiegen. Im Schnitt lag der Anteil 2009 bei den nicht-heterosexuellen Frauen bei 3,3 % (1,1 % homosexuell) und 3,6 % bei den Männern (2,2 % homosexuell). Dies bedeutet in absoluten Zahlen, dass etwa 9 Millionen Amerikaner nicht heterosexuell sind. Laut der US-Studie National Health Interview Survey (NHIS) von 2013 bezeichneten sich 1,6 % der US-Bevölkerung als homosexuell und 0,7 % als bisexuell.

Was das tatsächliche Sexualverhalten angeht, kam der Kinsey-Report 1948 zu dem Ergebnis, dass 37 Prozent der männlichen US-Bevölkerung nach Beginn der Pubertät „zumindest einige physische homosexuelle Erlebnisse bis zum Orgasmus“ haben und weitere 13 Prozent „erotisch auf andere Männer“ reagieren, „ohne tatsächliche homosexuelle Kontakte“ zu unterhalten. Zusammengerechnet seien daher nur 50 Prozent der männlichen erwachsenen Bevölkerung ausschließlich heterosexuell und gar nur vier Prozent ausnahmslos – und über ihr gesamtes Leben hinweg – homosexuell.

Schon bei Kinsey war der Anteil von Homosexualität an der „Gesamt-Triebbefriedung“ nichts Festes, sondern hing in hohem Maße von der jeweiligen Klassenzugehörigkeit ab. So pflegten Angehörige der unteren Schichten in dieser Zeit wesentlich mehr homosexuelle Kontakte als das Bürgertum und die Eliten.

Jüngere Studien zeigen darüber hinaus, wie sehr diese Zahlen dem historischen Wandel unterliegen können. So gaben in einer Studie zur Jugendsexualität, die 1970 vom Hamburger Institut für Sexualforschung durchgeführt wurde, 18 Prozent der befragten 16- und 17-jährigen Jungen an, gleichgeschlechtliche sexuelle Erfahrungen gemacht zu haben. Zwanzig Jahre später waren es nur noch zwei Prozent – ohne dass sich der Anteil von Jungen mit heterosexuellen Kontakten dadurch signifikant erhöht hätte.

Der Sexualwissenschaftler Volkmar Sigusch erklärt diesen Einbruch gleichgeschlechtlicher Jugenderfahrungen u. a. mit der wachsenden öffentlichen Thematisierung von „Homosexualität“ und der damit verbundenen Befürchtung der Jungen, aufgrund solcher Handlungen „womöglich als ‚Schwuler‘ angesehen zu werden“. Allerdings verharrte der Anteil der Mädchen mit homosexuellen Kontakten im selben Zeitraum konstant bei sechs Prozent.

Ähnlich stellte auch die Bundeszentrale für gesundheitliche Aufklärung (BZgA) in ihrer repräsentativen Wiederholungsbefragung zwischen 1980 und 1996 eine Halbierung des Anteils 14- bis 17-jähriger Jungen fest, die zugaben, „enge körperliche Erlebnisse“ mit dem eigenen Geschlecht gesammelt zu haben (von zehn auf fünf Prozent), während sich umgekehrt der Anteil der Mädchen, die von solchen Erlebnissen berichteten, zwischen 2001 und 2005 von acht auf 13 Prozent erhöhte.

Die tatsächliche Häufigkeit von homosexuellen Erfahrungen kann nicht überzeitlich und für alle sozialen Schichten einheitlich bestimmt werden. Gesellschaftliche und kulturelle Rahmenbedingungen beeinflussen die in Umfragen ermittelte Selbsteinschätzung zum Thema Homosexualität, sodass ein direkter Bezug auf die Tatsachenlage schwierig ist.

Bei vielen Menschen, die sich eher zum eigenen Geschlecht hingezogen fühlen, kommt es im Laufe ihres Lebens zum sogenannten Coming-out. Mittlerweile wird dieser Prozess in zwei Phasen beschrieben: Im ersten Schritt steht das „Sich-bewusst-Werden“ oder „Sich-Selbst-Eingestehen“ im Vordergrund, also die Erkenntnis oder aber auch die Entscheidung, dass man für die gleichgeschlechtliche Liebe offen ist. Sie wird auch als "inneres Coming-out" bezeichnet. Die zweite Phase bezeichnet das „Sich-Erklären“, also den Schritt nach außen, das Coming-out bei Familie, Freunden und/oder Kollegen. Bei manchen geschieht dieser Prozess schon im Alter von elf Jahren, andere sind sich erst mit 40 oder mehr Jahren über ihre sexuelle Orientierung im Klaren. Die meisten haben ihr Coming-out mittlerweile im Schulalter, also etwa zum Zeitpunkt der Pubertät. In diesem Alter trauen sich viele nicht, Hilfe von anderen zu erbitten, besonders dann, wenn sie bemerken, dass ihre Neigung gesellschaftlich nicht akzeptiert wird. Selbst die eigenen Eltern werden manchmal nicht darüber informiert. Das Coming-out kann manchmal in eine Lebenskrise führen, die sich bis hin zu suizidalen Absichten oder realisiertem Suizid steigern kann. Beratungsstellen in den größeren Städten und Info-Seiten im Internet versuchen diesen Menschen zu helfen, ihre Homosexualität anzunehmen. Tatsächlich ist die Suizidrate bei pubertierenden Homosexuellen deutlich höher als bei gleichaltrigen Heterosexuellen.

Unter dem soziologischen Begriff "situationsbezogene Homosexualität" (engl. "Situational Homosexuality"), die manchmal noch als Pseudohomosexualität bezeichnet wird, versteht man gleichgeschlechtliche Handlungen von Personen, welche nach standardmäßiger Definition keine homosexuelle, ja nicht einmal eine bisexuelle Orientierung haben, also heterosexuelle Sexualkontakte bevorzugen. Grundgedanke ist, dass die Aktivität nie passiert wäre, wenn sich die Menschen nicht in einer ungewöhnlichen Situation befunden hätten. Solche Männer werden auch der Gruppe der "heterosexuellen Männer, die Sex mit Männern haben" (englisch: "Straight Men Who Have Sex with Men, SMSM)" zugerechnet.

Situationsbedingte Homosexualität kommt vor allem in Umgebungen vor, in denen über längere Zeit nur Personen des gleichen Geschlechts leben. Als typische Orte gelten Haftanstalten, Erziehungsanstalten, Schiffe auf See, U-Boote, Bohrinseln, Kasernen, Klöster und Konvente, Internate, Sportteams auf Tournee und abgelegene Arbeitslager etwa bei Minen oder Großbauprojekten. Vor allem dort wird sie auch als Not-Homosexualität, Knasthomosexualität und während des Nationalsozialismus als Lagerhomosexualität bezeichnet. In der Wissenschaft spricht man manchmal auch von bisexuellem Sexualverhalten, homosexuellen Ersatzhandlungen oder experimenteller Homosexualität. Unter situativer Homosexualität fällt auch oft mannmännliche Prostitution; diese ist Standardbeispiel für Pseudohomosexualität. Jugendliche gleichgeschlechtliche Handlungen werden nur in getrenntgeschlechtlichen Umgebungen dazugezählt, manchmal werden sie als Entwicklungshomosexualität bezeichnet. Einige Aspekte in dieser sonst eigenen Betrachtung von Jugendlichen sind aber der situativen Homosexualität sehr ähnlich.

Reverend Louis Dwight berichtete 1826 über die Verhältnisse in amerikanischen Gefängnissen. Dies ist der früheste Bericht über amerikanische Strafanstalten. Josiah Flynt beschrieb 1899 situationsbezogenen Sex bei den amerikanischen Hobos, mit denen er reiste. Hans Otto Henel beschrieb 1926 in "Eros im Stacheldraht" die Situation im Ersten Weltkrieg, was Karl Plättner zu seinem 1929 erschienenen Werk "Eros im Zuchthaus" inspirierte. Viele erotische Fantasien und Geschichten spielen in Settings mit situativer Homosexualität.

Nachdem viele Gesellschaften homosexuelle Identität und offen homosexuelles Leben ablehnen, ist es oft schwer herauszufinden, was hinter einer individuellen heterosexuellen Identität steckt. Manchmal kann auch sozialer Druck und internalisierte Homophobie zu einer solchen Identität führen. Möglicherweise würden sich mehr Menschen als bisexuell identifizieren, wenn es sowohl von der heterosexuellen wie auch der homosexuellen Gesellschaft stärker akzeptiert würde. Das Konzept der situativen Homosexualität wirft Fragen auf, inwiefern aktives Sexualverhalten interne Wünsche ausdrückt und durch externe Umstände beeinflusst wird. Sexuelle Orientierung ist ein sehr komplexes System mit vielen Zwischenstufen zwischen zwei Extremen oder auf zwei getrennten Skalen und genauer betrachtet sogar gleichzeitig auf mehreren emotionalen Ebenen. Die Entbehrung gegengeschlechtlicher Sexualkontakte wird von unterschiedlichen Personen verschieden bewältigt. Schon im späten 19. Jahrhundert erkannte man, dass manche Individuen niemals gleichgeschlechtliche Aktivität zeigen, egal wie lange und wie intensiv sie heterosexuellen Kontakt entbehren. Ebenso zeigen auch viele homosexuelle Menschen keine heterosexuelle Aktivität, auch wenn Homosexualität repressiv behandelt wird und praktisch nicht durchführbar ist. Grundsätzlich geht man davon aus, dass durch nicht der sexuellen Orientierung entsprechende Handlungen dieselbe nicht beeinflusst wird. Dazu nicht im Widerspruch zeigen kulturübergreifende Vergleiche, dass gleichgeschlechtliches Sexualverhalten in Situationen gegengeschlechtlicher Entbehrungen öfter vorkommt, vor allem bei Männern in ihrer sexuellen Hauptzeit.

In vielen Kulturen wird situationsbezogene Gleichgeschlechtlichkeit toleriert. Manche sozialen Analysten gehen davon aus, dass situative Homosexualität verwendet wird, um Homophobie und Biphobie zu bekräftigen, indem jenen, die homosexuelle Sexualkontakte in gleichgeschlechtlichen Umgebungen haben, erlaubt wird, sich weiter als heterosexuell zu definieren. Oft wird in solchen Umgebungen zwischen „echten Homosexuellen“ und jenen, die heterosexuell bleiben, unterschieden. Erstere sind sozial stigmatisiert, während ihr Partner es nicht ist. Durch diese Unterscheidung wird Homophobie bestärkt, obwohl gleichgeschlechtliche Aktivität toleriert wird. Auch wenn sie oft stillschweigend erwartet wird und zu einem gewissen Grad toleriert wird, wird trotzdem erwartet, dass sie versteckt bleibt. Wird sie öffentlich sichtbar, so wird sie bestraft, selbst wenn jeder davon gewusst hat. Der „echte Homosexuelle“ wird dabei oft härter bestraft als sein mutmaßlich heterosexueller Partner, welcher vorgeblich nur aus der Situation heraus handelt. Oft wird die Unterscheidung auch dadurch getroffen, wer beim Sex „aktiver/männlicher“ und wer „passiver/weiblicher“ Partner ist. Diese Anzeichen zeigten sich beispielsweise auch in Südeuropa und vor allem im Orient (Nordafrika bis Pakistan) mit streng getrenntgeschlechtlicher Gesellschaft, wohin viele Europäer vor der hier schon herrschenden starken Ablehnung „flüchteten“ und welcher hierzulande teilweise einen schlechten Ruf hatte. Erst in den 1960ern änderte sich dort die Haltung, manchmal existieren aber noch alte Traditionen weiter oder flammen wieder auf.

Vor allem in Gefängnissen, aber auch in Erziehungsanstalten, ist sexuelle Befriedigung nur ein Teilaspekt, die Ausübung von Macht, Erhalt der eigenen Männlichkeit durch den psychisch heterosexuell und oft nicht einmal als homosexuelle Handlung angesehenen Verkehr – solange man der Aktive bleibt oder unter Gewaltanwendung gezwungen wird – und der Status in der geschlossenen Gesellschaft ist ein Hauptaspekt. Auch ethnische Konflikte spielen dabei eine Rolle. Wünsche des passiven Partners zählen meist nicht und ihm wird die Männlichkeit genommen. Untergeordnet spielt auch die generelle menschliche Sehnsucht nach Zuneigung und Bindung eine Rolle. Paarbeziehungen von einem Beschützer und einem Beschützten basieren auf einer sehr starken Anpassung an das heterosexuelle Modell, das die Gefangenen von der Straße mitbringen; sexueller Stellungswechsel ist selten, und wenn er vorkommt, wird er immer sehr geheim gehalten. Ein weiterer Faktor ist Prostitution als interne Währung sowie als Ausbeutung der Untergeordneten. Mehr als in der Außenwelt muss man eine Dreiteilung machen zwischen freiwilligem Sexualverhalten, gewalttätigen Vergewaltigungen und sexuellen Handlungen „um des Überlebens willen“. Ein großes Druckmittel sind dabei durchgeführte oder angedrohte Gruppenvergewaltigungen. Das Phänomen ist generell sehr wenig wissenschaftlich erforscht, in Hinblick auf das System und nicht aus antihomosexueller Sichtweise, am meisten noch in den USA. Das Durchschnittsalter in den US-amerikanischen Haftanstalten beträgt etwa 28 Jahre, wobei fast drei Viertel der Gefangenen unter 35 Jahre alt sind und damit im sexuell aktivsten und körperlich aggressivsten Alter sind. Sie stammen überwiegend aus den Unterschichten und der Arbeiterklasse und sind zu einem großen Teil Schwarze. Jede sexuelle Aktivität seitens der Gefangenen ist in allen Gefängnissen verboten und wird bei konkretem Anlass wirklich – oft auch gegenüber Vergewaltigungsopfern – durch Disziplinierungsmaßnahmen (bis hin zu Isolationshaft, Verlust von Hafterleichterungen und Ablehnung von Entlassung auf Bewährung) geahndet, im Gegensatz zu den nicht so konsequent durchgesetzten früher bestehenden „Sodomiegesetzen“ der Bundesstaaten. Was nicht offensichtlich ist, wird wegen der erhofften friedensstiftenden Wirkung in der Institution oft geduldet und nicht näher betrachtet. Als Beispiel für eine Größenordnung sei eine Untersuchung aus dem Jahre 1982 genannt, wo 65 % aller in einer Stichprobe befragten Gefangenen angaben, in diesem Gefängnis sexuell aktiv gewesen zu sein. Von den sich selbst als heterosexuell Beschreibenden berichteten 55 % von sexuellen Aktivitäten, wobei sich die Zahl der sexuell aktiven Heterosexuellen auf 35 % der Weißen, 81 % der Schwarzen und 55 % der Hispanics verteilen. Verheiratete Heterosexuelle, die eheliche Besuche erhalten konnten, beteiligten sich häufiger, nicht weniger, am Sex mit anderen Gefangenen. Es bestehen Unterschiede zwischen Strafanstalten, Haftanstalten, Polizeigefängnissen und Jugendgefängnissen. Der Hauptgrund für die Begrenzung der Rate liegt am Mangel an passiven Partnern. Jene, die sich nicht am Sex beteiligen, beteiligen sich dennoch oft am System, indem verbale Bemerkungen gemacht werden und versucht wird, andere in die passive Rolle zu drängen. Im Gegensatz zu sexuellen Phantasien einiger urteilen die Homosexuellen, dass der Sex draußen wesentlich besser sei.

→ "Hauptartikel: Transgender"

Geht es bei Homosexualität um das Geschlecht des bevorzugten Partners, so geht es bei Transgender, wozu auch Transsexualität gehören kann, um das Empfinden der eigenen Geschlechtsidentität, die unabhängig von der sexuellen Orientierung ist. Beide sind aber Teile der mehrschichtigen sexuellen Identität.

Beziehungen zu Personen gleichen Identitätsgeschlechts werden dabei als homosexuell empfunden, solche zu Personen eines anderen Identitätsgeschlechts als heterosexuell, wobei die Quote der homo- oder bisexuell empfindenden Transgender weit höher liegt als die von Nicht-Transgendern; je nach Schätzung sind dies mindestens ein Drittel. In älterer Fachliteratur findet sich noch der Gebrauch von Homo- bzw. Heterosexualität relativ zum ursprünglich zugewiesenen Geschlecht, also würde beispielsweise eine mit einem Mann verheiratete Transfrau als homosexuell beschrieben, konträr zu ihrem Empfinden, ein schwuler Transmann als heterosexuell. In der neueren Literatur nimmt diese Verwendung kontinuierlich ab, in hauptsächlich sozialwissenschaftlich geprägten Texten ist er nicht mehr zu finden.

Aufgrund der ursprünglichen, als abwertend empfundenen Verwendung und aufgrund der Schwierigkeiten, "gleich" und "verschieden" genau zu definieren, bevorzugen viele Transgender anstelle von homo- und heterosexuell "schwul", "lesbisch", "queer" etc. als Selbstbezeichnungen. Selten werden die, für den Begehrenden geschlechtsneutralen, Begriffe Gynäkophilie und Androphilie verwendet.

Dass Homosexualität oft mit Transgender, Transsexualität und manchmal auch Intersexualität in Verbindung gebracht wird, hat mehrere Gründe:

Weltweit werden derzeit (Stand: Mai 2012) Homosexuelle in 78 der 193 Mitgliedstaaten der Vereinten Nationen strafrechtlich verfolgt, so etwa in Nigeria, Uganda, Tansania, Simbabwe, Angola, Jamaika, Belize und in den meisten islamischen Staaten, wobei in fünf dieser Länder – Iran, Jemen, Sudan, Saudi-Arabien und Mauretanien – sowie in Teilen Nigerias und Somalias die Todesstrafe für gleichgeschlechtlichen Verkehr vorgesehen ist. In Indien und dem Irak ist die rechtliche Lage unklar oder nicht überschaubar.

Aber auch in Teilen Europas, zum Beispiel in Russland, Weißrussland, Albanien und sogar in manchen der neuen EU-Länder ist die Lage der Menschenrechte derzeit bedenklich: So werden in Polen und Lettland Demonstrationen für Toleranz gegenüber Schwulen und Lesben von offiziellen Stellen verboten oder teilweise mit massiver Gewalt konfrontiert, die von den Kirchen und rechtsradikalen Nationalisten geschürt wird. In Polen sind in letzter Zeit Forderungen einiger führender Politiker laut geworden, Homosexuelle in Lager zu stecken bzw. aus Polen zu eliminieren. Im Jahr 2007 wurde über ein Gesetz beraten, das selbst die Erwähnung von Homosexualität für Lehrer unter Strafe stellen soll. So wird auch verboten, aufzuklären, wie sich homosexuelle Männer vor Aids schützen können. Lehrer, die dagegen verstoßen, können aus dem Schuldienst entlassen werden. In der UNO versuchen der Vatikan und die islamischen Staaten gemeinsam, allein nur die Diskussion über die Menschenrechtslage für Schwule und Lesben zu verhindern. Für gewisses Aufsehen sorgte die Verhinderung bzw. Störung schwul-lesbischer Demonstrationen, Prides und Petitionsübergaben in Warschau, Riga und in Moskau durch die Polizei in den Jahren 2005, 2006 und 2007, wobei auch der parlamentarische Geschäftsführer und Bundestagsabgeordnete von Bündnis 90/Die Grünen Volker Beck kurzfristig verhaftet wurde.

Unabhängig von der Diskriminierung durch benannte gesellschaftliche Gruppen oder fehlenden Schutz durch staatlichen Eingriff, sind Schwule und Lesben auch häufig homophoben Angriffen ausgesetzt, die durch Menschen mit Angst vor der eigenen, latent vorhandenen Homosexualität ausgeübt werden. So zeigen wissenschaftliche Untersuchungen mit nach eigenem Bekunden heterosexuellen Männern, dass jene, die sich homophob äußerten, deutlich stärker auf gleichgeschlechtliche sexuelle Reize reagierten als solche, die sich nicht homophob geäußert hatten. Andere Untersuchungen legen nahe, dass Männer, die bezüglich dessen, was sie für typisch männliche Eigenschaften halten, dahingehend verunsichert werden, dass sie möglicherweise selbst nicht diesem Bild entsprechen, dies durch ausgeprägten Machismus und Aggression gegen Homosexuelle überkompensieren wollen.

→ Siehe Hauptartikel Homophobie: "Abschnitt Angst vor eigenen lesbischen bzw. schwulen Zügen" und "Abschnitt Wissenschaftliche Untersuchungen"

→ "Hauptartikel: Gesetze zur Homosexualität, Gleichgeschlechtliche Ehe, Eingetragene Partnerschaft"

Die weitgehende rechtliche Gleichstellung von Lesben und Schwulen mit Heterosexuellen wird in der Lesben- und Schwulenbewegung überwiegend begrüßt, auch wenn es immer noch umstritten ist, ob man sich damit gesellschaftlich und beziehungsdynamisch den klassischen Normen der „bürgerlichen Ehe“ annähern möchte, bei denen einige noch meinen, Überbleibsel einer patriarchalen Gesellschaftsordnung zu finden, mit einer strengen Aufteilung von Geschlechtsrollen, die für eine gleichgeschlechtliche Beziehung nicht anwendbar wären.

Gesetzliche Regelungen für gleichgeschlechtliche Partnerschaften gibt es bereits in einer Reihe von Ländern. Mehrere Länder haben die Ehe für gleichgeschlechtliche Paare ermöglicht: Niederlande (2001), Belgien (2003), Spanien (2005), Kanada (2005), Südafrika (2006), Norwegen (2009), Schweden (2009), Portugal (2010), Island (2010), Argentinien (2010), Dänemark (2012), Neuseeland (2013), Uruguay (2013), Brasilien (2013), Frankreich (2013), Vereinigtes Königreich (2014), Irland (2015), Luxemburg, Homosexualität in den Vereinigten Staaten (2015, siehe hierzu "Anerkennung gleichgeschlechtlicher Partnerschaften in den Vereinigten Staaten"), Kolumbien (2016), Finnland und Deutschland (2017). In Österreich und Australien ist die Eheöffnung für gleichgeschlechtliche Paare im Dezember 2017 beschlossen worden und wird in Österreich spätestens am 1. Januar 2019 und in Australien am 1. Januar 2018 in Kraft treten

In vielen weiteren Ländern existieren registrierte Partnerschaften, die teilweise dieselben Rechtswirkungen wie die Ehe haben, teilweise jedoch auch geringere Rechte, wie z. B. die Eingetragene Lebenspartnerschaft in der Schweiz.

In "Deutschland" gibt es seit dem 1. August 2001 das Rechtsinstitut der Lebenspartnerschaft. Nach ihrer Verabschiedung durch den Bundestag meldeten einige Politiker Zweifel daran an; die unionsregierten Länder Bayern, Sachsen und Thüringen bemühten sich sogar um eine völlige Aufhebung des Gesetzes vor dem Bundesverfassungsgericht. Dieses stellte jedoch klar, dass einer vollständigen Gleichstellung mit der Ehe nichts im Wege stünde, da die Lebenspartnerschaft mit der Ehe schon allein deshalb nicht konkurriere, weil sie einen anderen Personenkreis betreffe.

Die Lebenspartnerschaft entspricht – was das Bürgerliche Gesetzbuch betrifft – weitestgehend der Ehe. Lediglich die gemeinschaftliche Adoption von nichtleiblichen Kindern ist nicht möglich. Lebenspartner können aber das leibliche Kind ihres Partners adoptieren (sogenannte Stiefkindadoption). Auf diese Weise können zwei Frauen oder zwei Männer rechtlich gemeinschaftliche Eltern von Kindern werden. Ebenso erlaubt wurde im Februar 2013 durch Entscheidung des Bundesverfassungsgerichtes die sukzessive Zweitadoption eines adoptierten Kindes. Auch in der gesetzlichen Kranken-, Pflege- und Rentenversicherung (unter anderem Witwenrente) sind Lebenspartner mit Ehegatten gleichgestellt. Sie leben – wie Ehegatten – im Güterstand der Zugewinngemeinschaft, wenn sie nichts anderes vereinbaren. Gleichbehandlung erfolgt nach der Trennung auch beim Unterhaltsrecht. Es gelten Befangenheitsvorschriften und Zeugnisverweigerungsrechte wie bei Eheleuten auch. Zudem ist ein Verlöbnis für Lebenspartner entsprechend dem Verlöbnis für Ehegatten rechtswirksam.

Im Bundesbeamtenrecht werden Lebenspartner rückwirkend ab 2001 (Familienzuschlag, Hinterbliebenenpension usw.) gleichbehandelt. Hier erfolgte die Gleichstellung gegen den Widerstand der CDU/CSU-Fraktion im Bundestag und der unionsregierten Länder im Bundesrat durch Urteil des Bundesverfassungsgerichtes, das parlamentarisch gesetzlich danach umgesetzt wurde. Die Zuständigkeit für das Beamtenrecht ist inzwischen durch die Föderalismusreform auf den Bund für seine Beamten und auf die Bundesländer für die Landesbeamten übergegangen. Als erstes Bundesland hat Bremen seine verpartnerten Beamten und Richter mit seinen verheirateten Beamten und Richtern gleichgestellt; danach folgten Berlin, Hamburg, Mecklenburg-Vorpommern, Brandenburg, Rheinland-Pfalz, Saarland, Niedersachsen, Nordrhein-Westfalen, Sachsen-Anhalt, Schleswig-Holstein, Hessen, Baden-Württemberg, Mecklenburg-Vorpommern, Sachsen und Bayern.
Im Zuge der Erbschaftsteuerreform wurden im Januar 2011 die eingetragenen Lebenspartnerschaften der Ehe gleichgestellt. Im Einkommenssteuerrecht (Einkommensteuer) werden seit 2013 Lebenspartner gleichbehandelt. Eine Angleichung bei der Einkommensteuer, im Rahmen des Ehegattensplittings erfolgte im Sommer 2013, nachdem zuvor ein Urteil des Bundesverfassungsgerichtes zugunsten homosexueller, verpartnerter Paare erging.

Radikalere Teile der Lesben- und Schwulenbewegung lehnen die Lebenspartnerschaft – als "Ehe light" verpönt – und die damit verbundene notwendige Sondergesetzgebung für Homosexuelle ab. Stattdessen fordern sie die Abschaffung der Ehe und plädieren für sogenannte „Wahlverwandtschaften“ auf Zeit.

Am 30. Juni 2017 beschloss der Bundestag die Öffnung der Ehe für gleichgeschlechtliche Paare.

In der "Schweiz" wurde zuerst im Kanton Genf am 1. Mai 2001 eine PACS eingeführt, welche die Eintragung von homosexuellen wie auch heterosexuellen Partnerschaften ermöglichte. Am 22. September 2002 wurde im Kanton Zürich eine eingetragene Partnerschaft vom Stimmvolk mit 62,7 % Ja-Anteil genehmigt. Diese Regelung ging um einiges weiter als die Genfer Lösung und stellte eingetragene Lebenspartnerschaften Ehepaaren gleich, soweit dies in der Kompetenz des Kantons lag. Mit Beschluss des Kantonsparlaments vom 27. Januar 2004 führte auch der Kanton Neuenburg die registrierte Partnerschaft für unverheiratete Paare ein.

Am 5. Juni 2005 stimmte das gesamte Schweizer Stimmvolk über das Partnerschaftsgesetz (PartG) zur eingetragenen Partnerschaft ab. Es war das erste nationale Referendum über diese Frage weltweit. 58 % der teilnehmenden Stimmberechtigten stimmten dem Gesetz zu. Ziemlich homogene Mehrheiten gab es vor allem im Mittelland vom Kanton St. Gallen bis zum Kanton Genf; nicht nur Städte stimmten zu, sondern auch ländlichere Gebiete. Ablehnend verhielten sich vor allem ländlich-bäuerliche, katholische Kantone. Das Gesetz angenommen haben insgesamt 16,5 von 23 Kantonen. Die eingetragene Partnerschaft in der Schweiz schafft eine Gleichstellung mit der Ehe in Steuerfragen, Sozialleistungen, Erbrecht, Besuchsrecht, Zeugnisverweigerungsrecht etc. Es unterbindet aber ausdrücklich den Zugang zur Fortpflanzungsmedizin und die Adoption. Die eingetragene Partnerschaft in der Schweiz hat Auswirkungen auf den Zivilstand. Der Zivilstand ist nicht mehr „ledig“, sondern „in eingetragener Partnerschaft“. Das Gesetz trat am 1. Januar 2007 in Kraft.

In "Österreich" trat – nachdem im Herbst 2007 eine Perspektivengruppe der Koalitionspartei ÖVP und ein Teil des Parteivorstandes entschieden hatten, dass es ein Rechtsinstitut geben soll – am 1. Januar 2010 das Eingetragene Partnerschaft-Gesetz in Kraft. Nach einem Meinungsaustausch mit dem Juristen Helmut Graupner waren viele sogar für eine Öffnung der Ehe, was auch einer der Vorschläge an den Parteivorstand war. Laut Bundesparteiobmann und Vizekanzler Wilhelm Molterer diente die Schweiz als Vorbild.

Ein Gegenpol zu den Bestrebungen zur Gleichstellung homosexueller Beziehungen mit der Ehe findet sich in den Vertretern der Lebensformenpolitik.

Das Aufwachsen von Kindern in gleichgeschlechtlichen Partnerschaften und die sich damit stellenden rechtlichen Fragen werden neuerdings unter dem Begriff „Regenbogenfamilie“ diskutiert. Die Kinder stammen meist aus früheren Beziehungen, andere sind Pflege- oder Adoptionskinder, wurden durch künstliche Befruchtung oder heterologe (Heim)-Insemination mit Samen von persönlich bekannten oder anonymen Spendern gezeugt oder zwei Paare arrangieren eine Co-Elternschaft, Leihmütter kommen schon durch die rechtlichen Gegebenheiten eher selten zum Einsatz.

Mit der Verabschiedung der europäischen Richtlinien zur Antidiskriminierung im Arbeitsrecht sind Kündigungen und sonstige diskriminierende Maßnahmen aufgrund Bekanntwerdens der homosexuellen Identität von Mitarbeitern in der Privatwirtschaft sowie von Angestellten und Beamten im öffentlichen Dienst in den Mitgliedstaaten der EU unzulässig. Ausnahmen bestehen für weltanschauliche Organisationen und Vereinigungen. Diese können . Diese Regelungen haben mit § 8 Abs. 1 und § 9 des Allgemeinen Gleichbehandlungsgesetzes Eingang in deutsches Recht gefunden.

Entsprechend finden im deutschen Arbeitrecht auch die ethischen Positionen der Kirchen und anderer Religions- und Weltanschauungsgemeinschaften Anwendung. Für Homosexuelle bedeutet dies, dass sie von Organisationen oder Vereinen, bei denen die Ablehnung von Homosexualität oder homosexuellen Handlungen zum Ethos gehört, entlassen werden können.

In der römisch-katholischen Kirche wird gelebte Homosexualität als nicht mit dem christlichen Glauben vereinbar angesehen.
Angestellte der katholischen Kirche, welche sich offen zu ihrer Homosexualität bekennen, werden daher in der Regel entlassen. Ein solcher Widerspruch wird auch gesehen, wenn eine Gleichgeschlechtliche Lebenspartnerschaft nach dem Lebenspartnerschaftsgesetz eingegangen wird. Vergleichbar zu geschiedenen Kollegen, die erneut heiraten, erfolgt daher meist die Entlassung wegen Verletzung der Loyalitätspflichten als Arbeitnehmer.
So wurde im Jahr 2010 beispielsweise einer weiblichen Reinigungskraft eines katholischen Kindergartens des Bistums Essen gekündigt, weil sie eine eingetragene Lebenspartnerschaft mit einer Frau eingegangen war. In einzelnen kirchennahen katholischen Organisationen kann auch bereits ein Chatprofil bei einem Internetportal für Homosexuelle zu einer fristlosen Entlassung führen, wenn es der Organisationsleitung bekannt wird (siehe Kirchen als Tendenzbetrieb). Eine solche Kündigung hatte aber vor dem Arbeitsgericht Frankfurt keinen Bestand.

2005 hat der Heilige Stuhl ferner eine Instruktion veröffentlicht, in der Personen mit "„tiefsitzenden homosexuellen Tendenzen“" und "„Unterstützer einer homosexuellen Kultur“", als nicht geeignete Kandidaten für Weihämter, wie Priester oder Diakon, angesehen werden. Personen mit weniger "„tiefsitzenden homosexuellen Tendenzen [die] Ausdruck eines vorübergehenden Problems wie etwa einer nicht abgeschlossenen Adoleszenz“" wären, sollten "„mindestens drei Jahre vor der Diakonenweihe“" ausgeschlossen sein. Im Mai 2015 hob die Deutsche Bischofskonferenz die "Erklärung zur Unvereinbarkeit von Lebenspartnern nach dem Lebenspartnerschaftsgesetz" vom 24. Juni 2002 auf.

Demgegenüber sind Beschäftigte, auch Pastoren, in den evangelischen Landeskirchen der EKD von einer arbeitsrechtlichen Kündigung oder Disziplinarmaßnahme nicht bedroht, wenn sie mit ihrem Partner oder ihrer Partnerin eine standesamtliche Lebenspartnerschaft eingehen oder ihre homosexuelle Identität in sonstiger Weise bekannt wird. In einigen Landeskirchen der EKD sind sie sogar besoldungsrechtlich zur Ehe gleichgestellt, was auch in der altkatholischen Kirche der Fall ist.

Insgesamt hat die Bundeswehr – nicht zuletzt durch den zunehmenden Anteil von Soldatinnen – ihr Bewusstsein für Sexualität weiterentwickeln müssen. Dies begann erst spät Ende 2000 durch die Änderung der „Führungshilfe für Vorgesetzte“, Bd. 2, A.III.7. Allerdings wird hier schon verlangt, dass militärische Vorgesetzte im Blick auf sexuelle Minderheiten („Toleranz gegenüber anderen nicht strafbewehrten sexuellen Orientierungen“ also einschließlich transsexueller Soldaten/innen) aktiv „jeder Diskriminierung energisch entgegentreten“ müssen.

Auch mit dem im Rechtsrang höher stehenden Sexualerlass „Umgang mit Sexualität in der Bundeswehr“ zur Zentralen Dienstvorschrift (ZDv) 14/3, Anlage B 173 ist eine Diskriminierung verboten worden. Mit der letzten Änderung im Juli 2004 ist nach jahrzehntelanger Ächtung homosexueller Vorgesetzter, die unter Billigung höchstrichterlicher Rechtsprechung mit Versetzungen und sogar Entlassungen rechnen mussten – wie etwa bei der Kießling-Affäre – ein liberalerer Umgang mit der Sexualität gewählt worden: 

Eine weitere Änderung trat mit dem Soldatinnen- und Soldaten-Gleichbehandlungsgesetz im Jahr 2006 in Kraft, durch das verboten sind, aber zusätzlich von diesem Maßstab der Nichtdiskriminierung auch der berufliche Erfolg abhängt, nämlich bei . Die Einfügung der sexuellen Identität in dieses Gesetz wurde in der Großen Koalition kontrovers diskutiert: CDU/CSU lehnte es zunächst ab, gab dann aber im Rahmen eines Kompromisspakets dem Wunsch der SPD nach.

Künftig sind grundsätzlich alle Beziehungsformen in den Privatbereich verwiesen. Homosexuelle Beziehungen können außer Dienst auch innerhalb militärischer Anlagen gepflegt werden, auch spielt der Dienstgrad der Beziehungspartner keine Rolle mehr. Soldatinnen und Soldaten in eingetragener Lebenspartnerschaft haben eine eigene Personenstandsbezeichnung (ELP) und sind berechtigt, Trennungsgeld zu erhalten.

Einer der Vertreter der Belange homosexueller Menschen in der Bundeswehr ist der Arbeitskreis homosexueller Angehöriger der Bundeswehr.

Eine jüngere Generation von lesbisch-schwulen Soziologen, Philosophen und Historikern wie Mary McIntosh ("The Homosexual Role", 1968), Michel Foucault ("La Volonté de savoir," 1976), Alan Bray ("Homosexuality in Renaissance England," 1982) oder gegenwärtig insbesondere David Halperin ("How to do the History of Homosexuality," 2002) betrachtet Homosexualität nicht mehr als eine überzeitliche Essenz, sondern als eine Erfindung der europäischen Neuzeit. Damit ist nicht gemeint, dass Frauen und Männer an anderen Orten und zu anderen Zeiten keinen gleichgeschlechtlichen Sex gehabt hätten. Vielmehr beziehen die genannten Autoren die Position, dass unsere heutige Auffassung von Homosexualität als „Seinsweise“, die eine Minderheit von einer Mehrheit unterscheidet, eine verhältnismäßig junge Konstruktion sei.

Das theologische Modell der Sodomie, das dem modernen Begriff der Homosexualität vorausging, steht zu diesem in einem deutlichen Gegensatz. Sodomie – als „widernatürlicher“ (Platon) Verkehr zwischen Männern, aber auch zwischen einem Mann und einer Frau – wurde als ein allgemeinmenschliches Laster angesehen und nicht einer bestimmten Kategorie von Personen zugeordnet. Foucault spitzte diesen Unterschied zu, indem er in einer berühmt gewordenen Sentenz behauptete: „Der Sodomit war ein Gestrauchelter, der Homosexuelle ist eine Spezies.“ (siehe auch Sodomiterverfolgung).

Neben dem Diskurs der Sodomie, der sich im Mittelalter vor allem auf den Akt des Analverkehrs bezog, gab es jedoch auch Begriffe, die eine positive Sichtweise ausdrückten, wie etwa den der Freundschaft.

„Freundschaft“ konnte fast zu allen Zeiten auch eine romantische Beziehung zwischen zwei Personen des gleichen Geschlechts bezeichnen. Küssen, Umarmen und Händchenhalten, das gemeinsame Schlafen in einem Bett (daher der altertümliche Begriff des „Bettgenossen“) ebenso wie leidenschaftliche Liebesbekundungen und Treueschwüre wurden unter Männern bis weit in die frühe Neuzeit und oft sogar noch am Beginn des 20. Jahrhunderts als völlig normal wahrgenommen. Unter Frauen ist das – seit Ende des 19. Jahrhunderts allerdings mit immer größeren Einschränkungen – teilweise auch heute noch der Fall. Die Semantiken (Bedeutungsinhalte) von Freundschaft und Liebe waren deshalb kaum voneinander zu unterscheiden. Das griechische Wort "philos" (φίλος) etwa kann sowohl ‚Freund‘ als auch ‚Geliebter‘ bedeuten.

Im Christentum wurden solche Beziehungen nur selten mit der „monströsen“ Figur des Sodomiten in Verbindung gebracht, und wenn, dann meist im Rahmen einer politischen Intrige (wie im Fall von Eduard II. oder dem mittelalterlichen Templerorden).

Die christliche Mystik lud, beeinflusst vom islamischen Sufismus, die Liebe zwischen Freunden sogar mit einer religiösen Bedeutung auf. Ebenso adaptierte das Christentum den sowohl im Gilgamesch-Epos wie in der jüdischen Bibel, aber auch im Satyricon für eine Liebesbeziehung zwischen zwei Männern verwendeten Begriff des „Bruders“ (Schwurbruderschaft). Zu deren Vereinigung hatte die orthodoxe Kirche den Ritus des „Brüdermachens“ (Adelphopoiesis) ausgearbeitet, der den beiden Freunden für ihre Liebe, die bis in den Tod anhalten sollte, zahlreiche Heiligenpaare als Vorbilder nannte. Dies schloss die parallele Eheschließung mit einer Frau jedoch nicht aus. Im lateinischen Westen, wo bis weit in die Neuzeit weder Eheleute noch geschworene Brüder "(fratres iurati)" der Segnung eines Priesters bedurften, sind zumindest eine Reihe von Grabmälern erhalten, in denen Männer- und später auch Frauenpaare miteinander bestattet wurden. Die Gravuren enthalten oft Symbole unsterblicher Liebe wie beispielsweise die Darstellung eines Kusses oder die Inschrift „Im Leben vereint, im Tode nicht getrennt“. Dass Schwurbruderschaften als von der Kirche akzeptierte gleichgeschlechtlicher Partnerschaften (d. h. inkl. genitaler Handlungen) in der Form der Adelphopoiesis legitimiert wurden, ist jedoch nicht haltbar. Der ursprüngliche Zweck der Adelphopoiesis war, eine geistige Verwandtschaft (wie bei einer Taufpatenschaft) herzustellen. Die Schwurbruderschaft muss aber tatsächlich auch von gleichgeschlechtlich Liebenden in Anspruch genommen worden sein, denn aus diesem Grund wurde dieser Ritus vom oströmischen Recht und von der orthodoxen Kirche später wieder abgeschafft, bzw. verboten.

Von der Institution der geschworenen Bruderschaft (siehe oben) ist heute nur noch der Begriff des "warmen Bruders" als Synonym für einen „Schwulen“ übrig geblieben. In diesem Begriffswandel offenbart sich der geschichtliche Bruch, der durch das moderne Konzept der Homosexualität verursacht wurde: Gesten der Zuneigung, die früher einfach als Zeichen einer Freundschaft verstanden worden wären, identifiziert man heute als „homosexuell“ und stellt sie damit als Abweichung von der gesellschaftlichen Norm unter Verdacht. Die Konsequenz ist ein vor allem unter männlichen Jugendlichen belegbarer drastischer Rückgang gleichgeschlechtlicher Sexualerfahrungen von 18 auf zwei Prozent allein zwischen 1970 und 1990. Diese Entwicklung geht mit wachsender Homophobie einher, weil viele junge Menschen aus Angst, womöglich als „Schwuler“ beziehungsweise als „Lesbe“ zu gelten, sich von allem Homosexuellen demonstrativ (und teilweise sogar gewaltsam) abgrenzen. Unter jungen Männern ist diese Tendenz zur Abgrenzung im Allgemeinen nochmals deutlich stärker ausgeprägt als unter jungen Frauen. Es ist diese tätige Abwehr, durch die sich das stigmatisierende Etikett der Homosexualität wie von selbst reproduziert.

Die binäre Kategorisierung von Personen als "hetero" und "homo" beziehungsweise „normal“ und „andersartig“ ist mittlerweile fast weltweit zu einer scheinbar unumstößlichen Realität geworden. Dieser Prozess, der sich in manchen Metropolen Europas wie London, Paris und Amsterdam bereits um 1700 ereignet hat, erreichte Regionen wie China dagegen erst Anfang beziehungsweise Mitte des 20. Jahrhunderts. Einige postkoloniale Autoren wie der Hongkonger Soziologieprofessor Zhou Huashan kritisieren die Homophobie in ihren Ländern daher als eine Folge des europäischen Imperialismus. Durch die strikte Geschlechtertrennung im Sport ist die Ausbreitung der binären Kategorisierung mit der Entwicklung des Sports verbunden.

Bis zum Hochmittelalter galt der Analverkehr im christlichen Bereich als Sünde, aber noch nicht als Verbrechen; folglich drohte höchstens eine Kirchenbuße und ein zeitweiliger Ausschluss von der Eucharistie, aber noch keine weltlichen Maßnahmen. Vom 13. Jahrhundert bis zur Aufklärung wurde Analverkehr zwischen Männern dann in fast ganz Europa unter der Bezeichnung „Sodomie“ durch weltliche Gesetze mit dem Scheiterhaufen bedroht, hier wird noch von der Sodomiterverfolgung gesprochen. Zu größeren Verfolgungen und jeweils Hunderten von Hinrichtungen kam es während des Spätmittelalters in Norditalien und Spanien sowie während des gesamten 18. Jahrhunderts auch in England, Frankreich und den Niederlanden.

Die Ideen der Französischen Revolution führten in zahlreichen Staaten, die sich am französischen Code pénal orientierten, um 1800 herum zur Abschaffung aller Gesetze gegen die „widernatürliche Unzucht“ (so etwa in den Niederlanden, im Rheinland und in Bayern). Preußen wandelte 1794 mit der Einführung des Allgemeinen Landrechts die Todesstrafe in eine Zuchthausstrafe um. 1871 wurde der preußische Paragraph in das Reichsstrafgesetzbuch des Deutschen Reichs aufgenommen und als § 175 in der folgenden Zeit immer häufiger angewandt.

Großen Einfluss hatte zu dieser Zeit der deutsch-österreichische Psychiater und Rechtsmediziner Richard von Krafft-Ebing (siehe hier). Seine durch Kriminalfälle und in der Psychiatrie gewonnenen Forschungen stellten Homosexuelle als erblich belastete Perverse dar, die für ihre angeborene „Umkehrung“ des Sexualtriebes nicht verantwortlich seien und deshalb nicht in die Hände eines Strafrichters, sondern in die von Nervenärzten gehörten. Diesen erschloss er damit ein neues „Patientengut“ für Zwangsbehandlung und Forschungsexperimente.
In seinem Buch "Psychopathia Sexualis" (1886, das Buch wurde zu einem Standardwerk) definierte er die Homosexualität als angeborene neuropsychopathische Störung, also als eine erbliche Nervenkrankheit.
Diese Diagnose erlaubte es ihm, sich für eine vollständige Straffreiheit der Homosexualität auszusprechen, da Homosexuelle für ihre „Missbildung“ nicht selbst verantwortlich seien und die Homosexualität nicht ansteckend sei. Allerdings wurde Homosexualität dadurch erst pathologisiert und homosexuelle Menschen für unzurechnungsfähig erklärt. Obwohl Krafft-Ebing zu seiner Zeit als maßgebliche Instanz auf dem Gebiet der Gerichtsmedizin galt, blieb diese Theorie für die Straflosigkeit folgenlos, da vor allem kirchlich-konservative Kreise auf die moralische Ächtung der Homosexuellen nicht verzichten wollten.

Bis zur Reform des § 175 im Jahr 1969 arbeitete die Polizei dabei mit Spitzeln in der schwulen Subkultur und geheimen Rosa Listen, auf denen zahlreiche Namen von homosexuellen Männern verzeichnet waren. Da Homosexualität verfolgt und bis in die 1970er Jahre als psychische Erkrankung diagnostiziert wurde, konnten Homosexuelle auch auf unbestimmte Zeit freiheitsentziehend in einer forensischen Psychiatrie untergebracht werden. Ein Beispiel ist die „Behandlung“ des britischen Mathematikers und Computerpioniers Alan Turing im Jahr 1952, der wenig später starb, wahrscheinlich durch Suizid.

Die Schätzungen hinsichtlich der Zahl der schwulen Männer, die während der Zeit des Nationalsozialismus in Konzentrationslagern ihr Leben lassen mussten, variieren erheblich. Die wohl verlässlichsten Zahlen stammen bis heute von Rüdiger Lautmann. Er schätzte die Zahl der in Konzentrationslager verschleppten homosexuellen Männer auf 10.000 bis 15.000. Von ihnen kamen etwa 53 % ums Leben. Der Grund für z. T. erheblich darüber hinausgehende Schätzungen liegt u. a. darin, dass nicht ermittelt werden kann, wie viele aus anderen Gründen ermordete Menschen homosexuell waren.

Einige Männer wurden trotz ihrer, dem NS-Regime bekannten Homosexualität geduldet. Zu nennen sind etwa Reichswirtschaftsminister Walther Funk, der 1946 wegen seiner Verbrechen zu lebenslanger Haft verurteilt wurde, und der schwule Bildhauer Arno Breker, der von Adolf Hitler und Joseph Goebbels auf die Gottbegnadeten-Liste der wichtigsten Künstler aus NS-Sicht gesetzt wurde und dort sogar auf die Sonderliste der „unersetzlichen Künstler“ kam. John C. Fout zeigte für Hamburg, dass 90 Prozent der Homosexuellen, die in Konzentrationslager oder Heilanstalten kamen, Arbeiter waren; nur die übrigen 10 Prozent waren der bürgerlichen Gesellschaftsschicht zuzuordnen.

Obwohl es in Deutschland, im Gegensatz zu Österreich, kein Gesetz gegen die lesbische Liebe gab, verhaftete die Gestapo auch dort eine unbekannte Zahl von Frauen wegen ihrer Homosexualität oder unter anderem Vorwand.

In der Bundesrepublik Deutschland bestand der § 175 bis 1969 in der von den Nationalsozialisten verschärften Fassung weiter, was vom Bundesverfassungsgericht 1957 als rechtmäßig anerkannt wurde. Erst 1994 fiel er im Zuge der Rechtsangleichung mit der DDR weg. Während jener Zeit verurteilte Schwule wurden in Deutschland am 17. Mai 2002 durch den Bundestag symbolisch rehabilitiert. In der Deutschen Demokratischen Republik wurden einvernehmliche, homosexuelle Handlungen unter Erwachsenen schon seit Ende der 1950er Jahre nicht mehr rechtlich verfolgt.

In Österreich existierte der § 209 ÖStGB mit ähnlichem Wortlaut wie der § 175 StGB in Deutschland bis ins Jahr 2002, als er vom österreichischen Verfassungsgerichtshof aufgehoben wurde, und trat am 14. August 2002 außer Kraft. Dennoch wurde Österreich im Anschluss mehrfach vom EGMR, der ebenfalls ausdrücklich die Menschenrechtswidrigkeit des § 209 feststellte, verurteilt, da man es unterlassen hatte, menschenrechtswidrig Verurteilte zu rehabilitieren.

Erste Forderungen nach der "urnischen Ehe" wurden von Karl Heinrich Ulrichs 1867 auf dem deutschen Juristentag in München vor 500 Mitgliedern erhoben. Auch wenn sein Vortrag mit Spott und Ablehnung aufgenommen wurde, beginnt an diesem Tag die Geschichte der Homosexuellen-Emanzipation.

Der Beginn der organisierten homosexuellen Emanzipationsbewegung wird im Allgemeinen mit der Gründung des Wissenschaftlich-humanitären Komitees (WhK) durch den Berliner Arzt Magnus Hirschfeld im Jahr 1897 angesetzt. Es handelte sich dabei jedoch um eine Honoratiorenvereinigung, die nur ca. 500 Mitglieder umfasste und nach außen hin nicht als homosexuelle Bewegung in Erscheinung trat. Stattdessen warb sie ausschließlich mit wissenschaftlichen Argumenten für eine Streichung des § 175.

Zahlenmäßig weit bedeutsamer waren die nach 1919 gegründeten Freundschaftsbünde. Ihr Schwerpunkt lag in der Planung von Geselligkeitsveranstaltungen, umfasste jedoch auch politische und publizistische Aktivitäten sowie die Gewährleistung von Rechtsschutz für jene Mitglieder, die vom § 175 betroffen waren. Als Dachgruppen konkurrierten der im August 1920 gegründete Deutsche Freundschafts-Verband (DFB) und der im Mai 1922 entstandene Bund für Menschenrechte (BfM). Letzterer erwies sich in seiner Größenentwicklung als das bei weitem erfolgreichere Modell. Bereits 1924 zählte er über 12.000 Mitglieder; 1929, gegen Ende der Weimarer Republik, waren es sogar mehr als 48.000. Ausländische angegliederte Gruppen soll es laut Angaben des Vereins in der Schweiz, in Österreich, in der Tschechoslowakei, in New York City, Argentinien und Brasilien gegeben haben. Allerdings ist über die meisten dieser Gruppen kaum etwas bekannt. Eine Ausnahme bildet eine Schweizer Gruppe um Karl Meier mit ihrer Zeitschrift "Der Kreis," die als einzige in Europa nicht durch die Nationalsozialisten zerschlagen werden konnte. Dadurch wurde sie nach dem Zweiten Weltkrieg Vorbild für viele neu entstehende Gruppen.

Ein neuer Schwerpunkt der Homosexuellenbewegung bildete sich in den Vereinigten Staaten. Im Frühjahr 1951 gründete Harry Hay, Mitglied der CPUSA, zusammen mit Bob Hull, Chuck Rowland, Dale Jennings und Rudi Gernreich die "Mattachine Society". 1955 entstand unter Führung von Del Martin und Phyllis Lyon die Lesbenorganisation "Daughters of Bilitis". Beide Gruppen bezeichneten sich als "homophil," um sich der Reduzierung von Homosexualität auf den Akt des Beischlafs zu entziehen. Unter dem Druck der McCarthy-Ära entpolitisierten sich diese Organisationen und wurden zu Debattierclubs, die in der Öffentlichkeit nicht in Erscheinung traten. Erst Mitte der 1960er Jahre fand mit Dick Leitsch (New York) und Frank Kameny (Washington) eine Neuorientierung an den Protestformen der schwarzen Bürgerrechtsbewegung statt.

Am 28. Juni 1969 kam es anlässlich einer Polizeirazzia in der New Yorker Schwulenbar Stonewall zu einem Aufstand in der "Christopher Street," der drei Tage andauerte. Dieses Ereignis führte zu einer Radikalisierung zahlreicher Lesben und Schwuler. In Anlehnung an linke Bewegungen der damaligen Zeit gründeten sich gemischte Gruppen wie die Gay Liberation Front und die "Gay Activists Alliance".

Am 1. Mai 1970 machte schließlich die Gruppe "Radicalesbians" auf sich aufmerksam, indem sie in New York den Zweiten Jahreskongress zur Vereinigung der Frauen mit einem geplanten Happening unterbrach. Das dort verteilte "Manifest der frauenidentifizierten Frau" begründete das sich für die Frauenbewegung als einflussreich erweisende Konzept des "politischen Lesbianismus:" Lesbischsein wurde nicht als eine sexuelle Orientierung, sondern als die einzig mögliche Strategie des Widerstands gegen patriarchale Bevormundung und Unterdrückung aufgefasst.

Diese politischen Strategien und Konzepte wurden in den 1970er Jahren ausnahmslos nach Europa getragen.

In der Bundesrepublik Deutschland gründeten sich nach der Liberalisierung des § 175 im Jahre 1969 und nach der Fernsehausstrahlung des Filmes "Nicht der Homosexuelle ist pervers, sondern die Situation, in der er lebt" die ersten Schwulengruppen. Die offene und provozierende filmische Darstellung von Schwulen durch den Regisseur Rosa von Praunheim, der sich hierzu eines politischen Textes des Soziologen und Sexualforschers Martin Dannecker bediente, stieß in der Öffentlichkeit, aber auch bei konservativen und angepassten Homosexuellen auf Ablehnung.

Ähnlich wie in den USA trennten sich Lesben in der Bundesrepublik schon sehr früh von den männlich dominierten Schwulengruppen und engagierten sich stattdessen in der Frauenbewegung, wo gleichgeschlechtliche Liebe oft nicht nur anerkannt, sondern sogar präferiert wurde.

In den 1980er und 1990er Jahren kam es zu einer breiten Ausfächerung, aber auch zu einer fortschreitenden Entpolitisierung der homosexuellen Emanzipationsbewegung. Gleichzeitig fand eine Wiederannäherung von Lesben und Schwulen statt. Seit etwa Mitte der 1980er Jahre veranstalten sie gemeinsam in fast sämtlichen europäischen und amerikanischen Metropolen alljährliche Demonstrationen zur Erinnerung an den Stonewall-Aufstand. In den 1990er Jahren wurden daraus gewaltige Umzüge, die unter der Bezeichnung Christopher Street Day bzw. "Gay Pride Parade" in den Tagen zwischen Juni und Juli weltweit mehrere Millionen Menschen auf die Straße ziehen. Jedoch verbinden die Teilnehmer mit ihrer Anwesenheit nur noch selten eine konkrete politische Aussage. Entsprechende Gegenentwürfe zur Repolitisierung des CSD in Deutschland sind der Transgeniale CSD in Berlin-Kreuzberg und die Queerrr Street Days in Hamburg.

Eine neue Erscheinung bildet der Wunsch nach territorialer Abgrenzung von der Hetero-Welt, der häufig als Gay Nationalism bezeichnet wird. So wurde von einer Gruppe australischer Aktivisten am 14. Juni 2004 eine winzige Koralleninsel namens Cato besetzt und das "Gay & Lesbian Kingdom of the Coral Sea Islands" ausgerufen. Der neue Staat stellte sich ziemlich rasch als eine Mikronation unter vielen heraus, denn weder der Imperator Dale Parker Anderson noch sonst jemand war bereit, sich auf Cato niederzulassen. Die Unstimmigkeiten innerhalb der Führungsriege führten zur Zersplitterung der Bewegung in mehrere Gruppen.

Zur Emanzipation der Schwulen trug – neben der Öffentlichkeitsarbeit – auch die HIV-Pandemie zu Beginn der 1980er Jahre in erheblichem Maße bei. Dies klingt zunächst widersprüchlich, da sich AIDS in den westlichen Ländern, vermutlich aufgrund der höheren Promiskuität und der hohen Ansteckungsgefahr insbesondere bei Analverkehr, zunächst stark in schwulen Kreisen verbreitete.

Durch die von den AIDS-Hilfen und der deutschen Bundesregierung angestoßenen Aufklärungskampagnen rückte das Tabuthema "Homosexualität" aber stärker ins Blickfeld der Öffentlichkeit. Dadurch wurde nicht nur Aufklärung über das HI-Virus und das dadurch ausgelöste Krankheitsbild AIDS erreicht, sondern auch für sicherere Sexualpraktiken "(Safer Sex)" als Vorsichtsmaßnahme geworben. Dabei konnten viele Fehlmeinungen und Vorurteile im gesellschaftlichen Bewusstsein über Schwule und Lesben korrigiert werden. Es lässt sich eine stetig steigende Toleranz in der Bevölkerung gegenüber Homosexualität feststellen. Die moralischen Gesellschaftswerte haben sich verschoben, auch wenn manche Menschen Homosexualität verurteilen oder homosexuelle Menschen abwerten.

Viele Menschen bringen schwule Sexualität automatisch mit HIV bzw. AIDS in Verbindung. Schwule sind jedoch nach herrschender medizinischer Ansicht nur dann dieser besonderen Risikogruppe zuzurechnen, wenn sie häufig ungeschützten Analverkehr bzw. Verkehr mit wechselnden Sexualpartnern haben, da die Verletzungs- bzw. Infektionsgefahr bei analer Penetration drastisch höher ist als bei vaginaler Penetration und Promiskuität allgemein dem Risikoverhalten für sexuell übertragbare Krankheiten zuzurechnen ist.

Bisher (Stand: 08/2011) ist es deutschen und österreichischen Männern, die sexuelle Kontakte mit anderen Männern haben, untersagt, Blut oder Organe zu spenden, da diese generalisiert der „HIV-Risikogruppe“ zugeordnet werden, ungeachtet dessen, dass sich jeder gesunde Mensch, unabhängig von seiner sexuellen Orientierung, mit dem HI-Virus infizieren kann.

Die psychiatrische Pathologisierung der Homosexualität begann Mitte des 19. Jahrhunderts. Homosexualität wurde in der Regel als Symptom einer inneren Verkehrung des Geschlechtsempfindens („konträre Sexualempfindung“, „Inversion“) aufgefasst. Eine besondere und zugleich ambivalente Rolle spielte dabei – seit ca. 1900 – die Psychoanalyse.

Sigmund Freud bezeichnete Homosexualität „als Abweichung der sexuellen Funktionen, hervorgerufen durch eine gewisse Stockung der sexuellen Entwicklung“ Als psychischen „Normalfall“ sah Freud die Bisexualität an; auch die Heterosexualität beruhe „auf einer Einschränkung der Objektwahl“. Mehrfach bezog er öffentlich Stellung gegen Kriminalisierung und Pathologisierung der Homosexualität. 1903 betonte er in der Zeitschrift "Die Zeit", dass „Homosexuelle nicht als Kranke behandelt werden sollen“. 1905 stellte er fest: „Die psychoanalytische Forschung widersetzt sich mit aller Entschiedenheit dem Versuch, die Homosexuellen als eine besonders geartete Gruppe von den anderen abzutrennen.“ 1921 widerspricht er Ernest Jones, der einen homosexuellen Arzt nicht zur analytischen Ausbildung zulassen wollte. 1930 unterzeichnete er einen Appell an den Nationalrat zur Abschaffung der Strafbarkeit. Und 1935 schrieb er in einem Brief an eine Mutter, dass auch Homosexuelle – durch eine Analyse – zu „Harmonie, Seelenfrieden und volle[r] Leistungsfähigkeit“ gelangen können.

Seine Ansichten zum Thema resümiert er in dem Aufsatz „Über die Psychogenese eines Falles von weiblicher Homosexualität“ aus dem Jahr 1920. Darin wendet er sich gegen die Vorstellung, „vollentwickelte“ Homosexualität − mit dem Ziel der Wiederherstellung der „vollen bisexuellen Funktion“ – psychoanalytisch behandeln zu können. Dies sei „nicht viel aussichtsreicher als das umgekehrte“ – die Heilung von „vollentwickelter“ Heterosexualität –, „nur daß man dies letztere aus gut praktischen Gründen niemals versucht“. Entsprechende Therapieanstrengungen scheiterten zudem sehr häufig daran, dass homosexuelle Patienten nicht aus Unzufriedenheit mit ihrer Situation, sondern auf äußeren gesellschaftlichen Druck hin eine Therapie begännen:
Dennoch wurde Homosexualität erst 1973 von der American Psychiatric Association (APA) aus ihrem Krankheitenkatalog (Diagnostic and Statistical Manual of Mental Disorders, kurz: DSM, damalige Auflage DSM-II) gestrichen – nicht zuletzt aufgrund der Forschungsergebnisse von Evelyn Hooker. Zuvor galt Homosexualität als psychische Störung. Allerdings existierte von da an im DSM-II die „sexuelle Orientierungsstörung“, später im DSM-III „ich-dystone Homosexualität“ genannt, mit der ein Zustand anhaltenden Leidens an der eigenen Homosexualität diagnostiziert werden konnte. Im neuen, aktuellen DSM-IV-TR befindet sich eine Diagnosekategorie „nicht näher bezeichnete sexuelle Störung“, die auch ein „andauerndes und ausgeprägtes Leiden an der sexuellen Orientierung“ (302.9) beinhaltet. Die Streichung erfolgte 1973 gegen den Widerstand der American Psychoanalytic Association (APsaA), die dadurch erheblich an Renommée und Einfluss verlor, dann nach einem Generationswechsel neue Position bezog und sich 1991 entschuldigte:

Aus der von der Weltgesundheitsorganisation herausgegebenen International Classification of Diseases (ICD) wurde die Homosexualität erst mit der 1992 veröffentlichten ICD-10 entfernt. Dafür wurde dort das Störungsbild der ich-dystonen Sexualorientierung (F66.1) im Bereich der Persönlichkeits- und Verhaltensstörungen aufgenommen. In der ICD-8 wurde Homosexualität bereits 1968 als umstrittenes Krankheitsbild dargestellt.

In Psychoanalyse und Psychotherapie gibt es nach wie vor kontroverse Meinungen. Anhänger der Gay Affirmative Psychotherapy, die die internationale Mehrheitsmeinung vertreten, versuchen, den Umgang mit Homosexualität möglichst in das Menschsein zu integrieren. Im deutschen Sprachraum äußerten sich 2000 zwei Standardwerke klar und deutlich: Im Mertens/Waldvogel konstatierte Udo Rauchfleisch, dass Diskriminierung und Pathologisierung wissenschaftlich nicht vereinbar seien. Im Stumm/Pritz verlangte Wolfgang Till von der Psychotherapie „eine nichtpathologisierende, vorurteilsfreie Haltung zur Homosexualität“. Johannes Cremerius nannte (schon 1992) "die Pathologisierung der Homosexualität" und die Weigerung, Homosexuelle zur analytischen Ausbildung zuzulassen, als einen der wesentlichen Gründe für die Krise der Psychoanalyse.

Dazu entgegengesetzt gibt es eine immer kleiner werdende Minderheit von Medizinern bzw. Psychoanalytikern, die Homosexualität "im Gegensatz zum DSM-IV und zur ICD-10" als „krankhafte und behandlungsbedürftige Störung“ sehen (Charles Socarides und Joseph Nicolosi).
Der Psychotherapeut Douglas Haldeman, der ehemalige Vorsitzende der American Psychological Association, ist der Meinung, Lesben und Schwule hätten zwar ein Recht auf Veränderung ihrer sexuellen Orientierung, sofern sie mit ihren sexuellen Orientierungen unzufrieden seien. Bisher ist jedoch keine funktionierende „Therapie“ bekannt, mit der langfristig die sexuelle Orientierung verändert werden konnte. Die sogenannte reparative Therapie bezeichnet Haldeman als „Pseudowissenschaft“. Die sexuellen Neigungen als solche bestimmen noch nicht die psychologische Identität eines Menschen, da dazu wesentlich die freie Stellungnahme gehört. Im Sommer 2008 erklärte die deutsche Bundesregierung im Bundestag, dass die reparative Therapie in der Fachwelt weitestgehend abgelehnt werde. Die deutsche Bundesregierung vertritt weder die Auffassung, dass Homosexualität einer Therapie bedarf, noch dass Homosexualität einer Therapie zugänglich ist. Auch Haldeman ist der Meinung, die sogenannte „reparative Therapie“ passe nicht in die moderne „mental health profession“ hinein, und sei „seit Jahren diskreditiert.“

Welche Faktoren beim Einzelnen zur Ausbildung einer bestimmten sexuellen Orientierung führen, ist ungeklärt. Grundsätzlich können bei der Entstehung der sexuellen Orientierung zwei Hauptthesen unterschieden werden:

Außerdem werden Theorien vertreten, die eine Kombination dieser beiden Thesen darstellen.
Unter biologischen, evolutionären oder psychologischen Aspekten werden deshalb folgende Themen diskutiert:

Obwohl sich der weit überwiegende Teil der Wissenschaft darin einig ist, dass Homosexualität keine Krankheit oder Paraphilie ist, wird diese immer noch vereinzelt, häufig von religiös orientierten Gruppierungen, als "abnorm" oder "krankhaft" eingestuft und eine „Heilung“ für sinnvoll und möglich gehalten; „Therapien“ werden diskutiert und auch ausprobiert. Zu nennen ist dabei vor allem die hochumstrittene, im Umfeld evangelikaler Christen in den USA entstandene Ex-Gay-Bewegung, die so genannte Konversionstherapien zur „Umpolung“ von Homosexuellen zu Heterosexuellen propagiert und anbietet. Diese Therapien werden von der medizinischen, psychologischen und psychiatrischen Fachwelt praktisch einhellig abgelehnt und als potenziell schädlich für die Betroffenen angesehen (siehe auch unten bei "Beratungsstellen"). Im US-Bundesstaat Kalifornien sind solche Therapien bei Minderjährigen seit September 2012 wegen ihres Schadpotenzials gesetzlich verboten.

Der wissenschaftliche Streit über die Ursachen homosexuellen Verhaltens ist sehr alt. Solange jedes homosexuelle Verhalten strafbar war, waren die Argumentationen in diesem Streit oft von dem Bestreben geleitet, entweder die „Unausweichlichkeit“ homosexuellen Verhaltens zu belegen und damit die Forderung nach dessen Straflosigkeit zu begründen oder aber es als freie Entscheidung für „moralischen Verfall“ zu kennzeichnen, dem mit Bestrafung entgegengewirkt werden müsse. Auch heutige Untersuchungen zeigen, dass Menschen, die von einer angeborenen sexuellen Orientierung ausgehen, eine tolerantere Haltung gegenüber Homosexuellen haben als jene, die es als persönliche Entscheidung sehen.

Teile der Lesben- und Schwulenbewegung distanzieren sich von Ursachenforschung. Die Erfahrung der letzten 150 Jahre zeigt, dass Wissenschaftler, Mediziner, Psychologen und andere sich für die Ursachen der Entwicklung sexueller Orientierungen, primär der homosexuellen Orientierung, interessiert haben. Als Teil dieser Studien haben viele versucht, Homosexuelle zu erkennen und sie zu „heilen“, wobei das behauptete Ergebnis nicht zwingend Heterosexualität sein musste. Viele homosexuelle Menschen befürchten deshalb, dass Ursachenforschung letztlich gegen sie eingesetzt werden soll, um Homosexualität als unnatürlich, abnormal oder krankhaft, beziehungsweise als Symptom einer Krankheit anzusehen. Vor allem bei Menschen bzw. Gruppierungen, die Homosexualität aus ideologischer, religiöser oder aus persönlicher Abneigung nicht tolerieren wollen, können solche Forschungen den Drang wecken, diese einzusetzen, um Homosexualität zu beseitigen oder zumindest Homosexuelle zu erkennen und zu isolieren. Solche Befürchtungen stützen sich u. a. auf Erfahrungen, die homosexuelle Männer im Dritten Reich machen mussten, in welchem man Homosexuelle mittels psychologischer Experimente erkennen und mit grausamen medizinischen Menschenversuchen „zu heilen“ beabsichtigte. Selbst Menschen, denen Schwule und Lesben sympathisch sind, und aktiv unterstützende Eltern homosexueller Kinder wollen meist heterosexuelle Kinder, und sei es nur aus Angst vor den potentiell negativen Folgen von Heterosexismus und Homophobie in der Gesellschaft. Zu beachten ist auch, dass Untersuchungsergebnisse nicht in der westlichen Welt verbleiben, sondern global verfügbar sind. An einer Universität in Singapur, wo damals gleichgeschlechtliche Akte mit lebenslanger Haft bestraft werden konnten, standen die Psychiater vor der Frage, ob ein präsymptomatischer Test ohne die Möglichkeit einer Behandlung angeboten werden dürfe. Während zu Zeiten Karl Heinrich Ulrichs (1825–1895) eine angeborene biologische Ursache als gegeben hingenommen werden musste, wäre heutzutage sogar ein „homosexuelles Gen“ potentiell per Pränataldiagnostik erkennbar und man könnte darauf mit selektiver Abtreibung oder Gentherapie reagieren. Befürworter eines Rechtes der Eltern auf Selektion sind beispielsweise Aaron S. Greenberg und J. Michael Bailey. Kulturell wurde das Thema im 1993 uraufgeführten Stück "The Twilight of the Golds/Der letzte Gold" von Jonathan Tolins behandelt, welches 1997 mit alternativem Ende verfilmt wurde.

Bisweilen wird kritisiert, dass die Erforschung der sexuellen Orientierung zu stark auf die Erforschung von Homosexualität ausgerichtet sei. Gelegentlich wird auch die Ursachenforschung zur Homosexualität an sich kritisiert. Hierin wird von diesen Kritikern eine Wertung oder Pathologisierung gesehen, die auf einen heteronormativen Blickwinkel zurückzuführen sei, der als soziokulturelles Konstrukt angesehen wird. Eine wertneutrale Forschung in diesem Bereich müsse das gesamte Spektrum der sexuellen Orientierungen im Blick haben. Biologische Ursachenforschung, die sich im Wesentlichen auf die gleichgeschlechtliche sexuelle Orientierung konzentriere, würde einen Rechtfertigungszwang für bestimmte Verhaltensweisen einschließen und Moralvorstellungen von „richtiger“ oder „falscher“ Sexualität transportieren.

Eine fundierte Zusammenfassung und Kritik der aktuelleren Ansätze und Untersuchungen zur männlichen Homosexualität lieferte etwa Robert Allen Brookey 2002 mit seinem Band "Reinventing the Male Homosexual. The Power and Rhetorics of the Gay Gene".

Nach einem Schlaganfall kann es zu Persönlichkeitsveränderungen kommen. Neben üblichen, teilweise temporären Veränderungen, die als Defizit wahrgenommen werden, wie beispielsweise Depressionen, Apathie, Ängstlichkeit, Labilität und Impulsivität kann seltener vermeintlich ein fremder Akzent angenommen werden, sich der Kunststil ändern oder überhaupt erst sich ein künstlerisches Talent zeigen. Noch seltener soll es zu einer Veränderung der sexuellen Orientierung kommen können – in beide Richtungen:


Der Zwillingsforscher Franz Josef Kallmann befragte in den 1950er Jahren männliche Zwillingspaare. Bei dieser Stichprobe ermittelte er, dass von 40 eineiigen und 45 zweieiigen männlichen Zwillingspaaren, von denen mindestens ein Bruder sich selbst als schwul bezeichnete, bei 100 Prozent der eineiigen Zwillinge der andere Bruder sich ebenfalls als schwul definierte und dass bei den zweieiigen Zwillingen diese in diesem Punkt der allgemeinen männlichen Bevölkerung glichen. Andere wie Willhart S. Schlegel fanden ähnliche genetische Komponenten der sexuellen Orientierung. Spätere Forschungsarbeiten der 1960er Jahre kamen zu verschiedenen Ergebnissen: Einige konnten einen Zusammenhang zur Zygozität finden, während andere keinen Unterschied zwischen eineiigen Zwillingen, zweieiigen Zwillingen und Adoptivgeschwistern feststellten.

Im Jahre 1993 brachte der US-amerikanische Forscher Dean Hamer einen genetischen Marker auf dem X-Chromosom mit Homosexualität in Verbindung. Die Annahme bestätigte sich zunächst, weil eineiige Zwillingsbrüder, die diesen Chromosomenabschnitt trugen, beide schwul waren. Spätere Forschungsarbeiten der gleichen Forschungsgruppe konnte dieser Zusammenhang allerdings nicht bestätigen. Eine Studie, der Forschungsgruppe um Bailey et al. (1991 und 1993) hatte zum Ergebnis, dass eineiige Zwillinge häufiger beide homosexuell sind als zweieiige. Definierte sich ein eineiiger Zwilling als gleichgeschlechtlich orientiert, so stimmte dies zu etwa 50 Prozent mit der Selbstdefinition seines Zwillingsgeschwisters überein; bei zweieiigen Zwillingen waren es nur 20 bis 25 Prozent, deren angegebene sexuelle Orientierung übereinstimmte. Diese Arbeiten, wurden, wie auch jene von Kallmann und Schlegel, als methodisch fehlerhaft kritisiert – so wurde die Zygozität der Zwillinge (eineiig oder zweieiig) nicht molekulargenetisch bestimmt, sondern anhand eines Fragebogens nach Gemeinsamkeiten und Unterschieden im Verhalten.

In einer Zwillingsstudie aus Schweden von 2008 wurden 3826 zwischen 1959 und 1985 geborene
Zwillingspärchen untersucht, bei denen mindestens ein Zwilling
einen gleichgeschlechtlichen Sexualpartner hatte. Durch Vergleich zwischen
eineiigen und zweieiigen Zwillingspärchen wurde
statistisch analysiert, welche genetischen und Umweltfaktoren die Auswahl des
Partnergeschlechts beeinflussen.
Einflussfaktoren (Konfidenzintervall in Klammern):
Bei allen Untersuchungen ist zu beachten, dass eine homosexuelle Neigung nicht immer sicher festgestellt werden kann. Manche Probanden verschweigen aus Scham eine ihnen bewusste homosexuelle Orientierung, andere sind sich ihrer noch nicht bewusst oder haben sie sich noch nicht eingestanden („inneres Coming-out“). Das führt dazu, dass die Zahl homosexuell empfindender Probanden in den Studien regelmäßig geringer erscheint, als sie tatsächlich ist. Erschwerend kommt hinzu, dass die Anzahl der Probanden in sämtlichen Studien nur gering war.

Es gibt Hinweise dafür, dass eine genetische Veranlagung zur bzw. Empfänglichkeit zumindest für männliche Homosexualität existiert, aber es gibt wohl kein einzelnes Homosexualitäts-Gen. Wie groß der Einfluss der Gene tatsächlich ist, ist zwar noch unbekannt, es kann jedoch ausgeschlossen werden, dass die Gene keine Rolle spielen. Denkbar sind unter anderem eine Kombination von verschiedenen Erbfaktoren, eine Kombination von Erbfaktoren und hormoneller Prägung während der Schwangerschaft oder auch eine Kombination genetischer und sozialer Faktoren.

Nach einer Hypothese von William R. Rice, Urban Friberg und Sergey Gavrilets aus dem Jahr 2012 könnte die Entstehung der menschlichen Homosexualität durch epigenetische Vererbung verursacht sein. So würde bei einigen Individuen die sexuelle Präferenz der Mutter an den Sohn und die Präferenz des Vaters auf die Tochter übertragen. Das passiere dann, wenn die Epi-Marks bei den Genen, die für die sexuelle Ausrichtung verantwortlich sind, bei der Keimzelle erhalten blieben. So bilde dann beispielsweise ein Embryo zwar männliche Geschlechtsorgane aus, die sexuelle Ausrichtung auf das männliche Geschlecht wäre aber dieselbe wie bei der Mutter. Die Homosexualität des Menschen ist nach dieser Hypothese angeboren, ohne zwangsläufig in der DNA-Sequenz erkennbar zu sein. Die Hypothese erklärt, weshalb das Vorkommen von Homosexualität beim Menschen über die Zeit statistisch stabil bleibt. Nach dieser Hypothese entsteht die homosexuelle Prägung bei jedem Individualzyklus neu und darum stirbt sie evolutionär nicht aus, obwohl die meisten homosexuellen Menschen keinen eigenen Nachwuchs haben. Die Autoren der Studie geben allerdings an, dass es sich nur um eine Hypothese handele, hingegen derzeit keine empirischen Befunde für einen Zusammenhang von Homosexualität und Epigenetik sprechen würden. Eine kritische Analyse der Studie von Rice et al. hat Heinz J. Voss vorgenommen.

Eine Theorie, die auf Forschungsarbeiten des deutschen Endokrinologen und Sexualwissenschaftlers Günter Dörner zurückgeht, besagt, dass Stresshormone in der Schwangerschaft für Homosexualität verantwortlich seien. Bei männlichen Föten verhinderten sie, dass deren Gehirn, das zunächst keine Unterschiede zu einem weiblichen habe, durch bestimmte Hormone ein männliches Geschlecht bekomme. Diese das Gehirn modifizierenden Hormone „vermännlichen“ das Gehirn des männlichen Babys normalerweise in der Schwangerschaft in drei Phasen, von denen jede durch Stress gestört werden könne. Zur lesbischen Anlage findet sich eine analoge Aussage, nämlich, dass diese das Produkt von sehr „entspannten“ Müttern seien, deren Vermännlichungshormone mangels Stress seltener ausgeblieben seien.

Allerdings wenden Kritiker dieser und ähnlicher Theorien ein, dass es sich bei der Annahme, dass schwule Männer irgendwie „weiblicher“ sein müssten als heterosexuelle, oder lesbische Frauen „männlicher“, lediglich um ein heteronormatives Postulat handelt, welches keinesfalls bewiesen ist. Es erklärt ebenfalls nicht, warum schwule Männer einen anderen „verweiblichten“ Mann gegenüber einer „vermännlichten“ Frau als Partner bevorzugen sollten (siehe auch Straight Acting).

In einer Veröffentlichung der schwedischen Forscher Ivanka Savic und Per Lindström vom Karolinska-Institut in Stockholm in der Zeitschrift „Proceedings of the National Academy of Sciences“ wird von Unterschieden in der Gehirnstruktur von homosexuellen und heterosexuellen Menschen berichtet.
Darin wird beschrieben, dass die Gehirne von homosexuellen Frauen und heterosexuellen Männern eine ähnliche Asymmetrie aufweisen, da die rechte Hirnhälfte ein wenig größer ist als die linke. Bei homosexuellen Männern und heterosexuellen Frauen fanden sich keine solchen Größenunterschiede.

Weiterhin wird von unterschiedlich stark ausgeprägten Nervenzellverbindungen in der Amygdala, einem Teil des limbischen Systems, berichtet. Hier zeigten sich die gleichen Zusammenhänge wie bei den unterschiedlichen Gehirngrößen: In den Gehirnen von homosexuellen Frauen und heterosexuellen Männern waren die Amygdala-Verbindungen in der rechten Hirnhälfte stärker ausgeprägt als in der linken. Bei homosexuellen Männern und heterosexuellen Frauen waren die Amygdala-Verbindungen in der linken Hirnhälfte ausgeprägter. Diese lassen sich, so die Forscher, bereits bei Babys unmittelbar nach der Geburt nachweisen.

Genetische Unterschiede, so die Forscher, seien wahrscheinlich nicht für diese Unterschiede verantwortlich, ebenso wenig Wahrnehmung und erlerntes Verhalten. Welche Mechanismen für die unterschiedliche Entwicklung verantwortlich sind und, ob diese pränatal oder erst unmittelbar nach der Geburt eine Rolle spielen, ist nicht bekannt. Wilson/Rahman sprechen sich jedoch gegen die durch diese Studie implizierte Annahme aus, homosexuelle Männer hätten „weibliche“ Gehirne und homosexuelle Frauen „männliche“, was nur gängigen Stereotypen entspräche. Sie postulieren, homosexuelle wie heterosexuelle Menschen besäßen eine mosaikartige Gehirnstruktur, bestehend aus männlichen und weiblichen Anteilen.

1996 veröffentlichten Anthony Bogaert und Ray Blanchard von der Brock University in Kanada eine Untersuchung, wonach statistisch gesehen jüngere Brüder eher homosexuell werden als ältere Brüder. Nach ihren Daten steigt die Wahrscheinlichkeit der Homosexualität bei jedem weiteren männlichen Nachkommen um ein Drittel. In einer Nachfolgeuntersuchung konnte Bogaert zudem belegen, dass dieser Effekt nicht nachträglich durch familiäre Verhältnisse (zum Beispiel Adoption) beeinflusst wird, sondern ein rein biologischer Effekt ist. Bogaert vermutet, dass beim Tragen des ersten männlichen Kindes gewisse unbekannte biochemische Prozesse bei der Mutter ausgelöst werden, die sich bei jedem weiteren männlichen Nachkommen verstärken und zu diesem Effekt führen.

Unter der Annahme, Homosexualität sei genetisch disponiert oder die Ausbildung sei genetisch mit beeinflusst, wird die Frage nach dem evolutionären Nutzen gestellt, da Eigenschaften, welche die Fortpflanzung einer Art verringern, als schädlich eingestuft werden. Da die als wahrscheinlich anzusehende Häufigkeit von Homosexualität als nicht vernachlässigbare Größe angesehen werden kann, wird in der Wissenschaft die Frage erörtert, ob Homosexualität oder homosexuelles Verhalten, gerade auch in sozial lebenden Arten, einen evolutionären Vorteil haben könnte oder aber die offensichtlichen Nachteile bezüglich der Vermehrungsraten durch andere Vorteile oder Verhaltensweisen kompensiert werden könnten.

Verschiedene Thesen und Untersuchungsergebnisse werden erörtert:

Homosexuelles Verhalten kommt auch bei Tieren vor und kann im Tierreich „als nahezu universelles Phänomen“ bezeichnet werden. Bei ca. 1500 Tierarten wurde gleichgeschlechtliches Verhalten festgestellt, wobei ca. ein Drittel dieser Fälle gut dokumentiert ist. Homosexuelles Verhalten lässt sich beispielsweise bei den Bonobos beobachten, die über eine für Menschenaffen ungewöhnliche matriarchale Organisationsstruktur verfügen. Darüber hinaus kann man Bonobos belegbar als grundsätzlich bisexuelle Tierart betrachten. Mehr als für männliche Homosexualität sind Bonobos vor allem für ihren Lesbianismus bekannt.

Einige Trauerschwäne Australiens bilden sexuell aktive männliche Paare, die entweder Nester stehlen oder zeitweilige Dreierbeziehungen mit Weibchen eingehen, um in den Besitz von Eiern zu gelangen. Sobald die Eier gelegt sind, wird das Weibchen vertrieben. Der von homosexuellen Paaren aufgezogene Nachwuchs erreicht das Erwachsenenalter dabei häufiger als derjenige von gemischtgeschlechtlichen Paaren.

Im Zoo von Bremerhaven lebten um 2006 drei homosexuelle Paare von Pinguinen, die auch nach dem Import mehrerer schwedischer Pinguinweibchen ihre Beziehung fortsetzen. Die Ankündigung des Versuchs erregte vor allem durch die unsensible Wortwahl weltweites Aufsehen und Proteste. 2009 zog eines der Paare auch ein Küken groß.

Unter vielen Delfinarten gibt es zahlreiche Formen homosexuellen Verhaltens. Diese Verhaltensweisen treten unter anderem aus Gründen der Festigung von Beziehungen in einer Delfinschule sowie beim Dominanzkampf zwischen Männchen auf, zeigen sich also in verschiedenen sozialen Situationen.

Die Fachzeitschrift Nature berichtete im Oktober 2006 von der ersten wissenschaftlichen Ausstellung über Homosexualität bei Tieren im Osloer Naturhistorischen Museum. Dort ist z. B. von zwei männlichen Walen (Südkaper) mit erigierten Penissen „bei Sexspielen“ ("engaged in sexual games") und von zwei männlichen Giraffen beim erfolgreichen Versuch einer analen Penetration mit Ejakulation zu lesen. In einer Feldstudie unter männlichen Giraffen wurde sogar beobachtet, dass Homosexualität 94 Prozent der gesamten beobachteten sexuellen Aktivität ("94 percent of all observed sexual activity") ausmachte (der Anteil liegt nie unter 30 %) – ein Wert, der bei keiner anderen Art beobachtet wurde.



"Siehe auch Homosexualität in Deutschland#Vereine und Organisationen"



"Siehe auch: Homosexualität in der Schweiz, Geschichte der Homosexualität in der Schweiz"



Es gibt in sehr vielen Städten Rosa Telefone, um betroffene Menschen und Angehörige zu beraten. Die Beratung erfolgt anonym. Die meisten haben bundeseinheitlich die Nummer 19446. In einigen Städten gibt es auch sogenannte Überfalltelefone für Opfer antihomosexueller Gewalt. Die meisten haben bundeseinheitlich die Nummer 19228.

Des Weiteren gibt es häufig Coming-out-Gruppen, auch speziell für Jugendliche.

Eine große Bedeutung hat mittlerweile die Onlineberatung. Sie wird von verschiedenen Trägern angeboten.

Beratungsstellen und Organisationen, die entgegen der in der Sexualwissenschaft und Psychologie weithin akzeptierten wissenschaftlichen Meinung an eine Veränderlichkeit der sexuellen Orientierung glauben und diese fördern wollen, sind eher selten. Sie gehören meist der sogenannten Ex-Gay-Bewegung an, die von christlichen Fundamentalisten in den Vereinigten Staaten als Teil eines „Kulturkampfs“ gegen die „Ausbreitung der Homosexualität“ gegründet wurde, inzwischen aber auch in Deutschland unter anderem durch die Laienseelsorgeorganisation Wuestenstrom vertreten ist. Aufgrund ihres „Potentials, Schaden zuzufügen“ "(American Psychological Association)" warnen viele größere psychologische und medizinische Fachverbände vor einer Teilnahme an solchen Programmen. Einige Teilnehmer solcher Programme sagen öffentlichkeitswirksam von sich, sie hätten Veränderungen in ihrer sexuellen Orientierung erfahren. Diese Veränderungen werden von Kritikern allerdings als unwahrscheinlich angesehen: Jeremy Marks, 14 Jahre lang einer der Wortführer der christlichen Ex-Gay-Bewegung in Großbritannien, hat seine Ansichten über die „Heilbarkeit“ von Homosexualität revidiert. Marks hat geäußert, dass er niemals in der Lage gewesen sei, seine sexuelle Orientierung oder die Orientierung anderer Menschen zu verändern. "„Keiner der Menschen, die ich betreut habe, hat seine sexuelle Orientierung geändert, egal wie viel Mühe und Gebete er auch investiert hat“". Der ehrliche Weg bringe einen größeren Nutzen. – Selbst der wohl bekannteste Vertreter der Veränderungstheorie in Deutschland, Markus Hoffmann, Leiter der Laienseelsorgeorganisation Wüstenstrom, räumt ein, dass er auch nach längeren und erheblichen Veränderungsbemühungen immer noch homoerotische Gefühle hat. – Günter Baum, der Vorgänger von Markus Hoffmann als Leiter bei Wüstenstrom, sagt heute: "„In all den Jahren bei Wüstenstrom hat sich an meinen schwulen Gefühlen nichts geändert. Ich habe mir wirklich viel Mühe gegeben“". Die Therapien seien wie eine Haartönung: "„Man kann sich so viel Blond ins Haar schmieren wie man will – die eigentliche Haarfarbe kommt immer wieder durch“".

In wissenschaftlicher Hinsicht berufen sich viele dieser Gruppen auf eine vielkritisierte Studie von Robert L. Spitzer aus dem Jahr 2001. Spitzer selbst hat im Jahr 2012 die Studie zurückgezogen und die daran geäußerte Kritik weitgehend bestätigt. Der von der Ex-Gay-Bewegung häufig in ihrem Sinn zitierte Professor Gunter Schmidt, Sexualwissenschaftler, Sozialpsychologe und Psychotherapeut aus Hamburg, äußerte sich zur entsprechenden Verwendung eines seiner Aufsätze wie folgt: 












</doc>
<doc id="8896" url="https://de.wikipedia.org/wiki?curid=8896" title="Beethoven (Begriffsklärung)">
Beethoven (Begriffsklärung)

Beethoven ist der Familienname folgender Personen:

Beethoven ist der Name folgender Personen:

Beethoven bezeichnet:

Siehe auch:



</doc>
<doc id="8897" url="https://de.wikipedia.org/wiki?curid=8897" title="Heterosexualität">
Heterosexualität

Heterosexualität (sehr selten auch "Alloiophilie") ist die sexuelle Orientierung, bei der Romantik und sexuelles Begehren ausschließlich für Personen des anderen Geschlechts empfunden werden. Das Adjektiv "heterosexuell" wird auch auf sexuelle Handlungen mit andersgeschlechtlichen Partnern angewendet, wenn die Beteiligten nicht ausschließlich heterosexuell aktiv sind.

Heterosexuelles Verhalten ist kein zwingendes Merkmal. Heterosexuelles Begehren oder Handlungen kann auch gemeinsam mit homosexuellem Begehren oder Handeln vorhanden sein. Nicht jeder, der heterosexuelle Erfahrungen hatte, muss heterosexuell sein. Sexuelle Erfahrungen können – vor allem im Jugendalter – mehr von sexueller Neugier oder von gesellschaftlichen Erwartungen bestimmt sein als von einer festen sexuellen Orientierung.

Der Begriff „Heterosexualität“ ist eine hybride Wortneubildung aus dem Jahre 1868, geprägt vom Schriftsteller Karl Maria Kertbeny aus Griech. ἕτερος "heteros" ‚der andere‘, ‚ungleich‘ und lat. "sexus" ‚das männliche und das weibliche Geschlecht‘. Gleichzeitig prägte er als Antonym den Begriff „Homosexualität“. Nach ihm verwendete den Begriff erstmals Gustav Jäger 1880 in der zweiten Auflage seines Buches "Die Entdeckung der Seele".

Der Begriff „Heterosexualismus“ wird manchmal vor 1933 verwendet, danach nur sehr vereinzelt und in neuerer Zeit vereinzelt als Synonym für Heteronormativität. Der Begriff „Heterosexualisten“ wird in neuerer Zeit vereinzelt als Antonym in Verbindung mit dem historischen Wort „Homosexualisten“ verwendet.

Der Begriff „Alloiophilie“ wurde von Magnus Hirschfeld spätestens 1918 aus den Griechischen Wörtern ἄλλοιος "alloios" ‚der andersartige‘ und φιλία "philia" ‚Freundschaft‘ geprägt. Es wird seit je her äußerst selten verwendet.

In der Nuance „Allophilie“ (von ἄλλος "allos" ‚der andere‘) wird es in anderem Kontext in der Literaturwissenschaft, Biologie, Zoologie und Anatomie verwendet. Alloerotik wurde von Sigmund Freud eingeführt, um die Ausrichtung der Libido auf andere Personen zu beschreiben, im Gegensatz zur Autoerotik, die Kertbeny „Monosexualität“ genannt hatte.

Von LGBT werden Menschen heterosexueller Orientierung im deutschen Sprachgebrauch umgangssprachlich auch als „Heten“ (Einzahl: die Hete, erste Silbe lang und betont, grammatisch feminin, jedoch vorwiegend für heterosexuelle Männer verwendet; oder engl. "straight" (deutsch etwa: geradlinig)) bezeichnet. Weiterhin ist die Kurzbezeichnung „Hetero“ üblich, für heterosexuelle Frauen auch „Hetera“.

Die heterosexuelle Vermehrung hat sich innerhalb der Evolution der Sexualität vor ca. 600 Millionen Jahren weitgehend gegenüber der asexuellen Reproduktion durchgesetzt und verbessert die Durchmischung der Gene im Rahmen der Fortpflanzung. Die Sexualität des Menschen, sowie einiger anderer Spezies, ist jedoch komplexer, da diese nicht allein auf triebgesteuerte Vermehrung ausgerichtet ist.

Ende des 19. Jahrhunderts wurde die Kategorie der Heterosexualität in Analogie zum psychiatrischen Begriff der Homosexualität geprägt, welche man damals im Gegensatz zur aktuellen Sexualwissenschaft als „krankhafte Perversion“ ansah. Die sexualkonzeptionelle Verortung der Heterosexualität im Lehrgebäude der Wissenschaft verdankt ihre Erfindung daher einer medizinischen Normierung der Lüste, wie sie bereits im 18. Jahrhundert mit dem Kampf gegen die Masturbation begann (Pathologisierung). 

Für Sigmund Freud beruhte die Heterosexualität, ähnlich wie die Homosexualität, auf einer lebensgeschichtlichen "„Einschränkung der Objektwahl“". Alle Normalen ließen daher, so Freud, "„neben ihrer manifesten Heterosexualität ein sehr erhebliches Ausmaß von latenter oder unbewusster Homosexualität“" erkennen.

Die soziale Bindung zum Sexualpartner kann bei Heterosexuellen verschiedene Ausmaße annehmen. Sie reicht von einem einmaligen Treffen („One-Night-Stand“), einer Liebschaft oder Affäre bis zu festen Beziehungen in Form von Partnerschaft, Ehe oder eheähnlicher Verbindung. Anders als bei anderen sexuellen Orientierungen ist für heterosexuellen Beziehungen in jedem Staat der Erde immer eine rechtliche Anerkennung einer Beziehung möglich, sofern die Partner dies wünschen und sie die allgemeinen Voraussetzungen erfüllen (Mindestalter, Inzestverbot …). Sexuelle Kontakte außerhalb der anerkannten Beziehungen erregen gesellschaftlich oftmals Missfallen, in einigen Ländern werden sie sogar strafrechtlich sanktioniert.

Gewaltfreie, einvernehmliche heterosexuelle Handlungen sind in allen Ländern erlaubt, sofern diese innerhalb einer staatlich anerkannten Partnerschaft (Ehe) stattfinden. Solche Partnerschaftsformen stehen Heterosexuellen in allen Ländern auch zur Verfügung. Außerhalb einer solchen Partnerschaft sind gewaltfreie, einvernehmliche heterosexuelle Handlungen in der Regel erlaubt, sofern die Beteiligten ein Mindestalter haben, welches staatlich unterschiedlich bestimmt sein kann. Allerdings ist bzw. war die Gesetzgebung in Bezug auf heterosexuelle und davon unterschiedliche wie beispielsweise gleichgeschlechtliche sexuelle Handlungen in vielen Ländern asymmetrisch. So waren beispielsweise in Deutschland bis 1994 durch § 175 StGB und Österreich bis 2002 durch § 209 heterosexuelle Kontakte mit Jugendlichen jüngeren Alters eher erlaubt als homosexuelle Kontakte.

In einigen islamisch geprägten Ländern wird heterosexuelle Betätigung außerhalb der Ehe strafrechtlich verfolgt. Außereheliche sexuelle Handlungen von verheirateten Frauen und Männern werden oft sogar mit dem Tode bestraft. Prostitution ist in diesen Ländern meist verboten und wird, beispielsweise in Saudi-Arabien, ebenfalls streng bestraft. Die geringe Anzahl der bekannt gewordenen Urteile lässt allerdings vermuten, dass dies bei verheirateten Männern in der Rechtspraxis dieser Länder selten Anwendung findet. Das Prostitutionsverbot kann von Schiiten allerdings durch das Konstrukt der Zeitehe umgangen werden (siehe: " Prostitution in islamischen Ländern").

In den mehr von der christlichen Kultur geprägten Ländern war Ehebruch bis weit in die zweite Hälfte des 20. Jahrhunderts meist strafbar; die sexuelle Betätigung außerhalb der Ehe wurde als unmoralisch angesehen und durch gesetzliche Regelungen erschwert. So wurde erst im Jahre 1969 in Deutschland der Straftatbestand der Kuppelei, der unter anderem auch die Gewährung von Unterkünften für nicht verheiratete Paare zum Zweck des Beischlafs unter Strafe stellte, zunächst entschärft und dann 1973 weitgehend abgeschafft. Die Wandlung der Sexualmoral, unter anderem infolge der sexuellen Revolution in den 1960er Jahren, und die Möglichkeit der Empfängnisverhütung durch die Pille haben in den christlich geprägten Gesellschaften dazu beigetragen, dass sich im Allgemeinen die moralische Bewertung des vorehelichen Geschlechtsverkehrs verändert hat.

Fast alle Glaubensgemeinschaften halten sexuelle Handlungen zwischen einem Mann und einer Frau für generell zulässig. Einige sehen sie jedoch als sündhaft an und fordern stattdessen einen zölibatären Lebensstil, wie etwa die Shaker, die Harmony Society und das Ephrata Cloister.

In Teilen der Queer Theory und den Gay&Lesbian Studies wird Heterosexualität als gesellschaftliches Organisationsprinzip verstanden, siehe hierzu Heteronormativität.




</doc>
<doc id="8899" url="https://de.wikipedia.org/wiki?curid=8899" title="Kohle">
Kohle

Kohle (von altgerm. "kula", althochdeutsch "kolo", mittelhochdeutsch "Kul") ist ein schwarzes oder bräunlich-schwarzes, festes Sedimentgestein, das durch Karbonisierung von Pflanzenresten entsteht. Sie besteht überwiegend aus dem chemischen Element Kohlenstoff.

Kohle ist auf allen Kontinenten vorhanden. Sie ist vorwiegend zwei geologischen Formationen zuzurechnen: Pflanzen aus dem Tertiär, aus denen sich meist Braunkohle entwickelt hat, und Karbon. Die Pflanzen dieser Epoche wurden zu Steinkohle. Daneben gibt es auch noch jüngere Steinkohlen aus der Jura- und Kreidezeit, die z. B. in Westkanada gefunden werden.

Verwendet wird Kohle hauptsächlich als Träger fossiler Energie. Bei ihrer Verbrennung wird Wärme freigesetzt, die z. B. zum Heizen genutzt werden kann. Kohleverbrennung ist weltweit eine der meistverbreiteten Techniken zur Erzeugung elektrischer Energie. Sie ist ebenso als Ausgangsstoff bei der Koks- und Graphit<nowiki>herstellung</nowiki> sowie der Gewinnung flüssiger Kohlenwasserstoffe von Bedeutung. Der Heizwert einer Steinkohleeinheit dient als Vergleichsmaßstab für andere Brennstoffe.

Sowohl Inkohlung als auch Kohleverbrennung sind wesentliche Bestandteile des globalen Kohlenstoffzyklus.

Die Menge der weltweiten Kohlevorräte ist Gegenstand anhaltender Untersuchungen. Manche Schätzungen rechnen bei gleichbleibendem Verbrauch (2004) mit mehreren hundert Jahren bis zu deren Erschöpfung, andere Schätzungen gehen davon aus, dass das Kohlefördermaximum bereits im Jahr 2025 erreicht sein könnte. Fördermengen der einzelnen Kohlearten finden sich unter Kohle/Tabellen und Grafiken.

Das Ausgangsmaterial von Kohle ist hauptsächlich pflanzlichen Ursprungs. Im Karbon, der erdgeschichtlichen Entstehungszeit der heute abbaubaren Steinkohle, herrschte ein sehr warmes und feuchtes Klima mit einem ausgeprägten Pflanzenwachstum. Beim Absterben einzelner Pflanzen versanken diese im Sumpf und wurden so dem normalen aeroben Zersetzungsprozess entzogen – es entstand Torf.

Bei Meereseinbrüchen wurden diese Sümpfe mit Sedimenten bedeckt. Unter dem wachsenden Druck und der erhöhten Temperatur begann der Prozess der Inkohlung. Der Druck presste das Wasser aus dem Torf und es entstand zuerst Braunkohle. Der zu dieser Zeit noch geringe Druck presste nur wenig Wasser aus der Kohle. Mit der Ablagerung weiterer Schichten erhöhte sich der Druck und immer mehr Wasser wurde aus der Kohle herausgepresst. Nach und nach wurde aus der Braunkohle Steinkohle und mit nochmals mehr Druck Anthrazit. Deshalb ist die wirtschaftliche Qualität der Kohle umso besser, je tiefer sie in der Erde liegt und je älter sie ist.

Insbesondere während des Karbons vor etwa 280 bis 345 Millionen Jahren entstanden mächtige Steinkohlelagerstätten, die heute zu den weltweit wichtigsten Energielieferanten zählen. Die Braunkohlelagerstätten sind wesentlich jünger und sind im Tertiär vor 2,5 bis 65 Millionen Jahren entstanden.

Je nach Bildungsort und Bildungsumständen unterscheidet man zwischen palustrischen, paralischen und intramontanen Kohlebildungen. Unter palustrisch versteht man dabei Kohlebildungen, die auf Moore in Feuchtgebieten nahe Gewässern, wie z. B. Flüssen zurückgehen. Paralisch bedeutet, dass das Kohlelager auf Moorbildungen im Bereich der Meeresküste zurückgeht. In die einzelnen Flözbildungen sind dabei immer wieder marine Sedimente eingeschaltet, die auf kurzzeitige transgressive Phasen zurückgehen. Intramontane Kohlelagerstätten finden ihren Ursprung in Moorbildungen innerhalb von Becken in gebirgigen Regionen.

Wichtigstes Ausgangsmaterial des Torfs waren im frühen Oberkarbon mit 60–80 % Bärlapppflanzen, wobei Periderm (Borke) das häufigste Gewebe war. Im mittleren Pennsylvanium der Kohlevorkommen in den Appalachen dominierte "Cordaites". Im späten Unterkarbon Euramerikas herrschten Baumfarne der Gattung "Marattiaceae" vor.

Neue Untersuchungen legen einen Zusammenhang nahe zwischen der Bildung mächtiger Kohleflöze und der Evolution von Weißfäule. Im Kambrium gab es keine Lebewesen, die Lignin abbauen konnten. Erst im Tertiär entwickelten sich Weißfäulepilze, die Lignin zersetzten. In der Zeit danach konnte sich Kohle nur noch unter Luftabschluss bilden.

Kohle kann sowohl überirdisch im Tagebau als auch im Untertagebau gefördert werden. Weltweit werden ca. 40 % der Kohle im Tagebau gewonnen, der Rest im Tiefbau.

Die Braunkohlevorräte in Deutschland betrugen im Februar 2014 etwa 76,8 Milliarden Tonnen, von denen 40,3 Mrd. t mit dem Stand der heutigen Technologie wirtschaftlich gewinnbar wären. Damit würden die Vorräte bei konstanter Förderung (2013: 183 Mio. t) noch für 220 Jahre ausreichen.

Von den deutschen Steinkohlevorräten gelten rund 24 Milliarden Tonnen als gewinnbar. Angesichts einer aktuellen Förderquote von 25,7 Millionen Tonnen (2004) ergibt sich eine theoretische Reichweite von über 900 Jahren. Aufgrund ungünstiger geologischer Bedingungen sind diese jedoch zurzeit nicht international wettbewerbsfähig förderbar.

Vertreter der deutschen Kohlewirtschaft beziffern deshalb unter Beibehaltung der derzeitigen Fördermengen die Reichweite der deutschen Kohle auf etwa 400 Jahre.

Die deutsche Energy Watch Group, eine unabhängige Analytikergruppe um Wissenschaftler der Ludwig-Bölkow-Stiftung (München), kam im Frühjahr 2007 hinsichtlich der weltweiten Kohlereserven und insbesondere hinsichtlich der Reservensituation in Deutschland zu einem anderslautenden Ergebnis:

Im 2. Quartal 2016 lag Weltmarktpreis für Kraftwerkskohle bei zirka 56 Euro pro Tonne SKE.

Nach dem Weltenergiereport von BP fördert China im Jahr 2012 47,5 % aller Kohle weltweit. Weit abgeschlagen folgen die USA (13,4 %), Australien (6,3 %), Indonesien (6,2 %), Indien (6,0 %), Russland (4,4 %), Deutschland (1,2 %) und die Türkei (0,4 %).

Bei der Einteilung der Kohlen sind zwei Begriffe sorgfältig auseinanderzuhalten: "Kohlenarten" und "Kohlensorten".

Kohle besteht aus kristallinem Kohlenstoff, organischer Substanz, Mineralen und Wasser. Häufig wird vom „Aschegehalt“ der Kohlen gesprochen; die Asche entsteht jedoch erst bei der Verbrennung.
Die organische Substanz wird in verschiedene Mazerale unterteilt, welche aufgrund der unterschiedlichen Herkunft des Ausgangsmaterials und der Entwicklungsgeschichte auch unterschiedliche Eigenschaften aufweisen. Die Petrographie nutzt die Eigenschaft des Mazerals Vitrinit, Licht zu reflektieren, um den Rang und damit die Kohlenart einer Kohle zu bestimmen.
Die organische Substanz besteht hauptsächlich aus Kohlenstoff, Wasserstoff, Stickstoff, Schwefel und Sauerstoff, die Verteilung variiert mit der Herkunft der Kohle und der Kohlenart (siehe nachstehende Tabelle). Kohle ist ein organisches Makromolekül, dessen genaue Struktur unbekannt ist und sich zudem je nach Herkunft ändert. Es gibt verschiedene Modelle, die versuchen, diese Struktur abzubilden; eines davon ist rechts dargestellt.

Während der Gehalt an organischer Substanz und Mineralien unter normalen Umgebungsbedingungen praktisch unveränderlich sind, kann der Wassergehalt stark schwanken, d. h. Kohle nimmt Wasser auf und gibt es auch wieder ab.
Daher sind Bezugszustände definiert, die dies berücksichtigen. Unterschieden werden hauptsächlich die Zustände:
Der Bezugszustand waf ist hypothetisch, hier sind der Anteil an Wasser und Asche rechnerisch abgezogen; er dient zur Charakterisierung der organischen Substanz. Alle Bezugszustände können ineinander umgerechnet werden.

Da einige Eigenschaften der Kohle von erheblicher finanzieller Bedeutung sind (Preisbildung aufgrund von Analysedaten) und national und international mit denselben Methoden bestimmt werden müssen, wurden nationale (DIN) und internationale Normen (ISO) erarbeitet. In Deutschland machte dies der Arbeitsausschuss „Prüfung fester Brennstoffe“ im Normenausschuss Bergbau (FABERG) im DIN, international das technische Komitee ISO/TC 27 „Solid Mineral Fuels“ in der ISO.

Beim Wassergehalt unterscheidet man zwischen grober und hygroskopischer Feuchtigkeit. Grobe Feuchtigkeit bezeichnet das rein mechanisch anhaftende Wasser, hygroskopische Feuchtigkeit ist das in den Kapillaren der Kohlekörner festgehaltene Wasser. Die Bestimmung erfolgt nach DIN 51718. Bei den meisten Kohlen wird die Bestimmung zweistufig durchgeführt: Die grobe Feuchtigkeit bestimmt man im Trockenschrank bei 30±2 °C, die hygroskopische Feuchtigkeit bei 106±2 °C unter Stickstoffatmosphäre. Bei oxidativ stabilen Kohlen (Anthrazit) kann der Gesamtwassergehalt auch einstufig bei 106 °C in Luft bestimmt werden. Eine weitere Methode ist die Xylol-Destillation mit anschließender volumetrischer Wasserbestimmung.

Als Asche wird der anorganische Rückstand bezeichnet, der nach DIN 51719 durch Verbrennung der Kohle im Ofen bei 815 °C verbleibt. Asche ist ein Gemisch von Alkali-, Erdalkali-, Eisen- und Aluminiumsalzen wie Oxiden, Sulfaten, Silikaten und Phosphaten.

Flüchtige Bestandteile sind nicht eindeutig zu definieren, die Bestimmung ist ein Konventionsverfahren. Bestimmt werden diese nach DIN 51720: die Kohle wird 7 min auf 900 °C erhitzt. Entweichende Bestandteile, korrigiert um das ebenfalls entweichende Wasser, sind konventionsgemäß die flüchtigen Bestandteile.

Kohlenarten werden international nicht einheitlich unterteilt. Die Tabelle zeigt eine Zusammenstellung der Bundesanstalt für Geowissenschaften und Rohstoffe (BGR). Wesentliche Kriterien zur Klassifizierung sind Energiegehalt (Heizwert), Flüchtige Bestandteile und Vitrinitreflexion.
Eine ähnliche Einteilung der Kohlearten erfolgt nach dem Gehalt an Flüchtigen Bestandteilen, diese Einteilung ist vor allem im Ruhrbergbau üblich. Die nächste Tabelle zeigt die Kohlenarten sowie typische Elementarzusammensetzungen.

Aufgrund des hohen Wassergehaltes ist der Heizwert der Rohbraunkohle nur etwa 2/3 so hoch wie der von Steinkohle.

Kohle enthält auch Stickstoff im Bereich von etwa 0,8 % bis etwa 2 %, Schwefel im Bereich von etwa 0,2 % bis etwa 3 %, in Einzelfällen auch höher.
Mit steigender Inkohlung nehmen die flüchtigen Bestandteile, der Wasserstoff- und Sauerstoff-Gehalt ab, der Heizwert nimmt zu.

"Braunkohle" wird heute – gemahlen und getrocknet – fast ausschließlich als Brennstoff für die Stromerzeugung genutzt. Der Anteil der Förderung, der zu Briketts gepresst wird, ist erheblich zurückgegangen. Braunkohle ist bräunlich bis schwarz und hat mit bis zu 50 Prozent einen hohen Feuchtigkeitsanteil. Ihr Kohlenstoffgehalt liegt bei 65-70 % in der wasserfreien Kohle. Der Schwefelgehalt beträgt bis zu 3 %. Sie wird meist im Tagebau gewonnen.

In Deutschland gibt es drei große Braunkohle-Reviere:

Das größte deutsche Braunkohleunternehmen ist die RWE Rheinbraun AG in Köln, ihre Briketts werden unter dem Namen Union-Brikett vermarktet.

Entstehungszeit der Braunkohle ist das Tertiär. Wie bei der Steinkohle, spielt auch hier das Holz abgestorbener Bäume eine Rolle, welches unter Druck und Luftabschluss den Prozess der Inkohlung durchlief. Jedoch ist Braunkohle in einem jüngeren Erdzeitalter entstanden, deswegen unterscheidet sie sich qualitativ von der Steinkohle zum Beispiel durch einen höheren Schwefelgehalt und eine grobe, lockere und poröse Grundmasse, in der auch große Einschlüsse (mitunter ganze Baumstubben) zu finden sind.

Bei der Braunkohle unterscheidet man die Glanzbraunkohle, Mattbraunkohle und die Weichbraunkohle. Die Sorten mit einem hohen Anteil flüchtiger Bestandteile lassen sich in einer Kokerei zu Koks verarbeiten. Je nach Temperatur des Verfahrens erhält man Schwel- oder Grudekoks. Braunkohlenkoks wird in erster Linie im großtechnischen Maße zur Filtration verwendet, wobei das Material die im Labormaßstab übliche Aktivkohle aus Holz ersetzt.

Bei der Braunkohlenverfeuerung fällt als Nebenprodukt Braunkohlenflugasche an.

Huflattich ist laut Heilpflanzenbuch von Gerhard Madaus von 1938 die einzige Pflanze, die problemlos auf reiner Braunkohle gedeihen kann.

Steinkohle ist ein Sammelbegriff für höherwertige Kohlen. Entstanden ist sie aus großen Urwaldbeständen, die im Prozess des Absterbens große Mengen Biomasse anhäuften, ähnlich wie in einem Torfmoor zur heutigen Zeit. Diese Ablagerungen wurden teilweise in regelmäßigen Abständen (deswegen gibt es im Steinkohlebergbau meist mehrere <nowiki>Kohle</nowiki>flöze) durch andere Sedimente wie Tone und Sand/Sandsteine abgedeckt. Dadurch wurde das organische Ausgangsmaterial unter Luftabschluss und hohen Drücken und Temperaturen solange verdichtet und umgewandelt, bis ein fester Verbund aus Kohlenstoff, Wasser und unbrennbaren mineralischen Einschlüssen entstand. Die Mineralsubstanz wird bei der Verbrennung verändert und erscheint dann in Form von Asche. Steinkohle zeichnet sich durch eine schwarze, feste Grundmasse aus, in welcher mitunter Einschlüsse und Abdrücke prähistorischer Pflanzen zu finden sind.

Abbaureviere in Deutschland:

Der Abbau von Steinkohle erfolgt in Deutschland in Bergwerken von bis zu 1.750 Meter Teufe. Hereingewonnen wird sie entweder mit dem Kohlenhobel oder dem Walzenschrämlader. In Kolumbien, Südafrika oder Australien kann die Kohle billiger als in Deutschland gefördert werden. Daher kommt auch in deutschen Kraftwerken Importkohle häufiger zum Einsatz als heimische Kohle. Damit in Deutschland nicht ausschließlich Kohle aus dem Ausland verstromt wird, existiert die Steinkohlesubvention, die im Jahr 2018 ausläuft.

Pechkohle wurde in Bayern (Penzberg, Peißenberg, Peiting, Hausham etc.) gefördert. Ihr Alter entspricht dem von Braunkohle. Infolge des höheren Bergdrucks hat sie jedoch Eigenschaften wie Anthrazit. Die Stollen in der Bergbauabteilung des Deutschen Museums in München sind mit Pechkohle an den Wänden errichtet worden.

An einigen Orten in Finnland und Russland gefundenes Gestein, das bis zu 95 % aus Kohlenstoff besteht, wird als Shungitkohle bezeichnet.

In Niedersachsen und im nördlichen Westfalen kommt die Wealdenkohle aus der Unterkreide vor. Sie erreicht verschiedene Inkohlungsstadien von Braunkohle bis zum Anthrazit. Bis zu 5 Flöze wurden in dieser Zeit gebildet.

In verschiedenen Steinkohle-Lagerstätten kommt in sehr geringem Umfang Sapropelkohle vor, die aus Faulschlamm entstanden ist. Sie ist weich und lässt sich gut bearbeiten, so dass aus ihr z. B. Figuren geschnitzt werden können.

Kohle wird überwiegend als fester Brennstoff benutzt, um Wärme durch Verbrennung zu erzeugen. Um z. B. in Kohlekraftwerken elektrische Energie zu erzeugen, wird mittels der freigesetzten Wärme Wasserdampf erzeugt, der wiederum Dampfturbinen antreibt. Um zu vergleichen, welche Energiemenge mit welcher Kohle gewonnen werden kann, bedient man sich meist der Steinkohleeinheit.

2003 wurden weltweit 24,4 % der Primärenergie und 40,1 % der Elektroenergie durch die industrielle Nutzung von Kohle als Brennstoff erzeugt. Steinkohle und Braunkohle sind dabei in etwa gleich stark vertreten. In modernen Kohlekraftwerken werden verschiedene Techniken zur Schadstoffreduzierung und Effizienzsteigerung angewandt.

Ein nicht unbeträchtlicher Teil der Kohle wird nach ihrer Verkokung zur Reduktion von Erzen, hauptsächlich Eisenerz, in Hochöfen verwendet.

In Großbritannien entwickelte sich der Kohlebergbau seit Beginn des 18. Jahrhunderts zu einer wesentlichen Grundlage der von dort ausgehenden Industrialisierung. Ab dem 19. Jahrhundert fand die Kohle auch Verwendung zur Herstellung von Stadtgas, welches für die Straßenbeleuchtung und das Kochen sowie Heizen verwendet wurde. In Gaswerken gewann man das Stadtgas durch die Trockendestillation aus der Kohle – ein Nebenprodukt war der Koks. Im 20. Jahrhundert wurde das Stadtgas weitgehend durch Erdgas ersetzt.

Im 18. Jahrhundert wurde Braunkohle unter dem Namen Umber oder "Cöllnische Erde" als Farbpigment verwendet.

Koks ist ein fester, kohlenstoffhaltiger Rückstand, der vorzugsweise aus asche- und schwefelarmer Fettkohle gewonnen wird. Dabei werden in Kokereien die flüchtigen Bestandteile entfernt, indem sie in einem Ofen unter Luftausschluss bei mehr als 1000 °C erhitzt wird, so dass der feste Kohlenstoff und die verbleibende Asche verschmelzen. Als Nebenprodukt fällt Koksofengas und Steinkohlenteer an. Dieser ,Verkokung‘ genannte Prozess gehört zu den Verfahren der Kohleveredlung. Koks brennt mit einer nahezu unsichtbaren blauen Flamme. Es entstehen dabei keinerlei Ruß oder sichtbares Rauchgas. Koks wird als Brennstoff und als Reduktionsmittel bei der Eisenproduktion in Hochöfen eingesetzt. Er hat eine stumpf-graue Farbe und ist dabei hart und porös.

Briketts werden durch Pressen hergestellt, überwiegend aus Braunkohle. Steinkohlebriketts (Eierkohlen) haben für den Hausbrand in Deutschland nur noch geringe Bedeutung, lediglich aus Anthrazit hergestellte werden noch vertrieben.

Kohleverflüssigung bezeichnet chemische Verfahren, die aus fester Kohle flüssige Kohlenwasserstoffe erzeugen. Das Verfahren der direkten Hydrierung von Kohle diente zur Erzeugung von Gasen, Vergaser- und Dieselkraftstoffen.
Die Motivation zum großtechnischen Einsatz der Kohleverflüssigung ist der Ersatz von Erdöl als Ausgangsstoff für die Petrochemie und den Energiesektor. Die Verfahren erlangen an Bedeutung, wenn Erdöl nicht in ausreichender Menge zur Verfügung steht.

Die Gewinnung von Kohle im Tagebau ist mit einem immensen Flächenverbrauch verbunden. Während in Deutschland nur Braunkohle im Tagebau abgebaut wird, wird in anderen Ländern z. T. auch Steinkohle auf diese Art gefördert, beispielsweise in der Mine El Cerrejón in Kolumbien, der mit einer Fläche von ca. 690 km² größten Steinkohlemine Südamerikas.

Eine weitere Form ist das v. a. in den amerikanischen Appalachen praktizierte Mountaintop removal mining, bei dem zunächst Bergkuppen gesprengt und abgetragen werden, um anschließend die Steinkohle im Tagebau fördern zu können. Alleine in den Appalachen erstrecken sich die Abbaugebiete derzeit (2012) über eine Fläche von ca. 5.700 Quadratkilometer, häufig ursprüngliche Waldflächen. Zudem werden beim Bergbau Schwermetalle wie Arsen und Quecksilber freigesetzt, die die Umwelt belasten sowie die Gesundheit der Anwohner gefährden, auch kommt es häufig zu Überschwemmungen, da durch die Ablagerung des Abraums in Tälern oftmals Flussläufe begraben werden.

Um Lagerstätten möglichst vollständig ausbeuten zu können, werden bisweilen ganze Dörfer umgesiedelt, was zu Konfliktpotenzial mit der Bevölkerung führen kann (siehe auch Liste abgebaggerter Ortschaften). Unter Umständen werden auch ökologisch oder kulturell besonders wertvolle Gebiete zerstört – Beispiele dafür sind die Lacomaer Teichlandschaft und das Dorf Lakoma, die dem Tagebau Cottbus-Nord weichen mussten sowie zahlreiche Dörfer im sorbischen Siedlungsgebiet, die in den letzten Jahrzehnten weichen mussten bzw. teilweise bis heute bedroht sind (sh. Mühlrose).

In Braunkohletagebauen können ähnlich wie bei weiten, trockenen Äckern in der Landwirtschaft große Staubmengen entstehen. Daher ist der Einsatz von effizienter Staubbekämpfungstechnik unerlässlich.

Ein weiterer Aspekt ist die Absenkung des Grundwasserspiegels auf ein Niveau unterhalb der tiefsten Fördersohle im Braunkohletagebau. Dies geschieht mit Tauchpumpen in extra dafür geschaffenen Brunnen. Eine Absenkung des Grundwasserspiegels kann negative Auswirkungen auf die Flora haben, da obere Bodenschichten trockenfallen können. Auch führt die Absenkung zu einem Trockenfallen nahegelegener Brunnen, die ihr Wasser aus dem betroffenen Grundwasserleiter beziehen.

Umgekehrt führt eine Stilllegung eines Tagebaus zur Erhöhung des Grundwasserspiegels, sobald die Tauchpumpen abgestellt werden. Dies kann bei den in der Umgebung erbauten Gebäuden zu großen Schäden führen. Ein bekanntes Beispiel dafür ist die Umgebung der Stadt Korschenbroich, deren Bewohner seit der schrittweisen Stilllegung des Tagebaus Garzweiler I mit dem steigenden Grundwasserspiegel zu kämpfen haben.

Bei der Verbrennung von Kohle entstehen Kohlendioxid, Wasserdampf und andere Gase wie Schwefeldioxid, zudem werden umwelt- und gesundheitsschädliche Schadstoffe wie Flugasche, Feinstaub und Schwermetalle emittiert. Verglichen mit anderen in großer Menge genutzten fossilen Energieträgern pro nutzbarem Energiegehalt die größte Menge des Treibhausgases Kohlendioxid (CO) freigesetzt. Aufgrund ihres niedrigeren Wirkungsgrads sind Braunkohlekraftwerke (ca. 1080 Gramm CO/kWh) ungünstiger als Steinkohlekraftwerke (ca. 800 g CO/kWh). Die Freisetzung von CO bei einer Verbrennung mit Sauerstoff kann nur durch einen besseren Wirkungsgrad der Kraftwerke und dadurch auch geringeren Kohleverbrauch reduziert werden. Zusätzlich zu dem in Kraftwerken und Industrieanlagen direkt emittierten CO kann durch ungewollte Kohlebrände in Bergwerken weiteres CO freigesetzt werden.

Das Schwefeldioxid, das vor allem bei der Verbrennung von Braunkohle entsteht, ist mitverantwortlich für den Sauren Regen. Bei modernen Stein- und Braunkohlekraftwerken werden die Abgase in Rauchgasentschwefelungsanlagen (siehe auch REA-Gips) von Schwefeldioxid, durch katalytische (SCR) oder nichtkatalytische (SNCR) Entstickung von Stickoxiden und in elektrischen Abscheidern vom Staub gereinigt. Die bei der Verbrennung von Kohle entstehende Asche enthält erhöhte Konzentrationen von Schwermetallen wie z. B. Arsen und Quecksilber, aber auch der radioaktiven Elemente Uran und Thorium. Durch die Emission von Stäuben kommt es insbesondere in Staaten mit nur geringen Umweltschutzvorschriften zu starken, gesundheitsgefährdenden Luftverschmutzungen sowie Smog. Besonders in China, das stark auf Kohle als Energieträger setzt, ist dies ein großes Problem.

Umwelt-, Gesundheits- und Klimaschäden sind negative Auswirkungen der Kohlenutzung, die nicht von den Verursachern getragen werden, sondern von der Allgemeinheit. Volkswirtschaftlich spricht man hierbei von sogenannten Externen Kosten. Diese sozialen und ökologischen Kosten der konventionellen Energiegewinnung haben einen erheblichen Einfluss und übersteigen in manchen Fällen sogar die Endkundenpreise der Stromverbraucher. Für die USA wurden beispielsweise externe Kosten der Kohlestromerzeugung in Höhe von 175 bis 523 Mrd. US-Dollar pro Jahr ermittelt, mit einem konservativ gerechneten wahrscheinlichen Mittelwert von 345 Mrd Dollar, bzw. 17,8 ct/kWh. Nicht berücksichtigt wurden hierbei einige negativen Folgeeffekte wie z. B. Umwelteffekte durch Freisetzung giftiger Chemikalien und Schwermetalle in die Umwelt, Eutrophierung von Gewässern durch Stickstoffeintrag, Folgeeffekte von sauren Regens und ein Teil der Folgen der globalen Erwärmung.

Um die Umwelt-, Gesundheits- und Klimaschäden durch die Energieerzeugung zu minimieren und um Zwei-Grad-Ziels noch einhalten zu können, wird ein Ausstieg aus der Verstromung von Kohle als notwendig angesehen. Soll das Zwei-Grad-Ziel mit einer Wahrscheinlichkeit von mehr als 50 % erreicht werden, dürfen im Zeitraum 2011 bis 2050 nach Daten des IPCC maximal zwischen 870 und 1.240 Gigatonnen (Mrd. Tonnen) Kohlenstoffdioxid freigesetzt werden. Umgerechnet auf die Reserven bedeutet dies, dass im globalen Kontext etwa ein Drittel der Ölreserven, die Hälfte der Erdgasreserven und mehr als 80 % der Kohlereserven nicht verbrannt werden dürfen. Ein zentrales Element der Energiewende ist daher ein Kohleausstieg.

Der noch bis vor wenigen Jahren starke Anstieg des Ausbaus von Kohlekraftwerken verlor zuletzt an Dynamik; seit 2010 wurde weltweit nur ein Drittel der geplanten Kohlekraftwerke tatsächlich realisiert. In China und den USA sinkt der Kohleverbrauch und entkoppelt sich vom Wirtschaftswachstum. Mehrere Regierungen streben eine deutliche Reduzierung oder einen vollständigen Ausstieg aus der Kohle an. Die kanadische Provinz Ontario hat als erste größere Verwaltungseinheit einen Kohleausstieg bis 2014 umgesetzt. Zur Verminderung des CO-Ausstoßes und von Smog wollen 12 von 34 chinesischen Provinzen ihren Kohleverbrauch reduzieren.




</doc>
<doc id="8900" url="https://de.wikipedia.org/wiki?curid=8900" title="Holzkohle">
Holzkohle

Holzkohle (englisch charcoal; medizinisch (DAB): "Carbo pulveratur", "Carbo medicinalis"), ist ein fester Brennstoff und entsteht, wenn lufttrockenes Holz (auf 13 % bis 18 % Wasser getrocknet) unter Luftabschluss und ohne Sauerstoffzufuhr auf 275 °C erhitzt wird (Pyrolyse). Die Temperatur steigt dabei von selbst auf 350 °C bis 400 °C an (Holzverkohlung, ähnlich der Verkokung von Kohle). Dabei verbrennen die leichtflüchtigen Bestandteile des Holzes. Als Rückstand erhält man neben gasförmigen Zersetzungsprodukten (siehe Methanol) etwa 35 % Holzkohle.

Mit der gleichen Technologie kann man auch Torf zu "Torfkohle" und allgemein pflanzliche Ausgangsstoffe in Biokohle umwandeln. Torfkohle, die ähnliche Eigenschaften hat wie Holzkohle, war im 18. und frühen 19. Jahrhundert aufgrund von Holzknappheit recht verbreitet, ist heute aber kaum noch anzutreffen. Biokohle wird ein Potential als Mittel zur Kompensation der Kohlendioxid-Emissionen in Anbetracht der globalen Erwärmung zugeschrieben, ihre Herstellung wurde in den vergangenen Jahren intensiviert.

Nicht zu verwechseln ist Holzkohle mit den Pyrokohlen (Pflanzenkohle und Hydrokohle) oder "Faserkohle" (Fusit), auch fossile Holzkohle oder mineralische Holzkohle.

Seit dem Altertum konnten Metalle, insbesondere Eisen und Stahl, nur durch die Verbrennung von Holzkohle hergestellt werden. Steinkohle war dafür wegen verschiedener schädlicher Inhaltsstoffe nicht verwendbar. Die große Nachfrage nach Holzkohle führte im 16. Jahrhundert zur Holzknappheit.
Anfang des 18. Jahrhunderts waren die Wälder insbesondere in Großbritannien durch den zunehmenden Verbrauch von Holzkohle (und durch die anderen Verwendungen wie Schiffbau, Bauholz, Pfahlfundamente, Kanalbau etc.) so weit ausgebeutet worden, dass sich die Knappheit an Holzkohle in der Eisenproduktion zu einem ernsten nationalen Problem entwickelte. Die Eisenhütten waren deshalb gezwungen, andere Verbrennungsmaterialien zu suchen. Diese Rohstoffkrise zählt zu den Auslösern der Industriellen Revolution. Die Lösung des Problems bestand im Einsatz von Koks als Brennstoff für die Hochöfen und in der Entwicklung des mit Steinkohle betriebenen Puddelverfahrens zur Erzeugung von Schmiedeeisen.
Mit dem Ausbau der Eisenbahnen und der Verkehrswege wurde die Holzkohle immer mehr durch die Steinkohle verdrängt. Auch gewannen Strom und Gas als Energieträger immer mehr an Bedeutung. Während des ersten und Zweiten Weltkriegs wurde die Holzkohle vom Gewerbe, der Industrie und der Armee wieder begehrt (Holzvergaser etc.). Heute wird die Holzkohle fast überall nur noch für Grillzwecke gebraucht. Aber in Brasilien wird Holzkohle in großen Mengen für die Eisenverhüttung hergestellt, weil das Land kaum Steinkohle hat. Um den Holzbedarf zu decken, wurden große Eukalyptus­plantagen angelegt.

Auch heute noch ist in manchen Ländern die Abholzung zur Holzkohlegewinnung ein gravierendes Problem, zum Beispiel in Haiti, wo sie 60 % der heimischen Energie liefert. Das gleiche Schicksal droht Madagaskar, wo ebenfalls Holzkohle der verbreitetste Brennstoff ist.

Holzkohle ist ein Gemisch organischer Verbindungen mit 81 % bis 90 % Kohlenstoff, 3 % Wasserstoff, 6 % Sauerstoff, 1 % Stickstoff, 6 % Feuchtigkeit und 1 % bis 2 % Asche und nur unbedeutende Mengen Schwefel.

Die Holzkohle bildet ein lockeres, schwarzes Produkt mit einer Rohdichte zwischen 0,15 und 0,40 g cm und Reindichte zwischen 1,38 und 1,46 g cm. Die Porosität der Holzkohle schwankt in Abhängigkeit von der Holzart sowie der "Geschwindigkeit" und "Endtemperatur" der Verkohlung zwischen 72 % und 85 %, die Innere Oberfläche beträgt 50–100 m/g, deshalb weist sie ein hohes Adsorptionsvermögen auf. Die Wärmeleitfähigkeit beträgt circa 0,042 W/(m·K) bei 0 °C und 0,073 W/(m·K) bei 200 °C.
Bezogen auf das Holzeinsatzmaterial treten bei der Pyrolyse ein Volumenschwund von 35 % bis 45 % und ein Massenschwund von 65 % bis 75 % ein.

Holzkohle lässt sich verhältnismäßig "leicht entzünden" (350 °C bis 400 °C) und brennt "ohne Flamme" weiter, weil die "flammenbildenden Gase" bereits bei der Verkohlung entwichen sind und brennt mit einer höheren Temperatur als Holz. Die Verbrennungstemperatur der Holzkohle beträgt 800 °C.

Pro Kilogramm Holzkohle werden bei der Verbrennung je nach Qualität etwa 28–35 MJ an Energie frei.
Nach einer anderen Quelle beträgt der Heizwert je nach verwendeter Holzsorte umgerechnet 31,6–32,9 MJ/kg.

Der Heizwert erhöht sich mit wachsender Verkohlungstemperatur. Durch Erhöhung der Pyrolyseendtemperatur von 400 °C auf 1200 °C steigt die spezifische Wärme von 1,02 auf 1,60 kJ/(kg·K) an.

Harzfreies, "nicht saftreiches" Holz gibt "glanzlose", höchst poröse Kohle, die aus "harzigem", "saftreichem" Holz erhaltene Holzkohle, enthält im Innern der Zellen, die aus den Saftbestandteilen gebildete "Glanzkohle". Stets ist Holzkohle "leicht zerreibbar", aber nur infolge ihrer Struktur; die "Kohlensubstanz" selbst ist "hart" und ein gutes Poliermittel für Metall. Bei gewöhnlicher Temperatur ist sie höchst beständig und liegt jahrhundertelang im Boden, ohne sich zu verändern; an der Luft adsorbiert sie bestimmte Gasbestandteile und Dämpfe sowie aus Flüssigkeiten suspendierte Stoffe.

Durch die Adsorption von Gasen und speziell Wasserdampf tritt eine Gewichtszunahme der Holzkohle nach dem unter Luftabschluss ausgeführten Erkalten ein.
Die "Gewichtszunahme" frischer Kohle beim Liegen an der Luft beträgt in 24 Stunden bei Eichen- und Birkenkohle 4–5 %, Fichten-, Buchen-, Erlen­kohle 5–8 %, Kiefern-, Weiden-, Pappel­kohle 8–9 %, Tannenkohle 16 %.

Qualitätskriterien für Holzkohle sind Stückigkeit, Wassergehalt, Aschegehalt und Gehalt an "flüchtigen" Bestandteilen. Frische, trocken aus den Verkohlungsanlagen ausgetragene Holzkohle neigt zur Selbstentzündung. Deshalb ist eine Alterung unter kontrollierten Bedingungen erforderlich, wobei das Entstehen von Wärmestauungen in den ersten 48 Stunden nach der Pyrolyse vermieden werden muss. Gute Holzkohle ist glänzend schwarz und hat die Struktur des Holzes, "schlecht verkohlte Kohlen" haben ein braune, braunrote Färbung, diese werde als "Rotkohlen", "Brände", "Füchse" bezeichnet und sind von einer zu niedrigen Verkohlungstemperatur, unter 300 °C. Diese wurde auch absichtlich erzeugt, zur "Schwarzpulverherstellung" oder in der hüttenmännischen Verarbeitung.

Holzkohle wird durch Erhitzen von Holz bei Luftabschluss hergestellt, es entsteht Holzkohle, Holzessig, Holzgas, Holzteer. Dabei werden abhängig von der Temperatur verschiedene Phasen des Pyrolyseprozesses unterschieden.

In der Initialphase kommt es bei Temperaturen von bis zu 220 °C vor allem zu einer "Aufheizung" und "Trocknung" des Materials, wobei vor allem Wasserstoff sowie Spuren von Kohlendioxid, Essigsäure und Ameisensäure entweichen. Holz gibt beim Erhitzen bis 150 °C nur hygroskopisches Wasser ab; dann entwickeln sich saure Dämpfe, steigt die Temperatur über 150 °C, so zersetzt es sich. Bis etwa 280 °C vermehrt sich die Freisetzung dieser Stoffe in einer "pyrolytischen Zersetzungsphase", die ebenso wie die "Initialphase" endotherm ist. Ab 280 °C kommt es dann zu einer starken exothermen Reaktion, bei der etwa 880 kJ/kg Holz als Energie freiwerden und den Prozess auf über 500 °C aufheizen. Von 300 °C ab entstehen immer dichter werdender "gelber" oder "gelbbrauner" Dampf und Gase. Die brennbaren Gase (siehe auch unter Holzgas), vor allem Kohlenmonoxid, Methan, Formaldehyd, Essig- und Ameisensäure sowie Methanol und Wasserstoff, die verbrennen und unter Rauchentwicklung austreten, beim Abkühlen der entweichenden Produkte erhält man Holzteer und Holzessig. Die Struktur des Holzes verändert sich oberhalb von 400 °C von der fibrillären Struktur des Holzes in die kristalline Struktur von Graphit. Beim letzten "endotherm" ablaufenden Prozess werden die Rauchgase beim Durchtritt durch bereits verkohlte Schichten in brennbares Kohlenmonoxid und Wasserstoff aufgespalten, zurück bleibt die Holzkohle als Rückstand.


Die "Dichtigkeit" und quantitative Ausbeute der Kohle wird durch die "Verkohlungsgeschwindigkeit" bestimmt, eine geringere Geschwindigkeit ergibt eine dichtere Kohle sowie eine größere Ausbeute.
Die Ausbeute an Kohle sinkt mit steigenden Temperaturen. Zugleich wird die Kohle beständig reicher an Kohlenstoff und Asche und entsprechend ärmer an Wasserstoff und Sauerstoff.

Rotkohle wie Schwarzkohle lassen noch makroskopisch deutlich die Holzstruktur erkennen, mikroskopisch lässt sich die Holzart erkennen, aus der die Holzkohle erzeugt wurde.

Es werden sowohl Laub- wie Nadelhölzer zu Kohle verarbeitet, erstere hauptsächlich, wenn die flüssigen Destillationsprodukte, Essigsäure und "Holzgeist"→Holzessig, das Haupterzeugnis sind, letztere wenn das Hauptgewicht auf die Gewinnung der Kohle, Teer und Terpentinöl gelegt wird.

Die älteste Methode der Holzkohlengewinnung (Kohlenbrennerei) ist der aus dem Altertum stammende Meilerbetrieb (Köhlerei), bei dem das Holz in annähernd halbkugel- oder kegelförmigen Haufen (Meilern) in großen Scheiten regelmäßig (und zwar stehend oder liegend) um drei in der Mitte errichtete Pfähle (Quandel) aufgesetzt und mit einer Decke bedeckt wird.

Früher verkohlte man das Holz besonders in Süddeutschland, Russland und Schweden in Haufen oder liegenden Werken. Die Verkohlung des geschichteten Holzes erfolgte nur allmählich von einem Ende des länglichen Haufens zum anderen. Die fertig verkohlten Stücke wurden nach und nach "gezogen".

Meilerkohle wird folgendermaßen unterschieden:

Ganz ähnlich wie in Meilern oder Haufen verläuft die Verkohlung in runden oder eckigen gemauerten Pechöfen, die eine leichtere, vollständigere Gewinnung der Nebenprodukte (Teer, Holzessig, Gase, die beim Meilerbetrieb in der Regel verloren gehen) gestatten, aber eine geringere Ausbeute und weniger gute Kohle liefern. Bei diesen Einkammeröfen tritt ebenfalls Luft zu dem zu verkohlenden Holz, und ein Teil desselben erzeugt durch seine Verbrennung die nötige Temperatur. Diese Kohle wird als "Ofenkohle" bezeichnet.

Man hat aber den Verkohlungsprozess besser unter Kontrolle, wenn man das Holz in Gefäßen, die von außen geheizt werden, also ohne Luftzutritt, verkohlt. Dies geschieht in "Zweikammeröfen", Retorten, Röhren oder Zylindern zuweilen mit erhitzter Luft, mit Gichtgasen der Hochöfen, mit überhitztem Wasserdampf oder mit Anwendung von Gebläseluft. Diese Kohle wird als "Retortenholzkohle" bezeichnet, bei 500 °C bildet sich eine "harte" Holzkohle.

Eine solche sorgfältige Verkohlung ist besonders zur Gewinnung von Kohle für die Schwarzpulver<nowiki>fabrikation</nowiki> erforderlich. Dazu benutzt man große eiserne Zylinder, die außerhalb des Ofens gefüllt, mit einem Deckel verschlossen und in den Ofen geschoben werden. Ein großer beweglicher Deckel schließt den Raum, in dem sich der Zylinder befindet. Die aus dem Holze sich entwickelnden Gase leitet man in die Feuerung. Die Temperatur 300–400 °C, wird mittels eines Pyrometers bestimmt. "Rotkohle" für Jagdpulver wird mit überhitztem Wasserdampf hergestellt. Faulbaum, Pappel oder Erlenholzkohle eignet sich hier am besten.

Dem tatsächlichen Volumen nach beträgt die durchschnittliche Kohlenausbeute 47,6 %.

Vergleicht man das scheinbare Volumen (ohne Abzug der Zwischenräume) des Holzes mit dem der Kohle, so liefern die Holzarten folgende Volumenprozentanteile Kohle:

Man erhält bei verschiedenen Holzarten folgende Gewichtsprozente der Kohle (bei 150 °C getrocknet und bei 300 °C verkohlt):

Die Holzkohle war und blieb unentbehrlich als Adsorbens und Filter, "Klärmittel" in vielen Bereichen.
Holzkohle, deren fein gegliederte Oberfläche viele unerwünschte organische Substanzen bindet, kann auch als Aktivkohle zum Filtern und Reinigen diverser Stoffe eingesetzt werden. Zum Beispiel bei der Herstellung von Wodka, als Kohletablette bei Durchfall oder für Filter von Gasmasken.
Im Allgemeinen adsorbiert bei niedriger Temperatur hergestellte Kohle am stärksten. Kohle adsorbiert Sauerstoff und wird dabei oxidiert. Dieser reagiert beispielsweise mit Schwefelwasserstoff zu Schwefelsäure und Wasser, mit Ammoniak zu Ammoniumnitrat und mit Ammoniumhydrogensulfid zu Ammoniumsulfat.

Auch Fäulnis­produkte werden energisch zerstört. Mit Kohle umgebenes Fleisch zersetzt sich erst nach längerer Zeit, und zwar ohne Fäulniserscheinungen. Kohle adsorbiert auch Gerüche. Übelriechendes, fauliges Wasser kann durch frisch ausgeglühte Holzkohle gereinigt und Weingeist von Fuselölen befreit werden.

Aber die Kohle wirkt nicht auf die im Wasser enthaltenen mikroskopischen Organismen (Bakterien usw.), und beim Filtrieren des Wassers durch Kohle gehen dieselben durch den Filter; das Wasser wird also geruchlos, aber nicht von den Krankheiten übertragenden Organismen befreit. Kohle kann einige große, unpolare, organische Wasserinhaltsstoffe zurückhalten, z. B. chlorierte Kohlenwasserstoffe, Pflanzenbehandlungsmittel oder Medikamente. Aber Schwermetallionen (z. B. von Blei), Nitrat und Calcium/Magnesium (Kalk) lassen sich durch dieses Verfahren nicht herausfiltern.

Holzkohle adsorbiert auch

Bei längerem Liegen an der Luft verliert Holzkohle ihr Adsorptionsvermögen, erlangt es aber wieder durch "Ausglühen"; auch können der Kohle die aus Flüssigkeiten aufgenommenen Substanzen wieder entzogen werden (Wiederbelebung), so dass sie nach Ausglühen erneut benutzbar ist.

Man benutzt Holzkohle zur Erzeugung intensiver Hitze besonders überall da, wo Rauch- und Flammenbildung vermieden werden muss, beispielsweise im Schmiedefeuer, beim "Glühendmachen" von Plättstählen, bei chemischen Vorgängen usw. Heute wird Holzkohle vor allem für Freizeitzwecke, z. B. zum Grillen DIN 1860 (51749), technisch hergestellt. Holzkohle war vor Entwicklung der Verkokung von Steinkohle zu Koks der notwendige Brennstoff zur Metallverhüttung sowie für die Esse des Schmiedes. Da sie Metalloxide reduziert, sowie wenig Asche und praktisch keinen Schwefel enthält, eignet sie sich hervorragend zur Gewinnung von Metallen aus den Erzen, ist aber viel zu teuer und verbrennt zu schnell. Silber- und Kupfersalze werden durch Kohle reduziert.→Löten. Holzkohle kann auch zur Stahlhärtung verwendet werden, dazu wird das Werkstück mehrere Stunden in glühende Holzkohle gelegt und danach mit Wasser abgeschreckt. Der Kohlenstoff dringt dann in die äußeren Schichten des Stahls ein. Weiter kann aus Holzkohle Schwefelkohlenstoff, Natriumcyanid→Castner-Kellner-Verfahren, Ferrosilicium gewonnen werden.

Weitere Verwendungszwecke:

Als Bodenverbesserer macht Holzkohle den Boden locker und wirkt außerdem durch ihre Adsorptionsfähigkeit für Ammoniak und Kohlensäure. Zierpflanzen mit faulenden Wurzeln können geheilt werden, wenn sie in mit Kohle gemischte Erde gebracht werden. Große Wunden an Saftgewächsen heilen leicht, wenn man sie mit Kohlenpulver bestreut, auch kann man solche Gewächse, Knollen und Samen für einen längeren Transport gut in Kohle verpacken. Seit kurzem wird Holzkohle auch als wiederentdeckter Bodenhilfszusatzstoff im Gartenbau und in der Landwirtschaft diskutiert unter dem Namen Pflanzenkohle ("auch Biokohle oder biochar") oder Terra preta.

Der Beruf des Köhlers, eines Arbeiters, der in Meilern Holzkohle herstellte, ist in Europa praktisch ausgestorben. Vereinzelte Köhlereien gibt es unter anderem noch im Harz und im Frankenwald. Dabei ist der Rückgang vor allem auf gestiegene Umweltstandards (im Zuge der EU-Osterweiterung auch in Osteuropa) sowie den Mangel an billig verfügbarem Holz als Rohstoff zurückzuführen.

Der größte Teil der weltweit hergestellten Holzkohle stammt aus den so genannten Schwellen- und Entwicklungsländern. Die Gesamtmenge wird auf mindestens 43 Millionen Tonnen geschätzt, aufgrund der Datenunsicherheit wird diese Menge allerdings als Mindestmenge angenommen. Die in europäischen Ländern verwendete Holzkohle wird überwiegend importiert, wobei für Deutschland eine Importquote von etwa 98 % angenommen wird. Im Zeitraum von Januar bis April 2009 importierte Deutschland 76.700 Tonnen Grillkohle. Davon stammten 32,4 % aus Paraguay, 17,5 % aus Argentinien und 10,7 % aus Polen.





</doc>
<doc id="8903" url="https://de.wikipedia.org/wiki?curid=8903" title="Ladung">
Ladung

Ladung (ahd. "ladan" ‚hinbreiten, aufschichten‘) steht für:

Ladung, Ortsnamen:
Siehe auch:


</doc>
<doc id="8907" url="https://de.wikipedia.org/wiki?curid=8907" title="Pseudonym">
Pseudonym

Ein Pseudonym (in bestimmten Zusammenhängen auch: Alias) ist der fingierte Name einer Person, insbesondere eines Urhebers (oder mehrerer Urheber) von Werken. Das Pseudonym wird anstelle des bürgerlichen Namens (Realname, Orthonym) verwendet und dient meist zur Verschleierung der Identität. Das zugehörige Adjektiv lautet pseudonym (von altgriechisch "pseudōnymos" „fälschlich so genannt“). Sollen Realname und Pseudonym gegenübergestellt werden, so werden sie häufig mit dem Adverb „alias“ verbunden („"X" alias "Y"“).

Von Künstlernamen (auch Bühnenname) ist eher die Rede, wenn Werke künstlerisch dargeboten werden (darstellende Kunst), etwa bei Schauspielern, Musikern oder Artisten. Tarnnamen sind gebräuchlich, wenn die Identität einer Person in einem bestimmten Zusammenhang verhüllt werden soll. Im Bereich der Spionage ist "Deckname" der übliche Begriff (der Realname wird hier "Klarname" genannt). Der Kampfname, auch "Nom de guerre" genannt, ist hingegen kein eigentliches Pseudonym, da die wahre Identität des Trägers in der Regel bekannt ist. Er dient in der Regel nicht der Verschleierung der Identität.

Pseudonyme werden heute besonders im Internet und in der digitalen Kommunikation verwendet. Allerdings geht es im Internet in der Regel nicht um die Veröffentlichung bedeutender Werke. Der Zweck, die wahre Identität zu verbergen, steht hier meist im Vordergrund.

Neben den von einer Person selbst gewählten Pseudonymen gibt es auch als Sonderfall Pseudonyme, die von anderen für eine bestimmte Person gebraucht werden. Meist geschieht dies in den Medien um z. B. einen Informanten zu schützen oder um die Privatsphäre eines Menschen zu wahren.

Pseudonyme wurden historisch in erster Linie von Schriftstellern verwendet. In diesem Zusammenhang gibt es den französischen Ausdruck nom de plume (wörtlich: „Schreibfeder-Name“), der heute nur noch selten verwendet wird.

Einige Personen arbeiteten zeitlebens unter einem Pseudonym (zum Beispiel Friedrich Freiherr von Hardenberg → Novalis, Marguerite de Crayencour → Marguerite Yourcenar). Bei manchen dieser Pseudonyme ist die tatsächliche Identität unbekannt oder umstritten (zum Beispiel der Schriftsteller → B. Traven oder der Maler → J. Metzler)

Andere ändern nur für bestimmte Lebens- oder Schaffensperioden oder für besondere Werke ihren Namen (zum Beispiel Jasmin Wagner → Blümchen, Agatha Christie → Mary Westmacott, Stephen King → Richard Bachman, Paul Dessau → Henry Herblay). Manche Personen haben auch verschiedene Künstlernamen, um verschiedene persönliche Werke zu differenzieren, da sie etwas anderes ausdrücken sollen (zum Beispiel Jean Giraud → Mœbius oder Gir, Patrick Losensky → Fler oder Frank White).

Bei der Nutzung von Pseudonymen im Internet und im E-Mail-Verkehr gibt es erhebliche Unterschiede zu den Pseudonymen von Schriftstellern und anderen Künstlern: In der virtuellen Welt kann sich jedermann unter beliebigen und beliebig vielen Pseudonymen äußern. Benutzernamen und selbstgewählte Spitznamen können ohne Weiteres geändert werden und sind insofern mit den realen Personen nur lose verknüpft.

Im Prinzip kann jeder Pseudonyme verwenden – im Privatleben und teilweise auch beruflich. In manchen Berufen sind Pseudonyme üblich (zum Beispiel bei Geheimdiensten oder in der Prostitution).

Es gibt viele Motive, ein Pseudonym zu verwenden.

Vereinfachung oder Verschönerung des Namens

Wirtschaftliche Vorteile

Imagepflege

Vermeidung von Nachteilen

Biographische Hintergründe

Schutz von Personen in der Berichterstattung der Medien

Sonstiges

Bei der Verwendung von Pseudonymen im Internet spielen mehrere der oben genannten Motive eine Rolle. Dazu gehören die Vermeidung von Nachteilen, Imagepflege und der Spaß am Verwirrspiel.

Viele Pseudonyme sind reine Phantasiegebilde. Manchmal wird aus den Buchstaben des richtigen Namens ein Anagramm gebildet (zum Beispiel François Rabelais → Alcofrybas Nasier bzw. Alcofribas Nasier, Paul Ancel → Paul Celan) oder ein Ananym, das den wirklichen Namen rückwärts gelesen wiedergibt (zum Beispiel Kurt W. Marek → C. W. Ceram). Bei einem Kryptonym ist ein Zusammenhang mit dem wirklichen Namen kaum noch oder gar nicht mehr erkennbar (zum Beispiel starke Verkürzung bei Horst Bosetzky → -ky).
Von einem Prenonym spricht man, wenn der Vorname oder mehrere eigene Vornamen als Name verwendet werden (Jean Paul Friedrich Richter → Jean Paul, Peter Alexander Neumeyer → Peter Alexander). Auch ein einzelner Vorname kann verwendet werden, indem man ihn in einen vermeintlichen Vor- und Familiennamen teilt (Illobrand von Ludwiger → Illo Brand).
Gelegentlich verbergen sich Frauen hinter Männernamen mit einem Pseudandronym (zum Beispiel Karen Blixen → Isak Dinesen) oder Männer hinter Frauennamen mit einem Pseudogynym (zum Beispiel Prosper Mérimée → Clara Gazul).
Ein Traduktionym entsteht durch die Übersetzung des wirklichen Namens in eine andere Sprache. Beispielsweise erscheinen die Schallplatten des niederländischen Musikers Ton Koopman (niederländisch "koopman" bedeutet „Kaufmann“) unter dem Label "Antoine Marchand" – die französische Version von „Anton Kaufmann“. (Bei den Gelehrten der Renaissance war es üblich, latinisierte oder gräzisierte Namen zu verwenden, zum Beispiel Georg Bauer → Georgius Agricola, Gerhard Kremer → Gerhard Mercator, Philipp Schwarzerdt → Philipp Melanchthon.)
Ein Geonym ist aus einem geografischen Namen abgeleitet. Beispiele sind der Zeichner e.o.plauen (lebte als Kind in Plauen), der Maler Georg Baselitz (geboren in Deutschbaselitz), Jürgen von der Lippe (geboren in Bad Salzuflen im Kreis Lippe) und der österreichische Sänger Hubert von Goisern (geboren in Goisern). Theo Lingen wählte seinen Künstlernamen nach dem Geburtsort seines Vaters. Der Renaissance-Gelehrte Johann Georg Turmair nannte sich Johannes Aventinus, eine latinisierte Anspielung auf seinen Geburtsort Abensberg.
Seltener wird das Aristonym ausgesucht, das seinen Träger mit einem Adelstitel aufwerten soll. Viele vermeintliche Aristonyme sind allerdings in Wahrheit lediglich Geonyme und wollen keinen Adelstitel vortäuschen (Hoffmann von Fallersleben). Die Abgrenzung ist mitunter schwierig. Adelig klingende Pseudonyme werden häufig auch in ironischer Absicht gewählt (Jürgen von der Lippe, Rosa von Praunheim, beides eigentlich Geonyme; Hella von Sinnen). Ein dem Aristonym ähnlicher Fall sind scheinbare bzw. unechte akademische Titel (zum Beispiel Dr. Seuss oder Dr. Kurt Ostbahn). Ebenso selten ist das Hagionym, das den Namen eines Heiligen enthält (zum Beispiel Halldor "Kiljan" Laxness).

Soll das Pseudonym auf eine satirische oder ironische Absicht des Autors hindeuten, spricht man von einem "Ironym" (Friedrich Theodor Vischer → Deutobold Symbolizetti Allegorowitsch Mystifizinski; Hella von Sinnen).
Darüber hinaus gibt es das Allonym, welches den Namen einer bekannten Persönlichkeit vorgibt (zum Beispiel bei Pablo Neruda, der sich nach Jan Neruda benannte) und das Phraseonym, das den Namen in Form einer Redewendung wiedergibt (zum Beispiel Farin Urlaub).
Eine im Bereich der Unterhaltungsindustrie häufig anzutreffende Sonderform ist das Sammelpseudonym oder Verlagspseudonym: Der fingierte Name wird in diesem Falle nicht einer bestimmten real existierenden Person zugeordnet, sondern von einem Unternehmen zur einheitlichen Publikation von Werken genutzt, die in Wahrheit von verschiedenen Urhebern stammen. Gängige Praxis ist dieses Vorgehen bei Verlagen, die Trivialliteratur in Heftform herausgeben. Eines der bekanntesten Beispiele ist „Dr. Sommer“ aus der Jugendzeitschrift BRAVO: Ursprünglich ein Pseudonym des Autors Martin Goldstein, beantwortete schon bald ein Team von Mitarbeitern unter diesem Pseudonym die Anfragen der jungen Leser. Inzwischen wird offen vom „Dr.-Sommer-Team“ gesprochen.
Auch Tonträgerkonzerne bedienten sich schon früh eines ähnlichen Konzepts. Ein bekanntes Beispiel ist die Interpretenangabe "Orchester Eric Harden", die seit den späten 1920er Jahren unter den verschiedensten Warenzeichen des damals größten europäischen Plattenproduzenten, der Carl Lindström AG, verwendet wurde. "Eric Harden" war keine existierende Person, sondern konnte für jedes beliebige Studioorchester stehen, das gerade für die benötigten Aufnahmen zur Verfügung stand. Ein anderes jahrelang bedientes Pseudonym der Carl Lindström AG war "Fred Lustig". Unter diesem Namen verkaufte der Konzern verschiedene Schlagersänger, so den damals sehr bekannten Luigi Bernauer auf seiner Marke Odeon.

Ein jüngeres Verwendungsbeispiel ist Bert Brac, ein Sammelpseudonym für Begleitmusik-Komponisten des Hörspiellabels Europa. Ein Beispiel mit enormem Einfluss aus der Wissenschaft ist Nicolas Bourbaki. Ein aus der Filmbranche bekanntes Beispiel entstand nicht aus Erwägungen des Produzenten, sondern aus Selbstschutzwunsch: Regisseure, die aus verschiedenen Gründen mit einem ihrer Filme unzufrieden sind (beispielsweise wegen starker Eingriffe der Produzenten), wählen üblicherweise das Notfall-Pseudonym Alan Smithee.
Pseudonyme, die von mehreren Autoren gemeinschaftlich genutzt werden, werden auch als Gemeinschaftspseudonym oder Kollektivpseudonym bezeichnet. Ein bekanntes Beispiel ist „H. Bustos Domecq“, das von Jorge Luis Borges und Adolfo Bioy Casares für Kollaborationen verwendete Pseudonym. Die Grenzen zwischen Sammel- und Gemeinschaftspseudonym sind fließend, man wird aber bei gemeinschaftlich erstellten Einzelwerken eher von Gemeinschaftspseudonym, bei einer Reihe, die unter einheitlichem Pseudonym erscheint, deren einzelne Bände aber individuelle Verfasser haben, eher von Sammelpseudonym sprechen.

Als Synonyme werden bedeutungsgleiche Wörter bezeichnet. Dieser Begriff kann also in gewisser Weise als Oberbegriff des Sonderfalls Pseudonym aufgefasst werden. Dagegen spricht, dass Synonyme im üblichen Sinne dem natürlichen Wortschatz einer Sprache entstammen, während es sich bei Pseudonymen gewöhnlich um Eigennamen handelt, welche absichtsvoll neu gebildet werden, um einen bestimmten Zweck zu erfüllen.

Künstlernamen sind heute namensrechtlich geschützt. Nach dem Urheberrecht hat ein Künstler das Recht, festzulegen, unter welchem Künstlernamen er genannt werden will. Für die Wahl des Namens gibt es gewisse Einschränkungen durch Persönlichkeitsrechte anderer. Im europäischen Reisepass und in die Ausweisdokumente vieler Länder können Künstlernamen (und auch Ordensnamen) eingetragen werden.

Künstlernamen können in den Personalausweis und Reisepass eingetragen werden. Möglich war dies schon seit vielen Jahrzehnten – bis zu einer Änderung des Personalausweisgesetzes zum 1. November 2007, mit der die Eintragungsfähigkeit von Künstlernamen abgeschafft wurde. Auf diese Änderung im Jahr 2007 gab es Proteste von Künstlern und Journalisten, die für ihre berufliche Tätigkeit auf Pseudonyme angewiesen sind. Diese Proteste waren im Jahr 2008 Anlass für die Bundesregierung, die Frage der Eintragungsfähigkeit von Künstlernamen erneut zu prüfen. Der Bundesrat beschloss am 18. Dezember 2008 das Gesetz über Personalausweise und den elektronischen Identitätsnachweis, in dem als Nebenregelung auch Künstlernamen wieder eintragungsfähig gemacht wurden. Das Gesetz trat gemäß Art. 7 des Gesetzes erst am 1. November 2010 in Kraft, dies fiel zusammen mit der Einführung des neuen Personalausweises. Bis dahin blieb es den Meldebehörden versagt, Künstlernamen in den Personalausweis einzutragen. Von dieser Regelung waren auch Ordensnamen betroffen. Seit 1. November 2010 ist die Eintragung in Personalausweise und Pässe wieder möglich.

Rechtsverbindlich und zulässig ist die Unterschrift mit einem Pseudonym, sofern die als Aussteller in Betracht kommende Person ohne Zweifel feststeht. Wird mit dem Künstlernamen unterschrieben, so ist damit der gesetzlichen Schriftform Genüge getan.

Der Schutz des Pseudonyms gemäß BGB bleibt davon unberührt. Bei Klagen kann der Künstlername zur Parteienbezeichnung verwandt werden. Bei Grundstückskäufen sind Eintragungen im Grundbuch unter ausschließlicher Verwendung des Künstlernamens nach Abs. 1 a GBV nicht zulässig. Dieser darf jedoch zusätzlich zum Familiennamen eingetragen werden.

Für Künstlernamen sind seit dem 1. November 2012 alle Zeichen zulässig, die in dem Zeichensatz "String.Latin" der Bundesdruckerei enthalten sind.

Nach einer Entscheidung des Verwaltungsgerichts Berlin vom Januar 2015 können Prostituierte ihr Pseudonym nicht als Künstlernamen im Personalausweis eintragen lassen.

Es steht in Österreich jeder Person frei, sich eines beliebigen Namens zu bedienen, sofern sie damit nicht gegen den Namensschutz gemäß § 43 ABGB verstößt. Ein Pseudonym bzw. Künstlername wird durch bloßen Gebrauch erworben, ohne dass es hierzu eines größeren Umfangs des Gebrauches oder einer längeren Dauer bedürfte; erforderlich ist nur, dass der Deckname einem weiteren Kreis bekannt geworden und aufgefallen ist, so dass sich mit dem Gebrauch auch die Vorstellung der Öffentlichkeit von einer bestimmten Persönlichkeit verbindet. Seit der Einführung der neuen Sicherheitspässe im Juni 2006 ist die Eintragung von Künstlernamen im Reisepass nicht mehr zulässig.

Künstlernamen und Ordensnamen können in der Schweiz als „amtliche Ergänzungen“ in den Pass aufgenommen werden. Für den Eintrag eines Künstlernamens „muss ein begründetes Gesuch gestellt und glaubhaft dargelegt werden, dass dieser Name im wirtschaftlichen und gesellschaftlichen Leben einer Person auch objektiv von Bedeutung ist“.

Eine andere Form von alternativen Namen sind die Spitznamen und Benutzernamen, oft auch Nickname oder kurz „Nick“ genannt – im Zuge der Verbreitung von Computern und des Internets inzwischen für viele unerlässlich. So bedarf es ihrer zum Beispiel bei der Rechtevergabe bei Betriebssystemen oder für die Nutzung von Internetnutzungsmöglichkeiten wie E-Mail oder Foren. Hierbei ist eine Pseudonymisierung nicht zwingend, aber in einigen Fällen durchaus empfehlenswert (siehe Anonymisierung und Pseudonymisierung).

Um den Schutz personenbezogener Daten zu gewährleisten, legt in Deutschland das Telemediengesetz fest: „Der Diensteanbieter hat die Nutzung von Telemedien und ihre Bezahlung anonym oder unter Pseudonym zu ermöglichen, soweit dies technisch möglich und zumutbar ist. Der Nutzer ist über diese Möglichkeit zu informieren.“

Problematisch kann die Anonymisierung werden, wenn eine Person mehrere Identitäten als Sockenpuppen verwendet, um in Diskussionen oder Abstimmungen eine Mehrheit zu erzielen.

Fahrradboten fahren unter einem Nick, der ursprünglich auch der schnellen und guten Verständlichkeit im Funkverkehr dient.

CB-Funker haben stets einen Nick (hier: Skip), Trucker plakatierten diesen Rufnamen auch per Schild hinter der Windschutzscheibe.

Überwiegend Kurzzeit-Teams besonders bei studentischen Sportveranstaltungen wählen einen Team-Namen.

Autofahrer der Pionierzeiten fuhren Rennen unter Pseudonym – häufig betuchte Herren, die sich das teure Hobby leisten konnten, und bei Misserfolg keinen schlechten Ruf davontragen wollten. So zwei Teilnehmer der Automobil-Fernfahrt Salzburg–Wien am 1. + 2. Juni 1900, nämlich Baron Anton Codelli unter dem Pseudonym "Karl Findeisen" und der Nesselsdorfer Direktor und Eisenbahnpionier Hugo Fischer von Röslerstamm unter "Vorwärts".




</doc>
<doc id="8920" url="https://de.wikipedia.org/wiki?curid=8920" title="Connecticut">
Connecticut

Connecticut ( []) ist ein Bundesstaat im Nordosten der Vereinigten Staaten und Teil der Region Neuengland. Connecticut war eine der dreizehn Kolonien, die während des Amerikanischen Unabhängigkeitskrieges gegen das Königreich Großbritannien rebellierten. Der Name entstand aus der französischen Verballhornung des moheganischen Wortes ", das so viel wie ‚langer Fluss (der Gezeiten)‘ bedeutet.

Der Bundesstaat trägt den offiziellen Beinamen ' (Verfassungsstaat), der 1959 durch Gesetzesakt angenommen wurde, und wird darüber hinaus auch ' (Proviantstaat) genannt. Diese Bezeichnung rührt daher, dass während des Amerikanischen Unabhängigkeitskrieges die Kontinentalarmee von Connecticut aus mit Proviant versorgt wurde; der Bundesstaat erlebte kaum Kämpfe im eigenen Land.

Connecticut, der drittkleinste Bundesstaat der USA, liegt im Nordosten der USA am Atlantischen Ozean. Der Staat erstreckt sich über 177 km zwischen 40°58′N und 42°3′N sowie über 113 km zwischen 71°47′W und 73°44′W. Connecticut grenzt im Norden an Massachusetts, im Osten an Rhode Island, im Süden an den Long Island Sound und im Westen an New York. Durchflossen wird der Staat vom größten Fluss Neuenglands: dem Connecticut River. 1.809 km² (12,6 Prozent) der Fläche des Landes ist von Wasser bedeckt und 56 bis 63 Prozent der Landfläche ist bewaldet, sonst kennzeichnen Hügel, Seen und Ebenen (einschließlich der 16 km breiten Küstenebene) das Land. Ein Teil der Appalachen bildet die höchsten Erhebungen (Mount Frissell 725 m) des Staates.

Connecticut ist in acht Countys unterteilt: Fairfield, Hartford, Litchfield, Middlesex, New Haven, New London, Tolland und Windham.

Die Flagge von Connecticut ist blau, es erinnert an die Uniformfarbe der Miliz des Staates. In der Mitte befindet sich auf einem Wappen das Siegel Connecticuts. Auf ihm sind drei Weinreben mit jeweils drei roten Weintrauben-Rispen zu sehen. Man vermutet auch, dass die drei Rispen die drei Kolonien New Haven, Saybrook und Hartford repräsentieren sollen. Unten auf dem Banner steht " was so viel bedeutet wie ‚Wer hinüberbrachte, besteht‘. Einige Deutungen dieses Mottos beziehen es auf die Bibel : und übersetzen freier: „Wer verpflanzte, wird überdauern“. Das Siegel war früher das Siegel der Saybrook Colony und wurde an die Connecticut Colony weitergegeben, als Colonel George Fenwick es aus England 1639 mitbrachte.

Der Niederländer Adriaen Block kam 1614 als erster Europäer nach Connecticut. Zu dieser Zeit lebten 16 Algonkin-Indianerstämme mit 6.000–7.000 Menschen dort.

1633 kamen aus Massachusetts die ersten Engländer ins Land. Zwei Jahre später wurden die Siedlungen Windsor und Wethersfield gegründet. Thomas Hooker gründete 1636 Hartford. Die drei Siedlungen schlossen sich 1639 zur Kolonie Connecticut zusammen. Im selben Jahr wurde die Kolonie New Haven gegründet. 1662 erhielt Connecticut eine von König Karl II. genehmigte Autonomie sowie eine eigene Verfassung. 1665 kam es zum Zusammenschluss von Connecticut und New Haven.

Die Planung eines Zusammenschlusses mit dem Dominion of New England erfolgte 1685. Zwei Jahre später wollte König Jakob II. die Verfassung von 1662 rückgängig machen. Nach dem Sturz von König Jakob II. 1688 konnte Connecticut im Juli 1689 die Autonomie zurückgewinnen.

Im Juli 1776 erklärte Connecticut als eine der Dreizehn Kolonien die Unabhängigkeit von Großbritannien und wurde am 9. Januar 1788 der fünfte Bundesstaat der Vereinigten Staaten. Die Sklaverei wurde 1848 abgeschafft.

Connecticut hat 3.574.097 Einwohner (Stand: U.S. Census 2010), davon sind 71,2 Prozent Weiße (ohne Hispanics und Latinos), 13,4 Prozent Hispanics oder Latinos, 10,1 Prozent Schwarze oder Afroamerikaner, 3,8 Prozent Asiatische Amerikaner und 0,3 Prozent Indianer. 322.941 Einwohner (9,0 Prozent der Gesamtbevölkerung) gaben bei der American Community Survey im Jahr 2014 an, deutsche Vorfahren zu haben.

Die Bevölkerung von Connecticut gilt als die ursprünglichste Verkörperung des Yankee.

Die mitgliederstärksten Religionsgemeinschaften waren im Jahre 2000 die Römisch-katholische Kirche mit 1.372.562, die United Church of Christ mit 124.770 und die anglikanische Episcopal Church mit 73.550 Anhängern. Jüdischen Gemeinden gehörten 108.280 Mitglieder an.

Connecticut ist seit den 1990er-Jahren bei Präsidentschaftswahlen stets näher an die Demokratische Partei herangerückt. Nach der republikanischen Dominanz während der 1970er- und 1980er-Jahre gewannen die Demokraten Connecticut und seine acht (seit 2004 sieben) Wahlmänner seit der Wahl von 1992 stets deutlich für sich. Mittlerweile ist Connecticut eine der stärksten demokratischen Hochburgen in den USA. Der erste Rückschlag, den die Demokraten in Connecticut seitdem hinnehmen mussten, war der Sieg des ehemaligen Demokraten und mittlerweile unabhängigen Kandidaten Joe Lieberman bei der Senatswahl 2006. Nach Liebermans Verzicht auf eine erneute Kandidatur im Jahr 2012 fiel der Sitz mit Chris Murphy wieder an einen Demokraten. Zweiter Senator des Staates ist seit 2011 der Demokrat Richard Blumenthal. Alle fünf Sitze Connecticuts im US-Repräsentantenhaus werden seit dem Jahr 2009 von Demokraten eingenommen.

Seit 2011 ist der Demokrat Dan Malloy als Nachfolger der Republikanerin Jodi Rell Gouverneur Connecticuts.

Am 25. April 2012 wurde die Todesstrafe in Connecticut abgeschafft und durch lebenslang ohne die Möglichkeit einer Entlassung als neue Höchststrafe ersetzt.





Das reale Bruttoinlandsprodukt pro Kopf (engl. per capita real GDP) lag im Jahre 2016 bei USD 73.643 (nationaler Durchschnitt der 50 US-Bundesstaaten: 57.118; nationaler Rangplatz: 4). Connecticut ist einer der wohlhabendsten Bundesstaaten mit einem der höchsten Lebens- und Bildungsstandards.
Angebaut werden in Connecticut vorwiegend Getreide, Kartoffeln, Tabak, Mais und Hafer. Im Staat wird gewöhnliche Viehzucht betrieben.
Connecticut ist ein sehr industrieller Staat, unter anderem werden dort Flugzeugmotoren, Maschinen, Metallwaren, Textilien, Atom-U-Boote und verschiedene chemische Erzeugnisse hergestellt. Angeblich wurde dort auch die Frisbee-Scheibe erfunden.

Die Arbeitslosenrate lag im November 2017 bei 4,6 % (Landesdurchschnitt: 4,1 %).

Zu den größten staatlichen Hochschulen gehören die Central Connecticut State University, die University of Connecticut und die Southern Connecticut State University.
Die bekanntesten privaten Hochschulen in Connecticut sind die Yale University, die Wesleyan University, die University of Hartford, die Fairfield University, und die Connecticut College. Weitere Hochschulen sind in der Liste der Universitäten in Connecticut verzeichnet.





</doc>
<doc id="8921" url="https://de.wikipedia.org/wiki?curid=8921" title="Meile">
Meile

Die Meile ist eine Längenmaßeinheit außerhalb des Internationalen Einheitensystems.

Die heute gebräuchlichsten Einheiten sind die "Seemeile" (1852,0 Meter) und die englische "statute mile" (1609,3 Meter). In Deutschland galten bis ins spätere 19. Jahrhundert die "deutsche Meile" oder "Landmeile" (7532,5 Meter) und die "geographische Meile" (7420,44 Meter, definiert als der fünfzehnte Teil eines Äquatorialgrades). In der vormetrisch-metrischen Übergangszeit wurde auch mit einer deutschen Meile zu 7500,0 Meter gerechnet.

Vor der Umstellung auf das metrische System (in den meisten Ländern um 1840–1870) gab es allein in Europa etwa 60 Definitionen mit Längen zwischen 1,5 und 11 Kilometern.

Im Römischen Reich entsprachen fünf Fußlängen (lat. "pes") der Länge eines Doppelschrittes (lat. "passus"), also eines vollen Schrittzyklus aus zwei aufeinanderfolgenden Schritten. Tausend Doppelschritte wurden als "mille passus" „tausend Schritte“ (Singular, eine Meile) oder "milia passuum" „tausende von Schritten“ (Plural, mehrere Meilen) bezeichnet; die römische Meile maß demnach rund 1,5 km. Von der verkürzten Form "milia" entstand im Deutschen das Wort „die Meile“ (aus Neutrum Plural wurde Femininum Singular), in anderen Sprachen "mil, miil, mijl, mila, milja" oder "mile." Der Meilenstein (lat. "lapis milliarium" oder verkürzt "miliarium") gab fortan Entfernungen an.

Die häufigste alte englische Meile hieß "London mile," die neue "British mile" oder "statute mile." Sie war die Grundlage des Pferderennsports, und auf diese Distanz wurde 1780 das Derby in Epsom ausgetragen. Um 1784 wurde die Distanz auf das Anderthalbfache verlängert, und diese wird bis heute gelaufen.

Im deutschen Sprachraum waren die "Landmeile" und die "geographische Meile" üblich. Anders als die englische Meile, die mit ihren 1,6 km in etwa die römische Meile tradiert, entsprachen die deutsche Landmeile und die geographische Meile rund 7,5 km.

Die ehemals in Europa und Lateinamerika verwendete Längeneinheit Leuge wird gelegentlich als „Meile“ übersetzt, beispielsweise im Titel zu Jules Vernes Roman "20.000 Meilen unter dem Meer."

Viele Meilen und verwandte Einheiten (Leuge, League, Legua) waren festgelegt als einfache Vielfache einer Bogenminute des Äquators oder eines Meridians. So liegen die historisch vielfältigen Seemeilen alle im Bereich der Länge einer Bogenminute. Auch die deutsche "geographische Meile" wurde als der fünfzehnte Teil eines Äquatorialgrades definiert.

1929 einigte man sich international auf die heute in der Schifffahrt und in der Luftfahrt für Entfernungen gebräuchliche Seemeile, englisch "nautical mile," abgekürzt NM.

1959 wurden die in den englischsprachigen Ländern bestehenden Längenmaßunterschiede durch internationale Übereinkunft aufgehoben, seitdem wird in angloamerikanischen Ländern die englische Meile, Landmeile oder "statute mile" verwendet, beispielsweise in der Einheit Meilen pro Stunde (mph = "miles per hour").

Ähnliche Einheiten:




</doc>
<doc id="8928" url="https://de.wikipedia.org/wiki?curid=8928" title="Quod erat demonstrandum">
Quod erat demonstrandum

Die Wendung (lat. für „was zu beweisen war“) bindet das Ergebnis einer logischen oder mathematischen Beweisführung an den vorangestellten Zweck zurück und schließt damit die Beweisführung ab. Sie wird häufig abgekürzt als q. e. d. Besonders im Englischen ist die Großschreibung "Q. E. D." oder "QED" in Anlehnung an die lateinische "Capitalis monumentalis" üblich.

Die Wendung stammt aus Euklids Lehrbuch "Elemente" (3. Buch, 4. Kapitel, Theorema XIII) aus dem 3. Jahrhundert v. Chr. und lautet im altgriechischen Original "hóper édei deîxai". Die heute verbreitete lateinische Übersetzung prägte der italienische Humanist Bartolomeo Zamberti, als er Euklids "Elemente" übersetzte und 1505 in Venedig drucken ließ.

Bei einer Behauptung, die erst noch bewiesen werden muss, lauten die Worte in traditioneller Gelehrtensprache "quod esset demonstrandum" oder auch „was zu beweisen wäre“.

Zum Abschluss einer mathematischen Funktion wird die lateinische Wendung "quod erat faciendum" verwendet.

Heutzutage wird "q. e. d." auch oft durch ein schwarzes Quadrat ■ symbolisiert. Es wird "Grabstein", "Kiste" oder nach dem Erstverwender "Halmos" genannt. Manchmal wird der Grabstein auch offen dargestellt: □

Einige Autoren verwenden ■ nur bei kurzen Beweisen, bei langen und komplizierten Beweisen jedoch eher "q. e. d."

"Unicode" schlägt das "End-of-Proof"-Zeichen codice_1 (∎) vor.


</doc>
<doc id="8931" url="https://de.wikipedia.org/wiki?curid=8931" title="Lateinisch (Begriffsklärung)">
Lateinisch (Begriffsklärung)

Lateinisch steht für:

Latein steht auch für:
Siehe auch:


</doc>
<doc id="8935" url="https://de.wikipedia.org/wiki?curid=8935" title="TCP">
TCP

TCP steht als Abkürzung für:

Chemische Substanzen:


</doc>
<doc id="8936" url="https://de.wikipedia.org/wiki?curid=8936" title="Internet Protocol">
Internet Protocol

Das Internet Protocol (IP) ist ein in Computernetzen weit verbreitetes Netzwerkprotokoll und stellt die Grundlage des Internets dar. Es ist die Implementierung der Internetschicht des TCP/IP-Modells bzw. der Vermittlungsschicht ("engl. Network Layer") des OSI-Modells. IP ist ein verbindungsloses Protokoll, d. h. bei den Kommunikationspartnern wird kein Zustand etabliert.

Das IP bildet die erste vom Übertragungsmedium unabhängige Schicht der Internetprotokollfamilie. Das bedeutet, dass mittels IP-Adresse und Subnetzmaske "(subnet mask)" für IPv4, bzw. Präfixlänge bei IPv6, Computer innerhalb eines Netzwerkes in logische Einheiten, sogenannte Subnetze, gruppiert werden können. Auf dieser Basis ist es möglich, Computer in größeren Netzwerken zu adressieren und ihnen IP-Pakete zu senden, da logische Adressierung die Grundlage für Routing (Wegewahl und Weiterleitung von Netzwerkpaketen) ist.

Öffentliche IP-Adressen müssen in der Regel weltweit eindeutig zugeordnet werden können, daher ist deren Vergabe durch die Internet Assigned Numbers Authority (IANA) geregelt. Diese delegiert große Netze an Regional Internet Registries (RIRs), welche dann Subnetze davon z. B. an Internetprovider vergeben, die weitere Subnetze bilden können oder einzelne Adressen an Kunden vergeben.

Am 1. Februar 2011 vergab IANA die letzten beiden freien IPv4-Adressblöcke 39/8 und 106/8 an das Asia-Pacific Network Information Centre APNIC. Am 3. Februar 2011 starteten IANA und ICANN daraufhin die sog. „Exhaustion Phase“, in der je einer der letzten fünf Adressblöcke für die RIRs reserviert wurde. Damit ist der IPv4-Adresspool der internationalen Vergabestelle IANA ausgeschöpft.

Im Mai 1974 veröffentlichte das Institute of Electrical and Electronics Engineers (IEEE) eine Forschungsarbeit mit dem Namen „Ein Protokoll für die gegenseitige Paket-Netzverbindung“. Darin beschrieben die Autoren Vint Cerf und Bob Kahn ein Zwischennetzwerkanschlussprotokoll, das die zur Verfügung stehenden Paketvermittlungsmöglichkeiten zwischen Knotenpunkten verwendet. Ein Hauptkontrollbestandteil dieses Modells war das „Übertragungskontrollprogramm“ (TCP), das sowohl verbindungsorientierte Verbindungen als auch Datenpaket-Dienstleistungen zwischen den Hosts vereinigte. Das monolithische Übertragungskontrollprogramm wurde später in eine Modularchitektur geteilt, die aus dem Übertragungskontrollprotokoll an der verbindungsorientierten Schicht und dem Internetprotokoll beim Zwischennetzwerkanschluss (Datenpaket) bestand. Das Modell wurde bekannt als TCP/IP.

Das Internetprotokoll ist eines der Elemente, die das Internet definieren. Dass es sich beim heute gebräuchlichen Protokoll IPv4 um die vierte Generation des Internet-Protokolls handelt, ist ein populärer Irrtum. Die Versionsnummer 4 bezieht sich lediglich darauf, dass die vierte Version des TCP-Protokolls zum Einsatz kommt. Da der Protokollstapel zusammenfassend als TCP/IP bezeichnet wird, entstand die Bezeichnung TCP/IPv4 bzw. IPv4. Die Angabe der Versionsnummer in den IP-Headern wurde in der Vergangenheit verwendet, um eine Abwärtskompatibilität zu gewährleisten und bei Bedarf Daten mit älteren Versionen des TCP- und RTP-Protokolls übertragen zu können. Das Internet-Protokoll als solches wurde bis in die 1980er Jahre hinein sehr oft überarbeitet. Mit der Einführung des TCP-Protokolls in der Version 4 kam immer noch ein Internetprotokoll zum Einsatz, das sich in seinem Aufbau von jenem unterscheidet, das wir heute umgangssprachlich als IPv4 bezeichnen (siehe DoD Standard Internet Protocol).

Bei den überarbeiteten Fassungen des Internetprotokolls ging es im Wesentlichen darum, den Verbindungsaufbau zu optimieren und dem Internetprotokoll neue Möglichkeiten zu verleihen. Lediglich bei der ersten Version des Internetprotokolls nach RFC 675 kamen noch kürzere IP-Adressen zum Einsatz, wobei in den Paket-Headern getrennte Angaben für das Netz und Host vorhanden waren. In dem vier Bit breiten Feld zur Angabe des Ziel-Netzes wurde ein Wert angegeben, der das Netz kennzeichnet, in das das Paket geleitet werden soll (i.e.S.: ARPANET, UCL, CYCLADES, NPL, CADC, oder EPSS). Die Host-Felder hatten eine Länge von jeweils 16 Bit, so dass in jedem der mit dem Internet verbundenen Netze bis zu 65.536 Rechner adressiert werden konnten. Doch bereits ein Jahr später wurde ein Internetprotokoll eingeführt, bei dem Adressfelder mit einer Länge von 32 Bit eingesetzt wurden und IP-Adressen bereits das Format hatten, wie wir es auch heute kennen. Subnetting war allerdings noch nicht vorgesehen und eine Vergabestelle zur Verteilung von Adressen gab es auch noch nicht. Das erste Oktett einer IP-Adresse bestimmte nach wie vor das Netz und die übrigen drei Oktette der Adresse den Host. Faktisch existierten somit nur flache Class-A Netze, wobei mit der Zeit immer mehr Netze über das Internet miteinander verbunden waren.

Durch die ständige Überarbeitung des Internet-Protokolls sowie darauf basierender Netzwerkprotokolle entwickelte sich langsam das Internet-Protokoll, wie wir es heute kennen und umgangssprachlich als IPv4 bezeichnen.

Der Nachfolger von IPv4 ist IPv6, wobei die Version 6 hierbei keine überarbeitete Fassung des TCP-Protokolls mehr kennzeichnet. Seine wichtigste Modifizierung ist die Länge der Adressen. IPv4 verwendet 32-Bit-Adressen (ca. 4 Milliarden, oder 4,3·10 Adressen), während IPv6 128-Bit-Adressen verwendet (ca. 340 Sextillionen, oder 3,4·10 Adressen).

Obwohl die Verwendung von IPv6 erst langsam im Juni 2008 erfolgte, verfügen alle US-Regierungssysteme über diese grundlegende Infrastruktur-Unterstützung, wenn auch nur über grundlegende Voraussetzungen. Ein Protokoll mit der Bezeichnung IPv5 hat es nie gegeben. Die Version 5 wurde in einem IP-Header nur dann angegeben, wenn es sich um Datenpakete des "Internet Streaming Protocol" (Version 2) handelte, um beteiligte Netzwerkgeräte auf bestimmte Eigenheiten dieses Protokolls aufmerksam zu machen, das eine besondere Behandlung der Datenpakete erforderte. Dieses experimentelle Protokoll spielte in der Praxis jedoch kaum eine Rolle. Allerdings ist dies der Grund, warum das IPv6-Protokoll nicht IPv5 heißt. Die Versionsnummern 6 bis 9 wurden für verschiedene Protokoll-Modelle vorgeschlagen, die entworfen wurden, um IPv4 zu ersetzen.

Andere Protokoll-Vorschläge, genannt IPv9 und IPv8, tauchten nur kurz auf; es erfolgte aber keine weitere Umsetzung.

In der Praxis ist IP fast ausschließlich in der Version IPv4 im Einsatz. Die Nachfolgeversion IPv6 wird bereits von zahlreichen Betriebssystemen sowie einer Reihe von Endanwendungen unterstützt und gilt als genügend ausgereift für einen umfassenden Einsatz. Beide Versionen können gleichzeitig auf derselben Infrastruktur betrieben werden; daneben gibt es weitere Übergangsmechanismen von IPv4 zu IPv6. Auch die wichtigsten Backbones im Internet leiten bereits IPv6-Pakete weiter, so dass ein schrittweiser Umstieg nur noch von der Umsetzung durch die Serverbetreiber und Diensteanbieter abhängig ist.

Die Designgrundsätze der Internetprotokolle nehmen an, dass die Netzinfrastruktur an jedem einzelnen Netzelement oder Übertragungsmedium von Natur aus unzuverlässig ist. Auch setzen diese voraus, dass sich die Infrastruktur im Bezug auf Verfügbarkeit von Verbindungen und Knoten dynamisch verhält. Um jedoch die Netzinfrastruktur aufrechtzuerhalten, wird das Hauptaugenmerk der Datenübertragung vorsätzlich größtenteils auf den Endknoten jeder einzelnen Datenübermittlung gelegt. Router im Übertragungspfad schicken Datenpakete nur zu direkt erreichbaren und bekannten Übergängen, die die für den Bestimmungsort festgelegten Adressen vom Routenplanungspräfix vergleichen.

Demzufolge stellen diese Internetprotokolle nur beste Übergänge zur Verfügung, wodurch diese Dienste als unzuverlässig charakterisiert werden. Die im Netz architektonische Sprache ist ein Protokoll der Verbindung, im Gegensatz zu den weniger verbindungsorientierten Übertragungen.
Jedes einzelne Datenpaket wird unabhängig behandelt. Da jeder einzelne Übermittlungsweg eines Datenpaketes neu definiert wird (dynamisch), ist es möglich, dass die Pakete auf verschiedenen Pfaden zu ihrem Bestimmungsort gesendet werden.

Die Internetprotokoll-Version 4 (IPv4) stellt den benötigten Schutz zur Verfügung, um sicherzustellen, dass der Protokollkopf jedes Datenpaketes fehlerfrei ist. Ein Routenplanungsknoten berechnet eine Prüfsumme für den Paketkopf. Wenn die Prüfsumme ungültig ist, verwirft der Routenplanungsknoten das Paket. Der Routenplanungsknoten muss keinen Endknoten bekannt geben, obwohl das Internetkontrollnachrichtenprotokoll (ICMP) solche Ankündigungen erlaubt. Im Gegensatz dazu verfügt die Internetprotokoll-Version 6 (IPv6) über keine Prüfsumme, was zu einer schnelleren Verarbeitung während der Routenplanung führt.

Alle Fehlerquellen im Übertragungsnetz müssen entdeckt und mit Hilfe der Übertragung auf Endknoten ersetzt werden. Die oberen Schicht-Protokolle der Internetprotokoll-Familie sind dafür verantwortlich, Zuverlässigkeitsprobleme aufzulösen. Zum Beispiel kann ein Host Daten zurückhalten und eine Richtigstellung durchführen, bevor die Daten an den jeweiligen Empfänger geliefert werden.

Selbst wenn der Übermittlungspfad verfügbar und zuverlässig ist, besteht wegen der dynamischen Natur und der Heterogenität des Internets und seiner Bestandteile keine Garantie, dass auch tatsächlich jeder dieser einzelnen Pfade fähig ist, eine Datenübermittlung durchzuführen. Zum Beispiel stellt die erlaubte Übermittlungsgröße der jeweiligen Datenpakete eine technische Einschränkung dar. Jede Anwendung muss versichern, dass richtige Übertragungseigenschaften verwendet werden.

Ein Teil dieser Verantwortung liegt auch in den oberen Schicht-Protokollen. IPv6 verwendet die Fähigkeit, die maximale Übertragungseinheitsgröße einer lokalen Verbindung, sowie den dafür komplett geplanten Pfad zum Bestimmungsort zu untersuchen. Die IPv4-Zwischennetzwerkanschlussschicht hat die Fähigkeit ursprünglich, große Datenpakete automatisch in kleinere Einheiten für die Übertragung zu zerlegen.

Das Übertragungskontrollprotokoll (TCP) ist ein Beispiel eines Protokolls, das seine Segment-Größe reguliert, um kleiner zu sein als der maximal erlaubte Durchfluss, die Maximum Transmission Unit (MTU). Das Benutzerdatenpaket-Protokoll (UDP) und das Internetkontrollnachrichtenprotokoll (ICMP) ignorieren jedoch die MTU-Größe, wodurch das IP gezwungen wird, übergroße Datenpakete zu splitten.






</doc>
<doc id="8938" url="https://de.wikipedia.org/wiki?curid=8938" title="BRT">
BRT

BRT steht für:


</doc>
<doc id="8940" url="https://de.wikipedia.org/wiki?curid=8940" title="Emacs">
Emacs

Emacs [] ist eine Familie von Texteditoren. Die erste Emacs-Implementierung wurde von Richard Stallman (zusammen mit Guy L. Steele, Jr. und anderen) entwickelt. Besonders populär ist heute der GNU Emacs, der durch seine Programmierschnittstelle in der Programmiersprache Emacs Lisp mit beliebigen Erweiterungen ausgestattet werden kann. Es gibt aber auch noch eine Vielzahl von anderen Editoren, die zur Emacs-Familie zählen.

GNU Emacs ist als freie Software unter der GNU General Public License erhältlich und läuft auf den meisten heute üblichen Betriebssystemen (Unix, GNU/Linux, macOS und Windows).

Emacs bietet eine ganze Reihe Betriebsarten (engl. "modes"), die bei der Erstellung von Quelltext für diverse Programmier- bzw. Auszeichnungssprachen hilfreich sind. So kann man Emacs z. B. als HTML-Editor verwenden, der auch Syntaxüberprüfungen vornimmt.

Syntaxhervorhebung wird in den meisten dieser Betriebsarten unterstützt. Dabei wird der Text aufgrund der Syntax des bearbeiteten Textes (LaTeX, HTML, Perl, Java und andere) eingefärbt, was dem Benutzer die Orientierung erleichtert. Die Modi bieten in der Regel wesentlich mehr als eine Syntaxhervorhebung: Übersetzungsvorgänge, Syntaxprüfer, Debugger und dergleichen mehr lassen sich von Emacs aus aufrufen.

In der Grundkonfiguration verfügt Emacs bereits über einen Kalender, mehrere News- und Mailreader mit POP- und IMAP-Unterstützung, eine eingebaute Shell, Spiele, einen FTP-Client und einen Webbrowser.
Es gibt zusätzlich zahlreiche Tools, die in Emacs eingebunden werden können, darunter IRC-Clients, IM-Clients, Adressbücher, Audioplayer und sogar Webserver.

Emacs ermöglicht per Wiki Mode das Bearbeiten von Webseiten als Wikitext.

Zum Spaß und zur Demonstration, was mit Emacs Lisp alles möglich ist, enthält Emacs mit ELIZA ein Programm zur Unterhaltung mit einem vom Computer generierten „Psychologischen Psychotherapeuten“ (Aufruf mit „M-x doctor“; „M-x“ ist z. B. „ESC x“ oder „Alt-x“). Das Programm wandelt Aussagen des Benutzers in Fragen um, ermuntert ihn, mehr zu erzählen, und suggeriert Lebensprobleme allgemeinster Art. Ein weiterer nostalgischer Zusatz ist ein Textadventure („M-x dunnet“).

Man kann Emacs auch als eine Umgebung zur Programmierung von Spezialeditoren betrachten; so gibt es einen po-mode, mit dem man Übersetzungen erstellen kann.

Emacs entstand 1976 am MIT zunächst als Sammlung von Makros für den Editor TECO.
Der Name ist die Abkürzung von „Editor MACroS“.

1978/79 portierte Bernard Greenberg den Editor auf das Großrechner-Betriebssystem Multics. Er benutzte dazu die Programmiersprache Maclisp.

1981 schrieb James Gosling den ersten Emacs für Unix-Systeme in C. Die Erweiterungssprache Mocklisp ähnelt Lisp, kennt aber keine strukturierten Datentypen. Gosling schränkte die Verbreitung zunächst nicht ein, aber verkaufte den Code später an UniPress, die diese Version als UniPress Emacs vertrieben. Gosling Emacs zeichnet sich durch einen hocheffizienten Code zur Textausgabe aus; Stallman verwendete Teile des Gosling-Codes in GNU Emacs, was später zu einer Kontroverse mit UniPress führte.

1984 begann Richard Stallman, an einer neuen Implementierung von Emacs, GNU Emacs, zu arbeiten, die das erste Programm des damals entstehenden GNU-Projekts wurde. Die Lizenz des Programms war zu Beginn der Entwicklung die "GNU Emacs General Public License". Es war die erste Copyleft-Lizenz und die Grundlage für die später entwickelte "GNU General Public License" (GPL). GNU Emacs ist zum größten Teil in Emacs Lisp, einem eigenen Dialekt der Lisp-Programmiersprache, programmiert. Diese Lisp-Version von Emacs fußt nicht auf Greenbergs in Maclisp geschriebenem Multics-Emacs, der ersten Lisp-Version, und benutzt auch ganz andere Datenstrukturen. Den Kern bildet ein in C geschriebener Interpreter für Emacs Lisp. Gerd Möllmann hat Version 21 (21.1 und 21.2) als Hauptprogrammierer betreut und veröffentlicht. Version 23 wurde im Jahr 2009 fertiggestellt.

Wie Clifford Stoll aufdeckte, ermöglichte ein Programmfehler in Emacs 1986 dem für den KGB spionierenden Hacker Markus Hess den Einbruch in das Lawrence Berkeley National Laboratory.

Von Emacs wurden einige Derivate entwickelt, das am weitesten verbreitete ist XEmacs, dessen Projektgruppe sich schon seit längerem mit der Einbindung von GUI-Elementen in Emacs beschäftigt. Bekannt ist auch MicroEmacs, der unter anderem mit AmigaOS ausgeliefert wurde. Ein weiterer Klon ist QEmacs.

Aquamacs von David Reitter ist eine an die Human Interface Guidelines angepasste Emacs-Variante für Mac OS X, die – wie der ehemalige Carbon Emacs von Seiji Zenitani – viele zusätzliche Pakete bereits vorinstalliert enthält. Aquamacs kann aber auch über die klassische Emacs-Bedienoberfläche verwendet werden.

Eine besonders kleine, aber dennoch recht leistungsstarke Version ist Zile. Der Name ist ein rekursives Akronym und bedeutet "Zile is lossy Emacs".


Richard Stallman hat scherzhafterweise den Editor Emacs zu einer Spaßreligion erhoben, der „Church of Emacs“, und bezeichnet sich selbst als St. IGNUcius. Als Glaubensbekenntnis muss man dreimal „There is no system but GNU, and Linux is one of its kernels.“ sagen. Dazu gibt es noch die Newsgroup "alt.religion.emacs", die sich dieser Parodie widmet. Als Reaktion darauf gründeten die Anhänger des Konkurrenz-Editors vi den Cult of Vi.

Benutzer haben weitere, scherzhafte Deutungen aus Eigenarten von Emacs abgeleitet:
Eight Megabytes And Constantly Swapping ("Acht Megabyte groß und swappt dauernd") nimmt den für damalige Zeiten großen Arbeitsspeicher-Bedarf aufs Korn, ebenso Emacs Makes Any Computer Slow ("Emacs macht jeden Computer langsam").
Escape-Meta-Alt-Control-Shift ist eine Anspielung auf die Tastenkombinationen, mit denen die meisten Funktionen von Emacs auszulösen sind.

In Anspielung auf den großen Funktionsumfang schrieb Thomer M. Gil: (deutsch: Emacs ist ein großartiges Betriebssystem – allerdings fehlt ein guter Editor).




</doc>
<doc id="8941" url="https://de.wikipedia.org/wiki?curid=8941" title="Moral">
Moral

Moral bezeichnet zumeist die faktischen Handlungsmuster, -konventionen, -regeln oder -prinzipien bestimmter Individuen, Gruppen oder Kulturen. 

So verstanden sind die Ausdrücke Moral, Ethos oder Sitte weitgehend gleichbedeutend, und werden beschreibend (deskriptiv) gebraucht. Daneben wird mit der Rede von "Moral" auch ein Bereich von praktischen Wertvorgaben (Werte, Güter, Pflichten, Rechte), Handlungsprinzipien, oder allgemein anerkannter (gesellschaftlicher) Urteile verbunden. Eine so verstandene Unterscheidung von Moral und Unmoral ist nicht "beschreibend", sondern "normsetzend" (normativ). Eine moralische Bewertung kann als bloßer Ausdruck subjektiver Zustimmung oder Ablehnung verstanden werden (vergleichbar mit Applaus oder Buhrufen), vor allem bei der Beurteilung von Handlungen, deren Maximen oder sonstige Prinzipien als moralisch gut oder moralisch schlecht gelten. Daher bezeichnet Moral im engeren Sinn die subjektive Neigung, der Sitte oder Moral im weiteren Sinne, oder davon abweichenden, jedoch als richtig angesehenen eigenen ethischen Maximen, zu folgen. In diesem Sinne wird auch Engagement oder besondere Disziplin innerhalb einer Gruppe als „Moral“ bezeichnet.

Positionen, die einen metaethischen Realismus vertreten, gehen davon aus, dass der moralische Wert einer Handlung, eines Weltzustands oder eines Gegenstandes nicht auf deren bzw. dessen subjektive Bewertung reduziert werden kann. Die theoretische Ausarbeitung unterschiedlicher methodischer Vorgehensweisen und Kriterien moralischer Urteile und Gefühle sind Gegenstand der philosophischen Disziplin der Ethik.

Der deutsche Ausdruck „Moral“ geht über das französische "morale" auf das lateinische "moralis" (die Sitte betreffend; lat: "mos, mores" Sitte, Sitten) zurück, das im von Cicero neugeprägten Ausdruck "philosophia moralis" als Übersetzung von "êthikê" (Ethik) verwendet wird.

Moral beschrieb ursprünglich vor allem, wie Menschen "faktisch" handeln und welches Handeln in bestimmten Situationen erwartet bzw. für richtig gehalten wird. Dieser deskriptive Bedeutungsaspekt einer Moral wird auch als Sittlichkeit oder Ethos bezeichnet und umfasst „regulierende Urteile und geregelte Verhaltensweisen“, ohne dass die rationale oder moraltheoretische Rechtfertigung derselben beurteilt oder bewertet wird. Eine solche Beurteilung wird als „Reflexionstheorie der Moral“ oder „Ethik“ bezeichnet.

Moral ist Gegenstand diverser Wissenschaften:


Als soziales Wesen erfährt der Mensch von Geburt an im Normalfall Liebe, die Bereitschaft zum Verzicht und zur Fürsorge. Ohne diese Eigenschaften wäre ein dauerhaftes Zusammenleben in Gemeinschaften nicht möglich. Sie haben sich im Laufe der Evolution entwickelt und die Veranlagung dazu liegt demnach in den Genen. Der Biologe Hans Mohr drückt es folgendermaßen aus: Die konkreten Moralvorstellungen eines Menschen sind jedoch kulturell überprägt: Sie äußern sich etwa in der „goldenen Regel“, in religiösen Handlungsvorschriften (etwa die Zehn Gebote im Christentum, die Fünf Silas im Buddhismus oder die Traumzeit-Mythologie der australischen Aborigines) oder in den Rechtsnormen der modernen Staaten. Trotz der moralischen Veranlagung können Erziehung und ideologische Manipulation selbst destruktive Verhaltensweisen zum angeblich „Guten“ erheben, die den eingangs genannten Eigenschaften komplett widersprechen.

Es ist eine der Grundfragen der Rechtsphilosophie, in welchem Verhältnis Recht und Moral zueinander stehen. In vielerlei Hinsicht stimmen Moral und Recht (z. B. das Tötungsverbot) überein. Die Frage, wie es z. B. um moralisch verwerfliche Gesetze steht, wurde seit der Antike (siehe Naturrecht) und in der jüngeren Geschichte besonders intensiv in der deutschen Nachkriegszeit diskutiert. Nennenswert sind hierbei insbesondere die Radbruchsche Formel zum Verhältnis von Recht und Ungerechtigkeit, die Gehorsamsverweigerung und die Frage, ob Deserteure amnestiert werden sollten (siehe Gesetz zur Aufhebung nationalsozialistischer Unrechtsurteile in der Strafrechtspflege).

In "deskriptiver" Verwendung beschreibt „Moral“ eine Handlungsregelung, die für eine Gesellschaft, soziale Gruppe oder ein Individuum leitend ist oder „die in einer konkreten Gemeinschaft eingelebten oder von einer Person internalisierten Verhaltensregeln“. Dies wird je nach Theorieansatz unterschiedlich präzisiert, etwa als „Gesamtheit der sozial repräsentierten und im Persönlichkeitssystem der Individuen verankerten regelbezogenen Handlungsorientierungen und wechselseitigen Verhaltenserwartungen oder als eine näher bestimmte Teilklasse“ derselben. Luhmann definiert, „rein empirisch gemeint“: „Eine Kommunikation nimmt moralische Qualität an, wenn und soweit sie menschliche Achtung oder Missachtung zum Ausdruck bringt“. In diesem deskriptiven Sinne werden auch „moralisch“ oder „sittlich“ schlicht deskriptiv im Sinne von „zur Moral gehörig“, nicht normativ im Sinne von „moralisch gut“ gebraucht. „Moral“ bezeichnet dann etwa „ein Unternehmen der Gesellschaft“ zur „Lenkung des einzelnen und kleinerer Gruppen“. Derartigen deskriptiven Redeweisen entsprechen alltagssprachliche Formeln wie „herrschende Moral“, „bürgerliche Moral“ oder „sozialistische Moral“. Der Psychologe Jonathan Haidt hat folgende Definition vorgeschlagen: „Moralische Systeme sind ineinandergreifende Zusammenstellungen von Werten, Tugenden, Normen, Gebräuchen, Identitäten, Institutionen, Technologien und entwickelten psychischen Mechanismen, die zusammenwirken, um Selbstsucht zu unterdrücken oder zu regulieren und soziales Zusammenleben zu ermöglichen.“

In seinem 1969 veröffentlichten Werk "Moral und Hypermoral" hat der 
Philosoph Arnold Gehlen eine pluralistische Ethik entworfen und zeitkritisch Tendenzen der Gesellschaft beschrieben, die er als "hypermoralisch" bezeichnet. Er kritisierte, dass Hypermoral sich ungebührlich an Privatem und Innerlichem (im Extremfall: an Gedankenverbrechen) festbeiße, während Missstände gleichzeitig vernachlässigt werden, die auch außerhalb des Persönlichen und Gedanklichen existieren, wo ihnen gesellschaftliche Institutionen wie Politik oder Rechtssystem entgegenwirken könnten. Odo Marquard hat Gehlens Gedanken 1986 in seinem Aufsatz "Entlastungen" weitergeführt und schrieb von „Übertribunalisierung“.

Im politischen Diskurs der Gegenwart wird über „Hypermoral“ erneut nachgedacht, etwa im Hinblick auf Debatten um „Mikroaggression“, die aktuell (2016) an Hochschulen in den Vereinigten Staaten geführt werden, aber auch in Deutschland, etwa im Streit um die politisch korrekte Mediendarstellung von Straftaten Angehöriger ethnischer oder religiöser Minderheiten.





</doc>
<doc id="8943" url="https://de.wikipedia.org/wiki?curid=8943" title="Körperflüssigkeit">
Körperflüssigkeit

Die zahlreichen Körperflüssigkeiten des menschlichen Körpers, bestehend hauptsächlich aus Wasser, können grob untergliedert werden in solche, die in Flüssigkeitskreisläufen zirkulieren und solche, die innerhalb bestimmter Räume (Kompartimente) vorkommen. Diese Trennung ist aber eher formal, da auch die scheinbar „stehenden“ Flüssigkeiten, wie z. B. das Kammerwasser des Auges, an bestimmten Stellen gebildet (sezerniert) und andernorts wieder aufgenommen (resorbiert) werden und somit einem (wenn auch sehr langsamen) Fluss unterliegen.

Außerdem kann eine Unterscheidung in intrazelluläre und extrazelluläre Flüssigkeit getroffen werden. Doch auch der Intrazellularraum unterliegt einem ständigen Austausch (Diffusion durch Membranen als Folge des zellulären Stoffwechsels). Mageres Muskelgewebe enthält in seinen Zellen ungefähr 75 % Wasser. Das Blutplasma enthält zu 90 bis 95 % Wasser, das Körperfett 25 % Wasser und auch die Knochen haben noch einen Wasseranteil von 22 %. Das Wasser dient ebenso wie das Blut dem Transport von Sauerstoff und Nährstoffen zu den Organen und deren Zellen ebenso wie dem Abtransport von Stoffwechselprodukten („Abfällen“). Der Wasserbestand im Körper und das Volumen der großen Kompartimente werden unter anderem hormonell reguliert.

Die Bezeichnung der Zwischenzellflüssigkeit, die sich auf dem Transport von Zelle zu einem Blutgefäß oder umgekehrt befindet: interstitielle Flüssigkeit oder Gewebsflüssigkeit.

Blut dient dem Transport von Sauerstoff und Nährstoffen zu den Organen und deren Zellen ebenso wie dem Abtransport von Stoffwechselprodukten und Abfällen. Außerdem werden darin Hormone und weitere Wirkstoffe zwischen den Zellen befördert. 55 % des Blutes sind Blutplasma, das bis 95 % aus Wasser besteht, der Rest sind im Trägerstoff Wasser gelöste weitere Stoffe. 

Blut wird von der rechten beziehungsweise linken Herzhälfte gepumpt und zirkuliert in den Blutgefäßen. In ihm schwimmen rote Blutkörperchen (Erythrozyten), weiße Blutkörperchen (Leukozyten) und Blutplättchen (Thrombozyten).

Lymphe erfüllt im Körper zwei Aufgaben: Zum Einen ist sie Bestandteil des Immunsystems, indem sie Krankheitserreger zu den Lymphknoten transportiert, zum Anderen transportiert sie im Rahmen der Verdauung Moleküle, die zu groß sind, um direkt vom Gewebe in den Blutkreislauf transportiert zu werden, wie Eiweiße und Lipide aus dem Verdauungstrakt.

Lymphe zirkuliert in den Lymphgefäßen. Diese sammeln das Plasma, das nicht direkt aus dem Gewebe in die Kapillaren zurückkehrt, und führen es den Venen zentral wieder zu.

Speichel wird in den Speicheldrüsen produziert und dann im Mund freigesetzt. Er enthält ein Verdauungsenzym, die α-Amylase, die in der Nahrung enthaltene Polysaccharide aufspaltet, und Muzin und Immunglobulin A als Schutz vor Krankheitserregern.

Magensaft oder auch Magensäure wird in den Belegzellen der Magenschleimhaut produziert und enthält Salzsäure und Pepsin. Letzteres ist ebenso ein Verdauungsenzym und spaltet Proteine.

Das Pankreas (die Bauchspeicheldrüse) scheidet verschiedene Verdauungsenzyme aus, die ebenso der Spaltung von Proteinen dienen. Diese gelangen über Ausführungsgänge, die sich mit dem Hauptgallengang vereinigen, in den Zwölffingerdarm.

Galle dient zum einen der Fettverdauung, in dem die darin enthaltenen Gallensalze die wasserunlöslichen Lipide umhüllen und Mizellen bilden, die dann von der Darmschleimhaut absorbiert werden können. Außerdem werden mit ihrer Hilfe Medikamente und ihre Abbauprodukte ebenso wie physiologische Abbauprodukte (z. B. Bilirubin) mit dem Stuhl ausgeschieden.

Der Gallensaft wird in der Leber produziert, in der Gallenblase gespeichert und über die Gallengänge in den Darm abgegeben. Hier wird dann ein Teil der Galle über die Darmschleimhaut wieder in das Blut resorbiert, der Andere wird mit dem Stuhlgang ausgeschieden.

Sekrete und Exkret sind unterschiedliche Flüssigkeitsausscheidungen verschiedener Organe, die den Körper verlassen und damit zu einer (evtl. geringen oder reversiblen) Gewichtsabnahme führen.

Urin wird in den Nieren produziert. Seine Ausscheidung dient der Regulation des Flüssigkeits- und Elektrolythaushalts sowie der Eliminierung von Stoffwechselabbauprodukten, insbesondere der beim Abbau von Proteinen und Nukleotiden entstehenden Stickstoff-Verbindungen.

Von den Nieren gelangt er durch die Harnleiter zur Harnblase, wo er gesammelt und dann über die Harnröhre ausgeschieden wird.

Schweiß wird in den Schweißdrüsen produziert, die sich in der Lederhaut befinden, und durch Poren in der Haut ausgeschieden.

Schweiß dient durch die bei seiner Verdunstung entstehende Verdunstungskälte der Regulation des Wärmehaushalts, er hält die Haut geschmeidig und ihren pH-Wert konstant. Außerdem enthält er sexual- und individualspezifische Duftstoffe.



Sowie bei Erkrankungen:

Die Körperflüssigkeit verteilt sich auf die beiden Kompartimente Extra- und Intrazellulärraum (EZR, IZR), wobei der Intravasalraum (innerhalb von Gefäßen) wichtiger Teil des Extrazellulären ist.

Die prozentuale Wasserverteilung in den Flüssigkeitsräumen des Menschen (EZR, IZR) unterscheidet sich in Abhängigkeit vom Lebensalter (sie nimmt etwa zwischen 75 % bei Neugeborenen auf 55 % beim Erwachsenen ab). Bei Frauen ist die Gesamtflüssigkeitsmenge im Körper etwa 5 % bis 10 % niedriger als beim erwachsenen Mann. Bei Erwachsenen ist der Gesamtwassergehalt von ca. 65 % des jeweiligen Körpergesamtgewichts etwa im Verhältnis 3:2 auf Intrazellularraum und Extrazellularraum verteilt. 

Nicht nur in der Größe unterscheiden sich die Räume der Körperflüssigkeit, sondern auch in der Zusammensetzung wie bei den Elektrolyten, Proteinen und der Osmolalität.

Beim Geschlechtsakt und bei erotischen Handlungen wie Küssen kommt es zum Austausch von geringen Mengen an Körperflüssigkeiten. Über den Austausch von Körperflüssigkeiten können Krankheiten übertragen werden, wenn jemand mit einem Erreger infiziert ist. Es ist wissenschaftlich erwiesen, dass Küssen bei gesundem Befinden das Immunsystem stärkt.


Bei Personen mit einem normalen Gewicht ist Wasser in den meisten Körperteilen außer dem Fettgewebe reichlich vorhanden. Die folgenden Berechnungen sind für Erwachsene mit einem durchschnittlichen Körperbau gedacht und für übergewichtige oder besonders muskulöse Personen nicht geeignet. Die Anteile wurden sehr vereinfacht und verwenden gerundete Zahlen für eine schnelle Berechnung. Bei Männern bestehen 60 % der Körpermasse aus Wasser. Bei Frauen liegt dieser Wert bei ungefähr 55 % wegen eines höheren Anteils an Körperfett. Dies wird "Gesamtkörperwasser" genannt.

Von diesem Wasser sind zwei Drittel in den Zellen und wird deshalb "intrazelluläres Wasser" genannt. Das andere Drittel ist "extrazelluläres Wasser".

Das extrazelluläre Wasser besteht aus Blutplasma, das ein Fünftel des extrazellulären Wassers darstellt, und extrazellulärer Flüssigkeit, aus welcher die anderen vier Fünftel bestehen. Wasser befindet sich auch an anderen Orten wie dem Auge, aber dieses kann als eine bedeutungslose Quantität in der schnellen Berechnung ignoriert werden.

Es gibt nicht viele Methoden, die verwendet werden können, um den Anteil des Körperwassers in den verschiedenen Kompartimenten festzustellen. Die einfachste Art und Weise, um eine überschlägige Schätzung zu erhalten, ist die Berechnung eines Schätzwertes anhand von Körpergewicht, Geschlecht und Alter.

Im Folgenden sind die Richtwerte der Weltgesundheitsorganisation (WHO) für die Abschätzung des Wasseranteils bei normalgewichtigen Personen aufgeführt.


Das Gesamtkörperwasser kann mit der Massenspektrometer-Messung des Deuterium-Überschusses in den Atemproben von Einzelpersonen festgestellt werden. Eine bekannte Dosis des Deuteriums-Wassers (DO) wird innerhalb des Körperwassers ausgleichend eingenommen und dort belassen. Das Massenspektrometer misst dann das Verhältnis des Deuteriums zum Wasserstoffs (D:H) im ausgeatmeten Atemwasserdampf. Das Gesamtkörperwasser wird dann genau von der Zunahme des Atemdeuteriuminhalts in Bezug zur Abgabe des eingenommen DO gemessen.

Eine Flüssigkeitsbilanz dient der getrennten Erfassung von Ein- und Ausfuhr verschiedener Flüssigkeiten bei einem Patienten; z. B. als Infusion oder mit Nahrungsmitteln, beim Schwitzen oder über die Ausscheidung. Sie ist wie eine Art Tabelle aufgebaut, in welcher der Patient oder das Pflegepersonal die zugeführte und die abgegebene Flüssigkeit in die dafür vorgesehenen Spalten einträgt. Meistens wird nach 24 Stunden zusammengerechnet, ob ein Negativbilanz (vermehrte Ausscheidung) oder eine kaum bemerkbare Einlagerung (Überschuss) bis hin zu sichtbaren Einlagerungen (Ödemen) vorliegt. Auch beim gesunden Menschen gibt es dafür eine Bandbreite, die zunächst unauffällig und nicht krankhaft ist. Bei ständigem Flüssigkeitsverlust kann es jedoch zur allmählichen Exsikkose kommen. 

Die Angaben der Flüssigkeitsaufnahme (Stoffwechsel, Getränke und Nahrung) und der Flüssigkeitsabgabe (Miktion) werden in der Regel in Milliliter (ml) notiert. Die Genauigkeit dieser Messung kann erhöht werden, indem die Flüssigkeit über Blasenkatheter zur Kontrolle der Nierenfunktion in Sammelgefäße abgeleitet und die Zusammensetzung analysiert (beprobt, Urinlaborwerte) wird. Dies kann in beliebigen Zeitabständen, z. B. stündlich, fraktioniert erfolgen. Ferner wird bei bestimmten Patienten festgehalten, ob die Person die Toilette noch rechtzeitig erreichen konnte, oder ob die Blasenentleerung in ein Inkontinenzhilfsmittel wie z. B. eine Vorlage oder eine Inkontinenzeinlage (fälschlich Windel genannt) erfolgte. 

Auch eine genaue und tägliche Gewichtskontrolle kann hier hilfreich sein. Außerdem wird noch bei diagnostischem Bedarf zusätzlich vermerkt, ob vor der Miktion ein Harndrang bestand. Eine teilweise Bilanzierung, bei der nur die Ausfuhr aus der Blase notiert wird, heißt "Miktionsprotokoll". Notizen nur über die orale Zufuhr von Getränken heißt "Trinkprotokoll/Trinkzettel".

Folgende Probleme können durch einen anomalen Gehalt an Wasser im Körper (bzw. in Teilen des Körpers) verursacht werden:

Dagegen beruhen die verschiedenen Formen von "Urininkontinenz" i. d. R. "nicht" auf dem Flüssigkeits-/Wasseranteil in einem Gewebe. Nur die Flüssigkeitsmenge in der Blase kann Auslöser (intraindividuelle Bandbreite der Menge) bzw. evtl. nicht der Auslöser (pathologisch) des Harndranggefühls sein.





</doc>
<doc id="8944" url="https://de.wikipedia.org/wiki?curid=8944" title="Gray">
Gray

__NoToC__
Das Gray [] (Einheitenzeichen "Gy") ist eine von den SI-Einheiten Joule und Kilogramm abgeleitete Größe. Sie gibt die durch ionisierende Strahlung verursachte Energiedosis an und beschreibt die pro Masse absorbierte Energie. Auch die Kerma wird in Gray gemessen.

Das "Gray" ist nach dem britischen Physiker und Vater der Radiobiologie, Louis Harold Gray, benannt. 

Die Einheit ist der Quotient aus der aufgenommenen Energie und der Masse des Körpers.
1 Gy = 1 J/kg = 100 rd.

Es findet vor allem Verwendung in der Medizin, etwa zur Angabe der angewendeten Strahlungsdosis bei einer Strahlentherapie oder der nuklearmedizinischen Therapie.

Um die Energiedosis unterschiedlicher Strahlungsarten hinsichtlich der schädigenden Wirkung für Organismen besser vergleichen zu können, wird sie mit einem Gewichtungsfaktor multipliziert, dem Strahlungswichtungsfaktor. Durch ihn wird die relative biologische Wirksamkeit der Strahlung berücksichtigt. Da der Strahlungswichtungsfaktor eine dimensionslose Zahl ist, haben die Äquivalentdosis und die Energiedosis gleiche Dimension. Um jedoch den Unterschied zwischen den Dosisgrößen für den praktischen Gebrauch hervorzuheben und da sich der Zahlenwert der Dosisgröße durch den Strahlungswichtungsfaktor auch ändern kann, werden für Energiedosen Gray verwendet, für gewichtete Dosisgrößen Sievert.

Bis zum 31. Dezember 1985 war die offizielle Einheit der Energiedosis das "rad" (Einheitenzeichen "rd"). 



</doc>
<doc id="8945" url="https://de.wikipedia.org/wiki?curid=8945" title="Strahlung">
Strahlung

Der Begriff Strahlung bezeichnet die Ausbreitung von Teilchen oder Wellen. Im ersten Fall spricht man von "Teilchenstrahlung" oder "Korpuskularstrahlung", im zweiten von "Wellenstrahlung".

Die Unterscheidung zwischen Teilchen und Wellen ist historisch und hat als angenäherte, anschauliche Aussage nach wie vor Bedeutung. Jedoch hat nach heutiger Kenntnis "jede" Strahlung sowohl Teilchen- als auch Welleneigenschaften (siehe auch Welle-Teilchen-Dualismus).

"Strahlen", die Mehrzahl zu "Strahl", wird manchmal gleichbedeutend mit dem Begriff Strahlung verwendet, auch in Zusammensetzungen wie etwa "Alpha-" oder "Röntgenstrahlen". Eine Einzahl wie etwa "Röntgenstrahl" bezeichnet dagegen fast immer ein Strahlenbündel, das gerichtet ist und dabei Energie und Impuls transportiert. Wenn der Strahl aus Teilchen mit Masse, Ladung oder anderen Eigenschaften besteht, werden auch diese transportiert. "Lichtstrahl" kann allerdings beides, den idealisierten linienförmigen Strahl (siehe geometrische Optik) oder ein Strahlenbündel, bedeuten. Die Mehrdeutigkeit des deutschen Wortes "Strahl" zeigt sich auch darin, dass in anderen Sprachen jeweils mehrere verschiedene Ausdrücke dafür existieren. Im Englischen bezeichnet zum Beispiel ' einen gedachten, idealisierten Strahl, ' ein Strahlenbündel und "" einen Strahl aus makroskopischer Materie.

Die Ausbreitung von Schall und anderen mechanischen Wellen folgt ähnlichen Gesetzen wie die Ausbreitung von elektromagnetischer Strahlung. Sie werden dennoch kaum als Strahlung bezeichnet.

Trifft die Strahlung auf ein Hindernis, wird sie entweder absorbiert (aufgenommen und umgewandelt), unbeeinflusst transmittiert (hindurchgelassen), gestreut oder reflektiert (zurückgeworfen). 

Die historische Debatte, ob Lichtstrahlen aus Teilchen oder Wellen bestehen, wurde in der Quantenphysik damit beantwortet, dass ein Lichtstrahl aus Photonen besteht, deren Aufenthaltsort im Rahmen der Quantenmechanik durch eine Wahrscheinlichkeitswelle beschrieben wird. Diese Wahrscheinlichkeitswellen können miteinander interferieren (siehe Doppelspaltversuch). Louis de Broglie hat in seiner Theorie der Materiewellen gezeigt, dass jedem Teilchen eine Wellenlänge zugeordnet werden kann. Dies erklärt, warum zum Beispiel ein Elektronenstrahl auch Interferenzphänomene zeigt (siehe auch Welle-Teilchen-Dualismus).

Man unterscheidet Strahlung nach ihren Bestandteilen, nach ihrer Quelle oder nach ihrer Wirkung.

Elektromagnetische Wellen bestehen aus Photonen. Elektromagnetische Wellen mit kurzer Wellenlänge, also hoher Photonenenergie, werden im Sprachgebrauch häufig als "elektromagnetische Strahlung" bezeichnet: so z. B. Röntgenstrahlung, Bremsstrahlung, UV-Strahlung, Wärmestrahlung oder Infrarotstrahlung. Im langwelligen Bereich spricht man eher von "Wellen", etwa Radiowellen.

Die Ausdrücke "Teilchenstrahlung" und "Korpuskularstrahlung" werden manchmal als Oberbegriffe für Strahlung verwendet, deren Bestandteile eine von Null verschiedene Masse haben.

Teilchenstrahlung unterscheidet man nach der Sorte der Teilchen, aus denen sie besteht, beispielsweise Alphastrahlung (α-Teilchen), Betastrahlung (Elektronen oder Positronen) oder Neutronenstrahlung.

Strahlung aus dem Weltraum unterscheidet man nach ihrer Quelle, beispielsweise Sonnenstrahlung, kosmische Strahlung, Hintergrundstrahlung und Hawking-Strahlung. Strahlung, die von radioaktiven Stoffen ausgeht, wird häufig fälschlicherweise als "radioaktive Strahlung" bezeichnet, obwohl nicht die Strahlung radioaktiv ist, sondern der emittierende Stoff. Die Strahlung aufgrund der natürlichen Radioaktivität der Erde heißt terrestrische Strahlung.

Ist die Energie der Strahlungsteilchen so hoch, dass sie aus Atomen oder Molekülen Elektronen entfernen kann, wird die Strahlung als ionisierende Strahlung bezeichnet. Elektromagnetische Wellen in diesem Energiebereich geben oft beim ersten Stoß einen Großteil ihrer Energie ab. Für die Energie- und Materialabhängigkeit siehe Massenschwächungskoeffizient. Geladene Teilchen mit hoher Energie geben diese beim Durchgang durch Materie in vielen kleinen Portionen ab. Für die Energie- und Materialabhängigkeit siehe Bremsvermögen.

Bei verschiedenen Strahlenarten, z. B. Röntgen-, Gamma- und auch Betastrahlung, wird manchmal von „harter“ (hier gleichbedeutend mit energiereicher) oder „weicher“ (energieärmerer) Strahlung gesprochen. Genaue Abgrenzungen dieser Begriffe gibt es aber nicht. Von "Harter Röntgenstrahlung" wird beispielsweise in der Röntgenoptik gesprochen, wenn in etwa die Wellenlänge kürzer ist als der Abstand der Atome im Festkörper, also im Bereich 0,01 bis ca. 0,5 nm.





</doc>
<doc id="8946" url="https://de.wikipedia.org/wiki?curid=8946" title="Strahlentherapie">
Strahlentherapie

Strahlentherapie (auch Radiotherapie) ist die medizinische Anwendung von ionisierender Strahlung auf den Menschen und auf Tiere, um Krankheiten zu heilen oder deren Fortschreiten zu verzögern. Die Strahlung kann aus Geräten oder aus radioaktiven Präparaten stammen. Fachgebiete für diese spezielle Anwendung elektromagnetischer Wellen heißen Strahlenheilkunde und Radioonkologie.

Als Strahlen werden vorwiegend Gammastrahlung, Röntgenstrahlung und Elektronenstrahlung verwendet. In den letzten Jahren wurden auch Anlagen zur Behandlung mit Neutronen, Protonen und schweren Ionen (häufig Kohlenstoff-Ionen) errichtet. Nicht ionisierende Strahlen wie zum Beispiel Mikrowellen- und Wärmestrahlen, Licht- und UV-Therapie sowie die Behandlung mit Ultraschallwellen werden der Strahlentherapie nicht zugeordnet.

Strahlentherapie umfasst die Behandlung von gut- und bösartigen Erkrankungen. Sie wird von Fachärzten für Radiologie oder für Strahlentherapie unter Mitwirkung von medizinisch-technischen Assistenten und spezialisierten Medizinphysikern ausgeübt. Ihre Tätigkeit unterliegt der Strahlenschutzverordnung und nachgeordneten Normen. Der eigentlichen Therapie geht ein komplexer Planungsprozess – die Bestrahlungsplanung – voraus. Umfangreiche organisatorische und technische Qualitätssicherungsmaßnahmen sorgen dafür, dass Bestrahlungsfehler weitgehend ausgeschlossen werden können.

Nach den in Deutschland gültigen Weiterbildungsordnungen umfasst das Gebiet Strahlentherapie auch die medikamentösen und physikalischen Verfahren zur Radiosensibilisierung und Verstärkung der Strahlenwirkung am Tumor (Radioimmuntherapie und Radiochemotherapie), unter Berücksichtigung von Schutzmaßnahmen der gesunden Gewebe.

Die Strahlentherapie kennt Methoden zur "Teletherapie" (von ‚fern‘), wobei die Strahlung von außen auf den Körper des Patienten einwirkt, und die Brachytherapie (von griech. ‚nah‘, ‚kurz‘), bei der die Strahlenquelle sich im oder direkt am Körper befindet.

Bei der Teletherapie wird die erforderliche Strahlung normalerweise in Linearbeschleunigern erzeugt, die sowohl Elektronen- als auch Röntgenbremsstrahlung mit hohen Energien bis 23 MeV (= Mega-Elektronenvolt; für Photonen wird manchmal auch die zahlengleiche, theoretische Beschleunigungsspannung in MV angegeben) bereitstellen können.

Elektronen haben fast die gleiche biologische Wirksamkeit wie die Photonen der Röntgenstrahlung, jedoch eine andere Tiefendosisverteilung im Gewebe. Während die im Gewebe deponierte Dosis einer Photonenstrahlung mit zunehmender Tiefe langsam in Form einer Exponentialkurve abnimmt, haben Elektronen wegen ihrer elektrischen Ladung eine begrenzte mittlere Reichweite (Faustregel: Betrag in MeV/2 in cm). Sie eignen sich für oberflächliche Zielvolumina vor Risikoorganen, etwa für die Brustwand vor Herz und Lunge.

Das Dosismaximum einer Bestrahlung mit hochenergetischen Photonen oder Elektronen liegt nicht auf der Hautoberfläche, sondern einige Millimeter tiefer. Die Ursache dieses Dosisaufbaus ist, dass der eigentliche Dosisbeitrag überwiegend erst durch Sekundärelektronen stattfindet, die im durchstrahlten Material ausgelöst werden. So wird die Haut besser geschützt als bei Strahlung geringerer Energie. Ist dies nicht erwünscht, etwa bei der Behandlung eines Hauttumors, wird eine Schicht aus Plexiglas oder gewebeäquivalentem Weichplastik ("Bolusmaterial") auf die Haut aufgelegt.

Optimal für den Beschuss tieferliegender Organe sind energiereiche Protonen, die das Gewebe davor kaum und dahinter nur minimal schädigen.

Eine wesentliche Voraussetzung für die erfolgreiche Strahlentherapie ist die vollständige Erfassung der Tumormasse mit möglichst wenig umgebendem Normalgewebe. Früher wurden die Einstrahlrichtungen und Feldgrenzen nach klinischer Erfahrung mithilfe von Röntgenbildern festgelegt. Heute wird fast immer mithilfe einer Computertomographie, gelegentlich nach Bildfusion mit MR- oder PET-Daten, ein dreidimensionales Modell der Bestrahlungsregion im Planungsrechner erstellt und für die Bestrahlungsplanung zugrundegelegt. Die Tumormasse und Risikoorgane werden meist manuell oder neuerdings auch halbautomatisch segmentiert. Die Strahleigenschaften des Beschleunigers sind dem System aufgrund von aufwendigen Messreihen bekannt. Zur Berechnung der erwarteten Dosisverteilung einer bestimmten Feldanordnung dienen Summations- und Nadelstrahl-Algorithmen und Monte-Carlo-Simulationen. Moderne Planungssysteme können umgekehrt aus einer gegebenen Dosisverteilung mögliche Feldanordnungen berechnen ("inverse Planung").

Die vom Medizinphysiker berechnete und vom Arzt ausgewählte Feldanordnung wird auf den Patienten übertragen. Dies geschieht entweder an einer dem Bestrahlungsgerät ähnlich dimensionierten Röntgendurchleuchtungsanlage ("Therapiesimulator"), oder neuerdings vermehrt am CT-Gerät. Bei der „virtuellen Simulation“ im CT werden nacheinander die Mittel- und Eckpunkte der Einzelfelder eingestellt und auf der Haut markiert. Am CT muss zu diesem Zweck ein geeignetes Laser-Koordinatensystem installiert sein. Die Feldränder werden mit individuell gegossenen Bleiblöcken geformt, die für Photonen ca. 5 cm dick sein müssen, für Elektronen 0,5 cm (siehe Abschirmung).

Bei der echten Bestrahlung werden die Lagerung des Patienten und die Feldparameter aus der Simulation mithilfe von Lichtvisieren und Lasermarkierungen millimetergenau reproduziert. Die Einstellungsgenauigkeit wird überprüft, indem ein Röntgenfilm (oder ein elektronischer Bilddetektor; sog. "portal imaging system") hinter dem Patienten in den Strahlenweg gebracht wird. Das so erzeugte Bild wird mit den Aufnahmen aus der Simulation verglichen ("Verifikation"). So kann die Position des bestrahlten Zielvolumens selbst auf dem Bestrahlungstisch nochmals überprüft und nötigenfalls korrigiert werden (sog. "image-guided radiotherapy", IGRT). Mit der „harten“ Photonenstrahlung lassen sich allerdings nur relativ kontrastarme Bilder erzeugen. Darum ist in manchen Beschleunigern zusätzlich eine Röntgendurchleuchtung integriert, mit der auch das Weichteilgewebe der Umgebung in der Deutlichkeit eines Röntgenbilds dargestellt werden kann. Moderne Geräte können auch Schnittbilder erzeugen, die sich direkt mit den CT-Bildern der Simulation vergleichen lassen.

Standardverfahren der Teletherapie ist heute die "3-dimensional konformale Strahlentherapie", bei dem die zu behandelnde Körperregion in den Überschneidungsbereich der Achsen mehrerer Strahlen platziert wird, die aus unterschiedlichen Richtungen einwirken ("Isozentrum"), und durch individuell geformte Bleiblenden oder Anpassung der beschleunigerseitig vorhandenen Blenden des Multileafkollimators (MLC) der Zielkontur angepasst sind. Die Felder können noch mit Keilfiltern moduliert werden, um unterschiedliche durchstrahlte Gewebsdicken auszugleichen. Wenn die Einstrahlrichtungen aller Teilfelder auf einer gemeinsamen Ebene liegen (typischerweise eine Schnittebene quer zur Patientenlängsachse), spricht man von "koplanarer", sonst von "non-koplanarer" Planung. Moderne Behandlungspläne nutzen mehrere Volumendefinitionen (Zielvolumen erster und zweiter Ordnung), die unterschiedlich intensiv bestrahlt werden. Diese Techniken sind international genormt, zum Beispiel im ICRU-Report 50.

Ein neueres Verfahren, das zunehmend zur Anwendung kommt, ist die "intensitätsmodulierte Strahlentherapie" (IMRT), bei der nicht nur die Feldbegrenzung, sondern auch die Strahlendosis innerhalb der Feldfläche moduliert wird. Das geschieht durch metallische Ausgleichskörper ("Kompensatoren") im Strahlenweg mit individuell gegossenem Profil für jedes der 6–8 Felder, oder durch zeitgesteuerte Verschiebung beweglicher Multileaf-Blenden während der Bestrahlung. Letzteres Verfahren ist schneller und billiger, die Genauigkeit der Feldformung auf die Breite der einzelnen Lamellen des Multileaf-Kollimators (4–5 mm) ist jedoch begrenzt. Die IMRT erlaubt sehr kompliziert geformte, selbst konkav begrenzte Zielvolumina und eignet sich daher für Tumoren in unmittelbarer Nähe von sensiblen Risikoorganen. Eine IMRT ist sehr zeitaufwendig zu berechnen, auszuführen und zu kontrollieren.

Als Tomotherapie wird ein radiologisches Verfahren bezeichnet, bei dem ähnlich wie in einem Computertomografen die Strahlen von allen Seiten auf die zu bestrahlende Stelle gerichtet werden können. Zu diesem Zweck rotiert die Strahlenquelle in einem entsprechenden Ring (siehe Abbildung). Die gewünschte Präzision in der Ausrichtung der Bestrahlung wird dadurch erreicht, dass ein mit dem Tomotherapie-Gerät kombinierter Computertomograf verwendet wird, um die genaue Lokalisation eines zu bestrahlenden Tumors regelmäßig neu zu bestimmen. Die mit der Bestrahlung von Tumorpatienten oft einhergehenden Nebenwirkungen sollen dadurch verringert werden. Die Behandlungsmethode wurde 2003 erstmals klinisch eingesetzt. Sie basiert auf Entwicklungen an der Universität Wisconsin (USA). Hauptanwendungsgebiete der Tomotherapie sind bösartige Neubildungen wie Prostatakrebs, Lungenkrebs, Brustkrebs und Kopf-Hals-Karzinome.

Ebenso eine Weiterentwicklung der IMRT ist die Volumetric Intensity Modulated Arc Therapy (VMAT). Hierbei wird das Strahlenfeld während der Rotation der Strahlenquelle um den Patienten moduliert; der Multileaf-Kollimator wird bei eingeschalteter Strahlung kontinuierlich angepasst. Neben der Feldform können auch Rotationsgeschwindigkeit, Kollimatorwinkel und Dosisleistung variiert werden. Das Verfahren erlaubt einen hohen Modulierungsgrad der Strahlung. Die Gesamtapplikationszeit ist erheblich kürzer als bei der IMRT.

Die Wirkung der Bestrahlung beruht auf der Energieübertragung auf das durchstrahlte Gewebe in Streuprozessen. Dabei sind direkte Treffer an für das Zellwachstum wesentlichen Biomolekülen weniger bedeutsam als die Ionisierung von Wassermolekülen. Die entstehenden freien Radikale sind hochtoxisch und reagieren chemisch mit Zellbestandteilen. Die daraus resultierenden Schäden an der Erbsubstanz der Tumorzellen, insbesondere DNA-Doppelstrangbrüche, sind für die zerstörende Wirkung verantwortlich. Schäden, die die Reparaturfähigkeit der Tumorzelle überschreiten, hindern sie an der Vermehrung (Mitose) oder bringen sie sogar unmittelbar zur Apoptose. Da zur Wirkung mehrere Treffer in enger räumlicher und zeitlicher Nähe eintreten müssen, ist die Dosis-Wirkungs-Beziehung einer beliebigen Gewebsreaktion immer sigmoid (S-förmig) mit zuerst langsamem, dann schnellerem Anstieg und zuletzt Sättigung. Die Normalgewebe zeigen etwas geringere Wirkung als der Tumor, das heißt ihre S-Kurve liegt weiter rechts im höheren Dosisbereich. Die optimale Strahlendosis erreicht im Durchschnitt >90 % Tumorvernichtung bei <5 % schweren Nebenwirkungen.

Tumorzellen haben in der Regel eine schlechtere Reparaturfähigkeit für DNA-Schäden als normale Zellen. Diesen Unterschied nutzt man aus, indem die Dosisleistung verringert ("Protrahierung", wird heute kaum noch verwendet) oder die Gesamtdosis auf tägliche kleine Einzeldosen (1,8–2,5 Gy) verteilt ("Fraktionierung") wird. Damit verringert sich die mit der gleichen Dosis abgetötete Zellzahl. Die maximal tolerierte Gesamtdosis des Normalgewebes (ca. 10 Gy bei kleinem Volumen) kann so auf ein Vielfaches gesteigert werden; nur fraktionierte Schemata erreichen tumorizide Herddosen bis 80 Gy. Die biologische Wirkung verschiedener Fraktionierungsschemata kann mit dem linearquadratischen Modell berechnet werden.

Neutronenstrahlen haben keinen wesentlichen Fraktionierungseffekt, die aufgeteilte Dosis ist ebenso stark wirksam wie die einzeitig eingestrahlte. Das liegt an ihrer sehr hohen Energieabgabe auf kurzer Laufstrecke; ein einziges Teilchen kann eine Vielzahl von Doppelstrangbrüchen innerhalb eines Zellkernes verursachen und damit die Reparaturkapazität der Zelle überschreiten. Man hat versucht, Neutronenstrahlen gegen relativ strahlenunempfindliche Tumoren wie etwa der Prostata oder der Speicheldrüsen einzusetzen. Geladene schwere Teilchen scheinen aber noch bessere physikalische Eigenschaften zu haben (s. u.).

Da der Tumor während der Behandlung weiter wächst und radioresistente Zellklone selektioniert, ist die Heilungschance umso größer, je kürzer die Gesamtbehandlungszeit (2–7 Wochen) ist. Das gilt vor allem für schnellwachsende Tumorarten, etwa Rachenkrebs. Man kann die Gesamtbehandlungszeit verkürzen, indem man zwei oder sogar drei Fraktionen pro Tag einstrahlt ("Akzelerierung"). Dies erhöht jedoch die Nebenwirkungen am Normalgewebe erheblich.

Strahlentherapie und Chemotherapie verstärken sich gegenseitig in der Wirkung. Viele potentiell heilbare Tumoren bei Patienten in gutem Allgemeinzustand werden deshalb mit beiden Verfahren simultan (gleichzeitig) oder sequentiell behandelt, was als Radiochemotherapie bezeichnet wird. Für Lungenkrebs, Darmkrebs, Gebärmutterhalskrebs und Tumoren der Halsregion wurde nachgewiesen, dass die simultane Radiochemotherapie anderen Therapievarianten überlegen ist. Wichtige Zytostatika für den Strahlentherapeuten sind 5-Fluoruracil (5-FU) und Cisplatin. Allerdings sind die kombinierten Schemata auch mit stärkeren Nebenwirkungen belastet.

Substanzen, die die Resistenz der Normalgewebe erhöhen sollen, nennt man Radioprotektoren. Amifostin ist der erste und bisher einzige zugelassene Radioprotektor.

Schlecht durchblutete, sauerstoffarme Tumoren sind meist strahlenresistent. Umgekehrt kann man beobachten, dass solche Gewebe gegenüber therapeutischer Überwärmung besonders empfindlich sind. Die Kombination der Strahlentherapie mit Hyperthermie hat also theoretische Vorteile. Kleinere Studien bestätigen bessere Heilungschancen bei verschiedenen Tumoren, zum Beispiel schwarzem Hautkrebs, Sarkomen, und rezidiviertem Gebärmutterhalskrebs. Die Daten sind jedoch noch unsicher, auch wegen der unübersichtlichen Vielfalt von Hyperthermietechniken, sodass die Methode noch keine allgemeine Anerkennung gefunden hat.

Will man eine extrem kurze Behandlungszeit und trotzdem abtötende Dosen am Tumor, so ist das in ausgewählten Fällen mit der Radiochirurgie (Syn. "stereotaktische Radiochirurgie") möglich. Diese Methode ist praktisch nur bei kleineren Hirntumoren möglich. Der Kopf des Patienten wird während der Behandlung mit einem stereotaktischen Ring fest verschraubt. Neuere Geräte fixieren den Patienten schmerzfrei mit einer enganliegenden Maske. Geeignete Spezialanlagen für die Radiochirurgie sind das Gamma-Knife und vollautomatisierte Linearbeschleuniger, die Industrierobotern ähneln, wie beispielsweise Cyberknife oder "Novalis". Die Dosis beträgt 12–18 Gy.

Im Hinblick auf Schonung des umliegenden Gewebes ergibt eine Bestrahlung mit Protonen oder noch schwereren Teilchen einen im Vergleich zu Photonen oft günstigeren Tiefendosis-Verlauf. Anlagen zur Bestrahlung mit Protonen, Neutronen sowie Schwerionen sind in Betrieb. Leider sind die Erwerbs- und Betriebskosten solcher Anlagen sehr viel höher als bei konventionellen Elektronen-Linearbeschleunigern, bei denen der Elektronenstrahl oder die mit ihm erzeugte Röntgenstrahlung verwendet wird.

Anlagen mit Neutronen- und Protonenquellen zur Partikeltherapie sind in einigen großen Forschungszentren verfügbar, in Villigen (Schweiz) und in Deutschland in Berlin (Helmholtz-Zentrum Berlin für Materialien und Energie, ehemals Hahn-Meitner-Institut, nur Augenbestrahlung). Seit März 2009 gibt es eine klinische Einrichtung zur Protonenbestrahlung, das Rinecker Proton Therapy Center (RPTC), in München. In Essen wurden seit Mai 2013 rund 1.000 Patienten am Westdeutschen Protonentherapiezentrum Essen (WPE) behandelt. Im April 2014 wurde das WPE vollständig durch das Universitätsklinikum Essen übernommen. Seit dem Frühjahr 2016 sind alle 4 Behandlungsräume in Betrieb. Drei Räume sind mit sogenannten Gantries ausgestattet, bei denen die Strahlführung um 360 Grad gedreht werden kann, der vierte Behandlungsraum ist mit einer horizontalen Strahlführung (Fixed-Beam-Line) und einem Augentherapieplatz ausgerüstet. Hier können Tumoren mit der pencil beam scanning-Technologie bekämpft werden. Zukünftig wollen die Ärzte dort bis zu 1.000 Patienten pro Jahr behandeln.

Anlagen zur Schwerionentherapie gibt es in drei Zentren in Japan (Chiba, Gunda und Kyōto). In Deutschland waren im Rahmen eines Pilotprojektes von 1997 bis 2008 Patienten bei der GSI in Darmstadt behandelt worden. In der Nachfolge ging das Heidelberger Ionenstrahl-Therapiezentrum (HIT) am Universitätsklinikum Heidelberg 2009 in Betrieb. Dort können Patienten sowohl mit Protonen als auch Kohlenstoffionen in der Rasterscanning-Technik behandelt werden. In Kiel und Marburg waren Zentren für Partikeltherapie mit Protonen und Kohlenstoffionen im Bau. Das Zentrum in Kiel sollte seinen Betrieb 2012 aufnehmen. Die Rhön-Klinikum AG als Betreiberin des Universitätsklinikums Gießen/Marburg und das Uniklinikum Kiel hatten sich 2011 aus dem Aufbau und Betrieb von Ionenstrahl-Therapieanlagen verabschiedet. Im September 2014 einigten sich das Universitätsklinikum Heidelberg mit dem Land Hessen, der Rhön Klinikum AG, den Universitäten Marburg und Heidelberg, dem Universitätsklinikum Marburg sowie der Siemens AG vertraglich schließlich doch auf die Inbetriebnahme der Partikeltherapieanlage am Standort Marburg. Am Marburger Ionenstrahl-Therapiezentrum (MIT) wurden am 27. Oktober 2015 die ersten beiden Patienten mit Protonenstrahlen behandelt.

Schwere elektrisch geladene Teilchen, d. h. Schwerionen und Protonen, zeigen im Vergleich zur konventionell eingesetzten Photonenstrahlung eine viel dichtere Energieabgabe an das durchstrahlte Gewebe (Linearer Energietransfer LET), was als Hoch-LET-Effekt bezeichnet wird. Infolgedessen sind die verursachten Schäden an der DNA gravierender, ihre Reparatur schwieriger für die Zelle und der therapeutische Effekt ist größer. Hoch-LET-Strahlung hat noch weitere biologische Vorteile: Sie wirkt auch bei schlecht durchbluteten, langsam wachsenden Tumoren, die sich gegenüber konventioneller Bestrahlung als sehr resistent erweisen. Dieser Effekt ist aber bei Schwerionen lokal begrenzt und kann dem Tumor angepasst werden, während er sich bei Neutronen auf der gesamten Länge der Teilchenbahn zeigt, also unerwünschterweise auch das vor dem Tumor liegende gesunde Gewebe betrifft. Bei Neutronen wird die Strahlbegrenzung außerdem mit wachsender Tiefe immer stärker verwischt, und es kommt zu einem exponentiellen Abfall der Strahlendosis. Aufgrund dieses Tiefendosisverlaufs ist die Dosis im gesunden Gewebe vor dem Tumor höher als im Tumor selbst. Nach Neutronenstrahlung wurden erhöhte Nebenwirkungsraten beschrieben.

Schwerionen und Protonen haben - im Gegensatz zu Neutronen - eine definierte, scharf begrenzte Reichweite, so dass Gewebe hinter dem zu bestrahlenden Tumor vollständig geschont werden kann. Sie geben ihre Energie an die Materie zunächst nur geringfügig und erst nach fast vollständiger Abbremsung konzentriert ab (sogenannter Bragg-Peak); dies erlaubt es, durch geeignete Wahl der Ionenenergie auch "vor" dem Tumor gelegenes Gewebe zu schonen (siehe auch Teilchenstrahlung). Protonen- und Schwerionenstrahlung ist bei Tumoren indiziert, bei denen mit der konventionellen Strahlentherapie keine zufriedenstellenden Erfolge erzielt werden. Derzeit wird in klinischen Studien untersucht, welche Krebspatienten von dieser Therapie profitieren. Eine Protonenbestrahlung ist bei kompliziert lokalisierten, also nahe strahlensensiblem Normalgewebe gelegenen Tumoren indiziert. Der Einsatz von Schwerionen bietet sich an, wenn der Tumor darüber hinaus gegenüber konventioneller Strahlung vergleichsweise resistent ist. Schwerionen vereinen also die Vorteile der höheren biologischen Effektivität und der größeren physikalische Selektivität bei gleichzeitig geringer Nebenwirkungsrate.

Eine Protonen- und Schwerionentherapie ist zurzeit ausgewählten Krebserkrankungen vorbehalten, darunter beispielsweise Chordome und Chondrosarkome der Schädelbasis und des Beckens, (adenoidzystische) Speicheldrüsenkarzinome, Prostatakarzinome u. a.

Moderne Verfahren der Brachytherapie umfassen Afterloadingverfahren und Implantationen. "Afterloading" (eng. für „Nachladen“) nennt man ein Verfahren, bei dem eine kleine radioaktive Strahlenquelle (Ir-192) ferngesteuert in eine Körperöffnung verbracht und nach vorausberechneter Zeit wieder entfernt wird (Dekorporation). Diese Methode erlaubt einerseits die direkte Bestrahlung zum Beispiel der Gebärmutter und andererseits bestmöglichen Strahlenschutz für das behandelnde Personal. Gegenüber den älteren Radiumstrahlern hat Iridium-192 allerdings den strahlenbiologischen Nachteil der hohen Dosisleistung, die nur begrenzt durch Fraktionieren der Bestrahlung ausgeglichen werden kann. Der Dosisabstand zwischen erwünschten und unerwünschten Effekten ist kleiner als beim Radium. Afterloading ist besonders für weibliche Unterleibstumoren geeignet. Es gibt zahlreiche Trägersysteme für andere Zielorgane wie etwa die Luft- und Speiseröhren; auch Hohlnadeln zum Spicken solider Gewebe ("interstitielles Afterloading") sind verfügbar. Afterloading-Therapie wird in Strahlenschutzbauwerken ähnlich denen für die Teletherapie durchgeführt. Enge Zusammenarbeit mit den Gynäkologen, Internisten und Chirurgen ist zum Erfolg der handwerklich komplizierten Methode unabdingbar.

Bei den Implantationsverfahren werden kleine, gekapselte Strahlungsquellen (eng. "seeds") mit kurzer Halbwertszeit in den Körper verbracht und verbleiben dort dauerhaft, während ihre Aktivität abklingt. Eine typische Anwendung sind Seed-Implantationen der Prostata mit Iod-125 (59 Tage Halbwertszeit). I-125 gibt sehr schwachenergetische Gammastrahlen ab, die den Körper des Patienten nicht verlassen. Es ist daher kein spezieller Operationssaal und keine Quarantäne erforderlich.

Offene Radionuklide fallen prinzipiell in das Fachgebiet der Nuklearmedizin, die mit der Radiojodtherapie und ähnlichen Methoden der Radionuklidtherapie große Erfahrung hat. Einzelne Substanzen ergänzen jedoch auch in der Hand des Strahlentherapeuten das Therapiespektrum; beispielsweise wirkt eine Injektion mit radioaktivem Strontium-89 effektiv gegen die Schmerzen einer fortgeschrittenen Knochenmetastasierung. Noch nicht abschließend bewertet ist der Ansatz, tumorspezifische Antikörper mit radioaktiven Substanzen zu koppeln, die Betastrahlung im Nahbereich emittieren (Radioimmuntherapie). Allerdings hat die Anwendung von radioaktiv markiertem Ibritumomab (monoklonarer chimärer Antikörper gegen CD20) mit einem Yttrium-Isotop (Ibritumomab-Tiuxetan) bereits Einzug in die klinische Praxis erhalten.

Strahlentherapeuten sind dazu verpflichtet, sich nach Abschluss einer Strahlentherapie über die eingetretenen Wirkungen und Nebenwirkungen zu informieren. Gewöhnlich wird der Patient zu Nachsorgeuntersuchungen einbestellt, etwa nach sechs Wochen und nochmals nach einem Jahr. Prinzipiell sollte der Strahlentherapeut den Patienten über mehrere Jahre wegen der oft erst spät eintretenden Nebenwirkungen nachsorgen.

Unabhängig davon werden nach der Behandlung einer Krebserkrankung routinemäßige Nachsorgeuntersuchungen beim Hausarzt, Frauenarzt usw. empfohlen.

Am 28. Dezember 1895 versandte Wilhelm Conrad Röntgen die erste seiner drei Mitteilungen über "eine neue Art von Strahlen". Die Ärzteschaft nahm die Entdeckung begeistert auf; noch 1896 wurden in ganz Europa und in den USA Hunderte von Röntgenapparaten in Betrieb genommen. Der erste Hersteller, der Röntgens Idee aufgegriffen hatte, die Erlanger Vereinigten Physikalisch-Mechanischen Werkstätten Reiniger, Gebbert & Schall, ging 1925 im Siemens-Konzern auf. In den USA wurde General Electric zum größten einschlägigen Hersteller.

Schnell wurde erkannt, dass die Strahlen Hautentzündungen und Haarausfall verursachen. Erst in den Folgejahren gelangten die schwerwiegenden Strahlenschäden bis hin zum Tod vieler Röntgenärzte in das Bewusstsein der Anwender. Erst 1904 schrieb der Bostoner Zahnarzt William Herbert Rollins das weltweit erste Buch über Strahlenfolgen. Zunächst war man über die Wirkung erfreut und versuchte sie therapeutisch zu nutzen.

Am 6. März 1897 veröffentlichte der Österreicher Leopold Freund in der "Wiener Medizinischen Wochenschrift" einen Artikel mit dem Titel "Ein mit Röntgen-Strahlen behandelter Fall von Naevus pigmentosus piliferus" (Tierfell-Muttermal). Die Behandlung eines fünfjährigen Mädchens war der erste beschriebene Fall, bei dem Röntgenstrahlen zu Heilzwecken angewendet wurden. Im Jahr 1903 gab Freund das erste Lehrbuch der Strahlentherapie heraus: "Grundriss der gesamten Radiotherapie für praktische Ärzte". Wichtige Pioniere der Strahlentherapie waren Friedrich Dessauer und Hans Holfelder.

Neben diagnostischen Geräten entwickelten die Ingenieure bald spezielle Therapieröhren und -generatoren. Ein wichtiger Meilenstein war die von William David Coolidge erfundene Hochleistungsröhre. In Erlangen wurde 1925 eine Anlage vorgestellt, die es erlaubte, die Röntgenröhre um den Patienten herumzuschwenken und das Ziel aus mehreren Richtungen zu bestrahlen. Diese sogenannte „Kreuzfeuerbestrahlung“ war der Vorläufer der modernen Konformaltherapie. Nach dem Zweiten Weltkrieg ersetzten radioaktive Strahler mit höherer Leistung und Maximalenergie fast alle Therapieröhren. Nur für die Behandlung von oberflächlichen Hauttumoren werden gelegentlich noch Röntgenstrahler eingesetzt ("Grenzstrahlen-" und "Weichstrahlengeräte").
Fast gleichzeitig mit der Entwicklung von Röntgenstrahlern war die Entdeckung und technische Nutzung der natürlichen Radioaktivität einhergegangen, aufbauend auf der Entdeckung des Radiums durch Marie und Pierre Curie 1898. Die von Radium abgegebene Strahlung ist viel energiereicher als Röntgenstrahlung. Der Gammastrahlenanteil kann dabei sehr tief in den Körper eindringen. Radium lässt sich zudem industriell herstellen und konfektionieren. Die Radiumstrahler benötigen keine Stromquelle und zerfallen extrem langsam. Sie eignen sich besonders zur Brachytherapie in Körperhöhlen. Daher richteten viele Kliniken nach dem Muster des 1910 in Stockholm gegründeten Radiumhemmet Strahlentherapieeinheiten ein (Stockholmer Methode), vorzugsweise innerhalb der Gynäkologie. 1949 wurde in einem Demonstrationsfilm die an der Frauenklinik der Universität Göttingen entwickelte „Göttinger Methode“ vorgestellt, eine Kleinraumbestrahlung mit Radium im „Siemens-Körperhöhlenrohr“. Sie gilt als einer der Vorläufer des heute üblichen Afterloadings.
1941 wurde unter Leitung von Enrico Fermi der erste Atomreaktor der Welt kritisch und hielt eine Kettenreaktion selbständig aufrecht. In diesen Reaktoren ist die Herstellung von künstlichen Radionukliden möglich, die gegenüber dem Radium geeignetere physikalische Eigenschaften haben, vor allem eine höhere Dosisleistung pro Masseeinheit. In der Teletherapie wurden die Röntgenröhren bis 1960 überall durch Strahlenkanonen mit Quellen aus radioaktivem Cobalt-60 oder Cäsium-137 ersetzt. Wegen der damit verbundenen Strahlenschutzprobleme gab es schon 1954 die ersten Versuche, elektrisch betriebene Teilchenbeschleuniger für die Therapie zu modifizieren, angefangen mit einem großen Van-de-Graaff-Beschleuniger in Berkeley, später vorwiegend mit beweglich konstruierten Betatrons. Diese Anlagen waren jedoch sehr teuer und aufwendig bei schwacher Dosisleistung, sodass die "Telecurie"-Geräte (sogenannte Kobaltkanonen) in den meisten Kliniken weiter genutzt wurden. Trotz strenger Strahlenschutzvorschriften für den Erwerb, die Nutzung und die Entsorgung kam es in der Vergangenheit zu folgenschweren Unfällen durch die illegale Entsorgung ausgedienter Strahlenquellen. Wegen ihrer überlegenen technischen Eigenschaften und im Hinblick auf solche Risiken ersetzten schließlich die Linearbeschleuniger, die seit ca. 1970 verfügbar waren, die Cobalt- und Cäsiumstrahler in der Routinetherapie. In Deutschland sind Anfang der 2000er Jahre die letzten Kobaltkanonen außer Betrieb gegangen. Linearbeschleuniger dürfen im Gegensatz zu den Röntgen- und Telecurieanlagen nur in Anwesenheit eines Medizinphysikers benutzt werden, dem auch die technische Qualitätskontrolle obliegt. Die effektiv auf den Patienten abgegebene Dosis ist sehr vielen Einflüssen und Fehlermöglichkeiten unterworfen, sodass nur durch peinlich genaue Überwachung der Maschine schwere Unfälle wie der Therac-25-Fehler 1986 in Zukunft verhindert werden können. Neue Verfahren wie die IMRT ergeben weitere Risiken, die in ihrer Komplexität und Unanschaulichkeit begründet sind.

In der Brachytherapie wurde Radium noch lange weiterverwendet. Dabei mussten die etwa bohnengroßen Radiumpatronen von Hand eingesetzt, nach 1 bis 2 Tagen wieder entfernt und gereinigt werden. Wegen der hohen Strahlenbelastung des Personals und der Gefahr versehentlicher Kontaminationen wurden die europäischen Radiumstationen bis 1990 nach und nach durch Afterloading-Systeme ersetzt, die mit dem künstlichen und intensiv strahlenden Isotop Iridium-192 bestückt sind.

Zahlreiche chronisch-entzündliche und degenerative Erkrankungen wie Fersensporn, Tennisellbogen, Schulterschmerzen, Arthrosen der verschiedenen Gelenke, Wirbelkörperhämangiome, Induratio penis plastica und andere können strahlentherapeutisch behandelt werden. Diese sogenannte "Reizbestrahlung" weit unterhalb der gewebeabtötenden Dosis ist mit Ausnahme des stochastischen Risikos nebenwirkungsfrei. Die Ansprechraten liegen bei 50 bis 70 %. Die verwendeten Gesamtdosen liegen im Bereich von 1 bis 20 Gray und sind somit deutlich niedriger als die Dosen, die bei der Therapie von Krebserkrankungen (20 bis 80 Gy) verwendet werden müssen. Niedrigdosierte Strahlung reduziert die Aktivität von Leukozyten und Vorläuferzellen des Bindegewebes und mindert die Ausschüttung von Zytokinen; damit hemmt sie akute und chronische Entzündungsprozesse. Direkte Hemmung der Schmerzrezeptoren wird ebenfalls vermutet. Insbesondere Schulterschmerzen und Fersensporne sind der Strahlentherapie gut zugänglich. In Deutschland werden pro Jahr etwa 37.000 Patienten mit nicht-malignen Erkrankungen bestrahlt, mit steigender Tendenz.

Bösartige Tumoren werden sehr häufig bestrahlt; oft auch in Kombination mit anderen Behandlungsverfahren wie Operation und Chemotherapie. Etwa jeder zweite Krebskranke erhält eine oder mehrere Strahlentherapien. Etwa gleich häufig sind palliative Bestrahlungen, etwa von Knochenmetastasen, und kurative, das heißt in Heilungsabsicht angewendete Behandlungsserien. "Neoadjuvante" Strahlentherapie soll den Tumor für eine nachfolgende Operation verkleinern; "adjuvante" Strahlentherapie soll das Ergebnis einer vorangegangenen Operation sichern und mikroskopische Tumornester vernichten. Im onkologischen "Log cell kill"-Konzept kann von einer korrekt dosierten Strahlentherapie eine Reduktion der Tumorzellzahl um den Faktor 1000 erwartet werden.

In allen Fällen nutzt man die Tatsache aus, dass Tumorgewebe strahlenempfindlicher ist als das umgebende Normalgewebe. Die Heilwirkung erfordert Fraktionierung und hohe Gesamtdosen von 20 bis 80 Gray je nach Art des Tumors. Symptomlindernde Behandlungen für unheilbare Patienten können kürzer sein; beispielsweise können Knochenherde mit einmalig 8 Gy schmerzlindernd behandelt werden.

Mit modernen Strahlentherapieverfahren können heutzutage in stadienabhängiger Kombination mit Chirurgie und Chemotherapie eine Vielzahl von Tumorerkrankungen auch in fortgeschrittenen Stadien geheilt werden. Über alle Tumorarten und Stadien gemittelt beträgt die Heilungschance ca. 50 %. Einzelne Tumoren wie etwa der Morbus Hodgkin und das Seminom des Hodens können fast immer geheilt werden. Die häufigsten Indikationen zur Strahlentherapie sind zurzeit Prostatakrebs, adjuvant nach Brustkrebs-Operationen, und beim Enddarmkrebs. Ein besonderer Vorteil ist die Tatsache, dass durch eine Strahlentherapie ein Organerhalt auch in Situationen, bei denen die Erkrankung schon relativ weit fortgeschritten ist, möglich bleibt. Hier kann vor allem die Kombination von Strahlentherapie mit einer Chemotherapie bei Krebserkrankungen des Kehlkopfs angeführt werden. Bei anderen Tumorerkrankungen, wie zum Beispiel dem Prostatakarzinom, stehen operative Verfahren und strahlentherapeutische Verfahren zueinander in Konkurrenz und können vergleichbare Ergebnisse haben. Hier ist es die Aufgabe des beratenden Arztes, dem Patienten alle Vor- und Nachteile der jeweiligen Verfahren zu erläutern. Erstrebenswert sind in diesem Zusammenhang zertifizierte Tumorzentren, in denen alle Fachdisziplinen vertreten sind und die dem Patienten damit eine umfassende Beratung ermöglichen.

Einige Tumoren weisen jedoch eine nur geringe Strahlungsempfindlichkeit auf. Die Ursache hierfür ist ein Sauerstoffmangel (Hypoxie) im Tumorgewebe. Verschiedene Therapiekonzepte versuchen mit Hilfe von sogenannten Radiosensitizern (Strahlungssensibilisatoren), die Strahlungsempfindlichkeit dieser Tumoren zu erhöhen.

Sehr selten kann es zu einem abscopalen Effekt kommen, bei dem eine Tumorregression bis hin zur kompletten Remission auch an Stellen zu verzeichnen ist, die nicht bestrahlt wurden. Dieser Effekt wurde 1953 erstmals beschrieben und bisher nur in Einzelfällen berichtet, so bei Leukämie, Lymphom, Nierenzellkarzinom und malignem Melanom.

Der nach wie vor bestehenden Angst vor der Strahlentherapie setzt die Deutsche Krebshilfe Sachaufklärung mit der kostenfreien Informationsreihe Die Blauen Ratgeber entgegen. Dazu gehört ferner die erste DVD "Strahlentherapie". Der Patienten-Informationsfilm aus der Reihe „Die blaue DVD“ wird von der Deutschen Krebshilfe kostenlos abgegeben und klärt auch darüber auf, dass die Strahlentherapie ein Standard in der modernen Krebsbehandlung ist.

Außerdem hält der Krebsinformationsdienst (KID) des Deutschen Krebsforschungszentrums (DKFZ) auf seiner Website, telefonisch oder via E-Mail zahlreiche Informationen rund um das Thema Krebserkrankungen und -therapien bereit, darunter auch ausführliche Informationen zum Thema "Strahlentherapie und Nuklearmedizin".

Einige Nebenwirkungen ("Frühreaktionen") treten in Abhängigkeit von der Dosis, Eindringtiefe und Anzahl der applizierten Einzeldosen auf: Hautrötungen im Bestrahlungsfeld und Schleimhautentzündungen im Mund-Rachen-Bereich oder der Speiseröhre, wenn die Kopf-Hals-Region bestrahlt wird. Völlegefühl, Übelkeit oder Durchfälle sowie Blasenbeschwerden treten bei Bestrahlungen im Bauchbereich auf. Mit Haarausfall ist nur zu rechnen, wenn der Kopf bestrahlt wird.

Nebenreaktionen sind im Allgemeinen bezogen auf sogenannte Risikoorgane. Jedes Risikoorgan hat seine eigene Toleranzdosis (in Gray), ab der Nebenwirkungen zu erwarten sind. Diese Toleranzdosen dürfen nicht überschritten werden.
Diese Toleranzdosen ergeben sich aus der Strahlensensibilität des Gewebes, sowie dessen Regenerationsfähigkeit, und ob das gesamte Organ oder nur ein Teil bestrahlt wird. Seriell aufgebaute Organe wie der Dünndarm sind besonders kritisch, weil der Ausfall eines kleinen Teilsegments die Funktion des gesamten Organs gefährdet. Äußerst sensibel sind auch Gewebe mit streng hierarchischem Aufbau, die sich beständig aus einer kleinen Population teilungsfähiger Stammzellen regenerieren, beispielsweise Schleimhaut oder Knochenmark. Die Frühschäden werden nach der weltweit gültigen CTC-Klassifizierung ("common toxicity criteria") quantifiziert.

"Spätreaktionen" nach mehr als drei Monaten beruhen auf Gefäßverengungen und Fibrosierung (Narbenbildung) im Bindegewebe. Häufig sind Verfärbungen der Haut, Verhärtungen im Unterhautfettgewebe, Mundtrockenheit (Xerostomie) durch Schädigung der Speicheldrüsen, Geschmacksverlust, Knochen- und Zahnschäden, Lungenfibrose, und andere. Bei Bestrahlungen im Beckenbereich ist mit Infertilität zu rechnen. Spätfolgen werden in die Schweregrade 0-5 der LENT-SOMA-Klassifizierung ("late effects on normal tissues, in subjective, objective, management and analytic categories") eingeordnet. Zur Verminderung von Strahlenschäden an den Mundschleimhäuten können Strahlenschutzschienen eingesetzt werden.

Das Risiko einer koronaren Herzkrankheit ist nach einer Bestrahlung im Rahmen der Therapie eines Brustkrebses deutlich erhöht, abhängig von der auf das Herz einwirkenden genäherten Organ-Gesamtdosis um etwa 7,4 % pro Gray. Das Risiko steigt beginnend nach fünf Jahren bis über zwanzig Jahre kontinuierlich an, ohne dass ein Schwellenwert besteht und unabhängig von weiteren kardiologischen Risikofaktoren.

Während die frühen Strahlenreaktionen sich vollständig zurückbilden, bleiben die Spätreaktionen lebenslang erhalten. Vorbestrahlte Organe sind sehr empfindlich und neigen bei weiteren Dosiseinträgen zu schweren Nebenwirkungen bis hin zur Strahlennekrose beziehungsweise im Fall des Skelettsystems zur Osteoradionekrose. Strahlentherapeutische Faustregeln ermöglichen einige Jahre nach einer Behandlung eine Wiederholung mit reduzierter Dosis.

Stochastische Strahlenrisiken treten nicht zwangsläufig auf und haben keine Dosis-Wirkungsbeziehung, sondern eine dosisabhängige Eintrittswahrscheinlichkeit. Im Gegensatz zu den nicht-stochastischen Nebenwirkungen gibt es keinen unteren Schwellenwert. Stochastische Risiken sind die Induktion bösartiger Tumoren, und die Keimzellschädigung mit der Gefahr von Missbildungen in den kommenden Generationen. Naturgemäß sind diese Risiken in hohem Maß alters- und zustandsabhängig. Sie sind in den Veröffentlichungen der ICRP und UNSCEAR tabelliert. Die Krebsinduktion innerhalb von 10 Jahren nach einer Strahlentherapie wird auf bis zu 2 % geschätzt (je nach Region und Volumen); dabei muss berücksichtigt werden, dass auch Chemotherapien karzinogene Potenz besitzen und dass die Krebserkrankung selbst das statistische Risiko für weitere Krebserkrankungen erhöht. Das Risiko für eine Keimbahnschädigung liegt in der ersten Kindgeneration bei ca. 1,4 %, bei 0,7 % in der zweiten, und kumulativ bei 0,7 % in allen weiteren Generationen. Hier sind synergistische mutagene Wirkungen der Zytostatika anzunehmen. Männern wird empfohlen, im ersten Jahr nach einer Strahlentherapie auf die Zeugung zu verzichten.

Infolge der technischen und prozeduralen Weiterentwicklung im Gebiet der Strahlentherapie wurde sie aus dem Gebiet der Radiologie ausgegliedert und ein neuer Facharzt geschaffen, unter Bestandschutz für die bereits tätigen Ärzte für Radiologie. Die Muster-Weiterbildungsordnung der deutschen Bundesärztekammer dient den rechtsverbindlichen Länderregelungen als Vorlage. Die aktuelle MWBO (zuletzt revidiert 2003) fordert für die Anerkennung als Facharzt für Strahlentherapie eine fünfjährige Weiterbildungszeit in gemäß § 5 zur Weiterbildung ermächtigten Abteilungen, von der ein Jahr im Stationsdienst abgeleistet werden muss, 6 Monate davon in einem beliebigen Gebiet. Früher war ein Jahr in der diagnostischen Radiologie vorgeschrieben; dies ist immer noch auf freiwilliger Basis anrechenbar. Die reine Zeit in der Strahlentherapie beträgt also 3 bis 4 Jahre.

Am 31. Dezember 2008 waren in Deutschland 1054 Strahlentherapeuten registriert, von denen 260 niedergelassen waren. 121 übten keine ärztliche Tätigkeit aus. Die DEGRO (Deutsche Gesellschaft für Radioonkologie) bildet ihre nationale Fachgesellschaft. Einschlägige Normierungen und Forschungen werden von der Arbeitsgemeinschaft für Radioonkologie in der DKG organisiert. Internationale Fachgesellschaften der Strahlentherapeuten sind die ESTRO und die EORTC.

In der Veterinärmedizin kann die Strahlentherapie ebenso eingesetzt werden wie beim Menschen, dennoch spielt sie hier nur eine untergeordnete Rolle. Grund ist vor allem die mangelnde Verfügbarkeit entsprechender Einrichtungen. Die Nutzung humanmedizinischer Zentren für Strahlentherapie ist durch deren hohen Auslastungsgrad limitiert. Ein weiterer Aspekt sind die damit für den Tierbesitzer verbundenen Kosten, die mit der begrenzten Überlebenszeit abgewogen werden müssen. Zudem müssen die Patienten in eine Kurzzeitnarkose gelegt werden, um eine einwandfreie Positionierung zu gewährleisten. Kurative (heilende) und palliative (lindernde) Tumorbestrahlungen finden daher zumeist dort Einsatz, wo eine chirurgische Tumorresektion aus kosmetischen und anatomischen Gründen nur schwierig ist, wie beispielsweise am Kopf und den Gliedmaßen. Darüber hinaus findet auch die Radiojodtherapie Einsatz, vor allem bei der Schilddrüsenüberfunktion der Katzen.

Die häufigste Indikation sind bösartige Hauttumoren wie Karzinome, Weichteilsarkome und Mastzelltumoren. Hier wird die Strahlentherapie vor allem im Anschluss an eine chirurgische Entfernung zur Vermeidung von Rezidiven durchgeführt. Aufgrund der kosmetisch-chirurgischen Limitierung werden auch bei Tumoren der Maul- und Nasenhöhle wie Plattenepithelkarzinome, maligne Melanome, Fibrosarkome und Zahnfleischgeschwulste ("akanthomatöse Epuliden") Bestrahlungen durchgeführt. Bei Tumoren im Zentralnervensystem (Gliome, Meningiome und Adenome der Hypophyse) gilt die Strahlentherapie als Mittel der Wahl. Bei Osteosarkomen der Gliedmaßen wird dagegen eine Amputation mit anschließender Chemotherapie bevorzugt, die Strahlentherapie kann hier palliativ angewendet werden, wenn der Besitzer des Tieres eine Amputation ablehnt. Maligne Lymphome sind meistens weit im Tier verstreut und Ganzkörperbestrahlungen sind kaum vertretbar, so dass hier allenfalls einzelne Herde bestrahlt werden, in der Regel mit einer Chemotherapie kombiniert.





</doc>
<doc id="8948" url="https://de.wikipedia.org/wiki?curid=8948" title="Brennstoff">
Brennstoff

Ein Brennstoff ist ein chemischer Stoff, dessen gespeicherte Energie sich durch Verbrennung in nutzbare Energie umwandeln lässt.

Brennstoffe werden nach verschiedenen Kriterien unterteilt, etwa nach ihrem Aggregatzustand in feste, flüssige und gasförmige Brennstoffe. Je nach Herkunft wird differenziert zwischen natürlichen, veredelten oder synthetischen Brennstoffen. Brennstoffe können organischer Natur sein, wie Erdöl, Erdgas oder Kohle, oder anorganischer Natur wie Wasserstoff oder Kohlenstoffmonoxid.

Nach Art der Energiefreisetzung werden Brennstoffe unterteilt in chemische Brennstoffe, die durch Oxidation thermische Energie freisetzen und elektrochemische Brennstoffe, die zum Beispiel in Brennstoffzellen zur Stromerzeugung dienen.

Die Qualität eines Brennstoffes wird durch den Heizwert oder den Brennwert beschrieben. Der Brennwert berücksichtigt auch die Nutzung der Kondensationswärme des im Rauchgas als Dampf enthaltenen Wassers (Brennwertnutzung). Bei absolut trockenen Brennstoffen, die keinen chemisch gebundenen Wasserstoff enthalten, sind Brennwert und Heizwert identisch.

"Fossile Brennstoffe" sind meist über lange, oft Millionen von Jahren dauernde, bio- und geochemische Prozesse entstanden. Die festen Brennstoffe unterlagen dabei einer Inkohlung, der Anreicherung von Kohlenstoff in durch Erdschichten abgeschlossene organische Masse. Die beiden Hauptschritte der Inkohlung sind der Übergang von organischer Masse, die meist aus höheren Pflanzenarten wie Hölzer oder Farnen bestand, zur Braunkohle und der weitere Übergang der Braunkohle zur Steinkohle. Als Ursprung des Erdöls werden niedere Meerestiere und Pflanzen vermutet. Die gesamten bestehenden Brennstoffvorräte nennt man Ressourcen. Diese unterteilen sich in vermutete und nachgewiesene Ressourcen. Nachgewiesene Ressourcen unterteilen sich weiter in abbaubare und vermutlich nicht abbaubare Ressourcen. Ist der technische Abbau einer Ressource gesichert, wird diese Ressource "Reserve" genannt. Der Energiegehalt fester fossiler Brennstoffe wird oft in sogenannten Steinkohleeinheiten (SKE) angegeben.

Davon unterschieden werden heute "nachwachsende" Rohstoffe, das sind regelmäßig nachproduzierbare Ressourcen ("biogene Brennstoffe" wie Holz, Biokraftstoffe, Biogas).

Kernbrennstoffe, wie angereichertes Uran und Plutonium, die durch Kernspaltung oder wie Deuterium und Tritium, die durch Kernfusion Energie freisetzen können, gehören ebenfalls nicht zu den fossilen Brennstoffen.

Brennstoffe kommen in allen drei klassischen Aggregatzuständen (fest, flüssig und gasförmig) zum Einsatz. Sie werden verschiedensten Verwendungszwecken zugeführt.

Der Weltmarktpreis für Brennstoffe schwankt aufgrund vieler Faktoren. Gemessen an der Steinkohleeinheit schwankte der Preis im Zeitraum von 1970 bis 2004:
Die Preise für die wichtigsten Brennstoffe haben einen erheblichen Einfluss auf die nationalen Volkswirtschaften. Stark steigende Preise können zum Beispiel eine bereits vorhandene Inflation verstärken.

Die Bestimmung des "Brennstoffbedarfes" ist erforderlich, um Bestell- oder Lagermengen zu bestimmen, die Verbrauchskosten zu schätzen und um Vergleichsrechnungen mit anderen Energieträgern oder Technologien zu erstellen. Das gilt gesamtwirtschaftlich ebenso wie für eine einzelne Feuerungsanlage respektive Verbraucher, seien sie ortsfest installiert oder mobil.

Der Brennstoffbedarf ist unter anderem abhängig von: Leistung der Anlagen, Wirkungsgrad, Laufzeit, Brennstoffart, und speziellen Faktoren wie Regelung und Steuerung.

Die meisten der in Deutschland verwendeten Brennstoffe müssen importiert werden. Dieses gilt in erster Linie für die flüssigen und gasförmigen Brennstoffe, da Mitteleuropa auf diesem Gebiet nur geringe eigene Ressourcen besitzt. Feste natürliche Brennstoffe werden dagegen in diesem geographischen Gebiet als Bodenschatz in größeren Mengen gefördert, wobei die bei der Förderung anfallenden Kosten durch Subventionen unterstützt werden, um den heimischen Brennstoff gegenüber den auf dem Weltmarkt gezahlten Preisen konkurrenzfähig halten zu können.

Die Bereitstellung von Brennstoffen aus nachwachsenden Rohstoffen (biogene Brennstoffe) hat für die Energiewirtschaft und den Verkehrssektor eine wachsende Bedeutung. 2009 wurden 5,5 Prozent der in Deutschland benötigten Endenergie durch biogene Brennstoffe (als Bioenergie) bereitgestellt. Damit machen sie rund zwei Drittel der erneuerbaren Energien (insgesamt 10,1 Prozent) aus. Den überwiegenden Teil der Bioenergie macht Wärme aus. Das Gesamtpotential der Bioenergie in Deutschland ist jedoch begrenzt, so dass nur ein Teil des Energiebedarfs durch sie gedeckt werden kann.



</doc>
<doc id="8949" url="https://de.wikipedia.org/wiki?curid=8949" title="Umsatzsteuer">
Umsatzsteuer

Eine Umsatzsteuer (USt) ist

Die Umsatzsteuer wird prozentual vom Entgelt berechnet und bildet zusammen mit diesem den vom Leistungsempfänger zu entrichtenden Preis. Die Umsatzsteuer gehört nicht zu den betrieblichen Kosten und mindert nicht den Ertrag des Unternehmers.

Es gibt im Wesentlichen zwei Varianten der Umsatzsteuer: Die in Deutschland bis 1967 und in Österreich bis 1973 verwendete "Allphasen-Brutto-Umsatzsteuer" und die seither verwendete "Allphasen-Netto-Umsatzsteuer mit Vorsteuerabzug". Der Gesetzgeber in Deutschland und Österreich hat bei der Umstellung den Namen der Steuer unverändert gelassen, dennoch hat sich in der Umgangssprache der Ausdruck "Mehrwertsteuer" (MwSt.) für die neue Variante mit Vorsteuerabzug durchgesetzt. Diese Bezeichnung ist insofern passend, als bei der neuen im Gegensatz zur alten Variante grundsätzlich nur die Wertschöpfung belastet wird.


Während Zölle (lateinisch teloneum) und Verkehrsteuern bereits seit dem Altertum bekannt waren, wurden Verbrauchsteuern (Akzisen) erst zu Beginn der Neuzeit eingeführt. Ihre Anfänge liegen im 16. Jahrhundert in den Niederlanden, über die ein Beobachter schreibt, „dass daselbst keine Sache zum Lebensunterhalt und Nothdurft zu haben ist, die nicht fünf- bis sechsmal Abgaben entrichtet hätte.“

In Deutschland führte der gewaltige Finanzbedarf im Ersten Weltkrieg 1916 zu einer reichseinheitlichen Stempelsteuer auf Warenlieferungen und 1918 zu einer Allphasen-Bruttoumsatzsteuer, die in der Bundesrepublik bis Ende 1967 und in der DDR bis zum 18. September 1970 beibehalten wurde.

Der ursprüngliche Steuersatz von 0,5 % stieg in Deutschland und später in der Bundesrepublik nach wiederholten Änderungen 1935 auf 2,0 %, 1946 auf 3,0 % und 1951 auf 4,0 % an.

1968 wurde – im Rahmen der Harmonisierung der Umsatzbesteuerung innerhalb der Europäischen Gemeinschaft – der Übergang zum System der Mehrwertbesteuerung mit Vorsteuerabzug vollzogen. Es handelt sich dabei um eine grundlegende Änderung zur Netto-Allphasen-Umsatzsteuer mit Vorsteuerabzug. Die Bezeichnung Umsatzsteuer ist wohl rechtlich korrekt, vom wirtschaftlichen Gehalt ist die Umsatzsteuer aber als Mehrwertsteuer ausgestaltet.

Aufgrund des Vorsteuerabzugs innerhalb der Europäischen Union war die 6. Richtlinie zum gemeinsamen Mehrwertsteuersystem vom 17. Mai 1977 die erste wichtige Vorschrift zur Vereinheitlichung des Umsatzsteuerrechts. Diese Richtlinie bildet auch die Grundlage für die Berechnung der eigenen Einnahmen der Gemeinschaft aus dem Umsatzsteueraufkommen der Mitgliedstaaten. Sie enthält aus diesem Grunde umfassende Regelungen zur Abgrenzung der Umsätze und über die Bemessungsgrundlagen für die Umsatzsteuer und legt außerdem den Umfang der Steuerbefreiungen für alle Mitgliedstaaten verbindlich fest. Die 6. Richtlinie wurde durch die Richtlinie 2006/112/EG vom 28. November 2006 über das gemeinsame Mehrwertsteuersystem ersetzt (Mehrwertsteuersystemrichtlinie). Inhaltlich unterscheidet sich die Mehrwertsteuersystemrichtlinie nicht von der 6. Richtlinie. Es hat nur redaktionelle Anpassungen gegeben. Auswirkungen auf das Umsatzsteuerrecht in Deutschland ergeben sich aus der Änderung nicht.

Mit der Vollendung des europäischen Binnenmarktes sind mit Wirkung vom 1. Januar 1993 die Zölle und Einfuhrumsatzsteuern im Warenverkehr innerhalb der Union weggefallen. Die Ware ist im Land ihrer endgültigen Bestimmung der Besteuerung zu unterwerfen. Bei diesem Bestimmungslandprinzip handelt es sich jedoch um eine Übergangsregelung, die ursprünglich zeitlich befristet war. Zwischenzeitlich gilt das Bestimmungslandprinzip unbefristet. Ziel ist jedoch eine einheitliche Besteuerung im europäischen Binnenmarkt nach dem sogenannten Herkunftslandsprinzip. Danach wird die Besteuerung in dem Land vorgenommen, aus dem die Ware kommt. Zurzeit ist nicht absehbar, wann das Herkunftslandprinzip eingeführt werden wird.

Die Finanzminister der Europäischen Union hatten sich 2009 darauf verständigt, dass Mitgliedsstaaten in einigen lokalen Dienstleistungsbranchen die verminderte Mehrwertsteuer anwenden dürfen. Die Niederlande nutzen diese Möglichkeit bei Übernachtungen in der Hotellerie, die mit einem Steuersatz von 6,0 % berechnet werden und in Deutschland bis Ende 2009 bei 19 % lagen. Seit Januar 2010 gilt in Deutschland für Beherbergungsleistungen von Gastwirten und Hoteliers die reduzierte Mehrwertsteuer von 7 %. Die Bundesländer Bayern, Niedersachsen und das Saarland fordern indes eine zügige strukturelle Mehrwertsteuerreform in Zeiten der Finanzkrise, u. a. eine steuerliche Entlastung des Gastgewerbes (seit 2010 in Kraft) und für bestimmte Handwerkerleistungen.

Die meisten Wirtschaftsgüter durchlaufen bis zum Endabnehmer eine Vielzahl von Phasen. Eine Ware wird in der Regel aus unterschiedlichsten Stoffen hergestellt, an einen Großhändler verkauft, der diese an einen Einzelhändler weiterveräußert, bis sie schließlich an den Endverbraucher abgegeben wird. Eine Umsatzbesteuerung kann in dieser Wertschöpfungskette allphasig (Anwendung auf jedes Stadium), mehrphasig (Anwendung auf einige, aber nicht alle Stadien) und einphasig (Anwendung auf nur einen Umsatzvorgang) sein. Vom Ort des Umsatzes hängt dabei ab, ob die Umsatzsteuer überhaupt anfällt.

In Deutschland war diese Form der Umsatzsteuer von 1918 bis 1967 in Verwendung. Der Steuersatz steigerte sich von 0,5 Prozent (bei der Einführung) in den darauf folgenden Jahren auf vier Prozent (1967).

In der Gegenwart existiert diese Variante nur in wenigen Ausnahmefällen (z. B. bei Gebietskörperschaften, die nicht über die notwendige Administration für die Erhebung der Steuer verfügen, oder bei Kleinstunternehmen, die die buchhalterischen Voraussetzungen für die Allphasen-Netto-Umsatzsteuer mit Vorsteuerabzug nicht erfüllen).

Die Bemessungsgrundlage für die Allphasen-Brutto-Umsatzsteuer ist der "Brutto"umsatz einer jeden Produktions- und Handelsstufe. In der Folge ist die Höhe der Steuerbelastung des Endprodukts abhängig davon, wie viele Produktions- oder Handelsschritte ein Produkt durchläuft.

Die Allphasen-Brutto-Umsatzsteuer hat im Wesentlichen zwei Vorteile: Einerseits ihre einfache Berechenbarkeit und andererseits geringe Verwaltungskosten bei der Erhebung.

Als Nachteile sind anzuführen:

Bei dieser Variante wird im Gegensatz zur Allphasen-Brutto-Umsatzsteuer durch den Vorsteuerabzug sichergestellt, dass das Produkt während der Herstellung vollständig umsatzsteuerfrei bleibt und die Umsatzsteuer erst vom Endkunden gezahlt wird. Bis dahin wird die Umsatzsteuer auf jeder Weiterverarbeitungsstufe wieder auf Null gesetzt, das heißt, sie wird dem Unternehmen vom Finanzamt erstattet oder mit einer Umsatzsteuerschuld verrechnet. Erst wenn ein Endverbraucher oder ein nicht zum Vorsteuerabzug berechtigtes Unternehmen das Produkt erwirbt, verbleibt der Umsatzsteuerbetrag in der Staatskasse.

Zulieferer Z verkauft an den Produzenten P Rohstoffe für 15 €. P stellt daraus eine Ware her, die er dem Großhändler G für 60 € liefert. Dieser verkauft die Ware an den Einzelhändler E zu 75 €. Einzelhändler E schließlich veräußert diese Ware an den Endverbraucher für 100 €.
Zur Berechnung der Umsatzsteuer ist der Preis zunächst um die Umsatzsteuer zu verringern:
Betrachtung des Einzelhändlers (E):

Der Verbraucher zahlt im Ergebnis:

Damit wird deutlich, dass der Endverbraucher die gesamte Steuerlast wirtschaftlich trägt. Die einzelnen Beträge werden von unterschiedlichen Teilen der Wertschöpfungskette abgeführt (von Z 2,39 € + von P 7,19 € + von G 2,39 € + von E 4,00 € = 15,97 €).

Die Steuer ist aus betriebswirtschaftlicher Sicht zunächst kostenneutral. Weil die jeweils vereinnahmte Umsatzsteuer mit der zuvor gezahlten Vorsteuer gegenüber der Finanzbehörde verrechnet werden kann, stellt letztere nur einen durchlaufenden Posten dar.

Im vorstehenden Beispiel sind folgende Steuerbeträge an das Finanzamt geflossen:

Nicht zu vernachlässigen sind jedoch Kapitalbindungseffekte und -kosten sowie negative Nachfrageeffekte dadurch, dass aufgrund einer Umsatzsteuer der Endpreis eines Produktes bzw. einer Leistung für den Verbraucher höher ist und dadurch die Nachfrage nach dem Produkt sinkt (Preis-Absatz-Funktion). Besonders kleinere Unternehmen und Selbständige, die vor allem im Privatkundengeschäft tätig sind und einen relativ geringen Anteil an Betriebskosten haben, erfahren Umsatzsteuererhöhungen in realen Gewinnrückgängen.

Angefallene Umsatzsteuern müssen im Wege der Selbstveranlagung vom Unternehmer berechnet, per "Umsatzsteuer-Voranmeldung" im Jahr mehrfach an das Finanzamt gemeldet und abgeführt werden. Nach Ablauf des Jahres werden in der Umsatzsteuererklärung die bereits geleisteten Zahlungen angerechnet.


Auf bestimmte Produkte (zum Beispiel Lebensmittel, Druckerzeugnisse, Kunstgegenstände und seit dem 1. Januar 2010 auch Hotelübernachtungen) werden 7 % USt erhoben. Würden diese mit einem Steuersatz von 19 % besteuert, könnte der deutsche Staat die Umsatzsteuereinnahmen um 24,2 Milliarden Euro (Stand 2008) erhöhen.

Der Bundesrechnungshof hat 2010 in einem Gutachten deutliche Kritik an vielen Regelungen geübt und Korrekturen bei der USt angemahnt:

In bestimmten Branchen und insbesondere bei internationalen Geschäften ist die Umsatzsteuer besonders anfällig für Steuerhinterziehung. Die Hinterziehung von Umsatzsteuer wird in der Regel durch Umsatzsteuerkarusselle begangen. Dabei werden Waren über verschiedene Stationen geliefert. An einer Station wird vom Kunden der Vorsteuerabzug geltend gemacht, ohne dass der jeweilige Lieferant die Umsatzsteuer an die Finanzverwaltung abführt. Steuerhinterziehungen im Bereich der Umsatzsteuer nehmen einen immer größeren Anteil an den Mehrsteuern aufgrund aufgedeckter Steuerstraftaten ein.

Beim Export in ein anderes Land der EU wird keine Umsatzsteuer berechnet (steuerfreie innergemeinschaftliche Lieferung), wenn der gewerbliche Abnehmer im Empfängerland eine Umsatzbesteuerung mit dem Steuersatz des Ziellandes vornimmt. Das Vorliegen dieser Voraussetzung wird unterstellt, wenn der Empfänger seine Umsatzsteuer-Identifikationsnummer angibt. Durch dieses Bestimmungslandprinzip wird die Besteuerung im Wege des innergemeinschaftlichen Erwerbs in das Empfängerland verlagert. Darüber hinaus ist seit dem 1. Januar 2010 im Rahmen des „VAT Package 2010“ neue Voraussetzung, dass auch bei innergemeinschaftlichen Dienstleistungen die USt-IdNr. des Leistungsempfängers anzugeben ist, damit der Leistungsort im Ausland belegt ist und der Unternehmer eine Rechnung ohne Ausweis von Umsatzsteuer erstellen kann. Ferner ist die USt-IdNr. in der zusammenfassenden Meldung den Finanzbehörden seit 1. Juli 2010 monatlich (vorher quartalsweise) zu melden und es sind die mit dieser USt-IdNr. im Zusammenhang stehenden Umsätze anzugeben.

Innerhalb der Europäischen Union ist die Umsatzsteuer aufgrund der Mehrwertsteuer-Systemrichtlinie (MwStSystRL), die am 1. Januar 2007 die bis dahin geltende 6. EG-Richtlinie ersetzt hat, einheitlich geregelt. Jedes Land hat die Umsatzsteuer als Mehrwertsteuer ausgestaltet. Die Mitgliedstaaten müssen einen Regelsteuersatz von mindestens 15 % haben. Eine Obergrenze für die Steuersätze ist nicht vorgegeben. Auch ist es ihnen gestattet, für bestimmte Leistungs- und Warengruppen geringere Steuersätze vorzusehen. Der Mindeststeuersatz für den ermäßigten Steuersatz ist 5 %. Nach Maßgabe der Richtlinie können Leistungsgruppen von der Mehrwertsteuer befreit werden oder es kann für sie ein Nullsatz eingeführt werden. Leistungen, auf die eine Nullsatzregelung anwendbar ist, unterliegen weiterhin den umsatzsteuerrechtlichen Vorschriften und berechtigen insbesondere auch zum Vorsteuerabzug. In Deutschland wird die Nullsatzregelung nur in Bezug auf den innergemeinschaftlichen und internationalen Luft-, See- und teilweise auf den Binnenschifffahrtsverkehr angewandt. Eine konsolidierte, d. h. Fassung der 6. EG-RL, ist auf europa.eu zu finden.

Für die meisten Staaten stellt die Umsatzsteuer eine Hauptfinanzierungsquelle dar. Sie ist mindestens gleichrangig mit den direkt erhobenen Steuern und gilt als das weitestgehende praktizierte moderne Finanzierungsinstrument.

Im Dezember 2007 einigten sich die EU-Finanzminister, einen Systemwechsel bei der Besteuerung von Dienstleistungen vorzunehmen. Grenzüberschreitende Dienstleistungen sollten von 2015 an innerhalb der EU zur Vermeidung von Wettbewerbsverzerrungen nicht mehr im Ursprungsland, sondern im Bestimmungsland mehrwertsteuerpflichtig sein.

Im internationalen Sprachgebrauch spricht man von VAT (Value added tax) oder in einigen Ländern auch von GST, siehe hierzu Goods and Services Tax.


Sonderregelungen
Steuersatzänderungen

Bei dem folgenden Vergleich ist es wichtig zu beachten, dass die USt in manchen Ländern nicht auf Basis des Nettobetrages, sondern auf Basis des Bruttobetrages berechnet wird. Beispiel: In Brasilien beträgt der Umsatzsteuersatz bei einer Faktura innerhalb des Bundesstaates i. d. R. 18 % (18 % des Bruttopreises müssen als Steuer abgeführt werden). Nach der Berechnungsmethode auf Basis des Nettobetrages – wie in der EU üblich – entspricht das einer USt von 21,951 %.




</doc>
<doc id="8950" url="https://de.wikipedia.org/wiki?curid=8950" title="Bantusprachen">
Bantusprachen

Die Bantusprachen bilden eine Untergruppe des Volta-Kongo-Zweigs der afrikanischen Niger-Kongo-Sprachen. Es gibt etwa 500 Bantusprachen, die von ca. 200 Mio. Menschen gesprochen werden. Sie sind im gesamten mittleren und südlichen Afrika verbreitet und dort in allen Staaten die meistgesprochenen Sprachen, wenn auch als Amtssprache in der Regel Englisch, Französisch oder Portugiesisch verwendet wird.

Im Nordwesten grenzt das Bantu-Gebiet an die übrigen Niger-Kongo-Sprachen, im Nordosten an nilosaharanische und afroasiatische (genauer semitische und kuschitische) Sprachen. Im Südwesten bilden die Khoisansprachen eine Enklave innerhalb des Bantu-Gebiets (siehe Karte).

Die Wissenschaft von den Bantusprachen und den damit verbundenen Kulturen und Völkern wird Bantuistik genannt. Sie ist ein Teilgebiet der Afrikanistik.

Das Wort „bantu“ bedeutet „Menschen“ in der Sprache Kongo. Es wurde von W. H. I. Bleek 1856 in die sprachwissenschaftliche Diskussion als Bezeichnung für eine weitverbreitete afrikanische Sprachengruppe eingeführt, deren Sprachen viele gemeinsame Merkmale aufweisen. Dazu gehören vor allem ein ausgeprägtes Nominalklassensystem (siehe unten), aber auch umfassende lexikalische Gemeinsamkeiten. Die folgende Tabelle zeigt das Wort für „Mensch“ – Singular und Plural – in einer Auswahl wichtiger Bantusprachen:

Singular und Plural werden scheinbar durch die Präfixe "mu-" bzw. "ba-" gebildet. Tatsächlich handelt es sich um die beiden ersten Nominalklassen (siehe unten) nahezu aller Bantusprachen, die Personen im Singular bzw. Plural bezeichnen. Die Trennstriche werden in der normalen schriftlichen Wiedergabe von Bantuwörtern nicht geschrieben, in diesem Artikel aber zur Verdeutlichung durchgehend gesetzt.

Die bekannteste und als Verkehrssprache am häufigsten gesprochene Bantusprache ist Swahili (auch Suaheli, Kiswahili oder Kisuaheli). Die folgende Tabelle enthält alle Bantusprachen mit mindestens 3 Millionen Sprechern und gibt die Zahl ihrer Sprecher, ihre Einordnung innerhalb des "Guthrie-Systems" (siehe unten) und ihr Hauptverbreitungsgebiet an. Einige dieser Sprachen sind sogenannte "Verkehrssprachen", die nicht nur muttersprachlich (als Erstsprache) erlernt, sondern von vielen Sprechern als Zweit- oder Drittsprache erworben werden, um eine Kommunikation in einem größeren Gebiet über die Sprachgrenzen einzelner Volksgruppen hinweg zu ermöglichen.

Die Klassenpräfixe für Sprachnamen (z. B. "ki-, kinya-, chi-, lu-, se-, isi-") werden in der sprachwissenschaftlichen Literatur heute üblicherweise nicht mehr verwendet. Auch in diesem Artikel wird die Kurzform ohne Präfix benutzt, also z. B. "Ganda" statt "Luganda".

Es gibt zahlreiche weitere Bantusprachen mit mehr als 1 Million Sprechern. Eine Übersicht über alle Bantusprachen mit mindestens 100.000 Sprechern bietet der Anhang „Bantusprachen nach Guthrie-Zonen“ am Ende dieses Artikels.

Bereits 1659 erschien von Giacinto Brusciotto eine lateinisch geschriebene Grammatik der Sprache Kongo. Wilhelm Bleek beschrieb erstmals 1856 die Nominalklassen der Bantusprachen (siehe unten) und prägte den Begriff "Bantu". Carl Meinhof erarbeitete ihre erste vergleichende Grammatik (1901). Malcolm Guthrie hat sie 1948 klassifiziert und 1967–71 in 16 geografische Zonen eingeteilt, die er mit den Buchstaben A–S (ohne I, O, Q) bezeichnete. Innerhalb dieser Zonen sind die Sprachen in Zehnereinheiten gruppiert und durchnummeriert (siehe: Einteilung der Bantusprachen nach Guthrie). Guthrie hat auch das Proto-Bantu als hypothetische Vorgängersprache aller heutigen Bantusprachen rekonstruiert. Joseph Greenberg klassifizierte die Bantugruppe als eine Unter-Unter-Einheit der Niger-Kongo-Sprachen (siehe unten). Zuvor wurden die Bantusprachen, insbesondere von Carl Meinhof und seinen Schülern, als eine eigene Sprachfamilie angesehen, welche im Verbreitungsgebiet der schwarzafrikanischen Sprachen den Sudansprachen gegenübergestellt wurden.

Mit der Frage der Herkunft (Urheimat) und Entstehung der Bantusprachen beschäftigten sich seit 1860 zahlreiche Sprachforscher. Einige historisch wichtige Hypothesen sind hier aufgeführt, um den schwierigen Prozess bis hin zur heutigen Erklärung des Bantu als einer Untereinheit der Niger-Kongo-Sprachen deutlich zu machen.

Der Ägyptologe Richard Lepsius ging 1880 in der Einleitung zu seiner "Nubischen Grammatik" in Afrika von drei Sprachzonen aus, wobei er die Khoisan-Gruppe nicht berücksichtigte: (1) Bantusprachen im südlichen Afrika, die Sprache der eigentlichen „Neger“, (2) gemischte „Negersprachen“ zwischen Äquator und Sahara, die Sudansprachen, (3) hamitische Sprachen (Ägyptisch, Kuschitisch, Berberisch) im nördlichen Afrika.

Primäre Merkmale dieser Sprachgruppen seien das Klassensystem der Bantu und das Genus<nowiki>system</nowiki> der Hamiten, die von Westasien nach Afrika eingewandert seien. Durch ihr Vordringen drängten sie Teile der Vorbevölkerung nach Südafrika ab (eben die Bantu, die ihre „reine“ Sprachform behielten); andere Gruppen vermischten sich mit den Hamiten und bildeten Mischsprachen aus – die Sudansprachen –, die weder ein ausgeprägtes Klassen- noch Genussystem aufwiesen. Ihre Grammatik bezeichnete er als „formlos“, „zurückgegangen“ und „entblättert“.

Der Indogermanist August Schleicher hatte eine ganz andere Vorstellung, die er 1891 veröffentlichte. Seiner Meinung nach war Afrika zunächst unbewohnt und wurde von Südwestasien aus in vier großen Wellen bevölkert:

Dabei ging er davon aus, dass die sudanischen Nigriten bereits ein rudimentäres, unvollkommenes Klassensystem gehabt hätten, das dann die Bantuvölker vervollkommnet und ausgeprägt haben. Für ihn war also das Nigritische oder Sudanische ein evolutionärer Vorläufer des Bantu, und nicht ein Ergebnis des Zerfalls wie bei Lepsius.

Der Afrikanist Carl Meinhof äußerte sich zwischen 1905 und 1935 mehrfach über die Entstehung der Bantusprachen; er steht in deutlichen Gegensatz zu den Hypothesen von Lepsius und Schleicher. Für ihn sind nicht die Bantusprache, sondern die Sudansprachen ur-nigritisch. Bantu sei eine Mischsprache mit nigritischer „Mutter“ (Substrat) und hamitischem „Vater“ (Superstrat). Die Besiedlung Afrikas erfolgte nach Meinhof also in drei sprachlichen Schichten: (1) die nigritischen Sudansprachen, (2) die hamitischen Sprachen und (3) die Bantusprachen als Mischform des Nigritischen und Hamitischen.

Diedrich Westermann ging zunächst als Meinhof-Schüler von einem gemeinsamen nigritischen Substrat der Sudan- und Bantusprachen aus. Ab 1948 war er aber zunehmend von der genetischen Urverwandtschaft der "westlichen" Sudansprachen und der Bantusprachen überzeugt, wie er in mehreren Veröffentlichungen dargelegt hat. Damit bereitete er den Boden für Greenbergs Niger-Kongo-Ansatz.

Joseph Greenberg setzte die Ansätze Westermanns konsequent fort und etablierte 1949 das Niger-Kongo-Phylum als eine große Sprachfamilie im westlichen und südlichen Afrika, die die Bantusprachen mit einbeschließt und die aus einem westsudanischen „nigritischen“ Kern hervorgegangen ist. Die Struktur dieser Familie hat sich seit diesem ursprünglichen Ansatz noch mehrfach geändert; die letzte Greenbergsche Fassung ist sein Werk „Languages of Africa“ von 1963.

Auch nach Greenberg wurde der innere Aufbau des Niger-Kongo-Phylums noch mehrfach geändert (siehe Niger-Kongo-Sprachen), allerdings stimmen alle Fassungen – auch die aktuellen (z. B. Heine-Nurse 2000) – darin überein, dass die Bantusprachen eine Unter-Unter-Einheit des Niger-Kongo darstellen, die am nächsten mit den sogenannten bantoiden Sprachen Ostnigerias und Westkameruns verwandt sind.

Die große Bedeutung der Bantusprachen innerhalb der Niger-Kongo-Sprachen (und damit im Kontext der afrikanischen Sprachen generell) zeigen folgende Zahlen:

Dennoch stellt nach heutigen Erkenntnissen, die vor allem auf den Arbeiten von Joseph Greenberg beruhen, die Bantugruppe nur eine Unter-Unter-Einheit des Niger-Kongo dar. Die genaue Position der Bantugruppe innerhalb der Niger-Kongo-Sprachen zeigt das folgende etwas vereinfachte genetische Diagramm:

Position des Bantu innerhalb des Niger-Kongo


Die komplexe Abstammungslinie der Bantusprachen lautet also mit allen Zwischengliedern:


Zur detaillierten Klassifikation der Bantusprachen innerhalb der Guthrie-Gruppen mit Angabe der Sprecherzahlen siehe den Abschnitt am Ende des Artikels „Bantusprachen nach Guthrie-Zonen“ (für Sprachen mit mindestens 100.000 Sprechern) und den unten angegebenen Weblink (für alle Bantusprachen).

Alle Theorien über die Herkunft der Bantusprachen machen explizit oder implizit Aussagen über ihre Urheimat und spätere Ausbreitung bis in die heutigen Siedlungsgebiete der Bantuvölker.

Gemäß seiner Klassifikation – Bantu als eine Untergruppe der sonst in Nigeria und Kamerun verbreiteten bantoiden Sprachen – setzte Joseph Greenberg die Urheimat der Bantusprachen im mittleren Benue-Tal (Ostnigeria) und im westlichen Kamerun an. Das ist die heute von den meisten Forschern akzeptierte und vertretene Meinung.

Malcolm Guthrie dagegen äußerte noch 1962 auf Basis einer "Wort-Sach-Argumentation" (Zusammenhang zwischen archäologisch greifbaren Gegenständen oder angebauten Pflanzenarten und den sprachlichen Bezeichnungen dafür), Proto-Bantu sei in einem Gebiet südöstlich des äquatorialen tropischen Regenwaldes entstanden. Aus diesem Kerngebiet seien sternförmig Migrationen in die heutigen Siedlungsgebiete erfolgt. Das Problem der verwandten bantoiden Sprachen im weitentfernten Westafrika löste er durch die Annahme, dass einige Prä-Bantu-Gruppen den Urwald mit Hilfe von Booten nach Norden durchdrungen hätten. Diese Position Guthries spielt in der heutigen Forschung keine Rolle mehr; allgemein wird eine Urheimat der Bantu "nördlich" des tropischen Regenwaldes angenommen, die große Mehrheit stimmt Greenbergs Ansatz "Ostnigeria-Westkamerun" zu.

Westliche und östliche Ausbreitungsrouten

Die Ausbreitung der Bantuvölker von ihrer westafrikanischen Urheimat ins gesamte subsaharanische Afrika ist eine der größten Wanderungsbewegungen der Menschheit. Zur Frage, welche Wege die Bantu-Gruppen nun von ihrer Urheimat aus eingeschlagen haben, gibt es zwei Theorien, die sich aber nicht gegenseitig ausschließen, sondern nur unterschiedliche Schwerpunkte setzen. Die erste besagt (z. B. Heine-Hoff-Vossen 1977), dass die frühen Bantu hauptsächlich küstennah „am Regenwald westlich vorbei“ nach Süden gezogen seien, eine weitere Gruppierung am Nordrand des Regenwaldes zunächst in östlicher, dann südlicher Richtung gewandert sei. Die westliche Hauptgruppe habe dann einen neuen Nukleus am Unterlauf des Kongos gebildet, aus dem die Mehrzahl der Bantustämme in der Savanne und im ostafrikanischen Hochland hervorgegangen sei. Die zweite Theorie geht hauptsächlich von einer nördlichen Umgehung des Regenwaldes aus. Diese Gruppen seien dann später vom Gebiet der großen ostafrikanischen Seen nach Süden gezogen und hätten dann den Kongo-Nukleus gebildet (oder sich mit ihm vereinigt), von dem aus die weitere Besiedlung Südost- und Südafrikas erfolgt sei. Generell geht man von frühen westlichen und östlichen Bantugruppen aus, die den beiden Hauptwanderungswegen entsprechen.

Chronologie der Ausbreitung

Nach Vansina (1995) und Iliffe (1995) kann man aus dem rekonstruierten Proto-Bantu-Vokabular (Landwirtschaft, Keramikherstellung), den archäologischen Funden (vor allem der Keramik) und den von frühen Bantugruppen landwirtschaftlich genutzten Produkten (Ölpalme, Yams, aber noch kein Getreide) schließen, dass die erste Auswanderung aus der westafrikanischen Urheimat in Ostnigeria "nach" der Einführung von Landwirtschaft und Töpferei erfolgt sein muss. Damit ergibt sich aus der Archäologie Ostnigerias und Westkameruns als wahrscheinlicher Zeitraum etwa 3000–2500 v. Chr. als Beginn der Auswanderungsbewegung. Zunächst wanderten die frühen Bantu ins Kameruner Grasland, wo weitere Begriffe für Landwirtschaft, Viehhaltung ("Ziege", "Rind"), Fischzucht und Bootsbau das Vokabular bereicherten.

1500–1000 v. Chr. gab es dann eine Abwanderung von Bantugruppen westlich des trockener werdenden Regenwaldes nach Süden bis zum Unterlauf des Kongo. Dort werden Bantukulturen archäologisch etwa 500–400 v. Chr. greifbar. Sie kannten noch keine Metallverarbeitung. Manche dieser Gruppen wanderten weiter nach Süden bis nach Nordnamibia, andere schwenkten nach Osten, zogen durch die großen Flusstäler und vereinigten sich mit der östlichen Gruppe im Kongo-Nukleus (siehe unten).

Die (wahrscheinlich größere) östliche Gruppe zog ab 1500 v. Chr. von Kamerun am Nordrand des Regenwaldes entlang bis in das Gebiet der großen Seen Ostafrikas. Dort gibt es ab 1000 v. Chr. Belege für den ersten Getreideanbau (Sorghum), intensive Viehhaltung und – ab 800 v. Chr. – erste archäologische Belege für die Metallverarbeitung und Eisenherstellung (Schmelzöfen in Ruanda und Tansania). Begriffe für Metalle und Metallverarbeitung spiegeln sich auch sprachlich im Proto-"West"-Bantu wider, während das Proto-Bantu sie noch nicht kannte. Möglicherweise erfolgte dieser kulturelle Aufschwung der Bantuvölker in Landwirtschaft, Viehzucht und Metallverarbeitung durch den Einfluss nilosaharanischer Gruppen aus dem oberen Niltal, wo diese Kulturstufe deutlich früher erreicht wurde. Die Bantuvölker stellen offensichtlich den Kern der eisenzeitlichen Urewe-Kultur dar, die im Gebiet der großen ostafrikanischen Seen verbreitet war. Mit der intensiveren landwirtschaftlichen Nutzung durch Brandrodung und dem Bedarf an Brennholz für die Eisenherstellung geht eine weitgehende Abholzung der Wälder im ostafrikanischen Seengebiet einher, also eine erste großflächige Umgestaltung der Natur Afrikas durch den Menschen.

Vom Gebiet der großen Seen aus zogen die Urewe-Bantu (ausweislich ihrer spezifischen Keramik) etwa ab 500 v. Chr. nach und nach in alle Gebiete Ost- und Südafrikas. Am Sambesi ist Urewe-Keramik ab 300 v. Chr. nachweisbar. Im ersten nachchristlichen Jahrhundert werden Angola, Malawi, Sambia und Simbabwe erreicht, im 2. Jahrhundert Mosambik, schließlich um 500 n. Chr. Südafrika. Sesshafte Lebensformen (mit Brachland-Rekultivierung) bildeten die Bantuvölker erst ab 1000 n. Chr. aus, vorher zwang sie die Brandrodungstechnik zu ständigem Weiterzug und der Aufgabe der ausgelaugten Flächen.

Dem Druck der Bantuvölker mussten die Khoisan weichen, die damals noch wesentlich größere Gebiete Südafrikas besiedelten als heute. Ihr Rückzugsgebiet wurden die Wüsten- und Steppenzonen Südangolas, Namibias und Botswanas, die für den Anbau von Sorghum ungeeignet und damit für die Bantu unbrauchbar waren. Auch die als „Pygmäen“ zusammengefassten Volksgruppen bewohnten wahrscheinlich größere zusammenhängende Gebiete Zentralafrikas, ehe sie von den Bantu auf wenige kleinere Gebiete zurückgedrängt wurden. Sie sprechen heute die Sprachen der jeweiligen benachbarten Bantuvölker, dies jedoch mit einigen lautlichen und lexikalischen Besonderheiten, die möglicherweise auf frühere eigene Sprachen zurückgehen.

Trotz ihrer Verbreitung über ein riesiges Gebiet zeigen die Bantusprachen einen hohen Grad grammatischer Ähnlichkeit. Besonders charakteristisch sind die Bildung von Nominalklassen – alle Substantive werden je nach Sprache in zehn bis zwanzig Klassen eingeteilt, die Klasse des Nomens wird durch ein Präfix gekennzeichnet –, der Einfluss dieser Klassen auf Kongruenz oder Konkordanz aller grammatischen Kategorien (d. h., die Klasse des Nomens überträgt sich auf seine Attribute und die des Subjekts auf die Formen des Prädikats) sowie komplexe, in allen Sprachen ähnlich konstruierte Verbalformen. Sowohl Nominal- als auch Verbalbildung sind im Wesentlichen agglutinativ; es werden sowohl Präfixe als auch Suffixe verwendet.

Die Bantusprachen teilen einen großen gemeinsamen Wortschatz, so dass mehrere hundert Proto-Bantuwurzeln rekonstruiert werden konnten, deren Nachkommen in fast allen Zonen des Guthrie-Schemas auftreten. Wortarten sind in den Bantusprachen nach ihrer syntaktischen Verwendung, nicht nach äußerer Form zu unterscheiden. Außer den schon genannten Nomina und Verben gibt es relativ wenige eigenständige Adjektive (die meisten sind Ableitungen von Verben), ein unvollständiges System von Zahlwörtern (7, 8, und 9 sind in der Regel Fremdwörter) und ein reichhaltiges Inventar von Pronomen, wobei die Demonstrativpronomen bis zu vier verschiedene Stufen der Nähe und Ferne ausdrücken können („dieser“, „jener“ und weitere).

Die Syntax ist stark morphosyntaktisch bestimmt, insbesondere durch das Nominalklassensystem und die damit verbundene Konkordanz in der Nominalphrase und zwischen Subjekt und Prädikat. Die übliche Wortfolge ist Subjekt – Prädikat – Objekt (SVO).

Historisch haben die Bantusprachen eine einfache Phonetik. Die Wörter bestehen aus offenen Silben, Verschlusslaute können "pränasaliert" sein (z. B. "mb-" oder "nd-"). Das Konsonanteninventar bestand ursprünglich aus stimmlosen, stimmhaften, nasalen und pränasalierten Verschlusslauten: //, außerdem enthielt es . Diese Phoneme blieben auch in den heutigen Bantusprachen weitgehend erhalten. Protobantu hatte offensichtlich keine weiteren Frikative, in den modernen Bantusprachen sind allerdings // weit verbreitet. Somit erhält man folgenden Konsonantenbestand, von dem einzelne Sprachen aber nicht alle Phoneme besitzen (z. B. oder , oder ; Pränasalreihe 1 oder 3, 2 oder 4):

Die Ejektivlaute entsprechen der deutschen Aussprache von "b", "d" und "g". Die Implosivlaute – im Swahili drei, im Shona zwei, im Xhosa und Zulu nur das  – werden in der Schrift zumeist mit ihren ejektiven Pendants wiedergegeben. Diese werden teilweise orthografisch unterschieden, beispielsweise durch ein nachgestelltes h.

Einige südliche Bantusprachen haben durch Kontakt mit Khoisan-Sprachen auch deren Klicklaute übernommen. Dies betrifft vor allem Sprachen der Guthrie-Gruppen S40 und S50, insbesondere Zulu (12 Klicklaute) und Xhosa (15). Aber auch Yeyi (oder Yeye) (R40) hat bis zu 20 Clicks, während nahverwandte und benachbarte Sprachen, die auch Kontakt mit den Khoisan-Sprachen hatten und haben (z. B. Herero), keine Spuren von Klicklauten aufweisen. Wahrscheinlich ist das darauf zurückzuführen, dass die Herero erst sehr viel später als die Xhosa und andere östlich der Kalahari wohnende Völker mit den Khoisan-Sprachen in Kontakt gekommen sind.

Das Vokalsystem des Protobantu bestand aus den sieben Vokalen //. Es ist noch heute in den nordost- und nordwest-zentralen Bantusprachen erhalten, während es bei den übrigen (etwa 60 %) auf die fünf Vokale // reduziert wurde. In etlichen rezenten Bantusprachen sind auch die Unterschiede zwischen langen und kurzen Vokalen phonemisch relevant. Ob es sich dabei um eine Eigenschaft des Protobantu oder um eine Innovation in bestimmten Teilgruppen handelt, ist bisher nicht entschieden worden.

Das Protobantu war sicherlich eine Tonsprache, das heißt, dass die Tonhöhe einer Silbe bedeutungsrelevant ist. Ein großer Teil der Bantusprachen (97 % laut Nurse 2003) haben diese Eigenschaft bewahrt. Die meisten Bantusprachen haben nicht mehr als zwei differenzierende "Töne", die entweder als "hoch-tief" oder "hoch-neutral" charakterisiert werden können. Es gibt aber auch komplexere Systeme mit bis zu vier verschiedenen Tonhöhen. Einige wenige Sprachen, darunter Swahili, haben ihre Tondifferenzierung verloren.

In einigen Bantusprachen gibt es eine Form der Vokalharmonie, die sich auf die Vokalisierung von bestimmten Ableitungssuffixen auswirkt. Zum Beispiel erhält im Gikuyu das Umkehrungssuffix "-ura" hinter der Verbalwurzel "hing" („öffnen“) die Form "hing-ura" („schließen“), hinter dem Verb "oh" („binden“) aber die Form "oh-ora" („losbinden“). Eine Dissimilation anlautender Konsonanten des Nominalklassenpräfixes und des Nominalstamms zeigt gerade der Sprachname "Gi-kuyu", der regelmäßig gebildet "Ki-kuyu" lauten müsste (die Schreibweise "Kikuyu" ist also falsch).

Der Akzent liegt in fast allen Bantusprachen auf der zweitletzten Silbe.

Ein besonderes Merkmal der Bantusprachen ist die Einteilung der Nomina in sogenannte Klassen. Dieses Merkmal teilen sie allerdings mit einer Vielzahl anderer Niger-Kongo-Sprachen und auch mit Sprachen ganz anderer genetischer Herkunft, z. B. kaukasischen, jenisseischen oder australischen Sprachen. Die Zuordnung eines Nomens zu einer Klasse erfolgte ursprünglich nach der Bedeutungskategorie eines Wortes, erscheint aber in den heutigen Bantusprachen oft zufällig. Auch das grammatische Geschlecht z. B. in vielen indogermanischen Sprachen lässt sich als Klasseneinteilung interpretieren (so könnte man das Lateinische als eine 6-Klassen-Sprache auffassen: Maskulinum, Femininum und Neutrum, jeweils im Singular und Plural).

Es gab im Protobantu etwa zwanzig Klassen. Diese Anzahl hat sich bei einigen der heutigen Bantusprachen erhalten (z. B. im Ganda), in anderen wurde sie bis auf etwa zehn Klassen reduziert. Die Nominalklassen werden ausschließlich durch Präfixe markiert. Die Klassen von Nomina und zugehörigen Attributen sowie von Subjekt und Prädikat müssen in der Konstruktion eines Satzes übereinstimmen (Konkordanz), allerdings können die Präfixe "einer" Klasse bei Nomen, Zahlwort, Pronomen und Verb unterschiedlich sein. In den meisten Bantusprachen bilden die Klassen – und die sie markierenden Präfixe – paarweise den Singular oder Plural eines Wortes (siehe unten die Beispiele aus den Sprachen Ganda und Swahili).

Beispiele für Nominalklassen in der Sprache Ganda:


Die in diesem Artikel durchgehend zur Verdeutlichung gesetzten Bindestriche zwischen Präfix und Stamm werden in der normalen Bantuschreibung nicht verwendet.

Beispiele aus dem Swahili zeigen die weitverbreitete Dopplung der Klassen in eine Singular-Klasse und eine zugehörige Plural-Klasse.

Es gibt nur relativ wenige echte Adjektivwurzeln in den Bantusprachen, offensichtlich ein Erbe der Ursprache. Die meisten Adjektive sind von Verben abgeleitet. In vielen Fällen verwendet man Relativkonstruktionen, also z. B. „der Mann, der stark ist (vom Verbum "stark sein")“ statt „der starke Mann“. Die attributiven Adjektive folgen ihrem Kopfnomen, dabei wird das Nominalpräfix der Nominalklasse des Nomens dem Adjektiv vorangestellt, es gilt also die Klassenkonkordanz. Dazu Beispiele aus dem Swahili:


Sämtliche Glieder einer Nominalphrase, also neben dem Nomen auch Possessivpronomina, Adjektive, Demonstrativpronomina und Zahlwörter, unterliegen dabei der Klassenkonkordanz (außer einigen Zahlwörtern, die aus fremden Sprachen übernommen wurden, siehe unten). Dazu einige Beispiele:


Die Klasse des Subjekts muss vom Prädikat eines Satzes kongruent aufgenommen werden, es herrscht also auch hier Konkordanz. Folgende Beispiele aus dem Swahili zeigen das Prinzip (Details zur Verbalkonstruktion siehe unten):


Possessivkonstruktionen der Art „das Haus des Mannes“ (Haus = Besitz; Mann = Besitzer, im Deutschen Genitivattribut) haben in den Bantusprachen in der Regel folgende Form:
Die Verbindung des Klassenmarkers (Präfix der Nominalklasse) mit dem suffigierten "-a" führt dabei häufig zu Kontraktionen und sonstigen lautlichen Veränderungen des Bindegliedes.

Beispiele aus dem Swahili:

Obwohl die Klassenzugehörigkeit von Nomina heutiger Bantusprachen nur sehr schwer semantisch bestimmbar ist (siehe obige Beispiele), wurde in vielen Forschungsarbeiten zu diesem Thema eine Liste der Bedeutungsfelder der einzelnen Nominalklassen erarbeitet. Eine Zusammenfassung dieser Ergebnisse geben Hendrikse und Poulos (1992), hier zitiert nach Nurse (2003). Neben den rekonstruierten Protobantu-Klassenpräfixen (nach Meeussen 1967) sind als Beispiel die Ganda-Präfixe aufgeführt, hier erweitert durch die vokalischen Prä-Präfixe, die sogenannten Augmente. Die Ganda-Präfixe entsprechen – wie man sieht – den rekonstruierten Protobantu-Präfixen weitgehend. Dazu werden einige charakteristische Beispielwörter aus der Ganda-Sprache angegeben. Die letzte Spalte beschreibt die Bedeutungsfelder der einzelnen Klassen.

Ein Blick in diese Tabelle zeigt viele Überschneidungen der Bedeutungsfelder der einzelnen Klassen, z. B. können "Tiere" den Klassen 3–4, 5–6, 7–8, 9–10 und anderen zugeordnet werden. Somit ist fast nie vorhersagbar, zu welcher Klasse ein Nomen einer bestimmten Bedeutungskategorie gehört. Eine Ausnahme stellen die Personenbezeichnungen dar, die fast immer den Klassen 1 und 2 zugeordnet sind.

Neben den abhängigen Personalenklitika für pronomiales Subjekt und Objekt, die in Verbalkonstruktionen verwendet werden (siehe dort), gibt es in den Bantusprachen auch selbständige Personalpronomina. Sie werden zur besonderen Betonung (Emphase) der Person eingesetzt, in der Regel nur als Subjekt. Die Possessivpronomina sind nicht enklitisch, sondern werden dem zu bestimmenden Nomen mit Klassenkonkordanz (siehe oben) als eigenständiges Wort nachgestellt. Die beiden Pronomina lauten im Swahili:

Einige Beispiele zum Possessivpronomen:


Die Demonstrativa bieten im Protobantu ein differenziertes drei- oder sogar vierstufiges System der Nähe und Ferne des Verweises (während z. B. im Deutschen nur ein zweistufiges System mit „dieser“ und „jener“ existiert):


Zum Beispiel haben sich in der Sprache Venda (S20) alle vier Stufen erhalten. Durch lautliche Verbindung mit den Klassenmarkern entwickeln die Demonstrativa für jede Klasse eine besondere Form. Sie lauten im Venda in den Klassen 1 und 2 (Personenklassen, vereinfachte Phonetik):

Allerdings haben sich in vielen Bantusprachen davon nur zwei Stufen erhalten, z. B. im Swahili "Klassenmarker+le" „jener“, "hV+Klassenmarker" „dieser“ („V“ Vokal in Harmonie mit dem Klassenmarker). Ausnahmsweise wird beim Nah-Demonstrativum "hV-" der Klassenmarker nicht als Präfix, sondern als Suffix verwendet. Dazu einige Beispiele aus dem Swahili:


Während Possessiv- und Demonstrativpronomina der Klassenkonkordanz (siehe oben) unterworfen sind, unterscheidet das Fragepronomen in den Bantusprachen nur die Kategorien „Person“ und „Sache“, z. B. im Swahili "nani" „wer?“, "nini" „was?“.

Die Zahlwörter für 1–5 und 10 stammen in vielen Bantusprachen vom Urbantu und sind immer noch relativ ähnlich, für 6–9 sind sie unterschiedlicher Herkunft (Arabisch, europäische Sprachen, afrikanische Nicht-Bantu-Sprachen) und variieren in den einzelnen Sprachen stark. In Swahili lauten sie:

Die Zahlwörter für 1–5 und 8 werden wie Adjektive behandelt und nehmen an der Klassenkordanz teil (siehe oben). Die Zahlwörter für 6, 7 und 9 (kursiv) stammen aus dem Arabischen und unterliegen nicht der Konkordanz, sie erhalten also keine Klassenpräfixe (siehe oben). Die Zehner (außer „10“) und Hunderter sind ebenfalls arabischer Herkunft.

Beispiele aus dem Swahili:

Verbalderivationen

Durch verschiedene Suffixe am Verbalstamm können "abgeleitete Verben" (Derivate) gebildet werden, davon machen die meisten Bantusprachen regen Gebrauch. Einige der Derivationsendungen haben sich aus protosprachlichen Vorgängern entwickelt. Dazu zwei Beispiele:

Der protosprachliche Reziprok-Marker ("reziprok" = "wechselseitig") "„-ana“" hat sich in vielen Bantusprachen erhalten, z. B.

Der Kausativ-Marker "„-Vsha“" erscheint als "-Vsha" im Swahili, "-ithia" im Gikuyu, "-isa" im Zulu, "-Vtsa" im Shona, "-Vsa" im Sotho und "-isa" im Lingala. („V“ steht hier für einen beliebigen Vokal.)

Eine Übersicht über die Derivationssuffixe gibt die folgende Tabelle mit einigen Beispielen (nach Möhlig 1980).

Aspekt, Modus und Tempus

Aspekte und Modi werden durch Suffixe markiert, die meisten Bantusprachen haben sieben Aspekte bzw. Modi: Infinitiv, Indikativ, Imperativ, Konjunktiv, Perfektiv, Kontinuativ und Subjunktiv. (In der Bantuistik spricht man in der Regel nur von „Aspekten“.)

Tempora werden durch Präfixe gekennzeichnet, die zwischen dem Klassenpräfix (siehe oben) und Stamm eingefügt werden (konkrete Beispiele im nächsten Abschnitt). (In der afrikanistischen Literatur werden die Tempuspräfixe häufig fälschlich als „Tempusinfixe“ bezeichnet.) Die Tempora und ihre markierenden Präfixe variieren in den einzelnen Bantusprachen sehr stark, so dass sie sich kaum aus gemeinsamen protosprachlichen Morphemen entwickelt haben, sondern erst später in den einzelnen Zweigen der Bantusprachen mehr oder weniger unabhängig voneinander entstanden sind.

Im Folgenden werden einige Verbalkonstruktionen des Swahili dargestellt.

Infinitiv

Infinitive werden als "ku + Stamm + Endvokal" gebildet; der Endvokal ist "-a", wenn es sich um ein originales Bantuverb handelt, "-e/-i/-u", wenn ein aus dem Arabischen stammendes Fremdverb vorliegt. Beispiele:


Imperativ

Der Imperativ wird im Singular durch den "Stamm + Endvokal", im Plural durch Anhängen von "-eni" an den Stamm ausgedrückt.


Indikativ

Finite Verbalformen des Indikativs haben die Gestalt


Subjektmarker ist das Klassenpräfix (s. o.) des nominalen Subjekts, allerdings werden für Subjekte der Personenklassen "m- /wa-" (nominal und pronominal) besondere Subjektmarker verwendet. Gleiches gilt für die Objektmarker, die sich auf ein direktes oder indirektes Objekt beziehen können. Subjekt- und Objektmarker der Personenklassen sind in der folgenden Tabelle zusammengestellt.

Bei allen anderen Klassen sind Subjekt- und Objektmarker identisch und entsprechen dem jeweiligen Klassenmarker, z. B. "ki-" „es“, "vi-" „sie (pl.)“ in der ki-/vi-Klasse. Die folgende Tabelle stellt die Tempuspräfixe des Swahili zusammen.

Einige Konstruktionsbeispiele für den Indikativ


Benefaktiv

Um zu verdeutlichen, dass die Handlung zum Vorteil für eine Person geschieht, wird zusätzlich zum Objektmarker nach dem Verbstamm (aber vor dem Endvokal "-a") ein sog. Benefaktivsuffix "-i-" oder "-e-" eingefügt. Beispiel:


Relativkonstruktion

Relativkonstruktionen der Form „das Kind, das ein Buch las“ werden im Swahili durch das Relativpräfix "-ye-" ausgedrückt, das dem Tempuspräfix folgt. Beispiele:


Passiv

Das Passiv wird bei transitiven Verben durch Einfügen von "-w-" oder "-uliw-" vor dem Infinitiv-Endvokal (in der Regel "-a") gekennzeichnet. Beispiele:


Kausative

Kausative werden durch Anfügen des Suffixes "-sha" am Stamm gebildet. Beispiel:

"Die Beispiele sind teilweise aus Campbell (1995) übernommen."

Keine Bantusprache hat eine eigene Schrift entwickelt. Nur Swahili hatte bereits in vorkolonialer Zeit – vielleicht schon seit dem 10. Jahrhundert – die arabische Schrift zur Fixierung einer überwiegend islamisch-religiösen Literatur übernommen. Neben theologischen Ausführungen gab es aber auch Rechtstexte, Chroniken, Geographica, Märchen, Lieder und Epen. Diese Epen (z. B. „Das Geheimnis der Geheimnisse“, das „Herkal-Epos“) sind inhaltlich und formal nach arabischen Vorbildern geschaffen, weisen aber auch Einflüsse der ostafrikanischen Bantukultur auf. Die Bedeutung der arabisierten Swahili-Literatur kann man mit der der Literaturen in den Sprachen Hausa, Ful, Kanuri und Berber vergleichen, die ebenfalls frühzeitig (im 10.–14. Jahrhundert) arabisch verschriftet wurden. Seit dem späten 19. Jahrhundert wurde Swahili wie alle anderen verschriftlichten Bantusprachen in lateinischer Schrift geschrieben.

Auch ohne Schrift besaßen und besitzen die Bantuvölker eine reichhaltige orale Literatur, die Mythen, Märchen, Fabeln, Sprichwörter, Lieder und Stammesgeschichten umfasst. Unter europäischem – insbesondere missionarischem – Einfluss wurde vor allem für die größeren Bantusprachen seit der Mitte des 19. Jahrhunderts das lateinische Alphabet eingeführt (meist mit kleineren sprachspezifischen Modifikationen), häufig waren Bibelübersetzungen die ersten schriftlichen Texte in einer Sprache. Seit dieser Zeit setzte auch eine rege Sammeltätigkeit von Missionaren, Verwaltungsbeamten und Sprachwissenschaftlern ein, die sakrales und profanes Liedgut, Spruch- und Rätseldichtungen, Mythen, Märchen, Sagen und Epen der Bantuvölker zusammentrugen und in den Originalsprachen aufzeichneten. In Europa sind davon in der Regel nur Übersetzungen bekannt geworden.

Inzwischen hat sich eine recht umfangreiche und vielseitige neue schwarzafrikanische Literatur entwickelt, allerdings bevorzugen die meisten modernen Autoren eine der Kolonialsprachen als Vehikel ihrer Werke, da sie damit eine wesentlich größere Zielgruppe erreichen können. Die orale Bantuliteratur spielt aber sowohl inhaltlich als auch formal eine wichtige Rolle als Grundlage für große Bereiche der neoafrikanischen Literatur.

Malcolm Guthrie hat die Bantusprachen 1948 in 16 Gruppen („Zonen“) eingeteilt, die er mit den Buchstaben A – S (ohne I, O, Q) bezeichnete, zum Beispiel ist Zone A = Bantusprachen aus Kamerun und Äquatorialguinea. Innerhalb jeder Zone sind die Sprachen in Zehnereinheiten gruppiert, so sind etwa A10 = Lundu-Balong-Gruppe und A20 = Duala-Gruppe. Die einzelnen Sprachen sind in jeder Zehnergruppe durchnummeriert; zum Beispiel ist A11 = Londo und A15 = Mbo. Dialekte werden durch kleine Buchstaben gekennzeichnet, z. B. A15a = Nordost-Mbo.

Das System von Guthrie ist vor allem geografisch orientiert, eine genetische Bedeutung hat es nach heutiger Erkenntnis kaum. Es wird aber weiterhin allgemein als Referenzsystem der Bantusprachen benutzt.

Im Folgenden werden die einzelnen Zonen mit ihren Zehnergruppen aufgeführt und die Sprachen mit mindestens 100.000 Sprechern innerhalb der Zehnergruppen angegeben. Auf die Einzelnummerierung der Sprachen wird verzichtet, da sie je nach Autor unterschiedlich ausfällt. Details über diese Sprachen findet man in Ethnologue, das auch die Hauptquelle für die Sprecherzahlen ist. Die Zonen A, B und C werden als "Nordwest-Bantu", die übrigen als "Zentral-Süd-Bantu" klassifiziert. Sprachen mit mindestens 1 Mio. Sprecher sind in Fettdruck angegeben. In der Regel ist die Anzahl der muttersprachlichen Sprecher S1 angegeben, S2 ist die Sprecherzahl einschließlich der Zweitsprecher (wird nur angegeben, wenn sie signifikant von S1 abweicht).








</doc>
<doc id="8951" url="https://de.wikipedia.org/wiki?curid=8951" title="Swahili (Sprache)">
Swahili (Sprache)

Swahili (Eigenbezeichnung "Kiswahili"), deutsch (besonders historisch) Suaheli oder Kisuaheli, ist eine Bantusprache und die am weitesten verbreitete Verkehrssprache Ostafrikas. Das Wort "swahili" ist vom arabischen Plural "sawāḥil", Singular "sāḥil" für „Küste“ oder „Grenze“ (im Deutschen zu Sahel), abgeleitet. Swahili ist die Muttersprache der Swahili, die im etwa 1500 Kilometer langen Küstenstreifen von Süd-Somalia bis in den Norden von Mosambik leben, sowie einer ständig wachsenden Zahl von Einwohnern Ostafrikas, die mit dieser Sprache aufwachsen. Mehr als 80 Millionen Menschen beherrschen Swahili, was die Sprache zu der am meisten gesprochenen Bantusprache weltweit macht. Von den mehr als 80 Millionen Swahili-Sprechern sind nur 5–10 Millionen Muttersprachler.

Swahili ist Amtssprache in Tansania, wo es durch den Nationalen Swahilirat gepflegt wird, sowie zweite Amtssprache und weit verbreitete Verkehrssprache in Kenia. In Uganda ist es seit 2005 Amtssprache. Bereits 1967 hat sich die Ostafrikanische Gemeinschaft die Förderung des Swahili anstatt des von der britischen Kolonialmacht hinterlassenen Englischen zu einem Ziel gesetzt. In der Demokratischen Republik Kongo ist Swahili eine von vier Nationalsprachen, gesprochen wird es vor allem im Osten des Landes. Swahili wird auch in Ruanda, Burundi, Süd-Somalia, Mosambik, Malawi und auf den Komoren (einschließlich Mayotte) gesprochen.

Bei dem Gipfeltreffen der Afrikanischen Union im Juli 2004 wurde Swahili als Arbeitssprache genutzt. Als Verkehrssprache (Handelssprache, Lingua franca) wird Swahili von etwa 80 Millionen Menschen gesprochen.

Das Wort Swahili stammt von dem Plural ' des arabischen Wortes ', das „Küste“ oder „Grenze“ bedeutet (vgl. Sahelzone). Das daraus abgeleitete Adjektiv "" () bedeutet etwa „Küstenbewohner“. Unklar ist, ob die Endung „-i“ auf die arabische Nisba-Endung zurückgeht oder aus phonologischen Gründen angehängt wurde. Die Sprache wird im Swahili selbst (und gelegentlich auch im Deutschen) „Kiswahili“ genannt. Das Präfix "ki-" gibt dabei (ähnlich wie im Deutschen der Artikel) die Klassenangehörigkeit des Nomens an.

Swahili ist aus der Begegnung afrikanischer Küstenbewohner mit seefahrenden Händlern meist arabischen Ursprungs entstanden. Die Sprache wird genetisch zur Sabaki-Gruppe der Bantusprachen Ostafrikas gerechnet. Die größten Ähnlichkeiten bestehen zu einer Reihe von Sprachen des kenianischen Küstenraumes sowie der Komoren. In der geographischen Einteilung der Bantusprachen nach Malcolm Guthrie gehört Swahili zur Zone G40.

Auch wenn Swahili grammatikalisch eindeutig zu den Bantusprachen gehört, umfasst sein Wortschatz eine große Zahl arabischer Vokabeln. Dies veranlasste frühe europäische Besucher dazu, das Swahili als eine Variante des Arabischen anzusehen. In klassischen Gedichten kann dieser Anteil bei bis zu 50 % liegen; im modernen "Umgangsswahili" wird der Anteil arabischer Wörter auf 20 % geschätzt. Generell werden im islamisch geprägten Küstenraum, der traditionellen Heimat der Sprache, mehr Wörter arabischen Ursprungs benutzt als im Binnenland.

Im 20. Jahrhundert ist eine große Zahl von Begriffen aus dem Englischen aufgenommen worden. Als weitere Sprachen sind das Persische, indische Sprachen, Portugiesisch und im Kongo-Swahili auch das Französische mit Lehnwörtern im Swahili vertreten. Aus der deutschen Kolonialzeit sind nur wenige Ausdrücke dauerhaft ins Swahili eingedrungen, von denen "„shule“" (Schule) der bekannteste ist. Insgesamt wird geschätzt, dass der Anteil fremdsprachlicher Wörter im Swahili etwa dem Anteil französischer, lateinischer und griechischer Lehnwörter im Englischen entspricht.
Swahili ist eine der wenigen afrikanischen Sprachen, die schon vor der Kolonialzeit eine Schrifttradition hatten. Die ältesten erhaltenen Manuskripte stammen aus der Zeit um 1700 und benutzen die arabische Schrift (als „Adschami-Schrift“). Im 19. Jahrhundert wurde die Sprache erstmals mit lateinischer Schrift notiert; der schwäbische Missionar Ludwig Krapf verfasste das erste Wörterbuch und eine Grammatik. Unter dem Einfluss der Missionsschulen sowie der europäischen Kolonialmächte wurde das Lateinische Alphabet zum Standard. Im Küstenbereich gibt es heute nur noch wenige Swahilisprecher, die als Muslime und Koranleser mit der arabischen Schrift vertraut sind und auch Swahili noch mit arabischen Buchstaben notieren.

Durch den Karawanenhandel mit Sklaven und Elfenbein verbreitete sich das Swahili als Handels- und Verkehrssprache zunehmend entlang der Karawanenwege von der ostafrikanischen Küste ins Binnenland bis in die Region der Großen Seen und den östlichen Kongo. Im 19. Jahrhundert wurde die Sprache lexikalisch und grammatikalisch erschlossen, wobei vor allem Missionare wie Ludwig Krapf eine bedeutende Rolle spielten.

Mit dem Beginn der Kolonialzeit verstärkte sich die Bedeutung des Swahili, da sowohl in Deutsch-Ostafrika als auch in Kenia die deutschen und britischen Kolonialherren von der Küste her das Land unterwarfen, sich beim Aufbau der Verwaltung einheimischer Helfer aus dem swahilisprachigen Gebiet bedienten und die Sprache auch als Verwaltungssprache einsetzten. Die Deutschen setzten hierbei das Swahili konsequent als Amtssprache für den direkten Kontakt mit den Einheimischen auf den unteren Ebenen der Verwaltung sowie in den staatlichen Schulen ein (wohingegen die zahlreichen Missionsschulen eher auf die Lokalsprachen der einzelnen Volksgruppen setzten). Im britischen Bereich war Swahili in Uganda vorübergehend offizielle Verwaltungssprache, blieb aber dauerhaft nur die Kommandosprache für Polizei und Armee. In Kenia setzten die Briten bis in die 1950er Jahre Swahili als untere Verwaltungssprache und überregionale Bildungssprache in den Volksschulen ein, setzten dann aber stärker auf das Englische. In Tanganyika setzen die Briten die vormalige deutsche Sprachpolitik mit Swahili als unterer Verwaltungssprache bis zur Unabhängigkeit fort. In Belgisch-Kongo wurde die Sprache von den belgischen Kolonialherren in Verwaltung und Schulbildung in der östlichen Region Katanga verwendet und ist heute eine der vier anerkannten nationalen Sprachen der Demokratischen Republik Kongo.

In den Jahren zwischen den beiden Weltkriegen bemühte sich die britische Kolonialmacht um eine Standardisierung der Sprache, um sie besser zu Verwaltungszwecken einsetzen zu können. Im "Interterritorialen Sprachkomitee" der britischen ostafrikanischen Gebiete wirkten Regierungsvertreter, Einheimische und Vertreter der Missionsgesellschaften zusammen, denen an einer Vereinheitlichung für eine gemeinsame Bibelübersetzung sowie für ihre Schulen gelegen war. Dabei wurde der Dialekt von Sansibar zugrunde gelegt, der schon im 19. Jahrhundert durch den Karawanenhandel eine weitere Verbreitung entlang der Handelswege im Landesinneren von Tanganyika gefunden hatte. Hierauf baut bis heute das Standard-Swahili auf, wie es in Tansania und Kenia durch Schulbücher und Massenmedien verbreitet wird.

Die Pflege der Sprache obliegt heute den Nationalen Swahiliräten in Tansania und Kenia sowie dem sprachwissenschaftlichen Institut an der Universität Dar es Salaam, an dem eine Reihe von Wörterbüchern erarbeitet worden sind. Die tatsächliche Verbreitung ist in Tansania am weitesten fortgeschritten, wo Swahili die allgemeine Unterrichtssprache der siebenjährigen Volksschule ist. In Kenia und Tansania ist es auch Pflichtfach an den Sekundarschulen. In beiden Ländern gibt es Radio- und Fernsehprogramme auf Swahili.

Der Fortschritt der Sprache in Uganda ist bisher begrenzt. Sie gilt als „islamisch“ und erfreut sich geringer Beliebtheit, weil sie die Kommandosprache von Polizei und Militär ist. Tansanier machen sich in dieser Hinsicht gerne über ihre Nachbarn lustig, wie ein verbreitetes Scherzwort zeigt:
Im Jahre 2005 wurde Swahili neben Englisch zur zweiten Nationalsprache Ugandas erklärt und seither allmählich zunehmend in den Schulen unterrichtet.

Das Standard-Swahili verfügt über fünf Vokal-Phoneme: /a/, /e/, /i/, /o/ und /u/. Sie entsprechen den deutschen kurzen Vokalen und werden auch in unbetonten Silben nicht reduziert gesprochen. Der vorletzte Vokal ist lang, aber ebenfalls offen.

Aussprache:

Swahili kennt keine Diphthonge. Das Wort für „Leopard“, "chui", wird deshalb "chu’i" ausgesprochen.

Die folgende Tabelle enthält die Konsonanten des Swahili in Form ihrer schriftlichen Umsetzung. In eckigen Klammern ist jeweils der Laut in phonetischer Transkription angegeben. Neben den Genannten gibt es die Buchstabenkombination <ng'> (gesprochen: ). Sie unterscheidet sich von <ng> durch Fehlen des Verschlusslautes . <m> wird am Wortanfang vor einem Konsonanten zu []. Eine Besonderheit im Lautinventar des Swahili sind die Implosive, die an die Stellen der stimmhaften Plosive , und treten.

Wie alle Bantusprachen teilt das Swahili alle Nomen in Nominalklassen ein. Das ursprüngliche System hatte 22 Klassen (wobei Klassen für Nomen im Singular und Plural jeweils als eigene Klasse gezählt werden), von denen jede Bantusprache mindestens zehn verwendet. Im Swahili gibt es fünfzehn Klassen: sechs für Singular, fünf für Plural, eine für Infinitive und drei für Ortsbezeichnungen, darunter "mahali" („Ort, Stelle“).

Wörter, die im Singular mit "m-" (1.) und im Plural mit "wa-" (2.) beginnen, bezeichnen Personen, z. B. "mtoto" „Kind“, "watoto" „Kinder“. Eine Klasse mit "m-" (3.) im Singular und "mi-" (4.) im Plural wird hauptsächlich für Pflanzen verwendet, vgl. "mti" „Baum“ und "miti" „Bäume“. Infinitive beginnen mit dem Klassenpräfix "ku-" (17.), z. B. "kusoma" „lesen“. Bei allen anderen Klassen lassen sich nur schwer inhaltliche Bezüge herstellen. Die "ki-"/"vi-"-Klasse (7./8.) enthält hauptsächlich Werkzeuge und Artefakte, wird aber auch für Fremd- und Lehnwörter benutzt, bei denen das "ki-" ursprünglich zum Stamm gehörte: "kitabu"/"vitabu" „Buch“/„Bücher“ (aus arabisch "kitāb" „Buch“). In diese Klasse gehören außerdem Sprachen (wie der Name der Sprache selbst: "Kiswahili") und Diminutive (Verkleinerungsformen). Wörter mit dem Klassenpräfix "u-" (11., Plural nach der 6. oder 10. Klasse (s. u.) – oder ohne Plural) bezeichnen oft Abstrakta, z. B. "utoto" „Kindheit“.

Die 9. und 10. Klasse beginnt mit "n-" oder "m-" und hat im Plural die gleiche Form. Eine weitere Klasse (5.) hat "ji-" oder nichts (ø-) als Präfix im Singular; ihr Plural wird mit "ma-" (6.) gebildet. Diese Klasse wird häufig für Augmentative benutzt.

Oft kann am Nomen selbst nicht erkannt werden, zu welcher Klasse es gehört. Dies ist dann nur unter Berücksichtigung der mit ihm konkordierenden (übereinstimmenden) Wörter möglich. Adjektive und Zahlwörter tragen dasselbe Präfix wie das Nomen ("Set A"), Verben und andere Wortarten erhalten (sofern Übereinstimmung gefordert ist) andere Klassenpräfixe ("Set B").

Ein Beispiel für die 1. Klasse ("Singular") mit "m-" bei Nomen und "a-" bei Verben:

Im "Plural", also der 2. Klasse, wird "wa-" bei Nomen und "wa-" bei Verben verwendet:

Klasse 7/8 mit "ki-"/"vi-" (sowohl bei "Set A" (Nomen) als auch bei "Set B" (Verben)):

Aus ein und derselben Wurzel können durch Verwendung unterschiedlicher Klassenpräfixe Ableitungen gebildet werden: menschlich (1./2.) "mtoto (watoto)" „Kind(er)“; abstrakt (11.) "utoto" „Kindheit“; Verkleinerung (7./8.) "kitoto (vitoto)" „Kleinkind(er)“; Vergrößerung (5./6.) "toto (matoto)" „großes Kind/große Kinder“.

Ebenfalls möglich: Pflanzen (3./4.) "mti (miti)" „Baum/Bäume“; Werkzeuge (7./8.) "kiti (viti)" „Stuhl/Stühle“; Vergrößerung (5./6.) "jiti (mati)" „großer Baum“; Verkleinerung (7./8.) "kijiti (vijiti)" „Stock/Stöcke“;? (11./10.) "ujiti (njiti)" „schlanke(r), hohe(r) Baum/Bäume“.

Die Verbkonstruktion weist im Swahili einen agglutinierenden Sprachbau auf. Verben im Swahili bestehen aus einer Wurzel und einer Reihe von Affixen. Die Endung des Verbs wird zur Bezeichnung der Genera verbi verändert, während Tempus oder Modus, die Person des Subjekts und manchmal auch des Objekts, Verneinung und Relativpronomen als Präfixe vor das Verb gesetzt werden. Da sich diese Präfixe – bis zu vier an einem Verb – auch zwischen der Wurzel und anderen Präfixen befinden, ist manchmal irrtümlich angenommen worden, Swahili besitze Infixe.

Die wichtigsten Genera Verbi sind Indikativ/Aktiv (unmarkiert, Bantu-Endung "-a"), Passiv (Endung "-wa"), Applikativ (Endung "-ia" oder "-ea"), Zustandspassiv (Endung "-ika" oder "-eka"), Kausativ (Endung "-sha" oder "-za") sowie eine Reziprokform (Endung "-ana"); häufen sich dabei die Vokale, wird ein "-l-" eingefügt. Beispiel: "chukua" ([etwas] tragen), "chukuliwa" (getragen werden), "chukulia" (jemandem [etwas] tragen), "chukuza" (jemanden etwas tragen lassen), "chukuana" (einander unterstützen). Diese Endungen können auch kombiniert werden, z. B. "chukuliana" (einander [etwas] tragen), die Reziprokbildung zum Applikativ.

Es gibt reine Tempora wie im Deutschen (Vergangenheiten, Gegenwartsformen, Zukunft) und Tempora in Abhängigkeit vom Kontext (Gleichzeitigkeit, Abfolge), außerdem Formen für nicht realisierte Möglichkeiten ähnlich dem deutschen Konjunktiv sowie einen Optativ. Alle diese Tempora und Modi werden jeweils durch ein Präfix markiert, das zwischen die anderen Präfixe gesetzt wird, und zwar hinter Subjekt- oder Verneinungspräfix und vor Relativpronomen und Objekt soweit vorhanden, sonst vor die Wurzel.

In den meisten Wörterbüchern zum Swahili wird nur die Verbwurzel aufgeführt (bspw. "-kata" mit der Bedeutung „schneiden“). Im einfachen Satz werden Präfixe für die Person und das Tempus angehängt ("ninakata"). "ni-" steht für die 1. Person Singular („ich“) und "-na-" markiert das Tempus "Bestimmte Zeitform" – im Allgemeinen mit "Präsens" im progressiven Aspekt zu übersetzen.

Dieser Satz kann nun durch Austausch der Präfixe verändert werden.

Als weiteres Tempus gibt es eine Präsensform, die nicht mit dem genannten zu verwechseln ist: "Nasoma" ist im Standard-Swahili keine Verkürzung von "ninasoma" („Ich lese gerade“), es enthält stattdessen eine Zeitform, die mit dem Präfix "–a-" gebildet wird. "Nasoma" (assimiliert aus *"Ni-a-soma") bedeutet in etwa „Ich lese (für gewöhnlich)“/„Ich kann lesen“. Dieses Tempus wird auch als "Unbestimmte Zeitform" oder "gnomisches Präsens" bezeichnet; es ist eigentlich die allgemein bejahende Zeitform.

Die Liste aller "Subjekt"-Präfixe für die "m-/wa-"-Klasse (1./2. – „Menschen“):

Die gebräuchlichsten "Tempus"- und "Modus"-Präfixe sind:

Zwei Tempora bzw. Modi haben kein Tempus/Modus-Präfix, sondern ändern den Schlussvokal "-a" ab. Genau genommen handelt es sich bei dem Wörterbucheintrag "-soma" „lesen“ also nicht um die reine Wurzel, sondern um die Wurzel mit der Endung "-a". "-a" steht für den Indikativ. Einen anderen Schlussvokal haben die allgemeine Verneinung mit der Endung "-i" und der Optativ mit der Endung "-e" (Beispiele unter Optativ in Swahili).

Die Präfixe "-ki-", "-nge-" und "-ngali-" werden auch als Konditionalformen verwendet. Sie übernehmen dann die Rolle, die im Deutschen die Konjunktion „wenn“ hat:

Analog: "ningenunua" (wenn ich kaufen würde) und "ningalinunua" (wenn ich gekauft hätte).

Mit dem "Objekt"-Präfix kann ein drittes Affix an die Wurzel treten. Es steht direkt vor der Wurzel und muss gesetzt werden, wenn das Objekt definit (bestimmt) ist und kann Objektpronomen ersetzen.

Mit weiteren Präfixen kann man Relativpronomen hinzufügen, die sich auf Subjekt, Objekt, Zeit, Ort oder Art und Weise der Handlung beziehen. Auch die Verneinung ist ein weiteres Präfix, und zwar "ha-" vor der Subjektsilbe oder "si-" nach ihr, je nach dem Tempus-/Modus-Präfix, das bei Verneinung auch wegfallen oder sich ändern kann.

In dem ausgedehnten Sprachgebiet des Swahili zwischen Somalia, Mosambik und den Inseln des Indischen Ozeans entwickelte sich eine Vielzahl von Dialekten. Seit dem Ende des Ersten Weltkriegs war erstmals fast der größte Teil des Gebietes der Swahilikultur politisch durch die britische Herrschaft vereint. In den 1920er Jahren trieb die Kolonialverwaltung eine Vereinheitlichung des Swahili voran. Seit 1928 galt der in Sansibar gesprochene Kiunguja-Dialekt als Grundlage für das Standard-Swahili. Davon abgesehen umfasst die Sprache mehr als fünfzig unterschiedliche Dialekte, darunter:





</doc>
<doc id="8953" url="https://de.wikipedia.org/wiki?curid=8953" title="Turksprachen">
Turksprachen

Die Turksprachen – auch "türkische Sprachen" oder "Türksprachen" genannt – bilden eine in Eurasien weit verbreitete Sprachfamilie von rund 40 relativ nah verwandten Sprachen mit etwa 180 bis 200 Mio. Sprechern. Sie gliedern sich in eine südwestliche "(oghusische)," südöstliche "(karlukische" oder "uigurische)," westliche "(kiptschakische)" und nordöstliche "(sibirische)" Gruppe, außerdem in die Zweige Arghu und Bolgar-Türkisch.

Mit den mongolischen und tungusischen Sprachen werden die Turksprachen zur Gruppe der altaischen Sprachen zusammengefasst. Ob „Altaisch“ eine genetische oder nur eine areale Einheit bildet, ist bis heute ungeklärt. Die Mehrheit der Forscher geht von einem arealen Sprachbund aus.
Die Turksprachen sollten trotz der selteneren Bezeichnungen "Türksprachen" und "türkische Sprachen" nicht mit der Einzelsprache "Türkisch" verwechselt werden, die nur eine – allerdings die sprecherreichste – von etwa 40 Sprachen dieser Sprachengruppe darstellt. Es handelt sich bei den Turksprachen auch nicht um Dialekte des Türkischen, sondern um im (ethno-)linguistischen Sinne eigenständige Sprachen, die ihrerseits in der Regel eine dialektale Gliederung aufweisen, allerdings mit dem Türkischen verwandt sind.

Mit insgesamt etwa 40 Sprachen, die von 180 Millionen Menschen als Muttersprache gesprochen werden (bis zu 200 Millionen mit Zweitsprechern), bildet die Familie der Turksprachen die mit Abstand größte und bedeutendste der drei Untergruppen des Altaischen. Sie ist – nach der Zahl ihrer Sprecher – die siebtgrößte Sprachfamilie weltweit (nach Indogermanisch, Sinotibetisch, Niger-Kongo, Afroasiatisch, Austronesisch und Drawidisch).

Die meisten Turksprachen sind sich in der Phonologie, Morphologie und Syntax sehr ähnlich, allerdings weichen "Tschuwaschisch, Chaladsch" und die nordsibirischen Turksprachen "Jakutisch" und "Dolganisch" nicht unerheblich von den übrigen ab. Zwischen den Sprechern der meisten Turksprachen ist eine partielle wechselseitige Verständigung möglich, vor allem wenn sie zur gleichen Untergruppe gehören (zur Klassifikation vgl. den nächsten Abschnitt). Diese relativ große Ähnlichkeit der Sprachen erschwert die klare Festlegung von Sprachgrenzen, zumal zwischen Nachbarsprachen meist Übergangsdialekte bestehen. (Häufig werden diese Grenzen künstlich durch politische Entscheidungen und Zugehörigkeiten gezogen.) Auch die innere genetische Gliederung der Turksprachen ist wegen ihrer Ähnlichkeit und intensiven wechselseitigen Beeinflussung problematisch, was zu unterschiedlichen Klassifikationsansätzen geführt hat.

Die Turksprachen sind über ein riesiges Gebiet in Ost- und Südosteuropa sowie West-, Zentral- und Nordasien verbreitet ("siehe" Verbreitungskarte). Dieses Gebiet reicht vom Balkan bis nach China, von Zentralpersien bis zum Nordmeer. In rund dreißig Ländern Eurasiens werden eine oder mehrere Turksprachen in nennenswertem Umfang gesprochen, bemerkenswert ist der hohe Anteil Türkischsprechender in Deutschland und im sonstigen Westeuropa aufgrund der Migrationen der letzten Jahrzehnte. 

Die drei mit Abstand größten Turksprachen sind:
Weitere Turksprachen mit mehr als einer Million Sprecher:

Die Sprecherzahlen stammen vom März 2006 aus diversen geprüften Quellen. 5 % bis 10 % höhere Werte sind durch den zeitlichen Abstand zwischen Ermittlung und Veröffentlichung möglich.

Wie eng die Turksprachen miteinander verwandt sind – wenn man von Tschuwaschisch, Chaladsch und den nordsibirischen Turksprachen absieht – zeigt bereits ein Blick auf die folgende Tabelle, die einige Wortgleichungen des Grundwortschatzes für die Sprachen Alttürkisch (der ersten schriftlich überlieferten Vorstufe "aller" heutigen Turksprachen, also nicht nur des Türkei-Türkischen), Türkisch, Aserbaidschanisch, Turkmenisch, Tatarisch, Kasachisch, Usbekisch und Uigurisch enthält. 

"Vergleich einiger Grundwörter in wichtigen Turksprachen" 
Die Turksprachen Türkisch, Aserbaidschanisch, Turkmenisch, Kasachisch, Kirgisisch und Usbekisch sind Nationalsprachen in ihren jeweiligen Staaten. Einen besonderen Status als offizielle Regionalsprachen autonomer Republiken oder Provinzen haben darüber hinaus folgende Turksprachen: in Russland Tschuwaschisch, Kumykisch, Karatschai-Balkarisch, Tatarisch, Baschkirisch, Jakutisch, Chakassisch, Tuwa, Altaisch; in China Uigurisch und in Usbekistan Karakalpakisch.

Turksprachen werden in etwa 30 Staaten Europas und Asiens gesprochen. Die Tabelle zeigt ihre Verbreitung in den einzelnen Staaten. Die Sprachen sind nach Unterfamilien angeordnet ("siehe:" Klassifikation).

Einige Turksprachen sind in ihrer Existenz stark gefährdet, da sie nur noch von wenigen meist älteren Menschen gesprochen werden. Direkt vom Aussterben in den nächsten Jahren bedroht sind das südsibirische "Tofa" oder "Karagassische," das "Karaimische" in Litauen, das "Jüdisch-Krim-Tatarische" und das "Ili Turki" in Nordwestchina (Ili-Tal). Nur noch einige Tausend Sprecher haben das "Aynallu" im Iran, das "Yugur" (Gansu-Provinz) und "Ainu" (bei Kaschgar), beide China, das nordsibirische "Dolganisch" und das südsibirische "Tschulymisch" am Tschulym-Fluss nördlich des Altai. Alle anderen Turksprachen sind relativ stabil, die Sprecherzahlen der großen Sprachen nehmen zu.

Die relativ große Ähnlichkeit und intensive wechselseitige Beeinflussung der Turksprachen sowie die hohe Mobilität der Turkvölker erschwert die klare Festlegung von Sprachgrenzen und die innere genetische Klassifizierung, was zu unterschiedlichen Klassifikationsansätzen geführt hat. Dennoch haben sich heute relativ stabile und gleichartige Einteilungen ergeben, die alle letztlich auf den russischen Linguisten Alexander Samoilowitsch (1922) zurückgehen. Obwohl Klassifizierungen grundsätzlich genetisch sein sollten, spielt bei der Gliederung der Turksprachen die geographische Verteilung eine große Rolle. Zur Frage der Verwandtschaft der Turksprachen mit den mongolischen und tungusischen Sprachen "siehe" altaische Sprachen.

Das "Tschuwaschische" bildet (zusammen mit dem ausgestorbenen Bolgarischen) einen eigenen „bolgarischen“ Zweig der Turksprachen, der dem Rest der Familie (Turksprachen i. e. S.) mit relativ weitem Abstand gegenübersteht. Einige Forscher hielten das Tschuwaschische nicht einmal für eine „richtige“ Turksprache, da es so stark von allen anderen abweicht. Ob dieser große Unterschied auf eine frühe Abspaltung des bolgarischen Zweigs von den anderen Turksprachen oder auf eine längere Phase der sprachlichen und kulturellen Isolierung zurückzuführen ist, konnte bisher nicht geklärt werden. Ein Merkmal dieser Trennung ist die Opposition von finalem tschuwaschischem /-r/ zu gemeintürkischem /-z/, zum Beispiel die Finalkonsonanten in

Das Tschuwaschische wird vor allem im europäischen Teil Russlands östlich von Moskau in der autonomen Republik Tschuwaschien im großen Wolgabogen von 1 Mio. Sprechern gesprochen. Weitere Tschuwaschen gibt es in Tatarstan und Baschkirien (insgesamt 1,8 Mio. Sprecher). Die Tschuwaschen sind überwiegend russisch-orthodoxen Glaubens, verwenden in ihren eigenen Print- und Rundfunkmedien neben der kyrillischen Schrift auch ein angepasstes Lateinalphabet und sprechen überwiegend Russisch als Zweitsprache. Sie betrachten sich kulturell und historisch als Nachfolger der Wolga-Bolgaren, was aber fraglich ist.

Von den restlichen Turksprachen weicht das "Chaladsch" am stärksten ab. Es ist – nach der heute weitgehend akzeptierten Auffassung Gerhard Doerfers – der einzige noch existente Vertreter des Arghu-Zweiges der Turksprachen, der ebenfalls früh isoliert wurde und dann im Laufe des 13. Jahrhunderts in der zentraliranischen Provinz auftritt – umgeben von Sprechern des Persischen. Heute wird Chaladsch von etwa 40.000 Menschen in der iranischen Zentralprovinz zwischen Qom und Akar gesprochen und ist nach linguistischen Gesichtspunkten eine der interessantesten Turksprachen im Iran. Die frühe Isolation von anderen Turksprachen und die starke Beeinflussung durch das Persische haben einerseits archaische Merkmale erhalten (z. B. ein Vokalsystem mit drei Quantitäten kurz-mittellang-lang, Beibehaltung des anlautenden /h-/ und des alttürkischen Dativsuffixes /-ka/: chalad. "häv.kä" – türk. "ev.e" – „für das Haus“), andererseits zu verbreiteten Iranismen in Phonologie, Morphologie, Syntax und Lexikon (sogar bei einigen Zahlwörtern) geführt.

Die übrigen vier Gruppen der Turksprachen sind vor allem geographisch gegliedert, wobei für die Einteilung nicht die heutigen Siedlungsgebiete gelten, sondern die Frühphase der türkischen Sprachen nach ihren ersten Wanderungen und Siedlungsprozessen. Somit unterscheidet man "Kiptschakisch" oder Westtürkisch, "Oghusisch" oder Südwesttürkisch (die nach der Zahl ihrer Sprecher größte Gruppe mit den Sprachen Türkisch, Aserbaidschanisch, Turkmenisch, Kaschkai), "Karlukisch" oder Osttürkisch, Nordtürkisch, Nordosttürkisch und Bolgar-Türkisch. Kiptschakisch/Westtürkisch gliedert sich in drei Untergruppen: Kiptschak-Bulgarisch oder Uralisch, Kiptschak-Oghusisch oder Pontisch-Kaspisch und Kiptschak-Nogaisch oder Aralisch-Kaspisch.

Das Jakutische und Dolganische weichen aufgrund ihrer langen Isolierung im Grundwortschatz stark von den restlichen Sprachen ab. Unterschiedlich sind auch Wortstellung und Satzbau. In dieser Hinsicht hat sich das Jakutische mehr den mongolischen und tungusischen Sprachen angeglichen. Außerdem fehlen alle Fremdwörter persisch-arabischen Ursprungs, die in anderen Turksprachen vorkommen.

Zur Ähnlichkeit der Sprachen trägt auch die lange arabisch-persische Prägung von Wortschatz und Idiomatik bei, die die meisten Turksprachen durch den Islam erfahren haben. Für die Turksprachen auf dem Gebiet der ehemaligen Sowjetunion kommen viele gemeinsame russische Fremd- und Lehnwörter hinzu.

Insgesamt ergibt sich für die Turksprachen nach der aktuellen Literatur folgendes Klassifikationsschema (mit Sprecherzahlen Stand 2006):


Neben den geographischen gibt es einige traditionelle linguistische Kriterien für die obige Klassifikation:


Die folgende Tabelle gibt eine Übersicht über den Grundwortschatz in etwa 60 Wortgleichungen, wie er sich in mehreren wichtigen Turksprachen realisiert. Als erste Spalte sind die erschlossenen Proto-Formen nach der etymologischen Datenbank von Starostin aufgeführt. (In vielen Fällen erkennt man, wie protosprachliches finales und intervokalisches /r/ – hier als /r̩/ gekennzeichnet – zu gemeintürkischem /z/ wurde, allerdings nicht im Tschuwaschischen. Statt des IPA-Codes /ɨ/ wird das türkische /ı/ verwendet.)

Die Tabelle zeigt deutlich das abweichende Verhalten des Tschuwaschischen und Jakutischen und die große Ähnlichkeit der übrigen Turksprachen. "Prototürkisch" bezeichnet hier die erschlossene Protoform aller Turksprachen, "Alttürkisch" ist eine frühe Form der Turksprachen, nicht speziell des Türkei-Türkischen. Lücken in der Tabelle bedeuten natürlich nicht, dass die entsprechende Sprache kein Wort für den Begriff hätte, sondern nur, dass dieser Begriff von einer anderen Wurzel gebildet wird und somit für den etymologischen Vergleich im Sinne einer Wortgleichung ausfällt.

Typologisch weisen die Turksprachen große Ähnlichkeit mit den beiden anderen Gruppen der altaischen Sprachen (Mongolisch und Tungusisch) auf, diese Merkmale sind also weitgehend gemeinaltaisch und finden sich zum Teil auch bei uralischen und paläosibirischen Sprachen. 

Die wichtigsten typologischen Charakteristika der Turksprachen sind:









Das Türkische zeigt ein für die Turksprachen typisches Phoneminventar von acht Vokalen und 20 Konsonanten.

Vokale

Die Vokale können nach ihrer Artikulationsstelle (vorn – hinten), Rundung (gerundet – ungerundet) und Höhe (hoch – tief) eingeteilt werden. Diese Klassifikation ist für die Vokalharmonie von entscheidender Bedeutung.

Konsonanten

Hier werden die Buchstaben des türkischen Alphabets verwendet, in eckigen Klammern [ ] stehen die Lautwerte.

Die bei den Turksprachen weitverbreitete Lautharmonie betrifft sowohl die Vokale als auch einige Konsonanten. Im Türkischen sind dies k, g, ğ und l.

Die Vokalharmonie, also die Angleichung der Suffixvokale an die Vokale des Stammes oder der vorhergehenden Silbe, soll am Beispiel des Türkischen gezeigt werden. Dort beruht die Vokalharmonie sowohl auf einer Angleichung der Artikulationsstelle (vorne-hinten) als auch einer Assimilation im Rundungstyp (gerundet-ungerundet) der betreffenden Vokale. Einige Suffixe werden gemäß der sogenannten "kleinen Vokalharmonie", andere gemäß der "großen Vokalharmonie" gebildet. Während die kleine Vokalharmonie im Suffix ein /e/ nach den vorderen Vokalen (e, i, ö, ü) in der vorherigen Silbe und ein /a/ nach den hinteren Vokalen (a, ı, o, u) vorschreibt, wird bei den Suffixen, die gemäß der großen Vokalharmonie gebildet werden, ein /i/ nach den vorderen ungerundeten Vokalen (e, i), ein /ü/ nach den vorderen gerundeten Vokalen (ö, ü), ein /ı/ nach den hinteren ungerundeten Vokalen (a, ı) und ein /u/ nach den hinteren gerundeten Vokalen (o, u) verwendet.

Beispiele







Die Unterscheidung zwischen gerundeten und ungerundeten Vokalen ist zwar im Türkischen allgemein gültig, nicht aber in allen Turksprachen. Auch im Türkischen gibt es Ausnahmen. Ein Beispiel ist das Wort „söylemek“ mit der Bedeutung „sagen, sprechen“. Der Präsensstamm lautet meist entsprechend der vorstehend vorgestellten "großen Vokalharmonie" „söyl-ü-yor“. Im Futur und im Partizip Präsens sind dagegen die ungerundeten Formen häufiger: „söyl-i-yecek“ und „söyl-i-yen“.

Das Türkische kennt für die hellen und dunklen Formen der Konsonanten k, g, ğ und l keine unterschiedliche Schreibweise, wohl verwenden einige Turksprachen für das dunkle k den Buchstaben q. Das dunkle ğ – der Buchstabe steht nur nach und zwischen Vokalen – wird im Übrigen nicht mehr gesprochen, die helle Variante ist ein flüchtiger j-Laut, das dunkle l lautet wie im englischen Wort „well“. Sollen vor den Vokalen a oder u k, g oder l hell gesprochen werden, erhält der Vokal einen Zirkumflex, z.B. „kâr“ „Gewinn“, aber „kar“ „Schnee“ oder „klâvye“ „Tastatur“.

Turksprachen haben in der Regel sechs Kasus: Nominativ (unmarkiert), Genitiv, Dativ-Terminativ, Akkusativ, Ablativ (woher?) und Lokativ (wo?). Die Kennzeichnung dieser Fälle erfolgt durch angehängte Kasusmarker, die innerhalb der einzelnen Sprachen sehr unterschiedlich ausfallen können. Dennoch gibt es eine erkennbare generelle Struktur, die auf die gemeinsame Protosprache zurückgeht und die in der "Markerformel" angegeben ist. (V bezeichnet einen Vokal, der sich nach der Vokalharmonie richtet, K einen beliebigen Konsonanten). Diese Struktur lässt aber für die konkrete Realisierung der Kasus in den einzelnen Sprachen einen relativ großen Spielraum. Die folgende Tabelle zeigt die Kasusmarkerformeln und ihre Realisierungen in drei Beispielsprachen Kirgisisch, Baschkirisch und Türkisch, die einige – aber nicht alle – Varianten der Formel umsetzen.

"Die Kasusmarkerformeln und ihre Realisierung in einigen Turksprachen"

Die Personalpronomina sind in allen Turksprachen sehr ähnlich. Im Türkischen lauten sie:

Besonders wichtig sind die Possessivsuffixe, die in den Turksprachen das Possessivpronomen ersetzen, in ähnlichen Formen aber auch in der Verbalmorphologie verwendet werden:

Am Beispiel des Türkischen wird die Konstruktion von Nominalphrasen gezeigt. Die Reihenfolge der Konstituenten ist dabei festgelegt. Es ergeben sich im Wesentlichen folgende Positionen:

"1 Attribut – 2 Nomen – 3 Ableitungssuffix – 4 Pluralmarker – 5 Nominalisierung – 6 Possessivsuffixe – 7 Kasusmarker"

Beispiele:






Eine typische Verbalform weist folgende Positionen auf:

"1 Stamm – 2 Tempus-/Modus-Marker – 3 Personalendung"

Die folgende Tabelle zeigt die Tempora und Modi des Verbs in den Turksprachen mit genereller Formel und Realisierung im Aserbaidschanischen und Türkischen (1. Sg. der Wurzel "al-" „nehmen, bekommen, kaufen“)

Beispiele komplexerer türkischer Verbalformen, die auch ganze Nebensätze ersetzen können:






Manche Wissenschaftler vertreten die Ansicht, dass sich bereits in den Verbänden der Hunnen, die seit dem 1. Jahrhundert nach Westen migrierten, Stämme befanden, die frühe Formen einer Turksprache sprachen. Massive Wanderungen von Turkvölkern lassen sich zweifelsfrei seit dem 8. Jahrhundert nachweisen. Der Gipfel der Migration türkischer Bevölkerungsgruppen nach Westen war die Landnahme Anatoliens im 11. Jahrhundert. Die letzte Migration türkischer Bevölkerungsgruppen war die der Jakuten, die im 12. Jahrhundert einsetzte. Die Sprache der Türken Südsibiriens (die Sprache, in der die ältesten turksprachigen Texte – die Orchon-Inschriften – aufgezeichnet wurden, also Alttürkisch) ist die einzige Sprachform, die vor den großen Wanderungen der türkischen Völker Eigenprofil gewonnen hat.
















</doc>
<doc id="8954" url="https://de.wikipedia.org/wiki?curid=8954" title="Türkische Sprache">
Türkische Sprache

Die türkische Sprache – auch Türkeitürkisch oder Osmanisch-Türkisch – ist eine agglutinierende Sprache und gehört zum oghusischen Zweig der Turksprachen. Als meistgesprochene Turksprache ist sie die Amtssprache in der Türkei, in der international nicht anerkannten Türkischen Republik Nordzypern und neben dem Griechischen auch in der Republik Zypern. Außerdem wird das Türkische als lokale Amtssprache in Mazedonien, Rumänien und im Kosovo gesprochen. Eigenbezeichnungen sind ', ' und "".

Die türkische Sprache selbst weist eine Reihe von Dialekten auf, von denen der Istanbuler Dialekt von besonderer Bedeutung ist. Seine Phonetik ist die Basis der heutigen türkischen Hochsprache. Bei der Einführung des lateinischen Alphabets für die türkische Sprache im Jahr 1928 wurde nicht auf die historische Orthographie des Osmanisch-Türkischen zurückgegriffen, sondern die Aussprache von Istanbul als Grundlage der Verschriftung herangezogen. Die Dialekte innerhalb der Türkei werden in Gruppen der Schwarzmeerregion ("Karadeniz Şivesi") sowie in Ost-, Südost- und Zentralanatolien ("Anadolu Şivesi") und an der Ägäis ("Ege Şivesi") eingeteilt.

Die Alternativbenennung „Türkeitürkisch“ umfasst aber nicht nur die Türkei, sondern auch alle Gebiete des ehemaligen osmanischen Reichs. Das bedeutet, dass auch die Balkan- oder Zyperntürken ein „Türkeitürkisch“ sprechen.

Das heutige Türkisch ist die Muttersprache von etwa 80 Prozent der Menschen in der Türkei (das waren Ende 2015 gut 63 Millionen Menschen) und (Schätzungen 1979) von 37.000 Menschen in Usbekistan, Kasachstan, Kirgisistan, Tadschikistan und Aserbaidschan. Türkisch war 2011 zudem die Muttersprache für 606.000 Menschen in Bulgarien, für ungefähr 290.000 Menschen in der Türkischen Republik Nordzypern und 1976 die von 128.380 Menschen in Griechenland.

63.600 Sprecher lebten 1984 in Belgien, etwa 70.000 in Österreich (Ethnologue 2009) und mehr als 1,5 Millionen in Deutschland. Ferner sprachen 1982 in Rumänien noch 14.000 Menschen die türkische Sprache, und auf dem Gebiet des früheren Jugoslawien, insbesondere im Kosovo, wo es auch Amtssprache ist, und in Mazedonien, sprechen 250.000 Personen türkisch.

1990 war Türkisch im Irak noch für rund 3.000 und im Iran für 2.500 Menschen die Muttersprache. In den USA lebten 1970 24.123 Sprecher des Türkischen, und für Kanada wurden 1974 8.863 türkische Muttersprachler angegeben. In Frankreich gaben 1984 rund 135.000 und in den Niederlanden knapp 150.000 Menschen Türkisch als Muttersprache an. 1988 wurden in Schweden rund 5.000 Türkischsprachige registriert.

2009 sprachen etwa 85 Millionen Menschen Türkeitürkisch, darunter 65 Millionen als Muttersprache und 20 Millionen als Zweitsprache.

Als engste Verwandte des Türkeitürkischen gilt heute das Aserbaidschanische. Vielfach wird die Sprache der südosteuropäischen Gagausen (Republik Moldau und Balkan) als ein Dialekt des Türkeitürkischen angesehen, was allerdings umstritten ist. So führen die Turkologen Westeuropas das Gagausische als eigene Sprache und jene in den Turkstaaten auf Grund des geringen Abstandes als einen Dialekt des Türkeitürkischen an. Gegenseitige mündliche wie schriftliche Kommunikation zwischen Sprechern des Türkischen, Aserbaidschanischen und Gagausischen ist ohne größere Schwierigkeiten möglich. Das Sprachverhältnis ist etwa vergleichbar mit der Verwandtschaft zwischen Dänisch und Norwegisch.

Einen etwas größeren Sprachabstand gegenüber dem Türkischen hat das Turkmenische, daher ist eine Unterhaltung beispielsweise zwischen Türkisch- und Turkmenischsprechern in Wort oder Schrift deutlich mühsamer. Das Verhältnis entspricht ungefähr dem Sprachabstand zwischen dem Schwedischen und dem Dänischen. Die sprachlichen Unterschiede rühren hauptsächlich daher, dass das Turkmenische, das bis heute dialektal stark zersplittert ist, unter dem Einfluss verschiedener Sprachen wie des Persischen und des Russischen, nicht zuletzt auch unter dem Einfluss nicht-oghusischer, zentralasiatischer Turksprachen, wie des Tschagataischen stand.

Aufgrund dieser unterschiedlichen Sprachabstände fasst man innerhalb der oghusischen Sprachen Türkisch, Gagausisch und Aserbaidschanisch als westoghusische Sprachen zusammen, während das Turkmenische einem ostoghusischen Zweig zugerechnet wird.

Grundzüge einer Rekonstruktion des Prototürkischen, auf das alle etwa 40 Turksprachen letztlich zurückgehen, bietet Róna-Tas „The Reconstruction of Proto-Turkic and the Genetic Question“ in Johanson & Csató eds. „The Turkic Languages“, Routledge Language Family Descriptions, 1998.

Alttürkisch ist die Sprache der Orchon-Inschriften und die erste bekannte Verschriftlichung des Uigurischen. Der Name "Alttürkisch" darf nicht im Sinne einer direkten Vorgängerform des heutigen Türkischen interpretiert werden. Das Alttürkische ist eine frühe Form der uighurischen oder südöstlichen Turksprachen, während das Türkische zur oghusischen oder Südwestgruppe der Turksprachen gehört. Zur Klassifikation siehe den Artikel Turksprachen.

Mahmud al-Kāschgharī verfasste im 11. Jahrhundert seine monumentale „Sammlung der Dialekte der Türken“ ("dīwān lughāt at-turk"). Dieses türkisch-arabische Wörterbuch enthält neben der Übertragung von Wortmaterial eine Fülle von historischen, geographischen und folkloristischen Einzelheiten.

Das Türkeitürkische geht direkt auf das Oghusische zurück, die Sprache der türkischen Stämme, die im Gefolge der Schlacht von Mantzikert im Jahre 1071 nach Anatolien einwanderten. Die Oghusen siedelten ab dem 8. Jahrhundert in Zentralasien am Unterlauf des Syr-Darya in der Kasachensteppe und islamisierten sich dort allmählich. Ein Teil dieser Stämme folgte im 11. Jahrhundert den Seldschuken bei deren Eroberungen in den Iran und das westliche Vorderasien (Syrien, Irak, später auch Kaukasien und Anatolien).

Die frühesten Werke in türkischer Sprache in Anatolien stammen vom 13. Jahrhundert und gehen damit der Gründung der osmanischen Dynastie kurz voraus. Die ersten Zeugnisse türkischer Sprache sind einige Verse von Sultan Veled, dem Sohn des Mystikers Dschalal ad-Din ar-Rumi. Die Seldschuken-Dynastie, die Anatolien ab dem Ende des 11. Jahrhunderts beherrschte, verwendete die türkische Sprache weder in der Verwaltung ihres Reiches, noch förderte sie ihre Verwendung in der Literatur. Erst nachdem die Macht der Seldschuken-Herrscher in Anatolien im 13. Jahrhundert durch eine Vielzahl türkischer Fürstentümer (Beyliks) ersetzt worden war, begann Türkisch als Verwaltungssprache in Anatolien verwendet zu werden, weil diese Herrscher des Arabischen nur unzureichend mächtig waren. Gleichzeitig kam es zu einem Aufschwung von Literatur in türkischer Sprache (siehe z. B. Yunus Emre). Das Persische, bislang die vorherrschende Sprache der Literatur und Poesie im seldschukischen Anatolien, verlor mit dem Hof der seldschukischen Herrscher seinen wichtigsten Mäzen.

Die türkische Sprache in ihrer „Standardform“ als Amtssprache des Osmanischen Reichs und als Sprache der osmanischen Literatur nahm ab dem Ende des 15. Jahrhunderts arabische und persische Elemente auf. Diese Entwicklung liegt an der damaligen Dominanz der arabischen und persischen Sprachen in der islamischen Hochkultur, die die osmanische Elite zum Nachahmen und Weiterentwickeln dieser Sprachen motivierte.

Im Jahre 1928 erfassten die laizistischen und kemalistischen Reformen nach der Ausrufung der Republik Türkei im Jahr 1923 auch die Sprache. Die Einführung der lateinischen Schrift für die türkische Sprache in Staaten der Sowjetunion erleichterte die Verwendung des lateinischen Alphabets auch in der Türkei. Die Kontakte zu anderen Turkvölkern jenseits der Grenze sollten gewahrt werden. Erwägungen, die arabische Schrift des Türkischen zu reformieren oder sogar durch die lateinische Schrift zu ersetzen, waren in der Türkei allerdings nicht neu. In der Tanzimatära hatte bereits der Bildungsminister Münif Pascha mit diesem Gedanken gespielt. Münif Pascha sah die arabische Schrift als Ursache für den verbreiteten Analphabetismus in der damaligen Türkei. Dieser Schritt vergrößerte die Distanz zur osmanischen und mehr noch zur islamischen Vergangenheit der Türken. Die Säkularisierung der modernen Türkei setzte sich fort.

Nach der Gründung der Türkischen Republik 1923 begann man in den 1930er Jahren, fremde Lehnwörter durch teils bereits vorhandene, teils neugebildete türkische Wörter zu ersetzen. Diese Ersetzungen des hergebrachten Wortschatzes sind bis heute nicht vollständig durchgeführt, so dass sich immer noch viele Wörter arabischen und persischen Ursprungs finden. In vielen türkischen Dialekten sind Gräzismen vertreten, die in der bäuerlichen Terminologie oder in der Seefahrt-, Fischfang-, Weinbau-, Bienenzuchtterminologie vorkommen. Armenismen kommen in türkischen Dialekten seltener vor. Seit dem 19. Jahrhundert kam vor allem Vokabular französischer, im 20. Jahrhundert auch englischer Herkunft hinzu.

Die "Türk Dil Kurumu", die "Gesellschaft der türkischen Sprache", ist eine staatliche Einrichtung, die 1932 zur Returkisierung bzw. Modernisierung der türkischen Sprache gegründet wurde. Oberstes Ziel dieser Gesellschaft war anfangs, zahlreiche arabische und persische Wörter durch traditionelle türkische Entsprechungen und, wenn es diese nicht gab, ohne Rücksicht auf die anderen Turksprachen durch eigens geschaffene „neu-türkische“ Wörter zu ersetzen. Die 1951 entstaatlichte und mit der Verfassung von 1982 der "Atatürk Kültür, Dil ve Tarih Yüksek Kurumu" unterstellte und wieder unter staatlichen Einfluss gebrachte "Türk Dil Kurumu" betreibt heute keine Sprachreformpolitik mehr.

Das Phonem (normalerweise "yumuşak g" genannt („weiches g“)), "ğ" erscheint niemals am Wortanfang, sondern folgt stets einem Vokal. Am Wortende oder vor Konsonanten zeigt es die lange Aussprache des vorhergehenden Vokals an.

In Wörtern türkischen Ursprungs stellen die Laute , und Allophone von , und dar; die ersteren erscheinen vor Vorderzungenvokalen, die letzteren vor Hinterzungenvokalen. Die Verteilung dieser Phoneme ist jedoch in Wörtern und Eigennamen fremdsprachlichen Ursprungs oft unvorhersehbar. In solchen Wörtern erscheinen , und oft vor Hinterzungenvokalen.

Das Türkische ist (ähnlich wie das Deutsche) auslautverhärtend, d. h. stimmhafte Laute werden am Ende eines Wortes stimmlos, so lautet der eigentliche Stamm von "kebap" "kebab-".

Die Vokale der türkischen Sprache sind, in ihrer alphabetischen Reihenfolge, "a", "e", "ı", "i", "o", "ö", "u" und "ü". Der Buchstabe "e" wird sowohl regelmäßig für den ungerundeten fast offenen Vorderzungenvokal, als auch seltener für den ungerundeten halbgeschlossenen Vorderzungenvokal verwendet. Das ohne Punkt ist der ungerundete geschlossene Hinterzungenvokal . Im Türkischen gibt es keine Diphthonge; wenn zwei Vokale aufeinandertreffen, was selten und nur in Lehnwörtern geschieht, wird jeder Vokal einzeln ausgesprochen. Jedoch kann eine Art Diphthong auftreten, wenn das "yumuşak g" zwischen zwei Vokalen steht. So kann das Wort "soğuk" („kalt“) von manchen Sprechern [soʊk] ausgesprochen werden.

Die älteste türkische Schrift ist die türkische Runenschrift mit 38 Zeichen.

Ab dem 10. Jahrhundert galten die Oghusen als islamisiert, und sie übernahmen die arabische Schrift, die um vier von Persern hinzugefügte Konsonanten ergänzt war. Das osmanisch-türkische Alphabet beinhaltete darüber hinaus noch einen von den Türken selbst hinzugefügten Konsonanten, den "kāf-i nūnī" oder "sağır kef" (). 

Anfang 1926 nahm Mustafa Kemal Atatürk im aserbaidschanischen Baku an einem Kongress der Turkologen teil, bei dem unter anderem die Schaffung einer Lateinschrift für die Turkvölker gefordert wurde. Aserbaidschan hatte schon seit 1922 eine lateinisch-basierte Schrift: das einheitliche türkische Alphabet.

Seit 1928 wird das Türkeitürkische durch eine von Kemal Atatürk mit entwickelte Variante der lateinischen Schrift wiedergegeben. Atatürk nannte dieses neue Schriftsystem "Neues türkisches Alphabet". Grundlage für die Neuschreibung der Wörter wie für die allgemeine Sprachreform war der Istanbuler Dialekt. Bei den Schreib- und Ausspracheregeln existieren deshalb keine Ausnahmen.

Das heutige Alphabet des Türkeitürkischen umfasst 29 Buchstaben, wobei jedem Buchstaben ein Laut zugeordnet ist:

a b c ç d e f g ğ h ı i j k l m n o ö p r s ş t u ü v y z

Die in der lateinischen Schrift vorkommenden Buchstaben q, w und x sowie die in der deutschen Schrift verwendeten Buchstaben ä und ß kommen nicht vor. j erscheint nur in einigen Fremdwörtern wie "jakuzi" „Whirlpool“.

Die übrigen Laute werden wie im Deutschen ausgesprochen.

Bei der Bildung von Begriffen aus Wörtern, die auf Konsonanten enden, und bei Suffixen muss stets die Stimmhaftigkeit des Endkonsonants beachtet werden. Es wird unterschieden zwischen stimmhaften und stimmlosen Konsonanten. Die stimmlosen Konsonanten ç, f, h, k, p, s, ş und t lassen sich mit den Merksätzen Çift Haseki Paşa („doppelter Haseki-Pascha“) oder Fe Paşa çok hasta („Fe Pascha ist sehr krank“) einprägen. Wird nun ein Suffix an einen stimmlosen Konsonanten angehängt, so muss ein stimmhafter Konsonant im Suffixanlaut angepasst werden.

Umgekehrt verwandeln sich die stimmlosen Konsonanten p, t, k und ç im Auslaut eines Wortes oft in ihr stimmhaftes Pendant (b, d, g bzw. ğ und c), wenn vokalisch anlautende Suffixe angeschlossen werden. Davon sind besonders mehrsilbige Substantive betroffen und von diesen wiederum sehr viele, die auf -k enden.
Vor allem bei der Bildung von Lokativ- und Possessivverbindungen sowie Deklinationen spielt dies eine Rolle.

In einigen Wörtern wird der Zirkumflex (ˆ, türkisch "uzatma işareti") verwendet. Dieses Zeichen gibt die Länge des Vokals an und dient oft zur Unterscheidung zweier ansonsten gleich geschriebener Wörter (beispielsweise "adet" [Anzahl] gegenüber "âdet" [Gewohnheit]), ist jedoch heute in den meisten Fällen außer Gebrauch geraten. Der Zirkumflex kann auch die Palatalisierung eines Konsonanten anzeigen (beispielsweise "kâğıt" [Papier]) und findet sich dann häufiger.

"Vorgeschichte": Osmanische Sprache#Vokalharmonie

Eine Besonderheit der türkischen Sprache ist die Vokalharmonie, welche zwischen hellen und dunklen Vokalen unterscheidet. Das Gesetz der Vokalharmonie zieht sich durch die gesamte türkische Formenlehre. Ein rein türkisches Wort (bis auf wenige Ausnahmen) enthält nur Vokale aus der Reihe der hellen (e, i, ö, ü) oder der dunklen (a, ı, o, u) Vokale. Wird ein Suffix an ein Wort angehängt, muss es sich dem Vokal der letzten Silbe des Grundwortes angleichen.

Sie unterscheidet nur zwischen hellen und dunklen Vokalen. Die Vokale der Suffixe nach der kleinen Vokalharmonie sind zweifach und lauten e/a.

Die kleine Vokalharmonie tritt u. a. bei den Pluralsuffixen und einigen Kasussuffixen auf.

Bei der großen Vokalharmonie haben die Suffixe vier (statt zwei) mögliche Formen. Sie werden mit einem der Vokale i/ı/ü/u gebildet, der sich jeweils nach dem Vokal im Grundwort richtet. Es gilt folgendes Schema:

Die große Vokalharmonie erfolgt bei allen Personal- und Possessivsuffixen, beim Fragesuffix mi und bei den Kasussuffixen des Genitivs und des Akkusativs.

Als weiteres Beispiel für die große Vokalharmonie dient die Endung "-li/-lı/-lu/-lü"; („aus … stammend“):

' (der Berliner/die Berlinerin), aber: ', ', '.

Es kommt vor, dass infolge der Vokalharmonie mehrere Endungen mit dem gleichen Vokal aufeinander folgen (zum Beispiel ': "ihr seid unruhig", ': „ihr seid traurig, es tut euch leid“).

"Vorgeschichte": Osmanische Sprache#Konsonantenharmonie

Die stimmlosen Verschlusslaute t und k werden in ihre stimmhaften Entsprechungen umgewandelt, wenn ihnen ein Vokal folgt. Aus t wird d, aus k wird ğ. Beispiele: ' (gehen) ⇒ ' – er geht; ' – groß, ' – ich bin groß (s. Stimmhafte und stimmlose Konsonanten).

Die türkischen Sprachen sind agglutinierend und unterscheiden sich somit wesentlich von den indogermanischen Sprachen. Agglutination bedeutet, dass grammatische Formen durch eine (eindeutige) Endung angezeigt werden. Dabei können mehrere Endungen aufeinander folgen, wobei die Reihenfolge festgelegt ist.

Beispiel:
" – „Sie sollen den Drachen nicht runterschießen.“ (Filmtitel)

Man könnte den Satz wie folgt zerlegen:  " – "[Ein/Den] Drachen-den runterschießen-nicht-sollen-sie."Die Endung "-yı" zeigt den bestimmten Akkusativ an; "-ma" steht für die Verneinung; "-sın" steht für den Imperativ, "-lar" für die 3. Person Mehrzahl.

Weiterhin kennt das Türkische keinen Artikel und kein grammatisches Geschlecht. Zur Kennzeichnung einer einzelnen, individuellen, aber nicht weiter bestimmten Sache (das sind Fälle, in denen im Deutschen der unbestimmte Artikel verwendet wird), kann im Türkischen das Zahlwort "bir (eins)" stehen. Unbestimmtheit kann auch durch andere grammatische Mittel ausgedrückt werden, dabei bleibt aber offen, ob es sich um einen oder mehrere Gegenstände handelt.

Türkisch weist die Satzstellung Subjekt – Objekt – Verb auf, ist also eine SOV-Sprache. Eine weitere Besonderheit für Sprecher der meisten europäischen Sprachen ist, dass es keine Präpositionen gibt, sondern ausschließlich Postpositionen verwendet werden, Beispiele: ' – "für Fatma"; ' – "wie (eine) Rose".

"Vorgeschichte": Osmanische Sprache#Fälle

Im Türkischen zählt man gemeinhin sechs Fälle: Nominativ, Genitiv, Dativ, Akkusativ (bestimmt: eigene Endung; unbestimmt: formengleich mit dem Nominativ), Lokativ und Ablativ. Die entsprechenden Endungen sind

Zur Kennzeichnung eines Plurals wird das Suffix -lar, -ler verwendet. Es tritt vor allen anderen Suffixen, also auch vor den Kasussuffixen, unmittelbar an den Wortstamm. Bsp.: ' – der Hodscha, ' – die Hodschas; ' – der See, ' – die Seen; "" – in den Seen (Lokativ Plural).

Gelegentlich sind im Türkischen noch Reste von Kasusbildungen vorhanden, deren Suffixe aber nicht mehr produktiv sind und die nur mehr in festen Redewendungen und bei bestimmten Vokabeln vorkommen. Hierzu zählt ein archaischer Instrumentalis mit der Endung "-in" / "-ın"; diese Form ist in wenigen Worten heute noch anzutreffen, z. B. "yazın" (im Sommer, sommers), kışın (im Winter, winters) "gelmeksizin" (ohne zu kommen). In früherer Zeit war dieser Kasus noch weiter verbreitet.

Weiter werden gelegentlich auch Ableitungen, die anderwärts der Wortbildung zugerechnet werden, als eigene Kasusbildungen geführt, wie die auf "-ce"/"-ca" als Äquativ oder Relativ oder auf "-siz"/"-sız"/"-suz"/"-süz" als Abessiv. Zusammenziehungen mit der Postposition "ile" (mit), die unter Wegfall des anlautenden "i-" in der Form "-le"/"-la" enklitisch an das Bezugswort treten, werden von Korkut Buğday als Instrumentalis bezeichnet, anderwärts aber zutreffender als Komitativ. Die angeführten Beispiele: "halk ile" ⇒ "halkla" (mit dem Volk), "eşim ile" ⇒ "eşimle" (mit meinem Partner); nach Vokal verwandelt sich das -i- zum -y-, "ümidi ile" ⇒ "ümidiyle" (mit der Hoffnung), "araba ile" ⇒ "arabayla" (mit dem Wagen) sind oft keine Antworten auf die Frage "Womit?", sondern auf die Frage "Mit wem/was?".

Im Türkischen werden alle Kategorien der Konjugation, nämlich Genus verbi, Tempus und Person, jeweils durch verschiedene aneinanderzureihende Suffixe ausgedrückt. Lediglich Person und Numerus werden durch dieselbe Klasse von Suffixen ausgedrückt, es gibt also nur ein Suffix für die 1. Person Plural und nicht ein Suffix für die 1. Person generell und eines zusätzlich für den Plural. Eine Ausnahme von dieser Regel betrifft die 3. Person. Hier ist das Personalsuffix (in der Mehrzahl der Tempora wird die 3. Person allerdings durch die Grundform, also ohne Suffix, ausgedrückt) in Singular und Plural gleich und der Plural wird durch das Pluralsuffix -ler/-lar wiedergegeben. Der Plural der 3. Person wird beim Prädikat aber nur dann zum Ausdruck gebracht, wenn der Satz kein ausdrückliches pluralisches Subjekt enthält, das Subjekt also im Prädikat inhärent enthalten ist.

Beispiele für die 3. Person:
im Singular:
Ahmet geliyor - Ahmet kommt. Mit inhärentem Subjekt: Geliyor - "Er" kommt. Mit Verwendung des Personalpronomens: "O" geliyor - "Er" kommt.
im Plural:
Öğrenci"ler" geliyor - "Die Schüler" komm"en". Mit inhärentem Subjekt: Geliyor"lar" - "Sie" komm"en". Mit Verwendung des Personalpronomens: "Onlar" geliyor - "Sie" komm"en".

An Genera Verbi gibt es im Türkischen weit mehr als im Deutschen. Neben dem Passiv existieren ein Kausativ, ein Reflexiv und ein Reziprok. Die Suffixe können auch kombiniert werden.

Beispiele:
für Kausativ: " – zur Welt kommen, " – gebären, " – entbinden, " – entbinden lassen
für Reflexiv: " – lieben, " – sich freuen, (Kombination mit Kausativ:) ' – erfreuen, (mit Passiv:) ' – erfreut werden
für Reziprok: ' – küssen, ' – sich küssen

Diesen Genera verbi stehen die Verneinungs- und die Unmöglichkeitformen nahe: Grundform: ' – kommen, verneinte Form: ' – nicht kommen, Unmöglichkeitform: "" – nicht kommen können.

Im Türkischen gibt es kein etwa dem Deutschen oder Lateinischen vergleichbares, auf die Einteilung in Präsens, Perfekt und Futur gestütztes Tempussystem. Auch ein Modus als eigenständige Kategorie ist nicht vorhanden. Dafür haben aber die türkischen Tempora auch modale oder aspektbezogene Bedeutungen, einige haben sogar hauptsächlich modale Bedeutung. An einfachen Tempora, die eine zeitbezogene Bedeutung haben, sind zu nennen: (bestimmtes) Präsens, Aorist, (bestimmtes) Präteritum, Perfekt, auch unbestimmtes Präteritum genannt, und Futur. An weiteren „Zeitformen“ existieren der selten gewordene Optativ, der Nezessitativ (Notwendigkeitsform) sowie der Konditionalis. Im Gegensatz zu dem Konditional der indoeuropäischen Sprachen bezeichnet der Konditionalis im Türkischen nicht die (im Hauptsatz stehende) bedingte Handlung, sondern die (im Konditionalsatz stehende) bedingende Handlung.

Mit Ausnahme der Konditionalsätze werden Nebensätze fast nur durch Verbalnomina (Partizipien und Infinitive) sowie sogenannte Konverben ausgedrückt. Diese können auch ein eigenes Subjekt haben.

Die Konjugation türkischer Verben erfolgt nach sehr festen Gesetzmäßigkeiten. In der nachfolgenden Tabelle sind am Beispiel von gelmek ("kommen") die einfachen Tempora dargestellt.

Aus diesen Grundformen lassen sich durch beliebig viele Kombinationen mit Hilfsverben eine fast unbegrenzte Zahl weiterer Zeiten bilden, deren nuancenhaften Unterschiede im Deutschen oft kaum wiederzugeben sind. Das gebräuchlichste Hilfsverbum ist die Kopula "sein", die in eigenen Formen nur im Präteritum ("idi"), im Perfekt ("imiş") und im Konditionalis ("ise") (sowie als Konverb "iken") vorkommt. Die Formen des Hilfsverbums werden der modifizierten Form nachgestellt, übernehmen von dieser die Personalsuffixe und verschmelzen mit dieser oft zu einem Wort und gleichen sich dann wie Suffixe entsprechend der Vokalharmonie an: "gelmiş idi" > "gelmişti". Für das Wort "bulmak" – finden lauten diese Formen "bulmuş idi" > "bulmuştu". Die Zeitformen der Kopula haben z. T. eine von den Zeitformen der Vollverben abweichende Bedeutung und allein stehend eine besondere Art der Verneinung. Eine Auswahl der zusammengesetzten Zeiten, die auch im Deutschen eine Entsprechung haben, ist im Folgenden aufgeführt.

Neben diesen Formen existieren weitere Zeiten, etwa Umschreibungen mit dem Infinitiv, auf die hier aber nicht näher eingegangen werden kann. Es bestehen auch weitere Kombinationsmöglichkeiten mit Hilfsverben.

Zum Beispiel lassen sich Präsens und Präteritum des Hilfsverbums ("idi") zur „-iyordu“-Vergangenheit, einem Imperfekt, kombinieren, das eine dauernde oder dauernd dargestellte Handlung in der Vergangenheit wiedergibt: geliyordu ("er kam", entspricht etwa: "he was coming" im Englischen) oder der Aorist mit "imiş": gelirmiş ("er soll kommen" als Ausdruck einer auf Mitteilungen anderer beruhender Erwartung). "İmiş" ist zwar formal ein Perfekt, hat aber keine Zeitbedeutung, sondern vermittelt lediglich den Aspekt des Hörensagens.

Die Verneinung wird in der Regel mit dem verneinten Verbalstamm (s. o.) gebildet. Eine Ausnahme bildet der Aorist, der ein eigenes Suffix für die verneinte Form besitzt. Z. B. lautet im Präteritum die verneinte Form von "geldi" – er ist gekommen "gelmedi" – er ist nicht gekommen. Im Aorist ist dies anders, die Verneinung von "gelir" – er kommt (schon) lautet "gelmez" – er kommt (definitiv) nicht. Weitere Ausnahmen sind die Kopula, die mit "değil" verneint wird, und das wichtige, zwischen Nomen und Verbum schwankende Wort "var" -vorhanden (sein), das zum Ausdruck für Besitz und Eigentum verwendet wird und mit "yok" ein eigenes Wort für die Verneinung besitzt.

Es gibt sowohl den vollen als auch den verkürzten Infinitiv. Der volle Infinitiv endet je nach Vokalharmonie auf -mek oder -mak. Der verkürzte Infinitiv endet auf -me oder -ma ("gelme" – das Kommen; "gitme" – das Gehen; "yumurtlama" – das Eierlegen; eskiden "kalma" – seit Alters her "geblieben"; "doğma büyüme" – geboren (und) aufgewachsen; "dondurma" – Speiseeis (wörtl.: eingefroren); "dolma" – gefüllt(-e Weinblätter/Paprikaschoten etc.)).

Im Türkischen werden die Personalsuffixe direkt an die Zeitformen angehängt. Die endungslosen Zeitformen von Aorist und Perfekt können auch adjektivisch als Partizipien verwendet werden:

"gelmiş-im" (mit Personalsuffix): ich bin gekommen (Perfekt), dagegen: "gelmiş bir tren" (als adjektivisches Partizip): ein (an)gekommener Zug

Die Personalsuffixe können auch unmittelbar an ein Nomen, gleich ob Substantiv oder Adjektiv, treten. In diesem Fall nehmen sie die Bedeutung einer Kopula an. Bei der Wahl der Suffixe ist auf die große Vokalharmonie zu achten.

In der dritten Person wird das Pluralsuffix weggelassen, wenn es zum Verständnis nicht erforderlich ist, weil etwa das Subjekt schon ein Plural ist: "Evler büyük." (Die Häuser sind groß.)

Die Zugehörigkeitsverhältnisse (Possessivverbindungen) werden im Türkischen so gebildet, dass direkt am betreffenden Nomen die Possessivendung angehängt wird. Dabei wird die große Vokalharmonie berücksichtigt. Ist der letzte Buchstabe des Wortes ein Konsonant, wird zudem auf dessen Stimmhaftigkeit geachtet.


Eine wichtige Rolle spielen die Possessivsuffixe bei der Bildung von Genitivkonstruktionen. Der Genitiv drückt aus, dass eine andere Sache oder Person der oder zu der Person oder Sache gehört, die im Genitiv steht. Dieses in Genitiv stehende Substantiv oder Pronomen wird vorangestellt, und das Wort, das die zugehörige Person oder Sache bezeichnet, nimmt das passende Possessivsuffix an.

Beispiele mit Hervorhebung des Genitiv- und des Possessivsuffixes:
Fällt das Genitivsuffix weg, verliert das betreffende Substantiv seine Individualität und wird zum Typ. Das Possessivsuffix der 3. Person (-(s)i/ı/ü/u) verknüpft dann zwei Substantive zu einem neuen Begriff.

Die Reihenfolge, in der die verwendeten Suffixe an Substantive angehängt werden, ist strikt definiert. Als erstes wird das Pluralsuffix angehängt, dann folgen der Reihe nach das Possessivsuffix, das Kasussuffix und zum Schluss ein Personalsuffix:
Das türkische Präsens hat dieselbe Bedeutung wie das deutsche Präsens. Das Besondere daran ist aber, dass es bei jedem Verb exakt gleich konjugiert wird und keine Ausnahmen kennt.

Bildungsregel: Verbstamm (+Bindevokal entsprechend der großen Vokalharmonie) + -yor + Personalsuffix -um/-sun/-/-uz/-sunuz/-lar

Beispiel gülmek (lachen):

Beispiel uyumak (schlafen):

Beispiel aramak (suchen):

Die „di-Vergangenheit“ hat die gleiche Funktion wie das Perfekt oder das Imperfekt im Deutschen und wird gebraucht bei abgeschlossenen Handlungen.

Bildungsregel: Verbstamm + -di/-dı/-dü/-du + Personalsuffix -m/-n/-/-k/-niz/-ler.

Beispiel gitmek (gehen)

Im Türkischen werden Satzanfänge, Eigennamen, Titel, Beinamen und Anreden groß geschrieben. Sprach-, Religions-, Volks-, Stammes- und Clanzugehörigkeit erscheinen ebenfalls in Großbuchstaben. Zusätzlich gibt es noch zahlreiche Sonderregeln.

Der Stamm von ursprünglichen türkischen Wörtern war in der Literatursprache des Osmanischen Reichs zu Gunsten von Lehnwörtern aus dem Persischen (Kunst, Kultur und Lebensart) und dem Arabischen (Religion) möglichst klein gehalten worden und galt als bäuerlich. Seit der zweiten Hälfte des 19. Jahrhunderts begann sich dies zunehmend zu ändern und gipfelte in der kemalistischen Sprachreform der 1930er Jahre. Nicht alle diese Lehnwörter konnten im modernen Türkisch durch alte türkische Wörter oder durch türkische Neuschöpfungen ersetzt werden. Der Umfang des Gebrauchs und der Verständlichkeit dieser Lehnwörter ist aber vom Kreis der Verwender und vom Publikum abhängig.

Die neueste Ausgabe des "Büyük Türkçe Sözlük" („Großes Türkisches Wörterbuch“), des offiziellen Wörterbuches der türkischen Sprache, veröffentlicht durch das Institut für die türkische Sprache Türk Dil Kurumu, beinhaltet 616.767 Wörter, Ausdrücke, Begriffe und Nomen.

Obwohl bei der kemalistischen Sprachreform viele arabische und persische Wörter durch türkische ersetzt worden sind, liefert die arabische Sprache neben dem Französischen besonders viele Lehnwörter. Viele der Lehn- und Fremdwörter arabischen Ursprungs sind über das Persische entlehnt.

Die folgenden statistischen Angaben nach einem türkischen Wörterbuch von 2005 erfassen alle Wörter der Schriftsprache.

Insgesamt 14,18 % (14.816 von 104.481) der Wörter im Türkischen sind Lehnwörter. Lehnwörter stammen aus folgenden Sprachen (Rangfolge nach der Anzahl der Wörter):

Im Jahre 1973 untersuchte die Wissenschaftlerin Kâmile İmer anhand fünf türkischer Tageszeitungen (Ulus, Akşam, Cumhuriyet, Milliyet und Hürriyet) den Wortgebrauch in der Presse, wo der sich stark verändernde Sprachanteil von Lehnwörtern ersichtlich wird:

Einige Beispiele für Lehnwörter aus anderen Sprachen:

Anzahl der türkischen Wörter in anderen Sprachen:

Beispielwörter mit türkischer Herkunft:









</doc>
<doc id="8955" url="https://de.wikipedia.org/wiki?curid=8955" title="Vokalharmonie">
Vokalharmonie

Die Vokalharmonie (auch "Synharmonismus") ist ein phonologischer Prozess, bei dem mehrere Vokale bezüglich bestimmter Merkmale aneinander angeglichen werden. 

Vokalharmonie bezeichnet die Erscheinung, dass der Vokal der Stammsilbe die Verwendung von bestimmten Vokalen in den Folgesilben des Wortes und in den Enklitika bedingt. Sie ist ein charakteristisches Lautgesetz in den finno-ugrischen Sprachen und den Turksprachen.

Im weiteren Sinn ist die Vokalharmonie jede Angleichung von Vokalen an den Artikulationsort oder die Artikulationsart eines anderen Vokals. Es handelt sich also um einen Assimilationsvorgang. Die Entstehung der Umlaute in den germanischen Sprachen fällt in diese Kategorie eines Assimilationsvorganges, nicht der Vokalharmonie im engeren Sinne. 

Der Vorgang ist, wie andere Assimilationsprozesse auch, an bestimmte Domänen gebunden, in den meisten Fällen an das phonologische Wort. In einigen Sprachen kann es vorkommen, dass nur bestimmte Vokale harmonieren, während andere für den Prozess transparent sind, was bedeutet, dass diese Vokale dem Prozess der Vokalharmonie nicht unterliegen.

In den meisten Fällen betrifft die Vokalharmonie die Merkmale gerundet/ungerundet bzw. geschlossen/offen.

Im Türkischen gibt es zwei verschiedene Allomorphe, um den Plural anzuzeigen, nämlich "-ler" und "-lar". Die Wahl des Allomorphs hängt vom letzten Vokal in der vorangehenden Silbe ab: "-lar" folgt auf a, ı, o und u; "-ler" auf e, i, ö und ü. So heißt es zum Beispiel „ev – evler“ (Haus – Häuser), aber „kitap – kitaplar“ (Buch – Bücher). Gleiches gilt für die den Infinitiv eines Verbs anzeigende Endung "-mek" (gelmek – kommen) bzw. "-mak" (konuşmak – sprechen). Dies stellt die sogenannte "kleine Vokalharmonie" "(küçük sesliler uyumu)" dar.

Faustregel: Buchstaben mit Punkten darüber (ö, ü, i und zusätzlich e [vgl. e=ä]) in der letzten Silbe erhalten die Endung -ler bzw. -mek.

Die "große Vokalharmonie" "(büyük sesliler uyumu)" unterscheidet vier statt zwei Fälle: Im Suffix werden nicht "a" oder "e", sondern "i", "ı", "ü" oder "u" eingesetzt, gemäß folgender Regel:

Finnisch hat drei Klassen von Vokalen:

Laut der Regel dürfen Vokale aus Gruppe 3 mit Vokalen aus jeweils einer der beiden anderen Gruppen kombiniert werden. Vokale der Gruppen 1 und 2 hingegen dürfen nicht im selben Wort vorkommen. Das bedeutet, dass Bildungen wie "tyttö", "katselmus" und "lyönti" den phonotaktischen Regeln des Finnischen genügen, "*kätselmus" und "*tytto" hingegen nicht. 

Bei Zusammensetzungen kann diese Regel verletzt werden; die Suffixe werden dann an den hinteren Teil angepasst. Kommen nur Vokale der Gruppe 3 im Wort vor, wird in der Regel das Suffix für die erste Gruppe genutzt.

Als Turksprache wie Türkisch hat Tatarisch ebenfalls eine ausgeprägte Vokalharmonie, die sich auf Suffixe sowie Postpositionen auswirkt: "bala-lar" – "Kinder", "äni-lär" – "Mütter"; "bala-sı – sein Kind", "äni-se – seine Mutter"; "bala-sız-lı – kinderlos", "äti-sez-le – vaterlos".

Zu den Sprachen mit Vokalharmonie gehören z. B. Turksprachen, finno-ugrische Sprachen, mongolische Sprachen, tungusische Sprachen, Koreanisch (nach einigen Theorien zu den tungusischen Sprachen gehörend bzw. abstammend), aber auch manche afrikanischen Sprachen wie das Tangale und einige Bantusprachen.




</doc>
<doc id="8956" url="https://de.wikipedia.org/wiki?curid=8956" title="Sinn des Lebens">
Sinn des Lebens

Die Frage nach dem Sinn des Lebens beinhaltet die auf einen Zweck gerichtete (teleologische) Bedeutung des Lebens im Universum an sich. Im engeren Sinn erfragt sie die biologische und sozio-kulturelle Evolution und insbesondere die Bestimmung des Homo sapiens. Im engsten Sinn forscht sie nach der „Deutung des Verhältnisses, in dem der Mensch zu seiner Welt steht“.

Sie ist eng verbunden mit den Fragestellungen „Woher kommen wir? Wohin gehen wir? Warum sind wir hier auf Erden?“ und weiter: „Wie sollen wir leben, um unseren Daseinszweck zu erfüllen?“ Hierbei wird diskutiert, ob diese durch eine äußere Institution vorgegeben ist, etwa ein göttliches Gebot, ob ein bestimmtes Verhalten der Natur entspringt, dass z. B. der Mensch dem Zweck der Fortpflanzung oder der Arterhaltung folgt, oder ob er gefordert ist, autonom ein selbstbestimmtes Leben zu führen und sich einen Lebensweg zu wählen, den er als sinnvoll erachtet. Sinnvoll erscheint ein Leben dann, wenn es einer idealen Wertvorstellung entspricht.

Zumeist wird die Frage nach dem Sinn des Lebens so verstanden, dass nach einem bestimmten Zweck gefragt wird, dem das Leben dienen soll, oder nach einem bestimmten Ziel, das angestrebt werden soll. Genauso kann nach dem Wert, dem Nutzen oder der Bedeutung des Lebens gefragt werden. Viele Missverständnisse bei den Versuchen, einen solchen Zweck zu bestimmen, haben ihre Ursache darin, dass versäumt wird, die Begriffe "Sinn" und "Leben" eindeutig und klar zu definieren. Hinzu kommt, dass man bezweifeln kann, ob eine vernünftige Antwort überhaupt möglich ist. Günther Anders hat dazu geschrieben: „Warum setzen Sie eigentlich voraus, dass ein Leben, außer da zu sein, auch noch etwas haben müsste oder auch nur könnte – eben das, was Sie Sinn nennen?“

Ein anderer Autor unterscheidet zuerst die Begriffswelt: "„Sinn und Zweck“, das wird oft zusammen gebraucht. Ziel ist fern, Zweck ist nah. Sinn ist tief, Zweck ist flach. Ziel ist erreichbar, Sinn nicht. Sex im Alter ist zwecklos, aber nicht sinnlos. Sinn ist ein Füllstand in einem Gefäß – ein „erfülltes Leben“, sagt man."

Eine Verständigung über das mit „Lebenssinn“ Gemeinte setzt zunächst eine sprachliche Klärung und Eingrenzung voraus. „Sinn“ ist ein mehrdeutiger Begriff, er kann entweder als teleologischer Sinn oder als rein sprachlicher Begriff („Sinn“ als Bedeutung einer Aussage) aufgefasst werden. Auch der Begriff „Leben“ muss geklärt werden, denn es kann das „Leben“ als biologisches Phänomen – insbesondere eines Kollektivs höherer Lebewesen (vgl. hierzu Anthropisches Prinzip) – oder das „Leben“ eines einzelnen Individuums gemeint sein.

Die Fragestellung nach dem Sinn des Lebens ist offenbar nur dem Menschen zu eigen. Schon in der antiken Philosophie hat man festgestellt, dass er offenbar das einzig bekannte sprachbegabte Vernunft- und Verstandeswesen ("zoon logon echon") ist, das die Voraussetzungen für eine selbstbezügliche Sinnreflexion vorweisen kann.

Der Mensch als gesellschaftliches Wesen ist mit der Erziehung, Ausbildung, dem Berufsleben usw. stets in Beziehung zu anderen Menschen und wird von klein auf zur Nützlichkeit, zur Arbeit, zu sinnvollem Handeln, zu einem sinnvollen Leben angehalten, oftmals ohne dieses ausdrücklich als Sinn seines Lebens benannt zu bekommen.

Viele Menschen stellen sich die Frage nach dem Sinn des Lebens im Alltag in der Regel nicht, solange die eigene Lebensführung nicht zweifelhaft oder fragwürdig wird. Häufig kommt es zu einer existenziellen Sinnkrise, wenn Ereignisse nicht mehr in das vorhandene Sinnkonzept integriert werden können: z. B. durch Enttäuschungen, Unglücke oder die Anforderungen eines neuen Lebensabschnitts. Die Folge ist oftmals der Beginn oder die Wiederaufnahme der Reflexion über den Lebenssinn, zu denen dann auch Fragen wie die nach dem Glück oder gar dem Sinn des Leidens gehören. Wenn eine Person ein kritisches Ereignis bewältigt, das zu einem Sinnverlust geführt hatte, findet sie für es häufig auch einen Sinn (s. Sinnfindung).

Der Tod macht das eigene Leben fraglich.

Viele Menschen, die sich an die Psychotherapie wenden, betrachten den Sinnverlust als Krankheit und äußern die Hoffnung, ihrem Leben einen neuen Sinn geben zu können. Die Empfindung völliger Sinnlosigkeit des eigenen Daseins, zusammen mit einem „Gefühl der Gefühllosigkeit“ und der inneren Leere, kann auch ein Symptom einer (schweren) Depression sein.

Eine grundsätzliche Schwierigkeit bei der Auseinandersetzung mit den Fragen nach Lebenssinn ist die prinzipielle Möglichkeit des menschlichen Verstandes, einen einmal angenommenen Standpunkt wiederum zu hinterfragen oder auch die Urteilsperspektive potenziell beliebig zu wechseln (vgl. Relativismus). Die Suche kann darum scheinbar endlos fortgesetzt werden (vgl. infiniter Regress), bzw. es entsteht der Eindruck, dass die Frage nicht – oder nicht endgültig  – zu beantworten wäre (Thomas Nagel). „Der Sinn des Lebens ist ein sinnvolles Wort; aber es läßt sich nichts Sinnvolles aussagen.“ Einige Sinnangebote erweisen sich aufgrund von Dogmatisierung oder Immunisierungsstrategien als mit Hilfe rationaler Argumente nur schwer oder gar nicht innerhalb ihres eigenen Systems kritisierbar. Ebenso lässt sich in manchen ideologischen Lehren kaum eine oder keine immanente theoretische Lücke entdecken. Die Beurteilung solcher Sinnangebote kann aber mittels externer Maßstäbe erfolgen.

Ein Problem, das in einer unendlichen Rekursion münden kann, ist die Frage, warum die meisten Sinnsuchenden nie nach dem Sinn der Lebenssinn-Frage selbst fragen: Warum stellen sich viele Menschen überhaupt die Frage nach dem Sinn ihres Daseins? Warum drängt es viele Menschen so sehr, sie zu beantworten? Was ist eigentlich der Gewinn oder der Verlust, wenn sie beantwortet oder nicht beantwortet wird? So könnte auch eingewendet werden, dass es sinnlos sei, so viel Zeit und Arbeit auf die Lösung dieses „Rätsels“ zu verwenden. Auf der anderen Seite scheint die Erklärung für das Sich-Stellen dieser Frage auf der Hand zu liegen: Fragt man nach dem Sinn, dann deswegen, "weil er verlorengegangen ist" – ohne diese „Verlorenheit“ ("Uneigentlichkeit" nach Heidegger) würde sich die Frage gar nicht erst stellen; es ist der Sinn einer jeden Frage, ihre Antwort zu finden.

Das Stellen der Frage nach dem Sinn des Lebens muss nicht zwingend eine positiv bestimmte Antwort nach sich ziehen. Verschiedenen psychologischen Ansätzen nach wählen viele Menschen den Weg der Verdrängung. Sie weichen einer Auseinandersetzung mit der Sinnfrage und letztlich auch mit sich selbst aus. So „funktionieren“ sie zwar im Alltag unauffällig weiter, doch haben sie, im Wortlaut der Existenzphilosophie, eine Existenzform der "Uneigentlichkeit" (Martin Heidegger), d. h. eine nicht authentische Lebensweise gewählt.

Eine andere Reaktion ist der Zynismus. Davon spricht man, wenn Menschen zwar eine große Sinnleere in ihrem Leben empfinden, das Leiden daran jedoch unterdrücken. Ihr Leben wird dann nur noch von Sachzwängen und dem Selbsterhaltungstrieb vorangetrieben (Peter Sloterdijk).

Wenn kein Sinn (mehr) im Leben gefunden bzw. gesehen werden kann, ist die Verzweiflung, so bei Kierkegaard, eine mögliche Reaktion. In einer solchen Verfassung droht das Leben zu scheitern. Ihre Ausprägungen können Depressionen und Suizid sein, d. h. eine chronische oder akute Lebensunfähigkeit, -verneinung oder -verweigerung.

Weiterhin gibt es (vor allem im Existentialismus) die Auffassung, das Leben habe keinen Sinn an sich (was a priori weder als gut noch als schlecht bewertet werden könne), so z. B. bei Albert Camus, für den das Leben grundsätzlich absurd ist. Wie Camus in "Der Mythos des Sisyphos" ausführt, widerspreche diese Vorstellung allerdings nicht notwendigerweise der Bejahung des Lebens und dem Glück des Menschen, das gerade in den nie endenden Anstrengungen gegen eine absurde Welt gefunden werden könne.

Theodor W. Adorno charakterisierte in seinem Werk "Negative Dialektik" die Frage nach dem Sinn des Lebens als eine der letzten, in denen die Kategorien der Metaphysik in der Welt nach dem Holocaust weiterlebten. Ihr Anspruch auf Objektivität widerspreche aber nahezu jeder Antwort, die erst subjektiv zu erschaffen und zu affirmieren wäre. Die Frage selbst sei Symptom einer objektiv sinnlosen Welt, in der Sinn für das subjektiv-individuelle Leben fehle. „Was ohne Schmach Anspruch hätte auf den Namen Sinn, ist beim Offenen, nicht in sich Verschlossenen; die These, das Leben habe keinen, wäre als positive genauso töricht, wie ihr Gegenteil falsch ist; wahr ist jene nur als Schlag auf die beteuernde Phrase.“ Nicht durch einen Sinneswandel hin zur Erfüllung, sondern nur durch die objektive Abschaffung des die Versagung erzeugenden Prinzips könne das menschliche Dasein von seiner Leere kuriert werden.

Die Auseinandersetzung mit dem eigenen Lebenssinn kann aber auch zu positiven Antworten führen. Voraussetzung für eine Antwort auf die Sinnfrage ist dabei die Annahme einer gewissen Freiheit des Menschen, den Sinn seines Lebens selbst zu (er)finden oder aus (z. B. von Gott) vorgegebenen Möglichkeiten zu wählen.

Für die Mehrzahl der Glaubensrichtungen ergibt sich der Sinn verbindlich aus ihren (z. B. auf Offenbarung zurückgehenden) Grundwahrheiten.
Einige Institutionen erheben ein Monopol auf die Sinnfrage und geben vor, was das Lebensziel zu sein hat und welche Rechte und Pflichten sich daraus für den Einzelnen ergeben. Eine in gewisser Weise ähnliche Situation kann sich auch in (im weitesten Sinne) totalitären sozialen Gemeinschaften oder Staaten vorfinden. Allerdings gehört es zum Wesen des Totalitarismus, den Menschen seine Sichtweise aufzuzwingen, während wahrer Glaube stets ein (Hilfs-)Angebot ist, das in Freiheit angenommen oder ausgeschlagen werden darf.

Nach Viktor Frankl kann der Mensch seinem Leben prinzipiell in jeder Situation Sinn abgewinnen oder geben, solange er bei Bewusstsein ist. Der ehemalige KZ-Häftling zieht aus dem Holocaust andere Schlussfolgerungen als Adorno und stellt fest: „Wer ein Warum zu leben hat, erträgt fast jedes Wie.“ Dies gilt selbst für Extremsituationen. „Was ist der Mensch? Er ist das Wesen, das immer entscheidet, was er ist. Er ist das Wesen, das die Gaskammern erfunden hat; aber zugleich ist er das Wesen, das in die Gaskammern gegangen ist, aufrecht und ein Gebet auf den Lippen.“ Ähnlich wie Frankl sah Karl Jaspers in der Freiheit und dem Bewusstsein der Verantwortung den entscheidenden Impuls, „der hier wie überall in der Aktivität des Daseins die Grenzsituationen überwindet, der ein positives Bewußtsein von den Grenzsituationen schafft, der das Erleben von Sinn, von Halt, von Notwendigkeit gibt, der daraus Kräfte für konkrete Lebensaktionen schöpft, sie aber nie verbindlich und zureichend in gegenständlicher Form für andere aussprechen kann.“ Eine wirklich freie Entscheidung können die meisten aber nur dann treffen, wenn ihre Grundbedürfnisse erfüllt sind (Abraham Maslow). Hunger, Durst, Schmerz, Angst, Unfreiheit usw. können daher sehr schnell zu einem Verlust an wahrgenommenem Lebenssinn führen. Die Befriedigung der menschlichen Bedürfnisse darf jedoch nicht mit der von diesen grundsätzlich "unabhängigen" Sinnsetzung bzw. -findung verwechselt werden.

Steht die eigene Person im Zentrum der Suche nach dem Lebenssinn, so können Wünsche nach Befriedigung körperlicher, materieller, sozialer und geistiger Bedürfnisse dominieren. Der Sinn kann z. B. im Streben nach Macht, Besitz, Ansehen, sowie Fortpflanzung, erfüllter Partnerschaft oder Selbstverwirklichung gesehen werden. Eine andere Ausprägung liegt vielleicht in der Suche nach Erkenntnis oder persönlicher Entwicklung.

Eine weitere Ausrichtung des Lebenssinns entsteht durch die Bedeutungsgebung im Hinblick auf andere Menschen bis hin zur Menschheit oder allgemein der Umwelt. Konkret kann es sich dabei etwa um Hilfe im weitesten Sinne handeln: die Weitergabe von Wissen und Fähigkeiten, alltäglich gelebte Mitmenschlichkeit oder auch um soziales oder politisches Engagement. Oftmals orientiert sich das Handeln an einem Ideal (z. B. Liebe, Harmonie oder Gerechtigkeit). Hermann Hesse formulierte hierzu: „Sinn erhält das Leben einzig durch die Liebe: das heißt: je mehr wir zu lieben und uns hinzugeben fähig sind, desto sinnvoller wird unser Leben.“ Ähnlich ausgerichtet ist die Formulierung von Dag Hammarskjöld: „Wage das Ja – und Du erlebst Sinn. Wiederhole das Ja – und alles bekommt Sinn. Wenn alles Sinn hat, wie kannst Du anderes leben als ein Ja.“

Wird die Antwort auf die Frage nach dem Sinn des Lebens nicht in menschlichen Belangen gesucht, so kann sie auch in philosophischen oder spirituellen Angelegenheiten gefunden werden. Fragen nach dem Sinn oder Ursprung allen Seins (Ontologie) spielen hier häufig eine Rolle, möglicherweise stehen die Suche nach Erleuchtung oder das Streben nach der Vereinigung mit dem Absoluten bzw. Gott im Mittelpunkt. Zu diesem Zweck könnte etwa Philosophie studiert, einer bestimmten Religionsgemeinschaft beigetreten, ein spezieller spiritueller Weg beschritten oder einem geistigen Vorbild nachgefolgt werden.

Die Auffassungen über den Sinn des Lebens, die in der Geschichte der Philosophie dargelegt und begründet wurden, können als repräsentativ für die nicht-religiösen Ansichten gelten. Einige der Antworten, die im Laufe der Zeit auf die Frage nach dem Lebenssinn gegeben wurden, sollen hier im Folgenden vorgestellt werden.

Der Sinn des Lebens bestand in der Philosophie der Antike in der Hauptsache in der Erlangung der Glückseligkeit "(eudaimonía)". Diese wurde gemeinhin als das höchste, erstrebenswerteste Gut angesehen. Unterschiede in den philosophischen Schulen ergaben sich vor allem aus der Definition dessen, was unter Glück zu verstehen sei, und der Art und Weise, wie man glaubte, dass dieses erreicht werden könnte.

Nach Platon (Politeia) besteht die unsterbliche menschliche Seele aus drei Teilen: der Vernunft, dem Mut und den Trieben. Nur wenn diese drei Seelenteile im Gleichgewicht sind und sich nicht gegenseitig widersprechen, kann der Mensch glücklich sein. Dann erreicht er die Gerechtigkeit als höchste der Kardinaltugenden. Der höchste Sinn liegt in der philosophischen Reflexion: „Ein Leben ohne Selbstprüfung verdient nicht gelebt zu werden.“

Sein Schüler Aristoteles betrachtete die Glückseligkeit nicht als statischen Zustand, sondern als ein stetiges Tätigsein der Seele. Vollkommenes Glück kann der Mensch nur im kontemplativen Leben ("bios theoretikos"), d. h. im Philosophieren bzw. im wissenschaftlichen Forschen finden.

Die Stoa identifizierte das Erreichen der Tugend mit der Glückseligkeit. Nur der Weise, der im Einklang mit der Ordnung des Kosmos lebe, frei von Affekten, Wünschen und Leidenschaften und gleichgültig gegenüber dem eigenen äußeren Schicksal, könne den Endzustand der Apathie erreichen. Diese Unempfindlichkeit gegen die Wechselfälle des Lebens, die "stoische Ruhe", bedeute das einzige Glück.

Für Epikur wiederum lag der Sinn des Lebens in der (vornehmlich nicht-sinnlichen) Lust, die im Ideal der Seelenruhe (Ataraxie) besteht. Die maßvolle Befriedigung der Grundbedürfnisse bildete für ihn die Basis der besonders erstrebenswerten geistig-seelischen inneren Freuden. Voraussetzungen für die Glückseligkeit waren die Überwindung von Angst und Schmerz. Seine Empfehlung war auch ein Rückzug aus der Öffentlichkeit in einen kleinen Kreis von Freunden.

Das Mittelalter schließlich war die Zeit, in der in Europa das Christentum dominierte, das in dieser Zeit das Monopol auf die Sinnangebote besaß. Die Kirche lehrte, dass nur das Befolgen der sakralen Gebote dem Leben einen Sinn geben könne. Im späten Mittelalter verlagerte sich der Schwerpunkt von der eher kollektiven auf eine individuellere Form des Lebenssinns, der in der persönlichen Nachfolge Christi und der mystischen Vereinigung mit Gott schon zu Lebzeiten gesucht wurde. Stark verkürzt kann man aus Sicht des Mittelalters als den Sinn des Lebens das ewige Leben, also die ewige und maximal mögliche Gemeinschaft mit Gott, angeben. „Die christlich-abendländische Metaphysik war eines der großen Gehäuse, in dem Menschen auf Erden ihre Platzanweisung und damit ihre Einfügung in einen größeren Sinnzusammenhang erhalten haben.“

Auch zu Beginn der Neuzeit orientierten sich die meisten Menschen noch an der christlichen Lehre. Erst die Aufklärung begann die auf Frömmigkeit und Traditionen vertrauende, autoritätsgläubige Geisteshaltung kritisch zu hinterfragen. Der Mensch sollte sich wieder seines "eigenen" Verstandes bedienen "(sapere aude!)" und die Verantwortung für sein eigenes Leben selbst übernehmen, statt sich auf weltliche oder kirchliche Institutionen zu verlassen.

Immanuel Kant kritisierte die herkömmlichen Vorstellungen von Glück, da diese bedeuteten, dass jeder den unvorhersehbaren Schwankungen seiner eigenen wechselhaften Triebe, Bedürfnisse, Gewohnheiten und Vorlieben ausgeliefert sei. Auch der Freiheit, der Unsterblichkeit und Gott könne man sich im Wege der Vernunft nicht nähern. Man könne sie allein postulieren. Kant forderte stattdessen, dass sich der Mensch freiwillig den Gesetzen der Moral (kategorischer Imperativ) unterwirft. Dadurch könne ein selbstbestimmtes (autonomes), vernünftiges Leben geführt werden, in dem sich immerhin Zufriedenheit erreichen lasse. „Um der Sinnhaftigkeit der Sittlichkeit willen und um der Sinnhaftigkeit der Welt willen müssen wir Gott und Unsterblichkeit postulieren: Die ethische Bestimmung des Menschen fordert seine Weiterdauer.“

Auch der Determinismus hatte Auswirkungen auf die Debatte um den Sinn des Lebens. Deterministen behaupten, dass ein Zustand der Welt zusammen mit den Naturgesetzen jeden weiteren Zustand der Welt festlegt. Nicht wenige Philosophen waren und sind der Auffassung, dass dies einen freien Willen unmöglich mache. Wenn der Weltverlauf schon feststehe, so könne man sich nicht mehr frei für eine Handlung entscheiden. Doch damit, so wurde weiter argumentiert, drohe auch der Sinn des Lebens zu einer Farce zu werden. Schließlich könne man sich einen "Sinn" dann nicht mehr aus freier Entscheidung geben und für seine Erfüllung sorgen.

In Arthur Schopenhauers philosophischer Konzeption ist das Leben durch das Prinzip des Willens geprägt. Dieser Wille sei kein individueller Wille, sondern ein metaphysisches Grundprinzip, das sich als Resultat des unbewussten und ziellosen Dranges des Willens in allen bekannten Erscheinungen manifestiere. Dies ist für ihn gleichbedeutend mit Leiden, da das Wollen des Menschen niemals dauerhaft zufriedenzustellen sei. Nur der ästhetische Genuss, die Versenkung in Kunst und Musik könne den Menschen in einen Zustand der reinen Anschauung versetzen, in dem das Leiden aufgehoben ist.

Stark befördert wurde die Sinnfrage durch die sich im 19. Jahrhundert ausbreitende Religionskritik. So fragte Ludwig Feuerbach: „Verliert nicht das Leben gerade durch das Jenseits, in dem es erst einen Sinn finden soll, allen Sinn, allen Zweck?“ Aus der gleichen Perspektive problematisierte Søren Kierkegaard in "Entweder – Oder" die Bedeutung des Lebens für die Existenz des Menschen.

Einen völlig anderen Ansatz vertrat Friedrich Nietzsche. Nach seiner Analyse lebte er in einer Zeit, die er als zerrissen betrachtete, geprägt vom inneren Verfall. Gott war für den Menschen kein Leitbild mehr (Gott ist tot). „Das verehrende Herz zerbrechen, als man am festesten gebunden ist. Der freie Geist. Unabhängigkeit, Zeit der Wüste. Kritik alles Verehrten.“ Diese Haltung ist der Nihilismus, die Zeit der absoluten Wert- und Sinnlosigkeit. „Denken wir diesen Gedanken in seiner furchtbarsten Form: das Dasein, so wie es ist, ohne Sinn und Ziel, aber unvermeidlich wiederkehrend, ohne ein Finale ins Nichts: “die ewige Wiederkehr.” Das ist die extremste Form des Nihilismus: das Nichts (das “Sinnlose”) ewig!“ Um diesem zu entkommen sah Nietzsche die Aufgabe des Menschen darin, einen höher entwickelten Menschentypus hervorzubringen: den Übermenschen. Dieser soll hart und ohne jedes Mitleid gegen sich selbst und andere sein. Sein Lebenszweck besteht darin, aus seinem Leben und aus der Menschheit ein Kunstwerk zu formen. Er forderte: „wozu Du da bist, das frage dich: und wenn Du es nicht erfahren kannst, nun so stecke Dir selber Ziele, hohe und edle Ziele und gehe an ihnen zu Grunde“ Ähnlich wie Max Stirner betrachtete er den Egoismus als ein Korrektiv zum Ethizismus von Kirche und Staat, die den Sinn des Lebens in einer höheren Ordnung verorten, dem sich der Einzelne unterzuordnen hat.

In der Lebensphilosophie Wilhelm Diltheys, der die Geschichtlichkeit des Menschen betonte, kommt vor allem der Philosophie die Aufgabe zu, den Stand der geistigen Entwicklung zu reflektieren und damit dem Denken der jeweiligen Epoche einen Sinn zu geben. „Philosophie ist das Umfassendste, die allseitigste jener Funktionen, durch die der menschliche Geist zum Bewußtsein seiner Zwecke gelangt: Sinn des Lebens und Universums. Der immer rege Zusammenhang des menschlichen Geistes mit sich selbst. Das Letzte, das Höchste der menschlichen Kultur überhaupt.“ Lutz Geldsetzer merkt hierzu an: „So sieht Dilthey implizit in der Philosophie jedenfalls die ausschlaggebende Macht in der Geschichte, durch die und mittels derer der Mensch in der Kultur über sein erkennendes Verhältnis zur Welt, seine ‚Wertbestimmungen des Gefühls’ und die Willenshandlungen in der ‚Führung des Lebens und der Leitung der Gesellschaft’ Klarheit gewinnt und sich Rechenschaft abgibt.“

Der Existenzialismus beschäftigte sich besonders intensiv mit der Problematik des Lebenssinns. Im Mittelpunkt steht die Erkenntnis, dass es jedem Menschen aufgegeben ist, frei zu wählen und zu entscheiden, was er mit seinem Leben tun will. Jean-Paul Sartre formulierte hierzu: „Frei sein heißt zum Freisein verurteilt sein.“ Der Mensch ist in die Welt „hineingeworfen“ worden und müsse sich nun selbst definieren. Das bedeutet: Der Mensch sei nichts anderes als das, wozu er sich selbst macht. Er konzipiert ständig neue Entwürfe von sich, die er dann (nach)lebt. Diese totale Freiheit bedeute aber auch die Bürde einer vollständigen Verantwortung für sich und sein Handeln, denn das eigene Leben könne durch keine andere, höhere Instanz mehr entschuldigt werden.

Der modernen analytischen Sprachphilosophie, wie sie sich auf Wittgenstein beziehen lässt, schien unklar, welchen Status Aussagen über das, was man tun oder nicht tun soll, haben können. „Die Lösung des Problems des Lebens merkt man am Verschwinden dieses Problems. (Ist nicht dies der Grund, warum Menschen, denen der Sinn des Lebens nach langen Zweifeln klar wurde, warum diese dann nicht sagen können, worin der Sinn bestand?)“ (TLP 6.521) Er konzedierte aber, dass der Mensch im religiösen Glauben einen Sinn finden kann: „An einen Gott glauben, heißt sehen, daß es mit den Tatsachen der Welt noch nicht abgetan ist. An Gott glauben, heißt sehen, daß das Leben einen Sinn hat.“ Vor allem Bertrand Russell betonte, dass man dem objektiven Naturprozess keinen Sinn entnehmen könne. Damit geriet die Sinnfrage in einen Raum individueller Entscheidung außerhalb eines strenger zu fassenden philosophischen Projekts. Sie trennt aus dieser Perspektive daher den Bereich des Lebens von den Bereichen, in denen die Philosophie aus ihrer Sicht Antwort zu geben vermag.

Die Logotherapie von Viktor Frankl ist eine Methode der Psychotherapie, um Menschen von krankmachender Sinnleere zu befreien. Denn wenn der Mensch seinen Willen zum Sinn in der Lebenspraxis nicht zur Geltung bringen kann, entstehen bedrückende Sinn- und Wertlosigkeitsgefühle.

Die drohende Auslöschung der Menschheit durch einen globalen Krieg, das mögliche Ende des Fortschritts, die zunehmend sichtbar werdende Zerstörung der Umwelt, die warnende Prophezeiung vom Ende des Wachstums und der Wegfall der Wertesysteme (wie im Holocaust) in der westlichen Hemisphäre haben im Verlauf der zweiten Hälfte des 20. Jahrhunderts zu einem gesellschaftlichen Phänomen geführt, das häufig als "allgemeine Sinnkrise" bezeichnet wurde. In ihrem Gefolge gewannen auch skeptische Positionen stark an Bedeutung. So vertrat etwa Emil Cioran eine existentielle Skepsis, die von einem „Dasein ohne Endergebnis“ ausgeht. Die Argumente des Zweifels an jedem Lebenssinn drohen hier allerdings stets zur völligen Verzweiflung zu führen.

Obwohl die Naturwissenschaften sich – dem positivistischen Verständnis gemäß – nicht mit metaphysischen Sinnfragen beschäftigen, oder metaphysische Sinnfragen gar für sinnlos erklären, werden aus ihren Ergebnissen Folgerungen für die Beantwortung eschatologischer Fragen abgeleitet. So sieht beispielsweise Hoimar von Ditfurth den Sinn des Lebens untrennbar mit dem Sinn der kosmischen Evolution verknüpft.

Der moderne Hedonismus betont in der Nachfolge der Utilitaristen und klassischen Hedonisten das Erleben und Erstreben von sinnlicher Lust. Der Lebenssinn liege in der Befriedigung von Bedürfnissen. Intensive, reizvolle und angenehme Lustempfindungen werden aktiv gesucht und nach Möglichkeit gesteigert. Die Verrechnung von Lust und Unlust soll dabei für das Individuum möglichst zu jedem Zeitpunkt ein positives Gesamtergebnis aufweisen.

Wieder an Aristoteles anknüpfend stellt Martha Nussbaum die Frage nach einem guten Leben in den Mittelpunkt ihres Denkens. Sie warnt vor der Zerbrechlichkeit des guten Lebens und hat in Zusammenarbeit mit Amartya Sen ein Konzept entworfen, in dem sie konkret zu bestimmen versucht, welches die Befähigungen sind, die ein Mensch benötigt, um ein gutes Leben führen zu können. Ähnlich bezieht sich auch Alasdair McIntyre auf die Tugendlehre des Aristoteles.

Die verschiedenen Religionen geben unterschiedliche Antworten auf die Frage nach dem Sinn des Lebens, die hier – geordnet nach Verbreitung – nur kurz skizziert werden sollen. Die Abschnitte beschreiben jeweils die Sicht der entsprechenden Religion.

Das Christentum ist durch die Apostel begründet worden, die in der Nachfolge Jesu von Nazaret das Evangelium verkündeten. Der Überlieferung nach war er der Sohn der Jungfrau Maria, der Frau eines jüdischen Handwerkers mit Namen Joseph. Als Gottes Sohn und Messias verkündigte er das kommende Reich Gottes und erlöste die Menschen von Sünde und Tod durch seinen freiwilligen Tod am Kreuz und seine Auferstehung. Durch dieses stellvertretende Opfer können die Menschen Vergebung für ihre Sünden erlangen, sofern sie die so begründete Gemeinschaft mit Gott für sich persönlich im Glauben annehmen.

Der "Sinn des Lebens" im Christentum ist es, diese Gemeinschaft mit Gott und untereinander im Leben wie im bzw. nach dem Tod zu pflegen. Voraussetzung ist hierzu das Leben in Liebe, die innere und äußerliche Umkehr (Buße) und den Glauben an die Erlösung durch Jesus Christus, wie sie in der Bibel beschrieben wird, voraussetzt. Mit der Wiedergeburt bzw. der Taufe beginnt das von der Sünde und dem Tod erlöste Leben, das sich in Gebeten, Sakramenten und guten Werken fortsetzt.

Dietrich Bonhoeffer sah den Sinn des Lebens vor allem in der Nachfolge Christi:

Der Islam wurde durch Mohammed begründet, der als Sohn eines Händlers im heutigen Saudi-Arabien geboren wurde. Der Überlieferung nach erschien ihm der Erzengel Gabriel und übermittelte ihm die Verse des Korans. Der Islam sieht sich als Fortsetzung und Wiederkehr einiger Ursprungsreligionen (Judentum, Christentum) in korrekter und vollendeter Form.

Der "Sinn des Lebens" im Islam besteht darin, Allah zu dienen und sein Wohlgefallen zu erreichen. Dies wird im Koran in der Sure 51, Vers 56 so beschrieben: „Und die Menschen und die Dschinn habe ich erschaffen, nur damit sie mir dienen.“ Dazu dienen u. a. der feste Glaube an Gott und seine Vorsehung, die Überwindung schlechter Eigenschaften und falscher Ideen, verantwortliches Handeln, Eintreten gegen Ungerechtigkeit und das Vollbringen guter Taten.

Der Hinduismus besteht aus verschiedenen Wegen ohne gemeinsame Gründerfigur oder allgemeingültige kanonisierte Heilige Schrift. Die einzelnen philosophischen Auffassungen haben teilweise verschiedene Konzepte hinsichtlich der Lehre von Leben, Tod und Erlösung. Ebenso unterschiedlich sind die Konzepte vom "Sinn des Lebens". Für viele bedeutet es ein Leben nach den traditionellen „vier Lebenszielen“, nämlich Artha (Wohlstand), Kama (Begierde), Dharma (Pflicht, Moral) und schließlich als letztes Ziel Moksha, die Erlösung. Für die Anhänger der monistischen Advaita-Lehre bedeutet Moksha ein Aufgehen in das „kosmische Bewusstsein“, ins Brahman. Für die Anhänger der Dvaita-Lehre hat die Gottesliebe (Bhakti) einen zentralen Stellenwert, Erlösung bedeutet für sie ewige Gemeinschaft mit Gott.

Der Begründer des Buddhismus, Siddhartha Gautama, lebte der Überlieferung nach vor etwa 2500 Jahren als reicher Fürstensohn unbekümmert und von allen Unannehmlichkeiten ferngehalten in einem Palast. Er wehrte sich gegen diese Abschottung. Als er sie als junger Heranwachsender überwand und sich der Realität des unausweichlichen Leidens („Dukkha“) und Todes stellte, erkannte er die Sinnlosigkeit seines bisherigen Lebens. Er beschloss, nach einem Ausweg aus dem Leiden zu suchen, und fand seinen eigenen Weg durch Meditation. Im Ringen um Erlösung erreichte er schließlich die oft ungenau als „Erleuchtung“ bezeichnete Einsicht ("Bodhi" „Erwachen“) in die Ursache des Leidens und deren Aufhebung.

Der "Sinn des Lebens" im alten Buddhismus ist es, dem Kreislauf der Reinkarnationen im Samsara durch das Eingehen in das Nirvana zu entkommen, in das völlige Verlöschen – was das Verlöschen der Sinnfrage logisch einschließt. In der Lehre der Buddhisten wird alles Leben und Tun als schließlich zum Leiden führend entlarvt. Hierfür wird die Gier nach Leben, Macht und Lust als ursächlich erkannt. Nur die völlige Auslöschung dieser Gier kann zur Überwindung des Leidens führen.

Im Verlauf der langen Entwicklung des Buddhismus entstand eine Vielzahl buddhistischer Schulen und Strömungen, die zum Teil sehr verschiedene Methoden als Wege zur Befreiung aus dem Kreislauf des Leidens anwenden. Allen buddhistischen Traditionen ist der sogenannte „Edle Achtfache Pfad“ gemein.

In den späteren Schulen des Mahayana-Buddhismus wird vorrangig nicht die Erlösung der eigenen Person durch das Eingehen ins Nirvana angestrebt, sondern die Idealexistenz eines Bodhisattva, der zunächst den anderen Lebewesen hilft, sich aus dem endlosen Kreislauf zu retten, um erst danach selbst ins Nirvana überzugehen.

Die jüdische Religion basiert auf den religiösen Überlieferungen des Volkes der Juden. In der Geschichte des Judentums entstanden eine Reihe grundlegender Glaubensprinzipien, deren Einhaltung von Juden mehr oder weniger erwartet wird, um in Einklang mit der jüdischen religiösen Gemeinschaft und ihrem Glauben zu sein.

Der "Sinn des Lebens" im Judentum besteht in der Einhaltung der göttlichen Gesetze, d. h. in der Ehrfurcht vor Gott und seinem Willen. Die Regeln und göttlichen Gebote („Mitzwa“) sind im Tanach gesammelt, der in Talmud und Midraschim diskutiert und ausgelegt wird.

Teilhard de Chardin und Frank Tipler sehen einen teleologischen Sinn in der Evolution des Universums, deren Ende von beiden als Omegapunkt bezeichnet wird.

In den naturphilosophischen Arbeiten Teilhard de Chardins um 1930 ist Leben im Kosmos ein schöpferischer Vorgang in Richtung immer höherer Organisiertheit, die von Gott bewirkt wird. Ziel dieser Entwicklung ist die absolute Liebe, die in Jesus Christus vorweggenommen wurde. Die "„Radiale Energie“" der Evolution – und damit auch die Menschheit – mündet schließlich im „Punkt Omega“. Der Sinn des Lebens jedes Einzelnen ist demnach die Teilhabe an der Entwicklung der „absoluten Liebe“.

Der Kosmologe Frank J. Tipler veröffentlichte 1994 eine auf einer ähnlichen Vorstellung aufbauende Omegapunkt-Theorie, nach der der Sinn des Lebens darin besteht, die in der kosmischen Geschichte vollendete Gottheit zu schaffen. Nach dieser These ist Gott sowohl Ziel als auch Ursprung der Evolution des Universums. Im Big Crunch lässt Gott aus Agape das Universum und alle jemals existent gewesenen geliebten Lebewesen, nunmehr „geheilt“, als perfekte Simulation virtuell auferstehen. Die Partizipation an der Evolution der menschlichen Kultur im Universum ist demnach Sinn des einzelnen Lebens.

Nach Niklas Luhmann ist Sinn "das" universale Medium der Formbildung sozialer und psychischer Systeme.

Physische, soziale und psychische Systeme bilden Formen. Zur Formenbildung benötigen sie ein Medium. So wie für physische Systeme die Gegenstände, mit denen sich die Physik befasst, ein universales Medium zur Bildung von Formen sind, so ist Sinn das universale Medium, mit dem soziale und psychische Systeme Formen bilden können.

Zu den Formen, die im Medium Sinn geformt werden können, gehören zum Beispiel alle Sinnentwürfe, alle Fragen nach dem Sinn und alle Antworten darauf.

Im Medium Sinn gibt es "mögliche" Formen (die noch nicht aktualisiert wurden) und "aktualisierte" (tatsächlich verwirklichte) Formen. Soziale Systeme erleben diesen Unterschied und handeln, indem sie aus möglichen Formen wählen (selektieren) und sie zu aktualisierten Formen werden lassen. Die Differenz zwischen möglichen und aktualisierten Formen ist überhaupt der Grund für das Erleben und Handeln sozialer Systeme. Und Sinn als Medium für Formen ist die Voraussetzung dazu.

Dem Leben einen Sinn abzugewinnen ist das Eine, ihn umzusetzen das Andere. Die globalen Herausforderungen bis hin zu den Gefahren der Selbstvernichtung der Menschheit machen die Frage nach dem Sinn des Lebens dringlicher denn je und die damit verbundenen moralischen Anforderungen an jeden Einzelnen.

China will bis 2020 ein Sozialkompetenzsystem einführen, um jeden Bürger zu moralisch gutem Verhalten anzuhalten. Mittels Big Data wird eine Bürgerbewertung angestrebt, die so gut wie alle Belange des gesellschaftlichen Lebens betreffen soll. Bei vollständiger Umsetzung ist ein Überwachungsstaat zu befürchten, der sogar noch umfassender und gefährlicher wäre als derjenige aus George Orwells Roman "1984".

Auf die ewige Grundfrage des Menschen gibt es viele weitere Antworten, dazu gehören auch folgende nicht vollständig ernst zu nehmende:





</doc>
<doc id="8958" url="https://de.wikipedia.org/wiki?curid=8958" title="Vokal">
Vokal

Ein Vokal (von lat. "[litera/littera] vocalis" ‚tönender [Buchstabe]‘; zu "vox" ‚Stimme‘) oder Selbstlaut ist ein Laut, bei dessen Artikulation der Phonations­strom weitgehend ungehindert durch den Mund ausströmen kann. Vokale sind im Allgemeinen stimmhaft.

Vokallaute (oder Vokale; Terminus überschneidet sich mit der Bedeutung „Vokalbuchstabe“) sind Phone, also Einheiten der gesprochenen Sprache. Buchstaben der Schriftsprache wie "A, E, I, O, U" sind "Vokalbuchstaben," sie werden allgemeinsprachlich, so auch im Duden, aber meist ebenfalls „Vokale“ genannt.

Zur Darstellung aller Vokale wird das sogenannte Vokaldreieck oder Vokaltrapez benutzt (siehe Abbildung). Darin sind die Vokale in der Senkrechten nach dem zu ihrer Bildung notwendigen Grad der Öffnung des Mundraums von "offen" (a-Laut) nach "geschlossen" (i-, u-Laut) angeordnet. In der Waagerechten werden sie in vorne (e-, i-Laute) oder hinten (o-, u-Laute) im Mundraum entstehend unterteilt. Sie heißen dementsprechend Vorderzungenvokale bzw. Hinterzungenvokale.

Grundsätzlich kann jeder Vokal auch mit Lippenrundung gerundet gebildet werden, was eine Änderung der Lautqualität herbeiführt: Aus einem gespannten i-Laut wird durch Lippenrundung ein ü-Laut und aus einem ungerundeten ein ö-Laut . Im Deutschen gibt es die gerundeten Vokale ü [], ö [], o [] und u [] und die ungerundeten a , e [] und i [].

Weiterhin können durch Absenken des Gaumensegels Nasalvokale gebildet werden, wie das [ɔ͂] im französischen "on" „man“.

Vokale werden in vielen Sprachen nach ihrer Länge (Vokalquantität) in kurze oder lange Vokale unterteilt. Im Deutschen sind die kurzen Vokale ungespannt, z. B. das [ı] in Mitte. Die langen Vokale sind gespannt, z. B. das [i:] in Miete. Es gibt aber auch kurze gespannte Laute, wie das [i] in Musiker (vgl. Zellerhoff 2011, 275).

Der polnische Linguist Piotr Żyromski hat für die deutschen Vokale vorgeschlagen, fünf verschiedene Parameter zu unterscheiden: Qualität, Quantität, Dorsalität, Höhe der Dorsalität (Vokalhöhe) und Lippenrundung (Lippenposition).

Nach der Form der Schallwelle gehören Vokale also, im Gegensatz zu den meisten Konsonanten, zu den Klängen.

Vokale haben einen Primärklang. Dieser besteht aus einem Grundton der Grundfrequenz f0, der wiederum Obertöne bildet. Diese sind immer ganzzahlige Vielfache des Grundtons, d. h. wenn man den Grundton ändert, dann ändern sich auch immer die Obertöne.

Durch Resonanzen im Vokaltrakt (oder Ansatzrohr), d. h. in der Stimmritze, im Larynx- und Pharynxraum und im Mund- und Nasenraum, entstehen ausgeprägte Obertonbereiche, so dass sie gegenüber den anderen Teiltönen dominanter werden. Diese resonanzverstärkten Teiltöne heißen Formanten.

Bei Vokalen kann man vier bis fünf Formanten nachweisen: F1 und F2 (bei Vokalen die im vorderen Zungenbereich gebildet werden) sind für die Identifizierung der Vokale verantwortlich. Sie entscheiden also darüber, ob wir beispielsweise ein [] oder ein [] hervorbringen.

Die Artikulation kann quantitativ mit drei Kenngrößen (Formanten) wiedergegeben werden: F1 gibt die Offenheit oder Zungenhöhe an, F2 die Zungenstellung von hinten nach vorn, und F3 die Lippenrundung. Ein hat beispielsweise einen höheren F2-Wert als ein aber einen weitaus geringeren F1-Wert als zum Beispiel ein .

Klassifiziert man Laute lediglich nach ihrer Artikulationsart, dann unterscheiden sich Vokale von Konsonanten allein dadurch, dass der Phonationsstrom bei Vokalen fast ungehindert durch das Ansatzrohr strömt. (Genauer: Im Gegensatz zu Konsonanten ist die Konstriktion [Verengung] im Artikulationsraum nicht so stark, dass sie ein Geräusch erzeugte.) Dieses Kriterium allein leistet jedoch noch keine Unterscheidung von Vokalen einerseits und halbvokalischen oder konsonantischen Approximanten andererseits.

Eine andere Möglichkeit ist die Klassifizierung nach akustischen Kriterien, vor allem nach ihrer Sonorität (Schallfülle). Danach sind die meisten Konsonanten Geräusche ohne Sonorität, während es sich bei Vokalen und einigen Konsonanten wie beispielsweise dem l-, m-, n-, ng- und (zum Teil) r-Laut um Klänge handelt. Diese sind also sonorant.

Sonorität bzw. Schallfülle ist eine wichtige Eigenschaft hinsichtlich der Funktion eines Lautes als Silbenträger: je sonorer ein Laut ist, desto deutlicher hebt er sich von den anderen ihn umgebenden Lauten ab. Sonorante Laute können deshalb Silbenträger sein.

In einigen Sprachen nehmen "l, m, n, ng", aber auch "r" silbischen Charakter an und tragen den Hauptton, beispielsweise l in "Plzeň" auf Tschechisch und r in Krk auf Serbokroatisch.

Auch im Deutschen kommen die Laute "l, m, n, ng" silbisch vor, allerdings nur in unbetonter Silbe, so in der Standardaussprache bei den Endungen -em, -en und -el/-l nach Konsonanten (silbischer m-Laut: „großem“, „leben“, „Bremen“; silbischer n-Laut: „reden“, silbischer ng-Laut: „liegen“, silbischer l-Laut: „Apfel“, „Dirndl“).

Das Deutsche ist, was das Lautinventar im Sprachvergleich betrifft, verhältnismäßig reich an Vokalen. Zu den gesprochenen Vokalen (Monophthongen) des Standarddeutschen gehören:
"Siehe auch": Diphthonge im Deutschen

In betonten Silben unterscheidet man im Deutschen zwischen zwei Typen von Vokalen:
Dieses Muster wird in zwei Fällen durchbrochen:

Einige weitere Vokale, die in der Tabelle nicht aufgeführt sind, erscheinen ausschließlich in Fremdwörtern. Besonders produktiv war und ist hier das Französische, dem das Deutsche die Nasalvokale [ɑ̃] (wie in "Orange"), [ɛ̃] ("Teint", "Mannequin"), [õ] ("Contenance"), [ɔ̃] ("Jargon") und [œ̃] ("Parfum") sowie den Langvokal [œ:] ("Œuvre") verdankt. Die Benutzung der Nasalvokale gilt teilweise als bildungssprachlich; auch die Aussprache mit gewöhnlichem, nicht-nasaliertem Vokal ‒ meist von [ŋ] gefolgt (z. B. "Orange" [oˈʀaŋʒə], "Bonbon" [bɔŋˈbɔŋ]) ‒ wird zunehmend als normgerecht empfunden.

Aus dem Englischen gelangte unter anderem der Langvokal [ɔː] ("Smalltalk") ins Deutsche.

In den unbetonten Silben besitzt das Deutsche sieben weitere Vokale, die in ihrer Qualität dem korrespondierenden langen Vokal entsprechen, aber kürzer sind.

Beispiel: In betonten Silben entsprechen dem Graphem <e> die Allophone [e:] (wie in "ledig") bzw. [ɛ] ("nett"). In unbetonten Silben ‒ z. B. in "lebendig", "Gewissen" ‒ erscheint stattdessen oft (statt des unten erwähnten Schwa) der kurze Laut [e]. Obwohl diese Situation auch bei deutschen Wörtern vorkommt (z. B. "daheim", "wieso", "sodass", "zuvor"), sind mehrheitlich Fremdwörter betroffen (z. B. direkt, Debatte, Physik, Ökonomie).

Zwei Laute kommen im Standarddeutschen ausschließlich in Affixen und Reduktionssilben vor: <nowiki>[</nowiki>ə<nowiki>]</nowiki>, genannt Schwa ("Blume), und <nowiki>[</nowiki>ɐ<nowiki>]</nowiki> ("Leder). Sie werden oft nicht zu den Phonemen gerechnet und in den Darstellungen des deutschen Vokalbestandes darum hin und wieder vergessen.

In dem von Eva-Maria Krech u. a. herausgegebenen "Großen Wörterbuch der deutschen Aussprache" (1982) werden neben langen und kurzen auch halblange Vokale aufgeführt. Beispiele sind der a-Laut in "Leda" oder "Oma"; in "Oma" einen wirklich kurzen [und vorderen] a-Laut zu sprechen, wie es beispielsweise umgangssprachlich im Ruhrgebiet üblich ist, wird nicht als normgerecht empfunden. Weitere Beispiele sind der halblange e-Laut in "Káffee" (auf der ersten Silbe betont; betont man die zweite, so spricht man keinen halblangen, sondern einen wirklich langen e-Laut) oder in "Meteorologe", "demobilisieren", "Regeneration", "Deeskalation" (jeweils der erste e-Laut; halbe Länge hier in allen Fällen mit mehr als zwei Silben zwischen sich und der Hauptbetonung); der halblange i-Laut in Wörtern wie "Omi", "Ami", "Gabi"; der halblange o-Laut in Wörtern wie "Kino", "Auto", "Eskimo"; der halblange u-Laut in Wörtern wie "Akku" oder "Uhu" (zweiter u-Laut; der erste besitzt volle Länge); der halblange ö-Laut in Fremdwörtern wie "Ökologisierung"; der halblange ü-Laut in Fremdwörtern wie "Pyroelektrizität", "Hyperboloid" oder "Hybridisation" (auch hier in allen Fällen mit mehr als zwei Silben zwischen sich und der Hauptbetonung).

Der deutsche Vokalbestand umfasst einige Eigentümlichkeiten, die sich einer simplen Kategorisierung entziehen. Einordnungsprobleme bietet bereits der Laut [ɐ] (wie in "Leder"), der eine Realisierungsvariante des konsonantischen <nowiki>[</nowiki>ʁ<nowiki>]</nowiki> darstellt.

Ein weiteres Problem ist das Verhältnis zwischen gespannten, unter Akzent langen Vokalen (wie in "Ofen") und ungespannten kurzen Vokalen (wie in "offen"). Der Streit darüber, welches dieser Unterscheidungsmerkmale das "primäre" sei, hat zur Entstehung des Silbenschnittkonzepts geführt, eines silbenanalytischen Konzepts, das solche Minimalpaare alternativ erklärt:

Das Silbenschnittkonzept geht davon aus, dass für die Unterscheidung zwischen diesen beiden Vokalklassen weder die Quantität (Länge) noch die Qualität (Gespanntheit, „Geschlossenheit“) verantwortlich sei. Grundlage des Unterschiedes sei vielmehr der Silbenschnitt, ein prosodischer Kontrast am Silbenende, der in manchen Wörtern höher, in anderen geringer sei. Sanft laufe eine Silbe dann aus, wenn die Vokalbildung von der Artikulation eines eventuell nachfolgenden Konsonanten nicht beeinflusst wird. In solchen Silben stehen gespannte Langvokale; eventuelle postvokalische Konsonanten sind nur lose angeschlossen. In allen anderen Silben wird die Vokalbildung durch die vorgezogene Artikulation des nachfolgenden Konsonanten sozusagen scharf abgeschnitten; der Vokal ist dann kurz und ungespannt (offen); der postvokalische Konsonant ist fest angeschlossen.

Englisches [ɐ] wird am häufigsten /ʌ/ transkribiert.

Das Englische ist deutlich ärmer an Vokalphonen (nicht allerdings an Vokalphonemen) als das Deutsche, besitzt dafür aber mehr Diphthonge ("take", "go", "night", "flower", "boy", "here", "there", "sure").

In der spanischen Sprache, genauer dem Kastilischen, existieren nur die fünf Vokalphoneme /a e i o u/. Dies unterscheidet das Kastilische von den anderen iberoromanischen Sprachen Portugiesisch, Galicisch und Katalanisch (mit Valencianisch, wobei hier die Unterschiede zum Kastilischen geringer sind). Der a-Laut ähnelt dem deutschen kurzen (offenen) a, während i-Laut und u-Laut den langen (geschlossenen) Vokalen im Deutschen gleichen. Der e-Laut und der o-Laut des Spanischen haben eine mittlere Höhe und daher kein genaues Gegenstück im Hochdeutschen.

Der ungespannte (mit weniger Muskelanspannung geformte) i-Laut (wie in "Wind"), der ungespannte ü-laut (wie in "wünscht"), der ungespannte u-Laut (wie in "Wunsch") und der a-ähnliche, sogenannte tiefe "Schwa"-Laut (wie der "-er" geschriebene Vokal in "Mutter") fehlen im Französischen. Ansonsten besitzt es die gleichen oralen (mit dem Mund geformten) Vokale wie das Deutsche plus vier Nasalvokale. Allerdings kennt das Französische nicht den systematischen bedeutungsverändernden Unterschied zwischen kurzen ungespannten und langen gespannten Vokalen, der für das Deutsche typisch ist (wie in "Wahl/Wall, den/denn, ihn/in, Ole/Olle, pult/Pult, Tönchen/Tönnchen, Fühler/Füller").

Es werden unter Vokalen gemeinhin auch die Buchstaben verstanden, die derartige Laute repräsentieren. Um der verbreiteten Verwechslung bzw. Gleichsetzung von Lauten und Buchstaben vorzubeugen, ist es sinnvoll, die Begriffe Vokallaut und Vokalbuchstabe zu verwenden.

Als Vokalbuchstaben gelten im Deutschen: A, Ä, E, I, O, Ö, U, Ü, Y.

Sie stehen den Konsonanten­buchstaben B, C, D, F, G, H, J, K, L, M, N, P, Q, R, S, ẞ, T, V, W, X, Z gegenüber.

Diese Zuordnung beruht auf den grundlegenden Buchstaben-Laut-Beziehungen (Graphem-Phonem-Korrespondenzen), die aufgrund verschiedener Kriterien ermittelt werden können (Häufigkeit, Kontextbedingtheit, Eindeutigkeit). Vokalbuchstaben können in bestimmten Kontexten auch die Funktion übernehmen, nicht-silbische Laute wiederzugeben, und (eher seltener) Konsonantenbuchstaben die Funktion, silbische Laute darzustellen. Je nach Definition und Abgrenzung zwischen Vokalen und Konsonanten (vgl. oben) und dem Status von Halbvokalen kann dann formuliert werden, Vokalbuchstaben stehen auch für Konsonanten und Konsonantenbuchstaben für Vokale.

Im Deutschen betrifft das vor allem die Vokalbuchstaben I, U und Y, sowie die Konsonantenbuchstaben R, J und W. Bei einer an der Sonorität orientierten Definition der Vokale kommen auch noch die Konsonantenbuchstaben L, N und M hinzu. Eine besondere Rolle bei der Darstellung von Vokalen nimmt auch der Konsonantenbuchstabe H ein, allerdings nur in Kombination mit Vokalbuchstaben, nicht für sich allein.

Beispiele für unsilbisch, halbvokalisch bzw. konsonantisch verwendete I, U und Y: "Mai, Aktie, Union, Harpyie; Mauer, eventuell, Biskuit, Etui, Qualle; Bayern, Yak, Maya", daneben gelegentlich auch weitere Vokalbuchstaben, z. B. O: "Kakao, Coiffeur".

Beispiele für zwar unsilbische, aber eher halbvokalisch verwendete Konsonantenbuchstaben: bei R im Silbenendrand, vor allem nach langen Vokalen ("mehr, vier, rührt"); bei J lässt sich ganz allgemein streiten, ob der repräsentierte Laut eher Halbvokal oder Konsonant ist; auch bei W kann die Aussprache Richtung Halbvokal gehen.

Beispiele für die selteneren Fälle, in denen R, J und W als silbische Vokale verwendet werden: "Zentaur, Matrjoschka, Rwanda".

Außerdem können Konsonantenbuchstaben in bestimmten Kombinationen ihren konsonantischen Charakter verlieren: z. B. in der häufigen Endung "-er", aber auch z. B. in der Namensendung "-ow". Hierher gehört dann auch die vielfältige Funktion des stummen H nach Vokalbuchstaben (als Dehnungs-h, als silbentrennendes h, ...).





</doc>
<doc id="8959" url="https://de.wikipedia.org/wiki?curid=8959" title="Entwicklungsland">
Entwicklungsland

Als Entwicklungsland wird ein Land bezeichnet, bei dem die Mehrzahl seiner Bewohner hinsichtlich der wirtschaftlichen und sozialen Bedingungen einen messbar niedrigeren Lebensstandard haben. Dies äußert sich vor allem durch eine schlechte Versorgungslage mit Nahrungsmitteln und Konsumgütern, Armut, Unterernährung und Hunger, Einschränkungen bei der Gesundheitsversorgung, eine hohe Kindersterblichkeitsrate und eine geringe Lebenserwartung, mangelhafte Bildungsmöglichkeiten, eine hohe Analphabeten- und Arbeitslosenquote.

Welches Land als Entwicklungsland einzustufen ist oder nicht, hängt vom Maßstab ab, an dem man die Entwicklung eines Landes misst (siehe Abschnitt Gemeinsame Merkmale der Entwicklungsländer).

Der Begriff „Entwicklungsland“ entstammt der Fach- und Alltagssprache der Entwicklungspolitik und genießt hohe Bekanntheit. Eine allgemein anerkannte Definition existiert – trotz vieler Ansätze – nicht.

Mit den Zielen nachhaltiger Entwicklung (Agenda 2030) wird die Dualität zwischen Entwicklungsländern und entwickelten Ländern weniger bedeutend. Alle Länder, auch Deutschland, werden zu Entwicklungsländern, denn alle müssen sich weiterentwickeln, um die globalen Nachhaltigkeitsziele zu erreichen.

Für den Begriff „Entwicklungsland“ gibt es eine Vielzahl Synonyme, wie „Dritte Welt“ oder „Vierte Welt“, „Globaler Süden“ oder „Trikont“. Diese Begriffe sind – ebenso wie „Entwicklungsland“ – teilweise umstritten und werden von einigen Fachleuten abgelehnt.

„Entwicklung“ gelte als Kernkonzept der modernen Welt, das eine quasi naturgesetzliche Veränderung zu einem angeblich "besseren" Endzustand suggeriere. Die moderne Ethnologie zweifelt jedoch an, dass ein solcher zielgerichteter Prozess "(siehe Evolutionismus)" existiert. So nutzten zum Beispiel Jäger und Sammler ihre Ressourcen höchst effizient und nachhaltig; sie genössen eine ihnen eigene Form des Wohlstandes. Auch traditionelle Subsistenzwirtschaften, die ihre Mitglieder mit allen lebensnotwendigen Dingen aus eigener Herstellung selbst versorgen, seien nicht „unterentwickelt“, sondern lediglich "anders strukturiert".

Aus ökonomischer Sicht wenden Kritiker des Begriffs „Entwicklungsland“ beispielsweise ein, dass er einen Prozess suggeriere, der manchmal gar nicht stattfinde – oder auch nicht stattfinden müsse: nämlich Entwicklung. Einer der prominentesten Kritiker aus diesem Lager ist der schwedische Ökonom Gunnar Myrdal. Auch der umgangssprachliche Begriff „Hungerland“ werde in den Medien bei der Veranschaulichung einer Berichterstattung über Hungersnöte in Entwicklungsländern genutzt, ohne dass diesem Ausdruck eine Definition zugrunde liegen könne.

Nicht mehr gebräuchlich sind: „unterentwickelte Länder“ ("underdeveloped countries"), „rückständige Länder“ ("backward countries") oder „nicht-entwickelte Länder“ ("undeveloped countries"). Diese Begriffe erschienen zum ersten Mal im UNO-Programm von 1949, sind jedoch stark wertbehaftet und können von den Bewohnern der betroffenen Länder als verletzend empfunden werden. Sie werden deshalb von UNO und Weltbank nicht mehr verwendet. In einigen Entwicklungsländern selbst werden derartige Begriffe allerdings absichtlich genutzt, um die Missstände im Land hervorzuheben und Euphemismen zu vermeiden.

International gibt es keine eindeutige Sprachregelung. So wurde zum Beispiel infolge einer UN-Vollversammlung im Jahr 1971 die Least Developed Countries (LLDC) von den Less Developed Countries (LDC) unterschieden. Nicht alle UN-Organisationen unterscheiden jedoch zwischen den beiden Gruppen.

Im deutschen Sprachgebrauch besteht das Problem der Übersetzbarkeit der Begriffe. Der umständliche Ausdruck „weniger entwickelte Länder“ hat sich daher nicht durchgesetzt. So verwendet das Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung (BMZ) entweder den englischsprachigen Begriff „LDC“ oder den deutschen unbestimmten Begriff „Entwicklungsland“. Auch macht das BMZ keinen Unterschied zwischen LDC und LLDC und kürzt die „Least Developed Countries“ mit LDC ab. Die DEZA (Direktion für Entwicklung und Zusammenarbeit; eine Bundesbehörde der Schweiz; dem Außenministerium (EDA) unterstellt) verwendet neben dem Begriff „Entwicklungsland“ auch den Begriff „Partnerland“.

Der Ausdruck „Nord-Süd“ wird auch von Entwicklungsländern selbst benutzt. Der Ausdruck „Nord-Süd-Beziehungen“ hat als Ersatz für den Begriff „Entwicklungspolitik“ zugenommen. Das BMZ verwendet beispielsweise diese Bezeichnung. Sie gilt als wertfrei bzw. als politisch korrekt. Der Begriff impliziert nicht, dass Entwicklungsländer zwangsläufig auf der südlichen Halbkugel liegen.

Mit „Industriestaaten“ versucht man industrialisierte Staaten sprachlich abzugrenzen von Entwicklungsländern. Die historische Industrialisierung Europas lässt sich nicht mit den Prozessen vergleichen, die heute in den Entwicklungsländern stattfinden.

Einige industrialisierte, ehemalige sozialistische Länder (Länder des ehemaligen Ostblocks) sind bis heute durch ihre industrielle Vergangenheit geprägt, in anderen (z. B. Tschechien und Slowakei) hat ein starker industrieller Wandel stattgefunden (u. a. Modernisierung, Reduktion einiger Industriezweige).

Der Anteil des Industriesektors am Bruttonationaleinkommen ist in vielen "Industriestaaten" heute geringer als der des Dienstleistungssektors (sie werden aber nicht als „Dienstleistungsstaaten“ bezeichnet). Zum Beispiel gilt Großbritannien als relativ de-industrialisiert (z. B. gab es dort ein langanhaltendes Werftensterben und Zechensterben); dies trägt dazu bei, dass das Land seit vielen Jahren ein großes Handelsbilanz-Defizit und eine hohe jährliche Netto-Neuverschuldung des Staates hat (siehe Vereinigtes Königreich#Staatshaushalt, Wirtschaft des Vereinigten Königreichs (Kennzahlen oben rechts)).

Der Begriff „Dritte Welt“ stammt aus den 1950er Jahren und war ursprünglich politisch geprägt. Er definierte die Blockfreien Staaten, welche sich nicht durch den Kalten Krieg ideologisch vereinnahmen lassen wollten.
Zu Beginn der 1980er Jahre (also einige Jahre vor dem Ende des Ost-West-Konfliktes und auf einem Höhepunkt des Kalten Krieges) wurde vorgeschlagen, den Begriff „Dritte Welt“ nicht mehr zu verwenden. Ulrich Menzel schrieb in seinem 1992 veröffentlichten Buch „Ende der Dritten Welt“, die Zweite Welt sei verschwunden; deshalb könne keine Dritte Welt mehr existieren. Viele ehemalige „Dritte-Welt-Gruppen“ nannten sich daraufhin „Eine-Welt-Gruppen“.
Das BMZ verwendet den Begriff „Dritte Welt“ kaum noch; in der Alltagssprache wird der Begriff weiterhin verwendet.

Die Begriffe „reich“ und „arm“ definieren den Entwicklungszustand eines Landes nur unzureichend. Sie finden ihre Verwendung eher in Verbindung mit Vermögen von Einzelpersonen. So findet sich Armut auch in Ländern mit hohem Durchschnittseinkommen (beispielsweise in Deutschland oder der Schweiz) und Reichtum in Entwicklungsländern (zum Beispiel in den ölexportierenden Ländern). Besser als das Durchschnittseinkommen ist das Mittlere Einkommen geeignet, etwas über die gesellschaftliche Entwicklung auszusagen. Ähnliches gilt für das Mittlere Vermögen. Wichtige Faktoren sind zudem die Einkommensverteilung und die Vermögensverteilung.

Manche Forscher der Entwicklungsökonomie wie der Nobelpreisträger Theodore W. Schultz haben entdeckt, dass ein Landwirt, der schreiben und lesen kann, in Entwicklungsländern produktiver ist als ein Analphabet. Daher befürworteten sie die Investition in Humankapital (Bildung, Gesundheit, etc.) als wirksames Mittel für die Entwicklung. Andere, wie Mohammed Tamim, sind der Meinung, dass die Entwicklungsländer sich seit der industriellen Revolution im 19. Jahrhundert in einer langen Übergangsphase von traditionellen Lebensarten hin zu einer modernen Lebensart befinden (sozialer, wirtschaftlicher und demographischer Kulturübergang) und dass Entwicklung allein am Bildungsniveau (von der Grundschule bis zur Universität) messbar ist. Daher könne der von Walt Whitman Rostow beschriebene „“ in einem Land stattfinden, sobald dessen Bevölkerung über ein ausreichendes Bildungsniveau verfügt. Wo das Bildungsniveau hoch ist, ist auch das Entwicklungsniveau hoch. Viele Forscher sehen darin eine Kausalität. Auch ein weiterer Zusammenhang – das Bildungsniveau ist umgekehrt proportional zum Bevölkerungswachstum – wird von vielen für kausal gehalten.

Walt Whitman Rostow veröffentlichte 1960 das Buch "". Seine Inhalte und Thesen sind zusammenfassend auch als "Take-Off-Modell" bekannt. Darin beschreibt Rostow eine Abfolge von fünf Stufen der ökonomischen Entwicklung. Diese Entwicklung werde von jedem einzelnen Staat durchlaufen; sie zeige (trotz gewisser Abweichungen, Unterschiede oder Verzögerungen von Fall zu Fall) immer einen grundsätzlich gleichen Verlauf. Rostows Modell basiert auf Daten zu historischen Wirtschaftsentwicklungen, vor allem in Europa und den USA. Die Entwicklungsländer befinden sich – in der Terminologie dieses Modells – in Phase drei, dem „Take-off“. Die Phasen drei, vier und fünf setzen ein gewisses Bildungsniveau der Bevölkerung eines Landes voraus.

Strukturelle Probleme wirken grundsätzlich über längeren Zeitraum und äußern sich in Vernetzung bestimmter Phänomene. Mit Strukturen sind Basiselemente und Wirkungszusammenhänge gemeint, welche interne Vorgänge und Reaktionsweisen eines Systems prägen.

In der Regel sind für strukturelle Probleme der Entwicklungsländer viele verschiedene Faktoren verantwortlich. Zu den Ursachen dieser strukturellen Probleme und des relativ geringen Entwicklungsniveaus in den betroffenen Ländern existieren eine Vielzahl von Entwicklungstheorien. Die meisten Theorien betonen dabei entweder stärker die endogenen (vom betreffenden Land selbst verursachten) oder die exogenen (extern verursachten) Faktoren. Ziel der Entwicklungspolitik ist, diese strukturellen Probleme zu beseitigen.

Charakteristisch für Entwicklungsländer ist die oft unzureichende Fähigkeit, die eigene Bevölkerung mit lebensnotwendigen Gütern und Dienstleistungen zu versorgen; mit anderen Worten: ihr ein menschenwürdiges Leben zu ermöglichen. In diesem Zusammenhang hat beispielsweise die Weltbank nachgewiesen, dass die Mehrheit der lateinamerikanischen Staaten bereits durch geringe Umverteilung des dort vorhandenen Reichtums in der Lage wäre, die Massenarmut zu überwinden. Hier handelt es sich also nicht um ein Produktionsproblem, sondern um ein politisches Strukturproblem.

Strukturelle Probleme müssen aber nicht zwangsläufig politischer Natur sein, sondern können auch in anderen Bereichen bestehen (Wirtschaft, Gesellschaft, Umwelt etc.). So führt die Unterversorgung der Bevölkerung zu Armut, Hunger und dadurch zu geringerer Produktivität. Dies hat eine noch schlechtere Versorgungslage zur Folge. Chronische Unterernährung hemmt darüber hinaus (vor allem bei Kindern) die geistige und körperliche Entwicklung. Dadurch ist ihre Fähigkeit eingeschränkt, durch Kreativität oder Produktivität ihre eigene Situation zu verbessern, also sich zu entwickeln.

Ein anderes strukturelles Problem ist die Diskriminierung von Frauen, was in den letzten Jahren vermehrt als grundlegende Ursache der Probleme der Entwicklungsländer erkannt wurde.

Ebenso gravierend kann sich schnelles Bevölkerungswachstum auf bereits vorhandene Entwicklungsprobleme auswirken. Wenn das Wirtschaftswachstum mit dem Bevölkerungswachstum nicht mehr Schritt halten kann, kommt es zum Beispiel in Städten zu Slumbildung und Arbeitslosigkeit sowie im ländlichen Raum zu Ernährungsproblemen und unangemessener Landnutzung (einhergehend mit schweren ökologischen Schäden).

Die Ölkrise von 1973 führte zu einer Preisexplosion des Erdöls, wovon die erdölexportierenden Länder (größtenteils OPEC) profitierten. Die ölimportierenden Industrieländer waren sowohl Opfer als auch Begünstigte (als Lieferanten der zunehmend nachgefragten Investitions- und Konsumgüter). Die ölimportierenden Entwicklungsländer konnten jedoch die entstandenen Verluste nicht durch Gegengeschäfte ausgleichen (nimmt man den Kapitalrückfluss von Arbeitsmigranten einmal aus) und wurden in ihrer Entwicklung gebremst oder zurückgeworfen. In den 80er-Jahren kam es dann schließlich aufgrund verschiedener weltwirtschaftlicher Entwicklungen zu einem dramatischen Preissturz, was zur Folge hatte, dass sich das Pro-Kopf-Einkommen von Ländern wie Libyen oder Nigeria halbierte.
Allgemein sind Entwicklungsländer durch Erdölkrisen stärker betroffen als Industrieländer und es ist davon auszugehen, dass kommende Energiekrisen weiterhin einen hemmenden Faktor in ihrer Entwicklung darstellen werden.

Unter den Merkmalen versteht man die Symptome der strukturellen Probleme.
Seit den 50er-Jahren gibt es schon die sogenannten „Merkmalslisten“, welche die zentralen Entwicklungsprobleme aufzulisten versuchen. Es ist umstritten mit welchen gemeinsamen Merkmalen die Entwicklungsländer beschrieben werden können, sollte es solche gemeinsamen Merkmale überhaupt geben. Die Kritik an einem Merkmalskatalog für Entwicklungsländer basiert vor allem auf der Tatsache, dass die Gemeinsamkeiten zweier Entwicklungsländer in Bezug auf diesen Merkmalskatalog nicht zwangsläufig größer sein müssen als zwischen einem Entwicklungsland und einem Industrieland. Auch bei einzelnen Industrieländern können die in der Liste aufgeführten Merkmale beobachtet werden. Deshalb wirft die Klassifizierung von Entwicklungsländern anhand von schematisierten Merkmalen immer wieder Fragen auf, da die verschiedenen Merkmale und ihre relative Bedeutung kontrovers diskutiert werden. Darüber hinaus bestehen zwischen den genannten Punkten Wechselwirkungen.

Ein großer Teil der ökonomischen Merkmale entsteht als direkte Folge der geringen Wertschöpfung in den Entwicklungsländern. So ist meist ein hoher Anteil der Bevölkerung in den Entwicklungsländern im primären Sektor tätig, wo volkswirtschaftlich keine große Wertsteigerung erzielt wird. Die einseitige Exportpalette (z. B. landwirtschaftliche Güter oder Bodenschätze) und die außenwirtschaftliche Ausrichtung auf die Industrieländer wurzelt auch in der kolonialen Vergangenheit.

Andere ökonomische Merkmale sind:

Viele Entwicklungsländer sind in besonderem Ausmaß von ökologischen Problemen betroffen. So kommen das UN-Umweltprogramm UNEP und das World Watch Institute zu dem Schluss, dass in den Entwicklungsländern 90 % des weltweiten Artensterbens, der Bodenerosion und der Waldrodung stattfinden. Da die natürlichen Ressourcen der Entwicklungsländer zu ihren wichtigsten Reichtümern und damit zur eigenen Existenzgrundlage zählen, treffen Umweltkrisen die Entwicklungsländer besonders hart. Aufgrund der globalen Auswirkungen von Umweltkrisen müssen hier aber auch Rolle und Verantwortung der Industrieländer betrachtet werden. Die Debatte um das Kyoto-Protokoll ist ein aktuelles Beispiel dafür.

Gravierende ökologische Merkmale sind:

Die derzeitige Entwicklung von Sterbe- und Geburtenrate, die in vielen Entwicklungsländern zu beobachten ist, lässt sich mit der frühen Phase des Modells des demographischen Übergangs vergleichen. Das bedeutet, dass ihre Bevölkerungsdynamik sich durch eine hohe Geburtenrate und eine hohe, jedoch stark rückläufige Sterberate (zum Beispiel durch bessere medizinische Versorgung) charakterisieren lässt. Dies führt zu einem starken und oft unkontrollierbaren Bevölkerungswachstum, welches mit einer extremen Verjüngung der Bevölkerungsstruktur einhergeht. Im Vergleich zum Verlauf des demographischen Übergangs in den heutigen Industrieländern, der mit der Industrialisierung einsetzte, dürfte sich die transformative Phase in den Entwicklungsländern durch noch stärker sinkende Sterberaten auszeichnen, da diese auf bereits bekanntes medizinisches Wissen zurückgreifen können.
Pandemien der Moderne (z. B. AIDS), die sich in einigen Entwicklungsländern stark verbreitet haben (in Botswana sind etwa 40 % der Erwachsenen mit HIV infiziert), können diese Entwicklung hingegen konterkarieren (vereiteln) und die Sterberaten ansteigen lassen. In solch einem Fall besitzt die Bevölkerungspyramide die Form einer Sanduhr. Besonders betroffen ist davon der wirtschaftlich aktivste Teil der Bevölkerung, sodass wiederum die wirtschaftliche Leistungsfähigkeit des Landes gemindert ist.

Demographische Merkmale sind:

Der gesundheitliche Zustand der Bevölkerung in Entwicklungsländern ist oft problematisch. Dies äußert sich beispielsweise in einer geringen Lebenserwartung und einer hohen, jedoch stark rückläufigen Säuglingssterberate. Wegen mangelnder Hygiene in Slums (z. B. fehlende Abwasserreinigung) ist die Bevölkerung in Armenvierteln besonders anfällig für Krankheiten und Epidemien (zum Beispiel Cholera; siehe Abb. rechts).

Beispiele für volksgesundheitliche Merkmale:

Unter soziokulturellen Merkmalen versteht man das Zusammenwirken von gesellschaftlichen, kulturellen und religiösen Verhaltensweisen. Ein soziokulturelles Merkmal einiger Entwicklungsländer ist beispielsweise die Benachteiligung der Frauen, wodurch Entwicklungspotentiale blockiert werden. Spezielle Initiativen, wie das Rahel-Bildungsprojekt in Äthiopien des Instituts für Weltkirche und Mission (IWM) der Philosophisch-Theologischen Hochschule Sankt Georgen und das OVC-Projekt des Bistums Adigrat in Adigrat in der Region Tigray, fördern vorrangig die Ausbildung junger Frauen in Entwicklungsländern. Auch das entwicklungshemmende wirtschaftliche Verhalten einer reichen Oberschicht kann ein soziokulturelles Merkmal sein.

Weitere soziokulturelle Merkmale:

Die politischen Probleme der Entwicklungsländer werden seit Ende der 1980er-Jahre wieder verstärkt berücksichtigt. Die politischen Merkmale sind dabei nicht nur die Folge des staatspolitischen Unvermögens der politischen Elite in einem Entwicklungsland, sondern auch der mangelnden Effizienz und Stabilität der politischen Institutionen sowie der defizitären Präsenz des Staates in den Provinzen. Das Funktionieren eines politischen Systems hängt weiterhin auch von der politischen Kultur eines Landes ab. Dazu kommt die Korruption, durch welche Staatseinnahmen nicht für Entwicklungsprogramme im eigenen Land, sondern für unsachgemäße Zwecke verwendet werden.

Weitere politische Merkmale sind:

Oft tauchen in Merkmallisten die Punkte „unzureichende Faktorausstattung“ oder „Kapitalmangel“ auf. Mit unzureichender Faktorausstattung bezeichnet man Merkmale, die aus dem Geodeterminismus abgeleitet werden können: ungünstige Klimabedingungen, fehlende Bodenfläche (zum Beispiel bei einem Inselstaat), Mangel an Bodenschätzen, Isolierung durch Binnenlage usw. Kritiker bezweifeln, dass eine unzureichende Faktorausstattung oder ein Kapitalmangel eines Landes zwangsläufig auf ein Entwicklungsland hinweist. Es handelt sich somit nicht um typischen Merkmale von Entwicklungsländern; das Fehlen von Wirtschaftsfaktoren und von Kapital kann durch andere Maßnahmen ausgeglichen werden.

Auch der umgekehrte Schluss ist nicht zulässig: Das Vorhandensein bestimmter natürlicher Gegebenheiten, wie zum Beispiel Klima, Böden oder insbesondere Rohstoffe, führt nicht automatisch zu einer Entwicklung. In einer Reihe von vielen anderen Faktoren kann es dabei beispielsweise auf die Rohstoffverarbeitung ankommen, die erst zur höheren Wertschöpfung führt oder auf eine geschickte Politik, die es vermag den Rohstoffreichtum in Entwicklung umzusetzen.
Der Kapitalmangel ist ebenfalls überbetont. Das Vorhandensein von Kapital macht noch keine Entwicklung eines Landes aus (Beispiel: ölexportierende Staaten). Folgende Punkte verhindern auch bei vorhandenem Kapital eine positive Entwicklung:


Die Einteilung der Entwicklungsländer in LDC-Staaten und LLDC-Staaten ist im internationalen Bereich noch gebräuchlich, jedoch wird sie selbst von einigen UN-Organisationen nicht mehr unterschieden. Die Aussonderung der LLDC-Staaten erfolgte auf einer UN-Vollversammlung im Jahre 1971. Eine deutsche Entsprechung für diese Begriffe gibt es nicht. Nach einer Reform aus dem Jahre 1991 geschieht dies anhand von vier Kriterien:


Die differenzierten zugrunde liegenden Indikatoren genießen weltweit eine hohe Akzeptanz. Kritisiert wird der Bevölkerungsindikator, aufgrund dessen diese Einteilung wenig über die tatsächliche Verteilung von Armut in der Welt aussagt, die mögliche politische Instrumentalisierung dieser Klassifizierung und dass die Einteilung sehr aufwendig zustande kommt.

Die Aufnahme in die LLDC-Länder kann für den betroffenen Staat durchaus begehrt sein, da in den Geberländern die Qualität der Entwicklungspolitik oft an ihrer Ausrichtung auf die LLDC-Staaten gemessen wird. Daher erhalten diese bevorzugt nichtrückzuzahlende Zuschüsse ("Grants") oder Kredite zu günstigeren Bedingungen (International Development Association, IDA).

Hinter den Abkürzungen „MSAC“, „LLDC“ und „SIDS“ verbergen sich weitere Klassifikationen der UNO. Die Bezeichnung „MSAC“ ("Most Seriously Affected Countries") entstand infolge der Ölkrise 1973 und bezeichnet ein UNO-Sonderprogramm für die am stärksten betroffenen Länder. Diese Unterteilung verschwand gegen Ende der 80er Jahre aus dem UN-Vokabular. Geblieben sind die Bezeichnungen „LLDC“ und „SIDS“.

Mit „LLDC“ ("Landlocked Developing Countries") werden Länder bezeichnet, deren Außenhandel unter ihrer küstenfernen Lage erheblich leidet. Dazu zählen vor allem Ruanda, Burundi, Nepal oder in Südamerika beispielsweise Bolivien. Durch ihre ungünstige Lage können sich sowohl Importe, als auch Exporte erheblich verteuern.

Die „SIDS“ ("Small Island Developing States") formierten sich später zur „AOSIS“ (Association of Small Island States). Ihre Mitglieder vertreten gemeinsame Interessen beispielsweise in Umweltfragen wie z. B. Anstieg des Meeresspiegels, da die Inselstaaten von den Folgen der globalen Erwärmung besonders betroffen wären.

Im Jahre 1990 wurde vom UNDP ("United Nations Development Programme"), dem Entwicklungsprogramm der Vereinten Nationen, der Versuch unternommen, einen Gegenentwurf zum eindimensionalen Konzept der Weltbank zu entwerfen. Dabei sollten zunehmend auch soziale Faktoren berücksichtigt werden. Der Index der menschlichen Entwicklung (HDI) wird im jährlich vom UNDP herausgegebenen "Bericht über die menschliche Entwicklung" (englisch "Human Development Report", HDR) veröffentlicht.

Kritik am HDI kam zu großen Teilen aus politischen Motivationen: Frauengruppen beklagten sich über die hohe Position Japans, ostasiatische Länder gegen die Bewertung ihrer Menschenrechtslage und andere Länder wegen ihrer Eingruppierung vor oder hinter einem bestimmten anderen Land. Auf Antrag Indiens wird der HDI seit der Mitte der 1990er Jahre in offiziellen UN-Dokumenten nicht mehr erwähnt.

Im Unterschied zu den UNO-Einteilungen in LDC und LLDC misst die Weltbank die Förderungswürdigkeit eines Landes ausschließlich mit dem Pro-Kopf-Einkommen bzw. nach dem Bruttonationaleinkommen pro Kopf. Sie unterscheidet dabei zwischen „Ländern mit niedrigem Einkommen“ (LIC; "Low Income Countries") und „Ländern mit mittlerem Einkommen“ (MIC; "Middle Income Countries"). Die MIC werden dabei noch in eine untere und in eine obere Einkommensgruppe eingeteilt. Nach der Klassifizierung von 2004 gibt es 61 LIC und 93 MIC, darunter auch einige aus Osteuropa, dem Kaukasus und Zentralasien. Nach dem Stand vom 30. Juni 2004 betrug die Obergrenze für LIC 765 US-$, für die unteren MIC 766–3035 US-$ und für die oberen MIC 3036–9385 US-$. Die Obergrenzen können sich aber geringfügig von Jahr zu Jahr ändern und sind im Vergleich zu den Vorjahren aufgrund der Inflation des US-Dollars gestiegen.
Manchmal benutzt die Weltbank auch die Ländergruppe der LIFDC ("Low Income Food Deficit Countries"), um innerhalb der LIC noch einmal Länder mit schwerwiegenden Ernährungsproblemen auszugliedern. In der Klassifizierung von 2004 taucht dieser Begriff aber nicht auf.

Die Weltbank benutzt diese Klassifizierung als analytische Datenbasis für ihre Kreditvergabepraxis. Im Sprachgebrauch der Bretton-Woods-Institutionen ist ansonsten der Begriff „Developing Countries“ gebräuchlich. Im Weiteren macht die Weltbank klar, dass die Einteilung in diese Gruppen nach Pro-Kopf-Einkommen nicht notwendigerweise den Entwicklungsstand eines Landes widerspiegelt.

Der große Vorteil dieser Klassifizierung ist seine einfache Struktur. Aufgrund der oft erhobenen und berechtigten methodischen Einwände bei der Beschränkung auf das Pro-Kopf-Einkommen hat die Einteilung der Weltbank nur einen begrenzten Aussagewert über die Entwicklung einzelner Länder. Das hat aber auch einen Grund: Die Weltbank ist eine Bank und beschränkt sich naturgemäß und im Gegensatz zur UNO auf ökonomische Faktoren.

Aufgrund der großen entwicklungspolitischen Bedeutung der Schuldenlast der Entwicklungsländer hat die Weltbank die zusätzlichen Gruppen „SILIC“ ("Severely Indebted Low-Income Countries") und „SIMIC“ ("Severely Indebted Middle-Income Countries") gebildet. Bei Letzteren gibt es die Abstufung in „mäßig verschuldet“ (MIMIC) und „wenig verschuldet“ (LIMIC).

„Severely Indebted“ bedeutet, dass drei von vier Kennziffern eine kritische Marke überschreiten. „Moderately indebted countries“ sind solche, die bei drei von vier Kennziffern 60 % der kritischen Marke überschreiten, diese aber nicht erreichen. Der Rest wird als „less indebted countries“ bezeichnet. Bei den vier Kennziffern handelt es sich um (die Werte in den Klammern geben die kritische Marke an):
Derzeit gelten 45 Länder als „severely indebted“ und 43 Länder als „moderately indebted“. Zu Letzteren zählen auch einige obere MIC wie die Türkei, Argentinien und Lettland.

Die vier Schlüsselindikatoren weisen auf zentrale Probleme der verschuldeten Entwicklungsländer hin. Allerdings ist die kritische Marke von entscheidender Bedeutung und deshalb heftig umstritten.
Während der 1990er Jahre konnte ein Teil der SILIC ihre Schuldenlast nicht mehr alleine tragen, sie wurden unter den Begriff „HIPC“ ("Heavily Indebted Poor Countries") zusammengefasst und 1996 in eine von Weltbank und Internationalem Währungsfonds (IWF) initiierte groß angelegte Entschuldungsinitiative, die sogenannte HIPC-Initiative, aufgenommen. Die erweiterte HIPC-Initiative umfasst derzeit 38 Länder, von denen aber nur 22 SILIC sind.

Die Vorstellung von „reichen“ ölexportierenden Ländern (meist eine Projektion der reichen und kleinen Golfstaaten) ist falsch. In einer Rangfolge, die neben dem Pro-Kopf-Einkommen auch soziale Indikatoren berücksichtigt, schneiden beispielsweise die arabischen Staaten sehr schlecht ab. Durch ihre Erdölreserven und durch die Politik der OPEC konnten diese zwar gewaltige Einkommenssprünge verzeichnen, waren jedoch nicht in der Lage, ihre Produktivkräfte mit lebenswichtigen Gütern und Dienstleistungen zu versorgen. Ölmilliarden wurden für unproduktive Zwecke verwendet wie zum Beispiel Luxus oder den achtjährigen Krieg zwischen dem Irak und dem Iran. Als weiteren negativen Effekt konnten durch den Ölboom marode und menschenrechtsfeindliche Regime aufrechterhalten werden, da sie sich Loyalität und Schutz erkaufen konnten. Besonders negative Beispiele dazu sind Nigeria und der Iran. Nuscheler bezeichnet die ölexportierenden Länder daher auch als „Fata Morgana der Entwicklung“.

Ölexportierende Länder spielen in der Gruppe der Entwicklungsländer eine besondere Rolle: Sie haben ein Gut, das die Industrieländer unbedingt brauchen. Die OPEC-Staaten halten zirka 3/4 der weltweiten Ölreserven und im Nahen Osten befinden sich 2/3 der Weltreserven. Dadurch entsteht eine gestärkte weltpolitische Verhandlungsposition, die ihnen einiges an politischer Macht zukommen lässt. Man unterscheidet die ölexportierenden Länder daher aus guten Gründen von den LDC und LLDC. Sie haben durch ihre Öleinnahmen ein Potenzial für Entwicklung, welches andere Entwicklungsländer nicht haben. Diese Länder werden auch in der Zukunft weltpolitisch relevant bleiben, ganz im Gegensatz zu einigen anderen Entwicklungsländern, die nach dem Ende des Kalten Krieges in eine Irrelevanzfalle geraten sind. Die Industrieländer benötigen nach wie vor das begehrte Öl und somit werden die ölexportierenden Länder ihre strategische und geopolitische Bedeutung beibehalten.

Schwellenländer ("Newly Industrializing Economies") sind eine Gruppe von Staaten, die traditionell noch zu den Entwicklungsländern gezählt werden, aber nicht mehr deren typische Merkmale aufweisen. Deshalb werden sie begrifflich von den Entwicklungsländern getrennt. Die deutsche Bezeichnung suggeriert, dass sie an der Schwelle zum Industriestaat stehen, diese „Schwelle“ ist jedoch nicht definiert. Der englischsprachige Begriff entstand in den 70ern und bezog sich ursprünglich auf die asiatischen Tigerstaaten.

Von verschiedenen Seiten (Weltbank, OECD, IWF, EG) wurden in den letzten Jahrzehnten Listen mit Schwellenländern erstellt. Eine verbindliche Liste der Schwellenländer gibt es jedoch nicht, ihre Zahl schwankt je nach Liste zwischen 10 und 30. Die Weltbank und der Internationale Währungsfonds (IWF) kategorisieren jeweils 10 Länder als Schwellenländer. Die OECD weist hingegen wesentlich mehr Länder als Schwellenländer aus. Das Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung (BMZ) und die Europäische Union unternahmen gemeinsam den Versuch, auch soziale und politische Indikatoren zur Bestimmung von Schwellenländern durchzusetzen, wurden jedoch auf internationaler Ebene abgewiesen. Daraufhin zog das BMZ seine 30 Schwellenländer umfassende Liste, die unter anderem auch Ecuador und Nicaragua enthielt, wieder zurück.

Eine besondere Beachtung erfahren im Rahmen einer Einteilung der Entwicklungsländer die ehemaligen sozialistischen Staaten der ehemaligen Sowjetunion.

Folgende Gründe sprechen für eine eigene Ländergruppe:

Bei den Transformationsländern unterscheidet man zwischen den Ländern, die durch ihre kollektive Einbindung in die EU Teil der Ersten Welt geworden sind (Polen, Tschechien, Ungarn, Slowakei, Slowenien, Litauen, Lettland, Estland, Rumänien, Bulgarien) und den "Newly Declining Countries" (NDC), die weiterhin zwischen weiterem Abstieg und Stabilisierungsbemühungen stehen (vor allem Länder in Zentralasien, z. B. Usbekistan).

Eine Reihe von Staaten ist zurzeit weder der einen noch der anderen Gruppe zuzuordnen. Durch die gestiegenen Rohstoffpreise ist die Wirtschaft in Kasachstan und in Aserbaidschan in den letzten Jahren nominell stark gewachsen; zu welcher Gruppe diese beiden Länder in Zukunft gehören werden, hängt davon ab, ob die Einnahmen erfolgreich zu einer Diversifizierung der Wirtschaft und einer Verbesserung des Bildungssystems verwandt werden. In Georgien wurden seit dem Amtsantritt Micheil Saakaschwilis Wirtschaftsreformen durchgeführt, außerdem profitiert das Land von Transiteinnahmen der 2006 eingeweihten Baku-Tiflis-Ceyhan-Pipeline. Georgien verfolgt inzwischen das Ziel, Mitglied der EU und der Nato zu werden. Armenien weist seit 2002 ein zweistelliges Wirtschaftswachstum auf. Seit dem Abschluss der ersten Phase der Osterweiterung interessiert sich die EU zunehmend für den Kaukasus (genauer: Transkaukasien, d. h. Georgien, Armenien und Aserbaidschan). Im Rahmen der Europäischen Nachbarschaftspolitik werden wohl noch 2006 Partnerschafts- und Kooperationsverträge mit den drei genannten Ländern abgeschlossen.

Durch die gewaltsamen und blutigen Konflikte in den 1990er Jahren wurde die Gruppe der Gescheiterten Staaten gebildet. Als Ländergruppe tauchen die Gescheiterten Staaten erstmals in einem Artikel von "Le Monde diplomatique" 1999 auf. Sie sind charakterisiert durch den vollständigen Kollaps des Staatsapparats, wobei der Staat (bzw. Reste davon) nicht mehr fähig ist, sein Territorium zu kontrollieren, keine staatlichen Dienstleistungen mehr anbietet und eine politische Ordnung nicht mehr erkennbar ist. Diese Länder fallen dadurch sowohl aus dem Erklärungsbereich der Entwicklungstheorien als auch aus dem Zielgebiet der Entwicklungspolitik. Zu ihnen zählen mehrheitlich afrikanische Staaten wie beispielsweise DR Kongo, Liberia, Somalia und Sierra Leone.
Darüber hinaus schafft das hier entstehende Ordnungsvakuum besondere Anforderungen an die Entwicklungspolitik und Sicherheitspolitik, da die Krisen solcher Länder die Entwicklung und Sicherheit ganzer Regionen und schließlich der ganzen Welt bedrohen (internationaler Terrorismus). Wie mit solchen Staaten umgegangen werden soll, ist globalpolitisch noch unklar.





</doc>
<doc id="8960" url="https://de.wikipedia.org/wiki?curid=8960" title="Industriestaat">
Industriestaat

Als Industriestaat, Industrieland oder Staaten der Ersten Welt bezeichnet man technisch und wirtschaftlich hoch entwickelte Staaten (Länder) mit einer bedeutenden industriellen Produktion. In ihnen besteht eine durch hochspezialisierte Arbeitsteilung gekennzeichnete Industriegesellschaft.

Industriestaaten im ursprünglichen Sinn waren Staaten, die ihre Wirtschaftskraft überwiegend aus industrieller Produktion erzeugten, im Gegensatz zu den Agrarstaaten, in denen die Landwirtschaft überwog. Der erste Staat der Welt, auf den diese Definition zutraf, war Großbritannien. Dort setzten die Anfänge der Industrialisierung bereits Ende des 18. Jahrhunderts ein. Im 20./21. Jahrhundert wurde der Dienstleistungssektor weltweit bedeutender.

Der Begriff Industriestaat wird heute ausgeweitet auf Staaten mit einer hoch entwickelten, starken, überwiegend vom Dienstleistungssektor getragenen Wirtschaft. Die Industriestaaten sind deshalb heute die wirtschaftsstärksten und somit reichen Staaten. 

Die Wortwahl "Industriestaat" kann irreführend sein, da sie, wie in der englischen Sprache ("industrialized country" bzw. "developed country"), eigentlich "industriell entwickelter Staat" bzw. "entwickelter Staat" heißen müsste. In der deutschen Sprache wird er allgemeinsprachlich verwendet.
Die Bezeichnung wird also der Bedeutung nicht mehr gerecht, da die Einteilung der Staaten vorwiegend nicht mehr nach dem Grad ihrer Industrialisierung erfolgt, sondern überwiegend auf der Basis des Bruttoinlandsprodukts pro Einwohner. Staaten mit einem hohen Dienstleistungssektor nehmen dabei in der Rangfolge die vorderen Plätze ein. In Veröffentlichungen werden deshalb Begriffe verwendet, wie „OECD-Staaten und übrige marktwirtschaftlich organisierte Industriestaaten“, „Staaten der Ersten Welt“ oder englisch "advanced economies".

Der Begriff Industriestaat wird unter anderem zur Abgrenzung gegenüber den Schwellenländern und den Entwicklungsländern angewendet.

Der Begriff "Erste Welt" wurde zur Zeit des Kalten Krieges für die hochindustrialisierten marktwirtschaftlichen Länder geprägt, während die planwirtschaftlich organisierten Staaten als "Zweite Welt" bezeichnet wurden. Mit dem Ende des Ost-West-Konflikt hat diese Einteilung an Bedeutung verloren. Dagegen ist die Bezeichnung "Dritte Welt" gebräuchlich geblieben.

Als weitere Kriterien der Einteilung von Staaten könnten auch gelten: Infrastruktur, Investitionstätigkeit, Handelsbedingungen (Terms of Trade), aber auch demographische Merkmale (z. B. Lebenserwartung und Bildungsstand), die Gesundheitsversorgung oder ökologische Bedingungen. Tatsache ist aber, dass zur begrifflichen Abgrenzung von Staaten fast ausschließlich das ökonomisch orientierte Bruttoinlandsprodukt pro Einwohner dient.

Von verschiedenen Seiten (zum Beispiel Weltbank, OECD, IWF, EG) wurden in den letzten Jahrzehnten Listen mit Schwellen- oder Entwicklungsländern erstellt. Eine verbindliche Liste gibt es jedoch nicht; die Zahlen in den Listen schwanken erheblich und die Festlegungen sind auch politisch gekennzeichnet. Allgemeingültige, messbare und akzeptierte Normen fehlen.

1990 begann das Entwicklungsprogramm der Vereinten Nationen (englisch "United Nations Development Programme", UNDP), ein differenziertes Bewertungskonzept zu entwerfen. Dabei sollten zunehmend auch soziale Faktoren berücksichtigt werden. Der hieraus entstandene Index der menschlichen Entwicklung (HDI) wird im jährlich vom UNDP herausgegebenen "Bericht über die menschliche Entwicklung" (englisch "Human Development Report", HDR) veröffentlicht.


</doc>
<doc id="8961" url="https://de.wikipedia.org/wiki?curid=8961" title="Stagflation">
Stagflation

Der Begriff Stagflation (ein Kofferwort aus den Begriffen „Stagnation“ und „Inflation“) beschreibt eine Situation eines Währungsraumes, in der wirtschaftliche Stagnation und Inflation miteinander einhergehen. Dieses Phänomen wurde in den 1970er Jahren im Zuge der Ölkrise in fast allen westlichen Volkswirtschaften beobachtet. Die Wortschöpfung "Stagflation" wird dem 1970 verstorbenen britischen Finanzminister Iain Macleod und John Overcountry zugeschrieben.

Auslöser von Stagflationen sind in der Regel Angebotsschocks, etwa durch einen Krieg im Nahen Osten stark steigende Energiepreise. Der stark gestiegene Ölpreis erhöht die Produktionskosten. Die Unternehmen reagieren darauf indem sie die Produktion verringern. Bei gleichbleibender gesamtwirtschaftlicher Nachfrage steigen die Preise. Es kommt also zu der Situation, dass das Wirtschaftswachstum sinkt, während die Arbeitslosigkeit und die Inflation steigen. Wenn sich eine Inflationserwartung herausbildet, kann dies zudem zu einer Lohn-Preis-Spirale führen, welche die Stagflation noch verschärft.

Eine Stagflation wurde in den 1970er Jahren im Zusammenhang mit der Ölkrise beobachtet. Die OPEC verknappte 1973 wegen politischen Spannungen im Nahen Osten die Ölförderung und verdoppelte damit den Ölpreis innerhalb von zwei Jahren (1973–75). In den westlichen Industriestaaten kam es daraufhin zu einer deutlichen Erhöhung der Inflationsraten, von 6 % (1972) auf 13 % (1974). 1979/80 kam es erneut zu einer starken Steigerung des Ölpreises und zu einer stark erhöhten Inflationsrate. Mitte der 1980er Jahre schwand die Einigkeit des OPEC-Preiskartells, der Ölpreis entspannte sich. Zu dieser Zeit kam es auch zu einem starken Anstieg der Arbeitslosenquote. Im Jahr 1975 stieg die Arbeitslosenquote der USA auf 8,4 % und hatte sich somit gegenüber dem Jahr 1973 ebenfalls fast verdoppelt (4,9 %).

Ab 2002 kam es erneut zu einem starken Anstieg des Ölpreises, der 2006–08 den Höhepunkt erreichte. Dieser Anstieg des Ölpreises verursachte keine Stagflation. Warum dies so war, ist volkswirtschaftlich noch ungeklärt. Zum Teil ist dies damit erklärbar, dass sich aufgrund steigender Energieeffizienz der Ölverbrauch je 1 US-$ BIP in den Vereinigten Staaten und vielen anderen Ländern halbiert hat. Die meisten Ökonomen sind der Ansicht, dass wir hauptsächlich Glück hatten.

Alban W. Phillips entdeckte 1958 den statistischen Zusammenhang zwischen Inflation und Arbeitslosenquote. Dieser wurde in der ursprünglichen Phillips-Kurve dahingehend interpretiert, dass ein einfacher trade-off zwischen Inflation und Arbeitslosenquote bestehe. Diese simplistische Sichtweise wurde durch die Stagflation widerlegt. 

Nach Ansicht des heutigen Mainstreams der Volkswirtschaftslehre (Monetarismus, Neukeynesianismus) besteht ein simpler trade-off zwischen Inflation und Arbeitslosenquote nur bei stabilen Inflationserwartungen. Wenn sich bei den privaten Haushalten eine erhebliche Inflationserwartung verfestigt, kann steigende Arbeitslosigkeit mit erhöhter Inflation einhergehen (Verschiebung der Phillips-Kurve). Die Entwicklung einer Stagflation wird auch als Problem für die traditionellen Instrumente (über Nachfragesteuerung) einer keynesianisch begründeten Globalsteuerung betrachtet, da die zu Grunde liegende Theorie nicht das gleichzeitige Auftreten von Stagnation und Arbeitslosigkeit vorsieht. Monetaristen und Angebotstheoretiker empfehlen die Brechung der Inflationserwartungen um die Inflation zu reduzieren. Monetaristische Konzepte lösten keynesianische Konzepte zunehmend ab den 1970ern ab.

Die heterodoxe Österreichische Schule sieht den Ursprung der Stagflation hingegen in der ihrer Ansicht nach grundsätzlich schädlichen Wirkung einer wachsenden Geldmenge, weil durch eine anfänglich ungleiche Verteilung des neuen Geldes Produktionsentscheidungen und Güterpreise verzerrt würden.


</doc>
<doc id="8965" url="https://de.wikipedia.org/wiki?curid=8965" title="ELIZA">
ELIZA

ELIZA ist ein 1966 von Joseph Weizenbaum entwickeltes Computerprogramm, das die Möglichkeiten der Kommunikation zwischen einem Menschen und einem Computer über natürliche Sprache aufzeigen sollte.

Den Namen ELIZA wählte Weizenbaum in Anlehnung an das Schauspiel Pygmalion von George Bernard Shaw. Das Programm kann über Skripte verschiedene Gesprächspartner simulieren. Bekannt geworden ist es für die oberflächliche Simulation eines Psychotherapeuten, der die non-direktiven Methoden der klientenzentrierten Psychotherapie nach Carl Rogers verwendet.

Weizenbaum schrieb das Programm in MAD-SLIP für eine IBM 7094, die das Time-Sharing-System CTSS des Massachusetts Institute of Technology verwendete.

ELIZA kann als frühe Umsetzung des Turing-Tests gesehen werden. Diesen hätte es jedoch nicht bestanden, da ein Benutzer recht einfach herausfinden kann, dass er mit einer Maschine kommuniziert.

In einer einfachen Version verwendet ELIZA einen Thesaurus, also ein strukturiertes Wörterbuch. Der eingegebene Satz wird durchsucht, ob er ein Wort enthält, das im Thesaurus vorhanden ist, und dann von diesem Wort ausgehend nach Synonymen, meist aber nach Oberbegriffen gesucht. Das Programm enthält weiterhin eine Sammlung von Phrasen zu verschiedenen Themengebieten, häufig Fragen oder Aufforderungen, weiterzureden.
Exemplarisch kann das folgende Frage-Antwort-Paar herangezogen werden:

In diesem Fall hat das Programm die im Thesaurus festgelegte Zuordnung "Vater" zu dessen Oberbegriff "Familie" umgesetzt und eine Phrase zum Themenkomplex "Familie" herausgesucht. Diese einfache Funktionsweise zeigt die Beschränkung des Programms, das den Benutzer keineswegs versteht. So führt das Schlüsselwortprinzip auch zu folgendem scheinbaren „Dialog“:

Wenn das Programm kein Wort findet, mit dem es etwas anfangen kann, antwortet es mit allgemeinen Ausweichphrasen wie z. B.

Die Wahl des Psychotherapeuten als simulierten Gesprächspartner begründete Weizenbaum damit, dass es einem solchen Gesprächspartner erlaubt ist, keinerlei Wissen über die Welt zeigen zu müssen, ohne dass dadurch seine Glaubwürdigkeit verloren geht. In seinem Artikel verdeutlicht Weizenbaum dies anhand eines Beispiels: Wenn der menschliche Gesprächspartner den Satz „Ich bin mit dem Boot gefahren“ äußert, und der Computer antwortet darauf mit „Erzählen Sie mir etwas über Boote“, wird der Mensch nicht annehmen, dass sein Gesprächspartner kein Wissen über Boote besitzt.

Das Kommunikationsverhalten von Versuchspersonen gegenüber dem Programm entsprach demjenigen gegenüber einem menschlichen Gesprächspartner. Offensichtlich war es ihnen nicht allzu wichtig, ob der Antwortende am anderen Ende der Leitung wirklich ein Mensch war oder ein Computerprogramm. Es kam nur darauf an, dass die Antworten und Fragen „menschlich“ erschienen. Dies ist der sogenannte Eliza-Effekt, der heute bei vielen Chatbots ausgenutzt wird.

Die Versuchspersonen in den Experimenten waren zu einem großen Teil sogar überzeugt, dass der „Gesprächspartner“ ein tatsächliches "Verständnis" für ihre Probleme aufbrachte. Selbst wenn sie mit der Tatsache konfrontiert wurden, dass das Computerprogramm, mit dem sie „gesprochen“ hatten, auf der Basis einiger simpler Regeln und sicherlich ohne „Intelligenz“, „Verstand“, „Einfühlungsvermögen“ usw. einfach gegebene Aussagen in Fragen umwandelte, weigerten sie sich oft, dies zu akzeptieren.

Weizenbaum war erschüttert über die Reaktionen auf sein Programm, vor allem, dass praktizierende Psychotherapeuten ernsthaft daran glaubten, damit zu einer automatisierten Form der Psychotherapie gelangen zu können. Er entwickelte sich nicht zuletzt aufgrund dieser Erfahrungen zu einem Gesellschaftskritiker. Diese Entwicklung Weizenbaums ist Thema eines Dokumentarfilms mit dem Titel "Plug & Pray", der 2010 veröffentlicht wurde.

In weiterentwickelten Versionen kann ein Satz auch grammatikalisch zerlegt und analysiert werden, um so z. B. Verneinungen zu erkennen oder Fragesätze von Aussagen unterscheiden zu können. In komplexeren und leistungsfähigeren Weiterentwicklungen tritt an die Stelle des "Thesaurus" meist eine Ontologie. Damit können komplexere Abhängigkeiten verarbeitet und eventuell auf die Gesprächshistorie eingegangen werden. Eventuell ist es dann nicht notwendig, komplette Sätze zu speichern, da die Antwort aus mehreren Satzfragmenten zusammengesetzt und ein Satz entsprechend variiert werden kann. Dadurch werden Antwortvarianten möglich, wie:

Mit weiterentwickelten Systemen lassen sich heute zu umrissenen Themenbereichen Anwendungen realisieren, z. B. eine Fahrplanauskunft. Auch Jahrzehnte nach der Erstentwicklung stößt ein solches System aber nach wie vor schnell an seine Grenzen.
Innerhalb dieser engen Grenzen lassen sich Ansätze zu "Verständnis" postulieren, sofern das System auf eine Anfrage eine passende Antwort gibt. Ein echtes Verständnis erreichen aber auch solche Weiterentwicklungen nicht.




</doc>
<doc id="8967" url="https://de.wikipedia.org/wiki?curid=8967" title="Sternbild">
Sternbild

Unter Sternbild wird in der heutigen Astronomie eine Region des Sternenhimmels verstanden, die in Bezug auf einen Sternenzug freiäugig sichtbarer heller Sterne von anderen Himmelsregionen abgegrenzt ist. Für den Anblick des Himmels von der Erde aus wurden so in der sphärischen Astronomie Anfang des 20. Jahrhunderts mit internationaler Übereinkunft die Grenzen von 88 Sternbildern festgelegt, mit denen die Fläche der Himmelskugel restlos aufgeteilt ist, um Himmelskörper leicht zuordnen zu können.

Mit freiem Auge, ohne Hilfsmittel, sind am Sternenhimmel bis auf einige wenige Ausnahmen – wie den Andromedanebel – nur Sterne zu sehen, die unserer Galaxie, der Milchstraße, angehören. Helle Sterne wurden schon in der Jungsteinzeit zur visuellen Orientierung genutzt, zu Sterngruppen zusammengefasst und als Figuren gesehen, im frühen Altertum mythologisch gedeutet und seit der Antike mythischen Gestalten, Tieren oder Gegenständen zugeordnet. Ein bekanntes Beispiel ist der sogenannte "Große Bär" (Ursa maior) beziehungsweise ein Teil dessen als "Großer Wagen", dessen zwei Kastensterne die Richtung zum Polarstern zeigen.

Die Sterne eines solchen Sternzuges oder eines Sternbildes haben von der Erde aus betrachtet untereinander relativ geringe Winkelabstände und liegen daher im Sinne der Himmelskoordinaten sphärischer Astronomie relativ nahe beieinander. Diese Nachbarschaft ist jedoch nur eine scheinbare. Durch Messung ihrer Parallaxe lässt sich für einzelne Sterne die Entfernung vom Sonnensystem bestimmen, sie kann bei den Sternen eines Sternbildes um ein Vielfaches differieren (siehe etwa Sternenliste des Orion). So können Sterne zu anderen Sternen desselben Sternbildes einen größeren Abstand haben als den zur Sonne. Andererseits können zwei Sterne verschiedener Sternbilder tatsächlich in kleinerer Distanz zueinander stehen als scheinbar eng benachbarte Sterne des gleichen Sternbildes.

Sternbilder waren in vielen Kulturen ein wichtiges Mittel zur Orientierung und insbesondere für die Seefahrt von Bedeutung. Heute dienen sie klar definiert der Kartierung des Himmels wie der örtlichen Zuordnung von Objekten. Die Internationale Astronomische Union (IAU) hat die Sternbildgrenzen nach Himmelskoordinaten festgelegt und verwendet sie u. a. zur genäherten Ortsangabe veränderlicher Himmelsobjekte wie Meteore oder Novae. Sternenzüge werden auch als Konstellation, nicht präzise definierte "Sternkonstellationen" als Asterismus bezeichnet. Dieser Begriff umfasst auch die "historischen" Sternbilder der westlichen Astronomiegeschichte und die Konstellationen anderer Kulturen.

Sternbilder lassen sich in fast allen Kulturen feststellen und bis in die frühen Hochkulturen zurückverfolgen. Dabei spielen menschenähnliche Figuren (etwa Orion), Drei- bis Sechsecke und längere Sternreihen (wie Andromeda, Fünfsternreihe, Wasserschlange) bzw. Sternzüge (Drache, Schlange, Eridanus) eine besondere Rolle. Die heutigen Sternbilder gehen zurück auf zwölf babylonische sowie altägyptische Tierkreiszeichen, die im antiken Griechenland auf 48 erweitert wurden. Zwischen 1600 und 1800 wurden noch weitere eingeführt. Seit 1922 werden international anerkannt 88 Sternbilder verwendet, deren Grenzlinien nach Vorarbeit von Eugène Delporte 1928 von der IAU offiziell definiert wurden.

Die Astrognosie ist das Fachgebiet der Astronomie, das sich mit Sternbildern und Sternenzügen befasst. Die Namen der Sternbilder sind für die systematische Benennung von Sternen mit griechischen Buchstaben (Alpha, Beta, Gamma, …) und lateinischer Bezeichnung des Sternbilds von Bedeutung, z. B. α-Centauri.

Als "Asterismus" (von lateinisch "astrum" aus griechisch "astron" „Sternbild, Gestirn, Stern“) wird in der Astronomie eine Gruppe von (meist hellen) Sternen bezeichnet, die – mit gedachten Verbindungslinien („Sternzügen“) verknüpft – eine auffällige Form oder Figur am Himmel bildet. Ein Asterismus kann Teil eines Sternbildes sein und aus benachbarten Sternen bestehen (Beispiele hierfür sind der „Große Wagen“ als Teil des Sternbilds Großer Bär oder der „Gürtel“ des Orion), er kann aber auch auffallende Sterne unterschiedlicher Sternbilder einbeziehen, wie etwa das Sommerdreieck oder das Wintersechseck, deren Sterne sich über einen großen Teil des Himmels verteilen. Charakteristisch für einen Asterismus ist seine Auffälligkeit, daher sind lichtschwache, unauffällige Sternbilder keine Asterismen. Gelegentlich werden jedoch auch historische Sternbilder, die früher in Sternkarten verzeichnet waren, wie z. B. das Sternbild „Schiff Argo“, als Asterismen bezeichnet. 

Daneben gibt es in den Kartiersystemen anderer Kulturen, etwa in der alten indischen oder chinesischen Astronomie oder der der Mayas, Sternbilder, in welchen ganz andere Figuren erkannt werden.

Sternbilder stellen sich nur subjektiv für den Beobachter dar. Wie bereits im zweiten Absatz beschrieben wurde, sind die Sterne der meisten Sternbilder weit voneinander entfernt. Der Eindruck, dass die Sterne eines Sternbilds am Himmel nahe beieinander liegen, beruht auf dem Projektionseffekt.

Die zwölf Tierkreiszeichen, die auf die babylonischen Sternbilder der Ekliptik zurückgehen, bilden eine der Grundlagen der Astrologie. Da sich die Sternbilder durch die Präzession gegen die Tierkreiszeichen seither um etwa 30 Grad verschoben haben, stimmen sie jedoch nicht mehr überein, und die westliche Astrologie lehnt die auf tatsächliche Sternbilder bezogene Deutung "(siderische Astrologie)" im Allgemeinen ab.

Sternbilder und Asterismen tauchen auch in griechischen Lehrgedichten zum Jahreslauf und Ackerbau auf, wo sie zur Einteilung der Jahreszeiten benutzt werden.

In der Astronomie wurden Sternbilder in Sternkatalogen bereits ab der Antike zur Positionsangabe benutzt. Noch um 1800 teilen Sternkataloge die Himmelsobjekte in die Sternbilder ein, bald danach geht man aber zu einer reinen Positionsangabe mit Rektaszension und Deklination über. Aber die astronomische Nomenklatur der sichtbaren Sterne im Bayer/Flamsteed-Code von 1603 und 1712 abseits ihrer Trivialnamen beruht noch auf diesem System der Areale des Sternhimmels, wie beispielsweise "Alpha Centauri" nach dem Sternbild des Zentauren.

Trotz der heute relativ geringen Bedeutung haben Sternbilder bis heute nichts von ihrer Faszination auf den Betrachter eines dunklen Sternenhimmels verloren und spielen für die Popularisierung der Astronomie eine wichtige Rolle. Die Didaktik der Astronomie nutzt sie – in Verbindung mit den dazugehörenden Sternsagen – um Jugendliche für die „Sternenkunde“ zu begeistern und anhand der historischen Entwicklungen an die moderne Astronomie heranzuführen.

Sternbilder gab es vermutlich bereits in prähistorischer Zeit. Von den meisten Kulturen, die noch in einem vor-eisenzeitlichen und schriftlosen Stadium anthropologisch untersucht werden konnten, sind jedenfalls Sternbilder bekannt, wie etwa Sternbilder der Nordamerikanischen Indianer, der Aborigines und der San im südlichen Afrika. Wie weit erste Sternbilder in die europäische Frühgeschichte zurückreichen, ist unbekannt,
aber es ist möglich, dass bereits im Stiersaal der Höhle von Lascaux ein kompletter Tierkreis abgebildet wird. Die Identifikation einer Gruppe von Punkten oberhalb des Auerochsen als die Plejaden scheint wahrscheinlich, da sowohl die Position relativ zum Auerochsen (Stier) als auch die relativen Positionen der sechs Punkte zueinander derjenigen der Plejaden entspricht. Das erfordert jedoch die implizite Annahme, im Auerochsen ein Sternbild zu identifizieren. In ägyptischen Grabanlagen gibt es ebenfalls vereinzelt Sternbilddarstellungen, etwa im Grab Sethos I. Die im Tempel von Dendera im ersten vorchristlichen Jahrhundert dargestellten Bilder zeigen die ägyptischen bereits zusammen mit dem Tierkreis der Babylonier.

Die ersten gesicherten der heutigen Sternbilder, besonders die auch in der Astrologie benutzten Tierkreiszeichen, gehen auf die Altägypter und Babylonier zurück. Die ersten dieser Sternbilder des Tierkreises tauchen bereits im dritten Jahrtausend v. Chr. auf. Ein erster vollständiger Tierkreis entstand um 410 v. Chr. Die heutigen Tierkreiszeichen stimmten vor etwa 2100 Jahren mit den entsprechenden Sternbildern ungefähr überein, haben sich aber im Zuge der Präzession gegen diese verschoben. Von einem dreizehnten Sternbild, das von der Ekliptik geschnitten wird, Schlangenträger, ragt nur ein Fuß über die Ekliptik.

Mit seinen Katasterismen beschrieb Eratosthenes die ursächliche Entstehung von 44 Sternbildern, einige Jahrhunderte später beschrieb Ptolemäus 48 Sternbilder. Die Sternbilder sind hauptsächlich nach Gestalten, Personen sowie Objekten aus der griechischen Mythologie benannt. Beide stehen damit in einer literarischen Tradition, die hauptsächlich durch die Lehrgedichte "Phainomena" (Himmelserscheinungen) des Aratos von Soloi und dem "Poeticon Astronomicon" von Hyginus bis ins Mittelalter überliefert wurde. Der ptolemäische Sternkatalog des Almagest, in dem die zu den Bildern gehörigen Sterne aufgeführt sind, ist zunächst nur in der arabischen Welt bekannt und wird dann seit dem 12. Jahrhundert durch Übersetzungen aus dem Griechischen und Arabischen auch in der lateinischen Welt verbreitet. Auf arabische Gelehrte gehen viele der heute benutzten Sternnamen zurück, aber keine neuen Sternbilder. Diese übernahm die islamische Welt aus der Antike, ältere präislamische Sternbilder verschwinden und sind nur teilweise bildhaft überliefert, zum Beispiel stellt al-Sufi die Cassiopeia zusammen mit dem Beduinensternbild Kamel dar.

Das heutige Sternbild Haar der Berenike war in der Antike als Asterismus bekannt, galt aber nicht als eigenes Sternbild, sondern als Teil des Löwen. Der von Augustus an den Himmel gesetzte „Thron des Cäsar“ wurde nach der Antike nicht mehr benutzt. Das althergebrachte Sternbild Antinous dagegen, das der römische Kaiser Hadrian zu Ehren seines im Nil verunglückten Favoriten eingeführt hatte (der Legende nach opfert sich Antinous, um das Leben Hadrians zu verlängern), galt zunächst als Asterismus und Teil des Adlers und wurde erst in der Neuzeit als eigenes Sternbild geführt. Es wird heute allerdings nicht mehr benutzt. Da Antinous in den Klauen des Adlers dargestellt wurde, wurde das Sternbild auch gelegentlich als Ganymed gesehen, zu dessen Mythos eine solche Darstellung passt.

Weitere Sternbilder wurden in der Neuzeit dann zunächst am Südhimmel eingeführt, der Europäern in der Antike unbekannt gewesen war. Johann Bayer übernahm in der "Uranometria" von 1603 einige Sternbilder von Petrus Plancius’ Himmelsgloben, der sie wiederum nach Beschreibungen der niederländischen Seenavigatoren Pieter Dirkszoon Keyser und Frederick de Houtman als erster auf seinen Himmelsgloben darstellte. Oft wird dennoch Bayer als derjenige, der diese Sternbilder einführte, genannt, vermutlich weil seine "Uranometria" eine ungleich größere Verbreitung hatte. Bayer übernimmt jedoch nur dreizehn von Plancius Sternbildern, das Kreuz lässt er aus. Die heute bekannten Sternbilder Giraffe und Einhorn wurden erst 1612 (ebenfalls von Plancius) veröffentlicht.

Diese drei Sternbilder tauchen, zusammen mit anderen von Plancius, die heute nicht mehr benutzt werden, zunächst bei Jakob Bartsch 1624 auf, einige davon übernimmt schließlich Johannes Hevelius in seinem 1687 erschienen Atlas "Firmamentum Sobiescianium", der noch weitere Bilder festlegt. In Unkenntnis von Plancius Werk werden Giraffe, Kreuz und Einhorn auch gelegentlich Hevelius, Bartsch oder dem ansonsten wenig bekannten Astronomen Augustin Royer zugeschrieben. Hevelius widmete seinen Sternatlas seinem König, Jan III. Sobieski, und platzierte dazu den Wappenschild dessen Hauses als Sternbild Schild an den Sommerhimmel. Das Frontispiz zeigt Hevelius mit dem neuen Sternbild, vor der Muse Urania im Kreis berühmter Astronomen. Nicolas Louis de Lacaille erweiterte um 1750 nach einer Beobachtungsreise zum Kap der Guten Hoffnung in Südafrika die Sternbilder des Südhimmels um meist thematische Sternbilder, die den technischen Fortschritt symbolisieren sollten, etwa der Chemische Ofen oder die Luftpumpe. Neben zwölf neuen Sternbildern geht auf ihn auch die Aufteilung des „Schiffes Argo“ in Segel des Schiffs (Vela), Achterdeck (Puppis) und Kiel (Carina) zurück. In diesem Gebiet des Himmels führte Lacaille auch ein weiteres nautisches Sternbild ein, den Kompass.

Julius Schiller versuchte 1627 durch Herausgabe eines christianisierten Sternatlas, des "Coelum Stellatum Christianum", die heidnischen Sternbilder durch Figuren der Bibel und Heilige zu ersetzen, wobei er auf Bayers Katalogkoordinaten zurückgriff und diese, in Zusammenarbeit mit Bayer, verbesserte und erweiterte. Aus den Tierkreisbildern wurden zum Beispiel die Apostel. Der Versuch fand keinen großen Anklang, aber die Schiller’schen Konstellationen wurden immerhin von Andreas Cellarius in dessen künstlerischen Meisterwerk "Harmonia Macrocosmica" auf zwei Platten abgebildet, zusammen freilich mit den herkömmlichen Sternbildern auf weiteren Platten. Die einzig originär christlichen Sternbilder, das Kreuz und das Einhorn, hatte Bayer, auf den Schiller sich bezog, bei der Adaption von Plancius weggelassen. Einen weniger radikalen Versuch der Christianisierung unternahm Jakob Bartsch, der 1624 in seinem Buch "Usus Astronomicus" biblische Bezüge der bestehenden Sternbilder herstellt.

Dem Beispiel Hevelius’ mit dem Schild folgten in den nächsten 150 Jahren zahlreiche Hofastronomen und setzten Insignien ihrer jeweiligen Herrschaft an den Himmel. Zu den bekannteren und zumindest zeitweise in Himmelsatlanten erschienenen Sternbildern zählen das Brandenburgische Szepter oder der Königliche Stier von Poniatowski, andere sind dagegen, außer in den originalen Widmungsdokumenten, nie in einer Sternkarte erschienen und dienten hauptsächlich der Karriereförderung des jeweiligen Hofastronomen. Das Sternbild Schild selbst ist das einzige dieser politischen Bilder das heute noch anerkannt wird.

1754 schlug John Hill, vermutlich in satirischer Absicht angesichts der zahlreichen zeitgenössischen Erweiterungen, 13 weitere Sternbilder vor, die nach dem Zeitempfinden niederen Kreaturen gewidmet waren, zum Beispiel die Kröte, den Erdwurm, oder die Spinne. Der Scherz blieb in der Fachwelt allerdings unbeachtet. 1789, nach der Entdeckung des Uranus setzte Maximilian Hell dem Entdecker ein Denkmal, indem er gleich zwei neue Sternbilder, das große und kleine Teleskop Herschels einführte, wovon allerdings nur das große, zwischen Zwillingen und Auriga, länger auf den Sternkarten zu finden war. Sternbilder wurden mitunter auch ohne politische, wissenschaftliche oder überhaupt besondere Motivation eingeführt. So begründete Jérôme de Lalande das 1799 von ihm eingeführte Sternbild "Felis" mit „ich liebe diese Tiere sehr [...] Der Sternenhimmel hat mir genug Arbeit beschert, jetzt kann ich auch einen Scherz damit haben.“ Er hatte aber wohl als Hintergedanken, damit Voltaire zu widerlegen, der keine Katzen mochte und zu Lebzeiten gelästert hatte, dass die Katze keines der vielen Tiersternbilder sei.

Da seit der Erfindung des Teleskops immer mehr Sterne und Nebel gefunden und katalogisiert wurden, brauchte man diese neuen Sternbilder, um die Übersicht zu bewahren, besonders auch, da die antiken Sternbilder Teile des Himmels, die dem bloßen Auge unspektakulär (aber nicht sternlos) erscheinen, schlicht auslassen. Die Anzahl der bekannten Objekte nahm aber so sehr zu, dass sich allzu viele dieser Erweiterungen als unpraktisch erwiesen, und so verschwanden die späteren wieder. Ein erster Schritt zur Vereinheitlichung und allgemeinen Anerkennung der Sternbilder wurde auf dem ersten europäischen Astronomenkongress 1798 unternommen, bei dem zahlreiche der in den Jahren zuvor vorgeschlagenen Sternbilder verworfen, andere neu vorgeschlagen wurden. In einem Sternatlas von 1801 von Johann Elert Bode, der an dem Kongress teilgenommen hatte, sind immerhin noch insgesamt 99 Sternbilder eingetragen, wie etwa der „Heißluftballon“, die „Buchdruckerwerkstatt“, die „nördliche Fliege“ oder auch die „Katze“.

Selbst nach der Festlegung der Sternbilder 1928 gab es gelegentlich Vorschläge zur Umbenennung von Sternen und Sternbildern. Am bekanntesten ist wohl das 1944 in Großbritannien erschienene Buch „A better sky“ von Alan Patrick Herbert, in dem dieser die Neubenennung der Konstellationen und von fast 300 Sternen zu zeitgemäßeren Namen vorschlägt, da diese den Menschen eingängiger seien. Aus Orion sollte zum Beispiel „The Sailor“ werden und dessen Sterne nach Seefahrern wie James Cook umbenannt werden. Fünf Sterne in dem zu „The Tyrants“ umbenannten Draco sollten die Namen von Attila, Hitler, Mussolini, Robespierre und Kublai Khan tragen.

1922 wurde von Henry Norris Russell eine Liste von dreibuchstabigen Abkürzungen für 89 Sternbilder vorgeschlagen, wobei sowohl das „Schiff Argo“ als auch seine Teile aufgeführt waren. Die Gründe Russells für seine Auswahl sind nicht überliefert, aber er beschränkte sich wohl auf die Bilder, von denen Sternnamen im Harvard Revised Catalogue, einem damaligen Standardwerk, aufgeführt waren. Auf der ersten Generalversammlung der Internationalen Astronomischen Union (IAU) in Rom schlug Russell die Liste anderen Astronomen vor, die sie positiv aufnahmen, und sie wurde zunehmend gebräuchlich. Unabhängig davon vorgeschlagene zwei- und vierbuchstabige Abkürzungen setzten sich dagegen nicht durch. Die Sternbilder werden auch heute noch durch die drei von Russell vorgeschlagenen Buchstaben abgekürzt, z. B. bei der Bezeichnung von Sternen: Der Hauptstern im Schwan (Cygnus), α Cygni ist abgekürzt α Cyg.

Auf der ersten Generalversammlung 1922 wurde die Anzahl der Sternbilder zugleich auf 88 festgelegt. Auf der zweiten im Jahr 1925 in Cambridge wurde der belgische Astronom Eugène Delporte damit beauftragt, die exakten Grenzen dieser Sternbilder festzulegen, um jede Himmelskoordinate eindeutig einem Sternbild zuordnen zu können.
Delporte definierte die Grenzen nach den Koordinatenkreisen der Epoche vom 1. Januar 1875. Benachbarte Punkte der dabei entstehenden Linienzüge hatten also entweder gleiche Deklination oder gleiche Rektaszension. Dazu konnte er an die Vorarbeit von Benjamin Gould anknüpfen, der nach seiner Durchmusterung des Südhimmels in seinem Werk "Uranometria Argentina" die Sternbilder rund um den Himmelssüdpol schon nach den Koordinaten von 1875 abgesteckt hatte. Auf der dritten Generalversammlung 1928 in Leiden wurden die genauen Grenzen von der IAU genehmigt und endgültig festgelegt. Die Arbeit von Delporte ging 1930 in Druck (siehe Literatur). Damit sich die Zuordnung von Objekten zu den Sternbildern nicht Aufgrund der Präzession ändert, müssen die Koordinaten der Grenzen für jede Epoche berechnet werden und verlaufen auch nicht mehr exakt auf Koordinatenkreisen. Dadurch müssen zwischen den Ecken liegende Grenzpunkte heute interpoliert werden.

Das riesige Sternbild Schiff Argo, benannt nach dem Schiff der Argonautensage, wurde durch die verbindliche Grenzziehung endgültig in Vela (das Segel), Puppis (das Achterdeck) und Carina (den Kiel) aufgeteilt und von der Sternbildliste gestrichen. Diese drei Sternbilder haben daher nur einen einzigen Satz Bayer’scher Sternbezeichnungen: Es gibt zum Beispiel zwar α Car, nämlich Kanopus, aber kein α Pup oder α Vel. In ähnlicher Weise springen die griechischen Buchstaben auch zwischen den beiden nicht zusammenhängenden Teilen von Serpens (Serpentis caput und Serpentis cauda) hin und her. Die Sterne γ Aur und δ Peg existieren überhaupt nicht bzw. heißen jetzt β Tau und α And. Früher trugen sie beide Bezeichnungen nebeneinander, was jedoch heute im Sinne der Eindeutigkeit nicht mehr statthaft ist.

Eine einfache geometrische Figur der Sternbildgrenzen wie ein (sphärisches) Viereck kommt fast nur im Süden vor, und hier neunmal, während ein zehntes auf dem Äquator liegt (Sternbild Sextans). Die meisten der abgrenzenden Polygonzüge haben eine deutlich höhere Zahl an Ecken und an Seiten. Das Extrem stellt ein für die Abgrenzung des Sternbilds Draco (Drache) festgelegtes Vieleck dar mit nicht weniger als 50 Ecken beziehungsweise Seiten. Die größte Fläche eines sphärischen Polygons hat das Sternbild Hydra mit 1302,84 Quadratgrad, gefolgt von Virgo, Ursa Maior, Cetus und Hercules (alle über 1200 Quadratgrad). Das größte südliche Sternbild ist Centaurus, gefolgt von Sagittarius und Puppis. Bezogen auf die Gesamtheit aller 88 Sternbilder liegt Phönix (469,32 Quadratgrad) am nächsten an der durchschnittlichen Fläche von 468,8 Quadratgrad (der volle Raumwinkel der Himmelskugel umfasst knapp 41.253 Quadratgrad). Am kleinsten ist Crux (68,45 Quadratgrad) und danach Equuleus (71,64 Quadratgrad). Die Größenextreme von Crux und Hydra spiegeln sich auch wider in der je extremalen Anzahl von Nachbarn. Crux hat nur 2, Hydra hingegen 12 echte und einen unechten Nachbarn, der allein in einem Punkt berührt wird. Solche unechten Berührungspunkte gibt es insgesamt nur 4, davon einen am Nordhimmel (nahe α Lyncis).

Früher wurden die Sternbilder nach ihrem Bezug zur Ekliptik eingeteilt, es gab also die "Ekliptiksternbilder" und die Sternbilder nördlich der Ekliptik sowie die südlichen Sternbilder. Diese Einteilung findet man noch bis in die Mitte des 20. Jahrhunderts. Sie war zunächst durch die Tradition bestimmt, später durch die Notwendigkeit, den Bereich längs der Ekliptik zur Asteroidensuche gesondert und gründlich zu katalogisieren. Mittlerweile ist diese Gliederung nicht mehr in Gebrauch und als "Nordsternbilder" werden heute diejenigen nördlich des Himmelsäquators bezeichnet.

Alle Kulturen kennen Asterismen. Der bekannteste Asterismus der westlichen Welt ist der große Wagen, dessen Sterne einen Teil des Großen Bären ausmachen. In einigen westlichen Kulturen wird die gleiche Konstellation anders benannt, zum Beispiel "Big Dipper" („großer Löffel“) in den USA. Bekannte Asterismen sind auch das Sommerdreieck und das Wintersechseck. Der weltweit und seit Jahrtausenden bekannteste ist der Sternhaufen der Pleiaden, dessen Verständnis als eigenständige Gruppe sich fast in jeder Kultur nachweisen lässt. Asterismen können im Laufe der Zeit zu Sternbildern werden, wie etwa beim Haar der Berenike geschehen.

Die alten Ägypter teilten den Himmel weniger nach Sternbildern ein, es sind nur wenige bekannt, und diese stimmen nicht mit den modernen westlichen überein. In China folgte man einer anderen Tradition, die chinesischen Sternenkonstellationen sind kleiner als die westlichen, alleine die Ekliptik wird von der chinesischen Tradition in 28 aufgeteilt, entsprechend der Strecke, die der Mond pro Tag zurücklegt. Ähnliche 28-segmentige Aufteilungen gibt es auch in Indien und im islamischen Raum, sie werden aber nicht mit figürlicher Darstellung wie die Sternbilder verbunden. Die Darstellung in chinesischen Sternkarten ist auch für die anderen dort bekannten Sternbilder nicht figürlich, sondern wie in den moderneren westlichen Sternkarten durch mit Linien verbundene Sterne. Neben astronomischen Sternkarten sind auch zur Seenavigation benutzte Sternkarten in dieser Tradition erhalten.

Bei den Azteken spielte das Sternbild des Feuerbohrers eine große Rolle in einer alle 52 Jahre stattfindenden Erneuerungszeremonie. Welche Sterne dieses bildeten, ist heute umstritten. Nur wenige der aztekischen Sternbilder sind bekannt, und nur ein paar davon können am Himmel lokalisiert werden.

Die prä-islamischen Beduinensternbilder Arabiens sind ebenfalls nur in Ausnahmefällen bekannt und am Himmel lokalisiert.

Die australischen Aborigines und die San (Buschleute) im südlichen Afrika kennen außer den durch Sterne gebildeten Bildern noch weitere. Die dunklen Staubwolken vor dem Band der Milchstraße werden von den Aborigines als Emu, von den San als Strauß erkannt, mit dem Kohlensack als Kopf und den Staubbändern vor der Milchstraße im Schützen als Körper. Dies sind die größten „Stern“-Bilder am Himmel. Daneben kennen zumindest die Aborigines noch weitere Dunkelsternbilder.

Die pazifischen Völker haben nur wenige Sterne und Sternbilder benannt. Neben den Plejaden, deren Sichtbarkeit am östlichen Abendhimmel den Jahresanfang markiert, sind vor allem Dinge der alltäglichen Umwelt und Meeresbewohner als Sternbilder verewigt. Während einige Sternbilder deckungsgleich mit den westlichen sind, unterscheiden sich die Grenzen der meisten.

Die Bewohner der Insel Manus nördlich von Papua-Neuguinea kennen auch heute noch unter anderem die folgenden Bilder: Die Gürtelsterne des Orion gelten als Kanuinsassen, die Südliche Krone als Netz, der Fluss Eridanus als Fischnetz. Ein riesiges Sternbild ist der Vogel mit den Sternen Sirius, Canopus und Prokyon. Zu den Meerestieren zählen die Krabbe (Nördliche Krone), und als Fische der Hai (Teile des Schützen und des Skorpions), der Stachelrochen (der Teil des Skorpions mit den Scheren) und weitere Fischarten, die zum Beispiel im Delphin oder in einigen Sternen des Zirkels gesehen werden. Mit den Sternbildern sind keine Sagen verbunden, sondern höchstens kurze Geschichten, die sich in wenige Worte fassen lassen. Besonders die Fischsternbilder spielen hierbei eine interessante Rolle. In der Hauptfangsaison steht keines davon am Himmel, sondern nur wenn sich das Fischen nicht lohnt. Die Sternbilder am Himmel symbolisieren so die Abwesenheit der Fische im Meer. Auch der Beginn des Monsuns wird in Verbindung mit dem dann gerade aufgehenden Sternbild Vogel gebracht. Anders als andere Kulturen benutzten die Manus die Sternbilder nicht zur Navigation, weil man nach ihrer Aussage „jeden Stern nehmen kann, denn sie bewegen sich alle gleich“.





</doc>
<doc id="8969" url="https://de.wikipedia.org/wiki?curid=8969" title="Samuel Barber">
Samuel Barber

Samuel Barber (* 9. März 1910 in West Chester, Pennsylvania; † 23. Januar 1981 in New York) war ein US-amerikanischer Komponist.

Barber begann im Alter von sieben Jahren zu komponieren. Mit neun erklärte er in einer Mitteilung an seine Mutter:

Barber studierte am Curtis Institute of Music in Philadelphia, bevor er 1935 Mitglied der American Academy in Rom wurde. Im Jahr darauf schrieb er sein Quartett in B-dur, dessen bekannten zweiten Satz er für Streichorchester als "Adagio for Strings" (UA. 1938 unter Arturo Toscanini) arrangierte. 1961 wurde er in die American Academy of Arts and Sciences gewählt.

Die Popularität des "Adagio" hat das restliche Schaffen des Komponisten überstrahlt. Er wird jedoch als einer der talentiertesten amerikanischen Komponisten des 20. Jahrhunderts angesehen. Er vermied den Experimentalismus einiger anderer Komponisten seiner Generation und bevorzugte relativ traditionelle Harmonien und Formen. Sein Werk ist melodiös und wurde oft als die "neo-romantische Periode" in der Musik beschrieben. Keines seiner anderen Werke kam der Popularität des "Adagio" nahe, aber einige werden noch immer aufgeführt und aufgenommen.

Barbers Lebensgefährte war der Komponist Gian Carlo Menotti.

Neben dem "Adagio for Strings" ist sein bekanntestes Werk die Oper "Vanessa" nach einem Libretto von Gian Carlo Menotti, uraufgeführt am 15. Januar 1958 in der Metropolitan Opera in New York. Gian Carlo Menotti verfasste auch das Libretto zu Samuel Barbers neunminütiger Kurzoper "A Hand of Bridge" (1959).

Die "neue" Met im Lincoln Center wurde am 16. September 1966 mit der Uraufführung seiner Oper nach der gleichnamigen Tragödie Antonius und Cleopatra von William Shakespeare eingeweiht.

Von seinen Klavier-Kompositionen ist die Sonate op. 26 die bedeutendste, nicht nur, weil sie seine einzige Auseinandersetzung mit der Zwölftonmusik darstellt, sondern auch aufgrund ihrer enormen Schwierigkeiten für den Pianisten – so vor allem in der auf Wunsch von Vladimir Horowitz (der diese Sonate am 9. Dezember 1949 uraufführte) hinzugefügten vierstimmigen Schluss-Fuge über ein mit zahlreichen Intervall-Sprüngen im schnellen Tempo gespicktes Thema.









</doc>
<doc id="8970" url="https://de.wikipedia.org/wiki?curid=8970" title="Verbrechen gegen die Menschlichkeit">
Verbrechen gegen die Menschlichkeit

Verbrechen gegen die Menschlichkeit ist ein Straftatbestand im Völkerstrafrecht, der durch einen ausgedehnten oder systematischen Angriff gegen eine Zivilbevölkerung gekennzeichnet ist. Erstmals völkervertraglich festgelegt wurde der Tatbestand 1945 im Londoner Statut des für den Nürnberger Prozess gegen die Hauptkriegsverbrecher des NS-Regimes geschaffenen Internationalen Militärgerichtshofs. Die heute wichtigste vertragliche Rechtsquelle ist Artikel 7 des Römischen Statuts des Internationalen Strafgerichtshofs. Die Strafbarkeit von Verbrechen gegen die Menschlichkeit ist daneben auch völkergewohnheitsrechtlich anerkannt. Entsprechende Straftatbestände finden sich in einer Vielzahl nationaler Strafgesetzbücher.

Eine wichtige völkerrechtliche Setzung war die Verurteilung des Völkermordes an den Armeniern im Osmanischen Reich am 24. Mai 1915 in einer Protestnote durch die Triple Entente; England, Frankreich und Russland drohten der jungtürkischen Regierung darin, diese "„Verbrechen gegen die Menschheit und gegen die Zivilisation“" würden nach Kriegsende verfolgt werden. Juristisch wurde der Begriff erstmals 1946 zur Ahndung der Kriegsverbrechen bei den Nürnberger und Tokioter Prozessen definiert und benutzt ("siehe auch:" Völkermord). Dieses Vorgehen war damals umstritten, da nach rechtsstaatlichen Prinzipien eigentlich nur Verbrechen verfolgt werden können, die "nach" dem Erlass des entsprechenden Gesetzes begangen werden. Damit soll Willkür bei Strafmaß und Definition des Straftatbestands verhindert werden. Der Hinweis auf das nationalstaatliche Rückwirkungsverbot im Strafrecht greift hier zu kurz, da das Nürnberger Tribunal sich auf das Völkerrecht bezog und auf internationale Verträge und Verbindlichkeiten hinwies, die durch das NS-Regime im internationalen Maßstab verletzt oder ignoriert worden waren.

In den Nürnberger Prozessen wie auch in mehreren Verlautbarungen der Vereinten Nationen wurde und wird die Massenvernichtung in Konzentrationslagern als Verbrechen gegen die Menschlichkeit beurteilt. Da die „industrielle Tötung von Menschen“ (Hannah Arendt) sich nicht ausschließlich gegen solche jüdischer Abstammung gerichtet habe, handle es sich nicht durchgehend um Völkermord, sondern wird unter „crime against humanity“ subsumiert.

Seit dem 1. Juli 2002 besteht der Internationale Strafgerichtshof (IStGH) in Den Haag als ständige Institution zur Verfolgung dieser Verbrechen. Der IStGH berücksichtigt den oben genannten Rechtsgrundsatz und darf nur Straftaten verfolgen, die nach dem Inkrafttreten des Römischen Statuts des Internationalen Strafgerichtshofs (1. Juli 2002) begangen wurden.

Mit der Londoner Charta verständigten sich die Alliierten auf ein gemeinsames Strafrecht, das ihren jeweiligen nationalen Rechtssystemen übergeordnet war. Sie bildete die juristische Grundlage für die Nürnberger Prozesse gegen die wichtigsten gefangenen NS-Machthaber.

Artikel 7 des 2002 in Kraft getretenen Römischen Statuts als Rechtsgrundlage des Internationalen Strafgerichtshofes definiert den Begriff „Verbrechen gegen die Menschlichkeit“ im Einzelnen. Die Vorschrift listet eine Vielzahl einzelner Handlungen (wie etwa vorsätzliche Tötung) auf, die jeweils dann ein Verbrechen gegen die Menschlichkeit darstellen, wenn sie im Zuge eines „ausgedehnten oder systematischen Angriffs gegen die Zivilbevölkerung“ erfolgen. Im Gegensatz zu Kriegsverbrechen können Verbrechen gegen die Menschlichkeit auch außerhalb bewaffneter Konflikte begangen werden. Zudem werden auch Angriffe gegen die eigene Zivilbevölkerung völkerstrafrechtlich erfasst. Der Tatbestand der Verbrechen gegen die Menschlichkeit ist weiter als der Tatbestand des Völkermordes. Während Völkermord die Zerstörung bestimmter abschließend aufgezählter Gruppen (nationale, ethnische, rassische, religiöse und soziale Gruppen) voraussetzt, können Verbrechen gegen die Menschlichkeit gegen "jede" Zivilbevölkerung begangen werden. In subjektiver Hinsicht ist erforderlich, dass ein Täter bezüglich der einzelnen Tathandlung vorsätzlich sowie in Kenntnis des systematischen Angriffs gegen die Zivilbevölkerung handelt.

Im deutschen Strafrecht sind "Verbrechen gegen die Menschlichkeit" nach Völkerstrafgesetzbuch (VStGB) überall sowie durch jeden und an jedem strafbar (Weltrechtsprinzip, siehe VStGB). Die Anwendung wird durch die Ausprägung des Opportunitätsprinzips in Strafprozessordnung (StPO) wesentlich eingeschränkt. Insbesondere kann hiernach von einer Verfolgung abgesehen werden, wenn die "Verbrechen gegen die Menschlichkeit" „vor einem internationalen Gerichtshof oder durch einen Staat, auf dessen Gebiet die Tat begangen wurde, dessen Angehöriger der Tat verdächtig ist oder dessen Angehöriger durch die Tat verletzt wurde, verfolgt werden“.

2001 verabschiedete das französische Parlament (Nationalversammlung) ein Gesetz namens "loi Taubira". In diesem Gesetz erkannte Frankreich den Sklavenhandel und die Sklaverei als Verbrechen gegen die Menschlichkeit an. (Berichterstatterin für das Gesetz war Christiane Taubira; sie war von 2012 bis 2016 Justizministerin).

Der englische Begriff "humanity" kann sowohl mit "Menschlichkeit" als auch mit "Menschheit" übersetzt werden. Neben anderen Kritikern hielten Karl Jaspers und Hannah Arendt die offizielle und übliche Übersetzung "Verbrechen gegen die Menschlichkeit" für einen Euphemismus und sprachen von "Verbrechen gegen die Menschheit". Arendt schrieb dazu in ihrem Buch über den Eichmann-Prozess 1963:




</doc>
<doc id="8990" url="https://de.wikipedia.org/wiki?curid=8990" title="Herzinsuffizienz">
Herzinsuffizienz

Die Herzinsuffizienz ist die krankhafte Unfähigkeit des Herzens, die vom Körper benötigte Blutmenge ohne Druckanstieg in den Herzvorhöfen zu fördern.

Der gebräuchliche deutsche Begriff "Herzschwäche" trifft den Sachverhalt nur ungenau, weil nicht nur eine krankhaft verminderte Pumpfunktion ("systolische Herzinsuffizienz" oder "Herzmuskel­schwäche"), sondern auch eine gestörte Füllung des Herzens ("diastolische Herzinsuffizienz") bei unbeeinträchtigter Pumpfunktion zur Herzinsuffizienz führen kann. Eine akute, schwere Herzinsuffizienz wird gelegentlich als "Herzversagen" bezeichnet, wobei eine allgemein akzeptierte Definition dieses insbesondere in Leichenschauscheinen häufig verwendeten Begriffs fehlt. Kritiker wenden ein, dass Herzversagen wohl auch deshalb als häufigste Todesursache genannt wird, weil das Herz letztlich bei jedem Verstorbenen zum Stillstand gekommen ist und die tatsächlich zum Tode führende Krankheit oft nicht ermittelt wurde.

Die Herzinsuffizienz wird nach ihrem Verlauf (akut oder chronisch), nach der überwiegend betroffenen Herzhälfte (rechts oder links, andernfalls global) und nach dem Mechanismus unterteilt.
Die akute Herzinsuffizienz entwickelt sich im Verlauf von Stunden bis Tagen. Ursachen sind:

Die chronische Herzinsuffizienz entwickelt sich im Verlauf von Monaten bis Jahren. Sie ist charakterisiert durch Kompensationsprozesse des Organismus (schnellerer Herzschlag, Verdickung des Herzmuskels, Engstellung der Blutgefäße, Vermehrung des Blutvolumens usw.), was die verminderte Pumpleistung des Herzens eine Zeit lang ausgleichen kann.
Die "kompensierte Herzinsuffizienz" zeigt keine oder erst bei stärkerer körperlicher Belastung Symptome. Im "dekompensiert"en Stadium kommt es zu pathologischen Wasseransammlungen (Ödemen) und Luftnot (Dyspnoe). Diese tritt bereits in Ruhe oder unter geringer Belastung auf.

Die Herzinsuffizienz betrifft häufig nicht beide Herzhälften gleichmäßig.

Die Linksherzinsuffizienz zeigt sich durch Rückstau des Blutes in die Lungengefäße mit Husten und Atemnot bis hin zum Lungenödem. Die eingeschränkte Pumpleistung führt zu verminderter Belastbarkeit und evtl. niedrigem Blutdruck.

Bei Rechtsherzinsuffizienz staut sich das Blut zurück in die Körpervenen. Durch den erhöhten Venendruck können sich sowohl Wasseransammlungen in den Geweben mit Ödemen vor allem der unteren Körperpartien (Beine) bilden, als auch im Bauchraum (Aszites) und in der Pleurahöhle (Pleuraerguss).

Die globale Herzinsuffizienz zeigt Symptome der Links- und Rechtsherzinsuffizienz.

Unabhängig von der Lokalisation der Störung kommt es durch die Wassereinlagerungen zur "Gewichtszunahme". In der Nacht (körperliche Ruhe, Hochlagerung der Beine) wird ein Teil der Wassereinlagerungen aus dem Gewebe mobilisiert und über die Nieren ausgeschieden; dadurch tritt vermehrtes nächtliches Wasserlassen auf ("Nykturie").

Funktionell kann man zwischen einem "Vorwärts-" und "Rückwärtsversagen" des Herzens unterscheiden. Beim Vorwärtsversagen kann kein ausreichender Druck in den Arterien aufgebaut werden, während beim Rückwärtsversagen ein Rückstau des Blutes in den Körper- und Lungenvenen stattfindet.

Nicht immer liegt der Herzinsuffizienz ein Pumpversagen mit Verminderung des Herzzeitvolumens zugrunde (Low-output-failure). Die Symptome der Herzinsuffizienz können auch auftreten, wenn ein pathologisch erhöhter Durchblutungsbedarf der Organe nicht mehr gedeckt werden kann (High-output-failure):

Die Herzinsuffizienz ist eine der häufigsten internistischen Erkrankungen mit geschätzt mehr als
10 Mio. Betroffenen in Europa. Weitere knapp 10 Mio. Menschen weisen bereits eine Herzmuskelschwäche ohne Symptome auf. Herzinsuffizienz ist in Deutschland einer der häufigsten Beratungsanlässe in einer allgemeinmedizinischen Praxis und der häufigste Grund für eine stationäre Krankenhausaufnahme.

Prävalenz und Inzidenz der Herzinsuffizienz sind altersabhängig. Im Alter von 45 bis 55 Jahren leiden weniger als 1 Prozent der Bevölkerung an Herzinsuffizienz, 65- bis 75-Jährige bereits zu 2–5 Prozent und über 80-Jährige zu fast 10 Prozent. Männer sind etwa 1,5-fach häufiger betroffen als gleichaltrige Frauen. Jedes Jahr werden ungefähr so viele Patienten mit Herzinsuffizienz diagnostiziert wie mit Brust-, Lungen-, Prostata-, und Darmkrebs zusammen. Mit zunehmendem Lebensalter steigt der Anteil der diastolischen Herzinsuffizienz auf mehr als 30 Prozent, bei Frauen auf mehr als 40 %. Menschen, die an einem Diabetes Mellitus Typ 2 leiden, haben ein zwei- bis sechsfach erhöhtes Risiko, an einer Herzinsuffizienz zu erkranken. Zudem missinterpretiert diese Gruppe Symptome der Herzinsuffizienz häufig als Altersschwäche.

In der Todesursachenstatistik Deutschlands des Statistischen Bundesamtes von 2006 liegt die Herzinsuffizienz auf Platz drei noch vor Krebserkrankungen wie Brust-, Lungen- oder Darmkrebs. Bei Frauen rückt die Herzinsuffizienz mit einem Anteil von 7,4 % unter den häufigsten Todesursachen sogar an die zweite Stelle vor.

Das rechte Herz nimmt über die obere und untere Hohlvene das Blut aus dem Körper auf und pumpt es durch die Lungenarterie "(Arteria pulmonalis)" in die Lunge (vgl. Lungenkreislauf). Sauerstoffreiches Blut fließt von dort durch die Lungenvenen zum linken Herz, von wo es durch die Hauptschlagader "(Aorta)" in den Körper gepumpt wird. Diese Pumpfunktion des Herzens kann aus verschiedenen Ursachen gestört sein.


Bei 80–90 Prozent der von Herzinsuffizienz Betroffenen liegt eine Funktionsstörung des Herzmuskels zugrunde, knapp zwei Drittel davon im Sinne einer Herzmuskelschwäche. Die häufigste Ursache der Herzinsuffizienz ist in westlichen Ländern eine Durchblutungsstörung des Herzens (Koronare Herzkrankheit oder KHK) bei 54–70 Prozent der Patienten, bei 35–52 Prozent begleitet von Bluthochdruck. Bei 9–20 Prozent ist der Bluthochdruck (Hypertonie) alleinige Ursache der Herzinsuffizienz. Eine Herzmuskelentzündung (Myokarditis) manifestiert sich bei 60–70 % der betroffenen Patienten als (meist vorübergehende) Herzinsuffizienz. Vitamin-D-Mangel begünstigt Herzinsuffizienz, ebenso ein Eisenmangel, auch ohne gleichzeitig bestehende Anämie.

In einer der weltweit größten Beobachtungsstudien mit einer Kohorte von etwa 70.000 erwachsenen Bewohnern Norwegens wurde nun auch festgestellt, dass Patienten mit schweren Schlafstörungen (Einschlafstörung, Durchschlafstörung und fehlende nächtliche Erholung) später deutlich häufiger an einer Herzinsuffizienz erkranken.<ref name="DOI10.1093/eurheartj/eht019"></ref> Dabei konnte ausgeschlossen werden, dass andere Risikofaktoren wie Übergewicht, Diabetes, Bewegungsmangel, Hypertonie, Nikotin oder Alkohol zu der vermehrten Entwicklung einer Herzinsuffizienz geführt haben.

Durch das im Zuge der Herzinsuffizienz abnehmende Herzzeitvolumen kommt es zu einer unzureichenden Durchblutung der Organe. Als Reaktion darauf werden Kompensationsmechanismen aktiviert, die die Versorgung der Organe mit sauerstoffreichem Blut gewährleisten sollen. Dazu zählt die vermehrte Ausschüttung von Hormonen wie Adrenalin und Noradrenalin, die die Schlagkraft des Herzens steigern und über eine Engstellung der Gefäße den Blutdruck erhöhen. Die Effektorhormone des Renin-Angiotensin-Aldosteron-Systems führen zu einer Steigerung des Blutvolumens und ebenfalls zur Erhöhung des Blutdrucks. Obwohl die genannten Mechanismen vorübergehend zum Erhalt der Organdurchblutung notwendig sind, führen sie doch langfristig zu einer Schädigung des Herzens und einem Fortschreiten der Erkrankung. So führen sie im Herzen zu einer Verdickung der Muskulatur (Hypertrophie). Diese ist zunächst hilfreich, um eine vermehrte Pumparbeit leisten zu können. Daneben kommt es zu vermehrtem Einbau von Bindegewebe (Fibrose). Beide Prozesse gemeinsam werden auch als „kardiales Remodeling“ bezeichnet.

Solange das Herz der erhöhten Anforderung gerecht werden kann, spricht man von kompensierter Herzinsuffizienz. Kann im Verlauf der Erkrankung ein immer größerer Anteil des Blutes nicht mehr ausgeworfen werden, kommt es zur Erweiterung der Herzkammern (Dilatation). Dadurch verschlechtert sich die Kontraktionsfähigkeit des Herzens weiter (Frank-Starling-Mechanismus) und es kommt schließlich zur Dekompensation.

Die zellulären Mechanismen, die dem „kardialen Remodeling“ zu Grunde liegen, sind Gegenstand intensiver Forschung. Unter anderem werden veränderte Genexpression, die Aktivierung intrazellulärer Signalwege, z. B. von MAP-Kinasen, und die Veränderung des Calcium-Stoffwechsels sowie des Energiestoffwechsels untersucht. Neuere Untersuchungen lieferten außerdem Hinweise auf eine Beteiligung von epigenetischen Veränderungen.

Leitsymptom der Linksherzinsuffizienz ist die Luftnot "(Dyspnoe)" zunächst bei körperlicher Belastung "(Belastungsdyspnoe)," im fortgeschrittenen Stadium auch in Ruhe "(Ruhedyspnoe)." Die Luftnot verstärkt sich oft nach dem Hinlegen, was in schweren Fällen zu bedrohlichen nächtlichen Anfällen von Atemnot und Husten führen kann "(Asthma cardiale)." Schließlich kann es zum kardialen Lungenödem („Wasser in der Lunge“) mit schwerster Luftnot und Austritt von Flüssigkeit in die Lungenbläschen "(Alveolen)" kommen, erkennbar an „brodelnden“ Nebengeräuschen bei der Atmung und schaumigem Auswurf.

Ein häufiges Symptom bei fortgeschrittener Herzinsuffizienz sind nächtliche Störungen der Atmung, häufig in Form der Cheyne-Stokes-Atmung, die durch ein periodisch wiederkehrendes An- und Abschwellen der Atmung gekennzeichnet ist.

Die Herzinsuffizienz führt zur Flüssigkeitsretention („Wasseransammlung“) im Körper, bei der Linksherzinsuffizienz in der Lunge und bei der Rechtsherzinsuffizienz hauptsächlich in den Beinen "(Beinödeme)" und im Bauchraum "(Aszites)." Weiterhin kann es zur Schädigung der Leber kommen („cirrhose cardiaque“).

Die schwerste Form der Herzinsuffizienz ist der kardiogene Schock, der sich meist mit schwerer Atemnot, Bewusstseinstrübung, kaltem Schweiß, schwachem und schnellem Puls und kühlen Händen und Füßen bemerkbar macht.

Herzinsuffizienz tritt selten allein auf. Sie ist vielmehr häufig mit begleitenden Erkrankungen vergesellschaftet, zu denen, je nach Ursache der Herzinsuffizienz, koronare Herzkrankheit, Bluthochdruck, Fettstoffwechselstörungen oder Übergewicht gehören. Die Häufigkeit der Begleiterkrankungen steht im Zusammenhang mit dem Alter des Patienten, persönlichen Risikofaktoren (z. B. Rauchen) und dem Schweregrad der Herzinsuffizienz selbst, sodass auch von einer Multisystemerkrankung gesprochen wird. Daneben sind Begleiterkrankungen anderer Organsysteme häufig anzutreffen. Dazu gehören chronische Niereninsuffizienz und die Schlafapnoe, die beide fast die Hälfte der Patienten mit Herzinsuffizienz betreffen, Eisenmangel (mehr als 40 %), Anämie (35-40 %), Diabetes mellitus (etwa 30 %), die chronisch obstruktive Lungenerkrankung (COPD, etwa 30 %), Sarkopenie (etwa 20 %) und Kachexie (etwa 10 %) sowie einige andere. Die Symptome der Begleiterkrankungen können mit den eigentlichen Herzinsuffizienz-Symptomen überlappen und gelegentlich schwer zu trennen sein.

Die Diagnose Herzinsuffizienz wird gestellt, wenn typische Symptome (s. o.) und entsprechende objektive Befunde zusammentreffen.

Bereits bei der körperlichen Untersuchung können einige klinische Zeichen auf eine Herzinsuffizienz hinweisen. Dazu zählen die Halsvenenstauung, Rasselgeräusche über der Lunge, eine Herzvergrößerung "(Kardiomegalie)," ein 3. Herzton, Unterschenkelödeme, eine Vergrößerung der Leber "(Hepatomegalie)," Pleuraergüsse, Nykturie und eine Pulsbeschleunigung.

Wichtigstes Untersuchungsverfahren bei der Herzinsuffizienz ist die Ultraschalluntersuchung des Herzens "(Echokardiografie)." Sie erlaubt eine schnelle und risikofreie Beurteilung der Herzmuskelfunktion, der Herzklappen und des Herzbeutels. Einerseits kann so die Verdachtsdiagnose Herzinsuffizienz bestätigt oder ausgeschlossen, andererseits können bereits wesentliche Ursachen festgestellt werden.

Die Röntgenaufnahme des Brustkorbes bildet u. a. das Herz und die Lunge ab. Bei leichteren Formen der Herzinsuffizienz zeigt sie in der Regel noch einen normalen Befund, in fortgeschrittenen Fällen sind eine Herzvergrößerung und eine Erweiterung der Lungenvenen („Lungenstauung“) sichtbar.

Um eine koronare Herzkrankheit als Ursache der Herzinsuffizienz feststellen oder ausschließen zu können, wird oft eine Herzkatheteruntersuchung mit Koronarangiografie durchgeführt. Dabei können die Druckverhältnisse im und am Herzen direkt gemessen und evtl. Verengungen der Herzkranzgefäße beurteilt werden.

Beurteilung der Anatomie des Herzens und der großen Gefäße bei komplexen angeborenen Vitien und bei Kardiomyopathien, Vitalitätsnachweis vor ggf. geplanter Wiederherstellung der Durchblutung (Herzkatheter, Bypass-Operation), Stresstest mit Dobutamin oder Adenosin zur Beurteilung und Identifikation relevanter Einengungen (Stenosen) im Bereich der Herzdurchblutung (Koronararterien)

Eine neue Studie zeigt, dass die Magnetresonanzspektroskopie dabei helfen kann, das Risiko einer Herzinsuffizienz besser abzuschätzen<ref name="DOI10.1126/scitranslmed.3007328">P. A. Bottomley, G. S. Panjrath, S. .. Lai, G. A. Hirsch, K. .. Wu, S. S. Najjar, A. .. Steinberg, G. .. Gerstenblith, R. G. Weiss: "Metabolic Rates of ATP Transfer Through Creatine Kinase (CK Flux) Predict Clinical Heart Failure Events and Death." In: "Science Translational Medicine." 5, 2013, S. 215re3–215re3, .</ref>.

Üblicherweise werden Blutuntersuchungen nur benötigt, um bestimmte Ursachen und Komplikationen der Herzinsuffizienz (wie Diabetes mellitus, Niereninsuffizienz oder Elektrolytstörungen) und mögliche Nebenwirkungen der Therapie erkennen zu können.

Seit Anfang des 21. Jahrhunderts steht mit der Bestimmung der Plasmakonzentration des "brain natriuretic peptide" (BNP bzw. NTproBNP) ein Test zur Verfügung, der auch in der Alltagsroutine für die Diagnostik einer Herzinsuffizienz hilfreich sein kann. Je nach Ausmaß der Herzinsuffizienz sind die Werte mäßig bis stark erhöht, während niedrig normale BNP- oder NTproBNP-Spiegel bei einem unbehandelten Patienten eine Herzinsuffizienz weitgehend ausschließen. Der Normbereich ist vom Alter und Geschlecht abhängig. Die Messung des BNP zur Differentialdiagnose und Verlaufskontrolle der Herzinsuffizienz ist inzwischen in die Leitlinien der Deutschen Gesellschaft für Kardiologie und Kinderkardiologie eingeflossen.

Wann immer möglich, sollte zunächst die Ursache der Herzinsuffizienz beseitigt werden:

Eine Reduktion kardiovaskulärer Risikofaktoren ist anzustreben. Zur nicht-medikamentösen Therapie gehören Gewichtsnormalisierung, reduzierte Kochsalzzufuhr, Limitierung der Flüssigkeitszufuhr (<2 Liter/Tag) und Alkohol- und Nikotinreduktion bzw. -karenz. In den NYHA-Stadien I-III wird moderates körperliches Training empfohlen, bei dekompensierter Herzinsuffizienz körperliche Schonung bis zur Bettruhe. Reisen in Höhenlagen sowie heißes und feuchtes Klima sollten vermieden werden. Zur dauerhaften Lebensstilveränderung kann der Besuch einer Herzschule sinnvoll sein.

Bei der medikamentösen Therapie der Herzinsuffizienz wird zwischen Medikamenten mit einer gesicherten prognostischen Indikation und solchen mit einer symptomatischen Indikation unterschieden.






</doc>
<doc id="8992" url="https://de.wikipedia.org/wiki?curid=8992" title="Falscher Freund">
Falscher Freund

Falscher Freund (oder Fauxami, „falscher Freund“) ist ein Begriff aus der Interlinguistik. Er bezeichnet Wortpaare aus verschiedenen Sprachen, die sich äußerlich stark ähneln, aber in ihren jeweiligen Sprachen eine unterschiedliche Bedeutung haben. Häufig haben beide Benennungen einen gemeinsamen etymologischen oder begrifflichen Ursprung. Falsche Freunde gehören zu den Übersetzungsschwierigkeiten und Interferenzfehlern und können zu einer falschen Übersetzung verleiten.
Normalerweise handelt es sich um "interlinguale" falsche Freunde. Das sind Paare von Wörtern oder Ausdrücken aus zwei Sprachen, die orthografisch oder phonetisch ähnlich sind, jedoch unterschiedliche Bedeutungen haben.

Daneben existieren noch innerhalb einer Sprache die "intralingualen" falschen Freunde oder Paronyme, zum Beispiel "antiseptisch" – "aseptisch". Auch Scheinentsprechungen zwischen zwei Dialekten derselben Sprache bzw. zwischen einem Dialekt und der Standardsprache gelten als falsche Freunde.

Manchmal werden auch Wörter als „falsche Freunde“ bezeichnet, die wie gewöhnliche Fremd- oder Lehnwörter anmuten, aber in der vermeintlichen Herkunftssprache keine oder eine ganz andere Bedeutung haben. Ein bekanntes Beispiel ist die deutsche Bezeichnung „Handy“ für ein Mobiltelefon. Im Englischen bedeutet das Wort "handy" nicht „Mobiltelefon“, sondern „handlich, geschickt, gewandt“. Solche Wörter werden korrekter als Pseudoentlehnung bezeichnet (oder Scheinanglizismus, Scheingallizismus und so weiter).

Cartagena/Gauger unterscheiden sechs Typen von falschen Freunden.

Annette Kroschewski legte 2000 einen differenzierten Klassifizierungsvorschlag vor:

Die beiden Wörter können ursprungsverwandt sein, sich aber verschieden entwickelt haben (etwa durch Pejoration in nur einer der beiden Sprachen), oder eine rein zufällige Ähnlichkeit aufweisen. Bei verwandten Sprachen (Hochdeutsch und Niederländisch) oder solchen, die seit langem engen Kontakt haben oder hatten (Deutsch und Französisch), sind falsche Freunde wesentlich häufiger als bei nicht verwandten Sprachen (Deutsch und Japanisch). Bei letzteren sind gleich oder ähnlich lautende Vokabeln in ihrer Bedeutung oder ihrer Funktion so verschieden, dass das Problem gar nicht erst entsteht. Verwechslungsgefahren können sich hier hauptsächlich durch Entlehnungen bei gleichzeitiger Bedeutungsverschiebung ergeben oder dadurch, dass zufällig die Wörter nicht nur ähnlich klingen, sondern auch im Satzzusammenhang dieselbe Funktion erfüllen wie bei folgenden Zahlwörtern, wo die deutsche Zahl „sechs“ dem türkischen Zahlwort „sekiz“ ähnelt, das aber „acht“ bedeutet. In anderen Fällen sind die Bedeutung und/oder die grammatische Funktion ähnlich klingender Wörter meist so unterschiedlich, dass eine Verwechselungsgefahr gar nicht erst aufkommt.

Manchmal sind die Bedeutungen nicht eindeutig voneinander abzugrenzen. So bedeutet englisch "serious" normalerweise nicht „seriös“, sondern „ernst“ oder „erheblich“. Manchmal bedeutet aber auch "seriös" so viel wie „ernst“. In diesem Überschneidungsbereich entsprechen sich die falschen Freunde tatsächlich. Ein weiteres Beispiel ist englisch "tip", das meistens „Spitze“ bedeutet und eine Reihe weiterer Bedeutungen hat, darunter auch „Tipp“. Die Abbildung am Artikelanfang zeigt ebenfalls ein Beispiel dieser Art.

Falsche Freunde können auch zur Bedeutungsverschiebung von Wörtern führen. Bei häufiger und dauerhaft falscher Verwendung kann ein Wort die ihm unterstellte Bedeutung annehmen, und die Falscher-Freund-Übersetzung wird zum Standard. Zum Beispiel hatte "realisieren" bis vor einiger Zeit nur die Bedeutung „verwirklichen, umsetzen“. Durch den dauerhaften Einfluss des englischen falschen Freundes "to realize/realise" wird "realisieren" heute auch im Sinne von „erkennen, bemerken“ verwendet. Diese Bedeutung ist mittlerweile auch von Wörterbüchern anerkannt.

Durch die enge Verwandtschaft mit dem Deutschen hat die niederländische Sprache besonders viele und auffallende falsche Freunde.



Anmerkung: es empfiehlt sich, die "faux amis" in der Richtung deutsch → französisch zu lernen, denn beim Übersetzen in die Fremdsprache sind die "faux amis" „verlockender“ als umgekehrt. Alphabetische Liste deutscher Wörter, die im Französischen zu einer Falschübersetzung verleiten:





</doc>
<doc id="8993" url="https://de.wikipedia.org/wiki?curid=8993" title="V2">
V2

V2 steht für

V-2 steht für


</doc>
<doc id="8995" url="https://de.wikipedia.org/wiki?curid=8995" title="Niederländisch">
Niederländisch

Niederländisch bzw. niederländisch steht für:
Siehe auch:


</doc>
<doc id="8998" url="https://de.wikipedia.org/wiki?curid=8998" title="Möbiusband">
Möbiusband

Ein Möbiusband, Möbiusschleife oder Möbius’sches Band ist eine Fläche, die nur eine Kante und eine Seite hat. Sie ist nicht orientierbar, das heißt, man kann nicht zwischen unten und oben oder zwischen innen und außen unterscheiden.

Es wurde im Jahr 1858 unabhängig voneinander von dem Göttinger Mathematiker und Physiker Johann Benedict Listing und dem Leipziger Mathematiker und Astronomen August Ferdinand Möbius beschrieben.

Ein Möbiusband ist leicht herzustellen, indem man einen längeren Streifen Papier mit beiden Enden ringförmig zusammenklebt, ein Ende aber vor dem Zusammenkleben um 180° verdreht.
Solche Möbiusbänder besitzen eine Mittellinie, die keinen Kreis einnehmen kann – es sei denn, das Band wird örtlich gedehnt. Die Form, die ein solches Band ungedehnt einnehmen kann, wird vollständig durch den Verlauf der Mittellinie beschrieben.

Möbiusbänder, deren Mittellinie auch im entspannten Zustand ein Kreis ist, können nicht aus einem geraden zweidimensionalen Papierstreifen gefertigt werden – sie besitzen entlang ihres Umfanges ungleich geformte Teilelemente, aus denen sie zusammengesetzt gedacht werden können.

Möbiusbänder sind chiral.

Das Möbiusband geht derart in sich selbst über, dass man, wenn man auf einer der scheinbar zwei Seiten beginnt, die Fläche einzufärben, zum Schluss das ganze Objekt gefärbt hat.

Andere interessante Effekte entstehen, wenn man auf dem Band eine Mittellinie oder zwei zur Mittellinie parallele Linien einzeichnet und das Band längs dieser Linie(n) aufschneidet, also es scheinbar halbiert oder drittelt. Im ersten Fall, also beim Durchschneiden entlang der Mittellinie, entsteht ein zweifach verdrillter (um 720° in sich verdrehter) Ring mit zwei Seiten und zwei Rändern. Im zweiten Fall entstehen zwei Objekte: Ein Möbiusband und ein zweifach verdrillter Ring, die ineinander hängen. Dieses Spiel kann man mit beliebig kleiner Einteilung fortsetzen: „viertelt“ man das Band, entstehen zwei doppelt verdrillte Bänder, die nicht nur ineinander hängen, sondern auch noch einmal häufiger umeinander geschlungen sind; „fünftelt“ man es, entsteht dieselbe Figur mit einem zusätzlichen Möbiusband, das in den beiden Ringen hängt; „sechstelt“ man das Band, erhält man zwei Ringe, die sich doppelt umschlingen und von einem weiteren Ring doppelt umschlungen werden, wobei der äußere und die beiden inneren Ringe beliebig untereinander austauschbar sind; „siebtelt“ man es wiederum, kommt wieder ein Möbiusband hinzu, das in den drei Ringen hängt usw. Ist "n" der Nenner des Bruchteils, in den man das Band scheinbar einteilt, und "n" gerade, also "n" = 2"r", so erhält man "r" Ringe; ist "n" ungerade, "n" = 2"r"+1, so ist zusätzlich ein Möbiusband durch die Ringe geschlungen.

Mathematisch gesehen ist das Möbiusband eine nicht-orientierbare Mannigfaltigkeit. Eine weitere Fläche, die in diese Kategorie gehört, ist die Kleinsche Flasche; man kann eine Kleinsche Flasche so in zwei Teile zerlegen, dass aus ihr zwei Möbiusbänder entstehen.

Das mathematische Symbol für die Unendlichkeit wird manchmal fälschlicherweise als Möbiusband interpretiert.


Berühmte Darstellungen des Möbiusbandes in der Kunst gibt es z. B. von M. C. Escher (Möbiusband I und II, 1963) sowie in neuerer Zeit von Gideon Möbius-Sherman. Auch der argentinische Spielfilm "Moebius" setzt sich mit dem Thema auseinander. In der Literatur wird das Möbiusband ebenfalls thematisiert: Die Struktur von John Barths Kurzgeschichtenserie „Lost in the Funhouse“ (dt. „Ambrose im Juxhaus“) basiert auf dem Unendlichkeits- oder Wiederholungsprinzip (z. B. fehlende Mitte) des Möbiusbandes. Auch wird dem Buch ein Möbiusband mitgeliefert, das postmoderne Literaturansätze („Frame-Tale“) spiegelt. Es ist beschriftet mit: „Once upon a time there was a story that began once upon a time …“. Diese Form der Selbstreferenz ist typisch für sogenannte Seltsame Schleifen. Der Lyriker Erich Fried bezieht sich in seinem Gedicht „Topologik“ auf das Möbiusband: „Ich habe mir ein Möbiusherz gefasst, das sich in ausweglose Streifen schneidet.“ Max Bill schuf ab den 1930er Jahren zahlreiche Plastiken, die den visuellen Repräsentationen des Möbiusbandes entsprechen: z. B. 'Unendliche Schleife' (1935/37), 'Kontinuität' (Zürichsee; 1947, zerstört 1948) oder 'Unendliche Schleife' (Stadtgarten Essen, an der Hohenzollernstraße; 1974). Seine Skulptur Kontinuität (1986) stellt jedoch kein Möbiusband dar, entgegen gängiger Auffassung.

Auch in der seit 1986 existierenden Romanreihe "Necroscope" des englischen Autors Brian Lumley spielt das Möbiusband eine wichtige Rolle. Es ist das Symbol einiger Figuren, vor allem aber bedeutend für die Hauptperson Harry Keogh. Er erlernt die Fähigkeit des Zeitreisens mit Hilfe des sogenannten Möbiuskontinuums, das sich ähnlich dem Möbiusband verhält.

Ebenso wird das Möbiusband in der Perry-Rhodan-Serie thematisiert und bildet hier die dreidimensionale Modellbeschreibung für die beiden Seiten des n-dimensionalen Universums (Arresum und Paresum).

Lars Gustafsson entwickelt das Möbiusband in seinem Roman "Frau Sorgedahls schöne weiße Arme" weiter zu einer Möbius-Zeitflasche, in der wir gefangen sind. Außerhalb unseres Lebens gibt es nichts.

In der Manga-Reihe "Angel Sanctuary" wird das Schicksal des hohen Engels Alexiel und der steten Wiedergeburt seiner Seele in menschliche Körper, denen ein grausames und blutiges Schicksal vorherbestimmt ist, mit einer Möbius-Scheife verglichen.

Im 2011 in deutscher Sprache erschienenen Roman "Karte und Gebiet" von Michel Houellebecq ist ein Möbiusband auf der Grabplatte der Romanfigur Michel Houellebecq eingemeißelt.

Im Jahr 2011 hat der Student der Robotik Aaron Hoover an der Berkeley Universität ein Möbius-Getriebe als technische Spielerei mittels 3D-Druck hergestellt.

Das Möbiusschach ist eine Variante des Zylinderschach, bei der man sich beim „Anschluss“ der Längsseiten noch eine Verdrillung des Spielfeldes hinzudenkt.

In der Mode wurden auch schon Möbius-Schals entworfen.

Im Schauspiel "Solaris" nach Stanislaw Lem von Bettina Bruinier und Katja Friedrich am Münchner Volkstheater (2011) ist ein von einem Modellauto befahrenes Möbiusband wichtiger Bestandteil der Inszenierung (Bühnenbild: Markus Karner).

Das Logo der Commerzbank und des deutschen Gebäudereiniger-Handwerks zeigt ein Möbiusband.

Die DDR-Avantgarde-Band AG. Geige widmete dem Möbiusband ein Lied auf dem 1989 erschienenen Album "Trickbeat".







Das Möbiusband kann als Fläche mittels der folgenden Parameterdarstellung gezeichnet werden:

wobei formula_4 und formula_5. Damit wird ein Möbiusband mit einer Breite von 1 erstellt, dessen Mittellinie der in der xy-Ebene liegende Kreis mit Radius 1 und Zentrum formula_6 ist. Der Winkel formula_7 hat seinen Scheitel im Zentrum; während er sich ändert, führt die Variation von formula_8 zur Fläche, die sich zwischen der einzigen Kante spannt. Wie im Bild rechts leicht zu erkennen ist, handelt es sich nicht um ein aus einem Papierstreifen zu fertigendes Möbiusband – im waagerechten Teil ähneln die Teilelemente symmetrischen Trapezen.

Mit Hilfe von Zylinderkoordinaten formula_9 wird durch die folgende Gleichung eine unbeschränkte Version des Möbiusbandes definiert:

Die Topologie bietet einen mathematischen Weg, ein Möbiusband durch das gegensinnige Zusammenkleben der Enden eines Papierstreifens herzustellen. Dort wird ein Möbiusband als Quotientenraum des Quadrats formula_11 definiert, wobei zwei gegenüberliegende Seiten durch die Äquivalenzrelation formula_12 für formula_13 miteinander identifiziert werden. Das nebenstehende Diagramm verdeutlicht dies.

Man kann den Rand des Möbiusbandes auch als Spinor auffassen: Die Gruppe formula_14 sei durch formula_15 parametrisiert. Den Spinor formula_16 kann man als Teilmenge
auffassen; dies ist genau der Rand des Möbiusbandes

Neue Erkenntnisse zur mathematischen Beschreibung eines Möbiusbands wurden im Jahr 2007 durch die Wissenschaftler "E.L. Starostin" und "G.H.M. van der Heijden" publiziert. Sie haben insbesondere die Form mathematisch berechnet, die ein aus einem Band gefertigtes Möbiusband von selbst einzunehmen bestrebt ist, um so den energieärmsten Zustand anzunehmen.




</doc>
<doc id="9000" url="https://de.wikipedia.org/wiki?curid=9000" title="Wahrscheinlichkeit">
Wahrscheinlichkeit

Die Wahrscheinlichkeit "(Probabilität)" ist eine Einstufung von Aussagen und Urteilen nach dem Grad der Gewissheit (Sicherheit). Besondere Bedeutung hat dabei die Gewissheit von Vorhersagen. In der Mathematik hat sich mit der Wahrscheinlichkeitstheorie ein eigenes Fachgebiet entwickelt, das Wahrscheinlichkeiten als mathematische Objekte beschreibt, deren formale Eigenschaften im Alltag und der Philosophie auch auf Aussagen und Urteile übertragen werden.

Man unterscheidet unterschiedliche Auffassungen von Wahrscheinlichkeiten (Wahrscheinlichkeitsbegriffe).

Die Wahrscheinlichkeit eines Ereignisses ist das Verhältnis der Zahl der günstigen Ergebnisse zur Gesamtanzahl der Ergebnisse. So ist zum Beispiel die Wahrscheinlichkeit, mit einem Sechserwürfel eine ungerade Zahl zu werfen, 0,5. Dies entspricht einer relativen Häufigkeit von 50 %, denn es gibt sechs mögliche Ergebnisse, von denen drei die genannte Eigenschaft besitzen.

Dies ist die sogenannte "klassische" Definition, wie sie von Christiaan Huygens und Jakob I. Bernoulli entwickelt und von Laplace formuliert wurde. Sie ist die Grundlage der klassischen Wahrscheinlichkeitstheorie. Die Elementarereignisse besitzen gleiche Eintrittswahrscheinlichkeiten. Voraussetzung ist eine endliche Ergebnismenge und Kenntnis der A-priori-Wahrscheinlichkeiten.

Beispiel: Bei einem „fairen“ Würfel (das heißt kein Ergebnis wird durch unsymmetrische Massenverteilung oder Ähnliches bevorzugt) überlegt man sich, dass jede Zahl die gleiche Chance hat und daher in 1/6 aller Versuche erscheinen wird. Die Wahrscheinlichkeit des Ereignisses „gerade Zahl“ berechnet man wie folgt: Es gibt drei günstige Ergebnisse (2, 4, 6), aber sechs mögliche Ergebnisse, daher erhält man 3/6 = 0,5 als Resultat.

Ein Zufallsexperiment wird so oft wie möglich wiederholt, dann werden die relativen Häufigkeiten der jeweiligen Elementarereignisse berechnet. Die Wahrscheinlichkeit eines Ereignisses ist nun der Grenzwert seiner relativen Häufigkeit bei (theoretisch) unendlich vielen Wiederholungen. Dies ist die sogenannte „Limes-Definition“ nach von Mises. Das Gesetz der großen Zahlen spielt hier eine zentrale Rolle. Voraussetzung ist die beliebige Wiederholbarkeit des Experiments; die einzelnen Durchgänge müssen voneinander unabhängig sein. Ein anderer Name für dieses Konzept ist Frequentistischer Wahrscheinlichkeitsbegriff. Dieser Wahrscheinlichkeitsbegriff ist zum Beispiel in der Physik bei der Zerfallswahrscheinlichkeit eines Radionuklids gemeint; die Experimente sind hier die einzelnen, voneinander unabhängigen Zerfälle der Atomkerne.

Beispiel: Man würfelt 1000-mal und erhält folgende Verteilung: Die 1 fällt 100-mal (das entspricht einer relativen Häufigkeit von 10 %), die 2 fällt 150-mal (15 %), die 3 ebenfalls 150-mal (15 %), die 4 in 20 %, die 5 in 30 % und die 6 in 10 % der Fälle. Der Verdacht kommt auf, dass der Würfel nicht fair ist. Nach 10.000 Durchgängen haben sich die Zahlen bei den angegebenen Werten stabilisiert, sodass man mit einiger Sicherheit sagen kann, dass zum Beispiel die Wahrscheinlichkeit, eine 3 zu würfeln, bei 15 % liegt.

Die Propensitätstheorie interpretiert Wahrscheinlichkeit als Maß für die Neigung eines Prozesses zu einem bestimmten Ergebnis.

In der nichtrelativistischen Quantenmechanik wird die Wellenfunktion eines Teilchens als seine fundamentale Beschreibung verwendet. Das Integral des Betragsquadrates der Wellenfunktion über ein Raumgebiet entspricht dort der Wahrscheinlichkeit, das Teilchen darin anzutreffen. Es handelt sich also nicht um eine bloß statistische, sondern um eine nicht-determinierte Wahrscheinlichkeit.

Bei einmaligen Zufallsereignissen kann man deren Eintretenswahrscheinlichkeit nur schätzen, nicht berechnen.
Zentrale Gesichtspunkte sind hier Experten­wissen, Erfahrung und Intuition. Daher spricht man von einer subjektivistischen Wahrscheinlichkeitsauffassung, siehe auch Bayesscher Wahrscheinlichkeitsbegriff.

Beispiel: Nachdem jemand verschiedene Autos besessen hat, schätzt er die Wahrscheinlichkeit als hoch ein (zum Beispiel „Ich bin mir zu 80 % sicher“), mit der Marke "XY" auch beim nächsten Autokauf wieder zufrieden zu sein. Dieser Vorhersagewert kann zum Beispiel durch einen Testbericht nach oben oder unten verändert werden.

Diese intuitive Wahrscheinlichkeitserfassung birgt jedoch eine Vielzahl von "Stolpersteinen", die z.B. in der subjektiven Wahrnehmung (Risiken, wie etwa wegen erhöhter Geschwindigkeit zu verunfallen, werden tendenziell niedriger und Chancen, wie etwa auf einen Lottogewinn, höher als die tatsächliche Wahrscheinlichkeit des Eintritts eingeschätzt) oder der asymmetrischen Informationen liegen (Die subjektive Beurteilung von Unfallgefahren entspricht eher der Erwähnungshäufigkeit in den Medien als der tatsächlichen Unfallstatistik, klassische Beispiele dafür sind zu hoch eingeschätzte Eintrittswahrscheinlichkeiten eines Haiangriffes oder eines Flugunfalls).

Die axiomatische Definition der Wahrscheinlichkeit nach Kolmogorow ist die heute für die Mathematik maßgebende Definition, siehe Axiome von Kolmogorow.

Stochastik als ein Teilgebiet der Mathematik ist die Lehre der Häufigkeit und Wahrscheinlichkeit. Sie ist ein verhältnismäßig junger Teilbereich der Mathematik, zu dem im weiteren Sinne auch die Kombinatorik, die Wahrscheinlichkeitstheorie und die mathematische Statistik gehören.

Häufig wird der mathematische "Begriff von Wahrscheinlichkeit" benutzt: Die Wahrscheinlichkeitsrechnung oder Wahrscheinlichkeitstheorie (Teilgebiet der Stochastik) kümmert sich um die mathematische Systematisierung von Wahrscheinlichkeiten. Hier werden Wahrscheinlichkeitsverteilung, Wahrscheinlichkeitsfunktion, bedingte Wahrscheinlichkeit und viele andere Begriffe unterschieden.

Wahrscheinlichkeiten sind Zahlen zwischen 0 und 1, wobei null und eins zulässige Werte sind. Einem unmöglichen Ereignis wird die Wahrscheinlichkeit 0 zugewiesen, einem sicheren Ereignis die Wahrscheinlichkeit 1. Die Umkehrung davon gilt jedoch nur, wenn die Anzahl aller Ereignisse höchstens abzählbar unendlich ist. In „überabzählbar unendlichen“ Wahrscheinlichkeitsräumen kann ein Ereignis mit Wahrscheinlichkeit 0 eintreten, es heißt dann "fast unmöglich," ein Ereignis mit Wahrscheinlichkeit 1 muss nicht eintreten, es heißt dann "fast sicher."

Es wird oft behauptet, der Mensch besitze ein schlechtes Gefühl für die Wahrscheinlichkeit, man spricht in diesem Zusammenhang auch vom „Wahrscheinlichkeitsidioten“ (siehe auch Zahlenanalphabetismus). Dazu folgende Beispiele:


Während über den mathematischen Umgang mit Wahrscheinlichkeiten weitgehend Einigkeit herrscht (siehe Wahrscheinlichkeitstheorie), besteht Uneinigkeit darüber, worauf die Rechenregeln der mathematischen Theorie angewendet werden dürfen. Dies führt zur Frage nach der Interpretation des Begriffs „Wahrscheinlichkeit“.

Häufig wird „Wahrscheinlichkeit“ in zwei verschiedenen Zusammenhängen gebraucht:
Aleatorische und epistemische Wahrscheinlichkeit sind lose mit dem frequentistischen und dem bayesschen Wahrscheinlichkeitsbegriff assoziiert.

Es ist eine offene Frage, ob sich aleatorische Wahrscheinlichkeit auf epistemische Wahrscheinlichkeit reduzieren lässt (oder umgekehrt): Erscheint uns die Welt zufällig, weil wir nicht genug über sie wissen, oder gibt es fundamental zufällige Prozesse, wie etwa die objektive Deutung der Quantenmechanik annimmt? Obwohl für beide Standpunkte dieselben mathematischen Regeln zum Umgang mit Wahrscheinlichkeiten gelten, hat die jeweilige Sichtweise wichtige Konsequenzen dafür, welche mathematischen Modelle als gültig angesehen werden.





</doc>
<doc id="9002" url="https://de.wikipedia.org/wiki?curid=9002" title="Gesundheits- und Krankenpflege">
Gesundheits- und Krankenpflege

Die Gesundheits- und Krankenpflege als Beruf umfasst die Versorgung und Betreuung von Menschen aller Altersgruppen. Insbesondere kranke, behinderte und sterbende Menschen werden in der Pflege versorgt und betreut, aber auch die Verhütung von Krankheiten und die Versorgung von gesunden Menschen gehören zu den Aufgaben der Pflege.

Krankenpflege hat zum Ziel als mitmenschliches Handeln sowohl eine selbstbestimmte Lebensgestaltung des Kranken wiederherzustellen als auch innerhalb einer Gesellschaft die Gesundheit aller Mitglieder zu erhalten.

Während früher vor allem von Krankenpflege die Rede war, wird heute auch der Begriff "Gesundheits- und Krankenpflege" verwendet. Dieser neuen Wertepriorität entspricht die Berufsbezeichnung Gesundheits- und Krankenpfleger, sowie Gesundheits- und Krankenpflegehelfer in Deutschland und Österreich.

Der International Council of Nurses (ICN) definiert Pflege wie folgt (hier in deutscher Übersetzung):

Diese Definition ist keine international gültige Richtlinie, sondern nur eine allgemeine Vorgabe. Die einzelnen Länder und deren Gesetzgeber sind für die genaue Berufsbeschreibung und das Tätigkeitsfeld zuständig.

Die Pflegegeschichte ist untrennbar mit Entwicklungen in anderen Fachbereichen wie der Medizin, der Sozialwissenschaft und der Theologie verbunden und ist über weite Strecken ihrer Entwicklung Teil deren Geschichte, insbesondere der Geschichte der Medizin.

Die Rolle, die Funktion, das Selbstverständnis und das Bild der Pflege haben sich im Wandel der Zeit stark verändert. Die zunächst als Überlebensstrategie zur Existenzsicherung innerhalb des Familien- oder Stammesverbandes geleistete zwischenmenschliche Hilfe wandelte sich in eine strukturierte und teilweise organisierte gesellschaftliche Aufgabe ärztlicher Assistenz in den archaischen Hochkulturen und der Antike, während die grundständige Pflege überwiegend von der Familie erbracht wurde.

Mit dem frühen Christentum setzt eine Neuorientierung ein, der die Pflege bis in die Neuzeit prägt: Das Ideal der tätigen Nächstenliebe, der Karitas. Diese Grundlage pflegerischen Handelns entwickelt sich im antiken Rom und verbreitet sich mit dem Christentum über ganz Europa. Im Mittelalter bildet die Karitas die Grundlage für zahlreiche Ordensgründungen, unter anderen auch die Beginen und die Orden der Barmherzigen Brüder, die sich der Kranken- und Armenpflege verschreiben.

Mit Beginn des 18. Jahrhunderts werden Forderungen nach einer Professionalisierung der Krankenpflege laut, erste von Ärzten initiierte Krankenpflegeschulen entstehen. Im 19. Jahrhundert bemüht sich Theodor Fliedner mit der Einrichtung der Diakonissenanstalt Kaiserswerth um eine Verbesserung der Ausbildung religiös motivierter Pflegekräfte. Florence Nightingale veröffentlicht die erste pflegetheoretische Schrift "Notes on Nursing" und reformiert mit dem Nightingaleschen System die konfessionell unabhängige Pflegeausbildung.

Zu Beginn des 20. Jahrhunderts entstehen eine Reihe von Pflegeverbänden und Berufsorganisationen, darunter das International Council of Nurses und der Agnes-Karll-Verband. Erste pflegewissenschaftliche Studiengänge werden 1910 in den Vereinigten Staaten eingeführt, die Pflegeforschung beginnt sich kurz darauf zu etablieren. Die beiden Weltkriege unterbrechen die Entwicklung der Pflege in Deutschland und Österreich, die Krankenpflege im Nationalsozialismus stellt dabei eines der dunkelsten Kapitel der Pflegegeschichte dar.

Nach 1950 findet in den angloamerikanischen Ländern eine rasante pflegetheoretische Weiterentwicklung statt, die wegweisenden konzeptionellen Pflegemodelle entstehen und die Pflege emanzipiert sich als eigenständiger akademischer Beruf. Zur gleichen Zeit finden in Deutschland und Österreich Bemühungen statt, die Pflegeausbildung dem internationalen Stand anzugleichen, die dreijährige Berufsausbildung wird eingeführt und es entwickeln sich eine Reihe von Fachweiterbildungen.

„Seitdem in den 1990er Jahren in Deutschland Pflegestudiengänge eingeführt wurden, professionalisiert sich die Pflege; man bemüht sich darum, ein gemeinsames Pflegeverständnis und ein gemeinsames Profil professioneller Pflege zu entwickeln und zu etablieren. Dass eigenständige Wissensbestände durch Pflegeforschung und Expertenstandards entwickelt werden, ist ein Zeichen der Emanzipation, aber auch der eigenständigen Profilierung der Pflege.“

Im Düsseldorfer Stadtteil Kaiserswerth mit seiner traditionsreichen Diakonissenanstalt gibt es in 15 Räumen des ehemaligen Schwesternkrankenhauses Tabea ein Pflegemuseum mit einer umfangreichen Sammlung zur Geschichte der Diakonie und der Krankenpflege. Auch die Krankenhausmuseen in Bielefeld, Bremen, München und Nürnberg beachten dieses Thema.

Nachdem die Pflege im 19. und 20. Jahrhundert zunehmend professionalisiert und akademisch verankert worden war, entstand insbesondere gegen Ende des 20. Jahrhunderts das Bedürfnis, die Entwicklung des Berufsbildes und die Veränderung der Rolle von Pflegekräften in der Gesellschaft zu verstehen. Pflegeforschung wird überwiegend von Laien, interessierten Pflegekräften und Wissenschaftlern betrieben, die nicht über eine geschichtswissenschaftliche Ausbildung verfügen. Weltweit führend ist die angloamerikanische Pflegeforschung, die mit anderen Fachbereichen zusammenarbeitet. In Europa haben vergleichbare Forschungsprojekte begonnen. Medizingeschichte und Pflegegeschichte sind heute Teil der Ausbildung von Pflegefachkräften aller Bereiche und werden im Rahmen der Berufskunde unterrichtet.

Die berufliche Krankenpflege wird in Deutschland nach folgenden Qualifikationsebenen unterschieden:


Die Ausbildung in der Krankenpflege unterteilt sich zum einen in theoretischen und praktischen Unterricht und zum anderen in die praktische Ausbildung. In Deutschland findet der Unterricht in der Regel an Krankenpflegeschulen oder Berufsfachschulen statt, während die praktische Ausbildung in Teilen in Krankenhäusern, Pflegeheimen oder ambulanten Pflegediensten absolviert wird. Dort hat nach der jeweiligen Ausbildungsordnung eine Praxisanleitung stattzufinden. Diese Ausbildungsstruktur ist der in der dualen Ausbildung von Handwerkern recht ähnlich, stellt jedoch einen eigenen Zweig innerhalb der Berufsbildung dar.

Die berufliche Grundausbildung ist in ihrer Funktion der Lehre oder dem Studium in anderen Berufen vergleichbar, Fort- und Weiterbildung bauen darauf auf. Umgangssprachlich werden diese Begriffe häufig als gleichbedeutend verwendet.

Der Männeranteil beträgt 14 %.

In Österreich ist im Gegensatz zu anderen europäischen Ländern die Reifeprüfung (Matura(A)/Abitur(D)) nicht Voraussetzung für eine Ausbildung im Krankenpflegeberuf. Die Ausbildung wurde 1997 einheitlich geregelt. Sie dauert für den Gehobenen Dienst für Gesundheits- und Krankenpflege drei Jahre und für Pflegehilfen ein Jahr.

Zu beiden Ausbildungsvarianten gehört eine praktische und theoretische Ausbildung, die einander im Laufe der Ausbildung immer wieder abwechseln. Die Abwechslung der Theorie- und Praxisblöcke bringt den Vorteil, schneller theoretisches Wissen in der Praxis umsetzen sowie praktische Erfahrungen in den Unterricht einbringen zu können.

Insbesondere für diplomierte Pflegekräfte wird in der Ausbildung darauf geachtet, ein breites Wissen aus medizinischen Grund- und Fachkenntnissen und dem Pflegewissen miteinander zu verbinden. Auf vernetztes Denken wird in der Ausbildung und der Berufsausübung besonders Wert gelegt. Die Ausbildung in der Krankenpflege wird mit einer Diplomprüfung abgeschlossen.

Innerhalb des Gehobenen Dienstes für Gesundheits- und Krankenpflege können folgende Ausbildungen erworben werden:


In die Schule für das psychiatrische Gesundheits- und Krankenpflegediplom werden nur Schüler aufgenommen, die das 18. Lebensjahr erreicht haben.

Nach abgeschlossener Ausbildung als diplomiertes Krankenpflegepersonal können noch weitere Spezial- und Sonderausbildungen wie z. B. Intensiv- oder OP-Pflege, Pflege bei Nierenersatztherapie, Führungs- und Lehraufgaben absolviert werden. In Österreich ist jede Pflegekraft dazu verpflichtet, ihre Kenntnisse auf dem aktuellen Stand der Pflege zu halten und sich mit den aktuellen Erkenntnissen vertraut zu machen. Um dies in der Praxis kontrollieren zu können, muss jede Pflegekraft einen Aus- und Fortbildungsnachweis führen und im Laufe von 5 Jahren mindestens 40 Ausbildungsstunden nachweisen. Kommt eine Pflegekraft dieser Pflicht nicht nach, so kann ihr vorübergehend die Führung der Berufsbezeichnung untersagt oder diese entzogen werden.

Für weibliche Pflegekräfte gilt der Beruf als Schwerarbeit im Sinne der Schwerarbeitsverordnung und der dazu ergangenen Berufsliste. Der gesamte Männeranteil beträgt 13 %, in der psychiatrischen Pflege 44 %.

In allen skandinavischen Ländern ist Pfleger ein akademischer Beruf – dies gleichermaßen für die Alten- und die Krankenpflege. In Schweden promovierte 1980 die erste Krankenschwester, und 1982 wurde die erste Krankenschwester zur Professorin berufen. Auch in vielen anderen Staaten – nicht aber in Deutschland und Österreich – sind eine zwölfjährige Schulzeit sowie eine akademische Ausbildung Voraussetzung für die Aufnahme des Pflegeberufs.

Während die Pflege im angelsächsischen Raum seit etwa 1920 neben der beruflichen auch eine akademische Tradition aufweisen kann, konnte sich die Pflegewissenschaft in den deutschsprachigen Ländern erst in den letzten Jahrzehnten des 20. Jahrhunderts etablieren. Inzwischen wird auch dort eine Reihe von Studiengängen (z. B. in Pflegepädagogik, Pflegemanagement oder Pflegewissenschaft) zumeist an Fachhochschulen angeboten.

Siehe auch: Virginia Henderson, Liliane Juchli, Monika Krohwinkel, Nancy Roper, Cicely Saunders

In Österreich berechtigt das Diplom zur Ablegung einer Berufsreifeprüfung und damit zum Zugang zu einem Universitätsstudium.
Die Studiendauer beträgt acht Semester, und man schließt mit dem akademischen Grad Magistra/Magister der Philosophie ab.
In den Bakkalaureats- und Masterstudiengängen Gesundheits- und Pflegewissenschaft an der Medizinischen Universität Graz können nach sechs bzw. vier Semestern die Akademischen Grade Bachelor of Science (BSc) bzw. Master of Science (MSc) erworben werden.

In der Europäischen Union haben viele Länder in den 90er Jahren umfassende Ausbildungsreformen verwirklicht, die über EU-Standards hinausgehen. Der Bolognaprozess hat diese Entwicklungen verstärkt. Zugangsvoraussetzung für die Ausbildung ist der Sekundarstufe II-Abschluss. Ausnahmen sind: Deutschland, Luxemburg und Österreich. Die Pflegeausbildungen finden an Fachhochschulen und Universitäten statt. Ausnahmen sind: Deutschland, Frankreich, Luxemburg und Österreich. Die Mehrheit der Qualifikationsabschlüsse der akademischen Pflegeausbildungen in den EU-Ländern führen zum Bachelor und die in den jeweiligen Ländern erworbenen Abschlüsse werden weitgehend wechselseitig anerkannt. Eine Anerkennung europäischer Abschlüsse im amerikanischen Berufssystem ("Nursing board certification") ist bislang nicht realisiert.

Weltweit findet die Pflegeausbildung an Hochschulen u. a. in folgenden Ländern statt: Norwegen, Island, Kanada, den USA, Mexiko, Brasilien, Australien, Neuseeland, den Philippinen, Südkorea und Japan.

Zur Berufsausbildung in verschiedenen Ländern siehe auch: Altenpfleger (D), Hebamme/Entbindungspfleger, Gesundheits- und Krankenpfleger (D), Diplomierte/-r Gesundheits- und Krankenschwester/-pfleger (A), Krankenpflegehelfer (D), Krankenschwester/Krankenpfleger (historisch), Pflegefachfrau (CH).

Weiterbildung in der Pflege hat das Ziel einer Zusatz-Qualifizierung, um sich beruflich weiterzuentwickeln oder um aufzusteigen. Die Weiterbildung dauert in der Regel zwei Jahre, endet im Gegensatz zu einer Fortbildung immer mit einer Prüfung (mündlich/schriftlich/praktisch) und führt zu einer neuen Berufsbezeichnung bzw. erweiterten Berufsbezeichnung.

Nach den Grundausbildungen in Krankenpflege (GuK) oder Kinderkrankenpflege (KGuK) etc. existieren als weiterführende Ausbildungsmöglichkeiten die sogenannten Fachweiterbildungen, z. B. „Geprüfte Fachkraft für Leitungsaufgaben in der Pflege“. Inzwischen gibt es für nahezu jede spezialisierte Richtung im Krankenhaus Fachweiterbildungen. Aber auch im ambulanten Bereich schreitet die Differenzierung voran. Die bekanntesten sind die für die Bereiche Kinderkrankenpflege, Onkologie, Chirurgie, Schmerztherapie, Anästhesie, Intensivpflege, Nephrologie und Psychiatrie. Exemplarisch wird die Fachweiterbildung für Psychiatrie dargestellt:

Zugangsvoraussetzungen sind die erfolgreich abgeschlossene Ausbildung zum/r Gesundheits- und Krankenpfleger/-in sowie der Nachweis über zwei Jahre Berufserfahrung in einer psychiatrischen Einrichtung. Die Weiterbildung selbst dauert zwei Jahre und findet berufsbegleitend statt. In diesen zwei Jahren muss man vier Bereiche in der Psychiatrie (z. B. Aufnahmestation, Institutsambulanz, Gerontopsychiatrie und stationäre Suchtbehandlung) absolviert haben. Die Bereiche können mit der jeweiligen Weiterbildungsstätte ausgehandelt werden. Für jeden Bereich bekommt man eine spezielle Aufgabe (Praxisberichte) von Seiten der Weiterbildungsstätte zur Bearbeitung gestellt (examensrelevant). Die Weiterbildung schließt mit mündlichen, schriftlichen und praktischen Prüfungen ab. Anschließend darf man je nach landesrechtlicher Ordnung den Titel „Fachkrankenschwester/-pfleger für Psychiatrie“ führen.

Neben diesen fachbereichsbezogenen Weiterbildungen gibt es noch weitere Möglichkeiten. Wer außerhalb der praktischen Pflege im Krankenhaus mit seiner Ausbildung weiterarbeiten möchte, kann sich z. B. zur Hygienefachkraft oder zum Case Manager fortbilden.

Weiterqualifizierungsmöglichkeit sind auch die akademischen Studiengänge der Pflegewissenschaft, Pflegepädagogik und des Pflegemanagers/Pflegewirtes. Tätigkeiten in Pflegeforschungseinrichtungen, Pflegedienstleitungen oder Ausbildungseinrichtungen (z. B. Krankenpflegeschulen) setzen immer häufiger ein abgeschlossenes Studium voraus.

Die hohen ethischen Anforderungen und die in manchen Bereichen von Mangel und Kosteneffizienz geprägte Berufswirklichkeit erzeugen für viele in diesem Beruf eine schwer auszuhaltende Spannung. Das Ideal des Helfers führt oft dazu, dass Pflegepersonal materielle und organisatorische Mängel durch erhöhtes Engagement auszugleichen versucht. Langfristig führt dies aber zusammen mit einer oft hohen körperlichen Belastung z. B. der Wirbelsäule nicht selten in eine Berufsunzufriedenheit und letztlich Berufsflucht. Es wird vermutet, dass nur in wenigen anderen Berufen die Verweildauer ausgebildeter Kräfte im Beruf so gering ist, wie in Pflegeberufen, besonders in der Altenpflege.
Weitere Stressfaktoren und Belastungen sind vor allem der Umgang mit Sterbenden, onkologischen und geriatrischen Patienten. Häufig ist die Personalsituation auf geriatrischen Stationen nicht optimal.
In Österreich wurden weibliche Pflegekräfte in das Schwerarbeitergesetz einbezogen.

Die Bezahlung in den Pflegeberufen wird im Verhältnis zur physischen und psychischen Beanspruchung oft als eher gering empfunden; durch verschiedene Zulagen wird zwar das Arbeitseinkommen aufgebessert, meist wirken sich solche Zulagen aber später beim Rentenanspruch nicht aus.

Seit Jahren gibt es eine öffentliche Diskussion, ob die tägliche Belastung der in den Pflegeberufen Tätigen dauerhaft das zumutbare Maß überschreite und unter anderem zu Burnout führe. Scheinbare oder wirkliche gravierende Pflegefehler sorgen gelegentlich als „Pflegeskandal“ für Schlagzeilen, in den meisten Fällen aber nicht für eine systematische Verbesserung der Arbeitsbedingungen für alle Mitarbeitenden.

In der Tat kann übermäßig hohe Beanspruchung des Pflegepersonals erfahrungsgemäß zu Pflegefehlern führen. Ebenso kann der Verzicht auf eine (noch oder ergänzend) mögliche natürliche Nahrungsaufnahme zugunsten einer automatischen maschinellen Nahrungszufuhr wegen mangelnder Pflegekapazität die Frage nach einer menschenwürdigen Behandlung der Patienten auslösen.

Obwohl der Pflegeberuf der zahlenmäßig größte Beruf in jedem Gesundheitssystem ist, hat er in keinem Land eine starke politische Stellung. Seine Einflussmöglichkeiten sind gering. Die Hauptgründe:

Seit dem Lainz-Skandal, der die Bevölkerung in Österreich aufgrund der menschenverachtenden Elemente dieses Falles bewegt hat, wurde die Pflege immer wieder zum Wahlkampfthema, so etwa nach einem Vorfall im Wiener Otto-Wagner-Spital, bei dem behauptet wurde, dass Pflegepersonal die Patienten „bestrafe“.

Weitere hitzig diskutierte Themen der letzten Jahre waren das Pflegegeld, die 24h-Pflege (die in Österreich aber nichts mit dem Berufsstand einer Diplomierten Gesundheits- und Krankenpflegeperson zu tun hat), sowie immer wieder Fälle von Patienten und Langzeitklienten, die tot oder unterkühlt aufgefunden wurden.
Anzumerken ist, dass in Österreich ein Patient nicht beliebig fixiert werden darf und dass es oft kein geschultes Sicherheitspersonal in Krankenhäusern gibt.

Grundsätzlich gelten für die Krankenpflege die Regeln der Medizinethik, insbesondere in medizinischen Einrichtungen. Daneben gibt es aber den Pflegebereich, wo die Pflegenden selbstständig handeln müssen und können.
Die Überzeugung, dass es die Aufgabe jedes Einzelnen ebenso wie die der Gesellschaft sei, kranken Menschen zu helfen, ist ein zentraler Teil der Berufsethik in der Krankenpflege. Allerdings verlangt Krankenpflege nicht nur Mitgefühl und Engagement. Neben der Stärke, tiefe Krisen von Patienten mitzutragen und womöglich als erniedrigend empfundene Situationen (sowohl für Patienten als auch für Pflegende) möglichst würdig zu gestalten, bedarf es umfangreichen Fachwissens, um den Anforderungen des Berufs gerecht zu werden.
Allerdings wird durch den massiven Kostendruck im Gesundheitswesen auch das Arbeitsfeld Krankenpflege erheblich von Zeit-, Personal- und Geldmangel geprägt .Der Wettbewerbsdruck im Pflegesektor verschärfte sich mit Einführung des Pflegeversicherungsgesetzes am 1. April 1995 deutlich. Ein Markt, der bisher durch freigemeinnützige Pflegedienste dominiert war, wurde nun durch private ambulante Pflegedienste deutlich ergänzt. Fast zeitgleich wurden die finanziellen Förderungen der Bundesländer stark eingeschränkt, was zur weiteren Erhöhung des Kostendrucks führte.

Vom Deutschen Pflegerat wurde im Jahr 2004 eine Rahmenberufsordnung veröffentlicht, in der die allgemeinen Grundsätze und Verhaltensregeln für professionell Pflegende in Deutschland festgeschrieben sind. Diese Rahmenberufsordnung löste die Berufsordnungen ab, die zuvor vom Deutschen Berufsverband für Pflegeberufe sowie von der Arbeitsgemeinschaft deutscher Schwesternverbände und Pflegeorganisationen herausgegeben wurden.

Die Arbeitsbedingungen für Arbeitnehmer, die in der Pflege arbeiten, können von Gewerkschaften und Arbeitgebern autonom in Tarifverträgen geregelt werden (z. B. Höhe der Arbeitsentgelte, Arbeitszeiten, Urlaub). Das gilt auch für kirchliche Arbeitgeber, die sich allerdings fast ausnahmslos weigern, mit den Gewerkschaften überhaupt über Tarifverträge zu verhandeln. Im Rahmen einer Tarifauseinandersetzung stellt sich für die Gewerkschaft und ihre Mitglieder die Frage, ob es opportun ist, auch Pflegekräfte zu einem Streik zur Durchsetzung der tarifpolitischen Ziele aufzurufen bzw. sich an einem solchen Streik zu beteiligen.

Nach deutschem Recht sind Streiks grundsätzlich auch in der Pflege zulässig. Es muss allerdings gewährleistet sein, dass Leib und Leben der Patienten durch die Arbeitsniederlegung nicht in Gefahr gerät. Das bedeutet, dass in der Regel ein Notdienst bereitgehalten werden muss und dass für die Gesundheit der Patienten unerlässliche Pflegehandlungen nicht unterlassen werden dürfen. Soweit das Pflegepersonal in einem Betrieb arbeitet, der dem öffentlichen Dienst angehört und der damit in den Geltungsbereich der dort geltenden Tarifverträge fällt, können Kollegen in anderen Bereichen außerhalb der Pflege, wie z. B. Busfahrer, Müllwerker oder Bademeister des öffentlichen Dienstes für sie „mitstreiken“. Die erkämpfte Lohnerhöhung gilt danach für alle Gewerkschaftsmitglieder.

In Finnland gingen Pflegekräfte im Tarifstreit um mehr Gehalt 2007 einen anderen Weg. Sie hatten kollektiv angekündigt, ihre Arbeitsverhältnisse zu einem bestimmten Stichtag zu kündigen, wenn ihre Forderungen bis dahin nicht erfüllt worden sind. Da Pflegekräfte auch in Finnland rar sind, erhoffte sich die Gewerkschaft, mit der Drohung der Massenkündigung so viel Druck auf die meist kommunalen Pflege-Arbeitgeber ausüben zu können, dass diese Zugeständnisse machen. Ihre Forderungen und auch ihre Drohung mit Massenkündigungen als Arbeitskampfmittel waren laut Umfragen in der finnischen Bevölkerung sehr populär, weil das Pflegepersonal wirklich als unterbezahlt gilt. Im November antwortete die Regierung darauf mit einem Notstandsgesetz und schließlich kam es zu einer Einigung.

Eine Umfrage in der Schweiz zum Vertrauen in verschiedene Berufe im Jahr 2003 führt in der Bewertungskategorien „Vertrauen sehr hoch“ bzw. „Vertrauen ziemlich hoch“ zu dem Ergebnis in "Tabelle 1". Die in der Allensbacher Berufsprestige-Skala 2005 ist in "Tabelle 2" dargestellt.

Die Umfrage von Reader’s Digest wurde 2006 wiederholt. Die Vertrauensrangliste (Durchschnitt Europa): 1. Feuerwehrleute 95 % sehr hohes oder ziemlich hohes Vertrauen. 2. Piloten 92 %. 3. Apotheker 89 %. 4. Krankenschwestern 86 %. 5. Ärzte 85 %.

Pflegefehler sind in der Pflege schwerwiegende Ereignisse, bei denen es meist zu einer Verschlechterung der medizinischen Gesamtsituation kommt. Pflegefehler haben in den letzten Jahren anteilmäßig stetig abgenommen, Grund hierfür war die Professionalisierung.

In rund 10 % der stationären Einrichtungen in Deutschland wurden bei Prüfungen der Dokumentationen schwerwiegende Pflegefehler mit möglicherweise gesundheitlichen Schäden bei den Heimbewohnern festgestellt (2003: 17 %). Bei 34 Prozent der Pflegebedürftigen in stationären Einrichtungen gab es Defizite in der Ernährungs- und Flüssigkeitsversorgung (zumindest im Nachweis in den Dokumentationen).
Rund 5,7 % der ambulanten Einrichtungen weisen solche Pflegefehler auf (2003: 8,8 %), bei rund 30 % gab es mögliche Defizite in der Ernährungs- und Flüssigkeitsversorgung.

Ein Problem, mit dem Pflegende zunehmend zu kämpfen haben, sind die Fehlinterpretationen durch Angehörige oder Dritte. Der Fehler liegt hier in alten Moral- und Wertevorstellungen, die auf Pflegepersonen noch teilweise angewandt werden, begründet. Besonders ältere Generationen sehen die Pflegeperson als eine dem Arzt unterstellte Dienende, die dem Patienten helfen soll.
Es kommt dadurch häufig zu Konflikten mit Angehörigen, deren Bild von Alten und Krankenpflege von dem veralteten Defizitmodell geprägt ist. Während für die pflegende Person der Erhalt oder das Erreichen der Selbstständigkeit im Vordergrund steht, meinen Angehörige oft, einen Pflegefehler zu erkennen. Dadurch entsteht der Eindruck, dass die Pflegepersonen entweder überfordert sind, oder sich nicht ausreichend um die Pflegebedürftigen kümmern.






</doc>
<doc id="9003" url="https://de.wikipedia.org/wiki?curid=9003" title="Plenk">
Plenk

Plenk (vom englischen "" für „ Leerzeichen“) bezeichnet im Netzjargon ein typografisch falsch gesetztes Leerzeichen "vor" einem Satzzeichen oder Wortzeichen (darunter fallen beispielsweise Komma, Punkt, Ausrufezeichen und Fragezeichen bzw. Bindestrich und Abkürzungspunkt); die Verwendung widerspricht der Rechtschreibung.

Den Regelungen zur deutschen Sprache (DIN 5008) folgend, werden weder vor das Frage- noch vor das Ausrufezeichen Leerzeichen gesetzt, anders als beispielsweise im Französischen (siehe weiter unten).

Leerzeichen vor Satzzeichen sind im Deutschen hingegen erforderlich bei einem Gedankenstrich, bei der sich öffnenden Klammer und bei Auslassungspunkten, sofern diese für mindestens ein ganzes Wort stehen (siehe auch Ellipse), sowie bei einem Ergänzungsstrich, wenn dieser einem Wortteil voransteht, um einen dem Inhalt nach zu wiederholenden Wortteil zu vertreten (Beispiel: Warenherstellung und ‑verkauf).

Plenks führen bei automatischem Zeilenumbruch durch Programme, die den Anhang UAX #14 der Unicode-Spezifikation nicht oder nicht richtig implementieren, oft zu unerwünschten Effekten, wie folgendes Beispiel zeigt:


Hier wird der Ausdruck sinnentstellend auseinandergerissen:


Im letzten Beispiel liegt zusätzlich die Ersetzung des Bindestrichs durch einen Gedankenstrich vor, siehe weiter unten.

Das Wort Plenk ist eine Verballhornung eines englischen Wortes für Leerzeichen, "". Es wurde um 1988 von Johannes „Jödel“ Leckebusch im MausNet eingeführt; seit Mitte der 1990er Jahre ist Plenk auch im deutschsprachigen Usenet ein feststehender Ausdruck.

Formal und linguistisch handelt es sich bei Plenk um eine Wortbildung auf dem Wege der Lehnwortbildung.

Aus Plenk selbst entstand wiederum der komplementäre Ausdruck Klemp.

Oft wird sowohl nach einer sich öffnenden als auch vor einer schließenden Klammer ein Leerzeichen gesetzt. Das ist falsch, das Leerzeichen gehört nur vor die öffnende sowie hinter die schließende Klammer (sofern dieser nicht ein Satzzeichen folgt).

Würde das Fehlen eines eigentlich nicht zulässigen Leerzeichens vor einer schließenden Klammer zu einer Sinnentstellung führen, darf es jedoch gesetzt werden:

Bei Textverarbeitungs­programmen wie LibreOffice und Microsoft Word wird in der Voreinstellung ein von zwei Leerzeichen umschlossener Bindestrich automatisch durch einen Halbgeviertstrich (Gedankenstrich) ersetzt. Hierdurch kann Plenken zu völlig sinnentstellenden Schreibweisen führen.

Im Gegensatz zum Deutschen und Englischen ist im Französischen – nicht aber in der französischen Schweiz – die Verwendung eines Leerzeichens vor Fragezeichen (?), Ausrufezeichen (!), Doppelpunkt (:) und Semikolon (;) im Schriftsatz vorgeschrieben. Außerdem steht hier nach öffnenden («) und vor schließenden (») Anführungszeichen "(guillemets)" ein Leerzeichen. Diese Regel gilt auch bei Kombinationen von Satzzeichen: "« Quoi ? »"

Um einen ungewollten Zeilenumbruch zu verhindern, wird dabei ein geschütztes Leerzeichen verwendet. Alternativ können Programme eingesetzt werden, die den Umbruch entsprechend dem Unicode-Zeilenumbruch-Algorithmus vornehmen, der in diesem Fall auch bei der Verwendung eines gewöhnlichen Leerzeichens einen Umbruch verhindert. Ist die Verwendung eines geschützten Leerzeichens nicht möglich, so gilt auch für das Französische, dass kein Leerzeichen zu verwenden ist. Sind unterschiedliche Spatien­breiten vorhanden, wird vor das Semikolon häufig nur ein halbes Leerzeichen gesetzt.

Bei typografisch korrektem Schriftsatz werden die oben genannten Zeichen mit Viertelgeviert ausgeschlossen, wobei der Schriftgrad noch nicht berücksichtigt ist.

Die Besonderheit hängt mit der französischen typografischen Tradition zusammen: So wurden in frühen Drucken sehr viele Kursiven verwendet; außerdem ist das Graphem "f" am Wortende häufiger als in anderen Sprachen. Dadurch entsteht immer ein Abstand (vom Fußpunkt des f) zum Punkt oder Komma, der dann auch bei den anderen Satzzeichen analog umgesetzt wird.

In der französischen Schweiz werden die typografischen Regeln der Schweiz verwendet, die – außer beim Doppelpunkt : – ohne Leerzeichen vor den oben genannten Satzzeichen auskommen (Siehe auch Anführungszeichen#Schweiz, Liechtenstein, Frankreich).

Beispiele:

Im Spanischen können Leerzeichen vor den gestürzten Frage- und Rufzeichen auftreten, die am Satzbeginn von Fragen und Ausrufen stehen: ¿Te gusta andar? ¡Vamos!

Komplementär zu Plenk wird – wiederum insbesondere im Netzjargon – das Fehlen eines Leerzeichens "nach" einem Satz- oder Wortzeichen als Klemp bezeichnet und ist typographisch in der Regel genauso falsch wie ein Plenk (Ausnahmen siehe unten).

Das Wort Klemp ist – ebenso wie Plenk – eine Lehnwortbildung (aus dem englischen Wort "clamp" für Schraubzwinge, das mit dem deutschen Wort "Klammer" verwandt ist); in dem Sinne, dass damit zwei Wörter zu nah aneinander geklemmt werden). Zum anderen fiel die Wahl wohl auch deshalb auf diese Wortbildung, weil es die konsonantische Umkehrung von Plenk ist. So spiegelt sich die Komplementarität der beiden Begriffe sowohl orthographisch als auch phonetisch wider (Vertauschung von p und k).

Das Auslassen eines solchen Leerzeichens wird in Ableitung als Klempen bezeichnet.

Das Weglassen eines typographisch korrekten Leerzeichens geschieht manchmal bewusst, insbesondere um beispielsweise bei Beschränkungen der Zeichenanzahl Schriftzeichen einzusparen. Beispiele:
In Fällen wie den genannten wird ein Klemp meist toleriert, auch wenn er nach wie vor als typographisch „unschön“ gilt.

In den chinesischen und japanischen Schriftsystemen werden vor und nach Satzzeichen regulär keine Leerzeichen gesetzt. Hintergrund ist, dass dort Satzzeichen im Allgemeinen denselben quadratischen Raum (Geviert) wie ein Schriftzeichen einnehmen und innerhalb dieses Raumes gegebenenfalls am linken oder rechten Rand stehen. Der Rest des Gevierts bleibt dann leer, was wie ein kleiner Zwischenraum aussieht. Satzzeichen wie Punkte (), Kommas () und schließende Klammern () stehen dabei links, öffnende Klammern () rechts innerhalb ihres Gevierts. Daher besteht bei Schreibern mit entsprechendem kulturellen Hintergrund teilweise eine Neigung zum Klempen auch im lateinischen Schriftsystem.




</doc>
<doc id="9004" url="https://de.wikipedia.org/wiki?curid=9004" title="Limburg an der Lahn">
Limburg an der Lahn

Limburg an der Lahn (amtlich: Limburg a. d. Lahn) ist die Kreisstadt des Landkreises Limburg-Weilburg im Land Hessen und mit rund 34.000 Einwohnern zugleich dessen einwohnerstärkste Stadt.

Die Stadt Limburg erfüllt nach hessischer Landesplanung die Funktion eines Mittelzentrums mit oberzentraler Teilfunktion und bildet zusammen mit der angrenzenden rheinland-pfälzischen Stadt Diez ein länderübergreifendes Doppelzentrum mit rund 45.000 Einwohnern. Limburg hat aufgrund seiner Lage eine Zentrumsfunktion für den dünn besiedelten westlichen Teil Hessens sowie für Teile des Westerwaldkreises und des Rhein-Lahn-Kreises im Land Rheinland-Pfalz.

Überregional bekannt ist die Stadt hauptsächlich durch das gleichnamige Bistum mit seiner Kathedralkirche, dem spätromanischen Dom "St. Georg", und den Bahnhof Limburg Süd an der Schnellfahrstrecke Köln–Rhein/Main.

Limburg liegt unmittelbar an der Westgrenze Hessens zwischen Taunus und Westerwald zu beiden Seiten des Flusses Lahn.

Die Stadt liegt relativ zentral in einem Becken innerhalb des Rheinischen Schiefergebirges, das von den Mittelgebirgshöhen des Taunus und Westerwald umgeben ist und Limburger Becken heißt. Dank seines fruchtbaren Bodens und seines günstigen Klimas bildet das Limburger Becken eine der ertragreichsten Agrarlandschaften Hessens und hat darüber hinaus als Lahnübergang seit dem Mittelalter eine hohe verkehrsgeographische Bedeutung. Innerhalb des Beckens weist das sonst recht enge Untertal der Lahn einige deutliche Weitungen auf, so dass die mittlere Höhenlage Limburgs lediglich 117 Meter beträgt.

Ein Teilstück des Stadtteils Staffel liegt als Exklave westlich außerhalb des Stadtgebiets. Zwischen der Exklave und der Hauptgemarkung Limburgs liegen die hessische Gemeinde Elz und die rheinland-pfälzische Ortsgemeinde Gückingen.
Der Siedlungsbereich der Stadt Limburg reicht über die Stadtgrenzen hinaus. Die Stadt Diez im benachbarten Rheinland-Pfalz schließt sich nahtlos an Limburg an.

Umliegende Städte und Gemeinden sind die Gemeinde Elz und die Stadt Hadamar im Norden, die Gemeinde Beselich im Nordosten, die Stadt Runkel im Osten, die Gemeinden Villmar und Brechen im Südosten, die Gemeinde Hünfelden im Süden (alle im Landkreis Limburg-Weilburg), die Gemeinde Holzheim im Südwesten sowie im Westen die Stadt Diez und die Gemeinden Aull und Gückingen (alle im Rhein-Lahn-Kreis in Rheinland-Pfalz).

Die nächstgrößeren, zwischen 40 und 60 Kilometern entfernten, Städte sind Wetzlar und Gießen im Nordosten, Frankfurt am Main im Südosten, Wiesbaden im Süden und Koblenz im Westen.

Limburg liegt in der Wetterregion Mittelhessens und damit in einer gemäßigten Klimazone der mittleren Breiten. Aus den Talverläufen und unterschiedlichen Geländehöhen ergeben sich unterschiedliche kleinklimatische Verhältnisse. Auf den Anhöhen südlich und nördlich des Lahntals regnet es mit 800 Millimetern genau den Durchschnittswert.

Die Stadt besteht neben der Kernstadt aus den sieben ehemals selbstständigen Gemeinden, die im Zuge der hessischen Gebietsreform eingemeindet wurden. Am 1. Oktober 1971 wurde die Gemeinde Dietkirchen eingegliedert. Am 31. Dezember 1971 kamen Ahlbach und Lindenholzhausen hinzu. Eschhofen, Linter, Offheim und Staffel folgten am 1. Juli 1974. Die Stadtteile (nach HGO: Ortsbezirke) nach Einwohnerzahl:

In der Anschrift beispielsweise wird von Einheimischen und Unternehmen als ein weiterer Stadtteil oftmals Blumenrod genannt, das jedoch nur ein Siedlungsgebiet im Süden der Kernstadt ist. Namensgebend für das 1967 neu entstandene Siedlungsgebiet und heutiges Wahrzeichen ist die Domäne Blumenrod, ein ehemaliges Hofgut, das von der Freien evangelischen Gemeinde Limburg saniert und umgebaut wurde ("siehe auch:" Burgstall Blumenrod). Gleiches gilt für die Brückenvorstadt auf der anderen Lahnseite der Kernstadt, die schon im Jahr 1564 unter dem Namen Keuch erwähnt wurde und 1795, nach dem Einmarsch französischer Truppen, fast vollständig abbrannte. Das neueste Siedlungsgebiet ist seit 2002 die ICE-Stadt Limburg.

Weitere Unterteilungen der Kernstadt in Siedlungsgebiete sind die Frankfurter Vorstadt, die Diezer Vorstadt und die Nordstadt mit einem eigenen Gemeindezentrum. Die Innen- und Altstadt von Limburg zählen ebenso zu den abgegrenzten Siedlungsgebieten innerhalb der Kernstadt, haben in der Stadtgliederung jedoch nur informellen Charakter.

Der Ursprung des Namens "Limburg" ist nicht vollständig geklärt. Er geht wahrscheinlich auf eine dort gegründete Burg in merowingischer Zeit zurück. Im Jahr 910 wurde das erste Mal der Ort unter dem Namen Lintpurc erwähnt. Zwei der gängigsten Thesen sind:

Archäologische Grabungen legten 2012 auf dem Areal der geplanten, neuen Lahntalbrücke Limburg bedeutende Spuren frei. Zum einen entdeckte man eine weilerartige Siedlung aus der Jungsteinzeit. Gefundene Scherben von Töpfen stammen aus der Zeit um ca. 5000 v. Chr. und sind damit bisher die ältesten entdeckten Spuren einer Besiedlung des Limburger Raums. Des Weiteren wurden auf einer Fläche von insgesamt 14 Hektar zwei römische Militärlager aus der Zeit des Kaiser Augustus freigelegt, die an dieser Stelle, aufgrund der großen Entfernung zum Limes, nicht erwartet wurden. Vergleichbare Spuren blieben bei Grabungen 1935, zum Bau der Reichsautobahn, teils unentdeckt und wurden zerstört. (siehe Römerlager Limburg)

Frühere Funde auf dem Limburger Domberg lassen darauf schließen, dass dieser bereits um 500 v. Chr. eine keltische Siedlung trug. Ihr Zentrum lag auf dem heutigen Domplatz.

Um 760 entstand die erste merowingische Befestigung auf dem späteren Domberg. Sie wurde wahrscheinlich zum Schutz einer Furt der Lahn errichtet. In ihrem Schutz entstand zunächst eine Ansiedlung, aus der sich später die Stadt entwickelte.

Der Name „Limburg“ erscheint erstmals 910 in einer Urkunde und bezeichnet keine Ansiedlung, sondern eine topographische Gegebenheit, nämlich einen Berg (mons), der als „Lintpurc“ bekannt sei: Ludwig das Kind überließ dem Gaugrafen Konrad Kurzbold einen Hof in Oberbrechen samt Zubehör und Ländereien zur Bewirtschaftung. Mit diesem Gut konnte Konrad das von ihm in seiner Burg auf dem Kalkfelsen oberhalb der Lahn gegründete und laut Ausweis der Urkunde noch zu errichtende Kanonikerstift St. Georg ausstatten (von dieser Burg gibt es heute keine Überreste mehr). Die darüber ausgestellte Urkunde befindet sich heute im Hauptstaatsarchiv Wiesbaden. Mit der Gründung des Stifts erlangte der Ort schnell an Bedeutung und profitierte durch den regen Warentransfer der Höhenstraße (Via Publica).

940 erhielt das Stift auf Bitten von Kurzbold und Bischof Diethard von Hildesheim erneut eine urkundlich erwähnte Zuwendung, diesmal von König Otto I. Dieser überließ ihnen das zuvor von dem Adligen Eberhard aus Niederzeuzheim besessene Gut zwecks Vermehrung des Unterhalts der Geistlichen. Ausgestellt wurde die Urkunde in Quedlinburg. Kurz darauf nahm Otto I. das Limburger Stift mitsamt dessen Besitztümern in seinen Schutz. Dadurch musste jeder königliche Repressalien fürchten, der es wagte, das Stift anzugreifen. Auch verfügte er, dass nach dem Tode Kurzbolds das Limburger Stift niemals mehr zu Lehen gegeben oder übereignet werden durfte. Im 11. Jahrhundert übereignete Kaiser Konrad II. dem Stift St. Georg neben Weinbergen auch den „Hof zu Kamp“ (heute Kamp-Bornhofen).
1160 wurde eine hölzerne Brücke als Teil der Fernstraße von Köln nach Frankfurt am Main über die Lahn errichtet, für deren Passieren spätestens seit dem Jahr 1227 bis zum Ersten Weltkrieg Zoll entrichtet werden musste. In der gesamten Stadtgeschichte war sie eine der wichtigsten Einnahmequellen der Stadt. Als Erbauer ist der Priester Gottfried von Beselich überliefert. Ende des 12. Jahrhunderts wurden die ersten Bauten der heutigen Burg Limburg errichtet. Kurz darauf ging der Ort in den Besitz der Herren von Ysenburg über. 1214 erlangte der Ort das Stadtrecht vom staufischen Kaiser Friedrich II. Reste der Schutzmauer aus den Jahren 1130, 1230 und 1340 mit einer größten Länge von rund 1000 Metern zeigen noch heute die schnelle Entwicklung der aufblühenden Stadt im Mittelalter. Für 1180 ist eine Münzprägung in Limburg verbürgt.

Bereits 1232 wurde der Franziskanerorden in Limburg ansässig und besaß ab 1252 eine eigene hölzerne Kirche auf dem Limburger Roßmarkt, die zu Beginn des 14. Jahrhunderts durch die heutige Stadtkirche ersetzt wurde. Der auf dem Platz der alten Stiftskirche erbaute Dom St. Georg, dessen Baubeginn heute nicht mehr genau zu datieren ist, wurde 1235 geweiht.

Ab 1246 lassen sich in Limburg Beginen nachweisen. Bei ihnen handelte es sich wohl vor allem um Frauen aus der Bürgerschaft. Auf Einladung von Gerlach I. († 1298) wurde das Wilhelmitenkloster Limburg zunächst auf der Lahninsel gegründet.

Am 14. Mai 1289 vernichtete ein verheerender Stadtbrand weite Teile der Limburger Innenstadt, die aber umgehend wieder aufgebaut wurden. Eines der errichteten Häuser ist der Römer 2-4-6, heute wahrscheinlich das älteste freistehende Gebäude Deutschlands.

Für 1304 ist erstmals ein Scholaster am Georgsstift nachgewiesen. Eine spätere Quelle verortet das Haus der Stiftsschule zwischen dem heutigen Dom und der Michaelskapelle. Darin soll auch eine kleinere Lateinschule neben der Stiftsschule untergebracht gewesen sein. Nicht vor 1484 existierte an der Nonnenmauer eine Mädchenschule.

Im Jahr 1317 wurde das Wilhelmitenkloster, aufgrund der ständigen Hochwassergefahr, in die Vorstadt am Diezer Tor verlegt. Weiterhin unterhielten das Prämonstratenserkloster Arnstein und Zisterzienserkloster Eberbach (Erbacher Hof) repräsentative Stadthöfe. Spätestens 1341 besaß die Gemeinschaft der Beginen ein eigenes Haus, 1417 wurden sie letztmals erwähnt. Die zu dieser Zeit herrschende Schicht der mittelalterlichen Bürgerschaft waren reiche Kaufmannsfamilien, deren Häuser in unmittelbarer Nähe des Burgtores standen und beim Bau der ersten Stadtmauer (frühes 12. Jahrhundert) von dieser eingehegt wurden. Der Bereich des heutigen Roßmarkts, in dem viele einfache Handwerker lebten, wurde erst beim zweiten Mauerbau 1225–1230 (heute Grabenstraße) in die Befestigung einbezogen. Ihre Bewohner erhielten im Vergleich zur Kaufmanns-Elite kein Mitspracherecht in städtischen Angelegenheiten und durften zunächst keine Vertreter in den Rat der Stadt entsenden, mussten aber die finanzielle Hauptlast des Gemeinwesens tragen. Erst 1458 durften sie zwei Vertreter in den Rat entsenden.

Eine Linie der Herren von Isenburg residierte von 1258 bis 1406 auf der Burg Limburg und nannte sich nach ihrem Sitz Herren von Limburg. Dieser Linie entstammte Imagina von Isenburg-Limburg, die Gattin des deutschen Königs Adolf von Nassau.

1315 und 1346 wurde, vermutlich in zwei Abschnitten, die steinerne Lahnbrücke errichtet. Spätestens mit dem Bau der steinernen Lahnbrücke entstand die Brückenvorstadt, ursprünglich Neustadt genannt. Die erste größere Baumaßnahme war das alte Hospital mit der Hospitalkirche (vor 1310). Das Zisterzienserkloster Marienstatt musste zur Einrichtung seines Stadthofs ab 1340 vier bestehende Häuser aufkaufen. Noch vor 1359 wurde die Brückenvorstadt durch einen Wall mit Graben geschützt. Sie verfügte mit der "Keucher Pforte", der "Dietkircher Pforte" und der "Hanenpforte" über drei Tore. Eine ähnliche Anlage, der "Schiedegraben", war 1343/44 bereits zum Schutz der Vorstädte auf der linken Lahnseite angelegt worden.

1336 bestätigte Kaiser Ludwig IV. der Bayer den Herren von Limburg den Besitz der Kammerknechtschaft über die Stadtjuden. Schon ein Jahr später wurden die Juden aus der Stadt vertrieben. Erst 1341 durften sie wieder auf königlichen Befehl in der Stadt siedeln. Die Ansiedlung stieß auf den Widerstand der Bevölkerung und war nur von kurzer Dauer. Bereits 1349 kam es erneut zu Pogromen und einer Vertreibung der Juden aus der Stadt. Im Zuge dieser Vertreibung wurden die Mikwe und das jüdische Festhaus nahe dem Eberbacher Hof eingezogen und an das Kloster verkauft. In der zweiten Hälfte des 14. Jahrhunderts siedelten zwar wieder Juden in der Stadt, diese waren jedoch nun auf das Ghetto am Kornmarkt beschränkt. Hier entstanden auch die erste erwähnte Synagoge und eine neue Mikwe.

1344 wurde die Stadt zur Hälfte an Kurtrier verpfändet. Dieser Vorgang, der erneute Stadtbrand 1342 (der nicht das Ausmaß des ersten hatte) und die Pest (1349, 1356 und 1365), vor allem aber der Aufstieg der Territorialfürsten, führten zum allmählichen Niedergang des Hauses Limburg. Hierbei wurde die Stadt zunehmend in lokale Fehden verwickelt. 1359 beteiligte sie sich an der Eroberung Villmars durch Kurtrier, 1360 an der Zerstörung der Burg Gretenstein. Im Oktober 1372 zerstörte ein Limburger Heer Ellar, das erst zwei Jahre zuvor die Stadtrechte erhalten hatte. Als Begründung diente der Aufenthalt der Räuber "Crae" und "Busse" in der Stadt. Auf Bestreben der Grafen von Katzenelnbogen wurde die Stadt Limburg vom obersten Hofgericht zur Zahlung von 2000 Mark Wiedergutmachung verurteilt. Da Limburg jedoch nur der Gerichtsbarkeit der Stadt Frankfurt unterstand, wurde dieses Urteil nie rechtskräftig vollzogen. 1380 greifen die Herren von Staffel Limburg an, in der Brückenvorstadt werden 20 Häuser durch die Staffeler niedergebrannt.

Mit dem Tod von Johann II. im Jahr 1406 war der letzte männliche Vertreter des Hauses Limburg gestorben. Dem Trierer Erzbischof gelang es, nachdem Stadt und Burg bereits zur Hälfte an ihn verpfändet waren, die Herrschaft zu übernehmen. 1420 ging sie ganz in kurtrierischen Besitz über.

Nachdem Kurtrier die Hälfte Limburgs an den Ritter Frank von Cronberg verpfändete, der als Mitherrscher bei den Limburger Bürgern unbeliebt war, kam es zu Unruhen. Daraufhin trat Cronberg das Pfand 1435 an den Landgrafen Ludwig I. von Hessen ab. Nach weiteren Aufteilungen der Besitztümer teilten sich diese ab 1482 wieder nur die Landgrafschaft Hessen und Kurtrier.

Im Umfeld des Deutschen Bauernkrieges kam es 1525 auch zu Unruhen in der Limburger Bevölkerung. Nachdem der Trierer Kurfürst verlangt hatte, dass die Bürgerschaft einen lutherischen Prediger aus der Stadt ausweise, überreichte ein Ausschuss aus nicht-ratsfähigen Bürgern am 24. Mai dem Rat eine 30 Punkte umfassende Forderungsliste. Darin ging es vor allem um finanzielle Mitbestimmung und die Gleichbehandlung in Abgaben-, Handels- und Baufragen mit der Kaufmannschaft. Diese Forderungen wurden in den folgenden Tagen in Verhandlungen zwischen Ausschuss und Rat auf 16 Punkte reduziert, die danach wohl mit dem Kurfürsten verhandelt wurden. Am 5. August verfügte Erzbischof Richard jedoch, dass der Rat sämtliche Zugeständnisse der Bürgerschaft gegenüber wieder aufheben sollte. Zudem wurde ein Versammlungsverbot erlassen und die nicht-ratsfähige Bürgerschaft sollte auch ihre beiden Vertreter nicht mehr in den Rat schicken dürfen.

Die Reformation führte zu Konflikten in der Stadt. Aufgrund der Zugehörigkeit Kurtrier blieb die Stadt jedoch Katholisch. Allerdings traf es die Klöster der Stadt. Das Wilhelmitenkloster wurde nach dem Tod des letzten Priors 1568 aufgelöst und das Franziskanerkloster war zwischen 1577 und 1582 geschlossen.

Während des Dreißigjährigen Kriegs wurde Limburg 1631 und 1635 von durchziehenden Soldaten geplündert und belagert. Aufgrund der verkehrsgünstigen Lage war die Stadt Teil der Marschrouten, weshalb sich die Soldaten bevorzugt bei der Bevölkerung einquartierten. Auch vom Ersten Koalitionskrieg im folgenden Jahrhundert blieb Limburg nicht verschont. Während ihres Rückzugs 1796 versuchten die französischen Streitkräfte während der sog. Schlacht bei Limburg die österreichischen am Überqueren der Lahn zu hindern. Bei einem Feuergefecht brannten sie dabei Teile der Stadt ab.

In die 1760er Jahre fiel eine grundlegende Reform des Schulwesens. Die zahlreichen Winkelschulen wurden geschlossen und zusätzlich zu den beiden seit dem Mittelalter bestehenden Schulen, der Stiftsschule für Jungen und der Nonnenschule für Mädchen, zwei weitere Schulen eingerichtet, die beide aus dem Hospitalfonds der Stadt finanziert wurden. So entstanden die im Hospitalgebäude angesiedelte Hospitalschule für Jungen und die Jungfernschule für Mädchen. Ende des 18. Jahrhunderts wurden die beiden Mädchenschulen zusammengelegt, die bereits durch die Auflösung des Stifts stark dezimierte Stiftsschule 1817 der Hospitalschule zugeschlagen. Die Lateinschule zog in die so genannte „Aula“ am Rossmarkt und wurde später zur Volksschule. 1872 wurde ein Neubau für die Mädchenschule in der Hospitalstraße fertiggestellt. Zu einem späteren Zeitpunkt wurde eine weitere Volksschule unweit der Hospitalschule in der Werner-Senger-Straße eingerichtet. Der Turnunterricht fand im 19. Jahrhundert in einer Halle am Rossmarkt statt.

1806 fiel Limburg an das neu gegründete Herzogtum Nassau, womit auch das 900 Jahre alte Stift erlosch und die Stiftskirche zur Pfarrkirche wurde. 1813 endete die Existenz des Franziskanerklosters. 1818 wurde die Stadtmauer niedergelegt. 1827 wurde die Stadt auf Drängen des Herzogs Wilhelm von Nassau hin zum katholischen Bischofssitz erhoben, wodurch die Pfarrkirche den Rang einer Kathedrale erhielt. 1830 wurde am Kornmarkt die Stadtmauer durchbrochen und später vollständig niedergelegt. Die Freiheitsbewegung des Jahres 1848 hatte zur Folge, dass die Limburger Katholiken wieder viele Jahrzehnte zur Wallfahrtskapelle Maria Hilf Beselich pilgerten und dort eindrucksvoll ihren Glauben kundgaben. Ab 1862 war Limburg Knotenpunkt wichtiger Eisenbahnstrecken, bevor das Herzogtum und damit Limburg 1866 als Folge des Deutschen Krieges an Preußen fiel. So wurde Limburg ab 1886 zur Kreisstadt des neuen Kreis Limburg bestimmt und Sitz des königlichen Landrates.

Die Freiwillige Feuerwehr der Stadt gründete sich am 6. Februar 1867 und wählte den Seifenfabrikanten Joseph Müller zu ihrem Kommandanten. Am 2. Juni 1873 war die Limburger Feuerwehr Ausrichter der ersten Verbandsversammlung des Feuerwehrverbandes für den Regierungsbezirk Wiesbaden nach dessen Gründung.

1892 kamen die Pallottiner erstmals nach Deutschland und ließen sich in Limburg nieder. 1895 folgten die Pallottinerinnen. Weil der Walderdorffer Hof zu klein wurde, erwarb die Glaubensgemeinschaft 1896 ein Gelände, auf dem sie zunächst ihr Missionshaus und 1926/1927 die Marienkirche baute.

1900 bezog die Stadtverwaltung das neue Rathaus in der Werner-Senger-Straße.

Limburg war nie eine Garnisonsstadt im eigentlichen Sinn, aber Standort mehrerer militärischer Versorgungs- und Verwaltungseinrichtungen. 1889 wurde ein Bezirkskommando des Deutschen Heeres von Weilburg nach Limburg verlegt und blieb dort bis 1918 bestehen. Wegen starkem Wind musste am 24. April 1910 das Luftschiff "Zeppelin II (LZ 5)" auf dem Rückflug von einer Parade in Bad Homburg, zu Ehren Kaiser Wilhelms II., auf dem Hofgut Blumenrod notlanden. Am nächsten Morgen riss es sich los und flog unbemannt in Richtung Weilburg, wo es an einem Berg zerschellte. Abgeleitet von diesem Ereignis erhielt die „Zeppelinstraße“, die den ab 1967 entstandenen Stadtbezirk Blumenrod von Osten nach Westen durchzieht, ihren Namen.

1904 wurde die Werner-Senger-Schule erheblich erweitert. 1908 folgte die Aufgabe des Volksschulgebäudes am Rossmarkt, die dortige Volksschule wurde unter dem Namen „Wilhelmitenschule“ in den Anbau des alten Gymnasiums verlegt und 1909 um die erste Limburger Hilfsschulklasse ergänzt. Bei einem Bombenangriff Anfang 1944 wurde das Gebäude der Werner-Senger-Schule zur Hälfte zerstört.

Nach Ende des Ersten Weltkriegs wurde Limburg während der Rheinlandbesetzung von 1919 bis 1923 nicht besetzt. Weil es die nächste unbesetzte Stadt der Weimarer Republik war, wurde sie „Hauptstadt“ und Gerichtssitz des Freistaats Flaschenhals.

Bevor Stadtverordnete und der Limburger Bürgermeister Krüsmann sich 1933 den SA-Truppen beugten und zurücktraten, hielt ein Jahr zuvor Adolf Hitler im Rahmen seines „Deutschlandflugs“ eine Rede auf dem Neumarkt – dem späteren Adolf-Hitler-Platz. 1938 entstand in der Stadt ein Wehrbezirkskommando, kurz vor Kriegsende wurde die 13. SS-Eisenbahnbaubrigade nach Limburg verlegt zur Aufrechterhaltung des „Bahnhofsbetriebs“.

Zwischen 1941 und 1944 wurden auf Veranlassung der Gestapo Frankfurt/Main, die ab 1944 im Erbacher Hof eine Außenstelle unterhielt, zwölf Limburger Pallottiner in Schutzhaft genommen, um sich das Anwesen der Pallottiner aneignen zu können. Sie wurden zunächst nach Frankfurt und von dort aus in das KZ Dachau gebracht, wo zwei von ihnen starben. Indem der Limburger Stadtpfarrer Heinrich Fendel, unter Mitwirkung von Domkapitel, Kirchenvorstand und den Pallottinern, 1943 eine Pfarrvikarie mit der Marienkirche als Zentrum errichtete, konnte die endgültige Vertreibung der Pallottiner aus Limburg verhindert werden. Obwohl die Glaubensgemeinschaft ein Jahr später gezwungen wurde, Missionshaus und Kirche zu verlassen, kehrte sie unmittelbar nach Ende des Zweiten Weltkriegs zurück und blieb bis heute auf ihrem Anwesen ansässig.

Ab Juli 1945 gehörte Limburg, als ehemaliger Teil der Provinz Nassau, zur amerikanischen Besatzungszone und wurde so später Teil des neuen Landes Groß-Hessen und schließlich Hessen.

1966 zog die „Depotgruppe Mitte“ der Bundeswehr, später Versorgungskommando 850, von Mainz nach Limburg um. Sie diente der Materialversorgung, insbesondere des III. Korps, und wurde in der ehemaligen Fabrik Scheid untergebracht. Das Versorgungskommando verwaltete mehrere Depots mit mehr als 3000 Mitarbeitern. 1976 wurde zudem ein Gerätedepot aus Lindenholzhausen an den Sitz des Versorgungskommandos in Limburg verlegt. 1994 wurde die Limburger Bundeswehr-Niederlassung aufgelöst.

In den 1960er Jahren wurden mehrere Wohn- und Gewerbegebiete ausgewiesen, die die Kernstadt vor allem nach Norden, Nordosten und Süden wachsen ließen. Nach Osten und Westen blieben die Bebauungsgrenzen weitgehend unverändert.

2010 feierte Limburg den 1100. Jahrestag seiner ersten Erwähnung. Aus diesem Anlass erschien Anfang Januar 2010 eine Sonderbriefmarke, deren Motiv ein Gemälde von George Clarkson Stanfield von 1862 ist, das die Alte Lahnbrücke mit äußerem Brückenturm und den Dom von Nordwesten zeigt. Das Original befindet sich im Rheinischen Landesmuseum Bonn.

In erhaltenen Urkunden wurde Limburg unter den folgenden Ortsnamen erwähnt (in Klammern das Jahr der Erwähnung):

Die folgende Liste zeigt die Territorien bzw. Verwaltungseinheiten denen Limburg unterstand im Überblick:

Die Stadt ist der Sitz des Bistums Limburg, das 1827 gegründet wurde. Es umfasst bis heute das Gebiet des damaligen Herzogtums Nassau, der Landgrafschaft Hessen-Homburg und der Stadt Frankfurt am Main.

Die beiden evangelischen Kirchengemeinden Limburgs gehören zum Dekanat Runkel in der Evangelischen Kirche in Hessen und Nassau. Die Gemeinde in Limburg bildete sich im frühen 19. Jahrhundert und war eine Filialgemeinde der evangelischen Kirche in Staffel. 1831 erhielt sie vom Herzog von Nassau die Kapelle in Erbach geschenkt, die im Dezember vom Staffeler Pfarrer Georg Ninck eingeweiht wurde. Wegen des stetigen Wachstums der Limburger Gemeinde wurde eine größere Kirche benötigt. Diese wurde am 29. Mai 1866 geweiht und steht bis heute in der Nähe des Limburger Regionalbahnhofs. Erst 1879 erhielt Limburg den Status einer selbstständigen evangelischen Gemeinde.

Von 1973 bis 1975 wurde die Kirche umgebaut. Zur Unterteilung wurden zwei Zwischendecken eingezogen. Das oberste Drittel blieb Kirche. Im mittleren Drittel wurden die Räume eines benachbarten Gemeindehauses eingerichtet. Im Erdgeschoss befindet sich heute eine Jugendfreizeitstätte.
Die ersten Juden kamen wohl um 1190 aus Frankreich nach Limburg. Urkundlich erwähnt ist eine jüdische Gemeinde im Jahr 1278. In diesem Jahr wechselte die Kammerknechtschaft vom Kaiser zum Herrn der Burg Limburg. Das Judenviertel befand sich zwischen dem Kornmarkt und dem heutigen Bischofsplatz sowie zwischen Fleischgasse und der heutigen Kolpingstraße. Es war teilweise durch eine Mauer von der restlichen Stadt abgetrennt. Eine Synagoge ist dort seit der ersten Hälfte des 14. Jahrhunderts belegt. Ein Tanzhaus, eine Schule und ein Badehaus (Reste im Keller des Hauses Plötze 3 sind noch zu sehen) waren ebenfalls vorhanden. Im frühen 14. Jahrhundert wurden im Gefolge der Frankfurter „Judenschlachten“ nahezu alle Juden aus der Stadt vertrieben. Die Gemeinde erholte sich zunächst wieder. Als Limburg 1420 kurtrierisch wurde, begann auch dort, wie im gesamten Erzbistum, die Vertreibung der Juden. Um 1450 war die jüdische Gemeinde erneut ausgelöscht. Vereinzelt lebten Juden in den folgenden Jahrhunderten in Limburg, eine größere Gemeinde scheint sich aber nicht mehr gebildet zu haben. Als Synagoge wurde bis ins 18. Jahrhundert ein Keller am Fischmarkt genutzt. Als Ambrosio Spinola um 1620 nassauische Gebiete besetzte, flohen noch einmal sechs jüdische Familien nach Limburg, die aber bis 1629 von der Bürgerschaft wieder vertrieben wurden.

Erst nach dem Dreißigjährigen Krieg bildete sich wieder eine dauerhafte jüdische Gemeinschaft in Limburg. 1725 wurde eine besondere Judenordnung erlassen. Für 1754 sind sechs jüdische Familien verbürgt. 1852 waren 60 Einzelpersonen jüdischen Glaubens registriert, 1910 waren es 281. Ein Teil der jüdischen Bewohner Limburgs im 18. Jahrhundert muss sehr wohlhabend gewesen sein, da zwei von ihnen die besten Steuerzahler der Stadt waren. Ein jüdischer Friedhof entstand im heute als „Schlenkert“ bezeichneten Teil der Stadt und wurde bis 1820 genutzt. Danach wurde der heute noch bestehende Friedhof am Schafsberg eingerichtet. Das zuständige Rabbinat befand sich in Diez. 1868 kaufte die jüdische der evangelischen Gemeinde die ehemalige Kapelle des Klosters Eberbach ab und wandelte sie zur Synagoge um. 1903 war die neue Synagoge im neoromanischen Stil an der Schiede fertiggestellt. Das Gotteshaus bot 201 Männer- und 104 Frauenplätze und verfügte über eine benachbarte Mikwe.

1932 lebten 296 Juden in Limburg. Mit der einsetzenden Verfolgung im Dritten Reich sank ihre Zahl schnell. 1937 gab es noch 154 jüdische Einwohner, nach der Pogromnacht 1938, in der auch die Synagoge zerstört wurde, noch 86. An die ehemalige Synagoge erinnert heute ein Modell aus Bronze, das 2015 gegenüber dem Landgericht errichtet wurde. Ende September 1939 lebten noch acht ältere jüdische Einwohner in Limburg. Für rund 80 jüdische Limburger ist eine Auswanderung nachgewiesen. Über das Schicksal der Übrigen ist nichts bekannt. Die meisten von ihnen wurden vermutlich ermordet. 1945 kehrten lediglich drei jüdische Limburger in ihre Heimatstadt zurück. Der Dichter und Heimathistoriker Leo Sternberg, der 1933 zum Katholizismus konvertierte, bekanntestes Mitglied der jüdischen Gemeinde der Stadt, starb 1937 im Exil in Jugoslawien.

Seit 1998 existiert in Limburg wieder eine jüdische Gemeinde, die 2009 eine Mitgliederzahl von 200 aufwies. Die Gemeinde besteht ausschließlich aus russischen Immigranten. Im Februar 2009 wurde die neue Synagoge eingeweiht. Wie in vielen anderen Städten auch, wurden in Limburg ab 2013 sogenannte Stolpersteine zum spontanen Gedenken an die jüdischen Opfer der Nationalsozialisten installiert.

Die "Bait-ul-Ahad Moschee" der Ahmadiyya-Muslim-Jamaat-Gemeinde wurde im Mai 2012 gegenüber der Polizeistation Limburg eröffnet. Eine etwas kleinere Moschee, die "Fatih-Moschee" („Eroberer-Moschee“) der Islamischen Gemeinde Millî Görüş (IGMG), benannt nach Mehmed II., dem Eroberer des christlichen Konstantinopel, befindet sich in der Eisenbahnstraße und wurde 1990 gegründet. Bereits seit 1980 existiert die "Bilal-i Habesi Moschee" der DITIB-Gemeinde, die bis zur Grundsteinlegung eines neuen Gebäudes im Jahr 2008 als Provisorium in einem ehemaligen Sägewerk eingerichtet war. Am 2. Oktober 2010 wurde das neue Bauwerk in der Blumenröder Straße eröffnet und ist mit einem etwa 18 Meter hohen Minarett heute die größte Moschee in Limburg an der Lahn.

Bereits zu Beginn des 19. Jahrhunderts zählte Limburg zu den Kleinstädten. Nach heutiger Definition wurde die Schwelle erst mit den Eingemeindungen überschritten, wodurch die Einwohnerzahl sprunghaft anstieg. Mit Ausnahme der Zeit des Nationalsozialismus und dem Ende der 1960er Jahre erreichte sie nach stetigem Anstieg mit 33.977 Einwohnern im Jahr 2005 ihren Höchststand. Zwischen 2005 und 2010 sank die Einwohnerzahl geringfügig. Dieser Trend endete 2011 wieder mit einer Zunahme auf 33.619 Einwohner im Jahr 2012.

Aufgrund der demografischen Entwicklung gehen Prognosen des statistischen Landesamtes in Hessen mittelfristig von einem Zuwachs der Bevölkerung deutlich über die Marke von 34.000 aus. Insbesondere Personen ab 60 Jahren werden demnach vom ländlichen Umland in die Stadt ziehen.

Der Ausländeranteil bezifferte sich am 31. Dezember 2015 auf 15,2 Prozent (5.205 Personen).

Bereits um 1200 führte die Stadt ein Siegel, dessen Abbild heute jedoch nicht mehr bekannt ist, da es nicht erhalten wurde. Ein zweites, erhaltenes Siegel mit einer Abbildung ähnlich dem heutigen Wappen ist hingegen für das Jahr 1243 belegt. Das Wappen ist seit 1908 amtlich.

Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:

Bürgermeister der Stadt ist seit dem 2. Dezember 2015 Marius Hahn (SPD). Er folgte Martin Richard (CDU), der vom 2. Dezember 1997 bis zum 1. Dezember 2015 Bürgermeister war. Bei der Bürgermeisterwahl am 14. Juni 2015 setzte sich der von einer Wählerinitiative nominierte Hahn mit 53,2 Prozent der gültigen Stimmen gegen den Ersten Stadtrat Michael Stanke (CDU) durch. Die Wahlbeteiligung betrug 41,6 Prozent.

Liste der Bürgermeister seit 1833


Mit der Schließung der Partnerschaft zwischen Limburg und Lichfield wurde zugleich eine Ringpartnerschaft mit Sainte-Foy-lès-Lyon geschlossen, deren zwanzigjähriges Bestehen 2012 in Limburg gefeiert wurde. Als Zeichen der Verbundenheit mit seinen Partnerstädten enthüllte die Stadt zu diesem Anlass offiziell die im selben Jahr in Naturstein gepflasterten Wappen der vier Partnerstädte auf dem Europaplatz.

1956 wurde die Patenschaft für die vertriebenen Sudetendeutschen aus der Stadt Mährisch Neustadt im Kreis Sternberg übernommen. 1961 übernahm die Stadt die Patenschaft für einen U-Boot-Tender der Bundesmarine. Das Schiff erhielt zwar den Namen "Lahn", trug aber das Wappen der Stadt Limburg.

Nach Abgrenzung der IHK zählt Limburg als eines der städtischen Zentren zum Rhein-Main-Gebiet und weist seit vielen Jahren schwankende Gewerbeinvestitionen auf. Kontinuierlich stark vertreten ist der Einzelhandel, was sich u. a. an einem überdurchschnittlich hohen Umsatz, gemessen an der Einwohnerzahl, bemerkbar macht.
Eine dominierende Branche ist in Limburg jedoch nicht vorhanden, da neben Niederlassungen weniger Konzerne überwiegend mittelständische Unternehmen aus den unterschiedlichsten Branchen angesiedelt sind. Kennzeichnend dafür sind auch die Gewerbegebiete, die in den 1960er und 1970er Jahren auf den Gemarkungen einzelner Stadtteile ausgewiesen wurden. Heute befinden sich in fast allen Himmelsrichtungen der Stadt Gewerbeansiedlungen.

Als Entscheidungskriterien für die Wahl des Standortes Limburg nennen Unternehmen seit jeher u. a. die zentrale Lage und eine gute Verkehrsanbindung. Seit der Anbindung an die Schnellfahrstrecke Köln–Rhein/Main der Deutschen Bahn wurde Limburg durch die schnellere Erreichbarkeit zunehmend als Tagungsort für Unternehmen interessant, die nahe den Metropolen Köln und Frankfurt am Main angesiedelt sind. Die Stärkung des Dienstleistungssektors, in dem mittlerweile über 18.000 Menschen arbeiten, wurde dadurch begünstigt.

Trotz geringfügigem Rückgang der Einwohnerzahl im erwerbsfähigen Alter ist im Zeitraum von 2000 bis 2010 die Zahl der Beschäftigten in Limburg mit 8,5 Prozent signifikant gestiegen, verglichen mit dem Anstieg von 0,5 Prozent in Hessen. In Zusammenhang damit stehen neu entstandene Arbeitsplätze und ein Pendlerüberschuss. 2012 betrug die Zahl der sozialversicherungspflichtig beschäftigten Einpendler 16.063, die Zahl der Auspendler 6401. Die Zahl der sozialversicherungspflichtig Beschäftigten im selben Jahr betrug 20.471 Personen. Damit entfielen 44 Prozent der Arbeitsplätze im Landkreis Limburg-Weilburg auf die Stadt Limburg.

Limburg ist ein traditioneller Verkehrsknoten. Bereits im Jahr 1248 bestand eine hölzerne Lahnbrücke, die nach dem Hochwasser 1306 durch eine steinerne Brücke "(Alte Lahnbrücke)" ersetzt wurde. Weitere Straßenbrücken sind die "Lahntalbrücke Limburg" (1964) der A 3, die Lahnbrücke bei Staffel und die "Neue Lahnbrücke" von 1968. Auf der Neuen Lahnbrücke überqueren die Bundesstraßen die Lahn, bevor sie im Schiedetunnel den Bahnhofsvorplatz sowie die Eisenbahnstraße im Innenstadtbereich unterqueren. Bereits im Mittelalter kreuzte die Via Publica hier die schiffbare Lahn. Heute durchqueren die Stadt die A 3 und die B 8, die dem Verlauf der Via Publica weitestgehend folgen. Die B 49 verbindet Limburg nach Westen mit Koblenz und nach Osten mit Wetzlar und Gießen. Der Abschnitt zwischen Limburg und Wetzlar wird zurzeit vierspurig ausgebaut. Der Teilabschnitt bis Obertiefenbach wird auch als die "Lange Meil" bezeichnet.
Die B 54 verbindet Limburg einerseits in Richtung Norden mit Siegen und führt andererseits über Diez nach Wiesbaden in südliche Richtung. Über die B 417 "(Hühnerstraße)" ist Wiesbaden ebenfalls zu erreichen. Eine Südumgehung der B54 ist geplant, aber nicht unumstritten.

Seit Jahren zählt Limburg zu Hessens Städten mit den höchsten Schadstoffbelastungen in der Luft. Der Jahresbericht 2014 des HLNUG bescheinigt der Stadt bei den gemessenen Stickstoffdioxiden im Jahresmittel den schlechtesten Wert im Städtevergleich. Zurückgeführt wird dieser Umstand vor allem auf den starken Durchgangsverkehr auf den Bundesstraßen, weshalb die Stadt Maßnahmen zur Erfüllung eines Luftreinhalteplans des hessischen Umweltministeriums beschlossen hat. Seit 31. Januar 2018 ist Limburg eine Umweltzone.

Die Stadtbuslinien wurden Ende 1959 in Betrieb genommen. Bis zur Sanierung des Schiedetunnels 2008 und der damit verbunden Umgestaltung des Bahnhofsvorplatzes des Regionalbahnhofs und Änderung der Verkehrsführung, war der zentrale Omnibusbahnhof Nord (ZOB Nord) in der Graupfortstraße Dreh- und Angelpunkt der Stadtlinien. Mit der Errichtung des nahegelegenen zentralen Omnibusbahnhof West (ZOB West) hat dieser für die heute insgesamt sechs Stadtlinien, von denen vier werktags, eine an Samstagen und eine an Sonn- und Feiertagen verkehren, an Bedeutung verloren. Alle Linien der Stadtbusse beginnen und enden am ZOB West am Bahnhofsvorplatz. Am Übergang von der Holzheimer Straße in die Eisenbahnstraße liegt der zentrale Omnibusbahnhof Süd (ZOB Süd), der durch eine Fußgängerunterführung mit dem Bahnhofsvorplatz verbunden ist. Dieser Busbahnhof verbindet den Bahnhof Limburg Süd bzw. die Innenstadt mit der ICE-Stadt über Shuttlebusse und ein Anruf-Sammel-Taxi. Einziger Stadtteil außerhalb der Kernstadt mit direkter Anbindung an die Stadtlinien ist Offheim.

Seit Ende Mai 2015 wird das ÖPNV-Angebot durch Anruf-Sammel-Taxen ergänzt, die auf drei Linien und somit in allen Stadtteilen verkehren. Insbesondere die Nachfrage während der Abend- und Nachtstunden, wenn keine Stadtbusse fahren, soll mit diesem Angebot gedeckt werden.

Darüber hinaus decken fünf Taxibetriebe die Personenbeförderung im Bereich des Individualverkehrs ab, die sich insgesamt auf 18 von der Stadt Limburg vergebene Taxilizenzen aufteilen (Stand: Februar 2014). Der zentrale Taxistand befindet sich auf dem Bahnhofsvorplatz des Regionalbahnhofs.

Mit dem Bau der Lahntalbahn wurde Limburg 1862 an das Eisenbahnnetz angeschlossen. Der Regionalbahnhof Limburg (Lahn) entwickelte sich zu einem zentralen Verkehrsknoten. Weitere Bahnlinien sind die Unterwesterwaldbahn nach Siershahn über Montabaur, die Westerwald-Sieg-Bahn nach Kreuztal über Westerburg, Hachenburg, Altenkirchen, Au (Sieg) und Siegen sowie die Main-Lahn-Bahn nach Frankfurt (Main) Hauptbahnhof. Am Bahnhof Niedernhausen an der Main-Lahn-Bahn kann in die Ländchesbahn nach Wiesbaden Hauptbahnhof umgestiegen werden, ebenfalls existieren durchgängige Zugverbindungen von Limburg nach Wiesbaden Hauptbahnhof.

Mit dem Bau der Schnellfahrstrecke Köln–Rhein/Main erhielt die Stadt mit dem Bahnhof Limburg Süd einen Fernbahnhof, an dem ausschließlich ICE halten. Die Schnellfahrstrecke überquert die Lahn mit der Lahntalbrücke und „verschwindet“ nördlich des Flusses im Limburger Tunnel. Es bestehen direkte Verbindungen unter anderem nach Frankfurt am Main, Mainz, Wiesbaden, Nürnberg und München sowie nach Köln und Dortmund.

"Siehe auch:" Bahnhof Limburg Süd

Seit dem 14. Mai 2015 ist Limburg über die Fernbusanbieter DeinBus.de, Flixbus und Onebus.de an das nationale Fernbusnetz angeschlossen. Verbindungen bestehen von der Haltestelle am Bahnhof Limburg Süd nach Aachen, Bonn, Frankfurt am Main, Frankfurt Flughafen, Heidelberg, Heilbronn, Köln, zum Flughafen Köln/Bonn, zum Flughafen Stuttgart sowie nach Tübingen.

Die Lahn ist zwischen Lahnstein und Gießen eine Bundeswasserstraße und fällt in den Zuständigkeitsbereich des Wasser- und Schifffahrtsamt Koblenz, dessen Außenstelle Diez die Limburger Schleuse betreibt und unterhält. Seit dem Ausbau der Lahntalbahn von Koblenz Hbf nach Wetzlar ist die Bedeutung als Wasserstraße jedoch rückläufig. Genutzt wird die Wasserstraße vorwiegend touristisch durch kleinere Motorboote sowie Kanus und Ruderboote und das Fahrgastschiff "Wappen von Limburg".

Der nächstgelegene Verkehrsflughafen ist der Flughafen Frankfurt am Main mit einer Entfernung von 63 Kilometern über die A 3. Die Fahrzeit mit dem ICE vom Fernbahnhof "Limburg Süd" nach Frankfurt Flughafen (Fernbahnhof) beträgt je nach Verbindung 18 bis 22 Minuten. Der Flughafen Köln/Bonn ist 110 Kilometer entfernt und mit dem ICE in 35 bis 46 Minuten zu erreichen.

Der Hessische Radfernweg R7 verläuft von seinem Startpunkt im Stadtteil Staffel aus vorbei am Limburger Dom und führt weiter über den Stadtteil Dietkirchen nach Runkel. Bis Dietkirchen verläuft er identisch mit dem Lahntalradweg und dem Hessischen Radfernweg R8, der an der Rad- und Fußgängerbrücke abzweigt und weiter durch die Gemarkungen der Stadtteile Eschhofen und Lindenholzhausen führt. In der Kernstadt gibt es Radwege, die jedoch kein durchgängiges Netz bilden. Stellenweise sind sie durch Schutzstreifen miteinander verbunden, die aufgrund beengter Platzverhältnisse verstärkt im Innenstadtbereich eingerichtet wurden.

Die beiden Wanderwege Lahnhöhenweg und Lahnwanderweg führen durch Limburg. Auch ist die Stadt ein Teil des Lahn-Caminos, einem Abschnitt des Weges der Jakobspilger.

Bereits im Jahr 1872 als "Josef Heppel Blech-Emballagen-Fabrik" gegründet und bis heute am Standort ansässig ist die Blechwarenfabrik Limburg, die ihre lange Verbundenheit mit der Region auch in ihrem Firmennamen zum Ausdruck bringt.

Zurück bis in das Jahr 1900 reicht die Geschichte des ehemaligen Buderus-Werks, das nach zweimaliger Veräußerung im Jahr 2009 als Buderus Kanalguss GmbH von der MeierGuss-Gruppe übernommen wurde. Seit dem 1. Januar 2012 wird der Betrieb als "MeierGuss Limburg GmbH" fortgeführt. Flächenmäßig zählt es zu den größten ansässigen Unternehmen am Standort Limburg. Weithin sichtbar ist der im Jahr 1928 errichtete und denkmalgeschützte Wasserturm des Werks. Bis Anfang 2014 prangte der Markenname Buderus an dem Turm, wurde dann aber durch das MeierGuss-Logo ersetzt.

Zu den traditionsreichen Unternehmen Limburgs zählt auch die Glashütte Limburg. Seit dem Gründungsjahr 1947 hält das Bundesland Hessen Gesellschaftsanteile des Unternehmens.

Dem Unternehmen Tetra Pak Produktions GmbH & Co. KG verdankt die Stadt Limburg den Brunnen neben der Stadthalle, eines ihrer Wahrzeichen. Der von Einheimischen oftmals als „Pusteblume“ bezeichnete Brunnen wurde 1975 von dem Unternehmen finanziert.

1975 verlagerte die Mundipharma GmbH, deren Firmengelände heute unter Denkmalschutz steht, ihren Sitz von Frankfurt am Main nach Limburg. Ebenso verlegte die heutige Harmonic Drive AG ihren Firmensitz 1988 von der Stadt Langen (Hessen) nach Limburg.

Unter dem Namen "Soda-Club GmbH" errichtete 1994 die mittlerweile in SodaStream umbenannte Unternehmensgruppe ihren bis heute einzigen deutschen Standort in Limburg.

Im Jahre 2003 wurde zur Bedienung des Westerwald-Taunus-Bahnnetzes die Vectus Verkehrsgesellschaft mbH mit Sitz in Limburg gegründet. Sie war ein Tochterunternehmen der Hessischen Landesbahn und der Westerwaldbahn GmbH und betrieb vom Fahrplanwechsel 2004 bis zum Fahrplanwechsel 2014 die Regionalbahnlinien der Lahntalbahn zwischen Limburg und Koblenz, die Unterwesterwaldbahn, die Oberwesterwaldbahn sowie die Ländchesbahn zwischen Limburg und Wiesbaden Hbf. Heute werden diese Linien durch die DB Regio und die Hessische Landesbahn betrieben.

In Limburg ansässige Geldinstitute sind die Kreissparkasse Limburg, die seit 2010 ihre EC-Karte wahlweise mit einer Abbildung des Limburger Doms anbietet, sowie die Volksbank Rhein-Lahn-Limburg. Diese stiftete aus Anlass ihres 125-jährigen Bestehens im Jahr 1985 ein Denkmal, das auf dem Europaplatz neben ihrer Hauptgeschäftsstelle zum Gedenken an Werner Senger und sein Vermächtnis errichtet wurde.

In Limburg erscheint die "Nassauische Neue Presse", ein Kopfblatt der "Frankfurter Neue Presse", mit einer Auflage von knapp 25.000 Exemplaren. Der Sitz der Lokalredaktion befindet sich in der Limburger Innenstadt.

Mit der "Lahn-Post" der "Verlag für Anzeigenblätter GmbH" erscheint seit über 40 Jahren ein Anzeigenblatt in Limburg. Bis zur Insolvenz der "MedienErleben-Verlag GmbH" Anfang 2013 erschien kurzzeitig einmal wöchentlich "medienerleben.de – Die Zeitung".
Seit 2016 erscheint mit der "Limburger Zeitung" ein neues Anzeigenblatt, das u. a. Nachrichten aus der Limburger Region beinhaltet.

Der Hessische Rundfunk setzt in Limburg eine Regionalkorrespondentin ein.

In Limburg gibt es elf Hotels, darunter drei 3-Sterne- und zwei 4-Sterne-Hotels, sowie zwei Pensionen (Stand: Januar 2014).

Die Zahl der Touristen in Limburg nimmt seit Jahren kontinuierlich zu, ebenso die Zahl der Übernachtungen. Nach einer Erhebung des Statistischen Landesamtes erreichte die Zahl der ankommenden Gäste in den Beherbergungsbetrieben 2011 ein Zuwachs von 7,1 Prozent gegenüber dem Vorjahr. Einzelne Monate erreichten in der Hauptsaison zweistellige Zuwachsraten und lagen damit über dem Landesdurchschnitt von 4,1 Prozent. Die Zahl der Übernachtungen in Limburg stieg um 10,4 Prozent gegenüber dem Vorjahr (3,1 Prozent im Landesdurchschnitt). Einen großen Anteil daran hat die Zunahme des Geschäfts-Tourismus. Mit über 70 Prozent kommt der größte Teil der Besucher aus Deutschland.

Seit 1965 besteht ein Campingplatz am rechten Lahnufer. Limburg ist seit 1981 Anlegestelle des Fahrgastschiffs "Wappen von Limburg". Das aktuelle Schiff wurde 1987 in Dienst gestellt und fährt seit 2012 unter einem neuen Betreiber. Die Jugendherberge am Ostrand der Stadt wurde 1964 eingeweiht. Zuvor befand diese sich in der Stadtmitte.

Limburg liegt an den beiden Ferienstraßen Deutsche Fachwerkstraße und Lahn-Ferien-Straße.

Regional bedeutsame öffentliche Einrichtungen in Limburg sind die IHK Limburg und die Kreishandwerkerschaft Limburg-Weilburg.

Limburg ist Sitz des Amtsgerichtes Limburg und des Landgerichtes Limburg, eines von neun Landgerichten in Hessen. Unmittelbar neben dem Landgericht befinden sich die Staatsanwaltschaft Limburg sowie die Justizvollzugsanstalt Limburg, in der ausschließlich männliche Gefangene untergebracht sind.

Mit rund 150 ansässigen Rechtsanwälten (Stand: 2013) ist Limburg ein regional bedeutsamer Justizstandort.
Bis zum 31. Dezember 2011 war die Stadt Sitz des Arbeitsgerichtes Limburg; seitdem ist das Arbeitsgericht Wiesbaden zuständig.

Mit der Polizeidirektion Limburg-Weilburg und einer regionalen Kriminalinspektion ist das "Polizeipräsidium Westhessen" in Limburg vertreten. Untergeordnete Polizeistationen befinden sich in Limburg, Weilburg und Bad Camberg.

Neben der Kreisverwaltung Limburg-Weilburg haben die Agentur für Arbeit Limburg-Wetzlar und das Finanzamt Limburg-Weilburg ihren Sitz in Limburg. Der TÜV Hessen ist mit einem Kundenzentrum in Limburg vertreten.

Gegenüber dem ICE-Bahnhof ist eines von sieben hessischen Ämtern für Bodenmanagement (AfB) angesiedelt, dessen Zuständigkeitsbereich sich mit seiner Außenstelle Hofheim und den Anlaufstellen Bad Homburg, Eltville, Frankfurt, Wiesbaden, Usingen und Bad Schwalbach über den Großteil des Südwestens Hessens erstreckt.

Neben Grundschulen verfügt Limburg über drei Haupt- und Realschulen sowie fünf weiterführende Schulen, die u. a. als Abschluss die Allgemeine Hochschulreife (Abitur) anbieten. Die heutige Struktur der Haupt- und Realschulen beruht weitgehend auf einem Beschluss der Stadtverordneten von 1966, der die Kernstadt in drei Schulbezirke für die Südstadt (Johann-Wolfgang-von-Goethe-Schule), die Weststadt (Theodor-Heuss-Schule) und die Alt- sowie die Brückenvorstadt (Leo-Sternberg-Schule) einteilte.

Seit 1895 besteht die Marienschule, ein privates Gymnasium, dessen Träger die Sankt Hildegard Schulgesellschaft ist. Einziger Gesellschafter ist das Bistum Limburg. Die Schule wurde ursprünglich als reines Mädchengymnasium gegründet, seit 2011 werden an der Schule neben Mädchen jedoch auch Jungen monoedukativ unterrichtet. Neben allgemeiner wird dort auch berufliche Bildung für sozialpädagogische und sozialpflegerische Berufe angeboten. 2003 wurde die Marienschule von der Kultusministerin ausgezeichnet als erfolgreichste Schule im Landeswettbewerb Mathematik der Jahrgangsstufe 8 von 1999 bis 2003.

Die seit den 1960er Jahren in einem Neubau auf dem Schafsberg untergebrachte Tilemannschule ist ein Gymnasium mit den Schwerpunkten Sprache, Sport und Musik. Sie ist seit den 1950er Jahren nach dem Limburger Stadtschreiber Tilemann Elhen von Wolfhagen benannt. Wie die Marienschule besteht sie seit Ende des 19. Jahrhunderts.

Einzige Realschule außerhalb der Kernstadt ist die "Schule am Eschilishov", die bis zum Schuljahr 2012/2013 noch über einen Hauptschulzweig verfügte. Gegründet wurde sie 1908. Im Grundschulzweig wurde 1999 mit der Hochbegabtenförderung begonnen.

Die Peter-Paul-Cahensly-Schule, benannt nach dem Limburger Kaufmann und Ehrenbürger, ist berufliches Gymnasium und Fachschule mit den Schwerpunkten Wirtschaft und Bautechnik. Sie wurde 1910 als erste kaufmännische Fortbildungsschule der Stadt Limburg gegründet und bezog 1966 das ehemalige Gebäude des Gymnasiums am Fuße des Schafsbergs. Die PPC-Schule hat heute ihren Standort im Süden Limburgs, in der Zeppelinstraße. 2004 hat sie vom hessischen Kultusministerium das Gütesiegel für Schulen, die besonders hochbegabte Schüler fördern, zuerkannt bekommen.

Die "Astrid-Lindgren-Schule" bildet Schüler mit geistigen und körperlichen Beeinträchtigungen aus, die "Albert-Schweitzer-Schule" Schüler mit Lernproblemen und sozialen und emotionalen Entwicklungsstörungen. Beide Schulen beruhen auf der Sonderschule, die zunächst mit einigen Klassen an die Wilhelmitenschule angeschlossen war und 1964 deren vorheriges Schulgebäude vollständig übernahm.

Die "Johann-Wolfgang-von-Goethe-Schule" trägt diesen Namen seit 1967. Sie ging aus der ehemaligen „Volksschule III“ und der städtischen Realschule an der Hospitalstraße hervor. Der Altbau der Goetheschule wurde 1954 zunächst für die Volksschule allein fertiggestellt. 1965 wurde ein Erweiterungsbau begonnen. Im September 1967 zogen die Schüler der Realschule in das neue Gebäude.

Eine berufliche Schule mit verschiedenen Fachrichtungen der Technik ist seit 1960 die Friedrich-Dessauer-Schule. An dieser seit 2012 selbstständigen Schule kann die Allgemeine Fachhochschulreife erworben werden, auch in den Schulformen Wirtschaft und Wirtschaftsinformatik. Im Rahmen einer Kooperation mit der Peter-Paul-Cahensly-Schule stellt die Friedrich-Dessauer-Schule ihre Fachräume und Lehrwerkstätten für deren gymnasialen Schwerpunkt Bautechnik zur Verfügung.

Die "Theodor-Heuss-Schule" ist Grund- und Mittelstufenschule und geht auf die einstige Wilhelmitenschule zurück. 1966 erhielt sie ihren heutigen Namen und ihr Schulgebäude am Schafsberg.

1967 wurde die "Lahntalschule Limburg" (heute: "Leo-Sternberg-Schule") als Mittelpunktschule für die Alt- und Brückenvorstadt sowie die damals noch eigenständige Gemeinde Dietkirchen gegründet. Sie ist Grund-, Haupt- und Realschule.

Die Adolf-Reichwein-Schule wurde 1966 als Kreisberufsschule gegründet und erhielt ihren Namen ein Jahr später. Nach stetiger Erweiterung umfasst sie mittlerweile neben Fachschulen für Wirtschaft und Soziales beispielsweise auch Fachoberschulen für Agrarwirtschaft, Gesundheits- und Sozialwesen und ein berufliches Gymnasium mit den Schwerpunkten Ernährung, Gesundheit und Pädagogik. Mehrfach ausgezeichnet wurde die Adolf-Reichwein-Schule für ihre Internetpräsenz.

Seit dem Wintersemester 2016/2017 betreibt die Technische Hochschule Mittelhessen eine Außenstelle in Limburg. Angeboten werden u. a. duale Studiengänge in BWL, Architektur, Elektrotechnik, Maschinenbau und Technische Informatik.


Das über der Stadt auf dem Schafsberg gelegene "St. Vincenz-Krankenhaus" gilt mit 16 Fachabteilungen und über 500 Betten als ein Schwerpunktkrankenhaus. Es geht zurück auf ein 1850 aus einer Stiftung geschaffenes und von Vinzentinerinnen betriebenes Hospital am Lahnufer. Von 1950 an zog das Krankenhaus mit damals rund 100 Betten nach und nach in ein Gebäude auf dem Schafsberg, das zunächst als Jugendheim hatte dienen sollen und dann zum Krankenhaus erweitert wurde. 1958 folgte der komplette Abriss des Baus, 1959 die Inbetriebnahme des neuen Krankenhauses und bis 1972 die Errichtung der meisten übrigen Gebäude. 2013 wurden in der Klinik erstmals über 20.000 Patienten behandelt und knapp 1000 Kinder geboren. Auch ist das St. Vincenz-Krankenhaus ein akademisches Lehrkrankenhaus der Justus-Liebig-Universität Gießen in dem Projekte, wie z. B. das Therapiekonzept „Joint Care“ in der Orthopädie, welches neben Berlin einmalig in Deutschland ist, durchgeführt werden.

Die gemeinnützige Krankenhausgesellschaft St. Vincenz ist mit rund 1100 Mitarbeitern der größte Arbeitgeber der Region. Mit dem Logistik- und Dienstleistungszentrum wurde 2013 im ICE-Gebiet Hessens modernste Zentralapotheke in Betrieb genommen, von der aus 24 umliegende Kliniken mit Medikamenten versorgt werden.

Der Kreisverband Limburg e. V. des Deutschen Roten Kreuzes (DRK) untergliedert sich in die Ortsvereine Bad Camberg, Elz, Frickhofen, Hünfelden, Limburg und Niederbrechen. 2012 wurde dem DRK ein Verkehrskreisel an der Limburger Kapellenstraße gewidmet.

Die evangelische Kirche bietet mit der "Jugendfreizeitstätte Limburg" (JFS) einen betreuten Treffpunkt für Jugendliche. Mit Kicker, Internet-Café und zahlreichen Veranstaltungen ist die Einrichtung nicht nur kirchlich geprägt.

Das "Mütterzentrum Limburg" in der Hospitalstraße ist eine familiäre Begegnungsstätte für Menschen mit oder ohne Kinder. Der Verein wird von der Stadt Limburg und dem Land Hessen unterstützt und bietet unter anderem einen Elternservice, der Kinderbetreuung vermittelt, ein vielfältiges Kursangebot für Kinder und Erwachsene, einen Minikindergarten und ein Café.

Die Stadt Limburg nimmt am Projekt Notinsel teil, bei dem geeignete Geschäfte sich selbst verpflichten, Kindern in Notsituationen eine Zuflucht zu bieten und Hilfe zu leisten.


Die Stadthalle Limburg, seit 1996 nach dem Altbürgermeister und Ehrenbürger der Stadt Limburg an der Lahn, Josef Kohlmaier, benannt, wird für Shows, Tagungen, Kongresse und ähnliche Veranstaltungen genutzt. Mit der gesamten Region zwischen Westerwald und Taunus verfügt sie über einen Einzugsbereich von fast 300.000 Menschen. Im ganzen Umkreis gibt es keine ähnlich frequentierte Mehrzweckhalle. Sie zählt jährlich über 150.000 Besucher.

Die Kleinkunstbühne „Thing“, gegründet vor über 25 Jahren, wechselte nach einiger Zeit aus ihrem anfänglichen Domizil im Stadtteil Staffel in die "Josef-Kohlmaier-Halle", wo sich heute ihre Bühne in den Clubräumen befindet. Die Bühne wird von einem selbstständig agierenden Verein getragen. Zum Programm gehören Chanson, Kabarett, Literatur und Jazz sowie Folk, Rock und Auftritte von Liedermachern. Dabei liegt ein Schwerpunkt auf der Förderung junger Künstler. Pro Monat werden zwei oder drei Veranstaltungen angeboten.

Das Engagement von „Thing“ wurde am 6. Dezember 2003 durch die Verleihung des „Kulturpreises Mittelhessen“ gewürdigt.

Mit den Limburger Domsingknaben verfügt Limburg seit 1967 über einen Knabenchor, dessen Träger das Bistum Limburg ist. Von ehemaligen Sängern dieses Chores wurde 1999 das Männervokalensemble Camerata Musica Limburg gegründet. Die Mitglieder dieses Ensembles führen damit die erfolgreiche Tradition der Männerkammerchöre in Limburg fort.

Ein Veranstaltungsort mit dem Status eines Kulturdenkmals, insbesondere für Freiluftkonzerte in den Sommermonaten, ist seit 2012 die Villa Scheid. Mit parkähnlichem Anwesen ist dieser Veranstaltungsort vollständig in privater Hand. Neben Konzerten gehören zum Programmangebot u. a. auch Kabarett, ein Englisches Gartenfest und Kunstausstellungen.

Einer der ältesten Limburger Sportvereine mit überregionaler Bedeutung ist der Limburger Club für Wassersport 1895/1907 e. V. (LCW). Die Jahreszahl 1895 in seinem Namen gibt das Gründungsdatum des Limburger Rudervereins (LRV) wieder, aus dem der heutige Verein hervorging. Heute ist er ein Trainingsstützpunkt des Deutschen Ruderverbandes (DRV).

1923 von Mitgliedern des VfR 07 Limburg gegründet und mit Unterbrechungen in den beiden Bundesligen vertreten ist der Limburger Hockey Club (LHC). Er ist heute ein Hockey-Leistungszentrum und Olympiastützpunkt. Der Verein gewann mehrfach – im Jugendbereich bis heute in regelmäßigen Abständen – deutsche Meistertitel und brachte eine Reihe von Nationalspielern und -trainern hervor.

Zu den größten Leistungen in der Vereinsgeschichte des VfR 19 Limburg zählt der Aufstieg der Abteilung Basketball in die 2. Bundesliga in der Saison 1997/1998, auf den jedoch in der folgenden Saison wieder der Abstieg folgte. Der Verein besteht neben dieser und der Hauptabteilung Fußball noch aus einigen weiteren Abteilungen.

Schachsport wird in Limburg bereits seit den 1930er Jahren organisiert betrieben. Der Schachverein Lahn Limburg wurde am 17. November 1930 gegründet und konnte in seiner langjährigen Geschichte mehrfach den Aufstieg in die höchste Spielklasse Hessens (Oberliga Hessen bzw. später Hessenliga) feiern. Zuletzt spielte man in der Saison 2005/2006 in Hessens höchster Spielklasse. In den 1980er Jahren konnte sich mit dem Schachclub Königsflügel Lindenholzhausen ein weiterer Schachverein in Limburg etablieren.

In der rheinland-pfälzischen Nachbarstadt Diez befindet sich mit der Eishalle das Trainingszentrum des Eishockeyvereins mit dem Beinamen Rookie Rockets der EGDL (Eissport Gemeinschaft Diez-Limburg). Der in der Hessenliga und Rheinland-Pfalz-Liga erfolgreich spielende Verein wurde 2004 gegründet und zählt somit zu den neueren in Limburg. Vorläufervereine waren in den 1980er und 1990er Jahren der Eishockeyclub Diez-Limburg (ECDL) sowie die Limburger EG, die in der Saison 1997/1998 in der 1. Eishockey-Liga, der damals zweithöchsten deutschen Spielklasse, mitwirkte.

Die Kernstadt verfügt über ein Freibad, das 1961 eröffnete, heute "Parkbad" genannte Schwimmbad mit einem 50-Meter-Wettkampfbecken am rechten Ufer der Lahn. Das ehemals städtische Hallenbad im Stadtteil Offheim wird als eines von wenigen Schwimmbädern in Deutschland eigenständig von einem gemeinnützigen Verein getragen. Zudem ist die Stadt Limburg gemeinsam mit der Stadt Diez, dem Landkreis Limburg-Weilburg und dem Rhein-Lahn-Kreis Betreiberin des in Diez befindlichen "Oranienbades". Ein weiteres ehemals städtisches Hallenbad im Stadtteil Linter wurde zum 31. Dezember 2003 geschlossen und zu einer Kegelbahn umgebaut.

Der Limburger Dialekt, den man unter verschiedenen Ausprägungen des im "„Nassauer Land“" vorherrschenden „Platts“ ausmachen kann, zählt zu den mittelhessischen Dialekten. In reiner Form gesprochen wird er allgemein nur noch von der älteren Generation, die in Limburg oder den nächsten umliegenden Gemeinden aufgewachsen ist.

Einzelne Silben haben im Lauf der Jahre geringfügige Änderungen erfahren, teils hin zum Hochdeutschen, weshalb es mitunter schwer ist, den ureinstigen Limburger Dialekt von regional abweichenden zu unterscheiden. Jedoch ist auch bei jüngeren Einwohnern Limburgs, in deren Elternhaus der gesprochene Dialekt vom Hochdeutschen beeinflusst war, durch bestimmte Laute überwiegend eine typische Betonung des Gesprochenen festzustellen. Für den geneigten Hörer ist eine präzise Bestimmung der Limburger Herkunft des Gegenübers somit durchaus möglich.

Der Trend zum Sprechen mit Dialekt ist in der Limburger Region wie in vielen anderen auch rückläufig, was u. a. der Bevölkerungswanderung und dem Einfluss moderner Medien geschuldet ist. Aus diesem Grund erfahren mundartbezogene Veranstaltungen regen Zuspruch als ein Versuch, den regionalen Dialekt für kommende Generationen zu erhalten.

Ein traditionelles Limburger Gericht ist der "Limburger Säcker". Hierbei handelt es sich um ein mit Senf gewürztes und paniertes Kotelett, gefüllt mit Sauerkraut, Dörrfleisch und Essiggurken sowie der Beilage Bratkartoffeln oder Brot. Der Name dieses Gerichts geht zurück auf die Einwohner der Limburger Altstadt, die im Mittelalter am Haus Kleine Rütsche 4, der engsten Stelle des alten Handelsweges von Köln nach Frankfurt, die vorbeikommenden und mit Säcken zu breit beladenen Wagen neu beluden.

Erstmals von einem Limburger Gastronom während der Amtszeit des Bischofs Franz Kamphaus kreiert wurden die "Limburger Bischofsknödel". Dies sind mit Blut- und Leberwurst gefüllte Kartoffelklöße, serviert mit Sauerkraut und Specksoße. Überregional bekannt wurde das Traditionsgericht durch die verstärkte Anpreisung mehrerer Gaststätten im Jahr 2013, als die Vorgänge um die Kostenentwicklung des Diözesanen Zentrums St. Nikolaus unter Kamphaus' Nachfolger Franz-Peter Tebartz-van Elst bekannt wurden und die Zahl der Touristen sprunghaft anstieg.

Eine jährlich im Frühjahr wiederkehrende Veranstaltung mit überregionaler Bedeutung ist die Messe „Whisky-Fair“.

Am letzten Wochenende im Juni lockt das Limburger Altstadtfest tausende Besucher in die Stadt. Ebenfalls im Sommer präsentieren Winzer bei den "Rheingauer Weintagen" in der Innenstadt ihre Produkte.

Seit 2007 veranstaltet die Stadt Limburg mit Unterstützung heimischer Vereine und Unternehmen den "Sommernachtslauf" im Zweijahresrhythmus. Bei diesem Stadtlauf mit in der Regel über 1000 Teilnehmern werden verschiedene Disziplinen für Einzelläufer, Familien und Unternehmen angeboten.

Mit den "Summer Games" hat sich seit 2005 ein Stadtfest etabliert, das jährlich in der dritten Augustwoche veranstaltet wird. Durch die Unterstützung zahlreicher Unternehmen und Vereine entwickelte es sich zu einem großen Stadtfest in der Region mit mittlerweile über 50.000 Besuchern pro Jahr. 2010 jährte sich am ersten Sonntag im September zum 40. Mal der über die Region hinaus bekannte Limburger Flohmarkt, der größte Stadtflohmarkt Hessens.

Mit dem "Limburger Oktoberfest" findet alljährlich eines der größten Volksfeste Mittelhessens über mehrere Tage statt und orientiert sich seit 2011 verstärkt am Münchner Original. Vom 27. November bis zum 30. Dezember öffnet der Weihnachtsmarkt.
In Limburg gibt es vier Museen. Diese sind:


Aufgrund des Zweiten Weltkrieges blieb nur wenigen Städten wie Limburg das gesamte Ensemble mittelalterlicher Bebauung nahezu unversehrt erhalten. Deshalb steht heute der ehemals ummauerte Stadtkern zwischen St.-Georg-Dom, Grabenstraße und der 600 Jahre alten Lahnbrücke als Gesamtanlage Altstadt und Frankfurter Vorstadt unter Denkmalschutz.



In der Altstadt stehen zahlreiche Fachwerkhäuser, erbaut vom 13. bis zum 19. Jahrhundert. Seit Beginn der Altstadtsanierung 1972 werden diese behutsam restauriert. Eine Besonderheit sind die mittelalterlichen Hallenhäuser, die im Erdgeschoss über eine große Halle verfügen. Zu den bekanntesten Häusern Limburgs gehören:

Auf dem Limburger Hauptfriedhof befindet sich das zentrale deutsche Denkmal, das den in der Schlacht von Stalingrad gefallenen und in anschließender Gefangenschaft verstorbenen Soldaten gewidmet ist. Es wurde von einem ehemaligen Stalingradkämpfer entworfen und im Jahr 1964 errichtet.








</doc>
<doc id="9005" url="https://de.wikipedia.org/wiki?curid=9005" title="LP">
LP

LP steht als Abkürzung für:

LP als Unterscheidungszeichen auf Kfz-Kennzeichen:
lp steht für:
formula_1 steht für:
formula_2 oder formula_1 steht für:
Siehe auch:


</doc>
<doc id="9006" url="https://de.wikipedia.org/wiki?curid=9006" title="Schallplatte">
Schallplatte

Eine Schallplatte ist eine in der Regel kreisförmige und meistens schwarze Scheibe mit einem Mittelloch, deren beidseitige Rillen als analoge Tonträger für Schallsignale dienen.

Umgangssprachlich wird die Schallplatte auch als "Platte" oder mit den aus der Jugendsprache stammenden Begriffen "Scheibe" beziehungsweise "Vinyl" bezeichnet. Das Wort "Schallplatte" wurde bereits zur Zeit der Grammophon-Ära geprägt. Der seit dem Produktionsende der Grammophonplatten wesentlich geläufigere Begriff "Schellackplatte" grenzt diesen älteren Tonträger deutlich von der späteren Schallplatte aus Polyvinylchlorid ab.

Die Signale sind in einer vom Rand der Platte zum Mittelpunkt verlaufenden, spiralförmigen Rille gespeichert, deren Flanken die Schallschwingung des gespeicherten Signals abbilden. Bei der Wiedergabe wird die Abtastspitze eines Tonabnehmers entsprechend ausgelenkt. Die Rückverwandlung in hörbare Schallsignale kann rein mechanisch über eine Membran und einen Schalltrichter oder – bei heute üblichen Plattenspielern – auf elektromechanischem Weg mit anschließender elektronischer Verstärkung erfolgen.

Im März 1857 meldete der Franzose Édouard-Léon Scott de Martinville den Phonautograph als Patent (Nummer 17,897/31,470) an. Ihm gelang es 1860, das französische Kinderlied "Au clair de la lune" mit Hilfe eines großen Trichters einzufangen und mit einer Membran, die die Schwingungen auf eine Schweineborste übertrug, auf eine rußgeschwärzte Walze zu kratzen. Allerdings war sein Phonautograph nicht dazu gedacht, den aufgezeichneten Schall auch wiederzugeben. Das schaffte erst die 1877 von John Kruesi gebaute Erfindung des Amerikaners Thomas Alva Edison, der mit dem ebenfalls patentierten Phonographen weltberühmt wurde. Auch dessen erste Aufzeichnung war ein Kinderlied "(Mary had a little lamb…)". Die Töne wurden zunächst in eine Zinnfolie geritzt, später auf einer Phonographenwalze mit wendelförmiger Tonspur in Höhenschrift gespeichert, wobei das Prinzip der Amplitudenauslenkung auch hier unmittelbar akustisch (Membran/Trichter) genutzt wurde. Wichtige theoretische Grundlagen entwickelte auch der Franzose Charles Cros, der seine Arbeiten ebenfalls im Jahr 1877 bei der Naturwissenschaftlichen Akademie in Paris einreichte.

Bereits im Jahre 1880 machte der US-amerikanische Physiker Charles Sumner Tainter (Columbia Graphophone Company) die Entdeckung, dass viele technische Nachteile der Edisonischen Walzen (umständliche Handhabung und aufwändige Vervielfältigung) beseitigt werden könnten, wenn man die Tonspur spiralförmig in die Oberfläche einer flachen, runden Scheibe eingraviert. Tainter entwickelte den Prototyp eines entsprechenden Aufnahmeapparates und stellte einige bespielte Wachs­platten her, gab die Versuche aber infolge technischer Probleme nach kurzer Zeit wieder auf. Tainters Wachsplatten befinden sich heute im Smithsonian Institute in Washington. Sie gelten als die ersten Schallplatten der Welt.
Unabhängig von Tainter, der seine Ideen nicht publiziert hatte, gelangte im Jahre 1887 der deutsch-amerikanische Erfinder und Industrielle Emil Berliner bei seinen Versuchen mit dem Edison-Phonographen zu einem ähnlichen Verbesserungskonzept. Er hatte sich mehrere Jahre lang mit dem Edison-Phonographen befasst und früher als Edison erkannt, dass die Zukunft der Tonaufzeichnung in erster Linie im Unterhaltungsbereich lag. Als Geschäftsmann sah auch er in der umständlichen – und damit teuren – Vervielfältigung der Walzen den entscheidenden Schwachpunkt des Phonographen und verwendete seine Zeit und Mühe vorrangig auf die Lösung dieses Problems.

1887 gelang Berliner der entscheidende Durchbruch. Er konstruierte ein Gerät, das die Schallwellen nicht wie bei Edisons Höhenschrift-Phonographen in vertikaler, durch Auf-und-ab-Bewegung des Schneidstichels entstehender Modulation speicherte, sondern die Rille horizontal auslenkte; die mechanischen Schwingungen ließ er eine Stahlnadel schneckenförmig in eine dick mit Ruß überzogene Glasplatte einritzen. Nach chemischer Härtung des Rußes war er in der Lage, auf galvanoplastischem Wege ein Zink-Positiv und von diesem ein Negativ der Platte anzufertigen, das als Stempel zur Pressung beliebig vieler Positive genutzt werden konnte – die Schallplatte war erfunden. Am 4. Mai 1887 wurde Berliner für seine Idee das US-Patent No. 372,786 erteilt; kurioserweise fast zeitgleich mit der Erteilung des Patents für Edisons Phonographen.

Die älteste bis heute erhaltene Berliner-Schallplatte ist ein am 25. Oktober 1887 von Berliner selbst angefertigtes Zink-Positiv. Der Öffentlichkeit wurde das neue Aufzeichnungsverfahren erstmals in einem Bericht der Zeitschrift "Electrical World" vom 12. November des gleichen Jahres vorgestellt; die frühesten zu Demonstrationszwecken angefertigten Zinkplatten hatten einen Durchmesser von 28 cm und bei etwa 30 min eine Spieldauer von vier Minuten.

In den folgenden Monaten entwickelte Berliner in Zusammenarbeit mit dem Techniker Werner Suess sein Verfahren weiter, indem er das rußbeschichtete Glas durch eine mit Wachs überzogene Zink- oder Kupferplatte ersetzte. Nach der Gravur der Schallrille in die Wachsschicht wurde die Platte einem Säurebad ausgesetzt, das die noch mit Wachs bedeckten Teile der Platte nicht angriff, die freigelegten Rillen aber in das Metall einätzte, so dass nach Entfernung des Wachses eine haltbare metallene Urplatte entstand, die zur Herstellung der Pressmatrizen verwendet werden konnte. Am 16. Mai 1888 präsentierte Berliner ein erstes funktionsfähiges Gerät den Wissenschaftlern des "Franklin Institute" in Philadelphia. Der zeittypischen Vorliebe für Gräzismen folgend nannte er es "Grammophon" (sinngemäß: „geschriebener Laut“).

Im August 1888 begann er erstmals, die eigentlich von Anfang an vorgesehene Vervielfältigung seiner Zinkplatten durch Pressen der Negative in weiches Material zu erproben. Zunächst verwendete er als Pressmasse Zelluloid, das er unmittelbar vom Erfinder dieses Werkstoffs, John W. Hyatt, bezog und das sich bald als technisch ungeeignet erwies. Von diesen als "Hyatt Disks" bekannten ersten experimentellen Zelluloid­platten sind nur sehr wenige Exemplare erhalten geblieben.

Im Juli 1889 kam Berliner aufgrund materialkundlicher Versuche zu dem Schluss, dass vulkanisiertes Hartgummi als Pressmaterial die günstigsten Eigenschaften aufweise, und erachtete seine Erfindung für ausgereift genug, um den Beginn der Serienproduktion einzuleiten.

Emil Berliner ging auf Investorensuche, stieß aber bei der US-amerikanischen Industrie auf wenig Resonanz. Daher reiste er im August 1889 nach Deutschland, um das Grammophon potentiellen Interessenten vorzuführen. Am 26. November 1889 demonstrierte er das Gerät den staunenden Experten der Berliner "Elektrotechnischen Gesellschaft", die ihn sofort als Mitglied aufnahm.

Von diesem Erfolg ermutigt entschloss er sich Ende 1889, die Serienfertigung der Platten zunächst auf eigene Rechnung in die Wege zu leiten. Die renommierte Spielwarenfabrik Kämmer & Reinhardt (andere Quellen: „Kämmerer & Reinhardt“) in Waltershausen (Thüringen), fertigte für ihn – vermutlich ab Juli 1890 – sehr einfache Grammophone mit Handkurbelantrieb und entwickelte auch eine sprechende Puppe mit Miniatur-Grammophon im Rumpf. Die passenden Platten wurden bei zwei deutschen Firmen in Auftrag gegeben. Einer der beiden Hersteller war die "Rheinische Gummi- und Celluloidfabrik" (später "Schildkröt") in Mannheim. Ein weiterer Hersteller war die Grammophon-Fabrik Kämmer & Co, Firmenkürzel „G-F-K-C“. Hergestellt wurden Platten mit 8 cm Durchmesser für die Sprechpuppe und 12,5 cm Durchmesser für das Grammophon; zumindest teilweise kamen dabei wohl in den USA entstandene Matrizen zur Verwendung. Die Pressungen waren in Gummi-, Zelluloid- und Zink-Ausführung erhältlich, wobei nicht bekannt ist, inwieweit Zelluloid und Gummi zueinander in zeitlicher Abfolge standen; die Zinkplatten wurden offenbar gegen Aufpreis verkauft.

Diese ersten Serienschallplatten der Welt waren von so minderwertiger Klangqualität, dass Zettel mit dem vollständigen Text der jeweiligen Aufnahme auf die Plattenrückseite geklebt wurden, damit der Käufer den Inhalt der Platte nachvollziehen konnte. Insgesamt wurden 1889/90 in Deutschland etwa 25.000 Platten gepresst, von denen heute weltweit nur noch sehr wenige Exemplare bekannt sind. Die einzige erhaltene Berliner-Sprechpuppe befindet sich im Heimatmuseum Schloss Tenneberg in Waltershausen. Kurzzeitig wurden die deutschen Berliner-Produkte auch nach England exportiert; das Geschäft mit dem unausgereiften System erwies sich aber als wenig lukrativ, weshalb Berliner 1891 die Fertigung einstellen ließ und in die USA zurückkehrte.

Am 23. April 1889 gründete er die "American Gramophone Co." die die Verwertung seiner Erfindung übernehmen sollte, aber nach kurzer Zeit zusammenbrach. Die folgenden zwei Jahre verbrachte Berliner damit, das Grammophon technisch zu verbessern. Er ließ von einem New Yorker Uhrmacher einen Federantrieb entwickeln, der sich allerdings als nicht praxistauglich erwies, und engagierte einen Techniker namens Edward L. Wilson, der für ihn ein Grammophon mit Münzmechanik konstruierte.

Erst im April 1893 wagte Berliner zusammen mit den Brüdern Fred und Will Gaisberg, die zuvor schon bei der Columbia Records tätig gewesen waren, eine neue Firmengründung. Es entstand die "United States Gramophone Company" mit Sitz in Washington (1205 G Street NW), die die Erfindung kommerziell verwerten sollte und an die er seine Patente abtreten musste. Die Firma produzierte einige wenige Grammophone und Schallplatten aus "Vulcanite" beziehungsweise Hartgummi, geriet allerdings bald in finanzielle Schwierigkeiten.

1895 gelang es Berliner, eine Gruppe von Investoren aus Philadelphia für seine Erfindung zu begeistern. Es kam zur Gründung der "Berliner Gramophone Company", deren Anteile allerdings nur zum kleineren Teil Berliner selbst gehörten. Die "United States Gramophone Co." bestand allerdings parallel dazu als Inhaberin der Patente weiter. Das neue Unternehmen eröffnete in Baltimore (109 North Charles Street) eine Fabrik nebst "show room" und begann mit der Fertigung von Geräten und Tonträgern.

Der Durchmesser der Platten wurde 1894 auf zehn Zoll (25,4 cm) festgelegt. Bis zum Herbst 1894 verließen etwa 1000 Grammophone und 25.000 Platten die Fabrik. Berliner veröffentlichte die erste gedruckte Bestellliste der verfügbaren Aufnahmen. Das Unternehmen bot neben den handbetriebenen Grammophonen auch zwei Luxusmodelle mit Elektromotoren an. 1895 änderte man den Plattendurchmesser auf 17,5 cm (6,9″); im selben Jahr erhielt Berliner nach langen juristischen Auseinandersetzungen jenes US-Patent für sein Horizontalschrift-Aufnahmeverfahren, dessen Existenz später die jahrzehntelange monopolartige Position der "Victor Talking Machine Company" (später Teil von RCA) auf dem nordamerikanischen Plattenmarkt begründen sollte.

Die Umsätze der Firma waren zunächst bescheiden, da die potentiellen Kunden in Berliners primitivem, handkurbelbetriebenem Grammophon noch immer eher ein Kinderspielzeug als ein ernstzunehmendes Unterhaltungsgerät sahen. Angeregt durch entsprechende Entwicklungen auf dem Phonographen-Sektor beschäftigte sich Berliner erneut mit dem Gedanken, seine Geräte mit Federmotoren auszustatten. Er beauftragte den Mechaniker Eldridge R. Johnson aus Camden City mit der Entwicklung und Serienfertigung eines passenden Federwerk-Motors, dessen Markteinführung 1896 erfolgte und dem Produkt tatsächlich zu einer enormen Umsatzsteigerung verhalf.

Im Oktober 1896 gab Berliner die Verwendung von Hartgummi als Plattenmaterial auf und ersetzte die Substanz durch eine von der "Duranoid Co." Newark, New Jersey, hergestellte Pressmasse, die im Wesentlichen aus Schieferpulver, Baumwollflock und Schellack bestand, das sich beim heißen Pressvorgang kurz verflüssigte, sich auch an die Oberfläche der Scheibe drückte und damit ein strapazierfähiges Material für die Rillen bildete. Das verbesserte die Klangqualität und Haltbarkeit der Platten enorm. Die Schellackplatte war geboren.

Im selben Jahr engagierte Berliner den Werbefachmann Frank Seaman, dessen Aufgabe es sein sollte, den Vertrieb der Berliner-Produkte zu übernehmen. Seaman gründete unter dem Namen "National Gramophone Company" ein eigenes Unternehmen und schloss mit der "Berliner Gramophone Company" einen auf fünfzehn Jahre Laufzeit ausgelegten Vertrag, der ihm die Exklusivrechte am Vertrieb aller Berliner-Erzeugnisse sicherte. Seamans brillante Werbekampagnen machten das Grammophon binnen kürzester Zeit weltweit bekannt und ließen die Verkaufszahlen der Platten und Geräte förmlich explodieren – im Geschäftsjahr 1898 konnte Berliner bereits 713.753 Schellackplatten absetzen. Berliners Erfindung lag nun in den Händen dreier voneinander unabhängiger Firmen. Die "United States Gramophone Company" hielt noch immer die Patente, die "Berliner Gramophone Company" produzierte Platten und Abspielgeräte, die "National Gramophone Company" beherrschte die Vermarktung.
Berliner expandierte durch Gründung ausländischer Tochtergesellschaften, als deren wichtigste im Jahre 1898 die britische "Gramophone Company" entstand. Diese gründete ihrerseits als Tochter-Tochterunternehmen die "Deutsche Grammophon-Gesellschaft" mit Sitz in Hannover, geleitet von Berliners Bruder Joseph.

Das zunächst sehr erfolgreiche unternehmerische Konzept sollte sich für Berliner bald als fatal erweisen, denn Seaman, der als wenig vertrauenswürdiger Charakter galt, war angesichts der enormen Gewinne, die in Berliners Kassen flossen, mit seinem langfristigen Vertrag unzufrieden. Er begann 1898 heimlich mit der Herstellung seiner "Zonophone", die technisch lediglich Plagiate der Berliner-Geräte darstellten. Seaman bot Berliner an, er möge künftig die Zonophone kaufen und unter seinem Namen vertreiben lassen. Berliner lehnte das empört ab, zumal er den Vertrag mit Johnson nicht gefährden wollte und Seaman allgemein misstraute; überdies waren die Zonophone von schlechterer Qualität als Berliners Erzeugnisse.

Seaman wertete die Ablehnung Berliners als Aufhebung seines Vertrags, benannte seine Firma in "United Talking Machine Company" (UTMC) um und begann, auf eigene Rechnung Platten und "Zonophone" zu fertigen. Berliner verklagte Seaman wegen Vertragsbruchs. Daraufhin nahm dieser vertrauliche Verhandlungen mit dem Walzenhersteller Columbia auf, der die Patente an den Erfindungen von Chichester Bell und Charles Sumner Tainter hielt. 1899 erhob Columbia auf Seamans Betreiben Klage gegen Berliner mit der Begründung, die Berliner-Patente von 1887 wären unter Verletzung älterer Bell- und Tainter-Patente erteilt worden und damit nichtig.

Die komplizierten juristischen Auseinandersetzungen, in die auch Eldridge Johnson verwickelt war und in denen der von Columbia engagierte Star-Anwalt Philipp Mauro eine entscheidende Rolle spielte, endeten für Seaman erfolgreich.

Das führte dazu, dass Berliner ab dem 25. Juni 1900 die Verwendung des Namens "Gramophone" in den USA verboten war und er auch sonst für sein amerikanisches Unternehmen keine Zukunft mehr sah. Er löste seine Firmen auf, verkaufte alle seine US-amerikanischen Patente an Eldridge Johnson, mit dem er sich wieder versöhnt hatte, und siedelte nach Montreal über. In den folgenden Jahrzehnten leitete er sehr erfolgreich die kanadische Niederlassung seines Imperiums und nahm auch maßgeblichen Einfluss auf die Entwicklung der aus seiner britischen Filiale entstandenen "Gramophone Company". Nebenbei beschäftigte er sich mit aeronautischen Problemen und war an der Entwicklung eines der ersten brauchbaren Hubschrauber beteiligt.

Der große kommerzielle Erfolg der Berliner-Schallplatte und der relativ schlechte patentrechtliche Schutz der Erfindung ermunterten ab etwa 1900 vor allem in Europa zahlreiche Unternehmer, die Produktion eigener Schallplatten und Abspielgeräte aufzunehmen. Binnen weniger Jahre entwickelte sich so ein äußerst innovativer, schnelllebiger Industriezweig, als dessen Zentren London, Paris, Hannover, Berlin und Wien galten.

Zum damaligen Zeitpunkt konkurrierten noch einige Systeme, die untereinander oft nicht kompatibel waren. Vor dem Ersten Weltkrieg war beispielsweise die französische Firma Pathé sehr dominant am Weltmarkt. Ihre Platten konnten nur mit einer abgerundeten Saphirnadel bei 90–100 min abgespielt werden, liefen immer von innen nach außen und waren mit der sog. Tiefenschrift aufgenommen worden. Da Pathé zum Zeitpunkt des Aufkommens der Schellackplatten bereits über einen großen Fundus älterer Walzenaufnahmen verfügte, deren Grundlage ebenfalls die Tiefenschrift war, konnte die Firma mit einem für die damalige Zeit unerwartet großen Sortiment an Musiktiteln ins Geschäft einsteigen. Die mit Tiefenschrift aufgenommenen Platten durften niemals mit einer Grammophonnadel abgespielt werden, da diese die Platte sofort zerstörten. Für diese Platten gab es das Pathéphone oder als Adapter für Grammophone eine Pathé-Schalldose, welche jedoch nur mit Fachkenntnis installiert werden durfte.

Eine weitere frühe Variante stellte die nordamerikanische Edison-Diamond-Disc dar, die der Öffentlichkeit 1911 präsentiert wurde. Auch diese Schallplatten waren mit Tiefenschrift bespielt und konnten ebenfalls nur mit speziellen Plattenspielern abgespielt werden. Die etwa fünf Millimeter dicken Tonträger bestanden nicht aus Schellack, sondern einer Mischung aus Phenol, Formaldehyd, Holzmehl und Lösungsmittel, die mit einer Lackschicht aus phenolhaltigem Kunstharz überzogen war. Diese Platten sind in Europa äußerst selten.
Letztendlich setzte sich die Schellackplatte mit einer Geschwindigkeit von 78 min durch, für die es spezielle Nadeln zu kaufen gab, die nach verschiedenen damaligen Empfehlungen nach jeder Platte ausgewechselt werden sollten. Preiswerte Grammophone einfacher Bauart kamen in vielfältigen Formen auf den Markt und ließen die Schallplatte zu einem auch für die Unterschicht erschwinglichen Unterhaltungsmedium werden. Bis 1914 entstanden allein in Deutschland etwa 500 konkurrierende Schallplattenmarken. Durch technische Verbesserungen konnte die Klangqualität stetig steigen. Allmählich begannen sich auch große Musiker wie Enrico Caruso, Nellie Melba und Hermann Jadlowker für das Medium Schallplatte zu interessieren und verhalfen mit ihren Einspielungen den Plattenkonzernen zu beträchtlichen Gewinnen.

Besonders das zunächst eigenständige, später vom Konzern des schwedischen Schallplattenproduzenten Carl Lindström übernommene Label "Odeon" tat sich mit technischen Innovationen hervor und brachte die ersten großformatigen Platten (25, 27 und 30 cm Durchmesser) sowie die ersten doppelseitigen Pressungen auf den Markt.

Der Erste Weltkrieg ließ die Schallplattenproduktion weltweit stark zurückgehen, was primär durch den vorübergehenden Zusammenbruch des internationalen Handelsnetzes für Rohschellack bedingt war. Nach Kriegsende erholte sich die Schallplattenindustrie zunächst nur langsam. Die Wirtschaftskrisen der 1920er Jahre und auch die Entstehung der ersten Rundfunksender beeinträchtigten die Plattenabsätze weltweit erheblich. Ins Gegenteil verkehrte sich diese Entwicklung allerdings, als ab 1925 mehrere US-amerikanische, holländische und deutsche Firmen annähernd gleichzeitig elektrische Aufnahmeverfahren präsentierten, die die alten akustisch-mechanischen Aufnahmeapparate binnen kurzer Zeit völlig verdrängten, die Kosten der Schallplattenaufnahme dramatisch reduzierten und die Klangqualität enorm verbesserten. Die Einführung der elektrischen Aufnahme ließ in Deutschland erneut eine große Zahl kurzlebiger kleiner Plattenfirmen entstehen, die technisch und musikalisch oft sehr experimentierfreudig waren. Das 1928 von dem Erfinderkollektiv Tri Ergon entwickelte Lichttonverfahren erlaubte erstmals das Schneiden und Nachbearbeiten von Aufnahmen. Erste elektrische Plattenspieler kamen auf den Markt. Die Musikbegeisterung der späten 1920er Jahre garantierte den Schallplattenkonzernen ausgezeichnete Umsätze.

Ein technischer Fortschritt in den frühen 1930er Jahren war die Einführung des Selbstschneidens von Grammophonplatten. Anfangs schnitt man in weiche Folien ohne Nachbehandlung, später in "Decelith"-Rohlinge mit den üblichen 78 min. Die Haltbarkeit der geschnittenen Decelithscheibe wurde durch eine härtende Nachbehandlung der Oberflächenschicht verbessert, die aber ein nachträgliches Einsenden der fertigen Platte an den Hersteller der Rohlinge erforderte. Dieses Aufzeichnungsverfahren war noch bis zur Serienreife des Magnetophons in den frühen 1950er Jahren auch in professionellen Rundfunkstudios in Gebrauch.

Das Jahr 1933 brachte für die deutsche Schallplattenindustrie dramatische Veränderungen: Zahlreiche bis dahin in jüdischem Besitz gewesene Unternehmen wurden in den ersten Jahren der NS-Diktatur enteignet und aufgelöst. Gegen viele prominente Plattenkünstler, wie die Comedian Harmonists, ergingen aus rassischen und politischen Gründen Berufsverbote. Die Einfuhr ausländischer Platten nach Deutschland war kaum noch möglich. Bis 1939 reduzierte sich die Zahl der auf dem deutschen Markt präsenten Schallplattenmarken daher beträchtlich. Da deutsche Plattenfirmen jedoch im Ausland Aufnahmen mit bekannten Künstlern machten, waren diese am Inlandsmarkt durchaus bekannt. Gerade die in den späten 1930er Jahren in Deutschland populär werdende Swing-Musik profitierte von dieser Praxis. Unabhängig von den Maßregelungen der Reichsmusikkammer gegen ausländische Musikströmungen leistete sich beispielsweise Telefunken mit Heinz Wehners „Telefunken-Swing-Orchester“ eine Swing-Kapelle amerikanischer Prägung. In der Ausgabe 12/1937 bezeichnete das amerikanische Jazz-Magazin „Down Beat“ das Telefunken-Swing-Orchester „als beste Band im Nazireich“. Auch internationale Swing-Bands wie die von Teddy Stauffer und Fud Candrix wurden zunächst über ihre Platten bekannt, bevor sie beispielsweise Engagements im damals bekanntesten deutschen Jazz-Club, dem Berliner Delphi Filmpalast, bekamen.

Während des Zweiten Weltkriegs sollte ein groß angelegtes Altplatten-Verwertungssystem sichergestellt werden, tatsächlich brach ab etwa 1943 die deutsche Schallplattenproduktion trotzdem weitgehend zusammen. Nur für den Bedarf von Rundfunk und Lichtspieltheatern wurde bis zum Kriegsende weiter produziert.

Nach 1945 nahmen die Schallplattenfabriken, soweit unzerstört geblieben, ihre Arbeit recht bald wieder auf, wobei zunächst die Schellack-Technik beibehalten wurde. Im Westen Deutschlands entstanden viele neue Plattenmarken, die besonders den neu entstandenen Bedarf nach US-amerikanisch geprägtem Swing und Jazz zu decken versuchten. In der sowjetischen Besatzungszone wurde die Schallplattenfabrikation dagegen als einer der ersten Industriezweige komplett verstaatlicht. Es verblieb als einziger Schallplattenhersteller der VEB Lied der Zeit, später VEB Deutsche Schallplatten, mit den Labels "Amiga", "Eterna", "Litera", "Nova", "Aurora" und "Schola".

In der Bundesrepublik und den meisten anderen westlichen Ländern wurde die Fertigung von Schellackplatten im Juli 1958 aufgegeben. Die DDR vollzog diesen Schritt im Jahr 1961.

Bereits in den Anfängen der Schallplattenherstellung hatte es – etwa in Großbritannien durch "Nicole Records" – erfolglose Versuche gegeben, das teure Naturprodukt Schellack durch preiswertere synthetische Kunststoffe zu ersetzen. Dazu wurden unter anderem Tonträger aus Polyvinylchlorid (PVC) erprobt. Man spricht bei Schallplatten meist vereinfachend von „Vinyl“ statt von PVC.

1930 brachte RCA Victor die erste langspielende Vinylschallplatte heraus, vermarktet als „Program Transcription Discs“. Diese revolutionäre Platte war gedacht für 33⅓ min, hatte einen Durchmesser von 30 cm und war nahezu unzerbrechlich. Roland Gelatt stellte in seinem Buch „The Fabulous Phonograph“ fest, dass RCA Victors frühe Einführung einer Langspielplatte ein kommerzieller Fehler war, vor allem wegen des Mangels an geeigneten Wiedergabegeräten.

Ende der 1930er Jahre wurde in den USA damit begonnen, voraufgezeichnete Radio-Programme und Werbespots in Vinyl zu pressen, da diese beim Versand nicht zerbrachen. Dieser Vorteil führte auch dazu, dass in den USA Schallplatten für Kinder in Vinyl gepresst wurden.

Mit der Schellackverknappung während des Zweiten Weltkrieges wurde die Verwendung von Vinyl forciert, z. B. bei den V-Discs der US-Armee. Das Material ermöglichte deutlich schmalere Rillen (Mikroschrift) als Schellack. Es wurden kleinere Abtastnadeln eingesetzt und es kam zu einer deutlichen Steigerung sowohl der Tonqualität als auch der Spieldauer. Man verwendete meist 33⅓ min, nur für kürzere Aufnahmen 78 min. Somit hatte das Radio bereits ein der Vinyl-LP ähnliches Schallplattenformat in Verwendung, lange bevor die „Mikrorillenschallplatte“ der breiten Öffentlichkeit vorgestellt wurde.

Auch wenn die Vinylschallplatte bis dahin keinen kommerziellen Erfolg hatte, war man sich ihrer Vorteile (geringere Störgeräusche, bessere Haltbarkeit, längere Laufzeit) durchaus bewusst.

Ende der 1940er Jahre brachten zwei Elektrogerätehersteller zunächst abweichende Formate heraus: Am 21. Juni 1948 stellte "Columbia Records" die 12-Zoll-(30 cm)-Langspielplatte mit 33⅓ min und kleinem Mittelloch (7 mm) vor, die von Peter Carl Goldmark seit 1939 entwickelt worden war (abgekürzt LP).
1949 folgte "RCA Victor" mit der 7-Zoll-(17,5 cm)-Schallplatte mit 45 min und großem Mittelloch (1½″ bzw. 38,1 mm). Die Entscheidung für dieses Format rührte von der Überlegung her, dass sich fast alle Musikstücke sinnvoll in Sätze von ungefähr 5 Minuten unterteilen lassen. Um mit der LP vergleichbare Gesamtspielzeiten zu erreichen, wurden für dieses Format automatische Plattenwechsler angeboten. Der Verkauf sollte, wie dies in ähnlicher Form bereits bei den Schellacks üblich war, in einer buchartigen Verpackung mit mehreren Einzelschallplatten erfolgen, daher stammt die heute noch verwendete Bezeichnung „Album“. Gemeinsam war beiden Formaten die Verwendung von PVC als Plattenmaterial und die Mikrorille. Beide Formate wurden zunächst in Konkurrenz zueinander vermarktet. Damalige Plattenspieler beherrschten jeweils nur eines der beiden Formate, so dass Konsumenten sich entscheiden mussten, was zu Unsicherheit unter den Kunden führte. Es handelte sich um ein klassisches Beispiel für einen Formatkrieg, auch bekannt als „Battle of the Speeds“.

Erst seit etwa Mitte der 1950er Jahre wurden Plattenspieler üblich, die die drei wesentlichen bis dahin üblichen Geschwindigkeiten (33⅓, 45 und 78 min) beherrschten und mit Hilfe von Adaptern Platten beider oben erwähnter Mittellochgrößen abspielen konnten. Etliche Plattenspieler besaßen zusätzlich die Geschwindigkeit 16⅔ min (die Hälfte von 33⅓ min), die Mitte der 1950er Jahre aufkam und für Sprachschallplatten Verwendung fand. Letztlich fand dieses Format aber kaum Verbreitung, gegenüber Schallplatten mit 33⅓ min brachte es kaum Kostenersparnis und der Vorteil der längeren Laufzeit ging zu Lasten der Tonqualität. Anfang der 1970er Jahre konnte man davon ausgehen, dass nur eine verschwindend kleine Minderheit Bedarf für die Geschwindigkeiten 16⅔ und 78 min hatte, die danach auf den Markt gebrachten Plattenspieler boten diese Geschwindigkeiten kaum noch an.

Der einzige Bereich, in dem eine mit 16⅔ min laufende Schallplatte erhebliche Vorteile bringen konnte, waren in PKW eingebaute Plattenspieler, bei denen die Verwendung von Schallplatten mit 30 cm Durchmesser aus Platzgründen nicht möglich war, dagegen 17 cm große Schallplatten maximal 8 Minuten laufen (mit 33⅓ min). Peter Carl Goldmark entwickelte für Chrysler Automobile die Highway Hi-Fi 16⅔ min-Schallplatte, aber auch das konnte der Geschwindigkeit 16⅔ min nicht zum Erfolg verhelfen.

Columbias 33⅓-min-Platten wurden für Langspielplatten verwendet, während sich die 45-min-Platten als "Singles" einen eigenen Markt eröffneten. So wurden die ursprünglich absichtlich inkompatibel gestalteten Konkurrenzformate zu Varianten ein und desselben Formats, als das sie heute noch wahrgenommen werden. Mischformate in vielen Varianten sind seitdem dazugekommen. Die Entzerrung nach RIAA („RIAA-Kurve“) wurde ebenfalls erst Mitte der 1950er Jahre standardisiert.

Nach diesen Formatbereinigungen, die hauptsächlich in den USA stattfanden, war die Vinylplatte bereit, den Schallplatten-Weltmarkt zu erobern. Schellackplatten wurden parallel dazu allmählich aus dem Angebot genommen. 1958 wurden in Westdeutschland die Restbestände an Schellackplatten zu Schleuderpreisen verkauft. In Westeuropa und Nordamerika waren sie ab etwa 1960 aus den Läden verschwunden. Bis Ende der 1960er Jahre wurden sie aber in manchen Ländern noch produziert. Bekannt sind zum Beispiel Schellack-Pressungen von Beatles-Platten aus Indien aus dieser Zeit.

Bereits am 14. Dezember 1931 erfand der Ingenieur Alan Dower Blumlein das bis heute benutzte Verfahren für die Aufnahme und Wiedergabe von zwei Kanälen in einer Rille. Die kommerzielle Einführung der "Stereo-Schallplatte" fand allerdings erst 1958 durch Mercury Records statt. Anfangs konkurrierten für kurze Zeit zwei Systeme miteinander. Statt nur die reine Seitwärtsbewegung auszuwerten, setzte das „+“-System die Information des zweiten Kanals zusätzlich in eine Tiefenbewegung um, was eine Vereinigung der Patente von Berliner und Edison bedeutete. Das „×“-System, nach dem Blumlein-Verfahren, setzte dagegen die Tonsignale beider Kanäle in jeweils 45° gegen die Senkrechte geneigte Schwingungen um. Damit war es im Gegensatz zum Konkurrenz-System vollkompatibel zur Monoaufzeichnung.

Die Kanaltrennung erfolgt beim Blumlein-Verfahren durch die 90-Grad-Anordnung der einzelnen, jeweils unter 45° zur Rille ausgerichteten Bewegungsmöglichkeiten der Nadel. Dadurch ist es möglich, im Tonabnehmer induktiv die Projektion dieser Bewegungen in Bezug auf die jeweilige Achse in getrennten Magnetsystemen zu erfassen. Da die Nadelbewegungen der einzelnen Kanäle um 90° versetzt sind, sind die Kanäle voneinander entkoppelt. Monoaufnahmen in Seitenschrift führen zu gleichphasigen Signalen in beiden Tonabnehmerspulen. Umgekehrt führt nur der Summenanteil beider Kanäle einer Stereorille zu einem Ausgangssignal in einem Mono-Tonabnehmer. Dadurch ist sowohl Auf- als auch Abwärtskompatibilität zur Monotechnik gegeben.

Um 1980 stellte CBS Laboratories ein Kompandersystem für Langspielplatten vor, das den nutzbaren Dynamikbereich auf etwa 85 dB (Praxiswert) bzw. 100 dB (unter Laborbedingungen) erweiterte. In Deutschland wurde das CX-System vor allem von Telefunken vermarktet. Der Name stand für „Compatible Expansion“; das Adjektiv „kompatibel“ wies darauf hin, dass ein Abspielen auch ohne Dekoder grundsätzlich möglich war.

Nadelgeräusche wie Knistern und Rumpeln wurden von dem System wirkungsvoll unterdrückt, während andererseits Kratzer infolge der Kontrastwirkung überdeutlich hervortraten.

Um die klanglichen Vorteile ausspielen zu können, erforderten CX-kodierte Schallplatten entweder einen externen CX-Dekoder zwischen Plattenspieler und Audioverstärker oder einen der angebotenen Plattenspieler mit eingebautem CX-Dekoder. Elektronikzeitschriften veröffentlichten auch Anleitungen zum Selbstbau von Dekodern.

Ein bedeutender Nachteil des CX-Systems lag darin, dass der Dekoder genau auf das jeweilige Abtastsystem abgestimmt sein musste, um optimal arbeiten zu können. Der durchschnittliche Anwender musste diese Arbeit einer Fachwerkstatt überlassen. Schon die Verstellung der Auflagekraft des Tonabnehmers erforderte einen erneuten Abgleich des Dekoders. Ein weiterer Nachteil des in Deutschland überwiegend verkauften CX-Dekoders RN100CX von Telefunken lag darin, dass dieses Gerät bereits einen Entzerrer-Vorverstärker für MM-Systeme enthielt und ein Anschluss an den Phono-Eingang des Verstärkers nicht mehr sinnvoll war, der damit ungenutzt bleiben musste. Es gab jedoch auch Dekoder, welche über die Tape-Monitor-Anschlüsse an den Verstärker angeschlossen wurden, so dass die verstärkereigenen Phono-MC- bzw. -MM-Anschlüsse genutzt werden konnten, wie z. B. das Modell 220 des amerikanischen Herstellers Phase Linear. Weitere Anbieter von CX-Dekodern waren die Firmen CM Labs, Kort und Phoenix Systems.

Aufgrund der systembedingten Umständlichkeiten und des zeitgleichen Aufkommens digitaler Tonträger (besonders der Audio-CD) konnte sich das CX-System trotz seiner klanglichen Vorteile bisher nicht bei den Schallplatten durchsetzen. Das Label CBS brachte von 1980 bis 1982 etwa 50 CX-kodierte LPs auf den Markt. Angesichts des im letzten Jahrzehnt rasanten Wachstums des Marktes für Vinyl-Schallplatten wird in Fachkreisen derzeit überlegt, Neuerscheinungen wieder in einer CX-Variante pressen zu lassen. Entsprechende CX-Dekoder werden bereits wieder angeboten.

1982 wurde das CX-System in einer technisch leicht modifizierten Form jedoch zum Standardverfahren für den analogen Zweikanal-Ton der Laserdisc, deren PAL-Version zwar nur von 1982 bis 1985 mit CX-Ton, die NTSC-Version jedoch von 1982 bis 2007 mit CX-Ton produziert wurde – insgesamt mehr als 50000 verschiedene CX-kodierte Titel. Alle seit 1982 gebauten Laserdisc-Player enthalten daher standardmäßig einen CX-Decoder.

Neben der mechanischen Abtastung mit Nadeln kann eine Schallplatte berührungslos optisch mittels Laserplattenspieler oder softwaregestützter „Abtastung“ eines hochauflösenden optischen Digitalisats in einem Computer gelesen werden.

Mit der Einführung der digitalen Compact Disc (CD), die sich in der Fachsprache Audio-CD nennt, gingen ab Mitte der 1980er Jahre die Verkäufe und Produktionszahlen von Schallplatten immer rascher zurück. 1990 wurden doppelt so viele CDs verkauft wie LPs. Anfang der 1990er verkündeten die wichtigsten Konzerne der Phonoindustrie gemeinsam den „Tod der Schallplatte“. Fortan wurde nur noch auf die Audio-CD gesetzt, beziehungsweise später auf Weiterentwicklungen wie SACD und DVD-Audio, welche sich jedoch bisher nicht durchsetzen konnten.

Tonaufzeichnungen auf Schallplatten bringen aus physikalischen Gründen Klangverfälschungen mit sich. Dazu zählen insbesondere

Je hochwertiger die Komponenten des Wiedergabesystems sind, desto kleiner fallen diese Klangverfälschungen aus. Im Vergleich zur Audio-CD ist bei Schallplatten ein recht großer Aufwand bezüglich der Wiedergabekette nötig, um die physikalischen Unzulänglichkeiten der analogen LP möglichst klein halten zu können: Laufwerk, Kombination Tonabnehmer-Tonarm, Abtastnadelschliff und Phonovorstufe sind hier die markantesten Komponenten, die die Klangqualität beeinflussen.

Bei einer Audio-CD unterliegt die Abtastung keinerlei Limitierungen bezüglich geometrischer Unzulänglichkeiten und Gleichlauf. Auch entfällt unter anderem das bei der Schallplatte unvermeidliche Rillen-Grundgeräusch (Rumpeln, Grundrauschen), welches vor allem bei klassischer Musik Einbußen in der Dynamik verursacht.

Unterschiede zwischen Schallplatte und CD bestehen auch in der Langzeithaltbarkeit sowie in der Auswirkung von Fehlern des Tonträgers: Während es bei einer zerkratzten Schallplatte zu den bekannten Toneinbußen und eventuellem Hängenbleiben bzw. Springen über die Tonrille kommt, machen sich Fehler bei der CD mit Tonausfällen bemerkbar, falls die fehlenden Informationen von der internen Fehlerkorrektur des Abspielgerätes nicht rekonstruiert werden können.

Im Vergleich mit dem kleineren CD-Booklet bringt die Plattenhülle die Gestaltung der Hülle deutlicher zum Ausdruck, zudem liegen beispielsweise Gesamtaufnahmen von Opern usw. oft regelrechte Bücher als „Beiheft“ bei.

Unter DJs, insbesondere in den Bereichen House, Techno, Hip-Hop, Drum and Bass u. ä. ist die Schallplatte nach wie vor beliebt, weil sie sich aufgrund ihrer Technik gut zum Zusammenführen einzelner "Tracks" (Musikstücke) eignet. Dabei wird die Schallplatte mittels zweier spezieller Plattenspieler mit stufenlos einstellbarer Abspielgeschwindigkeit in die anderen Tracks gemischt "(mixen)" oder zur Erzeugung spezieller Klangeffekte von Hand abwechselnd in und gegen die Abspielrichtung bewegt "(scratchen)".

Diese Vorliebe der DJs für die auch kurz „Vinyl“ genannten Schallplatten hat zur Entwicklung von neuen Systemen "(„Final Scratch“, „Rane Serato Scratch Live“)" geführt, mit denen digitale Aufzeichnungen mit gewöhnlichen Plattenspielern gesteuert werden können. Dazu werden spezielle Schallplatten benutzt, auf denen statt des Tonsignals ein Timecode aufgezeichnet wurde. Eine externe Soundkarte (die oft gleichzeitig als Dongle dient) digitalisiert diesen Timecode, mit dem dann eine Software die Abspielgeschwindigkeit und -richtung eines digitalen Musikstücks steuert.

In Europa werden zu Beginn der 2010er Jahre jährlich wieder rund 15.000.000 Schallplatten gefertigt. Nach der Einführung der neuen Tonträger konnte sich der Absatz von Vinylplatten wieder etwas erholen, so ist der weltweite Verkauf von neu produzierten Vinylschallplatten im Jahr 2008 gegenüber dem Vorjahr um 89 Prozent gestiegen. 2006 wurden in Deutschland noch 600.000 Langspielplatten verkauft, 2007 700.000 und 2008 900.000 Exemplare. Weiterhin entfällt auf das Schallplattengeschäft aber nur knapp ein Prozent des Umsatzes des deutschen Musikmarktes.

Die erste Schallplatten-CD, welche aus einer Seite CD und einer Seite Vinyl bestand, kam 2007 mit der Single "Deathcar" der Band Fightstar auf den Markt.

Die Schallspeicherung erfolgt mechanisch durch Aufzeichnen des Schalls in einer spiralförmigen Rille und gehört zu den Nadeltonverfahren. Zur Wiedergabe können unterschiedliche mechanische oder elektrische Tonabnehmersysteme verwendet werden. Die technischen Eigenschaften der Schallplatte in der heute verwendeten Form sind in der DIN-Norm DIN IEC 98 festgelegt (frühere Normen: DIN 45536, DIN 45537, DIN 45546 und DIN 45547).

Die technische Ausführung der Schallplatte wurde während ihrer Entwicklung ständig verändert, um Spieldauer, Frequenzgang und Haltbarkeit zu verbessern. Die Formate wurden von Anfang an in Zentimetern festgelegt, die englischen Inch-Bezeichnungen sind nur grobe Näherungen; man beachte:
Neben diversen Sonderformaten konnten sich einige Formate als Standard etablieren. Die folgenden sind bei gegenwärtigen Veröffentlichungen anzutreffen:
Die angegebenen Spielzeiten sind lediglich grobe Richtwerte, da die tatsächliche Spielzeit unter anderem von der Aussteuerung des Schneidstichels und den tieffrequenten Anteilen, besonders im S-Signal (Tiefenschrift) bei der Plattenherstellung abhängt, welche die Packungsdichte der Abspielrille beeinflussen.

Es gibt drei wesentliche Gravurverfahren, von denen zwei noch heute verwendet werden. Die Größe der Rille ist im Laufe der Jahre immer weiter verringert worden. Hatten Schellackplatten eine Rillenbreite von 120 µm "(Normalrille)", ist die heute allgemein verwendete Mikrorille unmoduliert 40 µm breit; der Rillengrund hat einen Radius von 8 μm. Der Rillenabstand beträgt hier bei linearem Vorschub ohne Verwendung von Füllschrift etwa 70 µm.

Bei der von Edison und Pathé verwendeten Tiefenschrift wird die Information durch die Eintauchtiefe des Schneidstichels in die Schallplatte eingeprägt. Die Tiefe ist direkt proportional zur Amplitude des aufgezeichneten Signales. Die maximale aufzuzeichnende Amplitude ist gering, da die Eintauchtiefe nicht beliebig groß werden kann. Um hohe Frequenzen wiedergeben zu können, muss die Nadel den Vertiefungen der Rille zudem sehr schnell folgen. Um das zu erreichen, muss die Auflagekraft des Tonabnehmers vergrößert werden, was jedoch zu einem erhöhten Plattenverschleiß führt.

Bei der 1888 von Emil Berliner eingeführten Seitenschrift ist die Information in der horizontalen Auslenkung der Rille eingeprägt. Der Vorteil gegenüber der Tiefenschrift ist ein größerer Dynamikbereich und die einfachere Herstellung von Kopien. Auch ist im Gegensatz zur Tiefenschrift das Knistern deutlich reduziert. Die Seitenschrift wurde bei Grammophonen und frühen Plattenspielern verwendet. Sie ist nur für einen Kanal geeignet und lenkte bei Grammophonen über eine Stahlnadel direkt eine in einen Trichter mündende Membran aus. Beim Abspielen einer Mono-Schallplatte in Seitenschrift mit einem Stereo-Tonabnehmer wird auf beiden Wiedergabekanälen das Monosignal wiedergegeben.

Die von Alan Blumlein bereits um 1930 entwickelte, aber erst 1957 von der EMI vermarktete Flankenschrift ermöglichte erstmals monokompatible Stereo-Aufzeichnungen. Die Schallinformation für den linken und rechten Kanal wird dabei in die 45°-Flanken der Rille eingeprägt. In der innenliegenden Flanke wird dabei der linke Kanal, in der außenliegenden Rillenflanke der rechte Kanal abgespeichert. Die Richtung der Auslenkung des Schneidstichels ist beim linken und rechten Kanal gegenphasig, so dass ein Monosignal, das mit einem Stereoschneidkopf aufgezeichnet wird, eine Seitenschrift erzeugt. Dadurch ist die Abwärtskompatibilität zu Mono-Systemen gewährleistet: Wenn eine Stereo-Schallplatte auf einem Mono-Abspielgerät wiedergegeben wird, wird lediglich die horizontale Auslenkung der Rille wiedergegeben. Diese entspricht der Summe (L + R) beider Kanäle.

Das Füllschriftverfahren wurde 1949 von Eduard Rhein erfunden und bezeichnet keine eigene neue Plattenschriftart, sondern vielmehr die aussteuerungsabhängige Steuerung des Rillenabstandes auf dem Tonträger. Es wurde entwickelt, um die Spieldauer weiter zu erhöhen. Da die seitliche Auslenkung der aufgezeichneten Rille von der Lautstärke des Tonmaterials abhängt, kann der Rillenabstand bei leisen Passagen verringert werden. In der Praxis wird dazu das aufzuzeichnende Material zeitlich vor dem Schneidkopf abgehört. Der Rillenvorschub wird dann durch die Lautstärke bei der folgenden Umdrehung beeinflusst. Die durch die Verwendung der Füllschrift anstelle des konstanten Rillenabstandes gewonnene zusätzliche Spieldauer ist programmabhängig. Bei Sprache ist sie am größten, bei Musik kann die Spieldauer bis zum 1,7-fachen betragen. Das Wort „Füllschrift“ war sehr werbewirksam, weil es auch einen „volleren“ Klang suggerierte.

Je nach Größe der Rille müssen Abtastnadeln mit verschiedenen Verrundungsradien verwendet werden; angegeben sind die Größen von sphärischen Abtastnadeln:
Die Normalrille entspricht einer Schellackplatte (78 min), die Mikrorille einer Mono-Platte der 1950er und -60er Jahre, die Stereorille einer bis heute üblichen Stereo-Platte. Für die letztere werden heute allerdings meistens modernere Nadelschliffe mit kleineren und differenzierten Verrundungsradien verwendet.

Die Spieldauer einer LP-Seite mit 33⅓ min ist durch die technischen Vorgaben und die gewünschte Klangqualität begrenzt. Ohne Klangverlust sind Spielzeiten bis zu 25 Minuten möglich. Spielzeiten bis zu etwa 28 Minuten je Seite kommen im Pop- und Rockbereich jedoch durchaus vor, in Einzelfällen auch über 30 Minuten (UFO "2 – Flying – One Hour Space Rock)." Je „lauter“ man den Inhalt eingraviert, desto weniger Spielzeit steht zur Verfügung. Auch ein hoher Bassanteil wirkt sich hier nachteilig aus, da er größere Auslenkungen beansprucht. Um eine möglichst hohe Tonqualität zu erreichen, vermeidet man es, zu weit nach innen zu schneiden. Insbesondere bei Klassikplatten ist das auffällig. Letztlich ist es so möglich, sehr unterschiedliche Schwerpunkte bei der Optimierung zu setzen bzw. entsprechende technische „Philosophien“ zu vertreten. Den Lautstärkeunterschied nimmt man besonders gut zwischen Hit-Samplern und Alben wahr. Ein Musiktitel auf einem Album ist bei unveränderter Lautstärkeeinstellung des Verstärkers erheblich lauter als derselbe Titel auf einer Hit-Kompilation, auf die pro Seite zehn Titel gepresst werden. Noch größer ist der Dynamikunterschied bei zeitgenössischen Techno-, Bigbeat-, Trance- und Goa-Schallplatten, die grundsätzlich in LP-Größe gepresst werden, aber nur einen Titel pro Seite enthalten. Diese nur für DJs produzierten Clubtitel sind besonders laut aufgenommen, weil entsprechend viel Platz für die Rille auf der Schallplatte zur Verfügung steht.

Der französische Hersteller "Trimicron" trieb das Ganze 1974 auf die Spitze und brachte unter seinem Namen LPs heraus, die eine Spieldauer von über 53 Minuten pro Seite aufwiesen. Die Qualität war entsprechend minderwertig, denn diese Spielzeit ließ sich nur durch extrem engen Rillenabstand und niedrigen Pegel erreichen.

Eine Vinyl-Schallplatte hat bei entsprechender Pflege und richtiger Lagerung eine nahezu unbegrenzte Haltbarkeit; beim Abspielen mit einer Nadel tritt allerdings mechanischer Verschleiß auf, der die Lebensdauer einer Schallplatte verkürzt und die Klangqualität beeinträchtigt - dies kann durch optische Abtastung verhindert werden.

Für die Herstellung einer Schallplatte in großer Stückzahl wird das gemasterte Programmmaterial zunächst mit einem beheizten Schneidstichel in den Lack einer beschichteten Folie geschnitten. Dabei werden nach einer genormten Kennlinie die höherfrequenten Schallanteile angehoben (Preemphasis) und die tieferen abgeschwächt; bei der Wiedergabe wird der Frequenzgang umgekehrt entzerrt. Diese Lackplatte wird zunächst mit Silber beschichtet, damit sie elektrisch leitend ist, und dann galvanisch verkupfert oder vernickelt. Diese Metallschicht bildet ein etwa 0,5 mm dickes Negativ, den „Vater“. Von diesem werden in einem weiteren galvanischen Verfahren mehrere Positive, „Mütter“, abgezogen. Diese werden zur Kontrolle der Aufnahme vom Mutterstecher abgespielt und gegebenenfalls nachbearbeitet. Die eigentlichen Pressmatrizen („Söhne“) werden wiederum durch einen galvanischen Prozess aus den Mutterplatten gefertigt. Um die Haltbarkeit der Pressmatrizen für größere Stückzahlen zu erhöhen, werden diese verchromt. Dieser Vorgang muss für beide Seiten der Schallplatte wiederholt werden.
Um den Umweg über „Väter“ und „Mütter“ zu vermeiden, wurde zu Beginn der 1980er Jahre von Teldec das sogenannte DMM-Verfahren "(„Direct Metal Mastering“)" entwickelt. Hier erfolgt der Schnitt direkt in eine auf einer Edelstahlplatte aufgebrachte Kupferschicht, von der dann unmittelbar die „Söhne“ erstellt werden. Dem Vorteil geringerer Verzerrungen stehen die Nachteile geringerer Rillentiefe (Haltbarkeit) und -auslenkung (Wiedergabepegel) gegenüber.

Als Rohstoff für die gepressten Schallplatten wird Polyvinylchlorid (PVC) verwendet, dem etwa 20 % Polyvinylacetat (PVAc) und weitere Additive zugesetzt werden. Der eigentliche Rohstoff ist milchig-transparent und kann durch Zusatz von Farbstoffen eingefärbt werden. Die früher notwendige Beimischung von Ruß ist heute nicht mehr erforderlich. Es gibt keine erheblichen qualitativen Unterschiede zwischen schwarzen und farbigen Pressungen, auch variiert die genaue Zusammensetzung des Materials zwischen unterschiedlichen Presswerken.

Bei der Herstellung einer Schallplatte (sogenannte audiophile Pressung) kann kein wiederaufbereitetes Vinyl, sondern nur reines, frisches PVC-Rohmaterial („Virgin Vinyl“) verwendet werden, da beim Recycling das Labelpapier nicht völlig entfernt werden kann und diese Papierreste zu Pressfehlern oder erhöhtem Knistern führen können.

Der Pressvorgang einer Schallplatte dauert etwa 30 Sekunden. Eine dosierte Menge Rohmaterial (150–180 g) wird zusammen mit den Etiketten zwischen die beiden Pressmatrizen gebracht und bei einem Druck von etwa 8·10 Pa (etwa 80 kg/cm²) und einer Temperatur von etwa 150 °C gepresst. Nach einer kurzen Abkühlphase, in der die Matrizen mit Wasser gekühlt werden, wird die Presse geöffnet und die Schallplatte entnommen. Die Fertigung kann auch teilautomatisiert erfolgen: Nach dem Öffnen der Presse wird die Schallplatte mittels eines Halterings und eines Stanztellers auf den Schneideteller gelegt. Dieser erzeugt zur Fixierung ein Vakuum und wird danach in Rotation versetzt. Ein am Außenrand des Schneidetellers angebrachtes Messer trennt dabei den beim Pressvorgang entstandenen Quetschgrat ab. Anschließend wird die Schallplatte mittels eines Transportarms vom Teller entnommen und bis zum Verpacken auf einer Spindel zwischengelagert, dabei wird die Schallplatte etwa 10 Sekunden von der Umgebungsluft gekühlt. Währenddessen produziert der Extruder wiederum einen Presskuchen, und der Vorgang beginnt von neuem.
Bei der Schallplattenproduktion darf man kein Hightech erwarten, da die neuesten Maschinen aus dem Jahre 1983 stammen. Entsprechend personalaufwändig ist der Betrieb dieser Anlagen.

Der Basispreis einer Vinylschallplatte beträgt 1,50 Euro bei Abnahme von mindestens 500 Stück.

Für die Herstellung von kleinen Formaten (7″-Singles und kleineren Sonderformaten) in großer Auflage kann auch ein Spritzgussverfahren angewendet werden. Dazu wird heißes Kunststoffmaterial in flüssiger Form zwischen die Matrizen gebracht. Das Material kühlt in der Hohlform ab und erstarrt.

Diese Platten haben kein Papieretikett, stattdessen enthält die Matrize die Beschriftung in erhabener Form, so dass auf der Platte die Beschriftung etwas vertieft ist. In einem weiteren Vorgang wird der Mittelteil der Platte eingefärbt, damit er sich von den vertieften schwarzen Schriftzeichen optisch abhebt. Zumindest ein Teil dieser Platten ist leichter zerbrechlich als übliche gepresste Vinylplatten.

Eine Weiterentwicklung des Spritzgusses ist der Pressspritzguss, der auch in der CD-Herstellung verwendet wird.

Für sehr kleine Stückzahlen wird das Tonmaterial direkt mit einem Schneidstichel in einen Rohling "(Dubplate)" eingeschnitten. Ursprünglich gab es nur "Lack-Dubplates"; diese bestehen aus einer dünnen Aluminiumplatte, die mit Polyvinylacetat (PVAc) beschichtet ist, in welches das Audiomaterial geschnitten wird. Lack-Dubplates haben nur eine begrenzte Lebensdauer.

Mittlerweile gibt es auch "Vinyl-Dubplates", hier werden Rohlinge aus einem PVC-PVAc-Gemisch auf einem sogenannten "Vinyl-Cutter" geschnitten. Das Ergebnis ist eine echte, langlebige Vinyl-Schallplatte, die sich bei fachgerechter Herstellung nicht von gepresstem, handelsüblichem Vinyl unterscheidet. Die Herstellung solcher Einzelstücke ist insbesondere für Produzenten und DJs interessant, die selbstproduzierte Stücke schnellstmöglich auf Veranstaltungen ausprobieren möchten oder nicht auf Schallplatte erschienene Titel entsprechend verwenden wollen.

Ab etwa 1900 nutzten verschiedene Hersteller, darunter vor allem der französische Konzern "Pathé", das sogenannte Vertikalschriftformat. Die Aufzeichnung erfolgte hier nicht durch seitliche Ausschläge, sondern durch eine Auf-und-Ab-Bewegung. Als Tonabnehmer fungierte keine Nadel, die die Tiefenunterschiede der Rille schnell zerstört hätte, sondern eine winzige, beweglich gelagerte Saphirkugel, die sich rollend durch die Rille bewegte. Systeme dieser Art blieben besonders in Frankreich und den USA bis in die 1920er Jahre populär.

Neben den Standard-Plattenformaten mit 25 und 30 cm Durchmesser gab es in der Schellack-Ära eine Vielzahl anderer kommerziell verwendeter Plattendurchmesser. Die kleinsten Serienplatten der Schellack-Ära waren, sieht man von Werbe- und Spielzeugplatten ab, die amerikanischen "Durium-Junior"-Pressungen aus dem Jahre 1930 mit einem Durchmesser von 10,2 cm (4″). Als die größten Serienplatten gelten die gewaltigen "Pathé Concert"-Pressungen aus der Zeit vor 1914, die einen Durchmesser von 50 cm aufwiesen. In den USA wurden während und nach dem Zweiten Weltkrieg Rundfunksendungen auf Schallplatten mit 16″ (40 cm) [seltener 12″ (30 cm)] aufgezeichnet und zu den in aller Welt stationierten Truppen geschickt. Anfangs aus Schellack, hatten diese so genannten "V-Disc" eine für damalige Verhältnisse erstaunlich hohe Tonqualität; sie sind heute gesuchte Sammlerstücke. Rundfunkanstalten verwendeten auch in den 1960er und 1970er Jahren noch zum Teil Platten mit 16″ (40 cm) Durchmesser, die auch entsprechend dimensionierte Abspielgeräte erforderten.

Sehr selten gibt es auch Platten mit normal großem Außendurchmesser, aber mehr als normal großem Labeldurchmesser "innen". Der Rillenring von außen nach innen wird dadurch effektiv schmaler, und eine so geschnittene LP wird dadurch de facto zur EP. Ein Beispiel liefert die deutsche Kinderschallplatte "Der Froschkönig" (zudem auf farbiges Vinyl gepresst).

Im Verlauf der ersten siebzig Jahre der Schallplattenherstellung gab es immer wieder Versuche, den teuren, schwer zu beschaffenden Rohstoff Schellack durch andere, billigere Materialien zu ersetzen. Die britische Firma "Nicole Records" präsentierte kurz nach 1900 Schallplatten aus einem mit Zelluloid beschichteten Fasermaterial. Wenig später erschienen bei "Zonophone" in Berlin die ersten „klingenden Postkarten“ mit aufgeklebten Schallfolien aus Zelluloid. Die Stahlnadeln der Grammophone zerstörten diese Tonträger allerdings sehr schnell, weil dem verwendeten Zelluloid die nötige Abriebfestigkeit fehlte. Später, nachdem sich die Mikrorille allgemein durchgesetzt hatte, vertrieb Qualiton „klingende“ Postkarten, sogenannte Tonpostkarten für 45 min (oder 33 min), im Umschlag, um Beschädigungen beim Versand und Staubeinwirkung bei der Lagerung zu vermeiden. Abspielbare Postkarten werden noch heute in kleinen Auflagen hergestellt.

Der britische "Neophone"-Konzern fertigte um 1906 vorübergehend Platten aus Pappe, die mit einem wachsartigen Trägermaterial überzogen waren und Probleme in Sachen Hitzebeständigkeit aufwiesen. In den 1920er Jahren produzierte das in Berlin ansässige Unternehmen "Metallophon" Schallplatten aus lackiertem Stahlblech – die Vorzüge dieses Konzepts waren begrenzt, die Umsätze offenbar auch. Vor allem in der Studiotechnik wurden in den 1940er Jahren Schallplatten aus beschichtetem Glas eingesetzt. Das kurioseste jemals zur Plattenherstellung verwendete Material dürfte allerdings Schokolade gewesen sein: Um 1910 präsentierte der Süßwarenhersteller Stollwerck seine Schallplatten, die tatsächlich aus Schokolade gepresst waren, bekannte Kinderlieder enthielten und nach dem – nur wenige Male möglichen – Abspielen aufgegessen werden konnten.

Während des Kalten Krieges wurden in der Sowjetunion "illegale Aufnahmen" teilweise auf gebrauchte Röntgenbilder geschnitten und auf dem Schwarzmarkt gehandelt. Der Vorteil des Trägermaterials lag in seiner allgemeinen Verfügbarkeit und seiner Flexibilität. Die flexiblen Scheiben konnten besser unter der Kleidung versteckt werden als übliche Platten. Die Qualität der Aufnahmen war naturgemäß sehr schlecht. Aufgrund der Herkunft der Folien wurden diese Tonträger umgangssprachlich als „"Rock auf den Knochen"“ (), „"Rippen"“ () oder einfach „"Knochen"“ () bezeichnet.

Die deutsche Bundesanstalt für Materialforschung und -prüfung gab Ende 2015 bekannt, dass es Wissenschaftlern gelungen sei, eine Schallplatte aus ultrahochfestem Beton herzustellen, die auf jedem Plattenspieler in hervorragender Qualität abspielbar ist.

Vor allem im Bereich der Rockmusik, dem Rap und der elektronischen Musik werden Schallplatten angefertigt, die sich durch ihr Aussehen von der Standardplatte abheben. So sind z. B. die so genannten "Picture discs" recht verbreitet, bei denen transparentes Material um ein gedrucktes Bild herum gepresst wird. Ebenfalls gebräuchlich ist gefärbtes und/oder transparentes PVC. Diese Platten erscheinen oft in einer limitierten Auflage und können einen großen Sammlerwert haben. Eine weitere Variante sind die so genannten "Shape vinyls". Anders als gewöhnliche Schallplatten sind diese nicht rund, sondern können die ausgefallensten Formen haben (was sich jedoch nur auf den äußeren Umriss bezieht – die Schallaufzeichnung erfolgt in der üblichen Spiralform). Genutzt wird diese Ausgefallenheit als Kombination von Foto und Form: Ein Foto (Konterfei des Stars, Gitarre, Herz, Ganzkörper- oder Bandfotos usw.) wird nicht auf die runde Schallplattenform verkleinert, sondern das Foto sozusagen aus der Schallplatte ausgesägt. Die Umrisse bilden das Abspielmedium, der Maximalbereich im Inkreis der Platte enthält die Rillen.

Während die Nadel bei „normalen“ Schallplatten von außen nach innen wandert, gibt es auch Formate, die von innen nach außen abgespielt werden. Am bekanntesten in der Schallplattengeschichte sind die Tiefenschrift-Platten des französischen Unternehmens „Pathé Frères“ aus der Zeit vor dem Ersten Weltkrieg. Aber auch die von den Reichssendern während der NS-Zeit für den internen Gebrauch und gegenseitigen Austausch produzierten Platten gehörten teilweise dazu. Plattenteller drehen sich normgemäß – von oben betrachtet – nach rechts, also im Uhrzeigersinn, die ruhigstehende Nadel also relativ zur sich bewegenden Platte linksherum. Die normale Rille führt linksherum von außen nach innen. Die Rille einer Platte mit sogenanntem „inside out cut“ führt jedoch als Spirale linksherum betrachtet nach außen und im Betrieb die Nadel ebenso. Nahe dem äußeren Rand muss jedenfalls die Rille einen Abschlusskreis bilden, denn Plattenspieler schalten hier nicht ab.

Die Umkehrung der Spiralrichtung kann auch aus rein klanglichen Gründen von Vorteil sein: Eine niedrigere Laufgeschwindigkeit verursacht weniger Laufgeräusche, während eine höhere Laufgeschwindigkeit lauteres und dynamischeres Material sicherer wiedergeben kann. So können beispielsweise Musikstücke optimal untergebracht werden, welche leise beginnen und laut enden, ohne die Nachteile der sonst üblichen Laufrichtung von außen nach innen in Kauf nehmen bzw. durch entsprechende klangliche Veränderungen kompensieren zu müssen.

Die ersten Versuche mit Mikrorillen-Formaten, die eine längere Spieldauer der Schallplatte ermöglichen sollten, unternahm der britische Tontechnik-Pionier Michaelis bereits im Jahre 1906. Sein Unternehmen "Neophone" produzierte 25-cm-Platten mit einer Laufzeit von etwa 12 Minuten. 1926 präsentierte Thomas Alva Edison als eine seiner letzten Erfindungen eine Langspielplatte mit extremer Mikrorille (siehe Diamond Disc), die bei 80 min und 24 cm Durchmesser eine Laufzeit von mehr als 20 Minuten pro Seite aufwies. Die Platte konnte nur mit einem speziellen Diamant-Abnehmer wiedergegeben werden und war einen halben Zentimeter dick, um jegliche Flexibilität zu beseitigen. Die Empfindlichkeit der Mikrorillen, deren Wände schon durch normales Berühren der Platte beschädigt werden konnten, verhinderte jeden kommerziellen Erfolg des Systems, das nur einige Monate lang auf dem Markt blieb. Die erste kommerziell bespielte Langspielplatte mit 33 ⅓ Umdrehungen kam am 17. September 1931 auf den Markt.

Ein besonders in den USA vor 1914 verbreitetes Phänomen waren Schallplatten mit besonders großen Mittellöchern. Plattenhersteller wie "Aretino" und "Busy Bee" versuchten auf diesem Gebiet eigene Standards zu setzen. Hintergrund waren keine technischen Überlegungen, sondern bloße Vermarktungsstrategien. Ungewöhnlich kleine Mittellöcher, die aus einer vom westlichen Standard abweichenden Industrienorm resultierten, waren dagegen ein Merkmal sowjetischer Schallplatten aus den 1920er und 30er Jahren.

Ende der 1950er Jahre brachte Seeburg, ein US-amerikanischer Hersteller von Musikboxen, einen speziellen Plattenwechsler heraus und ein Schallplattenformat, das 22 cm Durchmesser, Mittellöcher von 5 cm und eine Geschwindigkeit von 16⅔ min hat. Dieser war für Untermalungsmusik gedacht.

Der Weg, den die Nadel beim Abspielvorgang auf der Platte pro Umdrehung zurücklegt, wird durch den schneckenförmigen Verlauf der Schallrille mit zunehmender Abspieldauer kleiner. Da bei konventionellen Platten die Drehzahl des Plattentellers konstant bleibt, sinkt so die Geschwindigkeit der Nadel relativ zur Plattenoberfläche vom äußeren Rand der Platte bis zum inneren kontinuierlich ab. Diese Veränderung ist unter bestimmten Umständen akustisch wahrnehmbar. Um das Phänomen zu beseitigen, wurde seit den 1920er Jahren mit Platten experimentiert, die durch permanente Anpassung der Drehzahl eine konstante Lineargeschwindigkeit aufwiesen. Jedoch musste jedes Abspielgerät mit einer speziellen Vorrichtung ausgerüstet und justiert werden. Eine dieser Vorrichtungen war in den USA der sogenannte "World Disc Record Controller". Durchgesetzt haben sich solche Systeme nicht.

Erst mit der Einführung von optischen Medien wie der mit Puls-Pausen-Längen codierten Laserdisc und später auch der voll digitalen Compact Disc sowie deren Nachfolgern gingen entsprechende Techniken in Serie. Siehe dazu CLV.

In den frühen 1930er Jahren gab es Schallplatten mit vier Rillen: „Die Serie für die gute Hausmusik“. Am Anfang jeder Seite befand sich eine Rille mit dem Kammerton A, nach dem man seine eigenen Instrumente stimmen konnte, erst nach erneutem Aufsetzen des Tonabnehmerarmes begann mit der zweiten Rille der Titel zum Mitspielen und -singen.

In der Frühphase des Selbstschneidens von Schallplatten waren mehrere Sektionen üblich, jede mit einer Rille, bis zur vollständigen Füllung, um Rohlinge zu sparen. Jede Sektion besaß an ihrem Ende eine eigene Auslaufrille, und man musste den Tonabnehmer per Hand am Beginn der nächsten Sektion aufsetzen, wenn man die Wiedergabe fortsetzen wollte.

Multi-Groove
Vereinzelt wurden Schallplatten mit zwei oder mehr ineinander verschränkten, parallel laufenden Rillen veröffentlicht. So ist es vom Zufall abhängig, in welcher Rille der Tonarm aufsetzt, so dass der Hörer zunächst nicht weiß, welche Musik er zu hören bekommen wird. Allgemein bekannt wurde diese Technik 1979 mit einer Sonderpressung der Hit-Single "Pop-Muzik" von Robin Scott, die auf derselben Seite auch das Stück "M-Factor" enthielt und laut Werbetext auf dem Plattencover als „The First ‚Double Groove‘ Disk“ vermarktet wurde. Jedoch hatte die Comedy-Truppe Monty Python bereits 1973 die „three-sided“ LP "Matching Tie and Handkerchief" veröffentlicht, die zusätzlich für Verwirrung sorgte, weil beide Plattenseiten mit identischen Labels versehen waren (Vorder- und Rückseite konnten aber bei genauem Hinsehen an der eingeprägten Matrizen-Nummer unterschieden werden). 1975 kam die Platte „Wim Thoelke präsentiert: Der große Preis – Die volkstümliche Schlager-Starparade“ heraus (Sternmusik im Ariola-Vertrieb, 88909XT). Der erste Titel auf der A-Seite ist ein mit Musik unterlegtes und von Wim Thoelke kommentiertes Autorennen, bei dem je nach getroffener Einlaufrille einer von 3 Wagen gewinnt. Beliebt waren Multi-Groove-Schallplatten vor allem bei der Veröffentlichung desselben Songs in verschiedenen Versionen, wegen des höheren Platzbedarfs meist als 12″-Single herausgebracht. 2009 wurde die Dreifach-A-Single "HimmelblauPerfektBreit" der Band Die Ärzte auf einer 12″-Vinylschallplatte mit Dreifachhelix auf den Markt gebracht.

Jeder Audio-Träger kann nach Modulation (digital-analoge Umwandlung) auch zur Speicherung digitaler Inhalte zweckentfremdet werden. Zur Rückgewinnung der Daten benötigt man einen Demodulator. Diese Verfahren fanden besonders in der Ära der Kleincomputer großen Anklang, wobei dafür nur selten eine Schallplatte genutzt wurde. Man wich schon früh auf Magnetbandkassetten (sogenannte Datasette) aus, weil diese dem Nutzer zugleich die private Datenspeicherung ermöglichten.

Die einzige in größerer Stückzahl produzierte Datenschallplatte war eine Pressung des DDR-Jugendradios DT64, die häufig benutzte Software für die Kleincomputer KC 85, HC 900, Z1013 u. a. enthielt. Sie wurde zusammen mit einem Buch vertrieben.

Auch die deutsche Band hat 2005 auf ihrer ersten Vinyl-Veröffentlichung "„Horizonterweiterungen“" ein Programm für den Heimcomputer Commodore 64 gepresst. Nach Umkopieren auf eine Magnetbandkassette konnte man es über eine Datasette auf dem C64 ausführen.

Ein weiteres Beispiel liefert die Synthpop-Band Information Society mit ihrem Album „Love & Peace Inc.“, die aus Spaß auf diese Weise einen kurzen Text hinterlegte. Dessen Decodierung wurde allerdings dem Anwender überlassen.

Heute lebt Vinyl als digitaler Datenträger durch die sogenannten digitalen Vinylsysteme wieder auf. Die dafür verwendeten Timecode-Platten enthalten keine Musik, sondern ein digitales Zeitsignal, das von einer Software ausgewertet wird. Die Technik erlaubt es, jederzeit die Geschwindigkeit und die Position des Tonarms auf der Platte zu bestimmen und quasi verzögerungsfrei auszuwerten. Die Timecode-Vinyl kann so zum Steuern von digitalen Musikdateien (zum Beispiel im MP3-Format) genutzt werden. Der Plattenspieler dient dabei als Steuerinterface. Dabei wird die exzellente Handhabung von Vinyl beim DJ-ing mit den umfangreichen Möglichkeiten einer digitalen Musikbibliothek verknüpft.

Der Wiener Künstler Gebhard Sengmüller stellte Ende der 1990er Jahre gemeinsam mit dem Physiker Martin Diamant ein System vor, mit dem es möglich war, Videofilme auf gewöhnlichen Langspielplatten zu speichern.
Die Videobilder wurden dabei nach dem Prinzip der Datenschallplatte in digitaler Form auf die Platten gepresst. Da eine Schallplatte, bedingt durch die maximale speicherbare Frequenz von knapp 20 kHz, nur eine sehr geringe Bitrate zuließ, musste das Videosignal sehr stark komprimiert werden. Das resultierende Bild war aus diesem Grund nur schwarzweiß, die Bildfrequenz war gegenüber einem gewöhnlichen Fernsehbild stark reduziert und die Auflösung lag weit unter der von VHS. Zusätzlich kam es, bedingt durch die fehleranfällige analoge Speicherung, zu vielen Bildstörungen, wie Flimmern und Zittern. Aufgrund dieser technischen Einschränkungen erinnerten die so wiedergegebenen Filme qualitativ eher an die Anfangszeit der Fernsehgeschichte.
Zur Wiedergabe konnte jeder gewöhnliche Plattenspieler verwendet werden, wobei ein zusätzliches Gerät benötigt wurde, das die digitalen Signale decodierte und in Videosignale umwandelte.

Das Projekt war rein als Experiment und technische Spielerei gedacht, sozusagen um eine Lücke in der Geschichte der Videoaufzeichnung zu schließen, zumal zu dieser Zeit bereits eine Vielzahl an sehr hochwertigen Videosystemen auf dem Markt existierte. Entsprechend wurde Vinylvideo im 1950er-Jahre-Stil beworben.
Es wurde nur eine sehr geringe Stückzahl an Vinylvideo-Decodern gefertigt und insgesamt waren nur wenige dutzend Titel auf Vinylvideo erhältlich, wobei es sich großteils um Kurzfilme von Independentkünstlern handelte.

Die flexible Disc ("Flexidisc", "Schallfolie" oder "Flexi") ist eine Schallplatte aus sehr biegsamem Material. Zumeist sind es Folien aus Polyethylen oder ähnlichen Kunststoffen. Flexi werden zum Beispiel als Beilage in Fanzine oder Kinderzeitschriften verwendet. Zum Abspielen ist als Unterlage eine normale Platte mit kleinem Mittelloch notwendig. Aus technischen Gründen sind sie oft nur einseitig bespielt. Die Tonqualität ist schlechter als bei Vinyl-Schallplatten.

Eine Kombination aus analoger Schallplatte und CD bzw. DVD wird seit Herbst 2007 exklusiv von der deutschen Firma "optimal media GmbH" gefertigt und vermarktet. Die geschützte VinylDisc hat die Größe einer CD und enthält auf der silbernen Unterseite digitale Informationen sowie auf der schwarzen Oberseite eine Schallplattenrille für die Wiedergabe auf analogen Plattenspielern mit einer Spielzeit von bis zu 3 Minuten bei 33⅓ min. Die Zentrierung auf dem Plattenteller erfolgt mittels eines beigefügten Adapters. Die Vinylseite kann mit bis zu drei Farben im CD-Siebdruck bedruckt werden. Dabei wird die Oberfläche zusätzlich mattiert, um das Aussehen eines herkömmlichen Schallplatten-Etikettes zu simulieren. Hauptverwendungszweck ist die Promotion neuer Musiktitel (analoger Part), wobei auf dem digitalen Part zusätzlich Informationen, Videoclips oder Computerspiele untergebracht werden können.

Die eigentliche Idee stammt von dem kleinen Label "Squoodge-Records" aus Österreich. Dieses veröffentlichte bereits im Januar 2007 die erste Kleinserie einer VinylDisc im sogenannten "DigitalVinylTrash-Club" ohne sich das Format rechtlich schützen zu lassen.

Bis in die 1970er Jahre war es im Kino üblich, Werbedias (vereinzelt Kleinbild-, aber meist Mittelformat) vor dem Hauptfilm zu zeigen. Musik und Texte zu einzelnen Dias wurden auf Schallplatten bereitgestellt. Der Vorführer musste zu jedem Dia manuell die passende Werbesingle abspielen.

In Sprechpuppen für Kinder wurden sehr kleine auswechselbare Schallplatten eingesetzt, die nach dem Grammophon-Prinzip abgespielt wurden, sogenannte Miniphon-Platten. Auch in andere Spielzeuge wurden kleine Schallplatten zur Tonwiedergabe eingebaut („Lachsack“). Eine musikalische Verwendung fanden diese Platten im Werk „Chöre und Soli“ der deutschen Band Die Tödliche Doris.

Die Voyager Golden Records sind Datenplatten mit Bild- und Audio-Informationen, die an Bord der beiden 1977 gestarteten interstellaren Raumsonden Voyager 1 und Voyager 2 angebracht sind. Sie sind vergoldete Scheiben aus Kupfer mit einem Durchmesser von 30 cm, die Grußbotschaften, Geräusche und Musik enthalten, außerdem analog gespeicherte Bilder.

Neuere Schallplatten bestehen aus dem Nichtleiter Polyvinylchlorid und ziehen somit durch elektrostatische Aufladung des Materials Staub und Schmutz an. Außerdem reibt die Platte in der Innenhülle, wenn sie aus dieser herausgezogen wird. Es gibt in der Geschichte der Schallplatte daher eine Vielzahl von Methoden, die Platten zu entladen und die Rillen sauber zu halten.



Beim Nassabspielen läuft außer dem Tonarm noch ein zusätzlicher Reinigungsarm auf der rotierenden Platte, an dessen Unterseite sich eine kleine Bürste befindet, die die Rillenflanken mit einem Gemisch aus destilliertem Wasser und Isopropanol benetzt. Das reinigt die Flanken und schwemmt verbleibende Staubpartikel auf, so dass sie sich beim Abtasten nicht störend bemerkbar machen. Nach dem Verdunsten des Flüssigkeitsfilmes verbleiben jedoch geringe Rückstände in der Rillenflanke, und die verbliebenen Staubpartikel haften jetzt fester an der Oberfläche. Daher sollten einmal nass abgespielte Schallplatten auch künftig wieder nass abgespielt werden. 

Außerdem muss beim Nassabspielen die Umdrehungsgeschwindigkeit des Plattentellers neu eingestellt werden, da die zusätzliche Bürste die Platte beim Drehen leicht abbremst.





</doc>
<doc id="9007" url="https://de.wikipedia.org/wiki?curid=9007" title="Landkreis Limburg-Weilburg">
Landkreis Limburg-Weilburg

Der Landkreis Limburg-Weilburg ist eine Gebietskörperschaft im Regierungsbezirk Gießen im Land Hessen und liegt in der Metropolregion Frankfurt/Rhein-Main zwischen den Städten Frankfurt am Main und Köln/Bonn. Die Kreisstadt ist Limburg a. d. Lahn.

Der Landkreis liegt zwischen den Mittelgebirgen Taunus und Westerwald in Mittelhessen. Dabei wird ein großer Teil des Kreisgebietes von den Tallandschaften der Lahn (Weilburger Lahntalgebiet und Limburger Becken) eingenommen, welche den Kreis von Nordosten nach Südwesten durchfließt. Das Limburger Becken bildet mit seiner Boden- und Klimagunst eine der ertragreichsten Agrarlandschaften Hessens und hat darüber hinaus als Lahnübergang seit dem Mittelalter eine hohe verkehrsgeographische Bedeutung. Im Anschluss an das Limburger Becken setzt sich der Goldene Grund im Emsbachtal fort. Dort ist ebenfalls ein sehr ertragreiches Agrargebiet. Nördlich des Limburger Beckens und dem weiter östlich verlaufenden Weilburger Lahntalgebiets liegen Teile des Kreises im Westerwald. Südlich davon erstreckt sich der Landkreis im östlichen Hintertaunus mit den Hochflächen und dem Weiltal mit seinen Nebentälern.

Der Landkreis grenzt im Uhrzeigersinn im Norden beginnend an den Lahn-Dill-Kreis, Hochtaunuskreis und Rheingau-Taunus-Kreis (alle in Hessen) sowie an den Rhein-Lahn-Kreis und den Westerwaldkreis (beide in Rheinland-Pfalz).

Weit über das Frühmittelalter hinaus datieren frühgeschichtliche Funde, wie das eiszeitliche Lössprofil, das bei archäologischen Grabungen auf dem Gelände der ICE-Stadt Limburg gefunden wurde. Siedlungsspuren aus der mittleren Altsteinzeit (vor etwa 100.000 Jahren) bezeugen die heute zerstörten Steinkistenfunde von Niedertiefenbach, Ober- und Niederzeuzheim, die zerstörte Wildscheuerhöhle bei Steeden und die Wallanlage auf der Dornburg bei Wilsenroth. Im Südkreis stehen die Hünengräber aus der Hallstattzeit (750–450 v. Chr.) für eine Siedlungskontinuität ebenso wie die merowingischen Gräber in Neesbach. Das Kreisgebiet birgt in einigen Gemarkungen eine besondere Funddichte und jedes Fundstück dokumentiert die einstige Bedeutung der Region, des Niederlahngaus.

Die im fränkischen Reich hoch angesehene frühmittelalterliche gräfliche Adelsfamilie der Konradiner beherrschte den Lahnraum des heutigen Kreisgebietes. Zu der Gründung einer Stiftkirche auf der Lintpurc, dem ersten urkundlichen Zeugnis von 910 für die heutige Kreisstadt Limburg an der Lahn, gesellt sich das Walpurgisstift in Weilburg. Auch für das Weilburger Stift liegt eine Urkunde vor, nach der Konrad I. dem Stift für das Seelenheil seiner Vorfahren 912 eine Güterschenkung zukommen ließ. Die konradinische Hauspolitik bezeugt eine große verkehrspolitische Weitsicht bei der Erschließung ihres Territoriums. Im Abstand von Tagesetappen sicherten die Konradiner ihren Herrschaftsraum durch Stiftsgründungen entlang der Lahn (Limburg, Weilburg, Wetzlar) und auf dem Westerwald (Montabaur). Reichspolitisch erfährt die Familie mit König Konrad I., der einzige konradinische Herrscher auf dem deutschen (ostfränkischen) Thron, ihren Zenit. Auf dem Sterbebett, so überliefert der Chronist Widukind, habe Konrad staatsmännische Größe gezeigt, als er seinen Bruder bat, die Reichsinsignien seinem erbitterten Gegner, dem Sachsenherzog Heinrich, zu überbringen, dem so genannten Weilburger Testament.

Limburg am Schnittpunkt wichtiger alter Straßen kam (samt Burg und Stiftsvogtei) zu Beginn des 13. Jahrhunderts an die Herren von Ysenburg, 1344 zur Hälfte, 1420 ganz an das Kurfürstentum Trier und blieb bis zur Säkularisation im Jahr 1803 trierisch und damit katholisch.

Weilburg, über einer großen Lahnschleife gelegen, ursprünglich ebenfalls im Besitz der Konradiner Grafen, hatten die Wormser Bischöfe seit dem 10. Jahrhundert als Reichslehen. Als deren Vögte fungierten seit 1195 die Grafen von Nassau, die 1294, als mit Adolf einer der ihren deutscher König war, Ort und Burg als Eigentum erwarben. Die Stadt (seit 1295) wurde 1355 Residenz der Grafen (ab 1737 Fürsten) von Nassau-Weilburg und blieb dies bis 1816.

Während der von Trier beherrschte Teil katholisch war, trat das Haus Nassau zum protestantischen Glauben über. Der erst 19-jährige Graf Philipp III. von Nassau-Weilburg berief im Herbst 1526 den evangelischen Pfarrer Erhard Schnepf nach Weilburg. Seine öffentliche Disputation im Hause des Stiftsdekans am 31. Oktober des gleichen Jahres gilt als Beginn der Reformation in Weilburg.

Die Glaubenszerrissenheit des 17. Jahrhunderts lässt sich am Haus Nassau-Hadamar besonders gut veranschaulichen. Johann VI. von Nassau-Dillenburg war zunächst Lutheraner, dann überzeugter Calvinist und ließ seinen fünften am 12. August 1590 in dritter Ehe mit Johannetta von Sayn-Wittgenstein geborenen Sohn Johann Ludwig in diesem Sinne erziehen. Johann Ludwig von Nassau-Hadamar, 1650 in den Reichsfürstenstand erhoben, trat 1629 zum Katholizismus über und vollzog mit großem Eifer und mit Hilfe der von ihm ins Land geholten Jesuiten die Rekatholisierung der Grafschaft. Das 1652 von ihm initiierte Gymnasium wurde bis 1773 von Jesuiten geleitet. Er tolerierte weiterhin den reformierten Glauben seiner Gattin, Gräfin Ursula von Lippe-Detmold. Als kaiserlicher Gesandter stellte er 1638 in Köln, Münster und Osnabrück bei den Verhandlungen über den Westfälischen Frieden sein diplomatisches Geschick unter Beweis. Die Kandidatur des inzwischen verwitweten Fürsten auf den Bischofsstuhl von Münster (1650) scheiterte jedoch.

Aus den territorialen Veränderungen der deutschen Länder zu Beginn des 19. Jahrhunderts ging das Herzogtum Nassau (1806) hervor, dessen Regierung zunächst in gemeinsamer Hand von Herzog Friedrich August von Nassau-Usingen und Fürst Friedrich Wilhelm von Nassau-Weilburg lag. Nach deren beider Tod 1816 ging die alleinige Regentschaft an das nassau-weilburgische Haus über. Das Schloss in Weilburg, das heute den Rahmen für die Weilburger Schlosskonzerte bietet, trat nun hinter das Biebricher Schloss in Wiesbaden zurück. Nassau, das 1866 auf Seiten Österreichs kämpfte, wurde zur Kriegsbeute und am 20. September 1866 offiziell durch das Königreich Preußen annektiert. Herzog Adolph ging nach Luxemburg und wurde Großherzog von Luxemburg. Nassau bestand als Regierungsbezirk Wiesbaden in der Provinz Hessen-Nassau (mit Kassel) fort.

Das Gebiet um Weilburg war schon früh Teil des Herzogtums Nassau. Nach der im Juni 1866 erfolgten Besetzung durch Preußen wurde 1867, durch die preußische Verordnung vom 22. Februar, der Oberlahnkreis mit Weilburg als Sitz gebildet. Fast 20 Jahre später entstand 1886 aus den Ämtern Limburg (vorher Unterlahnkreis), Hadamar (vorher Oberlahnkreis) und Camberg (vorher Untertaunuskreis) der Kreis Limburg mit Sitz in Limburg an der Lahn.

Im Rahmen der hessischen Kreisreform vereinigte sich der Oberlahnkreis mit dem Kreis Limburg durch einen freiwilligen Gebietsänderungsvertrag vom 1. Juli 1974 zum Landkreis Limburg-Weilburg. Hierbei wechselten drei Gemeinden in benachbarte Landkreise über. Hasselbach (Taunus) wurde Teil der Großgemeinde Weilrod und schloss sich somit dem Hochtaunuskreis an. Altenkirchen und Philippstein, vormals dem Oberlahnkreis zugehörig, wurden zu Stadtteilen der Stadt Braunfels und gehören somit nunmehr dem Lahn-Dill-Kreis an.

Als erster kreisweiter Verband im neu gebildeten Landkreis wurde am 20. September 1975 in Obertiefenbach der Kreisfeuerwehrverband Limburg-Weilburg gegründet. Die Delegierten der damaligen Kreisfeuerwehrverbände Limburg und Oberlahn beschlossen, sich mit sofortiger Wirkung zu vereinigen.

Die ältesten Städte (mit ersten Erwähnungsdaten) sind Hadamar (832), Weilburg (906), Limburg (910) und Camberg (1000).

Am zählte der Landkreis Limburg-Weilburg Einwohner. Gemäß der Volkszählung 2011 betrug der Anteil der Frauen 50,8 Prozent, der der Männer 49,2 Prozent. Der Ausländeranteil (melderechtlich registrierte Einwohner ohne deutsche Staatsangehörigkeit) lag bei 7,2 Prozent, während sich der Anteil an der Bevölkerung mit Migrationshintergrund (melderechtlich registrierte Einwohner mit deutscher Staatsangehörigkeit und zugleich ausländischer Herkunft) auf 18,1 Prozent bezifferte. 44,7 Prozent der Einwohner waren laut Volkszählung römisch-katholisch, während 31,8 Prozent der evangelischen Kirche angehörten. 23,5 Prozent waren weiteren Glaubensgemeinschaften zugehörig, gehörten keiner Glaubensgemeinschaft an oder machten keine Angaben.

Am 23. September 2008 erhielt der Landkreis den von der Bundesregierung verliehenen Titel "Ort der Vielfalt".

Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:

Sitz der Kreisverwaltung ist das Kreishaus in Limburg an der Lahn mit Adresse "Schiede 43". Das dreigeschossige Gebäude mit hohem Walmdach steht unter Denkmalschutz und wurde 1925/26 nach Plänen der Architekten F. Gais und Gottlob Schaupp aus Frankfurt errichtet. Die elfachsige westliche Front folgt dem leicht gebogenen Verlauf der Schiede, während die südlichen Bauabschnitte vom Straßenverlauf zurückspringen, um so die Einmündung der Diezer Straße optisch zu erweitern.

Landrat des Landkreises Limburg-Weilburg ist seit 22. Dezember 2006 Manfred Michel (CDU) nach erfolgreicher Direktwahl am 26. November 2006. Er wurde Nachfolger von Manfred Fluck (SPD), der aus Altersgründen ausschied. Bei der Direktwahl am 9. September 2012 verteidigte Manfred Michel sein Amt mit 62,6 Prozent der Stimmen.

Seit der Entstehung des Landkreises Limburg-Weilburg gab es vier Landräte. 1974 übernahm der Landrat des ehemaligen Kreises Limburg, Heinrich Anton Wolf (CDU), kommissarisch diese Aufgabe im neu geschaffenen Landkreis, bevor Georg Wuermeling (CDU) in dieses Amt im Februar 1975 gewählt wurde. Nachfolger wurde im Juli 1989 Manfred Fluck (SPD), der im Jahr 2006 sein inzwischen direkt gewähltes Amt abgab.

Helmut Jung (SPD) ist seit 2007 Erster Kreisbeigeordneter des Landkreises Limburg-Weilburg. Er wurde im Februar 2007 vom Kreistag gewählt und am 16. März 2007 in sein Amt eingeführt. Er wurde Nachfolger von Manfred Michel.

Der Landkreis Limburg-Weilburg führt ein Wappen sowie eine Hiss- und Bannerflagge.

Flaggenbeschreibung: „Die Flagge zeigt auf einer gelben, von zwei blauen Seitenstreifen im Verhältnis 1:3:1 eingefassten Bahn, in der Liekhälfte das Wappen. Das Banner zeigt auf einer gelben, von zwei blauen Seitenstreifen im Verhältnis 1:3:1 eingefassten Bahn, das Wappen mittig deutlich nach oben versetzt.“

Der "Ehrenbecher des Landkreises Limburg-Weilburg" wird als Ausdruck des Dankes und der Anerkennung für die besonderen ehrenamtlichen Verdienste zum Wohle der Bürger des Landkreises Limburg-Weilburg durch den Kreisausschuss verliehen. Dieser Zinnbecher trägt die Prägung des Wappens des Landkreises.

Am 1. Juli 1974 wurde dem Landkreis das seit dem 1. Juli 1956 für den Landkreis Limburg gültige Unterscheidungszeichen "LM" zugewiesen. Es wird durchgängig bis heute ausgegeben. Bis in die 1990er-Jahre erhielten Fahrzeuge aus dem Altkreis Weilburg (bzw. Oberlahn) Kennzeichen mit den Buchstabenpaaren "NA" bis "ZZ" und den Zahlen von "1" bis "999".

Zudem ist seit dem 2. Januar 2013 das Unterscheidungszeichen "WEL" (Weilburg bzw. Oberlahnkreis) erhältlich.

Im Zukunftsatlas 2016 belegte der Landkreis Limburg-Weilburg Platz 204 von 402 Landkreisen, Kommunalverbänden und kreisfreien Städten in Deutschland und zählt damit zu den Regionen mit „ausgeglichenem Chancen-Risiko Mix“.

Die wichtigste Fernverkehrsstraße des Landkreises Limburg-Weilburg ist die Bundesautobahn 3 (Köln–Frankfurt) mit den Anschlussstellen 42 (Limburg Nord), 43 (Limburg Süd) und 44 (Bad Camberg). Wichtige Bundesstraßen sind die B 8, B 49 "(Lange Meil)", B 54, B 417 und B 456.

Durch den Kreis führt die Trasse der Schnellfahrstrecke Köln–Rhein/Main. In Limburg, in unmittelbarer Nähe zur Anschlussstelle Limburg Süd der A 3, befindet sich der Bahnhof "Limburg Süd" mit Verbindung in nordwestlicher Richtung (Montabaur–Siegburg/Bonn–Köln–Aachen–Brüssel) sowie in südöstlicher Richtung (Frankfurt am Main) durch Intercity-Express-Züge. Umsteigefreie ICE-Verbindungen bestehen unter anderem zum Frankfurter Flughafen sowie dem Frankfurter Hauptbahnhof, Köln, Düsseldorf, Stuttgart und München. 

Der Landkreis Limburg-Weilburg ist Träger von 37 Grundschulen, drei kombinierten Grund- und Hauptschulen, sechs Grund-, Haupt- und Realschulen, drei Haupt- und Realschulen, zwei Gymnasien, zwei integrativen Gesamtschulen, drei kooperative Gesamtschulen, vier Förderschulen und vier beruflichen Schulen. Im Jahr 2006 wurden diese Schulen von rund 29.000 Schülern besucht.

In Kooperation mit der "Wirtschaftsförderungsgesellschaft Limburg-Weilburg-Diez" begann die Deutsche Telekom AG im Frühjahr 2013 in der Stadt Limburg (mit Ausnahme des Stadtteils Ahlbach), in der Gemeinde Elz (mit Ausnahme des Ortsteils Malmeneich) und in Runkel-Dehrn mit dem Ausbau einer Internet-Breitbandversorgung mit einer Leistung von 50 Mbit/s. Der Landkreis Limburg-Weilburg war damit einer der ersten hessischen Landkreise, der mit einer flächendeckenden Versorgung der Bürger und Unternehmen mit leistungsstarken Internetanschlüssen begann. Im Rahmen einer öffentlichen Ausschreibung erhielt die Deutsche Telekom später den Zuschlag für die vollständige Realisierung des Breitbandausbaus im Landkreis. Am 18. Juli 2014 begannen in Hünfelden-Dauborn die Arbeiten für den Ausbau des Breitband-Internetzugangs. Später folgte der Ausbau im Limburger Stadtteil Ahlbach, in der Gemeinde Beselich und in der Stadt Weilburg. Der Abschluss der Arbeiten war auf den 30. Juni 2016 datiert. Es ist vorgesehen, dass 95 Prozent der Einwohner mit mindestens 30 Mbit/s das Internet nutzen können. Dem Landkreis und den Kommunen entstehen Aufwendungen für den Ausbau in Höhe von 5,5 Millionen Euro.


Im Landkreis Limburg-Weilburg existieren 19 Kommunen. Von den 19 Kommunen besitzen fünf das Stadtrecht, die restlichen 14 Kommunen sind Gemeinden ohne Stadtrecht. Vier der 14 Gemeinden verfügen über das Marktrecht ohne Stadtrecht und heißen offiziell "Marktflecken" (siehe unten).

Die Kommunen des Landkreises lauten wie folgt (Einwohner am ):

Die Kreisstadt Limburg a. d. Lahn verfügt über eine Exklave (siehe nebenstehende Karte): Ein Teil des Limburger Stadtteils Staffel ist nicht mit Limburg verbunden, sondern nur mit der Gemeinde Elz in Hessen und der Ortsgemeinde Gückingen in Rheinland-Pfalz.

Seit rund 50 Jahren besteht eine enge, von lebendiger Aufgeschlossenheit geprägte Freundschaft mit dem Bezirk Friedrichshain-Kreuzberg von Berlin. Im Mai 1962 – wenige Monate nach dem Mauerbau – fuhren Vertreter des früheren Oberlahnkreises an die Spree, um dort ihre Solidarität mit den Menschen der zweigeteilten Stadt zu bekunden. Sie übernahmen inmitten Kreuzbergs, in der Hagelberger Straße, die Patenschaft für ein Kinderheim, das fortan "Weilburger Land" heißt. Damit legten sie den Grundstein zur offiziellen Partnerschaft, die am 22. März 1980 und 28. Juni 1980 in Berlin und Limburg feierlich begründet wurde.
Was diese Partnerschaft besonders auszeichnet, sind die vielfältigen persönlichen Bindungen und Kontakte zwischen Vereinen und Schulen, Begegnungen von Sportlern, Polizeibeamten oder in den Personalräten aus Limburg-Weilburg und Berlin.



</doc>
<doc id="9013" url="https://de.wikipedia.org/wiki?curid=9013" title="Raymond Queneau">
Raymond Queneau

Raymond Queneau (* 21. Februar 1903 in Le Havre; † 25. Oktober 1976 in Neuilly bei Paris) war ein französischer Dichter und Schriftsteller. Sein bekanntestes Werk ist der Roman "Zazie dans le métro".

Schon in seiner Jugend hatte Raymond Queneau Kontakt zu den französischen Surrealisten. 1949 gehörte er zu den ersten Unterzeichnern des Manifestes des "Collège de ’Pataphysique" (Künstlergruppe ’Pataphysik), das auf den Dichter und Dramatiker Alfred Jarry zurückgeht – er erhielt 1977 auch die höchste Auszeichnung ("Satrape") dieser Institution. In der Folge gründete er zusammen mit François Le Lionnais die Künstlerbewegung OuLiPo, der auch die Schriftsteller Georges Perec und Italo Calvino angehörten. Gleichzeitig arbeitete er als Direktor der Encyclopédie de la Pléiade für den angesehenen Verlag Gallimard. 1951 wurde er Mitglied der Académie Goncourt. Mit seinem 1959 erschienenen Roman "Zazie dans le métro" (deutsch: "Zazie in der Metro"), der die Abenteuer eines Landmädchens in Paris beschreibt, wurde Queneau berühmt. Das Buch wurde im Folgejahr von Louis Malle verfilmt (deutscher Titel "Zazie").

Queneaus Werke wurden größtenteils von Eugen Helmlé ins Deutsche übersetzt.





Die Anekdote „Autobus S“ wird in 99 Varianten erzählt und dient dem Experimentieren Queneaus mit der französischen Sprache durch alle Stile, Slangs und Dialekte bis hin zur Lautschrift. Die Lektüre ist wegen der Sprach- und Lautspiele im französischen Original zu empfehlen, wenngleich die deutsche Übersetzung von Harig/Helmlé als geistig ebenbürtig gilt. 2016 erschien, ebenfalls bei Suhrkamp, eine erweiterte Neuübersetzung von Frank Heibert und Hinrich Schmidt-Henkel (die Übersetzer wurden dafür mit dem Straelener Übersetzerpreis der Kunststiftung NRW 2017 ausgezeichnet).





</doc>
<doc id="9014" url="https://de.wikipedia.org/wiki?curid=9014" title="M. C. Escher">
M. C. Escher

Maurits Cornelis Escher (* 17. Juni 1898 in Leeuwarden, Provinz Friesland; † 27. März 1972 in Hilversum, Provinz Nordholland) war ein niederländischer Künstler und Grafiker, der vor allem durch seine Darstellung unmöglicher Figuren bekannt wurde.

M. C. Escher kam 1898 als jüngster von fünf Söhnen des Wasserbauingenieurs George Arnold Escher im Princessehof in Leeuwarden zur Welt. 1903 zog die Familie nach Arnheim, wo der junge Escher die schulische Grundausbildung absolvierte. Allerdings war er ein ziemlich schlechter Schüler, musste zwei Klassen wiederholen und hatte trotz seiner zeichnerischen Begabung sogar im Fach Kunst schlechte Noten. F. W. van der Haagen, Eschers Kunstlehrer auf der Oberschule unterwies ihn in der Technik des Linolschnitts. 1917 zog die Familie erneut um (nach Oosterbeek). Escher begann 1919 ein Architekturstudium in Haarlem, das er schon nach einer Woche abbrach. Sein dortiger Lehrer Samuel Jessurun de Mesquita, ein portugiesischstämmiger Jude, erkannte die außerordentliche Begabung und unterrichtete ihn weiter in grafischen Techniken. Escher beherrschte bald die Holzschnitttechnik vollkommen. Mesquitas starke Persönlichkeit hat ebenfalls großen Einfluss auf Eschers weitere Entwicklung zum Grafiker ausgeübt.

Ab 1921 bereiste er mehrfach Italien, meist zu Fuß oder auf dem Esel, sowie Spanien, wo er sich mit arabischer Ornamentik (Alhambra) auseinandersetzte. 1923 lernte er die Schweizerin Jetta Umiker kennen, die er 1924 in Viareggio heiratete. Das Paar ließ sich in der Nähe von Rom nieder. Am 23. Juli 1926 war die Geburt ihres ersten Sohnes Georg. Arthur, ihr zweiter Sohn wurde am 8. Dezember 1928 geboren. 

Während der 1920er Jahre erlangte Escher eine gewisse Popularität und hatte 1929 gleich fünf Ausstellungen in den Niederlanden und der Schweiz. Bis 1937 entstanden überwiegend mediterrane Landschaftsbilder, darunter die große Lithografie eines kleinen Abruzzendorfes ("Castrovalva" 1930). Vor allem in den USA hatte Escher Aufmerksamkeit erregt.

Aus Abneigung gegen den italienischen Faschismus verlegten die Eschers ihren Wohnsitz 1935 in die Schweiz nach Château-d’Œx, nachdem sie sich eine Weile in Steckborn aufgehalten hatten. Auf einem Frachtschiff bereiste er die Adria, Sizilien und die Riviera. Nach einem zweiten Besuch der Alhambra 1936 veränderte sich Eschers Thematik, die Periode der Metamorphosen ("Tag und Nacht" 1938) begann. 1937 folgte ein weiterer Umzug in die Nähe von Brüssel und er experimentierte zunehmend mit Flächenfüllungen (Parkettierung). 

Als die Nationalsozialisten in Brüssel einmarschierten, zog die Familie erneut um, ins niederländische Baarn. Sein Lehrer Mesquita wurde 1944 von den deutschen Besatzern verschleppt und im Konzentrationslager Auschwitz ermordet. Escher konnte zumindest einen Großteil von Mesquitas Werk retten.

Nach Kriegsende erlernte Escher die Mezzotintotechnik und wandte sich ab 1946 verstärkt perspektivischen Bildern ("Oben und Unten" 1947) zu. Er erhielt vermehrt gut bezahlte Aufträge, verkaufte viele seiner Drucke und war 1950 in den USA ein gefragter Künstler. Sein großer Durchbruch in Europa erfolgte im September 1954, als ihm das Stedelijk Museum eine Einzelausstellung in Amsterdam anlässlich eines gleichzeitig dort abgehaltenen Mathematiker-Kongresses gewährte. 1955 wurde Escher die Ritterwürde des "Oranje-Nassau-Ordens" verliehen. Die beiden letzten Ausstellungen zu seinen Lebzeiten fanden 1969 im Rheinischen Landesmuseum Bonn und in der Kunsthalle Bern statt.

1964 erkrankte er, musste operiert werden, und zwei Jahre nach einer zweiten Operation starb er im engsten Familienkreis im "Rosa-Spier-Haus" in Hilversum.

Escher schuf vor allem grafische Arbeiten und brachte es in den Techniken des Holzschnitts, des Holzstichs und der Lithografie zur technischen Perfektion. Als junger Mann reiste er durch Italien und den Mittelmeerraum bis Portugal und schuf viele Landschaftsdarstellungen und Architekturstudien in einer breiten Palette grafischer Stile. Die Motive der südlichen Landschaften bildeten zugleich den Formen-Kanon seiner späteren „naturfremden“ Arbeiten.

Durch seine vielen Reisen verband Escher eine besondere Liebe mit dem Meer und der Schifffahrt. So hielt er noch im Alter Vorträge über die mannigfachen Naturphänomene und Eindrücke, die einem Schiffsreisenden im Mittelmeer begegnen.

Seine bekanntesten Werke, die Escher nahezu den Status eines "Popstars" einbrachten, beschäftigen sich mit der Darstellung perspektivischer Unmöglichkeiten, optischer Täuschungen und multistabiler Wahrnehmungsphänomene. Man sieht Objekte oder Gebäude, die auf den ersten Blick natürlich zu sein scheinen, auf den zweiten aber vollkommen widersprüchlich sind („unmögliche Figuren“).

Die von Lionel Penrose entworfene unendliche Penrose-Treppe bildete die Grundlage zu Eschers Bild "Wasserfall". Es zeigt einen Wasserlauf, der sich von einem Wasserrad im Vordergrund im Zick-Zack vom Betrachter fortbewegt, jede Ecke Teil von insgesamt zwei Türmen auf Säulen. Schließlich läuft das Wasser als ein Wasserfall im Vordergrund nach unten und streicht über das Wasserrad vom Anfang und macht die Konstruktion scheinbar zu einem Perpetuum Mobile. Das Wasser läuft größtenteils bergauf, gleichzeitig scheinbar in immer weitere Ferne, obwohl die Ecken des Wasserlaufs trotzdem abwechselnd in einem der beiden Türme liegen. Das Bild "Treppauf Treppab" zeigt eine auf ähnliche Weise konstruierte viereckige, endlose Treppe.

Die "unmögliche Lattenkiste" ist Ausgangspunkt des Bildes "Belvedere". Ober- und Untergeschoss eines Aussichtspavillons in einer norditalienischen Landschaft sind um 90° gegeneinander verdreht. Eine Leiter, die auf dem Boden des Untergeschosses steht, lehnt an der Außenwand der oberen Etage. Die Säulen, die das Gebäude tragen, wechseln unmerklich die Seiten. Das paradoxe Gebäude wirkt dennoch auf den ersten Blick völlig stabil.

Seine Lithographie "Drawing Hands" (1948) zeigt zwei einander gegenüberliegende Hände, die sich in einem paradox wirkenden Vorgang gegenseitig zeichnen.
Ausgehend von den Metamorphosen der Ornamental-Kunst der maurischen Majolika, die Escher in Südspanien studiert hatte, entwickelte er in seinen Bildern "Metamorphose I" bis "Metamorphose III" und weiteren eine Technik der regelmäßigen Flächenfüllung durch teilweise fantastische Figuren. Das verfeinerte er, indem er in diese Flächenmuster immer wieder leichte Variationen (Vogel Fisch, 1938 oder Luft und Wasser, 1938) einfließen ließ, so dass sich die verwendeten Figuren verwandeln und beispielsweise Vögel zu Fischen werden und die Ebene füllen.

Escher widmete sich in seinen Arbeiten auch Themen wie Möbiusbändern, Kristallformen, Spiegelungen, optischen Verzerrungen, Fraktalen und "Unendlichkeitsannäherungen" . Bekannt ist ein Selbstporträt in der Spiegelung einer Glaskugel.

Die „Kaleidozyklen“ hat er nicht erfunden. Das sind aus mindestens acht Tetraedern bestehende Körper, die sich so drehen lassen, dass der Betrachter alle Seiten des Dreiecks sieht. Diese Körper sind vielmehr eine Weiterführung seines Werkes durch die amerikanische Mathematikerin Doris Schattschneider und den Künstler Wallace G. Walker, die Herausgeber des Buches „M. C. Escher Kaleidozyklen“.

Escher schuf auch einige gewerbliche Arbeiten, so ein 58 Meter langes "Metamorphose"-Wandbild für die Hauptpost in Den Haag, das sich heute im Flughafen Schiphol befindet, eine Säulengestaltung für eine Schule und eine Keksdose in Form eines Dodekaeders mit Seestern-Motiv in limitierter Auflage. Seine Entwürfe für Geldscheine, für die er an einem Auswahlverfahren teilgenommen hat, scheiterten daran, dass sie nicht fälschungssicher waren. Ebenso nicht verwirklicht wurden Entwürfe für Briefmarken.

Escher ist für die Kunstgeschichte immer ein Problem geblieben. Seine Auseinandersetzung mit perspektivischen Unmöglichkeiten und optischen Täuschungen unterscheidet sich stark von den klassischen Themen bildender Kunst und lässt sich in keine der klassischen Schubladen einordnen. So wurde Escher von der Kunstwelt lange Zeit nicht als Künstler im klassischen Sinne akzeptiert.

Im Gegensatz dazu wurde Escher schon früh von Wissenschaftlern und Mathematikern sehr geschätzt, da seine sauberen, exakten Arbeiten sich auf eine intuitive und sinnliche Weise mathematischen Themen annähern und Problemstellungen der Wissenschaft illustrieren. Escher wurde nicht selten zu Mathematik-Vorlesungen eingeladen, obwohl er von sich selbst sagte, er verstünde nichts von Mathematik. Er hielt auch selbst stark frequentierte Vorlesungen über seine Arbeit in ganz Europa.

Besondere Aufmerksamkeit bekam er auch durch den 1979 veröffentlichten Bestseller "Gödel, Escher, Bach" von Douglas R. Hofstadter, der das Werk des Mathematikers Kurt Gödel, die Illustrationen von Escher und die Musik von Johann Sebastian Bach in Beziehung zueinander setzt.

Das Paradoxe und nicht selten Mystische seiner geheimnisvollen Bilder fand auch Anklang bei Esoterikern und der Popkultur des zwanzigsten Jahrhunderts. Seine Bilder wurden als Poster gedruckt und als Plattencover verwendet.

2002 wurde im ehemaligen Palais der Königin Emma ein eigenes "Escher-Museum" eingerichtet, das neben seinem grafischen Werk auch Privatfotos und Arbeitsskizzen zeigt. Letztere vermitteln einen Eindruck davon, wie der Künstler Flächenfüllungen und unmögliche Geometrien entworfen hatte.

2006 wurde der am 16. September 1985 entdeckte Asteroid (4444) Escher nach ihm benannt.




</doc>
<doc id="9016" url="https://de.wikipedia.org/wiki?curid=9016" title="Schellack">
Schellack

Schellack, Tafellack, Plattlack oder Lacca in tabulis, auch fälschlich als "Gummi Lacca" (Gummilack) und "Lackharz" bezeichnet, ist eine harzige Substanz, die aus den Ausscheidungen der Lackschildlaus "Kerria lacca" (Pflanzenläuse, Familie Kerridae) nach ihrem Saugen an bestimmten Pflanzen gewonnen wird.

Schon vor mehr als 3000 Jahren wurde "Lac" in frühen indischen Sanskrit-Schriften erwähnt. In der 5. "Suktha" des 5. Buches der Atharvaveda werden unter dem Titel „Laksha“ das Vorkommen von "Lac", das "Lac-Insekt" und auch die Verwendung von "Lac" für medizinische Zwecke beschrieben. "Lac" und "Laksha" stammen mit großer Wahrscheinlichkeit von dem Sanskrit-Wort "laksa" oder Hindi-Wort "Lakh" (Bedeutung: ‚100.000‘) ab und deuten auf die große Anzahl der auf Bäumen Indiens und Thailands anzutreffenden Schildläuse hin, die dieses Harz produzieren. Der Name passt dazu, dass etwa 300.000 Läuse pro Kilogramm Lack benötigt werden. Das Wort Lack und das gehen wiederum auf "Lac" zurück. Als erster Europäer hat Jan Huygen van Linschoten, der in portugiesischen Diensten Indien bereiste, bereits 1596 über "Lac" berichtet. Das Wort „Schellack“ leitet sich von einer Zusammensetzung aus dem "Schale"; "Schuppe" und „lak“ "Lack" ab. Die East India Company exportierte schon im 17. Jahrhundert zunächst Lac Dye (den im Lac enthaltenen Farbstoff "Laccainsäure") und später Schellack nach Europa. Schellack war das erste industriell genutzte Harz mit vielen bedeutenden Anwendungen in der "Farben"- und "Lack"-, "Druckfarben"-, Elektroindustrie usw. Die wohl bekannteste Anwendung war der Einsatz von Schellack als Bindemittel in Schallplatten.

Hauptsächlich in Süd- und Südostasien und hier speziell in Indien und Thailand wurde früher Schellack in großen Mengen gewonnen. Die Jahresproduktion belief sich auf etwa 50.000 Tonnen.

Kolonien von Lackläusen parasitieren auf verschiedenen Bäumen, meist Pappelfeige "Ficus religiosa", "Ficus indica", und Jujube "Zizyphus jujuba", Indische Jujube "Ziziphus mauritia" und Malabar-Lackbaum "Butea frondosa", Kusumbaum "Schleichera oleosa", Regenbaum "Samanea saman" sowie "Croton lacciferus". Die Weibchen der Lackschildläuse ernähren sich von den Pflanzensäften dieser Bäume, sie stechen den Baum an, nehmen seinen Saft auf und scheiden die harzartigen Substanzen über ihren Körper wieder aus, der Saft wird in der Laus chemisch verändert. Jede weibliche Laus sondert Saft ab, um die gelegten Eier zu schützen. Der Lack von tausenden Läusen verbindet sich zu einer harten Hülle über den Schwarm. Die mit Unmengen solcher Läuse dichtbesetzten Zweige werden allmählich bis zu 1 cm dick überkrustet und dadurch selbst zum Absterben gebracht. Die alten Läuse, die zu einer mit roter Farbe gefüllten Blase aufgeschwollen sind, sterben ab; nach 6 bis 7 Monaten schlüpfen die jungen Läuse aus der Hülle heraus. Nach dem Schlüpfen werden die Zweige mit dem Lack geerntet, diese Harzabscheidung ist das Ausgangsprodukt für die Schellackgewinnung. Bevor der "Brutlack" geerntet wird, werden rechtzeitig einige mit Jungläusen "bewohnte" Zweige abgeschnitten und auf neue Bäume gesetzt, um Nahrung für die nächste Generation zu bieten.

"Quellen zur Verarbeitung und Eigenschaften:" 

Die harzig umkrusteten „Brutlackzweige“ werden abgeschnitten, gesammelt (Rohschellack, "Stocklack", "Stangenlack", "Gummilack", "Lacca in ramulis", englisch "Sticklac") und das Harz vom Holz getrennt, unter "Lacca en grappes" (englisch "Seedlac") versteht man die von den Zweigen abgelösten Harzstücke. "Stocklack" enthält ca. 60–80 % Reinschellack.

Der "Stocklack" wird gereinigt und gemahlen und zur Entfernung des wasserlöslichen roten Farbstoffs Laccainsäure mit kaltem Wasser oder schwacher Sodalösung gewaschen und anschließend als "Körnerlack" oder "Lacca in granis" getrocknet, dabei werden auch Holzpartikel und Sand entfernt; aus der Waschlösung wird der abgetrennte Farbstoff als Färberlack ("Lac-Dye") gewonnen. Der verbleibende wasserunlösliche Farbstoff Erythrolaccin gibt dem "Körnerlack" die charakteristische gelblichbraune Eigenfarbe, durch Zusammenschmelzen des Granulats wird "Blocklack" hergestellt. Der getrocknete "Körnerlack" enthält noch sehr feine Verunreinigungen. Diese werden in mehreren Stufen ausgesiebt und ergeben verschiedene Feinheitsgrade wie den sehr feinen Rückstand "Molamma", der sich bei der weiteren Verarbeitung entzünden würde.

Das als Erdschellack bezeichnete Harz wird aus verschiedenen Arten von Grasbäumen gewonnen. Mit dieser pflanzlichen Herkunft unterscheidet sich rotes und gelbes Akaroidharz prinzipiell von Schellack.

Zur Herstellung des Schellacks wird der "Körnerlack," unter Zusatz von Kolophonium zur Förderung der Schmelzung, durch Schmelzen in Baumwollschläuchen bei etwa 140 °C und Filtration durch Auswringen oder Pressen und Ausgießen auf heiße drehende Ton- oder Messingzylinder oder Pisangblätter verarbeitet und dünn ausgezogen ("Handmade Shellac"). Bei der gebräuchlichen Schmelzfiltration ("Machinemade Shellac") wird der in einer Wanne oder über heißen Röhren aufgeschmolzene "Körnerlack" unter hohem Druck durch einen Filter (mitunter Leinentücher) gepresst. Der flüssige Schellack wird auf einem Abrollband zu einem dünnen Film gezogen, der beim Abkühlen bricht. Der natürliche Wachs­anteil von 3 % bis 6 % bleibt bei diesen Verfahren im Schellack erhalten.

Der bei Verarbeitung anfallende Rückstand in den Stoffschläuchen wird herausgeholt und ergibt den "Kirilac," daraus wird durch Lösemittelextraktion "Garnet Schellack" hergestellt. Der verbleibende Rückstand im Gewebe der Stoffschläuche, "Passewa", wird herausgeschmolzen und zu Kuchen gepresst.

Der Schellack kommt in kleinen, dünnen, eckigen, tafelartigen Bruchstückchen, die durchscheinend, glänzend, hart, mit muscheligem Bruch sind, als "Blätterschellack" in den Handel. Weitere Handelsformen entstehen durch Zusammenschmelzen aus "Körnerlack", Klumpen- oder Blocklack "Lacca in massis" oder, in Form von Kuchen, als "Kuchenlack". Eine weitere Form sind runde Scheiben mit ca. 5−8 cm Durchmesser ("Blatt-" oder "Knopfschellack"), die durch Auswringen der Baumwollsäcke und Auftupfen auf Bleche entstehen.

Wird der farbstoffreiche "Stocklack" mit einer Alkohollösung warm filtriert und der Alkohol unter Zugabe von Kolophonium gegen ein Überhitzen des Lacks abdestilliert, so bildet er eine blutrote Masse, die, auf Blech ausgegossen, den Blutlack, "Rubinschellack" oder "Lacca sanguinea" ergibt.

Die im Lösemittelextraktionsverfahren hergestellten Schellacke werden aufgrund des Verfahrens und des Aussehens als wachsfreie Blätterschellacke bezeichnet.

Bei diesem Verfahren wird "Körnerlack" in Alkohol (üblicherweise Ethanol) gelöst. Durch verschiedene Filtrationsstufen werden Schellackwachs und Verunreinigungen entfernt, bevor der im "Körnerlack" verbliebene Farbstoff Erythrolaccin durch Einwirken von Aktivkohle reduziert wird. Nach dem Verdampfen und der Rückgewinnung des Alkohols wird der flüssige Schellack auf einem Abrollband zu einem Film gezogen, der beim Abkühlen bricht und die typische Blätterstruktur ergibt. Für bestimmte Anwendungen, bei denen wachshaltige Blätterschellacke benötigt werden, kann das gereinigte Schellackwachs vor dem Verdampfen des Alkohols der Schellacklösung wieder zugegeben werden.

Je nach Art des eingesetzten "Körnerlacks" und dem Grad der Entfärbung durch die Aktivkohle können sehr helle bis dunkelrote wachsfreie Blätterschellacke hergestellt werden. Die Farbzahl wird in alkoholischer Lösung nach "Lovibond" oder "Gardner" bestimmt.

"Körnerlack" wird wässrig-alkalisch gelöst, entwachst, filtriert und durch Einwirken von Chlorbleichlauge (NaOCl) gebleicht. Die noch vorhandenen Farbstoffe "Laccainsäure" und "Erythrolaccin" werden hierdurch vollständig zerstört, sodass eine klare, nahezu farblose Lösung vorliegt. Nach dem Auswaschen der Chlorbleichlauge wird der gelöste Schellack durch Zugabe von Schwefelsäure ausgefällt, es kann auch Essigsäure verwendet werden (säurefreier Lack). Das so erhaltene weiße Pulver wird als weißer, gebleichter Schellack "Lacca alba" getrocknet, wobei eine Restfeuchte von 2 bis 6 % verbleibt. Für einige Anwendungen werden auch gebleichte wachshaltige Schellacke angeboten.

Durch die vollständige Zerstörung der verbleibenden Farbstoffe können auch sehr dunkle "Körnerlacke," etwa aus Thailand, und "Kirilac", der dunkle Rückstand des "Schmelzfiltrationsverfahrens", als Rohstoffe eingesetzt werden. Von Nachteil sind die geringe Lagerbeständigkeit und die Veränderung der Löslichkeit aufgrund einer durch das Bleichen ausgelösten "Selbstvernetzung" der Carboxygruppen. Auch ist der gebleichte Lack weniger hart und elastisch.

Während es bis 1961 noch mehrere Schellackhersteller in Deutschland mit Werken in Mainz, Hamburg und Bremen gab, die sich damals zur "Kalkhof GmbH Petersen & Stoever KG" zusammengeschlossen hatten, ist mit Stand 2014 nur die Produktionsstätte der "SSB (Stroever Schellack Bremen)" in Bremen als Europas einzige Schellack-Fabrik übrig geblieben.

Stocklack besteht aus "freien" und "veresterten" aliphatischen und aromatischen Hydroxysäuren (Schellolsäure, Aleuritinsäure) 65–75 %, 4–8 % Farbstoff, Bitterstoff, sowie etwas Wachs (Schellackwachs; rotbraun, spröde, sehr hart) (Ceryllignocerat, Cerylcerotinat und Wachsalkohole).
Das rein natürliche Granulat ist biologisch abbaubar und physiologisch unbedenklich. Durch verschiedene Verarbeitungsverfahren lässt sich die Helligkeit der Farbe des Lackes variieren. Für besondere Anwendungen wird außerdem der natürliche Wachsgehalt von 3 bis 6 % fast vollständig herausgefiltert. Die Qualität des Lacks ist von der Wirtsspezies abhängig; als beste Sorte gilt der von Kesambi "Schleichera trijuga".

Schellack ist in der Kälte und bei Zimmertemperatur sehr spröde und brüchig, ziemlich hart, geruch- und geschmacklos. Er schmilzt beim Erhitzen und verbreitet bei höherer Temperatur einen angenehmen Geruch. Er brennt mit hell leuchtender Flamme. Der Schmelzbereich liegt bei 65 bis 85 °C, die Säurezahl liegt bei 60–75 mg/g.

Schellack ist unlöslich in Wasser, quillt aber bei Kontakt mit Wasser und ist daher nicht wasserfest. Er ist löslich in Ethanol (bis auf das beigemengte Wachs), Borax, Ammoniak und Alkali-Carbonaten und in einer Mischung von einem Teil Wasser und zehn Teilen Aceton, jedoch nicht in wässrigen Säuren. Dies ist darauf zurückzuführen, dass die Moleküle des Harzes Carbonsäuren enthalten, die in basischen Lösungen zu Ionen deprotoniert werden. Das vorher unpolare und hydrophobe organische Molekül wird dadurch polar genug, um von Wasser gelöst zu werden.

Schellack ist je nach Sorte farblos, gelb bis bernsteinfarbig (Lemonschellack), orange bis braunrot und nimmt beim Kneten und Ausziehen einen seidenartigen Glanz an.


Schellack ist in zahlreichen Anwendungen ein Vorläufer synthetischer Harze und behielt in einigen wenigen Einsatzgebieten seine Bedeutung. In der Farben- und Lackindustrie werden jährlich über 5000 Tonnen Farben auf Schellackbasis produziert. Hervorzuheben ist außerdem die Unbedenklichkeit in Nahrungsmitteln und Pharmazeutika.






</doc>
<doc id="9019" url="https://de.wikipedia.org/wiki?curid=9019" title="Kleinsche Flasche">
Kleinsche Flasche

Die Kleinsche Flasche (auch Kleinscher Schlauch) wurde erstmals 1882 von dem deutschen Mathematiker Felix Klein beschrieben. Sie ist ein Beispiel einer nicht-orientierbaren Fläche. Umgangssprachlich formuliert hat sie die Eigenschaft, dass "innen" und "außen" nicht unterschieden werden können, oder anders formuliert, dass sie nur eine einzige "Seite" besitzt, die gleichzeitig "innen" und "außen" ist. Auf der Kleinschen Fläche kann deshalb, so wie beim Möbiusband, kein stetiger Normalenvektor definiert werden. Im Gegensatz zum Möbiusband hat diese Fläche keinen Rand.

Man beginnt mit einem Quadrat und klebt die Ecken und Ränder mit den entsprechenden Farben zusammen, so dass die Pfeile zueinander passen. Dies ist in der nachfolgenden Skizze dargestellt. Formell gesagt wird die Kleinsche Flasche beschrieben durch die Quotiententopologie des Quadrates formula_1 mit Kanten, welche die folgenden Relationen erfüllen: formula_2 für formula_3 und formula_4 für formula_5.

Das Quadrat ist ein Fundamentalpolygon der Kleinschen Flasche.

Man beachte, dass diese Beschreibung das „Kleben“ in einem abstrakten Sinn meint, das versucht, die dreidimensionale Kleinsche Flasche mit sich selbst überkreuzenden Kanten zu konstruieren. Faktisch hat die Kleinsche Flasche keine sich überkreuzenden Kanten. Dessen ungeachtet ist es eine Möglichkeit, dieses Objekt in seiner Konstruktion zu veranschaulichen.

Man klebe die roten Pfeile des Quadrats zusammen (linke und rechte Kanten), so dass man einen Zylinder erhält. Man ziehe den Zylinder etwas auseinander und klebe weiterhin die Enden so zusammen, dass die Pfeile auf den Kreis passen. Dabei wird die Kreisfläche der einen Zylinderfläche durch die der anderen geschoben. Beachte, dass dieser Vorgang zur Überkreuzung von Kanten führt. Man bezeichnet dies als Immersion der Kleinschen Flasche im dreidimensionalen Raum. 
Bettet man die Kleinsche Flasche in den vierdimensionalen reellen Raum ein, kann eine Selbstdurchdringung vermieden werden. Anschaulich geschieht dies folgendermaßen: Man nimmt die oben abgebildete Immersion in den dreidimensionalen Raum und belässt die vierte Koordinate zunächst bei null. In der Nähe der Selbstdurchdringung erhöht man den Wert der vierten Koordinate für eine der (lokalen) Komponenten stetig auf eins und senkt sie danach wieder ab. Grafisch lässt sich die vierte Koordinate durch eine unterschiedliche Farbwahl veranschaulichen.

Wie das Möbiusband ist die Kleinsche Flasche eine zweidimensionale differenzierbare Mannigfaltigkeit, die nicht orientierbar ist. Im Gegensatz zum Möbiusband kann die Kleinsche Flasche nur mit Selbstdurchdringung in den dreidimensionalen Euklidischen Raum formula_6 abgebildet (also nicht eingebettet, sondern nur immergiert) werden. Ohne Selbstdurchdringung ist eine Einbettung aber in den formula_7 und höhere Dimensionen möglich.
Eine immergierte Kleinsche Flasche kann für formula_8 und formula_9 durch folgende Gleichungen im formula_6 dargestellt werden:
wobei formula_12 ist. formula_13 ist die ungefähre Breite, formula_14 die ungefähre Höhe der Figur. Übliche Werte: formula_15, formula_16.

Anmerkung: Die Kleinsche Flasche lässt sich so zerteilen, dass zwei Möbiusbänder daraus entstehen (siehe die Abbildung rechts).

Die Fundamentalgruppe der Kleinschen Flasche hat die Präsentation
Die Homologiegruppen sind 
Die Kleinsche Flasche ist die nicht-orientierbare geschlossene Fläche vom Geschlecht 2.

Es gibt eine 2-blättrige Überlagerung der Kleinschen Flasche durch den Torus.



</doc>
<doc id="9023" url="https://de.wikipedia.org/wiki?curid=9023" title="800">
800












</doc>
<doc id="9024" url="https://de.wikipedia.org/wiki?curid=9024" title="Kriton">
Kriton

Der Kriton (altgriechisch "Krítōn") ist ein in Dialogform verfasstes Werk des griechischen Philosophen Platon. Den Inhalt bildet ein fiktives, literarisch gestaltetes Gespräch. Platons Lehrer Sokrates diskutiert mit seinem Freund und Schüler Kriton, nach dem der Dialog benannt ist.

Sokrates ist wegen Asebie (Gottlosigkeit) und Verführung der Jugend zum Tode verurteilt worden. Er befindet sich im Gefängnis, wo er auf seine bald bevorstehende Hinrichtung wartet. Kriton besucht ihn, um ihn zur Flucht zu bewegen. Sokrates lehnt jedoch den Vorschlag ab. Er erläutert seine Entscheidung ausführlich. Sie beruht auf den philosophischen Grundsätzen, zu denen er sich bekennt: Konventionelle Ansichten sind belanglos, maßgeblich ist nur die Vernunft, die Richtschnur hat unter allen Umständen die Gerechtigkeit zu sein. Man darf Unrecht nicht mit Unrecht vergelten und generell nichts Schlechtes tun; Verpflichtungen sind einzuhalten. Diese Prinzipien sind wichtiger als die Rettung des Lebens. Auch einem ungerechten Gerichtsurteil darf sich ein Bürger nicht entziehen, da er sonst die Gültigkeit der Gesetze und damit die Grundlage des geordneten Zusammenlebens im Staat verneinen würde, was ein Unrecht wäre. Es wäre ein Verstoß gegen die Loyalitätspflicht des Bürgers gegenüber der staatlichen Gemeinschaft. Kriton kann auf die Argumente des Sokrates nichts erwidern.

In der altertumswissenschaftlichen Forschung, aber auch in modernen philosophischen Debatten wird der im "Kriton" thematisierte unbedingte Gesetzesgehorsam kontrovers diskutiert. Im philosophischen Diskurs geht es dabei um den Gewissenskonflikt, der entsteht, wenn eine gültige Rechtsnorm zu einem Verhalten zwingt, das aus der Sicht eines Betroffenen ein offenkundiges schweres Unrecht darstellt. In der philosophiegeschichtlichen Forschung wird die Frage, ob der "Kriton" als Plädoyer für bedingungslosen Gehorsam zu verstehen ist, unterschiedlich beantwortet; „autoritäre“ Deutungsmodelle konkurrieren mit „liberalen“. Über die Stichhaltigkeit der Argumentation im Dialog gehen die Meinungen auseinander.

Das Gespräch, dessen literarische Darstellung möglicherweise auf einer historischen Begebenheit fußt, findet 399 v. Chr. im Gefängnis von Athen statt. Dort ist Sokrates in Haft, seit vor rund vier Wochen das Todesurteil gegen ihn verhängt wurde. Die Hinrichtung wird für den nächsten oder übernächsten Tag erwartet. Der Ort des Gefängnisses kann nicht sicher bestimmt werden. Es befand sich wohl in der Nähe des Gerichtshofs Heliaia, nach Grabungsergebnissen vermutlich etwa 100 Meter südwestlich dieses Gebäudes, knapp außerhalb des Geländes der Agora.

Platon zeichnet von seinem verehrten Lehrer Sokrates ein lebensnah wirkendes Bild. Da es sich aber um ein literarisches Werk handelt, ist stets zu beachten, dass die Ansichten und Argumente, die Platon seiner Dialogfigur in den Mund legt, nicht mit denen des historischen Sokrates identisch sein müssen.

Der einzige Gesprächspartner des Philosophen, sein Freund und Altersgenosse Kriton, war eine historische Gestalt. Er war ein wohlhabender Athener, der wie Sokrates aus dem Demos Alopeke stammte. Als Sokrates angeklagt wurde, bot Kriton vergeblich an, sich bei Verhängung einer Geldstrafe für deren Zahlung zu verbürgen. Nach dem Todesurteil war er bereit, beim Gericht dafür Bürgschaft zu übernehmen, dass Sokrates nicht fliehen würde. Damit wollte er seinem Freund den Gefängnisaufenthalt ersparen, doch wurde auch dieser Vorschlag abgelehnt. Sowohl beim Prozess als auch bei der Hinrichtung war Kriton unter den Anwesenden.

Platon lässt Kriton auch in anderen Dialogen – im "Euthydemos" und im "Phaidon" – auftreten. Dabei fällt auf, dass Kriton als konventionell denkender Athener der Philosophie des Sokrates, den er persönlich sehr schätzt, trotz aufrichtigen Bemühens bis zuletzt fremd gegenübersteht.

Der Vorschlag und die Argumente Kritons

Kriton hat sich schon sehr früh zum Gefängnis begeben, lange vor Tagesanbruch und vor dem Beginn der regulären Besuchszeit. Mit besonderer Genehmigung des Gefängniswärters ist er eingetreten; schweigend hat er sich neben den schlafenden Philosophen gesetzt. Inzwischen hat die Morgendämmerung eingesetzt, Sokrates ist erwacht und zeigt sich verwundert, den Freund so früh zu sehen. Kriton erklärt, dass er ihn nicht vorzeitig wecken wollte. Er äußert sein Staunen über die Leichtigkeit, mit der Sokrates sein Schicksal auf sich nimmt. Dieser weist darauf hin, dass es in seinem Alter – er ist siebzigjährig – unangebracht wäre, über den ohnehin nahenden Tod unwillig zu sein.

Der Anlass für das frühe Kommen des Besuchers ist die bevorstehende Hinrichtung des Häftlings. Kriton erwartet sie für den folgenden Tag, Sokrates für den übernächsten. Für Kriton, der seinen Freund überreden will, in der kommenden Nacht zu fliehen, drängt daher die Zeit. Er hat bereits einen Fluchtplan entworfen und will das Vorhaben durch Bestechung der Gefängniswärter bewerkstelligen. Dabei geht er davon aus, dass sein Vermögen ausreichen wird; sollte dies jedoch nicht der Fall sein, sind viele weitere Freunde gewillt, für die Bereitstellung der benötigten Summe zu sorgen. Dank dieser breiten Hilfsbereitschaft ist es, wie Kriton versichert, nicht erforderlich, dass sich ein Einzelner finanziell ruiniert. In Thessalien hat Kriton Freunde, die Sokrates schätzen und die ihn gerne aufnehmen und seine Sicherheit gewährleisten.

Kriton fürchtet einen Freund zu verlieren, wie er nie wieder einen finden wird. Mit mehreren Argumenten versucht er Sokrates zu überzeugen. Er hält es für die Pflicht der Freunde, in einer solchen Lage ihren Besitz zu opfern; daher könne Sokrates das Angebot ohne Bedenken annehmen. Im Fall der Hinrichtung befürchtet Kriton üble Nachrede; er meint, man werde dann die Freunde des Philosophen beschuldigen, sie hätten es aus Geiz und Feigheit versäumt ihn zu retten. Solchen Vorwürfen möchte sich Kriton nicht aussetzen. Überdies findet er es ungerecht, dass Sokrates, wenn er in den Tod geht, den Willen seiner Feinde erfüllt. Des Weiteren bringt Kriton vor, ein Vater trage Verantwortung für seine noch jungen Kinder. Wer wie Sokrates Kinder gezeugt habe, müsse sich um ihre Erziehung kümmern und dürfe sie nicht im Stich lassen und dem Schicksal von Waisen überlassen.

Die theoretischen Voraussetzungen der Argumentation des Sokrates

Nachdem Kriton seinen Vorschlag dargelegt, erläutert und begründet hat, geht Sokrates ausführlich darauf ein. Er will ergebnisoffen diskutieren und bittet Kriton, Einwände gegebenenfalls zu äußern. Kriton bringt aber gegen die folgenden Ausführungen nichts vor, denn die Argumentation seines Freundes erscheint ihm schlüssig. Er hört nur noch zu und äußert seine Zustimmung zu den einzelnen Gedankengängen oder sagt, dass er etwas nicht verstanden hat.

Zunächst erinnert Sokrates an den Grundsatz, dass nur objektives, rationales Abwägen eine Entscheidung bestimmen darf. Wenn subjektive Befürchtungen oder Bedürfnisse dem für richtig Befundenen entgegenstehen, darf ihnen kein Einfluss eingeräumt werden, anderenfalls ist man nicht mit den eigenen Prinzipien im Einklang. Belanglos ist auch der Umstand, dass manche Meinungen weit verbreitet sind, denn wichtig ist nicht die Anzahl, sondern nur die Kompetenz der Vertreter einer Auffassung. So wie ein Sportler nur dem Rat von Sportlehrern und Ärzten folgt und nicht dem einer Menge von Unkundigen, da er sonst seinen Körper schädigen würde, so ist auch in Fragen von Recht und Unrecht, gutem und schlechtem Handeln nur ein sachkundiges Urteil relevant. Die Meinungen der unwissenden Menge zählen nicht.

Wer seinen Körper aus Inkompetenz falsch behandelt hat und damit seine Gesundheit zerrüttet hat, dem erscheint das Weiterleben nicht mehr lohnend. Es gibt aber auch eine andere Art von Zerrüttung. Sie betrifft dasjenige im Menschen – „was es auch sein mag“ –, worauf sich Gerechtigkeit und Ungerechtigkeit beziehen. Mit dieser Umschreibung meint Sokrates die unsterbliche Seele. Sie wird nach seiner Überzeugung durch unrechtes Handeln geschädigt. Nach einer solchen Zerrüttung ist das Leben für den Philosophen nicht mehr lebenswert. Da die Seele weitaus edler und wichtiger ist als der Körper, ist ihre Schädigung viel schlimmer als eine körperliche Beeinträchtigung und auch schlimmer als der Tod. Nicht das Leben an sich ist erstrebenswert, sondern nur ein gutes Leben. Gut leben bedeutet tugendhaft leben, also stets gerecht sein. Dem stimmt Kriton zu. Somit ist die Frage, ob eine Flucht aus dem Gefängnis für einen zu Unrecht Verurteilten angebracht ist, nur unter dem Aspekt der Gerechtigkeit zu untersuchen und zu klären.

Kritons Überlegungen zum finanziellen Aspekt, zu einer möglichen Rufschädigung und zur Erziehung der Kinder nimmt Sokrates nicht ernst. Derartige Gesichtspunkte sind für ihn belanglos. Er vergleicht solche Beweggründe mit den Impulsen der vernunftlosen Leute, die jemand leichtsinnig zum Tode verurteilen und später ihre Tat gern wieder rückgängig machen würden, wenn das möglich wäre.

Sokrates erinnert seinen Freund an ihre gemeinsamen Überzeugungen, auf die sie sich vor langem verständigt haben und zu denen sie sich seither immer bekannt haben. Es wäre abwegig, diese durchdachten Grundsätze nun in fortgeschrittenem Alter einfach fallen zu lassen, als wären es nur kindische Einfälle gewesen. Den Ausgangspunkt bildet die Überzeugung, dass es ausnahmslos unter allen Umständen prinzipiell falsch ist, etwas Unrechtes zu tun. Unter Unrecht versteht Sokrates alles, was jemand schädigt. Das Schädigen hält er auch dann für absolut unzulässig, wenn es sich um Vergeltung für ein Unrecht handelt, das man selbst erlitten hat, beispielsweise das Beleidigen eines Beleidigers. Ein weiteres Prinzip lautet, dass eine Verpflichtung zu etwas Gerechtem, die man eingegangen ist, unbedingt einzuhalten ist. Kriton bekräftigt sein früheres Bekenntnis zu diesen Grundsätzen.

Die Anwendung der Theorie auf den aktuellen Fall

Wie Sokrates anschließend darlegt, ist nun die Frage zu untersuchen, ob er mit einer Flucht jemand schädigen oder eine gerechte Verpflichtung missachten würde. Hierzu trägt er das vor, was nach seiner Auffassung die Gesetze sagen würden, wenn sie reden und ihren Geltungsanspruch begründen könnten. Er lässt die personifizierten Gesetze das Wort ergreifen und die Position des Staates vertreten, wie es ein Redner täte, der die Gesetzlichkeit zu verteidigen hätte.

Die Gesetze würden geltend machen, dass ohne Respektierung ihrer Vorschriften die staatliche Gemeinschaft nicht bestehen könne. Sie würden einen in Kritons Sinn handelnden Sokrates fragen, ob er den Staat zugrunde richten wolle, indem er sich und damit jedem Bürger die Berechtigung zubillige, nach seinem persönlichen Ermessen rechtskräftige Gerichtsurteile zu missachten.

Darauf könnte Kriton – oder Sokrates, wenn er Kriton zustimmte – erwidern, er widersetze sich nicht dem gesamten Gesetzwesen, sondern nur einem Unrechtsurteil. Dann wäre aber an ihn die Frage zu richten, was er denn seiner Heimatstadt vorzuwerfen habe, deren Rechtswesen er mit seinem Verhalten untergrabe. Sokrates müsste, wenn er sich so verhielte, an die Grundlage seiner Existenz erinnert werden: Ihm wäre dann entgegenzuhalten, dass das Bestehen der staatlichen Ordnung die Voraussetzung dafür gewesen sei, dass sein Vater seine Mutter heiraten konnte. Dank dieser Ordnung sei er geboren worden, in ihrem Rahmen sei er gut erzogen worden. Den Gesetzen verdanke er wie jeder Athener alles Gute, was eine gesetzliche Ordnung den Staatsbürgern verschaffen könne. Daher sei das Vaterland mit seinen Gesetzen für ihn gleichsam ein Vater und habe noch mehr als ein Vater Anspruch auf seine Ehrerbietung und Loyalität. Wer die Verhältnisse und Gesetze in Athen missbillige, der könne mit seinem gesamten Besitz auswandern. Wer aber bleibe, der schließe damit implizit eine Vereinbarung mit dem Staat, durch die er sich zur Gesetzestreue verpflichte. Wenn er etwas im Justizwesen für verfehlt halte, sei es ihm anheimgestellt, die Ungerechtigkeit argumentativ aufzuzeigen; falls er dazu nicht in der Lage sei, habe er das geltende Recht zu respektieren. Dies gelte für Sokrates ganz besonders, denn er habe sein ganzes Leben in Athen verbracht und diesen Ort jedem anderen vorgezogen, sogar den Staaten, die er zu rühmen pflegte. Auch durch seine Familiengründung in seiner Heimatstadt habe er sein Einverständnis mit den athenischen Lebensverhältnissen demonstriert. Außerdem habe er in seinem Prozess die Verbannung als mögliche Alternative zur Hinrichtung abgelehnt und ihr den Tod ausdrücklich vorgezogen. Wenn er gewollt hätte, hätte er sich noch während des Prozesses für das Exil entscheiden und dann Athen auf legalem Weg verlassen können. Ein nachträglicher Versuch, eine freie und verbindliche Entscheidung einseitig rückgängig zu machen, sei schändlich.

Ferner führen die Gesetze gegen Kritons Meinung an, dass Sokrates, wenn er das Angebot annähme, mit der Flucht seine Helfer der Gefahr aussetzen würde, ebenfalls fliehen zu müssen oder ihr Vermögen einzubüßen. Hinzu komme, dass er als flüchtiger Gesetzesbrecher in einem gut eingerichteten Staat den Gutgesinnten suspekt wäre, denn man würde ihn verdächtigen, auch dort die Gesetze zu missachten. Er müsste also mit einer Gegend wie Thessalien vorliebnehmen, wo Unordnung und Zügellosigkeit herrschten. Dort könnte er mit der Geschichte von seinem jämmerlichen Entweichen Heiterkeit erregen. Als Philosoph, der seinen Grundsätzen untreu geworden ist, wäre er aber so diskreditiert, dass er seinen bisherigen Lebensinhalt, den philosophischen Dialog, aufgeben müsste. Dann bestünde sein Lebenssinn nur noch im Essen. Seine Kinder müsste er, wenn er sie nicht im Stich lassen wollte, nach Thessalien mitnehmen, wo sie heimatlos wären. Wenn er sie hingegen in Athen zurückließe, wäre zwar ihre gute Erziehung durch die Freunde gewährleistet, doch wäre sein Überleben ihnen dann zu nichts nütze.

Abschließend tragen die Gesetze eine eindringliche Mahnung vor: Wenn Sokrates nun stirbt, scheidet er aus dem Leben als einer, dem von den Menschen – nicht von den Gesetzen – Unrecht geschehen ist. Flieht er aber, so wird er seinerseits zum Unrechttäter gegen sich selbst, gegen seine Freunde, sein Vaterland und die Gesetze. Dann erwartet ihn als Übeltäter auch im Hades, dem Totenreich, Schlimmes.

Nachdem Sokrates das imaginäre Plädoyer der Gesetze beendet hat, bekennt er, davon selbst so ergriffen zu sein wie ein vom Klang der Flötenmusik berauschter Kulttänzer, ein Korybant. Dennoch fordert er Kriton auf, allfällige Gegenargumente zu äußern. Kriton weiß aber nichts einzuwenden, und Sokrates beendet das Gespräch mit dem Hinweis auf die göttliche Lenkung, der er sich anvertrauen will.

Ein zentraler Aspekt des "Kriton" ist die Betonung der Vernunft, die das einzige Kriterium für alle ethischen Entscheidungen liefern soll. Sokrates geht in diesem Dialog zuversichtlicher als in anderen Werken Platons von der objektiven Richtigkeit des Wissens aus, das dem Sachkundigen von der Vernunft vermittelt wird. Er nimmt also hier erkenntnistheoretisch eine relativ optimistische Haltung ein.

Über die ethischen und staatsphilosophischen Konsequenzen aus der Argumentation der personifizierten Gesetze bestehen in der Forschung erhebliche Meinungsverschiedenheiten. Schwierige Probleme, die sich in diesem Zusammenhang ergeben, werden im Dialog nicht geklärt, denn dessen Thema ist nur die Frage, wie Sokrates sich verhalten soll; es geht hier nicht um eine umfassende Klärung möglicher Konflikte im Spannungsfeld zwischen gesetzlichen Vorschriften und persönlichen ethischen Normen.

Die Vertragstheorie

Auffällig ist die unerbittliche Konsequenz, mit der die personifizierten Gesetze im "Kriton" die Gesetzestreue verteidigen. Sie kontrastiert mit der scharfen Kritik, die Platon sonst am athenischen Staat und dessen Institutionen übt. Zur Begründung der Loyalitätsforderung dient vor allem eine Vertragstheorie: ein Konzept, dem zufolge die Loyalitätspflicht der Bürger auf einer Übereinkunft zwischen ihnen und dem Staat basiert. Eine Stütze findet die Vertragstheorie im damaligen Verständnis der Staatsbürgerschaft. Das athenische Bürgerrecht wurde nicht bei der Geburt oder beim Erreichen des Volljährigkeitsalters automatisch erworben, sondern die Nachkommen von Stadtbürgern mussten, um die Bürgerrechte zu erhalten, einen Antrag stellen und sich einer „Prüfung“ "(dokimasía)" unterziehen. Der Inhalt der „Prüfung“ ist nicht bekannt, vielleicht beschränkte sie sich auf die Feststellung der Abstammung. Dieses Verfahren des Erwerbs der Staatsbürgerschaft wird im Dialog erwähnt. Aus dem Willensakt, mit dem man sich hierfür entschied, ließ sich ein vertragsähnliches Verhältnis des Bürgers zum Staat ableiten.

Überzeugen oder gehorchen

Zu den kontrovers diskutierten Fragen gehört die Auslegung der von den Gesetzen vertretenen „persuade-or-obey doctrine“. Dabei handelt es sich um den Grundsatz, dass der Bürger, der ein Urteil oder eine Vorschrift für ungerecht hält, dennoch gehorchen muss, wenn es ihm nicht gelingt, die zuständige Instanz von der Ungerechtigkeit zu überzeugen und so auf legalem Weg eine Änderung herbeizuführen. Wer keine dieser beiden Alternativen wählt, dem bleibt nach dem Ethikverständnis der Gesetze nur die Auswanderung. Das hat allerdings die problematische Konsequenz, dass das Inkrafttreten eines einzigen ungerechten Gesetzes alle anständigen Bürger zwingt, aus Gewissensgründen ins Exil zu gehen.

Obwohl Sokrates das Exil grundsätzlich ablehnte, fand sich für ihn persönlich eine ethisch vertretbare Lösung, da er durch das Gerichtsurteil nicht zum Begehen, sondern nur zum Erleiden eines Unrechts verpflichtet war, was nach Platons Ethikverständnis weniger schlimm und nötigenfalls hinnehmbar ist. Die Frage, wie man sich zu verhalten hat, wenn der Staat aktive Beteiligung an einem Unrecht fordert und Auswanderung nicht möglich ist, wird im Kriton nicht erörtert. Aus dem Gesamtzusammenhang von Platons Ethik ergibt sich jedoch, dass für ihn in diesem Fall die Vermeidung des Unrechts zweifellos Vorrang hat.

Mit der grundsätzlichen Problematik der Alternative „überzeugen oder gehorchen“ hängt die in der neueren Forschung kontrovers diskutierte Frage zusammen, wie „autoritär“ oder „liberal“ man sich das staatsrechtliche Denken des platonischen Sokrates vorzustellen hat. Strittig ist insbesondere, inwieweit sich der von Platon dargestellte Sokrates mit den einzelnen Argumenten, die er von den Gesetzen vortragen lässt, und deren weitreichenden Konsequenzen identifiziert. Die Problematik der „autoritären“ Interpretation, der zufolge er auf ausnahmsloser Gesetzestreue beharrt, tritt dort zutage, wo diese Position mit seinem Gebot, unter keinen Umständen jemals ein Unrecht zu begehen, in Konflikt gerät. Ein solcher Konflikt entsteht, sobald ein Bürger durch eine Rechtsnorm zum Unrechthandeln gezwungen wird und es ihm nicht möglich ist, mit Argumenten eine Änderung zu erwirken.

Zur Behebung dieses Problems plädieren manche Forscher für eine „liberale“ Auslegung der Stellungnahme, die Sokrates den personifizierten Gesetzen in den Mund legt. Nach diesem Verständnis ist die dort erhobene Forderung nach Gesetzestreue nicht absolut bedingungslos, sondern hat bestimmte Voraussetzungen, bei deren Wegfall sie hinfällig würde. Andere Interpretationen gehen davon aus, dass Sokrates die Gesetze zwar „autoritär“ argumentieren lässt, selbst jedoch wesentlich „liberaler“ denkt.

„Autoritäre“ und „liberale“ Deutungsmodelle

Eine „liberale“ Auslegung der Ausführungen der Gesetze schlägt Sandrine Berges vor. Sie geht von der Überlegung aus, dass die Übereinkunft zwischen der staatlichen Gemeinschaft und dem Individuum eine gegenseitige Verpflichtung beinhaltet. Die Gesetzgebung verschafft – wie die Gesetze im Dialog betonen – dem Staatsbürger seine Existenzgrundlage und ein für sein Gedeihen förderliches Umfeld und er hält sich dafür loyal an die Gesetze. Gedeihen bedeutet im Sinne des Sokrates Charakterbildung, das heißt Erwerb der Tugendhaftigkeit als Voraussetzung zu einem guten Leben. In diesem Sinne ist die Analogie zum Verhältnis zwischen Eltern und Kind zu verstehen: Die Eltern erziehen das Kind zu einem guten Menschen und können dafür dessen Gehorsam erwarten, die Gesetze fördern die Tugendhaftigkeit der Bürger und sind daher zu respektieren. In beiden Fällen muss die übergeordnete Instanz ihre Verpflichtung erfüllen, um einen Anspruch auf Gehorsam zu haben. Beim Verhältnis zwischen Sokrates und den athenischen Gesetzen ist das trotz des Fehlurteils des Gerichts der Fall. Anderenfalls bestünde keine Pflicht zur Gesetzestreue.

Nach dem Verständnis von Richard Kraut fordern die Gesetze nur einen ernsthaften Versuch, die zuständige Instanz zu überzeugen. Missglückt dieser Versuch, so ist bürgerlicher Ungehorsam zulässig. Dagegen wendet eine Reihe von Kritikern ein, dies lasse sich Platons Text nicht entnehmen, vielmehr werde bei einem Scheitern des Überzeugungsversuchs unbedingter Gesetzesgehorsam gefordert. Somit bestehe das Dilemma in voller Schärfe.

Während die „liberale“ Auslegung der Stellungnahme der personifizierten Gesetze auf starke Bedenken gestoßen ist, findet ein völlig anderer Lösungsweg, der von einem „autoritären“ Ausgangspunkt zu einem „liberalen“ Ergebnis führt, in der neueren Forschung viel Anklang. Die Vertreter dieses Ansatzes gehen davon aus, dass die Stellungnahme der Gesetze zwar im „autoritären“ Sinn zu verstehen ist, aber nicht oder nur teilweise mit Sokrates’ eigener Position übereinstimmt. Demnach hält Platons Sokrates zwar das Ergebnis der Argumentation, die er den Gesetzen in den Mund legt, hinsichtlich seines eigenen Falles für richtig, aber seine Wertordnung unterscheidet sich von der ihren. Die generellen Forderungen der Gesetze und ihre „autoritären“ Begründungen müssen nicht seiner und Platons eigener Sichtweise entsprechen. Nach der schwächeren Variante dieser Hypothese ist Sokrates zwar von der Argumentation der Gesetze beeindruckt, doch bedeutet das nicht, dass er sich mit allen ihren Überlegungen identifiziert und deren Konsequenzen bejaht. Nach der stärkeren Variante stimmt er den Gesetzen nur hinsichtlich des Ergebnisses – der Ablehnung der Flucht – zu, lehnt aber den Weg, auf dem sie zu dem Ergebnis gelangt sind, grundsätzlich ab; seine Zustimmung zum Ethikverständnis der Gesetze ist nicht ernst gemeint, sondern ironisch.

Vertreter dieser Deutungsweise weisen darauf hin, dass Sokrates am Schluss des Dialogs die Wirkung, die das Plädoyer der Gesetze auf ihn hat, mit der ekstatischen Verzückung vergleicht, die bei kultischen Tänzen erzeugt wird. Das ist ein irrationaler Aspekt, der mit der philosophischen Forderung nach unbedingter Herrschaft der Vernunft kontrastiert. In Platons Werken erscheint Sokrates als Philosoph, der stets vernünftig handelt und mit seiner außergewöhnlichen Selbstbeherrschung Bewunderung erregt, zugleich aber auch als ein sehr starken Affekten ausgesetzter Mensch. Wesentlich ist dabei, dass die Vernunft immer die Kontrolle behält und dass er Wissensansprüche, die auf einer irrationalen Grundlage erhoben werden, skeptisch beurteilt. Daher wird der Vergleich mit den von Begeisterung ergriffenen Tänzern als Indiz dafür gewertet, dass zwischen den radikalen, suggestiven Forderungen der Gesetze und der philosophisch reflektierten Position des Sokrates ein Unterschied besteht. Demnach ist Sokrates’ Beschreibung seiner Ergriffenheit ironisch zu verstehen, so wie in der "Apologie", seiner Verteidigungsrede vor Gericht, wo er zu Beginn ironisch behauptet, die Überredungskraft seiner Ankläger habe ihn fast dazu gebracht, sich selbst zu vergessen.

Die starke Variante der Interpretation, die den Standpunkt des Sokrates von dem der Gesetze unterscheidet, vertritt insbesondere Roslyn Weiss. Sie weist darauf hin, dass der im Dialog dargestellte Kriton zwar ein alter Freund des Sokrates ist und daher die sokratische Ethik gut kennen müsste, aber – wie seine Überlegungen und Reaktionen zeigen – als offensichtlich unphilosophischer Mensch außerstande ist, ihre Grundlagen zu verinnerlichen. Nach der Hypothese von Weiss ist dies der Grund dafür, dass Sokrates die Gesetze auftreten lässt und ihnen die Aufgabe überträgt, dem Freund im Rahmen von dessen Verständnishorizont – also „autoritär“ – begreiflich zu machen, dass eine Flucht falsch wäre. Ein Indiz hierfür sieht Weiss darin, dass Sokrates die Gesetze erst einführt, nachdem Kriton ihm gesagt hat, dass er der philosophischen Argumentation nicht folgen kann. Als weiteren Anhaltspunkt führt Weiss an, dass Sokrates die Ausführungen zugunsten der Gesetzestreue als etwas bezeichnet, was ein Redner vortragen würde. Dies drücke eine Distanzierung aus, da der platonische Sokrates generell die Rhetorik als unredliche, manipulative Art des Überzeugens ablehne.

Auch Thomas Alexander Szlezák betont, dass die Begründung für die Haltung des Sokrates, die dieser seinem unphilosophischen Freund gibt, nicht philosophisch anspruchsvoll, sondern emotional sei, denn sie orientiere sich zwangsläufig an Kritons Reflexionsniveau. Der für Sokrates selbst maßgebliche Gesichtspunkt sei nicht hier, sondern im Dialog "Phaidon" zu finden. Auffällig ist, dass Sokrates im "Kriton" das Wort „Seele“ – einen zentralen Begriff seines Denkens – vermeidet und mit einer metaphysisch neutralen Umschreibung umgeht, offenbar weil – wie im "Phaidon" deutlich wird – Kriton die philosophische Annahme einer unsterblichen Seele nicht akzeptiert.

Anderer Meinung ist David Bostock. Er glaubt, das „autoritäre“ Konzept, also ein ethisches Gebot absoluter Gesetzestreue, entspreche der Auffassung, die Platon dem Leser im "Kriton" nahebringen wolle. Erst in späteren Werken habe der Philosoph die Problematik dieser Position erkannt und seinen Standpunkt modifiziert. Auch eine Reihe weiterer Stimmen in der neueren Forschung halten an der traditionellen Auslegung fest, wonach die Position der Gesetze mit der des platonischen Sokrates zu identifizieren ist.

"Kriton" und "Apologie" – Gesetzestreue und ethische Autonomie

Ein in der Forschungsliteratur oft erörtertes Problem ist das Verhältnis des "Kriton" zur "Apologie", der von Platon als literarisches Werk gestalteten Verteidigungsrede, die Sokrates vor Gericht gehalten haben soll. In der "Apologie" erklärt Sokrates, dass er einem hypothetischen Gerichtsurteil, das ihn unter Androhung der Todesstrafe zum Verzicht auf öffentliches Philosophieren zwänge, nicht Folge leisten würde, denn eine solche Unterlassung eines Dienstes an der Öffentlichkeit wäre für ihn ein Unrecht. Hier haben manche Forscher einen Widerspruch zu Sokrates’ Haltung im "Kriton" gesehen und verschiedene Erklärungen für die scheinbare oder tatsächliche Inkonsistenz vorgeschlagen. Eine der Erklärungen lautet, es liege keine wirkliche Inkonsistenz vor, da die beiden Fälle – der reale im "Kriton" und der hypothetische in der "Apologie" – fundamental verschieden und nach verschiedenen Prinzipien zu beurteilen seien. Nach einem weiteren Lösungsvorschlag ist die Überlegung in der "Apologie" rein theoretischer Natur, da ein Philosophieverbot keine Rechtsgrundlage hätte und keine Situation vorstellbar ist, in der das Gericht tatsächlich Sokrates eine solche Strafe hätte auferlegen können, es sei denn, der Angeklagte hätte dies selbst vorgeschlagen. Dagegen wird allerdings eingewendet, es handle sich um ein Gedankenexperiment, das seinen Sinn verliere, wenn man davon ausgehe, dass der hypothetische Fall niemals eintreten könne. Für eine andere Lösung treten Mario Montuori und Giovanni Reale ein: Sie glauben, der "Kriton" sei erst lange nach der "Apologie" entstanden und die Unterschiede hingen mit dem großen zeitlichen Abstand der beiden Werke zusammen. Reale meint, in der "Apologie" – einem Jugendwerk – lasse Platon den historischen Sokrates zu Wort kommen, im "Kriton" stecke er selbst hinter der Maske des Sokrates. James Stephens nimmt einen nicht auflösbaren Widerspruch an.

Die Echtheit des "Kriton" gilt als sicher; in der neueren Forschung hat nur Holger Thesleff Platons Autorschaft bezweifelt. Mitunter ist der Dialog in die Nähe der mittleren Schaffensperiode Platons gerückt oder dieser sogar zugezählt worden, doch gewöhnlich wird er als Frühwerk betrachtet. Einige italienische Forscher plädieren für eine Spätdatierung, sie weisen dem "Kriton" einen Platz unweit der spät entstandenen "Nomoi" zu. Vermutlich fällt seine Abfassung ungefähr in dieselbe Zeit wie die der inhaltlich verwandten "Apologie", deren Datierung allerdings umstritten ist. Jedenfalls entstand der "Kriton" nach dem Tod des Sokrates, der im Frühjahr 399 v. Chr. hingerichtet wurde.

Es ist wohl eine historische Tatsache, dass Freunde des Philosophen einen Fluchtplan entworfen haben und dass er dieses Vorhaben abgelehnt hat, denn auch der Zeitgenosse Xenophon, ein Schüler des Sokrates, berichtet davon. Unbekannt ist allerdings, inwieweit das theoretische Konzept, das im "Kriton" zur Begründung der Entscheidung gegen den Plan vorgetragen wird, der Position des historischen Sokrates entspricht. Manche Philosophiehistoriker vermuten, dass die im "Kriton" dargestellte Sokratesfigur dem historischen Sokrates relativ nahesteht. William K. C. Guthrie hält die Vertragstheorie für einen Bestandteil der Philosophie des historischen Sokrates.

In der Tetralogienordnung der Werke Platons, die anscheinend im 1. Jahrhundert v. Chr. eingeführt wurde, gehört der "Kriton" zur ersten Tetralogie. Der Philosophiegeschichtsschreiber Diogenes Laertios zählte ihn zu den „ethischen“ Schriften und gab als Alternativtitel „Über das, was getan werden muss“ an. Dabei berief er sich auf eine heute verlorene Schrift des Mittelplatonikers Thrasyllos.

Cicero griff den Gedanken Platons auf, dass der Bürger dem Staat zu tätiger Dankbarkeit verpflichtet sei.

In platonfeindlichen Kreisen fand der "Kriton" einen negativen Widerhall. Der Gelehrte Athenaios, der einer antiplatonischen Quelle Material entnahm, beschuldigte Platon, Kriton angegriffen zu haben. Damit bezog er sich offenbar auf den Umstand, dass Kriton im Dialog einen unvorteilhaften Eindruck macht, da er eine undurchdachte Position vertritt, zu deren Verteidigung er dann nichts vorzubringen weiß. Ein weiterer antiplatonischer Autor, der Epikureer Idomeneus von Lampsakos, behauptete, der Fluchtvorschlag stamme in Wirklichkeit nicht von Kriton, sondern von dem Sokratiker Aischines von Sphettos. Platon habe den wahren Urheber verschwiegen, da sein Verhältnis zu ihm schlecht gewesen sei, und habe im Dialog Kriton die Rolle des Aischines zugewiesen.

Es ist kein antiker Textzeuge erhalten geblieben. Die älteste erhaltene mittelalterliche "Kriton"-Handschrift wurde im Jahr 895 im Byzantinischen Reich angefertigt. Bei den lateinischsprachigen Gelehrten des Westens war der "Kriton" im Mittelalter unbekannt; in der islamischen Welt hingegen existierte eine arabische Übersetzung.

Im Westen wurde der "Kriton" im Zeitalter des Renaissance-Humanismus wiederentdeckt. Die erste lateinische Übersetzung fertigte der italienische Humanist und Staatsmann Leonardo Bruni im ersten Jahrzehnt des 15. Jahrhunderts an. Da er sie später unbefriedigend fand, erstellte er eine überarbeitete Fassung, die er spätestens 1427 abschloss. Beide Fassungen versah er mit Einleitungen. Ihm gefiel besonders die staatsphilosophische Argumentation der personifizierten Gesetze, die er für seine eigene Schrift "De militia" verwertete. Für sein eigenes Verhalten im Dienst seines Vaterlandes – der Republik Florenz – berief er sich auf das Vorbild des im "Kriton" dargestellten Sokrates. Eine weitere lateinische Übersetzung stammt von Rinuccio da Castiglione; es handelt sich um eine Überarbeitung von Brunis erster Fassung. Der dritte humanistische Übersetzer des Dialogs war Marsilio Ficino. Er veröffentlichte seinen lateinischen "Kriton" 1484 in Florenz in der Gesamtausgabe seiner Platon-Übersetzungen.
Die Erstausgabe des griechischen Textes erschien im September 1513 in Venedig bei Aldo Manuzio in der von Markos Musuros herausgegebenen Gesamtausgabe der Werke Platons.

Der Philosoph David Hume (1711–1776) erwähnte die Vertragstheorie im "Kriton" als das einzige ihm bekannte antike Beispiel für die Vorstellung eines der Regierung implizit gegebenen Loyalitätsversprechens der Bürger. Hume war ein scharfer Gegner der neuzeitlichen Vertragstheorien. Er meinte, Platons Sokrates begründe den Gesellschaftsvertrag in der Art der Whigs und ziehe daraus die Konsequenz eines passiven Gehorsams in der Art der Tories.

Literarische Aspekte

Der einflussreiche Platon-Übersetzer Friedrich Schleiermacher schrieb zu seiner 1805 erschienenen "Kriton"-Übersetzung eine Einleitung, in der er die Meinung äußerte, dieser Dialog sei wohl „nicht ein vom Platon eigentlich gebildetes Werk“, sondern ein „wirklich so vorgefallenes Gespräch“. Platon habe an dem ihm mitgeteilten Gesprächsverlauf nur nebensächliche Veränderungen vorgenommen. Zwar sei „die Idee im Ganzen schön und klar ausgeführt“, doch zeige sich im Einzelnen ein Mangel an Durchgestaltung, der nicht zu einer literarischen Fiktion passe.

Die literarische Qualität wird meist günstig beurteilt. Für Paul Shorey und William K. C. Guthrie ist der Dialog ein Meisterwerk. Dieser Ansicht ist auch Thomas Alexander Szlezák: „Sprache, Argumentation und Charakterdarstellung sind meisterhaft aufeinander abgestimmt.“ Kurt Hildebrandt hebt die „Stimmung großartiger Gelassenheit“ hervor. Franz von Kutschera urteilt, es sei ein sehr gut geschriebener Dialog, der sowohl durch die Atmosphäre des Gesprächs beeindrucke als auch durch die Haltung des Sokrates und „die Konzeption eines philosophischen Lebens als ausschließlicher und konsequenter Orientierung an Vernunftgründen“. Luis Noussan-Lettry lobt die „Vereinigung von spekulativer Tiefe und poetischer Gestaltungskraft in diesem Kunstwerk Platons“. Michael Erler findet den "Kriton" literarisch reizvoll.

Ernst Milobenski meint, der "Kriton" sei für das Sokratesbild unentbehrlich, da er den Hauptzug dieses Bildes, die Bewährung der Lehre durch die Tat, in besonderer Klarheit zu Bewusstsein bringe.

Philosophische Aspekte

Der einflussreiche Philologe Ulrich von Wilamowitz-Moellendorff fand im "Kriton" keinen philosophischen Gehalt. Der Dialog belehre nur „über die Pflicht des Staatsbürgers, aber auch das nicht abstrakt, sondern sokratisch, athenisch“. Sokrates werde als „peinlich gehorsamer und pflichttreuer Staatsbürger“ dargestellt; damit habe ihn Platon „bei den braven Bürgern, die sich um Philosophie nicht kümmerten“, rechtfertigen wollen. Auch in neuerer Fachliteratur wird die Ansicht vertreten, Platon sei es hier nicht darum gegangen, universelle Grundsätze philosophisch darzulegen und zu begründen, sondern er habe nur seinen Lesern die persönliche Entscheidung des Sokrates verständlich machen wollen. Olof Gigon sieht in dem Dialog ein anspruchsloses, aber liebenswürdiges kleines Werk von philosophisch bescheidenem Gewicht. Ungeachtet solcher Urteile wird der "Kriton" aber in der philosophiegeschichtlichen und rechtsphilosophischen Forschung als Stellungnahme zu grundsätzlichen Fragen ernst genommen und lebhaft und kontrovers diskutiert. Reginald E. Allen hält den Dialog für eines der großen Meisterwerke der Rechtsphilosophie; dies könne man allerdings erst bei einem richtigen Verständnis des oft missverstandenen Werks erkennen. Luis Noussan-Lettry argumentiert in einer eingehenden Untersuchung gegen die Annahme, es handle sich um eine biographische, nicht philosophische Schrift.

Erst die intensiven Forschungsdiskussionen haben die Vertracktheit des scheinbar einfachen Textes ans Licht gebracht. Hellmut Flashar konstatiert, der "Kriton" sei nicht so einfach, wie er aussehe. Er sei kompliziert und mehrschichtig und berge erhebliche Schwierigkeiten.

In modernen ethischen und staatsphilosophischen Debatten spielt die Frage, wie weit die Gesetzestreue zu gehen hat und wie ein unbedingter Gesetzesgehorsam zu beurteilen ist, eine wichtige Rolle. Sie wird besonders in Zusammenhang mit dem zivilen Ungehorsam und der Problematik des Gewissensnotstands erörtert. Dabei nehmen vor allem Autoren im angelsächsischen Raum zustimmend oder ablehnend auf Gedankengänge im "Kriton" Bezug. Allerdings wird in diesem Diskurs der antike Dialog im Licht moderner Fragestellungen betrachtet. Darin liegt die Gefahr, heutige politische Vorstellungen auf die Zeit Platons zu übertragen.

Im 19. und frühen 20. Jahrhundert war die „autoritäre“ Interpretation der Stellungnahme der personifizierten Gesetze und des gesamten Dialogs die allgemein vorherrschende Betrachtungsweise und wurde kaum problematisiert. Es galt als selbstverständlich, dass Platon eine absolute Unterwerfung unter die geltenden Gesetze als heilige staatsbürgerliche Pflicht angesehen und keinerlei Ausnahmen in Betracht gezogen hat. Erst in der zweiten Hälfte des 20. Jahrhunderts begann eine intensive, weiterhin andauernde Debatte um alternative „liberale“ Erklärungsmodelle.

Die Qualität der von den personifizierten Gesetzen vorgebrachten Argumente wird in der Forschungsliteratur sehr unterschiedlich eingeschätzt. Viele Befürworter einer „autoritären“ Interpretation gelangen zu einem ungünstigen oder sogar vernichtenden Urteil über die Ausführungen in diesem Teil des Werks. Diejenigen von ihnen, welche den Standpunkt der Gesetze mit dem Platons und seines Sokrates gleichsetzen, beurteilen daher auch den philosophischen Gehalt des gesamten Dialogs negativ. Sie halten die Forderung nach bedingungsloser Unterwerfung unter die Herrschaft der Gesetze für schlecht begründet und ethisch inakzeptabel. Gerügt wird, die Argumentation sei überzogen und appelliere an Emotionen; der Vergleich zwischen den Eltern und dem Staat, wonach man dem Staat noch mehr Loyalität schulde als den Eltern, könne nicht einmal als Argument gelten. Die Gegenmeinung lautet, der Text sei sorgfältig ausgearbeitet und wenn man die Gedankengänge richtig verstehe, seien sie schlüssig.

Romano Guardini hebt hervor, im "Kriton" dringe „das philosophische Ur-Erlebnis der Gültigkeit“ durch, wonach diese – in diesem Fall als ethische Norm – „aus sich heraus, unabhängig von allen empirischen Bedingungen“ bestehe und als solche erkannt werden könne.

Karl Popper hält den "Kriton" für eine im Wesentlichen richtige Darstellung der Denkweise des historischen Sokrates. Vielleicht sei der Dialog sogar auf dessen Wunsch geschrieben worden. Zusammen mit der "Apologie" bilde er den letzten Willen des Philosophen. Als überzeugter Demokrat habe Sokrates die Flucht abgelehnt, um seine Treue zum demokratischen Staat und dessen Gesetzen zu beweisen. Er habe nicht ins Exil gehen wollen, da man ihn sonst für einen Gegner der Demokratie gehalten hätte und er damit der Staatsform, zu der er sich bekannt habe, geschadet hätte.

Peter Sloterdijk meint, der "Kriton" sei einer der „initiatischen Texte der Philosophie schlechthin“, mit denen Platon „eine neue Lebensform der Wahrheitssuche“ gestiftet habe. Kriton sei der Verteidiger des Diesseits gegen den machtvollen Todestrieb seines Meisters. Er spiele eine „halb lächerliche, halb ergreifende Rolle“. Für Sokrates sei das Leben nichts als ein Lehren, daher habe er konsequenterweise „noch seinen letzten Atemzug in ein Argument und seine letzte Stunde in ein Beweisstück verwandelt“.

Kritische Ausgaben, teilweise mit Übersetzung
Moderne Übersetzungen, teilweise mit unkritischen Ausgaben

Lateinische Übersetzungen (15. Jahrhundert)

Übersichtsdarstellung
Kommentare
Untersuchungen



</doc>
<doc id="9025" url="https://de.wikipedia.org/wiki?curid=9025" title="Holz">
Holz

Holz (von germanisch "*holta(z)", ‚Holz‘, ‚Gehölz‘; aus idg. "*kl̩tˀo"; ursprüngliche Bedeutungen, abgeleitet von idg. "*kel-", ‚schlagen‘: "Abgeschnittenes", "Gespaltenes", "schlagbares Holz") bezeichnet im allgemeinen Sprachgebrauch das harte Gewebe der Sprossachsen (Stamm, Äste und Zweige) von Bäumen und Sträuchern. Botanisch wird Holz als das vom Kambium erzeugte sekundäre Xylem der Samenpflanzen definiert. Nach dieser Definition sind die "holzigen Gewebe" der Palmen und anderer höherer Pflanzen allerdings kein Holz im engeren Sinn. Kennzeichnend ist aber auch hier die Einlagerung von Lignin in die Zellwand. In einer weitergehenden Definition wird Holz daher auch als "lignifiziertes" (verholztes) pflanzliches Gewebe begriffen.

Kulturhistorisch gesehen zählen Gehölze wohl zu den ältesten genutzten Pflanzen. Als vielseitiger, insbesondere aber nachwachsender Rohstoff ist Holz bis heute eines der wichtigsten Pflanzenprodukte als Rohstoff für die Weiterverarbeitung und auch ein "regenerativer" Energieträger. Gegenstände und Bauwerke aus Holz (z. B. Bögen und Schilde, Holzkohle, Grubenholz, Bahnschwellen, Holzboote, Pfahlbauten, Forts) sowie die Holzwirtschaft waren und sind ein Teil der menschlichen Zivilisation und Kulturgeschichte.

Die Abholzung von Wäldern an Küsten des Mittelmeers war einer der ersten großen Eingriffe des Menschen in ein Ökosystem. Rodungen waren der erste Schritt, um das zu großen Teilen bewaldete Europa urbar zu machen.

Holz wird vom Kambium, dem Bildungsgewebe zwischen Holz und Rinde, gebildet (sekundäres Dickenwachstum).

Bei der Teilung einer Kambiumzelle entstehen zwei Zellen, von denen eine ihre "Teilungsfähigkeit" behält und zu einer neuen "Initialzelle" heranwächst. Aus der anderen wird eine "Dauerzelle", die sich noch einmal oder mehrmals teilt. Aus den später zu "Leitungs"-, "Festigungs"- oder "Speichergewebe" ausdifferenzierenden Zellen entsteht nach innen Holz (sekundäres Xylem). Nach außen entsteht Bast (Phloem, sprich Phlo-em), aus dem die "Innenrinde" besteht und aus dem später die vom Phellogen gebildete Borke entsteht. Die Produktion von "Xylemzellen" übersteigt die Produktion von "Phloemzellen" um ein Vielfaches, so dass der "Rindenanteil" am gesamten Stamm nur etwa 5 bis 15 Prozent beträgt.

In unseren Breiten gibt es klimatisch bedingt vier Wachstumsphasen:
Durch dieses "zyklische" Wachstumsverhalten entstehen Jahresringe, die deutlich in einem Querschnitt durch einen Stamm erkennbar sind (siehe auch Dendrochronologie).

Holz weist einen "artspezifischen" anatomischen Aufbau auf, so dass sich Holzarten anhand ihrer "Makro"- und "Mikrostrukturen" voneinander unterscheiden lassen. Die wissenschaftliche Beschreibung von "Holzstrukturen" und Bestimmung von Holzarten ist Aufgabe der Holzanatomie.

Die verholzte Zellwand der Laub- und Nadelhölzer enthält die "Gerüstsubstanzen" Zellulose, Hemicellulosen und Lignin sowie in geringem Umfang sogenannte Extraktstoffe. Zellulose und Hemicellulose werden oft unter dem Begriff "Holozellulose" zusammengefasst. Mikrofibrillen stellen das wesentliche "Strukturelement" der Zellwand dar.

Die Anteile des Lignins und der Hemicellulose sind bei Nadel- und Laubhölzern unterschiedlich. Die elementaren Massenanteile von trockenem Holz sind etwa 50 % Kohlenstoff, 43 % Sauerstoff, 6 % Wasserstoff und 1 % Stickstoff und andere Elemente. 

Entwicklungsgeschichtlich sind "Nadelhölzer" älter als "Laubhölzer", haben daher einen einfacheren anatomischen "Zellaufbau" als diese und besitzen nur zwei Zellarten.

Das entwicklungsgeschichtlich jüngere Laubholzgewebe ist wesentlich differenzierter als das des Nadelholzes. Man kann es in drei funktionale Gruppen einteilen.
Charakteristisch für Laubhölzer sind die in Nadelhölzern "nicht vorhandenen" Gefäße. Sie sind oft mit bloßem Auge als kleine Poren im Holzquerschnitt und als Rillen im "Tangentialschnitt" zu erkennen. Nach der Anordnung dieser Tracheen unterscheidet man:

Die Zuwachszonen (Jahresring­muster) sowie die "artspezifische" Anordnung von "Poren"- und "Parenchymsträngen" ergeben die charakteristische Maserung der Holzarten.

Von der Verkernung von Holz spricht man, wenn die inneren "Wasserleitbahnen" des Stammes unterbrochen werden und die Zellen absterben. Dies geschieht bei Nadelhölzern durch "Verschließen" der "Hoftüpfel" und bei zahlreichen Laubhölzern durch eine Verthyllung der "Zelllumen" in einem Alter von ca. 20–40 Jahren. Danach werden phenolische "Kerninhaltsstoffe" gebildet und in die Zellwände eingelagert, was oft zu einer Erhöhung der natürlichen Dauerhaftigkeit führt. Ist der Kernbereich deutlich durch eine dunkle Färbung zu erkennen, spricht man von Kernholzbäumen (z. B. Eiche, Kiefer, Douglasie, Lärche, Robinie). Wenn kein Farbunterschied zu erkennen ist, aber über den "verringerten Feuchtigkeitsgehalt" darauf geschlossen werden kann, dass der Innenbereich "verkernt" ist, spricht man von Reifholzbäumen (z. B. Fichte, Tanne, Linde, Birnbaum). Reifholz ist echtes Kernholz.

Zahlreiche Bäume neigen demgegenüber zu einer "fakultativen Verkernung" (z. B. Esche, Buche, Kirsche). Der Kern ist zwar farblich abgesetzt, man spricht aber von einem Falschkern, da die "Kernbildung" nicht endogen und regelmäßig stattfindet, sondern durch exogene Einflüsse (Verletzungen) ausgelöst wird. Der Falschkern hat keine erhöhte Dauerhaftigkeit. Als Splintholz bezeichnet man den Bereich des Stammes, der aktiv am "Wasser"- und "Nährstofftransport" und der Speicherung teilnimmt.

Der Begriff "Tropenholz" ist durch die Herkunft des Holzes definiert und steht daher außerhalb der Pflanzensystematik. Unter "Tropenholz" wird vorwiegend das Kernholz tropischer Laubholzarten verstanden. Tropische Hölzer enthalten meist eine "artspezifische" charakteristische Anordnung der "Poren" und des "Parenchyms". Viele tropische Hölzer zeichnen sich durch vorteilhafte "mechanische Eigenschaften" infolge des sogenannten Wechseldrehwuchses und durch "höhere Dauerhaftigkeit" infolge eines sehr hohen "Kernstoffgehalts" aus. Oftmals werden "Farbe" oder "Maserung" als ansprechend empfunden (Edelholz). Die Struktur von Tropenhölzern ist aufgrund des konstanteren Klimas in den Tropen gleichmäßiger als die von Jahrringen geprägte Struktur von Hölzern aus den gemäßigten Breiten. Der Konsum von "Tropenholz" wird in den Industrieländern seit den 1970er Jahren kritisch diskutiert, da der Bestand der tropischen Regenwälder unter anderem durch Raubbau gefährdet ist. Andererseits stellt Holz einen wichtigen Wirtschafts­faktor für viele tropische Länder dar und ist (wie auch in den gemäßigten Zonen) eine wichtige Einkommensquelle für die ländliche Bevölkerung. Umweltverbände kritisieren allerdings, dass dieser Bevölkerungsteil am wenigsten am gesamten Holzeinschlag in den Tropen profitiere.

Eine hohe Entwaldungs­rate hat Indonesien: Der jährliche "Nettoverlust" an Wald betrug im Zeitraum 2000 bis 2005 nach Angaben der Ernährungs- und Landwirtschaftsorganisation der Vereinten Nationen (FAO) etwa 1,8 Millionen Hektar Wald, eine Fläche, die der Größe Sachsens entspricht. (Der "Nettozuwachs" an Wald betrug in China in diesem Zeitraum jährlich etwa vier Millionen Hektar.) Der größte Anteil des globalen "Tropenwaldverlustes" wird laut FAO durch Wanderfeldbau und Brennholz­nutzung verursacht. Letztere machte im Jahr 2000 etwa 83 Prozent des Holzeinschlags in tropischen Ländern aus. Zum Schutz der tropischen Regenwälder haben Umweltschutzorganisationen wie WWF, Greenpeace, NABU und BUND die FSC-Zertifizierung maßgeblich mit initiiert. Andere Organisationen wie "Pro Regenwald", "Rettet den Regenwald" und "Watch Indonesia!" fordern den vollständigen Verzicht auf "Tropenholz" zum Schutz der letzten noch erhaltenen Regenwälder, da sie der Meinung sind, dass ein "Schutzsiegel" die ökologisch verantwortliche und sozial verträgliche Waldbewirtschaftung nicht gewährleisten kann.

Tropenhölzer lassen sich bei sorgfältiger Holzauswahl und sachgerechter Planung in Bezug auf die Dauerhaftigkeit immer auch durch "heimische" Hölzer ersetzen, es muss lediglich auf die besonderen optischen Merkmale der Tropenhölzer verzichtet werden.

Beispiele: Meranti, Mahagoni, Teak, Balsaholz, Palisander, Bangkirai (Yellow Balau), Bongossi, Abachi, Framiré, Merbau, Ovangkol, Ramin, Afzelia, Wengé

Die Eigenschaften des Holzes sind geprägt durch seine organische Natur, seine Porosität, seine Anisotropie und seine Hygroskopizität. Holzeigenschaften sind grundsätzlich artspezifisch, variieren aber auch innerhalb einer Art bedingt durch die Herkunft des Holzes. Splint- und Kernholz unterscheiden sich nur in Bezug auf Permeabilität und Dauerhaftigkeit, in ihren technischen Eigenschaften jedoch meist nicht.

Die hygroskopische Eigenschaft von Holz – d. h. seine Neigung, Feuchtigkeit aus der Umgebung aufzunehmen oder abzugeben – bewirkt seine vergleichsweise "geringe Dimensionsstabilität" bei wechselnder Umgebungsfeuchte. Die Holzfeuchtigkeit gleicht sich dem Umgebungsklima an. Feuchtigkeitsänderungen unterhalb des Fasersättigungsbereiches (je nach Holzart 25–35 % Holzfeuchte) gehen mit "Dimensionsänderungen" einher (Quellung und Schwindung). Einige Holzarten wie z. B. Teak haben aufgrund der Einlagerung hydrophober Substanzen ein geringes Schwindmaß. Ein technisches Verfahren zur Verminderung der Hygroskopizität ist die Holzmodifikation.

Nahezu alle Holzeigenschaften unterscheiden sich in den drei "anatomischen Grundrichtungen" des Holzes (axial, radial, tangential). Das bewirkt z. B. ein ungleichmäßiges Schwinden des Holzes während der Trocknung. Bei den mitteleuropäischen "Nutzholzarten" beträgt das maximale Schwindmaß im Mittel axial 0,3 %, radial 5 % und tangential 10 %. Holz schwindet beim Trocknen also tangential (parallel zu den Jahrringen) etwa doppelt so stark wie radial (parallel zu den Holzstrahlen), so dass insbesondere bei "großdimensionierten" Hölzern leicht radiale Risse (Schwindrisse) entstehen. Der "Quellungs-/Schwindungskoeffizient" gibt die Maßänderung pro Prozent "Holzfeuchteänderung" an (Schwindmaß).

Die sogenannte Rohdichte des Holzes schwankt mit der Holzfeuchte. Bei einer Holzfeuchte von 12 % (Normalfeuchte in beheizten Innenräumen) umfasst die Rohdichte in Abhängigkeit von der Holzart einen Bereich zwischen 200 kg/m und 1200 kg/m. Frisches Holz weist wesentlich höhere Werte auf. So liegt das "Landungsgewicht" von frischem Eichenholz um 1000 kg/m, im getrockneten Zustand (12 % Holzfeuchte) bei 670 kg/m. Die Rohdichte gilt als "Schlüsselvariable" für die meisten technischen Holzeigenschaften, mit denen sie korreliert ist. "Dichtemessungen" werden daher häufig zur Prüfung der "Holzgüte" eingesetzt (Beispiel: Resistograph). Im Gegensatz zur Rohdichte ist die Reindichte der darrtrockenen, "hölzernen" Zellwand weitgehend unabhängig von der Holzart und beträgt 1500 kg/m.

Holz ist ein viskoelastischer Werkstoff, und seine "elastomechanischen" Eigenschaften unterliegen daher dem Zeiteinfluss. Es müssen also sowohl die "Belastungsdauer" als auch die "Art der Krafteinwirkung" (statisch oder dynamisch) berücksichtigt werden. Neben der Dichte und der "Belastungsrichtung" beeinflussen die Struktur des Holzes, seine Vorgeschichte und die Holzfeuchte die "elastomechanischen" Eigenschaften. Es ist ferner zu beachten, dass Dichte und "elastomechanische" Eigenschaften einzelner Holzarten einer natürlichen Varianz von 10–22 % unterliegen können.

Von allen Festigkeiten des Holzes hat seine Zugfestigkeit die höchsten Werte, während die Druckfestigkeit des Holzes etwa 50 % und die Scherfestigkeit (Schubfestigkeit) nur etwa 10 % der Zugfestigkeitswerte erreichen. Die Zugfestigkeit von herkömmlichem Baustahl (370 N/mm; 7800 kg/m) ist zwar fünf- bis sechsmal so hoch wie die Zugfestigkeit von Bauholz (~80 N/mm; 450 kg/m), letzteres ist aber etwa 16-mal so leicht; der hier genannte Festigkeitswert bezieht sich auf die Belastung längs zur Faser. Holz zeichnet sich daher durch sein günstiges Verhältnis von Festigkeit und Gewicht aus.

Die Schall­geschwindigkeit erreicht in Holz "faserparallel" Werte von 4000 bis 6000 m/s, quer zur Faser nur 400 bis 2000 m/s. Einflussparameter auf die Schallgeschwindigkeit sind Dichte, Elastizität, "Faserlänge", "Faserwinkel", Holzfeuchte, Holzfehler (Äste, Risse). Wegen seiner guten akustischen Eigenschaften wird Holz im Musikinstrumentenbau eingesetzt. Es ist aber auch als Material für Schalldämmungen geeignet. Spanplatten mit einer "Flächendichte" von 15 bis 20 kg/m erreichen eine Schalldämmung von 24 bis 26 dB.

"Schalllaufzeitmessungen" werden zur Prüfung des "dynamischen" E-Moduls bei der "Gütekontrolle" von Schnitthölzern und zur Diagnose des Zustands von Bäumen (Schalltomographie) eingesetzt.

Holz ist aufgrund seiner Porosität ein schlechter Wärmeleiter und eignet sich daher bedingt als Wärmedämmung. Fichtenholz hat eine Wärmeleitfähigkeit von 0,13 W/(m·K), zum Vergleich Stahlbeton: 2,00 W/(m·K). Bei Spanplatten liegt sie mit etwa 0,10 W/(m·K) noch niedriger. "Dämmplatten" aus Holzweichfaser erreichen 0,04 W/(m·K). Die Wärmeleitfähigkeit steigt mit der Holzfeuchte und der Rohdichte des Materials.

Die spezifische Wärmekapazität, d. h. die Wärmemenge, die nötig ist, um 1 kg eines Materials um 1 Kelvin zu erwärmen, ist bei Holz mit 0,472 Wh/(kg·K) fast doppelt so hoch wie bei Beton mit 0,244 Wh/(kg·K). Die Wärmedehnung kann bei Holz in der Praxis vernachlässigt werden, da sie durch das Schwindverhalten infolge Trocknung "überkompensiert" wird.

Die thermische Zersetzung von Holz setzt bei Temperaturen über 105 °C ein, wird ab 200 °C stark beschleunigt und erreicht ihren Höhepunkt bei 275 °C. Ein thermischer Holzabbau kann aber bei längerer "Exposition" schon bei Temperaturen unter 100 °C stattfinden. Der Flammpunkt des Holzes liegt zwischen 200 und 275 °C. Bei Abwesenheit von Sauerstoff kommt es zur Pyrolyse. Mitteleuropäische "Nutzhölzer" haben bei einem üblichen Wassergehalt von 20 % einen Heizwert zwischen 3,9 und 4,0 kWh/kg.

Farbe und Struktur des Holzes werden als ästhetisch ansprechend empfunden. Starke Astigkeit und unregelmäßige Verfärbungen gelten aber als Holzfehler. Infolge der Wirkung des ultravioletten Lichts dunkelt Holz nach. Über einen langen Zeitraum schädigt Ultraviolettstrahlung das Holz oberflächlich. Dabei wird vor allem das Lignin denaturiert und "abgebaut" und im Falle direkter Bewitterung nachfolgend vom Regenwasser ausgewaschen. Die Oberfläche wirkt dann schmutzig grau. Unterbleibt die Einwirkung von Regenwasser, erhält das Holz infolge der UV-Wirkung eine silbrig-weiße Farbe. Die Wirkung des Sonnenlichts ist auf die Oberfläche begrenzt. Ihr kann durch pigmenthaltige Lasuren bzw. Lackierung begegnet werden.

Holz ist biologisch abbaubar, ist dadurch aber auch anfällig gegenüber "biotischen" Schädlingen. Es kann also z. B. von Insekten, Pilzen oder Bakterien angegriffen und in seiner Substanz nachhaltig zerstört werden. Pilze können ab einer Holzfeuchte von etwa 20 % Holz angreifen. Bläuepilze (Ascomyceten, Fungi imperfecti) bewirken nur eine oberflächliche Verfärbung, während "holzabbauende" Ständerpilze Weißfäule bis Braunfäule verursachen. Moderfäule und Abbau durch Bakterien ist nur bei hoher Feuchtigkeit, vor allem im "Erdkontakt" möglich. Die Larven holzzerstörender Insekten wie Hausbock und Nagekäfer können noch bei geringerem Feuchtegehalt das Holz angreifen. Widerstandsfähigere Kernhölzer werden nur sehr langsam "biotisch" abgebaut. Ihre Resistenz wird nach Resistenzklassen 1–5 entsprechend DIN EN 350-2 eingeteilt.

Der "biotische Holzabbau" lässt sich weitgehend durch "konstruktiven" Holzschutz vermeiden oder vermindern. Dabei stehen die Verhinderung der "Befeuchtung" sowie ggf. der Einsatz geeigneter "resistenter" Kernhölzer im Vordergrund. Bei "direkt bewitterten" Hölzern im Außenbau wie "freistehenden" Holzkonstruktionen und Masten ist ein fachgerechter chemischer Holzschutz angeraten und für "tragende Konstruktionen" nach DIN 68 800 vorgeschrieben. Eine neue Möglichkeit, Holz gegen "feuchtebedingte Dimensionsänderungen" und Fäule unempfindlicher zu machen, ist die Holzmodifikation als Thermoholz oder acetyliertes Holz.

Zu den biologischen Holzeigenschaften gehört auch die Durchlässigkeit des Holzes, die durch dessen "anatomische" Struktur bedingt ist. Tüpfelverschluss und Verthyllung vermindern die "Durchlässigkeit" und damit die Tränkbarkeit des Holzes.

Die "Holzgewinnung" als Urproduktion zählt als Teil respektive nachgeschalteter Wirtschaftszweig zur Forstwirtschaft und mit dieser zum Agrarsektor. Das umfasst die ersten Verarbeitungsschritte bis zur Sägeware respektive zu Industrieholz und Brennholz. Die folgende "Holzverarbeitung" gehört schon zum produzierenden Gewerbe.

Holz zählt zu den nachhaltigen Rohstoff- bzw. Energiequellen, sofern die genutzte Menge nicht die nachgewachsene Menge übersteigt. Die leichte "Bearbeitbarkeit" und der damit verbundene niedrige Energiebedarf bei der Gewinnung und Verarbeitung spielen ebenfalls eine wichtige Rolle bei der ökologischen Bewertung. In Ökobilanzen schneiden Holzprodukte hervorragend ab.

Holz wird entweder als Schnittholz, als Furnier, als Holzwerkstoff oder als Faserstoff verarbeitet. Schnittholz und Furnier werden durch Holztrocknung und anschließende "Konditionierung" auf die jeweilige "Verwendungsfeuchte" gebracht. Dies geschieht heutzutage ausschließlich durch "industrielle Trocknungsverfahren".

Holz wurde mindestens seit der Altsteinzeit zur Energie­gewinnung (Feuer), als Wurfgegenstand, als Werkzeug­material und als Baumaterial intensiv genutzt, wahrscheinlich teilweise schon viel länger, da auch andere Primaten Holz zum Nest­bau, zum "Werfen" und zum "Stochern" einsetzen (siehe Werkzeuggebrauch bei Tieren).

Holz findet im Bauwesen als Bauholz Verwendung und kann dort z. B. als Vollholz, Brettschichtholz oder in Form von Holzwerkstoffen eingesetzt werden. Es wird sowohl für "konstruktive", "isolierende" als auch für "Verkleidungen" eingesetzt. Auf "tragenden" Holzkonstruktionen basiert der Holzrahmenbau, der Holzskelettbau sowie der traditionelle Fachwerkbau. Der Einsatz von Brettschichtholz und Holzwerkstoffen erlaubt dem modernen Ingenieurholzbau "ungewöhnliche" Holzkonstruktionen, wie z. B. das EXPO-Dach in Hannover und die 190 m lange Holzbrücke bei Essing über den Main-Donau-Kanal. Die zunehmende Verwendung von Brettschichtholz (Leimholzträger) in Hallen­konstruktionen ist durch Unglücksfälle in die Diskussion geraten. Die Schäden beruhten jedoch auf Konstruktionsfehlern und "mangelnder Kontrolle". Die normgerechten "Tragfähigkeitsreserven" von Holzkonstruktionen sind derart hoch, dass bei regelmäßiger Inspektion keine Risiken bestehen.

Prinzipiell steht selbst dem Bau von Hochhäusern aus Holz nichts entgegen. Dies ist aber aus statischer Sicht nur für die obersten Etagen gebräuchlich. Das höchste Holzgebäude Deutschlands steht in Magdeburg. Es handelt sich um den Jahrtausendturm (eröffnet 1999 im Rahmen der Bundesgartenschau auf dem Gelände des Elbauenparks). Das höchste europäische wirtschaftlich genutzte Haus mit fünf Stockwerken steht in Espoo in Finnland. Der Bau wurde hauptsächlich von dem finnischen Unternehmen "Finnforest" geleitet und im Jahre 2005 abgeschlossen.

2013 wurde der Aussichtsturm Pyramidenkogel mit 70 m hoher Plattform in Kärnten aus geschwungenen Leimholzpfählen – "ausgesteift" und "verspannt" mit Stahlelementen – errichtet.

Im Betonbau werden wesentliche Teile von Schalungen, nämlich die Standardelemente Schalungsträger, Schaltafeln (aus beschichtetem Dreischicht-Holz) und Schalelemente (aus "wasserfestem" Sperrholz in Metallrahmen) aus Holz hergestellt. Formen für Säulen aus abwickelbarem Karton basieren auf "Zellulosefasern" aus Holz. Ein Teil der Holzschalung geht als Brennholz verloren, viele Elemente werden – eventuell nach Entnagelung weiterverwendet.

Holz "geringer Dichte" kann in "roher" oder "verarbeiteter" Form zur thermischen Isolation (Dämmstoffe) eingesetzt werden (z. B. Faserdämmplatten, Balsa zur Isolation von Flüssiggas­tanks). Holzfaserplatten höherer Dichte haben gute akustische Dämmeigenschaften. Spanplatten (Flachpressplatte, OSB) werden ebenso wie "Sperrholzplatten" für Schalungen und für "Wandelemente" im Holzrahmenbau eingesetzt.

Im Unterschied zu Metallen ist Holz "elektrisch nicht leitfähig". Aus diesem Grund baute man in den dreißiger Jahren zahlreiche Sendetürme für Mittelwellen­sender aus Holz, wobei der "Antennendraht" im Innern des Turmes aufgehängt wurde.

Mit Ausnahme des Sendeturms des Senders Gleiwitz wurden alle diese Bauwerke entweder am Ende des Zweiten Weltkriegs zerstört oder inzwischen abgerissen. Weiterhin nutzt die Deutsche Telekom AG in Brück zwei 54 Meter hohe Holztürme, die ohne Verwendung von Metallteilen hergestellt wurden. Diese dienen zur Aufnahme von auszumessenden Antennen. Durch die metallfreie Konstruktion der Türme ist ein ungestörtes Ausmessen der Antennendiagramme möglich.

Weitere Anwendungen: Holz wird als "Schalungsholz" in Baugruben sowie für Masten und Holz-Bahnschwellen zur Körperschall­dämpfung auf Brücken und über Tunnel­bauten eingesetzt. Früher wurde Nadelholz im Bergbau als Stempel zum Abstützen der Stollen verwendet, da es vor dem Brechen knackende Geräusche abgibt (Warnfähigkeit des Holzes). Holz wird auch zur Herstellung von Behältern und Silos zur Aufbewahrung "aggressiver" Salze verwendet.

Die "Brennbarkeit" von Holz erscheint zwar zunächst als ein Nachteil beim Einsatz als Bau- und Konstruktionswerkstoff. Holz wird jedoch bei großen Querschnitten als "brandhemmend" eingestuft, da auf seiner Oberfläche unter Feuereinwirkung eine "hitzeisolierende" Kohleschicht entsteht, die das innere Holz schützt. Durch Bauweise und durch "brandhemmende" Anstriche lässt sich die "Widerstandsdauer" einer Holzkonstruktion steigern. Die "Gebäudestabilität" sinkt im Brandfall nur langsam und abschätzbar, wohingegen Stahlkonstruktionen aufgrund des temperaturbedingten "Festigkeitsverlustes" zum "plötzlichen", "unkontrollierten" Zusammenbruch neigen. Für Stützen und "tragende Teile" aus Holz gibt es spezielle Anstriche.

"Gewachsenes Holz" ist ein natürlicher dreidimensionaler Faserverbundwerkstoff mit vergleichsweise geringer Dichte, aber hoher Steifigkeit und Festigkeit. Die Leichtbaueigenschaften sind näherungsweise vergleichbar mit denen von glasfaserverstärktem Kunststoff (GFK). Gewachsenes Holz ist meistens gegen Materialermüdung sehr widerstandsfähig, lässt sich "gut bearbeiten" und hat vorteilhafte "ästhetische" sowie "ergonomische" Eigenschaften. Je nach Holz- und Holzwerkstoff sind Kostenvorteile gegenüber anderen Konstruktionswerkstoffen vorhanden. Holz- und Holzwerkstoffe dämpfen gut mechanische Schwingungen, vergleichbar mit Kunststoffen. Problematisch bei der konstruktiven Verwendung sind oftmals die "Richtungsabhängigkeit" der Werkstoffeigenschaften (Anisotropie) und die Interaktion mit Wasser. Das Quellen und Schwinden hat Einfluss auf die Dimensionsstabilität und wird umgangssprachlich oft als "Arbeiten" des Holzes bezeichnet.

Holz wird in Vollholz (Massivholz) und Holzwerkstoffe eingeteilt. Für die Holzwerkstoffe existieren unterschiedliche Einteilungen. Häufig wird in:
unterschieden. Holzwerkstoffe bestehen immer aus einzelnen Holzelementen (z. B. Holzfasern, Furnieren) und Bindemittel. Weiterhin kann eine Einteilung nach:
erfolgen. Je nach Holzelement und verwendetem Bindemittel werden die Eigenschaften von Holzwerkstoffen im Vergleich zu "gewachsenem Holz" verändert. Es ist deshalb sehr wichtig, für einen Konstruktionswerkstoff eine jeweils sinnvolle Auswahl des Holzwerkstoffs zu treffen.
Anwendungsfelder von Holz- und Holzwerkstoffen sind:

Die ästhetischen Holzeigenschaften stehen bei der Verwendung von Holz als Parkett sowie für "Decken"- und "Wandver"täfelungen im Vordergrund. Hier kommen zum Teil "tropische Edelhölzer" oder sogenannte Buntlaubhölzer (z. B. Kirschbaum, Elsbeere), die vorwiegend als Furnier verarbeitet werden, zum Einsatz. Auch im Möbelbau wird heutzutage hauptsächlich "gemessertes Deckfurnier" verwendet. "Holzfußböden" müssen zudem ausreichend abriebfest sein, daher werden meist Harthölzer verarbeitet.

Auch psychophysiologische Wirkungen sind bekannt: bei einer Vergleichsstudie des Joanneum-Instituts an einer österreichischen Schule ergab sich ein deutlicher stressreduzierender, u. a. die Herzfrequenz senkender Effekt auf diejenigen Schüler, die in holzverkleideten Klassenzimmern unterrichtet wurden. Ebenso sank die von den Lehrern empfundene soziale Beanspruchung durch die Schüler.

Holz ist der wichtigste Grundstoff in der Zellstoff- und Holzwerkstoff­industrie. Der Rohstoff wird dabei entweder nur "mechanisch zerkleinert" oder zusätzlich "chemisch aufgeschlossen". Vorprodukte sind Hackschnitzel (zerkleinertes Holz), Späne, "Holzfasern" oder auch "Furniere" (Holzblätter). Grundsätzlich wird nur "entrindetes" Holz verarbeitet. Für die Herstellung von Holzwerkstoffen werden beleimte "Späne" oder "Holzfasern" verpresst. Sperrholz hingegen besteht aus "kreuzweise" verleimten Furnieren, die meist aus gedämpften Blöcken geschält wurden.

Für die Zellstoffherstellung muss das Lignin weitestgehend aus dem Fasergrundstoff entfernt werden. Gängige Aufschlussverfahren sind das Sulfatverfahren und das Sulfitverfahren. Das "Restlignin" wird durch Bleichen des Zellstoffs beseitigt. Bei der Herstellung von Holzstoff oder Holzschliff als Grundstoff für Pappen und "minderwertige" Papiere verbleibt das Lignin in der Fasermasse. Papier aus Zellstoff erhielt früher die Bezeichnung "holzfrei". Aus Zellstoff und Holzstoff werden u. a. Papier, Pappe und Zelluloseprodukte wie Zelluloid und Viskose­fasern hergestellt.

Holz kann in reiner Form problemlos durch Kompostierung oder durch Verbrennung bei gleichzeitiger Energiegewinnung entsorgt werden. Brennholz weist als "nachwachsender Rohstoff" eine gute Ökobilanz auf, wenn es nachhaltig angebaut und gewonnen wird. Alt- und Abfallholz wird zunehmend als Brennmaterial in Biomassekraftwerken zur regenerativen und CO-neutralen Energiegewinnung genutzt. Holz findet außerdem Verwendung als Brennstoff in Holzöfen. Durch die Entwicklung "automatisierter Befeuerungsanlagen" für Holzpellets oder Hackschnitzel ist Holz als Brennstoff inzwischen nicht nur ökonomisch, sondern auch hinsichtlich des Komforts der Verbrennung von Öl oder Gas gleichwertig. 2006 wurden in Deutschland damit etwa 2 Prozent der Primärenergieversorgung gedeckt, was angesichts des Fehlens von Subventionen als wirtschaftlicher Erfolg zu betrachten ist.

Eine weitere Recycling-Methode ist die Hochtemperatur-Verschwelung. Mittels dieses Verfahrens können aus Holz und anderen organischen Stoffen "chemische Grundstoffe" hergestellt werden, die fossile Quellen ersetzen. Sie stellt zugleich eine stoffliche Nutzbarkeit von Holz und anderen "nachwachsenden Rohstoffen" dar, die mit Rückgang der fossilen Energieträger stark an Bedeutung gewinnen könnte. Holz hat den Recycling-Code-50 (FOR).

Weitere stoffliche Anwendungen:

Holz ist einer der ältesten und wichtigsten Roh- und Werkstoffe der Menschheit. Nach wie vor übersteigt die jährliche Holzproduktion die Mengen an Stahl, Aluminium und Beton. Die Gesamtmenge der weltweit in den Wäldern akkumulierten "Holzmasse" wurde von der FAO für das Jahr 2005 auf etwa 422 Gigatonnen geschätzt. Jährlich werden derzeit 3,2 Milliarden m³ Rohholz eingeschlagen, davon fast die Hälfte in den Ländern der Tropen. Das Rundholz­aufkommen (2011) belief sich lauf FAO auf 1,578 Mrd. m³. Die höchste jährliche Einschlagsintensität findet sich allerdings mit 2,3 m³/ha in Westeuropa. Fast die Hälfte des globalen Holzaufkommens wird als Brennholz verwendet, was vor allem auf die Länder der tropischen Zone zurückgeht. Hier ist die Energiegewinnung noch immer die wichtigste Holznutzungsart – der "Brennholzanteil" in Westeuropa beträgt demgegenüber nur knapp ein Fünftel des Einschlags.

Im Jahre 2000 wurden lediglich 2 % des weltweit eingeschlagenen Holzes als Rohholz exportiert; der Verbrauch bzw. die Verarbeitung zu Halbwaren (Schnittholz, Holzwerkstoffe, Faserstoffe für Papier sowie Papier und Pappe) erfolgt also fast ausschließlich in den Herkunftsländern. Die größten Verbraucher an weltweit produzierten Holzhalbwaren sind mit 73–87 % die Länder der temperierten Zone. Auf der Produzentenseite hatte 1998 hier die Schnittholzproduktion nur einen Anteil von 35 % an der Gesamtproduktion, jeweils 16 % entfielen auf Holzwerkstoffe sowie auf Faserstoffe für Papier und 32 % auf Papier und Pappe.

Die prozentual waldreichsten Länder Europas ohne Russland sind Finnland, Slowenien, Schweden und mit etwas Abstand Österreich. Die in absoluten Werten größten Waldflächen finden sich in Schweden (etwa 28 Millionen Hektar), Finnland, Spanien, Frankreich und Deutschland. Über die höchsten mittleren Vorräte Holz pro Hektar Wald verfügt man in der Schweiz, Österreich, Tschechien, der Slowakei und Slowenien (jeweils mehr als 250), während Deutschland mit über 3,4 Milliarden "Vorratsfestmetern" in Europa über die höchsten Holzvorräte insgesamt verfügt (gefolgt von Schweden, Frankreich und Finnland).

Die Holznot, ein bevorstehender oder bestehender Mangel an Holz, wurde seit dem 16. Jahrhundert bis in das frühe 19. Jahrhundert als bedeutendes wirtschaftliches und gesellschaftliches Problem wahrgenommen. Die Debatte darüber führte mit zur Umstellung auf fossile Brennstoffe im Verlauf der Industrialisierung und mit zur systematischen Professionalisierung der Forstwirtschaft und Forstwissenschaft.

Der Wald in Deutschland bedeckt mit 11,4 Millionen Hektar 32 Prozent der Gesamtfläche des Landes und besitzt einen Holzvorrat von insgesamt 3,7 Milliarden "Vorratsfestmeter".

Der jährliche Holzeinschlag kann aufgrund von "Wetterereignissen" und "Holzpreisentwicklungen" stark schwanken. Im längerfristigen Vergleich hat er deutlich zugenommen: Im Durchschnitt der Jahre 1993–2002 wurden jährlich 38,4 Millionen Kubikmeter eingeschlagen, im Zeitraum 2003–2012 waren es durchschnittlich 56,8 Millionen Kubikmeter. Im Jahr 2007 wurde mit Höchstmarke erreicht: 76,7 Millionen Festmeter wurden eingeschlagen (vor allem Nadelholz). Dies lag an extremen "Wind"- und "Sturmereignissen" wie dem Orkan Kyrill.

Im Jahr 2014 betrug der Holzeinschlag in Deutschland insgesamt 54,4 Millionen "Erntefestmeter ohne Rinde". Davon entfielen 40,1 Millionen "Erntefestmeter" auf Nadelholz und 14,2 Millionen "Erntefestmeter" auf Laubholz. 44 Prozent des bundesweiten Holzeinschlags wurden 2014 im Privatwald getätigt, 20 Prozent im Körperschaftswald, 34 Prozent im Staatswald der Länder und 2 Prozent im Bundeswald.

Die wichtigsten "Nutzholzarten" sind Fichte, Kiefer, Buche und Eiche. Die Forstwirtschaft und vor allem die Holzwirtschaft (Holzindustrie) tragen mit ca. 2 % zur Bruttowertschöpfung bei. Holz hat als "Roh"- und "Werkstoff" eine stark steigende Bedeutung erlangt, da es nahezu "CO-neutral" erzeugt werden kann, sich gut mit ökologischer und nachhaltiger Wirtschaftsweise verträgt, mit geringem Energieaufwand zu verarbeiten ist und vollständig stofflich verwertet werden kann. Fachgerecht hergestellt und verarbeitet, ist Holz zudem ein dauerhafter Werkstoff. Im Jahr 2011 lag der Gesamtumsatz in der deutschen Holzindustrie bei 14,95 Milliarden Euro.

Österreich hat eine Waldfläche von 3,92 Millionen Hektar (1998), das sind über 46 % des Staatsgebietes, mit steigender Tendenz. Der "Ertragswald" umfasst 83 % der Waldfläche, Baumartenzusammensetzung im Ertragswald (nach Holzvorrat): Fichte 61,4 %, Buche 9,2 %, Kiefer 9,0 % und Lärche 6,8 %.






</doc>

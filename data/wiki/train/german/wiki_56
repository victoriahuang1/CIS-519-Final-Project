<doc id="8342" url="https://de.wikipedia.org/wiki?curid=8342" title="8. Jahrhundert">
8. Jahrhundert

Das 8. Jahrhundert begann am 1. Januar 701 und endete am 31. Dezember 800.

Die Weltbevölkerung in diesem Jahrhundert wird auf 200 bis 300 Millionen Menschen geschätzt. In Europa übernahmen die Karolinger die Macht im Frankenreich und beherrschten am Ende des Jahrhunderts große Teile des Kontinents. Das byzantinische Reich blieb trotz zahlreicher Angriffe stabil. Im Zuge der im vorherigen Jahrhundert begonnenen islamischen Expansion fielen weitere Gebiete, wie die iberische Halbinsel und große Teile Zentralasiens, unter muslimische Herrschaft. Zur Jahrhundertmitte übernahm die Abbasiden-Dynastie, die die Dominanz der Araber in der muslimischen Welt beendete, das Amt des Kalifen. China erreichte unter den Tang eine große wirtschaftliche und kulturelle Blüte, bis Mitte des Jahrhunderts eine Periode des Umbruchs begann.

In Europa ist dieses Jahrhundert Teil des Frühmittelalters (ca. 500-1050).

Europa wurde im 8. Jahrhundert durch den Aufstieg des Fränkischen Reiches und der Karolinger geprägt. Bis zum Jahr 714 regierte Pippin der Mittlere als Hausmeier das Frankenreich. Er unterstand weitgehend nur noch formal den merowingischen Königen, die einen großen Teil ihrer faktischen Macht eingebüßt hatten. Nach seinem Tod setzte sich sein Sohn Karl Martell als sein Nachfolger durch, der den Karolingern die Führungsposition endgültig sicherte. Im Gegensatz zu ihm strebte dessen Sohn und Nachfolger, Pippin der Jüngere, die Königswürde an. Nach der Absetzung des letzten merowingischen Königs, ließ er sich selbst als König ausrufen. Pippins Erbe, Karl der Große, dehnte die Grenzen des Frankenreiches durch regelmäßig durchgeführte Kriegszüge, gegen die Langobarden, die Awaren und die Sachsen, stark aus und beherrschte schließlich West- und Zentraleuropa und einen großen Teil der italienischen Halbinsel.

Zu Beginn des Jahrhunderts hatten lokale Herrscher im Frankenreich einen hohen Grad an Selbständigkeit. So regierten Herzöge über einige Reichteile mit weitreichenden Befugnissen. Die Karolinger waren bestrebt, die Macht der lokalen Regenten zu ihren Gunsten zu verringern. Dazu entmachteten sie schrittweise alle Herzöge. Ferner etablierte insbesondere Karl der Große eine auf ihn ausgerichtete lokale Herrschaftsstruktur, die "Grafschaftsverfassung" genannt wird. Grafen waren vom König eingesetzte und ihm rechenschaftspflichtige Amtsträger. Teilweise gehörte ihnen das Land, über das sie Herrschaftsrechte hatten, teilweise bekamen sie es vom König zur Verfügung gestellt. In den von ihm eroberten Gebieten setzte Karl Grafen ein, die sowohl ihm gewogene Adelige der eroberten Völker, als auch Adelige aus den fränkischen Kerngebieten waren. Neben den weltlichen Adeligen waren auch Bischöfe und Äbte, auf deren Einsetzung die Karolinger wesentlichen Einfluss hatten, in das System der Grafschaftsverfassung eingebunden. Diese nahmen sowohl geistliche als auch weltliche Aufgaben wahr. Ihre weltlichen Aufgaben hatten einen ähnlichen Umfang wie die der Grafen, was bei Karl dem Großen auch die aktive Teilnahme an Kriegszügen einschloss. Zur Kontrolle reiste Karl regelmäßig durch sein Reich (Reisekönigtum). Zusätzlich setzte er Königsboten ein.

Die Ernennung zum König war bei den Karolingern an das Einverständnis der Mächtigen des Volkes gebunden. Dennoch spielte auch der Gedanke der Erblichkeit des Königtums eine große Rolle. Um den Wechsel der Königsdynastie von der Familie der Merowinger zur Familie der Karolinger zu rechtfertigen, ließ Pippin der Jüngere die Zustimmung des Papstes zu diesem Schritt einholen. Die Königsalbung Pippins war ein weiterer Schritt zur Legitimation seiner Königswürde und sollte dieser einen religiösen Charakter verleihen. Die Karolinger verstanden das Königtum als ein im Auftrag Gottes geführtes Amt. Ihr Reich sollte deshalb ein christliches Reich sein. Schon Karl Martell förderte die christliche Missionierung, was seine Nachfolger vor dem Hintergrund der Idee eines christlichen Reiches fortsetzten. Die Karolinger setzten dabei auf angelsächsische Missionare, die eng mit dem Papst verbunden waren. Während Karl Martell eine militärische Unterstützung des Papstes ablehnte, unterstützte Pippin den Papst militärisch, nachdem dieser sein Königtum legitimiert hatte. Fortan sahen sich die fränkischen Könige als Schutzmacht des Papsttums, was auch die Päpste so sahen. Im Jahr 800 wurde Karl der Große dann durch den Papst zum west-römischen Kaiser gekrönt. Die Kaiserkrone gewährte ihm einen höheren Rang, aber keinen Zuwachs an Rechten. Die Krönung begründete das (west)-europäische Kaisertum des Mittelalters. Es verstand sich als ideeller Nachfolger des Kaisertums des römischen Reiches und war damit ein Konkurrent des byzantinischen Kaisertums. Im 9. Jahrhundert lösten die beiden Kaiser dieses Zweikaiserproblem, indem sie sich gegenseitig das tragen unterschiedlicher Kaisertitel einräumten.

Eine Institution der Karolinger war die Erbteilung, wobei das Erbe nach dem Tod des Königs unter den Söhnen geteilt wurde. So teilte sich Pippin die Herrschaft zunächst mit seinem Bruder Karlmann, der nach einigen Jahren ins Kloster ging. Karl der Große teilte sich die Herrschaft zunächst mit seinem ebenfalls Karlmann genannten Bruder, der drei Jahre nach Herrschaftsantritt starb.

Nach dem Bevölkerungsrückgang der vergangenen Jahrhunderte stieg die Bevölkerung in West- und Mitteleuropa wieder an. Dennoch beeinflussten Kriege, durch Unwetter verursachte Hungersnötige und Seuchen immer noch die Lebenserwartung der Menschen, doch die justinianische Pest verschwand Mitte des Jahrhunderts endgültig. Die Kindersterblichkeit war hoch und die Lebenserwartung lag nach überstandener Kindheit bei 44 bis 47 Jahren. Die Menschen ernährten sich überwiegend von Getreideprodukten, ferner von Milchprodukten und Gemüse.

Die Gesellschaft war stark agrarisch geprägt. Der weitaus größte Teil der Menschen wohnte in kleinen Dörfern auf dem Land. Die meisten Städte, die auf römische Gründungen zurückgingen, lagen in West- und Südeuropa. Im Zuge der fränkischen Expansion entwickelten sich auch östlich des Rheins um Klostergründungen und Bischofssitze Vorläufer städtischer Siedlungen.

Die Gesellschaft gliederte sich in Freie und Unfreie, wobei der jeweilige Status erblich war. Aus den Freien hob sich der Adel heraus, der durch Ämter privilegiert war. Freie waren rechtlich unabhängig, schuldeten dem König jedoch Kriegsdienste. Die Unfreien waren von einem Herren abhängig, der ihnen Schutz zu gewähren hatte, jedoch in vielen Lebensbereichen über sie bestimmen konnte. Die Rechte und Pflichten des Unfreien und seines Herren waren jedoch im Einzelfall sehr verschieden.

Reichtum begründete sich im Wesentlichen auf Landbesitz. Der Grund und Boden gehörte meistens Großgrundbesitzern, wie Königen, Adeligen, Bischöfen oder Klöstern. Diesen bewirtschafteten sie zum Teil mit Hilfe ihrer Unfreien selbst, andere Teile verpachteten sie an unfreie und freie Pächter. Schuldeten die freien Bauern dem Grundherren lediglich Abgaben, mussten die unfreien Pächter zusätzlich Dienstleistungen für den Herren, die Frondienste, erbringen.

Viele Kriegsdienste und immer aufwendige Waffen und Rüstungen, die sie selber stellen mussten, waren für die Freien eine zunehmende Belastung. So hielten es zahlreiche Freie für wirtschaftlich günstiger unfreie Pächter eines Grundherrn zu werden, um von den Kriegslasten befreit zu werden. So ist eine Abnahme der nicht adeligen Freien in diesem Jahrhundert festzustellen.

Die wirtschaftlichen Quellen des Königs gründeten sich auf den Besitz der Krone, der nur einen Teil des Reichsgebietes ausmachte, und den Königsschatz.

Das fränkische Reich umfasste viele unterschiedliche Volksgruppen. Für die Angehörigen vieler Völker, wie der Sachsen, der Thüringer und der Alemannen, galt das eigene Volksrecht. Die Volksrechte ließ Karl der Große aufschreiben und teilweise anpassen.

Der Vereinheitlichung der Verhältnisse im Frankenreich und die erleichterten Herrschaftsausübung diente die Münzreform Karls des Großen. Hatte Pippin der Jüngere schon das Recht zur Ausgabe von Münzen, das Münzregal, als alleiniges Königsrecht durchgesetzt, so führte Karl den Denar als einheitliche Silbermünze im Frankenreich ein und legte fest, wie viele Münzen aus einem Pfund Silber geprägt werden durften. Geld wurde vor allem im Fernhandel eingesetzt, während auf lokaler Ebene der Tauschhandel dominierte.

In den vorherigen Jahrhunderten hatten die Langobarden große Teile der italienischen Halbinsel vom byzantinischen Reich erobert, das zu Beginn des Jahrhunderts noch einen Landstreifen quer durch Mittelitalien, einschließlich Rom, und Gebiete im Süden der italienischen Halbinsel beherrschte. Die Langobarden wurden durch Könige regiert, wobei Herzöge unter dem König die Herrschaft über Teilreiche ausübten. Im Laufe des Jahrhunderts brachten die Könige auch das selbständige langobardische Herzogtum Benevent unter ihre Kontrolle. Durch die Ausdehnung der Macht der Langobarden fühlten sich die Päpste, die die weltliche Herrschaft über Rom ausübten, zusätzlich bedrängt. Byzanz konnte und wollte ihnen keine Hilfe gewähren, zumal seit dem vorherigen Jahrhundert religiöse Meinungsverschiedenheiten bestanden. Nach weiteren Eroberungen byzantinischen Gebietes durch die Langobarden zur Jahrhundertmitte, rief der Papst den Frankenkönig, Pippin den Jüngeren, der sich als Schutzherr des Papsttums verstand, zu Hilfe. Dieser eroberte wesentliche Gebiete zurück und übertrug diese dem Papst. Diese Pippinsche Schenkung machte den Papst, der schon zu Beginn des Jahrhunderts eine Landzuweisung bekam, endgültig zum weltlichen Herrscher über einen größeren Flächenstaat. Dieser Kirchenstaat umfasste bis ins 19. Jahrhundert größere Territorien Mittelitaliens. Als der Papst in den 770er Jahren seine weltliche Herrschaft wiederum durch die Langobarden bedroht sah, rief er Karl den Großen zur Hilfe. Karl eroberte das ganze Langobardenreich und setzte sich als dessen König ein. Nur das südlich von Rom gelegene langobardische Herzogtum Benevent blieb selbständig.

Anfang des Jahrhunderts kämpften mehrere Parteien der Westgoten auf der iberischen Halbinsel um die Macht. Ein Hilfegesuch einer der Konfliktparteien nahmen Berber und Araber zum Anlass einen großen Teil der Halbinsel zu erobern. Die Eroberung des Landes, das sie Al-Andalus nannten, erfolgte durch relativ autonom agierende muslimische Gruppen. Die Herrschaft errangen diese Gruppen neben militärischer Gewalt auch durch Verhandlungen und Bündnisse, in denen die Eroberer den regionalen Machthabern oder führenden Gruppen die Wahrung vieler ihrer angestammten Rechtspositionen zusicherten. Die Gebirge im Norden der iberischen Halbinsel blieben jedoch frei von muslimischer Herrschaft. Sie dienten gotischen Migranten als Rückzugsgebiet. In der zweiten Hälfte des Jahrhunderts konnten diese größere Gebiete im Norden der Halbinsel von den muslimischen Eroberern zurückerobern und dort das christliche Königreich Asturien zu gründen. Dieses diente als Ausgangspunkt für die Reconquista genannte Rückeroberung der muslimisch beherrschten Gebiete durch die Nachfahren der Westgoten, die im Jahr 1492 abgeschlossen wurde. Alle Gebiete, die die Muslime nördlich der Pyrenäen eroberten, wie Septimanien, wurden im selben Jahrhundert von den Franken zurückerobert. Den ersten großen militärischen Sieg gegen die muslimischen Truppen errangen die Franken unter Karl Martell in der Schlacht von Tours und Poitiers. Ob es sich hierbei um die Vereitelung eines Eroberungsversuches oder bloß um die eines Raubzuges handelte ist umstritten.

Zwischen den beiden Gruppen der Eroberer, den Berbern und den Arabern, kam es nach der Eroberung zu Spannungen und Kämpfen. Mitte des Jahrhunderts eroberte Abd ar-Rahman I. aus der Umayyaden-Dynastie die Macht in Al-Andalus und baute eine zentrale Herrschaft auf. Das von ihm gegründete Emirat von Córdoba war das erste muslimische Reich, das vom Kalifat in Bagdad formell politisch unabhängig war. In seiner Hauptstadt Córdoba wurde im Auftrag des Emirs mit dem Bau der Moschee von Córdoba begonnen.

Das im Karpatenbecken gelegene Awarenreich geriet ab dem Jahr 788 in kriegerische Auseinandersetzungen mit dem Frankenreich. Die militärischen Erfolge der Franken in den 790er Jahren führten zur Destabilisierung der Awarenreiches, der im folgenden Jahrhundert der Untergang folgte.

In Südosteuropa lag südlich der transsilvanischen Alpen das Bulgarische Reich, das sich weiter entlang der Schwarzmeerküste bis zur Mündung des Dnepr erstreckte. Anfang des Jahrhunderts war es mit dem byzantinischen Reich verbündet, geriet aber in der Folgezeit in kriegerische Auseinandersetzungen mit diesem. In den Jahren 750 bis 775 nutzten die Byzantiner das Abflauen der arabischen Angriffe, um große Teile des bulgarischen Reiches zu erobern. Das Ende byzantinischer Angriffe nach dem Tod des Kaisers, nutzten die Bulgaren, um wesentliche Teile ihres Reiches zurückzuerobern. Während des Jahrhunderts begann ein Prozess, in dem sich die bulgarische Führungsschicht an die slawische Mehrheit assimilierte.

Das Reich der Chasaren lag nördlich des Kaukasus und reichte von der Krim bis zum Norden des Kaspischen Meeres. Das chasarische Khanat, das mit Byzanz verbündet war, führte zahlreiche Kriege gegen das Kalifat mit wechselndem Erfolg. Nach der Übernahme des Kalifenamtes durch die Abbasiden wurde das Verhältnis friedlich. An der Spitze des Reiches standen ein Khagan und ein Bek, wobei letzterer alle militärischen und Verwaltungsaufgaben wahrnahm. Zumindest die Elite nahm im 8. Jahrhundert die jüdische Religion an. Für die Chasaren, die an einer geografisch wichtigen Stelle für den Welthandel lagen, war dieser von zentraler wirtschaftlicher Bedeutung.

Zu Beginn des 8. Jahrhunderts war in Gallien, auf den iberischen und italienischen Halbinseln, sowie auf den irischen und britischen Inseln das Christentum etabliert. Gefördert von den Karolingern breitete sich das Christentum durch die angelsächsische Mission in den fränkischen Gebieten nördlich und östlich des Rheins aus. Beruhte die Bekehrung im Wesentlichen auf Freiwilligkeit, kam es während der Sachsenkriege Karls des Großen zu zahlreichen Zwangstaufen im Zuge der Etablierung der Herrschaft der Franken. Mit der Konversion großer Gruppen von Langobarden zum katholischen Bekenntnis, war dieses das vorherrschende Bekenntnis Europas. Nach der muslimischen Eroberung der iberischen Halbinsel, durfte die dort lebende Bevölkerung ihren christlichen Glauben beibehalten, doch konvertierten große Bevölkerungsteile im Süden der Halbinsel zum Islam. Die Konvertiten als auch der christliche Teil der ursprünglichen Bevölkerung übernahmen arabische Gebräuche und Sitten. Dennoch blieben die Christen eine klar abgegrenzte Gruppe.

In diesem Jahrhundert wandte sich der Papst von Byzanz als Schutzmacht ab und dem Frankenreich als Schutzmacht zu. Dieses und die Kaiserkrönung Karls des Großen durch den Papst führten zu einer weiteren Entfremdung zwischen römischer Kirche und griechisch-orthodoxer Kirche. Auf religiösem Gebiet konnte jedoch der Streit um die Rechtmäßigkeit der Verehrung religiöser Bilder im zweiten Konzil von Nicäa nochmals beigelegt werden.

Insbesondere die fränkischen Könige begründeten ihr Königtum religiös. Sie verstärkten die Einbindung der Kirche in ihre Herrschaftsausübung. Bischöfe und Äbte hatten neben den religiösen auch weltliche Funktionen, was unter Karl dem Großen auch die Kriegsführung einschloss. Vor dem Hintergrund nahmen sie nicht nur maßgeblichen Einfluss auf die Besetzung der Kirchenämter, sondern auch auf die Entscheidung religiöser Fragen. Klöster spielten sowohl im politischen als auch religiösen Bereich eine tragende Rolle. Neu gegründete Klöster, wie das Kloster Fulda, waren Ausgangspunkte für die Missionsarbeit östlich des Rheins. Die angelsächsischen Missionare propagierten die Ordensregel des Benedikt von Nursia, die im Laufe des Jahrhunderts zunehmend Grundlage für das Leben in den Klöstern wurde.

Das Frankenreich prägte über die erste Hälfte des Jahrhunderts hinaus ein geringer Bildungsstand der Bevölkerung einschließlich der Eliten. Dem setzte Karl der Große am Ende des Jahrhunderts die Karolingische Renaissance, auch Bildungsreform genannt, entgegen. Akteure der Reform waren zum einen bedeutende Gelehrte, die er an den Hof einlud, zum anderen die Klöster. Unter dem Motto „correctio“, Korrektur, fand eine Überarbeitung und Vereinheitlichung der politischen und religiösen Regeln im Frankenreich statt. So wurden liturgische Texte und der Bibeltext redigiert. Ferner wurde die lateinische Sprache als Verkehrssprache im Frankenreich eingeführt und eine einheitliche Schrift, die karolingische Minuskel, entwickelt. Die Bildung und der Wissenstransfer wurde vor allem Klöstern aber auch Bistümern übertragen. In den Klöstern wurden zahlreiche Schriften der Antike kopiert und getauscht. Es wurden Dom- und Klosterschulen eingerichtet, wo sowohl künftige Kleriker als auch Laien unterrichtet wurden. Die karolingische Bildungsreform beförderte einen kulturellen Austausch zwischen den europäischen Regionen. Als herausragendes Bauobjekt wurde in den letzten Jahren des Jahrhunderts die Pfalzkapelle begonnen, bei deren Bau Anleihen von byzantinischen Bauten in Italien genommen wurden.

In Britannien und Irland wurde zu Beginn des Jahrhunderts die Entwicklung einer vorwiegend religiösen Schriftkultur fortgesetzt. Träger waren besonders die Klöster, in die Evangelien der insularen Buchmalerei geschrieben und gemalt wurden. Besonders bekannter Vertreter klösterlichen Gelehrsamkeit war Beda Venerabilis.

Nach dem Verlust von zwei Dritteln seines Territoriums im 7. Jahrhundert erstreckte sich das byzantinische Reich in diesem Jahrhundert auf Kleinasien, Teile der italienischen Halbinsel, Teile der Südausläufer des Balkans und mehrere Mittelmeerinseln. In der ersten Jahrhunderthälfte griffen die Araber das Reich durch wiederkehrende Angriffe in Kleinasien an. Nach der erfolgreichen Abwehr ihrer Belagerung Konstantinopels in den Jahren 717/18 konnte sich Byzanz jedoch zunehmend besser verteidigen. Während des Abwehrkampfes von Konstantinopel vernichteten die Byzantiner die arabische Flotte weitgehend und brachen damit die arabische Seeherrschaft auf dem Mittelmeer. Der Bürgerkrieg um das Kalifat ermöglichte Byzanz in den Jahren 750 bis 775 große Gebiete, die es im vorherigen Jahrhundert an die Bulgaren verloren hatte, von diesen zurückzuerobern. Aufgrund erneuter Angriffe des Kalifats stellte es die Eroberungen ein und verlor in den 790er Jahren einen Teil der zurückeroberten Gebiete auf dem Balkan wieder an die Bulgaren.

Die Gesellschaft spiegelte die ständigen Angriffe und Kriegszüge, indem sie sich im Wesentlichen an militärischen Belangen orientierte. Der im 7. Jahrhundert begonnene Ausbau der Gliederung des Reiches nach Militärbezirken, den Themen, in denen die Militärführer auch zivile Aufgaben wahrnahmen, wurde fortgesetzt. Einem bedeutenden Teil der Soldaten gehörte gleichzeitig Landbesitz. Vom 726 bis in die 780er Jahre beherrschte eine religiöse Auseinandersetzung über den richtigen Gebrauch und die Verehrung von religiösen Ikonen, byzantinischer Bilderstreit. Die religiöse Streitfrage, die auch in anderen Teilen der Christenheit diskutiert wurde, wurde im Byzanz zu einer innenpolitischen Auseinandersetzung.

Seit Mitte des 7. Jahrhunderts wurde das Kalifenreich von Kalifen der Umayyaden-Dynastie regiert. Die Fortführung der islamischen Expansion brachte den größten Teil der iberischen Halbinsel, Transoxanien und das Indusgebiet unter ihre Kontrolle. Diese Eroberungen trugen dazu bei, dass Kalif Hischam (724-742) über das flächenmäßig größte Reich herrschte, das es bis dahin auf der Welt gab. Nach den Erfolgen mehrten sich die militärischen Niederlagen an mehreren Fronten. Der starken Belastung der Staatskasse, die durch diese militärischen Aktivitäten verursacht wurde, begegneten die Kalifen mit deutlichen Steuererhöhungen.

Innenpolitisch stützten sich die Umayyaden auf wechselnde Mehrheiten arabischer Clans, schafften es jedoch nicht, der insbesondere ab den 740er Jahren aufkommenden Unzufriedenheit darüber, wie die Beute aus den Eroberungen und das Steueraufkommen verteilt wurden, zu begegnen. Die Unzufriedenen stellten im Jahre 747 eine vorwiegend arabische Rebellenarmee auf, die im Jahr 750 die Umayyaden stürzte. Die Rebellion wurde von zahlreichen Muslimen unterstützt, die die Legitimität der Umayyaden anzweifelten, da sie nicht von Mitgliedern der Familie Mohammeds abstammten. Ferner spielten persische Konvertiten, Mawālī, die sich von den Machthabern gegenüber den arabischen Muslimen zurückgesetzt fühlten, eine große Rolle beim Sturz der Dynastie. An die Spitze der Rebellion stellte sich die arabische Familie der Abbasiden, die als Nachkommen eines Onkels Mohameds nach Ansicht der Rebellen eine größere Legitimität hatten. Mit der Machtübernahme der Abbasiden stoppte die islamische Expansion. Durch einen Sieg in der Schlacht am Talas gegen die Chinesen wurde im Jahr 751 die arabische Vormachtstellung in Zentralasien abgesichert. In der folgenden Zeit regionalisierte sich die islamische Herrschaft. Diesen Prozess leitete Abd ar-Rahman I. ein, einer der wenigen Umayyaden, der das Blutbad, das die Abbasiden unter seiner Dynastie anrichteten, überlebte. Er entzog kurz nach deren Machtübernahme die iberische Halbinsel der politischen Kontrolle der Abbasiden und errichtete dort das Emirat von Córdoba. Zum Ende des Jahrhunderts verloren dann die Abbasiden die Kontrolle über den Maghreb an eine lokale Dynastie.

Kennzeichnend für das 8. Jahrhundert war eine zunehmend konkrete Ausgestaltung islamischer Herrschaft. Der Prozess der Arabisierung und der Islamisierung der Gesellschaft entfalteten zunehmend ihre Wirkung. Anfang des Jahrhunderts führten die Übertritte nicht arabischer Bürger des Kalifenreiches zum Islam zu einem Rückgang der Einnahmen aus der Grundsteuer und der Kopfsteuer, Dschizya, von denen alle Muslime befreit waren. Dem begegneten die Kalifen zumindest im Irak, indem sie die Grundsteuer unabhängig von der Religionszugehörigkeit erhoben. Da die Konvertiten wie alle Muslime statt der Kopfsteuer eine Abgabe, Zakāt, entrichten mussten, wurden die fiskalischen Hindernisse einer zunehmenden Islamisierung ausgeräumt. Die Islamisierung erfolgte regional unterschiedlich, so waren die persischen Eliten schnell zum Islam übergetreten, während in Ägypten die Islamisierung sehr verhalten erfolgte. Insgesamt waren große Teile der Bevölkerung des Kalifenreiches im 8. Jahrhundert Nicht-Muslime. Die Arabisierung war mit der Islamisierung nicht identisch. Mit der Einführung als Verwaltungssprache zum Ende des 7. Jahrhunderts wurde die arabische Sprache zur allgemeinen Verkehrssprache im Kalifenreich sowie zur Sprache der Wissenschaft. So war Arabisch zunächst die Sprache der gebildeten Eliten, einzig in Persien konnte sich das Arabische dauerhaft nicht durchsetzen.

Die Herrschaft der Umayyaden baute auf wechselnde arabische Clans und Gruppen. Die höchsten Posten wurden fast nur an Mitglieder arabischer Abstammung vergeben. Um den Kontakt zur arabisch beduinischen Kultur zu pflegen, bauten die Kalifen Wüstenschlösser in die syrische Wüste. Dennoch wohnte die überwiegende Zahl der arabischen Auswanderer in den Städten des Reiches. Hier errichteten die Umayyaden große Bauten, wie die Umayyaden-Moschee in Damaskus, die die neue islamische Kultur repräsentieren sollten.

Mit dem Machtwechsel zu den Abbasiden wandelte sich dieses „arabische Reich“ zu einem „islamischen Reich“. Diese Kalifendynastie strebte eine Gleichbehandlung der Muslime arabischer und nicht-arabischer Herkunft an. War Syrien die Machtbasis der Umayyaden, lag die Machtbasis der Abbasiden vornehmlich auf dem Gebiet des ehemaligen persischen Sassanidenreichs. Zunächst errichteten die neuen Kalifen einen Palast und eine Moschee nahe der Ortschaft Bagdad. Um den Palast siedelten sie kreisförmig Behörden und Armee an. Schnell bildeten sich an den Rändern der Stadt Märkte und Vorstädte, sodass Bagdad zum Ende des Jahrhunderts eine der größten Städte der Welt war. Mit den Abbasiden wies das Hofzeremoniell der Kalifen erstmals große Ähnlichkeiten mit dem der persischen Könige auf. Die Reichsverwaltung legten sie in die Hände eines Wesirs, ein Amt von großer Macht, das über einen langen Zeitraum in der Hand der persischen Familie der Barmarkiden lag. Wie dieses Amt wurden viele Ämter des zentralistisch geführten Reiches von Persen bekleidet.

Es folgte ein starker wirtschaftlicher Aufschwung. Begünstigt durch das Arabische als Verkehrssprache entwickelte sich ein ausgedehntes, muslimisches Händlernetz im Kalifat und über seine Grenzen hinaus. Auch über dieses Händlernetz erlangten die Kalifen Wissen aus den Nachbarstaaten, das sie zusammentragen ließen. Zum Ende des Jahrhunderts begannen sie, bedeutende griechische Schriften der Antike ins Arabische übersetzen zu lassen. Die griechischen Schriften und das zusammengetragene Wissen bildeten in den folgenden Jahrhunderten die Basis für die islamische Wissenschaft und Kultur. In diesem Jahrhundert entstanden die meisten für die Auslegung des islamischen Rechts, Scharia, bis heute bedeutenden Rechtsschulen. Ferner wurde das Leben Mohammeds zum ersten Mal aufgeschrieben und ein Werk über vorislamische Kulte Arabiens verfasst.

Am Horn von Afrika brach die königliche Zentralgewalt im aksumitischen Reich zusammen. Die Araber errangen die Seeherrschaft im Roten Meer und eroberten große Gebiete des heutigen Eritreas, Dschibutis und Somalias. Damit schnitten sie Aksum den Zugang zum Meer ab. Im äthiopischen Hochland blieb jedoch die christliche Kultur erhalten und vom Islam unabhängig. Die äthiopisch-orthodoxe Kirche spielte in den nächsten Jahrhunderten eine bedeutende Rolle durch den Schutz für die Bauern. Ferner übernahm sie die Erhaltung der Schriften, die in die lokale Sprache übersetzt wurden. Die Bildung der Aristokraten erfolgte in Klöstern. Die Kirche hielt auch Verbindungen zu den Christen Ägyptens und Nubiens aufrecht.

Die im vorherigen Jahrhundert begonnene Vereinigung der nubischen Reiche Nobatia und Makuria wurde abgeschlossen. Das nun Makuria genannte Reich genoss aufgrund eines Friedensvertrags mit dem muslimisch beherrschten Ägypten seine Unabhängigkeit. Als Tribut mussten jedoch jährlich Sklaven an Ägypten übergeben werden. Am Ende des Jahrhunderts erlebten die nubischen Reiche Makuria und Alwa einen wirtschaftlichen Aufschwung, der sich in der Entwicklung der Städte widerspiegelte.

Die ostafrikanische Küste war in diesem und im folgenden Jahrhundert Ziel arabischer Einwanderer, die in diesem Jahrhundert bis nach Sansibar kamen. An der Küste entstanden die Swahili-Handelsstädte die vom Islam geprägt waren. Die Städte wurden neben den eingewanderten Arabern hauptsächlich von Afrikanern der Bantu-Völkergruppe bewohnt. Die Handelskontakte dieser Städte reichten über den gesamten indischen Ozean, aber auch ins afrikanische Hinterland.

Den indischen Subkontinent teilten sich mehrere Regionalreiche. Ab der Jahrhundertmitte bauten die Dynastien der Pala im nordöstlichen Bengalen, der Pratihara im Nordwesten und die der Rashtrakuta auf dem Dekkan-Plateau im Westen des Subkontinents größere konkurrierende Reiche auf. Diese führten in den folgenden Jahrhunderten untereinander Kriege um die Vorherrschaft im Norden Indiens. Durch mehrere Schlachten konnten die Pratihara die Expansion des Kalifenreiches in die östlich des Indus gelegenen Gebiete vereiteln. Die Rashtrakuta-Dynastie löste durch militärische Siege die Chalukya-Dynastie als Herrscher des Dekkan-Plateau ab. Auch wenn die Pala-Könige den Buddhismus in ihrem Herrschaftsgebiet förderten, so verlor er im Rest des Subkontinents zugunsten des Hinduismus zunehmend an Anhängern und Bedeutung. Die meisten Herrscher nutzten den Hinduismus zur Legitimierung ihrer Herrschaft. Die Gesellschaft war in Gruppen, die Kasten, gegliedert, wobei Einwanderer flexibel in das Kastensystem eingeordnet wurden. Die Zugehörigkeit zu einer Kaste, die durch Geburt erworben wurde, bestimmte religiöse und gesellschaftlichen Pflichten und Rechte.
Ab dem 8. Jahrhundert wurden auch größere Gebiete außerhalb der Flusstäler durch Bewässerungsfeldbau für die intensive landwirtschaftliche Nutzung erschlossen.

Nach der Entmachtung der Kaiserin Wu Zhao im Jahr 705 kam die Tang-Dynastie, die die chinesischen Kaiser bis zum Jahr 907 stellte, wieder an die Macht. Während der Regentschaft von Kaiser Xuanzong in den Jahren 712 bis 756 erlebte China eine innenpolitisch stabile und friedliche Zeit, wirtschaftlicher Prosperität und kultureller Blüte, die oft auch "goldenes Zeitalter" genannt wird. Im Gegensatz dazu war das Reich an seinen Grenzen in häufige militärische Auseinandersetzungen verwickelt. Im Norden und Nordosten griffen die Kitan und das zweite türkische Reich regelmäßig an, im Südosten gab es Auseinandersetzungen mit den Tibetern. Im Nordwesten expandierte China entlang der Seidenstraße und erlangte Einfluss auf Zentralasien. Nach der Niederlage in der Schlacht am Talas gegen das muslimische Kalifat verlor China jedoch seinen Einfluss auf Zentralasien wieder. Um die militärischen Herausforderungen zu bewältigen, wurde die Armee von Milizsoldaten, die ihre eigenen Felder bestellten, auf eine Armee aus Berufssoldaten umgestellt. Die Grenztruppen wurden Militärgouverneuren, den Jiedushi, unterstellt. Diese gewannen im Laufe der Zeit immer mehr Macht, wodurch sich Spannungen zwischen ihnen und der Zentrale aufbauten. Diese mündeten im Jahr 755 in einem vom Militärgouverneur An Lushan geführten Aufstand. Zwar konnte der Kaiser den Aufstand mit Hilfe der benachbarten Turkstämme, der Uiguren und der Tibeter, niederschlagen, doch die bürgerkriegsartigen Auseinandersetzungen richteten erhebliche Zerstörungen in den Hauptstädten Luoyang und Chang’an sowie in großen Teilen des Landes an. Der Aufstand des An Lushan schwächte die Macht der nachfolgenden Tang-Kaiser erheblich zugunsten der Militärgouverneure, die in ihren Machtbereichen mit einem hohen Grad an Autonomie herrschten. Von der Schwäche der Kaiser profitierten auch die Nachbarreiche. Die Tibeter plünderten in den folgenden Jahren mehrmals Chang’an und verwüsteten große Gebiete in China. Im Jahr 791 eroberten sie das Tarimbecken und die darin liegenden Abschnitte der Seidenstraße. China verlor dadurch seinen direkten Zugang nach Zentralasien.

Zu Beginn des Jahrhunderts konzentrierte sich die Bevölkerung entlang der fruchtbaren Ufer des Gelben Flusses, wobei ein kleiner Teil in Städten wohnte, die weit größer waren als die Europas zu dieser Zeit. Betrug die Bevölkerung in der ersten Jahrhunderthälfte rund 50 Mio. Menschen, so verursachten die mit den Unruhen in der Mitte des Jahrhunderts verbundenen Plünderungen und Zerstörungen eine Verringerung der Bevölkerung. Ferner begann eine bis zum 12. Jahrhundert anhaltende Migration vom Norden Chinas in den Süden. Über die Jahrhunderte kam es dort zu einer massiven Ausweitung der Nutzung landwirtschaftlicher Flächen und die Wirtschaft des Südens expandierte.

In der ersten Jahrhunderthälfte florierte in allen Teilen Chinas die Wirtschaft. Güter wurden in großen Mengen aus zahlreichen Gebieten Asiens ein- und ausgeführt. Dies erfolgte hauptsächlich über die Seidenstraßen. Guangzhou, das heutige Kanton, war der wichtigste Hafen des Landes. Hier legten Schiffe aus Südostasien, Ceylon, Indien, Persien und Arabien an. Durch ansässige Kaufleute aus diesen Ländern war die Stadt multikulturell ausgerichtet. In der zweiten Jahrhunderthälfte wurden die chinesischen Handelsrouten zu den benachbarten Völkern durch die Eroberungen der Tibeter und durch Unruhen in China unterbrochen oder stark gestört.

Zu Beginn des Jahrhunderts war das Land aufgrund des im vorherigen Jahrhundert eingeführten Systems der „gleichmäßigen Landverteilung“ relativ gleichmäßig unter der Landbevölkerung verteilt. Diese pachtete das Land vom Kaiserhaus. Nach dem Aufstand von An Lushan entstanden im zunehmenden Maße privater Grundbesitz und große Landgüter, auf denen abhängige Bauern, teilweise als Schuldsklaven, arbeiteten.

Die Finanzierung des Staates erfolgte zunächst allein über ein Steuersystem. Vor dem Aufstand von An Lushan erzielte der Staat seine Einnahmen mittels einer Kopfsteuer, die auf dem System der gleichen Landverteilung basierte. Sie wurde meistens in Naturalien und Dienstleistungen geleistet. Nach dem Aufstand war das Land wieder sehr ungleich verteilt. So wurde im Jahr 780 eine Steuer auf Vermögen und Land eingeführt, die in Geld errichtet werden musste. Dies förderte die Ausweitung der Geldwirtschaft in China.

Bis zur Mitte des Jahrhunderts wurde China stark zentralistisch regiert, wobei der Kaiser an der Spitze stand. Dieser übte seine Herrschaft mittels hierarchisch organisierter Beamten aus. Im 7. Jahrhundert auch im 8. Jahrhundert erhielten vorwiegend Menschen privilegierter Herkunft den Beamtenstatus meist auch ohne Zugangsprüfungen aufgrund von Empfehlungen. Hohe Beamtenstellungen wurden von Adeligen bekleidet. Dennoch büßten während und nach der Herrschaft Wu Zhao einige der alten Adelsfamilien ihre Stellung ein. Zunehmend nahmen Beamte, die ihren Status über Zugangsprüfungen bekamen, bedeutende Stellungen bei Hof ein. Nach dem Aufstand von An Lushan nahm das Prüfungssystem weiter zu. Zentrale Bildungsinstitutionen, wie Akademien und Hochschulen, professionalisierten das Bildungswesen und gewannen an Einfluss.

In der ersten Jahrhunderthälfte entfaltete sich in den Städten ein umfangreiches Kunstleben, das vom Kaiserhaus gefördert wurde. Dichter, wie Li Bai, Du Fu und Wang Wei, und Maler, wie Zhang Xuan und Zhou Fang, erschufen weit über die Epoche hinaus beachtete Werke. Sie stellen die Verlorenheit des Individuums in der Welt dar. Nach dem An Lushan Aufstand diskutierten die Literaten zunehmend kritischer. Sie setzten sich kritisch mit politischer Ökonomie und dem Konfuzianismus auseinander. Eine rationale Weltanschauung gewann an Bedeutung. Insgesamt wurde in China in diesem Jahrhundert eine große Zahl literarischer Werke erstellt. Die Maler malten neben religiösen Bildern bevorzugt die Damen des Hofes.

Der Daoismus und der Buddhismus der Chan-Schule waren die vorherrschenden Religionen in China. Beide wurden von den Kaisern gefördert, aber auch reglementiert, um ihre Macht zu begrenzen. Neben diesen war der Konfuzianismus für Gesellschaft und Staat ein führendes Leitbild. Trotzdem der Konfuzianismus eher eine philosophische und politische Lehre war, wurden Konfuzius und seinen Schülern Tempel errichtet, in denen sie rituell verehrt wurden.

In Japan begann mit der Verlegung der Hauptstadt nach Heijō-kyō (heute Nara), die Nara-Zeit. Die Hauptstadt war mit ungefähr 200.000 Bewohnern die bevölkerungsreichste Stadt des sonst ländlichen Japans. Das herrschende Rechtssystem Ritsuryō verlieh dem Tennō zentrale Macht, die er mit Hilfe von ihm abhängiger Beamter ausübte. Die Oberschicht orientierte sich auf den meisten Gebieten am China der Tang-Dynastie. Stadtplanung, Mode, Recht und Schrift orientierten sich am chinesischen Vorbild. Nara wurde zu dieser Zeit an die Seidenstraße angeschlossen. Geschichtswerke entstanden, zahlreiche in den vergangenen Jahrhunderten entstandene Gedichte wurden in der Sammlung Man’yōshū kompiliert und erste Vorläufer der Manga entstanden. Mitte des Jahrhunderts förderte der Tenno die Missionierung seiner Untertanen zum Buddhismus, den er mit dem Shintōismus, der traditionellen Religion der bäuerlichen Bevölkerung, verband. Hohe Steuerlasten und Abwesenheiten für den Wehrdienst schwächten die Bauern, so dass die Versorgung der Hauptstadt zur Jahrhundertmitte gefährdet war. Die Versorgung wurde verbessert, in dem die Möglichkeiten, Land als Privatbesitz zu erwerben, erweitert wurden. Die zunehmende Aneignung von Land durch Provinzadelige führte in den folgenden Jahrhunderten zu einer Schwächung der Tenno. Gegen Ende des Jahrhunderts war dem Kaiser die Macht der buddhistischen Klöster in Nara zu groß geworden und er zog nach Heian-kyō, dem heutigen Kyōto, das bis ins 19. Jahrhundert japanische Hauptstadt und Sitz des kaiserlichen Hofes bleiben sollte.

In den Steppen nördlich von China etablierte sich das im vorherigen Jahrhundert gegründete zweite türkische Reich. Zur Jahrhundertmitte führten innenpolitische Auseinandersetzungen in diesem Nomadenreich zu dessen Schwächung. Dies nutzten die Uighuren, eroberten das Reich und errichteten nördlich von Tibet und China ihr Großreich.

Das Königreich Tibet war im 8. Jahrhundert eine bedeutende Regionalmacht, die mit China in Rivalität stand. Häufige militärische Auseinandersetzungen waren die Folge. Zum Ende des Jahrhunderts nutzte es die Schwäche Chinas und gewann die Kontrolle über die Abschnitte der Seidenstraße im Tarimbecken. Mitte des Jahrhunderts kam der indische Mahayana- und Vajrayana-Buddhismus nach Tibet und die Nyingma-Schule des tibetischen Buddhismus entstand. Diese Richtungen des Buddhismus hatten Ähnlichkeiten mit der im tibetischen Volk verbreiteten Bo-Religion. So gelang es Mönchen dieser Schule, gefördert vom tibetischen Königshaus, den Buddhismus, der schon im vorherigen Jahrhundert die vorherrschende Religion der Hauptstadt geworden war, auch im Volk zu etablieren. Mitte des Jahrhunderts entstand südöstlich von Tibet das Königreich Nanzhao.

Das Reich Balhae erstreckte sich über die südliche Mandschurei und den Norden der koreanischen Halbinsel. Den wesentlichen Teil der Halbinsel beanspruchte das Reich Silla. Die Gesellschaft dieses Reiches war in Stände gegliedert, an deren Spitze ein König stand, dessen Amt erblich war. Das Verwaltungssystem orientierte sich am chinesischen Vorbild, wobei der Zugang zu den Beamtenstellen vom Adelsrang, der sogenannten "Knochen-Klasse" abhängig war. Die Könige setzten ihre im vorherigen Jahrhundert begonnenen Versuche fort, die Macht des Hochadels zugunsten ihrer Macht zu schwächen. Nach einer Reihe von Verschwörungen der alten Adelsclans wurde im Jahr 780 König Hyegong getötet. Der ihm folgende König Sondok gab den Adeligen die Rechte zurück, die ihnen seit dem Jahr 689 genommen wurden. Die zentrale Religion in Korea war der Buddhismus. Über buddhistische Lehren gab es einen intensiven Austausch mit China und Japan.

In Südostasien führte das Königreich Srivijaya, das buddhistisch geprägt war, seine im 7. Jahrhundert begonnene Expansion fort. Durch Kriege und Handel dehnte es seinen Einflussbereich auf den Süden der Malaiischen Halbinsel und Teile Javas aus. Die Ausdehnung ging mit einer zunehmenden Seemacht über die angrenzenden Seegebiete einher. Dazu gehörte auch die Straße von Malakka, die Teil der maritimen Seidenstraße war. In der zweiten Jahrhunderthälfte wurden jedoch bedeutende Teile Javas von der Sailendra-Dynastie, die zum Buddhismus übergetreten war, beherrscht. Es wird geschätzt, dass sie zum Ende des Jahrhunderts begannen, die bedeutende buddhistische Tempelanlage Borobudur erstellen zu lassen.

In Mittelamerika war das Reich der Maya in seiner späten klassischen Epoche, bevor im 9. Jahrhundert sein Niedergang begann. Im Westen Südamerikas stand die Tiahuanaco-Kultur, eine Prä-Inka-Kultur, in voller Blüte. Auch die nördlich von dieser gelegene Wari-Kultur setzte ihren Aufstieg fort. Für beide Anden-Kulturen spielten aufwendig hergestellte Textilien eine große Rolle.











</doc>
<doc id="8343" url="https://de.wikipedia.org/wiki?curid=8343" title="7. Jahrhundert">
7. Jahrhundert

Das 7. Jahrhundert begann am 1. Januar 601 und endete am 31. Dezember 700.

Die Weltbevölkerung in diesem Jahrhundert wird auf 200 bis 300 Millionen Menschen geschätzt. In Europa konsolidierten sich die aus der Völkerwanderung hervorgegangen germanisch-romanisch beherrschten Reiche der Franken, Westgoten und Langobarden. Der Begründung des Islam folgte die islamische Expansion, die eine signifikante Änderung der Herrschaftsverhältnisse im Nahen Osten und im Mittelmeerraum zur Folge hatte. Einte die griechisch-römische Kultur in den vorherigen Jahrhunderten die Staaten um das Mittelmeer, so beendete die islamische Expansion diese Einheit. Ab dem 7. Jahrhundert trennt das Mittelmeer mehr den christlichen Norden vom islamischen Süden, als dass es die Staaten an seinen Ufern eint. Den indischen Subkontinent teilten sich mehrere Fürstentümer, die miteinander konkurrierten, sich jedoch auch kulturell beeinflussten. China gewann unter der Tang-Dynastie an Größe, Macht und Einfluss. Seine Kultur übte einen prägenden Einfluss auf die anderen Staaten Ostasiens aus.

Im Bezug auf die Geschichte Europas wird dieses Jahrhundert der ausgehenden Spätantike bzw. dem beginnenden Frühmittelalter (je nach Region ca. 500–1050) zugeordnet. Zu Beginn des Jahrhunderts gelingt es dem merowingischen König Chlothar II. das durch den merowingischen Bruderkrieg geteilte Frankenreich wieder zu vereinen. Als Preis für die Einigung gestand der König dem Adel im Edictum Chlotharii zu, dass alle lokalen Amtsträger (Grafen) nur aus dem grundbesitzenden Adel der jeweiligen Region gewählt wurden. An die Spitze der Teilländer Austrien und Neustrien wurde ein Hausmeier gestellt. Nach dem Tod seines Sohnes, König Dagobert I., im Jahr 639 wurde das Reich administrativ geteilt, wobei Austrien und Neustrien jeweils von einem eigenen König regiert wurden. Durch innere Kämpfe und zahlreiche Regierungszeiten minderjähriger Könige verlor das merowingische Königtum zunehmend an Bedeutung. Das stärkte die Stellung der östlich des Rheins wohnenden germanischen Volksgruppen der Thüringer und Alemannen. Zwar gehörten diese immer noch zum Frankenreich, unter der Führung von Stammenherzögen erzielten sie jedoch einen hohen Grad an Autonomie. Ferner wuchs den Hausmeiern der Reichsteile Austrien und Neustrien, deren Amt im Laufe des Jahrhunderts erblich wurde, die faktische Herrschaft über ihren Reichsteil zu. Am Ende des Jahrhunderts konnten die Pippiniden, die späteren Karolinger, das Hausmeieramt beider Teilreiche auf sich vereinen und ihren Aufstieg beginnen. Im folgenden Jahrhundert einigten sie das Frankenreich und vergrößerten es zur dominierenden Macht West- und Mitteleuropas.

Nachdem das toledanische Westgotenreich zu Beginn des Jahrhunderts die letzten oströmischen Gebiete an den Küsten eroberte, beherrschte es die ganze iberische Halbinsel. Im Laufe des Jahrhunderts belasteten Machtkämpfe um das Königsamt das Land und führten zu einer Steigerung der Macht des Adels. So wurde ab dem Jahr 633 der westgotische König von Adeligen gewählt. Dem von König Rekkared I. im Jahr 587 eingeleiteten Wechsel der Westgoten vom arianischen zum katholischen Bekenntnis der iberoromanischen Bevölkerung, folgte im 7. Jahrhundert die Vereinheitlichung des Rechts für beide Bevölkerungsgruppen. Damit waren die trennenden Gegensätze zwischen beiden Bevölkerungsgruppen beseitigt und es entstand diesbezüglich eine innere Einheit.

Das Reich der Langobarden auf der italienischen Halbinsel wurde von Königen regiert, die in rascher Folge wechselten. Das hinderte die Langobarden jedoch nicht ihren Eroberungszug zu Lasten des oströmischen Reiches fortzusetzen. Zur Jahrhundertmitte beherrschten sie große Teile des italienischen Festlandes. Einige Hafenstädte an der Adria, ein Landstreifen in Mittelitalien in der Höhe von Rom und große Teile Süditaliens blieben jedoch oströmisch. Die oströmischen Hafenstädte, Venedig und Ancona an der italienischen Adriaküste ermöglichten den wirtschaftlich wichtigen Handel mit den Kerngebieten des oströmischen Reiches. Auch im Langobardenreich begann im 7. Jahrhundert eine Katholisierung der eingewanderten Germanen. Die Übernahme des Bekenntnisses der römischen Bevölkerungsmehrheit setzte sich jedoch erst Anfang des 8. Jahrhunderts vollständig durch.

Nachdem im vorherigen Jahrhundert das oströmische Reich sein Gebiet auf dem Balkan gegen das Reitervolk der Awaren und die Slawen mit den Balkanfeldzügen des Maurikios erfolgreich verteidigte, zog es Anfang des Jahrhunderts seine Truppen vom Balkan ab, um sie im Kampf gegen die Sassaniden einzusetzen. Dies schuf für die in Pannonien beheimateten Awaren die Gelegenheit ihre Macht zu Lasten des oströmischen Reiches auszudehnen. Ihre im Jahr 626 zusammen mit den Sassaniden durchgeführte Belagerung Konstantinopels scheiterte jedoch. In der ersten Jahrhunderthälfte wanderten Gruppen von Slawen in größerer Zahl in den Balkan ein. In der zweiten Jahrhunderthälfte konnten die slawischen Fürsten auf dem Balkan zunehmend Autonomie im Machtbereich der Awaren gewinnen. Auch im Norden des Awarenreiches führte Samo die Slawen zur Autonomie und gründete das erste slawische Reich Ostmitteleuropas.

Die Chasaren, ein Turkvolk, vergrößerten und festigten ihr nördlich des Kaukasusgebirges gegründetes Reich und begannen sich damit als Regionalmacht zu etablieren. Nach jahrzehntelangen Auseinandersetzungen konnten sie in der zweiten Jahrhunderthälfte das nördlich des Schwarzen Meeres gelegene großbulgarische Reich erobern und zerstören. Einige dort lebende Bulgaren vereinigten sich daraufhin mit in der Nachbarschaft lebenden slawischen Gruppen und zogen auf den Balkan, wo sie um 680 das erste bulgarische Reich gründeten.

Das Leben der Menschen im 7. Jahrhundert war in hohem Maße abhängig von der Natur, so zum Beispiel von der Länge der Tage und dem Nahrungsangebot. Schon in den vorherigen Jahrhunderten führten durch Unwetter verursachte Hungersnöte und Seuchen, wie die justinianische Pest, sowie kriegerische Auseinandersetzungen zu einem Bevölkerungsrückgang, der zur Mitte des 7. Jahrhunderts seinen Tiefpunkt erreichte. Die Kindersterblichkeit war hoch und die Lebenserwartung lag nach überstandener Kindheit bei 44 bis 47 Jahren. Die Menschen ernährten sich überwiegend von Getreideprodukten, ferner von Milchprodukten und Gemüse.

Die Gesellschaft war stark agrarisch geprägt. Der weitaus größte Teil der Menschen wohnte in kleinen Dörfern auf dem Land. Insbesondere in West- und Südeuropa existierten aber auch meist auf römische Gründungen zurückgehende Städte, deren Einwohnerzahl und Bedeutung jedoch erheblich geringer war als vor der Völkerwanderung. War die Verkleinerung der Städte nicht schon in vorherigen Jahrhunderten erfolgt, so setzte sie, wie in einigen Städten des Mittelmeerraums, spätestens in diesem Jahrhundert ein.

Die Gesellschaft der germanisch-romanisch beherrschten Reiche war eine Ständegesellschaft, die sich in Adelige, Freie und Unfreie gliederte, wobei es regionale Unterschiede in der Ausgestaltung der Stände gab. Der jeweilige Status war erblich, jedoch war der gesellschaftliche Aufstieg oder Abstieg möglich und im Gegensatz zum Hochmittelalter viel häufiger. An der Spitze der Adeligen stand der König, der auf die Akzeptanz des Adels angewiesen war. Deshalb musste er bei der Herrschaftsausübung auf diesen Rücksicht nehmen, war er schwach so übernahmen die Adeligen faktisch die Regentschaft. Die Unfreien waren von einem Herren abhängig, der ihnen Schutz zu gewähren hatte, jedoch in fast allen Lebensbereichen über sie bestimmen konnte. Im Gegensatz zu den Sklaven der Antike wurden die Unfreien nicht als rechtliche Sache gesehen, so dass der Herr das Leben und die körperliche Unversehrtheit der Unfreien zu wahren hatte.

Reichtum begründete sich im Wesentlichen auf Landbesitz. Der Grund und Boden gehörte meistens Großgrundbesitzern, wie Königen, Adeligen, Bischöfen oder Klöstern. Diesen bewirtschafteten sie zum Teil mit Hilfe ihrer Unfreien selbst, andere Teile verpachteten sie an freie Bauern. Der Fernhandel, für den die Flüsse einen wichtigen Transportweg darstellten, hatte seit der Spätantike stark abgenommen. Im 7. Jahrhundert erreichte der Mittelmeerhandel einen Tiefpunkt, während sich der Handel des Frankenreiches in dieser Zeit mehr und mehr nach Norden orientierte.

In den Reichen der Franken, Westgoten und Langobarden unterlagen die eingewanderten Germanen, die geschätzt 2 bis 5 % der Bevölkerung stellten, und die Bevölkerung römischen Ursprungs jeweils einem eigenen Recht. Im Westgotenreich führte König Rekkeswinth im Jahr 654 ein einheitliches Recht für beide Bevölkerungsgruppen ein. Die beiden anderen Reiche folgten diesem Beispiel im nachfolgenden Jahrhundert. War eine schriftliche Fixierung des Rechtes der germanischen Einwanderer im Westgoten- und Frankenreich mit dem Codex Euricianus und der Lex Salica schon in den vorausgegangen Jahrhunderten erfolgt, geschah dies im Reich der Langobarden in diesem Jahrhundert durch das Edictum Rothari. Das Recht einiger zum Frankenreich gehörender, östlich des Rheins lebender, germanischer Volksgruppen wurde ebenfalls in diesem Jahrhundert schriftlich niedergelegt.

Die herrschende Religion der kontinentalen Germanenreiche der Franken, Westgoten und Langobarden war das Christentum. Der Bekehrung der germanisch-romanischen Führungsschicht der Franken im vorherigen Jahrhundert, die zunächst oft nur formell war, folgte eine christliche Unterweisung und inhaltliche Bekehrung, die in diesem Jahrhundert fortgesetzt wurde. Neben der Amtskirche spielten hier auch die Klöster eine wichtige Rolle.

Die christlichen Kirchen waren als Nationalkirchen vollständig in das Herrschafts- und Gesellschaftssystem der jeweiligen Reiche eingebunden. Sie nahmen sowohl geistliche als auch weltliche Aufgaben wahr. Herrschaftlich und wirtschaftlich sowie teilweise auch geistlich waren sie dem jeweiligen König unterstellt. Überwiegend wurden Klostergründungen von Königen oder Adeligen vorgenommen, die auch nach der Gründung diese für ihre wirtschaftlichen, herrschaftlichen oder geistlichen Interessen nutzten. Viele der von zahlreichen adeligen Frauen gegründeten Frauenklöster dienten diesen zur Altersversorgung. Den zahlreichen Klosterneugründungen, im Frankenreich hat sich ihre Anzahl in diesem Jahrhundert mehr als verdoppelt, standen schon in diesem Jahrhundert häufige Klagen über die Abkehr des Klosterlebens vom monastischen Ideal gegenüber.

Iroschottische Mönche zogen vom christlichen Irland und Schottland vorwiegend nach England und ins fränkische Reich, um die Bevölkerung zum christlichen Glauben zu bekehren oder diesen bei ihr zu vertiefen. Dazu gründeten sie zahlreiche Klöster. Bei der Bekehrung der Angelsachsen kam es zu Differenzen mit römischen Missionaren, die England von Süden im Auftrag des Papstes missionierten. Diese wurden in der Synode von Whitby zugunsten der römischen Missionare beigelegt. Schon zum Ende des 7. Jahrhunderts begannen angelsächsische Kleriker in Kontinentaleuropa zu missionieren. Die angelsächsische Mission, die im 8. Jahrhundert ihren Höhepunkt erreichte, trug neben der iro-schottischen Mission wesentlich zur Verbreitung des Christentums in Europa bei.

Nur wenige Menschen, fast ausschließlich Kleriker und Angehörige der Oberschicht, waren fähig schriftlich zu kommunizieren, wobei die Verbreitung der Lese- und Schreibfähigkeit zum Ende des Jahrhunderts weiter abnahm. Im 7. Jahrhundert entwickelte sich die Schriftsprache von Latein immer mehr zu einer romanischen Sprache. Als Schreibstoff wurde insbesondere im Frankenreich statt Papyrus immer häufiger Pergament eingesetzt.

Auch im Mittelmeerraum und dem Nahen Osten endete spätestens in der Jahrhundertmitte die Spätantike.

Größere Gebietsverluste als in Europa erlitt das oströmische Reich, das durch innere Unruhen geschwächt war, durch den im Jahr 603 beginnenden Eroberungszug des von Chosrau II. beherrschten persischen Sassanidenreiches. Die Eroberung Syriens, Palästinas und schließlich Ägyptens (619) war für das oströmische Reich besonders wirtschaftlich ein schwerwiegender Verlust. Der seit 610 regierende Kaiser Herakleios schaffte es durch einen siebenjährigen Krieg, für den er alle Ressourcen seines Reiches mobilisierte, die verlorenen Gebiete von den Persern zurückzuerobern. Nach dem Friedensschluss zwischen den Kriegsparteien (629) ließ der Krieg beide Reiche geschwächt zurück. Die folgende islamische Expansion führte ab dem Jahr 634 zu einem jetzt endgültigen Verlust der zurückeroberten Gebiete. Mit den Gebietsverlusten in Italien und auf dem Balkan schrumpfte das Reich bis zum Ende des Jahrhunderts auf ein Drittel des Territoriums, das es zu Beginn des Jahrhunderts beherrschte.

Das oströmische Reich wandelte sich ab dem 7. Jahrhundert so grundlegend, dass es in der Folgezeit von heutigen Historikern "byzantinisches Reich" genannt wird. Den Wandel führten der Verlust von zwei Dritteln des Staatsgebietes, der Verlust bedeutender wirtschaftlicher Ressourcen – insbesondere durch den Verlust Ägyptens – und die Abwehrkämpfe gegen seine äußeren Feinde herbei. Ihn kennzeichnete die Entwicklung von einer kulturell und religiös heterogenen Gesellschaft, mit vielen städtischen Zentren, zu einer vom griechisch-orthodoxen Bekenntnis und der griechischen Kultur geprägten Gesellschaft. Es begann ein Prozess, der militärischen Aspekte in Gesellschaft und Staat einen immer stärkeren Rang einräumte. Neue von Militärgouverneuren geleitete Militärbezirke, die Themenbezirke entstanden. Damit verbunden war die Wandlung des Heeres von einem steuerfinanzierten Berufsheer in ein regional organisiertes, durch Landbesitz abgegoltenes Heer. Waren die Themenbezirke in diesem Jahrhundert noch lokal beschränkt, verbreiteten sie sich in den folgenden Jahrhunderten über das gesamte Reich und verdrängten die zivile Verwaltung.

Ab dem Beginn des Jahrhunderts warb Mohammed auf der arabischen Halbinsel Anhänger und stiftete eine der Weltreligionen, den Islam. Er vereinte die unterschiedlichen Stämme und Gruppen der arabischen Halbinsel in einer übergeordneten Gemeinschaft, der Umma. Als seine Nachfolger wurden die Kalifen gewählt. Die ersten vier Kalifen stammten aus der näheren Verwandtschaft Mohammeds und werden auch „rechtgeleitete Kalifen“ genannt. Für einen Teil der Muslime hatten nur ʿAlī ibn Abī Tālib, der vierte „rechtgeleitete Kalif“ und Schwiegersohn Mohammeds, und dessen Nachkommen einen legitimen Anspruch auf das Kalifenamt. Die unterschiedliche Auffassung über die Rechtmäßigkeit der Nachfolge Mohammeds teilt bis heute die Muslime in solche, die nur Ali und seine Nachkommen anerkennen, die Schiiten, und solche die alle „rechtgeleiteten Kalifen“ anerkennen, die Sunniten.

Mitte der 630er Jahre begann die militärische Expansion des Kalifenreiches, auch islamische Expansion genannt. Die Araber eroberten große Gebiete des oströmischen Reiches, wie Syrien, Palästina und Ägypten. Neben der gewaltsamen Erstürmung der Städte war die Verhandlung einer Kapitulation eine Methode der Eroberung. Im Jahr 642 errangen die Araber einen entscheidenden Sieg über das Sassanidenreich. Dennoch zogen sich die anschließenden Eroberungen des Ostens des Reiches noch einige Jahre hin. Zur Sicherung ihrer Eroberungen stationierten die Araber Truppen in den bestehenden Städten oder gründeten Militärlager, aus denen mit der Zeit Städte, wie das irakische Basra, entstanden. Die Eroberungszüge unter den ersten Kalifen wurden im Wesentlichen autonom von lokalen Anführern gesteuert, die parallel zueinander operierten.

In der Mitte des Jahrhunderts kam es zu Auseinandersetzungen zwischen den arabisch-muslimischen Anhängern und Gegnern des vierten Kalifen ʿAlī ibn Abī Tālib. In deren Folge kämpften zum ersten Mal zwei muslimische Armeen gegeneinander. Nach Alis Tod im Jahr 661 setzte sich sein Gegenspieler Muʿāwiya I. als Kalif durch. Dieser unterschied sich von seinen Vorgängern, weil er weder aus dem familiären Umfeld Mohammeds stammte, noch sich bei seiner Unterstützung Verdienste erworben hatte. Vielmehr stammte er von den mekkanischen Machteliten ab, die den Propheten der Muslime zunächst bekämpften. Da Muʿāwiya zuvor Gouverneur der Provinz Syrien war, wo er seine Machtbasis hatte, verlegte er die Hauptstadt vom arabischen Mekka ins syrische Damaskus. Indem er durchsetzen konnte, dass sein Sohn zu seinem Nachfolger als Kalif erklärt wurde, begründete er die Umayyaden-Dynastie. Da mehrere arabische Gruppen damit nicht einverstanden waren, kam es im Jahr 680 zu einem Bürgerkrieg, den erst der nachfolgende umayyadische Kalif Abd al-Malik im Jahr 691 beenden konnte. Auch unter den Kalifen der Umayyaden-Dynastie wurde das Kalifenreich durch Eroberung weiter ausgedehnt, so dass es gegen Ende des Jahrhunderts ein Gebiet von Nordafrika bis nach Zentralasien umfasste.

Während die Eroberer die politische und militärische Gewalt in den eroberten Gebieten übernahmen, ließen sie die Zivil- und Finanzverwaltung bestehen. Damit gab es nur wenige einheitliche Strukturen im Kalifenreich des 7. Jahrhunderts. Erst zum Ende des Jahrhunderts wurde unter Abd al-Malik das Griechische und Persische als Amtssprache durch das Arabische ersetzt. Die Einführung des Dinars als Währung des Kalifenreiches demonstrierte die arabische Herrschaft, führte jedoch nicht zu einem einheitlichen Münzsystem.
Die Kalifen setzten Gouverneure ein, die die Provinzen relativ autonom regierten. Auch andere höchste politische und militärische Ämter besetzten die Kalifen mit ihren arabischen Vertrauten, während in der Verwaltung auch hohe Posten von nicht arabischen Muslimen und lokalen Anhängern anderer Religionen bekleidet wurden. Weite Gebiete des heutigen Irak sowie die byzantinischen und sassanidischen Krongüter gingen an die muslimische Gemeinschaft beziehungsweise die Kalifen. Die Kämpfer erhielten anstelle von Sold einen Anteil an der übrigen Beute und einige Kämpfer auch finanzielle Zuwendungen. Der Erhalt dieser Zuwendungen setzte jedoch eine Aufnahme in ein Register, den Dīwān, voraus, die nur verdienten muslimischen Kämpfern zuteilwurde. Die Zuwendungen erfolgten durch Geldzahlungen, die auch aus Steuergeldern finanziert wurden. Diese wurden hauptsächlich von den Nicht-Muslimen aufgebracht, die eine spezielle Kopfsteuer (Dschizya) und eine Grundsteuer entrichten mussten.

Die muslimischen Eroberer übten auf die Bevölkerung der eroberten Gebiete keinen Zwang aus, zum Islam zu konvertieren. Die Anhänger der Buchreligionen, aber auch die Zoroastrier konnten ihren Glauben größtenteils unbehelligt leben, die Glaubensausübung unterlag aber Beschränkungen, die mit der Zeit restriktiver wurden. Da den neuen Herrschern die konfessionelle Ausrichtung der anderen Religionen gleichgültig war, konnten einige Konfessionen, wie die Nestorianer im Irak, sich freier entfalten als unter der alten Herrschaft. Nach der Eroberung konvertierten zahlreiche Menschen zum Islam. Eine große Gruppe der Konvertiten waren Kriegsgefangene, die nach dem Übertritt zum Islam freigelassen wurden. Unter der Leitung der ersten Kalifen wurde der Korantext fixiert und die Anfänge des islamischen Rechts, Scharia, etabliert. Im letzten Jahrzehnt wurden unter dem Kalifen Abd al-Malik bedeutende islamische Bauwerke, wie der Felsendom in Jerusalem, gebaut.

Einhergehend mit der Expansion etablierten arabische Kaufleute in alle Himmelsrichtungen Handelsrouten. Insbesondere der arabische Handel entlang der Seidenstraße mit China sowie mit Indien als auch entlang der ostafrikanischen Küste und zwischen den Gebieten nördlich und südlich der Sahara begann sich zu etablieren.

Mitte des Jahrhunderts konnten die christlichen nubischen Staaten die Eroberungsversuche, die die Araber von Ägypten, das sie zuvor eroberten, aus vornahmen, erfolgreich abwehren. Anschließend schlossen jene Staaten mit dem arabischen Statthalter in Ägypten einen Friedensvertrag, der die christliche Herrschaft in Nubien für die nachfolgenden Jahrhunderte sicherte. In der zweiten Jahrhunderthälfte vereinigten sich zwei der drei nubischen Staaten, Nobatia und Makuria, zu einem einzigen Königreich.

Auch wenn das am Horn von Afrika gelegene, christliche aksumitische Reich zunächst gute Kontakte zum Islam pflegte, hatte die islamische Expansion für es unmittelbare Auswirkungen. Seine politischen und kommerziellen Kontakte zum oströmischen Reich brachen mit den arabischen Eroberungen oströmischer Gebiete im östlichen Mittelmeerraum ab. Im Zuge dieser Schwierigkeiten intensivierte das Reich seine Beziehungen zu den nubischen Staaten.

Der indische Subkontinent war in mehrere Herrschaftsgebiete aufgeteilt. In der ersten Hälfte des Jahrhunderts konnte Harshavardhana ausgehend von seiner Hauptstadt Kannauj am mittleren Ganges das in Teilfürstentümer zersplitterte Nordindien sukzessive unter seiner Herrschaft vereinen. Seine Expansion nach Süden konnte jedoch durch das südwestindische Chalukya-Reich im Jahr 630 gestoppt werden. Das Reich Harshas brach nach seinem Tod 647 zusammen.

In den Auseinandersetzungen mit ihrem südostindischen Nachbarreich Pallava konnten die Chalukya-Könige zwar taktische, jedoch keine nachhaltigen Siege erziehen. Dennoch kam es im Zuge der Auseinandersetzung zu einem kulturellen Austausch zwischen den Reichen. Bei seinen Feldzügen nach Sri Lanka setzte das Pallava-Reich erstmals in der indischen Geschichte in konzentrierter Form Seestreitkräfte ein und schaffte damit eine Basis, auf der der Nachfolgestaat der Chola seine Seeherrschaft vom 10. bis zum 12. Jahrhundert aufbauen konnte.

Zur Ausübung der Herrschaft in weiten Teilen ihres Herrschaftsgebietes bedienten sich die Herrscher größerer indischer Reiche, wie Harsha, der Hilfe verbündeter Herrscher und unterworfener Regionalfürsten.

Wichtigstes Element der indischen Wirtschaft war die Landwirtschaft. In diesem Jahrhundert wurden wie in den vergangenen und folgenden Jahrhunderten die Ausdehnungen der landwirtschaftlichen Flächen kontinuierlich fortgesetzt. Besonders in Südindien wurden komplexe Bewässerungssysteme genutzt, erweitert und verbessert.

In Indien waren Hinduismus, Buddhismus und Jainismus nebeneinander verbreitet. Mit dem Tod Harshavardhana im Jahr 647, einem Förderer des Buddhismus, verlor dieser zunehmend Anhänger sowohl bei den Eliten als auch beim Volk. Der Hinduismus gewann auf Kosten der beiden anderen Religionen zunehmend an Bedeutung und Förderung. Buddhistische Klöster, die noch großen Landbesitz hatten, verloren zu Gunsten der Brahmanen an politischem Einfluss. Das 7. Jahrhundert war ein Höhepunkt der Errichtung hinduistischer Höhlentempel.

In Zentralasien waren die westlichen Kök-Türk bis zur Jahrhundertmitte die bedeutendste Regionalmacht. Bis zum Jahr 630 konnten sie ihren Herrschaftsbereich vom kaspischen Meer bis zum Tarimbecken und von der kasachischen Steppe bis zum nördlichen Hindukusch ausdehnen. Die nomadischen Türk profitierten von den Abgaben der ackerbauenden Bevölkerung, die in Gebieten wie Sogdien und den Oasen des Tarimbeckens lebte. Ferner erzielten die Nomaden wirtschaftlichen Wohlstand aus der Kontrolle von Teilen der Seidenstraße. Zusätzlichen Nutzen zogen sie von der Zusammenarbeit mit sogdischen Kaufleuten, die einen bedeutenden Teil des Handels auf der Seidenstraße betrieben. In den oströmisch-sassanidischen Kriegen unterstützen sie Ostrom, indem sie das sassanidische Reich von Osten angriffen. Mit dem Vordringen der Araber im Westen und der Niederlage gegen China im Jahr 657 im Osten verloren die westlichen Türk zunehmend an Einfluss. Die östlichen Türk waren 630 von den Chinesen unterworfen worden und dienten diesen als Söldner. Im Zuge der wachsenden politischen Probleme Chinas am Ende des Jahrhunderts lösten sie sich aus der chinesischen Abhängigkeit und errichteten das "zweite türkische Reich".

In China wurde 618 die Sui-Dynastie von der Tang-Dynastie abgelöst. Der letzte Sui-Kaiser verfolgte das im vorherigen Jahrhundert begonnene Projekt der Einigung des über Jahrhunderte zersplitterten Chinas unter einer zentralistischen Herrschaft. Mehrere Großprojekte, wie die Erweiterung des Kaiserkanals und die Befestigung der Nordgrenze, sowie zahlreiche Feldzüge, besonders die Niederlage gegen das nordkoreanische Goguryeo banden viele Ressourcen, zerrütteten die Staatsfinanzen und forderten große Opfer unter der Bevölkerung. Dieses, wie die Versuche des Kaisers den alten Adel zu entmachten, führten zu zahlreichen Revolten im Land. Vor dem Hintergrund dieser Probleme konnte General "Li Yuan" die Macht erringen und als Kaiser Tang Gaozu die Tang-Dynastie begründen. Diese stellte in den folgenden Jahren die innere Stabilität wieder her. Mit der Unterwerfung der östlichen Kök-Türk, die zuvor zahlreiche Raubzüge in China unternahmen, konnten die Tang die Bedrohung aus dem Norden im Jahr 630 bannen. In den folgenden Jahren dehnten die Tang das Reich insbesondere entlang der Seidenstraße bis nach Zentralasien aus, wobei sie die westlichen Kök-Türk besiegten. Die konkurrierenden Expansionsbestrebungen des chinesischen und des tibetischen Reiches führten in diesem Jahrhundert zu zahlreichen militärischen Auseinandersetzungen.

Durch Intrigen und Machtränke stieg Wu Zetian in der zweiten Jahrhunderthälfte von einer kaiserlichen Konkubine zur faktischen Herrscherin (ab 660) und schließlich zur "Kaiserin Wu Zhao" (690) auf. Sie war die einzige Frau, die jemals China offiziell als Kaiserin regierte. Unter ihrer Führung konnte China die meisten Angriffe seiner Nachbarn auf die Grenzgebiete abwehren.

Durch Siege über die Osttürken und die Westtürken wurde der chinesische Herrschaftsbereich entlang der Seidenstraße ausgedehnt und mit Garnisonen gesichert. Das brachte den Handel auf der Seidenstraße zu einer neuen Blüte. Verstärkt kamen Waren und Ideen aus dem Mittelmeerraum, dem Nahen Osten, Indien und Zentralasien nach China. Die Offenheit der chinesischen Politik galt auch den ostasiatischen Nachbarn. Vermehrten wirtschaftlichen und kulturellen Austausch gab es mit den koreanischen Staaten und Japan. Für einflussreiche Gruppen in diesen Ländern wurde die chinesische Kultur zum Vorbild. Die chinesische Hauptstadt Chang’an, die an der Seidenstraße lag, wird heute vielfach als die größte und kulturell bedeutendste Stadt der Welt zu dieser Zeit angesehen. In ihr lebten Menschen aus vielen Regionen der Welt. Die Existenz kleiner Minderheiten von Juden, Christen und Muslimen, die ihren Glauben relativ frei praktizieren konnten, gilt als Zeichen der Offenheit der Tang-Kaiser. Die Kaiser versuchten Daoismus und Buddhismus für ihre politischen Zwecke zu instrumentalisieren. Dabei griffen sie fördernd, aber auch reglementierend in Glaubensinhalte und -organisation ein. Während die ersten Tang-Herrscher vorwiegend den Daoismus förderten, begünstigte Kaiserin Wu Zhao den Buddhismus, besonders die Chan-Schule. Neben diesen Religionen beeinflusste auch der Konfuzianismus die staatliche Ordnung.

Mit der Stabilität, der Offenheit nach außen und dem wachsenden wirtschaftlichen Wohlstand blühten Dichtkunst, Erfinder- und Entdeckergeist und andere kulturelle Aktivitäten auf. Die Gründung mehrerer staatlicher Hochschulen, die auch der Beamtenausbildung dienten, förderte die Verbreitung von Wissen. In der Enzyklopädie Yiwen leiju versuchte man das erworbene Wissen zu sammeln und geordnet aufzuschreiben. Der buddhistische chinesische Pilgermönch Xuanzang reiste über die Seidenstraße nach Zentralasien und nach Indien. Neben zahlreichen religiösen Schriften, die die Ausbreitung des Buddhismus in China förderten, brachte er Reisebeschreibungen mit, die noch heute als wichtige Quelle für das Leben im damaligen Indien gelten.

Aufgrund der Ergebnisse der damals durchgeführten Volkszählungen wird die Einwohnerzahl Chinas in diesem Jahrhundert auf etwa 50 Millionen geschätzt. Die Bevölkerung konzentrierte sich entlang der fruchtbaren Ufer des gelben Flusses, wobei ein Teil in Städten wohnte, die weit größer als die Europas waren.
Im Laufe des 7. Jahrhunderts setzte ein bedeutender wirtschaftlicher Aufschwung ein. Dieser wurde durch mehrere Faktoren bedingt. Der Kaiserkanal verband den Jangtsekiang mit dem gelben Fluss und damit den südchinesischen Wirtschaftsraum mit dem Nordchinesischen. Das ermöglichte einen verstärkten Import landwirtschaftlicher Produkte aus dem fruchtbaren Süden, was die Verfügbarkeit von Lebensmitteln im Norden steigerte. Ferner wurden die Verwaltung und das Steuersystem reformiert. Der zuvor knappe Umlauf von Kupfermünzen wurde beschleunigt und im Bereich der Kreditinstrumente kam es zu Innovationen. Eine Landreform teilte das Land in genormte Parzellen auf, die Bauern, Beamten und Adel zugeteilt wurden. Porzellan wurde gewerbsmäßig in größeren Mengen hergestellt.

Die Tang-Kaiser übernahmen von den Sui die zentralistische Staatsordnung, die letzte etabliert hatten. Die Herrschaft wurde durch ein hierarchisches Beamtensystem ausgeübt das dem Kaiser unterstellt war. Im Gegensatz zu den Sui würdigten die Tang-Herrscher bei der Herrschaftsausübung und Postenbesetzung stärker die traditionellen Rechte der Adelsschicht. Für einen Beamtenposten konnten sich die Bewerber sowohl durch eine Empfehlung als auch durch eine bestandene Beamtenprüfung qualifizieren, wobei insbesondere die Stellen in den regionalen Präfekturen primär durch Empfehlungen besetzt wurden. Von der Politik des jeweiligen Kaisers hing es ab, ob ein höherer Beamtenposten aufgrund einer Empfehlung oder einer bestandenen Prüfung besetzt wurde. An den Prüfungen für höhere Stellen nahmen überwiegend die Söhne der Eliten teil, so dass unabhängig von der Wahl des Zugangs die hohen Posten von der Elite besetzt wurden.

Die bereits vorhanden schriftlichen Gesetze wurden im 7. Jahrhundert systematisiert und reformiert. Das von den Tang eingeführte Gesetzbuch, das eine Gleichheit vor dem Gesetz anstrebte, war auch in den nachfolgenden Jahrhunderten Grundlage der Strafrechts und wurde später auch in Japan übernommen.

Im Hochland von Tibet gründete Songtsen Gampo das Königreich von Tibet, indem er die dort heimischen Fürstentümer nacheinander unterwarf. Im Zuge einer Expansionspolitik führte Tibet unter seinen Nachfolgern in der zweiten Hälfte des Jahrhunderts Kriege mit China über die Kontrolle des Tarimbeckens, in dem Teile der Seidenstraße verlaufen. Dabei gelang es Tibet in den 670er Jahren größere Teile des Beckens unter seine Kontrolle zu bringen. In den 690er Jahren konnte China diese Gebiete wieder zurückerobern. Im 7. Jahrhundert konnte sich der Buddhismus erstmals in Tibet etablieren, während er sich im folgenden Jahrhundert flächendeckend verbreiten konnte.

Im Norden der koreanischen Halbinsel lag das Reich Goguryeo, das China von Beginn des Jahrhunderts zu erobern suchte. Die im Süden der koreanischen Halbinsel gelegenen Reiche Silla und Baekje waren mit China beziehungsweise mit Goguryeo verbündet. Im Jahr 660 half China dem Königreich Silla seinen Rivalen Baekje zu erobern. Acht Jahre später gelang es Silla und China dann, das Reich Goguryeo zu besiegen. Anschließend versuchten die Chinesen, die Gebiete der beiden besiegten Reiche zu kolonisieren. Daraufhin wandte sich Silla gegen seinen ehemaligen Verbündeten China und vertrieb ihn von der koreanischen Halbinsel. Das Gebiet von Baekje und große Teile des ehemaligen Goguryeo wurden nun Teil des Reiches Silla. In diesem regierte ein König über eine Ständegesellschaft, die sich nach "Knochen-Klassen" gliederte. Die Abstammung bestimmte die Klassenzugehörigkeit. Seit der Mitte des Jahrhunderts drängten die Könige den Einfluss der oberen Klassen zu ihren Gunsten zurück.

In Japan, in dem das 7. Jahrhundert der Asuka-Zeit zugeordnet wird, erfolgten durch den Kaiserhof weitreichende Reformen. Die sogenannte 17-Artikel-Verfassung war eine Schrift zur ethischen Ausübung der Herrschaft, die sowohl von der Staatsreligion, dem Buddhismus, als auch von konfuzianischen Einflüssen geprägt war. Durch die Taika-Reformen des Jahres 646 wurde der japanische Zentralstaat etabliert, der sich stark an dem chinesischen Staatsmodell orientierte. Das Land wurde formal Eigentum des Kaisers, der es jedoch der Kontrolle der Adelsfamilien überließ, die es bisher besaßen. Im 7. Jahrhundert wurde eine Hofhierarchie etabliert. Ferner bezeichneten sich die japanischen Herrscher erstmals als Kaiser, ab den 670er Jahren mit dem Titel Tennō.

Kreuzten die Handelsrouten zwischen Indien und China vor dem 7. Jahrhundert die malaiische Halbinsel über den Landweg, benutzten die Kaufleute ab dem 7. Jahrhundert durchgehend den Seeweg, indem sie durch die Straße von Malakka um die malaiische Halbinsel herumfuhren. Das auf der südostasiatischen Insel Sumatra gegründete Königreich Srivijaya dehnte sich durch Eroberungen bis zum Ende des Jahrhunderts auf große Teile des Südens der Insel aus. Damit schaffte es die Basis dafür, die Verlagerung des Handelsverkehrs in die Straße von Malakka zu nutzen und in den folgenden Jahrhunderten zur bedeutendsten Thalassokratie Südostasiens aufzusteigen.

Die verstärkte aktive Aneignung indischen Wissens, Kultur und Religion durch die Herrschereliten Südostasiens kann für die Zeit ab dem 7. Jahrhundert belegt werden. So war das Königreich von Srivijaya buddhistisch geprägt.

In Mittelamerika stand das Reich der Maya in voller Blüte. An der Westküste Südamerikas wurde Tiahuanaco zur Zentralstadt einer Pre-Inka-Kultur ausgebaut. Nördlich von dieser etablierte sich die Wari-Kultur. Die nördlich der Wari-Kultur beheimatete Moche-Kultur erlebte ihren Niedergang, wahrscheinlich aufgrund von Klimaeinflüssen.









</doc>
<doc id="8344" url="https://de.wikipedia.org/wiki?curid=8344" title="6. Jahrhundert">
6. Jahrhundert

Das 6. Jahrhundert begann am 1. Januar 501 und endete am 31. Dezember 600.
Das sechste Jahrhundert gilt als das letzte Jahrhundert der "Spätantike" und zugleich als das erste des "Frühmittelalters". In die Mitte des 6. Jahrhunderts fällt das Ende der Völkerwanderungs­zeit.

Das weströmische Reich ist bereits vor Jahrzehnten zerfallen: Seit 476/80 gibt es für den Westen des "Imperium Romanum" keinen eigenen Kaiser mehr, und in Mittel- und Südwesteuropa beginnen sich auf den Trümmern des kollabierten Imperiums "poströmische Reiche" unter zumeist germanischen Herrschern zu bilden: Das "Ostgotenreich" Theoderichs in Italien, Westgoten und Sueben auf der Iberischen Halbinsel, das "Reich der Franken" unter den Merowingern, Burgunden, Thüringer, Gepiden in Ungarn und Siebenbürgen, Vandalen in Nordafrika, auf Sardinien und Korsika. 

Die formale und teils auch faktische Oberhoheit der oströmischen Kaiser ist aber im 6. Jahrhundert auch im Westen noch weitgehend unangefochten. Kaiser Justinian lässt seine Generäle seit 533 Nordafrika, Italien mit Sardinien und den Süden Spaniens erobern; er herrscht 565 nach der "restauratio imperii" nochmals über ein Reich, das sich von Gibraltar über Italien und den Balkan bis nach Ägypten, Syrien und Georgien erstreckt – ein letzter Höhepunkt der spätrömischen Geschichte und zugleich ein mögliches Datum für das „Ende der Antike“. Die Kaiser nach Justinian sprechen kein Latein mehr, und Ostrom wandelt sich zusehends in das griechische Byzanz des Mittelalters.

Seit 541 wütet die Justinianische Pest im Mittelmeerraum und fordert in mehreren Wellen über Jahrzehnte hinweg zahlreiche Opfer, bevor die Krankheit im 8. Jahrhundert aus Europa verschwindet, um erst 1347 erneut auszubrechen.

Ab 568 fallen die "Langobarden" in Italien ein; dies gilt traditionell als der Endpunkt der spätantiken Völkerwanderung. Die "Bajuwaren" werden um 550 erstmals erwähnt und besiedeln das nördliche Alpenvorland. In die durch die Völkerwanderung entvölkerten ehemaligen ostgermanischen Gebiete wandern "slawische Völker" (z. B. Wenden) ein. Etwa um diese Zeit sind die Slawen auch auf dem Balkan präsent, die ab den 580er Jahren zu einer dauerhaften Ansiedlung südlich der Donau übergingen, wenngleich die Landnahme der Slawen auf dem Balkan durch die Balkanfeldzüge des Maurikios (ab 592) verzögert wird. Zusammen mit den Langobarden dringen auch die ursprünglich zentralasiatischen "Awaren" in den Raum Ungarn/Östliches Österreich ein. Das "Reich der fränkischen Merowinger" erreicht um 560 seine vorerst größte Ausdehnung und gerät nach 562 aufgrund innerer Wirren in eine Schwächephase. Die arianischen Westgotenkönige treten 589 zum katholischen Christentum über.

In England bedrängen die "Angelsachsen" zunehmend die einheimischen romano-keltisch-christlichen Regenten. Der Abwehrkampf lässt wohl die Sage von König Artus entstehen. Die keltische Kirche Englands geht gegen Ende des Jahrhunderts in der römisch-katholischen Kirche auf. In Skandinavien erstarken Kleinkönigtümer, die den Ostseehandel betreiben. In Schweden beginnt die Vendelzeit.

Von Syrien bis Pakistan und an die Grenzen der Gobi erstreckt sich das "neupersische Reich" der Sassaniden, unter Chosrau I. auf dem Höhepunkt seiner Macht. Mit einem persischen Angriff beginnt 540 nach dem Bruch des Ewigen Friedens von 532 ein neuer Krieg zwischen Ostrom und den Sassaniden – die "römisch-persischen Kriege" – der (mit zwei kurzen Unterbrechungen) bis 630 dauert und die Islamische Expansion ermöglicht. 

Um 560 vernichten die Perser das Reich der hunnischen Hephthaliten durch ein Bündnis mit den Türken, die sich aber wenig später gegen ihre vormaligen Verbündeten wenden und 572 im Bündnis mit den Oströmern die Sassaniden angreifen. Diese können sich aber im Zweifrontenkrieg behaupten.

Um 570 erfolgt die Geburt des Propheten Mohammed, der dann, muslimischer Überlieferung zufolge, um 610 den Erzengel Gabriel empfangen und damit zum Begründer einer neuen Weltreligion werden sollte.

Das "Kaiserreich China" ist seit 420 in die "Südliche und Nördliche Dynastien" gespalten, erst die Sui-Dynastie eint 581 das Reich für knapp 40 Jahre wieder, mit wirtschaftlichem Aufschwung und kultureller Neuorganisation. Über die Seidenstraße beginnt sich der Buddhismus in China zu verbreiten, dehnt sich auch nach Korea und auch Japan aus. Hier endet die antike "Kofun-Zeit" und beginnt die "Asuka-Zeit" 582 mit der Übernahme des Buddhismus als Staatsreligion, oder 590 mit einer buddhistischen Kaiserdynastie. Damit einher geht die Verbreitung der chinesischen Schrift in Japan.




</doc>
<doc id="8345" url="https://de.wikipedia.org/wiki?curid=8345" title="5. Jahrhundert">
5. Jahrhundert

Das 5. Jahrhundert begann am 1. Januar 401 und endete am 31. Dezember 500.








</doc>
<doc id="8346" url="https://de.wikipedia.org/wiki?curid=8346" title="4. Jahrhundert">
4. Jahrhundert

Das 4. Jahrhundert begann am 1. Januar 301 und endete am 31. Dezember 400.

Es gehört zur Epoche der Spätantike.







</doc>
<doc id="8347" url="https://de.wikipedia.org/wiki?curid=8347" title="3. Jahrhundert">
3. Jahrhundert

Das 3. Jahrhundert begann am 1. Januar 201 und endete am 31. Dezember 300.
Es markiert für das Römische Reich den Übergang vom Prinzipat zur Spätantike.







</doc>
<doc id="8348" url="https://de.wikipedia.org/wiki?curid=8348" title="Prosa">
Prosa

Prosa (lat. ' „gerade heraus“, „schlichte Rede“) bezeichnet die ungebundene Sprache im Gegensatz zur Formulierung in Versen, Reimen oder in bewusst rhythmischer Sprache. Ein Schriftsteller, der ausschließlich oder überwiegend Prosa verfasst, wird auch als Prosaist bezeichnet.
Als prosaisch bezeichnet man davon abgeleitet eine vergleichsweise trockene, nüchterne Darstellung.

Ursprünglich wurde der Begriff der Prosa als Bezeichnung für wissenschaftliche, schriftlich fixierte Texte (beispielsweise in der Geschichtsschreibung, Philosophie oder in den Naturwissenschaften) verwendet, sowie für Notizen in Schriftform im Gegensatz zur Dichtung, die bis ins 18. Jahrhundert überwiegend in Versform verfasst und für den mündlichen Vortrag bestimmt war.
Dieser Unterschied bestimmte das neuzeitliche Literaturverständnis bis in das 18. Jahrhundert, das ein Wertungssystem beinhaltete, welches die Versrede grundsätzlich bevorzugte und die Prosa in diesem System als Mangel „an formativer Kraft“ verstand.

Später wurde der Prosabegriff allgemein für jede Textsorte benutzt, sowohl für die ungezwungene Alltagsrede als auch für den kunstvoll gestalteten fiktionalen Text. 
Seit dem 18. Jahrhundert wird der Begriff ebenso synonym verwendet für die erzählende Literatur bzw. Epik, vor allem für den Roman als Hauptmedium dieser literarischen Gattung.
Als derartiger gattungstheoretischer Begriff bezeichnet die Prosa jene unterschiedlichen Gattungselemente der Literatur, die Beobachtetes, Empfundenes, Erdachtes und Gedachtes mitteilen und mehr oder weniger interpretieren: in einen ausgesprochenen oder unausgesprochenen Sinnzusammenhang stellen, erklären, kommentieren, analysieren oder bewerten und die (im Unterschied zu Versdrama und Lyrik) in ihrer Darstellungsform nicht versförmig sind.

Prosagenres sind z. B. Romane, Novellen, Erzählungen, Kurzgeschichten, Essays, Feuilletons, Memoirenliteratur, Biografien, Briefe, Sachtexte aller Art und die gesamte wissenschaftliche Literatur.

Obwohl die Prosa nicht zwingend durch feste Regeln der Textkomposition bestimmt ist, kann sie sich jedoch durchaus strukturell verdichten. So können Prosatexte beispielsweise durch rhetorische Figuren oder rhythmische Satzschlüsse oder auch durch eine „der Gedankenentwicklung entsprechende, logische Eurythmie“ (W. v. Humboldt) oder eine „geistige Syntax“ (J. Grimm) in ihrer Gestaltungsoffenheit in vielfältiger Weise intensiviert werden. Ebenso können sich die Merkmale der Prosa mit ihrer jeweiligen Quantität verändern. Länge stellt dementsprechend eine Voraussetzung für epische Entfaltung, Kürze für aphoristische Prägnanz dar.

Der (literatur-)geschichtliche Erfolg der Prosa vor allem seit dem 20. Jahrhundert ist zugleich verknüpft mit dem „Zerfall verbindlicher Weltbilder im Prozess der Moderne“, in der die Prosa „zur Darstellungsform für eine Welt“ wird, „in der sich nichts mehr reimt‘“.

Die "Gebrauchsprosa" teilt ihre Inhalte mit z. B. in Rede, Gespräch, Brief, Artikel und Sachtext (z. B. Gesetzestexte oder Gebrauchsanleitungen). Davon unterscheidet man die "literarische Prosa", die sich in Wortwahl, Satzbau, Sprachmelodie, Bildhaftigkeit und Sprachrhythmus bewusst poetischer Gestaltungsmittel bedient. Die wissenschaftliche Prosa (z. B. in der Philosophie) überschneidet sich bisweilen mit der literarischen Prosa.

In der Literaturwissenschaft ist umstritten, ob sich der Gegensatz "Prosa-Versdichtung" eignet, um ein Gattungssystem zu begründen, das dem Konzept der „Naturformen“ Lyrik, Epik und Dramatik gleichwertig ist.

In Gegensatz zum Begriff "prosaisch", der auf Nüchternheit im Ausdruck verweist, zeichnen sich „politische Prosa“ und „wissenschaftliche Prosa“ (ähnlich „Antragsprosa“) durch den ungewöhnlich blumigen Bezug auf aktuelle Schlagworte der politischen und/oder wissenschaftlichen Tagesdiskussion aus.



</doc>
<doc id="8349" url="https://de.wikipedia.org/wiki?curid=8349" title="2. Jahrhundert">
2. Jahrhundert

Das 2. Jahrhundert begann am 1. Januar 101 und endete am 31. Dezember 200.

Im Mittelmeerraum fällt es in die Epoche der Antike.






"Einige der aufgelisteten Persönlichkeiten wurden schon gegen Ende des vergangenen Jahrhunderts geboren und werden dennoch hier aufgeführt; andere wiederum wurden zwar in diesem Jahrhundert geboren, werden aber erst im nächsten Jahrhundert aufgeführt. Dies rührt daher, dass es als Kriterium für die Aufnahme nicht entscheidend war, ob das Geburtsjahr in dieses Jahrhundert fällt, sondern ob das hauptsächliche Werk und Wirken der Person in diesem Jahrhundert stattfand. Freilich ist eine klare Abgrenzung dieser Art nicht immer möglich."




</doc>
<doc id="8350" url="https://de.wikipedia.org/wiki?curid=8350" title="Rhein-Neckar-Kreis">
Rhein-Neckar-Kreis

Der Rhein-Neckar-Kreis ist der einwohnerstärkste Landkreis in Baden-Württemberg und der nach Einwohnerzahl fünftgrößte in Deutschland. Er liegt im Nordwesten des Landes im Regierungsbezirk Karlsruhe und gehört zur Metropolregion Rhein-Neckar.

Sitz des Landkreises ist Heidelberg, das selbst ein Stadtkreis ist und dem Landkreis nicht angehört. Größte Stadt im Landkreis ist die Große Kreisstadt Weinheim.

Der Rhein-Neckar-Kreis hat Anteil an der Oberrheinischen Tiefebene, am westlichen Kraichgau und am südlichen Odenwald. Die Landschaft am westlichen Rand des Odenwalds von Wiesloch nordwärts über Heidelberg und Weinheim bis Darmstadt bezeichnet man als Bergstraße, eine der wärmsten Gegenden Deutschlands. Die nordwestliche Hälfte des Kreises stellt das historische Gebiet der Kurpfalz dar. Im Osten liegt das Tourismusgebiet Brunnenregion. Der Landkreis grenzt direkt an Hessen und Rheinland-Pfalz (Brühl (Baden) – Altlußheim/Altrip – Speyer) und besitzt zudem eine Exklave, die andererseits nur zwei Kilometer von Bayern (Badisch-Schöllenbach/Kirchzell-Breitenbach) entfernt ist.

Der Rhein-Neckar-Kreis grenzt im Uhrzeigersinn im Norden beginnend an die Landkreise Bergstraße, Odenwaldkreis (beide in Hessen), Neckar-Odenwald-Kreis, Heilbronn und Karlsruhe (alle in Baden-Württemberg). Im Westen bildet der Rhein die Landesgrenze zum Bundesland Rheinland-Pfalz mit der kreisfreien Stadt Speyer und dem Rhein-Pfalz-Kreis, mit Ausnahme der linksrheinischen Kollerinsel, die zur Gemeinde Brühl und damit zum Rhein-Neckar-Kreis gehört. Weiterhin grenzt der Rhein-Neckar-Kreis an die Stadtkreise Mannheim und Heidelberg.

In das Kreisgebiet ragt von Norden bis zum Neckar ein schmaler Streifen, der zu Hessen gehört. Es handelt sich hierbei um die zum Kreis Bergstraße gehörigen Gemeinden Neckarsteinach und Hirschhorn (Neckar) sowie das gemeindefreie Gebiet Michelbuch.

Nach Daten des Statistischen Landesamtes, Stand 2015.

Der Rhein-Neckar-Kreis entstand durch die Kreisreform in Baden-Württemberg am 1. Januar 1973. Damals wurden die Altkreise Heidelberg und Mannheim mit der nördlichen Hälfte des Altkreises Sinsheim und der Gemeinde Lindach des Altkreises Mosbach zum Rhein-Neckar-Kreis vereinigt. Am 1. Januar 1975 erfolgte die Ausgliederung von Ziegelhausen in den Stadtkreis Heidelberg. Am 1. April 1976 wurde der Kümmelbacher Hof aus der Stadt Heidelberg aus- und in die Stadt Neckargemünd eingegliedert. Nach einer kleinen Grenzkorrektur zum Neckar-Odenwald-Kreis, die am 1. Januar 1977 wirksam wurde, erhielt der Kreis seine derzeitige Gestalt.
Historisch spielte der Rhein-Neckar-Kreis schon vor der Kreisreform in der Revolution 1848/49 eine wichtige Rolle. Von hier gingen entscheidende Impulse für den Kampf um Freiheit und Demokratie aus.

Die Altkreise Heidelberg, Mannheim und Sinsheim gehen zurück auf die ehemals badischen Bezirksämter, die im Laufe der Geschichte mehrmals verändert wurden und 1936/1939 in Landkreise überführt wurden. Gleichzeitig entstanden 1939 die Stadtkreise Heidelberg und Mannheim. Seither gehören beide Städte nicht mehr zu den jeweiligen Kreisgebieten, blieben jedoch bis 1972 Sitz der jeweiligen Kreisverwaltung.

Nach der Kreisreform wurde Heidelberg Sitz des neuen Rhein-Neckar-Kreises. Dieser umfasst nach Abschluss der Gemeindereform noch 54 Gemeinden, darunter 17 Städte und hiervon wiederum sechs „Große Kreisstädte“ (Hockenheim, Leimen, Schwetzingen, Sinsheim, Weinheim und Wiesloch). Größte Stadt ist Weinheim, kleinste Gemeinde ist Heddesbach. Mit seinen insgesamt 54 Städten und Gemeinden liegt der Rhein-Neckar-Kreis nur knapp hinter dem Alb-Donau-Kreis, der 55 Städte und Gemeinden hat. In Weinheim, Ladenburg, Neckargemünd, Sinsheim und Wiesloch befinden sich, zusätzlich zu den zahlreichen Dienststellen in Heidelberg, Außenstellen des Landratsamts.

Die Einwohnerzahlen sind Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen des Statistischen Landesamts Baden-Württemberg. Es wurden nur Hauptwohnsitze berücksichtigt.

Der Landkreis wird vom Kreistag und vom Landrat verwaltet.

Der Kreistag wird von den Wahlberechtigten im Landkreis auf fünf Jahre gewählt. Die Kommunalwahl am 25. Mai 2014 führte zu folgendem vorläufigen Ergebnis. Das amtliche Endergebnis wird vom Statistischen Landesamt gegen Ende des Jahres bekannt gegeben.


Der Kreistag wählt den Landrat für eine Amtszeit von acht Jahren. Er ist gesetzlicher Vertreter und Repräsentant des Landkreises sowie Vorsitzender des Kreistags und seiner Ausschüsse. Er leitet das Landratsamt und ist Beamter des Kreises. Zu seinem Aufgabengebiet zählen die Vorbereitung der Sitzungen des Kreistags sowie seiner Ausschüsse. Er beruft Sitzungen ein, leitet sie und vollzieht die dort gefassten Beschlüsse. In den Gremien hat er kein Stimmrecht. Sein Stellvertreter ist der Erste Landesbeamte.

Die Landräte des Rhein-Neckar-Kreises seit 1973:

Beschreibung: "Gespalten: vorn in Blau ein silberner, mit einem gewellten schwarzen Faden belegter Wellenbalken; hinten in Gold ein rot gekrönter, rot bezungter und rot bewehrter schwarzer Löwe"

Bedeutung: Das Wappen trägt der geografischen Lage und der historischen Herrschaftsverhältnissen des Kreisgebiets und den ehemals selbständigen Gemeinden Rechnung. Der geteilte Wellenbalken symbolisiert den Rhein und den Neckar, die dem Kreis den Namen gaben. Der kurpfälzische Löwe symbolisiert die ehemalige Zugehörigkeit des überwiegenden Teils des heutigen Kreisgebiets zum Kurfürstentum Pfalz mit seiner Residenzstadt Heidelberg und später ab 1720 Mannheim. Die Farben des Löwen wurden dabei gegenüber dem Originalwappen vertauscht, um der heraldischen Farbregel gerecht zu werden.

Der Rhein-Neckar-Kreis unterhält eine Partnerschaft mit dem ungarischen Komitat Somogy.

Die Industriestruktur ist vielfältig mit zahlreichen Betrieben der metallverarbeitenden und chemischen Industrie (z. B. Freudenberg in Weinheim), darunter auch zahlreiche mittelständische Betriebe. Der Dienstleistungssektor ist zwar in den nicht zum Rhein-Neckar-Kreis gehörenden Großstädten konzentriert, doch gibt es auch im Kreisgebiet bedeutende Unternehmen wie z. B. das Software-Unternehmen SAP in Walldorf und St. Leon-Rot oder den Finanzdienstleister MLP und die Heidelberger Druckmaschinen in Wiesloch. In den ländlichen Teilen des Kreises sind auch Land- und Forstwirtschaft stark vertreten. Im Rheingraben (Sankt Leon-Rot, Schwetzingen) befindet sich eines der Hauptanbaugebiete für Spargel in Deutschland.

Im Zukunftsatlas 2016 belegte der Rhein-Neckar-Kreis Platz 31 von 402 Landkreisen, Kommunalverbänden und kreisfreien Städten in Deutschland und zählt damit zu den Regionen mit „sehr hohen Zukunftschancen“.

Durch das Kreisgebiet führt die Bundesautobahn 5 Basel–Frankfurt, Bundesautobahn 61 Venlo–Hockenheim sowie die Bundesautobahn 6 Saarbrücken–Mannheim–Nürnberg, die sich am Autobahnkreuz Walldorf kreuzen, mehrere Bundesstraßen, darunter die B 3, Basel–Frankfurt, die B 39, die B 37 und die B 45 sowie Landesstraßen.

Im Kreisgebiet ging am 14. Dezember 2003 das Netz der S-Bahn RheinNeckar in Betrieb, das den gesamten Rhein-Neckar-Raum bis in die Pfalz und nach Südhessen erschließt. Die Anknüpfung an das Netz der Stadtbahn Karlsruhe soll noch ausgebaut werden. Die Oberrheinische Eisenbahn (OEG), eine Schmalspurbahn, verbindet auf einer Rundstrecke Mannheim, Heidelberg und Weinheim.
Straßenbahnen der RNV fahren außerdem von Heidelberg aus nach Leimen und Eppelheim. BRN, RNV und SWEG versorgen das Heidelberger Umland mit einem relativ dichten Busliniennetz.

Der Rhein-Neckar-Kreis ist Schulträger folgender Beruflichen Schulen: Theodor-Frey-Schule (Gewerbliche und Kaufmännische Schule) Eberbach, Ehrhart-Schott-Schule (Gewerbliche Schule) Schwetzingen, Friedrich-Hecker-Schule (Gewerbliche Schule) Sinsheim, Hans-Freudenberg-Schule (Gewerbliche Schule) Weinheim, Hubert-Sternberg-Schule (Gewerbliche Schule) Wiesloch, Carl-Theodor-Schule (Kaufmännische Schule) Schwetzingen, Max-Weber-Schule (Kaufmännische Schule) Sinsheim, Johann-Philipp-Reis-Schule (Kaufmännische Schule) Weinheim, Johann-Philipp-Bronner-Schule (Kaufmännische Schule) Wiesloch, Louise-Otto-Peters-Schule (Hauswirtschaftliche Schule) Hockenheim mit Außenstelle Wiesloch, Albert-Schweitzer-Schule (Hauswirtschaftliche Schule) Sinsheim und Helen-Keller-Schule (Hauswirtschaftliche Schule) Weinheim, ferner folgender Sonderschulen: Comeniusschule für Geistigbehinderte Schwetzingen, Steinsbergschule für Geistigbehinderte mit Schulkindergarten Sinsheim, Maria-Montessori-Schule für Geistigbehinderte mit Schulkindergarten Weinheim und Martinsschule für Körperbehinderte Ladenburg.

Der Rhein-Neckar-Kreis unterhält Krankenhäuser, Alten- und Pflegeheim und Geriatrische Rehabilitationskliniken. Diese wurden bislang in der Form eines Eigenbetriebs geführt. Seit 1. Januar 2006 werden diese Einrichtungen von der „GRN Gesundheitszentren Rhein-Neckar gemeinnützige GmbH“ betrieben. Im Einzelnen handelt es sich um die Kreiskrankenhäuser Eberbach, Schwetzingen, Sinsheim und Weinheim, die Kreispflegeheime Schriesheim, Schwetzingen, Nußloch und Weinheim sowie die Geriatrischen Reha-Kliniken Hockenheim und Weinheim. Ferner gibt es an den Kreiskrankenhäusern Schwetzingen jeweils eine Krankenpflegeschule.

Am 1. Januar 1973 wurde dem Landkreis das seit dem 1. Juli 1956 für den Landkreis Heidelberg gültige Unterscheidungszeichen "HD" zugewiesen. Es wird durchgängig bis heute ausgegeben.

Bis in die 1990er Jahre erhielten Fahrzeuge aus den Teilkreisen besondere Erkennungsnummern:




</doc>
<doc id="8351" url="https://de.wikipedia.org/wiki?curid=8351" title="Orhan Pamuk">
Orhan Pamuk

Orhan Pamuk (* 7. Juni 1952 in Istanbul, Türkei) ist ein türkischer Schriftsteller.

Er gilt als einer der wichtigsten Schriftsteller seines Landes und ist Träger des Literatur-Nobelpreises 2006. In seiner Erzählkunst vermittelt er zwischen dem modernen europäischen Roman und der mystischen Tradition des Orients. Sein Werk ist mittlerweile in 35 Sprachen übersetzt und in über 100 Ländern veröffentlicht worden.

Auch sein im Wesentlichen menschenrechtlich begründetes politisches Engagement, mit dem er einerseits die türkische Regierung unter anderem zu historischer Aufklärung und Verantwortungsbereitschaft anhält und andererseits politisch und religiös begründeten Widerständen gegen einen EU-Beitritt der Türkei entgegentritt, zeigt ihn in einer beide Seiten fordernden Mittlerposition.

Orhan Pamuk ist in einem von der Großfamilie bewohnten fünfstöckigen Haus im Viertel Nişantaşı im Istanbuler Stadtteil Şişli nördlich des Bosporus aufgewachsen. In "Cevdet Bey ve Oğulları" (dt.: "Cevdet und seine Söhne") und dem "Kara Kitap" (dt.: "Das Schwarze Buch") schildert er den Istanbuler Alltag und Familienverhältnisse, die den seinen ähneln. Sein Großvater war als Ingenieur und Industrieller beim Eisenbahnbau zu Reichtum gekommen. Die ganze Familie war auf die prowestliche Linie Atatürks eingeschworen, auch hinsichtlich der kulturellen Interessen:

Anders als es der Familientradition entsprach, lagen Pamuks Interessen nicht beim Ingenieurberuf. Er begann als Siebenjähriger zu malen und wollte von da an Maler werden. Nach der Grundschule besuchte er das englischsprachige "Robert College" und anschließend – wie schon der Großvater und der Vater – die Istanbuler Technische Universität, wo er ein Architekturstudium aufnahm. Nach einem Fachwechsel erwarb er 1977 einen universitären Abschluss als Journalist. Unterdessen hatte er mit 23 Jahren die Entscheidung getroffen, sich nur noch dem Schriftstellerdasein zu widmen. In politisch instabiler Lage mit einer gewalttätig aufgeheizten Links-Rechts-Polarisierung und verkappter Militärherrschaft (siehe: Geschichte der Republik Türkei) fielen für ihn die Würfel:

Vor seiner Erstpublikation "Cevdet Bey ve Oğulları" (1982 erschienen) verbrachte Pamuk acht Jahre in Schreibklausur ohne eigenen Verdienst bei seiner Mutter im Sommerhaus auf einer der Prinzeninseln im Marmarameer, deren Stille seinem Arbeitsprozess besonders förderlich war und ist. Bis auf einen dreijährigen USA-Aufenthalt 1985–1988 an der Columbia University in New York, wo seine Frau promovierte und er selber "Kara Kitap" erarbeitete, hat Pamuk Istanbul nicht für längere Zeit verlassen: „"Istanbuls Schicksal ist mein Schicksal. Ich fühle mich dieser Stadt verbunden, weil sie mich zu dem gemacht hat, der ich bin".“

Lesereisen, Preisverleihungen und Vor-Ort-Recherchen für seine Romane sind es hauptsächlich, die Pamuk gelegentlich aus seiner Heimatstadt hinausführen. Auf diese Weise hat er z. B. Anschauungsmaterial für seinen Roman "Kar" (dt. Titel: "Schnee") gewonnen, wie er in seiner Dankesrede für den Friedenspreis des Deutschen Buchhandels am 23. Oktober 2005 in der Frankfurter Paulskirche verdeutlichte. Ka, die Zentralfigur dieses vielschichtigen politischen Romans, ein türkischer Dichter, der als Emigrant in Frankfurt lebt und den es auf Heimatbesuch in die nahe den östlichen Grenzen der Türkei gelegene Stadt Kars verschlägt, wird jeweils in örtlichem Umfeld inszeniert:

Pamuks Werke reflektieren das Identitätsproblem der seit osmanischen Zeiten zwischen Orient und Okzident hin- und hergerissenen türkischen Gesellschaft. Eine wichtige Rolle spielt dabei die in Vergessenheit geratene mystische Tradition, der sich auch Pamuk erst mit 35 Jahren zuwandte. Seither ist für seine Bücher „ein dichtes und vielfältiges Gewebe aus schwarzem Humor, blitzendem Scharfsinn, sinnlicher, stark visuell geprägter Darstellung, kriminalistischer Kombinationsgabe und romantischen Sehnsüchten im Bewusstsein der schnöden Wirklichkeit“ (Friedenspreis-Laudatio Sartorius) ebenso charakteristisch wie der Bezug zur reichen Erzähltradition der islamischen Sufi-Dichtung.

Pamuk, so heißt es in der Begründung der Friedenspreis-Verleihung, gehe wie kein anderer Dichter unserer Zeit den historischen Spuren des Westens im Osten und des Ostens im Westen nach. Er sei einem Begriff von Kultur verpflichtet, der ganz auf Wissen und Respekt vor dem anderen gründe. Der Dichter bestätigt in seiner Dankesrede:

Pamuks zwischen 1974 und 78 entstandener Erstling "Cevdet und seine Söhne" (türk.: Cevdet Bey ve Oğulları) erzählt in drei Teilen entscheidende Entwicklungsphasen aus der Geschichte der Kaufmanns- und Fabrikantenfamilie Işıkçı vor dem Hintergrund der wechselhaften, durch Reformbewegungen nach europäischen Vorbild geprägten, türkischen Historie im 20. Jahrhundert.

Die Haupthandlung beginnt 1905 mit der Heirat des Firmengründers Cevdet mit Nigân, der Tochter einer angesehenen Paşafamilie, und endet mit deren Tod 1970. Am Beispiel der Titelfigur, seiner Kinder Osman, Refik, dessen Freunden Ömer und Muhittin, sowie Ayşe und des Enkels Ahmet gestaltet der Autor unterschiedliche Konzeptionen, welche in den späteren Werken variierend aufgegriffene zentrale Themen behandeln: die Suche nach dem Sinn des Lebens, das Schwanken zwischen den Schwerpunkten Familie, Geschäft und Selbstfindung sowie Karriere und Moral, die Diskussionen über Tradition und Fortschritt, die künstlerische Verarbeitung der Realität und das politische Engagement. Dabei repräsentieren Osman und sein Sohn Cemil einerseits und Refik sowie dessen Sohn Ahmet andererseits zwei kontrastierende Entwicklungslinien. Durch die breit angelegte personale Vernetzung und die verschiedenen Handlungsorte Istanbul, Ankara und die ländliche Region um Kemah entsteht ein differenziertes Bild des Wandels der türkischen großbürgerlichen Gesellschaft.

Zwischen 1980 und 1983 schrieb Pamuk seinen zweiten, 1983 publizierten Roman "Das Stille Haus" "(Sessiz ev)". Wie in seinem Erstling "Cevdet und seine Söhne" zeichnet er ein differenziertes Bild der türkischen Gesellschaft im 20. Jahrhundert in der Spannung zwischen Tradition und Reformbemühungen am Beispiel einer Familie. Eingeschoben in die siebentägige Handlung in Cennethisar bei Gebze am Marmarameer sind die Erinnerungen von fünf Erzählern aus drei Generationen: Dabei legen sie die Hintergründe der Geschichte des Großvaters, des Arztes und radikalen Aufklärers Selâhattin Darvinoğlu, frei, der mit der traditionell-konservativen Fatma aus dem Istanbuler Großbürgertum verheiratet ist. Dieser geht eine eheähnliche Beziehung zu seinem Dienstmädchen aus der armen Bevölkerungsschicht ein. Der nicht legalisierten Verbindung entstammen zwei, später als Diener und Losverkäufer arbeitende Söhne, der zwergwüchsige Recep und der hinkende İsmail, sowie ein Enkel, Hasan. Diese Nachkommen treffen nun im Juli 1980, in einer Zeit links- und rechtsradikaler Kontroversen und Gewalttaten zwei Monate vor dem Militärputsch, mit den drei legalen Enkeln (Faruk, Dozent für Geschichte, Nilgün, Soziologiestudentin, und Metin, Gymnasiast) aufeinander, was zu einem tragischen Ende der Sommerferienwoche führt.

Der 1990 bei Insel erschienene Roman "Die weiße Festung" ("Beyaz Kale", 1985) berichtet von den Abenteuern eines jungen Venezianers, der bei einem Seegefecht in die Hände der Türken gerät. Als Sklave eines Hodschas, der am osmanischen Hofe eine Rolle spielt, und dem Ich-Erzähler auf verblüffende Weise ähnlich sieht, verstrickt er sich in eine Herr-und-Knecht-Beziehung, in der sich die beiden Kontrahenten immer ähnlicher werden. So klar die Rollen zwischen dem an westlicher Wissenschaft orientierten Venezianer und dem islamisch-konservativen Hodscha anfangs verteilt sind, so sehr verschwimmen die Konturen mit der Zeit. In einem raffinierten Vexierspiel werden Erwartungen der Leser an das typisch Orientalische aufgenommen, in Frage gestellt und schließlich ad absurdum geführt. Die erhoffte klare Trennung zwischen Ost und West erweist sich zunehmend als Illusion.

Literarisch erinnert "Die weiße Festung" an Umberto Ecos "Der Name der Rose". Vor dem Hintergrund einer spannenden Story entfaltet sich die Welt der Köprülü-Großwesire, mischen sich „aufklärerische“ und „konservative“ Ideen der Zeit mit Geschichtsbildern, politischen Strategien und den ideologischen Machtkämpfen zwischen Sultan, Hof und Moschee. „Natürlich, eine alte Handschrift“, stellt Umberto Eco nicht ohne Ironie seinem Text voran. „Dieses Manuskript fiel mir 1982 in die Hände“, knüpft Orhan Pamuk an, und nennt damit das Erscheinungsjahr vieler Eco-Übersetzungen und vielleicht seiner Erstlektüre der „Rose“ als Funddatum, während Ecos Erzähler „sein“ Manuskript 1968, mitten in der für Eco bedeutsamen Studentenrevolte, gefunden haben will. Wie Ecos „Berichterstatter“ äußert der Erzähler Pamuks zunächst Zweifel an der Echtheit des Dokuments, zudem gibt er an, die Originalgeschichte nicht genau abgedruckt, sondern eher nachlässig nacherzählt zu haben.

Die „Weiße Festung“ ist insofern die Geschichte einer literarischen Entführung. Der Transfer des europäischen Romans in die moderne Türkei bereichert dabei beide Seiten. Verblüfft Eco im „Namen der Rose“ durch das verwirrende Arrangement moderner und mittelalterlicher Ansichten, so konfrontiert Pamuk den westlichen Leser mit unerwarteten Seiten osmanischer und europäischer Geschichte.

Im Grunde eine simple Geschichte: Der junge Anwalt Galip wird von seiner schönen jungen Frau und Cousine Rüya verlassen. Es beginnt eine spannende Suche quer durch die Stadtviertel Istanbuls, durch Moscheen und Katakomben, durch Bars und Bordelle. Es mehrt sich der Verdacht, dass Rüya sich bei Celâl versteckt, ihrem Halbbruder, einem erfolgreichen Kolumnisten, dem großen Vorbild Galips. Celâl aber bleibt unauffindbar. Er ist offensichtlich in allerlei Machenschaften verstrickt, unterhält Verbindungen zur Mafia, zu Geheimorganisationen und Sekten.

Galip sucht verzweifelt nach Zeichen, nach Hinweisen in der Kolumne Celâls etwa, die immer wieder Bezug auf das Leben der Familie nimmt, versteckte Anspielungen und chiffrierte Botschaften enthält. Immer tiefer verstrickt sich Galip in die Kunst der Textauslegung, verfolgt Anweisungen mystischer Koraninterpreten, geht auf die Spurensuche in Celâls Texten und findet literarische Vorlagen, Schicksale, die Galip verarbeitet hat, lernt Menschen kennen, die ebenfalls auf der Suche sind. Es ist eine Suche nach Identität in einer Welt, in der sich Ost und West hoffnungslos vermischen und in der niemand „er selbst sein“ kann.

Orhan Pamuks Buch ist ein Dokument der Zerrissenheit, des Schwankens der Menschen zwischen sinnentleerten Traditionen, Aberglauben und westlichen Vorbildern von der großen Literatur bis zum Filmsternchen. Aber auch bei der Suche nach den wahren Quellen stößt er auf immer neue Mischungen. Auf dem Grunde des Bosporus finden diese Spuren zusammen, Kreuzritter und Sultane, Gangster und Gehenkte, alte Münzen und Alltagsgegenstände bilden den Boden, auf dem Istanbul wächst. In den alten Schächten finden sie sich, mystische Texte, vergessene Kleidungsstücke, die Gebeine Ermordeter, ein Kabinett von Wachsfiguren, die die Menschen Istanbuls verkörpern, bevor die Stadt ihre Identität verlor.

Wie in Llosas Roman "Tante Julia und der Kunstschreiber" mischt Pamuk die Erzählung mit Beiträgen des Journalisten, wobei die Geschichten beginnen, ihre Grenzen zu überschreiten. Realität und Kolumne verweisen aufeinander, die Figuren aus Celâls Geschichten tauchen in der Realität Galips auf, werden bedrohlich, interpretieren die Darstellung Celâls, sind ebenfalls auf der Suche nach dem verschollenen Autor. Am Ende fallen die Grenzen zwischen den Identitäten. Immer mehr wird Galip zu Celâl, sitzt in einer von Celâls geheimen Wohnungen und setzt die Kolumnenserie fort.

„Eines Tages las ich ein Buch, und mein ganzes Leben veränderte sich.“ Mit diesem Satz beginnt Orhan Pamuks literarisch vielleicht bedeutendster Roman, dessen Titel "Das neue Leben" "(Yeni Hayat)" auf Dantes gleichnamiges Werk anspielt. Die Geschichte des geheimnisvollen Buches verweist auf die deutsche Romantik, auf Novalis’ Heinrich von Ofterdingen und dessen Suche nach der blauen Blume. Es ist eine Geschichte von Liebe und Tod, von einer geheimnisvollen Reise, vom Spiel mit literarischen und mystischen Quellen aus Ost und West. Das Werk ist ein Klassiker in dem Sinne, dass man es, unbeeinflusst von allen geistesgeschichtlichen Spielereien, als geheimnisvollen Abenteuerroman lesen kann, gleichzeitig aber auch ein perfektes Spielzeug für den gebildeten Leser, der den Anspielungen, versteckten Zitaten und irreführenden Hinweisen nachgehen kann.

In seinem zwischen 1990 und 1998 entstandenen Roman mit dem auf die alte Symbolfarbe (Kap. 31) bezogenen Titel „Rot ist mein Name“ erzählt der Autor die abenteuerliche Lebensgeschichte Karas und Şeküres, die mit dem Buchmalerstreit im Osmanischen Reich des 16. Jhs. und einer sich daraus entwickelnden Kriminalhandlung verwoben ist. Er verlagert damit die aktuelle Thematik anderer Werke, die Spannung zwischen östlicher Tradition und westlichen Einflüssen, in eine historische Zeit mit sagenhaften Wurzeln.

Da die Werbung Karas um seine zwölfjährige Cousine von deren Vater abgelehnt wird, nimmt er eine Stellung als Sekretär des Finanzmeisters in östlichen Provinzen an, erlebt dort die Kriege gegen die Perser und kehrt nach zwölf Jahren als 36-Jähriger 1591 nach Istanbul zurück. Hier soll er den Oheim beim Schreiben eines Buches unterstützen. Er nimmt den Auftrag an, da Şeküre, inzwischen Witwe eines Soldaten, mit ihren zwei Söhnen Şevket und Orhan wieder im Haus ihres Vaters lebt und er hofft, nun seine Jugendliebe heiraten zu können. Der Oheim hat als Gesandter des Sultans Murad III. in Venedig die individuellen Renaissance-Porträts kennengelernt und soll für seinen Herrscher ein illustriertes Buch im neuen „fränkischen“ Stil anfertigen lassen. Da die staatliche Malerwerkstatt des Meisters Osman jedoch dem traditionellen, von den meisten Vertretern des Islams tolerierten Stil verpflichtet ist, arbeiten die besten Illustratoren Velican („Olive“), Hasan Celibi („Schmetterling“) und Musavvir Mustafa („Storch“) zu Hause an diesem geheimen Projekt. Sie erhalten Teilaufträge mit genauen Anweisungen, die ihnen nur einen fragmentarischen Einblick gewähren. Die Künstler geraten durch ihre Arbeiten in einen Gewissenskonflikt zwischen religiöser Anschauung, wonach sie wie die „Franken“ die Verherrlichung des Menschen betreiben und zudem die jahrhundertealte Bildgestaltung verdrängen, und dem Interesse an neuen künstlerischen Möglichkeiten, die sie aber technisch noch nicht beherrschen und nur imitieren. Aus dieser Situation heraus werden der vom Prediger Nusret Hodscha aus Erzurum fanatisierte Ornamentierer Fein und bald darauf der Oheim erschlagen. Mordinstrument der zweiten Tat ist ein 300 Jahre altes mongolisches Tintenfässchen aus Täbris, und die darin enthaltene Farbe Rot vermischt sich symbolträchtig mit dem Blut des Opfers. Dessen das Universum durchstreifende Seele erreicht schließlich einen wundervoll roten Bereich. Auf ihre Frage, ob sie sich nicht zu sehr von den Bildern der Ungläubigen habe berühren lassen, hört sie eine Stimme: „Der Westen wie der Osten, beide sind mein“ (Kap. 37, S. 310).

Die ohne ihren Vater schutzlose und von ihrem Schwager Hasan bedrängte Şeküre heiratet Kara unter der Bedingung, den Tod des Vaters aufzuklären. Dieser vermutet den Mörder unter den Malern der Werkstatt und versucht diesem gemeinsam mit Meister Osman durch Stilanalysen auf die Spur zu kommen, indem sie in der Schatzkammer des Sultans die Vorbilder der Künstler mit einer bei dem toten Fein gefundenen Pferdezeichnung vergleichen. So erreichen sie ihr Ziel und der persische Illustrator Olive muss die Taten gestehen. Im Kampf mit Kara verletzt er diesen schwer und kann entkommen. Doch wird er auf der Flucht vom eifersüchtigen Hasan getötet, der ihn kurioserweise für einen Gefährten seines Rivalen hält. Şeküre pflegt den Verwundeten und entdeckt, da er den Fall gelöst hat und dies fast mit seinem Leben bezahlen musste, ihre Liebe zu ihm. Trotz der Erfüllung seines Jugendtraumes und der Zuwendung seiner Frau sind die 26 Jahre bis zu seinem Tod von Melancholie begleitet, vielleicht eine Reaktion auf das Desinteresse der Nachfolger des Sultans an der Kunst und den Niedergang der Werkstätten: Man malt jetzt weder im östlichen noch im westlichen Stil, sondern überhaupt nicht mehr: „Das Bild wurde aufgegeben“ (Kap. 59, S. 550). Der Roman endet mit dem unerfüllten Wunsch der alt gewordenen Protagonistin, von sich sowohl ein individuelles Jugend-Porträt als auch ein Bildnis der Glückseligkeit – eine Mutter, die ihr Kinder stillt – im Stil der alten, die Zeit anhaltenden Herater Meister zu besitzen.

Der Autor lässt den Roman von Şeküres Sohn Orhan nach den Erzählungen seiner Mutter, die wiederum die persönlichen Mitteilungen der anderen Personen gesammelt hat, gemischt mit eigenen Vorstellungen und Erfindungen, schreiben und eröffnet damit ein breites Spektrum sich überlagernder und, durch die Berichte der ermordeten Opfer, die Grenzen der Realität überschreitender fiktiver Perspektiven. So entsteht eine komplexe polyphone Struktur: Elf Haupt- und Nebenfiguren präsentieren abwechselnd die Handlung, im Allgemeinen in chronologischer Reihenfolge. Durch die jahrhundertealten Märchen- und Sagenstoffe, z. B. von Rüstem aus dem „Königsbuch“ (Schāhnāme) von Firdausi oder vor allem
die leitmotivisch eingesetzte Situation, als sich die schöne Şirin durch ein Bild in König Hüsrev verliebt, ebenso durch die Historien der alten Meistermaler und ihrer kostbaren Werke und die von einem Märchenerzähler (Meddah) wie in einem Rollenspiel zur Sprache gebrachten gemalten Figuren (Hund, Pferd, Frau, Satan, Tod usw.) erweitert sich die Kriminalgeschichte zu einem phantasievollen breiten Gemälde.

Die Fähigkeit zu verstehen und verständlich zu machen liegt auch Pamuks Roman "Kar" zugrunde, wie die nachfolgenden zusammengetragenen Äußerungen des Autors unterstreichen:

Pamuk entwickelt für diesen türkischen Mikrokosmos folgenden Handlungsrahmen: Eine Theatergruppe inszeniert einen kemalistischen Putsch in der durch einen Schneesturm von der Außenwelt abgeschotteten Grenzstadt Kars, in welcher junge Musliminnen begonnen haben, wegen des Kopftuchverbotes an der Universität Suizid zu begehen. Ein bekannter Islamist hält sich in der Stadt versteckt, und der Dichter Ka verstrickt sich auf der Suche nach Inspiration und nach der Liebe der schönen "Ipek" in die Ereignisse.

Seine Erzählhaltung charakterisiert Pamuk wie folgt:

"Schnee" wurde im Jahr 2006 als „Buch für die Stadt“ in Köln ausgewählt.

In diesem Werk beschreibt Pamuk die tiefe Melancholie seiner Bewohner, im Türkischen "hüzün" (deutsch: Trübsinn, Traurigkeit) genannt. Sie sei aus der Istanbuler Alltagskultur nicht wegzudenken.

Dem Literaturnobelpreisträger zufolge ist Hüzün „das Gefühl, mit dem sich im letzten Jahrhundert Istanbul und seine Bewohner auf intensivste Weise infiziert haben“. Es handele sich dabei nicht um Schwermut des Einzelnen, sondern um ein von Millionen Menschen zugleich empfundenes „schwarzes Gefühl“, der Hüzün einer ganzen Stadt.

Dem Besucher der Stadt zeige sich der dunkle Seelenzustand der Menschen an der „mausgrauen Kleidung“ der Menschenmassen auf der Galatabrücke. Große Moscheen und geschichtsträchtige Bauten osmanischer Glanzzeiten stünden in scharfem Kontrast zum allgemeinen Niedergang der Stadt. Der „heruntergekommene Zustand, in dem sich alles und jedes befindet“, lösten jenen Hüzün aus, der die Stadt in Lethargie versinken ließe. Im wochenlangen Nieselregen der dunklen Jahreszeit sei das Schwarz-Weiß-Gefühl mit Händen zu greifen.

Nur eine Annäherung an Europa könne dieser Starre entgegenwirken.

In diesem Roman erzählt Pamuk eine Liebesgeschichte, die zwischen 1975 und 1985 in Istanbul spielt. Der reiche Fabrikantensohn Kemal und die gut ausgebildete Sibel planen zu heiraten. Eines Tages trifft Kemal zufällig eine Familienverwandte, Füsun, eine junge Frau, die aus einer niedrigeren Schicht stammt. Sie verlieben sich, aber können nicht zusammenleben. Kemals Liebe für Füsun wächst mit der Zeit und er fängt an Füsuns persönliche Gegenstände zu sammeln, mit denen er am Ende ihrer traurigen Liebesgeschichte ein Museum öffnet.

Das Museum der Unschuld ist der Titel des Buches und gleichzeitig der Name des Museums. Pamuk beschreibt das Museum als „eine bescheidene Sammlung des täglichen Lebens in Istanbul.“

"Ich bin ein Baum" (türkisch: "Ben Bir Ağacım"), erschienen im August 2013, enthält ein Kapitel aus dem noch nicht publizierten Roman "Kafamda bir tuhaflık'ın" (etwa: „Eine Skurriliät in meinem Kopf“), in dem der Protagonist Mevlut Karataş aus seiner Schulzeit erzählt, sowie eine Auswahl von Geschichten aus den Romanen "Das schwarze Buch", "Mein Name ist Rot" (Kp. 10 "Ich bin ein Baum": erzählt wird die Geschichte eines zur Illustration eines Buches vorgesehenen Baumes), "Schnee" und "Istanbul". Diese Zusammenstellung kann als Einführung in Pamuks Werk angesehen werden, denn die ausgewählten Texte sind in einer leicht verständlichen Sprache geschrieben, damit sie möglichst viele (insbesondere jüngere) Menschen, lesen und verstehen können.

"Kar" soll nach Pamuks eigenem Bekunden sein einziger politischer Roman bleiben, gerade weil er sich auf diesem Feld bereits im Übermaß gefordert sieht:

Um die Zensurmöglichkeiten zu unterlaufen, bevorzugt er Live-Auftritte im Fernsehen für seine öffentlichen Stellungnahmen. Die Fatwa gegen Salman Rushdie verurteilte er als erster Autor in der muslimischen Welt und setzte sich für seinen bedeutenden türkisch-kurdischen Schriftstellerkollegen Yaşar Kemal ein, als dieser 1995 in der Türkei unter Anklage gestellt wurde. Pamuk hat die bis vor kurzem von Missachtung und Unterdrückung gekennzeichnete Kurdenpolitik der türkischen Regierung harsch kritisiert und den ihm gleichwohl angetragenen höchsten türkischen Kulturpreis zurückgewiesen.

Die im sogenannten „Krieg gegen den Terrorismus“ an der Seite der USA von der türkischen Führung eingenommene Haltung hat er als „Anbiederung“ disqualifiziert. Seiner Auffassung nach genügt es nicht, den Terror nur zu verdammen und rücksichtslos zu bekämpfen, ohne einer verschämten, aber verbreiteten Kritik an der weltpolitischen Rolle der USA Rechnung zu tragen. Man müsse der diesbezüglichen Empörung durchaus nicht recht geben, wohl aber versuchen, sie zu verstehen.

Wiederholt hat Pamuk seiner Überzeugung Ausdruck verliehen, dass der EU-Beitritt der Türkei ein wichtiges politisches Ziel darstellt, so auch am 23. Oktober 2005 in seiner Frankfurter Dankesrede für den Friedenspreis:

Mit Blick auf die jüngsten Entwicklungen meint Pamuk, der wirtschaftliche Aufschwung in der Türkei habe das Interesse an einem EU-Beitritt abgekühlt. Viele Türken fühlten sich in dem seit Jahren stockenden Beitrittsprozess hingehalten: „Europa hat in der Türkei viele Herzen gebrochen.“

Die Proteste um den Gezi-Park in Istanbul, den er als „Istanbuls letzte Kastanie“ bezeichnet, unterstützte Pamuk in einem am 6. Juni 2013 in der Süddeutschen Zeitung veröffentlichten Artikel, in dem er sich auch an eine Aktion seiner Familie, 1957 in Nişantaşı, zur Erhaltung einer Kastanie erinnert.

Mit seinem politischen Roman "Kar" löste Pamuk nicht nur ein weltweites positives Echo auf sein schriftstellerisches Schaffen aus, sondern bot den Kritikern des angestrebten türkischen EU-Beitrittes ungewollt Argumente: Eine in sich so zerklüftete und spannungsgeladene Gesellschaft sei in die EU schwer integrierbar. Pamuk verwahrte sich dagegen, den phantasievoll verdichteten Realismus seines Romanes gegen die EU-Beitrittsperspektive auszuspielen und verwies darauf, dass sich die Türkei gegenüber dem von ihm beschriebenen gesellschaftlichen Szenario bereits erheblich gewandelt habe:

In einem Interview mit dem Magazin des Zürcher Tages-Anzeigers vom 5. Februar 2005 hatte Pamuk zu den Polarisierungstendenzen in der türkischen Gesellschaft Stellung bezogen und auch direkt darauf hingewiesen, dass es in der Türkei ein massenhaftes Töten von Armeniern gegeben hat, den Völkermord an den Armeniern, der in der Republik Türkei bis heute von staatlicher Seite abgestritten wird, wobei Pamuk das Wort „Völkermord“ vermied:

Daraufhin betrieben türkische Nationalisten eine Kampagne gegen ihn, in deren Rahmen z. B. Demonstrationen gegen ihn organisiert wurden. Er wurde in der Presse beschimpft und erhielt Morddrohungen. Im Kreis Sütçüler in der Provinz Isparta ordnete ein Landrat an, dass seine Bücher ausgesondert und vernichtet werden sollten. Diese Anordnung konnte jedoch nicht umgesetzt werden, da keine Bücher von Orhan Pamuk auffindbar waren. Sie wurde später von vorgesetzter Stelle aufgehoben und der verantwortliche Landrat "Mustafa Altınpınar" vom Dienst suspendiert.

Von einem Istanbuler Bezirksstaatsanwalt wurde Pamuk wegen Verstoß gegen den Artikel 301 des türkischen Strafgesetzbuches, der sogenannten „öffentlichen Herabsetzung des Türkentums“ angeklagt, worauf in der Türkei bis zu fünf Jahre Haft stehen. Der Prozess begann am 16. Dezember 2005, wurde jedoch noch am selben Morgen wegen offener Verfahrensfragen auf Februar 2006 vertagt. Gegen den Prozess protestierten unter anderen Amnesty International und zahlreiche Schriftstellerorganisationen sowie der Präsident des Deutschen Bundestages Norbert Lammert.

Der Fall ähnelt dem Prozess gegen Elif Shafak, die (ebenfalls 2006) angeklagt wurde, weil eine fiktive Romanfigur in dem Buch "Der Bastard von Istanbul" über den Völkermord an den Armeniern im Ersten Weltkrieg spricht.

Das Verfahren wurde am 22. Januar 2006 zunächst eingestellt. Nach Wiederaufnahme des Verfahrens wurde Pamuk zu einer Schadenersatzzahlung in Höhe von 6000 türkischen Lira an sechs Kläger verurteilt, die sich durch seine Äußerungen zum Völkermord an den Armeniern beleidigt fühlten; Anklagevorwurf war Pamuks Äußerung: „Die Türken haben auf diesem Boden dreißigtausend Kurden und eine Million Armenier getötet.“

Im Januar 2007 sagte Pamuk eine geplante Deutschlandreise ab. Nach Angaben seines Verlages sieht er sich nach dem Mord an Hrant Dink vermehrt mit Morddrohungen konfrontiert und könne nicht sicher reisen. Am 1. Februar 2007 verließ Pamuk die Türkei jedoch und flog in die USA. Nach Aussagen der Zeitung „Vatan“ werde er lange in den USA bleiben und nur noch als Gast in der Türkei leben. Mehr als zwei Monate später wurde Pamuk in die Wettbewerbsjury der 60. Filmfestspiele von Cannes berufen.

Im Januar 2008 hob die türkische Polizei die vermeintliche ultranationalistische Untergrundorganisation Ergenekon aus, die zahlreiche Mordanschläge geplant haben soll, darunter einen auf Orhan Pamuk. Zu den Festgenommenen gehört unter anderen der Rechtsanwalt Kemal Kerinçsiz, der auch das Strafverfahren gegen Pamuk wegen Verstoßes gegen den Artikel 301 des türkischen Strafgesetzbuches angestrengt hatte.

Für die französische Übersetzung seines Romans "Sessiz Ev" (dt. "Das stille Haus") erhielt er 1991 den ’Prix de la découverte européenne’, für den Roman "Beyaz Kale" (dt: "Die weiße Burg") 1990 den Independent Foreign Fiction Prize. Der Roman "Benim Adım Kırmızı" (dt: "Rot ist mein Name") wurde 2003 mit dem hochdotierten IMPAC Dublin Award ausgezeichnet.

Am 12. Oktober 2006 gab die Schwedische Akademie ihren Beschluss bekannt, Pamuk, „der auf der Suche nach der melancholischen Seele seiner Heimatstadt neue Sinnbilder für Streit und Verflechtung der Kulturen gefunden“ habe, den Nobelpreis für Literatur des Jahres 2006 zuzuerkennen.












</doc>
<doc id="8352" url="https://de.wikipedia.org/wiki?curid=8352" title="1. Jahrhundert">
1. Jahrhundert

Das 1. Jahrhundert begann am 1. Januar 1 und endete am 31. Dezember 100.

Das 1. Jahrhundert zählt im Mittelmeerraum zur Epoche der Antike.





"Einige der aufgelisteten Persönlichkeiten wurden schon gegen Ende des vergangenen Jahrhunderts geboren und werden dennoch hier aufgeführt. Andere wiederum wurden zwar in diesem Jahrhundert geboren, werden aber erst im nächsten Jahrhundert aufgeführt. Dies rührt daher, dass es als Kriterium für die Aufnahme nicht entscheidend war, ob das Geburtsjahr in dieses Jahrhundert fällt, sondern ob das hauptsächliche Werk und Wirken der Person in diesem Jahrhundert stattfand. Freilich ist eine klare Abgrenzung dieser Art nicht immer möglich."




</doc>
<doc id="8353" url="https://de.wikipedia.org/wiki?curid=8353" title="Showmaster">
Showmaster

Der Begriff Showmaster wurde vom niederländischen Entertainer Rudi Carrell – zuerst in dem Lied "Showmaster ist mein Beruf" – für den Moderator einer großen Fernsehshow geprägt. Bei seiner Tätigkeit übt der Showmaster häufig selbst Elemente der darstellenden Kunst aus, indem er humoristische, musikalische oder vokale Einlagen liefert, die das Publikum neben der reinen Sachinformation zusätzlich ansprechen (z. B. die Gesangseinlagen von Rudi Carrell oder die Schnellsprecheinlagen von Dieter Thomas Heck).

Bei dem Begriff selbst handelt es sich um einen Scheinanglizismus. Zwar wurde er aus englischen Begriffen zusammengesetzt, wird aber in der englischen Sprache selbst nicht verwendet, dort sagt man "host"/"emcee" (oder "MC" für "Master of Ceremony") (AE) oder "presenter/compère" (BE). Vom Begriff Showmaster wurde später auch der Quizmaster – so bezeichnete sich Hans-Joachim Kulenkampff – und der Talkmaster (Moderator einer Talkshow) abgeleitet.



</doc>
<doc id="8357" url="https://de.wikipedia.org/wiki?curid=8357" title="Algarve">
Algarve

Die Algarve ist die südlichste Region Kontinentalportugals. Sie hat eine Fläche von 4989 km² (5,59 % von Festland-Portugal) und rund 441.000 Einwohner (4,30 % von Festland-Portugal). Die Algarve bildet eine von sieben Regionen Portugals "()". Sie ist außerdem deckungsgleich mit der statistischen Subregion Algarve, der Metropolregion Algarve "()" und dem Distrikt Faro, einem von 18 Distrikten des Landes "()". Größte Stadt und Verwaltungssitz der Region ist Faro. Vor allem die Südküste („"die" Algarve“) der Algarve ist touristisch stark erschlossen.

Seit der alte deutsche Name „Algarbien“ außer Gebrauch geraten ist, hat sich auf Deutsch teilweise „die Algarve“ eingebürgert, womit im Wesentlichen die Algarve-Küste gemeint ist und obwohl der Name der gesamten Landschaft auf Portugiesisch männlich ist "( – also der Algarve bedeutet)". Wie bei vielen Toponymen auf der Iberischen Halbinsel zeugt auch bei der Algarve der Anlaut auf "Al-" vom arabischen Ursprung des Namens: Das arabische Wort bedeutet auf Deutsch ‚der Westen‘.

In oftmals nur undeutlicher Abgrenzung voneinander wird die Bezeichnung „Algarve“ für unterschiedliche geographische, politische und historische Regionen verwendet:

Das südportugiesische Königreich wurde zusammen mit den marokkanischen Überseegebieten als "" (Plural: die "Algarven"), früher auch als "Algarbien" bezeichnet.

Die Algarve ist eine Region im äußersten Südwesten Europas. Am Cabo de São Vicente bei Sagres liegt der südwestlichste Punkt des Kontinents und der Parque Natural do Sudoeste Alentejano e Costa Vicentina. Begrenzt wird die Algarve im Norden von der Region Alentejo, im Westen und Süden vom Atlantik und im Osten bildet der Rio Guadiana die Grenze zu Spanien. Die Küstenlinie der Algarve erstreckt sich über 155 km von Ost nach West und 52 km vom Süden zum Norden.

Naturräumlich wird die Algarve von Nord nach Süd in drei Bereiche unterteilt
Die Serra ist ein aus Sandstein und Tonschiefer bestehendes und sich auf einer Höhe zwischen 300 und 500 m hinziehendes Hügelland. Im Nordwesten ragt die Serra de Monchique empor mit dem höchsten Punkt Pico da Foia (902 m). Die Serra macht etwa die Hälfte des Gebietes der Algarve aus, sie ist dünn besiedelt und wird mit Ausnahme von Monchique und Umgebung von Touristen kaum besucht. Die Vorgebirgslandschaft des Barrocal schließt sich nach Süden an und umfasst etwa ein Viertel der Region. Auf dem bis zu 400 m hohen aus Kalksandstein bestehenden Hügelland wird hauptsächlich Landwirtschaft betrieben. Der dicht besiedelte Küstenstreifen „Litoral“ bildet das touristische Zentrum des Algarve. Dieser lässt sich wiederum in den ' (wörtl. ‚Lee‘, ‚windabgewandt‘) im östlichen Teil zwischen spanischer Grenze und Faro, den ' (‚Luv‘, ‚dem Wind zugewandt‘) zwischen Faro und dem Cabo de São Vicente und die Costa Vicentina an der Westküste des Algarve und darüber hinaus des Alentejo Litoral gliedern. Den Barlavento nennt man „Felsalgarve“, handelt es sich doch um eine zerfurchte 20–50 m hohe Steilküste mit malerischen Formationen aus gelben und rötlich braunen Kalk- und Sandsteinfelsen und kleinen Buchten. Der Sotavento wird auch als „Sand-Algarve“ bezeichnet, denn das Gebiet ist von Sandstränden und Lagunenlandschaften geprägt. Im Osten schließt sich die weite Bucht des Golfes von Cádiz an.

Die Anwesenheit des Menschen ist in Portugal seit dem "Homo erectus" belegt. Vom Neandertaler sind die Spuren durch einen Lagerplatz bei Vilas Ruivas im Distrikt Castelo Branco bereits deutlicher. Muschelschalenhaufen, von Archäologen Køkkenmøddinger genannt, entstanden vom Mesolithikum bis zum frühen Neolithikum durch den Verzehr von Muscheln an immer derselben Stelle. Aus diesen Muschelhaufen sind auch Bestattungen bekannt. Ab etwa 5000 v. Chr. ist der Ackerbau belegt (Cardial- oder Impressokultur). Später entstanden die zahlreichen Megalithanlagen (Alcalar).

Etwa 1000 v. Chr. drangen die Kelten hier ein und vermischten sich mit den Einheimischen zu Keltiberern. Etwa zeitgleich errichteten die Phönizier erste Häfen entlang der Küste der Algarve. Die Karthager gründeten ca. 550 v. Chr. Portimão (lat. Portus Hannibalis). Im zweiten Jahrhundert v. Chr. entstanden im Zuge der römischen Besiedelung der Iberischen Halbinsel zahlreiche Villen, deren Ruinen (Abicada, Boca do Rio, Milreu-Estói, Vilamoura) – vornehmlich in der Gegend von Faro und Lagos – besichtigt werden können.

Nach Eroberung durch die Goten im fünften Jahrhundert wurde die Algarve ab 711 von den muslimischen Mauren besiedelt. Der arabische Name "al-gharb" (der Westen) erklärt sich aus der geographischen Sicht Andalusiens. Bis zur portugiesischen Eroberung (Reconquista) im 13. Jahrhundert war zumindest im Westen der Algarve (Silves) die Mehrheit der Bevölkerung arabisch-maurisch. Seit Mitte des 15. Jahrhunderts eingeführte Sklaven stellten zum Ende des 16. Jahrhunderts mehr als die Hälfte der Bevölkerung der Algarve. Von 1595 bzw. 1640/66 bis 1808 war die Algarve ein halb-autonomes Gebiet mit eigener Steuerhoheit unter der Krone Portugals. Die portugiesischen Könige führten in dieser Zeit den Titel eines Königs von Portugal und Algarve. Als 1807 Napoleon in den Norden Portugals einmarschierte, wurde die Algarve von spanischen Truppen besetzt, der spanische Minister Manuel de Godoy wurde "Fürst der Algarven". Diese Besetzung endete durch die Rebellion von Olhão im Jahre 1808.
In der Algarve werden europaweit die meisten Sonnentage gezählt. Die Sommer sind heiß und trocken; auch im Winter sinkt die Temperatur selten unter 10 Grad Celsius. Selbst der Januar bietet sonnige Tage um 20 Grad. Die Nachttemperaturen sind durchweg angenehm. Starke Sommerhitze stellt sich in der Regel immer nur für wenige Tage ein, wenn statt der kühlen Winde vom Atlantik der sogenannte Levante aus Afrika bläst. In den jüngsten Jahrzehnten hat die Brandgefahr zugenommen.

Die Vegetation entspricht heute nicht mehr den ursprünglichen natürlichen Verhältnissen, denn Portugal war mit Ausnahme der Dünen und Marschen an der Küste mit Wald bedeckt. Die Rodung begann teils schon im Neolithikum und erreichte ihren Höhepunkt um 1550, als die Spanier immer mehr Holz für Schiffe und Handwerk sowie als Brennmaterial benötigten.

Heute gibt es größtenteils nur noch Macchie. Ende der 1970er Jahre begann eine große Aufforstungskampagne, in der vor allem Nadel- und Eukalyptussetzlinge gepflanzt wurden. Unter anderem wachsen dort noch winterharte Eichenarten und Korkeichen. Im südlichen Küstenbereich findet man auch Johannisbrot-, Feigen-, Mandel-, Lorbeer-, und Granatbäume. Sie wurden durch die Römer eingeführt, die ebenfalls den für den Mittelmeerraum typischen Ölbaum verbreiteten. Die aus Südamerika stammenden Palisanderholzbäume (Jacaranda mimosifolia) findet man häufig als städtische Straßenbäume, die im Frühjahr durch ihre blau-violette Blüte auffallen. Die Region Algarve ist zudem auch ein Anbaugebiet von Korkeichen.
Im Vergleich mit dem BIP der EU ausgedrückt in Kaufkraftstandards erreicht die Region einen Index von 78.7 (EU-25:100) (2003).
Während auf den 4989 km² nur 440.777 (2010) dauerhafte Einwohner leben, kann diese Zahl in den Sommermonaten auf mehr als das Dreifache ansteigen, wenn die Sommerquartiere gefüllt sind. Berühmt ist die Algarveküste für ihre zahlreichen feinsandigen Strände und die teils bizarren und monumentalen Felsformationen im westlichen Teil. Wegen der vielen Golfplätze, von denen manche bis direkt an die Steilküste hin angelegt sind, ist die Region insbesondere bei Golfern beliebt.

Beliebte Ferienorte im Osten der Algarveküste sind Tavira und Monte Gordo. An der Zentralalgarve sind es Vale de Lobo, Vilamoura, Albufeira, weiter westlich Carvoeiro und Ferragudo. Sehr beliebt sind auch Praia da Rocha, Alvor und Lagos. Weiter westlich sind bei Salema, Sagres, Carrapateira und Monte Clerigo (bei Aljezur) die Surf-Spots. Seit dem Jahr 2000 ist der Ferienhausurlaub an der Algarve stark gestiegen. Eine Vielzahl von Ferienhäusern und Poolvillen findet man bei Carvoeiro, Ferragudo, Lagos und Albufeira. Seit 2011 gibt es von der Hotellerie vermehrt Angebote zum All inclusive Urlaub. Die Algarve konnte in den Jahren 2013 und 2014 jeweils zweistellige prozentuale Zuwachsraten bei den Zahlen der Besucher und der Übernachtungen verzeichnen. Dabei profitierte die Region touristisch sowohl von Steigerungen durch ausländische als auch durch inländische Besucher.

Die archäologischen Sehenswürdigkeiten liegen im küstennahen Hinterland. Hier sind besonders römische Villen interessant.

Das südwestlichste Ende Europas am Cabo de São Vicente unweit der Stadt Sagres wurde früher als das Ende der Welt bezeichnet. Der Tourismus an der Algarveküste ist die wichtigste Einnahmequelle der ganzen Region.
Der Algarve ist im Gegensatz zu den anderen portugiesischen Verwaltungseinheiten sowohl eine Region, eine Subregion und mit dem Distrikt Faro, eine homogene Verwaltungseinheit, die exakt die gleiche Fläche und Bevölkerungszahl hat. Die Algarve unterteilt sich in die folgenden 16 Municípios:



</doc>
<doc id="8360" url="https://de.wikipedia.org/wiki?curid=8360" title="Sagres">
Sagres

Sagres [] ist eine portugiesische Gemeinde im Westen der Algarve, nahe dem Cabo de São Vicente, dem südwestlichsten Punkt des europäischen Festlands. Der Ort gehört zum Kreis Vila do Bispo, hatte am Einwohner und eine Fläche von km². Er liegt zudem im Parque Natural do Sudoeste Alentejano e Costa Vicentina, einem Naturschutzgebiet, das sich an der gesamten südwestportugiesischen Küste entlangzieht.

Wegen seiner exponierten Lage diente der Ort in früheren Jahrhunderten als Ausgangspunkt zahlreicher Seereisen. Bei der angeblich im 15. Jahrhundert gegründeten sogenannten "Seefahrtsakademie" (escola náutica) des Prinzen Heinrichs des Seefahrers, damals Gouverneur der Algarve, handelt es sich allerdings um eine Legende späterer Jahrhunderte, wie portugiesische Historiker schon Anfang des letzten Jahrhunderts bewiesen haben. Die eigentliche Ausbildung der Seefahrer fand im Wesentlichen in Lissabon und Lagos statt.

Die südliche Grenze des damals erforschten Gebietes lag bei Kap Bojador, ca. 27°N, an der afrikanischen Westküste, dem "Kap ohne Wiederkehr." Befürchtet wurden dahinter Seeungeheuer und Verbrennungen. Der erste neuzeitliche Europäer, der es umrundete und zurückkam, war 1434 Gil Eanes aus Lagos.
Südwestlich von Sagres auf einer ein Kilometer langen und etwa 300 Meter breiten Landzunge mit steil abfallenden Klippen, der "Ponta de Sagres", befindet sich das Fort 'Fortaleza de Sagres', ein Nationaldenkmal von überragender Bedeutung. Innerhalb dieser Festungsanlage liegt ein erst 1928 freigelegter, berühmter Steinkreis, dessen Alter und Zweck unklar ist. Der Kreis, unterteilt in 42 Felder und mit einem Durchmesser von 43 m, wird als Windrose (Rosa dos Ventos) interpretiert, könnte aber auch eine Sonnenuhr gewesen sein. Einig ist man sich aber darin, dass er aus der Zeit Heinrichs des Seefahrers stammt.

Nördlich davon befindet sich auf den Klippen oberhalb des Badestrandes "Praia de Beliche" ein kleineres Fort, 'Fortaleza de Beliche', das im Jahre 1587 von den Soldaten Francis Drakes zerstört wurde, ausgenommen die kleine Kapelle Santa Catharina. Nachdem es 1632 wieder aufgebaut und 1755 durch das große Erdbeben und dem darauf folgenden Tsunami erneut zerstört worden war, lag es über 200 Jahre in Trümmern. Dieses Erdbeben hatte nach heutigen Schätzungen eine Stärke von etwa neun auf der Richter-Skala und sein Epizentrum lag nur 200 km südwestlich des Cabo de São Vicente im Atlantik. Es gehörte zu den zerstörerischsten Naturkatastrophen der europäischen Geschichte. Die "Fortaleza de Beliche" wurde 1960 zum 500. Todestag Heinrichs des Seefahrers restauriert.

Am Cabo de São Vicente, dem südwestlichsten Punkt Europas, befindet sich ein großer Leuchtturm, der mit rund 90 Kilometern Nenntragweite als der stärkste Europas gilt. Der Leuchtturm kann zeitweise besichtigt werden.

In den 1980er und 1990er Jahren noch ein Geheimtipp für Individualisten, Aussteiger und Erholungssuchende, brachten Surfer Leben und Geld in den Ort, dennoch hat er seine Ruhe nicht verloren, obwohl es inzwischen auch Abendunterhaltungsmöglichkeiten gibt. Seit 1998 wurde in Sagres sehr viel gebaut, es entstanden - der Natur angepasste - Hotels und Unterkünfte in allen Preisklassen.


Sagres ist Sitz einer gleichnamigen Gemeinde (Freguesia) im Kreis (Concelho) von Vila do Bispo im Distrikt Faro. In ihr leben  Einwohner auf einer Fläche von  km² (Stand ).

Die Gemeinde besteht nur aus der Ortschaft Sagres.





</doc>
<doc id="8361" url="https://de.wikipedia.org/wiki?curid=8361" title="Große Kreisstadt">
Große Kreisstadt

Große Kreisstadt ist ein Begriff aus dem deutschen Kommunalrecht. Sie ist in einigen Bundesländern ein besonderer rechtlicher Status einer in der Regel größeren kreisangehörigen Gemeinde, die bestimmte zusätzliche Zuständigkeiten im Vergleich zu den sonstigen kreisangehörigen Gemeinden innehat. Dieser Sonderstatus fußt auf hoheitlicher Verleihung. Einerseits können kreisfreie Städte (Stadtkreise) aus Gründen des öffentlichen Wohls durch Gesetz oder Rechtsverordnung mittels Eingliederung in einen Landkreis zu kreisangehörigen Großen Kreisstädten herabgestuft werden, wie es im Rahmen von Kommunalreformen geschehen ist. Andererseits können kreisangehörige Gemeinden auf eigenen Antrag durch Rechtsverordnung des Landes- bzw. Staatsministeriums des Inneren den Status einer Großen Kreisstadt erlangen, der erst ab einer bestimmten Mindesteinwohnerzahl erteilt werden kann. Das Staatsministerium des Inneren hat bei der Abwägung, ob eine kreisangehörige Gemeinde zu einer Großen Kreisstadt werden soll, zu berücksichtigen, inwieweit die betreffende Gemeinde mit ihrer Leistungs- und Verwaltungskraft deren Aufgaben ordnungsgemäß erfüllen kann. Große Kreisstädte müssen nicht zwingend auch Kreisstädte (Sitz des Landratsamtes) sein.

In Baden-Württemberg, Bayern und Sachsen nennt man diese Städte mit Sonderstatus "Große Kreisstädte". Sie gehören also weiterhin zum Landkreis, übernehmen jedoch teilweise Aufgaben, die ansonsten der Landkreis erledigt. Die Einwohnergrenze ist unterschiedlich geregelt. In Baden-Württemberg liegt sie bei 20.000, in Sachsen bei 17.500 und in Bayern bei 30.000 Einwohnern. Hat eine Stadt die betreffende Einwohnerzahl erreicht, kann sie den Status "Große Kreisstadt" bei der Landesregierung beantragen. In aller Regel wird diesem Antrag dann auch entsprochen und die Stadt zur Großen Kreisstadt erklärt. Bei Gemeinden, die noch nicht Stadt waren, ist diese Erklärung automatisch mit dem Stadtrecht verbunden. Sinkt die Einwohnerzahl wieder unter die jeweilige Zahl ab, so behält die Stadt dennoch den Status einer Großen Kreisstadt (so z. B. Giengen an der Brenz in Baden-Württemberg).

Hier gilt eine Einwohnerzahl von mindestens 20.000. Am 1. April 1956 wurden mit Inkrafttreten der Gemeindeordnung alle Städte mit mehr als 20.000 Einwohnern zur "Großen Kreisstadt" erklärt. Inzwischen haben die meisten der ehemaligen Kreisstädte den Status Große Kreisstadt, weil sie meist durch Eingemeindungen die Grenze überschritten haben. Lediglich die heutigen Kreisstädte Künzelsau, Sigmaringen und Tauberbischofsheim sowie die ehemaligen Kreisstädte Buchen (Odenwald), Hechingen, Neustadt im Schwarzwald (heute Titisee-Neustadt), Müllheim (Baden), Münsingen, Bad Säckingen, Bad Saulgau, Stockach, Tettnang und Wolfach sind keine Großen Kreisstädte, weil sie weniger als 20.000 Einwohner haben. Der Bürgermeister einer Großen Kreisstadt trägt wie in Bayern die Amtsbezeichnung Oberbürgermeister. Es gibt derzeit in Baden-Württemberg 94 Große Kreisstädte.

Nach ihrer Aufgabenstellung sind Große Kreisstädte in Baden-Württemberg grundsätzlich „Untere Verwaltungsbehörden“, d. h., sie erledigen auch Aufgaben der Landkreise (denen sie weiterhin angehören) bzw. vergleichbaren Stadtkreisen. Welche Aufgaben jedoch weiterhin vom Landkreis übernommen werden, ist in § 19 Landesverwaltungsgesetz Baden-Württemberg abschließend geregelt. Dazu gehören z. B. das Staatsangehörigkeitswesen, die Aufsicht im Personenstandswesen sowie der Katastrophenschutz und die zivile Verteidigung.


In Bayern wurde der Status einer Großen Kreisstadt mit der Kreisgebietsreform am 1. Juli 1972 durch das 2. Gesetz zur Stärkung der kommunalen Selbstverwaltung vom 15. Dezember 1971 eingeführt. Am Vortag hatte Bayern noch 48 kreisfreie Städte, von denen 23 ihre Kreisfreiheit verloren und in die umliegenden bzw. neu gebildeten Landkreise eingegliedert wurden. Diese 23 vormals kreisfreien Städte hatten sich als zu schwach erwiesen, sämtliche Aufgaben eines Landkreises und einer Kreisverwaltungsbehörde (Aufgaben der allgemeinen und inneren Verwaltung, der Bauverwaltung, der Sicherheitsverwaltung usw.) namens des Staates zusätzlich zu ihren normalen Gemeindeaufgaben tragen zu können. Andererseits hatten sie eine höhere Leistungs- und Verwaltungskraft als die bisherigen „normalen“ kreisangehörigen Gemeinden. Daher wollte man den in die Landkreise eingegliederten Gemeinden einige Aufgaben überlassen, die über die einer „normalen“ kreisangehörigen Gemeinde hinausgehen. Somit wurden diese 23 Städte am 1. Juli 1972 zu „Großen Kreisstädten“.

Neben diesen durch Eingliederung von kreisfreien Städten in Landkreise veranlassten Statusänderungen sind in Bayern auch sechs kreisangehörige Gemeinden zu Großen Kreisstädten erhoben worden: Dachau durch Rechtsverordnung vom 4. Januar 1973, Dinkelsbühl und Donauwörth 1998 durch Gesetz vom 26. November 1997, Germering durch Rechtsverordnung vom 19. August 2004 mit Wirkung ab 1. Oktober 2004 und Fürstenfeldbruck durch Rechtsverordnung 2006. Erding wurde am 1. Januar 2013 als letzte bayerische Stadt mit mehr als 30.000 Einwohnern Große Kreisstadt.

Bei den Städten Dinkelsbühl und Donauwörth war ein Parlamentsgesetz erforderlich, da laut der Bayerischen Gemeindeordnung die Erhebung einer Gemeinde zur Großen Kreisstadt durch Rechtsverordnung erst ab einer Einwohnerzahl von 30.000 möglich ist. Bei beiden vormaligen Freien Reichsstädten handelt es sich um ehemalige kreisfreie Städte, die ihren Status jedoch bereits 1940 verloren und daher nicht bereits 1972 Große Kreisstädte wurden. Somit gibt es in Bayern derzeit 29 Große Kreisstädte. Kleinste Große Kreisstadt ist Rothenburg ob der Tauber mit 10.926 Einwohnern (Stand: 31. Dezember 2013).

Die Große Kreisstadt übernimmt nach Art. 9 Abs. 2 der Bayerischen Gemeindeordnung im übertragenen Wirkungskreis Zuständigkeiten als Gemeindeaufgaben, die sonst vom Landratsamt als der unteren staatlichen Verwaltungsbehörde wahrzunehmen sind. Der konkrete Umfang der auf die Großen Kreisstädte im Vergleich zu den anderen kreisangehörigen Gemeinden übertragenen Zuständigkeiten ist in der Verordnung über Aufgaben der Großen Kreisstädte (GrKrV) geregelt. Zu den zahlreichen übertragenen Zuständigkeiten zählen insbesondere die Aufgaben der unteren Bauaufsichtsbehörde, das Wasserrecht, die untere Straßenverkehrsbehörde, das Gaststättenrecht und einzelne Aufgaben nach der Gewerbeordnung. Die Aufgaben des eigenen Wirkungskreises der überörtlichen Gemeinschaft, wie Krankenhäuser, Kreisstraßen und Sachaufwandsträgerschaft für weiterführende Schulen verbleiben dagegen auch bei Großen Kreisstädten beim Landkreis.

In Bayern sind von den Großen Kreisstädten die leistungsfähigen kreisangehörigen Gemeinden zu unterscheiden, die nur auf dem Gebiet der Bauaufsicht und des Wasserrechts Aufgaben namens des Staates übertragen bekommen haben.

Der Bürgermeister einer Großen Kreisstadt trägt wie in Baden-Württemberg die Amtsbezeichnung Oberbürgermeister, wie in Art. 34 Abs. 1 Satz 2 GO festgelegt.


Hier gilt eine Mindesteinwohnerzahl von 17.500 als Voraussetzung, dass Städte auf ihren Antrag von der Staatsregierung zu Großen Kreisstädten erklärt werden, wenn sie Gewähr für die ordnungsgemäße Erfüllung der damit verbundenen Aufgaben bieten. Darüber hinaus konnten auch Städte mit weniger als 17.500 Einwohnern, die im Zuge der Kreisreformen der Jahre 1994/1996 und 2008 die Funktion des Kreissitzes verloren, den Status einer Großen Kreisstadt erlangen. Die Zuständigkeiten der Großen Kreisstädte sind gemäß § 3 Abs. 1 der Sächsischen Gemeindeordnung durch die "Verordnung der Sächsischen Staatsregierung über die Zuständigkeit der Großen Kreisstädte" vom 30. Juni 2011 (SächsGVBl. S. 202) veröffentlicht. Übertragen sind damit Aufgaben aus dem Gewerberecht und nach der Straßenverkehrsordnung.


Vergleichbare Bezeichnungen aus anderen Bundesländern sind:
¹ 

In Hessen gibt es auch sieben Städte mit Sonderstatus, sie haben dort jedoch keine besondere Bezeichnung. In den anderen Bundesländern sind solche Sonderstatusstädte nicht vorgesehen.



</doc>
<doc id="8362" url="https://de.wikipedia.org/wiki?curid=8362" title="Humanismus">
Humanismus

Humanismus ist eine seit dem 19. Jahrhundert gebräuchliche Bezeichnung für verschiedene, teils gegensätzliche geistige Strömungen in diversen historischen Ausformungen, unter denen der Renaissance-Humanismus begriffsbildend herausragt. Gemeinsam ist ihnen eine optimistische Einschätzung der Fähigkeit der Menschheit, zu einer besseren Existenzform zu finden.

Es wird ein Gesellschafts- und insbesondere Bildungsideal entworfen, dessen Verwirklichung jedem Menschen die bestmögliche Persönlichkeitsentfaltung ermöglichen soll. Damit verbindet sich Kritik an bestehenden Verhältnissen, die aus humanistischer Sicht diesem Ziel entgegenstehen. Hinsichtlich der konkreten Inhalte bestehen zwischen den einzelnen Humanismuskonzepten große Unterschiede, die sich aus der Verschiedenheit der anthropologischen Grundannahmen ergeben. Insbesondere besteht ein Gegensatz zwischen den Modellen, die aus der Tradition des Renaissance-Humanismus hervorgegangen sind, und alternativen Entwürfen der Moderne, die sich in Opposition zum traditionellen Humanismus begreifen und mit ihm wenig gemeinsam haben, aber am Begriff Humanismus als Selbstbezeichnung festhalten.

Der Humanismus der Renaissance war eine breite Bildungsbewegung, die auf antike oder als antik angesehene Vorstellungen zurückgriff. Die Renaissance-Humanisten erhofften sich eine optimale Entfaltung der menschlichen Fähigkeiten durch die Verbindung von Wissen und Tugend. Humanistische Bildung sollte den Menschen befähigen, seine wahre Bestimmung zu erkennen und durch Nachahmung klassischer Vorbilder ein ideales Menschentum zu verwirklichen und eine entsprechende Gesellschaftsform zu gestalten. Der humanistische Lebensentwurf, der an das antike römische Konzept der "humanitas" anknüpfte, trat als Alternative neben das traditionelle, aus dem Mittelalter überkommene Menschenbild, das stark auf Gott und das Jenseits ausgerichtet war. Scharf grenzten sich die Renaissance-Humanisten vom spätmittelalterlichen scholastischen Gelehrtentum ab.

Die auf antike Schriften und Kunstwerke als klassische Bildungsgüter fokussierte humanistische Bewegung verbreitete sich im 15. und 16. Jahrhundert von Italien aus in Europa, verlor aber im Lauf des 16. Jahrhunderts an Schwungkraft. Sie beeinflusste alle europäisch geprägten Teile der Welt. Einen neuen Impuls erhielt sie im 18. und 19. Jahrhundert durch den in Deutschland florierenden Neuhumanismus, der sich in erster Linie an der griechischen Antike orientierte und im deutschsprachigen Raum das höhere Bildungswesen prägte. Eine Begleiterscheinung war der Griechenenthusiasmus, der sich im Philhellenismus auch politisch auswirkte.

Kritik an der als einseitig empfundenen Ausrichtung des Neuhumanismus auf die Antike und das „klassische“ Griechentum kam von verschiedenen Seiten. Im englischen Sprachraum wurde die deutsche Griechenbegeisterung teils als „Tyrannei Griechenlands über Deutschland“ wahrgenommen. Verfechter einer gleichberechtigten neusprachlichen Bildung wie Friedrich Paulsen wandten sich gegen das Übergewicht des altsprachlichen Unterrichts im humanistischen Gymnasium, der daraufhin schrittweise zurückgedrängt wurde.

Neuartige Ausprägungen hat der Humanismusbegriff in der existentialistischen Philosophie sowie in Marxismus und Realsozialismus erfahren, wobei es von völlig neuen Ansätzen aus zu scharfer Abgrenzung vom „klassischen“ Humanismus kam. Als verbindendes Element alter und neuer Ansätze kann der Anthropozentrismus gelten, die Konzentration des Interesses und der Bemühungen auf den Menschen und seine Einzigartigkeit, im Gegensatz etwa zu Weltanschauungen, die Gott oder das Naturganze in den Mittelpunkt stellen oder die menschliche Lebensform nur als eine unter vielen auffassen.

Der deutsche Begriff "Humanismus" wurde erstmals von Friedrich Immanuel Niethammer in der 1808 erschienenen Schrift "Der Streit des Philanthropinismus und Humanismus in der Theorie des Erziehungs-Unterrichts unserer Zeit" verwendet. Er verteidigt die an der griechischen Klassik orientierte Bildung gegen die praktisch-technische Ausbildung an den Realschulen. Der praktische Nutzen soll nicht allein im Vordergrund stehen. Die humanistische Bildung gibt den Jugendlichen klassische Muster vor, die zu einer ästhetischen, moralischen und geistigen Entwicklung beitragen. Nach Niethammer hat der von den Griechen thematisierte Logos den Menschen über seine rohe Natur hinaus zum Geistigen geführt. Erst damit wurde seine wahre Menschlichkeit begründet. Der Logos, der sich in Jesus Christus inkarniert habe (), sei zugleich das Urprinzip menschlicher Bildung.

Neben die von Niethammer geprägte ursprüngliche, epochenübergreifende Bedeutung des Ausdrucks Humanismus trat im Lauf des 19. Jahrhunderts eine weitere: Er wurde als kulturhistorischer Epochenbegriff auch zur Bezeichnung für die lange Zeit des Übergangs vom Spätmittelalter zur Frühen Neuzeit verwendet. Erstmals sprach Karl Hagen 1841 in diesem engeren Sinn von „Humanismus“. 1859 veröffentlichte Georg Voigt das Standardwerk "Die Wiederbelebung des classischen Alterthums oder das erste Jahrhundert des Humanismus", das maßgeblich zur Etablierung der speziellen historischen Begriffsverwendung beitrug.

Gemeinsam ist den traditionellen Begriffsverwendungen, dass das Menschenbild und Bildungsideal führender Renaissance-Humanisten und deren Haltung gegenüber der Antike als Kernbestandteil dessen, was „Humanismus“ ausmacht, aufgefasst wird. Der traditionelle, „klassische“ Humanismus umfasst im engsten Sinne nur die Bildungsbewegung in der Epoche der Renaissance. In einem weiteren Sinne gehören dazu auch alle späteren Konzepte bis zur Gegenwart, deren Vertreter sich auf die Tradition des Renaissance-Humanismus berufen und dessen Hauptgedanken übernommen und weiterentwickelt haben. Das gemeinsame Hauptmerkmal aller traditionellen Richtungen ist die Vorstellung einer überzeitlichen Vorbildlichkeit antiker Muster.

In Anbetracht der zentralen Bedeutung römischer Vorbilder für die Inhalte des neuzeitlichen Humanismus werden mitunter auch diese Vorbilder selbst – in erster Linie Cicero – zum Humanismus gezählt. Als „Humanist“ kann Cicero auch unter dem Gesichtspunkt gelten, dass Humanisten Bewunderer und Nachahmer von Klassikern sind: Seine Rezeption der griechischen Literatur und Bildung ist dem Verhältnis der Renaissance-Humanisten zur altrömischen Kultur in gewisser Hinsicht vergleichbar. Daher ist in manchen Werken der Forschungsliteratur von einem antiken „römischen Humanismus“ die Rede. Die Anwendung des Begriffs „Humanismus“ auf Erscheinungen der römischen Geistesgeschichte hat zwar bei einer Reihe von Forschern Anklang gefunden, ist aber auch auf Kritik gestoßen und hat sich nicht allgemein durchgesetzt. Ein Einwand lautet, eine humanistische Haltung entspreche nicht der römischen Mentalität. Parallelen zum neuzeitlichen Humanismus schon in hochmittelalterlichem Schrifttum haben dazu geführt, dass mitunter auch von einem „mittelalterlichen Humanismus“ die Rede ist.

Für die Befürworter einer epochenübergreifenden, auch die antiken Vorbilder einbeziehenden Begriffsverwendung ist das verbindende Element – das spezifisch Humanistische – ein Konzept von "humanitas", das sowohl „Menschlichkeit“ im Sinne von humaner Gesinnung als auch sprachlich-literarische Bildung umfasst. Damit werde der Humanismus sowohl der ethischen als auch der intellektuellen Komponente des Menschseins gerecht. Auch in den Debatten um einen modernen Humanismus geht es um die beiden Aspekte der Humanität und einer idealistischen Erziehung und Bildung, wobei deren Gewichtung schwankt.

Dem kulturhistorischen Humanismusbegriff steht eine Vielzahl von im 19. und 20. Jahrhundert neu entwickelten Konzepten gegenüber, deren Vertreter unter „Humanismus“ nicht ein Phänomen einer abgeschlossenen Epoche oder einen bloßen Kanon herkömmlicher Bildungsgüter verstehen, sondern ein gesellschaftspolitisches Programm, das zur Bewältigung gegenwärtiger Herausforderungen und zur Gestaltung der Zukunft dienen soll. Die neuen Ansätze treten nicht nur als individualphilosophische Ausprägungen, sondern auch in Form breiterer Strömungen in Erscheinung. Teils bewegen sie sich in traditionellen Bahnen, indem sie an die Grundgedanken der Renaissance-Humanisten und des Neuhumanismus anknüpfen, teils distanzieren sie sich nachdrücklich davon und beschreiten völlig neue Wege. Walter Rüegg unterscheidet sechs Richtungen:

Die Vielfalt der gedanklichen Konzepte, Deutungen und gesellschaftspolitischen Verwendungen, die mit "Humanismus" in der jüngeren Vergangenheit verbunden wurden, unterstreicht einerseits eine überzeitliche Aura und Attraktivität dieses Begriffs, lässt ihn andererseits aber auch schillernd bis zur Beliebigkeit erscheinen. Daran wurde mitunter harsche Kritik geübt, so zum Beispiel durch den Romanisten Ernst Robert Curtius 1960:
Die Ansicht, dass nur ein auf die Ausgangskonstellation zielender Humanismus den Begriff richtig erfasse, ist zuletzt von Volker Reinhardt wieder betont worden. Als Erscheinung der europäischen Kulturgeschichte sei Humanismus auf die Zeit zwischen 1350 und 1550 beschränkt und danach untergegangen. Anderweitige Verwendungen, die in der Gleichsetzung von Humanismus mit "humanitär" gründeten, seien unhistorisch und sinnentstellend. Bei den „echten“ Humanisten habe es sich um „begnadete Polemiker, Selbstprofilierer, Ab- und Ausgrenzer in jeder Hinsicht“ gehandelt. Für sie sei der Perfektionsgrad des jeweiligen Lateins der Maßstab sittlicher Vervollkommnung gewesen. „Von dieser Sprachverherrlichung und diesem Sprachkult aber sind die Geistesbewegungen, die sich als neo-humanistisch verstehen oder als „modern-humanistisch“ angesprochen werden, denkbar weit entfernt.“

Den Ausgangspunkt für die Formulierung und Verbreitung des Gedankenguts, das später „humanistisch“ genannt wurde, bildete der antike römische Begriff "humanitas" („Humanität“, „Menschlichkeit“). Das vom Adjektiv "humanus" („menschlich“) abgeleitete Wort ist erstmals in der von einem unbekannten Verfasser stammenden Schrift "Rhetorica ad Herennium" bezeugt, die im frühen 1. Jahrhundert v. Chr. entstanden ist. Dort ist das Merkmal der "humanitas" das Mitgefühl, das als besondere Qualität des Menschen gilt, die sein Wesen von der tierischen Wildheit und Grausamkeit abhebt. In diesem Sinne wurde "humanus" schon früher in der römischen Komödie verwendet. Als „menschlich“ bezeichnete man in der lateinischen Umgangssprache eine milde, mitfühlende Person, wobei die Konnotationen „liebenswürdig“, „freundlich“, „wohlwollend“ und „hilfsbereit“ mitschwingen konnten. Eine solche Haltung wurde eher von kultivierten, vornehmen Bewohnern der Großstadt Rom als von der Landbevölkerung erwartet, daher erhielt das Adjektiv schon früh auch die Nebenbedeutungen „großstädtisch“ und „gebildet“ („urban“).
Das Bedeutungsfeld von "humanitas" wurde vor allem von Cicero geprägt, der später zum wichtigsten antiken Impulsgeber des Renaissance-Humanismus wurde. Kein anderer römischer Autor hat auf diesen Begriff so großes Gewicht gelegt wie Cicero. Auch für ihn war der Aspekt der menschenfreundlichen Gesinnung wichtig, den er als Kenner der griechischen Literatur im griechischen Begriff "philanthrōpía" („Philanthropie“) vorfand. Er war vom Philanthropie-Ideal beeindruckt, das er für eine spezifisch griechische Errungenschaft hielt. In diesem Sinne konstatierte er, die Menschlichkeit sei von den Griechen nicht nur praktiziert worden, sondern von ihnen zu den anderen Völkern ausgegangen. Daher schuldeten die Römer nun, da sie Griechenland beherrschten, den Griechen ganz besonders eine menschenfreundliche Behandlung.

Spätestens bei Cicero trat aber neben die herkömmliche Hauptbedeutung von "humanus" und "humanitas" eine weitere, die sogar in den Vordergrund rückte. Nach seinem Verständnis gehörte zum Menschentum nicht nur eine wohlwollende „Humanität“, sondern in erster Linie Bildung. Dabei ging es ihm um die Verwirklichung eines Bildungsideals, das an die griechische "paideía" anknüpfte. Bei seinem Bildungsziel setzte Cicero allerdings einen anderen Akzent als die griechischen Vorbilder, indem er mit der "humanitas" die Eigenart des spezifisch Menschlichen (im Unterschied zum Tier und zum Gott) hervorhob. Zu kultivieren war nach seiner Überzeugung die „Menschennatur“, das Menschengemäße, den Menschen Auszeichnende. Für dieses Element des nur dem Menschen Eigentümlichen, nur ihn Charakterisierenden hatten die griechischen Klassiker keinen besonderen Ausdruck. Ein Ideal vollendeten Menschentums gab es im griechischen Denken der klassischen Zeit nicht, da das Menschliche primär als etwas im Vergleich zum Göttlichen prinzipiell Mangelhaftes wahrgenommen wurde. Die römische "humanitas", in der die beiden Elemente Menschenfreundlichkeit und Bildung verschmolzen, stellte eine Neuschöpfung dar.

Eine zentrale Rolle spielte für dieses Verständnis von Menschentum die Fähigkeit zur sprachlichen Kommunikation auf hohem Niveau, die ein erstrangiges Bildungsziel war. Sie zeigte sich im öffentlichen Leben – in der Politik und im Rechtswesen – als Beredsamkeit, im alltäglichen privaten Umgang als Urbanität, das heißt als Höflichkeit, Witz, Anmut und Leichtigkeit in der Ausdrucksweise, worin sich eine gelassene Haltung spiegelte. Neben der Fähigkeit, sich der Sprache souverän zu bedienen und andere zu überzeugen, war die philosophische Charakterbildung, die Aneignung von Tugenden wie Milde, Gerechtigkeit und Würde ein Hauptelement des Strebens nach "humanitas" im Sinne Ciceros. Auch Freigebigkeit gehörte dazu.

In der römischen Kaiserzeit griff Seneca das Konzept der "humanitas" auf, verengte es aber, indem er nur die ethische Zielsetzung als wesentlich betrachtete.

Im engeren Sinne wird als Humanismus das sich vom Mittelalter und der Scholastik abwendende geistige Klima des 15. und 16. Jahrhunderts bezeichnet. Man unterscheidet dabei zwischen der Renaissance als dem umfassenden kulturellen, wissenschaftlichen und sozialen Wandel zwischen Mittelalter und Neuzeit und dem Humanismus als der Bildungsbewegung, die den Umbruch begleitete und ihm wichtige Impulse gab.

Wortführer der humanistischen Bewegung grenzten sich von der Vergangenheit der vorhergehenden Jahrhunderte, die man „Mittelalter“ (mittleres Zeitalter) zu nennen begann, scharf und verächtlich ab. Ihrer enthusiastischen Hinwendung zur Antike entsprach die Verdammung der als finster und barbarisch wahrgenommenen „mittleren“ Zeit. Diese demonstrativ hervorgehobene Seite des humanistischen Selbstverständnisses verdeckt aber den Umstand, dass es zwischen Mittelalter und Renaissance-Humanismus auch eine breite Kontinuität gab. Ein fließender Übergang zur neuen Epoche zeigte sich u. a. in den Phänomenen des spätmittelalterlichen „Vorhumanismus“ (Prähumanismus, Protohumanismus). In der modernen Forschung wird teils der Aspekt des Bruchs mit der Vergangenheit betont (Remigio Sabbadini, Eugenio Garin), teils der Aspekt einer Fortführung mittelalterlicher Ansätze, die in mancher Hinsicht sogar als nahtlose Kontinuität erscheint (Ernst Robert Curtius, Paul Oskar Kristeller).

Als das Byzantinische Reich im Spätmittelalter existenzgefährdende Krisen durchmachte und schließlich im Jahr 1453 mit der Eroberung Konstantinopels durch die Osmanen unterging, flüchteten zahlreiche byzantinische Gelehrte nach Italien und brachten eine Fülle von griechischen Handschriften in den Westen. Erst mit der Einbeziehung der griechischen Sprache und Literatur gewann der humanistische Kanon seine volle Gestalt. Auch die Erfindung des Buchdrucks war den Bestrebungen der Humanisten nützlich. Er verhalf ihren Werken zu weiter Verbreitung und machte die ganze gelehrte Welt mit ihren Ideen bekannt.

Die antike Kultur wurde als unübertrefflich nachgeahmt. Das Studium der antiken Literatur und Philosophie diente dazu, sich einer in sich ruhenden Bildung zu vergewissern und sich von theologischen und philosophischen Vorentscheidungen zu lösen. Der über den ständischen Gliederungen stehende "uomo universale" verkörperte das ideale Menschenbild. Bereits im 15. Jahrhundert bestand ein Selbstverständnis gebildeter Kreise, die sich als "humanistae" begriffen und so bezeichneten, also als Humanisten. Der Begriff "humanista" tauchte zum ersten Mal 1490 in einem volkssprachlichen Brief auf. Er bezeichnet die Gräzisten, Latinisten, Dichter und Redner, die sich den "studia humanitatis" widmeten und Cicero sowie Quintilian besonders in der Rhetorik als Vorbilder betrachteten. Diese Gelehrtenbewegung wollte das antike Menschenbild erneuern. 

Das lebensbejahende und schöpferische Individuum wurde rehabilitiert. Die Verherrlichung des Menschen ergab sich bei den italienischen Humanisten aus der Überzeugung, dass der Mensch als das Ebenbild Gottes das Höchste in der ganzen Schöpfung sei. Der berühmteste und einflussreichste Humanist der frühen Neuzeit war Erasmus von Rotterdam, dessen "philosophia christiana" die Überbetonung der rhetorischen Kultur relativierte. Weder Philipp Melanchthons Grundlegung der protestantischen Bildung noch das Schulwesen der Jesuiten sind ohne humanistischen Einfluss denkbar. Den Humanismus als Bildungsbewegung in seiner Vielschichtigkeit hatte vor Jacob Burckhardt schon Georg Voigt erkannt.

Der Renaissance-Humanismus wurde von den Päpsten finanziell gefördert. Mit Papst Pius II. stellten sie selbst einen bedeutenden Humanisten. Die Scholastikkritik der humanistischen Reformtheologen, die sich für eine Reform der herrschenden Theologie einsetzten, prägte viele spätere Reformatoren. Der teilweise unmoralische Lebenswandel der Kirchenoberen und Priester zog einen mehr oder weniger ausgeprägten Antiklerikalismus nach sich. 

Philipp Melanchthon stützte sich bei der Ausarbeitung seiner frühprotestantischen Hermeneutik auf die humanistische Rhetoriktradition.

Zum Untergang des Renaissance-Humanismus kam es im Verlauf des 16. Jahrhunderts unter anderem dadurch, dass er von den repräsentativen Intellektuellen als nicht mehr zeitgemäß angesehen wurde.

Im Zeitalter von Reformation und Glaubensspaltung in Europa vermochte sich das auf Frieden und Versöhnung zielende Wirken des fraglos bedeutendsten Humanisten seiner Zeit, des Erasmus von Rotterdam, nicht zu behaupten. Seine Werke wurden 1559 katholischerseits verboten. Sie blieben jedoch vor allem im Zuge des niederländischen Befreiungskampfes gegen die spanische Krone eine wichtige Inspirationsquelle seiner gelehrten Landsleute, die wiederum weit ausgreifende Impulse vor allem in von der Reformation erfassten Teilen Europas setzten. Zu den wichtigsten Vertretern des niederländischen Späthumanismus gehörte der an die stoizistische Tradition des europäischen Humanismus anknüpfende Justus Lipsius, der als Lehrer einer praktischen Vernunft vor allem durch sein Werk "De constantia" zum Erzieher staatstragender Schichten und calvinistischer Fürsten wurde. Zu ihnen gehörten auch Joseph Justus Scaliger, der Begründer einer nicht allein philologisch ausgerichteten Altertumswissenschaft, und Hugo Grotius, der erasmisches Denken in die Jurisprudenz einführte und das Rechtsbewusstsein um den Anspruch der Humanität und um das Recht der Völker auf Frieden erweiterte.

Die Ausrichtung des niederländischen Späthumanismus an stoischem Gedankengut konnte sich auf Erasmus stützen, der von den Schriften Senecas noch stärker angezogen war als von denen Ciceros. Die kirchliche Verurteilung des Erasmus und seiner Schriften ließ den Neustoizismus zu einer Morallehre werden, die mit der Berufung auf Erasmus und sein konfessionsunabhängiges humanistisches Programm zugleich einem verweltlichten Denken und einer aus kirchlicher Beaufsichtigung gelösten Wissenschaft entgegenkam.

Der vom niederländischen Späthumanismus ausgehende Einfluss im frühneuzeitlichen Europa war vielfältig gestreut und kam speziell im brandenburg-preußischen Staatswesen zur Entfaltung. Von seinem zum Calvinismus übergetretenen Vater Georg Wilhelm wurde der nachmalige Große Kurfürst Friedrich Wilhelm zu einem dreijährigen Studium unter anderem der Schriften Senecas nach Leiden geschickt. So entstand bei den Brandenburger Hohenzollern eine langlebige Tradition calvinistisch-puritanischer und stoisch-moralischer Ausrichtung, die auch zur Richtschnur für die preußische Offiziersausbildung und für das preußische Beamtentum wurde. Der darauf beruhende Katalog praktischer Tugenden umfasste Mannhaftigkeit, Gehorsam, Staatstreue, Pflichterfüllung, Redlichkeit, Beständigkeit und Ausdauer sowie Gottvertrauen.

Der Späthumanismus habe, so Günther Böhme, durch seinen protestantisch-römischen Charakter ein vom platonisch-christlichen Humanismus stark abweichendes Profil. Es handle sich unter anderem durch die Diskreditierung der Kirchen, durch Politisierung der Öffentlichkeit, durch die Verbürgerlichung von Staat und Verwaltung, durch das Vordringen der Volkssprachen sowie durch einen auf Vernunft als humane Gestaltungskraft setzenden Rationalismus um einen vielfältig gebrochenen Humanismus. „Die sich aus seiner Fortwirkung und der erasmischen Tradition eröffnenden Perspektiven weisen auf den Neuhumanismus, der die griechische Tradition erneuern wird.“

Die stoischen Lehren waren seit Beginn der Neuzeit auf Griechisch bei Epiktet, Mark Aurel und Diogenes Laertios sowie in lateinischer Sprache bei Cicero, Persius und Seneca gut zugänglich. Von Lipsius, Grotius und Pufendorf aktualisiert, bildete der Neostoizismus zusammen mit dem Epikureismus und der Skepsis eine neue Rezeptionsstufe der antiken Philosophie, auf der die hellenistischen Autoren in den Vordergrund traten. In der pädagogischen Theorie des 18. Jahrhunderts und für die Naturrechtslehren spielten, so Hubert Cancik, stoische Begriffe und Argumente eine erhebliche Rolle: „Der emphatische und positive Gebrauch des Wortes ‚Mensch’ und das überdeterminierte Wort "humanitas – humanité" – Humanität stammen aus dieser Tradition.“

Mochte der Humanismus als pedantische Buchgelehrsamkeit am Ende des 16. Jahrhunderts in Verruf geraten sein und anderen geistigen Strömungen wie Rationalismus, Fortschrittsdenken und Historismus Platz gemacht haben, so riss das Interesse an antiken Kulturzeugnissen und Gestalten auch außerhalb der gängigen Schulbildung doch nicht gänzlich ab, sondern wurde im 17. und 18. Jahrhundert zum Beispiel auch im weitgehend katholischen Frankreich durch Schriftsteller und Dichter wie Bruyère, Fénelon, Molière, Corneille und Racine und durch Vordenker der Aufklärung wie Montesquieu, Voltaire, Diderot und Rousseau aufrechterhalten und wiederbelebt. In der Französischen Revolution brach sich der Freiheitsdrang in antikem Dekor Bahn: „Man trägt die phrygische Mütze als Symbol der Freiheit, man stellt allerorts Brutus-Büsten auf, man ahmt antike Feste nach. Personen wie Straßen und Städte erhalten antike Namen.“
Während die französische humanistische Tradition die griechische Antike zwar nicht unbeachtet ließ, aber doch ungleich stärker das Römertum favorisierte, erwachte in bildungsorientierten Kreisen in England und besonders in Deutschland im 18. Jahrhundert ein vehementes Interesse an altgriechischer Kunst und Kultur. Zur Erklärung dieser besonderen Vorliebe kommen mehrere Faktoren in Betracht: Anders als im Falle Frankreichs hatte sich das Römische Reich nur auf Teile des späteren Deutschland erstreckt; die Anfänge deutscher Geschichte wurden mit der Befreiung der Germanen von römischer Herrschaft verknüpft. „Die Reformation“, heißt es bei Hans Oppermann, „trug weiter dazu bei, große Teile Deutschlands in eine neue Oppositionsstellung zu allem zu bringen, was Rom hieß und römisch war.“ In den von der Reformation erfassten Gebieten erhielt das Griechische als Sprache des Neuen Testaments eine besondere Bedeutung. Hinzu kam in der Epoche von Sturm und Drang eine Vorliebe für das Originalgenie und alles Ursprüngliche auf Kosten jeglicher Nachahmung bzw. Ableitung „aus zweiter Hand“. Damit waren die Römer gegenüber den Griechen, war Latein gegenüber Altgriechisch allenfalls zweite Wahl: „Diese besondere Bedeutung des Griechischen wird unendlich vertieft, das Griechenlanderlebnis selbst rückt in den Mittelpunkt der deutschen Kultur in der großen Bewegung des Neuhumanismus der Winckelmann und Wilhelm von Humboldt. […] ‚Das Land der Griechen mit der Seele suchend’ – das ist durchaus die Haltung der Adepten des Neuhumanismus, wobei das ‚das Land der Griechen’ sich dem Rang eines säkularisierten Gottesreiches zumindest nähert.“ Laut Oppermann bekam also das besagte Griechenlanderlebnis der Neuhumanisten einen pseudoreligiösen Akzent, nahm der Neuhumanismus den Charakter einer Menschheitsreligion an.

Wegweisend wurde 1755 Johann Joachim Winckelmanns Schrift "Gedanken über die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst", zumal in den hymnischen Sätzen:

Die äußere und die seelische Schönheit sind in den Bildwerken der Griechen demnach vereint, das Schöne und das Gute gemäß dem griechischen Bildungsideal der Kalokagathia untrennbar miteinander verbunden.
Die im Zusammenhang mit Impulsen der Französischen Revolution von Johann Gottfried Herder verfassten "Briefe zur Beförderung der Humanität" enthalten einen deutlichen Rückbezug auf stoisches Gedankengut. So leitet Herder aus der Vernunftbegabung der Menschen einerseits und ihren vielfältigen Schwächen andererseits die Bereitschaft und Notwendigkeit gegenseitiger Hilfe sowie den Bedarf zur Bildung des Einzelnen wie der menschlichen Gattung ab. Stoische Kosmologie, Anthropologie und Ethik können dem Individuum laut Cancik einen festen Stand verschaffen. Der von Bestialität, Grausamkeit und Destruktivität bedrohten Humanität ist durch Bildung entgegenzuwirken. Der Schlüssel zu Herders Humanitätsbegriff, so Cancik, liegt in den Ausdrücken "Menschheit, Menschlichkeit, Menschenrechte, Menschenpflichten, Menschenwürde, Menschenliebe".

Durch Herder gewann der Begriff "Humanität" laut Martin Vöhler im deutschsprachigen Raum nachhaltige Verbreitung. Die dabei von Herder eingenommene Perspektive war eine kosmopolitische. Er kritisierte Sklaverei und Ausbeutung ebenso wie Kolonialismus und Rassismus. Im 114. Humanitätsbrief wendete er sich gegen die von den vermeintlich kultivierten Nationen Europas verübten Verbrechen an der Menschheit: „Nenne man das Land, wohin Europäer kamen, und sich nicht durch Beeinträchtigungen, durch ungerechte Kriege, Geiz, Betrug, Unterdrückung, durch Krankheiten und schädliche Gaben an der unbewehrten, zutrauenden Menschheit, vielleicht auf alle Aeonen hinab, versündigt haben!“
Die Entwicklung eines theoretischen Gesamtkonzepts neuhumanistischer Bildung wie auch dessen Verankerung in staatlichen Institutionen war das Werk Wilhelm von Humboldts. Einerseits mit den in "Sturm und Drang" sich entfaltenden und bald als „Dichterfürsten“ gefeierten Schiller und Goethe befreundet, andererseits zu den führenden klassischen Philologen seiner Zeit in Beziehung stehend, propagierte auch Humboldt das Studium der Griechen als wirksames Mittel der Persönlichkeitsbildung in intellektueller, ethischer und ästhetischer Hinsicht: „Wir haben in den Griechen eine Nation vor uns, unter deren glücklichen Händen alles, was unserm innigsten Gefühl nach das höchste und reichste Menschendasein bewahrt, schon zu letzter Vollendung gereift war; wir sehen auf sie wie auf einen aus edlerem und reinerem Stoff geformten Menschenstamm, auf die Jahrhunderte ihrer Blüte wie auf eine Zeit zurück, in welcher die noch frischer aus der Werkstatt der Schöpfungskräfte hervorgegangene Natur die Verwandtschaft mit ihnen noch unvermischter erhalten hatte“.

Im Zuge der Preußischen Reformen unter König Friedrich Wilhelm III. wurde Humboldt als Leiter der "Sektion des Kultus und des öffentlichen Unterrichts" mit der Neuordnung des staatlichen Bildungswesens beauftragt und setzte binnen weniger Monate neue Lehrpläne und die Gründung der Berliner Universität, die heute seinen und seines Bruders Namen trägt, auf der Grundlage seines neuhumanistischen Bildungsideals ins Werk. Viele zeitgenössische Geistesgrößen waren davon beeinflusst und warben dafür, so zum Beispiel auch Hegel im Jahre 1809: „Lassen wir es gelten, daß überhaupt vom Vortrefflichen auszugehen ist, so hat für das höhere Studium die Literatur der Griechen vornehmlich und dann die der Römer die Grundlage zu sein und zu bleiben. Die Vollendung und Herrlichkeit dieser Meisterwerke muß das geistige Bad, die profane Taufe sein, welche der Seele den ersten und unverlierbaren Ton und Tinktur für Geschmack und Wissenschaft gebe.“

Bis gegen Ende des 19. Jahrhunderts spielte Humboldts neuhumanistische Ausrichtung an Gymnasien wie im Hochschulwesen eine wichtige Rolle. Seit der Jahrhundertmitte wurde die zeitlose Gültigkeit des idealisierten Griechenlandbildes allerdings zunehmend in Zweifel gezogen und durch den Wandel zur Industriegesellschaft mit verändertem Qualifikationsbedarf herausgefordert.

Der bedeutendste Repräsentant des sogenannten "Dritten Humanismus" war Werner Jaeger. Die Bezeichnung "Dritter Humanismus" – nach dem Renaissance-Humanismus und dem Neuhumanismus – stammt aus einer 1921 gehaltenen Rede des Berliner Philosophen Eduard Spranger, mit dem Jaeger befreundet war und der sich mit ihm gemeinsam für die alten Sprachen und eine "Philosophie der Bildung" einsetzte: „Aber ein Unterschied unseres Humanismus, den man den dritten nennen könnte gegenüber jenem zweiten, liegt in der Weite des Suchens und des Verstehens, das wir Modernen aufzubringen vermögen.“

Die Rechtfertigung für eine intensive Neubelebung bzw. nötige Rettung der humanistischen Bildungsidee leitete Jaeger aus veränderten Zeitumständen und Herausforderungen ab: „Der Prozentsatz der Bevölkerung, der an dem angestammten geistigen Besitzstand unserer Nation wirklich inneren Anteil hat, nimmt im Zeichen der fabrikmäßigen Massenproduktion der Popularwissenschaft und der Einführung von Kino, Rundfunk und Taschenmikroskop auf der Schule von Jahr zu Jahr ab. Die mächtigsten Wirtschaftsschichten unseres Volkes, Arbeitermasse und Großkapital, sind mit den wohlbekannten Ausnahmen den Grundlagen unserer humanen Kultur fremd, ja ihr teilweise feindselig. Der mittlere Bürgerstand aber, bei dem diese Interessen erblich und wenn auch nicht ohne Schwankungen bis vor kurzer Zeit am sichersten geborgen waren, wird zwischen den großen Mühlsteinen der modernen Wirtschaft zerrieben.“ Immer frühzeitiger erfasse „das Triebwerk der Berufsmaschine“ den Geist der Heranwachsenden und füttere ihn mit nutzenbezogenem „Zivilisationswissen“ auf Kosten der geistigen Individualität und freier seelischer Entfaltung. Das führe „zu rationalistischer Entleerung und Abplattung des Lebens, zu brutalen Reaktionen der vergewaltigten Natur, zur ungesunden Hypertrophie des Erwerbs- und Vergnügungssinnes, zur Aufhebung der geistigen Selbständigkeit von Staat und Kultur.“ Überzivilisation einerseits und Zivilisationsflucht andererseits vernichteten in letzter Übersteigerung die Kultur. Denn diese sei nicht äußerer Apparat noch formlose Innerlichkeit, sondern „hellstes Wissen des Geistes um sich selbst und sicheres Ruhen in seiner Form, zweckfreies Sein und Können.“ Alle echte Bildung sei humanistisch: „Bildung des Menschen zum Menschen.“

Nach Jaeger hat aber die Kultur im Griechentum schlechthin ihren Ursprung. Die Griechen haben ihre geistige Gesamtschöpfung als Erbe an die übrigen Völker des Altertums weitergegeben. Für Jaeger beginnt der Humanismus mit der Übernahme der griechischen Kultur im Römischen Reich. Der griechische Bildungsgedanke sei dann im Christentum in eigenständiger Weise fortgesetzt worden. Konstitutiv für jede Erscheinungsform von Humanismus sei dabei die Struktur des Wiederaufnehmens. Die abendländische Geschichte wird bei Jaeger zu einer Reihe von Erneuerungen der griechischen Bildungsidee. Der auf dem Gedanken der reinen Menschenbildung beruhende Kulturbegriff griechischen Ursprungs begründe für alle Völker des „hellenozentrischen Kulturkreises“ – angefangen mit den Römern – nicht lediglich eine historische Abhängigkeit, sondern beinhalte eine „geistigen Durchdringung mit griechischer Kultur“.

Im Zentrum von Jaegers Bildungsidee steht die Paideia. Die Gesamtheit der griechischen Kultur sei Ausdruck des Strebens, den Menschen zu formen. Das höchste Kunstwerk, das es zu bilden gelte, ist demnach der Mensch. „Jede Erweiterung des griechischen Wissens, jeder große Fortschritt führt zu einer Erweiterung auch der Bildung. Komödie, Tragödie, Rhetorik, Publizistik und Philosophie hallen wider von dem leidenschaftlichen Kulturkampf um Bildung. Sie wird Mittelpunkt des öffentlichen Lebens.“ Dabei hätten die Griechen die Dinge „organisch“ betrachtet. Sie hätten das Einzelne als Teil eines Ganzen aufgefasst. Erst dadurch seien sie zur Schöpfung des Begriffs „Natur“ fähig geworden. Mit diesem habe sich das Interesse verbunden für die Gesetze, welche in den Dingen selbst wirkten. Aus der Einsicht in die Gesetzmäßigkeiten des menschlichen Wesens entsprängen die Normen für die persönliche Führung der Seele und für den Aufbau der Gemeinschaft.

Im altsprachlichen Studium als geistformender Kraft im Sinne Wilhelm von Humboldts sah Jaeger weiterhin „die tragfähigste Grundlage einer humanistischen Bildung der Jugend.“ Aus der Verbindung von formaler Geistesschulung mit einem auf antike Fundamente gegründeten modernen Kulturbewusstsein ergebe sich „die Anschauung von Schöpfungen maßstabgebender, zeitloser Größe.“ Die Idee einer solchen Jugendbildung sei zwar hoch gegriffen; allerdings sei schon viel bewirkt, wenn auch nur dies oder jenes davon eindrucksvoll verwirklicht werde.

Seit dem Ende des 16. Jahrhunderts bildete sich unter Humanisten bzw. Späthumanisten die Vorstellung aus, Anteil zu haben an einer Republik der Gelehrten (französisch: "République des Lettres"). Man gehörte demnach gewissermaßen einem Stand der geistig Unabhängigen an, die eigene Traditionen ausbildeten. Dazu zählten die Pflege der lateinischen Sprache, die Briefkultur und eine bürgerliche Lebensform. Verhaltenslehre und Moralerziehung nach den Vorgaben des Justus Lipsius zielten aber zudem darauf, den Menschen zur Lebenstauglichkeit durch eine akademische Bildung auszubilden, die einerseits den menschlichen Bildungsprozess des Individuums förderte, zudem aber Grundlagen für die spezifische Berufsbildung einschloss. „Bildung wird einerseits zur Privatangelegenheit, Bereich des Rückzugs in die schöne Welt der Klassizität einer gedachten Vollkommenheit, und andererseits zum kulturellen Anspruch der Öffentlichkeit, Grundlage der Bewährung im Dienste der Allgemeinheit.“

Spannungsreich und zugleich befruchtend stellte sich das Verhältnis des Humanismus und seiner Anhänger zum Aufkommen der neuzeitlichen Naturwissenschaften dar. Für die Rechtsentwicklung wurden durch den niederländischen Späthumanisten Hugo Grotius wichtige Impulse gesetzt. Im 20. Jahrhundert entstand als gesonderte psychologische Schule die Humanistische Psychologie. Formen eines politischen Humanismus wurden seit der Mitte des 20. Jahrhunderts auch im geteilten Deutschland entwickelt und gesellschaftlich organisiert.

Der als Heilmittel gegen den Verfall der Bildung in Deutschland besonders von Werner Jaeger nach dem Ersten Weltkrieg initiierte „dritte Humanismus“ hatte die beabsichtigte Wirkung verfehlt, sodass eine breitenwirksame Wiederbelebung der griechischen Antike als Bildungskerngut an den realen Umständen und Herausforderungen der Zeit scheiterte. In Auseinandersetzung mit Jaeger, der einen politischen Humanismus zwar angestrebt, aber verfehlt habe, plädierte Bruno Snell für einen handlungsorientierten politischen Humanismus. Während der Jaegersche Ansatz weder Engagement noch Verpflichtung vorgesehen habe und so lediglich akademische Attitüde geblieben sei, ging es für Snell bei einem politischen Humanismus um bedeutsame Erfahrungen, die bei aller Verschiedenheit der geschichtlichen Voraussetzungen vielleicht Musterbilder ergäben, aus denen etwas zu lernen wäre. Er verwies darauf, dass die amerikanische Unabhängigkeitserklärung, mehr noch die Verfassung der Vereinigten Staaten auf antiken Grundlagen beruhten und führte dabei Aspekte der Präambel wie die Sorge für Gerechtigkeit, für inneren Frieden, gemeinsame Verteidigung, allgemeine Wohlfahrt und nachhaltige Freiheitssicherung an. Ihre Entstehung und erste politische Bedeutung verdankten diese Leitvorstellungen dem in Sparta wirkenden Tyrtaios und dem Athener Solon im 7. bzw. 6. Jahrhundert v. Chr.

Der Unterschied zwischen ästhetischem und politischem Humanismus liegt laut Snell darin, dass man bei ersterem die großen, vorbildhaften klassischen Werke habe, die sich allerdings der Theorie entzögen; für den politischen Humanismus verfüge man hingegen über die theoretischen Gedanken, die sich jedoch im Altertum ungenügend verwirklicht hätten. „Aber gleich ist in beiden Fällen, daß etwas von den Griechen Gewonnenes über sich hinaus weist, daß es ein immer wieder von neuem zu Gewinnendes ist.“

Die Anfänge der neuzeitlichen Naturwissenschaften im 16. Jahrhundert standen in einem spannungsreichen, aber auch fruchtbaren Verhältnis zum Humanismus. Dies zeigte sich u. a. in der Scheidung beider Sphären hinsichtlich des Lernstoffs bzw. Bildungsangebots. Wie die humanistischen Lehrpläne in den Schulen zunächst keinen Raum für Naturwissenschaften boten, hielt es die naturwissenschaftliche Forschung umgekehrt ebenso: Mit humanen Unwägbarkeiten wollte sie sich so wenig abgeben, wie anderseits die Geisteswissenschaften das Humanum keiner Naturgesetzlichkeit zu unterwerfen bereit waren.

Dennoch sind auch Bedingungs- und Wirkungszusammenhänge feststellbar. Zum einen erstreckte sich die intensive Befassung mit antiken Schriften nicht nur auf Dichtung und Philosophie, sondern betraf auch die wichtigsten Werke u. a. von Euklid, Apollonios von Perge, Archimedes und Pappos. Zudem lehrte die humanistische Philologie mit ihren textkritischen und vergleichenden Methoden ein rationales Denken der Vorurteilslosigkeit und der wissenschaftlichen Prüfung von Sachverhalten. Latein als einheitliche Wissenschafts- und Gelehrtensprache bestimmte auch die Terminologie der Naturwissenschaften. Andererseits favorisierten Humanisten für den Gebrauch in der Verwaltung und in der Volkserziehung auch die Muttersprachen. Indem auch Techniker und Praktiker die Naturwissenschaften betrieben und in Technologien umsetzten, wurde die Verbreitung wissenschaftlichen Schrifttums in den Landessprachen gefördert. Davon wurde in den Naturwissenschaften zunehmend Gebrauch gemacht.

Botanik, Zoologie und Mineralogie entfalteten sich im Zuge der philologischen Beschäftigung mit den antiken Texten nicht nur aufgrund von technisch-praktischem Interesse, sondern auch mit Hilfe des humanistischen Interesses an der gewissenhaften Darstellung des überlieferten Wissens. Im 16. Jahrhundert gab es zahlreiche Auflagen der Geometrie Euklids, der Naturalis historia (Naturgeschichte) des Plinius und der Medizin des Dioskurides. Die Werke Galens erschienen bis 1598 in insgesamt 660 Editionen.

In der aus ihren Gesetzmäßigkeiten zu erklärenden Wirklichkeit und in der Ablehnung der Metaphysik lag ein gemeinsames Interesse der sich ausdifferenzierenden Geistes- und Naturwissenschaften. Sie stellten sich gegen die „Mystifikation des Heilswissens“ durch Theologen und Kirchenfürsten wie auch gegen das pragmatische Herrschaftswissen der weltlichen Fürsten. Sie brachten den Zeitgeist hervor, „indem sie zwischen politisch dahinsinkendem Papsttum, aufsteigendem Nationalstaat und auf Servitut [Dienstbarkeit] fixierter Universität die freie Meinung unabhängiger Intelligenz durchsetzten und der Wissenschaft ‚herrschaftsfreie’ Erkenntnis“ zurückgewannen.

Es war der niederländische Späthumanismus, der den Humanitätsbegriff des Völkerrechts mit stoizistischem Moralbewusstsein verband. Bei der Schaffung einer neuen rechtlichen Ordnung der Welt wurde menschliche Vernunft maßgeblich. „Das bürgerliche Bildungsbewußtsein lernt, mit der Natur auf andere Weise umzugehen, analog dem Naturverständnis der Naturwissenschaften. Die Welt ist nicht mehr sakrosankte Schöpfung; und aus einer nach ‚natürlichen’ Gesetzen geordneten Natur wird das ‚natürliche’ Recht des Menschen abgelesen.“ 

Auf der Grundlage erasmischer Ideen entwickelte Hugo Grotius mit seinem Werk "De jure belli ac pacis" das Völkerrecht. Im Zuge der Säkularisierung traten die Juristen die Nachfolge der Geistlichen als wegweisende Autoritäten in einer nicht mehr hauptsächlich kirchlich geprägten Weltordnung an. Für ein Jahrhundert gelangte die Jurisprudenz und mit ihr das humanistisch erarbeitete und erweiterte römische Recht in der Rangfolge der Wissenschaften an die Spitze. Das ergab sich aus der Schaffung einer Rechtsgeschichte samt systematischer Quellenkritik sowie aus der Entwicklung eines Naturrechtsbegriffs und der Verbindung von Natur- und Völkerrecht. Grotius selbst folgerte: „Das Naturrecht ist so unveränderlich, daß es selbst von Gott nicht verändert werden kann.“

Die humanistischen Theorien in der Psychologie wurden maßgeblich von Abraham Maslow und Carl Rogers geprägt. Die Persönlichkeit entwickelt sich mit dem Ziel, sich selbst zu verwirklichen. Die eigenen Fähigkeiten und Talente sollen entwickelt werden, um das innere Potential zu realisieren. Das Streben nach Selbstverwirklichung ist zugleich der „Organisator all der unterschiedlichen Kräfte, deren Zusammenspiel ununterbrochen das erschafft, was eine Person ausmacht … Dieses angeborene Streben nach Selbsterfüllung und nach Realisierung des eigenen einzigartigen Potentials ist eine konstruktive leitende Kraft, die jede Person im Allgemeinen zu positiven Verhaltensweisen und zur Weiterentwicklung des Selbst bewegt.“

Orientiert am Leitbild des von Karl Marx vorgestellten realen Humanismus (siehe unten), wurde in der Deutschen Demokratischen Republik die Umwandlung des bürgerlichen Humanismus, vormals „Angelegenheit einer herrschenden Minderheit“, zur Massenangelegenheit propagiert: als „Entschluß zur tätigen Anteilnahme an den Bestrebungen unserer Zeit, das Bestehende zu ändern und durch Besseres zu ersetzen“, hieß es bei Heinrich Deiters. „Wenn wir auf die Geschichte der Menschheit seit der Wende vom 19. zum 20. Jahrhundert zurücksehen, so entdecken wir die Quellen, aus denen sich der neue Humanismus hauptsächlich genährt hat. Es sind die Kämpfe aller fortschrittlichen Menschen für einen gesicherten Frieden und für eine demokratische Staatsordnung, die Kämpfe der Werktätigen gegen die Reste der feudalen Eigentumsverhältnisse und das Monopolkapital, die Kämpfe der Kolonialvölker um ihre politische Selbstständigkeit.“ Im Gegensatz zum idealistischen Vorläufer entwickle der reale Humanismus seine Programmatik „aus der vollen Wirklichkeit und wendet sie auf die volle Wirklichkeit an.“ Somit werde der Gegensatz von Geist und Materie aufgehoben, der Mensch demnach nun als Einheit behandelt.

In der bereits seit Anfang der 1950er Jahre in den europäischen Einigungsprozess einbezogenen Bundesrepublik Deutschland, die nach dem verheerend gescheiterten Nationalsozialismus ebenfalls zu anderen politischen und weltanschaulichen Ufern aufbrach, ging die Wiederbelebung eines humanistischen Engagements unter neuen Vorzeichen von Bürgervereinigungen in Selbstorganisation aus. 

Die 1961 gegründete überkonfessionelle Humanistische Union, die sich hauptsächlich für den Schutz und die Durchsetzung der Menschen- und Bürgerrechte einsetzt, strebt vor allem die Verwirklichung des Gebots zur Achtung der Menschenwürde und des Rechts auf freie Entfaltung der Persönlichkeit an. Zu den vorrangigen Betätigungsfeldern zählen die Aufarbeitung von Bürgerrechtsverletzungen und die Förderung politischer Partizipation.

Mitglied in der Europäischen Humanistischen Föderation ist der Humanistische Verband Deutschlands, der 1993 als Zusammenschluss diverser älterer Freidenker- und humanistischer Vereinigungen gegründet wurde. Vorrangig wichtig ist den Mitgliedern der Einsatz für Menschenrechte, Frieden, Gleichberechtigung der Geschlechter und eine wissenschaftliche Welterklärung. Sie lehnen jeden Dogmatismus ab und favorisieren den Dialog auf der Grundlage rational nachvollziehbarer Begründungen. Zu den Leitprinzipien gehören Weltlichkeit, Selbstbestimmung, Solidarität und Toleranz. Hauptbetätigungsfelder sind laut Selbstauskunft praktische Lebenshilfe, Erziehung, Bildung und Kultur.

Aus marxistischer Sicht erscheint der „klassische“, auf der Tradition der Renaissance fußende Humanismus als eine bürgerliche Weltanschauung. Ihr wird vorgeworfen, kein Interesse für die soziale Frage aufzubringen. Das Proletariat bleibe von humanistischer Bildung ausgeschlossen. Nur für eine privilegierte Minderheit sei der Zugang zu Kultur und insbesondere Literatur gewährleistet. Dennoch hat Karl Marx den Begriff Humanismus aufgegriffen und im Rahmen seiner Lehre mit neuem Inhalt gefüllt. Er setzte den Kommunismus mit einem atheistischen Humanismus gleich. Der Kommunismus hebe das Privateigentum auf, das Ausdruck menschlicher Selbstentfremdung sei. Der Kommunismus sei deshalb die „wirkliche Aneignung des menschlichen Wesens durch und für den Menschen; darum als vollständige, bewußt und innerhalb des ganzen Reichtums der bisherigen Entwicklung gewordne Rückkehr des Menschen für sich als eines gesellschaftlichen, d. h. menschlichen Menschen. Dieser Kommunismus ist als vollendeter Naturalismus ≈ Humanismus, als vollendeter Humanismus ≈ Naturalismus, er ist die wahrhafte Auflösung des Widerstreites zwischen dem Menschen mit der Natur und mit dem Menschen, die wahre Auflösung des Streits zwischen Existenz und Wesen, zwischen Vergegenständlichung und Selbstbestätigung, zwischen Freiheit und Notwendigkeit, zwischen Individuum und Gattung.“

Der katholische Philosoph Jacques Maritain vertrat im 20. Jahrhundert einen christlichen Humanismus. Dieser sei aber erst dann integral, wenn der Mensch in seinem wahren Wesen, in seiner Bindung an Gott und seiner Erneuerung durch Gott erfasst werde. Die modernen Auffassungen von Humanismus sollten mit der von der mittelalterlichen Scholastik entwickelten Seinslehre verbunden werden.

In seinen vor 1945 veröffentlichten Werken nahm Jean-Paul Sartre eine kritische Haltung zum Humanismus ein oder verschwieg ihn. Erst in dem 1945 publizierten Essay "L’existentialisme est un humanisme" bekannte er sich zu einem eigenen Humanismuskonzept, dem "existentialistischen Humanismus". Dieser Humanismus betont die Eigenverantwortlichkeit des Menschen. Für Sartre ist der Existenzialismus „eine Lehre der Tat“. Er entwarf einen Humanismus im Gewand der Moderne: Die Existenz geht der Essenz voraus. Der Mensch tritt in die Welt ein und erst dann entwirft bzw. erfindet er sich selbst. Der Mensch ist nichts Anderes als das, wozu er sich in seiner totalen Freiheit macht. Deshalb ist er auch für das, was er ist, verantwortlich. Dies verleiht ihm seine Würde. Das Leben hat a priori keinen Sinn. Der Mensch wählt sich seine Moral, sie ist seine Schöpfung und Erfindung. Mit sich selbst erschafft der Mensch ein Vorbild. Der Mensch ist nichts Anderes als sein Leben. Er ist die Summe seiner Handlungen, seiner Beziehungen und Unternehmungen. Er existiert nur in dem Maße, in dem er sich selbst verwirklicht.

Sartre beschrieb sein Konzept als einen „Humanismus des Bedürfnisses“, den er dem „Humanismus der Arbeit“ – der Idee der Leistungsgerechtigkeit – als Alternative entgegenstellte. Der Humanismus des Bedürfnisses sei der einzige, der die ganze Menschheit zum Gegenstand habe; er beruhe auf dem Prinzip, dass das Bedürfnis und nicht das Verdienst Recht schaffe (Bedarfsgerechtigkeit). Die Beseitigung des Verdienstes sprenge die letzte Schranke, die die Menschen trenne.

In den Jahren von 1961 bis 1978 veröffentlichte Erich Fromm mehrere Aufsätze und Reden, die in dem Sammelband "Humanismus als reale Utopie" herausgegeben wurden. Die Entfremdung ist nach Fromm die Krankheit des modernen Menschen. Der Mensch wird zum Götzendiener, der das Werk seiner eigenen Hände anbetet. Er ist nur noch damit beschäftigt zu arbeiten, um konsumieren zu können. Er möchte viel haben, statt viel zu sein. Machtstreben, Vergnügungssucht und Besitz verdrängen Liebe, Freude und persönliches Wachstum. Ängstlichkeit verbindet sich mit der Unfähigkeit, zu lieben. Der moderne Mensch flieht in ein leeres Geschäftigsein. An die Stelle der traditionellen Werte des Guten, Schönen und Wahren, die der Entfaltung des Menschen dienten, ist der technologische Wert getreten: Das technisch Mögliche wird zum Selbstzweck; ist etwas technisch möglich, dann wird es auch getan. 

Nach Fromm soll man sich der "humanistischen Alternative" bewusst werden. Der Humanismus geht vom fühlenden, lebendigen, leidenden und denkenden Menschen als der zentralen Kategorie aus. „Bei diesem Bezugsrahmen besteht der Sinn des Lebens in der völligen Entwicklung der menschlichen Eigenkräfte, insbesondere in der von Vernunft und Liebe, im Transzendieren der Enge des eigenen Ichs und in der Entwicklung der Fähigkeit, sich hingeben zu können, in der vollen Bejahung des Lebens und von allem Lebendigen im Unterschied zur Anbetung von allem Mechanischen und Toten.“ Über das Unbewusste kann man den Kontakt zum "ganzen, universalen" Menschsein gewinnen:

Die Liebe ist der "Hauptschlüssel", mit dem sich die Tore zum persönlichen Wachstum öffnen lassen. Die Praxis der Liebe ist das menschlichste Tun, das den Menschen ganz zum Menschen macht und ihm zur Freude am Leben gegeben ist.

Der japanische Denker Daisaku Ikeda legte einen Humanismus-Entwurf vor, der einen universalen Humanismus in Verbindung von östlicher und westlicher Tradition entwickelt und als Leitbild für die Weltgemeinschaft vorschlägt.

Der österreichische Professor Mouhanad Khorchide vertritt eine islamische Auslegung des Humanismus.

Martin Heidegger antwortete 1947 mit seinem "Brief über den »Humanismus«" auf eine Anfrage des französischen Philosophen Jean Beaufret. Er warf dem klassischen Humanismus vor, dass in seiner Bestimmung des Menschen als vernünftiges Subjekt die eigentliche Würde des Menschen noch nicht erfahren sei und er die Humanitas des Menschen nicht hoch genug angesetzt habe. Die Philosophie sei schon bei antiken griechischen Denkern zur Metaphysik entartet. Das Wesen des Menschen müsse anfänglicher erfahren werden. Heideggers fundamentale Kritik am Humanismus betrifft nicht nur eine als verhängnisvoll betrachtete historische Entwicklung, sondern läuft auf die Forderung hinaus, den Humanismus als von vornherein verfehlten Ansatz aufzugeben.

Helmuth Plessner kritisierte den Humanismus aus der Sicht des Historismus: Die Geschichte der eigenen und der fremder Kulturen habe gezeigt, dass die Selbstauffassung des Menschen im Sinne einer Idee, was der Mensch sein solle, vom Menschen selbst geschichtlich und unter kulturell-kontingenten Annahmen hervorgebracht worden sei, also keinen Anspruch auf allgemeine Geltung erheben könne. So zeige die Erfahrung, „daß die Selbstauffassung des Menschen als Selbst-Auffassung, als Mensch im Sinne einer […] ‚Idee‘ selbst ein Produkt seiner Geschichte bedeutet, die Idee Mensch, Menschlichkeit von ‚Menschen‘ eroberte Konzeptionen sind, denen das Schicksal alles Geschaffenen bereitet ist, untergehen […] zu können.“

Er setzt dem eine Anthropologie entgegen, die die wesentliche "Unergründlichkeit" des Menschen ins Zentrum stellt: Was der Mensch sei, lasse sich nicht ergründen, denn der Mensch sei kein abgeschlossenes, sondern ein unfertiges Wesen. Diese Einsicht beende auch die Überheblichkeit einer missionierenden christlich-europäischen Kultur, die meine, die Menschlichkeit erst den anderen Kulturen bringen zu müssen.

Michel Foucault stellt sich die Frage, wie man als freier Mensch leben könne. Er wettet darauf, dass „der Mensch verschwindet wie am Meeresufer ein Gesicht im Sand“. Dabei ist "„der Mensch“" für Foucault eine epistemologische Denkfigur und lediglich ein Element in einem dem Subjekt notwendig vorausgehenden Gesamtzusammenhang. Das Subjekt kann nicht mehr Ursprung aller Erkenntnis und Wahrheit sein. Im Humanismus sieht Foucault die dunkle Seite der Aufklärung:

Für Foucault ist Humanismus nichts anderes als eine Säkularisierung idealistischer Gedanken. Es gebe weder ein Wesen des Menschen noch objektive und universelle Menschenrechte. Es bestehe auch keine überhistorische Norm, die das Wesen des Menschen bestimmen könne. Es handele sich beim Humanismus um den trügerischen Versuch von Selbstrechtfertigungen, die davon ablenken sollen, dass es dem Menschen wie allen Lebewesen um das bloße Funktionieren ohne irgendwelche höheren Zwecke gehe. Den Gedanken des Humanismus, dass der Mensch sich selbst Zweck sein könne, weist Foucault ab. 

Nicht der Mensch nehme die Stelle Gottes ein, sondern das System. Tatsächlich denke man innerhalb eines anonymen und zwingenden Gedankensystems einer bestimmten Sprache und Epoche. Die noch von Sartre verfochtene Freiheit sei letztlich eine Illusion. Mit dieser Erkenntnis werde die Idee vom Menschen überflüssig. Sie sei nur ein Hindernis, die wahren Zusammenhänge zu erkennen. Das am meisten belastende Erbe, das uns aus dem 19. Jahrhundert zufalle, sei der Humanismus. Alle politischen Regime des Ostens oder des Westens brächten ihre schlechte Ware unter der Flagge des Humanismus durch: „All diese Herzensschreie, alle diese Ansprüche der menschlichen Person, der Existenz sind abstrakt: d. h. abgeschnitten von der wissenschaftlichen und technischen Welt, die nämlich unsere wirkliche Welt ist. Was mich gegen den Humanismus aufbringt, ist der Umstand, dass er nur noch der Wandschirm ist, hinter den sich reaktionärstes Denken flüchtet […] Der Versuch, der gegenwärtig von einigen unserer Generation unternommen wird, besteht daher nicht darin, sich für den Menschen gegen die Wissenschaft und gegen die Technik einzusetzen, sondern deutlich zu zeigen, dass unser Denken, unser Leben, unsere Seinsweise bis hin zu unserem alltäglichsten Verhalten Teil des gleichen Organisationsschemas sind und also von den gleichen Kategorien abhängen wie die wissenschaftliche und technische Welt.“

Christliche Humanismuskritik wendet sich gegen den anthropozentrischen, als „weltlich“ betrachteten Ansatz der humanistischen Modelle, der als unvereinbar mit dem christlichen Konzept eines auf Gott ausgerichteten Lebens angesehen wird. Christliche Kritiker des Humanismus missbilligen nicht nur die glaubensferne, teils religionsfeindliche Haltung vieler Humanisten, sondern verwerfen auch den „christlichen Humanismus“, in dem sie einen Versuch der Harmonisierung von Unvereinbarem sehen. Im englischsprachigen Raum ist "secular humanism" („weltlicher Humanismus“) ein Kampfbegriff in Auseinandersetzungen um Religion und Christentum.

Der evangelische Dogmatiker Karl Barth meinte, man müsse in erster Linie von einem Humanismus Gottes sprechen: von der Liebe Gottes zum Menschen. Der Mensch als das von Gott bewirkte Wesen solle sich aus seiner irdischen Wirklichkeit in das Geheimnis seines Ursprungs öffnen. Dabei erfahre er dann die Heiligung der Gnade, den Humanismus Gottes. Die weltlichen Humanismen seien eigentlich überflüssig. Sie seien nur „abstrakte Programme“ gegenüber der von den Evangelien verkündeten Gotteskindschaft des Menschen. 

Für den im 20. Jahrhundert sehr einflussreichen evangelischen Theologen Rudolf Bultmann ist der Humanismus ein Glaube an den Adel des Menschen als Geistwesen. Der Geist verwirkliche sich im Wahren, Guten und Schönen. Diese Ideen bestimmten Wissenschaft, Recht und Kunst. Der Humanismus mache die Welt so zur Heimat des Menschen. Dagegen sei für das Christentum die Welt die Fremde. Der christliche Glaube entweltliche den Menschen. Gott als schlechthin jenseitiger sei von der Welt geschieden. Der Mensch als Sünder bedürfe der Gnade, da er nicht so sei, wie er sein solle. Die Gnade Gottes befreie den Menschen von sich selbst und mache ihn zu einem neuen Geschöpf. Der christliche Glaube bedürfe deshalb des Humanismus nicht, es bestehe vielmehr ein Widerspruch. Der einzelne Christ sei aber auf den Humanismus angewiesen, weil er die Welt durch Wissenschaft, Recht und Kunst beherrschbar mache.

Im 20. und 21. Jahrhundert ist das klassische humanistische Bildungsideal oft als unzeitgemäß und wirklichkeitsfremd kritisiert worden. Selbst unter Altphilologen wurde und wird seine Realisierbarkeit teils skeptisch beurteilt. So befand Uvo Hölscher, angesichts eines „kränkelnden Humanismus“ sei der Anspruch, durch altsprachlichen Unterricht humanistische Bildung zu erwerben, eine „Donquichotterie“.

Volker Reinhardt meint, der Renaissance-Humanismus sei untergegangen, da er die historische Entwicklung nicht mehr habe erklären und die Probleme der Zeit nicht mehr habe lösen können. Daher sei es „anachronistisch und aussichtslos, solche qua Untauglichkeit untergegangenen Wertekanones und Vorstellungswelten in späteren, ihnen völlig fremden Epochen wieder beleben zu wollen.“

Ein Vorwurf, der sich besonders gegen den noch im 20. Jahrhundert nachhaltig neuhumanistisch geprägten Gymnasialunterricht richtete, betraf das Bild der Antike, das den Schülern vermittelt wurde und damit einen maßgeblichen Einfluss auf die Vorstellungen einer breiten gebildeten Öffentlichkeit hatte. Das neuhumanistische Bild wurde als idealisierend und damit unhistorisch kritisiert; eine Antike, wie sie von vielen humanistischen Gymnasiallehrern dargestellt wurde, habe es in Wirklichkeit nie gegeben.

Schon der führende Gräzist Ulrich von Wilamowitz-Moellendorff (1848–1931) befand, die undifferenzierte Verherrlichung der Antike als Norm habe den Forschungsergebnissen nicht standhalten können. Er stellte fest: „Die Antike als Einheit und als Ideal ist dahin; die Wissenschaft selbst hat diesen Glauben zerstört.“

Hans Blüher, der die humanistische Bildung grundsätzlich vehement befürwortete, setzte sich in seiner 1927 veröffentlichten Abhandlung "Die humanistische Bildungsmacht" eingehend mit der Problematik der idealisierten Antike auseinander. Er meinte, es sei durch den Klassizismus, in Deutschland insbesondere durch den Einfluss der Weimarer Klassik, eine „Erstarrung der humanistischen Bildung“ eingetreten. Dadurch sei sie „in ihrer menschenfördernden Kraft“ zum Stillstand gekommen und habe, statt die Jugend zu bilden, „ganze Felder des seelischen Geschehens“ verheert. Diese These veranschaulichte Blüher anhand einer Analyse von Goethes Schauspiel "Iphigenie auf Tauris", das damals nach seinen Worten „ein gängiges Thema der üblichen Oberlehrerpädagogik“ war. Es seien künstliche, lebensfremde Charaktere, die es in der Antike nie gegeben habe und auch sonst nie habe geben können, geschaffen und der Jugend als Vorbilder hingestellt worden. Ein irriges Bild der Antike, geprägt von „der Winckelmannschen Auffassung von der ‚edlen Einfalt und stillen Größe’ der Griechen und ihrem ‚glücklichen Temperamente’“, sei „in die Hände der Epigonen und Oberlehrer“ gekommen. Dies habe eine „Erziehung zur Verlogenheit, Verbogenheit und Schiefheit“ zur Folge gehabt. Dennoch sei und bleibe das humanistische Gymnasium „Garant der europäischen Kultur“: „Führende Europäer (…) sollten nur auf einem vollen Gymnasium erzogen werden. Wer nicht weiß, was Europa ist, der hat auch nicht mitzureden.“ Erforderlich sei allerdings zunächst eine radikale Reform der humanistischen Bildung.

Gleichzeitig mit Blüher übte auch Egon Friedell in ähnlichem Sinn Kritik an der seit dem 18. Jahrhundert herrschenden neuhumanistischen Bildungstradition. Im 1927 publizierten zweiten Teil seiner "Kulturgeschichte der Neuzeit" betitelte er ein Kapitel programmatisch "Die Erfindung der Antike" und konstatierte: „Wir wissen heute, daß das Altertum nicht antik war. (…) Was den sogenannten humanistisch Gebildeten vom Altertum zurückgeblieben ist, sind einige tote Kostümstücke (…) Wir wissen heute, dass es den Griechen mit dem Sonnenauge und den Römer mit der Erzstirn niemals gegeben hat, weil es ganz unmöglich ist, dass es solche Menschen zu irgendeiner Zeit und an irgendeinem Ort gegeben haben kann.“ Der Neuhumanismus habe eine „Karikatur und Marionette von Hellenentum“ konstruiert. Im Übrigen sei schon der Gedanke der Nachahmung klassischer Vorbilder eine Absage an die eigene Schöpferkraft und ein Armutszeugnis, auch eine Absage an den wahren Geist der bewunderten Griechen, der nicht nachahmend, sondern erfinderisch gewesen sei.

Einige Jahre später trat die britische Germanistin Eliza Marian Butler mit einer fundamentalen Kritik am neuhumanistischen Einfluss auf die deutsche Literatur seit dem 18. Jahrhundert hervor. 1934 veröffentlichte sie ihre Untersuchung "The Tyranny of Greece over Germany", die im englischsprachigen Raum viel Beachtung fand. Butler meinte, es seien Phantasievorstellungen über das antike Griechenland kultiviert worden, was verhängnisvolle Auswirkungen gehabt habe. Führende Vertreter eines wirklichkeitsfremden klassizistischen Ideals seien bezeichnenderweise selbst nie in Griechenland gewesen.

Eine zeitgemäße humanistische Bildung kann nicht einfach geschichtliche Modelle kopieren oder die Antike schlicht als Vorbild anpreisen. Gleichwohl plädiert der Humanismusforscher August Buck dafür, in Anbetracht der vielfältigen Durchdringung der europäischen Kultur mit antiken Elementen die Erfahrungen der Antike für die Gegenwart nutzbar zu machen. Dabei könnten neben und nach der antiken „Anthropologie“, die so lange im Zentrum des humanistischen Bildungswesens stand, auch die naturwissenschaftlichen Erkenntnisse der Antike für Gegenwart und Zukunft fruchtbar gemacht werden. Buck verweist unter anderem auf Werner Heisenbergs Studie "Gedanken der antiken Naturphilosophie in der modernen Physik", in der es heißt:
Jörn Rüsen versteht Humanismus als „eine kulturelle Orientierung der menschlichen Lebenspraxis, die ihre entscheidenden Gesichtspunkte einer Deutung des Menschseins entnimmt“. Im Sinne von Kants Kategorischem Imperativ sei der Mensch nicht Mittel für die Zwecke anderer, sondern zu eigenen Zwecken begabt und mit je eigener Würde ausgestattet. Die Sache des Humanismus in der Gegenwart sieht Rüsen zwischen Abgelebtheit und Wiederbelebungsfähigkeit schwankend. Einen zukunftstauglichen Humanismus, der das fundamentale Merkmal kultureller Differenz im Globalisierungsprozess einbeziehen müsste, hält er für möglich, aber an bestimmte Voraussetzungen gebunden:

Die Frage, wie die zeitgemäße Ausrichtung eines Humanismus der Gegenwart beschaffen sein könnte oder müsste, stellt sich nicht nur im Lichte seines historisch-abendländischen Werdegangs, sondern neuerdings auch unter dem Eindruck von begrifflich und inhaltlich herausfordernden Ableitungen wie Transhumanismus und Posthumanismus. Für den ersteren ist der Mensch ein unvollendetes Produkt der biologischen Evolution, das der Optimierung bedarf, etwa mit gentechnischen Mitteln oder mit dem Einsatz von „bewusstseins- und intelligenzverstärkenden“ Drogen und Diäten. Für den Posthumanismus wiederum stellt der herkömmliche Mensch ein Auslaufmodell dar, das von seinen technischen Schöpfungen überholt wird und künstlichen Intelligenzen sowie Robotern als neuen Triebkräften der Evolutionsgeschichte weicht.

Dagegen reklamiert Peter Cornelius Mayer-Tasch im Geiste humanistischer Selbstbestimmung und Eigenverantwortlichkeit die Wahrung des rechten Verhältnisses zwischen Geschöpf und Schöpfung. Es bestehe ein oft unerkannter, aber grundlegender Unterschied zwischen selektiver Nutzung wissenschaftlicher Entwicklungen und der Unterwerfung unter deren Gesetzlichkeiten. Zu den Voraussetzungen für die selbstbestimmte Entfaltung menschlicher Würde im humanistischen Sinne gehöre die Unverfügbarkeit und Unberechenbarkeit des Lebens, die Sterblichkeit, Hinfälligkeit und Fehlbarkeit des Menschen. Erst aus dem Zusammenspiel der Erfahrungen von Sonnen- und Schattenseiten des Lebens könne sich sittliche Bejahung und Verneinung ergeben, ebenso die Ausbildung von Mitgefühl und Verantwortungsbewusstsein. Für am Leitbild von Maß und Mitte geschulte Humanisten, so Mayer-Tasch, ist das posthumanistische Programm ein Irrweg: „Sinnstiftung und Sinnverwirklichung ist nun einmal dem aus Körper, Seele und Geist bestehenden Menschen vorbehalten. Er ist es, der die Maschinen nicht nur konzipiert, sondern stets aufs Neue programmiert.“ Der erstrebenswerte aufrechte Gang des Menschen in eine wie auch immer unsichere Zukunft ist demnach vor allem im Einklang mit weiterzuentwickelnden humanistischen Wertungen und Haltungen gut möglich.
Bedeutsame humanistische Organisationen sind in Europa Humanists UK, der Humanistische Verband Deutschlands, der Human-Etisk Forbund, die Giordano-Bruno-Stiftung und die Allianz vun Humanisten, Atheisten an Agnostiker. In den Vereinigten Staaten sind das Council for Secular Humanism und die American Humanist Association von Bedeutung, auf internationaler Ebene trifft dies auf die Internationale Humanistische und Ethische Union zu. In Bayern ist vor allem der Bund für Geistesfreiheit aus geschichtlicher Tradition entstanden. 


Übersichtsdarstellungen

Allgemeine Einführungen und Gesamtdarstellungen

Renaissance-Humanismus

Späthumanismus, Neuhumanismus und Dritter Humanismus

Neuartige Ansätze der Moderne

Christlicher Humanismus




</doc>
<doc id="8365" url="https://de.wikipedia.org/wiki?curid=8365" title="Nacktsamige Pflanzen">
Nacktsamige Pflanzen

Die Nacktsamigen Pflanzen (Gymnospermae, von "gymnós" „nackt“ und "spérma" „Keim“, „Same“ – wörtlich „nackter Same“), kurz Nacktsamer, werden in die Unterabteilung Coniferophytina mit den Klassen Ginkgoatae und Pinatae und die Unterabteilung Cycadophytina mit den Klassen Cycadatae und Gnetalae unterteilt. Die Samenfarne bilden die Klasse Lyginopteridatae.

Die Nacktsamer sind Samenpflanzen (Spermatophytina), deren Samenanlagen nicht wie bei den Bedecktsamigen Pflanzen in einem Fruchtknoten eingeschlossen sind. Die Fruchtblätter sind, anders als bei Bedecktsamern (Angiospermen), nicht ganz geschlossen. Dies stellt innerhalb der Samenpflanzen den ursprünglichen Zustand dar.

Seit dem Oberperm, vor 270 Millionen Jahren (270 Ma), entwickelten sich die Gruppen von Nacktsamern. Später entwickelten sich in der mittleren Kreide, vor 120 Millionen Jahren, die Bedecktsamer (einzelne Vorläufer sind schon aus dem Oberen Jura bekannt). Alle nacktsamigen Taxa stellen heute Reliktgruppen dar. Gegenüber ehemals hunderttausenden von Arten gibt es heute nur noch gut fünfhundert Arten. Selbst die höheren Taxa wie Klassen enthalten oft nur wenige Arten. Die Areale sind oft disjunkt, das zeigt auch, dass diese Gruppen nur noch Relikte sind. Den Höhepunkt ihrer Entwicklung mit den meisten Arten hatten die Nacktsamer im Jura, sie dienten unter anderem vielen Dinosauriern als Nahrung. Heute artenreich und weitverbreitet sind alleine die Nadelholzgewächse (Pinophyta) mit über 350 Arten. Fossilfunde geben einen kleinen Überblick über den früheren Artenreichtum der nacktsamigen Taxa und der Ökosysteme, die sie damals bildeten.

Alle heutigen Nacktsamer sind Holzpflanzen mit sekundärem Dickenwachstum. Von den Bedecktsamern unterscheiden sich die nacktsamigen Pflanzen in der Anordnung ihrer Leitungsbahnen. Außerdem besitzen sie im Gegensatz zu den Bedecktsamigen Pflanzen im Xylem nur Tracheiden. Des Weiteren werden die Samenanlagen der Bedecktsamer von Fruchtblättern (Karpelle) eingeschlossen und der Samen wird durch eine Frucht verbreitet.

Die Blüten sind getrenntgeschlechtig und bestehen nur aus Mikro- und Megasporophyllen (die Bezeichnungen Staubblätter bzw. Fruchtblätter sollten auf die Bedecktsamer beschränkt bleiben). Häufig sind viele Blüten eines Geschlechtes in Zapfen zusammengefasst. Gymnospermen sind in der Regel einhäusig (monözisch), es gibt aber auch zweihäusige (diözische) Arten. Die Ausbreitung des Pollens erfolgt meistens mit Hilfe des Windes (Anemophilie). Es wurde aber auch Insektenbestäubung beobachtet. Dann gelangt der Pollen direkt auf die ungeschützte Mikropyle, die Empfängnisstelle. Bei Ginkgopflanzen (Ginkgophyta) und Palmfarnen (Cycadophyta) gibt es ähnlich wie bei vielen Algen begeißelte Spermazellen (Spermatozoide). Diese werden vom Pollenschlauch in eine flüssigkeitsgefüllte Vertiefung am Nucellusscheitel (Pollenkammer) entlassen, so dass sie zur Eizelle schwimmen können. Bei den anderen Gruppen wächst der Pollenschlauch zur Eizelle und entlässt dort einen Gametenkern, der die Eizelle befruchtet. Zwischen Bestäubung und Befruchtung liegt nur ein kurzer Weg ohne Barrieren. Allerdings kann zwischen beiden Vorgängen sehr viel Zeit vergehen.

Die Nacktsamer bilden ein monophyletisches Taxon. Zu den Nacktsamern werden traditionell gezählt: 



</doc>
<doc id="8366" url="https://de.wikipedia.org/wiki?curid=8366" title="Bush">
Bush

Bush steht für:


Orte im Vereinigten Königreich:

Orte in den Vereinigten Staaten:
im NRHP gelistete Objekte:
Siehe auch:


</doc>
<doc id="8370" url="https://de.wikipedia.org/wiki?curid=8370" title="Schiff (Begriffsklärung)">
Schiff (Begriffsklärung)

Schiff steht für:
Schiff ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="8371" url="https://de.wikipedia.org/wiki?curid=8371" title="Gehirn">
Gehirn

Als Gehirn oder Hirn (, , ) wird bei Wirbeltieren der im Kopf gelegene Teil des zentralen Nervensystems bezeichnet. Das Gehirn, anatomisch Encephalon genannt (von und "kephalē" ‚Kopf‘), liegt geschützt in der Schädelhöhle, wird von Hirnhäuten umhüllt und besteht hauptsächlich aus Nervengewebe. In Höhe des Foramen magnum geht es in das Rückenmark über, beide zusammen bilden das Zentralnervensystem (ZNS).

Das Wirbeltier-Gehirn verarbeitet hochdifferenziert Sinneswahrnehmungen und koordiniert komplexe Verhaltens­weisen. Es ist somit der Speicher für alle komplexen Informationen, die der Organismus verarbeitet.

Nicht jede Information gelangt bis zur Hirnrinde und führt zu Bewusstsein. Peripher liegende Nervengeflechte (Plexus) und vor allem Zentren im Hirnstamm verarbeiten die meisten der von Rezeptoren ankommenden Erregungen unbewusst. Reflexbögen übernehmen Aufgaben, die mit höchster Geschwindigkeit und ohne bewusste Verarbeitung und verzögernde Einflussnahme erledigt werden. Beim Menschen gibt es ebenfalls ein solches autonomes Nervensystem. Es koordiniert vegetative Funktionen wie Atmung, Herzkreislauf, Nahrungsaufnahme, -verdauung und -abgabe, Flüssigkeitsaufnahme und -ausscheidung sowie Fortpflanzung.

Im Gehirn interagieren stark vernetzte Neuronen (siehe Neuronales Netz und Erregungsleitung). Seine Tätigkeit wird in vivo durch die Messung der Gehirnströme per Elektroenzephalografie (EEG) und der vom Gehirn produzierten elektrischen Felder per Magnetoenzephalographie (MEG) untersucht.

Im Lauf der Evolution hat das Gehirn „höherer“ Tiere ein beachtliches Maß an Differenzierung und innerer Organisation erreicht (Zerebralisation). Das spiegelt sich in der psychischen und körperlichen Entwicklung des Einzelnen wider (siehe Embryologie). Die Struktur und – in geringerem Maß – das Volumen des Gehirns korrelieren mit Lernfähigkeit und Intelligenz. Erst in der Hierarchie des Nervensystems ist die Leistung des Gehirns verständlich.

Neben den Wirbeltieren besitzen Tintenfische hochkomplexe Gehirne, die sie zu gezielten Tätigkeiten befähigen. Im weiteren Sinne ist es die Zentralstelle des Nervensystems verschiedener wirbelloser Tiere, etwa Ringelwürmern oder Insekten. Je nach Gehirntyp handelt es sich um ein Cerebralganglion oder ein Oberschlundganglion. Zwei Gruppen wirbelloser Tiere haben besonders komplizierte Gehirne: Gliederfüßer (Insekten, Krebstiere, und andere), und Kopffüßer (Kraken, Tintenfische, und ähnliche Weichtiere). Die Gehirne der Gliederfüßer und der Kopffüßer gehen aus zwei nebeneinander liegenden Nervensträngen hervor. Kopffüßer wie der Krake und der Tintenfisch haben die größten Gehirne aller wirbellosen Tiere.

Das hochentwickelte Gehirn von Wirbeltieren unterscheidet sich deutlich vom Strickleiternervensystem der Gliederfüßer. Bei Insekten zieht sich der Verdauungstrakt direkt durch das vordere Nervensystem (zwischen Tritocerebrum und subösophagealem Ganglion), sodass die Bauchganglien ventral (bauchseitig) des Darmrohrs liegen, während bei Wirbeltieren das Rückenmark dorsal (rückenseitig) des Darms liegt.

Für eine Gliederung des Gehirns können unterschiedliche Kriterien maßgeblich sein, sodass verschiedene Einteilungen in Hirnbereiche möglich sind, die sich nicht gegenseitig ausschließen müssen. Für eine Gliederung des ausgewachsenen menschlichen Gehirns kann es auch durchaus sinnvoll sein, die aus der Untersuchung seiner Entwicklungsschritte gewonnenen Erkenntnisse zu berücksichtigen.

Beispielsweise zeigen sich in der ontogenetischen Gehirnentwicklung beim Menschen nach der Neurulation der zentralen Anteile der Neuralplatte zum Neuralrohr als der frühen embryonalen Anlage des Zentralnervensystems im weiteren Verlauf aufeinander folgende Stadien bei der Ausbildung des Gehirns. So bilden sich nach Schluss der vorderen Neuralrohröffnung Ende der vierten Entwicklungswoche zunächst drei sogenannte primäre Hirnbläschen aus dem vorderen Neuralrohrdrittel, die Anlagen von Prosencephalon, Mesencephalon und Rhombencephalon. Sie entwickeln sich verschieden, sodass sich beim über fünf Wochen alten Embryo fünf sekundäre Hirnbläschen unterscheiden lassen – diese führen zur
Die hier dargestellte Grobgliederung folgt dem Werk von Pinel.

Die Länge aller Nervenbahnen des Gehirns eines erwachsenen Menschen beträgt etwa 5,8 Millionen Kilometer, das entspricht dem 145-fachen Erdumfang.

Das Volumen eines menschlichen Gehirns liegt bei einem Mann bei durchschnittlich etwa 1,27 Litern, bei einer Frau bei etwa 1,13 Litern.

Es lassen sich vereinfacht vier Hauptbereiche unterscheiden.

Das Großhirn ist in der Mitte durch einen Einschnitt in zwei Halbkugeln (Hemisphären) geteilt. Zwischen diesen gibt es eine breite Verbindung aus einem dicken Nervenstrang, Corpus callosum oder Balken genannt, und weitere kleinere Verbindungen.

Seine 2–4 mm dicke Oberflächenschicht (Großhirnrinde, "Cortex") ist stark gefaltet und fast einen Viertel Quadratmeter groß. Sie enthält etwa 16 Milliarden Nervenzellen, was etwa einem Fünftel der Nervenzellen des gesamten Gehirns entspricht. Unter der Rinde verlaufen Nervenfasern. Ansammlungen von Neuronen sind rosa, die myelinhaltigen Fasern weiß. Im toten Gehirn färben sich die Neuronen grau. Deshalb heißen sie, obwohl sie während des Lebens rosa sind, graue Substanz.

Auf der Rinde lassen sich die sogenannten "Rindenfelder" lokalisieren, unterschieden zwischen primären Feldern und Assoziationsfeldern. Die primären Felder verarbeiten nur Informationen einer bestimmten Qualität, solche über Wahrnehmungen (Empfindung, zum Beispiel Sehen, Riechen, Berührung) oder über einfache Bewegungen. Die Assoziationsfelder stimmen verschiedene Funktionen aufeinander ab. Die Zuweisung eines Rindenfeldes zu einer bestimmten Funktion wird immer wieder definiert und relativiert. Erst das korrekte Zusammenspiel verschiedener Felder ermöglicht eine Funktion.

Zu den primären Feldern zählen zum Beispiel der visuelle Cortex, der am hinteren Pol des Gehirns liegt und auf dem die Projektionen der Sehbahn münden, und der auditorische Cortex, der der Verarbeitung akustischer Reize dient und seitlich im Schläfenlappen liegt.

Assoziative Felder finden sich unter anderem im vorderen Teil des Gehirns. Ihre Aufgaben sind zum Beispiel Gedächtnis und höhere Denkvorgänge.

Die Rindenfelder und ihre Funktionen können voneinander abgegrenzt werden, indem nach deren Ausfall (zum Beispiel durch Schlaganfall) die Tätigkeit des Patienten oder durch elektrische Stimulation, mikroskopische und andere Techniken das gesunde Gehirn untersucht wird. Neben der Großhirnrinde sind meist andere Hirnregionen an einer bestimmten Funktion beteiligt.

Zum Zwischenhirn gehören vier Teile:


Der "Thalamus" ist der Vermittler sensorischer und motorischer Signale zum und vom Großhirn. Bei ihm laufen alle Informationen der Sinnesorgane zusammen und werden weiter vermittelt. Er besteht hauptsächlich aus grauer Substanz. Der "Hypothalamus" steuert zahlreiche körperliche und psychische Lebensvorgänge und wird selbst teils neuronal über das vegetative Nervensystem, teils hormonell über den Blutweg gesteuert. Hypothalamus und Hypophyse (wichtige Hormondrüse des Körpers, die über den Hypophysenstiel mit dem Hypothalamus verbunden ist) sind das zentrale Bindeglied zwischen dem Hormon- und dem Nervensystem. Das Zwischenhirn ist beteiligt an der Schlaf-Wach-Steuerung (siehe aufsteigendes retikuläres Aktivierungssystem, Schmerz­empfindung, Temperaturregulation).

Am Kleinhirn lassen sich ebenfalls zwei Hemisphären unterscheiden. Zusätzlich werden weitere Teile abgegrenzt. Es ist zum Beispiel für Gleichgewicht und Bewegungen und deren Koordination verantwortlich. Bei Tieren ist es – im Vergleich zum Großhirn – oft stärker entwickelt als beim Menschen, insbesondere bei Arten mit Flugvermögen oder bei schnellen Räubern.

Außerdem wird dem Kleinhirn eine Funktion beim unbewussten Lernen zugeschrieben. Neuere Forschungen (2005) lassen darauf schließen, dass es am Spracherwerb und dem sozialen Lernen beteiligt ist.

Der Hirnstamm ist der stammesgeschichtlich älteste Teil des Gehirns. Er bildet den untersten Gehirnabschnitt und besteht aus auf- und absteigenden Nervenfasern (Weiße Substanz) und Ansammlungen von Neuronen beziehungsweise von Somata (Graue Substanz), morphologisch aus dem Mittelhirn, der Brücke (Pons) und dem Nachhirn (auch verlängertes Mark = Medulla oblongata genannt, da zwischen Rückenmark und Brücke gelegen). Der Hirnstamm verschaltet und verarbeitet eingehende Sinneseindrücke und ausgehende motorische Informationen und ist zudem für elementare und reflexartige Steuermechanismen zuständig.

Im "Nachhirn" kreuzen sich die Nervenbahnen der beiden Körperhälften. Außerdem werden hier viele automatisch ablaufende Vorgänge wie Herzschlag, Atmung oder Stoffwechsel gesteuert. Ebenso befinden sich hier wichtige Reflexzentren, die zum Beispiel Lidschluss-, Schluck-, Husten- und andere Reflexe auslösen. Das untere Ende des Nachhirns schließt an das Rückenmark an.

Die Gehirne von Männern und Frauen unterscheiden sich in der Größe und im Aufbau. Durchschnittlich wiegt das Gehirn eines erwachsenen Mannes je nach Ethnie etwa 1400 g. Bei gleicher Statur von Mann und Frau ist das Gehirn bei Männern durchschnittlich 100 g schwerer. Nicht nur die Gesamtgehirngröße unterscheidet sich zwischen den Geschlechtern, sondern die relative Größe verschiedener Gehirnareale. Am besten erforscht sind hierbei der Hippocampus und die Amygdala.

Zur Entstehung dieses Dimorphismus gibt es verschiedene Theorien. Zum einen kommt alternatives Spleißen von mRNA in Frage. Zum Beispiel das Spleißen von Kanalproteinen, sodass deren Durchlässigkeit für Ionen verändert ist. Zum anderen sind epigenetische Kontrollmechanismen relevant. Hierzu zählen unter anderem die genomische Prägung und die Histonmodifikation. Zudem wird immer wieder die Frage gestellt, inwiefern die Umwelt Einfluss auf den Dimorphismus hat.

Ein anderer Erklärungsansatz ist folgender: Geschlechtshormone, wie Testosteron und die Östrogene, wirken nicht nur auf die Keimdrüsen, sondern in vielfältiger Weise auf das gesamte Nervensystem: auf Nervenzellen, Synapsen, Genexpression. Dies gilt für die Zeit der Embryonalentwicklung und während der Kindheit, der Pubertät und im Erwachsenenalter. So bewirken die Geschlechtshormone eine typische männliche beziehungsweise weibliche Entwicklung des Nervensystems. Dies wird zum Beispiel in der "Regio praeoptica" im Hypothalamus sichtbar, die bei jungen Männern im Vergleich zu Frauen vergrößert ist.

Ein entscheidender Faktor sind vermutlich die Barr-Körperchen, da viele X-chromosomale Gene in die neuronalen Prozesse der Gehirnentwicklung involviert sind. Die Barr-Körperchen entstehen durch zufällige Inaktivierung eines X-Chromosoms bei der Frau. Dies hat zur Folge, dass das weibliche Gewebe und die Organe, inklusive des Gehirns, ein Mosaik darstellen, da in jeder Zelle ein anderes Gen des polymorphen X-Gens exprimiert wird. Daher wird angenommen, dass die unterschiedlichen Geschlechtschromosomen der wahrscheinlichste Grund für den Dimorphismus sind. Diese können auf zwei Arten die Entwicklung beeinflussen. Zum einen können die Genprodukte der Chromosomen direkt in den Zellen wirken, in denen sie exprimiert werden. Zum anderen bedingen die Gonosomen die Entwicklung der Gonaden, die die Geschlechtshormone bilden.

Im Rahmen einer bildgebenden Studie zur Geschlechtsidentität zeigten sich markante Unterschiede zwischen männlichen, weiblichen und transsexuellen Studienteilnehmern im Hinblick auf die Mikrostruktur der weißen Hirnsubstanz. Die Faserverläufe und damit die Struktur der Nervenverbindungen wiesen deutliche Unterschiede auf, bei denen die Ergebnisse der Transgenderpersonen zwischen denen von Männern und Frauen lagen. Dieselbe Studie lieferte Hinweise auf einen engen Zusammenhang zwischen den Faserverläufen und den Blutwerten von Geschlechtshormonen. Diese Befunde stützen die Annahme eines Einflusses der Geschlechtshormone auf die embryonale und frühkindliche Hirnentwicklung.

Das Gehirn ist ein sehr aktives Organ mit einem besonders hohen Energiebedarf. Es macht beim Erwachsenen etwa 2 % der Körpermasse aus, verbraucht mit etwa 20 Watt etwa 20 % des Grundumsatzes, beim Neugeborenen 50 %. Energie gewinnt es aus der aeroben Verbrennung von Glucose, aus Laktat und Ketonkörpern. Glucose kann nicht vollständig durch die anderen Energieträger ersetzt werden. Säuglingsgehirne können unmittelbar nach der Geburt zu einem ganz erheblichen Anteil Ketonkörper zur Energiegewinnung nutzen. Einige Zeit nach Umstellung der Ernährung des Kleinkindes auf kohlenhydratreiche Nahrung wird die dafür erforderliche Enzymproduktion wieder reduziert oder ganz abgebaut und die Fähigkeit zur Ketose (zur Nutzung von Ketonkörpern für die Energiegewinnung) geht wieder verloren. Das Verhalten des Blutglucosespiegels im Hungerstoffwechsel lässt vermuten, dass ein vollständig ketolysefähiges Gehirn Ketonkörper "vorrangig" vor der Glucose verarbeitet, selbst bei ausreichender Glucosezufuhr über das Blut.

90 % der Leistung benötigt die Natriumpumpe, größtenteils im Zusammenhang mit Aktionspotentialen. Da das Gehirn nur geringe, arealabhängige Speicherkapazitäten für Energie besitzt, führt ein Ausfall der Sauerstoff- oder Glucoseversorgung bereits nach zehn Sekunden zu einem Funktionsausfall (Synkope, Ohnmacht) und nach wenigen Minuten zu spezifischen Hirnschäden. Die geringen, auf den ersten Blick evolutionär unverständlichen Reservoirs werden manchmal durch Platzmangel erklärt. Gemäß einer anderen – evolutionären – Erklärung wich die Ernährungsweise der Menschen in der Altsteinzeit sehr stark von der heutigen Zivilisationskost ab, wodurch die Ketolysefähigkeit der damaligen Gehirne zu jedem Zeitpunkt auf natürliche Weise erhalten blieb. Dies wird so erklärt, dass der menschliche Organismus zwar zu viel aus Lebensmitteln aufgenommene Energie letztlich in den Körperfettdepots speichert - bei einer 70 kg schweren, gesunden, schlanken Person liegen 85 % der verwertbaren Körperenergien als Körperfett vor, 14,5 % als Proteine und nur 0,5 % als Kohlenhydrate - aus Fett jedoch kaum noch Glukose herstellen kann: Anteilsmäßig nur noch 6 % aus dem Glycerin der Triglyceride, in deren Form Fett im Organismus gespeichert wird. Einige Wissenschaftler nehmen an, dass die fettreichere Ernährung in der Altsteinzeit zum Wachstum des Gehirns des Menschen beitrug.

Mit der natürlichen Fähigkeit von menschlichen Gehirnen zur Ketolyse begründet sich die Wirksamkeit der ketogenen Diät bei Epilepsie, GLUT1-Defizit-Syndrom und anderen zerebralen Erkrankungen und der Hungerstoffwechsel.

Seit 1994 ist bekannt, dass die Nervenzellen über die Astrozyten bei Bedarf eine genau bemessene Energiemenge aus dem Blut erhalten, es ist der aktive Vorgang „Energy on Demand“. Die bedarfsabhängige Regulierung der Blutversorgung von Hirnarealen wird als "Neurovaskuläre Kopplung" bezeichnet. 1998 bis 2004 entwickelte Achim Peters die Selfish-Brain-Theorie, wonach das menschliche Gehirn bei der Regelung der Energieversorgung im Organismus vorrangig den eigenen, vergleichsweise hohen Bedarf deckt. Gemäß einer anderen Erklärung trifft dies jedoch nur für Gehirne zu, die aufgrund langjähriger Anwendung kohlenhydrat- und kalorienreicher Ernährungsweisen keine Ketonkörper mehr zur Energiegewinnung nutzen können. Diese sind also nicht mehr ketolysefähig. Solche Gehirne seien nicht mehr auf natürliche Weise am Fettstoffwechsel angeschlossen und müssen folglich ihren gesamten Energiebedarf über den viel leistungsschwächeren Kohlenhydratstoffwechsel mit seinen äußerst geringen Energiereserven decken.

Durch den ungewöhnlich hohen durchschnittlichen Stoffwechsel im Gehirn besteht auch ein ungewöhnlich hoher Bedarf an biochemischer Abfallbeseitigung. Diese ist hier noch zusätzlich deshalb von erhöhter Bedeutung, da manche Stoffe, insbesondere fehlgefaltete Proteine, typische Gefährdungen des Gehirns beinhalten.

Erschwert wird die Abfallentsorgung im Gehirn durch die Filtersysteme der Blut-Hirn-Schranke und der Blut-Liquor-Schranke sowie die Aussperrung des lymphatischen Systems. Letzteres reicht von außen nur bis in die Hirnhaut.
Obwohl es schon seit den 1980er Jahren konkrete Anzeichen für die Existenz eines speziellen Ausschwemmungssystems im Gehirn gab, wurde es erst 2012 mit Hilfe neuartiger Nachweismethoden als eigenständiges internes Kreislaufsystem entdeckt. In Anlehnung an das lymphatische System und wegen der entscheidenden Rolle der Glia (Stützzellen) wurde es Glymphatisches System genannt.

Durch sehr enge Gefäßräume rund um die Außenwand von Adern, den so genannten "perivaskulären Raum" "(Spatium perivasculare)", gelangt ein kleiner Teil der Gehirn-Rückenmarks-Flüssigkeit (Liquor cerebrospinalis) aus dem Zwischenraum zwischen Schädeldecke und Gehirn (Subarachnoidalraum oder "äußerer Liquorraum") in alle Bereiche des Gehirns, wird mit Hilfe der Glia dort verteilt und fließt am Ende – unter Mitnahme von Abfallstoffen – wieder ab zur Gehirnhaut und zum lymphatischen System außerhalb des Gehirns.

Oft werden Vergleiche zwischen der Leistungsfähigkeit eines Computers und der des menschlichen Gehirns angestellt. Seit das Gehirn als Sitz kognitiver Leistung erkannt wurde, wurde es in der Literatur immer mit dem komplexesten verfügbaren technischen Apparat verglichen (Dampfmaschine, Telegraph). So wurde versucht, aus der Funktionsweise von Computern auf die des Gehirns zu schließen. Mittlerweile besteht das Bemühen in der Computational Neuroscience und der bionischen Neuroinformatik, die Funktionsweise des Gehirns teilweise auf Computern nachzubilden oder dadurch auf neue Ideen zur „intelligenten“ Informationsverarbeitung zu kommen (siehe Blue Brain). Es ergibt sich die Perspektive, dass das Gehirn als Struktur für Denk- und Wissensproduktion eine Architektur liefert, die sich zur Nachahmung empfiehlt. Künstliche neuronale Netzwerke haben sich bereits bei der Organisation künstlicher Intelligenzprozesse etabliert.

Bei Vergleichen mit modernen Computern zeigt sich die Leistungsfähigkeit des menschlichen Gehirns. Während das Gehirn etwa 10 analoge Rechenoperationen pro Sekunde schafft und dabei etwa 15 bis 20 Watt Leistung benötigt, schafft der Supercomputer BlueGene/L von IBM bis zu 3,6·10 Gleitkommaoperationen pro Sekunde mit doppelter Genauigkeit, wozu jedoch etwa 1,2 Megawatt benötigt werden. Intels erster Teraflop-Chip Prototyp „Terascale“ mit 80 Prozessorkernen schafft hingegen etwa 10 Gleitkommaoperationen mit einfacher Genauigkeit bei 85 Watt (oder 2·10 Gleitkommaoperationen bei 190 Watt und 6,26 GHz), was immer noch dem 50- bis 5000-fachen Energiebedarf entspricht. Zwar erreichen moderne 3D-Grafikkarten vergleichbare Werte bei geringerem elektrischen Leistungsbedarf, Grafikchips sind jedoch stärker auf bestimmte Rechenvorgänge spezialisiert.

Es ist allerdings zu beachten, dass die hohe Rechenleistung des Gehirns vor allem durch seine vielen parallelen Verbindungen (Konnektivität) und nicht durch eine hohe Geschwindigkeit bei den einzelnen Rechenvorgängen (Taktfrequenz) erzielt wird. Künstliche Neuronen arbeiten 10-mal schneller als Neuronen des menschlichen Gehirns.

Zusätzlich zur Parallelisierung stellt ein neuronales Netzwerk gleichzeitig eine Speicher- und eine Verarbeitungslogik dar, während diese bei Computern, die auf der Von-Neumann-Architektur basieren, getrennt sind. Dies bewirkt, dass in einem einfachen neuronalen Netzwerk mit jedem Taktzyklus der gesamte Speicher aktualisiert wird, während ein Computer den Inhalt des Speichers schrittweise aktualisieren muss.

Rechenvorgänge, die auf einem Computer effizient ablaufen, sind meistens nicht effizient in einem neuronalen Netzwerk abbildbar und umgekehrt. Aufgrund der Ineffizienz bestehender Computerarchitekturen für bestimmte Aufgaben, wie beim Sehen, werden neuronale Netzwerke, wie dasjenige des Neocortex, durch Neuromorphing nachgebildet.

Im März 2009 bildeten künstliche neuronale Netzwerke im Rahmen des FACETS-Projekts 200.000 künstliche Neuronen mit 50 Millionen künstlichen Synapsen auf einem einzelnen 8 Zoll (20,32 cm Diagonale) großen Computerchip ab. Im Juli 2014 stellte IBM TrueNorth vor, welcher 1 Million Neuronen und 256 Millionen Synapsen auf einem Chip mit einer TDP von 70 mW, oder 16 Millionen Neuronen mit 4 Milliarden Synapsen in einem einzelnen Rack integriert.

Die Ansicht, das Gehirn als ein „Hypothesengenie“ oder eine „Vorhersagemaschine“ zu sehen, hatte bereits Hermann von Helmholtz, da andere Ansätze, das Gehirn künstlich nachzuempfinden, zu bisher unlösbaren Probleme führten und scheiterten. Der Ansatz geht davon aus, dass das Gehirn Hypothesen bildet und alle Eindrücke und Wahrnehmungen in die gespeicherten Muster einbaut und vergleicht. Wenn das Wahrgenommene nicht mehr auf die einzelne Hypothese passt, wird diese verworfen und nach Bedarf eine neue erstellt. Dies zeige sich klassisch bei der Interpretation von Kippfiguren.

Während das Gehirn einer Ratte etwa 200 Millionen Neuronen enthält, besitzt das eines Menschen neueren Untersuchungen zufolge durchschnittlich etwa 86 Milliarden Nervenzellen. Davon liegen etwa 16 Milliarden Neuronen in der Großhirnrinde "(Cortex cerebri)", etwa 69 Milliarden im Kleinhirn "(Cerebellum)" und rund 1 Milliarde in den restlichen Hirnregionen (von Hirnstamm, Zwischenhirn und Basalganglien). 

Miteinander verbunden sind Neuronen über Synapsen, im menschlichen Hirn geschätzt rund 100 Billionen, sodass durchschnittlich eine Nervenzelle mit 1000 anderen verbunden wäre und von jedem anderen Neuron aus in höchstens vier Schritten erreicht werden könnte. Doch gibt es lokal deutliche Abweichungen von diesem Mittelwert, denn nicht die Dichte, sondern das Muster von neuronalen Verknüpfungen ist für neurale Funktionen entscheidend. Ein häufiges Organisationsprinzip des Gehirns ist die Abbildung von Nachbarschaftsverhältnissen: was nebeneinander im Körper liegt, wird in Hirnarealen oft nebeneinander repräsentiert (Somatotopie).

Obwohl ausschließlich die Nervenzellen Erregungen als neuronale Impulse leiten und an Synapsen über Neurotransmitter als Signal weitergeben, spielen die sie umgebenden Gliazellen dabei keine unwesentliche Rolle. Die insgesamt etwa ebenso häufigen, meist kleineren Gliazellen ermöglichen Nervenzellen eine rasche Erregungsleitung und störungsfreie Signalübertragung, nehmen ausgeschüttete Botenstoffe auf, sorgen für die Bereitstellung von Nährstoffen und sind an den physiologischen Barrieren der Blut-Hirn- und der Blut-Liquor-Schranke beteiligt. Im sich entwickelnden Gehirn, und in sich weiterentwickelnden Hirnregionen, nehmen sie Einfluss auf die Ausbildung, Stabilität und Gewichtung der synaptischen Verbindungen zwischen Neuronen; bei Schädigungen peripherer Nerven bilden sie eine zur Wiederherstellung nötige Leitstruktur.

Die Konnektom-Forschung hat das Ziel, alle Verbindungen zwischen den Neuronen zu kartieren.


US-Präsident Barack Obama hat zu Beginn seiner zweiten Amtszeit Planungen für ein sehr großes Forschungsprojekt namens Brain Activity Map Project bekanntgegeben: Das menschliche Gehirn soll komplett kartiert werden. Dies wäre das größte wissenschaftliche Vorhaben seit vielen Jahren (das letzte war das Human Genome Project). Experten hoffen auf Therapien gegen Alzheimer-Krankheit und Parkinson sowie auf Erkenntnisse über menschliches Denken und Fühlen. Erste Ansätze wurden im Juli 2012 in der Fachzeitschrift "Neuron" veröffentlicht.<ref name="DOI10.1016/j.neuron.2012.06.006">A. Paul Alivisatos, Miyoung Chun, George M. Church, Ralph J. Greenspan, Michael L. Roukes, Rafael Yuste: "The Brain Activity Map Project and the Challenge of Functional Connectomics." In: "Neuron." Band 74, 2012, S. 970–974, .</ref>

Das US-Projekt ist nicht mit dem Human Brain Project zu verwechseln, das im Februar 2013 durch die EU gestartet wurde. Eine Jury hatte die Erforschung des Gehirns ebenfalls als ein Schlüsselprojekt der Zukunft ausgewählt; gefördert wird es mit einer Milliarde Euro.

2008 wurden auf dem Gelände der University of York (England) die Überreste eines 2500 Jahre alten menschlichen Schädels gefunden, dessen Gehirn überwiegend erhalten ist. Forscher vermuten, dass das Gehirn des wahrscheinlich 26–45 Jahre alten Mannes unter anderem deswegen bis heute so gut erhalten blieb, weil der Kopf – ein Körper wurde nicht gefunden – seinerzeit unmittelbar nach dem Tod in nasser Lehmerde begraben wurde. Eine vollständige Klärung, warum das Gehirn nicht schon längst zerfallen ist, konnte bislang nicht gefunden werden.

Hirn als Rohstoff findet Verwendung bei der Fettgerbung.

Neurolinguistik untersucht wie die Sprache durch das Gehirn dargestellt, aufgearbeitet und erlernt wird.





</doc>
<doc id="8372" url="https://de.wikipedia.org/wiki?curid=8372" title="Schiff">
Schiff

Ein Schiff, in einer kleineren Form auch "Boot" genannt, ist ein Wasserfahrzeug, das nach dem archimedischen Prinzip schwimmt.
Der Schiffbau findet auf Werften statt. Nach Fertigstellung des Rumpfes wird das Schiff mit dem Stapellauf zu Wasser gelassen, erst dann wird es endgültig ausgerüstet. Die erste Fahrt eines Schiffes wird als Jungfernfahrt bezeichnet. Schiffe werden in der Regel in sogenannten Docks repariert.

Schiffe sind das wichtigste Transportmittel für Massengut. Die Bedeutung der Passagierschifffahrt hat sich aufgrund des zunehmenden Flugverkehrs als Verkehrsmittel für Langstrecken insbesondere in den Bereichen der Erlebnisreisen und Kreuzfahrten entwickelt.

Schiffe zur Seeschifffahrt werden auch als „Seeschiffe“ bezeichnet; Schiffe zur Binnenschifffahrt werden „Binnenschiffe“ genannt.

Schiffe werden in verschiedene Typen oder Klassen unterteilt, ohne dass es allerdings eine
einheitliche Herangehensweise gibt. So kann nach Einsatzgebiet, Verwendungszweck, Antrieb, Material oder Rumpfbau typisiert werden.
Eine genauere Beschreibung entsteht durch die Angabe der Schiffsmaße. Hierunter werden Begriffe wie die Verdrängung, die Tragfähigkeit, Raumgehalt, Brutto- und Nettoraumzahl (BRZ, NRZ), Tiefgang, die verschiedenen Längenangaben oder besondere Formkoeffizienten subsumiert.
Spätestens mit der Notwendigkeit für Menschengruppen, im Rahmen der Nahrungssuche oder auf der Suche nach Lebensraum, Wasser über längere Wege überqueren zu müssen, suchte der Mensch nach entsprechenden Transportmitteln. Es wird angenommen, dass bereits vor über 50.000 Jahren entsprechende Fahrzeuge bekannt waren, das erste nachweisbare Fahrzeug muss jedoch auf ca. 6500 v. Chr. datiert werden. Zunächst waren diese Fahrzeuge einfache behauene Baumstämme, später wurden diese immer weiter fortentwickelt. Eine Grenze hinsichtlich der Größe war aufgrund der Eigenschaften des Baumaterials Holz zunächst mit den Klippern erreicht. Erst mit der Nutzung von Stahl konnten dann größere Schiffe gebaut werden - mit einer Länge, die wie bei den Schiffen der UASC A18-Klasse heute bis zu 400 m betragen kann.

Schiffe sind derzeit das wichtigste Transportmittel für Massengut. Stückgut wird dabei heutzutage vor allem in Containern auf Containerschiffen transportiert. Das bisher größte je gebaute Schiff der Welt ist das Kranschiff Pieter Schelte mit einer Vermessung von 403.342 BRZ (Bruttoraumzahl).

Seit spätestens den 1960er Jahren sieht sich die Passagierschifffahrt zunehmend einem Konkurrenzkampf mit dem Flugverkehr gegenüber und verlagerte sich aus diesem Grund vom reinen Transportmittel vermehrt hin zum Verkehrsmittel, insbesondere im Bereich Erlebnisreisen/Kreuzfahrten.

Schiffsnamen sind in Nordeuropa und Nordamerika unabhängig vom eigentlichen Genus des Namens im Allgemeinen weiblich, insbesondere wenn es sich um Schiffe handelt, die nach Personen oder geografischen Begriffen benannt sind (die „Eisenhower“, die „Hamburg“). Schiffe, die nach einem Ausdruck benannt sind, der gewöhnlich einen Artikel bei sich hat (zum Beispiel Tiere, astronomische Begriffe), behalten dessen Genus meist bei (der „Widder“, das „Frettchen“), es kann jedoch auch die weibliche Form verwandt werden (der/die „Pfeil“). In romanischen und slawischen Sprachen wird das Genus des Namens beibehalten. Die österreichische Seemannssprache (bis 1918) lehnt(e) sich daran an – es gab also den „Szent Istvan“, die „Kaiserin Elisabeth“, den „Sankt Georg“ und die „Wien“ " (von: die Stadt)".

Schiffsnamen wird zum Beispiel in der Literatur häufig ein Präfix wie MS oder SV vorangestellt, der eine grobe Kategorisierung des bezeichneten Schiffs ermöglicht (MS: motor ship, SV: sailing vessel). Beginnt die Bezeichnung eines Schiffsnamens mit einem solchen Präfix, wird der Artikel meist vermieden.

Die Geschwindigkeit von Schiffen wird gemeinhin in Knoten (kn) angegeben. Ein Knoten entspricht einer Seemeile (sm) pro Stunde. Die Geschwindigkeit wurde ursprünglich mit einem Log (Messgerät) gemessen, das an einer Logleine über Bord geworfen wurde. Die Leine hatte in festen Abständen (üblicherweise alle ca. 7 m) Knoten. Der Messende zählte die Knoten, während sie ihm durch die Hand glitten. Die Zahl der gemessenen Knoten je Zeiteinheit (Messdauer waren ca. 14 Sekunden) ergab dann die Geschwindigkeit in Seemeilen pro Stunde. Daher rührt auch der Begriff „Knoten“ als Maßeinheit für die Schiffsgeschwindigkeit. Modernere Bauformen des Logs messen die Geschwindigkeit über die Umdrehungsgeschwindigkeit eines nachgeschleppten Propellers ("Patentlog"), eines am Schiffsboden befestigten Impellers oder mittels eines Staurohrs ("Staudrucklog", "Rohrlog").

Die maximale Geschwindigkeit eines Schiffes wird wesentlich von der Rumpfgeschwindigkeit bestimmt. Diese ist nichts anderes als die Ausbreitungsgeschwindigkeit des vom Schiff selbst erzeugten aus Bug- und Heckwelle bestehenden Wellensystems. Die Ausbreitungsgeschwindigkeit einer Welle in Wasser steigt mit ihrer Wellenlänge. Das Schiff ist also zwischen seiner Bug- und Heckwelle „gefangen“. Bei Schiffen mit normalem Verdrängerrumpf lässt sich die Geschwindigkeit auch mit erhöhter Motorleistung nicht über die Rumpfgeschwindigkeit steigern. Diese wird bestimmt durch die Länge, mit der das Schiff im Wasser liegt. Höhere Geschwindigkeiten lassen sich bei Schiffen mit Gleiter-Rumpf erzielen. Dabei wird durch die Motorleistung der Widerstand der Bugwelle überwunden, der Bug des Schiffes steigt dabei an. Auch moderne Verdrängerschiffe erreichen bei raumem Wind unter Segel Geschwindigkeiten, die geringfügig über der theoretischen Rumpfgeschwindigkeit liegen können. Grund hierfür sind die modernen, glatten, lang gestreckten Rümpfe, die teilweise kaum noch Bugwellen erzeugen.

Da die Geschwindigkeit außerdem noch erheblich von Wind und Strömung sowie der variierenden Beladung (durch Treibstoffverbrauch) abhängt, wird die Schiffsgeschwindigkeit häufig in größeren Einheiten als der Stundengeschwindigkeit angegeben. Ein Etmal ist die von einem Schiff an einem Tag von 12:00 Uhr bis zum nächsten Tag um 12:00 Uhr zurückgelegte Wegstrecke. Das Etmal wird gemeinhin in Seemeilen (sm) angegeben.

In den Zeiten, als die Passagierschifffahrt das alleinige Übersee-Massenverkehrsmittel war, wurde dem schnellsten Passagierdampfer jeweils das symbolische Blaue Band, später ein Pokal, zuerkannt. Maßstab war die Durchschnittsgeschwindigkeit während einer kompletten Atlantiküberquerung von einem europäischen zu einem nordamerikanischen Hafen.

Exemplarische Geschwindigkeiten einiger Schiffe und Boote:
Herkömmliche Frachtschiffe werden aus wirtschaftlichen Gründen meist nicht wesentlich älter als etwa 30 Jahre. Übersteigen die nötigen Investitionen in den Erhalt der Schiffssubstanz und der eingebauten Technik den eingefahrenen Ertrag, werden sie abgebrochen. Bei entsprechender Bauweise und Pflege können Schiffe eine sehr viel höhere Lebensdauer erreichen. Ein Beispiel war das bis 2010 älteste immer noch fahrende Hochsee-Fahrgastschiff, die 1914 gebaute "Doulos".

Noch älter sind die am 2. August 1856 in Betrieb genommene "Skibladner" auf dem norwegischen Mjösa und die auf schwedischen Binnengewässern aktive "Juno" aus dem Jahr 1874. Die meisten der heute als „alt“ bezeichneten Schiffe sind aus Holz. Eines der höchsten bekannten Lebensalter erreichte die holsteinische Jagt "De fire Brødre". Sie wurde 1794 gebaut und transportierte über 150 Jahre lang Lasten. Das englische Schiff "Besty Canes" existierte bereits 1688 als Jacht von König Wilhelm III. und erlitt 1827 Schiffbruch. Sie wurde nachweislich 139 Jahre alt. 113 Jahre erreichte die englische "Royal William", die am 16. März 1700 auslief und 1813 demontiert wurde. Im November 2004 ist die "Cutty Sark" 135 Jahre alt geworden. Sie ist der einzige verbliebene Klipper und befindet sich im Trockendock zu Greenwich, London, England.
28 Jahre älter ist die "Charles W. Morgan" von 1841. Sie ist das einzig erhaltene Walfang-Segelschiff, benannt nach Charles Waln Morgan, ihrem Haupteigner. Ursprünglich als Vollschiff in New Bedford, Massachusetts, aus Holz erbaut, wurde sie 1867 zur Bark umgeriggt und stand 80 Jahre in Dienst. Heute ist sie als Museumsschiff in Mystic, Connecticut, Vereinigte Staaten zu besichtigen. Rekordhalter ist die britische "HMS Victory" (Stapellauf 1765), Admiral Lord Nelsons Flaggschiff mit 239 Jahren (2004), gefolgt von der US-amerikanischen Fregatte "USS Constitution" in Boston von 1797 mit 207 Jahren (2004). Da die "HMS Victory" in Portsmouth im Trockendock liegt, ist die "USS Constitution" das älteste noch seetüchtige Schiff der Welt.

Es wurden auch, jedoch selten, Orden und Auszeichnungen für Schiffe vergeben. Dem Kanonenboot "SMS Iltis" wurde das „Pour le Mérite“ von Kaiser Wilhelm II. (1900) wegen seines Einsatzes während des Boxeraufstandes verliehen.
Das „Eiserne Kreuz“ erhielten die "Emden" und das U-Boot "SM U 9". Zur "SMS Emden" gibt es noch eine Besonderheit: Die Besatzungsangehörigen bekamen das Recht verliehen, zu ihrem Hausnamen/Familiennamen den Namen „Emden“ als Zusatz zu führen. Beispiel: Der Matrose Meyer hieß jetzt Meyer-Emden.


</doc>
<doc id="8373" url="https://de.wikipedia.org/wiki?curid=8373" title="Parlament">
Parlament

Das Parlament (von altfranz. "parlement" ‚Unterredung‘; ‚reden‘) ist die politische Volksvertretung, die in der Regel aus einer oder zwei Kammern bzw. Häusern (Einkammersystem oder Zweikammersystem) besteht, aber auch aus drei Kammern (Dreikammersystem) konstituiert sein kann. Zumeist versteht man unter Parlament die in demokratischen Staaten vom Staatsvolk gewählte und legitimierte Vertretungskörperschaft, jedoch gibt es auch in Staaten mit nicht-demokratischem politischen System Parlamente. 

Jeder demokratisch verfasste Nationalstaat (Einheitsstaaten oder Bundesstaaten) besitzt ein Parlament auf nationalstaatlicher Ebene bzw. Bundesebene. In Bundesstaaten gibt es Parlamente auch jeweils zusätzlich auf der Ebene der Gliedstaaten, da diese Staatsqualität und somit eine beschränkte, geteilte staatsrechtliche Souveränität mit eigenem politischen System (Exekutive, Legislative und Judikative) besitzen.

Im übertragenen und weiteren Sinne werden auch andere politische Versammlungen mit dem Begriff bezeichnet. Diese Versammlungen stellen jedoch keine unmittelbar oder nur eingeschränkt vom Volk legitimierten Volksvertretungen dar:

In einer Demokratie werden die Vertreter eines Parlaments durch Wahlen bestimmt, in anderen Regierungssystemen finden auch Ernennungen statt.

In demokratischen Staaten übt das Parlament außer der Gesetzgebung auch das Budgetrecht und die Kontrolle der Regierung aus. Abgeordnete haben gegenüber der Regierung und einzelnen Ministern das Recht auf Auskunft und gegebenenfalls zum Misstrauensantrag. Die Regelungen hierzu sind in der Verfassung des jeweiligen Staates und in der parlamentarischen Geschäftsordnung niedergelegt.

Etwa 30 bis 40 Prozent der Parlamente weltweit bestehen aus zwei Kammern; die Mitglieder der kleineren Kammern werden vielfach nicht direkt gewählt, sondern von Gliedstaaten entsandt. Wichtige Organe sind Parlamentspräsident und Stellvertreter, Fraktions-Vorsitzende der Parlamentsparteien und die themenbezogenen Ausschüsse, in denen die Gesetzentwürfe vorbereitet werden.

Hinsichtlich der Arbeitsweise werden sogenannte Arbeits- und Redeparlamente unterschieden:
Als Parlament im weiteren Sinne werden zum Teil auch die Delegiertenversammlungen parlamentarischer Versammlungen bezeichnet. Vielfach haben auch Parteitage die Funktion eines „Parteiparlaments“, wenngleich ihre Delegierten nicht immer gewählt, sondern auch ernannt oder nominiert werden können.

Parlamente, deren Mitglieder nur ehrenamtlich oder nebenberuflich tätig sind, werden als Feierabendparlamente bezeichnet.



Zu der obigen Liste sind folgende Erklärungen besonders hervorzuheben:


Im Frankreich des Ancien Régime wurde mit "Parlement" ein Gerichtshof bezeichnet, der als eine der ältesten Institutionen des Reiches galt. Das Parlament konnte die königliche Rechtsprechung bestätigen oder auch korrigieren, in dem es, vor allem im 18. Jahrhundert, ein Gesetz zur „remontrance“ an den König zurückverwies. Die verschiedenen Kammern der Parlamente wurden nach ihren Jurisdiktionsbereichen unterschieden: „grande chambre“, „chambre des enquêtes“, „chambre de requêtes“, „tournelle criminelle“ und auch die „chambre de l'édit“ (bis 1685, siehe Widerrufung des Ediktes von Nantes). Besonders im 18. Jahrhundert galten die Parlamente als ein Hort der Opposition von Teilen des Adels („noblesse d'épée“ wie auch der „noblesse de robe“) als auch von Teilen des dritten Standes gegen einen als despotisch empfundenen Absolutismus, zu dem sich die jansenistische Opposition gegen die Jesuiten sowie die gallikanische Opposition gegen die ultramontane Kirche gesellte.

Im Königreich Frankreich wurden neben dem ersten und wichtigsten Parlament von Paris noch die Parlamente von Toulouse (1303), Grenoble (1453), Rouen (1499), Aix (1502), Rennes (1533), Pau (1620), Metz (1633), Douai (1686), Dôle (1676), Besançon (1676) und zuletzt Nancy (1775) eingerichtet.

Das aktuelle französische Parlament besteht seit 1958 aus Senat und Nationalversammlung.

Das englische Parlament entwickelte sich aus dem adligen Beraterkreis der angelsächsischen Könige, dem "witan". In ihm waren nicht nur persönliche Vertrauensleute des Königs vertreten, sondern sowohl Hoch- als auch Landadlige und hohe geistliche Würdenträger, die aufgrund ihrer Macht einen Anspruch auf die Mitgliedschaft besaßen. Die Beratung des Königs durch den "witan" wurde nicht nur als Pflicht seiner Mitglieder, sondern auch als ihr Recht verstanden. Der König war also verpflichtet, den Rat einzuholen. Unter den frühen Normannenkönigen wurden die Parlamente nur jeweils nach Bedarf einberufen, wenn wichtige Themen zu beraten waren (diese Treffen fielen mit den christlichen Festen Ostern und Weihnachten zusammen). Die Geschichte des angelsächsischen "Witan" endete mit der Invasion der Normannen von 1066, die ihn durch eine curia regis (Gerichtshof des Königs) ersetzten; jedoch war diese noch bis ins 12. Jahrhundert auch unter den traditionellen Namen "Witan" oder "Witenagemot" bekannt.

Am 20. Januar 1265 lud Simon V. de Montfort, der gegen seinen Schwager Heinrich III. rebellierte, seine Anhänger ohne vorherige königliche Zustimmung zu einem Parlament. Neben 120 Kirchenmännern und 23 Earls wurden auch je zwei Ritter aus jeder Grafschaft und je zwei Bürger aus jedem Borough eingeladen – das erste Mal, dass Bürgerliche an einem englischen Parlament teilnahmen. De Montforts neue Regeln wurden 1295 durch Eduard I. mit dem "Model Parliament" formell bestätigt. Mit der Zeit entwickelte sich daraus das englische Parlament. Nach den Rosenkriegen im 15. Jahrhundert nahmen Selbstbewusstsein und Macht des Parlaments zu, ebenso die Zahl der Mitglieder. Das Parlament verstand sich nicht nur als Beratungs-, sondern zunehmend als Kontrollorgan dem König gegenüber. Zudem beanspruchte es die Funktion des obersten Gerichtshofs und vor allem das Recht, Steuern zu bewilligen. Auch die Einberufung war nicht mehr allein vom Willen des Königs abhängig. Die Parlamentsmitglieder konnten zunehmend auch auf eigene Initiative zusammentreten. Allerdings wurde das englische Parlament dadurch auch mehr und mehr zum Schauplatz von Auseinandersetzungen zwischen den Adelsgruppen des Landes.

Das allgemein-polnische Parlament – der Sejm walny – entstand Ende des 14. Jahrhunderts. Es setzte sich aus Vertretern der Landtage – Sejmiki – zusammen, welche wiederum von den adligen Bevölkerungsgruppen gewählt wurden. Der Adel machte in der polnisch-litauischen Rzeczpospolita 10–12 % der Bevölkerung aus. Der "Sejm walny" kam einmal pro Jahr sowie zur Königswahl zusammen. Mit den Privilegien von 1454 wuchs die Rolle des "Sejm walny" und er rang dem König immer mehr Rechte zugunsten des Adels ab. Mit der Verfassung Nihil Novi – „Nichts über uns ohne uns“ – von 1505 wurde die Legislative auf den "Sejm walny" übertragen, der König durfte ohne dessen ausdrückliche Zustimmung keine Gesetze mehr erlassen. In diese Zeit fällt auch die Umwandlung des "Sejm walny" in drei Kammern – „drei Stände“ – den Sejm (Abgeordnetenhaus), den Senat (Königsrat) und den König. Eine weitgehende Reform erfuhr der "Sejm walny" mit der Lubliner Union von 1569 und der Warschauer Verfassung von 1572, die insbesondere die Gleichstellung der Konfessionen und die Religionsfreiheit sicherte. Zudem wurde festgelegt, dass der Sejm mindestens einmal in zwei Jahren zusammenkommen soll. Ende des 16. Jahrhunderts wurde der Tagungsort des "Sejm walny" von Petrikau nach Warschau verlegt. Ab 1673 tagte er auch jedes dritte Mal in Grodno. 1654 wurde das "Liberum Veto" eingeführt, das die Einstimmigkeit der Beschlüsse vorschrieb. Stimmte ein Abgeordneter gegen ein Vorhaben, dann musste weiterverhandelt werden. Der Vierjährige Sejm, der von 1788 bis 1792 im Warschauer Königsschloss tagte, erließ 1791 die Verfassung vom 3. Mai, die erste aufgeklärte Verfassung Europas und nach den USA zweite in der Welt. Mit der dritten Teilung Polens wurde der "Sejm walny" aufgelöst. Im Großherzogtum Warschau (1807–1814) und am Anfang des russischen Königreichs Polen (1815–1832) bestand ein polnischer Sejm in Warschau. Nach 1867 wurde in Galizien ein Landtag in Lemberg eingerichtet. Erst wieder in der Zweiten Polnischen Republik wurde ein gesamtpolnischer Sejm mit zwei Kammern (Sejm und Senat) gebildet. In der Volksrepublik gab es nur einen Sejm mit einer Kammer. 1989 wurde der Senat wieder eingeführt.

Das erste demokratisch gewählte deutsche Parlament war die Frankfurter Nationalversammlung von 1848 in der Paulskirche. Hier wurde der Beschluss zur Paulskirchenverfassung gefasst, die allerdings nie umgesetzt wurde.






</doc>
<doc id="8374" url="https://de.wikipedia.org/wiki?curid=8374" title="Beryll">
Beryll

Beryll ist ein häufig vorkommendes Mineral aus der Mineralklasse der „Silikate und Germanate“. Es kristallisiert im hexagonalen Kristallsystem mit der chemischen Zusammensetzung AlBe[SiO], ist also ein Aluminium-Beryllium-Silikat. Strukturell gehört es zu den Ringsilikaten.

Beryll entwickelt vorwiegend große Kristalle mit tafeligem oder prismatischem bis säuligem Habitus und glas- bis fettähnlichem Glanz auf den Oberflächen. Die größten bekannten Kristalle waren bis zu 18 Meter lang und 180 Tonnen schwer. Beryll tritt aber auch in Form körniger oder massiger Aggregate auf, die leicht mit Quarz verwechselt werden. In reiner Form ist Beryll farblos und durchsichtig und wird in dieser Form als "Goshenit" bezeichnet. Durch vielfache Lichtbrechung aufgrund von polykristalliner Ausbildung kann er aber auch weiß erscheinen und durch Fremdbeimengungen verschiedene Farben annehmen, wobei die Transparenz entsprechend abnehmen kann.

Aufgrund seiner hohen Mohshärte von 7,5 bis 8 und seiner oft gut ausgebildeten Kristalle wird Beryll vorwiegend zu Schmucksteinen verarbeitet, wobei vor allem der blaue Aquamarin, der grüne Smaragd und der hellgelbe bis grünlichgelbe Goldberyll bekannt sind.

Das Fremdwort "Beryll" wurde aus dem lateinischen ' oder ' entlehnt und geht über das griechische [""] auf das mittelindische (Prakrit) "veruliya" und altindische (Sanskrit) "vaiḍūrya" zurück. Letzteres leitet sich wiederum wohl von einem dravidischen Ortsnamen "vēḷūr" ab.

Das lateinische ' wurde im Mittelalter als Oberbegriff für alle klaren Kristalle gebraucht. Über mittelhochdeutsch ' und "" entstand das Wort "Brille" („Augengläser“), da die ersten Linsen aus Kristall geschliffen wurden. Der feminine Singular "die Brille" beruht auf einer späteren Umdeutung der Pluralform "die b[e]rille" (Singular "der b[e]rille" = einzelnes Augenglas), nachdem zwei Augengläser üblich geworden waren.

Aus dem lateinischen ' leitet sich auch italienisch ' („glänzen, strahlen“) ab – und daraus französisch ', dessen Partizip ' („glänzend, strahlend“) den deutschen Fremdwörtern "Brillant" (ein speziell geschliffener Diamant) und "Brillanz" zugrunde liegt. Das englische ' hat dieselbe Herkunft. Nach der Aussprache wäre eigentlich auch im Deutschen die englische Schreibweise zu erwarten ('). Die Norm (z. B. "brillant", "Brillanz") richtet sich im Deutschen jedoch nach der französischen Herkunft, was in diesem Fall zu häufigen Rechtschreibfehlern führt.

Der Abbau der Beryll-Varietät Smaragd lässt sich bis ins 13. Jahrhundert v. Chr. nach Ägypten zurückverfolgen. Aber auch im präkolumbischen Südamerika wurde der Schmuckstein weiträumig gehandelt.

Bereits in der veralteten, aber teilweise noch gebräuchlichen 8. Auflage der Mineralsystematik nach Strunz gehörte Beryll zur Mineralklasse der „Silikate und Germanate“ und dort zur Abteilung der „Ringsilikate (Cyclosilikate)“, wo er als Namensgeber die „Beryllgruppe“ mit der System-Nr. "VIII/E.12" und den weiteren Mitgliedern Bazzit, Cordierit, Indialith, Pezzottait, Sekaninait und Stoppaniit bildete.

Die seit 2001 gültige und von der International Mineralogical Association (IMA) verwendete 9. Auflage der Strunz'schen Mineralsystematik ordnet den Beryll ebenfalls in die Abteilung der „Ringsilikate“ ein. Diese ist allerdings weiter unterteilt nach der Struktur der Silikatringe, so dass das Mineral entsprechend seinem Aufbau in der Unterabteilung „[SiO] Sechser-Einfachringe ohne inselartige, komplexe Anionen“ zu finden ist, wo es ebenfalls namensgebend die „Beryllgruppe“ mit der System-Nr. "9.CJ.05" und den weiteren Mitgliedern Bazzit, Indialith, Pezzottait und Stoppaniit bildet.

Auch die vorwiegend im englischen Sprachraum gebräuchliche Systematik der Minerale nach Dana ordnet den Beryll in die Klasse der „Silikate und Germanate“ und dort in die Abteilung der „Ringsilikate: Sechserringe“ ein. Hier ist er ebenfalls als Namensgeber der „Beryllgruppe“ mit der System-Nr. "61.01.01" und den weiteren Mitgliedern Bazzit, Indialith, Stoppaniit und Pezzottait innerhalb der Unterabteilung „“ zu finden.

In 100 % reiner Form, die allerdings nur synthetisch herzustellen ist, besteht Beryll aus rund 19 % Aluminiumoxid (AlO), 14 % Berylliumoxid (BeO) und 67 % Siliciumdioxid (SiO).

Natürlicher Beryll kann verschiedene Fremdbeimengungen enthalten wie unter anderem Rubidiumoxid (RbO) und Caesiumoxid (CsO). Auch Kristallwasser (HO, bis 3 %) sowie Argon und Helium können in den Kanälen der Ringsilikat-Struktur eingelagert sein.

Weitere mögliche Fremdbeimengungen sind unter anderem Lithium und Natrium sowie verschiedene Oxide und Hydroxide, Halogenide und/oder Fluoride.

Beryll kristallisiert hexagonal in der mit den Gitterparametern "a" = 9,22 Å und "c" = 9,20 Å sowie zwei Formeleinheiten pro Elementarzelle.

Die Kristallstruktur von Beryll besteht aus Sechser-Einfachringen mit der Strukturformel [SiO], die in Richtung der c-Achse konzentrisch übereinander geschichtet und jeweils um 30° verdreht sind. Aufgrund der konzentrischen Anordnung der Ringe bilden sich offene Kanäle mit einem Durchmesser von einigen Ångström. In diesen Hohlkanälen sind die verschiedenen Fremdbeimengungen austauschbar eingelagert.

Zwischen den Ringen befinden sich die Aluminium- und Beryllium-Ionen, wobei Aluminium von jeweils sechs und Beryllium von jeweils vier Sauerstoffionen umgeben ist. Man spricht daher auch von einer [6]- bzw. [4]-Koordination beim Aluminium bzw. Beryllium.

Die Kristallmorphologie von Beryll ist überwiegend einfach und in der Tracht gekennzeichnet durch das hexagonale Prisma {100} und dem abschließenden Pinakoid {0001}. Hinzu kommen gelegentlich hexagonal-dipyramidale Formen in erster und zweiter Stellung {111} und {101} sowie die dihexagonal-dipyramidale Vollform (Holoedrie).

Der Habitus kann kurz- bis langprismatisch sein, wobei die Prismenflächen oft längsgestreift sind. Gelegentlich finden sich auch stengelige und körnige bis derbe Massen.
Der Schmelzpunkt von Beryll beträgt 1650 °C.

Beryll ist gegenüber verschiedenen Säuren unempfindlich und nur sehr schwach löslich in Fluorwasserstoff (HF). Dagegen ist er empfindlich gegenüber Alkalischen Lösungen und daher unter anderem löslich in Natriumhydroxid (NaOH, "Natronlauge") und Kaliumhydroxid (KOH, "Kalilauge").

Die farblose Reinform von Beryll erhielt wie die Reinform von Quarz ("Bergkristall") eine eigenständige Bezeichnung und ist als "Goshenit" bekannt. Farblose Berylle sind allerdings sehr selten.

Weitaus bekannter sind als farbige Varietäten der grüne Smaragd, der blaue Aquamarin, der gelbe Goldberyll (auch "Heliodor") sowie der rosafarbene "Morganit", der nach dem New Yorker Bankier John Pierpont Morgan benannt wurde (nicht zu verwechseln mit "Mogánit" aus der Familie der Kieselsäuren).

Eine ebenfalls rosafarbene, cäsiumhaltige Beryllvarietät wird als "Vorobieffit", "Worobieffit" oder einfach "Caesium-Beryll" bezeichnet.

Sehr selten ist auch die Varietät Roter Beryll, dessen veraltete Bezeichnung "Bixbit" aufgrund der deutlichen Verwechslungsgefahr mit dem Mineral Bixbyit nach den Bestimmungen der CIBJO zu den unerwünschten Handelsnamen gehört.

Beryll bildet sich entweder magmatisch in Pegmatit und Granit oder hydrothermal in Greisen oder Quarz-Gängen. Auch metamorph gebildete Berylle sind gefunden worden, unter anderem in Gneis. Zudem kann es sekundär in Form von Seifenlagerstätten in Flusssedimenten angereichert sein.

Einige der vielen Fundorte sind unter anderem Minas Gerais und Rio Grande do Norte in Brasilien, Coscuez und Muzo in Kolumbien, Antsirabé in Madagaskar, Spitzkoppe in Namibia, Iveland in Norwegen, Habachtal in Österreich, Gilgit in Pakistan, Malyshevo und Murzinka im Ural in der Russischen Föderation, Adun-Chilon in Sibirien, sowie Keystone/South Dakota und Pala/Kalifornien in den USA

Beryll-Kristalle können außergewöhnlich groß werden. So sind im US-amerikanischen Bundesstaat Maine schon sechs Meter lange und eineinhalb Tonnen schwere Exemplare gefunden worden. Kristalle bis zu 177 Tonnen wurden in Namivo/Alto Ligonha in Mosambik gefunden.

Beryll ist die Hauptquelle für das giftige Leichtmetall Beryllium, das unter anderem in der Raumfahrttechnik als Bestandteil von Speziallegierungen eingesetzt wird. Mehr als 80 Prozent der Weltjahresproduktion stammen aus den USA. Zudem wurden im Mittelalter Berylle zu Linsen geschliffen, die als Brille verwendet wurden und dieser ihren Namen gaben.

Berylle aller Farbvarietäten werden bei guter Qualität zu Schmucksteinen verarbeitet. Der Smaragd wurde allerdings als eine der ersten Varietäten für diese Zwecke genutzt und in größeren Mengen abgebaut. Die ältesten Minen lassen sich auf etwa 1.300 v. Chr. datieren.

Klare Schmucksteine erhalten üblicherweise einen facettierten Schliff. Beim Schleifen ist jedoch der bei einigen Beryll-Varietäten deutliche Pleochroismus zu berücksichtigen.

Durchscheinende bzw. undurchsichtige Steine erhalten einen Cabochon-Schliff. Größere Mineralaggregate werden manchmal auch zu kunstgewerblichen Gegenständen verarbeitet.




</doc>
<doc id="8376" url="https://de.wikipedia.org/wiki?curid=8376" title="Bedeutung (Begriffsklärung)">
Bedeutung (Begriffsklärung)

Bedeutung bezeichnet:



</doc>
<doc id="8378" url="https://de.wikipedia.org/wiki?curid=8378" title="Arthur Fellig">
Arthur Fellig

Arthur "Weegee" Fellig, geboren als "Ascher Fellig" (* 12. Juni 1899 in Złoczów bei Lemberg, Galizien; † 26. Dezember 1968 in New York, USA), war ein amerikanischer Fotograf.

Als die Familie Fellig 1910 in die USA einwanderte und sich in der New Yorker East Side niederließ, wurde der Name des Sohnes von "Ascher" bzw. (anglisiert) "Usher" auf "Arthur" geändert. Um seine Familie zu unterstützen, verließ Arthur Fellig die dortige staatliche Schule mit 14 Jahren und arbeitete zuerst als Süßigkeitenverkäufer, danach als Straßenfotograf und Assistent bei einem Fotohändler. Mit 18 Jahren zog er aus dem Elternhaus aus und schlug sich mit Gelegenheitsarbeiten durch. Er übernachtete in Bahnhöfen und Obdachlosenasylen, bis er schließlich einen Job als Passbildfotograf fand. Mitte der 1920er Jahre begann er als Fotolaborant und Aushilfsreporter bei der Bildagentur "Acme Newspictures", die später in United Press International aufging. 1935 verließ er "Acme" und versuchte sich von nun an als selbstständiger Pressefotograf. Er spezialisierte sich auf Bilder zumeist nächtlicher Verkehrsunfälle, Brandkatastrophen und Gewalt. Ausgestattet mit Polizeifunk, war er oft noch vor den Einsatzkräften am Ort des Geschehens. Die Aufnahmen aus seiner 4×5″-Kamera wurden regelmäßig in allen großen Boulevardzeitungen veröffentlicht. Ab 1938 war er der erste Pressefotograf, der ganz offiziell mit Polizeifunk ausgestattet war. Während dieser 10-jährigen Tätigkeit hatte er sein Büro praktisch im "Manhattan Police Headquarters". Gegen Ende dieser gewalttätigen Epoche hatte er Exklusivverträge bei den Zeitschriften PM Daily und Vogue.

Zunächst war "Weegee" als Spitzname entstanden, weil "Fellig" aufgrund der außergewöhnlichen Herangehensweise, unter der seine Fotos entstanden, seinerzeit von Millionen von Zeitungslesern in New York als „Zaubermeister“ betrachtet wurde. Dabei verglich man ihn mit der Faszination des als „Zauber-Brett“ angesehenen Ouija, das ebenfalls in diesen Jahren in großen Kreisen der US-amerikanischen Bevölkerung sehr en vogue war. Die beiden Wörter des Begriffs, der sich aus dem französischen „oui“ (es bedeutet „ja“) und dem deutschen „ja“ gebildet hatte, wurden im Slang „wee“ und „gee“ ausgesprochen.

Sein Selbstbewusstsein war sehr ausgeprägt. Nachdem er "WEEGEE" als Pseudonym für sich angenommen hatte, begann er bald, seine Fotos mit "WEEGEE THE FAMOUS" ("Weegee der Berühmte") zu signieren. Sein Markenzeichen waren Fotos aus nächster Nähe, frontal und hart mit dem Blitz ausgeleuchtet. Wirklich berühmt wurde er damit jedoch zunächst nur in Pressekreisen. Das änderte sich mit der Publikation seines Buches "Naked City", in dem er seine Verbrechens- und Unfallfotos mit Aufnahmen der Armen und Obdachlosen kombinierte und das 1947 verfilmt wurde.

In den Bildunterschriften seiner Fotografien zeigte sich häufig sein unvergleichlicher Hintersinn. Eines seiner wohl berühmtesten Bilder ist betitelt "Simply add boiling water" – es zeigt ein brennendes Hochhaus, an dem genau dieser Werbeschriftzug prangt, und die Löschversuche der Feuerwehr.

Ab Mitte der vierziger Jahre gab er die Reportagefotografie auf und versuchte sich als Werbefotograf für verschiedene Magazine, Berater für Filmprojekte, Foto-Karikaturist und Kurzfilmer. Obwohl er damit nicht den ganz großen Durchbruch schaffte, blieb er diesen Experimenten treu. 

Zur Zeit seines Todes war er weitgehend vergessen, mittlerweile gehören seine Bilder in Fachkreisen und auf Fotoausstellungen allerdings zum weltweiten Standard zumindest der 1930er und 1940er-Jahre.





</doc>
<doc id="8381" url="https://de.wikipedia.org/wiki?curid=8381" title="Desinfektion">
Desinfektion

Desinfektion macht einen wesentlichen Teil der antiseptischen Arbeitsweise aus. Laut dem Deutschen Arzneibuch (DAB) bedeutet Desinfektion: „totes oder lebendes Material in einen Zustand versetzen, dass es nicht mehr infizieren kann“.

Zur Desinfektion können chemische oder physikalische Verfahren eingesetzt werden. Es gibt verschiedene Listen mit geprüften Desinfektionsmitteln und -verfahren, in denen diese nach verschiedenen Einsatzbereichen aufgeführt sind: Händedesinfektion, Hautantiseptik, Flächendesinfektion, Instrumentendesinfektion, Wäschedesinfektion, Raumdesinfektion und Desinfektion von Abfällen.

Technisch unterscheidet man zwischen Desinfektion und Sterilisation. Von Desinfektion spricht man bei einer Keimreduktion in einem bestimmten Testverfahren mit bestimmten Prüfkörpern um einen Faktor von mindestens 10, das heißt, dass von ursprünglich 1.000.000 vermehrungsfähigen Keimen (sogenannten koloniebildende Einheiten (KbE)) nicht mehr als 10 überleben (Ausnahme: Wäschedesinfektionsverfahren: Keimreduktion um einen Faktor von mindestens 10).

Bei der Desinfektion der Hände unterscheidet man zwischen der sogenannten „hygienischen“ und der „chirurgischen“ Händedesinfektion.

Alle Stoffe, die als Oxidationsmittel Sauerstoff abspalten, sind bakterizid und wirken sowohl gegen behüllte wie auch unbehüllte Viren. Gegen Pilze, Sporen und Tuberkuloseerreger sind sie nur teilweise und in begrenztem Umfang effektiv.

Wasserstoffperoxid ist als dreiprozentige wässrige Lösung zur Desinfektion von Haut und Schleimhaut geeignet, weil es nur Organismen an der Oberfläche tötet, im Gewebe hingegen durch Katalase/Peroxidase zersetzt wird. In höheren Konzentrationen (meist 30 %) wird es in Medizin, Pharmazie und Lebensmittelherstellung zur Sterilisation von Instrumenten und Behältern eingesetzt. Die Begasung von Räumen und raumlufttechnischen Anlagen zu Dekontaminationszwecken mit HO kann bei Erfüllung bestimmter Anforderungen eine wirksame Alternative zum Einsatz von Formaldehydgas darstellen.

Sporizidie wird nach der EN Norm 13704 mit Sporen von Bacillus subtilis getestet. Damit ein Wirkstoff/Desinfektionsmittel als sporizid eingestuft werden kann muss er eine 3-log Reduktion bei Sporen hervorrufen.

Ein Wirkstoff, der "überhaupt" Sporen keimunfähig machen kann (ein "sporozider" Wirkstoff, ein "Sporizid"), benötigt dafür eine Mindesteinwirkzeit, um die panzernde Hülle der Spore zu durchdringen. Diese erforderliche Einwirkdauer ist dann ein Maß für seine Effizienz als Sporizid.

Die in der Tabelle aufgeführten Sporizide Peressigsäure, Wasserstoffperoxid, Ozon und Natriumhypochlorit sind stark reagierende bzw. schnell zerfallende Oxidationsmittel. Sie müssen gegen Wärme und Licht geschützt aufbewahrt und chemisch stabilisiert werden, falls sie nicht gleich nach der Herstellung zur Desinfektion eingesetzt werden sollen.

Wasserstoffperoxid bildet mit Peressigsäure und Essigsäure eine „schnell“ sporizide Mischung, die durch die Essigsäure stabilisiert wird. Solche Mischungen werden nur im professionellen Bereich – Reinraumtechnik – eingesetzt. Als Beimischung zu Alkoholen kann Wasserstoffperoxid deren Wirkung bei der Händedesinfektion verbessern.

Zur Prüfung von Flächendesinfektionsmitteln werden nach der Europäischen Norm CEN TC 216 WG1 und WG3 Testorganismen eingesetzt.

In der Phase 2 Stufe 1 wird in einem quantitativen Suspensionsversuch zunächst im Reagenzglas unter praxisnahen Bedingungen geprüft.
In der Phase 2 Stufe 2 wird die Wirksamkeit der Desinfektionsmittel im praxisnahen Versuch durchgeführt.

Desinfektionsmittel müssen professionell und strategisch verwendet werden. Eine gewohnheitsmäßige Anwendung "im Haushalt" ist dagegen eher nachteilig. Unsachgemäße Anwendung kann zu Resistenzen führen, wenn insbesondere Wirkstoffkonzentration und Einwirkzeit und damit der Keimreduktionsfaktor zu gering sind (Selektion robuster Stämme). Oft weisen gegen Desinfektionsmittel widerstandsfähige Bakterien auch eine erhöhte Antibiotikaresistenz auf.

Gewohnheitsmäßige Anwendung von Desinfektionsmitteln zur Reinigung der Hände im Haushalt kann neben die Gesundheit bedrohenden Keimen gleichzeitig die Hautflora zerstören, welche z. B. gegen Dermatosen schützt. Verwendet man stattdessen nur Seife o. ä., so wirken die enthaltenen Tenside weniger desinfizierend (mikrobiozid), als dass sie die Wasserlöslichkeit von Verschmutzungen erhöhen. Seife entfernt eher den zuletzt von außen eingetragenen Schmutz als die dauerhaft vorhandene und erhaltenswerte Hautflora.

Angemessene Haut- bzw. Händedesinfektion in der Medizin schädigt die Hautflora dagegen nicht nachhaltig. Nur eine relativ geringe Zahl der Hautflora-Mikroben wird getötet. Die lokal dezimierte Hautflora regeneriert sich bald.
Die Kombination von übermäßigem Waschen mit Seife vor der Händedesinfektion und der Desinfektion selbst kann die Hautflora jedoch nachhaltig schädigen, da ein großer Teil der Hautflora im fettartigen Talg der Haarfollikel (Haarbalg) siedelt. Vor tensidfreien oder tensidarmen Desinfektionsmittel sind diese Mikroben geschützt, die Desinfektion zerstört nur von den Haaren weiter entfernte Mikroben. Diese werden in den folgenden Stunden bzw. Tagen durch Ausbreitung der in den Haarfollikeln gebildeten Keime ersetzt. Übermäßiges Waschen der Hände mit Seife löst dagegen den schützenden Talg. Eine anschließende Händedesinfektion zerstört dann auch die Keime im Haarfollikel, aus denen sich die umliegende Hautflora sonst regenerieren würde.

Wenn Desinfektionsmittel bedenkenlos im Haushalt eingesetzt werden oder Reste davon nicht richtig entsorgt werden, gelangen sie in Flüsse oder Kläranlagen und stören dort das wichtige Zusammenspiel einer Vielzahl von Bakterienarten, wodurch die Reinigungswirkung (in den Klärbecken oder in den Gewässern) herabgesetzt wird. Viele Desinfektionsmittel (z. B. Phenol) wirken zudem ökotoxisch auf Gewässer.

Manche Wirkstoffe von Desinfektionsmitteln können die menschlichen Schleimhäute, insbesondere die Nasenschleimhaut irritieren. Beispiele sind Chlor sowie Benzen und Phenol, wie auch andere Aromaten.

Bei allen Desinfektionsmitteln sollte man dem Etikett Beachtung schenken und auf die Gefahrensymbole achten. Viele dieser Mittel sind ätzend, reizen die Haut und/oder Schleimhäute, oder sie sind entflammbar oder sogar explosiv oder können in Mischung mit anderen Haushaltsreinigern giftiges Chlorgas freisetzen. Darüber hinaus sind manche Desinfektionsmittel humantoxisch oder karzinogen (Aldehyde, Phenol), und manche können Allergien hervorrufen. Oxidierende Wirkstoffe wie Peroxide oder Halogene können bestimmte Metalle angreifen.

Die Anwendung von Niedertemperaturplasma (TTP) ist eine Technologie zur Desinfektion mit kaltem Plasma, die bei einer Temperatur von unter 100 °C zeitsparend auch antibiotikaresistente Erreger sogar durch die Kleidung abtöten kann. Sie eignet sich zum Beispiel zur Desinfektion von Luft, Oberflächen, Gegenständen, zur Handdesinfektion und zur Behandlung von schlecht heilenden chronischen Wunden. Bei in vitro-Versuchen konnte nicht nur ein abtötende Wirkung gegenüber Bakterien, sondern auch gegenüber Viren und Pilzen beobachtet werden. Es erscheint daher eine Behandlung mit TTP bei Mykosen wie beispielsweise auch bei Fußpilz als möglich, was jedoch erst noch durch weitere Studien bestätigt werden muss.

Die Desinfektion von "Abwässern", "Trink"wasser oder flüssigen "Medien" kann durch verschiedene Verfahren erfolgen. Man unterscheidet grundsätzlich zwischen "chemischen" und "physikalischen" Verfahren zur Desinfektion.

Außer dem § 37 des Infektionsschutzgesetzes fordert die Trinkwasserverordnung (TrinkV 2001) in die Freiheit des Trinkwassers von Krankheitskeimen. Darüber hinaus sind die anerkannten Regeln der Technik, die in der DIN-Vorschrift 1988 und in DVGW-Vorschriften festgeschrieben sind zu beachten, Arbeitsblätter W 551 „Technische Maßnahmen zur Verminderung des Legionellenwachstums“ und W 553 „Bemessung von Zirkulationssystemen in zentralen Trinkwassererwärmungsanlagen“, sowie die VDI-Vorschrift 6023 „Hygiene in Trinkwasser-Installationen“. Zulässige Stoffe und Verfahren zur Abwehr sind in einer aktualisierten Liste des Umweltbundesamtes nach Trinkwasserverordnung 2001 beschrieben.

Dazu sind alle Zapfstellen für drei Minuten mit heißem Wasser von 70 °C zu betreiben.

Meist wird mit Chlor, Chlordioxid, oder Natrium- und Calciumhypochloritlösungen desinfiziert, wegen des hohen Aufwandes seltener wird Ozon in der Trinkwasserhygiene genutzt. Dabei ist die Dosierung von Chlorgaslösungen oder der Zusatz von Natrium- und Calciumhypochloritlösungen erlaubt. Zudem kann Chlor vor Ort elektrolytisch hergestellt und dosiert werden oder es wird vor Ort eine Chlordioxidlösung hergestellt und zugesetzt. Ozon und Ozonlösungen sind ebenfalls vor Ort zu erzeugen und in geeigneter Menge zuzusetzen. Nach § 6 der Trinkwasserverordnung darf nur die minimale Menge an Desinfektionsmittel zugesetzt werden. Die Kaltentkeimung wird zur Desinfektion von manchen Getränken verwendet.

Durch Bestrahlen mit UVC bei 254 nm werden Bakterien inaktiviert, allerdings können Legionellen in Amöben überleben. Zur Verbesserung der Wirkung kann zusätzlich Ultraschall genutzt werden.

Bei SODIS wird länger einwirkende UV-A-Strahlung der Sonne zusammen mit der Wärme zur einfachen Wasserentkeimung auf Haushaltsebene in Entwicklungsländern genutzt.

Zunehmend werden auch Membranen zur Entfernung von Mikroorganismen benutzt. Mit Mikro- und Ultrafiltration lassen sich bei einer Porengröße von kleiner als 0,2 µm auch Bakterien, teilweise sogar Viren ausfiltern. Ultrafiltrationsanlagen mit einer Trenngrenze von 0,02 µm sowie einer integrierten, täglichen Prüfung der Membran auf Defekte sind in den Vereinigten Staaten von Amerika als Desinfektionsverfahren im Trinkwasser zugelassen. Für solche Anlagen wird in den USA der Nachweis für vollständige Entfernung von Bakterien, Viren und Parasiten gefordert, der tägliche durchzuführende Test muss in der Lage sein, langfristig vollständige Entfernung von Bakterien und Parasiten zu gewährleisten.

Die Herstellung und Verwendung von Desinfektionsmitteln werden durch Gesetze geregelt. Die Produkteinstufung ist aber relativ komplex, denn sie richtet sich immer nach der spezifischen Anwendung, teilweise aber auch nach den Inhaltsstoffen. Ethanol beispielsweise kann in jede Produktkategorie fallen, je nach spezieller Desinfektionsanwendung.

Definition Humanarzneimittel: Desinfektionsmittel sind Humanarzneimittel, wenn sie am Menschen angewendet werden zur Vorbeugung oder Behandlung von Infektionserkrankungen.

Beispielsweise Ethanol zur Desinfektion bei einer Blutentnahme.

Definition Tierarzneimittel: Desinfektionsmittel sind Tierarzneimittel, wenn sie am Tier angewendet werden zur Vorbeugung oder Behandlung von Infektionserkrankungen oder wenn sie angewendet werden, um Geräte antiseptisch zu machen, bevor diese Geräte mit dem Tier in Kontakt kommen oder den tierärztlichen Behandlungsbereich antiseptisch zu machen.

Beispielsweise Ethanol zur Desinfektion einer Wunde.

Desinfektionsmittel sind Medizinprodukte, wenn sie angewendet werden, um Medizinprodukte oder den humanärztlichen Behandlungsbereich antiseptisch zu machen.

Beispielsweise Ethanol zur Desinfektion eines Katheters.

Desinfektionsmittel sind Biozide, wenn sie zur Flächen- und Raumdesinfektion im klinischen, öffentlichen (z. B. im Schwimmbad, siehe Schwimmbadverordnung) oder privaten Bereich eingesetzt werden sollen, um Infektionen zu verhindern. Da sie seit einigen Jahren einem Zulassungsverfahren unterliegen, kann die Verwendung einzelner Produkte auf bestimmte Personengruppen beschränkt sein (z. B. beruflich Qualifizierte, Desinfektoren). 

Hand- und Hautdesinfektionsmittel fallen EU-weit ebenfalls unter die Zulassung gemäß EU-Biozidverordnung, sind in Deutschland aber weiterhin ein Streitfall zwischen dem Bundesinstitut für Arzneimittel und Medizinprodukte (BfArM), Bonn, und der Bundesstelle für Chemikalien (BfC) in Dortmund. Desinfektionsmittel für medizinische Instrumente (z. B. OP-Besteck) fällt weiterhin unter das Medizinprodukterecht. 

Beispielsweise Alkohol zur Desinfektion des Arbeitsbereiches.

Desinfektionsmittel für den Lebensmittelbereich (Produktionsanlagen, Gastronomie, Küchen, Kantinen) und zur Trinkwasserdesinfektion gehören ebenfalls zu den Biozidprodukten (gemäß Anlage V, Biozidverordnung). Nicht für diese Verwendung zugelassene Desinfektionsmittel dürfen nicht im Kontakt mit Lebensmitteln verwendet werden, da das Risiko des Verbleibs im Lebensmittel besteht.

Beispielsweise Ethanol zur Desinfektion einer Produktionsanlage.

Die VDI 6022 enthält die anerkannten Regeln der Technik für Raumluft- und Klimatechnik.





</doc>
<doc id="8382" url="https://de.wikipedia.org/wiki?curid=8382" title="Eugène Atget">
Eugène Atget

Jean Eugène Auguste Atget (* 12. Februar 1857 in Libourne; † 4. August 1927 in Paris) war ein französischer Fotograf.

Atget begann seine Karriere zunächst mit mäßigem Erfolg als Schauspieler. Nachdem er einige Zeit aus Liebhaberei fotografiert hatte, machte er die Fotografie zu seinem Beruf. Sein Thema war Paris, die Stadt, in der er in der "Rue Campagne-Première 17" wohnte und die er liebte. Bemerkenswert sind seine Serien "Paris pittoresques" und "Le vieux Paris". Er dokumentierte um die Jahrhundertwende mit seiner sperrigen Großformatkamera das alte Paris, um systematisch die kleinsten Details der Stadt zu katalogisieren. Seine Fotos zeigen die Parks, Gebäude, Straßen, Schaufenster, Prostituierten, Arbeiter und sogar Türklinken von Paris. Bedarf für diese Aufnahmen gab es nicht nur bei Touristen und Sammlern, sondern auch bei Malern und Bühnenbildnern, die seine Fotos als Vorlagen für ihre eigene Arbeit nutzten. Später verkaufte er auch Serien an Museen und Bibliotheken. Obwohl er sich auf diese Weise seinen Lebensunterhalt verdienen konnte, gelang ihm weder künstlerisch noch finanziell der Durchbruch zu anhaltendem Erfolg.

Atget arbeitete mit einer 18×24 cm Kamera, was zu seiner Zeit bereits als veraltete Ausstattung galt. Häufig kolportiert wird die historisch falsche Anekdote, der zufolge Atget sich beharrlich weigerte, auf eine ihm von seinem Freund Man Ray angebotene Rolleiflex umzusteigen, da ihm diese „zu schnell“ sei. Rolleiflex-Prototypen gab es erst ab 1928, die Produktion dieser Kamera begann 1929, Atget starb jedoch bereits 1927.

1920 verkaufte Atget einen großen Teil seiner Sammlung an die École nationale supérieure des beaux-arts de Paris. Dieser Verkauf war zwar kein geschäftlicher Erfolg, rettete aber seine Sammlung, die bereits deutliche Lagerschäden aufwies. Durch Man Ray hatte Atget die junge Fotografin Berenice Abbott kennengelernt, die nach seinem Tod die verbliebene Sammlung aus dem Nachlass erwarb, in Büchern publizierte und schließlich an das Museum of Modern Art in New York verkaufte. Ihrer Arbeit ist es zu verdanken, dass Atget nicht in Vergessenheit geriet.

Im Alter von 70 Jahren starb Eugène Atget am 4. August 1927 in Paris.

Nach ihm ist der Krater Atget auf dem Merkur benannt.






</doc>
<doc id="8383" url="https://de.wikipedia.org/wiki?curid=8383" title="Ansel Adams">
Ansel Adams

Ansel Easton Adams (* 20. Februar 1902 in San Francisco, Kalifornien; † 22. April 1984 in Carmel-by-the-Sea, Kalifornien) war ein US-amerikanischer Fotograf, Autor und Lehrer der künstlerischen Fotografie. Er wurde vor allem bekannt durch seine eindrucksvollen Landschafts- und Naturfotografien aus den Nationalparks, National Monuments und den Wilderness Areas im Westen der Vereinigten Staaten, für deren Erhalt er sich zeitlebens aktiv einsetzte.

Als Mitbegründer der Gruppe f/64 zählt er zu den Pionieren der "straight photography" und gilt als einer der bedeutendsten amerikanischen Fotografen. Adams verfasste zahlreiche Lehrbücher zu Theorie und Praxis der Fototechnik. Das von ihm zeitgleich mit Fred Archer formulierte Zonensystem wurde wegweisend für die künstlerische Schwarzweißfotografie.

Ansel Easton Adams war das einzige Kind von Charles Hitchcock Adams und Olive Bray Adams, einer Händlerfamilie aus San Francisco. Der Junge wurde nach dem Onkel Ansel Easton benannt. Die Adams-Familie stammte väterlicherseits aus Neuengland. Die Familie war in den ersten Jahren des 18. Jahrhunderts, von Nordirland kommend, eingewandert. Der Großvater hatte in San Francisco ein prosperierendes Holzhandelsunternehmen aufgebaut, das in der Nachfolge von Adams’ Vater Charles geleitet wurde. Die Familie der Mutter stammte aus Baltimore, der Großvater mütterlicherseits hatte sich als Transportunternehmer und Grundstücksspekulant in Carson City, Nevada, niedergelassen.

Adams’ Eltern waren politisch liberale, ansonsten eher konservativ-bürgerliche Menschen. Der Vater war ein begeisterter Hobby-Astronom, der sich allgemein für optische Geräte und für die Fotografie im Besonderen interessierte und als modernste Errungenschaft eine „Brownie-Bullseye“-Boxkamera von Kodak sein Eigen nannte; die Mutter war künstlerisch ambitioniert und widmete sich bevorzugt der Porzellanmalerei.
Eine erste Kindheitserinnerung von Ansel Adams war das verheerende San-Francisco-Erdbeben von 1906, bei dem sich der Vierjährige infolge eines Sturzes das Nasenbein zertrümmerte, das nie gerichtet wurde und Adams seine unverwechselbare schiefe, nach links gerichtete Nase einbrachte. Da die Adams’ in einem selbstgebauten Haus in den Dünen außerhalb San Franciscos wohnten, blieben sie von den Folgen des Bebens ansonsten weitgehend verschont.

1907 starb Ansels Großvater William James Adams. Mit seinem Tod und dem als "The Panic of 1907" bezeichneten ersten großen Börsenkrach in den USA ging auch der Untergang seines Unternehmens einher. Die schleichende Rezession der folgenden Jahrzehnte beanspruchte das gesamte Familienvermögen der Adams’, und Ansels Vater versuchte, das wenige noch vorhandene Firmenkapital zu retten. Geheime Absprachen und Anteilsverkäufe von Ansels Onkel Ansel Easton sollen schließlich zu einem weiteren finanziellen Desaster geführt haben. Letztlich übernahm die Bank den Grundbesitz, und die einstmals florierende Firma wurde zerschlagen.

Als Kind war Ansel oft kränklich, litt an Erkältungen und diversen Kinderkrankheiten. Dennoch tobte er sich bei stundenlangen Klettertouren in den steilen Klippen an der nahe gelegenen Pazifikküste von Fort Scott oder China Beach aus. Der wissbegierige Junge sammelte Insekten und botanisierte Pflanzen. Überdies begeisterte er sich für Sport, war aber stets zu ungeduldig, um sich auf eine Sportart zu konzentrieren. 1912 erkrankte Ansel an Masern und musste zwei Wochen in einem abgedunkelten Zimmer im Bett verbringen. Zum Zeitvertreib erklärte ihm der Vater seine Boxkamera und das uralte Prinzip der Camera obscura, das dahinter stand, und weckte damit erstmals das Interesse des Jungen an der Fotografie.

Die Grundschulzeit verbrachte Ansel größtenteils an der Rochambeau-Schule in San Francisco. Weil er jedoch als schwieriges Kind galt, das sich meistens im Unterricht langweilte und oft in Raufereien verstrickt war, musste er mehrmals die Schulen wechseln. Nach einem heftigen Wutanfall wurde er schließlich ganz der Schule verwiesen und erhielt Hausunterricht beim Vater, der ihm grundlegende Kenntnisse in Französisch und Algebra beibrachte. Außerdem sorgte Charles Adams dafür, dass sein Sohn englische Literaturklassiker las und Unterricht im Altgriechischen bei einem befreundeten Pfarrer bekam. Wie Adams in seinen Erinnerungen schilderte, reifte während der zahlreichen Gespräche mit dem Geistlichen in ihm schnell die Erkenntnis, sich mit Hilfe des Intellekts ein eigenes kritisches Weltbild schaffen zu müssen, das sich, wie er sagte, „gegen Intoleranz, Unverstand und Standesdünkel“ richtete. Etwa zu dieser Zeit zeigte sich auch ein musikalisches Talent in dem Jungen, und so erhielt er ab 1914 zusätzlich Klavierunterricht.

1915 schenkte ihm der Vater eine Jahreskarte für die Panama-Pacific International Exposition, der Weltausstellung, die zur Feier der Eröffnung des Panamakanals veranstaltet wurde. Die gigantische Ausstellung hinterließ einen nachhaltigen Eindruck bei dem Jungen, vor allem begeisterte er sich für die Konzerte, die in der Festhalle der Ausstellung, einem riesigen Kuppelbau, an einer imposanten Orgel zu Gehör gebracht wurden. Wenn möglich, ließ er keines der Konzerte aus. Oft besuchte er auch die Gemälde- und Skulpturenausstellung im Palast der schönen Künste, wo Werke von Bonnard, Cézanne, Gauguin, Monet, Pissarro und van Gogh gezeigt wurden.

Auf der Suche nach einem regulären Schulabschluss besuchte Ansel in der Folgezeit noch einige Schulen, mit dem Abschlusszeugnis der achten Klasse beendete er schließlich formell seine schulische Laufbahn.

Ab seinem 13. Lebensjahr erhielt der Junge intensiven Klavierunterricht bei einer älteren Dame namens Marie Butler, einer Absolventin des "New England Conservatory" mit langjähriger Erfahrung im Unterrichten. Sie besaß Virtuosität im Spiel und ein profundes Wissen über Musiktheorie und Musikgeschichte; sie verstand es, dem Jungen mit viel Geduld und Ausdauer ein gewisses Maß an Disziplin abzuringen und die Faszination für das Instrument zu wecken. 1918 empfahl sie den jungen Adams zum Musikstudium an den Komponisten Frederick Zech (1858–1926) weiter. Bald entstand in Adams der Wunsch, Berufsmusiker zu werden.

Ansel Adams war erstmals 1916 zusammen mit seinen Eltern auf einer Urlaubsreise im Yosemite-Nationalpark gewesen. Während des Urlaubs schenkte ihm der Vater eine "„Brownie“"-Boxkamera von Kodak, Ansels erste eigene Kamera. Leidenschaftlich begann der 14-Jährige alles festzuhalten, was ihm vor die Linse kam. Der Junge war von dem Urlaub so begeistert, dass er auch in den folgenden Jahren die Sommermonate in dem Naturreservat verbrachte. 1919 trat Adams dem von John Muir begründeten Sierra Club bei. 1922 veröffentlichte Adams seinen ersten Artikel für das "Sierra Club Bulletin". Im Jahr 1934 sollte er schließlich Mitglied im Direktorium des Clubs werden (bis 1971).

Bei einem Ausflug des Clubs nach Yosemite begegnete Adams im Sommer 1923 seinem Jugendfreund, dem Violinisten und späteren Fotografen Cedric Wright (1907–1950) wieder. Während einer mehrtägigen Exkursion durch den Park festigte sich die Freundschaft der beiden Naturliebhaber, und man tauschte sich über die Musik und das beiderseitig aufkeimende Interesse für die Fotografie aus. Wright befasste sich mit dem Pictorialismus und schuf bevorzugt Porträts, die in ihrer technischen Qualität den frühen Arbeiten Edward Westons ähnlich waren. Durch Wright wurde Adams unter anderem mit den künstlerisch gedruckten Büchern von Elbert Hubbard, dem Begründer der "Roycroft"-Bewegung, bekannt.

Die Freundschaft mit Wright, die Beziehung zum Sierra Club und die unzähligen Exkursionen im Yosemite-Park sollten in Adams eine tiefe lebenslange Faszination für die Wildnis und deren Schutz wecken. In späteren Jahren erinnerte sich Adams an diese Zeit als „die denkwürdigste Erfahrung in seinem Leben“ und betonte, wie das starke Erleben der Natur, die Kindheit am Meer und die jungen Jahre in der Sierra Nevada sein gesamtes Leben geprägt habe.

Im Lauf der Zeit begann Adams die Schnappschüsse, die er auf seinen Exkursionen im Yosemite machte, als ein „visuelles Tagebuch“ zu betrachten, und je mehr er fotografierte, desto mehr interessierte ihn das fotografische Verfahren, das dahinter steckte. Schließlich wollte er lernen, wie man die Bilder selbst zu Papier bringt. Um 1917 bot ihm ein Nachbar, der ein Fotolabor betrieb, einen Job als Laborhilfe an. Innerhalb kurzer Zeit erlernte Adams die Routine der Filmentwicklung. Schließlich perfektionierte er sein Hobby, und es gelang ihm, ausdrucksstarke Bilder zu schaffen.

Bis Mitte der 1920er Jahre sah sich Ansel Adams allenfalls als ambitionierter Amateurfotograf. Adams datierte einen Frühlingstag, den 17. April 1927 im Yosemite, der, wie er sagte, „sein Verständnis für das Medium Fotografie verändern sollte.“ An diesem Tag brach Adams mit seinen Freunden Cedric Wright, Arnold Williams, Charlie Michael und seiner zukünftigen Frau Virginia Best zu einer Wanderung zum "Diving Board" auf, einem Felsvorsprung mit einem imposanten Blick auf den "Half Dome". Adams schleppte im Rucksack eine 40 Pfund schwere Kamera-Ausrüstung mit sich, bestehend aus einer "Korona"-Studiokamera, mehreren Objektiven, Filtern, sechs Plattenhaltern mit zwölf Glasplatten und einem Holzstativ. Während des Aufstiegs fertigte Adams mehrere Aufnahmen an, von denen einige misslangen, eine Glasplatte wurde unbeabsichtigt belichtet, weil Adams vergessen hatte, das Kameraobjektiv vor der direkten Sonneneinstrahlung zu schützen. Schließlich hatte er nur noch zwei Platten übrig, um sie, wie er sagte, „mit dem großartigsten Anblick, den die Sierra bietet zu belichten – dem "Face of Half Dome" selbst.“ Von dieser Exkursion brachte Adams eines seiner berühmtesten Bilder mit: "Monolith, The Face of Half Dome".

1937 kam es zu einem Brand in Adams’ Fotolabor, wobei tausende seiner originalen Negative vernichtet oder beschädigt wurden. Er brauchte mit seinen Helfern mehrere Tage, um die geretteten Negative zu wässern und zu trocknen. Von einigen Fotografien, beispielsweise "Monolith, The Face of Half Dome", die nur am Rand beschädigt wurde, gibt es Vergrößerungen vor und nach dem Feuer, wobei die neueren Vergrößerungen notwendigerweise einen kleineren Bildausschnitt zeigen, damit die beschädigten Bereiche nicht sichtbar werden. Adams bewahrte die Originalnegative in späteren Jahren in einem Panzerschrank auf.

Im Frühjahr 1926 machte Cedric Wright seinen Freund Ansel mit dem Kunstsammler und Mäzen Albert Maurice Bender (1866–1941) bekannt. Der aus Irland stammende Bender hatte sein Vermögen als Versicherungsmakler gemacht und galt als Philanthrop, der einen großen Bekanntenkreis besaß und recht einflussreiche Beziehungen zu wichtigen Galeristen, Künstlern und Verlagen an der Westküste unterhielt. Sein besonderes Interesse galt der Druckkunst und seltenen Künstlerbüchern. Er interessierte sich für Adams’ Fotoarbeiten und entschied kurzerhand, zusammen mit dem jungen Fotografen ein Portfolio zu realisieren. Bender kümmerte sich um Verlag und Vertrieb. Nach Adams’ Vorstellung sollte das Portfolio schlicht "Photographs" betitelt werden, doch die Verlegerin Jean Chambers Moore hatte Vorbehalte gegen das Wort, und so einigte man sich auf das Kunstwort "„Parmelian Prints“" als Titel, mit dem Adams jedoch nicht sehr glücklich war. Als Adams das fertige Druckwerk schließlich in den Händen hielt, war seine Enttäuschung umso größer, da dem Titel die fehlerhafte Unterzeile "„…of the High Sierras“" hinzugefügt worden war: denn „Sierra“ ist bereits Mehrzahl. "Parmelian Prints of the High Sierras" wurde 1927 mit einer Auflage zu 100 Portfolios plus 10 Künstlerkopien mit jeweils 18 Fotografien zum Verkaufspreis von 50 US-Dollar das Exemplar veröffentlicht.

Ansel Adams und Albert Bender wurden enge Freunde und unternahmen gemeinsam zahlreiche ausgedehnte Landpartien mit dem Auto. Durch Bender wurde Adams bald mit zahlreichen Kreativen der Bay Area bekannt, so lernte er unter anderem die Journalistin und Dichterin Ina Coolbrith kennen oder den zurückgezogen lebenden, dem Humanismus kritisch gegenüberstehenden Poeten und Naturphilosophen Robinson Jeffers, der in symbolbeladenen Gedichten eine Zukunft prognostizierte, in der die Natur sehr gut ohne den Menschen auskommen werde, was einer gewissen Grundstimmung von Adams recht nahekam. Jeffers’ zunehmend antihumanistische Radikalität und seine gesteigerte Verachtung für die menschliche Zivilisation, sollte ihn in späteren Jahren häufig in die Kritik bringen.

Am 2. Januar 1928 heiratete Ansel Adams seine Jugendliebe Virginia Rose Best in Yosemite. Virginia war die 1904 geborene Tochter von Harry Best, einem ortsansässigen Landschaftsmaler, der Gemälde, Holzschnitzereien und Souvenirs in einem eigenen Studio mit Shop im Yosemite-Park verkaufte. Ansel hatte Virginia bereits 1921 in Harry Bests Studio kennengelernt. Beide verband die Leidenschaft zu Yosemite und zur Musik: Virginia wollte ursprünglich Sängerin werden. Virginia Best und Ansel Adams hatten über sechs Jahre lang eine wechselhafte Beziehung geführt. 1932 kam der gemeinsame Sohn Michael zur Welt, zwei Jahre später folgte die Tochter Anne. Als Virginias Vater Harry Best 1936 überraschend starb, übernahm sie dessen Ladenatelier in Yosemite.

In den ersten Jahren seiner Ehe schwankte Adams noch immer zwischen zwei Berufen: der Karriere als Konzertpianist und der des professionellen Fotografen. Spätestens Anfang der 1930er Jahre mit dem Beginn der großen Depression konnte sich Adams den Spagat weder finanziell noch gefühlsmäßig weiter erlauben. Um sich Klarheit über seinen weiteren Werdegang zu verschaffen, unternahm er in dieser Zeit mehrere Reisen nach New Mexico.

Eine Reise mit Albert Bender hatte Adams bereits 1927 nach Santa Fe in New Mexico geführt. Es war Adams erste Bekanntschaft mit der kargen Wüstenregion im Südwesten der Vereinigten Staaten. Er war stark beeindruckt von dem eigentümlichen Licht New Mexicos, der teilweise bizarren Landschaft und den gewaltigen Wolkenformationen. In Santa Fe trafen sie auf den Lyriker Witter Bynner und auf die Schriftstellerin Mary Hunter Austin, die sich insbesondere für die Belange der Indianer und der Frauen einsetzte. Während Adams’ erster Reise nach New Mexico 1927 entstanden nur wenige Fotos. In den beiden Folgejahren nahm der Fotograf eine "Korona"-Studiokamera mit und belichtete auf orthochromatischen Filmen.
1929 machten Ansel Adams und Virginia in Begleitung der irischen Autorin und Theosophin Ella Young, einer Bekannten von Albert Bender, einen längeren Besuch in Santa Fe. Zu diesem Zeitpunkt spekulierte Adams erstmals ernsthaft damit, den Lebensunterhalt ausschließlich mit der Fotografie zu verdienen und sich eventuell im Norden von New Mexico niederzulassen. Ansel und Virginia waren einer Einladung von Mary Austin gefolgt, bei ihr zu wohnen. Man freundete sich rasch miteinander an, und bald entstand die Idee, ein gemeinsames Buch über ein Thema New Mexicos zu verfassen. Adams und Austin einigten sich in Rücksprache mit Albert Bender als Sponsor auf Taos Pueblo und nahmen Kontakt mit der Kunstmäzenin Mabel Dodge Luhan auf, die im nahegelegenen Taos ihre Künstlerkolonie "Los Gallos" begründet hatte. Die wohlhabende Luhan hatte bereits in Europa und New York einflussreiche Salons unterhalten, in denen sich die Intellektuellen und Kreativen ihrer Zeit versammelt hatten. Ihr Ehemann Tony, selbst ein Pueblo-Indianer, stellte den Kontakt zum Häuptling und Ältestenrat des Pueblos her. "Taos Pueblo" erschien 1930 in einer Erstauflage von 100 Büchern.

Als Adams ein weiteres Mal bei Mabel Dodge Luhan in Los Gallos zu Besuch war, machte er Bekanntschaft mit dem Fotografen Paul Strand und dessen Frau Becky sowie der Malerin und Fotografin Georgia O’Keeffe, dem Schriftsteller D. H. Lawrence und dem Architekten und Maler John Marin, die alle zu den Gästen der Kunstmäzenin zählten. Paul Strand interessierte sich sehr für Adams’ Taos-Buch, und so kamen die beiden Fotografen ins Gespräch. Strand zeigte Adams etwas umständlich seine Arbeiten, die er zu diesem Zeitpunkt nur als 4 × 5 inch Großformat-Negative im Pappkarton zur Hand hatte. Trotz fehlender Positivabzüge war Adams von Strands perfekt durchkomponierten Bildern fasziniert: 

Die Begegnung mit Paul Strand gab Adams den entscheidenden Impuls: Plötzlich erkannte er die kreativen Möglichkeiten, die in dem Medium Fotografie stecken mochten. Mit dem Entschluss, die Karriere als Musiker endgültig aufzugeben und zukünftig als professioneller Fotograf zu arbeiten, kehrte Adams nach San Francisco zurück. In den Folgejahren pflegte Adams eine angeregte Brieffreundschaft mit Strand.

1930 errichtete Adams neben seinem Elternhaus ein Haus mit Studio und begann als kommerzieller Fotograf zu arbeiten. Unter dem steigenden Druck der Wirtschaftskrise fotografierte er in der Anfangszeit, wie er sagte, „einfach alles: vom Katalog bis zum Industriereport, von der Architektur bis zum Porträt.“ Obwohl er stets der künstlerischen Fotografie und seiner späteren Lehrtätigkeit den Vorzug gab, sollte Adams bis ins Alter auch als Auftragsfotograf erfolgreich bleiben und Fotoreportagen beispielsweise für Fortune oder das Life-Magazin machen oder Werbefotografien unter anderem für AT&T, Kodak oder Nissan anfertigen. Viele seiner späteren Auftragsarbeiten für zahlende Kunden entstanden in Farbe.

Bereits 1929 hatte ihn die "Yosemite Park and Curry Company (YPCCO)", die die Konzessionsbetriebe des Parks leitete, engagiert, die Öffentlichkeitsarbeit für Yosemite zu übernehmen und vornehmlich Aufnahmen der Wintersport-Möglichkeiten zu fotografieren, um Touristen anzulocken. Für viele Jahre wurde die "YPCCO" Adams wichtigster Auftraggeber. Seine Fotografenkollegin Imogen Cunningham, die er ungefähr zu dieser Zeit, Ende der 1920er Jahre, über Albert Bender kennengelernt hatte und mit der er zeitlebens befreundet war, sah seine kommerziellen Arbeit stets mit gemischten Gefühlen und kritisierte ihn manchmal humorvoll mit den Worten „Adams, du hast dich wieder verkauft.“ Cunningham stand anfangs unter dem Einfluss des Pictorialismus, wandte sich aber Mitte der 1920er Jahre ebenfalls der "straight photography" zu.

In künstlerischer Hinsicht schätzte Adams die damals weit verbreitete pictorialistische Fotografie nicht besonders. Der Stil erschien ihm als zu manieriert, und bislang hatte er nur wenige Fotografien gesehen, die er als kunstvoll empfand, überdies war sein Wissen um die Geschichte der Fotografie und Fotografen bis dahin äußerst gering. Spätestens seit seiner Begegnung mit Paul Strand in New Mexico hatte Adams damit begonnen zu experimentieren; er probierte neue fotografische Richtungen aus und arbeitete nun mit unstrukturierten, glatten Fotopapieren mit glänzender Oberfläche, wie es auch seine Vorbilder Strand und Edward Weston taten. Schließlich bekam er ein feineres Gefühl für das Licht und die Tonwertabstufungen in den Abzügen. Er notierte: „Ich fühlte mich befreit: Ein gutes Negativ konnte ich durch die Visualisierung schaffen und jetzt auch zuverlässig als "feines Bild" auf glänzendem Papier umsetzen.“

An einem Abend im Jahr 1932 trafen sich Ansel Adams und die Fotografen Imogen Cunningham, John Paul Edwards, Sonya Noskowiak, Henry Swift und Edward Weston bei dem Fotografen, Filmemacher und Berkeley-Studenten Willard Van Dyke, um sich über den Gedanken der "„straight photography“", der „reinen Fotografie“, auszutauschen. Obwohl die Arbeiten der Beteiligten teilweise sehr unterschiedlich waren, einigten sie sich darauf, ein gemeinsames Ziel zu verfolgen und einen neuen Weg der kreativen Fotografie, die sich deutlich vom tradierten Pictorialismus abheben sollte, zu definieren. Bei einem weiteren Treffen diskutierte man über einen Namen für die Gruppe. Der anwesende junge Fotograf Preston Holder, ein Studienkollege von Willard Van Dyke, machte den Vorschlag „US 256“, der veralteten Systembezeichnung für die sehr kleine Blendenzahl 64, die eine große Schärfentiefe bedeutete. Doch weil die Verwechslung mit einer US-Autobahn nahelag, einigte man sich auf "„f/64“", entsprechend der neuen Bezeichnung für US256.

„Gruppe f/64“ sollte zum Schlagwort für eine direkte, geradlinige Fotografie als eigenständige Kunstform werden und nicht die Imitation einer anderen Kunstform. Die Mitglieder nahmen sich zum Ziel, dies möglichst bald in einem „visuellen Manifest“ darzulegen und konzipierten eine Ausstellung. Die etablierten Galeristen zeigten sich anfangs wenig begeistert, und die lokale Kunstszene von San Francisco reagierte eher ablehnend auf die Vorstellung, dass Fotografie Kunst sein sollte. Lediglich Lloyd Rollins, der Direktor des "M. H. de Young Memorial Museums" in San Francisco, zeigte sich aufgeschlossen und willigte nach einer Begutachtung der fotografischen Arbeiten ein. Für Ansel Adams war es die dritte größere Ausstellung in einem Kunstmuseum. Bereits 1931 hatte er eine Einzelausstellung an der Smithsonian Institution in Washington D.C. gehabt und im selben Jahr im de Young ausgestellt. Zu der Ausstellung luden die sieben Gründungsmitglieder der Gruppe f/64, Adams, Cunningham, Edwards, Noskowiak, Swift, Weston und van Dyke, noch vier weitere Fotografen ein, die ein ähnliches Konzept in ihren Fotografien verfolgten: Preston Holder, Consuela Kanaga, Alma Lavenson und Brett Weston, den Sohn von Edward Weston.

Die Ausstellung lief vom 15. November bis zum 31. Dezember 1932. Gezeigt wurden 80 Fotografien, die käuflich zu erwerben waren: Edward Weston verlangte fünfzehn Dollar pro Bild, die anderen Teilnehmer zehn. Während der Ausstellung verteilte die Gruppe ein gemeinsam verfasstes Manifest. Sowohl Ausstellung wie Manifest erregten Aufsehen und führten zu heftigen Diskussionen, die, so Adams, „größtenteils negativer Prägung waren.“ Vornehmlich Künstler und Galeristen führten schriftliche Beschwerde gegen das Museum, das es wagte, in einem öffentlichen Raum Fotografie als Kunstform zu zeigen. Letztlich stellten sich das Kuratorium und Museumsdirektor Rollins auf die Seite der Fotografen. Federführend in der Kritik waren die Piktorialisten, allen voran William Mortensen, ein der malerischen Tradition verhafteter Fotograf aus Los Angeles, der sich nach Adams’ Meinung despektierlich in Fachzeitschriften über die Gruppe f/64 ausließ. Mit Mortensen lieferte sich Adams noch einige Zeit nach der Ausstellung einen kontroversen Briefwechsel.

Die Gruppe f/64 machte nur noch eine weitere Ausstellung. Nachdem Willard Van Dyke, der Initiator der Gruppe, nach New York gezogen war und auch die anderen Mitglieder eigene Ziele verfolgten, traf man sich nur selten; schließlich löste sich die Gruppe formlos auf. Als „Nachlassverwaltung“ der Gruppe eröffnete Adams 1933 eine kleine Galerie in San Francisco, die "Ansel Adams Gallery", die unter dem nachhaltigen Eindruck eines Besuches bei Alfred Stieglitz stand und die nach dem Vorbild von Stieglitz’ Galerien in New York gestaltet war. Kommerziell orientiert präsentierte Adams neben den Bildern der ersten Gruppe-f/64-Ausstellung auch Gemälde, Skulpturen und Drucke verschiedener Künstler.

Im März 1933 unternahm Ansel Adams in Begleitung seiner Frau Virginia eine ausgedehnte Reise an die Ostküste, die über Chicago und Detroit mit dortigen Museumsbesuchen auch nach Rochester führte, wo Adams die Fabrik von Eastman Kodak besichtigte. Ziel der Reise war New York City, wo die Adams am 28. März eintrafen. Neben Theater- und Museumsbesuchen beabsichtigte Ansel, mit Fotografien im Gepäck, unbedingt Alfred Stieglitz zu treffen, den seinerzeit einflussreichsten Galeristen und Mentor der Fotografie in den Vereinigten Staaten, um ihm seine Bilder zu zeigen.

Bei seiner ersten Begegnung mit Alfred Stieglitz in dessen Galerie "An American Place" in der Madison Avenue empfand Adams den New Yorker Fotografen als kühl und abweisend, doch schließlich betrachtete Stieglitz Adams’ Arbeiten mit Wohlwollen. Mit Stieglitz’ Absegnung wurde Adams bei der einflussreichen New Yorker Galeristin Alma Reed vorstellig, die mit den "Delphic Studios" eine der wenigen renommierten Kunstgalerien betrieb, die auch Fotografien zeigten. Im November 1933 eröffnete schließlich eine Verkaufsausstellung mit 50 Fotografien von Adams in den "Delphic Studios", die zwar in den Zeiten der großen Depression kein finanzieller Erfolg wurde, aber von einer überraschend guten Kritik in der New York Times begleitet wurde.

Adams besuchte Alfred Stieglitz von diesem Zeitpunkt an einmal im Jahr in New York, um sich mit ihm auszutauschen und neue Bilder zu zeigen. Erst im Januar 1936 erklärte sich Stieglitz bereit, eine Ausstellung mit Adams zu machen; die Bilder wurden im November 1936 in einer erfolgreichen Show im "An American Place" gezeigt. Erfreut schrieb Adams an seine Frau Virginia: „Die Schau bei Stieglitz ist ungewöhnlich – nicht nur, daß die Bilder stilvoll aufeinander abgestimmt und gehängt sind. Ihre Beziehung zum Raum und die Beziehung zu Stieglitz sind Dinge, die nur einmal im Leben geschehen.“

1939 traf Adams in New York mit dem Kunsthistoriker Beaumont Newhall und dessen Frau Nancy zusammen, mit denen Adams bereits seit seinem 1935 erschienenen Buch "Making a Photograph", in Briefkontakt gestanden hatte. Beaumont Newhall war zu diesem Zeitpunkt Bibliothekar am Museum of Modern Art (MoMA), er hatte ein starkes Interesse an der Fotografie als Kunstform und schrieb zahlreiche Essays und Kritiken zu dem Thema. Aus den Briefwechseln erwuchs schließlich eine lebenslange Freundschaft. Gemeinsam mit Nancy Newhall veröffentlichte Adams in späteren Jahren einige Bücher.
1940 wurde Adams Kurator einer größeren Bilderschau in San Francisco, die unter dem Titel "A Pageant of Photography" im Rahmen der Golden-Gate-Ausstellung einen Querschnitt durch die Geschichte und Entwicklung der Fotografie zeigen sollte. Das Spektrum reichte von den Anfängen der Fotografie mit Timothy H. O’Sullivans Aufnahmen vom Bürgerkrieg bis zu Man Rays Rayographien. Der begleitende Ausstellungskatalog war umfangreich und enthielt Essays von Beaumont Newhall, Dorothea Lange, László Moholy-Nagy, Nicholas Ulrich Mayall vom Lick-Observatorium, Grace Morley der Direktorin des San Francisco Museum of Modern Art und Paul Outerbridge.

Auch Beaumont Newhall war mit seiner Frau extra aus New York an die Westküste gekommen. Adams’ Fotoausstellung im Palast der schönen Künste inspirierte die Newhalls, die fotografische Abteilung des MoMA umfangreich aufzubauen. Nach der Rückkehr konnten sie den unzugänglichen Alfred Stieglitz von dieser Idee überzeugen. Adams gelang es seinerseits, den Fotografen Arnold Genthe für die noch im selben Jahr im MoMA geplante Eröffnungsausstellung zu gewinnen. Sowohl Stieglitz’ wie auch Genthes Beiträge galten als unverzichtbar, zählten doch beide zu den Pionieren der künstlerischen Fotografie in den USA.

Am 31. Dezember 1940 eröffnete unter dem Titel "Sixty Photographs" die erste Ausstellung der neuen fotografischen Abteilung am MoMA. Die Schau war umfangreich und dokumentierte die gesamte kreative Fotografie von ihren Anfängen bis in die Gegenwart. Gezeigt wurden Arbeiten von Berenice Abbott, Ansel Adams, Eugène Atget, Ruth Bernhard, Mathew B. Brady, Henri Cartier-Bresson, Harold E. Edgerton, P. H. Emerson, Walker Evans, Arnold Genthe, David Octavius Hill und Robert Adamson, Dorothea Lange, Henri Le Secq, Helen Levitt, Lisette Model, Moholy-Nagy, Dorothy Norman, T. H. O’Sullivan, Eliot Porter, Man Ray, Henwar Rodakiewicz, Charles Sheeler, Edward Steichen, Alfred Stieglitz, Paul Strand, Luke Swank, Brett Weston, Edward Weston und Clarence White sowie unbekannte Pressefotografen.

Mit Ausbruch des Zweiten Weltkriegs meldete sich Beaumont Newhall zur Luftaufklärung nach Europa, und seine Frau Nancy und Ansel Adams übernahmen kurzfristig das Kuratorium für die Fotoabteilung des MoMA. Adams fungierte als Vizepräsident des fotografischen Komitees. Im Verlauf des Krieges übernahm Edward Steichen, der von der US-Marine mit der fotografischen Dokumentation des Pazifikkriegs beauftragt worden war, die Führung im MoMA. Adams hegte seit seiner ersten Begegnung mit Steichen eine ausgeprägte Antipathie für den Fotografen, zumal dieser kriegspropagandistische Ausstellungen konzipierte. Nach Adams’ Ansicht überschritten diese den Aufgabenbereich eines Kunstmuseums. Schließlich kam es zu Diskrepanzen, in deren Folge sowohl die Newhalls als auch Adams von ihren Posten zurücktraten. Steichen trat 1947 das Amt als neuer Direktor der fotografischen Abteilung des MoMA an, die er bis 1962 leitete. Sein Nachfolger wurde John Szarkowski, der in den 1970er Jahren eine große Wanderausstellung mit Adams’ Arbeiten kuratieren sollte.

Im Jahre 1954 kamen Adams und Steichen noch einmal miteinander in Kontakt, als Steichen seine Ausstellung "The Family of Man" (1955) zusammenstellte und von Adams Negative erbat. Adams schickte Steichen die Bilder "Mount Williamson", "Sierra Nevada" und "From Manzanar, California" als Duplikate zu und bat darum, die Abzüge herstellen zu dürfen. Steichen lehnte ab und ließ Adams’ Arbeiten, wie dieser kritisierte, „unverhältnismäßig vergrößern“. Adams bemerkte später: „Als ich das fertige Wandbild sah, wurde mir schlecht. Er hatte "Mount Williamson", eines meiner stärksten Bilder, zu einer Wandtapete degradiert. […] Ich verlor auf Jahre das Interesse am MoMA.“

Im Frühjahr 1941 erhielt Adams ein Schreiben des damaligen US-Innenministers Harold L. Ickes mit der Bitte, die Nationalparks in den Vereinigten Staaten zu fotografieren, um davon Wandbilder für das Ministerium zu erstellen; etwa zur gleichen Zeit beauftragte ihn die "U.S. Potash Company" in Carlsbad, New Mexico die Kali-Minen in der Nähe von Carlsbad zu fotografieren. Zu diesem Zweck reiste Adams in Begleitung seines achtjährigen Sohnes Michael und seines langjährigen Freundes Cedric Wright in das nördliche New Mexico, um verschiedene Landschaftsaufnahmen zu machen. In der Nähe des Dorfes Hernandez bot sich den Reisenden ein außergewöhnlicher Anblick, als der Mond plötzlich über den schneebedeckten Bergspitzen aufstieg, während im Westen die spätnachmittägliche Sonne einige Kreuze an einem Kirchhof weiß aufblitzen ließ. Im Wissen darum, dass sich solch ein Motiv nie wiederholen würde, hielt Adams den Wagen an, um eiligst seine sperrige Plattenkamera auszuladen und aufzubauen. Ihm gelang nur eine einzige Aufnahme, beim zweiten Negativ war die Sonne bereits hinter einer Wolkenbank verschwunden und „der magische Augenblick war für immer vorüber“, wie er sich erinnerte. "Moonrise, Hernandez, New Mexico" sollte Ansel Adams bekannteste Fotografie werden. Noch Jahre später bekam der Fotograf Zuschriften, in denen er gefragt wurde, ob es sich um eine Doppelbelichtung handele, was er jedoch stets verneinte.

Unglücklicherweise vergaß Adams in seiner Aufregung, das Bild zu datieren, weshalb sich Biografen und Fotohistoriker lange Zeit darüber stritten, "wann" das genaue Entstehungsdatum des legendären Fotos gewesen sein mochte. Beaumont Newhall gelang es in den 1980er Jahren mit Hilfe des befreundeten Astronomen David Elmore, das Datum der seltenen Sonnen-Mond-Konstellation mit Hilfe von Azimut-Tabellen und Landkarten an einem Computer zurückzurechnen, demzufolge sollte "Moonrise" am 31. Oktober 1941 zwischen 16.00 und 16:05 Uhr Ortszeit aufgenommen worden sein. Jüngste Nachforschungen datieren das Entstehungsdatum jedoch auf den 1. November 1941, 16:49 Uhr MST.

Nachdem Adams die Kali-Minen der "U.S. Potash Company" fotografiert hatte, reiste er weiter in den Carlsbad-Caverns-Nationalpark, um mit den Fotografien für das US-Innenministerium zu beginnen. Während der Reise entstanden Aufnahmen von den Felssiedlungen der Anasazi im Mesa-Verde-Nationalpark oder von den Adobe-Pueblos der Acoma, außerdem empfand Adams auf seine Weise die historischen Fotografien von Timothy H. O’Sullivan nach, die dieser bereits 1873 im Canyon de Chelly angefertigt hatte.

Im Sommer 1942 setzte der Fotograf seine ausgedehnte Fotoexkursion für die Regierung durch diverse Nationalparks fort: Er fotografierte die Geysire des Yellowstone-Nationalpark und machte unter anderem Station im Rocky-Mountain-Nationalpark im Glacier-Nationalpark und schließlich im "Mount McKinley National Park" (heute Denali-Nationalpark). Zu Adams’ Missfallen wurde das Wandbildprojekt im Juli 1942 unter dem Druck des Zweiten Weltkriegs eingestellt und nach dem Krieg nicht wieder aufgenommen.

Nach Kriegsende bewarb sich Adams bei der "John-Simon-Guggenheim-Stiftung" um ein "Fellowship" (Stipendium), damit er seine Arbeit in den Nationalparks für ein eigenes Buchprojekt fortsetzen konnte. Adams erhielt das "Guggenheim-Stipendium" zweimal: 1946 und 1948. Das Stipendium ermöglichte ihm unter anderem eine Flugreise in das südliche Alaska, wo er zusammen mit einer Geologengruppe Aufnahmen der Eisfelder rund um die "Glacier Bay" im Glacier-Bay-Nationalpark anfertigte. Aus dem gesamten Nationalpark-Material extrahierte er das Portfolio "The National Parks and Monuments" und den Fotoband "My Camera in the National Parks", der 1950 veröffentlicht wurden.

Im Sommer 1943 wurde Adams von Ralph Merrit, dem neuernannten Leiter des Internierungslagers Manzanar, mit dem Projekt betraut, die missliche Situation der Nisei, im Lande geborene amerikanische Bürger japanischer Herkunft, die nach dem Angriff auf Pearl Harbor im Rahmen eines Internierungsprogramms der Regierung in einsame Gegenden zwangsumgesiedelt worden waren, zu dokumentieren. Ansel Adams entschied sich im Spätherbst des Jahres, nach Manzanar zu fahren. Er kannte die verlassene Gegend im Owens Valley bereits aus den Erzählungen Mary Austins und von Dorothea Langes Dokumentarfotografien, die sie hier ein Jahr zuvor angefertigt hatte. Der Besuch in der trostlosen Baracken-Siedlung berührte Adams zutiefst. In Manzanar entstand eines seiner bekanntesten Bilder, "Winter Sunrise, The Sierra Nevada, from Lone Pine, California" (1943).

Die Fotoreportage aus dem Internierungslager Manzanar, die Adams’ einziger Beitrag mit direktem Bezug zum Krieg blieb, veröffentlichte er 1944 als Buch "Born Free and Equal: The Story of Loyal Japanese-Americans". Das Werk erhielt positive Kritiken und führte im Frühjahr 1945 die Bestsellerliste des "San Francisco Chronicle" an.
Mit Dorothea Lange arbeitete Ansel Adams in den 1950er Jahren gemeinsam an mehreren Geschichten. Lange wohnte ebenfalls in der Bay Area von San Francisco und hatte sich in den 1930er Jahren im Auftrag der unter Roosevelt gegründeten "Farm Security Administration" "(F.S.A.)" mit ihren eindringlichen Bildern des amerikanischen Landlebens ein Renommee als sozialdokumentarische Fotografin erworben. Adams’ Beziehung zu Lange war trotz mancher Meinungsverschiedenheit freundschaftlich, und die beiden tauschten sich oft angeregt über fotografische und politische Ansichten aus. Adams sah in Langes Werk eine gewisse Sympathie für den Trotzkismus, was die Fotografin jedoch niemals direkt aussprach. „Sie war integer in ihren Überzeugungen und voller Skepsis gegenüber der selbstgefälligen ‚Good Old Boy‘-Haltung, die sie vorherrschend in Industrie und Politik erkannte.“ erinnerte sich Adams in seinen Memoiren an die Fotografin. Die beiden unterhielten über lange Jahre eine angeregte Korrespondenz.

Im Auftrag des "Life-Magazines" realisierten Adams und Lange im Sommer 1953 den Fotoessay "Three Mormon Towns" über die zurückgezogen lebenden Mormonen im Südwesten von Utah.

1948 lernte Adams den Physiker und Fotopionier Edwin Herbert Land bei einer Party in dessen Haus in Cambridge, Massachusetts, kennen. Land hatte gerade sein neuartiges Polaroid-Land-Trennbildverfahren vorgestellt und lud den neugierig gewordenen Adams am folgenden Tag in sein Labor ein, wo Land ein Sofortbild-Porträt von Adams anfertigte. Es war Adams erste Begegnung mit dem Polaroid-Verfahren. Da Land zu diesem Zeitpunkt ausschließlich Wissenschaftler und Theoretiker, aber keinen Kreativen mit fototechnischen Kenntnissen zu seinem Umfeld zählte, engagierte er Adams kurzerhand als technischen Berater. Adams erhielt neben einer Sofortbildkamera mit den dazugehörigen Filmen die Aufgabe, sich mit der Qualität und Leistung des Materials auseinanderzusetzen, woraus sich schließlich eine lebenslange geschäftliche Beziehung zwischen Adams und Polaroid ergab, sowie eine enge Freundschaft zwischen Land und Adams.

Ab den 1950er Jahren fertigte Ansel Adams zahlreiche Fotografien auf Polaroid-Material an. Eine bekannte Aufnahme von 1968, "El Capitan, Winter Sunrise", die den Monolithen "El Capitan" in Yosemite zeigt, entstand auf Polaroid Type 55 P/N, einem hochauflösenden Positiv/Negativ-Schwarzweißfilm.

1940 übernahm Ansel Adams kurzfristig die fotografische Abteilung an der "Art Center School" (heute "Art Center College of Design") in Los Angeles und leitete überdies zahlreiche Workshops „vor Ort“ im Yosemite-Nationalpark. 1946 wurde er vom damaligen Präsidenten der "San Francisco Art Association", Ted Spencer, gebeten, eine fotografische Abteilung an der "California School of Fine Arts", dem heutigen San Francisco Art Institute, einzurichten. Adams sagte begeistert zu und begann umgehend mit der Planung von drei Dunkelkammern und einem großen Demonstrations- und Unterrichtsraum für die Universität. Doch die Kosten für Adams’ Projekt überschritten bei weitem den Etat, was den Unmut der anderen Lehrbereiche erweckte. „Die Maler, Bildhauer, Graphiker und Töpfer erhoben sich wie ein Mann im Zorn. Photographie sei keine Kunst, behaupteten sie, und hat an einer Kunstschule nichts zu suchen …“ erinnerte sich Adams.

Mit Spencers Unterstützung konnte er dennoch seine Lehrtätigkeit aufnehmen und überzeugte schließlich seine Kritiker von den sowohl handwerklichen wie künstlerisch-ästhetischen Aspekten, die mit der Fotografie verbunden sind. Adams fotografische Abteilung war eine der ersten, die das Medium Fotografie in einer Institution der schönen Künste lehrte. Doch nach bereits einem Jahr, mit Erhalt des Guggenheim-Stipendiums, gab Adams seine Tätigkeit am Institut aus Zeitgründen wieder ab. Moralisch in der Pflicht fand er schließlich in Minor White einen gleichwertigen Nachfolger.

Ansel Adams veranstaltete noch bis ins hohe Alter zahlreiche Workshops, in denen er über sein Zonensystem und andere Erkenntnisse in Theorie und Praxis der Fotografie dozierte. In diesem Kontext entstanden zahlreiche Lehrbücher zur Fototechnik, wie "Camera and Lens" und "The Negative" (beide 1948), "The Print" (1950) oder "Natural Light Photography" (1952) und " Artificial Light Photography" (1956).

Nachdem Ansel und Virginia Adams seit den frühen 1920ern abwechselnd in San Francisco und in Yosemite gelebt hatten, zog das Paar 1961 nach Carmel-by-the-Sea um, wo Dave McGraw, ein Freund der Adams’, eine kleine Kolonie von Künstlern versammelt hatte. Die Überlegung, nach Carmel zu ziehen, war für die Adams’ vor allem logistischer Natur: Ansel hatte noch immer in New Mexico zu tun, während Virginia nach wie vor "Best’s Studio", das ehemalige Ladenatelier ihres Vaters in Yosemite, leitete. Dick McGraw bot den Adams ein drei Hektar großes Grundstück am "Wild Cat Hill" in den Carmel Highlands an. Schweren Herzens trennte sich Adams von seinem Elternhaus in den Dünen von San Francisco, in dem er seit dessen Erbauung 1903 zeitweise gelebt hatte. Adams’ Mutter Olive war bereits 1950 verstorben, der Vater Charles war ihr nur ein knappes Jahr später gefolgt. Mit Hilfe des Architekten Ted Spencer ließ sich Adams ein Haus nach seinen Vorstellungen entwerfen, dessen Zentrum eine große Dunkelkammer bilden sollte, die von allen Bereichen des Hauses zugänglich war.

In Carmel begründete Adams 1967 zusammen mit Morley Baer, Beaumont und Nancy Newhall sowie Brett Weston die "Friends of Photography" als gemeinnützigen Verein, der sich der Förderung der kreativen Fotografie annahm und Ausstellungen und Ausstellungsräume organisierte. Binnen weniger Jahre wurde der Verein zu einer international bekannten Institution mit mehreren tausend Mitgliedern. Nach Adams’ Tod 1984 zog die Gruppe von Carmel nach San Francisco und eröffnete 1989 das "Ansel Adams Center for Photography" bei den Yerba Buena Gardens.

Mit zunehmendem Alter beschränkte Ansel Adams seine Tätigkeiten auf kleinere Workshops für die "Friends of Photography", auf die Publikation von Fotobüchern und auf Beiträge in Fachzeitschriften sowie auf die Reproduktion seiner bekanntesten Fotografien, die mittlerweile zu begehrten Sammlerobjekten geworden waren. Überdies kümmerte sich Adams seit der Ära Nixon verstärkt um politische Initiativen zum Erhalt der Nationalparks. 1975 machte er zu diesem Zweck eine Eingabe in Form eines Memorandums beim amtierenden US-Präsidenten Gerald Ford.

In den 1970er Jahren begann der Fotograf seinen Nachlass zu regeln und gründete zu diesem Zweck zwei "Trusts": zum einen den "Ansel Adams Publishing Rights Trust", der alle künftigen Veröffentlichungen und Wiedergaberechte kontrollieren sollte und zum anderen den "Ansel Adams Family Trust", in den die Nettoerlöse des "Ansel Adams Publishing Rights Trust" fließen sollten und der ausschließlich der Familie von Adams, nach Ansel und Virginias Ableben, den Kindern Anne und Michael zugutekommt. Überdies verfügte Adams, dass seine Fotografien nicht mehr mit einem kommerziellen Produkt in Zusammenhang gebracht werden sollten.

Mitte der 1970er Jahre fertigte Adams seine letzten Auftragsarbeiten an und gab öffentlich bekannt, dass er ab dem 31. Dezember 1975 keine Bildbestellungen mehr annehmen würde. Mit Adams’ Verlautbarung begannen die Preise seiner Fotografien auf Kunstauktionen kontinuierlich zu steigen. Die Jahre 1976, 1977 und 1978 verbrachte er größtenteils damit, noch offene Bestellungen auszuführen. In den letzten Lebensjahren sichtete der Fotograf ungefähr 40.000 Negative. Noch zu Adams Lebzeiten erreichte "Moonrise, Hernandez, New Mexico" bei einer Auktion den Rekordpreis von 71.000 US-Dollar, der höchste Preis, der bis dahin für eine Fotografie bezahlt worden war.

Obwohl Adams’ Arbeiten bereits zu seinen Lebzeiten in internationalen Schauen gezeigt wurden, reiste er selbst erst 1974 zum ersten Mal nach Europa, um in Arles eine Ausstellung seiner Arbeiten zu besuchen und Vorträge zu halten. Dabei traf er mit Fotografenkollegen wie Bill Brandt, Brassaï, Henri Cartier-Bresson und Jacques-Henri Lartigue zusammen. 1976 wiederholte er seine Vortragsreise nach Arles, ein weiteres Mal besuchte er 1979 das Victoria and Albert Museum in London. Im selben Jahr unterzog sich Adams, der seit den frühen 1970er Jahren an zunehmenden Herzproblemen litt, einer Operation, bei dem ihm ein dreifacher Bypass gesetzt wurde. Eine vierte Vortragsreise nach Europa 1982 lehnte er ab. 1979 organisierte John Szarkowski, der Nachfolger Edward Steichens im Kuratorium der fotografischen Abteilung des MoMA, die große Wanderausstellung "Ansel Adams and the West", die 153 Landschaftsbilder des Fotografen zeigte. Die Ausstellungseröffnung ging mit Adams’ Buchveröffentlichung "Yosemite and the Range of Light" einher. Die Schau war ein großer Erfolg und wurde von dem Kunstkritiker Robert Hughes mit einer Titelgeschichte auf dem Time Magazine bedacht.
Im Jahre 1981 wurde Adams als zweiter nach Lennart Nilsson von der "Erna und Victor Hasselblad Stiftung" mit der Hasselblad-Goldmedaille geehrt. Die Verleihung durch König Carl XVI. Gustaf von Schweden fand im MoMA statt. Den schwedischen Fotografen und Erfinder Victor Hasselblad hatte Adams bereits 1950 bei einem Besuch in New York kennengelernt. Hasselblad hatte Adams damals gebeten, eine seiner ersten Kameras, die Mittelformatkamera "Hasselblad 1600F", auszuprobieren. Von diesem Zeitpunkt an zählten Hasselblad-Modelle zu Adams bevorzugten Kameras.

Ansel Adams’ achtzigster Geburtstag am 20. Februar 1982 wurde mit zahlreichen, von den "Friends of Photography" organisierten Ausstellungen, Retrospektiven und Festivitäten gefeiert. Zu Adams’ besonderer Freude gab der von ihm sehr verehrte russische Pianist Wladimir Aschkenasi ein privates Klavierkonzert im Wohnhaus der Adams in Carmel.

Adams starb am 22. April 1984 im Alter von 82 Jahren an Herzversagen. Ihm zu Ehren wurde noch im selben Jahr das Wildnisgebiet "Minarets Wilderness", das die Bergkette "The Minarets" in der Sierra Nevada umgibt, in "Ansel Adams Wilderness" umbenannt. Seine Ehefrau Virginia Best Adams verstarb am 29. Januar 2000.

Ansel Adams gilt als ein Vertreter der "„straight photography“", der „reinen Fotografie“, die, der Tradition des Realismus in der Malerei folgend, einer strengen Bildästhetik verpflichtet ist und sich, gemäß der von der Gruppe f/64 postulierten Dogmatik, demonstrativ gegen den seinerzeit beliebten Pictorialismus richten sollte, der mit seinem sentimentalen Stil als geschmäcklerisch „unrein“ empfunden wurde. Seine Anfang der 1930er Jahre entstandenen Arbeiten standen noch sehr unter dem Einfluss der Gruppe f/64 und orientieren sich in der Darstellung reiner Formen an Paul Strand oder Edward Weston. Erst in der Folgezeit, ab Mitte der 1930er Jahre, löste sich Adams von den einengenden Theorien der Gruppe f/64 und fand von einer eher zweidimensionalen Gestaltung zu einem plastischeren Bildaufbau. Wenngleich Adams’ Landschaften und die Präzision ihrer Umsetzung fotohistorisch wegweisend waren, fanden Fotografen der Neuen Sachlichkeit im deutschsprachigen Raum wie beispielsweise Albert Renger-Patzsch zu einer ähnlich „reinen“ Bildsprache.

In seinen zahlreichen Schriften, Vorträgen und Workshops stellte Adams seine Verfahrensweisen zur Erstellung und Verfeinerung einer perfekt durchgezeichneten „wohl komponierten“ Fotografie dezidiert dar und demonstrierte, welche Möglichkeiten die reine Fotografie als künstlerisches Ausdrucksmedium bieten kann. Von einer klassischen Musikausbildung her kommend, übertrug Adams seine Kenntnisse der musikalischen Komposition auf die Komposition in der Kunst und erklärte die Fotografie legitim als „den schönen Künsten zugehörig“. Adams betrachtete (und bezeichnete) dabei die Kamera mit ihrem Zubehör aus verschiedenen Objektiven und Filtern als äquivalent zur Musik als „Instrument“.

Im Unterschied zur schnelllebigen Reportagefotografie wie zur aufkommenden Schnappschussfotografie, die spätestens seit Einführung der handlichen Kodak-Fabrikate ("„You press the button – we do the rest“" – "„Sie drücken auf den Knopf, wir erledigen den Rest“", so der damalige Slogan der Firma) und der Kleinbildformate oft zu einer maschinellen Beliebigkeit des Dargestellten führte, konzentrierte sich Adams bereits vor Ort auf eine bestimmte „vorausgeahnte“ Idealkomposition, die er visualisierte und dem Abzug schließlich im aufwändigen Dunkelkammer­prozess mittels Tonwert­korrekturen die gewünschte Präsentationsform verlieh, die er selbst als ausdrucksstarkes „feines Bild“ bezeichnete. Diese „Vorausahnung“ betonte Adams in Bezug auf seinen Fotografenkollegen Henri Cartier-Bresson, der zwar als Schnappschussfotograf schlechthin gilt, aber trotz dieser Schnelligkeit die bereits im Unterbewusstsein vorhandene Komposition des Bildes im „entscheidenden Moment“ visualisierte und somit zum optimalen Ausdruck fand.

Gemeinsam mit dem Fotografen und College-Dozenten Fred Archer entwickelte und formulierte Ansel Adams gegen Ende der 1930er Jahre das berühmte Zonensystem. Dem Verfahren, das Adams anschließend weiterentwickelte, liegt eine Artikelserie in der Fachzeitschrift "U.S. Camera" zugrunde. Mit Hilfe des Zonensystems versuchte Adams, den Kontrastumfang des Motivs so geschickt auf den (in aller Regel deutlich geringeren) Kontrastumfang des Schwarzweißfilms zu übertragen, dass ein natürlicher Bildeindruck entstand. Das Ziel waren technisch perfekte, sauber durchgezeichnete Negative, die sich gut vergrößern ließen. Das bedeutet aber nicht, dass er Manipulationen in der Dunkelkammer ablehnte. Das Negativ war für ihn nur eine Zwischenstufe auf dem Weg zu dem in seinem Kopf bereits fertig existierenden Bild – nur musste diese Zwischenstufe höchsten Ansprüchen genügen, damit er am Ende im fertigen Abzug genau seine Vorstellung realisieren konnte. In Anlehnung an die Musik fasste er das Negativ als Partitur auf, doch erst der Abzug "(Print)", war die Interpretation und das vollendete Werk.

Adams’ Zonensystem wurde von Fotografen und Fachpresse ambivalent aufgenommen; manche empfanden die Methode als hilfreich, um gestalterische Möglichkeiten unter dem Aspekt der „kalibrierten Aufnahme“ zu erweitern, Kritikern des Zonensystems und Befürwortern der Schnappschussfotografie war das Prinzip zu didaktisch, zu umständlich und wenig praxisnah.

Seine Arbeitsweise legte Adams dezidiert in zahlreichen Fachbüchern vor, in denen er oft anhand der Entstehungsgeschichte eines ausgewählten Werkes auf technische Aspekte wie beispielsweise Belichtungszeiten, verwendete Geräte, Filter, Filmmaterialien oder die anschließenden Arbeiten in der Dunkelkammer, respektive im Fotolabor, eingeht.
Adams arbeitete überwiegend mit "Korona-" und später "Linhof"-Großformat(Fach-)kameras sowie ab den 1950er Jahren auch mit "Hasselblad"-Mittelformatkameras auf Schwarzweiß-Filmmaterial. Bis etwa Anfang der 1930er Jahre verwendete er dabei die üblichen orthochromatischen Filme, weshalb manche Aufnahmen, die bei blauem Himmel aufgenommen worden waren, ungefiltert relativ helle Resultate zur Folge hatten. Um den Himmel dramatisch dunkel wirken zu lassen, verwendete der Fotograf bei panchromatischen Filmen Farbfilter (zumeist "Wratten No. 29"-Rotfilter). Deutlich wird dies beispielsweise bei "Monolith, The Face of Half Dome" von 1927.

In seinem eigenen Fotolabor benutzte Adams einen speziell angefertigten Horizontalvergrößerer, der auf einer alten umgebauten Plattenkamera beruhte. Das Gerät erlaubte es ihm, auch seine frühen Großformatnegative, zum Teil auf Glasplatten im 8×10-Zoll-Format, zu vergrößern.

Obwohl der Trend bereits in den 1930ern zum Großformat ging, fertigte Adams zu Ausstellungszwecken oft nur Kontaktabzüge seiner Negative im Format 20×25 cm an, die er in weißen Passepartouts präsentierte. Zur Kontrast­erhöhung und um eine möglichst hohe "Archivfestigkeit" zu erreichen, tonte Adams die Abzüge zumeist mit einer direkten Selen­tonung.

Alfred Stieglitz brachte Adams auf die Idee, die Werke optimalerweise vor einer neutralen Wand in einer Mischbeleuchtung aus indirektem Kunstlicht und gedämpften Tageslicht zu zeigen, um die Wirkung zu steigern – eine Präsentationsart, wie sie heutzutage in den White Cubes üblich ist.

Weniger bekannt ist, dass Adams auch Farbaufnahmen machte: Während seines fotografischen Lebens fertigte er über 3.000 Fotografien auf Farb-Diafilm. Die Fotografien entstanden überwiegend in den 1940er und 1950er Jahren teilweise als Testaufnahmen für den von Kodak neu entwickelten Kodachrome-Film. Als Adams 1984 starb, hatte er bereits ein Buch über die Farbfotografie in Planung. Die Thematik beschäftigte ihn, wenn auch mit Unbehagen, bereits seit den 1950er Jahren. In den 1980er Jahren gab er zu, dass, könnte er jetzt noch einmal als junger Fotograf beginnen, er wohl in Farbe fotografieren würde, „doch eigentlich“, so Adams „mag ich die Farbfotografie nicht besonders. Das ist nicht mein Fall.“

Auf die Frage, ob er in Schwarzweiß arbeite, weil vielleicht sein Farbsehen gestört sei, antwortete er, dass er sein Farbsehen habe überprüfen lassen und es in Ordnung sei. Er bevorzuge Schwarzweiß, weil er bei diesem Prozess eine größere Kontrolle habe. Eine Vielzahl seiner kommerziellen Auftragsarbeiten entstand in Farbe. Durch seine Bekanntschaft mit Edwin Land hatte er auch Gelegenheit, zahlreiche neue Sofortbildmaterialien zu testen, mit denen er beeindruckende Bildergebnisse erzielte.

Ansel Adams zählte bereits zu Lebzeiten zu den wichtigsten US-amerikanischen Fotografen des 20. Jahrhunderts. Sein Name, der mittlerweile untrennbar mit der fotografischen Dokumentation und der Bewahrung der Nationalparks und National Monuments im Westen der USA verbunden ist, wurde gleichermaßen zum Synonym wie zum Etikett für eine technisch versierte, hochqualitative Natur- und Landschaftsfotografie, die bereits zu seinen Lebzeiten umfangreich kommerzialisiert wurde.

Adams verbrachte einen Großteil seines Lebens in den US-amerikanischen Nationalparks und Indianerreservaten, in denen er nicht nur als Fotograf wirkte, sondern sie mit seiner Arbeit, seinen Publikationen und in seinen Workshops unterstützte. Seine zahlreichen Schriften weckten schnell ein öffentliches Interesse an den bis dahin noch unbekannten Wildnisgebieten des Westens.

Adams’ 1939 erstveröffentlichtes Werk "Sierra Nevada: The John Muir Trail" hatte einen maßgeblichen Einfluss auf den damaligen Innenminister Harold Ickes und fügte sich in den regionalistischen Aspekt des staatlichen Programms der „ökonomischen Erneuerung“ unter Präsident Franklin D. Roosevelt ein, was unter anderem 1940 zur Gründung der "Sequoia-&-Kings-Canyon-Nationalparks" führte. 1968 wurde Adams vom National Park Service (NPS) für seine Verdienste mit dem "Conservation Service Award", der höchsten zivilen Auszeichnung einer Behörde des US-Innenministeriums, ausgezeichnet.

Für den Sierra Club, dessen Vorstandsmitglied Adams von 1934 bis 1971 war, fertigte der Fotograf zahlreiche Fotoreportagen mit begleitenden Essays an, die in der Klubzeitschrift "Sierra Club Bulletin" erschienen und die maßgeblich zur touristischen Erschließung und zur wirtschaftspolitischen Bedeutung der Anfang des vorigen Jahrhunderts noch unberührten Naturreservate beitrugen. Geschah dies anfangs noch unter einfachen Naturschutzaspekten, setzte mit Beginn des Zweiten Weltkriegs eine Erschließung und Popularisierung der Region ein, die spätestens in den 1960er Jahren zu einer expansiven Kommerzialisierung führte, als Teile der Naturgebiete für Grundstücksspekulanten freigegeben und Konzessionen zum Bau von Kraftwerken erteilt wurden. Darüber kam es schließlich zu Differenzen, in deren Folge Adams 1971 als Vorstandsmitglied zurücktrat.

In einer Neuauflage seines vielbeachteten Werks "Geschichte der Photographie" "(History of Photography: From 1839 to the Present)" betont der Fotohistoriker und Kurator Beaumont Newhall, ein Zeitgenosse und Freund von Adams, die technische Qualität von Adams’ Arbeiten, die „bereits 1936 in seiner Ausstellung in Stieglitz’ Galerie "An American Place" von einer Sensibilität und aufrichtigen Direktheit waren, wie man sie nur selten antraf.“ Newhall konstatierte: „Adams hat in seiner Photographie, seinen Schriften und seiner Lehrtätigkeit hervorragend demonstriert, welche Möglichkeiten die reine Photographie als Ausdrucksmedium bietet.“ und verweist auf Adams’ Experimentierfreude, seine technische Meisterschaft, und sein „durchgeistigtes Gespür für die unberührten Regionen der Erde, Fähigkeiten, mit denen der Fotograf großartige Landschaftsaufnahmen geschaffen hat.“

1979 widmete der Kunstkritiker Robert Hughes im "Time Magazine" dem Fotografen eine Titelstory und betonte darin, dass kein anderer lebender Fotograf mehr dazu beigetragen habe, den Unterschied zwischen dokumentarischem und ästhetischem, beziehungsweise „emotionalen“ Gebrauch der Fotografie festzustellen. Der Fotohistoriker John Szarkowski – von 1962 bis 1991 Direktor der fotografischen Abteilung des MoMA – charakterisierte Adams anlässlich der von ihm kuratierten Ansel-Adams-Retrospektive, "Adams at 100", die 2003 im MoMA in New York und im SFMOMA in San Francisco gezeigt wurde, in einem Interview der "New York Times": „Ein Ziel der Ausstellung ist es, Adams von dem Image des grünen Sozialrealisten zu befreien. Obwohl Adams ein lebenslanges Interesse am Erhalt der Wildnis hatte, entstanden seine besten Arbeiten aus Gründen, die wesentlich persönlicher und mystischer waren […] er wäre jedoch entrüstet, wenn ihm jemand unterstellen würde, dass er dabei im traditionellen Sinne annähernd religiöse Empfindungen gehabt hätte. In seinen privaten Briefen wird deutlich, dass seine Erfahrungen der natürlichen Welt im Grunde mystische Erfahrungen waren und dass es sein einziges wirklich nachhaltigstes künstlerisches Problem war, den physikalischen Beweis für diese Erfahrung zu erbringen.“

Im Vergleich zu sozialkritischen Zeitgenossen relativierte Szarkowski Adams’ mutmaßlich geringere Bedeutung für die sozialdokumentarische Fotografie in den USA: „Bis etwa 1960 wurde die Tatsache, dass er Bäume und schneebedeckte Berge fotografierte, von vielen, die meinten, Fotografie sollte eher menschliches Leid dokumentieren, als ‚moralische Verfehlung‘ empfunden. Erst später wurde er zum Helden für etwas, was er nie beabsichtigt hatte, als er seine besten Arbeiten machte.“

Im Kontext der Retrospektive beschrieb die New York Times Adams hinsichtlich der Kommerzialisierung seines Namens als „Amerikas meist geliebten Fotografen, […] dessen Fotografien von donnernden Felsen und glitzernden Hainen durch einen endlosen Strom aus Postern, Kalendern, Büchern und Bildschirmschonern populärer sind denn je…“ Der Kunsthistoriker John Pultz betrachtete Adams in einem Dossier über die „Neue Fotografie in den Vereinigten Staaten“ als Fotografen mit handwerklicher und technischer Perfektion, der „sich in den 1940er und 1950ern fast ausschließlich der Darstellung majestätischer Landschaftspanoramen widmete, während er in der Vielfalt seiner Motive in den 1930er Jahren bereits jene Präzision erreichte, die Strand und Weston in den 1920ern etabliert hatten.“

In Abkehr von dem kurzlebigen, der Malerei verbundenen und somit als „schwächlich sentimental“ verachteten Pictorialismus führte Adams im Gefolge von Edward Weston und der gemeinsam begründeten "Gruppe f/64" eine streng komponierte Bildsprache in die Fotografie ein, die sich durch ihre präzise, scharf fokussierte Wiedergabe „reiner Formen“ der Realität auszeichnete. Bei zeitgenössischen Kunstschaffenden und Kritikern stießen Adams und Gruppe f/64 damit auf Ablehnung. Eine kulturelle ästhetische Aufwertung des Mediums Fotografie als eine neue und legitime Erscheinungsform der „schönen Künste“ wurde allgemein bis Mitte des 20. Jahrhunderts nur zögerlich aufgenommen, weshalb Adams’ Ernennung zum Hochschullehrer für Fotografie 1946 zunächst auf Kritik in den etablierten Fachbereichen stieß.

In einem Nachruf auf den Fotografen konstatierte "Der Spiegel" 1984, dass Adams eine Verehrung zuteilwurde, „die ihn an die Seite der großen Schriftsteller, Maler und Komponisten seiner Generation stellte: Der Amerikaner Ansel Adams hatte wesentlich dazu beigetragen, die Photographie als eigenständige Kunstform zu etablieren.“

Der Kunstfotograf und Pictorialist William Mortensen (1897–1965) gehörte in den 1930er Jahren zu Adams schärfsten Kritikern. Nach Mortensens Tod verhinderte Adams mutmaßlich die Archivierung seines fotografischen Nachlasses am "Center for Creative Photography" der University of Arizona und Mortensen geriet in Vergessenheit. Die neuzeitlich vom "Center for Creative Photography" herausgegebene kunsthistorische Rehabilitierung "William Mortensen: A Revival" lässt dabei auf ein Bestreben von Adams, den Gegenspieler aus den Annalen der Fotogeschichte zu tilgen, schließen. In seiner Autobiografie gab Adams den langjährigen Briefwechsel mit Mortensen nur einseitig wieder.

Eine deutlich kritische Position nimmt der Autor Jonathon Green in seinem Werk "American Photography: A Critical History 1945 to the Present" ein, der Adams puritanisch-technokratische Charakteristika zuordnet und ihn „als archetypischen amerikanischen Ingenieur des 19. Jahrhunderts“ bezeichnet, der, „wie alle großen Baumeister seiner Zeit, sein Wissen zum ästhetischen und spirituellen Wohlergehen der Menschheit einsetzte“. Adams’ Werk ist von puritanischem Korn: streng und konservativ mit der Obsession der technologischen Kontrolle, was einen anderen fundamentalen amerikanischen Wesenszug zeigt, wie man ihn schon bei Stieglitz und Steichen gesehen hat: die aggressive, akquisitorische, erfinderische Beschäftigung mit Technik und die praktische Nutzung der neuen Technologien.

„Der Formalismus, der Adams’ Arbeiten durchdringt, bezieht sich direkt auf seinen Glauben an die Technik. Für Adams ist Technik erlösend. Er könnte guten Gewissens sogar Autos oder Fernseher verkaufen. Tatsächlich scheint es, dass er sich mehr auf die Technik als auf das Sehen verlässt. Von allen großen amerikanischen Fotografen ist er derjenige mit dem uneinheitlichsten Werk. Wenn seine Arbeit glückt, ist sie atemberaubend, wenn sie jedoch misslingt, ist sie nichts anderes als eine Fingerübung im Zonensystem – süßlich und dekorativ.“

Im Jahre 2000 erwarb der Anstreicher Rick Norsigian aus dem kalifornischen Fresno zwei Schachteln mit 65 Glasplattennegativen zu 45 US-Dollar, die Aufnahmen des Yosemite-Parks zeigen. Laut einem von Norsigian beauftragten Gutachten sollen die Negative von Adams stammen. Die Beschriftung der Papierhüllen der Negative soll zudem mit der Handschrift von Adams’ Frau Virginia identisch sein und die Negative aus den 1920er und 1930er Jahren stammen. Norsigian trat 2007 über die Los Angeles Times mit dem Fund an die Öffentlichkeit. In einer Pressekonferenz im Jahr 2010 gab der Kunstexperte und Galerist David W. Streets an, dass es sich „um ein fehlendes Glied in der Geschichte und dem Werk von Ansel Adams“ handele und bezifferte den Wert des Fundes auf etwa 200 Millionen Dollar (rund 153,5 Millionen Euro). Ansel Adams’ Erben und Nachlassverwalter bezweifelten hingegen die Echtheit des Fundes und bemerkten, sollten die Negative tatsächlich echt sein, hätten sie keinen derart großen Wert, da nur von Adams selbst angefertigte Originalabzüge hohen Sammlerwert besäßen. Spätere Vergleiche ergaben Ähnlichkeiten mit den frühen Fotografien eines sonst eher unbekannten Porträtfotografen namens Earl Brooks. Im März 2011 schlossen Norsigian und der Adams Trust einen außergerichtlichen Vergleich, der Norsigian das Recht abspricht, Abzüge der fraglichen Negative weiterhin als „echte“ Adams-Fotografien zu vermarkten; außerdem muss er darauf hinweisen, dass seine Negative nicht vom Adams-Trust autorisiert sind.


Seit 1971 verleiht der Sierra Club den "Ansel Adams Award for Conservation Photography" an Fotografen, die sich mit ihrer Arbeit vornehmlich für den Erhalt und den Schutz der Natur eingesetzt haben. Ein bekannter Preisträger ist der Niederländer Frans Lanting.

Als Klassiker der Landschaftsfotografie gelten:


Porträtfotografien:

Andere Motive:

Eine Auswahl seiner Arbeiten aus den Nationalparks sowie die Fotografien aus dem Internierungslager Manzanar stiftete Ansel Adams 1965 dem Bildarchiv der Library of Congress.

"postum:"

"Lehrbücher:"






</doc>
<doc id="8385" url="https://de.wikipedia.org/wiki?curid=8385" title="801">
801












</doc>
<doc id="8387" url="https://de.wikipedia.org/wiki?curid=8387" title="Georg Cantor">
Georg Cantor

Georg Ferdinand Ludwig Philipp Cantor (* in Sankt Petersburg; † 6. Januar 1918 in Halle an der Saale) war ein deutscher Mathematiker. Cantor lieferte wichtige Beiträge zur modernen Mathematik. Insbesondere ist er der Begründer der Mengenlehre und veränderte den Begriff der Unendlichkeit.

Cantor wurde als Sohn von Georg Woldemar Cantor, einem wohlhabenden Kaufmann und Börsenmakler, und Marie Cantor, geb. Böhm, in St. Petersburg, der damaligen Hauptstadt Russlands, geboren. Sein Vater war in Kopenhagen geboren und in jungen Jahren mit seiner Mutter nach St. Petersburg gekommen, wo er in der dortigen deutschen lutherischen Mission aufgezogen worden war. Nach Aussagen Cantors stammte der Vater aus einer sephardischen Familie und wurde erst in Sankt Petersburg lutherisch getauft. Die Mutter war in St. Petersburg geboren, römisch-katholisch, und stammte aus einer bekannten österreichischen Musikerfamilie. Die Großeltern mütterlicherseits, Franz Böhm und Marie Böhm, geb. Morawek, waren beide Berufsmusiker (Violinisten), Franz Böhm war Kapellmeister der Kaiserlichen Oper in Sankt Petersburg und der Bruder des Geigers Joseph Böhm.

Die Kinder wurden im lutherischen Glauben und in einem deutschen kulturellen Umfeld aufgezogen. Der Vater war sehr fromm und instruierte seinen Sohn in religiösen Dingen. Zeit seines Lebens blieb Georg Cantor ein tief religiöser Mensch. Als er 11 Jahre alt war, siedelte die Familie wegen des schlechten Gesundheitszustandes des Vaters 1856 von St. Petersburg in das mildere Klima der Kurstadt Wiesbaden und etwas später nach Frankfurt am Main über.

Nach dem Schulabschluss („mit Auszeichnung“) 1860 in Darmstadt studierte er an der Universität Zürich und an der Universität Göttingen und wurde 1867 an der Universität Berlin bei Ernst Eduard Kummer promoviert. Zu seinen Lehrern zählten Karl Weierstraß, Ernst Eduard Kummer und Leopold Kronecker. Nach der Promotion lehrte und arbeitete er von 1869 an bis zu seinem Lebensende in Halle, zunächst als Privatdozent, seit 1872 als Extraordinarius und seit 1877 bis zu seiner Emeritierung im Jahr 1913 als ordentlicher Professor. In Halle verkehrte er unter anderem freundschaftlich mit Edmund Husserl, dem Begründer der Phänomenologie.

1874 heiratete er Vally Guttmann, mit der er zwei Söhne und vier Töchter hatte (das letzte Kind wurde 1886 geboren). Der Sohn Erich war Arzt, die Tochter Else eine Konzertsängerin und bekannte Musikpädagogin. Seine Flitterwochen verbrachte er im Harz, wo er auch intensiv mit Richard Dedekind, einem engen Freund, den er zwei Jahre zuvor während eines Urlaubs in der Schweiz kennengelernt hatte, über Mathematik diskutieren konnte.

Von 1884 an litt Cantor wiederholt an einer manisch-depressiven Erkrankung und musste sich erstmals in psychiatrische Behandlung begeben. Cantors Beschäftigung mit der Frage nach dem „wahren“ Autor der shakespeareschen Werke fällt in die erste Zeit seiner geistigen Erkrankung. Er sprach sich in mehreren Veröffentlichungen für Francis Bacon als Verfasser aus. Ähnliche Erörterungen stellte Cantor auch im Hinblick auf die Werke von Jakob Böhme und John Dee an. Dieses sehr forcierte literaturgeschichtliche Engagement wird oft als Folge seiner Geisteskrankheit betrachtet, doch war die Beteiligung an dem Rätselraten um Shakespeare allgemein sehr verbreitet, und Cantor zeigte stets an Fragen außerhalb seines Fachgebietes großes Interesse, besonders an Philosophie und (katholischer) Theologie, die für ihn in engem Bezug zu den mengentheoretischen Problemen der Unendlichkeit stand.

Bis 1899 gibt es keine weiteren Aufzeichnungen bezüglich eines Aufenthaltes in einem Sanatorium.
Kurz nach diesem zweiten Aufenthalt im Sanatorium starb Cantors jüngster Sohn plötzlich (während eines Vortrags von Cantor bezüglich der Bacon-Theorie und Shakespeare). Durch diese Tragödie verlor Cantor viel an Leidenschaft für die Mathematik und seine Depressionen traten verstärkt auf, weshalb er 1903 in einem Sanatorium behandelt wurde.

Ein Jahr später hielt Julius König auf dem Internationalen Mathematikerkongress einen Vortrag, in dem er vermeintlich beweisen konnte, dass die Mächtigkeit des Kontinuums unter den Alephs überhaupt nicht vorkommt. Als Reaktion auf diesen in seiner Wirkung als „sensationell“ empfundenen Vortrag soll Cantor sich aufgewühlt und empört darüber gezeigt haben, dass man es gewagt hatte, seine (laut seiner Aussage von Gott übermittelten) Studie widerlegen zu wollen, aber auch darüber, dass seine Töchter und Kollegen die vermeintliche Widerlegung mitanhören mussten und die damit verbundene an ihm vollzogene Demütigung. Obwohl Ernst Zermelo einen Tag später schon demonstrierte, dass Julius Königs Beweisführung falsch war, verblieb Cantor schockiert, verärgert und begann sogar, an seinem Glauben zu zweifeln. (Hinsichtlich der Reaktion Cantors auf Königs Vortrag liegen seitens der Teilnehmer des Kongresses aber auch abweichende Schilderungen vor.)

1911 wurde Cantor als einer der bevorzugten ausländischen Gelehrten zum 500. Jahrestag der Gründung der Universität St. Andrews in Schottland eingeladen. Es war gerade jene Zeit, in der Bertrand Russell das Werk Principia Mathematica veröffentlichte, ein Werk über mathematische Prinzipien, in dem Russell sich regelmäßig auf Cantors Arbeiten bezog. In der Hoffnung, Bertrand Russell bei diesem Anlass zu treffen, nahm Cantor daran teil, eine Begegnung kam aber nicht zustande. Ein Jahr später wollte dieselbe Universität Cantor den Ehrendoktortitel verleihen, aber Cantor konnte durch seine Krankheit nicht persönlich daran teilnehmen.

1913 ging Cantor in Pension, während des Ersten Weltkrieges litt er an Armut und sogar unter Mangelernährung. Die öffentliche Feier zu seinem 70. Geburtstag wurde wegen des Krieges abgesagt. Am 6. Januar 1918 starb Georg Cantor an einer Herzinsuffizienz in Halle (Saale) in jenem Sanatorium, in dem er das letzte Jahr seines Lebens verbracht hatte. Sein Grab ist auf dem Friedhof Giebichenstein in Halle (Saale) erhalten.

Sein Nachlass wird vom Zentralarchiv deutscher Mathematiker-Nachlässe an der Niedersächsischen Staats- und Universitätsbibliothek Göttingen aufbewahrt.

Cantor befasste sich zunächst mit Zahlentheorie und wandte sich in Halle unter dem Einfluss von Eduard Heine Fourierreihen zu. Er bewies 1869 die Eindeutigkeit der Darstellung von Funktionen durch trigonometrische Reihen, veröffentlicht im "Journal für die reine und angewandte Mathematik" 1870. Genauer bewies er, dass falls

für alle formula_2, dass formula_3 für alle i. Der Satz bleibt auch bei endlich vielen Ausnahmestellen x gültig (in denen die Fourierreihe nicht konvergiert oder ungleich Null ist).

Er baute beim Beweis auf den Untersuchungen von Bernhard Riemann auf und korrespondierte im Vorfeld des Beweises mit seinem Studienfreund Hermann Amandus Schwarz, der einen wichtigen Baustein des Beweises lieferte. Die Theorie der Fourierreihen war auch der Ausgangspunkt seiner Beschäftigung mit Mengenlehre, als er sich fragte, ob sein Eindeutigkeitssatz bei unendlich vielen Ausnahmestellen erhalten bleibt.

Cantor begründete in den Jahren 1874 bis 1897 die Mengenlehre, die er anfangs (1877) noch Mannigfaltigkeitslehre nannte. Er formulierte 1895 folgende oft zitierte Definition der Menge:

Cantor kam zu seiner Mengenlehre durch die Betrachtung eindeutiger (heute: „bijektiver“) Zuordnungen der Elemente von unendlichen Mengen. Er bezeichnete Mengen, für die eine solche Beziehung hergestellt werden kann, als äquivalent oder „von gleicher Mächtigkeit“, auch „gleichmächtig“. Demnach ist die Menge der natürlichen Zahlen formula_4 der Menge der rationalen Zahlen (Brüche) äquivalent, was er durch sein Diagonalisierungsverfahren zeigte. Mit seinem zweiten Diagonalargument bewies er dann, dass die Menge der reellen Zahlen mächtiger ist als die der natürlichen Zahlen. Eine Verallgemeinerung war der Satz von Cantor. Die Arbeiten waren unter den Mathematikern seiner Zeit wegen der ungeklärten Fragen hinsichtlich des „aktual Unendlichen“ und der Einführung der transfiniten Zahlen umstritten. Insbesondere geriet Cantor in einen tiefgreifenden wissenschaftlichen Gegensatz zu Leopold Kronecker. Man vermutet hierin den Grund für die Verzögerung der Publikation von Cantors Artikel "Ein Beitrag zur Mannigfaltigkeitslehre" in Crelles Journal. Diese Kontroverse zwischen Cantor und Kronecker wird als „Präludium für den späteren Streit zwischen Intuitionisten und Formalisten“ gesehen.

Cantor selbst gehörte auch zu den ersten Entdeckern der Antinomien der naiven Mengenlehre und bewies mit den beiden Cantorschen Antinomien, dass gewisse Klassen keine Mengen sind. Er ist sogar als Schöpfer der axiomatischen Mengenlehre anzusehen, denn Cantors Mengenaxiome aus Briefen von 1889/99, die allerdings erst posthum publiziert wurden, nehmen die Axiome der späteren Zermelo-Fraenkel-Mengenlehre vorweg.

Auf Cantor gehen auch die Cantorsche Paarungsfunktion (auch Nummerierungsfunktion) und der Cantorsche Algorithmus zurück.

Schließlich schuf Cantor 1870 mit der sogenannten Punktmenge die Grundlagen der Theorie der später von Benoît Mandelbrot so bezeichneten Fraktale. Die Cantorsche Punktmenge folgt dem Prinzip der unendlichen Wiederholung selbstähnlicher Prozesse. Die Cantor-Menge gilt als das älteste Fraktal überhaupt.


Die Oper "Cantor – Die Vermessung des Unendlichen" von Ingomar Grünauer widmet sich dem Leben und Werk Georg Cantors und wurde aus Anlass des 1200-jährigen Stadtjubiläums am 10. November 2006 im Opernhaus Halle uraufgeführt. Die letzte Vorstellung fand am 5. Januar 2007 statt.


Zur Zahlentheorie

Zur Analysis

Zur Mengenlehre

Sonstige



Werke

Über Cantor

Biografien


</doc>
<doc id="8390" url="https://de.wikipedia.org/wiki?curid=8390" title="Konstantinopel">
Konstantinopel

Die Stadt Konstantinopel wurde von dorischen Siedlern aus dem griechischen Mutterland um 660 v. Chr. unter dem Namen Byzantion gegründet. Am 11. Mai 330 n. Chr. machte sie der römische Kaiser Konstantin der Große zu seiner Hauptresidenz, baute sie großzügig aus und benannte sie offiziell in "Nova Roma" (Νέα Ῥώμη) um. In der Spätantike beanspruchte die Stadt auch den Rang als „Neues Rom“. Nach dem Tod Kaiser Konstantins 337 wurde die Stadt offiziell in "Constantinopolis" umbenannt. Sie war die Hauptstadt des Oströmischen Reichs und blieb dies – abgesehen von der Eroberung im Vierten Kreuzzug – ununterbrochen bis zur Eroberung durch die Osmanen 1453. Unter den Namen und war es dann bis 1923 die Hauptstadt des Osmanischen Reichs.

Spätestens ab 1930 setzte sich der Name "Istanbul," der bereits im Seldschukischen und Osmanischen Reich gebräuchlich war, auch international durch. Als Prototyp einer imperialen Stadt ist es seit dem 4. Jahrhundert eine Weltstadt.

Gegründet wurde Konstantinopel als Byzantion (). Bereits im 10. Jahrhundert nannten Griechen die Stadt auch "Bulin" und "Stanbulin." Die Türken nannten sie bereits im Sultanat der Rum-Seldschuken und im frühen Osmanischen Reich . Nach 1453 hieß die Stadt unter den Osmanen offiziell , so z. B. auf Münzen oder Fermans. "Istanbul" war ein Alternativname.

Von Griechen wird sie heute noch „Die Stadt“ () bzw. "Konstantinopel" () genannt. In skandinavischen Quellen wurde sie hingegen stets als "Miklagarð" bezeichnet, von russischen und bulgarischen meist als „Kaiserstadt“ (Царьград "Zargrad" oder Цариград "Zarigrad)." Konstantinopel wird in Überlieferungen oft auch als "Stadt der sieben Hügel" bezeichnet, ebenso wie Rom.


Wegen der wachsenden Bedeutung der Osthälfte des Römischen Reiches und zur Feier des Sieges über seinen letzten Rivalen Licinius, der den östlichen Reichsteil bis 324 beherrschte, wurde Byzantion 326 vom römischen Kaiser Konstantin I. zur Residenz ausgebaut und vier Jahre später, am 11. Mai 330, feierlich eingeweiht. Sie erhielt den neuen Namen "Constantinopolis" (griech. Κωνσταντινούπολις „Stadt des Konstantin“), womit die Tradition hellenistischer Könige und früherer römischer Kaiser aufgegriffen wurde, neuen Stadtgründungen den eigenen Namen zu geben. Zugleich blieb aber auch der Name "Byzantion" (Βυζάντιον) üblich. Mehrere Städte waren von Konstantin zuvor in Betracht gezogen worden, darunter das alte Troja an der kleinasiatischen Küste und angeblich auch Jerusalem, doch, so behauptete der Kaiser nachträglich selbst, habe er sich aufgrund einer nächtlichen Erscheinung der Jungfrau Maria auf ihren Rat hin für das am Bosporus liegende Byzantion entschieden. Der Ort lag strategisch günstig, in Reichweite sowohl der Donau- wie der Euphratgrenze. Die Stadt wurde auf das Fünffache der ursprünglichen Fläche vergrößert und wie das Vorbild Rom auf (angeblich) sieben Hügeln errichtet. Auch die politischen und weltlichen Einrichtungen der alten Hauptstadt wurden vielfach nachgeahmt. So erhielt Konstantinopel ein Kapitol, einen Circus für 100.000 Zuschauer, ein Forum ("Forum Constantini") und eine Hauptverkehrsachse in ostwestlicher Richtung. Aus dem ganzen Reich wurden Kunstwerke in die Stadt geschafft, um ihr Glanz zu verleihen. Trotz Konstantins Förderung des Christentums war die neue Stadt keine rein christliche Gründung, wie die (angebliche) Überführung des einst aus Troja geraubten Palladions aus Rom, vor allem aber die Renovierung der Tempel und die bei der Stadtgründung, wie sonst auch üblich, vollzogenen paganen Rituale zeigen: Die Stadt war nicht als „christliches Rom“ geplant, auch wenn spätere Quellen dies teils behaupten. Ferner gewährte Konstantin dem Rat der Stadt fast dieselben Privilegien, wie sie der römische Senat genoss, allerdings mit dem Unterschied, dass die Senatoren von Konstantinopel zunächst lediglich den Ehrentitel "clarus" („der Strahlende“) tragen durften, wohingegen sich die römischen Senatoren mit dem Superlativ "clarissimus" schmückten. Erst Konstantins Sohn Constantius II. beseitigte diesen Unterschied.

Ob Konstantin Byzantion wirklich als Konkurrenz zu Rom geplant hat, ist unter den Forschern umstritten und gilt heute als unwahrscheinlich, denn auch andere Kaiser vor und nach ihm hatten Städte wie Trier oder Nikomedia als Residenzen großzügig erweitert und teils nach sich selbst benannt. Konstantins Stadt hatte erst von 359 an einen Stadtpräfekten wie Rom und wurde bis dahin durch einen gewöhnlichen Statthalter "(proconsul)" regiert; es gab für die dortigen Senatoren zunächst keinen "cursus honorum," und die rechtliche Gleichstellung mit Rom wurde frühestens 421, also erst nach fast einem Jahrhundert, erreicht. All dies spricht gegen die Annahme, Konstantinopel habe von Anfang an ein neues Rom werden sollen. Aber wie dem auch sei: Ohne Frage wuchs die Bedeutung der Stadt in den Jahren nach 330 rasch. Die ägyptischen Getreideflotten steuerten fortan nicht mehr Rom an, sondern die Stadt am Bosporus. Konstantinopel wurde in der Spätantike zum Mittelpunkt von Verwaltung, Wirtschaft und Kultur des Oströmischen Reiches ausgebaut und erfüllte diese Aufgabe (mit Unterbrechung) vom späten 4. Jahrhundert bis in die Neuzeit "par excellence." Nach der faktischen Reichsteilung von 395 war die Stadt das Zentrum der östlichen Mittelmeerwelt. Solange Byzanz/Konstantinopel stand, stand auch das (von der modernen Geschichtsschreibung so genannte) Byzantinische Reich. Mit dem Fall der Stadt fiel auch das Reich. In Folge der Machtstellung wurde Konstantinopel auch zum kirchlichen Mittelpunkt. Der Bischof der Stadt, der sein Amt auf den Apostel Andreas zurückführte, war ab 381 Patriarch und beanspruchte eine herausgehobene Stellung (auf kaiserlichen Beschluss hin war er fortan nur dem Bischof von Rom nachgeordnet). Auch kulturell lebte die Stadt in der Spätantike auf: Die Hochschule war die jüngste, aber bald auch größte des Ostreiches und erreichte unter Theodosius I. eine erste Blütezeit, wobei auch die Bibliotheken ausgebaut wurden. Als eigentlicher Gründer der sogenannten Universität von Konstantinopel gilt Kaiser Theodosius II.

Konstantinopel konnte auf Grund der Lage auf einem Kap nur nach Westen hin erweitert werden. Bereits Theodosius I., unter dem sich Konstantinopel ab 379 endgültig gegen Antiochia als Hauptresidenz des Ostens durchsetzte, baute die Stadt aus und verlegte mit der Errichtung des Großen Palastes den Sitz der Kaiser hierhin. Um 412 wurde unter seinem Enkel Theodosius II. etwa 1500 m westlich der von Konstantin errichteten Stadtmauer eine weitere, teilweise noch heute erhaltene Mauer errichtet und so das Areal der Stadt von sechs auf zwölf km² verdoppelt. Das gewaltige Befestigungswerk wurde danach noch wiederholt restauriert und erweitert. Die Bevölkerung Konstantinopels wuchs rasch und schließlich gegen den Willen der Herrscher, doch selbst Beschränkungen vermochten den Zuzug nicht zu verhindern. Die Versorgung der weit über 400.000 Einwohner (zur Zeit Justinians waren es vor dem Ausbruch der Pest in den 540er Jahren gar zwischen 500.000 und 600.000) stellte die Machthaber zeitweise vor Probleme, insbesondere im späteren 7. Jahrhundert nach dem Verlust der „Kornkammer“ Ägypten nach der islamischen Expansion an die Araber, wodurch die Einwohnerzahl wieder zurückging. Bis etwa 600 gab es in der Stadt noch zahlreiche Einwohner mit Latein als Muttersprache, wie unter anderem durch Grabinschriften bezeugt wird, erst danach wurde Konstantinopel vollständig gräzisiert.

Um die Stadt mit Waren zu versorgen, wurden früh Häfen an der Küste zum Goldenen Horn und zum Marmarameer aus- oder neugebaut. Für die Versorgung der riesigen Hauptstadt mit Trinkwasser wurden mehrere Aquädukte aus dem nordwestlich gelegenen Hügelland errichtet, deren Wasser in mehreren, insgesamt 130.000 m³ fassenden, unterirdischen Zisternen (bspw. der 532 unter Justinian I. gebauten sog. Yerebatan Sarnıçı) gespeichert wurde. Allgemein erfasste die oströmischen Kaiser im 4. Jahrhundert bis 6. Jahrhundert eine auffällige Baulust, von der auch Chalcedon – obwohl es ständig im Schatten von Konstantinopel stand – profitierte. So wurde der Hafen erweitert sowie Paläste und Kirchen gebaut. Nach den Zerstörungen während des Nika-Aufstandes 532 ließ Justinian I. zahlreiche Gebäude, darunter die Hagia Sophia, das bedeutendste spätantike Bauwerk der Stadt, neu errichten.
Konstantinopel galt aufgrund der Theodosianischen Mauern lange Zeit als uneinnehmbar und als die stärkste Festung der bekannten Welt; zahlreiche Angriffe und Belagerungen scheiterten an dem mehrfach gestaffelten Befestigungswerk der Stadt. Die Zufahrt zum Hafen konnte mit einer gewaltigen Kette versperrt werden. Die Festung Konstantinopel beherrschte damit den Übergang von Europa nach Asien und trug entschieden dazu bei, dass die reichen römischen Orientprovinzen während der Völkerwanderung für Hunnen und Germanen unerreichbar blieben. Umgekehrt war die Stadt auch bei der Abwehr von Angriffen von Osten her ähnlich bedeutend. Zu einer ersten echten Bewährungsprobe kam es mit der großen Belagerung von Konstantinopel (626) durch die persischen Sassaniden und den mit diesen verbündeten Awaren. Mit der islamischen Expansion, während der auch die Araber wiederholt an der dreifachen Mauer der Stadt scheiterten, endete wenige Jahre später die spätantike Phase der Stadtgeschichte.

Die beiden abgewehrten Belagerungen durch die Araber in den Jahren 674–678 sowie 717–18 stoppten den Vormarsch der Muslime nach Europa und sind ebenso wie die Schlacht bei Tours und Poitiers durch die Franken von welthistorischer Bedeutung. Allerdings wirkte sich der endgültige Verlust der reichen römischen Orientprovinzen nach 636 auch auf die Hauptstadt aus; so entfielen nun die Getreidelieferungen aus Ägypten. Während die Araber im Laufe des 8. bis 10. Jahrhunderts teilweise zurückgedrängt werden konnten, wurden die Bulgaren zur neuen Bedrohung für die Stadt. Zu einer ersten (ebenfalls erfolglosen) Belagerung kam es 813. Die Serie der Angriffe riss auch im 9. und 10. Jahrhundert nicht ab, als Bulgaren und Rus, im Jahr 1090 die Petschenegen, mehrfach versuchten, Konstantinopel zu erobern. In der Regel führten diese Belagerungen zur Verwüstung des thrakischen Umlands der Stadt, und auch das leichter befestigte Chalcedon wurde mehrfach von Persern und Arabern eingenommen, geplündert und zerstört. Infolgedessen sind dort heute kaum noch Spuren der byzantinischen Baukunst zu finden.

Trotz wiederkehrenden Stadtbränden, Seuchen und Erdbeben blieb Konstantinopel bis ins Mittelalter eine der wenigen „Weltstädte“ der westlichen Welt (neben Bagdad, Kairo und Córdoba), und die mit Abstand größte und wichtigste christliche Metropole. Unter Justinian hatte sie im 6. Jahrhundert, wie bereits beschrieben, ihre erste und wohl auch größte Blüte erreicht, die Einwohnerzahl soll spätantiken Quellen zufolge damals die 500.000er Marke überschritten haben. Dagegen nehmen kritische Historiker und Archäologen an, dass die Stadt wohl niemals eine halbe Million erreicht, geschweige denn überschritten habe. Bis zur Mitte des 8. Jahrhunderts ging die Einwohnerzahl nicht zuletzt auf Grund der arabischen Belagerungen deutlich zurück (nach Ansicht von Forschern wie Chris Wickham sogar auf deutlich unter 100.000), stieg dann allerdings bis ins 12. Jahrhundert wieder auf angeblich etwa 700.000 Einwohner an. Vorsichtigere Schätzungen setzten für das Ende des 12. Jahrhunderts demgegenüber 400.000 Einwohner an.

Gebietsverluste infolge militärischer Niederlagen (unter anderem in der Schlacht von Manzikert im Jahr 1071) zwangen die Byzantiner Ende des elften Jahrhunderts, Hilfe im christlichen Westen zu suchen. Dem Vordringen der Normannen über Süditalien bis auf das griechische Festland konnte nur dank der Venezianer Einhalt geboten werden, im Gegenzug wurden ihnen Handelsprivilegien, Zollnachlässe sowie eine Handelsniederlassung in Konstantinopel vertraglich gewährt. Weitere Hilfegesuche im Westen führten zum Ausruf des Ersten Kreuzzugs durch Papst Urban II., infolgedessen ein Heer aus allen Teilen Westeuropas Richtung Konstantinopel zog, wo im April 1097 die letzten Abteilungen eintrafen. In der Metropole am Bosporus sahen die Kreuzfahrer eine fortschrittliche Infrastruktur, die sie aus keiner ihrer Städte auch nur annähernd kannten. Es gab Aquädukte, Bäder und Kanalisation, Kliniken mit Abteilungen für die unterschiedlichsten Krankheiten, eine große Universität, selbst Polizei und Feuerwehr. Händler aus aller Welt trafen sich auf den Märkten der Stadt, deren großer Reichtum auf dem Überseehandel beruhte. Kaiser Alexios I., der angesichts der barbarisch anmutenden Horden um seine Hauptstadt besorgt war, beeilte sich, das Kreuzfahrerheer auf die asiatische Seite des Bosporus zu befördern. Das gut 50.000 Mann starke Heer eroberte noch im gleichen Jahr die nahe gelegene Sultanats-Hauptstadt Nicäa und zog dann weiter Richtung Jerusalem. Dem bedrängten Konstantinopel war wieder etwas Luft verschafft worden; doch zugleich hatte sich das Verhältnis zum Westen, das ohnehin durch das Schisma von 1054 belastet war, im Zuge des Kreuzzugs erheblich verschlechtert.

Auch das traditionell freundliche Verhältnis der Byzantiner zu Venedig schlug im 12. Jahrhundert unter Manuel I. Komnenos in Misstrauen, Verachtung und Hass um, nicht zuletzt durch die immer wieder auf byzantinischem Boden ausgetragenen Machtkämpfe der Dogenrepublik mit Pisa und Genua. Die Einheimischen empfanden das anmaßende Auftreten der sogenannten „Lateiner“ als Provokation und man betrachtete sich gegenseitig als Häretiker. Die explosive Stimmung entlud sich 1171 in den Lateinerpogromen, als die byzantinische Regierung zuerst den Besitz tausender Venezianer konfiszierte und sie anschließend einkerkerte. Angeblich wurde damals sogar der anschließend zu Verhandlungen angereiste Enrico Dandolo geblendet, doch ist dies fraglich. Trotz einem 1177 beschlossenen Frieden beeinträchtigte das Ereignis dieser "Lateinerpogrome" die Beziehung zwischen Konstantinopel und Venedig nachhaltig. Im Jahre 1203 nahm ein von Venedig ausgerüstetes und vom Dogen Dandolo geführtes Kreuzfahrerheer die Eroberung Konstantinopels in Angriff, unter dem Vorwand, die dortigen Thronstreitigkeiten zu klären (allerdings ist in der modernen Forschung bestritten worden, dass Venedig wirklich von Anfang an einen Angriff auf Byzanz geplant habe). Kaiser Alexios III. floh vor dem anrückenden Heer, und Isaak II. nahm, eingesetzt von den Kreuzfahrern, (wieder) Platz auf dem Thron. Die Kreuzfahrer blieben trotz „getaner Arbeit“ zunächst in der Stadt und warteten auf die versprochene reiche Belohnung. Als sie eine Moschee entdeckten – es gab ab 718 infolge der Niederlassung arabischer Händler eine muslimische Gemeinde in Konstantinopel – und sie anzündeten, zerstörte der dadurch entstandene Flächenbrand ein ganzes Stadtviertel.

Als Isaak II. sowie sein Sohn Alexios IV. (unter ungeklärten Umständen) starben und ihnen Alexios V. auf den Thron folgte, wurden die Kreuzfahrer der Stadt verwiesen. Diese fühlten sich um die versprochene Belohnung betrogen und beleidigt, sie bereiteten daraufhin einen erneuten Angriff auf Konstantinopel vor. Unter Führung des 96-jährigen 41. venezianischen Dogen Enrico Dandolo, einem erbitterten Gegner des orthodoxen Byzanz, gelang es ihnen am 13. April 1204 gemeinsam mit den Venezianern, die Stadt von der Seemauer am Goldenen Horn her zu stürmen. Anschließend wurde die Stadt drei Tage geplündert. Viele Einwohner der kosmopolitischen Metropole wurden dabei getötet. Zahlreiche Monumente wurden zerstört, großartige Kunstwerke wurden vernichtet oder geraubt, etliche Bibliotheken niedergebrannt und eine große Anzahl der in Konstantinopel aufbewahrten Heiligenreliquien entwendet und über ganz Europa zerstreut. Von dieser Zerstörung und Plünderung durch die Venezianer und Kreuzfahrer erholte sich Konstantinopel im restlichen Verlaufe des Mittelalters nicht wieder.

Die Kreuzfahrer zerstückelten das Byzantinische Herrschaftsgebiet und errichteten das sogenannte Lateinische Kaiserreich. Dieses hatte nur kurz Bestand, bereits 1261 eroberte ein Söldnerheer des von geflohenen byzantinischen Familien getragenen Kaiserreiches Nikaia die Stadt im Handstreich zurück "(→ Rückeroberung von Konstantinopel 1261)." Das Byzantinische Reich wurde in vergleichsweise bescheidenem Umfang wiederhergestellt, verlor aber in der Folge immer weitere Gebiete seines Territoriums. Um 1300 hatte Konstantinopel noch etwa 100.000 Einwohner. Seine Rolle als wichtigstes Handelszentrum des Mittelmeers hatte es an die italienischen Hafenstädte, insbesondere Venedig, verloren. Die Italiener unterhielten Handelsniederlassungen im Stadtteil Pera (heute Beyoğlu) auf der nördlichen, europäischen Seite des Goldenen Horns.

1326 begann mit der Eroberung Bursas durch Osman I., einen Heerführer eines kleinen türkischen Stammes, der Siegeszug der Osmanen. In rascher Folge eroberten diese ganz Anatolien und Teile des europäischen Festlandes. Byzanz glich bald einer Insel im Osmanischen Reich. Im 15. Jahrhundert bestand es nur mehr aus dem eigentlichen Stadtgebiet und den umliegenden Dörfern, die Einwohnerzahl sank auf etwa 40.000 ab.

Mehrere Angriffe auf Konstantinopel blieben erfolglos, bis am 29. Mai 1453 die Stadt unter Mehmed II. unter großen Verlusten eingenommen werden konnte "(Siehe auch Belagerung von Konstantinopel (1453))." Die Zahl der Toten wird mit 50.000 angegeben. Die zahlenmäßig weit unterlegenen Verteidiger hielten knapp zwei Monate der Belagerung stand, warteten am Ende aber vergeblich auf Hilfe aus Venedig und Polen "(siehe: Schlacht bei Varna)." Die Überlebenden wurden mit Ausnahme der Juden und Genuesen deportiert. Diese konnten dank ihrer umsichtigen Haltung während der Belagerung ihren Privatbesitz retten.

Viele Einwohner und Intellektuelle flohen nach Westeuropa und vor allem Norditalien, und nahmen dabei viele erhalten gebliebene Kopien antiker Schriftstücke mit. Diese verbreiteten sich durch die ungefähr gleichzeitig erfundene Buchdruck-Kunst schnell in Norditalien und lösten eine Welle der „Wiederentdeckung“ antiker Denkmodelle und Vorstellungen aus. Diese Wiederentdeckung beschleunigte den vielschichtigen Prozess, der heute als Renaissance bezeichnet wird.

Mit der Eroberung Konstantinopels endete das Oströmische Reich. Kleinere Landesteile, vor allem Mystras auf der Peloponnes, konnten sich noch einige Jahre halten, wurden dann aber auch erobert.

Inzwischen prägten die muslimischen Herrscher, die Konstantinopel zur Hauptstadt ihres Reiches machten, das Stadtbild vollkommen neu. Zahlreiche Kirchen, deren bedeutendste die Hagia Sophia war, wurden um Minarette ergänzt und zu Moscheen umgebaut.

Am 14. September 1509 erschütterte ein schweres Erdbeben die Stadt. Ein ganzer Stadtteil wurde auch durch die in der Folge ausbrechenden Brände unbewohnbar. Etwa 13.000 Menschen fielen den Auswirkungen des Bebens zum Opfer.

Nach der Eroberung durch Fatih Sultan Mehmet im Mai 1453 nannten die Osmanen die Stadt zunächst "Islambol" (türk. „Islamreich“), später im Alltagsgebrauch "İstanbul," auch wenn der offizielle Name bis 1930 weiter "Konstantinopel" blieb. Im griechischen Sprachbereich wird bis heute von "Konstandinúpoli" gesprochen. Der Name İstanbul (im deutschen Sprachraum früher auch „Stambul“) leitet sich nach traditioneller Ansicht aus dem griechischen "εἰς τὴν πόλι(ν)," in der Koine zu "is tin poli(n)" verschliffen, ab, was „in die Stadt“ bedeutet. Es existiert aber eine Vielzahl von anderen Hypothesen zur Namensgebung.

Die Stadt wurde Residenz der Sultane und Hauptstadt des Osmanischen Reiches. Sie behielt neben der politischen große wirtschaftliche und kulturelle Bedeutung und ein internationales Gepräge. Das Patriarchat blieb als übergreifende Institution für die Christen des Reiches mit bedeutenden Rechten und Pflichten erhalten, bis 1821 spielten Griechen eine wichtige Rolle (unter anderem in der Diplomatie und bei der Verwaltung der Donaufürstentümer). Der griechische Einfluss in Wirtschaftsleben und Diplomatie war noch bis 1922 bedeutend. Unter Süleyman dem Prächtigen (1520–1566) war Konstantinopel die Hauptstadt eines riesigen Reiches, das von Ungarn über Belgrad bis Bagdad und weit nach Nordafrika reichte. Das Osmanische Reich war auf dem Höhepunkt seiner Macht, was sich in einer Vielzahl von Palästen und Moscheen des Architekten Sinan, des größten osmanischen Baumeister seiner Zeit, widerspiegelt. Bereits damals begann aber der Niedergang. Fehlende Reformen, korrupte Wesire, die Macht der Sultansfrauen sowie die Abschottung gegen moderne Tendenzen bewirkten, dass man trotz einer schönen Fassade im 19. Jahrhundert schließlich vom „kranken Mann am Bosporus“ sprach, wenn man das Osmanische Reich meinte.

Ab dem 17. Jahrhundert kam es zu einem massiven Zuzug von Armeniern aus allen Gebieten des Osmanischen Reichs. Ende des 19. Jahrhunderts lebten mindestens 250.000 Armenier in Konstantinopel. Es bildete sich eine kulturelle armenische Infrastruktur, die schließlich zu einem kulturellen sowie politischen Aufbruch der westarmenischen Gemeinschaft führte und das Gesicht der Stadt mitprägte.
Ein wichtiger Chronist dieser Zeit ist der deutsche Journalist und Schriftsteller Friedrich Schrader, der von 1891 bis 1918 in Konstantinopel lebte und arbeitete.

Als Kaiser Konstantin zwischen 324 und 330 ein neues Zentrum für das römische Reich am alten Byzanz gründete, sollte dieses allmählich Rom als Hauptstadt ablösen. Um den Gedanken eines "Nova Roma" Nachdruck zu verleihen, musste dieses dann auch architektonisch ausgebaut werden. Da Konstantinopel zudem von Anfang an christlich geprägt war und das Christentum Staatsreligion wurde, ohne dass im Übrigen auf den Kaiserkult verzichtet wurde, bekam Konstantinopel ein durch Votiv- und Gedenksäulen, Foren, Paläste, das Hippodrom und natürlich zahlreiche christliche Kirchen geprägtes Aussehen.

Ältestes erhaltenes Baudenkmal Konstantinopels ist die Konstantinssäule. Die ehemals 52 Meter hohe Säule aus Porphyr wurde ursprünglich von einer Statue des Helios bekrönt. Der Kopf des Sonnengottes war von sieben Strahlen umkränzt, in die der Legende nach Passionsnägel eingearbeitet worden waren. Auch das Fundament der Säule soll einer Überlieferung des 9. Jahrhunderts zufolge einen Splitter vom Kreuz Christi, das Palladion und weitere teils christliche, teils pagane Kultobjekte geborgen haben. Im Jahr 1105 wurde die Statue bei einem Unwetter zerstört und durch ein Kreuz ersetzt. Die Höhe der Säule beträgt nur noch 35 Meter. Sie wurde zum Symbol der Stadt, und die letzten byzantinischen Chronisten berichten, dass sich am Tag der Eroberung durch Sultan Mehmed II. die Stadtbewohner frühmorgens um sie versammelten, um auf den rettenden Engel des Herrn zu warten.

Neben der Konstantins-Säule bildete vor allem das Hippodrom den Mittelpunkt der Stadt und war Brennpunkt des öffentlichen Lebens. Hier begegneten sich Kaiser und Volk, hier demonstrierte der Kaiser seine Macht und dort finden sich daher auch einige repräsentative Objekte. Entlang der Spina, der Trennmauer zwischen den beiden Richtungsbahnen, um welche die Streitwagen fuhren, stellten Konstantin und seine Nachfolger Standbilder und Denkmäler auf. Darunter ein ägyptischer Obelisk vom Tempel in Karnak und die bronzene Schlangensäule aus dem 5. Jahrhundert v. Chr. Diese Säule war ursprünglich von 31 griechischen Städten zur Erinnerung an die Schlacht von Plataiai 479 v. Chr. direkt gegenüber dem Apollotempel von Delphi aufgestellt worden. Konstantin I. ließ das Denkmal 330 nach Konstantinopel bringen. Die von dieser Säule ursprünglich getragene goldene Schale wurde während des 4. Kreuzzuges geraubt. Die Köpfe der Schlangen zerstörten Muslime im 17. oder 18. Jahrhundert, der Rest eines der drei Köpfe ist noch im archäologischen Museum in Istanbul zu sehen.

Unter Kaiser Theodosius wurde entlang der Wegstrecke der kaiserlichen Triumphzüge drei Foren errichtet. Auf dem "Forum Tauri" stand die Ehrensäule des Kaisers Theodosius, geschaffen nach dem Vorbild der Trajanssäule in Rom. Weitere Säulen sind die Arcadius-Säule, Markian-Säule sowie die Justinian-Säule. Diese jüngste der Säulen ist ebenso wie die Konstantins-Säule aufs engste mit der Geschichte Konstantinopels verbunden. Die 543 eingeweihte 35 m hohe Säule trug ein Reiterstandbild Justinians I. in drei bis vierfacher Lebensgröße. Als Mehmed II. Konstantinopel eroberte, war eine seiner ersten Taten, diese Statue zu vernichten.

Als eines der zentralen spätantiken Monumente der Stadt ragt die heute als Museum genutzte Hagia Sophia aus dem 6. Jahrhundert hervor. Sie war bis zum Bau der Kathedrale von Sevilla das größte Gotteshaus der Welt. Gleich nach der Eroberung Konstantinopels machten sich die neuen türkischen Herren daran, den Bau für die mitgebrachte islamische Religion zu vereinnahmen und gestalteten ihn um. Dabei wurden nicht nur alle wertvollen christlichen Symbole entfernt und die kostbaren Mosaiken zerstört oder überputzt, sondern neben diversen Umbauten auch vier große Minarette an den Flanken der Kirche durch drei Sultane emporgezogen.

Entgegen der weitläufigen Auffassung gab es auch schon in vorosmanischer Zeit Muslime und Moscheen innerhalb der Stadt. Die erste Moschee Konstantinopels (und somit die erste Moschee auf der Balkanhalbinsel bzw. in ganz Südosteuropa überhaupt) soll schon im Jahr 718 entstanden sein.

Nach der erfolglosen Zweiten Belagerung von Konstantinopel (717–718) hatten sich der arabische Heerführer Maslama und der byzantinische Kaiser Leo III. auf die Errichtung einer Moschee für die arabischen Kriegsgefangenen bzw. für die in der Stadt aktiven muslimischen Händler geeinigt. Sie wurde von Konstantin Porphyrogennetos in De Administrando Imperio ebenso erwähnt wie in der Korrespondenz zwischen dem arabischen Kalifen ar-Rādī bi-'llāh und dem byzantinischen Kaiser Romanos I. und in den Chroniken von Niketas Choniates, Ibn al-Athīr, Al-Muqaddasi, Yaqut al-Hamawi ar-Rumi, Al-Dimashqi und anderen. Unterschiedlichen Angaben zufolge soll sich diese "Sarazenische Moschee" nahe dem kaiserlichen Palast, innerhalb oder nahe dem Praitorion (östlich des Konstantinsforums, heute zwischen "Atik Ali Paşa Camii Çemberlitaş" und "Sultan Iı. Mahmut Türbesi)" bzw. in einem „sarazenischen“ Viertel hinter der Hagia Irene (nahe dem Kaiserpalast) befunden haben (vermutlich in Regio IV oder Regio V).

Im Rahmen einer Übereinkunft mit Tughrul Beg soll Konstantin IX. um 1050 Sanierungsarbeiten an der Moschee beauftragt haben (daher gelegentlich auch als "Seldschukische Moschee" bezeichnet). Von den lateinischen Kreuzrittern im August 1203 in Brand gesteckt (nach anderen Angaben von den "Sarazenen" selbst oder bereits bei Unruhen im Jahr 1200), soll die Moschee nach der byzantinischen Rückeroberung Konstantinopels von Michael VIII. 1263 im Interesse guter Beziehungen zum ägyptischen Mamluken-Reich restauriert worden sein. Der letzte byzantinische Kaiser, Konstantin XI. verfügte offenbar die Schließung aller Moscheen in Konstantinopel und drängte die Muslime zur Annahme des Christentums. Ob die Maslama-Moschee bis zum Zeitpunkt der osmanischen Eroberung 1453 genutzt wurde bzw. noch existierte, bleibt unklar. Archäologische Funde gibt es kaum bzw. können nicht eindeutig zugeordnet werden.

Bereits im 12. Jahrhundert hatte die Zahl arabischer Händler und muslimischer Zuwanderer so stark zugenommen, dass eine zweite Moschee errichtet wurde. Sie soll sich außerhalb der Seemauer am Goldenen Horn, nordwestlich der Galatabrücke befunden haben, möglicherweise in der Nähe des heutigen Ägyptischen Basars (Mısır Çarşısı) bzw. der "Neuen Moschee" (Yeni Cami).

Genau gegenüber, auf der anderen Seite des Goldenen Horns, befindet sich heute im Stadtteil Galata die Arap Camii "(Arabische Moschee)". 

Einigen Quellen zufolge sollen sich auch die Grabmäler eines Nachkommen Ali ibn Abi Talibs sowie des Abu Ubaidah, eines der zehn Kampfgefährten des Propheten Mohammed, in Konstantinopel befunden haben. Diese arabische Überlieferung ist jedoch offensichtlich eine Verwechslung mit dem Grab des bereits bei der Ersten Belagerung von Konstantinopel (674–678) gefallenen Abu Ayyub al-Ansari, des Fahnenträgers des Propheten, im früher außerhalb der Stadtmauern befindlichen Stadtteil Eyüp. Sein Grab soll von den Byzantinern zunächst respektiert, von den Lateinern aber 1203 zerstört und erst von den Osmanen wiedergefunden worden sein. Über dem Grab entstand dann nach der osmanischen Eroberung 1458 die Eyüp-Sultan-Moschee.





</doc>
<doc id="8395" url="https://de.wikipedia.org/wiki?curid=8395" title="Jemen">
Jemen

Die Republik Jemen () ist ein Staat in Vorderasien, im Süden der Arabischen Halbinsel. Er ist etwa anderthalbmal so groß wie Deutschland und grenzt im Norden an Saudi-Arabien, im Osten an Oman, im Süden an den Golf von Aden und das Arabische Meer, im Westen an das Rote Meer. Die Staaten Dschibuti und Eritrea liegen etwa 20 bzw. 30 Kilometer entfernt jenseits des Roten Meeres. Die Küstenlänge beträgt 2400 Kilometer; die Binnengrenzen sind 1746 Kilometer lang. Zum Jemen gehören auch die 3814 km² große Inselgruppe Sokotra sowie zahlreiche kleinere Inseln im Bab al-Mandab im Roten Meer und im Arabischen Meer.

Im Jahr 1990 vereinigten sich die zwei früheren Staaten Nordjemen (Hauptstadt Sanaa) und die Demokratische Volksrepublik Jemen (Südosten, Hauptstadt Aden) zum heutigen Staat. Seit 2013 kämpfen schiitische Huthi-Rebellen, Anhänger von Ex-Präsident Ali Abdullah Salih, Al-Qaida-Ableger der AQAP, mit der Armee der Zentralregierung unterstützt von den Separatisten des Südjemen um die Macht. In diesem Konflikt gelang es den Huthi-Milizen, die Hauptstadt Sanaa und große Teile des Landes zu erobern, und sie standen kurz vor der Eroberung der provisorischen Hauptstadt Aden. Daraufhin begann Saudi-Arabien unter militärischer Mitwirkung acht anderer Staaten am 25. März 2015 eine militärische Intervention unter dem Namen Sturm der Entschlossenheit zu Gunsten des Staatspräsidenten Hadi und des Regierungschefs Chalid Bahah.

Der Jemen nimmt auf dem Global Innovation Index, der 2016 die Innovationsfähigkeit von insgesamt 128 Staaten bewertet hat, den letzten Platz ein. Ebenfalls letzter ist der Jemen im Global Gender Gap Report 2016, der die Gleichberechtigung von Männern und Frauen in einem Land misst.

Der Jemen lässt sich in drei Großlandschaften gliedern:

Die zwischen 30 und 60 Kilometer breite, sanft ansteigende Küstenebene wird vor allem im Südwesten durch vorstoßende Gebirgsflanken gegliedert. Teilweise finden sich Zeugen von früherem Vulkanismus; so liegt etwa Aden, die einstige Hauptstadt der Demokratischen Volksrepublik Jemen (Südjemen), in einem Doppelkrater. Die Ebene an der Westküste, die Tihama, wird von Sand- und Kiesflächen beherrscht.
Zum Landesinneren hin erhebt sich steil das zerklüftete, im Westen mehrfach über 3000 Meter hohe Randgebirge. Südwestlich der Hauptstadt Sanaa erhebt sich der Dschabal an-Nabi Schuʿaib, mit 3760 Metern der höchste Berg des Landes.

An das Gebirge schließt sich ein Hochland an, mit Durchschnittshöhen von 2000 bis 2500 Meter. Es ist von Wadis durchzogen; das bekannteste ist das parallel zur Südküste verlaufende Wadi Hadramaut. Nach Nordosten hin fällt das Hochland in Stufen zur zentralarabischen Sandwüste ar-Rubʿ al-chali ab.

Die Inseln und die Küstenebene sind feuchtheiß und insgesamt sehr niederschlagsarm (Aden: Januarmittel 25 °C, Junimittel 33 °C, 40 mm Jahresniederschlag). Hier ist die Luftfeuchte mit 60 bis 85 Prozent das ganze Jahr über sehr hoch. Der Niederschlag ist ganzjährig äußerst gering und beträgt meist nur zwischen 25 mm und 150 mm, was mit 5 bis 15 Regentagen im Jahr gleichzusetzen ist. In der Winterhälfte des Jahres ist es sehr warm, bei 19 bis 23 °C in der Nacht und 28 bis 31 °C am Tag. Die Sommer werden durch die hohe Luftfeuchte sowie Tagestemperaturen von 34 bis 38 °C und mehr häufig unerträglich heiß. Dazu sinken die Nachtwerte meist nicht unter 26 °C, oft gibt es sogar Perioden von Tropennächten mit beständig über 30 °C. Die einzige, jedoch sehr seltene Abkühlung im Sommer bringen gelegentliche Ausläufer des indischen Monsuns, die es mit leichten Regenschauern manchmal bis an die jemenitische Südostküste schaffen (an der Westküste bleiben sie gänzlich aus). Dem stehen allerdings hin und wieder auftretende Hitzewellen von 40 °C und darüber gegenüber. Ein Phänomen an den Küsten ist nicht selten auftretender Morgennebel, den die starken Sonnenstrahlen jedoch bald lichten. An der Westküste handelt es sich weitgehend um Winternebel, an der Südostküste um Sommernebel.

Das Gebirge nimmt mehr als ein Drittel des Landes ein und wird durch den Hauptgebirgszug des Al-Sarat geprägt. Diese Gebirgsregion kennt viele, sehr dicht besiedelte Becken, die durchwegs auf einer Höhe von 1500 bis 2500 Meter liegen. Das Klima zeigt sich hier von einer für die Region sehr milden Seite. Die Winter sind trocken und von hohen Temperaturschwankungen gezeichnet: nachts kühlt es häufig bis fast auf den Gefrierpunkt ab (0 bis 4 °C), während tagsüber die wärmenden Sonnenstrahlen für angenehme Werte sorgen (22 bis 24 °C). Der Sommer zeigt sich mäßig feucht, was vor allem der Landwirtschaft zugutekommt. Im jemenitischen Gebirge werden die höchsten Niederschläge verzeichnet. In manchen Gegenden regnet es an bis zu 50 Tagen im Jahr (200 bis 700 mm), wobei sich der Schwerpunkt der Niederschläge in die Zeit zwischen März und August einordnen lässt. An Regentagen ist es etwas kühler, ansonsten steigen die Tagestemperaturen auf 26 bis 30 °C, in den Nächten bleibt es jedoch bei eher gedämpften Werten von 9 bis 13 °C. Die Luftfeuchte ist ganzjährig mittel und pegelt sich bei etwa 40 Prozent ein.

Das Klima im Hochland ist das ganze Jahr über weitgehend trocken (5 bis 25 Regentage). Die Winter sind mild, aber großen Temperaturschwankungen unterworfen (23 bis 28 °C tagsüber, 0 bis 6 °C nachts), die Sommer relativ heiß mit Tageswerten um 36 °C, denen aber kühle Nächte folgen (10 bis 16 °C). An den Wüstenrändern sind Werte von 45 °C keine Seltenheit. Die Luft ist ganzjährig eher trocken (25 bis 45 Prozent).

Der Jemen liegt an der Grenze zwischen dem Pflanzenreich der Holarktis und der Paläotropis. Er beherbergt nur in der Küstenebene eine Steppenlandschaft. Zum Bergland hin entspricht die Vegetation der einer Dornbuschsavanne. In den bis über 3000 Meter hohen Bergen siedelt eine afroalpine, frostverträgliche Pflanzendecke. Nur im äußersten Osten geht die Vegetation über das Stadium einer Halbwüste allmählich in eine echte Wüste über, durch jahrtausendelange Bewirtschaftung (Holzeinschlag, Weideverbiss, Ackerbau) sind nur noch Reste naturnaher Pflanzengesellschaften vorhanden.

Der Jemen beherbergt eine Fülle endemischer Pflanzenarten. Kleine Mangrovengebiete kommen entlang der Küste des Roten Meeres vor. Akazien bestimmen weitgehend das landschaftliche Bild. In Abhängigkeit von Höhenlage und Niederschlagsmenge – vom trockeneren (tiefe Lagen) zum feuchteren (Höhenlagen) – kommt folgende Zonierung vor: "Acacia tortilis, Acacia mellifera" (Honig-Akazie), "Acacia asak, A. etbaica". "A. ehrenbergiana" und "A. oerfota" (von "urfut," die „Stinkende“) sind in Wadis im Bergland und in der Tihama verbreitet. Der Jemen war im Altertum berühmt durch seine „Duftsträucher“ (Weihrauchstraße). Weihrauch ("Boswellia sacra") auf dem Hochplateau des Jol im Süden, Myrrhen- ("Commiphora erythrea, C. myrrha") und Balsamstrauch ("C. opobalsamum") wachsen in den feuchten, westlichen Berghängen. Die imposant blühende Wüstenrose ("Adenium obesum") gilt als Nationalbaum des Landes. In tief eingeschnittenen Wadis wachsen große Würgefeigen ("Ficus sycomorus") und Tamarinden ("Tamarindus indica"). Aus der Gruppe der Hirsen wachsen Kolbenhirse ("Pennisetum") eher in den Tieflagen der Tihama und Rispenhirse ("Panicum miliaceum") eher in den Gebirgslagen. In den Hochlagen wachsen Weizen und Gerste. Kaffee kommt in Höhenlagen etwa zwischen 1000 und 2000 m vor, wobei die Untergrenze durch Hitze, die Obergrenze durch Frost gebildet wird. Der ökologisch wesentlich anspruchslosere Qat, der bekannte Drogenstrauch im Jemen, hat den Kaffee bereits weitgehend verdrängt. Hennasträucher wachsen in mittleren Höhenlagen bei ausreichender Wasserversorgung. Kultivierte Dattelpalmen ("Phoenix dactylifera") kommen entlang von Flussläufen mit hohem Grundwasserstand vor. Okra, Paprika und Dicke Bohnen sind wichtige Gemüsekulturen. Im Tiefland wird tropisches Obst wie Papaya und Bananen kultiviert, in den Bergen wachsen Äpfel und Birnen. Melonen kommen in fast allen Höhenlagen vor.

Für ein Trockengebiet ist die Fülle von Reptilienarten normal. Die Lage an der Südwestzone der Arabischen Halbinsel hat diverse Endemiten hervorgebracht, die nur im Jemen leben. So wurde der auf Bäume kletternde Jemen-Waran ("Varanus jemense") erst 1988 wissenschaftlich beschrieben. Bis 1985 war der Wissenschaft die Existenz dieser Tierart unbekannt. Das Jemen-Chamäleon ("Chamaeleo calyptratus") und die Jemen-Agame ("Acanthocerus adramitanus") sind weitere farbenprächtige endemische Vertreter. Auch die Vogelwelt ist aufgrund der Lage und Topografie des Landes reichhaltig. Goliathreiher ("Ardea goliath") an den Küsten, spektakuläre Arten wie Hammerkopf (Scopus umbretta), Abessinische Blauracken und diverse Nektarvögel an Wadis der Gebirgstihama, Steppenadler ("Aquila nipalensis orientalis") und Schlangenadler ("Circaetus gallicus"), diverse Weihen ("Circus") im Bergland sowie Gänsegeier überall im Land, wo großes Aas sie anlockt, sind auffallende Vertreter der Vögel.

Wild lebende Säugetiere sind wegen extremer Bejagung selten geworden. Gebirgstiere wie der Nubische Steinbock und Steppentiere wie die Oryxantilope sind bereits ausgestorben oder vom Aussterben bedroht. Paviane existieren noch in unzugänglichen Lagen des Dschabal Burrah. Einige Leoparden sind ebenfalls im Jemen vorhanden. Es sollen auch noch kleinere Populationen von Hyänen vorkommen. Der Golf von Aden ist reich an Fischen, besonders an Sardinen, Thunfischen und Haien.

Die Bevölkerung des Jemens wächst schnell und stieg zwischen 2000 und 2017 von 17,8 auf 28,2 Mio. Sie ist im Durchschnitt sehr jung; fast die Hälfte ist 15 Jahre oder jünger. Dies resultiert in einem schlechten Verhältnis zwischen erwerbstätiger und nicht erwerbstätiger Bevölkerung (100:477) und führt zu einer hohen Belastung der öffentlichen Infrastruktur und des Arbeitsmarkts: Die Bevölkerung im Alter zwischen 15 und 24 Jahren wurde im Jahr 2000 auf fast 4 Mio. und für 2010 auf mehr als 5 Mio. geschätzt; für 2050 werden 10 Mio. erwartet. Die Stadtbevölkerung wächst jährlich um fast 5 %. Die Jugendarbeitslosigkeit 2005/2006 wurde auf 29 % geschätzt, 57 % aller Arbeitslosen waren Jugendliche. Aufgrund der hohen Geburtenrate wird die Bevölkerung bis zum Jahr 2050 auf knapp 50 Mio. anwachsen (Schätzung der UN), was die wirtschaftliche und politische Stabilität weiter belasten wird. Die begrenzten Ressourcen des Jemen können die wachsende Bevölkerung kaum tragen.

Nach Angaben des Hochkommissars für Flüchtlinge (UNHCR) hielten sich im Juli 2015 etwa 250.000 Flüchtlinge in den städtischen Gebieten des Jemen auf. Die meisten von ihnen kamen aus Somalia.
Bei der von Schmugglerbanden organisierten Flucht übers Meer ertranken am 5. September 2005 vor der Küste des Jemen mindestens 58 Flüchtlinge aus Somalia, weitere 155 wurden vermisst, nachdem man sie mehrere Kilometer vor der Küste gezwungen hatte, an Land zu schwimmen. Weitere Unglücksfälle ereigneten sich laut UNHCR auch in den folgenden Monaten.

Entwicklung der Bevölkerung

Quelle: UN

Rund 97 Prozent der Einwohner sind Araber. Die Bevölkerung Tihamas ist teilweise schwarzafrikanischer Herkunft: Die Bevölkerungsgruppe der Achdam soll äthiopischer Abstammung sein. Die Achhdam sind bis heute eine diskriminierte „Kaste“ in der jemenitischen Gesellschaft, was zu Problemen führt. Etwa ein Prozent der Bevölkerung sind pakistanische oder muslimische indische Arbeitsmigranten, etwa zwei Prozent ethnische Somali, von denen viele schon länger im Lande leben.

Der Jemen beherbergte 2007 etwa 110.000 Flüchtlinge aus Somalia. Allein 2007 flohen 30.000 Menschen über den Golf von Aden aus Somalia in Richtung des Jemen, wobei die Zahl der auf der Überfahrt ertrunkenen oder verschwundenen Menschen auf 1400 geschätzt wird. Weiterhin hat der Konflikt im Norden des Landes 35.000 Menschen zu Flüchtlingen im eigenen Land ("internally displaced persons") gemacht. Die Behandlung der Flüchtlinge im Jemen wird als "inadäquat" bezeichnet.

Amtssprache ist Hocharabisch. Daneben werden Beduinendialekte und südarabische Sprachen verwendet. Zur Kommunikation taugliche Fremdsprachenkenntnisse sind selbst im Süden sehr selten; die am meisten an den Schulen unterrichtete Fremdsprache ist Englisch, die vor allem im ehemals von Großbritannien kolonisierten Süden anzutreffen ist.

Nahezu alle Einwohner des Jemen sind Muslime. Den größten Anteil stellen die Sunniten, mehrheitlich Anhänger der schafiitischen Rechtsschule. Eine große Minderheit (30-45 % der Bevölkerung) gehört den schiitischen Zaiditen an. Im Nordjemen lebt eine kleine Minderheit Ismailiten sowie eine Diaspora weniger Juden (etwa 300). 4500 Religionsschulen wurden geschlossen, und ausländische Schüler der Einrichtungen wurden des Landes verwiesen. Die Anzahl der Christen wird auf wenige Hundert bis einige Tausend geschätzt.

Wiederholt wurden religiös motivierte bewaffnete Aufstände durch das Militär bekämpft; zuletzt seit 2004 im nördlichen Gouvernement Sa'da. Die al-Haq-Partei, deren Führer mit den Aufständischen Verbindungen gehabt haben sollen, wurde 2007 verboten. Die Regierung versucht durch Überwachung von Predigten in den Moscheen und durch die Observation der Aktivitäten islamischer Organisationen den Extremismus einzudämmen. Im Jemen befinden sich allerdings mehrere große salafistische Religionsschulen, so das „Dar al-Hadith“ in Dammaj bei Saadah.

Die Verfassung des Jemen erklärt den Islam zur Staatsreligion und verlangt, dass der Präsident der Republik seinen "Pflichten als Muslim nachkommen sollte". Gleichzeitig räumt die Verfassung Glaubensfreiheit ein. Dies wird von der Regierung nur zum Teil umgesetzt: Missionierung und Proselytismus unter Muslimen sind verboten, für die Errichtung von nichtislamischen Gebetshäusern benötigt man eine spezielle Genehmigung, Nichtmuslime dürfen zwar an Wahlen teilnehmen, dürfen sich aber nicht zur Wahl stellen. Öffentliche Schulen bieten nur islamischen Religionsunterricht. Der öffentliche Genuss von Alkohol ist im Jemen nach islamischem Recht strafbar. Homosexuelle Handlungen sind ebenfalls verboten und können mit dem Tod bestraft werden.

Ein Sozialversicherungssystem existiert nicht; wichtigster Träger der sozialen Absicherung ist nach wie vor der traditionelle Familienverband. Die sinkenden Öleinnahmen und die vom massiven Bevölkerungswachstum und Wassermangel verschärfte soziale Krise bedrohen die Stabilität des jemenitischen Staates zusätzlich.

Für 2015 wurde geschätzt, dass 85 Prozent der Männer und 55 Prozent der Frauen lesen und schreiben können. Somit lag die Analphabetenquote in der Bevölkerung über 15 Jahre knapp unter 30 Prozent. Die Schulpflicht ist im Jemen zwar gesetzlich verankert und der Schulbesuch ist kostenlos, die Quote der Schulabbrecher ist dennoch hoch. Im Jahr 2012 wurden 86 Prozent aller Kinder eingeschult, doch lediglich 60 Prozent der Mädchen schlossen die Grundschule ab. Der Grund dafür ist in den meisten Fällen, dass die Mädchen schon in jungen Jahren zwangsverheiratet werden. Die Unterrichtsbedingungen an jemenitischen Schulen sind schlecht und die Bildungsqualität ist äußerst gering. Insbesondere in den naturwissenschaftlichen Fächern sowie in Mathematik und Arabisch sind die Leistungen der Schüler im Vergleich mit den anderen Staaten der Region unterdurchschnittlich. Obwohl es nach den Gesetzen des Jemens Schulpflicht gibt und der Schulbesuch kostenlos ist, besuchen nur etwa 75 Prozent der Kinder die Grundschule. Bei den Mädchen ist der Anteil sogar noch niedriger; nur 65 Prozent der schulpflichtigen Mädchen gehen zur Schule. Nach Beendigung der Grundschule erhalten nur 37 Prozent der Jugendlichen – 26 Prozent der Mädchen – eine weitergehende Ausbildung. Diese niedrigen Prozentsätze sind einerseits den mit dem Schulbesuch verbundenen Kosten (10 $ pro Kind und Jahr), andererseits dem Fehlen der nötigen Infrastruktur geschuldet. Bildungseinrichtungen und Unterrichtsmaterial sind nicht genügend und nur in schlechter Qualität vorhanden.

Die Ausgaben der Regierung für Bildung stiegen von 4,5 Prozent des BIP im Jahr 1995 auf 9,6 Prozent des BIP im Jahr 2005. Mit Unterstützung internationaler Organisationen laufen mehrere Programme zur Verbesserung der schulischen Infrastruktur sowie zur Verminderung der Benachteiligung von Mädchen.

Im Jemen gibt es sieben staatliche und acht private Universitäten. Die bedeutendste Universität des Landes ist die 1970 in Sanaa gegründete Universität Sanaa, ebenfalls 1970 wurde der Vorläufer der Universität Aden gegründet. Die Zahl derjenigen, die heute auch auf eigene Kosten im Ausland studieren, wächst; die jemenitischen Universitäten bleiben stark von ausländischem Personal abhängig. Verbreitetste Fremdsprache ist Englisch; der Verbreitungsgrad von Fremdsprachen ist jedoch sehr gering.

Der Jemen hat in den vergangenen Jahren deutliche Fortschritte beim Ausbau und der Verbesserung seines Gesundheitssystems gemacht. Trotzdem ist das Gesundheitssystem unterentwickelt. Im Jahr 2004 wurden für Gesundheit 5 Prozent des Bruttoinlandsproduktes ausgegeben. Die WHO schätzte, dass die Ausgaben pro Kopf 34 US-Dollar betrugen, was verglichen mit anderen Ländern des Nahen Ostens sehr niedrig ist. 2004 kamen auf 10.000 Personen drei Ärzte, 2005 gab es 6,1 Krankenhausbetten für 10.000 Einwohner.

Die Versorgung mit medizinischen Dienstleistungen ist besonders auf dem Land sehr schlecht. Während 80 Prozent der Städte über medizinische Einrichtungen verfügen, sind es nur 25 Prozent der ländlichen Gegenden. Notarztdienste oder Blutbanken gibt es nicht. Viele Kinder sterben an Krankheiten, gegen die es Impfungen gibt oder die sonst verhinderbar oder behandelbar wären. Die Zahl der HIV-positiven Einwohner Jemens wurde für das Jahr 2003 auf 12.000 geschätzt.

Die Lebenserwartung ist in den vergangenen zehn Jahren um 14 Jahre gestiegen, bleibt jedoch auch im Vergleich mit anderen Entwicklungsländern niedrig. Sie liegt bei etwa 64,2 Jahren (62,2 Jahre für Männer, 64,9 Jahre für Frauen). Die Fruchtbarkeitsrate liegt bei etwa 5,9 Lebendgeburten pro Frau, wobei sie auf dem Land mit 7,0 deutlich höher liegt als in der Stadt (5,0). Frauen ohne Bildung bekommen im Durchschnitt mehr Kinder (6,9) als Frauen mit Grundbildung (3,2). Im Jahr 2016 betrug die Kindersterblichkeit 55 pro 1000 Lebendgeburten.

Im Jemen ist nicht zuletzt die Malaria ein Problem, während sie in fast allen anderen arabischen Staaten bereits eliminiert wurde; ein weiterer Fortschritt bei der Malariabekämpfung im mittleren Osten hängt davon ab, ob Somalia, der Sudan und Jemen Fortschritte erzielen können.

Im Oktober 2016 kam es im Jemen zum Ausbruch der schweren bakteriellen Infektionskrankheit Cholera. Die Zahl der Verdachtsfälle beträgt 700.000 und nach Auskunft der Vereinte Nationen (OCHA) sind bereits 2.100 Menschen an der Krankheit gestorben. Im Mai 2017 meldete die Weltgesundheitsorganisation, dass es innerhalb einer Zeitspanne von zwei Wochen zu 51 Todesfällen kam.
Im Dezember 2017 bestätigte das Internationale Komitee vom Roten Kreuz 1 Million Verdachtsfälle. Es ist die größte je erfasste Cholera-Epidemie in der Geschichte der Menschheit.

Entwicklung der Lebenserwartung
Quelle: UN

Die Geschichte des Jemen ist, von kurzen Intervallen abgesehen, von Armut geprägt. Diese wird von den knappen Wasserressourcen, dem wenigen für die Landwirtschaft zur Verfügung stehenden Land, der rauen Geographie und der politischen Instabilität verursacht.

Die Schätzungen, wie viele Jemeniten in Armut leben, reichen von 41,8 Prozent (1998) bis 59,5 Prozent (2002). Der Human Poverty Index des Landes wird mit 36,6 Prozent angegeben, wobei der Jemen besonders bei Bildung, Zugang zu sauberem Trinkwasser und Ernährung für Kinder schlecht abschneidet. Die Zahl jener, die sich nicht ausreichend ernähren können, ist in den vergangenen Jahren gestiegen. 57 Prozent der Menschen haben keinen Zugang zu Hygieneeinrichtungen, und 32 Prozent haben keinen Zugang zu sauberem Trinkwasser.

Armut ist im Jemen vor allem ein ländliches Problem. 83 Prozent der Armen leben auf dem Land, dort lebt fast die Hälfte der Bevölkerung unter der Armutsgrenze. Die Landbevölkerung muss zwei Drittel ihres Einkommens für die Ernährung ausgeben. Armut ist im Jemen nicht gleich verteilt: Die Gouvernements mit dem höchsten Anteil an Armen sind Ta'izz, Ibb, Abyan und Lahidsch, am wenigsten von Armut betroffen sind al-Baida’, der Hauptstadtbezirk, Sa'da und Adan.

Die Zahl der Menschen im Jemen, die sich nicht ausreichend ernähren können, wird auf 8 Millionen geschätzt; 38 Prozent der Bevölkerung sind großer Ernährungsunsicherheit ausgesetzt. Die durchschnittliche Kalorienaufnahme pro Person beträgt nur 2000 kcal. Neben dem Sudan ist der Jemen somit das Land mit dem größten Heer an Hungrigen. Große Familien, die Landbevölkerung, Familien, die nur kleine Landflächen zur Verfügung haben, oder Haushalte, die allein von Frauen unterhalten werden müssen, sind von Hunger besonders bedroht. Die Zahl der Hungrigen ist zwischen 1990 und 2002 sogar gestiegen, sowohl in absoluten Zahlen (von 4,2 Millionen auf 7,8 Millionen) als auch ihr Anteil an der Gesamtbevölkerung (von 34 auf 38 Prozent). Somit verfehlt der Jemen nicht nur die ersten UN-Millenniumsziele, nämlich die Zahl der Hungernden zu reduzieren, sondern er entfernt sich sogar weiter davon. 2003 waren 45,6 Prozent der Kinder unter fünf Jahren im Jemen untergewichtig.

Der Bürgerkrieg und die von Saudi-Arabien angeführte Militärintervention haben die Ernährungslage und das Ausmaß der Armut weiter verschärft. Knapp zwei Drittel der Bevölkerung waren 2017 vom Hunger bedroht und auf Hilfe aus dem Ausland angewiesen.

In der vorislamischen Zeit unter den Kulturen der Minäer und Sabäer (ab dem 2. Jahrtausend v. Chr.) entwickelte sich das Gebiet des heutigen Jemen als Drehscheibe des Fernhandels zwischen Ostafrika, Indien und dem Mittelmeerraum und Hauptlieferant begehrter Erzeugnisse wie Edelsteine, Gewürze, Weihrauch und Myrrhe zum politischen und kulturellen Zentrum Arabiens. Die wirtschaftliche Grundlage bildete eine hochentwickelte Bewässerungstechnik, die den Regen aus dem Gebirge nutzbar machte. Die bedeutendste Anlage war der (heute als Großprojekt neu konstruierte) Staudamm von Ma'rib (8. Jahrhundert v. Chr.).

Unter mehreren regionalen Königreichen übte Saba besonders vom 6. bis 4. Jahrhundert v. Chr. eine gewisse Vormachtstellung aus. Mit der Gründung der neuen Hauptstadt Zafar um 20 v. Chr. begann der Aufstieg des Himyar-Reichs (bis 525 n. Chr.). Die Römer nannten den Jemen wegen seiner Reichtümer "Arabia Felix" (glückliches Arabien). Ihr Versuch, das Land zu erobern, scheiterte. Nach ihrer Niederlage gegen die Römer im Ersten Jüdischen Krieg 70 n. Chr. brachten Flüchtlinge das Judentum in den Jemen. Zwar gelang den Himjariten im 3. Jahrhundert nochmals die Einigung des Landes, doch wurde es 525 vom äthiopischen Königreich Aksum erobert.

Unter äthiopischem Einfluss verbreitete sich in Teilen Südarabiens das Christentum. Von ca. 570 bis 627 war der Jemen eine Provinz des persischen Sassanidenreichs. Eine persische Hinterlassenschaft war das 1980 wiederentdeckte Bergwerk von ar-Radrad.

Im 7. Jahrhundert breitete sich die Lehre des Propheten Mohammed auf der Arabischen Halbinsel aus. Der letzte persische Statthalter, Badham, wurde 628 Muslim. Ab dieser Zeit fiel der Jemen in den Herrschaftsbereich des Islams und gehörte ab 661 zum Reich der umayyadischen Kalifen. Aufgrund religiös-politischer Machtkämpfe zerfiel dieses Reich Ende des 9. Jahrhunderts in Teilstaaten. Im 10. Jahrhundert bildete sich im Jemen ein zaiditisches Imamat, das mit Unterbrechungen bis zur Mitte des 20. Jahrhunderts weiterbestand. Daneben herrschten zeitweise verschiedene andere Dynastien über weite Teile des Jemen: die ismailitischen Fatimiden und Sulaihiden (11./12. Jahrhundert), die Ayyubiden (12./13. Jahrhundert) und Rasuliden (13.–15. Jahrhundert) sowie von 1538 bis 1630 die Osmanen. Im 16. Jahrhundert besetzten die Portugiesen zeitweise Aden und Sokotra.

1839 besetzten die Briten Aden, das zum Stützpunkt auf dem bedeutenden Seeweg nach Indien wurde (ab 1937 Kronkolonie). Mit der Eröffnung des Sueskanals 1869 stieg die strategische Bedeutung Adens für Großbritannien weiter. 1905 legten das Osmanische Reich und Großbritannien die Grenze zwischen ihren Protektoraten fest. Nach dem Zusammenbruch des Osmanischen Reiches im Ersten Weltkrieg wurde der Norden Jemens 1918 ein unabhängiges Königreich unter dem Imam Yahya. Dies führte zu einem Territorialkonflikt mit Saudi-Arabien, der sich 1934 in einem Krieg zwischen den beiden Monarchien entlud. 1944 gründeten im Adener Exil Kaufleute, Intellektuelle und religiöse Führer die Oppositionsbewegung der „Freien Jemeniten“ gegen Yahya. Im Verlauf einer Revolte gelang es der Gruppe, diesen 1948 zu ermorden; sein Sohn, Imam Ahmad, konnte den Aufstand jedoch niederschlagen. Ein weiterer Aufstand scheiterte 1955.

Allerdings war es den konservativen Imamen im Nordjemen nicht gelungen, das Land zu modernisieren. Ahmad lehnte Nassers arabischen Nationalismus ab, der aber auf die Zustimmung großer Teile der Streitkräfte stieß. Bevor die Situation eskalieren konnte, starb der Herrscher. Nach Ahmads Tod stürzte am 26. September 1962 eine Gruppe nationalistischer, sunnitischer Offiziere unter der Führung von General Abdallah as-Sallal die zaiditische Monarchie und proklamierte im Norden die Jemenitische Arabische Republik. Der letzte zaiditische Imam, Muhammad al-Badr, floh in die Berge zu loyalen Stämmen. Im darauf ausbrechenden achtjährigen Bürgerkrieg zwischen Royalisten und Republikanern unterstützten in einem Stellvertreterkrieg Großbritannien und Saudi-Arabien die gestürzte Monarchie, während Ägypten den Republikanern mit einer 20.000 Mann starken Expeditionsarmee half, die schließlich die Oberhand behielt. Ähnlich wie in London dominierte in Washington die Befürchtung, ein Fehlschlag der Saudis könnte den panarabischen Nationalismus stärken und somit die saudische Monarchie gefährden. Auch nach der Niederlage von al-Badr blieb die politische Lage instabil. Der Krieg zwischen den Republikanern und den Royalisten, bei dem auch chemische Waffen durch ägyptische Truppen zum Einsatz kamen, hatte 200.000 Tote und die totale Zerrüttung des Nordens zur Folge. 1970 endete der Bürgerkrieg mit einem Kompromiss, der keine Seite zufriedenstellte und vor allem die Autonomie der Stämme stärkte.
Auch der Süden wurde von politischen Unruhen erschüttert. Während des Bürgerkriegs waren viele linke Nationalisten und Kommunisten nach Aden geflohen. 1963 begannen dort die neugegründete, radikalere „Nationale Befreiungsfront“ (NLF), die, gut ausgerüstet, in ihrem Kampf auch Minenwerfer und Panzerfäuste einsetzte, und die von Kairo unterstützte, einen arabischen Nationalismus vertretende Front for the Liberation of South Yemen (Flosy), einen Guerillakrieg gegen die Kolonialmacht Großbritannien. Nachdem dieses für 1968 die Unabhängigkeit in Aussicht gestellt hatte, konnte die NLF mit Hilfe der Bevölkerung die meisten Gebiete der Kronkolonie unter ihre Kontrolle bringen. Großbritannien nahm daraufhin Verhandlungen mit der NLF auf und zog seine Truppen zurück. Am 20. November 1967 reiste der letzte Britische Hochkommissar Humphrey Trevelyan in einem Flugzeug aus, das ihn nach London zurückbrachte. Am 30. November 1967 rief die NLF die Republik Südjemen aus. In der Folge kam es zu einem Konflikt zwischen linken Kräften, welche die NLF dominierten, und dem Militär, was beinahe zu einem Bürgerkrieg führte. Die neue Regierung unter Qahtan Muhammad asch-Scha'abi verfolgte von Beginn an einen sozialistischen Kurs und lehnte sich eng an die Sowjetunion an. Als der rechte Flügel der NLF die Forderungen des Parteikongress blockierte, entstand die „Bewegung des 14. Mai“, die das Volk zur Unterstützung der Reformen mobilisieren sollte. Nach einem Jahr gewann diese Bewegung die Oberhand gegen die Armee.

Nach dem Sturz von as-Sallal 1967 folgten im Norden häufige Regierungswechsel und Attentate. Präsident Abdul Rahman al-Iriani wurde 1974 gestürzt, dessen Nachfolger Ibrahim al-Hamdi und Ahmed Hussein al-Ghaschmi 1977 bzw. 1978 ermordet. Ein weiteres Konfliktpotential bot der sich zuspitzende Gegensatz zwischen den fundamentalistischen schiitischen Stammesföderationen im Nordosten und der überwiegend sunnitischen, modernen, westlichen Strömungen gegenüber aufgeschlossenen Stadtbevölkerung.

Der Süden erhielt 1970 entgegen Moskaus und Pekings Rat eine neue, sozialistische Verfassung, nachdem 1969 Salim Rabi Ali neues Staatsoberhaupt geworden war. Gleichbedeutend damit war in der Folge das Monopol der Jemenitischen Sozialistischen Partei (JSP), einer marxistischen Einheitspartei, sowie ein totales Verbot von traditionell wichtigen Kleinunternehmen. 1976 kam es nach wiederholten Zusammenstößen zu einer Aussöhnung mit Saudi-Arabien, das ebenso wie Kuwait umfangreiche Wirtschaftshilfe anbot. 1978 war kurzzeitig Ali Nasir Muhammad Staatsoberhaupt, der im selben Jahr von Abd al-Fatah Ismail abgelöst wurde. 1980 übernahm erneut Ali Nasir Muhammad, ein skrupelloser und fast analphabetischer Apparatschik, die Macht. Er wird mit gegenrevolutionären Einflüssen aus dem Ausland, vor allem Saudi-Arabien und den USA, in Verbindung gebracht. Sein Vorgänger, der charismatische Abdul Fattah Ismail, war aus gesundheitlichen Gründen zurückgetreten. Ismail kehrte erst 1985 nach einer langen Rekonvaleszenz aus Moskau zurück. Er hatte eine führende Rolle im Kampf gegen die britische Kolonialmacht gespielt und genoss deshalb noch große Unterstützung. Bald nach seiner Rückkehr wurde er erneut ins Politbüro der Staatspartei gewählt, wo er eine Mehrheit der Mitglieder hinter sich hatte. Auch die wirtschaftliche Bindung an den Ostblock verstärkte sich. Am 13. Januar 1986 kam es zu einem Bürgerkrieg, der damit begann, dass Ali Nasir nicht an der Sitzung des Politbüro erschien, sondern seine Leibwächter den Vizepräsidenten Ali Ahmed Antar und vier weitere Mitglieder des Politbüros töteten. In den folgenden Auseinandersetzungen kamen mehrere tausend Menschen ums Leben, und Haidar Abu Bakr al-Attas gelangte an die Macht, während der am 24. Januar 1986 abgesetzte Ali Nasir mit 60.000 anderen nach Nordjemen flüchtete. In den westlichen Medien wurde diese Episode als ein von Moskau unterstützter gescheiterter Putschversuch von Kommunisten gegen einen gemäßigten und pragmatischen Präsidenten kommuniziert.

1972, 1979 und 1981 kam es immer wieder zu Grenzzwischenfällen zwischen dem Norden und dem Süden. Parallel dazu fanden Verhandlungen statt, die eine politische Union der beiden Staaten zum Ziel hatten. 1973 scheiterte ein Vorstoß noch am nordjemenitischen Widerstand, doch verbesserten sich die bilateralen Beziehungen seit Beginn der 1980er Jahre. In den 80er Jahren litt das sozialistische Südjemen unter außenpolitischem Druck und innenpolitischen konterrevolutionären Bestrebungen vor allem während der Reagan-Ära.

Am 22. Januar 1990 verkündeten die Ministerpräsidenten beider Staaten die Öffnung ihrer gemeinsamen Grenze. Am 22. Mai desselben Jahres schlossen sich die Arabische Republik Jemen und die Demokratische Volksrepublik Jemen zur Republik Jemen zusammen. Der erste gesamtjemenitische Präsident wurde Ali Abdullah Salih, der seit 1978 die Arabische Republik Jemen regierte. Im Golfkrieg von 1990 hatte Jemen noch den Irak unterstützt, was sich für den Jemen insofern katastrophal auswirkte, als sie als votierendes Mitglied des UN-Sicherheitsrats nunmehr den Kürzungen, oft Streichungen der Entwicklungshilfemaßnahmen der arabischen Öl-Staaten ausgesetzt waren. Zudem wiesen die Golfstaaten alle jemenitischen Arbeitsmigranten, mithin etwa 800.000 Menschen aus ihren Ländern aus, was zum Ausfall von Rücküberweisungen von rund einer Milliarde Dollar führte und den Staatshaushalt extrem belastete. 1999 konnte der Jemen seine Beziehungen zu Kuwait normalisieren.

Am 27. April 1993 fanden im Jemen die ersten freien Parlamentswahlen statt, in denen sich drei große Parteien gegenüberstanden: der Allgemeine Volkskongress, die Sozialistische Partei sowie die Jemenitische Vereinigung für Reformen (Islah). Die Koalition von Islah und Volkskongress wurde fast Modell für eine arabische Demokratisierung. Allerdings behielten alle Parteien ihre Truppen, was durch militärische Ausgewogenheit für eine gewisse Stabilität sorgte. Am 20. Februar 1994 wurde in Amman, Jordanien, ein Abkommen zwischen den politischen Führern des Nord- und Südjemens unterzeichnet, aber dies konnte den Bürgerkrieg zwischen den Beteiligten nicht verhindern, der von Mai bis Juli 1994 ausgetragen wurde und mit der Niederlage der südlichen Streitkräfte und der Flucht ins Exil vieler Jemeniten und Anhänger der Sozialistischen Partei endete. Der Bürgerkrieg begann, indem die Regierung in Sanaa den Notstand ausgerufen hatte. In der Zeit vom 5. Mai bis zum 7. Juli 1994 hatten 7000 Menschen ihr Leben verloren. Für den Demokratisierungsprozess war der Bürgerkrieg ein eklatanter Rückschlag.

Die Parlamentswahl im April 1997 wurde von den Sozialisten boykottiert, da sie nach dem Bürgerkrieg von 1994 in der südjemenitischen Stammwählerschaft diskreditiert waren und aufgrund der Konfiszierung ihrer Konten und Immobilien nach Beendigung des Krieges nicht über die für eine Wahlkampagne nötigen Ressourcen verfügten, so dass Präsident Salih fortan mit einer absoluten Mehrheit ohne die Islah regieren konnte.

Am 23. September 1999 wurde Salih ein fünftes Mal zum Präsidenten gewählt. Sein einziger Gegenkandidat, der langjährige Parlamentsvorsitzende und Scheich Abdallah al-Ahmar, war aus dessen eigenen Reihen ausgewählt worden und somit entfielen 96,3 % der Stimmen auf Salih. In nur sechs Jahren war das Land wieder zu einem Einparteienstaat geworden.

Im Februar 2001 konnte die Staatspartei ihre Macht mit einer durch ein Referendum abgesicherten dritten Verfassungsreform stärken. Der Konsultationsrat wurde in eine zweite Kammer umgewandelt (Madschlis asch-Schura) und die präsidiale Amtszeit dauert nun sieben statt fünf Jahre. Umgehend wurde der Druck auf die Oppositionsparteien erhöht, obwohl die Regionalwahlen im Februar 2002 durch ein Dezentralisierungsgesetz zu pluralistischen Gemeinde- und Regionalräten führten.

Salih selbst kündigte an, dass er bei den nächsten Präsidentschaftswahlen nicht antreten werde. Diese Entscheidung revidierte er im Juni 2006, nachdem in – von seiner Partei organisierten – Massendemonstrationen seine erneute Kandidatur gefordert worden war. 2006 siegte Ali Abdullah Salih bei den ersten von echter Konkurrenz geprägten Präsidentschaftswahlen auf der arabischen Halbinsel gegen den Kandidaten des Oppositionsbündnisses „Gemeinsames Treffen“, Faisal bin Schamlan, mit 77,2 % der Stimmen.

Seit der Abschiebung jemenitischer Wanderarbeiter aus Saudi-Arabien 1991 nahmen Anschläge auf westliche Einrichtungen und Touristen im Jemen zu. Auch Anschläge im Ausland wurden mit terroristischen Strukturen im Jemen (wie al-Qaida auf der arabischen Halbinsel) in Verbindung gebracht.

Der militärische Konflikt mit der zaiditischen al-Huthi-Bewegung im Nordjemen, der sich auch auf angrenzende Gouvernements und Saudi-Arabien ausgedehnt hatte, forderte Tausende Todesopfer und trieb schätzungsweise 77.000 Zivilisten in die Flucht. Hussein Badr ed-Din al-Huthi war bereits im September 2004 nach einer dreimonatigen Rebellion getötet worden. Präsident Salih gewährte am 25. September 2005 den inhaftierten Anhängern (über 600 Personen) des schiitischen Predigers Amnestie; allerdings kam es später zu neuen Festnahmen und Verurteilungen, auch zu Todesstrafen. Auch eine sezessionistische Bewegung im früheren Südjemen ist seit 2009 aktiv und führt teilweise blutige Auseinandersetzungen mit regimetreuen Einheiten.

In den letzten Jahren kam es immer wieder zu Entführungen ausländischer Touristen. Diese haben anders als im Irak oder in Afghanistan in der Regel keinen religiösen oder ideologischen Hintergrund. Den Entführern ging es meist vielmehr darum, die Geiseln als Druckmittel gegenüber der Regierung zu benutzen, so etwa für die Freilassung von inhaftierten Stammesangehörigen oder den Bau von Schulen oder Straßen in ihrer Region. Am 28. Dezember 2005 wurde der auf einer privaten Reise im Jemen weilende frühere Staatssekretär im Auswärtigen Amt der Bundesrepublik Deutschland, Jürgen Chrobog, zusammen mit seiner Familie entführt, aber bereits am 31. Dezember wieder freigelassen. Dabei handelte es sich um die dritte Entführung von Ausländern innerhalb weniger Wochen. Am Weihnachtswochenende waren zwei Österreicher nach mehrtägiger Entführung freigelassen worden, die Geiselnahme von fünf Italienern am 1. Januar 2006 endete fünf Tage später mit deren Freilassung. Nicht immer verlaufen solche Entführungen harmlos: Am 12. Juni 2009 wurden die beiden Krankenschwestern Anita Grünwald und Rita Stump in einer Wüstenregion des Nordjemens entführt und wenig später mit auf den Rücken gefesselten Händen erschossen. Außerdem starb dabei eine koreanische Kollegin.

Mittels einer spektakulären Flucht gelang es am 3. Februar 2006 einer Gruppe von 23 Gefangenen, aus einem Hochsicherheitsgefängnis in Sanaa zu entkommen. Darunter waren auch 13 Angehörige von al-Qaida, die unter anderem wegen des Anschlags auf das US-amerikanische Kriegsschiff USS Cole im Oktober 2000 sowie den französischen Öltanker Limbourg am 6. Oktober 2002 inhaftiert worden waren. Neun der Ausbrecher konnten bis zum Mai 2006 wieder gefasst werden. Am 27. Februar wurde in Sanaa die Todesstrafe gegen den Mörder dreier US-amerikanischer Mitarbeiter eines Missionskrankenhauses in Dschibla im Dezember 2002 vollstreckt. Einen ungewöhnlichen Weg ging die jemenitische Regierung mit dem von dem Richter al-Hitar geleiteten Umerziehungsprogramm für inhaftierte Islamisten.

Infolge der Proteste in der Arabischen Welt Anfang 2011 kam es auch im Jemen ab dem 27. Januar zu Demonstrationen. Die Demonstranten forderten den Rücktritt des seit mehr als 30 Jahren regierenden Präsident Ali Abdullah Salih, den sie für die schlechte wirtschaftliche Lage großer Bevölkerungsteile verantwortlich machen. Salih kündigte im November 2011 seinen Rücktritt an. Bei der folgenden Präsidentschaftswahl wurde der bisherige Vizepräsident Abed Rabbo Mansur Hadi "als einziger Kandidat und Protegé Saudi-Arabiens" für eine Amtszeit von zwei Jahren gewählt, in der er eine Verfassungsreform erwirken soll.
"Siehe auch": Proteste im Jemen 2011

Als nach den Wahlen im Februar 2012 Präsident Ali Abdullah Salih nach 34 Regierungsjahren zurücktrat, erhoffte man von seinem Nachfolger Mansur Hadi mehr Demokratie und eine ausgleichende Wirkung auf die Kontrahenten. Er erwies sich allerdings als dafür ungeeignet und verlor bald die Kontrolle über seinen Machtapparat. Einzelne Generäle kämpfen seit 2013 mit ihren Truppen auf eigene Faust.

Seit die aus dem ehemaligen Nordjemen kommenden schiitischen Huthi-Rebellen neben der Hauptstadt Sanaa auch die wichtige Hafenmetropole al-Hudaida eingenommen haben, stoßen sie nun mit den von Osten kommenden Al-Qaida-Kämpfern in den Küstenregionen zusammen. Dem jemenitischen Al-Qaida-Ableger, den die US-Drohnenangriffe nicht wesentlich behindern können, gelang es 2014, die Provinzhauptstadt Ibb und westlich davon Mudaichira einzunehmen. Mitte Oktober 2014 sprengte sich ein Selbstmordattentäter in einer Huthi-Versammlung in die Luft und tötete 50 Personen, am 21. Oktober 2014 starben weitere 33 Menschen bei einer Bombe in einem Amtshaus.

Die Zentralregierung versucht vergeblich, die Lage zu kontrollieren. Mittlerweile haben sich sunnitische Stammeskämpfer mit der Al-Qaida gegen die Huthi verbündet, und die Kämpfe weiten sich immer mehr aus.

Am 23. Januar 2015 traten Präsident, Premierminister und Kabinett zurück. Am 6. Februar 2015 verkündeten die Huthi-Rebellen eine Übergangsverfassung und erklärten das Parlament für aufgelöst. Es soll provisorisch durch einen Nationalrat mit 551 Mitgliedern ersetzt werden, ebenso Präsident Hadi für zwei Jahre durch einen fünfköpfigen Präsidentschaftsrat.

Am 26. März 2015 begann eine Militärintervention mit saudi-arabischen Luftangriffen im Jemen unter der Bezeichnung "Sturm der Entschlossenheit". An der saudi-arabisch angeführten und von den Vereinigten Staaten von Amerika, Frankreich und Großbritannien logistisch unterstützten Militärintervention nahmen unter anderem die Streitkräfte Ägyptens, Bahrains, Katars, Kuwaits, Jordaniens, Marokkos, Sudans und der Vereinigten Arabischen Emirate aktiv teil. Anfang Juli 2015 rief die UNO aufgrund der eskalierenden humanitären Notlage während des Krieges die höchste Notstandsstufe der UN für den Jemen aus, während die UNESCO aufgrund des bewaffneten Konflikts zwei Weltkulturerbestätten im Jemen für bedroht erklärte. Seitdem werden "No-Strike"-Listen von Blue Shield zum Schutz der Kulturgüter erstellt. Nach Angaben der Vereinten Nationen wurden bis Februar 2017 über 4600 Zivilisten im Jemen getötet. Mindestens 19 Millionen Jemeniten waren nach UN-Angaben auf humanitäre Hilfe angewiesen. Die Norwegische Flüchtlingshilfe warnte vielfach vor einer akuten Nahrungsmittelknappheit. So sei die Versorgungslinie zum Hafen von al-Hudaida essentiell, da über diesen die meisten Importe abgewickelt werden. Bereits im Januar 2017 veröffentlichte die internationalen Organisation für medizinische Nothilfe Médecins Sans Frontières (Ärzte ohne Grenzen) einen Bericht im Bezug auf die medizinische Versorgung im Jemen, hervorgehoben wurde die verheerende Lage in der Stadt Taizz, im südlichen Teil des Landes.

Nach Art. 1, Abs. 1 der Verfassung von 1994, zuletzt geändert 2001, ist Jemen ein arabisch-islamischer unabhängiger und souveräner Staat. Das Parlament, das "Repräsentantenhaus des Jemen", wird alle sechs Jahre gewählt und besteht aus 301 Abgeordneten (159 aus dem Norden und 111 aus dem Süden sowie 31 politische Persönlichkeiten, die die „nationalen Kräfte“ repräsentieren). Das Staatsoberhaupt wird alle sieben Jahre gewählt mit der Möglichkeit der einmaligen Wiederwahl. Alle Jemeniten im Alter ab 18 Jahren verfügen über das Wahlrecht.

Die letzten Parlamentswahlen vom 27. April 2003 gewann der "Allgemeine Volkskongress" (MSA – ehemals Einheitspartei im Nordjemen) mit 238 (1997: 187) Sitzen. Die "Vereinigung für Reformen" ("Islah" – erhält seit der Rückkehr von Mudschaheddin aus Afghanistan in ihre Heimatländer politischen Auftrieb) gewann daneben 46 (53), die "Sozialistische Partei Jemens" (YSP – ehemals Einheitspartei des Südjemen) 8 (0), die "Nasseristische Unionistische Volkspartei" (TWSN) gemeinsam mit der "Arabischen Sozialistischen Baath-Partei" (Baath) 5 (7) und unabhängige Kandidaten 4 (54) Sitze.

Am 23. September 2006 wurde Präsident Salih unter Protesten der Oppositionsparteien für eine weitere Amtszeit wiedergewählt. Seinem Sohn Ahmad Salih wurde die Absicht nachgesagt, seinem Vater 2013 nachzufolgen, bis dieser am 23. November 2011 infolge anhaltender Proteste der Bevölkerung die Macht an seinen bisherigen Stellvertreter Abed Rabbo Mansur Hadi übergab und damit den Weg zu vorgezogenen Neuwahlen am 21. Februar 2012 freimachte.

Seit 2004 bemüht sich die Regierung, den Aufstand der zaiditischen Bewegung „Gläubige Jugend“ (الشباب المؤمنين) unter der Führung der al-Huthi-Familie im Gouvernement Sa'da niederzuschlagen. Die „Gläubige Jugend“ wendet sich gegen sunnitisch-wahhabitische Bekehrungskampagnen im zaiditischen Norden, gegen die Benachteiligung der an der saudischen Grenze gelegenen, traditionell antirepublikanischen Gouvernements bei der Entwicklung des Landes und gegen die jemenitische Regierung, die als Verbündeter der Vereinigten Staaten wahrgenommen wird.

Die Hirak-Bewegung hingegen betreibt in Anknüpfung an den Bürgerkrieg von 1994 eine Sezession des Südjemens. Zu ihren Führern gehören der im Exil lebende ehemalige sozialistische südjemenitische Präsident Salim al-Bid, aber auch prominente Islamisten. Seit Anfang 2009 flackern auch in den südlichen Gouvernements des Landes (insbesondere Lahedsch, Aden, Abjan) gewaltsame Proteste gegen die Vormachtstellung der nordjemenitischen Elite auf.

Die Konflikte nähren Befürchtungen, dass dem Staat die – ohnehin durch die Stammesstrukturen beschränkte – Kontrolle entgleitet und der Jemen wie Afghanistan oder Somalia zu einem gescheiterten Staat werden könnte, der terroristischen Bewegungen Zuflucht bietet. In diesem Zusammenhang besteht auch die Gefahr, dass al-Qaida-Terroristen aus Somalia und Jemen verstärkt zusammenarbeiten. Andererseits ist die Situation im Jemen insofern besonders, als die Organe des jemenitischen Staates grundsätzlich nach wie vor effektive Kontrolle über alle Teile seines Territoriums ausüben, und die Stämme weder ethnisch divers sind noch in größeren Verbänden miteinander im Konflikt stehen. In großen Teilen des Landes, vor allem den Städten und im postkommunistischen Süden, spielen Stammesstrukturen heutzutage keine politische Rolle mehr. Dennoch beschränkt die mangelnde Ausstattung und Korruptionsanfälligkeit der Sicherheitsorgane in einem geographisch weit ausgedehnten und zunehmend armen Land die Interventionsmöglichkeiten des Staates.

Das Parlament ist gesetzgebendes Organ, die Judikative autonom. Nur die Verhängung der Todesstrafe erfordert laut Verfassung die Genehmigung des Staatspräsidenten, der gleichzeitig dem aufsichtsführenden Kontrollgremium über die Gerichtsorganisation vorsteht. Höchstes judikatives Organ ist der Oberste Gerichtshof.
Der Islam ist Staatsreligion, es gilt die Scharia. Die strenge Ausrichtung des Rechtes führt zur Verweigerung vieler Menschenrechte, wie zum Beispiel die freie Wahl der Religion.

Das Schutzalter, von dem ab eine Person juristisch als einwilligungsfähig bezüglich sexueller Handlungen angesehen wird, wurde 1999 von ehemals 15 Jahren auf den Beginn der Pubertät gesenkt, womit im Jemen im Regelfall ein Alter von neun Jahren gemeint ist. Der Anteil der Mädchen, die noch vor ihrer Volljährigkeit verheiratet werden, liegt im Jemen bei 37 % und wird nur von Somalia (45 %) übertroffen. Für weltweite Schlagzeilen sorgte in diesem Zusammenhang Anfang 2008 der Fall eines zehnjährigen Mädchens Nojoud Ali, das vor Gericht die Scheidung von ihrem 22 Jahre älteren Ehemann erstritt. Ende Februar 2009 beschloss das jemenitische Parlament ein Gesetz, das das Mindestalter für Heiraten auf 17 Jahre festlegt. Gegen dieses Gesetz wandte sich eine Gruppe prominenter religiöser Persönlichkeiten des Jemen, die es als unvereinbar mit der Scharia bezeichneten.

Homosexuelle Handlungen stehen unter Strafe. Das Strafmaß reicht hierbei von Geldstrafen und Auspeitschung bis hin zur Todesstrafe bei Männern für homosexuellen Geschlechtsverkehr.

Der Jemen gehört zu den Staaten, in denen die Beschneidung weiblicher Genitalien praktiziert wird. Etwa 22,6 % der Frauen zwischen 15 und 49 Jahren waren 1997 betroffen. Obwohl der Jemen bereits 1984 das Übereinkommen zur Beseitigung jeder Form von Diskriminierung der Frau ratifiziert hat, wird geschätzt, dass 50 % aller verheirateten Frauen Gewalt ausgesetzt sind. Der Jemen hat das Zusatzprotokoll zur UN-Kinderrechtskonvention ratifiziert, welches die Rekrutierung von Kindern in bewaffneten Konflikten verbietet.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte der Jemen Platz 166 von 180 Ländern. Meinungs- und Pressefreiheit sind damit nicht gegeben. In Jemen sitzt ein Journalist in Haft.

Am 22. Januar 2018 wurde ein Journalist, Mohamed Al Qadesi, in Jemen getötet. Nachweislich steht der Tod des Opfers in direktem Zusammenhang mit seiner journalistischen Tätigkeit.

Der Jemen ist Mitglied der Vereinten Nationen (UNO) und der Arabischen Liga. Jemen will Mitglied des Golf-Kooperationsrates werden. Im Januar 2002 trat der Jemen dem Golf-Kooperationsrat bei, vorerst nur als Beobachter. Der Jemen beschuldigt den Iran, den Aufstand der jemenitischen Schiiten im eigenen Interesse zu unterstützen.

Die Vereinigten Staaten und den Jemen verbindet der Anti-Terrorkampf, aber die Beziehungen sind immer wieder belastet. Der Jemen schlägt seit Beginn der Zweiten Intifada in Palästina eine härtere Gangart im Nahostkonflikt ein und ist bei der Zusammenarbeit für mehr Sicherheit zaghaft. Aber auch der Einsatz einer US-Drohne, die am 3. November 2002 sechs mutmaßliche al-Qaida-Kämpfer im Jemen tötete, sowie die Ermordung dreier amerikanischer Missionare am 31. Dezember 2002 belasten die Beziehungen. Ein großer Teil der in Guantanamo Inhaftierten stammt aus dem Jemen. Trotzdem ist Washington an einer Stärkung der jemenitischen Regierung interessiert. 2004 empfing US-Präsident George W. Bush Ali Abdullah Salih im Weißen Haus. Auch die nachfolgende Regierung unter Präsident Obama leitete weiterhin Militärhilfe in den Jemen, auch nach dem Sturz von Salih.

Der Konflikt um den genauen Grenzverlauf zum Sultanat Oman wurde beigelegt. Das Abkommen von Dschidda beendete die Grenzstreitigkeiten mit Saudi-Arabien. Jemen erhielt einen Gebietsstreifen, unter dem sich höchstwahrscheinlich Erdöl befindet, und akzeptierte im Gegenzug das Abkommen von Taif 1934, in dem Imam Yahya bin Muhammad Saudi-Arabien zwei Provinzen überließ. In Bezug auf die Hanisch-Inseln wurde der Internationale Gerichtshof angerufen, der im Oktober 1998 gegen Eritrea entschied.

1998 wurden diplomatische Beziehungen zwischen Jemen und dem Heiligen Stuhl aufgenommen, und Frankreich stufte den Jemen als „Zone de solidarité prioritaire“ ein, was eine verstärkte Zusammenarbeit zwischen beiden Ländern bedeutet.
Im Somalia-Konflikt konnte die jemenitische Regierung im Dezember 2000 mit Erfolg vermitteln.

Die "Jemenitischen Streitkräfte" entstanden offiziell aus der Vereinigung der Armeen Nord- und Südjemens im Mai 1990, wobei es ab Mai 1994 zu Kampfhandlungen zwischen den beiden Armeen kam, die erst mit ihrer vollständigen Zusammenfassung im März 1995 endgültig beendet werden konnten. Im Zuge des Bürgerkriegs im Jemen und der Militärintervention im Jemen seit 2015 sind die Streitkräfte in Anhänger von Ex-Präsident Ali Abdullah Salih im Norden und die mit der Golf-Allianz verbündeten Truppen im Süden gespalten.

Der Jemen gliedert sich in 20 Gouvernements und den Hauptstadtdistrikt. Diese 21 Verwaltungseinheiten werden in 333 Distrikte gegliedert, diese in 2200 Subdistrikte, diese in 36.986 Dörfer und diese in 91.489 Ortsteile ("localities and neighborhoods").

Sechs der 20 Gouvernements bildeten bis 1990 den Südjemen.

Die größten Städte sind (Stand 1. Januar 2005): Sanaa 1.937.451 Einwohner, al-Hudaida 617.888 Einwohner, Taizz 615.467 Einwohner, Aden 550.744 Einwohner und al-Mukalla 258.428 Einwohner.

Das Bruttoinlandsprodukt (BIP) schrumpfte 2015 aufgrund des Bürgerkriegs um 28,1 %. 2016 schrumpfte sie erneut um 4,2 %. Die Landwirtschaft hatte im selben Jahr einen Anteil von 23,6 %, die Industrie von 8,8 % und der Dienstleistungssektor von 67,5 % am BIP. 1999 waren 48,5 % der Beschäftigten in der Landwirtschaft, 15,1 % in der Industrie und 36,4 % im Dienstleistungssektor beschäftigt. Die Arbeitslosigkeit lag 2014 im Durchschnitt bei 27 % und die Inflation bei 31,5 %.

Mit einem kaufkraftbereinigten Bruttoinlandsprodukt von ca. 2500 US-Dollar im Jahr 2016 war der Jemen eines der ärmsten Länder der Welt. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt das Land den letzten Platz (Stand 2017–2018).

In dem Korruptionswahrnehmungsindex von Transparency International lag der Jemen 2017 auf Platz 175 von 180 Ländern und gehört damit zu den korruptesten Ländern der Welt.

Nur 2,9 % des Territoriums des Jemen sind landwirtschaftlich nutzbar, und weniger als 0,3 % werden ganzjährig bebaut. Etwa 5.500 Quadratkilometer werden bewässert. Weiterhin sind fast 4 % des Territoriums bewaldet. Mehr als 70 % des Landes bestehen aus Wüste.

Die Landwirtschaft des Jemen beschäftigt mehr als 50 % der Arbeitskräfte und trägt 20 % zum Bruttoinlandsprodukt bei. Angebaut werden Hirse, vor allem Sorghum, Mais, Früchte, Gemüse und Kaffee. Die Produktivität der Landwirtschaft ist aufgrund der fehlenden Wasserressourcen und der Knappheit an bebaubarem Land niedrig. So liegt die Getreideernte pro Hektar im Jemen bei 800 kg, was weit unter dem Weltdurchschnitt von 3000 kg liegt. Die eigene Landwirtschaft ist somit nicht in der Lage, die Bevölkerung zu ernähren. Jemen war zwar bis vor wenigen Jahren noch Selbstversorger, muss heute jedoch 75 % seiner Nahrungsmittel importieren, weshalb Nahrungsmittel 23 % der gesamten Importe ausmachen (Weltdurchschnitt: 7 %). Die Einwohner des Jemen sind aufgrund ihrer niedrigen Einkommen steigenden Weltmarktpreisen für Nahrungsmittel besonders stark ausgesetzt.

Der Anbau der Alltagsdroge Kath hat in den vergangenen Jahren viele traditionelle landwirtschaftliche Produkte verdrängt, was zur weiteren Steigerung der Importabhängigkeit für Nahrungsmittel geführt hat. Im Jahre 1990 wurde Kath auf der Hälfte der zur Verfügung stehenden Nutzfläche angebaut, mit steigender Tendenz. Für Kath geben viele Familien einen bemerkenswert hohen Anteil ihres Einkommens aus, der Preis für Kath ist sehr volatil. Insgesamt werden mit Kath etwa 5 % des BIP umgesetzt. Abgesehen von den Auswirkungen auf die Produktivität der Menschen verbraucht der Anbau von Kath sehr viel Wasser. Er bietet jedoch der Landbevölkerung die Möglichkeit, ein höheres Einkommen zu erwirtschaften als durch Subsistenzwirtschaft oder den Anbau anderer Kulturen und hat somit die Landflucht und das rapide Anwachsen der Stadtbevölkerung gebremst.

Die Landwirtschaft verbraucht 90 % der verfügbaren Wasserressourcen. Die Bewässerungsverfahren sind jedoch ineffizient und verlustreich, es gibt keine staatliche Kontrolle über die Nutzung des Wassers und die Wasserversorgungs- und Abwasserentsorgungsbetriebe haben keine ausreichenden Management- und Betriebskapazitäten. Die Vielzahl der Brunnen hat zu einem starken Absinken des Grundwasserspiegels geführt, in der Region um Sanaa sinkt er um sechs bis acht Meter pro Jahr. Die erneuerbaren Süßwasserressourcen wurden für 2005 auf 200 m³ pro Person geschätzt. Dies liegt weit unter dem weltweiten Durchschnitt von 6700 m³ und auch unter dem als Wasserknappheit betrachteten Niveau von 1000 m³; das UNDP spricht von "serious water stress". Gleichzeitig steigt die Verschmutzung der zur Verfügung stehenden Wasserressourcen an. Darüber hinaus wird erwartet, dass der Klimawandel zu noch größerer Trockenheit im Jemen führen wird. Von den 146 Ländern, für welche das UNDP einen "Environmental Sustainability Index" berechnet hat, liegt der Jemen demzufolge auf dem 137. Platz. Es ist deshalb fraglich, wie lange im jemenitischen Hochland, wo der größte Teil der Bevölkerung lebt, überhaupt noch Bauern siedeln können.

Die Fischerei trägt etwa 1,7 % zum BIP bei, Fische sind jedoch das zweitwichtigste Exportgut nach Erdöl. Jährlich werden etwa 290.000 Tonnen Fisch gefangen, meist von Fischern mit kleinen, nicht seetauglichen Booten. Die Infrastruktur zum Kühlen und Weiterverarbeiten von Fisch sowie Einrichtungen zur Überwachung der Fischereiaktivitäten wird gerade mit Hilfe der Weltbank errichtet.

Jemen ist ein erdölfördernder Staat. Seine Fördermenge ist jedoch, verglichen mit seinen Nachbarn, gering, und das Land ist auch kein OPEC-Mitglied. Im Unterschied zu anderen Staaten des Nahen Ostens überlässt die jemenitische Regierung die Förderung des Erdöls ausländischen (amerikanischen, französischen und südkoreanischen) Unternehmen, die die Gewinne mit der Regierung teilen. Die noch vorhandenen Reserven wurden 2007 auf drei Milliarden Barrel geschätzt, und es wird damit gerechnet, dass die Ölvorräte des Landes bereits vor 2020 erschöpft sein werden. Die tägliche Ölförderung ist ebenfalls im Sinken begriffen. Sie betrug 400.000 Barrel pro Tag im Jahr 2005, im Jahr 2008 wurden nur mehr 350.000 Barrel täglich gefördert.

Um die Erdgasvorräte des Landes besser exportieren zu können, wurde für 2,6 Milliarden US-Dollar eine Verflüssigungsanlage in Balhaf errichtet. Sie wurde 2009 in Betrieb genommen und kann 6,8 Millionen Tonnen Flüssiggas pro Jahr erzeugen, welches zu zwei Dritteln in die USA exportiert wird. Die Einnahmen aus dem Erdölexport stellen momentan etwa drei Viertel des Staatsbudgets. Der Export von Flüssiggas kann den zu erwartenden Einnahmeausfall nach Erschöpfung der Erdölvorräte allerdings nur teilweise ausgleichen.

Der Anteil der Wertschöpfung des produzierenden Gewerbes am BIP des Jemen beträgt nur 7 %. Dies ist auch für ein arabisches Land niedrig, wo der Durchschnitt bei 9,5 % liegt. Der größte Teil der Produktion findet in Kleinstunternehmen mit ein bis vier Mitarbeitern statt. Sie konzentrieren sich auf die Verarbeitung von landwirtschaftlichen Produkten und auf die Herstellung von Materialien für den Wohnungsbau. Weiters haben die Zement- und Textilindustrie eine gewisse Bedeutung.

Rohöl und Erdgas sind die wichtigsten Exportgüter des Jemen, 2007 machten sie 90 % aller Exporte aus. Weiter exportiert das Land Fisch in sehr begrenztem Umfang. Weitere Exportgüter gibt es praktisch nicht. Importiert werden vor allem Maschinen, Fahrzeuge und Fertigwaren. Da der Jemen über keine nennenswerten Raffinerien verfügt, müssen Treib- und Schmierstoffe eingeführt werden. Bemerkenswert ist auch der hohe Anteil von Nahrungsmitteln an den Importen. Wichtigste Lieferanten sind die Vereinigten Arabischen Emirate, Saudi-Arabien, die Volksrepublik China, die Schweiz und die USA. Für die deutschsprachigen Länder hat der Jemen als Absatzmarkt eine sehr geringe Bedeutung; da das Land die Sicherheit von Ausländern nicht garantieren kann, lehnen mitteleuropäische Unternehmen Aufträge aus dem Jemen nicht selten ab.

Die Handelsbilanz ist negativ. 2007 betrug das Handelsbilanzdefizit 7 %, nach positiven Jahren zwischen 2002 und 2006. Das Defizit spiegelt einerseits die hohen Investitionen wider, die in den Aufbau der Flüssiggasproduktion getätigt werden, wobei die Anlagen zur Gänze importiert werden müssen. Andererseits verdeutlicht das Defizit die Verletzlichkeit des Landes gegenüber fallenden Ölpreisen und steigenden Nahrungsmittelpreisen. Das Handelsbilanzdefizit muss durch Überweisungen von Gastarbeitern aus dem Ausland, von Direktinvestitionen und Hilfsgeldern der Gebergemeinschaft ausgeglichen werden. Die Zahlungsbilanz des Jemen ist stark unter Druck, seitdem jemenitische Gastarbeiter im arabischen Raum aus Sicherheitsgründen durch Arbeitskräfte aus dem asiatischen Raum ersetzt werden.

Die Anzahl der Touristen, die 2005 den Jemen besuchten, wurde auf 336.000 geschätzt und ist nunmehr auf wenige Tausend gesunken. Attraktiv für europäische Touristen sind die Altstadt von Sanaa, die historische Hauptstadt Schibam, die Medina von Zabid, die historische Stadt Tarim oder die Ausgrabungen von Baraqisch. Erholungstourismus hingegen findet nur sehr begrenzt statt.

Die Weiterentwicklung des Tourismus wird durch das Fehlen der dazu notwendigen Infrastruktur, vor allem aber durch die instabile Sicherheitslage erschwert. Das Auswärtige Amt der Bundesrepublik Deutschland warnt derzeit vor dem „erheblichen Risiko terroristischer Anschläge“ und dem „ständig hohen Entführungsrisiko“ sowie den „immer wieder aufflammenden Stammeskonflikten“ und rät von Reisen in den Jemen ausdrücklich ab.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 5,6 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 1,7 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 13,9 % des BIP.

Die Staatsverschuldung betrug 2016 23,4 Mrd. US-Dollar oder 85,8 % des BIP.

Im Jahr 2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:


Telekommunikationsdienstleistungen werden fast ausschließlich von TeleYemen angeboten. Die hohen Kosten angesichts der sehr niedrigen Einkommen der Bevölkerung bedingen, dass es 2016 im Land nur 24,7 % der Bevölkerung das Internet nutzte. Aus dem gleichen Grund gab es 2006 weniger als eine Million Telefonanschlüsse und zwei Millionen Mobiltelefoniebenutzer.

Passagier- und Gütertransport wird im Jemen fast ausschließlich auf der Straße bewerkstelligt. Das Straßennetz ist in den vergangenen Jahren von 48.000 auf 71.300 Kilometer angewachsen. Trotzdem ist es noch immer in einem wenig zufriedenstellenden Zustand: nur 6200 Straßenkilometer sind asphaltiert, viele ländliche Gebiete sind nicht an das Straßennetz angeschlossen. Im Nordteil des Landes werden die wichtigsten Städte jedoch mit guten Straßen verbunden und es wurden Linienbusse eingerichtet. Verbesserungen am Straßennetz werden mit Hilfe der Weltbank durchgeführt. Die Anzahl der Fahrzeuge pro 1000 Einwohner wurde für 2002–2004 auf 50 geschätzt. Die zahlreichen betagten Fahrzeuge im Jemen führen zu hoher Luftverschmutzung.

Der wichtigste Hafen des Jemen ist in Aden. Weitere Häfen befinden sich in al-Hudaida, Al-Mukalla und Mokka, während Ras Isa die Ölexporte des Landes abwickelt. Der Hafen von Aden verfügt über einen 1999 eröffneten Containerterminal, sah sich aber nach dem Bombenanschlag auf den französischen Tanker Limburg im Oktober 2002 einem drastischen Rückgang des Umschlages gegenüber. Die umgeschlagene Menge hat sich jedoch seitdem erholt und betrug 2007 503.325 TEU. Es gibt im Jemen keine Binnenwasserstraßen.

Vier jemenitische Städte verfügen über internationale Flughäfen, nämlich Aden, Sanaa, Taizz und al-Hudaida. Flugverbindungen existieren vor allem zu anderen Staaten in der Region und einigen Zielen in Europa. Die nationale Fluglinie heißt Yemenia.

Der Jemen verfügt über keine schienengebundenen Transportmittel. In osmanischer Zeit wurde 1911 mit dem Bau einer meterspurigen Bahnstrecke vom Hafen Ra’s Kathib (nördlich von al-Hudaida) nach Sanaa begonnen. Nach einem italienischen Bombardement des Hafens wurden die Arbeiten abgebrochen, so dass nur 7 km Gleis verlegt waren. Anschließende Konflikte zwischen den osmanischen Beamten und lokalen Machthabern verhinderten, dass die Arbeiten wieder aufgenommen wurden. Noch in den 1980er Jahren waren Spuren des Vorhabens zu sehen, einschließlich der Reste einer Dampflokomotive.

Es gibt prinzipiell Einverständnis, den Jemen an das geplante Bahnnetz des Golf-Kooperationsrat anzubinden.

Die Versorgung mit elektrischer Energie kann mit den Anforderungen nicht Schritt halten. Weniger als die Hälfte der Bevölkerung des Landes und weniger als ein Viertel der Landbevölkerung sind an das Stromnetz angeschlossen, die Versorgung ist instabil und zwingt Wirtschaftstreibende, teure Alternativen zum öffentlichen Netz zu installieren oder Produktivitätseinbußen in Kauf zu nehmen. 2005 stammte die gesamte Stromproduktion von 4,46 Milliarden kWh aus thermischer Gewinnung. Die Entwicklung erneuerbarer Energiequellen ist vorgesehen.

Jemen besitzt eigene Erdöl- und Erdgasvorkommen, die jedoch nicht mit dem Rohstoffreichtum der Nachbarländer zu vergleichen sind und deren Erträge zudem zurückgehen. Nach einer von der Regierung beschlossenen Erhöhung der Treibstoffpreise auf fast das Doppelte brachen am 20. Juli 2005 in sechs Provinzen Unruhen aus, bei denen mindestens 39 Personen, darunter zwölf Angehörige der Sicherheitskräfte, ums Leben kamen. In Aden kam es trotz starker Militärpräsenz zu Plünderungen.

Die Treibstoffpreise waren mittels staatlicher Subventionierung bisher niedrig gehalten worden. Durch die zuvor stark gestiegene Nachfrage – sie war von der Regierung auf ein florierendes Schmuggelgeschäft mit den Nachbarländern zurückgeführt worden, da dort die Preise deutlich höher liegen –, war der im Budget veranschlagte Rahmen für Subventionen bereits in den ersten Monaten des Jahres überschritten worden. Die Reduzierung der Subventionen war auch Teil eines mit dem Internationalen Währungsfonds (IWF) ausgehandelten Reformprogramms. Ein Teil der eingesparten Gelder sollte für die Erhöhung der Löhne der Staatsbediensteten und die Anpassung der Renten verwendet werden. Nachdem Präsident Salih eine teilweise Rücknahme der Preiserhöhung zugesagt hatte, beruhigte sich die Lage wieder.




</doc>
<doc id="8396" url="https://de.wikipedia.org/wiki?curid=8396" title="Thukydides">
Thukydides

Thukydides (; * vor 454 v. Chr.; † wohl zwischen 399 v. Chr. und 396 v. Chr.) war ein aus gut situierten Verhältnissen stammender Athener Stratege und herausragender antiker griechischer Historiker. Für Thukydides’ Auffassung der geschichtlichen Wirkkräfte bedeutsam sind insbesondere seine Annahmen über die Natur des Menschen und die Motive menschlichen Handelns, die auch die politischen Verhältnisse grundlegend beeinflussen.

Sein bis heute Maßstäbe setzendes Werk "Der Peloponnesische Krieg" (ein Originaltitel ist nicht überliefert) hinterließ er zwar unvollendet, doch begründete er in methodischer Hinsicht erst damit eine dem Geist neutraler Wahrheitssuche durchgängig verpflichtete Geschichtsschreibung, die einem objektiv-wissenschaftlichen Anspruch genügen will. Uneins ist die heutige Thukydides-Forschung darüber, in welchem Umfang er diesem Anspruch bei der Abfassung seines Werkes gerecht geworden ist. Teilweise in Zweifel gezogen wird speziell seine Darstellung der Rolle des Perikles bei der Entstehung des Peloponnesischen Krieges.

Thukydides selbst sah den Sinn seiner Aufzeichnungen darin, der Nachwelt „ein Besitztum für immer“ zu hinterlassen. Als markantestes Beispiel für das Gelingen dieses Vorhabens erweist sich die Unterscheidung von diversen kurzfristigen Anlässen des Peloponnesischen Krieges und seinen in der damaligen griechischen Großmächte-Rivalität zwischen der Seemacht Athen und der Landmacht Sparta begründeten langfristigen Ursachen. Von eigener zeitloser Bedeutung ist auch der machtpolitisch exemplarische Melierdialog.

Eine auch nur in den Grundzügen annähernd vollständige Lebensbeschreibung des Thukydides ist wegen Quellenmangels nicht möglich. Das Wenige, was als gesichert gelten kann, beruht auf Eigenbezeugungen von Thukydides, die er an vier Stellen seines Werkes über den Peloponnesischen Krieg ohne autobiographische Absicht hat einfließen lassen. Einzelne Hinweise finden sich bei Plutarch. Eine erste überlieferte Auseinandersetzung mit seiner Lebensgeschichte datiert ca. ein Jahrtausend später; weitere obskure Kurzviten standen seiner Epoche noch ferner. Eklatante Lücken und verbleibende Ungewissheiten sind folglich wesentliche Merkmale des folgenden Überblicks.

Für das Geburtsjahr des Thukydides lässt sich nur sagen, dass es spätestens 454 v. Chr. gewesen sein kann, weil er mindestens 30 Jahre alt sein musste, um das Strategenamt bekleiden zu können, das er 424 innehatte. Das attische Bürgerrecht besaß er wie sein Vater auf Grund seiner Zugehörigkeit zum Demos Halimus der Phyle Leontis an der Westküste Attikas. Väterlicherseits gab es eine thrakische Abstammungslinie, denn der Vater trug den thrakischen Namen Oloros und vererbte dem Sohn Besitzungen in Thrakien sowie die Nutzung der dortigen Goldbergwerke. Thukydides verfügte demnach über beträchtliches Vermögen und konnte sich daher schließlich ganz seinen historischen Studien widmen.

Die verwandtschaftlichen Beziehungen nach Thrakien legen noch in anderer Hinsicht die Zugehörigkeit des Thukydides zu herausgehobenen Kreisen der attischen Gesellschaft nahe. Oloros hieß auch jener thrakische König, dessen Tochter Hegesipyle den bei Marathon siegreichen Feldherrn Miltiades heiratete und deren politisch in Athen lange Zeit höchst einflussreicher Sohn Kimon nach Plutarch mit Thukydides verwandt war. Das Interesse für Staatsangelegenheiten, Machtfragen und Militäroperationen, das Thukydides’ Darstellung des Peloponnesischen Krieges kennzeichnet, könnte ihm also schon von Hause aus zugewachsen sein. Sein spätantiker Biograph Markellinos sieht in ihm einen Schüler des Philosophen Anaxagoras und des Sophisten Antiphon; vermutlich habe er auch Vorträge Herodots gehört.

Bereits unmittelbar bei Ausbruch des Peloponnesischen Krieges, so betont Thukydides gleich eingangs seines Werkes, sei ihm die beispiellose Bedeutung dieser kriegerischen Auseinandersetzung der griechischen Großmächte bewusst gewesen, und so habe er sofort mit Aufzeichnungen des Geschehens begonnen. Ein weiteres Mal erwähnt Thukydides sich selbst im Zusammenhang mit der Schilderung der Attischen Seuche, die unter den in ihren Mauern von den Spartanern eingeschlossenen Athenern 430 v. Chr. ausbrach und verheerend um sich griff; an ihr erkrankte auch Thukydides. Seine anschauliche und sachverständige Darstellung der Krankheit ist heute eine wichtige Quelle für Medizinhistoriker. Bemerkenswert ist nicht nur Thukydides’ kenntnisreiche Beschreibung der Seuche, sondern auch sein Wissen um die gewonnene Immunität der Überlebenden gegen eine spätere Wiederansteckung. Um welche Krankheit es sich handelte, ist allerdings umstritten. Über 200 Veröffentlichungen zum Thema bringen zumindest 29 Möglichkeiten (vom Ebola-Virus bis zum Typhus abdominalis) ins Spiel. Thukydides’ genaue Schilderung des oft als Pest gedeuteten Geschehens entfaltete beträchtliche Nachwirkungen, in der Antike zum Beispiel in "De rerum natura" bei Lukrez, im 20. Jahrhundert bei Camus in seinem Roman "Die Pest".

Für das Jahr 424 v. Chr. wurde Thukydides in das Zehnerkollegium der Strategen gewählt, in eine militärische Führungsposition also, die zugleich als letztes politisch bedeutendes Wahlamt der Attischen Demokratie fungierte. Die zehn Kollegen übten das Amt unter Aufgabenteilung parallel aus. Thukydides sah sich vor die Aufgabe gestellt, das thrakische Amphipolis vor der Übernahme durch den spartanischen Feldherrn Brasidas zu schützen, der um die Stadt einen Belagerungsring errichtet hatte und die Übergabe erzwingen wollte. Die Bürgerschaft von Amphipolis tendierte unterschiedlich; aber zunächst waren die zur Verteidigung Entschlossenen noch in der Überzahl, sodass Thukydides, der eine halbe Tagesreise entfernt auf Thasos stationiert war, mit sieben Trieren zu Hilfe eilte.

Brasidas habe, so Thukydides, im Wissen um den Einfluss des anrückenden Gegners in Thrakien, seine Bemühungen um die Einnahme von Amphipolis verstärkt und den Bewohnern der Stadt so attraktive Bleibe- oder wahlweise Wegzugskonditionen zugesichert, dass sie ihm die Stadt tatsächlich übergaben, bevor Thukydides am Abend eintraf. Dem blieb bei seinem Ankommen nur noch die Sicherung der benachbarten Siedlung Eion am Strymon, die nach seiner Einschätzung andernfalls am nächsten Morgen ebenfalls an Brasidas gefallen wäre. Gleichwohl lasteten die Athener den Verlust von Amphipolis, des wichtigen Stützpunkts in der Nord-Ägäis, ihrem Strategen Thukydides als schuldhaftes Versagen an und fassten einen Beschluss zu seiner Verbannung. Unsicher ist, ob er die Verurteilung überhaupt abwartete oder ob er ihr durch freiwilliges Fernbleiben von Athen bereits zuvorkam.

Der Historiker schildert dieses Geschehen, aus dem zwei Jahrzehnte erzwungenen Fernbleibens von Athen für ihn folgten, ebenso nüchtern und scheinbar unbeteiligt wie die übrigen Begebenheiten des Peloponnesischen Krieges, ganz so, als habe der Chronist Thukydides mit dem Strategen Thukydides nichts zu tun. Seinem spartanischen Kriegskontrahenten Brasidas aber zollte Thukydides – wie sonst nur ganz wenigen – höchstes Lob für das, was er für Sparta leistete: „Denn damals gleich bewog er durch sein gerechtes und maßvolles Auftreten in den Städten die meisten zum Abfall [von Athen] […] und für den nachmaligen Krieg nach den sizilischen Ereignissen machte nichts so wie Brasidas’ edle Haltung und Einsicht von damals, die die einen aus Erfahrung kannten, die andern dem Gerücht glaubten, die Verbündeten Athens begierig auf Sparta.“

Über die mit der Verbannung verbundene grundlegende Wendung im eigenen Leben berichtet Thukydides im Zuge seiner chronologisch angelegten Darstellung der Kriegsereignisse zunächst aber gar nicht. Er bringt sie erst mit großer zeitlicher Verzögerung zur Sprache, neun Jahre nach dem Fall von Amphipolis und seinem Weggang aus Athen, als er die Wiederaufnahme offener Feindseligkeiten, die den Nikiasfrieden ablösten, mit einer Überleitung zu seiner Schilderung des Kriegsfortgangs verbindet. Dabei fehlt auch jeder Hinweis auf die konkreten Umstände seiner Abberufung als Stratege und auf die der Verbannung zugrunde liegende Anklage, Verhandlung und Entscheidung:
Es ist möglich, dass Kleon, den Thukydides sehr negativ schildert, an der Verbannung maßgeblich beteiligt war. Darüber, wo und wie Thukydides die 20 Jahre in Verbannung verbracht hat, gibt es keine gesicherten Erkenntnisse. Angenommen wird, dass er die meiste Zeit auf seinen thrakischen Besitzungen verbrachte. Den zitierten Hinweis in seinem Geschichtswerk, er habe infolge der Verbannung Näheres zu beiden Kriegsparteien erforschen können, hat man teilweise so verstanden, dass er reisend viele Vor-Ort-Recherchen durchgeführt habe. Dafür sprächen etwa seine eingehenden Kenntnisse der politischen Verhältnisse in Korinth. Wegen seiner detaillierten Schilderung der Umstände des Ausschlusses der Spartaner von den Olympischen Spielen 420 v. Chr. wird teils auch seine persönliche Anwesenheit in Olympia zu dieser Zeit für wahrscheinlich gehalten. Ebenso möglich ist aber, dass ihm jeweils Informanten für die einzelnen Begebenheiten zur Verfügung standen.

Dass die Verbannung des Thukydides mit dem Ausgang des Peloponnesischen Krieges endete, wird nicht nur von ihm selbst bezeugt, sondern auch von Pausanias, der einen die Rückkehrerlaubnis für Thukydides enthaltenden Volksversammlungsbeschluss erwähnt. Wiederum unklar ist, wie viel Zeit dem Historiker danach für die Arbeit an seinem Werk noch blieb, das mitten im Satz unvollendet abbricht. Allerdings kann man darin Hinweise finden, bis wann er noch gelebt hat. Seine Beschreibung des makedonischen Königs Archelaos klingt wie ein Nachruf. Da dieser 399 v. Chr. starb, kann man annehmen, dass Thukydides zu diesem Zeitpunkt noch lebte. Sollte eine auf das Jahr 397 v. Chr. datierte, in Thasos aufgefundene Inschrift, die einen "Lichas" als Lebenden benennt, denselben "Lichas" betreffen, von dessen Tod Thukydides berichtet, so schrieb der Historiker zumindest 397 v. Chr. noch an seinem Werk.

Ungeklärt sind bei Thukydides auch die Todesumstände, was zu allerlei Legendenbildung in späterer Zeit geführt hat. Unterschiedliche Versionen einer Ermordung des Thukydides kursierten und wurden möglicherweise von dem abrupten Ende seiner Niederschrift inspiriert. Sein Grabdenkmal befand sich nach übereinstimmender Auskunft von Pausanias und Plutarch beim Familiengrab der Familie Kimons im Demos Koile.

Nicht nur als einzigartige Quelle für die Geschehensabläufe des innergriechischen Machtkampfes zwischen 431 und 411 v. Chr. ist Thukydides’ Darstellung bedeutsam. Sie ist, wie Bleckmann hervorhebt, auch der maßgebliche Grund dafür, gerade diesen Zeitraum als eine eigenständige Epoche der griechischen Geschichte anzusehen. Das sei, wie jede geschichtliche Epocheneinteilung überhaupt, das Ergebnis einer von bewusster historischer Analyse ausgehenden gedanklichen Entscheidung: „Daß das Gesamtgeschehen zwischen 431 und 404 als Einheit, als ein einziger Krieg zu betrachten war, war jedenfalls vielen Zeitgenossen gar nicht bewusst und ist eine (durchaus begründete) Sicht der Dinge, die erst dem Thukydides und später der griechischen Geschichtsdeutung des vierten Jahrhunderts zu verdanken ist.“

Die Nüchternheit der Darstellung und die Demonstration überlegener Einsichtsfähigkeit weisen nach Bleckmann auf ein Bemühen um aufklärendes politisches Wirken bei Thukydides hin; denn eine solche Fähigkeit zeichne auch den guten Politiker aus. Auch Landmann betont die politische Dimension des Werkes. Erst vom Geist durchleuchtet könne Geschichte – „der täglich wachsende Haufe stummer dummer Fakten“ – die Gegenwart erhellen. Thukydides gehe es darum, durch fruchtbares Wissen zum richtigen Handeln zu führen, und zwar nicht durch bestimmte situationsbezogene Anweisungen, sondern durch die Schulung des Denkens in der Verknüpfung von Ursachen und Wirkungen, sodass die passende Orientierung für das eigene aktuelle Handeln schließlich selbst gefunden werden kann.

Aus anderer Sicht geht es Thukydides wesentlich darum, Geschichte als einen irreversiblen Prozess auszuweisen, in dem es gilt, die Gunst der historischen Stunde zu nutzen – von Seiten Athens etwa das spartanische Friedensangebot von 425 v. Chr. –, weil ausgeschlagene Chancen unter den im Fortgang des Geschehens veränderten Bedingungen nicht wiederkehren. Nicht zuletzt aber sind es die Motive, die menschlichem Handeln zugrunde liegen, die Thukydides vorrangig beschäftigen. Sie erklären nach Will nicht nur das Verhalten wichtiger Einzelpersönlichkeiten, sondern auch das von Städten und Staaten.
Zu den Thukydides besonders wichtigen Darstellungsaspekten zählt Bleckmann die zunehmende Verrohung der Akteure im Kriegsgeschehen:

Auch wenn in der Forschung mit Recht davor gewarnt wird, die thukydideische Arbeitsweise mit dem ganz anderen Ansatz und Anspruch moderner Geschichtswissenschaftler zu verwechseln, war sein Einfluss enorm. Ganz deutlich erhebt Thukydides den Anspruch, eine neue, zukunftsdienliche Form der Geschichtsschreibung zu betreiben. Er betont die Mühen, die es ihn gekostet hat, die Vorgeschichte des Peloponnesischen Krieges zu rekonstruieren, weil er, anders als seine Mitmenschen, Berichte und Aussagen über Vergangenes nicht ungeprüft übernehme. Während andere mehr auf eine effektvolle Darbietung zielten, komme für ihn alles auf die Wahrheit an:
Eigene Beobachtungen und die Augenzeugenberichte anderer dienten Thukydides demnach in bewusst kritischer Auseinandersetzung mit möglichen Fehlerquellen dazu, den Tatsachen auf den Grund zu gehen. Nicht nur in Bezug auf Attika, sondern auch bei einer ganzen Reihe anderer Schauplätze des Kriegsgeschehens spricht etwa die genaue Schilderung topographischer Gegebenheiten dafür, dass Thukydides sich selbst vor Ort informiert haben könnte. Mit nachdrücklicher Begründung also fordert er dazu auf, seiner von Ausschmückungen freien, streng der Wahrheit verpflichteten Darstellung zu folgen und nicht einfach an den herkömmlichen Sichtweisen festzuhalten:
Auf reine Tatsachenermittlung und -darstellung ist das Werk demnach nicht angelegt. Thukydides zielte auf eine tiefer gegründete Wahrheit als die aus dem politischen Tagesgeschäft mit seinen Ereignisfolgen sich ergebende. Besonders deutlich wird dies nach inzwischen klassischer Lesart bei der Behandlung von Entstehungsgründen für den Peloponnesischen Krieg, die Thukydides den Hinweisen auf seine methodische Sorgfalt unmittelbar anschließt. Er spricht das Ende des zwischen Athen und Sparta ein Jahrzehnt zuvor vereinbarten Friedens an und weist hin auf die aktuellen Streitfälle und örtlichen Verwicklungen, die von den Beteiligten als Kriegsgründe angeführt und von den Zeitgenossen als solche wahrgenommen wurden, stellt aber ergänzend heraus:
Nicht die in den wechselseitigen Vorhaltungen der beteiligten Mächte thematisierten, propagandistisch griffigen Anlässe und Streitgründe (αἰτίαι καὶ διαφοράι) sind für Thukydides, der hier ausnahmsweise in der Ich-Form urteilt, also hauptsächlich ausschlaggebend für die Entscheidung zum Krieg, sondern als wahrhaftigstes Motiv (ἀληθεστάτη πρόφασις) die kaum eingestandene Furcht der Spartaner vor der wachsenden Macht Athens.

Aus den von Thukydides selbst gesetzten inhaltlichen Akzenten und Kompositionsmerkmalen ergeben sich hauptsächlich fünf zu unterscheidende Werkteile. Die erst in hellenistischer Zeit vorgenommene Einteilung in acht Bücher, die als Grundlage für sämtliche Stellenangaben dient, entspricht dem nur teilweise.

Im einführenden Teil, der mit Buch I identisch ist, formuliert und erläutert Thukydides nicht nur sein Darstellungsmotiv, dass der Krieg zwischen den Großmächten Athen und Sparta der für alle Griechen bisher größte und bedeutendste überhaupt sei (1. 1–19), sondern verweist auch auf die eigenen methodischen Vorkehrungen (1. 22) und entwickelt den Unterschied zwischen kriegsauslösenden aktuellen Verwicklungen und der tiefer liegenden Kriegsursache, indem er die Anlässe ausführlich referiert (1. 23–88) und das wachsende Spannungsverhältnis zwischen Sparta und Athen im Zeitraum der vorangegangenen 50 Jahre beleuchtet (1. 89–118). Dieser erste Teil schließt mit den unmittelbaren Kriegsvorbereitungen und Rechtfertigungsreden beider Seiten (1. 119–146).

Im zweiten Teil des Werkes schildert Thukydides den Verlauf des 431 v. Chr. begonnenen Archidamischen Krieges (2. 1 – 5. 24) bis zum vereinbarten 50-jährigen Frieden zwischen Athen und Sparta 421 v. Chr. Als chronologisches Ordnungsprinzip dienen ihm dabei jeweils die einzelnen Jahre, bei denen er nochmals regelmäßig nach Ereignissen des Sommer- und des Winterhalbjahres unterscheidet – eine Neuerung für die Griechen, die eine einheitliche Jahreszählung noch nicht kannten.

Den dritten, von Thukydides selbst mit sechs Jahren und zehn Monaten zeitlich genau umrissenen Werkteil (5. 25–116) bildet jene „argwöhnische Waffenruhe“, die infolge des Nikiasfriedens zustande kam und die wegen nicht eingehaltener Abreden und wechselseitiger Übervorteilungsversuche von Spartanern und Athenern keine nachhaltige Beendigung des Krieges bedeutete. Thukydides schließt diesen Teil mit einer Schilderung der brutalen Unterwerfung von Melos 415 v. Chr. Im Mittelpunkt dieses aus Athener Sicht erfolgreichen Gewaltstreichs steht der berühmte Dialog zwischen Meliern und Athenern (5. 85–113), ein im Gesamtwerk einmaliges Beispiel von schneller Wechselrede, in der das Spannungsverhältnis zwischen Macht und Recht drastisch zur Sprache gebracht wird. Für Will steht diese markante Episode im Zentrum des Werkes: „Hätte Thukydides seine Geschichte des Krieges bis 404 führen können, hätte Melos den Angelpunkt gebildet.“

Auch zum unmittelbar folgenden vierten Teil des Werkes, der den Versuch der Athener beschreibt, durch eine Flotten-Großexpedition 415–413 v. Chr. die Herrschaft über Sizilien zu erlangen (Buch VI und VII), werden die Vorgänge um Melos in der Thukydides-Forschung in engem Bezug gesehen, sei es als Auftakt und Ansporn für das weit größere Folgeunternehmen, sei es als Vorzeichen wachsender Hybris in Athen, die dem katastrophalen Ausgang der sizilischen Expedition mit der entscheidenden Niederlage der athenischen Flotte und Hoplitenstreitmacht bei Syrakus Vorschub geleistet hat.

Der unvollendete fünfte Werkteil behandelt den dekeleisch-ionischen Krieg in den Jahren 413–411 v. Chr., den Sturz der Demokratie in Athen durch das oligarchische Regime der 400 sowie dessen Ablösung durch die Verfassung der 5000 (Buch VIII). Bald danach bricht die Darstellung abrupt ab.

Mit seinen zeitlich unmittelbar anschließenden "Hellenika" setzte zwar u. a. der Historiker Xenophon die Darstellung des Thukydides bis zum Ende des Peloponnesischen Krieges und darüber hinaus fort (und begründete damit in Form der "historia perpetua" eine antike historiographische Tradition). Die bei Thukydides anzutreffende Genauigkeit und Dichte der Darstellung wurde jedoch in der Nachfolge nicht erreicht.

Bedenkt man, dass die Geschichtsschreibung in der griechischen und römischen Antike allgemein den Künsten zugeordnet wurde, so setzte Thukydides sich mit seiner zumeist nüchternen Darstellungsweise davon deutlich ab:
Verdichtung und prägnante Kürze kennzeichnen seinen Stil, für den der häufige Gebrauch von substantivierten Infinitiven, Partizipien und Adjektiven bezeichnend ist. Der Rhetoriklehrer Dionysios von Halikarnassos kritisierte ihn dafür als undeutlich, übertrieben kurz, komplex, streng, hart und dunkel. Scardino meint, dieser Stil rege die vom Leser geforderte aktive geistige Mitarbeit an. Landmann findet die Satzperioden oft schwer und ungelenk: „Kein Wort steht um des Wortes willen, immer steht ein Gedanke dahinter, der, neu gedacht, sich neuen Ausdruck schafft, knapp, geschliffen, stichhaltig.“

Eine spannende Lektüre ist das Werk nach Sonnabend über weite Strecken nicht, in denen militärische Aktionen in aller Ausführlichkeit abgehandelt werden oder Notate zur Ereignisgeschichte ohne Erschließungshilfen zu deren historischer Bedeutung zu verarbeiten sind. Doch seien auch diese Passagen Bestandteil eines historischen Konzepts, bei dem Sorgfalt und Akribie dominierten. Insbesondere aber werde der Leser durch jene Werkteile entschädigt, „die ohne Frage zu den Klassikern der Geschichtsschreibung gehören“ und die Thukydides’ historisch-literarische Fähigkeit eindrucksvoll unterstrichen.

Neben fesselnden Schilderungen wie dem Ausbruch und den Verheerungen der attischen Seuche unter den belagerten Athenern (Thuk. 2. 47–54) und dem erst beschlossenen und dann doch abgewendeten Untergang Mytilenes (3. 35–50) gehören dazu vor allem die Reden, in denen die politischen Akteure ihre jeweiligen Auffassungen vortragen. Sie machen insgesamt etwa ein Viertel des Gesamtwerks aus. Die Gestaltung der Reden ist sowohl von der sophistischen Rhetorik als auch von der Tragödiendichtung beeinflusst. Rede und Gegenrede (die "dissoi logoi") als Darstellungsmittel entsprechen einem damals verbreiteten Muster. Häufig vertreten sind die Reden speziell im ersten Buch, wo es um die Entscheidung zwischen Krieg und Frieden geht, und auch sonst vor allem dann, wenn die Motive für wichtige Entschlüsse zu verdeutlichen sind. Thukydides erläutert auch für dieses Darstellungsmittel sein methodisches Vorgehen:
Eine wortgetreue Wiedergabe des Redetextes beansprucht Thukydides also nicht; es handelt sich um Schöpfungen des Autors, die aber in einem tieferen Sinn als historisch getreu angesehen werden können, da sie auf die jeweilige geschichtliche Situation (περὶ τῶν αἰεὶ παρόντων), auf die von ihr an den Redner gestellten Forderungen (τὰ δέοντα) und auf die politische Gesamthaltung des Sprechers (τῆς ξυμπάσης γνώμης) zielen. Thukydides hat sich dabei typischer Elemente einer echten Rede bedient und sie unter anderem mit Wortspielen und rhetorischen Tricks angereichert. Das versetzt den Leser in die Situation eines Hörers, der sich anhand des tatsächlichen Geschehensablaufs ein eigenes Urteil über die von den Parteien vorgetragenen verschiedenen Standpunkte zu bilden hat. Durch Konfrontation mit der jeweiligen rhetorischen Strategie und Argumentationswirkung vermittelt sich dem Leser nach Hagmaier „ein lebendigeres und tiefer gehendes Bild, als es eine analytische Darstellung zutage fördern könnte.“

Die Einheit des thukydideischen Werkes wird durch Über- und Einleitungsformeln sowie durch die sinnvolle Verknüpfung von Rückblenden und Vorgriffen auch jenseits der vorherrschend chronologischen Darstellungsweise unterstützt. Die Auswahl und Anordnung der Fakten sowie das logisch aufeinander bezogene Zusammenspiel von Reden und Erzählung tragen ebenfalls dazu bei.

Die Nichtvollendung des Werkes durch Thukydides und die uneinheitliche Gestaltung verschiedener Werkteile durch den Historiker geben der Thukydides-Forschung bis heute Rätsel auf und regen sie an zu Fragen und Deutungen. Anhaltend erörtert werden u. a. die Entstehungsgeschichte des von einem unbekannten Herausgeber publizierten Werkes, die von Thukydides damit und darin verfolgten Absichten sowie seine persönliche Ausrichtung in gesellschafts- und verfassungspolitischer Hinsicht.

Eine ungemein anregende neue Sicht auf das Werk des Thukydides entwickelte ab 1845 der Philologe Franz Wolfgang Ullrich, dem aufgefallen war, dass Thukydides nicht schon in seiner umfänglichen Einleitung vor der Schilderung des Archidamischen Krieges auf die 27-jährige Gesamtdauer der Auseinandersetzung zwischen Sparta und Athen hinweist, sondern dies erst angesichts des gescheiterten Nikias-Friedens im Rahmen eines zweiten Vorworts tut. Für Ullrich ergab sich in Verbindung mit weiteren Ableitungen der Schluss, dass Thukydides zunächst nur den Archidamischen Krieg habe darstellen wollen, dann aber durch das Wiederaufleben der Kampfhandlungen im Zuge der sizilischen Expedition zu einem Neuansatz veranlasst worden sei, den er nach der Niederlage Athens 404 v. Chr. ins Werk setzte. Indem Ullrich eine Ineinanderschichtung und Überlappung ursprünglicher Darstellungsteile mit Elementen einer Neuinterpretation des Gesamtgeschehens durch Thukydides nachzuweisen suchte, begründete er den Interpretationszweig der „Analytiker“.

Während diese in ihrer Werkexegese auf Textstellen verweisen, die für unterschiedliche Abfassungszeiträume stehen und einen Auffassungswandel des Thukydides markieren sollen, geht es für den Interpretationszweig der Unitarier um den Nachweis, dass Thukydides sein Werk in einem Zuge nach 404 v. Chr. umgesetzt habe. „Es ist leicht einzusehen“, schreibt Will, „daß eine Vermittlung zwischen den teilweise diametral entgegengesetzten Standpunkten kaum möglich war; eine ‚unitarische’ Interpretation zeitigte eine ‚analytische’ Reaktion und umgekehrt.“

Zu konkreten Gegenständen der Auseinandersetzung werden dabei insbesondere die von den Analytikern angeführten Hinweise einerseits auf „Frühindizien“ und andererseits auf „Spätindizien“ im Werk des Thukydides, die der Zuordnung zu einer frühen oder späten Abfassungszeit des jeweiligen Darstellungsabschnitts dienen sollen. So werden z. B. Thukydides’ Behauptung und Erläuterung der ganz neuen Dimensionen dieses Krieges ebenso wie seine methodischen Akzente hauptsächlich einer Frühphase des Werkes zugeordnet in der Annahme, zu jenem Zeitpunkt habe Thukydides sich vor allem gegenüber dem gerade besonders populären Herodot abgrenzen und behaupten wollen. Dies habe aber nach 404 v. Chr. keine Rolle mehr gespielt: „Thukydides schrieb jetzt für die Generation des verlorenen Krieges, eine Leserschaft“, meint Will, „der unter dem frischen Eindruck der spartanischen Gewaltherrschaft der Ruhm der Vorfahren gleichgültig war und die stattdessen zu wissen begehrte, wer diesen Krieg, dessen Anfänge die wenigsten noch bewußt erlebt hatten, um welcher Ziele willen geführt und wer schließlich auch die Katastrophe zu verantworten hatte.“

Erst in Kenntnis der endgültigen Niederlage Athens oder zumindest im Bewusstsein von deren Unvermeidlichkeit sei Thukydides, der nun auch Sparta gegenüber eine negativere Haltung entwickelt habe, die Einsicht in die für ihn wahre Ursache des Krieges gekommen: in den unversöhnlichen Dualismus der beiden griechischen Großmächte nämlich, aus dem sich zwangsläufig der Krieg bis zur Vernichtung einer Seite ergeben habe. „Diese Überzeugung“, so Will, „steht nicht am Anfang, sondern am Ende seiner Beschäftigung mit der Materie.“ Erst mit dieser späten Erkenntnis sei auch die auf die Herausarbeitung der zunehmenden Rivalität beider Großmächte gerichtete Darstellung der Pentekontaetie sinnvoll und nötig geworden, weshalb u. a. diese beiden Werkkomponenten eindeutig den Spätindizien zuzuordnen seien.

Nicht einverstanden mit einer solchen Theorie der Komplementärversatzstücke im ersten Buch des Werkes ist beispielsweise Hagmaier, der es vielmehr als eine in sich geschlossene Einheit ansieht, „die kaum das Resultat nachträglicher Erläuterungen, Einschübe oder Zusätze sein kann.“ Eine skeptisch-vermittelnde Haltung in der Auseinandersetzung zwischen Analytikern und Unitariern nimmt etwa Scardino ein, indem er resümiert:

Aus der analytischen Sicht Wills war die von Thukydides schließlich entdeckte phasendifferenzierte Ganzheitlichkeit des Peloponnesischen Krieges die Richtschnur für die „Redaktion letzter Hand“, die speziell dem Einführungsteil und der Zeit bis zu Perikles’ Tod gewidmet gewesen sei. Thukydides sei es in seinem Werk überhaupt wesentlich um das von Perikles zu entwerfende Bild gegangen. Die Darstellung der zahlreichen übrigen Kriegsjahre erscheine geradezu als Fußnote zu der abschließenden Würdigung des Perikles (2. 65).
Als Ergebnis dieser Darstellung werde aber nicht der Politiker gezeigt, der Athen in den Krieg führte, sondern ein Wunschbild, der Stratege nämlich, der aufgrund seines überlegenen Kriegsplans die Auseinandersetzung mit Sparta schließlich siegreich gestaltet hätte. „Was zunächst als Apologie des Helden geplant war, endet in einer Art Apotheose“, schreibt Will im Vorwort zu seinem Werk "Thukydides und Perikles. Der Historiker und sein Held". Folgt man ihm, so genügt Thukydides seinen eigenen methodischen Vorgaben und Ansprüchen nicht. Im Vergleich mit anderen von Thukydides breit ausgeführten Vorkriegsstreitgegenständen wird die von Perikles veranlasste und von ihm auch gegen Drohungen von außen verteidigte Handelsblockade gegen Megara (das Megarische Psephisma) gezielt marginalisiert, meint Will.

Nicht einmal ein „Anschein von Historizität“ findet sich für Will in Thukydides’ Wiedergabe einer Perikles-Rede zu Kriegsanfang, wo er seinen Mitbürgern die Einsicht zumutet, dass Athens rigide Herrschaftsausübung im Attischen Seebund auf Unrecht beruhen könnte (2. 63). „Die Anfangsphase des Krieges, in der Euripides in seinen Tragödien Athen als Hort der Freiheit feierte, war nicht die Situation, in der Athen sich solchen Unrechts zieh, die Pnyx nicht der Platz, an dem die Anklage formuliert wurde.“

Bei verschiedenen Gelegenheiten bezweifelt Will Thukydides’ erklärtes Vorhaben, den Redengehalt sinngemäß korrekt wiederzugeben: „Von der zunächst unerwarteten Fortsetzung des Krieges und der schließlich erst sehr spät absehbaren Niederlage Athens vor neue Darstellungs- und Deutungsprobleme gestellt, gestaltete Thukydides seine Reden in einer Weise, die den eingangs aufgestellten Richtlinien nicht mehr voll gerecht wurde; […] Thukydides fingierte wohl nicht nur Reden wie den Logos der Athener im ersten Buch, sondern auch Anlässe und vielleicht sogar die Person des Redners.“ Der berühmte Epitaphios (Rede auf die Gefallenen, Thukydides 2. 35–46) spiegele weit mehr die Gedanken des Historikers Thukydides als die Worte des Staatsmannes Perikles. „In dreißig Jahren verwandelten sich Perikleische Gedanken in Thukydideische, Thukydideische Ansichten gerannen zu Perikleischen.“ In der Summe ergibt sich für Will: „Perikles ist das Selbstportrait des Historikers als Staatsmann.“

Die Identifikationsbereitschaft des Thukydides mit Perikles sieht Will wesentlich gefördert durch die thrakischen Besitzungen des Historikers, für die sich im Zuge der von Perikles gestützten imperialen Politik Athens eine verbesserte Anbindung und bessere Nutzungsmöglichkeiten eröffneten. Dadurch sei der Kimon-Verwandte, von Hause aus also ein Perikles-Gegner, zu dessen Anhänger und zum Kriegsbefürworter geworden – „in der Rolle des politischen Konvertiten mit all den damit verbundenen psychologischen Implikationen.“

Demgegenüber hält Bleckmann den Deutungsansatz des Thukydides und die von ihm für Perikles bezeugte Haltung bei der Entstehung des Peloponnesischen Krieges für durchaus nachvollziehbar: „Die ultimativen Forderungen Spartas gipfelten in der Forderung, den Bündnern Athens die Autonomie zurückzugeben und damit einen großen Teil der organisatorischen Entwicklung des Bundes in Frage zu stellen. Diese Forderungen standen am Ende einer Reihe von Versuchen Spartas und seiner Verbündeten, den Attischen Seebund auseinanderzusprengen.“ Athens Versorgung, Wohlstand und Demokratie aber seien zu dieser Zeit bereits viel zu eng mit dem Instrument des Attischen Seebunds verbunden gewesen, als dass die Athener solchen Forderungen ohne weiteres hätten nachgeben können: „Der Kriegseintritt barg große Risiken, aber eine Vermeidung des Kriegseintritts konnte die Integrität der Herrschaft nicht sichern.“ Da Thukydides als Angehöriger der aristokratischen Elite Athens Perikles persönlich gekannt habe und über Erwägungen zum Kriegseintritt aus erster Hand informiert gewesen sei, plädiert Bleckmann dafür, sich dem Urteil des Thukydides hinsichtlich der Motive des Perikles für den Kriegseintritt anzuschließen.

Eindimensionale Positionierung in der politischen Auseinandersetzung und offene politische Parteinahme lässt der Historiker Thukydides in seinem Werk kaum erkennen. Auf den Vorgang der Berufung in das Amt des Strategen sowie auf die in dieser damals wichtigsten staatspolitischen Funktion gemachten persönlichen Erfahrungen geht Thukydides geradezu ostentativ überhaupt nicht ein und vermittelt auf diese Weise, dass er auf anderes zielt als auf die Verallgemeinerung von individuellen Erfahrungen. Nach Hartmut Leppin lässt auch sein aristokratisches Herkunftsmilieu keine einfachen Rückschlüsse etwa auf eine oligarchische Orientierung zu.

Wichtige Anregungen für sein Menschenbild und sein Urteil über gestaltende politische Kräfte wie auch über Verfassungsaspekte mögen vor allem die zeitgenössischen Sophisten gegeben haben, die mit aufklärerischem Anspruch gerade auch in der Athener Öffentlichkeit wirkten. Da Thukydides jegliche Art direkten politischen Bekenntnisses meidet, kann nur die Werkinterpretation über sein politisches Denken Aufschluss geben.

Maßgebliche Bedeutung für Geschichtsverständnis und politisches Denken des Thukydides hat sein Menschenbild. Eine allen Menschen gemeinsame und die Zeiten überdauernde menschliche Natur bestimmt als regulatives Prinzip das historische Geschehen, wie Hagmaier z. B. aus Thukydides’ verallgemeinernder Einschätzung des Kriegs- und Bürgerkriegsgeschehens in Kerkyra ableitet:
Mit derlei Reflexionen möchte Thukydides dazu anleiten, folgert Hagmaier, „die Gesetzmäßigkeiten historisch-politischer Prozesse, die aus den Grundtriebkräften der ἀνθρώπεια φύσις [menschlichen Natur] resultieren, am Beispiel des peloponnesischen Krieges zu erfassen, um die aus der Lektüre seines Geschichtswerkes gewonnenen Einsichten auch auf künftige Geschehensabläufe anzuwenden.“

Als eine von Thukydides vielfach und insbesondere im Melierdialog angesprochene wesentliche Komponente der Menschennatur stellt sich das Machtstreben von Individuen, Gruppen wie auch ganzen Staaten dar, das von Ehrgeiz, Eigensucht und Furcht angetrieben wird. „Wer immer Schwäche zeigt, muß dem Stärkeren unterliegen“, resümiert Will die von Thukydides aufbereiteten Erfahrungen, „wer immer die Möglichkeit zu herrschen sieht, scheut kein Verbrechen.“ Die Herrschsucht gründe in der Raffgier, im Mehrhabenwollen zum eigenen Vorteil, sowie in der Ehr- und Ruhmsucht.
Im Übrigen geht Thukydides laut Scardino davon aus, dass der Mensch zweckrational im Sinne des eigenen Vorteils handle, sofern ihn nicht Wissensmängel, Affekte, von denen er sich mitreißen lässt, oder äußere Umstände daran hinderten. Oft lässt er sich allerdings mehr von Wünschen und Hoffnungen leiten als von vernünftiger Überlegung – „wie denn die Menschen gewöhnlich, was sie begehren, unbedachter Hoffnung überlassen, was aber nicht bequem ist, mit selbstherrlichen Begründungen wegschieben.“ Deshalb, so Leppin, wird in den von Thukydides behandelten Reden zumeist an den Eigennutz der Zuhörer appelliert, während moralische und rechtliche Überlegungen demgegenüber zurücktreten.

So sehr Thukydides den Einfluss der natürlichen menschlichen Eigenschaften auf das politische und historische Geschehen hervorgehoben hat – und damit der herkömmlichen Vorstellung vom bestimmenden Einfluss der Götter auf das menschliche Schicksal entgegengetreten ist –, so erweist sich sein Menschenbild andererseits weder als vorherbestimmt (deterministisch) noch als statisch: „Seine Aussagen über die menschliche Natur erlauben für sich genommen keine präzisen Vorhersagen, denn der Historiker weiß, daß Naturkatastrophen und Zufälle die Entwicklung beeinflussen können.“ Während die Natur (φύσις) des Menschen sich gleich bleibe, seien die Verhaltensweisen (τρόποι) für Thukydides durchaus wandlungsfähig, zum Besseren wie zum Schlechteren. Im Athen des 5. Jahrhunderts v. Chr. hatten sich mit den Tributen der Bundesgenossen im Seebund, mit der komfortablen Machtposition der Stadt auch in wirtschaftlicher Hinsicht und mit der Demokratisierung der Bürgerschaft die Wünsche nach Mehrung des Reichtums stark verbreitet. So wurde nach Thukydides Geldgewinn zum Motiv Einzelner, von Gruppen bzw. der Bevölkerung insgesamt.

Indem Thukydides von der Individualpsychologie zu sozialpsychologischen Ableitungen im Hinblick auf die Reaktionen und Verhaltensweisen von Menschenansammlungen – in Sonderheit der athenischen Volksversammlung – gelangt und dort eine verstärkte Neigung zu Affekten und Leidenschaft auf Kosten der Vernunft konstatiert, erwartet er von Politikern, die sich wie Perikles durch Rationalität und persönliche Integrität auszeichnen, so Scardino, dass sie das Volk durch analytische und kommunikative Fähigkeiten in die richtigen Bahnen lenken. Das ist nach Thukydides umso nötiger, als in der Massenversammlung noch weitere abträgliche Eigenschaften stark ausgebildet sind:
Um solche Tendenzen der Masse zu neutralisieren, bedarf es führender Politiker mit entgegengesetzten Eigenschaften, die neben der uneigennützigen Liebe zur eigenen Polis über analytischen Verstand verfügen, sich anderen gut mitzuteilen vermögen, durchsetzungsfähig sind und sich in ihrem Wirken für das Gemeinwesen als unbestechlich erweisen. Solche Eigenschaften findet Thukydides bei Perikles, aber auch bei Hermokrates und Themistokles. Alkibiades dagegen genügte trotz seiner Brillanz diesem Eigenschaftsprofil nicht, insofern er hauptsächlich eigenen Interessen folgte und nicht die Fähigkeit besaß, das Vertrauen des Volkes auf Dauer zu gewinnen. In seiner abschließenden Würdigung des Perikles rühmt Thukydides ihm nach:

Verfassungstheoretische Fragen stehen weder im Zentrum des Thukydideischen Werkes, noch gibt es dazu von ihm überhaupt zusammenhängend zielgerichtete Reflexionen. Welches die beste Polisverfassung sei, hat Thukydides nicht ausdrücklich behandelt. Dennoch haben Thukydides-Forscher verbreitet ein Interesse daran zu klären, wie ein oft so akribischer und weitläufig orientierter Beobachter des Zeitgeschehens in Bezug auf das ihm vertraute Verfassungsspektrum der griechischen Poleis eingestellt war.

Als maßgeblichen Anhaltspunkt für das Verfassungsideal des Thukydides nimmt Will dessen Urteil, wonach Athen in der Ära des Perikles zwar dem Namen nach Demokratie, tatsächlich aber die Herrschaft des ersten Mannes war, und zieht den Schluss, es sei Thukydides um die Aussöhnung der demokratischen Welt mit der oligarchischen gegangen, indem er als neues Staatsmodell die aristokratische Herrschaft innerhalb der demokratischen propagierte.

Ergebnisoffener fällt die diesbezügliche Werkanalyse Leppins aus. Die von Thukydides behandelten Reden mit Verfassungsbezug etwa gäben nicht zwingend Thukydides’ eigenes Denken darüber wieder, sondern zielten vornehmlich auf Schärfung des Problembewusstseins beim Leser. Deutlich sei die besondere Wertschätzung einer stabilen gesetzlichen Ordnung und die Warnung vor der Anomie, die z. B. infolge der Attischen Seuche auftrat. In der wohl eingehendsten Darstellung eines demokratischen Verfassungssystems durch den Syrakusaner Athenagoras werden Gesetzesgeltung und rechtliche Gleichheit der Bürger als Grundprinzipien ausgewiesen; hinsichtlich ihrer politischen Funktion werden die Bevölkerungsgruppen, die als Demos ein Ganzes bilden, jedoch unterteilt: „Die Reichen (οἱ πλόυσιοι) sind die geeignetsten Wächter über die staatlichen Gelder; die Verständigen (ζυνετόι) sind am tauglichsten darin, Ratschläge zu erteilen; die Masse (οἱ πολλόι) ist am besten geeignet zu entscheiden, nachdem sie sich über den Sachverhalt unterrichtet hat.“

Innerhalb der verfassungstypologischen Debatte wird von demokratischer Seite eher „institutionalistisch“ argumentiert, etwa mit der Hervorhebung der Ämterlosung, von oligarchischer Seite eher „personalistisch“, also wesentlich unter Hinweis auf die besonderen politischen Qualitäten der Herrschaftseliten. Einen prinzipiellen qualitativen Unterschied zwischen Demokratien und Oligarchien macht Thukydides anscheinend nicht. Das Problem der von Affekten geleiteten Massen stelle sich bei beiden Verfassungstypen. Kriterium einer guten Verfassung sei nach Thukydides im Wesentlichen der geglückte Interessenausgleich zwischen der Masse und den Wenigen.

Seine größte ausdrückliche Zustimmung fand die nach der oligarchischen Gewaltherrschaft der 400 in Athen 411 v. Chr. praktizierte Verfassung der 5000, in der eine auf die Anzahl der Hopliten beschränkte Größe der Volksversammlung die politische Entscheidungsmacht hatte:
Thukydides’ positives Urteil über das demokratische Athen zur Zeit des Perikles steht dazu nach Leppin nicht im Widerspruch, wenn man zugrunde legt, dass es Thukydides kaum um eine Festlegung im Rahmen der klassischen Verfassungstypologie (Monarchie, Oligarchie, Demokratie) gegangen ist, sondern um die Einheit und politische Funktionstüchtigkeit der Polis im jeweils gegebenen historisch-politischen Umfeld.

„Das erste Blatt des Thukydides ist der einzige Anfang aller wahren Geschichte“, schrieb Immanuel Kant in Übereinstimmung mit David Hume („The first page of Thucydides is the commencement of real history“). Die damit auch unter geschichtsphilosophisch Interessierten auf den Höhepunkt der Wertschätzung gelangende Thukydides-Rezeption hat jedoch nicht durchweg ein solches Ausmaß an Zuwendung angenommen. Nicht erst die anhaltend intensive neuere Thukydides-Forschung hat neben die Reverenz an den Protagonisten einer wissenschaftlich reflektierten Geschichtsdarstellung auch kritische Akzente gesetzt. Gerade der Beginn seiner Wirkungsgeschichte lässt auf unterschiedliche Resonanz schließen.

Zu schreiben wie Thukydides war das Ziel mancher antiker Autoren – wenn sie sich denn für politische Geschichte interessierten. Xenophon schloss an ihn an, ebenso wie wohl auch Kratippos von Athen. Philistos von Syrakus ahmte ihn nach und Polybios nahm ihn sich zum Vorbild. Dagegen konstatiert Will eine zunächst bescheidene allgemeine Wirkung des Thukydides auf Historiker, Redner, Publizisten und Philosophen, die erst mit dem Attizismus des ersten vorchristlichen Jahrhunderts in verbreitete Rezeption umschlug. Weder Platon noch Demosthenes beispielsweise haben sich im Rahmen der bekannten Überlieferung mit ihm auseinandergesetzt. Plutarch wiederum wendete sich ihm intensiv zu: Etwa fünfzig Zitate aus Thukydides’ Werk sind bei ihm zu finden, „die Viten des Alkibiades und Nikias können stellenweise als Paraphrasen des Thukydideischen Berichts angesehen werden.“

Während Cicero sich als Stilkritiker ablehnend über die im Werk enthaltenen Reden des Thukydides äußerte, haben sowohl Sallust als auch Tacitus sich teils stark an ihm orientiert. Allerdings ist Cicero das thukydideische Werk bestens geläufig, denn er zitiert in seinen Briefen an Atticus und an anderen Stellen daraus und lobt sowohl die Leistung des Historikers als auch den Stil seiner Darstellung. Überhaupt nahm das Interesse am Werk des Thukydides in der römischen Kaiserzeit anscheinend noch deutlich zu: Lukian von Samosata machte sich in seinem Werk "Wie man Geschichte schreiben soll" darüber lustig, dass mehrere Geschichtsschreiber (so Crepereius Calpurnianus) ihre Werke vollständig an dem des Thukydides ausrichteten und ganze Passagen von ihm nur leicht verändert übernahmen. Im 3. Jahrhundert wurde Cassius Dio von Thukydides beeinflusst, ebenso Dexippos, von dessen Werk aber nur Fragmente erhalten sind.

Auch in der Spätantike blieb Thukydides oft Vorbild, so für Ammianus Marcellinus (bzgl. seiner Vorgehensweise in den zeitgenössischen Büchern), Priskos (der sich bei Beschreibungen teils topisch an Thukydides anlehnte) oder für Prokopios von Caesarea. Die in der klassizistischen Hochsprache verfassten Werke byzantinischer Geschichtsschreiber waren ebenfalls von Thukydides beeinflusst.

Im Westen kannte man Thukydides während des Mittelalters nur in Auszügen und in indirekter Überlieferung aus Byzanz, während er in der Renaissance wieder Verbreitung fand. 1502 gab Aldus Manutius in Venedig die griechische Editio princeps heraus. Eine lateinische Übersetzung wurde von Lorenzo Valla 1452 vollendet und 1513 gedruckt. Die erste Übertragung ins Deutsche, angefertigt vom Theologieprofessor Johann David Heilmann, erschien 1760.

In der Neuzeit wurde Thukydides u. a. als „Vater der politischen Geschichtsschreibung“ gefeiert und für seine Objektivität gerühmt. Außer Hume und Kant priesen ihn Machiavelli, der stark von ihm beeinflusste Thomas Hobbes, der ihn ins Englische übersetzte und sein Werk interpretierte, sowie Georg Wilhelm Friedrich Hegel. Friedrich Nietzsche notierte: 

Unerreicht nennt Wolfgang Will Thukydides’ Akribie; doch vor allem werde sich an ihn halten müssen, wer Großmachtpolitik im 21. Jahrhundert verstehen wolle. Von zeitgenössischen Geschichtswerken sei wenig Hilfestellung zu erwarten.

In vieler Hinsicht nachvollziehbar ist Thukydides’ Orientierung am Grundsatz der größtmöglichen Objektivität. Zwar lassen sich nicht alle Angaben verifizieren, aber doch ein bedeutender Teil, wie epigraphische und prosopographische Studien belegen. Dass Thukydides oft allein als Quelle für bestimmte historische Vorgänge zur Verfügung steht und dass er nicht alle interessanten gesellschaftsgeschichtlichen Aspekte erfasst, muss in diesem Kontext stets mitbedacht werden. Die Wirkmächtigkeit seines Werkes sollte nicht dazu verleiten, seine Darstellung unreflektiert zu übernehmen. Thukydides’ Aufriss der griechischen Frühgeschichte ("Archaiologia") kann im Lichte der neueren Forschung nicht bestehen, und auch die Darstellung der so genannten "Pentekontaetie" weist erhebliche Lücken auf.

Trotz der Komplexität, die es nicht leicht macht, das Werk im Ganzen zu erfassen, entwickelte es eine große Breitenwirkung bis in heutige Zeit hinein. Die darin enthaltene Charakterisierung der Demokratie stand – vor ihrer Streichung – als Motto im Textentwurf zur EU-Verfassung. Am "Naval War College" in Newport, USA, – ebenso wie an anderen Militärakademien – ist das Werk Pflichtlektüre.





</doc>
<doc id="8399" url="https://de.wikipedia.org/wiki?curid=8399" title="Atoll">
Atoll

Ein Atoll ist ein ringförmiges Riff, in der Regel ein Korallenriff, das eine Lagune umschließt. Das Wort "Atoll" stammt aus dem Dhivehi, der Sprache der Malediven ( "atolhu"). Das Korallenriff bildet einen Saum von häufig äußerst schmalen Inseln aus, die nach dem polynesischen Wort für „Insel“ meist als "Motu" bezeichnet werden. In der Lagune können sich noch Reste des ehemaligen Vulkangipfels als Inseln über den Meeresspiegel erheben.

Nach der Theorie Charles Darwins (1809–1882) entstehen Atolle aus Saumriffen, die um eine Vulkaninsel herum entstehen. Die Insel kann im Laufe der Zeit im Meer versinken, sei es durch Erosion oder weil der Meeresboden absinkt bzw. der Meeresspiegel steigt, wobei das Riff weiter nach oben wächst. Am Ende reicht nur noch das Riff bis an die Wasseroberfläche und bildet einen Ring aus kleinen Inseln.

Eine andere Theorie des österreichischen Zoologen und Meeresforschers Hans Hass (1919–2013) kommt dagegen ohne Vulkane aus. Nach ihr bilden sich Atolle aus kegelförmigen Riffen, bei denen die Korallen im Zentrum wegen ungenügender Wasserversorgung absterben und nur die Korallen am Rand weiterwachsen, so dass ebenfalls eine ringförmige Struktur entsteht.

Durch spätere Einwirkung (Anhebung der Erdkruste, Absinken des Meeresspiegels) kann es geschehen, dass sich ein Atoll „hebt“ und die vom Korallenring umschlossene Lagune weitgehend, gelegentlich auch vollständig austrocknet. Man spricht dann von einem „gehobenen Atoll“. Beispiele hierfür sind Niue, Nauru oder Henderson Island.

Atolle kommen ausschließlich in tropischen Gewässern, hauptsächlich im Pazifischen Ozean und im Indischen Ozean vor, im Karibischen Meer gibt es nur vier Atolle. Einen Staat, der nur aus Atollen besteht, bilden die Malediven.

Das nördlichste Atoll ist das Kure-Atoll nordwestlich Hawaiis auf 28° 24’ n. Br., am südlichsten gelegen ist das Atoll Ducie der Pitcairninseln bei 24° 41’ s. Br. Atollähnliche Strukturen weiter südlich finden sich beim meist unterseeischen Elizabeth-Riff in der Tasmansee bei 29° 58’ s. Br.

Die größte Gesamtfläche (Lagune und Riff) hat mit 3.850 km² das maldivische Atoll Thiladhunmathi-Miladummadulhu (zwei Namen, jedoch geografisch ein Atoll). Davon sind 51 km² Landfläche. Die größte zusammenhängende Atoll-Struktur bildet die Great Chagos Bank im Chagos-Archipel mit einer Gesamtfläche von 12.642 km², bei nur 4,5 km² Landfläche. Die unterseeische Saya de Malha Bank, ein ringförmiges Korallenriff, das vollständig unter dem Meeresspiegel liegt und daher nicht als Atoll klassifiziert wird, hat eine Gesamtfläche von rund 35.000 km² (ohne die separate nördliche Ritchie-Bank).

Das Atoll mit der größten Landfläche ist Kiritimati im pazifischen Inselstaat Kiribati mit 321 km², gefolgt von Aldabra (Seychellen) mit 155 km².

Viele Atolle besitzen jedoch eine nur geringe Landfläche und keine natürlichen Süßwasserquellen und sind daher unbewohnte Inseln.

Die Bewohner vieler Atolle sind aufgrund des befürchteten Anstiegs des Meeresspiegels infolge der globalen Erwärmung um ihre Lebensgrundlage besorgt. Auch die wegen der dünner werdenden Ozonschicht zunehmende UV-Strahlung steht im Verdacht, das Wachstum der Korallen zu stören. Die stetig ansteigende CO-Konzentration in der Atmosphäre bewirkt darüber hinaus eine Übersäuerung des oberflächennahen Meerwassers. Das Kohlenstoffdioxid löst sich im Meerwasser zu Hydrogencarbonat, welches den Kalk angreift, der dem Riff die Stabilität verleiht.





</doc>
<doc id="8400" url="https://de.wikipedia.org/wiki?curid=8400" title="Lagune">
Lagune

Eine Lagune ist ein verhältnismäßig flaches Gewässer, das durch Sandablagerungen (Nehrung) oder Korallenriffe – wie zum Beispiel bei einem Atoll – vom Meer weitgehend oder vollständig abgetrennt ist.

Das Wort "Lagune" leitet sich über das italienische "laguna" („in einem Sumpfgebiet liegende Wasserfläche“; Sumpf, Strandsee, Haffsee) vom lateinischen "lacuna" (Weiher, Lache) ab. An der vorpommerschen und polnischen Ostseeküste finden sich lagunenartige Gewässer in "Haff" und "Bodden" genannten Buchten, in Schleswig-Holstein in Buchten, die "Noore" genannt werden. Am Schwarzen Meer heißen lagunenartige Küstengewässer im Mündungsbereich von Flüssen "Limane". Oft haben Lagunen Namensteile wie "See" oder "Bucht".

Die Abgrenzung zu kleinen Binnenmeeren ist fließend, wobei vor allem relativ kleine (meist wenige hundert Quadratkilometer Fläche) und besonders flache (selten mehr als 10 Meter) Gewässer als Lagunen gelten.

Lagunen entstehen in gemäßigten Breiten und klastisch dominierten Küsten aus „normalen“ Buchten: Durch Strandversetzung, die zur Bildung bzw. Verlängerung von Sandhaken und Nehrungen führt, werden diese Buchten vom offenen Meer abgetrennt. Ist die Abtrennung vollständig, so entsteht aus der Lagune ein Strandsee, der in humidem Klima langsam aussüßt. Solange eine Verbindung zum offenen Meer besteht, enthält die Lagune Brackwasser. In aridem Klima hingegen neigen Lagunen und Strandseen zur Übersalzung.

Völlig anders entstehen die Lagunen an karbonatisch dominierten Küsten tropischer Breiten. Dort wird die Lagune von der Riffkrone eines Korallenriffs gegen das Meer begrenzt, das heißt, das Material, aus dem die Barriere besteht, wird nicht von Strömungen angeschwemmt, sondern wird vor Ort von den riffbildenden Organismen (nicht nur Korallen) produziert. Auch dort bestimmt das Klima, ob die Lagune eher brackisch oder eher übersalzen ist. Einen Spezialfall dieser Lagunenentstehung stellen Atolle dar. Bei diesen sinkt eine kleine ozeanische Vulkaninsel ab bzw. wird erodiert währenddessen ihr Saumriff schnell genug wächst, um mit dem Absinken schrittzuhalten. Am Ende bleibt nur das Saumriff nahe der Meeresoberfläche und dort, wo einst die Vulkaninsel aus dem Meer ragte, befindet sich die Lagune. Wird ein Atoll angehoben (siehe → gehobenes Atoll), fällt die Lagune weitgehend oder ganz trocken. Die übrig gebliebene Senke und/oder ein darin befindlicher See behält dann die Bezeichnung „Lagune“.

Da Lagunen meist nur schlecht zugänglich sind, haben sie sich in vielen Fällen als von Menschen wenig beeinflusste Ökosysteme erhalten können. Diese Feuchtgebiete dienen dabei als wichtige Rückzugsgebiete von Wasservögeln, Fischen, kleineren Tieren und Pflanzen. Die ökologisch wichtigsten Lagunen werden international durch die Ramsar-Konvention geschützt.

Ein Beispiel einer – zumindest teilweise – erschlossenen und genutzten Lagune ist die "Lagunenstadt Venedig" in der Nähe der Mündung des Po in Italien. Grund für die Stadtgründung in Lagunen ist meist die sehr sichere Lage. Viele Lagunen sind außerdem sehr fischreich, was insbesondere in der Vergangenheit für die Bewohner der Umgebung von großer wirtschaftlicher Bedeutung war.




</doc>
<doc id="8403" url="https://de.wikipedia.org/wiki?curid=8403" title="Deutsche Kolonien">
Deutsche Kolonien

Die deutschen Kolonien wurden vom Deutschen Kaiserreich gegen Ende des 19. Jahrhunderts erworben und nach dem Ersten Weltkrieg gemäß dem Versailler Vertrag von 1919 abgetreten. Sie wurden von Bismarck Schutzgebiete genannt, weil er in ihnen den deutschen Handel schützen wollte. Die deutschen Kolonien waren 1914 das an Fläche drittgrößte Kolonialreich nach dem britischen und französischen Kolonialreich. Gemessen an der Bevölkerungszahl lag es an vierter Stelle nach den niederländischen Kolonien. Sie waren gemäß Artikel 1 der Verfassung des Deutschen Reichs nicht Bestandteil des Reichsgebiets, sondern überseeischer Besitz des Deutschen Reichs.

Ausgewanderte Deutsche gründeten in Übersee Siedlungen, die bisweilen als „deutsche Kolonien“ bezeichnet werden, aber keine Souveränitätsrechte des Herkunftslandes ausübten.

In den Staaten des 1815 gegründeten Deutschen Bundes und des 1833 gegründeten Deutschen Zollvereins wurde insbesondere ab den 1840er Jahren von privater und wirtschaftlicher Seite der Ruf nach deutschen Kolonien laut. Doch von staatlicher Seite gab es keine solchen Bestrebungen. Von privater Seite wurde 1839 die "Hamburger Kolonialgesellschaft" gegründet, die die Chathaminseln östlich von Neuseeland käuflich erwerben wollte, um dort deutsche Auswanderer anzusiedeln, aber Großbritannien machte ältere Ansprüche auf die Inseln geltend. Hamburg brauchte den Schutz der Royal Navy für seine weltweite Schifffahrt und verzichtete deshalb auf politische Unterstützung der Kolonialgesellschaft. Einigermaßen erfolgreich war der 1842 gegründete Verein zum Schutze deutscher Einwanderer in Texas, der die deutschen Siedlungen in Texas zu einer Kolonie „Neu Deutschland“ ausweiten wollte, aber die Annexion der Republik Texas durch die Vereinigten Staaten von Amerika 1845 machte dieses Ansinnen zunichte.

Entscheidende Punkte für das Desinteresse staatlicherseits an Kolonien war die Begrenzung des deutschen politischen Denkens zu der Zeit auf die Belange in Deutschland und Europa und das Fehlen einer deutschen Seemacht, die für den Erwerb überseeischer Kolonien erst den machtpolitischen Rückhalt bieten konnte. Mit dem Aufbau der österreichischen Flotte und der preußischen Flotte ab 1848 wurden solche Machtmittel geschaffen.

1857 lief die österreichische Fregatte "Novara" von Triest zu einer Expedition aus, die auch die Erforschung und Inbesitznahme der Nikobaren im Indischen Ozean beinhaltete. 1858 lief die "Novara" die Nikobaren an, aber zu einer Übernahme in österreichischen Besitz kam es nicht.
Zum nächsten Versuch der Erwerbung einer Kolonie von staatlicher Seite kam es ab 1859, als Preußen sich die chinesische Insel Formosa aneignen wollte. Preußen hatte sich bereits beim französischen Kaiser Napoleon III. seiner Zustimmung für das Unternehmen versichert, da gleichzeitig Frankreich in Ostasien Kolonien erwerben wollte. Da Frankreich an Vietnam interessiert war, nicht aber an Formosa, konnte Preußen an die Inbesitznahme der Insel gehen. Ein preußisches Geschwader, das Ende 1859 Deutschland verließ und für Preußen und alle weiteren Staaten des Deutschen Zollvereins Handelsverträge in Asien abschließen sollte, sollte auch Formosa besetzen, aber wegen zu schwacher Kräfte des Geschwaders, und um einen Handelsvertrag mit China nicht zu gefährden, wurde der Auftrag nicht ausgeführt. Mit Kabinettsorder vom 6. Januar 1862 wurde der das Geschwader begleitende Botschafter Graf Eulenburg „von der Ausführung der ihm erteilten Aufträge wegen Ermittlung eines zu einer preußischen Ansiedlung geeigneten überseeischen Territoriums entbunden“.

Trotzdem sollte ein Schiff des preußischen Geschwaders, die "Thetis", noch Patagonien in Südamerika anlaufen für eine Erkundung als Kolonie, wobei die preußische Marineführung hauptsächlich an die Schaffung eines Marinestützpunktes an der südamerikanischen Küste dachte. Die "Thetis" hatte bereits Buenos Aires erreicht, als der Kommandant des Schiffes wegen der Erschöpfung der Mannschaft nach dem jahrelangen Auslandsaufenthalt, und der Überholungsbedürftigkeit des Schiffes selbst, die Heimfahrt befahl.

Nach dem Deutsch-Dänischen Krieg 1864 strebten kolonialwillige Kreise in Preußen an, die zuvor dänischen Nikobaren in Besitz zu nehmen. Dänemark seinerseits bot 1865 vergeblich Dänisch-Westindien an, um den vollständigen Verlust Schleswigs zu verhindern. 1866 und noch einmal 1876 machte der Sultan der Sulu-Inseln, die zwischen Borneo und den Philippinen liegen, ein Angebot seine Inseln unter den Schutz Preußens, beziehungsweise des Reiches zu stellen, was aber beide Male abgelehnt wurde. Der Sultan von Witu bat den Reisenden Richard Brenner 1867, ein preußisches Protektorat über sein Land zu erwirken, das in Berlin aber nicht einmal erwogen wurde.

In der 1867 in Kraft getretenen Verfassung des Norddeutschen Bundes wurde in Artikel 4 Punkt 1 auch „die Kolonisation“ als eine der Angelegenheiten unter der „Beaufsichtigung Seitens des Bundes“ gestellt und dieser Artikel 4 Punkt 1 wurde unverändert in die Verfassung des Deutschen Reiches von 1871 übernommen.

Auf Otto von Bismarcks Wunsch fuhr das Kriegsschiff "Augusta" 1867/68 in der Karibik, um für den Norddeutschen Bund Flagge zu zeigen. Auf das persönliche Drängen des Oberbefehlshabers der Marine des Norddeutschen Bundes, Adalbert von Preußen, hin, und ohne die Zustimmung Bismarcks, handelte der Kommandant der "Augusta", Franz Kinderling, mit dem Präsidenten von Costa Rica eine Marinebasis in Puerto Limón aus. Bismarck lehnte das Angebot ab, mit Rücksicht auf die Monroe-Doktrin der USA. Sein Wunsch, nicht die Vereinigten Staaten herauszufordern, ließ ihn auch ein Angebot der Niederlande, eine Marinebasis auf der niederländischen Insel Curaçao, vor der venezolanischen Küste, einzurichten, zurückweisen.

1868 hatte Bismarck in einem Brief an den preußischen Kriegs- und Marineminister Albrecht von Roon seine Ablehnung jeglichen Kolonialerwerbs deutlich gemacht:

Die Politik des Norddeutschen Bundes setzte zu dieser Zeit nicht auf den Erwerb von Kolonien, sondern von einzelnen Marinestützpunkten. Von ihnen aus sollte mit einer Kanonenbootpolitik im Sinne eines informellen Imperialismus der Welthandel der Bundesstaaten geschützt werden. 1867 wurde beschlossen, fünf Auslandsstationen einzurichten. So wurde 1868 beim japanischen Yokohama Land für ein deutsches Marine-Krankenhaus gekauft, das bis 1911 bestand. Bis das 1897 vom Reich erworbene Tsingtau in China als Kriegshafen zur Verfügung stand, blieb Yokohama Stützpunkt für die deutsche Flotte in Ostasien. 1869 wurde als erste Auslandsstation die "Ostasiatische Station" von der Marine als ein ständig mit deutschen Kriegsschiffen besetztes Seegebiet eingerichtet, was sich später beim Erwerb der Kolonien im Pazifik und von Kiautschou als nützlich erwies.

Der französische Kompensationsvorschlag, nach dem Deutsch-Französischen Krieg anstatt Elsass-Lothringen die französische Kolonie Cochinchina zu übernehmen, wurde von Bismarck und der Mehrheit der Abgeordneten des Reichstags des Norddeutschen Bundes 1870 abgelehnt. Nach der Reichsgründung von 1871 behielt er diese Meinung bei. Im Laufe der 1870er Jahre gewann die Kolonialpropaganda in Deutschland allerdings zunehmend an Öffentlichkeitswirksamkeit. 1873 wurde die "Afrikanische Gesellschaft in Deutschland" gegründet, die ihre Hauptaufgabe in der geographischen Erkundung Afrikas sah. 1878 erfolgte die Gründung des Centralvereins für Handelsgeographie und Förderung deutscher Interessen im Auslande, der Kolonien für Deutschland erwerben wollte und 1881 wurde der Westdeutsche Verein für Colonisation und Export gegründet, in dessen Satzung der stand. 1882 wurde der Deutsche Kolonialverein gegründet, der sich als Interessenverein für die Kolonialpropaganda sah. 1884 entstand die konkurrierende Gesellschaft für Deutsche Kolonisation, die sich die praktische Kolonisation zum Ziel setzte. Beide Vereine fusionierten 1887 zur Deutschen Kolonialgesellschaft. Für den Erwerb von Kolonien wurden in der Hauptsache vier Argumente angeführt:

Bismarck blieb gegenüber diesen Argumenten verschlossen und präferierte ein informelles Handelsimperium, in dem deutsche Firmen mit außereuropäischen Gebieten erfolgreich Handel trieben und sie ökonomisch durchdrangen, ohne aber deren Territorien zu okkupieren oder eine eigene Staatlichkeit aufzubauen.
Die ersten Fälle kolonialen Ausgreifens nach Übersee erfolgten daher auch ausgesprochen zögerlich: 1876 wurde ein Freundschaftsvertrag zwischen dem Deutschen Reich und Tonga abgeschlossen, der Deutschland die Errichtung einer Kohlestation in der zu Tonga zählenden Inselgruppe Vavaʻu zusicherte. Dem Deutschen Reich wurden alle Rechte der freien Benutzung des dafür nötigen Grund und Bodens garantiert. Die Hoheitsrechte des Königs von Tonga sollten allerdings unbeschadet bleiben. Zur eigentlichen Kolonisation kam es nicht. Der Kommandant des Kriegsschiffes SMS "Ariadne", Bartholomäus von Werner besetzte am 16. Juli 1878 die Orte Falealili und Saluafata auf der Samoa-Insel Upolu „im Namen des Reiches“. Die deutsche Besetzung der Ortschaften wurde im Januar 1879 rückgängig gemacht, durch den Abschluss eines „Freundschaftsvertrages“ der örtlichen Herrscher mit Deutschland. Am 19. November 1878 schloss von Werner mit den Oberhäuptlingen von Jaluit und der Ralik-Inselgruppe, Lebon und Letahalin, einen Vertrag über Vorrechte, wie die exklusive Anlage einer deutschen Kohlestation. Offizielle deutsche Kolonie wurden die Marshallinseln erst 1885. Von Werner erwarb auch je einen Hafen auf den Inseln Makada und Mioko in der Duke-of-York-Gruppe, die 1884 als Bestandteile des zukünftigen Schutzgebiets Deutsch-Neuguinea unter Reichsschutz gestellt wurden. Am 20. April 1879 unterzeichneten der Kommandant der SMS "Bismarck", Karl Deinhard und der kaiserliche deutsche Konsul für die Südsee-Inseln, Gustav Godeffroy Junior, mit der „Regierung“ der Insel Huahine in den Gesellschaftsinseln einen Freundschafts- und Handelsvertrag, der unter anderem der deutschen Flotte auch Ankerrecht in allen Häfen der Insel gewährte. Im September 1879 übernahm der deutsche Konsul auf Samoa, zusammen mit den Konsuln von Großbritannien und der USA, die Verwaltung von Stadt und Distrikt Apia auf der samoanischen Insel Upolu. Die westlichen samoanischen Inseln mit der Hauptstadt Apia wurden 1899 deutsche Kolonie.

Der Wandel in Bismarcks Politik in Bezug auf Kolonien fällt genau in die Zeit seiner um 1879 einsetzenden Schutzzollpolitik zur Sicherung der deutschen Wirtschaft gegen ausländische Konkurrenz. 

Im April 1880 griff Bismarck erstmals aktiv für eine koloniale Angelegenheit ein, als er die Samoa-Vorlage als Gesetzesvorlage in den Reichstag einbrachte, die vom Bundesrat befürwortet, aber vom Reichstag abgelehnt wurde. Dabei sollte ein in Schwierigkeiten geratenes privates deutsches Kolonialhandelsunternehmen vom Reich finanziell aufgefangen werden.

Im Mai 1880 bat Bismarck den Bankier Adolph von Hansemann um eine Ausarbeitung über deutsche koloniale Ziele im Pazifik und die Möglichkeiten für deren Durchsetzung. Hansemann sandte seine „Denkschrift über die Colonial-Bestrebungen in der Südsee“ im September des Jahres dem Reichskanzler zu und die darin vorgeschlagenen Gebietserwerbungen wurden vier Jahre später fast übereinstimmend als Kolonien genommen oder beansprucht. Die 1884–1885 beanspruchten, aber noch nicht übernommenen Gebiete im Pazifik wurden schließlich 1899 in deutschen Kolonialbesitz überführt. Bezeichnenderweise war Hansemann denn auch Gründungsmitglied des 1882 geschaffenen "Neuguinea-Konsortiums" für den Erwerb von Kolonien auf Neuguinea und in der Südsee.

Im November 1882 nahm der Bremer Tabakhändler Adolf Lüderitz mit dem Auswärtigen Amt Verbindung auf und bat um Schutz für eine Handelsniederlassung südlich der Walfischbucht an der südwestafrikanischen Küste. Im Februar und November 1883 fragte Bismarck bei der Regierung in London an, ob England den Schutz der Handelsniederlassung von Lüderitz übernehmen wolle. Beide Male lehnte die englische Regierung ab.

Nachdem im März 1883 die "Sierra Leone Convention" zwischen England und Frankreich veröffentlicht wurde, in der Interessenssphären zwischen den beiden Staaten in Westafrika abgegrenzt wurden, ohne andere Handelsnationen dabei zu berücksichtigen, bat die deutsche Regierung im April 1883 die Senate der Städte Lübeck, Bremen und Hamburg um eine Stellungnahme dazu. Die Hamburger Überseehändler verlangten in ihrer Antwort den Erwerb von Kolonien in Westafrika. Im Dezember 1883 ließ Bismarck den Hamburgern mitteilen, dass für die Sicherung des deutschen Handels ein Kaiserlicher Kommissar nach Westafrika entsandt werde, auch um Verträge mit „unabhängigen Negerstaaten“ zu schließen, und ein Kriegsschiff, die SMS "Sophie", solle den militärischen Schutz dafür übernehmen. Weiterhin erbat sich Bismarck für dieses Vorhaben Vorschläge und bat den Hamburger Kaufmann Adolph Woermann persönlich um seinen Rat, welche Instruktionen man dem Kaiserlichen Kommissar mit auf den Weg geben solle. Im März 1884 wurde Gustav Nachtigal zum Reichskommissar für die westafrikanische Küste ernannt und schiffte sich auf dem Kriegsschiff SMS "Möwe" nach Westafrika ein, um die entsprechenden Verträge abzuschließen.

Das Jahr 1884 markiert den eigentlichen Beginn der deutschen Kolonialerwerbungen, wenn auch schon seit 1876 Besitz und Rechte für das Deutsche Reich in Übersee erworben wurden. In gut einem Jahr wurde das flächenmäßig nach dem britischen und französischen drittgrößte Überseekolonialreich geschaffen. Bismarck stellte nach englischem Vorbild mehrere Besitzungen deutscher Kaufleute unter den Schutz des Deutschen Reichs. Damit nutzte er eine Phase außenpolitischer Entspannung zum Beginn des „kolonialen Experiments“, dem er selbst allerdings weiterhin skeptisch gegenüberstand.

Zunächst wurden die vom Bremer Kaufmann Adolf Lüderitz erworbenen Besitzungen an der Bucht von Angara Pequena („Lüderitzbucht“) und das angrenzende Hinterland („Lüderitzland“) im April 1884 als Deutsch-Südwestafrika unter den Schutz des Deutschen Reichs gestellt. Im Juli folgten Togoland und die Besitzungen von Adolph Woermann in Kamerun, im Februar 1885 das von Carl Peters und dessen Gesellschaft für deutsche Kolonisation erworbene ostafrikanische Gebiet und im April erwarben die Brüder Denhardt schließlich noch Wituland im heutigen Kenia. Mit der Übernahme von pazifischen Gebieten – im Mai 1885 Nordost-Neuguinea (Kaiser-Wilhelms-Land) und der davor gelegenen Inselgruppe (Bismarck-Archipel), sowie später der Marshall- und mehreren Salomon-Inseln – war die erste Phase deutscher Kolonialpolitik bis 1886 weitgehend abgeschlossen. Eine Ausweitung der pazifischen Besitzungen scheiterte zunächst am sogenannten "Karolinenstreit". 1888 beendete das Reich auf dem mittelpazifischen Nauru den Stammeskrieg und annektierte auch diese Insel.

Die Motive für Bismarcks plötzliche Kolonialerwerbungen im großen Maßstab sind in der historischen Forschung umstritten. Bei den Erklärungen dominieren zwei Strömungen, die entweder von einem „Primat der Innenpolitik“ oder einem „Primat der Außenpolitik“ ausgehen. Als ein innenpolitischer Grund wird der öffentliche Druck durch das entstandene „Kolonialfieber“ in der deutschen Bevölkerung angeführt. Zwar war die Kolonialbewegung organisatorisch nicht sehr stark, ihr gelang es aber, ihre Propaganda in die gesellschaftlichen Debatten einzubringen. Einer an Bismarck gesandten Denkschrift der Handelskammer Hamburg vom 6. Juli 1883, vom Reeder Adolph Woermann in die Wege geleitet, wird in der Forschung dabei besondere Bedeutung zuerkannt. Auch die bevorstehende Reichstagswahl 1884 und Bismarcks Intention, sowohl seine eigene Position zu stärken als auch die kolonialfreundliche Nationalliberale Partei an sich zu binden, werden als innenpolitische Motive gesehen. Hans-Ulrich Wehler vertritt die These des Sozialimperialismus, wonach die koloniale Expansion dem Zweck diente, die durch die wirtschaftliche Krisensituation entstandenen sozialen Spannungen nach außen „abzuleiten“ und so der charismatischen Herrschaft Bismarcks eine innenpolitische Legitimation zu verschaffen. Die sog. „Kronprinzenthese“ geht hingegen davon aus, Bismarck habe versucht, vor dem zu erwartenden Thronwechsel die Beziehungen zu England bewusst zu schwächen und so die Politik des als „anglophil“ geltenden Thronfolgers im Voraus zu beeinflussen.

Im außenpolitischen Bereich wird der Entschluss zur Expansion als eine Verlängerung des Konzepts des europäischen Gleichgewichts in globaler Perspektive gesehen: Durch das „Mitziehen“ im Wettlauf um Afrika habe demnach das Deutsche Reich auch weiterhin seine Stellung unter den Großmächten verteidigen wollen. Ebenso wird eine Annäherung an Frankreich durch eine „koloniale Entente“ als ein Motiv gesehen. Sie habe Frankreich von Revanche-Gedanken in Bezug auf das 1871 annektierte Elsass-Lothringen ablenken sollen.

Zusammenfassend wird heute nicht mehr geglaubt, dass die Entscheidung zum Erwerb außereuropäischer Gebiete einen radikalen Richtungswechsel der Politik Bismarcks darstellte. An Bismarcks liberal-imperialistischen Idealvorstellung einer überseeischen Politik durch privatwirtschaftliche Initiativen, die er von Beginn an verfolgt habe, änderte sich auch durch die Schutzerklärungen nicht viel.

Bismarck übertrug durch staatliche Schutzbriefe den privaten Organisationen den Handel und die Verwaltung der jeweiligen "Deutschen Schutzgebiete". Die Verwaltung der erworbenen Gebiete übten im Auftrag des Deutschen Reiches zunächst die Deutsch-Ostafrikanische Gesellschaft (1885–1890), die Deutsche Witu-Gesellschaft (1887–1890), die Neuguinea-Kompagnie (1885–1899) und die auf den Marshallinseln tätige Jaluit-Gesellschaft (1888–1906) aus. Auch die deutschen Kolonien in Südwest- und Westafrika sollten auf Bestreben Bismarcks in dieser Weise verwaltet werden, doch weder die Deutsche Kolonialgesellschaft für Südwestafrika noch das Syndikat für Westafrika waren hierzu gewillt oder in der Lage.
Diese Gebiete waren nach militärischen Machtdemonstrationen durch extrem ungleiche Verträge in den Besitz der Deutschen gelangt: Gegen ein vages Schutzversprechen und eine nach deutschen Verhältnissen lächerlich geringe Kaufsumme übergaben die indigenen Herrscher große Gebiete, auf die sie nach afrikanischem Rechtsverständnis oft keinen Anspruch hatten; häufig blieben ihnen auch die Details des Vertrags mangels Sprachkenntnissen dunkel. Sie spielten aber mit, weil die langen Verhandlungen mit den Kolonisatoren und der rituell vollzogene Vertragsabschluss ihre Autorität enorm erhöhten. Diese Verträge wurden nun vom Deutschen Reich bestätigt; den Organisationen wurden umfassende Hoheitsrechte ohne Gewaltenteilung zugesprochen. Das Reich behielt sich allein im Schutzgebietsgesetz von 1886 die Oberhoheit und gewisse Eingriffsrechte vor, ohne dass dies spezifiziert oder konkretisiert worden wäre. Damit war das staatliche Engagement auf finanziell und organisatorisch auf ein Mindestmaß reduziert.

Diese Strategie scheiterte allerdings innerhalb weniger Jahre: Aufgrund der schlechten finanziellen Situation in fast allen „Schutzgebieten“ sowie der teilweise prekären Sicherheitslage – in Südwestafrika und in Ostafrika drohten 1888 Aufstände der indigenen Bevölkerung, in Kamerun und Togo bestand die Gefahr von Grenzstreitigkeiten mit den benachbarten britischen Kolonien, überall hatten sich die Gesellschaften als mit dem Aufbau einer effizienten Verwaltung überfordert erwiesen – waren Bismarck und seine Nachfolger gezwungen, alle Kolonien direkt und formell der staatlichen Verwaltung des Deutschen Reiches zu unterstellen.

Nach 1885 wandte sich Bismarck wieder gegen weiteren Kolonialerwerb und setzte seine politischen Prioritäten bei der Beziehungspflege mit den Großmächten England und Frankreich fort. Als ihn 1888 der Journalist Eugen Wolf drängte, Deutschland müsse weitere Kolonien erwerben, um im sozialdarwinistisch verstandenen Wettbewerb mit den anderen Großmächten nicht ins Hintertreffen zu geraten, erwiderte Bismarck 1888, dass Priorität für ihn weiterhin die Sicherung der vor kurzem errungenen nationalen Einheit war, die er durch Deutschlands Mittellage gefährdet sah:

1889 erwog Bismarck einen Rückzug Deutschlands aus der Kolonialpolitik. Die deutschen Aktivitäten in Ostafrika und auch die Bestrebungen bezüglich Samoas wollte er nach Aussage von Zeitzeugen ganz beenden. Weiter wurde berichtet, Bismarck mochte nichts mehr mit der Verwaltung der Kolonien zu tun haben und wollte sie der Admiralität übergeben. Dem italienischen Ministerpräsidenten, Francesco Crispi, bot Bismarck im Mai 1889 die deutschen Besitzungen in Afrika zum Kauf an – was dieser mit einem Gegenangebot bezüglich der italienischen Kolonien beantwortete.

Die Kolonien dienten Bismarck in diesem Zusammenhang aber auch als Verhandlungsmasse. So wurde bei der Kongokonferenz 1884/85 in Berlin Afrika unter den Großmächten aufgeteilt. 1884 wurde im Namen von Lüderitz mit dem Zulu-König Dinuzulu ein Vertrag geschlossen, der Deutschland einen lokalen Gebietsanspruch an der Santa-Lucia-Bucht im Zululand sichern sollte. Im Zuge eines Ausgleichs mit Großbritannien wurde das Ansinnen aber im Mai 1885 fallengelassen und auch Pondoland in Südafrika zugunsten Englands nicht als deutsche Kolonie anerkannt. 1885 gab Deutschland auch Ansprüche auf die westafrikanischen Territorien Kapitaï und Koba zugunsten Frankreichs auf. Gleiches galt für das Mahinland in Bezug auf Großbritannien. 1886 einigten sich Deutschland und Großbritannien auch auf die Abgrenzung ihrer Interessenssphären in Ostafrika. Nach Bismarcks Rücktritt verzichtete das Deutsche Reich im Helgoland-Sansibar-Vertrag vom 1. Juli 1890, den er noch maßgeblich vorbereitet hatte, auf alle etwaigen Ansprüche nördlich Deutsch-Ostafrikas. Dadurch sollte ein Ausgleich mit Großbritannien erzielt werden. Auch die deutschen Ansprüche auf die gesamte Somaliküste zwischen Buur Gaabo und Aluula wurden aufgegeben, wovon die Beziehungen zum Dreibund-Partner Italien profitierten. Deutsch-Südwestafrika wurde im Gegenzug mit dem Sambesi verbunden (Caprivizipfel). Unter diesen Umständen scheiterten wiederum deutsche Kolonialbestrebungen in Südostafrika.

Unter Kaiser Wilhelm II. (1888–1918) versuchte Deutschland durch Erwerb weiterer Handelsvertretungen seinen Kolonialbesitz auszubauen. Die wilhelminische Ära steht für eine schwärmerisch-expansionistische Politik und eine forcierte Aufrüstung, insbesondere der Kaiserlichen Marine. Die Kolonialbewegung war zu einem ernstzunehmenden Faktor in der deutschen Innenpolitik angewachsen. Zu ihr rechneten neben der Deutschen Kolonialgesellschaft auch der 1891 gegründete, extrem nationalistische Alldeutsche Verband. Zusätzlich zu den bisher vertretenen Argumenten wurde von der deutschen Kolonialbewegung nun vorgebracht, man müsse den Sklavenhandel in den Kolonien bekämpfen und die indigene Bevölkerung von ihren muslimischen Sklaventreibern befreien. Diese abolitionistischen Forderungen mit deutlich antimuslimischer Stoßrichtung nahmen nach dem so genannten Araberaufstand an der ostafrikanischen Küste 1888 Züge einer Kreuzzugsbewegung an. Im Vordergrund standen jetzt aber Fragen des nationalen Prestiges und der Selbstbehauptung in einer sozialdarwinistisch verstandenen Konkurrenz der Großmächte: Deutschland als „Nachzügler“ müsse jetzt den ihm zustehenden Anteil einfordern.

Diesem Trend folgte die Reichsregierung. Im Rahmen einer neuen Weltpolitik einen „Platz an der Sonne“ (so der spätere Reichskanzler Bernhard von Bülow am 6. Dezember 1897 vor dem Deutschen Reichstag) für die angeblich „zu spät gekommene Nation“ an, womit neben dem der Besitz von Kolonien ein Mitspracherecht in allen kolonialen Angelegenheiten gemeint war. Diese Politik des nationalen Prestiges befand sich in scharfem Kontrast zu Bismarcks eher pragmatisch begründeten Kolonialpolitik von 1884/1885.
Nach 1890 gelang nur noch der Erwerb weniger Gebiete. 1897/98 wurde das chinesische Kiautschou mit dem Hafenort Tsingtau ein deutsches Pachtgebiet. In einem 50-km-Halbkreis um die Kiautschou-Bucht wurde eine "Neutrale Zone" eingerichtet, in der Chinas Souveränität durch Deutschland eingeschränkt war. Ferner bestanden deutsche Bergbau- und Eisenbahnkonzessionen in der Provinz Schantung. Durch den deutsch-spanischen Vertrag von 1899 kamen die mikronesischen Inseln der Karolinen, Marianen und Palau im Mittelpazifik hinzu. Deutsche Ansprüche auf die Philippinen konnten hingegen nicht umgesetzt werden, verschlechterten aber die diplomatischen Beziehungen zu den USA (Manila-Zwischenfall). Durch den Samoa-Vertrag wurde 1899 auch der Westteil der Samoa-Inseln im Südpazifik ein deutsches Schutzgebiet. Der Ostteil des vorher neutralen Salaga-Gebietes wurde dem deutschen Togo zugeschlagen. Gleichzeitig wurde die Herrschaft innerhalb der Kolonien ausgedehnt, z. B. in Deutsch-Ostafrika auf die Königreiche Burundi und Ruanda. Im Bafut- und Hehe-Krieg stieß Deutschland 1891 jedoch auch auf hartnäckigen Widerstand unter Volksgruppen des Hinterlandes von Kamerun bzw. Ostafrika. Ein Versuch des Kaisers im Jahre 1902 Niederkalifornien von Mexiko – auch als weitere Basis für die deutsche Flotte im Pazifik – zu erwerben, scheiterte.

Ein während der französisch-britischen Faschoda-Krise Ende 1898 von französischer Seite erneut unterbreitetes Angebot, Elsass-Lothringen gegen eine der französischen Kolonien zu tauschen, wurde erneut abgelehnt. Eine von manchen Kolonialpropagandisten angestrebte koloniale Neuordnung Afrikas fand nicht statt. Die Ausnahme stellte hier der Erwerb eines Teils des französischen Kongogebiets für Kamerun im Zuge der Zweiten Marokkokrise von 1911 dar (Neukamerun). Vergeblich hatte Deutschland als Kompensation für Marokko die gesamte französische Kongo-Kolonie gefordert. Die zunehmende Isolierung im Kreis der Großmächte, die in Deutschland als „Einkreisung“ wahrgenommen wurde, war der Preis für dieses forsche deutsche kolonialpolitische Auftreten.

Für die wirtschaftliche Entwicklung der Kolonien wurde 1896 das Kolonialwirtschaftliche Komitee gegründet.
1898 wurde in Witzenhausen die Deutsche Kolonialschule (Tropenschule) gegründet, um Menschen für eine Übersiedlung in die Kolonien landwirtschaftlich auszubilden. Die Nachfolgeeinrichtungen bilden heute einen Nebenstandort der Universität Kassel. Im Jahre 1900 nahm in Hamburg das Institut für Schiffs- und Tropenkrankheiten für die Ausbildung von Schiffs- und Kolonialärzten seine Arbeit auf.

Nach einer Viehseuche im Jahr 1897 in Deutsch-Südwestafrika hatten die Herero ihre überlebenden Viehbestände weit über das deutsche Kolonialgebiet verteilt. Diese Weideflächen waren zuvor jedoch an Großgrundbesitzer verkauft worden, welche nun das Vieh der Herero für sich beanspruchten. 1904 eskalierte die Situation schließlich zum Aufstand der Herero und Nama, dem die personalschwache Schutztruppe der Kolonie nicht gewachsen war. Die Reichsregierung entsandte daraufhin ein Marineexpeditionskorps und später Verstärkungen der Schutztruppe. Mit insgesamt etwa 15.000 Mann unter Generalleutnant Lothar von Trotha wurde der Aufstand der Herero im August 1904 in der Schlacht am Waterberg niedergeschlagen. Trotha erließ den sogenannten Vernichtungsbefehl, nach dem Überlebende in die Wüste zurückgetrieben wurden. Von den Überlebenden Hereros der Schlacht haben 1800 bis Ende November 1904 Britisch-Betschuanaland erreicht, Tausende flohen ins nördlichste Südwestafrika und Tausende kamen in der Wüste um. Von den geschätzten 50.000 Menschen des Hererovolkes kamen bis 1908 wahrscheinlich die Hälfte ums Leben. Mit 10.000 Opfern kam auch rund die Hälfte der Nama ums Leben. Diese hatten zuvor noch auf Seiten der Deutschen als Hilfstruppe bis Ende 1904 gegen die Herero gekämpft.

In Deutsch-Ostafrika kam es 1905/06 zum sogenannten Maji-Maji-Aufstand, bei dem geschätzte 100.000 Einheimische umkamen. Die Ablehnung eines Nachtragshaushaltes für eine weitere Unterstützung der Kolonialkriege führte Ende 1906 zur Auflösung des Reichstages und zu Neuwahlen. Die Reichstagswahl vom Januar 1907 (die sogenannte „Hottentottenwahl“) sollte über die Zukunft der Kolonien entscheiden.

Als Ergebnis der Kolonialkriege in Deutsch-Südwestafrika und Deutsch-Ostafrika, deren Ursachen in einer falschen Behandlung der einheimischen Bevölkerungen lagen, wurde ein Umbau der Kolonialverwaltung in Deutschland, eine wissenschaftliche Herangehensweise an die Nutzung der Kolonien und eine Verbesserung der Lebensbedingungen der Völker in den deutschen Kolonien als notwendig erachtet. Dafür wurde die oberste Verwaltungsbehörde für die Kolonien, die Kolonialabteilung, aus dem Außenministerium (die damalige Bezeichnung für ein Ministerium war „Amt“) ausgegliedert und im Mai 1907 zu einem eigenen Ministerium erhoben, das Reichskolonialamt. Als Gestalter der neuen Kolonialpolitik wurde nicht zufällig ein erfolgreicher Firmensanierer aus der Privatwirtschaft für das Amt als Staatssekretär – im heutigen Sprachgebrauch Minister – gewonnen, Bernhard Dernburg. Dernburg ging auf Reisen in die Kolonien, um vor Ort die Probleme zu erkunden und Lösungen zu finden. Gleichzeitig wurden wissenschaftliche und technische Einrichtungen für koloniale Zwecke gefördert oder gegründet, um auf dieser Grundlage die Kolonien zu entwickeln. Aus dem Hamburgischen Kolonialinstitut und der Deutschen Kolonialschule entstanden etwa Teile der heutigen Universitäten von Hamburg und Kassel. Für die Einheimischen wurde die medizinische Versorgung verbessert, Schulen gebaut und die Prügelstrafe wurde abgeschwächt. Straßen, Eisenbahnen und Häfen wurden im erweiterten Maße angelegt für die wirtschaftliche Erschließung der Kolonien. Dernburg im Januar 1909: „Das Ziel müssen mit dem Vaterland eng verbundene, administrativ unabhängige, wirtschaftlich selbstständige, gesunde Kolonien sein.“ Auch Kolonialstaatssekretär Wilhelm Solf unternahm 1912 und 1913 Reisen nach Afrika. Die dabei gesammelten Eindrücke gingen in sein Kolonialprogramm ein, das unter anderem eine Kompetenzerweiterung der Gouverneure und ein Verbot des Arbeitszwangs für Afrikaner vorsah. Des Weiteren befürwortete Solf die Idee eines Autostraßennetzes in den Kolonien, um weniger Lastenträger einzusetzen. Wilhelm Solf gewann für seine vergleichsweise friedfertige Kolonialpolitik, die sich an Diplomatie und geschickter Machtpolitik anstatt militärischer Stärke orientierte, alle Fraktionen des Reichstages mit der Ausnahme der rechten.

Als Ergebnis dieser neuen Politik gab es nach 1907 keine großen Aufstände in den deutschen Kolonien mehr und die wirtschaftliche Leistungsfähigkeit der Überseebesitzungen Deutschlands steigerte sich schnell. So verdoppelte sich von 1906 bis 1914 die Herstellung von Palmöl und Kakao in den Kolonien, die Kautschuk-Ausfuhr aus den afrikanischen Kolonien vervierfachte sich, der Baumwollexport aus Deutsch-Ostafrika erhöhte sich um das Zehnfache. Der gesamte Handel zwischen Deutschland und seinen Kolonien steigerte sich von 72 Millionen Mark im Jahre 1906 auf 264 Millionen Mark im Jahre 1913. Durch den wirtschaftlichen Aufschwung in den Schutzgebieten versechsfachten sich die Zoll- und Steuereinnahmen in den Kolonien von 1906 bis 1914. Mit der wirtschaftlichen Entwicklung der Kolonien waren sie von finanzieller Unterstützung durch das Reich unabhängig geworden oder waren auf dem Weg dahin. 1914 wurden nur noch Deutsch-Neuguinea und Kiautschou und die Schutztruppen in Afrika subventioniert.

1898 und 1913 schlossen das Deutsche Reich und Großbritannien Abkommen zur Übernahme der portugiesischen Kolonien. Der Vertragsfall sollte eintreten, wenn von der Regierung in Lissabon als Sicherheit für eine Anleihe Einnahmen aus den Kolonien eingesetzt würden. Außerdem wurde im Vertrag von 1913 als zusätzlicher Grund angegeben, die „Mißwirtschaft“ der Portugiesen in ihren Kolonien beenden zu wollen. Konkrete Schritte zur Übernahme erfolgten 1914, etwa mit der Gründung des "Übersee Studiensyndikats" von den deutschen Großbanken, mit der die wirtschaftliche Übernahme Angolas gewährleistet werden sollte. Im Juli 1914 legte die portugiesische Regierung eine Staatsanleihe für die wirtschaftliche Entwicklung von Angola auf, mit Sicherung durch angolanische Zolleinkünfte, die von einem deutschen Bankenkonsortium finanziert wurde. Damit war eine entscheidende Vertragsbestimmung aus dem deutsch-britischen Abkommen von 1913 über die Aufteilung der Kolonien Portugals zwischen Deutschland und England erfüllt. Am 27. Juli 1914 gab der deutsche Reichskanzler Theodor von Bethmann Hollweg der Regierung in London sein Einverständnis für die Veröffentlichung des bisher geheimgehaltenen Vertrages von 1913 über die beabsichtigte Aufteilung der portugiesischen Kolonien mit den Begründungen für die Wegnahme der Kolonien von Portugal. Der Ausbruch des Ersten Weltkrieges im August 1914 verhinderte weitere Schritte der Übernahme der portugiesischen Kolonien.

Bei Ausbruch des Ersten Weltkrieges, im August 1914, waren die Truppen in den deutschen Kolonien nicht auf einen Krieg mit europäischen Mächten vorbereitet. Die deutsche Seite hoffte auf die Einhaltung des Beschlusses der Kongo-Konferenz von 1885, die ihrer Auffassung nach alle Kolonialstaaten zur Handelsfreiheit und friedlichen Lösung kolonialer Probleme in Afrika verpflichte. Doch nur wenige Tage nach dem deutschen Kriegseintritt begann ein hoffnungsloser Widerstand der deutschen Truppen. Bis Ende 1914 waren Togo, Deutsch-Neuguinea, Samoa und Kiautschou in die Hände der Entente gefallen. In den größeren Schutzgebieten gelangen den Deutschen hingegen Anfangserfolge, etwa in den Schlachten bei Garua, Sandfontein und Tanga sowie im Kampf um Naulila. Mit der Besetzung der südafrikanischen Exklave Walvis Bay, der Provinz Cunene im portugiesischen Angola, des Grenzorts Taveta und der Stadt Kisii in Britisch-Ostafrika und der Insel Idjwi im Kivu-See kam es sogar zu geringfügigen deutschen Gebietsgewinnen. Anhaltender Widerstand scheiterte jedoch an der vergleichsweise geringen Truppenstärke sowie dem Mangel an Nachschub und schweren Waffen.

Die 5.000 Mann starke südwestafrikanische Schutztruppe ergab sich im Juli 1915 gegen die zehnmal so starken südafrikanischen Unionstruppen. In die Kolonie Kamerun schickten die Briten und Franzosen insgesamt 19.000 Soldaten und 24 Kriegsschiffe. Trotzdem ergaben sich die deutschen Truppen nicht und traten schließlich im Februar 1916 vor der feindlichen Übermacht in die neutrale spanische Kolonie Rio Muni über, begleitet von 14.000 kamerunischen Eingeborenen, die nicht unter der Herrschaft der Entente-Mächte leben wollten. Ab 1917 wurden die Interessen des Deutschen Reiches in seinen besetzten Kolonien durch die neutrale Schweiz wahrgenommen, was unter dem Druck der Entente jedoch nur teilweise gelang.

Nur in Deutsch-Ostafrika blieben die deutschen Truppen – ihre Höchstzahl betrug im Krieg 16.670 Mann, davon etwa 90 % afrikanische Askaris – unter Führung von Oberstleutnant Paul von Lettow-Vorbeck bis zum Waffenstillstand im November 1918 unbesiegt. Jedoch wich von Lettow auf Nachbarkolonien Portugals und Großbritanniens aus, um seinen Widerstand fortzusetzen. Auch eine mehrere Dutzend Mann starke Truppe unter dem Hauptmann Hermann Detzner in Neuguinea ergab sich nicht und führte Guerillakrieg. Als Detzner vom Waffenstillstand hörte löste er seine Truppe auf, ritt aus den Bergen nach Finschhafen und ergab sich dort Mitte Dezember 1918 den Australiern.

In Deutschland wurden auch im Krieg die Pläne für ein geschlossenes Deutsch-Mittelafrika weiterverfolgt. Es sollte sich vom Niger bis zur Kalahari-Wüste erstrecken und auch Angola, Mosambik, Belgisch-Kongo und weite Teile Französisch-Äquatorialafrikas miteinschließen.

Mit dem Inkrafttreten des Versailler Vertrages im Januar 1920 verlor Deutschland alle Kolonien. Die Entente teilten die Kolonien als Mandatsgebiete unter sich auf:


Nach dem Zweiten Weltkrieg übernahm der UN-Treuhandrat die Verwaltung der verbliebenen Mandatsgebiete. Als letzte ehemalige Kolonie wurde 1994 Palau unabhängig.

Die Beziehung zwischen den Deutschen und der indigenen Bevölkerung war durch rechtliche und soziale Ungleichheit gekennzeichnet. Es bestanden zwei Rechtskreise, deren Zugehörigkeit nach rassischen Kriterien festgelegt wurde. Die „weiße“, das heißt deutsche Bevölkerung in den Kolonien stellte eine kleine, stark privilegierte Minderheit dar. Ihr Verhältnis zur indigenen Bevölkerung überstieg selten die Ein-Prozent-Marke. 1914 lebten nicht mehr als 25.000 Deutsche in den Kolonien, etwas weniger als die Hälfte davon in Deutsch-Südwestafrika, das noch am ehesten als Siedlungskolonie galt. Sie genossen alle Vorteile des deutschen Rechts, europäischstämmige Ausländer waren ihnen rechtlich gleichgestellt.

Die rund 13 Millionen „Eingeborenen“ des deutschen Kolonialreichs, wie sie nach einer kaiserlichen Verordnung aus dem Jahr 1900 offiziell hießen, wurden nicht zu deutschen Staatsbürgern als die deutsche Staatsbürgerschaft erstmals 1913 eingeführt wurde, sie galten noch nicht einmal als Reichsangehörige, sondern lediglich als Untertanen oder Schutzbefohlene des Deutschen Reiches. Die deutschen Gesetze des Reiches galten für sie nur, wenn es per Verordnung extra festgelegt war. Insbesondere war ihnen der Rechtsweg verschlossen: Gegen Verfügungen der Kolonialbehörden und erstinstanzliche Urteile der Kolonialgerichte standen ihnen keinerlei rechtsstaatliche Mittel zur Verfügung. Für die Gerichtsorganisation siehe Gerichtsorganisation der ehemaligen deutschen Kolonien. Für die etwa 10.000 Menschen arabischer und indischer Abstammung, die in Deutsch-Ostafrika lebten, konnten die Gouverneure Sonderbestimmungen verfügen. Eine Aufnahme von „Eingeborenen“ in die Reichsangehörigkeit und auch deren Weitergabe an die Nachkommen war nach dem Schutzgebietsgesetz aber möglich. Nachdem sich zunehmend Liebesbeziehungen zwischen den Bevölkerungsgruppen ergeben hatten, verboten die Kolonien ab 1905 schrittweise „standesamtliche Eheschließung zwischen Weißen und Eingeborenen“. Außereheliche Sexualbeziehungen wurden von der Gesellschaft geächtet, um eine „Verkafferung“ zu unterbinden. 1912 debattierte der Reichstag über die Möglichkeit von Mischehen, mit dem Ergebnis, dass die Mehrheitsparteien von der Reichsregierung verlangten, Mischehen gesetzlich zu ermöglichen. Das Gesetz kam aber nie zustande. Die Verbote bestanden bis zum faktischen Ende des deutschen Kolonialreiches im Ersten Weltkrieg fort.

In der Vorstellungswelt der Deutschen bestand die indigene Bevölkerung aus „Kindern“: Menschen zwar, doch auf einer niedrigen Reifungsstufe, die man zu behüten, zu belehren und zu erziehen hatte. Für Belehrung und Missionierung sorgten die deutschen Missionsgesellschaften, die bereits ab den 1820er Jahren in Übersee tätig waren. Auf evangelischer Seite waren dies das Berliner Missionswerk, die Rheinische Mission, das Leipziger Missionswerk und die Norddeutsche Mission. Ihnen durften nach dem Abflauen des Kulturkampfes ab 1890 auch katholische Missionsgesellschaften an die Seite treten.
Diese Missionswerke errichteten in den Kolonien Stationen, in denen sie der indigenen Bevölkerung neben elementarer Bildung und Methoden moderner Landwirtschaft das Christentum näherbrachten. Dabei hatten sie großen Erfolg, da der Zusammenbruch der präkolonialen Gesellschaften, den die deutsche Landnahme und die nachfolgenden Kolonialkriege verursacht hatten, häufig auch eine spirituelle Krise mit sich gebracht hatten und die indigene Bevölkerung beim Gott der neuen Herren, der sich als der überlegene erwiesen zu haben schien, Trost und Halt suchten. Da die Missionare die Bekehrung der indigenen Bevölkerung zum Ziel hatten und den Anspruch hatten, ihr mit Nächstenliebe entgegenzutreten, sahen sie häufig Anlass, gegen deren grausame Behandlungen und Ausbeutung durch die Kolonialverwaltung und Plantagenbesitzer zu protestieren. Zur Selbstversorgung und als Mustergüter unterhielten sie aber oft selbst Plantagen und waren somit von der Arbeitskraft und -willigkeit der indigenen Bevölkerung abhängig; somit gerieten sie hier nicht selten in einen Zielkonflikt. Gegenüber den traditionellen Sitten und Gebräuchen der indigenen Bevölkerung zeigten sich die deutschen Missionare meist eher tolerant; selbst die in Afrika und der Südsee verbreitete Polygynie wurde oft geduldet. Einzig die an der Küste Ostafrikas dominierende islamische Kultur wurde von den Missionswerken bekämpft.
Aufgrund mangelnder Leistungsfähigkeit der Missionswerke und um keine Konflikte in muslimischen Gebieten zu provozieren, wurden in den deutschen Kolonien ab 1887 auch staatliche Schulen eingerichtet. Eine Schulpflicht bestand, anders als im Reich, aber nicht, auch um das Selbstbewusstsein der indigenen Bevölkerung nicht durch höhere Bildung zu stärken. Einige Fachschulen für Handwerk und Ackerbau wurden eingerichtet sowie als einzige Universität im deutschen Kolonialreich die Deutsch-Chinesische Hochschule in Tsingtau. Die staatlichen Elementarschulen unterschieden sich im Lehrplan deutlich von den Missionsschulen: Diese unterrichteten in der Muttersprache ihrer Schüler, also etwa auf Ewe oder Otjiherero, und erteilten bis zu 15 Stunden Religionsunterricht pro Woche, während auf jenen die Unterrichtssprache Deutsch war und nutzbare Fächer wie Rechnen dominierten.
Seit Mitte der 1890er Jahre errichteten die Deutschen in ihren Kolonien Lazarettstationen und Hospitäler, die allerdings zunächst nur Europäern offenstanden. „Eingeborenen-“ oder „Farbigenstationen“ wurden etwas später eingerichtet, doch wurde die Trennung zwischen den Rassen stets aufrechterhalten. Nicht zuletzt im eigenen Interesse legten die Deutschen besonderen Wert auf Bekämpfung und Prophylaxe von Tropenkrankheiten: Sümpfe wurden trockengelegt, Chinin gegen Malaria ausgegeben, gegen Pocken geimpft und Leprakranke wurden isoliert. Zur Bekämpfung von Seuchen fassten die Deutschen Erkrankte unterschiedlicher Ethnien und beiderlei Geschlechts in eigens dafür eingerichteten sogenannten „Konzentrationslagern“ zusammen, aus denen die Betroffenen wegen des damit verbundenen Freiheitsentzuges und der zum Teil schmerzhaften Untersuchungen, die dort vorgenommen wurden, immer wieder zu fliehen suchten. Um Mittel gegen die Schlafkrankheit zu erproben, unternahmen deutsche Mediziner auch Menschenversuche an erkrankten Afrikanern, die mitunter tödlich verliefen. Erfolge stellten sich vor allem bei der Bekämpfung von Pocken und Pest ein, während in der allgemeinen Hygiene und der Sozialmedizin noch große Rückstände herrschten: „Es gibt sehr wenig alte Neger“, klagte der Staatssekretär im Reichskolonialamt 1908. Erst gegen Ende der deutschen Herrschaft zeigten sich Ansätze, hier Abhilfe zu schaffen, etwa durch erste Arbeitsschutzverordnungen oder eine Verbesserung der Sanitätsaufsicht.

Als schwieriges Problem erwies es sich für die deutschen Kolonialherren, die indigene Bevölkerung zum Arbeiten zu bewegen. Da sie bislang in Subsistenz- und Naturalwirtschaft gelebt hatten, sahen Afrikaner und Polynesier oft nicht ein, warum sie mehr tun sollten als zum täglichen Lebensunterhalt erforderlich. Außerdem galt Landwirtschaft in vielen Landstrichen Afrikas als typische Frauenarbeit, von der die Männer sich fernhielten. Um der indigenen Bevölkerung diese „notorische Indolenz und Faulheit“ auszutreiben, verhängten die Deutschen Kopf- oder Hüttensteuern. Zur Beschaffung des zu deren Begleichung nötigen Geldes mussten Überschüsse erwirtschaftet werden, was nur durch Arbeit auf Plantagen möglich war. Wer nicht bezahlen konnte, wurde – oft weit von seinem Heimatdorf entfernt – zu Zwangsarbeit verurteilt.

Große Teile der indigenen Bevölkerung gerieten so in Unfreiheit. Die traditionelle Sklaverei wurde geduldet, weil vor allem in Ostafrika eine radikale Abschaffung den Zusammenbruch der lokalen Wirtschaftsstrukturen herbeigeführt hätte. Um 1900 waren etwa zehn Prozent der Bevölkerung Ostafrikas Sklaven im Besitz afrikanischer und arabischer Eliten, zu dem Sklavenhändler Tippu-Tip auf Sansibar unterhielt die deutsche Kolonialverwaltung freundschaftliche Beziehungen. Gleichzeitig galt die Sklaverei in den deutschen Kolonien offiziell als abgeschafft und die deutsche Propaganda hob dies als eine der Kulturleistungen des deutschen Kolonialismus hervor. Deshalb wurden andere Formen des Arbeitszwangs und der Unfreiheit gefunden, in denen die Mortalitätsraten hoch waren. Darunter fiel auch der Import von etwa 1.000 chinesischer Kulis nach Samoa, Neuguinea und Ostafrika, die gleichfalls häufig unter Zwang angeworben worden waren.

Bei der Zwangsarbeit und auch auf den Plantagen waren Körperstrafen an der Tagesordnung, die gemeinhin mit einer Nilpferdpeitsche verabreicht wurden. Dieses Instrument wurde in Deutschland als Symbol für die Behandlung der indigenen Bevölkerung durch mehrere Kolonialskandale bekannt: So hatte etwa der stellvertretende Gouverneur von Deutsch-Kamerun Heinrich Leist 1893 die Frauen von arbeitsunwilligen Afrikanern vor deren Augen auspeitschen lassen; die Männer waren zuvor aus der Sklaverei freigekauft worden, doch verweigerte Leist ihnen nun den Lohn, da ja durch den Freikauf bereits genug für sie bezahlt worden sei. Bereits im Jahr zuvor war bekanntgeworden, dass der Reichskommissar am Kilimandscharo Carl Peters seine afrikanische Konkubine und deren Liebhaber erst hatte auspeitschen und dann aufknüpfen lassen.

Die alltägliche Gewalt provozierte immer wieder Gegengewalt der indigenen Bevölkerung, die sich zum Teil in blutigen Aufständen und Kolonialkriegen niederschlug. Sowohl Peters’ als auch Leists Übergriffe hatten eine solche Folge gehabt. Die blutigsten Aufstände waren 1899–1901 der chinesische Boxeraufstand, 1905–1907 der Maji-Maji-Aufstand in Ostafrika und 1904–1908 der Aufstand der Herero und Nama in Südwestafrika, bei dem es zum ersten Völkermord des 20. Jahrhunderts kam. Staatssekretär Dernburg setzte 1907 eine großangelegte Kolonialreform ins Werk: Nunmehr solle mit „Erhaltungsmitteln“ anstelle von „Zerstörungsmitteln“ kolonisiert werden. Nicht mehr alkohol- und waffenhandelnde Kompanien sollten die Kolonialwirtschaft prägen, sondern der Missionar, der Arzt, die Eisenbahn und die Wissenschaft. Die Hüttensteuer wurde abgeschafft, die Enteignung von Land, das sich in indigenem Besitz befand, verboten und die Prügelstrafe wurde eingeschränkt.
Dernburgs Konzept blieb gleichwohl auf die größtmögliche Ausschöpfung der einheimischen Arbeitskräfte durch die Kolonialisten ausgerichtet. Der Erfolg war begrenzt: Zwar gingen die Prügel- und Rutenstrafen von 1905/06 auf 1907/08 deutlich zurück, stiegen danach aber wieder an und überstiegen 1912/13 mit über 8.000 gemeldeten Züchtigungen den Wert vor den dernburgschen Reformen deutlich. Die Dunkelziffer nicht gemeldeter Auspeitschungen auf den Plantagen wird noch erheblich höher gewesen sein.

Seit 1899 befanden sich alle „Schutzgebiete“, mit Ausnahme der Marshallinseln (seit 1906 auch diese), als Kolonien unter direkter Verwaltung des Reiches. An ihrer Spitze standen Gouverneure, denen Kanzler (zur Vertretung und Rechtspflege), Sekretäre und sonstige Beamte beigegeben waren.

Die Bezirke, die größten gebietsmäßigen Verwaltungseinheiten in einer Kolonie, wurden durch je einen Bezirksamtmann an der Spitze verwaltet. Den Bezirken unterstanden teilweise Bezirksnebenstellen. Dazu kamen Schutztruppen (in Deutsch-Ostafrika, Kamerun und Deutsch-Südwestafrika), militärisch organisierte Polizeitruppen und nach dem Vorbild der Konsulargerichte geschaffene Schutzgebietsgerichte. Die oberste Instanz war das Reichsgericht in Leipzig.

Kaiser-Wilhelms-Land, der Bismarck-Archipel, die Karolinen, Palau-Inseln und die Marianen (sowie seit 1906 die Marshallinseln einschließlich der Providence- und Brown-Inseln) wurden zu einem Gouvernement Deutsch-Neuguinea vereinigt.

Die oberste Leitung der „Schutzgebiete“ lag zwischen 1890 und 1907 in den Händen der Kolonialabteilung, die dem Reichskanzler unterstand. 1907 wurde diese zum Reichskolonialamt umgewandelt und Bernhard Dernburg zum Staatssekretär ernannt.
Schon der Kolonialabteilung wurde gemäß kaiserlichem Erlass vom 10. Oktober 1890 der Kolonialrat zur Seite gestellt, in dem Vertreter der Kolonialgesellschaften und vom Reichskanzler berufene Sachverständige vertreten waren.

Die Rechtslage in den Kolonien wurde erstmals 1886 mit dem Gesetz betreffend die Rechtsverhältnisse der deutschen Schutzgebiete genauer geregelt, das nach mehreren Änderungen ab 1900 als Schutzgebietsgesetz bezeichnet wurde. Es führte über den Umweg der Konsulargerichtsbarkeit deutsches Recht für Europäer in den deutschen Kolonien ein. Das Konsulargerichtsbarkeitsgesetz von 1879 erlaubte den deutschen Konsuln im Ausland unter bestimmten Bedingungen, die Gerichtsbarkeit über deutsche Staatsangehörige auszuüben. Das Schutzgebietsgesetz bestimmte nun, dass die Vorschriften zur Konsulargerichtsbarkeit entsprechend auch in den Kolonien angewendet werden sollten. Soweit sie für die Konsulargerichtsbarkeit relevant waren, wurden dadurch wichtige rechtliche Bestimmungen des bürgerlichen Rechts, des Strafrechts, der gerichtlichen Verfahren und der Gerichtsverfassung des Reichs auch für die deutschen Kolonien in Kraft gesetzt. Daneben wurden im Laufe der Zeit weitere spezielle kolonialrechtliche Bestimmungen erlassen. Für die indigenen Bevölkerungen der Kolonie hatte zunächst der Kaiser die Rechtssetzungsbefugnis. Im Laufe der folgenden Jahre konnten auch der Reichskanzler und von ihm ermächtigte Beamte Vorschriften erlassen, die zum Beispiel die Verwaltung, Gerichtsbarkeit oder Polizei regelten. In den deutschen Kolonien existierte somit von der Grundstruktur her eine duale Rechtsordnung die unterschiedliches Recht für die Europäer und die Indigenen vorsah. Die Gerichtsbarkeit über die indigene Bevölkerung, insbesondere in Strafrechtssachen, wurde den Kolonialbeamten übertragen, ohne dass in der Zeit der deutschen Kolonialherrschaft allerdings ein koloniales Strafrechts kodifiziert wurde. In nicht-strafrechtlichen Angelegenheiten wurden zudem indigene Autoritäten zur Gerichtsbarkeit über ihre Gemeinschaften ermächtigt, die nach dem lokalen Recht urteilen sollten.

Die Wirtschaft im deutschen Kolonialreich war ganz überwiegend vom Primärsektor geprägt. Verarbeitende Gewerbe wurden nicht aufgebaut, produziert wurden vielmehr Rohstoffe für den Export nach Europa. Dabei handelte es sich vor allem um landwirtschaftliche Produkte, wie Kautschuk, der von der um 1900 boomenden Fahrrad-, Auto- und Elektroindustrie nachgefragt wurde, Ölfrüchte, namentlich Palmöl und Kopra, die von der chemischen Industrie in Deutschland weiterverarbeitet wurden, Sisal und Baumwolle für die Textilherstellung, die große Palette der so genannten Kolonialwaren (Kaffee, Kakao, Zuckerrohr, Pfeffer, Tabak usw.), sowie Tierhäute, Felle und Elfenbein. 1908 wurde in Kamerun mit der Anpflanzung von Bananen für den Export begonnen. Manche dieser Produkte hatte Deutschland schon vor der Kolonialisierung aus diesen Gebieten importiert, wo sie ursprünglich in Sammelwirtschaft produziert und vor allem gegen Spirituosen eingetauscht worden waren. Hiermit hatten die Handelshäuser Woermann und Hansemann bereits vor 1884 gute Geschäfte gemacht. Neben der Landwirtschaft existierten auch Ansätze zur Gewinnung von Bodenschätzen durch Bergbau, von denen aber allein die Diamantengewinnung in Südwest-Afrika profitabel wurde.

Noch bevor diese Ressourcen von den Kolonialherren ausgebeutet werden konnten, hatte man mit dem Boden Profite zu machen gesucht. Ausgehend von der Rechtsfiktion der terra nullius, wonach die Gebiete, in die sie kamen, herrenlos wären, hatten die Kolonialgesellschaften große Teile der bewirtschaftbaren Fläche an sich gebracht und die indigene Bevölkerung auf weniger gutes Land oder in Reservationen verdrängt. Die so erworbenen riesigen Flächen namentlich Südwestafrikas wurden in Deutschland spekulativ gehandelt, ein Teil von ihnen wurde tatsächlich nie erschlossen. Auch diese fortlaufenden Enteignungen trugen zur Frustration der indigenen Bevölkerung bei und waren ein Grund für Rebellionen.

Nach der Erschließung des Landes boten sich drei Formen der landwirtschaftlichen Produktion an:
Zwischen Vertretern dieser drei Formen gab es in der gesamten Zeit des deutschen Kolonialreichs Konflikte: Einerseits wegen der Vertreibungen und Enteignungen, die die Anlage von Farmen und Plantagen auf gutem Boden mit sich brachte; andererseits wegen Profitmöglichkeiten, da die indigenen Bauern in direkter Konkurrenz zu Farmern und Plantagenbesitzern standen. Obwohl die Missionen zu Letzteren zählten, sprachen sie sich doch für ein indigenes Kleinbauerntum aus, um eine Proletarisierung zu verhüten, die mit einer Ausdehnung der Plantagen notwendig einherging.

Zur Verbesserung der Profitabilität der Kolonien setzte die Kolonialverwaltung auf die Förderung und Verbesserung der tropischen Landwirtschaft: Versuchs- und Lehrplantagen wurden errichtet, die auch der indigenen Bevölkerung offenstanden, außerdem wurde in den Usambara-Bergen das Biologisch-Landwirtschaftliche Institut Amani und im kamerunischen Victoria eine weitere landwirtschaftliche Forschungsstation errichtet.

Die deutschen Kolonien waren weitgehend ländlich geprägt. Die wenigen urbanen Gebiete lagen zumeist an den Hafenorten und Handelspunkten, vor allem an der ostafrikanischen Küste. Infrastrukturen im europäischen Sinne gab es aber kaum. In Togo bestand beispielsweise anfangs kein Hafen für Hochseeschiffe. Erst die Landungsbrücke in Lome schuf die Bedingungen für das sichere Be- und Entladen von europäischen Schiffen. Durch die kolonialen Eingriffe veränderten sich besonders an den Garnisonsorten und Verwaltungszentren die Siedlungsstrukturen. An der Küste von Südwestafrika entstanden mit Lüderitz und Swakopmund neue Städte. Ortschaften mit zuvor wenigen Tausend Einwohnern, etwa Daressalam, Tsingtau oder Windhoek, erlebten ein rasantes Bevölkerungswachstum. Auf die dadurch herbeigeführten sozialen und hygienischen Missstände reagierten die Verwaltungen mit Regeln zur Straßenführung und Bauordnung sowie einer Siedlungsverteilung nach rassischen Kriterien.

Etwa ab der Jahrhundertwende investierte das Deutsche Reich verstärkt in das Verkehrs- und Nachrichtenwesen der Kolonien. Bis dahin war der Schwerlastverkehr innerhalb Afrikas häufig durch menschliche Träger abgewickelt worden, da wegen der Tse-Tse-Fliege Ochsengespanne nicht einsetzbar waren. Ganze Dampfschiffe wurden, in Einzelteile zerlegt, bis zu ihrem Einsatzort auf den ostafrikanischen Seen von indigenen Trägern geschleppt. Bis 1914 wurden mehrere Tausend Schienenkilometer der Kolonialbahnen verlegt, um eine bessere Kontrolle über die Kolonien zu erlangen, und um Rohstoffe besser erschließen zu können (siehe auch: Liste der deutschen Kolonialbahnen). Es entstanden regelmäßige Schiffspassagen zwischen Europa und den Kolonien. Zur Sicherung der Seewege wurden Leuchttürme errichtet und Wetterstationen eingerichtet, die von der Deutschen Seewarte in Hamburg aus betrieben wurden.

Insgesamt blieb die Bilanz des Infrastrukturausbaus in den deutschen Kolonien aber bescheiden: Der Eisenbahnbau blieb auf wenige Strecken beschränkt, da es sowohl an Investoren als auch an Arbeitskräften fehlte und in Ermangelung einer standortnahen Stahlproduktion sämtliche Materialien aufwändig per Schiff herantransportiert werden mussten. In den Jahren vor dem Ersten Weltkrieg wurden Kolonialfunkstellen errichtet, um unabhängiger von internationalen Unterseekabeln zu werden. Seit 1912 wurde vom Deutsch-Südwestafrikanischen Luftfahrerverein und aus Mitteln der Nationalflugspende das Flugwesen in den deutschen Kolonien aufgebaut sowie die ersten Flugplätze geschaffen. Der Aufbau des Funk- und Flugnetzes konnte aber bis zum Ersten Weltkrieg nicht abgeschlossen werden.

Wirtschaftlich gesehen waren die deutschen Kolonien ein Verlustgeschäft. Lediglich die kleinsten und wirtschaftlich unbedeutendsten Kolonien Samoa und Togo erwirtschafteten in den letzten Jahren der deutschen Herrschaft einen geringen Überschuss. Alle anderen Kolonien hatten gegenüber dem Reich eine passive Handelsbilanz, das heißt der Wert der Güter, die aus Deutschland in diese Kolonien geliefert wurden (Konsumgüter für die Deutschen in den Kolonien, Textilien, Metallwaren, Alkohol und Waffen zum Tauschhandel mit der indigenen Bevölkerung, Investitionsgüter zum Aufbau der Infrastruktur), überstieg den Wert der Lieferungen aus den Kolonien nach Deutschland zum Teil drastisch. Hinzu kam, dass sich die Kolonien finanziell nicht selber trugen. Im Allgemeinen bildete jede Kolonie ein abgeschlossenes Zollgebiet mit einem eigenen Zolltarif. Der weitaus größte Teil der Zolleinnahmen kam aus den Einfuhrzöllen. Nur in Deutsch-Südwestafrika gab es dank der Diamantenexporte mehr Einnahmen aus den Ausfuhrzöllen. Weil die Steuer- und die Zolleinnahmen, die Deutschland mit den Kolonien erwirtschaftete, unter den Kosten für die Verwaltung und die Aufstandsbekämpfung blieben, waren die meisten deutschen Kolonien Zuschussprojekte der Reichskolonialverwaltung. Besonders teuer waren das aufstandsgeplagte Südwestafrika und das infrastrukturintensive Kiautschou. Ausnahmen waren wieder Togo und Samoa.
Mit dem Ende der Kolonialkriege und der neuen Kolonialpolitik seit 1907, dem allgemeinen Infrastrukturausbau und der Ausweitung der wirtschaftlichen Aktivitäten in den Schutzgebieten, verbesserte sich die finanzielle Lage der Kolonien erheblich und entwickelte sich hin zu einem Ausgleich von Einnahmen und Ausgaben. In den afrikanischen Kolonien betrug der Außenhandel 1904 an Einfuhren 40.672.000 Reichsmark und an Ausfuhren 20.821.000 Reichsmark. 1908 erreichten die Einfuhren 84.264.000 Reichsmark und die Ausfuhren 37.726.000 Reichsmark. 1912 führten die afrikanischen Schutzgebiete für 128.478.000 Reichsmark ein und für 103.748.000 Reichsmark aus. Die Entwicklung ist also deutlich absehbar.

In der Gesamtbilanz des deutschen Außenhandels spielten die Kolonien eine vernachlässigbare Rolle: Der Handelsverkehr mit ihnen machte 1914 nicht einmal 2,5 % des gesamten deutschen Außenhandels aus. Eine Förderung des Kolonialhandels erfolgte nicht, die Kolonien wurden als zollpolitisches Ausland behandelt. Der Import aus den Kolonien betrug nicht einmal ein halbes Prozent der gesamten deutschen Einfuhr. Die Produkte, die man aus den Kolonien ins Deutsche Reich importierte, deckten meist nur einen sehr geringen Teil des Inlandsbedarfs. Sie konnten die Stellung des Deutschen Reiches auf dem Weltmarkt, abgesehen von Kupfer und Diamanten aus Deutsch-Südwestafrika, weder stärken noch nachhaltig verändern. Die Kolonien bildeten daher keine Konjunkturstütze. Privatwirtschaftlich konnten einzelne Investoren, etwa die Deutsche Handels- und Plantagengesellschaft, die die Kopraausfuhr aus Neuguinea kontrollierte, jedoch große Gewinne verzeichnen.

Im Ergebnis des Vertrags von Versailles mussten alle Deutschen in den Kolonien das Land verlassen mit Ausnahme von Deutsch-Südwestafrika, in dem heute noch deutsche Siedler zu Hause sind (siehe auch Deutschnamibier).

Schon in der Frühphase der Weimarer Republik wurden Stimmen laut, die sich die Kolonien zurückwünschten, unter ihnen Konrad Adenauer, damals Bürgermeister von Köln. Adenauer war 1931–1933 Stellvertretender Präsident der Deutschen Kolonialgesellschaft. 1925 gründete sich die Dachorganisation Koloniale Reichsarbeitsgemeinschaft (Korag) aus der über diverse Zwischenschritte 1933 der Reichskolonialbund hervorging. Ebenfalls 1925 schuf der ehemalige Kolonialminister Johannes Bell die „Interfraktionelle koloniale Vereinigung“, der Parteimitglieder von der NSDAP bis zur SPD angehörten.

Die meisten Deutschen fühlten sich nicht schuldig im Sinne der Behauptungen im Versailler Vertrag, und viele sahen die Übernahme der Kolonien durch die Alliierten als Diebstahl an, vor allem nachdem der südafrikanische Premierminister Louis Botha ausnahmslos alle Behauptungen, die von den Alliierten während des Krieges über die Deutschen als Kolonialherren aufgestellt wurden, als haltlos und erfunden bezeichnete. Deutsche Kolonialrevisionisten sprachen von einer „Kolonialen Schuldlüge“.

Das Deutsche Reich unterstützte in den 1920er Jahren Kolonialunternehmen mit staatlichen Darlehen, und 1924 gelang mit staatlicher finanzieller Hilfe der Rückerwerb der meisten Pflanzungen in Kamerun. In Erwartung der Wiedererlangung der Kolonien wurde 1926 mit Unterstützung des Reiches die Koloniale Frauenschule Rendsburg gegründet. 1931 wurde an der Forstlichen Hochschule Tharandt das "Institut für ausländische und koloniale Forstwirtschaft" gegründet.

Nach der Machtübernahme der NSDAP wurden verschiedene Anstrengungen unternommen, die kolonialpolitischen Bestimmung des Versailler Vertrags zu revidieren und die Kolonien zurückzubekommen. Die NSDAP richtete 1934 ein eigenes Kolonialpolitisches Amt ein, das zunächst von Heinrich Schnee, dann von Franz Ritter von Epp geleitet wurde und eine rege Tätigkeit aufnahm. Zu einer erneuten Kolonialisierung in Übersee kam es jedoch nicht. Welche Rolle der Kolonialismus in der Politik Hitlers tatsächlich spielte, ist in der Forschung umstritten.

In der Politik der Nachkriegszeit spielten die ehemaligen deutschen Kolonien kaum noch eine Rolle. Jedoch forderten einzelne westdeutsche Politiker die Übernahme spät- bzw. postkolonialer Aufgaben, etwa in der Treuhandverwaltung von Tanganjika und Togo. Auch innerhalb der afrikanischen Freiheitsbewegung kam es im Rahmen der Dekolonisation vereinzelt zu entsprechenden Anregungen. Ende 1952 schlugen Vertreter der Ewe dem UN-Treuhandrat in einem Memorandum vor, Deutschland möge die durch Großbritannien und Frankreich verwalteten Landeshälften wieder vereinen und in die Unabhängigkeit führen (siehe auch Deutscher Togobund). Die Initiative wurde nicht aufgegriffen. Adolf Friedrich zu Mecklenburg, letzter deutscher Gouverneur Togos, nahm aber 1960 auf Einladung von Sylvanus Olympio als Ehrengast an der Unabhängigkeitsfeier teil.

Bestrebungen, den Kolonialkrieger-Bund nach dem Zweiten Weltkrieg wiederzubeleben, führten 1955 in Hamburg zur Gründung des „Verbandes ehemaliger Kolonialtruppen“, aus dem der heute noch existierende „Traditionsverband ehemaliger Schutz- und Überseetruppen“ hervorging.

Letzte Reste der schutzgebietsbezogenen Gesetzgebung überdauerten bis zum gesetzlichen Auslaufen der „Kolonialgesellschaften“ 1975 und steuerrechtlichen Anpassungen 1992 (siehe auch Kolonialrecht).

Vertreter der Volksgruppen der Herero und Nama, deren Vorfahren in den Jahren 1904 bis 1908 zu Zehntausenden in der deutschen Kolonie Deutsch-Südwestafrika, dem heutigen Namibia, getötet wurden, reichten in den USA Klage gegen Deutschland ein. Ein Bezirksgericht in New York gab im Januar 2017 einer Sammelklage gegen die deutsche Regierung statt. Die Klageschrift spricht von über 100.000 Todesopfern. Dieser Kolonialkrieg gilt als der erste Völkermord des 20. Jahrhunderts. Im März 2017 wurde außerdem bekannt, dass die Regierung in Windhoek eine Klage gegen Deutschland vor dem Internationalen Gerichtshof in Den Haag prüft. In diesem Zusammenhang war von einer Entschädigungssumme von 30 Milliarden Dollar die Rede.

Deutschland besitzt keine besonderen Beziehungen zu seinen ehemaligen Kolonien, mit Ausnahme von Namibia, in dem es die höchste Pro-Kopf-Entwicklungszusammenarbeit von Deutschland in Afrika gibt. Die Namibische Armee steht in enger Zusammenarbeit mit der deutschen Bundeswehr im Rahmen einer in Namibia stationierten Beratergruppe. Die deutsche Sprache existiert im Gegensatz zum Englischen und Französischen nicht mehr als Amtssprache in den ehemaligen Kolonien. Als Umgangssprache existiert noch die deutsche Sprache in Namibia, wo es circa 20.000 deutschsprachige Bewohner gibt. Ferner ging die deutsche Sprache vereinzelt in anderen Sprachen auf, etwa dem Unserdeutsch, das im Südpazifik nur noch wenige Menschen beherrschen. Deutschland kooperiert wirtschaftlich und kulturell mit vielen Ländern in Afrika, Asien und im Pazifik, unabhängig von seiner Kolonialgeschichte.

In den Jahren 1884 und 1885 schlossen deutsche Reisende in Südwest- und Ostafrika rechtlich zweifelhafte „Schutzverträge“ ab, die gleichwohl durch die Anerkennung der deutschen Reichsregierung offiziellen Status erlangten. Zudem ließ das Deutsche Reich auf Betreiben von Unternehmern Gebiete in Westafrika und im Pazifik direkt „unter Schutz stellen“. Mehreren Gebieten wurde dieser Status jedoch verwehrt oder bald wieder entzogen. Auch nach 1885 kam es zu Grenzabkommen und Gebietsabtretungen bzw. -verpachtungen, die im Falle von Kiautschou und Samoa weitere Kolonien begründeten.



Im Roten Meer:
In der Südsee:
Deutsche Spuren auf Antarktika:






</doc>
<doc id="8404" url="https://de.wikipedia.org/wiki?curid=8404" title="Friedensvertrag von Versailles">
Friedensvertrag von Versailles

Der Friedensvertrag von Versailles (auch "Versailler Vertrag, Friede von Versailles") wurde bei der Pariser Friedenskonferenz 1919 im Schloss von Versailles von den Mächten der Triple Entente und ihren Verbündeten bis Mai 1919 ausgehandelt. Mit der Unterzeichnung des Friedensvertrags endete der Erste Weltkrieg völkerrechtlich. Sie war zugleich der Gründungsakt des Völkerbunds.

Bereits am 11. November 1918 hatte der Waffenstillstand von Compiègne die Kampfhandlungen des Ersten Weltkriegs beendet, nicht aber den Kriegszustand. Die deutsche Delegation durfte an den Verhandlungen nicht teilnehmen, sondern konnte erst am Schluss durch schriftliche Eingaben wenige Nachbesserungen des Vertragsinhalts erwirken. Der Vertrag konstatierte die alleinige Verantwortung Deutschlands und seiner Verbündeten für den Ausbruch des Weltkriegs und verpflichtete es zu Gebietsabtretungen, Abrüstung und Reparationszahlungen an die Siegermächte. Nach ultimativer Aufforderung unterzeichnete Deutschland am 28. Juni 1919 den Vertrag unter Protest im Spiegelsaal von Versailles. Nach der Ratifizierung und dem Austausch der Urkunden trat er am 10. Januar 1920 in Kraft. Wegen seiner hart erscheinenden Bedingungen und der Art seines Zustandekommens wurde der Vertrag von der Mehrheit der Deutschen als illegitim und demütigend empfunden.

Zu den Unterzeichnern gehörten neben Deutschland die Vereinigten Staaten (USA), das Vereinigte Königreich, Frankreich, Italien, Japan sowie Belgien, Bolivien, Brasilien, Kuba, Ecuador, Griechenland, Guatemala, Haiti, Hedschas, Honduras, Liberia, Nicaragua, Panama, Peru, Polen, Portugal, Rumänien, das Königreich der Serben, Kroaten und Slowenen, Siam, die Tschechoslowakei und Uruguay.

China, das sich seit 1917 mit Deutschland im Krieg befand, unterzeichnete den Vertrag nicht.

Der Kongress der Vereinigten Staaten verweigerte dem Versailler Vertrag 1920 die Ratifikation. Die USA traten dem Völkerbund nicht bei und schlossen 1921 einen Sonderfrieden mit Deutschland, den "Berliner Vertrag".

Als weitere Pariser Vorortverträge mit den Verlierern folgten am 10. September 1919 der Vertrag von St. Germain mit Deutschösterreich, am 27. November 1919 der Vertrag von Neuilly-sur-Seine mit Bulgarien, am 4. Juni 1920 der Vertrag von Trianon mit Ungarn sowie am 10. August 1920 der Vertrag von Sèvres mit dem Osmanischen Reich.

Der Vertrag war das Ergebnis der Pariser Friedenskonferenz 1919, die im Schloss von Versailles vom 18. Januar 1919 bis zum 21. Januar 1920 tagte. Ort und Eröffnungsdatum waren nicht zufällig gewählt worden: 1871 hatten deutsche Würdenträger während der Belagerung von Paris die Kaiserproklamation im Spiegelsaal von Versailles vorgenommen. Dies verstärkte (neben vielen anderen Faktoren, zum Beispiel den hohen Reparationen Frankreichs an Deutschland) die deutsch-französische Erbfeindschaft und den französischen Revanchismus („Toujours y penser, jamais en parler“).

Vorangegangen war am 8. Januar 1918 das 14-Punkte-Programm von US-Präsident Woodrow Wilson, das aus deutscher Sicht Grundlage für den zunächst auf 36 Tage befristeten Waffenstillstand von Compiègne am 11. November 1918 war.

Vorab tagte ein engerer Ausschuss des Kongresses, der sogenannte "Rat der Vier", dem US-Präsident Woodrow Wilson, der französische Ministerpräsident Georges Clemenceau, der britische Premierminister David Lloyd George und der italienische Minister Vittorio Emanuele Orlando angehörten. Der Rat legte die wesentlichen Eckpunkte des Vertrags fest. An den mündlichen Verhandlungen nahmen nur die Siegermächte teil; mit der deutschen Delegation wurden lediglich Memoranden ausgetauscht. Das Ergebnis der Verhandlungen wurde der deutschen Delegation schließlich als Vertragsentwurf am 7. Mai 1919 vorgelegt – nicht zufällig am Jahrestag der Versenkung der "RMS Lusitania". Die deutsche Delegation – zu der auch die Professoren Max Weber, Albrecht Mendelssohn Bartholdy und Hans Delbrück sowie der General Max Graf Montgelas gehörten – weigerte sich zu unterschreiben und drängte auf Milderung der Bestimmungen, wobei die deutsche Delegation zu den mündlichen Verhandlungen nicht zugelassen wurde; stattdessen wurden Noten ausgetauscht. Zu den wenigen Nachbesserungen in der am 16. Juni von den Alliierten vorgelegten "Mantelnote" gehörte die Volksabstimmung in Oberschlesien. Die Siegermächte ließen weitere Nachbesserungen nicht zu und verlangten ultimativ die Unterschrift. Andernfalls würden sie ihre Truppen nach Deutschland einrücken lassen. Hierfür hatte Marschall Ferdinand Foch einen Plan ausgearbeitet: Vom bereits besetzten Rheinland aus sollten die Truppen der Entente entlang des Mains nach Osten vorrücken, um auf kürzestem Wege die tschechische Grenze zu erreichen und so Nord- und Süddeutschland voneinander zu trennen. In Kreisen um den Oberpräsidenten von Ostpreußen, Adolf von Batocki, den Sozialdemokraten August Winnig und General Otto von Below wurden Pläne entwickelt, die Friedensbedingungen rundweg abzulehnen und Westdeutschland den einrückenden Truppen der Siegermächte kampflos zu überlassen. In den preußischen Ostprovinzen, wo die Reichswehr noch verhältnismäßig stark war, sollte dann ein Oststaat als Widerstandszentrum gegen die Entente gegründet werden.
Ministerpräsident Philipp Scheidemann trat in dieser Situation zurück: Am 12. Mai 1919 begründete er seinen Schritt in der Weimarer Nationalversammlung mit der zum geflügelten Wort gewordenen Frage:
Unter dem Druck des drohenden Einmarsches und der trotz Waffenstillstand fortbestehenden britischen Seeblockade, die eine dramatische Zuspitzung der Ernährungslage befürchten ließ, votierte die Nationalversammlung am 22. Juni 1919 mit 237 gegen 138 Stimmen für die Annahme des Vertrags. Scheidemanns Parteifreund und Nachfolger Gustav Bauer rief in der Sitzung aus:
Außenminister Hermann Müller (SPD) und Verkehrsminister Johannes Bell (Zentrum) unterzeichneten daher – unter Protest – am 28. Juni 1919 den Vertrag.

Die Vertreter der USA, der wichtigsten Signatarmacht neben Großbritannien und Frankreich, hatten den Vertrag nach den zwei deutschen Delegierten zwar als Erste unterzeichnet, der amerikanische Kongress ratifizierte den Vertrag jedoch nicht. Am 19. November 1919 und nochmals am 19. März 1920 wurden das Vertragswerk und der Beitritt der Vereinigten Staaten zum Völkerbund abgelehnt. Die USA schlossen daher mit Deutschland den Berliner Vertrag vom 25. August 1921.

Zwei der wichtigsten Mächte aus der Zeit des Kriegsbeginns existierten nicht mehr:

Beide Kriegsparteien hatten sich Nationalitätenprobleme in gegnerischen Staaten zunutze gemacht: Die Mittelmächte hatten auf dem Gebiet des Zarenreiches Regentschaftspolen gegründet und die Gründung Litauens wohlwollend geduldet. Die Alliierten und die slawischen Minderheiten der Donaumonarchie hatten sich gegenseitig unterstützt und waren nun einander verpflichtet.

So war eine generelle Rückkehr zu den Vorkriegsgrenzen unmöglich und die Neuordnung mit jenen Problemen belastet, die die Grenzziehung zwischen Nationalstaaten unausweichlich mit sich bringt.

Die mit Abstand schwersten Kriegsschäden an der zivilen Infrastruktur hatten Frankreich und das von Deutschland überfallene Belgien zu verzeichnen.

Die Ziele Frankreichs, Großbritanniens und der Vereinigten Staaten unterschieden sich beträchtlich; die französischen standen vielfach im Widerspruch zu denen der beiden angelsächsischen Mächte.

Clemenceaus Mitarbeiter André Tardieu fasste die Ziele Frankreichs auf der Versailler Friedenskonferenz folgendermaßen zusammen:
Frankreich hatte mit dem Deutsch-Französischen Krieg und dem Ersten Weltkrieg zwei deutsche Invasionen innerhalb eines halben Jahrhunderts erlebt, von denen die erste für Deutschland erfolgreich gewesen war und die zweite weite Landstriche Frankreichs verwüstet hatte. Daher war es vorrangiges Ziel Clemenceaus, neben der als selbstverständlich angesehenen Rückgabe Elsass-Lothringens einen erneuten deutschen Einmarsch von vornherein unmöglich zu machen. Zu diesem Zweck strebte er die Rheingrenze und eine möglichst weitgehende Schwächung Deutschlands an. Dies ging einher mit seinem zweiten Ziel: der Entschädigung für die Kriegszerstörungen und der Abdeckung der interalliierten Schulden, die Frankreich vor allem bei den Vereinigten Staaten hatte. Eine vollständige Abdeckung aller Auslagen, die der Krieg gebracht hatte, schien durchaus geeignet, den gefährlichen Nachbarn nachhaltig zu schwächen.

Das Vereinigte Königreich hatte weit weniger unter dem Krieg gelitten als Frankreich, aber sich ebenfalls zur Finanzierung seiner Kriegsbeteiligung hoch bei der amerikanischen Regierung verschuldet. Nicht zuletzt angesichts der Entwicklung in Russland wollte die britische Regierung ein Machtvakuum in Mitteleuropa vermeiden und Deutschland daher im Sinne der klassischen Balance of Power-Strategie nicht zu sehr schwächen. Jedoch strebte die Regierung seiner Majestät eine nachhaltige Schwächung der deutschen Position in Übersee an, nachdem das Deutsche Kaiserreich zuletzt die jahrhundertelange Vormacht zur See des British Empire infrage gestellt hatte. Deutlich wird die britische Position in einem Memorandum vom Lloyd George vom März 1919:
Lloyd Georges finanzielle Forderungen sollten ursprünglich nur die britischen Kriegskosten decken. Die öffentliche Meinung in Großbritannien war durch den Krieg stark gegen Deutschland aufgebracht, was sich nicht zuletzt in den sogenannten Khaki-Wahlen vom 14. Dezember 1918 gezeigt hatte. Unter dem starken innenpolitischen Druck hatte Lloyd George eingewilligt, dass in die Reparationen, die Deutschland auferlegt wurden, auch der Wert sämtlicher Pensionen für Invalide und Kriegshinterbliebene einberechnet wurde, was die Höhe der Reparationsforderungen enorm steigen ließ.

Das Königreich Italien war sehr zögerlich und erst infolge des Londoner Geheimvertrags von 1915 und der darin in Aussicht gestellten territorialen Gebietsgewinne an der Seite der Triple Entente in den Krieg eingetreten, nutzte aber die Chance, mit dem Sieg die letzten „Irredenta“-Gebiete Trentino und Triest dem italienischen Staatsgebiet anzufügen, darüber hinaus eine leicht zu verteidigende Nordgrenze am Brenner zu gewinnen und eine Kolonie (Dodekanes). Italienische Forderungen gingen folglich im Wesentlichen in die Vertragstexte von Saint-Germain-en-Laye und Sèvres ein.

Amerikanische Kriegsziele waren die Aufhebung sämtlicher Handelsbeschränkungen und die Freiheit der Seeschifffahrt, deren Verletzung durch Deutschlands uneingeschränkten U-Boot-Krieg der Anlass zum Kriegseintritt der USA gewesen war. Darüber hinaus strebte Präsident Wilson eine gerechte Friedensordnung an, die einen weiteren Weltkrieg unmöglich machen sollte. Die Skizze einer solchen Friedensordnung, die auch die anderen amerikanischen Kriegsziele enthielt, hatte er im Januar 1918 mit seinem Vierzehn-Punkte-Programm veröffentlicht. Postuliert wurde darin unter anderem das Verbot jeglicher Geheimdiplomatie, ein Selbstbestimmungsrecht der Völker, eine weitgehende Abrüstung, ein Völkerbund, der Rückzug der Mittelmächte aus allen besetzten Gebieten und die Wiederherstellung Polens, das einen Zugang zum Meer erhalten sollte. Diese Forderungen waren teilweise nicht vereinbar; an der Ostseeküste gab es damals nirgends eine polnische Bevölkerungsmehrheit, weshalb der später im Versailler Vertrag geschaffene polnische Korridor zur Ostsee gegen das Selbstbestimmungsrecht der Völker verstieß. Auf Grundlage dieser Forderungen strebte Wilson einen Verständigungsfrieden ohne Sieger und Besiegte an, rückte aber nach dem deutschen „Diktatfrieden“ von Brest-Litowsk davon ab.

Im Artikel 231 heißt es:

Der Vertrag wies allein dem kaiserlichen Deutschen Reich und seinen Verbündeten die Verantwortung für den Ersten Weltkrieg zu. Er bedeutete eine anfängliche Isolation des Deutschen Reiches, das sich als Sündenbock für die Verfehlungen der anderen europäischen Staaten vor dem Weltkrieg sah.

Der Artikel wurde als einseitige Schuldzuweisung verstanden und führte zur Kriegsschulddebatte. Die Unterschriften durch Hermann Müller und Johannes Bell, die durch die Weimarer Nationalversammlung 1919 in ihre Ämter gelangt waren, nährten die vor allem durch Paul von Hindenburg und Ludendorff sowie später von Adolf Hitler propagierte Dolchstoßlegende.

Historiker beurteilen die Ursachen des Ersten Weltkriegs heute differenzierter, als es in dem Vertrag ausgedrückt wird. Der Artikel 231 sollte nicht die historischen Ereignisse bewerten, sondern die für das Deutsche Reich nachteiligen Friedensbedingungen juristisch und moralisch legitimieren. Darüber hinaus sollte das Deutsche Reich finanziell für die Schäden an Land und Menschen haftbar gemacht werden, welche die kaiserlichen Truppen insbesondere in Frankreich angerichtet hatten. Der Vertrag von Versailles legte daher den Grund für die Reparationsforderungen an das Deutsche Reich, deren Höhe allerdings zunächst nicht festgelegt wurde. Die Vertreter des Deutschen Reiches protestierten gegen den Artikel 231 daher nicht bloß aus Gründen der Selbstrechtfertigung, sondern mit dem Ziel, die moralische Basis der gegnerischen Forderungen insgesamt zu unterminieren. Die deutschen Reparationen nach dem Ersten Weltkrieg belasteten den neuen republikanischen Staat; sie waren eine von mehreren Ursachen der Inflation der folgenden Jahre bis 1923.

Das Reich musste zahlreiche Gebiete abtreten: Nordschleswig an Dänemark, den Großteil der Provinzen Westpreußen und Posen sowie das oberschlesische Kohlerevier und kleinere Grenzgebiete Schlesiens und Ostpreußens an den neuen polnischen Staat, die Zweite Republik. Außerdem fiel das Hultschiner Ländchen an die neu gebildete Tschechoslowakei. Im Westen ging das Gebiet des Reichslandes Elsaß-Lothringen an Frankreich, und Belgien erhielt das Gebiet Eupen-Malmedy mit einer ebenfalls überwiegend deutschsprachigen Bevölkerung. Insgesamt verlor das Reich 13 % seines vorherigen Gebietes und 10 % der Bevölkerung. Darüber hinaus wurde der gesamte reichsdeutsche Kolonialbesitz dem Völkerbund unterstellt, der ihn als Mandatsgebiete an interessierte Siegermächte übergab. Das Deutsche Reich musste die Souveränität Österreichs anerkennen. Der von Deutschösterreich angestrebte Zusammenschluss mit dem Deutschen Reich wurde im Artikel 80 des Versailler Vertrags untersagt. Dieses Anschlussverbot fand sich ebenfalls in Artikel 88 des Vertrags von Saint-Germain.






Nach Artikel 91 des Versailler Vertrags erwarben grundsätzlich alle deutschen Reichsangehörigen, die ihren Wohnsitz in den endgültig als Bestandteil des wiedererrichteten polnischen Staates anerkannten Gebieten hatten, von Rechts wegen die polnische Staatsangehörigkeit unter Verlust der deutschen. Zwei Jahre lang nach Inkrafttreten des Vertrags waren die hier wohnhaften über 18 Jahre alten deutschen Reichsangehörigen berechtigt, für die deutsche Staatsangehörigkeit zu optieren. Polen deutscher Reichsangehörigkeit im Alter von über 18 Jahren, die in Deutschland ihren Wohnsitz hatten, waren berechtigt, für die polnische Staatsangehörigkeit zu optieren. Allen Personen, die von dem Optionsrecht Gebrauch machten, stand es frei, innerhalb von zwölf Monaten ihren Wohnsitz in den Staat zu verlegen, für den sie optiert hatten. Sie durften dabei ihr gesamtes bewegliches Gut zollfrei mitnehmen. Es stand ihnen frei, das unbewegliche Gut zu behalten, das sie im Gebiete des anderen Staates besaßen, in dem sie vor der Option wohnten.

Diese Bestimmungen erzeugten in den ersten Jahren nach der Transformation in innerstaatliches Recht eine nicht unerhebliche Wanderungsbewegung zwischen dem Deutschen Reich und Polen. Viele Deutsche, die die deutsche Reichs- und Staatsangehörigkeit nicht verlieren wollten und entsprechend optiert hatten, sahen sich gezwungen, ihre angestammte Heimat zu verlassen und auch ihren Grundbesitz zu verkaufen, um sich im Reich wieder eine Existenz aufzubauen. Polen sah die in den Nachkriegswirren vorübergehend Abgewanderten als stillschweigende Optanten an, auch wenn diese Deutschen sich noch nicht für oder gegen die deutsche Staatsangehörigkeit entschieden hatten. Das dadurch erhöhte Angebot auf dem polnischen Grundstücksmarkt führte zu fallenden Preisen der Grundstücke und zu Vermögensverlusten bei den Verkaufenden.

Als Folge des Wiener Abkommens emigrierten zwischen 1924 und dem Sommer 1926 etwa 26.000 Deutsche teils freiwillig, teils erzwungen aus dem neuen polnischen Staat. Das Deutsche Reich war für die Aufnahme dieser Menschen schlecht vorbereitet. Die meisten wurden zunächst in einem Lager bei Schneidemühl aufgefangen.

In der Präambel zum fünften Teil des Vertrages wurde erklärt, dass sich Deutschland, „um den Anfang einer allgemeinen Beschränkung der Rüstungen aller Nationen zu ermöglichen“, zur genauen Befolgung der nachstehenden Bestimmungen über die Land- See- und Luftstreitkräfte verpflichtet.


Artikel 177 des Vertrages verlangte die Entwaffnung auch im zivilen Bereich. Der Deutsche Reichstag beschloss in der Folge am 5. August 1920 (damals regierte das Kabinett Fehrenbach) mehrheitlich das Entwaffnungsgesetz.

Das Deutsche Reich wurde zur Wiedergutmachung durch Geld- und Sachleistungen in noch durch die Reparationskommission festzulegender Höhe verpflichtet. Eine erste Rate von 20 Milliarden Goldmark war bis April 1921 zu zahlen. Außerdem wurde eine Verkleinerung der reichsdeutschen Handelsflotte festgeschrieben. Die großen deutschen Schifffahrtswege, namentlich Elbe, Oder, Donau und Memel, wurden für international erklärt. Für fünf Jahre musste das Deutsche Reich den Siegermächten einseitig die Meistbegünstigung gewähren. Im sogenannten Champagnerparagraphen 274 wurde festgelegt, dass Produktbezeichnungen, die ursprünglich Herkunftsbezeichnungen aus den Ländern der Siegermächte waren, nur noch verwendet werden durften, wenn die so bezeichneten Produkte auch tatsächlich aus der genannten Region stammten: Seitdem darf Branntwein in Deutschland nicht mehr als Cognac und Schaumwein nicht mehr als Champagner verkauft werden – Bezeichnungen, die bis dahin in den deutschen Ländern durchaus üblich waren. Luxemburg musste die bislang bestehende Zollunion mit dem Deutschen Reich aufgeben.

Außerdem sah der Vertrag die Gründung des Völkerbunds vor, eines der erklärten Ziele von Präsident Wilson. Der Völkerbund war Vorläuferorganisation der heutigen Vereinten Nationen, die nach dem Zweiten Weltkrieg gegründet wurden. Deutschland war bis 1926 kein Mitglied.

Ebenso wurde durch den Versailler Vertrag (Kapitel XIII) die Internationale Arbeitsorganisation (ILO) ins Leben gerufen, welche bis heute besteht. Auch die Regelungen über diese Organisation sind in allen Pariser Vororteverträgen enthalten und heben Problemstellungen der Arbeitswelt erstmals auf die Stufe des internationalen Rechtssystems. Der Versailler Vertrag geht somit über die Regelungen klassischer Friedensverträge hinaus.

Als Garantie für die Durchführung der übrigen Bestimmungen des Vertrags wurde eine alliierte Besetzung des linksrheinischen Gebietes und zusätzlicher Brückenköpfe bei Köln, Koblenz und Mainz vereinbart. Diese sollte zeitlich gestaffelt 5, 10 und 15 Jahre nach dem Ratifizierungsdatum aufgehoben werden (Artikel 428–430).

Das Deutsche Reich wurde durch die territorialen Abtretungen in seiner Wirtschaftskraft erheblich geschwächt. Große Teile seiner Schwerindustrie wurden getroffen. Es verlor 80 % seiner Eisenerzvorkommen, 63 % der Zinkerzlager, 28 % seiner Steinkohleförderung und 40 % seiner Hochöfen. Der Verlust Posens und Westpreußens verringerte die landwirtschaftliche Nutzfläche um 15 %, die Getreideernte um 17 % und den Viehbestand um 12 %. Die deutsche Landwirtschaft konnte diesen Verlust zunächst nicht ausgleichen. Deutschlands Bevölkerung verringerte sich um sieben Millionen Menschen (11 %), von denen in den Folgejahren etwa eine Million ins Reich strömte, vor allem aus Elsass-Lothringen und aus den an Polen abgetretenen Gebieten. Durch den Verlust von 90 % der Handelsflotte und des gesamten Auslandsvermögens wurde der deutsche Außenhandel stark beeinträchtigt.

Da das Deutsche Reich seine Armee nach Art. 159 ff. Versailler Vertrag auf eine Stärke von 115.000 Soldaten (100.000 Heer und 15.000 Marine) verkleinern musste, war es nicht in der Lage, eine etwaige alliierte Invasion militärisch zu verhindern. Bereits 1921 drohten die Siegerstaaten im Londoner Ultimatum mit einer Besetzung des Ruhrgebiets; 1923 wurde es dann von französisch-belgischen Truppen tatsächlich besetzt (→ Ruhrbesetzung).

Verschiedene Historiker bezeichneten es als ein Grundproblem des Versailler Vertrages, dass er zwei Ziele gleichzeitig zu erreichen versuchte: zum einen die von Wilson vertretenen Ideale der Selbstbestimmung der Völker und der territorialen Übereinstimmung zwischen Volk und Staat, zum anderen die Absichten der Siegermächte, insbesondere Frankreichs, das Deutsche Reich entscheidend zu schwächen. 

Sebastian Haffner schrieb nach dem Zweiten Weltkrieg, das Deutsche Reich als immer noch stärkste und geographisch in der Mitte beheimatete, also für die Stabilität des Kontinents unentbehrliche europäische Macht sei "„weder dauerhaft entmachtet noch dauerhaft integriert“" worden.

Der Vertrag von Versailles - gelegentlich „"Karthagischer Friede"“ genannt - war für Deutschland zu hart, als dass ein als politische Einheit und wirtschaftliche Großmacht bestehen gebliebenes Deutsches Reich ihn dauerhaft akzeptieren würde. Gleichwohl ließ er es mächtig genug, dass eine deutsche Regierung weniger als 20 Jahre später Revanchegedanken in Politik umsetzen konnte, womit sie Europa in die Katastrophe des Zweiten Weltkriegs stürzte. Marschall Foch äußerte zur Zeit des Vertragsabschlusses: "„Das ist kein Frieden. Das ist ein zwanzigjähriger Waffenstillstand.“" – Foch war für eine Zerschlagung des Deutschen Reiches eingetreten.

John Maynard Keynes, der Vertreter des Schatzamts der britischen Delegation bei den Vertragsverhandlungen, trat noch vor Abschluss der Verhandlungen unter Protest gegen die Vertragsbedingungen, die Deutschland auferlegt werden sollten, von seinem Posten in der Delegation zurück. Die wirtschaftlichen Folgen des Friedensvertrages würden sowohl die internationalen Wirtschaftsbeziehungen destabilisieren als auch größeren sozialen Sprengstoff für Deutschland mit sich führen.

Die Friedensbedingungen wurden in Deutschland als überraschend und als extrem hart empfunden. Lange hatte die deutsche Öffentlichkeit geglaubt, auf der Grundlage der wilsonschen Vierzehn Punkte einen milden Frieden erreichen zu können, der im Wesentlichen den "Status quo ante" wiederherstellen würde. Der Kulturphilosoph Ernst Troeltsch schrieb, Deutschland habe sich im „Traumland der Waffenstillstandsperiode“ befunden, aus dem es mit der Veröffentlichung der Friedensbedingungen brutal geweckt worden sei. Hinzu kam die Tatsache, dass die Siegermächte das Deutsche Reich von den Verhandlungen ausgeschlossen und ihm nur am Schluss schriftliche Eingaben gestattet hatten: Das Schlagwort vom „Versailler Diktat“ machte die Runde. Diese beiden Faktoren trugen dazu bei, dass der Widerstand der Reichsregierung gegen den Vertrag, wie der Historiker Hans-Ulrich Wehler schreibt, „von einem nahezu lückenlosen Konsens im ganzen Land“ getragen wurde. In den folgenden Jahren war der Revisionismus dieses Vertrages erklärtes Ziel der deutschen Außenpolitik: Weder die "Legitimität des Friedens" noch die Tatsache, dass Deutschland den Krieg militärisch verloren hatte (→ Dolchstoßlegende), wurden akzeptiert. Auf unterschiedlichen Wegen versuchten alle Regierungen der Weimarer Republik, die „Fesseln von Versailles abzuschütteln“, weshalb man von einem regelrechten „Weimarer Revisionssyndrom“ sprechen kann. Neben der Art seines Zustandekommens und den Inhalten des Vertrages – insbesondere auch die Gebietsabtretungen mit deutschen Bevölkerungsgruppen – beschädigte dieses Revisionssyndrom nachhaltig das Ansehen der demokratischen Westmächte und das Vertrauen in die neue Demokratie in Deutschland. Manche Historiker sehen in dem Vertrag eine wichtige Ursache für den Aufstieg des Nationalsozialismus. So äußerte Theodor Heuss, damals liberaler Reichstagsabgeordneter, 1932 in seiner Schrift "Hitlers Weg": „Der Ausgangspunkt der nationalsozialistischen Bewegung ist nicht München, sondern Versailles.“

Auf die hohen Reparationsforderungen und die Industriedemontagen im Ruhrgebiet versuchte die deutsche Reichsregierung mit einem Generalstreik zu reagieren, der mit ständig nachgedrucktem Geld unterstützt werden sollte. Das heizte die Inflation zu einer Hyperinflation an, die große Teile der Bevölkerung in Not und Elend stürzte. Sie war vor allem dadurch zustande gekommen, dass den Kriegsanleihen, mit denen das Kaiserreich vorher den Krieg finanziert hatte, durch die militärische Niederlage keine Sachwerte gegenüberstanden. Während und nach der Inflation geriet das Reich in eine zunehmende Abhängigkeit von ausländischen Krediten, besonders US-amerikanischen. Die von den USA ausgehende Weltwirtschaftskrise traf das Deutsche Reich extrem hart, da seine Volkswirtschaft stärker als andere mit der US-Wirtschaft verwoben war.

Die durch den Versailler Vertrag begründeten bedeutsamen wirtschaftlichen Folgen und die außenpolitische Isolation des Deutschen Reichs versuchte Walther Rathenau im Vertrag von Rapallo zu entschärfen. Darin wurde das Verhältnis zur Sowjetunion normalisiert und auf gegenseitige Ansprüche verzichtet.

Hitler konnte in den ersten Jahren seiner Regierungszeit durch die Beseitigung der letzten Zwänge des Versailler Vertrags, unter anderem durch die militärische Wiederaufrüstung und Wiederbesetzung des Rheinlandes, großes innenpolitisches Prestige ernten. Die USA zogen sich alsbald von der europäischen Politik zurück; Frankreich und Großbritannien entschieden sich für eine Politik des Appeasement.

Neben dem hier erläuterten Friedensvertrag von Versailles existiert noch ein weiterer weniger bekannter Pariser Vorortvertrag mit gleichem Namen. So wird der polnische Minderheitenvertrag vom 28. Juni 1919 als „der kleine Vertrag von Versailles“ bezeichnet. Dabei handelt es sich um den ersten völkerrechtlichen Vertrag mit konkret ausgearbeiteten Schutzrechtbestimmungen für nationale Minderheiten.




</doc>
<doc id="8407" url="https://de.wikipedia.org/wiki?curid=8407" title="Great Barrier Reef">
Great Barrier Reef

Das Great Barrier Reef (deutsche Bezeichnungen: "(Großes) Barriereriff", "Großes Barrierriff" oder Great-Barrier-Riff) vor der Nordostküste Australiens im Korallenmeer ist das größte Korallenriff der Erde. Im Jahr 1981 wurde es von der UNESCO zum Weltnaturerbe erklärt und wird auch als eines der sieben Weltwunder der Natur bezeichnet. Es besteht aus über 2.500 einzelnen Riffen. 

Die Existenz des Great Barrier Reefs sowie seiner Biodiversität ist durch die menschengemachte globale Erwärmung sowie die dadurch ausgelöste Ozeanversauerung erheblich bedroht. Zwischen 1985 und 2012 ging die Korallenbedeckung von 28 auf 13,8 % zurück; ein weiterer Rückgang auf 5–10 % binnen 10 Jahren gilt infolge des unbegrenzten Kohlenstoffdioxidausstoßes als wahrscheinlich. Wenn Korallenriffe wie das Great Barrier Reef auch in Zukunft weiter existieren sollen, sind sehr schnell wirksame Klimaschutzmaßnahmen für eine rasche Bekämpfung der globalen Erwärmung notwendig.

Das Great Barrier Reef liegt nordöstlich von Australien an der Ostküste des Bundesstaates Queensland im Südpazifik und erstreckt sich von der Torres-Straße vor Papua-Neuguinea bis zur Lady-Elliot-Insel, die etwa 75 Kilometer nordöstlich von Bundaberg liegt. Es ist inzwischen auf eine Länge von gut 2.300 Kilometern angewachsen und erreicht damit eine Ausdehnung vom 10. bis zum 24. südlichen Breitengrad. Entdeckt wurde es am 11. Juni 1770 durch den britischen Seefahrer James Cook, als er während seiner ersten Südseereise (1768–1771) dort mit seinem Schiff "HMS Endeavour" auf Grund lief.

Aufgrund der Dimension ist das Riff zur besseren Unterscheidung in mehrere Sektionen (Abschnitte) aufgeteilt. Diese lauten von Nord nach Süd:

Das Riff verläuft am östlichen Rand des australischen Kontinentalsockels. Es liegt zwischen 30 Kilometern (bei Cairns) und rund 250 Kilometern (bei Gladstone) von der fast parallel verlaufenden australischen Ostküste entfernt.

Es besteht aus einer Kette von über 2.900 Einzelriffen, etwa 1.000 Inseln, wie z. B. den Whitsunday Islands oder Dunk Island, und unzähligen Sandbänken. Die Fläche des Great Barrier Reef beträgt etwa 347.800 km². Es kann mit bloßem Auge vom Weltraum aus gesehen werden.

Das Riff liegt komplett in den Tropen und im Taifun-Gebiet. Die Taifun-Saison dauert von Oktober bis März, regenreich ist es besonders im Herbst (Februar /März). Die monatliche Niederschlagsmenge reicht dann von 215 mm in Rockhampton im Süden des Riffs über 350 mm in Mackay und Townsville bis zu 525 mm in Cairns und auf der Kap-York-Halbinsel. Trocken ist es in diesem Gebiet zwischen März und Oktober.

Der Ursprung des Great Barrier Reefs liegt rund 600.000 Jahre zurück. Es ist insgesamt betrachtet kein geschlossenes Riffsystem, sondern setzt sich aus einer Vielzahl verschiedener Typen von Einzelriffen mit unterschiedlicher Entstehungsgeschichte zusammen.

Der nördliche – von der Torres-Straße bis ungefähr Cooktown reichende – Abschnitt des Great Barrier Reef entstand erst, nachdem sich die Nordspitze Australiens im Zuge der Kontinentalverschiebungen vor etwa 15 Millionen Jahren in tropische Breitengrade vorschob. Erst die dort vorherrschenden Bedingungen und die höhere Wassertemperatur ermöglichten das Ansiedeln von riffbildenden Steinkorallen-Polypen auf dem nahe der Küste gelegenen Kontinentalsockel.
Die Riffbildung durch die kalkabsondernden Steinkorallen war jedoch kein kontinuierlicher Prozess. Als Folge der Eiszeiten und des damit verbundenen Absinkens des Meeresspiegels trockneten die besiedelten Gebiete mehrfach aus und die lebenden Korallenpolypen starben ab. Zurück blieben jeweils Erhebungen durch Küstenkalksteine, die im Laufe der Zeit durch Sedimentation entstanden waren. Während der ebenso regelmäßig wiederkehrenden Überflutungen – verursacht durch die auf jede Eiszeit folgenden Eisschmelzen – siedelten sich die Korallen auf dem verbliebenen Sedimentgestein immer wieder erneut an und schufen weitere Kalkberge, die der Nachfolgegeneration jeweils als Fundament dienten.

Durch diese Prozesse entstand im Laufe der Zeit im nördlichen Teil des Riffs ein relativ zusammenhängendes Gebilde von Korallenriffen, das im Wesentlichen aus Riffen des Typs "Barriere-Riff" besteht. Durch die fortwährende Meeresboden-Absenkung bzw. den Meeresspiegelanstieg wächst diese Riffbarriere auf dem Kontinentalsockel seewärts. Zur Küstenseite ist das Riff durch eine breite und zwischen 50 und 100 Meter tiefe, mit kleineren Riffen und Korallenbänken durchzogene Lagune vom Festland getrennt. Auf der vom Festland abgewandten Seite – dem "Outer Reef" – fällt das Barriereriff, bzw. der Kontinentalabhang, an dem das Riff liegt, teilweise bis in 2.000 Meter Tiefe zum Meeresboden hin steil ab.

Die mittleren und südlichen Teile des Riffs sind später entstanden. Die einzelnen Barriereriffe sind dort in wesentlich weniger kompakten Formationen angeordnet. In diesem Abschnitt des Riffs haben sich vermehrt "Saum-Riffe" (engl.: fringing reef) gebildet, die den Barriereriffen sehr ähnlich sind und sich ebenfalls überwiegend seewärts ausbreiten. Sie wachsen aber nicht am Rand des Kontinentalsockels, sondern in direkter Küstennähe und haben zum Festland hin meist nur eine wenige Meter tiefe Lagune, die durch Erosion entstanden ist.

Große Gebiete des Riffs gehörten früher zum Festland und wurden ebenfalls durch den steigenden Meereswasserspiegel überflutet. So sind auch die meisten zum Riff gehörenden Inseln die Spitzen versunkener Berge. Sie stammen von einem Küstengebirge, welches während der letzten Eiszeit vom Festland abgetrennt wurde. Sie bestehen im Gegensatz zu echten Koralleninseln aus festem Gestein und sind meist von einem schmalen Saumriff umgeben, welches unmittelbar an deren Ufer wächst.

Auf den ebenen Anhöhen der versunkenen Gebiete – die nun ebenfalls dem Kontinentalsockel zugerechnet werden – haben sich "Plattform-Riffe" (engl.: platform reef) unterschiedlicher Größe gebildet. Sie sind auf jenen Flächen entstanden, wo der Meeresboden so weit an den Meeresspiegel heranreichte, dass der Lichteinfall stark genug war, um eine Ansiedlung von Korallen zu ermöglichen. Einige Plattformriffe erreichen inzwischen einen Durchmesser von bis zu 15 Kilometern.

Die sichtbaren Teile der farbenprächtigen Korallenriffe stammen aus der letzten Kaltzeit, die etwa 10.000 Jahre zurückliegt. Von diesem Zeitpunkt an konnten sich die unterschiedlichen Korallenarten wieder auf den verbliebenen Kalksteinen ansiedeln und zum einzigartig vielfältigen Lebensraum unzähliger Tierarten und Pflanzen werden. Auch in der Zukunft wird das Riff sowohl sein Aussehen als auch seine Ausdehnung verändern.

Das Great Barrier Reef bildet mit seinen 359 Steinkorallenarten die größte von Lebewesen geschaffene Struktur auf der Erde. Es bietet Lebensraum für eine Vielzahl von weiteren Arten; unter anderem sind dort 80 Arten von Weichkorallen und Seefedern, über 1.500 Fischarten, 1.500 Schwammarten, 5.000 Arten von Weichtieren, 800 Arten von Stachelhäutern wie zum Beispiel Seesternen, 500 verschiedene Arten von Seetang und 215 Vogelarten heimisch.

Man findet im Great Barrier Reef sechs von insgesamt sieben weltweit vorkommenden Arten von Meeresschildkröten. Darunter sind auch die vom Aussterben bedrohten unechten Karettschildkröten und die pazifischen Suppenschildkröten, die das Riff zur Eiablage nutzen. Ebenfalls vom Aussterben bedroht sind die dort lebenden Dugongs (Seekühe). Ferner nutzen – neben weiteren dort vorkommenden Walarten – die nahe der Antarktis lebenden Buckelwale die warmen Gewässer zum Gebären ihrer Jungen.

Die zackigen und verästelten Korallenstöcke bieten Schnecken und seltenen Muscheln, wie zum Beispiel "Arthritica"-Arten oder Stachelschnecken, wie etwa "Murex pecten", ideale Bedingungen.

Korallenriffe sind sehr empfindliche Ökosysteme. Jede Veränderung kann unvorhersehbare Schäden verursachen.

Korallen können nur in einem klaren, sonnendurchfluteten Gewässer mit sehr eingeschränktem Temperaturbereich, etwa zwischen 18 und 30 °C, gedeihen und überleben. Eine Erhöhung der Wassertemperatur führt zum Abstoßen und anschließenden Absterben der Zooxanthellen, dem lebensnotwendigen Algenbewuchs der Korallen. Da diese Algen neben der symbiotischen Versorgung mit Nährstoffen auch für die Farbgebung der Koralle zuständig sind, verbleicht der Korallenstock, und das weiße Kalkgerüst wird sichtbar. Hält dieser Zustand über längere Zeit an, weil die Algen aufgrund konstanter Wassererwärmung nicht nachwachsen können, sterben die Korallen an Nährstoffmangel.

Alarmiert wurde die Weltöffentlichkeit, als am 3. April 2010 der chinesische Massengutfrachter MS "Shen Neng 1" mit einer Ladung von 65.000 Tonnen Kohle und 950 Tonnen Öl an Bord am Great Barrier Reef auf Grund lief. Am 4. April 2010 traten vier Tonnen Öl aus, die jedoch mit einer Chemikalie zersetzt werden konnten. Obwohl verhindert werden konnte, dass eine Ölpest das Ökosystem in Mitleidenschaft zog, wurde durch das Auflaufen des Schiffes ein kleiner Bereich des Riffs zerstört. Pressemeldungen zufolge wurden 250 mal 100 Meter bis 3 Kilometer mal 250 Meter zerstört.

Ein besonderes Ereignis stellt die jährliche farbenprächtige Korallenblüte dar, die der Vermehrung der Korallen dient. Diese findet am Great Barrier Reef während des australischen Frühjahrs im November statt und richtet sich nach einem von der Natur streng vorgegebenen Zeitplan. Die den Zeitpunkt beeinflussenden Faktoren sind die Wassertemperatur von etwa 27 °C, die Tageslänge und die Mondphase. Nur ein minutiös aufeinander abgestimmter Ausstoß der Eizellen und Samen in großer Menge gewährleistet unter den durch Fressfeinde und starken Meeresströmungen verursachten schwierigen Bedingungen eine erfolgreiche Fortpflanzung der Tiere.

Etwa zwei Millionen Touristen besuchen das Riff jedes Jahr. 2003 gaben sie über vier Milliarden Australische Dollar aus. Entsprechend wichtig ist der marine Tourismus für das nördliche Queensland. Es gibt etwa 820 Anbieter von Fahrten zum Riff, die ungefähr 1.500 Boote, Hubschrauber und Kleinflugzeuge für Touren zum Riff bereitstellen. Sie operieren von den Küstenstädten aus, wobei sich 85 % des Tourismus auf die beiden Städte Cairns und Airlie Beach konzentriert.

In den Städten bieten die Unternehmen Ausflüge an, von Tagestouren bis zu mehrwöchigen Segeltörns. Die Fahrzeuggröße reicht von kleinen Segelbooten bis hin zu großen Katamaranen mit über 400 Plätzen. Ungefähr 40 % des Tourismusgeschäfts liegt dabei in den Händen der zehn größten Anbieter.

Trockenen Fußes kann man das Riff sowohl durch eine Fahrt mit einem Glasbodenboot oder einem Halb-U-Boot bestaunen, als auch durch einen Besuch eines der unzähligen Unterwasserobservatorien. Die am meisten nachgefragte Art des Rifferlebnisses ist allerdings das Schnorcheln oder Tauchen.

Ideale Reviere zum Tauchen und Schnorcheln liegen am äußeren Gürtel des Riffs, dem Outer Barrier. Begünstigt durch die Nähe zum offenen Meer ist die Sichtweite unter Wasser erhöht, da sich weniger Sedimente im Wasser ablagern können. Eigens für den Massentourismus und Tauchanfänger wurden Pontons im Außenriff fest verankert, die mit Schiffen angefahren werden. Oftmals sind die umgebenden Gebiete mit Netzen abgegrenzt, um Taucher und Schnorchler vor Angriffen von Riff-Haien zu schützen. Erfahrene Taucher benutzen [[Tauchsafari]]-Boote, die meist von [[Cairns]] oder [[Port Douglas (Queensland)|Port Douglas]] auslaufen, um an entlegenen [[Tauchgebiet|Tauchplätzen]] im Riff zu tauchen.

Am 26. Oktober 1981 wurde das Riffsystem von der UNESCO zum Weltnaturerbe erklärt. Es erfüllt alle vier der damaligen Kriterien.

Schutzmaßnahmen sind aufgrund der vielfältigen Ursachen nur zum Teil kurzfristig umsetzbar. Ein Großteil der Zerstörungen rührt vom hohen Nähr- und Schwebstoffeintrag durch intensive Landwirtschaft in Küstennähe her. Die Pflanzenschutz- und Düngemittel der Zuckerrohr- und Bananenplantagen gelangen mit dem jährlichen [[Monsunregen]] ins Küstengewässer und zerstören ganze Korallenstöcke. Weitere Schadstoffe gelangen infolge der vermehrten Bebauung einiger Inseln und der touristischen Nutzung des Riffs ins Meerwasser. Summiert setzen Treibstoffe und Ankerwürfe von Booten, Sonnenöl von Schnorchlern und das Sammeln von Souvenirs dem Ökosystem ebenso zu wie unbedachte Taucher, die die Empfindlichkeit des Riffs und damit ihre zerstörerische Wirkung nicht erkennen. Auch die Erwägung, in dem rund 200.000 km² großen Gebiet nach [[Erdöl]] zu bohren, bedroht das Riff. Gegen [[Überfischung]] und die Zerstörung der Riffe durch [[Schleppnetzfischerei|Schleppnetze]] wurden bereits Maßnahmen ergriffen. Doch trotz der Verbote wird es Jahre dauern, bis sich das aus dem Gleichgewicht geratene Ökosystem wieder erholt hat.

Zum Schutz des Welterbes wurde ein umfangreicher, auf 25 Jahre angelegter Strategieplan ausgearbeitet mit dem Ziel, das gefährdete Ökosystem intensiv zu erforschen und wieder ins Gleichgewicht zu bringen – auch, oder gerade weil das Great Barrier Reef im Gegensatz zu vielen anderen Korallenriffen, die abzusterben drohen, noch in einem verhältnismäßig guten Zustand ist. Das Wrack des am 23. März 1911 gesunkenen Passagierschiffs "[[Yongala (Schiff)|SS Yongala]]", 1958 wiederentdeckt, ist bei Tauchern sehr beliebt und ein wertvolles Schutzgebiet. Gleiches gilt für die Wracks der "[[Gothenburg (Schiff)|SS Gothenburg]]", der "[[RMS Quetta]]" und zahlreiche andere. Damals wurde schon eine erste aufwendige Bestandserfassung vorgenommen, deren Daten besonders wertvoll geworden sind, weil sie Langzeitergebnisse ermöglichen.

Das Welterbe-Komitee reagierte kritisch auf Pläne, die australischen Küsten hinter dem Riff ökonomisch auszubauen. Insbesondere der geplante Hafen für den Export von Kohle und [[Flüssigerdgas]] stieß auf Bedenken. 2014 äußerte das Komitee seine Sorge, vertagte aber eine Entscheidung, das Riff auf die [[Liste des gefährdeten Welterbes]] zu setzen, um ein Jahr. Bis 2015 sollte die australische Regierung daraufhin weitere Daten über die Planungen und die Maßnahmen zum Schutz des Riffs vorlegen. 2016 publizierte die UNESCO einen Bericht zur Bedrohungslage verschiedener Weltkulturstätten durch die [[Globale Erwärmung]]. Auf Intervention der australischen Regierung, die Einbußen beim Tourismus befürchtete, wurden alle Passagen zu australischen Stätten aus dem Bericht entfernt, darunter auch die zum Great Barrier Reef. Diese als [[Zensur]] wahrgenommene Kürzung fand erhebliche öffentliche Aufmerksamkeit und löste starke Kritik an der australischen Regierung als auch der UNESCO aus.

Zwischen 1985 und 2012 hat das Riff die Hälfte seiner Korallen verloren. Infolge der [[Globale Erwärmung|globalen Erwärmung]] war das Great Barrier Reef in den Jahren 1998 und 2002 und vor allem 2016 stark vom [[Korallenbleiche|Massenkorallenbleichen]] betroffen. Im Jahr 2002 waren 60 bis 95 % des Riffs geschädigt. Der Großteil konnte sich wieder erholen – fünf Prozent des Riffs wurden jedoch in so gravierender Weise zerstört, dass es mehrere Jahre bis Jahrzehnte dauern wird, bis sich das Riff von den Schäden wieder vollständig erholt hat. Ein weiteres durch die Erderwärmung verursachtes Problem sind [[Zyklone]], die in ihrer Intensität zugenommen haben, und mit ihren Wellen große Bereiche zerstören können. Die [[Versauerung der Meere]] durch Aufnahme von [[Kohlenstoffdioxid]] hat nachweislich die Wachstumsrate der Steinkorallen reduziert. 

Ursache für diese zunehmend wiederkehrenden Korallenbleichen ist der Umstand, dass durch die globale Erwärmung wie auch das immer wieder auftretende Wetterphänomen [[El Niño]] außergewöhnlich hohe Wassertemperaturen im Pazifik entstehen. Die Korallenbleiche des Jahres 2016 war die stärkste, die jemals festgestellt wurde. 55 % der Riffe wurden schwer geschädigt, während es bei den beiden vorangegangenen Bleichen 1998 und 2002 nur 18 % gewesen waren. Insgesamt waren 2016 93 % aller Riffe betroffen. Hält die Bleiche über einen längeren Zeitraum an, sterben die Riffe ab. Laut einem Bericht der [[James Cook University|James Cook Universität]] vom Mai 2016 sind infolge der Korallenbleiche 2016 bereits 35 % der Korallen fast oder ganz abgestorben. Auch 2017 kam es wieder zu einer Korallenbleiche, dieses Mal vor allem im Mittelteil. Es war das erste Mal, dass zwei Korallenbleichen direkt in zwei aufeinanderfolgenden Jahren auftraten, darüber hinaus wurde die Bleichen des Jahres 2017 nicht durch einen El Niño verursacht. Bei den beiden Bleichen 2016 und 2017 starben rund die Hälfte der zu diesem Zeitpunkt noch vorhandenen Korallen des Riffes ab. 

Nach einer Bleiche können sich Korallen unter Umständen regenerieren. Bei bestimmte Korallen, die besonders gut im Wiederbesiedeln sind und schnell wachsen, können sich Riffe nach einer Bleiche im Zeitraum von 10 bis 15 Jahren erholen, bei alten Riffen dauert dieser Prozess aber viele Jahrzehnte. In diesem Zeitraum darf es zu keiner weiteren Korallenbleiche oder sonstigen weiteren Störung der Erholungsphase kommen, was angesichts der weiter voranschreitenden Erwärmung als unrealistische Annahme beurteilt wird.

Die konservative australische Regierung unter [[Tony Abbott]] hat im Dezember 2013 den Ausbau des Kohlehafens "[[Abbot Point]]" am Great Barrier Reef zum größten Kohlehafen der Welt genehmigt. Zu diesem Zweck sollen drei Millionen Kubikmeter Schlamm durch ein Unternehmen der [[Adani Group]] abgebaggert und im Meer entsorgt werden. Umweltschützer befürchten massive Schäden durch den Schlamm im Riff. Über den Hafen sollen Kohlevorkommen aus dem [[Galilee-Becken]] in Queensland ([[Carmichael Kohlemine]]) mit einem geschätzten Marktwert von 28 Milliarden US-Dollar erschlossen werden. Die [[Deutsche Bank]] gab auf ihrer Hauptversammlung im Mai 2014 bekannt, dass sie die Absicht fallengelassen habe, sich an der Finanzierung des Hafens zu beteiligen. Die [[KfW]]-Bank fördert den Export deutscher Technologie zum Ausbau eines anderen Hafens (Kohlehafen Wiggins Island) mit insgesamt 110 Millionen [[Euro]].

Über Jahrzehnte war das Abkippen von Baggergut im Meer die Norm. Erst im November 2015 erlies das Parlament von Queensland ein entsprechendes Verbot auf dem gesamten Gebiet des Great Barrier Reef, um die zerstörerische Praxis zu beenden. Dennoch bestritt im Mai 2016 der australische Umweltminister [[Greg Hunt]], dass die Kohleförderung und -Verfeuerung eine „signifikante“ Auswirkung auf den Klimawandel und die nationalen Ökosysteme haben.

Ein natürlicher, auch zerstörerischer Feind ist der giftige [[Dornenkronenseestern]]. Er tritt in zeitlichen Abständen massiv auf und verschwindet dann wieder. Sein noch unerforschtes Auftreten hinterlässt komplett abgestorbene Riffsektionen, da er die lebenden Korallenpolypen aus ihren schützenden Kalkgehäusen saugt und auffrisst.








[[Kategorie:Korallenriff]]
[[Kategorie:Korallenmeer]]
[[Kategorie:Weltnaturerbe in Australien und Ozeanien]]
[[Kategorie:Weltnaturerbe in Australien]]
[[Kategorie:Betauchtes Riff]]
[[Kategorie:Geographie (Queensland)]]
[[Kategorie:Geographischer Rekord]]
[[Kategorie:Meerespark (Australien)]]

</doc>
<doc id="8408" url="https://de.wikipedia.org/wiki?curid=8408" title="Lyrik">
Lyrik

Als Lyrik () bezeichnet man die Dichtung in Versform, die die dritte literarische Gattung neben der Epik und der Dramatik darstellt. Lyrische Werke werden auch Gedichte (oder veraltend "Poeme") genannt.

Die Unterscheidung der literarischen Gattungen Lyrik, Epik und Dramatik geht auf die griechische Antike, insbesondere auf die Poetik des Aristoteles zurück. Der Ordnungsbegriff "Lyrik" (in der Form "lyrische Poesie") wird seit dem 18. Jahrhundert als Gattungsbezeichnung verwendet, seit dem 19. Jahrhundert wird er zudem oft synonym mit Poesie, Gedicht und (seltener) Dichtung gebraucht. Der Verfasser poetischer Texte formuliert Gefühle und Gedanken eines lyrischen Subjekts (siehe auch lyrisches Ich), das der Perspektive des Autors entsprechen kann, aber nicht muss. Beziehungen zwischen Subjekt und der es umgebenden Welt werden dabei oft in hohem Maße reflektiert und abstrahiert. Lyrische Texte sind in der Regel reich an rhetorischen Stilmitteln (Tropen und Figuren), rhythmisiert, manchmal gereimt und gelegentlich auch mit Musik verbunden, was auf ihren Ursprung verweist: Im antiken Griechenland wurde der Vortrag von Dichtung in der Regel von einer Lyra oder Kithara begleitet.

Mit dem Begriff „Gedicht“ wurde ursprünglich alles schriftlich Abgefasste bezeichnet; in dem Wort „Dichtung“ hat sich noch etwas von dieser Bedeutung erhalten. Seit etwa dem 18. Jahrhundert wird der Begriff im heutigen Sinn nur noch für poetische Texte verwendet.

Ein umfangreiches (oft auch mehrteiliges) lyrisches Werk mit unter Umständen auch epischen Elementen wird als Langgedicht bezeichnet, ein zyklisch angelegtes als Gedichtzyklus. Eine historische Sonderform des Langgedichts ist das Poem.

Lyrische Texte unterscheiden sich sprachlich-formal von epischen und dramatischen vor allem durch ihre Kürze, ihre strengere sprachliche Form, ihre semantische Dichte (Ausdruckskraft) und sprachliche Ökonomie (Prägnanz), ihre Subjektivität und ihren Bezug auf ein lyrisches Subjekt (z. B. ein lyrisches Ich, Du oder Wir). Dazu werden in erhöhtem Maße und auf verschiedenen Ebenen rhetorische und formale Ausdrucksmittel verwendet (siehe beispielsweise Reim, rhetorische Figur, Metapher), was nicht selten zu einer vom Gewohnten abweichenden Anordnung von Wörtern, Wortgruppen und Sätzen führt. Eine besondere Rolle spielen zudem die lautlichen Qualitäten des verwendeten Sprachmaterials, von einfachen Assonanzen bis hin zu Formen der Onomatopoesie; im 20. Jahrhundert entwickelten sich zahlreiche Formen der Lautpoesie, die diesen Aspekt in den Mittelpunkt stellen. Bei einzelnen Autoren der antiken und mittelalterlichen Lyrik, vor allem jedoch in der Lyrik des Barock und später in literarischen Avantgarden des 20. Jahrhunderts, etwa der konkreten Poesie, wird die graphische Gestalt des Textes zu einem eigenständigen, teilweise dominanten Formelement erhoben (siehe auch Visuelle Poesie).

In der Regel unterscheiden sich lyrische Texte von solchen der Prosa auch durch ihre äußere Form (Vers, Versmaß, Strophenbau). Im Lauf der Gattungsgeschichte verlor dieses Kriterium allerdings an Bedeutung; so finden sich bereits in Goethes Dichtung Gedichte ohne Reim und mit freien Rhythmen, die dann im 19. Jahrhundert in Frankreich als "vers libre" kultiviert wurden. Mit dem weitgehenden Verzicht auf die Regeln der Metrik und der Orientierung an der lebendigen Rede nähert sich der "freie Vers" der Prosa an. Zentrales Distinktionsmerkmal und Formelement lyrischer Texte bleibt letztlich der Vers selbst, der durch absichtsvollen, sinnstiftenden Zeilenumbruch (u. a. in Form des Enjambements) entsteht – im Unterschied dazu sind die Zeilenumbrüche in Prosatexten rein technisch erzeugt, folgen keiner textimmanenten Logik und sind für die Konstitution der Textbedeutung irrelevant.

Aus der Sicht eher linguistisch orientierter Lyriktheorien wird ein lyrischer Text als überstrukturierter Text aufgefasst. Diese Überstrukturierung bezieht sich auf die in der Sprachwissenschaft angesetzten Ebenen jeder sprachlichen Äußerung wie Phonologie, Semantik oder Syntax. So werden Reime als phonologische Überstrukturierung aufgefasst, Metaphern als semantische usw.

Poetische Texte treten in zahlreichen sprachlichen Formen auf. Auf verschiedenen Ebenen der sprachlichen Gestaltung unterscheidet man Versfuß (Anapäst, Daktylus, Jambus, Trochäus u. a.), Versmaß (Alexandriner, Blankvers, Hexameter, Pentameter u. a.), Strophenform (Odenstrophen wie Alkäische Strophe, Asklepiadeische Strophe und Sapphische Strophe, Chevy-Chase-Strophe, Distichon, Sestine, Stanze (mit Sonderformen wie Siziliane, Nonarime, Huitain, Spenserstrophe), Terzine u. a.) und – formal verschieden streng definierte – Gedichtformen (Ballade, Elegie, Epigramm, Ghasel, Haiku und Senryū, Hymne, Lied, Ode, Ritornell, Sonett, Villanelle u. a.). 

Hinzu kommen poetische Texte, die sich aus dem Umgang mit Sprache als Material ergeben, etwa der sprachlichen Collage und Montage, Formen des Listengedichts und Flarf, sowie Formen, die sich aus einer bestimmten Organisation des Textes ergeben, etwa Anagramm, Lipogramm und Palindrom, Akrostichon, Bildreihengedicht, Rollengedicht und Formen des Prosagedichts. 

Über den einzelnen Text hinaus geht etwa die 14 Sonette und ein „Meistersonett“ umfassende Form des Sonettenkranzes, über den einzelnen Autor z. B. das japanische Kettengedicht. Gedichte, die sich diesen und ähnlichen Bestimmungen entziehen, haben nicht selten eine explizit "offene Form".

Historische, heute kaum noch verwendete Gedichtformen sind u. a. Dithyrambos, Kanzone, Madrigal und Rondeau.

Thematisch und/oder stilistisch bestimmte Genres und Subgenres von Lyrik sind u. a. Confessional Poetry, Dinggedicht, Kinderlyrik, Liebeslyrik, Naturlyrik und politische Lyrik. Hinzu kommen zahlreiche, meist auch formal bestimmte Spielarten der sog. Unsinnspoesie (z. B. Clerihew, Klapphornvers, Leberreim, Limerick, Wirtinnenvers) und der Gelegenheitsdichtung.

Historische lyrische Genres sind u. a. Trobadordichtung, Minnesang, Sangspruchdichtung, Bukolik bzw. Schäferdichtung und Meistersang, sowie zahlreiche zeitgebundene Subgenres, etwa die Makkaronische Dichtung.

Genreübergreifende Formen gebundener Rede sind beispielsweise Ballade, Romanze und Haibun. Zeitgenössische Mischformen finden sich u. a. im Spoken Word. Auch Liedtexte (aller Genres) sowie Hip Hop und Rap haben Gemeinsamkeiten mit poetischen Texten.

Die Lyrik ist eine der frühen literarischen Formen. Wenn auch die frühesten überlieferten lyrischen Texte nicht als Gedichte im heutigen Sinne verstanden wurden – das Vorkommen von Reim bzw. Alliteration, einer Metrik oder eines sprachlichen Rhythmus’ genügt, um etwa die Merseburger Zaubersprüche oder frühe religiöse Texte als lyrische Texte einzustufen.

Der heutige Begriff von Lyrik geht auf den antiken griechischen Kulturkreis zurück; dort war die Lyrik zunächst das zur Lyra gesungene Lied, das in den Chorgesängen der antiken Dramen und im religiösen Kultus seinen „Sitz im Leben“ hatte. Bis heute steht Lyrik in einer gewissen Beziehung zur Musik und zum Lied.


Im volkssprachlichen Mittelalter treten Individualpersönlichkeiten vor allem im Minnesang und in der Spruchdichtung hervor: die provencalischen Trobadours ab dem Ende des 11. Jahrhunderts, Heinrich von Veldeke, der als erster deutschsprachiger Dichter gilt, und Walther von der Vogelweide im 12. Jahrhundert, Heinrich von Morungen und Frauenlob im 13. Jahrhundert, Oswald von Wolkenstein im – spätmittelalterlichen – 15. Jahrhundert. Hauptsächlich wurde die mittelalterliche Lyrik gesungen und mündlich tradiert; die Quellen, zunächst Handschriften und später auch Drucke, auf denen das heutige Wissen über die Lyrik des Mittelalters beruht, sind häufig erst lange nach der Entstehung der Texte entstanden. Deren Urfassungen sowie die Transformationen, denen sie vor ihrer Niederschrift unterlegen haben, lassen sich nur selten durch Quellenvergleiche rekonstruieren. Geistliche Lyrik (z. B. die Sequenzen) und die lateinische Vagantendichtung sind oft anonym in größeren Sammlungen überliefert, etwa der Carmina Burana (11./12. Jahrhundert). Die Meistersinger des ständisch geprägten Spätmittelalters (u. a. Hans Sachs, 16. Jahrhundert) inszenierten ihre Dichtung als ein lern- und abprüfbares Silben- und Töne-Handwerk.

Im altenglischen Epos Beowulf singt ein Skop von der Weltschöpfung. Das Gedicht "The Battle of Maldon" lässt sich bereits auf das 11. Jahrhundert datieren. Nach der Christianisierung Englands entstanden zahlreiche religiöse Gedichte, wobei sich an manchen Elegien wie im "The Wanderer" noch die Umbrüche der Zeit bemerkbar machen. Naturgedichte wie "The Seafarer" beinhalten heidnische und christliche Motive. Einer der ersten namhaften Lyriker ist Cynewulf. Nach der Eroberung Englands durch normannische Truppen im Jahre 1066 verschwand das Altenglische als allgemeine Literatursprache. Das in mittelenglischer Sprache verfasste Werk "Brut" des Dichters Layamon gehört zu den wichtigsten Dichtungen des 13. Jahrhunderts. Es ist nicht nur mit angelsächsischem Vokabular durchsetzt, sondern steht am Anfang der literarischen Artus-Rezeption in England, zu der auch die bekannte Versdichtung "Sir Gawain and the Green Knight" zählt. Im 14. Jahrhundert entstehen Allegorien und Gedichte wie "Piers Plowman", "Patience" und "Pearl".

Als Formerneuerer ersetzte Geoffrey Chaucer im 15. Jahrhundert den germanischen Stabreim durch den Endreim und passte den ursprünglich französischen Balladenvers der englischen Sprache an. Dieser Rhyme royal besteht aus sieben Versen, jambischen Fünfhebern und dem Reimschema . Die starke Wirkung Chaucers zeigte sich in der hohen Zahl seiner Nachahmer, zu denen u. a. John Gower, John Lydgate und John Hoccleve zählen. Selbst der schottische König James I. verfasste Gedichte im Stil Chaucers.

Im 16. Jahrhundert schrieb Sir Thomas Wyatt die ersten Sonette in englischer Sprache. Sir Phillip Sidneys Sonettzyklus "Astrophel and Stelle" setzte das schon in Wyatts Dichtung angelegte englische Sonett schließlich durch. Daneben verfasste der Jesuit Robert Southwell religiöse Gedichte und Thomas Campion Lieder. Die englische Sonettdichtung fand ihren Höhepunkt mit William Shakespeare. Weitere Sonettdichter sind Walter Raleigh, Michael Drayton und Samuel Daniel. Edmund Spenser schrieb die Versepen "The Shepheardes Calender" und "The Faerie Queene".

John Donnes metaphysische Dichtung grenzte sich im 17. Jahrhundert von der starren Sonettdichtung der englischen Renaissance ab. Die Cavalier poets Ben Jonson, Richard Lovelace und Edmund Waller nahmen sich weltlicher Themen an. Im späten 18. Jahrhundert überwanden Thomas Gray und Robert Burns die Auswirkungen der Restauration, deren Dichtung sich hauptsächlich auf die Übersetzung lateinischer Klassiker beschränkte, und besonders der spätere Nationaldichter Schottlands Burns ebnete den Weg zur englischen Romantik. Die Romantik repräsentieren die Dichter William Blake, William Wordsworth, Samuel Taylor Coleridge, Percy Bysshe Shelley, Lord Byron und John Keats. Zur viktorianischen Epoche werden Alfred Tennyson und Robert Browning gezählt. Hauptvertreter des Symbolismus war der Ire William Butler Yeats, aber auch spätere Dichter der Moderne wie der Waliser Dylan Thomas können teilweise zu dieser Richtung gerechnet werden.

Bedeutende US-amerikanische Lyriker sind u. a. Edgar Allan Poe, Walt Whitman und Emily Dickinson im 19. Jahrhundert, Wallace Stevens, E. E. Cummings, William Carlos Williams, Ezra Pound, Elizabeth Bishop, Sylvia Plath, Ann Sexton, Allen Ginsberg und John Ashbery im 20. Jahrhundert. Eine wichtige Rolle spielte die Lyrik auch in der Popkultur seit den 1960er Jahren, etwa bei John Lennon, Cat Stevens, Bob Dylan, Leonard Cohen und anderen Songwritern.

Die französischsprachige Lyrik beginnt mit den Trouvèren im 12. Jahrhundert, deren Werke auf altfranzösisch verfasst wurden. Die Trobadordichtung im Süden Frankreichs wurde in provenzalischer Sprache geschrieben. Marie de France verwendete Versformen für ihre "Lais", Chrétien de Troyes für seine Verserzählungen Erec et Enide und Le Conte du Graal ou le Roman de Perceval. Guillaume de Lorris und Jean de Meung, die Autoren des Le Roman de la Rose, nutzten paarweise reimende Verse für ihren Roman. Im 14. Jahrhundert verfasste Eustache Deschamps über tausend Balladen, manchen gilt er gar als Begründer dieser Form. Zu seiner Dichtung werden traditionelle Liebesdichtungen, Satiren und Sentenzen gezählt, sowie die Poetik "L'art de dictier et de fere chançons, ballades, virelais et rondeau". Als bedeutendster Dichter der Zeit gilt François Villon. Sein Hauptwerk "Le Testament", das sechzehn Balladen und drei Rondeaus enthält, wurde auch außerhalb des französischen Sprachraums über Jahrhunderte hinweg rezipiert. Sein Dichtung beeinflusste Mitte des 19. Jahrhunderts die Präraffaeliten und im 20. Jahrhundert die deutschsprachigen Naturalisten, später die Expressionisten und prägte die Gelegenheitsdichtung. Charles de Valois, duc d’Orléans, der Villon in seinem Schloss aufnahm und ihn später wegen eines Spottgedichtes in den Kerker werfen ließ, dichtete selbst in englischer und französischer Sprache.

Bedeutende Lyriker der Renaissance waren Pierre de Ronsard und Joachim Du Bellay, für die französische Klassik ist François de Malherbe, für die Aufklärung Jacques Delille zu nennen. In der Romantik sind Alphonse de Lamartine, Alfred de Musset und Victor Hugo von Bedeutung, etwa zeitgleich schrieben die „Parnassiens“ Théophile Gautier und Théodore de Banville. Die großen französischen Dichter der frühen Moderne (Symbolismus) sind Charles Baudelaire, Arthur Rimbaud, Paul Verlaine und Stéphane Mallarmé. Bedeutende Lyriker zu Beginn des 20. Jahrhunderts sind Guillaume Apollinaire und Paul Valéry, später André Breton, Paul Éluard, Ivan Goll, Tristan Tzara, Yves Bonnefoy u. a. m.

Die wichtigsten (neu-)griechischen Lyriker der Moderne waren Konstantinos Kavafis, Kostis Palamas, Odysseas Elytis, Giorgios Seferis und Giannis Ritsos.

In Italien waren die Lyriker der Renaissance Dante Alighieri (13. Jahrhundert) und Petrarca (14. Jahrhundert) bahnbrechend, weitere wirkmächtige Lyriker waren Michelangelo (15. Jahrhundert) und Torquato Tasso (16. Jahrhundert). Giacomo Leopardi (Anfang 19. Jahrhundert) und Gabriele D’Annunzio (19./20. Jahrhundert) waren jeder auf seine Weise Erneuerer der italienischen Dichtung; im 20. Jahrhundert waren Giuseppe Ungaretti, Eugenio Montale und Andrea Zanzotto – auch international – wegweisend.

Der Nationaldichter Polens ist Adam Mickiewicz (19. Jahrhundert). Die wichtigsten polnischen Lyriker des 20. Jahrhunderts waren Czesław Miłosz, Zbigniew Herbert, Tadeusz Różewicz und Wisława Szymborska; ein bedeutender Gegenwartsdichter ist Adam Zagajewski.

Der Nationaldichter Portugals ist Luís de Camões (16. Jahrhundert), ihm ist auch der Nationalfeiertag gewidmet; neben ihm steht António Ferreira. Einflussreiche Lyriker des 19. Jahrhunderts sind der Romantiker Soares de Passos und die symbolistischen Dichter Antero de Quental und Cesário Verde. In der portugiesischen Lyrik des 20. Jahrhunderts ist Fernando Pessoa die wichtigste Stimme; ein weiterer Dichter von Weltrang ist Eugénio de Andrade.

Nach den russischen Nationaldichtern des 19. Jahrhunderts Alexander Puschkin und Michail Lermontow waren in der ersten Hälfte des 20. Jahrhunderts vor allem Sergei Jessenin, Osip Mandelstam, Anna Achmatova, Marina Zwetajewa, Boris Pasternak und Wladimir Majakowski herausragende russische Dichter. Für die Entwicklung des russischen Futurismus und nachfolgende Avantgarden sind Velimir Chlebnikov und Alexei Krutschonych entscheidend. In der zweiten Hälfte des 20. Jahrhunderts lebten viele bedeutende russische Lyriker außerhalb des Landes, etwa Joseph Brodsky in den USA und Alexeij Parschtschikov in Deutschland.

Als slowenischer Nationaldichter gilt France Prešeren, weitere bedeutende Lyriker des 19. Jahrhunderts waren Dragotin Kette und Josip Murn. Im 20. Jahrhundert sind Srečko Kosovel und Matej Bor zu nennen, die bedeutendsten Dichter waren Dane Zajc und Tomaž Šalamun.

Luis de Góngora und Francisco de Quevedo (16./17. Jahrhundert) sind die wichtigsten Lyriker des spanischen Barock. Bedeutende Lyriker des 20. Jahrhunderts sind u. a. Juan Ramón Jiménez, Antonio Machado sowie die Lyriker der Generación del 27 Ramón Gómez de la Serna, Rafael Alberti, Vicente Aleixandre, Jorge Guillén, Pedro Salinas, Miguel Hernández und Federico García Lorca.

Bedeutende spanischsprachige Lyriker Chiles sind Pablo Neruda und Nicanor Parra. Der bedeutendste spanischsprachige Lyriker Mexikos ist Octavio Paz, der Perus César Vallejo.

Bedeutende tschechische Lyriker des 20. Jahrhunderts sind u. a. Jiří Wolker, Vítězslav Nezval, Konstantin Biebl, Jiří Orten, František Halas, Vladimír Holan, Jaroslav Seifert, Jan Skácel und Jiří Kolář, in jüngerer Zeit u. a. Jáchym Topol und Petr Borkovec.

Der allgemeine Begriff für "Gedicht" im Japanischen ist "uta" (, in Zusammensetzungen auch "-ka" oder ), was auch „Lied“ bedeutet. Traditionell unterscheidet man japanische Gedichte "(Waka)" und chinesische Gedichte "(Kanshi)". Die Hauptformen des "Waka" sind das Kurzgedicht, "Tanka", mit 5-7-5-7-7-Moren und das Langgedicht, "Chōka", mit 5-7-5-7- … -5-7-7-Moren. Aus der Verkettung von Tanka entstand das Kettengedicht, "Renga", dessen Eröffnungsvers mit 5-7-7-Moren später zur eigenständigen Gedichtform "Haiku" wurde. Ähnlich kurz ist auch das "Senryū", das außerhalb Japans nach dem Haiku die bekannteste Form japanischer Poesie darstellt.

Gedichte sind bereits in den beiden ältesten überlieferten japanischen Werken, den Reichschroniken "Kojiki" und "Nihonshoki" von 712 bzw. 720 n. Chr. enthalten. 759 n. Chr. erschien mit dem "Man’yōshū" die erste Gedichtanthologie, die knapp 4500 Gedichte umfasst, wobei ein Teil der Gedichte bis in das frühe 6. Jahrhundert n. Chr. zurückreicht. Obwohl die Werke im "Man’yōshū" zum Großteil der Hofdichtung zuzuordnen sind, finden sich darin auch Gedichte aus dem einfachen Volk, etwa Soldatengedichte. Die japanischen Kaiser ließen von 905 mit dem "Kokin-wakashū" bis 1439 mit dem "Shinshokukokin-wakashū" regelmäßig Waka-Anthologien wie die Sammlungen aus einundzwanzig Epochen zusammenstellen.

Die bedeutendsten Dichter bis ins 12. Jahrhundert wurden als „Die Sechsunddreißig Unsterblichen der Dichtkunst“ bezeichnet. Als die bedeutendsten Dichter der Edo-Zeit (17.–19. Jahrhundert) gelten Matsuo Bashō, Yosa Buson und Kobayashi Issa, während für die Moderne Hagiwara Sakutarō, Ishikawa Takuboku, Masaoka Shiki, Miyazawa Kenji, Ogiwara Seisensui, Takamura Kōtarō und Yosano Akiko zu nennen sind.

Zu den bedeutendsten persischen Dichtern gehört Abū ʾl-Qāsim Firdausī (940–1020). Das von ihm verfasste Epos "Schāhnāme" (, auch "Šāhnāmeh", „Königsbuch“ oder „Buch der Könige“) gilt als Nationalepos der persischsprachigen Welt. Mit nahezu 60.000 Versen ist es mehr als doppelt so umfangreich wie Homers Epen und mehr als sechsmal so lang wie das Nibelungenlied. Ein weiterer herausragender Dichter ist Hafis (14. Jahrhundert), dessen Werk unter anderem Goethe zu seinem West-östlichen Divan inspirierte. Auf diesem Weg nahm Hafis Dichtung nachhaltig Einfluss auch auf die europäische Lyrik. Im 20. Jahrhundert gilt Forugh Farrochzad als eine der bekanntesten iranischen Dichterinnen.

Im Koran ist den zumeist schicksalsgläubigen altarabischen Dichtern ein eigener, kritischer Abschnitt gewidmet. Die letzten vier Verse der „Die Dichter“ "(asch-Schuʿara)" genannten Sure 26 setzen sie mit Wahrsagern und ziellos Umherirrenden gleich, die von Dschinn oder gar dem Satan selbst besessen seien und ihren Einfluss auf das Stammesleben falsch nützten. Der Prophet Mohammed grenzt sich zwar von ihnen ab, bescheinigt aber (in den letzten beiden später offenbarten bzw. hinzugefügten Versen) zumindest einigen unter ihnen Rechtgläubigkeit. Der Gesamtinhalt der Sure ist eine Zusammenfassung der wichtigsten Prophetengeschichten des Islam, die Mohammed trösten und die Ungläubigen warnen sollen. Die muslimisch-arabischen Dichter erfreuten sich nach Mohammed unter den Umayyaden höchster Protektion, sofern sie die Quraisch glorifizierten und halfen, die Nichtaraber zu arabisieren. Hauptthema der Dichtung vor bzw. bis Mohammed war die Suche des liebenden (und deshalb umherirrenden) Dichters nach der verlorenen Geliebten.

Weltweit hat Lyrik auch im 21. Jahrhundert eine große Bedeutung, vor allem in der arabischen Literatur, aber auch in vielen anderen Kulturkreisen. Im deutschen Sprachraum hatte Lyrik nie einen solchen Stellenwert, im 20. Jahrhundert ging ihre Rezeption eher noch weiter zurück bzw. verharrte auf niedrigem Niveau. Unabhängig davon entwickeln sich beständig neue Formen von Lyrik und poetische Sprechweisen, zuletzt z. B. im Internet und in den Jugendkulturen (Spoken Word, Hip Hop, Sprechgesang). Auch die Lyrik im engeren Sinn hat sich in den vergangenen Jahrzehnten tiefgreifend verändert und bezüglich ihrer Formen, Mittel und Gegenstände erweitert. Eine Wiederaufnahme metrischer und gereimter Dichtung zeigt sich im amerikanischen Neuen Formalismus.







</doc>
<doc id="8411" url="https://de.wikipedia.org/wiki?curid=8411" title="Eis">
Eis

Als Eis wird gefrorenes Wasser bezeichnet, welches – neben flüssigem Wasser und Wasserdampf – dessen dritten möglichen klassischen Aggregatzustand darstellt. Allgemeiner können auch andere leicht flüchtige Verbindungen in ihrem festen Aggregatzustand als „-Eis“ bezeichnet werden, etwa Trockeneis, Ammoniakeis, Methaneis. Letztere werden meist in einem Zusammenhang mit astronomischen Objekten so genannt. Im Folgenden ist von Wassereis die Rede.

Eis bildet sich im Allgemeinen bei der Abkühlung von flüssigem Wasser oder Wasserdampf auf null Grad Celsius und zählt als natürlich vorkommender kristalliner Festkörper mit einer definierten chemischen Zusammensetzung zu den Mineralen. Aufgrund seiner chemischen Struktur HO gehört Eis zur Stoffgruppe der Oxide.

Eis kristallisiert im hexagonalen Kristallsystem und tritt in der Natur in verschiedenen Erscheinungsformen auf, von der Schneeflocke über das Hagelkorn und der gefrorenen Oberfläche meist stehender Gewässer bis zum Gletscher. Seine Dichte von 0,918 g/cm³ (reines, luftfreies Eis bei 0 °C) ist geringer als die von Wasser (1 g/cm³), weswegen es auf der Wasseroberfläche schwimmt und dort "Eisdecken", Eisschollen und Eisberge bildet. Dabei befinden sich zirka 90 Volumenprozent des Eises unter Wasser (Auftriebskraft des Wassers gegen Gewichtskraft des Eises) und nur zirka 10 Volumenprozent oberhalb der Wasseroberfläche.

In reiner Form besteht Eis aus farblosen, transparenten Kristallen. "Eisblöcke" enthalten jedoch meist viele feine Luftbläschen, die während der Erstarrung der "Eiskristalle" eingeschlossen werden und erscheinen daher durch vielfache Lichtbrechung weiß. Als chemischer Stoff zeichnet es sich durch einige besondere Eigenschaften aus, die auf den Anomalien des Wassers beruhen.

Bei zahlreichen meteorologischen Phänomenen spielt Eis eine wichtige Rolle. Die Eiskappen der Polarregionen sind von großer Bedeutung für das globale Klima und speziell für den globalen Wasserkreislauf. Einen dementsprechend entscheidenden Einfluss hat es daher auch auf unsere Biosphäre.

Die Wissenschaft von Formen, Auftreten und Eigenschaften von Eis und Schnee nennt man Glaziologie.

Die Wortherkunft (Etymologie) von Eis lässt sich über das althochdeutsche, mittelhochdeutsche und niederdeutsche ‚îs‘ bis zum germanischen ‚īsa‘ zurückverfolgen. Durch Diphthongierung (Lautwandel von einem zu zwei Vokalen) wurde aus diesem Urwort unter anderem das deutsche Eis und das englische ice.

Als eigenständige Mineralart taucht Eis allerdings erst Anfang des 19. Jahrhunderts auf. Zuvor galt es (einschließlich Wasser, Schnee und Hagel) seit der Antike gemäß der Vier-Elemente-Lehre neben Feuer, Luft und Erde als eines der vier Grundelemente, und selbst in den Systematiken von Abraham Gottlob Werner wird Eis bis zur letzten Auflage 1817 nicht aufgeführt (1. Auflage 1787).

Erst Friedrich Hausmanns beschreibt Wasser bzw. seine verschiedenen festen Formen (Varietäten) in seinem "Handbuch der Mineralogie" von 1813 als Mineral, eingereiht in die zweite Klasse der „Inkombustibilien“ und der zweiten Ordnung der „Oxydoide“. Eis und Schnee gehören nach Hausmann zum „Weichwasser“, das tafelförmig als Eisschollen, stalaktitisch als Eiszapfen, rindenförmig als Glatteis und sphäroidisch als Hagel vorkommt.

In der mittlerweile veralteten, aber noch gebräuchlichen gehörte Eis zur Abteilung der „Oxide mit dem Stoffmengenverhältnis Metall : Sauerstoff = 1 : 1 und 2 : 1 (MO, MO)“, wo es als einziges Mitglied die unbenannte Gruppe "IV/A.01" bildete.

Die seit 2001 gültige und von der International Mineralogical Association (IMA) verwendete ordnet Eis ebenfalls in die Abteilung der „Oxide mit dem Stoffmengenverhältnis Metall : Sauerstoff = 2 : 1 und 1 : 1“ ein. Diese ist allerdings weiter unterteilt nach dem genauen Anion-Kationen-Verhältnis und der relativen Größe der Kationen, so dass das Mineral entsprechend seiner Zusammensetzung in der Unterabteilung „Kation : Anion (M : O) = 2 : 1 (und 1.8 : 1)“ zu finden ist, wo es als einziges Mitglied die unbenannte Gruppe "4.AA.05" bildet.

Auch die vorwiegend im englischen Sprachraum gebräuchliche Systematik der Minerale nach Dana ordnet das Eis in die Klasse der „Oxide und Hydroxide“ und dort in die Abteilung der „Oxide“ ein. Hier ist es als einziges Mitglied in der unbenannten Gruppe "04.01.02" innerhalb der Unterabteilung „Einfache Oxide mit einer Kationenladung von 1+ (AO)“ zu finden.

Im festen Aggregatzustand des Wassers wird als Eis normalerweise eine hohe Fernordnung durch Ausbildung eines Kristallgitters im Zuge der Kristallisation erreicht. Im flüssigen Zustand herrscht eine Mischung von Ordnung und Chaos.

Natürliches Eis kristallisiert im hexagonalen Kristallsystem in der mit den Gitterparametern "a" = 4,497(5) Å und "c" = 7,322(4) Å sowie vier Formeleinheiten pro Elementarzelle.

Sechs Wassermoleküle schließen sich dabei über Wasserstoffbrücken jeweils zu einem Ring zusammen, wobei jedes Molekül ebenfalls Teil von zwei benachbarten Ringen ist. Die hexagonale Symmetrie der Kristallstruktur spiegelt sich in der makroskopischen Gestalt der Eiskristalle wider. In dieser Struktur ist jedes Sauerstoffatom tetraedrisch von jeweils vier anderen O-Atomen umgeben.

Hexagonales Eis wird mit Eis I bezeichnet.

Unter −22 °C und über 207,5 MPa bilden sich noch andere, zum Beispiel kubische Eisformen aus, etwa das metastabile, kubische Eis I, in welchem die Sauerstoffatome eine Diamantstruktur aufweisen. Bisher sind 17 kristalline und 5 amorphe Modifikationen bekannt (Stand Januar 2010). Letztere sind Formen ohne Kristallstruktur.

Die 17 kristallinen Formen werden als "Eis I", "Eis I", sowie "Eis II" bis "Eis XVI" bezeichnet.

Eiswolken im interstellaren Raum haben eine Temperatur von ca. −260 °C und sind amorpher Struktur („fließen“).

Der Schmelz- bzw. Gefrierpunkt von Eis liegt unter Normalbedingungen bei 0 °C und eine spezifische Schmelzwärme von λ = 332,8 kJ/kg.

Kristallisationskeime, also Verunreinigungen wie Staubpartikel, Bakterien usw. sind allerdings Bedingung für eine Eiskristallbildung, da sich die kristallisierenden Wassermoleküle an diese anlagern müssen. In so genanntem „unterkühltem Wasser“, nicht gefrorenem Wasser unter 0 °C, besitzen die Moleküle eine vom Normalfall abweichende Nahordnung, und es bilden sich Ikosaederstrukturen aus: so kann z. B. sauberes unterkühltes Mineralwasser an den beim Öffnen der Flaschen entstehenden Gasperlen spontan gefrieren. Ohne externe Auslöser gefriert Wasser bei −48 °C. Sehr reines (destilliertes) Wasser kann bis zu −70 °C unterkühlt werden.

Der Gefrierpunkt kann durch Bestreuen mit Salzen (Streusalz) herabgesetzt werden. Dies ist eine kolligative Eigenschaft, die Gefrierpunktserniedrigung hängt nur von der Menge der gelösten Teilchen, nicht jedoch von ihrer Art ab. Der gleiche Effekt lässt sich also auch mit Zucker erreichen.

Zusätzlich kann auch die Lösungswärme eines Stoffs Eis zum Schmelzen bringen. Entscheidend hierfür ist, dass der hinzugegebene Stoff im festen Lösungsmittel unlöslich ist. Erreicht wird dieser Effekt durch die Erniedrigung des chemischen Potenzials der Flüssigphase. Dieser Effekt erzeugt gleichzeitig eine Siedepunkterhöhung des Wassers.

Der Übergang von festem zu flüssigem Aggregatzustand heißt Schmelzen.
Um Eis zu schmelzen, müssen Wasserstoffbrückenbindungen zwischen den Wassermolekülen des Eises aufgebrochen werden.
Dazu muss dem Eis Energie zugeführt werden.
Beim Schmelzen absorbiert es so viel Energie, wie benötigt würde, um eine äquivalente Wassermasse auf 80 °C zu erhitzen.
Die Temperatur der schmelzenden Eisoberfläche bleibt während des Schmelzens konstant bei 0 °C.
Die Geschwindigkeit des Schmelzvorgangs hängt daher von der Effizienz der Energiezufuhr zur Eisoberfläche ab. Eine Eisoberfläche in Süßwasser schmilzt allein durch freie Konvektion bei gemäßigter Wassertemperatur mit einer Geschwindigkeit, die wie (T – 4 °C) von der Temperatur des Süßwassers, T, abhängt.

Eis tritt bei ausreichend kalter und trockener Luft bei Atmosphärendruck durch Sublimation direkt in Gasform (Wasserdampf) über. Dieser Effekt wird u. a. bei der Gefriertrocknung im industriellen Maßstab genutzt.

Eis ändert seine Farbe mit dem Luftgehalt und kann so auch in unterschiedliche Gruppen eingeteilt werden. Eis, das viel Luft enthält, ist weiß, solches, das wenig Luft enthält, ist durchsichtig und blau oder grün. Ein besonderer Fall von „farbigem“ Eis sind sogenannte Grüne Eisberge, bei welchen es sich um alte umgekippte Eisberge handelt, deren algenbewachsene Unterseite nun sichtbar ist.

Eis und Schnee reflektieren das Sonnenlicht. Innerhalb der Erdatmosphäre verursachen Eispartikel damit Lichtsäulen. (Die verwandten Halos entstehen dagegen durch Brechung des Lichts in Eiskristallen.) Astronomisch und geophysikalisch sind Eis und Schnee häufig Verursacher einer hohen Rückstrahlung eines Gegenstands.

Die Schallgeschwindigkeit in Eis bei maximaler Dichte liegt bei 3250 m/s. Die Dispersion für Schallausbreitung in Eis ist im Gegensatz zu den meisten Festkörpern negativ. Dieser Effekt kann auf zugefrorenen Seen beobachtet werden. Entsteht zum Beispiel in hinreichend großer Entfernung zum Beobachter ein Riss in der Eisfläche (zum Beispiel durch Sonneneinstrahlung), kann ein pfeifendes Geräusch wahrgenommen werden, bei dem die Tonhöhe in Sekundenbruchteilen von ganz hohen Frequenzen zu sehr tiefen abfällt. Das Geräusch ähnelt dem eines vorbeifliegenden Projektils, das durch den Dopplereffekt eine fallende Tonhöhe erzeugt.

Nach der Mohsschen Härteskala hat Eis bei wenigen Grad unter Null nur eine geringe Härte von 1,5 und lässt sich mit dem Fingernagel ritzen. Die Mohshärte von Eis steigt allerdings bei tieferen Temperaturen an. Bei −30 °C übertrifft es mit einer Härte von 3,5 die von Kalkstein (Härte 3), bis es schließlich bei −80 °C die Härte von Vergütungsstahl (Mohshärte ca. 6) erreicht.

Bereits bei wenigen Graden unter Null ist Eis in der Lage, Menschen und sogar schwere Fahrzeuge wie LKW zu tragen. Voraussetzung dafür ist allerdings eine ausreichende Dicke der Eisdecke für die jeweilige Belastung. Die Mindestdicke für eine sichere Belastbarkeit entsprechend der Anforderung beruht auf empirischen Erfahrungswerten bzw. kann mit verschiedenen Methoden berechnet werden. Die Belastbarkeit und die Mindestdicke wird wesentlich von der Beschaffenheit des Eises, wie Risse und Lufteinschlüsse, sowie den Schwimmzustand beeinflusst.

Folgende Eisdicken (auf flüssigem Wasser) gelten als ausreichend:

Die Tragfähigkeit einer Eisdecke hängt einerseits von ihrem Schwimm-Auftrieb auf dem tragenden Wasser und andererseits von der Lastverteilfähigkeit (Durchbiegung) bei punktueller Belastung ab. In beiden Fällen ist die Eisdeckenstärke der maßgebliche Parameter für die Tragfähigkeit. Die Belastbarkeit aufgrund der Schwimmfähigkeit ist dabei proportional zur Eisdicke, während die Lastverteilfähigkeit dem Quadrat der Eisdicke proportional ist.

Bei einer gleichmäßigen Lastverteilung auf großen Flächen ohne Durchbiegung ist die Belastbarkeit, wie bei einem Floß, durch die Schwimmfähigkeit der Eisdecke begrenzt. Entsprechend dem Auftrieb von blasenfreiem Eis der Dichte 917 kg/m³ beträgt die Tragfähigkeit formula_1 (in kg/m) für große Flächen der Dicke:formula_2 (in m).

Also z.B. 8,3 kg/m² bei einer Eisdicke von 10 cm.

Durch Lastverteilung in die umgebende Fläche können begrenzte Teilflächen einer Eisdecke erheblich höher belastet werden. Es ist jedoch immer zu beachten, dass durch die zulässige Belastung von Teilflächen die Höchstbelastung der gesamten Eisdecke nicht überschritten wird.

Die Tragfähigkeit einer Eisstraße bezogen auf Einzelfahrzeuge lässt sich auch mit der sogenannten „Gold-Formel“ abschätzen (benannt nach Lorne W. Gold):

mit

Die kanadische Provinz Manitoba benutzt diese Formeln, um die Tragfähigkeit einer Eisfläche für die Nutzung als Winterstraße zu bestimmen. Die Entscheidung, für welche Belastung die Eisstraße freigegeben wird, trifft letztlich immer ein Experte für Eisstraßen.
Eisstraßen gibt es temporär etwa in Schweden, Finnland, Estland, Kanada, den Vereinigten Staaten und Russland.

Das Betreten von Eisflächen ist prinzipiell gefährlich und im Zweifel zu vermeiden. Dies gilt vor allem auch, weil die Dicke und Beschaffenheit des Eises häufig nicht zuverlässig zu bestimmen ist. Zur Bestimmung der Dicke des Eises eignen sich Eisschrauben oder Bohrer mit aufgetragenen Zentimetermarken sowie das Messen an ins Eis geschlagenen Löchern. 

Durch Einbrechen in das Eis entsteht die Gefahr von starken Unterkühlungen, Erfrierungen und Ertrinken. Bei der Rettung sollten nach Möglichkeit Rettungshilfsmittel benutzt werden, die das Gewicht des Hilfeleistenden auf eine größere Fläche verteilen. Dem Eingebrochenen soll nicht die Hand gereicht werden, sondern Hilfsmittel, die im Notfall auch losgelassen werden können. Zur Eigenrettung können Eiskrallen mitgeführt werden, die das Herausziehen aus dem Loch erleichtern. 

Wasser weist zahlreiche Anomalien auf: Eigenschaften, die von den Regeln, die auf die meisten Stoffe angewendet werden können, abweichen. Folgende Anomalien sind für seinen festen Zustand als Eis von Bedeutung:


Eis bildet sich weltweit dort, wo die Luftfeuchtigkeit hoch genug und die Temperatur auf und unter den Gefrierpunkt gesunken ist.

Freie Eiskristalle entstehen in Form von Reif und Raureif durch Resublimation (direkter Übergang vom gasförmigen in den kristallinen Zustand) des atmosphärischen Wasserdampfs. Graupel und Hagel besteht aus rundlichen Eiskörnern. Sie bilden sich in Gewitterwolken aus Wassertröpfchen, welche in tiefen Wolkenschichten kondensieren und dann durch Aufwinde in höhere und kältere Luftschichten transportiert werden, wo sie dann gefrieren. Größere Hagelkörner sind oft Zusammenballungen kleinerer Eispartikel und durchlaufen in ihrer Entstehungsgeschichte mehrmals den Prozess des Aufstiegs durch Winde und des Absinkens durch ihre Gewichtskraft. Schnee besteht aus mehr oder weniger filigran verästelten Eiskristallen. Schneeflocken bilden sich durch langsames Anlagern und Gefrieren von feinsten Wassertröpfchen an einem Kristallisationskeim (zum Beispiel Staubteilchen).

Die Seegfrörnen des Bodensees sind Jahrhundertereignisse. Die Eisdecke ist dann so tragfähig, dass der gesamte See zu Fuß überquert werden kann.

Dauerhaft mit dem Festland verbundene Eisflächen werden Schelfeis genannt. Die Schelfeisflächen werden meist durch fließende Gletscher gespeist. Eisberge sind von Gletschern abgebrochene (gekalbte) Eismassen.

Bei der Kristallisation von Meerwasser entsteht so genanntes Meereis; dabei wird das Salz an das Meer abgegeben oder sammelt sich in Sole(Salz)-Einschlüssen (Eis selbst ist immer festes Süßwasser). Je nach Größe und Zusammenballung des Eises unterscheidet man Nadeleis, Grieseis, Pfannkucheneis, Eisschollen und Packeis. Eine natürliche eisfreie Fläche, die jedoch vollständig von Packeis umgeben ist, heißt Polynja. Künstliche, in das Eis geschlagene Rinnen und Löcher werden Wuhnen genannt.

Eis, welches sich ausnahmsweise wegen seiner Entstehungsgeschichte am Boden eines Gewässers befindet, wird Grundeis genannt. Die Bildung von Neueis auf dem Meer wird als Nilas bezeichnet.

Die Eisverhältnisse auf Meeresgebieten werden mit einem internationalen "Ice Code" bezeichnet:

Als Einschluss in Diamanten kann auch auf der Erde Eis-VII vorkommen. Dieses hat eine kubische Kristallstruktur.

Eisvorkommen wurden in unserem Sonnensystem nachgewiesen in Kometen, Asteroiden, auf dem Mars und auf einigen Monden der äußeren Planeten. Bei Eismonden ist nahezu die gesamte Oberfläche von Eis bedeckt.

Von zahlreichen Kometen ist bekannt, dass sie zu einem Großteil aus Wassereis bestehen, weshalb sie auch hin und wieder als „Schmutzige Schneebälle“ tituliert werden. Es wird spekuliert, dass ein Großteil der irdischen Wasservorkommen auf ein lang anhaltendes Bombardement der noch jungen Erde durch Kometen zurückgeht. Das meiste Wasser im Universum liegt als Eis vor.

Auch auf dem Mars konnten bisher Eisvorkommen nachgewiesen werden. Neben den Polkappen, die zweifelsfrei zu einem Teil aus gefrorenem Wasser bestehen, gibt es möglicherweise auch in anderen Regionen Eisvorkommen, und zwar als Permafrost in tieferen Bodenschichten.

Hinweise auf das Vorhandensein von Eis in Meteoritenkratern in Polnähe bei Merkur, dem sonnennächsten Planeten, lieferte 1975 die Raumsonde Mariner 10. Genauere Untersuchungen der Raumsonde MESSENGER konnten im November 2011 Wasser auf dem Nordpol, auf den kein Sonnenlicht fällt, bestätigen.

Von einigen Monden der äußeren Planeten ist bekannt oder wird vermutet, dass sie von einer Eiskruste bedeckt sind. Beispiele sind die Jupitermonde Europa, Ganymed und Kallisto, die Saturnmonde Enceladus und Titan, der Neptunmond Triton sowie der Plutomond Charon. Auch sollen einige dieser Monde unter ihrer Oberfläche Schichten aus Eismodifikationen besitzen, die nur bei hohem Druck vorkommen.

Frühe Radarbilder des Mond-Südpols aus den 1990er-Jahren mit vielen kleinen, auffallend hell erscheinenden Flecken ließen bei zahlreichen Forschern die Hoffnung aufkeimen, dass der Mond über große Wasserreserven verfüge, die unter anderem am Grund tiefer Krater als Relikte von Kometeneinschlägen überlebt haben könnten. Solche Vorkommen wären wichtige Wasser- und Sauerstoffquellen für künftige Mondbasen. Untersuchungen im Jahre 2006 mit Radioteleskopen verliefen negativ. 2009 konnte die LCROSS-Mission Wassereis nachweisen. 2010 fand die Sonde Chandrayaan-1 Hinweise auf mindestens 600 Millionen Tonnen Wasser am Nordpol des Mondes.

Schon die Römer nutzten teuer importiertes Gletschereis zur Kühlung von Speisen und zur Herstellung von Erfrischungsgetränken.

Im 19. Jahrhundert begann in Nordamerika die kommerzielle Nutzung von Wintereis, zunächst als Luxusgut für Menschen in tropischen Ländern, später auch als Massengut für den Hausbedarf. Der Eismann brachte Eisblöcke, mittels derer verderbliche Nahrungsmittel, typischerweise in einem Eisschrank, länger verzehrbar gehalten werden konnten. Mit der Elektrifizierung und Einführung des Kühlschranks fand dieses Gewerbe sein Ende. Heute wird fast das gesamte vom Menschen zu Speisezwecken genutzte Eis von Kältemaschinen oder in Kühlschränken hergestellt.

Speiseeis ist dagegen eine aus Fruchtsäften oder Milchmixgetränken hergestellte Schneemasse oder Eisschlamm.

Die entstehende Reibungswärme von Kufen auf festem Eis lässt unter einem Schlittschuh eine wenige µm dicke Wasserschicht entstehen, auf der der hintere Teil der Kufe dann vergleichsweise reibungslos gleitet. Eislauf, aber auch Skifahren, Schlittenfahren oder Schlitten als Transportmittel sind deswegen möglich. Durch den Druck unter den schmalen Kufen wird der Gefrierpunkt des Wassers nur um wenige zehntel Grad gesenkt.

Eis dient im Pflanzenbau als Frostschutz, indem Wasser bei Frost auf die Pflanzen gesprüht wird, wodurch alle Teile von einer Eisschicht überzogen werden. Durch das Gefrieren des Wassers entsteht Kristallisationswärme.

Zugefrorene Wasserflächen können einerseits die Schifffahrt behindern, andererseits aber auch Transportwege verkürzen, indem Landtransporte direkt über die Wasserfläche geführt werden können, zum Beispiel auf dem Baikalsee.

Früher wurde Eis von Eskimos auch zum Bau von Iglus verwendet.

Aus Eisblöcken werden Eisskulpturen errichtet. Auch Häuser aus Eis sind möglich.

Behindernd wirken Eisvorkommen vor allem auf den Verkehr in Form von Packeis für die Schifffahrt (siehe auch Eisbrecher), als glatter Eisfilm auf Straßen (siehe auch Schneeketten), Fußwegen oder an Flugzeugen sowie als Schneewehen bei allen Land-Verkehrsträgern. Um die Rutschgefahr zu vermindern, werden Eisflächen mit Streusand abgestumpft oder mit Streusalz weggetaut.

Eisblumen an Fensterscheiben behindern die Sicht, sind jedoch ästhetisch oft sehr reizvoll.

Auch Bauvorhaben können durch Verfestigungen des Bodens durch Eis behindert werden. Andererseits kann die Verfestigung des Bodens gewollt sein und zum Beispiel Tunnelarbeiten in losem Boden erst möglich machen. Hierbei wird die Vereisung meist künstlich mit großen Kühlaggregaten erzeugt.
In Permafrostgebieten stellt die Aufweichung des Bodens durch den fehlenden Frost eine Gefahr für Bauwerke dar. Teile der Lhasa-Bahn werden hierzu mit großen Wärmerohren (Heatpipes) gekühlt.

Wasserleitungen platzen, wenn sie unkontrolliert – etwa auf größerer Länge oder zu einer Absperrung oder einem Eispropfen hin – einfrieren. Zum Schutz werden solche Leitungen unterhalb der Frostgrenze im Boden verlegt oder ein Mindestdurchfluss sichergestellt oder rechtzeitig entleert. Wasser- und Abwasserleitungen, gelegentlich auch Regenrohre von Dächern, werden, wo sie Kälte ausgesetzt sein können, eventuell mit einer elektrischen Begleitheizung ausgeführt.

Um einen Heizkörper oder ein Stück Rohr zu tauschen, werden 2 kleine Stellen in der Vor- und Rücklaufleitung mit Eis verpfropft, indem jeweils wenige cm Länge der Rohrleitung per Kohlensäureschnee oder Kältemaschine stark von außen gekühlt werden.

Als "Kunsteis" wird eine durch technische Kühlung erzeugte Eisfläche zum Eislaufen und für Eishockey bezeichnet.

In einer am 31. Dezember 2016 eröffneten Boulderhalle in Klagenfurt wird Eisklettern durch Griffpakete aus Kunststoff simuliert, die auch einstechenden Kletterpickeln Halt geben.





</doc>
<doc id="8412" url="https://de.wikipedia.org/wiki?curid=8412" title="Prostata">
Prostata

Die Prostata (altgr. ' ‚Vorsteher‘, ‚Vordermann‘) oder Vorsteherdrüse"' ist eine akzessorische Geschlechtsdrüse aller männlichen Säugetiere einschließlich des Menschen und produziert einen Teil des Spermas. Sie liegt beim Menschen unterhalb (bei Tieren entsprechend hinter) der Harnblase und umkleidet den Anfangsteil der Harnröhre "(Urethra)" bis zum Beckenboden. Sie gleicht beim Mann in Größe und Form einer Kastanie. An die Rückseite der Prostata grenzt der Mastdarm "(Rektum)". Deshalb kann sie vom Enddarm aus mit den Fingern ertastet und beurteilt sowie in sexuellem Kontext auf diesem Weg stimuliert werden. 
Das Pendant zur männlichen Prostata ist bei weiblichen Säugetieren die Paraurethraldrüse.

Die Prostata liegt subperitoneal, das heißt unter (bei Tieren entsprechend hinter) dem Bauchfell "(Peritoneum)". Sie ruht auf dem Diaphragma urogenitale und schmiegt sich von kaudal (beim Menschen unten, bei Vierfüßern hinten) an den Fundus (klinisch auch „Hals“) der Harnblase an. Dorsal (zum Rücken hin) wird sie durch den Mastdarm "(Rektum)" begrenzt, ventral (zum Bauch hin) durch die Schambeinfuge "(Symphysis pubis)". Mit dieser ist sie durch ein Band, das "Ligamentum puboprostaticum", verbunden. Durch die Mitte der Prostata verläuft die Harnröhre "(Urethra)". Aus diesem Grund kann es bei einer krankhaften Vergrößerung der Prostata zu Problemen beim Wasserlassen bis hin zum Blasenverschluss kommen.

Durch die Prostata verläuft beim Menschen außerdem der paarig angelegte Spritzkanal (lat. Ductus ejaculatorius), durch den während der Ejakulation ca. 70 % des Ejakulat-Volumens hindurchfließen, nämlich die Fraktionen aus dem gleichseitigen Nebenhoden und aus der gleichseitigen Bläschendrüse.

Die Prostata ist eine exokrine Drüse mit Ausführungsgängen in die Harnröhre. Sie besteht aus circa 30 bis 50 Einzeldrüsen, genauer tubuloalveolären Drüsen. Diese produzieren ein Sekret, das bei der Ejakulation in die Harnröhre abgegeben wird und sich dort mit den Spermien vermischt. Das Sekret macht beim Menschen etwa 30 % des Ejakulates aus. Da die Vagina zum Schutz vor Infektionen sauer ist, erhöht das Prostatasekret mit seinem pH-Wert von 6,4 die Überlebenschancen der Spermien. Zum anderen enthält das Prostatasekret ein biogenes Amin zur Zellproliferation, welches bewegungsauslösend auf die Spermien wirkt. Des Weiteren wird aus den Epithelzellen der Prostata das prostataspezifische Antigen (PSA) sezerniert. Es handelt sich um eine Serinprotease, welche das Ejakulat durch Spaltung bestimmter Eiweiße dünnflüssiger macht. Das PSA ist ein wichtiger laborchemischer Marker für Erkrankungen der Prostata, insbesondere bei Prostatakrebs. 

Die die Prostata versorgenden Arterien entspringen hauptsächlich Ästen der "Arteria iliaca interna" (innere Beckenarterie), besonders der "Arteria vesicalis inferior" (untere Blasenarterie, bei Tieren als hintere Blasenarterie, "Arteria vesicalis caudalis", bezeichnet), aber auch der "Arteria pudenda interna" (innere Schamarterie) und "Arteria rectalis media" (mittlere Mastdarmarterie).

Die Venen der Prostata bilden einen Plexus (Geflecht) um ihre Seiten und ihre Basis. Dieser "Plexus venosus prostatae" entleert sich in die "Vena iliaca interna" (innere Beckenvene). Außerdem hat er nach kranial (kopfwärts) Verbindungen mit dem "Plexus venosus vesicalis" (Venengeflecht der Harnblase) und nach dorsal (zum Rücken) mit dem "Plexus venosus vertebralis internus" (inneres Venengeflecht der Wirbelsäule). Die Lymphdrainage der Prostata erfolgt durch Lymphgefäße, die sich in die Lendenlymphknoten und die Kreuzlymphknoten entleeren.

Die Prostata wird durch Sympathikus und Parasympathikus innerviert. Die sympathischen Fasern entstammen dem "Plexus hypogastricus inferior". Sie innervieren die Ausführungsgänge und die glatten Muskelzellen. Die parasympathischen Fasern entstammen den Rückenmarkssegmenten S2−S5. Sie verlaufen als "Nervi splanchnici pelvici" ebenfalls zum "Plexus hypogastricus inferior". Sie enden unter der Basalmembran des Epithels.

Bei den Nichtprimaten unterscheidet man vergleichend-anatomisch einen kompakten Drüsenkörper ("Corpus prostatae", fehlend bei Schafen und Ziegen) und in die Harnröhrenwand eingelagerte Einzeldrüsen ("Pars disseminata", „verstreuter Teil“, fehlt bei Pferden).

Das Corpus prostatae ist bei Hunden, Katzen, Rindern, Schweinen und Pferden in einen linken und rechten Lappen unterteilt. Beim Hund ist es relativ am größten, beide Lappen verschmelzen weitestgehend und umschließen die Harnröhre vollständig, bei den anderen Tieren liegt es jeweils seitlich an der Harnröhre und in direkter Nachbarschaft zur davor liegenden Samenblasendrüse. Bei Nagetieren besteht die Prostata aus drei paarigen Lappen, wobei der am weitesten vorn gelegene "Lobus cranialis" meist als "Koagulationsdrüse" bezeichnet wird. Die anderen beiden Lappen "(Lobus dorsalis" und "ventralis)" liegen dahinter, seitlich und ober- oder unterhalb der Harnröhre. Bei Hasenartigen lassen sich beidseits zwei Lappen "(Lobus dorsalis" und "ventralis)" unterscheiden.

Die Ausführungsgänge der Prostata münden seitlich des Samenhügels in den Beckenteil der Harnröhre.

Der Querschnitt der Prostata kann in drei Zonen unterteilt werden, die sich in den Ausführungsgängen der Drüsen unterscheiden: die periurethrale Mantelzone, die Innenzone und die Außenzone. Die Ausführungsgänge der Drüsen in der inneren Zone enden direkt in die Harnröhre. Die Drüsen in der äußeren Zone sammeln ihr Sekret in gemeinsamen Ausführungsgängen, bevor sie in der Harnröhre enden. Diese Einteilung ist bei der Entstehung von Tumoren von Bedeutung (s. unten).

Die Ausführungsgänge ("Ductuli prostatici") der Prostatadrüsen in der Prostata münden im "Sinus prostaticus" beidseits des "Colliculus seminalis" (Samenhügel) der Harnröhre. Ihr Drüsenepithel ist funktionsabhängig entweder einschichtiges Plattenepithel oder mehrreihiges hochprismatisches Epithel. Der Hohlraum (Lumen) der Drüsen enthält "Concretio prostatica", konzentriertes geschichtetes Sekret.

Das Drüsenepithel setzt sich aus drei Zelltypen zusammen: am häufigsten sind basale und luminale sekretorische Zellen, die unterschiedliche Keratin-Subtypen exprimieren und sich dadurch unterscheiden lassen. Luminale Zellen zeichnen sich zudem durch die Expression von prostataspezifischem Antigen und Androgenrezeptor aus. Sehr viel seltener kommen als dritter Typ neuroendokrine Zellen vor, die anhand der von ihnen produzierten Neuronenspezifischen Enolase und verschiedener Neuropeptide identifiziert werden können. Stammzellen in der basalen Zellschicht bilden vermutlich die Vorläufer aller dieser Zelltypen.

Zwischen den Drüsen liegen glatte Muskelzellen, die sich bei der Ejakulation zusammenziehen und so das Sekret ausstoßen, und Bindegewebe mit elastischen Fasern, das so genannte "Stroma myelasticum prostatae". 

Außen wird die Prostata von einer Bindegewebskapsel, der "Capsula prostatica" umschlossen.

In der Prostata wird ein Teil der Samenflüssigkeit produziert, die bei der Ejakulation ausgestoßen wird. Dieses Sekret bildet zusammen mit den Samenzellen aus dem Hoden, dem Sekret der Samenblase (vesicula seminalis) und dem Sekret der Bulbourethraldrüse das Sperma. Die Funktion der Prostata wird über das Hormon Testosteron reguliert.

Das Sekret der Prostata ist leicht sauer (pH-Wert etwa 6,4), dünnflüssig und trübe und gibt dem Sperma den charakteristischen Geruch. Das Sekret enthält zahlreiche Enzyme, die die Spermien für die Befruchtung benötigen. 

Das Polyamin Spermin fördert die Beweglichkeit und die Befruchtungsfähigkeit der Samenzellen. Weiterhin sind im Prostatasekret Phosphatase, Zitronensäure, Cholesterin und Zink enthalten.

Als Corpora amylacea oder "Prostatakörperchen" werden Beimengungen von Prostatasekret im Harnsediment bezeichnet.

Das Epithel (Deckgewebe), das die Harnröhre umgibt, ist entodermalen Ursprungs. Im Gegensatz dazu sind das Bindegewebe und die glatte Muskulatur, die dieses umgibt, mesodermalen Ursprungs. Das Epithel beginnt gegen Ende des dritten Schwangerschaftsmonats zu proliferieren (sich zu vermehren) und dringt in das umgebende Gewebe ein. Aus den so entstandenen Knospen bildet sich bei Männern unter Einfluss der männlichen Sexualhormone Testosteron und Dihydrotestosteron (DHT) die Prostata, in Abwesenheit dieser Hormone (unter anderem bei Frauen) die Paraurethraldrüse.

Die Untersuchungsmöglichkeiten der Prostata sind zwar mittlerweile recht vielfältig geworden, aber eine Hauptfragestellung, nämlich ob die Prostata durch einen bösartigen Tumor befallen ist oder nicht, ist nach wie vor zumindest mit den nichtinvasiven Methoden wie dem Ultraschall oder der Computertomographie nur unsicher zu beantworten. Die Prostata des älteren Mannes neigt zur Knotenbildung, und es fällt schwer, mit nichtinvasiven Maßnahmen gutartige von bösartigen Knoten zu unterscheiden. Die Elastographie ist ein neues bildgebendes Verfahren, das Tumorareale aufzeigen und gezielt Gewebeproben entnehmen kann. Auch das sogenannte HistoScanning ist ein neues, ebenfalls ultraschallgestütztes Verfahren zur Detektion von Tumorarealen, um eine gezieltere Prostatabiopsie zu ermöglichen. Bisher ist dieses Verfahren jedoch nur an wenigen Kliniken in Deutschland verfügbar. 

Die Prostata kann mit einem Finger rektal ertastet werden. Als bildgebende Verfahren finden bisher Ultraschall, Magnetresonanztomographie (MRT) und Computertomographie (CT) Anwendung. Als weiteres bildgebendes Verfahren etabliert sich die Positronen-Emissions-Tomographie (PET) mit Cholin als Radionuklid. Bei Verdacht auf Veränderungen kann eine Biopsie der Vorsteherdrüse mit einer so genannten "Prostatastanze" vorgenommen werden. Mit der Elastographie kann wegen der unterschiedlichen Härte Krebsgewebe von Normalgewebe unterschieden werden, so dass gezielt Gewebeproben entnommen werden können. Das HistoScanning misst nicht die Elastizität des Gewebes, sondern durchmustert das Gewebe und greift auf eine große Prostata-Gewebedatenbank zurück, indem mit Hilfe eines Computers ein Datenvergleich mit den Ultraschalldaten und der Datenbank erfolgt. Krebsverdächtige Strukturen werden dabei farblich gekennzeichnet und ermöglichen so eine gezielte Punktion bei der Biopsie.

Zur weiteren Diagnostik können Laborwerte wie das prostataspezifische Antigen (PSA, erhöht bei allen Erkrankungen der Prostata), die Aktivität der sauren Prostataphosphatase (erhöht bei Prostatakarzinom) und allgemeine Entzündungsmarker wie CRP und Leukozytenzahl herangezogen werden. Im Weiteren steht eine Protein-Muster-Diagnostik aus Urin zur Verfügung.

Ab 45 Jahren wird die Vorsorgeuntersuchung von den Krankenkassen übernommen, ab 40 Jahren wird sie empfohlen.

Als Prostatitis bezeichnet man eine Entzündung der Prostata. Die benigne Prostatahyperplasie (BPH) ist eine gutartige Vergrößerung der Prostata, die oft zu einer Harnabfluss-Störung bis hin zu einem lebensbedrohlichen Blasenverschluss führen kann. Prostatakrebs nennt man einen bösartigen Tumor der Prostata. Er ist das häufigste Malignom und nach Lungen- und Dickdarmkrebs die dritthäufigste krebsbedingte Todesursache bei Männern in Deutschland. Während die BPH in der Regel die zentrale "(paraurethrale)" Organzone betrifft, geht das Prostatakarzinom meist von den peripheren Drüsenanteilen aus. Beide Erkrankungen sind typischerweise Leiden des höheren Lebensalters.

Die Stelle im Rektum, von der die Prostata getastet werden kann, gilt auch als männlicher G-Punkt. Durch sexuelle Stimulation kann ein Orgasmus herbeigeführt werden, der sich von einem phallisch generierten Orgasmus deutlich unterscheidet (siehe auch Prostatamassage). Vergleichbar ist dieser Unterschied beim Mann mit dem Unterschied zwischen vaginal und klitoral hervorgerufenem Orgasmus bei der Frau. Allerdings wird die Prostata beim Mann, genau wie der G-Punkt bei der Frau, erst ab einem gewissen Erregungsgrad als erogene Zone aktiv.

"Siehe auch den Hauptartikel zur Urologie."

Die erste anatomische Beschreibung der Prostata erfolgte 300 vor Christus durch Herophilos von Chalkedon. Er war es auch, der ihr den Namen "Die Vorstehende" gab. Lange Zeit blieb dies die einzige Beschreibung. Eine anatomisch genauere Beschreibung verfasste zuerst 1536 der italienische Anatom Niccolò Massa. Andreas Vesalius' Werk "Tabulae anatomicae" aus dem Jahr 1538 enthielt erstmals Illustrationen, die die Prostata als Bestandteil des männlichen Urogenitalsystems zeigen. Dem folgten weitere genaue anatomische und physiologische Beschreibungen der Prostata. Ambroise Paré ging zwar davon aus, dass sie aus zwei Teilen besteht, machte aber ansonsten genaue Aussagen über ihre Lage zu den Ductuli ejaculatorii und ihre Rolle bei der Ejakulation. Die Erstbeschreibung der normalen Anatomie erfolgte durch Reinier De Graaf 1668.

Giovanni Battista Morgagni beschrieb 1761 in seinem Buch "De sedibus et causis morborum per anatomen indagatis" die Prostatahyperplasie. Die erste vollständige Entfernung der Prostata (Prostatektomie) zur Behandlung des Prostatakarzinoms wurde 1889 durch Vincenz Czerny in Heidelberg durchgeführt. Während er den Weg über den Damm wählte, führte Fuller ihn 1898 erstmals über einen Bauchschnitt aus. Diese Eingriffe begründeten den Beginn der Prostatachirurgie. Die erste Prostatektomie in Frankreich führte der Chirurg Antoine Gosset (1872–1944) durch.




</doc>
<doc id="8414" url="https://de.wikipedia.org/wiki?curid=8414" title="Testosteron">
Testosteron

Testosteron ist ein Sexualhormon (Androgen), das bei beiden Geschlechtern vorkommt, sich dabei aber in Konzentration und Wirkungsweise bei Mann und Frau unterscheidet. Wie bei allen Androgenen besteht das Grundgerüst des Testosterons aus Androstan (19 C-Atome). Die Vorläufer des Testosterons sind die Gestagene (21 C-Atome) bzw. Dehydroepiandrosterone (DHEA). Testosteron ist ein Kunstwort, das von "testis" (Hoden) und Steroid abgeleitet ist.

Adolf Butenandt versuchte 1930 männliches Sexualhormon aus Stierhodenextrakten und später aus Männerharn zu isolieren, was ihm für Androsteron, gemeinsam mit Kurt Tscherning 1931 gelang. Butenandt vermutete anhand seiner Analysen bereits die richtige Strukturformel für Testosteron, dessen Teilsynthese aus Cholesterol 1934 Leopold Ružička in Zürich gelang. Entdeckt und als Testosteron bezeichnet hatte es dann erstmals 1935 Ernst Laqueur, der dieses Steroidhormon aus Stierhoden isolierte.

Bei Männern wird Testosteron zum größten Teil unter dem Einfluss des LH (Luteinisierendes Hormon) in den Leydigschen Zwischenzellen im Hoden produziert. Die Nebennierenrinde bildet zwar kleine Mengen anderer Androgene, jedoch nur in sehr geringem Maße Testosteron.

Bei Frauen produzieren die Eierstöcke und die Nebennierenrinde geringe Mengen an Testosteron. In der Biosynthese des Organismus ist das Cholesterol der Präkursor (Vorstufe), bzw. das Progesteron ein Zwischenprodukt für die Testosteronsynthese.

Das Gesamttestosteron besteht zu 40 bis 50 % aus bioaktivem, d. h. Albumin­-gebundenem, Testosteron wie auch SHBG­-gebundenem Testosteron (50 bis 60 %) und freiem Testosteron (1 bis 2 %).

Testosteron hat verschiedene Wirkungen auf diverse Organe. Es bewirkt z. B. die Entstehung des männlichen Phänotyps, ist für das Wachstum (insbesondere den Aufbau von Muskelmasse und Fettspeicher) mit verantwortlich und sorgt für die Spermienproduktion.

Testosteron wird, an ein Protein gebunden, über das Blut auch zu vielen anderen Zielorganen transportiert, die Rezeptoren für dieses Hormon haben. Das Transportprotein heißt Sexualhormon-bindendes Globulin (SHBG). 

Im Körper wird ein Teil des Testosterons in den Zielzellen durch das Enzym Steroid-5"α"-Reduktase (SRD5) zu dem biologisch noch aktiveren Dihydrotestosteron (DHT) metabolisiert. 

Über eine negative biologische Rückkopplung ("Feedback") hemmt Testosteron in der Hirnanhangsdrüse die Sekretion von Luteinisierendem Hormon (LH) und im Hypothalamus die des Gonadoliberins, welches auch Gonadotropin-releasing-Hormon (GnRH) genannt wird.

Testosteron wird über das Androgenbindungsprotein (ABP) der Sertoli-Zellen zu den Samenkanälchen transportiert. Hier bewirkt es die Reifung der Spermatiden zu Spermien (siehe auch Hodenfunktion). Darüber hinaus bewirkt Testosteron bei männlichen Individuen in der Pubertät die Entwicklung des Penis, Hodensacks, der akzessorischen Geschlechtsdrüsen sowie der sekundären Geschlechtsmerkmale und sorgt bei Erwachsenen für die Aufrechterhaltung dieser Merkmale.

Außerhalb der Geschlechtsorgane fördert das Hormon das Wachstum der Körperbehaarung und der Barthaare (aber nicht der Kopfhauptbehaarung; siehe auch Haarausfall) und besitzt eine anabole, das heißt muskelaufbauende Wirkung. Des Weiteren verstärkt Testosteron die Knorpel- und Knochenneubildung, ähnlich wie Thyroxin. Ein hoher Testosteronspiegel fördert das Entstehen bzw. die Steigerung sexuellen Verlangens (Libido) und generell Antrieb, Ausdauer und „Lebenslust“ sowie dominante und aggressive Verhaltensweisen. Schließlich kommt es durch Testosteronwirkung zu einer Vermehrung der roten Blutkörperchen (Erythrozyten) durch die Stimulation der Freisetzung von Erythropoetin in der Niere und die Aktivierung des Knochenmarks.<ref name="DOI10.1056/NEJMoa1206168">Joel S. Finkelstein, Hang Lee u. a.: "Gonadal Steroids and Body Composition, Strength, and Sexual Function in Men." In: "New England Journal of Medicine." 369, 2013, S. 1011–1022, .</ref>

Künstliche Testosteronzufuhr bei Frauen kann zu einer Vermännlichung (Stimme, Muskulatur, Gesichtszüge, Behaarung) und Vergrößerung der Klitoris führen, welche sich nach Absetzen oft nicht mehr vollständig zurückbildet (abhängig von der Dauer, Höhe der Dosis und individueller Veranlagung).

Als verhaltensbiologische Wirkungen bei Tieren wurden Imponiergehabe, Kampfverhalten sowie Begattungsdrang erforscht und beobachtet. Dies wurde u. a. durch Kastration und anschließende Hormonzufuhr an Tieren (aggressive Hengste werden zu sanften, angepassten Wallachen) nachgewiesen. 

Bei Menschen ist der Einfluss des Hormons auf das Verhalten weniger etabliert als bei Tieren. Eine systematische Übersichtsarbeit zur Beziehung zwischen Testosteron und antisozialem Verhalten ergab, dass ein hoher Testosteronspiegel zu einer beeinträchtigten Regulation emotionaler und motivationaler Prozesse, geringerer sozialer Sensibilität und starker Belohnungsmotivation führt. Ob sich das in antisozialem Verhalten äußert, hängt jedoch von einer Reihe sozialer und genetischer Faktoren ab. Eine Metaanalyse von insgesamt 45 Studien zum Verhältnis zwischen Testosteron und Aggressivität bei Menschen ergab hingegen einen schwachen, aber signifikanten positiven Zusammenhang zwischen Aggressivität und Testosteron. Zwei systematische Übersichtsarbeiten kamen zu dem Schluss, dass es nicht allein Testosteron ist, das aggressives Verhalten steigert, sondern das Verhältnis von Testosteron zu Cortisol. Ein hoher Testosteronspiegel gepaart mit einem niedrigen Cortisolspiegel sei besonders stark mit Aggressivität assoziiert. Eine Studie aus dem Jahr 2012 zeigte, dass subjektiv empfundene Wut mit erhöhtem Testosteron zusammenhing, nicht jedoch mit erhöhtem Cortisol.

Einzelne Untersuchungen kommen zu dem Ergebnis, dass Testosteron dissoziales Verhalten wie egozentrische Entscheidungen fördert und kognitive Empathie verringert. Andere Einzelstudien kamen zu umgekehrten Ergebnissen, so zum Beispiel dass die Gabe von Testosteron die Tendenz zum Lügen bei Männern reduziert. Eine weitere Studie an Männern kam zu dem Resultat, dass exogenes Testosteron aggressives, anti-soziales Verhalten bei Verhandlungen signifikant erhöhen kann. Männer, denen Testosteron verabreicht wurde, behielten im Vergleich zur Placebo-Gruppe 27 % mehr Geld für sich in Verhandlungssituationen.

Eine Untersuchung zeigte, dass Testosteron bei Frauen dazu führt, dass die Versuchsteilnehmenden fairere Angebote in einem Verhandlungsexperiment machten. Die Forscher erklären diese Wirkung damit, dass das Hormon die Sensitivität für den Status erhöht, und vermuteten, dass in der sozial komplexen Umwelt des Menschen nicht Aggression, sondern pro-soziales Verhalten den Status sichert. Bei Frauen im mittleren Lebensalter gehen erhöhte Testosteronwerte mit einem höheren Risiko für eine Depression einher.

Der Ausgangsstoff für die Testosteronbiosynthese in den Leydig-Zellen ist Cholesterol (Cholesterin), welches über zwei verschiedene Wege verarbeitet werden kann. Den letzten Schritt, die Reduktion von Androstendion, katalysiert das Enzym Testosteron-17β-Dehydrogenase.

Bei Tieren, bei denen Mehrlingsgeburten die Regel sind, also auch den meisten Säugetieren, ist der adulte Testosteronspiegel maßgeblich von der Position der Föten im Uterus abhängig. In der Gebärmutter zwischen zwei Weibchen liegende Föten weisen später eine niedrigere Testosteronkonzentration auf als solche, die zwischen zwei männlichen Geschwistern liegen. Bei adulten Tieren zeigen sich infolgedessen sehr unterschiedliche Verhaltensmuster.

Der Testosteronspiegel im Blutserum eines gesunden Mannes unterliegt tageszeitlichen Schwankungen und folgt einer circadianen Rhythmik, wobei der Wert frühmorgens ein Maximum und nachmittags ein Minimum durchläuft. Abhängig vom Alter schwanken die Werte morgendlicher Konzentration des Gesamttestosterons dabei bei Jungen vor der Pubertät zwischen 1 und 4 nmol/l und nach Erreichen der Geschlechtsreife zwischen 13 und 23 nmol/l mit einem Mittelwert von etwa 16 für "ältere Männer" und etwa 18 für "jüngere Männer". Der Normbereich liegt dabei für alle Männer zwischen 12 und 40 nmol/l (12 und 30 nmol/l). 

Es besteht offensichtlich eine Korrelation zwischen der Schlafdauer und dem Testosteronspiegel. So stieg in einer mit 800 gesunden Männern aller Altersstufen durchgeführten Studie der Testosteronspiegel mit zunehmender durchschnittlicher Schlafdauer (gemessen über drei Wochen) zuerst an, erreichte nach etwa acht Stunden einen Höhepunkt und fiel unmittelbar danach überraschenderweise wieder stark ab. Während der Anstieg mit einer vermehrten Hormonproduktion, die vor allem im Schlaf stattfindet, erklärt wird, ist der Abfall bisher noch ungeklärt.

Sinkt der Wert des Testosteronspiegels eines geschlechtsreifen Mannes unter 12 nmol/l (12 nmol/l = 3,5 ng/ml = 346 ng/dl), so gilt er als behandlungsbedürftig. Bei Serumspiegeln unter 12 nmol/l kann eine Substitutionstherapie erwogen werden, und bei Werten kleiner 8 nmol/l wird sie in der Regel empfohlen und angewendet. Bei Vorliegen eines Hypogonadismus kann auch bei höheren Spiegeln eine Substitutionstherapie angezeigt sein. 

Die häufigste Form des Testosteronmangels ist der altersassoziierte Hypogonadismus (Late-Onset Hypogonadism [LOH]) im sogenannten Klimakterium virile, von dem 3–7 % aller Männer zwischen 30 und 70 Jahren betroffen sind, bei über 70-jährigen sind es ca. 18 %. Testosteronmangel bei alternden Männern wird unter dem Begriff Testosteronmangelsyndrom bzw. dem englischen Fachbegriff "partial androgen deficiency in the aging male" (abgekürzt mit PADAMEs, PADAM) beschrieben. Eine Studie der Universität Manchester von 2010 stellt allerdings in Frage, ob die beklagten vielschichtigen Symptome überhaupt mit der Höhe des Testosteronspiegels korrelieren und ob eine Testosteronsubstitution ursächlich hilft. Zwischen den vermeintlichen Wechseljahresbeschwerden bei Männern in fortgeschrittenem Alter und einem niedrigen Testosteronspiegel konnten die Forscher keinen Zusammenhang feststellen.

Der Testosteronspiegel wird vorwiegend aus dem Blutserum bestimmt. Auf Grund der tageszeitlichen Schwankungen des Testosteronspiegels findet die Blutabnahme in den Morgenstunden statt. Die Bestimmung erfolgt meist per Immunassay-Methodik. Da diese bei niedrigen Testosteronwerten ungenaue Ergebnisse liefert, wird in einem solchen Fall eine Kombination aus Extraktion, chromatographischen Methoden und nachfolgender Immunoassay- oder massenspektrometrischer Bestimmung empfohlen. Flüssigchromatographie mit Massenspektrometrie-Kopplung ermöglicht die Messung verschiedener Steroide in derselben Probe.

Das Hormon Dihydrotestosteron (DHT) ist ein Abbauprodukt von Testosteron. Beim erblich bedingten Haarausfall kann eine vererbte Überempfindlichkeit der Haarwurzeln gegenüber DHT bestehen und zu einer fortschreitenden Verkleinerung der Haarwurzeln führen. Eine Behandlungsmöglichkeit besteht in Medikamenten zur Senkung des DHT-Spiegels.

Ein Testosteronmangel kann unter anderem zu Erektionsstörungen und Osteoporose führen. Reicht die natürliche Produktion von Testosteron nicht aus, kann eine Langzeittherapie mit von außen zugeführtem Testosteron erfolgen.

Als Ersatztherapie gibt es folgende Möglichkeiten:

Anmerkungen:

Nach diagnostizierter Transsexualität wird Transmännern Testosteron verabreicht. Dies führt zu einer allgemeinen Vermännlichung mit entsprechender Gesichts- und Körperbehaarung, Veränderung der Fettverteilung, Muskelaufbau und einer wahrnehmbaren Veränderung der Stimmlage.

Manche Bodybuilder sowie Ausdauersportler verwenden Testosteron als Dopingmittel, um ihren Muskelaufbau zu beschleunigen oder die natürliche Leistungsgrenze zu überwinden. Dabei besteht aber die Gefahr, eine überhöhte Dosis zu verwenden, die zu ernsthaften, womöglich dauerhaften urologischen Problemen führen kann. Gebräuchlich sind synthetische Testosterone in Form kurzkettiger (Propionat), mittelkettiger (Enanthat/Cypionat) und langkettiger Ester (Undecanoat, Buciclat), wobei der größte Teil über den Schwarzmarkt bezogen wird. Bei diesen Produkten besteht unter anderem die Gefahr der Verunreinigung, der falschen Dosierungen und der Leberschädigung.

Mögliche Nebenwirkungen, vor allem bei Zufuhr synthetischer Testosterone und Verwendung hoher Dosen, sind:



Monopräparate
Andriol (A, CH), Androgel (A), Androtop (D), Axiron (D), Intrinsa (D, A), Livensa (A), Nebido (D, A, CH), Testim (D), Testogel (D, A, CH), Testopatch (D), Testotop (D), Testoviron (CH), Tostran (D, A, CH)



</doc>
<doc id="8423" url="https://de.wikipedia.org/wiki?curid=8423" title="König (Begriffsklärung)">
König (Begriffsklärung)

König bzw. Koenig steht für:


Von dem Familiennamen König abgeleitet sind:


Könige steht für

Siehe auch:



</doc>
<doc id="8426" url="https://de.wikipedia.org/wiki?curid=8426" title="Tragwerk (Bauwesen)">
Tragwerk (Bauwesen)

Tragwerk ist im Bauwesen eine Bezeichnung für das statische Gesamtsystem der Tragglieder, die maßgeblich für die Standsicherheit eines Bauwerks sind.

Das Tragwerk eines Gebäudes besteht in der Regel aus Decken, Balken, Stützen, Wänden und der Gründung.

Die Baustatik kennt zwei große Gruppen von Tragwerken:





</doc>
<doc id="8427" url="https://de.wikipedia.org/wiki?curid=8427" title="Gründung">
Gründung

Gründung bezeichnet:

Umgangssprachlich: []
Siehe auch:



</doc>
<doc id="8428" url="https://de.wikipedia.org/wiki?curid=8428" title="Flachgründung">
Flachgründung

Unter einer Flachgründung wird im Bauwesen eine Form der Gründung verstanden, bei der die Bauwerkslasten direkt unterhalb des Bauwerks in den Untergrund geleitet werden.

Bei der Flachgründung muss darauf geachtet werden, dass sie mindestens bis unter die Frostgrenze (in Deutschland mindestens 80 cm) in den Boden einbindet. Dadurch wird verhindert, dass Hebungen und Setzungen, und damit Risse, beim Gefrieren des Bodens entstehen.

Je nach Anwendungsfall und nach Kostenoptimierung wird eine der folgenden Flachgründungen angewendet.

Diese werden unter Säulen und Stützen angeordnet, wobei hier aus sehr große Lasten abgeleitet werden können.
Beim Fertigteilbau werden die Fertigteilstützen in diese Köcher eingestellt und dann mit Mörtel vergössen.
Unter Mauern und Lasten in kurzen Abständen eignen sich Streifenfundamente bestens zur Lasteinleitung in den Untergrund.
Müssen die Bauwerkslasten verteilt werden um gleichmäßig in den Untergrund eingeleitet zu werden, ist die Fundamentplatte besonders wirtschaftlich. Diese hat auch den Vorteil, dass zugleich ein Boden geschaffen wird, auf dem gearbeitet werden kann.
Bei vorhandenem Grundwasser wird dieses seitlich von der Wanne zurückgehalten, wobei dabei auch auf die Dichtheit der Wanne zu achten ist. Dazu gibt es umfangreiche Vorschläge und Richtlinien.

Grundsätzlich sollte aus wirtschaftlichen Gründen zunächst untersucht werden, ob die einfachere Form der Flachgründung gewählt werden kann.

Eine Tiefgründung wird in der Regel nur gewählt, wenn die tragfähigen Bodenschichten erst in größeren Tiefen anstehen.

Die Tragfähigkeit der Flachgründung kann über die Fundamentgrößen und über die Einbindetiefen gesteuert werden. Diese sind so zu wählen, dass sowohl die Standsicherheit gewährleistet ist als auch die Verformungen verträglich sind.

Bei tragfähigen Bodenschichten in leicht erreichbaren Tiefen kann auch ein Bodenaustausch wirtschaftlicher als eine Tiefgründung sein.

Zwischen Flach- und Tiefgründung gibt es auch noch Zwischenlösungen wie z. B. Pfeiler- oder Brunnengründungen. Hier wird vorher punktuell der nicht tragfähige Boden unverbaut oder verbaut ausgehoben und durch unbewehrten Beton ersetzt.

Bei erforderlichem Verbau werden hierfür gerne auch Schachtringe verwendet (Brunnengründung). Sie werden von innen ausgeschachtet und dadurch gleichzeitig bis auf tragfähige Bodenschichten abgesenkt. Sie werden anschließend mit unbewehrtem Beton verfüllt.

Bekanntestes Beispiel für eine misslungene Flachgründung ist der Schiefe Turm von Pisa.



</doc>
<doc id="8429" url="https://de.wikipedia.org/wiki?curid=8429" title="Tiefgründung">
Tiefgründung

Die Tiefgründung beschreibt ein Bauverfahren, um die Bauwerkslasten nicht direkt unterhalb des Bauwerks in den Untergrund zu leiten (wie bei der Flachgründung), sondern über zusätzliche senkrechte Elemente tiefer in die Erde abzuleiten und dort abzutragen. Eine Tiefgründung wird dann erforderlich, wenn die oberflächennahen Schichten nicht tragfähig genug sind.

Man unterscheidet die Tiefgründungen nach ihren Tragelementen:

Pfähle sind die älteste Tiefgründung. Es gibt die verschiedensten Arten, die nach folgenden Kriterien eingeteilt werden:





Bei Brunnengründungen wird ein kreisförmiger oder elliptischer Querschnitt mit den Grundrissabmessungen von 2 bis 10 m in den Untergrund bis in große Tiefen hergestellt (abgeteuft). Damit ist es möglich, sehr hohe Bauwerkslasten in den tieferen Untergrund abzuleiten. Besonders geeignet sind Brunnengründungen zum zusätzlichen Ableiten von Horizontalkräften wie Erddruck und auch von Biegemomenten aus Säulen und Stützen.
Bei der Herstellung werden die unterschiedlichsten Verfahren angewendet, wobei der Unterschied hauptsächlich in der Art der Stützung des Erdreiches besteht.

Bei der Brunnengründung werden entsprechend den statischen Erfordernissen die zu übertragenden Lasten und Momente in den Brunnenkörper eingeleitet. Dazu wird der Innenraum mit Stahlbeton oder Stahlfaserbeton ausgefüllt.

Neben der Pfahlgründung sind Senkkästen die älteste Art von Tiefgründungen. Schon im Altertum wurden Hafenbauten und Brücken mittels Senkkästen gegründet. Heute werden Senkkästen vornehmlich für Bauwerke im und unter dem Grundwasserbereich verwendet. Dabei werden sie im Endausbau für die unterschiedlichsten Aufgaben eingesetzt. So sind Pumpstationen eines Grundwasserwerkes mittels Senkkasten hergestellt worden. Auch ganze U-Bahn-Strecken (z. B. Amsterdam) wurden mittels Senkkasten hergestellt.

Es gibt bei der Herstellung zwei unterschiedliche Arten:



</doc>
<doc id="8430" url="https://de.wikipedia.org/wiki?curid=8430" title="Schlitzwand">
Schlitzwand

Eine Schlitzwand ist eine Schutzwand aus Ortbeton oder Dichtungsmaterial, die abschnittsweise in einem Bodenschlitz hergestellt wird, der durch eine Stützflüssigkeit vor dem Zusammenfall gesichert wird. Die Schlitzwand ist als Baugrubensicherung von tiefen Baugruben oder für Tiefgründungen gebräuchlich. Als Sonderform dient sie an Deponien oder Tagebauen als Dichtwand.

Die Schlitzwand wurde in den 1930er Jahren mit grundlegenden Untersuchungen über die Stützfunktion von Bentonit-Suspensionen durch Christian Veder entwickelt und in den 1950er Jahren bei einer italienischen Baufirma zur Einsatzreife gebracht. Eine Schlitzwand, die zunächst als Baugrubensicherung errichtet wurde, kann – wie auch die Bohrpfahlwand – später Bestandteil des zu errichtenden Gebäudes werden, z. B. als Kellerwand oder Tiefgaragenwand.

Zur Errichtung der Schlitzwand wird im Boden zunächst mit einem Schlitzwandgreifer oder einer Schlitzwandfräse ein Schlitz ausgehoben. Zur Stabilisierung des Schlitzes wird eine stützende Flüssigkeit (in der Regel eine Bentonitsuspension, eine Mischung aus Bentonit und Wasser) in den Schlitz eingefüllt. Soll die Schlitzwand gegen Grundwasser abdichten, so wird der Schlitz bis in eine wassersperrende Bodenschicht geführt, z. B. Ton. Beidseits des späteren Schlitzes werden Leitwände erstellt, die den Schlitzwandgreifer führen und die obersten ca. 1 bis 2 Meter der Schlitzzone sichern.

Nach Ausheben der vollen Schlitztiefe wird bei Ortbetonschlitzwänden ein Bewehrungskorb eingeführt und die stützende Flüssigkeit durch Beton oder Stahlfaserbeton ersetzt. Hierzu wird ein Rohr "(Kontraktorrohr)" bis kurz vor den Boden des Schlitzes herabgelassen, wodurch der Beton direkt auf den Grund des Schlitzes gelangt. Der von unten nach oben aufsteigende Beton verdrängt die Stützflüssigkeit, die abgepumpt, über eine Regenerationsanlage entsandet und für den weiteren Gebrauch aufbereitet wird. Übliche Schlitzwanddicken liegen zwischen 50 und 120 Zentimeter.

Im Tunnelbau und im Hochbau werden Schlitzwände auch als Seitenwände für die sogenannte Deckelbauweise verwendet.

Eine Variante der Schlitzwand ist die Drainagewand, die gleichzeitig zum Abführen des Grundwassers verwendet werden kann.

Eine weitere Variante ist die Fräswand bei der das Ausheben des Schlitzes entfällt, da der anstehende Boden gleich beim Fräsen mit Zementleim vermischt wird. Dadurch verfestigt sich das Erdreich selber zu Beton. Die Betonqualität ist dabei natürlich von der Zusammensetzung der Bodenschichten abhängig. Das Einbringen von Bewehrungsstahl in den frischen Beton ist nur eingeschränkt oder gar nicht möglich. Dieser Nachteil kann teilweise durch die Zugabe von Fasern ausgeglichen werden.

Bei einer Einphasenschlitzwand, welche auch als Dichtwand bekannt ist, wird der stützenden Flüssigkeit ein Bindemittel (in der Regel Zement) zugesetzt, so dass die stützende Flüssigkeit ohne Austausch erhärtet. Eine Anwendung für diese Bauweise ist eine nachträglich zu erstellende Deponieabdichtung.

Um die Funktion eines Baugrubenverbaus wahrnehmen zu können, werden in Dichtwände vor dem Ansteifen der Zement-Bentonit-Suspension auch Spundbohlen als spätere Trag- und Dichtungselemente eingestellt bzw. eingehängt.

Schlitzwände werden auch im Braunkohletagebau in der Lausitz verwendet. Um die Kohleflöze in 50 m bis 70 m Tiefe aufzuschließen, ist eine Grundwasserabsenkung bis zu dieser Tiefe notwendig. Durch Abpumpen würde sich ein weit reichender Grundwasser-Trichter ausbilden – als Parabel mit einer durchschnittlich anzunehmenden Neigung von 1/10. Als Schutzmaßnahme werden um die Tagebaue 70 m bis 90 m tiefe Schlitzwände als Dichtwände angelegt, sodass die Grundwasserabsenkung räumlich begrenzt ist und somit Setzungsschäden an Gebäuden und Beeinträchtigungen von Gewässern verringert werden. Im Bereich der Schlitzwand entsteht ein Grundwassersprung. Ein Beispiel für den Schutz des Grundwasserpegels durch Schlitzwände ist die Ostseite des Tagebaues Jänschwalde. Dieser Tagebau liegt unmittelbar an der Grenze zu Polen. Da eine Grundwasserabsenkung auf das Gebiet der Bundesrepublik beschränkt bleiben musste, wurde hier eine Schlitzwand notwendig.


</doc>
<doc id="8431" url="https://de.wikipedia.org/wiki?curid=8431" title="Stahlbeton">
Stahlbeton

Stahlbeton, ein künstlicher Baustoff im Stahlbetonbau, ist ein Verbundwerkstoff aus den beiden Komponenten Beton und Bewehrungsstahl. Der Verbund beider Komponenten entsteht durch die Verklebung des Bindemittels Zement mit der Rippung des runden Bewehrungsstahls.

Beton hat im Vergleich zur Druckfestigkeit nur eine Zugfestigkeit von etwa 10 %. Stahl besitzt dagegen eine hohe Zugfestigkeit. Das Tragprinzip beim Baustoff Stahlbeton ist es daher, auf Zug beanspruchte Stellen eines Bauteils mit Stahl zu verstärken (z. B. bei Balken im Feldbereich unten), also zu bewehren, und in den übrigen Bereichen die Druckfestigkeit des Betons auszunutzen (z. B. bei Balken im Feldbereich oben). Bei stark auf Druck beanspruchten Bauteilen (z. B. Stützen) wird der Stahl (Bewehrung) auch zur Erhöhung der Druckfestigkeit herangezogen, also auf Druck beansprucht.

Stahlbeton ist mit über 100 Millionen verbauten Kubikmetern im Jahr der wichtigste Baustoff Deutschlands. 12 % der deutschen Stahlproduktion werden jährlich zu rund 6 Millionen Tonnen Betonstahl verarbeitet. Der Einsatz von Stahlbeton statt des unbewehrten Betons ist notwendig, wenn in einem Bauteil Zugspannungen auftreten, die zu einem schlagartigen Versagen der Gesamttragfähigkeit führen könnten. Im Vergleich zu anderen Baustoffen, wie Stahl, Holz oder Kunststoff, ist seine Anwendung immer dann sinnvoll, wenn keine filigranen und leichten Tragstrukturen notwendig sind. Wie der Einsatz beim Bau von Bunkern zeigt, ist Stahlbeton bei ausreichenden Abmessungen auch für extreme Einwirkungen geeignet. Vorteilhaft sind insbesondere die Nichtbrennbarkeit und der hohe Feuerwiderstand. Grenzen bei der Benutzung des Baustoffes ergeben sich aus dem hohen Eigengewicht des Betons, was als tote Last die erforderliche Betonstahlmenge vergrößert und bei schlanken Konstruktionen infolge der Rissbildung zu großen Verformungen führt. In diesen Fällen ist der Einsatz einer Verbundkonstruktion oder von Spannbeton geeigneter. Der Spannbeton unterscheidet sich vom Stahlbeton durch eine planmäßige Vorspannung (= Vordehnung) der Stahleinlagen, der so genannten Spannglieder. Damit wird eine zusätzliche äußere Drucklängskraft aufgebracht, wodurch die Zugspannungen überdrückt werden und eine Rissbildung, somit die Bauteilverformung, stark reduziert wird.

Typische Stahlbetonbauteile sind unter anderem biegebeanspruchte Bauteile, wie Decken, Balken oder Bodenplatten. Aber auch massive, großvolumige Bauteile wie Brückenpfeiler oder Stützwände werden im Regelfall mit diesem Material hergestellt.

Im 17. Jahrhundert entwickelte der Mathematiker Jakob I. Bernoulli das Prinzip der Balkentheorie. Er schuf damit die Voraussetzung für das Verständnis von Kraftverläufen in auf Biegung beanspruchten Bauteilen.

Grundlagen der Entwicklung des Stahlbetons waren die Erfindung des Romanzements im Jahre 1798 durch den Engländer J. Parker, des künstlichen hydraulischen Kalkes 1817 durch den Franzosen Louis-Joseph Vicat, des Portlandzements durch den Engländer Joseph Aspdin im Jahre 1824 sowie die Entdeckung der Bedeutung des Sinterns 1840/1844 durch Vicat und Isaac Charles Johnson.

In der Mitte des 19. Jahrhunderts wurden erstmals in Frankreich Betonbauteile durch Stahleinlagen verstärkt. 1848 baute Joseph-Louis Lambot ein Boot aus eisenverstärktem Zementmörtel, das er 1855 patentieren ließ. Seit 1861 stellte der Gärtner Joseph Monier Pflanzkübel aus Zementmörtel her, die er mit einem Eisengeflecht verstärkte, damit sie nicht so leicht zerbrachen. 1867 erhielt er darauf ein Patent. Der Begriff "Moniereisen" wird auch heute noch verschiedentlich verwendet. Ältere Bezeichnungen für Stahlbeton sind "Eisenbeton" (heute auch noch im Russischen und Bulgarischen üblich) und "Monierbeton". Bereits 1861 veröffentlichte François Coignet Grundsätze für die Verwendung von bewehrtem Beton und stellte 1867 auf der Weltausstellung in Paris Träger und Röhren aus bewehrtem Beton aus. Schon 1852 hatte Coignet in Saint-Denis ein Gebäude mit Beton und Eisenprofilen gebaut.
Der Gutspächter Joseph-Louis Lambot meldete 1855 ein Patent für einen neuen Holzbauwerkstoff an, den er „Ferciment“ nannte. Seiner Patentschrift kann folgendes entnommen werden: Dieses Patent wurde dann von Coignet erweitert.

Parallel zu den französischen Ingenieuren führte der amerikanische Rechtsanwalt Thaddeus Hyatt seit 1855 Versuche über die Verwendung von Eiseneinlagen in Beton durch. In seinem Grundpatent von 1878 schrieb er: "„[…] Hydraulic cements and concretes are combined with metal bars and rods, so as to form slabs, beams and arches. The tensible strength of the metal is only utilized by the position, in which it is placed in slabs, beams etc. […].“" Hyatt hatte die Tragwirkung erkannt. Auch der englische Bauunternehmer William Boutland Wilkinson erhielt schon 1854 ein Patent auf Eisenbeton und verwendete es um 1860 für Decken in Häusern.

In Deutschland erwarben 1885 Conrad Freytag und Gustav Adolf Wayss die Monierpatente. Im gleichen Jahr traf Wayss den Regierungsbaumeister Matthias Koenen, dem die Leitung des damals im Bau befindlichen Reichstagsgebäudes unterlag. Nach dem Ausräumen von Bedenken wegen der Korrosionsgefahr, Haftfestigkeit und unterschiedlicher Temperaturdehnungen sowie aufgrund von Versuchen entschloss sich Koenen, das neue System für Wände, Deckenplatten und Gewölbe anzuwenden. Seine Erkenntnisse veranlassten ihn eine Broschüre zu verfassen, die Wayss 1887 unter dem Titel „Das System Monier in seiner Anwendung auf das gesamte Bauwesen“ herausgab. Dennoch war der im Reichstagsgebäude vorrangig eingesetzte Baustoff Mauerwerksziegel, die für Fundamente und Pfeiler sowie ebenfalls für Wände und Gewölbedecken verwendet wurden.

Ein weiterer Pionier des Eisenbetonbaus war der Bauingenieur François Hennebique, der 1892 ebenfalls ein Patent auf Stahlbeton erhielt und sowohl im Brücken- als auch im Wohnungsbau Meilensteine setzte, unter anderem mit der Erfindung des Plattenbalkens. Das von ihm lizenzierte „System Hennebique“ wurde u. a. von Eduard Züblin und Max Pommer übernommen, die – wie Hennebique selbst – nach dieser Methode Ende des 19. Jahrhunderts die ersten reinen Stahlbetonbauwerke in Europa errichteten und sich nicht nur auf Gebäudeteile beschränkten. Pioniere des Eisenbetonbaus in Russland waren Nikolai Beleljubski und Artur Loleit.

Wenig später brachte Emil Mörsch eine erste wissenschaftlich begründete Darstellung der Wirkungsweise des Eisenbetons. Die wurde 1902 veröffentlicht. Dazu führte Emil Mörsch als einer der Ersten umfangreiche Versuchsreihen durch. Er war schließlich von 1916 bis 1948 Professor für Statik der massiven Tragwerke, gewölbten Brücken und Eisenbetonbau an der Technischen Hochschule Stuttgart und hat dort die Bemessungsverfahren für Stahlbeton entscheidend mitgeprägt. 1920 kam es zur Einführung des Begriffes Stahlbeton. 1942 folgte die Umbenennung des Deutschen Ausschusses für Eisenbeton in Deutscher Ausschuss für Stahlbeton und dementsprechend der Ersatz der DIN 1045 von 1937 "Bestimmungen des Deutschen Ausschusses für Eisenbeton" im Jahr 1943 durch die "Bestimmungen des Deutschen Ausschusses für Stahlbeton".

Monier errichtete 1875 bei Chazelet seine erste Eisenbetonbrücke, die 16,5 m Stützweite hatte und in der Schweiz entstand 1890 auf dem Areal der Jura-Cement-Werke in Möriken-Wildegg über einen Fabrikkanal eine 37,2 m weit spannende Bogenbrücke nach dem System Monier. In den 1890er Jahren wurden in Europa und den Vereinigten Staaten die ersten Brücken mit einbetonierten Eisenträgern nach einem System von Joseph Melan gebaut, 1899 mit der Georgsbrücke in Meiningen die erste in Deutschland. Der 1900 freigegebene Pont Camille-de-Hogues gilt weltweit als die erste größere Eisenbetonbrücke. Sie wurde von Hennebique entworfen; die erstmals Stützweiten von 100 m überwindenden Bogenbrücken Ponte del Risorgimento 1911 und Langwieser Viadukt 1914 wurden ebenfalls nach seinem System konstruiert. 1942 erreichte der Martín-Gil-Viadukt 210 m, 1964 die Gladesville-Brücke 305 m und 1980 die Krk-Brücke 390 m. Seit 1995 hat die Wanxian-Brücke mit 420 m den größten Betonbogen.

Zu den ersten Stahlbetonhochbauten in Deutschland zählt das Gebäude der „Königlichen Anatomie“ in München, erbaut von 1905 bis 1907 nach Plänen des Architekten Max Littmann. In den USA entstand 1902 mit dem 16-stöckigen Ingalls Building in Cincinnati das erste Hochhaus und 1903–1904 mit dem Packard-Automobilwerk der erste Fabrikbau in Eisenbeton.

Der 1956 eröffnete Stuttgarter Fernsehturm wurde als erster großer Funkturm der Welt in Stahlbetonbauweise errichtet und dient seither als Vorbild für zahlreiche weitere Funk- und Fernsehtürme.

"Beton" ist ein künstliches Gestein aus Zement, Gesteinskörnung (Sand und Kies oder Splitt), gegebenenfalls Zusatzmitteln und Wasser. Dieser Baustoff ist preiswerter als metallische Baustoffe (beispielsweise Stahl) herzustellen, je nach Konsistenz relativ einfach formbar und wegen seines verhältnismäßig günstigen Preises besonders geeignet für massive, großvolumige Bauteile, wenn bestimmte Randbedingungen, wie z. B. die Hydratationswärme oder Entmischung durch Schütthöhen besonders beachtet werden. Ein wichtiger Einsatzbereich ist auch der Bau im Wasser (sofortige Wasserbeaufschlagung durch hydraulische Aushärtung möglich), wobei hier weiches Wasser oder chemische Belastungen besonders zu beachten sind.

Seine mechanischen Eigenschaften sind gekennzeichnet durch eine relativ hohe Druckfestigkeit sowie eine niedrige Zugfestigkeit (ungefähr 10 % der Druckfestigkeit).

Betonstahl, auch als Bewehrungsstahl bezeichnet, ist ein spezieller, heutzutage gerippter oder profilierter Rundstahl mit einer hohen Zugfestigkeit (formula_1 = 500 N/mm). Dieser wird in die Schalung des Bauteils eingebaut und anschließend einbetoniert. Damit sich die Bewehrungsstäbe im fertigen Betonteil an der planmäßigen Stelle befinden und während des Betonierens nicht verschieben, werden sie mit Hilfe von Bindedraht untereinander zu einem Korb fixiert (zusammengerödelt). Beim Einfüllen des Betons, dem Betonieren, wird der Betonstahl durch den Beton komplett umhüllt, was den Verbund zwischen den beiden Baustoffen bewirkt. Um eine Mindestdicke an Beton zwischen der Stahlbewehrung und der Außenfläche des Betonteiles sicherzustellen, werden zwischen der Bewehrung und der unteren oder seitlichen Schalung Abstandshalter aus geeignetem Material (Kunststoff, Beton) eingebaut und mit einbetoniert.

Der Beton muss den Bewehrungsstahl zum Korrosionsschutz mit einer bestimmten, in den Normen festgelegten, Überdeckung einschließen. Dazu sind Unterstützungen und Abstandhalter einzubauen. Diese stellen den Abstand zwischen dem Betonstahl und der Schalung und damit der späteren Betonoberfläche sicher.

Der Verbund zwischen dem Beton und dem Betonstahl entsteht durch die Haftung des Bindemittels Zement (Haftverbund), durch die Reibung zwischen Stahl und Beton (Reibungsverbund) und durch den infolge der Rippung des Betonstahls erzeugten Formschluss (Scherverbund). In ungerissenem Stahlbeton sind die Dehnungen der beiden Baustoffe gleich groß. Dieser Zustand, ohne Relativverschiebungen zwischen Beton und Stahl, wird auch als vollkommener Verbund bezeichnet.

Unbewehrter Beton versagt bei Zugbeanspruchung aufgrund seiner Sprödigkeit ohne ankündigende Rissbildung schlagartig. Dies geschieht im Vergleich zur Druckbeanspruchung schon bei geringer Belastung, weil die Zugfestigkeit klein ist. Aus diesem Grund werden die zugbeanspruchten Bereiche des Betons mit Bewehrungsstahl versehen, der einbetoniert ist. Da der Beton auf Zug den großen Dehnungen des Stahls nicht folgen kann, reißt er im Zugbereich. Im Bereich eines Risses ist dann nur noch der Bewehrungsstahl wirksam. Zug- bzw. biegezugbeanspruchte Bauteile können daher so bemessen und hergestellt werden, dass sich das Bauteilversagen durch eine intensive Rissbildung und signifikante Verformungen vorankündigt. Zur wirklichkeitsnahen Berechnung der Verformungen werden die Berechnungsverfahren der Baustatik erweitert, wie beispielsweise mit der nichtlinearen Stabstatik. Bei Bauteilen, die auf Druck beansprucht werden, können Stahleinlagen die Tragfähigkeit auf Druck erhöhen.

Stahl und Beton haben einen gleich großen Wärmeausdehnungskoeffizienten (10 K nach den Stahlbetonnormen), was bei Temperaturänderungen in etwa gleich große Wärmedehnungen der beiden Materialien zur Folge hat und somit keine nennenswerten Eigenspannungen im Verbundwerkstoff Stahlbeton bewirkt.

Eine Voraussetzung für die Dauerhaftigkeit des Verbundwerkstoffs ist das alkalische Milieu mit einem pH-Wert von 12 bis 14. Dieses entsteht durch die Umwandlung von Kalkstein in Calciumhydroxid während der Hydratation des Betons und stellt bei ausreichender Betonüberdeckung einen langfristigen Schutz des Betonstahls vor Korrosion sicher (siehe auch Betonkorrosion). Mit einem pH-Wert von weniger als 10 ist dieser Schutz, die sogenannte Passivierung, nicht mehr vorhanden. Ausgehend von der Betonoberfläche wird durch Feuchtigkeit und Kohlensäure die Alkalität des Betons und somit die Dicke der Passivierungsschicht um den Betonstahl mit der Zeit reduziert, wobei die sogenannte Karbonatisierungsgeschwindigkeit abnimmt. Risse im Stahlbetonbauteil können diesen Prozess fördern.
Sobald Bewehrungsstahl korrodiert, vergrößert sich sein Volumen und ein Druck wird auf den umgebenden Beton aufgebaut. Dies kann etwaige Risse verbreitern, was den Korrosionsprozess wiederum beschleunigt und schließlich ein Abplatzen des Betons zur Folge hat.

Für einen verbesserten Korrosionsschutz kann Betonstahl feuerverzinkt oder mit Epoxid beschichtet werden. Auch die Verwendung von Edelstahl und GFK-Bewehrung ist möglich. Die genannten Bewehrungselemente unterliegen in Deutschland der bauaufsichtlichen Zulassung. Eine Liste bauaufsichtlich zugelassener Bewehrungselemente führt das Deutsche Institut für Bautechnik.
Edelstahl kostet je nach Qualität etwa das 10-fache von normalem BSt 500 Bewehrungsstahl.

Zum Schutz gegen Korrosion des Bewehrungsstahles infolge Karbonatisierung oder Chlorideindringung kann auch ein Kathodischer Korrosionsschutz mit einer Fremdstromanode, die über Gleichrichter mit einem Schutzstrom (eigentlich nur eine Polarisierung) gesteuert werden, angewendet werden. Dies kann beispielsweise im Brückenbau zur Anwendung kommen.

Der Nachweis der Dauerhaftigkeit von Stahlbetonbauteilen beruht auf einem Zeitraum von 50 Jahren.

Risse in Stahlbetonbauteilen sind Bestandteil des Tragverhaltens und daher meist kein Mangel, sofern die Rissbreiten die in den Normen als zulässig definierten Werte nicht überschreiten und keine rissfreie Fläche vereinbart wurde. Risse können prinzipiell drei Ursachen haben:


Risse sind im Verbundwerkstoff Stahlbeton im Regelfall (zwangsläufig) zulässig, in Abhängigkeit von Umweltbedingungen und Nutzung des Bauteils sieht der Eurocode 2 beispielsweise eine Begrenzung der Breite auf 0,1 bis 0,4 mm vor. Die Schweizer Norm SIA 262 begrenzt die Spannungen im Bewehrungsstahl auf bis zu 50 % der Streckgrenze.

Eine konstruktive Maßnahme gegen zu große Rissbreiten ist das Einlegen einer ausreichenden, feinverteilten Bewehrung (viele dünne statt weniger dicker Stähle), die die Risse zwar nicht verhindert, aber dafür sorgt, dass statt einiger weniger, breiter Risse entsprechend mehr, aber schmale und somit unbedenklichere Risse entstehen. Diese Maßnahme steigert die Dauerhaftigkeit des Bauteils und verbessert den optischen Eindruck.

Bei Sonderbauteilen, wie Bodenplatten von Tankstellen, die rissfrei ausgeführt werden müssen, wird dies durch entsprechende Bauteilgeometrien und Dehnfugen oder durch Vorspannen sichergestellt. Der Einfluss der Bewehrung zur Sicherstellung einer Rissfreiheit ist von untergeordneter Bedeutung.

Von den unvermeidbaren konstruktiven Rissen sind Oberflächenrisse zu unterscheiden, die grundsätzlich unerwünscht sind und häufig betontechnologische Gründe haben, wie eine ungünstige Frischbetonzusammensetzung (mit z. B. zu hoher Hydrationswärmeentwicklung), einen nicht ordnungsgemäßen Betoneinbau und eine ungenügende Nachbehandlung der Frischbetonoberfläche.

Neben dem Betonstahl werden planmäßig auch andere Bauelemente einbetoniert. Diese werden als Einbauteile bezeichnet. Sie dienen meist der Befestigung von Bauelementen am Stahlbetonbauteil, wie zum Beispiel Stahlkonstruktionen. Dazu zählen unter anderem Ankerplatten und Ankerschienen. Weitere Einbauteile, wie Dübelleisten oder Seilschlaufen, ersetzen eine geometrisch schwierige und aufwändige Betonstahlbewehrung durch eine für die Beanspruchung des Betons spezielle entwickelte „Stahlkonstruktion“.





</doc>
<doc id="8432" url="https://de.wikipedia.org/wiki?curid=8432" title="Pfahlgründung">
Pfahlgründung

Die Pfahlgründung ist in der Bauausführung eine Variante der Tiefgründung. Mit ihr können die Lasten von Tragwerken in tiefere, tragfähige Bodenschichten abgetragen werden. Nicht oder schlecht tragfähige Bodenschichten werden mit Pfählen überbrückt.
Bei der Pfahlgründung werden Pfähle (auch Piloten genannt) in den Baugrund gebohrt oder gerammt, bis eine ausreichend tragfähige Boden- oder Gesteinsschicht erreicht ist. Die Lasten des Tragwerkes werden dann zum einen durch die Reibung des Pfahls mit dem Baugrund (Mantelreibung) und zum anderen über den Spitzendruck der Pfähle abgetragen.

Wird bei der Einleitung von Bauwerkslasten in den Baugrund sowohl die Tragwirkung der Pfähle als auch der Fundamentplatte berücksichtigt, dann wird das als kombinierte Pfahl-Platten-Gründung bezeichnet.

Bis Ende des 19. Jahrhunderts (und heute noch bei kleineren Bauten, wie Bootsschuppen) wurden angespitzte Holzpfähle oder Baumstämme in den Boden getrieben. An der Spitze dieser Pfähle war häufig ein eiserner Beschlag, der sogenannte Pfahlschuh angebracht der das Einschlagen der Pfähle auch in festem Untergrund ermöglichte. Manchmal wurden die auf gleicher Höhe abgesägten Pfähle durch aufgelegte Balken verbunden, der so entstandene Pfahlrost diente dann als Auflage für Mauerwerk.

Holzpfähle sind äußerst stabil und haltbar, wenn sie dauernd im Wasser stehen, selbst aus römischer Zeit stammende und bis heute unbeschädigte Pfähle sind bekannt. Holzpfähle in Wasserwechselzonen, also z. B. an Standorten mit wechselnden Grundwasserständen, verrotten. Dies kann auch durch langfristige Grundwassersenkungen geschehen, wie sie beispielsweise durch Bergbau, Flussregulierungen oder zunehmende Überbauung und damit Versiegelung eines Gebiets eintreten, weshalb insbesondere historische Bauten vor solchen Maßnahmen auf eventuell vorhandene Pfahlgründungen überprüft werden müssen. Das Verfaulen der Pfahlgründung kann zu erheblichen Schäden am Gebäude führen, wie beispielsweise beim Trierer Dom. Als größte Pfahlgründung gilt Venedig, wo die ganze Stadt im Laufe von Jahrhunderten auf Hunderttausenden Baumstämmen gegründet wurde. Aber auch viele alte und neue Gebäude in den Niederlanden, namentlich das Königliche Palais in Amsterdam und die alten Lagerhäuser der Speicherstadt in Hamburg stehen auf Holzpfählen.

Heute werden teilweise (aber wegen mangelnder Festigkeit nicht in Deutschland) Stampfbetonsäulen, also Säulen aus unbewehrtem Beton, verwendet.

Gründungspfähle können auch thermisch aktiviert werden. Diese Energiepfähle funktionieren dann ähnlich wie Erdwärmesonden.

Pfahlgründungen können je nach Einsatzzweck, Boden- und Umgebungsbedingungen aus verschiedenen Materialien bestehen, bzw. auf verschiedene Weisen hergestellt werden.



Stahlpfähle in Form von Rohren werden als Vollverdrängungspfähle bezeichnet, da sie den Boden beim Einbau zur Seite verdrängen. Wird das Stahlrohr nicht gerammt, sondern eingedreht und gedrückt, spricht man von Vollverdrängungs-Bohrpfählen.

Stahlpfähle in Form von Doppel-T-Trägern werden gerne als temporäre Pfähle genutzt, zum Beispiel für Behelfsbrücken und Traggerüste. Sie können nach dem Einsatz wieder gezogen werden. Reicht die Reibung und der zwischen den Flanschen entstehende Druckbogen zum Lastabtrag aus, können diese Pfähle ohne weitere Maßnahmen eingerammt werden. Muss eine höhere Last (auch Zugkräfte) aufgenommen werden, kann ein solcher Träger auch mit Fußverstärkungen versehen werden und nach dem Einrammen verpresst werden („Ramminjektionspfähle“, „RI-Pfähle“).

Im Gegensatz zu gerammten Stahlbetonfertigteilpfählen zeichnen sich gerammte Stahlpfähle durch ihre Bruchfestigkeit aus, wenn sie auf Hindernisse, wie z. B. große Steine, treffen. Dadurch ist es möglich Stahlpfähle selbst durch harte Bodenschichten bis auf Fels zu rammen.

Eingesetzt werden gerammte Stahlpfähle gerne auch im Hafenbau und im Offshorebereich. Hier sind die beim Einbau entstehenden Erschütterungen nicht störend.

Der Duktilpfahl ist ein Fertigteil-Rammpfahlsystem aus duktilem Gusseisen. Der Pfahl ist für zulässige Gebrauchslasten von 300 kN bis 1100 kN je nach Durchmesser und Wandstärke des Pfahlrohres ausgelegt. Die Pfahlrohre werden im Schleudergussverfahren aus duktilem Gusseisen hergestellt. Das Herzstück des Pfahles ist die Muffe, durch die der Pfahl endlos kuppelbar und in beliebiger Länge hergestellt werden kann. Beim unteren Abschluss des Pfahles, der Pfahlfußplatte, unterscheidet man zwischen zwei Ausführungsmöglichkeiten, dem mörtelverfüllten Pfahl und dem mantelverpressten Pfahl. Der mörtelverfüllte Pfahl wird mit einer Pfahlfußplatte ausgestattet, die das Pfahlrohr dicht abschließt. Der Pfahl wird auf die erforderliche Endtiefe gerammt und anschließend mit Betonmörtel verfüllt. Der Lastabtrag erfolgt hauptsächlich über den Spitzendruck. Beim mantelverpressten Pfahl wird eine Fußplatte verwendet, die größer ist als der Pfahlrohrquerschnitt. Diese erzeugt einen Ringraum, der durch die Verpressung aufgefüllt wird. Dazu wird während der Rammung Mörtelbeton durch das Pfahlrohr zum Pfahlfuß gefördert und bei der Fußplatte in den Boden gepresst, dabei entsteht ein Betonmantel. Der Lastabtrag kann so über die Mantelreibung erfolgen. Die Einzelrohrschüsse werden mit einem Schnellschlaghammer und speziellem Einsteckwerkzeug eingerammt, als Trägergerät eignet sich z. B. ein Hydraulikbagger. Dieses flexible Pfahlsystem ist auch bei beengten und unwegsamen Baufeldern anwendbar wo große Pfahlgeräte nicht arbeiten können. Durch seine vielseitige und wirtschaftliche Einsetzbarkeit bei Kleinbaustellen, Wohnungsbau bis zur Großbaustelle, gewinnt dieses Pfahlsystem in Deutschland immer mehr an Bedeutung.

Stahlbetonpfähle zählen heute in Deutschland zu den Pfahlsystemen mit den höchsten Material- und Qualitätsstandards. Weltweit werden aktuell hauptsächlich Fertigbetonrammpfähle aus Stahlbeton oder Spannbeton mit quadratischen Vollquerschnitt zwischen 20 × 20 cm und 45 × 45 cm verwendet, die für die Beanspruchung beim Transport, Einbringung und Bauwerkslasten (Druck, Zug, Biegung) standardmäßig bewehrt oder vorgespannt sind. Weniger häufig sind runde Schleuderbetonpfähle (z. B. in Österreich und der Schweiz) die, als Folge der Herstellung, innen hohl sind, so dass nur der bewehrte Betonaussenring zur Lastabtragung zur Verfügung steht.

Die axialen Pfahlwiderstände (Pfahltragfähigkeit) von Stahlbetonfertigpfählen liegen je nach Querschnitt und Untergrundverhältnissen zwischen 0,5 und 2,0 MN.

In Deutschland und im skandinavischen Raum werden Stahlbetonfertigpfähle im Pfahlwerk unter Einhaltung definierter Qualitätskriterien sowie kontinuierlicher externer Kontrollen hergestellt. Aus logistischen, wirtschaftlichen und einbautechnischen Gründen ist die Länge eines Einzelpfahls auf 15 m begrenzt. Für den Einsatz von Pfahllängen >15 m lassen sich Teilstücke durch geprüfte und bauaufsichtlich zugelassene Kupplungen miteinander verbinden und beliebig verlängern, in Schweden wurden auf diese Weise schon Pfahllängen von über 80 m realisiert.

Im innerstädtischen Bereich werden heute kaum noch Rammpfähle verwendet. Die durch die Rammung entstehenden Geräusche und vor allem Erschütterungen sind in eng bebauten Bereichen nicht mehr tragbar. Die durch die Rammung entstehenden Schäden an Nachbargebäuden sind meistens so hoch, dass diese Gründungsart durch Regressforderungen der Nachbarn nicht mehr wirtschaftlich ist. Als Alternative hierzu hat sich die Bohrpfahlgründung durchgesetzt. Diese Pfähle haben zwar eine geringere Tragfähigkeit, verursachen dafür nur geringe Erschütterungen.

Wie bei allen Pfahlarten erfolgt bei Betonfertigteilpfählen die Abtragung von Lasten über Mantelreibung und Spitzenwiderstand. Dieses Pfahlsystem ist in nahezu jeder Bodenart und jeder Baugrundschichtung anwendbar. Fertigpfähle aus Stahlbeton eignen sich aufgrund ihrer Form- und Querschnittsbeständigkeit besonders in breiigen oder weichen, stark wasserhaltigen oder kontaminierten Bodenschichten. Bei der Ausführung in feste Böden oder auf Fels ist bei Schleuderbetonpfählen grundsätzlich eine Stahlspitze vorzusehen, die bei einem Stahlbetonpfahl mit Vollquerschnitt aufgrund der hohen Betonfestigkeit nicht erforderlich ist.

Das Einsatzspektrum von Stahlbetonfertigrammpfählen reicht von Einfamilienhäusern über Industrieanlagen bis hin zu großen Infrastrukturprojekten. Eines der wichtigsten Einsatzgebiete stellt allerdings bereits seit Jahrzehnten die Tiefgründung von Windkraftanlagen in Deutschland und ganz Europa dar.

Die Vielseitigkeit der Anwendung von Stahlbetonfertigrammpfählen zeigt sich auch darin, dass werkmäßig Sonderpfähle produziert werden können, die z. B. Injektionsrohre oder Leitungen zur Nutzung von Erdwärme durch sogenannte Energiepfähle enthalten.

Wird der Beton vor Ort eingebracht (heute meist Transportbeton), spricht man von Ortbetonpfählen.
Der Durchmesser und die Länge der Pfähle kann jeweils auf die entsprechenden Anforderungen ausgelegt werden.

Insbesondere beim Tragfähigkeitsnachweis müssen Ortbetonpfähle in Groß- und Kleinbohrpfähle („Mikropfähle“) unterschieden werden. Mikropfähle sind im Durchmesser kleiner als 30 cm. Von Großbohrpfählen spricht man entsprechend bei Pfählen mit einem Durchmesser größer gleich 30 cm.

Ein Beispiel für den Einsatz von Ortbetonpfählen ist der Kaispeicher A in Hamburg. Der in den sechziger Jahren errichtete 108 mal 85 Meter große Bau steht im weichen Elbschlick und ist auf exakt 1111 Stahlbetonpfählen gegründet. Da das berechnete Gewicht des Speichers, der zeitweise komplett mit Kakaosäcken gefüllt werden sollte, sehr groß war, wurden die Pfähle auf eine Tragkraft von je 160 Tonnen ausgelegt und mit einem Durchmesser von 50 cm ausgeführt. Die Stahlbetonpfähle leiten das Gewicht durch Bodenschichten aus Klei und Torf in stabilere Sandschichten.

Bei einem Bohrpfahl wird die Erde nicht verdrängt, sondern ein unten offenes Stahlrohr in die Erde gebohrt. Die Erde wird dabei aus dem Inneren des Rohres entfernt, dann wird eine Stahlbewehrung oder Stahltragglied eingebracht und das Rohr mit Ortbeton oder Mörtel verfüllt. Entsprechend dem Betonierfortschritt wird das Stahlrohr schrittweise herausgezogen, dabei wird auch evtl. in der Tiefe stehendes oder eindringendes Wasser vom schwereren frischen Beton nach oben gespült, bis es letztendlich als „Kissen“ auf der Oberfläche des Pfahls steht und den frischen Beton dort etwas entmischt. Diese Art Ortbetonpfahl wird für statisch anspruchsvolle Gründungen wie bei Hochhäusern oder Brücken verwendet. Eine weitere Verwendungsmöglichkeit besteht im Aneinanderreihen von Bohrpfählen um eine durchgehende Bohrpfahlwand zu erstellen, die dann z. B. als Baugrubensicherung dient.

Ein anderes Verfahren zur Herstellung von Bohrpfählen ist die Schneckenbohrtechnik. Hierbei wird eine hohle Endlos-Bohrschnecke in den Boden „gedreht“. Ist die gewünschte Tiefe erreicht, wird Beton durch die hohle Bohrschnecke gepresst und diese damit nach oben gedrückt. Die Stahlbewehrung wird, nachdem die Bohrschnecke und der nach oben beförderte Boden entfernt wurde, in den noch frischen Beton eingerüttelt.

Bei dem Pressbetonbohrpfahl oder Mörtelverpresspfahl werden nach der Herstellung die Zwischenräume mit Mörtel verpresst, um zusätzlichen Halt zu erzeugen.

Nachdem der Pfahl betoniert wurde und der Beton ausreichend abgebunden hat, wird die Baugrube ausgehoben, sodass die Pfahlköpfe zum Vorschein kommen. Die Bohrpfahlköpfe werden anschließend auf das erforderliche Niveau abgebrochen, so dass der Pfahlkopf an der Unterkante des Fundaments oder der Bodenplatte endet. Ist der Pfahl bewehrt, bindet man die Armierung mit der erforderlichen Länge in das anzuschließende Bauteil ein.
Das Abbrechen des überstehenden Pfahlkopfes erscheint zunächst wie eine Materialverschwendung. Der Grund für dieses „Überbetonieren“ der Pfähle liegt im Betonierverfahren. Um eine Entmischung des Frischbetons zu vermeiden, wird mit dem Kontraktorverfahren betoniert, d. h. das Betonierrohr endet immer unterhalb der Frischbetonoberfläche. Der „schlechte“, weniger tragfähige Beton schwimmt oben auf und muss nach dem Abbinden abgestemmt werden.

Die Pfähle werden je nach Anforderung verschiedenen Tests unterzogen. Die Tests müssen von einem zertifizierten Pfahlprüfer durchgeführt werden. Die derzeit häufigsten Prüfungsverfahren:

Ehemals wurden Piloten bei gemeinschaftlicher Arbeit mit händisch geschwungenem Schlägel eingeschlagen – häufig zur Fluss- oder Seeuferbefestigung.
Um raschen Fortschritt zu erzielen wurde arbeitsteilig zusammengewirkt und schlugen auch zwei Männer abwechselnd auf einen Piloten. Das volkstümliche Pilotenschlager-Lied zeugt vom Rhythmus und der notwendigen Koordination und Ausdauer der einander gegenüber stehenden Hammerschwingenden.



</doc>
<doc id="8434" url="https://de.wikipedia.org/wiki?curid=8434" title="Kanone">
Kanone

Kanone ist ursprünglich die Bezeichnung für ein Geschütz, das sowohl bei der Artillerie (Erdartillerie, Schiffsartillerie, Flakartillerie) als auch zur Flugzeug- (Bordkanone auch Maschinenkanone) und Panzerbewaffnung (Kampfwagenkanone oder Panzerkanone) verwendet wird. Die Rohrlänge beträgt mindestens das Zwanzigfache des Kalibers (Kaliberlänge L). Im Militärwesen des ehemaligen Warschauer Pakts war die "Kanone" der Erdartillerie als "Flachfeuergeschütz" mit einer Rohrerhöhung bis +40° und einer Rohrlänge von über 30 bis 70 Kalibern definiert.

Heute gilt die "Kanone" als Flachfeuergeschütz, das im Unterschied zum Steilfeuergeschütz (Haubitze, Mörser, Raketenwerfer oder Granatwerfer) vornehmlich im direkten Feuerkampf (auch direktes Richten) verwendet wird. Weitere von der "Kanone" abgeleitete Waffensysteme sind beispielsweise Feldkanone, Flugabwehrkanone, Jagdkanone, Kanonenhaubitze, Panzerabwehrkanone, Panzerjägerkanone, Schiffskanone und Sturmgeschütz (Sturmkanone).

Umgangssprachlich wird zwischen den Begriffen "Geschütz" und "Kanone" oft kein Unterschied gemacht, obwohl Geschütz ein Oberbegriff ist, der sowohl die Kanonen als auch die "Mörser" und "Haubitzen" umfasst.

Der Begriff stammt vom italienischen Wort "canna" (wie französisch "canon") für „Röhre“ oder „Rohr“, das mit einem Augmentativendung zu "cannone" erweitert ist. Das zugrundeliegende lateinische Wort "canna" ist seinerseits eine Übernahme aus dem Griechischen, wo κάννα (kanna) "Rohr" bedeutet. Geschütze gibt es, unter verschiedenen Bezeichnungen, im deutschen Sprachraum seit dem 14. Jahrhundert. Der Begriff "Kanone" hat sich im Deutschen erst im 17. Jahrhundert eingebürgert. Eine Unterscheidung der Geschützarten Kanone, Haubitze und Mörser lässt sich im späten 18. Jahrhundert belegen.

In frühen Zeiten galt vor allem die Art der verschossenen Munition und die Kaliberlänge als Unterscheidungskriterium. Kanonen verschossen Vollkugeln und auf kurze Distanz Kartätschen im Direktschuss. Die 1683 erfundenen Haubitzen verschossen Kugelgranaten mit Zeitzündern im Direkt- und leichten Bogenschuss, und Mörser verschossen solche im Steilfeuer. Haubitzen und Mörser wurden früher auch als Kammerstück bezeichnet, da das hintere Ende der Rohrseele, die Pulverkammer, im Durchmesser reduziert war. Kammergeschütze haben ein zweigeteiltes Rohr.

Moderne Kanonen sind Flachfeuergeschütze mit einer Elevationsmöglichkeit bis etwa 35°. Da sie dem Geschoss aufgrund der hohen Mündungsgeschwindigkeit eine gestreckte Flugbahn erteilen, sind sie für indirektes Feuer und zum Direktschuss einsetzbar. Im Gegensatz dazu werden Haubitzen (Elevation bis 75°) und Mörser als Steilfeuergeschütze für indirektes Feuer eingesetzt, wobei bei Haubitzen zur Nahabwehr auch Direktfeuer möglich ist.

Die Panzerabwehrkanonen wurden wegen Ineffektivität gegenüber den modernen Panzerungen durch die leichteren und beweglichen Panzerabwehrlenkwaffen abgelöst.

Der Begriff "Kanone" wird heute noch für die Hauptwaffe (auch Kampfwagenkanone) von Kampfpanzern, die Flugabwehrkanone oder kleinere Maschinenkanonen verwendet.
In der Militärtechnik wird der Begriff ebenfalls für als Waffe benutzte Laser und elektromagnetische Kanonen wie Railgun und Coilgun verwendet.
Mit dem Begriff "Kanone" sind Redensarten und Metaphern verbunden wie beispielsweise "Kanonenfutter".



</doc>
<doc id="8436" url="https://de.wikipedia.org/wiki?curid=8436" title="Graphem">
Graphem

Grapheme oder Grafeme ( "graphē" ‚Schrift‘ und Suffix "-em") sind die kleinsten bedeutungsunterscheidenden, aber nicht selbst bedeutungstragenden grafischen Einheiten des Schriftsystems einer bestimmten Sprache. Sie fassen also bedeutungsgleiche Graphe in gemeinsame Klassen zusammen.

Ein bestimmter Laut in einer gesprochenen Sprache, ein Phonem, kann auf verschiedene Weise geschrieben werden. So ist in den beiden Wörtern "schrift" und "sprache" der Wortanfang das Phonem und wird einmal mit den Graphen und einmal mit dem Graph dargestellt. Ähnliches gilt für die Wörter "flug" und "vogel": das Phonem korrespondiert mit den unterschiedlichen Graphen und . Verschiedene Graphe, die entweder frei ausgetauscht werden können oder nach deterministischen Regeln komplementär verteilt sind, nennt man Allographe. Ein Graphem besteht aus solchen Allographen.

Die Linguistik untersucht in ihrer Teildisziplin Graphem(at)ik die Strukturen und Zusammenhänge von Graphemen, um durch die Bildung von Klassen und Konstruktionsprinzipien allgemeine Aussagen über eine Sprache und deren Verschriftlichung treffen zu können.
Grapheme aus dem Skript der Metasprache, d. h. lateinische sowie bei manchen Autoren auch griechische und kyrillische Buchstaben, werden in der Linguistik üblicherweise in nach außen weisende spitze Klammern gefasst, behelfsweise auch in Guillemets oder Kleiner- und Größerzeichen. Dies gilt auch für Graphemketten, also Buchstabenfolgen, die normalerweise orthographische Wörter bilden.

Grapheme aus Objektsprachen mit anderem phonographischen Skript werden meistens nicht gesondert gekennzeichnet, stattdessen wird ihre Transliteration eingeklammert und ggf. ihr Name angegeben:

Die (Grund-)Bedeutung funktioneller und logographischer Grapheme wird üblicherweise in Kapitälchen angegeben. Für komplexe Grapheme werden analog die Konventionen für Lexeme angewandt:

Der Begriff "Graphem" für die graphische Ebene der Sprache folgt demselben Bildungsmuster wie Phonem und Morphem. Diese drei emischen, sprachabhängigen Begriffe tauchten erstmals Anfang des 20. Jahrhunderts bei Baudouin de Courtenay auf. Der Graphembegriff wurde aber um 1932 von Penttilä vermutlich unabhängig davon nochmals neu geprägt und setzte sich erst anschließend international durch. Allerdings herrscht bis heute keine vollständige Einigkeit darüber, was ein Graphem ist und was nicht.

Auf Grundlage dieser Begriffe entstanden die entsprechenden Bezeichnungen für etische, sprachunabhängige Grundeinheiten, also Phon, Morph und Graph, sowie für Fachdisziplinen, z. B. Phonem(at)ik (Phonologie), Morphem(at)ik (Morphologie) und Graphem(at)ik oder entsprechend Phonetik und Graphetik.

Manche Linguisten sehen Grapheme als unidirektional abhängige visuelle Abbilder von lautsprachlichen Phonemen.
Im allgemeinen Sprachgebrauch werden Grapheme häufig nicht oder nicht deutlich von den sprachunabhängigen Schriftzeichen und den nur in segmentalen Schriftsystemen vorkommenden Buchstaben unterschieden. Selbst unter Schriftlinguisten ist die Terminologie uneinheitlich und bisweilen eurozentrisch. Die Abgrenzung zu den graphetischen Einheiten Graph und Glyphe ist teilweise schwierig und umstritten, für viele Anwendungsfälle außerdem irrelevant.

Das Graphem wurde analog zum Phonem geschaffen. Beide sind die kleinsten bedeutungsunterscheidenden Einheiten ihres Mediums. Als lautliche ("phonische") Einheit ist das Phonem der Untersuchungsgegenstand der Phonologie.

Vielfach wird mit Bezug auf Aristoteles, Saussure oder Bloomfield die Schriftsprache nicht nur als geschichtlich (phylogenetisch) und individuell (ontogenetisch) sekundär zur Lautsprache aufgefasst, sondern als unselbständiges, unidirektional abhängiges Zeichensystem. Vertreter dieser "Dependenztheorie", nach der die Schrift also der Rede nachgeordnet und nicht nebengeordnet ist, sehen das Graphem entsprechend als Abbild des Phonems. Entsprechend entspricht für sie jedem Graphem genau ein Phonem und umgekehrt. Für Anhänger der konkurrierenden "Autonomiehypothese" – wie die Prager Schule – oder der vermittelnden "Interdependenz-" bzw. "Korrespondenztheorie" sind die Begriffe "Phonem" und "Graphem" hingegen parallel angelegt und gleichberechtigt: In einer Orthographie können sie einander regelbasiert zugeordnet werden ("phonographisches Prinzip").

Dies drückt sich bspw. auch in der Terminologie der Glossematik nach Hjelmslev aus, worin die kleinsten ausdrucksseitigen Einheiten "Keneme" genannt werden und sowohl Phoneme als auch Grapheme sein können; zusammen mit den inhaltsseitigen "Pleremen" bilden sie die kleinsten sprachlichen Zeichen, sogenannte "Glosseme".
Um es beiden Lagern recht zu machen, schlug McLaughlin für die hyponyme Graphemgruppe der phonemabhängigen graphischen Einheiten den Terminus "Graphonem" vor, während später Heller stattdessen die Grapheme aufteilt in phonematisch bestimmte "Phonographeme" – mit gekapselter Notation wie in – und graphematisch bestimmte "Graphographeme" als verallgemeinerte Klassen von konkreten Schriftzeichen. Rezec trennt analog "Phonemabbilder" von den eigentlichen Graphemen.

Als semantisch distinktive (d. h. bedeutungsunterscheidende, aber nicht selbst bedeutungstragende) Einheiten sollten sowohl Phoneme als auch Grapheme per Minimalpaaranalyse gefunden werden, wobei sie auf derselben Ebene nicht weiter zerlegbar sein dürfen. Da das Lautmedium kontinuierlich und analog, das Schriftmedium hingegen (annähernd) diskret und digital ist, werden Phoneme beim paarweisen Vergleich phonematischer Wörter als Bestandteile der größeren Einheit "Silbe" gefunden, Grapheme hingegen als Kompositionen der kleineren Einheit "Schriftzeichen". Dies liegt daran, dass graphematische Wörter in vielen Schriftsystemen einerseits bereits klare Außengrenzen aufweisen und andererseits auch intern schon segmental strukturiert sind. So unterscheiden sich die beiden einsilbigen Wörter und nur im Silbenkopf, der mit den Phonemen und besetzt ist, was sich auch in ihren graphischen Entsprechungen und wiederfindet, nur dass das Graphem aus drei Einzelzeichen besteht, die – trotz anderer Minimalpaare wie – untrennbar verbunden sind. In beiden Realisationen von Sprache tritt dabei als (je nach Sprache häufigen) Sonderfall die Identität auf, indem ein Phonem einem alleinstehend silbenfähigen Vokal bzw. indem ein Graphem einem Buchstaben entspricht.

Das Graphem sollte (gemeinsam mit dem Phonem) den vor der Linguistischen Wende verwendeten Begriff des "Buchstabens" (in anderen Sprachen "letter", "littera" u. ä.) ersetzen, da dieser sowohl für schriftliche als auch für lautliche Sprachzeichen verwendet wurde. Seitdem seine phonische Bedeutung weitgehend verschwunden ist, verwenden einige Linguisten "Buchstabe" synonym zu oder anstelle von "Graphem", zumindest solange sie sich mit segmentalen, d. h. alphabetischen Schriftsystemen beschäftigen.

Neef argumentiert, dass die Graphematik einer Alphabetschrift allein mit der Einheit Buchstabe auskommen können müsse, da die Einheit Graphem sowohl durch Einzelbuchstaben, bspw. , als auch durch Buchstabengruppen, , realisiert werden kann, und sich bspw. die Anfangsgroßsschreibung auf Buchstaben, , statt auf Grapheme, *, bezieht.

Ein Alphabet ist eine konventionalisierte Menge von Buchstaben. Es unterscheidet sich von einem beliebigen geschlossenen Zeichensatz vor allem dadurch, dass es die Sortierreihenfolge der Buchstaben festlegt. Die Elemente einiger Alphabete dienen daher auch als Zähl- oder Zahlzeichen, so hat sich bspw. in der griechischen Schrift das nur als Ordinalzeichen für 90. erhalten. Grapheme haben derartige Eigenschaften nicht, dafür kann ein Graphem rekursiv andere, kleinere Grapheme enthalten.

Das Konzept der Buchstaben umschließt in manchen Skripten verschiedene Ausformungen oder Fälle, insbesondere die Unterscheidung in Minuskeln und Majuskeln, d. h. Klein- und Großbuchstaben; daher hat bspw. das englische Alphabet 26 und nicht 52 Buchstaben.
Grapheme sind hingegen, obwohl sie graphische Einheiten sind, nicht notwendigerweise an einen sichtbaren Zeichenkörper gebunden und oft wird bei ihnen entweder überhaupt nicht zwischen Majuskel und Minuskel unterschieden oder diese Buchstabenvarianten werden genauso wie zwei voneinander unabhängige Graphe behandelt.

Grapheme sind sprachabhängig, d. h. sie müssen für jede Schriftsprache bzw. für jedes Schriftsystem eigens bestimmt werden. Schriftzeichen (engl. "character") als Konstituenten eines Skripts sind hingegen übersprachlich oder sogar unsprachlich definierbar. Es gibt Sonderfälle wie das serbokroatische Schriftsystem, in welchem jedes Grundgraphem je ein lateinisches und ein kyrillisches Schriftzeichen verwendet.

Im Sinne der Semantik sind Schriftzeichen trotz der anderes implizierenden Bezeichnung noch keine sprachlichen Zeichen, sondern erst Grapheme. Beide werden jedoch mittels Glyphen als Graphe realisiert.

Grapheme sind kleinste sprachliche Einheiten, trotzdem lassen sie sich mitunter weiter unterteilen. Diese weitergehende Analyse ist per Definition nicht mehr Teil der (Schrift-)Linguistik, sondern Aufgabe einer ihrer (paralinguistischen) Hilfswissenschaften, der Graphetik. Deren Betrachtungsgegenstand ist jede „konkrete, klassifizierbare graphische Erscheinung“ und heißt "Graph". Grapheme sind dann abstrakte Klassen von äquivalenten konkreten Graphen, die Allographe genannt werden.

Einige Graphetiker, darunter Primus und Brekle, haben die lateinischen und teilweise auch die griechischen Minuskeln dahingehend analysiert, wie ihre abstrakten Formbestandteile mit phonologischen Eigenschaften korrelieren; so haben bspw. die "normalen" Vokalbuchstaben keine Ober- und Unterlängen. Da damit begründbar wäre, dass Grapheme kleinere Einheiten als Buchstaben sein können, wird die Definition mitunter um die Forderung ergänzt, dass Grapheme aus (in einem Frame) abgeschlossenen, ungebundenen, aber möglicherweise komplex zusammengesetzten Einheiten bestehen. Grapheme können also rekursiv sein, was sowohl bei Buchstabenverbindungen wie der Fall ist als auch bei den meisten Sinogrammen, da zumindest einige ihrer Konstituenten jeweils selbst Grapheme sein können.

Viele Bezeichnungen graphischer Einheiten verwenden den Morph oder . Mit dem ersten ist eher eine graphetische, mit dem zweiten eher eine sprachabhängige graphematische Bedeutung assoziiert, aber die Unterscheidung wird nicht einheitlich und systematisch getroffen.

Bei Rezec entsprechen "Grundformen" den Graphen in diesem Sinne, während dort "Graph" mit "Glyphe" gleichgesetzt wird.

Wenn es um das (prototypische) Aussehen eines Graphs in einer bestimmten Hand- oder Druckschrift geht, spricht man stattdessen von "Glyphen". Sie sind u. a. der Arbeitsgegenstand von Schriftgestaltern und Schriftkünstlern (Kalligraphen). Damit sind sie noch weniger als Graphe ein Gegenstand der Linguistik.

So zeigen sich etwa "positionale" oder "direktionale" Allographen in mehreren verschiedenen, aber optisch unterschiedlichen, ähnlichen Glyphen. Die Position kann dabei der Wortanfang ("initial"), mitten im Wort ("medial"), das Wortende ("final") und alleinstehend ("isoliert") sein und die Schreibrichtung ist zeilenweise (horizontal) oder spaltenweise (vertikal), rechtsläufig ("dextrograd") oder linksläufig ("sinistrograd"). Auch Ligaturen haben eigene Glyphen, obwohl sie aus mehreren Graphen bestehen, während einzelne Graphen aus Basis und Diakritikum aus zwei Glyphen zusammengesetzt werden können. Glyphgrenzen können also, müssen aber nicht mit Graph- oder Graphemgrenzen korrelieren. Dieselbe Glyphe kann für verschiedene Schriftsysteme verwendet werden, z. B. beim lateinischen Großbuchstaben und dem griechischen großen , deren Kleinbuchstaben sich voneinander unterscheiden.

Der Status von graphischen Zeichen in nicht- und parasprachlichen Notationen, z. B. in mathematischen oder chemischen Formeln, wird nicht einheitlich gehandhabt. In manchen Theoriegebilden werden sie einfach ignoriert oder übersehen, andere versuchen sie als speziellen Typus von Graphemen zu beschreiben. Viele Schriftlinguisten beschränken sich also auf glottographische Graphemtypen, das sind Zeichen zur Niederschrift von Sprache. Davon gibt es im Grundsatz zwei verschiedene Gruppen:
Daneben existiert eine Gruppe von Hilfsgraphemen für die Interpunktion, die üblicherweise erst auf syntaktischer Ebene von Bedeutung sind und zwar Einfluss auf die Aussprache der Phrase ("Prosodie") haben können, selbst aber nicht verlautet werden. Dazu gehört als „Nullgraphem“ auch das Leerzeichen (Spatium) in verschiedenen Formen. Die meisten dieser Zeichen indizieren außerdem Wortgrenzen.

Obwohl sie häufig "Logogramme" genannt werden, stehen Zeichen dieses Typs in der Regel nicht für vollständige Wortformen, sondern für das meist freie lexikalische Wortparadigma und die üblicherweise gebundenen grammatischen Affixe oder freie Partikeln, also für L- und G-Morpheme. Deswegen wird alternativ von "Morphogrammen" gesprochen.

Der Grundstock dieser graphischen Zeichen ist häufig ursprünglich mittels einer von zwei ikonischen Methoden gebildet worden: entweder konkret abbildend, "piktographisch", oder abstrakt symbolisierend, "ideographisch". Beachtet man einige graphische Konventionen, die aus dem bevorzugt verwendeten Schreibmedien resultieren, besteht diese Möglichkeit der Zeichengenese prinzipiell weiterhin, doch normalerweise existiert ein beschränktes (Sub-)Inventar von Konstituenten, aus denen nach bestimmten Kombinationsregeln neue Zeichen zusammengesetzt werden können.

Üblicherweise wird der Begriff des Graphems hierbei abweichend von seiner eigentlichen Definition auf das Produkt und nicht auf die bedeutungsdifferenzierenden Konstituenten angewendet, die stattdessen als "Subgrapheme" o.ä. bezeichnet werden. Dies gilt umso mehr, wenn dieses, wie bei den ostasiatischen Sinogrammen üblich, unabhängig von seiner inhärenten und kombinatorischen Komplexität, d. h. der Anzahl der Striche und Teilzeichen, in einen unsichtbaren Rahmen ("Frame") fester, üblicherweise quadratischer Größe eingeschrieben wird. Aufgrund dieser Formationsregel werden sie auch als "Tetragramme" bezeichnet, welche im normalen Schreib- und Leseprozess trotz ihres systematischen Aufbaus als atomar wahrgenommen werden. Da dies lediglich ein graphetischer und kein graphemischer Begriff ist, deckt er auch die Zeichen der geschlossenen, phonographischen Systeme der japanischen Kana und des koreanischen Hangul ab.

Die Subgrapheme können je nach Skript auch eigenständig verwendbar bleiben, sind dann also rekursive Einheiten auf verschiedenen Ebenen des Schriftsystems. Sie können verschiedene positionell motivierte Allographen aufweisen, d. h. sie sehen je nach Position und Kombination etwas anders aus.

Die Konstituenten leisten entweder als pleremisches "Determinativ" (Δ), auch "Signifikum", einen Beitrag zur Bedeutung oder geben als kenemisches "Phonetikum" (Φ) einen Hinweis auf die Aussprache des Summengegenstandes, d. h. des Morphogramms. Beide sind in der Regel ungenau und nur in der konventionalisierten Kombination eindeutig. Wenn es nur wenige mögliche Determinative gibt, die somit nur eine grobe Kategorisierung zulassen, spricht man auch von "Taxogrammen", wenn es für eine genauere semantische Einordnung taugt, auch von "Semagrammen". In manchen Skripten können einige oder alle Subgrapheme beide Rollen einnehmen, in anderen sind sie auf eine beschränkt, z. B. bei den ägyptischen Hieroglyphen. Die Positionen im Gesamtzeichen können bevorzugt oder ausschließlich von dem einen oder dem anderen Typ belegt sein. Es treten je nach System nur einige oder alle denkbaren Kombinationen der beiden Typen auf: ΔΦ, ΦΔ, ΔΔ, ΦΦ sowie komplexere Konstrukte. Im einfachsten Fall steht ein einzelnes Phonetikum für ein Homophon und wenn mehrere, aber ausschließlich Phonetika verwendet werden, greift das Rebus-Prinzip, wobei unter mehreren möglichen Zeichenalternativen oft die semantisch naheliegendste gewählt wird, bspw. in der chinesischen Transkription ausländischer Orts- und Personennamen.

Bei einer engen Auslegung des Graphembegriffs, wenn also die Konstituenten als "Grapheme" und die Produkte als (zweidimensionale) "Graphemketten" oder als "graphematische Wörter" bezeichnet werden, verschwindet der Unterschied zu den Silbensystemen weitgehend. Das Phonographeminventar der Phonetika ist lediglich sehr groß, enthält Duplikate und überschneidet sich unter Umständen mit dem Graphographeminventar der Determinativa, welches es in klassischen Syllabaren nicht oder nur sehr eingeschränkt gibt.

Klassischerweise werden Phonogramme danach unterteilt, ob sie hauptsächlich für die Repräsentation von Silben oder von Silbensegmenten ("Buchstaben") verwendet werden.

In Silbenschriften werden das Zeicheninventar "Syllabar" und die Zeichen "Syllabogramme" genannt. Es gibt jedoch viele Typen von Silbenschriften und Silbenzeichen, da nie alle möglichen phonematischen Silben einer Sprache mit je einem exklusiven Syllabogramm verschriftet werden. Stattdessen gibt es orthographische Regeln, um mit einem beschränkten, ggf. sprachübergreifendem Repertoire auszukommen. Wegen derartiger Zeichenkombinationen wird der Graphembegriff manchmal auf digraphische „Syllabogrammketten“ wie die japanischen Yōon ausgedehnt ( ).

In einer "echten" Silbenschrift können die silbischen Schreib- und Leseeinheiten als Grapheme angenommen werden, doch viele Systeme sind nicht völlig arbiträr:

Einerseits gibt es manchmal als Abugida bezeichnete "synthetische" Schriften wie die des indischen Brahmi-Schriftkreises, in denen Vokale entweder inhärent sind oder diakritisch an die silbischen Basen gebunden werden und dabei z. T. komplexe Ligaturen bilden. Da Konsonanten und Vokale auf unterschiedlichen Ebenen des Schriftsystems notiert werden, sehen manche Linguisten die gebildeten Silbenligaturen als Grapheme an, für andere unterscheidet sich dieses hierarchisch segmentale Prinzip nicht wesentlich von den gleichberechtigt segmentalen Schriften, und entsprechend gelten vorbehaltlich einzelsprachlicher Untersuchungen beide phonographischen Zeichentypen als Grapheme. Eine besondere Art von Graphem in diesen Schriften ist das "Virama", das wie ein Vokalzeichen verwendet wird, aber den Wegfall des inhärenten Vokals kennzeichnet.

Andererseits sind vor allem in der Neuzeit entwickelte Schriften wie die der Cree häufig "systematisch", indem sowohl die Konsonanten als auch die Vokale der -Syllabogramme reihen- bzw. spaltenweise einheitlich geometrisch variieren. Hier stellt sich die Frage, ob die Zeichenorientierung graphemisch ist, also ob bspw. ein allgemeines Vokalgraphem postuliert werden kann, dem der genaue Wert über ein abhängiges Ausrichtungsgraphem usw. zugeteilt wird, also , , , , oder aber vier selbständige Vokalgrapheme , , , ausgemacht werden.

In den meisten sogenannten Silbenschriften gibt es auch nicht-silbische Zeichen, z. B. solche wie das japanische , die nur für die Koda verwendet werden können. Diese haben in der Regel ebenfalls Graphemstatus.

Manche mit den Syllabogrammen verwendete Schriftzeichen wie die japanischen und haben durch orthographische Konventionen variablen bzw. funktionalen Charakter und kommen nicht frei vor, sondern sind an das vorangehende oder nachfolgende Syllabogramm gebunden, wodurch dessen phonographische Qualität verändert wird.

Oft werden für Alphabetschriften pauschal die jeweiligen Buchstaben als Grapheme angesetzt ("alphabetisches Graphem"). Dies gilt auch für manchmal Abdschad genannte andere segmentale Skripte, in denen (manche) Vokale nicht auf derselben Schriftebene wie Konsonanten oder optional oder überhaupt nicht geschrieben werden.

Viele Linguisten verstehen die Groß- und Kleinbuchstaben als allographische Varianten desselben Graphems oder als „funktional verbundenes Paar von Graphemen“, doch vor allem im Deutschen kann die Unterscheidung semantisch relevant sein, vgl. Adjektiv vs. Substantiv vs. Akronym . Zu beachten ist außerdem, dass die Anfangsgroßschreibung in vielen Sprachen auf Buchstaben, in manchen aber auf Grapheme wirkt, z. B. niederländisch statt *. Daher postulieren einige Wissenschaftler abstrakte Funktionsgrapheme, die ihr Vorhandensein nur durch Interaktion mit anderen Zeichenkörpern anzeigen, z. B. „Supragrapheme“ bei Gallmann. Diese Funktionen können syntagmatisch (aus dem Satz heraus) oder paradigmatisch (aus dem Wort heraus) und sogar rein graphostilistisch aktiviert werden. Damit können , und jeweils als unmarkierte oder markierte Schreibungen von "arm", "Arm" und "ARM" legitimiert und gleichzeitig , , und ausgeschlossen werden.

Weil das Graphem die kleinste bedeutungsunterscheidende Einheit ist, müssen bedeutungsgleiche Graphemketten, d. h. graphematische Wörter, aus denselben Graphemen aufgebaut sein. Viele Orthographien lassen aber bestimmte Varianten zu, z. B. und , sodass schon allein deswegen nicht pauschal davon ausgegangen werden kann, dass ein Graphem einem Buchstaben entspricht ("orthographisches Graphem"). In der dependenztheoretischen Begriffswelt, wozu häufig die Schreibdidaktik gehört, kann zwischen dem im Normalfall verwendeten "Basisgraphem", bspw. → , und seinen orthographisch begründeten Varianten, den "Orthographemen", unterschieden werden, bspw. → wegen Morphemkonstanz.

Da in vielen Schriftsystemen manche (feste) Buchstabenverbindungen in der Minimalpaaranalyse Positionen einnehmen, die sonst nur von Einzelbuchstaben eingenommen werden können, werden solche Digraphe, Trigraphe oder Plurigraphe oft ebenfalls als Grapheme dieser Sprache angesehen. In der lateinischen Schrift wird besonders häufig das als hintere Komponente dieser Kombinationen verwendet. Der Buchstabe in den (nicht nur) deutschen Verbindungen und kann als allographische Variante des Folgebuchstabens interpretiert werden, sodass sich die regelmäßiger gebildeten (theoretischen) Grapheme und ergeben; ähnliches gilt für das Verhältnis von zu .

Die diakritischen Zeichen, welche es übrigens auch in Silbenschriften gibt, können nach Gallmann (hier am Beispiel der deutschen Umlaute) auf drei verschiedene Arten graphematisch analysiert werden:
Der zweite und dritte Ansatz entspricht der umgangssprachlichen Ansicht, dass das "deutsche Alphabet" 26 Buchstaben habe.

So schreibt man nach traditioneller deutscher Rechtschreibung und ohne eingedeutschte Schreibung französischer Lehnwörter , wobei der Akzent häufig weggelassen wird, , und das End- stumm bleiben kann. Mit der Rechtschreibreform muss durch ersetzt werden, während der Gallizismus wahlweise als geschrieben und dann auch je nach intendierter Aussprache zu oder geändert werden kann. Damit teilt sich dieses Wort ohne Bedeutungsänderung in die orthographischen Grapheme , die allerdings bezüglich ihrer Kombination miteinander nicht völlig frei sind.

Der Bindestrich (und auf etwas andere Weise der Apostroph) nimmt in den europäischen Buchstabenschriften eine Sonderrolle ein, da er kein Phonogramm ist, also nicht direkt gesprochen wird, sondern lediglich den Status anderer Graphemketten verändern kann, indem er sie zu Komposita zusammenfügt.
Neben diesem expliziten Bindestrich gibt es auch einen impliziten Trennstrich an allen möglichen Trennstellen> , an denen er ausschließlich am Zeilenende sichtbar wird. Obwohl hierbei häufig pauschal von "Silbentrennung" gesprochen wird, gibt es in den verschiedenen Schriftsystemen der Welt Trennverfahren an Silben-, Morphem- und Graphem-, selten auch an Glyphen-Grenzen.

Die Zusammensetzung mehrerer Einzelwörter zu einem neuen Wort kann je nach orthographischen Konventionen allerdings nicht nur durch einen Bindestrich , sondern auch durch direktes Aneinanderhängen oder – häufig außerhalb von Texten, aber nicht regelkonform – mit Leerzeichen (d. h. horizontaler Weißraum oder Zeilenumbruch ) erfolgen, wobei die Anfangsgroßschreibung des hinteren Gliedes nicht erhalten bleiben muss, da nicht unbedingt ein Wortbeginn enthalten ist.

Das Graphem oder kann also die Ausprägungen , , ("leer"), ("Spatium"), und ("Zeilenwechsel") haben. Dabei ist zu beachten, dass diese scheinbaren Allographen die Teilwörter unterschiedlich stark aneinanderbinden und damit zur Bedeutungsdifferenzierung genutzt werden können, wodurch sie ggf. zu unterschiedlichen Graphemen werden: , und sowie sind Teil der Extension desselben graphischen Wortes, wohingegen in und verschiedene semantisch relevante Akzente gesetzt werden, nämlich im ersten Fall in Abgrenzung bspw. zu oder und im zweiten Fall bspw. zu oder .

Für das erste Graphem nach der initialen Wortgrenze kann entweder syntaktisch (wenn es mit initialer Satzgrenze zusammenfällt), grammatisch (beim nichtpronominalen Kopf einer Nominalphrase) oder lexemisch (bei Namen und Substantiven) motiviert eine Allographiebeschränkung greifen, der zufolge für den ersten Buchstaben des Graphems nur eine Majuskel zulässig ist.
Ähnlich galten früher, vor allem im Fraktursatz, für das letzte Graphem vor einer medialen oder finalen Wortgrenze Sonderbedingungen, wenn es mit dem Buchstaben endet, welcher dann nicht mit dem Graph , sondern als dargestellt wurde, und zusätzlich greift für ein ursprüngliches eine orthographisch Ligationsregel, die statt den Graph fordert.

Damit lässt sich dieses Beispielwort mit seinen diachronen Varianten, die z.T. noch nicht oder nicht mehr orthographisch valide sind, graphemisch folgendermaßen beschreiben:
Diese Variantenanalyse trifft allerdings noch keine Entscheidung über tatsächlich im deutschen Schriftsystem vorhandene Grapheme, sondern identifiziert lediglich erste Kandidaten.

Graphische Zeichen eines Schriftsystems, die nicht zur Wortbildung, sondern nur dem Satzbau dienen, werden teils ebenfalls als Grapheme angesehen und "syntaktische Grapheme" oder "Syngrapheme" genannt.
Dazu gehören neben der Interpunktion auch indirekt sichtbare Funktionsgrapheme wie die Großschreibung am Satzanfang, die ggf. dafür sorgt, dass auch Wörter, die nach lexikalischen Regeln mit einem Kleinbuchstaben beginnen, mit einer Initialmajuskel geschrieben werden.

Unter den Satzzeichen gibt es verschiedene Typen: einige können nur am Anfang oder Ende eines Wortes oder Satzes stehen, andere auch oder nur mittig (trennend oder verbindend) und wieder andere, bspw. Klammern und Anführungszeichen, treten i.d.R. nur als Paar auf.

Zur angewandten Linguistik gehört an der Schnittstelle zur Computertechnik die elektronische Kodierung von Schrift und ihrer Zeichen. Dabei gibt es glyphenbasierte Ansätze wie in 7bit-SMS-Nachrichten, graphem- / konstituentenbasierte Ansätze oder framebasierte Ansätze.

In Unicode werden die Schriftzeichen zwar prinzipiell als Kombinationen von Grundeinheiten, welche kodiert werden, modelliert, aber teilweise aus Kompatibilitätsgründen und teilweise aus Pragmatik gibt es viele vorgefertigte Zeichen, was es erforderlich macht, kanonische Äquivalenzen und bevorzugte Codes festzulegen (bspw. NFC). So kann ein als Kombination aus und oder direkt als gespeichert werden. Die koreanischen Hangul liegen nicht nur in From von 70 miteinander kombinierbaren Einzelkomponenten ("Jamo"), sondern auch in über 11000 Silbenblöcken vor, während Sinogramme nur als Gesamteinheit kodiert werden.

Die Zeichenkodierung braucht nicht auf einfache Weise mit der Eingabe über die Tastatur o.ä. korrelieren, bspw. werden Akzente über Tottasten vor dem Basisbuchstaben eingetippt, aber gespeichert werden sie andersherum oder als eine gemeinsame Einheit.

Der Unicode-Standard verwendet den Begriff "Graphem" in vereinfachter und sprachunabhängiger Bedeutung. Ein Graphem ("Grapheme") ist danach entweder „eine minimale distinktive Einheit der Schrift im Kontext eines Schriftsystems, also ein graphisches Zeichen, mit dem zwei Wörter voneinander unterschieden werden können“ (Minimalpaaranalyse) oder das, „was Benutzer für ein Schriftzeichen ("Character") halten“. Außerdem werden die Begriffe Graphembasis ("Grapheme Base"), Graphemhaufen ("Grapheme Cluster") – „eine horizontal segmentierbare Texteinheit, bestehend aus irgendeiner Graphembasis kombiniert mit beliebig vielen breitenlosen Markern“ –, Graphemerweiterung ("Grapheme Extender") – alle nullbreiten Marker, Verbinder und Trenner sowie einige nichtnullbreite Marker –, graphisches Schriftzeichen ("Graphic Character") – Buchstabe ("Letter"), kombinierender Marker, Ziffer, Interpunktion, Symbol oder Spatium.





</doc>
<doc id="8440" url="https://de.wikipedia.org/wiki?curid=8440" title="Texas">
Texas

Texas (englisch oder [], von cadd. "táyshaʔ" ‚Freunde‘ oder ‚Verbündete‘)
ist ein Bundesstaat im mittleren Süden der Vereinigten Staaten von Amerika. Texas hat den Beinamen Lone Star State (Staat des einsamen Sterns), da seine Flagge nur einen Stern verwendet. Texas hat von allen US-Bundesstaaten nach Alaska die zweitgrößte Fläche und nach Kalifornien die zweitgrößte Bevölkerungszahl. Mit seinen 254 Countys hat Texas die meisten Countys eines Bundesstaats der Vereinigten Staaten.

Texas grenzt im Süden an Mexiko. Dies macht etwa die Hälfte der Grenze zwischen den Vereinigten Staaten und Mexiko aus. Daneben grenzt Texas an die Bundesstaaten New Mexico im Westen, nördlich an Oklahoma, nordöstlich an Arkansas und Louisiana im Osten.

Von der Küste aus, die fast ihrer ganzen Länge nach von Lagunen eingefasst ist, erstreckt sich 50 bis 100 Kilometer landeinwärts ein relativ flaches Gebiet, das zum Teil sehr fruchtbar und für den Anbau von Baumwolle, Zuckerrohr und stellenweise auch Reis vorzüglich geeignet ist. Dahinter erhebt sich ein wellenförmiges hügeliges Land, welches, bis 320 Kilometer breit, den ganzen Nordosten des Staats umfasst und großteils von Prärien bedeckt ist. Der nordwestliche Teil des Staatsgebiets ist Berg- und Hochland und besteht zum Teil aus einem 1300 Meter hohen wüsten Sandsteinplateau (Llano Estacado). Der Norden, auch Texas Panhandle genannt, ist sehr fruchtbar und wird für die Viehzucht genutzt. Im gesamten Süden und Westen wurde bis Anfang der 1980er Jahre Erdöl gefördert. An Flüssen ist Texas reich, wenn auch die wenigsten während des gesamten Jahres schiffbar sind. Der Red River scheidet es von Oklahoma und Arkansas, der Sabine von Louisiana und der Rio Grande von Mexiko. Weitere wichtige Flüsse sind der Colorado River, der Pecos River und der Brazos River.

Texas lässt sich in drei verschiedene Klimazonen unterteilen.

Die größte Stadt Houston liegt im Südosten des Staates. Die zweitgrößte Stadt San Antonio liegt im Süden, Dallas, die drittgrößte Stadt, und Fort Worth liegen im Nordosten. El Paso liegt im äußersten Westen und Corpus Christi liegt im Süden an der Golfküste.

Texas liegt im Süden der USA und grenzt im Norden an Oklahoma, im Nordosten an Arkansas, im Osten an Louisiana, im Südwesten an Mexiko und im Nordwesten an New Mexico.

Texas ist in 254 Countys unterteilt. Dies sind die meisten Countys eines Staates der USA.

Texas hatte 2010 25.145.561 Einwohner (US Census 2010), davon 45,3 % Weiße (ohne Hispanics und Latinos), 11,8 % Afroamerikaner, 3,8 % Asiaten, 0,7 % Indianer, 0,1 % Hawaiianer oder von anderen Pazifikinseln stammend. 2,7 % zwei oder mehr Gruppen zugehörig. 37,6 % der Gesamtbevölkerung waren Hispanics. 2010 stellten die Minderheiten 50,2 % der Bevölkerung. Texas gilt somit als Majority-Minority-State. Für Mitte 2017 wird die Bevölkerung auf 28.304.596 Einwohner geschätzt. Die Bevölkerung von Texas gehört zu den am schnellsten wachsenden der Vereinigten Staaten.

2014 waren 

Das Medianalter betrug 34,3 Jahre (nationaler Durchschnitt der 50 US-Bundesstaaten: 37,7 Jahre).
49,6 % der Bevölkerung waren männlich und 50,4 % weiblich.

9,6 % der Einwohner sind deutscher Abstammung und bilden damit die größte Gruppe nach den Mexikanischstämmigen. Es folgen die Gruppen der Irisch- (6,9 %), Englisch- (6,2 %), Amerikanisch- (5,6 %) und Französischstämmigen (2,0 %).(Stand 2014)

Die wichtigsten Religionsgemeinschaften im Jahr 2000:

4.368.969 Katholische Kirche, 3.519.459 Southern Baptist Convention, 1.022.342 United Methodist Church

Es gibt viele andere, vor allem protestantisch geprägte Konfessionen.

Große Teile des Ostens, des Nordens und des Zentrums Texas’ werden von Weißen bewohnt, die protestantischen Kirchen angehören. Es handelt sich vor allem um Nachfahren von Einwanderern aus Großbritannien und Irland. Große Teile von Zentral- und Südost-Zentral-Texas werden von Weißen bewohnt, die deutsche Vorfahren haben. Afroamerikaner, die in der Vergangenheit ein Drittel der Bevölkerung ausmachten, leben vor allem in den Teilen von Texas, wo vor dem Bürgerkrieg die Baumwollplantagen besonders verbreitet waren sowie in Dallas und Houston.

Einwohner 2000/2010: U.S. Census 2000 (grün) und U.S. Census 2010 (rot)

In Texas wurden die bislang ältesten, datierbaren Funde menschlicher Artefakte in Nordamerika gemacht. Im Buttermilk Creek Complex im Bell County wurden auf ein Alter zwischen 15.500 und 13.200 Jahren Before Present bestimmte Steingeräte und Abschläge entdeckt, die unter einem Fundhorizont der Clovis-Kultur lagen.

Texas liegt in zweien der Nordamerikanischen Kulturareale, einerseits Prärien und Plains, andererseits dem Südwesten. 
Die texanischen Paläoindianer der Zeit von 9200 bis 6000 v. Chr. standen in Beziehung zur Clovis-Kultur und der von Folsom. Sie hinterließen vor allem Spuren im Norden, im heutigen Alibates Flint Quarries National Monument. Sie lebten von der Jagd auf Mammut und Bison (bison latifrons). Die ältesten Funde machte man mit dem "Midland Man", der 1953 im Midland County gefunden wurde, einer 1983 entdeckten Frau nahe Leander im Williamson County sowie mit einem Mann und einem Jungen bei Waco. Um 4000 v. Chr. entstanden am Pecos River Petroglyphen. Die ersten Maisbauern lebten um 1500 v. Chr. am unteren Pecos.

Um 500 v. Chr. entstand im Osten eine sesshafte Kultur, die unter dem Einfluss der Mississippi-Region stand, jenseits des Rio Pecos dominierte die Mogollon-Kultur.

Nach 700 begann der Bogen die Speerschleuder (Atlatl) zu verdrängen, Tonwaren wurden gefertigt. Der Handel mit Obsidian reichte im Norden bis in die Rocky Mountains, im Süden vor allem bis nach Teotihuacán. Zwischen etwa 800 und 1500 bestand die sogenannte "Buried City", steinerne Wohnhäuser südöstlich von Perryton im Ochiltree County.

Die um 1150 bis 1450 lebenden "Plains Village People" gelten als Vorfahren der Caddo, Pawnee und Wichita. Sie lebten in dauerhaft bewohnten Großdörfern, deren Häuser aus einem, aber auch aus bis zu hundert Räumen bestanden. Vor 1500 sind sie, vermutlich durch Apachen, westwärts vertrieben worden.

Die Kulturen waren bei Ankunft der Spanier in zahlreiche ethnische Gruppen von verschiedener Zusammensetzung gegliedert. Zu ihnen gehörten die Alabama, Apachen, Atakapan, Bidai, Caddo, Coahuiltecan, Comanche, Cherokee, Choctaw, Coushatta, Hasinai, Jumano, Karankawa, Kickapoo, Kiowa, Tonkawa und Wichita.

Heute gibt es in Texas nur drei von der Bundesregierung in den 1960er- und 1970er-Jahren anerkannte Stämme, die Alabama-Coushatta, die aus der Verschmelzung einer Gruppe der Alabama mit den Coushatta hervorgegangen sind, der Kickapoo Traditional Tribe of Texas (am Rio Grande im Maverick County), sowie die Ysleta del Sur Pueblo in El Paso und Socorro. Daneben gibt es noch eine südtexanische Untergruppe des "Kickapoo Tribe of Oklahoma". Sie waren 1839 von der Republik Texas nach Mexiko vertrieben worden.

1519 entstand durch den Spanier Alonso Álvarez de Pineda die erste kartografische Erfassung der texanischen Küste. Dies war auch der Beginn der spanischen Inbesitznahme des Territoriums. Im Jahr 1528 erlitt der Spanier Cabeza de Vaca an der Küste in der Gegend des heutigen Galveston Schiffbruch. Eine kleine Gruppe Überlebender marschierte quer durch die Indianergebiete bis nach Mexiko-Stadt und sorgte später für die Verbreitung der Legende von den „Sieben Städten aus Gold“. Der spanische Abenteurer Coronado, angezogen durch diese Geschichte, durchquerte den westlichen Teil von Texas und Teile des heutigen New Mexico bis hinauf nach Kansas. Obwohl er die goldenen Städte nicht fand, hielt sich das Gerücht trotzdem über die Jahrhunderte.

In der Folgezeit entstanden viele Ortschaften und vor allem Missionen im heutigen Staatsgebiet von Texas. 1621 gründeten spanische Einwanderer die Stadt Corpus Christi de la Isleta. 1659 folgte El Paso.
Zu einem französischen Kolonisierungsversuch auf dem Territorium von Texas kam es 1685. Der Abenteurer Robert Cavelier de La Salle erreichte per Schiff die Matagorda Bay und gründete dort das Fort St. Louis. Die Ansiedlung litt jedoch stark unter Indianerangriffen, Krankheiten und dem Verlust wichtigen Materials durch Schiffbrüche. Bereits zwei Jahre später wurde La Salle von seinen eigenen Leuten ermordet, als er versuchte, Hilfe zu holen. St. Louis wurde daraufhin aufgegeben, Texas war wieder spanisch. In den Siedlungen im Delta des Mississippi konnten sich die Franzosen jedoch behaupten. 1686 bis 1689 suchte Alonso de Leon, Gouverneur von Coahuila, die französische Kolonie und fand nur wenige Überlebende.

Im Zuge einer weiteren spanischen Expedition wurden mehrere Missionsstationen gegründet. So entstand 1718 die Mission San Antonio de Valero. Über 100 Jahre später ging diese Mission in die Geschichte ein als The Alamo. 1786 fand Pedro Vial einen Pfad von San Antonio nach Santa Fe, der aber keine Bedeutung als Handelsweg erlangte.

1821 wurde Texas ein Teil des von Spanien unabhängig gewordenen Mexiko. Schon während des Mexikanischen Unabhängigkeitskrieges sammelten sich hier viele Abenteurer aus den Vereinigten Staaten an. Nachdem der nordamerikanische Oberst Stephen F. Austin 1823 die Genehmigung der Zentralregierung Mexikos erhalten hatte, mit 300 Familien im Staatsgebiet von Texas zu siedeln, gründete er die Stadt San Felipe de Austín. Die Vereinbarung mit Austin war sehr einfach. Er musste seine US-amerikanische Staatsbürgerschaft gegen eine mexikanische eintauschen und unterstand somit der mexikanischen Gerichtsbarkeit. Immer mehr Siedler aus dem Norden erreichten den Golf von Mexiko. Es war der Beginn der angloamerikanischen Kolonisation, bis 1835 siedelten etwa 45.000 Menschen aus dem Norden in Texas.
Die Spannungen zwischen amerikanischen Siedlern auf der einen und den Mexikanern und der mexikanischen Regierung unter Präsident General Santa Anna auf der anderen Seite wurden immer heftiger, als Mexiko die Sklaverei verbot. Weil die Vereinigten Staaten den ganzen Staat Texas kaufen wollten, verboten mexikanische Landesbehörden 1830 die weitere Immigration aus den USA. Besonders religiöse, kulturelle und politische Probleme schienen unüberbrückbar. Doch neue Gesetze und Verordnungen gewährten den Siedlern in Texas soviel Ausnahmen und Freiheiten, dass die Spannungen abnahmen und 1835 zuerst ein relativ ruhiges Jahr war. Landspekulanten aus den USA schürten jedoch das Misstrauen gegen Mexiko. Als Stephen F. Austin bei einem Besuch in Mexiko-Stadt inhaftiert wurde und sich auf Grund persönlicher Enttäuschung gegen einen Verbleib in Mexiko aussprach, sahen Separatisten ihre Chance. Nach Austins Rückkehr erklärten sie in einer eilig einberufenen Versammlung die Loslösung Texas’ von Mexiko. Santa Anna entsandte deswegen kurz darauf Truppen (ca. 5000 Mann) nach Texas. Am 2. Oktober 1835 begann mit dem Gefecht von Gonzales der Texanische Unabhängigkeitskrieg.

Am 2. März 1836 riefen die Texaner, im Vertrauen auf den Beistand der herrschenden Partei in den Vereinigten Staaten, die sich für eine Vermehrung der Sklavenstaaten einsetzte, die unabhängige Republik Texas aus und ernannten den General Sam Houston zum militärischen Oberbefehlshaber. Das mexikanische Heer unter Santa Anna besetzte im Zuge der Feindseligkeiten San Felipe de Austín, die Hauptstadt von Texas.
Am 6. März 1836 wurde die Missionsstadt Alamo von den Mexikanern nach dreizehntägiger Belagerung eingenommen. Dabei kamen alle der etwa 190 Verteidiger ums Leben. Unter den Gefallenen waren auch Davy Crockett, James Bowie und William Travis.

Die etwa 1600 Soldaten umfassenden mexikanischen Truppen wurden am 21. April 1836 in der Schlacht von San Jacinto von den Texanern unter Sam Houston überraschend geschlagen. Der mexikanische Präsident General Santa Anna wurde gefangengenommen. In den folgenden Jahren versuchte die mexikanische Regierung die Unabhängigkeit der Texaner durch weitere militärische Expeditionen rückgängig zu machen; scheiterte aber.

Als unabhängige Republik wurde Texas von Frankreich und dem Vereinigten Königreich am 23. November 1839 beziehungsweise am 14. November 1841 anerkannt. Erster Präsident der unabhängigen Nation und Republik Texas wurde der General Sam Houston. Mit Unterbrechungen war er es bis kurz vor dem Zusammenschluss mit den USA. Von 1856 bis 1861 war Sam Houston Gouverneur des US-Bundesstaates. Stephen F. Austin wurde Außenminister seines Staates, starb aber bereits zwei Monate nach Amtsantritt an einem Lungenleiden. Innenpolitisch war die junge Republik Texas in zwei Lager gespalten. Eine Gruppe, die von Sam Houston angeführt wurde, trat für einen schnellen Beitritt zu den Vereinigten Staaten ein. Die andere Gruppe, die von Mirabeau B. Lamar dem zweiten Präsidenten der Republik, angeführt wurde, wollte einen solchen Schritt vermeiden. Stattdessen wurde an eine Expansion der Republik Texas bis zum Pazifik gedacht. Dabei sollte Texas quasi ein Pufferstaat zwischen Mexiko und den Vereinigten Staaten bilden. Schließlich setzte sich die Gruppe um Sam Houston durch.

In Texas verlangte die Mehrheit den Anschluss an die Vereinigten Staaten. Das Land wurde daraufhin am 19. Februar 1845 von den USA annektiert; der US-Kongress billigte die Annexion am 1. März 1845. Die Aufnahme in den Staatenbund erfolgte am 29. Dezember 1845. Hierüber entbrannte 1846 der Mexikanisch-Amerikanische Krieg, der am 2. Februar 1848 mit dem Vertrag von Guadalupe Hidalgo endete. Mexiko verzichtete auf seine Ansprüche auf Texas und das Gebiet zwischen Rio Grande und Nueces River, doch schlug die Regierung der USA durch Beschluss vom 7. September 1850 einen Teil dieses Gebiets New Mexico zu, das inzwischen als Territorium an die Union angegliedert worden war. Texas erhielt hierfür eine Entschädigung von 10 Millionen Dollar.
Der Wahlsieg des für seine strikten Prinzipien in der Sklavenfrage bekannten Abraham Lincoln in der Präsidentschaftswahl vom 6. November 1860 löste, beginnend mit South Carolina am 20. Dezember 1860, die Loslösung der sklavenhaltenden Südstaaten von der Union aus. Texas, dessen auf einem Konvent in Austin am 1. Februar 1861 beschlossene Austrittserklärung am 23. Februar per Referendum gebilligt wurde und damit zum 2. März in Kraft trat, war der siebte und letzte Südstaat, der noch vor dem Amtsantritt Abraham Lincolns am 4. März und dem Beginn des Sezessionskrieges aus der Union austrat und sich den Anfang Februar 1861 gegründeten Konföderierten Staaten von Amerika anschloss ("siehe auch:" Texas-Deutsche im Amerikanischen Bürgerkrieg). Nach Beendigung des Sezessionskrieges im Jahr 1865 setzte US-Präsident Andrew Johnson den Unionisten Andrew Jackson Hamilton als provisorischen Gouverneur von Texas ein. Der Prozess der Reconstruction endete für Texas am 30. März 1870 mit der Wiederaufnahme der Vertreter von Texas in den Kongress der Vereinigten Staaten.

Bis heute hält sich die fehlerhafte moderne Sage, dass Texas der einzige US-Bundesstaat sei, der das Recht zum beliebigen Wiederaustritt aus den Vereinigten Staaten habe, da er durch den Abschluss eines Vertrages in die Union aufgenommen wurde.

Deutsche Einwanderer haben den US-Bundesstaat wesentlich mitgeprägt. Caroline Ernst war mit ihrer Familie die erste deutsche Siedlerin. Ein schwärmerischer Brief ihres Vaters nach Deutschland 1832 war einer der Auslöser für das deutsche Engagement in Texas.

Die organisierte deutsche Einwanderung begann 1834 mit den "Dreißigern" und ist vor allem der "Gießener Auswanderungsgesellschaft" und dem "Verein zum Schutze deutscher Einwanderer in Texas", auch bekannt als "Mainzer Adelsverein", zu verdanken. Nach der Märzrevolution von 1848 folgten noch einige der "Forty-Eighters".

Zu Beginn des 20. Jahrhunderts waren etwa 100.000 Texaner deutschsprachig. Die meisten siedelten in Zentraltexas im so genannten Texas Hill Country, den "German Hills" im Bereich von Austin und San Antonio. Frühe Siedlungen waren die "Latin Settlements", gegründet von deutschen Auswanderern. Vor allem der Einfluss des Ersten Weltkriegs führte zu einem deutlichen Rückgang der deutschen Sprache. Der deutsch-englische Mischdialekt wird auch Texasdeutsch genannt.

Der Einfluss deutscher Einwanderer zeigt sich an Ortsnamen wie New Braunfels (gegründet 1845 von Carl Prinz zu Solms-Braunfels, einem Mitglied des Mainzer Adelsvereins) und der Bezeichnung des Wasserparks "Schlitterbahn". In New Braunfels, etwa 65 km südlich der Hauptstadt Austin, wird alljährlich das "Wurstfest" gefeiert. Im Ortsteil Gruene, gegründet 1872 von Henry D. Gruene, befindet sich die älteste erhaltene Dance Hall in Texas, die "Gruene Hall". Sie ist heute ein historischer Ort für Live-Musik und Tanzveranstaltungen.

Auch bei Fredericksburg (gegründet 1846 und zu Ehren von Prinz Friedrich von Preußen (1794–1863), einem weiteren Mitglied des Mainzer Adelsvereins, „Friedrichsburg“ benannt) ist eine deutsche Kolonie. Die örtliche Handelskammer begrüßt Besucher im Internet mit „Willkommen“, die Speisekarten von Restaurants sind teilweise zweisprachig.

Bekannt wurde hier auch die Ansiedlung Luckenbach, welche in einem Lied, interpretiert von Waylon Jennings und Willie Nelson mit dem Titel "Luckenbach, Texas" erwähnt wurde. Das Lied brachte es immerhin zur Nummer 1 der US-Country-Charts. Bis zu 20.000 Texaner sprechen heute noch deutsch.

Auch Sorben aus der Lausitz wanderten Mitte des 19. Jahrhunderts in nennenswerter Zahl nach Texas aus und gründeten dort unter Führung von Jan Kilian unter anderem die Siedlung Serbin. Obwohl die Einwohner schon lange kein Sorbisch mehr sprechen, erinnern u. a. das Texas Wendish Heritage Museum und eine "Wendish Heritage Society" an dieses Kapitel texanischer Geschichte.

Texas gilt heute in Europa als Hochburg des amerikanischen Konservatismus. Die Präsidenten George W. Bush und sein Vater George H. W. Bush machten in Texas Karriere. In den USA wird Texas zwar als durch und durch konservativ angesehen, aber Staaten wie Mississippi oder Alabama rangieren in der internen Ansicht noch deutlich vor Texas. Sowohl in den USA als auch in Europa fällt Texas durch die rigorose Anwendung der Todesstrafe auf. In keinem anderen Bundesstaat leben so viele Kinder ohne Krankenversicherung (14 %).

Politisch ist Texas seit den 1970ern eine Hochburg der Republikaner. Nur die Demokraten John F. Kennedy, Lyndon B. Johnson, Hubert H. Humphrey und Jimmy Carter konnten seit Beginn der 1960er-Jahre in Texas siegen. Danach aber gewannen die Republikaner in Texas stets mit deutlichem Vorsprung. Hochburgen der Demokraten sind heute die Countys zwischen San Antonio und der mexikanischen Grenze sowie die Countys im Raum El Paso und Houston. Im Electoral College stellt Texas ab 2012 38 Wahlmänner, die Zahl nimmt stetig zu, da die texanische Bevölkerung schneller wächst als die der gesamten USA. Im US-Senat wird Texas von den Republikanern John Cornyn und Ted Cruz vertreten. Die texanische Delegation im Repräsentantenhaus des 114. Kongresses besteht aus 25 Republikanern und elf Demokraten.



Texas ist ein Staat der USA, der die Todesstrafe anwendet. Sie wird von den Strafgerichten bei entsprechender Schwere des Verbrechens verhängt. Der Gouverneur von Texas kann im Gegensatz zu anderen Bundesstaaten die Häftlinge nicht in Eigenregie begnadigen. Eine Begnadigung durch den Gouverneur ist nur möglich, wenn der texanische Begnadigungsausschuss (Texas Board of Pardons and Paroles) eine Begnadigung empfiehlt. Liegt ein negativer Bescheid des Ausschusses vor, kann der Gouverneur lediglich die Hinrichtung um 30 Tage aufschieben. Bei der Zahl der Vollstreckungen nimmt Texas die Spitzenposition in den USA ein; von 1976 bis September 2015 wurden in Texas 528 Menschen hingerichtet, was 37 % aller Hinrichtungen in den USA entspricht. In der texanischen Gesetzgebung findet die Sunset-Klausel Anwendung.

Körperliche Züchtigungen von Schülern sind durch das sogenannte Paddle erlaubt. Texas zählt kontinuierlich zu den fünf US-Bundesstaaten, in denen die meisten "Paddlings" durchgeführt werden.

Das größte Kunstmuseum des Staates Texas ist das Museum of Fine Arts, Houston in Houston, das über eine Sammlung von 56.000 Objekten verfügt und zudem bedeutende Ausstellungen beheimatet.

In Texas spielt das Barbecue (kurz BBQ) im Alltag eine wichtige Rolle. In Taylor, nordöstlich von Austin, findet jährlich das „International Barbeque Cookoff“, ein Fest mit vielen BBQ-Speisen, statt. Darüber hinaus ist Texas für seine Chili-Gerichte bekannt. Die Küche des Bundesstaats ist mexikanisch beeinflusst, eine Spezialität ist daher das Chicken Fried Steak.

Außerdem gibt es in Texas ein National Monument: Alibates Flint Quarries National Monument im Norden des Staates. Dabei handelt es sich um eine Fundstelle von Feuerstein, die Indianer und ihre Vorgänger-Kulturen zwischen 11000 v. Chr. und ca. 1870 genutzt haben, um aus dem Material Werkzeuge und Waffen zu fertigen.

Texas ist Heimat unterschiedlicher Sportmannschaften in den fünf großen Sportligen des Landes. Die Basketballteams der San Antonio Spurs, Houston Rockets und der Dallas Mavericks um Superstar Dirk Nowitzki spielen allesamt in der NBA. American Football genießt in Texas ebenfalls einen sehr hohen Stellenwert. Mit den Dallas Cowboys und den Houston Texans sind zwei Teams in der NFL beheimatet. In der MLB gehen mit den Houston Astros und den Texas Rangers ebenfalls zwei in Texas beheimatete Teams an den Start. Die Dallas Stars spielen in der Nordamerikanischen Eishockeyliga NHL. Auch Fußball erfährt in Texas stetig wachsende Begeisterung. Die Houston Dynamos und der FC Dallas spielen in der höchsten Fußballliga, der MLS. 

Wichtigste Wirtschaftszweige sind:


In Texas wird das meiste Öl der USA gefördert. Texas ist nach Kalifornien der zweitwichtigste Industrie- und Handelsstaat der USA. Das BIP betrug im Jahr 2016 1.616 Milliarden USD, was ca. 8,76 Prozent am Gesamt-BIP der USA ausmachte. Damit steht Texas auf einer Stufe mit Ländern wie Brasilien oder Kanada. Das reale Bruttoinlandsprodukt pro Kopf (engl. per capita GDP) betrug 2016 USD 58.028 (Durchschnitt der 50 US-Bundesstaaten: USD 57.118; nationaler Rangplatz: 19). Die Arbeitslosenrate lag im November 2017 bei 3,8 % (Landesdurchschnitt: 4,1 %).

In Texas gibt es über 10.000 Windkraftanlagen; sie haben eine Gesamtkapazität von 18.500 Megawatt (Stand 2016). Wäre Texas ein eigenständiger Staat, wäre es der sechstgrößte Windenergieerzeuger der Welt knapp hinter Spanien.

Texas verfügt mit dem Hafen von Houston über den, nach umgeschlagenen Tonnen, zweitgrößte Seehafen der Vereinigten Staaten.

Die wichtigsten staatlichen Universitäten sind in dem University of Texas System mit Hauptstandort in Austin, dem Texas A&M University System, dem Texas Tech University System, dem University of Houston System und dem Texas State University System zusammengefasst. Weitere staatliche Hochschulen sind die University of North Texas, die Midwestern State University, die Stephen F. Austin State University und die Texas Southern University.

Die bekanntesten privaten Hochschulen sind die Rice University, die Southern Methodist University, die Baylor University sowie die Texas Christian University.

Weitere Hochschulen sind in der Liste der Universitäten in Texas verzeichnet.

Die erste Nationalflagge von Texas diente als „bürgerliche Flagge“ seit November 1835. Sie zeigte die Nationalfarben von Mexiko mit der Aufschrift „1824“ (Jahr der mexikanischen Verfassung).
Bei der angeblichen Nationalflagge vom März 1836 – blau, mit weißem Stern und den kreisförmig angeordneten Buchstaben T E X A S – handelt es sich um eine Fälschung.

Seit 1929 ist „Texas, our Texas“ Staatslied.

Seit 1901 ist die Lupinenart Bluebonnet ("Lupinus texensis") Staatsblume von Texas.

Seit 1927 ist die Spottdrossel (Mockingbird) Staatsvogel.

Seit 1919 ist der Pekanbaum Staatsbaum. Seine Nüsse finden vielfältige Verwendung in der texanischen Küche.

Seit 2005 ist der Blue Lacy "State Dog" von Texas.

Seit 2011 ist der Western Swing aufgrund eines Beschlusses von Senat und Repräsentantenhaus des Staates Texas die „"official State Music of Texas"“.




</doc>
<doc id="8449" url="https://de.wikipedia.org/wiki?curid=8449" title="E-Learning">
E-Learning

Unter E-Learning ( = „elektronisch unterstütztes Lernen“, wörtlich: „elektronisches Lernen“), auch als E-Lernen "(E-Didaktik)" bezeichnet, werden – nach einer Definition von Michael Kerres – alle Formen von Lernen verstanden, bei denen elektronische oder digitale Medien für die Präsentation und Distribution von Lernmaterialien und/oder zur Unterstützung zwischenmenschlicher Kommunikation zum Einsatz kommen.

Für E-Learning finden sich als Synonyme auch Begriffe wie: "Online-Lernen" "(Onlinelernen)", "Telelernen", "multimediales Lernen", "computergestütztes Lernen", "Computer-based Training", "Open and Distance-Learning" u. a.

Die wahrscheinlich erste Lernmaschine wurde 1588 vom italienischen Ingenieur Agostino Ramelli entwickelt, als er für den König von Frankreich ein Leserad erfand. Durch dieses Leserad wurde das Zurückgreifen auf verschiedene Literaturquellen ohne Hin- und Herlaufen ermöglicht.

1866 meldete der New Yorker Webstuhlentwickler Halcyon Skinner ein erstes US-Patent auf eine Maschine an, mit der sich Rechtschreibung üben ließ. Bis 1936 wurden 700 weitere Patentanträge für vergleichbare „Übungsmaschinen“ bestätigt.

1938 entwickelten B. F. Skinner und James G. Holland lineare Lernprogramme nach dem Gesetz der operanten Konditionierung "(Skinner-Holland’sches Lernprogramm)". Demnach wurden den Lernenden der Lehrstoff in kleinen Schritten (Frames) präsentiert, jeweils gefolgt von Fragen.

1959 erfand Norman Crowder die verzweigten Lernprogramme, bei denen eine fehlerabhängige Darbietung des Lehrinhaltes ermöglicht wurde. Dadurch konnte der Lernprozess individualisiert werden.

In Deutschland wurden seit 1964 Lehrmaschinen entwickelt, jedoch weder nach Vorstellungen von Skinner/Holland noch nach denen von Crowder. Die Lernprogramme, die in Deutschland entwickelt wurden, dienten der Gruppenschulung. Beispiele für solche Lehrautomaten sind der Geromat III, bei dem drei Lernende die richtige Antwort angeben mussten, um im Lernstoff vorwärtszukommen, und das Lernprogramm „Bakkalaureus“. An diesem Programm konnten bis zu 64 Personen gleichzeitig lernen, es waren Verzweigungen möglich und drei verschiedene Schwierigkeitsstufen einstellbar.

Im Jahre 1971 startete die NSF (National Science Foundation) in den USA zwei Großprojekte mit dem Ziel, die Effizienz von computergestützter Instruktion für den Unterricht zu beweisen. Zum einen handelte es sich dabei um das Projekt TICCIT ( "Time-shared, Interactive, Computer-Controlled Information Television") und zum anderen um das Projekt PLATO (engl.: "Programmed Logic for Automated Teaching Operations"). Als Fazit dieser beiden Untersuchungen konnte die NSF den computerunterstützten Unterricht als wirksames Hilfsmittel bestätigen.

Anfang der 1970er Jahre gab es eine Reihe von Forschungs- und Entwicklungsprojekten zum computergestützten Unterricht. Neben dem Einsatz an Schulen entwickelte sich ein weiterer Schwerpunkt: die betriebliche Aus- und Weiterbildung. So wurden beispielsweise interaktive Videos zum Verhaltenstraining für Vertriebsmitarbeiter eingesetzt. 1978 entwickelte die Agentur M.I.T. zusammen mit ihrem Kunden Hertie eines der ersten Computer Based Trainings (CBT) und die dazu passende Hardware „Videomit 2000“. Seit Anfang der 1990er Jahre wurden die Planspiele unter Forschungsaspekten bedeutsam.

Alfons Rissberger hat im Kultusministerium Rheinland-Pfalz bereits 1986 den BLK-Modellversuch TOAM als ersten E-Learning-Schulversuch in Europa zur wissenschaftlich begleiteten Erprobung computerunterstützter Lernsysteme im Fach Mathematik an allen berufs- und allgemeinbildenden Schularten inkl. Grundschulen initiiert. Im Jahr 1995 wurde in der Frankfurter Allgemeinen Zeitung (FAZ) eine Ausarbeitung von Rissberger und Günter Serfas, dem Schulleiter des Gauß-Gymnasiums Worms, veröffentlicht, wo sich beide fiktiv mit der Zukunft des E-Learnings beschäftigten. Die dort aufgezeigten Vorstellungen sind heute schon in vielen Schulen zur Tatsache geworden.

Seit Ende der 1990er Jahre erfuhr das E-Learning durch die Verbreitung des Internets einen starken Aufschwung. Der Begriff „E-Learning“ hat sich seit Mitte der 1990er Jahre etabliert. Das Bundesministerium für Bildung und Forschung hat seitdem einige Initiativen ins Leben gerufen. Dazu gehören zum Beispiel die Initiativen „Schulen ans Netz“ (zum Ende des Jahres 2012 eingestellt), „Neue Medien in der Bildung“ und „Notebook-University“.

Einen Ausblick in die "Zukunft" des E-Learning bieten Forschungsprogramme, die Innovationsentwicklung und Innovationstransfer in den Mittelpunkt stellen.

Unter E-Learning versteht man die Unterstützung von Lehr-/Lernprozessen durch digitale Medien oder Werkzeuge. Neben dem Ausdruck E-Learning existieren verschiedenste andere Ausdrücke, wie des computerbasierten Lernens, Onlinelernens, multimedialen Lernens etc.
Da die Begriffsbestimmung des E-Learning noch keine allgemein anerkannte Definition erbracht hat, versuchte man, E-Learning durch verschiedene Facetten zu beschreiben: Interaktivität, Multicodalität, Multimedialität und Multimodalität.





E-Learning kann auf sehr unterschiedlichen Technologien basieren und in unterschiedlichen didaktischen Szenarien realisiert werden. Häufig diskutiert werden folgende Varianten:

Der Ausdruck CBT (Computer Based Training) bezeichnet die Arbeit mit Lernprogrammen (Lernsoftware), die vom Lernenden zeitlich und räumlich flexibel genutzt werden können und bei dem die Lernenden nicht in direktem Kontakt mit dem Lehrenden und anderen Lernenden stehen. Diese Programme können multimediale Lerninhalte (wie z. B.: Animationen oder Videodokumente) beinhalten und werden meist auf CD-ROM oder DVD vertrieben. Beim CBT handelt es sich um eine in erster Linie nichttutorielle Form des E-Learning, bei dem das Selbststudium im Vordergrund steht und die Kommunikation, wenn überhaupt, auf asynchrone Weise erfolgt. CBT existiert bereits seit den 1980er Jahren.
Für ältere computerunterstützte Lernsysteme existiert auch eine Vielzahl anderer Bezeichnungen. Bspw. sind das CAT (engl.: "Computer Aided Teaching"), CAI "(Computer Aided Instruction" bzw. "Computer Assisted Instruction)", CBI "(Computer Based Instruction)", CAL "(Computer Aided Learning" rsp. "Computer Assisted Learning)", CUL („Computerunterstütztes Lernen“), CUU („Computerunterstützter Unterricht“ oder „Computerunterstützte Unterweisung“), CBL ("Computer Based Learning"/„Computerbasiertes Lernen“), CBE "(Computer Based Education)", CGU („Computergestützter Unterricht“), RGU („Rechnergestützter Unterricht“), CUA („Computerunterstützte Ausbildung“) oder CUIV („Computerunterstütztes interaktives Video“). Andere im Deutschen häufige Bezeichnungen sind die Begriffe "Courseware" oder "Teachware".

Den grundlegenden Baustein netzbasierter Lernangebote bildet das sogenannte WBT (Web Based Training) oder (Webbasiertes Lernen) – eine Weiterentwicklung des CBT. Hierbei werden Lerneinheiten nicht auf einem Datenträger verbreitet, sondern von einem Webserver online mittels des Internets oder eines Intranets abgerufen. Die Einbettung ins Netz bietet vielfältige weiterführende Möglichkeiten der Kommunikation und Interaktion des Lernenden mit dem Dozenten/Tutor bzw. seinen Mitlernern. So können Mails, News, Chats und Diskussionsforen mit dem WBT verknüpft und Audio- und Videosignale live gestreamt werden. Eine Weiterentwicklung des WBT hin zu einer kommunikativeren Nutzung ist die Lernplattform. Diese unterstützt unterschiedliche Kommunikationsarten, wie z. B. Chat und Foren, um somit die Nutzer der Lernplattform beim direkten Austausch und Anwenden des Gelernten zu fördern.

Autorensysteme sind Entwicklungswerkzeuge für die Erstellung von digitalen Lernangeboten. Ihr Zweck besteht darin, Inhalte für ein Lernangebot zu erstellen und aufzubereiten. Sie bieten beispielsweise Dozenten die Möglichkeit, Inhaltsunterlagen für das Netz oder einen Datenträger, zum Beispiel CD-ROM zu entwickeln.
Es gibt leicht bedienbare Autorensysteme, so dass Autoren kein größeres Wissen über Programmierung, HTML, XML und Internet besitzen müssen. Diese einfachen Systeme empfehlen sich, wenn man Lernmaterialien für einen bestimmten Zweck produzieren will, d. h. nicht die Absicht hat, die Inhalte später für andere Kurse wiederzuverwenden.
Wenn man einmal produzierte Lerninhalte für verschiedene Kurse wiederverwenden möchte, empfehlen sich Autorensysteme, die Kurse und Medien z. B. in Form von Bibliotheken abspeichern, auf die ein oder mehrere Kursautoren zugreifen und aus denen sie Elemente in anderen E-Learning-Produkten verwenden können. Moderne Autorensysteme dieser Art arbeiten oft mit XML-Technologien (zum Beispiel zum Export aller im Kurs verwendeten Texte, um diese übersetzen zu lassen) und können an ein Dokumenten-Management-System angeschlossen werden.

Grob lassen sich Autorensysteme unterteilen in:


Einige Autorensysteme integrieren alle diese Funktionen. Dies bedeutet, diese Autorensysteme ermöglichen es dem Ersteller von Online-Kursen, verschiedene Medien in eine Lerneinheit zu integrieren, um professionellen, dynamischen und interaktiven Lerninhalt zu erstellen.

Die Kursnavigation und -steuerung wird in manchen Autorensystemen durch mehr oder weniger offenliegende Programmier- bzw. Scriptsprache angeboten (um das „Drehbuch“ definieren zu können).

Ein großer Teil der verfügbaren Autorensysteme war bis Anfang der 2010er Jahre darauf ausgelegt, Kurse für Festrechner und Laptops herzustellen. Mittlerweile unterstützen alle etablierten Autorensysteme auch die Entwicklung für Smartphones und Tablets, teilweise allerdings mit reduziertem Funktionsumfang. Es werden verschiedene Medien- und Dateitypen wie Text, Grafik, Video und Audio unterstützt. Manche Autorensysteme sind auf sogenannte Lernmanagementsysteme abgestimmt und ermöglichen es, ausschließlich für diese Systeme Inhalte zu erstellen. Daneben existieren Autorensysteme, die spezielle Standards, wie SCORM, AICC oder IMS Content Packaging unterstützen. Diese Standards können, neben anderen Möglichkeiten, mit Lernmanagementsystemen über eine genormte Schnittstelle kommunizieren und so zum Beispiel den Namen eines Lerners und dessen Lernfortschritt übermitteln und abspeichern.

Je einfacher ein Autorensystem zu bedienen ist, desto eingeschränkter ist man im Regelfall bei der Gestaltung der Inhalte. Systeme, die einem Ersteller große kreative Freiheiten lassen, sind oft sehr komplex und erfordern eine längere Einarbeitungszeit.

Simulationen sind Lösungen von Modellen, welche bedeutsame Eigenschaften der Realwelt abzubilden versuchen, um Lernenden durch freies oder gezieltes Experimentieren oder Beobachten Wissen über strukturelle oder funktionale Eigenschaften des Originals zu vermitteln. Komplizierte Sachverhalte und Prozesse der Wirklichkeit können so vereinfacht und auf das Wesentliche reduziert dargestellt werden, besonders dann, wenn Realexperimente zu teuer oder zu gefährlich sind.

Die Videokonferenz schafft virtuelle Hörsäle, indem sie räumlich verteilte Lernende und Vortragende miteinander kommunizieren lässt. Diese als Teleteaching bezeichnete Variante des E-Learnings ist in erster Linie durch die Übertragung von Bild und Ton gekennzeichnet. Sie ermöglicht eine der Präsenzlehre ähnliche Kommunikation zwischen Lehrenden und Lernenden, die auf verbale Äußerungen ebenso zurückgreifen kann wie auf Gestik und Mimik. Eingeschränkt wird das Teleteaching durch die relativ hohen technischen Anforderungen. Mit zunehmender Bandbreite der Internet-Verbindungen entwickeln sich aus dieser Technologie neue eVideo-Formate und global skalierbare Unterrichtsformen wie z. B. MOOC.

"Siehe Hauptartikel "Lernplattform

Als Lernplattform (engl. "Learning Management System", LMS) werden Systeme bezeichnet, die für das Online- und/oder Präsenz-Kursangebot den kompletten (oder Teile des) Arbeitsablauf des Veranstaltungsmanagements von Buchungsprozessen, Lehr- und Lernprozessen bis zur Ressourcenadministration unterstützen können.

Die Aufgaben eines LMS können im Einzelnen umfassen:


In manchen LMS ist eine Lehrer- und Raumverwaltung enthalten, die auch nachträglich Termine und Personen tauschen kann und ggf. Terminkonflikte meldet. Diese Funktionalitäten können recht umfangreich werden, z. B. bei integrierter Arbeitszeit-/Urlaubsverwaltung der Lehrer, Speicherung von Raumdaten (Anzahl von Plätzen, vorhandenen Ressourcen wie Beamern oder Tageslichtprojektoren), etc. Aus den vorhandenen Daten können später umfangreiche Berichte erstellt werden wie beispielsweise Raumbelegungsdaten, Stundenpläne oder Lernfortschritte einzelner Personen.

Die Aufgabe eines LCMS (Learning Content Management System) ist das Erstellen, Wiederverwenden, Auffinden, Nachbearbeiten und Ausliefern von Lerninhalten (Content). Der Content wird oft in einem zentralen Repository in Form von „reusable“ Lernobjekten (RLOs) vorgehalten. Objekte können aus mehreren verschiedenen Kursen referenziert werden, so dass im Falle einer Anpassung nur eine einmalige Änderung notwendig ist, um sämtliche Inkarnationen auf den aktuellen Stand zu bringen. Das LCMS verfügt (im Gegensatz zu Autorentools) über eine Userverwaltung, die es ermöglicht, verschiedenen Personen und Personengruppen bestimmte Rechte zuzuweisen, so dass z. B. für fachspezifische Experten, Mediengestalter, Projektadministratoren jeweils unterschiedliche Zugriffsfunktionen definiert/realisiert werden können.

Eine Multi-User-Funktionalität erlaubt es, konkurrierende Zugriffe zu verwalten, so dass es nicht dazu kommen kann, dass zwei Benutzer gleichzeitig (widersprüchliche) Änderungen am selben Objekt vornehmen können. Weiterhin verfügen LCMS in der Regel über eine Versionskontrolle, die es ermöglicht, vorgenommene Änderungen nachzuvollziehen.

Eine der wichtigsten Aufgaben eines LCMS ist die Unterstützung von wiederverwertbaren Lernobjekten (RLOs). Ziel ist es, ungewollte Redundanzen und widersprüchliche Informationen weitgehend zu verhindern.

Content-Kataloge unterstützen den Austausch von Lernobjekten – von kompletten Kursen bis hin zu Rohmaterialien. Bereitsteller können Angebotsbedingungen spezifizieren. Zugriffe werden dokumentiert und gegebenenfalls abgerechnet. Diese Kataloge können sehr spezifisch auf eine bestimmte Zielgruppe (Schule, Universität, Branche, Unternehmen) ausgerichtet sein. Siehe auch Content-Sharing als Form des E-Learnings.

Siehe Digitales Lernspiel.

Als Audience Response Systems werden technisch-elektronische Geräte bezeichnet, die im Rahmen von Lehrveranstaltungen oder bei Vorträgen mit zahlreichen Teilnehmern die Interaktivität zwischen Dozent (bzw. Referent) und den Zuhörern erhöhen soll. Der Einsatz solcher Systeme wird zumeist durch konkrete didaktische Konzepte geleitet und ist somit als Teilbereich des E-Learning zu verstehen.

Virtuelle Lehre bezeichnet Lehre, die vorrangig über das Internet durchgeführt wird und keine signifikanten Anteile von Präsenzlehre umfasst. Zum Spektrum der Darbietungsformen virtueller Lehre zählen Webinare, web-unterstützte Lehrbuchkurse, Hypertext-Kurse (z. B. mit Lehrtexten, Multimediaelementen, Animationen und Übungen), videobasierte Kurse (z. B. Vortrag samt Foliensatz) oder audiobasierte Kurse bzw. Podcasts. Da bei virtueller Lehre im Unterschied zu Präsenzlehre und „Blended Learning“ wenig Gelegenheit zu direkter Interaktion besteht, nutzen Lehrende und Studierende zur Kommunikation häufig elektronische Medien wie Chatrooms, Diskussionsforen, Voice Mail oder E-Mail oder spezielle Plattformen. Virtuelle Lehre spielt eine wachsende Rolle im Kontext der Einführung weiterbildender Online-Master-Studiengänge an zahlreichen Hochschulen.

Wenn die Vorteile von Präsenzveranstaltungen mit denen von virtueller Lehre verknüpft werden, spricht man von Blended Learning (dt. integriertes Lernen). Blended Learning verbindet dabei beide Lernformen in einem gemeinsamen Lehrplan (Curriculum). Blended Learning wird insbesondere dann eingesetzt, wenn neben reiner Wissensvermittlung auch die praktische Umsetzung trainiert werden soll (z. B.: im Arbeitsschutz).

Es gibt mittlerweile Webseiten, die es erlauben, Lerneinheiten auszutauschen. Solche Initiativen existieren als kommerzielle oder freie Angebote. Ein Beispiel für eine kommerzielle Initiative ist StuDocu. Hier werden monetäre Anreize zur Aufbereitung von Lehrinhalten gesetzt. Dadurch soll die Qualität der publizierten Mitschriften, Lernkarten oder Zusammenfassungen steigen. Als typisches Beispiel für ein nicht-kommerzielles Forum kann z. B. das Fachschaftsforum für Wirtschaftsingenieurwesen an der Uni Duisburg-Essen dienen (WiING-DU). Schon bei solchen lokalen Initiativen zeigt sich oft ein hoher Bedarf, wie die dokumentierten Zugriffszahlen von WiING-Du beispielhaft zeigen.

Personengruppen, die gleiche Ziele und/oder fachliche Interessen haben, können sich über ein Informations- und Kommunikationssystem eine gemeinsame Wissensbasis aufbauen. Jedes Mitglied dieser Learning Community kann sein eigenes Wissen einbringen und somit wird die Wissensbasis über gemeinsame Lernprozesse erweitert und angepasst. Viele Learning Communitys entstehen durch videobasierte Kurse. In vielen dieser Kurse können sich Mitglieder miteinander vernetzen und sich in einem geschützten Mitgliederbereich austauschen. 

Computer-Supported Cooperative Learning (CSCL) beschreibt Lernansätze, bei denen das kooperative Lernen durch den Einsatz von computergestützten Informations- und Kommunikationssystemen unterstützt wird.

Der Begriff Web Based Collaboration beschreibt die Zusammenarbeit einer Gruppe von Personen an einer Lernaufgabe über das Internet.

Beim Virtual Classroom dient das Internet als Kommunikationsmedium, um geographisch getrennte Schüler und Lehrer miteinander zu verbinden. Das virtuelle Klassenzimmer ermöglicht somit eine synchrone Form des Lernens.
Als eine extreme Form wird das Tele-Teaching betrachtet.

Ein interaktives Whiteboard ist vergleichbar mit einer Tafel oder einem Flipchart. Die Nutzer haben die Möglichkeit, über ein Netzwerk gemeinsam Skizzen zu erstellen und zu betrachten. Dazu stehen sowohl Zeichen-/Mal- als auch Textwerkzeuge zu Verfügung.

Business TV ist ein exakt auf die Zielgruppe zugeschnittenes Fernsehprogramm. Business TV stellt eine sehr wirkungsvolle Methode dar, um eine Gruppe (Mitarbeiter, Lieferanten und Kunden) zum Lernen anzuregen.

Beim Mikrolernen, auch Microlearning genannt, geht es um das Lernen in kleinschrittigen Lerneinheiten, häufig über Web oder Mobiltelefon. Der Überlastung durch zu viele Informationen soll durch benutzerfreundliches, flexibel einteilbares Training entgegengewirkt werden.

Hierbei handelt es sich um eine audiovisuelle Darstellung von Lehreinheiten mit dem Ziel, einen Erkenntnisprozess – welcher zu neuem Wissen führen kann – Schritt für Schritt in Film, Bild, Text und Ton aufzubereiten.

Zunehmend gewinnen 3D-Infrastrukturplattformen wie Second Life oder Twinity an Bedeutung für E-Learning-Anwendungen. Durch den Erlebnis-Charakter dieser virtuellen Welten wird ein sehr hoher Immersionsgrad erreicht. Hiervon verspricht man sich eine entsprechend höhere Lerneffizienz, da Spielen & Lernen zusammenwachsen. Durch simulierte Erlebniswelten kann man nun in Situationen eintauchen und diese erleben. Eine hohe Immersion (virtuelle Realität) wird u. a. dadurch erreicht, wenn eine hohe Identifikation des Nutzers mit seinem Avatar eintritt und der Nutzer sich als Teil der Welt fühlt. Somit wird sich auch der Begriff Action Learning durch den Begriff E-Action-Learning erweitern. Ein weiterer erheblicher Vorteil durch die Vernetzung von Team in virtuellen Welten ist dort zu sehen, wo Teams perfekt zusammenspielen müssen, bspw. bei Einsätzen der Polizei, Feuerwehr, Rettungsdienste usw. Die Beteiligten können sich nun von überall auf der Welt einloggen und Szenarien regelmäßig in einer virtuellen Welt durchspielen. Insbesondere Orientierungstrainings können hier in häufigeren und regelmäßigen Abständen durchgeführt werden, was die Effektivität und Effizienz solcher Einsätze erhöhen kann. Das Fremdsprachenlernen ist laut einem Mitarbeiter von Linden Lab die am weitesten verbreitete Form von Bildung in Second Life.

Noch vor wenigen Jahren galt E-Learning als die Bildungsform des 21. Jahrhunderts. Mittlerweile vermutet man, dass E-Learning die traditionellen Bildungsformen nicht ersetzen kann. Es ist lediglich als eine sinnvolle Unterstützung im Lernprozess zu sehen. Durch Kombination verschiedener medialer Vermittlungsformen („hybride Lernarrangements“) kann Lernen optimiert werden. Insbesondere Menschen, die lieber den PC und das Internet nutzen als Bücher lesen, können durch E-Learning viele Lerninhalte besser aufnehmen oder bereits bekannte Inhalte ergänzend und interaktiv erarbeiten. Zu weiteren Vorteilen des E-Learning gehören zweifellos die ökonomischen Aspekte. Die Lernenden sind räumlich und zeitlich unabhängig. Die Vermittlung von Lernstoffen kann also unabhängig von der persönlichen Anwesenheit geschehen. Dank PC und Internet ist die Verteilung größerer Informationsaspekte machbar. Gerade diese ökonomischen Vorteile sind nicht zu unterschätzen. Denn „life-long learning“ bedeutet meistens berufsbegleitendes Lernen, und da ist die Flexibilität hinsichtlich Ort und Zeit besonders wichtig.

Medien sind nur zu einem geringen Teil für den Lernerfolg ausschlaggebend, deshalb kann nicht per se von effizienterem Lernen durch E-Learning gesprochen werden. Erst wenn eine Vielzahl von Faktoren zusammenkommt, kann E-Learning erfolgreich sein (zu beachten sind etwa Erkenntnisse aus der Mediendidaktik bzw. Medienpädagogik).

Auf der Grundlage von Erkenntnissen der Mediendidaktik zeigen sich "Vorteile" u. a. darin, dass:


"Nachteile" werden unter anderem darin gesehen, dass:


Aktuelle Ansätze kombinieren daher E-Learning mit der Präsenzlehre, also der personalen Vermittlung. Diese Ansätze firmieren unter dem Begriff „Hybride Lernarrangements“ oder Blended Learning. Ziel dieser Ansätze ist es, die Vorteile des Präsenzunterrichts mit denen des mediengestützten Lernens zu verbinden und Nachteile beider Methoden zu vermeiden.

Während viele E-Learning-Konzepte sich nach wie vor an lineare Wissensvermittlung halten, wie sie von Büchern und anderen traditionellen Lehrmitteln bekannt sind, erlauben moderne E-Learning-Systeme "flexible und adaptive Strukturen," die mit einem gewissen Human Touch auf die Lernenden eingehen. Solchen Systemen liegen "netzwerkartige Dialogstrukturen" zugrunde. Der Vorteil solcher E-Learning-Modelle besteht darin, dass eine viel höhere Interaktivität über dem gesamten Lernprozess liegt. Oft reduziert sich dabei der Bedarf an Blended Learning (es sei denn, praktische Fähigkeiten – wie z. B. Gerätebedienungen oder soziale Kompetenzen – sind Gegenstand der Lernaktivität). Der Nachteil besteht im erhöhten Aufwand in der Notwendigkeit, variabel auf Ergebnisse von Lernkontrollen zu reagieren, und in der Notwendigkeit, zahlreiche Lern-, Vertiefungs- und Exkurspfade zu implementieren, die möglicherweise in der Praxis nur von wenigen Lernenden je beschritten werden.

Die rasante technologische Entwicklung der Computerindustrie und die damit verbundenen neuen Methoden für die Gestaltung von Inhalten (insb. Hypertext und Multimedia) förderte die Entwicklung sogenannter E-Learning-Programme in einem großen Ausmaß. In der Fachdiskussion ist man sich noch nicht einig, ob E-Learning im Sinne von E-Mail, E-Business usw. für Lernen im Internet oder als moderneres Synonym für CUL (Computer-unterstütztes Lernen) verwendet werden soll. Heute tendiert man sogar eher dazu, E-Learning als reine Ergänzung zur Präsenzlehre als Teil des sogenannten Blended Learning anzusehen. Zudem wird von verschiedenen Akteuren versucht, den eigentlichen Lerninhalten mehr Gewicht zu geben, da bisher vor allem die technologischen Aspekte im Vordergrund standen.

Alfons Rissberger fragte schon 1997 in einem Artikel der FAZ: „Verschlafen wir die multimediale Zukunft?“

Erfolgreiche Einführung von E-Learning erfordert für die jeweilige Institution eine Reihe begleitender Maßnahmen:


Robin Mason hat mit "Models of Online Courses" ein Vorgehen vorgeschlagen, wie E-Learning etappenweise eingeführt werden kann:


Wenn E-Learning effektiv und effizient betrieben werden soll, bedarf es in Bezug auf die Lehrpläne gewisser Modifikationen. Insbesondere gilt es, Schlüsselkompetenzen wie Medien-, Informations- und Computerkompetenz zu fördern. Diese Schlüsselkompetenzen werden idealerweise mit Hilfe von Informations- und Kommunikationstechnik (Werkzeugcharakter) erlernt.

Gilly Salmon (Salmon, 2000) unterscheidet fünf Stufen des Lernprozesses auf dem Weg vom E-Learning-Anfänger zum E-Learning-Profi:


Unternehmen können ihre Ausgaben für Personalentwicklung mittels E-Learning drastisch senken. Außerdem werden Prozess- und Fehlerkosten durch
qualifiziertes Personal, gesunkene Fehlerquoten und die Erhöhung der Effizienz durch den Einsatz modernerer Methoden reduziert. Gleichzeitig wird die Qualität durch einen einheitlichen Wissensstand aller Mitarbeiter gesteigert.

Ziele des E-Learnings in der Personalentwicklung sind unter anderem:

Notwendigkeit der Personalentwicklung


Kosteneffizienz


Die Konzeption der Lehrveranstaltungen bleibt in den meisten Fällen, in denen E-Learning eingesetzt wird, gleich. Es zeigt sich die Tendenz, gewohnte Lehr- und Lerninhalte in digitale Formate zu übersetzen. In Zukunft sollte gefragt werden, ob das Lernen und Lehren mittels Computer nicht ungewohnte Wege gehen sollte und somit neue Lernformen und Kontexte entstehen werden.

Auch die Organisation der Hochschulen könnte grundlegend überdacht werden, indem man ihre Vernetzung vorantreibt. Da sich die Lehr- und Lernsituationen meistens innerhalb der institutionellen Grenzen abspielen und diese Zugangsbeschränkungen unterliegen, führe dies dazu, dass Dozenten innerhalb einer Institution auf sich alleine gestellt sind. Aus der Sicht der Institution sei es die Aufgabe des Dozenten innerhalb eines Fachgebietes ein konsistentes Curriculum entstehen zu lassen. Der einzelne Dozent würde innerhalb seiner eigenen Institution keine Ansatzpunkte für eine eigene fachspezifische Entwicklung vorfinden, denn die Kommunikation findet außerhalb mit anderen Dozenten, welche auf der ganzen Welt verteilt sind, statt.

Der andere Punkt wäre, dass sich trotz horizontaler Vernetzung keine kollaborativen Arbeitsweisen an Hochschulen entwickeln. In der Zukunft könnten Kursinhalte gemeinsam erstellt und an allen Hochschulen angeboten werden. Durch das Wegfallen von Einzelkämpfern und die Hinwendung von einer vertikalen zu einer horizontalen Hochschulkultur würden nicht nur qualitative, sondern auch materielle Synergien entstehen. Der Diskurs und die Kritik müssen aber weiterhin ein zentraler Bestandteil der Wissenschaft bleiben.

Bis es soweit ist, muss man eine ganze Reihe von Problemen lösen, welche im organisatorischen, personellen, kulturellen und technischen Bereich angesiedelt sind.

Auch für die Studierenden wird E-Learning in den nächsten Jahren ein Umdenken erfordern. Die Gegenwart zeichnet sich durch medienvermittelte Informationen aus, welche einem erheblichen Einfluss auf privates und öffentliches Leben ausüben. Menschen als Wissensträger werden immer wichtiger und die Studierenden müssen mehr als bisher die Qualität vorhandener Informationen beurteilen können. Reines Faktenwissen wird an Wichtigkeit verlieren. Dagegen werden Grundlagenwissen und die damit einhergehende Beurteilungskompetenz und Verstehensprozesse immer wichtiger.

Neben einer reinen Vernetzung werden auch MOOC, sogenannte virtuelle Klassenräume immer interessanter für Hochschulen. Dabei können Studenten die Vorlesung von zu Hause oder unterwegs aus verfolgen. Der große Vorteil für die Hochschulen liegt darin, dass die Lehrveranstaltungen zunehmend entlastet werden und Studenten können verpasste Vorlesungen nachholen. Sie sind somit flexibler. An einigen Universitäten in Deutschland wird diese Art des E-Learnings bereits benutzt, zum Beispiel an der Technischen Universität und der Ludwig-Maximilians-Universität in München. Eine Studie der HIS zeigt, dass etwa 12 Prozent der Befragten angaben, dass an ihrer Hochschule virtuelle Seminare angeboten werden.

Mittlerweile ist unbestritten, dass der Erfolg von E-Learning durch den Einsatz von Tutoren verbessert wird. Trainer und Lehrer qualifizieren sich zunehmend zu Tele-Tutoren weiter, um Lernen über das Internet (Online-Lernen) zu ermöglichen. Abhängig vom Anbieter der Qualifizierung wird von Online-Tutoren, Tele-Tutoren, E-Trainern, E-Coaches, etc. gesprochen. In der Literatur wird vor allem von Tele- bzw. Online-Tutoren gesprochen (vgl.: Christina Rautenstrauch: "Tele-Tutoren"!).

Man kann drei unterschiedliche Anforderungsprofile unterscheiden:


Die Betreuung von Lernenden durch Tutoren ist in vielen Fällen für den Erfolg von E-Learning ganz entscheidend. Die Abhängigkeit des Lernenden von Online- oder Präsenz-Tutoren kann aber auch als hinderlich erlebt werden.

Dadurch dass Qualitätsaspekte betont werden und im Zuge dessen der Nachweis von Qualität sichernden Maßnahmen erbracht wird, findet E-Learning verstärkt Verbreitung und Anerkennung. Eine ausgeklügelte Multimedia-Präsentation garantiert noch lange nicht, dass Lernen zum Kinderspiel wird. Denn die Qualität des E-Learning wird sich auch durch noch so moderne Technik nicht automatisch steigern. Im Zuge dessen wird seit Beginn der 2000er Jahre mehr und mehr versucht Qualitätsstandards für E-Learning zu formulieren und diese Standards durch Anwendungsleitfäden und Easy-To-Use-Tools weiter zu verbreiten. Auch nationalen und internationalen Standardisierungsgremien (insb. DIN-Norm, ISO, IEC, CEN/ISSS) sind dabei an der Formulierung ein E-Learning Qualitätsstandards beteiligt, z. B. ISO/IEC 19796-1:2005, Informationstechnik - Lernen, Ausbilden und Weiterbilden - Qualitätsmanagement, -sicherung und -metriken. So können durch vergleichbare und allgemein verständliche Anforderungen und Kriterien die Bedürfnisse der Nutzer, Käufer und Anbieter besser aufeinander abgestimmt werden. Dabei ist es von großer Wichtigkeit Qualitätsentwicklung nicht nur als eine Beilage des E-Learning, z. B. in Form eines für sich stehenden Evaluationsansatzes am Ende eines Kurses, zu betrachten. Denn es handelt sich bei der Qualitätsentwicklung um einen Schlüsselaspekt, der bei der Entwicklung und Durchführung von E-Learning-Kursen und -programmen immer zum Tragen kommt. Um mehr Transparenz über die Qualität einer Lösung für die Anwender und um die Wahrnehmung der Qualität von E-Learning-Angeboten am Markt zu erhöhen gibt es einige Initiativen Gütesiegel für gutes E-Learning zu etablieren, z. B. das Gütesiegel des E-Learning Verbands vebn oder das eLearning-Label der Ruhr-Universität Bochum.

Mit dem Begriff Evaluation, der sich seit den 1970er Jahren im Bildungsbereich durchgesetzt hat, werden Dinge wie Qualitätskontrolle, Qualitätssicherung, Bewertung oder Wirkungskontrolle beschrieben. Da eine Vielzahl von multimedialen Lernangeboten am Markt miteinander konkurriert, ist dieser Qualitätsgedanke auch im E-Learning-Bereich bedeutsam.

Evaluation kann während des Entwicklungsprozesses als prozessbegleitende oder nach der Entwicklung als produktbewertende Evaluation durchgeführt werden.

Die prozessbegleitende, auch formative Evaluation dient der Beurteilung und Verbesserung des Programms während der Entwicklungsphase. Sie kann als schrittweise Optimierung des Gesamtproduktes gesehen werden, um Fehlentwicklungen vorzubeugen und das System optimal an die Bedürfnisse der Zielgruppe anzupassen.

Bei der produktbewertenden, auch summativen, Evaluation steht die abschließende Qualitätsbewertung im Vordergrund. Es gilt anhand verschiedener Evaluationskriterien das Ergebnis, den Erfolg oder den Nutzen der Maßnahme zu bewerten.

Das Spektrum der produktbewertenden Evaluationskriterien variiert dabei je nach Evaluationsfokus.
Mögliche Kriterien und deren Systematisierung:





Die Konzeption und Entwicklung von E-Learning-Angeboten erfordert vielfältige Kompetenzen im Schnittfeld zwischen Mediendidaktik, -informatik, -gestaltung und Betriebswirtschaft. Sie werden in konventionellen Studiengängen bislang selten vermittelt. Mit zunehmendem Interesse sowohl der Wirtschaft als auch von Bildungsinstitutionen an E-Learning Mitte der 1990er Jahre entstand eine Nachfrage nach „E-Learning-Experten“, die in der Lage sind, E-Learning-Angebote zu planen, umzusetzen und ein-/durchzuführen. Standen zunächst vor allem technische Kompetenzen im Vordergrund, rückten später konzeptionelle Kenntnisse und Fertigkeiten aus der Mediendidaktik in den Vordergrund.

Die Nachfrage wurde zunächst stark durch Quereinsteiger bedient, später durch Absolventen von z. B. Fachhochschul-Studiengängen, die Anfang und Mitte der 1990er Jahre verstärkt interdisziplinäre Studiengänge (etwa der Medieninformatik oder Informationsdesign) aufsetzten. Als Zusatzqualifikation entwickelten sich Weiterbildungsangebote als Zertifikatskurse oder Master-Studienprogramme. Sie richten sich an Personen, die bereits in dem Bereich tätig sind, oder in diesem Bereich tätig werden wollen. Bspw. gibt es an der "tele-akademie" der Hochschule Furtwangen seit 1998 den berufsbegleitenden Zertifikatskurs „Experte/Expertin für Neue Lerntechnologien“, der als "Blended Learning-Programm" angeboten wird. Seit 1999 wird in der Schweiz der tertiäre berufsbegleitende Masterstudiengang „E-Learning und Wissensmanagement“ angeboten.
Die Universität Duisburg-Essen bietet seit 2003 unter Leitung von Prof. Michael Kerres das modulare Studienprogramm Educational Media an, das online und berufsbegleitend studiert werden kann. Das akkreditierte Programm kann am Duisburg Learning Lab der Universität mit einem Zertifikat oder einem Master of Arts abgeschlossen werden. An der Universität Rostock gibt es seit 2004 den berufsbegleitenden Masterstudiengang „Medien und Bildung“. Die FernUniversität Hagen bietet darüber hinaus einen weiterbildenden Masterstudiengang „e-education“ an.

Fernunterricht im Sinne des FernUSG ist jede Vermittlung von Kenntnissen und Fähigkeiten:
(1) auf vertraglicher Grundlage,
(2) gegen Entgelt,
(3) die ausschließlich oder überwiegend über eine räumliche Distanz erfolgt, und
(4) bei der der Lehrende oder sein Beauftragter den Lernerfolg überwachen,
dann handelt es sich dabei laut Definition des Fernunterrichtsschutzgesetzes (FernUSG) von 1977 um Fernunterricht. Solche E-Learning-Angebote bedürfen in Deutschland einer Zulassung durch die Staatliche Zentralstelle für Fernunterricht (ZFU), bevor sie an den Markt gehen (im Oktober 2009, hat der Bundesgerichtshof entschieden, dass wenn eine Zulassung nicht vorliegt, dass dann gezahlte Kursentgelte zurückbezahlt werden müssen. Entscheidendes Merkmal: Die Lernerfolgsüberwachung kann in Form von Korrektur- und Prüfungsaufgaben sowohl während der häuslichen Selbstlernphase als auch während des begleitenden Unterrichts vorgenommen werden; ausreichend ist allerdings auch eine einmalige Abschlussprüfung nach Durchführung des Fernunterrichts. Zur Überwachung des Lernerfolgs ist es nach der Rechtsprechung ausreichend, dass der Lernende die Möglichkeit hat, durch mündliche Fragen zum erlernten Stoff eine individuelle Kontrolle des Lernerfolgs durch den Lehrenden oder seinen Beauftragten zu erhalten).

Die ZFU registriert auch nicht zulassungspflichtige Fernlehrgänge („Hobby-Lehrgänge“, die ausschließlich der Freizeitgestaltung dienen). Der Vertrieb dieser Lehrgänge ist der ZFU anzuzeigen. Die Entscheidung, ob es sich tatsächlich um einen „Hobby-Lehrgang“ handelt, liegt bei der ZFU. Die Fernunterrichtsverträge solcher Fernlehrgänge unterliegen ebenfalls dem FernUSG und werden von der ZFU geprüft. Hierbei handelt es sich um eine besondere Regelung für deutsche Anbieter (daneben, im Jahre 2000, hat die Europäische Union eine „Fernabsatzrichtlinie“ für alle Partnerländer vorgegeben, die ihr Vorbild im deutschen Fernunterrichtsschutzgesetz hat). 2005 waren von den 2097 staatlich zugelassenen Fernlehrgängen 632 – also: 31 % – als E-Learning-Kurse klassifiziert. Über 80 % aller Fernschulen unterstützen ihre Fernlehrgänge mittlerweile elektronisch. Damit verwischt die Grenze zwischen klassischem Fernunterricht und E-Learning.





</doc>
<doc id="8450" url="https://de.wikipedia.org/wiki?curid=8450" title="Elisabeth I.">
Elisabeth I.

Elisabeth I., , eigentlich "Elizabeth Tudor", auch bekannt unter den Namen "The Virgin Queen", "The Maiden Queen" („Die jungfräuliche Königin“), "Gloriana" oder "Good Queen Bess" (* 7. September 1533 in Greenwich; † 24. März 1603 in Richmond), war von 1558 bis an ihr Lebensende Königin von England.

Elisabeth war die Tochter von Heinrich VIII. und das fünfte und letzte Mitglied der Tudor-Dynastie auf dem englischen Thron. Ihre Mutter war Anne Boleyn. Ihre Regierungszeit als Königin von England und Irland von 1558 bis 1603 wird als "Elisabethanisches Zeitalter" bekannt. In jener Zeit erhielt die Anglikanische Kirche ihre endgültige Ausprägung, es entstanden zahlreiche künstlerische Werke von Dramatikern wie William Shakespeare, Christopher Marlowe oder Ben Jonson, Lyrik mit Sonetten und Liedgedichten, es wurden die modernen Wissenschaften mit Francis Bacon begründet und die Welt von Francis Drake umsegelt. Die erste englische Kolonie in Amerika wurde in dieser Zeit gegründet und zu ihren Ehren Virginia benannt.

Elisabeth Tudor wurde im Greenwich Palace an der Themse in der „Chamber of Virgins“ am 7. September 1533 zwischen drei und vier Uhr nachmittags geboren und nach ihren Großmüttern väterlicher- und mütterlicherseits, Elizabeth of York und Elizabeth Howard, benannt. Sie war die Tochter von Heinrich VIII. und Anne Boleyn. Elisabeth wurde allerdings nach der Hinrichtung ihrer Mutter für illegitim erklärt und zusammen mit ihrer älteren Halbschwester Maria von der Thronfolge ausgeschlossen, da Heinrich einen Sohn als Nachfolger wollte. Erst unter dem Einfluss von Heinrichs sechster und letzter Frau Catherine Parr reihte man sie, durch einen Parlamentsbeschluss, 1544 wieder in die Thronfolge ein.

Elisabeth war bei der Hinrichtung ihrer Mutter noch keine drei Jahre alt und lebte in eigener Hofhaltung. Die Gouvernante ihrer Kindheit war Katherine Champernowne. Zeitlebens sah Elisabeth in "Kat" ihre Ersatzmutter und enge Freundin. Ihr Tod 1565 traf Elisabeth.

In der Öffentlichkeit identifizierte sich Königin Elisabeth stets mit ihrem Vater Heinrich, doch spricht vieles dafür, dass sie privat auch das Andenken an ihre Mutter pflegte. So übernahm sie beispielsweise das Wappen ihrer Mutter als ihr eigenes und machte Anne Boleyns Kaplan Matthew Parker zum Erzbischof von Canterbury. Sie protegierte ihre Verwandten mütterlicherseits – eine ihrer engsten Freundinnen war ihre Cousine Catherine Carey, die Tochter von Anne Boleyns Schwester Mary, die als Zehnjährige Augenzeugin von Anne Boleyns Hinrichtung gewesen war – und trug einen Ring mit einer Kapsel, in der sich ein Doppelporträt von ihr und ihrer Mutter befand.

Nach Heinrichs Tod 1547 lebte sie am Hof von Catherine Parr. Als diese ihren Mann Thomas Seymour, der Elisabeth nachstellte, in einer eindeutigen Situation mit dieser ertappte, sah sie sich veranlasst, Elisabeth fortzuschicken. Nach dem Tod Catherines im September 1548 warb der ehrgeizige Thomas Seymour offiziell um die Hand der Prinzessin, der Staatsrat verbot jedoch eine Heirat. Es ist anzunehmen, dass Seymour beabsichtigte, seine Position durch die Hochzeit mit der Zweiten in der Thronfolge zu verbessern. Wegen seiner verschwörerischen Machenschaften gegen seinen Bruder Lordprotektor Edward Seymour, den Vormund des jungen Eduard VI. (1537–1553), wurde Thomas Seymour im Januar 1549 verhaftet und im Tower of London eingekerkert. Er wurde des Verrats für schuldig befunden und am 20. März 1549 hingerichtet.

Nach dem frühen Tod von Heinrichs Sohn und Thronfolger Eduard VI. im Alter von 15 Jahren folgte Jane Grey ihm auf den Thron. Auf dem Sterbebett hatte Eduard sie zur Nachfolgerin bestimmt und seine katholische Halbschwester Maria sowie Elisabeth selbst testamentarisch von der Thronfolge ausgeschlossen. Er wollte die protestantische Thronfolge sichern. Innerhalb von neun Tagen konnte Maria ihren rechtmäßigen Anspruch auf den englischen Thron durchsetzen. Am 3. August 1553 zog sie zusammen mit Elisabeth triumphierend in London ein. Schon bald kam es zum Zerwürfnis zwischen den Schwestern. Maria war eine überzeugte Katholikin und wollte die protestantische Elisabeth zu ihrem Glauben bekehren. Obwohl sich Elisabeth danach als Katholikin ausgab, signalisierte sie gegenüber den Protestanten, dass sie weiterhin heimlich Protestantin sei.

Als kurze Zeit später die Pläne der Königin zur Heirat mit dem spanischen Thronfolger Philipp II. von Spanien bekannt wurden, kam es zur Wyatt-Verschwörung. Philipp stieß bei den Engländern, die einen zu starken spanischen Einfluss fürchteten, auf große Ablehnung. Thomas Wyatt wollte Elisabeth mit ihrem Vetter Edward Courtenay, 1. Earl of Devon, verheiraten, um diesen an Stelle Marias auf den Thron zu erheben. Mit Hilfe von Folter brachte man Thomas Wyatt dazu, gegen sie auszusagen. Elisabeth wurde verdächtigt, zu den Verrätern Kontakt gehabt zu haben, und die Königin entschied auf Drängen des Abgesandten Kaiser Karls V., Simón Renard, und des Lordkanzlers Stephen Gardiner, Elisabeth in den Tower zu sperren. Dort wurde sie im Bell Tower untergebracht. Der Gang von dort zum Beauchamp Tower, in dem sie spazieren gehen durfte, wird noch "Elizabeth’s Walk" genannt. Die Legende will, dass sie sich dort mit dem im Beauchamp Tower eingesperrten Robert Dudley, Sohn des hingerichteten Herzogs von Northumberland, traf, den sie seit Kindertagen kannte.

Angesichts des Todesurteils gegen ihn widerrief Thomas Wyatt auf dem Schafott jede Beteiligung Elisabeths am Komplott. Dennoch blieb sie noch in Haft. Die weiteren Untersuchungen brachten keine Ergebnisse. Sie wurde aus dem Tower entlassen, nach Woodstock in Oxfordshire gebracht und dort unter Hausarrest gestellt.

Wenig später heiratete Maria den spanischen Kronprinzen Philipp. Ein Kind der beiden hätte die katholische Thronfolge in England gesichert. Maria hatte einige Scheinschwangerschaften, die ihre Gesundheit schwächten. 1558 starb sie kinderlos, vermutlich an Unterleibskrebs.

Am 17. November 1558 wurde Elisabeth die Nachricht vom Tode ihrer Halbschwester überbracht. An diesem Tag war es sommerlich warm, was als gutes Zeichen für ihre kommende Regentschaft galt. Philipp, nach der Abdankung seines Vaters Karls V. inzwischen König von Spanien, machte Elisabeth mehrere Heiratsanträge, welche sie jedoch als „unschicklich“ ausschlug. Am 15. Januar 1559 wurde sie im Alter von fünfundzwanzig Jahren in der Westminster Abbey zur Königin von England und Irland gekrönt.

Zur Zeit von Elisabeths Thronbesteigung war die Lage in England sehr angespannt. Die Wirtschaft lag am Boden, das Land befand sich im Krieg mit Frankreich und wurde außerdem von Glaubensfragen zerrissen. Elisabeth machte sich zuerst daran, den von ihrer Schwester wieder eingeführten Katholizismus zurückzudrängen. 1559 führte sie mittels der Uniformitätsakte den verpflichtenden Gebrauch des Book of Common Prayer in den Gottesdiensten ein. Im selben Jahr erneuerte die Königin die Suprematsakte Heinrichs VIII. und unterstellte so abermals die Kirche Englands der Krone; fortan war das englische Staatsoberhaupt „Oberster Gouverneur der Kirche von England“ ("Supreme Governor of the Church of England"). 1563 wurden die 39 Anglikanischen Artikel verabschiedet, die gemäßigt reformatorisch formuliert waren. Damit trennte sich Elisabeth endgültig von der katholischen Kirche. Es ist jedoch festzuhalten, dass sie nicht in den im Zeitalter der Glaubenskriege oft praktizierten religiösen Fanatismus verfiel.

Der Krieg mit Frankreich wurde am 3. April 1559 im Frieden von Cateau-Cambrésis beigelegt. Noch in der Regierungszeit Marias war im Januar 1558 Calais, Englands letzte Bastion auf dem Festland, an Frankreich gefallen. Nach dem Misslingen der militärischen Bestrebungen entschied Elisabeth, keine teuren Kriege mehr zu unterstützen, und gab 1564 die englischen Ansprüche auf Calais gegen eine finanzielle Entschädigung auf. Damit hatte England endgültig die letzten Stellungen auf dem Festland aufgegeben. Der Friedensvertrag mit Frankreich erlaubte dem englischen Staat endlich, seine Schulden zu bezahlen. Nun war die Voraussetzung für eine Genesung der angeschlagenen englischen Wirtschaft geschaffen.

Elisabeths Großvater Heinrich VII. (regierend 1485–1509) hatte die englische Handelsmarine gegründet, ihr Vater Heinrich VIII. die englische Kriegsmarine, indem er die englischen Schiffe mit weitreichenden Kanonen ausrüsten ließ. Elisabeths Marineschatzmeister Sir John Hawkins (1532–1595) konnte die Marine noch weiter verstärken. Das Land entwickelte sich zusehends zur Seemacht. 1566 wurde in London die erste Börse eröffnet, und es wurden diverse Wirtschaftsgesetze verabschiedet, woraufhin sich die Preise stabilisierten. Um den Handel anzutreiben, wurden neue Gesetze erlassen. So durften Engländer nur englische Filzhüte tragen, um die französische Konkurrenz auszuschalten.

Ihr Jugendfreund Robert Dudley spielte eine große Rolle in Bezug auf die Frage einer Ehe. Zwar ist heute nicht mehr zu klären, ob – wie die Zeitgenossen vermuteten – Robert Dudley tatsächlich der Liebhaber der „jungfräulichen Königin“ war. Sicher ist jedoch, dass Elisabeth in ihn verliebt war und die beiden den Eindruck eines Liebespaares machten. Da Dudleys Ehefrau Amy Robsart offenbar krank war, wurde bald über eine mögliche Heirat Elisabeths mit "Lord Robert" spekuliert. Die Berater der Königin wie William Cecil sowie einige Vertreter des Hochadels waren jedoch entschieden gegen eine solche Verbindung, da sie fürchteten, Dudleys großer Einfluss könne weiter steigen. Auch wurde er als Hindernis für eine Ehe Elisabeths mit einem ausländischen Prinzen (wie Erzherzog Karl) wahrgenommen.

Im September 1560 wurde Dudleys Frau tot am Fuße einer Treppe in ihrem Haus aufgefunden. Obwohl die gerichtliche Untersuchung einen Unfall festgestellt hatte, kursierten überall Gerüchte, Dudley habe seine Frau ermorden lassen, um die Königin heiraten zu können. Dieser Skandal machte Heiratspläne zunächst unmöglich. Tatsächlich wurde auch die Möglichkeit eines Selbstmordes nicht ausgeschlossen, wusste Amy Robsart doch von der Untreue ihres Mannes. Außerdem litt sie möglicherweise an Brustkrebs.

Im Oktober 1562 erkrankte Elisabeth schwer an Pocken. Man fürchtete um ihr Leben, und es brach eine lebhafte Diskussion um mögliche Nachfolgekandidaten unter den Politikern aus. Hierbei offenbarte sich eine gefährliche Situation: Elisabeth war die letzte noch lebende Tudor und immer noch unverheiratet und kinderlos, so dass die Nachfolgefrage völlig offen war. Als sie kurz aus dem Koma erwachte, ernannte sie Robert Dudley zu ihrem Nachfolger. Er sollte als Lordprotektor England regieren. Zur allgemeinen Erleichterung erholte sich Elisabeth wieder von der Krankheit. Das Ober- und Unterhaus legten ihr eine Petition vor, die ihr antrug, endlich zu heiraten. Elisabeth antwortete stets ausweichend.

Die Freundschaft zwischen Dudley und Elisabeth dauerte bis zu seinem Tode im September 1588 an und konnte auch durch ihren Zorn über seine Heirat mit ihrer Cousine zweiten Grades Lettice Knollys 1579 nur vorübergehend getrübt werden. Lettice wurde von Elisabeth allerdings mit lebenslangem Hass verfolgt.

Im Frühjahr 1563 schlug Elisabeth Robert Dudley als Gemahl für Maria Stuart vor, um einen für England günstigen Kandidaten auf den schottischen Thron zu bringen. Dudley wurde deshalb 1564 zum Earl of Leicester erhoben. Maria Stuart zeigte zunächst wenig Interesse, nachdem Elisabeth ihr jedoch die öffentliche Zusicherung der englischen Thronfolge versprach, für den Fall und nur für den Fall, dass sie Robert Dudley heiratete, stimmte sie schließlich Anfang 1565 dem Angebot zu. Lediglich Elisabeth selbst hatte wieder Bedenken, sich in der Thronfolge festzulegen. Als die Heirat Marias mit Henry Stuart, Lord Darnley, drohte, erneuerte Elisabeth ihr Angebot zu denselben Bedingungen. Robert Dudley selbst lehnte allerdings von vornherein und durchgängig eine Verbindung mit Maria Stuart ab, so dass das Projekt daran scheiterte.

Infolge eines Aufstands eines Großteils der schottischen Lords war die schottische Königin Maria Stuart im Sommer 1567 auf Loch Leven Castle gefangengesetzt und zur Abdankung gezwungen worden. Am 2. Mai 1568 gelang ihr der Ausbruch, doch nachdem die Armee ihrer noch getreuen Lehnsmänner am 13. Mai vernichtend geschlagen worden war, flüchtete sie über die Grenze nach England und ersuchte dort Elisabeth um Unterstützung gegen die rebellierenden schottischen Adligen. Das brachte Elisabeth in äußerste politische Bedrängnis. Da die Ehe ihres Vaters mit Anne Boleyn nie vom Papst legitimiert worden war, sah sich die katholische Maria Stuart als rechtmäßige Königin von England. Sie war die Urenkelin Heinrichs VII. und hatte ihren Anspruch auf den englischen Thron noch nicht aufgegeben. Elisabeth ließ die ehemalige schottische Königin am 19. Mai 1568 verhaften. Die ehrenvolle Gefangenschaft rechtfertigte sie mit dem schweren Verdacht der Mittäterschaft Marias am Mord an deren Gatten Lord Henry Darnley. Trotz der Haft wurde ihr der Luxus eines Hoflebens mit Gefolge gestattet.

Elisabeth ordnete eine Untersuchung an, die zwischen Oktober 1568 und Januar 1569 in York vorgenommen wurde. Die schottischen Lords, die als Ankläger auftraten, brachten als Beweismittel die Kassettenbriefe mit, die Maria angeblich vor Lord Darnleys Tod geschrieben hatte und die ihre Schuld beweisen sollten. Maria erklärte die Briefe für Fälschungen. Die Untersuchung erkannte die Briefe zwar für echt, jedoch wünschte Elisabeth weder eine Verurteilung noch einen Freispruch Marias, da sie das zu eindeutigen politischen Entscheidungen gezwungen hätte. So entstand offiziell kein Ergebnis, und Elisabeth behielt Maria weiter in Haft.

Am 25. Februar 1570 wurde Elisabeth von Papst Pius V. (1566–1572) mit der päpstlichen Bulle "Regnans in Excelsis" exkommuniziert. In dieser Bulle wurde Elisabeth das Recht auf den englischen Thron abgesprochen und englischen Katholiken mit der Exkommunikation gedroht, falls sie Elisabeth weiterhin die Untertanentreue hielten. Daraufhin kam es zur Ridolfi-Verschwörung: Elisabeth sollte ermordet und durch Maria Stuart ersetzt werden, dies alles mit Unterstützung spanischer und französischer Truppen. Verwickelt in die Verschwörung war neben Maria Stuart Thomas Howard, 4. Duke of Norfolk, der Maria heiraten wollte. Am 7. September 1571 wurde der Herzog von Norfolk, der einzige Herzog von England, festgenommen und nach langem Zögern Elisabeths im Juni 1572 hingerichtet. Das englische Parlament verlangte die Hinrichtung Marias, diese wurde im Oktober 1586 der Beteiligung an der Verschwörung angeklagt und am 25. Oktober zum Tode verurteilt. Elisabeth unterzeichnete den Hinrichtungsbefehl am 1. Februar 1587. Am 8. Februar wurde Maria hingerichtet. Hinter der Aufklärung der Verschwörung stand der englische Geheimdienstler Francis Walsingham, der damit seine spätere Position begründete.

In allen wichtigen Fragen besprach sich Elisabeth mit William Cecil. Er war ihr Berater von ihrer frühen Jugend an bis zu seinem Tod im Jahr 1598. Im Jahr 1571 erhielt er von Elisabeth den Titel Marquess of Exeter. 1572 wurde er zum Lord High Treasurer befördert; seinen alten Posten als Chefsekretär erhielt Francis Walsingham. Walsingham nutzte seine neue Stellung, um den Geheimdienst Englands weiter auszubauen. Er gilt als der Erfinder der modernen Spionage.

Im selben Jahr versuchte Elisabeth England durch eine Allianz mit Frankreich gegen einen Angriff Spaniens abzusichern. Das Bündnis kam zwar zustande, wurde jedoch durch die Ereignisse in der Bartholomäusnacht von 1572 schwer belastet. In dieser Nacht wurden zwischen 3000 und 10.000 Protestanten in Paris ermordet. Um das Verhältnis mit Frankreich wieder zu verbessern, begann Elisabeth 1581 Heiratsverhandlungen mit François Hercule de Valois, Herzog von Alençon, dem jüngeren Bruder des Königs von Frankreich, Heinrich III. Beide unterhielten eine enge Beziehung, in der Elisabeth ihm den Spitznamen „mein Frosch“ gab. Bei den Engländern stieß François jedoch auf große Ablehnung. Der Herzog starb 1584 – noch bevor eine Heirat stattfinden konnte. Dabei bleibt zweifelhaft, ob eine solche Verbindung jemals stattgefunden hätte, da dies für Elisabeth bedeutet hätte, nicht nur ihre eigene, sondern auch Englands Unabhängigkeit aufzugeben. Zeit ihres Lebens stand ihr das Beispiel ihrer Schwester vor Augen, deren Ansehen in England durch die Verbindung mit Spanien stark beschädigt worden war.

In diesen Jahren wuchs der Reichtum des Landes neben Handel nicht zuletzt auch durch Schmuggel und Raubzüge wie die des englischen Kapitäns Francis Drake. Drake wurde zum Volkshelden, als es ihm von 1577 bis 1580 als zweitem nach Magellan gelang, die Welt zu umsegeln. Drake war einer der Hauptakteure im von Elisabeth unterstützten Seekrieg gegen Spanien – der jedoch nie offiziell erklärt wurde. Es gelang ihm mehrfach, eine immense Menge des spanischen Kolonialreichtums nach England zu schaffen. Ein großer Teil der Beute ging an das englische Königshaus und einige Adlige, die sich als Kreditgeber an den Unternehmungen beteiligten.

1580 kam Walter Raleigh als Experte für irische Fragen an den englischen Hof und erwarb sich die Gunst der Königin. 1585 organisierte Raleigh eine Expedition nach Amerika. In deren Zuge wurde auf Roanoke Island die erste englische Kolonie in Amerika gegründet. Das Land nannte Raleigh zu Ehren der Königin Virginia. Wegen des drohenden Krieges gegen Spanien musste die Kolonie aufgegeben werden. Raleigh wurde zum Ritter geschlagen und damit zu einer der einflussreichsten Persönlichkeiten Englands. Eine dauerhaft besiedelte Kolonie zu etablieren, gelang in der Regierungszeit Elisabeths nicht mehr. Die Engländer mussten bis zur Herrschaft von Jakob I. (1566–1625) warten, um den Krieg gegen Spanien 1604 durch den Vertrag von London beenden und ihre erste Kolonie 1607 bei Jamestown gründen zu können. Eine Anekdote über Raleigh besagt, er sei der Königin so ergeben gewesen, dass er seinen Mantel über eine Pfütze legte, damit sie darüber schreiten konnte. Ob dies auf Wahrheit beruht, ist umstritten.

1586 wurde Maria Stuart wegen der „Babington-Verschwörung“ angeklagt. Auch diese hatte wie die Ridolfi-Verschwörung 1570 das Ziel, Elisabeth zu ermorden und Maria Stuart auf den englischen Thron zu bringen. Marias Beteiligung konnte schlüssig durch von ihr verfasste Briefe nachgewiesen werden. Im Oktober beschlossen Ober- und Unterhaus gemeinsam Marias Todesurteil. Elisabeth ließ im Parlament nachfragen, ob es keine andere Möglichkeit gebe, als Maria zu töten. Ihr Unbehagen vor diesem letzten Schritt und ihr jahrelanges Zögern erklären sich durch ihren tiefen Glauben an das göttliche Recht eines Monarchen und dementsprechend durch die Auffassung des Königsmords als Verstoß gegen die göttliche Ordnung. Das Parlament überzeugte sie jedoch davon, dass die ständige Bedrohung, die Maria Stuart als Galionsfigur der katholischen Opposition darstellte, nur mit ihrem Tode zu beenden war, und das Todesurteil wurde offiziell in London verkündet. Am 8. Februar 1587 wurde Maria auf Fotheringhay Castle enthauptet.

Die Raubzüge der englischen Freibeuter und die Hinrichtung Maria Stuarts gaben dem spanischen König Philipp II. den Anlass, den Krieg gegen England weiter zu verstärken und eine Invasion Englands zu planen. Doch während Philipp eine große Seestreitmacht ausrüstete, schlug Francis Drake Elisabeth ein Unternehmen vor, das auch als "das Ansengen des Bartes des Königs von Spanien (singeing of the King of Spain’s beard)" bekannt wurde. Es bestand darin, dass Drake mit seiner Flotte in den Heimathafen der spanischen Schiffe einlief, um diese dort zu zerstören. Am 2. April 1587 stach er in See. Das Unternehmen war ein voller Erfolg, 20 bis 30 Schiffe wurden versenkt oder als Prise genommen. Die Kriegspläne Spaniens mussten verschoben werden, um die beschädigte Flotte wiederaufzurüsten. Dies verschaffte England Zeit, die eigene Marine auszubauen.

Anfang April 1588 sandte Philipp II. die Spanische Armada (insgesamt 130 Schiffe) zur Invasion Englands aus. Die Armada sollte in den Niederlanden eine spanische Invasionsarmee unter Alessandro Farnese, Herzog von Parma und einer von Philipps besten Generälen, an Bord nehmen und nach England übersetzen. Der Plan war es, zuerst auf der Isle of Wight einen Stützpunkt zu errichten. Doch am 19. Juli sichteten englische Späher die Armada vor der Küste von Plymouth, und die englische Flotte unter Francis Drake und Sir Charles Howard, dem Earl of Nottingham, konnte die Spanier im Ärmelkanal abfangen. Da der Plan, einen vorgeschobenen Truppenstützpunkt zu etablieren, misslungen war, machten die Spanier ihre Flotte nahe der französischen Stadt Calais fest. Der Oberbefehlshaber, der Herzog von Medina Sidonia, sollte hier die Armee des Herzogs von Parma an Bord nehmen. In England wurde Robert Dudley, 1. Earl of Leicester, zum Oberkommandierenden der Landtruppen ernannt. Er organisierte eine Landstreitmacht entlang der Themsemündung, um dort der Invasion Widerstand zu leisten. Auf seinen Vorschlag hin traf Elisabeth am 8. August bei ihren Truppen ein und hielt ihre berühmte Tilbury-Rede, die einen stürmischen Jubel der Soldaten auslöste.

<poem style="margin-left:5em;">
Ich weiß, dass ich zwar den Leib eines schwachen kraftlosen Weibes,
dafür aber Herz und Mark eines Königs,
noch dazu eines Königs von England habe.
</poem>

Obwohl man stündlich die Truppen der Spanier erwartete, blieb Elisabeth im Lager der Truppen. In Calais ließ Drake eine Gruppe von Brandern in die Richtung der Spanier segeln. Die Spanier mussten ihre Anker lichten, um den brennenden Schiffen zu entgehen, deshalb konnten die englischen Verteidiger gegen die Spanier kämpfen. Weil die Engländer leichtere und schnellere Schiffe hatten, war es ihnen möglich, den spanischen Schiffen mehr Schaden zuzufügen als umgekehrt. Die Seeschlacht von Gravelines ging so im Großen und Ganzen unentschieden aus. Da die Spanier ihre Anker gelichtet hatten und deshalb die Truppen des Herzogs von Parma nicht transportieren konnten, entschied der Herzog von Medina Sidonia, um die Küsten von Schottland und Irland herum zu segeln, um auf diesem Weg wieder nach Spanien und Portugal zu kommen.

Auf dem Weg dorthin gerieten die Spanier allerdings in einen schweren Sturm, durch den fast 60 der 130 spanischen Schiffe an der Küste Irlands strandeten und sanken. Kaum die Hälfte der spanischen Schiffe und nur ein Drittel der Mannschaft erreichte die Heimat. Währenddessen starben unter den Engländern viele Soldaten hauptsächlich durch Krankheiten wie Dysenterie und Flecktyphus. Als Elisabeth das Lager in Tilbury verließ, war die Armada geschlagen und die Gefahr einer Invasion gebannt. In einem Triumphzug kehrte sie nach London zurück. Ihr alter Freund Robert Dudley starb jedoch kurz nach den Ereignissen in Tilbury, was ein schwerer Schlag für sie war.

Obwohl die Spanier mit ihrer Armada einen schweren Verlust erlitten hatten, war diese Schlacht noch nicht entscheidend, weil die Spanier ihre wichtigsten atlantischen Schiffe, die die Grundlage des spanischen Amerikareichs bildeten, gerettet hatten. Eine große englische Invasionsflotte segelte deshalb 1589 nach Spanien und Portugal, um die übrigen spanischen Marineschiffe zu versenken, Philipp aus Portugal zu vertreiben und die spanische Silberflotte abzufangen. Diese englische Flotte, die von Francis Drake und Sir John Norris geführt wurde, war erfolglos, tausende Soldaten starben dabei an schweren Krankheiten. Deshalb fiel der englische Invasionsversuch aus und Philipp baute die spanische Marine wieder auf. Danach war die spanische Marine viel stärker, als sie es während der 1580er Jahre gewesen war. Die Spanier transportierten dreimal mehr Silber und besiegten die Engländer bei einigen Gelegenheiten. So schlug 1595 ein neuer Raubzug in der Karibik fehl, auf dem Drake und Hawkins starben. Im selben Jahr landeten spanische Truppen unter Don Carlos de Amésquita erfolgreich bei Penzance in Südwestengland, eroberten einige Städte, setzten sie in Brand und zogen sich wieder aufs Meer zurück. Dies bewies, dass die Spanier den Kampf gegen England noch nicht aufgegeben hatten.

1593 begann Hugh O’Neill in Irland einen blutigen Kampf gegen die englischen Besatzer. Der Krieg, der daraufhin losbrach, war extrem brutal, teuer und für die Engländer äußerst verlustreich, wodurch Elisabeths Ansehen unter der Bevölkerung litt. Er war zudem die Ursache dafür, dass sich der englische Staat wieder verschuldete, weshalb Elisabeth eine Menge Kronbesitz und viele Regierungsstellen verkaufen musste. Aus demselben Grund war es notwendig, das englische Parlament öfter zusammenzurufen. Diese Änderungen stärkten die Volksvertretung und sorgten dafür, dass diese Institution die englischen Könige im 17. Jahrhundert besser herausfordern konnte. So kam es Mitte des 17. Jahrhunderts schließlich zur Enthauptung von Charles I. und einer 17-jährigen Herrschaft des Parlaments unter Oliver Cromwell.

Robert Devereux, 2. Earl of Essex, war der Sohn von Lettice Knollys, der späteren Ehefrau Robert Dudleys. Ab 1588 bekleidete er das Amt des Oberstallmeisters, im folgenden Jahr wurde er zum Ritter des Hosenbandordens geschlagen. Obwohl er als Liebhaber der Königin galt, heiratete er Anfang 1590 Frances Walsingham, die Tochter Sir Francis Walsinghams, ohne die Königin um Erlaubnis zu fragen. Nach mehreren verlustreichen Schlachten in Irland und einem für England nachteiligen Waffenstillstand wurde er zurückgerufen. Bei der Königin fiel er in Ungnade und wurde unter Hausarrest gestellt. Dort bereitete der bei der Bevölkerung populäre Devereux seinen Staatsstreich vor. Am 7. Februar 1601 wurde ihm mitgeteilt, dass er am Hof dem Privy Council vorsprechen müsse. Devereux weigerte sich und gab das Signal zum Aufstand. Am Vormittag des 8. Februar 1601 hatten sich mehrere hundert seiner Anhänger versammelt. Sein Versuch, mit einem Staatsstreich die Kontrolle über die Stadt London zu bekommen, schlug fehl und seine Gefolgsleute flohen in ihre Häuser, als sich die königlichen Truppen näherten. Robert Devereux wurde verhaftet, wegen Verrats zum Tode verurteilt und am 25. Februar 1601 im Alter von 35 Jahren im Tower hingerichtet.
Die Auseinandersetzungen in Irland konnten 1603 beendet werden. Die Königin wurde im Februar schwer krank. Sie litt an Schwäche und Schlaflosigkeit und starb am 24. März 1603 im Alter von 69 Jahren in Richmond. 

Noch am selben Tag wurde ihr Leichnam in den Palace of Whitehall überführt und dort aufgebahrt. Ihr Begräbnis fand am 28. April 1603 statt. Ein Trauerzug begleitete den Sarg der Königin zur Westminster Abbey, wo sie im Grabmal ihres Großvaters Heinrich VII. unter dem Altar beigesetzt wurde. 

Elisabeths Nachfolger wurde König Jakob VI. von Schottland, der Sohn Maria Stuarts und Urenkel der Schwester Heinrichs VIII., Margaret Tudor. Er wurde nur wenige Stunden nach Elisabeths Tod zum König von England ausgerufen, nachdem die Frage der Thronfolge zuvor umstritten gewesen war. Er nannte sich fortan Jakob I. von England und Schottland und vereinigte damit als erster englischer König die beiden Königreiche England und Schottland. Seinen Anspruch auf den englischen Thron versuchte er durch die Betonung seiner direkten Abstammung von Heinrich VII. zu legitimieren und inszenierte Elisabeth als Letzte eines ausgestorbenen Geschlechts. Den Sarg seiner Mutter Maria Stuart ließ er von der Kathedrale von Peterborough nach Westminster Abbey überführen, wo sie in einem prächtigen Grabmal im rechten Seitenschiff begraben wurde.

Im Zuge dessen ließ er ein Grabmal für Elisabeth im linken Seitenschiff von Westminster Abbey an der Stelle des bisher nicht markierten Grabs ihrer Halbschwester Maria errichten, wo Elisabeth nach der Fertigstellung des Grabdenkmals im Jahr 1606 ein zweites Mal beigesetzt wurde. Auf dem Grabdenkmal selbst findet sich allerdings nur eine Darstellung von Elisabeth, Maria wird lediglich in der Grabinschrift genannt.

Die lateinische Inschrift auf ihrem Grabdenkmal lautet:
Inschrift auf der Kopfseite: 

Inschrift auf der Fußseite: 

Trotz der verlustreichen Kriege gegen Spanien und Irland sowie Englands Wirtschaftsproblemen in den letzten, schwierigen Jahren ihrer Herrschaft blühte England in ihrer 44-jährigen Regentschaft auf. Die spanische Marine blieb bis 1650 die stärkste Seestreitkraft, doch die englische Marine holte mit der Weltumsegelung des Francis Drake und der gewonnenen Schlacht gegen die spanische Armada auf. 

Das aufsteigende englische Bürgertum machte seine Ansprüche in Politik und Kultur geltend. Elisabeth schaffte es, die eigene Kirche vom Einfluss Roms gelöst zu halten und somit die religiösen Wirren im Land zu beenden. Die Widersprüche innerhalb der protestantischen Kirche verschärfte sie allerdings, da sie mit äußerster Härte gegen puritanische Bestrebungen vorging und somit die Puritaner zunehmend in den Untergrund drängte.

Elisabeth förderte Musik, bildende Kunst und Literatur. Sie sprach sechs Sprachen fließend, musizierte und übersetzte antike Philosophen. Es war die Zeit von William Shakespeare und Christopher Marlowe, des elisabethanischen Theaters und des Francis Bacon. Das verarmte und religiös zersplitterte Land wurde selbstbewusst und erstarkte wirtschaftlich, so wird die Epoche „Elisabethanisches Zeitalter“ genannt.

Als politische Berater ragten vor allem William Cecil, 1. Baron Burghley, und Robert Dudley, 1. Earl of Leicester, in späteren Jahren zusammen mit Francis Walsingham heraus; im letzten Jahrzehnt Elisabeths rivalisierten Robert Devereux, 2. Earl of Essex, und Robert Cecil, der Sohn William Cecils, um die Macht. In ihren letzten Jahren führte der jüngere Cecil die Staatsgeschäfte.

Elisabeth soll von ausgeprägter Koketterie und Eitelkeit gewesen sein, berüchtigt dafür, ihre Laune in Sekunden zu ändern und heftig zu fluchen. Sie verglich sich oft mit ihrem Vater Heinrich; über ihre verstoßene Mutter Anne Boleyn sprach sie zeitlebens nie.

Heimliche Eheschließungen an ihrem Hof ahndete sie meist mit Härte. Von ihren Hoffräulein verlangte sie, . Der unverheiratet gebliebenen Königin wurden außer Leicester eine Reihe weiterer Liebhaber nachgesagt: François Hercule de Valois, mit dem sie um 1579 Heiratsverhandlungen führte; Robert Devereux; Christopher Hatton; Thomas Heneage; Edward de Vere, 17. Earl of Oxford; Walter Raleigh. Am Anfang ihrer Herrschaft äußerte sie, dass sie zufrieden sein werde, als Jungfrau gelebt zu haben und begraben zu werden. Zur Legende Elisabeths trug ihre letzte Ansprache, die Goldene Rede, bei. Seit etwa 1578 wurde sie aus politischen Erwägungen zur „Virgin Queen“ stilisiert, zur jungfräulichen Königin. Es entstand im Rahmen einer protestantischen Weltpolitik ein regelrechter Kult um ihre Gestalt, mit Turnieren, Dichtungen, symbolhaltigen Porträts. Auch weitere Beinamen kamen auf: Gloriana, Astraea, Cynthia, Belphoebe.

Andererseits wiederum vergab Elisabeth gern Spitznamen. Ihren langjährigen Freund Robert Dudley nannte sie . Ihr Berater William Cecil, 1. Baron Burghley, hingegen war . Williams Sohn und Nachfolger, Robert Cecil, wurde zum wenig schmeichelhaften . Sir Christopher Hatton wurde von ihr genannt. Der Leiter des Geheimdienstes Francis Walsingham war . Der Seefahrer Sir Walter Raleigh wurde ("water") und . Der französische Heiratskandidat François Hercule de Valois wurde genannt, sein Begleiter Jean de Simier war ("simian" ‚affenartig‘).

Elisabeth wurde vor allem als Gegenspielerin Maria Stuarts in der Literatur verewigt. Bekanntestes Beispiel ist Friedrich Schillers Schauspiel "Maria Stuart". Elisabeth ist auch die Heldin mehrerer historischer Romane, unter anderem von Susan Kay ("Legacy", deutscher Titel "Die Königin", 1985), Rosalind Miles ("Königin von England", 1998), Cornelia Wusowski ("Elisabeth I. Der Roman ihres Lebens", 2004) und Philippa Gregory ("The Queen’s Fool", 2004, und "The Virgin’s Lover", 2005). In Virginia Woolfs Roman "Orlando" beauftragt Elisabeth den jungen Adligen Orlando, niemals alt zu werden. In der Verfilmung von 1992 spielt Quentin Crisp die Rolle der Königin. Tanja Kinkel verwendete die historische Personenkonstellation um Elisabeth, ihre Schwester Maria, den Vater Heinrich und das Motiv des Gattenmordes sowie Philipp von Spanien und Robert Dudley für eine sehr ähnlich angelegte Konstellation in ihrem historischen Roman "Im Schatten der Königin" (2010).

Die bildende Kunst des 20. Jahrhunderts fand Elisabeth Eingang. Ihre Rolle in der Geschichte der Frauen machte die feministische Künstlerin Judy Chicago deutlich: In ihrer Arbeit The Dinner Party widmete sie ihr eines der 39 Gedecke am Tisch.

Auf der Opernbühne tritt Elisabeth unter anderem in Rossinis "Elisabetta regina d’Inghilterra" und Donizettis "Maria Stuarda", "Elisabetta al Castello di Kenilworth" und "Roberto Devereux" in Erscheinung. Benjamin Brittens Oper "Gloriana" hat die Essex-Verschwörung zum Thema und wurde 1953 aus Anlass der Krönung von Elisabeth II. uraufgeführt, die – ohnehin als keine große Opernfreundin bekannt – dem Vernehmen nach von der Oper enttäuscht war. Auch das Publikum konnte sich nicht für die als „national opera“ konzipierte, aber wenig staatstragende Oper begeistern, so dass "Gloriana" zu einem der wenigen Misserfolge in Brittens Opernkarriere wurde. Erst ab den 1990er Jahren wurde die Oper von Theatern und Publikum wiederentdeckt. Den Konflikt zwischen Elisabeth und Maria Stuart nahm Wolfgang Fortner als Vorlage für seine 1972 in Berlin uraufgeführte Oper "Elisabeth Tudor" (Libretto: Matthias Braun und der Komponist).

Sarah Bernhardt spielte 1913 im Film "Königin Elisabeth von England" die Elisabeth. In zwei Filmen spielte Flora Robson die Königin: in "Feuer über England" ("Fire over England") von 1937 und in "Der Herr der sieben Meere" ("The Sea Hawk") von 1940. Ebenfalls in zwei Filmen spielte Bette Davis die Königin: "Günstling einer Königin" ("The Private Lives of Elizabeth and Essex") von 1939 und "Die jungfräuliche Königin" ("The Virgin Queen") von 1955. 
Jean Simmons ist die Hauptdarstellerin in "Die Thronfolgerin", einem historischen Spielfilm über die Jugend der Königin Elisabeth I. von England aus dem Jahr 1953. Als literarische Vorlage diente der Roman "Die junge Bess" (Originaltitel: "Young Bess") von Margaret Irwin.

Glenda Jackson spielte in "Elizabeth R", einer sechsteiligen BBC-Serie. 1971 wurde die Serie mit fünf Emmys ausgezeichnet, darunter zwei für Jackson selbst. In den zahlreichen Filmen über das Leben der schottischen Königin Maria Stuart verkörpert Elisabeth stets ihre Gegenspielerin. Darunter in "Das Herz der Königin" mit Zarah Leander, "Maria von Schottland" mit Katharine Hepburn und "Maria Stuart, Königin von Schottland" mit Vanessa Redgrave. In diesem Film spielte Glenda Jackson zum zweiten Mal die Königin. Häufig fällt Maria hier die Rolle der Märtyrerin zu, während Elisabeth als Antagonistin eher negativ dargestellt wird.

In der satirischen BBC-Fernsehserie "Blackadder" mit Rowan Atkinson in der Hauptrolle spielt Miranda Richardson die Rolle einer überkandidelten Elisabeth, die jedoch manchen Charakterzug des Originals treffend darstellt. In dieser Version fällt sie einem Giftanschlag zum Opfer.

Die neuesten Verfilmungen sind die vierstündige Serie "Elizabeth I – The Virgin Queen" von 2005 mit Anne-Marie Duff, die die komplette Regierungszeit umfasst, und die zweiteilige Verfilmung "Elizabeth I" von 2006 mit Helen Mirren in der Hauptrolle. Der erste Teil handelt von den Heiratsverhandlungen mit dem Herzog von Alençon, und dem Konflikt mit Maria Stuart. Der zweite Teil hat die Essex-Verschwörung zum Thema. Der Film gewann neun Emmys, unter anderem für Helen Mirren und Jeremy Irons als Leicester.

Die bekannteste Verfilmung von Elisabeths Lebensgeschichte ist "Elizabeth" von 1998, in dem ihre späte Jugend unter ihrer Halbschwester Maria und die ersten fünf Regierungsjahre beschrieben werden. Auch die Liebesbeziehung zu Robert Dudley spielt hier eine größere Rolle. Der Film endet mit der Zerschlagung der Ridolfi-Verschwörung, ist dabei aber historisch eher ungenau, indem Figuren wie Maria von Guise (alias Fanny Ardant), die Mutter Maria Stuarts, tragende Rollen erhalten, die historisch schlicht unkorrekt sind. Cate Blanchett erhielt für ihre Darstellung der Titelfigur einen Golden Globe und ihre erste Oscar-Nominierung. Insgesamt wurde der Film in sieben Sparten vorgeschlagen, gewann jedoch nur in der Kategorie Makeup. Im gleichen Jahr erhielt "Shakespeare in Love" weitere sieben Oscars, unter anderem auch für Judi Denchs acht Minuten langen Auftritt als gealterte Elizabeth.

2007 kam mit "Elizabeth – Das goldene Königreich" eine Fortsetzung der Verfilmung von 1998 in die Kinos. Hier wird Elisabeths Beziehung zu Sir Walter Raleigh und der Krieg gegen Spanien beschrieben. Die Rolle der Königin wurde wieder von Cate Blanchett übernommen. Cate Blanchett erhielt für ihre Darstellung eine Golden Globe- und eine Oscar-Nominierung.

Im Film "Anonymus" von Roland Emmerich aus dem Jahr 2011 wird Elisabeth ein Verhältnis mit Edward de Vere unterstellt, der sich als ihr unehelicher Sohn erweist. Aus dem Verhältnis geht ein weiterer unehelicher Sohn hervor.





</doc>
<doc id="8451" url="https://de.wikipedia.org/wiki?curid=8451" title="Georges-Louis Leclerc de Buffon">
Georges-Louis Leclerc de Buffon

Georges-Louis Leclerc, Comte de Buffon (* 7. September 1707 in Montbard; † 16. April 1788 in Paris) war ein französischer Naturforscher im Zeitalter der Aufklärung. Sein offizielles botanisches Autorenkürzel lautet „Buffon“.

Georges-Louis Leclerc, später Comte de Buffon, war das erste der fünf Kinder von Benjamin-François Leclerc (1683–1775) und dessen erster Frau Anne-Christine Marlin (1681–1731). Sein Vater war als Anwalt für das Parlament von Burgund tätig und für das Eintreiben der Salzsteuer verantwortlich. Zur Hochzeit erhielt seine Mutter von ihrem kinderlosen Onkel Georges-Louis Blaisot († 1714), der als Steuereintreiber für den Herzog von Savoyen Viktor Amadeus II. sein Vermögen erworben hatte, eine üppige Aussteuer. Nach dem Tod von Blaisots Witwe im Jahr 1717 erbte der junge Buffon das Vermögen seines Taufpaten. Sein Vater erwarb mit diesem Geld die Seigneurie von Buffon, einem kleinen Dorf unweit von Montbard, sowie die Herrschaftsrechte über das Gut von Montbard. 1720 erkaufte sich Buffons Vater außerdem das Amt eines Beraters des Parlaments von Burgund.

Bereits 1717 hatte die Familie ihren Wohnsitz nach Dijon verlegt. Hier erhielt Buffon auf dem von den Jesuiten geleiteten Collège des Godrans von 1717 bis 1723 seine erste Ausbildung und freundete sich mit dem späteren Abbé Le Blanc (1707–1781) an. Seine Leistungen waren nicht besonders herausragend. Er entwickelte jedoch ein ausgeprägtes Interesse für Mathematik. Buffon las Euklids Schriften und studierte Marquis de l’Hospitals 1696 erschienenes Lehrbuch über Differentialrechnung. Nach Abschluss der Schule schrieb sich Buffon 1723 an der Juristischen Fakultät der kurz zuvor gegründeten Universität von Dijon ein. Hier traf er seinen ehemaligen Klassenkameraden Charles de Brosses wieder und lernte Gilles-Germain Richard de Ruffey (1706–1794) kennen. Mit beiden verband Buffon eine lebenslange Freundschaft. Die Freunde erhielten Zugang zum Kreis des Parlamentspräsidenten Jean Bouhier de Savigny (1673–1746), der einmal wöchentlich in seine umfangreiche Bibliothek lud. Der Humanist Bouhier begeisterte sich für die philosophischen Ideen von John Locke und Gottfried Wilhelm Leibniz. Vermutlich war es Bouhier, der Buffon in seinem Interesse an Naturforschung und Philosophie bestärkte.

Buffon beschloss sich vollständig der Wissenschaft zuzuwenden. Seine mathematischen Kenntnisse waren bereits auf dem Forschungsstand seiner Zeit. Er selbst gab später an, zu dieser Zeit den von Isaac Newton verallgemeinerten Binomischen Lehrsatz entdeckt zu haben. 1727 begann Buffons Briefwechsel mit dem Genfer Mathematikprofessor Gabriel Cramer. Den Kontakt zu Cramer vermittelte ihm eventuell Charles-Catherine Loppin de Gemeaux (1714–1805). 1728 zog Buffon nach Angers um dort seine Studien fortzusetzen. Die Gründe für seine Wahl sind unklar. Möglicherweise war die Lehrtätigkeit des Mathematikprofessors Père de Landreville am College de l’Oratoire ausschlaggebend. In Angers studierte er Mathematik, botanisierte und belegte verschiedene Medizinkurse. Hier las er Newtons Schriften und Bernard le Bovier de Fontenelles "Elements de la geometrie de l' infini" aus dem Jahr 1727. Aus ungeklärten Gründen war Buffon in ein Duell verwickelt, infolgedessen er gezwungen war, Angers im Oktober 1730 zu verlassen und nach Dijon zurückzukehren.

Am 3. November 1730 brach Buffon mit Evelyn Pierrepont (1711–1773), dem zweiten Duke von Kingston-upon-Hull, und dessen Lehrer Nathan Hickman (um 1695–1746) zu einer ausgedehnten Reise durch Südfrankreich und Italien auf. Sie führte zunächst über Nantes, Bordeaux, Toulouse, Montpellier bis nach Lyon, das sie im Mai 1731 erreichten. Buffon musste die Reise unterbrechen, da seine Mutter erkrankte; sie starb am 1. August 1731. Im Oktober 1731 war Buffon in Genf und konnte dort Cramer sprechen, der ihn mit dem Sankt-Petersburg-Paradoxon vertraut machte. In Genf traf er seine Reisegefährten wieder. Die weitere gemeinsame Reise führte über Turin, Mailand, Genua, Pisa, Florenz und endete in Rom. Nach dem Faschingsdienstag 1732 kehrte Buffon in seine Heimatstadt zurück.

Nach dem Tod von Buffons Mutter heiratete sein Vater am 30. Dezember 1732 die deutlich jüngere Antoinette Nadault (1709–1770). Buffon zwang seinen Vater zur Herausgabe seines vom Onkel Blaisot geerbten Vermögens. Ob es dabei zu einer gerichtlichen Auseinandersetzung kam, ist nicht bekannt. Das Verhältnis zu seinem Vater blieb jedoch lange Zeit angespannt.

Im Juli 1732 ließ sich Buffon im Pariser Bezirk Faubourg Saint-Germain nieder, heute Teil des 7. Arrondissement, bei Gilles-François Boulduc (1675–1741), einem Apotheker von König Ludwig XV. Hier wollte er seine Aufnahme in die "Académie des sciences" vorantreiben. Buffon verfasste eine Abhandlung mit dem Titel "Mémoire sur le jeu du franc-carreau" (Denkschrift über das Spiel Franc-Carreau), in der er die Differentialrechnung auf die Wahrscheinlichkeitsrechnung anwandte und das Studium geometrischer Wahrscheinlichkeiten initiierte. In dieser Abhandlung, die er der Akademie zur Begutachtung vorlegte, führte er auch das später nach ihm benannte „Nadelproblem“ ein. Émilie du Châtelet und Pierre-Louis Moreau de Maupertuis legten am 25. April 1733 ein sehr wohlwollendes Gutachten vor. Châtelet verlas Buffons Schrift auf dem nächsten Treffen der "Académie des sciences".

Buffon war bereits nach Montbard zurückgekehrt, wo er seinen Hauptwohnsitz errichten wollte. Er ließ sein bescheidenes Geburtshaus abreißen, kaufte einige Nachbargebäude und ließ ein ausgedehntes Herrenhaus erbauen. Auf der umgebenden Hügelkuppe ließ er einige der mittelalterlichen Befestigungsanlagen niederreißen. An ihrer Stelle entstand ein ausgedehnter, terrassenförmiger Park mit einer Menagerie, einem Laboratorium und einer Arbeitsstätte. Jean-Frédéric Phélypeaux, comte de Maurepas, Staatssekretär des königlichen Haushalts und der Marine, hatte sich bereits 1731 an die Akademie mit der Bitte gewandt, Methoden zu entwickeln, mit denen sich die Festigkeit und Langlebigkeit des zum Schiffsbau eingesetzten Holzes verbessern lasse. Der Akademie standen zur Durchführung der notwendigen Untersuchungen jedoch nicht die notwendigen Mittel zur Verfügung. Buffon, dem die Wälder bei Montbard gehörten, begann im Mai 1733 mit entsprechenden Experimenten.

Im Herbst 1733 war Buffon wieder in Paris. Die Akademie lud ihn ein, am 25. November eine Arbeit über Geometrie zu verlesen, die schließlich ein Problem der Mechanik behandelte. Am 12. Dezember 1733 wurde bekanntgegeben, dass in der "Académie des sciences" die Stelle eines "Associé astronome" neu zu besetzen sei. Die Akademie schlug Giovanni Domenico Maraldi, der bereits "Adjoint mécanicien" war, sowie Buffon für die freie Position vor. Der König bestimmte, dass Maraldi die Position erhalten solle und zu gleich Jean-Paul Grandjean de Fouchy (1707–1788) dessen alte Stelle einnehmen solle. Damit wurde die Position eines "Adjoint mécanicien" frei, für die am 23. Dezember unter anderem Buffon vorgeschlagen wurde. Am 9. Januar 1734 hatte Buffon sein Ziel, die Aufnahme in die "Académie des sciences", erreicht.

1739 wurde er von König Ludwig XV. zum Direktor des Königlichen Botanischen Gartens, heute Jardin des Plantes, in Paris ernannt und später in den Grafenstand erhoben.

Nachdem Buffon 1733 Mitglied der Académie des sciences geworden war, wurde er 1753 zudem Mitglied der Académie française. Mit seiner Antrittsrede "Discours sur le style" profilierte er sich auch als Literaturtheoretiker und begründete eine eigene Stiltheorie, die den Geist des Ancien Régime treffend charakterisierte. Berühmt geworden ist der Satz "Le style est l'homme même" („Der Stil ist der Mensch selbst“). Dieser Satz wurde von anderen Autoren im Lateinischen paraphrasiert als "Stilo primus, doctrina ultimus" („Zuerst kommt der Stil, dann die Lehre“ bzw. ironisch nach Jean Paul „Wohllaut statt Wahrheit“), womit sein wissenschaftlicher Habitus dem von Linné entgegengesetzt werden sollte. Die Antrittsrede gilt als eine der besten, die in der Académie jemals gehalten wurde und erschien in mehr als 60 Ausgaben.

Zudem machte sich Buffon in einer anderen Weise verdient: Im Jahre 1768 ließ er unweit von Montbard die "Forges de Buffon" errichten"," eines der leistungsfähigsten Hüttenwerke seiner Zeit. Zum ersten Mal in der Gegend wurden die drei Etappen der Eisenherstellung am selben Ort durchgeführt:


Am 21. September 1752 heiratete Buffon Marie-Françoise de Saint-Belin-Malain (1732–1769). Mit ihr hatte er die Tochter Marie-Henriette (1758–1759) und den Sohn Georges Louis Marie (1764–1794), der am 10. Juni 1794 guillotiniert wurde.

1782 wurde Buffon in die American Academy of Arts and Sciences gewählt.

Pierre-Louis Moreau de Maupertuis half zwar, das newtonsche Denken in Frankreich bekannter zu machen, er sah aber auch deutlich die Grenzen simpler newtonscher Paradigmen für die Chemie und besonders für die Biologie. Er nahm aus diesem Grunde die Ideen von Gottfried Wilhelm Leibniz in sein Gedankenkonstrukt mit auf. Durch ihn und Émilie du Châtelet wurde nun auch Buffon von den leibnizschen Ideen inspiriert.

Für Buffon bleibt die Natur immer dieselbe, aber Änderungen in ihrer substantiellen Ordnung und Form brächten immer neue Bildungen hervor. Im Jahre 1742 gelang es Buffon, den ebenfalls aus Montbard stammenden und ausgebildeten Mediziner Louis Jean-Marie Daubenton für die Arbeiten an seiner "Histoire naturelle générale et particulière" zu gewinnen. In ihm fand Buffon einen geschickten Präparator für seine anatomischen Studien. 1745 wurde er Aufseher und Erklärer, "garde-démonstrateur" am naturhistorischen Kabinett in Paris, "Cabinet du roi", später "Muséum national d’histoire naturelle".
Buffons Hauptwerk ist die "Allgemeine und spezielle Geschichte der Natur" ("Histoire naturelle générale et particulière"), die er in Zusammenarbeit mit Louis Jean-Marie Daubenton verfasste und die ursprünglich fünfzig Bände umfassen sollte. Ab 1749 bis zu seinem Tod 1788 erschienen 36 Bände. Unter Federführung des Comte de Lacépède wurden weitere acht Bände veröffentlicht. In Frankreich verschaffte das in vielen Sprachen übersetzte Werk seinem Urheber große wissenschaftliche Anerkennung und Popularität. Eine deutsche Ausgabe ("Allgemeine Historie der Natur") versehen mit einem Vorwort Albrecht von Hallers, erschien ab 1752 bei Grund und Holle in Hamburg, ab 1766 auch bei Holle in Leipzig. Eine Berliner Ausgabe besorgte Joachim Pauli ab 1771. Die "Histoire naturelle" war von Buffon als fortlaufende Edition jeweils einzelner Artikel und Bände herausgegeben worden, deren Zusammenstellung in den deutschen Ausgaben in der Systematik differiert. Die Hamburg/Leipziger Ausgabe hat eine andere Gliederung als die Berliner.

Entgegen der von seinem Zeitgenossen Carl von Linné vertretenden Auffassung, dass die ganze Natur mittels einer Taxonomie erfasst werden könne, vertrat Buffon die Ansicht, dass die Natur zu unterschiedlich und zu reich sei, um sich einem so strengen Rahmen anzupassen. Das naturwissenschaftliche Wirken Buffons basierte auf den Methoden von Beobachtung und Experiment. Er versuchte, die Entstehung der Lebewesen durch Urzeugung aus kleinsten Teilchen und ihre Entwicklung als Folge klimatischer Änderungen zu erklären, und setzte dem hierarchischen System Linnés die Idee einer evolutionären Stufenleiter entgegen. Seine Theorie stützte Buffon durch vergleichend-anatomische Studien. So erklärte er nutzlose Körperteile durch die Rückbildung ehemals nützlicher Teile eines Vorfahren. Buffon vertrat die Ansicht, dass alle Mitglieder einer "Familie" von Arten vom gleichen Vorfahren abstammen, von dem ausgehend sich einige vervollkommnet, andere jedoch zurückgebildet haben. Buffon sah zum Beispiel in einem Affen einen unvollständigen oder rückgebildeten Menschen.

Buffon hat nicht die ganze Arbeit allein getragen, vielmehr hatte er eine Reihe von Partnern, wie Philippe Guéneau de Montbeillard, Barthélemy Faujas de Saint-Fond, Gabriel Bexon und Charles-Nicolas-Sigisbert Sonnini de Manoncourt (1751–1812).

Buffons Stufenleiteridee hatte einen sehr großen Einfluss auf die Naturwissenschaft seiner Zeit und wirkte bis ins 19. Jahrhundert hinein. (Die Stufenleiteridee, "scala naturae" ist eine sehr viel weiter zurückreichende – neuplatonische – Idee, die besonders durch Leibniz im 18. Jahrhundert sehr populär wurde, vgl. dazu: Arthur O. Lovejoy: "Die Große Kette der Wesen".) Von großer Bedeutung ist dabei auch, dass Buffon für die stufenweise Entwicklung der Lebewesen lange Zeiträume annahm. Er teilte die Entwicklung der Erde in sieben Epochen ein. Ausgehend von der These, dass die Erde durch Zusammenstoß eines Kometen mit der Sonne entstanden sei und das erste Leben sich im Meer entwickelt habe, nahm Buffon als Alter der Erde 75.000 Jahre an. Dazu führte er Experimente mit Kugeln aus Eisen und anderen Materialien unterschiedlichen Volumens aus, erhitzte diese und maß die Abkühlungszeit (veröffentlicht im Supplement a l´histoire naturelle 1774). Damit wagte er es, wenn auch nicht als erster, die von den Theologen aufgrund biblischer Angaben errechnete Grenze von 6000 Jahren zu überschreiten. Buffon versuchte, dem durch seine Thesen hervorgerufenen Widerstand durch Abänderung besonders umstrittener Ansichten zu entgehen.

Seine Theorien sowie seine Methodik der Naturforschung erläuterte er ausführlich in den ersten drei Bänden seiner "Histoire naturelle". Den Hauptteil des Werkes bilden Beschreibungen der einzelnen Tier- und Pflanzenarten. Hierbei wurde erstmals auch die Skelettanatomie der Tiere dargestellt, was die Grundlagen der vergleichenden Anatomie schuf.

Buffon ist in die französische Literaturgeschichte eingegangen. Sein "Discours du style" und Auszüge aus der "Histoire naturelle" waren lange Zeit in den Lesebüchern für Gymnasien zu finden.

In der "Histoire Naturelle" vertrat Buffon zu Amerika die Auffassung, alle Dinge auf dem Kontinent Amerika würden „unter einem kärglichen Himmel und auf unfruchtbarem Land schrumpfen und verkümmern“. Tiere und Pflanzen sah er, der selbst niemals den Kontinent betreten hatte, in der Neuen Welt kleiner und schwächer. Da Buffons Werk weit verbreitet war, erreichte er, dass allein mit der Größe der Lebewesen, die er als einzigen Maßstab verwendete, im übertragenen Sinn die kulturelle und politische Stärke und Überlegenheit der alten Welt bzw. die Unterlegenheit der Neuen Welt begründet wurde. Die Degenerationsthese brachte die Gegenwehr Thomas Jeffersons ins Spiel, der sich jahrelang gegen Buffons Ansichten wehrte und sogar eine Expedition in die Wälder von New Hampshire aussandte, um einen Elch aufzufinden, der mächtig genug war, um Buffons Aussagen zu widerlegen. Alexander von Humboldt trat Buffon ebenfalls entgegen, wenn es um Südamerika ging und statuierte mit Bezug auf die Schönheit und Stärke indigener Menschen vom Orinoco, dass Buffon manches „ganz falsch“ beurteilt hatte. Humboldt vertrat die Auffassung, die Degenerationstheorie Buffons sei nur deshalb so beliebt, „weil sie der Eitelkeit der Europäer schmeichelte“. Buffon gab seinen Irrtum später zu.

Bereits 1745 hatte de Buffon behauptet, die Erde sei durch den Zusammenstoß eines Kometen mit der Sonne entstanden.

Es war der Abschnitt des "second discours" der "Histoire Naturelle" aus dem Jahre 1749, in welchem er seine Gedanken zur Entstehung der Erde, "preuves de la théorie de la terre" ausführte.
Jedoch ging er 1779 von einem Erdalter von mindestens 75.000 bis 80.000 Jahren aus. Dies stellt die erste bekannte Datierung innerhalb der modernen Wissenschaft dar, die nicht mehr von den aus der Bibel errechneten sechstausend Jahren ausging. Wegen des Einspruchs der Geistlichen an der Sorbonne veröffentlichte Buffon seine Schriften jedoch nicht.

Zunächst war Buffon durch die Lehren des Abraham Gottlob Werner ein überzeugter Neptunist, was in seiner Allgemeinen Naturgeschichte von 1746 deutlich wird.
Buffon sah in Meeresströmungen unter dem Meer die ausschlaggebende Ursache für die Veränderungen der Landmassen. Unter dem Einfluss der Erdrotation und der Strömungen würden sich, so nahm Buffon an, am Meeresboden die Sedimente zu gewaltigen Gebirgszügen auftürmen. Diese Gebirge traten an die Oberfläche und wurden zu Festland, wenn Wassermassen in die großen, ab und zu einstürzenden Hohlräume der Erdkruste eindrangen und der Meeresspiegel dadurch absank.

Erst später in seinen "Epoque de la nature" (1778) gelangte er zu einer vorsichtigen Annäherung an die Überlegungen der Plutonisten. Anhand von geologischen Tatsachen und Zeugnissen stellt er Überlegungen zum Aufbau der Erde an. Immer wieder findet sich in den „Epochen der Natur“ die Beschreibung einer „eigentümlichen Wärme“. Buffon meinte, dass diese unabhängig von der Sonneneinstrahlung sei, denn diese wäre lediglich in der Lage, maximal 15 bis 20 Fuß tief in die Erdoberfläche einzudringen. Er ging deshalb davon aus, dass die „eigentümliche Wärme“ aus dem Erdinneren stamme, Gesteine zunächst flüssig seien, ehe sich diese kristallisieren oder verfestigten. Als Beweis für die „eigentümliche Wärme“ führte er die Beobachtungen von Bergleuten an, die über unterschiedliche, unterirdische Grubentemperaturen berichteten, was heute als geothermische Tiefenstufe bekannt ist.

Carl von Linné benannte ihm zu Ehren die Gattung "Bufonia" der Pflanzenfamilie der Nelkengewächse (Caryophyllaceae). "Buffonia" Adans. (1763), "Buffona" Cothen. (1790) und "Buffonea" W.D.J.Koch (1836) sind weitere (ungültige) Schreibweisen des wissenschaftlichen Namens der Gattung.

Der Mondkrater Buffon wurde ebenfalls nach ihm benannt. Gleiches gilt für die Buffon-Inseln in der Antarktis.










</doc>
<doc id="8452" url="https://de.wikipedia.org/wiki?curid=8452" title="Grandma Moses">
Grandma Moses

Grandma Moses (* 7. September 1860 in Greenwich im US-Bundesstaat New York; † 13. Dezember 1961 in Hoosick Falls, New York; eigentlich "Anna Mary Robertson Moses") war eine Malerin, Illustratorin und Vertreterin der Naiven Kunst. Bemerkenswerterweise fing sie erst mit 75 Jahren mit dem Malen an.

Mit zwölf Jahren verließ sie die Farm ihrer Eltern, Mary Shannahan Robertson und Russell King Robertson. Bis zu ihrer Hochzeit mit dem Farmer Thomas Salmon Moses am 9. November 1887 arbeitete sie als Dienstmagd. Sie brachte zehn Kinder zur Welt, von denen fünf im Kindesalter starben. Als ihr Mann am 15. Januar 1927 verstarb und der jüngste Sohn mit seiner Frau die Farm übernahm, wandte sich Grandma Moses zur Beschäftigung der Malerei zu. Schon als Kind hatte sie gerne gemalt, war aber wegen vielfältiger Pflichten im Haushalt selten dazu gekommen. Auch in ihrer späteren Anstellung als Dienstmagd und während ihrer Ehe hatte sie ihr malerisches Talent aus Zeitmangel nicht entwickeln können. Nur die Ausschmückung der Familienwohnräume hatte ihr Gelegenheit gegeben, sich gestalterisch zu betätigen. 

Mit 75 Jahren, als die tägliche Hausarbeit ihr zu schwer wurde, fing Grandma Moses an Bilder anzufertigen. Zuerst stickte sie Wollbilder und benutzte gewöhnliche Anstreicherfarben. Später wandte sie sich Ölfarben und Leinwand zu. Von ihren Kindern ermutigt stellte Grandma Moses einige ihrer Bilder in einem Drugstore in Hoosick Falls aus. Der Kunstsammler Louis Caldor, der 1938 nach Hoosick Falls kam, nahm einige der Bilder nach New York mit. Nach mehreren erfolglosen Versuchen, die Kunstwelt für diese Werke zu interessieren, wollte er seine Bemühungen 1939 eigentlich einstellen. Als er aber zufällig von einer geplanten kleinen Ausstellung mit dem Namen „Unbekannte zeitgenössische amerikanische Maler“ des Museum of Modern Arts erfuhr, nahm er noch einmal einen Anlauf. Für die Ausstellung wurden drei Bilder von Grandma Moses ausgewählt. Es brauchte jedoch noch einige Zeit, um Grandma Moses einen größeren Bekanntheitsgrad zu verschaffen.

Grandma Moses’ Bilder bieten dem Betrachter Einblicke in das einfache Leben auf dem Land in Nordamerika zur damaligen Zeit. Dabei verarbeitete Grandma Moses viele persönliche Erfahrungen. Um die 30 Hauptwerke sind im Bennington Museum in Vermont zu sehen.






</doc>
<doc id="8456" url="https://de.wikipedia.org/wiki?curid=8456" title="Albert Bassermann">
Albert Bassermann

Albert Bassermann (* 7. September 1867 in Mannheim; † 15. Mai 1952 in Zürich) war ein deutscher Schauspieler.

Albert Bassermann wurde als Sohn des Fabrikanten Wilhelm Bassermann (1839–1906) und dessen Frau Anna geb. Pfeiffer in Mannheim geboren. Sein Onkel war der Schauspieler und Theaterintendant August Bassermann. 
Bassermann machte zunächst eine kaufmännische Lehre und studierte von 1884 bis 1886 Chemie, bevor er 1887 mit einer Schauspielausbildung begann.

Nach Engagements in Mannheim und Basel war er vier Jahre am Hoftheater in Meiningen tätig, bevor er 1895 nach Berlin kam. Ab 1899 war er dort bei Otto Brahm engagiert (bis 1904 am Deutschen Theater und dann bis 1909 am Lessing-Theater). Max Reinhardt holte ihn 1909 bis 1915 erneut ans Deutsche Theater. Danach gehörte Bassermann keinem Ensemble mehr an und war freischaffend tätig.

Von Friedrich Haase erhielt Albert Bassermann 1911 den Iffland-Ring. Nach seinem Tode wurde der Ring, den Bassermann dem verstorbenen Alexander Moissi auf den Sarg legte, 1954 vom Kartellverband deutschsprachiger Bühnenangehöriger an Werner Krauß weitergegeben. Der Ring ist seitdem Eigentum der Republik Österreich und wird momentan von Bruno Ganz getragen.

Bassermann gehörte zu den ersten deutschen Theaterschauspielern, die sich für den Film engagierten. Bereits 1913 spielte er die Hauptrolle des Rechtsanwalts Hallers in Max Macks "Der Andere" (es war auch sein erster Film) nach dem gleichnamigen Theaterstück von Paul Lindau. Bei zahlreichen weiteren Filmauftritten im deutschen Stummfilm arbeitete er unter Richard Oswald, Ernst Lubitsch, Leopold Jessner und
Lupu Pick.

Bassermann, der am 20. April 1933 in der Uraufführung von Hanns Johsts Schauspiel "Schlageter" mitgewirkt hatte, verließ 1934 wegen seiner als Jüdin diskriminierten Frau, der Schauspielerin Else Bassermann, Deutschland und emigrierte zunächst nach Österreich. Nach dem Anschluss Österreichs an das nationalsozialistische Deutsche Reich verließ er am 13. März 1938 zusammen mit seiner Frau Else Wien und lebte ab da in den USA. In Hollywood wurde er, obwohl er die englische Sprache nur mit einem sehr starken Mannheimer Akzent beherrschte, ein gefragter Charakterdarsteller. Für seine Nebenrolle in Alfred Hitchcocks "Der Auslandskorrespondent" (1940) wurde Albert Bassermann für den Oscar nominiert. 1944 hatte er sein Bühnendebüt am Broadway in einem englischsprachigen Stück, als Papst in der Uraufführung der Bühnenbearbeitung von Franz Werfels Roman "Der veruntreute Himmel".

Nach dem Krieg trat Bassermann ab 1946 auch wieder in Europa auf. Bei einem Gastspiel am Wiener Volkstheater spielte er in Paul Osborns "Der Himmel wartet (Der Tod im Apfelbaum)", sowie in Henrik Ibsens "Baumeister Solness" und – „zugunsten der politischen Opfer des Naziterrors“ – in Ibsens "Gespenster" jeweils in der Regie von Walter Firner und im Bühnenbild von Gustav Manker. Der Premiere wohnten Bundespräsident Karl Renner, Bundeskanzler Leopold Figl, Wiens Bürgermeister Theodor Körner sowie Vertreter der vier alliierten Besatzungsmächte bei. Allerdings war Bassermann angeblich, wie Fritz Kortner es formulierte, als „gebrochener Greis (...) zurückgekehrt. Das Publikum konnte sich für den schon Sterbenden nicht mehr erwärmen“. Trotzdem nahm Bassermann in seinen letzten Lebensjahren oft am Tourneetheater teil und hatte auch zahlreiche deutschsprachige Hörspielrollen: u. a. Michael Kramer in dem gleichnamigen Drama, Vater Knie ("Katharina Knie"), Striese ("Der Raub der Sabinerinnen"), Nathan ("Nathan der Weise"), Attinghausen ("Wilhelm Tell"). Weiterhin spielte er auch in Amerika und pendelte arbeitshalber zwischen der neuen und der alten Heimat hin und her.

Er galt seit Ende des 19. Jahrhunderts als einer der bedeutendsten deutschsprachigen Bühnenkünstler. Bassermann, der seit 1908 mit Else Bassermann, gebürtig Elisabeth Sara Schiff (1878–1961), verheiratet und Vater einer Tochter war, starb auf einem Flug von New York nach Zürich und ist auf dem Hauptfriedhof Mannheim beerdigt. Die Geburtsstadt von Albert Bassermann benannte eine Straße nach ihm. Seine Tochter Carmen verunglückte 1970 bei einem Verkehrsunfall tödlich.

Auf seinem Grab liegt eine tonnengewölbte Grabplatte aus Muschelkalk.

Bassermann hinterließ bei seinem Tod eine Taschenuhr, die sogenannte "Albert-Bassermann-Uhr", die auf seinen Wunsch hin 1952 der Schauspieler Martin Held als Anerkennung seiner Kunst erhielt. Diese Uhr wurde seitdem an den Schauspieler Martin Benrath und dann an den Hörspielregisseur und langjährigen Leiter des Süddeutschen Rundfunks Otto Düben weitervererbt. Derzeitiger Träger ist seit dem 1. Mai 2012 der Schauspieler Ulrich Matthes.


Zudem war er Ehrenmitglied der Genossenschaft Deutscher Bühnen-Angehöriger.






</doc>
<doc id="8457" url="https://de.wikipedia.org/wiki?curid=8457" title="Taylor Caldwell">
Taylor Caldwell

Taylor Caldwell, eigentlich Janet Miriam Reback, (* 7. September 1900 in Prestwich bei Manchester, England; † 30. August 1985 in Greenwich (Connecticut), USA) war eine mehrfach preisgekrönte Bestsellerautorin und Journalistin.

Caldwell war die Tochter des schottischen Kunsthändlers August William Reback. Sie veröffentlichte unter dem Pseudonym "Taylor Caldwell" zahlreiche Werke, von denen über 30 Millionen Exemplare verkauft wurden. Mit ihrem Werk – überwiegend Gesellschaftsromane sowie Romane aus biblischer Zeit – wurde sie schon bald zu einer wichtigen Autorin gehobener Gesellschaftsliteratur.

Romane
Gliederung nach deutschem Titelanfang.




</doc>
<doc id="8459" url="https://de.wikipedia.org/wiki?curid=8459" title="Michael Ellis DeBakey">
Michael Ellis DeBakey

Michael Ellis DeBakey bzw. Michael Ellis De Bakey (* 7. September 1908 in Lake Charles, Louisiana; † 11. Juli 2008 in Houston, Texas) war ein US-amerikanischer Herzchirurg; er perfektionierte u. a. die Herz-Lungen-Maschine und war an der Entwicklung chirurgischer Verfahren für das Kunstherz beteiligt. Die Schreibweise seines Nachnamens ist selbst in den von ihm verfassten Publikationen uneinheitlich, die nach ihm benannten Institutionen verwenden die Schreibweise "DeBakey".

Michael Ellis DeBakey wurde unter dem Namen Michel Dabaghi als Sohn von Shaker und Raheeja Dabaghi (die später den Familiennamen zu DeBakey anglisierten) geboren. Seine Eltern waren libanesische Maroniten, die in die USA immigriert waren. Seine medizinische Ausbildung absolvierte er an der Tulane University in New Orleans. Am Zweiten Weltkrieg nahm DeBakey als Freiwilliger teil und wurde Direktor der "Surgical Consultants' Division" der United States Army. In seiner Militärzeit schlug er u. a. die Einrichtung der Mobile Army Surgical Hospital (M.A.S.H.) vor, die später durch den gleichnamigen Roman und Film bekannt wurden.

1969 wurde DeBakey Direktor des "Baylor College of Medicine", ebenfalls 1969 erhielt er die Presidential Medal of Freedom, 1987 die National Medal of Science.

DeBakey gehörte 1980 zum Ärzteteam, das kurz vor dessen Tod den jugoslawischen Staatschef Josip Broz Tito aufgrund seiner pAVK ("Raucherbein") behandelte. 1996 leitete DeBakey das internationale Operationsteam, das die Bypass-Operation des damaligen Präsidenten Russlands, Boris Jelzin durchführte.

Neben einer Klassifikation für Aortendissektionen tragen die von ihm entwickelte Gefäßklemme und Coronarpinzette (für Bypasschirurgie), ein Kunstherzsystem für den Linksherzersatz (im Einsatz an der Medizinischen Universität Wien), die "DeBakey High School for Health Professions" und das "Michael E. DeBakey Veteran’s Affairs Hospital" des "Texas Medical Center" in Houston seinen Namen.

Beim Kunstherzen kam ihm 1969 Denton Cooley mit der ersten Verpflanzung zuvor, was – da Cooley ohne Rücksprache handelte – zum Bruch der beiden Chirurgen führte, die seit den 1950er Jahren an der Baylor College eng zusammengearbeitet hatten und unter anderem Techniken zur Entfernung von Aneurysmen entwickelten.

DeBakeys erste Frau Diana starb 1972, aus dieser Ehe entstammen die Söhne Michael und Dennis, die Söhne Ernest und Barry starben schon vor ihm. Seine zweite Frau war die deutsche Schauspielerin Katrin Fehlhaber, mit der er die Tochter Olga-Katarina hatte.

Seit 1999 war er ausländisches Mitglied der Russischen Akademie der Wissenschaften.

2008 wurde der renommierte Albert Lasker Award for Clinical Medical Research, den er selbst 1963 erhielt, ihm zu Ehren in Lasker~DeBakey Clinical Medical Research Award umbenannt.


DeBakey hat weit über 100 wissenschaftliche Fachaufsätze/-arbeiten veröffentlicht.

Auf Deutsch sind folgende Bücher erschienen:



</doc>
<doc id="8460" url="https://de.wikipedia.org/wiki?curid=8460" title="Elia Kazan">
Elia Kazan

Elia Kazan, , (* 7. September 1909 in Konstantinopel, Osmanisches Reich; † 28. September 2003 in New York City, New York) war ein griechischstämmiger US-amerikanischer Film- und Theaterregisseur und Schriftsteller. Er gilt als einer der angesehensten und erfolgreichsten Regisseure seiner Generation. Einige von Kazans Theaterproduktionen galten als bahnbrechend. Seine Filmdramen "Endstation Sehnsucht" (1951), "Die Faust im Nacken" (1954) und "Jenseits von Eden" (1955) erlangten ebenfalls große Popularität. Er wurde jeweils dreimal mit dem Oscar und dem Tony Award ausgezeichnet.

Elia Kazan, 1909 unter dem Namen Elias Kazancıoğlu als Sohn griechischer Eltern aus Kayseri in Istanbul geboren, galt als einer der herausragenden Regisseure Hollywoods. Seit er vier Jahre alt war (1913), lebte Kazan in New York. 1932 schloss er das Yale College mit dem Wunsch ab, Filmregisseur zu werden. Im selben Jahr heiratete er Molly Day Thatcher († 1963). Das Ehepaar hatte vier Kinder. In den 1930er Jahren gab es noch keine Ausbildungsmöglichkeiten für Filmregisseure, und so schloss er sich zunächst als Schauspieler dem "Group Theatre" an. Die Teilnehmer an der politisch links stehenden unabhängigen Theatergruppe lebten in den Sommermonaten wie in einer Kommune zusammen und erarbeiteten ihre sozialkritischen Inszenierungen. Zwischen 1934 und 1936 führte die Arbeit mit dem "Group Theatre" zu Kazans Mitgliedschaft in der kommunistischen Partei. Als er 1936 (nach nur 19-monatiger Mitgliedschaft) mit der Partei brach, weil diese sich zu stark in die Theaterarbeit des "Group Theatres" einmischte, begann seine Feindschaft gegenüber dem Stalinismus und seinen Methoden. Ab 1936 führte Kazan hier auch erstmals Regie; im selben Jahr spielte er eine Nebenrolle in Kurt Weills erster rein amerikanischer Produktion "Johnny Johnson". 1937 ging Kazan mit einigen Schauspielern des "Group Theatre" zu Probeaufnahmen nach Hollywood. Seine erste Hollywoodrolle spielte er 1940 in "Im Taumel der Weltstadt" an der Seite von James Cagney unter der Regie von Anatole Litvak. Kazan erhielt daraufhin kleinere Filmrollen. Die Mitglieder der Gruppe Franchot Tone und John Garfield entwickelten sich zu Filmstars. Kazan ging jedoch zurück nach New York und hatte ab 1942 erste große Erfolge als Regisseur am Broadway. 1943 ging dann sein lang gehegter Traum in Erfüllung und er drehte mit "Ein Baum wächst in Brooklyn" seinen ersten Film.

Kazans Film Tabu der Gerechten wurde 1947 von Darryl F. Zanuck produziert. Es war der erste Hollywoodfilm, der sich mit dem Thema Antisemitismus beschäftigte. Kazan mochte den Film nicht sonderlich. Er sei zu höflich und zeige nicht, wie schlimm Antisemitismus sei. Zanuck verlangte, dem Zuschauer das Thema über eine Liebesgeschichte zwischen Dorothy McGuire und Gregory Peck näher zu bringen. Obwohl Kazan der Meinung war, dass dies dem Realismus der Geschichte schade, hatte Zanuck mit dieser Methode Erfolg. Der Film wurde mit dem Oscar für den besten Film des Jahres ausgezeichnet und Kazan erhielt seinen ersten Regie-Oscar.

1947 war er neben Cheryl Crawford und Robert Lewis Mitbegründer des Actors Studio, aus dem Schauspieler wie Marlon Brando, James Dean und Julie Harris hervorgingen, die alle auch in Kazans späteren Filmen mitwirkten. Kazan besetzte in seinen Filmen immer wieder Schauspieler aus dieser Schule, die seit 1951 von Lee Strasberg als maßgeblichem Schauspiellehrer geleitet wurde. Kazan zog es wieder ans Theater. Er inszenierte zwischen 1947 und 1949 die Arthur-Miller-Erfolgsstücke "Alle meine Söhne" und "Tod eines Handlungsreisenden". Lee J. Cobb spielte den Willy Loman und die Inszenierung machte Kazan zu einem der wichtigsten Theaterregisseure dieser Zeit. Mit dem Schauspielschüler Marlon Brando inszenierte er am Broadway 1947 "Endstation Sehnsucht" von Tennessee Williams. Im Zentrum des Stückes stand eigentlich die Geschichte der zwei Schwestern Blanche und Stella, doch Brando veränderte mit seiner fulminanten Darstellung des Stanley Kowalski die Wahrnehmung des Stückes. Am Broadway spielte Jessica Tandy die Blanche. Die Produzenten von Warner Brothers tauschten sie jedoch in der späteren Verfilmung von 1951 gegen die als Star höher eingeschätzte Vivien Leigh. Vivien Leigh erhielt für diese Arbeit ihren zweiten Darstelleroscar. Kim Hunter und Karl Malden gehörten wie Brando bereits zum Ensemble der erfolgreichen Broadwayproduktion.

Neben der Theatertätigkeit drehte er für Hollywood den Streifen "Endlos ist die Prärie" mit Spencer Tracy. Seiner eigentlichen Idee vom realistischen Filmemachen ging er mit dem Film "Bumerang" weiter nach. Für diesen Film ging er mit dem Team in die Kleinstadt Stamford (Connecticut) und drehte ohne große Filmstars in den Straßen und Gebäuden der Stadt. Zeitweise hatte die Produktion Tausende von Zuschauern, da die Menschen Filmproduktionen an realen Orten noch nicht gewohnt waren. "Bumerang" handelt von einem unter dem Verdacht des Priestermordes stehenden Mann und beruht auf einer wahren Geschichte über den späteren US-Justizminister Homer S. Cummings.

Elia Kazan empfand eine große Nähe zu den Menschen der Südstaaten der Vereinigten Staaten. 1937 hatte er hier bereits erste Erfahrungen gesammelt mit dem kurzen Dokumentarfilm "Die Menschen aus Cumberland", der die Armut der Menschen während der Weltwirtschaftskrise in erschreckenden Bildern einfing. 1949 drehte er seinen ersten Spielfilm über den Süden. "Pinky" mit Jeanne Crain, Ethel Barrymore und Ethel Waters gehört allerdings zu den eher unbekannteren Werken Kazans.

1950 entwickelte er die Art Filme zu machen, die er mit "Bumerang" begonnen hatte, weiter und ging nach New Orleans, um den Thriller "Unter Geheimbefehl" mit Richard Widmark, Barbara Bel Geddes und Jack Palance an Originalschauplätzen zu drehen.

Nach dem großen Erfolg von "Endstation Sehnsucht" – der Film erhielt insgesamt vier Oscarauszeichnungen, davon drei Darstellerpreise für Vivien Leigh, Kim Hunter und Karl Malden – ging Kazan nach Mexiko, um 1952 die Geschichte des Revolutionärs Emiliano Zapata zu erzählen. John Steinbeck hatte das Drehbuch über den Aufstieg eines Bauern, der sich politisch gegen die Zustände in seinem Land auflehnt, zum erfolgreichen Revolutionär verfasst. Historisch betrachtet war Zapata der einzige Revolutionsgeneral, der weitreichende Sozialisierungen des Agrarlandes und der natürlichen Ressourcen des Landes anstrebte. Er wurde im Auftrag des Präsidenten Venustiano Carranza im Jahr 1919 ermordet. Entsprechend dem antikommunistischen Zeitgeist der 1950er Jahre verdrehte Kazan den historischen Hintergrund jedoch zu einer antisowjetischen und antirevolutionären Propagandaparabel, nach der Zapata von „totalitären Linken“ ermordet worden sei und die soziale Revolution sinnlos sei. Er kam auf dem Höhepunkt der „Hexenjagd“ von Senator Joseph McCarthy in die amerikanischen Kinos und spielte dessen antikommunistischer Hetzjagd in die Hände.

Im Gegensatz zu vielen Kollegen, die die Aussage verweigerten, um sich nicht an McCarthys Hetzjagd zu beteiligen, sagte Kazan vor dem sogenannten „Komitee für unamerikanische Umtriebe“ aus. Er berichtete vor dem Ausschuss von seiner Abscheu vor angeblichen „roten Methoden“ und denunzierte bereitwillig Kollegen, die in den 1930er Jahren bis zum Hitler-Stalin-Pakt Parteimitglieder gewesen waren. Kazans ehemalige Kollegen aus dem "Group Theatre", etwa John Garfield oder der Regisseur Jules Dassin, wurden nach den Aussagen Kazans auf die Schwarze Liste gesetzt und erhielten Berufsverbot.

Selbst war Kazan in den 1930er Jahren zu Beginn seiner Arbeit mit dem "Group Theatre" Mitglied der kommunistischen Partei, schied aber aus, als die Partei sich in die Theaterarbeit der Gruppe einmischte. Kazan wollte unbeeinflusst von ideologischer Bevormundung die künstlerische Ausrichtung bestimmen. Außerdem schien ihm seine frühere kommunistische Weltanschauung bei seiner Karriere hinderlich, weshalb er in dieser Zeit zum überzeugten Anti-Kommunisten wurde. So erklärt sich auch Kazans spätere Unterstützung der reaktionären McCarthy-Politik, die ebenfalls – aber von rechts und mittels staatlicher Repression – Einfluss auf das künstlerische Leben nahm und Hollywood von kritischen, linken Einflüssen „säuberte“. Kazan wurde dafür in New York und in Hollywood von Linken öffentlich scharf kritisiert.

Kazan wies die Kritik mittels öffentlicher Statements in Zeitungen zurück, in der er Stellung nahm zu seinen politischen Motiven. Er plädierte für „die Aufklärung der Amerikaner“. Sie müssten wissen, „was der Kommunismus wirklich“ sei, und zwar „mit allen Fakten“. Kazan war der Meinung, dass sich das vorgeblich „liberale Amerika“ nicht bewusst sei, was es hieße, „in einem totalitären System zu leben“. Die Dramatiker Lillian Hellman und Arthur Miller widersprachen ihm öffentlich.

Kazans weiteres Werk war geprägt von seinen Erfahrungen während der McCarthy-Ära. "Ein Mann auf dem Drahtseil" entstand 1953 und soll das Leben von Menschen in der Tschechoslowakei unter dem Druck des sowjetischen Totalitarismus zeigen. Fredric March und Gloria Grahame spielten die Hauptrollen. Kazans Kritiker warfen ihm nach "Die Faust im Nacken" vor, er habe sich für seinen Verrat mit diesem Film rechtfertigen und um Vergebung butten wollen. Der Film behandelt Korruption und Verrat unter New Yorker Gewerkschaftern und endet damit, dass dem Helden des Films sein Verrat vergeben wird. Kazan kam mit diesem Film seinem Ideal von Realismus am nächsten. "Die Faust im Nacken" wurde im strengen Winter 1954 auf den Straßen New Yorks und in Hoboken, New Jersey gedreht. Die Kälte spielte dabei eine weitere Hauptrolle und ist in jeder Szene spürbar. Die Schauspieler wirken dadurch nie künstlich. Im Mittelpunkt stand allerdings die Liebesgeschichte zwischen Marlon Brando und Eva Marie Saint und nicht das Sozialdrama. Elia Kazan erhielt für diese Arbeit seinen zweiten Regie-Oscar.

Bis zuletzt rechtfertigte Kazan seine Aussagen in dem Ausschuss. Das Misstrauen der Linken gegenüber Kazan, auch die Ablehnung durch Schauspielerkollegen aufgrund seines Verrats, sollte bis zu seinem Tod anhalten.

Marlon Brando war sicherlich der erste und für Kazan auch der wichtigste junge Schauspieler, der unter seiner Führung zum Weltstar wurde. Weitere Schauspielerentdeckungen sollten in den weiteren Jahren folgen. 1954 suchte er mit Autor John Steinbeck einen Schauspieler für die Rolle des jungen Cal in Jenseits von Eden, der im für die 1950er Jahre typischen Generationenkonflikt zu seinem Vater stand und fand ihn in James Dean. Kazan begründete damit den Weltruhm des Teenageridols schlechthin. Dean lebte den Konflikt mit dem Vater, gespielt von Raymond Massey, bereits während der Dreharbeiten voll aus. Ihre gemeinsamen Szenen sind geprägt von der Feindschaft der beiden Schauspieler auf dem Set. Dean lernte nie seinen Text und improvisierte die Streite mit dem Vater, was den perfekt vorbereiteten Schauspieler Massey fast in den Wahnsinn trieb. Kazan nutzte dies aus und unterbrach nie, wenn Dean einen völlig anderen Text sprach, als im Drehbuch stand. Diese Szenen erreichten dadurch eine enorme Authentizität. "Jenseits von Eden" war der einzige Film mit James Dean, der zu Lebzeiten des jungen Schauspielers seine Uraufführung hatte. Neben Dean spielte außerdem Julie Harris ihre erste große Rolle in einem Film.

Mit "Baby Doll – Begehre nicht des anderen Weib" 1956 ging Kazan wieder in die Südstaaten der Vereinigten Staaten und sorgte für den Durchbruch der Hauptdarstellerin Carroll Baker. Die kuriose Komödie "Baby Doll" entwickelte sich zu einem der größten Skandale der prüden 1950er Jahren. Karl Malden, bereits seit den Theatererfolgen Kazans der 1940er Jahre einer der wichtigsten und loyalsten Schauspieler Kazans, ist nach zahlreichen Nebenrollen hier in einer Hauptrolle zu sehen. Er spielt den unbedarften Archie Lee, der die Minderjährige Baby Doll heiratet und verspricht, erst mit ihr Geschlechtsverkehr zu haben, wenn sie 19 Jahre alt ist. Der provokante Film wurde vor allem aus kirchlichen Kreisen vehement attackiert. In einem Interview bezeichnete Kazan Karl Malden und Eli Wallach, die beide in "Baby Doll" spielen, als seine wichtigsten Entdeckungen: „Es sind eher unscheinbare Typen, aber sie haben das Method Acting nahezu perfektioniert.“

1957 blieb Kazan in den Südstaaten und kritisiert mit seinem Film "Ein Gesicht in der Menge" den Einfluss des noch jungen Mediums Fernsehen auf die politische Meinungsbildung der amerikanischen Bürger. Andy Griffith spielte Lonesome Rhodes, der durch das Fernsehen von einem provinziellen Lokalhelden zu einem nationalen Star aufsteigt. Der Film zeigt bereits auf, was mittlerweile Alltag geworden ist: das Fernsehen als Manipulator der Massen. Ein Mann beschränkten Intellekts erlangt durch seinen Charme und seine Popularität politische Macht. Griffith selbst wurde später ein beliebter TV-Star. Kazan entdeckte für diesen Film die junge Schauspielerin Lee Remick, die genauso wie Griffith ihr Kinodebüt gab.

Lee Remick erhielt dann auch eine Hauptrolle in Kazans nächsten Film "Wilder Strom". Sie spielt die Ehefrau von Montgomery Clift, der in Alabama in den 1930er Jahren das notwendige Land, der von der Depression gebeutelten Leute aufkaufen soll, damit ein Fluss umgeleitet werden kann. Eine alte Frau wehrt sich am heftigsten dagegen. Diese Frau wird von Jo Van Fleet gespielt, die 1960, als der Film gedreht wurde, nur halb so alt war wie ihre Rolle.

1961 verfilmte Kazan die eine Erzählung von William Inge, der zu "Fieber im Blut" auch selbst das Drehbuch schrieb. Natalie Wood verliebt sich in diesem Film in den Sohn der bedeutendsten Familie einer Kleinstadt in Kansas, doch sie werden niemals die Möglichkeit haben zusammenzukommen. Der Junge wird von einer weiteren Entdeckung Kazans gespielt, der danach eine Weltkarriere beginnt: Warren Beatty.

1963 drehte Kazan einen Film, an dem er bereits über 30 Jahre gearbeitet hatte. "Die Unbezwingbaren" ist die Geschichte seines Onkels und seiner Familie. Nach großen Schwierigkeiten bei der Finanzierung konnte er mit Warner Brothers den Film über seine griechischen Vorfahren und ihren Weg nach Amerika realisieren. Die Hauptrolle spielte der Laiendarsteller Stathis Giallelis, der erst für Monate in die Vereinigten Staaten kommen musste, um Englisch zu lernen. Während der Arbeit an diesem Film starb Kazans erste Frau. 1967 heiratete er die Schauspielerin Barbara Loden, die in seinen Filmen Wilder Strom und Fieber im Blut gespielt hatte. Sie hatten ein gemeinsames Kind.

"Die Unbezwingbaren" basierte bereits auf seinem eigenen Roman "America, America", und Kazan hatte auch selbst das Drehbuch geschrieben. Als Regisseur wurde es immer schwieriger für ihn, seine Filmprojekte zu finanzieren. Mit "Das Arrangement" (1969 nach dem gleichnamigen Roman von Kazan) mit Kirk Douglas und Faye Dunaway, "Die Besucher" 1972 und "Der letzte Tycoon" 1976 drehte er nur noch drei Filme und zog es vor, sich durch die Schriftstellerei künstlerisch auszudrücken. Insgesamt schrieb Kazan sieben Romane und seine Autobiografie. Sein letzter Film "Der letzte Tycoon" war eine Großproduktion mit Starbesetzung und ist ein eher untypischer Kazan-Film. Der Film basiert auf dem gleichnamigen Roman von F. Scott Fitzgerald; das Ensemble setzte sich zusammen aus Stars wie Robert De Niro, Tony Curtis, Robert Mitchum, Jeanne Moreau, Jack Nicholson, Donald Pleasence, Ray Milland, Dana Andrews, Peter Strauss und John Carradine. Erfolg war diesem Film trotz des riesigen Aufwandes nicht mehr beschieden, und Elia Kazan verabschiedete sich vom Regiestuhl.

1980 starb Elia Kazans zweite Frau Barbara Loden an Krebs. Der zweifache Witwer Kazan heiratete 1982 ein drittes Mal. Seine dritte Frau Frances Rudge war bis zu seinem Tod 2003 seine Ehefrau. Eines seiner Kinder aus erster Ehe ist der 1950 geborene Drehbuchautor Nicholas Kazan, der mit Regisseurin und Drehbuchautorin Robin Swicord verheiratet ist. Deren Tochter Zoe Kazan, Elia Kazans Enkeltochter, ist Schauspielerin.

1988 war Kazan Leiter des 7. Istanbuler Filmfestivals. Seit den 1970er Jahren verbrachte er viel Zeit in seiner alten Heimat; eine große Freundschaft verband ihn mit dem türkischen Musiker, Autor und Filmemacher Zülfü Livaneli, in dessen Film "Sis" er auch eine kleine Gastrolle innehatte. Auch mit dem türkischen Schauspieler und Regisseur Yılmaz Güney verband ihn eine enge Freundschaft. Beispielsweise besuchte er Güney 1978 im Toptaşı-Gefängnis in Istanbul. Daraufhin veröffentlichte Kazan am 4. Februar 1979 einen Artikel „The View from a Turkish Prison“ für die New York Times, in dem er über das Treffen mit Yilmaz Güney und die Zustände im Gefängnis berichtete. Kazan sprach Englisch, Griechisch und Türkisch.

1999 wurde Elia Kazan mit einem Ehrenoscar für sein Lebenswerk ausgezeichnet. Aufgrund seines Verhaltens während der McCarthy-Ära wurde diese Entscheidung nicht ausschließlich positiv aufgenommen; viele Zuschauer blieben bei der Verleihung demonstrativ sitzen und einige anwesende Schauspieler applaudierten nicht, unter anderem Ed Harris und Nick Nolte. Andere wiederum applaudierten, um ihre Bewunderung für seine Leistungen als Filmemacher auszudrücken.

Kazan starb eines natürlichen Todes im Alter von 94 Jahren in New York.








</doc>
<doc id="8462" url="https://de.wikipedia.org/wiki?curid=8462" title="Baudouin (Belgien)">
Baudouin (Belgien)

Baudouin – gebürtig , , – (* 7. September 1930 auf Schloss Stuyvenberg, Laeken; † 31. Juli 1993 in Motril, Spanien) aus dem Haus Sachsen-Coburg und Gotha war von 1951 bis 1993 König der Belgier. 

Nach der Abdankung seines Vaters Leopold III. folgte ihm Baudouin am 16. Juli 1951 als belgischer König nach.

Die Königliche Familie wurde nach der Landung der Alliierten 1944 von den Deutschen zunächst ins sächsische Hirschstein und im März 1945 nach Strobl in Österreich verschleppt. Am 7. Mai 1945 kam es zur Befreiung durch US-amerikanische Truppen und König Leopold III. sah sich von der belgischen Öffentlichkeit mit Vorwürfen konfrontiert, er habe im Krieg gegen die Deutschen zu früh kapituliert. Er konnte deshalb zunächst nicht nach Belgien zurückkehren. Der Bruder des Königs, Prinz Karl von Belgien, nahm daher die Regentschaft mit der Begründung wahr, dass sein Bruder sich in „der Unmöglichkeit zu regieren“ befinde.

Da man keine politische Lösung der Kontroverse um den König fand, gab es eine Volksabstimmung für oder gegen die Rückkehr des Königs. Die Antwort der Wählerschaft erbrachte eine Mehrheit von 58 % zu seinen Gunsten, mit starken regionalen Unterschieden. Am 22. Juli 1950 kehrte der König nach Brüssel zurück. Doch nach schweren Unruhen, vor allem von wallonischen Arbeitern, dankte Leopold III. ab und schlug vor, seine königlichen Befugnisse seinem Sohn Prinz Baudouin zu übertragen. So legte dieser am 17. Juli 1951 seinen Eid auf die Verfassung ab und wurde der fünfte König der Belgier.

Anlässlich seines 25-jährigen Thronjubiläums 1976 wurde die "König-Baudouin-Stiftung" ins Leben gerufen, die sich die Verbesserung der Lebensbedingungen der belgischen Bevölkerung zum Ziel gesetzt hat.

Weil sein katholischer Glaube dagegen sprach, weigerte Baudouin sich 1990, ein Gesetz zur Liberalisierung des Abtreibungsgesetzes zu unterzeichnen. Die belgische Regierung erklärte Baudouin deshalb auf dessen eigenen Wunsch hin am 4. April 1990 für regierungsunfähig. Für diesen Fall sieht die belgische Verfassung vor, dass die gesamte Regierung die Funktion des Staatsoberhauptes übernimmt. Nachdem alle Regierungsmitglieder das Gesetz unterzeichnet hatten, erklärte die Regierung am nächsten Tag, dem 5. April 1990, Baudouin wieder für regierungsfähig. 

Baudouin regierte 42 Jahre bis zu seinem Tod am 31. Juli 1993, als er in der Villa Astrida in Motril in Südspanien an Herzversagen starb. Die Trauer um den Tod des „einzigen Belgiers“ wurde in der Bevölkerung über alle Sprachgrenzen hinweg tief empfunden. Baudouin wurde in der königlichen Gruft in der Liebfrauenkirche zu Laeken, Belgien, beigesetzt. Da er keine Kinder hatte, wurde Baudoins Bruder Albert sein Nachfolger.

Eine im Jahre 2002 einberufene Fachkommission des belgischen Parlaments untersuchte die Ereignisse um die Ermordung des ersten kongolesischen Ministerpräsidenten Patrice E. Lumumba (* 2. Juli 1925; † 17. Januar 1961). Die Demokratische Republik Kongo war am 30. Juni 1960 in die Unabhängigkeit entlassen worden. In ihrem Schlussbericht kam die Kommission zu dem Ergebnis, dass König Baudouin von den Plänen zur Ermordung Lumumbas wusste. Fest steht, dass die belgische Regierung die Lumumba feindlich gesinnten Kräfte im Kongo logistisch, finanziell und militärisch unterstützte. Ein Großteil der Schuld wird unmittelbar König Baudouin zugeschrieben, der unter Umgehung der politischen Instanzen seine eigene postkoloniale Politik betrieben haben soll.

König Baudouin heiratete am 15. Dezember 1960 die spanische Adelige Fabiola Mora y Aragón (* 11. Juni 1928 in Madrid; † 5. Dezember 2014 in Brüssel), eine ehemalige Krankenschwester und Kinderbuchautorin. Die Ehe blieb kinderlos, da alle Schwangerschaften der Königin mit Totgeburten endeten.




</doc>
<doc id="8465" url="https://de.wikipedia.org/wiki?curid=8465" title="Albert Schweitzer">
Albert Schweitzer

Ludwig Philipp Albert Schweitzer (* 14. Januar 1875 in Kaysersberg im Oberelsass bei Colmar; † 4. September 1965 in Lambaréné, Gabun) war ein deutsch-französischer Arzt, Philosoph, evangelischer Theologe, Organist, Musikwissenschaftler und Pazifist.

Schweitzer, der „Urwaldarzt“, gründete ein Krankenhaus in Lambaréné im zentralafrikanischen Gabun. Er veröffentlichte theologische und philosophische Schriften, Arbeiten zur Musik, insbesondere zu Johann Sebastian Bach, sowie autobiographische Schriften in zahlreichen und vielbeachteten Werken. 1953 wurde ihm der Friedensnobelpreis für das Jahr 1952 zuerkannt, den er 1954 entgegennahm.

Schweitzer stammte aus einer alemannisch-elsässischen Familie. Geboren wurde er als Sohn des Pfarrverwesers Ludwig (Louis) Schweitzer, der eine kleine evangelische Gemeinde betreute, und dessen Frau Adele, geb. Schillinger, der Tochter eines Mühlbacher Pfarrers. Zu diesem Zeitpunkt gehörte seine Heimat als Reichsland Elsaß-Lothringen zu Deutschland. Noch im Jahr seiner Geburt zog die Familie von Kaysersberg nach Günsbach um. Seine Muttersprache war der elsässische Ortsdialekt des Oberdeutschen. Daneben wurde in seiner Familie auch Französisch gesprochen. Das Hochdeutsche erlernte Schweitzer erst in der Schule. Deutsch und Französisch beherrschte er fast gleich gut.

Nach dem Abitur 1893 in Mülhausen studierte er an der Universität Straßburg Theologie und Philosophie (Erstes Theologisches Examen 1898) und war Mitglied der 1855 gegründeten Studentenverbindung "Wilhelminata Straßburg". Zudem studierte er in Paris Orgel bei Charles-Marie Widor, bei dem er seit 1893 schon gelegentlich Unterricht genommen hatte, und Klavier bei Marie Jaëll.

1899 wurde er dann nach einem kurzen Studienaufenthalt an der Berliner Friedrich-Wilhelms-Universität mit einer Dissertation über "Die Religionsphilosophie Kants von der Kritik der reinen Vernunft bis zur Religion innerhalb der Grenzen der bloßen Vernunft" in Straßburg zum Dr. phil. promoviert. 1901 folgte die theologische Dissertation zum Lic. theol. "Kritische Darstellung unterschiedlicher neuerer historischer Abendmahlsauffassungen" (Erstauflage 1906), die in der zweiten Fassung den Titel "Geschichte der Leben-Jesu-Forschung" (Tübingen 1913) trägt.

1902 erfolgte an der Universität Straßburg die Habilitation in Evangelischer Theologie mit der Schrift "Das Messianitäts- und Leidensgeheimnis". Mit der Habilitation wurde er Dozent für Theologie an der Universität Straßburg. Seit 1898 war er Lehrvikar und ab November und zweiter Theologischer Prüfung ordinierter Vikar an der Kirche St. Nikolai. Ein Teil seiner dortigen Predigten und Predigtentwürfe ist erhalten durch die Hand der mit ihm befreundeten Annie Fischer, Witwe des Straßburger Professors der Chirurgie, Fritz Fischer, und Schwester von Hugo Stinnes. Seine Theologie fand unter anderem bei Fritz Buri Nachhall. Schweitzer schrieb 1905 die französische Ausgabe von "Johann Sébastien Bach", auf die drei Jahre später 1908 seine neu verfasste deutsche Bach-Monographie folgte.

Im Jahr 1905 wurde Schweitzer als Missionar bei der Pariser Missionsgesellschaft wegen seiner liberalen theologischen Ansichten abgelehnt. Von 1905 bis 1913 studierte Albert Schweitzer so Medizin in Straßburg mit dem Ziel, in Französisch-Äquatorialafrika als Missionsarzt tätig zu werden. Die Immatrikulation zum Studium der Medizin war sehr kompliziert. Schweitzer war ja bereits Dozent an der Universität Straßburg. Erst eine Sondergenehmigung der Regierung machte das Studium möglich. 1912 wurde er als Arzt approbiert, im gleichen Jahr wurde ihm der Titel eines Professors für Theologie verliehen auf Grund seiner „anerkennenswerten wissenschaftlichen Leistungen“. 1913 folgte seine medizinische Doktorarbeit: "Die psychiatrische Beurteilung Jesu: Darstellung und Kritik". In dieser Arbeit widerlegt er, analog seiner theologischen Dissertation, zeitgenössische Versuche, das Leben Jesu aus psychiatrischer Sicht zu beleuchten. Somit war er, im Alter von 38 Jahren und bevor er nach Afrika ging, in drei verschiedenen Fächern promoviert, hatte sich habilitiert und war Professor.

Albert Schweitzer heiratete 1912 Helene Bresslau (1879–1957), die Tochter des jüdischen Historikers Harry Bresslau und dessen Frau Caroline, geborene Isay. 1919 wurde die Tochter Rhena Schweitzer-Miller († 2009) geboren, die bis 1970 die Stiftung ihres Vaters weiterführte.

1913 setzte Schweitzer sein Vorhaben in die Tat um und gründete am Ogooué, einem 1200 km langen Fluss in Gabun, das Urwaldhospital Lambaréné. Das Gebiet gehörte damals zu Französisch-Äquatorialafrika. Schon ab 1914, als der Erste Weltkrieg ausbrach, wurden er und seine Frau Helene, eine Lehrerin, aufgrund ihrer deutschen Staatsangehörigkeit von der französischen Armee unter Hausarrest gestellt.

1917, erschöpft von mehr als vier Jahren Arbeit und von einer Art tropischer Anämie, wurde das Ehepaar Schweitzer festgenommen, von Afrika nach Frankreich überführt und in Bordeaux, Garaison und dann St. Rémy de Provence bis Juli 1918 interniert. Diese Zeit nutzte Albert zur Entwicklung und zum Ausbau seiner Ethik der "Ehrfurcht vor dem Leben". Zentral für diese Ethik ist der Satz: „Ich bin Leben, das leben will, inmitten von Leben, das leben will.“

Gegen Kriegsende kamen sie 1918 ins Elsass zurück, das am 6. Dezember wieder an Frankreich angeschlossen wurde. Dort nahm Schweitzer die französische Staatsbürgerschaft an, er selbst bezeichnete sich jedoch gern als Elsässer und „Weltbürger“. Er nahm wieder die Stelle als Vikar in St. Nikolai in Straßburg an und trat als Assistenzarzt in ein Straßburger Spital ein.

Dank des schwedischen Bischofs Nathan Söderblom konnte Albert Schweitzer ab 1920 in Schweden Vorträge über seine Ethik der Ehrfurcht vor dem Leben halten, mittels Orgelkonzerten seine Schulden bezahlen und Geld für die Rückkehr 1924 nach Afrika verdienen, um dort das Urwaldhospital auszubauen.

Bekannt wurde Albert Schweitzer vor allem durch seine Autobiografie „Zwischen Wasser und Urwald“, die er in kurzer Zeit 1920 geschrieben hatte. In seiner Rede zum 100. Todestag Johann Wolfgang von Goethes 1932 in Frankfurt am Main warnte Schweitzer vor den Gefahren des aufkommenden Nationalsozialismus. Versuchen von Joseph Goebbels, den in Lambaréné weilenden Schweitzer einzuladen und für die NS-Ideologie zu gewinnen, erteilte er auf die "mit deutschem Gruß" geschlossenen Anfrage "mit zentralafrikanischem Gruß" eine höfliche Absage.

Nach dem Zweiten Weltkrieg wurde ihm viel öffentliche Ehre zuteil. In seiner erst 1954 gehaltenen Dankesrede zur Verleihung des Friedensnobelpreises von 1952 sprach sich Schweitzer deutlich für eine generelle Verwerfung von Krieg aus: „Krieg macht uns der Unmenschlichkeit schuldig“, „zitiert“ Albert Schweitzer Erasmus von Rotterdam. Infolge der Genfer Konvention von 1864 und der Gründung des Roten Kreuzes sei es zu einer „Humanisierung des Krieges“ gekommen, die dazu geführt hätte, dass die Menschen 1914 den beginnenden Ersten Weltkrieg nicht in der Weise ernst genommen hatten, wie sie dies hätten tun sollen.

Zum Teil wurden Schweitzer rassistische, paternalistische und pro-kolonialistische Einstellungen vorgeworfen. So kritisierte er die Unabhängigkeit von Gabun, weil das Land dafür noch nicht bereit sei. Chinua Achebe berichtete, dass Schweitzer gesagt habe, Afrikaner seien seine Brüder jedoch seine „jüngeren Brüder“. Der amerikanische Journalist John Gunther besuchte Lambaréné in den 1950ern und kritisierte Schweitzers paternalistische Einstellung gegenüber Afrikanern. Auch würden diese dort nicht als Fachkräfte eingesetzt. Nach Jahrzehnten, die Schweitzer schon in Afrika wirkte, kämen die Krankenschwestern noch immer aus Europa.

Schweitzer geht 1962 in der Quintessenz seines philosophischen Denkens davon aus, dass sich Menschen beim Nachdenken über sich selbst und ihre Grenzen wechselseitig als Brüder erkennen, die über sich selbst und ihre Grenzen nachdenken. Im Zuge des Zivilisationsprozesses wird die Solidarität, die ursprünglich nur auf den eigenen Stamm bezogen war, nach und nach auf alle, auch unbekannte Menschen übertragen. In den Weltreligionen und Philosophien sind diese Stadien der Kulturentwicklung konserviert.
Analog wirkt in den weltverneinenden Religionen des indischen Kulturkreises nach der Philosophie von Arthur Schopenhauer eine Ausbreitung des Mitleids, das im Brahmanismus jenseits der (wahren) Metaphysik im Leid der (falschen) materiellen Welt begründet ist und deshalb abgelehnt, im Buddhismus mit Bezug auf eine erweiterte Metaphysik gefordert und im Hinduismus ins Alltagsleben integriert wird, das als Spiel der Götter mit Menschen verstanden wird (Bhagavad Gita). Die geforderte Teilnahmslosigkeit gegenüber Leid verpflichtet zum Pazifismus. Schweitzer bezog sich auch auf Mahatma Gandhi.

Auch die Ausbreitung des weltbejahenden Zoroastrismus persischer Siedler, vereint in Solidarität gegen heidnische Nomaden, beeinflusst die griechische Philosophie, in der der Stoiker Panätios die Weltbejahung mit einer allumfassenden Vernunft begründet, in der Seneca, Epiktet und Marc Aurel als Tugend aller Tugenden den Humanismus entwickeln.

Im Schmelztiegel der persischen und der griechischen Kultur waren das Judentum und das Christentum entstanden, die die Welt als wahr, aber unvollkommen sehen. Das Christentum fordert Weltentsagung zur Ausweitung des Guten im Menschen und findet auf der Suche nach dem Gebot aller Gebote ebenfalls zum Ideal des Humanismus.

Seit der Renaissance verwachsen die außengeleitete Tugend aller Tugenden und das innengeleitete Gebot aller Gebote zu einem weltlichen Recht (Erasmus von Rotterdam), Grundlage für den Utilitarismus von Jeremy Bentham, während David Hume eine natürliche Empathie als Ursache annimmt. Immanuel Kant verbindet diese mit dem Dualismus und verlegt die Moral in der Form des Kategorischen Imperativ in die Natur des Menschen, der in der geistigen Welt als Subjekt lebt und in der gegenständlichen nur Objekt ist.

Das häufige Scheitern am moralischen Anspruch macht aus dem guten Gewissen einen Mythos, während die Zivilisation das Vertrauen und den Sinn mit der Folge von Resignation und reaktiver Sentimentalität untergräbt. Damit dieser Druck dazu führt, dass das Subjekt sein Sein als „Wille zum Leben inmitten vom Willen zum Leben“ anderer begreift und diese Erfahrung mit dem Liebesgebot Jesu unterfüttert, braucht es Anleitung. Dann verbindet es die Gebote des Gewissens in der Form des Kategorischen Imperativ in der geistigen Welt mit den Tugenden in der gegenständlichen Welt und erkennt den Unterschied zwischen böse und gut als Ausdruck lebensschädigender und lebensfördernder Wirkungen und findet darin den höchsten sittlichen Wert.

Dieser sittliche Wert ermöglicht eine Lebensanschauung, in der Lebensbejahung keine Erkenntnis-, sondern eine Willenskategorie ist, Lebensverneinung in der Rücksichtnahme auf den Willen anderer liegt und Lebensentsagung im verinnerlichenden, sich selber sammelnden (Musik) und vervollkommnenden Gebot besteht, auch das eigene Leben aus Berufung auf den sittlichen Wert der Ethik zu heben, die Volksweisheiten von „Was du nicht willst, das man dir tu', das füg' auch keinem andern zu“ bis hin zu „Liebe deinen Nächsten wie dich selbst“ vereint und auf alles Lebendige überträgt.

Entscheidungen zwischen Moral und Sachzwang führen zur Beschäftigung mit dem Ideal der Ethik, in die der Mensch hineinwächst. Die Verantwortung braucht einen individuellen, sozialen und politischen Willen, der dem eigenen Dasein einen geistigen Wert verleiht und zur gegenständlichen Welt ein Verhältnis knüpft, in dem der Mensch von einer naiven zu einer vertieften Weltbejahung gelangt. Elementares Denken ist die Voraussetzung einer verständlichen und überzeugenden Ethik, die bei der Auseinandersetzung mit der Wirklichkeit in dieser wie Sauerteig im Brot wirkt.

Das zwischenmenschliche Verhältnis ist von Fremdheit und Kälte geprägt, weil sich niemand traut, sich so herzlich zu geben, wie er ist. Die Überwindung verwurzelt die Herzlichkeit in der Ehrfurcht vor dem Leben und verhilft zu einer Güte in Bescheidenheit, weil man bei jeder Entscheidung immer wieder auf sich selbst zurückgeworfen wird und zu resignieren droht. Doch gerade die Jugend verfügt über die Energie, die resignierte Vernünftigkeit der gereiften Persönlichkeit zu hinterfragen und hat den Mut, einen moralischen Kompass für einen lebensfördernden Umgang mit Sachzwängen zu justieren.

Da die Kreatur wehrlos der menschlichen Willkür ausgesetzt ist, beziehen ethische Entscheidungen die Willkür mit ein und schädigen Leben nicht aus Gedanken- oder Teilnahmslosigkeit. Mitleid mit Tieren ist trotz ihrer angeblichen Seelenlosigkeit keine Sentimentalität, denn alles notwendige Töten ist ein Grund zu Trauer und Schuld, der man nicht entkommen, die man nur verringern kann.

Albert Schweitzer ist zur Schonung der Tiere zur vegetarischen Ernährung übergegangen. „Meine Ansicht ist, dass wir, die für die Schonung der Tiere eintraten, ganz dem Fleischgenuss entsagen und auch gegen ihn reden. So mache ich es selber.“

Im Pazifismus, oft als Utopie belächelt, sieht Schweitzer ein überlebenswichtiges Gegengewicht zur Patt-Situation der Abschreckung. Die Gesinnung der Unmenschlichkeit will sich die Entscheidungsfreiheit über Krieg oder Frieden als Voraussetzung der Friedensgarantie mit einer Position der Stärke erhalten. Sie übersieht die Bedrohung der Stärke durch die Ausweitung von Sachzwängen zur Aufrüstung mit der Folge einer Steigerung der Kriegsgefahr als selbsterfüllende Prophezeiung (Rüstungsspirale). Sie bemerkt nicht, dass auch der Sieger vom Sieg nichts hat.

Trotz aller Zweifel rät Schweitzer aus Angst vor der Gesinnung der Unmenschlichkeit zur einseitigen Abrüstung. Da die resignierte Vernunft nicht erkennt, dass Vernichtungskriege mehr Probleme schaffen als lösen, kann die Ehrfurcht vor dem Leben nur mit Mut die Hoffnung entwickeln, mit der die Öffentlichkeit die Idee einer weltbejahenden Kultur entwirft und die Verantwortung über Krieg und Frieden übernimmt.

Vereinzelte stehen einer absoluten Wirklichkeit gegenüber, die wegen ihrer Transzendenz so unverständlich ist, dass sie sich in ihr nur ihre einzelnen Vorstellungswelten errichten können, in denen sich, jeweils in Objekt und Subjekt getrennt, der Wille der absoluten Wirklichkeit widerspiegelt. Der Wille an sich ist einerseits frei, aber blind, andererseits sehend, da von der eigenen Vorstellung festgelegt (Determinismus), aber unfrei. Deshalb kann das Subjekt den Willen nicht mehr zur Unterscheidung von Schöpfung und Zerstörung nutzen und Sinn entwickeln. Schweitzer sieht die Essenz zur Überwindung dieses Paradoxons a priori im Menschen angelegt, Inneres wird entsprechend externalisiert. Die kritische Auseinandersetzung mit der in Frankreich populär gewordenen Existenzphilosophie beschäftigte Schweitzer noch in seinen letzten Lebensjahren; Jean-Paul Sartre war der Sohn von Schweitzers Cousine Anne-Marie. Sartres Existentialismus geht von den gleichen Vorstellungen aus: Der Sinnlosigkeit steht die freie Verantwortung des vereinzelten Gewissens gegenüber, das sich allerdings in seiner Ich-Bezogenheit seine Essenz in der Intersubjektivität durch das Eintreten für bestimmte Werte selber schafft: Außeneinflüsse werden entsprechend internalisiert.

Albert Schweitzer verstand sich selbst in erster Linie als Philosoph. Als Schüler des Straßburger Neutestamentlers Heinrich Julius Holtzmann, ein führender Vertreter der historisch-kritischen Forschung seiner Zeit, befasste sich Albert Schweitzer zeitlebens auch mit theologischen Themen, insbesondere Fragen der Bibelauslegung und Theologie des Neuen Testaments sowie mit dem Thema der religiösen Mystik.

Schweitzer erkennt in allen historistischen Entwürfen vom Leben Jesu nur die Projektionen der betreffenden Forscher, die ihre eigenen Vorannahmen und Vorstellungen von Jesus in ihre Darstellung hineinlesen. Keinen der Versuche der liberalen Theologie seiner Zeit, sich mit den Mitteln historischer Rekonstruktion der authentischen Gestalt und Botschaft Jesu Christi zu nähern, hält Schweitzer für gelungen. Lediglich das Werk von Johannes Weiß nimmt er ernst. Während nach Johannes Weiß jedoch nur die Predigt Jesu vom Gedanken des in Kürze bevorstehenden Weltendes und Anbrechens des Gottesreiches bestimmt war, behauptet Schweitzer, dass auch Jesu Handeln durch diese Naherwartung bestimmt werde. Diese Position wird in der Theologie als konsequente Eschatologie bezeichnet. Schweitzer betont den großen Abstand zwischen dem jesuanischen Weltbild und dem Weltbild der eigenen, modernen Zeit. Bedingt durch diesen Abstand begegne uns der Galiläer wieder wie ein Unbekannter, der von Grund auf neu entdeckt werden müsse. Obwohl sich viele spätere Theologen bezüglich der Unmöglichkeit einer authentischen Leben-Jesu-Rekonstruktion auf Schweitzer berufen, war er selbst in dieser Hinsicht weniger pessimistisch als z. B. Rudolf Bultmann.

In den Schlussbetrachtungen zeigt er auf, dass die Leben-Jesu-Forschung Jesus in unsere Zeit habe holen wollen. Er betont, dass dabei viel geleistet worden sei und bezeichnet die Leben-Jesu-Forschung als „einzigartig große Wahrhaftigkeitstat“. Dennoch hat er Kritikpunkte. Die Modernisierungsversuche am Jesusbild scheiterten, weil Jesus sich nicht in unserer Zeit festhalten lasse. Er bleibe in seiner eigenen Zeit und habe seine eigenen Vorstellungen: Er arbeite mit der jüdischen Eschatologie und lebe mit der jüdischen Metaphysik. Das „zeitlich Bedingte in seiner Verkündigung abzuschwächen und umzudeuten“ führe jedoch nicht dazu, dass „er uns dadurch mehr würde“. Allgemein lasse sich keine Person durch historische Betrachtung wieder zum Leben erwecken. Daher sei unser Verhältnis zu Jesus ein mystisches. In Beziehung zu ihm träten wir dadurch, dass wir ein gemeinsames Wollen und Anliegen erkennten und uns selbst in ihm wiederfänden. Wir ließen dabei unseren Willen von seinem klären, bereichern und beleben. Die christliche Religion sei demnach nicht Jesuskult, sondern Jesusmystik. Ähnlich wie bei den Jüngern am See Tiberias (Joh 21) komme Jesus auf uns zu als „ein Unbekannter und Namenloser“. Und er rufe uns zu „Du aber folge mir nach!“„Und denjenigen, welche ihm gehorchen, Weisen und Unweisen, wird er sich offenbaren in dem, was sie in seiner Gemeinschaft an Frieden, Wirken, Kämpfen und Leiden erleben dürfen, und als ein unaussprechliches Geheimnis werden sie erfahren, wer er ist.“

In seiner Untersuchung über Paulus betont Schweitzer dessen mystische Dimension, aus der heraus Paulus nur die Ethik Jesu und die mythologische Dimension seiner Kreuzigung und Wiederauferstehung als Christus beachte und die Parusie-Verzögerung als Aufforderung zur weltweiten Ausbreitung der Lehre Christi als Voraussetzung für den Beginn des Reiches Gottes interpretiere, zumal Christen schon im Diesseits Teil des Reiches geworden seien (z. B. Römerbrief 6, 1–14, Epheserbrief 2,5 ff). Die von Paulus betriebene Bekehrung von Heiden zu Christus mache über den eingeschränkten Kreis der Jünger hinaus die christliche Gemeinde (und später die Kirche) zum eigentlichen Vermächtnis Jesu; seine Kreuzigung sei nicht das Ende, sondern der Anfang der Eschatologie, die durch die zweite Rückkehr des „Gottessohnes“ vollendet werden soll. Sowohl Schweitzers Deutung der Gestalt Jesu als auch seine Sicht von Paulus wurden von der überwiegenden Mehrzahl der zeitgenössischen Theologen abgelehnt.

Albert Schweitzer war ein bekannter Organist, Musikwissenschaftler, Theoretiker des Orgelbaus und einer der für das 20. Jahrhundert stilbildenden Interpreten der Musik Johann Sebastian Bachs.

Schweitzers Ansichten zum Orgelspiel sind von seinen religiösen Vorstellungen nicht zu trennen. So meint er z. B. in Bezug auf die Wiedergabe von Orgelwerken im Konzertsaal:

Als einer der Hauptvertreter der sogenannten Elsässisch-Neudeutschen Orgelreform propagierte Schweitzer seit Anfang des 20. Jahrhunderts gegen die damals in Deutschland üblicherweise gebauten Instrumente einen neuen Orgeltyp: Diese Orgel sollte den ausgewogenen Plenum-Klang der französischen spätromantischen Orgel Cavaillé-Colls, die verschmelzungsfähigen Zungenstimmen der deutschen und englischen Romantik und den Obertonreichtum der alten klassischen Orgeln des Elsass („Silbermann-Orgeln“) miteinander verbinden. Eine neue Spieltischgestaltung sollte die Logik und Übersichtlichkeit der französischen Spielanlage und die in Deutschland gebräuchlichen Spielhilfen vereinen ("Deutsche und französische Orgelbaukunst und Orgelkunst." Leipzig 1906).

Vor allem im Elsass wurden mehrere Orgeln nach Schweitzers Vorstellungen realisiert. Registerreiche Reformorgeln entstanden in St. Reinoldi, Dortmund (1909, V/P 105, 1939 um ein Rückpositiv mit sechs Registern erweitert, 1943/44 zerstört), und Sankt Michaelis, Hamburg (1912, V/P 163, nach Kriegsschäden 1943 durch den Neubau von 1962 ersetzt). Schweitzers Vorstellungen von der Orgel galten nach dem Ende des Zweiten Weltkriegs mit der zunehmenden Bedeutung der Orgelbewegung zunächst als weitgehend überholt. Mit der erneuten Wertschätzung der Orgel des 19. Jahrhunderts, mit der Begeisterung für Orgelbau und Orgelmusik der französischen Spätromantik seit den 1970er Jahren zeigen besonders im deutschsprachigen Raum viele Orgelneubauten, die eine Synthese verschiedener historischer Stilelemente anstreben, eine Nähe zu Schweitzers Vorstellungen. Schweitzer wirkte bewusstseinsbildend für die wachsende Wertschätzung alter Orgeln im frühen 20. Jahrhundert. Auch in der Zeit seines Wirkens in Afrika setzte er sich immer wieder für die Erhaltung historischer Instrumente ein und begleitete Neubauten mit seinem Rat.

Neben der Orgel beschäftigte Schweitzer sich mit dem Geigenbau, genauer mit dem Geigenbogen. Ausgangspunkt war seine Kritik an dem Spiel der mehrstimmigen Passagen in Bachs Solo-Violinsonaten und Suiten für Violoncello solo. Mit dem modernen, steifen, leicht konkaven Bogen lassen sich nur zwei Saiten gleichzeitig zum Klingen bringen. Als Notbehelf wird arpeggiert oder mit Intervallzerlegung gearbeitet, d. h. zunächst werden die unteren beiden, danach die oberen beiden Töne gespielt. Schweitzer störte das Zerbrechen der Akkorde, die damit verbundenen Kratzgeräusche, die Pausen zwischen den Akkorden, das ständige Fortespiel und die unsinnige Stimmführung. Dagegen ging er davon aus, dass vierstimmiges Geigenspiel zu Bachs Zeit auch tatsächlich möglich und üblich war und sah sich in Berichten zum Beispiel über den norddeutschen Musiker und Bachs älteren Zeitgenossen Nicolaus Bruhns bestätigt. Der Schlüssel lag in der Verwendung eines konvexen Bogens, dessen Haare beim Spiel so entspannt werden können, dass ein gleichzeitiges Anstreichen aller Saiten möglich ist. Schweitzer sah die einzige Möglichkeit, das Problem zu lösen, in einer Neukonstruktion; gemeinsam mit dem Geiger Rolph Schröder entwickelte er einen konvexen Bogen mit einer Hebelapparatur am unteren Ende, mit der die Entspannung der Haare beim Spiel möglich war. Er nannte diesen Bogen „Bachbogen“, wohl wissend, dass er damit kein historisches Instrument aus Bachs Zeit, sondern eben eine Neukonstruktion vorgelegt hatte. Heute wird dieser Bogen als Rundbogen bezeichnet. Einige Geiger praktizieren heute dieses Spiel, unter ihnen Rudolf Gähler, der eine Einspielung aller Sonaten und Partiten für Violine solo von Bach und zu diesem Thema auch ein Buch veröffentlicht hat. Der Geiger Philippe Borer wies insbesondere auf die vierstimmigen Werke für Violine solo von Niccolò Paganini hin, die mit einem Rundbogen realisiert werden können. Der Cellist Michael Bach entwarf in den 1990er Jahren einen eigenen Rundbogen zunächst für die zeitgenössische Musik und später auch einen flacheren Rundbogen für die Suiten für Violoncello solo von Bach. An dieser Entwicklungsarbeit beteiligte sich Mstislaw Rostropowitsch. 

Als Bach-Interpret wandte sich Schweitzer gegen die seiner Meinung nach übertriebene dynamische und farbliche Differenzierung des spätromantischen Orgelspiels, wie sie sich in Deutschland und Mitteleuropa seit der Mitte des 19. Jahrhunderts unter dem Einfluss der Liszt-Schule etabliert hatte. Er wurde darin bestärkt durch seine Kenntnis der französischen Tradition des Bach-Spiels und seine Studien bei Charles-Marie Widor, Komponist und Organist an Saint-Sulpice in Paris.

Schweitzer propagierte für die freien Orgelwerke Bachs eine einheitliche, behutsam terrassendynamisch gestaffelte Registrierung. Der Jalousieschweller sollte allenfalls für großräumige Steigerungen und zum Nachzeichnen melodischer Bögen verwendet werden. Der Gebrauch des Registerschwellers (Walze) beim Solovortrag alter Orgelmusik galt Schweitzer als unkünstlerisch. Er vermied als Interpret Extreme. Er wählte und propagierte langsame Tempi, die seiner Ansicht nach die Erfassbarkeit der polyphonen Strukturen gewährleisten, der Aufführungspraxis zu Bachs Zeit entsprächen, und sah die häufige Praxis eines aus seiner Sicht zu schnellen Spiels als erfolglosen Versuch des Ausgleichs mangelnder Plastik des Vortrags. Außerdem praktizierte er eine zurückhaltende Agogik. Die Phrasierung soll nach Schweitzer immer dem jeweiligen Formzusammenhang untergeordnet sein. Er verwirft dabei gleichermaßen ein durchgängiges staccato wie legato.

In Lambaréné spielte Schweitzer nach seiner Arbeit im Hospital auf einem extra für ihn gebauten tropenfesten Klavier mit Orgelpedal. Er übte damit auch für seine Schallplatteneinspielungen und die Orgelkonzerte, deren Erlös seiner karitativen Arbeit zugutekam. Seine Schallplattenaufnahmen mit Werken Bachs in Allhallows Barking-by-the-Tower, London (Dezember 1935), und Sainte-Aurélie, Straßburg (Oktober 1936), sowie an der 1931 nach seinen Vorstellungen gebauten kleinen Orgel der Pfarrkirche in Günsbach (Anfang 1950er-Jahre) mit Werken von Bach, Franck und Mendelssohn liegen in verschiedenen Wiederveröffentlichungen vor.

Schweitzers Orgellehrer Charles-Marie Widor regte auch ein Buch über Johann Sebastian Bach an, durch das die französische Orgelwelt stärker mit der für Bach grundlegenden protestantischen Kirchenmusik und ihrem Wortbezug vertraut gemacht werden sollte ("J. S. Bach, le musicien-poète." Paris u. Leipzig 1905). Widor selbst, Schweitzer freundschaftlich zugetan, verfasste dazu das Vorwort. Er riet auch zu einer deutschen Fassung, woraus durch völlige Neubearbeitung Schweitzers große Bach-Monographie ("Johann Sebastian Bach." Leipzig 1908) entstand, ebenfalls mit einem Vorwort Widors versehen. Während die biographischen Details und die Datierung insbesondere der Kantaten inzwischen durch die Bachforschung weitgehend überholt beziehungsweise erweitert worden sind, ist die Bach-Monographie in musikästhetischer Hinsicht nach wie vor ein Standardwerk von großer geistes- und wissenschaftsgeschichtlicher Bedeutung. Schweitzer hebt besonders den im Werk J. S. Bachs konventionalisierten Gebrauch von Themen und Motiven, Tonarten und Instrumenten hervor. Er hat damit vergleichsweise früh, ohne die Termini zu verwenden, die rhetorische Qualität („Klangrede“) der Alten Musik und die Bedeutung der Affektenlehre thematisiert. Den Schlüssel sah er dabei in den Kantaten. Er fand immer wiederkehrende, sehr bildliche Motive, am auffallendsten bei der Beschreibung von Bewegungen wie etwa Gehen, Laufen, Fallen, Darniedersinken oder bewegungsintensiven Dingen wie Schlangen, Wogen, Schiffe, Flügel, ebenso abstrakte, bestimmte Affekte wie Freude, Trauer, Schmerz oder Lachen, Seufzer, Ächzen, Weinen beschreibende Motive. Schweitzer stellt diese musikalische Sprache systematisch dar und gibt dem Bach-Interpreten Hinweise, wie einzelne Motive zu artikulieren und gestalten seien, um die zugrunde liegenden Bilder herauszuarbeiten. Er zeigt auch, dass zum Beispiel die Orgel-Choralbearbeitungen diese Sprache enthalten und zum Verständnis und zur Darbietung dieser Musik die Kenntnis des Choraltextes gehört.

Ein wichtiger Denkanstoß dürfte Schweitzer von der an sich völlig anders gearteten Leitmotivik Richard Wagners gekommen sein, dessen Musik er sehr schätzte. Allerdings arbeitet er in dem Kapitel „Dichterische und malerische Musik“ seiner Bach-Monographie die grundlegend unterschiedlichen Herangehensweisen der beiden Komponisten beim Umgang mit Themen und Motiven heraus. Bei Wagner und anderen „dichtenden“ Musikern werde versucht, ein dramatisches Geschehen als „ästhetische Ideenassoziationen“ mit der Musik auf die Zuhörer zu übertragen; sie richteten sich mitsamt ihren (Leit-)Motiven an das Gefühl. Bach und andere „malende“ Musiker stellten das Geschehen in Bildern oder aufeinander folgenden Bildern dar. Ihre Motive und Themen wendeten sich an die Vorstellungskraft und die Phantasie der Zuhörer.

Schweitzer war auch Mitherausgeber einer Ausgabe von Bachs Orgelwerken. Die ersten fünf Bände der Bachschen Orgelwerke erschienen 1912/13 in Deutsch, Englisch und Französisch. Herausgeber waren Charles-Marie Widor und Albert Schweitzer. Sie enthalten die Präludien, Toccaten, Fantasien, Fugen, die Canzona und Passacaglia, sowie die Konzerte und Triosonaten. Band VI wurde 1954 veröffentlicht, die Bände VII und VIII folgten erst 1967 nach Schweitzers Tod. Die von Widor und Schweitzer damals angewandten Prinzipien, wie die Ausgabe der Noten als unberührtem Urtext ohne z. B. nachträglich eingefügte dynamische Abstufungen, Fingersätze usw., die genaue Beachtung aller auf Bach selbst zurückgehenden Anordnungen und die Beschränkung der Auffassung der Herausgeber auf das Vorwort, waren für die damalige Zeit ungewöhnliche und zukunftsweisende Prinzipien.

Albert Schweitzer hat versucht, sich möglichst wenig in politische Auseinandersetzungen hineinziehen zu lassen. Dies änderte sich allerdings mit seinem Engagement gegen die atomare Rüstung. Bereits am 14. April 1954 schrieb er einen Leserbrief im "Daily Herald", London, „Die Folgen der Wasserstoffbomben-Explosion bilden ein höchst beängstigendes Problem. … Erforderlich wäre, dass die Welt auf die Warnrufe der einzelnen Wissenschaftler hörte, die dieses furchtbare Problem verstehen. So könnte die Menschheit beeindruckt werden, Verständnis gewinnen und die Gefahr begreifen, in der sie sich befindet.“ Bei der Rede anlässlich der Übergabe des Friedensnobelpreises vom 4. November 1954 in Oslo mit dem Titel "Das Problem des Friedens in der heutigen Welt" äußerte er sich erneut zur Gefahr der Atomrüstung.

Albert Schweitzer wurde von mehreren Freunden, unter anderem Albert Einstein und Otto Hahn, gedrängt, seine Autorität gegen die Atomrüstung einzusetzen. Er zögerte allerdings, weil er sich zunächst nicht kompetent genug fühlte. Endgültig überzeugte ihn dann allerdings der Publizist Norman Cousins. Nachdem er sich intensiv auch mit den wissenschaftlichen Grundlagen der Atomphysik und den Folgen von Atomwaffentests auseinandergesetzt hatte und brieflich und persönlich befreundete Fachleute wie Werner Heisenberg, Frédéric Joliot-Curie und Albert Einstein befragt hatte, sendete er am 23. April 1957 über den Sender Radio Oslo einen „Appell an die Menschheit“. Dieser Appell erfuhr weltweite Aufmerksamkeit und wurde in 140 Sendern übernommen. Am 28., 29. und 30. April 1958 folgten drei weitere Appelle, „Verzicht auf Versuchsexplosionen“, „Die Gefahr eines Atomkrieges“, „Verhandlungen auf höchster Ebene“ die vom Präsidenten des norwegischen Nobelpreiskomitees, Gunnar Jahn vorgelesen wurden. Sie wurden unter dem Titel „Friede oder Atomkrieg“ gedruckt. Schweitzer gehörte 1958 neben Otto Hahn zu den prominentesten Unterzeichnern einer von Linus Pauling initiierten Unterschriftensammlung bei namhaften Wissenschaftlern gegen die Atomversuche. Schweitzer trat auch der 1957 gegründeten amerikanischen Friedensgruppe "National Committee for a sane nuclear policy (SANE)" bei.

Schweitzer wurde für sein Engagement und seine Aussagen neben vielfacher Zustimmung auch heftig angegriffen. Die "Neue Zürcher Zeitung" schrieb am 10. September 1958 unter dem Titel „Seltsamer Albert Schweitzer“: „Der verehrte Name Albert Schweitzers darf nicht davon abhalten, festzustellen, dass dieses Dokument politisch und philosophisch, militärisch und theologisch wertlos ist. Das Wagnis, das er dem Westen zumutet, ist an sich schon ungeheuerlich. Das Urteil über Amerika und die Sowjetunion anderseits macht es vollends unmöglich, Albert Schweitzers Rat ernsthaft in Erwägung zu ziehen.“

Nach dem Abschluss des Versuchsstoppabkommens im Jahr 1963 beglückwünschte Schweitzer John F. Kennedy und Nikita Chruschtschow brieflich zu ihrem „Mut und Weitblick, eine Politik des Friedens einzuleiten“. Allerdings protestierte er im selben Jahr noch einmal öffentlich gegen die nach dem Vertrag weiterhin erlaubten unterirdischen Kernwaffentests.

Ende der 1950er Jahre – ausgehend von dem Publizisten John Gunther (Der Spiegel vom 3. Juli 1957) – wich die Verehrung Schweitzers einer kritischen Bestandsaufnahme seines Hospitals. Diese Kritik wurde damals von Edmund Duboze zurückgewiesen, dem damaligen Generalinspektor des militärärztlichen Dienstes Gabuns. Siegwart-Horst Günther, Mitarbeiter Schweitzers, bezeichnet die Kritik als oberflächlich, subjektiv und gehässig.

Viele kritische Äußerungen richteten sich vordergründig gegen Schweitzers Tätigkeit in Lambaréné, zielten aber offensichtlich auf die Diskreditierung seines öffentlichen Ansehens als Friedensnobelpreisträger im Zusammenhang mit seinem Engagement gegen die Atomrüstung ("Appell an die Menschheit" vom 23. April 1957) und für die Friedensbewegung ab Mitte der fünfziger Jahre. Theodor Heuss, den er noch aus seiner Jugendzeit kannte und den er bei dessen Heirat getraut hatte, beanstandete Schweitzers Briefwechsel mit Walter Ulbricht und die Kontakte mit der DFU.

André Audoynaud, ärztlicher Direktor des Hôpital Administratif in Lambaréné von 1963 bis 1966, kritisierte, Schweitzer habe seine Aufbauleistung übertrieben, da Lambaréné schon in das Kolonialsystem und die Zivilisation eingebunden gewesen sei. Er habe sein Hospital trotz hoher Spenden nicht modernisiert und unelektrifiziert gelassen, unhygienische und krankheitsfördernde Zustände mit der Begründung von Tierliebe geduldet, Symptomkuriererei betrieben und blind das europäische Modell der Krankenversorgung übertragen. Überdies habe er einen kolonialen Führungsstil gepflegt, schwarze Angehörige von Erkrankten zu Fronarbeit gezwungen und geschlagen. Er sei – dem 19. Jahrhundert verhaftet – in Afrika ein Fremder geblieben, habe trotz großer Unterstützung wenig bewirkt, sich aber medienwirksam mit fremden Federn geschmückt.

Diese Kritik wurde erst im Jahre 2005 veröffentlicht; es gibt so gut wie keine Augenzeugen mehr, um die Vorwürfe zu überprüfen. Einzelne Vorwürfe können zudem widerlegt werden: Im dokumentarischen Film „Albert Schweitzer“ bereitet sich ein schwarzer Mediziner auf eine Operation vor. Zumindest im Jahre 1964 war der Operationssaal mit einem Generator versehen und mit elektrischen Operationsleuchten ausgestattet.

In seiner 2009 erschienenen Biographie über Albert Schweitzer bezeichnete ihn der Theologe Nils Ole Oermann als einen „Meister der Selbstinszenierung“ , ohne jedoch die großen Leistungen Schweitzers zu leugnen. Oermanns Schlagwort wurde wenige Jahre später vom Theologen Sebastian Moll aufgegriffen und zu einem eigenen Buchtitel erhoben. Moll stellt den historischen Albert Schweitzer seinem autobiographischen Alter Ego gegenüber und kommt zu dem Ergebnis, dass Schweitzers autobiographische Angaben oft der positiven Inszenierung der eigenen Persönlichkeit dienen.

1964, ein Jahr vor seinem Tode, übertrug Schweitzer die ärztliche Leitung des Spitals dem Schweizer Arzt Walter Munz (* 1933), der von 1961 bis 1971 in Lambaréné arbeitete und später lange Jahre im Stiftungsrat tätig war.

Seit seiner Gründung im Jahre 1913 wurde das Spital viermal (1913 und 1924 in Andende, 1927 und 1981 in Lambaréné) neu aufgebaut, um es den Bedürfnissen der Patienten und dem medizinischen Fortschritt anzupassen.

1961 bestand das Ärzteteam aus einem Japaner, einem Arzt aus Ungarn, einem US-Amerikaner und zwei Schweizern. Die zwölf diplomierten Krankenschwestern kamen aus den Niederlanden, dem Elsass, aus Deutschland, Großbritannien, Schweden und der Schweiz. Vierzig Heilgehilfen, Laboranten, Pflegerinnen und Hilfshebammen stammten aus Afrika und waren in Lambaréné ausgebildet worden. Das Spital war wirtschaftlich, administrativ und technisch selbständig. Neben einem großen Gemüsegarten und Fruchtpflanzungen gab es 250 Schafe und Ziegen, eine Schreinerei, Mechaniker- und Elektrikerwerkstätte, Wäscherei, Küche und Bäckerei. Das am Fluss gelegene Hauptspital bestand aus einem Dorf mit 70 einfachen Holzhäusern mit Wellblechdächern und konnte 470 stationäre Patienten beherbergen. Im nahegelegenen "Village de Lumière" (dem ersten Spital von Lambaréné) konnten 70 Leprapatienten gepflegt werden. Täglich wurden 100 bis 200 Kranke ambulant behandelt. Die Patienten kamen aus Dörfern im Umkreis von 600 Kilometern. Im Sinne von Schweitzers Ethik der Ehrfurcht vor dem Leben wurden in zwanzig Gehegen auch kranke Tiere – Hunde, Schafe, Ziegen, Pelikane, Antilopen und Affen – behandelt.

1991 beherbergte die ganze Spitalsiedlung weit über tausend Menschen, das Hauptspital hatte 226 Betten. Die medizinischen Hauptbereiche kurative und präventive Medizin sowie Ausbildung und medizinische Forschung wurden von einer internationalen Mitarbeiterschaft getragen, von denen der überwiegende Teil aus dem Gabun stammte. Das Spital wird seit 1974 von einer internationalen Stiftung geleitet, in welcher die Gabunesen die Mehrheit haben und in der die wichtigsten unterstützenden Länder vertreten sind.

2015 wurde die vom Schweizer Hilfsverein finanzierte neue Geburtsstation (Maternité) eröffnet.

Nach dem Tode von Albert Schweitzer wurde die "Association Internationale de l’œuvre du Dr. Albert Schweitzer de Lambaréné" (AISL) Erbin des Spitals und leitete es von Europa aus. 1974 wurde das Spital in eine eigene Stiftung überführt, und die AISL machte es sich zur Aufgabe, das geistige Werk und die Philosophie der Ehrfurcht vor dem Leben zu erhalten und weiter zu verbreiten.

Im Wohnhaus von Albert Schweitzer in Günsbach richtete ab 1967 die Mitarbeiterin Alida Silver das Archiv und Museum ein. Heute befinden sich hier 10.000 Briefe Schweitzers und über 70.000 Briefe, die ihm geschrieben wurden. Dazu gehören auch viele Manuskripte seiner veröffentlichten und unveröffentlichten Bücher und Predigten. Alle wichtigen Dokumente sind auf Mikrofilm festgehalten. Ebenso werden Zeitungsausschnitte, Dias, Filme, Tonband- und Videokassetten, Tonbänder und Schallplatten gesammelt, die Reden und Orgelkonzerte Schweitzers oder Berichte über das Spital in Lambaréné festhalten und so Einblick in sein Leben, Wirken und Denken geben.

Alle wichtigen Albert-Schweitzer-Vereinigungen rund um die ganze Welt sind Mitglied in der AISL.

Die Zahl an Einrichtungen und Veranstaltungen, die mit dem Namen "Albert Schweitzer" verbunden sind, ist unüberschaubar. Beispielhaft angeführt sei für den sportlichen Bereich das Albert-Schweitzer-Turnier, ein wichtiges Basketballturnier für Jugendmannschaften aus Europa und Übersee. Der Deutsche Basketball Bund (DBB) spielt in Erinnerung an Albert Schweitzer jedes zweite Jahr im Frühjahr in Mannheim den Dr.-Albert-Schweitzer-Pokal für Jugend-Nationalmannschaften aus. Ebenfalls an Schweitzer knüpft die Albert Schweitzer Stiftung für unsere Mitwelt (ASSfuM) an. Sie ist eine 1999 gegründete deutschlandweit tätige, gemeinnützige Tierschutzorganisation, deren Schirmherr der Philosoph Peter Sloterdijk bis zum Jahr 2013 war. Diese Organisation wird von der Überzeugung angetrieben, dass der Umgang mit Tieren, insbesondere zur Nahrungsmittelproduktion, zu den größten Ungerechtigkeiten weltweit gehört. Die Stiftung setzt sich für bessere Haltungsbedingungen der Tiere ein und wirkt der Überzüchtung (sog. Qualzucht) entgegen.

Die Evangelische Jugend nahm sich Albert Schweitzer in vielfacher Weise zum Vorbild. Die VCP-Stämme in Breitenbach, Lambsheim, Mosbach-Neckarelz und Remagen sind nach ihm benannt.

In Darmstadt ist ihm die Albert-Schweitzer-Anlage gewidmet.

In Tübingen befindet sich eine Albert-Schweitzer-Kirche, die auch eine Albert-Schweitzer-Wand mit Bildern und Texten enthält.

Der Name Albert Schweitzers wird auch für die Namensgebung zahlreicher Schulen verwendet. Die erste deutsche Schule mit seinem Namen war das Gymnasium Albert-Schweitzer-Schule Nienburg in Nienburg/Weser, das den Namen im Jahre 1949 mit Zustimmung Albert Schweitzers erhielt.
In einer Liste der Schulen, die Albert Schweitzers Namen führen, werden 2007 insgesamt 118 deutsche Schulen aufgeführt.

Ende des Zweiten Weltkrieges entstanden in der Schweiz, Österreich und Deutschland Dörfer, die verwaiste, verlassene Kinder und Jugendliche aufnehmen. 1957 folgte in Waldenburg (Baden-Württemberg) die Gründung des ersten Albert-Schweitzer-Kinderdorfs durch Margarete Gutöhrlein. Elternpaare übernahmen die Betreuung; Albert Schweitzer übernahm persönlich die Patenschaft. Ausgehend von dem ersten Kinderdorf entwickelten sich viele Albert-Schweitzer-Kinderdörfer in Deutschland.

Albert Schweitzer war auch das Thema mehrerer Spielfilme:

Die ÖASG wurde 1984 gegründet und ist als Entwicklungshilfeorganisation weltweit sowie als mildtätige Organisation in Österreich tätig. Sie hat nur ehrenamtliche Mitarbeiter und ist von der UNO und UNESCO als NGO anerkannt.

Erstmals am 29. Mai 2011 verliehen an Eugen Drewermann und das Arztehepaar Rolf und Raphaela Maibach in Königsfeld im Schwarzwald, Ort des früheren Wohnhauses Schweitzers, in dem heute das Albert-Schweitzer Museum zu finden ist.

Albert Schweitzers Gedenktag am 4. September ist nicht im offiziellen Evangelischen Namenkalender enthalten.


 Gesammelte Werke 






Der größte Teil des Nachlasses von Albert Schweitzer befindet sich in der Zentralbibliothek Zürich, zunächst seit den 1960er Jahren als Depositum. Mit finanzieller Unterstützung des Lotteriefonds des Kantons Zürich konnte die Zentralbibliothek im Jahr 2009 den Nachlass für eine Million Franken definitiv erwerben. Er umfasst etwa zwölf Laufmeter mit Werkmaterialien, Notizen, Reden, Manuskripten und anderen Dokumenten, die erschlossen und der interessierten Öffentlichkeit zugänglich gemacht werden. Nur die Korrespondenz befindet sich zum größten Teil in der Stiftung Albert-Schweitzer-Zentrum in Günsbach, die Zentralbibliothek besitzt davon aber zahlreiche Kopien. Dass von einer Persönlichkeit des 20. Jahrhunderts der schriftliche Nachlass fast in seiner Gesamtheit an "einem" Ort aufbewahrt wird, ist ein seltener Glücksfall.








</doc>
<doc id="8467" url="https://de.wikipedia.org/wiki?curid=8467" title="Hanoi">
Hanoi

Hanoi (, Hán tự 河內, wörtlich: "Stadt innerhalb der Flüsse") ist die Hauptstadt und nach Ho-Chi-Minh-Stadt die zweitgrößte Stadt Vietnams. Nach der Neugliederung der Verwaltungsgrenzen im Jahr 2008, bei der die gesamte Provinz Hà Tây und Teile weiterer Provinzen Hanoi zugeschlagen wurden, wies die Stadt Ende 2009 rund 6,45 Mio. Einwohner auf.

Hanoi ist die älteste der bestehenden Hauptstädte Südostasiens. Belegt ist sie in ihrem Gründungsjahr 1010 als Zitadelle Thăng Long.

Bereits seit der Bronzezeit besiedelt war die nur wenige Kilometer nördlich des heutigen Stadtzentrums gelegene Zitadelle von Cổ Loa, aus der viele Relikte aus der Dong-Son-Kultur bis ins siebte vorchristliche Jahrhundert hinein nachweisbar sind. Sie wurde 257 v. Chr. von Thục Phán zur Hauptstadt des von ihm gegründeten frühvietnamesischen Königreichs Âu Lạc bestimmt.

Im Jahre 866 errichtete die chinesische Tang-Dynastie zur Konsolidierung ihrer Besatzung am Westufer des Roten Flusses eine Zitadelle namens Đại La, die König Lý Thái Tổ, der Begründer der Lý-Dynastie, im Jahre 1010 zu seiner Residenzstadt auserwählte und „Thăng Long“ (Hán tự: 昇龍, „aufsteigender Drache“) nannte.

Zahlreiche Sagen und Legenden umranken Hanois Geschichte.

Im Laufe der Jahrhunderte wurde Hanoi wiederholt von Invasoren erobert, verlor dabei auch seinen Status als Hauptstadt und wurde mehrfach umbenannt.

Während der Ho-Dynastie (1400–1407) trug die Stadt den Namen Đông Đô (östliche Hauptstadt), während der Besetzung durch die chinesische Ming-Dynastie hieß sie Đông Quan (östliches Tor). Die Le-Könige benannten sie 1430 wieder in Đông Kinh (östliche Hauptstadt) um; als die Niederländische Ostindien-Kompanie im 17. Jahrhundert hier eine Handelsniederlassung einrichtete, gelangte dieser Name als Tongking ins europäische Schrifttum.

Während der Nguyen-Dynastie (1802–1945) verlor Hanoi seinen Status als Hauptstadt und musste diesen an Huế abtreten, blieb jedoch administratives Zentrum des Nordens.

Da der Drache als Symbol der kaiserlichen Macht der Hauptstadt Huế vorbehalten bleiben sollte, wurde die Stadt abermals umbenannt.
Der Nguyen-Kaiser Minh Mạng (1820–1841) gab ihr 1831 ihren heutigen Namen Hà Nội (Hán Nôm: 河内) – die „Stadt innerhalb der Flüsse“, der nichts weiter als eine geografische Lage bezeichnet.
1873 wurde Hanoi von den Franzosen erobert. Von 1883 bis 1945 war die Stadt Verwaltungszentrum der Kolonie Französisch-Indochina. Die Franzosen errichteten südlich von Alt-Hanoi eine moderne Verwaltungsstadt, legten breite, rechtwinklig zueinander liegende, baumgesäumte Alleen mit Oper, Kirchen, öffentlichen Bauten und Luxusvillen an, zerstörten aber auch große Teile der Stadt, schütteten Seen und Kanäle zu oder verkleinerten diese; Kaiserpaläste und Zitadelle mussten ebenfalls weichen.

Von 1940 bis 1945 war Hanoi, wie auch der größte Teil von Französisch-Indochina und Südostasiens, japanisch besetzt. Am 2. September 1945 rief Ho Chi Minh hier die Demokratische Republik Vietnam (Nordvietnam) aus. Die vietnamesische Nationalversammlung beschloss am 6. Januar 1946, Hanoi zur Hauptstadt der Demokratischen Republik Vietnam zu machen.

Zwischen 1946 und 1954 war die Stadt Schauplatz heftiger Kämpfe zwischen Franzosen und den Việt Minh (Indochinakrieg). Während des Vietnamkrieges wurde Hanoi von den Amerikanern bombardiert; die ersten Bombenangriffe erfolgten 1966, die letzten Ende 1972. Allein zum Weihnachtsfest 1972 trafen 40.000 t Sprengstoff die Stadt und zerstörten sie zu 25 Prozent.
Seit der Wiedervereinigung des Landes 1976 ist Hanoi Hauptstadt von ganz Vietnam.

Die Stadt liegt am fruchtbaren Delta des Roten Flusses "(Sông Hồng)," etwa 100 km von dessen Mündung in den Golf von Tonkin entfernt.

Das Klima ist subtropisch-monsunal mit feucht-heißen Sommern und mild-trockenen Wintern. Die Jahresniederschlagsmenge beträgt 1.682 mm; acht Monate sind humid, vier arid.

Hanoi ist direkt der Zentralregierung unterstellt und verwaltungstechnisch einer Provinz gleichgestellt.

Verwaltungsgliederung

Zum 1. August 2008 wurde Hanoi erweitert. Die Provinz Hà Tây sowie der Bezirk Mê Linh der Provinz Vĩnh Phúc und die Bezirksteile Đông Xuân, Tiến Xuân, Yên Bình und Yên Trung des Provinzbezirks Lương Sơn (Provinz Hòa Bình) wurden dem Verwaltungsgebiet Hanoi hinzugefügt. Durch diese Erweiterung hat sich die Fläche verdreifacht, womit Hanoi heute zu den größten Hauptstädten der Welt gehört.

Die Stadt ist in die folgenden 29 Distrikte untergliedert:


Städtepartnerschaften einzelner Stadtbezirke

Kooperations- und Freundschaftsabkommen

Der Flughafen Hanoi (Nội Bài International Airport) ist der zweitgrößte Flughafen in Vietnam. Über die Nord-Süd-Schnellstraße ist Hanoi mit Giang verbunden.

Vom Hauptbahnhof "(Ga Hà Nội)" fahren meterspurige Züge nach Lào Cai, Đồng Đăng (Grenzübergang nach China, Provinz Lạng Sơn, durch die ca. 10 km davor gelegene gleichnamige Provinzhauptstadt), Hải Phòng und Ho-Chi-Minh-Stadt. Vom Bahnhof "Gia Lâm" verkehrt ein täglicher normalspuriger Nachtreisezug nach Nanning (China).

Der öffentliche Personennahverkehr wird heutzutage ausschließlich mit Omnibussen betrieben. Ein U-Bahn-System mit fünf Linien ist in der Bauphase und soll bis 2020 vollständig in Betrieb genommen werden.



Es gibt ein mathematisches Knobel- und Geduldsspiel namens „Türme von Hanoi“.

Halle-Neustadt wird im Volksmund in ironischer Anspielung auf die vietnamesische Hauptstadt auch "HaNeu" genannt.




</doc>
<doc id="8468" url="https://de.wikipedia.org/wiki?curid=8468" title="XPath">
XPath

Die XML Path Language (XPath) ist eine vom W3-Konsortium entwickelte Abfragesprache, um Teile eines XML-Dokumentes zu adressieren und auszuwerten. XPath dient als Grundlage einer Reihe weiterer Standards wie XSLT, XPointer und XQuery. XPath ist derzeit in der Version 3.1 vom 21. März 2017 standardisiert.

In Webbrowsern, XSLT-Prozessoren und anderer Software wird oft nur die XPath-Version 1.0 aus dem Jahr 2007 unterstützt.

Ein "XPath"-Ausdruck adressiert Teile eines XML-Dokuments, das dabei als Baum betrachtet wird, wobei einige Unterschiede zum „klassischen“ Baum der Graphentheorie zu beachten sind:
Ein "XPath"-Ausdruck setzt sich aus einem oder mehreren Lokalisierungsschritten "(Location Steps)" zusammen. Sie werden mit dem Zeichen codice_5 getrennt.

Ein "Lokalisierungsschritt" codice_6 besteht aus:
Beliebig viele XPath-Ausdrücke lassen sich mit dem Pipe-Zeichen | mengenmäßig vereinigen.

Es gibt stets verschiedene Möglichkeiten, eine gesuchte Knotenmenge in XPath auszudrücken.

XPath operiert auf der logischen Dokumentenstruktur. Das bedeutet zum Beispiel, dass man Entitäten schon geparst vorfindet oder dass auch eventuelle Standard-Attribute und -Knoten, die durch ein Schema vorgegeben werden, schon im Baum enthalten sind.

Durch Angabe von Achsen wird ausgehend vom aktuellen Kontextknoten in der Baumstruktur des XML-Dokuments navigiert.

Wird dabei vom Dokument-Knoten (der Wurzel des XML-Dokuments) ausgegangen, wird dem XPath-Ausdruck das Zeichen codice_5 vorangestellt.

Dieser Baum visualisiert beispielhaft die Struktur eines XML Dokuments

Die fünf Achsen codice_8, codice_9, codice_10, codice_1 und codice_2 bilden ausgehend von einem beliebigen Knoten den Dokumentbaum vollständig und überlappungsfrei ab.

Knotentests (geschrieben codice_13) schränken die Elementauswahl einer Achse ein:

Durch Angabe von Prädikaten kann das Ergebnis weiter eingeschränkt werden. Prädikate werden in eckige Klammern eingeschlossen und können in beliebiger Zahl hintereinander geschrieben werden, wobei die Reihenfolge wesentlich ist. Prädikate können XPath-Ausdrücke enthalten, außerdem kann eine Vielzahl von Funktionen und Operatoren verwendet werden. Die sind zum Beispiel:


Beispiele:
(das gleiche leistet codice_33)

Gegeben sei folgendes XML-Dokument:

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<dok>
</dok>
Beispiele für XPath-Ausdrücke:
XPath-Visualisierer helfen, die mitunter komplizierten XPath-Abfragen auf konkrete XML-Dateien anzuwenden.





</doc>
<doc id="8469" url="https://de.wikipedia.org/wiki?curid=8469" title="Mehrheitswahl">
Mehrheitswahl

Eine Mehrheitswahl ist ein Repräsentationsprinzip mit dem Ziel, eine parlamentarische Regierungsmehrheit für eine Partei herbeizuführen. Es bezeichnet ein Verfahren zur Auswahl eines Vorschlages aus einer Reihe vorgegebener Alternativen durch die Mehrheit einer Gruppe von Wählern. Somit zeichnet sie sich als ein Verfahren zur direkten Wahl von Repräsentanten aus.

Die Mehrheitswahl ist insbesondere von der Verhältniswahl abzugrenzen und in der Regel als Persönlichkeitswahl ausgestaltet.

Mehrheitswahlen können sowohl in Wahlkreisen, in denen nur eine Person pro Vorschlag gewählt wird, als auch in solchen, in denen mehrere bis alle "(Einheitswahl)" Personen in einem Vorschlag gewählt werden, vonstattengehen.

Bei der relativen Mehrheitswahl ist der Vorschlag oder Kandidat gewählt, der die meisten Stimmen erhält. Davon profitieren in der Regel Parteien mit regionalen Hochburgen und Regionalparteien überproportional. Durch eine Anwendung dieses Typs der Mehrheitswahl bilden sich oft Zweiparteiensysteme heraus, z. B. in den USA. Ebenso im Vereinigten Königreich von Großbritannien und Nordirland, da aber mit der zusätzlichen Ausbildung von regional starken Parteien.

Ausgenommen davon sind gewählte Direktkandidaten, die auch nur als Unabhängige vorgesehen sind, so z. B. auch für den Allgemeinen Nationalkongress in Libyen.

Bei der absoluten Mehrheitswahl ist der Vorschlag gewählt, der mehr als die Hälfte der Stimmen erhält. Um in allen Fällen die erforderliche Mehrheit zu erreichen, wird oft eine Stichwahl durchgeführt, bei der nur die beiden besten Kandidaten des ersten Durchgangs zugelassen werden. Dies findet z. B. Anwendung bei den meisten Bürgermeister- und Landratswahlen in Deutschland. Aber nicht bei der Bürgermeisterwahl in Baden-Württemberg – da können im zweiten Wahlgang sogar noch neue Bewerber aufgestellt werden.

Eine Alternative hierzu ist die integrierte Stichwahl, bei der die Wähler die Vorschläge nach Präferenz nummerieren. Auf diese Weise werden u. a. die Mitglieder der beiden Kammern des Parlaments von Australien gewählt.

Als romanische Mehrheitswahl bezeichnet man eine Mehrheitswahl, bei der in bis zu zwei Wahldurchgängen gewählt wird. Wer im ersten Wahlgang die absolute Mehrheit der Stimmen auf sich vereint, ist gewählt. Trifft dies auf keinen der Kandidaten zu, findet ein zweiter Wahlgang "(„Neuwahl“, „Wiederholungswahl“)" statt, bei dem der Kandidat mit den meisten Stimmen gewählt ist. Wer an diesem zweiten Wahlgang teilnehmen darf, ist unterschiedlich geregelt. Ein solches Verfahren wird z. B. bei den Wahlen zur Französischen Nationalversammlung angewendet. 

Die Klassifizierung dieser Mehrheitswahl als relative oder absolute Mehrheitswahl ist umstritten, da Elemente beider auffindbar sind und dennoch einige spezifische Eigenschaften resultieren.

Es können auch mehrere Bewerber in einem Wahlkreis nach Mehrheitswahl gewählt werden.

Üblicherweise hat der Wähler hierbei so viele Stimmen, wie Sitze zu vergeben sind, und kumulieren ist nicht möglich. In fast allen Kantonen der Schweiz werden die Regierungsmitglieder nach absoluter Mehrheitswahl durch das Volk gewählt, wobei hierfür jeweils der ganze Kanton den Wahlkreis bildet und die Wähler so viele Stimmen haben, wie Regierungsmitglieder zu wählen sind. Bei relativer Mehrheitswahl sind bei "n" zu vergebenen Sitzen die "n" Bewerber mit den meisten Stimmen gewählt. Bei absoluter Mehrheitswahl kann bei Mehrpersonenwahlkreisen die absolute Mehrheit unterschiedlich definiert werden.

Auch möglich ist eine Mehrheitswahl in Mehrpersonenwahlkreisen mit bloß einer Stimme. Hierbei können entweder mehrere Bewerber zusammen gewählt werden, z. B. die Wahlmänner bei US-Präsidentschaftswahlen, oder es wird bloß ein Bewerber mit der Stimme gewählt, in diesem Fall spricht man auch von nicht-übertragbarer Einzelstimmgebung.

Wird nur ein Abgeordneter im Wahlkreis gewählt, kann man die relative Mehrheitswahl auch als Verhältniswahl betrachten. Im umgekehrten Fall werden Verhältniswahlen in besonders kleinen Wahlkreisen auch als Mehrheitswahlen betrachtet, da sie dem gleichen Ziel wie diese dienen. Die sogenannte faktische Hürde ist dann oft sehr hoch.

Zu einer Einheitswahl kann die Verhältniswahl dann werden, wenn die Sperrklausel sehr hoch angesetzt wird.

Der Einsatz der Mehrheitswahl kann auch mit dem der Verhältniswahl kombiniert werden. Bei der personalisierten Verhältniswahl gibt es eine integrierte Mehrheitswahl, die aber auf das Stimmenverhältnis im Parlament keine Auswirkungen hat. Beim Grabenwahlrecht dagegen wird ein Teil der Abgeordneten durch Mehrheitswahl und unabhängig davon der andere Teil durch Verhältniswahl bestimmt.

Das sogenannte "minderheitenfreundliche Mehrheitswahlrecht" sieht vor, dass die stimmenstärkste Partei automatisch einen gewissen Mindestanteil der Parlamentssitze zugesprochen bekommt. Ansonsten ist es eine Verhältniswahl.

Im Vereinigten Königreich werden die Mitglieder des Unterhauses nach relativer Mehrheitswahl gewählt. Dieser Typ hat seinen Ursprung im angelsächsischen Raum und ist heute nur noch dort verbreitet. Da alle Stimmen bis auf die des Gewinners verfallen, wird dieses Wahlverfahren auch "winner-takes-all" oder "first-past-the-post system" genannt.

In Frankreich wird bei Nationalversammlungswahlen ein romanisches Mehrheitswahlrecht angewandt. Um im ersten Wahlgang gewählt zu sein, muss eine absolute Mehrheit der abgegebenen Stimmen sowie die Stimmen von 25 % der Wahlberechtigten erreicht worden sein. Am möglichen zweiten Wahlgang darf neben den beiden Bestplatzierten des ersten Wahlganges teilnehmen, wer die Stimmen von mehr als 12,5 % der Wahlberechtigten erhalten hat. Bei Präsidentschaftswahlen wird nach absoluter Mehrheitswahl gewählt.

Die Mitglieder des Kongresses der Vereinigten Staaten (Repräsentantenhaus und Senat) und der meisten Parlamente der Bundesstaaten werden in Einerwahlkreisen gewählt, wobei die genaue Ausgestaltung der Gesetzgebung der Bundesstaaten unterliegt und der Wahlkreis der Senatoren immer einen ganzen Bundesstaat umfasst. Bei der Wahl des US-Präsidenten durch das Wahlmännerkollegium fallen in den meisten Bundesstaaten die jeweiligen Wahlmänner ebenso gemäß dem Mehrheitswahlrecht dem stimmenstärksten Kandidaten im jeweiligen Staat zu.

In Deutschland gilt als Bundestagswahlrecht ein personalisiertes Verhältniswahlrecht. Zwar werden in den Wahlkreisen auch Direktkandidaten nach dem relativen Mehrheitswahlrecht gewählt (die Hälfte der Bundestagssitze). Parteilose Direktkandidaten hatten seit der Bundestagswahl 1949 gegen die parteiunterstützten Kandidaten jedoch keine Chance mehr.

Im Gegensatz zu vielen anderen Verfassungen schreibt das Grundgesetz kein konkretes Wahlsystem vor. Dies ist der Tatsache geschuldet, dass sich die verschiedenen Parteien im Parlamentarischen Rat nicht auf eine dauerhafte Lösung verständigen konnten. Nachdem bereits in den 1950er-Jahren die Einführung eines Grabenwahlrechts diskutiert worden war, wollte die Große Koalition (1966–1969) ein Mehrheitswahlrecht einführen. Diese Wahlrechtsreform war eines der Reformprojekte, um derentwillen die Koalition gebildet worden war. Das Vorhaben wurde insbesondere von der CDU unterstützt, die auf diese Weise unabhängig von der FDP werden wollte, die im damaligen Dreiparteiensystem die Richtung der Politik bestimmen konnte. Die SPD war zunächst bereit, eine solche Reform zu unterstützen, rückte aber später davon ab, da die FDP eine sozialliberale Koalition ins Spiel gebracht hatte. Bundesinnenminister Paul Lücke (CDU) trat daraufhin von seinem Amt zurück. Helmut Schmidt (SPD), der zu dieser Zeit Fraktionsvorsitzender der SPD im Bundestag war, gab als einer der wenigen in seiner Partei die damalige Forderung nicht auf. Vertreter der Mehrheitswahl an den Universitäten waren unter anderem die Politologen Ferdinand A. Hermens und Wilhelm Hennis.

Nachdem die Linkspartei 2007 erstmals in ein westdeutsches Parlament einzog, wurde vereinzelt erneut ein Mehrheitswahlrecht für Deutschland gefordert. Unabdingbare Kompromisse würden eine klare, eindeutige und sinnvolle Politik verhindern, so die Argumentation der Reform-Befürworter. Dies sei ein großer Schaden für Deutschland. Unter anderen forderte Ernst Benda die Einführung des Mehrheitswahlrechts in Deutschland.

Nach der Nationalratswahl 2006 forderten einige prominente Politiker in Österreich, unter ihnen auch Landeshauptmann Erwin Pröll, die Einführung eines Mehrheitswahlrechtes bei Wahlen zum Nationalrat mit dem Ziel, klare Mehrheiten zu schaffen und große Koalitionen weniger häufig zu machen.

In einem Zwischenentwurf zu einem veränderten Parteiprogramm der ÖVP „denkt die Partei in ihrem „Evolutionsprozess“ die Einführung des Mehrheitswahlrechts an.“

In der Schweiz ist die Wahl nach romanischer Mehrheitswahl üblich. Dies gilt für die Wahl des Ständerates (außer in den Kantonen Jura und Neuenburg), einige kantonale Parlamente und Regierungen sowie Gemeinderäte. 

In den Kantonen, die nur einen Vertreter in den Nationalrat entsenden, wird dieser mit relativer Mehrheit bestimmt.

Bundesratswahlen werden mit einer modifizierten absoluten Mehrheitswahl durchgeführt.

In Italien wird für die Wahlen zum Italienischen Abgeordnetenhaus ein minderheitenfreundliches Mehrheitswahlrecht angewandt, wobei die stimmenstärkste Partei 54 % der Sitze erhält. Gleiches gilt in jeder Region einzeln für die Wahl des Senats, wodurch die Mehrheitsverhältnisse verzerrt werden und wiederum nur zufälligerweise stabilere Mehrheiten zustandekommen als bei einer reinen Verhältniswahl.

In Indien hat sich kein Zweiparteiensystem herausgebildet, weil sich dort die regionalen Besonderheiten stark auswirken.

In der Regel ist eine Personenwahl in den Wahlkreisen möglich. Die Wähler haben die Möglichkeit, Kandidaten ihres Wahlkreises persönlich kennenzulernen und aufgrund ihrer Persönlichkeit zu wählen.


Das Mehrheitswahlrecht tendiert zu einem Zweiparteiensystem (Duvergers Gesetz).





Die Mehrheitswahl führt häufig zu eindeutigen Mehrheitsverhältnissen im Parlament.


Es ist möglich, das Wahlergebnis durch „geschicktes“ Ziehen der Wahlkreisgrenzen zu beeinflussen („Gerrymandering“, „Wahlkreisgeometrie“):

Ein Teil der Bevölkerung kann "de facto" seines Wahlrechts beraubt werden, wenn er in einem Wahlkreis oder -bezirk lebt, der fest in der Hand einer der beiden Parteien ist, und somit keine Chance hat, auf das Wahlresultat Einfluss zu nehmen. So leben z. B. in den USA 80 % der Bevölkerung in einem fest einem Lager zugerechneten Bundesstaat.

Ein Merkmal ist die Abhängigkeit des Wahlausgangs von irrelevanten Alternativen. Die amerikanische Präsidentschaftswahl 2000 wird von vielen als Beispiel dafür angesehen. Es wird argumentiert, dass der Demokrat Al Gore die Wahl gegen den Republikaner George W. Bush deswegen verloren habe, weil viele links-orientierte Wähler für Ralph Nader, einen von den Grünen nominierten Kandidaten ohne Aussicht auf Erfolg, gestimmt hatten. Ohne diese Alternative hätten sie wahrscheinlich Gore gegenüber Bush vorgezogen und ersterem zum Sieg verholfen.

Die Abhängigkeit der Mehrheitswahl von irrelevanten Alternativen verleitet zu strategischem Wahlverhalten.

Bei Wahlen, bei denen es nur einen Sieger geben kann und dieser direkt gewählt wird (z. B. der amerikanische oder französische Präsident) kann es stark vom Auszählungsmodus abhängen, welcher Kandidat gewinnt. Das folgende Beispiel nach Michel Balinski soll dies verdeutlichen:
Für eine Auszählung nach der Wahl durch Zustimmung und der Rang-Wahl müssten vom Wähler weitere Entscheidungen verlangt werden. Geht man davon aus, dass bei einer Wahl durch Zustimmung jeder Wähler seinen ersten beiden Kandidaten zustimmen würde, läge – zumindest nach einem ersten Wahlgang Kandidat B mit 49 Punkten knapp vor Kandidat E mit 48 Punkten.





</doc>
<doc id="8470" url="https://de.wikipedia.org/wiki?curid=8470" title="Mehrheit">
Mehrheit

Mehrheit, auch Majorität oder Mehrzahl, bezeichnet die überwiegende Anzahl aus einer Anzahl von Menschen oder Dingen. Vorwiegend wird das Wort benutzt, um Regeln für Wahlen und Abstimmungen zu formulieren, wobei insbesondere beschrieben wird, "welche" Art von Mehrheit notwendig ist, damit eine Entscheidung Gültigkeit erlangt.

Mit "Mehrheit" wird der größte oder der überwiegende Anteil ("oder Teilhabe") an einer Sache oder bei einer Entscheidungsfindung bezeichnet. Sie ist von grundlegender Bedeutung bei demokratischen Entscheidungen in Form von Wahlen und Abstimmungen, aber gibt auch im Sachenrecht ("unter anderem bei Aktienrecht, Wohneigentum, Erbrecht") Auskunft über die Eigentumsverteilung.

Das Vorliegen einer Mehrheit bei einer Abstimmung oder Wahl gilt als ausreichende Zustimmung für das Zustandekommen einer Entscheidung. Die etwaige Definition basiert in der Regel auf mündlichen Vereinbarungen oder ist in Satzungen, Verordnungen, Gesetzen oder auch Verfassungen geregelt. Dieser Artikel beschreibt die zugrunde liegende Klassifikation, die auch in anderen Bereichen Anwendung findet.

Diese Klassifikation kann in mehreren Ebenen erfolgen, wobei sich die verschiedenen Arten miteinander kombinieren lassen.
Man unterscheidet dabei

Durch historische kulturelle und regionale Prägung kann sich die Auffassung zu einzelnen der nachfolgenden Begriffe unterscheiden.


Folgerung: Jede absolute Mehrheit ist auch eine einfache, jede einfache Mehrheit ist auch eine relative.

Im juristischen Bereich wird auf Grund der teilweisen Unschärfe der Begrifflichkeiten auf die oben genannten Attribute weitgehend verzichtet. Stattdessen wird auf Formulierung wie diese zurückgegriffen:

Bei einer Abstimmung hat derjenige Vorschlag mit relativer Mehrheit gewonnen, der die meisten Stimmen auf sich vereint. Beispielhaft ist das Auftreten dieses Prinzips bei den Wahlkreisbewerbern bei der Bundestagswahl. Der Kandidat mit den meisten Stimmen zieht direkt in den Bundestag ein. Kommt es zur Stimmengleichheit zwischen mehreren Vorschlägen, ist die relative Mehrheit formal von keinem ebendieser erreicht. Im Bundeswahlrecht entscheidet sodann das Los.

Mathematisch gesprochen handelt es sich also um diejenige Teilmenge, wobei diese immer die Zustimmung zu einem Vorschlag beziehungsweise die Gesamtheit der Nein-Stimmen bezeichnen sollen, der Grundmenge (aller gültigen Stimmen ohne Berücksichtigung von Enthaltungen), die die meisten Elemente enthält.

Die relative Mehrheit stellt die geringste Anforderung für die Annahme eines Vorschlages dar. Stimmenthaltungen bleiben ohne Wirkung, mit „Nein“ zu stimmen kann zugelassen werden – etwa bei der Besetzung von Vorstandspositionen in Vereinen oder Parteien, wenn oft nur eine Person zur Wahl steht, nicht jedoch bei der Bundestagswahl. Bei relativen Mehrheitswahlen wird oftmals ein Vorschlag gewählt, der nicht von der Mehrheit der Abstimmenden oder der Stimmberechtigten getragen wird. Damit werden Stichwahl-Verfahren häufig begründet.

Bei der "relativen" Mehrheit sind alle nicht für den gewählten Kandidaten abgegebenen Stimmen „verloren“. Ein bekanntes Beispiel sind die Wahlen zu Elternvertretungen in Schulen, wo derjenige, der die meisten Stimmen, auch wenn es nicht mehr als die Hälfte der abgegebenen Stimmen sind, die Wahl gewonnen hat (z. B. § 64 Abs. 2 Schulgesetz NRW: „Gewählt ist, wer die meisten Stimmen erhalten hat.“). Dieser Problematik soll etwa das Instant-Runoff-Voting Rechnung tragen.

Bei einer Abstimmung hat derjenige Vorschlag mit einfacher Mehrheit gewonnen, der mehr Stimmen als alle anderen Vorschläge zusammen auf sich vereint.

Mathematisch gesprochen handelt es sich also um diejenige Teilmenge, wobei diese immer die Zustimmung zu einem Vorschlag beziehungsweise die Gesamtheit der Nein-Stimmen bezeichnen sollen, der Grundmenge (aller (gültigen) Stimmen ohne Berücksichtigung von Enthaltungen), die mehr als die Hälfte der Elemente der Grundmenge enthält.

Stimmenthaltungen finden somit auch bei der Frage nach einer einfachen Mehrheit keine Berücksichtigung. Die Problematik, dass Vorschläge gewinnen, die nicht von der Mehrheit der Abstimmenden oder der Stimmberechtigten getragen werden, bleibt ebenso bestehen.

Die Bezeichnung "einfache Mehrheit" wird mitunter auch für relative Mehrheiten benutzt, wodurch es zu Irrtümern kommen kann. Tatsächlich sind beide nur im Falle zweier Abstimmungsalternativen identisch.

Bei einer Abstimmung hat derjenige Vorschlag mit einer qualifizierten Mehrheit gewonnen, der mehr als einen zuvor festgelegten Anteil (Quorum) der Grundmenge auf sich vereint. Das Quorum liegt in der Regel bei 50 % und mehr, kann aber auch darunter liegen, wenn etwa alle Bewerber um ein Mandat, die das Quorum als Schwelle überschreiten, in das zu wählende Gremium einziehen.
Die Grundmenge kann dabei entweder alle (gültigen) abgegebenen Stimmen (sogenannte "einfach qualifizierte Mehrheit") oder alle Stimmrechte (sogenannte "absolut qualifizierte Mehrheit") umfassen.

"Beispiel: Zwei-Drittel-Mehrheit (Quorum=2/3) der abgegebenen Stimmen"

Anmerkung: Wenn Stimmenthaltungen z. B. entsprechend § 32 BGB keine Berücksichtigung finden, sind sie auch nicht bei den abgegebenen Stimmen zu berücksichtigen. Sie werden wie nicht anwesende Mitglieder behandelt. Im Beispiel sind es dann eben nicht 598 abgegebene Stimmen, sondern "nur" 588 abgegebene Stimmen. Es sind dann mindestens 392 Stimmen erforderlich.

"Beispiel: Zwei-Drittel-Mehrheit (Quorum=2/3) der Stimmberechtigten"

Anmerkung: Auch hier sind es nur 588 abgegebene Stimmen ("s. o.")
In beiden Beispielen ist die jeweils geringste Anzahl an Zustimmenden für eine Annahme des Vorschlages angegeben.

Prinzipiell ist es auch möglich beim Einsatz von Beteiligungsquoren, die sich also ihrer Grundmenge nach auf die Stimmrechte und nicht auf die Zustimmung, sondern auf die Abgabe der Stimmen beziehen, von einer qualifizierten Mehrheit zu sprechen.

Bei der absoluten Mehrheit handelt es sich um eine besondere qualifizierte Mehrheit (siehe oben) mit Quorum >50 %.

Zum einen können die verschiedenen Arten sequenziell, das heißt in Abfolge, genutzt werden. Dann wird, wie bei der deutschen Bundeskanzler- und Bundespräsidentenwahl, zum Beispiel in den ersten beiden Wahlgängen eine absolute Mehrheit verlangt und im dritten nur die relative. Nur bei Stimmengleichheit beziehungsweise bei Überwiegen der Neinstimmen können weitere Wahlgänge nötig werden.

Andererseits ist es aber auch möglich, verschiedene Mehrheiten auf verschiedene Grundmengen gestützt gleichzeitig zu verlangen.

Beispiel – Doppelte Mehrheit

Der Vertrag von Nizza verlangt für Entscheidungen im Rat der Europäischen Union:

Für bestimmte Volksabstimmungen in der Schweiz gilt das Ständemehr. Dies bedeutet, dass eine Vorlage nur dann angenommen wird, wenn sie:

In der juristischen Fachsprache spricht man, wenn eine Gruppe von etwas betroffen ist oder etwas tut, von Personenmehrheiten (Pluralitäten). Im deutschen Strafrecht ist beispielsweise eine Beleidigung von Personenmehrheiten im Allgemeinen (in Juristendeutsch: grundsätzlich) nicht möglich.





</doc>

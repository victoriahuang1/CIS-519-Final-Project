<doc id="3961" url="https://de.wikipedia.org/wiki?curid=3961" title="Prädikatenlogik">
Prädikatenlogik

Die Prädikatenlogiken (auch Quantorenlogiken) bilden eine Familie logischer Systeme, die es erlauben, einen weiten und in der Praxis vieler Wissenschaften und deren Anwendungen wichtigen Bereich von Argumenten zu formalisieren und auf ihre Gültigkeit zu überprüfen. Auf Grund dieser Eigenschaft spielt die Prädikatenlogik eine große Rolle in der Logik sowie in Mathematik, Informatik, Linguistik und Philosophie.

Gottlob Frege und Charles Sanders Peirce entwickelten unabhängig voneinander die Prädikatenlogik. Frege entwickelte und formalisierte sein System in der 1879 erschienenen Begriffsschrift. Ältere logische Systeme, zum Beispiel die traditionelle Begriffslogik, sind hinsichtlich ihrer Ausdrucksstärke echte Teilmengen der Prädikatenlogik. Sie lassen sich vollständig in diese übersetzen.

Prädikatenlogik ist eine Erweiterung der Aussagenlogik. In der Aussagenlogik werden zusammengesetzte Aussagen daraufhin untersucht, aus welchen einfacheren Aussagen sie zusammengesetzt sind. Zum Beispiel besteht die Aussage „Es regnet oder die Erde ist eine Scheibe“ aus den beiden Aussagen „Es regnet“ und „Die Erde ist eine Scheibe.“ Diese beiden Aussagen lassen sich ihrerseits nicht in weitere Teilaussagen zerlegen – sie werden deshalb atomar oder elementar genannt. In der Prädikatenlogik werden atomare Aussagen hinsichtlich ihrer inneren Struktur untersucht.

Ein zentrales Konzept der Prädikatenlogik ist das "Prädikat". Ein Prädikat ist eine Folge von Wörtern mit Leerstellen, die zu einer wahren oder falschen Aussage wird, wenn in jede Leerstelle ein Eigenname eingesetzt wird. Zum Beispiel ist die Wortfolge „… ist ein Mensch“ ein Prädikat, weil durch Einsetzen eines Eigennamens – etwa „Sokrates“ – ein Aussagesatz, zum Beispiel „Sokrates ist ein Mensch“, entsteht. Die Aussage „Die Erde ist eine Scheibe“ lässt sich prädikatenlogisch in den Eigennamen „die Erde“ und das Prädikat „… ist eine Scheibe“ zerlegen. Anhand der Definition und der Beispiele wird klar, dass der Begriff „Prädikat“ in der Logik, speziell in der Prädikatenlogik, nicht dieselbe Bedeutung hat wie in der traditionellen Grammatik, auch wenn historisch und philosophisch ein Zusammenhang besteht. Statt eines Eigennamens kann in das Prädikat auch eine Variable eingesetzt werden, wodurch das Prädikat zu einer Satzfunktion wird: φ=„"x" ist ein Mensch“ ist eine Funktion, die in der klassischen Prädikatenlogik für die Eigennamen derjenigen Individuen, die Menschen sind, den Wahrheitswert "wahr" ausgibt und für alle anderen den Wahrheitswert "falsch".

Das zweite charakteristische Konzept der Prädikatenlogik ist der "Quantor". Quantoren geben an, von wie vielen Individuen des Diskursuniversums eine Satzfunktion erfüllt wird. Ein Quantor "bindet" die Variable einer Satzfunktion, so dass wieder ein Satz entsteht. Der Allquantor sagt aus, dass ein Prädikat auf alle Individuen zutreffen soll. Der Existenzquantor besagt, dass ein Prädikat auf mindestens ein Individuum zutrifft. Die Quantoren ermöglichen Aussagen wie „Alle Menschen sind sterblich“ oder „Es gibt mindestens einen rosa Elefanten“.

Gelegentlich werden zusätzlich numerische Quantoren verwendet, mit denen ausgesagt werden kann, dass ein Prädikat auf eine bestimmte Anzahl von Individuen zutrifft. Diese sind jedoch nicht unbedingt nötig, denn sie lassen sich auf den All- und den Existenzquantor sowie auf das Identitätsprädikat zurückführen.

Die oben gegebene Definition eines Prädikats als Folge von Wörtern mit klar definierten Leerstellen, die zu einer Aussage wird, wenn in jede Leerstelle ein Eigenname eingesetzt wird, ist eine rein formale, inhaltsfreie Definition. Inhaltlich betrachtet können Prädikate ganz unterschiedliche Gegebenheiten ausdrücken, zum Beispiel Begriffe (z. B. „_ ist ein Mensch“), Eigenschaften (z. B. „_ ist rosa“) oder Relationen, d. h. Beziehungen zwischen Individuen (z. B. „_ ist größer als _“ oder „_ liegt zwischen _ und _“). Da die genaue Natur und der ontologische Status von Begriffen, Eigenschaften und Relationen umstritten sind bzw. von unterschiedlichen philosophischen Richtungen unterschiedlich betrachtet werden und da auch die genaue Abgrenzung von Begriffen, Eigenschaften und Relationen untereinander unterschiedlich gesehen wird, ist diese formale Definition die anwendungspraktisch günstigste, weil sie es erlaubt, Prädikatenlogik zu verwenden, ohne bestimmte ontologische bzw. metaphysische Voraussetzungen akzeptieren zu müssen.

Die Zahl der unterschiedlichen Leerstellen eines Prädikats wird seine Stelligkeit genannt. So ist ein Prädikat mit einer Leerstelle einstellig, eines mit zwei Leerstellen zweistellig usw. Gelegentlich werden Aussagen als nullstellige Prädikate, d. h. als Prädikate ohne Leerstellen betrachtet. Bei der Zählung der Leerstellen werden nur unterschiedliche Leerstellen berücksichtigt.

In formaler Prädikatenlogik werden Prädikate durch Prädikatbuchstaben ausgedrückt, meist Großbuchstaben vom Anfang des lateinischen Alphabets, zum Beispiel F__ für ein zweistelliges Prädikat, G_ für ein einstelliges Prädikat oder H___ für ein dreistelliges Prädikat. Oft werden die Argumente eines Prädikats in Klammern gesetzt und durch Kommata getrennt, sodass die genannten Beispiele als F(_,_) bzw. G(_) und H(_,_,_) geschrieben würden.

In Sprachphilosophie und Sprachwissenschaft ist das Thema der Eigennamen ein durchaus komplexes. Für die Behandlung im Rahmen einer einleitenden Darstellung der Prädikatenlogik soll es ausreichen, solche Sprachausdrücke als Eigennamen zu bezeichnen, die genau ein Individuum bezeichnen; das Wort „Individuum“ wird hier in einem ganz allgemeinen Sinn verstanden und meint jedes „Ding“ (physikalischer Gegenstand, Zahl, Person...), das in irgendeiner erdenklichen Weise von anderen Dingen unterschieden werden kann. Eigennamen im genannten Sinn werden meistens eigentliche Eigennamen (z. B. „Gottlob Frege“) oder Kennzeichnungen (z. B. „der gegenwärtige Bundeskanzler von Österreich“) sein.

Das Gegenstück zu den Eigennamen der natürlichen Sprache sind die Individuenkonstanten der Prädikatenlogik; meist wählt man Kleinbuchstaben vom Anfang des lateinischen Alphabets, zum Beispiel a, b, c. Im Gegensatz zu natürlichsprachlichen Eigennamen bezeichnet jede Individuenkonstante tatsächlich genau ein Individuum. Dies bedeutet keine impliziten metaphysischen Voraussetzungen, sondern legt lediglich fest, dass nur solche natürlichsprachlichen Eigennamen mit Individuenkonstanten ausgedrückt werden, die tatsächlich genau ein Individuum benennen.

Mit dem Vokabular von Prädikatbuchstaben und Individuenkonstanten lassen sich aussagenlogisch atomare Sätze wie „Sokrates ist ein Mensch“ oder „Gottlob Frege ist Autor der ‚Begriffsschrift’“ bereits in ihrer inneren Struktur analysieren: Übersetzt man den Eigennamen „Sokrates“ mit der Individuenkonstante a, den Eigennamen „Gottlob Frege“ mit der Individuenkonstante b, den Eigennamen bzw. Buchtitel „Begriffsschrift“ mit der Individuenkonstante c und die Prädikate „_ ist ein Mensch“ und „_ ist der Autor von _“ mit den Prädikatbuchstaben F_ bzw. G__, dann lässt sich „Sokrates ist ein Mensch“ als Fa und „Gottlob Frege ist der Autor der ‚Begriffsschrift’“ mit Gbc ausdrücken.

Mit Quantoren können Aussagen darüber gemacht werden, ob eine Satzfunktion auf keines, einige oder alle Individuen des Diskursuniversums zutrifft. Im einfachsten Fall ist die Satzfunktion ein einstelliges Prädikat. Setzt man in das Prädikat eine Individuenvariable ein und stellt den Existenzquantor und dieselbe Variable davor, so wird damit behauptet, dass es mindestens ein Individuum gibt, auf das das Prädikat zutrifft. Es muss also mindestens einen Satz der Form geben, dass in das Prädikat eine Individuenkonstante eingesetzt wird, der im betreffenden Diskursuniversum wahr ist. Der Allquantor sagt aus, dass ein Prädikat auf alle Individuen aus dem Diskursuniversum zutrifft. In der klassischen Prädikatenlogik sind daher alle atomaren, allquantifizierten Aussagen wahr, wenn das Diskursuniversum leer ist.

Der Existenzquantor wird in halbformaler Sprache als „es gibt mindestens ein Ding, sodass...“ oder „es gibt mindestens ein (Variablenname), für das gilt...“ ausgedrückt. In formaler Sprache werden die Zeichen formula_1 oder formula_2 verwendet. Der Allquantor wird in halbformaler Sprache als „Für alle (Variablenname) gilt: ...“ ausgedrückt, in formaler Sprache durch eines der Zeichen formula_3 oder formula_4.

Unmittelbar einsichtig ist die Verwendung von Quantoren bei einstelligen Prädikaten, zum Beispiel „_ ist ein Mensch.“ Die existenzquantifizierte Aussage würde lauten „Es gibt mindestens ein Ding, für das gilt: es ist ein Mensch,“ in formaler Sprache: formula_5. Dabei ist M_ die Übersetzung des einstelligen Prädikats „_ ist ein Mensch“ und formula_6 ist der Existenzquantor. Der Buchstabe x ist keine Individuenkonstante, sondern erfüllt dieselbe Funktion, die in der halbformalen Formulierung das Wort „es“ erfüllt: Beide kennzeichnen die Leerstelle, auf die sich der Quantor bezieht. Im gewählten Beispiel erscheint das als redundant, weil es nur einen Quantor und nur eine Leerstelle enthält und daher keine Mehrdeutigkeit möglich ist. Im allgemeinen Fall, in dem ein Prädikat mehr als eine Leerstelle und ein Satz mehr als einen Quantor und mehr als ein Prädikat enthalten kann, wäre ohne die Verwendung geeigneter „Querverweiszeichen“ keine eindeutige Lesart vorgegeben.

Zum Herstellen der Beziehung zwischen einem Quantor und der Leerstelle, auf die er sich bezieht, werden meist Kleinbuchstaben vom Ende des lateinischen Alphabets verwendet, zum Beispiel die Buchstaben x, y und z; sie werden als Individuenvariablen bezeichnet. Die Leerstelle, auf die sich ein Quantor bezieht, bzw. die Variable, die zum Herstellen dieser Verbindung verwendet wird, bezeichnet man als durch den Quantor gebunden.

Bindet man in einem mehrstelligen Prädikat eine Leerstelle durch einen Quantor, dann entsteht ein Prädikat von um eins niedrigerer Stelligkeit. Das zweistellige Prädikat L__, „_ liebt _“, das die Relation des Liebens ausdrückt, wird durch Binden der ersten Leerstelle durch den Allquantor zum einstelligen Prädikat formula_7, sozusagen zur Eigenschaft, von jedem geliebt zu werden (der Allquantor bezieht sich auf die erste Leerstelle, in der das Individuum steht, von dem die Liebe ausgeht). Durch Binden der zweiten Leerstelle wird daraus hingegen das einstellige Prädikat formula_8, sozusagen die Eigenschaft, alles und jeden zu lieben (der Allquantor bindet die zweite Leerstelle, also jene, in der das Individuum steht, das die Rolle des oder der Geliebten innehat).

Interessant sind Sätze mit Prädikaten, in denen mehr als eine Leerstelle durch einen Quantor gebunden wird. Die Möglichkeit der Behandlung solcher Sätze macht die große Leistungsfähigkeit der Prädikatenlogik aus, ist aber zugleich der Punkt, an dem das System für den Neueinsteiger etwas kompliziert wird und intensiverer Auseinandersetzung und Übung bedarf. Als kleiner Einblick in die Möglichkeiten der Prädikatenlogik sollen für das einfache zweistellige Prädikat L__, das zum Beispiel wie oben gelesen werden kann als „_ liebt _“, alle Möglichkeiten aufgezählt werden, die Leerstellen durch Quantoren zu binden:

Die Matrizen veranschaulichen die Formeln für den Fall, dass fünf Individuen als Liebende und Geliebte in Frage kommen. Abgesehen von den Sätzen 6 und 9/10 handelt es sich um Beispiele. Die Matrix zu Satz 5 steht z. B. für „b liebt sich selbst.“; die zu Satz 7/8 für „c liebt b.“

Wichtig und instruktiv ist es, zwischen den Sätzen 1, formula_9, und 3, formula_10, zu unterscheiden: In beiden Fällen wird jeder geliebt; im ersten Fall jedoch wird jeder von irgendjemandem geliebt, im zweiten Fall wird jeder von ein und demselben Individuum geliebt.

Zwischen einigen dieser Sätze bestehen Folgerungszusammenhänge – so folgt etwa Satz 1 aus Satz 3, aber nicht umgekehrt. (Siehe Hasse-Diagramm)

Mit dreistelligen Prädikaten können Formeln wie formula_11 gebildet werden. Mit dem Prädikat „x will, dass y z liebt.“ bedeutet diese Formel „Jemand wünscht allen jemanden zu lieben.“

In natürlicher Sprache treten Quantoren in sehr unterschiedlichen Formulierungen auf. Oft werden Wörter wie „alle,“ „keine,“ „einige“ oder „manche“ verwendet, manchmal ist die Quantifizierung nur aus dem Zusammenhang erkennbar – zum Beispiel meint der Satz „Menschen sind sterblich“ in der Regel die Allaussage, dass alle Menschen sterblich sind.

Die Äquivalenzrelation zwischen zwei prädikatenlogischen Aussagen ergibt sich durch den schematischen Austausch von Allquantor und Existenzquantor. Im Folgenden exemplarisch einige häufiger gebrauchte prädikatenlogische Äquivalenzen.

Wenn – wie bisher skizziert – Quantoren die Leerstellen von Prädikaten binden, dann spricht man von Prädikatenlogik erster Stufe oder Ordnung, englisch: first order logic, abgekürzt FOL; sie ist sozusagen das Standardsystem der Prädikatenlogik.

Eine naheliegende Variation der Prädikatenlogik besteht darin, nicht nur die Leerstellen von Prädikaten zu binden, also nicht nur über Individuen zu quantifizieren, sondern auch Existenz- und Allaussagen "über Prädikate" zu machen. Auf diese Weise kann man Aussagen wie „Es gibt ein Prädikat, für das gilt: es trifft auf Sokrates zu“ und „Für jedes Prädikat gilt: es trifft auf Sokrates zu, oder es trifft nicht auf Sokrates zu“ formalisieren. Zusätzlich zu den individuellen Leerstellen der Prädikate erster Stufe hätte man auf diese Weise Prädikatsleerstellen eingeführt, die zu Prädikaten zweiter Stufe führen, zum Beispiel eben zu „_ trifft auf Sokrates zu“. Von hier ist es nur ein kleiner Schritt zu Prädikaten dritter Stufe, in deren Leerstellen Prädikate zweiter Stufe eingesetzt werden können, und allgemein zu Prädikaten höherer Stufe. Man spricht in diesem Fall daher von Prädikatenlogik höherer Stufe, englisch higher order logic, abgekürzt HOL.

Die formal einfachste Erweiterung der Prädikatenlogik erster Stufe besteht jedoch in der Ergänzung um Mittel zur Behandlung von Identität. Das entstehende System heißt Prädikatenlogik der ersten Stufe mit Identität. Zwar lässt sich Identität in der Prädikatenlogik höherer Stufe definieren, d. h. ohne Spracherweiterung behandeln, doch ist man bestrebt, möglichst lange und möglichst viel auf der ersten Stufe zu arbeiten, weil es für diese einfachere und vor allem vollständige Kalküle gibt, d. h. Kalküle, in denen alle in diesem System gültigen Formeln und Argumente hergeleitet werden können. Für die Prädikatenlogik höherer Stufe gilt das nicht mehr, d. h. es ist für die höhere Stufe nicht möglich, mit einem einzigen Kalkül alle gültigen Argumente herzuleiten.

Umgekehrt kann man Prädikatenlogik der ersten Stufe einschränken, indem man sich zum Beispiel auf einstellige Prädikate beschränkt. Das aus dieser Einschränkung entstehende logische System, die monadische Prädikatenlogik, hat den Vorteil, entscheidbar zu sein; das bedeutet, dass es mechanische Verfahren (Algorithmen) gibt, die für jede Formel bzw. für jedes Argument der monadischen Prädikatenlogik in endlicher Zeit feststellen können, ob sie bzw. ob es gültig ist oder nicht. Für einige Anwendungszwecke ist monadische Prädikatenlogik ausreichend; zudem lässt sich die gesamte traditionelle Begriffslogik, namentlich die Syllogistik, in monadischer Prädikatenlogik ausdrücken.

Parallel zur bereits thematisierten Unterscheidung prädikatenlogischer Systeme nach ihrer Stufe bzw. Ordnung gibt es klassische und nichtklassische Ausprägungen. Von klassischer Prädikatenlogik bzw. allgemein von klassischer Logik spricht man genau dann, wenn die beiden folgenden Bedingungen erfüllt sind:

Weicht man von mindestens einem dieser Prinzipien ab, dann entsteht nichtklassische Prädikatenlogik. Selbstverständlich ist es auch innerhalb der nichtklassischen Prädikatenlogik möglich, sich auf einstellige Prädikate zu beschränken (nichtklassische monadische Prädikatenlogik), über Individuen zu quantifizieren (nichtklassische Prädikatenlogik der ersten Stufe), das System um Identität zu erweitern (nichtklassische Prädikatenlogik der ersten Stufe mit Identität) oder die Quantifikation auf Prädikate auszudehnen (nichtklassische Prädikatenlogik höherer Stufe). Ein häufig verwendetes nichtklassisches prädikatenlogisches System ist die modale Prädikatenlogik (siehe Modallogik).

Für jedes prädikatenlogische System kann eine formale Semantik aufgestellt werden. Dazu wird eine Interpretationsfunktion definiert, eine Funktion im mathematischen Sinn, die den Prädikaten der formalen prädikatenlogischen Sprache einen Umfang und den atomaren Sätzen einen Wahrheitswert zuordnet. Zunächst wird ein Diskursuniversum festgelegt, das ist die Gesamtheit der unterscheidbaren Gegenstände („Individuen“), auf die sich die zu interpretierenden prädikatenlogischen Aussagen beziehen sollen. Für die klassische Prädikatenlogik werden dann die einzelnen Sprachelemente folgendermaßen interpretiert:

Vor dem Aufblühen von Aussagenlogik und Prädikatenlogik dominierte die Begriffslogik in Gestalt der von Aristoteles entwickelten Syllogistik und darauf aufbauender relativ moderater Erweiterungen. Zwei in den 1960er-Jahren in der Tradition der Begriffslogik entwickelte Systeme werden von ihren Vertretern als der Prädikatenlogik gleichmächtig (Freytag) bzw. sogar überlegen (Sommers) bezeichnet, haben aber in der Fachwelt wenig Resonanz gefunden (siehe Artikel Begriffslogik).

Die Gesetze der Prädikatenlogik gelten nur dann, wenn der Bereich der untersuchten Individuen nicht leer ist, d. h. wenn es überhaupt mindestens ein Individuum (welcher Art auch immer) gibt. Eine Modifikation der Prädikatenlogik, die dieser Existenzvoraussetzung nicht unterliegt, ist die Freie Logik.

Prädikatenlogiken sind von zentraler Bedeutung für verschiedene Grundlegungen der Mathematik.

Daneben gibt es einige konkrete Anwendungen in der Informatik: Sie spielt in der Konzeption und Programmierung von Expertensystemen und in der künstlichen Intelligenz eine Rolle. Logische Programmiersprachen basieren zu Teilen auf – oft eingeschränkten – Formen der Prädikatenlogik. Eine Form der Wissensrepräsentation kann mit einer Sammlung von Ausdrücken in Prädikatenlogik erfolgen.

Der Relationenkalkül, eine der theoretischen Grundlagen von Datenbankabfragesprachen wie etwa SQL, bedient sich ebenfalls der Prädikatenlogik als Ausdrucksmittel.

In der Linguistik, speziell der formalen Semantik, werden Formen der Prädikatenlogik zur Repräsentation von Bedeutung angewendet.

Arten und Erweiterungen der Prädikatenlogik sind in folgenden vertiefenden Einzelartikeln beschrieben:



Kalküle für prädikatenlogische Systeme werden in folgenden weiterführenden Einzelartikeln angegeben:





</doc>
<doc id="3962" url="https://de.wikipedia.org/wiki?curid=3962" title="Piers Anthony">
Piers Anthony

Piers Anthony Dillingham Jacob (* 6. August 1934 in Oxford) ist ein US-amerikanischer Schriftsteller.

Als er vier Jahre alt war, zog seine Familie mit ihm nach Spanien. 1940 flüchtete sie vor dem Bürgerkrieg und dem Franco-Regime in die USA, wo Piers Anthony seine weitere Kindheit verbrachte. Dort ging er auch zur Schule. Er war nicht gerade ein Musterschüler und verbrachte drei Jahre in der ersten Klasse, bis er lesen und schreiben konnte.

1956 schloss Piers Anthony die Schule (Goddard College, Vermont) ab und heiratete Carol Ann Marbel (kurz Cam). 1958 erlangte er nach dem Dienst in der U. S. Army die amerikanische Staatsbürgerschaft. Piers und Cam ließen sich nach seinem Kriegsdienst in Florida nieder, wo sie bis heute leben. Ihre Töchter nannten sie Penny (* 1967; † 3. September 2009) und Cheryl (* 1970).

Nach einigen Jobs als Lektor und Englischlehrer kündigte er und begann zu schreiben; seit dieser Zeit kürzte er aus praktischen Gründen auch seinen Namen.

1962 wurde seine erste Geschichte veröffentlicht. Sein erster Roman erschien fünf Jahre später. Der Durchbruch gelang Anthony mit den Romanen aus der magischen Welt Xanth. 1977 gewann der erste Roman der Serie den British Fantasy Award.

Von Anthony sind mittlerweile weit über 100 Romane erschienen.



In Deutschland erschien auch noch ein Sammelband mit der gesamten Trilogie:



Dieser Zyklus wird in Deutschland auch als "Cluster-Trilogie" bezeichnet, weil Heyne den Roman "Tausendstern" nicht als zum Zyklus gehörend gekennzeichnet hat.



Sammelbände:













</doc>
<doc id="3963" url="https://de.wikipedia.org/wiki?curid=3963" title="Pascal (Programmiersprache)">
Pascal (Programmiersprache)

Pascal ist eine Anfang der 1970er Jahre entwickelte imperative Programmiersprache.

Pascal wurde von Niklaus Wirth an der ETH Zürich als Lehrsprache eingeführt, um die strukturierte Programmierung zu lehren.

Pascal ist eine Weiterentwicklung von Algol 60. Es lehnt sich in seiner Syntax stark an die englische Grammatik an. Dies soll die Lesbarkeit für Programmiereinsteiger verbessern; Pascal ist daher besonders gut als Lehrsprache geeignet. Seine große Verbreitung in der professionellen Programmierung fand es als Borland/Turbo Pascal (später Object Pascal) – gegenüber dem Ur-Pascal wesentlich erweiterte und verbesserte Versionen.

Ein wichtiges Konzept, das Wirth zur Anwendung brachte, ist die starke Typisierung (engl. „strong typing“): Variablen sind bereits zur Übersetzungszeit einem bestimmten Datentyp zugeordnet, und dieser kann nicht nachträglich verändert werden. Typenstrenge bedeutet, dass Wertzuweisungen ausschließlich unter Variablen gleichen Typs erlaubt sind. In Pascal gibt es von diesem strikten Zwang lediglich wenige Ausnahmen:

Heute findet Pascal im universitären Bereich (Entwicklung/Ausbildung) und in sicherheitskritischen Bereichen (z. B. Verkehrstechnik, Energieversorgung, Medizintechnik, Raumfahrt, Militär, teilweise im Banken- und Versicherungswesen) Anwendung. Dies beruht hauptsächlich auf der guten Prüfbarkeit und Wartbarkeit des Codes und der klaren Zuordnung der Variablen. So ist die 2005 eingeführte Betriebsleittechnik IV der Transrapid-Versuchsanlage Emsland in Pascal programmiert.
Eine pascalähnliche Notation wird von jeher in der Informatik und Mathematik zur Darstellung von Algorithmen benutzt. Aus didaktischen Gründen, es seien hier die Typstrenge, hohe Fehlersicherheit und frei verfügbare portierbare Pascalcompiler (Free Pascal, GNU Pascal) genannt, wird im aktuellen Informatikunterricht Pascal ebenfalls häufig eingesetzt. Im Hobby-Bereich erlangte Pascal zeitweilig eine sehr weite Verbreitung, die jedoch mit neueren Microsoft-Windows-Versionen wieder zurückging.

Ein Hallo-Welt-Programm sieht in Pascal folgendermaßen aus:

oder in neueren Pascal-Versionen sieht das Programm so aus:
Ein Detail am Rande: In manchen alten Beispielen findet man ein codice_11 nach dem codice_12-Befehl. Das war nur notwendig, weil die I/O-Architektur der CDC-Cyber 6000, auf der Wirth den ersten Pascal-Compiler entwickelte, das benötigte, um die internen Puffer zu leeren – sonst wäre im interaktiven Betrieb keine Ausgabe zu sehen gewesen. IDEs konnte man mit diesem READLN zwingen, am Ende der Programmausführung nicht sofort wieder in den Editor-Modus umzuschalten, was die Ausgabe bei manchen IDEs (z. B. Turbo-Pascal) sonst hätte verschwinden lassen. Im Batch-Betrieb war das ohnehin nie notwendig.

Jeder Variablen muss in einer Variablendeklaration ein Datentyp zugeordnet werden. Der Datentyp bestimmt, welche möglichen Werte die Variable während des Programmablaufs speichern kann. In Pascal gibt es vier einfache Standardtypen, nämlich codice_3, codice_1, codice_15 und codice_16.

Die Länge der verschiedenen Datentypen außer boolean sind abhängig vom Compiler. Weiterhin haben viele Implementation noch weitere vordefinierte einfache Datentypen.

In Pascal ist es möglich neben den vier Standardtypen, eigene einfache Datentypen zu definieren. Dabei handelt es sich entweder um Aufzählungstypen oder Ausschnittstypen. Um einen neuen Aufzählungstyp zu definieren, schreibt man alle Werte, die eine Variable dieses Typs speichern können soll, der Reihe nach auf. Im folgenden Beispiel wird ein neuer Typ mit dem Namen codice_17 definiert, wobei das optionale codice_18 am Anfang für den weiteren Programmverlauf klarstellen soll, dass es sich um einen Namen eines Typs handelt: codice_19

Einen Ausschnittstyp definiert man, wenn der Wertebereich eines umfassenderen Typs nicht voll ausgeschöpft wird. Wenn eine Variable beispielsweise nur die Zahlen zwischen 0 und 255 speichern können soll, so ist es möglich, den Datentyp codice_3 auf diese Teilmenge einzuschränken. Im folgenden Beispiel wird ein neuer Typ mit dem Namen codice_21 definiert: codice_22

Pascal bietet Programmierern vier strukturierte Datentypen, die es ermöglichen, mehrere Werte in einer Variablen zu speichern. In einem Feld ("array") kann eine feste, also während der Laufzeit unveränderbare Anzahl von Werten gespeichert werden, die alle vom selben Datentyp sein müssen. Im folgenden Beispiel wird ein Feld definiert, in welchem maximal 100 ganze Zahlen abgespeichert werden können:

tBeispielFeld = array [1..100] of integer;
In einer Menge ("set") können ebenfalls Werte vom selben Datentyp gespeichert werden, allerdings ist deren Anzahl nicht fest, sondern variabel. Ein Beispiel für die Definition einer Menge: codice_23

In einem Verbund ("record") kann eine feste Anzahl von Werten gespeichert werden, wobei die Werte von verschiedenem Datentyp sein dürfen. Ein Beispiel einer Definition:
tPerson = record
end;
Eine beliebig lange Folge von Werten desselben Typs kann in Pascal mithilfe des Datentyps Datei ("file") dargestellt werden. Beispiel einer Definition: codice_24

Hauptzweck der Verwendung von Zeigern ist in Pascal das Erstellen von verschiedenen Datenstrukturen, je nach den Vorstellungen des Programmierers. So lassen sich mithilfe von Zeigern etwa Listen und Bäume erstellen, die bei der Speicherung von Daten Vorteile gegenüber Feldern, Mengen, Verbünden und Dateien bieten. Ein weiterer Vorteil von Zeigern ist, dass Zeigervariablen den von ihnen benötigten Speicherplatz während der Laufzeit anfordern können. Der Programmierer muss also nicht schon im Vorhinein definieren, wieviel Platz die Variablen seines Programms bei der Ausführung benötigen werden und kann grundsätzlich vom Minimum ausgehen, welches dann je nach Bedarfsfall überschritten wird.

Charakteristisch für Pascal ist das Konzept der Verschachtelung von Prozeduren und Funktionen. Im Deklarationsteil einer Prozedur oder Funktion können andere Prozeduren und Funktionen definiert werden, die außerhalb nicht sichtbar sind. Beispiel:
program HalloWelt;
begin
end.
Als bedingte Anweisungen und Verzweigungen stehen in Pascal die if- und die case-Anweisung zur Verfügung. Ein Beispiel einer if-Verzweigung:
if (1 = 2) then
else if (1 = 1) then
else
Pascal bietet drei Arten von Schleifen. Soll vor jedem Schleifendurchlauf eine Abbruchbedingung geprüft werden, verwendet man die while-Anweisung, für eine Prüfung nach jedem Durchlauf die repeat-Anweisung. Für eine vorgegebene Anzahl von Durchläufen gibt es die for-Anweisung. Keine Schleife ist die zum Verarbeiten des record-Typs verwendbare with-Anweisung.
while wert < 100 do
Sprunganweisungen können mit codice_25 auch verwendet werden, dessen Gebrauch jedoch umstritten ist (siehe Artikel zu Sprunganweisungen).

Der erste Pascal-Compiler selbst entstand auf der CDC Cyber 6000 der ETH Zürich. Daraus entstand dann Pascal 6000, das als erste operative Version eines Compilers der Sprache gesehen werden kann.

Ein zweiter Pascal-Compiler – der P4 „Portable Pascal Compiler“ von Urs Ammann, Kesav Nori und Christian Jacobi – stammte ebenfalls von der ETH Zürich. Der P4 erzeugte eine plattformunabhängige, als Assemblersprache ausgelegte Zwischensprache, den P-Code, der durch eine virtuelle Maschine (ein Programm) interpretiert wird. Später entstanden dann Compiler, die auf dem P4 basierten. Die verbreitetste Version wurde unter dem Namen UCSD Pascal bekannt, das auf vielen Systemen implementiert wurde, u. a. auf Apple II und Texas Instruments TI-99/4A und wesentlich zur Verbreitung der Sprache sowohl (zuerst) in den Vereinigten Staaten, später auch in Europa beitrug. Im Mai 2006 wurden die Quelltexte von UCSD Pascal freigegeben.

Bei den meisten Pascal-Compilern handelte es sich um "Single-Pass-Compiler", d. h. der Compiler muss die Quelle nur ein einziges Mal lesen und analysieren. Das Design der Sprache war so gehalten, um genau das zu ermöglichen. Bei der damaligen geringen Geschwindigkeit der Rechner war dies ein großer Vorteil.

Sehr hohe Prozesssicherheit

Da Pascal sowie der Quasi-Standard Borland/Turbo-Pascal eine strikte Trennung unterschiedlicher Typen vorsehen und die Zuweisungskompatibilität von Ausdrücken beachten, kommen implizite Typumwandlungen, anders als in C, praktisch nicht vor. Insbesondere bei der Übergabe von Daten (z. B. aus externen Dateien, aber auch innerhalb des Programms) an Funktionen oder Prozeduren kann der Compiler schon beim Kompilieren die Korrektheit der Typen kontrollieren.

Keine nullterminierten Zeichenketten

Standard Pascal hatte zwar Zeichenketten-Literale, aber keinen Typ codice_26 (s. oben). In den meisten späteren Implementierungen wurden Zeichenketten als codice_16-Arrays definiert, bei denen das erste Feld (Byte) zur Speicherung der Länge verwendet wurde. Daraus ergab sich eine Längenbegrenzung von 255 Zeichen. Bei den in der C-Standard-Bibliothek verwendeten Strings handelt es sich dagegen um NUL-terminierte Strings. Durch die Längenspeicherung in Pascal können verschiedene Operationen wie Stringverkettung effizienter durchgeführt werden (kein Durchsuchen bis zum NUL-Zeichen erforderlich). Größere Zeichenketten mussten selbst definiert werden (z. B. als "Array of char"), sofern sie nicht (wie z. B. in Borland Pascal 7) durch die Compilerhersteller bereits implementiert wurden. Im Extended-Pascal-Standard wurden Zeichenketten als Scheme-Typ definiert. Auch hier muss die Kapazität angegeben werden, sie wird aber in runde Klammern geschrieben, und es gibt keine formale Längenbeschränkung: codice_28.

Strikte Trennung zwischen Programm, Funktionen und Prozeduren

Pascal trennt strikt zwischen einer Funktion (mit Rückgabewert) und einer Prozedur (kein Rückgabewert). Eine Funktion darf nicht als Prozedur aufgerufen werden – d. h. der Rückgabewert muss stets entgegengenommen werden. Seit Turbo Pascal 4.0 (1991) ist es jedoch möglich, Funktionen auch ohne Entgegennahme des Funktionsergebnisses aufzurufen. Weiterhin wird ein ausführbares Programm in Pascal durch das Schlüsselwort codice_29 gekennzeichnet, wohingegen in C der Einstiegspunkt für ein Programm die Funktion codice_30 ist, die sich außer durch ihren Namen nicht von anderen Funktionen unterscheidet.

Deklarationen

Programme, Funktionen und Prozeduren sind im Gegensatz zu C dreigegliedert: Es gibt neben dem Kopfteil, der den Namen und die Signatur enthält, einen separaten Deklarationsteil, in dem Typen, Variablen und verschachtelte Prozeduren/Funktionen deklariert werden und einen Definitionsteil, in dem implementiert wird. In C gibt es nur einen Funktionskopf und Funktionsrumpf, der den Deklarations- und Definitionsteil vereinigt. In C dürfen Funktionen nicht verschachtelt definiert werden.

Case sensitivity

Im Gegensatz zu C ist Pascal in Bezug auf Schlüsselwörter, Bezeichner von Variablen, Funktionen oder Prozeduren "case-insensitive".

Semikolon

Das Semikolon wird nicht wie in C als Befehlsabschluss interpretiert, sondern als Trennzeichen zwischen Anweisungen. Vor einem codice_31 oder codice_32 kann es somit weggelassen werden. Vor einem codice_33 darf es in der Regel gar nicht stehen, da sonst der codice_34-Zweig als abgeschlossen angesehen werden würde. Ein Fall, bei dem es vor dem ELSE steht (und stehen muss), ist am Ende einer CASE-Auflistung.

Delimiter

Für Zeichenketten-Literale und Zeichen-Literale wird dasselbe Begrenzungszeichen (Delimiter), nämlich der Apostroph, verwendet.

Zuweisungen

Zuweisungen an Variablen werden durch die Sequenz codice_2 definiert, das Gleichheitszeichen allein dient dem Gleichheitsvergleich und der Definition von Konstanten. Damit werden „mathematisch falsche“ Ausdrücke z. B. i = i + 1 vermieden. Verkürzte Schreibweisen für Zuweisungen wie i++ statt i = i + 1 oder i *= 2 statt i = 2 * i existieren in Pascal nicht.

Operatoren

Pascal verwendet die Operatoren codice_36, codice_37, codice_38, codice_39 (nur für "Integer", bei "Real" ist es codice_40), codice_41 und codice_42 anstatt der C-Operatoren codice_43, codice_44, codice_45, codice_40, codice_47 und codice_48. Die „Short Circuit“-Funktionalität von codice_43 und codice_44 in C (sobald das Ergebnis feststeht, nämlich wenn der linke Teil beim codice_36 FALSE beziehungsweise beim codice_37 TRUE ist, wird der rechte nicht mehr ausgewertet) schloss Niklaus Wirth explizit aus, da compilerinterne Optimierungsvorgänge die Reihenfolge der Auswertung verändern können. Spätere Pascalversionen implementierten eine lokal oder global zuschaltbare „Short Circuit“-Funktionalität, so dass der Programmierer im Einzelfall entscheiden kann, welche Art der Auswertung er wünscht. In Standard-Pascal gibt es keine Bitoperationen für "Integer" (wie codice_53, codice_54, codice_55 und codice_56 von C), aber stattdessen den Typ codice_57 (mit den Operatoren codice_58, codice_59 und codice_60 für Durchschnitt, Vereinigung und Mengendifferenz).

Strenge Typentrennung

Unter anderem ist codice_9 nicht austauschbar mit codice_62, die Umwandlung muss mit den Funktionen codice_63 und codice_64 durchgeführt werden. Dadurch werden verdeckte (oder ungewollte) Umwandlungen vermieden; z. B. führt die Anweisung i = 7/2 zu einer Fehlermeldung, wenn i vom Typ Integer = Ganzzahl ist da 7/2 = 3.5 keine Ganzzahl ist. Spätere Pascalversionen (z. B. Turbo Pascal) führten zusätzlich „explizite“ Typumwandlungen, sog. „type casts“, ein, so dass z. B. mit codice_65 der Wertebereich der Ganzzahlvariablen i erweitert oder eingeschränkt werden konnte. In Standard-Pascal musste dafür ein Umweg über (den Missbrauch von) Variantenrecords (codice_66 in C) genommen werden.

Zeigerarithmetik

In Pascal existiert keine Zeigerarithmetik.

Wirths erste Veröffentlichung der neuen Programmiersprache erschien 1971, eine überarbeitete und erweiterte Version dieses Berichts 1973. Ein weiteres Jahr später folgte ein mit Kathleen Jensen geschriebener Bericht, welchen Wirth als ultimative Referenz für Programmierer und Implementoren bezeichnete; das darin beschriebene Pascal nannte er Standard Pascal.

Es gibt drei Standards, die sich auf Pascal beziehen:

Allerdings sind – wie bei den meisten anderen Programmiersprachen auch – nur die wenigsten Compiler zu diesen Standards vollständig kompatibel. Diese Tatsache verleitete Scott A. Moore zu der bissigen Bemerkung „Pascal is, unfortunately, very much a great improvement on its successors“ („Pascal ist leider so ziemlich eine große Verbesserung seiner Nachfolger“ – damals bereits ein geflügelter Satz).

Selbst großen Compilern wie Delphi oder Free Pascal fehlen bis heute einige Elemente aus Standard Pascal, während Extended Pascal von kaum einem unterstützt wird. Lediglich Prospero Pascal ist vollständig kompatibel zu Extended Pascal, während auch GNU Pascal vollständige Kompatibilität anstrebt.

Historisch interessant ist die Tatsache, dass Apple bei der Entwicklung des Betriebssystems und der darauf laufenden Applikationen des Apple-II-Nachfolgers Macintosh auf Pascal gesetzt hatte. Hierzu wurde bei Apple in einem von Larry Tesler geführten Team unter Konsultation von Niklaus Wirth eine eigene Object-Pascal-Variante entwickelt, die von einer früheren objektorientierten Pascal-Variante namens Clascal abgeleitet wurde, welche schon auf Lisa verfügbar war. Object Pascal war für die Unterstützung von MacApp, einem Macintosh Application Framework erforderlich, das heute unter den Begriff Klassen-Bibliothek fallen würde. Object Pascal wurde 1986 fertiggestellt. Apple beendete den Fokus auf Object Pascal mit der Umstellung von Motorola 68K-CPUs auf PowerPC-CPUs von IBM im Jahre 1994 mit der Betriebssystem-Version Mac OS 7.5. Zwecks Rückwärtskompatibilität blieb die Übergabe von Parametern für Betriebssystemaufrufe bis zur letzten Mac-OS-Version 9.2.2 des Jahres 2002 an Pascal-Konventionen angelehnt.

Pascal hatte von Anfang an im universitären Bereich sehr großen Erfolg, was sich u. a. auch in der Programmiersprache Ada niederschlug, die sehr stark an der Syntax von Pascal angelehnt ist. Ebenfalls fand es weite Verbreitung, auch im kommerziellen Bereich, mit den Produkten der Firma Borland Turbo Pascal, Borland Pascal und Delphi. Diese Versionen sind durch starke Erweiterungen des ursprünglichen Sprachstandards gekennzeichnet. Die Objektorientierung wurde dabei mit Version 5.5 eingeführt.



Die Entwicklung der Programmiersprache Algol (bis 1958) wie auch die ihrer Nachfolger war Produkt von Entwicklungskomitees. Niklaus Wirth war maßgeblich an der Erstellung des Abkömmlings Algol W beteiligt, das in Zürich entwickelt und 1966 fertiggestellt wurde. Er berichtete später von den Nachteilen einer gemeinsamen Entwicklung. Konsens innerhalb des Komitees musste mühsam erarbeitet werden, auch erschienen dem praktisch denkenden Wirth, der sich nicht nur als Wissenschaftler, sondern auch als Ingenieur sah, einige Entwicklungsziele mancher Kollegen als übertrieben und unumsetzbar.

Im Jahr 1968 erhielt Wirth eine Professur an der ETH Zürich, wo er neben seiner Lehrtätigkeit Zeit fand, mit der Entwicklung einer neuen Sprache zu beginnen. Er arbeitete diesmal alleine und konnte Pascal im Lauf des Jahres 1969 fertigstellen. Einer der Gründe für die Neuentwicklung war laut Wirth das Fehlen einer für den universitären Unterricht geeigneten Sprache. Weder das zu unwissenschaftlich angelegte Fortran noch das für die praktische Ingenieurstätigkeit ungeeignete Algol konnten Wirths Ansprüchen genügen. Über den Programmierunterricht hinaus sollte Pascal aber ebenso in der Wirtschaft, der Forschung und bei der Entwicklung von Compilern und Betriebssystemen einsetzbar sein.

Nachdem Wirth die Sprache 1969 fertiggestellt hatte, folgte ein Jahr später die Entwicklung des ersten Pascal-Compilers. Dieser wurde für Computer der CDC 6000 Serie geschrieben und ging bereits 1970 in Betrieb. Im Jahr 1971 veröffentlichte Wirth die nur 28 Seiten lange Beschreibung der Sprache und 1972 konnten Lehrende Pascal erstmals für einführende Programmierkurse verwenden. 1973 folgte eine überarbeitete und 54 Seiten lange Überarbeitung der Spezifikation.

Da die Sprache als Lehrsprache konzipiert war, wies das Standard-Pascal einige Merkmale auf, die den kommerziellen Einsatz erschwerten: Das Konzept für Dateizugriffe („file I/O“) war nicht mächtig, die Laufzeitbibliothek wenig umfangreich, und Zeichenketten konnten nur über Umwege (packed array) direkt im Code verwendet werden. Das führte dazu, dass praktisch jede kommerzielle Implementierung hierzu eigene Lösungen anbot, was zunächst (ähnlich wie die Diversifizierung bei C) zu eigenen Pascal-Dialekten und damit zu Kompatibilitätsproblemen führte. Mit der monopolartigen Dominanz der Turbo-Pascal-Produkte von Borland verschwanden diese Unterschiede fast vollständig.

Ein anderer wesentlicher Nachteil war, dass eine Modularisierung im Sinne getrennter Kompilierung nicht vorgesehen war – was mit der Weiterentwicklung Modula-2 geändert wurde. Plattformspezifische Implementierungen sahen dafür eigene Lösungen vor (z. B. die Units von Turbo-Pascal, oder später die Module in ANSI/ISO Extended Pascal).





</doc>
<doc id="3964" url="https://de.wikipedia.org/wiki?curid=3964" title="Pascal">
Pascal

Pascal bezeichnet:

Pascal ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="3965" url="https://de.wikipedia.org/wiki?curid=3965" title="Prolog (Programmiersprache)">
Prolog (Programmiersprache)

Prolog (vom Französischen: "Programmation en Logique", dt.: „Programmieren in Logik“) ist eine Programmiersprache, die Anfang der 1970er Jahre maßgeblich von dem französischen Informatiker Alain Colmerauer entwickelt wurde und ein deklaratives Programmieren ermöglicht. Sie gilt als die wichtigste logische Programmiersprache.

Erste Implementierungen wichen in ihrer Syntax stark voneinander ab, aber der Edinburgh-Dialekt setzte sich bald als Quasistandard durch. Er war jedoch nicht formal definiert, bis er 1995 zur Grundlage eines ISO-Standards wurde (ISO/IEC 13211-1), der auch ISO-Prolog genannt wird.

Der erste Prolog-Interpreter wurde in Marseille in ALGOL W realisiert. Der erste Ansatz für einen Compiler stammte von David H. D. Warren aus Edinburgh. Dieser hatte als Zielsprache die des Logik-Prozessors Warren’s Abstract Machine und erlaubte deshalb weder dynamische Änderungen noch einen Anschluss rücksetzbarer Prädikate in anderen Programmiersprachen. Der erste voll nutzbare Compiler, der beides erlaubte, wurde von Preben Folkjaer in München entwickelt. Er verwandte einen anderen, von der TU Wien stammenden, Zwischencode, der inkrementell kompiliert wurde; wurden Prädikate verändert, wurde das Kompilat gelöscht und beim nächsten Aufruf neu kompiliert.

Prolog-Programme bestehen aus einer Wissensdatenbank, deren Einträge sich Fakten und Regeln nennen. Der Benutzer formuliert Anfragen an diese Wissensdatenbank.
Der Prolog-Interpreter benutzt die Fakten und Regeln, um systematisch eine Antwort zu finden.
Ein positives Resultat bedeutet, dass die Anfrage logisch ableitbar ist. Ein negatives Resultat bedeutet nur, dass aufgrund der Datenbasis keine Ableitung gefunden werden kann.
Dies hängt eng mit der Closed world assumption zusammen (siehe unten).

Das typische erste Programm in Prolog ist nicht wie in prozeduralen Programmiersprachen ein Hallo-Welt-Beispiel, sondern eine Wissensdatenbank mit Stammbauminformationen.

Folgendes Beispiel repräsentiert den Stammbaum einer kleinen Familie.
Das erste Faktum in Form einer Aussage codice_1 liest sich als: Tobias ist ein Mann. codice_2 definiert das Faktum: Tobias ist der Vater von Frank. Für Hinweise zum Laden von Prolog Texten siehe entsprechenden Abschnitt:
In einem Prolog-Interpreter können nun interaktiv Anfragen an die Datenbasis gestellt werden. Das Ausführen eines Prolog-Programms bedeutet immer das Stellen einer Anfrage.
Das System antwortet entweder mit codice_3 oder codice_4, abhängig davon, ob die Anfrage bewiesen werden konnte.
Der Interpreter signalisiert mit der Eingabeaufforderung codice_5, dass er eine Anfrage erwartet:

Eine Anfrage mit einer Variablen liefert als Antwort zusätzlich Belegungen, mit denen die Anfrage wahr wird. Man nennt eine solche Variablenbelegung Unifikation und sagt, die Variable wird mit diesem Wert unifiziert. Variablen sind in Prolog Token, die mit einem Großbuchstaben beginnen:

"no" ist die Antwort auf die um die vorher ausgegebenen Antworten reduzierte Faktenliste.

Der Interpreter liefert nur positive Antworten auf Anfragen, die explizit definiert oder folgerbar sind (Closed world assumption). So liegen etwa über "heinrich" keinerlei Informationen in der Datenbasis:

Zusätzlich zu Fakten lassen sich in Prolog Regeln formulieren.
Der Regeloperator codice_6 ist dabei wie ein umgedrehter Implikationspfeil zu lesen. Beispiel:
Die Regel besagt: X ist Großvater von Y, wenn es ein Z gibt, sodass X Vater von Z ist und Z Vater von Y. Damit ist der Großvater väterlicherseits definiert.
Eine zweite Regel für den Großvater mütterlicherseits sieht so aus:
Der Operator codice_7 in dieser Regel definiert eine Konjunktion und wird "und" gesprochen.
Der Term links vom Implikationsoperator nennt sich auch Head oder Konsequenz.
Haben zwei Regeln (wie oben) die gleiche Konsequenz, folgt
diese, wenn mindestens in einer Regel die Vorbedingung erfüllt ist (Disjunktion).

Durch die Definition von Regeln können auch Fakten geschlossen werden, die nicht
explizit in der Datenbasis stehen:

Das logische Und wird durch ein Komma dargestellt:

?- true,true.
true.

?- true,false.
false.

?- false,true.
false.

?- false,false.
false.

Das logische Oder wird durch ein Semikolon dargestellt:
?- true;true.
true.

?- true;false.
true.

?- false;true.
true.

?- false;false.
false.

?- 3 < 4.
true.

?- 3 > 4.
false.

?- anton == anton. % == prüft ob das muster links und rechts übereinstimmt
true.

?- 3 == 1+2. % muster stimmt nicht überein
false.

?- 3 \== 1+2. % \== prüft ob das muster links und rechts nicht übereinstimmt
true.

?- 3 =:= 1+2. % =:= ist der numerische vergleich
true.

?- 3 =\= 4. % =\= die numerische Ungleichheit
true.

?- 3 =\= 1+2. % 3=3
false.

?- 3 =< 4. % =< bedeutet kleiner/gleich; '<=' ist nicht zulässig
true.

?- 3 >= 4. % >= bedeutet größer/gleich
false.

?- 4 \= 2. % \= die muster können durch unifikation nicht ident gemacht werden
true.

?- X + 4 \= 2 + Y. % Durch X=2 und Y=4 werden die muster ident
false.

% Wenn X Vater von Z ist und Z Vater von Y ist, dann ist X Großvater von Y

% Adam ist der Vater von Tobias

% Tobias ist der Vater von Frank

% Abfrage ob Adam der Großvater von Frank ist
?- grossvater(adam,frank).
true.

% anton und berta sind ein Ehepaar

% das Ehepaar X Und Y ist genau dann ein Ehepaar,
% wenn ein Ehepaar Y Und X existiert

% Abfrage ob Berta und Anton ein Ehepaar sind
?- ehepaar(berta,anton).
true.
Prolog kennt die Grundrechenarten, also Addition +, Subtraktion -, Skalarprodukt *, Division / und Modulo mod. Die Zuweisung eines Wertes zu einer Variable erfolgt mit dem Schlüsselwort is. Im Gegensatz zu imperativen Programmiersprachen können in Prolog Variablenwerte nicht überschrieben werden.

Listen sind rekursive Datenstrukturen bestehend aus einem Kopf (') und einem Rest ('). Der Rest kann hierbei wieder aus Listen bestehen.

% Head ist die Zahl 1
% Tail ist die Zahl 2
[1,2]

% Head ist die Zeichenkette 'one'
% Tail ist die Zeichenkette 'two'
['one', 'two']

% Listen können auch gemischt sein
[1,'two']

% Head ist die Ziffer 1
% Tail ist die Liste [2,3]
[1,2,3]
Um zu prüfen ob ein bestimmtes Element in einer Liste enthalten ist, wird die vordefinierte Funktion codice_8 verwendet:

member(X, [X|_]).
member(X, [_|T]) :-

?- member(2, ['anton', 'berta', 'caesar']).
false.

?- member('berta', ['anton', 'berta', 'caesar']).
true.
Ein Prolog-Text kann direkt über die Konsole eingegeben werden. Dazu kann man auf
der Eingabezeile [user] tippen. Die Eingabe der Klauseln muss mit einem Dateienendezeichen abgeschlossen werden (^D oder ^Z je nach Plattform):
?- [user]. % tippe Prolog-Text direkt ein
append([], X, X).
append([X|Y], Z, [X|T]) :- append(Y, Z, T).
^D
Alternativ kann ein Prolog-Text in einer Datei gespeichert werden und z. B. mit dem Prädikat consult geladen werden. Das Prädikat nimmt einen physischen Pfad zu einem Prolog-Text entgegen:

?- consult('append.pl'). % lade Prolog Text-aus Datei
true.
Entscheidend für die Prolog-Programmierung sind die Techniken der Rekursion und die Nutzung von Listen.

Ist die Rekursion in den meisten Programmiersprachen nur eine zusätzliche Variante zur Iteration, ist sie bei der Prolog-Programmierung die einzige Möglichkeit, "Schleifen" zu produzieren. Benötigt man in obigem Beispiel eine allgemeine "Vorfahr"-Relation, wird das wie folgt realisiert (";" zeigt in der Klausel die Disjunktion bzw. das logische "oder"):
% X ist genau dann ein elternteil von Y,
% wenn X die mutter von Y ist Oder
% wenn X der vater von Y ist.
elternteil(X,Y) :- mutter(X,Y); vater(X,Y).

% X ist einerseits dann ein vorfahr von Z,
% wenn X ein elternteil von Z ist.
vorfahr(X,Z) :- elternteil(X,Z).

% X ist andererseits dann ein vorfahr von Z, wenn
% X ein elternteil von Y ist Und
% Y ein vorfahre von Z ist
vorfahr(X,Z) :- elternteil(X,Y), vorfahr(Y,Z).
Dies lässt sich wie folgt lesen: X ist ein Vorfahr von Z, wenn X Elternteil von Z ist (Regel 1) oder es ein Y gibt, das Vorfahr von Z ist und gleichzeitig X Elternteil von Y (Regel 2) (Es wurde hier "elternteil" statt "mutter" oder "vater" verwendet.)

Auch Listen sind ein entscheidender Bestandteil von Prolog. Die meisten Prolog-Implementationen bringen dafür viele Basisfunktionen mit ("concat" = Anhängen von Werten, "count" = Anzahl der Werte, etc.), die sich aber auch alle selbst definieren lassen. In einer gedachten Familienstruktur muss die Anzahl der Kinder ja variabel sein. Folgendes wäre denkbar:
Dann ließen sich z. B. mit einer Abfrage alle Männer ohne Kinder anzeigen:

Dabei ist X die Variable, deren verschiedene Werte ausgegeben werden sollen. Der Unterstrich _ ist in Prolog die anonyme Variable, wodurch Prolog veranlasst wird hier jeden Wert zuzulassen. Die eckigen Klammern stehen für die leere Liste, welche die nicht vorhandenen Kinder repräsentiert.

Eine weitere Eigenschaft und Besonderheit gegenüber anderen Programmiersprachen ist, dass Prolog in der Lage ist, während der Laufzeit seine vorhandene Datenbank zu erweitern oder zu löschen. Ein Beispiel für das Löschen eines einzelnen Elements:
Die Abfrage:

ergibt (anfänglich) ganz normal:

Die Abfrage:

würde beim ersten Mal:

beim zweiten Mal nur noch:

liefern, da die Informationen:
aus der Datenbank gelöscht wurden. Auch "?- auto(bmw,X)" liefert jetzt nur noch "No".
Zum Löschen aller gleichen Elemente (also z. B. "auto()") auf einmal benutzt man retractall(), zum Ausgeben asserta() (oben in der Datenbank) und assertz() (unten in der Datenbank).

 ABB - CD = EED

A bis H stehen jeweils für eine Ziffer 0 bis 9, wobei nicht klar ist, welche Ziffer welchem Buchstaben entspricht. Gesucht ist die Zahl, die bei den Fragezeichen stehen muss. Dieses Problem ist in Prolog sehr einfach zu lösen. Man schreibt zunächst eine Regel, die bewirkt, dass A bis H später mit allen möglichen Kombinationen von 0 bis 9 belegt werden (Permutation):
Nun müssen nur die fünf entstehenden Gleichungen ("ABB – CD = EED", "FD + EF = CE", "ABB – FD = EGD", "CD – EF = FH" und "EED * CE = EGD * FH = X") in Prolog-Syntax geschrieben werden:
Interessiert nur "X", wird eine Lösungsregel angelegt, die alles zusammenführt und "X" ausgibt:
Wird nun die Abfrage "loesung." eingegeben, wird die Lösung ausgegeben. Wie man sieht, benötigt man zur Lösung dieses Problems fast keine Programmierkenntnisse über Schleifen oder ähnliches, sondern gibt nur die Fakten ein und welches Ergebnis man benötigt. Prolog steht in der Abstraktionshierarchie aus genau diesem Grund über imperativen und objektorientierten Sprachen.

Eine häufig gestellte Aufgabe an Programmiersprachen ist die Verarbeitung hierarchischer Strukturen, wie z. B. SGML oder XML. Insbesondere für XML bildet Prolog eine sehr wirkungsvolle und ausdrucksstarke Alternative zu der verbreitetsten Verarbeitungssprache XSLT.

Ein typischer XML-Baum wie
wird unter Prolog als rekursive Liste von Elementen "element(TagName, Attribute, Kinder)" dargestellt.
Ein sehr einfaches Paradigma (untere drei Klauseln) erlaubt es, jeden Baum rekursiv zu durchlaufen. Folgende Beispiele löschen (oberster Klausel mit "delete") und konkatenieren (zweiter Klausel von oben mit "concat") bestimmte Tags. Der erste Unifikator ist die Operation ("delete" oder "concat"), der zweite die zu bearbeitende Struktur, der dritte das spezifizierte Tag, der vierte der Ergebnisbaum. "append" ist ein Befehl zum Konkatenieren von Listen.

Stößt der Backtracker bei der Operation "delete" auf ein Tag, das wie das zu löschende heißt, so wird dieses entfernt und bei den Nachbarn weitergesucht. Ein entsprechender Aufruf ist z. B. "transform(delete, Tree, autor, ResTree).", der alle Autoren entfernt.

Ähnlich können durch "transform(concat, Tree, paragraph, ResTree)." alle nebeneinanderstehenden Paragraphen miteinander verschmolzen werden. Dazu werden zunächst deren Inhalte konkateniert, daraus eine neue Paragraphstruktur erzeugt und diese weiterverarbeitet.

Planungssysteme suchen eine Möglichkeit, von einem Ausgangszustand in einen gewünschten Zielzustand zu gelangen. Sie lassen sich für die Suche von Straßen- oder Verkehrsverbindungen, aber auch für allgemeinere Problemstellungen einsetzen. Zunächst der allgemeinste Ansatz für eine "blinde Tiefensuche" (d. h. es ist unbekannt, ob der einzelne Schritt auch näher zum Ziel führt):
Nur die Prädikate "operator", "anwendbar", "fuehrt_zu" und "zulaessig" sowie die Beschreibung eines Zustands sind problemspezifisch zu formulieren. Aufgerufen wird das Prädikat mit einer Zustandsliste, die den Anfangszustand enthält.

Abhängig vom Problemtyp lässt sich einiges vereinfachen und/oder weglassen; für eine Wegesuche in einem Straßennetz ergibt sich z. B.
Bei realen Problemen führt eine blinde Suche selten zum Ziel; man benutzt eine Breitensuche, bei der alle vom Start aus erreichbaren neuen Zustände ermittelt, mit einer "Heuristikfunktion" bewertet und nur der beste ("Heuristische Suche") oder eine sortierte Liste der besten ("Best-first-Suche") weiterverfolgt werden. (Die einfache heuristische Suche kann dazu führen, dass nicht immer die optimale Lösung gefunden wird, da bestimmte Lösungsschritte, die fälschlicherweise als ungünstig aussortiert wurden, sich als bessere Lösung ergeben würden.) Die Kunst liegt in der richtigen problemspezifischen Formulierung der Heuristikfunktion. In vielen Fällen hilft die "A-Heuristik", das ist die Summe aus bisher erbrachtem Aufwand und geschätztem Restaufwand zum Ziel (z. B. zurückgelegte Fahrtstrecke + Luftliniendistanz zum Zielort):
Jedes Element von BewerteteListe hat die Struktur [Heuristikwert,gesamte Fahrtstrecke,Ort]; zur Berechnung der A-Heuristik sind die bisherige Strecke, der letzte Ort und der Zielort (Luftlinie) erforderlich.

Dies ist eine Version des Zebrarätsels. Es wurde angeblich von Albert Einstein im 19. Jahrhundert verfasst. Einstein wird oft der Vermerk zugeschrieben, nur 2 % der Weltbevölkerung seien im Stande, das Rätsel zu lösen. Es existiert jedoch kein Hinweis auf jedwede Autorenschaft. Hier soll es ein Beispiel für ein Problem darstellen, das mit Prolog lösbar ist.


Frage: Wem gehört der Fisch?

Hinweise:


Lösung:

Jedes Haus ist eine Liste der Form [Farbe, Nationalität, Getränk, Zigarettenmarke, Haustier].

Zuerst vier einfache Hilfsprädikate zur Listenbearbeitung:

Lösungsprädikat:

Um Regeln für Parser zu schreiben, haben die meisten Prologsysteme einen Präprozessor implementiert. Er erlaubt es, die Regeln in einer besser lesbaren Form zu notieren, die in der Form den Regeln entsprechen, die verwendet wird, um eine kontextfreie Sprache zu beschreiben. Der Präprozessor ergänzt Platzhalter und erzeugt die oben erwähnten Prolog-Logik-Formeln. Durch Übergabe weiterer Attribute ist es möglich, mit Definite Clause Grammars auch komplexere Sprachen als die kontextfreien zu beschreiben.

Ein Prolog-Programm ist eine geordnete Liste so genannter Horn-Klauseln, einer eingeschränkten Form der Prädikatenlogik erster Ordnung. Stellt man dem System eine Anfrage ("Query"), versucht es, diese auf der Grundlage dieser Datenbasis mittels Resolution zu beweisen. Das Ergebnis einer Query ist "yes" oder "no". Seine eigentliche Wirkung entfaltet ein Prolog-Programm streng genommen durch Nebenwirkungen, die während der Beweissuche auftreten. Also kann ein Prolog-System auch als ein sehr effizienter – wenn auch eingeschränkter – automatischer Theorembeweiser verstanden werden. Die einzige in Prolog eingebaute Suchstrategie bei der Beweisfindung ist Tiefensuche mit Backtracking.

In den 1980er Jahren spielte die Sprache eine wichtige Rolle beim Bau von Expertensystemen.
Die Sprache wird heute noch in den Bereichen Computerlinguistik und Künstliche Intelligenz verwendet. Zum Beispiel sind Sprachverarbeitungskomponenten des durch seinen Auftritt bei Jeopardy! bekannt gewordenen KI-Systems Watson in Prolog geschrieben. Außerdem gibt es einige kommerzielle Anwendungen im Bereich des Systemmanagements, bei denen asynchrone Ereignisse (Events) mit Hilfe von Prolog oder darauf basierenden proprietären Erweiterungen verarbeitet werden. Ein Beispiel hierzu ist das Produkt "Tivoli Enterprise Console" (TEC) von IBM, das auf IBM-Prolog basiert.







</doc>
<doc id="3967" url="https://de.wikipedia.org/wiki?curid=3967" title="Phonem">
Phonem

Ein Phonem (selten: Fonem) (von , "phōnḗ", „Laut, Ton, Stimme, Sprache“) ist die abstrakte Klasse aller Laute (Phone), die in einer gesprochenen Sprache die gleiche bedeutungsunterscheidende (distinktive) Funktion haben.


Das Phonem kann somit als die kleinste bedeutungsunterscheidende Einheit des Lautsystems einer Sprache definiert werden. Das Phonem ist nicht allein durch seinen Klang definiert, sondern durch seine Funktion. Phoneme sind somit Untersuchungsgegenstand der Phonologie, während die Einheiten der Phonetik (als Klangereignisse) "Phone" genannt werden.
Beide sind zu unterscheiden von Graphemen, den kleinsten funktionstragenden graphischen Einheiten eines Schriftsystems (die auch in Alphabetschriften nicht immer genau einem Phon oder Phonem entsprechen).

Zur Notierung von Phonemen bedient man sich im Allgemeinen der Lautschrift-Symbole des Internationalen Phonetischen Alphabets. Dabei handelt es sich jedoch lediglich um eine Vereinfachung: Da Phoneme nicht mit den Lauten identisch, sondern Positionen innerhalb einer Systematik sind, könnte man im Prinzip jedes beliebige Symbol für ein Phonem verwenden. Zur Unterscheidung werden Phoneme durch Schrägstriche und Phone in eckigen Klammern notiert.


Phone gehören zu unterschiedlichen Phonemen, wenn der phonetische Unterschied in der jeweiligen Sprache einem Bedeutungsunterschied entspricht. Dies stellt man anhand von Wörtern fest, die sich nur in einem Laut unterscheiden. Wenn beide Wörter Unterschiedliches bedeuten, sind die untersuchten Laute Realisierungen unterschiedlicher Phoneme. (Ebendies ist mit „bedeutungsunterscheidend“ gemeint.)


Ergeben sich Bedeutungsunterschiede, werden die Wörter „Wortpaare“ oder „Minimalpaare“ genannt. Zu erwähnen ist hier auch, dass Phoneme nicht nur als Lautsegmente realisiert werden, sondern auch als suprasegmentale Eigenschaften von Silben auftreten können. So bei den Tonsprachen, die verschieden hohe oder verlaufende Töne auf einer Silbe kennen, die eindeutig bedeutungsunterscheidend sind. Man spricht hier auch von Tonemen, die eine Untergruppe der Phoneme sind.

Mit Hilfe dieser sogenannten Minimalpaaranalyse lassen sich alle Phoneme einer Sprache systematisch erfassen und identifizieren: Führt das Ersetzen eines Lauts durch einen anderen zu einer Änderung (oder zum Verlust) der Bedeutung des Wortes, können beide Laute unterschiedlichen Phonemen zugeordnet werden. Bei Phonemen handelt es sich jedoch nicht um die Laute selbst; vereinfacht ausgedrückt kann man ein Phonem als eine Gruppe von Lauten, die von Muttersprachlern der jeweiligen Sprache als „ungefähr gleich“ empfunden werden, auffassen. Es sind also von den Einzellauten (Phonen) einer Sprache abstrahierte Einheiten. Als solche sind sie keine physischen Laute im eigentlichen Sinn, sondern müssen durch entsprechende Allophone realisiert („hörbar gemacht“) werden.

Die Phoneme sind keine Atome, sondern „kontrastieren in bestimmten Lauteigenschaften“. Die Lauteigenschaften, die ein Phonem von einem anderen unterscheiden, werden auch distinktive Merkmale genannt.

Wenn das Phonem als kleinste bedeutungsdifferenzierende Einheit bezeichnet wird, kann sich das also nur „auf kleinste "in der Sequenz aufeinander folgende" Einheiten“ beziehen, während „eine Gliederung in noch kleinere "simultan" im Phonem gebündelte Merkmale“ damit nicht ausgeschlossen wird.

In phonologischen Theorien, die primär mit distinktiven Merkmalen arbeiteten, gibt es allerdings eigentlich keinen Bedarf für den Phonembegriff mehr, und Symbole wie „/p/“ werden nur als eine praktische Abkürzung für ein Merkmalbündel betrachtet.

Welche Lauteigenschaften distinktiv sind, erschließt sich nicht einfach aus dem Klang, sondern ist eine Eigenschaft, die durch die Grammatik einer Einzelsprache festgelegt wird. Beispielsweise beruht der Kontrast der deutschen Wörter „Bass“ und „Pass“ darauf, ob der mit den Lippen gebildete Verschlusslaut stimmhaft oder stimmlos ist. In einer Sprache wie dem Koreanischen etwa bildet jedoch derselbe Kontrast keine Minimalpaare (sondern ein und dasselbe Phonem wird stimmhaft ausgesprochen wenn es zwischen zwei Vokalen steht und sonst stimmlos). Stattdessen benutzt das Koreanische die Behauchung (Aspiration) oder die gespannte Ausführung eines Verschlusslautes als distinktive Merkmale; minimale Kontraste sind z. B. [pal] „Fuß“ - [phal] „Arm“ – [ppal] „schnell“ (hierbei ist „ph“ als ein einziges phonetisches Zeichen für ein aspiriertes p und „pp“ als ein einziges phonetisches Zeichen für ein gespanntes p zu verstehen). Die Aspiration des Verschlusslautes „p“ liegt in dem deutschen Wort „Pass“ zwar vor, eine Aussprache ohne Hauch ergibt aber im Deutschen, anders als im Koreanischen, nie ein anderes Wort.

Gleichgültig, ob man Phoneme als das Ergebnis einer rein linguistischen Systematisierung oder als mentale Entitäten auffasst, in jedem Fall handelt es sich bei ihnen um Abstraktionen einer konkreten lautlichen Äußerung. Genauer handelt es sich um eine „Klasse von Lauten …, die alle distinktiven Eigenschaften gemeinsam haben, in den nicht-distinktiven dagegen differieren können.“

Dies bedeutet, dass sich konkrete Realisierungen von Phonemen erheblich voneinander unterscheiden können und dennoch ein und demselben Phonem zugeordnet werden. Die Realisierungen (Instanzen) eines Phonems werden auch Allophone genannt. Nach dem Gesagten können Allophone mitunter in verschiedenen Varianten auftreten.


Für eine Reihe von Phonemen existieren jedoch phonologische Regeln, die in Abhängigkeit von der lautlichen Umgebung eines Phonems eindeutig festlegen, mit welchem Allophon es zu realisieren ist. Man spricht von (kontextgebundenen) "kombinatorischen Varianten eines Phonems" – im Gegensatz zu "freien Varianten eines Phonems".


Entscheidend ist also einzig und allein die lautliche Umgebung, inhaltliche Unterschiede zwischen den Wörtern spielen keine Rolle.

Bei derartigen sogenannten kombinatorischen Varianten sind beide Allophone zumeist so verteilt, dass dort, wo das eine stehen muss, das andere nicht stehen darf und umgekehrt (komplementäre Distribution).


Führen solche Regeln dazu, dass ein eigentlich distinktives Merkmal seine bedeutungsunterscheidende Funktion verliert, spricht man von Neutralisation.


Auch Assimilationsprozesse führen häufig zu Neutralisation.

Phoneme sind also nicht bloß phonetisch bestimmt, sondern „linguistische Elemente, die durch ihre Stellung im sprachlichen System, durch ihre syntagmatischen und paradigmatischen Relationen, das heißt durch ihre Umgebung und durch ihre Substituierbarkeit, bestimmt sind“.

Phonem und Graphem werden auch unter der Sammelbezeichnung „Distingem“ zusammengefasst.

Das Phonem ist zu unterscheiden vom Graphem. Das Graphem ist die kleinste bedeutungsunterscheidende Einheit der geschriebenen Sprache.

Das Phonem ist die kleinste bedeutungsunterscheidende Einheit der gesprochenen Sprache.


Das Phonem ist zu unterscheiden vom Morphem. Das Phonem ist als kleinste bedeutungsunterscheidende, während das Morphem als kleinste bedeutungstragende sprachliche Einheit definiert ist, da es einen semantischen Inhalt enthält. Die Phoneme /r/ und /t/ zum Beispiel unterscheiden die Lexeme (Wörter) „rot“ und „tot“ als Minimalpaar voneinander; sie selbst tragen keinerlei Bedeutung. Eine eigene Bedeutung haben dagegen die genannten beiden Wörter (Lexeme) und bilden jeweils ein Morphem. Ein Morphem besteht in der Regel aus einem oder mehreren Phonemen, die in der Schriftsprache als Grapheme notiert werden. Das Fehlen eines Phonems kann als Morphemform "(Nullmorphem)" bezeichnet werden, etwa wenn eine Flexionsform an der jeweiligen Stelle Phoneme enthält, eine andere dagegen nicht.

Beispiele für deutsche Phoneme:
Umstritten ist der phonematische Status im Deutschen u. a. bei den Schwa-Lauten (e-Schwa und a-Schwa), dem glottalen Verschlusslaut (auch Knacklaut, engl. „glottal stop“), den Diphthongen (vokalische Doppellauten mit Gleitbewegung von einem Ausgangs- hin zu einem Endvokal) und den Affrikaten (Abfolge von Plosiv und Frikativ, die mit dem gleichen Organ gebildet werden). Am weitesten auseinandergehen die in der Forschungsliteratur anzutreffenden Anzahlen an Vokalphonemen (nämlich von 8 bis 26).

Die Gesamtheit aller Phoneme wird auch als „Phoneminventar“ bezeichnet, dessen Größe von Sprache zu Sprache teilweise erheblich schwankt. Am Phoneminventar orientieren sich auch die meisten Alphabetschriften, im Idealfall existiert eine 1-zu-1-Zuordnung von Phonemen und Buchstaben.

Sprecher nutzen nur eine eingeschränkte Zahl potentieller Laute, die das menschliche Sprachorgan hervorbringen kann. Aufgrund von Allophonen ist die Anzahl der zu unterscheidenden Phoneme in der Regel kleiner, als die Anzahl der Laute, die in einer Einzelsprache identifiziert werden kann. Verschiedene Sprachen unterscheiden sich maßgeblich in der Anzahl der Phoneme, die ihrem Sprachsystem eigen sind. Das gesamte phonetische Inventar in den Sprachen variiert zwischen nur 11 in Rotokas und 10 – nach allerdings umstrittenen Analysen – in Pirahã, der phonemärmsten Sprache der Welt, und bis zu 141 Phonemen in oder !Xũ, der phonemreichsten.

Die Zahl der phonemisch distinktiven Vokale kann niedrig sein wie in Ubyx und Arrernte mit nur zwei oder hoch wie in der Bantusprache Ngwe, die 14 Grundvokale aufweist, wovon 12 lang und kurz unterscheiden, plus 6 nasalierte Vokale, ebenfalls jeweils lang und kurz realisiert, was insgesamt 38 phonemische Vokale macht. !Xóõ (!Xũ) hingegen besitzt schon 31 reine Vokale, ohne dass man die zusätzlichen Variationen hinsichtlich der Vokallänge durch Tonhöhen dazuzählt. Hinsichtlich der konsonantischen Phoneme besitzt Puinave gerade einmal sieben und Rotokas sechs. !Xóõ (!Xũ) dagegen hat um die 77 und Ubyx gar 81 konsonantische Phoneme.

Das häufigste Vokalsystem besteht aus den fünf Grundvokalen /i/, /e/, /a/, /o/, /u/. Die häufigsten Konsonanten sind /p/, /t/, /k/, /m/, /n/. Sehr wenigen Sprachen fehlen diese Konsonanten, so gibt es im Arabischen kein /p/, im Standardhawaiianischen fehlt das /t/, Mohawk und Tlingit haben kein /p/ und /m/, Hupa hat weder /p/ noch ein /k/, umgangssprachliches Samoanisch hat weder /t/ noch /n/, wohingegen Rotokas und Quileute die Nasale /m/ und /n/ nicht haben.

Englisch besitzt eine große Variationsbreite an vokalischen Phonemen (zwischen 13 und 21, einschließlich der Diphthonge). Die 22 bis 26 Konsonanten entsprechen hingegen dem Durchschnitt der meisten Sprachen. Das Standarddeutsche besitzt ungefähr 40 Phoneme (etwa 20 Vokalphoneme und 20 konsonantische Phoneme, je nach Zählweise).

Die Phonetik gibt die Möglichkeit, Phoneme als Mengen von (distinktiven) Merkmalen aufzufassen, anhand auserwählter Merkmale Phonemklassen zu bilden und das Phoneminventar als "Phonemsystem" zu betrachten.

Die Merkmale, durch die sich Phoneme unterscheiden, nennt man "phonologische Merkmale" im Gegensatz zu den "phonetischen Merkmalen" der Phone.

Phoneme lassen sich anhand ihrer Merkmale klassifizieren. Gibt es ein Merkmal, das zwei Phoneme voneinander unterscheidet, so wird es als distinktives Merkmal bezeichnet.


Für manche Phoneme gelten Einschränkungen, was ihre Position anbelangt: Im Deutschen etwa darf nicht am Wortanfang auftauchen, nicht am Wortende.

Nach der klassischen Charakterisierung des Strukturalismus sind Phoneme abstrakte Einheiten einer systematisierenden Untersuchung von Sprache.

Noam Chomsky und Morris Halle begründeten eine psychologische Interpretation der Phoneme als mentale Einheiten.

Im Laufe des Spracherwerbs erlernt ein Kind, welche phonetischen Merkmale eines Lautes für die Bedeutung eines Wortes entscheidend sind und welche nicht. Die im Zuge dieses Prozesses entstehenden Kategorien werden als mentale Entsprechungen (Repräsentationen) der ursprünglich rein linguistisch definierten Phoneme angesehen. Nach dieser Auffassung haben Phoneme eine eigenständige Existenz im mentalen Sprachverarbeitungssystem eines Sprechers: Das System greift bei der Sprachverarbeitung tatsächlich auf diese Einheiten zurück. (Eine gegenteilige Hypothese wäre etwa die Behauptung, dass durch das Zusammenspiel von gelernten Wörtern und einzelnen Lautwahrnehmungen nur der "Eindruck" entsteht, Phonemkategorien seien im System am Werk.)

Der Einfluss dieser Phonemkategorien auf die Wahrnehmung lässt sich besonders gut beim Umgang mit einer Fremdsprache beobachten. Phonetische Unterscheidungen, die in der eigenen Sprache keine Rolle spielen, werden vom untrainierten Ohr auch in anderen Sprachen nicht wahrgenommen oder fälschlicherweise ein und demselben Phonem zugeordnet. Beispiel: Das chinesische wird retroflex gebildet, das chinesische in etwa wie unser . Wenn ein Deutscher seinen Laut ausspricht, wird dieser von Chinesen als wahrgenommen und nicht als das chinesische retroflexe .

Es kommt vor, dass in bestimmten Wörtern ein Phonem durch ein anderes ersetzt werden kann, ohne dass sich die Bedeutung ändert. Man nennt das Phonemvariation oder "Phonemfluktuation". Sie ist in der standardsprachlichen Aussprachenorm relativ selten. Dort wo sie standardsprachlich anerkannt ist, kann sie auch Auswirkungen auf die Schreibung haben.

Beispiele:
Standardsprachlich anerkannt (nach Aussprache-Duden und den einschlägigen Rechtschreibwörterbüchern, zum Teil als anerkannte Regionalismen):

Standardsprachlich nicht anerkannt, aber in der Umgangslautung zu finden (nach Aussprache-Duden):

Standardsprachlich zum Teil bzw. inzwischen anerkannt. Das amtliche Wörterverzeichnis von 2006 vermerkt: "Spaß, (österr. auch) Spass", in der Schweiz schreibt man ausspracheunabhängig ausschließlich „Spass“:

Der Begriff des Phonems wurde erkennbar bei der Untersuchung von Lautsprachen entwickelt. Aber auch Gebärdensprachen verfügen über ein bestimmtes Inventar von gebärdensprachlichen Phonemen. Wegen des Modalitätsunterschieds (oral-auditorisch vs. manuell-visuell) wird dieses Phoneminventar in die vier Parameter Handkonfiguration, Handorientierung, Bewegung und Platzierung aufgeteilt, statt in Vokale und Konsonanten wie in Lautsprachen. Alle Gebärden werden mit mindestens einem Phonem aus jedem Parameter aufgebaut und simultan ausgeführt. Die Art und Anzahl der Phoneme können auch in Gebärdensprachen variieren, so dass man deswegen auch einen fremden Gebärdensprach-Akzent erkennen kann.





</doc>
<doc id="3969" url="https://de.wikipedia.org/wiki?curid=3969" title="Plattentektonik">
Plattentektonik

Plattentektonik ist ursprünglich die Bezeichnung für eine Theorie der Geowissenschaften über die großräumigen tektonischen Vorgänge in der äußeren Erdhülle, der Lithosphäre (Erdkruste und oberster Erdmantel), die heute zu den grundlegenden Theorien über die endogene Dynamik der Erde gehört. Sie besagt, dass die äußere Erdhülle in Lithosphärenplatten (umgangssprachlich als Kontinentalplatten bezeichnet) gegliedert ist, die dem übrigen Oberen Erdmantel aufliegen und darauf umherwandern (→ Kontinentaldrift).

Vorrangig bezeichnet der Begriff "Plattentektonik" heute jedoch nicht mehr die Theorie, sondern das mittlerweile in weiten Teilen direkt oder indirekt nachgewiesene Phänomen als solches. Selbiges kann als an der Erdoberfläche auftretender Ausdruck der Mantelkonvektion im Erdinneren aufgefasst werden, hat aber noch weitere Ursachen.

Zu den mit der Plattentektonik verbundenen Prozessen und Erscheinungen zählen die Entstehung von Faltengebirgen (Orogenese) durch den Druck zusammenstoßender Kontinente sowie die häufigsten Formen von Vulkanismus und Erdbeben.

Grundlegend für die Plattentektonik ist die fragmentierte Struktur der Lithosphäre. Sie ist in sieben große Lithosphärenplatten gegliedert, die auch als tektonische Platten oder (vor allem von Nicht-Geologen) als Kontinentalplatten bezeichnet werden:

Daneben gibt es noch eine Reihe kleinerer Platten wie beispielsweise die Nazca-Platte, die Indische Platte, die Philippinische Platte, die Arabische Platte, die Karibische Platte, die Cocosplatte, die Scotia-Platte sowie weitere Mikroplatten, über deren Abgrenzung jedoch teilweise noch wenig bekannt ist oder deren Existenz bislang nur vermutet wird.

Die Plattengrenzen werden an der Erdoberfläche meist entweder durch mittelozeanische Rücken oder Tiefseerinnen repräsentiert. An den Rücken driften die benachbarten Platten auseinander ("divergierende Plattengrenze"), wodurch basaltisches Magma aus dem Oberen Erdmantel emporsteigt und neue ozeanische Lithosphäre gebildet wird. Dieser Prozess wird auch als Ozeanbodenspreizung oder "Seafloor Spreading" bezeichnet. Er geht mit intensivem, meist unterseeischem Vulkanismus einher.

An anderen Plattengrenzen taucht im Gegenzug ozeanische Lithosphäre unter eine angrenzende (ozeanische oder kontinentale) Platte tief in den Erdmantel ab (Subduktion). An diesen "konvergierenden Plattengrenzen" befinden sich die Tiefseerinnen. Entwässerungsprozesse in der abtauchenden Platte führen in der oben bleibenden Platte ebenfalls zu ausgeprägtem Vulkanismus.

Die eigentlichen Kontinentalblöcke oder Kontinentalschollen aus vorwiegend granitischem Material werden – zusammen mit den umgebenden Ozeanböden sowie dem jeweils darunter befindlichen lithosphärischen Mantel – wie auf einem langsamen Fließband von den Spreizungszonen weg und zu den Subduktionszonen hin geschoben. Nur eine Kollision zweier Kontinentalblöcke kann diese Bewegung aufhalten.

Da die kontinentale Kruste spezifisch leichter ist als die ozeanische Kruste, taucht sie an einer Subduktionszone nicht zusammen mit der ozeanischen Platte ab, sondern wölbt sich stattdessen zu einem Gebirgszug auf (Orogenese). Hierbei kommt es zu komplexen Deformationsvorgängen. Zwischen der Indischen und der Eurasischen Platte findet eine Kontinent-Kontinent-Kollision statt, die ebenfalls zur Gebirgsbildung führte (Himalaya).

Darüber hinaus können zwei Platten auch horizontal aneinander vorbeigleiten ("konservative Plattengrenze"). In diesem Fall wird die Plattengrenze als Transformstörung (Transformverwerfung) bezeichnet.

Das Lager, auf dem die Lithosphärenplatten gleiten, befindet sich im Grenzbereich zwischen der starren Lithosphäre und der darunterliegenden, extrem zäh fließenden Asthenosphäre (engl.: "Lithosphere-Asthenosphere Boundary", LAB). Die Ergebnisse seismischer Untersuchungen des Ozeanbodens im Westpazifik lassen darauf schließen, dass im Bereich der LAB zwischen 50 und 100 km Tiefe eine geringviskose Schicht existiert, die die mechanische Entkopplung der Lithosphäre von der Asthenosphäre erlaubt. Als Grund für die geringe Viskosität wird angenommen, dass der Mantel in diesem Bereich entweder teilweise aufgeschmolzen ist oder einen hohen Anteil flüchtiger Stoffe (hauptsächlich Wasser) aufweist.

Während früher die Reibung des konvektiven Mantels (engl.: "convective drag") an der Basis der Lithosphärenplatten als die wichtigste Triebkraft der Plattentektonik betrachtet wurde, gelten heute eher die von den Platten selbst ausgehenden Kräfte als die entscheidenden. Der sogenannte "Ridge Push" („Rückendruck“) geht von der jungen, warmen, auf dem Mantel „aufschwimmenden“ und daher hoch aufragenden Kruste der Mittelozeanischen Rücken aus, die einen horizontal von den Spreizungszonen weg gerichteten Druck erzeugt. Der "Slab Pull" („Plattenzug“) ist der Zug, den alte, kalte Lithosphäre erzeugt, wenn sie an Subduktionszonen in den konvektiven Erdmantel eintaucht. Durch Gesteinsumwandlungen subduzierter ozeanischer Kruste in größerer Manteltiefe erhöht sich die Dichte des Krustengesteins und bleibt höher als die Dichte des sie umgebenden Mantelmaterials. Dadurch kann der Zug auf den noch nicht subduzierten Teil der entsprechenden Lithosphärenplatte aufrechterhalten werden.

Nachdem einige Forscher bereits ähnliche Gedanken geäußert hatten, war es vor allem Alfred Wegener, der in seinem 1915 veröffentlichten Buch "Die Entstehung der Kontinente und Ozeane" aus der teilweise sehr genauen Passung der Küstenlinien auf beiden Seiten des Atlantiks folgerte, dass die heutigen Kontinente Teile eines großen Urkontinents gewesen sein müssen, der in der erdgeschichtlichen Vergangenheit auseinandergebrochen war. Die Passung ist noch genauer, wenn man nicht die Küstenlinien, sondern die Schelfränder, das heißt die untermeerischen Begrenzungen der Kontinente betrachtet. Wegener nannte diesen Urkontinent Pangaea und den Prozess des Auseinanderbrechens und Auseinanderstrebens seiner Bruchstücke "Kontinentaldrift". Wegener sammelte zwar viele weitere Belege für seine Theorie, jedoch konnte er keine überzeugenden Ursachen für die Kontinentaldrift benennen. Eine vielversprechende Hypothese kam von Arthur Holmes (1928), der vorschlug, dass Wärmeströme im Erdinneren genügend Kraft erzeugen könnten, um die Erdplatten zu bewegen. Zu diesem Zeitpunkt konnte sich seine Hypothese jedoch nicht durchsetzen.

Der Paradigmenwechsel zum Mobilismus setzte deshalb erst etwa um 1960 vor allem durch die Arbeiten von Harry Hammond Hess, Robert S. Dietz, Bruce C. Heezen, Marie Tharp, John Tuzo Wilson und Samuel Warren Carey ein, als man grundlegend neue Erkenntnisse über die Geologie der Ozeanböden erlangte.

Die einleuchtendste Erklärung für diese Phänomene war, dass der ständige Austritt basaltischen Magmas an den langgezogenen mittelozeanischen Bruchzonen Teil eines Prozesses ist, durch welchen der Ozeanboden in entgegengesetzte Richtungen auseinandergedrückt wird, sodass er sich im Laufe der Zeit immer weiter ausdehnt (Seafloor Spreading).

Da es keine Anzeichen dafür gibt, dass sich der Radius der Erde im Laufe ihres Bestehens kontinuierlich vergrößert, wie es z. B. in Careys Expansionstheorie gefordert wurde, liegt der Gedanke nahe, dass die in Form ozeanischer Kruste neu gebildete Erdoberfläche an anderer Stelle wieder verschwinden muss. Dieser Ansatz wird durch die Tatsache gestützt, dass sich in den heutigen Ozeanen (abgesehen von tektonischen Sonderpositionen wie im Mittelmeer) keine Lithosphäre findet, die älter ist als 200 Millionen Jahre (Mesozoikum). Die Hälfte der Meeresböden aller Ozeane ist nicht einmal älter als 65 Millionen Jahre (Känozoikum). Dadurch wurde die ursprüngliche Vorstellung widerlegt, nach der die Ozeane uralte Vertiefungen seien, die sich zusammen mit den Kontinenten schon bei der Formung der ersten festen Kruste um die glutflüssige Urerde gebildet hatten. Stattdessen bestehen die Ozeanböden, verglichen mit den Kontinenten, aus geologisch außerordentlich jungen Gesteinen. Unter Berücksichtigung der kontinuierlichen Ozeanbodenbildung an den Mittelozeanischen Rücken, kann hieraus zudem der Rückschluss gezogen werden, dass vor dem Mesozoikum gebildete Ozeanböden wieder von der Erdoberfläche verschwunden sein müssen.

Als Ort des Verschwindens von ozeanischer Lithosphäre wurden in den 1970er Jahren die Tiefseerinnen erkannt, die vor allem den Pazifischen Ozean umgeben. Wegen der damit verbundenen starken seismischen und vulkanischen Aktivität wird diese Zone auch als Pazifischer Feuerring bezeichnet.

Die neuen Methoden der Satellitengeodäsie und des VLBI, die sich in den 1990ern der Zentimeter-Genauigkeit näherten, liefern einen direkten Nachweis der Kontinentaldrift. Die Geschwindigkeit der Ozeanboden-Spreizung beträgt einige Zentimeter pro Jahr, variiert aber zwischen den einzelnen Ozeanen. Die geodätisch ermittelten Driftraten zwischen den großen Platten liegen zwischen 2 und 20 cm pro Jahr und stimmen mit den geophysikalischen NUVEL-Modellen weitgehend überein.

Neben Wegeners Theorie der Kontinentaldrift enthält die Plattentektonik auch Elemente der Unterströmungstheorie von Otto Ampferer (siehe auch Geschichte der Geologie, Permanenztheorie).

Im Gegensatz zu der klassischen Geosynklinal-Theorie geht man heute davon aus, dass die meisten gebirgsbildenden und vulkanischen Prozesse an die Plattenränder bzw. Plattengrenzen gebunden sind. Hier entstehen als Begleiterscheinungen der sich bewegenden Platten für den Menschen bedeutsame Naturphänomene wie Vulkanausbrüche, Erdbeben und Tsunamis.

Es gibt „einfache“ Plattengrenzen, an denen zwei tektonische Platten zusammentreffen und Tripelpunkte, an denen drei tektonische Platten zusammentreffen. Nicht an Plattengrenzen gebunden sind Hotspots, die durch thermische Anomalien im unteren Erdmantel verursacht werden.

Das Auseinanderdriften zweier Platten nennt man "Divergenz". Hier entsteht neue Lithosphäre.

Die Mittelozeanischen Rücken (MOR) werden (als sogenannte Rücken und Schwellen) mit einer Gesamtlänge von rund 70.000 km als die größten zusammenhängenden Gebirgssysteme des Planeten Erde angesehen.

Die Flanken der MOR steigen relativ sanft an. Die Kammregion weist oft über weite Strecken Einsenkungen auf – den "Zentralen Graben". An der Längsachse der MOR erfolgt die eigentliche Neubildung von Erdkruste bzw. Lithosphäre, indem dort große Mengen an größtenteils basaltischem Magma ausschmelzen, aufsteigen und kristallisieren. Nur ein kleiner Bruchteil erreicht hierbei als Lava den Meeresboden. Die junge Lithosphäre mit den frisch auskristallisierten Krustengesteinen hat im Vergleich zu älterer Lithosphäre eine geringere Dichte. Dies ist ein Grund dafür, dass die MOR sich mehrere Tausend Meter über den benachbarten Ozeanboden erheben. Mit steigendem Alter der Lithosphäre steigt deren Dichte, weshalb der Ozeanboden mit wachsender Entfernung von der Längsachse der MOR zunehmend tiefer liegt. Quer zum Zentralgraben verlaufen Bruchzonen (siehe Konservative Plattengrenzen), an denen die einzelnen Abschnitte des MOR gegeneinander versetzt sind. Daher haben die MOR keine durchgehende Kammlinie.

Ein eigentümliches vulkanisches Phänomen, das an die Mittelozeanischen Rücken gebunden ist, sind die Schwarzen und Weißen Raucher – hydrothermale Schlote, an denen überhitztes, mineralgesättigtes Wasser austritt. Dabei kommt es an den Schwarzen Rauchern zur Ablagerung von Erzen, die dann sogenannte sedimentär-exhalative Lagerstätten bilden.

Auch Riftzonen wie der Ostafrikanische Graben, die als die erste Phase einer Ozeanbildung aufgefasst werden können, sind mit vulkanischer Aktivität verbunden. Allerdings handelt es sich nicht um konstruktive Plattengrenzen im eigentlichen Sinn. Die Plattendivergenz wird hier zu einem Großteil durch das Einsinken und Verkippen kontinentaler Krustenblöcke ausgeglichen. Charakteristisch ist die Aufwölbung der umgebenden kontinentalen Kruste, die aus der Aufheizung und damit einhergehenden Dichteabnahme der ausgedünnten Lithosphäre resultiert und sich in Form herausgehobener Grundgebirgsmassive äußert, welche die "Riftflankengebirge" "(Riftschultern)" des Grabensystems bilden.

Grabensysteme wie der Ostafrikanische Graben entstehen durch die Tätigkeit sogenannter Manteldiapire. Diese heizen die Lithosphäre auf, dünnen sie aus und wölben sie domartig auf. Die entstehenden Spannungen führen schließlich dazu, dass die Kruste nachgibt und sich dreistrahlige Grabensysteme, ausgehend von den domartigen Aufwölbungen, radial ausbreiten, wobei aufeinandergerichtete Riftstrahlen zusammenwachsen und ein langgestrecktes Grabensystem bilden. Die übrigen Äste des Riftsystems verkümmern. An den tiefreichenden Brüchen in der Kruste, die bei diesen Prozessen entstehen, steigt Magma auf, was für den typischen alkalischen Vulkanismus kontinentaler Riftzonen sorgt.

Bei zunehmender Ausweitung der Bruchzonen bilden sich schmale, langgezogene Meeresbecken, die, wie das Rote Meer, bereits mit ozeanischer Kruste unterlegt sind und sich mit der Zeit zu ausgedehnten Ozeanbecken ausweiten können.

Die gegeneinander gerichtete Bewegung zweier Platten wird Konvergenz genannt. Dabei findet entweder eine Überschiebung statt, bei der entlang einer Subduktionszone die dichtere unter die weniger dichte Platte geschoben wird (Subduktion), oder eine Kollision, bei der eine oder beide Platten in den Randbereichen gefaltet werden.

Der klassische Kordillerentyp der Kettengebirge findet sich über jenen Subduktionszonen, bei denen ozeanische Lithosphäre direkt unter kontinentale Lithosphäre subduziert wird, wie an der Westküste Südamerikas.

Durch das Abtauchen der ozeanischen Platte unter den Kontinentalblock befindet sich unmittelbar an der Subduktionsfront eine Tiefseerinne. Auf dem Kontinent entsteht durch den horizontalen Druck, den die subduzierte Platte ausübt, ein Faltengebirge, jedoch ohne ausgedehnte Deckenüberschiebungen. Die erhöhten Drücke und Temperaturen der Gebirgsbildung können zu Regional-Metamorphosen und Aufschmelzungen (Anatexis) in den betroffenen kontinentalen Krustenbereichen führen.

Innerhalb des Faltengebirges bildet sich ein vulkanischer Bogen aus. Dies geht darauf zurück, dass die subduzierte Platte im Gestein gebundene Fluide – insbesondere Wasser – mit in die Tiefe transportiert. Unter den dort vorherrschenden Druck- und Temperaturbedingungen kommt es zu Phasentransformationen im Gestein, wobei Wasser aus der abtauchenden Platte in den darüberliegenden Mantel abgegeben wird. Dadurch wird die Schmelztemperatur des Mantelgesteins verringert und es kommt zu einer Teilaufschmelzung. Die zunächst basaltische Schmelze steigt durch die darüberliegende Lithosphäre auf und differenziert sich dabei zum Teil gravitativ oder vermengt sich mit Krustenmaterial. Die resultierenden zähflüssigen andesitischen bis rhyolithischen Magmen können bis an die Oberfläche gelangen und rufen dort zum Teil hochexplosive vulkanische Eruptionen hervor. Die Anden als Typusregion der Anden-Typ-Subduktion sind entsprechend auch beispielhaft für den damit verbundenen Vulkanismus, der durch zahlreiche aktive Vulkane, wie z. B. den Cerro Hudson oder den Corcovado, aber auch durch weit verbreitete fossile Lavagesteine und Ignimbrite repräsentiert wird.

Bei der Kollision von ozeanischer mit kontinentaler Kruste wird der Ozeanboden nicht immer vollständig subduziert. Kleine Reste von Meeresbodensedimenten und basaltischem Material (Ophiolithe) werden zuweilen bei der Subduktion von ihrer Unterlage „abgeschabt“ (abgeschert) und versinken nicht im Oberen Mantel. Stattdessen werden sie, keilförmig auf den Kontinentalrand aufgeschoben (obduziert) und in das Kettengebirge und damit die kontinentale Kruste integriert. Da sie der Subduktionsfront am nächsten sind, erfahren sie den höchsten Druck und werden zusammen mit den übrigen Gesteinen des Kontinentalrandes gefaltet und einer Hochdruck-Niedrig-Temperatur-Metamorphose unterzogen.

Am Westrand des Pazifiks sowie in der Karibik wird ozeanische Kruste unter andere ozeanische Kruste subduziert. Auch dort bilden sich Tiefseerinnen und vulkanische Bögen. Letztere heißen Inselbögen, weil nur die höchsten Teile der Vulkanbögen oberhalb des Meeresspiegels liegen. Die Bogenform ist auf das geometrische Verhalten einer Kugeloberfläche, wie der Erdkruste, beim Abknicken und Untertauchen eines Plattenteils zurückzuführen. Die konvexe Seite des Bogens weist dabei stets in Richtung der subduzierten Platte. Beispiele sind die Marianen, die Alëuten, die Kurilen oder die japanischen Inseln sowie die Kleinen und Großen Antillen.

Typisch für Subduktionszonen vom Marianen-Typ sind sogenannte Backarc-Becken (von engl. "back" für ‚hinter‘ und "arc" für ‚Bogen‘). Der Name verweist darauf, dass sich diese Dehnungszonen in der Kruste hinter dem Inselbogen (von der subduzierten Platte aus gesehen) befinden.

Wenn die ozeanische Kruste zwischen zwei Kontinentalblöcken vollständig subduziert worden ist, kommt es zum Kollisionstyp der Gebirgsbildung, wie bspw. im Fall des Himalayas durch den Zusammenstoß des indischen Subkontinents mit der Eurasischen Platte. Bei einem solchen Zusammenstoß wird die Lithosphäre durch die Bildung ausgedehnter tektonischer Decken enorm verdickt. Nach einer mehrphasigen Gebirgsbildung (Orogenese), d. h. zeitlich versetzten Zusammenstößen mehrerer Kleinkontinente oder vulkanischer Inselbögen (sog. Terrane) mit einem größeren Kontinentalblock und zwischenzeitlichen Subduktionsphasen, zeigen die erhaltenen Ophiolithzonen die Grenze zwischen den einzelnen Kleinkontinentalblöcken an (s. a." Geosutur"). Sowohl an der West- als auch an der Ostküste Nordamerikas finden sich Anzeichen, dass der nordamerikanische Kontinent durch solche mehrphasigen Orogenesen im Laufe seiner geologischen Geschichte immer mehr Kruste ansetzte.

Das Bild kann bei schrägem Aufeinandertreffen der Blöcke, wie bei der Apenninhalbinsel im Mittelmeer, noch komplizierter werden. So gibt es Indizien, dass ozeanische Mittelmeerkruste zeitweilig sowohl unter die Afrikanische als auch unter die Eurasische Platte subduziert wurde, während die Iberische Halbinsel, der Sardokorsische Block und die Apenninhalbinsel zwischen den großen Kontinentalblöcken gegen den Uhrzeigersinn rotierten.

An konservativen Plattengrenzen oder Transform-Störungen wird Lithosphäre weder neu gebildet noch subduziert, denn die Lithosphärenplatten „gleiten“ hier aneinander vorbei. An und nahe der Erdoberfläche, wo die Gesteine spröde sind, ist eine solche Plattengrenze als Blattverschiebung ausgebildet. Mit zunehmender Tiefe ist das Gestein infolge der hohen Temperaturen nicht spröde, sondern hochviskos, d. h., es verhält sich wie eine extrem zähe Masse. Daher geht die Blattverschiebung in größerer Tiefe in eine sogenannte duktile Scherzone über.

Transform-Störungen in kontinentaler Kruste können eine beachtliche Länge erreichen und gehören, wie alle Plattengrenzen, zu den Erdbebenschwerpunkten. Bekannte Beispiele sind die San-Andreas-Verwerfung in Kalifornien oder die Nordanatolische Verwerfung in der Türkei.

An den Mittelozeanischen Rücken (MOR) gibt es nicht nur vulkanisch aktive Längsgräben, sondern auch querlaufende Störungen, bei denen es sich ebenfalls um Blattverschiebungen bzw. Scherzonen handelt. Diese zerschneiden die Flanken der MOR in unregelmäßigen Abständen und teilen den Rücken in einzelne, gegeneinander versetzte Abschnitte. Allerdings sind nur die Bereiche der Störungen, die zwischen den Zentralgräben zweier benachbarter MOR-Abschnitte verlaufen, tatsächlich auch konservative Plattengrenzen und damit Transformstörungen im eigentlichen Sinn. Auch die Transformstörungen der MOR sind seismisch aktiv.

Hotspot-Vulkanismus steht nicht unmittelbar mit der Plattentektonik in Zusammenhang und ist nicht an Plattengrenzen gebunden. Stattdessen wird aus Quellen im tieferen Mantel heißes Material in Form sogenannter Manteldiapire oder Plumes in den Oberen Mantel gefördert, wo aus diesem Material basaltische Magmen mit charakteristischer chemischer Zusammensetzung herausschmelzen, die als "Ocean Island Basalts" (OIBs, „Ozeaninsel-Basalte“) den Meeresgrund bzw. die Erdoberfläche erreichen. Als Paradebeispiel für Hotspot-Vulkanismus gilt die Insel Hawaii, die mitten auf der Pazifischen Platte liegt. Die Hawaii-Inselkette (bis einschließlich Midway und Kure) und ihre untermeerische Fortsetzung, der Emperor-Rücken, sind dadurch entstanden, dass die ozeanische Lithosphäre kontinuierlich über einen Hotspot geglitten ist, dessen Magmen in regelmäßigen Abständen den Ozeanboden durchschlagen haben. Da Hotspots traditionell als ortsfest gelten, wurden aus dem Verlauf solcher Vulkanketten und dem Alter des Lavagesteins ihrer Vulkane die Bewegungsrichtung und die Geschwindigkeit von Lithosphärenplatten rekonstruiert.

Zumindest für den Hawaii-Emperor-Rücken lassen neue Erkenntnisse vermuten, dass es sich dort nicht um einen stationären, sondern um einen beweglichen Hotspot handelt. Wissenschaftler untersuchten paläomagnetische Daten in Basalten mehrerer untermeerischer Berge (englisch: ), d. h. vormaliger Vulkaninseln, des Hawaii-Emperor-Rückens, die Hinweise auf die geographische Breite liefern, in der die Lava seinerzeit erstarrte („Paläobreite“). Die Ergebnisse der Analyse zeigten, dass mit zunehmendem Alter des Gesteins auch die Paläobreite zunimmt, was nahelegt, dass der Hotspot nicht stationär war, sondern im Laufe der letzten 80 Millionen Jahre eine Eigenbewegung nach Süden vollzogen hat, und zwar mit einer mittleren Geschwindigkeit von 4 cm pro Jahr. Da diese Geschwindigkeiten in der gleichen Größenordnung liegen, wie die Plattengeschwindigkeiten (Pazifische Platte aktuell ca. 10 cm pro Jahr) sind mögliche Eigenbewegungen von Hotspots bei Berechnungen der Bewegungsrichtung und der Geschwindigkeit von Lithosphärenplatten anhand von Altersdaten von Hotspotvulkanketten zu berücksichtigen.

Auch unter Island befindet sich ein Hotspot. Dort liegt jedoch der Sonderfall vor, dass der Hotspot-Vulkanismus mit dem Vulkanismus eines Mittelozeanischen Rückens zusammenfällt.

Wenn die Realität der Kontinentaldrift unter Geowissenschaftlern auch kaum noch bezweifelt wird, so besteht über die Kräfte im Erdinnern, die die Bewegungen der Platten auslösen und vorantreiben, noch fast so viel Unklarheit wie zu Zeiten Wegeners (siehe hierzu auch Mantelkonvektion). Die beiden hier angeführten Theorien galten lange Zeit als gegensätzlich und miteinander unvereinbar. Nach heutiger Sicht werden sie immer mehr als einander ergänzend angesehen.

Die heute am meisten vertretene Meinung geht von langsamen Konvektionsströmen aus, die sich durch den Wärmeübergang zwischen dem heißen Erdkern und dem Erdmantel ergeben. Der Erdmantel wird hierbei von unten aufgeheizt. Die Energie für die Aufheizung des Mantelmaterials könnte nach einer Modellvorstellung noch von der Akkretionsenergie herrühren, die bei der Entstehung der Erde frei wurde. Zum Teil tragen auch radioaktive Zerfallsprozesse zur Aufheizung bei. Die Reibungsenergie der Gezeitenwirkung des Mondes auf den Erdkörper kann wohl vernachlässigt werden. Allerdings bilden Konvektionsströme unter Laborbedingungen, zum Beispiel in erhitzten zähen Flüssigkeiten, sehr hoch strukturierte und symmetrische Formen aus, die z. B. eine Wabenstruktur haben. Dies lässt sich kaum mit der tatsächlich beobachteten Gestalt der geotektonischen Platten und ihren Bewegungen vereinbaren.

Eine andere Theorie geht von nur zwei sich gegenüber liegenden Konvektionszentren aus. Eine heute dominante Zelle läge unter Afrika, was das dortige Vorherrschen von Dehnungsbrüchen und das Fehlen einer Subduktionszone am Rand der Afrikanischen Platte erklären würde. Die andere Konvektionszelle läge auf der Gegenseite des Globus – unter der Pazifischen Platte, die ständig an Größe verliert. Der Pazifik, der interessanterweise keinerlei kontinentale Kruste beinhaltet, wäre somit der Überrest eines urzeitlichen Superozeans Panthalassa, der einst Pangaea umschlossen habe. Erst wenn sich im Gebiet des heutigen Pazifik alle Kontinente wieder zu einem neuen Superkontinent vereinigt hätten, würde sich die Bewegung umkehren (Wilson-Zyklus). Die neue Pangaea würde wieder auseinanderbrechen, um den neuen Superozean, der sich aus Atlantik, Indischem und Arktischem Ozean gebildet hätte, ein weiteres Mal zu schließen.

Andere Autoren sehen die Platten nicht nur passiv auf dem Mantel liegen. So nehmen die Mächtigkeit und die Dichte einer ozeanischen Lithosphärenplatte stetig zu, während sie sich vom Mittelozeanischen Rücken entfernt und abkühlt, wodurch sie bereits ein wenig in den Mantel einsinkt und dadurch leichter von der Oberplatte überschoben werden kann. Nach dem Abtauchen unter die Oberplatte wird das subduzierte Gestein schließlich unter den Druck- und Temperaturbedingungen bei zunehmender Tiefe in Gestein höherer Dichte umgewandelt. So wird aus dem Basalt der ozeanischen Kruste schließlich Eklogit, wodurch die Dichte der subduzierten Platte die Dichte des umliegenden Erdmantels übersteigt. Deshalb wird die bei der Subduktion in den Mantel sinkende Platte durch ihr eigenes Gewicht tiefer gezogen, wobei Plattenmaterial im Extremfall bis nahe an den unteren Rand des Erdmantels sinken kann. Die auf die Lithosphärenplatte ausgeübte Kraft wird Plattenzug genannt (engl. "slab pull", von "pull" ‚ziehen‘; "slab" ‚Platte‘). Eine etwa um den Faktor 10 kleinere Kraft entsteht darüber hinaus an der dem Mittelozeanischen Rücken zugewandten Seite einer Lithosphärenplatte, da die dort aufgewölbte Kruste eine Hangabtriebskraft erfährt, den Rückendruck (engl. "ridge push", von "ridge" ‚Rücken‘ und "push" ‚drücken‘). Auch auf die gegenüberliegende, nicht in den Mantel sinkende Platte wirkt in einer Subduktionszone eine Kraft, eine Zugspannung. Mit welcher Geschwindigkeit sich eine ozeanische Lithosphärenplatte allerdings tatsächlich bewegt, hängt auch von der Größe der Gegenkräfte ab.

Nach dem bisherigen Stand der Forschung scheint der Mechanismus der Plattentektonik nur auf der Erde wirksam zu sein. Das ist für den kleinen Planeten Merkur und für die großen Monde der Gasplaneten und den Erdmond noch plausibel. Die Lithosphäre dieser relativ zur Erde viel kleineren Himmelskörper ist im Verhältnis zu mächtig, um in Form von Platten mobil sein zu können. Allerdings zeigt die Kruste des Jupitermondes Ganymed Ansätze einer zum Erliegen gekommenen Plattentektonik. Bei der fast erdgroßen Venus ist wiederum schwer zu verstehen, warum eine Plattentektonik trotz starkem Vulkanismus nicht in Gang gekommen sein dürfte. Eine erhebliche Rolle könnte dabei das nur auf der Erde vorkommende freie Wasser spielen. Offensichtlich dient es hier bis hinab auf die Kristallgitterebene als reibungsminderndes „Schmiermittel“. An den Subduktionszonen der Erde werden im Porenraum der Sedimente des Ozeanbodens Milliarden Tonnen Wasser mit in die Tiefe gezogen, das den überliegenden Erdmantel partiell aufschmilzt. Auf der Venus sind flüssiges Wasser und folglich Meere zumindest heute nicht mehr vorhanden.

Der Mars dagegen scheint eine Zwischenstellung zu besitzen. Wasser bzw. Eis ist vorhanden, und man meint, Ansätze einer Plattentektonik erkennen zu können. Die aufgereihten gigantischen Schildvulkane und Grabensysteme, die den halben Planeten umspannen, erinnern in gewisser Weise an das Rifting auf der Erde. Dem steht wiederum das Fehlen von eindeutigen Verschluckungszonen gegenüber. Wahrscheinlich reichte die innere Hitzeentwicklung und die daraus folgende Konvektion auf diesem relativ kleinen Planeten nicht ganz aus, um den Mechanismus wirklich in Gang zu setzen, oder der Vorgang kam bereits in der Frühgeschichte des Planeten wieder zum Stillstand.

Ob eine Art Plattentektonik auf anders aufgebauten Himmelskörpern stattfindet, ist nicht bekannt, aber vorstellbar. Als Kandidaten für konvektionsgetriebene weiträumige horizontale Krustenverschiebungen können die Monde Europa und Enceladus gelten. Der knapp erdmondgroße Europa weist einen Eispanzer von etwa 100 km Dicke über einem felsigen Mondkörper auf, der in den unteren Bereichen teilweise oder vollständig aufgeschmolzen sein könnte, so dass der Eispanzer möglicherweise wie Packeis auf einem Ozean schwimmt. Der nur etwa 500 km kleine Enceladus wird wahrscheinlich durch Gezeitenkräfte aufgeheizt. Flüssiges Wasser oder durch hohen Druck duktiles Eis könnte bei beiden Himmelskörpern an tiefreichenden Störungen aufsteigen und das spröde Eis der Kruste zur Seite drücken, was wiederum folgen ließe, dass andernorts Kruste verschluckt werden müsste. Die Oberfläche dieser Monde ist jedenfalls geologisch aktiv oder zumindest aktiv gewesen und zeigt Anzeichen dafür, dass dort eine Krustenerneuerung stattfand. Der Vulkanismus auf Io dagegen scheint derartig stark zu sein, dass stabile Krustenbereiche in der Art der Platten erst gar nicht entstanden sind.





</doc>
<doc id="3971" url="https://de.wikipedia.org/wiki?curid=3971" title="Jean-François Pilâtre de Rozier">
Jean-François Pilâtre de Rozier

Jean-François Pilâtre de Rozier (* 30. März 1754 in Metz; † 15. Juni 1785 in Wimereux, Département Pas-de-Calais) war ein französischer Physiker, der erste Luftfahrtpionier und zusammen mit Pierre Romain das erste Todesopfer der Luftfahrtgeschichte.

Jean-François Pilâtre de Rozier wurde in Metz als dritter Sohn von Magdeleine Wilmard und Mathurin Pilastre geboren. Sein Vater, der den Beinamen „du Rosier“ trug, war ein ehemaliger Soldat, der Wirt geworden war. Im Militärhospital seiner Heimatstadt Metz, einer wichtigen Garnison an der Grenze Frankreichs, wurde Pilâtres Interesse an der Chemie von Drogen geweckt. Er ging mit achtzehn Jahren nach Paris, arbeitete dort als Apotheker und hörte daneben Vorlesungen über Mathematik, Physik und Naturgeschichte. Danach lehrte er Physik und Chemie an der Akademie in Reims, wodurch er die Aufmerksamkeit des Grafen von Provence erregte, dem späteren Ludwig XVIII. und Bruder von König Ludwig XVI.

Pilâtre kehrte nach Paris zurück, wo er mit dem naturgeschichtlichen Kuriositätenkabinett des Grafen beauftragt wurde. Zudem wurde er Höfling der Gattin des Grafen, was ihm den Adelsnamen „Pilâtre de Rozier“ einbrachte. Er eröffnete im Pariser Quartier Marais am 11. Dezember 1781 sein eigenes Museum, das "Musée technique", wo er physikalische Experimente vornahm und seinen adligen Besuchern Demonstrationen vorführte. Er forschte auf dem neuen Gebiet der Gase und erfand ein Beatmungsgerät (Respirator).

Am 15. Oktober 1783 gelang dem damals 29-jährigen Physiker mit königlicher Bewilligung die Fahrt in einer Montgolfière, einem Heißluftballon der Brüder Montgolfier, in der er eine Höhe von etwa 26 Metern erreichte. Damit gelang ihm die erste historisch gesicherte bemannte Luftfahrt der Menschheit, der Ballon war jedoch noch mit Seilen am Boden verankert.

Am 21. November 1783 führte Pilâtre die erste Freiballonfahrt in der Geschichte der Menschheit durch. Bei der 25-minütigen Fahrt schwebte er zusammen mit François d’Arlandes nach dem Start vom Schlossgarten des Schlosses La Muette in Passy, heute ein Stadtteil von Paris, über die Seine hinweg etwa 10 km weit bis zur Butte aux Cailles in der Gemeinde Gentilly, wenige Kilometer vom Zentrum der französischen Hauptstadt entfernt (heute Teil von deren 13. Arrondissement). Über diese Luftreise berichtete er in der Schrift "Première expérience de la Montgolfière" (1784).

Er entwickelte die nach ihm benannte Rozière, eine Kombination aus Wasserstoffballon und Heißluftballon. Am 15. Juni 1785 startete er mit einem solchen Heißluft-Gas-Hybrid-Ballon von Boulogne-sur-Mer aus in Richtung Großbritannien. Nach 5 km Fahrt entzündete sich in 900 Metern Höhe der Wasserstoff und zerstörte die Ballonhülle, woraufhin die Gondel in die Tiefe stürzte. Pilâtre de Rozier und sein Mitfahrer Pierre Romain kamen beim Absturz ums Leben. Sie waren die ersten Todesopfer der Luftfahrt.

Der Mondkrater Pilâtre wurde 1991 nach ihm benannt. Zudem ist er Namensgeber für den Rozier-Gletscher in der Antarktis.




</doc>
<doc id="3972" url="https://de.wikipedia.org/wiki?curid=3972" title="Per Anhalter durch die Galaxis">
Per Anhalter durch die Galaxis

Per Anhalter durch die Galaxis, Originaltitel "The Hitchhiker’s Guide to the Galaxy", auch Per Anhalter ins All, abgekürzt HHGTTG, HHGG oder H2G2, ist das bekannteste Werk des britischen Schriftstellers Douglas Adams. Es handelt sich um eine Mischung aus Komödie, Satire und Science Fiction, die zunächst als Hörspielserie vom BBC-Radio ausgestrahlt wurde. Im Anschluss entwickelte der Autor das Werk zu einer Serie von Romanen weiter. Außerdem wurde der Stoff als Fernsehserie, Computerspiel, Musical, Theaterstück und als Kinofilm umgesetzt. "Per Anhalter durch die Galaxis" ist dabei gleichzeitig der Titel des ersten Buches und der Name der Serie. Die Geschichte erlangte schon früh Kultcharakter, wobei der charakteristische Humor des Autors besonderen Anklang fand.

Ursprünglich im Jahr 1978 als Hörspielserie für die BBC geschrieben, entwickelte Adams die Geschichte weiter. Es entstand eine Romanreihe, deren fünf Teile in den Jahren 1979 bis 1992 erschienen. Außerdem wurde 1981 – ebenfalls für die BBC – eine Fernsehserie produziert, die später in stark gekürzter Form ins Deutsche übersetzt und ausgestrahlt wurde. In Zusammenarbeit mit Douglas Adams erfolgte 1984 eine Umsetzung der Geschichte für ein Computerspiel.

Die grundsätzliche Idee zu seiner Geschichte hatte Adams nach eigenen Angaben, als er einmal betrunken bei Innsbruck (Österreich) in einem Acker lag, das Buch "Per Anhalter durch Europa" las und sein Blick auf den Sternenhimmel fiel. → "Zitat im Artikel: Douglas Adams"

Obwohl die verschiedenen Adaptionen – Radio, Bücher, Computerspiel und Fernsehserie – im Großen und Ganzen die gleiche Handlung beschreiben, gibt es doch zahlreiche Variationen in einzelnen Punkten; es gibt sogar einige widersprüchliche Elemente und Varianten.

Die Geschichte, die allen Versionen zugrunde liegt, sind die Abenteuer des Arthur Dent, eines Durchschnittsengländers, der mit knapper Not und mit Hilfe seines Freundes Ford Prefect der totalen Zerstörung des Planeten Erde durch eine außerirdische Rasse namens Vogonen entgeht. Zu Arthurs Verblüffung erweist sich sein Freund als außerirdischer Besucher, der die Erde zu Recherchezwecken wegen eines galaktischen Nachschlagewerks bereiste und zu dessen Unglück sich für einige Jahre keine Weiterreisemöglichkeit ergeben hatte. Als die Flotte der Vogonen auftaucht, um die Erde zwecks Baus einer galaktischen Hyperraum-Expressroute zu zerstören, nutzt Ford die Gelegenheit und bringt sich und Arthur mittels „Subraum-Äther-Winker“ (Sub-Etha-Sens-O-Matic, eine Art elektronischer Daumen) sozusagen per Anhalter an Bord eines der Vogonenraumschiffe.

Vogonen zeichnen sich jedoch nicht durch besondere Gastfreundlichkeit aus, und so werden Arthur und Ford durch eine Luftschleuse ins All befördert. Anstatt nun im Weltraum umzukommen, werden die beiden in letzter Sekunde vom Raumschiff "Herz aus Gold" gerettet. Dort treffen Arthur und Ford die anderen Hauptfiguren der Geschichte: Zaphod Beeblebrox, Halbcousin von Ford und zeitweise Präsident der Galaxis. Zaphod hat gemeinsam mit Tricia McMillan (genannt „Trillian“), einer Frau, die Arthur einmal auf einer Party in einer Wohnung in Islington kennengelernt hatte, das Raumschiff gestohlen. Mit an Bord ist außerdem Marvin, ein depressiver Roboter. Gemeinsam begeben sie sich auf die Suche nach dem legendären Planeten Magrathea, den sie auch finden.

Auf Magrathea treffen sie auf Slartibartfaß, der ihnen erklärt, dass die Erde in Wahrheit ein Supercomputer war, an dem er mitgearbeitet hat. Der einzige Zweck der Erde bestand darin, die Frage nach dem Sinn des Lebens, dem Universum und dem ganzen Rest zu finden. Die Antwort auf die Frage lautete „42“ und wurde nach siebeneinhalb Millionen Jahren Rechenzeit von einer früheren Version des Supercomputers "Deep Thought" errechnet, der sich allerdings außerstande sah, die zugrunde liegende Frage zu formulieren.

Nachdem die Vogonen die Erde fünf Minuten vor Ablauf des 10 Millionen Jahre dauernden Programms zerstört haben, versuchen die Auftraggeber, eine Rasse hyperintelligenter, pandimensionaler Lebewesen in Gestalt von weißen Mäusen, Arthur sein Gehirn abzukaufen, da sie es für einen Teil der Computermatrix halten und hoffen, die Frage herausfiltern zu können. Nachdem Arthur abgelehnt hat, versuchen die Mäuse, ihm das Gehirn mit Gewalt zu entnehmen. Genau in diesem Moment trifft allerdings die galaktische Polizei auf der Suche nach Zaphod ein. In der entstehenden Verwirrung können Arthur, Ford, Zaphod und Trillian zur "Herz aus Gold" flüchten und beschließen, erst einmal etwas zu essen, wodurch Adams zum zweiten Teil der Romanreihe, "Das Restaurant am Ende des Universums", überleitet.

In der Hörspiel- und Fernsehserie ist dieses Ende etwas anders: Die Polizisten zerstören einen Computer, hinter dem sich die Protagonisten verstecken. Durch die folgende Explosion werden diese durch die Zeit direkt ins Milliway's geschleudert, womit die Handlung erst bei Kapitel 14 des Buches beginnt.

"The Hitchhiker’s Guide to the Galaxy" wurde zuerst in zwei Staffeln als Hörspielserie für die BBC geschrieben. Die erste Folge lief am 8. März 1978 bei BBC Radio 4. Nach der Produktion der Hörspielserie brachte Adams die Geschichte in Romanform, wodurch die ersten beiden Bände entstanden. In den folgenden Jahrzehnten schrieb er noch drei weitere Bücher, auf deren Grundlage nach seinem Tod wiederum drei weitere Staffeln der Hörspielserie produziert und in den Jahren 2004 und 2005 ausgestrahlt wurden. Viele Sprecher der ursprünglichen Serie konnten dafür wieder verpflichtet werden. Die Titelmusik ist das Eagles-Stück "Journey of the Sorcerer".

Unter dem Titel "Per Anhalter ins All" wurde 1981/82 eine deutsche Hörspielvariante vom BR, SDR und WDR unter Regie von Ernst Wendt produziert, die die sechs Folgen der ersten englischen Radioserie umfasst. 1990/91 wurden dann vom BR und SWF weitere Teile unter Regie von Hartmut Kirste aufgenommen, die den Inhalt des dritten und vierten Buches umfassen. Die Titelmusik stammt von Frank Duval.

Später entwickelte Adams daraus einen Roman, den er in mehreren Bänden fortsetzte (engl. Originaltitel in Klammern):
Obwohl es am Ende fünf Bände waren, wurde das Gesamtwerk sowohl im englischen Original als auch in der deutschen Übersetzung durchgängig als Trilogie bezeichnet.
Die ersten vier Teile wurden von Benjamin Schwarz ins Deutsche übersetzt, der fünfte Teil von Sven Böttcher.

Zur Anhalter-Serie zählt ferner die Kurzgeschichte "Der junge Zaphod geht auf Nummer sicher" (Originaltitel „Young Zaphod Plays it Safe“), die als Verbindungsstück zwischen dem 4. und dem 5. Band angesehen werden kann. Die Geschichte liegt in zwei etwas unterschiedlichen Versionen vor und ist in verschiedenen Sammelbänden erschienen, u. a. im Nachlassband "Lachs im Zweifel" (The Salmon of Doubt), ISBN 3-453-40045-3.

Im Oktober 2009 erschien mit der Erlaubnis von Douglas Adams’ Witwe eine von Eoin Colfer geschriebene Fortsetzung unter dem Titel:

Aufgrund der hohen Beliebtheit des Hörspiels wurde 1981 eine sechsteilige Fernsehserie produziert, in der die meisten Akteure des Hörspiels mitspielten (Ausnahmen: David Dixon als Ford Prefect und Sandra Dickinson als Trillian). Die Serie basiert auf den sechs Folgen der ersten Radioserie, welche grob den Inhalt der ersten beiden Bücher ("Per Anhalter durch die Galaxis" und "Das Restaurant am Ende des Universums") umfasst. Geplant war eine zweite Serie auf Grundlage von Douglas Adams’ Projekt "Doctor Who and the Krikkitmen", die aber wegen verschiedener Probleme nicht verwirklicht wurde. Aus dem Projekt wurde später Band drei "Das Leben, das Universum und der ganze Rest".

Die Serie ist auf zwei DVDs im Handel erhältlich.

1997 unterschrieb Douglas Adams einen Vertrag bei Disney/Spyglass, die damals seinen Kultroman in die Kinosäle bringen wollten. Regie sollte Jay Roach führen. Adams meinte 2001, dass er sich Hugh Laurie oder Hugh Grant für die Hauptrolle "Arthur Dent" gut vorstellen könnte.

Im Oktober 2003 wurde das von Fans lang erwartete Filmprojekt dann begonnen. Verfilmt wurde der Bestseller "Hitchhiker’s Guide to the Galaxy" von Regisseur Garth Jennings. Das damalige Drehbuch von Douglas Adams wurde noch einmal von Karey Kirkpatrick bearbeitet, der unter anderem durch den Animationsfilm "Chicken Run – Hennen rennen" bekannt ist. Die Hauptrolle Arthur Dent spielt Schauspieler Martin Freeman, Ford Prefect wird vom Musiker Mos Def umgesetzt, sein Cousin Zaphod Beeblebrox, der Präsident der Galaxis, vom amerikanischen Schauspieler Sam Rockwell, und in der Rolle der Trillian sieht man Zooey Deschanel. Im englischen Original leiht der britische Schauspieler Alan Rickman dem depressiven Roboter Marvin seine Stimme. Helen Mirren spricht den Super-Computer „Deep Thought“, und Stephen Fry, persönlicher Freund Adams’, ist der Erzähler, der den Film begleitet. In der Rolle des Slartibartfaß ist Bill Nighy zu sehen.

Extra für die Kinoversion fügte Douglas Adams eine neue Figur ins Drehbuch ein: Humma Kavula, den geistigen Führer der Jatravartiden. Er verlor die Wahl zum Präsidenten der Galaxis gegen Zaphod Beeblebrox. Die Rolle wurde John Malkovich auf den Leib geschneidert, der sie schließlich auch annahm.

Filmstart in Deutschland, Österreich und der Schweiz war der 9. Juni 2005.

Die Musik des Musicals ist auf CD erhältlich: MäGäDäm: "Don’t Panic". Produziert von MäGäDäm (Andrea Bongers, Jo Jacobs) und Tim Brettschneider. Hamburg 1997.

Das Linzer Theater Phönix setzte den Stoff der ersten beiden Bücher 2001 mit großem Erfolg als Theaterstück um. Regie führten Stefan Kurowski und Jürgen Heib.

Das Thema wurde auch in ein Textadventure-Computerspiel von der Firma Infocom umgesetzt (The Hitchhiker’s Guide to the Galaxy). Letzteres wurde vom Autor für die Öffentlichkeit freigegeben und kann daher online gespielt werden. Weiterhin wurden die ersten drei Anhalter-Bücher als Comics umgesetzt.

Anspielungen auf die Geschichte können auch immer wieder in verschiedensten Computerspielen (z. B. dient "Marvin" als Cheatcode in Gothic) oder Serien (z. B. Futurama) gefunden werden.

Beim Erstflug der Falcon-Heavy-Rakete des US-amerikanischen Raumfahrtunternehmens SpaceX prangte der Spruch "Don’t Panic" (Keine Panik) auf dem Armaturenbrett des Elektrosportwagens, der als Nutzlast in den Weltraum befördert wurde. Im Handschuhfach des Fahrzeugs befinden sich ein Exemplar des "Anhalters durch die Galaxis" und ein Handtuch. Der Roadster bewegt sich seitdem auf einer Umlaufbahn um die Sonne zwischen Erd- und Marsorbit. "Don’t Panic" ist in der Handlung des "Anhalters durch die Galaxis" eine Aufschrift auf dem Anhalter durch die Galaxis.






</doc>
<doc id="3973" url="https://de.wikipedia.org/wiki?curid=3973" title="Probus (Kaiser)">
Probus (Kaiser)

Marcus Aurelius Probus (* 19. August 232 in Sirmium, Unterpannonien, nahe dem heutigen Sremska Mitrovica; † 282 ebenda) war römischer Kaiser von 276 bis 282.

Probus wurde am 19. August 232 in Sirmium als Sohn eines Militärtribuns geboren. Er selbst schlug ebenfalls eine militärische Laufbahn ein, wobei er sich bei seinen Vorgesetzten und Truppen gleichermaßen Achtung verschaffte. Schon unter Aurelian verteidigte Probus Germanien gegen die Alamannen. Als Kaiser Tacitus 276 starb, hatte er das Truppenkommando im Orient inne. Im selben Jahr wurde er von seinen Truppen zum Gegenkaiser ausgerufen, woraufhin ihn Tacitus’ Bruder und Nachfolger Florianus angriff. Trotz zahlenmäßiger Unterlegenheit besiegte er Florianus offenbar durch geschicktes Taktieren, worauf dieser von seinen Truppen ermordet wurde. Nach dessen Tod fand er auch die Anerkennung des Senats, der ihn wohl als Kaiser bestätigte. Wahrscheinlich war dies das letzte Mal, dass ein Kaiser in dieser Weise auf den Senat Rücksicht nahm. Fast alle späteren Herrscher verzichteten auf die nominelle Anerkennung durch dieses Organ; der augusteische Prinzipat ging seinem Ende entgegen.

Probus, einer der illyrischen Kaiser, wurde von der spätantiken Geschichtsschreibung (Aurelius Victor, Historia Augusta) ungewöhnlich positiv beurteilt. Von 277 bis 282 bekleidete er fünfmal das Konsulat und führte mehrere Kriege. Es ist allerdings unklar, wie viele der ihm zugeschriebenen Siege tatsächlich errungen wurden. 277 scheint Probus zunächst die Goten im Gebiet von Pannonien, Moesia oder Thrakien besiegt zu haben, da er laut Münzfunden den Ehrentitel "Gothicus Maximus" annahm. Dann wandte er sich der Verteidigung Galliens zu, in das Germanen über den Rhein eingedrungen waren, da es nach dem Ende des Gallischen Sonderreichs schutzlos war. Im selben Jahr ließ er wohl die Franken hinter den Fluss zurückdrängen und am rechten Ufer Befestigungen und Brückenköpfe errichten. Zur gleichen Zeit überquerte er selbst den Neckar, um die eingefallenen Alamannen und Longioner zu vertreiben. Er besiegte sie und nahm ihren Anführer Semnon gefangen, dem er aber die Freiheit schenkte. Im Gegenzug mussten sie wie auch die Franken in ihre Heimat zurückkehren, ihre Beute abgeben und die gefangenen Römer ausliefern.
Am Ligys (wohl der Lech) schlug er danach die Burgunden, die zahlenmäßig überlegen waren, indem er sie taktisch klug zu einem unüberlegten Vorstoß verleitete. Auch mit ihnen schloss er ein Abkommen. Da sie ihre Gefangenen nicht ausliefern wollten, wie es vereinbart worden war, besiegte er sie ein zweites Mal. 10.000 der dabei gemachten Gefangenen siedelte er zur Verstärkung des Heeres nach Britannien um. Für seine Siege erhielt er den Titel "Germanicus Maximus". 278 besiegte er noch die Vandalen in Illyrien und Rätien, vermutlich am Lech, was ebenfalls auf einer Münze gefeiert wurde ("Restitutor Illyrici").

279 wandte sich Probus dem Osten des Reiches zu und bekämpfte rebellierende isaurische Stämme in Kleinasien unter ihrem Anführer Lydius. Als dieser im Kampf fiel, ergaben sie sich. Ferner wurden 279 die nubischen Blemmyer in Ägypten, die schon die Städte Koptos und Ptolemais eingenommen hatten, besiegt (Triumphzug 281). Damit waren die wichtigen Getreidelieferungen von Ägypten nach Rom vorerst gesichert. Als im Westen des Reichs Aufstände ausbrachen, verschaffte sich Probus Zeit, indem er mit den Sassaniden einen Waffenstillstand schloss. Dann schlug er den gallischen Aufstand nieder. Schließlich endeten 280/81 die Usurpationen des Saturninus in Syrien, des Proculus in Gallien und des Bonosus in Köln.

In Rom ließ er den Bau der Aurelianischen Mauer zum Abschluss bringen.

Probus wurde im September/Oktober 282 von unzufriedenen Soldaten in Sirmium ermordet. Hintergrund war angeblich, dass der Kaiser daran dachte, die Armee zu entlassen, nachdem relativer Frieden im Reich eingekehrt war. Ob diese völlig unglaubwürdige Nachricht einen realen Kern hat, ist unklar – selbst eine Verkleinerung des Heeres wäre zum fraglichen Zeitpunkt unklug gewesen. Denkbar ist allenfalls, dass der Sold verringert werden sollte, doch ist auch dies spekulativ. Fest steht: Der Kaiser verlor trotz seiner militärischen Erfolge den Rückhalt im Heer. Offensichtlich wurden die Legionäre von ihm mehr und mehr zu zivilen Aufgaben herangezogen, die bei den Truppen jedoch sehr unbeliebt waren. Schließlich schlugen sich die Soldaten offenbar auf die Seite des Usurpators Carus; Probus wurde von meuternden Legionären bei einer Inspektion erschlagen.

Probus’ Bruder Dometius und seine beiden Neffen Probus von Byzantium und Metrophanes von Konstantinopel waren angeblich christliche Bischöfe von Konstantinopel.

Trotz seiner kurzen Regierungszeit und obwohl man nicht viel über sein Leben weiß, gehört Probus in einigen Regionen heute zu den auch Laien bekannten römischen Kaisern. Dies rührt von einer Nachricht in der Probus-Biographie der (sehr problematischen) "Historia Augusta" her, der zufolge er allen Einwohnern Galliens, Hispaniens und Britanniens erlaubte, Reben zu besitzen und Wein herzustellen.

Deshalb gilt Probus in zahlreichen Weinbaugebieten nördlich der Alpen (zum Beispiel in Österreich und an der Mosel) als derjenige, der dort den Weinbau eingeführt hat. Sicher ist zwar, dass die Römer bereits lange vor Probus den Weinanbau in ihren Nordprovinzen eingeführt hatten, allerdings deutet zugleich vieles darauf hin, dass die Weinproduktion in dieser Region nach der Mitte des 3. Jahrhunderts deutlich an Bedeutung gewonnen hat. Eine Verbindung mit den Maßnahmen des Kaisers kann also nicht ausgeschlossen werden.

In Bad Godesberg erinnert eine "Probussäule" an den Kaiser.




</doc>
<doc id="3974" url="https://de.wikipedia.org/wiki?curid=3974" title="Paul von Hindenburg">
Paul von Hindenburg

Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg (* 2. Oktober 1847 in Posen; † 2. August 1934 auf Gut Neudeck, Ostpreußen) war ein deutscher Generalfeldmarschall und Politiker. Im Ersten Weltkrieg übte die von ihm geführte Oberste Heeresleitung von 1916 bis 1918 quasi diktatorisch die Regierungsgewalt aus. Hindenburg wurde 1925 zum zweiten Reichspräsidenten der Weimarer Republik gewählt. 1932 wurde er wiedergewählt. Am 30. Januar 1933 ernannte er Adolf Hitler zum Reichskanzler.

Paul von Hindenburg entstammte väterlicherseits einem alten ostpreußischen Adelsgeschlecht, der Familie von Beneckendorff und von Hindenburg. Er wurde 1847 als Sohn des preußischen Offiziers und Gutsbesitzers Hans Robert Ludwig von Beneckendorff und von Hindenburg (1816–1902) und seiner bürgerlichen Ehefrau Luise Schwickart (1825–1893) geboren. Sein elf Jahre jüngerer Bruder Bernhard von Hindenburg verfasste 1915 die erste Biografie des Feldmarschalls.

Zunächst war Paul von Hindenburg mit Irmengard von Rappard (1853–1871) aus Sögeln (Bramsche) verlobt, die jedoch vor der Hochzeit mit 17 Jahren an Schwindsucht verstarb (bis zu seinem Lebensende sandte er an jedem Todestag einen Kranz ans Grab). Am 24. September 1879 heirateten Hindenburg und Gertrud von Sperling (1860–1921). Aus dieser Ehe gingen die Kinder Irmengard Pauline Louise Gertrud (1880–1948), Oskar (1883–1960) und Annemarie Barbara Ilse Ursula Margarete Eleonore (1891–1978) hervor. Die ältere Tochter heiratete 1902 Hans Joachim von Brockhusen (1869–1928), die jüngere 1912 Christian von Pentz (1882–1952) und der Sohn 1921 Margarete von Marenholtz (1897–1988). Seinen Neffen Wolf von Beneckendorff (1891–1960), den späteren Schauspieler, adoptierte Hindenburg, nachdem dessen Eltern gestorben waren.

Als Sohn eines preußischen Offiziers beschritt Hindenburg ebenfalls die militärische Laufbahn. Nach jeweils zweijährigem Besuch der Bürgerschule (Grundschule) und des evangelischen Gymnasiums in Posen besuchte er von 1859 bis 1863 die Kadettenanstalt in Wahlstatt in Schlesien und ab Ostern 1863 die Hauptkadettenanstalt in Berlin. 1865 wurde er Königin Elisabeth, der Witwe des verstorbenen preußischen Königs Friedrich Wilhelm IV., als Leibpage zugeteilt. Im April 1866 wurde er als Leutnant in das 3. Garderegiment zu Fuß aufgenommen und nahm an der Schlacht von Königgrätz teil.

Hindenburg kämpfte 1870/71 im Deutsch-Französischen Krieg. Am 18. Januar 1871 repräsentierte er sein Garderegiment bei der Kaiserproklamation im Spiegelsaal des Schlosses von Versailles. Von 1873 bis 1876 besuchte er die Kriegsakademie in Berlin, die er mit der Qualifikation für den Generalstab verließ. 1877 wurde er in den Großen Generalstab versetzt und im folgenden Jahr zum Hauptmann befördert. 1881 diente er im Generalstab der 1. Division in Königsberg und wurde zum Major befördert. Im März 1888 zählte er zu den Offizieren, die am aufgebahrten Leichnam Kaiser Wilhelms I. Totenwache hielten.

1890 leitete er die II. Abteilung im Kriegsministerium und wurde im Jahr darauf Oberstleutnant. 1893 kommandierte er das Oldenburgische Infanterieregiment Nr. 91 und am 17. März 1894 wurde er zum Oberst befördert.

Am 15. August 1896 wurde er Chef des Generalstabes des VIII. Armee-Korps in Koblenz und im Jahr darauf am 22. März 1897 zum Generalmajor ernannt. Am 9. Juli 1900 erfolgte seine Beförderung zum Generalleutnant und die Ernennung zum Kommandeur der 28. Division in Karlsruhe. Am 27. Januar 1903 wurde er zum Kommandierenden General des IV. Armee-Korps in Magdeburg ernannt und am 22. Juni 1905 zum General der Infanterie befördert. Im März 1911 wurde er unter Verleihung des Schwarzen Adlerordens in den Ruhestand verabschiedet.

Als Pensionär zog Hindenburg erstmals nach Hannover und bezog in der Oststadt als Mieter die Villa Köhler, Am Holzgraben 1.

Am 22. August 1914 wurde Hindenburg Oberbefehlshaber der 8. Armee. Bereits am nächsten Morgen reiste er nach Ostpreußen ab, wo er vier Tage später bei der Schlacht bei Tannenberg zum Generaloberst befördert wurde. Am 2. September 1914 wurde ihm vom Kaiser der Orden Pour le Mérite verliehen. Vom 6. bis 14. September nahm er an der Schlacht an den Masurischen Seen teil. Er wurde am 1. November 1914 Oberbefehlshaber Ost und am 27. November 1914 zum Generalfeldmarschall befördert. Am 23. Februar 1915 wurde Hindenburg für den Sieg in der Winterschlacht in Masuren mit dem Eichenlaub zum Pour le Mérite geehrt. Am 29. August 1916 erfolgte seine Ernennung zum Chef des Generalstabes des Feldheeres. Er wurde am 9. Dezember 1916 mit dem Großkreuz des Eisernen Kreuzes geehrt. Am 25. März 1918 bekam Hindenburg die Sonderstufe zum Großkreuz des Eisernen Kreuzes, den sogenannten Hindenburgstern. Am 25. Juni 1919 trat er als Chef des Generalstabes des Heeres zurück. Am 3. Juli 1919 wurde die Mobilmachungsbestimmung aufgehoben.

Bei Beginn des Ersten Weltkrieges bemühte sich Hindenburg zunächst vergeblich um ein Kommando. Erst als die Lage an der Ostfront außer Kontrolle zu geraten drohte, wurde er zum Oberbefehlshaber der 8. Armee ernannt, mit Generalmajor Erich Ludendorff als Stabschef. Unter seinem Kommando wurde die nach Ostpreußen eingedrungene russische Narew-Armee in einer Umfassungs- und Vernichtungsschlacht geschlagen, die vom 26. bis zum 30. August 1914 andauerte. Dieser Sieg war für Hindenburg in zweierlei Hinsicht von entscheidender Bedeutung. Zum einen war er der Beginn der engen Zusammenarbeit mit Ludendorff, dessen strategischem Geschick der Sieg in erster Linie zu verdanken war – Hindenburg selbst traf kaum Entscheidungen und erwähnte wiederholt, dass er während der Schlacht sehr gut geschlafen habe. Zum anderen begründete er Hindenburgs ganz außerordentliches Prestige, das ihn im weiteren Verlauf des Krieges zum mächtigsten Mann in Deutschland machen sollte. An diesem politischen Mythos, der sich um seine Person und den Sieg ranken sollte, arbeitete er selbst aktiv mit. Unmittelbar nach der Schlacht setzte er durch, dass sie nach dem vom Kampfgeschehen am Rande betroffenen Ort Tannenberg genannt werden sollte. In der Schlacht bei Tannenberg (Polnisch: Schlacht bei Grunwald) hatte 1410 ein polnisch-litauisches Heer den Deutschen Orden vernichtend geschlagen, eine „Scharte“, die der auf die Wirkung in der Öffentlichkeit bedachte Hindenburg durch die Namensgebung auszuwetzen versuchte. Der triumphale Sieg wurde von der Öffentlichkeit in der Folge Hindenburg zugeschrieben und brachte ihm die Ernennung zum Generalfeldmarschall und die Verleihung des Sterns zum Großkreuz des Eisernen Kreuzes. Von großer Bedeutung und Nachwirkung war seine und Ludendorffs Rolle bei der ab 1915 erfolgenden Etablierung des Militärstaates „Land Ober Ost“.

Hindenburgs Rolle im Ersten Weltkrieg beruhte vor allem auf dem Mythos als „Sieger von Tannenberg“, weniger auf seinen tatsächlichen militärischen Leistungen. Im August 1916 übernahm er mit Ludendorff die Oberste Heeresleitung, die schnell an Einfluss auf die Politik des Deutschen Reiches gewann und Wilhelm II. praktisch entmachtete. Hindenburg war dabei (mit)verantwortlich für entscheidende Weichenstellungen im Krieg wie die Eröffnung des uneingeschränkten U-Boot-Krieges, die Ablehnung eines Verständigungsfriedens und die Diktatfrieden von Brest-Litowsk und Bukarest. Die Machtfülle von Hindenburg und Ludendorff war so groß, dass verschiedene Zeitgenossen wie Max Weber, Wilhelm Solf und Friedrich Meinecke von einer regelrechten „Militärdiktatur“ der dritten OHL sprachen. Dieser Begriff wurde von verschiedenen Historikern übernommen. Andere Historiker wie Gregor Schöllgen und Hans-Ulrich Wehler weisen dagegen darauf hin, dass die Machtausübung der OHL nicht im strengen Sinne als Militärdiktatur gewertet werden könne, da sie die politische Führung nie verantwortlich übernommen habe und durchaus auch innenpolitisch an Grenzen gestoßen sei. Wehler betont aber, dass „die indirekte, gleichwohl massive ‚faktische Machtausübung‘ der 3. OHL unübersehbar zutage“ getreten sei. Wolfram Pyta charakterisiert Hindenburgs Herrschaft, wie sie seit 1916 ausgeübt wurde, als Sonderform der charismatischen Herrschaft.

Nach der militärischen Niederlage 1918 riet Hindenburg Wilhelm II., das Land zu verlassen. Durch die Zusammenarbeit mit der neuen republikanischen Regierung versuchte er, Unruhen innerhalb des Heeres entgegenzuwirken. Mit Abschluss des Versailler Vertrages im Juli 1919 erteilte Reichspräsident Friedrich Ebert Hindenburg auf dessen Wunsch den Abschied. Vor dem Untersuchungsausschuss der Weimarer Nationalversammlung verbreitete er die Dolchstoßlegende, wonach das deutsche Heer „im Felde unbesiegt“ geblieben und von den Novemberrevolutionären durch einen Waffenstillstand „von hinten erdolcht“ worden sei.

Am 25. Juni 1919 trat Hindenburg von seinem Posten als Chef des Generalstabes des Heeres zurück und verließ seinen letzten Dienstort Kolberg. Er wählte Hannover, das ihn im August 1915 zum Ehrenbürger ernannt und ihm im Oktober 1918 im Zooviertel eine Villa zum lebenslangen Nießbrauch überlassen hatte, zu seinem Alterssitz. Von dort unternahm er in den folgenden Jahren viele Reisen durch das Reich, besonders durch Ostpreußen, wo er sich als Befreier Ostpreußens einer großen Popularität erfreute. 1921 wurde er Vorsitzender der Deutschenhilfe und Ehrenbursch des Corps Montania Freiberg.

Nachdem beim ersten Wahlgang zur Reichspräsidentenwahl am 29. März 1925 kein Kandidat eine absolute Mehrheit erreicht hatte, fragten die Rechtsparteien bei dem parteilosen Hindenburg eine Kandidatur an. Der 77-Jährige äußerte sich zunächst zögerlich, stimmte jedoch schließlich zu.

Am 23. November 1925 wurden sein Sohn Oskar von Hindenburg erster und Wedige von der Schulenburg zweiter militärischer Adjutant. Sein Sohn wurde im Laufe der Zeit persönlicher Assistent des Reichspräsidenten und damit faktisch Bindeglied zwischen dem Staatsoberhaupt und dem Reichswehrministerium in der Bendlerstraße.

Im ersten Wahlgang der Reichspräsidentenwahl erhielt der Duisburger Oberbürgermeister Karl Jarres, der für den rechtsparteilichen Reichsbürgerblock kandidierte, zwar mit 10,8 Millionen Stimmen die relative Mehrheit, dieser verzichtete allerdings im zweiten Wahlgang zugunsten Paul von Hindenburgs. Am 26. April 1925 wurde Hindenburg als Vertreter des antirepublikanischen „Reichsblocks“, dem Wilhelm Marx vom republikanischen „Volksblock“ gegenüberstand, im zweiten Wahlgang im Alter von 77 Jahren als Nachfolger Friedrich Eberts zum Reichspräsidenten gewählt und am 12. Mai vereidigt. Damit ist er bis heute das einzige deutsche Staatsoberhaupt, das je vom Volk direkt gewählt wurde.

In England wurde seine Wahl ruhig aufgenommen. Der "Daily Chronicle" schrieb, es liege kein Bruch des Friedensvertrages vor, und man müsse Deutschland an seinen Taten messen, nicht an seinen Wahlen. Die "Times" meinte, die Wähler hätten den alten Soldaten als typischen und besten Repräsentanten der Nation gewählt, und es sei das beste für Deutschland und Europa, wenn an der Spitze des Staates ein Mann von Ehre und Tatkraft stünde. In Frankreich war man kritischer. "Le Temps" merkte an, dass ein ehemaliger Armeeführer gewählt worden sei, was zum Ausdruck bringe, dass Deutschland sich seine Niederlage im Krieg nicht eingestehen wolle.

Im Urteil über Hindenburgs Amtsführung bis zum Beginn der Weltwirtschaftskrise ist die Forschung gespalten. Hagen Schulze etwa betont Hindenburgs Treue zur Weimarer Reichsverfassung, der er als Monarchist zwar distanziert gegenüberstand, die er jedoch bis 1930 hoch gehalten habe „wie die preußische Felddienstordnung“. Hindenburg habe sich durch seinen Amtseid strikt an sie gebunden gefühlt und daher auch bis 1930 ihren Notstandsartikel 48 nie angewendet. Schulzes Berliner Kollege Henning Köhler bestätigt zwar, dass Hindenburg sich bis 1930 verfassungskonform verhielt, macht jedoch darauf aufmerksam, dass der durchaus machtbewusste Präsident Ansätze, seine Amtsbefugnisse durch ein Ausführungsgesetz zum Artikel 48 einzuschränken, hintertrieb. Auch habe er deutlichen Einfluss auf die Zusammensetzung der Kabinette genommen und dabei „deutlich konservative Politiker bevorzugt“.

Wegen seiner Unterschrift unter den Young-Plan, der von den rechtsradikalen Parteien als Verpflichtung zu jahrzehntelanger "Versklavung" des Volkes hingestellt wurde, rückten seine ehemaligen politischen Freunde immer mehr von ihm ab. Hindenburg beschloss, die derzeit regierende Große Koalition unter Kanzler Hermann Müller (SPD) durch eine "antimarxistische und antiparlamentarische" Regierung zu ersetzen. Die Gelegenheit hierzu ergab sich, nachdem die Große Koalition an der Frage des Beitragssatzes zur Arbeitslosenversicherung zerbrochen war. Am 29. März 1930 berief er Heinrich Brüning (Zentrum) zum Reichskanzler eines Minderheitskabinetts, ohne das Parlament zu konsultieren. Damit begann die Zeit der Präsidialkabinette, in denen der jeweilige Kanzler hauptsächlich vom Vertrauen des Präsidenten abhängig sein sollte. Ganz gelang die geplante Ausschaltung des Parlaments indes nicht, da der Reichstag die von der Regierung gemäß Artikel 48 der Reichsverfassung erlassenen Notverordnungen jederzeit aufheben konnte. Als er das im Juni 1930 tat, löste Hindenburg ihn kurzerhand auf – eine folgenschwere Entscheidung, denn dieser Reichstag war der letzte, in dem die demokratischen Parteien die Mehrheit hatten. Da die beginnende Weltwirtschaftskrise zur Radikalisierung großer Teile der Wähler geführt hatte, stieg der Stimmanteil der beiden extremen Parteien, der KPD und vor allem der NSDAP. Damit war die politische Notlage, die nach dem Sinn der Verfassung durch die Anwendung der Artikel 48 und 25 doch eigentlich behoben werden sollte, durch die Politik Hindenburgs erst herbeigeführt worden.

Um weitere Parlamentsauflösungen zu verhindern, beschloss daraufhin die SPD, künftig die Regierung Brüning zu tolerieren, das heißt, gegen weitere Anträge der extremistischen Parteien auf Aufhebung der Notverordnungen zu stimmen. Damit war der zweite Teil von Hindenburgs Plan gescheitert: Die Regierung blieb weiter abhängig vom Parlament und von den bei Hindenburg verhassten Sozialdemokraten.

Bei der Reichspräsidentenwahl 1932 wurde Hindenburg für weitere sieben Jahre in seinem Amt bestätigt. Dies ist dem Umstand zu verdanken, dass sich alle demokratischen Parteien, einschließlich der Sozialdemokraten und des Zentrums, hinter den überzeugten Monarchisten stellten, um so Hitler als Reichspräsidenten zu verhindern. Dazu hatte während des Wahlkampfs im Reichstag der NSDAP-Abgeordnete Joseph Goebbels erklärt, dass Hindenburg „seinem Namen, seiner Vergangenheit und seinen Leistungen nach zu uns [gehört] und nicht zu denen, die heute bereit sind, ihm ihre Stimme zu geben.“

Hindenburg sollte 1927 zu seinem 80. Geburtstag den alten Familienbesitz Gut Neudeck von einem Freundeskreis um Elard von Oldenburg-Januschau geschenkt bekommen, nachdem Hindenburgs Familie es aus finanziellen Gründen nicht mehr hatte halten können. Die gesammelten Mittel reichten jedoch bei weitem nicht aus und wurden durch Sammlungen in Vereinen, vor allem jedoch durch Spenden der Wirtschaft so aufgestockt, dass schließlich der Betrag von 1 Million Reichsmark erreicht wurde. Um Erbschaftssteuern zu sparen, wurde es gleich auf seinen Sohn Oskar überschrieben. Dieses im Prinzip legale, jedoch für einen Mann in seiner Position anrüchige Verhalten schädigte sein Ansehen. Im rechten, antisemitischen Lager erntete Hindenburg Kritik, weil er sich für ein Staatsporträt von „dem Juden Liebermann“ malen ließ. Außerdem gab es Korruptionsvorwürfe gegen Hindenburg im Zusammenhang mit dem zwei Jahre darauf verabschiedeten „Ostpreußengesetz“, das den Kreis der Schenker und anderer Junker wirtschaftlich begünstigte. Diese Vorgänge und die anschließenden Auseinandersetzungen und Untersuchungen gingen als Osthilfeskandal in die Geschichte ein. Historiker vermuten, dass diese Verwicklungen Hindenburgs Entscheidung für Hitler beeinflusst haben könnten.

Nach der Wahl geriet Hindenburg noch stärker als zuvor unter den Einfluss der "Kamarilla", eines Kreises von Freunden und Weggefährten der politischen Rechten. Zu dieser gehörte unter anderen Oskar, der „in der Verfassung nicht vorgesehene Sohn des Reichspräsidenten“ (so ein viel zitiertes Bonmot Kurt Tucholskys), ferner sein Nachbar auf Neudeck Elard von Oldenburg-Januschau sowie Generalleutnant Kurt von Schleicher und schließlich Franz von Papen. Diese überredeten Hindenburg, Brüning zu entlassen und stattdessen von Papen zum Reichskanzler zu ernennen, der "mehr nach rechts" regieren sollte. (Hindenburgs Biographen, insbesondere Wolfram Pyta und sein früherer Biograph Dorpalen, heben hervor, dass Hindenburg diese Entscheidungen in eigener Verantwortung getroffen habe. Beide Biographien und Memoiren von Beteiligten – etwa Staatssekretär Meißner – relativieren den Einfluss der Berater und heben Hindenburgs Eigenverantwortung bei diesen Entscheidungen hervor). Als dies nicht zum Erfolg führte, erwog der Kreis kurzfristig einen Staatsstreich, um ein autoritäres Regime zu errichten, doch weigerte sich Schleicher, dafür die Reichswehr zur Verfügung zu stellen.

Letztlich stand der Reichspräsident nur noch vor der Alternative: Entweder würde er erneut eine Präsidialregierung ohne Rückhalt im Volk einsetzen, was möglicherweise zu einem Bürgerkrieg führen würde, den die Reichswehr – wie entsprechende von Reichswehrminister Schleicher in Auftrag gegebene Planspiele in seinem Ministerium Anfang Dezember 1932 zeigten – nicht gewinnen könne, oder er bildete eine Mehrheitsregierung im Reichstag bzw. eine Regierung, die zwar formal eine Minderheitsregierung war, jedoch begründete Aussicht haben würde, eine Mehrheit im Reichstag zu erlangen. Dieses war seit den Wahlen im Juli und im November 1932 ohne eine Beteiligung der Nationalsozialisten nicht mehr möglich. Am 6. November sprach sich ein „Deutscher Ausschuss“ unter der Überschrift „Mit Hindenburg für Volk und Reich!“ für die Regierung Papen, für die DNVP und gegen die NSDAP aus. Diesen Aufruf hatten insgesamt 339 Persönlichkeiten unterschrieben, darunter mehrere Dutzend Großindustrielle wie Ernst von Borsig, der Vorsitzende des Bergbauvereins Ernst Brandi, Fritz Springorum und Albert Vögler. Am 19. November 1932 erhielt Hindenburg eine gegenläufige Eingabe von zwanzig Industriellen, mittelständischen Unternehmern, Bankiers und Agrariern mit der Aufforderung, Adolf Hitler zum Reichskanzler zu ernennen. Hindenburg berief am 2. Dezember 1932 jedoch Kurt von Schleicher zum Reichskanzler. Der versuchte noch, Teile der NSDAP um Gregor Strasser von Hitler weg in eine Querfront zu bringen, doch dies misslang. Als Schleicher dann seinerseits vorschlug, den Reichstag aufzulösen und unter Bruch der Reichsverfassung bis auf Weiteres keinen neuen wählen zu lassen, entzog ihm Hindenburg seine Unterstützung.

Am 30. Januar 1933 berief Hindenburg Adolf Hitler zum Reichskanzler (sog. Machtergreifung). Außer Hitler gehörten mit Innenminister Wilhelm Frick und Hermann Göring als Minister ohne Geschäftsbereich nur zwei Nationalsozialisten dem neuen Kabinett Hitler an. Trotz seiner anfänglichen persönlichen Abneigung gegen Hitler, den er abschätzig den „böhmischen Gefreiten“ nannte, geriet Hindenburg immer stärker in dessen Einflussbereich. Am 1. Februar 1933 löste er den Reichstag auf. Die Verordnung zur Auflösung des Reichstages ist unterschrieben von Hindenburg, Hitler und Frick. Im Laufe des Februars wurde eine ganze Reihe von Maßnahmen wie die „Verordnung des Reichspräsidenten zum Schutze des Deutschen Volkes“ und (unmittelbar nach dem Reichstagsbrand vom 27. Februar 1933) die „Verordnung des Reichspräsidenten zum Schutz von Volk und Staat“ erlassen, mit denen die Grundrechte bis auf Weiteres (faktisch bis zum Ende des Zweiten Weltkrieges) außer Kraft gesetzt wurden. In der Folge kam es zu Massenverhaftungen von Anhängern der KPD und der SPD. Bei der von Propagandaminister Joseph Goebbels am 21. März 1933 (dem so genannten Tag von Potsdam) inszenierten Eröffnung des neu gewählten Reichstags in der Garnisonkirche wurde durch eine symbolische Verneigung Hitlers vor dem greisen Reichspräsidenten eine symbolträchtige Kontinuität zwischen der Kaiserzeit und dem Dritten Reich hergestellt und Hindenburgs hohes Ansehen für das neue Regime instrumentalisiert und vereinnahmt.

Das am 23. März 1933 mit Zweidrittelmehrheit vom Reichstag verabschiedete Ermächtigungsgesetz hob das in der Weimarer Verfassung festgelegte alleinige Gesetzgebungsrecht des Reichstags auf. Nun konnte die Regierung selbst Gesetze erlassen und war nicht mehr wie bisher auf das Notverordnungsrecht des Reichspräsidenten angewiesen, wobei auf Forderung der bürgerlichen Parteien, allen voran des Zentrums, als Bedingung für ihre Zustimmung zum Ermächtigungsgesetz das Notverordnungsrecht des Reichspräsidenten unangetastet blieb.

Von der Reichsregierung und der Preußischen Regierung erhielt Hindenburg 1933 Dotationen von insgesamt 1 Million Reichsmark.

Anfang März 1934 trat Papen an Hindenburg heran, dieser möge ein politisches Testament verfassen, um im Falle der Regierungsunfähigkeit einen „chaotische[n] Zustand“ zu vermeiden. Hindenburg sollte dem deutschen Volk die Einführung der Monarchie empfehlen. Papen glaubte zu diesem Zeitpunkt, Hitler sei der monarchischen Staatsform nicht abgeneigt, und hielt sich selbst für einen geeigneten Reichspräsidenten. Er verfasste einen Entwurf, der sich an Hindenburgs Rechenschaftsbericht in "Aus meinem Leben" anlehnte. Ende April 1934 informierte Hindenburg den Vizekanzler, dass er keine offizielle Empfehlung zur Staatsform abgeben wolle. Die Monarchie werde er vielmehr Hitler persönlich in einem Brief empfehlen. Anfang Mai 1934 ließ Hindenburg seinen zweiten Adjutanten Wedige von der Schulenburg auf der Grundlage von Papens Entwurf eine Reinschrift erstellen, die außerdem das letzte Kapitel aus Hindenburgs Memoiren und den persönlichen Brief an Hitler enthielt. Die Dokumente wurden in Hindenburgs Arbeitszimmer hinterlegt. Zuvor hatte Hindenburg noch am 4. April 1933 in einem Brief an Hitler auf die Aufnahme einer Frontkämpferklausel im Gesetz zur Wiederherstellung des Berufsbeamtentums gedrängt. 

Er selbst brach noch im Juni nach Ostpreußen auf und war in Berlin nicht mehr anwesend. Ende Juni 1934 fanden nach längerer Vorbereitung durch Hitler und dessen Freunde die Mordaktionen des sogenannten „Röhm-Putsches“ statt (Säuberung der SA und Ermordung vermuteter Gegner Hitlers). Ob der greise Hindenburg zusammen mit Reichswehrminister von Blomberg Hitler anlässlich dessen Besuches vom 21. Juni durch verbale Aufforderungen noch angestachelt hat, aktiv zu werden, ist unter Historikern umstritten und spielte letztlich keine Rolle mehr. Wilhelm von Ketteler, Mitarbeiter der Vizekanzlei und selbst als potenzielles Opfer der Verfolgungsaktion gefährdet, fuhr während des Hausarrests seines Dienstherrn Franz von Papen selbst nach Ostpreußen und brachte den Reichspräsidenten über Umwege dazu, die Einstellung der Erschießungen anzuordnen, was Hitler befolgte.

Im Juli 1934 verschlechterte sich der Gesundheitszustand Hindenburgs weiter. Bis dahin hatte er seine Dienstpflichten als Reichspräsident noch wahrgenommen. Auch in der Endphase seines Blasenleidens „blieb Hindenburg im Vollbesitz seiner geistigen Kräfte. Erst zwanzig Stunden vor dem Ableben fiel er in Bewusstseinstrübungen, erkannte jedoch Hitler, als dieser den Sterbenden am Nachmittag des 1. August erneut aufsuchte.“

Am Morgen des 2. August 1934 um 9 Uhr starb Hindenburg auf Gut Neudeck. Dort sollte er eigentlich begraben werden, jedoch organisierte Hitler eine Beisetzung im Denkmal der Schlacht bei Tannenberg.

Das Kabinett Hitler erließ bereits am 1. August, also dem Tag vor Hindenburgs Tod, ein Gesetz über die Zusammenlegung der Ämter des Reichskanzlers und des Reichspräsidenten in der Person Hitlers. Dieses Gesetz trat mit dem Ableben Hindenburgs in Kraft. Oskar von Hindenburg hielt die Dokumente des politischen Testaments seines Vaters eine Woche lang zurück. Am 9. August erhielt sie Papen, der sie am 14. August Hitler übergab. Hitler war von Papen bereits vorab über den Inhalt informiert worden. Den an ihn persönlich gerichteten Brief hielt Hitler zurück und ließ ihn später vermutlich vernichten. Die anderen Dokumente wurden am 15. August als „das politische Testament Hindenburgs“ veröffentlicht. Aus den Umständen der Veröffentlichung lässt sich schließen, dass Hitler, Papen und Oskar von Hindenburg sich abgesprochen hatten, das Testament erst kurz vor der Volksabstimmung über das Staatsoberhaupt des Deutschen Reichs am 19. August 1934 zu veröffentlichen, damit Hitler davon profitieren könne, obwohl Hindenburg ihn darin nicht zu seinem Nachfolger berufen hatte. Am Tag vor der Wahl hielt Oskar von Hindenburg eine Rundfunkrede, in der er behauptete, sein Vater habe in Hitler „seinen unmittelbaren Nachfolger als Oberhaupt des Deutschen Reiches gesehen“. In der Volksabstimmung stimmten fast neunzig Prozent der Wähler dem "Gesetz über das Staatsoberhaupt des Deutschen Reiches" zu.

Beim Anrücken der Roten Armee im Januar 1945 brachte die Wehrmacht Hindenburgs Sarg und den seiner Frau aus dem Tannenberg-Denkmal auf den Leichten Kreuzer "Emden", um die Leichname von Königsberg nach Pillau und von dort mit dem Passagierschiff "Pretoria" nach Stettin zu transportieren. Zusammen mit den Särgen der preußischen Könige Friedrich II. und Friedrich Wilhelm I. wurden sie anschließend in einem thüringischen Salzbergwerk eingelagert. Nach der Eroberung Thüringens durch die US-Armee brachte man die Särge nach Marburg, wo Hindenburg mit seiner Frau in der Nordturmkapelle der Elisabethkirche endgültig beigesetzt wurde.

Hindenburg wurde bereits während des Ersten Weltkriegs Ehrenbürger von mehreren Städten und Gemeinden. Die Zahl wuchs insbesondere ab 1933 auf insgesamt 3824 Ehrenbürgerschaften. Seit den 1970er Jahren gab es in vielen Städten und Gemeinden erinnerungskulturelle Bürgerdiskussionen und geschichtspolitische Initiativen zum Widerruf der Ehrenbürgerschaft.

Seit dem Ende des NS-Regimes haben zahlreiche Kommunen wie Dortmund, Köln, Leipzig, München, Münster und Stuttgart die Ehrenbürgerschaft als NS-belastet gelöscht. An anderen Orten wie in Berlin gab es entsprechende Initiativen, die sich nicht durchsetzten.

siehe auch: Paul von Hindenburg als Ehrenbürger

Hindenburg war Ehrendoktor aller vier Fakultäten der Universität Königsberg, der Rechts- und Staatswissenschaften der Universität Breslau, der juristischen und philosophischen Fakultät der Universität Bonn sowie der juristischen Fakultät der Universität Graz. Gleichzeitig war Hindenburg Dr.-Ing. E. h. aller Technischen Hochschulen der Weimarer Republik und der Freien Stadt Danzig sowie Dr. med. vet. h. c. der Tierärztlichen Hochschule Hannover. Zudem war er Ehrenbürger der Universitäten Göttingen, Königsberg, Köln und Jena sowie der Technischen Hochschule Stuttgart und der Forstwirtschaftlichen Hochschule Eberswalde.

Hindenburg war Ritter folgender hoher Orden (Auswahl):
Er war Ehrenkommandeur des Johanniterordens und Dechant des Domstifts Brandenburg.

Zahlreiche Straßen, Plätze, Brücken und öffentliche Einrichtungen wie Schulen oder Kasernen wurden nach ihm benannt, ebenso wie der 1927 durch ihn eingeweihte Hindenburgdamm nach Sylt. Der Ort Zabrze in Oberschlesien benannte sich in Anerkennung seiner Verdienste am 21. Februar 1915 in "Hindenburg" um. Ramsau bei Berchtesgaden taufte 1933 die bis dahin als „Große Linde“ bekannte überdimensionale Linde in "Hindenburglinde" um. In der Hindenburg-Gedächtniskirche Stetten ordnete das französische Militärgouvernement 1948 die Verdeckung der Hindenburg-Plastik an. 1980 wurde die Plastik wieder freigelegt.

Auch Schiffe und Luftschiffe wurden nach Paul von Hindenburg benannt. Bereits im Ersten Weltkrieg, also zu Lebzeiten des Namensgebers, trug ein Schlachtkreuzer der Derfflinger-Klasse, die SMS Hindenburg, seinen Namen. Die Kriegsmarine plante angeblich, einem der projektierten Schlachtschiffe der H-Klasse den Namen "Hindenburg" zu geben. Bekannter wurde das Luftschiff Hindenburg, mit dem die deutsche Passagierluftschifffahrt ihren Höhepunkt und 1937 ihr Ende erreichte, als die "Hindenburg" bei der Katastrophe von Lakehurst verbrannte.

In zahlreichen Kommunen wurden erst mehrere Jahrzehnte nach dem Ende des NS-Regimes nach Hindenburg benannte öffentliche Orte wegen dessen NS-Belastung umbenannt. Im April 2009 änderte das "Hindenburg-Gymnasium Trier" seinen Namen. Die "Anton-Leo-Schule" in Bad Säckingen war die letzte nach Hindenburg benannte Schule, sie wurde 2013 umbenannt. Die Umbenennung scheiterte bis 2013 trotz mehrerer Initiativen.

Im März 2012 beschloss der Rat der Stadt Münster die Umbenennung des Hindenburgplatzes in Schlossplatz. Entsprechende Initiativen waren zuvor in den Nachkriegsjahrzehnten immer wieder gescheitert, zuletzt 1998. Ein Bürgerbegehren gegen den Ratsentscheid war letztlich nicht erfolgreich, bei einem Bürgerentscheid im September 2012 lehnten es fast 60 Prozent der Münsteraner Wähler ab, den Platz erneut in seinen alten Namen Hindenburgplatz umzubenennen.

In Ludwigsburg scheiterte am 30. Juli 2015 die Vorlage der Stadtverwaltung, die Hindenburgstraße umzubenennen an der Ablehnung der CDU-Fraktion, der Fraktion der Freien Wähler und des Stadtrats der REP. Zudem lehnte ein Stadtrat der FDP die Vorlage ab.

2014 berief die Stadt Hannover einen Beirat aus Fachleuten zur Überprüfung, ob es bei Personen als Namensgeber für Straßen „eine aktive Mitwirkung im Nazi-Regime oder schwerwiegende persönliche Handlungen gegen die Menschlichkeit gegeben hat“. Er regte die Umbenennung der nach Hindenburg benannten Straße an. Nach der Darstellung dieses Beirats habe Hindenburg „Hitler den Weg zur Macht geebnet und alle politischen Maßnahmen Hitlers mitgetragen“. Dies könne auch nicht dadurch relativiert werden, dass Hindenburg „nicht mehr Herr seiner Entscheidungen“ gewesen sei, denn diesbezügliche Thesen seien widerlegt.

"Siehe auch: Hindenburgallee, Hindenburgstraße, Hindenburgplatz, Hindenburgbrücke, Hindenburgschule, Hindenburg-Kaserne, Hindenburgufer, Hindenburgschleuse, Hindenburgpark"

Von 1928 bis 1936 gab die Deutsche Reichspost zwei Freimarkenserien mit dem Porträt Hindenburgs heraus. Im August 1934 verausgabte Serienmarken wurden mit schwarzem Trauerrand überdruckt.







</doc>
<doc id="3975" url="https://de.wikipedia.org/wiki?curid=3975" title="Primzahl">
Primzahl

Eine Primzahl (wörtlich „erste Zahl“ oder eher „Zahl erster Klasse“) ist eine natürliche Zahl, die größer als 1 und ausschließlich durch sich selbst und durch 1 teilbar ist. Die Primzahlen sind damit innerhalb der Menge formula_1 der natürlichen Zahlen dadurch charakterisiert, dass jede von ihnen genau zwei natürliche Zahlen als Teiler hat. Die aus allen Primzahlen bestehende Teilmenge von formula_1 wird in der Regel mit dem Symbol formula_3 bezeichnet.

Unmittelbar mit formula_3 verknüpft ist die Folge formula_5 der nach ihrer Größe geordneten Primzahlen, die man auch kurz die "Primzahlfolge" nennt. Es ist demnach

mit

Die Zahl 1 ist wegen ihrer Invertierbarkeit weder "prim" noch "zusammengesetzt". Alle anderen natürlichen Zahlen sind eines von beiden, entweder prim (also Primzahl) oder zusammengesetzt.

Das Wort „Primzahl“ kommt aus dem Lateinischen "(numerus primus)" und bedeutet „die erste Zahl“. Die Bedeutung der Primzahlen formula_3 für viele Bereiche der Mathematik beruht auf drei Folgerungen aus dieser Definition:


Diese Eigenschaften werden in der Algebra für Verallgemeinerungen des Primzahlbegriffs genutzt.

Schon im antiken Griechenland interessierte man sich für die Primzahlen und entdeckte einige ihrer Eigenschaften. Obwohl Primzahlen seit damals stets einen großen Reiz auf die Menschen ausübten, sind viele die Primzahlen betreffenden Fragen bis heute ungeklärt, darunter solche, die mehr als hundert Jahre alt und leicht verständlich formulierbar sind. Dazu gehören die Goldbachsche Vermutung, wonach außer 2 jede gerade Zahl als Summe zweier Primzahlen darstellbar ist, und die Vermutung, dass es unendlich viele Primzahlzwillinge gibt (das sind Paare von Primzahlen, deren Differenz gleich 2 ist).

Über 2000 Jahre lang konnte man keinen praktischen Nutzen aus dem Wissen über die Primzahlen ziehen. Dies änderte sich erst mit dem Aufkommen elektronischer Rechenmaschinen, bei denen die Primzahlen beispielsweise in der Kryptographie eine zentrale Rolle spielen.

Es gilt der Fundamentalsatz der Arithmetik: Jede positive ganze Zahl lässt sich als (ggf. leeres) Produkt von Primzahlen darstellen, und diese Darstellung ist bis auf die Reihenfolge der Primzahlen eindeutig. Diese Primzahlen nennt man die "Primfaktoren" der Zahl.

Aufgrund dieses Satzes, also dass sich jede natürliche Zahl größer 0 durch Multiplikation von Primzahlen eindeutig darstellen lässt, nehmen die Primzahlen eine besondere atomare Stellung in der Mathematik ein, sie „erzeugen“ gewissermaßen alle anderen natürlichen Zahlen. Alexander K. Dewdney bezeichnete sie als den Elementen der Chemie weitgehend ähnlich.

Daraus wird auch klar, warum die 1 nicht als Primzahl definiert wird. Sie ist das neutrale Element der Multiplikation und kann demzufolge multiplikativ keine weiteren Zahlen erzeugen. Zählt man die 1 zu den Primzahlen, verliert sich außerdem die Eindeutigkeit der Primfaktorzerlegung, weil man zu jeder Zerlegung beliebig viele Einsen hinzufügen kann, ohne den Wert der Zahl zu ändern.

Man kennt bisher keine Methode, um die Primfaktorzerlegung einer beliebigen gegebenen Zahl effizient zu bestimmen, d. h. in einer Zeit, die polynomiell mit der Länge der Zahl wächst. Die "Faktorisierungsannahme" besagt, dass es eine solche Methode auch nicht gibt. Man hat eine Reihe von Faktorisierungsverfahren entwickelt, um allgemeine Zahlen oder auch solche von spezieller Form möglichst schnell zu zerlegen.

Mit Ausnahme der Zahl 2 sind alle Primzahlen formula_9 ungerade, denn alle größeren geraden Zahlen lassen sich außer durch sich selbst und 1 auch noch (mindestens) durch 2 teilen. Damit hat jede Primzahl außer 2 die Form formula_10 mit einer natürlichen Zahl formula_11.

Jede Primzahl formula_12 lässt sich einer der beiden Klassen „Primzahl der Form formula_13“ oder „Primzahl der Form formula_14“ zuordnen, wobei formula_11 eine natürliche Zahl ist. Darüber hinaus hat jede Primzahl formula_16 die Form formula_17 oder formula_18, wobei formula_11 eine natürliche Zahl ist. Nach dem dirichletschen Primzahlsatz gibt es in jeder dieser vier Klassen unendlich viele Primzahlen.

Jede natürliche Zahl der Form formula_20 mit einer nichtnegativen ganzen Zahl formula_21 enthält mindestens einen Primfaktor der Form formula_14. Eine entsprechende Aussage über Zahlen der Form formula_23 oder Primfaktoren der Form formula_13 ist nicht möglich.

Eine Primzahl formula_25 lässt sich genau dann in der Form formula_26 mit ganzen Zahlen formula_27 schreiben, wenn formula_9 die Form formula_13 hat. In diesem Fall ist die Darstellung im Wesentlichen eindeutig, d. h. bis auf Reihenfolge und Vorzeichen von formula_27. Diese Darstellung entspricht der Primfaktorzerlegung

im Ring der ganzen gaußschen Zahlen.

Die Zahl −1 ist ein quadratischer Rest modulo jeder Primzahl der Form formula_13 und quadratischer Nichtrest modulo jeder Primzahl der Form formula_14.

Es sei formula_9 eine Primzahl. Für jede ganze Zahl formula_35, die nicht durch formula_9 teilbar ist, gilt (für die Notation siehe Kongruenz):

Für nicht durch formula_9 teilbare Zahlen formula_35 ist die folgende Formulierung äquivalent:

Es gibt Zahlen, die keine Primzahlen sind, sich aber dennoch zu einer Basis formula_35 wie Primzahlen verhalten und somit den kleinen Satz von Fermat erfüllen. Solche zusammengesetzten Zahlen nennt man fermatsche Pseudoprimzahlen zur Basis formula_35. Eine fermatsche Pseudoprimzahl formula_43 die pseudoprim bezüglich "aller" zu ihr teilerfremden Basen formula_35 ist, nennt man Carmichael-Zahl.

In diesem Zusammenhang zeigt sich die Problematik fermatscher Pseudoprimzahlen: sie werden von einem Primzahltest, der den kleinen Satz von Fermat nutzt (Fermatscher Primzahltest), fälschlicherweise für Primzahlen gehalten. Wenn allerdings ein Verschlüsselungsverfahren wie RSA eine zusammengesetzte Zahl statt einer Primzahl verwendet, ist die Verschlüsselung nicht mehr sicher. Deshalb müssen bei solchen Verfahren bessere Primzahltests verwendet werden.

Eine einfache Folge aus dem kleinen Satz von Fermat ist die folgende Aussage: Für jede ungerade Primzahl formula_9 und jede ganze Zahl formula_35, die nicht durch formula_9 teilbar ist, gilt entweder

oder

Man kann zeigen, dass der erste Fall genau dann eintritt, wenn es eine Quadratzahl formula_50 gibt, die kongruent zu formula_35 modulo formula_9 ist, "siehe" Legendre-Symbol.

Für Primzahlen formula_9 und formula_54 gilt

zusammen mit dem binomischen Satz folgt daraus

Für ganze Zahlen formula_57 folgt diese Aussage auch direkt aus dem kleinen fermatschen Satz, aber sie ist beispielsweise auch für Polynome mit ganzzahligen Koeffizienten anwendbar; im allgemeinen Kontext entspricht sie der Tatsache, dass die Abbildung formula_58 in Ringen der Charakteristik formula_9 ein Homomorphismus ist, der sogenannte Frobenius-Homomorphismus.

Aus dem Satz von Wilson (formula_9 ist genau dann eine Primzahl, wenn formula_61 ist) folgt, dass für jede Primzahl formula_9 und jede natürliche Zahl formula_9 die Kongruenz

erfüllt ist.

Charles Babbage bewies 1819, dass für jede Primzahl formula_25 diese Kongruenz gilt:

Der Mathematiker Joseph Wolstenholme (1829–1891) bewies dann 1862, dass für jede Primzahl formula_67 die folgende Kongruenz gilt:

Aus dem kleinen Satz von Fermat folgt, dass für eine Primzahl formula_9 gilt:

Beispiel formula_71:

Giuseppe Giuga vermutete, dass auch die umgekehrte Schlussrichtung gilt, dass also eine Zahl mit dieser Eigenschaft stets prim ist. Es ist nicht geklärt, ob diese Vermutung richtig ist. Bekannt ist aber, dass ein Gegenbeispiel mehr als 10.000 Dezimalstellen haben müsste. Im Zusammenhang mit Giugas Vermutung werden die Giuga-Zahlen untersucht.

Den kleinen fermatschen Satz kann man auch in der Form lesen: In der Folge formula_73 ist das formula_9-te Folgenglied für eine Primzahl formula_9 stets durch formula_9 teilbar. Ähnliche Eigenschaften besitzen auch andere Folgen von exponentiellem Charakter, wie die Lucas-Folge (formula_77) und die Perrin-Folge (formula_78). Für andere lineare Rekursionen gelten analoge, aber kompliziertere Aussagen, beispielsweise für die Fibonacci-Folge formula_79: Ist formula_9 eine Primzahl, so ist formula_81 durch formula_9 teilbar; dabei ist

das Legendre-Symbol.

Die Reihe der Kehrwerte der Primzahlen ist divergent. Somit gilt:

Das ist gleichbedeutend mit der Aussage, dass die durch formula_85 definierte Folge keinen endlichen Grenzwert besitzt, was wiederum bedeutet, dass sich für ein genügend groß gewähltes formula_86 jede erdenkliche reelle Zahl übertreffen lässt. Dies ist zunächst einmal verblüffend, da die Primzahllücken im Schnitt immer weiter zunehmen. Der Satz von Mertens trifft eine Aussage über das genaue Wachstumsverhalten dieser divergenten Reihe.

Ob eine beliebige natürliche Zahl prim ist, kann mit einem Primzahltest herausgefunden werden. Es gibt mehrere solcher Verfahren, die sich auf besondere Eigenschaften von Primzahlen stützen. In der Praxis wird der Miller-Rabin-Test am häufigsten verwendet, der eine extrem kurze Laufzeit hat, allerdings mit kleiner Wahrscheinlichkeit falsch-positive Ergebnisse liefert. Mit dem AKS-Primzahltest ist es möglich, über die Primalität ohne Gefahr eines Irrtums in polynomieller Laufzeit zu entscheiden. Allerdings ist er in der Praxis deutlich langsamer als der Miller-Rabin-Test.

Herauszufinden, ob eine natürliche Zahl prim ist oder nicht, kann sehr aufwändig sein. Zu jeder Primzahl lässt sich aber eine Kette von Behauptungen angeben, die alle unmittelbar nachvollziehbar sind, zusammen die Primalität belegen und deren Gesamtlänge höchstens proportional ist zum Quadrat der Länge der Primzahl. Ein solcher Beleg wird "Zertifikat" (engl. "primality certificate") genannt.

Bei der Zusammengesetztheit (Nichtprimalität) einer Zahl ist der Unterschied zwischen Beleg und Finden eines Belegs noch augenfälliger: Als Beleg genügen zwei Faktoren, deren Produkt die zusammengesetzte Zahl ergibt; das Finden eines echten Teilers kann aber sehr viel Aufwand bedeuten.

Der Grieche Euklid hat im vierten Jahrhundert vor Christus logisch geschlussfolgert, dass es unendlich viele Primzahlen gibt; diese Aussage wird als "Satz von Euklid" bezeichnet. Euklid führte einen Widerspruchsbeweis für die Richtigkeit dieses Satzes ("Elemente," Buch IX, § 20): Ausgehend von der Annahme, dass es nur endlich viele Primzahlen gibt, lässt sich eine weitere Zahl konstruieren, die eine bisher nicht bekannte Primzahl als Teiler hat oder selbst eine Primzahl ist, was einen Widerspruch zur Annahme darstellt. Somit kann eine endliche Menge niemals alle Primzahlen enthalten, also gibt es unendlich viele. Heute kennt man eine ganze Reihe von Beweisen für den Satz von Euklid.

Der Satz von Euklid besagt, dass es keine größte Primzahl gibt. Es ist jedoch kein Verfahren bekannt, das effizient beliebig große Primzahlen generiert – deshalb gab es stets eine jeweils "größte bekannte" Primzahl, seitdem sich die Menschen mit Primzahlen befassen. Derzeit (Stand: Januar 2018) ist es formula_87 eine Zahl mit 23.249.425 (dezimalen) Stellen, die am 26. Dezember 2017 berechnet wurde. Für den Entdecker Jonathan Pace gab es für den Fund 3.000 US-Dollar vom Projekt Great Internet Mersenne Prime Search, das Mersenne-Primzahlen mittels verteiltem Rechnen sucht.

Die größte bekannte Primzahl war fast immer eine Mersenne-Primzahl, also von der Form formula_88 da in diesem Spezialfall der Lucas-Lehmer-Test angewendet werden kann, ein im Vergleich zur allgemeinen Situation sehr schneller Primzahltest. Bei der Suche nach großen Primzahlen werden deshalb nur Zahlen dieses oder eines ähnlich geeigneten Typs auf Primalität untersucht.

Zur Untersuchung der Verteilung der Primzahlen betrachtet man unter anderem die Funktion

die die Anzahl der Primzahlen formula_90 angibt und auch "Primzahlzählfunktion" genannt wird.
Zum Beispiel ist

Diese Funktion und ihr Wachstumsverhalten ist ein beliebter Forschungsgegenstand in der Zahlentheorie. Mit der Zeit wurden einige Näherungsformeln entwickelt und verbessert.

Der Primzahlsatz besagt, dass

gilt, das heißt, dass der Quotient von linker und rechter Seite für formula_93 gegen 1 strebt:

Der dirichletsche Primzahlsatz dagegen schränkt die Betrachtung auf Restklassen ein: Es sei formula_21 eine natürliche Zahl. Ist formula_35 eine ganze Zahl, die zu formula_21 nicht teilerfremd ist, so kann die arithmetische Folge

höchstens eine Primzahl enthalten, weil alle Folgenglieder durch den größten gemeinsamen Teiler von formula_35 und formula_21 teilbar sind. Ist formula_35 aber teilerfremd zu formula_21, so besagt der dirichletsche Primzahlsatz, dass die Folge unendlich viele Primzahlen enthält. Beispielsweise gibt es unendlich viele Primzahlen der Form formula_13 und unendlich viele der Form formula_14 (formula_11 durchläuft jeweils die nichtnegativen natürlichen Zahlen).

Diese Aussage kann noch in der folgenden Form präzisiert werden: Es gilt

dabei ist formula_107 die eulersche Phi-Funktion. In diesem Sinne liegen also für ein festes formula_21 in den Restklassen formula_109 mit formula_110 jeweils „gleich viele“ Primzahlen.

Die (bewiesene) Bonsesche Ungleichung garantiert, dass das Quadrat einer Primzahl kleiner ist als das Produkt aller kleineren Primzahlen (ab der fünften Primzahl).

Nach der (unbewiesenen) Andricaschen Vermutung ist die Differenz der Wurzeln der formula_86-ten und der formula_112-ten Primzahl kleiner als 1.

Die Differenz zwischen zwei benachbarten Primzahlen heißt Primzahllücke. Diese Differenz schwankt, und es gibt Primzahllücken beliebiger Größe. Es gibt aber auch Beschränkungen für die Lückengröße in Abhängigkeit von ihrer Lage:

Der Satz von Bertrand sichert die Existenz einer Primzahl zwischen jeder natürlichen Zahl formula_86 und ihrem Doppelten formula_114.

Nach der (unbewiesenen) Legendreschen Vermutung gibt es stets mindestens eine Primzahl zwischen formula_115 und formula_116.

Im Folgenden sei die Folge der Primzahlen mit formula_117 bezeichnet.

Für Indizes formula_118 gelten folgende Abschätzungen:





Mit dem Primzahlsatz ergeben sich folgende Resultate:



Für jede positive reelle Zahl formula_130 existiert eine Folge formula_131 von Primzahlen mit

Die Menge der aus allen Primzahlen gebildeten Quotienten ist eine dichte Teilmenge der Menge aller positiven reellen Zahlen. D. h.: Für beliebige positive reelle Zahlen formula_57 mit formula_134 existieren stets Primzahlen formula_135, sodass

erfüllt ist.

Einer der ältesten Algorithmen zur Bestimmung von Primzahlen ist das Sieb des Eratosthenes. Bis heute ist kein effizienter Primzahlgenerator bekannt. Es gibt allerdings Formeln, bei denen eine gewisse Wahrscheinlichkeit besteht, dass die erzeugten Zahlen prim sind. Solche Zahlen müssen nachträglich noch auf ihre Primalität getestet werden.


Weitere spezielle Arten von Primzahlen finden sich in der .

In der Ringtheorie wird das Konzept der "Primzahl" auf die Elemente eines beliebigen kommutativen unitären Rings verallgemeinert. Die entsprechenden Begriffe sind "Primelement" und "irreduzibles Element."

Die Primzahlen und deren Negative sind dann genau die Primelemente und auch genau die irreduziblen Elemente des Rings der ganzen Zahlen. In faktoriellen Ringen, das sind Ringe mit eindeutiger Primfaktorisierung, fallen die Begriffe "Primelement" und "irreduzibles Element" zusammen; im Allgemeinen ist die Menge der Primelemente jedoch nur eine Teilmenge der Menge der irreduziblen Elemente.

Insbesondere im zahlentheoretisch bedeutsamen Fall der Dedekindringe übernehmen Primideale die Rolle der Primzahlen.

In Nordamerika weisen manche Zikadenarten einen besonders langen Fortpflanzungsrhythmus von genau 13 oder 17 Jahren auf, mit dem sie den 2-, 4- und 6-jährigen Entwicklungsrhythmen ihrer Fressfeinde ausweichen.





</doc>
<doc id="3976" url="https://de.wikipedia.org/wiki?curid=3976" title="Pearl Harbor (Begriffsklärung)">
Pearl Harbor (Begriffsklärung)

Pearl Harbor bezeichnet:



</doc>
<doc id="3977" url="https://de.wikipedia.org/wiki?curid=3977" title="Pierre-Simon Laplace">
Pierre-Simon Laplace

Pierre-Simon (Marquis de) Laplace (* 28. März 1749 in Beaumont-en-Auge in der Normandie; † 5. März 1827 in Paris) war ein französischer Mathematiker, Physiker und Astronom. Er beschäftigte sich unter anderem mit der Wahrscheinlichkeitstheorie und mit Differentialgleichungen.

Laplace wurde als Sohn eines reichen Landwirtes und Cidre-Händlers geboren. Der Beruf des Vaters sicherte der Familie ein relativ komfortables Leben. Von seinem siebten bis zu seinem sechzehnten Lebensjahr besuchte Laplace als Tagesschüler die Schule des Benediktinerordens im Ort. Nach der Schulausbildung schlugen Kinder des Dritten Standes normalerweise einen militärischen oder kirchlichen Lebensweg ein. Der Vater von Laplace wünschte sich für seinen Sohn eine geistliche Karriere und so studierte Laplace ab 1766 Theologie und Philosophie am Jesuiten-Kolleg von Caen. Dort machte er die Bekanntschaft der Professoren Christoph Gadblet (1734–1782) und Pierre Le Canu, die seine mathematische Begabung erkannten und seine Augen für die Mathematik öffneten.

1768 verließ er deshalb Caen und ging nach Paris mit einem Empfehlungsschreiben an Jean-Baptiste le Rond d’Alembert, den damals berühmtesten Mathematiker Frankreichs, um bei ihm Mathematik zu studieren. Als Laplace bei d’Alembert vorstellig wurde, gab dieser ihm ein mathematisches Problem und eine Woche Zeit, um dieses zu lösen. Laplace löste es über Nacht und klopfte schon am nächsten Tag wieder an d’Alemberts Tür. Darauf gab dieser ihm ein schwierigeres Problem, das Laplace jedoch in derselben Zeit löste. Es ist nicht sicher, ob sich diese Begebenheit wirklich zugetragen hat, sicher ist jedoch, dass d’Alembert sehr von Laplace beeindruckt war und ihn unterstützte und förderte.
Um seine finanzielle Situation zu sichern, verschaffte d’Alembert Laplace 1771 eine Stelle als Lehrer für Geometrie, Trigonometrie, elementare Analysis und Statistik an der Pariser Militärakademie. Von 1770 bis 1773 verfasste Laplace 13 wichtige Abhandlungen zu unterschiedlichen und schwierigen Themen, zu Extremwertproblemen, Astromechanik, Differentialgleichungen, Wahrscheinlichkeits- und Spieltheorie, sowie zur Integralrechnung, um Reputation zu erlangen und an der Pariser Académie des sciences aufgenommen zu werden. Laplace bewarb sich 1771 und 1772 an der Académie, wurde jedoch zugunsten älterer Bewerber abgelehnt. Trotz des Altersunterschieds traf Laplace dies tief, besonders 1772, da Laplace seine mathematischen Fähigkeiten sehr viel höher einstufte als die seines Konkurrenten (Cousin). Überhaupt war Laplace relativ arrogant. D’Alembert schrieb darauf Anfang 1773 einen Brief an Lagrange, der damals in Berlin weilte, um zu erfahren, ob dort die Möglichkeit eines Postens und eines Platzes an der Preußischen Akademie der Wissenschaften für Laplace bestehe. Laplace wäre also fast ein deutscher Wissenschaftler geworden. Dieser Brief erübrigte sich jedoch, als Laplace 1773 im Alter von gerade 24 Jahren als Adjunkter an der Académie française aufgenommen wurde. In den folgenden Jahren steigerte Laplace sein wissenschaftliches Ansehen stetig und wurde zu einem der wichtigsten und einflussreichsten Wissenschaftler. Darunter litt aber sein Verhältnis zu seinen Kollegen. So spürte zum Beispiel d’Alembert, dass die Arbeiten von Laplace einen großen Teil seines eigenen Lebenswerks hinfällig machten. 1784 wurde Laplace Examinateur für die Königliche Artillerie, was damals ein verantwortungsvoller Posten war, da die Anwärter, deren Eignung Laplace prüfte, fast ausschließlich aus sehr gutem Hause kamen und seine Berichte an höchsten Stellen Beachtung fanden. In dieser Funktion prüfte er 1785 auch den damals sechzehnjährigen Napoleon Bonaparte. Im April jenes Jahres wurde Laplace dann ordentliches Mitglied an der "Académie des sciences".

1788 heiratete Laplace die zwanzig Jahre jüngere Marie-Charlotte de Courty de Romanges (* 8. Oktober 1769 in Besançon (Doubs); † 20. Juli 1862 in Paris). Sie gebar ihm ein Jahr später einen Sohn, Charles-Emile Laplace (1790–1874), der später General wurde. 1792 kam ihre Tochter Sophie-Suzanne Laplace (1792–1813) zur Welt. Während der Französischen Revolution konnte Laplace seine Forschungen weitestgehend weiterführen und wurde 1792 Mitglied des Komitees für Maße und Gewichte, das später für die Einführung der Einheiten Meter und Kilogramm sorgte. Dieses Amt musste Laplace aber im Dezember des Jahres aufgeben, genau wie seine übrigen Tätigkeiten, da mit der Herrschaft Robespierres die Mitwirkung an der Revolution und Hass auf die Monarchie Bedingung für die Arbeit wurden. Laplace floh mit seiner Familie vor der Schreckensherrschaft der Jakobiner in das 50 Kilometer südöstlich von Paris liegende Melun.

Nachdem Robespierre am 28. Juli 1794 selbst durch die Guillotine gestorben war, kehrte Laplace nach Paris zurück und wurde im Dezember des Jahres einer der beiden Examinateure für die École polytechnique. 1795 nahm Laplace seine Arbeit im Komitee für Maße und Gewichte wieder auf und wurde dessen Vorsitzender. Im selben Jahr wurde die Akademie mit der Dachorganisation Institut de France neu gegründet. Laplace war Gründungsmitglied und wurde Vizepräsident des Instituts. Fünf Monate später wurde er dessen Präsident. Des Weiteren übernahm er die Leitung des Pariser Observatoriums und des Forschungsbereichs.

Zur selben Zeit knüpfte er erste Beziehungen zu Napoleon, sodass er nach dessen Staatsstreich 1799, angeblich auf eigene Bitte, Innenminister wurde. Dieses Amt versah er jedoch so schlecht, dass er schon nach sechs Wochen durch einen Bruder Napoleons, Lucien, ersetzt wurde. Zum Trost machte ihn Napoleon zum Mitglied des relativ einflusslosen Senats. 1803 wurde Laplace Vizepräsident des Senats und damit ein reicher Mann. Durch die Vielzahl seiner Ämter verdiente er 100.000 Francs im Jahr, was damals eine mehr als stattliche Summe war. Zum Vergleich: 1810 verdiente Gauß als Leiter des Göttinger Observatoriums ungefähr 4.000 Francs.

Nachdem Laplace 1804 im Senat für die Einsetzung Napoleons zum Kaiser gestimmt hatte, adelte dieser ihn 1806 zum Grafen. Im selben Jahr zog Laplace nach Arcueil, einem Vorort von Paris, in das Nachbarhaus des Chemikers Claude-Louis Berthollet. Mit diesem gründete er die Société d’Arcueil, in der die beiden mit anderen, meist jungen Wissenschaftlern, Experimente durchführten. Zu diesen Wissenschaftlern gehörten unter anderen auch Jean-Baptiste Biot und Alexander von Humboldt. Durch diese Arbeit machte er sich jedoch Feinde, da er ein klares Forschungsprogramm aufstellte, das hauptsächlich seine eigenen Forschungsschwerpunkte beinhaltete, und dieses auch gnadenlos durchführte. An weiterem Ansehen verlor Laplace, weil er weiterhin an der Teilchennatur des Lichtes festhielt, während die Wellentheorie durch Augustin Jean Fresnel immer mehr Anerkennung fand. 1813 starb seine Tochter im Kindbett, schenkte Laplace jedoch eine Enkelin. Von ihr stammen alle heutigen Nachkommen von Laplace ab, da sein Sohn zwar 85 Jahre alt wurde, aber kinderlos starb. Weitere Gegner schuf sich Laplace, als er 1814 für die Absetzung Napoleons stimmte und sich sogleich der bourbonischen Restauration zur Verfügung stellte. König Ludwig XVIII. hingegen machte Laplace zum Dank 1815 zum Pair von Frankreich und 1817 zum Marquis (Markgraf). 1816 legte Laplace seine Arbeit an der École polytechnique nieder und wurde Mitglied der 40 Unsterblichen der Académie française. 1822 wurde er in die American Academy of Arts and Sciences gewählt. 1826 erließ der König ein verschärftes Pressegesetz, gegen das viele Wissenschaftler mit Unterschriften zu Felde zogen. Laplace verlor seine letzten politischen Freunde, als er versuchte, Kollegen für das Gesetz einzunehmen. Wegen seiner Eigenschaft, immer auf der Seite der Mächtigen zu sein, wurde Laplace nach seinem Tode trotz großer wissenschaftlicher Erfolge nicht im Panthéon beigesetzt, sondern auf dem Pariser Friedhof.

Laplace’ größtes wissenschaftliches Werk liegt auf dem Gebiet der Astronomie oder genauer der Himmelsmechanik. Von 1799 bis 1823 verfasste er sein Hauptwerk "Traité de Mécanique Céleste" (Abhandlung über die Himmelsmechanik). Dieses fünfbändige Buch erschien auf Deutsch unter dem Namen "Himmelsmechanik". Darin gibt er einen Überblick über alle seit Newton gewonnenen Erkenntnisse sowie über seine eigenen Forschungen und erweist sich als Vollender Newtons. Er gibt einen rechnerischen Beweis für die Stabilität der Planetenbahnen. Aufgrund von Unregelmäßigkeiten in den Bahnkurven war man damals der Meinung, dass das Sonnensystem kollabieren könnte. Außerdem postuliert er die Existenz von Schwarzen Löchern und beschäftigt sich mit dem Drei-Körper-Problem. Obwohl das Werk mathematisch sehr anspruchsvoll und deshalb sehr schwer zu lesen ist, wurde es in der Folgezeit Pflichtlektüre für alle angehenden Astronomen.

Laplace stellte sein Buch dem Ersten Konsul Napoleon Bonaparte vor. Dabei ergab sich ein Gespräch, dessen Wortlaut allerdings in zahlreichen Variationen überliefert ist. Der französische Astronom Hervé Faye zitierte es wie folgt:

Dieses Gespräch wurde von vielen als Ausdruck eines radikalen Atheismus gewertet. Möglicherweise meinte Laplace mit seiner Antwort jedoch etwas anderes. Newton hatte in seinem Werk eine ordnende Funktion Gottes postuliert. Gott sollte regelmäßig in das mechanische Weltgeschehen eingreifen, um die säkularen Störungen, die es zunehmend in Unordnung brachten und zu zerstören drohten, quasi wieder zurechtzurücken. Seit Newton war aber die Himmelsmechanik weiter fortgeschritten und Laplace konnte solche Störungen erklären und berechnen, ohne einen ordnenden Gott zu bemühen. Deswegen ist die Antwort von Laplace wahrscheinlich nicht als vollständige Negierung der Existenz Gottes zu verstehen.

Das Werk war eine zu seiner Zeit umfassende Zusammenstellung des Wissens über die Himmelsmechanik, die noch 50 Jahre später als weitgehend vollständig angesehen wurde. Bereits 1796, drei Jahre vor den ersten beiden Bänden der "Himmelsmechanik", veröffentlichte Laplace die "Exposition du systeme du monde" (Darstellung des Weltsystems). Dieses ebenfalls fünfbändige Buch ist gewissermaßen eine nichtmathematische Einführung zur "Himmelsmechanik". Laplace gibt darin das astronomische Weltbild seiner Zeit wieder und beweist, dass die Wahrscheinlichkeit einer Kollision eines Kometen mit der Erde nur klein, im Verlauf astronomischer Zeiträume jedoch groß ist. Im letzten Band und letzten Kapitel des Werkes entwickelt Laplace auf 19 Seiten eine Theorie über die Entstehung des Sonnensystems, die heute als Nebularhypothese bekannt ist.

1799 stellte er die Hypothese auf, dass auf der Rückseite des Mondes eine größere „Beule“ existieren müsse, die die Bewegung des Mondes beeinflusst.

Das zweite große Forschungsgebiet von Laplace war die Wahrscheinlichkeitsrechnung. Für Laplace stellte sie einen Ausweg dar, um trotz fehlender Kenntnisse zu gewissen Resultaten zu kommen. In seinem zweibändigen Werk "Théorie Analytique des Probabilités" (1812) gab Laplace eine Definition der Wahrscheinlichkeit und befasste sich mit abhängigen und unabhängigen Ereignissen, vor allem in Verbindung mit Glücksspielen. Außerdem behandelte er in dem Buch den Erwartungswert, die Sterblichkeit und die Lebenserwartung. Das Werk stellte eine Widerlegung der These dar, dass eine strenge mathematische Behandlung der Wahrscheinlichkeit nicht möglich sei. Diese These wurde damals von vielen Mathematikern vertreten und auch Laplace’ ehemaliger Lehrer d’Alembert war bis zu seinem Tod 1783 dieser Auffassung.

Zwei Jahre später erschien das Buch "Essai philosophique sur les Probabilités" (Philosophischer Essay über die Wahrscheinlichkeit). Dieses Werk war, wie die "Exposition du systeme du monde", für einen breiten Leserkreis geschrieben, ersparte dem Leser jedoch nur die Formeln und war sonst in keiner Weise einfacher. Neben den Themen der "Théorie Analytique" beschrieb Laplace außerdem einen alles rational erfassenden „Weltgeist“, der die Gegenwart mit allen Details kennt und daher die Vergangenheit und Zukunft des Weltgeschehens in allen Einzelheiten beschreiben kann. Laplace meinte jedoch auch, dass die menschliche Intelligenz dieses nie erreichen könne. Dieser „Weltgeist“ wurde später als Laplacescher Dämon bekannt und sorgte für erbitterten Widerstreit um die Frage, ob ein solcherart verkörperter, vollkommener Determinismus kompatibel oder inkompatibel zur Willensfreiheit sei (siehe Kompatibilismus und Inkompatibilismus).

Das vollkommen deterministische Weltbild wird oft als inkompatibel zu statistischen Interpretationen der modernen Quantenmechanik gesehen (z. B. Bornsche Wahrscheinlichkeitsinterpretation). Einige Physiker konnten sich mit einem grundlegend indeterministischen Charakter mikroskopischer Naturvorgänge nicht anfreunden. Am bekanntesten ist sicher der skeptische Ausspruch Einsteins „Gott würfelt nicht“. Die Frage nach einem allumfassenden Determinismus ist nach wie vor ungeklärt (siehe Determinismus).

1780 machte Laplace mit Lavoisier Experimente mit einem Eiskalorimeter: Sie bestätigten die Äquivalenz von tierischer Atmung mit der Verbrennung von Holzkohle – in beiden Fällen wird durch Sauerstoff Kohlenstoff zu Kohlendioxid verbrannt. Interessant erscheint die Einschätzung des Zufalls: Die Experimentatoren nahmen an, dass die Schwankungen im Experiment prinzipiell auszumerzen wären, wenn man nur alle Bedingungen des Experiments kennen würde ("siehe auch" Determinismus).

Laplace war stets mehr Physiker als Mathematiker. Die Mathematik diente ihm nur als Mittel zum Zweck. Heute sind jedoch die mathematischen Verfahren, die Laplace entwickelte und anwandte, viel wichtiger als das eigentliche Werk an sich. Die wichtigsten sind der Laplacesche Entwicklungssatz, der Laplace-Operator, die Laplace-Gleichung sowie die Laplace-Transformation.

Er ist namentlich auf dem Eiffelturm verewigt, siehe: Die 72 Namen auf dem Eiffelturm.

Nach Pierre-Simon Laplace sind auf dem Mond verschiedene Oberflächenstrukturen benannt worden, z. B. das Promontorium Laplace und der Mondkrater Laplace. Der Asteroid (4628) Laplace ist ebenfalls nach ihm benannt. Gleiches gilt für die Île Laplace in der Antarktis.








</doc>
<doc id="3978" url="https://de.wikipedia.org/wiki?curid=3978" title="Projekt">
Projekt

Ein Projekt ist ein zielgerichtetes, einmaliges Vorhaben, das aus einem Satz von abgestimmten, gesteuerten Tätigkeiten mit Anfangs- und Endtermin besteht und durchgeführt wird, um unter Berücksichtigung von Vorgaben bezüglich Zeit, Ressourcen (zum Beispiel Finanzierung bzw. Kosten, Produktions- und Arbeitsbedingungen, Personal und Betriebsmittel) und Qualität ein Ziel zu erreichen.

Projekt leitet sich ab von , Neutrum zu ‚nach vorn geworfen‘, Partizip Perfekt von ‚vorwärtswerfen‘ (vgl. "Projektil"). Bei Projekten wird unter ‚nach vorn' eine zeitliche Dimension verstanden (siehe auch "Terminplanung").
Das deutsche Wort kommt im späteren 17. Jahrhundert in Gebrauch in der Bedeutung „Bauvorhaben“.

Zur Durchführung von Projekten werden in der Regel Projektteams gebildet, denen Steuerungsaufgaben obliegen. Um dieses Projektmanagement effizient zu gestalten, werden von Beratungsfirmen und Hochschulen spezielle Lehrgänge angeboten.

Viele Projektmanagement-Lehren empfehlen, dass die Ziele bzw. Zielvorgaben eines Projektes nach den SMART-Regeln vorab formuliert werden (SMART = Spezifisch, Messbar, Akzeptiert, Realistisch, Terminiert). Für Forschungsprojekte gilt dies aber nur bedingt.

Das Projektziel bestimmt das strategische Vorgehen. Darauf basieren die nötigen Prozesse/Tätigkeiten und die hilfreiche Grundstruktur, welche den Umgang mit den Ressourcen festlegen.

Aus Sicht des Arbeitsstudiums, welches menschliche Arbeit in Organisationen untersucht, wird der Begriff „Projekt“ in verschiedenen Quellen ähnlich definiert:

Eine Aufgabenstellung kann und sollte in der Regel als Projekt betrachtet werden, sofern das zu lösende Problem relativ komplex erscheint, der Lösungsweg zunächst unbekannt ist, eine Zielrichtung und ein Zeitrahmen vorliegen und/oder bereichs-/fachübergreifende Zusammenarbeit erforderlich ist.

Die Komplexität des Problems liegt beispielsweise darin, dass:

Meist wirken alle diese Faktoren zusammen.

Die Gesamtheit der Tätigkeiten, die mit der erfolgreichen Abwicklung eines Projektes zusammenhängen, münden in einen Regelkreis zur Steuerung von Projekten. Wichtig sind verlässliche Anfangs- und Enddaten zur Planung des Vorhabens und Zusagen für die benötigten Ressourcen im Rahmen eines Projektplans. Im Rahmen großer Investitions- und Bauvorhaben verwenden Engineering- und Projektgesellschaften spezielle Projektkontenrahmen und vorhabensorientierte Projektkontenpläne zur leistungsgerechten Strukturierung der Projektpläne und zur ordnungsgemäßen Erfassung und Abrechnung der Planungs- und Bauleistungen.

In der Regel birgt ein Projekt – im Gegensatz zu regelmäßigen, stets ähnlich durchgeführten, großteils identischen Vorhaben – meist ein höheres Risiko des Scheiterns und wird in einer speziellen und befristeten Organisationsform, der so genannten Projektorganisation, abgewickelt, innerhalb derer auf das Ziel hingearbeitet wird.

Typische Projekte sind Produktentwicklungs-, (Re-)Organisations-, EDV-, Sanierungs- oder Bauprojekte.

Organisationen, die regelmäßig ähnliche Projekte durchführen, sollten bestrebt sein, diese zu Produkten weiterzuentwickeln. Dies wird selten uneingeschränkt möglich sein. Jedoch ist eine Standardisierung des Vorgehens, die den Lerneffekt aus vorangegangenen Projekten wieder in neue Projekte einfließen lässt, ein Vorteil gegenüber einer ständigen „Neuerfindung des Rades“. Diese Standardisierung äußert sich in der Regel in definierten Prozessen, in denen neue Projekte angegangen werden, sowie in vorhandenen Schablonen für Dokumentationen etc., die zwar ggf. projektspezifisch angepasst werden, jedoch bereits die Punkte enthalten, die – aus Erfahrung – nicht vergessen werden sollten.

Neben Projekten im Wirtschaftsbereich gibt es Projekte im Pädagogikbereich mit einer andersartigen Definition und Aufgabenstellung. Diese wurden bereits zu Anfang des Zwanzigsten Jahrhunderts von John Dewey, William Heard Kilpatrick u. a. als Gegenentwurf zum kopfbetonten Frontalunterricht entwickelt. Es handelt sich um Lehr- und Lernformen, mit denen bestimmte Unterrichts- und Erziehungsziele erreicht werden sollen. Als didaktische Konzeptionen werden sie einerseits lediglich als Methode (Frey), andererseits (komplexer) als eine sozialintegrative Unterrichtsform (Warwitz/Rudolf, Bastian u. a.) verstanden und eingesetzt, bei der die Impulse der Lernprozesse nicht vom Lehrer allein ausgehen, sondern in Interessenabstimmung von der gesamten Lehr- und Lerngemeinschaft gestaltet werden. Bei dem Verständnis als komplexer Unterrichtsform werden neben den Lernwegen und der Organisation des Unterrichts (= Methoden) auch die Inhalte, Ziele, Begründungsfragen und Lernerfolgskontrollen der Lernprozesse miteinander ausgehandelt und gemeinsam verantwortet. Ist die anspruchsvollere Form des Projektunterricht durch bestimmte harte Kriterien definiert, so hat der Projektorientierte Unterricht die Funktion, methodisch und motivational auf ihn hinzuarbeiten. Dabei wird von bestimmten Fächern ausgegangen, die sich einer interdisziplinären Kooperation öffnen, etwa als „Projektorientierter Physik-, Deutsch- oder Sportunterricht“. Beide Unterrichtsformen werden vereinfachend auch bisweilen als „Projektarbeit“ bezeichnet.

Nicht konform im Sinne der zeitlichen Begrenztheit, jedoch im Sinne der thematisch/organisatorischen Abgrenzung vom „Normalfall“, also mit der Bedeutung, dass es „etwas Besonderes“ sei, verwendet man die Bezeichnung "Projekt" auch, um:

Sprachwörterbücher (z. B. Duden) definieren Projekte eher in diesem allgemeineren Sinn als "Planung, Unternehmung, Entwurf oder Vorhaben".

In der psychosozialen Betreuung und der Sozialen Arbeit werden (neue) Angebote oft ebenfalls als Projekte bezeichnet, auch wenn sie von vorhinein als dauerhaft oder wiederkehrend konzipiert werden, unabhängig von deren Finanzierung.

Projekte können nach folgenden Gliederungskriterien klassifiziert / differenziert werden (Beispiele). Die individuellen Besonderheiten in diesen Projektarten führten in der Projektorganisation zur Entwicklung spezieller Vorgehensweisen und Bearbeitungstechniken.






Je nach Projektsituation (z. B. der Größe eines Projekts) können bestimmte Rollen in „Personalunion“ besetzt sein, Mitarbeiter können ggf. auch gleichzeitig in mehreren Projekten tätig sein.




</doc>
<doc id="3979" url="https://de.wikipedia.org/wiki?curid=3979" title="Propaganda">
Propaganda

Propaganda (von ‚weiter ausbreiten, ausbreiten, verbreiten‘) bezeichnet in seiner modernen Bedeutung die zielgerichteten Versuche, politische Meinungen und öffentliche Sichtweisen zu formen, Erkenntnisse zu manipulieren und das Verhalten in eine vom Propagandisten oder Herrscher erwünschte Richtung zu steuern. Dies steht im Gegensatz zu pluralistischen und kritischen Sichtweisen, welche durch unterschiedliche Erfahrungen, Beobachtungen und Bewertungen sowie einen rationalen Diskurs geformt werden.

Der Begriff leitet sich vom lateinischen Namen einer päpstlichen Behörde ab, der 1622 von Gregor XV. im Zuge der Gegenreformation ins Leben gerufenen "Sacra congregatio de propaganda fide", zu deutsch etwa „Heilige Kongregation für die Verbreitung des Glaubens“, heute offiziell „Kongregation für die Evangelisierung der Völker.“ Noch im 17. Jahrhundert bürgerte sich die Kurzform "propaganda" – eigentlich die Gerundivform von lat. "propagare", „verbreiten, ausdehnen“ – als Name für diese Missionsgesellschaft ein, deren Zweck es war, dem Protestantismus entgegenzutreten sowie die Neue Welt zu missionieren.

Etwa seit der Zeit der Französischen Revolution wird das Wort im heutigen, weltlichen Sinne gebraucht, also als Bezeichnung für die Verbreitung politischer Ideen. So formierte sich 1790 in Paris der "Club de la propagande", eine Geheimgesellschaft der Jakobiner zur Verbreitung revolutionärer Ideen. In dieser Bedeutung findet sich der Begriff heute in vielen weiteren Sprachen.

In den 1920er Jahren begann die wissenschaftliche Darstellung des Themas, wobei der Begriff weitgehend wertneutral als grundlegender und notwendiger Sachverhalt des gesellschaftlichen Lebens verstanden wurde. Edward L. Bernays, der Begründer der später von ihm in Öffentlichkeitsarbeit umbenannten modernen Propaganda, definierte sie zunächst als „in sich stimmige, anhaltende Bemühung zur Schaffung oder Formung von Ereignissen, um die Beziehungen der Öffentlichkeit zu einem Unternehmen, einer Idee oder Gruppe zu beeinflussen.“ Er unterschied die moderne und mithilfe von Marktforschung, Demoskopie und Psychologie systematisch geplante medial vermittelte Propaganda von älteren weniger professionellen Formen. Seine psychoanalytische Orientierung ließ ihn die Bedeutung des Unterbewussten und der Phänomene der Verdrängung und Verschiebung als zentral verstehen. Statt direkter Appelle setzte seine Propagandatechnik auf die dem Empfänger unbewusste indirekte Erzeugung von Bedürfnissen, die dieser als Ausdruck seiner eigenen Wünsche erlebte.

In Harold D. Lasswells Verständnis von Propaganda spielte die Semiotik symbolischer Interaktion erstmals eine entscheidende Rolle.

Durch die Monopolisierung der Propaganda in diktatorischen Regimen, insbesondere des Nationalsozialismus und Stalinismus, erhielt der Terminus einen stark pejorativen (abwertenden) Charakter und wird fast nur noch kritisch verwendet. Auf Grund dieser negativen Konnotation wurde der Begriff "Propaganda" schon früh durch Edward Bernays selbst durch "Öffentlichkeitsarbeit" (oder dem englischen "Public Relations") ersetzt. Der Begriff „Propaganda“ wird heute vor allem kritisch für politische und militärische Beeinflussung der öffentlichen Meinung benutzt; in wirtschaftlichen spricht man heute eher von „Werbung“ und „Öffentlichkeitsarbeit“, in religiösen von „Missionierung“, in der Politik affirmativ eher von Public Diplomacy, bei Militäraktionen von Psychologischer Kriegsführung.

Als Folge der Abwertung des Begriffs verwendet z. B. keine der demokratischen Parteien der Bundesrepublik Deutschland für ihre Werbemaßnahmen die Bezeichnung "Propaganda".

Kriegspropaganda ist gemäß dem von 168 Staaten ratifizierten Internationalen Pakt über bürgerliche und politische Rechte, Artikel 20, seit 1976 verboten:

Gezielte und organisierte Kriegspropaganda wurde von allen kriegführenden Mächten betrieben, im deutschen Kaiserreich dabei stark von der Obersten Heeresleitung, in Großbritannien vom War Propaganda Bureau und in Frankreich vom Maison de la Presse.

Zum Beispiel spielten bei der psychologischen Kriegführung sogenannte Maueranschläge eine wichtige Rolle, sowohl bei den Mittelmächten als auch bei der Entente und ihren Alliierten. So beteiligten sich in Deutschland zahlreiche Künstler, u. a. Walter Trier, Louis Oppenheim und Paul Brockmüller, an der Gestaltung zahlreicher Plakate.

In den kriegführenden Ländern wurde Propaganda gegen die Kriegsgegner gemacht. Vor allem die Erfindung des Films führte zu einer großen Anzahl von Propagandafilmen.

Adolf Hitler und sein Reichsminister für Volksaufklärung und Propaganda Joseph Goebbels gaben in der Zeit des Nationalsozialismus der Propaganda eine totalitäre und dominante Bedeutung und nutzten dazu vor allem die Presse, den Rundfunk, sämtliche Medien der Künste und symbolisch markant aufgezogene Massenveranstaltungen.

Die Anti-Hitler-Koalition betrieb über das United States Office of War Information und das Informationsministerium des Vereinigten Königreichs mehrere Feindsender und Rundfunkpropaganda, als auch wurde die Deutsche Wehrmacht und vor allem Adolf Hitler auf Plakaten häufig lächerlich gemacht.

Lenin verstand unter Propaganda die allgemeine Überzeugungsarbeit von Kommunisten, im Unterschied zur Agitation, die ein „"Appell an die Massen zu bestimmten konkreten Aktionen"“ sei. Besonders in den Anfangszeiten der Sowjetunion war die Agitprop durch moderne Kunstrichtungen (den Futurismus) beeinflusst.

Agitprop war ein wichtiges Mittel der Herrschaftssicherung der Sozialistischen Einheitspartei Deutschlands (SED) in der Deutschen Demokratischen Republik. Ihr Ziel bestand u. a. in der Diskreditierung der Wirtschafts- und Gesellschaftsordnung der Bundesrepublik Deutschland. Sie richtete sich allgemein gegen Kapitalismus und „westlichen Imperialismus“. Da alle Medien vom Staat zensiert und gesteuert wurden, war dessen Propaganda allgegenwärtig. Als permanente politisch-ideologische Indoktrination wurde sie bereits in den staatlichen Kindergärten praktiziert und im Schulunterricht (Staatsbürgerkunde) fortgesetzt. Massenorganisationen wie Junge Pioniere, FDJ, FDGB und andere waren integraler Bestandteil des staatlichen Propagandaapparates. Das Eindringen mittels Propaganda in Familien, die Unterdrückung der Opposition und die versuchte Einflussnahme auf die gesamte Gesellschaft sind typische Kennzeichen einer totalitären Herrschaft.

Ein wichtiges Element der DDR-Propaganda war die Fernsehsendung „Der schwarze Kanal“. Propagandamethoden waren ein fester Ausbildungsbestandteil für Kader, so z. B. im „Roten Kloster“, der "Fakultät für Journalistik" in Leipzig, einer Ausbildungsstätte, die direkt dem Zentralkomitee der SED unterstand.

Die DDR setzte sich auch propagandistisch mit den Notstandsgesetzen der Bundesrepublik Deutschland auseinander und stellte eine Verbindung zur nationalsozialistischen Justiz her.

In der Bundesrepublik wurde Propaganda zu Zeiten des Kalten Krieges in den öffentlich-rechtlichen Rundfunk- und Fernsehanstalten und privaten Medien sowie in vielen übrigen Bereichen des täglichen Lebens eingesetzt, oft mit starker Wendung gegen die DDR. Eine tragende Rolle hatte das Bundesministerium für innerdeutsche Beziehungen und privat-rechtliche Propaganda-Organisationen wie z. B. der Volksbund für Frieden und Freiheit, aber auch einige politischen Parteien, die mit ihrer antikommunistischen Haltung Angst schürten und Wahlkampf betrieben.

Neben der offenen Propaganda im alltäglichen Leben gab es auch verdeckte staatliche Aktionen, die vom Bundesministerium für Verteidigung als operative Information systematisch durchgeführt wurden. Der damalige Verteidigungsminister Franz Josef Strauß richtete 1958 ein Referat für "Psychologische Kampfführung" ein, in dem u. a. Eberhard Taubert, früherer Mitarbeiter im Reichspropagandaministerium, führend mitwirkte.

Kurz nach der Geiselnahme von Beslan im September 2004 verstärkte Präsident Putin ein von der russischen Regierung getragenes Projekt, das Bild Russlands in der Welt zu verbessern. Ein Hauptprojekt war 2005 die Schaffung des englischsprachigen Programms von "Russia Today", das ein durchgängiges Programm aktueller Nachrichten anbietet und nach dem Vorbild des US-Senders CNN gestaltet wurde. Es wurde anfangs mit 30 Millionen Dollar aus öffentlichen Mitteln finanziert. CBS News zitierte zum Start des Senders den Politologen Boris Kagarlitsky, "Russia Today" setze weitgehend die alte Sowjetpropaganda fort.

Im Zuge des Krieges gegen den Terror durch die Regierung George W. Bush startete das Verteidigungsministerium der Vereinigten Staaten im Jahr 2002 eine Propaganda-Strategie, um die Sichtweise der amerikanischen Regierung zu außenpolitischen Themen wie dem Irak- und Afghanistankrieg sowie dem Gefangenenlager von Guantanamo Bay in der Öffentlichkeit zu verbreiten. Das Ministerium ließ ausgewählten Experten, meist Veteranen, bevorzugte Informationen zukommen, woraufhin sie die Argumente der Regierung im Radio und Fernsehen verbreiteten. Aus der Befürchtung, den privilegierten Zugang zu verlieren, verbreiteten einige von ihnen diese Sichtweise nachdem sie selber Zweifel unterdrückt hatten. Das Programm wurde 2008 von dem Journalisten David Barstow unter Zuhilfenahme des Freedom of Information Act aufgedeckt, was zu großer öffentlicher Aufmerksamkeit führte. Die betroffenen Rundfunkanstalten, die im Verdacht standen, mitgewirkt zu haben, berichteten jedoch kaum darüber.

Propaganda, Überredung und Rhetorik werden oft gleichsinnig oder im selben Kontext benutzt. Wissenschaftler haben eine Vielzahl von Propagandatechniken analysiert oder entwickelt, die zum Teil auch als logische Fehlschlüsse (Trugschlüsse) und Scheinargumente, als rhetorische Mittel oder eristische Strategeme klassifiziert werden.

Nach Charles U. Larson wird Sprache für drei Typen von Propaganda eingesetzt: hypnotische, semantische und kognitive Propaganda. Die hypnotische lässt den Empfänger selbsttätig den erwünschten Gedanken denken, indem er durch Leerstellen dazu gebracht wird, den Sachverhalt aus seiner Sicht zu interpretieren: „Wir müssen "unsere Werte" verteidigen“; Die semantische Propaganda arbeitet mit Auslassungen, Verallgemeinerungen und Verdrehungen des sprachlichen Sinnes, die kognitive führt zu kognitiver Verzerrung oder nutzt psychologische Tendenzen der Verzerrung aus. Hypnotische, kognitive und semantische Propaganda sieht Larson in enger Verbindung.

Serge Moscovici sieht sprachliche Propaganda in einem Herrschaftssystems als dritte Ebene der Beeinflussung der Vorstellungen der Menschen. Erste Ebene ist der Raum und seine atmosphärische Wirkung, die zweite Ebene die Art der Veranstaltung, etwa eine Feier, das an diesem Ort und bei diesem Anlass gesprochene Wort enthält nach seiner Darstellung auch durch Atmosphäre und Kontext eine persuasive Bedeutung.











</doc>
<doc id="3980" url="https://de.wikipedia.org/wiki?curid=3980" title="Pazifischer Ozean">
Pazifischer Ozean

Der Pazifische Ozean ( ‚friedlich‘), auch Pazifik, Stiller Ozean oder Großer Ozean genannt, ist der größte und tiefste Ozean der Erde und grenzt an alle anderen Ozeane.

Am 28. November 1520 erreichte Ferdinand Magellan aus der Magellanstraße kommend auf seiner Weltumsegelung den Pazifik oder Stillen Ozean. Er nannte ihn "Mar Pacifico" (Portugiesisch und Spanisch für "Friedliche See"), weil die ihn bis dahin begleitenden Stürme sich legten. Trotz dieses Namens gibt es im Pazifik auch heftige Stürme und Wirbelstürme, die je nach Region Taifun oder Hurrikan genannt werden.

Der Pazifik liegt zwischen der Arktis im Norden, Nordamerika im Nordosten (siehe Pazifischer Nordwesten), Mittelamerika im Osten, Südamerika im Südosten, der Antarktis im Süden, Australien im Südwesten, Ozeanien im Westen und Asien im Nordwesten (siehe Nordwestlicher Pazifikraum). Im Norden grenzt er an den Arktischen Ozean und im Süden an den Südlichen Ozean, der südlich des 60. Breitengrades liegt.

Der pazifische Pol der Unzugänglichkeit (Point Nemo), die Stelle, welche am weitesten von Festland und Inseln entfernt ist, befindet sich im Südpazifik zwischen Neuseeland und Chile.

Die meisten Randmeere des Pazifiks liegen vor der asiatisch-australischen Seite und werden zum Ozean hin durch eine Kette von Inseln und Tiefseegräben getrennt.

Randmeere des Pazifik sind:

Die zahllosen Inseln der Südsee werden mehr nach kulturellen als nach geographischen Gesichtspunkten in die Archipele Melanesien, Polynesien und Mikronesien unterteilt. Zu Melanesien gehört mit Neuguinea die zweitgrößte Insel der Welt. Neuseeland und die umliegenden Inseln wie z. B. die Chathaminseln werden üblicherweise zu Polynesien gezählt.

Andere größere Inseln und Inselgruppen außerhalb dieser Räume, im Uhrzeigersinn von Westen aus gesehen:

Einige größere Inseln und Inselgruppen am Rande des Pazifiks sind in ihrer Zuordnung umstritten. Das sind Feuerland (Chile) und die vorgelagerten Inseln in der Drakestraße, die am Übergang zum Atlantik liegen, Taiwan und die Philippinen, die am Rand zum Südchinesischen Meer liegen und Tasmanien (Australien), das gelegentlich zum Indischen Ozean gezählt wird. Weiter südlich existieren noch einige antarktische und subantarktische Inseln wie die Peter-I.-Insel, die Scott-Insel und die Balleny-Inseln, die im Allgemeinen zum Südlichen Ozean gezählt werden.

Am Pazifik liegen folgende Anrainerstaaten (Aufzählung im Uhrzeigersinn):<br>
Kanada, USA, Mexiko, Guatemala, El Salvador, Honduras, Nicaragua, Costa Rica, Panama, Kolumbien, Ecuador, Peru, Chile, Australien, Malaysia, Thailand, Kambodscha, Vietnam, China, Südkorea, Nordkorea und Russland.

Am und im Pazifik liegen die folgenden Inselstaaten (alphabetische Nennung):<br>
Cookinseln, Fidschiinseln, Französisch-Polynesien, Indonesien, Japan, Kiribati, Marshallinseln, Föderierte Staaten von Mikronesien, Nauru, Neuseeland, Osttimor, Palau, Papua-Neuguinea, Philippinen, Republik China (Taiwan), Salomonen, Samoa, Tonga, Tuvalu, Vanuatu sowie Wallis und Futuna.

Die Fläche des Ozeans beträgt mit Nebenmeeren 181,34 Mio. km², ohne die Nebenmeere 166,24 Mio. km², was rund 35 Prozent der gesamten Erdoberfläche oder die Hälfte der Meeresfläche der Erde und damit mehr als die Fläche aller Kontinente zusammen ausmacht. Sein Volumen beträgt mit Nebenmeeren 714,41 Mio. km³, ohne die Nebenmeere 696,19 Mio. km³ – mehr als die Hälfte allen Wassers auf der Erde. Während seine mittlere Tiefe mit Nebenmeeren 3.940 m (4.188 m ohne Nebenmeere) beträgt, liegt seine tiefste Stelle etwa 11.000 m unter dem Meeresspiegel.

Die größte Ausdehnung in West–Ost-Richtung hat der Pazifik bei 18°39′N (Hainan, VR China 110°15′O < "15.409 km" > 103°42′W Mexiko); die größte Ausdehnung Nord–Süd hat der Pazifik bei 172°9′W (Sibirien, Russische Föderation 64°45′N < "15.883 km" > 78°20′S Ross-Schelfeis, Kontinent Antarktika). Die West–Ost-Distanz ist die längste aller Weltmeere; die Nord–Süd-Distanz wird nur noch vom Atlantischen Ozean übertroffen (15.986 km bei 34°46′W).

Innerhalb des Pazifischen Ozeans beziehungsweise auf dessen Grund befinden sich teils hohe und langgestreckte Mittelozeanische Rücken, viele niedrigere Schwellen, riesige Tiefseebecken, Tiefseerinnen, verschiedene Meerestiefs und der Pazifische Feuerring.

Zu den Mittelozeanischen Rücken gehört insbesondere der Ostpazifische Rücken, der sich durch den Südosten des Pazifischen Ozeans zieht. Zu den Tiefseerinnen beziehungsweise Meerestiefs gehört der Marianengraben mit dem (nach einer umstrittenen Messung von 1957) 11.034 m tiefen Witjastief 1 und dem Challengertief (10.984 ± 25 m nach Messungen von 2010), den tiefsten Stellen des Pazifischen Ozeans. Zu den Tiefseebecken gehört das äußerst große Nordpazifische Becken, das den Großteil des nördlichen Pazifiks ausmacht, in Richtung Süden sogar über den Äquator hinaus reicht und nur wenig vom Mittelozeanischen Rücken und Schwellen durchzogen ist.

Am Pazifischen Feuerring, einem weitausgedehnten, um den Ozean verlaufenden Ring von aktiven Vulkanen, verbunden mit Tiefseerinnen, fanden einige der gewaltigsten Erdbeben des 20. Jahrhunderts statt, zum Beispiel 1960 das Große Chile-Erdbeben und in Japan 1923 das Große Kantō-Erdbeben. Eine weitere Gefahr sind Seebeben, welche meterhohe Tsunamis auslösen können.

Ein Strudel aus Plastikteilen schwimmt etwa tausend Kilometer nördlich von Hawaii und umfasst dabei eine Fläche von rund 7000 km² – etwa die dreifache Größe des Saarlands. Es kursieren wiederholt Angaben, in denen die Fläche sehr stark übertrieben wird; zum Teil um Faktor 200 mehr als tatsächlich der Fall ist. Der Plastikmüll stammt zu ca. 80 % vom Festland und wird über die Flüsse ins Meer gespült. Der restliche Teil kommt von Schiffen, die ihre Abfälle über Bord werfen. Die Plastikteile treiben bis zu 16 Jahre in diesem Strudel und stellen eine große Gefahr für die Fauna dar. Durch die Reibung der Wellen zerfällt der Müll zu immer kleineren Teilen, löst sich aber nicht ganz auf.

Die nachfolgend aufgezählten Häfen stellen aufgrund ihrer Lage und ihrer Größe die wichtigsten Liegestellen für Schiffe aller Art dar. Die Auflistung erfolgt dabei nach Staaten von Norden nach Süden sortiert.

In Kanada finden sich in Vancouver, Kitimat, Prince Rupert und in Victoria wichtige Seehäfen (alle in der Provinz British Columbia). Auf dem Gebiet der Vereinigten Staaten liegen die Häfen von Anchorage, Long Beach, Los Angeles, Oakland, Portland, San Diego und San Francisco. Südlich davon, in Mexiko, befinden sich Seehäfen in Acapulco, Ensenada und Salina Cruz. Am Panamakanal liegt der Hafen von Panama-Stadt.

In Südamerika liegen in Kolumbien der Hafen von Buenaventura, weiterhin in Ecuador der von Guayaquil. Peru weist in Callao einen bedeutenden Seehafen vor. Des Weiteren befinden sich in Chile folgende wichtige Häfen: Antofagasta, Arica, Iquique, Puerto Montt, San Antonio sowie Talcahuano und Valparaiso.

Der bedeutendste russische Hafen am Pazifik liegt in Wladiwostok, das südkoreanische Pendant dazu ist Busan. Südlich davon schließen sich Häfen auf chinesischem Hoheitsgebiet an, darunter Dalian, Qingdao, Shanghai, Shenzhen, Tianjin, Xiamen und Yantai.

In Thailand finden sich in Bangkok, Laem Chabang sowie Songkhla wichtige Seehäfen. Am Südende des südchinesischen Meeres liegt der Hafen von Singapur.

Inmitten des Pazifik liegen auf den Inseln Japans die Seehäfen von Kōbe, Nagoya, Osaka und Yokohama. Auf Taiwan folgen die Häfen Kaohsiung, Keelung und Taichung. Auf den Philippinen befinden sich wichtige Liegestellen in Cebu City sowie in Metro Manila.

Der australische Kontinent weist vier wichtige Häfen auf: Brisbane, Melbourne und Sydney in Australien sowie Auckland in Neuseeland.





</doc>
<doc id="3982" url="https://de.wikipedia.org/wiki?curid=3982" title="Pisa (Begriffsklärung)">
Pisa (Begriffsklärung)

Pisa steht für:


Pisa ist der Familienname folgender Personen:

Nach ihrer Herkunft aus Pisa wurden benannt:

PISA steht als Abkürzung für:

PiSA steht als Abkürzung für:

Siehe auch:



</doc>
<doc id="3983" url="https://de.wikipedia.org/wiki?curid=3983" title="Provitamin">
Provitamin

Provitamine sind Vorstufen der Vitamine und müssen erst noch in diese umgewandelt werden, um für den Körper von Nutzen zu sein.




</doc>
<doc id="3984" url="https://de.wikipedia.org/wiki?curid=3984" title="Pest">
Pest

Die Pest ( ‚Seuche‘) ist eine hochgradig ansteckende Infektionskrankheit, die durch das Bakterium "Yersinia pestis" ausgelöst wird. Sie ist ursprünglich eine Zoonose von Nagetieren (Murmeltiere, Ratten, Eichhörnchen), bei deren Populationen sie enzootisch sein kann. Daher kommt auch der Begriff „silvatische Pest“ (lat. ‚Wald‘) bei sich unmittelbar ansteckenden Menschen. Der Ausbruch dieser Erkrankung ist gegebenenfalls weltweit möglich.

Ein historischer Überblick über die Krankheit und ebenfalls als "Pest" bezeichnete Seuchen, die viele Menschenleben forderte, ist unter Geschichte der Pest nachzulesen.

Die Pest wird bei Mensch und Tier durch das Bakterium "Yersinia pestis" ausgelöst. Dieses Bakterium, eine Mutation des für den Menschen relativ ungefährlichen Bakteriums "Yersinia pseudotuberculosis", ist sehr anpassungsfähig, und es werden sehr viele verschiedene Varianten beschrieben. Die krankmachenden Eigenschaften von "Yersinia pestis" entstehen durch Ektotoxin-, Endotoxin- und Bakterienkapselbildung.

Die Verbreitung der Pest hängt von der Verbreitung der Zwischenwirte ab. Wo diese festgestellt werden, sind immer auch Pestfälle möglich. Ob sie zu Epidemien auswachsen können, hängt von mehreren Faktoren ab, wie beispielsweise Resistenz der Bakterien gegen Medikamente, den vorherrschenden hygienischen Verhältnissen und der Bekämpfung der lokalen Zwischenwirte.

Die Pest kann auf verschiedene Weise übertragen werden: zum einen durch den Biss von mit Krankheitserregern verseuchten Insekten, vorwiegend Flöhen, zum anderen durch Tröpfcheninfektion. Letztere Übertragungsart führt zur primären Lungenpest.

Das Zwischenglied bei der Übertragung von der Ratte auf den Menschen ist der Floh. Als erster entdeckte diesen Zusammenhang 1898 Paul-Louis Simond. An erster Stelle steht die tropische Flohart "Xenopsylla cheopis" (Rattenfloh). Über die Bedingungen und Mechanismen der Verbreitung der Pest durch diesen Floh siehe dort. Diese Flohart kommt in Europa wegen der für diese Art zu kühlen Witterungsbedingungen nicht vor. A. W. Bacot vermutete, dass der Menschenfloh ("Pulex irritans"), der in Europa verbreitet ist und sich durch eine große Variationsbreite in Bezug auf Wirtstiere auszeichnet, für die Übertragung verantwortlich sei. Die Forscher Hariette Chick und C. J. Martin schlugen "Nosopsyllus fasciatus" (= "Ceratopsyllus fasciatus") als Überträger vor. Diese Flohart macht die Hälfte der Flöhe in England aus. Diese beiden Arten kommen mit tieferen Temperaturen weit besser zurecht als "Xenopsylla cheopis". Hinzu kommt, dass dessen Eier bei 13 °C absterben, so dass Bacot meinte, dass mindestens 15,5 °C vorliegen müssten, um dessen Flohpopulation am Leben zu erhalten. Demgegenüber überlebte ein Teil der Eier von "Pulex irritans" noch bei 8 °C, und die Hälfte der Eier von "Nosopsyllus fasciatus" überstand sogar Temperaturen von 5 °C. Heute geht man von einem Temperaturfenster von 0 bis 40 °C für diesen Floh aus. "Nosopsyllus fasciatus" und "Pulex irritans" finden sich weit verbreitet in England, Wales, Schottland, den Shetlands, den Orkneys sowie in Irland.

Diese Floharten unterscheiden sich in ihrer Vektoreffektivität. Damit bezeichnet man die Effektivität, mit der eine Flohart zur Krankheitsübertragung in der Lage ist. C. M. Wheeler und J. R. Douglas betrachteten die Vektoreffektivität als von drei Potentialen abhängig, deren jedes ein Maß für die jeweils nachgenannte Frage ist:

Man führte dann den Vektor-Index ein, um die verschiedenen Floharten miteinander in diesem Punkte vergleichen zu können. Die "Xenopsylla"-Arten wurden zum Maßstab genommen. "Nosopsyllus fasciatus" kommt diesen am nächsten. Dagegen zeigt "Pulex irritans" geringe Vektoreffektivität, ähnlich wie Katzen- und Hundeflöhe, weil bei ihnen die erforderliche Blockade durch Bakterienklumpen selten vorkommt. Bei Laborversuchen kam "Nosopsyllus fasciatus" auf den 2. Platz hinter "Xenopsylla cheopis". Bei "Pulex irritans" kam es nur bei einem von 57 Exemplaren zur Blockade, und dieses Exemplar starb, bevor es seine Infektion weitergeben konnte. Georges Blanc und Marcel Baltazard gingen einen anderen Weg: In der Pest von 1940 in Marokko fingen sie "Pulex irritans" in Häusern Pestverstorbener in Marrakesch, zerdrückten sie und spritzten ihre Lösung in Meerschweinchen, die alsbald an Pest verstarben. Damit lenkten sie den Blick auf die Möglichkeit, dass die Pest ohne Ratte vom Menschenfloh unmittelbar übertragen werden konnte, worauf sie in einer weiteren Veröffentlichung hinwiesen. Die marokkanischen Häuser waren voll von Menschenflöhen. Von gut 3500 eingesammelten Flöhen waren 3000 "Pulex irritans", während nur knapp 600 Exemplare "Xenopsylla cheopis" gefunden wurden. Dagegen wandte Georges Girard ein, dass die Pestepidemien in Indien, Senegal und Madagaskar starke Unterschiede zu der marokkanischen aufwiesen, obgleich auch dort "Pulex irritans" in Mengen aufgetreten waren. Er bestritt im Übrigen aus seiner Erfahrung die Effektivität als Übertragungsvektor von "Pulex irritans". Aber er hielt es für möglich, dass die Menge der Flöhe in Marokko den Mangel an Effektivität ausgeglichen habe. Andere Untersuchungen von Pest in Nordafrika, besonders in Ägypten, zeigten, dass der Menschenfloh an der Verbreitung der Pest nicht beteiligt war, obgleich er in hohem Grad von der Pest infiziert war. Atilio Macchiavello stellte andererseits das vollständige Fehlen von "Xenopsylla cheopis" bei einem Pestausbruch in Peru 1946 in 600–700 m Höhe fest.
Robert Pollitzer und Karl F. Meyer bestimmten dann die Pestübertragung durch Flöhe näher als massenhaften Befall von Flöhen, deren Saugwerkzeuge von vorherigem Befall von Nagern infiziert waren (mechanische Übertragung), oder als Bisse von im Verdauungssystem blockierten Flöhen (biologische Übertragung).<ref name="Pollitzer/Meyer">Robert Pollitzer und Karl F. Meyer: "The Ecology of Plague". In: Jacques M. May (Hrg.): "Studies in Disease Ecology, Studies in Medical Geography." Band 2, New York 1961, S. 433–590.</ref> In Nordamerika ist der Hauptüberträger der Pest von Tier auf Mensch der Floh „Oropsylla montana“, obwohl bei diesem keine Blockade eintritt.

Ein wesentlicher Faktor bei der Übertragung der Pest durch den Floh ist die Zahl der Bakterien, die er bei einem Biss injiziert. Ole Jørgen Benedictow ging von 25.000 Bakterien pro Biss eines blockierten Flohs aus. Allerdings waren die Zahlen vor Einführung der PCR-Technik sehr ungenau. Mit dieser Methode hat man um die 100.000 Bakterien von "Yersinia pestis" in den infizierten Exemplaren gefunden.<ref name="Engelthaler/Gage">D. M. Engelthaler, K. L. Gage: "Quantities of Yersinia pestis in fleas (Siphonaptera: Pulicidae, Ceratophyllidae, and Hystrichopsyllidae) collected from areas of known or suspected plague activity." In: "Journal of medical entomology." Band 37, Nummer 3, Mai 2000, S. 422–426, . PMID 15535587.</ref>

Auch wurde bei Untersuchungen von Flöhen in New Mexico und Colorado ein Zusammenhang zwischen Bakterienkonzentration und Mikromilieu der Flöhe festgestellt: Flöhe, die sich vom Wirtstier gelöst und in die Erde vergraben hatten, hatten höhere Konzentrationen als solche im Pelz des Wirtstieres. Die vom Boden aufgesammelten Flöhe waren nicht alle infiziert, aber die, die es waren, hatten eine ausreichende Konzentration für die Blockierung, während bei den Flöhen im Pelz eines Wirtstieres dies nur bei 1 von 50 Flöhen der Fall war. Dafür war die Infektionsrate bei den Letzteren höher.

Der Aufenthalt der Flöhe außerhalb von Wirtstieren in Nestern und im Boden ist jedoch keine besondere Verhaltensweise bestimmter Floharten, so dass die Unterscheidung zwischen Pelzfloh und Nestfloh nicht weiterführt. Pollitzer und Meyer stellten fest, dass es zwischen Nestflöhen und Pelzflöhen keine Trennungslinie gibt. Das unterschiedliche Verhalten in diesem Zusammenhang zwischen "Xenopsylla cheopis" und "Nosopsyllus fasciatus" beruht auf ihren Fressgewohnheiten: "cheopis" beißt oft und verlässt daher selten und nur kurz das Wirtstier, während "fasciatus" seltener beißt und daher längere Zeit auch ohne Wirtstier lebt. Nach Pollitzer und Meyer hängt dies aber nicht mit der Art, sondern mit dem Klima zusammen, in welchem die Flöhe leben: "cheopis" in tropischen Breiten, "fasciatus" in kühleren Gegenden. Von diesen Erkenntnissen ausgehend ist "fasciatus" nicht unbedingt ein schlechterer Pestvektor als "cheopis".

Es hat sich gezeigt, dass die Pest über 200 Säugetierarten befallen kann, also nicht auf Ratten beschränkt ist. Sie wurde auch bei Hunden und Katzen festgestellt. Neben der braunen bis schwarzen Hausratte ("Rattus rattus") und der grau-braunen Wanderratte ("Rattus norvegicus") wurde auch der Hausmaus ("Mus musculus") die Auslösung von Epidemien zugeschrieben, so die in Südost-Russland in den 20er Jahren, in Brasilien 1936–1945 und in Saigon 1943. Gleichwohl spielt die Hausmaus in diesem Zusammenhang nur eine untergeordnete Rolle, da sie nicht die hohe Bakterienkonzentration im Blut entwickelt, die erforderlich ist (Pollitzer 1954 S. 299–300). Außerdem ist deren Floh "Leptopsylla segnis" ein schlechter Überträger. Er nimmt nur wenig Pestbakterien auf. Auch ist der Floh in hohem Grade auf die Maus fixiert. Die Ratten standen daher immer im Vordergrund. Das beruhte auf der Beobachtung bei der Pest 1905 in Bombay, dass es zu dieser Zeit dort eine Überfülle von Ratten beider Arten gab. Die Kommission beobachtete, dass die Seuche zuerst die Wanderratte ergriff, etwa 10 Tage danach die Hausratte, und der Höhepunkt der Sterblichkeitsrate bei den Menschen knapp 1 Monat später auftrat. 1910 starben einige Kilometer entfernt von Ipswich einige Personen an einer bakteriologisch identifizierten Pest. Daraufhin machte man Jagd auf Ratten, und von den 568 gefangenen Exemplaren wiesen 17 Pestbakterien auf.
Alle in dieser ländlichen Gegend waren Wanderratten. Aber man geht davon aus, dass die Schwarze Ratte der wichtigste Vermittler der Pestepidemie von Indien 1898 bis Madagaskar 1998 gewesen ist. Der Floh bleibt nur bei lebenden Tieren. Sobald das befallene Lebewesen erkaltet, verlässt der Floh den Wirt.
Da in Südamerika häufig Meerschweinchen gegessen werden, kommt es auch in neuerer Zeit immer wieder zu Ansteckungen.
Wenn bei der Infektion ausreichend viele Bakterien in die Blutbahn gelangt sind, sodass die körpereigene Abwehr ihrer nicht mehr Herr wird, kommt es nach kurzer Zeit zu einer hohen Bakterienkonzentration im Blut, die dann zu einer Sepsis führt.

Die blutvergiftende Wirkung wird ausgelöst, wenn die Bakterien ihren normalen Lebenszyklus vollenden und absterben. Dabei werden große Mengen toxischen Sekrets direkt in den Blutkreislauf abgegeben; Nieren und Leber können nekrotisch werden, wenn sie versuchen, den Organismus von Toxinen zu reinigen. Am Ende erliegt das Opfer einem toxischen Schock.

Man unterscheidet vier Erscheinungsformen der Pest: Beulenpest, auch "Bubonenpest" genannt (griechisch ' „Beule“), Pestsepsis, Lungenpest sowie die abortive Pest"'. Bei Pandemien treten alle Formen der Erkrankung auf, am häufigsten jedoch die Beulenpest und die Lungenpest. Aus einer Beulenpest entwickelt sich ohne Behandlung oftmals eine Pestsepsis, die zu einer Lungenpest führt. Selten tritt auch die Pestmeningitis auf, wenn die hämatogene Streuung der Pesterreger ("Yersinia pestis") nach Beulenpesterkrankung die Hirnhäute befällt.

Bei der Beulenpest erfolgt die Ansteckung gewöhnlich durch den Biss eines Rattenflohs, der den Erreger als Zwischenwirt in sich trägt. Durch den Wirtswechsel wird das Bakterium von einem infizierten auf ein bislang gesundes Nahrungsopfer übertragen, nachdem es sich im Floh vermehrt hat.

Die Inkubationszeit liegt bei wenigen Stunden bis sieben Tagen. Die Symptome sind Fieber, Kopf- und Gliederschmerzen, starkes Krankheitsgefühl und Benommenheit. Später kommt es zu Bewusstseinsstörungen. Der Name Beulenpest stammt von den stark geschwollenen, sehr schmerzhaften Beulen am Hals, in den Achselhöhlen und in den Leisten, die durch die Infektion der Lymphknoten und Lymphgefäße im Bereich des Flohbisses entstehen. Diese Beulen können einen Durchmesser von bis zu zehn Zentimetern erreichen und sind aufgrund innerer Blutungen in den Lymphknoten blau-schwarz gefärbt. Die Geschwülste zerfallen, nachdem sie eitrig eingeschmolzen sind.

Die Pestsepsis entsteht durch Eintritt der Bakterien von ihrem Vermehrungsort in die Blutbahn. Dies kann durch Infektion von außen, zum Beispiel über offene Wunden, geschehen, aber auch als Komplikation aus den beiden anderen schweren Verlaufsformen, zum Beispiel durch Platzen der Pestbeulen nach innen. Die Erreger im Blut verteilen sich mit dem Blutstrom im gesamten Körper. Die Infektion bewirkt hohes Fieber, Schüttelfrost, Kopfschmerzen und ein allgemeines Unwohlsein, später Schock, großflächige Haut- und Organblutungen (daher der Name "Schwarzer Tod"). Pestsepsis ist unbehandelt praktisch immer tödlich, in der Regel spätestens nach 36 Stunden.

Heute kann durch die Behandlung mit Antibiotika die Sterblichkeit deutlich gesenkt werden.

Die Lungenpest ist noch nicht völlig verstanden, da sie heute relativ selten vorkommt. Gleichwohl ist sie interessant, weil sie die einzige Pestform mit spezifischem Ansteckungsweg und Ausbreitungsmuster ist. Sie dürfte der Influenza ähneln, wenn auch die Ausbreitungskraft wesentlich schwächer ist. Die Ausbreitung ist so spezifisch, dass sie nur unter besonders günstigen Umständen zur Epidemie werden kann. Zunächst sind die Ansteckungsquellen selten. Nur ein kleiner Teil der pestinfizierten Bevölkerung bekommt Lungenpest. Man kann zwar durch Säugetiere angesteckt werden, aber dabei handelt es sich in aller Regel um Schoßtiere. So hatten sich die meisten Patienten mit Lungenpest in Amerika in den letzten 10 Jahren an infizierten Katzen angesteckt. 

Die physische Nähe zur Pestquelle ist eine weitere Voraussetzung. Der kritische Abstand zum Gesicht eines Lungenpestkranken für eine Ansteckung liegt bei 30 cm und darunter. Im Gegensatz zu den Influenza-Viren sterben die Pestbakterien in der Luft rasch ab. Ein weiteres Moment, das die Ausbreitung vermindert, ist, dass die Infizierten sehr schnell sterben und damit nur eine geringe Zeitspanne verbleibt, in der die Lungenpest weitergegeben werden kann. Die Inkubationszeit wird mit 1 bis 3 Tagen angegeben, die Sterblichkeitsrate liegt bei 95 %, und der ansteckungsgefährliche Bluthusten tritt erst im fortgeschrittenen Stadium der Krankheit auf.

Gleichwohl sind im 20. Jahrhundert Lungenpestepidemien dokumentiert, die von pestinfizierten Reisenden ausgelöst wurden. Die beiden größten Lungenpestepidemien traten Anfang des 20. Jahrhunderts in der chinesischen Grenzregion Mandschurei auf. Das Auftreten war vor allem an ein kaltes Klima geknüpft. Die Epidemie in der Mandschurei 1910–1911 fand im Winter (September – April) statt und war an die Hauptverkehrswege geknüpft. Die Pest wurde über 2.700 km innerhalb von 7 Monaten transportiert. Es starben um die 60.000 Menschen.

Wu Lien-Teh beobachtete, dass die Lungenpest in der Mandschurei an die Jagd auf die Tabarganer oder auch Sibirischen Murmeltiere ("Marmota sibirica") gekoppelt und auf den wertvollen Pelz zurückzuführen war. Der Preis für die Felle war vor 1910 um das Vierfache gestiegen. Heutige Erfahrungen haben gezeigt, dass die Lungenpest regelmäßig mit der Erkrankung von Nagetierpopulationen auftritt. Der Zusammenhang zwischen der Lungenpest und einer vorangegangenen Nagererkrankung mit epidemischer Beulenpest ist gut dokumentiert.

Wenn die Erreger bei einer Beulenpest über die Blutbahn im Verlaufe einer Pestsepsis in die Lunge geraten, spricht man von sekundärer Lungenpest. Wird sie aber durch eine Tröpfcheninfektion von Mensch zu Mensch übertragen, spricht man von primärer Lungenpest.

Die Lungenpest verläuft heftiger als die Beulenpest, weil die Abwehrbarrieren der Lymphknoten durch direkte Infektion der Lunge umgangen werden. Sie beginnt mit Atemnot, Husten, Blaufärbung der Lippen und schwarz-blutigem Auswurf, der extrem schmerzhaft abgehustet wird. Daraus entwickelt sich ein Lungenödem mit Kreislaufversagen, welches unbehandelt nach zwei bis fünf Tagen zum Tod führt.

Die abortive Pest ist die harmlose Variante der Pest. Sie äußert sich meist nur in leichtem Fieber und leichter Schwellung der Lymphknoten. Nach überstandener Infektion werden Antikörper gebildet, die eine langanhaltende Immunität gegen alle Formen der Krankheit gewährleisten.

Die Diagnose erfolgt über den Nachweis der Erreger im Blut, im Sekret der Beulen oder bei der Lungenpest im Auswurf. Das französisch-madagassische Forschungsteam um Suzanne Chanteau vom Institut Pasteur de Madagascar (IPM) hat sowohl für die Lungen- als auch die Beulenpest 2003 einen Schnelltest entwickelt, mit dem sich Antikörper schon innerhalb von 15 Minuten nachweisen lassen. Davor ließen beide Erkrankungen sich erst nach einer 14-tägigen Auswertungsdauer nachweisen.

Bei den immerhin noch jährlich 4.000 weltweit auftretenden Pestfällen ist eine rasche Diagnose innerhalb von 24 Stunden entscheidender Bestandteil einer erfolgreichen Behandlung. In 20 Ländern, vor allem in Afrika, tritt die Pest heute vermehrt auf.

Die zunächst vieldeutigen und oft nur schwachen Symptome erforderten bislang in der Regel bakteriologische Untersuchungen, manchmal sogar über die DNA für eine eindeutige Zuordnung. Dabei sind Verwechselungen mit Blinddarmentzündung, Hirnhautentzündung und Streptokokkeninfektionen in den USA dokumentiert.

Der mikrobielle Nachweis wird aus Sputum, Blut oder Bubonenaspirat (Eiter) erhoben.

Die Pestausbreitung in den Epidemien von 1910 und 1921 ist auch auf die Entwicklung der Transportmittel zurückzuführen. 1921 traten die Pestfälle vor allem an den Eisenbahnstationen von Harbin bis Wladiwostok auf. Harbin war der Knotenpunkt zwischen der transsibirischen und der südmandschurischen Eisenbahn und besonders betroffen. Aber auch die Reise zu Pferd verbreitete die Pest über weite Strecken, wie die Pestausbrüche in den Jahren 1878–1925 in Astrachan und dem südlichen Ural beweisen, wo es keine Eisenbahnverbindungen gab. Es starben über 5.000 Menschen, davon 70 % an Lungenpest. Schuld am Ausbruch waren dort die unhygienischen Wohnverhältnisse: Dunkel, schmutzig und überbelegt. 10–15 Menschen wohnten auf ca. 10 m². Die Menschen wuschen sich selten oder nie und wechselten auch die Kleider nicht. Die Pestkranken wurden von vielen Menschen besucht, und die Gäste wischten den Auswurf mit Händen oder Kleidern ab. Dies galt auch für die Pestepidemie von 1910, wo die Tarbagan-Jäger sich als erste bei der Jagd nach Murmeltieren zur Gewinnung der Murmelfelle an den verseuchten Tieren ansteckten. Sie schliefen in besonders kleinen Hütten, bis zu 40 Mann in Kojen, was die Weiterverbreitung begünstigte. Ein weiteres Indiz waren die Verhältnisse an den Bitumen-Gruben am See Dalai Nur. Während der Pestepidemie von 1921 arbeiteten dort 4.000 Chinesen und 2.000 Russen. Von den insgesamt 1.027 Toten waren nur 4 Russen. Die Chinesen lebten zusammengepfercht in kleinen Hütten, halb in die Erde eingegraben, die Russen lebten in oberirdischen Häusern. Die Übertragung der Lungenpest per Tröpfcheninfektion kam also am Anfang des 20. Jahrhunderts durchaus vor.

Der endemische Verlauf der Pest folgt einem für diese Seuche typischen Muster, das so bei keiner anderen Seuche festzustellen ist: Der Tod setzt bei Ratten nach Befall einer Kolonie mit der Zeit immer schneller ein. Während anfangs mit ca. 7 Flöhen pro Ratte diese einen normalen Krankheitsverlauf zeigen, wird der Befall mit der Dezimierung der Kolonie bei den verbleibenden Ratten immer stärker, so dass 50 bis 100 Flöhe pro Ratte vorkommen, was zu einer wesentlich höheren Verseuchung führt. Nach 10 bis 14 Tagen ist die Rattenkolonie so stark reduziert, dass die Flöhe kaum noch Wirte finden. Diese Dauer von 10 bis 14 Tagen ist die erste wichtige Phase der Verbreitung. Danach nehmen die Flöhe ungefähr 3 Tage lang kein Blut auf, bis ihr Drang so groß ist, dass sie, da sie keine Ratten finden, nunmehr den Menschen anfallen. Es folgt die Inkubationsperiode von 3 bis 5 Tagen. Dieser folgt die Krankheitsperiode von 3 bis 5 Tagen, die bei der Mehrzahl der Befallenen zum Tode führt. Von der Ansteckung bis zum Tode vergehen durchschnittlich 8 Tage. Von der Erstinfizierung einer Rattenkolonie bis zum ersten Todesfall vergehen also 20 bis 28 Tage, gewöhnlich sind es 24 Tage.

Der Kontakt zwischen verseuchten und frischen Rattenkolonien führt zu einer langsamen Ausbreitung. Wichtiger ist der Ausbreitungsprozess über die Besuchspersonen. Sie nehmen die verseuchten Flöhe nach Hause mit und stecken so die eigene Rattenkolonie an. Das bedeutet, dass diese Form der Ausbreitung sich erst auswirkt, wenn die Pest bei einem Menschen sichtbar ausgebrochen ist, so dass im Spätmittelalter diese Form der Ausbreitung mit Krankenwache, Totenwache, Begräbnisfeier und Erbteilung einsetzte. Dieser Zeitpunkt ist etwa 3 bis 4 Wochen nach dem Einschleppen der Pest an einen Ort erreicht. Eine Woche später hat sich die Pest auf die Heimathöfe der Besucher verteilt, und die epidemische Phase beginnt. Bis dahin sind also ungefähr 40 Tage oder 5 1/2 Wochen vergangen.

Ein weiteres typisches Kennzeichen der Pestepidemie ist der Zusammenbruch im Winter. Es ist keine Epidemie der Beulenpest in einem Winter bekannt. Das hängt damit zusammen, dass bei Kälte die septische Bakteriendichte in den Ratten geringer ist, so dass die Flöhe weniger Bakterien aufnehmen, und damit, dass sich die Flöhe bei Kälte nicht vermehren. Das Ende von Pestepidemien, die durch Flöhe verbreitet werden, fällt regelmäßig auf die Wintermonate. Wurde die Pest erst im Spätherbst eingeschleppt, brach sie erst im nächsten Frühjahr aus.

Die Pestbakterien kommen auch heute noch in wild lebenden Nagetierpopulationen vor – wie beispielsweise bei den Präriehunden, Erdhörnchen und Murmeltieren. Diese Populationen sind die natürlichen Reservoire des Pestbakteriums, von denen aus gelegentlich häusliche Nager wie beispielsweise Ratten infiziert werden.

Während in Europa und Australien keine infizierten Tierpopulationen bekannt sind, kommen solche im Kaukasus, in Russland, in Südostasien, der Volksrepublik China, der Mongolei, Süd- und Ostafrika, Mittel- und Südamerika sowie im Südwesten der USA vor.

Nach Nordamerika gelangte der Erreger dabei über ein Handelsschiff während einer Pestepidemie, die ab 1894 in Südostasien grassierte. Obwohl nur sehr wenige Menschen in Nordamerika an der Pest erkrankten, infizierte der Erreger die amerikanische Eichhörnchenpopulation. Gelegentlich kommt es daher auch heute noch in Nordamerika zu Übertragungen von Tier zu Mensch. Meist sind es Jäger, die sich bei einem Nagetier anstecken. Norman F. Cantor verweist jedoch auch auf einen nordamerikanischen Fall aus den 1980er Jahren, bei dem eine Frau ein Grauhörnchen mit einem Rasenmäher überfuhr und sich dabei mit der Pest infizierte.

Weltweit registriert die Weltgesundheitsorganisation (WHO) etwa eintausend bis dreitausend Pestfälle pro Jahr, meistens in Form kleinerer, örtlich begrenzter Epidemien. In Europa gab es den letzten dokumentierten Pestausbruch im Zweiten Weltkrieg. Man nimmt an, dass die Pest gegenwärtig in Europa nicht mehr existiert.

Behandelt wird die Pest heutzutage mit Antibiotika, und bei frühzeitiger Erkennung bestehen gute Chancen auf Heilung. Eingesetzte Wirkstoffe sind beispielsweise Streptomycin und Chloramphenicol sowie Kombinationen aus Tetracyclinen und Sulfonamiden. Chloramphenicol ist zwar hochwirksam, gilt aber wegen seiner Nebenwirkungen nur als Reservemedikament. Anzumerken bleibt, dass dies jedoch nur bei rechtzeitiger Diagnose der Fall ist. Die Letalität steigt exponentiell zum Fortschreiten der Erkrankung.

In Entwicklung sind derzeit Hemmstoffe, die ein Enzym des Bakteriums blockieren sollen, das eine wichtige Rolle beim Aufbau der Zellschutzmembran spielt, ohne die das Bakterium nicht überleben kann.

Es stehen Schutzimpfungen zur Verfügung, die aber eine Immunität lediglich für drei bis sechs Monate gewähren, und dies auch nur bei der Beulenpest, nicht aber bei der Lungenpest. Die Autoren Eberhard-Metzger und Ries weisen jedoch auf die schlechte Verträglichkeit dieser Schutzimpfungen hin. Die Weltgesundheitsorganisation empfiehlt die Impfung daher nur Risikogruppen, zu denen beispielsweise Bauern, Landarbeiter und Jäger in Regionen zählen, in denen infizierte Nagetierpopulationen verbreitet sind.

Weitere Maßnahmen, um eine Pestepidemie einzudämmen, sind verbesserte Hygiene, Bekämpfung der Ratten und die Verhinderung des Transports von Ratten auf Schiffen. Da nach dem Tod der Ratten die Flöhe ihren Wirt wechseln, müssen die Menschen mit Insektiziden vor den Flöhen geschützt werden.

Die Pest gehört neben den hämorrhagischen Fiebern (Ebola, Lassa und anderen) in Deutschland zu den zwei Quarantäne-Krankheiten nach § 30 Infektionsschutzgesetz. Derart erkrankte Patienten müssen in speziellen Infektionsabteilungen abgeschirmt werden. Länderübergreifende Quarantäneregelungen für Schiff-, Luft-, Zug- oder Kraftfahrzeugverkehr sind in den Internationalen Gesundheitsvorschriften von 1971 festgehalten. Ein Hinweis auf die Pest, die Erkrankung an oder der Tod durch Pest müssen in Deutschland nach dem Infektionsschutzgesetz auch bei Verdacht namentlich gemeldet werden. Die Meldungen werden von den Gesundheitsämtern an die Landesgesundheitsbehörde und das Robert-Koch-Institut weitergeleitet. Das Robert-Koch-Institut meldet sie gemäß internationalen Vereinbarungen an die Weltgesundheitsorganisation.

Sehr lange war umstritten, ob bereits die spätantike Justinianische Pest, die ab 541 Europa und Vorderasien schwer traf und um 770 wieder verschwand, durch einen Erreger vom Stamm "Yersinia pestis" verursacht wurde. Schließlich zeigte Anfang 2013 eine an verschiedenen Laboratorien parallel durchgeführte internationale Studie unter der Leitung von Michaela Harbeck und Holger C. Scholz anhand von DNA-Material aus Gräbern aus Aschheim, die eindeutig in das spätere 6. Jahrhundert datiert werden können, dass es sich bei dieser ersten belegbaren Pest im engeren Sinne tatsächlich um den Erreger "Yersinia pestis" gehandelt hat.

Zudem gelang eine phylogenetische Einordnung des betreffenden Erregers zwischen den frühen Stammbaum-Abzweigungen N03 und N05. Mithin kann es nach aktuellem Forschungsstand als nahezu gesichert gelten, dass ein Erreger vom Stamm "Yersinia pestis" an der Justinianischen Pest zumindest prominent beteiligt war und es sich bei der Seuche somit tatsächlich um die Pest gehandelt hat. Als erster Ausbruch der Krankheit hatte bis 2013 vielen Forschern der Schwarze Tod von 1347 bis 1351 gegolten. Wieso die Pest um 770 für mehrere Jahrhunderte wieder aus Europa verschwunden zu sein scheint, ist bislang ungeklärt.

Mit der Pestpandemie von 1890 in Indochina begann die moderne Beschreibung der Krankheit. Alexandre Yersin hatte den nach ihm benannten Bazillus "Yersinia pestis" 1894 entdeckt, isoliert und der Pest zugeordnet. Gleichzeitig wurde in Indien von dem Franzosen Paul-Louis Simond die Ausbreitung von der Schwarzen Ratte ("Rattus rattus") über den orientalischen Rattenfloh auf den Menschen entdeckt.

Das führte zu einer Beschreibung der Pest als einer einheitlichen Krankheit. Die Entdeckung der Ausbreitung der Pest in Indien hatte eine beherrschende Bedeutung in der Anschauung der Pest in ihrer heutigen Bedeutung als moderne Krankheit. Sie führte zunächst zu der Auffassung, dass es nur diese eine Art der Ausbreitung der Krankheit gebe. Inzwischen haben sich die Forschungen auf eine große Zahl von Nagern und eine große Zahl von Floharten ausgeweitet. Die hohe Sterblichkeit in den Kolonien führte zu erhöhten Forschungsanstrengungen mit einer Kartografie der epidemischen Züge. Dass es sich immer um die Pest handele, war nicht hinterfragter Ausgangspunkt. So wurde die Krankheit mit dem historischen Begriff "Pest" belegt und auch die Bakterien danach benannt. Die Identität der mittelalterlichen Pest mit der in Indien erforschten Seuche wurde vorausgesetzt. Bei der Erforschung der Pest und ihrer Ausbreitung waren die Vorgaben der englischen Pestforschungskommission maßgeblich, die 1905 nach Indien entsandt worden war.

Viele Forschergruppen reisten nach Indien, darunter auch eine deutsche mit Wissenschaftlern aus der Umgebung Robert Kochs. Diese stellten 1897 fest: „Aus vielen Orten ist berichtet, dass dem Ausbruch der Pest eine seuchenartige Krankheit und massenhaftes Sterben der Ratten voranging.“ Eine vom indischen Vizekönig eingesetzte englische Kommission verkündete 1910 definitiv, dass die Pestepidemie in Indien direkt von der Pest in der Rattenpopulation abhängig sei. Für andere Tiere als Wirtstier wurden keine Belege gefunden. Dabei unterschied die Kommission zwischen Beulenpest und anderen klinischen Formen. Alle Beobachtungen deuteten darauf hin, dass die Pestepidemien ausschließlich in Form der Beulenpest auftraten.

Die Ansteckung der Ratten untereinander geschah nachweislich durch die Flöhe (Zum Nachweis wurden gesunde und kranke Ratten getrennt gehalten, wobei die Trennung für die Flöhe durchlässig war). Hinsichtlich der Pest bei den Menschen zog die Kommission eine Reihe von Schlüssen: 1. Die Pest wird nicht von Mensch zu Mensch übertragen, denn die Pfleger in den Krankenhäusern steckten sich nicht an. 2. Die Epidemie war nach ihrer Meinung fest mit der Epidemie unter den Ratten verknüpft. 3. Die in Indien vorherrschende Flohart "Pulex cheopis", heute "Xenopsylla cheopis", hatte sich erwiesen als eine, die auch Menschen anfällt, insbesondere, wenn ihre natürlichen Wirtstiere fehlten. Wiederholte Versuche mit Meerschweinchen und Affen in pestverseuchten Häusern zeigten, dass sie erkrankten, wenn sie nicht gegen Flöhe geschützt wurden. Weder pestverseuchter Boden noch die Kleider oder das Bettzeug von Pestkranken waren im Stande, ohne Flöhe mit Pest anzustecken. Da die Kommission experimentell feststellte, dass die Pestbakterien nur wenige Tage außerhalb eines Wirtstiers überleben konnten, kam sie zu dem Schluss, dass die Pest in den Landstädten von außerhalb hereingetragen worden sein musste. Da in den Großstädten die Pest auch außerhalb der pestgefährlichen Monate auftrat, meinte sie, dass die Pest dort in kleinen Rattenpopulationen oder einzelnen Menschen als Reservoir zwischen den Pestsaisonen erhalten blieb. Bei einem Untersuchungsgebiet in der Größe Indiens stellte sich die Frage nach den Ausbreitungswegen. Da die Ratten kaum große Distanzen zurücklegen konnten, meinte die Kommission, dass die Verbreitung in bislang pestfreie Zonen über den Warenverkehr stattgefunden haben müsse. Diese Untersuchungen und Schlussfolgerungen bezogen sich ausschließlich auf die in Indien damals aufgetretene Beulenpest.

2011 wurde das Genom des "Yersinia-pestis"-Stammes beschrieben, der von 1348 bis 1350 während der Zeit des „Schwarzen Todes“ Menschen in England infiziert hatte.

Mit den Ergebnissen kann die Evolution von Krankheitserregern besser nachvollzogen werden. Laut Studie veränderten sich die Pesterreger seit der Epidemie zwischen 1348 und 1353 kaum. Vermutungen, der Erreger sei in Ostasien im 13. oder 14. Jahrhundert entstanden, was bedeutete, dass frühere Pestepidemien wie die Justinianische Pest, die im 6. Jahrhundert weltweit zum Tod von mehr als 100 Millionen Menschen führte, von einem anderen, bisher nicht identifizierten Erreger verursacht worden wären, stellten sich Anfang 2013 als falsch heraus: Auch die Infektionen aus dem 6. Jahrhundert sind auf den Erreger Yersinia pestis zurückzuführen. Das Erbmaterial der jahrhundertealten Pesterreger gewannen die Forscher aus den Skeletten von Pestopfern, die im Mittelalter auf dem East-Smithfield-Friedhof in London begraben wurden. Dieser Friedhof gilt als der am besten dokumentierte Pestfriedhof in ganz Europa; er wurde nur drei Jahre lang – von 1348 bis 1350 – benutzt.

Die Pest ist auch heute noch nicht besiegt: Von 1978 bis 1992 meldete die Weltgesundheitsorganisation (WHO) 1451 Todesfälle in 21 Ländern. In den USA gab es beispielsweise 1992 dreizehn Infektionen und zwei Todesfälle.

Eine größere Pestepidemie ereignete sich von August bis Oktober 1994 im indischen Surat. Die WHO zählte 6344 vermutete und 234 erwiesene Pestfälle mit 56 Toten. Der dort festgestellte Pesterreger wies dabei bislang noch nicht beobachtete Eigenschaften auf. Er zeichnete sich durch eine schwache Virulenz aus und gilt aufgrund einiger molekularbiologischer Besonderheiten als neuartiger Erregerstamm.

Im Jahr 2003 kam es in Algerien nach 50 Jahren wieder zu einem Pestausbruch.

Im Februar 2005 breitete sich die Lungenpest in Bas-Uele im Norden der Demokratischen Republik Kongo aus. Nach Berichten der WHO gab es 64 Tote. Durch das Eingreifen der Organisation Ärzte ohne Grenzen konnte eine weitere Verbreitung verhindert werden. Am 14. Juni 2006 wurden im Kongo 100 Pesttote gemeldet, wobei die am stärksten betroffene Region der Distrikt Ituri im Nordosten war mit bis zu 1000 Fällen pro Jahr, sowohl Lungenpest als auch Beulenpest.

Im November 2008 wurde ein erneuter Ausbruch der Erkrankung in Uganda von den lokalen Zeitungen gemeldet. Betroffen waren insgesamt zwölf Menschen, von denen drei starben.

In den südwestlichen US-amerikanischen Bundesstaaten treten immer wieder Pestfälle auf. Das silvatische Erregerreservoir bilden hier Präriehunde. Werden erkrankte Präriehunde von Hauskatzen erbeutet, so erkranken diese in 10 % der Fälle an Lungenpest und scheiden große Mengen des Erregers aus. Sie sind dann eine Infektionsquelle für den Tierbesitzer und andere Kontaktpersonen. Insgesamt erkranken in den USA jährlich zehn bis zwanzig Menschen an der Pest, wobei die Zahlen rückläufig sind. Dies wird vom Osloer Biologen Nils Christian Stenseht auf den Klimawandel zurückgeführt.

Anfang August 2009 wurde in Ziketan in der tibetisch geprägten Provinz Qinghai im Nordwesten Chinas bei elf Menschen die Infektion mit Lungenpest festgestellt, ein Mensch starb.

Anfang 2008 brach auch in Madagaskar die Pest aus, 18 Menschen fanden dabei den Tod. 2010 starben 18 Menschen. Von Jahresbeginn bis März 2011 waren 60 Menschen gestorben und 200 weitere erkrankt. Betroffen sind vor allem abgeschiedene Regionen wie der Bereich um das Städtchen Ambilobe im Nordwesten, weitere Fälle gab es im Osten und im Hochland.

Ende 2013 starben im abgelegenen Norden der Tropeninsel Madagaskar im Distrikt Mandritsara 20 Menschen an der Lungenpest. Seit September 2013 sind in vier verschiedenen Distrikten auf Madagaskar 36 Menschen der Infektionskrankheit zum Opfer gefallen.

Im Jahr 2014 starben in Madagaskar, in einem Mitte November noch grassierenden Pestausbruch, erneut mindestens 40 Menschen. Ebenfalls 2014 wurde eine chinesische Kleinstadt unter Quarantäne gestellt, nachdem ein Mann an der Pest gestorben war.

Ende Oktober 2017 wurde gemeldet, dass die Zahl der Toten durch den jüngsten Pestausbruch auf Madagaskar auf 107 gestiegen ist. Mehr als 1100 Menschen haben sich mit der Krankheit infiziert, knapp 700 davon konnten bislang geheilt werden. Seit 2010 sind auf Madagaskar rund 600 Menschen an Pest gestorben.

Seit der Pestepidemie von 1348 entstanden "Pestbilder", die den göttlichen Zorn, meist in Form von Pfeilen oder Lanzen malerisch dargestellt, als Erklärung für die Erkrankungen versinnbildlichen sollten. Oft ist auf diesen Bildern eine vor diesem Gotteszorn schützende Schutzmantelmadonna abgebildet oder es werden die Schutzheiligen Sebastian und/oder Rochus gezeigt.

In der Stadt Flörsheim am Main wird seit 1666 bis in die Gegenwart am letzten Montag im August der sogenannte „Verlobte Tag“ zum Dank für die Verschonung der Bevölkerung von der Pest als örtlicher Feiertag begangen.

Arnold Böcklin schuf zu diesem Thema 1898 in Italien das Bild "Die Pest/Der Schwarze Tod", das heute im Kunstmuseum Basel ausgestellt ist. Böcklin personifiziert die Pest in seinem Bild als einen auf einem fliegenden Ungeheuer reitenden Sensenmann, vor dem es kein Entrinnen gibt. Die Sense und die skelettartige Gestalt greifen auf die mittelalterliche Todesallegorie zurück.

Nach einem Drehbuch von Fritz Lang entstand 1919 als erster Film der Monumentalfilmreihe "Decla-Weltklasse" der Stummfilm "Die Pest in Florenz", in dem die Pest das Florenz der Renaissance heimsucht. In der letzten Sequenz des Filmes zieht eine Personifikation der Pest tanzend und Geige spielend als eine Form des Totentanzes durch die Stadt. Die Darstellung der Pest dabei zeigt sehr deutliche Bezüge zu Arnold Böcklins "Pest". 1921/1922 drehte Friedrich Wilhelm Murnau den Stummfilm "Nosferatu – Eine Symphonie des Grauens", in dem ein Vampir symbolisch mit der Pest gleichgesetzt wird und deren bildhaft-körperliche Personifizierung darstellt. Noch deutlicher ist diese Metaphorik in Werner Herzogs Tonfilm-Adaption "Nosferatu – Phantom der Nacht" mit Klaus Kinski in der Titelrolle herausgearbeitet. Veit Harlan schildert in seinem 1938 gedrehten Seuchendrama "Verwehte Spuren", einer Adaption des gleichnamigen Hörspiels von Hans Rothe, einen angeblich authentischen Pestfall während der ersten Pariser Weltausstellung im Jahr 1867. Ingmar Bergman drehte 1957 "Das siebente Siegel" ("Det sjunde inseglet") mit Max von Sydow, der Film spielt im 14. Jahrhundert.

Albert Camus schrieb den Roman "Die Pest" (fr. "La Peste") über einen neuzeitlichen Pestausbruch in der algerischen Stadt Oran (publiziert 1947). Darin trifft ein Arzt trotz der Aussichtslosigkeit und Absurdität des Kampfes gegen die Pest auf Menschlichkeit und Solidarität. Die Pest wird hierbei oft als Symbol für den Nationalsozialismus interpretiert. Vier Jahre zuvor veröffentlichte Raoul Maria de Àngelis den Roman "La peste a Urana" (Die Pest in Urana).

1973 entstand das 1975 in Spoleto aufgeführte Theaterstück "Il sonno dei carnefici" (Der Traum der Totengräber) des Biologen und Schriftstellers Giorgio Celli, das sich mit der Pest in Sevilla befasst.

Auch Marcel Pagnol schrieb eine Erzählung über die Pest. Sie hat die Verwüstung von Marseille 1720 zum Thema. "Les Pestiférés" erschien postum 1977 in Band IV der "Souvenirs d’Enfance", "Le Temps des Amours". Jens Peter Jacobsen veröffentlichte 1881 die Novelle "Pesten i Bergamo" ("Die Pest in Bergamo").

Die Oberammergauer Passionsspiele finden als Einlösung eines Versprechens nach der überstandenen Pest 1634 statt. Seit 1680 finden sie in 10-jährigem Rhythmus statt und zählen zu den weltweit bekanntesten – wenn nicht den bekanntesten – Passionsspielen.

Der Pesterreger wird von der Weltgesundheitsorganisation zu den zwölf gefährlichsten biologischen Kampfstoffen gezählt. Zu diesem sogenannten "dreckigen Dutzend" gehören neben dem Erreger der Pest auch die des Milzbrands und der Tularämie sowie Pocken-, Ebola- und Marburg-Viren.

Es gibt die populäre Hypothese, dass die Pest als biologische Waffe bereits im 14. Jahrhundert zum Einsatz kam - als 1346 in der genuesischen Hafenstadt Kaffa auf der Krim der Tatarenführer Dschanibek Pestleichen über die Mauern der Stadt werfen ließ und die Belagerten vor der Pest nach Italien flüchteten. Nach einem Bericht von Gabriel des Mussis aus Piacenza sollen bei der Belagerung Kaffas beteiligt gewesene Genuesen und Venezianer die Seuche mit Galeeren nach Messina, Pisa, Genua und Venedig eingeschleppt haben, von wo aus sie sich dann über ganz Italien verbreitete. Dies wird jedoch kontrovers beurteilt und ist nicht eindeutig belegt.

Während des zweiten Chinesisch-Japanischen Krieges stellte die japanische Armee im "Einheit 731" genannten Gefangenenlager bei Harbin in der Mandschurei biologische Waffen her, die aus mit dem Pesterreger infizierten Flöhen bestanden und deren Einsatz in der Republik China in den Jahren 1940 bis 1942 lokale Pestausbrüche verursachte. Bei der Zerstörung der Produktionsstätten durch die japanische Armee 1945 bei Kriegsende kamen mit Pest infizierte Ratten frei und lösten in den chinesischen Provinzen Heilongjiang und Jilin eine Epidemie mit über 20.000 Todesopfern aus.

Zur Zeit des Kalten Krieges beschäftigten sich sowjetische Wissenschaftler (Im Institut Biopreparat unter Leitung von Ken Alibek) des Direktorium-15 mit dem Einsatz von Pesterregern als biologischer Waffe. Wie der ehemalige Forscher für biologische Waffen Alibek berichtete, gelang es der Sowjetunion Ende der 1980er Jahre, die Pest in eine sprühbare Form zu bringen und gegen Antibiotika resistent zu machen.

In Deutschland beschäftigt sich das Robert-Koch-Institut mit den Gefahren durch biologische Kampfführung. Dort wurde auch die "Informationsstelle des Bundes für biologische Sicherheit" (IBBS) eingerichtet. Wie groß die Gefahr eines Angriffs mit biologischen Kampfstoffen tatsächlich ist, ist umstritten. Die IBBS rät nicht zu einer Impfung gegen die Pest in Deutschland. Diese Empfehlung gilt sowohl für die Bevölkerung insgesamt als auch für Risikogruppen.

Am 28. August 2014 berichtete „Foreign Policy“ von einem Computer, der in einem ISIS-Versteck gefunden wurde und unter anderem Anleitungen zur Erstellung von Beulenpest-Waffen enthalten soll.





</doc>
<doc id="3987" url="https://de.wikipedia.org/wiki?curid=3987" title="Peer-to-Peer">
Peer-to-Peer

Peer-to-Peer (P2P) Connection (von „Gleichgestellter“, „Ebenbürtiger“) und Rechner-Rechner-Verbindung sind synonyme Bezeichnungen für eine Kommunikation unter Gleichen, hier bezogen auf ein Rechnernetz. In einigen Kontexten spricht man auch von Querkommunikation.

In einem reinen "Peer-to-Peer-Netz" sind alle Computer gleichberechtigt und können sowohl Dienste in Anspruch nehmen, als auch zur Verfügung stellen. In modernen P2P-Netzwerken werden die Netzwerkteilnehmer jedoch häufig abhängig von ihrer Qualifikation in verschiedene Gruppen eingeteilt, die spezifische Aufgaben übernehmen. Kernkomponente aller modernen "Peer-to-Peer-Architekturen", die meist bereits als Overlay-Netz auf dem Internet realisiert werden, ist daher ein zweites internes Overlay-Netz, welches normalerweise aus den besten Computern des Netzwerks besteht und die Organisation der anderen Computer sowie die Bereitstellung der Such-Funktion übernimmt.

Mit der Suchfunktion („lookup“) können Peers im Netzwerk diejenigen Peers identifizieren, die für eine bestimmte Objektkennung (Object-ID) zuständig sind. In diesem Fall ist die Verantwortlichkeit für jedes einzelne Objekt mindestens einem Peer fest zugeteilt, man spricht daher von "strukturierten Overlays". Mittels der Such-Operation können die Peers nach Objekten im Netzwerk suchen, die gewisse Kriterien erfüllen (z. B. Datei- oder Buddynamen-Übereinstimmung). In diesem Fall gibt es für die Objekte im P2P-System keine Zuordnungsstruktur, man spricht also von "unstrukturierten Overlays".

Sobald die Peers, die die gesuchten Objekte halten, in dem P2P-System identifiziert wurden, wird die Datei (in Dateitauschbörsen) direkt, d. h. von Peer zu Peer, übertragen. Es existieren unterschiedliche Verteilungsstrategien, welche Teile der Datei von welchem Peer heruntergeladen werden soll, z. B. BitTorrent.

Der Gegensatz zum Peer-to-Peer-Modell ist das Client-Server-Modell. Bei diesem bietet ein Server einen Dienst an und ein Client nutzt diesen Dienst. In Peer-to-Peer-Netzen ist diese Rollenverteilung aufgehoben. Jeder Teilnehmer ist ein "peer", denn er kann einen Dienst gleichermaßen nutzen und selbst anbieten.

Typische, aber nicht notwendige Charakteristika von Peer-to-Peer-Systemen sind:

P2P-Systeme lassen sich in unstrukturierte und strukturierte P2P-Systeme unterteilen.

Unstrukturierte P2P-Systeme unterteilen sich nochmals nach der Art ihres Aufbaus. Man unterscheidet:
Zentralisierte und reine P2P-Systeme bezeichnet man als Systeme erster Generation, während dezentrale Systeme als Systeme zweiter Generation bezeichnet werden. Systeme, die Dateien über nicht-direkte Verbindungen weiterreichen, sind Systeme dritter Generation. Siehe dazu auch ausführlich den Begriff Filesharing.

Strukturierte P2P-Systeme verwenden oftmals eine Verteilte Hashtabelle (DHT). In strukturierten Systemen können daher Suchen aus einem verteilten Index heraus beantwortet werden.

Die Zukunft der Peer-to-Peer-Technik wird vor allem davon abhängen, ob es gelingt, einen Standard zu definieren – eine Art Plattform-Technik, die es ermöglicht, weitere Anwendungen aufzusetzen.

JXTA ist ein solcher Standard, der stark von Sun Microsystems unterstützt wurde und Open Source ist. Sun stellte die zurzeit umfangreichste und stabilste Referenzimplementierung her.

Gnutella ist ein weiterer offener Standard, der umfangreich getestet ist, jedoch bisher fast ausschließlich für Dateiverteilung und dezentrales Suchen von Dateien genutzt wird.

Außerdem ist es denkbar, dass die Netzwerkübertragungsleistung ähnlich der Rechenleistung bei den PCs steigen wird, sodass die Möglichkeit besteht, dass ein Peer noch den „übernächsten“ Peer kennen kann und dass die Sichtweite eines Peer über Datenbestände und andere Peers weiter anwachsen kann.





</doc>
<doc id="3988" url="https://de.wikipedia.org/wiki?curid=3988" title="P2P">
P2P

P2P steht als Abkürzung für:



</doc>
<doc id="3990" url="https://de.wikipedia.org/wiki?curid=3990" title="Potsdamer Platz">
Potsdamer Platz

Der Potsdamer Platz ist ein Verkehrsknoten in den Berliner Ortsteilen Mitte und Tiergarten im Bezirk Mitte zwischen der alten Innenstadt im Osten und dem neuen "Berliner Westen". Als Doppelplatzanlage schließt er sich westlich an den Leipziger Platz an. 

Im Zuge des Baus der Berliner Zoll- und Akzisemauer wurde 1734 das Potsdamer Tor errichtet. Dort begann seit dem späten 18. Jahrhundert die Berlin-Potsdamer Chaussee (später Potsdamer Straße), die ein wichtiges Glied im Netz der Preußischen Staatschausseen war. Die westlich des Tores liegende Grünanlage, der "Platz vor dem Potsdamer Thor", erhielt 1831 seinen heutigen Namen. 

Mit dem Potsdamer Fernbahnhof, dem U-Bahnhof sowie den zahlreichen Straßenbahn- und Omnibuslinien war der Potsdamer Platz bis zum Ende des Zweiten Weltkriegs einer der verkehrsreichsten Plätze Europas und erhielt daher bereits 1924 eine der ersten Verkehrsampel-Anlagen auf dem Kontinent. In den ersten vierzig Jahren des 20. Jahrhunderts waren der Platz und die von ihm ausgehenden Straßen ein beliebter Treffpunkt der politischen, sozialen und kulturellen Szene Berlins. 

Nach Kriegsende bildete der Potsdamer Platz ein „Dreiländereck“ zwischen dem sowjetischen, dem britischen sowie dem amerikanischen Sektor im geteilten Berlin. Ab August 1961 verlief die Berliner Mauer über den Platz, der die folgenden fast drei Jahrzehnte ein randständiges Dasein als innerstädtische Brache fristete. Nach der Maueröffnung am 9. November 1989 stellte sich eine neue Situation ein: Schon wenige Tage später wurde am Potsdamer Platz ein Stück der Mauer abgebrochen und ein provisorischer Grenzübergang geschaffen. Das an der größtenteils erhaltenen Straßenführung nach 1990 neu bebaute Terrain zählt zu den markantesten Orten der Stadt und ist ein touristischer Anziehungspunkt.

Das Potsdamer Tor übernahm die Funktion des 1738 abgerissenen Leipziger Tores im Südwesten von Alt-Berlin und wurde daher lange Zeit synonym auch als "Neues Leipziger Tor" bezeichnet. Es war eines von vierzehn Toren in der bis 1737 fertiggestellten Berliner Akzisemauer. Westlich des Tores trafen fünf Straßen und Wege strahlenförmig zusammen. Die wichtigste Verbindung war die zum Neuen Garten bei Potsdam, der Sommerresidenz von König Friedrich Wilhelm II. Von 1788 bis 1795 wurde die Berlin-Potsdamer Chaussee angelegt, eine der ersten Kunststraßen im Königreich Preußen. Nach Plänen des königlichen Baumeisters Karl Friedrich Schinkel wurde 1824 das baufällige alte Potsdamer Stadttor durch zwei Torhäuser im klassizistischen Stil ersetzt. Auch nach dem Abriss der Akzisemauer 1867 blieben die beiden „Schinkel“ stehen und prägten den Platz bis zum Ende des Zweiten Weltkriegs.
Der damals noch am Stadtrand gelegene Potsdamer Platz wurde 1838 mit dem Bau des ersten Potsdamer Bahnhofs für die Berlin-Potsdamer Eisenbahn und der Eröffnung der Verbindungsbahn im Jahr 1850 (1871 wieder eingestellt) Zug um Zug zu einem großstädtischen Umschlagplatz für Menschen und Waren, weiter verstärkt vom Vorortverkehr über die beiden 1891 gebauten Seitenbahnhöfe der Wannsee- bzw. Ringbahn. In der Belle Époque nach der Reichsgründung 1871 eröffneten rund um den Platz neue Hotels und gastronomische Betriebe, darunter an der Einmündung der Königgrätzer Straße das "Hotel Fürstenhof" und an der Westseite des Potsdamer Platzes mit direktem Blick in die Leipziger Straße der Literaten- und Künstlertreffpunkt "Café Josty". Auf der Nordseite (Potsdamer Platz 1) eröffnete 1888 das "Grand Hotel Bellevue", auch "Thiergarten Hotel" bzw. "Hotel Du Parc" genannt, und östlich gegenüber am Leipziger Platz 18 hinter dem nördlichen Torhaus schließlich 1893 das "Palast Hotel".

Bis zum Ersten Weltkrieg etablierten sich weitere Betriebe rund um den Platz und in der näheren Umgebung. Zum Besuchermagnet wurde das von 1896 bis 1906 an der nordöstlichen Seite des Leipziger Platzes auf 27.000 m² Grundstücksfläche nach Plänen von Alfred Messel in drei Bauabschnitten errichtete Kaufhaus Wertheim, für das Heinrich Schweitzer 1911/1912 noch eine Erweiterung schuf. 

Eine weitere Verkehrsanbindung bildete ab 1902 der U-Bahnhof Potsdamer Platz, eine der ersten Stationen der Berliner U-Bahn, deren erste Linie von dort nach Süden zum U-Bahnhof Gleisdreieck führte.

Als zentrumsnaher Dreh- und Angelpunkt entwickelte sich die Gegend um den Potsdamer Platz zu einem großstädtischen Amüsierviertel. Während hier tagsüber Angestellte, Sekretärinnen und Geschäftsleute zu ihren Arbeitsstätten eilten oder Touristen flanierten, bestimmten nachts Amüsierwillige, Varietébesucher und Prostituierte das Bild. Obwohl die Ausübung der Prostitution im Berlin des Kaiserreichs strengstens untersagt war, entstand seit Beginn des 20. Jahrhunderts rund um den Platz ein Rotlichtmilieu. Diese Seite des Potsdamer Platzes ist durch das Gemälde des expressionistischen Künstlers Ernst Ludwig Kirchner, "Potsdamer Platz, 1914" bekannt geworden, das zwei mondän gekleidete Frauen sowie eine Reihe weiterer Personen vor einem Nachtleben-Hintergrund inszeniert. 

Unmittelbar hinter dem Gebäude des Cafés Josty an der Westseite (Potsdamer Platz 2) wurde von 1905 bis 1907 in dem Komplex Bellevuestraße 19/20 und Potsdamer Straße 3 (Nr. 8 ab 1937) das Weinhaus Rheingold gebaut, ein Großrestaurant des Aschinger-Konzerns mit Platz für bis zu 4000 Gäste.

Im Anschluss entstand direkt daneben an der Potsdamer Straße 4 (Nr. 10 ab 1937) in den Jahren 1907/1908 nach Plänen des Architekten Otto Stahn ein später "Vox-Haus" genannter Stahlskelettbau, der mit einer gemauerten Jugendstil­fassade verkleidet war. Er beherbergte ab 1921 die Vox-Schallplatten- und Sprechmaschinen-AG und im Dachgeschoss den Sender Funk-Stunde Berlin. Dort begann mit der ersten öffentlichen Hörfunksendung am 19. Oktober 1923 die Geschichte des Hörfunks in Deutschland. Nach dem Krieg zogen in das instandgesetzte Haus Kinos ("Camera", "Aladin") und Ladengeschäfte ein. Mit dem Bau der Mauer bleiben die Kunden aus. Am 22. März 1971 erfolgte die Sprengung des über 60 Jahre alten Hauses und das Grundstück wurde abgeräumt.

Im gleichen Zeitraum ließ eine Bauträgergesellschaft, zu deren Gesellschaftern Angehörige der Fürstenhäuser Hohenlohe, Fürstenberg und Henckel von Donnersmarck gehörten, auf der rückwärtigen Seite (Bellevuestraße 17/18) neben dem Weinhaus Rheingold nach Plänen des Architekten Otto Rehnig das Grand-Hotel Esplanade errichten. Es gehörte neben dem Hotel Kaiserhof am Wilhelmplatz und dem Hotel Adlon (Unter den Linden) zu den berühmtesten Hotels der Stadt. Der rund 23 Millionen Mark (inflationsbereinigt in aktueller Währung rund Millionen Euro) teure Bau hatte mehrere prachtvolle Säle, darunter den "Kaisersaal", wo Kaiser Wilhelm II. seine exklusiven Herrenabende veranstaltete. Als besondere Attraktion galt auch der 1600 m² große Garten im Innenhof. Der erhalten gebliebene Kaisersaal wurde im März 1996 in einer aufwendigen Aktion um 75 Meter verschoben und in das Sony Center integriert.

Die Gemeinschaft Conrad Heidenreich & Paul Michel entwarfen und bauten 1911/1912 das Weinhaus Huth an der Potsdamer Straße 139 (Nr. 5 ab 1937, seit 1997 Alte Potsdamer Straße 5), in der Nachkriegszeit nicht ganz zutreffend als „letztes Haus am Potsdamer Platz“ bezeichnet. Der mit einer Fassade aus Muschelkalkstein verblendete Stahlskelettbau stand nach dem Bau der Berliner Mauer allein auf freier Fläche auf West-Berliner Gebiet neben den Überresten des Hotels "Esplanade" und wurde zum Symbol für die Zerstörung und Teilung der Stadt.

Das in der Behrenstraße 24 ansässige "Bierhaus Siechen" ergänzte 1911 auf der südwestlichen Seite des Platzes vor dem Potsdamer Bahnhof den "Bierpalast" (Potsdamer Platz 2). Der von dem Architekten Johann Emil Schaudt entworfene Restaurationsbetrieb wurde später von der Pschorr-Brauerei übernommen und ist daher in der Berliner Baugeschichte eher als "Pschorr-Haus am Potsdamer Platz" bekannt. Die Ruine wurde 1952 abgerissen.

Auf der anderen (südöstlichen) Seite des Potsdamer Bahnhofs entstand 1911/1912 an der Einmündung der Köthener Straße in die Königgrätzer Straße (ab 1930: Stresemannstraße) in Sichtweite des Platzes nach Plänen des Architekten Franz Schwechten das sechsgeschossige "Haus Potsdam", eine Mischung aus Vergnügungspalast und Bürohaus mit einer markanten Kuppel zum Platz hin. Kurz nach Beginn des Ersten Weltkriegs wurde 1914 das darin betriebene "Café Piccadilly" in "Vaterland - Kaffeehaus Potsdamer Platz" umbenannt. Im Haus war ein großes Kino untergebracht, außerdem besaß dort die Ende 1917 gegründete UFA Büroräume. Nach dem Umbau zur Großgaststätte eröffnete es 1929 unter dem Namen "Haus Vaterland" neu und avancierte unter Leitung der Gastronomenfamilie Kempinski zum größten Amüsierpalast im Deutschen Reich. Bekannte Restaurants in dem Gebäude waren die "Rheinterrassen" (mit einer stündlichen Gewittersimulation), das "Wiener Café" und "Weinlokal Grinzing" sowie die bayerische "Bierstube Löwenbräu". Der langgestreckte Bau wurde bei den Luftangriffen auf Berlin mehrfach schwer getroffen. Nach dem Krieg konnten im nördlichen Teil des Gebäudes noch einige Räume mit einfacher Ausstattung als "HO-Gaststätte Haus Vaterland" wieder hergerichtet und weiter betrieben werden. Durch die Lage direkt an der Sektorengrenze geriet es jedoch in eine innerstädtische Randlage. Nach dem Mauerbau 1961 war der zunehmend verfallende Bau bis zu einem Gebietstausch im Jahr 1972 nicht mehr zugänglich. Aus Verkehrssicherungsgründen wurde die Ruine schließlich im Jahr 1976 abgetragen.

Vor dem Haus Potsdam befand sich nordöstlich des Bahnhofs bis 1922 der kleine Friedhof der Dreifaltigkeitsgemeinde und "Mutter Michaelis’ Blumenhalle".
In den „Goldenen Zwanziger Jahren“ prägte ab 1924 der "Verkehrsturm" mit der ersten Verkehrsampel Berlins das Straßenbild. Der Magistrat hatte aufgrund des steigenden Verkehrsaufkommens Siemens & Halske beauftragt - nach der 1922 auf dem Stephansplatz in Hamburg installierten Einrichtung - die zweite Verkehrsampel im Deutschen Reich aufzubauen. Die drei Lichter waren nicht, wie heute üblich vertikal, sondern horizontal angeordnet. Als Vorbild dienten die gerade in den USA aufgekommenen Ampeln. Bei den Bauarbeiten für den Nord-Süd-Tunnel wurde der technisch überholte Verkehrsturm in der Nacht vom 1. auf den 2. Oktober 1937 abgebaut.

Das Architekturbüro „Gebr. Luckhardt und Anker“ (Wassili und Hans Luckhardt mit Alfons Anker) baute 1928/1929 das neben dem Pschorr-Haus liegende "Telschow-Haus" (Potsdamer Straße 141; Nr. 1 ab 1937) im Stil der Neuen Sachlichkeit um. Das Gebäude der Conditorei Telschow erhielt eine elegant geschwungene Fassade aus mehrfarbigem Opalglas und wurde so zu einem der modernsten Gebäude Berlins. 

Das zehngeschossige Columbushaus, im Auftrag von Wertheim nach Entwürfen von Erich Mendelsohn auf dem Grundstück des 1928 abgerissenen Grand-Hotels Bellevue erbaut, vervollständigte ab 1932 die Reihe moderner, ehrgeiziger Gebäude am Platz.
Über den Platz führte ab 1932 die Fernverkehrsstraße Nr. 1 (ab 1934 Reichsstraße 1), die Aachen mit Ostpreußen verband. Heute ist es bis an die Oder die Bundesstraße 1.
Knapp vierzig Tage nach Beginn des Zweiten Weltkriegs wurde am 9. Oktober 1939 der S-Bahnhof Potsdamer Platz eröffnet.

Bis in den Zweiten Weltkrieg hinein war der Potsdamer Platz einer der belebtesten Plätze Europas. Nach den Luftangriffen der Alliierten in den 1943/1944 lag er jedoch zur Hälfte in Trümmern. Nach Kriegsende wurden die Ruinen rund um den Platz oberirdisch enttrümmert, die Fläche wurde dann einige Zeit als Bau- und Lagerplatz für die zu recycelnden Materialien benutzt. Bald diente das "Dreiländereck" zwischen dem sowjetischen, dem britischen sowie dem amerikanischen Sektor als blühender Schwarzmarkt. Mit der Einführung der Deutschen Mark in den westlichen Sektoren und dem Beginn der Berlin-Blockade im Juni 1948 änderte sich das Bild wieder und am 21. August des gleichen Jahres markierte die Verwaltung erstmals den Grenzverlauf zwischen dem sowjetischen und den angrenzenden Westsektoren mit einem Strich im Asphalt.

In Erwartung eines baldigen Wiederaufbaus wurde – wie auch in anderen Teilen der Stadt – am Potsdamer Platz der verbliebene Rest der Bebauung notdürftig wiederhergerichtet, freilich in einfacheren Formen ohne den Glanz der 1920er Jahre. Zum Beispiel zog in den ehemaligen Vergnügungspalast „Haus Vaterland“ wieder eine Gaststätte ein. In den unteren Stockwerken des ausgebrannten, zehngeschossigen Columbushauses eröffnete eines der ersten DDR-HO-Kaufhäuser. Beim Volksaufstand am 17. Juni 1953 brannten das genau an der Grenze des Sowjetsektors gelegene Geschäftshaus, das „Haus Vaterland“ und weitere Gebäude erneut aus. In den folgenden Jahren machte sich schrittweise in fast allen Gebäuden rund um den Potsdamer Platz Leerstand breit, da das gesamte Areal über Jahrzehnte hinweg für Investoren jeglichen Wert verloren hatte.

Als der Platz im August 1961 durch die Berliner Mauer geteilt wurde, verschärfte sich diese Entwicklung. Bis Mitte der 1970er Jahre wurden nahezu alle übriggebliebenen Gebäude abgerissen. Auf der Ostseite des Platzes war hierfür das Sicherheitsbedürfnis der DDR verantwortlich: An keiner anderen Stelle der Berliner Mauer waren die eigentliche Mauer und die "Hinterlandmauer" durch einen derart breiten Todesstreifen voneinander getrennt wie am Potsdamer Platz. Fast alle Gebäude, die innerhalb des Streifens lagen, mussten verschwinden, darunter die an der Ebert- und Stresemannstraße, ebenfalls die Reste des Kaufhauses Wertheim am Leipziger Platz. Erhalten blieb das ehemalige preußische Landwirtschaftsministerium, heute Sitz des Bundesumweltministeriums, an der Stresemannstraße. 

Auf der Westseite kaufte der Senat von Berlin nach und nach viele ungenutzte Ruinengrundstücke auf, um auch die letzten Reste der Gebäude entfernen zu lassen, da sie eine Gefahrenquelle darstellten. So wurden unter anderem die südöstlich vom Platz gelegenen Ruinen von Prinz-Albrecht-Palais, Völkerkundemuseum und Anhalter Bahnhof aufgelassen. Das größtenteils intakte Vox-Haus wurde 1971 aus Mangel an neuen Nutzern gesprengt und die noch recht beachtlichen Reste des „Hauses Vaterland“ 1976 abgetragen. Die Planungen jener Zeit sahen vor, das Gelände für den Bau einer Stadtautobahn zu nutzen, die dann erst nach der Wende in Gestalt des Tunnel Tiergarten Spreebogen gebaut wurde. Ein langsamer Meinungswechsel setzte im Westen im Jahr 1981 mit der Wiederherstellung des Martin-Gropius-Baus, des ehemaligen Kunstgewerbemuseums, ein.

Bis zur Öffnung der Mauer im Jahr 1989 fristete der Platz ein „randständiges“ Dasein als innerstädtische Brache, auf dessen westlichem Teil sich unter anderem ein Rollheimer-Dorf angesiedelt hatte. Außerdem ließen sich eine Handvoll Imbissbuden, Souvenirgeschäfte für Touristen nieder, zudem fanden sich Podeste, von denen aus man einen Blick in den östlichen Teil der Stadt erhalten konnte. In dem 1987 gedrehten Film "Der Himmel über Berlin" von Wim Wenders spielen einige Szenen auf dem Westteil des Potsdamer Platzes.

Durch seine zentrale Lage in der Nähe der nach dem Mauerbau geschaffenen Entlastungsstraße, die nun einen Teil der Verkehrsströme rund um den Großen Stern aufnahm, stellte der Potsdamer Platz einen der Punkte dar, an denen man im Westteil der Stadt die sonst vielfach verdrängte Mauer wahrnehmen konnte. Wesentliche Änderungen des Grenzverlaufs ergaben sich später durch Gebietsaustausch, teils verbunden mit Ausgleichszahlungen in DM, als 1972 das Areal des ehemaligen Potsdamer Bahnhofs und 1988 das Lennédreieck zu West-Berliner Gebiet wurde.
Nach der Öffnung der Berliner Mauer am 9. November 1989 stellte sich nach Jahrzehnten der Vernachlässigung quasi über Nacht eine neue Situation ein: Wenige Tage später wurde am Potsdamer Platz ein Stück der Mauer abgebrochen, ein aufgegrabenes Straßenstück asphaltiert und am 12. November 1989 ein provisorischer Grenzübergang geschaffen.

Im Jahr 1990, kurz nach der Öffnung der Mauer, führte Roger Waters das Konzert "The Wall" auf dem Niemandsland zwischen Potsdamer Platz und Pariser Platz auf. Es wurde das bislang größte Konzert in der Geschichte der Rockmusik. In Hinblick auf die Stadtentwicklung stellte sich spätestens seit der deutschen Wiedervereinigung die Frage, wie die traditionelle Klammer zwischen dem östlichen und dem westlichen Zentrum Berlins in Zukunft aussehen sollte. Die Veräußerung der Grundstücke an den Automobilkonzern "Daimler-Benz" (ab 1998 "DaimlerChrysler", seit 2007 "Daimler") seitens des Berliner Senats erfolgte sehr rasch. Senatsbaudirektor Hans Stimmann versuchte, eine an den traditionellen Traufhöhen und Blockstrukturen Berlins orientierte städtebauliche Anlage durchzusetzen, die im Stil der Postmodernen Architektur gehalten sein sollte. Sie sollte der von Stimmann so bezeichneten „Europäischen Stadt“ und dem Begriff einer „kritischen Rekonstruktion“ entsprechen. Dem entsprach das Projekt der Architekten Hilmer und Sattler mit seiner weitgehend einheitlichen Traufhöhe von 35 Metern. Die unzufriedenen Investoren lancierten allerdings ein Alternativprojekt des internationalen Stararchitekten Richard Rogers, und setzten sich mit ihrem stark verdichteten Konzept einer „(Hochhaus-)City für das 21. Jahrhundert“ durch. Die realisierte Lösung fand nicht nur Zustimmung. Kritisiert wurde zum Einen die Schnelle der Veräußerung, zum Zweiten die Aufgabe eigenen städtischen Planungswillens und zum Dritten die Tatsache, dass die Investoren die Straßen und Plätze nicht nur errichteten, sondern durch den Eigentumserwerb auch das Hausrecht in einem öffentlich zugänglichen Stadtgelände erwarben. Der Architekt Rem Koolhaas, als einer der Juroren des Potsdamer-Platz-Wettbewerbs, geißelte die Pläne als einem „dilettantischen Bild der Stadt“ entspringend und verließ 1991 die Jury. Befürworter hingegen wiesen auf die angespannte Berliner Finanzlage hin und argumentierten, dass die Neugestaltung der riesigen Brachfläche letztlich nur durch einen kühnen Wurf aus einem Guss möglich sei.

Zu den Vorbereitungen der künftigen Neubebauung gehörte auch die Sprengung des "Bellevue-Towers" an der Eichhornstraße im Oktober 1993. Das mit Waschbeton verkleidete 14-geschossige Hochhaus war 1971 als einer der wenigen Neubauten im Umfeld des Potsdamer Platzes errichtet worden. Ursprünglich als Hotel genutzt, wurde der Bellevue-Tower später zum Wohnheim für Studenten und Asylbewerber und trug zuletzt deutliche Zeichen von Verwahrlosung.

Während der 1990er Jahre wurde der Potsdamer Platz zur größten innerstädtischen Baustelle Europas. Von einem als "Infobox" bezeichneten Aussichts-Container auf dem gegenüberliegenden Leipziger Platz konnten man den Fortschritt der Bauarbeiten beobachten und Modelle der zukünftigen Bauten betrachten. Grob eingeteilt entstanden auf der an den ehemaligen Potsdamer Platz angrenzenden Brache vier unterschiedliche Komplexe. Das nordwestliche, zwischen der neu angelegten "Entlastungsstraße" und dem (neuen) Potsdamer Platz liegende, 27.000 m² umfassende Sony Center wurde von Helmut Jahn gestaltet. Untergebracht sind in dem dreieckigen Areal Cafés, das Filmmuseum Berlin mit der "Deutschen Kinemathek", Appartements, Büros sowie die europäische Zentrale von Sony. Richtung (neuer) Potsdamer Platz endet das Sony-Areal mit dem Sitz der Holding der Deutschen Bahn in Berlin, dem Bahntower.

Im Oktober 1996 wurde mit einem "Kranballett" das Richtfest für das 85 Meter hohe Gebäude der Daimler Benz Tochtergesellschaft "debis" gefeiert. Unter der Leitung von Daniel Barenboim „tanzten“ sieben Minuten lang 19 Krane synchron zu den Klängen des 4. Satzes der 9. Sinfonie von Ludwig van Beethoven (Ode „An die Freude“).
Südlich an das Daimler-Benz-Areal schließt sich das Quartier Potsdamer Platz, mit rund 70.000 m² der beherrschende Gebäudekomplex, an. Unter anderem steht hier das von dem italienischen Architekten Renzo Piano entworfene debis-Haus (heute: Atrium Tower) – erkennbar an dem markanten grünen Würfel auf seiner Spitze.

Direkt gegenüber dem Bahntower befindet sich ein weiteres markantes Hochhaus: der von Hans Kollhoff im New Yorker Backsteinstil entworfene, 103 Meter hohe Kollhoff-Tower. Laut Eigenaussage enthält er den schnellsten Aufzug Europas und ermöglicht von der oben liegenden Dachterrasse einen guten Überblick über den Potsdamer Platz und seine Umgebung.
Auf einem unteren Dach des Forum-Towers ließ der Hauseigentümer, die "SEB Asset Management", im Herbst 2012 ein designtes Taubenhaus aufstellen, dessen Herstellung 100.000 Euro gekostet hatte. Die Anlage besteht aus Alumnium in Form eines Kristalls und enthält rund 70 Nistplätze. Die Betreuung erfolgte durch den Tierschutzverein; es konnte die Taubenplage im Quartier Potsdamer Platz deutlich verringert werden. Im Dezember 2016 wurde das Taubenhaus abgebaut, weil an dieser Stelle eine Aussichtsterrasse eingerichtet werden soll; es wird voraussichtlich auf dem Parkhaus am Bahnhof Südkreuz wieder aufgestellt. Der Unterhalt dieser Anlage kostete rund 30.000 Euro pro Jahr.

Auch die restliche Architektur weist teilweise mediterrane Stilzitate auf. Mittelpunkt des Quartiers Potsdamer Platz ist der südwestlich gelegene Marlene-Dietrich-Platz. Um ihn herum angesiedelt sind das Theater am Potsdamer Platz (ehemals: "Musical-Theater Berlin"), das Bluemax (Theater der Blue Man Group, ehemaliges "IMAX-Kino"), ein Spielcasino, das Luxushotel Grand Hyatt Berlin sowie Varietébühnen und Restaurants. Durch die Lage zwischen zwei Gebäudereihen im südlichen Block und der Überdachung bilden die Potsdamer Platz Arkaden eine wettergeschützte Einkaufsstraße.

Den südlichen Abschluss der Hochhaus-Triade am Platz bildet ein nach Plänen von Renzo Piano errichtetes Gebäude, in dem PricewaterhouseCoopers seine Berlin-Niederlassung hat. Es hat, wie der benachbarte Kollhoff-Tower, einen dreieckigen Grundriss und ist im hinteren Bereich treppenartig gestaltet, sodass es zur nachfolgenden Bebauung überleitet. Der obere senkrechte Teil hat aber im Gegensatz zum Kollhoff-Tower eine verglaste Fassade.

Zwei kleinere Baueinheiten komplettieren die Neubebauung der ehemaligen Brache: im Norden das zwischen Sony Center und Tiergarten gelegene, unter anderem von Otto Beisheim errichtete Beisheim Center mit mehreren Hotels wie dem Marriott und dem Ritz-Carlton, und weiter im Süden die Park Kolonnaden – fünf vorwiegend mit Büroraum verplante Gebäude. Am östlichen Ende des Potsdamer Platzes steht das von Hans Kollhoff entworfene Delbrück-Hochhaus, auch bekannt als "P5" (Hausnummer des Platzes). Ebenso wie im Kollhoff-Tower finden sich im Delbrück-Hochhaus Rechtsanwalts-, Steuerberater-, Wirtschaftsprüfer- und Unternehmensberaterbüros. Unterirdische Tunnel ermöglichen eine Verbindung zwischen den Appartementhochhäusern und dem Bahntower.

Mit dem Tilla-Durieux-Park und dem Henriette-Herz-Park wurden auch zwei Parkanlagen realisiert. Der Tilla-Durieux-Park grenzt über eine schräge, sich der Länge nach um die eigene Achse drehende Wiesenfläche das Quartier Potsdamer Platz von den Park-Kolonnaden ab. Er befindet sich an der Stelle des ehemaligen Potsdamer Bahnhofs. Unter ihm verlaufen die vier Bahnröhren des Tunnels Nord-Süd-Fernbahn. In der Mitte der rechteckigen, 450 Meter langen Grundfläche ist die Rasenfläche unterbrochen. Fünf überdimensional lange Edelstahlwippen sind dort von den Architekten des Parks angeordnet worden. Sie erfüllten damit auf eine etwas andere Art und Weise die Forderung der Politik nach einem Spielplatz. 

Zwischen Sony Center und Beisheim-Center liegt der vom gleichen Architektenteam entworfene Henriette-Herz-Park. Besonderes Merkmal dieser zweiten, in Richtung des Tiergartens gelegenen Parkfläche ist die in Schollen gegliederte Höhenmodellierung sowie die aus finnischem Granit bestehende Einfassung der Rasenflächen. Sowohl der Tilla-Durieux-Park als auch der kleinere Henriette-Herz-Park ergänzen die Geschäftigkeit des restlichen Areals durch Räume für Ruhe und Entspannung. Beide Parkanlagen erfreuen sich breiter Akzeptanz und werden – vor allem im Sommer – auch als Liegewiesen zur Erholung genutzt.

Während Firmenhochhäuser, Geschäfts- und Bürobauten das Bild am neuen Potsdamer Platz bestimmen, ist langfristig ein Wohnflächenanteil von 20 % vorgesehen. Hochgerechnet heißt dies, dass hier einmal 20.000 Menschen wohnen sollen. Aufgrund des „Reißbrettentwurfs“ befürchten Kritiker, dass hier letztlich eine „Stadt in der Stadt“ entsteht. Angemerkt wird bei aller Geschäftigkeit, dass sonst zum metropolitanen Straßenbild dazugehörende Personengruppen wie etwa Obdachlose, Punks oder auch Straßenmusiker am Potsdamer Platz nicht anzutreffen sind und das Ganze so einen sehr künstlichen Charakter aufweise.

Zögerlichen Einzug in die lange verödete Berliner Mitte hält die Kultur. Ein Meilenstein war die Eröffnung der Berlinale im Jahr 2000 im "Sony Center". Der Innenhof des "Sony Centers", die "Sony Plaza", war zur Fußball-Weltmeisterschaft 2006 als Sport-Sendezentrum des ZDF umgebaut worden. Zumindest Teile des Areals werden mittlerweile von Einheimischen wie Touristen rege frequentiert. Der Komplex hat sich zur fünftwichtigsten Kaufadresse der Hauptstadt entwickelt. Skeptiker räumen mittlerweile ein, dass durch die Neubebauung zumindest das Leben wieder an den Potsdamer Platz zurückgekehrt ist. 2008 fand in den Parkkolonnaden erstmals die "Expressionale" mit Kunst des Expressionismus und der Neuen Sachlichkeit statt.

Die Großkonzerne Daimler und Sony haben ihre Areale inzwischen verkauft. Am 13. Dezember 2007 wurde das Daimler-Areal an die Immobilientochter des Finanzdienstleistungskonzerns SEB zu einem unveröffentlichten Preis veräußert; der gewünschte Mindestpreis wurde mit 1,2 Milliarden Euro angegeben. Hierzu gehören unter anderem die Potsdamer-Platz-Arkaden. Das Areal ging 2008 an die "Savills Fund Management" in Frankfurt, die es in den offenen "SEB Immoinvest Fonds" eingliederte.

Durch die Finanzkrise ab 2007 geriet der Fonds in eine Schieflage durch Kapitalabzug, sodass er im Jahr 2010 offiziell geschlossen wurde und die Besitzungen zum Verkauf standen. Als „Filetstück“ des Fonds mit mehreren Interessenten verblieb das Daimler-Areal noch mehrere Jahre im Besitz des Fonds – für den vollständigen Abverkauf der Immobilien bestand Zeit bis 2018. Im Februar 2013 wurde der Verkauf des Blocks mit dem Hotel "Grand Hyatt" an die "Al Rayyan Tourism and Investment (Artic)" bekannt, ein Unternehmen des Emirats Katar. Im Oktober 2015 wurde das restliche Areal an die Brookfield Property Partners mit Sitz in Kanada verkauft. Das restliche 267.000 Quadratmeter umfassende Areal hat dabei geschätzte 1,4 Milliarden Euro eingebracht, womit der SEB Immoinvest insgesamt mit einem Gewinn weiterverkauft hat.

Die rote Infobox wurde nach dem Ende der Neubebauung im Jahr 2001 demontiert. Ein ähnliches Bebauungskonzept wurde später auch für den östlich an den Potsdamer Platz anschließenden und inzwischen weitgehend bebauten Leipziger Platz verwendet.


Im Quartier Potsdamer Platz sind insgesamt acht Skulpturen aus der Sammlung der Daimler AG aufgestellt, die von den jeweiligen Künstlern überwiegend als Auftragswerke gefertigt wurden. Vier befinden sich auf den Flächen im Außenbereich des Areals:

Von Anfang 2000 bis Ende 2010 stand am Marlene-Dietrich-Platz die Skulptur "Balloon Flower" aus der "Celebration"-Serie von Jeff Koons. Das auffällige blau glänzende Werk, das ein beliebtes Fotomotiv darstellte, wurde im November 2010 über Christie’s New York für 16,9 Millionen US-Dollar veräußert.

An der Fassade des debis-Haus ist die von weitem erkennbare Skulptur "Gelandet" von Auke de Vries angebracht.

Nicht zu übersehen sind die Metallstelen in der Mitte der Potsdamer Straße nahe am Platz auf dem Boulevard der Stars, die im Jahr 2010 begonnen wurden und stetig ergänzt werden. Die Stelen und zugehörige Sterne für den Boden werden von der Firma Fittkau nach Entwurf von "ARGE ART+COM & Graft GbR" hergestellt.


In seinem "ersten Leben", vor dem Zweiten Weltkrieg, war der Potsdamer Platz ein von vielen Straßenbahnen oberirdisch befahrener Platz. Die letzten Reste der Schienen wurden bei seiner Totalerneuerung beseitigt.

Anders als beispielsweise der Bahnhof Friedrichstraße ist der Potsdamer Platz kein wirklich bedeutender Umsteigeschnittpunkt für das U- und S-Bahn-System. Allerdings wurde er aufgrund seiner Lage an der Nord-Süd-Trasse zum Hauptbahnhof, parallel zu den oberirdischen Errichtungen, mit einem Tunnelbahnhof dennoch auch an den Regionalverkehr angeschlossen. Aktuell halten am Regionalbahnhof Potsdamer Platz Regionalverkehrszüge der DB und der ODEG, die S-Bahn (Nord-Süd-Tunnel) sowie die U-Bahn-Linie U2. Über zahlreiche Buslinien ist der Platz ebenfalls zu erreichen. Mittelfristig ist eine Straßenbahnanbindung durch die Leipziger Straße geplant, die über die langfristig neugeplante U-Bahn-Linie U3 ergänzt oder gar ersetzt werden kann. In Nord-Südrichtung soll langfristig eine weitere S-Bahn-Strecke (Planungsname: S21), vor allem zur besseren ÖPNV-Erschließung des Hauptbahnhofs, gebaut werden.

Im Wesentlichen vier große Straßen, in West-Ost-Richtung Potsdamer- und Leipziger Straße (Bundesstraße 1) sowie in Nord-Süd-Richtung Ebert- und Stresemannstraße, führen den motorisierten Individualverkehr zum bzw. vom Potsdamer Platz. Kleinere Straßen innerhalb der einzelnen Quartiere sorgen für die Anbindung der unterirdischen Parkhäuser. Darüber hinaus wurde 2006 eine im Tunnel verlaufende Verbindung zwischen der Uferstraße am Landwehrkanal und dem Hauptbahnhof in Betrieb genommen: Der Tunnel Tiergarten Spreebogen bildet hier einen Teilabschnitt der Bundesstraße 96.

Bei der Neugestaltung des Potsdamer Platzes wurde auch der umweltverträgliche und lärmfreie nicht-motorisierte Individualverkehr berücksichtigt und alle großen Straßen erhielten gut sichtbare Fahrradwege. Bisher fehlt noch der Lückenschluss entlang der Leipziger Straße vom Potsdamer Platz zum Alexanderplatz (Stand im Jahr 2015).

Der Bereich des Kulturforums Berlin mit einigen der wichtigsten Berliner Museen, darunter die Neue Nationalgalerie und die Gemäldegalerie, grenzt westlich an den Platz. Außerdem befinden sich hier die Berliner Philharmonie, das Ibero-Amerikanische Institut und das Haus Potsdamer Straße der Staatsbibliothek zu Berlin. Der Verlauf der ehemaligen Berliner Mauer wird seit der Fertigstellung des Platzes durch in den Boden eingelassene Pflastersteine – wie an vielen anderen Stellen der ehemaligen Mauer – gekennzeichnet. Östlich des Potsdamer Platzes befindet sich das Oktogon des Leipziger Platzes. Nördlich liegt der Große Tiergarten.

In der näheren Umgebung befinden sich der Martin-Gropius-Bau, das Preußische Herrenhaus, Sitz des Bundesrates, das Denkmal für die ermordeten Juden Europas (Holocaust-Mahnmal), das Brandenburger Tor, das Musikinstrumenten-Museum sowie die Kunstsammlung Daimler Contemporary im Weinhaus Huth.

Nicht weit entfernt gab es östlich des Leipziger Platzes die beiden Diskotheken "Tresor" (Leipziger Straße 126–128; bis April 2005) und "E-Werk" (Wilhelmstraße 43; bis 1997), beides Geburtsstätten des Techno in Deutschland.

Der Potsdamer Platz wird in dem Lied "Hurra die Welt geht unter" von K.I.Z und Henning May erwähnt.





</doc>
<doc id="3991" url="https://de.wikipedia.org/wiki?curid=3991" title="Peter Mark Roget">
Peter Mark Roget

Peter Mark Roget (* 18. Januar 1779 in London; † 12. September 1869 in West Malvern, Worcestershire) war ein englischer Arzt und Lexikograph. 

Von 1834 bis 1837 war er "Fullerian Professor of Physiology" an der Royal Institution.

Am 29. April 1852 erschien sein Werk "Thesaurus of english words and phrases" das als "Roget’s Thesaurus" für die englische Sprache rasch Bedeutung erlangte. Die Erstauflage umfasste 15.000 Wörter. Der Wortschatz war in sechs Klassen eingeteilt:


Roget erfand 1815 den log-log Rechenschieber. Weiters entwickelte er um das Jahr 1835 die nach ihm benannte Rogetsche Spirale, einen Form von Selbstunterbrecherkontakt.


Nach Roget sind in der Antarktis das Kap Roget im Viktorialand und die Roget Rocks, Felsklippen vor der Küste der Antarktischen Halbinsel, benannt.



</doc>
<doc id="3993" url="https://de.wikipedia.org/wiki?curid=3993" title="Pandemie">
Pandemie

Unter Pandemie versteht man eine "länder- und kontinentübergreifende" Ausbreitung einer Krankheit beim Menschen, im engeren Sinn die Ausbreitung einer Infektionskrankheit. Im Unterschied zur Epidemie ist eine Pandemie somit örtlich nicht beschränkt.

Auch bei Pandemien gibt es Gebiete, die nicht von der Krankheit betroffen werden. Durch ihre abgeschiedene Lage können manche Gebirgstäler, Völker im Urwald oder Bewohner abgelegener Inseln von einer Infektion verschont bleiben.

Das Wort "Pandemie" (πανδημία) ist aus den griechischen Wörtern πᾶν (Transliteration "pãn") ‚alles‘ und δῆμος "dḗmos" ‚Volk‘ abgeleitet "Pandemie" bezeichnet folglich etwas, das ‚das ganze Volk‘ trifft.

Da sich die Endung "-demie" auf Menschen bezieht, sind in der Veterinärmedizin auch die Bezeichnungen "Seuchenzug" und Panzootie (gr.  "zõon" ‚lebendes Wesen‘, ‚Lebewesen‘, ‚Tier‘) statt "Pandemie" und "Epizootie" (von altgriech. επί "epí" „auf“) statt "Epidemie" üblich.

Flugrouten sind heute die schnellsten Ausbreitungswege von Infektionskrankheiten. So entwickelte sich AIDS, das durch das HI-Virus verursacht wird, u. a. durch den Flugtourismus von einem lokalen zu einem weltweiten Problem. Nachvollziehbar war dieser Effekt auch während der SARS-Pandemie 2002/2003: Während man in Asien noch die klassischen Verbreitungswege für SARS annahm, zeigte die zunehmende Zahl der Erkrankungen in Kanada diesen Reise-Effekt schon recht deutlich. Auch das Ausbreitungsgeschehen im Verlauf der Zikavirus-Epidemie 2015/2016 in Südamerika wurde mit der intensiven Reisetätigkeit während der Endrunde der Fußball-Weltmeisterschaft 2014 in Verbindung gebracht.

Die Pest im Mittelalter kam wahrscheinlich an Bord von Handelsschiffen von Asien nach Europa.

Natürlich können sich manche Krankheitserreger, je nach Übertragungsweg, auch ohne solche „technische Hilfsmittel“ schnell über große Flächen und Entfernungen ausbreiten, doch verhindern dabei meist geografische Barrieren die weltweite Verbreitung.

Zur quantitativen Beschreibung und Prognose pandemischer und ähnlicher Ausbreitungsprozesse nutzen Wissenschaftler Computersimulationen sowie Methoden der Theoretischen Biologie, beispielsweise das SIR-Modell.

Große Pandemien in der Geschichte waren:

In jüngerer Zeit haben vor allem die Influenza-Pandemien für große mediale Aufmerksamkeit gesorgt. Auslöser dieser Pandemien sind Influenzaviren der Gruppen A und – seltener – B, da diese in der Lage sind, ihre antigenen Oberflächenmoleküle Hämagglutinin (HA) und Neuraminidase (N) ständig zu verändern. Solche Veränderungen können jederzeit eintreten und dazu führen, dass sie vom Immunsystem trotz Impfung (oder Immunität nach einer vorhergegangenen Influenza-Infektion mit Viren, die noch andere Oberflächeneigenschaften hatten) bei einer erneuten Infektion nicht mehr oder nur schlecht erkannt werden.

Bei einer Influenza-Epidemie oder „Grippewelle“ werden 10 bis 20 Prozent einer Bevölkerung infiziert, aber die Ausbrüche bleiben lokal begrenzt. Bei einer Pandemie hingegen verbreiten sich die Viren rasch und mit Infektionsraten von bis zu 50 Prozent über den ganzen Globus. Auslöser ist immer ein neuer Subtyp des Influenza-A-Virus, der auch durch eine Antigenshift (eine Durchmischung von humanen und aviären, das heißt aus Geflügel stammenden Gen-Segmenten) entstehen kann. Eine solche Durchmischung von „Vogelgrippe“- und humanen Influenzaviren kann beispielsweise im Schwein stattfinden („Schweinegrippe“), wenn diese Tiere Träger beider Viren sind.

Auch in „Grippe-“Jahren ohne Pandemie stirbt jährlich eine Vielzahl von Menschen an dieser Krankheit oder an ihren Folgen, vor allem einer Lungenentzündung infolge bakterieller Superinfektion.

Beispiele für Influenzapandemien in jüngerer Zeit:


Der Plan der Weltgesundheitsorganisation (WHO) zur Vorbereitung auf Influenza-Pandemien teilt das Risiko des Übergangs auf den Menschen, also die Wahrscheinlichkeit für die globale Verbreitung unter Menschen in sechs Stufen ein (Nur in Fußnoten wird die Möglichkeit erwähnt, für die Unterscheidung der Stufen 3 bis 5 auch die Pathogenität und Letalität zu berücksichtigen.):


Besondere Brisanz wurde ab 2005/06 der sogenannten Vogelgrippe H5N1, verursacht durch das Influenza-A-Virus H5N1, zugeschrieben, deren Viren auch ohne jedes Zutun des Menschen von Zugvögeln verbreitet werden. Ihr hat die Weltgesundheitsorganisation seit mehreren Jahren die Pandemiewarnstufe 3 zugeordnet (Stand: April 2013.)

Sollten die A/H5N1-Viren mutieren, so dass sie von Mensch zu Mensch übertragen werden können, erwarten einige Experten ein Szenario, das in zwei Phasen abläuft:

Ferner sind Vorbereitungen dafür zu treffen, dass genügend Krankenhausbetten verfügbar gemacht werden können. So schrieb die "Deutsche Gesellschaft für Innere Medizin" Ende April 2006 in einer Pressemitteilung: „Sollte es eines Tages zu einem weltumspannenden Erkrankungsausbruch kommen, würden in Deutschland voraussichtlich 360.000 Menschen einen Platz im Krankenhaus benötigen.“

Auf einem Influenza-Fachkongress in Wien wurde im Oktober 2006 berichtet, dass damals mehr als 95 Prozent aller Impfstoffe in nur neun Ländern produziert wurden, was bedeutete, dass 86 Prozent aller Menschen in Ländern lebten, die selbst keine Produktionskapazitäten besaßen. Wären im Jahr 2007 alle Kapazitäten für den normalen Grippeimpfstoff auf Pandemie-Impfstoff umgestellt, hätten maximal 300 Millionen Menschen versorgt werden können. Bis zum Jahr 2011 begannen elf Entwicklungsländer mit dem Aufbau oder der Inbetriebnahme entsprechender Fertigungsanlagen. Trotz der mittlerweile erfolgten, erheblichen Ausweitung von Produktionskapazitäten gilt es auch weiterhin als illusorisch, dass ein weltweiter Schutz vor einer Influenza-Pandemie durch Impfungen angesichts der Größe der Weltbevölkerung möglich ist.

Sollten allerdings die Autoren eines im Juli 2006 in "Nature" veröffentlichten Berichts recht behalten, dann würden sich alle Planungen für Massenimpfungen – zumindest für die erste Erkrankungswelle – als obsolet erweisen: Sie prognostizierten, dass der Gipfelpunkt der ersten Erkrankungswelle zwei bis drei Monate nach dem Beginn der Pandemie erreicht und die akute Pandemiephase schon nach vier Monaten beendet sein würde.

Die Weltgesundheitsorganisation (WHO) hat im Jahre 1948 ein weltweites Überwachungssystem installiert, das die von 110 Referenzlaboratorien isolierten Virusstämme ständig auf neue Varianten überprüft, was zu den jährlich neuen Empfehlungen für die Impfstoffzusammensetzung der kommenden Saison führt, mit industriellem Herstellungszyklus von zehn bis zwölf Monaten. Zudem hat die WHO 1999 von allen Ländern die Erstellung nationaler Pandemiepläne gefordert.

Durch diese Maßnahmen soll zum einen sichergestellt werden, dass im Fall einer Pandemie rasch sichere Impfstoffe hergestellt werden können; für Europa hat die "Vaccine Expert Group" der Europäischen Arzneimittelagentur entsprechende Leitlinien erarbeitet. Zum anderen soll ein erprobter Maßnahmenkatalog dazu beitragen, Infektionsketten zu unterbrechen und die Krankenversorgung sicherzustellen.

Zwischen 2006 und 2015 wurde die Impfstoffproduktion weltweit von geschätzten 0,5-1,5 Milliarden Dosen pro Jahr auf nahezu 6 Milliarden Dosen gesteigert.

Ende 2006 kam es zwischen der indonesischen Regierung und der WHO zu Spannungen, da das Land sich weigerte, wie vereinbart die im Land gewonnenen Virusproben des Influenza-Subtyps H5N1 an die WHO-Labors zu liefern. Indonesien versuchte so, Druck auf die WHO auszuüben, um zu erreichen, dass dem Land im Fall einer Pandemie ausreichende Mengen an Impfstoff zur Verfügung gestellt werden. Da Fabriken zur Impfstoffproduktion nur in Industrieländern existieren, befürchtet die indonesische Regierung, dass diese Impfstoffe im Fall einer Pandemie vorwiegend in den Herstellungsländern verteilt werden. Im März 2007 schien Indonesien einzulenken, nachdem die WHO zugesagt hatte, den Aufbau von Impfstoffproduktionen auch in den weniger entwickelten Regionen zu fördern. Tatsächlich wurden im Jahr 2007 aber nur zwei indonesische Virusproben abgegeben, obwohl im Land mit mehr als 40 Neuerkrankungen die weltweit höchsten Fallzahlen aktenkundig wurden.

Die Weltgesundheitsorganisation hat in die betroffenen Gebiete Ermittler (Feldepidemiologen) entsandt. Diese beobachten zum Teil mit erheblichem Aufwand die aktuellen Übertragungswege und Entwicklungen des Virus. In vielen Staaten wurden nationale Krisenpläne für den Fall eines massiven Übergangs von Vogelgrippeviren auf Menschen erarbeitet. In Japan wurde beispielsweise ein Notstandsplan vorgelegt, der auch die Zwangsverlegung von Erkrankten in Hospitale vorsieht, die Schließung von Schulen und das Verbot großer Versammlungen. Die Volksrepublik China kündigte an, notfalls die Landesgrenzen zu schließen. Auch der australische Notfallplan sieht eine Schließung sämtlicher Häfen und Flugplätze für Verkehr mit dem Ausland vor.

Auf betrieblicher Ebene befürchteten 92 % der Unternehmen 2006 im Falle einer Pandemie wie der Vogelgrippe mittlere bis schwere Unterbrechungen des Geschäftsbetriebs, da bis dahin bestehende Notfallpläne dieses Szenario weitestgehend ausklammerten. Im Nachgang zu dieser Studie führten die Erfahrungen im Rahmen der Vogelgrippe bei zahlreichen Unternehmen (insbesondere bei der Produktion von Energie, Nahrungs- und Arzneimitteln) zu einer Integration in überregionale Pandemiepläne und zu einer präventiven Schulung der Mitarbeiter mit Hygienemaßnahmen im Pandemiefall.

Für Deutschland hat das Robert Koch-Institut – als "Nationales Referenzzentrum für Influenza" – als Grundlage für die Abschätzung der Folgen einer Pandemie mehrere Szenarien entwickelt. Die schlimmste Variante unterstellt 21 Millionen zusätzliche Arztbesuche und bis zu 160.000 Tote. Als realistischer wird allerdings eine mittlere Variante angesehen, bei der aber auch noch ca. 100.000 zusätzliche Todesfälle unterstellt werden. Das Auswärtige Amt hat zudem einen eigenen Pandemieplan erstellt, um den Schutz seiner Mitarbeiter in Auslandsvertretungen zu gewährleisten.

Gefördert mit je 10 Millionen Euro wurden die Firmen Novartis und GlaxoSmithKline vertraglich verpflichtet, ihre Produktionskapazitäten so zu erweitern, dass die gesamte Bevölkerung mit einem Pandemie-Impfstoff versorgt werden könnte – das wären zweimal 80 Millionen Dosen.

In dem Anfang 2005 veröffentlichten Nationalen Pandemieplan des Bundes und der Länder wurde für den Fall eines Ausbruchs einer Influenzapandemie als Mindestmaßnahme festgelegt, dass die Therapie aller Erkrankten durch antivirale Arzneimittel sichergestellt sein soll. Darauf begannen die Bundesländer mit Planungen zur Bevorratung dieser Medikamente. Ergänzt wurde der nationale Pandemieplan durch detailliertere lokale Planungen der Landkreise und der Gemeinden. Im Jahr 2016 wurde der Nationale Pandemieplan überarbeitet und auch danach noch aktualisiert.

In der Schweiz sollte das Pflichtlager mit antiviralen Medikamenten bis Ende 2005 aufgefüllt sein. Zudem wird vom Bundesamt für Gesundheit die vorsorgliche Anschaffung von Atemschutzmasken empfohlen.

Da in Österreich die Gesundheitsversorgung Landessache ist, kann der Bund nur Empfehlungen und Koordinationen anbieten. Bisher wurde vom Bund die Anschaffung von Medikamentendosen für 15 % der Bevölkerung beschlossen. Es werden auch laufend Stabsübungen der Einsatzorganisationen durchgeführt, wobei es immer eine Gratwanderung ist, die Menschen zu informieren und trotzdem keine Panik zu erzeugen. Der Zivilschutzverband in Österreich hat eine Homepage mit Verhaltensregeln für den Fall einer Pandemie aktiviert.

In den USA wurden ab 2005 erhebliche Gelder bereitgestellt, um eine neue Generation von Technologien zu entwickeln, damit innerhalb von sechs Monaten nach Ausbruch einer Epidemie genug Impfstoff für alle US-Bürger produziert werden kann. Seit 2005 hat die US-Regierung zudem eine Website mit Verhaltensregeln für den Pandemiefall geschaltet.

Im Rahmen der Notfallplanung wurde vom „National Vaccine Advisory Committee“ (NVAC) im März 2006 auch ein Vorschlag vorgelegt, welche Personengruppen im Falle einer Influenzapandemie vorrangig geimpft werden sollen, wenn der Impfstoff nicht für die gesamte Bevölkerung verfügbar ist. Zu diesem vorrangig zu schützenden Kreis gehören die mit der Herstellung und Verteilung des Impfstoffs befassten Personen, Ärzte und Krankenschwestern, hohe Regierungs- und Behördenvertreter, Schwangere sowie Schwerstkranke, bei denen ein erhöhtes Risiko für Lungenentzündungen besteht. Außerdem sollen diejenigen vorrangig geimpft werden, die zuhause immungeschwächte Personen oder ein Kind von unter sechs Monaten pflegen. Zweite Priorität wurde für folgende Gruppen vorgeschlagen: Menschen jenseits des 65. Lebensjahres, Kinder unter zwei Jahren sowie die Beschäftigten von Polizei und Feuerwehr, von Energieversorgungs- und Transportunternehmen, von Fernsprech- und IT-Firmen.





</doc>
<doc id="3994" url="https://de.wikipedia.org/wiki?curid=3994" title="Pragmatik">
Pragmatik

Pragmatik steht für:



</doc>
<doc id="3996" url="https://de.wikipedia.org/wiki?curid=3996" title="Politikwissenschaft">
Politikwissenschaft

Politikwissenschaft (früher meist Politische Wissenschaft, Wissenschaft von der Politik, Wissenschaftliche Politik oder auch Politologie) ist als Integrationswissenschaft ein Teil der modernen Sozialwissenschaften und beschäftigt sich mit dem wissenschaftlichen Lehren und Erforschen politischer Prozesse, Strukturen und Inhalte sowie den politischen Erscheinungen und Handlungen des menschlichen Zusammenlebens. Die Politikwissenschaft zählt von ihrer traditionellen Entwicklung als Wissenschaftsdisziplin im weiteren Sinne auch zu den Staatswissenschaften. Mit Nachbardisziplinen wie der Soziologie, der Rechtswissenschaft, der Geschichtswissenschaft, den Wirtschaftswissenschaften und der Psychologie erschloss sie sich inzwischen einen interdisziplinär angelegten Untersuchungsgegenstand, der über den Staat und seine Institutionen als Forschungsgegenstand hinausreicht.

Das Fach wird in verschiedene Teilbereiche untergliedert. Grundlegend ist die Differenzierung zwischen den Bereichen "Politische Theorie" (einschließlich "Politische Philosophie" und "Ideengeschichte"), "Vergleichende Politikwissenschaft" (früher "Vergleichende Regierungslehre" oder "Vergleichende Analyse politischer Systeme") und "Internationale Beziehungen" (einschließlich "Internationale Politik"). Im Fall eines breiter angelegten Lehrangebots, wie es an manchen Universitäten betrieben wird, werden beispielsweise zusätzlich die Teildisziplinen "System-" bzw. "Regierungslehre", "Politische Soziologie", "Politische Ökonomie", "Politische Methodenlehre", "Verwaltungswissenschaft", öffentliches Recht und "Politikfeldanalyse" oder in jüngerer Zeit "Geschlechterforschung" unterschieden.

Die Politikwissenschaft befasst sich mit dem gesellschaftlichen Zusammenleben der Menschen und untersucht, wie dieses Zusammenleben geregelt ist und geregelt werden kann. Ihr Gegenstandsbereich reicht demnach grundsätzlich über eine Beschäftigung mit der Tagespolitik hinaus. Ihr Untersuchungsinteresse erfordert die Analyse von grundlegenden Prinzipien, Zusammenhängen und von Ursache- und Wirkungsmechanismen des menschlichen Zusammenlebens in seinen unterschiedlichen Formen. Dabei berücksichtigt sie u. a. institutionelle, prozedurale, sachlich-materielle und politisch-kulturelle Gesichtspunkte. Ein besonderes Augenmerk richtet die moderne Politikwissenschaft auf die Frage, wie staatliche und zivilgesellschaftliche Akteure agieren, wie politische Entscheidungsprozesse ablaufen, wie Machtverhältnisse entstehen und auf gesellschaftliche Strukturen einwirken.

Ursprünglich hatte die Politikwissenschaft eine nahezu rein normativ-ontologische Ausrichtung: Seit dem Altertum beschäftigte sie sich mit der Frage, wie das Zusammenleben der Menschen "am besten" gestaltet werden könne. Dies lässt sich bis zu den antiken griechischen Philosophen – vor allem auf Platon und Aristoteles – zurückführen und ist bis heute Gegenstand des philosophischen und ideengeschichtlichen Zweiges der Politikwissenschaft. Als normative Wissenschaft wurde die Politikwissenschaft auch nach ihrer Wiederbegründung als akademische Disziplin in der Geschichte der Bundesrepublik Deutschland nach 1945 verstanden und konzipiert („Demokratiewissenschaft“). Die deutsche Tradition policeywissenschaftlicher und kameralwissenschaftlicher Forschung war vom 19. bis 20. Jahrhundert abgebrochen. Im klassischen Verbund mit der Rechtswissenschaft entstand die Politikwissenschaft zunächst als Teil der Staatswissenschaften, zu denen sie auch heute noch gezählt werden kann, obwohl der Staat und seine Funktionen nicht mehr ihr ausschließliches Untersuchungsobjekt ist.

Ausgehend von der Entwicklung des Faches in den Vereinigten Staaten wurde die Politikwissenschaft seit den 1960er Jahren methodisch stärker vom Aufkommen des Behavioralismus sowie von den sozialwissenschaftlich orientierten empirisch-analytischen Methoden beeinflusst. Damit einher ging eine zunehmende Orientierung des Faches hin zu positivistischen Fragestellungen.

Ziel der modernen "empirischen" Politikwissenschaft ist es, aus der Beschäftigung der Gesellschaft und ihren Strukturen Zusammenhänge zu bestimmen, die das Zusammenleben von Menschen erklären und beschreiben. Dieser Zweig des Faches ist stark methodisch geprägt und arbeitet sowohl quantitativ als auch qualitativ. Eine abschließende Wertung der Untersuchungsergebnisse muss hierbei entfallen. Damit orientiert sich dieser prominente Zweig des Faches analytisch und methodisch an den Naturwissenschaften und wird im Allgemeinen nach wie vor wesentlich durch US-amerikanische Entwicklungen und Innovationen geprägt. Dies betrifft vor allem die analytische Stringenz (Einsatz mathematischer Modelle, die sogenannte Theorie der rationalen Entscheidung) sowie methodische Rigorosität (Einsatz statistischer Verfahren).

Etwas anders stellt sich das Fach im Bereich der modernen theoretischen bzw. normativen Politikwissenschaft dar, der größtenteils mit dem eher geisteswissenschaftlich orientierten Teilfach der Politischen Theorie zusammenfällt: In Anknüpfung an die lange normative Tradition der Politischen Wissenschaft, werden hier gesellschaftliche Werthaltungen auf ihren normativen Gehalt hin analysiert und vor dem Hintergrund ideen- und philosophiegeschichtlicher Kontexte diskutiert und bewertet. Dabei bedient man sich beispielsweise der Methode der analytisch-hermeneutischen Textinterpretation oder anderer qualitativer Verfahren. Die Beschäftigung mit Werturteilen steht dementsprechend mitunter im Zentrum der Politischen Theorie als Teilfach der Politikwissenschaft. Im Besonderen gilt dies für die Politische Philosophie als betont normativer politischer Theorie.

Das wissenschaftliche Fach "Politik" wird im deutschsprachigen Raum meist unter der Bezeichnung "Politikwissenschaft" gelehrt. Diese Begriffsbestimmung hat die früheren Fachbezeichnungen "Politische Wissenschaft", angelehnt an die angelsächsische Bezeichnung "political science" und "Wissenschaft von der Politik" oder "Wissenschaftliche Politik", wie sie mit der Einrichtung von Lehrstühlen an Universitäten seit Beginn der 1950er Jahre genannt wurde, weitgehend abgelöst. Bei Instituts- oder Seminar-Bezeichnungen einiger Universitäten besteht noch die Fachbezeichnung "Politische Wissenschaft" oder "Wissenschaft von der Politik". Die Bezeichnung "Politologie" ist ebenfalls gebräuchlich. Die genannten Begriffe werden weitgehend synonym verwendet.

Vertreter der Disziplin bevorzugen heute die Bezeichnung "Politikwissenschaft", weil sie den Gegenstand des wissenschaftlichen Bemühens, die Erforschung der Politik und ihrer Prozesse, stärker betont. Dieser Wissenschaftsbegriff wird inzwischen an den Universitäten allgemein verwendet.

Das Problem der uneinheitlichen Terminologie rührt daher, dass "Politik" als Universitätsfach vom 19. bis 20. Jahrhundert über mehr als 100 Jahre unterbrochen war. Die Bezeichnung "Politikwissenschaft" grenzt sich begrifflich zudem besser als der Begriff der Politischen Wissenschaft vom möglichen Verdacht ab, es handele sich um eine Pseudowissenschaft, die aus vorwiegend politischen Motiven und zu politischen Zwecken betrieben wird. Politik als Zweck der Wissenschaft gilt jedoch als unvereinbar mit dem allgemein akzeptierten Wissenschaftsverständnis. Politikwissenschaft erhebt vielmehr den Anspruch auf Wertneutralität und auf eine strikte Unterscheidung von "Politikwissenschaft" und der realen "Politik". Ein Politiker "macht" Politik, ein Politikwissenschaftler setzt sich wissenschaftlich mit politischen Fragen auseinander.

Der Begriff "Politische Wissenschaft" übersetzt sehr eng die Fachbezeichnung im angelsächsischen Sprachraum, die "political science". Wenn in der Bundesrepublik und im weiteren deutschsprachigen Raum überhaupt noch an unterschiedlichen Bezeichnungen für den denselben Wissenschaftsgegenstand festgehalten wird, hat das vornehmlich kulturelle und wissenschaftsgeschichtliche Gründe.

Eine Herleitung aus dem Altgriechischen ("epistéme politiké") stellt der Begriff "Politologie" dar, in Anlehnung an die moderne "Soziologie". Allerdings entstand dieser Terminus ohne Rücksichtnahme auf das Griechische; eigentlich müsste er "Politikologie" lauten.

Die "Politikwissenschaft" als wissenschaftliche Disziplin hat sich hierzulande erst nach dem Zweiten Weltkrieg herausgebildet und etabliert. Gleichwohl gab es schon vorher Ansätze, eine solche Disziplin im Deutschen Reich zu etablieren: so wurde 1920 die Deutsche Hochschule für Politik in Berlin gegründet. An ihr lehrten hauptsächlich Wissenschaftler aus anderen Disziplinen, da es zum damaligen Zeitpunkt eine Politikwissenschaft im engeren Sinne in Deutschland noch nicht gab – anders als in den USA.
Politikwissenschaft wurde in der Zwischenkriegszeit und zunächst auch nach dem Zweiten Weltkrieg als Demokratie- und Integrationswissenschaft verstanden, die Inhalte und Methoden anderer, benachbarter Wissenschaften aufnahm. Die Nachfolgeinstitution der Deutschen Hochschule für Politik wurde das Otto-Suhr-Institut der Freien Universität Berlin.

Bereits im 18. Jahrhundert lehrte Joseph von Sonnenfels an der Universität Wien Politische Wissenschaften. Es etablierten sich im 19. Jahrhundert Fächer wie die Kameralwissenschaft und die Policeywissenschaft. Dabei führte die damalige politische Wissenschaft Ansätze fort, die schon seit der frühen Neuzeit von Rechtswissenschaftlern, Politischen Philosophen, Theologen und von Historikern begründet worden sind.

Eine eigene Disziplin entwickelte sich in Deutschland aber erst nach dem Zweiten Weltkrieg unter US-amerikanischem Einfluss. Anknüpfen ließ sich dabei an Aktivitäten der Deutschen Hochschule für Politik, die in der Frühphase der Weimarer Republik 1920 in Berlin gegründet worden war und bis zu ihrer Eingliederung in die Berliner Universität 1940 bestand. Politikwissenschaft wurde damals im Wesentlichen als Demokratiewissenschaft verstanden.

Nach dem Zweiten Weltkrieg stand ihr Selbstverständnis als Demokratiewissenschaft und damit als Wissenschaft von der Funktionsweise der Demokratie erneut im Zentrum. Mit ihrer Hilfe sollten insbesondere Mittler wie Lehrer und Journalisten befähigt werden, den demokratischen Gedanken zu vermitteln und demokratisches Denken in der Bevölkerung zu verankern. Daher beschäftigte sich die frühe nachkriegsdeutsche Politikwissenschaft hauptsächlich mit der Analyse, der Funktionsweise und dem formellen Interagieren von Institutionen wie etwa den Parteien, den Gewerkschaften, dem Parlament oder der Bundesregierung. Heute bezeichnet man diesen Gegenstandsbereich als Polity.

Mit dem politischen und wirtschaftlichen Erfolg der Bundesrepublik Deutschland rückte die Erforschung der eigentlichen politischen Prozesse in den Vordergrund: Man versuchte zu verstehen, was innerhalb der Institutionen selbst passiert und welche Funktionen sie jeweils im Gesamtsystem erfüllten, anstatt zu beschreiben, welche Aufgaben sie formal haben. Dabei traten insbesondere die Verbände in den Mittelpunkt des Interesses, die – obwohl nicht gesetzlich verankert – gleichwohl einen wichtigen Anteil am politischen Prozess haben.

Man versuchte also, die tatsächlichen Willensbildungs- und Entscheidungsprozesse (Politics) zu analysieren und zu verstehen.

In der bundesdeutschen Entwicklung der Politikwissenschaft bildeten sich in den Jahrzehnten nach dem Zweiten Weltkrieg mit der Kölner Schule, der Freiburger Schule sowie der Marburger Schule sogenannte Schulen der Politikwissenschaft heraus, die jeweils ein spezifisches Verständnis des universitären Faches besaßen und vertraten.

Ebenso wie ihr Forschungsfeld, die Politik, ist auch die Politikwissenschaft bestrebt, ihre Betrachtungen zu spezialisieren, etwa auf einzelne Politiksektoren wie z. B. die Gesundheitspolitik. Hierbei ist Fachkompetenz zur Analyse der tatsächlichen Probleme erforderlich. Diese neuere Teildisziplin der Politikwissenschaft, die sich mit Sachproblemen einzelner Politikbereiche auseinandersetzt, wird Policy-Forschung oder auch Politikfeldforschung genannt.

Diese spielt für die Politikberatung eine zunehmende Rolle, mit Hilfe derer sich politische Entscheidungsträger an wissenschaftlich fundierter Beratung orientieren oder eine politische Entscheidung treffen und absichern wollen. Die Grenzen der Wissenschaftlichkeit solcher Beratungen sind jedoch oft unklar – vielfach sind es „Gefälligkeitsgutachten“, also interessengeleitete Gutachten, die zu einem vom Auftraggeber gewünschten Ergebnis kommen.

Die Paradigmen der Integrationswissenschaft und der Demokratiewissenschaft werden daher heute zunehmend durch die Auffächerung der Disziplin Politikwissenschaft in die Teildisziplinen "Polity", "Politics" und "Policy" ersetzt.

Eine weitere, auch für die Lehrstuhlbezeichnungen gebräuchliche Unterteilung der Politikwissenschaft in Teildisziplinen ist die Unterteilung in Politisches System (bezogen auf einzelne Staaten, beispielsweise Deutschland; früher: Regierungslehre), Politische Theorie, Politikgeschichte, Internationale Politik oder Internationale Beziehungen, European Studies oder Europäische Politik, Vergleichende Politikwissenschaft oder Komparatistik (früher: Vergleichende Regierungslehre, auch Vergleichende Analyse politischer Systeme).

Zu den wichtigsten Gegenständen der Politikwissenschaft gehören die Strukturprobleme der Demokratie, politische Parteien und soziale Bewegungen, internationale Beziehungen, Konfliktforschung, Staatsinterventionen und Wirtschaft, politische Haltungen und Bewusstseinsformen, öffentliche Meinung, Massenmedien und Wahlverhalten.

Absolventen politikwissenschaftlicher Studiengänge sind jenseits der wissenschaftlichen Tätigkeit von Politikwissenschaftlern in vielen Berufsfeldern zu finden. Klassisch sind dabei vor allem die Politische Bildung, als Unterrichtsfach im Lehramt, in der Publizistik und in den Medien, in Parteien und Parlamenten, in Verbänden sowie auch in der öffentlichen Verwaltung und in internationalen Organisationen und zudem in der Wirtschaft. Die individuellen Berufslaufbahnen orientieren sich dabei neben der Absolvierung des politikwissenschaftlichen Studiums auch an Zusatzqualifikationen wie Sprachkenntnissen oder anschließenden weiteren fachlichen Qualifikationen.

An fast jeder größeren deutschen Universität kann man Politikwissenschaft entweder als Haupt- oder Nebenfach studieren. Vereinzelt bieten kleinere Universitäten aus Ressourcenmangel Politikwissenschaft nur als Nebenfach an. Während früher oftmals Diplom- und einige wenige Magisterstudiengänge mit politikwissenschaftlichem Schwerpunkt existierten, werden infolge des Bologna-Prozesses heutzutage für Studienanfänger fast ausschließlich Bachelor- und Masterstudiengänge angeboten. Viele Studiengänge sind interdisziplinär ausgerichtet und verbinden Inhalte verschiedener Sozialwissenschaften mit politikwissenschaftlichen Kernthemen, was dem früheren Magisterstudium ähnelt. Das Staatsexamen für das Lehramt befähigt zur Ausübung des Lehrerberufs – das korrespondierende Unterrichtsfach Politische Bildung firmiert in den meisten Bundesländern unter verschiedenen Bezeichnungen: Gemeinschaftskunde, Sozialkunde, Gesellschaftslehre, Politik- und Sozialwissenschaft, Politik und Wirtschaft etc.

In der Nachkriegszeit wurden in Deutschland zahlreiche Institute für Politikwissenschaft gegründet. Auch einzelne Lehrstühle und Professuren können an manchen Universitäten existieren.


In der "DDR" wurde offiziell eine Politikwissenschaft als bürgerliche Ideologie und Revisionismus abgelehnt. Insbesondere die Leiterin der Parteihochschule der SED, Hanna Wolf, lehnte eine marxistische Politikwissenschaft ebenso ab, wie die sich Anfang der 1960er Jahre in der DDR (Leipziger Universität) etablierende Disziplin „Wissenschaftlicher Sozialismus“. Das „vernichtende“ Argument war: „An der Leninschule in Moskau gab es das nicht.“ Tatsächlich begannen Anfang der 1970er Jahre unter der Überschrift Wissenschaftlicher Sozialismus und inspiriert von dem Leipziger Professor Günther Großer, zaghafte Versuche, eine marxistische Politikwissenschaft zu etablieren. Es entstand die sogenannte "Leipziger Schule", zu der neben Günter Großer auch die Leipziger Wissenschaftler Rolf Reißig, Frank Berg und Robert Weiß gehörten. Insbesondere seit deren Wirken an der Akademie für Gesellschaftswissenschaften, begann eine verstärkte Profilierung der Disziplin als Politikwissenschaft in Gestalt klassischer Politikfeldforschung (Menschenrechte Frank Berg) Anwendung von Systemtheorien in Gestalt einer komparativen Sozialismusforschung (Robert Weiß). Folgerichtig waren die Mitarbeiter des Instituts „Wissenschaftlicher Sozialismus“ (Leitung Rolf Reißig) der Akademie für Gesellschaftswissenschaften am aktivsten beteiligt an einer innerparteilichen Opposition im Herbst 1989.
Anfang 1990 wurde das Institut „Wissenschaftlicher Sozialismus“ in "Institut für Politikwissenschaft" umbenannt.

Als intellektueller Vater der Politikwissenschaft in Österreich gilt der österreichisch-US-amerikanische Historiker Ernst Florian Winter. 1938 musste er mit seinem Vater Ernst Karl Winter aus politischen Gründen in die Vereinigten Staaten emigrieren. Auf Einladung der Minister Drimmel und Klaus kehrte er 1960 nach Studien an der University of Michigan und Columbia University und Gastprofessuren an der Fletcher School of Law and Diplomacy, Princeton University, Georgetown University und Indiana University erneut nach Österreich zurück, um auch hier die Studienrichtung der Politikwissenschaft zu etablieren. 1964 wurde er von Bruno Kreisky zum Gründungsdirektor der Diplomatischen Akademie Wien bestellt. Ab 1967 war er im Institut für Höhere Studien in Wien tätig.

Ein Studium der Politikwissenschaft in Österreich ist als Teil des Bologna-Systems in ein Bachelor- und Masterstudium unterteilt. Danach ist die Promotion möglich. Neben dem wissenschaftlichen Abschluss eines Diplomstudiums ist auch ein Lehramtsabschluss möglich, bei dem Politikwissenschaft innerhalb des Lehrfachs Geschichte-Sozialkunde-Politische Bildung studiert wird. Politikwissenschaft wird in Österreich an den Universitäten Innsbruck, Salzburg und Wien angeboten. In Innsbruck wurde mit 1. Januar 2005 sogar eine eigene Fakultät für Politikwissenschaft und Soziologie eingerichtet. Dort gibt es seit dem Wintersemester 2007/08 auch die Bachelorstudien Politikwissenschaft und Soziologie. (Abschluss jeweils mit Bachelor of Arts). Im Wintersemester 2008/09 wurden die beiden Masterstudiengänge „Europäische Politik und Gesellschaft“ und „Soziale und Politische Theorie“ eingerichtet.

Auch in der Schweiz lässt sich Politikwissenschaft an fast allen großen Universitäten studieren, namentlich in Zürich, Basel, Bern, Genf, Lausanne, Luzern und St. Gallen. 

Das CIS (Center for Comparative and International Studies) ist ein politikwissenschaftliches Forschungsinstitut. Es wurde 1997 gebildet aus dem Institut für Politikwissenschaft der Universität Zürich und den politikwissenschaftlichen Lehrstühlen der ETH Zürich.

Die Universität Konstanz bietet interdisziplinäre politikwissenschaftliche Studiengänge mit verwaltungswissenschaftlichen Inhalten und besonderem Fokus auf der sozialwissenschaftlichen Methodenlehre an. Die Universität Erfurt und die Universität Passau bieten unter dem Namen Staatswissenschaften einen Studiengang, in dem Politikwissenschaft interdisziplinär mit Bezügen zu Nachbardisziplinen wie Rechtswissenschaft und Wirtschaftswissenschaften studiert werden kann. Ähnliche staatswissenschaftliche Programme existieren auch an der Leuphana Universität Lüneburg und mit starkem verwaltungspraktischen Bezug an der NRW School of Governance.

Die Zeppelin Universität Friedrichshafen bietet die interdisziplinär ausgerichteten 4-jährigen Bachelor- und 2-jährigen Masterstudiengänge „Politics, Administration & International Relations“ an, die Vertiefungen in „Managing Global Challenges & International Relations“, „Political Behavior & Decision Making“ und „Public Management & Policy, Regulation & E-Government“ ermöglichen. Zudem beinhaltet der Studiengang einen hohen Anteil an wirtschafts-, rechts- und kommunikationswissenschaftlichen Inhalten sowie Projektseminare auf Grundlage des forschenden Lernens.

An der Friedrich-Alexander-Universität Erlangen-Nürnberg kann Politikwissenschaft im Bachelor- und Masterstudiengang mit verschiedenen Schwerpunkten, teilweise unter Einbindung benachbarter Fächer, studiert werden. So bietet die Erlanger Universität u. a. die Schwerpunkte „Menschenrechte und Menschenrechtspolitik“ (als Teilfach vertreten durch einen eigenen Lehrstuhl), Öffentliches Recht (in Kooperation mit dem Fachbereich Rechtswissenschaft) oder „Außereuropäische Regionen“ (etwa Lateinamerika betreffend) an. Ein Masterstudiengang mit dem Schwerpunkt Politische Theorie ist in Vorbereitung. Umgekehrt kann Politikwissenschaft auch als Schwerpunkt im regionalwissenschaftlichen Masterstudiengang Nahoststudien gewählt werden. Eine Einbeziehung des Öffentlichen Rechts ist auch an der Julius-Maximilians-Universität Würzburg möglich.

Die FernUniversität in Hagen bietet ein Fernstudium der Politikwissenschaft an, das den Bachelorstudiengang Politikwissenschaft, Verwaltungswissenschaft, Soziologie (bis 2008: Politik- und Organisation, danach: Politik- und Verwaltungswissenschaft) und den Masterstudiengang Governance umfasst.

Es ist auch möglich, an der Helmut-Schmidt-Universität/Universität der Bundeswehr Hamburg im Diplomstudiengang Politikwissenschaft zu studieren. Voraussetzung hierfür ist eine Verpflichtung in der Offizierslaufbahn zum Soldat auf Zeit für mindestens 13 Jahre. Unter bestimmten Umständen ist auch das Studium als Zivilist ohne eine Verpflichtung bei der Bundeswehr möglich. Eine Besonderheit ist die Organisation des Studienablaufs in Trimestern statt in Semestern. Man studiert somit im Jahr drei Trimester anstatt zweier Semester. Der Arbeits- und Lernaufwand für ein Trimester entspricht dabei dem eines Semesters. Dadurch sind weniger Studienjahre bis zum Abschluss erforderlich und der Diplomstudiengang kann schon nach drei Jahren abgeschlossen werden.

Neuerdings bieten vereinzelt auch Universitäten Bachelor- oder Masterstudiengänge in Kooperation an. Die TU Darmstadt kooperiert beispielsweise mit den Universitäten Mainz und Frankfurt am Main und bietet damit Studierenden der Politikwissenschaft neben dem Masterstudiengang in Darmstadt (Governance und Public Policy) zwei weitere Studiengänge im Rahmen der Politikwissenschaft in Kooperation an.

An den Universitäten, Hochschulen und später Fachhochschulen entstand nach dem Zweiten Weltkrieg ein großer Bedarf an Lehrpersonal, weswegen die Politikwissenschaft eine attraktive Karrierechance für viele politikwissenschaftlich interessierte Wissenschaftler aus den Nachbardisziplinen darstellte.

Heute wird eine wissenschaftliche Karriere an Universitäten oder bei Forschungseinrichtungen nur etwa von jedem fünften Studierenden der Politikwissenschaft angestrebt. Die erfolgreiche Einbindung in den wissenschaftlichen Arbeitsmarkt ist dabei von unterschiedlichen Faktoren wie dem Alter zum Zeitpunkt der Promotion, dem Engagement des Betreuers, der breiten fachlichen und thematischen Ausrichtung der Ausbildung und gesellschaftlichen Rahmenbedingungen wie beispielsweise einem Generationenwechsel auf der Ebene der Professuren oder gesellschaftlichen Diskursen über die Bedeutung der Politikwissenschaft und der damit einhergehenden staatlichen Förderung der politikwissenschaftlichen Lehre und Forschung abhängig. In diesem Sinne unterliegt auch der politikwissenschaftliche Arbeitsmarkt gewissen Konjunkturen und weist somit momentan einen hohen Konkurrenzdruck auf. Frauen sind von diesen Aspekten auf eine sehr spezifische Art und Weise betroffen. Die Zahl der bei Parteien, Parlamenten, Verbänden oder Nichtregierungsorganisationen tatsächlich im politischen Sektor beschäftigten Politikwissenschaftlern liegt mit ca. 15 % nur unwesentlich unter der Zahl für die Wissenschaft.

Ein großer Anteil von Studienabsolventen des Faches Politikwissenschaft ist in unterschiedlichen Bereichen der Medien beschäftigt. Rund ein Fünftel ist in der freien Wirtschaft (insbesondere in den Bereichen Consulting und Public Relations) tätig, lediglich ein Zehntel in der öffentlichen Verwaltung. In diesem Bereich sehen sich Politologen in Deutschland ebenso wie Vertreter anderer staatswissenschaftlicher Disziplinen wie Verwaltungswissenschaftlern, Soziologen und Volkswirten durch das faktische „Juristenmonopol“ im höheren Dienst der öffentlichen Verwaltung in ihren Karrierechancen beschränkt.

Mehrere Fachverbände und wissenschaftliche Gesellschaften widmen sich der Förderung des Faches und der Vertretung seiner Anliegen in der Öffentlichkeit oder der Intensivierung der interuniversitären Zusammenarbeit:


Ferner existiert mit der "International Political Science Association" (IPSA) auch ein internationaler Fachverband für Politikwissenschaftler.

Mehrere fachspezifische Bibliografien und bibliografische Datenbanken verzeichnen politikwissenschaftliche Veröffentlichungen und helfen bei der systematischen Erschließung relevanter Literatur:
















</doc>
<doc id="4001" url="https://de.wikipedia.org/wiki?curid=4001" title="Preußen">
Preußen

Preußen () war ein seit dem Spätmittelalter bestehendes Land an der Ostsee, zwischen Pommern, Polen und Litauen, dessen Name nach 1701 auf ein weit größeres, aus Brandenburg-Preußen hervorgegangenes Staatswesen angewandt wurde, das schließlich fast ganz Deutschland nördlich der Mainlinie einschloss und bis zum Ende des Zweiten Weltkrieges bestand.

Ursprünglich bezeichnete der Name „Preußen“ nur den Kern des Deutschordensstaats im einstigen Stammesgebiet der Prußen und die aus ihm hervorgegangenen Herrschaftsgebiete außerhalb des Heiligen Römischen Reiches. Nachdem der hohenzollernsche Kurfürst von Brandenburg als „Herzog in Preußen“ 1701 den Titel „König in Preußen“ angenommen hatte, bürgerte sich für alle Besitzungen seines Hauses inner- und außerhalb des Reichs die Gesamtbezeichnung Königreich Preußen ein.

Preußen, seit 1815 ein Gliedstaat des Deutschen Bundes, wurde 1866 zunächst zur Vormacht des Norddeutschen Bundes und 1871 zu der des Deutschen Reiches. Im Jahr 1918 entstand aus ihm der republikanische Freistaat Preußen, der infolge des Preußenschlags von 1932 und der Gleichschaltung der Länder in der Zeit des Nationalsozialismus seine Autonomie verlor. Im Jahr 1947 erklärte der Alliierte Kontrollrat Preußen auch formell für aufgelöst. Die Hauptstadt des Herzogtums und späteren Königreichs Preußen war Königsberg, die des Gesamtstaates dagegen Berlin.
Die ursprüngliche "historische Landschaft Preußen", benannt nach ihren baltischen Ureinwohnern, den Prußen, entsprach in etwa dem späteren Ostpreußen. Nachdem der Deutsche Orden das Preußenland unterworfen hatte, für das er aufgrund der päpstlichen Bulle von Rieti (1234) keinem weltlichen Lehensherren unterstand, bildete es zusammen mit Pommerellen das Zentrum des "Deutschordensstaates". Dessen Gebiet wurde 1466 im Zweiten Frieden von Thorn geteilt: in das der polnischen Krone direkt unterstehende "Königliche Preußen", das Pommerellen einschloss, und in den "Restordensstaat", der die polnische Lehenshoheit anerkennen musste. Durch dessen Säkularisierung entstand 1525 das weltliche "Herzogtum Preußen", das 1618 durch Erbschaft an die Kurfürsten von Brandenburg fiel. Diese regierten nun beide Länder in Personalunion.

Kurfürst Friedrich Wilhelm konnte das Herzogtum 1657 aus der polnischen Lehensabhängigkeit lösen. Da es außerhalb der Reichsgrenzen lag, war er dort nunmehr ein souveräner Herrscher. Dies nutzte sein Sohn Kurfürst Friedrich III., um sich 1701 als Friedrich I. zum "König in Preußen" zu krönen. Zentrum des hohenzollernschen Herrschaftsgebiets blieb nach wie vor die Mark Brandenburg. In den von Friedrich II. ausgelösten Schlesischen Kriegen stieg der nun als "Königreich Preußen" bezeichnete Staat zur zweiten deutschen und fünften europäischen Großmacht auf. Nach der Niederlage gegen das napoleonische Frankreich verlor Preußen 1806 große Teile seines Staatsgebiets, errang aber schon wenige Jahre später infolge der Stein-Hardenberg’schen Reformen und der siegreichen Teilnahme an den Befreiungskriegen mehr Macht und Ansehen als zuvor. Der Wiener Kongress brachte Preußen 1815 erhebliche territoriale Zugewinne, vor allem im Westen Deutschlands. Im neugegründeten Deutschen Bund war es die bedeutendste Macht nach Österreich. Der Sieg Preußens im Deutschen Krieg 1866 führte zur Auflösung des Bundes und zum Ausschluss Österreichs aus Deutschland. Stattdessen bildete Preußen mit den deutschen Staaten nördlich der Mainlinie den Norddeutschen Bund. Im Deutsch-Französischen Krieg von 1870/71 vereinte es alle übrigen deutschen Staaten mit Ausnahme Luxemburgs unter seiner Führung und wurde 1871 zum dominierenden Bundesstaat des neu gegründeten Deutschen Reiches. Als dessen Oberhaupt trug der König von Preußen den zusätzlichen Titel "Deutscher Kaiser".

Nach dem Sturz der Monarchie in der Novemberrevolution von 1918 wurde aus dem Königreich der republikanisch verfasste "Freistaat Preußen", der während der Weimarer Republik ein Bollwerk der Demokratie war. Im sogenannten Preußenschlag wurde seine Landesregierung jedoch 1932 von der Reichsregierung entmachtet. Die preußischen Minister wurden durch Reichskommissare ersetzt und ihre Ministerien 1934 im Zuge der nationalsozialistischen Gleichschaltungspolitik mit den entsprechenden Ressorts des Reichs verschmolzen. Mit dem Kontrollratsgesetz Nr. 46 vom 25. Februar 1947 verfügte der Alliierte Kontrollrat der vier Besatzungsmächte in Deutschland die rechtliche Auflösung Preußens. De facto hatte es bereits mit dem Kriegsende 1945 aufgehört, als Staat zu bestehen. Sowohl die Deutsche Demokratische Republik als auch die Bundesrepublik Deutschland und viele ihrer Länder haben preußische Traditionen weitergeführt. Die Gebiete, die bis 1918 – also zum Zeitpunkt seiner größten Ausdehnung – den preußischen Staat bildeten, gehören heute zu Deutschland und sechs weiteren Staaten: Belgien, Dänemark, Polen, Russland, Litauen und Tschechien.

Das spätere Königreich Preußen entwickelte sich im Wesentlichen aus zwei Landesteilen: aus der Markgrafschaft Brandenburg, die zu den sieben Kurfürstentümern des Heiligen Römischen Reiches gehörte, sowie aus dem Herzogtum Preußen, das wiederum aus dem Staat des Deutschen Ordens hervorgegangen war.

Nach mehreren vergeblichen Versuchen, die Stammesgebiete der heidnischen Prußen zu erobern, rief der polnische Herzog Konrad von Masowien im Jahr 1209 den Deutschen Orden zu Hilfe und war bereit, ihm Landrechte in den zu erobernden Gebieten einzuräumen. Diese Pläne nahmen Gestalt an, nachdem 1226 Kaiser Friedrich II. den Großmeister des Ordens, Hermann von Salza, in der Goldenen Bulle von Rimini mit der so genannten „Heidenmission“ im Preußenland betraut hatte. 1234 wurden die Rechte des Ordens auch vom Papst bestätigt. Mit dem Jahr 1226 begann die Herausbildung des Ordensstaates in Preußen, der zwar mit dem Heiligen Römischen Reich in Verbindung stand, jedoch kein Teil von ihm war.

Nachdem die gewaltsame Christianisierung der Prußen und die Eroberung ihres Landes abgeschlossen waren, gerieten die Ordensritter zunehmend in eine Legitimationskrise. Dazu kamen Konflikte mit den Nachbarländern Polen und Litauen. In der Schlacht von Tannenberg erlitten die Ordensritter 1410 schließlich eine entscheidende Niederlage gegen Polen und Litauen. 1466 im Zweiten Frieden von Thorn musste der Ordensstaat den Westen seines Gebietes abtreten und für den Rest die Lehnshoheit der polnischen Krone anerkennen. Westpreußen und das Ermland unterstanden fortan als Königliches Preußen direkt der polnischen Krone.

Das verbliebene Gebiet des Ordensstaates umfasste in etwa das spätere Ostpreußen ohne das Ermland. Der Hochmeister des Deutschen Ordens, Albrecht von Brandenburg-Ansbach führte zunächst Krieg gegen Polen, besonders gegen das königliche Preußen mit dem Ermland. Als die erhoffte Unterstützung aus dem Reich ausblieb, änderte er seine Politik: Auf Anraten Martin Luthers wandelte er das Ordensgebiet in ein weltliches, im Haus Hohenzollern erbliches Herzogtum um, führte die Reformation ein und nahm es am 8. April 1525 aus der Hand des polnischen Königs Sigismund I. in Krakau zu Lehen. Wie der Herzog, so wurden auch seine Untertanen evangelisch.

Da Papst und Kaiser weder den zweiten Thorner Frieden noch die Säkularisierung des Ordensstaates anerkannten, galten auf den Reichstagen noch längere Zeit die Hochmeister des Deutschen Ordens formell als Landesherren der preußischen Gebiete.

Die eigentliche Keimzelle des späteren hohenzollernschen Staates Preußen war die Mark Brandenburg. Sie war 1157 von dem Askanier Albrecht I. gegründet worden, nachdem er das von Slawen besiedelte Territorium endgültig erobert hatte. Albrecht verstand das Gebiet als Allodialbesitz und bezeichnete sich seither als „Markgraf in Brandenburg“. Nach dem Tod des letzten askanischen Markgrafen Waldemar im Jahr 1320 fiel das Land zunächst an die Wittelsbacher, 1373 dann an die Luxemburger.

Dass Brandenburg schließlich an das damals noch vergleichsweise unbedeutende Haus Hohenzollern fiel, hatte seine Ursache in der strittigen Königswahl des Jahres 1410. Nach dem Tod König Ruprechts stellten sich Sigismund von Luxemburg und sein Cousin Jobst von Mähren zur Wahl. Zudem beanspruchten beide den Titel und die Stimme eines Kurfürsten von Brandenburg für sich. Sigismund entsandte seinen Schwager, den Burggrafen Friedrich VI. von Nürnberg als seinen Vertreter ins Kurkollegium, um dort die brandenburgische Stimme für ihn abzugeben. So setzte er sich zunächst mit 4:3 Kurstimmen gegen seinen als Favoriten geltenden Vetter durch. Am 1. Oktober 1410 aber erkannten die übrigen Kurfürsten Jobsts Anspruch auf die Kurmark doch noch als rechtmäßig an, sodass nun er zum römisch-deutschen König gewählt wurde. Allerdings starb Jobst von Mähren schon am 18. Januar 1411 aus ungeklärter Ursache. Die Krone ging nun endgültig an Sigismund. Zum Dank für Friedrichs Dienste bei der ersten Wahl und um seine Schulden bei ihm zu begleichen, verlieh König Sigismund dem Hohenzollern 1415 die erbliche Würde eines Markgrafen und Kurfürsten von Brandenburg. 1417 belehnte er ihn förmlich mit der Kurmark und dem Amt des Erzkämmerers. Im Gegenzug gewährte der wohlhabende Friedrich seinem Schwager Darlehen, mit denen dieser seine Kriegskosten in Ungarn decken konnte.

Friedrich entstammte der fränkischen Linie der Hohenzollern und war seit 1397 Burggraf in Nürnberg. In den Jahren nach 1411 sicherte er in jahrelangen Kämpfen gegen den widerstrebenden märkischen Adel seine Vormachtstellung im Land. Als Friedrich I. von Brandenburg vereinte er von nun an die Titel Kurfürst von Brandenburg, Markgraf von Brandenburg-Ansbach und Markgraf von Brandenburg-Kulmbach. Er begründete die brandenburgische Linie seines Hauses, die später alle Könige Preußens und von 1871 bis 1918 die deutschen Kaiser stellen sollte.

Im Jahr 1618 erlosch die herzoglich-preußische Linie des Hauses Hohenzollern im Mannesstamm. Ihre Erben, die Markgrafen und Kurfürsten von Brandenburg, regierten von da an beide Länder in Personalunion. Sie waren damit sowohl dem Kaiser als auch dem König von Polen lehenspflichtig. Die Bezeichnung Brandenburg-Preußen für die weit auseinander gelegenen hohenzollernschen Herrschaftsgebiete ist nicht zeitgenössisch, sondern hat sich in der Geschichtswissenschaft eingebürgert, um die Übergangszeit von 1618 bis zur Gründung des Königreichs Preußen im Jahre 1701 zu bezeichnen und zugleich die Kontinuität zwischen dem Kurfürstentum Brandenburg und dem Königreich Preußen zu betonen.

Wenige Jahre vor dem Dreißigjährigen Krieg hatte sich Brandenburg im Jülich-Klevischen Erbfolgestreit auch die Herrschaft über das Herzogtum Kleve sowie die Grafschaften Mark und Ravensberg im Westen des Reichs sichern können. Vom Krieg selbst blieb das Land zunächst verschont. Im Jahr 1625 brach jedoch der Dänisch-niedersächsische Krieg aus, in dem einige protestantische Staaten Norddeutschlands, angeführt von Dänemark und unterstützt von England und den Generalstaaten der Katholischen Liga und dem Kaiser entgegentraten. Nach der Niederlage der dänischen Armee bei Dessau im April 1626 drangen kaiserliche Truppen in die Mark ein. Kurfürst Georg Wilhelm, der über keine nennenswerten Streitkräfte verfügte, zog sich in das außerhalb des Reiches gelegene Herzogtum Preußen zurück und schloss 1627 gezwungenermaßen ein Bündnis mit dem Kaiser. Brandenburg diente fortan den kaiserlichen Truppen als Aufmarsch- und Rückzugsgebiet.

Am 6. Juli 1630 landete der Schwedenkönig Gustav Adolf mit 13.000 Mann auf Usedom. Damit begann ein neuer Abschnitt des Dreißigjährigen Kriegs. Als Gustav Adolf im Frühjahr 1631 in Brandenburg einzog, nötigte er den Kurfürsten, seinen Schwiegervater, zu einem Bündnis. Nachdem die schwedischen Truppen in der Schlacht bei Nördlingen am 6. September 1634 vernichtend geschlagen wurden, brach die protestantische Allianz auseinander. Brandenburg ging ein neues Bündnis mit dem Kaiser ein. Die Kurmark wurde nun abwechselnd von Gegnern und Verbündeten besetzt. Der Kurfürst zog sich erneut ins preußische Königsberg zurück, wo er am 1. Dezember 1640 starb.

Neuer Kurfürst wurde sein Sohn Friedrich Wilhelm. Primäres Ziel seiner Politik war, das Land zu befrieden. Dies versuchte er durch einen Ausgleich mit Schweden zu erreichen, der ab dem 24. Juli 1641 für zwei Jahre galt. In Verhandlungen mit dem schwedischen Reichskanzler Axel Oxenstierna gelang es den Brandenburgern am 28. Mai 1643, einen Vertrag auszuhandeln, der das ganze Land formell der kurfürstlichen Verwaltung zurückgab. Bis zum Westfälischen Frieden 1648 blieb Brandenburg jedoch von den Schweden besetzt. Im Westfälischen Frieden konnte Brandenburg-Preußen dann Hinterpommern, das Hochstift Halberstadt und das Fürstentum Minden erwerben sowie die Anwartschaft auf das Erzstift Magdeburg, das 1680 anfiel. Die Gebietsgewinne machten zusammen etwa 20.000 km² aus.

Brandenburg war eines der am stärksten vom Dreißigjährigen Krieg betroffenen deutschen Territorien. Weite Landstriche waren verwüstet und entvölkert. Um es dem Land in Zukunft zu ersparen, Spielball mächtigerer Nachbarn zu sein, betrieb Friedrich Wilhelm, später der "Große Kurfürst" genannt, nach dem Krieg eine vorsichtige Schaukelpolitik zwischen den Großmächten sowie den Aufbau einer schlagkräftigen Armee und einer effizienten Verwaltung. Er baute ein stehendes Heer auf, das Brandenburg zu einem begehrten Verbündeten der europäischen Mächte machte. Dies ermöglichte es dem Kurfürsten, Subsidienzahlungen von mehreren Seiten zu erhalten. Er betrieb den Aufbau einer eigenen kurbrandenburgischen Marine und verfolgte in späteren Jahren Kolonialprojekte in Westafrika und Westindien. Nach der Gründung der Festung Groß Friedrichsburg durch die Brandenburgisch-Afrikanische Compagnie im heutigen Ghana nahm Brandenburg am internationalen Sklavenhandel teil.

Im Inneren führte Friedrich Wilhelm Wirtschaftsreformen durch und initiierte umfangreiche Peuplierungsmaßnahmen, um sein ökonomisch geschwächtes Land zu entwickeln. Unter anderem lud er 1685 im Edikt von Potsdam – seiner Antwort auf das Edikt von Fontainebleau König Ludwigs XIV. – Tausende aus Frankreich vertriebene Hugenotten zur Niederlassung in Brandenburg-Preußen ein. Gleichzeitig entmachtete er die Stände zugunsten einer absolutistischen Zentralverwaltung. Er legte damit den Grundstein für das preußische Beamtentum, das seit dem 18. Jahrhundert den Ruf besonderer Effizienz und Staatstreue genoss.

Dem Kurfürsten gelang es im Jahre 1657 im Vertrag von Wehlau, das Herzogtum Preußen aus der polnischen Oberhoheit zu lösen. Im Frieden von Oliva von 1660 wurde die Souveränität des Herzogtums endgültig anerkannt. Dies war eine entscheidende Voraussetzung für seine Erhebung zum Königreich unter dem Sohn des Großen Kurfürsten. Durch den Sieg im Schwedisch-Brandenburgischen Krieg (1674–1679) konnte das Land seine Machtstellung trotz ausbleibender Landgewinne weiter ausbauen. Friedrich Wilhelm hatte in seiner Amtszeit das zuvor vergleichsweise unbedeutende Brandenburg zum nach Österreich zweitmächtigsten Territorium im Reich gemacht. Damit war der Grundstein für das spätere Königreich gelegt.

Auf Betreiben Friedrich Wilhelms und seiner oranischen Gemahlin Luise Henriette trugen bedeutende niederländische Gelehrte, insbesondere von der Universität Leiden, zur Modernisierung des brandenburg-preußischen Staates bei. 

Rang, Reputation und Prestige eines Fürsten waren in der Zeit des Absolutismus wichtige politische Faktoren. Kurfürst Friedrich III. nutzte daher die Souveränität des Herzogtums Preußen dazu, dessen Erhebung zum Königreich und seine eigene zum König anzustreben. Damit versuchte er vor allem, die Ranggleichheit mit dem Kurfürsten von Sachsen, der zugleich König von Polen war, und mit dem Kurfürsten von Braunschweig-Lüneburg („Kurhannover“), der Anwärter auf den englischen Thron war, zu wahren.

Da es innerhalb des Heiligen Römischen Reiches keine Krone außer der des Kaisers geben konnte, strebte Kurfürst Friedrich III. die Königswürde für das Herzogtum Preußen an und nicht für den eigentlich wichtigeren Landesteil, die Mark Brandenburg. Kaiser Leopold I. stimmte schließlich zu, dass Friedrich für das nicht zum Reich gehörende Herzogtum Preußen den Königstitel erhalten solle. Kurfürst Friedrich III. krönte sich selbst am 18. Januar 1701 in Königsberg und wurde zu Friedrich I., "König in Preußen".

Die einschränkende Titulatur „"in" Preußen“ war notwendig, weil die Bezeichnung „König "von" Preußen“ als Herrschaftsanspruch auf das gesamte preußische Gebiet verstanden worden wäre. Da Ermland und westliches Preußen (Pommerellen) damals aber noch unter der Oberhoheit der polnischen Krone waren, hätte dies Konflikte mit dem Nachbarland heraufbeschworen, dessen Herrscher noch bis 1742 den Titel eines „Königs von Preußen“ beanspruchten. Seit 1701 bürgerte sich aber im allgemeinen deutschen Sprachgebrauch allmählich die Landesbezeichnung "Königreich Preußen" für alle von den Hohenzollern regierten Gebiete ein – ob innerhalb oder außerhalb des Heiligen Römischen Reiches gelegen. Zentren des Hohenzollernstaates blieben die Hauptstadt Berlin und die Sommerresidenz Potsdam. Alle Königskrönungen fanden jedoch traditionsgemäß in Königsberg statt.

Friedrich I. überließ das politische Geschäft weitgehend dem sogenannten Drei-Grafen-Kabinett. Er selbst konzentrierte sich auf eine aufwändige Hofhaltung nach französischem Vorbild, die seinen Staat an den Rand des finanziellen Ruins brachte. Er finanzierte den Prunk am Hof u. a. dadurch, dass er preußische Soldaten an die Allianz im Spanischen Erbfolgekrieg vermietete. Als Friedrich I. am 25. Februar 1713 starb, hinterließ er einen Schuldenberg von zwanzig Millionen Talern.

Der Sohn Friedrichs I., Friedrich Wilhelm I., war nicht prunkliebend wie sein Vater, sondern sparsam und praktisch veranlagt. Folglich kürzte er die Ausgaben für die Hofhaltung auf ein Minimum. Alles, was dem höfischen Luxus diente, wurde entweder abgeschafft oder anderen Nutzungen zugeführt. Alle Sparmaßnahmen des Königs zielten auf den Ausbau eines starken stehenden Heeres, in dem der König die Grundlage seiner Macht nach innen und außen sah. Diese Haltung brachte ihm den Beinamen „Soldatenkönig“ ein. Gleichwohl führte Friedrich Wilhelm I. nur einmal in seiner Amtszeit einen kurzen Feldzug im Großen Nordischen Krieg während der Belagerung Stralsunds. In dessen Folge gewann Preußen nicht nur einen Teil Vorpommerns, sondern dank des prestigeträchtigen Siegs über die Schweden auch an internationalem Ansehen.

Friedrich Wilhelm I. revolutionierte die Verwaltung unter anderem mit der Gründung des Generaldirektoriums. Damit zentralisierte er das Land, das bisher noch immer territorial zersplittert war, und gab ihm eine einheitliche staatliche Organisation. Durch eine merkantilistische Wirtschaftspolitik, die Förderung von Handel und Gewerbe sowie eine Steuerreform gelang es dem König, die jährlichen Staatseinnahmen zu verdoppeln. Um die nötigen Fachkräfte zu gewinnen, führte er die allgemeine Schulpflicht ein und errichtete volkswirtschaftliche Lehrstühle an preußischen Universitäten, die ersten ihrer Art in Europa. Im Zuge einer intensiven Peuplierungspolitik ließ er Menschen aus ganz Europa in seinen dünnbesiedelten Provinzen ansiedeln.

Als Friedrich Wilhelm I. 1740 starb, hinterließ er ein wirtschaftlich und finanziell gefestigtes Land. Mit ihm begann jedoch auch die Militarisierung Preußens, wenngleich deren Umfang und Auswirkungen umstritten sind.

Am 31. Mai 1740 bestieg sein Sohn Friedrich II. – später "Friedrich der Große" genannt – den Thron. Noch in seinem ersten Regierungsjahr ließ er die preußische Armee in das zu Österreich gehörende Schlesien einmarschieren, auf das er Anspruch erhob. Damit begann der preußisch-österreichische Dualismus, der Kampf der beiden führenden deutschen Mächte um die Vorherrschaft im Reich.

In den drei Schlesischen Kriegen (1740–1763) gelang es, die neu gewonnene Provinz für Preußen zu sichern. Im dritten, dem Siebenjährigen Krieg (1756–1763), stand das mit Großbritannien verbündete Preußen einer Koalition aus Österreich, Frankreich, Russland und Sachsen gegenüber und geriet trotz großer militärischer Erfolge an den Rand des Zusammenbruchs. Vor der Niederlage wurde es nur durch das „Mirakel des Hauses Brandenburg“, das Versäumnis von Österreich und Russland, nach Friedrichs vernichtender Niederlage in der Schlacht bei Kunersdorf gemeinsam Berlin zu erobern, sowie durch den Tod der Zarin Elisabeth, bewahrt. Ihr Nachfolger, Zar Peter III., war ein Bewunderer Friedrichs und löste Russland aus der Allianz. Seine Gegner sahen sich dadurch gezwungen, sich mit Friedrich zu verständigen und gestanden ihm im Frieden von Hubertusburg den endgültigen Besitz Schlesiens zu. Preußen, dessen Armee nun als eine der besten Europas galt, war zur fünften Großmacht aufgestiegen.

Friedrich II. war ein Vertreter des aufgeklärten Absolutismus und verstand sich selbst als „ersten Diener des Staates“. So schaffte er die Folter ab, verminderte die Zensur, legte den Grundstein für das Allgemeine preußische Landrecht und holte mit der Gewährung völliger Glaubensfreiheit weitere Exulanten ins Land. Unter seiner Regierung wurde der Landesausbau ebenso vorangetrieben wie die Peuplierung von zuvor weitgehend unbesiedelten Gebieten, etwa des Oder- und des Netzebruchs.

Gemeinsam mit Österreich und Russland betrieb Friedrich die Teilung Polens. Bei der ersten Teilung 1772 erwarb er Polnisch Preußen, das in Westpreußen eingegliedert wurde, den Netzedistrikt und das Fürstbistum Ermland, das zu Ostpreußen kam. Damit waren die hohenzollernschen Territorien Pommern und Ostpreußen nicht mehr durch polnisches Staatsgebiet voneinander getrennt. Außerdem gehörten nun alle preußischen Gebiete zum Hohenzollernstaat, so dass sich Friedrich nun König „von Preußen“ nennen konnte. Er starb am 17. August 1786 auf Schloss Sanssouci.

Nach dem Tod Friedrichs II. bestieg sein Neffe Friedrich Wilhelm II. (1786–1797) den preußischen Thron. Berlin wuchs in den 1790er Jahren zu einer ansehnlichen, vom Klassizismus geprägten Stadt heran. Hier, wie im ganzen Reich, nahm das erstarkende Bildungsbürgertum die Französische Revolution meist positiv auf. Seit 1794 galt in Preußen das Allgemeine Landrecht, ein umfassendes Gesetzeswerk, dessen Ausarbeitung schon unter Friedrich II. begonnen hatte.

In der Außenpolitik zwang Preußen Österreich 1790 durch ein Bündnis mit dem Osmanischen Reich zu einem Separatfrieden im Russisch-Österreichischen Türkenkrieg. Friedrich Wilhelm setzte die Teilungspolitik gegenüber Polen fort, so dass sich Preußen in der zweiten und der dritten Teilung Polens (1793 und 1795) weitere Gebiete bis nach Warschau sichern konnte. Aus ihnen wurden die neuen Provinzen Südpreußen (1793), Neuostpreußen und Neuschlesien (beide 1795) gebildet. Die Bevölkerungszahl wuchs damit zunächst um 2,5 Millionen, jedoch gingen die Neuerwerbungen nach der Niederlage gegen Frankreich im Jahr 1806 wieder verloren.
Die französische Revolution bewirkte eine Annäherung zwischen Österreich und Preußen. Obwohl die preußische Regierung die Revolution zu Beginn noch wohlwollend betrachtet hatte, schloss sie am 7. Februar 1792 ein Verteidigungsbündnis mit Österreich. Wegen der Pillnitzer Deklaration zugunsten König Ludwigs XVI. erklärte Frankreich beiden Ländern am 20. April 1792 den Krieg. Im Ersten Koalitionskrieg folgte auf den anfänglichen raschen Vormarsch nach der Kanonade von Valmy der Rückzug der preußischen und österreichischen Truppen aus Frankreich. Anschließend stießen französische Revolutionstruppen bis zum Rhein vor. Nach dem Frieden von Basel 1795 schied Preußen für mehr als ein Jahrzehnt aus der antifranzösischen Allianz aus. Am 16. November 1797 starb Friedrich Wilhelm II. Ihm folgte sein Sohn Friedrich Wilhelm III. (1797–1840) auf den Thron.

Zwischen 1795 und 1806 profitierte Preußen von einer Außenpolitik, die Frankreich favorisierte. Mit dessen Unterstützung wurde es faktisch zur Vormacht Norddeutschlands. Im Reichsdeputationshauptschluss von 1803 erhielt das Land für Verluste auf dem linken Rheinufer einen Großteil des Hochstifts Münster, die Bistümer Hildesheim und Paderborn sowie weitere Gebieten als Entschädigung. Dadurch wuchs sein Territorium um etwa 3 und seine Bevölkerungszahl um rund 5 Prozent. Zudem besetzte Preußen kurzfristig das mit Großbritannien verbundene Kurfürstentum Hannover.

Als 1806 Verhandlungen mit Frankreich über die Aufteilung der Machtsphären in Deutschland scheiterten, kam es erneut zum Krieg. In der Schlacht bei Jena und Auerstedt erlitt Preußen eine vernichtende Niederlage gegen die Truppen Napoleons I., die den Untergang des bisherigen altpreußischen Staates bedeutete. Im Frieden von Tilsit verlor Preußen 1807 etwa die Hälfte seines Territoriums: alle Gebiete westlich der Elbe sowie die Zugewinne aus der zweiten und dritten polnischen Teilung. Zudem musste das Land eine französische Besatzung hinnehmen, die fremden Truppen versorgen und hohe Kontributionszahlungen an Frankreich leisten. Preußen verlor faktisch seine Großmachtstellung und war nach Größe und Funktion nur noch ein Pufferstaat zwischen Frankreich und Russland.

Die als untragbar empfundenen Bedingungen des Tilsiter Friedens bewirkten aber auch eine Erneuerung des Staates. Die grundlegenden Reformen, die nach 1807 in Angriff genommen wurden, zielten innenpolitisch darauf ab, die Zustände, die zur Niederlage von 1806 geführt hatten, zu verändern und außenpolitisch darauf, die französische Hegemonie abzuschütteln. Mit den Stein-Hardenbergschen Reformen unter Leitung von Freiherr vom Stein, Scharnhorst und Hardenberg wurde das Staatswesen modernisiert. So wurde 1807 die Leibeigenschaft der Bauern aufgehoben, 1808 die kommunale Selbstverwaltung eingeführt und 1810 die Gewerbefreiheit gewährt. Der aus Rom zurückberufene Gesandte Wilhelm von Humboldt gestaltete das Bildungswesen neu und gründete 1809 die erste Berliner Universität, die heute seinen Namen trägt. Die Heeresreform wurde 1813 mit der Einführung der allgemeinen Wehrpflicht abgeschlossen.

An Napoleons Russlandfeldzug von 1812 nahm Preußen als Verbündeter Frankreichs teil. Nach der Niederlage der „Grande Armee“ schloss der preußische Generalleutnant Graf Yorck jedoch bereits am 30. Dezember 1812 die Konvention von Tauroggen mit dem General der russischen Armee Hans von Diebitsch. Sie sah einen Waffenstillstand vor und besagte, dass Yorck seine preußischen Truppen aus der Allianz mit der französischen Armee herauslösen sollte. Yorck handelte aus eigener Initiative, ohne Befehl seines Königs, der noch mehrere Monate zwischen der erzwungenen Bündnistreue zu Frankreich und einer russlandfreundlichen Politik schwankte. Die Konvention von Tauroggen wurde in Preußen als Beginn des Aufstandes gegen die französische Fremdherrschaft verstanden. Schließlich rang sich auch Friedrich Wilhelm zu einem Politikwechsel durch. Als er am 20. März 1813 in der "Schlesischen privilegierten Zeitung" mit seinem Aufruf „An Mein Volk“, der auf den 17. März datiert war, zum Befreiungskampf aufrief, standen 300.000 preußische Soldaten (6 % der Gesamtbevölkerung) bereit. Für die Dauer des bevorstehenden Krieges wurde die Allgemeine Wehrpflicht eingeführt. Preußische Truppen unter Blücher und Gneisenau trugen in der Völkerschlacht bei Leipzig 1813, beim Vorstoß der Alliierten nach Paris in der Frühjahrskampagne 1814 und in der Schlacht bei Waterloo 1815 entscheidend zum Sieg über Napoleon bei.

Auf dem Wiener Kongress 1815 erhielt Preußen den Großteil seines seit 1807 bestehenden Staatsgebietes zurück. Neu hinzu kamen der Rest Schwedisch-Vorpommerns und der nördliche Teil des Königreichs Sachsen. Außerdem gewann Preußen beträchtliche Gebiete im Westen hinzu, die es unter Vereinigung mit früherem westlichem Staatsgebiet bald zur Provinz Westfalen und zur Rheinprovinz zusammenfasste. In den neuen Provinzen im Westen entstanden in Koblenz, Köln und Minden mächtige Festungen, gebaut nach neupreußischer Befestigungsmanier, zur Sicherung der preußischen Vormachtstellung. Preußen erhielt zwar die früher polnische, 1807 zum Herzogtum Warschau gekommene Provinz Posen zurück, verlor jedoch Gebiete der zweiten und der dritten polnischen Teilung an Russland. Der preußische Staat bestand seitdem aus zwei großen, aber räumlich getrennten Länderblöcken in Ost- und Westdeutschland. Preußen wurde Mitglied des Deutschen Bundes.

Das während der Freiheitskriege seinem Volk gegebene Versprechen, dem Land eine Verfassung zu geben, löste Friedrich Wilhelm III. nicht ein. Anders als in den meisten übrigen deutschen Staaten wurde in Preußen auch keine Volksvertretung für den Gesamtstaat geschaffen. Statt eines Landtages für ganz Preußen wurden lediglich Provinziallandtage einberufen. Das Gesetz vom 5. Juni 1823 gewährte ihnen ein Mitspracherecht. Es herrschten daher Zustände wie in einem Ständestaat, weil außer dem einflussreichen Adel in den Provinzen die Städte eine Selbstverwaltung besaßen, wenn auch eine gewisse Staatsaufsicht bestand.

Die königliche Regierung glaubte so, liberale Bestrebungen nach einer konstitutionellen Monarchie und demokratischen Mitwirkungsrechten verhindern zu können. Dem Ziel, die Demokratiebestrebungen in ganz Europa zu unterdrücken, diente auf außenpolitischer Ebene die Heilige Allianz, die Friedrich Wilhelm III. gemeinsam mit dem Zaren des Russischen Reiches und dem Kaiser von Österreich ins Leben rief.

Dem Bestreben der königlichen Regierung, Liberalismus, Demokratie und die Idee der Einigung Deutschlands zu bekämpfen, standen jedoch starke ökonomische Zwänge entgegen. Aufgrund der Zweiteilung seines Staatsgebiets lag die wirtschaftliche Einigung Deutschlands nach 1815 in Preußens ureigenem Interesse. Das Königreich gehörte daher zu den treibenden Kräften des Deutschen Zollvereins, dessen Mitglied es 1834 wurde.

Durch den Erfolg des Zollvereins setzten immer mehr Befürworter der deutschen Einigung ihre Hoffnungen darauf, dass Preußen Österreich als Führungsmacht des Bundes ablösen werde. Die preußische Regierung jedoch wollte sich nicht für die politische Einigung Deutschlands engagieren.

Die Hoffnungen, die der Regierungsantritt Friedrich Wilhelms IV. (1840–1861) bei Liberalen und Anhängern der deutschen Einigung zunächst geweckt hatte, wurden bald enttäuscht. Auch der neue König machte aus seiner Abneigung gegen eine Verfassung und einen gesamtpreußischen Landtag keinen Hehl.

Der große Finanzbedarf für den Bau der von den Militärs geforderten Ostbahn erforderte jedoch die Bewilligung von Etatmitteln aller Provinzen. Deswegen wurde im Frühjahr 1847 schließlich der Vereinigte Landtag einberufen.
Der König machte schon in seiner Eröffnungsrede unmissverständlich deutlich, dass er den Landtag nur als Instrument der Geldbewilligung ansah und dass er keine Verfassungsfragen erörtert sehen wollte. Da die Mehrheit des Landtags aber von Beginn an nicht nur das Etatbewilligungsrecht, sondern auch eine parlamentarische Kontrolle der Staatsfinanzen und eine Verfassung forderte, wurde das Gremium schon nach kurzer Zeit wieder aufgelöst. Preußen stand damit schon vor dem Ausbruch der Märzrevolution vor einem Verfassungskonflikt.

Nach den Volkserhebungen in Südwestdeutschland erreichte die Revolution am 18. März 1848 schließlich auch Berlin. Friedrich Wilhelm IV., der zunächst noch auf die Aufständischen hatte schießen lassen, ließ die Truppen aus der Stadt zurückziehen und schien sich nun den Forderungen der Revolutionäre zu beugen. Der Vereinigte Landtag trat noch einmal zusammen, um die Einberufung einer preußischen Nationalversammlung zu beschließen, die vom 22. Mai bis in den September 1848 in der Sing-Akademie zu Berlin tagte.

Der preußischen Nationalversammlung war von der Krone die Aufgabe zugedacht worden, mit ihr gemeinsam eine Verfassung auszuarbeiten. Die Nationalversammlung stimmte dem Regierungsentwurf für eine Verfassung jedoch nicht zu, sondern arbeitete mit der "Charte Waldeck" einen eigenen Entwurf aus. Auch durch die Verfassungspolitik der preußischen Nationalversammlung kam es zur Gegenrevolution: der Auflösung der Versammlung und zur Einführung einer oktroyierten (verordneten) Verfassung seitens der Staatsspitze. Diese oktroyierte Verfassung behielt zwar einige Punkte der Charte bei, stellte aber andererseits zentrale Vorrechte der Krone wieder her. Vor allem das eingeführte Dreiklassenwahlrecht prägte die politische Kultur Preußens bis 1918 entscheidend.

In der Frankfurter Nationalversammlung setzten sich zunächst die Befürworter eines großdeutschen Nationalstaats durch, die ein Reich unter Einschluss der deutschsprachigen Teile Österreichs vorsahen. Da Österreich aber nur unter Einbeziehung aller seiner Landesteile einer Reichseinigung zustimmen wollte, wurde schließlich doch die sogenannte kleindeutsche Lösung beschlossen, d. h. eine Einigung unter Preußens Führung. Demokratie und deutsche Einheit scheiterten aber 1849, als Friedrich Wilhelm IV. die Kaiserkrone ablehnte, die ihm die Nationalversammlung angetragen hatte. Die Revolution wurde in Südwestdeutschland mit Hilfe preußischer Truppen endgültig niedergeschlagen.

Noch während der Niederschlagung der Revolution unternahm Preußen einen erneuten Einigungsversuch, allerdings mit einem konservativeren Verfassungsentwurf und einer engeren Zusammenarbeit mit den Mittelstaaten. Unterdessen versuchte Österreich, ein Großösterreich durchzusetzen. Nachdem die politisch-diplomatische Auseinandersetzung zwischen den beiden deutschen Großmächten in der Herbstkrise 1850 beinahe zum Krieg geführt hatte, gab Preußen seine Erfurter Union endgültig auf. Der Deutsche Bund wurde fast unverändert wiederhergestellt.

Während der Reaktionsära arbeiteten Preußen und Österreich wieder eng zusammen, um demokratische und nationale Bewegungen niederzuhalten; die Gleichberechtigung wurde Preußen aber verweigert. König Wilhelm I. bestieg 1861 den preußischen Thron. Mit Kriegsminister Roon strebte er eine Heeresreform an, die längere Dienstzeiten und eine Aufrüstung der preußischen Armee vorsah. Die liberale Mehrheit des Preußischen Landtags, dem das Budgetrecht zustand, wollte die dafür nötigen Gelder jedoch nicht bewilligen. Es kam zu einem Verfassungskonflikt, in dessen Verlauf der König seine Abdankung in Erwägung zog.

Als letzten Ausweg entschloss sich Wilhelm 1862, Otto von Bismarck als Ministerpräsidenten zu berufen. Dieser war ein vehementer Befürworter des königlichen Alleinherrschaftsanspruchs und regierte jahrelang in der Konfliktsperiode gegen Verfassung und Parlament und ohne gesetzlichen Haushalt. Aus der Erkenntnis heraus, dass die preußische Krone nur dann Rückhalt im Volk gewinnen könne, wenn sie sich an die Spitze der deutschen Einigungsbewegung setzte, verfolgte Bismarck eine offensive Politik, die zu den drei Einigungskriegen führte.

Mit der sogenannten Novemberverfassung von 1863 versuchte die dänische Regierung – entgegen den Bestimmungen des Londoner Protokolls von 1852 – das Herzogtum Schleswig unter Ausschluss Holsteins stärker an das eigentliche Königreich Dänemark zu binden. Dies löste 1864 den Deutsch-Dänischen Krieg aus, den Preußen und Österreich im Namen des Deutschen Bundes gemeinsam führten. Nach dem Sieg der Truppen des Deutschen Bundes musste die dänische Krone im Frieden von Wien auf die Herzogtümer Schleswig, Holstein und Lauenburg verzichten. Die Herzogtümer wurden von Preußen und Österreich zunächst gemeinsam verwaltet.

Bald nach Ende des Krieges mit Dänemark brach zwischen Österreich und Preußen Streit um die Verwaltung und die Zukunft Schleswig-Holsteins aus. Dessen tiefere Ursache war jedoch das Ringen um die Vorherrschaft im Deutschen Bund. Es gelang Bismarck, den aus Gründen der Loyalität gegenüber Österreich lange zögerlichen König Wilhelm zu einer kriegerischen Lösung zu überreden. Auf Seiten Preußens trat neben einigen norddeutschen und thüringischen Kleinstaaten auch das Königreich Italien in den Krieg ein (→ Schlacht bei Custozza und Seeschlacht von Lissa).

Im Deutschen Krieg errang Preußens Armee unter General Helmuth von Moltke am 3. Juli 1866 in der Schlacht von Königgrätz den entscheidenden Sieg. Im Prager Frieden vom 23. August 1866 konnte Preußen seine Forderungen durchsetzen: Österreich musste die Auflösung des Deutschen Bundes anerkennen, auf eine Beteiligung an der „neuen Gestaltung Deutschlands“ verzichten und das „engere Bundesverhältnis“ anerkennen, das Preußen mit den deutschen Staaten nördlich der Mainlinie einging. Während Preußen sich mehrere Mitgliedstaaten des aufgelösten Deutschen Bundes einverleibte, blieb Österreich auf Drängen Bismarcks und gegen den Widerstand König Wilhelms territorial unangetastet. Dies war eine entscheidende Voraussetzung für das spätere Bündnis mit der Donaumonarchie.

Infolge des Deutschen Krieges vergrößerte Preußen seine Macht erheblich. Zunächst schloss es am 18. August 1866 ein Verteidigungsbündnis mit seinen Verbündeten. Das Augustbündnis bereitete die Gründung des Norddeutschen Bundes vor. Mit den Annexionen vom Oktober 1866 verleibte sich Preußen die bereits im Krieg besetzen Gebiete offiziell ein: das Königreich Hannover, das Kurfürstentum Hessen-Kassel, das Herzogtum Nassau, die Freie Stadt Frankfurt und ganz Schleswig-Holstein. Fast ganz Norddeutschland bildete von da an ein geschlossenes preußisches Staatsgebiet. Darüber hinaus ging Preußen sogenannte Schutz- und Trutzbündnisse mit den ehemals gegnerischen süddeutschen Staaten Bayern, Württemberg und Baden ein. Davon ausgenommen waren nur Österreich und Liechtenstein.

Im Inneren beendete Bismarck den seit 1862 schwelenden preußischen Verfassungskonflikt durch das Indemnitätsgesetz. Es billigte dem preußischen Landtag nachträglich das Budgetbewilligungsrecht, Bismarck dagegen Straflosigkeit für sein nicht verfassungskonformes Regierungshandeln zu. Die rechten Liberalen, die späteren Nationalliberalen, unterstützten die Vorlage des Gesetzes und arbeiteten eng mit Bismarck zusammen. Die linken Liberalen verblieben in der Opposition. Ebenso spalteten sich die Konservativen an der Frage, ob man Bismarck und seine Politik unterstützen sollte.

Preußens Politik gegenüber Österreich war nur möglich gewesen, weil Frankreich neutral blieb. Daher hatte Bismarck Napoleon III. mit vagen Versprechungen, Luxemburg eventuell Frankreich zu überlassen, dazu gebracht, diese Politik zu dulden. Nun sah sich Frankreich jedoch einem erstarkten Preußen gegenüber, das von den früheren territorialen Zusagen nichts mehr wissen wollte. Im Jahr 1870 eskalierte der Streit um die spanische Thronkandidatur des katholischen Hohenzollernprinzen Leopold von Hohenzollern-Sigmaringen, den Bismarck nutzte, um einen Krieg mit Frankreich zu provozieren. Nach der Veröffentlichung der sogenannten Emser Depesche durch Bismarck erklärte die französische Regierung Preußen den Krieg. Für die süddeutschen Staaten Bayern, Württemberg, Baden und das südlich der Mainlinie noch unabhängige Hessen-Darmstadt trat damit der Bündnisfall ein.

Nach dem raschen deutschen Sieg im Deutsch-Französischen Krieg und der darauf folgenden nationalen Begeisterung in ganz Deutschland sahen sich nun auch die süddeutschen Fürsten gedrängt, dem Norddeutschen Bund beizutreten. Daraufhin erfolgte die Gründung des Deutschen Reiches in der kleindeutschen Version, die schon von der Nationalversammlung 1848/49 als Einigungsmodell vorgesehen worden war. Die Reichsverfassung, die am 1. Januar 1871 inkraft trat, übertrug das Bundespräsidium dem preußischen König. Im Rahmen einer Proklamation im Spiegelsaal von Versailles nahm Wilhelm I. am 18. Januar 1871, dem 170. Jahrestag der Königskrönung Friedrichs I., den Titel „Deutscher Kaiser“ an. Daher wurde nicht der 1., sondern der 18. Januar später als offizieller Reichsgründungstag begangen.

Von der Reichsgründung 1871 bis zur Novemberrevolution 1918 blieben deutsche und preußische Politik stets aufs engste miteinander verknüpft, da der König von Preußen zugleich Deutscher Kaiser und der preußische Ministerpräsident – bis auf die kurzen Amtszeiten von Botho zu Eulenburg und Albrecht von Roon – immer auch Reichskanzler war.

Zwischen 1871 und 1887 führte Bismarck in Preußen den sogenannten Kulturkampf, um den Einfluss des Katholizismus zurückzudrängen. Widerstände der katholischen Bevölkerungsteile und des Klerus, insbesondere im Rheinland und in den ehemals polnischen Gebieten, zwangen Bismarck aber dazu, die Auseinandersetzung ergebnislos zu beenden. In den mehrheitlich von Polen bewohnten östlichen Landesteilen Preußens ging der Kulturkampf mit dem Versuch einer Germanisierungspolitik einher.

Auf Wilhelm I. folgte im März 1888 der bereits schwer kranke Friedrich III., der nach einer Regierungszeit von nur 99 Tagen verstarb. Im Juni des „Drei-Kaiser-Jahres“ bestieg Wilhelm II. den Thron. Er entließ 1890 Bismarck und bestimmte die Politik des Landes von da an weitgehend selbst. Dies änderte sich erst im Verlauf des Ersten Weltkriegs, als sowohl der Kaiser als auch die Reichsregierung die Richtlinienkompetenz weitgehend der Obersten Heeresleitung unter den Generälen Hindenburg und Ludendorff überließen. Die Siegermächte sahen im Kaiser jedoch einen der Hauptverantwortlichen für den Kriegsausbruch. In mehreren Antwortnoten auf das deutsche Waffenstillstandsgesuch vom Oktober 1918 drangen sie verklausuliert auf seine Abdankung. Wilhelm II. erwog zunächst, nur als Deutscher Kaiser, nicht aber als König von Preußen abzudanken. Aufgrund seines Zögerns verschärfte sich die revolutionäre Lage in Berlin. Um sie zu entschärfen, gab Reichskanzler Max von Baden am 9. November den Verzicht des Kaisers auf beide Kronen ohne dessen Zustimmung bekannt. Damit endete de facto die Monarchie in Preußen und Deutschland. Am 28. November dankte Wilhelm II. vom niederländischen Exil aus auch formell ab. Die preußische Königskrone befindet sich heute auf der Burg Hohenzollern bei Hechingen.

Preußen wurde mit dem Ende des Kaiserreichs zum eigenständigen Freistaat innerhalb des Reichsverbands proklamiert und erhielt 1920 eine demokratische Verfassung.

Die im Versailler Vertrag festgelegten Gebietsabtretungen Deutschlands betrafen – bis auf das nach dem Deutsch-Französischen Krieg gebildete Reichsland Elsass-Lothringen und Teile der bayerischen Pfalz – ausschließlich preußisches Territorium: Eupen-Malmedy ging an Belgien, Nordschleswig an Dänemark, das Hultschiner Ländchen an die Tschechoslowakei. Große Teile der Gebiete von Westpreußen und Posen, die Preußen im Rahmen der Teilungen Polens erhalten hatte, sowie Ost-Oberschlesien gingen an Polen. Danzig wurde Freie Stadt unter Verwaltung des Völkerbunds und das Memelland kam unter Alliierte Verwaltung. Wie schon vor den polnischen Teilungen war Ostpreußen von den übrigen Landesteilen durch polnisches Gebiet getrennt. Vom Reichsgebiet aus war es per Schiff – mit dem Seedienst Ostpreußen –, auf dem Luftweg oder per Bahn durch den Polnischen Korridor erreichbar. Auch das nun für 15 Jahre vom Völkerbund verwaltete Saargebiet wurde überwiegend aus preußischen Gebietsteilen gebildet.

Einen preußischen Gebietszuwachs in der Zeit der Weimarer Republik stellt die Angliederung des Freistaats Waldeck dar. Dieser Kleinstaat hatte bereits 1868 durch einen Akzessionsvertrag einen Teil seiner Hoheitsrechte an Preußen verloren. Nach einer Volksabstimmung kam 1921 zunächst der Waldecker Kreis Pyrmont zur preußischen Provinz Hannover. Die Kündigung des Akzessionsvertrages durch Preußen fünf Jahre später führte zu großen Finanzproblemen im verbliebenen Teil Waldecks, der daraufhin im Jahr 1929 schließlich in die preußische Provinz Hessen-Nassau eingegliedert wurde.

Von 1919 bis 1932 regierten in Preußen Regierungen der Weimarer Koalition (SPD, Zentrum und DDP), 1921 bis 1925 um die DVP erweitert. Anders als in einigen anderen Ländern des Reichs, war die Mehrheit der demokratischen Parteien bei Wahlen in Preußen bis 1932 nicht gefährdet. Der von 1920 bis 1932 fast ununterbrochen regierende Ostpreuße Otto Braun, der bis heute als einer der fähigsten sozialdemokratischen Politiker der Weimarer Republik gilt, verwirklichte zusammen mit seinem Innenminister Carl Severing mehrere zukunftsweisende Reformen, die später für die Bundesrepublik beispielgebend waren. Dazu gehörte das Konstruktive Misstrauensvotum, das die Abwahl des Ministerpräsidenten nur dann ermöglichte, wenn gleichzeitig ein neuer Ministerpräsident gewählt wurde. Auf diese Weise konnte die Preußische Landesregierung so lange im Amt bleiben, wie sich im Landtag keine positive Mehrheit bildete, also eine Mehrheit derjenigen Oppositionsparteien, die auch wirklich zusammenarbeiten wollten.

Auch die Landtagswahl vom 24. April 1932 brachte keine positive Mehrheit zustande, da die radikalen Parteien KPD und NSDAP zusammen mehr Mandate erhielten als alle übrigen Parteien zusammen. Weil im Parlament keine regierungsfähige Koalition zustande kam, blieb die Regierung Braun weiter geschäftsführend im Amt. Dies lieferte Reichskanzler Franz von Papen den Vorwand zum sogenannten „Preußenschlag“. Mit diesem Staatsstreich setzte die Reichsregierung am 20. Juli 1932 per Verordnung die preußische Landesregierung unter dem Vorwand ab, sie habe die Kontrolle über die öffentliche Ordnung verloren (siehe auch: Altonaer Blutsonntag). Vom Großteil des Staatsapparats begrüßt, übernahm von Papen als Reichskommissar selbst die Macht in Preußen, das bis dahin „zu einem gewissen Grad […] seiner Rolle als Bollwerk der Weimarer Demokratie gerecht werden“ konnte. Die Absetzung der wichtigsten demokratisch gesinnten Landesregierung erleichterte entscheidend die Machtübernahme Adolf Hitlers ein halbes Jahr später. Den Nationalsozialisten standen dadurch von Anfang an die Machtmittel der preußischen Regierung – vor allem der Polizeiapparat – zur Verfügung.

Nach der Ernennung Hitlers zum Reichskanzler wurde Hermann Göring "Reichskommissar für das preußische Innenministerium". Damit stand den Nationalsozialisten bei ihrer Machtübernahme die Exekutivgewalt der preußischen Landesregierung zur Verfügung. Wenige Wochen später, am 21. März 1933, fand der sogenannte Tag von Potsdam statt. Dabei wurde der am 5. März neugewählte Reichstag in Anwesenheit des Reichspräsidenten Paul von Hindenburg symbolträchtig in der Potsdamer Garnisonkirche eröffnet, der Grabstätte der preußischen Könige. Die propagandistische Veranstaltung, in der Hitler und die NSDAP feierten, sollte preußisch-monarchistische und deutschnationale Kreise für den nationalsozialistischen Staat gewinnen und die Konservativen im Reichstag zur Zustimmung zum Ermächtigungsgesetz bewegen, die zwei Tage später anstand.

Seit 1933 schuf die Reichsregierung durch Gleichschaltungsgesetze den nationalsozialistischen Einheitsstaat. Das "Reichsstatthaltergesetz" vom 7. April 1933 und das "Gesetz über den Neuaufbau des Reichs" vom 30. Januar 1934 lösten die Länder zwar nicht formell auf, beraubten sie aber ihrer Eigenständigkeit. Alle Landesregierungen wurden der Kontrolle durch Reichsstatthalter unterstellt. Eine Ausnahme davon bildete Preußen, wo laut Gesetz der Reichskanzler selbst die „Rechte des Reichsstatthalters“ wahrnehmen sollte. Die Ausübung dieser Rechte übertrug Hitler aber bereits am 10. April 1933 per Erlass dem preußischen Ministerpräsidenten Göring. Parallel dazu gewannen die (Partei-)Gaue zunehmend an Bedeutung für die Umsetzung der nationalen Politik auf regionaler Ebene. Die Gauleiter wurden von Hitler in seiner Eigenschaft als Führer der NSDAP ernannt. In Preußen ging diese anti-föderalistische Politik noch weiter: Seit 1934 wurden nahezu alle seine Landes- mit den entsprechenden Reichsministerien zusammengelegt. Nur das preußische Finanzministerium, die Archivverwaltung und wenige andere Landesbehörden blieben bis 1945 eigenständig.

Die räumliche Ausdehnung Preußens veränderte sich zwischen 1933 und 1945 kaum. Im Zuge des Groß-Hamburg-Gesetzes fanden noch kleinere Gebietsveränderungen statt. Preußen wurde am 1. April 1937 unter anderem um die bis dahin Freie und Hansestadt Lübeck erweitert. Die im Zweiten Weltkrieg annektierten polnischen, vormals preußischen, Gebiete wurden überwiegend nicht in das angrenzende Preußen eingegliedert, sondern sogenannten Reichsgauen zugeteilt.

Am 8. Mai 1945 endete der Zweite Weltkrieg in Europa. Mit der anschließenden Besetzung des Sonderbereichs Mürwik am 23. Mai war auch die preußische Provinz Schleswig-Holstein gänzlich besetzt und die im Sonderbereich befindliche letzte Reichsregierung wurde verhaftet. Nach dem Ende der nationalsozialistischen Herrschaft war Deutschland in Besatzungszonen aufgeteilt und seine Ostgebiete jenseits der neu errichteten Oder-Neiße-Grenze Polen und der Sowjetunion eingegliedert. Damit hörte der Staat Preußen 1945 de facto auf zu bestehen. De jure existierte er noch bis zu seiner formellen Auflösung durch das Kontrollratsgesetz Nr. 46 vom 25. Februar 1947. In ihm stellte der Alliierte Kontrollrat fest:

Schon vor Erlass dieses Gesetzes waren in den westdeutschen Besatzungszonen auf bis dahin preußischem Gebiet flächendeckend Länder gebildet worden. Nach dem Kontrollratsbeschluss schritt die Auflösung Preußens auch in der sowjetischen Besatzungszone fort: Die bis dahin nur noch als Verwaltungseinheiten existierenden Provinzen Sachsen(-Anhalt) und Brandenburg wurden in Länder umgewandelt, und der Zusatz „Vorpommern“ wurde aus dem Namen des Landes Mecklenburg-Vorpommern 1947 entfernt, so dass im amtlichen Sprachgebrauch z. B. Greifswalder „Mecklenburger“ genannt wurden. Auf dieselbe Weise wurden Bewohner der vormals niederschlesischen Oberlausitz zu „Sachsen“. Zugleich galt in der SBZ und später in der DDR die Benutzung der Begriffe „Pommern“ und „Schlesien“ für die deutsch gebliebenen Teile dieser ehemaligen preußischen Provinzen offiziell als unerwünscht.

Die Länder auf dem früheren Staatsgebiet des Freistaats Preußen sind in rechtlicher, insbesondere staats- und völkerrechtlicher Hinsicht Nachfolgestaaten Preußens. So ist zum Beispiel das Land Nordrhein-Westfalen an das Konkordat gebunden, das der Freistaat Preußen mit dem Heiligen Stuhl abgeschlossen hat.

Trotz der politischen Auflösung des preußischen Staates im Jahre 1947 blieben viele Aspekte bis heute im Alltagsleben, in der Kultur oder im Sport und selbst in Namensbezeichnungen erhalten. In folgenden, beispielhaft aufgeführten Bereichen wird die noch heute prägende Stellung Preußens deutlich:








Die Staatswerdung Preußens unterscheidet sich deutlich von der anderer europäischer Mächte wie Frankreich oder England. Das Königreich, das 1701 entstand, war kein Produkt einer gewachsenen Kultur oder Konsequenz der geschichtlichen Entwicklung eines Volkes. Da seine Gebiete weit verstreut lagen, fehlte ein weiterer wichtiger Anreiz für einen natürlichen Staatsbildungsprozess, nämlich die Organisation und Zusammenfassung (Synergie) geografisch zusammenhängender Gebiete. So war der preußische Staat ausschließlich Ausdruck des Machtwillens seiner Eliten.

In anderen historisch gewachsenen Staaten, so eine These, passten sich diese den Bedürfnissen der Gesellschaft an. In Preußen dagegen, wo die Voraussetzungen für eine Staatswerdung vollkommen fehlten, habe der Staat die Gesellschaft nach seinen Bedürfnissen geformt. So entstand ein gut organisierter Verwaltungs- und Herrschaftsapparat, der durch seine Machtfülle und Organisationsfähigkeit seinen Nachbarn für einige Jahrhunderte überlegen war und damit den Erfolg dieses „Preußischen Staatsmodells“ begründete. Im Norddeutschen Bund (ab 1. Juli 1867) und dann im Deutschen Kaiserreich (ab 1. Januar 1871) wirkte die preußische Verwaltung in den Bundesstaat hinein. Die enge Verbindung der Reichsbehörden mit den preußischen führte umgekehrt aber auch zu einer „Verreichlichung“ Preußens. Ernst Rudolf Huber:„Die Entwicklung des Reichs zu einem wirklichen Staat hing entscheidend davon ab, daß es einen Beamtenkörper gewann, der nicht nur durch technische Tüchtigkeit, sondern durch die Fähigkeit zur politischen Integration des Reichs ausgezeichnet war. […] Im Dienst der in schneller Folge entstehenden zentralen Reichsämter […] entwickelte sich ein dem Reich unmittelbar eingefügter Beamtenkörper, der in seiner Pflichttreue und seinem Leistungsvermögen dem vielgrühmten preußischen Beamtentum gleichkam, an Aufgeschlossenheit für die Verwaltungsaufgaben und für Verfassungsprobleme der modernen Zeit das preußische Beamtentum aber noch übertraf.“Die Bundesbeamten und dann die Reichsbeamten kamen vor allem aus dem preußischen Beamten- und Richterstand. Eigene Ausbildungsgänge für Bund bzw. Reich gab es noch nicht. Bei aller Treue zum Reich und zum Kaiser, so Huber, habe es ein kritisches Bewusstsein gegeben.

Seit der Reformation hatte Preußen eine überwiegend evangelische Bevölkerung. Im Vergleich zu stärker vom Katholizismus geprägten Nachbarstaaten galt Preußen in Fragen der Religionsausübung als verhältnismäßig ‚liberal‘. Letzteres traf insbesondere auf die Regierungszeiten Friedrich Wilhelms I., welcher die Salzburger Exulanten, protestantische Glaubensflüchtlinge, in Preußen ansiedelte, und Friedrichs des Großen zu, der die Auffassung vertrat, jeder Bürger sollte die Möglichkeit haben, „nach seiner eigenen Fasson selig zu werden“. In Nachbarstaaten verfolgte religiöse Minderheiten suchten in Preußen Schutz, andere Minderheiten blieben hier unbehelligt. Während der Volkszählung am Ende des Jahres 1840 wurden in Preußen 194.558 Juden gezählt.

Das preußische Staatsmodell stützte sich auf eine besondere Form der Ethik, die gemeinhin als "preußischer Geist" zusammengefasst wird und in die Legendenbildung eingegangen ist. So verbindet man mit Preußen auf der einen Seite die Stereotype der von protestantischen Werten (überwiegend lutherische Bevölkerung, aber calvinistisches Herrscherhaus) geprägten preußischen Tugenden wie beispielsweise Zuverlässigkeit, Sparsamkeit, Bescheidenheit, Ehrlichkeit, Fleiß und Toleranz. Das gegenteilige Stereotyp verweist auf Militarismus, Autoritarismus, aggressiven Imperialismus und auf eine grundsätzlich demokratiefeindliche und reaktionäre Politik. Dabei hat Preußen weniger Kriege geführt als etwa Frankreich und England.

Christopher Clark stellt für die erste Hälfte des 19. Jahrhunderts fest, dass in England und Wales jährlich etwa sechzehnmal so viele Menschen hingerichtet wurden wie im vergleichbar großen Preußen. Wurde in Preußen die Todesstrafe fast nur gegen Mörder verhängt, so gab es diese Strafe in England auch für zum Teil geringfügige Eigentumsdelikte. „Die Briten tolerierten staatliche Gewalt in einem Maße, wie sie in Preußen undenkbar gewesen wäre.“ Das Elend der Armen in Preußen in den 1840er-Jahren bleibe auch zurück hinter der irischen Hungerkatastrophe unter britischer Herrschaft. „Wären die Polen in Preußen von einer vergleichbaren Hungersnot dahingerafft worden, würden wir darin heute vielleicht Vorboten der Naziherrschaft nach 1939 sehen.“

Das heutige Bild Preußens in der Geschichtswissenschaft ist weitaus differenzierter als beide Stereotype, deren letzteres aber, wie nachfolgend gezeigt, als Gründungsmythos der Bundesrepublik Deutschland als notwendig erscheint; es wird auf die Vielschichtigkeit und lange historische Entwicklung dieses Staates verwiesen.

Die Landesfarben Preußens, Schwarz und Weiß, sind schon im Stammwappen der Hohenzollern enthalten. Das Wappentier Preußens ist der schwarze Preußische Adler. Der Wappenspruch lautet seit der Reformation "Suum cuique" – "Jedem das Seine". Das Preußenlied galt zeitweilig als inoffizielle Nationalhymne Preußens.







</doc>
<doc id="4004" url="https://de.wikipedia.org/wiki?curid=4004" title="Prutenische Tafeln">
Prutenische Tafeln

Die Preußischen oder Prutenischen Tafeln der Himmelsbewegungen ("Prutenicae Tabulae Coelestium Motuum") sind astronomische Tafeln zur Berechnung der Standorte von Sonne, Mond und den Planeten. Die Prutenischen Tafeln lösten die bisher verwendeten Alfonsinischen Tafeln ab, deren vorhergesagte Daten zunehmend von der Realität abwichen. Zum Beispiel wurden Ereignisse wie Konjunktionen von Jupiter und Saturn, denen in der Astrologie hohe Bedeutung zugeschrieben wurden, mit einem Fehler von bis zu einem Monat vorhergesagt, in den Prutenischen Tafeln nur noch mit einigen Tagen.

Sie wurden von Erasmus Reinhold im Jahre 1551 angefertigt. Er bezog sich auch auf die Arbeiten von Nikolaus Kopernikus während er eine große Anzahl von Sternen identifizierte und beschrieb. Die höhere Genauigkeit der Prutenischen Tafeln ist allerdings nicht auf das verwendete Kopernikanische System zurückzuführen, sondern in erster Linie auf die Aktualität der verwendeten Ausgangsdaten.

Herzog Albrecht von Brandenburg-Ansbach unterstützte Reinhold und finanzierte den Druck der Prutenischen Tafeln, obwohl es dabei zu unerquicklichen Auseinandersetzungen wegen der Bezahlung kam. Reinhold und seine Prutenischen Tafeln trugen dazu bei, dass das Kopernikanische System überall im deutschsprachigen Reich und darüber hinaus bekannt wurde, ebenso wie Königsberg durch die Wahl als Nullmeridian.

Die Erkenntnisse von Kopernikus und die Preußischen Tafeln waren später die Grundlage zur Kalenderreform von 1582 unter Papst Gregor XIII.

Die Tafeln erschienen erstmals 1551 bei Ulrich Morhard in Tübingen, Morhards Witwe verbreitete 1562 zudem einen Nachdruck der ersten Auflage. Herausgeber der zweiten Auflage (Tübingen, 1571) war Michael Mästlin (1550–1631), der später Keplers Lehrer wurde. Er fügte ein Nachwort und zwei Seiten mit Korrekturen hinzu. Die dritte Auflage (Wittenberg, Matthäus Welack, 1585) stammt von Caspar Strubius, Rektoren der Universität Wittenberg.

Die Rudolfinischen Tafeln von Brahe und Kepler lösten die Preußischen im 17. Jahrhundert ab.

Owen Gingerich untersuchte den Einfluss von Reinhard und dessen Tafeln auf die Verbreitung der Kopernikanischen Lehre.




</doc>
<doc id="4005" url="https://de.wikipedia.org/wiki?curid=4005" title="Pickelhaube">
Pickelhaube

Die Pickelhaube (amtlich seinerzeit: „Helm mit Spitze“) war eine zunächst rein militärische, dann auch polizeiliche Kopfbedeckung, die erstmals ab 1843 in der preußischen Armee eingesetzt und später auch von anderen Ländern übernommen wurde.

Der vom preußischen Militär 1843 eingeführte „Helm mit Spitze“ wurde im Volksmund bald „Pickelhaube“ genannt, was schlicht auf die „pieksende“ Spitze, also den „Pickel“, verwies. Die „Haube“ hingegen war im Deutschen schon immer ein anderes Wort auch für den militärischen Helm, wie etwa bei der Sturmhaube und der Beckenhaube. Letztere war – nach einigen Lautverschiebungen in manchen Teilen des deutschen Sprachraums – schon „Bickelhaube“ bzw. „Pickelhaube“ genannt worden. Helmtyp und Name dieser mittelalterlichen Beckenhaube verschwanden aber nach 1450 ganz. Der 1842/43 entwickelte preußische „Helm mit Spitze“ weist daher keine historische Verbindung zur Beckenhaube bzw. „Bickelhaube“ auf. 

Im amtlichen Sprachgebrauch kam der Begriff „Pickelhaube“ niemals vor, und auch in seriösen Publikationen (z. B. in Herstellerkatalogen, in sachlichen Artikeln der preußischen Tagespresse etc.) wurde meist nur vom „Helm“ bzw. vom „Helm mit Spitze“ oder „Lederhelm“ gesprochen. Der preußische Helm mit Spitze erlangte in den Staaten im deutschen Sprachraum jedoch gerade unter seiner volkstümlichen Bezeichnung „Pickelhaube“ rasch größere Bekanntheit, insbesondere nach der Reichsgründung 1871. Dort wurde die Pickelhaube bald als typisch preußisch-deutsche Kopfbedeckung bzw. als Sinnbild des preußischen Militarismus betrachtet.

1842 wurde unter König Friedrich Wilhelm IV. von Preußen für die preußische Armee (mit Ausnahme der Jäger, Schützen, Husaren und Ulanen) ein neuer Helm verordnet, der dann 1843 eingeführt wurde. Der von der Metallwarenfabrik Wilhelm Jaeger in Elberfeld (heute Wuppertal) 1841 entwickelte Prototyp für Kürassiere war aus Stahl gefertigt und hatte einen Augen- und Nackenschirm. Da dieser Helm für Fußtruppen zu schwer war, entwickelte der Unternehmer Christian Harkort, ein jüngerer Bruder von Friedrich Harkort, in seiner Lederwarenfabrik in Haspe einen Helm aus gepresstem Büffelleder mit Metallbeschlägen. Harkort erhielt im November 1842 den ersten Auftrag zur Ausrüstung von preußischen Truppenteilen mit diesem Lederhelm. Charakteristisch war die Metallspitze; sie sollte Hiebe mit Säbeln oder ähnlichen Blankwaffen seitlich ablenken. Bei einigen Regimentern, insbesondere bei der Garde, wurde zur Paradeuniform die Spitze durch einen Haarbusch ersetzt. Bei der Artillerie wurde anstelle der Spitze eine Kugel getragen, da sonst bei der Bedienung der Geschütze das Risiko von Verletzungen bestanden hätte. Die Kürassiere trugen eine Ausführung mit stählerner Helmglocke und tief nach unten gezogenem Nackenschirm. 

Bei der Garde du Corps, der Leibgendarmerie und den Gardekürassieren wurde zur großen Uniform statt der Spitze eine metallene Adlerskulptur getragen. Der metallene Helmtyp wurde später auch von den Teilen der Jäger zu Pferde (Regimenter Nr. 1 bis 7, 8 bis 13 trugen Lederhelme) übernommen, bestand jedoch aus Tombak. Die beiden sächsischen schweren Kavallerie-Regimenter trugen ab 1875 den Kürassierhelm in Gelbmetall mit weißem Haarbusch zur großen Uniform, ab 1910 beim 1. Regiment zur großen Uniform mit einer Löwenskulptur anstelle der Spitze.

Ob die moderne Pickelhaube wirklich in Preußen erfunden wurde, ist nicht sicher. Der Legende nach sah Friedrich Wilhelm IV. 1842 bei einem Besuch in Russland auf dem Schreibtisch des Zaren das Vorserienmodell einer russischen Pickelhaube und war davon so begeistert, dass er diese Helmform sofort in Preußen einführte, während Russland erst 1846 folgte. Angeblich wurden aber auch schon vor 1842 bei einer bayerischen Feuerwehr Helme dieses Typs getragen. Hier hat die Spitze (oder Spinne, heute: Kamm) die Funktion des Brechens herabfallender Trümmer oder einer Tür durch Kopfstoß.

Von Preußen ausgehend verdrängte diese Helmform nach und nach bei allen deutschen Staaten andere Helmtypen und den bis dahin üblichen Tschako. 1857 wurde die Helmglocke flacher und bekam ihre heute bekannte, charakteristische Form. Dieses Modell wurde in der Kaiserzeit (ab 1871) von allen deutschen Ländern verwendet und mit einem Messingemblem in Form von Landestier oder Landeswappen getragen. 1897 bekam sie zwei Kokarden (eine in den Reichs- und eine in den Landesfarben), die seitlich unter den Ansatzrosetten des Kinnriemens bzw. der Schuppenkette angebracht waren. 
1886 gab letztlich sogar Bayern den für sein Heer bis dahin typischen Raupenhelm auf und übernahm die Pickelhaube (wobei in Bayern, anders als in den übrigen Bundesstaaten, auch bei der Artillerie eine Spitze und kein Kugelaufsatz getragen wurde), auch wenn die bayerischen Generäle mit Rücksicht auf Vorbehalte von Prinzregent Luitpold weiterhin den in Bayern üblichen Generalshut trugen.

Den Einsatzbedingungen eines modernen Krieges war das 1895 eingeführte, vorletzte Pickelhaubenmodell zu Beginn des Ersten Weltkriegs nicht mehr gewachsen. Die Messingbeschläge reflektierten das Licht und erschwerten die Tarnung des Soldaten im Feld. Als Konzession an die moderne Kriegführung trug man deshalb schon seit 1892 im Kampf- und Manövereinsatz einen beigefarbenen Helmüberzug mit roter, aufgenähter oder aufgemalter Regimentsnummer. Die meisten Kopfverletzungen im Krieg infolge des gewaltig gesteigerten Artillerieeinsatzes wurden durch Granatsplitter verursacht, gegen die der alte Helm unzureichenden Schutz bot. Weiter ragte die Helmspitze oft verräterisch aus dem Schützengraben heraus. Als Übergangslösung ordnete die Oberste Heeresleitung daher 1915 an, die Spitze im Fronteinsatz nicht mehr zu tragen. Beim letzten, noch während des Krieges hergestellten Pickelhaubenmodell ließ sich die Spitze auf sehr einfache Weise abschrauben; auch der Helmüberzug wurde entsprechend abgeändert. Die Farbe wurde generell feldgrau, die auffällige rote Regimentsnummer entfiel. Um Leder einzusparen, wurde die letzte Generation der Pickelhaube zum Teil auch aus Ersatzmaterialien wie Filz oder Pappe hergestellt. Als verbesserter Kopfschutz wurde dann im Laufe des Jahres 1916 im deutschen Heer der Stahlhelm aus heißgepresstem Chromnickelstahl eingeführt.

Die Pickelhaube blieb nach dem Weltkrieg teilweise noch bei Polizei und Feuerwehr in Gebrauch. In den 1920er Jahren wurde sie häufig von Weltkriegsoffizieren und Mitgliedern von Kriegervereinen bei Veteranentreffen, Beerdigungen und ähnlichen Gelegenheiten getragen. Auch Reichspräsident Hindenburg trug diese Kopfbedeckung bei manchen offiziellen Anlässen noch, z. B. am „Tag von Potsdam“.

Auch in einigen anderen europäischen Ländern (z. B. von einigen englischen Regimentern), in lateinamerikanischen Staaten und in den USA wurden von einigen militärischen oder polizeilichen Formationen zeitweilig Pickelhauben getragen.

In England, Chile, Portugal und Schweden werden Helme in Pickelhaubenform heute noch von Paradeeinheiten bei besonderen Anlässen getragen. Auch der Helm der britischen Bobbys ist eine Abwandlung der ursprünglichen, höheren Version der Pickelhaube. Ein englisches Polizeimodell hat auch eine Spitze mit einer Kugel. Bei der Paradeuniform der britischen Gardekavallerie ist die Spitze durch den Rosshaarbusch ersetzt.

Zwischen 1842 und 1871 wurde die Pickelhaube im deutschsprachigen Raum bald als charakteristisches Symbol des preußischen Militarismus betrachtet. Nach der Gründung des Deutschen Reichs durch Preußen 1871 wurde dieser preußische Militarismus auch im nicht-deutschsprachigen Ausland zu einem deutschen Militarismus umgedeutet, als dessen charakteristischer Ausdruck die Pickelhaube wahrgenommen wurde. In zahlreichen Karikaturen wurde diese Einschätzung durch die Darstellung pickelhaubentragender deutscher Aggressoren ausgedrückt. Dies war insbesondere bis zum Ende des Ersten Weltkriegs (und damit bis zum Ende der preußischen Vormachtstellung in Deutschland) der Fall. Aber auch heute noch steht die Pickelhaube im Ausland mitunter stellvertretend für das Deutsche als solches. So wird sie zum Beispiel bei sportlichen Wettkämpfen von manchen deutschen Fans getragen, oder gegnerische Fans stellen „die Deutschen“ mit Pickelhauben dar.

In der Gebärdensprache der Gehörlosen symbolisiert der ausgestreckte, nach oben zeigende und über die Stirn gehaltene Zeigefinger die Pickelhaube und bedeutet "deutsch" und "Deutschland".





</doc>
<doc id="4006" url="https://de.wikipedia.org/wiki?curid=4006" title="Penicilline">
Penicilline

Die Penicilline oder Penizilline (Singular "Penicillin", von lateinisch "penicillium", ‚Pinselschimmel‘) sind eine Gruppe von antibiotisch wirksamen Substanzen, die sich strukturell von der 6-Aminopenicillansäure ableiten. Neben natürlich vorkommenden Penicillinen, die als sekundäre Stoffwechselprodukte von verschiedenen "Penicillium"-, "Aspergillus"-, "Trichophyton"- und "Streptomyces"-Arten gebildet werden, zählt man dazu auch biosynthetisch und teilsynthetisch hergestellte Penicilline.

Penicillin G, ein heute noch therapeutisch verwendetes natürliches Penicillin, gehört zu den ältesten verwendeten Antibiotika, dem neben seinem großen medizinischen Nutzen auch die Vorreiter-Rolle für die wissenschaftliche Verwendung dieser Wirkstoffgruppe zugeschrieben wird. Nach der Entdeckung der antibiotischen Wirksamkeit von Penicillinen durch Alexander Fleming wurde die enorme Bedeutung der Antibiotika für die Medizin erkannt, was das moderne Verständnis der Bedeutung bakterieller Krankheitserreger maßgeblich beeinflusst und revolutioniert hat. In den Jahrzehnten nach seiner Entdeckung trug Penicillin G zur Rettung ungezählter Menschenleben bei. Obwohl es heute zahlreiche Bakterienstämme gibt, die gegen dieses Antibiotikum resistent sind, kann es noch immer weltweit erfolgreich eingesetzt werden.

Penicilline gehören zur Gruppe der β-Lactam-Antibiotika. Die Summenformel lautet R-CHNOS, wobei „R“ für eine variable Seitenkette steht.

Die Muttersubstanz der Penicilline, die 6-Aminopenicillansäure, entsteht biologisch aus L-α-Aminoadipinsäure und den Aminosäuren L-Cystein und L-Valin, wobei zunächst Isopenicillin N gebildet wird. Durch den durch die Acyltransferase katalysierten Austausch des L-α-Aminoadipylrestes durch andere organische Säurereste entstehen verschiedene Penicilline. Von den natürlichen Penicillinen ist nur Penicillin G (Benzylpenicillin) therapeutisch bedeutsam, das fermentativ aus "Penicillium chrysogenum" gewonnen wird. Es wird nicht so rasch abgebaut wie das stärker wirksame Penicillin K. Unter Verwendung eines entsprechenden Präkursors, der Phenylessigsäure, lässt es sich zudem gezielt fermentativ herstellen. Neben "Penicillium"-Arten als biologische Produzenten werden Penicilline auch von anderen Schimmelpilzen wie "Acremonium chrysogenum" (ehemals "Cephalosporium acremonium") und "Aspergillus nidulans" gebildet, sowie von bestimmten Bakterien, die der Ordnung Actinomycetales angehören, beispielsweise produziert "Streptomyces clavuligerus" Penicillin N.<ref name="DOI10.1099/00207713-21-4-326">C. E. Higgens, R. E. Kastner: "Streptomyces clavuligerus sp. nov., a β-Lactam Antibiotic Producer." In: "International Journal of Systematic and Evolutionary Bacteriology." Band 21, Nummer 4, Oktober 1971, S. 326-331, .</ref> 
Das oral wirksame Penicillin V (Phenoxymethylpenicillin) entsteht ebenfalls fermentativ, aber nicht spontan, sondern durch Zusatz eines synthetischen Präkursors, der Phenoxyessigsäure („Biosynthese“).

Partialsynthetische Weiterentwicklungen der natürlichen Penicilline entstehen durch Umsetzung von 6-Aminopenicillinsäure mit Carbonsäurehalogeniden. Sie zeichnen sich gegenüber dem Penicillin G durch bestimmte Vorteile aus wie Säure- und Penicillinasestabilität oder ein erweitertes Wirkspektrum.

Durch eine verminderte Nucleophilie des Carbonylsauerstoffs ist der Angriff von Hydronium-Ionen erschwert, wodurch die Umwandlung zur unwirksamen Penillsäure gehemmt wird. Säurestabile Penicilline werden also durch Magensäure nicht zerstört und können daher oral verabreicht werden. Sie haben das gleiche Wirkspektrum wie Penicillin G.

Durch Abschirmung des Betalactamringes mit Isoxazolylstrukturen gegenüber der von manchen Erregern gebildeten Penicillinase sind solche Penicilline auch wirksam gegen penicillinasebildende Staphylokokken („Staphylokokken-Penicillin“). Im Vergleich zu Penicillin G ist ihre Wirkstärke deutlich geringer und gegenüber gramnegativen Erregern sind Isoxazolylpenicilline gänzlich wirkungslos.


Durch die Einführung polarer Substituenten vermögen solche hydrophilen Penicilline die Zellwände von gramnegativen Erregern zu passieren, wodurch ihr Wirkspektrum erweitert wird. Die Breitspektrumpenicilline sind teilweise säure- und penicillinaseempfindlich, teilweise ist auch die Wirksamkeit gegenüber einigen grampositiven Bakterien vermindert.

Kein Acylierungsprodukt der 6-Aminopenicillansäure ist das Pivmecillinam, es besitzt jedoch das bicyclische Lactam-Grundgerüst der Penicilline.

Penicilline wirken bei der Zellteilung der Bakterien bakterizid, und zwar beim Neuaufbau der Zellwand, indem sie in die Synthese der Zellwand eingreifen, dort die innere Vernetzung verhindern und damit die Zellwand so schwächen, dass diese bei Belastung platzt. Insbesondere grampositive Bakterien, die sich teilen, sterben unter Penicillineinfluss. Das Grundgerüst der Penicilline besteht aus 6-Aminopenicillansäure, einem bicyclischen Dipeptid aus L-Cystein und L-Valin. Dieser sogenannte Beta-Lactam-Ring wird (im dann geöffneten Zustand) von dem bakteriellen Enzym D-Alanin-Transpeptidase gebunden, das für die Quervernetzung der Peptidoglykane in den bakteriellen Zellwänden grampositiver Bakterien zuständig ist. Das Enzym wird vor allem bei sich teilenden Bakterien benötigt, da bei diesen die starre Zellwand geöffnet und zumindest teilweise neu synthetisiert werden muss. Da die Bindung an die D-Alanin-Transpeptidase irreversibel ist, kann keine Zellwand mehr synthetisiert werden, und das grampositive Bakterium verliert seine wichtigste Schutzhülle. Daneben kommt es durch den ständigen Auf- und Abbau der defekten Zellwand zu toxischen Abbauprodukten.

Die Wirkung der Penicilline betrifft also nur sich vermehrende Bakterien, nicht aber sich nicht teilende: Diese beeinflusst das Antibiotikum nicht mehr, weil keine Zellwand-Neusynthese stattfinden muss – sie ist bereits vollständig abgeschlossen und bildet für Penicillin somit keinen Angriffspunkt mehr. Sich nicht vermehrende Bakterien stellen aber keine Gefahr für den Wirtsorganismus dar und werden relativ schnell durch die körpereigene Immunabwehr der Patienten unschädlich gemacht. Treten sie dagegen erneut in einen Vermehrungszyklus ein, wird wiederum die Zellwand teilweise abgebaut und muss neu synthetisiert werden; solche Bakterien sind daher durch Penicilline wieder angreifbar. Aus diesem Grund müssen Penicilline auch eine gewisse Folgezeit nach Abklingen der Symptome weiter verabreicht werden.

Penicilline sind also nur wirksam, wenn die Bakterien in ihrem Wachstum ansonsten unbehindert sind; so sollen Penicilline nicht zusammen mit Medikamenten verabreicht werden, die die Bakterien an der Vermehrung hindern, da man sonst therapeutisch selbst den Ansatzpunkt der Penicillin-Wirkungsweise versperrt.

Penicilline wirken nicht nur auf Bakterien, inklusive Cyanobakterien, sondern sie blockieren auch die Teilung der Cyanellen, der photosynthetisch aktiven Organellen der "Glaucocystaceae" (einer Algenfamilie) sowie der Chloroplasten von Blasenmützenmoosen. Auf die Teilung der Plastiden der höher entwickelten Gefäßpflanzen wie beispielsweise bei Tomaten haben sie jedoch keinen Effekt. Dies ist ein Hinweis darauf, dass bei höheren Pflanzen durch erfolgte, evolutionäre Veränderungen der Plastidteilung β-Lactam-Antibiotika im Allgemeinen auf Chloroplasten keine Wirkung mehr zeigen.

Penicillin G und V wirken nicht gegen gramnegative Bakterien (mit Ausnahme gramnegativer Kokken wie beispielsweise Neisserien), die über ihrer Zellmembran noch eine zusätzliche äußere Membran besitzen. Diese macht den Angriff des Penicillins unmöglich, da es in die Ausbildung der darunter liegenden Peptidoglycan<nowiki></nowiki>schicht eingreifen muss. Daher ist der Einsatz von Penicillin G und V nur bei grampositiven Bakterien sinnvoll. Gegen gramnegative Bakterien werden strukturelle Varianten wie zum Beispiel Aminopenicilline eingesetzt.

Zahlreiche klinisch vorkommende Bakterien sind heute bereits gegen Penicillin G resistent, was zu einer Reihe von weiterentwickelten β-Lactam-Antibiotika geführt hat. Dabei bleibt das Problem der Kreuzresistenzen kritisch, was bedeutet, dass Keime, die einmal eine Resistenz gegen Penicilline entwickelt haben, auch gegen andere β-Lactam-Antibiotika (z. B. Cephalosporine) unempfindlich werden.

Resistente Mutanten würden eigentlich keinen Schaden anrichten, da sie nur in geringem Maße auftreten. Wirkt jedoch das Penicillin auf die anderen, nichtresistenten Bakterien und eliminiert diese, so kann sich eine resistente Bakterie viel besser fortpflanzen und wird damit zur Gefahr, da sie ihre Resistenz an die Folgegenerationen weitervererbt. Durch den Austausch von Resistenzgenen zwischen verschiedenen Bakterienarten wird eine Antibiotikaresistenz auch auf andere Arten weitergetragen. Besonders gefürchtet ist die Multiresistenz bei Staphylococcus aureus.

Der Vorgang der Resistenzentwicklung ist ein sehr anschauliches Beispiel der darwinschen Evolutionstheorie (natürliche Selektion); aufgrund der raschen Teilung und Generationenfolge werden die an ihre Umwelt angepassten, resistenten Bakterien selektiert und bilden die Grundlage für spätere Generationen. Die Bildung von penicillinresistenten Stämmen gilt als einer der ersten experimentellen Beweise für beobachtete Mikroevolution. Biologische Grundlage der Wirkstoffgruppe ist die Konkurrenz zwischen den beiden Organismenstämmen Pilze und Bakterien, die auf dieselben Ressourcen angewiesen sind, wobei die Pilze sich mit antibakteriellen wachstumshemmenden Substanzen gegen die Bakterien schützen.

Neben der Gefahr einer Resistenzbildung gegenüber Penicillinen liegt ein weiterer Nachteil in der relativ häufigen Allergie der Patienten gegen diese Arzneistoffe (etwa bei einem von 7000 Patienten). Allergische Reaktionen können dabei von leichter Hautrötung bis zum anaphylaktischen Schock reichen.

Durch Penicilline können nützliche Bakterien wie die der Darmflora abgetötet werden, insbesondere durch Breitbandpenicilline, die auch gegen gramnegative Bakterien wirken. Im ungünstigsten Fall können sich so schädliche Mikroorganismen im Darm ausbreiten und zu einer antibiotikaassoziierten Kolitis führen.

Eine weitere seltene (mit einer Wahrscheinlichkeit von etwa drei Promille) unerwünschte Wirkung ist die Auslösung epileptischer Anfälle.

Bereits 1874 hatte der Chirurg Theodor Billroth in Wien zweifelsfrei den das Wachstum von Bakterien hemmenden Effekt des Pilzes Penicillium erkannt.
Die (Wieder-)Entdeckung der Penicilline begann mit einer verschimmelten Bakterienkultur: Alexander Fleming, der sich am St. Mary’s Hospital in London mit Staphylokokken beschäftigte, hatte 1928 vor den Sommerferien eine Agarplatte mit Staphylokokken beimpft und dann beiseite gestellt. Bei seiner Rückkehr entdeckte er am 28. September 1928, dass auf dem Nährboden ein Schimmelpilz ("Penicillium notatum") wuchs und sich in der Nachbarschaft des Pilzes die Bakterien nicht vermehrt hatten. Fleming nannte den bakterientötenden Stoff, der aus dem Nährmedium gewonnen werden konnte, "Penicillin" und beschrieb ihn für die Öffentlichkeit erstmals 1929 im "British Journal of Experimental Pathology". Er untersuchte die Wirkung des Penicillins auf unterschiedliche Bakterienarten und tierische Zellen; dabei stellte er fest, dass Penicillin nur grampositive Bakterien wie Staphylokokken, Streptokokken oder Pneumokokken abtötete, nicht aber gramnegative Bakterien wie beispielsweise Salmonellen. Auch gegenüber weißen Blutkörperchen und menschlichen Zellen oder für Kaninchen erwies es sich als ungiftig. Fleming kam trotz dieser Kenntnis offenbar nicht auf die Idee, Penicillin als Medikament einzusetzen.

Fast zehn Jahre später – 1938 – machten sich Howard W. Florey, Ernst B. Chain und Norman Heatley daran, systematisch alle von Mikroorganismen gebildeten Stoffe zu untersuchen, von denen bekannt war, dass sie Bakterien schädigten. So stießen sie auch auf Flemings Penicillin. Sie reinigten es und untersuchten seine therapeutische Wirkung zunächst an Mäusen und dann auch an Menschen. Im Jahre 1939 isolierte René Dubos vom Rockefeller Institute for Medical Research aus menschlichen Tränen das Tyrothricin und zeigte, dass es die Fähigkeit besaß, bestimmte bakterielle Infektionen zu heilen. 1941 unternahmen Florey und Chain den ersten klinischen Test, der allerdings nur auf wenige Personen beschränkt war. Da die Herstellung von Penicillin noch sehr mühsam war, gewannen sie es sogar aus dem Urin der behandelten Personen zurück.
Mit dem Ausbruch des Zweiten Weltkriegs war den Alliierten daran gelegen, für ihre verwundeten Soldaten ein wirksames Medikament zu entwickeln. Die Antibiotika-Forschung verlagerte sich in die USA und nahm dort einen rasanten Verlauf. Man fand heraus, dass es günstiger ist, den Pilz in geeigneten flüssigen Nährmedien zu kultivieren. In den USA züchtete man neue Stämme von "Penicillium chrysogenum", die mehr Penicillin produzierten. Damit stand der Stoff als Medikament in der notwendigen Menge zur Verfügung. 1943 stellte sich heraus, dass Penicillin keine chemisch einheitliche Substanz ist: so bestand das von den britischen Forschern mittels der oberflächlichen Fermentation (emerse Fermentation) gewonnene Rohpenicillin vor allem aus Penicillin F, das in den USA durch submerse Fermentation erzeugte Rohpenicillin im Wesentlichen aus Penicillin G. Früh erkannte man insgesamt vier Varianten, die mit römischen Zahlen als Penicillin I, II, III und IV bzw. mit den Buchstaben F, G, X und K bezeichnet wurden.

1945 erhielten Fleming, Chain und Florey für ihre Entdeckung, die einen Wendepunkt in der Geschichte der Medizin markiert, gemeinsam den Nobelpreis. Der Wirkstoff beendete das seit der Antike bestehende medizinische Problem, dass chirurgische Verletzungen aufgrund einfacher Wundinfektionen auch lange nach den Kriegshandlungen zum Tod der Betroffenen führen können, und wurde deshalb auch nach dem Krieg noch in der Bevölkerung als Wundermedizin angesehen. In Deutschland konzentrierte man sich auf die Weiterentwicklung der Sulfonamide, die heute noch eingeschränkt gebräuchlich sind, da der Alliierte Kontrollrat die Nutzung von Penicillin weder in der Forschung noch in der medizinischen Anwendung erlaubt hatte. Erst 1946 gelangte ein Penicillinstamm unter letztlich ungeklärten Umständen zur Stolberger Firma Chemie Grünenthal, wo eine Produktion begonnen wurde.

Schon die Nubier verwendeten ein Bier mit antibakteriellen Wirkstoffen. Die Alten Ägypter versorgten Entzündungen mit aus Getreide gebrauten Heiltränken. In der Antike und im Mittelalter legten Chirurgen schimmelige Lappen auf Wunden, um Infektionen vorzubeugen. Die Wirkstoffe wurden jedoch nicht als solche erkannt, der Begriff des Antibiotikums wurde erst mit dem Penicillin eingeführt.

Fleming war jedoch nicht der erste neuzeitliche Wissenschaftler, der entdeckte, dass Schimmelpilze Bakterienwachstum hemmen können: Schon 1870 hatte John Scott Burdon-Sanderson einen Zusammenhang zwischen Schimmelpilzen und Bakterienwachstum erkannt. 1884 behandelte Joseph Lister den Abszess einer Krankenschwester mit einem Penicillium-Schimmelpilz (genauer: "Penicillium glaucum"), veröffentlichte die Ergebnisse jedoch nicht. 1896 führte Ernest Duchesne einen erfolgreichen Tierversuch mit Meerschweinchen durch. Alle diese Erkenntnisse blieben jedoch ohne Resonanz in der wissenschaftlichen Welt und wurden völlig verkannt. Erst Sir Fleming verwendete sicher das "Penicillium notatum" (welches Penicillin Burdon-Sanderson und Duchesne verwendeten, ist leider unbekannt).

Flemings Veröffentlichungen fanden zunächst bei Kollegen kaum Beachtung. Erst im Zweiten Weltkrieg erzielte das Penicillin den Durchbruch. Dabei spielte eine Rolle, dass die Sulfonamide, von denen ein Wirkstoff unter dem Handelsnamen "Prontosil" das erste praktisch eingesetzte Antibiotikum war, in Deutschland hergestellt und von deutschen Firmen patentiert waren, so dass sie für die Kriegsgegner nach Kriegsausbruch nicht mehr in der gleichen Weise verfügbar waren. Erst in der weiteren Forschung stellten sich die Vorzüge des Penicillin G gegenüber dieser Wirkstoffklasse heraus. Die Deutschen setzten jedoch bis Kriegsende weiterhin auf Sulfonamide.

Im Jahre 1939 interessierten sich Howard Walter Florey und Ernst Boris Chain für das Penicillin. Norman Heatley gelang es, Penicillin aus der Kulturflüssigkeit, in der man die Schimmelpilze züchtete, zu extrahieren und zu reinigen. Am 24. August 1940 fand ein Tierversuch an 50 Ratten statt, die mit einer tödlichen Dosis Streptokokken infiziert wurden. Die Hälfte von ihnen erhielt Penicillin, und nur eine aus dieser Gruppe starb. Die Ratten, die kein Penicillin erhielten, starben alle innerhalb weniger Stunden. Dieses Tierexperiment stellte überraschend die kraftvolle Wirkung des Penicillins heraus, die bei diesem aggressiven Bakterienstamm nicht erwartet wurde.

Am 12. Februar 1941 wurde der erste Patient mit dem gewonnenen Penicillin behandelt. Es handelte sich um einen 43-jährigen Londoner Polizisten, der sich beim Rosen schneiden verletzt und durch Infektion der Wunde eine Blutvergiftung erlitten hatte. Nach fünf Tagen Behandlung war das Fieber verschwunden. Die Penicillinvorräte waren jedoch aufgebraucht und die Behandlung musste abgebrochen werden. Der Mann verstarb einen Monat später. Dies stellte retrospektiv die Notwendigkeit heraus, dass Antibiotika grundsätzlich länger eingenommen werden müssen, als die sichtbaren Beschwerden andauern. Ein vorzeitiger Abbruch birgt immer das Risiko eines Krankheitsrückfalls, auch heute oftmals nur behandelbar durch Einsatz alternativer Antibiotika.

Erst als Florey und Heatley in die USA flogen, um dort für Penicillin zu werben, wurde das allgemeine Interesse an Penicillin geweckt, besonders beim amerikanischen Militär. Zunächst suchte man nach einem Pilzstamm, der mehr Penicillin produziert. Dazu sammelte die amerikanische Luftwaffe Bodenproben von möglichst vielen Flugplätzen weltweit. Der ergiebigste Stamm, "Penicillium chrysogenum", wurde jedoch auf einer verschimmelten Melone vor dem Forschungsinstitut entdeckt.

Nachdem 1940 und 1941 der „Oxforder Kreis“ um E. B. Chain und H. W. Florey ein Verfahren zur Produktion und Isolierung von Penicillin in „Lancet“ veröffentlichte, wurden ab 1942 die ersten industriellen Penicillinproduktionen gestartet, allen voran Glaxo und ICI in England, Merck & Co., Pfizer & Co. und Squibb & Sons in den USA sowie Schott Jena (vorangetrieben durch Hans Knöll) in Deutschland. Ab 1942 wurde auch bei den Farbwerken Hoechst an Penicillin geforscht. Die Forscher mussten sich dabei auf die knappen Veröffentlichungen Flemings stützen. Hoechst hatte auch nicht den ergiebigen Chrysogenum-Stamm zur Verfügung. Eine Probe dieses Stamms schickte erst 1950 im Rahmen einer Zusammenarbeit der US-Konzern Merck & Co. nach Deutschland.
Im Jahre 1945 war die in den USA produzierte Penicillinmenge 20-mal größer als die in Europa produzierte Menge.
Gewöhnlicher Mais, in Wasser eingeweicht, von den Amerikanern "" genannt (Maisquellwasser), erwies sich als ideales Nährmedium für den Pilz. Dabei wurde (1944 bei Pfizer) das zunächst angewandte Oberflächenverfahren (Emerskultur) durch das Submersverfahren (Flüssigkultur im Rührkesselreaktor) abgelöst, mit dem höhere Produktivitäten erreicht wurden.
Im Oktober 1944 wurden die ersten Injektionspräparate hergestellt.
1943 boten 22 Firmen Penicillin an. Es blieb zunächst hauptsächlich verwundeten Soldaten vorbehalten, denn die Produktionsmenge reichte noch nicht aus, um auch alle zivilen Patienten damit zu behandeln. Seit 1944 waren die USA jedoch in der Lage, ihren gesamten zivilen und militärischen Bedarf an Penicillin zu decken. Hingegen war in Europa nach dem Zweiten Weltkrieg die Nachfrage groß und die Penicillinproduktion reichte hier nicht für alle Patienten. Es entwickelten sich Schmuggel und Schwarzhandel mit Penicillin, was auch Thema des Films "Der dritte Mann" ist.

In den Jahrzehnten nach dem Zweiten Weltkrieg entwickelte sich die Nederlandsche Gist-en Spiritusfabrik (NG & SF), später Gist Brocades, heute DSM, zum weltgrößten Penicillinhersteller. Heute produziert die DSM Penicilline im Rahmen von Joint Ventures in China, während die Sandoz GmbH in Kundl (Österreich) heute den größten Produktionsstandort für Penicilline in der westlichen Welt betreibt.




</doc>
<doc id="4008" url="https://de.wikipedia.org/wiki?curid=4008" title="Phon (Begriffsklärung)">
Phon (Begriffsklärung)

Phon bedeutet

Vergleiche auch:


</doc>
<doc id="4009" url="https://de.wikipedia.org/wiki?curid=4009" title="Pfeffer">
Pfeffer

Der Pfefferstrauch ("Piper nigrum"), auch Schwarzer Pfeffer oder kurz Pfeffer genannt, ist eine Pflanzenart aus der Familie der Pfeffergewächse (Piperaceae). Die Früchte werden wegen des darin enthaltenen Alkaloids Piperin als scharf schmeckendes Gewürz verwendet. Zur Unterscheidung von ähnlichen Gewürzen spricht man auch vom echten Pfeffer.

Die Früchte des Schwarzen Pfeffers sind nicht immer schwarz. Je nach Erntezeitpunkt und weiterer Behandlung haben die Pfefferkörner verschiedene Färbungen: grüner Pfeffer, schwarzer Pfeffer, weißer Pfeffer, roter Pfeffer.

Die Pfefferpflanze ist eine ausdauernde, verholzende Kletterpflanze, die an Bäumen emporwächst und rund zehn Meter hoch werden kann. In der Kultur hält man die Pflanze üblicherweise auf einer Höhe von 3 bis 4 Metern.

Die kleinen, unauffälligen Blüten stehen in ca. 10 Zentimeter langen Ähren mit 50 bis 150 Einzelblüten, die bei kultiviertem Pfeffer, anders als bei Wildsorten, zum größten Teil (bis zu 90 Prozent) zwittrig sind.

Nach der Befruchtung entwickeln sich in typischerweise 8 bis 9 Monaten reife Früchte (Steinfrucht). Von einer Pfefferpflanze kann zweimal jährlich Pfeffer geerntet werden; die Pflanzen bleiben bis zu 30 Jahre ertragreich.

Die Chromosomenzahl beträgt 2n = 52, seltener 26, 78, 53, 54, 65 oder etwa 104.

Die ursprüngliche Heimat dieser Pflanze ist Indien, speziell die Malabarküste. Im Zuge der Ausbreitung der indischen Kultur nach Südostasien gelangte der Pfefferanbau vor ca. 1000 Jahren in das heutige Indonesien und Malaysia.

Die größten Anbauländer von Pfeffer sind Vietnam, Indonesien, Indien, Brasilien und Malaysia. Weltweit werden etwa 200.000 Tonnen Pfeffer jährlich produziert, wobei jedoch starke jährliche Schwankungen auftreten. Der Wert einer gesamten Jahresweltproduktion von Pfeffer wird auf 300 bis 600 Millionen US-Dollar geschätzt.

Pfeffer ist heute hauptsächlich als Gewürz bedeutsam. 

Grüner Pfeffer wird aus unreifen, früh geernteten Früchten gewonnen. Er unterscheidet sich vom schwarzen Pfeffer dadurch, dass er entweder frisch in Salzwasser eingelegt wird oder schnell und bei hohen Temperaturen getrocknet oder gefriergetrocknet wird. Daher behält er die ursprüngliche grüne Farbe. Durch Luftfracht ist mittlerweile frischer grüner Pfeffer weit verbreitet erhältlich.

Schwarzer Pfeffer wird ebenfalls aus den unreifen (grün) bis knapp vor der Reife (gelb-orange) stehenden, durch Trocknen runzlig und schwarz gewordenen Früchten des "Piper nigrum" gewonnen.

Weißer Pfeffer ist von der Schale befreiter vollreifer Pfeffer. Zu seiner Produktion werden die reifen roten Pfefferbeeren etwa acht bis vierzehn Tage in fließendem Wasser eingeweicht, so dass sich die Schale durch Fäulnis ablöst. Danach werden sie mechanisch geschält, getrocknet und teilweise in der Sonne gebleicht. Der recht lange Vorgang des Einweichens kann durch Behandlung mit Pektinasen verkürzt werden.
Roter Pfeffer besteht aus vollkommen reifen, ungeschälten Pfefferfrüchten und wird meist ähnlich wie auch der grüne Pfeffer in salzige oder saure Laken eingelegt. Verglichen mit dem grünen Pfeffer ist der eingelegte rote Pfeffer jedoch eher selten erhältlich. Noch viel seltener findet man den echten roten Pfeffer in getrockneter Form.

Pfeffer kommt bisweilen unter Bezeichnungen in den Handel, die auf die Herkunft und graduelle geschmackliche Unterschiede hinweisen, aber auch werblich verwendet werden. "Borneopfeffer" und "Sarawak-Pfeffer" weisen auf eine Herkunft aus dem malaysischen Bundesstaat Sarawak im Norden von Borneo hin. "Lampong-Pfeffer" stammt von der indonesischen Insel Sumatra (siehe die Provinz Lampung auf Sumatra).

Bevor der Gütertransport mit Schiffen von Asien nach Europa üblich wurde, musste Pfeffer aus seinem damals einzigen bedeutenden Anbaugebiet in Südwest-Indien nach Europa auf dem Landweg transportiert werden. Seine Haltbarkeit und Schärfe machten ihn zum idealen Fernhandelsgut. Pfeffer dominierte bereits in der Antike (Römisch-indische Beziehungen) den Gewürzhandel zwischen Asien und Europa. Mit Karawanen und über den Seeweg durch das Rote Meer wurde er in den Mittelmeerraum bzw. das römische Reich transportiert. Das Bremer Pfefferkorn vom Anfang des 13. Jahrhunderts ist der älteste erhaltene Fund als Gewürzmittel nördlich der Alpen.

Pfeffer war ein kostbares Gut und wurde zeitweilig mit Gold aufgewogen. Händler, die ihren Reichtum unter anderem dem Pfeffer zu verdanken hatten, wurden als Pfeffersäcke bezeichnet. Pfeffer war ein Luxusartikel, erhielt aber seine Bedeutung vor allem dadurch, dass mit seiner Hilfe Lebensmittel lange haltbar gemacht werden können. Darüber hinaus galt er auch als Heilmittel.

Im Mittelalter hatten die Türken und Araber, später Venedig, das Monopol auf den Gewürzhandel mit Indien und wachten eifrig darüber. Die kostbaren Gewürze waren ein Grund, warum Marco Polo sich um einen eigenen europäischen Karawanenbetrieb nach Asien bemühte und Christoph Kolumbus und andere Seefahrer einen Seeweg nach Indien suchten. 1498 gelang es Vasco da Gama erstmals, eine Ladung Pfeffer auf dem Seeweg aus Indien nach Europa zu bringen. Einen Teil seiner Bedeutung als Handelsgut in Asien büßte Pfeffer nach der Entdeckung Amerikas ein, als der von dort stammende Chili (scharfe Paprika) ihn in vielen asiatischen Gerichten aus seiner Rolle als wichtiges scharfes Gewürz verdrängte.

Pfeffer wurde ab dem hohen bis späten Mittelalter auch in Südostasien angebaut – zunächst in Thailand, später vor allem in Indonesien („Gewürzinseln“). Von dort wurden hauptsächlich der chinesische und der einheimische Markt bedient. Erst im 17. Jahrhundert gelangte Pfeffer von dort auch in nennenswerten Mengen, zunächst durch niederländische Handelsgesellschaften, nach Europa. Der südostasiatische Gewürzhandel spielte eine Rolle in diversen bewaffneten Auseinandersetzungen, vor allem zwischen den Niederlanden und anderen europäischen Seemächten.

Mit der Redensart „Geh dahin, wo der Pfeffer wächst!“ wünscht man jemanden, mit dem man nicht klarkommt, in ein sehr weit entferntes Gebiet, um ihm absehbar nicht mehr zu begegnen. – Eine andere Erklärung bezieht die Redensart auf die Strafkolonien in Französisch-Guayana (Hauptstadt: Cayenne). Das Land war zudem auch für den Pfefferanbau bekannt. Die Redensart „Geh dahin, wo der Pfeffer wächst“ würde den Gesprächspartner folglich in diese Strafkolonie wünschen. Der prominenteste Häftling in Französisch-Guayana war Alfred Dreyfus, der von 1895 bis 1899 auf der Teufelsinsel (Île du Diable) inhaftiert war.

Ausschlaggebend für die Schärfe des Pfeffers sind das Alkaloid Piperin (5-8 %) und Derivate des Piperins wie Piperettin, Piperylin, Piperanin, Chavicin (eine isomere Verbindung des Piperins) und andere je nach Pfefferart in wechselnder Zusammensetzung, die man auch als Alkamide (Säureamid-Alkaloide) bezeichnet. Piperin erhält man in Form von gelben Kristallen über Kristallisation aus einem alkoholischen Pfefferextrakt. Es wurde erstmals 1819 von Ørsted isoliert.

In weißem Pfeffer ist etwa 2,5 Prozent ätherisches Öl, sogenanntes Pfefferöl, und in grünem und schwarzem Pfeffer bis zu 4,8 Prozent davon enthalten, welches dem Pfeffer sein Aroma gibt. Das Pfefferöl erhält man über Wasserdampfdestillation von Pfeffer. Im ätherischen Öl sind die Hauptbestandteile Monoterpene, wie Pinene, 3-Caren, Terpinene, Terpinolen und Limonen, sowie Sesquiterpene, wie α- und β-Caryophyllen und β-Farnesen. Außerdem findet man im Pfefferöl auch oxidierte Terpene, wie Terpinen-4-ol.

Das sogenannte Pfefferharz erhält man nach Abtrennung des Piperins über Kristallisation und der Terpene über Wasserdampfdestillation. Es enthält Substanzen, die man unter dem Sammelbegriff Piperoleine zusammenfasst. Sie ähneln dem Piperin, haben aber längere Kohlenwasserstoffketten mit einer Doppelbindung und neigen zum Verharzen.

Weitere wichtige Bestandteile im schwarzen Pfeffer sind etwa 50 Prozent Stärke, 5 bis 6 Prozent fettes Öl sowie die Flavonoide (und Glykoside von) Kaempferol, Rhamnetin und Quercetin.

Als Heildroge dienen die ausgewachsenen, aber noch grünen, ungeschälten und getrockneten Früchte. Nach einer Behandlung mit kochendem Wasser werden sie in der Wärme getrocknet und werden dabei dunkel.

Wirkstoffe sind: Scharf schmeckende Säureamide, vor allem Piperin; ätherisches Öl mit Limonen, Sabinen, Caryophyllen und auch Safrol.

Anwendung: Der scharf brennende Geschmack des Pfeffers kommt über die Erregung von Wärme- und Schmerzrezeptoren zustande. Reflektorisch werden Speichel- und Magensaftsekretion angeregt, ebenso die Ausschüttung der Verdauungsenzyme. Pfeffer hat daher eine deutlich appetitanregende und verdauungsfördernde Wirkung, die aber in Arzneimitteln außer in Tonika in Europa kaum noch eingesetzt wird.

Langer Pfeffer besteht aus den getrockneten, unreifen Fruchtkätzchen von "Piper longum", einer anderen Art aus der Gattung Pfeffer ("Piper"). Das lateinische Wort "piper" (und davon das deutsche Wort "Pfeffer") entstammt dem indischen Wort für diesen Langpfeffer: "pippali".

Voatsiperifery-Pfeffer ("Piper borbonense") ist ein selten im Spezialitätenhandel erhältlicher Pfeffer aus Madagaskar von stark aromatischem Geschmack. Er zeichnet sich durch seine dunkelbraune Farbe und den typischen Stielansatz der ovalen Beeren aus. Da er aus Wildsammlungen stammt und die Kletterpflanze erst ab 10 bis 20 Meter Höhe Früchte trägt, ist die Jahresernte sehr gering und sein Preis sehr hoch.

Der Kubebenpfeffer ("Piper cubeba") spielt heute nur noch in der nordafrikanischen Küche eine Rolle, war aber im 15. und 16. Jahrhundert in Europa recht populär.

Spanischer Pfeffer ("Capsicum annuum") gehört zur Gattung Paprika ("Capsicum"). Aus dieser Pflanzenart sind sehr verschiedene Sorten gezüchtet worden: einerseits Gemüsepaprika, andererseits Sorten mit scharfen und sehr scharfen Früchten. Cayennepfeffer besteht aus den gemahlenen Schoten, wobei bevorzugt die scharfe Varietät Cayenne verwendet wird.

Im englischen Sprachraum werden sowohl echter Pfeffer als auch Paprikaschoten – scharfe und nicht scharfe Sorten – als "pepper" bezeichnet. Zum Beispiel können mit "green pepper" grüne Paprika gemeint sein, aber auch grüne Pfefferkörner. Entsprechendes gilt für "red pepper". "Black pepper" kann sich allerdings nur auf Pfeffer beziehen: entweder auf die Pfefferpflanze (Schwarzer Pfeffer) oder auf schwarze Pfefferkörner.

Im Gewürzmuseum in Hamburg ist ein Pfeffer-Ersatz aus der Zeit des Zweiten Weltkriegs ausgestellt, der Paprika enthält: 50 kg Haferschalenmehl, 50 kg gem. Dillstroh, 40 kg Paprikafruchtstängelpulver und 1/2 kg Sellerie-Öl wurden dafür gemischt.

Rosa Pfeffer (auch "brasilianischer Pfeffer" oder "rosa Beere" genannt) stammt vom Brasilianischen Pfefferbaum ("Schinus terebinthifolius") oder vom Peruanischen Pfefferbaum ("Schinus molle"). Aus optischen Gründen wird rosa Pfeffer oft zum weißen, schwarzen und grünen Pfeffer in die Pfeffermühle hinzugegeben.

Szechuanpfeffer ("Zanthoxylum piperitum") wird vor allem in der asiatischen Küche und Heilkunde verwendet.

Der Guineapfeffer (oder "Meleguetapfeffer") stammt aus Westafrika (siehe auch Pfefferküste). Es handelt sich um die getrockneten Samen von "Aframomum melegueta", einer Pflanzenart, die zu den Ingwergewächsen gehört. Sie werden auch als Paradieskörner bezeichnet.

Piment ("Pimenta dioica") ist auch unter den Bezeichnungen "Nelkenpfeffer" oder "Jamaikapfeffer" bekannt geworden. Er gehört zur Familie der Myrtengewächse.





</doc>
<doc id="4011" url="https://de.wikipedia.org/wiki?curid=4011" title="Macrotyloma uniflorum">
Macrotyloma uniflorum

Macrotyloma uniflorum ist eine Pflanzenart aus der Gattung "Macrotyloma" in der Unterfamilie der Schmetterlingsblütler (Faboideae) innerhalb der Familie der Hülsenfrüchtler (Fabaceae). Sie wird seit einigen Jahrhunderten im südlichen Indien genutzt, hat aber mittlerweile Verbreitung in weiten Teilen der Welt gefunden.

Die Art wird auf Deutsch gelegentlich "Pferdebohne" genannt, in Übersetzung des englischen Trivialnamens "Horsegram", und ist dann nicht mit der oft ebenso genannten Ackerbohne zu verwechseln. Diese Nutzpflanze ist nahe verwandt mit einer Reihe von anderen als Bohnen bezeichneten Feldfrüchte, insbesondere der Erdbohne.

"Macrotyloma uniflorum" wächst als windende oder halbaufrechte, einjährige bis ausdauernde krautige Pflanze und erreicht je nach Standort Wuchshöhen von 30 bis 90 Zentimetern. Die wechselständigen Laubblätter sind gestielt und dreiteilig gefiedert. Die drei eiförmigen Teilblätter sind an ihrer Basis gerundet, meist 3,5 bis 5 (2,5 bis 7,5) cm lang und 2 bis 4 cm breit. Das Endfiederblatt ist symmetrisch und die seitlichen Fiederblätter sind asymmetrisch. Ober und -unterseite der Fiederblätter sind weich-wollig behaart. Die Nebenblätter sind 7 bis 10 mm lang.

In den Blattachseln stehen die Blüten einzeln oder zu zweit bis viert in mehr oder weniger sitzenden, kurzen, traubigen Blütenständen. Die zwittrigen Blüten sind zygomorph und fünfzählig. Die fünf wollig behaart Kelchblätter sind verwachsen; die Kelchröhre ist 3 mm lang, die Kelchzähne sind 3 mm lang, wobei das obere Paar vollkommen verwachsen ist. Die fünf Kronblätter sind gelb bis grünlich-gelb. Die 9 bis 10,5 mm lang und 7 bis 8 mm breite Fahne besitzt zwei etwa 5 mm lange Anhängsel. Die zwei Flügel und das Schiffchen sind mit 8 bis 9,5 mm etwa gleich lang.

Die unbehaarte oder wollig behaarte, leicht gebogene, 2,5 bis 6 Zentimeter lange und etwa 6 Millimeter breite Hülsenfrucht endet in einer etwa 6 Millimeter langen Spitze und enthält fünf bis acht Samen. Die eiförmigen Samen sind meist 4 bis 6, selten bis zu 8 Millimeter lang, 3 bis 5 Millimeter breit und ihre Farben reichen von hellrot über braun bis schwarz, manchmal mit Flecken. Das Hilum ist zentral. Das Tausendkorngewicht beträgt zwischen 30 und 50 g.

Die Chromosomenzahl beträgt 2n = 22 oder 20.

Das heutige Verbreitungsgebiet ist sehr groß: in Afrika: Angola, Botswana, Demokratische Republik Kongo (Zaire), Äthiopien, Kenia, Mosambik, Namibia, Ruanda, Somalia, Südafrika (Transvaal), Sudan, Tansania, Uganda und Simbabwe; in Asien: Bhutan, China, Indien, Indonesien (Java), Nepal, Pakistan, die Philippinen, Sri Lanka und Taiwan; auch in Australien.

In den Gebieten der afrikanischen Aufsammlungen betragen die jährlichen Niederschlagsmengen 450 bis 750 mm und bei den indischen Aufsammlungen 600 bis 2200 mm, insgesamt hauptsächlich 550 bis 1000 mm, meist mit einer hohen Dominanz von Sommerregen. In den Anbaugebieten kann der Jahresniederschlag minimal 300 mm betragen. Blattkrankheiten können ein Problem in Gebieten mit hohen Niederschlagsmengen sein. Sie sind sehr trockenheitstolerant, aber sie tolerieren keine Überflutungen und vertragen Staunässe schlecht. 

Beim Anbau gibt es auch keine hohen Ansprüche an die Bodenbeschaffenheit. Der Anbau kann erfolgen wenn Durchschnittstemperatur zwischen 20 und 30 °C vorliegt. Bei genügend hohen Mindesttemperaturen findet ein Anbau bis in Höhenlagen von 1800 Meter statt.

Die Erstveröffentlichung erfolgte 1786 als "Dolichos uniflorus" durch Jean-Baptiste de Lamarck in "Encyclopedie Methodique. Botanique ... Paris", 2, S. 299. Der aktuell gültige Name "Macrotyloma uniflorum" wurde 1970 von Bernard Verdcourt in "Kew Bulletin", 24: 322, 401 veröffentlicht.

Innerhalb der Art "Macrotyloma uniflorum" gibt es vier Varietäten:





</doc>
<doc id="4013" url="https://de.wikipedia.org/wiki?curid=4013" title="Pippin der Jüngere">
Pippin der Jüngere

Pippin der Jüngere, genannt auch Pippin III., Pippin der Kurze und Pippin der Kleine (* 714; † 24. September 768 in Saint-Denis bei Paris) war ein fränkischer Hausmeier aus dem Geschlecht der Karolinger und seit 751 König der Franken. Er war der Sohn Karl Martells und der Vater Karls des Großen.

Pippin wurde 714 als zweiter Sohn Karl Martells und dessen erster Ehefrau Chrotrud geboren und im Kloster Saint-Denis erzogen. 

Im März 741 teilte Karl Martell sein Reich zwischen seinen Söhnen aus erster Ehe, Karlmann und Pippin, wobei sich über Art und Durchführung die wenigen, aus späterer Zeit stammenden Quellen durchaus widersprechen. Karlmann bekam Austrien, Alemannien und Thüringen, während Pippin Neustrien, Burgund und die Provence erhielt. Kurz vor seinem Tod änderte Karl Martell sein Testament. Grifo, sein Sohn von Swanahild, bekam nun auch seinen Teil. Nach dem Tod von Karl Martell ergriff Karlmann seinen Halbbruder Grifo mit dessen Mutter und setzte beide in unterschiedlichen Klöstern fest. 742, nach dem Aquitanien-Feldzug, teilten Karlmann und Pippin den "Annales regni Francorum" zufolge das Reich endgültig unter sich auf. Da die Macht nicht gefestigt schien, setzte Karlmann 743 mit Childerich III. erneut einen merowingischen König ein, um so seinem Amt als Hausmeier eine königliche Legitimierung zu geben. 

Zwischen den beiden Brüdern scheint es grundsätzlich kein Einvernehmen und keine erfolgreiche Kooperation gegeben zu haben. Als Karlmann 747, laut den karolingerfreundlichen Quellen, seine Länder Pippin übergab und sich in die Klöster Monte Soracte und Monte Cassino zurückzog, scheint es zumindest Gerüchte gegeben zu haben, dass dieser Rückzug nicht ganz freiwillig war. Pippin ließ explizit in zeitgenössischen Quellen erwähnen, dass es allein der Entschluss seines Bruders gewesen sei. Er übernahm dann jedoch unter Umgehung möglicher Erbrechte von Karlmanns Kindern und von seinem Halbbruder Grifo die Regentschaft des ganzen Frankenreichs, die allerdings nicht unangefochten blieb. So musste er erneut eine Empörung Grifos unterdrücken. Bei den Alemannen hob Pippin die Herzogswürde auf, und in Bayern setzte er Odilos unmündigen Sohn Tassilo III. als Herzog, aber unter fränkischer Oberhoheit, ein.

Pippin war bestrebt, nach dem Amt des Hausmeiers auch den Königstitel zu erlangen. Die faktische Macht lag schon lange bei den karolingischen Hausmeiern. Die merowingischen Könige waren nur noch dem Namen nach die Herrscher. Zur Legitimation dieser Titelübernahme bediente sich Pippin nach Darstellung der fränkischen Reichsannalen des Papstes Zacharias. Pippin schickte dazu seine wichtigsten Ratgeber, Fulrad von Saint-Denis und Bischof Burkard von Würzburg, nach Rom zum Papst mit der Frage: „Wegen der Könige in Francia, die keine Macht als Könige hätten, ob das gut sei oder nicht.“ Wunschgemäß antwortete der Papst: „Es ist besser, den als König zu bezeichnen, der die Macht hat“. Im November 751 ließ sich Pippin durch eine Versammlung der Franken in Soissons nach Absetzung Childerichs III., der mitsamt seinem Sohn Theoderich in das Kloster Prüm verwiesen wurde, zum König (Rex Francorum) ausrufen und beendete damit die Reihe der Könige aus dem Geschlecht der Merowinger.

Mit der Wahl Pippins zum König begann die Königsherrschaft der Karolinger im Frankenreich. Die exakte Rolle des Papstes bei diesem Vorgang wurde später unterschiedlich gesehen: Während Rom von einer Art autoritativer Aufforderung zur Krönung und damit einem päpstlichen Verfügungsrecht über das Königtum ausging, verstanden die weltlichen Großen das „Weistum“ des Papstes eher als eine Art Gutachten. Als der von den Langobarden bedrängte Papst Stephan II. in das Frankenreich kam, um Pippin um Hilfe zu bitten (6. Februar 754 in Ponthion), ließ sich dieser am 28. Juli 754 samt seinen Söhnen Karlmann und Karl in der Basilika Saint-Denis von ihm salben.

Im Frühjahr 755 zog Pippin nach Italien. Der Langobardenkönig Aistulf, in Pavia belagert, zeigte sich entgegenkommend, brach aber nach Pippins Abzug seine Zusagen und belagerte den Papst in Rom. Pippin kehrte 756 zurück, zwang Aistulf zur Anerkennung der fränkischen Oberherrschaft und zur Abtretung des Exarchats von Ravenna, das Pippin dem Papst schenkte (Pippinische Schenkung), und übernahm das Patriziat über die Stadt Rom. 

753 und 757 führte er erfolgreich Kriege gegen die Sachsen und trieb durch die Eroberung Narbonnes die Sarazenen über die Pyrenäen. 760 bis 768 unternahm er wiederholt Feldzüge gegen Herzog Waifar von Aquitanien. Überdies konnte er Herzog Tassilo III. von Bayern 757 zur Leistung des Vasalleneides zwingen.

Vor seinem Tode teilte Pippin das Reich unter seinen Söhnen Karl (747–814) und Karlmann I. (751–771). 

Pippin starb am 24. September 768 in Saint-Denis bei Paris und wurde im Westen vor der dortigen Klosterkirche, der heutigen Kathedrale von Saint-Denis beigesetzt. Bei der Plünderung der Königsgräber von Saint-Denis während der Französischen Revolution wurde sein Grab im August 1793 geöffnet und geplündert, seine Überreste wurden in einem Massengrab außerhalb der Kirche beerdigt. 

Von seiner Ehefrau Bertrada der Jüngeren hatte er außer Karl und Karlmann noch vier weitere Kinder:

Eine Gedenktafel für ihn fand Aufnahme in die Walhalla bei Regensburg.

Die Rolle des Bonifatius bei Pippins Königssalbung von 751 wird in der Wissenschaft kontrovers diskutiert. Dass Bonifatius Pippin damals gesalbt habe, wird erst in Quellen berichtet, die ein bis zwei Generationen nach dem Ereignis entstanden (Reichsannalen, Metzer Annalen), in zeitgenössischen Quellen wie der Fredegar-Fortsetzung und den Bonifatius-Briefen ist dies nicht erwähnt. In der Geschichtswissenschaft wird die Bonifatiussalbung daher teils skeptisch betrachtet, teils aber auch für möglich gehalten.

Unklar ist auch, ob gegebenenfalls das Vorbild der Westgoten eine Rolle gespielt haben könnte, die bis zum Untergang ihres Reiches enge Kontakte zu den Franken gepflegt hatten und deren Herrscher spätestens seit Wamba gesalbt worden waren. Eine andere These bezieht, sich darauf, dass Pippin nicht nur sich, sondern auch seine Frau salben ließ. Das Ritual ähnelt zunächst dem der Salbung Sauls durch den Propheten Samuel im Alten Testament. Doch durch die Einbeziehung seiner Frau werden auch die eventuellen Nachkommen mit in das Ritual einbezogen. Die Einbeziehung des Papstes und der Bischöfe sollte dabei für die entsprechende Aufmerksamkeit sorgen, da es innerhalb der Familie einen Streit um die Nachfolge gab und hier eine unbestreitbare Lösung präsentiert werden sollte.

Josef Semmler stellte 2003 die den bisherigen Forschungsstand völlig in Frage stellende These auf, dass es 751 überhaupt keine Salbung gegeben habe, während die Salbung 754 durch Papst Stephan II. keine Königssalbung war, sondern nur eine so genannte „postbaptismale Taufsalbung“ gewesen sei, was seitdem kontrovers diskutiert wird.

Falls Semmlers These zutreffend wäre, würde jedoch im Hinblick auf den symbolisch-rituellen Aspekt selbst eine nur fiktiv vollzogene Salbung von Bedeutung sein. Denn sie wurde als legitimierender Faktor in den karolingischen Quellen festgehalten und wurde somit wirksamer Bestandteil der symbolisch-politischen Kommunikation.

Pippins Beiname „der Kurze“ oder „der Kleine“ kommt nicht, wie gelegentlich behauptet, von der falschen Übersetzung des lateinischen „Pippinus minor“ für „Pippin den Jüngeren“. Vielmehr war „Pippin der Kurze“ (lateinisch: "Pippinus brevis") ein Beiname, der ursprünglich in Quellen des 11. und 12. Jahrhunderts Pippin dem Mittleren beigelegt wurde (so bei Ademar von Chabannes und noch im 13. Jahrhundert in den Grandes Chroniques de France). Die Übertragung des Beinamens auf Pippin den Jüngeren dürfte ab dem 12. Jahrhundert unter Einfluss einer bereits 883/884 von Notker Balbulus aufgeschriebenen Anekdote erfolgt sein ("Gesta Karoli Magni", Buch 2, Kap. 15). Danach habe Pippin der Jüngere einen Löwen mit dem Schwert besiegt und sich dabei mit dem kleinen David, wie er dem großen Goliat gegenüberstand, sowie mit dem kleinwüchsigen Alexander dem Großen verglichen. Gottfried von Viterbo nennt Pippin den Jüngeren in seinem „Königsspiegel“ ("Speculum regum", 1183) sogar "Pippinus nanus", also „Pippin den Zwerg“. Einige spätere Autoren sind ihm darin gefolgt, so Sicardus von Cremona in seiner Universalchronik ("Chronica universalis", 1213).

Für die Zuweisung des Beinamens „der Kurze“ an Pippin den Jüngeren spielte wohl ebenso eine Rolle, dass man die Größe seines Sohnes Karls "des Großen" mit einem "kleinen" Vorgänger kontrastieren wollte. Ein weiterer Erklärungsansatz geht davon aus, dass sich der Beiname „der Kurze“ auch aus dem Namen Pippin heraus angeboten hat, da „Pippin“ als „Kleiner“ (ursprünglich ein Kosename für ein kleines Kind) gedeutet werden kann. In der heutigen deutschen Geschichtswissenschaft ist der Beiname „der Kurze“ ungebräuchlich; allgemein wird von Pippin dem Jüngeren gesprochen, was auf seine Stellung als dritter und jüngster Hausmeier dieses Namens zurückgeht. Im Englischen ("Pepin the Short") und Französischen ("Pépin le Bref") ist die Bezeichnung dagegen noch heute geläufiger, vergleiche den Buchtitel von I. Gobry (2001).





</doc>
<doc id="4014" url="https://de.wikipedia.org/wiki?curid=4014" title="Lauch">
Lauch

Lauch ("Allium ampeloprasum" Lauch-Gruppe, Syn. "Allium porrum"), auch Porree, Breitlauch, Winterlauch, Welschzwiebel, Gemeiner Lauch, Spanischer Lauch, Aschlauch, Fleischlauch genannt, ist eine Sortengruppe des aus dem Mittelmeerraum stammenden Ackerlauchs ("Allium ampeloprasum"). Dieses Gemüse zählt zur Gattung "Allium" in der Unterfamilie der Lauchgewächse (Allioideae).

Es ist eine zweijährige krautige Pflanze, die Wuchshöhen von 60 bis 80 cm erreicht. Im Gegensatz zur Wildform hat Lauch keine Zwiebel. Die linealischen bis lanzettlichen Laubblätter sind 1 bis 5 cm breit.

Der Blütenstand steht auf einem glatten Blütenstandschaft und hat eine vielblättrige, lang geschnäbelte Hülle, welche länger als der große, kugelige, vielblütige, doldige Blütenstand ist. Die zwittrigen Blüten sind radiärsymmetrisch. Die weißen bis hellpurpurnen Blütenhüllblätter sind 4,5 bis 5 mm lang und 2 bis 2,3 mm breit. Es werden eirunde Kapselfrüchte gebildet. Manchmal werden im Blütenstand Brutknollen gebildet.

Der Lauch ist eine Kulturform des Ackerlauchs ("Allium ampeloprasum"), welcher wild im Mittelmeerraum vorkommt und in verschiedenen Formen kultiviert wird. Lauch war schon um 2100  v. Chr. bekannt. Der sumerische Herrscher Urnammu ließ ihn in den Gärten der Stadt Ur anbauen. Auch im alten Ägypten war Lauch in Verwendung. Nach Herodot soll er den Arbeitern, die die Pyramiden erbauten, als Nahrung gedient haben. Danach war Lauch in der Antike im gesamten Mittelmeergebiet geschätzt. Wahrscheinlich ist der Lauch im Mittelalter aus Italien nach Mitteleuropa gekommen.

Lauch wird hauptsächlich im Mittelmeerraum und in Europa angebaut, wobei in Deutschland der Schwerpunkt in Nordrhein-Westfalen liegt.
Der Anbau von Sorten für die Ernte im Herbst oder Winter erfolgt durch Aussaat in ein Freiland-Saatbeet von März bis April und in der Regel Verpflanzung auf den endgültigen Standort zwischen Ende Mai/Anfang Juni bis Ende Juli. Zwecks Ernte von Lauch bereits im Zeitraum zwischen Juni und August/September erfolgt die Aussaat in einem kalten Gewächshaus von Anfang Januar bis Anfang März an; die so gewonnenen Jungpflanzen werden zwischen Ende März und Ende Mai ins Freiland ausgepflanzt. Bei der Verpflanzung setzt man abhängig von der angestrebten Bestandsdichte die Lauchpflänzchen in Reihenabständen von 30 bis 50 cm und Pflanzenabständen zwischen 10 und 15 cm. Die Bestandsdichte je Hektar variiert zwischen 200.000 Pflanzen bei Frühsorten und 100.000 Pflanzen bei Wintersorten. Der Ertrag liegt bei Wintersorten um 25 t pro Hektar. Aufgrund der hohen Frosthärte der meisten Lauchsorten für den Winteranbau können diese oft über den Winter auf dem Feld verbleiben.

Als wichtigste und vor allem Qualität mindernde Schädlinge sind Zwiebelthripse ("Thrips tabaci"), die durch Saugtätigkeit die Blätter silbergrau werden lassen, und die Lauchmotte ("Acrolepiopsis assectella"), welche Löcher in Blatt und Lauchstange frisst, zu nennen. Seltenere Schädlinge sind Zwiebelminierfliege und Zwiebelfliege. Bei den Pilzkrankheiten sind es Porree-Rost ("Puccinia allii"), der durch rostige „Pusteln“, Papierfleckenkrankheit ("Phytophthora porri") durch papierartige Verfärbung der Blätter und Wachstumsverlust und Purpurfleckenkrankheit ("Alternaria porri") durch seine Blattflecken die Qualität mindern oder Lauch unverkäuflich machen. Seltener tritt auch der Pilz Samtfleckenkrankheit ("Cladosporium allii"), die Viruserkrankung Gelbstreifigkeit (' = LYSV, ' = SLV, oder "" = GLV) an Lauch auf, die durch mehrere Viren verursacht werden kann. Wichtig ist eine weit gestellte Fruchtfolge, die den Aufbau einer erhöhten Schadpopulation in einem Feld oder in einer Region verhindert. Dem kann im Hausgarten mit Mischkulturen etwas entgegengewirkt werden. Dabei spielen Nachbarpflanzen eine gewisse Rolle. Ein wichtiger Faktor hierfür ist die Allelopathie. Günstige Nachbarn wie Endivie, Erdbeere, Kamille, Kohl, Kohlrabi, Kopfsalat, Möhre, Schwarzwurzel, Sellerie und Tomate mindern, schlechte Nachbarpflanzen wie Bohne, Erbsen oder Rote Bete können Wachstum und Krankheitsdruck fördern.

Lauch wird sowohl als Gemüse (meist Winterlauch) als auch als Küchengewürz (meist Sommerlauch) genutzt; man kann ihn als Gemüse oder Salat kalt oder warm essen. Zusammen mit Karotten und Sellerie wird Lauch als Gewürz in Suppen als sogenanntes Suppengrün verwendet. Weitere Verwendung findet er auch in der nordhessischen Spezialität Speckkuchen oder als "Lauchtorte" in kalter und heißer Ausführung. 

Lauch führt bei der Verdauung – ähnlich wie Zwiebeln – zu Blähungen.

Lauch enthält unter anderem Vitamin C, Vitamin K und Folsäure sowie die Mineralstoffe Kalium, Calcium, Magnesium und die Spurenelemente Eisen und Mangan. Die Schwefelverbindung Propanthial-"S"-oxid, die aus Isoalliin und dem katalysierenden Enzym Alliinase entsteht, verursacht den intensiven Geruch und Geschmack von Lauch. Das Sulfoxid Cycloalliin kommt ebenfalls in diversen Laucharten vor.

Bis in die 1970er Jahre wurde im deutschsprachigen Raum hauptsächlich die Bezeichnung „Porrée“ benutzt, nur in der deutschsprachigen Schweiz, in Baden-Württemberg, Saarland, Rheinland-Pfalz, im südlichen Hessen und gestreut in Österreich und dem westlichen Bayern war die Bezeichnung „Lauch“ gebräuchlich. Die Bezeichnung „Lauch“ hat sich jedoch in den letzten Jahrzehnten ausgebreitet und ist mittlerweile beinahe so verbreitet wie „Porrée“.

Weitere zum Teil auch nur regional gebräuchliche Bezeichnungen für den Lauch sind oder waren: "Aeschlauch", "Bieslook" (niederdeutsch), "Biramsam" (mittelhochdeutsch), "Bolle" (Brandenburg), "Borren" (Brandenburg), "Burrä" (Oldenburg), "Burchon" (Oberhessen), "Burri" (Mecklenburg, Jever), "Burrei" (Butjerden), "Burren" (Altmark), "Burro" (Pommern), "Fristlich" (Erzgebirge), "Gemeinloch" (mittelhochdeutsch), "Kil" (mittelniederdeutsch), "Läuchel" (mittelhochdeutsch), "Lauchekyl", "Loek" (mittelniederdeutsch), "Lók" (mittelniederdeutsch), "Look" (Oldenburg, Ostfriesland), "Pfarr", "Pforisamo" (mittelhochdeutsch), "Pharren" (mittelhochdeutsch), "Phorro" (mittelhochdeutsch), "Phorsame", "Poré" (Siebenbürgen), "Pork" (mittelhochdeutsch), "Porlok" (mittelhochdeutsch), "Porsam", "Prei" (Oldenburg, Ostfriesland), "Prieslauch", "Priselocher", "Prö" (Lübeck) und "Slauch".

Lauch war im altgermanischen Ritual so heilig, dass er einer Rune den Namen gab. Auf den britischen Inseln soll er vom Britenkönig Cadwallader um 640 als Erkennungszeichen für seine Truppen verwendet worden sein. Porree ist eines der Nationalsymbole von Wales.

Ein Band mit Erzählungen Arno Schmidts von 1959 trägt den Titel "Rosen und Porree".



</doc>
<doc id="4015" url="https://de.wikipedia.org/wiki?curid=4015" title="Portugiesische Euromünzen">
Portugiesische Euromünzen

Die portugiesischen Euromünzen sind die in Portugal in Umlauf gebrachten Euromünzen der gemeinsamen europäischen Währung Euro. Am 1. Januar 1999 trat Portugal der Eurozone bei, womit die Einführung des Euros als zukünftiges Zahlungsmittel gültig wurde.

Portugiesische Euromünzen haben für jede der drei Münzreihen ein eigenes Motiv. Abgebildet werden drei königliche Siegel aus den Jahren 1134, 1142 und 1144, umgeben von – die sieben Schlachten Alfons III. gegen fünf maurische Könige symbolisierend – sieben Kastellen und fünf Wappenschilden sowie dem Wort "PORTUGAL" und dem Prägejahr. Alle Entwürfe stammen von Vítor Manuel Fernandes dos Santos, der mit "VS" signiert, und sind umgeben von den – vertieft geprägten – zwölf Sternen der EU. Die Randprägung aller 2-Euro-Münzen zeigt sieben Kastelle und fünf Schilde, die auch im Wappen Portugals symbolhafte Elemente sind.

Geprägt werden die Münzen in der „Imprensa Nacional – Casa da Moeda“ in Lissabon. Das Zeichen der Münzprägestätte – INCM – ist auf den Münzen über dem ersten Wappenschild von rechts zu finden. Die Münzen, die in den Jahren 1999–2002 geprägt wurden tragen das Ausgabejahr 2002. Die ab 2003 geprägten Münzen tragen das tatsächliche Prägejahr.

Die ab 2007 neu gestaltete Vorderseite der Euromünzen (neue Europakarte) wurde in Portugal erst 2008 eingeführt. Rund 100000 der insgesamt 5 Millionen 1-Euro-Münzen aus dem Jahre 2008 wurden noch mit der alten Europakarte auf der Vorderseite geprägt. 

Die Münzen ab 2011 weisen ein geringfügig verändertes Erscheinungsbild auf der Rückseite auf. Die Sterne sind nun etwas kleiner und die Schriftzüge sowie Burgen und Wappen wurden zarter gestaltet.

→ "Hauptartikel:" 2-Euro-Gedenkmünzen

Portugal hat bis heute folgende 2-Euro-Gedenkmünzen ausgegeben:

zukünftige Ausgaben











</doc>
<doc id="4016" url="https://de.wikipedia.org/wiki?curid=4016" title="Volksfront zur Befreiung Palästinas">
Volksfront zur Befreiung Palästinas

Die Volksfront zur Befreiung Palästinas (, , allgemein als PFLP abgekürzt) ist eine dem linken Flügel des Panarabismus zuzurechnende, 1968 gegründete Palästinenserorganisation, die politisch, terroristisch und militärisch aktiv ist. Die EU und die USA führen die PFLP auf ihren Listen von Terrororganisationen. Das britische Innenministerium rechnet sie im Jahr 2014 nicht mehr dazu.

Die PFLP wurde am 11. Dezember 1967 gegründet. Sie wurde in ihrer Anfangszeit von Wadi Haddad und George Habasch geführt.

Vor allem Mitte der 1990er-Jahre verlor die Organisation zunehmend an Einfluss. Mit dem Zerfall des Ostblocks und der Auflösung der Sowjetunion, sowie bedingt durch weitere Faktoren, wurden vor allem islamistische Kräfte wie die Hamas zusehends stärker und verdrängten die PFLP allmählich aus den Schlagzeilen.

Die Volksfront entstand aus der Vereinigung einiger linksgerichteter nicht-religiöser palästinensischer Organisationen, deren größte die palästinensische Sektion der Bewegung Arabischer Nationalisten war, und verband in ihrer Ideologie zunächst Elemente des Marxismus-Leninismus mit arabischem Nationalismus.

Das erklärte Ziel der Volksfront: „Die Befreiung ganz Palästinas im bewaffneten Kampf und die Errichtung eines demokratischen und sozialistischen palästinensischen Staates.“ Zu Beginn widersetzte sich die Volksfront jeglicher Lösung des israelisch-arabischen Konflikts, die zur Bildung zweier Staaten zwischen dem Mittelmeer und Jordanien führen sollte. Zu Beginn der 1990er-Jahre etwa spaltete sie sich von der PLO ab und bildete zusammen mit anderen Organisationen die sogenannte „Ablehnungsfront“ aus sich dem Oslo-Friedensprozess widersetzenden Organisationen, zu der unter anderem auch die DFLP und die islamistischen Organisationen Hamas und Islamischer Dschihad gehören. Die Gruppe hält an ihrem Widerstand gegen die Oslo-Abkommen fest und beharrt auf dem Rückkehrrecht der palästinensischen Flüchtlinge.

Auf taktischer Ebene bekennt sich die Volksfront zu spektakulären Terroraktionen, die die Aufmerksamkeit der Medien erregen und die Meinung der weltweiten Öffentlichkeit auf das palästinensische Problem lenken, wie beispielsweise Flugzeugentführungen sowie Attentate an zentralen Orten und Veranstaltungen.

Im April 2000 kündigte Habash seinen Rücktritt aus gesundheitlichen Gründen an. Habashs Nachfolger als Generalsekretär wurde am 8. Juli 2000 Abu Ali Mustafa, der am 27. August 2001 einer gezielten Tötung durch die israelische Armee zum Opfer fiel. Nach ihm benannte sich kurz danach der militärische Teil der PFLP Abu-Ali-Mustafa-Brigaden. Ein Kommando der Brigaden tötete kurz darauf als Vergeltung den israelischen Tourismusminister Rechaw’am Ze’ewi.

Während dieses Mordanschlages hatte bereits die zweite, Al-Aqsa-Intifada begonnen. Als Reaktion darauf forderte die israelische Regierung die Verhaftung des neuen Generalsekretärs Ahmad Saadat, des Chefs der "Abu-Ali-Mustafa-Brigaden", sowie einiger weiterer PFLP-Angehörigen, denen Israel die Planung bzw. Durchführung des Attentats vorwirft. Diesem Druck kam die Palästinensische Autonomiebehörde schließlich nach und ließ alle verhaften.

Währenddessen kämpften Brigaden-Einheiten in der zweiten Intifada aktiv mit. So waren die Abu-Ali-Mustafa-Brigade-Einheiten, obwohl kleiner als etwa die bewaffneten Kräfte der Hamas oder die Fatah-Milizen, an militanten und gewalttätigen Demonstrationen, direkten Kampfhandlungen und Schießereien mit der israelischen Armee, an Selbstmordanschlägen und Attentaten führend beteiligt.

Das Politbüromitglied Jamil al-Majdalawi sagte zur Beteiligung der PFLP an der Intifada 2001 zusammenfassend:
Während der Amtszeit des früheren palästinensischen Ministerpräsidenten Mahmud Abbas wurde im Sommer des Jahres 2003 eine Waffenpause ausgehandelt, an der sich jedoch einige Organisationen, unter ihnen auch die PFLP, nicht beteiligten.

Nach Arafats Tod im November 2004 bemühte sich die PFLP um eine gemeinsame Kandidatur mit der DFLP und der Palestinian People's Party und führte Gespräche mit diesen Gruppen, die aber zu keiner Einigung führten. Infolgedessen unterstützte die PFLP dann bei der Präsidentenwahl 2005 die Palästinensische Nationale Initiative um deren Spitzenkandidaten Mustafa Barghuti. Dieser gewann dann insgesamt etwa 19,5 Prozent der Stimmen und wurde damit hinter "Mahmud Abbas" zweitstärkster Kandidat.

Bei den Wahlen zum palästinensischen Parlament am 25. Januar 2006 trat die PFLP mit der Liste "Märtyrer Abu Ali Mustafa" an und erhielt 4,25 Prozent der Stimmen. Dabei gewann sie die meisten Prozente in ihren Hochburgen in Bethlehem 9,4 Prozent, in Ramallah 6,6 Prozent, gefolgt von Nord-Gaza mit 6,5 Prozent. Mit diesem Ergebnis ist sie drittstärkste Partei im Parlament und mit drei Abgeordneten vertreten: dem in israelischer Haft sitzenden Generalsekretär Ahmad Saadat, Jamil al-Majdalawi und Chalida Dscharrar.

An der Regierung „der nationalen Einheit“, aus Hamas, Fatah, DFLP und anderen kleineren linken und bürgerlichen Gruppen, beteiligte sich die PFLP als einzige Oppositionspartei im Parlament nicht.

Die Abgeordnete für die PFLP Chalida Dscharrar sagte dazu in einem Interview im April 2007:

Weiterhin beteiligte sich die PFLP bzw. ihr militärischer Arm an Anschlägen und Angriffe gegen israelische Ziele. Dazu gehörten beispielsweise die Entführung von vier Amerikanern und zwei Franzosen 2006 sowie Schießereien und Angriffe auf israelische und westliche Gebäude, die alle im Zusammenhang mit dem in Haft sitzenden Ahmad Saadat und anderen PFLP-Mitgliedern stehen. Auch während der Stürmung des Gefängnisses durch die israelische Armee selbst, kam es zu Gefechten zwischen den inhaftierten PFLP-Gefangenen und den Streitkräften Israels.

Im Laufe der israelischen Angriffe und Operationen gegen Ziele im Gaza-Streifen 2007 verstärkten die Abu-Ali-Mustafa-Brigaden ihre militärischen Aktivitäten.

Im Laufe der Geschichte haben sich einige Organisationen abgespalten:
1968, kurz nach der Gründung der PFLP spaltete sich von der jungen Organisation die PFLP-GC um Ahmad Dschibril ab. Die PFLP-GC stand damals für eine eher auf den praktischen bewaffneten Kampf ausgerichtete Gruppe und verstand sich damit als Gegensatz zur – von ihr als zu stark theoretisch orientierten – PFLP. Sie blieb immer kleiner als PFLP oder DFLP, ist aber durch ihre stärkere Radikalität bekannt.

Die Gruppe hat ihr Haupteinflussgebiet eher in den Flüchtlingslagern in Syrien und wird auch bis heute von Syrien unterstützt. Heute ist die Gruppe vor allem in libanesischen Flüchtlingslagern aktiv und soll in einigen sogar die Oberhand haben. Nach einigen Angaben soll es zu kleineren Kooperationen mit der in dem gleichen Gebiet aktiven Hisbollah gekommen sein, vor allem während des zweiten Libanonkrieges.

Zuerst nannte sich die 1969 abgespaltene Fraktion "„Volksdemokratische Front zur Befreiung Palästinas“ (PDFLP)". Erst 1974 wurde der Name in DFLP geändert. Grundsätzlich unterschieden sich die beiden Fraktionen nicht viel voneinander. Knackpunkt der internen Fraktionskämpfe zwischen den sogenannten "Progressiven (DFLP)" und "Historikern (PFLP)" war letztlich vor allem die theoretische Frage über den bewaffneten Kampf, zum Teil nicht als solches, obwohl die eher Moskau-orientierte DFLP viel weniger militant ist, sondern eher um dessen Stellung in taktischen Fragen.

Während des Jahres 1972 spitzten sich die internen inhaltlichen Konflikte wieder zu. An dessen Ende stand die Abspaltung der Gruppe der linken Fraktion um den damaligen militärischen Verantwortlichen für den Süd-Libanon "Salim Darduna" von der PFLP, die sich dann PRFLP nannte. Etwa 150 PFLP-Mitglieder sollen dieser Gruppe angehört haben. Die PRFLP konnte aber niemals nennenswert Einfluss gewinnen.

Weiterhin stellte auch die vom PFLP-Mitgründer Wadi Haddad (Abu Hani) geleitete PFLP-EO eine Abspaltung dar – zumindest aber eine autonome Gruppe innerhalb der PFLP. Nachdem die von der PFLP verantworteten Flugzeugentführungen vom September 1970 zu heftigen anti-palästinensischen Reaktionen innerhalb der arabischen Welt beigetragen hatten (s. Schwarzer September), distanzierte sich die Führung um Habash vom internationalen Terrorismus als Instrument der Kriegsführung, woraufhin Haddad seine folgenden Aktionen auf eigene Verantwortung organisierte und durchführte. Die Haddad-Gruppe arbeitete eng mit linksrevolutionären Guerillagruppen aus verschiedenen Teilen der Welt zusammen und wurde durch außerhalb des Nahen Ostens begangene Terroraktionen bekannt, darunter die Flugzeugentführungen von Entebbe 1976 und Mogadischu 1977 und der Überfall auf die OPEC-Konferenz in Wien 1975. 1976 wurde Haddad wegen seiner fortgesetzten Flugzeugentführungen aus der PFLP ausgeschlossen. Die von der westdeutschen Antiterror-Spezialeinheit GSG 9 der Bundespolizei beendete Flugzeugentführung nach Mogadischu war die letzte Aktion der Haddad-Gruppe, deren Anführer im März 1978 starb.

Seit der gezielten Tötung von Abu Ali Mustafa durch die israelischen Streitkräfte ist Ahmad Saadat Generalsekretär der PFLP. Dieser sitzt seit der Gefangennahme zusammen mit anderen PFLP-Aktivisten durch palästinensische Sicherheitskräfte vorher in palästinensischer Haft, zurzeit in israelischer Haft.

Ein Großteil der Anhänger der PFLP ist im zentraleren Westjordanland, vor in allem Ramallah, Bethlehem und in den Außenbezirken Ostjerusalems, weiterhin auch in Gaza bzw. Nord-Gaza oder Nablus zu finden. Ein großer Teil gerade auch der Kämpfer für die Abu-Ali-Mustafa-Brigaden kommt aus dem studentischen Umfeld aus den Universitäten in Ostjerusalem, Ramallah, Nablus und Dschenin.

Die Zahl der Mitglieder schwankt nach verschiedenen Quellen. So soll es nach einigen CIA-Angaben nur etwa 800 Mitglieder der PFLP geben. Wahrscheinlicher sind Angaben über mehrere Tausend in der PFLP. Nach Angaben in der FAZ soll die PFLP 2004 etwa 3000 Aktivisten haben und damit die größte Organisation der Linken, vor etwa der DFLP, sein.
Gesichert scheint außerdem, dass es einige hundert Mitglieder und Kämpfer der PFLP und ihrer Organisationen zurzeit in israelischer Haft gibt. Für die Gefangenenbetreuung der PFLP-Angehörigen ist auch die palästinensische Gefangenenorganisation "Adameer" verantwortlich.

Die Organisation hat eine eigene Zeitung mit dem Namen "al-Hadaf" (deutsch: "Das Ziel") und weitere Publikationen, sowie eine englische und arabische täglich aktualisierte Internetpräsenz. Das englischsprachige Blatt namens "Democratic Palestine" musste in den 1990er-Jahren wegen Geldmangel eingestellt werden. Die Organisation baut ihre Beziehungen zum Iran aus, indem sie den "Bannerträger im Kampf gegen Israel und für die Befreiung Palästinas" sieht.

Die PFLP besitzt auch eine Reihe von Nebenorganisationen oder von ihr beeinflusste Zusammenschlüsse wie die studentische "Progressive Student Action Front", die Jugendorganisation "Palestinian Progressive Youth Union", Frauengruppen sowie die Gefangenen-Solidaritätsorganisation "Adameer".

Weiterhin arbeiten viele Mitglieder in Kulturvereinen und vor allem Gewerkschaften aktiv mit. Teilweise sind bzw. sollen PFLP-Mitglieder oder Sympathisanten auch als Schuldirektoren oder Bürgermeister aktiv. So soll beispielsweise die Bürgermeisterin von Ramallah, Janet Micha'il, Verbindungen zur PFLP haben oder Mitglied sein, während der Bürgermeister von Bethlehem, Victor Batarseh, langjähriges Mitglied ist. Ein weiteres Beispiel ist der langjährige Schuldirektor und PFLP-Kader Imad Abd al-Aziz.


Der bewaffnete Kampf spielte in der PFLP immer eine entscheidende Rolle. Die PFLP verstand sich immer als revolutionär, also auch militant kämpfende, Organisation. Zur Geldbeschaffung arbeitete die PFLP zeitweise mit der dänischen Untergrundgruppe Blekingegadebanden zusammen.

Seit der Tötung von Abu Ali Mustafa durch die israelische Armee und seit der Al-Aqsa-Intifada bilden die Abu-Ali-Mustafa-Brigaden den Arm für militärische Operationen. Kommandierender ist der schon früher im militärischen Arm der PFLP arbeitende Ahad Yusuf Musa Olma.
Die Brigaden, die in den gesamten besetzten Gebieten vertreten sind, traten vor allem während der Al-Aqsa-Intifada, dem Kampf 2006 um das Gefängnis, in dem "Ahmad Saadat" saß, sowie seit den Operationen Israels gegen den Gaza-Streifen ab 2007 stark mit Selbstmordanschlägen, Kassam-Raketenangriffen, Verschleppungen, Angriffen auf Streitkräfte Israels und den Beschuss von mehrheitlich rechts-gerichteten jüdischen Siedlungen in Erscheinung. Neuerdings machte sie mit Drohungen gegen Mitglieder der Knesset auf sich aufmerksam.

Historisch hatte die RAF gute Beziehungen zur PFLP. In Deutschland gibt es zahlreiche Veranstaltungen und Veranstaltungsversuche zur Unterstützung und Rekrutierung für die PFLP, was Deutschland bereits in der internationalen Presse in die Kritik brachte. Abgeordnete der Knesset und des Bundestages fordern in einem Schreiben an Bundesinnenminister Thomas de Maizière ein Betätigungsverbot der Organisation in Deutschland zu erlassen. Auseinandersetzungen um Raumanmietungen für Unterstützungsveranstaltungen im Haus des Neuen Deutschland führen immer wieder zu Auseinandersetzungen.

Nach Einschätzung des Bundesamts für Verfassungsschutz ist die PFLP in Deutschland bisher nicht nennenswert in Erscheinung getreten, wenn auch vereinzelt über Benefizveranstaltungen und Demonstrationsteilnahmen berichtet wird. Daneben gab es auch eine Reihe von Auftritten bzw. Auftrittsversuchen von PFLP-Mitgliedern. Das baden-württembergische Innenministerium stellte im September 2017 in Beantwortung einer Kleinen Anfrage im Landtag fest, dass die PFLP vom Landesamt für Verfassungsschutz beobachtet werde, 2017 jedoch keine verfassungsfeindlichen Aktionen bekannt geworden seien. 

Ende August 2017 berichteten deutsche und israelische Medien, dass die PFLP bzw. einzelne ihrer Repräsentanten über eine „Internationalistische Liste“ mit der linksradikalen Splitterpartei Marxistisch-Leninistische Partei Deutschlands (MLPD) für die Wahl zum Bundestag kandidieren würde und dass der damalige bündnisgrüne Bundestagsabgeordnete Volker Beck die Kooperation von MLPD und PLFP scharf kritisiere. Zu einer entsprechenden Gegendarstellung der MLPD räumte die Zeitung "Die Welt" jedoch ein, dass keine PFLP-Mitglieder auf den MLPD-Landeslisten angetreten seien. Tatsächlich aber gehörten „Sympathisanten der PFLP“ zum „Internationalistischen Bündnis“, das im Wahlkampf mit der MLPD kooperierte. Eine im September von der MLPD erwirkte einstweilige Verfügung gegen Becks Äußerungen im Bundestagswahlkampf hob das Landgericht Hamburg im Dezember 2017 auf.

Nachfolgend eine unvollständige Liste von Terroranschlägen unter Beteiligung von Mitgliedern der PFLP:





</doc>
<doc id="4017" url="https://de.wikipedia.org/wiki?curid=4017" title="Portugal">
Portugal

Die Republik Portugal (amtlich "") ist ein europäischer Staat im Westen der Iberischen Halbinsel. Als westlichster Punkt Kontinentaleuropas wird das Land im Osten und Norden von Spanien begrenzt, im Westen und Süden dagegen vom Atlantischen Ozean. Zum portugiesischen Staatsgebiet gehören die Inseln der Azoren und Madeira (mit Porto Santo).

Das im 12. Jahrhundert gegründete Königreich Portugal stieg im 15. Jahrhundert zu einer Großmacht auf und spielte eine wesentliche Rolle im Zeitalter der Entdeckungen. Das Königreich schuf eines der größten Kolonialreiche mit Besitzungen in Afrika, Asien und Südamerika, dessen Niedergang jedoch bereits im Laufe des 17. Jahrhunderts eingeläutet wurde. 1910 kam es durch einen militärischen Aufstand zum Sturz der portugiesischen Monarchie, der König Manuel II. ins Exil zwang. Die Erste Portugiesische Republik trat am 5. Oktober 1910 in Kraft und bestand bis zum Militärputsch von General Gomes da Costa im Jahr 1926. Danach stand das Land für mehr als vierzig Jahre unter der autoritären Diktatur von António de Oliveira Salazar. Die Nelkenrevolution vom 25. April 1974 führte zum Sturz des Regimes und eröffnete den Weg zur demokratischen Dritten Republik.

Portugal ist Gründungsmitglied der NATO (1949) und der OECD (1948) und zudem Mitglied der Vereinten Nationen (seit 1955), des Europarats (seit 1976) und des Schengener Abkommens (seit 1991).
Zum 1. Januar 1986 trat Portugal zusammen mit Spanien den Europäischen Gemeinschaften (EG) bei, aus denen später die EU entstand. Portugal galt damals als ärmstes Land der EG. Portugal zählt außerdem zu den Gründungsstaaten der Eurozone. Nach Einführung des Euro (1999) sanken die realen Zinsen. Große Kapitalzuflüsse trugen zu Jahren des Wirtschaftswachstums bei; gleichwohl sank die Produktivität. Die Banken- und Finanzkrise seit 2008 hatte auch in Portugal Auswirkungen auf die Realwirtschaft (Rezession). Im Rahmen der Eurokrise wurde Portugal zu den besonders betroffenen PIIGS-Staaten gezählt. Die EFSF sagte Portugal im Mai 2011 Finanzhilfen in Höhe von 78 Milliarden Euro zu. Seither hat sich die wirtschaftliche Lage wieder erholt, während die Krise hohe soziale Folgekosten verursacht hat: Die Arbeitslosenquote im April 2017 betrug 9,8 Prozent, das BIP je Einwohner betrug 2016 16.900 Euro, die Staatsverschuldung war Ende 2016 1,30 mal so hoch wie das BIP des Jahres 2016.

Für lange Zeit war Portugal ein Auswanderungsland; wichtige Zentren der portugiesischen Kultur in der Diaspora gibt es heute in Frankreich und in den USA (wo jeweils etwa 1 bis 2 Millionen Portugiesen leben) sowie in Brasilien, Angola, Mosambik, der Schweiz und Luxemburg. Wichtigste bilaterale Partnerländer sind Brasilien und Spanien.

Der Tourismus ist eine wichtige Einnahmequelle. Mit 17 Millionen Touristen pro Jahr (2015) gehört Portugal zu den meistbesuchten Ländern der Welt, häufigste Reiseziele sind die Algarve und die Region um die Hauptstadt Lissabon. Das Land ist daneben für den Weinanbau bekannt, insbesondere für den Portwein, und als weltweit bedeutendstes Produktionsland für Rohkork.

Der Name "Portugal" entstammt dem vom Römischen Imperium angelegten Hafen Porto, (lateinisch bedeutet „Hafen“). Was mit "Cale" gemeint ist, ist umstritten. Die meisten Gelehrten meinen, "Cale" beziehe sich auf die Gallaeker (, lateinisch oder ) – „Hafen der Galläker“. Andere meinen, es handele sich um ein Überbleibsel von lateinisch , was „warm“ bedeutet – „Warmer Hafen“. Es gibt auch Historiker, die vermuten, die Griechen hätten als Erste dort gesiedelt und das altgriechische Wort für „schön“ sei namensgebend gewesen – „Schöner Hafen“. Im Mittelalter wurde zu "Portucale", später "Portugale", wobei der Name im 7. und 8. Jahrhundert nur die nördlichen Teile des Landes bezeichnete, also die Region zwischen den Flüssen Douro und Minho.

Im Westen und Süden wird Portugal vom Atlantischen Ozean begrenzt, im Osten und Norden von der Landesgrenze mit Spanien. Letztere ist 1214 km lang.

Der Norden Portugals hat ein relativ kühles und feuchtes Klima und besteht aus zwei traditionellen Provinzen oder Landschaften: Der Minho im Nordwesten gehört zu den am dichtesten besiedelten Gegenden des Landes. Die größten Städte des Minho sind Braga und Viana do Castelo. Der Minho wird wegen seines Klimas und der vergleichsweise üppigen Vegetation als der "grüne Garten" Portugals bezeichnet. Auf den Hängen der zahlreichen Flusstäler wird vor allem Wein kultiviert, der zum berühmten Vinho Verde weiterverarbeitet wird. Daneben gedeihen viele Gemüsesorten. Die natürliche Vegetation ist eine Mischung aus der Flora der gemäßigten Klimazone und der subtropischen Flora; je nach Höhenlage gibt es Eichen und Kastanien oder Pinien und Olivenbäume.

Im Nordosten liegt Trás-os-Montes ("Hinter den Bergen"). Dies ist die dem Meer abgewandte Seite Nordportugals, die sehr gebirgig ist und deshalb sehr kalte Winter und sehr heiße Sommer hat. Die Vegetation ist bedeutend weniger üppig als im Minho und wird zur Grenze nach Spanien hin spärlicher. Beiden Provinzen ist gemein, dass ihre Gebirgsmassive, wie z. B. Marão oder Peneda-Gerês von zahlreichen Flüssen, wie dem Rio Minho (Grenzfluss zu Spanien) oder dem Rio Douro, durchschnitten werden. Im Norden Portugals liegt der Nationalpark Peneda-Gerês, ein wichtiges Schutzgebiet des Landes. Dort existieren noch Restbestände naturbelassener Wälder, in denen sich insbesondere die immergrüne Steineiche findet. Bedeutende Städte des Nordens sind Porto, Vila Nova de Gaia, Matosinhos, Braga, Vila Real und Bragança.

Mittelportugal ist großenteils hügelig bis gebirgig und hat mit der "Serra da Estrela" ein beachtliches Gebirge mit Wintersportmöglichkeiten. Der höchste Berg ist der Torre mit . Er ist die höchste Erhebung Kontinentalportugals. Die wichtigsten Landschaften sind die Beira, der Ribatejo (die Tejo-Ebene mit Beinamen "Garten Lissabons"), die Estremadura sowie die Mündung des Tejo in den Atlantik. Die gesamte Region ist sehr fruchtbar und hat ein für den Weinanbau günstiges Klima. Die Tradition des Weinbaus reicht bis zu den Römern zurück. Daneben werden Getreide, Reis, Sonnenblumen und Gemüse angebaut. Die Region wird durch den Tejo geteilt. Seit dem Bau zahlreicher Staudämme kommen Überschwemmungen, die früher den Ribatejo regelmäßig heimsuchten, kaum mehr vor. Die wichtigsten Städte Mittelportugals sind Lissabon, Aveiro, Sintra, Coimbra, Viseu, Leiria, Castelo Branco sowie Santarém.

Der Süden Portugals setzt sich aus den drei Landschaften Terras do Sado, Alentejo und Algarve zusammen. Die Oberfläche der gesamten Region ist eben bis hügelig und hat ein trockenes und heißes Klima. Der Alentejo, die frühere Kornkammer Portugals, ist heute nur dünn besiedelt und von Abwanderung gekennzeichnet; weitläufige Getreidefelder mit Olivenhainen und Korkeichen dominieren die Landschaft. Zum Weinanbau kommen als Hauptprodukte Getreide und Sonnenblumen. Die Wiesen dienen zur Schafzucht und sind im Frühling mit Blumen übersät. Zum wirtschaftlichen Niedergang tragen nicht zuletzt die länger werdenden Trockenperioden bei, die mit dem Bau von Staudämmen gemildert werden sollen. Umstritten sind die Anpflanzungen von schnell wachsenden Eukalyptusbäumen. Diese stellen ein erhöhtes Risiko für Waldbrände dar, trotzdem nehmen die Anbauflächen zu. Die südlichen Küstenregionen sind häufig von Kiefernwäldern bewachsen. Daneben finden sich zahlreiche Palmenarten, von denen nur die Zwergpalme einheimisch ist.

Die Algarve markiert die gesamte Südküste des Landes und ist mit ihren hübschen Städten, den Steilküsten und Sandstränden zu einem beliebten Feriendomizil geworden, was übliche Begleiterscheinungen wie Massentourismus mit sich gebracht hat. Die größten Städte Südportugals sind Portalegre, Évora, Beja sowie Faro und Lagos. Der bestimmende Fluss ist der Rio Guadiana, der zweimal ein längeres Stück die Grenze zu Spanien bildet. An die große sommerliche Hitze angepasst sind zahlreiche sukkulente Pflanzen.

Zu Portugal gehören des Weiteren die Inselgruppen Madeira ("Holzinsel") und Azoren ("Habichtsinseln") im Atlantik. Sie sind, bis auf die Azoreninsel Santa Maria, vulkanischen Ursprungs. Die Inselgruppe Madeira vor der afrikanischen Küste hat eine teils tropische, teils subtropische Vegetation. Der höchste Berg Portugals befindet sich auf den Azoren (Ponta do Pico, ).

Die wichtigsten Flüsse Portugals sind der Tejo, welcher in Spanien unter dem Namen "Tajo" entspringt, der Douro (spanisch "Duero") und der Mondego, wobei letzterer nur durch Portugal fließt.

"Siehe auch": Liste der Städte in Portugal; Liste portugiesischer Inseln.

Fauna

Die Tierwelt Portugals unterscheidet sich nur unwesentlich von der Spaniens. Vereinzelt leben hier noch Wölfe. Der nur auf der Iberischen Halbinsel verbreitete Pardelluchs ist in Portugal nahezu ausgestorben; selten werden Einzeltiere angetroffen, die vermutlich über die Grenze zu Spanien eingewandert sind. Ansonsten finden sich Wildkatzen, Füchse, Wildschweine, Hirsche, wilde Ziegen sowie Wildkaninchen. Da Portugal auf der Zugvogelroute nach Afrika liegt, lassen sich zahlreiche Vögel beobachten, darunter insbesondere im Süden die Flamingos; Steinadler leben und jagen in den Küstengebieten. Im Landesinneren kommen verschiedene Schlangen und Skorpione vor.

Portugal ist, bezogen auf die alteingesessene Bevölkerung, in sprachlicher, ethnischer und religiöser Hinsicht ein sehr homogenes Land. Die portugiesische Sprache wird im ganzen Land gesprochen und nur in den Dörfern von Miranda do Douro wird ein dem Asturischen zugeordneter Dialekt (Mirandés) gesprochen, der als Minderheitensprache anerkannt wird. Die größte einheimische ethnische Minderheit bilden 40.000 bis 50.000 Roma, die sozial und ökonomisch marginalisiert sind.

Die dichteste Besiedelung weist ein Küstenstreifen von der spanischen Grenze im Norden bis in die Gegend um Lissabon auf. In diesem Streifen leben 70 % der Bevölkerung; das Hinterland und der Süden Portugals dagegen sind nur dünn besiedelt. Mehr als 10 % der Bevölkerung entfällt auf zwei Städte (Lissabon und Porto), während mehr als die Hälfte in Orten unter 2.000 Einwohnern lebt. Der Trend geht in Portugal in Richtung Verstädterung.

Die in Portugal ansässige Bevölkerung wurde für Ende Dezember 2008 auf 10.627.250 Personen geschätzt. Damit hat sich die Bevölkerung seit 1900 verdoppelt. Das Bevölkerungswachstum war dabei keineswegs konstant. Einem Bevölkerungsrückgang im Jahre 1920 aufgrund der Auswirkungen des Ersten Weltkrieges, der Spanischen Grippe und einer Auswanderungswelle folgte eine Wachstumsphase, die bis in die 1940er-Jahre dauerte und die von steigender Lebenserwartung der Menschen profitierte. Von etwa 1965 bis 1973 kam es zu starker Auswanderung. 1974 migrierten wegen der Unabhängigkeit der Kolonien viele Menschen nach Portugal. Die Auswanderung der 1980er-Jahre kam in den 1990ern zum Stillstand.

Das natürliche Bevölkerungswachstum war 2007 leicht negativ, 2008 stagnierte es. Das leichte Gesamtwachstum, das 2008 0,09 % und 2007 0,17 % betrug, ist nur auf die Nettomigration zurückzuführen. 2003 noch war die Bevölkerung um 0,64 % gewachsen. Die Geburtenrate, die vor 1920 noch bei 30 pro 1000 Einwohnern lag, ist bis 2008 auf 9,8 pro 1000 Einwohner gesunken. Die Fruchtbarkeitsrate erreichte 2007 mit nur 1,33 Kindern pro Frau den tiefsten Wert in der Geschichte des Landes. In den 1960er-Jahren bekam eine Frau im Durchschnitt drei Kinder. Regional gibt es hinsichtlich der Bevölkerungsentwicklung erhebliche Unterschiede: Während die Bevölkerung der Algarve, Lissabons und der Azoren wächst, geht jene des Alentejo und der Region Centro zurück. Innerhalb Portugals gibt es starke Migrationsbewegungen, wobei die Wanderungsbewegungen aus den Regionen des Hinterlandes in Richtung der Zentren von Industrie (Lissabon, Porto) und Tourismus (Algarve, Madeira und Azoren) gehen.

Die Bevölkerung Portugals altert: 2008 waren 15,3 % der Bewohner 15 Jahre oder jünger, während 17,6 % 65 Jahre oder älter waren. Dieser Trend ist im Hinterland besonders ausgeprägt, der Grund ist die Abwanderung der jüngeren Bewohner in die Ballungszentren. Da zahlreiche Portugiesen, die im Ausland gearbeitet haben, für ihren Lebensabend in ihre Heimat zurückziehen, ist die Alterung der Bevölkerung des Landes besonders prägnant. Die Lebenserwartung bei Geburt lag 2015 bei 83,5 Jahren für Frauen und 77,4 Jahren für Männer. Im Jahre 1970 hatte sie noch bei 70,3 bzw. 64,0 Jahren gelegen. Parallel dazu ging die Kindersterblichkeit zurück: 2008 lag sie bei 3,3 pro Tausend Lebendgeborenen, während sie 100 Jahre davor noch um 150 pro Tausend Lebendgeborenen gelegen hatte.

Das durchschnittliche Heiratsalter ist in den letzten Jahren gestiegen, 2008 heirateten Frauen zum ersten Mal mit 26,8 und Männer mit 29,7 Jahren. Dabei sank der Anteil kirchlicher Ehen rapide, von 59,6 % im Jahre 2003 auf 44,4 % 2008.

Innerhalb von zehn Jahren schrumpfte die Bevölkerungszahl Portugals um 45.000 Menschen (von 10,473 Mio. auf 10, 427 Mio).

Lange Zeit war Portugal ein Auswanderungsland; wichtige Zentren der portugiesischen Kultur in der Diaspora gibt es vor allem in Frankreich, wo allein 1.132.048 Portugiesen leben, aber auch in vielen anderen Staaten, insbesondere Brasilien, Südafrika, Venezuela, Schweiz, an der Ostküste der USA, und zuletzt verstärkt in Angola. In Luxemburg lebten 2012 81.274 Portugiesen, damit stellten sie 16 Prozent der Bevölkerung Luxemburgs. Andererseits war Portugal schon während der Unabhängigkeitskriege seiner Kolonien Zielland für Einwanderer aus den kolonisierten Regionen.

Seit dem Beitritt Portugals zur Europäischen Union 1986 und dem damit verbundenen politischen und wirtschaftlichen Wandel ist Portugal verstärkt zu einem Einwanderungsland geworden, wobei die Herkunftsländer der Zuwanderer vor allem in Afrika (Angola, Kap Verde, Guinea-Bissau), Südamerika (Brasilien) sowie in Osteuropa (Ukraine, Russland und Republik Moldau) liegen.

Ende 2008 lebten 443.102 ausländische Staatsangehörige in Portugal. Diese stammen zu mehr als der Hälfte aus anderen portugiesischsprachigen Ländern, sind meist katholischen Glaubens und haben deshalb einen ähnlichen kulturellen Hintergrund. Etwa ein Viertel der Ausländer, die in Portugal leben, sind Europäer, wovon ein Teil Rückkehrer sind, also aus Portugal ehemals ausgewanderte Portugiesen, die mit fremder Staatsbürgerschaft zurückgekehrt sind. Ein anderer Teil sind Dauerurlauber, die in Portugal ihren Ruhestand verbringen.

Die ausländische Bevölkerung lebt zu mehr als der Hälfte in Lissabon, davon abgesehen konzentriert sie sich auf die Stadtgebiete an der Küste. Im Hinterland liegt der Anteil bei unter 0,5 %.

Die große Mehrheit der Portugiesen bekennt sich zum römisch-katholischen Glauben, wobei der Anteil an der Gesamtbevölkerung mit Werten zwischen 85 % und 95 % angegeben wird.

In Portugal herrscht Glaubensfreiheit und, seit Einführung des „Gesetzes über die Glaubensfreiheit“ ("Lei da Liberdade Religiosa") offiziell auch Gleichheit zwischen den Religionen. Die Gleichheit ist aber in der Realität noch nicht erreicht: die katholische Kirche betreibt in Portugal bedeutende Kultureinrichtungen, eine angesehene Universität, Privatschulen und auch einen Radiosender. Weiter ist das Gesetz über die Glaubensfreiheit nur teilweise auf die katholische Kirche anzuwenden. Ob öffentliche Schulen verpflichtet sein sollen, Religionsunterricht anzubieten, ist in Portugal seit 25 Jahren umstritten.
In der ersten Verfassung Portugals (1822) wurde der Katholizismus zur Staatsreligion erklärt. Die Verfassung von 1826 schaffte die religiöse Verfolgung ab. Die offizielle Trennung von Staat und Kirche erfolgte mit der republikanischen Revolution von 1910, wobei Konkordate mit dem Vatikan der katholischen Kirche weiterhin weitreichende Privilegien einräumten.
Die portugiesische Ausprägung des Katholizismus wird, im Vergleich zur Praxis in anderen Ländern, als „menschlich, lyrisch und mit Verständnis für die fleischlichen Dinge des Lebens“ beschrieben. Typisch ist die starke Verehrung der Jungfrau Maria. Wichtigstes Pilgerziel ist der Wallfahrtsort Fátima. Hier soll die Jungfrau Maria 1917 drei Hirtenkindern erschienen sein.

Im Mittelalter spielten zwei weitere Religionen eine bedeutende Rolle in Portugal. Muslimische Mauren und Araber beherrschten zeitweise den Süden des Landes. Nach der Reconquista mussten sie das Land verlassen oder sich den Christen unterwerfen. Sie brachten zahlreiche technische Fortschritte mit sich, wie Verbesserungen im Brunnenbau, Bewässerung, Olivenanbau, Anbau von Zitrusfrüchten, Baumwolle und Zuckerrohr, die Seidenraupenzucht, die Herstellung von Fliesen, Jalousien, Hygiene und Ornamentik. Die Gesellschaft im damaligen Portugal bot auch unterworfenen oder versklavten Mauren die Möglichkeit, gesellschaftlich aufzusteigen; die muslimische Bevölkerung ging in der christlichen auf. Die Juden genossen im Mittelalter den Schutz der portugiesischen Könige. Das durch Handel und Verwaltungsposten in Staat und Kirche erworbene und gesparte Vermögen diente als Grundlage für den Aufbau der portugiesischen Flotte. 1504 und 1506 kam es in Lissabon zu anti-jüdischen Pogromen.

Laut einer repräsentativen Umfrage des Eurobarometers glaubten im Jahr 2005 81 % der Menschen in Portugal an Gott, 12 % glaubten an eine andere spirituelle Kraft. 6 % Prozent der Befragten glaubten weder an einen Gott noch an eine andere spirituelle Kraft, 1 % der Befragten war unentschlossen.

Es wird geschätzt, dass Portugal bereits vor 500.000 Jahren besiedelt wurde. Das älteste in Portugal gefundene menschliche Fossil stammt von Neandertalern, die vor etwa 100.000 Jahren Portugal bewohnten. Felszeichnungen aus der Altsteinzeit, die die weltweit bedeutendsten ihrer Art sind, werden auf ein Alter von 10.000 bis 25.000 Jahren geschätzt. Der Übergang zur Jungsteinzeit erfolgte zwar spät, dafür setzte sich speziell in Südportugal die Kupferverarbeitung besonders schnell durch. Erste Handelsbeziehungen mit anderen Regionen Europas sind für diese Zeit belegt. Ab 800 v. Chr. gründeten Phönizier Handelsstützpunkte an der Algarve. Ab etwa 600 v. Chr. gründeten Griechen im östlichen und nordöstlichen Bereich mehrere kleine Siedlungen. Frühestens ab dem 6. bis 3. Jh. v. Chr. wanderten in mehreren Wellen Kelten ein, die sich mit den Iberern vermischten. Neben den Kelten wird der Stamm der Lusitaner genannt, der den Römern als besonders wehrhaft galt und im Lateinischen namensgebend für das Land werden sollte. Die Vermischung der Kelten mit der einheimischen Kultur erschuf die Keltiberer.

Ab 450 v. Chr. wurde die südliche iberische Halbinsel von Karthago kolonisiert. Bis 206 v. Chr. gelang es den Römern, die Karthager zu vertreiben. Im Verlaufe des Zweiten Punischen Krieges, an dem zahlreiche lusitanische Söldner von Karthago eingesetzt wurden, kam es zu einer Gegeninvasion Roms auf der iberischen Halbinsel und damit zu einer Romanisierung. Von den Römern wurde das Territorium Portugals zunächst als Provinz Hispania ulterior, ab der Regierungszeit von Augustus unter dem Namen "Lusitania" verwaltet, die neben dem Großteil des heutigen Portugal weitere Gebiete im Westen des heutigen Spanien umfasste. In Nord- und Nordostportugal trafen die römischen Eroberer auf starken Widerstand; erst ab 19 v. Chr. galt die Region als unterworfen. Danach kam es zu starker Romanisierung, Städte nach römischem Vorbild, Römerstraßen, Villae und Bergwerke entstanden, mit den Siedlern kam das Vulgärlatein ins Land, aus dem später die portugiesische Sprache entstand, und auch das Christentum. Die römische Herrschaft endete in der Völkerwanderungszeit; Sueben (ab 409), Alanen, Vandalen und vor allem Westgoten (ab 416) fielen ein und gründeten kurzlebige Reiche auf dem Gebiet des heutigen Portugal. Nur die Sueben konnten sich länger halten, ihr Reich um Braga wurde jedoch 456 durch Theoderich II. und ein zweites Mal 585 durch Leovigild zerstört.

Im Jahr 711 besiegte ein von Tariq ibn Ziyad geführtes Berberheer die Armee von Westgoten-König Rodrigo. Bis 716 war das ganze Territorium des Westgoten-Reiches unter Kontrolle der Umayyaden, Lusitanien wahrscheinlich schon 713. Al-Andalus und vor allem das Emirat, später Kalifat von Córdoba wurde teils von sehr fähigen und erfolgreichen Herrschern wie Abd ar-Rahman I., Abd ar-Rahman III. oder al-Hakam II. geführt und gehörte zu den fortschrittlichsten Reichen seiner Zeit. Nach seinem Zerfall in mehrere Taifas, gehörte der Großteil Lusitaniens zur Taifa von Badajoz, der äußerste Süden zu Sevilla und zu anderen Kleinkönigreichen. Es kam zu einer Einwanderung von berberischen Siedlern, klimatisch bedingt vor allem in den Süden der Halbinsel. Der maurische Einfluss auf Kultur und Sprache Portugals war stark und nachhaltig.

Das Königreich Asturien war für die Mauren nicht von Interesse. Von hier ausgehend begann im 9. Jahrhundert die christliche Reconquista der Territorien des späteren Portugal. Im Jahr 868, während einer Schwächephase des Emirates von Córdoba, wurde "Portucale" erobert (Presúria), 879 Coimbra. Mit der Presúria von Portucale durch Vímara Peres entwickelte sich in der Gegend um Porto als Teil des Königreiches Asturien-León eine „erste“ Grafschaft Portucale ("Condado Portucalense"). Nachkommen aus der Familie von Vímara Peres herrschten in dieser Region bis 1071; es kam zum Wiederaufbau von Braga und zum Bau der Festung von Guimarães. 1071 wurde eine Revolte des letzten Grafen von Portucale, Nuno Mendes, gegen den 1065 zum König von Galicien und Portugal ernannten Garcia niedergeschlagen. Der König von León belehnte um 1095 Heinrich von Burgund mit Portucale und Coimbra. Es entstand eine, ebenfalls als Condado Portucalense bezeichnete, „zweite“ Grafschaft Portucale, die direkt zur Gründung des unabhängigen Königreiches von Portugal führte.

Bereits der Sohn Heinrichs von Burgund, Alfons I., rebellierte 1127 mit Unterstützung des lokalen Kleinadels ("infanções") nach dem Tod Heinrichs gegen seine eigene Mutter, die einen galizischen Prinzen geheiratet hatte. Nach der gewonnenen Schlacht von Ourique hatte er so weit an Prestige gewonnen, dass er 1143 mit Einverständnis von Alfons VII. von León den Königstitel annahm. Im Jahre 1166 gab León den Anspruch der Vorherrschaft über Portucale auf, womit die formelle Unabhängigkeit erlangt wurde. Die Herrscher des Hauses Burgund versuchten, ihr Territorium in Richtung Süden auszudehnen, wobei es das Ziel gewesen sein dürfte, die Macht über ganz Lusitanien zu erlangen. Kastilien verhinderte dies aber. Bis 1250 wurde die Reconquista mit der Eroberung der Algarve, unter starker Beteiligung ausländischer Ritter und Ritterorden, abgeschlossen.
1383 starb das Haus Burgund in Portugal aus. Ein nichtehelicher Abkömmling, Johann von Avis rief sich zum König aus, konnte kastilische Ansprüche auf den portugiesischen Thron in der Schlacht von Aljubarrota (1385) abwehren und gründete die zweite portugiesische Dynastie, das Haus Avis. Unter den Avis-Königen (besonders Emanuel I. – er herrschte von 1495 bis 1521) stieg Portugal zur führenden europäischen Handels- und Seemacht auf. Heinrich der Seefahrer (1394–1460) initiierte Entdeckungsreisen an der westafrikanischen Küste, die am Beginn der Errichtung des portugiesischen Kolonialreiches zuerst in Afrika, später in Südamerika (Brasilien) sowie Asien (Portugiesisch-Indien, Ceylon, Malakka, Macau u. a.) und der europäischen Expansion standen. Das Land war Weltmacht und eine der reichsten Nationen Europas. Auch kulturell kam es zu einer Blütezeit (Luís de Camões).

1580 starb das Haus Avis aus, Portugal fiel aus dynastischen Gründen an die Habsburger. Bis 1640 herrschten die Spanier; Portugal verlor seine Unabhängigkeit, sank zur spanischen Provinz herab und verlor Teile seines Kolonialreiches. 1640 führte der Herzog von Braganza eine Adelsrevolte gegen die spanische Herrschaft an und rief sich als Johann IV. zum König aus. Er gründete die vorletzte portugiesische Dynastie, das Haus Braganza. Außen- und wirtschaftspolitisch geriet das Land in immer größere Abhängigkeit von England (Methuenvertrag, 1703). 1755 vernichtete ein Erdbeben große Teile der Hauptstadt Lissabon. Unter dem Ersten Minister und Reformer Marquês de Pombal wurde die Stadt wieder aufgebaut und das Land mit zum Teil drastischen Methoden zu einem aufgeklärt absolutistischen Staat umgeformt. 1761 kam es zu einem Angriff Spaniens und Frankreichs auf das Land, Pombal trug Wilhelm Graf zu Schaumburg-Lippe den Oberbefehl über die vereinigten portugiesischen und britischen Truppen an. Wilhelm wehrte die Angriffe ab und sicherte damit die Unabhängigkeit Portugals. In den folgenden Jahren reformierte er das portugiesische Heer tiefgreifend und ließ die Festung Elvas an der spanischen Grenze errichten. 1807 besetzten napoleonische Truppen das Land; die königliche Familie floh nach Brasilien. Nachdem die Franzosen mit britischer Hilfe vertrieben worden waren, kam es zur liberalen Revolution, das Land erhielt zum ersten Mal in seiner Geschichte eine Verfassung (1821). Der anschließende Kampf zwischen Anhängern des Absolutismus und Befürwortern einer konstitutionellen Monarchie wurde erst durch den Sieg Letzterer im Miguelistenkrieg entschieden. Am 7. September 1822 erlangte Brasilien unter Kaiser Pedro I. seine Unabhängigkeit.

Die Zeit nach Ende des Miguelistenkrieges wurde von der Auseinandersetzung zwischen Rechts- und Linksliberalen (Cartisten und Setembristen) geprägt. 1853 starb mit Königin Maria II. das Haus Braganza in direkter Linie aus, über die Ehe der Königin mit Ferdinand II. von Sachsen-Coburg und Gotha übernahm der portugiesische Zweig dieses deutschen Adelshauses den Thron (bis 1910). Die Endphase der Monarchie war durch eine große Armut, geringe Bildung (80 Prozent der Portugiesen waren Analphabeten), allgemeine wirtschaftliche Probleme (Staatsbankrott 1891) und durch sich zu Staatskrisen ausweitende republikanische Aufstände geprägt. Unter João Franco wurden die Königsbezüge weiter erhöht und das Ansehen der Monarchie litt verschärft durch den sichtbaren Widerspruch zwischen zerrütteten Staatsfinanzen einerseits und dem luxuriösen, extravaganten Lebensstil der Herrscherfamilie andererseits. 1908 wurden der König Karl I. und sein Sohn, Thronfolger Ludwig Philipp, bei einer Kutschfahrt erschossen. Nur der Sohn Manuel überlebte das Attentat.

Am 3. Oktober 1910 wurde der republikanische Abgeordnete Miguel Bombarda unter ungeklärten Umständen ermordet. In der Nacht kam es daraufhin in Lissabon zu Aufständen. Eine schnell gebildete provisorische Regierung rief am 5. Oktober 1910 die Republik aus; König Manuel II. floh ins englische Exil.

Die junge Republik Portugal sagte Großbritannien 1914 materielle Unterstützung und die Entsendung eigener Truppen zu. Obwohl sie offiziell neutral war, rechtfertigte die portugiesische Regierung die Beteiligung am Ersten Weltkrieg mit einem alten Allianz-Abkommen der beiden Länder, das 1912 erneuert worden war.
Im März 1916 trat das Land auf Seiten der Entente in den Ersten Weltkrieg ein.
Portugal mobilisierte zeitweise 56.500 Soldaten. In der Vierten Flandernschlacht fielen an einem einzigen Tag fast 7.500 Männer bei einer deutschen Offensive.

In der sogenannten Ersten Republik (bis 1926) herrschten allgemeine politische Instabilität und anarchisch chaotische Zustände. Sie war durch monarchistische und kommunistische Aufstände, Putschversuche (unter anderen des Sidónio Pais, 1917) und schwache, häufig wechselnde Regierungen ohne parlamentarische Mehrheit gekennzeichnet.

1926 putschte das Militär und beendete die erste Republik. Unter den Militärs stieg ein Zivilist, António de Oliveira Salazar, ab 1928 Finanzminister, ab 1932 Ministerpräsident, zu höchster Macht auf. Er gründete ab 1933 den „Estado Novo“, den neuen Staat, ein autoritäres Gebilde mit faschistischen Tendenzen, mit Einheitspartei (Nationale Union), Staatsjugend und Geheimpolizei (PIDE). Die katholisch-autoritäre und antidemokratische Ideologie des Diktators verfolgte das Projekt eines „Ständestaates“.

Außenpolitisch baute Salazar auf die Bindung an Großbritannien, sympathisierte im spanischen Bürgerkrieg mit den nationalspanischen Kräften und taktierte geschickt zwischen den Blöcken. Im Zweiten Weltkrieg blieb das Land neutral, belieferte beide Seiten mit dem wichtigen Rohstoff Wolfram und wurde zum Tummelplatz von Geheimagenten vieler Kriegsparteien. Salazar, der von Anfang an mit einem alliierten Sieg rechnete und entscheidend auf Franco im Sinne einer spanischen Neutralität eingewirkt hatte, erlaubte den Alliierten schließlich im Herbst 1943 die Einrichtung von Militärbasen auf den Azoren.

Portugal ist Gründungsmitglied der 1949 geschaffenen NATO. Ab 1960 – dem "Afrikanischen Jahr", bei dem 18 Länder unabhängig wurden – begann der Kolonialkrieg, der in Afrika (Angola, Mosambik, Guinea-Bissau) mit großer Härte geführt wurde. Der Versuch des Offiziers Henrique Galvão, das Salazar-System durch die Entführung des Passagierschiffs "Santa Maria" im Januar 1961 in der Karibik zum Einsturz zu bringen, scheiterte, obwohl die Santa-Maria-Affäre international Aufsehen erregte.

1968 musste Salazar wegen gesundheitlicher Probleme zurücktreten. Sein Nachfolger, Marcelo Caetano, konnte sich nicht zu grundlegenden Reformen entschließen. Durch den Kolonialkrieg war Portugal außenpolitisch zunehmend isoliert, die Kriegskosten führten zu steigender Staatsverschuldung und Inflation.

Führende Militärs erkannten, dass der Kolonialkrieg militärisch für Portugal nicht zu gewinnen war. Wegen der Unfähigkeit der Regierung, eine politische Lösung des Problems zu finden, putschten sie 1974. Eine allgemeine Unzufriedenheit der Bevölkerung mit der Diktatur, durch die einsetzende Wirtschaftskrise (ausgelöst durch die erste Ölkrise 1973) noch verstärkt, führte dazu, dass sich große Teile der Bevölkerung mit den putschenden Offizieren solidarisierten. Es kam zu einer allgemeinen Volkserhebung, der Nelkenrevolution, die den Estado Novo beendete. Die neuen Machthaber entließen die portugiesischen Kolonien, bis auf Macau, in die Unabhängigkeit (1974/1975). Macau folgte 1999.

Die erste Zeit nach der Revolution war geprägt von der Auseinandersetzung zwischen einer eher konservativen Strömung (General Spínola) und einem sozialistischen Flügel (Hauptmann Otelo Saraiva de Carvalho) innerhalb des MFA (Movimento das Forças Armadas – Bewegung der Streitkräfte), der Vereinigung der putschenden Offiziere. Zunächst sah es so aus, als würde die sozialistische Strömung siegen, es kam zu Verstaatlichungen und zu einer Landreform. Die Verfassung von 1976 definierte den Übergang zum Sozialismus als Staatsziel.

Als sich bei den ersten Präsidentschaftswahlen nach der neuen Verfassung 1976 der gemäßigtere General Eanes überraschend deutlich gegen Otelo Saraiva de Carvalho durchsetzen konnte, waren die Weichen für eine Rückkehr des Landes zu einer parlamentarischen Demokratie westeuropäischen Zuschnitts gestellt. Eanes und der Vorsitzende der Sozialistischen Partei Mário Soares (Regierungschef von 1976 bis 1978 und 1983 bis 1985, Staatspräsident von 1986 bis 1996) führten das Land schließlich 1986 in die Europäische Gemeinschaft.

1979 gewann zum ersten Mal seit der Nelkenrevolution wieder eine politische Gruppierung die Parlamentswahlen, die rechts der Mitte stand, die Regierungen unter Francisco Sá Carneiro und Francisco Pinto Balsemão. Die Regierung konnte sich mit der sozialistischen Opposition auf eine Verfassungsänderung einigen, die die sozialistischen Überreste entfernte, welche nach der Nelkenrevolution in die Verfassung geschrieben worden waren. Die 1982 in Kraft getretene Verfassungsänderung ersetzte den bis dahin bedeutenden Revolutionsrat durch ein Verfassungsgericht nach dem Vorbild anderer demokratischer Staaten. 1985 wurde Aníbal Cavaco Silva Premierminister. Seiner konservativen Partido Social Democrata (PSD) gelang bei den Wahlen 1987 ein Erdrutschsieg; erstmals errang eine Partei die absolute Mehrheit. Cavaco Silva blieb bis 1995 Ministerpräsident. Er verfolgte eine neoliberale Wirtschaftspolitik und nahm die Verstaatlichungen aus der Zeit der Nelkenrevolution zurück. Von 1995 bis 2002 stellten wieder die Sozialisten mit António Guterres die Regierung.

Bei den Parlamentswahlen vom 17. März 2002 kam es zu einem neuerlichen Rechtsrutsch. Bei einer Wahlbeteiligung von 62,3 Prozent erreichte die konservative PSD unter José Manuel Durão Barroso eine relative Stimmenmehrheit von 40,1 Prozent, gefolgt von der sozialistischen Partido Socialista und der rechtskonservativen Volkspartei CDS-PP mit 37,9 beziehungsweise 8,8 Prozent. Mit letzterer bildete Barroso eine Koalitionsregierung, wobei der populistische Vorsitzende des CDS-PP, Paulo Portas, das Amt des Verteidigungsministers übernahm und zudem die Bereiche Justiz sowie Arbeit und Soziales an das CDS-PP gingen. Die Sozialisten stellten jedoch ununterbrochen den Präsidenten des Landes, da Nachfolger von Soares 1996 der Sozialist Jorge Sampaio wurde.

Im Juli 2004 wurde Barroso vom Europäischen Rat zum Nachfolger von Romano Prodi als Präsident der Kommission der Europäischen Union nominiert. Sein Nachfolger als Ministerpräsident wurde Pedro Santana Lopes, der nur kurze Zeit regieren konnte, da Präsident Sampaio bereits im November das Parlament vorzeitig auflöste und für Februar 2005 Neuwahlen ausschrieb, bei der die Partido Socialista mit 121 von 230 Sitzen zum ersten Mal in der Geschichte die absolute Mehrheit der Parlamentssitze errang. Ihr Spitzenkandidat José Sócrates wurde am 12. März 2005 neuer Ministerpräsident des Landes.

Am 22. Januar 2006 wählten ungefähr 8,9 Millionen Portugiesen einen neuen Präsidenten. Der bisherige Präsident, der Sozialist Jorge Sampaio, durfte sich nach zwei Amtszeiten nicht mehr zur Wahl stellen. Gegen fünf Kandidaten der Linken setzte sich bereits im ersten Wahlgang der Mitte-rechts-Kandidat und frühere Regierungschef Aníbal Cavaco Silva (PSD) mit einer absoluten Mehrheit von 50,6 Prozent bei einer Wahlbeteiligung von 62,6 Prozent durch. Er wurde von einem Bündnis aus PSD und CDS-PP unterstützt. Der als Architekt des portugiesischen Wirtschaftsaufschwungs in den Jahren 1985 bis 1995 geltende 66-jährige Wirtschaftsprofessor wurde damit der erste bürgerliche Präsident in Portugal seit der Nelkenrevolution von 1974. Er wurde am 9. März 2006 für fünf Jahre in sein Amt eingeführt. Am 23. Januar 2011 wurde Cavaco Silva im Amt bestätigt.

Die drastischen Auswirkungen der weltweiten Wirtschafts- und Finanzkrise dominierten den Wahlkampf bei den Parlamentswahlen 2009. Obwohl die regierenden Sozialisten deutlich an Wählerstimmen verloren und ihre absolute Mehrheit einbüßten, gelang es ihnen, sich als wählerstärkste Partei zu behaupten. Damit blieb auch die Regierung Sócrates im Amt.

Nachdem das Sparkonzept der Regierung im Parlament keine Mehrheit fand, reichte Sócrates am 23. März 2011 sein Rücktrittsgesuch ein. In den anschließenden Neuwahlen erfuhren die Sozialisten eine deutliche Wahlniederlage. Folgerichtig wurde am 15. Juni 2011 Pedro Passos Coelho, Vorsitzender der mit fast 40 % der Stimmen siegreichen liberal-konservativen Sozialdemokratischen Partei (PSD), zum neuen Ministerpräsidenten Portugals ernannt. Er führte eine Koalitionsregierung aus PSD und CDS-PP, die mit 132 von 230 Parlamentssitzen über eine solide Mehrheit verfügte.

Nach der Parlamentswahl am 4. Oktober 2015 blieb das bisher regierende Parteienbündnis PàF zwar stärkste Kraft, verlor jedoch die absolute Mehrheit. Die linken bisherigen Oppositionsparteien PS, BE und CDU halten zusammen mit 124 der insgesamt 230 Sitze im Parlament eine regierungsfähige Mehrheit. Am 20. Oktober 2015 sagte António Costa (PS) dem Staatspräsidenten Anibal Cavaco Silva, dass er eine Linksregierung bilden wolle.
Cavaco Silva blockierte jedoch vorerst die Versuche der Sozialisten und Kommunisten, ein Regierungsbündnis zu schließen, und ernannte den amtierenden bürgerlich-konservativen Ministerpräsidenten Pedro Passos Coelho abermals zum Regierungschef. In einer Fernsehrede an die Nation am Abend des 22. Oktober 2015 begründete Cavaco Silva dies mit Rücksichtnahme auf Europäische Union und Euro, „Finanzinstitutionen, Investoren und die Märkte“. Erst zwei Wochen, nachdem die Parlamentsmehrheit Passos Regierungsprogramm abgelehnt hatte, beauftragte der Präsident am 23. November Costa, der ein Ende der unsozialen Politik versprach und den Mindestlohn und die Renten anheben und die Wirtschaft ankurbeln will, mit der Regierungsbildung unter der Auflage, den Regeln der Euro-Zone zu folgen und den internationalen Verpflichtungen Portugals nachzukommen. António Costa ist seit 26. November 2015 als Premierminister im Amt.
Am 24. Januar 2016 wurde Marcelo Rebelo de Sousa (PSD) zum Staatspräsidenten gewählt.

Seit der Nelkenrevolution des Jahres 1974 hat sich Portugal zu einer stabilen, repräsentativen Demokratie mit semipräsidentiellem Regierungssystem entwickelt. Die vier wichtigsten Organe der Politik in Portugal sind der Präsident, der Premierminister und sein Ministerrat, das Parlament sowie die Justiz (siehe auch Verfassung Portugals). Im Demokratieindex 2016 belegt Portugal Platz 28 von 167 Ländern.

Der Präsident, der alle fünf Jahre direkt in allgemeinen und direkten Wahlen bestimmt wird, ist Oberkommandierender der Streitkräfte. Er ernennt einen Premierminister und den Ministerrat, wobei er sich am Ergebnis der Parlamentswahlen zu orientieren hat. Der Staatsrat ist ein Gremium, das den Präsidenten berät, und besteht aus dem Staatspräsidenten und seinen Vorgängern, dem Premierminister, dem Präsidenten des Verfassungsgerichtes (Tribunal Constitucional), dem Bürgerbeauftragten, den regionalen Präsidenten (Madeira und Azoren) sowie fünf vom Staatspräsidenten und fünf vom Parlament ausgewählten Personen.

Die Verfassung von 1976 galt als ein Kompromiss zwischen zwei Legitimitätsformen, die sich während der Transformation herausgebildet haben: dem Militär, dem gewählten Parlament und dem gewählten Präsidenten. Bis zu einer endgültigen Institutionalisierung des Regierungssystems in Portugal dauerte es bis 1982, als die Verfassung eine Revision erfuhr. Bis 1982 wurde die revolutionäre Legitimation des Militärs als notwendig empfunden. Seit der Revision 1982 wurden die Befugnisse des Präsidenten zu Gunsten des Parlamentes beschnitten. Man spricht in dem Zusammenhang von der Herausbildung eines parlamentarischen Regierungssystems. Die Regierung ist seitdem allein dem Parlament verantwortlich. Die Befugnisse des Militärs wurden durch die Auflösung des Revolutionsrates beschnitten und einer zivilen Kontrolle unterstellt. Seit der Verfassungsrevision 1982 hat der direkt gewählte Präsident in Portugal keine exekutiven Befugnisse mehr.

Die Regierung wird vom Premierminister geleitet, der sich einen Ministerrat zusammenstellt. Jede neue Regierung muss dem Parlament ihr Programm zur Debatte vorlegen. Wird es nicht abgelehnt, ist die Regierung vom Parlament akzeptiert.

Das Parlament wird als Assembleia da República ("Versammlung der Republik") bezeichnet und besteht aus einer Kammer mit bis zu 230 Abgeordneten (Einkammersystem). Die Abgeordneten werden für vier Jahre nach Verhältniswahlrecht gewählt. Der Präsident hat das Recht, das Parlament aufzulösen und Neuwahlen anzusetzen.

Das Oberste Gericht ist die höchste Instanz der portugiesischen Justiz. Zudem sind besondere Oberste Gerichte für militärische, verwaltungsrechtliche und steuerrechtliche Fragen zuständig. Das Verfassungsgericht Portugals hat neun Mitglieder und überwacht die verfassungsgemäße Auslegung des Rechts.

Die über lange Zeit größten Parteien sind die sozialdemokratisch orientierte "Sozialistische Partei" ("PS") und die bürgerlich-konservativ orientierte Sozialdemokratische Partei ("PSD"). Daneben gibt es noch die rechtspopulistische Volkspartei ("CDS/PP"), die traditionsreiche Kommunistische Partei ("PCP") und den vor wenigen Jahren als Sammelbecken der intellektuellen Linken gegründeten Linksblock ("BE"). Die fünf Parteien sind seit der Parlamentswahl am 5. Juni 2011 im Parlament vertreten.
Die Grünen ("PEV") treten in Portugal seit 1987 immer in Listenunion mit den Kommunisten an und bekommen seit Gründung dieses Bündnisses ("CDU") immer zwei Parlamentsmandate.

Portugal ist Mitglied der Europäischen Union und hatte im zweiten Halbjahr 2007 den Ratsvorsitz inne. Das Land hatte den Vorsitz bereits im ersten Halbjahr 2000. In dieser Zeit verfolgte Portugal vor allem das Ziel, den Dialog mit Afrika zu forcieren und Impulse zur Stärkung der Wettbewerbsfähigkeit der europäischen Wirtschaft zu geben.

Portugal war Gründungsmitglied der NATO und beteiligt sich mit Truppen an der Friedenssicherung auf dem Balkan. Zusammen mit Spanien ist Portugal an den Ibero-Amerikanischen Gipfeltreffen beteiligt, die vor allem den Dialog mit den Ländern Lateinamerikas fördern sollen. Federführend war das Land bei der Gründung der Gemeinschaft der Portugiesischsprachigen Länder (CPLP), deren Ziel es ist, die Zusammenarbeit dieser Länder zu vertiefen. Ferner ist das Land Mitglied in der Lateinischen Union, die den Erhalt und die Vielfältigkeit der romanischen Sprachen fördert.

Portugal unterstützte eins seiner früheren Kolonialgebiete, Osttimor, im Kampf um Unabhängigkeit von Indonesien, und kooperiert dabei zugunsten des jungen Staates finanziell und militärisch mit asiatischen Ländern, den Vereinigten Staaten sowie der UNO.

Es besteht ein Streit zwischen Portugal und Spanien um das Gebiet von Olivenza (oder "Olivença"), das gegenwärtig zum spanischen Staat gehört, jedoch von Portugal beansprucht wird. Olivença kam 1801 unter spanische Verwaltung, doch Spanien erklärte sich auf dem Wiener Kongress von 1815 bereit, das Gebiet an Portugal zurückzugeben. Portugal verlangt seither die Rückgabe.


Bis zur Nelkenrevolution 1974 wurde Bildung vernachlässigt, und nach der Revolution ging der Aufbau des Bildungssystems nur langsam voran. Dies macht sich bis heute bemerkbar: Im Jahr 2000 verfügten beispielsweise nur ungefähr ein Zehntel der Dreißigjährigen über einen Hochschulabschluss. Damit lag Portugal unter den EU-Mitgliedern vor der Osterweiterung mit großem Abstand weit zurück. Die Analphabetenquote liegt um 4,6 % (3,1 % bei Männern, 5,9 % bei Frauen). Im PISA-Ranking von 2015 erreichen Portugals Schüler Platz 29 von 72 Ländern in Mathematik, Platz 22 in Naturwissenschaften und Platz 21 beim Leseverständnis. Portugal liegt damit über dem Durchschnitt der OECD-Staaten.

Das Schulsystem besteht aus einer vierjährigen Grundschule und einer fünfjährigen Oberschule. Für Kinder ab dem sechsten Lebensjahr besteht eine gesetzlich festgelegte neunjährige Schulpflicht. Der Pflichtschulunterricht ist an staatlichen Schulen kostenlos. Für den Unterricht an einer der vergleichsweise zahlreichen privaten Schulen können bedürftige Familien Unterstützung erhalten.

Wer nach der Oberschule die dreijährige "Escola Secundária" absolviert, bekommt die Universitätsreife und kann zwischen mehreren Möglichkeiten des Hochschulstudiums wählen: Hochschulbildung wird in Portugal von staatlichen und privaten Universitäten ("universidades") sowie staatlichen und privaten Fachhochschulen ("escolas politécnicas") angeboten. Zur Förderung abgelegener Gebiete wurden in vielen mittleren Städten Hochschulen eingerichtet. In jedem Fall ist eine Aufnahmeprüfung zu absolvieren und es sind Studiengebühren zu entrichten, die bei privaten Schulen höher sind als bei staatlichen. Sie sind je nach Fachrichtung unterschiedlich, für staatliche Einrichtungen bis zu 850 € jährlich. Trotzdem ist etwa ein Drittel der Studenten bei einer privaten Institution eingeschrieben. Zusätzlich zu den Einschreibegebühren sind "propinas", Gebühren für die Vergabe von Zeugnissen und Diplomen zu zahlen. Etwa 20 % der Studenten kommen in den Genuss einer einkommensabhängigen staatlichen Unterstützung.

Mit dem steuerfinanzierten Serviço Nacional de Saúde steht seit 1979 allen Einheimischen und Besuchern ein, bis auf meist geringe Zuzahlungen, weitgehend kostenloses Gesundheitssystem zur Verfügung, wie es die portugiesische Verfassung von 1976 festgeschrieben hat. Daneben bestehen berufsständische und private Gesundheitssysteme. Mit 3,33 Ärzten pro 1000 Einwohner befindet sich der medizinische Versorgungsgrad in Portugal weltweit auf dem 28. Rang (Vergleich: Deutschland 3,73; Schweiz 3,57; Österreich 2,28). Die Lebenserwartung in Portugal liegt mit 79 Jahren inzwischen über dem europäischen Mittelwert von 77 Jahren.

Der öffentliche Rettungsdienst INEM deckt Kontinentalportugal mit einem einheitlichen Notfalldienst ab.

Quelle: UN

Portugal gliedert sich in fünf Regionen, 18 Distrikten und den zwei autonomen Regionen der Azoren und Madeira. Zu statistischen Zwecken sind zudem 28 sogenannte statistische Unterregionen eingerichtet. Eine Ebene darunter folgt die Kommunale Selbstverwaltung in Portugal mit 308 Kreisen (Concelhos) und 3091 Gemeinden (Freguesias). Bis zur administrativen Neuordnung 2013 waren es 4259 Gemeinden.

Die Portugiesischen Streitkräfte (portugiesisch: "Forças Armadas Portuguesas") unterstehen dem Verteidigungsministerium und bestehen aus den Teilstreitkräften

Der Präsident ist Oberkommandierender der Streitkräfte. Die bis 2003 herrschende allgemeine Wehrpflicht ist ausgesetzt.

Das portugiesische Recht hat sich aus dem römischen Recht entwickelt. Nach dem französischen Recht ist es im 20. Jahrhundert vor allem vom deutschen Recht beeinflusst worden.

Seit dem Beitritt Portugals zur EG im Jahre 1986 hat sich Portugal zu einer zunehmend diversifizierten, vor allem auf Dienstleistungen ausgerichteten Ökonomie entwickelt. Dienstleistungen sind mittlerweile für etwa zwei Drittel des BIP verantwortlich. Wie in anderen Staaten Europas wurden weit reichende Privatisierungen durchgeführt und die Staatsausgaben reduziert. Im Jahre 1998 hat sich Portugal für den Beitritt zur Europäischen Währungsunion qualifiziert und führte wie elf andere Staaten am 1. Januar 2002 den Euro als Zahlungsmittel ein und löste damit den Portugiesischen Escudo ab.

Das Wirtschaftswachstum hat mit etwa 3,3 % jährlich jenes des EU-Durchschnitts in den Jahren bis zur Weltwirtschaftskrise ab 2007 meist übertroffen. Trotzdem ist Portugal immer noch das ärmste "Altmitglied" der EU: Das Pro-Kopf-BIP (in Kaufkraftparitäten) liegt bei etwa 78 % des Durchschnitts der EU-Länder vor der Osterweiterung, wobei es im Jahr 1985 noch etwa bei 50 % lag. Im Vergleich mit dem BIP der EU ausgedrückt in Kaufkraftstandards erreicht Portugal einen Index von 77 (EU-28:100) (2015). Die Entwicklung im Land ist jedoch sehr unterschiedlich. Nimmt man nur den Großraum Lissabon, so liegt der Index hier mit über 100 inzwischen leicht über dem europäischen Durchschnitt (2014).

2011 schrumpfte die Wirtschaft um 1,7 % und 2012 um 3,2 %. Erst in der zweiten Jahreshälfte 2013 konnte Portugal die Rezession überwinden. Im Jahr 2014 wurde ein Wachstum von rund 1 % erzielt. Für das Jahr 2015 werden knapp 2 % Wirtschaftswachstum erwartet. Das durchschnittliche Bruttomonatseinkommen eines Arbeitnehmers beträgt aktuell ca. 1.100 Euro brutto, der gesetzliche Mindestlohn liegt bei 515 Euro brutto.

Das Bruttoinlandsprodukt Portugals betrug im Jahr 2014 173,1 Mrd. Euro. Das Bruttoinlandsprodukt pro Kopf betrug im selben Jahr ca. 16.600 Euro.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Portugal Platz 42 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegt das Land 2017 Platz 77 von 180 Ländern.

Als Hindernis für stärkeres Wachstum von Produktivität und Beschäftigung wurden traditionell strukturelle Probleme angegeben. Zunächst waren dies Defizite im Bildungssystem, die relativ hohe Analphabetismusrate, die teils schlechte Infrastruktur und die ineffiziente Verwaltung. Insbesondere seit den späten 1990er-Jahren gelten diese Defizite weitgehend als behoben. So liegt die Analphabetenrate in Portugal inzwischen bei 5 %, Tendenz sinkend (zum Vergleich: Deutschland 1 %). 2013 hatten 24,6 % der Männer und 36,1 % der Frauen zwischen 30 und 34 Jahren einen Hochschulabschluss (zum Vergleich: NRW 28,3 % / 29,9 %). Zudem erfuhr die Infrastruktur im Land enorme Investitionen, auch mit Hilfe der EU. Als Ergebnis kann beispielhaft ein überdurchschnittlicher Ausbau der Glasfaseranschlüsse für schnelles Internet gelten,(2012: 10,3 %, zum Vergleich: Deutschland 1,1 %) auch ist das Autobahnnetz Portugals heute eines der dichtesten in Europa, und eine Unternehmensgründung ist online innerhalb eines Tages möglich. Prämierte Entwicklungen wie die weitverbreiteten Multibanco-Bankautomaten oder das Via Verde-Mautsystem können als Beispiele für die Innovationsfähigkeit im Land gelten. Nach einer Boomphase bis Anfang der 2000er-Jahre geriet Portugal dann zunehmend in Wettbewerb mit den Niedriglohnländern Mittel- und Osteuropas, Asiens und Nordafrikas, worunter die Attraktivität Portugals für ausländische Direktinvestitionen litt. Diese steigen jedoch seit 2009 wieder deutlich an und betrugen 2012 über 57 Mrd. Euro. Die Durchschnittslöhne in Portugal sind für westeuropäische Verhältnisse weiterhin niedrig und die Arbeitszeiten zum Teil wesentlich länger.

Im April 2017 lag die Arbeitslosigkeit in Portugal bei 9,8 Prozent – 7,9 Prozentpunkte weniger als noch 2013. Vor der Krise, 2008, hatte die Arbeitslosenquote bei 7,6 Prozent gelegen.

Der Außenhandel wird zu etwa 80 % mit den EU-Partnern abgewickelt. Exportiert werden vor allem Bekleidung und Schuhe, Maschinen, Chemieprodukte, Kork sowie Zellstoff und Papier. Importiert werden Maschinen, Fahrzeuge, Öl und Ölprodukte sowie Landwirtschaftsprodukte. Dabei hat Portugal ein großes Handelsbilanzdefizit. Das Zahlungsbilanzdefizit ist durch Einnahmen aus dem Tourismus und den Rücküberweisungen der Auslandsportugiesen nicht so hoch wie das Handelsbilanzdefizit.

Ausländische Investitionen kommen vor allem aus Großbritannien und Spanien, zuletzt sorgten auch zunehmende Investitionen aus der ölreichen ehemaligen portugiesischen Kolonie Angola für Aufsehen, etwa durch den Kauf der Bank Banco BIC Português durch ein angolanisches Bankhaus.

Zu den bedeutendsten ausländischen Investitionen in Portugal zählt das Automobil-Montagewerk "Autoeuropa". Zu den wichtigsten international tätigen portugiesischen Unternehmen zählen Energias de Portugal, die Portugal Telecom, der besonders im Einzelhandel erfolgreiche Konzern Jerónimo Martins, und die Sonae-Gruppe, die beispielsweise das Alexa Einkaufszentrum am Berliner Alexanderplatz führt.
Laut dem port. Übertragungsnetzbetreiber REN betrug die installierte Leistung der Kraftwerke in Portugal im Jahre 2010 11.195 MW, davon entfielen auf kalorische Kraftwerke 6.561 MW (59 %) und auf Wasserkraftwerke 4.584 MW (41 %). Insgesamt wurden im Jahre 2010 48,5 Mrd. kWh erzeugt, davon 37,4 Mrd. (77 %) durch kalorische Kraftwerke und 11,1 Mrd. (23 %) durch Wasserkraftwerke.

In den letzten Jahren wurde die Windenergie zu einem wichtigen Faktor in der Elektrizitätsversorgung. Im Jahr 2017 waren in Portugal Windkraftanlagen mit einer Nennleistung von 5.316 MW installiert. 2014 deckten diese mit 27 % mehr als ein Viertel des portugiesischen Strombedarfes. Das war weltweit der zweithöchste Wert und wurde nur von Dänemark mit 39,1 % übertroffen. Die Stromerzeugung aus Photovoltaik ist hingegen mit einer installierten Leistung von etwa 419 MW vergleichsweise unbedeutend. 2015 deckten erneuerbare Energien ca. 48 % des portugiesischen Elektrizitätsbedarfes, rund die Hälfte stammte von Windkraftanlagen. Im Mai 2016 wurde das Land 107 Stunden bzw. 4 Tage lang ununterbrochen ausschließlich von erneuerbaren Energien versorgt. Ende 2008 wurde mit dem Windpark Alto Minho der zu diesem Zeitpunkt größte Onshore-Windpark Europas in Betrieb genommen.

Bezogen auf den gesamten Endenergiebedarf (Strom, Wärme, Verkehr) lieferten erneuerbare Energien im Jahr 2013 25,7 % der Energie. Bis 2020 will Portugal diesen Anteil auf 31 % erhöhen.

Portugal gehört zu den weltführenden Nationen in der Wolframproduktion. An abbaubaren Bodenschätzen gibt es außerdem Kohle, Kupfer, Zinn, Gold, Eisenerze wie Pyrit und Chalkopyrit, Tonminerale wie Kaolinit sowie Wolframit und Uraninit. In der zweiten Hälfte des 20. Jahrhunderts galt Portugal als wichtiger Lieferant für Uran. Der Uranabbau wurde jedoch zu Beginn des 21. Jahrhunderts wegen Unwirtschaftlichkeit eingestellt. Während des Zweiten Weltkrieges versorgte sich Deutschland mit portugiesischem Wolfram für die Waffenproduktion. Die Atombombe von Hiroshima enthielt portugiesisches Uran.

Die Landwirtschaft Portugals ist eine der ineffizientesten in Europa; der Anteil der Landwirtschaft am BIP liegt bei etwa 5 %, jedoch sind mehr als 15 % der Arbeitskräfte in der Landwirtschaft beschäftigt. Dies führte dazu, dass viele Betriebe aufgaben und fast die Hälfte der Nahrungsmittel importiert wird. Der Anbau von Mandeln befindet sich ebenso wie die Korkeichenplantagen ("montados") im Alentejo und im Douro-Tal in einer tiefen Krise. Zwar ist Portugal mit etwa 125.000 Tonnen und damit der Hälfte der weltweit geernteten Menge das bedeutendste Produktionsland für Rohkork. Dennoch steht der Wirtschaftszweig etwa seit der Jahrtausendwende durch die zunehmende Popularität und internationale Herstellung von synthetisch hergestellten Verschlussalternativen für Weinflaschen stark unter Druck. Auch die Hoffnung zahlreicher Korkbauern ("tiradores"), die portugiesische Korkeichennutzung sei zumindest für Weine im Hochpreissegment unersetzlich, hat sich nicht erfüllt. Denn der Trend weg vom Naturprodukt hin zu kostengünstigeren Alternativen aus Kunststoff ist auch bei teuren Weinen beobachtbar. Ausdruck findet diese Entwicklung in zahlreichen Firmeninsolvenzen und Abwanderungen aus den Korkanbauregionen. Portugals Regierung und die Korkindustrie reagieren inzwischen auf die Entwicklung mit weltweiten „Green-Marketing“-Werbekampagnen, in denen Winzer wieder von der ökologischen Nachhaltigkeit des Naturprodukts Kork überzeugt werden sollen.

Für die Zellstoffindustrie, einen wichtigen Wirtschaftsfaktor Portugals, werden große Flächen mit schnell wachsendem Eukalyptus als Rohstoff aufgeforstet. Dies ist aus ökologischen Gründen bedenklich, weil Eukalyptus den Boden auslaugt, den ursprünglichen Wald und damit die Tierwelt verdrängt, und die katastrophalen Waldbrände im Sommer begünstigt.

Ähnlich der Landwirtschaft kämpft die Fischerei mit Produktivitätsproblemen. Die portugiesische Fischereiflotte ist im Vergleich zur spanischen schwach entwickelt. Der meiste Fisch wird importiert.

Für etwa acht Prozent des BIP, mit steigender Tendenz, ist der Tourismus verantwortlich, wobei die meisten Besucher aus Spanien und Großbritannien kommen. Die Algarve ist dabei unangefochten das Zentrum. 

15 Stätten in Portugal zählen zum Welterbe der Unesco, darunter zwei auf den Azoren und eine auf Madeira.

Zur internationalen Vermarktung des Portweins aus der ältesten Weinbau Region Alto Douro ab dem 17. Jahrhundert - leisteten die 3 deutschen Kopke, Burmester und Andresen einen entscheidenden Beitrag zur Entwicklung des Portwein.

Die Paläste mit Parkanlagen bei Sintra und Portugals größte Schloss-, Klosteranlage in Mafra sind von deutschen Baumeistern von Eschwege und Ludwig geplant worden.

Vor der Algarve, Sétubal und den Inselgruppen Madeira und Azoren sind Möglichkeiten sowohl Delfine als auch Wale in freier Natur zu erleben.

Auf der Atlantikinsel Madeira begann der Tourismus in der Mitte des 19. Jahrhunderts; die Insel war eines der bevorzugten Ziele wohlhabender britischer Reisender. Sie stiegen vor allem im Hotel Reid’s Palace ab, das der Schotte William Reid 1891 hatte bauen lassen.


Der Staatshaushalt umfasste 2016 Ausgaben von 92,2 Mrd. Dollar, dem standen Einnahmen von 87,2 Mrd. Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 5,0 Mrd. Euro beziehungsweise 2,4 % des BIPs.

Die Staatsverschuldung betrug 2009 127,9 Mrd. Euro oder 76,1 % des BIP (laut folgender Tabelle 83,6 %), mittlerweile liegt sie bei über 100 % des BIP. Am 6. April 2011 gab der Premierminister Portugals bekannt, dass das Land im Zuge der Verschuldungskrisen von Eurozonen-Staaten Finanzhilfen der Europäischen Union annehmen werde.

Im Jahr 2011 betrug die Neuverschuldung 4,2 % des BIP. Damit erfüllte Portugal die vorgeschriebenen Sparziele der EU von 5,9 % des BIP deutlich. Dies gelang aber nur durch zusätzliche Zahlungen aus Pensionsfonds, ansonsten hätte das Defizit bei 7,7 % gelegen.

Veränderung des Bruttoinlandsprodukts (BIP), real Eurostat

Entwicklung des BIP (nominal), Eurostat
Entwicklung des Außenhandels (GTAI)
Haupthandelspartner Portugals (2016), Quelle: GTAI

Das Straßennetz ist nicht zuletzt dank EU-Mitteln aus Förderungsfonds seit den 1980er-Jahren schnell gewachsen und gut ausgebaut. Die wichtigen Routen werden von gebührenpflichtigen "Autoestradas" abgedeckt. Diese erreichen insgesamt eine Länge von 1100 km und werden größtenteils vom börsennotierten Unternehmen Brisa betrieben. Gebührenfrei sind hingegen "Itinerários Principais (IP)" oder "Itinerários Complementares (IC)". Die Investitionen in den Straßenverkehr verdeutlichen sich nicht zuletzt in den Unfallstatistiken, 2008 starben in Portugals Straßenverkehr 83 Menschen pro 1 Million Einwohner nach 323 im Jahre 1991 (zum Vergleich 54 pro 1 Million in Deutschland 2008).

Der Fernverkehr mit Bussen hat in Portugal einen höheren Stellenwert als in Mitteleuropa. Das größte Linienbusunternehmen ist Rede Expressos.

Das Eisenbahnnetz Portugals ist relativ weitmaschig. Das staatliche Unternehmen Infraestruturas de Portugal verwaltet ein Schienennetz, das eine Gesamtlänge von 2789 km hat. Davon sind 188 km Schmalspur, 607 km sind mehrspurig ausgebaut. Züge werden von der staatlichen Comboios de Portugal und seit kurzem von der privaten Fertagus angeboten. Auf den Hauptverkehrsrouten sind die Verbindungen jedoch effizient, der Alfa Pendular bietet die schnellsten Verbindungen zwischen den Ballungszentren.

Internationale Züge verbinden Portugal mit Städten in Spanien. Daneben gibt es eine Verbindung nach Irún an der spanisch-französischen Grenze mit direktem TGV-Anschluss nach Paris.

In Lissabon gibt es eine U-Bahn. Porto verfügt über eine Stadtbahn und einen davon unabhängigen Straßenbahnbetrieb. Weitere Straßenbahnen existieren in Lissabon und südlich des Tejo.

Die fünf wichtigsten Flughäfen Portugals sind Lissabon, Porto, Faro, Ponta Delgada und Funchal. Sie werden von mehreren Fluggesellschaften bedient, wobei die portugiesischen Linien TAP Portugal, SATA Air Açores und Portugália Airlines die meisten Verbindungen bieten. Im Jahr 2006 fertigten die portugiesischen Flughäfen insgesamt 22 Millionen Passagiere und 135.000 Tonnen Fracht ab. Es wurde erwartet, dass das Verkehrsaufkommen bis 2025 auf 46,8 Millionen Passagiere und 259.000 Tonnen Fracht steigen werde.

Angesichts der steigenden Passagierzahlen wurde entschieden, zusätzlich zu dem existierenden Flughafen Lissabons einen neuen kommerziellen Flughafen (Novo Aeroporto Lisboa) zu schaffen. Er soll auf dem wenig benutzten Militärflughafen in Montijo erbaut werden. Dieser liegt am südlichen Ufer des Tejo. Ab 2022 soll der Betrieb aufgenommen werden. Der Baubeginn und die ursprünglich für 2017 vorgesehene Eröffnung wurden im Jahr 2010 aufgrund der Finanzkrise auf unbestimmte Zeit verschoben.

Die wichtigsten Häfen Portugals befinden sich in Aveiro, Porto, Lissabon, Sines und Setúbal. Im Jahr 2007 kamen fast 70 % aller Importe über den Seeweg ins Land, während 41 % der Exporte über die Häfen abgewickelt wurden. Von den 58 Millionen Tonnen liefen 39 % über den Hafen von Sines. Die Häfen werden modernisiert und ihre Verkehrsanbindung verbessert, damit sie einen größeren Teil des spanischen Außenhandels abwickeln können.
Von den Flüssen sind der Douro und der Tejo schiffbar.

Portugal wird manchmal als "Land der Poeten" bezeichnet. In der portugiesischen Literatur hatte die Poesie immer stärkeren Einfluss als die Prosa. Im Mittelalter, als die portugiesische Nation entstand, war im Nordwesten der Iberischen Halbinsel die Poesie weit verbreitet. Es entstanden exzellente epische wie lyrische Werke. Während die bekanntesten klassischen Poeten Luís de Camões und Fernando Pessoa sind, gibt es eine Reihe weniger bekannter Künstler, die auf die moderne portugiesische Literatur bedeutenden Einfluss haben.

Die Prosa entwickelte sich später als die Poesie und prägte sich erst im 14. Jahrhundert in der Form von Chroniken oder der Beschreibung des Lebens von Heiligen heraus. Hier ist Fernão Lopes der berühmteste Vertreter; er verfasste eine Chronik der Regentschaften von drei Königen seiner Zeit. Für ihn waren Genauigkeit der Darstellung sowie eine lebhafte Schilderung am wichtigsten. International ist die portugiesische moderne Literatur am bekanntesten, besonders mit den Werken von José Maria Eça de Queiroz und dem Nobelpreisträger für Literatur 1998, José Saramago.

Auch Frauen sind unter den bedeutenden zeitgenössischen Schriftstellern des Landes zu finden, hier zu nennen wären besonders Sophia de Mello Breyner Andresen, Lídia Jorge und Agustina Bessa-Luís. Unter den jungen Autoren konnte sich zuletzt Valter Hugo Mãe einen Namen machen.

Die Filmkunst hat eine lange Tradition in Portugal. International genießt vor allem der anspruchsvolle Autorenfilm des Landes unter Cineasten einen guten Ruf, doch sind preisgekrönte Regisseure, wie João Botelho oder João Canijo, der breiten Bevölkerung, selbst in Portugal, eher wenig bekannt. Wird die Filmwirtschaft auch hier von den internationalen Großproduktionen und den Multiplex-Kinos bestimmt, so gibt es abseits davon aber noch immer eine rege Filmklub-Bewegung und eine Reihe verschiedener Filmfestivals im Land, die engagierte neue Regisseure wie die sehr unterschiedlichen Miguel Gomes, Jorge Pelicano oder Fernando Fragata hervorbringen, ebenso Schauspielernachwuchs mit Namen wie etwa Ana Moreira, Diogo Infante oder Lúcia Moniz.

Als bekanntester portugiesischer Regisseur ist hier Manoel de Oliveira († 2. April 2015) zu nennen, der zudem mit über 100 Jahren (Jahrgang 1908) der älteste noch arbeitende Regisseur der Welt war, und der letzte lebende, der bereits zu Stummfilmzeiten drehte. Die bekanntesten Schauspieler sind Maria de Medeiros, die in Pulp Fiction die Filmpartnerin von Bruce Willis war, und Joaquim de Almeida mit seinen zahlreichen Hollywood-Rollen.

Die wichtigste Musikform Portugals ist der Fado, der sehr melancholisch sein kann und zum Klischee der melancholischen Portugiesen (gegenüber den temperamentvollen Spaniern) beigetragen hat. Diese Musik geht eng einher mit "Saudade" (etwa: "Sehnsucht"), und ist wahrscheinlich durch die Vermischung der Lieder von portugiesischen Seefahrern mit den Rhythmen afrikanischer Sklaven entstanden. Hierbei werden zwei Stilformen unterschieden, nämlich der variantenreichere, volksnahe Fado von Lissabon, und der akademische, nur von Männern gesungene Fado von Coimbra. Das international bekannteste Fadolied wurde "April in Portugal", das in einigen hundert Versionen weltweit veröffentlicht wurde und von Raul Ferrão stammt, und zudem den Fado Coimbras mit dem Lissabons vereint. Amália Rodrigues war die bedeutendste Fado-Künstlerin, nach ihrem Tod sind mehrere Musiker aus ihrem Schatten getreten und bringen neue Formen des Fado hervor, die teils nur noch die "saudade" mit dem ursprünglichen Fado gemein haben, teils aber ganz bewusst an traditionellen Mustern des Fado festhalten. In den letzten Jahren hat die Zahl der Fado-Veröffentlichungen und seine öffentliche Präsenz wieder stärker zugenommen, durch Erfolge von jungen Sängern wie Mariza, Camané oder Ana Moura. In den früheren Kolonien Portugals hat sich der Fado ebenfalls verbreitet und sich zur kapverdischen Morna einer Cesária Évora und zum brasilianischen Choro weiterentwickelt. Zu den heute im deutschsprachigen Raum bekannten, vom Fado beeinflussten Gruppen zählt Madredeus mit der Sängerin Teresa Salgueiro.

Seit dem Mittelalter war die Kirchenmusik im Rahmen des starken portugiesischen Katholizismus sehr bedeutend in Portugal und erreichte ihren Höhepunkt in der Renaissance. Im Bereich der geistlichen Vokalpolyphonie hatte Portugal eine bemerkenswerte Generation portugiesischer Komponisten vorzuweisen, die im 16. und 17. Jahrhundert die Musikgeschichte von Portugal geprägt haben: Estêvão de Brito (c. 1575–1641), Filipe de Magalhães (c.1571–1652), Duarte Lobo (1565–1646) und Manuel Cardoso (1566–1650). Diese sogenannte "Generation der Polifonisten von Évora" stellt die Glanzzeit der Kirchenmusik in Portugal dar.

Im Bereich der klassischen Musik hat Portugal keine international bedeutenden Komponisten vorzuweisen. Im 18. und 19. Jahrhundert waren es einzelne Komponisten wie Carlos Seixas, João Domingos Bomtempo, José Vianna da Motta oder Luís de Freitas Branco, die zwar für Portugal bedeutende Werke schrieben, jedoch keine nennenswerte internationale Aufmerksamkeit erreichten. Hingegen gehörte Luísa Todi ("La Todi", 1753–1833) zu den europaweit berühmtesten Sängerinnen ihrer Zeit.

Im 20. Jahrhundert wurde die Tradition portugiesischer klassischer Musik von Komponisten wie Emmanuel Nunes, António Victorino de Almeida oder Eurico Carrapatoso fortgesetzt. Der Tenor Lomelino Silva erlangte in den 1920er- und 1930er-Jahren internationalen Ruhm, geriet danach jedoch in Vergessenheit.

Maria Joao Pires, Mário Laginha, Pedro Burmester, António Pinho Vargas oder Bernardo Sassetti wurden sowohl in der Klassik als auch im Jazz bedeutende moderne Komponisten und Interpreten über die Landesgrenzen hinweg.

Der Jazzclub "Hot Clube de Portugal" in Lissabon gilt als ältester noch bestehender Jazzclub Europas. Mit einer Reihe Jazzfestivals und zahlreichen Musikern ist die Jazzszene weiter lebendig im Land, mit Namen wie dem Trompeter Sei Miguel, dem Bassisten Carlos Bica, dem Gitarristen Manuel Mota, oder der bekannten Sängerin Maria João. Im Free Jazz und neuer Improvisationsmusik weist Portugal eine Reihe aktiver Musiker auf, etwa Carlos Zingaro, Ernesto Rodrigues, Carlos Maria Trindade oder Vítor Rua.

Der ehemalige Madredeus-Musiker Rodrigo Leão konnte sich mit seinen modern-klassischen Kompositionen im zeitgemäßen und doch traditionellen Gewand sowohl in seiner Heimat, als auch international einen Namen machen. Das Akkordeon-Quartett Danças Ocultas erhielt international ebenfalls einige Aufmerksamkeit. Auch gewann Portugal 2017 mit dem Sänger Salvador Sobral den 62. Eurovision Song Contest in Kiew.

Aus der Zeit des faschistischen "Estado Novo" unter Salazar kommt eine kritische Liedermachertradition. Bekannteste Vertreter dieser Protestbewegung waren José Afonso (häufig "Zeca" genannt) und Adriano Correia de Oliveira, während Vertreter der Bewegung wie José Mário Branco und insbesondere Sérgio Godinho bis heute in der Musikszene aktiv sind. Von José Afonso stammt das Lied "Grândola, Vila Morena", welches in der Nacht der Nelkenrevolution und darüber hinaus landesweit politische Symbolwirkung entfaltete.

Die musikalischen Traditionen der verschiedenen Regionen werden zudem immer wieder neu belebt und zeitgemäß interpretiert, oft auch durch die Vereinigung verschiedener Musikstile. Künstler wie Trovante, Júlio Pereira oder Rão Kyao, ein Komponist, Musiker und Sänger portugiesischer Musik und des Fado hat sich durch Aufnahme musikalischer Einflüsse aus indischer Musik (Goa, ehemalige portugiesische Kolonie), aus Macau (ehemalige portugiesische Kolonie), aus dem arabischen Raum und aus Nordafrika einen Namen gemacht.

Das Popmusik-Genre, das in den 1960er-Jahren vor allem mit Beatbands wie Quinteto Académico, Conjunto Académico João Paulo und insbesondere den Sheiks in Portugal eingeführt wurde, konnte seit den 1980er-Jahren neben der Rockmusik zur bestimmenden Musik der Jugend aufsteigen, mit Namen wie Heróis do Mar, den Delfins, oder dem früh verstorbenen, exzentrischen Sänger António Variações. Die Band The Gift hat in den letzten Jahren mit ihrem vielschichtigen Pop und ihrem Amália Rodrigues-Tribut-Projekt Hoje für vergleichbar viel Aufsehen unter Musikfreunden im Land gesorgt, ebenso Silence 4 und ihr inzwischen solo auftretender Sänger David Fonseca. Die erste Eurodance-Gruppe Portugals, „Santamaria“, brachte auch Technobeats in die Diskotheken.

Über die Landesgrenzen hinaus konnte sich der Rock und Blues Sänger, Gitarrist und Komponist Rui Veloso einen Namen machen. Neben bekannten Gruppen wie GNR oder UHF sind die unbestritten populärste Rockband des Landes die 1978 als Punkband gegründeten Xutos & Pontapés, während Moonspell die international bekannteste Metal-Band aus Portugal ist. Aus Bands wie der als Psychobilly-Band begonnenen Gruppe Tédio Boys oder der Punkband Censurados sind einige der prägendsten Formationen der vielfältigen Underground- und Independent-Szenen im Land entstanden. Mata-Ratos sind die älteste, noch bestehende Band der Punkszene Portugals.

Jede Region Portugals besitzt ihren Folklorestil ("Ranchos Folclóricos"). Projekte wie die Popbands Sétima Legião oder Sitiados verbinden diese mit zeitgemäßen Popstilen. Die portugiesische Musik- und Tanztradition hat sich in Brasilien mit den Traditionen der Sklaven aus dem heutigen Angola zur Samba gemischt und ist in dieser Mischung ebenso in Portugal populär. Vor allem unter angolanischen Einwanderern ist der Kuduro beliebt. Hierbei handelt es sich um eine Musikrichtung welche unter anderem Einflüsse des Sungura sowie des Afro Zouk beinhaltet. Der Rhythmus ist schnell und hart. Die verbreitete Kizomba ist eine Mischform aus dem angolanischen Semba und Zouk. Es handelt sich meist um romantische Lieder mit entsprechend langsamen Rhythmen. Populär sind diese beiden Musikrichtungen (jedoch die Kizomba im Besonderen) bei der jungen Generation afrikanischer Einwanderer. In den vergangenen Jahren wurde diese Musikrichtung unter Jugendlichen kulturell portugiesischer Herkunft verbreitet.

Zudem hat sich in Portugal der Hip Hop Tuga entwickelt, eine an Portugal adaptierte Version des Hip-Hop, die unter Jugendlichen populär ist. Bekannteste Vertreter wurden Da Weasel und Sam the Kid. Auch Reggae ist nach dem Erfolg der Gruppe Kussondulola in Portugal populärer geworden, mit heutigen Interpreten wie Richie Campbell, Mercado Negro oder Freddy Locks.

Im Bereich des Kunsttanzes ist Portugal seit Anfang der 1990er-Jahre recht erfolgreich und gehört mit Russland zu den wichtigsten Ländern in Europa auf diesem Gebiet. Zahlreiche Tänzer sowie Choreographen konnten europaweiten oder weltweiten Ruhm erlangen: Rui Horta, João Fiadeiro, Clara Andermatt. Dabei werden oftmals moderne und innovative Formen eingesetzt, neue Stile entwickelt. An der Ausbildung junger Tänzer ist Portugal regelmäßig beteiligt. Die Tanzkultur in Portugal wird als "Nova Dança Portuguesa" bezeichnet.

In der Malerei und Bildhauerei erreichten portugiesische Künstler nie große Bekanntheit. Das lag an diversen Gründen: Zum einen gab es keine neuen, innovativen Techniken und Formen aus dem Land. Oft wurden viele Gemälde und Skulpturen zur Ehre Gottes nur für bestimmte Klöster oder Kirchen erschaffen, ohne dass die Namen der Künstler bekannt waren (die oftmals ihren Namen absichtlich nicht angaben). Daneben spielte die oft schwierige Aussprache portugiesischer Namen sowie die Zerstörung von Kunst durch das Erdbeben von 1755 und durch napoleonische Truppen Anfang des 19. Jahrhunderts eine entscheidende Rolle. Dennoch hat auch Portugal viele Maler hervorgebracht. Die heutige Malerei ist an den Tendenzen moderner Malerei ausgerichtet.

Im Mittelalter und der Renaissance waren es oftmals ausländische Maler, die in Portugal wirkten, etwa aus Flandern, die aber keine berühmten Maler waren, sondern dort, weil sich durch die großen Meister ihres Landes verdrängt worden waren, dann in Portugal wirkten konnten. Bedeutende Namen aus dieser Zeit waren Nuno Gonçalves, Gregorio Lopes und Grão Vasco.

Barock, Rokoko und beginnendes 19. Jahrhundert wurde durch Maler wie Domingos de Sequeira, Vieira Portuense oder Francisco Augusto Metrass abgedeckt.

Im 20. Jahrhundert dann kamen viele Maler: Paula Rego, Almada Negreiros, Mario Eloy, Santa Rita Pintor, Maria Helena Vieira da Silva, Amadeo de Souza-Cardoso und viele andere.

Baustile umfassen nahezu alle Epochen der europäischen Kunstgeschichte. Klöster, Kirchen, Burgen, Schlösser und staatliche Einrichtungen wurden oftmals nach den in Europa vorherrschenden Stilen wie z. B. Gotik oder Neoklassizismus, erbaut. Der Architekt Álvaro Siza Vieira erhielt den Pritzker-Preis, ebenso sein Landsmann Eduardo Souto de Moura. Weitere bekannte Architekten waren oder sind (ein Eiffel-Schüler), Miguel Ventura Terra, Tomas Taveira.

In der Dekoration (Baudekoration) konnte Portugal vor allem durch den Manuelismus und die Azulejo-Kunst eine eigene, nationale Note erreichen.

Die portugiesische Küche ist mannigfaltig, sie folgt in mancher Hinsicht der iberischen Tradition, darüber hinaus nahm sie jedoch viele Elemente aus den kolonisierten Gebieten auf. Nach der maurischen Herrschaft über Portugal blieben auch viele nordafrikanische Einflüsse erhalten, dazu gehören der starke Gebrauch von Zucker, Zimt, Gewürzen und Eidotter.

Als Nationalgericht Portugals gilt der Bacalhau. Bereits seit dem 13. Jahrhundert spielte diese Art Trocken- und Salzfisch in der Ernährung der Portugiesen eine bedeutende Rolle. Heute sagt man, dass es in der portugiesischen Küche ein Bacalhau-Rezept für jeden Tag des Jahres gibt. Sardinen, im 16. Jahrhundert das billigste Nahrungsmittel im Land, sind auch heute ein traditionelles Essen ("Sardinhas Assadas"). Zahlreiche weitere Gerichte wie "Caldeirada", "Amêijoas à Bulhão Pato", "Rissóis de Camarão" oder "Arroz de marisco" unterstreichen die Bedeutung von Fisch und anderen Meeresprodukten in der portugiesischen Küche. Auch gegrillte Sardinen sind sehr populär, insbesondere im Sommer.

Typisch sind außerdem Suppen wie "Caldo verde", eine Grünkohl-Kartoffelsuppe aus dem portugiesischen Kohl "Couve-galega", die typischerweise mit "Broa" (Maisbrot) und Chouriço serviert wird, oder die "Sopa alentejana" mit Brot, Ei, Koriander, Knoblauch und Olivenöl. Fleisch wurde im Mittelalter Portugals nur sehr wenig gegessen, trotzdem sind Würste ("enchidos") verbreitet und es gibt einige berühmte Fleischgerichte wie Cozido à portuguesa oder das populäre Schnellgericht Francesinha. "Frango Assado" (Gegrilltes Hähnchen) insbesondere mit scharfem Piri-Piri gewürzt, ist heute ein weitverbreitetes Gericht, das aus den afrikanischen Kolonien nach Portugal kam. Weiters gibt es eine lange Tradition in der Käserei, nennenswert sind Queijo do Pico, Queijo Serra da Estrela oder Queijo de Azeitão.

Süßspeisen nehmen in Portugal einen ganz bedeutenden Platz ein. Die berühmten Pastéis de Nata (Pastéis de Belém) sind eine Spezialität aus Belém und verbreiteten sich in den letzten Jahren über Macau in weite Teile Südostasiens. Einige der zahlreichen weiteren Süßspeisen sind die "Pastéis de Tentúgal", die "Ovos moles de Aveiro", oder auch der vor allem zu Weihnachten verbreitete "Bolo Rei". Vor allem im Herbst werden traditionell geröstete Maronen an kleinen Straßenständen verkauft.

Für seinen Wein ist Portugal bekannt. Seit der Römerzeit wird Portugal mit dem Gott des Weines und der Feste, Bacchus/Dionysos, assoziiert. Einige portugiesische Weine gehören zu den besten der Welt. Eine bekannte Weinspezialität ist der spritzig-moussierende Vinho Verde. Weltbekannt ist der Portwein, während von der Insel Madeira ein weiterer bekannter Likörwein aus Portugal stammt, der Madeira. Daneben gibt es auch einige einheimische Bierbrauereien.

Im Juni finden in ganz Portugal Feste zu Ehren der drei Volksheiligen ("Santos Populares") statt. Diese drei Heiligen sind Antonius, Johannes und Petrus. Gefeiert wird mit Wein, "água-pé" (Most), traditionellem Brot mit Sardinen, Straßenumzügen und -tänzen, Hochzeiten, Feuer und Feuerwerk sowie viel guter Laune.

"Santo António" wird in der Nacht vom 12. auf den 13. Juni gefeiert, vor allem in Lissabon (wo dieser Heilige geboren wurde und lebte), wo eine Art Straßenkarneval ("Marchas Populares") stattfindet. Zu diesen Tagen gibt es Hochzeiten, die "Casamentos de Santo António". Der populärste Heilige ist "São João" (hl. Johannes), für den am Johannistag vor allem in Porto und Braga gefeiert wird, wobei es Sardinen und "Caldo Verde" (eine traditionelle Suppe) gibt. Zu Ehren von "São Pedro" wird am 28. und 29. Juni gefeiert, besonders in Póvoa de Varzim und Barcelos, wobei diese Feste dem Meer gewidmet sind. Dabei gibt es Feuer ("fogeiras") und einen Straßenkarneval.

Fußball ist der am meisten ausgeübte Sport in Portugal. Der portugiesische Fußball hat Weltklassespieler wie Eusébio, Nené, Paulo Sousa, Rui Costa, Nani, Cristiano Ronaldo, Vítor Baía, Deco, Fernando Meira oder Luís Figo hervorgebracht. Im Jahr 2004 wurde die Fußball-Europameisterschaft in Portugal ausgetragen, bei der die portugiesische Nationalmannschaft nach Griechenland Vize-Europameister wurde. Das Erreichen des dritten Platzes bei der WM 1966 war lange Zeit der größte Erfolg der portugiesischen Fußballgeschichte, bis zum Gewinn der EM 2016 in Frankreich. Die höchste Spielklasse, die "Primeira Divisão", wird von den drei bedeutendsten Vereinen FC Porto, Sporting Lissabon, und dem Rekordmeister Benfica Lissabon dominiert. Erster Gewinner des Landespokals "Taça de Portugal" wurde 1939 Académica Coimbra, der ihn 2012 erneut gewinnen konnte, und dank seiner Rolle als oppositioneller Studentenverein der 1960er-Jahre eine besondere Geschichte vorweisen kann. Weitere Traditionsvereine sind Belenenses Lissabon, Boavista Porto und Vitória Setúbal. Neben Fußball sind noch Futsal und Strandfußball verbreitet, und Portugal hat dort Erfolge zu verzeichnen.

Auch im Kanusport kann Portugal Erfolge vorweisen, etwa seine Silbermedaille bei Olympia 2012. Der portugiesische Kanuhersteller Nelo ist Weltmarktführer und rüstete auch die Mehrzahl der erfolgreichen Olympia-Wettbewerbsteilnehmer 2012 aus. In der Kleinstadt Montemor-o-Velho hat der portugiesische Kanuverband mit seinem Leistungszentrum seinen Schwerpunkt. Hier wurden auch mehrmals internationale Veranstaltungen ausgerichtet, zuletzt die Kanurennsport-Europameisterschaften 2013.

Insbesondere portugiesische Langstreckenläufer waren häufig international erfolgreich. Die bekannteste weibliche Läuferin dürfte die olympische Goldmedaillengewinnerin Rosa Mota sein, während Carlos Lopes 1984 die erste olympische Goldmedaille für Portugal überhaupt holte.

Die Orientierungslauf-Europameisterschaften 2014 richtete Portugal aus. Seit 1991 findet in der Hauptstadt mit dem Lissabon-Halbmarathon jährlich im März auch einer der weltweit bedeutendsten Halbmarathonläufe statt.

In der Nähe des Seebades Estoril, nahe der Atlantikküste, befindet sich mit dem Circuito do Estoril eine bekannte Rennstrecke für Auto- und Motorradrennen, auf der jahrelang der Formel 1 Grand Prix von Portugal ausgefahren wurde. Auch als Teststrecke für Rennwagen wird der Kurs in Estoril genutzt.

In der Hafenstadt Portimão ist das Autódromo Internacional do Algarve, 

wo die Superbike-Weltmeisterschaft und die FIA-GT-Meisterschaft ausgerichtet werden. 

In der Stadt Santarém befindet sich ein bekanntes Speedway-Stadion, in dem auch bereits internationale Meisterschaften ausgefahren wurden, wie die Europäische Speedway-Club-Meisterschaft im Jahre 2000.

Seit 1927 wird mit der "Volta a Portugal" ein landesweites Rennen des populären Radsports veranstaltet. Beliebte Radfahrer waren der erste Berufssportler Portugals 1896, José Bento Pessoa, der zweimalige Tour de France-Dritte Joaquim Agostinho, oder auch Alves Barbosa, der auf dem Höhepunkt seiner Popularität 1958 Titelheld des ersten Werks des Portugiesischen Films in Cinemascope wurde.

Ganzjährig bieten die Küsten im Süden und Westen ideale Bedingungen zum Wellenreiten. Einige der besten Surfspots Europas ziehen Surfer aus aller Welt an, etwa in Ericeira, das weltweit dritte und Europas erstes Surfreservat. Unter den vielen weiteren Surfspots sind das traditionsreiche Seebad Figueira da Foz, der nahe Lissabon gelegene Strand Praia do Guincho, oder auch das frühere Fischerdorf Nazaré, das für seine besonders große Welle bekannt ist.

Segeln hat eine lange Tradition in Portugal. Bekannt dafür sind insbesondere die Azoren, etwa mit der Les Sables–Les Açores–Les Sables-Regatta oder dem international bekannten Treffpunkt, der Stadt Horta. Segelveranstaltungen finden zudem an der Algarve und im Großraum Lissabon statt. So fanden die ISAF-Segel-Weltmeisterschaften 2007 in Cascais statt, das auch für seinen Yachthafen bekannt ist.

Mit dem ATP Oeiras beziehungsweise WTA Oeiras finden in Oeiras bekannte internationale Tennisturniere in Portugal statt. Weitere Turniere sind ATP Porto bzw. WTA Porto, und seit 2013 ATP Challenger Guimarães. Zuletzt war João Sousa aus Guimarães der vielversprechendste portugiesische Tennisspieler. Am 14. Juli 2014 erreichte er mit dem 35. Platz seine bisher beste Platzierung in der Tennisweltrangliste.

Für portugiesisches Badminton ist dagegen Caldas da Rainha der wichtigste Ort, etwa als häufiger Austragungsort für Turniere der Portugal International, und als Sitz des portugiesischen Badmintonverbandes. Als erfolgreichste Spielerin kann Isabel Rocha gelten, die in den 1960er- und 1970er-Jahren insgesamt 32 nationale Titel gewann. Als besonders erfolgreicher männlicher Spieler ist José Bento zu nennen. Er stammte aus Lourenço Marques, der heute Maputo genannten Hauptstadt der damaligen portugiesischen Kolonie Mosambik, und dominierte besonders während der 1970er-Jahre das Badminton in Portugal.

Außerhalb Portugals eher unbekannt sind die Snowboard-Meisterschaften in den Skigebieten der Serra da Estrela, oder auch der portugiesische Eishockeyverband "Federação Portuguesa de Desportos no Gelo".

Mit 13 Landesmeisterschaften ist Joaquim Durão Rekordmeister im Schach. Erfolgreichste weibliche Spielerin ist Catarina Leite, während Schachgroßmeister Luís Galego der aktuell bedeutendste Schachspieler Portugals sein dürfte.

Portugiesische Sportler sind international im Beachvolleyball und insbesondere im Rollhockey erfolgreich, wo sie mit Spanien im Wechsel Rekord-Rollhockey-Weltmeister sind. Traditionelle Sportarten Portugals wie das Jogo do pau sind dagegen international weitgehend unbekannt.

Portugal richtete eine Vielzahl internationaler Sportveranstaltungen aus, neben der Fußball-Europameisterschaft 2004 sind so verschiedene Turniere wie die Handball-Europameisterschaft 1994 oder die Spiele der Portugiesischsprachigen Länder, die Jogos da Lusofonia 2009 zu nennen. Inline-Speedskating-Europameisterschaften fanden mehrmals in Portugals statt, so 1989, 1995, 2001 und 2007.

Portugal verfügt, ausgehend von mittelalterlichen und klösterlichen Sammlungen, über eine lange Bibliothekstradition. So entwickelten sich bis heute vielfältige Bibliothekstypen, wie wissenschaftliche Bibliotheken, Universitätsbibliotheken, öffentliche Bibliotheken, Bibliotheken der zentralen Verwaltung und Spezialbibliotheken. Die genaue Anzahl von Bibliotheken und der Gesamtbestand an Medien sind nicht bekannt (die LIB2-Studie von 1986 ermittelte 556 portugiesische Bibliotheken). Systematische und methodische Arbeiten zur Förderung des öffentlichen Bibliothekswesens und der Bibliothekswissenschaft begannen Ende des 19. Jahrhunderts.

In der Zeit des Estado Novo (1928–1974) waren die Bedeutung der Bibliotheken und die Bibliotheksarbeit selbst durch Zensur und Restriktionen stark eingeschränkt. Dadurch gibt es bis heute Defizite in der Entwicklung des Bildungs- und Bibliothekssystems. Die jahrzehntelange Diktatur begünstigte eine mangelnde Volksbildung und den Analphabetismus. Nach der Nelkenrevolution 1974 kam es zur Demokratisierung im Bildungs- und Kulturbereich.

Gegen die mangelnde Volksbildung und den Analphabetismus wurde systematische Erwachsenenbildung und Leseförderung betrieben. Die besorgniserregende Situation der öffentlichen Bibliotheken hatte zahlreiche Initiativen und Neuregelungen innerhalb des Bibliothekswesens zur Folge, z. B. 1983 das „Manifest des öffentlichen Lesens“. 1986 wurde dies durch eine Gesetzgebung zur Schaffung und Koordinierung eines Netzes des öffentlichen Lesens untermauert. Gleichzeitig kam es zur in Portugal erst relativ spät einsetzenden Automatisierung der Bibliotheksarbeit und zum Einsatz moderner Informationstechnik, zunächst in Universitätsbibliotheken und der Nationalbibliothek „Biblioteca Nacional de Lisboa“.

Letztere war die erste öffentliche Bibliothek, die 1796 als Königlich-öffentliche Hofbibliothek gegründet wurde. Sie betreibt z. B. die nationale bibliographische Datenbank PORBASE. Diese enthält über 1 Million Titeleinträge, 800.000 Verfassereinträge von etwa 134 Bibliotheken und Dokumentationszentren und die Portugiesische Nationalbibliographie. Die Nationalbibliothek und vermutlich fast alle anderen Bibliotheken arbeiten mit dem Bibliothekssystem CDS/ISIS und dem Datenaustauschformat UNIMARC.

Eine Archivar- und Bibliothekar-Ausbildung ist über ein Studium an den staatlichen Universitäten Coimbra, Lissabon und Porto möglich. Einige zum Teil staatliche Institutionen übernehmen Koordinierungsaufgaben und unterstützen die Förderung des portugiesischen Buches und die Kooperation und Unterstützung von Bibliotheken.

Durch umfangreiche innovative Arbeiten der letzten Jahre hat das portugiesische Bibliothekswesen den Anschluss an europäische und internationale Standards geschafft. Noch existierende Defizite sollen durch Förderung des Lesens und der Bibliotheken und durch internationale Zusammenarbeit weiter abgebaut werden.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Portugal Platz 18 von 180 Ländern. Das Land hatte damit eine der freiesten Pressen der Welt.

Vier Hauptfernsehkanäle können im ganzen Land per Antenne empfangen werden: "RTP1" und "RTP2", die vom staatlichen portugiesischen Rundfunk "Rádio e Televisão de Portugal" (RTP) betrieben werden und die privaten Kanäle "SIC" (Sonae Group) und "TVI" (Media Capital, davon 32 % RTL-Group). Das Programm wird bei diesen Sendern, außer dem kulturell ausgerichteten "RTP2", besonders am Abend stark von brasilianischen und portugiesischen Telenovelas bestimmt; die Nachrichtensendungen sind mit meist ein bis zwei Stunden Dauer sehr lang und in hohem Maße auf das Tagesgeschehen in Portugal ausgerichtet. Fremdsprachige Spielfilme werden angesichts des kleinen Binnenmarktes selten synchronisiert, sondern mit Untertiteln gezeigt. Der Auslandssender "RTP Internacional" kann u. a. auch in Mitteleuropa empfangen werden und zeigt eine Auswahl der vier Programme, während "RTP África" aus den portugiesischsprachigen Ländern Afrikas berichtet. Daneben gibt es eine Vielzahl von Kabelkanälen, zu nennen ist insbesondere Sport TV und brasilianische Sender. In Porto und Lissabon gibt es Lokalsender, zudem unterhält die RTP auf Madeira und den Azoren eigene Sendeanstalten.

Es gibt etwa 150 Radiostationen in Portugal. Die Sender der "RTP", des katholischen Rádio Renascença und der TSF sind landesweit zu empfangen. Die RTP ist über Kurzwelle auch in Mitteleuropa zu hören, allerdings nur auf Portugiesisch.

Unter den zahlreichen Zeitungen, die in Portugal gedruckt werden, findet ein Konsolidierungs- und Konzentrationsprozess statt, bei dem viele der kleinen Blätter aufgeben müssen. Bedeutende täglich erscheinende Zeitungen sind der konservativ-liberale "Diário de Notícias", der linksliberale "Público" (beide aus Lissabon) und das "Jornal de Notícias" aus Porto, dazu die Boulevardzeitung "Correio da Manhã". Wichtige Wochenzeitungen sind "Expresso" und "Sol", auch das politische Wochenmagazin "Visão" und die Musikzeitung "Blitz" sind zu nennen. "Jornal de Letras" gehört zu den bedeutendsten Kulturzeitungen des Landes, während "Jornal de Negócios" und der "Diário Económico" die bedeutenden Wirtschaftszeitungen sind. "Destak" und "Metro" sind die wichtigsten Gratiszeitungen in Portugal.

Sehr große Auflagen haben Sportzeitungen, die täglich erscheinen und sich fast ausschließlich mit Fußball beschäftigen – die bedeutendsten sind "O Jogo", "A Bola" und "Record". Die Benfica Lissabon nahestehende "A Bola" ist die auflagenstärkste Zeitung in Portugal.

Football Leaks Betreiber der Seite sind unbekannte Whistleblower aus Portugal.

Von Bedeutung sind auch die zahlreichen Lokalzeitungen. Unter den Regionalzeitungen ist https://omirante.pt und der Diário As Beiras zu nennen.

Im Bereich der Klatschpresse sind die wöchentlich erscheinenden Magazine "Maria" und "Nova Gente" die auflagenstärksten. Die bedeutendste Parteizeitung ist der "Avante!" von der Portugiesischen Kommunistischen Partei.

"Anmerkung:" Jeder Kreis hat einen eigenen kommunalen Feiertag, oft für den Heiligen Antonius am 13. Juni, den Heiligen Johannes am 24. Juni oder den Heiligen Petrus am 29. Juni. Wenn der Kreis einen solchen Feiertag nicht hat, ist Karneval ein gesetzlicher Feiertag. Der Karneval wird wie ein normaler Feiertag behandelt, insbesondere Staatsbedienstete haben an diesem Tag normalerweise frei. Der erfolglose Versuch im Jahre 1995, diese Regelung abzuschaffen, kostete den damaligen Ministerpräsidenten Cavaco Silva viel Popularität. Daneben wird der 24. Juni als Feiertag diskutiert (Unabhängigkeit Portugals im Jahre 1128).

Neben den zahlreichen Reiseführern bietet sich folgendes an:



</doc>
<doc id="4018" url="https://de.wikipedia.org/wiki?curid=4018" title="Portugiesische Sprache">
Portugiesische Sprache

Die portugiesische Sprache (portugiesisch ) ist eine Sprache aus dem romanischen Zweig der indogermanischen Sprachfamilie und bildet mit dem Spanischen (der kastilischen Sprache), Katalanischen und weiteren Sprachen der Iberischen Halbinsel die engere Einheit des Iberoromanischen. Zusammen mit dem Galicischen in Nordwest-Spanien geht sie auf eine gemeinsame Ursprungssprache zurück, das Galicisch-Portugiesische, das sich zwischen Spätantike und Frühmittelalter entwickelte. Nach der Herausbildung der Staatlichkeit Portugals entwickelten sich daraus die beiden heutigen Sprachen. Heute gilt Portugiesisch als Weltsprache.

Es wird von über 240 Millionen Muttersprachlern gesprochen; einschließlich der Zweitsprachler beläuft sich die Zahl der Sprecher auf etwa 270 Millionen.

Die portugiesische Sprache verbreitete sich weltweit im 15. und 16. Jahrhundert, als Portugal sein Kolonialreich aufbaute, das in Teilen bis in das Jahr 1975 überdauerte und das heutige Brasilien sowie Gebiete in Afrika und an den Küsten Asiens umfasste. Als letztes ging Macau aus portugiesischem Besitz an China über. Daraus ergab sich, dass Portugiesisch heute die Amtssprache mehrerer unabhängiger Staaten ist und darüber hinaus von vielen Menschen als Minderheiten- oder Zweitsprache gesprochen wird. Neben dem eigentlichen Portugiesischen gibt es etwa zwanzig Kreolsprachen auf überwiegend portugiesischer Basis. Durch die Auswanderung aus Portugal ist Portugiesisch in den letzten Jahrzehnten in mehreren Staaten Westeuropas und in Nordamerika zu einer wichtigen Minderheitensprache geworden.

Portugiesisch ist alleinige Amtssprache in Angola, Brasilien, Mosambik, Portugal und São Tomé und Príncipe. Zusammen mit anderen Sprachen ist Portugiesisch Amtssprache in Osttimor (zusammen mit Tetum), Macau (zusammen mit Chinesisch) und Äquatorialguinea (zusammen mit Französisch und Spanisch). Auf Kap Verde und in Guinea-Bissau ist es zwar alleinige Amtssprache, jedoch nicht die wichtigste Sprache. Eine wichtige Sprache, aber keine Amtssprache ist Portugiesisch in Andorra, Luxemburg (aufgrund der Zuwanderung von portugiesischen Arbeitskräften von etwa zehn Prozent der Bevölkerung gesprochen), Namibia und Südafrika.

Mit über 190 Millionen Sprechern in Brasilien ist Portugiesisch die am weitesten verbreitete Sprache in Südamerika. Aber auch in den spanischsprachigen Ländern Südamerikas erfreut sich Portugiesisch wachsender Bedeutung. Wegen des großen Einflusses Brasiliens wird Portugiesisch in einigen der restlichen südamerikanischen Staaten unterrichtet, besonders in Argentinien und den anderen Mercosur (Mercosul)-Mitgliedsstaaten. Im Grenzgebiet von Brasilien zu Argentinien, Bolivien, Paraguay (Brasiguayos) und Uruguay gibt es Menschen, für die Portugiesisch Muttersprache ist (in Paraguay leben 122.520 Portugiesisch-Muttersprachler gemäß der 2002 durchgeführten Volkszählung). Unter den Menschen, die im Grenzgebiet leben, aber der jeweils anderen Sprache nicht mächtig sind, hat sich teilweise eine Mischsprache aus Portugiesisch und Spanisch namens Portunhol herausgebildet. Darüber hinaus ist Portugiesisch eine wichtige Minderheitensprache in Guyana und Venezuela.

In Nordamerika und der Karibik gibt es große portugiesischsprachige Kolonien in Antigua und Barbuda, Bermuda, Kanada, Jamaika und den Vereinigten Staaten, wobei sich die Mehrzahl aus Einwanderern beziehungsweise Gastarbeitern aus Brasilien oder Portugal zusammensetzt. In Mittelamerika ist die portugiesische Sprache dagegen nur von geringer Bedeutung.

In Europa wird Portugiesisch vor allem von den 10,6 Millionen Einwohnern Portugals gesprochen. In Westeuropa hat sich die Sprache vor allem durch Einwanderung aus Portugal in den letzten Jahrzehnten verbreitet und wird von mehr als zehn Prozent der Bevölkerung Luxemburgs und Andorras gesprochen. Daneben gibt es einen nennenswerten Anteil portugiesischsprachiger Bevölkerung in Belgien, Frankreich, Deutschland, auf Jersey und in der Schweiz. In Spanien wird Portugiesisch im Vale do Xalima gesprochen, wo es als "A fala" bezeichnet wird. Im heute spanischen Olivença wurde bis in die 1940er Jahre ein portugiesischer Dialekt gesprochen. Das mit Portugiesisch sehr eng verwandte Galicisch wird im nordwestspanischen Galicien gesprochen.

Galicisch und Portugiesisch haben dieselben Wurzeln und waren bis zum Mittelalter eine einzige Sprache, die man heute als Galicisch-Portugiesisch bezeichnet. Diese Sprache wurde sogar in Spanien (Kastilien) im poetischen Schaffen verwendet. Auch heute werden von vielen Linguisten Galicisch und Portugiesisch als eine Einheit gesehen. Aus soziolinguistischen Gründen werden die beiden Sprachen jedoch häufig getrennt gesehen. In Galicien haben sich zwei Standards der Schriftsprache gebildet, wobei sich derjenige, der von der Galicischen Autonomen Regierung gestützt wird, mehr am Spanischen (Kastilischen) anlehnt, während sich in gewissen politischen und universitären Kreisen ein Standard etabliert hat, der sehr nah am Portugiesischen liegt. Der einzige galicische Abgeordnete im Europäischen Parlament, Camilo Nogueira, spricht nach eigenen Angaben Portugiesisch.

Portugiesisch ist eine wichtige Sprache im Afrika südlich der Sahara. Angola und Mosambik sind zusammen mit São Tomé und Príncipe, Kap Verde, Äquatorialguinea und Guinea-Bissau als PALOP "(Paises Africanos de Língua Oficial Portuguesa)" bekannt und organisiert; sie vertreten etwa 32 Millionen Sprecher des Portugiesischen (großzügige Schätzungen gehen dabei von neun Millionen Muttersprachlern aus, der Rest ist zweisprachig). Paradoxerweise ist der Gebrauch der portugiesischen Sprache nach der Unabhängigkeit der früheren Kolonien von Portugal gewachsen. Die Regierungen der jungen Staaten sahen die portugiesische Sprache als Instrument zur Entwicklung des Landes und einer nationalen Einheit.

In Afrika ist Portugiesisch eine wichtige Minderheitensprache in der Demokratischen Republik Kongo, Malawi, Namibia, Südafrika (mehr als eine Million Sprecher), Sambia und Simbabwe.

In anderen Teilen Afrikas gibt es portugiesische Kreolsprachen. Im Süden Senegals, in Casamance, gibt es eine Gemeinschaft, die sprachlich und kulturell mit Guinea-Bissau verwandt ist und wo Portugiesisch gelernt wird. Auf der Insel Annobón (Äquatorialguinea) gibt es eine weitere Kreolsprache, die mit jener von São Tomé und Príncipe eng verwandt ist.

In Angola wurde Portugiesisch schnell zu einer Nationalsprache statt "nur" einer Verkehrssprache. Dies gilt insbesondere für die Hauptstadt Luanda. Gemäß der offiziellen Volkszählung von 1983 war Portugiesisch damals die Muttersprache von 75 % der Bevölkerung Luandas von etwa 2,5 Millionen (mindestens 300.000 davon sprachen es dazu als einzige Sprache), und 99 % davon konnten sich auf Portugiesisch verständigen, wenn auch mit unterschiedlicher Sprachkompetenz. Dieses Ergebnis ist kaum erstaunlich, denn bereits für die siebziger Jahre gab eine 1979 in den Slumgebieten Luandas geführte Umfrage an, dass alle afrikanischen Kinder von sechs bis zwölf Jahren Portugiesisch sprachen, aber nur 47 % eine afrikanische Sprache. Heute sprechen vor allem in Luanda junge Angolaner neben dem Portugiesischen nur noch selten eine afrikanische Sprache. Landesweit benutzen etwa 60 % der Bevölkerung, die laut Volkszählung von 2014 25,8 Millionen beträgt, Portugiesisch als Umgangssprache. Ebenfalls bei der Volkszählung von 2014 gaben 71,15 % der Befragten an, dass sie Portugiesisch auch zu Hause sprechen (85 % in den städtischen und 49 % in den ländlichen Gebieten). Die Fernsehstationen aus Portugal und Brasilien, die man in Angola empfangen kann und die sehr populär sind, tragen dazu ihren Anteil bei.

Das angolanische Portugiesisch beeinflusste auch das heute in Portugal gesprochene Portugiesisch, da die "Retornados," (portugiesische Rückkehrer nach der Unabhängigkeit Angolas) und angolanische Zuwanderer Wörter mitbrachten, die sich vor allem in der jungen Stadtbevölkerung verbreiteten. Dazu gehören "iá" "(ja)", "bué" "(viele)" oder "bazar" "(weggehen)".

Mosambik gehört zu den Ländern, in denen Portugiesisch Amtssprache ist, es wird aber größtenteils nur als Zweitsprache gesprochen. In den Städten ist es aber die am meisten verbreitete Sprache. Gemäß der Volkszählung von 1997 sprechen etwa 40 % der Gesamtbevölkerung Portugiesisch, jedoch etwa 72 % der Stadtbevölkerung. Andererseits bezeichnen nur 6,5 % (bzw. 17 % in den Städten und 2 % in den ländlichen Gebieten) Portugiesisch als ihre Muttersprache. Die mosambikanischen Schriftsteller verwenden alle ein Portugiesisch, das sich an die mosambikanische Kultur angepasst hat.

Auf Kap Verde und in Guinea-Bissau sind die wichtigsten Sprachen portugiesische Kreolsprachen, die als Crioulos bezeichnet werden, wohingegen der Gebrauch der portugiesischen Sprache als Umgangssprache im Abnehmen begriffen ist. Die meisten Kapverdier können aber auch Standard-Portugiesisch sprechen, das in formellen Situationen verwendet wird. Schulbildung und Fernsehen aus Portugal und Brasilien tragen andererseits zur Entkreolisierung bei. In Guinea-Bissau ist die Lage etwas anders, weil nur etwa 60 % der Bevölkerung Kreolisch sprechen, und gar nur 10,4 % davon beherrschen Standard-Portugiesisch (gemäß der Volkszählung von 1992).

In São Tomé und Príncipe spricht die Bevölkerung eine Art archaisches Portugiesisch, das viele Ähnlichkeiten mit brasilianischem Portugiesisch aufweist. Die Elite des Landes verwendet jedoch eher die europäische Version, ähnlich wie in den anderen PALOP-Ländern. Neben dem eigentlichen Portugiesisch gibt es noch drei Kreolsprachen. Kinder lernen in der Regel Portugiesisch als Muttersprache und eignen sich das Forro genannte Kreolisch erst später an. Der tägliche Gebrauch der portugiesischen Sprache auch als Umgangssprache ist im Wachsen begriffen, und fast die gesamte Bevölkerung beherrscht diese Sprache.

Portugiesisch wird in Osttimor, in den indischen Staaten Goa und Daman und Diu sowie in Macau (Volksrepublik China) gesprochen. In Goa wird Portugiesisch als "Sprache der Großeltern" bezeichnet, weil es nicht mehr in der Schule unterrichtet wird, keinen offiziellen Status hat und deshalb von immer weniger Menschen gesprochen wird. In Macau wird Portugiesisch nur von der kleinen portugiesischen Bevölkerung gesprochen, die nach der Übergabe der früheren Kolonie an China dort geblieben ist. Es gibt dort nur eine einzige Schule, in der auf Portugiesisch unterrichtet wird. Trotzdem bleibt Portugiesisch vorerst eine offizielle Sprache neben Chinesisch.

Es gibt in Asien mehrere portugiesische Kreolsprachen. In der malaiischen Stadt Malakka gibt es eine Kreolsprache namens "Cristão" oder "Papiá Kristang," andere aktive Kreolsprachen findet man in Indien, Sri Lanka und auf Flores. In Japan gibt es etwa 250.000 Personen, die als "dekasegui" bezeichnet werden; das sind Brasilianer japanischer Abstammung, die wieder nach Japan zurückgekehrt sind, deren Muttersprache jedoch Portugiesisch ist.

In Osttimor ist die am weitesten verbreitete Sprache Tetum, eine austronesische Sprache, die jedoch von der portugiesischen Sprache stark beeinflusst wurde. Am Ende der portugiesischen Kolonialzeit konnten durch eine rudimentäre Schulausbildung viele Timoresen zumindest in Grundlagen Portugiesisch sprechen. Die Wiedereinführung des Portugiesischen als Nationalsprache nach der indonesischen Besatzung (1975–1999) stieß aber bei der jüngeren Bevölkerung, die durch das indonesische Bildungssystem gegangen ist und Portugiesisch nicht beherrscht, auf Missfallen. Überwiegend spricht die ältere Generation Portugiesisch, doch der Anteil steigt, da die Sprache der jüngeren Generation und interessierten Erwachsenen unterrichtlich vermittelt wird. Osttimor hat die anderen CPLP-Staaten um Hilfe bei der Einführung des Portugiesischen als Amtssprache gebeten. Osttimor versucht, mit Hilfe der portugiesischen Sprache Anschluss an die internationale Gemeinschaft zu finden und sich von Indonesien abzugrenzen. Xanana Gusmão, der erste Präsident Osttimors seit der Wiederherstellung der Unabhängigkeit, hoffte, dass innerhalb von zehn Jahren Portugiesisch in Osttimor weit verbreitet sein würde. 2015 ergab die Volkszählung, dass 1.384 Osttimoresen Portugiesisch als Muttersprache haben, 30,8 % der Bevölkerung können Portugiesisch sprechen, lesen und schreiben, 2,4 % sprechen und lesen, 24,5 % nur lesen und 3,1 % nur sprechen. Es gibt Stimmen, die in der Einführung von Portugiesisch als Amtssprache durch die alten Bildungseliten einen Fehler sehen. So werden die meisten Universitätslehrgänge immer noch in Bahasa Indonesia gehalten. Englisch hat immer größere Bedeutung durch die Nähe zu Australien und durch die internationalen Friedenstruppen, die bis 2013 im Land waren. Portugiesisch wird erst nach und nach in der Schule den Kindern beigebracht und als Unterrichtssprache neben Tetum verwendet. Unter anderem liegt das am Mangel an portugiesischsprechenden Lehrern. Die portugiesische Kreolsprache Osttimors "Português de Bidau" starb in den 1960er Jahren aus. Die Sprecher verwendeten immer öfter das Standard-Portugiesisch. "Bidau" wurde nahezu nur im Stadtteil Bidau im Osten der Hauptstadt Dili von der Volksgruppe der Bidau gesprochen, Mestizen mit Wurzeln auf der Insel Flores. Kreolisches Portugiesisch aus Macau wurde während der stärksten Einwanderungsphase im 19. Jahrhundert auch auf Timor gesprochen, verschwand aber schnell.

Die Gemeinschaft der Portugiesischsprachigen Länder CPLP ist eine internationale Organisation von acht unabhängigen Staaten, deren Amtssprache Portugiesisch ist. Portugiesisch ist auch offizielle Sprache der Europäischen Union, des Mercosul, der Afrikanischen Union und einiger anderer Organisationen.

Portugiesisch ist Amtssprache in:

Die portugiesische Sprache entwickelte sich im Westen der iberischen Halbinsel aus einer Form der gesprochenen lateinischen Sprache (Vulgärlatein), die von römischen Soldaten und Siedlern seit dem 3. Jahrhundert v. Chr. auf die Halbinsel gebracht worden war. Nach dem Zusammenbruch des Römischen Reiches begann sich das Galicisch-Portugiesische unter Einfluss der vorrömischen Substrate und der späteren Superstrate getrennt von den übrigen romanischen Sprachen zu entwickeln. Ab dem 11. Jahrhundert sind schriftliche Dokumente überliefert, die auf Portugiesisch abgefasst wurden. Bis zum 15. Jahrhundert hatte sich die portugiesische Sprache zu einer reifen Sprache mit einer reichen Literatur entwickelt.

Ab dem Jahre 154 v. Chr. eroberten die Römer den Westen der Iberischen Halbinsel mit dem heutigen Portugal und Galicien, woraus die spätere römische Provinz Lusitanien wurde. Mit den Siedlern und Legionären kam auch eine volkstümliche Version des Lateins, das Vulgärlatein, von dem alle romanischen Sprachen abstammen. Obwohl das Gebiet des heutigen Portugal bereits vor der Ankunft der Römer bewohnt war, stammen 90 Prozent des portugiesischen Wortschatzes vom Lateinischen ab. Es gibt nur sehr wenige Spuren der ursprünglichen Sprachen im modernen Portugiesischen.

Vom Jahre 409 an, als das weströmische Reich zusammenzubrechen begann, drangen Völker germanischen Ursprungs auf die iberische Halbinsel vor. Diese Germanen, hauptsächlich Sueben und Westgoten, assimilierten sich langsam an die römische Sprache und Kultur. Da jedoch der Kontakt zu Rom gering war, entwickelte sich Latein unabhängig weiter, wobei sich die regionalen Unterschiede verstärkten. Die sprachliche Einheit auf der iberischen Halbinsel wurde somit langsam zerstört und es entwickelten sich voneinander unterscheidbare Dialekte, darunter die heute zu Standardsprachen weiterentwickelten Formen Galicisch-Portugiesisch, Spanisch und Katalanisch. Die Entwicklung des Galicisch-Portugiesischen weg vom Spanischen und Mozarabischen wird unter anderem auf die Sueben zurückgeführt. Germanismen kamen somit auf zwei Wegen in das Portugiesische: indirekt als germanische Entlehnungen, die als Bestandteil der gewöhnlichen lateinischen Umgangssprache der römischen Legionäre auf die iberische Halbinsel gelangten, darüber hinaus direkt als Lehnwörter gotischer und suebischer Herkunft. Andere Autoren gehen jedoch davon aus, dass das Portugiesische älter und konservativer ist als das in der Region um Burgos entstandene Castillano, welches verschiedene Lautverschiebungen mitgemacht hat, die sich im Portugiesischen nicht finden (z. B. vom lateinischen palatalen "f" zum friktionalen "h", von lat. "ct" zu "ch" wie in "nocte"/"noche", von Diphthongen wie "au" zu Monopthongen wie "o").

Ab 711 eroberten die Mauren die iberische Halbinsel und in den eroberten Gebieten wurde das Arabische zur Verwaltungssprache. Die Bevölkerung sprach jedoch weiterhin ihren iberomanischen Dialekt, weshalb der Einfluss des Arabischen auf das Portugiesische nicht sehr stark war. Es entwickelte sich auch eine romanische Schriftsprache in arabischer Schrift, das sogenannte Mozarabische. Nachdem die Mauren durch die Reconquista vertrieben worden waren, blieben viele in ihrem rechtlichen Status stark beschränkte Araber auf dem Gebiet des heutigen Portugals, sie waren später auch als freie Handwerker tätig und assimilierten sich an die portugiesische Kultur und Sprache. Aufgrund des Kontakts mit dem Arabischen lassen sich arabische Spuren hauptsächlich in der Lexik finden, wo das Neuportugiesische viele Wörter arabischen Ursprungs besitzt, die sich in anderen romanischen Sprachen nicht wiederfinden. Diese Einflüsse betreffen vor allem die Bereiche Ernährung und Landwirtschaft, in denen durch die Araber Neuerungen eingeführt wurden. Daneben ist der arabische Einfluss in Ortsnamen des südlichen Portugal wie "Algarve" oder des im Westen gelegenen "Fátima" ersichtlich.

Von der römischen Provinz Lusitanien spalteten die Römer im 1. Jh. v. Chr. die Gallaecia (das heutige Galicien) ab und gliederten sie der Tarraconensis (Hispania Citerior) an. Die portugiesische Sprache entwickelte sich (wie auch das Galicische) aus dem heute bis auf wenige Reste (A Fala) ausgestorbenen Galicisch-Portugiesischen, das im Zeitraum vom 8. bis 12 Jh. im heutigen Nordportugal sowie im heutigen Galicien entstand. Das Galicisch-Portugiesische existierte über lange Zeit nur als gesprochene Sprache, während als Schriftsprache weiterhin Latein benutzt wurde. Die frühesten schriftlichen Zeugnisse dieser Sprache sind die „Cancioneiros“ aus der Zeit um 1100. Das Galicisch-Portugiesische entwickelte sich im Hochmittelalter (13./14. Jh.) zur wichtigsten Sprache der Lyrik auf der Iberischen Halbinsel.

Die Grafschaft Portugal wurde im Jahr 1095 unabhängig, ab 1139 war Portugal Königreich unter König Alfons I.
Nach der Unabhängigkeit Portugals von Kastilien entwickelte sich die Sprache vor allem durch den normierenden Einfluss des Königshofs (im Gegensatz zu Galicien) auf portugiesischem Gebiet langsam weiter. Erste schriftliche Zeugnisse des sogenannten romanischen Dialekts sind das Testament von Alfons II. und die Notícia de Torto aus dem Jahr 1214.

Im Jahre 1290 gründete König Dionysius (Diniz) die erste portugiesische Universität, das "Estudo Geral" in Lissabon. Er legte fest, dass das "Vulgärlatein," wie das Portugiesische damals noch genannt wurde, dem klassischen Latein vorgezogen werden solle. Ab 1296 benutzten die königlichen Kanzleien das Portugiesische, womit die Sprache nicht mehr nur in der Poesie, sondern auch in Gesetzen und notariellen Schriftstücken Verwendung fand.

Durch die Ausstrahlung der höfischen Kultur Südfrankreichs auf die galicische Dichtersprache im 12. und 13. Jahrhundert gelangten auch okzitanische Lehnwörter in das Sprachgebiet Portugals. Im modernen Portugiesischen hat sich aber nur eine begrenzte Zahl dieser Wörter erhalten. Von größerer Bedeutung für die Ausprägung des Wortschatzes ist der französische Spracheinfluss, der heute nicht nur lexikalisch, sondern auch phraseologisch nachweisbar ist.

Mit der Reconquista-Bewegung dehnte sich der Einflussbereich des Portugiesischen allmählich nach Süden hin aus. Bis zur Mitte des 13. Jahrhunderts endete diese Ausdehnung an der Südgrenze des heutigen Portugals mit der Rückeroberung Faros im Jahr 1249, wodurch der gesamte Westen der Iberischen Halbinsel zum galicisch-portugiesischen Sprachgebiet wurde. Im 14. Jahrhundert war Portugiesisch zu einer reifen Sprache geworden, die eine reiche literarische Tradition besaß und auch in anderen Gegenden der iberischen Halbinsel in der Dichtung verbreitet war wie etwa im Königreich León, Kastilien, Aragón und Katalonien. Später, als sich Kastilisch (was praktisch das moderne Spanisch ist) in Kastilien fest etablierte und Galicien unter den Einfluss der kastilischen Sprache kam, wurde die südliche Variante des Galicisch-Portugiesischen zur Sprache Portugals.

Bei der Entwicklung der portugiesischen Sprache hatten das Arabische und Mozarabische einen erheblichen Einfluss: Während der Reconquista rückte das Zentrum Portugals immer weiter nach Süden Richtung Lissabon, wo es Sprecher verschiedenster Varietäten gab, die das Galicisch-Portugiesische beeinflussten – im Gegensatz zum Norden, dessen Sprache konservativer und noch stärker vom Latein geprägt war. An einem Dialekt im Norden Portugals (an der Grenze zu Spanien), dem Mirandés, ist die Verbindung zum Königreich León, d. h. der kastilische Einfluss, noch zu erkennen, während der leonesiche Dialekt in Spanien z. T. stark vom Galicisch-Portugiesischen geprägt ist.

Zwischen dem 14. und 16. Jahrhundert, also während der Zeit der portugiesischen Entdeckungen, verbreitete sich die portugiesische Sprache in vielen Regionen von Asien, Afrika und Amerika. Im 16. Jahrhundert war es die "lingua franca" in Asien und Afrika, wo es nicht nur der Kolonialverwaltung, sondern auch dem Handel und der Kommunikation zwischen den lokalen Machthabern und den Europäern aller Nationalitäten diente. In Ceylon (heutiges Sri Lanka) sprachen einige Könige fließend Portugiesisch und Adlige nahmen häufig portugiesische Namen an. Die Ausbreitung der Sprache wurde auch durch die Ehen zwischen Portugiesen und Einheimischen gefördert (was im portugiesischen Kolonialreich eine gängigere Praxis als in anderen Kolonialreichen war). Da die Sprache in vielen Erdteilen mit den missionarischen Aktivitäten der Portugiesen gleichgesetzt wurde, nannte man das Portugiesische dort auch "Cristão" (Christlich). Obwohl später die Niederländer versuchten, in Ceylon und dem heutigen Indonesien das Portugiesische zurückzudrängen, blieb es dort lange eine populäre und verbreitete Sprache.

In Indien, Sri Lanka, Malaysia und Indonesien entwickelten sich portugiesische Kreolsprachen heraus, nachdem Portugal den Einfluss in diesen Ländern an andere europäische Mächte verloren hatte. In vielen Sprachen findet man portugiesische Wörter in der modernen Lexik wieder, so zum Beispiel das Wort "pan" für „Brot“ im Japanischen (portugiesisch: "pão"), "sepatu" für „Schuh“ im Indonesischen (portugiesisch: "sapato"), "keju" für „Käse“ im Malaiischen (portugiesisch: "queijo") oder auch "meza" für „Tisch“ in Swahili (portugiesisch: "mesa").

Seit der Mitte des 16. Jahrhunderts fand eine große Anzahl von Lehnwörtern Eingang in die portugiesische Sprache, meist lateinischen oder griechischen Ursprungs. Italienische Wörter aus den Bereichen Musik, Theater, Malerei sowie spanische Lehnwörter, die aufgrund der Personalunion zwischen Portugal und Spanien von 1580 bis 1640 besonders zahlreich sind, machten die Sprache reicher und komplexer. Man unterscheidet aus diesem Grund zwei Entwicklungsphasen: das Altportugiesische (12. bis Mitte des 16. Jahrhunderts) und das Neuportugiesische, wobei als Ende des Altportugiesischen das Erscheinen des "Cancioneiro Geral" von Garcia de Resende im Jahre 1516 betrachtet wird.

Die Gegenden, wohin sich das Portugiesische vor der Entwicklung des Neuportugiesischen verbreitet hatte, machten diese Entwicklungen jedoch größtenteils nicht mit. In Brasilien und São Tomé und Príncipe, aber auch in einigen abgelegenen ländlichen Gebieten Portugals werden deshalb Dialekte gesprochen, die dem Altportugiesischen ähnlich sind.

Neben den gut 200 Millionen Brasilianern sprechen heute mehr als 10 Millionen Portugiesen und ebenso viele Bewohner der ehemaligen afrikanischen und asiatischen Kolonien Portugiesisch als Muttersprache. Portugiesisch entwickelte sich somit nach Spanisch zur zweithäufigsten romanischen Muttersprache. Diese Position verdankt Portugiesisch der Tatsache, dass sich die Bevölkerung Brasiliens innerhalb der letzten 100 Jahre mehr als verzehnfacht hat: 1900 hatte Brasilien eine Bevölkerung von nur 17 Millionen.

Portugiesisch hat als romanische Sprache Parallelen zu Spanisch, Katalanisch, Italienisch, Französisch, Rumänisch und den anderen romanischen Sprachen, vor allem was die Grammatik und Syntax angeht. Besonders der spanischen Sprache ist es in vielen Aspekten sehr ähnlich, in der Aussprache herrschen jedoch bedeutende Unterschiede. Mit etwas Übung ist es einem Portugiesen jedoch möglich, Spanisch zu verstehen. Wenn man den folgenden Satz betrachtet:

Fast alle Wörter der einen Sprache haben sehr ähnlich lautende Verwandte in der jeweils anderen Sprache, die jedoch unter Umständen sehr selten gebraucht werden.

Es gibt allerdings auch eine Anzahl von Wörtern, bei denen keine Verwandtschaft zwischen den Sprachen besteht und die jeweiligen Sprecher in dem anderen Land vor Probleme stellt. Beispiele:

Es gibt Orte, in denen Spanisch und Portugiesisch nebeneinander gesprochen werden. Muttersprachler des Portugiesischen können in der Regel Spanisch lesen und umgekehrt, auch wenn sie die gesprochene Sprache des jeweils anderen nicht verstehen. Andererseits wird der Versuch von Besuchern in Portugal oder Brasilien, mit den Einheimischen auf Spanisch zu kommunizieren, häufig nicht gern gesehen und lässt in den Augen der Einheimischen auf Ignoranz schließen.

Das Standardportugiesisch, auch als "Estremenho" bezeichnet, hat sich in der Geschichte häufiger geändert als andere Variationen. Alle Formen der portugiesischen Sprache Portugals können nach wie vor im brasilianischen Portugiesisch gefunden werden. Afrikanisches Portugiesisch, besonders die Aussprache von São Tomé und Príncipe (auch "Santomense" genannt), hat mit brasilianischem Portugiesisch viele Gemeinsamkeiten. Die Dialekte Südportugals haben ebenfalls ihre Eigenheiten bewahrt, wozu die besonders häufige Benutzung des Gerundiums zählt. Dagegen sind Alto-Minhoto und Transmontano in Nordportugal der galicischen Sprache sehr ähnlich.

Das Standard-Portugiesische aus Portugal ist in den früheren afrikanischen Kolonien die bevorzugte Aussprache. Deshalb kann man zwei Formen unterscheiden, nämlich die europäische und die brasilianische; wobei man gemeinhin vier große Standard-Aussprachen unterscheidet, nämlich jene von Coimbra, Lissabon, Rio de Janeiro und São Paulo, dies sind auch die einflussreichsten Ausspracheformen.

Die wichtigsten Ausspracheformen des Portugiesischen sind, jeweils mit Hörbeispiel als externem Link, die folgenden:

Einige Beispiele für Wörter, die in Portugal anders heißen als in Brasilien oder Angola, sind im Folgenden gegeben:
¹
²
³

Das Portugiesische hatte bis zum Inkrafttreten des "Acordo Ortográfico" 2009 zwei Varianten der Schriftsprachen (Port. "variedades"), die häufig als "padrões" (Standards) bezeichnet werden. Diese sind:

Die Unterschiede zwischen diesen Varianten betreffen das Vokabular, die Aussprache und die Syntax, besonders in der Umgangssprache, wohingegen in der Sprache der gehobenen Schichten diese Unterschiede geringer ausfallen.
Hierbei handelt es sich jedoch um Dialekte derselben Sprache, die Sprecher der beiden Varianten können die jeweils andere leicht verstehen.

Einige Unterschiede im Wortschatz sind in Wirklichkeit keine. In Brasilien ist der Standardausdruck für 'Teppich’ "tapete." In Portugal benutzt man eher "alcatifa." Jedoch gibt es in Portugal ebenfalls regional den Ausdruck "tapete," ebenso wie es in Brasilien regional den Ausdruck "alcatifa" gibt. Für alte Wörter trifft dies fast generell zu, während in neuen Wörtern diese Unterschiede in der Tat landesspezifisch sind wie etwa "ônibus" in Brasilien und "autocarro" in Portugal.

Signifikantere Unterschiede bestanden bis zum "Acordo Ortográfico" in der Orthografie. In Wörtern, die "cc," "cç" oder "ct" enthalten, wird in Brasilien das erste "c" weggelassen, in Wörtern, die "pc," "pç" oder "pt" enthalten, entfällt das "p". Diese Buchstaben werden nicht ausgesprochen, sondern stellen vielmehr Überbleibsel aus dem Latein dar, die man in Brasilien zumeist eliminiert hat. Man vergleiche mit dem Italienischen.

Ein paar Beispiele sind:
Daneben gibt es einige Unterschiede in der Akzentuierung, die folgende Gründe haben:

Eine Rechtschreibreform wurde mit dem "Acordo Ortográfico" im Jahr 1990 verabschiedet, um einen internationalen Standard für das Portugiesische zu erreichen. Allerdings dauerte es bis 2009, bis das Abkommen in Portugal und Brasilien in Kraft trat, nachdem es im Jahr 2008 von diesen beiden Ländern sowie von den Kapverden und São Tomé e Príncipe ratifiziert worden war (später folgte Guinea-Bissau; Angola und Mosambik haben es bis heute – Stand Oktober 2014 – nicht ratifiziert). Im Rahmen dieser Reform fielen in Portugal die oben genannten "c" in "cc," "cç" oder "ct" und "p" in "pc," "pç" oder "pt" weitgehend weg, daneben gab es kleinere Vereinheitlichungen und man versucht, sich auf ein koordiniertes Vorgehen in Bezug auf neue Lehnwörter aus anderen Sprachen zu einigen. Seitdem ist die Rechtschreibung in Brasilien und Portugal weitgehend identisch, Unterschiede bestehen nur noch für wenige Ausnahmen.

Als im Mittelalter Portugal sein Kolonialreich aufzubauen begann, kam die portugiesische Sprache in Kontakt mit den lokalen Sprachen der eroberten Gebiete und es entstanden Mischsprachen (Pidgins), die bis zum 18. Jahrhundert in Asien und Afrika als lingua franca verwendet wurden. Diese Pidgin-Sprachen erweiterten ihre Grammatik und Lexik im Laufe der Zeit und wurden zu Umgangssprachen von ethnisch gemischten Bevölkerungen. Sie existieren unter folgenden Namen in den folgenden Gebieten:

Kap Verde:

Äquatorialguinea:

Guinea-Bissau und Senegal:

Indien:

Macau:

Malaysia, Singapur:

Niederländische Antillen und Aruba:

São Tomé und Príncipe:

Sri Lanka:

Suriname:

Einige Hybriddialekte existieren dort, wo Spanisch und Portugiesisch aufeinander treffen:

Die portugiesische Sprache hat eine sehr komplexe phonetische Struktur, was sie für Sprachwissenschaftler besonders interessant macht. Die Sprache verfügt über 9 Vokale, 5 nasale Vokale, 10 Diphthonge, 5 nasale Diphthonge und 25 Konsonanten.

Im Portugiesischen befindet sich die Betonung (auch Wortakzent) der Wörter, die orthographisch mit den Vokalen "a", "e" bzw. "o" sowie einem "s" oder "m" enden, auf dem vorletzten Vokal, wohingegen sich die Betonung der Wörter, die orthographisch mit "i", "u" oder einem Konsonanten (meist "l," "r," "z") enden, auf der Endsilbe befindet. Eine von dieser Regel abweichende Betonung wird durch ein diakritisches Zeichen (Akut oder Zirkumflex) angezeigt. Durch Tilde gekennzeichnete Silben sind immer betont, es sei denn, eine andere Silbe trägt einen Akut oder einen Zirkumflex.

Beispiele:

Beispiele: Der kleine Prinz auf Portugiesisch

"Portugal"

"Brasilien"

Das Portugiesische ist, anders als das Französische und Spanische, eher eine akzentzählende Sprache.

Das Portugiesische Alphabet verwendet 23 Buchstaben des lateinischen Alphabets – A, B, C, D, E, F, G, H, I, J, L, M, N, O, P, Q, R, S, T, U, V, X, Z – und macht außerhalb von Namen keinen Gebrauch von K, W und Y. 2009 wurde mit dem "Acordo Ortográfico" eine Rechtschreibreform durchgeführt, die diese Buchstaben wieder ins Alphabet aufnimmt.

Zusätzlich werden folgende Buchstaben mit Diakritika verwendet:
Á, Â, Ã, À, Ç, É, Ê, Í, Ó, Ô, Õ, Ú, Ü

Die folgenden Aussprachehinweise gelten für sowohl europäisches als auch brasilianisches Portugiesisch; wenn es Unterschiede gibt, sind sie beim jeweiligen Laut angegeben.

¹

Die portugiesischen Nasalvokale werden nicht so vollständig nasal ausgesprochen wie im Französischen und in der Regel gibt es im Portugiesischen auch keinen Verschlusslaut am Ende des Nasals. In manchen Publikationen wird z. B. die Aussprache des Nasalvokals "ã" mit "ang" angegeben, was allerdings nicht richtig ist, weil es sich bei den nasalierten Vokalen um einen einzigen Nasenlaut handelt.

Genus.
In der Regel sind Substantive, die auf -o enden, männlich und Substantive, die auf -a enden, weiblich. Es gibt aber auch Ausnahmen ("o problema", "o motorista"). Das Geschlecht wird durch den vorangehenden Artikel angezeigt (männlich: "o"; weiblich: "a"). Daneben gibt es zahlreiche Substantive, die auf andere Vokale ("o estudante" – der Student; "o javali" – das Wildschwein; "o peru" – der Truthahn), auf Diphthonge ("a impressão" – der Eindruck; "o chapéu" – der Hut) oder auf Konsonanten enden ("o Brasil" – Brasilien; "a flor" – die Blume).

Numerus.
Der Plural wird im Allgemeinen durch das Anhängen von -s an das Substantiv gebildet (o amigo – os amigos; a mesa – as mesas; o estudante – os estudantes). Bei Substantiven, die auf einen Konsonanten enden, ergeben sich Änderungen. Bei auslautendem -r, -z und -n wird im Plural -es angehängt. Manche Substantive, die auf -s enden und auf der vorletzten oder drittletzten Silbe betont sind, sind im Plural unveränderlich (o ônibus – os ônibus; o lápis – os lápis). Ein auslautendes -m vor dem Plural-S wird zu einem -n (o homem – os homens). Auslautendes -l wird im Plural zu -i- vokalisiert (o animal – os animais; o papel – os papéis; o farol – os faróis). Eine Besonderheit sind die Pluralformen der Substantive auf -ão. Je nach der Etymologie des Wortes bilden diese Substantive unterschiedliche Pluralformen (o irmão – os irmãos; o alemão – os alemães; a informação – as informações).

Augmentativ / Diminutiv.
Durch das Anhängen von Suffixen kann die Bedeutung eines Substantivs verändert werden. So lässt sich durch das Augmentativsuffix -ão eine Vergrößerungsform bilden: o nariz – o narigão (die Riesennase). Ähnlich verhält es sich mit der Verkleinerungsform (Diminutiv) durch das Suffix -inho/-inha: o nariz – o narizinho (das Näschen).

Bestimmter Artikel.
Der bestimmte Artikel des Portugiesischen entstand aus dem lateinischen Demonstrativpronomen "ille" (jener). Im Altportugiesischen sind die Formen "el" (als Relikt heute erhalten in der Anredeform für den König „El-Rei“), "lo", "la", "los" und "las" zu finden. Durch die phonetische Entwicklung fiel das anlautende l- weg, so dass die heutigen Formen entstanden. In manchen regionalen Wendungen hat sich die altportugiesische Form dennoch erhalten (mais + o → mai-lo).

Der bestimmte Artikel dient im Portugiesischen dazu, Wörter zu bestimmen, wenn sie bereits bekannt sind oder eine neue Aussage über sie getroffen wird. Darüber hinaus kann der bestimmte Artikel Wörter substantivieren ("o saber" – das Wissen). Wie in manchen deutschen Substandardvarietäten wird im Portugiesischen vor dem Eigennamen praktisch immer der bestimmte Artikel verwendet: Eu sou "o" João. (Ich bin "der" Hans.) Auch vor Possessivpronomen ist der bestimmte Artikel häufig: O João é "o" meu amigo. (Hans ist mein Freund.)

Ähnlich wie im Italienischen wird auch der portugiesische Artikel nach bestimmten Präpositionen (a / de / em / por) mit diesen verbunden. Die Verbindung der Präposition "a" mit dem weiblichen Artikel "a" wird durch einen Gravis-Akzent angezeigt: "à". Dieses Phänomen heißt Krasis (port: crase).

Unbestimmter Artikel.
Der unbestimmte Artikel entstand aus dem lateinischen Zahlwort "unus, una, unum". Er dient dazu, bisher unbekannte Substantive einzuführen und zu präsentieren. Er individualisiert und definiert Wörter, die zuvor noch nicht näher bestimmt wurden.

Der unbestimmte Artikel wird mit der Präposition "em", in Portugal selten auch mit der Präposition "de" zu einer so genannten Artikelpräposition verbunden.

Das Adjektiv dient im Portugiesischen dazu, Substantive näher zu bestimmen. Es muss in Zahl und Geschlecht mit dem Substantiv übereinstimmen, auf das es sich bezieht.

In der Regel werden die Adjektive im Portugiesischen dem Substantiv nachgestellt (Como uma maçã vermelha – Ich esse einen roten Apfel.) Einige Adjektive werden dem Substantiv aber auch vorangestellt. Dazu gehören die Superlativformen (o melhor amigo: der beste Freund). Manche Adjektive erhalten durch die Voranstellung eine kleine Sinnveränderung (um homem grande: ein großer Mann; um grande homem: ein großartiger Mann). Eine Voranstellung ist auch aus stilistischen Gründen möglich. Die Bedeutung des Substantivs erhält dadurch eine subjektive Komponente.

Adjektive können gesteigert werden. In der Regel wird die erste Steigerungsstufe (Komparativ) durch Voranstellung des Steigerungswortes "mais" (mehr) oder "menos" (weniger) gebildet: O João é "mais" inteligente. A Carla é "menos" inteligente. Will man ausdrücken, im Vergleich womit etwas mehr oder weniger ist, verbindet man den Vergleich mit "do que" (als): O João é "mais" inteligente "do que" a Carla.

Die zweite Steigerungsform (Superlativ) existiert in zwei Formen. Einmal als relativer Superlativ, bei dem ausgedrückt wird, im Vergleich womit etwas das Maximum ist: O João é "o" aluno "mais" estudioso da escola (João ist der fleißigste Schüler der Schule, also im Vergleich zu allen anderen Schülern.) Beim absoluten Superlativ wird nicht angegeben, was die Vergleichsgröße ist. Diese Form wird durch das Anhängen des Suffixes "-íssimo/-íssima" oder durch den Elativ mit "muito" (sehr) ausgedrückt: O João é inteligent"íssimo" / "muito" inteligente (João ist sehr intelligent.)

Einige häufig vorkommende Adjektivformen bilden unregelmäßige Steigerungsformen: bom – melhor – o melhor / ótimo; ruim – pior – o pior / péssimo; grande – maior – o maior / máximo; pequeno – menor – o menor / mínimo.

Verben werden in drei Konjugationen eingeteilt, die man nach der Infinitivendung unterscheidet (entweder "-ar," "-er" oder "-ir"), wobei die meisten Verben zur "-ar"-Gruppe gehören. Diese Verben folgen dann den gleichen Konjugationsregeln. Ähnlich wie im Deutschen gibt es den Imperativ "(o imperativo)," den Indikativ ("o indicativo)" und den Konjunktiv ("subjuntivo" o "conjuntivo"), wobei die Regeln, wann der Konjunktiv zu verwenden ist, im Portugiesischen strenger sind und die Verwendung vom Gebrauch des deutschen Konjunktivs erheblich abweicht beziehungsweise weitgehend mit dem Gebrauch des spanischen "subjuntivo" übereinstimmt.

Eine weitere Besonderheit stellt der sogenannte persönliche Infinitiv "(infinitivo pessoal)" dar. Damit werden Infinitivformen bezeichnet, die eine Personalendung erhalten. Beispiel: Mostro-te para saber"es" disso. (Ich zeige es dir, damit du davon weißt. – wörtlich: für du wissen(-st) davon).

Im Portugiesischen gibt es, wie in den meisten indoeuropäischen Sprachen, zwei Formen der Anrede, eine der Nähe "(tu)" und eine der Ferne "(o senhor/a senhora)". Die ursprüngliche Form der zweiten Person Plural "(vós)" hat heute bis auf einige Dialekte in Nordportugal sowie die im Gottesdienst verwendete biblisch geprägte Sakralsprache kaum noch Bedeutung. Daneben hat sich in Brasilien aus der höflichen Anrede "vossa mercê" (Euer Gnaden) die Kurzform "você" entwickelt, die mit der dritten Person Singular des Verbes verwendet wird. Diese Anrede ist in den meisten Regionen Brasiliens die normale Umgangsform im Alltag. So gesehen siezen sich Brasilianer praktisch immer (Beispiel: "Você me dá seu livro?" – Gibst du mir dein Buch? – eigentlich: Geben Sie mir Ihr Buch?) Interessant dabei ist, dass die "você"-Form eine größere semantische Breite hat als die deutsche Du-Form. Das heißt, wo man auf Deutsch die Sie-Form verwenden würde, kann man in Brasilien durchaus auch die "você"-Form verwenden. In Portugal wird diese Form als "vertrautes Sie" verwendet, etwa unter gleichrangigen Arbeitskollegen oder unter älteren Nachbarn. "O senhor/a senhora" ist auf sehr formelle Situationen beschränkt. In beiden Sprachvarianten hat die Pluralform "vocês" das alte "vós" komplett ersetzt.

Allgemeine Erklärung der Menschenrechte, Artikel 1:

Da Portugiesisch eine romanische Sprache ist, entstammen die meisten portugiesischen Wörter der lateinischen Sprache. Man kann jedoch auch Spuren anderer Sprachen, mit denen das Portugiesische Kontakt hatte, beobachten:

Es gibt wenige Wörter aus der Zeit vor der römischen Herrschaft Hispaniens, die sich aus der Sprache der Urbevölkerung des heutigen Portugals (Lusitaner, Konii, Iberer) oder aus der Sprache von Siedlern (Phönizier, Karthager oder Kelten beziehungsweise Keltiberer) bis in das moderne Portugiesisch erhalten haben. Bei vielen dieser Wörter fehlt jedoch der genaue wissenschaftliche Nachweis ihrer Herkunft (Etymologie). Gerade bei den Keltismen könnte es sich um Wörter handeln, die auf dem Umweg über das Lateinische ins Portugiesische gelangt sind.

Iberismen:

Keltismen:

Da praktisch alle dieser Wörter keltischen Ursprungs auch in anderen romanischen Sprachen belegt sind, liegt es nahe, dass sie schon von den Römern aus dem Keltischen entlehnt worden und dann als lateinische Wörter ins Portugiesische gelangt sind.

Phönizismen:
Auch hier gilt: Diese Wörter sind auch in anderen romanischen Sprachen belegt. Es liegt daher nahe, dass sie schon von den Römern aus dem Phönizischen (oder einer anderen Sprache) entlehnt worden und dann als lateinische Wörter ins Portugiesische gelangt sind.

Portugiesisch ist ein Abkomme des Vulgärlateins, welches mit dem klassischen Latein zwar verwandt, jedoch nicht identisch ist. Die Transformation von lateinischen zu den heutigen portugiesischen Wörtern begann teils schon während des Römischen Reiches, bei anderen Wörtern begann dieser Prozess erst später. Die portugiesische Sprache wurde durch die lateinische immer wieder beeinflusst; so gelangten später auch Wörter aus der lateinischen Schriftsprache des Mittelalters und der frühen Neuzeit ins Portugiesische. Diese so genannten "Buchwörter" haben sich gegenüber der lateinischen Form wenig verändert, während die Wörter, die aus dem gesprochenen Latein entstanden sind "(Erbwörter)", stark verändert wurden.

Die Prozesse, durch die aus lateinischen Erbwörtern portugiesische Wörter wurden, sind im Einzelnen:


Etwa tausend Wörter des Portugiesischen sind arabische Lehnwörter, zum Beispiel:

Durch die Entdeckungen kam das Portugiesische in Kontakt mit lokalen afrikanischen, asiatischen und indianischen Sprachen, von denen die portugiesische Sprache viele Elemente aufgenommen und an andere europäische Sprachen weitergegeben hat. Besonders geografische Bezeichnungen in Afrika und Brasilien gehen auf die Sprachen der Einwohner dieser Regionen zurück.

Asiatische Sprachen:

Indianische Wörter:

Schwarzafrika:

In der frühen portugiesischen Literatur hatte die Poesie die höchste Bedeutung. Einer der berühmtesten Literaten Portugals ist Luís de Camões (* 1524; † 1580), der mit dem Epos "Die Lusiaden" eines der wichtigsten Werke geschaffen hat. Seine Bedeutung wird nicht zuletzt dadurch illustriert, dass die portugiesische Entsprechung zum deutschen Goethe-Institut "Instituto Camões" heißt und Portugals Nationalfeiertag („Dia de Portugal“ – Tag von Portugal) auf den 10. Juni, den Todestag des Nationaldichters gelegt wurde.

Andere wichtige Autoren sind der Romancier Eça de Queirós (1845–1900), der Dichter Fernando Pessoa (1888–1935), der brasilianische Dichter und Romancier Machado de Assis (1839–1908), der brasilianische Romancier Jorge Amado (1912–2001), und der Literaturnobelpreisträger José Saramago (1922–2010).
Die portugiesische Sprache wird reguliert durch





</doc>
<doc id="4020" url="https://de.wikipedia.org/wiki?curid=4020" title="Peter Sellers">
Peter Sellers

Peter Sellers, CBE (* 8. September 1925 als "Richard Henry Sellers" in Portsmouth; † 24. Juli 1980 in London), war ein britischer Komiker und Filmschauspieler. Der Spezialist für Auftritte in den unterschiedlichsten Rollen und Verkleidungen wurde spätestens als trotteliger Inspektor Clouseau in der Filmreihe "Der rosarote Panther" bekannt. Dessen charakteristische deutsche Synchronstimme wurde von Georg Thomalla (ausgenommen der erste Film, dort von Harald Juhnke) gesprochen.

"Richard Henry Sellers" kam 1925 als einziges Kind des Pianisten William Sellers und seiner jüdischen Ehefrau Agnes „Peg“ Marks zur Welt. Seine Mutter, die als Entertainerin mit den "Ray Sisters" auftrat, war eine Urenkelin des bekannten englischen Boxers Daniel Mendoza (1764–1836). Sellers begleitete seine Eltern in den ersten sechs Lebensjahren auf ihren Tourneen, ehe er die römisch-katholische Schule "St Aloysius College" in Highgate besuchte. Er beendete seine Schulausbildung bereits im Alter von 14 Jahren, um sich seinen Lebensunterhalt mit verschiedenen Aushilfsjobs hinter der Theaterbühne und später auch als Schlagzeuger einer Tanzkapelle zu verdienen. Gegen den Willen seiner Mutter folgte er der Einberufung durch die Royal Air Force zu Einsätzen im Zweiten Weltkrieg. Bis 1947 spielte er den Truppenunterhalter in verschiedenen Teilen Indiens und Südostasiens.

Zurück in der Heimat fiel dem BBC-Produzenten Roy Speer Sellers' Talent als Stimmenimitator auf. Speer verschaffte ihm einen Auftritt in einer Comedy-Show, daraufhin kam Sellers zu Engagements in verschiedenen Radiosendungen. Bekanntheit brachte ihm vor allem sein Mitwirken in der BBC-Radioproduktion The Goon Show ein, bei der er einmal pro Woche zusammen mit Spike Milligan und Harry Secombe live auftrat. Sellers übernahm in diesen Shows, die zwischen 1951 und 1960 vom "BBC Home Service" gesendet wurden, mindestens fünf verschiedene Sprechrollen. Die „Goons“ griffen mit surrealem Humor bizarre Figuren auf, die durch das Nachkriegs-Großbritannien reisten und dabei mit bürokratischem Nonsens, imperialen Illusionen oder im Niedergang begriffenen Figuren aus dem Establishment konfrontiert wurden.

Parallel zu seiner Arbeit im Radio trat Sellers ab Beginn der 1950er Jahre regelmäßig in britischen Spielfilmen auf. 1955 war er in einer Nebenrolle der preisgekrönten Kriminalgroteske "Ladykillers" an der Seite von Alec Guinness zu sehen. 1957/58 drehte er zusammen mit Richard Lester den komödiantischen Film "The Running Jumping & Standing Still Film," in dem sie auch die beiden Hauptrollen übernahmen: Sellers die des Fotografen und Lester die des Malers. Der handlungsarme, elfminütige Kurzfilm brachte Sellers 1959 einen Preis auf dem San Francisco International Film Festival sowie eine Oscar-Nominierung in der Kategorie Bester Kurzfilm ein. Bald darauf hatte Sellers mit John Boultings Spielfilm "Junger Mann aus gutem Hause" seinen internationalen Durchbruch als Filmschauspieler. In der zeitgenössischen Sozialkomödie erschien er als kommunistischer Arbeitnehmervertreter, was ihm 1960 den British Film Academy Award als bester britischer Darsteller einbrachte. Anknüpfen konnte Sellers an diesen Erfolg mit Stanley Kubricks groteskem Antikriegsfilm "", in dem er neben einem verrückten Wissenschaftler zwei weitere Figuren verkörperte. Der Film brachte ihm 1965 seine erste Nominierung für einen Oscar als bester Hauptdarsteller ein. Zu weltweiter Bekanntheit verhalf ihm dann die Rolle des unfähigen bis trotteligen "Inspektor Clouseau" in Blake Edwards’ Serie "Der rosarote Panther", die er in sechs Filmen zwischen 1963 und 1982 interpretierte. Die Rolle war ursprünglich Peter Ustinov zugedacht, der jedoch abgelehnt hatte. In Edwards fand Sellers seinen kongenialen Partner und Freund für eine Reihe weiterer Komödien.

Sellers gehörte zu den bestbezahlten Schauspielern seiner Zeit und wurde von der britischen Presse häufig als der größte englische Komiker seit Charlie Chaplin betitelt. Allerdings trat er in Ermangelung weiterer Angebote und nicht zuletzt aus finanziellem Interesse auch in Produktionen auf, die seinem Talent oder seinen Vorstellungen weniger entsprachen (z.B. in verschiedenen Rollen in Werbespots der Barclays Bank). Zu seiner populärsten Rolle als Inspektor Clouseau entwickelte Sellers daher eine regelrechte Hassliebe. Durch jeden weiteren Auftritt war er stärker auf diese komische Figur festgelegt, auf der anderen Seite konnte er sie dann aber auch als „Comeback“ nutzen.

Auf die Frage, wie es ihm denn möglich sei, so viele unterschiedliche Figuren darzustellen, gab der Schauspieler die ironische Antwort, er habe sich seine Identität operativ entfernen lassen. Unter diesem Blickwinkel sollte auch sein Auftritt bei der "Muppet Show" gesehen werden, bei dem der Gaststar normalerweise bei einem „privaten Moment“ mit einer der Muppets-Figuren in der Garderobe gezeigt wird; Sellers jedoch trat als eine Art durchgedrehter „Wikinger-Pirat-Landstreicher“ auf mit der Begründung, er selbst wäre doch wohl allzu langweilig. Diese Darbietung brachte ihm eine Nominierung für den US-amerikanischen Fernsehpreis Emmy ein. Einen weiteren Kritikererfolg verbuchte Sellers mit der Darstellung eines geistig zurückgebliebenen Gärtners in Hal Ashbys Spielfilm "Willkommen Mr. Chance" (1979), die ihm u.a. einen Golden Globe als bester Komödiendarsteller und eine weitere Oscar-Nominierung einbrachte. Der Titelheld, der das Anwesen seines Dienstherrn nie verlassen hat und die Welt nur von Fernsehsendungen kennt, kann über seine Freundschaft zu einem sterbenskranken Industriellen (gespielt von Melvyn Douglas) Einfluss auf die amerikanische und internationale Politik nehmen. Der zeitgenössischen Kritik der Neuen Zürcher Zeitung zufolge hätte "Willkommen Mr. Chance" den Grundstein „für einen Neubeginn jenseits der grellen Komik“ legen können, doch Sellers verstarb kurze Zeit später.

Er war viermal verheiratet; mit Anne Hayes (1951–1961), Britt Ekland (1964–1968), Miranda Quarry (1970–1974) und Lynne Frederick (1977–1980). Aus den Ehen gingen drei Kinder hervor: Michael Sellers (1954–2006), Sarah Sellers (* 1957) und Victoria Sellers (* 1965).

Peter Sellers starb am 24. Juli 1980 an den Folgen eines Herzinfarkts. In den letzten 15 Jahren seines Lebens hatte er unter einer Herzkrankheit gelitten, 1977 war ihm ein Schrittmacher eingesetzt worden. Er hinterließ ein Vermögen von umgerechnet ca. 10 Mio. Euro. Seine letzte Ruhestätte fand Peter Sellers auf dem Golders Green Crematorium and Mausoleum in London.

Der Film "The Life and Death of Peter Sellers" mit Geoffrey Rush in der Hauptrolle beleuchtet die Hintergründe seines Lebens und Wirkens.

Peter Sellers wurde 1966 zum Commander of the British Empire (CBE) ernannt. Er war Mitglied im Bund der Freimaurer, seine Loge "(Chelsea Lodge No. 3098)" ist in London ansässig.

Seine Wandlungsfähigkeit, sein stetes Konservieren von Würde und Haltung auch in komischen oder gar peinlichen Lagen mit seiner überspielenden oder eingefrorenen Mimik machte seine künstlerische „Handschrift“ aus.

Sellers konnte zwischen verschiedenen Akzenten hin und her wechseln, etwa zwischen Upper-Class English, amerikanischem Englisch, deutschem Akzent und anderen, was ihm beispielsweise in "Dr. Seltsam" erlaubte, gleich in drei Hauptrollen aufzutreten. Verschiedene Dialekte konnte auch Georg Thomalla in der deutschen Synchronisation umsetzen.

Roger Field, als Kind ein Nachbar von Peter Sellers, soll diesen 1975 gefragt haben, ob er lieber Drama oder Komödie spielen würde. Sellers soll eindeutig „Komödie!“ geantwortet haben.

Sellers hegte große Sympathien für die „Nonsens-Schule“ "(nonsense school)," jene Art von Komik, die er selbst in der "Goon Show" praktizierte und ihm die glücklichste Zeit seines Lebens beschert habe. Versuche, dies als Drehbuchautor und Regisseur auf den Film zu übertragen "(Das boshafte Spiel des Dr. Fu Man Chu," 1980), schlugen jedoch fehl. Sellers bekannte später, mit den "Goons" habe man versucht, ernsthafte Ideen oder überhaupt Ideen aufzugreifen und in eine „unlogische Folgerung“ zu setzen.




 für "Willkommen Mr. Chance"



</doc>
<doc id="4022" url="https://de.wikipedia.org/wiki?curid=4022" title="Primaten">
Primaten

Die Primaten (Primates) oder Herrentiere sind eine zu der Überordnung der Euarchontoglires gehörige Ordnung innerhalb der Unterklasse der Höheren Säugetiere. Ihre Erforschung ist Gegenstand der Primatologie. Der Ausdruck „Affen“ wird bisweilen für diese Ordnung verwendet, ist aber missverständlich, da Affen nur eine Untergruppe darstellen. Primaten werden in die beiden Unterordnungen der Feuchtnasenprimaten (Strepsirrhini) und Trockennasenprimaten (Haplorrhini) eingeteilt, wobei letztere auch die Menschenaffen (Hominidae) inklusive des Menschen ("Homo sapiens") mit einschließen. Die Bezeichnung stammt vom lateinischen "primus" (der Erste) und bezieht sich auf den Menschen als „Krone der Schöpfung“.

Mit Ausnahme des Menschen, der eine weltweite Verbreitung erreicht hat, sind die Verbreitungsgebiete anderer Primaten größtenteils auf die Tropen und Subtropen Amerikas, Afrikas und Asiens beschränkt. Auf dem amerikanischen Doppelkontinent reicht ihr heutiges Verbreitungsgebiet vom südlichen Mexiko bis ins nördliche Argentinien. Die Arten auf den Karibischen Inseln, die Antillenaffen (Xenotrichini), sind ausgestorben, heute gibt es dort nur vom Menschen eingeschleppte Tiere. In Afrika sind sie weit verbreitet, die größte Artendichte erreichen sie in den Regionen südlich der Sahara. Auf der Insel Madagaskar hat sich eine eigene Primatenfauna (ausschließlich Feuchtnasenprimaten) entwickelt, die Lemuren. In Asien umfassen die Verbreitungsgebiete der Primaten die Arabische Halbinsel (der dort lebende Mantelpavian wurde jedoch möglicherweise vom Menschen eingeschleppt), den indischen Subkontinent, die Volksrepublik China, Japan und Südostasien. Die östliche Grenze ihres Vorkommens bilden die Inseln Sulawesi und Timor. In Europa kommt frei lebend eine einzige Art vor, der Berberaffe in Gibraltar, doch ist auch diese Population wahrscheinlich vom Menschen eingeführt.

Nicht-menschliche Primaten fehlen in Nordamerika, dem größten Teil Europas, den nördlichen und zentralen Teilen Asiens, dem australisch-ozeanischen Raum sowie auf abgelegenen Inseln und in den Polarregionen.

Anders als andere Säugetiergruppen sind Primaten nicht im großen Ausmaß vom Menschen in anderen Regionen sesshaft gemacht worden, außer den bereits erwähnten Mantelpavianen auf der Arabischen Halbinsel und den Berberaffen in Gibraltar betrifft das nur kleine Gruppen, beispielsweise eine Population der Grünen Meerkatze, die von afrikanischen Sklaven auf die Karibikinsel Saint Kitts mitgebracht wurde, oder eine Gruppe Rhesusaffen in Florida.

Obwohl die Primaten eine relativ klar definierte Säugetierordnung sind, gibt es relativ wenig Merkmale, die bei allen Tieren dieser Ordnung und sonst bei keinem anderen Säugetier zu finden sind. Dennoch lassen sich laut dem Biologen Robert Martin neun Merkmale der Primatenordnung festhalten:


Die kleinste Primatenart ist der Berthe-Mausmaki mit weniger als 10 Zentimetern Kopfrumpflänge und maximal 38 g Gewicht. Am größten sind die bis zu 275 kg schweren Gorillas. Generell sind Feuchtnasenprimaten mit einem Durchschnittsgewicht um 500 g kleiner als die Trockennasenprimaten mit einem Durchschnittsgewicht von 5 kg. Dies gründet auch auf den unterschiedlichen Aktivitätszeiten (siehe unten). Einige Arten haben einen ausgeprägten Geschlechtsdimorphismus, wobei die Männchen mancher Arten doppelt so schwer wie die Weibchen sein können und sich auch in der Fellfarbe unterscheiden können (zum Beispiel beim Mantelpavian).

Der Körper der meisten Primaten ist mit Fell bedeckt, dessen Färbung von weiß über grau bis zu braun und schwarz variieren kann. Die Handflächen und Fußsohlen sind meistens unbehaart, bei manchen Arten auch das Gesicht oder der ganze Kopf (zum Beispiel Uakaris). Am wenigsten behaart ist der Mensch.

Die größten Augen aller Primaten haben die Koboldmakis. Bei den größtenteils nachtaktiven Feuchtnasenprimaten ist zusätzlich eine lichtreflektierende Schicht hinter der Netzhaut, das Tapetum lucidum vorhanden.

Namensgebender Unterschied der beiden Unterordnungen ist der Nasenspiegel (Rhinarium), der bei den Feuchtnasenprimaten feucht und drüsenreich ist und sich in einem gut entwickelten Geruchssinn widerspiegelt. Die Trockennasenprimaten hingegen besitzen einfache, trockene Nüstern und ihr Geruchssinn ist weit weniger gut entwickelt.

Die ältesten gefundenen fossilen Primaten besaßen eine Zahnformel von 2-1-4-3, das bedeutet pro Kieferhälfte zwei Schneidezähne, einen Eckzahn, vier Prämolaren und drei Molaren, insgesamt also 40 Zähne. Die maximale Zahnformel der rezenten Primaten lautet jedoch 2-1-3-3, die beispielsweise bei den Gewöhnlichen Makis und Kapuzinerartigen auftritt. Manche Gattungen haben ernährungsbedingt weitere Zähne eingebüßt, so besitzen die Wieselmakis keine Schneidezähne im Oberkiefer. Die wenigsten Zähne aller lebenden Arten hat mit 18 das Fingertier, das keine Eckzähne und nur mehr einen Schneidezahn pro Kieferhälfte besitzt. Die Altweltaffen, einschließlich des Menschen, haben die Zahnformel 2-1-2-3, also 32 Zähne.

Die Form insbesondere der Backenzähne gibt Aufschluss über die Ernährung. Vorwiegend fruchtfressende Arten haben abgerundete, insektenfressende Arten haben auffallend spitze Molaren. Bei Blätterfressern haben die Backenzähne scharfe Kanten, die zur Zerkleinerung der harten Blätter dienen.

Da die meisten Primatenarten Baumbewohner sind, sind ihre Gliedmaßen an die Lebensweise angepasst. Die Hinterbeine sind fast immer länger und stärker als die Vorderbeine (Ausnahmen sind die Gibbons und die nicht-menschlichen Menschenaffen) und tragen den größeren Anteil der Bewegung. Besonders ausgeprägt ist das bei den springenden Primaten und beim Menschen. Bei Arten, die sich hangelnd durch die Äste bewegen, ist der Daumen zurückgebildet (beispielsweise bei den Klammeraffen und Stummelaffen). Feuchtnasenprimaten haben an der zweiten Zehe eine Putz- oder Toilettenkralle, die der Fellpflege dient. Die Unterseite der Hände und Füße ist unbehaart und mit sensiblen Tastfeldern versehen.

Für viele baumbewohnende Säugetiere ist ein langer Schwanz ein wichtiges Gleichgewichts- und Balanceorgan, so auch bei den meisten Primaten. Jedoch kann der Schwanz rückgebildet sein oder ganz fehlen. Mit Ausnahme der Menschenartigen, die generell schwanzlos sind, ist die Schwanzlänge kein Verwandtschaftsmerkmal, da Stummelschwänze bei zahlreichen Arten unabhängig von der Entwicklung vorkommen. Sogar innerhalb einer Gattung, der Makaken, gibt es schwanzlose Arten (zum Beispiel der Berberaffe) und Arten, deren Schwanz länger als der Körper ist (zum Beispiel der Javaneraffe). Einen Greifschwanz haben nur einige Gattungen der Neuweltaffen ausgebildet (die Klammerschwanzaffen und die Brüllaffen). Dieser ist an der Unterseite unbehaart und mit sensiblen Nervenzellen ausgestattet.

Man vermutet, dass sich die Primaten aus baumbewohnenden Tieren entwickelt haben und noch heute sind viele Arten reine Baumbewohner, die kaum jemals auf den Boden kommen. Andere Arten sind zum Teil terrestrisch (auf dem Boden lebend), dazu zählen beispielsweise Paviane und Husarenaffen. Nur wenige Arten sind reine Bodenbewohner, darunter der Dschelada und der Mensch. Primaten finden sich in den verschiedensten Waldformen, darunter Regenwälder, Mangrovenwälder, aber auch Gebirgswälder bis über 3000 m Höhe. Obwohl man diesen Tieren generell nachsagt, wasserscheu zu sein, finden sich Arten, die gut und gerne schwimmen, darunter der Nasenaffe oder die Sumpfmeerkatze, die sogar kleine Schwimmhäute zwischen den Fingern entwickelt hat. Für einige hemerophile Arten (Kulturfolger) sind auch Städte und Dörfer Heimat geworden, zum Beispiel den Rhesusaffen und den Hanuman-Langur.

Vereinfacht gesagt sind Feuchtnasenprimaten meist nachtaktiv (Ausnahmen: Indri, Sifakas und Varis), während Trockennasenprimaten meist tagaktiv sind (Ausnahmen: Koboldmakis und Nachtaffen). Die unterschiedlichen Aktivitätszeiten haben sich auch im Körperbau niedergeschlagen, so sind in beiden Untergruppen nachtaktive Tiere durchschnittlich kleiner als tagaktive. Eine weitere Anpassung an die Nachtaktivität stellt der bessere Geruchssinn der Feuchtnasenprimaten dar. Vergleichbar mit anderen Säugetieren ist die Tatsache, dass Arten, die sich vorwiegend von Blättern ernähren, längere Ruhezeiten einlegen, um den niedrigen Nährwert ihrer Nahrung zu kompensieren.

Primaten verwenden unterschiedliche Arten der Fortbewegung, die sich in verschiedenen Anpassungen im Körperbau widerspiegeln und auch vom Lebensraum abhängig sind. Es lassen sich folgende Formen unterscheiden:

Primaten haben in den meisten Fällen ein komplexes Sozialverhalten entwickelt. Reine Einzelgänger sind selten, auch bei Arten, die vorwiegend einzeln leben (zum Beispiel der Orang-Utan), überlappen sich die Reviere von Männchen und Weibchen, und bei der Fortpflanzung werden Tiere aus solchen überlappenden Territorien bevorzugt. Andere Arten leben in langjährigen monogamen Beziehungen (zum Beispiel Indriartige oder Gibbons). Vielfach leben Primaten jedoch in Gruppen. Diese können entweder Harems- oder Einzelmännchengruppen sein, wo ein Männchen zahlreiche Weibchen um sich schart, oder gemischte Gruppen, in denen mehrere geschlechtsreife Männchen und Weibchen zusammenleben. In Gruppen etabliert sich meist eine Rangordnung, die durch Alter, Verwandtschaft, Kämpfe und andere Faktoren bestimmt ist. Vermutlich im Zusammenhang mit dem zunehmenden Gehirnvolumen ist die elterliche Fürsorge relativ hoch entwickelt.

Auch die Kommunikation und Interaktion spielt eine bedeutende Rolle. Etliche Arten haben eine Vielzahl von Lauten, die zur Markierung des Territoriums, zur Suche nach Gruppenmitgliedern, zur Drohung oder zur Warnung vor Fressfeinden dienen kann. Besonders bekannt sind die Urwaldkonzerte der Brüllaffen und die Duettgesänge der Gibbonpärchen. Der Mensch ist der einzige, der wirklich ein hochkomplexes Lautsystem (Sprache) benutzt. Auch Körperhaltungen und Grimassen können eine Kommunikationsform darstellen, eine weitere wichtige Form der Interaktion ist die gegenseitige Fellpflege. Bei den Feuchtnasenprimaten spielt der Geruchssinn eine bedeutendere Rolle, oft wird das Revier mit Duftdrüsen oder Urin markiert.

Unter den Primaten besteht eine erhebliche Variabilität in der Ernährungsweise. Folgende Verallgemeinerungen lassen sich dennoch treffen:


Vermutlich waren die Vorfahren der Primaten Insektenfresser, die Mehrzahl der Arten ist heute jedoch vorrangig Pflanzenfresser. Früchte stellen für viele Arten den Hauptbestandteil der Nahrung dar, ergänzt werden sie durch Blätter, Blüten, Knollen, Pilze, Samen, Nüsse, Baumsäfte und andere Pflanzenteile. Viele Arten sind jedoch Allesfresser, die neben pflanzlicher auch tierische Nahrung zu sich nehmen, insbesondere Insekten, Spinnen, Vogeleier und kleine Wirbeltiere. Zu den Gattungen, die gelegentlich Jagd auf größere Säugetiere (Hasen, kleine Primaten, junge Paarhufer) machen, gehören Paviane und Schimpansen.

Primaten gehören zu den wenigen Wirbeltieren, die das wichtige Vitamin C nicht selbst produzieren können. Sie müssen es deshalb mit der Nahrung aufnehmen.

Folivore Arten weisen besondere Anpassungen auf: so haben die Stummelaffen einen mehrkammerigen Magen, in welchem Mikroorganismen die Zellulose abbauen. Dieses Konzept ähnelt dem der Wiederkäuer oder mancher Känguruarten. Andere, wie die Brüllaffen oder die Gorillas, haben einen vergrößerten Dickdarm, der demselben Zweck dient.

Reine Fleischfresser sind selten unter den Primaten, dazu gehören beispielsweise die insektenfressenden Koboldmakis und Bärenmakis.

Da das Nahrungsangebot für Folivoren dazu tendiert, zeitlich und räumlich uniform und vorhersehbar zu sein, sind ihre Aktionsräume meist kleiner als die von Frugivoren und Insektivoren.

Generell zeichnen sich Primaten durch eine lange Trächtigkeitsdauer, eine lange Entwicklungszeit der Jungen und eine eher hohe Lebenserwartung aus. Die Jungtiere werden in der Regel von der Mutter umhergetragen und halten sich hierzu als aktive Traglinge in deren Fell fest. Die Strategie dieser Tiere liegt darin, viel Zeit in die Aufzucht der Jungtiere zu investieren, dafür ist die Fortpflanzungsrate gering. Die kürzeste Tragzeit haben Katzenmakis mit rund 60 Tagen, bei den meisten Arten liegt sie zwischen vier und sieben Monaten. Die längste Trächtigkeitsdauer haben der Mensch und die Gorillas mit rund neun Monaten.

Bei den meisten Arten überwiegen Einzelgeburten, und auch bei den Arten, die üblicherweise Mehrfachgeburten aufweisen (darunter Katzenmakis, Galagos und Krallenaffen) liegt die Wurfgröße selten über zwei oder drei Neugeborenen.

Die Primaten gehören innerhalb der Plazentatiere zu den Euarchontoglires, einer aufgrund molekulargenetischer Untersuchungen festgelegten Überordnung. Ihre nächsten Verwandten sind die Riesengleiter (Dermoptera). Die Spitzhörnchen (Scandentia), die früher manchmal den Primaten zugerechnet wurden, zeigen zwar im Schädelbau und im Verhalten Ähnlichkeiten, diese sind aber entweder generelle Merkmale der Säuger oder konvergente Entwicklungen, sodass sie heute in eine eigene Ordnung, Scandentia, gestellt werden. Das nachfolgende Diagramm gibt die vermuteten Entwicklungsverhältnisse innerhalb dieser Überordnung wieder:

Die Primaten umfassen mehr als 400 Arten, man teilt sie heute in zwei Unterordnungen, die Trockennasenprimaten (Haplorrhini) und die Feuchtnasenprimaten (Strepsirrhini). Die Feuchtnasenprimaten teilen sich in die Lemuren (Lemuriformes), die ausschließlich auf Madagaskar leben, und die Loriartigen (Lorisiformes), zu denen Loris und Galagos gehören. Bei den Trockennasenprimaten stehen die Koboldmakis den anderen Arten gegenüber, die als Affen (Anthropoidea oder Simiae) bezeichnet werden und sich wiederum in die Neuweltaffen und die Altweltaffen teilen. Früher wurden die Feuchtnasenprimaten und die Koboldmakis als Halbaffen (Prosimiae) zusammengefasst (teilweise inklusive der Riesengleiter und der Spitzhörnchen); diese wurden den „Echten“ Affen gegenübergestellt.

Das folgende Kladogramm zeigt nur die rezenten Familien:

Die ältesten zweifelsfrei den Primaten zuzuordnenden Fossil­funde stammen aus dem frühen Eozän (vor rund 55 Millionen Jahren). 
Diese Funde, wie diejenigen des Trockennasenprimaten "Teilhardina", dokumentieren jedoch bereits die Aufspaltung in die beiden Unterordnungen, daher liegt der Ursprung der Primaten vermutlich in der Oberkreide­zeit vor rund 80 bis 90 Millionen Jahren.

Es existieren einige Funde aus der Oberkreide und dem Paläozän wie "Purgatorius" oder die Plesiadapiformes, die manchmal als früheste bekannte Primaten bezeichnet werden. Ihre Stellung ist jedoch umstritten, viele Autoren sehen in ihnen eine gänzlich eigene Säugetierordnung.

Die Funde aus dem Eozän werden den Adapiformes und den Omomyidae, einer den Koboldmakis ähnlichen Familie zugeordnet und sind aus Afrika, Asien, Europa und Nordamerika bekannt. Während die Primaten in Nordamerika im Oligozän ausstarben, entwickelten sie sich auf den anderen Kontinenten weiter. Die heutigen Primaten Amerikas, die Neuweltaffen, sind seit rund 25 Millionen Jahren fossil belegt, älteste bekannte Gattung ist "Branisella". Aus dem Miozän sind Vorfahren der meisten heutigen Familien bekannt, eine Ausnahme bilden die Primaten Madagaskars, was aber wohl auf eine schlechte Fossilienfundrate zurückzuführen ist. In Europa starben die nichtmenschlichen Primaten – aus der Familie der Meerkatzenverwandten (Cercopithecidae) – im Pleistozän aus. In beispielloser Weise hat sich der Mensch (die Gattung "Homo)" innerhalb der letzten 100.000 Jahre über die gesamte Welt ausgebreitet, sodass heute – mit Ausnahme des antarktischen Kontinents, wo dauerhafte Wohnsiedlungen fehlen – überall auf der Erde Primaten zu finden sind.

Die folgenden Kapitel befassen sich mit dem Verhältnis zwischen Menschen und anderen Primaten, wobei der Mensch selbst weitestgehend unbeachtet bleibt.

Zu den frühesten im Mittelmeerraum bekannten Primaten zählten der Berberaffe Nordafrikas und der Mantelpavian Ägyptens. Der karthagische Seefahrer Hanno († 440 v. Chr.) brachte von seiner Afrikareise die Felle von drei „wilden Frauen“ mit, vermutlich Schimpansen. Aristoteles schreibt über Tiere, die sowohl Eigenschaften des Menschen als auch Eigenschaften der „Vierfüßer“ teilen und unterteilt sie in (Menschen-)Affen, „Affen mit Schwanz“ ( "kēboi", vermutlich Meerkatzen oder Makaken) und Paviane ( "kynokephaloi"). Den Pavianen attestierte er eine hundeähnliche Schnauze und Zähne und prägte so den Begriff der Hundsaffen. Im 2. Jahrhundert nach Christus sezierte Galenos von Pergamon Berberaffen und schlussfolgerte daraus die menschliche Anatomie; bis ins 16. Jahrhundert hinein waren seine Forschungen für die Medizin bestimmend. Die Vorstellungen von Primaten im Mittelalter waren überlagert von Fabelwesen wie behaarten, geschwänzten Menschen und Halbwesen ähnlich dem Satyr. "Pan", der Gattungsname der Schimpansen, abgeleitet vom bocksfüßigen Hirtengott Pan, geht auf solche Vorstellungen zurück. 1641 kam erstmals ein lebendiger Schimpanse nach Holland und wurde vom niederländischen Arzt Nicolaes Tulpius (1593–1674), der durch seine Verewigung in Rembrandts Gemälde "Die Anatomie des Dr. Tulp" berühmt wurde, untersucht und unter dem Titel „Indischer Satyr“ veröffentlicht. Als Begründer der Primatologie gilt der englische Arzt und Zoologe Edward Tyson (1650–1708), der 1699 eine Reihe von Gemeinsamkeiten zwischen dem von ihm untersuchten „Orang-Utan oder "Homo sylvestris"“ – in Wahrheit einem Schimpansen aus Angola – und dem Menschen feststellte. Carl von Linné schuf die grundsätzlich heute noch gültige Systematik der Tiere, er teilte in der zehnten Auflage seiner Systema Naturae (1758) die Primaten in vier Gattungen: "Homo" (Mensch), "Simia" (Menschenaffen und andere Affen), "Lemur" (Lemuren und andere „niedere“ Affen) und "Vespertilio" (Fledermäuse) – in früheren Auflagen hatte er auch noch die Faultiere zu den Primaten gerechnet.

Ganz mochte man sich mit der Einordnung der Menschen unter die Primaten nicht abfinden, so teilte Johann Friedrich Blumenbach diese Gruppe in die „Bimana“ (Zweihänder, also Menschen) und „Quadrumana“ (Vierhänder, also nicht-menschliche Primaten). Diese Einteilung spiegelt sich auch in der Tatsache wider, dass Menschenaffen in jener Zeit oft mit einem Stock dargestellt wurden, da das zweifüßige Gehen ohne Hilfe dem Menschen vorbehalten war. Im 19. Jahrhundert wurde die Evolutionstheorie entwickelt und Thomas Henry Huxley band mit seinem Werk "Evidence as to Man’s Place in Nature" (1863) den Menschen konsequent in die Evolutionsvorgänge ein, was noch jahrzehntelange Diskussionen anheizen sollte, ob der Mensch denn wirklich vom Affen abstamme. Der britische Zoologe St. George Mivart (1827–1900), ein konservativer Katholik und Autodidakt, versuchte einerseits, Darwins und Huxleys Thesen zu widerlegen, unter anderem mit der Behauptung, die Erde existiere für die beschriebenen Evolutionsprozesse noch nicht lang genug, andererseits aber modifizierte er die Einteilung Linnés, indem er die Fledermäuse von den Primaten abtrennte und die bis vor kurzem gültige Einteilung in Halbaffen und Affen durchführte. Mivart etablierte auch eine Merkmalsliste der Primaten, in der er unter anderem ausgebildete Schlüsselbeine, einen Greiffuß mit gegenüberstellbarer Großzehe und einen freihängenden Penis mit dahinterliegendem Skrotum anführte.

Ab dem 20. Jahrhundert spaltete sich die Forschungsgeschichte in zahlreiche Bereiche auf, die hier nur stichwortartig wiedergegeben werden können:


Generell lässt sich in den letzten Jahrzehnten ein Rückgang der Forschung mit anatomischen und physiologischen Fragestellungen und ein Aufschwung in Freilandforschung und Verhaltensbiologie erkennen.

Die Menschenähnlichkeit im Körperbau und mehrere Angewohnheiten haben oft zu mythischen Vorstellungen beigetragen. Zu diesen Angewohnheiten zählen das morgendliche Aalen in der Sonne, das als religiöse Sonnenverehrung gedeutet wurde, die Schreie und Gesänge und die vermutete eheliche Treue mancher Arten.

In verschiedenen Religionen wurden manche Arten zu heiligen Tieren erklärt. Der altägyptische Gott Thot wurde manchmal in Gestalt eines Pavians dargestellt. Im ägyptischen Totenbuch wird von den Pavianen berichtet, sie sitzen am Bug der Todesbarke und der Tote kann sich an sie wenden und beim Totengericht um Gerechtigkeit im Totenreich bitten. Paviane genossen deshalb Schutz und wurden sogar mumifiziert. In Indien gelten Rhesusaffen und Hanuman-Languren als heilig. Im Epos "Ramayana" helfen Affen, geführt von Hanuman, dem Prinzen Rama bei der Befreiung seiner Gattin aus den Fängen des Dämonenfürsten Ravana. Der affengestaltige Gott Hanuman gehört heute zu den populärsten Göttern des Hinduismus. In verschiedenen Regionen der Erde genossen gewisse Primaten aufgrund mythischer Vorstellungen Schutz vor der Bejagung, so zum Beispiel der Indri auf Madagaskar. In China wurden die Duettgesänge der Gibbons mit der angeblichen Melancholie dieser Tiere in Verbindung gebracht, was sich in Gedichten und Gemälden niedergeschlagen hat.

Bekannt ist das buddhistische Symbol der drei Affen, die nichts sehen, nichts hören und nichts sagen.

Die ältesten Belege über Primaten als Haustiere stammen aus dem Alten Ägypten, wo Bilder zeigen, wie Paviane an der Leine geführt wurden und mit Kindern spielten. Aus dem alten China sind Gibbons als Haustiere bekannt. Über Jahrtausende hinweg wurden Primaten als Haustiere gehalten, auch heute ist dies noch mancherorts üblich. Gehalten werden vor allem Menschenaffen und kleinere Arten wie Totenkopfaffen – bekannt war der Schimpanse Michael Jacksons. Problematisch ist dabei, dass diese Tiere selten gezüchtet, sondern meistens als Jungtiere gefangen werden, was oft mit der Tötung der Mutter einhergeht. Unter dem Aspekt des Tierschutzes werden Primaten als Haustiere generell abgelehnt, da eine artgerechte Haltung kaum möglich ist und es auch zur Übertragung von Krankheiten – in beide Richtungen – kommen kann.

Unter den Primaten finden sich keine klassischen Nutztiere. Im Bereich der medizinischen Forschung und der Erprobung von Kosmetika werden Primaten vielfach für Tierversuche verwendet. Am bekanntesten ist wohl der Rhesusfaktor, der 1940 am Rhesusaffen entdeckt wurde. Früher hat die Suche nach Versuchstieren die Populationen zum Teil drastisch dezimiert; heute stammen die Tiere für diese Zwecke meist aus eigener Züchtung. Der Sinn und Nutzen der Tierversuche ist heftig umstritten, und die Diskussion darüber wird äußerst kontrovers geführt.

Ein weiterer Verwendungszweck von Primaten war die Raumfahrt. Der erste war 1958 „Gordo“, ein Totenkopfaffe, der an Bord einer Redstone-Rakete ins All befördert wurde. Es folgten weitere Totenkopfaffen, Rhesusaffen und Schimpansen in den Raumfahrtprogrammen der USA, Frankreichs und der Sowjetunion.

In den USA gibt es Projekte, bei denen Kapuzineraffen als Hilfen für körperlich behinderte Menschen ausgebildet werden.

Das größte Artensterben in jüngerer Vergangenheit hat auf Madagaskar stattgefunden. Die Insel, die erst vor rund 1500 Jahren von Menschen besiedelt wurde, ist Heimat zahlreicher endemischer Tierarten, darunter fünf Primatenfamilien. Mindestens acht Gattungen und fünfzehn Arten sind seither dort ausgestorben, höchstwahrscheinlich aufgrund der Bejagung, möglicherweise gekoppelt mit klimatischen Veränderungen. Zu den dort ausgerotteten Primaten zählen vorrangig größere, bodenlebende Arten, darunter die Riesenlemuren "Megaladapis" und der gorillagroße "Archaeoindris" sowie die Palaeopropithecinae („Faultierlemuren“) und Archaeolemurinae („Pavianlemuren“).

Global betrachtet ist die Situation vieler Primatenarten besorgniserregend. Als vorrangig waldbewohnende Tiere sind sie den Gefahren, die mit den großflächigen Abholzungen der Wälder einhergehen, drastisch ausgeliefert. Die Verbreitungsgebiete vieler Arten machen nur mehr einen Bruchteil ihres historischen Vorkommens aus. Die Jagd tut ein Übriges: Gründe für die Bejagung sind unter anderem ihr Fleisch, das verzehrt wird, und ihr Fell. Hinzu kommt die Tatsache, dass sie Plantagen und Felder verwüsten, sowie die – weitgehend illegale – Suche nach Haustieren. Dabei werden meist die Mütter erlegt, um halbwüchsige Tiere einfangen zu können. Obwohl die International Union for Conservation of Nature keine Primatenart als in den letzten 200 Jahren ausgestorben listet, gilt eine Reihe als stark gefährdet. Zu den bedrohtesten Primaten zählen beispielsweise die Spinnenaffen und die Löwenäffchen Südamerikas, der auf Java endemische Silbergibbon, mehrere Stumpfnasenarten und der Sumatra-Orang-Utan.

Neuen IUCN-Zahlen zufolge sind 48 % der Primaten-Arten bedroht, darunter sind 11 % vom Aussterben bedroht ("critically endangered"), 22 % stark gefährdet ("endangered") und 15 % gefährdet ("vulnerable"). Besonders verheerend ist die Situation in Ost- und Südostasien.




</doc>
<doc id="4025" url="https://de.wikipedia.org/wiki?curid=4025" title="Photon">
Photon

Das Photon (von griechisch φῶς "phōs", Genitiv φωτός "phōtos" „Licht“) ist das Wechselwirkungsteilchen der elektromagnetischen Wechselwirkung. Anschaulich gesprochen sind Photonen das, woraus elektromagnetische Strahlung besteht. Daher wird gelegentlich auch die Bezeichnung Lichtquant oder Lichtteilchen verwendet. In der Quantenelektrodynamik gehört das Photon als Vermittler der elektromagnetischen Wechselwirkung zu den Eichbosonen.

Ein Photon hat keine Masse, aber eine Energie, die von seiner Frequenz abhängt: je höher die Frequenz, desto höher die Energie.

Seit der Antike gab es verschiedene, einander teilweise widersprechende Vorstellungen von der Natur des Lichts. Bis Anfang des 19. Jahrhunderts konkurrierten Wellen- und Teilchentheorien miteinander "(siehe Abschnitt Geschichte im Artikel Licht)". Dann schien die Wellennatur des Lichts durch viele Phänomene (z. B. Interferenz- und Polarisationserscheinungen) bewiesen und durch die 1867 aufgestellten Maxwellschen Gleichungen als elektromagnetische Welle verstanden. Daneben gab es auch Indizien für einen Teilchencharakter. Ein historisch wichtiges Experiment hierzu war im Jahre 1887 die Beobachtung des Photoelektrischen Effekts durch Heinrich Hertz und Wilhelm Hallwachs.

Die Entdeckung der Quantisierung der elektromagnetischen Strahlung ging im Jahr 1900 vom planckschen Strahlungsgesetz aus, das die Wärmestrahlung eines schwarzen Körpers beschreibt. Um dieses Gesetz theoretisch erklären zu können, musste Max Planck annehmen, dass die Oberfläche des schwarzen Körpers nur diskrete, zur Frequenz proportionale Energiemengen mit dem elektromagnetischen Feld austauschen kann. Planck selbst stellte sich allerdings nur den Energieaustausch quantisiert vor, noch nicht die elektromagnetische Strahlung an sich.

Albert Einstein stellte dann 1905 in seiner Publikation zum photoelektrischen Effekt die Lichtquantenhypothese auf. Ihr zufolge ist Licht ein Strom von „in Raumpunkten lokalisierten Energiequanten, welche sich bewegen, ohne sich zu teilen, und nur als Ganze absorbiert und erzeugt werden können“. Aufgrund verbreiteter Zweifel an diesen Ansichten wurden diese Arbeiten erst 1919 (Planck) und 1922 (Einstein) mit dem Nobelpreis ausgezeichnet.

Vielfach wurde der Teilchencharakter der elektromagnetischen Strahlung aber weiterhin bezweifelt, bis Arthur Holly Compton in den Jahren 1923–1925 nachweisen konnte, dass Röntgenstrahlung auf einzelne Elektronen genau so wirkt wie der Beschuss mit einzelnen Teilchen, deren Energien und Impulse gerade denen hochenergetischer Lichtquanten entsprechen. Für die Entdeckung und Interpretation des nach ihm benannten Compton-Effekts erhielt er 1927 (als einer von zwei Ausgezeichneten) den Nobelpreis für Physik.

Die formale Quantentheorie des Lichtes wurde seit 1925 beginnend mit Arbeiten von Max Born, Pascual Jordan und Werner Heisenberg entwickelt. Die heute gültige Theorie der elektromagnetischen Strahlung ist die Quantenelektrodynamik (QED); sie beschreibt auch die Lichtquanten. Sie geht in ihren Anfängen auf eine Arbeit von Paul Dirac im Jahre 1927 zurück, in der die Wechselwirkung von quantisierter elektromagnetischer Strahlung mit einem Atom analysiert wird.
Die QED wurde in den 1940er Jahren entwickelt und 1965 mit der Verleihung des Nobelpreises für Physik an Richard P. Feynman, Julian Schwinger und Shinichirō Tomonaga gewürdigt. In der QED ist das elektromagnetische Feld selbst quantisiert und das Photon seine elementare Anregung.

Albert Einstein schrieb 1951 in einem Brief an seinen Freund Michele Besso:

Das Wort "Photon" leitet sich vom griechischen Wort für Licht, "" ("phôs"), ab. Der Name war durch verschiedene Autoren schon seit 1916 für eine kleine Energiemenge, die einen photochemischen oder photoelektrischen Effekt auslösen kann, eingeführt worden, wurde aber kaum beachtet. Max Planck z. B. sprach in seiner Nobelpreisrede 1920 noch von „Lichtquanten“.
Endgültig wurde der Name durch Arthur Compton bekannt gemacht, der sich dabei auf eine Veröffentlichung des Chemikers Gilbert Newton Lewis im Jahre 1926 berief. Lewis verwandte den Begriff im Rahmen eines von ihm vorgeschlagenen Modells der Wechselwirkung von Atomen mit Licht. Dieses Modell sah unter anderem fälschlich eine Erhaltung der Photonenzahl vor und wurde allgemein nicht anerkannt.

Für das Photon wird im Allgemeinen das Symbol formula_1 (gamma) verwendet. In der Hochenergiephysik ist dieses Symbol allerdings reserviert für die hochenergetischen Photonen der Gammastrahlung (Gamma-Quanten), und die in diesem Zweig der Physik ebenfalls relevanten Röntgenphotonen erhalten häufig das Symbol "X" (von "X-Strahlen" und Englisch: "X-ray").

Sehr oft wird ein Photon auch durch die enthaltene Energie formula_2 dargestellt:
bzw.

Jegliche elektromagnetische Strahlung, von Radiowellen bis zur Gammastrahlung, ist in Photonen gequantelt. Das bedeutet, die kleinstmögliche Energiemenge an elektromagnetischer Strahlung bestimmter Frequenz ist ein Photon. Photonen haben eine unendliche natürliche Lebensdauer, können aber bei einer Vielzahl physikalischer Prozesse erzeugt oder vernichtet werden. Ein Photon besitzt keine Masse. Daraus folgt, dass es sich im Vakuum immer mit Lichtgeschwindigkeit formula_10 bewegt, sofern es in einem Zustand mit wohldefiniertem Impuls ist, also durch eine einzige ebene Welle darzustellen ist. Sonst bewegt es sich mit der Gruppengeschwindigkeit der beteiligten ebenen Wellen. Ein Photon im Überlagerungszustand von Impulsen mehrerer Richtungen bewegt sich auch im Vakuum langsamer als die Lichtgeschwindigkeit "(siehe Bessel-Strahl)". In optischen Medien mit einem Brechungsindex formula_11 ist die Gruppengeschwindigkeit aufgrund der Wechselwirkung der Photonen mit der Materie um den Faktor formula_12 verringert.

Photonen können auf vielerlei Arten erzeugt werden, insbesondere durch Übergänge („Quantensprünge“) von Elektronen zwischen verschiedenen Zuständen (z. B. verschiedenen Atom- oder Molekülorbitalen oder Energiebändern in einem Festkörper). Photonen können auch bei nuklearen Übergängen, Teilchen-Antiteilchen-Vernichtungsreaktionen (Annihilation) oder durch beliebige Fluktuationen in einem elektromagnetischen Feld erzeugt werden.

Zum Nachweis von Photonen können unter anderem Photomultiplier, Photoleiter oder Photodioden verwendet werden. CCDs, Vidicons, PSDs, Quadrantendioden oder Foto-Platten und Filme werden zur ortsauflösenden Detektion von Photonen benutzt. Im IR-Bereich werden auch Bolometer eingesetzt. Photonen im Gammastrahlen-Bereich können durch Geigerzähler einzeln nachgewiesen werden. Photomultiplier und Avalanche-Photodioden können auch zur Einzelphotonendetektion im optischen Bereich verwendet werden, wobei Photomultiplier im Allgemeinen die niedrigere Dunkelzählrate besitzen, Avalanche-Photodioden aber noch bei niedrigeren Photonenenergien bis in den IR-Bereich einsetzbar sind.

Das Photon ist ein Elementarteilchen mit der Masse formula_13. Neben experimentellen Messungen, die diese Tatsache sehr gut belegen (s. u.), ist dies theoretisch dadurch begründet, dass ein Masseterm des Photons die Eichinvarianz der Lagrangedichte im Rahmen der Quantenelektrodynamik verletzen würde. Im Gegensatz zu anderen Teilchen erfährt das Photon auch keine Massenänderung durch Wechselwirkung mit Vakuumfluktuationen. 

Ein physikalisches System mit einer Masse formula_14 erfährt jedoch wegen der Äquivalenz von Energie und Masse einen Massenzuwachs formula_15, wenn es ein Photon der Energie formula_2 aufnimmt.

Im Rahmen der Quantenelektrodynamik sind die Photonen die Übermittler der elektromagnetischen Wechselwirkung; die Bewegungsgleichung der Photonen muss demnach den klassischen Maxwell-Gleichungen
gehorchen (in diesem Abschnitt sei nur das Verhalten im Vakuum betrachtet). Die Lagrangedichte, die über den Lagrange-Formalismus zu den Maxwell-Gleichungen führt, lautet
ohne einen Masseterm des Photons der Gestalt formula_19. Ein solcher Term ist verboten, da er die Invarianz der Lagrangedichte unter den klassischen Eichtransformationen des elektromagnetischen Feldes verletzt. Auch in höheren Ordnungen quantenelektrodynamischer Störungstheorie bleibt die Masse des Photons durch die Eichsymmetrie geschützt.

Da das Higgs-Teilchen keine elektrische Ladung trägt, erhält das Photon darüber hinaus – im Gegensatz zu den anderen Eichbosonen der elektroschwachen Wechselwirkung – keine Masse durch den Higgs-Mechanismus.

Wenn die Masse des Photons verschieden von Null wäre, dann würde sie sich durch verschiedene Folgen bemerkbar machen. Keine von ihnen ist bisher beobachtet worden. Die Genauigkeit der Experimente erlaubt die Aussage, dass eine eventuelle Photonenmasse in jedem Fall unter formula_20 liegen muss, das ist der formula_21ste Teil der Masse des Wasserstoffatoms.

Falls Photonen Masse hätten,

Photonen werden auch vom Gravitationsfeld beeinflusst, was erst durch die Allgemeine Relativitätstheorie verständlich wird. Sie werden beim Vorbeiflug an einem schweren Körper von ihrer Bahn doppelt so stark abgelenkt, wie es nach der klassischen Physik für ein mit Lichtgeschwindigkeit bewegtes Teilchen zu erwarten wäre (siehe auch Gravitative Zeitdilatation und Tests der allgemeinen Relativitätstheorie). Diese Verdopplung rührt anschaulich gesprochen daher, dass in der Nähe der großen Masse die Zeit langsamer verstreicht und die Schwerkraft daher mehr Zeit hat einzuwirken. Nach der relativistischen Beschreibung der Gravitation folgen die Photonen, wie alle nicht von anderen Kräften beeinflussten Körpern auch, einer Geodäte der gekrümmten Raumzeit. Photonen gehören selbst zu den Quellen der Gravitation, indem sie mit ihrer Energiedichte die Krümmung der Raumzeit beeinflussen (siehe Energie-Impuls-Tensor in der allgemeinen Relativitätstheorie).

Photonen sind Spin-1-Teilchen und somit Bosonen. Es können also beliebig viele Photonen denselben quantenmechanischen Zustand besetzen, was zum Beispiel in einem Laser realisiert wird.

Während etwa der Elektronenspin parallel oder antiparallel zu einer "beliebig" vorgegebenen Richtung ist, kann der Photonenspin wegen fehlender Masse nur parallel oder antiparallel zur "Flugrichtung" orientiert sein. Zirkular polarisierte E-M-Wellen mit Energie formula_2 und Kreisfrequenz formula_30 haben nach den Maxwell-Gleichungen einen Drehimpuls der Größe formula_31, pro Photon mit formula_32 also genau den Drehimpulsbetrag von formula_33. Die Helizität der Photonen einer zirkular polarisierten Welle ist daher eine charakteristische Größe. Wird durch einen Spiegel die Ausbreitungsrichtung umgekehrt, oder wird die Rotationsrichtung umgekehrt, zum Beispiel durch eine λ/2-Platte, so wechselt die Helizität das Vorzeichen. "Linear" polarisierte elektromagnetische Wellen bestehen aus der Überlagerung von rechts und links polarisierten Photonen. Auch ein einzelnes Photon kann linear polarisiert werden, indem zwei entgegengesetzt zirkular polarisierte Zustände überlagert werden.

Photonen in einem Zustand mit wohldefiniertem Impuls bewegen sich mit Lichtgeschwindigkeit formula_34. Die Dispersionsrelation, d. h. die Abhängigkeit der Energie formula_35 von der Frequenz formula_36 (ny), ist linear, und die Proportionalitätskonstante ist das Plancksche Wirkungsquantum formula_37,
Der Impuls formula_39 eines Photons beträgt

Zahlenwerte, wie sie in optischen Spektren typischerweise auftreten, können wie folgt ermittelt werden:

Beispiel: Rotes Licht mit 620 nm Wellenlänge hat eine Photonenenergie von ungefähr 2 eV.

In einem optischen Medium wechselwirken Photonen mit dem Material. Durch Absorption kann ein Photon vernichtet werden. Dabei geht seine Energie in andere Energieformen über, beispielsweise in elementare Anregungen (Quasiteilchen) des Mediums wie Phononen oder Exzitonen. Möglich ist auch, dass das Photon sich durch ein Medium ausbreitet. Dabei wird es durch eine Abfolge von Streuprozessen behindert, in denen Teilchen des Mediums virtuell angeregt werden. Photon und Reaktion des Mediums zusammen kann man durch ein Quasiteilchen, das Polariton, beschreiben. Diese elementaren Anregungen in Materie haben üblicherweise keine lineare Dispersionsrelation. Ihre Ausbreitungsgeschwindigkeit ist niedriger als die Lichtgeschwindigkeit im Vakuum. In Experimenten der Quantenoptik konnte die Geschwindigkeit der Ausbreitung von Licht in einem verdünnten Gas von geeignet präparierten Atomen auf wenige Meter pro Sekunde gesenkt werden.

Photonen, die auf Materie treffen, können je nach Energiebereich unterschiedliche Prozesse auslösen. Im Folgenden sind für verschiedene Prozesse die Energiebereiche angegeben, in denen sie relevant sind:



"Zu Wechselwirkung von Photonen mit Photonen:"


</doc>
<doc id="4026" url="https://de.wikipedia.org/wiki?curid=4026" title="Pauli-Prinzip">
Pauli-Prinzip

Das Pauli-Prinzip (auch Pauli-Verbot oder Paulisches Ausschließungsprinzip) ist ein physikalisches Gesetz, das sich in der Quantenphysik auswirkt. Es wurde 1925 von Wolfgang Pauli zur quantentheoretischen Erklärung des Aufbaus der Atome formuliert und besagte, dass je zwei Elektronen in einem Atom nicht in allen Quantenzahlen übereinstimmen können. In der modernen Formulierung besagt das Pauli-Prinzip, dass die Wellenfunktion eines Quantensystems in Bezug auf Vertauschung von identischen Fermionen antisymmetrisch ist. Da auch die Quarks als Bausteine von Protonen und Neutronen zu den Fermionen zählen, gilt das Pauli-Prinzip für die gesamte Materie im allgemein verstandenen Sinne: Fermionen „schließen sich gegenseitig aus“, können also nicht am selben Ort existieren. Nur so lässt sich der differenzierte Aufbau der Materie mit Atomen und Molekülen verstehen. Das Pauli-Prinzip bestimmt demnach nicht nur den Aufbau des Atoms, sondern auch den größerer Strukturen. Eine Folge ist der Widerstand, den kondensierte Materie weiterer Kompression entgegensetzt. Es ist nicht zu verwechseln mit dem Pauli-Effekt.

Ausgangspunkt des Pauli-Prinzips ist die Tatsache, dass identische Teilchen in der Quantenmechanik "ununterscheidbar" sind: Der Verlauf eines Experiments oder allgemein die Entwicklung eines physikalischen Systems kann sich nicht ändern, wenn man darin zwei identische Teilchen vertauscht. Quantentheoretisch ergeben sich aber bei Vertauschung identischer Teilchen nur dann die gleichen Messwerte, wenn das Betragsquadrat der (Gesamt-)Wellenfunktion gleich bleibt, sich also allenfalls der Phasenanteil der Wellenfunktion ändert. Die experimentelle Erfahrung hat sogar die weitergehende Tatsache gezeigt, dass bei Vertauschung zweier identischer Teilchen je nach Teilchenart die Wellenfunktion entweder unverändert bleibt oder nur ihr Vorzeichen wechselt. Teilchen, bei denen sich das Vorzeichen ändert, nennt man Fermionen. Für sie ist also die Wellenfunktion antisymmetrisch bzgl. Teilchenvertauschung. Teilchen, bei denen die Wellenfunktion bei Vertauschung der Teilchen unverändert bleibt, nennt man Bosonen. Ihre Wellenfunktion ist symmetrisch bezüglich Teilchenvertauschung.

In seiner speziellen und zuerst beobachteten Form besagt das Pauli-Prinzip, dass in einem Atom keine zwei Elektronen in allen vier Quantenzahlen, die zu seiner Zustandsbeschreibung im Orbitalmodell notwendig sind, übereinstimmen. Wenn zwei Elektronen beispielsweise gleiche Haupt-, Neben- und magnetische Quantenzahlen haben, müssen sie sich in der vierten Quantenzahl, der Spin-Quantenzahl, unterscheiden. Da diese nur die Werte formula_1 und formula_2 annehmen kann, können sich in einem einzigen Atomorbital maximal zwei Elektronen aufhalten. Diese Tatsache bestimmt maßgeblich den Aufbau der chemischen Elemente (siehe Periodensystem).

Die Gesamtwellenfunktion formula_3 eines Systems von formula_4 identischen Fermionen muss total antisymmetrisch bezüglich jeder Vertauschung "P" zweier Teilchen sein:

Dabei ist formula_6 der Ort, formula_7 der Spin des formula_8-ten Fermions und formula_9 jeder Permutationsoperator, der die Vertauschung jeweils zweier Teilchen bewirkt, also z. B. für die Vertauschung des ersten Teilchens mit dem zweiten:

Betrachtet man ein System aus zwei nichtunterscheidbaren Fermionen, so gilt wegen der Antisymmetrie der Gesamtwellenfunktion
Für formula_12 ergibt sich daraus formula_13, d. h. formula_14. Somit muss auch das Betragsquadrat dieser Wellenfunktion, also die Wahrscheinlichkeitsdichte dafür, dass man bei einer Messung beide Fermionen am selben Ort formula_15 mit demselben Spin formula_16 findet, null sein.

In vielen Fällen (ein solcher Fall ist z. B. für nichtentartete Eigenfunktionen von Hamilton-Operatoren ohne Spin-Bahn-Kopplung stets gegeben) ist die Gesamtwellenfunktion formula_17 als Produkt von Ortswellenfunktion formula_18 und Spinwellenfunktion formula_19 darstellbar, also
Wegen der Antisymmetrie ist dann formula_21. Ist etwa die Spinwellenfunktion symmetrisch, also formula_22, so folgt daraus die Antisymmetrie der Ortswellenfunktion formula_18. Entsprechend gilt allgemein, dass die Symmetrie einer der Funktionen formula_18 oder formula_19 äquivalent zur Antisymmetrie der jeweils anderen ist.
Sind also die zwei Fermionen etwa im selben Spinzustand formula_26, dann ist formula_27 symmetrisch und daher folgt die Antisymmetrie der Ortswellenfunktion.

Diese Zusammenhänge gelten sinngemäß auch dann, wenn mehr als zwei nichtunterscheidbare Fermionen beteiligt sind.

In der Natur kommen Fermionen nur mit halbzahligem Spin und Bosonen nur mit ganzzahligem Spin vor, wie es das Spin-Statistik-Theorem beschreibt. Das Paulische Ausschließungsprinzip gilt also für alle Teilchen mit halbzahligem Spin und nur für diese. 

Für Bosonen gilt das Paulische Ausschließungsprinzip hingegen "nicht". Diese Teilchen genügen der Bose-Einstein-Statistik und können gleiche Quantenzustände einnehmen, im Extremfall bis hin zum Bose-Einstein-Kondensat.

Das verschiedene Permutationsverhalten von Fermionen und Bosonen passt zum verschiedenen Drehverhalten der jeweiligen Spinoren. In beiden Fällen ergibt sich ein Faktor von formula_28, mit dem (+)-Zeichen für Bosonen (formula_26 ganzzahlig) und dem (−)-Zeichen für Fermionen (formula_26 halbzahlig), entsprechend einer Drehung um 360°. Der Zusammenhang liegt unter anderem deshalb nahe, weil eine Vertauschung der Teilchen 1 und 2 einer komplementären Drehung der beiden Teilchen um 180° entspricht (zum Beispiel Teilchen 1 zum Ort 2 auf dem oberen Halbkreis, Teilchen 2 zum Ort 1 auf dem unteren Halbkreis).

Das Pauli-Prinzip führt zur Austauschwechselwirkung und erklärt die Spinordnung in Atomen (Hundsche Regeln) und Festkörpern (Magnetismus).

In der Astrophysik wird durch das Pauli-Prinzip erklärt, dass alte Sterne mit Ausnahme der Schwarzen Löcher – zum Beispiel Weiße Zwerge oder Neutronensterne – nicht unter ihrer eigenen Gravitation zusammenbrechen. Die Fermionen erzeugen einen Gegendruck, den Entartungsdruck, der einer weiteren Kontraktion entgegenwirkt. Dieser Gegendruck kann so stark sein, dass es zu einer Supernova kommt.

Bei Streuprozessen zweier identischer Teilchen ergeben sich für das Trajektorienpaar durch Vertauschung stets zwei verschiedene, aber von außen nicht unterscheidbare Möglichkeiten. Dies muss bei der theoretischen Berechnung von Wirkungsquerschnitt und Streuwellenfunktion berücksichtigt werden.



</doc>
<doc id="4027" url="https://de.wikipedia.org/wiki?curid=4027" title="Physiker">
Physiker

Ein Physiker ist ein Naturwissenschaftler, der sich mit Themen aus der Physik befasst.

Physiker sind in verschiedenen Berufsfeldern tätig: sie bearbeiten Aufgaben in der Grundlagen- und Industrieforschung, Entwicklung, Produktion, Beratung, Organisation und Verwaltung, im Marketing, im öffentlichen Dienst und in der Lehre an Schulen und Hochschulen. Dabei wenden sie Methoden der theoretischen, experimentellen und angewandten Physik an. Unverzichtbares Werkzeug ist die Mathematik.

Ein Teil der Physiker bleibt nach Studium und Promotion in der universitären Forschung und Lehre. In der Regel sind sie dann auf ein Spezialgebiet orientiert, wie zum Beispiel Kernphysik und Elementarteilchenphysik, Atom-, Hochenergiephysik und Molekularphysik, Clusterphysik, Festkörperphysik, Hydrodynamik, Aerodynamik, Thermodynamik, Optik und Laserphysik, Akustik, Elektrodynamik, Plasmaphysik und Tieftemperaturphysik, Astrophysik, Extraterrestrische Physik, Biophysik, Atmosphärenphysik, Ozeanographie.

Viele Physiker sind in Forschungs- und Entwicklungsabteilungen von Unternehmen fast aller Branchen tätig, zum Beispiel im Maschinen- oder Fahrzeugbau, der Rundfunk-, und Nachrichtentechnik, der Medizin-, Mess-, Steuer- und Regelungstechnik, der Energieerzeugung und -verteilung oder der Chemischen Industrie. Physiker, die auf Lehramt studiert haben, arbeiten in der Regel als Lehrer an Schulen. Etwa 25 % der rund 6.500 deutschen Patentanwälte haben ursprünglich – vor einer juristischen Zusatzausbildung – Physik studiert.

Laut einer im Februar 2010 veröffentlichten Studie der Deutschen Physikalischen Gesellschaft (DPG), durchgeführt vom Institut der Deutschen Wirtschaft Köln, ist die Situation für Physiker am Arbeitsmarkt schlechter als bei den übrigen Akademikern. Auf der Basis von Daten, die aus dem Mikrozensus des Statistischen Bundesamtes erhoben wurden, lässt sich die Erwerbslosenquote für alle diejenigen im erwerbsfähigen Alter bis 65 Jahre berechnen, die als Physiker ausgebildet wurden, d. h., die ein Physikstudium an einer Universität oder FH abgeschlossen haben (Ausbildungsberuf Physiker). Diese Quote lag im Jahr 2007 bei 4,5 Prozent und war damit etwas höher als die durchschnittliche Erwerbslosigkeit bei Akademikern (3,9 Prozent), allerdings geringer als die Erwerbslosenquote der Gesamtbevölkerung (8,6 Prozent). Die Werte lassen sich nur bedingt mit den Arbeitslosenzahlen der Bundesagentur für Arbeit vergleichen, die für 2007 eine Arbeitslosenquote für Physiker und Mathematiker von 6,6 % (für 2011 5,8 %) (bezogen auf die sozialversicherungspflichtig Beschäftigten) angibt, da die Arbeitssuchenden dort nicht nach dem Ausbildungsberuf, sondern nach dem gewünschten Zielberuf erfasst werden. Aus dieser Statistik fallen also alle Physiker heraus, die eine Stelle in einem anderen Beruf, z. B. im IT-Bereich, suchen. Aus der DPG-Arbeitsmarktstudie wird deutlich, dass die Bundesagentur für Arbeit nur ca. ¼ der Menschen mit einem Physikabschluss erfasst.

Der Unterschied in den Quoten wird zudem dadurch beeinflusst, dass verschiedene Grundgesamtheiten für die Ermittlung der Quote zu Grunde gelegt wurden. Z. B. besteht ein Unterschied zwischen Arbeitslosen und Erwerbslosen (siehe Wikipedia Artikel zur Arbeitslosenstatistik), außerdem betrachtet die Statistik der Bundesagentur für Arbeit Physiker, Statistiker und Mathematiker gemeinsam.

Ende 2014 waren in Deutschland etwa 14.800 Personen als Physikerinnen (2.600) oder Physiker (12.200) beschäftigt. Die Arbeitslosenquote unter Physikern, Mathematikern und Statistikern betrug 2013 2,6 Prozent. Im Vergleich zum Zeitraum 2008–2013 ist seit 2014 die Zahl der offenen Stellen stark gesunken, vor allem in Hochschulen und Forschungseinrichtungen. Bei den Berufsanfängern, deren Zahl bis ca. 2018 noch ansteigen wird, ist auch die Arbeitslosigkeit gestiegen. 2015 waren etwa 1.800 Physiker als arbeitslos gemeldet (2008: ca. 800).

Physiker arbeiten nicht selten in Berufsfeldern, die dem eigentlichen Physikstudium fremd sind (vgl. z. B. diese Liste bekannter Physiker in anderen Berufsfeldern). Viele Physiker arbeiten bei Finanzdienstleistern wie Banken oder Versicherungen. Viele Physiker, vor allem aus der Geophysik und der Meteorologie, sind in der Informatik mit der Programmierung von Modellen komplexer Systeme beschäftigt.

Physiker erlernen in ihrem Studium den Prozess der mathematischen Modellierung und seiner Überprüfbarkeit. Obwohl diese Fähigkeit im Studium primär auf physikalische Fragestellungen angewandt wird, finden sich Anwendungsgebiete auch in anderen Bereichen der Technik und Naturwissenschaften und darüber hinaus.

Der Abschluss als Diplom-Physiker setzt ein Studium an einer Universität voraus. Er gilt als berufsqualifizierender Abschluss. Gegenwärtig werden an vielen deutschen und Schweizer Universitäten die Diplom-Studiengänge durch die neuen internationalen Bachelor/Master-Studiengänge ersetzt.

Daneben gibt es Bachelor/Master-Studiengänge zum Physikingenieur und Lehramtsstudiengänge. Physik mit Abschlussziel Magister kann an fast allen Hochschulen jedoch nur als Nebenfach gewählt werden.

Die Regelstudienzeit beträgt für den Diplomstudiengang 10 Semester, jedoch war die tatsächliche Studiendauer im Jahr 2013 im Durchschnitt mit 12,1 Semestern höher. Die Regelstudienzeit eines Bachelorstudiengangs liegt bei 6 Semestern, ein Masterstudiengang dauert mindestens 4 Semester. Für beide Studiengänge wurde die Regelstudienzeit 2013 mit 6,6 Semestern bzw. 4,4 Semestern nur leicht überschritten.





</doc>
<doc id="4029" url="https://de.wikipedia.org/wiki?curid=4029" title="Peine">
Peine

Peine (niederdeutsch ""; frühneuhochdeutsch "Peina") ist eine Stadt und selbständige Gemeinde in Norddeutschland in Niedersachsen. Die um 1220 gegründete Stadt hat Einwohner (Stand ). Sie ist die Kreisstadt des gleichnamigen Landkreises. Bekannt ist die Stadt vor allem für ihre Stahlindustrie. Noch heute ist "Peiner" oder "Peiner Träger" in der Bauwirtschaft ein Synonym für breitflanschige Stahlträger.

Peine liegt an der Fuhse zwischen dem Harz und der Lüneburger Heide und zugleich zwischen den beiden größten Oberzentren Niedersachsens, der 40 Kilometer westlich gelegenen Landeshauptstadt Hannover und dem 25 Kilometer östlich gelegenen Braunschweig. Weitere größere Städte im näheren Umkreis sind Hildesheim, Salzgitter, Gifhorn, Wolfsburg und Celle.

Diese interessante Lage machte Peine seit seiner Gründung bei den umliegenden Herrschern begehrt, weshalb die Stadt in zahlreiche Auseinandersetzungen verwickelt war. Peine liegt in der Geestlandschaft zwischen Hannover und Braunschweig, der sogenannten Burgdorf-Peiner Geest.

Die Stadt hat eine Fläche von 119,51 km² und somit eine Bevölkerungsdichte von 417 Einwohner pro km². Ihre Höhe über dem Meeresspiegel beträgt etwa 70 Meter über Normalhöhennull.

Peine liegt innerhalb der gemäßigten Breiten im Übergangsbereich zwischen ozeanisch und kontinental geprägten Gebieten.

Zur Stadt Peine gehören 14 Ortschaften bzw. Ortsteile: Berkum, Dungelbeck, Duttenstedt, Eixe, Essinghausen, Handorf, Röhrse, Rosenthal, Schmedenstedt, Schwicheldt, Stederdorf, Vöhrum / Landwehr, Wendesse und Woltorf. In diesen Ortsteilen leben etwa 24.600 Einwohner.

In der Kernstadt selbst leben etwa 24.500 Einwohner. Sie besteht aus den folgenden neun statistischen Bezirken:

Eine Urkunde von 1130 erwähnt zum ersten Mal Berthold von Pagin, der ein Ministeriale des römisch-deutschen Königs Lothar III. (1075–1137) war. Vermutlich ließ er Burg Peine in dieser Zeit erbauen. Dementsprechend wurde der Name Peine wahrscheinlich von "Pagin" abgeleitet. Das genaue Gründungsjahr der Burg ist auf Grund fehlender Urkunden allerdings nicht mehr feststellbar, so dass deren Erbauung auch schon früher erfolgt sein könnte.
Für das Jahr 1202 berichtet das Chronicon Hildesheimense über eine Fehde zwischen dem Bischof Hartbert von Hildesheim und den Brüdern Ekbert und Gunzelin von Wolfenbüttel (* um 1170; † 2. Februar 1255), der letzte ein Dienstmann und Feldherr Kaiser Ottos IV. (* um 1175; † 19. Mai 1218). Gunzelin ging siegreich aus dieser Fehde hervor und erreichte die Belehnung mit Burg und Grafschaft Peine durch Bischof Hartbert.

Auf einer Landzunge südlich dieser Burg gründete er um 1220, vermutlich bereits im Jahr 1218, die eigentliche Stadt Peine. Seit 1223 hat Peine Stadtrechte. Unter anderem geht das Peiner Wappen auf dasjenige Gunzelins zurück. 1256 erobert Herzog Albrecht von Braunschweig-Lüneburg die Stadt. Die Söhne Gunzelins verloren nach dem Tod des Vaters das Lehen Peine bereits 1260 wieder an das Hochstift Hildesheim; Otto I. von Braunschweig-Lüneburg, Bischof von Hildesheim, gab Burg, Stadt und Grafschaft Peine dem Grafen Wedekind von Poppenburg zum Lehen. Dadurch gehörte Peine endgültig zum Einflussbereich des Bistums Hildesheim und wurde gleichzeitig Marktort.

Ab 1260 besaß Peine das Münzrecht und war mit Unterbrechungen bis 1428 eine Münzstätte Hildesheims. 1954 und 1956 wurden in Peine (in der Stederdorfer Straße und am Horstweg) zwei der größten mittelalterlichen Silberschätze Deutschlands gefunden. Es handelt sich um 95 flache, runde und zum Teil halbierte Silberbarren, einige von ihnen sind handtellergroß. Das Gesamtgewicht der beiden Funde beträgt 7,5 Kilogramm. Geprägt wurden die Silberstücke vermutlich im 14. Jahrhundert. Einige weisen die Wappen von Hildesheim und Hannover, den Braunschweiger Löwen und andere bisher noch nicht zugeordnete Prägungen auf. Der Fund ist ein Hinweis darauf, dass es der Stadt damals wirtschaftlich gut ging und die Peiner Kaufleute viel Einfluss hatten.

Im Jahre 1518 begann die Hildesheimer Stiftsfehde, die bis 1523 dauern sollte und unter der Peine besonders zu leiden hatte. Einige Jahre lang war es zwischen der Stadt und dem Hildesheimer Bischof bereits zu Streitigkeiten um zusätzliche Steuern und an den Stiftadel verpfändete Rechte gekommen. Im Januar 1518 verbündete sich Bischof Johannes IV. von Hildesheim, mit Herzog Heinrich I. von Lüneburg-Celle gegen Bischof Franz von Minden und Herzog Erich I. von Calenberg. Der offene Kampf begann 1519. Im Juni desselben Jahres kam es schließlich zur ersten Belagerung Peines. Nach dem ersten Angriff brannte der Südteil der Stadt ab, später stand die ganze Stadt in Flammen, doch die Burg konnte gehalten werden. Insgesamt wurde die Peiner Burg drei Mal jeweils für einige Monate belagert. Die zweite Belagerung erfolgte im Herbst 1521, die dritte im Sommer 1522. Die Burg Peine konnte zwar jedes Mal verteidigt werden – das so genannte „Eulennest“ wurde als uneinnehmbar bezeichnet –, doch überstand sie die Angriffe keineswegs unbeschadet. „Aus Gottes Gnad und Hülf allein, hat festgehalten das Haus Pein“.

Im Juli 1967 stieß man bei Arbeiten zur Verlegung einer Fernheizleitung am Marktplatz auf menschliche Schädel und Knochen, gefallene und verscharrte Braunschweiger Söldner, die in der Stiftsfehde 1521 die Burg vergeblich gestürmt hatten.

Der Friedensschluss durch den „Quedlinburger Rezess“ im Mai 1523 führte dazu, dass dem Hildesheimer Bischof nur noch das „kleine Stift“ blieb, zu dem auch Peine gehörte. Da Bischof Johann für die Kriegskosten aufkommen musste, ging Peine 1526 in den Pfandbesitz der Stadt Hildesheim über. Nach dem Tod Hans Wildefüers, des Hildesheimer Bürgermeisters und Anführers der katholischen Partei, entschied sich der Rat der Stadt Hildesheim am 27. August 1542, der lutherischen Lehre zu folgen. Da die Stadt Hildesheim immer noch Pfandinhaberin Peines war, wurde damit auch in Peine die Reformation durchgeführt. Im Jahre 1553 wurde Peine wieder an das Hildesheimer Hochstift zurückgegeben.

Am 18. März 1510 gab es einen Stadtbrand, bei dem ein großer Teil der Stadt vernichtet wurde. Es hieß: „de stat, de kerke unde de vörborch to Peine al ut in ver stunden“: Die Stadt, die Kirche und die Vorburg waren in vier Stunden ein Raub der Flammen geworden.

Am 15. Mai 1557 kam es zu einem weiteren verheerenden Stadtbrand, bei dem das Rathaus und auch die auf dem Marktplatz stehende Pfarrkirche St. Jacobi zerstört wurden. Alle Urkunden der Stadt wurden dabei vernichtet. Die Geschichte Peines vor 1600 lässt sich deshalb fast ausschließlich aus Hildesheimer und Braunschweiger Urkunden rekonstruieren.

1592 brannten noch einmal 66 Häuser nieder.

Der Dreißigjährige Krieg griff im Jahr 1623 erstmals auf das Gebiet des heutigen Niedersachsens über, was in Peine zur Vorbereitung auf Kampfhandlungen durch Stationierung einer Hildesheimer Kompanie führte. Die Wallanlagen und der Stadtgraben wurden ausgebessert. Ab 1625 fanden erste Kämpfe im Hildesheimer Stiftsgebiet statt, Peine wurde im August 1626 für kurze Zeit von den dänischen Truppen König Christians IV. besetzt. Graf von Tilly belagerte Peine und nahm es noch im August 1626 ein, bevor Tilly Christian bei Lutter am Barenberge schlug. Bis zum Sommer 1627 verlegte Tilly sein Hauptquartier nach Peine, was für die Stadt zusätzlichen Schutz bedeutete. Außerdem beließ er Peines protestantische Konfession.

1629 wurde das kaiserliche Restitutionsedikt erlassen, das die Gegenreformation durchsetzen sollte. In diesem Jahr kam auch Friedrich Spee von Langenfeld im kaiserlichen Auftrag; er blieb, bis ein Mordanschlag auf ihn verübt wurde. Die Peiner Bürger hatten die Wahl, den katholischen Glauben anzunehmen oder innerhalb von drei Monaten die Stadt zu verlassen. Zahlreiche Bürger verließen Peine. Ihren Grundbesitz hatten sie zuvor verkaufen müssen.
1632 kam es abwechselnd zu Besatzungen durch schwedische und katholische Truppen in Peine, woraufhin Herzog Friedrich Ulrich zu Braunschweig und Lüneburg um Hilfe gebeten und dadurch als Lehnsherr anerkannt wurde. Im Sommer 1633 wurde Peine erneut belagert und schließlich von Wolfenbütteler Truppen unter Verstärkung aus Goslar und Hildesheim am 28. Juli 1633 erobert. Wenige Tage später wurde Peine an Herzog Friedrich Ulrich übergeben. Die Gegenreformation war mit der Einnahme durch protestantische Truppen beendet, und die Glaubensflüchtlinge von 1628 kehrten zu einem Großteil zurück. 1637 ließ August der Jüngere, Herzog zu Braunschweig-Lüneburg erneut Befestigungsanlagen einrichten. Bis 1642 war das Große Stift Hildesheim unter Braunschweiger Kontrolle. 1642 wurde mit dem „Goslarer Akkord“ Frieden geschlossen, die Welfen gaben die Stadt Peine zurück an das Hochstift. Abgesehen von weiteren Einquartierungen in Peine und von Hildesheim geforderten Zahlungen war der Krieg für das nun bankrotte Peine beendet.

1756 kam es zwischen Frankreich und Großbritannien in den Kolonien zum Siebenjährigen Krieg. Durch die Bündnisse auf beiden Seiten wurde auch Peine in den Konflikt verwickelt. 1757 und 1758 wurde es von Franzosen besetzt. Bis 1763 musste Peine die Kriegsparteien – sowohl die Franzosen als auch Braunschweig – mit Geld und Arbeitskräften (unter anderem zum Festungsbau) unterstützen. Am Ende des Krieges war das Stift Hildesheim hoch verschuldet.

Nach der französischen Revolution 1789 verbündeten sich 1792 Preußen und Österreich gegen Frankreich. Daraufhin zogen 1792 und 1793 preußische Truppen durch Peine. 1802 kam es zu einer Besetzung des Hildesheimer Stiftsgebiets durch die Preußen, ein Jahr später wurde Peine eine preußische Stadt. Die kirchlichen Fürstentümer verloren wegen der Säkularisation im Zuge des Reichsdeputationshauptschlusses ihre Macht.

Das ehemals umkämpfte Burg Peine wurde wegen Verschlechterung der Bausubstanz baufällig. 1803 wurde der Abbau begonnen, 1816 war schließlich auch das letzte Gebäude abgerissen. Zudem wurde, da sich weitere Reparaturen am Rathaus nicht mehr lohnten, auch dieses Gebäude 1827 für 240 Reichstaler auf Abbruch verkauft und an gleicher Stelle am Marktplatz – teilweise mit den alten Steinen – ein neues Rathaus erbaut.

Infolge der Niederlage Preußens bei Jena und Auerstedt 1806 fiel Peine an das Königreich Westphalen unter Jérôme Bonaparte. Deshalb wurde auch die Verwaltung nach französischem Vorbild umgestaltet. Nach der Völkerschlacht bei Leipzig 1813 löste sich dieses Königreich jedoch wieder auf. Am 1. Mai 1815 wurde Peine, das damals ein Ackerbürgerstädtchen von 2.300 Einwohnern war, Teil des Königreiches Hannover. Nach dem Deutschen Krieg von 1866 wurde Hannover von Preußen annektiert. Damit war nun auch Peine Teil einer preußischen Provinz.

Die Stadt blühte auf mit der in der zweiten Hälfte des 19. Jahrhunderts einsetzenden Industrialisierung, ausgelöst durch die Gründung des Peiner Walzwerkes im Jahre 1872.

Ab der Mitte des 19. Jahrhunderts begann ein großer Wandel der bis dahin vorwiegend landwirtschaftlich geprägten Stadt Peine zum Industriestandort. Bereits 1844 wurde die Bahnlinie Hannover–Peine–Braunschweig fertiggestellt, denn man erhoffte sich durch den Bahnanschluss wirtschaftlichen Aufschwung. Nachdem 1855 ein Eisenerzlager in Groß Bülten bei Ilsede entdeckt wurde, kam die Gründung der Ilseder Hütte 1858, einer Eisenhütte, hinzu. Dieser Roheisenlieferant stellte eine wichtige Voraussetzung für die weitere Entwicklung Peines dar. In enger Zusammenarbeit wurde 1872 die Aktiengesellschaft "Peiner Walzwerk" gegründet. Das neu gebaute Walzwerk nahm bereits 1873 den Betrieb auf, hatte jedoch die ersten Jahre unter der beginnenden Wirtschaftskrise zu leiden.

Um die Zukunft der Eisenindustrie zu sichern, betrieb Gerhard Lucas Meyer 1880 den Zusammenschluss von Hütte und Walzwerk. Ziel war der Aufbau eines Thomas-Stahlwerks zur Veredelung des phosphorreichen Ilseder Eisens. Durch das Thomas-Verfahren veredeltes Ilseder Eisen wurde als Peiner Stahl überall konkurrenzfähig. Peine nahm infolgedessen einen rasenden Aufschwung, und bis 1891 wurden zwei weitere Walzwerke eröffnet. Durch das Wirtschaftswachstum und den damit verbundenen Bevölkerungszustrom wuchs und gedieh die Stadt.

Der wirtschaftliche Aufschwung des vorigen Jahrhunderts wurde durch den Ersten Weltkrieg unterbrochen. Peine war nicht direkt von Kampfhandlungen betroffen, aber die Deutsche Inflation 1914 bis 1923 und die Weltwirtschaftskrise machten der Wirtschaft der Stadt zu schaffen. Dennoch konnte ein weiterer Ausbau der Infrastruktur erfolgen. So wurde 1919 bis 1922 die Stromversorgung errichtet, und von 1921 bis 1929 erfolgte der Bau des Mittellandkanals für den Kohletransport der Wirtschaft. 1935 und 1936 wurde die jetzige Bundesautobahn 2 gebaut.
Im Laufe der „Reichspogromnacht“ wurde am 10. November 1938 die Synagoge von 1907 in der Bodenstedtstraße in Brand gesteckt und zerstört. Hans Marburger, Sohn eines jüdischen Kaufmanns, wurde in dem Gebäude niedergeschossen und verbrannte. Ihm zu Ehren wurde ein Teil der Straße später in Hans-Marburger-Straße umbenannt. Auf dem Platz der ehemaligen Synagoge befindet sich die zentrale Gedenkstätte, an der alljährlich am Jahrestag der „Reichspogromnacht“ Kränze niedergelegt werden.

Während des Zweiten Weltkriegs wurden das zur Rüstungsproduktion genutzte Walzwerk und die benachbarten Mineralölwerke mehrmals bombardiert, wobei es insgesamt etwa 50 Tote gab. Der Wiederaufbau dauerte noch bis 1951. Am 9. April 1945 wurde die Peiner Innenstadt mit leichten Bomben angegriffen, bei der es Tote und Gebäudeschäden gab. Tags darauf wurde die Stadt kampflos an amerikanische Truppen übergeben. Dadurch konnten größere Opfer und Zerstörungen vermieden werden.

Kurz nach dem Einmarsch der Alliierten in Peine rollte Mitte Mai 1945 ein beladener Munitionszug durch den Peiner Bahnhof. Ein Bahnarbeiter bemerkte im Bahnhofsbereich, dass Rauch aus einem Waggon stieg; er konnte den Lokführer warnen und koppelte mehrere Waggons ab, darunter auch den qualmenden Güterwagen. Der restliche Güterzug fuhr weiter in Richtung Hannover, der abgekoppelte Zugteil explodierte auf der Fuhsebrücke kurz vor der Horst. Die Explosion war so stark, dass in der Stadt Scheiben zu Bruch gingen, Dachziegel von den Dächern gedrückt wurden und dass Töpfers Mühle größtenteils zerstört wurde. Zu allem Unglück, die Dächer waren noch nicht wieder gedeckt, wurde die Stadt am nächsten Tag von einem Unwetter überrascht.
Die Alliierten verhafteten den Bahnarbeiter, weil sie Sabotage vermuteten. Er wurde aber bald wieder freigelassen. – Die alliierten Besatzungsbehörden ließen 1945 im Übrigen die 1934 in Berlin gegründete "Sammelstelle für Nachrichten über Führer von Kraftfahrzeugen", ein Vorgänger des Kraftfahrt-Bundesamtes, nach Peine verlagern, um diese schon im Jahr darauf weiter nach Bielefeld verlegen zu lassen. Später zog die Bielefelder "Sammelstelle für Nachrichten über Kraftfahrzeuge und Kraftfahrzeugführer" unter dem neuen Namen Kraftfahrt-Bundesamt letztlich nach Flensburg-Mürwik um, wo die Behörde sich noch heute befindet.

1946 wurde Niedersachsen als Teil der britischen Besatzungszone gebildet. Peine gehörte von da an zum Regierungsbezirk Hildesheim. Wegen der vielen Flüchtlinge verzeichnete die Stadt zwischen 1939 und 1950 einen Bevölkerungszuwachs von etwa 10.000 Menschen. Zur Zeit des Wirtschaftswunders wurden viele städtische Projekte und Straßenbauten verwirklicht. Durch die Gebietsreform 1974 wurden 14 zuvor selbstständige Dörfer und Gemeinden eingemeindet. Peine gehörte danach zum Regierungsbezirk Braunschweig, bis der Bezirk im Zuge einer landesweiten Verwaltungsreform am 31. Dezember 2004 aufgelöst wurde.
Die Stahlkrise 1975 hatte den Niedergang von etlichen Industriebetrieben in Peine zur Folge. Von 23.000 Arbeitsplätzen gingen mehr als 10.000 verloren. Die Ilseder Hochöfen wurden 1983 stillgelegt. Der Stadt Peine drohte der Verlust ihrer wirtschaftlichen Existenzgrundlage. Bis in die 1980er Jahre hinein wurden Arbeitsplätze abgebaut, und es fand ein Wandel der Industrie zu zukunftsorientierten Branchen (siehe Kapitel 5.2 Wirtschaft) statt. Die Stahlindustrie konnte sich wieder erholen, auch wenn heute nicht mehr so viele Arbeiter im Stahlwerk beschäftigt sind wie in der Blütezeit des Peiner Stahls. Die Salzgitter AG baute das Peiner Werk im November 1994 zu einem der modernsten Elektrostahlwerke Europas um (Peiner Träger GmbH). Durch den Bau eines zweiten Elektro-Ofens durch die Salzgitter AG im Verlaufe der Jahre 2008/2009 wurde dieser Anspruch unterstrichen.

Im Jahr 1995 war Peine Ausrichter des Kulturfestes Tag der Braunschweigischen Landschaft und im Jahr 2000 des Landesfestes Tag der Niedersachsen.

Am 1. September 2012 wurde vom Landesfrauenrat Niedersachsen in Peine der "frauenORT Hertha Peters" eröffnet. Hertha Peters war von 1964 bis 1972 Landrätin des Landkreises Peine, die erste Landrätin in Niedersachsen.

Vom 28. bis 30. August 2015 war Peine Gastgeber des 18. Europaschützenfestes, einer Veranstaltung der Europäischen Gemeinschaft Historischer Schützen.

Am 1. März 1974 wurden im Zuge der Gebietsreform Niedersachsens die Gemeinden Berkum, Dungelbeck, Duttenstedt (vorher im Landkreis Braunschweig), Eixe, Essinghausen (vorher im Landkreis Braunschweig), Landwehr (vorher im Landkreis Burgdorf), Röhrse (vorher im Landkreis Burgdorf), Rosenthal, Schmedenstedt, Schwicheldt, Stederdorf, Vöhrum, Wendesse und Woltorf eingegliedert. Handorf wurde bereits am 1. Juli 1968 eingemeindet.

In der Kernstadt Peines wohnen 25.088 Menschen, davon sind 12.795 (51 Prozent) weiblich und 12.293 (49 Prozent) männlich (Stand 31. Juli 2015). Darunter sind 15,24 Prozent Ausländer. Die Ausländer in Peine stammen überwiegend aus der Türkei, danach aus Polen. Die Quote der sozialversicherungspflichtig Beschäftigten beträgt rund 35,3 Prozent (Angabe von 2007) und ist in den letzten Jahren relativ konstant geblieben. Das Durchschnittsalter beträgt etwa 44 Jahre.

Rund 47 % der Bewohner sind evangelisch-lutherisch, 11,3 % römisch-katholisch. Neben den beiden großen Kirchen gibt es auch jeweils eine Gemeinde der Baptisten, der Siebenten-Tags-Adventisten, der Zeugen Jehovas und der Neuapostolischen Kirche.
Inzwischen stellen Menschen mit Migrationshintergrund rund 18 % der Peiner Bevölkerung. So sind auch Menschen muslimischen Glaubens in Peine zu Hause und finden sich beispielsweise in den Moscheen von DITIB und TAKVA zusammen.

Der Aufstieg der Wirtschaft in der zweiten Hälfte des 19. Jahrhunderts führte zu einem rasanten Bevölkerungswachstum von 3.823 Einwohnern im Jahre 1852 auf 15.421 Einwohner 1900. Durch Zuzug von Flüchtlingen nach dem Zweiten Weltkrieg hatte die Stadt 1954 bereits 28.918 Einwohner. Die Anzahl von 49.893 nach der Gebietsreform 1974 ist bis heute etwa gleich geblieben.

Es folgt eine Übersicht mit den Einwohnerzahlen nach dem jeweiligen Gebietsstand. Bis 1987 handelt es sich meist um Volkszählungsergebnisse (¹), danach um amtliche Fortschreibungen der jeweiligen Statistischen Ämter oder der Stadtverwaltung selbst. Die Angaben beziehen sich ab 1852 auf die „Ortsanwesende Bevölkerung“, ab 1925 auf die Wohnbevölkerung und seit 1987 auf die „Bevölkerung am Ort der Hauptwohnung“.
 Volkszählungsergebnis

Peine wird häufig als "Eulenstadt" oder auch als "Eulennest", mundartlich "Ulennest", bezeichnet. Es gibt mehrere Legenden und Sagen, die sich um die Eule ranken. Der Ursprung dieser Symbolik im Zusammenhang mit Peine als Stadt ist jedoch nicht eindeutig geklärt. Es ist möglich, dass die Burg Peine wegen der unwegsamen und gefährlichen Moore in der Umgebung „Ulennest“ genannt wurde und daraus die Sagen entstanden.

Die geläufigste ätiologische Herleitung des Namens ist die folgende: „Vor langer Zeit ließ sich eine Eule auf dem Wartturm der Vorburg nieder und fing an, unter dem Dache ein Nest zu bauen. Mit offenem Munde staunte die Besatzung das Tier an und geriet in Furcht angesichts des unbekannten und furchterregenden Wesens. Man sucht den ungebetenen Gast zu vertreiben, doch umsonst, er kommt immer wieder. Schließlich legt man Feuer in den Turm, die Eule jedoch fliegt zur Stadt hinüber. Die Peiner aber fürchten sich nicht, nehmen den Fremdling wohlgemut auf und verehren die Eule als einen Schutzgeist. (Kluge Leute behaupten freilich, auch die Peiner Bürger hätten den sonderbaren Ankömmling durch Feuer vertreiben wollen und dabei die ganze Stadt eingeäschert.)“

Eine andere Fassung der Erklärungslegende lautet so: Eine Eule hatte sich eine Scheune als Schlupfwinkel ausgesucht. Der Knecht bemerkte das Tier beim Strohholen und bekam eine große Angst. Ähnliches widerfuhr dem Bürger. Bald war die gesamte Nachbarschaft versammelt, doch selbst ein besonders mutiger Mann in Rüstung soll Angst bekommen haben. Man entschloss sich daraufhin, die Scheune niederzubrennen, was großen Spott über die Stadt brachte.

Tatsächlich war die Eule zu vielen Zeiten und verschiedenen Völkern als Unglücksbote bekannt oder zumindest vom Schauer des Numinosen umgeben. Der Charakter der Eulenlegende war demnach ein ironisch-bespöttelnder. So musste man sich – gemäß einem zeitgenössischen Spottvers – hüten, einem Peiner bei „Bier und Wein“ mit der allbekannten Eule zu kommen, da man sonst Prügel riskierte:

<poem style="margin-left:2em;">
„Ist einer keck, zieh er gen Pein,
Und geh daselbst zum Bier und Wein,
Frag sie, was ihn’ die Eul gethan,
Warumb sie die verbrennet han.
Und trink mit ihn’ den letzten aus,
Kommt er ungeschlagen wieder hrauß,
Wil ich ihm, was er drinn verzecht,
Duppelt bezahlen, wie es recht.“
</poem>

Als Peine im Verlauf der Hildesheimer Stiftsfehde mehrmaligen Eroberungsversuchen standhielt, schrieb man dem bisherigen Unglücksbringer nunmehr die Rettung der Stadt zu. So wurde die Eule nun zu einem Wahrzeichen der Stadt und Abzeichen ihres Bürgerstolzes. Auf eine positive Bedeutung der Eule deuten auch die aus dem Jahre 1534 erhaltenen Hochzeitsschüssel hin. Die nun veränderte Legende wollte es, dass die Eule die Peiner nicht erschreckt, sondern im Gegenteil in den Bedrängnissen ihrer Geschichte behütet habe. Charakteristischer Ausdruck dieser Umdeutung der Eule ist der mundartliche Zweizeiler „Peine was maket so feste,/ dat de Ule blev sitten in’n Neste!“ (hochdeutsch etwa „Peine machte so fest, dass die Eule im Nest sitzen blieb!“), der sich auch an einer Häuserwand nahe der Fußgängerzone findet. Er gehört in eine Legende aus der Zeit der Stiftsfehde, nach der die Angreifer sich schon mit Leitern an der Festung zu schaffen machten, als eine Eule, die in der Nähe ihr Nest hatte, mit ihrem Ruf Alarm schlug. Sie soll – wie die kapitolinischen Gänse nach der römischen Legende – die Wachen mit ihrem Lärm geweckt haben und somit einen Angriff der feindlichen Truppen vereitelt haben. Die Verteidiger konnten dadurch rechtzeitig Stellung beziehen und die nächtlichen Braunschweiger ‚Besucher‘ abwehren (aus dem Geschichtsunterricht an der Gunzelin-Realschule Peine und dem Gymnasium Groß Ilsede, Erzählung eines Lehrers).

Auch heute noch ist die Eule ein beliebtes Symbol in Peine, sei es bei Veranstaltungen, an Häusern, in Form von Statuen oder Ähnlichem. Besonders die Häuser um den Marktplatz herum, aber auch die Jakobi-Kirche sind mit Eulensymbolen verziert.

Peine ist Sitz eines Evangelisch-lutherischen Kirchenkreises, er gehört zum Sprengel Hildesheim-Göttingen der Landeskirche Hannovers. Zum Kirchenkreis gehört in der Kernstadt Peine die "St.-Jakobi-Kirche" in der Stadtmitte, in der heutigen Form Ende des 19. Jahrhunderts erbaut, sowie die "Friedenskirche" in der Gunzelinstraße und die "Martin-Luther-Kirche" in der Südstadt, beide 1955 errichtet. Ferner die "St.-Johannis-Kirche" aus der Nachkriegszeit auf Telgte und die "Horstkirche" (St. Nicolai) in der Horst, eine Fachwerkkapelle aus dem 18. Jahrhundert. Weitere evangelische Kirchen und Kapellen befinden sich in den Stadtteilen Berkum, Dungelbeck, Duttenstedt, Eixe, Essinghausen, Handorf, Röhrse, Rosenthal, Schmedenstedt, Schwicheldt, Stederdorf, Vöhrum und Woltorf.
Die katholische Kirche "Zu den heiligen Engeln" wurde 1867/68 am heutigen Von-Ketteler-Platz errichtet und 1923 durch die inzwischen profanierte "Herz-Jesu-Kapelle" in der Südstadt ergänzt. Weitere Kirchen wurden nach 1945 in den heutigen Stadtteilen Dungelbeck, Stederdorf und Vöhrum erbaut, ferner 1960 auf dem Telgter Friedhof an der Vöhrumer Straße die "St.-Barbara-Kirche". In Essinghausen befand sich eine Notkapelle. Sämtliche katholischen Kirchen in Peine gehören heute zur Pfarrgemeinde "Zu den heiligen Engeln" und zum Dekanat Braunschweig.

Zur Evangelisch-Freikirchlichen Gemeinde Peine (Baptisten) gehört die "Christuskirche" am Rosenhagen. Die Gemeinde gehört zum Bund Evangelisch-Freikirchlicher Gemeinden in Deutschland. Eine "Neuapostolische Kirche" befindet sich im Stadtteil Vöhrum. Ihre Gemeinde gehört zum Kirchenbezirk Braunschweig. Eine weitere Kirche befand sich in der Spittastraße 5. Am 7. Oktober 2012 fand dort der letzte Gottesdienst statt, 2013/14 wurde sie zu einem Konzertsaal umgebaut. Die "Adventgemeinde" Peine am Werderpark gehört zur Freikirche der Siebenten-Tags-Adventisten. Ein "Königreichssaal" der Zeugen Jehovas befindet sich in der Horst.

Neben den christlichen sind inzwischen auch islamische Religionsgemeinschaften in Peine ansässig. Die drei Moscheen in Peine befinden sich alle in der Südstadt.

Der Rat der Stadt Peine, das oberste politische Entscheidungsorgan, trifft Entscheidungen, welche die Selbstverwaltung der Stadt betreffen. Dazu gehören die Bestimmung von öffentlichen Abgaben, Bebauungspläne oder auch die Benennung von Ehrenbürgern. Darunter fällt, Regelungen für die Benutzung und Gebühren für öffentlichen Einrichtungen wie beispielsweise Bibliotheken, Straßenreinigung, Müllabfuhr, Wasserversorgung, Kanalisation und Fernwärme zu beschließen. Der Stadtrat besteht aus 40 Ratsfrauen und Ratsherren. Dies ist die festgelegte Anzahl für eine Stadt mit einer Einwohnerzahl zwischen 40.001 und 50.000 Einwohnern. Die 40 Ratsmitglieder werden durch eine Kommunalwahl für jeweils fünf Jahre gewählt. Die aktuelle Amtszeit begann am 1. November 2011 und endet am 31. Oktober 2016.

Stimmberechtigt im Rat der Stadt ist außerdem der hauptamtliche Bürgermeister Klaus Saemann (SPD).

Die letzte Kommunalwahl am 11. September 2016 führte zu folgendem Ergebnis (mit den entsprechenden Vergleichszahlen für 2011 und 2006):
Die Hauptaufgabe des Verwaltungsausschusses ist es, die Ratsbeschlüsse mit Hilfe von Empfehlungen der Fachausschüsse und Ortsräte vorzubereiten. Er fällt auch eigene Entscheidungen und kümmert sich um Einwohneranträge, Anregungen und Beschwerden der Bürger. Der Verwaltungsausschuss setzt sich aus dem Bürgermeister, den aus dem Rat bestimmten Beigeordneten und den Grundmandatsinhabern zusammen.

Der Bürgermeister, das dritte Organ der Stadt, wird bei der Kommunalwahl für eine Dauer von fünf Jahren gewählt. Er sorgt für die Vorbereitung und Ausführung der Beschlüsse von Stadtrat und Verwaltungsausschuss. Er repräsentiert die Stadt Peine und vertritt die Gemeinde in gerichtlichen Verfahren. 2001 wurde Udo Willenbücher mit 50,8 % zum ersten hauptamtlichen Bürgermeister der Stadt Peine gewählt. Er war bereits von 1996 bis 2001 in der Nachfolge von Dr. Boß Stadtdirektor. Er verzichtete 2006 aus gesundheitlichen Gründen auf eine Wiederwahl. Bei der Kommunalwahl am 11. September 2006 wurde Michael Kessler (SPD) im ersten Wahlgang mit 51,4 % zum hauptamtlichen Bürgermeister gewählt. Bei der gleichzeitig mit der Europawahl 2014 stattfindenden Bürgermeisterwahl am 25. Mai 2014 kandidierte Kessler erneut und setzte sich mit 73,9 % der Stimmen gegen Karl-Heinrich Belte von der Peiner Bürgergemeinschaft (PB) durch. Vorab gab Kessler jedoch bereits bekannt, das Amt im Falle des Wahlsieges nur noch zwei Jahre auszuführen. Bei der Bürgermeisterwahl am 11. September 2016 erhielt kein Kandidat die absolute Mehrheit, daher wurde am 25. September 2016 eine Stichwahl zwischen dem SPD-Kandidaten Klaus Saemann (43,0 %) und dem CDU-Kandidaten Andreas Meier (34,9 %) durchgeführt, in der Klaus Saemann 52,2 % der Stimmen erhielt.

Ehrenamtliche Bürgermeister von 1945 bis 2001
Hauptamtliche Bürgermeister seit 2001

Stadtdirektoren von 1948 bis 2001:

Blasonierung: „Gespalten von Gold und Rot, auf grünem Bogenschildfuß zwei nach außen gebogene, mit schwarzem Band gebundene goldene Garben, überhöht von einem springenden rotgezungten, schwarzen Wolf.“ 

Wappererklärung und -geschichte:

Das Wappen geht auf Gunzelin von Wolfenbüttel zurück. Seit dem 13. Jahrhundert haben sich an dem Wappen lediglich Kleinigkeiten geändert. So war der Boden ursprünglich silbern und die Farben der Garben wurden mehrfach gewechselt. Der Schild ist erst seit dem 17. Jahrhundert gespalten.
Die Farben Gold und Rot weisen auf die lange Zugehörigkeit zum Fürstbistum Hildesheim (von 1260 bis 1802) hin.

Es existierte auch eine andere Version des heute üblichen Stadtwappens aus dem 19. Jahrhundert. Das damalige Wappen war rot und grün gespalten. Oben mit einem schwarzen Wolf, der über zwei goldene Garben springt. Darüber beherrscht ein bläulicher Kolbenturnierhelm samt gelb-roter Helmzier die Szenerie. Das Peiner Wappen zeigt heute die Farbe Grün nur im unteren Teil – als Grund für die darauf gestellten goldenen Garben, über die ein schwarzer Wolf springt. Beim Wappen aus dem 19. Jahrhundert fehlt auch der ehemals über dem Helm hervortretende zweite Wolf. Erst seit 1924 gibt es ein festes Stadtwappen, das von dem Grafiker Emil-Werner Baule (1870–1953) gestaltet worden ist, wie man bei Recherchen im Stadt- und Kreisarchiv herausfand.

Die Stadtfarben von Peine sind bis heute Grün und Rot.

Peine unterhält mit folgenden Städten eine Städtepartnerschaft:

Nach längeren guten Beziehungen wurde am 13. Februar 2003 mit Asselheim, einem Stadtteil Grünstadts, ein Freundschaftsvertrag geschlossen.





Die Stadt Peine hat ein breit gefächertes Sportangebot. Dazu gehören zahlreiche Sport- und Turnhallen, Freisportplätze, Fitnesscenter und Schießstände. Außerdem gibt es Reithallen und Reitplätze, ein Hallenbad, ein Freibad, ein Kegelheim, und unweit von Peiner Stadtkern auf der Glindbruchkippe einen Segelflugplatz. Des Weiteren finden sich in Peine zwei Kanuvereine: Die Faltbootabteilung des „MTV Vater Jahn“ und die Kanu-Gemeinschaft Peine.
Im gesamten Landkreis Peine gibt es über 280 Kilometer befestigte Radwanderwege, auch der Peiner Herzberg lädt zum Radfahren oder Joggen ein. Die größte Sportveranstaltung in Peine ist jährlich der Peiner Triathlon, der fast 1000 Athleten an den Eixer See lockt.

Das größte Fest ist das jährliche Schützenfest, das „Peiner Freischießen“, das für fünf Tage um den ersten Sonntag im Juli stattfindet. Der Name rührt ursprünglich daher, dass sich die Peiner von den bürgerlichen Pflichten „freischießen“ konnten.

Der Ursprung des Festes ist wegen des bereits erwähnten Stadtbrandes von 1557 unbekannt. Urkundliche Erwähnung findet das Fest erst in den daraufhin angefertigten Statuten der Stadt Peine. Man geht heute davon aus, dass bereits kurz nach der Stadtgründung um 1220 eine Art „Bürgerwehr“ in der häufig umkämpften Stadt existierte. Diesen Willen zur Selbstverteidigung nutzten die Peiner Bürger im Laufe der Zeit um sich immer mehr Rechte von Seiten der Grundherren zu sichern (z. B. das "Freischießen" von den Steuern oder ein von den Bürgern gewählter Bürgermeister).

Seit dem Ende des 19. Jahrhunderts ist das Peiner Freischießen nicht mehr eine Veranstaltung eines einzelnen „Orts-Schützenvereins“, sondern es treten sieben „Korporationen“ an. Jede Korporation ermittelt ihren eigenen König. Unter den sieben Korporationen gibt es drei sogenannte „Bürgerkorporationen“, nämlich das "Neue Bürger Corps (1927)", das "Bürger Jäger Corps (1871)" und die "Schützengilde (1597)". Diese drei Bürgerkorporationen schießen untereinander den Bürgerkönig aus, der Bürgerkönig kommt also aus nur einer der drei Bürgerkorporationen. Der Bürgerkönig nimmt im Jahr seiner Regentschaft eine besondere Stellung im gesellschaftlichen Leben der Stadt ein.

Große Tradition hat das "Corps der Bürgersöhne (1814)", in dem die sogenannten Junggesellen das Freischießen feiern. Das Corps der Bürgersöhne schießt seinen eigenen aktiven „König“ aus. Die sogenannte passive Abteilung, bestehend aus nicht mehr ledigen Mitgliedern, bestimmt unter sich den passiven „König“.

Die Gründung von sieben Korporationen hat ihre Wurzeln in der Zeit der industriellen Revolution, die großen Einfluss auf Peine hatte. Die vielen Neubürger wollten auch aktiv am Freischießen teilnehmen, jedoch war ihnen der Eintritt in die Schützengilde versperrt, die damals aus alteingesessenen Peiner Kaufleuten und Handwerkern bestand. Neben den bereits erwähnten Korporationen nehmen der "MTV Vater Jahn Peine von 1862 Corporation", der "TSV Bildung von 1863" sowie der "Peiner Walzwerker Verein von 1878" an dem größten Heimatfest teil.

Seit 1966 wird das Fest mit einem Großfeuerwerk eröffnet, heute findet es auf dem 1979 gebauten Schützenplatz statt. Nach wie vor sind die offiziellen Termine und Freischießenämter eine Domäne der Männer.

Ein weiteres jährliches Großereignis ist das Peiner Stadtfest, der "Eulenmarkt". Weitere Ereignisse sind der "Autofrühling", bei dem die Autohäuser ihre neuen Modelle präsentieren und das "Highland Gathering", bei dem sich internationale Pipe- and Drumbands bei den offiziellen offenen deutschen Meisterschaften einen musikalischen Wettkampf liefern und anschließend die Highland Games veranstaltet werden.

Peine war bis zur Stahlkrise in den 1970er Jahren sehr weitgehend durch die Stahlindustrie geprägt (siehe auch Aufstieg der Wirtschaft). Nach dem Zusammenbruch der Stahlindustrie sah man sich wie in anderen Industriezentren auch gezwungen, die Monostruktur und die damit verbundene Abhängigkeit abzulösen.

Im Mittelzentrum Peine wurde eine Vielzahl zukunftsträchtiger Betriebe erfolgreich angesiedelt. Unter anderem hatte der Hersteller von Unterhaltungselektronik Matsushita eine Produktions- und Entwicklungsniederlassung in Peine. Das Gelände wurde inzwischen vom Berliner Schokoladen-Produzenten Rausch übernommen, der schon seit 1982 eine Fabrik im Norden von Peine unterhält. Die Firma Pelikan AG errichtete 1973 ein Werk im Peiner Ortsteil Vöhrum. Dieses Werk ist bis heute der bedeutendste Produktionsstandort der Pelikan-Gruppe weltweit.

Beliebt bei vielen Peinern ist das Bier der Privatbrauerei Härke, die seit der Übernahme der Rauls’schen Brauerei im Jahre 1890 auf eine lange Tradition zurückblicken kann. 2013 verlor das Unternehmen seine Eigenständigkeit. Es wurde von der Einbecker Brauhaus AG übernommen und firmiert seitdem als "Härke Braumanufaktur".

Auch die Firma Funkwerk Enterprise Communications GmbH (früher Elmeg Communication Systems GmbH) ist in Peine ansässig. 2011 eröffnete die Fa. NOWEDA (Arzneimittelgroßhandel) einen neuen Standort in Peine. Im Gewerbegebiet Peine-Ost konnte die Logistikfirma Meyer & Meyer mit Sitz in Osnabrück angesiedelt werden.

Neben den außerhalb gelegenen Industrie- und Gewerbegebieten mit günstiger Anbindung konnte die 1971 errichtete Fußgängerzone in der Breiten Straße mit ihrem Einzelhandel aufrechterhalten werden. Die Stadt ist Unternehmenssitz der Kreissparkasse Peine und der Volksbank Peine. Die Stadt Peine ist schuldenfrei, was angesichts überbordender kommunaler Verschuldung eine Besonderheit darstellt.

Zusammengefasst sind Kunststoff- und Metallverarbeitung, Daten- und Kommunikationselektronik, Lebensmittel- und Schokoladenhersteller, sowie öffentliche und private Dienstleistungsunternehmen in Peine vertreten. Die Stadt Peine unterhält selbst zwei Unternehmensparks, in denen verschiedene Firmen mit Produktion und Verwaltung ansässig sind. Unter anderem ist die Aula des Gymnasiums am Silberkamp im Unternehmenspark II untergebracht.

Der erste Nachweis einer Peiner Schule – vermutlich eine Lateinschule – geht auf das Jahr 1423 zurück. In einer Urkunde wurde der damalige, aus Hildesheim stammende Schuldirektor erwähnt. In den 1960er Jahren wurden zahlreiche Schulen ausgebaut und einige weitere neu errichtet.

Peine hat heute mehrere Grund- und Hauptschulen wie die "Bodenstedt-/Wilhelmschule" (Haupt-/Realschule) und die "Burgschule" (verlässliche Grund-/Hauptschule), die "Gunzelin-Realschule" und eine berufsbildende Schule. Dazu kommen mit dem "Ratsgymnasium", dem "Gymnasium am Silberkamp" und dem "beruflichen Gymnasium (ausschließlich Oberstufe)" drei Gymnasien. Seit 2001 ist im Stadtteil Vöhrum eine Integrierte Gesamtschule ansässig, welche ebenfalls eine gymnasiale Oberstufe bekommen hat und somit die vierte Möglichkeit bietet, im Stadtgebiet die Fach- und Allgemeinhochschulreife zu erlangen.

Die drei Schulen der Orientierungsstufe wurden im Sommer 2004 aufgelöst und in die weiterführenden Schulen eingegliedert. Zwar besitzt Peine keine eigene Hochschule, profitiert dafür aber von der Nachbarschaft zu den Forschungszentren in Braunschweig und Hannover.

Darüber hinaus hat Peine für die Erwachsenen- und Weiterbildung eine Kreisvolkshochschule.

Zum Ratsgymnasium gehört zudem eine Sternwarte mit elektrisch betriebener, 360°-drehbarer Baader-Sternwartenkuppel. Im Herbst 2014 stiftete die Erich-Mundstock-Stiftung (Kraftverkehr Mundstock) ein neues "Maede-12"-Teleskop.

Peine liegt direkt an der Bundesautobahn 2 (A 2), die das Ruhrgebiet mit der deutschen Hauptstadt Berlin verbindet, und verfügt über eine sogenannte Doppelanschluss-Stelle: Neben der ursprünglichen Autobahnausfahrt „Peine“ wurde im Jahre 1997 eine zusätzliche Ausfahrt „Peine-Ost“ fertiggestellt. Beide werden mit der Nr. 52 geführt. Daneben führen die Bundesstraßen B 65, B 444 und B 494 durch die Stadt. Die beiden wichtigsten straßenbaulichen Bauwerke sind die Nord-Süd-Brücke (1978) und die Stahlwerkbrücke (2003). Beide überspannen die in West-Ost-Richtung durch die Stadt verlaufende Bahnstrecke Hannover–Braunschweig. Die Errichtung der Stahlwerkbrücke dauerte insgesamt gut zwei Jahre: Der erste Spatenstich erfolgte am 1. August 2001, die Eröffnung fand unter großer Anteilnahme der Bevölkerung am 19. September 2003 statt. Ihren Namen verdankt die Brücke dem Peiner Stahlwerk, über dessen Werksgelände sie verläuft.

Peine liegt an der Bahnstrecke Hannover–Braunschweig. Eine Besonderheit besteht darin, dass das Bahnhofsgebäude im Besitz der Stadt ist, nachdem es Mitte der 1990er Jahre auf ihre Kosten errichtet wurde. An den Bahnhof angeschlossen ist zusätzlich ein Nahverkehrsterminal, der die öffentlichen Verkehrsmittel Bus und Bahn verknüpft. Der Peiner Bahnhof wurde mehrfach mit Preisen ausgezeichnet.

Der Bahnhof Peine war Endpunkt der 1922 eröffneten und bis 2003 vollständig stillgelegten Strecke aus Plockhorst. Außerdem ist die nur im Güterverkehr genutzte ehemalige Peine–Ilseder Eisenbahn der Verkehrsbetriebe Peine-Salzgitter mit dem Netz der Deutschen Bahn AG verknüpft.

Der Stadtverkehr wird hauptsächlich von der Peiner Verkehrsgesellschaft mbH (PVG) betrieben. Im Regionalverkehr fährt hauptsächlich die Regionalbus Braunschweig GmbH (kurz RBB).

Peine besitzt zwei Länden und einen Hafen am Mittellandkanal, der 1929 im Zuge des Baus des Mittellandkanals angeschlossen wurde. Die Häfen Peine befindet sich bei Kilometer 196,6 Süd und bei MLK km 202.

Weitere Anleger bestehen für die Personen- und Freizeitschifffahrt.

Die nächstgelegenen Flughäfen sind der Flughafen Hannover-Langenhagen in Langenhagen und der Flughafen Braunschweig-Wolfsburg in Braunschweig. Im Peiner Stadtgebiet bei Vöhrum befindet sich der Segelflugplatz Flugplatz Peine-Glindbruchkippe, auf dem Gebiet der Nachbargemeinde Edemissen der derzeit geschlossene Flugplatz Peine-Eddesse.




</doc>
<doc id="4031" url="https://de.wikipedia.org/wiki?curid=4031" title="Paris (Mythologie)">
Paris (Mythologie)

Paris [] (griech. Πάρις) ist in der griechischen Mythologie der Sohn des trojanischen Königs Priamos und der Hekabe. Er ist damit Bruder des Hektor und der Kassandra. Insgesamt hat er mehr als 50 Geschwister und Halbgeschwister. Indem er Helena entführt, löst er den Trojanischen Krieg aus.

Seit Homers "Ilias" trägt er gleichberechtigt den Namen Alexandros, was vermutlich die mit dem Namen Paris verbundene Eigenschaft einem griechischen Publikum erläutern sollte. Das griechische "Alexandros" bedeutet „der die Männer abwehrt“ und ist sein gebräuchlicher Name in den Beischriften der griechischen Vasenmalerei. Hyginus nannte ihn "Paris Alexandros". Möglicherweise hängt die Variante "Alexandros" mit dem König Alaksandu von Wilusa zusammen, der in einem hethitischen Dokument (CTH 76, sogenannter Alaksandu-Vertrag) aus dem frühen 13. Jahrhundert v. Chr. vorkommt.

Hekabe träumt vor der Geburt des Paris, sie gebäre eine Fackel, die Troja in Brand stecken werde. Nachdem sie Priamos von dem Traum erzählt hat, lässt dieser den Aisakos zu sich kommen, der die Fähigkeit besitzt, Träume zu deuten. Aisakos sagt, Hekabe werde einen Sohn gebären, der Trojas Verderben herbeiführen werde. Von dieser Weissagung erschreckt, beschließen Priamos und Hekabe, das Neugeborene auszusetzen. Der Auftrag wird Agelaos, einem Sklaven des Königs, übertragen. Der setzt das Kind auf dem Berg Ida aus; nach einiger Zeit kehrt er jedoch reumütig zurück. Zu seinem Erstaunen findet er das Kind gesund und munter vor: Eine Bärin hat es gesäugt. Agelaos nennt den Jungen Paris und zieht ihn bei sich auf dem Feld und bei den Hirten auf. Paris wächst als Schäfer auf. Mit Erreichen des Mannesalters heiratet er die Nymphe Oinone, eine Tochter des Flussgottes Kebren.

Eines Tages erscheint ihm Hermes, der ihn bittet auszuwählen, welche der drei Göttinnen Hera, Athene und Aphrodite die schönste sei, woraufhin diese versuchen, ihn zu bestechen. Paris trifft sein Urteil: Nachdem ihm Hera Macht verspricht und Athene Ruhm, entscheidet er sich für Aphrodite, die ihm die Hand der schönsten Frau auf Erden, Helena, der Frau des Menelaos, König von Sparta, versprochen hat. Die anderen beiden Göttinnen sind enttäuscht, Hera schwört Paris und den Trojanern ewige Feindschaft. Ihr Hass trägt zum Untergang Trojas bei und verfolgt den Trojaner Aeneas auch noch auf seinen Irrfahrten. (Siehe Vergils "Aeneis")

In Troja trauert Hekabe immer noch um den verlorenen Sohn. In ihrem Kummer wendet sie sich an Priamos; der verspricht ihr, Leichenspiele zu Ehren des verlorenen Prinzen zu veranstalten. Als Preis wird ein besonders kraftvoller Stier aus den Herden des Königs auf dem Berg Ida ausgesetzt. Dieser Stier ist jedoch das Lieblingstier des Paris, so dass dieser beschließt, an den Spielen in Troja teilzunehmen, um selbst den Stier zu gewinnen.

Tatsächlich gelingt es Paris, der sich weiterhin für einen einfachen Hirten hält und von den anderen dafür gehalten wird, den Sieg gegen seine Brüder und die stärksten jungen Trojer zu erringen. Paris’ Bruder Deiphobos jedoch will sich mit seiner Niederlage gegen einen Hirten nicht abfinden und möchte ihm am liebsten die Kehle durchschneiden. Aus Furcht vor Deiphobos flieht Paris zum Altar des Zeus. Dort sieht ihn seine Schwester Kassandra, die von Apollon mit der Fähigkeit des Wahrsagens ausgestattet worden ist, und erkennt in ihm den lange für tot gehaltenen Bruder. Als die Eltern hören, dass der verloren Geglaubte wieder aufgetaucht ist, nehmen sie ihn in den Königspalast auf, die Weissagung, Paris werde die Brandfackel Trojas sein, vergessend. Kassandra versucht vergeblich, sie daran zu erinnern; denn Apollon hat sie auch mit dem Fluch belegt, dass niemand ihre Prophezeiungen ernst nimmt.

Priamos war vor langer Zeit von den Griechen die Schwester Hesione geraubt worden. Aphrodite legt Paris in den Sinn, im Rat der Trojer vorzuschlagen, eine Gesandtschaft nach Sparta in Griechenland zu entsenden, die Hesione zunächst friedlich zurückverlangen, notfalls jedoch mit militärischer Gewalt zurückbringen soll. Bei dieser Gelegenheit berichtet Paris von seinem Urteil und davon, dass er nun unter Aphrodites Schutz stehe. Priamos vertraut der Hilfe Aphrodites und willigt in den Plan ein. Zur Gesandtschaft gehören auch Paris und Hektor.

In Sparta begegnet Paris jedoch Helena, die Aphrodite ihm als Gattin versprochen hat. Paris entführt sie, was nicht schwer ist, denn sie hat sich in ihn verliebt, und löst damit, ohne es zu wollen, den Trojanischen Krieg aus. Ethnologisch betrachtet handelte es sich um Brautraub, wie er in vielen Gesellschaften praktiziert und de facto geduldet wurde und bis heute in Hochzeitsbräuchen nachgespielt wird. In einer anderen Sagenversion handelt es sich freilich nur um ein Abbild, das Paris nach Hause führt, während die wirkliche Helena in Sparta verbleibt.

Zu großem Ruhm auf dem Schlachtfeld gelangt Paris nicht. Er ist ein guter Bogenschütze, im Kampf Mann gegen Mann versagt er jedoch, so z. B. bei seinem Duell mit Menelaos, dem Gemahl der Helena und König von Sparta, bei dem er von Menelaos fast mit seinem eigenen Helmriemen erdrosselt wird, bis sich Aphrodite selbst einmischt und ihn mit Hilfe einer Wolke in Sicherheit bringt.

In der Ilias wird es zwar nicht direkt herausgearbeitet, da die zentralen Heroen andere sind (Hektor, Achill, Ajax, Diomedes), aber auch Paris ist ein guter Kämpfer. Sein Themenbereich und seine Typisierung ist eine andere als die der anderen Heroen, aber er ist und bleibt ein Heros und ist somit den „Normalsterblichen“ an Kampfkraft immer noch überlegen.

Hektor lobt ihn sogar: „Tor du, schwerlich könnte ein Mann, der billig ist, tadeln, was in der Schlacht du vollbringst, denn du bist tapfer und wehrhaft.“ Er tadelt ihn daraufhin, dass er zu unwillig zum Kämpfen ist, und sagt, dass es ihn traurig stimme, wenn die Trojaner herablassend von Paris sprächen. Auf diese Rede folgt eine kurze Aristie von Hektor und Paris, das heißt, eine Phase, in der sie sehr schnell einige Griechen töten. Diese kurze Phase des ungehinderten Siegens ist weniger ausgearbeitet als andere Aristien, enthält weniger Details und keine einzige Rede und wird schroff von den Göttern beendet. So wird Paris nur kurz als ebenfalls tapferer Heros eingeführt und darf sich seiner Taten kaum so sehr wie die anderen Heroen rühmen. Dies ist die Darstellung von Paris’ Kampfesleistung in der Ilias.

Außerhalb der Ilias gelingt es ihm jedoch, den gefürchteten Achilleus zu töten, der bis auf seine Ferse als unverwundbar gilt. Dies vollbringt Paris mit dem Bogen, wobei ihm Apollon allerdings die Hand führt.

Der griechische Bogenschütze Philoktetes besitzt den Bogen und die Pfeile des Herakles, die mit dem tödlichen Gift der Lernäischen Schlange vergiftet sind. Mit zweien dieser Pfeile verwundet er Paris. Leidend schleppt dieser sich auf den Berg Ida zu Oinone, seiner ersten Ehefrau, und bittet sie, ihn mit einem Gegengift, das sie besitzt, zu retten. Aus Zorn darüber, dass er sie einst Helenas wegen verlassen hat, verweigert sie ihm jegliche Hilfe. Qualvoll erliegt Paris seiner Verletzung. Oinone aber wird von Reue, ihm nicht geholfen zu haben, überwältigt; sie lässt einen Scheiterhaufen schichten und springt zu dem geliebten Toten in die Flammen. Helena fällt als Ehefrau an dessen nächst jüngeren Bruder, Deiphobos.

Bei weitem nicht alles, was oben referiert ist, steht in der Ilias. Zumeist dürfte der Stoff außerhalb des Kampfes vor Troja auf die Kypria zurückgehen, aber natürlich hat Homer die Gestalt für uns geprägt, vor allem die Ambivalenz zwischen dem manchmal tüchtigen Kämpfer und dem dann wieder verweichlichten Schönling. Manche Züge, etwa der Eris-Apfel, sind offenbar erst später aufgekommen, als sich die griechische Tragödie und vor allem griechische Komödie intensiv mit der Geschichte befassten. Einen starken Einfluss hat auch die Vasenmalerei ausgeübt.




</doc>
<doc id="4032" url="https://de.wikipedia.org/wiki?curid=4032" title="Polyglotta Africana">
Polyglotta Africana

Polyglotta Africana war eine von Sigismund Wilhelm Koelle 1854 verfasste Studie, in der er afrikanische Sprachen miteinander verglich.

Sie umfasst etwa 200 Wortlisten (je 296 sorgfältig ausgewählte Einträge) und war damals bahnbrechend.
Nach heutiger Klassifikation handelt es sich um etwa 120 Sprachen.
Er verwendete ein standardisiertes umfassendes Alphabet und eine standardisierte, konsistente Orthographie.



</doc>
<doc id="4034" url="https://de.wikipedia.org/wiki?curid=4034" title="Politische Parteien in Deutschland">
Politische Parteien in Deutschland

Die Bundesrepublik Deutschland ist eine Parteiendemokratie. Die politischen Parteien bestimmen zu einem großen Teil die Politik in Deutschland. Ihre Gründung und ihr Aufbau ist mit einem Parteiengesetz geregelt; die Parteien werden auch in der Verfassung erwähnt.

Die Stellung politischer Parteien in Deutschland wird in Artikel Grundgesetz folgendermaßen beschrieben:
Die Aufgaben der Parteien und ihre wichtige Rolle in modernen Staaten werden damit zum ersten Mal in einer deutschen Verfassung anerkannt. Die Aufnahme der Parteien in das Grundgesetz wird als Lehre aus der Weimarer Republik gesehen; tatsächlich aber gab es nach 1945 auch in anderen Ländern den Trend, die Parteien in der Verfassung zu erwähnen. Grund dafür ist die große Bedeutung von Parteien im modernen Staat.

Eine Partei muss sich an Regeln halten, dafür kann sie gefördert und nur unter erschwerten Regeln verboten werden. Nur Parteien dürfen an Bundestags- und Landtagswahlen teilnehmen. Daher spricht man von einem Parteienprivileg.

Nach dem Gesetz über die politischen Parteien (Parteiengesetz) sind Parteien folgendermaßen definiert:

Auf die Parteien finden die Bestimmungen des Bürgerlichen Gesetzbuchs (BGB) Anwendung, soweit nicht Spezialgesetze abweichende Regelungen treffen. Dementsprechend sind sie zivilrechtliche rechtsfähige und nichtrechtsfähige Vereine mit besonderen Rechten und Pflichten.

Parteien müssen sich beim Bundeswahlleiter registrieren und dort Satzung, Programm und die Namen der Vorstandsmitglieder hinterlegen. Diese Daten kann dort jedermann kostenlos auf dem Postwege oder per Internet abrufen. Das aktuelle Anschriftenverzeichnis aller registrierten Parteien ist als PDF-Datei online abrufbar. Ihre Arbeit nimmt eine Partei in Deutschland offiziell mit der Gründungsversammlung auf.

Die Parteien geben sich ein Grundsatzprogramm, das in der Regel für mehrere Jahre oder Jahrzehnte Gültigkeit hat. In den Grundsatzprogrammen werden vor allem die Werte und Ideologien festgelegt.

Vor den einzelnen Wahlen werden aktuelle und für die kommende Legislaturperiode bestimmte Wahlprogramme oder Regierungsprogramme beschlossen.

Programmatische Beschlüsse sind neben der Bundesebene auch auf Landesebene und in den Kommunen üblich. Daneben gibt es zu aktuellen Anlässen auch programmatische Beschlüsse zu einzelnen Themenbereichen, die das Grundsatzprogramm ausfüllen und ergänzen sollen. Die deutschen Parteien haben nach der Wiedervereinigung Deutschlands meist auch neue Grundsatzprogramme verabschiedet.

Das Parteiengesetz benennt in als allgemeine Arbeitsaufgabe von Parteien das Mitwirken an der politischen Willensbildung des Volkes auf allen Gebieten des öffentlichen Lebens und anschließend acht Punkte, wie diese Aufgabe insbesondere umzusetzen ist:

In Deutschland erhalten die Parteien neben Spenden, Mitgliedsbeiträgen und sonstigen Einnahmen auch staatliche Zuwendungen.

Nachdem bei der ersten Bundestagswahl viele Parteien in den Bundestag gewählt wurden, befürchtete man teilweise wieder Weimarer Verhältnisse (Parteienzersplitterung). Bereits in der ersten Legislaturperiode nahm die Zahl der Fraktionen ab. Bei den folgenden Wahlen konzentrierten sich die Stimmen fast vollständig auf die vier Parteien (CDU/CSU, SPD, FDP), die drei fraktionen bildeten. Erst 1983 schaffte mit den Grünen wieder eine weitere Partei den Einzug in den Bundestag. Seit der Wiedervereinigung ist mit der PDS bzw. deren Nachfolgerin Die Linke eine weitere Partei im Bundestag vertreten. Die FDP scheiterte bei der Bundestagswahl 2013 erstmals an der 5-Prozent-Sperrklausel, kehrte aber 2017 in den Bundestag zurück. Bei der Bundestagswahl 2017 schaffte die AfD den Sprung in den Bundestag, sodass seit dieser Wahl sechs Fraktionen im Bundestag vertreten sind.

In Deutschland wird oft Kritik an der Macht und Arbeit der Parteien geäußert. Diese Kritik wird vielfach mit dem politischen Schlagwort der Parteienverdrossenheit beschrieben (siehe auch Politikverdrossenheit).

Kritikpunkte sind:

Im Deutschen Bundestag sind seit der Bundestagswahl 2017 sechs Fraktionen und acht Parteien vertreten: "Union" ("CDU" und "CSU"), "SPD", "AfD", "FDP", "Die Linke" und "Bündnis 90/Die Grünen." Die Vertreter der "Blauen Partei" sind "fraktionslos." 

Nahezu allen einflussreichen Parteien stehen – mehr oder weniger selbständige – Jugendorganisationen zur Seite. Diese sind die Junge Union (CDU/CSU), die Jusos (SPD), die Junge Alternative (AfD), die Junge Liberale (FDP), die Linksjugend Solid (Die Linke) und die Grüne Jugend (Bündnis 90/Die Grünen).

Die Christlich Demokratische Union Deutschlands (CDU) wurde 1945 als überkonfessionelle Nachfolgepartei bürgerlicher, christlich geprägter und nationalliberaler Parteien (Zentrumspartei, DDP, DVP u. a.) gegründet. Sie ist inhaltlich sowohl der christlichen Soziallehre als auch dem Konservatismus sowie dem Liberalismus zugeneigt, und deckt damit ein christlich-soziales und insbesondere auch ein liberal-konservatives Spektrum ab. Im Zuge der Wiedervereinigung ging die CDU (Ost) am 1. Oktober 1990 in die CDU auf. Die CDU (Ost) war noch bis 1989 zwangsweise eine DDR-Blockpartei und verstand sich als eine „Partei des Sozialismus“. Auch die DDR-Blockpartei DBD (Demokratische Bauernpartei Deutschlands) vertrat bis 1989 in wesentlichen Punkten die SED-Linie und schloss sich nach der Wiedervereinigung der CDU an. Mit der Ausnahme Bayerns ist die CDU im gesamten Bundesgebiet vertreten. Die CDU stellte in ihrer Geschichte 5 der bisher 8 Bundeskanzler der Bundesrepublik Deutschland. Sie bildet mit der CSU und der Sozialdemokratischen Partei Deutschlands (SPD) die gegenwärtige Koalitionsregierung der Bundesrepublik Deutschland, deren Bundeskanzlerin die CDU-Bundesvorsitzende Angela Merkel ist.

Die Christlich-Soziale Union in Bayern (CSU) hat eine konservativere Ausrichtung als die CDU, in der Wirtschafts- und Sozialpolitik gilt sie hingegen als sozialstaatlicher ausgerichtet. Die Partei tritt nur in Bayern zur Wahl an. Dafür verzichtet die CDU dort auf einen eigenen Landesverband. Sie bildet mit der CDU im Deutschen Bundestag eine Fraktionsgemeinschaft, zusammen werden sie „die Union“ oder auch „Unionsparteien“ genannt.

Die Sozialdemokratische Partei Deutschlands (SPD) existiert einschließlich ihrer Vorläuferorganisationen seit 1863 und ist damit die älteste bestehende politische Partei Deutschlands. Sie hat die Tradition der Sozialdemokratie begründet. Nach Verbot in der Zeit des Nationalsozialismus wurde sie 1945 wiedergegründet. Sie versteht sich seit ihrem Godesberger Programm von 1959 auch offiziell nicht mehr nur als Arbeiterpartei, sondern als eine Volkspartei, die für breite Schichten wählbar sein will. Ihr Wahlspruch lautet „Freiheit, Gerechtigkeit und Solidarität“. Mit Willy Brandt, Helmut Schmidt und Gerhard Schröder stellte die SPD bisher 3 Bundeskanzler.

Die AfD wurde am 6. Februar 2013 gegründet, ihre erste öffentliche Versammlung hatte sie am 11. März 2013 in Oberursel (Taunus). Bei der Bundestagswahl 2013 scheiterte die Alternative für Deutschland (AfD) an der 5-%-Hürde mit einem amtlichen Zweitstimmenergebnis von 4,7 %. Sie steht dem Euro kritisch gegenüber und wurde deshalb in den Medien nach ihrer Gründung oft als „Anti-Euro-Partei“ bezeichnet. Des Weiteren werden die Rückverlagerung von Kompetenzen von der EU an die Mitgliedstaaten und mehr direkte Demokratie sowie eine Steuerreform nach dem Vorbild Paul Kirchhofs gefordert. Eingeordnet wurde sie in ihrer Anfangszeit zumeist als konservativ und/oder wirtschaftsliberal. Bestimmte politische Forderungen, Formulierungen und Mitglieder werden in Kommentaren und Analysen verschiedener politischer Beobachter in Forschung und Medien mittlerweile jedoch häufig als rechtspopulistisch kategorisiert. Einige Politikwissenschaftler erkennen bei Teilen oder bestimmten Führungspersonen der AfD seit geraumer Zeit auch rechtsextreme Tendenzen. Insbesondere dem hochrangigen Thüringer AfD- Politiker Björn Höcke, der als Vertreter des rechten Flügels der Partei gilt, werden offen rassistische und nationalistische Ansichten bescheinigt. Der Führungswechsel der Partei im Juli 2015 wurde als weiterer Rechtsruck eingestuft. Verschiedene Wissenschaftler stellen seitdem bei Teilen der Partei auch völkisch- nationalistische und antisemitische Tendenzen fest. Im Mai 2014 gelang es der Partei, in das Europäische Parlament einzuziehen. Seit September 2014 konnte sie bei allen Landtagswahlen Mandate erringen und ist mittlerweile in 14 der 16 Landesparlamente mit Abgeordneten vertreten. Bei der Bundestagswahl 2017 wurde die AfD mit 12,6 % drittstärkste Kraft und konnte so erstmals in den deutschen Bundestag einziehen. 

Die Freie Demokratische Partei (FDP) wurde 1948 gegründet und beruft sich in ihrem Selbstverständnis auf die Tradition des deutschen Liberalismus, der sich bereits 1861 mit der Deutschen Fortschrittspartei in Preußen als erste politische Partei Deutschlands im heute verstandenen Sinne organisiert hatte. Die moderne FDP steht insbesondere in Wirtschafts-, aber auch in Bürgerrechtsfragen für mehr Freiheiten und Verantwortung des Einzelnen sowie für eine stärkere Zurückhaltung des Staates. 

Vor der Wiedervereinigung Deutschlands vereinigte sich die FDP 1990 mit der LDPD (Liberal-Demokratische Partei Deutschlands) und der NDPD (Nationaldemokratische Partei Deutschlands der DDR), die noch bis 1989 als Blockparteien an der Seite der Sozialistischen Einheitspartei Deutschlands politisch agierten.

Bei der Bundestagswahl 2013 verfehlte die FDP mit 4,8 % die Fünf-Prozent-Hürde bei Bundestagswahlen und war somit erstmals in ihrer Geschichte nicht mehr im Bundestag vertreten. Die Partei konnte seitdem jedoch wieder in neun Landesparlamente und das Europaparlament einziehen. Bei der Bundestagswahl 2017 gelang ihr der Wiedereinzug in den Bundestag.

Die Linke entstand am 16. Juni 2007 aus der Fusion der eher in Ostdeutschland verankerten Linkspartei.PDS (früher: SED) und der westdeutsch geprägten WASG. Die Partei zielt auf die Überwindung des Kapitalismus hin zu einem demokratischen Sozialismus ab. Einige Politikwissenschaftler sowie Vertreter anderer Parteien stufen die Partei auch als populistisch ein. 

Im Jahr 2005 wurde aus Protest gegen die Politik der rot-grünen Bundesregierung die Partei WASG gegründet, auf Initiative von linken Gewerkschaftern und regierungskritischen vormaligen SPD-Mitgliedern. Für die Bundestagswahl 2005 öffnete die PDS ihre Wahllisten für WASG-Kandidaten. Aus diesem Grund nahm die PDS eine Umbenennung in Linkspartei.PDS vor. Inhaltlich berief sich die Linkspartei.PDS auf demokratisch-sozialistische Ideale und beanspruchte für sich, das politisch linke demokratische Spektrum in Deutschland abzudecken. Die Linke, Mitglieder der Linken sowie einzelne Organisationen innerhalb der Partei wurden in der Vergangenheit mehrfach wegen linksextremer Bestrebungen vom Verfassungsschutz beobachtet. Insbesondere die Kommunistische Plattform (KPF), ein politischer Zusammenschluss innerhalb der Partei, wird vom Bundesamt für Verfassungsschutz als linksextrem eingeschätzt. In den Medien wurden in den vergangenen Jahren des Weiteren mehrfach antisemitische und antizionistische Tendenzen innerhalb der Partei thematisiert.
Die "Grünen" entstanden als bundesweite Partei 1980 aus Teilen der Neuen Sozialen Bewegungen, beispielsweise der damaligen Frauenbewegung, der Friedens- und der Ökologiebewegung der 1970er Jahre. 1983 zogen sie erstmals in den Bundestag ein. 1990 schlossen sie sich mit den ostdeutschen Grünen und 1993 mit Bündnis 90 zu Bündnis 90/Die Grünen zusammen. Ein wesentlicher inhaltlicher Schwerpunkt der Partei ist der Umweltschutz. Die Partei gilt zudem als linksliberal und ist dem Multikulturalismus zugeneigt. Sie sind stärker in den westlichen Bundesländern vertreten. 1985 waren sie (in Hessen) erstmals in einer Koalition mit der SPD an einer Landesregierung beteiligt, danach noch in vielen weiteren Ländern und schließlich, 1998 bis 2005, an der Bundesregierung. In den letzten Jahren kam es auf Landesebene auch mehrfach zu Koalitionen mit der CDU. 

Neben den im Bundestag vertretenen Parteien gibt es zahlreiche regionale und Kleinparteien, deren politischer Einfluss auf Bundesebene durch die Fünf-Prozent-Hürde in der Regel auf außerparlamentarische Aktivitäten beschränkt ist. Für regional starke Parteien besteht die Möglichkeit, über Direktmandate in den Bundestag einzuziehen. Bei mindestens drei Direktmandaten ist die Partei von der Fünf-Prozent-Hürde ausgenommen. Seit Einführung dieser Regelung gelang dies jedoch nur der Deutschen Partei, die 1957 über sechs Direktmandate bei 3,4 % der Zweitstimmen 17 Mandate erreichte, sowie der PDS, die 1994 mit vier Direktmandaten die Sperrklausel übersprang und bei 4,4 % Zweitstimmen 30 Mandate bekam, 2002 jedoch nur zwei Direktmandate erreichte und entsprechend nur mit zwei Sitzen im Bundestag vertreten war.

Bei der Bundestagswahl 2017 traten 63 Parteien, bei der Europawahl 2014 25 Parteien und Sonstige Politische Vereinigungen an. Neben den vier Bundestagsparteien zogen neun weitere Parteien ins Europaparlament ein. Drei weitere Parteien bzw. Wählergruppen sind in Landesparlamenten vertreten.

Drittstärkste nicht im Parlament vertretene Partei bei den Bundestagswahlen 2013 wurde die Piratenpartei Deutschland. Die Piraten entstanden am 10. September 2006 nach schwedischem Vorbild als Protestpartei gegen die von ihr beklagte zunehmende Überwachung und Einschränkung u. a. im Internet durch den Staat und die Urheberrechts-Verwerter. Sie konzentrieren sich auf die Themen Informationsfreiheit, Bürgerrechte, freie Bildung und „Transparenz statt Korruption“. 2009 war durch den Übertritt des SPD-Abgeordneten Jörg Tauss zeitweise ein Mitglied der Piratenpartei im Bundestag vertreten. Bei der Wahl zum Abgeordnetenhaus von Berlin 2011 gelang den Piraten erstmals der Einzug in ein Landesparlament. Im Jahr 2012 konnten sie Mandate bei allen drei Landtagswahlen, im Saarland, in Schleswig-Holstein und in Nordrhein-Westfalen erringen. Seither scheiterte sie jedoch bei allen weiteren Landtagswahlen sowie bei der Bundestagswahl 2013 an der Fünf-Prozent-Hürde. Bei der Landtagswahl 2017 in Nordrhein-Westfalen verloren die Piraten ihre letzten Landtagsmandate in einem deutschen Bundesland.

In den ersten Wahlperioden gab es im Bundestag rechtskonservative Parteien wie die Deutsche Reichspartei (1950), die Deutsche Partei (DP) und den Bund der Heimatvertriebenen und Entrechteten. Bis 1961 war die "Deutsche Partei" durch Direktmandate im Bundestag vertreten, da ihr die CDU Wahlkreise überließ. In den 1940er und 1950er Jahren setzte sich die DP vor allem gegen Kommunismus sowie Sozialismus, Planwirtschaft und Bodenreform ein und engagierte sich überwiegend für ehemalige Wehrmachtsangehörige und Vertriebene. Außerdem orientierte sich die Partei an bürgerlich-konservativen Werten und arbeitete auf der Basis des Grundgesetzes. Von 1961 bis 2017 wurden Parteien rechts der Union im politischen Spektrum nicht mehr in den Deutschen Bundestag gewählt. Auf Landesebene konnten Parteien des rechten Lagers zeitweise in Parlamente einziehen.

Die Sozialistische Reichspartei Deutschlands (Kurzbezeichnung: SRPD), auch Sozialistische Reichspartei (Kurzbezeichnung: SRP), war eine rechtsextreme Partei, die sich selbst in der Tradition der NSDAP sah. Das Parteiprogramm der SRP basierte daher auch in wesentlichen Teilen auf dem der NSDAP. Die Partei errang bei der Landtagswahl in Niedersachsen im Mai 1951 11,0 % der Stimmen. Im Oktober 1951 konnte die Partei bei der Bürgerschaftswahl in Bremen 7,7 % der Stimmen gewinnen. 1951 verfügte die damalige Bundesregierung unter Konrad Adenauer ein Verbot der angegliederten Organisationen wie der paramilitärischen Ordnergruppe Reichsfront und beschloss zugleich die Einleitung eines Verbotsverfahrens für die Partei selbst. Im Jahr 1952 wurde die SRP vom Bundesverfassungsgericht schließlich als verfassungswidrig verboten.

Die Nationaldemokratische Partei Deutschlands (NPD) wurde 1964 gegründet und entstand im Wesentlichen aus den verschiedenen rechtskonservativen und nationalliberalen Parteien und Gruppierungen jener Zeit. Ende der 60er Jahre war sie für jeweils eine Wahlperiode in sieben der damals zehn westdeutschen Landesparlamente vertreten. Nach 1970 gewannen zunehmend militante Gruppen in der Partei an Einfluss. Zu Beginn der 90er Jahre setzte unter dem damaligen Bundesvorsitzenden und Holocaustleugner Günter Deckert eine weitere Radikalisierung der Partei ein. So wurden nun offen nationalsozialistische Elemente in die Partei verankert. Nach der deutschen Wiedervereinigung gelangen ihr wieder vereinzelt Erfolge bei Wahlen, so dass sie für jeweils zwei Wahlperioden in Sachsen und in Mecklenburg-Vorpommern im Landtag vertreten war. Ende 2010 fusionierte die NPD mit der ebenfalls rechtsextremen und teilweise bei Landtagswahlen erfolgreichen Deutsche Volksunion (DVU), der Zusammenschluss wurde vom Landgericht München im Januar 2011 allerdings als rechtlich unwirksam gestoppt. Gegen die NPD gab es bereits zweimal Parteiverbotsverfahren. Im Januar 2017 wies das Bundesverfassungsgericht auch den zweiten Verbotsantrag ab. Zwar sahen es die Richter als erwiesen an, dass die Partei verfassungsfeindlich sei, die NPD habe aber nicht das „Potenzial“, die Demokratie in Deutschland zu beseitigen.

Die DVU war eine rechtsradikale Partei, die von dem Verleger Gerhard Frey 1971 ursprünglich als Verein gegründet und von ihm 38 Jahre lang geleitet worden war. Sie konnte insgesamt neun Mal in Landesparlamente einziehen. Ihre größten Wahlerfolge erzielte sie in Bremen, Brandenburg und insbesondere in Sachsen-Anhalt. Die DVU wandte sich gegen ein vereintes Europa, relativierte und verharmloste den Holocaust sowie die deutsche Kriegsschuld und wurde als völkisch, fremdenfeindlich und antisemitisch eingeordnet. Nach der gescheiterten Fusion mit der NPD löste sich die Partei 2011 endgültig auf. 

Die Republikaner (REP) wurden 1983 von ehemaligen CSU-Abgeordneten als rechtskonservative Partei gegründet und konnten in den 80er Jahren ins Europaparlament und ins Abgeordnetenhaus von (West-)Berlin sowie in den 90er Jahren in den Landtag Baden-Württembergs einziehen. Drei Politiker, die als Unionsvertreter in den Bundestag eingezogen waren, wechselten zu den Republikanern, zwei von 1983 bis 1985 und einer von 1993 bis 1994. Die Partei wurde zeitweise vom Verfassungsschutz beobachtet. Aktuell ist sie schon seit geraumer Zeit in keinem Parlament mehr vertreten. 

Von 2001 bis 2004 war die Partei Rechtsstaatlicher Offensive (auch Schill-Partei) in der Bürgerschaft von Hamburg vertreten und stellte dort zusammen mit CDU und FDP die Landesregierung. In der öffentlichen Diskussion wurde die Partei teils als rechtskonservativ, vor allem jedoch als rechtspopulistische Protestpartei wahrgenommen.

Die Partei Liberal-Konservative Reformer (kurz LKR, bis November 2016 Allianz für Fortschritt und Aufbruch, kurz ALFA) ist eine euroskeptische Kleinpartei, die am 19. Juli 2015 in Kassel als Abspaltung der Alternative für Deutschland (AfD) gegründet worden ist. Die Partei ist derzeit mit fünf Abgeordneten im Europäischen Parlament sowie drei Abgeordneten in der Bremischen Bürgerschaft vertreten. Diese Abgeordneten sind von der AfD zu der Partei gewechselt. Bei Landtagswahlen erhielt die Partei selbst bisher zwischen 0,3 % und 1 % der Stimmen.

Weiterhin gibt es zahlreiche vorrangig regional aktive Kleinparteien und Wählergruppen, beispielsweise die in den Neuen Bundesländern aktive, rechtskonservative Deutsche Soziale Union (DSU), die im Bremer Landtag vertretene Wählervereinigung Bürger in Wut, die in Nordrhein-Westfalen tätige Bürgerbewegung pro NRW oder die aus dem Umfeld der NPD bestehende Bürgerinitiative Ausländerstopp in München und Nürnberg.

Unter anderem im 19. Jahrhundert blieben linke Parteien und Gruppen über relativ große Zeiträume hinweg aus Parlamenten ausgegrenzt, waren verboten oder infolge eines Zensuswahlrechts in der Relation zu den tatsächlichen Mehrheitsverhältnissen in der Bevölkerung nur unterrepräsentiert in den Kammerparlamenten vertreten. Dies galt insbesondere für sozialistisch inspirierte Zusammenschlüsse, die gerade im 19. Jahrhundert, aber auch bis in die Gegenwart hinein revolutionäre Umwälzungen anstrebten und teilweise auch umsetzten. Entsprechend waren diese Gruppen auch immer vor Ort aktiv an Revolutionen, Aufständen, Revolten und anderen sozialen (Klassen-)Kämpfen über die gesamte Neuzeit hinweg beteiligt oder führten diese an.

Heute existieren neben der Partei Die Linke, die im Bundestag vertreten ist, viele außerparlamentarische politische Gruppierungen mit unterschiedlicher Wirkungskraft, die sich auf linke Positionen beziehen. Die inhaltlichen Vorstellungen darüber, was linke Politik mit welchen Mitteln anstrebt, sind sehr heterogen.

Die Kommunistische Partei Deutschlands (KPD) war eine kommunistische Partei in Deutschland, die am 1. Januar 1919 gegründet wurde. Sie ging aus mehreren linksrevolutionären Gruppierungen hervor, die sich nach dem Ersten Weltkrieg im Verlauf der Novemberrevolution von 1918 vereinten. Nach dem Zweiten Weltkrieg gründete sich die KPD unter Führung des späteren DDR-Staatsratsvorsitzenden Walter Ulbricht auf gesamtdeutscher Ebene neu, nach der Teilung in zwei deutsche Staaten arbeitete die KPD auch in der Bundesrepublik zunächst weiter, bis sie 1956 verboten wurde.

In Reaktion darauf gründete sich 1968 auf Initiative der DDR-Staatspartei SED die Deutsche Kommunistische Partei (DKP). Da sie in erheblichem Umfange politische Positionen des damaligen Ostblocks wie die gewaltsame Niederschlagung des Volksaufstands vom 17. Juni 1953 in der DDR und des Prager Frühlings 1968, sowie den Bau der Mauer verteidigte und den sowjetischen Einmarsch in Afghanistan 1979 begrüßte, konnte sie in der Bundesrepublik nie nennenswerte Wahlergebnisse erzielen. Die aus dem Westberliner Teil der SED hervorgegangene Sozialistische Einheitspartei Westberlins löste sich kurz nach der Wiedervereinigung auf. Insbesondere in den 1970ern entstanden die maoistisch orientierten sogenannten K-Gruppen, die, bzw. deren Nachfolgeorganisationen zum Teil bis heute bestehen. Daneben existierten bzw. existieren einige trotzkistische Kleinparteien.

Unter den kommunistischen Parteien nahmen neben der DKP in den letzten Jahren die Marxistisch-Leninistische Partei Deutschlands und die trotzkistisch orientierte Sozialistische Gleichheitspartei an Bundestags- und Landtagswahlen teil. Die 1990 in Ost-Berlin gegründete, stalinistisch orientierte Kommunistische Partei Deutschlands („KPD-Ost“ oder „KPD (Rote Fahne)“) nahm an Landtagswahlen in den neuen Bundesländern teil. Weitere linksradikale Parteien verfolgen eine entristische Strategie, nehmen nur an Kommunalwahlen teil oder sind hauptsächlich gewerkschaftlich und außerparlamentarisch tätig und sind damit keine Parteien im Sinne des Parteiengesetzes.

Ein Sonderfall ist der Landtag von Schleswig-Holstein, wo aufgrund einer Sonderregelung für die dänische Minderheit der Südschleswigsche Wählerverband gesetzlich von der Fünf-Prozent-Hürde befreit wurde. Die Regelung gilt auch für den Bundestag, allerdings trat der SSW seit den 50er Jahren nicht mehr zu Bundestagswahlen an.

Bei der Wahl zum Landtag Brandenburg ist die Minderheit der Sorben ebenfalls von der Fünf-Prozent-Hürde befreit. Die sorbische Lausitzer Allianz trat bisher jedoch nicht zur Landtagswahl an. In Sachsen, ebenfalls Siedlungsgebiet der Sorben, ist hingegen keine Befreiung von der Sperrklausel vorgesehen. Die Partei Die Friesen sieht sich als Vertreter der Minderheit der Friesen, ist in Niedersachsen jedoch auch nicht von der Sperrklausel befreit.

Die Bayernpartei als regionale Partei war 1949–1953 im Bundestag und 1950–1966 im Bayerischen Landtag sowie von 1954 bis 1957 und 1962 bis 1966 in der Bayerischen Staatsregierung vertreten. Seit 1966 hat sie nur noch Sitze in Kommunalparlamenten und in Bezirkstagen. Andere Regionalparteien (z. B. Die Westfalen) sind praktisch ohne Relevanz.

Bei den Wahlen zu Kommunalparlamenten, teilweise auch bei Landtagswahlen, kandidieren neben Parteien auch unabhängige Wählergruppen. Diese sind teilweise in der Bundesvereinigung Freie Wähler organisiert. Bei der Landtagswahl in Bayern 2008 zog als erster Landesverband die Freien Wähler Bayern in ein Landesparlament ein. Die Bundesvereinigung ist seit 2014 mit einer Abgeordneten im Europaparlament vertreten. Im September 2014 zog die unabhängige Brandenburger Vereinigte Bürgerbewegungen/Freie Wähler in den Brandenburgischen Landtag ein.

Viele der in Deutschland existierenden Kleinparteien sind Interessenparteien und politisch weitgehend bedeutungslos. Diese konzentrieren sich häufig auf eine bestimmte Zielgruppe – beispielsweise die Grauen Panther als Partei von Rentnern oder die im Europaparlament vertretene Familien-Partei Deutschlands als Partei von Eltern – oder einen eingegrenzten Themenbereich – zum Beispiel die ebenfalls im Europaparlament vertretene Tierschutzpartei.

Die Ökologisch-Demokratische Partei (ÖDP) entstand 1982 als konservative Abspaltung der Grünen. Sie hat ihren Schwerpunkte in Bayern und Baden-Württemberg und ist seit 2014 im Europäischen Parlament vertreten.

Einige Parteien wenden sich explizit an religiöse Menschen. Dies sind zum Beispiel Bündnis C – Christen für Deutschland oder Bündnis für Innovation und Gerechtigkeit für Muslime.

Die V-Partei³ - Partei für Veränderung, Vegetarier und Veganer ist eine am 30. April 2016 im Rahmen der VeggieWorld-Messe in München gegründete Partei. Schwerpunkte setzt die V-Partei³ bei den Themen Umwelt- und Klimaschutz, Tierrechte und Verbraucherschutz. Sie setzt sich nach eigenen Angaben ebenfalls für Chancengleichheit, soziale Gerechtigkeit und Gleichstellung ein, worunter auch die Einführung eines bedingungslosen Grundeinkommens fällt. An der Bundestagswahl 2017 nahm die V-Partei³ in zwölf Bundesländern teil und erreichte 64.130 Zweitstimmen (0,1 %).

Die PARTEI ist ein Satireprojekt, sie wird von Redakteuren des Satirezeitschrift Titanic geführt. Sie verschreibt sich als einzige Partei offen dem Populismus, eine zentrale Forderung ist der Wiederaufbau einer Mauer zwischen West- und Ostdeutschland. Seit 2014 ist sie im Europäischen Parlament vertreten, sie gewann bei Kommunalwahlen zahlreiche Mandate und wurde bei der Bundestagswahl 2017 mit 1,0 % gleichauf mit den freien Wählern zur stärksten nicht im Parlament vertretenen Partei. Die PARTEI wird von zahlreichen Prominenten unterstützt, die teilweise auch bei Wahlen als Kandidaten für sie antreten.

Weiterhin gibt es in Deutschland mehrere Parteien, die durch radikale, teilweise abstruse politische Forderungen auffallen. Diese sind zum Teil Spaßparteien. Die APPD versteht sich als „Anwalt des Pöbels und Sozialschmarotzer“ und fordert u. a. das Recht auf Arbeitslosigkeit bei vollem Lohnausgleich und die Abschaffung der Schulpflicht.

In der Geschichte der Bundesrepublik gab es zwei Parteiverbote: Gegen die Sozialistische Reichspartei 1952 und die Kommunistische Partei Deutschlands 1956. Ein NPD-Verbotsverfahren scheiterte 2003 vor allem wegen Koordinationsfehlern beim Bundesamt für Verfassungsschutz und den Verfassungsschutzämtern der Länder, da V-Leute auch in der Führungsebene der Partei tätig waren. Am 3. Dezember 2013 wurde das Verbot der NPD zum zweiten Mal beantragt und 2017 vom Bundesverfassungsgericht wegen der Bedeutungslosigkeit der Partei zurückgewiesen. 





</doc>
<doc id="4035" url="https://de.wikipedia.org/wiki?curid=4035" title="Planetarischer Nebel">
Planetarischer Nebel

Ein planetarischer Nebel ist ein astronomisches Objekt aus der Kategorie der Nebel und besteht aus einer Hülle aus Gas und Plasma, das von einem alten Stern am Ende seiner Entwicklung abgestoßen wird. 

Der Name ist historisch bedingt und irreführend, denn solche Nebel haben nichts mit Planeten zu tun. Die Bezeichnung stammt daher, dass sie im Teleskop meist kugelförmig erscheinen und aussehen wie ferne Gasplaneten.

Planetarische Nebel existieren meist nicht länger als einige zehntausend Jahre. Im Vergleich zu einem durchschnittlichen „Sternenleben“, das mitunter mehrere Milliarden Jahre dauert, ist diese Zeitspanne sehr kurz.

In unserer Galaxis, dem Milchstraßensystem, sind rund 1.500 planetarische Nebel bekannt.

Planetarische Nebel spielen eine entscheidende Rolle in der chemischen Evolution der Galaxis, da das abgestoßene Material die interstellare Materie mit schweren Elementen wie Kohlenstoff, Stickstoff, Sauerstoff, Calcium und anderen Reaktionsprodukten der stellaren Kernfusion anreichert. In anderen Galaxien sind planetarische Nebel manchmal die einzigen beobachtbaren Objekte, die genug Information liefern, um etwas über die chemische Zusammensetzung zu erfahren.

Mit dem Hubble-Weltraumteleskop wurden Aufnahmen vieler planetarischer Nebel angefertigt. Ein Fünftel der Nebel weist eine kugelförmige Gestalt auf. Die Mehrzahl ist jedoch komplex aufgebaut und weist unterschiedliche Formen auf. Die Mechanismen der Formgebung sind noch nicht genau bekannt. Mögliche Ursachen könnten Begleitsterne, Sternwinde oder Magnetfelder sein.
Planetarische Nebel sind im Allgemeinen schwach leuchtende Objekte und deshalb mit dem bloßen Auge nicht beobachtbar. Der erste entdeckte planetarische Nebel war der Hantelnebel im Sternbild Fuchs. Er wurde 1764 von Charles Messier entdeckt und wird in seinem Katalog mit dem Index M 27 aufgeführt.

Durch die relativ geringe optische Auflösung der damaligen Teleskope sah ein planetarischer Nebel wie eine winzige neblige Scheibe aus. Da der 1781 entdeckte Planet Uranus einen ähnlichen Anblick bot, führte sein Entdecker Wilhelm Herschel 1785 für diese Nebel die bis heute beibehaltene Bezeichnung ein. 

Die Zusammensetzung planetarischer Nebel blieb unbekannt, bis in der Mitte des 19. Jahrhunderts die ersten spektroskopischen Beobachtungen durchgeführt wurden. William Huggins war einer der ersten Astronomen, die das Lichtspektrum astronomischer Objekte studierten, indem er mit Hilfe eines Prismas ihr Licht spektral zerlegte. Seine Beobachtungen von Sternen zeigten ein durchgehendes, also kontinuierliches Spektrum mit einigen dunklen Absorptionslinien. Wenig später fand er heraus, dass einige neblige Objekte, wie der Andromedanebel, ein ganz ähnliches Spektrum aufwiesen. Diese Nebel stellten sich später als Galaxien heraus. Als Huggins jedoch den Katzenaugennebel beobachtete, fand er ein ganz anderes Spektrum vor. Dieses war nicht kontinuierlich mit ein paar Absorptionslinien, sondern wies lediglich einige Emissionslinien auf. Die hellste Linie hatte eine Wellenlänge von 500,7 Nanometern. Dies stand in keinem Zusammenhang mit irgendeinem bekannten chemischen Element. Zunächst wurde daher angenommen, es handle sich um ein unbekanntes Element, das daraufhin den Namen Nebulium erhielt. 

1868 hatte man bei der Untersuchung des Spektrums der Sonne das bis dahin unbekannte Element Helium entdeckt. Obwohl man bereits kurz nach dieser Entdeckung das Helium auch in der Erdatmosphäre nachweisen und isolieren konnte, fand man Nebulium nicht. Anfang des 20. Jahrhunderts schlug Henry Norris Russell vor, dass es sich nicht um ein neues Element handele, das die Wellenlänge 500,7 nm hervorrief, sondern eher ein bekanntes Element in unbekannten Verhältnissen.

In den 1920er Jahren wiesen Physiker nach, dass das Gas in den planetarischen Nebeln eine extrem niedrige Dichte besitzt. Elektronen können in den Atomen und Ionen metastabile Energieniveaus erreichen, die sonst bei höheren Dichten durch die ständigen Kollisionen nur kurzzeitig existieren können. Elektronenübergänge im Sauerstoff führen zu einer Emission bei 500,7 nm. Spektrallinien, die nur in Gasen mit sehr niedrigen Dichten beobachtet werden können, werden verbotene Linien genannt. 

Bis Anfang des 20. Jahrhunderts ging man davon aus, dass planetarische Nebel die Vorstufen von Sternen darstellen. Man glaubte, dass sich die Nebel unter der eigenen Schwerkraft zusammenzögen und im Zentrum einen Stern bildeten. Spätere spektroskopische Untersuchungen zeigten jedoch, dass sich alle planetarischen Nebel ausdehnen. So fand man heraus, dass die Nebel die abgestoßenen äußeren Schichten eines sterbenden Sterns darstellen, der als sehr heißes, aber lichtschwaches Objekt im Zentrum zurückbleibt.

Gegen Ende des 20. Jahrhunderts half die fortschreitende Technik, die Entwicklung der planetarischen Nebel besser zu verstehen. Durch Weltraumteleskope konnten Astronomen auch emittierte elektromagnetische Strahlung außerhalb des sichtbaren Spektrums untersuchen, das wegen der Erdatmosphäre durch bodengebundene Observatorien nicht beobachtet werden kann. Durch Beobachtung auch der infraroten und ultravioletten Strahlungsanteile der planetarischen Nebel kann man deren Temperatur, Dichte und chemische Zusammensetzung viel genauer bestimmen. Mit Hilfe von CCD-Techniken lassen sich die Spektrallinien viel präziser bestimmen und auch extrem schwache Linien sichtbar machen. Planetarische Nebel, die in bodengebundenen Teleskopen einfache und regelmäßige Strukturen aufwiesen, zeigten infolge der hohen Auflösung des Hubble-Weltraumteleskops sehr komplexe Gestalten.

Planetarische Nebel stellen das Endstadium eines durchschnittlichen Sterns wie unserer Sonne dar.

Ein typischer Stern weist weniger als die doppelte Sonnenmasse auf. Seine Energie wird im Kern erzeugt, in dem die Kernfusion von Wasserstoff zu Helium abläuft. Der dadurch entstehende Strahlungsdruck verhindert, dass der Stern durch seine eigene Gravitation kollabiert. Es stellt sich ein stabiler Zustand ein, der über Milliarden von Jahren andauern kann.

Nach mehreren Milliarden Jahren sind die Wasserstoffvorräte im Kern verbraucht. Der Strahlungsdruck lässt nach, der Kern wird durch die Gravitationskräfte komprimiert und heizt sich auf. Die Temperatur im Kern steigt in dieser Phase von 15 Millionen auf 100 Millionen K an. Im Kern fusioniert nun Helium zu Kohlenstoff und Sauerstoff, in der „Schale“ um den Kern fusioniert Wasserstoff zu Helium. Als Folge dehnt sich die Hülle des Sterns stark aus, er tritt in das Stadium eines Roten Riesen auf dem asymptotischen Riesenast ein.

Die genannte Heliumfusion ist sehr temperaturempfindlich: die Reaktionsgeschwindigkeit ist proportional zur 30. Potenz der Temperatur und verdoppelt sich daher bei einer Temperaturerhöhung von nur 2,3 %. Dies macht den Stern sehr instabil – eine kleine Erhöhung der Temperatur führt sofort zu einem erheblichen Anstieg der Reaktionsgeschwindigkeit, die erhebliche Energien freisetzt, wodurch die Temperatur weiter ansteigt. Die Schichten, in denen die Heliumfusion gerade stattfindet, dehnen sich mit hoher Geschwindigkeit aus und kühlen sich dadurch wieder ab, wodurch die Reaktionsrate wieder herabgesetzt wird. Die Folge ist eine starke Pulsation, die manchmal stark genug ist, um die ganze Sternatmosphäre in den Weltraum zu schleudern.

Das Gas der Sternhülle dehnt sich anfangs mit einer Geschwindigkeit von 20 bis 40 Kilometern pro Sekunde aus und besitzt eine Temperatur von etwa 10.000 K. Dieser vergleichsweise langsame Sternwind bildet die Hauptmasse des Nebels. In dem Maße, in dem der Stern nach und nach seine äußeren Hüllen verliert und den immer heißeren Kern freilegt, wechselt seine Farbe von orange über gelb bis hin zu weiß und schließlich blau – ein sichtbares Zeichen dafür, dass seine Oberflächentemperatur auf über 25.000 K ansteigt. Wenn die freigelegte Oberfläche rund 30.000 K heiß ist, werden genug hochenergetische ultraviolette Photonen ausgesendet, um das zuvor ausgeworfene Gas zu ionisieren. Die Gashülle wird dadurch als planetarischer Nebel sichtbar. Der Stern im Zentrum hat das Stadium eines Weißen Zwerges erreicht.

Die ausgestoßenen Gase der planetarischen Nebel bewegen sich mit einer Geschwindigkeit von einigen Kilometern pro Sekunde vom Zentrum weg. Der Sternwind flaut im Laufe der Zeit vollständig ab, und beim Gas tritt eine Rekombination ein, wodurch es unsichtbar wird. Für die meisten planetarischen Nebel beträgt die Zeitspanne zwischen Formation und Rekombination ungefähr 10.000 Jahre.

Planetarische Nebel spielen eine wichtige Rolle in der Entwicklung einer Galaxie. Das frühe Universum bestand fast vollständig aus Wasserstoff und Helium. Erst durch die in den Sternen ablaufende Nukleosynthese wurden die schwereren Elemente erzeugt, die in der Astrophysik auch Metalle genannt werden.
Planetarische Nebel bestehen zu nennenswerten Teilen auch aus Elementen wie Kohlenstoff, Stickstoff und Sauerstoff, mit denen sie die interstellare Materie anreichern. 

Nachfolgende Sterngenerationen bestehen zu einem geringen Anteil aus diesen schwereren Elementen, der einen Einfluss auf die Sternentwicklung hat. Die Planeten bestehen zu einem großen Teil aus schweren Elementen. 

Neben den planetarischen Nebeln stoßen auch Supernovaexplosionen in der Endphase massereicher Sterne schwere Elemente aus.

Typische planetarische Nebel bestehen zu etwa 70 % Wasserstoff und 28 % Helium. Den restlichen Anteil bilden hauptsächlich Kohlenstoff, Stickstoff und Sauerstoff sowie Spuren anderer Elemente. 

Sie haben einen Durchmesser von rund einem Lichtjahr und bestehen aus extrem verdünntem Gas mit einer Dichte von rund 1.000 Teilchen pro Kubikzentimeter. Die höchste Dichte besitzen „junge“ planetarische Nebel mit bis zu einer Million Teilchen pro Kubikzentimeter. Im Laufe der Zeit führt die Ausdehnung des Nebels zur Verringerung seiner Dichte.

Planetarische Nebel werden entweder als „materiebegrenzt“ oder als „strahlungsbegrenzt“ beschrieben:

Nebel, die Regionen aus ionisiertem Wasserstoff enthalten, nennt man Emissionsnebel. Sie bestehen zum größten Teil aus einem Plasma, in dem ionisierter Wasserstoff (Protonen) und freie Elektronen vorkommen. Anders als bei einem „einfachen“ Gas erhält der Nebel durch das Plasma charakteristische Eigenschaften wie Magnetfeld, Plasma-Doppelschichten, Synchrotronstrahlung und Plasmainstabilitäten.

Derzeit sind in unserer Galaxis, die aus etwa 200 Milliarden Sternen besteht, rund 1.500 planetarische Nebel bekannt. Diese geringe Anzahl wird verständlich, wenn man die sehr kurze Existenz der Nebel in Relation zum gesamten „Sternleben“ sieht. Man findet planetarische Nebel meist um die Ebene der Milchstraße, mit der größten Konzentration im Galaktischen Zentrum. Es sind nur ein oder zwei planetarische Nebel in Sternhaufen bekannt. 

Vor kurzem hat eine systematische fotografische Durchmusterung des Himmels die Anzahl der bekannten planetarischen Nebel drastisch erhöht. Obwohl CCDs den chemischen Film in der modernen Astronomie bereits ersetzt haben, wurde dabei ein "Kodak-Technical-Pan"-Film eingesetzt. In Kombination mit einem speziellen Filter für die Isolation der typischen Wasserstofflinien, die in allen planetarischen Nebeln vorkommen, konnten auch sehr lichtschwache Objekte nachgewiesen werden.

Im Allgemeinen haben planetarische Nebel eine symmetrische und ungefähr sphärische Gestalt, es existieren jedoch auch sehr unterschiedliche und komplexe Formen. Ungefähr 10 % sind stark bipolar ausgeprägt, einige sind asymmetrisch; der Retina-Nebel ist – von der Erde aus gesehen – sogar rechteckig.

Die Ursachen der extremen Formenvielfalt sind bislang nicht genau bekannt und werden kontrovers diskutiert. Gravitationswirkungen von Begleitersternen könnten zur Formgebung beitragen. Eine andere Möglichkeit wäre, dass massereiche Planeten den Materiefluss stören, wenn sich der Nebel ausformt. Im Januar 2005 wurde erstmals ein Magnetfeld um die zentralen Sterne zweier planetarischer Nebel gefunden. Man nimmt an, dass dieses Feld teilweise oder vollständig für die außergewöhnliche Struktur verantwortlich ist.

2011 fotografierte das Hubble-Weltraumteleskop einen planetarischen Nebel im Sternbild Pfeil, den ‚Necklace Nebula’ (‚Halsband-Nebel’), der aus einem Ring leuchtender Gasknoten besteht, die wie Diamanten eines Colliers funkeln.

Ein Hauptproblem bei der Erforschung planetarischer Nebel besteht darin, dass man ihre Entfernung nur schwer bestimmen kann. Bei relativ nahen planetarischen Nebeln ist die Entfernungsbestimmung mit Hilfe der Parallaxe möglich. Wegen der geringen Zahl naher Nebel und der Formenvielfalt ist daraus allerdings kein Vergleichsmaßstab ableitbar. Bei bekannter Entfernung lässt sich durch jahrelange Beobachtungen die Expansionsgeschwindigkeit des Nebels senkrecht zur Beobachtungsrichtung bestimmen. Durch die spektroskopische Untersuchung des Dopplereffekts erhält man auch die Expansionsgeschwindigkeit in Beobachtungsrichtung. 

Der Anteil der schweren Elemente in planetarischen Nebeln kann durch zwei Methoden bestimmt werden, wobei die Ergebnisse manchmal stark voneinander abweichen. Einige Astronomen meinen, dies könnte auf Temperaturschwankungen innerhalb der Nebel zurückzuführen sein. Andere sind hingegen der Auffassung, dass die Unterschiede zu groß sind, um sie mit diesem Temperatureffekt zu erklären und führen die Abweichungen auf kalte Gebiete mit sehr wenig Wasserstoff zurück. Solche Gebiete sind jedoch bisher nicht beobachtet worden.





</doc>
<doc id="4037" url="https://de.wikipedia.org/wiki?curid=4037" title="Psychoanalytische Filmtheorie">
Psychoanalytische Filmtheorie

Als Psychoanalytische Filmtheorie bezeichnet man eine Strömung der Filmwissenschaft bzw. Filmtheorie, welche die Methode der Psychoanalyse auf das Phänomen des Films und des Kinos anwendet.

Mitte der 1970er Jahre entwickelte sich, ausgehend von Frankreich, eine theoretische Auseinandersetzung mit dem Medium Kino, dessen Grundlage eine Mischung aus Psychoanalyse, Semiotik, Strukturalismus und Marxismus bildete. Im Mittelpunkt dieser filmtheoretischen Debatte stand das Zuschauersubjekt sowie dessen Beziehung zum Kino. Ausgangsbasis bildeten die Überlegungen des französischen Theoretikers Jean Louis Baudry sowie die filmtheoretischen Schriften von Christian Metz, dessen "Le signifiant imaginaire. Psychoanalyse et cinéma" (1977, dt.: "Der imaginäre Signifikant. Psychoanalyse und Kino") die Diskussion erst richtig eröffnete. Metz unternimmt den Versuch, psychoanalytische Termini – insbesondere der Theorie Jacques Lacans – auf den Bereich der Kinematographie zu übertragen.

Psychoanalytische Filmtheorie versucht in erster Linie herauszuarbeiten, wie das Unbewusste die Rezeption von Filmgeschehen unterstützt, bzw. wie Film und Kino unbewusste, irrationale Prozesse beim Betrachter auslösen und Filmschauen so zu einer lustvollen Erfahrung werden lassen. Wenn der Film, wie seit jeher behauptet, in Nähe des Traumes gerückt werden kann, so muss es möglich sein, ihm mit Mitteln der Psychoanalyse (analog einer Traumdeutung) beizukommen. 

Christian Metz umschreibt die Fragestellung psychoanalytischer Filmtheorie folgendermaßen: „Welchen Beitrag kann die Freudsche Psychoanalyse zur Erkenntnis über den kinematographischen Signifikanten leisten?“ Dass es sich dabei um einen Forschungsbereich handelt, der auch ohne die Analyse einzelner Filme oder Genres auszukommt, wird schnell deutlich. Ziel der Auseinandersetzung bleibt es, Aussagen über den Gesamtapparat Kino und seiner Anordnung zu treffen, wobei das Zuschauersubjekt stets im Mittelpunkt der Beschäftigung steht. Entsprechend war es niemals die Aufgabe psychoanalytischer Filmtheorie herauszuarbeiten, wie das Unbewusste filmisch sichtbar gemacht werden kann (etwa durch Traumsequenzen, Darstellung von Visionen, Rückblenden etc.). Ebenso wenig handelt es sich um die Auseinandersetzung mit Filmen, die die Darstellung psychoanalytischer Probleme oder die der Psychoanalyse selbst zum Thema haben.

Die Rolle des Zuschauers soll also entscheidend sein, nicht die Rolle einzelner Schauspieler oder Autoren. Eine solche neue psychoanalytische Methode, die sich seit Mitte der 1970er Jahre von Frankreich ausgehend zu entwickeln begann, konnte nicht länger den Film isoliert betrachten, sondern musste das gesamte Umfeld Kino miteinbeziehen. Dabei stießen die neuen Theoretiker vor allem auf zwei bestehende Strömungen früherer filmtheoretischer Auseinandersetzung, die sie um die Frage nach dem filmisch Unbewussten ergänzten: Zum einen hat die Realismusdiskussion um André Bazin, die davon ausgeht, dass die Leinwand als Fenster zur Welt fungiert, das die Objekte und den Raum außerhalb der Leinwand bereits impliziert, nichts von ihrer Aktualität verloren. Zum anderen gibt es die formalistische Position von Eisenstein und Rudolf Arnheim, die die Leinwand durch ihre Rahmung begrenzt sehen, wobei diese Begrenzung das auf der Leinwand sichtbare Bild formt und positioniert. 

Jean Mitry führt beide Metaphern wieder zusammen, indem er dem Kino sowohl den Status des Fensters als auch den der Rahmung zubilligt. Durch Hinzuziehen einer weiteren Metapher entwickelt sich schließlich die neue Theorie: Die Leinwand wird nun als Spiegel aufgefasst. Die Begriffe Realität (Fenster) und Kunst (Rahmung) werden so mit der Frage nach dem Zuschauer verknüpft, der Diskurs der Psychoanalyse und der Begriff des Unbewussten in die Diskussion eingebracht. Es geht also um mehr als nur um Film – zwei Systeme werden miteinander in Beziehung gesetzt: das Kino und die Psyche.

Jean Louis Baudry nimmt an, dass die Blickbeziehung des Zuschauers zum Bild so wie die Zentralperspektive in der Malerei Kontrolle und Allmacht suggeriert. Das erweist sich jedoch als Selbsttäuschung, da tatsächlich der „Apparat“ das Bild erschafft.

Die "klassische psychoanalytische Filmtheorie" begann bereits 1916 mit der Veröffentlichung von Hugo Münsterbergs Studie "Das Lichtspiel". Sie bedient sich vor allem der Konzepte Sigmund Freuds, als das wichtigste Konzept sei hier der das unbewusste Verarbeiten ödipaler oder narzisstischer Strukturen genannt.

Daneben gibt es seit den 1970er Jahren die auf den Theorien des französischen Psychoanalytikers Jacques Lacan aufbauenden Ansätze, die von Filmwissenschaftlern wie Laura Mulvey und Christian Metz aufgegriffen wurden. Wichtige Aspekte sind hierbei Identifikation und Symbolik sowie Lacans auf dem Konzept des Spiegelstadiums beruhende Konzeption des Imaginären. 

Auffallend bei der Beschäftigung mit psychoanalytischer Filmtheorie ist die nahezu ausschließliche Beschäftigung mit Filmen des klassischen narrativen Erzählkinos (besonders beliebt sind hierbei die Filme Alfred Hitchcocks), während Überlegungen etwa zum Avantgarde- und Experimentalfilm meist völlig unberücksichtigt bleiben.





</doc>
<doc id="4039" url="https://de.wikipedia.org/wiki?curid=4039" title="Paella">
Paella

Eine Paella [] ist ein spanisches Reisgericht aus der Pfanne und das Nationalgericht der Region Valencia und der spanischen Ostküste. Der Begriff "paella" kommt aus dem Katalanischen (um 1892) und hat seinen Ursprung im lateinischen Wort "patella" (eine Art große Platte oder flache Schüssel aus Metall). Um 1900 adaptierten die Valencianer das Wort "Paella" für die Metallpfanne, in der ihr Nationalgericht zubereitet wird.
Inzwischen wird der Begriff "paella" im Spanischen für das Gericht "und" für die Pfanne verwendet, die ansonsten auch als "paellera" [pae'ʎeɾa] bezeichnet wird.

Die Paella ist ein traditionell valencianisches Gericht. Obwohl die spanische Küche sehr regional geprägt ist, hat die Paella in ganz Spanien einen sehr hohen Bekanntheitsgrad. Aufgrund dieser regionalen Prägung werden jedoch in den verschiedenen Regionen Spaniens mehrere von der traditionellen "Paella Valenciana" abweichende Varianten zubereitet.

"Paellas" sind große, kreisrunde und sehr flache Pfannen. In spanischen Haushalten sind oft mehrere "paellas" mit unterschiedlichen Durchmessern vorhanden, da die Pfanne so bemessen werden sollte, dass der Reis nicht mehr als vier Zentimeter hoch steht.

Zum traditionellen Zubereiten von "Reis aus der paella" auf dem Holzfeuer werden meist Paella-pfannen aus Schmiedeeisen oder Gusseisen verwendet, in Katalonien gelegentlich auch Tonpfannen. Paellas aus emailliertem Stahl, Edelstahl oder teflonbeschichtetem Aluminium sind für den privaten Gebrauch erhältlich.

"Paellas" aus Schmiedeeisen bzw. aus dickem, gepresstem Blech sind nicht spülmaschinengeeignet. Sie können rosten, wenn sie beim Spülen zu stark entfettet und nicht umgehend getrocknet werden. Nach dem Abwaschen werden die getrockneten Pfannen mit Olivenöl eingerieben. Rostige Pfannen reinigt man mit Natron, das man mit einer halben Zitrone in die Paellapfanne einreibt. Die einfache und traditionelle Methode ist das Reinigen mit trockenem Sand.

Die "Paella Valenciana" besteht aus Reis, der mit Safran leuchtend gelb gefärbt wird, Gemüse und hellem Fleisch (Huhn, Kaninchen). Das Fleisch wird mit Knochen in mundgerechte Stücke gehackt. Gelegentlich werden auch kleine Schnecken beigegeben. Als Gemüse werden meist zerkleinerte Tomaten, Judías verdes (grüne breite Bohnen) sowie "garrofó" (große, flache weiße Bohnen) verwendet. Gewürzt wird mit Salz, gelegentlich mit fein geriebenem frischen Knoblauch und eben Safran. (Frische) Rosmarinzweige, die während der letzten Minuten auf den Reis gelegt und gegebenenfalls auch dem Feuer beigegeben werden, geben einen zusätzlichen mediterranen Geschmack.

Traditionell wurden neben weißen Gartenbohnen auch Feuerbohnen sowie als weitere Zutaten Aal, Fleisch von Westschermaus und Ente, Paprika und besonders im Winter auch Artischocken verwendet. Paellas mit Meeresfrüchten werden "Paella marisco" genannt.

Heutzutage wird die gelbe Farbe des Paellareis meist durch zwei Gewürze erreicht. Zum einen durch das traditionelle Azafran (Safran) und zum anderen durch Zugabe von Colorante (ein Lebensmittelfarbpulver auf Ringelblumenbasis), welches fast keinen Eigengeschmack besitzt.

"Siehe auch" Valencianische Küche

Es wird reichlich Olivenöl in der Pfanne erhitzt. Wenn Meeresfrüchte dabei sind, werden diese als erstes angebraten und wieder herausgenommen. Danach wird das Fleisch 10 Minuten angebraten, bis es fast gar ist. Dann kommen die ganzen, ungeschälten Knoblauchzehen dazu. Nun wird die ganze, ungeschälte Tomate auf einer Gemüsereibe über das Fleisch gerieben und die grünen Bohnen, die Streifen roten Paprikas und die "garrofones" zugegeben. Das Gemüse wird circa 10 Minuten in der Pfanne mitgeschmort. Dann kommt der Reis hinzu. Man vermischt ihn mit den anderen Zutaten und gießt dann Wasser oder "caldo", Brühe darüber. Ab jetzt darf die Paella nicht mehr umgerührt werden. Das Wasser und die Hitze des Feuers müssen so bemessen sein, dass das Wasser völlig vom Reis aufgesaugt wurde, wenn die Paella fertig ist. Man darf kein Wasser mehr hinzugießen. Ist die Paella fertig gekocht, lässt man sie acht bis zehn Minuten zugedeckt ruhen, am besten mit einem Küchentuch bedeckt. Vor dem Essen wird die Paella mit Zitronensaft beträufelt. Dazu gibt es Weißbrot.

Traditionell wird eine Paella auf einem offenen, vorwiegend im Freien gemachten Pinien- oder Mandelholz-Feuer am Wochenende für die ganze Familie zubereitet. Die Paellapfanne ruht auf einem eisernen "tripote", Dreifuß, der direkt im Holzfeuer steht. Diese Tradition erfreut sich – besonders in der Region von Valencia – nach wie vor großer Beliebtheit. Traditionell wird die Paella direkt aus der Pfanne mit dem Löffel, selten noch Holzlöffeln, gegessen.

Fälschlich wird die Paella oft als spanisches Nationalgericht bezeichnet. Das ist nicht richtig, denn die Paella ist ein traditionelles Gericht aus der Region Valencia. Die oftmals angebotene "Paella mixta" (Paella mit Meeresfrüchten "und" Fleisch) ist eine neuere Art der Paella Valenciana, die von Valenzianern gern als "Paella de turistas" (Touristen-Paella) bezeichnet wird.

Oft wird für Paellas kein Safran verwendet, obwohl Safran in Spanien nicht sehr teuer ist, sondern es wird mit Lebensmittelfarbstoffen wie E-102 Tartrazin gefärbt. Dieser Farbstoff ist unter dem Namen „Colorante alimentario“ im Handel. Es gibt aber auch geeignete, preiswerte Paellagewürzmischungen mit Safran.

Es gibt zahlreiche Arten, eine Paella zuzubereiten. Einig sind sich alle Paella-Köche, dass man sie als Mittagessen isst (gegen 15 Uhr) und niemals abends, weil sie zu schwer im Magen liegt. Einig ist man sich auch darüber, dass man eine Paella mit Paellareis, am besten arroz bomba zubereitet, der in der Gegend um Valencia angebaut wird. Langkornreis stellt einen adäquaten Ersatz dar. Risottoreis eignet sich dagegen nicht für eine Paella, da er sehr stärkehaltig ist und im Gegensatz zum Paellareis beim Kochen sehr weich und klebrig wird. Die Reiskörner in der Paella sollten aber trocken, körnig und nicht verklebt sein.

Einige Köche lassen den Reis in Kreuzform über das angebratene Fleisch und Gemüse rieseln und gießen dann Wasser oder Brühe hinzu. Andere bedecken die fertige Paella nicht nur mit einer Zeitung, sondern auch mit Rosmarinzweigen, manche wiederum gar nicht. Einig wiederum sind sich alle, dass kein Wasser nachgegossen werden darf. Manche gießen einen Schuss Bier oder Rotwein hinzu. Entscheidend für das Gelingen einer Paella ist der Reis. Paellareis "arroz bomba" benötigt etwa die doppelte bis zweieinhalbfache Volumenmenge an Wasser, also für eine Tasse Reiskörner zwei Tassen Wasser. Wenn das Wasser kocht, sollte die Hitze verringert werden, damit das Wasser nicht verkocht, ehe der Reis gar ist. Im Normalfall wird die Paella nicht umgerührt. Fertig ist die Paella, wenn das Wasser verdampft ist und der Reis gar, aber noch bissfest ist. Damit sich die geschätzte schmackhafte feine Kruste, "socarrat", am Boden der Paellaform bildet, sollte sie noch einige Minuten weiter garen, bis die untere Lage der Reiskörner an der Pfanne anzubacken beginnt.

Am 8. März 1992 wurde in Valencia eine Paella für 100.000 Personen zubereitet. Es wurden folgende Zutaten verwendet: 5.000 kg Reis, 6.800 kg Hähnchen, 5.000 kg Kaninchen, 400 kg Ente, 1.000 kg Schnecken, 2.400 kg Grüne Bohnen, 1.400 kg Weiße Dicke Bohnen, 1.000 kg Tomaten, 150 kg Salz, 5 kg Paprikapulver (mild), 1 kg Colorante, 1.000 Liter Öl und 12.500 Liter Wasser. Die Pfanne hatte einen Durchmesser von zwanzig Metern, ein Fassungsvermögen von 210.000 Litern, eine Blechstärke von acht Millimetern und wies insgesamt 4.000 Meter Schweißnähte auf; das Gesamtgewicht der gefüllten Paellera betrug etwa 30.000 Kilogramm und wurde von 90 Stützpfeilern getragen. Diese Paella wurde in das Guinness-Buch der Rekorde aufgenommen.

Die Paella ist eine Variante von unzähligen Reisgerichten ("arroces"), die es in der Region Valencia gibt (die erste Form ist auf Spanisch und die zweite auf "valenciano"):



</doc>
<doc id="4041" url="https://de.wikipedia.org/wiki?curid=4041" title="Pfund">
Pfund

Das Pfund (Abkürzung Pfd., "Pf." bzw. lb; mhd. und ahd. "pfunt, phunt," entlehnt von in "libra pondo" ‚[römisches] Pfund an Gewicht‘, zu "pondus" ‚Gewicht‘, ‚Last‘, ‚Masse‘) ist eine alte – nach Zeit und Ort verschieden bestimmte – Maßeinheit für das Gewicht.

Im heutigen Sprachgebrauch entspricht ein Pfund der Masse eines halben Kilogramms und ist der hundertste Teil eines Zentners zu 50 kg. Die historischen Maße weichen hiervon ab, ebenso das als „englisches Pfund“ im bestehenden angloamerikanischen Maßsystem gebräuchliche "pound" zu annähernd 454 Gramm wie auch das Apothekergewicht "troy pound" zu annähernd 373 Gramm. 
In Deutschland versteht man noch heute das Pfund entsprechend der Definition des ehemaligen Zollvereins von 1858 als 500 Gramm. Besonders in Deutschland, weniger in der Schweiz, wird dieses an das metrische Maß angepasste Pfund nach wie vor in der Alltagssprache benutzt, vor allem bei Lebensmitteln (halbes Pfund, Viertelpfund), in Deutschland auch für das Körpergewicht. Dieses Pfund ist allerdings weder SI-Einheit noch nach dem deutschen „Einheiten- und Zeitgesetz“ im geschäftlichen und amtlichen Verkehr zulässig.

In Österreich ist der Ausdruck "Pfund" wenig gebräuchlich; stattdessen sind hier Kilogramm und Dekagramm (10 Gramm) – umgangssprachlich "Kilo" und "Deka" – üblich.

Das englische Pfund mit der Bezeichnung "pound" ist eine Einheit des angloamerikanischen Maßsystems. Diese Maßeinheit entspricht

Davon zu unterscheiden ist das "troy pound," eine Maßeinheit für die pharmazeutische Feinabwägung,
Die Abkürzung "lb" (zu ‚Waage‘, ‚Pfund‘), gleichbedeutend mit "lb" oder "lbm", erscheint in den Vereinigten Staaten von Amerika häufig in der Pluralform als "lbs" (1 "lb", 2 "lbs" usw). Für Rohrleitungen nach US-amerikanischem Standard wird die Druckfestigkeit mit "psi" oder "lbs" angegeben, womit "lbs per square inch" gemeint sind, genauer "Pound-force per square inch". Auch der Reifendruck wird so angegeben.

Das Pfund des karolingischen Reichs geht zurück auf die altrömische "libra" (worauf auch die Kurzzeichen "Lb., lb., lb, ℔." oder "℔" [codice_1] bezogen sind). Das römische Gewichtsmaß "libra", gegen das im Gleichgewicht abgewogen wurde, entsprach als "as" dem Gegengewicht von 12 Unzen ("unciae"). Es maß mit einer Masse von ungefähr 327 g (in der Literatur werden Werte zwischen 322,6 g und 328,9 g angenommen). Die altrömische "mina" entsprach dagegen 16 Unzen, war also um ein Drittel schwerer (etwa 436 g). Unter Karl dem Großen wurde die für das Gewicht übliche Maßeinheit neu festgelegt. Das Karlspfund "(pondus Caroli)" betrug etwa 406,5 Gramm.

Im Mittelalter war das Pfund als Gewichtsmaß in ganz Europa verbreitet, sein Gewicht wich jedoch oft von Stadt zu Stadt ab. Hatte das Pfund in Nürnberg gut 510 Gramm, so waren es in Würzburg 480 Gramm und in Berlin nur etwa 467 Gramm.

Als „Krämergewicht“ war 1 Pfund = 16 Unzen = 32 Lot = 128 Quentchen = 512 Pfenniggewichte = 1024 Hellergewichte (oder 1 : 16 : 2 : 4 : 4 : 2).

Als Apothekergewicht in Medizin und Pharmazie konnte sich das "Nürnberger Apotheker-Pfund" zu etwa 358 g durchsetzen, in der Unterteilung 1 lb = 12 Unzen = 96 Drachmen (bzw. Quintlein) = 288 Skrupel = 576 Oboloi = 5760 Gran (oder 1 : 12 : 8 : 3 : 2 : 10).

In den österreichischen Kronländern galt hierfür das "Wiener Medizinal-Pfund" zu etwa 420 g, mit der Unterteilung in 12 Unzen. Eine Unze entsprach 2 Lot und 1 Lot entsprach 4 Quintlein bzw. Drachmen. Drei Skrupel entsprachen einem Quintlein. Die kleinste Einheit war das Gran, wobei ein Gran etwa gleich schwer war wie ein Pfefferkorn, umgerechnet etwa 0,073 Gramm. Zwanzig Gran entsprachen einem Skrupel. Ein normales Wiener Handelspfund entsprach etwa 560 Gramm.

Seit etwa der Mitte des 19. Jahrhunderts sind auch in der pharmazeutischen Industrie dezimal-metrische Einheiten üblich (für das britische bzw. amerikanische Apotheker-Pfund "troy pound" siehe Apothekergewicht).

Als Münzgewicht entsprach ein Pfund früher zwei Mark (Gewicht).

1854 legte der Deutsche Zollverein das Pfund (Zollpfund) auf exakt 500 Gramm fest, womit es etwa sieben Prozent schwerer war als die abgelöste Einheit. Diese Definition war schon zuvor in einigen süddeutschen Staaten (vgl. Alte Maße und Gewichte (Baden), Alte Maße und Gewichte (Hessen)) sowie in der Schweiz (vgl. Alte Masse und Gewichte (Schweiz)) gültig und wurde 1858 beispielsweise auch in Berlin eingeführt. Der Zentner entsprach in der Folge 50 Kilogramm.


Das heutige umgangssprachliche Pfund geht mit seinen 500 Gramm auf das Zollpfund zurück.

Mitte des 19. Jahrhunderts waren regional unterschiedliche Gewichtsmaße in Gebrauch, die jeweils als „Pfund“ bezeichnet wurden:
Pfund war auch ein Stückmaß. Es bezeichnete an einigen Orten eine Menge von 8 oder 240.
Das Stückmaß war in Regensburg gleichzeitig ein Salzmaß.

Das Pfund als Flächenmaß war in Österreich um Wien verbreitet. Es ist in den Einträgen der Grundbücher nachweisbar.

Im übertragenen Sinne bedeutet "Pfund" die Fähigkeit oder Begabung eines Menschen (in Anspielung auf das biblische Gleichnis von den anvertrauten Talenten (oft mit "… Pfunden" übersetzt) in Matthäus 25,18). Auch der Ausdruck "Talent" bezeichnet sowohl eine bestimmte Menge Silber als auch eine Begabung.

Abgeleitet von "Pfund" war um 1900 das Wort "pfundig" ein Modeausdruck der Jugendsprache. In einigen Teilen Bayerns hat sich der Ausdruck im Sinne von „toll, großartig“ erhalten. Überregional hat sich im deutschen Sprachgebrauch bis heute auch der Ausdruck "Pfundskerl" (ein anständiger, liebenswerter, lieber, tüchtiger Mensch) gehalten. In der Sprache der US-amerikanischen Amischen ist der „Pfunder“ eine Führungskraft und wird im Englischen öfters mit „founder“ (Gründer) verwechselt. Der Bedeutungshintergrund des Wortes ist jedoch auch hier der biblische Gebrauch von Talent.



</doc>
<doc id="4049" url="https://de.wikipedia.org/wiki?curid=4049" title="Pocken">
Pocken

Mit Pocken (auch "Blattern"; ) bezeichnet man eine für den Menschen gefährliche Infektionskrankheit, die von Pockenviren (Orthopoxvirus variolae) verursacht wird. Durch ihre hohe Infektiosität und Mortalität gehört die Erkrankung zu den gefährlichsten des Menschen.

Seit den letzten Erkrankungen im Jahr 1977 in Somalia sind keine neuen Pockenfälle mehr aufgetreten; der letzte Fall in Deutschland trat im Jahr 1972 auf. Durch ein konsequentes Impf- und Bekämpfungsprogramm der WHO und anderer Gesundheitsorganisationen wurde erreicht, dass im Jahr 1980 die Welt von der WHO für pockenfrei erklärt werden konnte, weil der Erreger nur durch den Menschen weitergegeben werden kann. 

Der Name „Pocken“ kommt zum ersten Mal in einer angelsächsischen Handschrift aus dem 9. Jahrhundert am Ende eines Gebets vor: "... geskyldath me wih de lathan Poccas and with ealleyfeln. Amen." („... beschützt mich vor den scheußlichen Pocken und allem Übel. Amen.“) Das Wort "Pocken" kommt aus dem Germanischen und bedeutet „Beutel“, „Tasche“, „Blase“, „Blatter“. Es ist mit den , , und verwandt.

Die Bezeichnung "variola" (von lat. ‚bunt‘, ‚scheckig‘, ‚fleckig‘) wurde von Bischof Marius von Avenches (heute Schweiz) um 571 n. Chr. geprägt und soll im 11. Jahrhundert der Krankheit auch von dem Arzt und Übersetzer Constantinus Africanus gegeben worden sein.

Die Erreger der Pocken beim Menschen sind Viren aus der Gattung "Orthopoxvirus". Pockenviren sind mit 200 bis 400 nm die größten bekannten animalen Viren. Diese Viren sind auch hinsichtlich ihrer DNA-Replikation im Cytoplasma im Vergleich zu anderen DNA-Viren ungewöhnlich. Eine mit einem Pockenvirus infizierte Zelle weist eine DNA-Synthese außerhalb des Kerns auf, was sonst nur in intrazellulären Organellen wie Mitochondrien (und bei Pflanzen in Chloroplasten) oder bei der Reifung des Hepatitis-B-Virus innerhalb des Kapsids im Cytoplasma vorkommt. Für Pockenviren ist eine zweite Membranstruktur innerhalb des Virions charakteristisch, die innerhalb der Virushülle liegt und während des Zusammenbaus des intrazellulären Viruspartikles "de novo" um das Kapsid herum synthetisiert wird.

Neben den Pockenerkrankungen des Menschen gibt es auch bei einer Reihe von Tieren durch verwandte Viren ausgelöste Erkrankungen. Die ebenfalls durch "Orthopox"-Viren hervorgerufenen Tierpocken – die sogenannten „Säugerpocken“ wie Kuhpocken, Affenpocken, Katzen-, Kamel- und Mäusepocken – sind mit Ausnahme der Mäusepocken prinzipiell auch für den Menschen pathogen. Sie sind also Zoonosen und daher meldepflichtige Tierkrankheiten, lösen aber meist nur leichte Erkrankungen aus. Die übrigen durch Pockenviren hervorgerufenen Tierkrankheiten wie Schweine-, Schaf- und Ziegen-, Euter- und Vogelpocken, Myxomatose, Kaninchenfibromatose, Stomatitis papulosa der Rinder sind dagegen streng wirtsspezifisch und für den Menschen ungefährlich.

Von besonderer Bedeutung ist der Erreger der Kuhpocken "Orthopoxvirus vaccinia," der mit dem Variolavirus eng verwandt ist, beim Menschen aber nur eine leichtere Krankheit auslöst. Dafür ist der Patient nach einer Ansteckung mit Kuhpocken gegen die echten Pocken immunisiert. Deshalb wurden Varianten von Vaccinia für die Pockenimpfung verwendet. Das Erregerreservoir stellen vermutlich Nagetiere dar, und ein wichtiger Überträger auf den Menschen sind Katzen (→ Katzenpocken). Bei Menschen mit geschwächtem Immunsystem (zum Beispiel durch AIDS oder eine hochdosierte Kortisonbehandlung) können durch Katzen übertragene Kuhpockeninfektionen auch tödlich enden.

Die Bläschenkrankheit der Schlangen wird fälschlicherweise auch als „Pocken“ oder „Wasserpocken“ bezeichnet. Sie ist jedoch keine Viruserkrankung, sondern eine bakterielle Hautentzündung infolge schlechter Haltungsbedingungen.

Pocken können direkt von Mensch zu Mensch durch Tröpfcheninfektion beim Husten übertragen werden. Daneben kann die Ansteckung auch durch Einatmen von Staub erfolgen, der z. B. beim Ausschütteln von Kleidung oder Decken von Pockenkranken entsteht.

Die Inkubationszeit beträgt eine bis zweieinhalb Wochen, meistens jedoch 12 bis 14 Tage. Bei Beginn der Erkrankung kommt es zu schwerem Krankheitsgefühl, Kopf- und Rückenschmerzen mit hohem Fieber und Schüttelfrost, ferner tritt ein Rachenkatarrh auf. Zu diesem Zeitpunkt ist der Patient hochinfektiös. Bei den Pocken ist ein biphasischer Fieberverlauf typisch: Nach 1–5 Tagen sinkt das Fieber und steigt nach einem Tag wieder an. Nun kommt es zu den typischen Hauterscheinungen.

Die Reihenfolge, in welcher die Hauterscheinungen (Effloreszenzen) auftreten, ist dabei typisch: Makula (Fleck) → Papel → Vesikel (Bläschen) → Pustel (Eiterbläschen) → Kruste. Die verschieden lokalisierten Hauteffloreszenzen sind, im Gegensatz zu den Effloreszenzen bei Windpocken, nacheinander alle im gleichen Stadium. Sie treten fast am gesamten Körper auf, wobei Kopf, Hände und Füße am stärksten, Brust, Bauch und Oberschenkel nur schwach betroffen sind. Ausgenommen sind die Achselhöhlen und Kniekehlen. Die eitrige Flüssigkeit in den Pusteln verbreitet einen sehr unangenehmen Geruch. Bei einem weniger schweren Krankheitsverlauf trocknen die Pusteln etwa zwei Wochen nach Ausbruch der Krankheit nach und nach ein und hinterlassen deutlich erkennbare Narben. In schwereren Fällen können Erblindung, Gehörlosigkeit, Lähmungen, Hirnschäden sowie Lungenentzündungen auftreten. Oft verläuft die Krankheit tödlich. Die geschätzte Letalität der unbehandelten Pocken liegt bei etwa 30 Prozent.

Gegen Pocken gibt es kein bekanntes Heilmittel, nur eine vorbeugende Impfung ist möglich; sie kann ihre Schutzwirkung auch noch entfalten, wenn sie bis etwa fünf Tage nach der Infektion vorgenommen wird. Die Pockenimpfung ist eine Lebendimpfung und ist durch eine Reihe von Impfkomplikationen belastet, so dass nur bei eindeutigen Pockenausbrüchen geimpft werden sollte. Eine Massenimpfung ist z. B. in den USA gar nicht vorgesehen – die dortigen Notfallpläne sehen nur eine Impfung der gefährdeten Personen vor. Zur Eindämmung der Erkrankung haben sich dagegen Quarantänemaßnahmen (Isolierung von Kranken und Krankheitsgebieten) bewährt.

Am 26. August 1807 wurde in Bayern als weltweit erstem Land eine Impfpflicht eingeführt. Baden und Preußen folgten 1815, Schweden 1816, England 1867 und das Deutsche Reich 1874. Im lutherischen Schweden hatte die protestantische Geistlichkeit bereits um 1800 eine Vorreiterrolle bei der freiwilligen Pockenschutzimpfung inne. In Liechtenstein war die erste Impfung 1801 durchgeführt worden, ab 1812 galt eine gesetzliche Impfpflicht.

Die ab 1967 weltweit von der Weltgesundheitsorganisation vorgeschriebene Impfpflicht gegen Pocken endete in Westdeutschland 1976. In Österreich endete die Impfpflicht 1981. Die Impfung wurde typischerweise mit einer Impfpistole oder Lanzette meist am Oberarm durchgeführt, wo sich an der Einritzstelle durch die resultierende, gewollte Infektion in der Regel eine Pustel und daraus schließlich eine rundliche vertiefte Impfnarbe bildete, die bis heute bei vielen geimpften Menschen zu sehen ist. In manchen Ländern war es in Zeiten der Impfpflicht bei der Einreise erforderlich, die Impfnarbe vorzuzeigen.

Einfache Formen der Impfung sind schon lange bekannt. Die vorbeugende Ansteckung mit geringen Mengen von Variolaviren, heute Variolation genannt, ist schon seit mindestens 3000 Jahren aus China bekannt, wo zerriebener Schorf der Pusteln geschnupft wurde. In Indien dagegen wurde dieses Material in die Haut eingeritzt. In Europa führte Lady Montagu (1689–1762), die Frau eines britischen Diplomaten in Istanbul, die Variolation durch Einritzen von etwas Flüssigkeit aus den Pockenbläschen in die Haut ein.

Die zweite, sicherere Impfmethode beruht auf der seit spätestens 1765 belegten Beobachtung, dass Menschen nach durchgemachter vermeintlicher Kuhpocken-Infektion vor Infektionen mit den echten Pocken geschützt sind.

Als Edward Jenner während seiner Ausbildung 1768 in Thornbury in Gloucestershire als Assistent bei den Landärzten Daniel und Edward Ludlow arbeitete, erfuhr er erstmals von Kuhpocken. Dort war seit 1766 durch die Landarztkollegen Hugh Grove und Daniel Sutton mit einer reihenweisen Inokulation von Pocken begonnen worden, die eine schwache Pockeninfektion auslöste, dann aber zu einer Immunität führte. Dies war als "Suttonsche Methode" bekannt. Ein weiterer Landarztkollege, John Fewster, machte in diesen Jahren die Entdeckung, dass bei einigen Patienten nicht einmal eine schwache Pockeninfektion ausgelöst wurde, wenn sie vorher an Kuhpocken erkrankt waren. Dies berichtete er vor der lokalen Ärztegesellschaft, und so galt Fewster als Entdecker des Nutzens von Kuhpocken. Erst die Kuhpocken-Inokulation als folgerichtiger Schritt war die Entdeckung Jenners. Die infizierten Kühe waren jedoch nicht an echten Kuhpocken erkrankt, sondern an Vaccinia-Viren, deren Infektionen deutlich harmloser verlaufen. 

Sein Biograph John Baron setzte 13 Jahre nach dem Tod Jenners den Mythos in Umlauf, dass Jenner von den Vorteilen einer vorhergehenden Kuhpocken-Infektion über "Gerüchte in den Kuhställen" gehört habe, und verschwieg Fewsters Erkenntnis. Dies galt als "Milchmädchen-Mythos". Jenner selber hatte nie beansprucht, Entdecker oder Erstbeschreiber der Vorteile einer Kuhpockeninfektion gewesen zu sein. 

Erst 1796 wurde die "Kuhpocken"-Inokulation mit einer gewissen Breitenwirkung in England eingeführt. Zur Überprüfung seiner These vom Schutz vor Pocken durch Inokulation mit Kuhpocken infizierte Jenner zunächst den achtjährigen James Phipps mit aus den Pocken der infizierten Kühe gewonnenem Material und, nach Abklingen der Krankheit, mit den echten Pocken. Der Junge überlebte. Als Jenners Artikel von der Royal Society abgelehnt wurde, unternahm er weitere Versuche – auch mit seinem 11 Monate alten Sohn. Im Jahr 1798 veröffentlichte er seine Ergebnisse und musste erleben, dass man ihn lächerlich zu machen versuchte. Dennoch setzte sich die von ihm propagierte Methode durch. Dieses Verfahren wird Vakzination genannt. Da der Impfstoff von Kühen stammte, nannte Jenner seinen Impfstoff Vaccine (von lat. vacca „Kuh“) und die Technik der künstlichen Immunisierung „Vaccination“ (von lat. vaccinus „von Kühen stammend“). Das Wort "vaccination" bedeutet heute im Englischen Impfung ganz allgemein, auch im Deutschen werden Impfstoffe Vakzine genannt.

Nach dem Engländer Edward Jenner galt Wilhelm Bernhard Nebel von der Universität Heidelberg als einer der ersten Ärzte Deutschlands, der sich ab 1729 mit der Inokulation der Pocken befasst hatte und darüber publizierte. Die künstliche Pockeninfektion mit weniger pathogenen Pockenarten zum Schutz vor "echten Pocken" wurde ab ca. 1771 auch anderswo in Einzelfällen beschrieben und erprobt, u. a. von Sevel, Jensen, Benjamin Jesty (1774), Rendall und Peter Plett (1791). Selbst auf dem Lande wurde sie 1787 erprobt. Der Arzt Calmeyer impfte in Gehrde (Niedersachsen) damals 57 Kinder so erfolgreich, dass keines davon starb. 

1799 führte der Arzt Jean de Carro, der an der Universität Wien wirkte, aufgrund der Vorarbeiten von Jenner als erster auf dem europäischen Kontinent in Wien die Pockenschutzimpfung ein. Bereits ein Jahrzehnt später wurde in vielen Ländern eine Impfpflicht bestimmt.

Das zur Pockenimpfung verwendete Modified-Vaccinia-Ankara-Virus wurde in den 1970er Jahren von Anton Mayr entwickelt. In vielen Ländern wurde die Impfung von Kleinkindern und auch die Nachimpfung nach etwa 12 Jahren gesetzlich vorgeschrieben.

Pocken sind vermutlich schon seit Jahrtausenden bekannt. Die an mehreren Stellen des Alten Testaments (u. a. ; und das Leiden des Hiob) hebräisch als "schechin" (Pustel, Geschwür) bezeichnete Seuche wurde von Medizinhistorikern mit den Pocken in Verbindung gebracht. Besonders die sechste ägyptische Plage () gilt vielen Exegeten als Beschreibung einer Pockenepidemie. In der "Vita Mosis" beschreibt Philon von Alexandria diese Plage mit allen Symptomen der Pocken. Diese auch als „ägyptisches Geschwür“ ("schechin mizraim." ) bezeichnete Seuche wurde auch mit der Uhedu-Krankheit identifiziert, die mehrfach im Papyrus Ebers genannt wird. Das altägyptische Wort "uhedu" oder "uhet" steht für einen eigentümlichen, tödlichen Hautausschlag, der mit Geschwüren einhergeht. Die Mumie von Pharao Ramses V. von Ägypten zeigt Läsionen, die histologisch den Pockennarben entsprechen könnten.

In China wurden die von dem Alchemisten Ge Hong beschriebenen Pocken vermutlich um 250 v. Chr. über die noch unfertige chinesische Mauer durch die Hunnen eingeschleppt. Daher rührt die dort verwendete Bezeichnung "Hunnenpocken".

Nach Europa kamen die Pocken wahrscheinlich 165 n. Chr. mit dem Einzug der siegreichen römischen Legionen nach der Einnahme der parthischen Stadt Seleukia-Ktesiphon im heutigen Irak. Die Pocken breiteten sich rasch bis zur Donau und zum Rhein hin aus. Die Folge war ein Massensterben über 24 Jahre hin, das als "Antoninische Pest" in die Geschichte eingegangen ist. Der Bagdader Arzt Muhammad ibn Zakarīyā ar-Rāzī beschrieb um 910 den Unterschied zwischen Pocken und Masern, und wusste, dass kein zweiter Pockenbefall bei einem Menschen auftreten kann.
Die Kreuzritter des 11.–13. Jahrhunderts trugen zur Verbreitung der Pocken wesentlich bei.

Die europäischen Eroberer brachten die Pocken nach Amerika mit, wo sie ab 1518 unter den Indianern verheerende Epidemien auslösten, die Millionen von Toten forderten. Gut untersucht ist die Pockenepidemie an der Pazifikküste Nordamerikas ab 1775. Die Europäer dagegen waren durch zahlreiche frühere Pockenepidemien stark durchseucht und daher relativ wenig gefährdet. Ob die Pocken als biologische Waffe gegen die Indianer eingesetzt wurden, ist trotz aller Forschung umstritten. Aus dem Juni 1763 existieren ein Briefwechsel und eine Quittung, laut denen zwei Decken und ein Taschentuch aus dem Hospital des belagerten Fort Pitt – in dem die Pocken ausgebrochen waren – an eine Delegation der Lenni Lenape-Indianer überreicht worden sind. Der Versuch hatte aber keinen Erfolg, die später doch ausgebrochene Epidemie wird auf andere Ursachen zurückgeführt. Die vielfältig erhobenen Vorwürfe bezüglich eines Pockenausbruchs 1837 bei den Mandan am Missouri River wurden als wissenschaftliche Fälschung enttarnt. Der Verantwortliche, Ward Churchill, verlor seinen Lehrstuhl infolge des akademischen Skandals. Im Fall der Pockenepidemie an der Pazifikküste Nordamerikas 1862 lässt sich zeigen, dass äußerst verschieden motivierte Entscheidungsträger im Ergebnis ihrer Handlungen die Ausbreitung förderten und in der Bevölkerung erfreute Reaktionen über das folgende Massensterben vorkamen. Nach Impfung der meisten Weißen und vieler Indianer in Victoria konnte in diesem Fall eine unerwünschte Ausbreitung der Pocken unter den weißen Siedlern nahezu ausgeschlossen werden.

Nach Australien kamen die Pocken vermutlich mit Seefahrern aus Makassar in Indonesien, die ab etwa 1700 alljährlich im Arnhemland Seegurken sammelten. Als erste Pockenepidemie wurde die von 1789 von den Mitgliedern der First Fleet im heutigen Gebiet von Sydney dokumentiert.

In Europa galten Pocken teilweise als Kinderkrankheit und die Erkrankung, an der bis zum Ende des 18. Jahrhunderts noch bis zu 10 % aller Kleinkinder starben, wurde auch „Kindsblattern“ genannt. Ab dem 18. Jahrhundert häuften sich die Pockenfälle und lösten die Pest als schlimmste Krankheit ab. Nach Schätzungen starben jedes Jahr 400.000 Menschen an Pocken. Oft zählten Kinder erst zur Familie, wenn sie die Pocken überstanden hatten. Berühmte Persönlichkeiten wie Mozart, Haydn, Beethoven und Goethe blieben von der Krankheit nicht verschont, Ludwig XV. von Frankreich und Zar Peter II. starben daran. Die Heiratspolitik der Habsburger wurde gleichfalls von den Pocken immer wieder durcheinandergebracht. Die Kaiserin Maria Theresia, die mit der Verheiratung ihrer Töchter an andere Herrschaftshäuser Allianzpolitik betrieb, musste mehrfach ihre Pläne ändern, weil zwei ihrer Töchter an den Pocken starben und eine dritte (Marie Elisabeth) durch diese völlig verunstaltet wurde.

Der erste noch vage morphologische Nachweis des Pockenvirus wird dem in Mexiko gebürtigen Hamburger Impfarzt Enrique Paschen (1860–1936) zugeschrieben, der bereits 1906 mit einem Lichtmikroskop die von ihm „Elementarkörperchen“ und nach ihm „Paschenschen Körperchen“ benannten Partikel in der Lymphe von an "Variola major" erkrankten Kindern sah.

Noch in den 1950er und 1960er Jahren gab es in Europa Pockenepidemien, so z. B. 1950 in Glasgow, 1958 in Heidelberg (18 Krankheitsfälle, davon zwei tödlich), 1963 in Breslau (99 Krankheitsfälle, davon sieben tödlich) und 1967 in der Tschechoslowakei. Ein Einzelfall im Frühsommer 1957 in Hamburg konnte hingegen isoliert werden. Ab 1967 wurde die Pockenimpfung auf Beschluss der Weltgesundheitsorganisation (WHO) weltweit Pflicht. Es wurde mit großangelegten Impfaktionen ein weltweiter Feldzug zur Ausrottung der Pocken gestartet.

Die letzte Pockenepidemie in Deutschland fand Anfang 1970 statt, als ein 20-Jähriger die Pocken in das nördliche Sauerland einschleppte und insgesamt 20 Personen infizierte. Die Erkrankten wurden im Marienhospital in Wimbern isoliert. Der letzte Pockenfall in der Bundesrepublik Deutschland wurde 1972 in Hannover bei einem jugoslawischen Gastarbeiter festgestellt, der aus dem Kosovo zurückkehrte, wo es zu einer Pockenepidemie gekommen war. Der letzte Fall von Echten Pocken wurde 1975 in Bangladesch dokumentiert, der letzte Fall von Weißen Pocken 1977 in Merka, Somalia, der letzte Erkrankte war der damals 23-jährige Ali Maow Maalin. Zum weltweit letzten Todesfall durch Pocken kam es 1978 in Birmingham, England, durch eine Laborinfektion. Am 8. Mai 1980 wurde von der WHO festgestellt, dass die Pocken ausgerottet sind. Dennoch sind weitere Pockeninfektionen nicht völlig ausgeschlossen. Offiziell existiert das Virus noch in zwei Laboratorien der Welt; allerdings ist unklar, ob in einigen Staaten unveröffentlichte Virenbestände gehalten werden.

Die meisten Staaten hoben ab den 1970er Jahren die Pockenimpfpflicht wieder auf (in Teilen Deutschlands wurde die Erstimpfung 1975 ausgesetzt, später die Wiederimpfung), da auch die Impfung nicht völlig risikofrei ist. Nach Erfahrungswerten aus den 1950er und 1960er Jahren rechnet das CDC mit 15 lebensbedrohlichen Komplikationen und zwei Todesfällen pro einer Million Geimpfter.

Seit 1980 gibt es offiziell nur noch zwei Orte, an denen Pockenviren lagern, nämlich das Forschungszentrum der US-amerikanischen Seuchenbehörde CDC "(Centers for Disease Control and Prevention)" in Atlanta und ihr russisches Gegenstück VECTOR in Kolzowo südöstlich von Nowosibirsk. Über eine Vernichtung der letzten Bestände wurde nachgedacht, die Gedanken wurden allerdings verworfen. Die Bestände wären die letzte Möglichkeit, Impfstoffe gegen die Pocken auf ihre Wirksamkeit zu prüfen.

Nachdem die „natürliche“ Verbreitung der Pocken mit den Impfkampagnen eliminiert wurde, rückte das Virus als mögliche Methode eines Biowaffenanschlags wieder in die Aufmerksamkeit der Öffentlichkeit. Iris Hunger, die Leiterin der Forschungsstelle Biowaffenkontrolle am Carl-Friedrich von Weizsäcker-Zentrum für Naturwissenschaft und Friedensforschung, kritisierte schon 2003, dass durch einen Unfall oder Terrorangriff die tiefgekühlten Erreger freigesetzt werden könnten.

Die Industriestaaten haben sich nach den Terroranschlägen am 11. September 2001 umfassend mit Pocken-Impfstoff für ihre Bevölkerung eingedeckt (u. a. die USA mit 100 Millionen Impfdosen), so dass umgehend reagiert werden könnte. Unter dem Eindruck des bevorstehenden Irakkriegs gab die Bundesrepublik Deutschland der Firma Bavarian Nordic um den Jahreswechsel 2002/2003 den Auftrag, für jeden Einwohner Deutschlands etwas mehr als eine Impfdosis auf Vorrat zu produzieren. Neben den USA und Deutschland hielten 2003 noch Südafrika, das Vereinigte Königreich und Israel größere Bestände an Impfstoffen vorrätig. Die Entwicklungsländer waren dagegen nicht in der Lage, sich die Anschaffung der kostspieligen Impfdosen für ihre ganze Bevölkerung zu leisten. Zwar hält auch die WHO 64 Millionen Impfstoffdosen vorrätig, doch für die Bevölkerung in den Entwicklungsländern wird diese Anzahl an Impfdosen im Ernstfall nicht ausreichen und die Pocken könnten sich erneut mit katastrophalen Folgen ausbreiten.

Um das von den gelagerten Pocken-Viren ausgehende Restrisiko zu beseitigen, wollten die Vertreter der Mitgliedsstaaten der Weltgesundheitsorganisation (WHO) bis 2014 eine Einigung darüber erzielen, wann die zwei noch vorhandenen Sammlungen der tödlichen Erreger unschädlich gemacht werden. Auf der Sitzung im Mai 2014 sprach sich die Mehrheit für eine Vernichtung aus, sie erreichte aber nicht die notwendige Einigkeit. Insbesondere wollten „russische Forschungseinrichtungen“ und das US-Verteidigungsministerium das Virus erhalten. Dies wurde damit begründet, dass es zur weiteren Forschung und Entwicklung benötigt werde. Zur Herstellung eines Impfstoffes und eines Heilmittels wird das Virus selbst jedoch nicht mehr benötigt. Ein neues Datum für die Diskussion wurde nicht beschlossen, die Viren bleiben also auf unbestimmte Zeit in den beiden Laboren erhalten.

2014 wurden versiegelte Ampullen mit gefriergetrockneten Pocken-Viren in einem Abstellraum in den National Institutes of Health entdeckt, das seit 1972 zur Food and Drug Administration gehört. Nach ersten Untersuchungen stammen die Proben aus den 1950er Jahren.

Der kanadische Mikrobiologe David Evans aus Edmonton konnte 2016 zeigen, dass es nur eines kleinen Teams, etwa sechs Monaten Arbeit und weniger als 100.000 Euro brauche, um im Labor Pockenviren nachzubauen, was ihm mit den Pferdepocken-Viren gelang, die den humane Pockenviren sehr ähnlich und im Rekonstruktionsverfahren gleich sind.




</doc>
<doc id="4050" url="https://de.wikipedia.org/wiki?curid=4050" title="Piltdown-Mensch">
Piltdown-Mensch

Als Piltdown-Mensch wurden die angeblichen Überreste eines Frühmenschen bekannt, die vor 1912 in einer Kiesgrube bei dem Dorf Piltdown in der Nähe von Uckfield in Südostengland gefunden und 1953 als wissenschaftliche Fälschung entlarvt wurden.

Die Fragmente eines Schädels und eines Unterkieferknochens wurden von den damaligen britischen Experten für Überreste eines bisher unbekannten Vorfahren des Menschen gehalten. Sie gaben ihnen den wissenschaftlichen Namen "Eoanthropus dawsoni" (etwa „Dawsons Mensch der Morgenröte“), zu Ehren seines Entdeckers Charles Dawson (1864–1916), eines britischen Rechtsanwalts und Amateurarchäologen.

Zweifel an der Echtheit des Fundes und, nach der Entlarvung der Fälschung, die Suche nach deren Urheber währten über Jahrzehnte und brachten einen umfangreichen Bestand an Literatur hervor.

Zur Zeit der Piltdown-Funde waren nur wenige Überreste von Frühmenschen gefunden worden; zu den bekanntesten zählten die des Neandertalers 1856, des Java-Menschen 1891 und der Unterkiefer von Mauer 1907. Die Wissenschaft interpretierte die Funde als Übergangsformen zum modernen Menschen, allerdings erlaubten die spärlichen Funde noch eine Vielzahl von Deutungen, so auch in der Frage, in welcher Reihenfolge die Entwicklungsschritte auf dem Weg zum modernen Menschen erfolgten. Der Fund von Piltdown schien diese Frage ein für alle Mal beantworten zu können und war schon von daher eine sensationelle Entdeckung. Daraus erklärt sich wohl seine schnelle Akzeptanz, obwohl bereits die Fundumstände dubios blieben.

Die besonderen Merkmale des Fundes waren sein hohes Alter, das auf rund 500.000 Jahre geschätzt wurde, eine große, schon dem modernen Menschen ähnelnde Schädelkapsel und ein noch primitiver Unterkiefer, der an den eines Menschenaffen erinnerte. Aus dieser Kombination wurden weitreichende Schlussfolgerungen zur Stammesgeschichte des Menschen abgeleitet, unter anderem, dass die Entwicklung zum modernen Menschen in Europa stattgefunden habe und dass sich beim Menschen bereits sehr früh ein großes Gehirn entwickelte. Die ersten Australopithecus-Funde, so das Kind von Taung, wurden daher von britischen und US-amerikanischen Paläoanthropologen jahrzehntelang nicht als Vormenschen anerkannt.

Deutsche und französische Forscher hatten hingegen aufgrund genauer Kenntnis der Neandertaler-Funde von Beginn an Zweifel an der Aussagekraft der Piltdown-Fragmente. Auch im Licht späterer Fossilfunde aus Asien und Afrika konnte der Piltdown-Mensch sich bestenfalls als rätselhafter Nebenast im Stammbaum des Menschen behaupten, da jene einen völlig anderen Entwicklungsweg zum modernen Menschen belegten, insgesamt jünger und mit spät einsetzender Vergrößerung des Gehirnvolumens. Die Bedeutung des Piltdown-Menschen blieb 40 Jahre lang umstritten, bis er 1953 als Fälschung entlarvt wurde: Der Schädel eines modernen Menschen und der geschickt manipulierte Unterkieferknochen eines Affen waren zusammen mit weiteren Fragmenten von Tierknochen und Steinwerkzeugen im genannten Kiesbett vergraben worden.

Die genauen Umstände, unter denen der Piltdown-Schädel entdeckt worden ist, wurden nur unzureichend dokumentiert. Charles Dawson berichtete, ihm sei 1908 bei einem Besuch der Piltdown-Kiesgrube von einem Arbeiter ein erstes Fragment eines menschlichen Schädels übergeben worden. Er habe daher in den folgenden Jahren die Kiesgrube wiederholt aufgesucht und mehrere weitere Schädelfragmente entdeckt. Diese Funde übergab er jeweils Arthur Smith Woodward, dem Kustos der geologischen Abteilung des British Museum. Woodward war an den Fundstücken sehr interessiert und begleitete Dawson mehrfach zur Fundstelle, wo sie gemeinsam zwischen Juni und September 1912 weitere Teile des Schädels und eine Hälfte eines Unterkieferknochens fanden, bei dem der Gelenkansatz fehlte. Zeitweise beteiligte sich auch Pierre Teilhard de Chardin an den Grabungen in der Kiesgrube.

Am 18. Dezember 1912 gaben Arthur Smith Woodward und Charles Dawson während eines Treffens der Geological Society of London bekannt, die Schädelfragmente seien ein epochemachender Fund. Die von Woodward anhand der gefundenen Fragmente angefertigte Rekonstruktion ähnelte weitgehend dem Schädel eines modernen Menschen, mit Ausnahme des Occiput, einer Region am Übergang des Schädels zur Wirbelsäule, sowie der Größe des Gehirns, die nur ungefähr zwei Drittel eines modernen Menschen betrage, und der nur annähernd menschenähnlichen Zähne und des Kieferknochens, der sich nicht von dem eines heutigen jungen Schimpansen unterscheide. Gestützt auf die hohe wissenschaftliche Autorität des British Museum interpretierte Woodward die Funde von Piltdown als ein "Missing Link" zwischen Menschenaffe und Mensch. Die Gelehrten des Natural History Museum in London schrieben – trotz anfänglicher Zweifel – dem Fund ein Alter zwischen 200.000 und 500.000 Jahren zu. Damit wäre er wesentlich älter gewesen als der Neandertaler. Die Bekanntgabe der Entdeckung fand sofort ein großes Interesse sowohl in der Fachwelt als auch in der britischen Bevölkerung: Die Tageszeitung "Manchester Guardian" hatte bereits Anfang Dezember 1912 über den Fund berichtet, mit der Folge, dass das Treffen der "Geological Society of London" so gut besucht war wie keines zuvor. Die Meldung des Piltdown-Fundes ging als Sensation um die ganze Welt. Über 500 wissenschaftliche und publizistische Veröffentlichungen beschäftigten sich mit dem "Eoanthropus dawsoni", darunter der Überblick "The earliest Englishman" von Arthur Smith Woodward, posthum herausgegeben von Arthur Keith.

Vor allem für die britischen und einige US-amerikanische Paläoanthropologen stellte der Fund eine Bestätigung ihrer theoretischen Überlegungen dar, dass die Entwicklung eines großen Gehirns Voraussetzung für die Menschwerdung gewesen sei und das Größerwerden des Gehirns der Ausbildung anderer Merkmale des modernen Menschen (terrestrische Lebensweise, aufrechter Gang, Werkzeuggebrauch, Entwicklung von Sprache und Kultur) vorausgegangen sei. Daher blieb die früh einsetzende Kritik an Woodwards Rekonstruktion der Piltdown-Fragmente weitgehend unbeachtet. Am "Royal College of Surgeons" wurden beispielsweise Kopien der Fragmente für eine Rekonstruktion verwendet, die hinsichtlich der Größe des Gehirns und anderer Eigenschaften dem modernen Menschen wesentlich stärker ähnelte als jene von Woodward. Das Ansehen, das Woodward unter seinen Fachkollegen genoss, verhinderte jedoch letztlich jede offene Kritik an seiner Rekonstruktion.

1915 behauptete Dawson, Fragmente eines zweiten Schädels an einer Stelle gefunden zu haben, die etwa zwei Meilen vom Ort der ursprünglichen Funde entfernt lag. Nach Dawsons Tod im folgenden Jahr konnte die in Frage kommende Stelle (Piltdown II) nicht mehr exakt identifiziert werden, zudem sind die Funde schlecht dokumentiert. Selbst Woodward scheint den zweiten Fundort nicht besucht zu haben.

Am 23. Juli 1938 enthüllte Sir Arthur Keith in der Nähe von Barkham Manor ein Denkmal, um die Stelle zu kennzeichnen, wo der Piltdown-Mensch von Charles Dawson entdeckt wurde. Keith beendete seine Ansprache mit den Worten:

Die (übersetzte) Widmung auf dem Gedenkstein lautet:

Der nahe gelegene Pub wurde zu Ehren des Fundes in „The Piltdown Man“ umbenannt.

Die Enthüllung der Piltdown-Fälschung am 21. November 1953 durch Wilfrid Le Gros Clark, Kenneth Oakley (1911–1981, British Museum) und – auf dessen Initiative hin – Joseph Sidney Weiner (1915–1982, Universität Oxford) wurde in vielen akademischen Kreisen mit Erleichterung aufgenommen. Der Piltdown-Mensch war zuvor bereits als Abweichung betrachtet worden, die vollständig im Widerspruch zur wesentlichen Hauptrichtung menschlicher Evolution stand, wie sie zwischenzeitlich an anderen Orten entdeckte fossile Hominiden zu zeigen schienen. Die Anfang der 1950er Jahre entwickelte Methode, den Fluoridgehalt der Fossilien zu messen, ergab für die Piltdown-Funde ein unterschiedliches, in jedem Fall aber weit geringeres Alter als die ursprünglich geschätzte Datierung ins mittlere Pleistozän. Eine anschließende Altersbestimmung durch die Radiokohlenstoffdatierung konnte 1959 nachweisen, dass sowohl Schädel als auch Unterkiefer nur wenige hundert Jahre alt waren. Sie zeigte damit, dass der Piltdown-Mensch eine zusammengesetzte Fälschung ist.

Die Knochenfunde bestehen aus einem mittelalterlichen Menschenschädel, dem 500 Jahre alten Unterkiefer eines Orang-Utans und dessen Zähnen. Das Aussehen von hohem Alter wurde erzeugt, indem man die Knochen mit einer Eisenlösung und Kaliumdichromat einfärbte. Schwierigkeiten machte bei der Fälschung der Bereich, an dem der Kiefer an den Schädel anschließt, da jener sich bei Affe und Mensch in der Form deutlich unterscheidet. Dieses Problem löste man, indem man die verräterischen Enden des Kiefers abbrach. Die Zähne im Kiefer wurden passend gefeilt, und es war dieses Feilen, das zu Zweifeln an der Glaubwürdigkeit des gesamten Stücks führte: Man bemerkte, dass die Spitze eines der Backenzähne im Vergleich mit den anderen Zähnen in einem stark unterschiedlichen Winkel abgeschrägt war. Mikroskopische Untersuchungen zeigten Schleifspuren an den Zähnen, und man folgerte daraus, dass die Zähne bearbeitet worden waren, um ihre Form zu verändern, da Affenzähne eine andere Form als menschliche Zähne haben. Hochauflösende Röntgenaufnahmen brachten ans Licht, dass Zähne und Knochen im Inneren mit Kieselsteinchen gefüllt waren, um das höhere Gewicht von Fossilien im Vergleich zu rezenten Knochen herzustellen.

Der vielleicht unglaublichste Fund war ein „Artefakt“ in der Nähe der Knochen, von dem die Wissenschaftler glaubten, es sei ein Werkzeug oder Teil eines Skeletts. Dieser versteinerte Oberschenkelknochen eines Elefanten wies Bearbeitungsspuren von Menschenhand auf, doch konnte ihm kein sinnvoller Verwendungszweck zugeschrieben werden. Seine Form erinnerte 1914 einige der untersuchenden Wissenschaftler noch am ehesten an einen Cricket-Schläger, aber diese Erkenntnis zog damals keine weiteren Konsequenzen nach sich. Vermutlich wollte der Urheber der Fälschung damit auf sein Werk aufmerksam machen, doch die Forscher um Woodward sahen sich mit jedem neuen Fund nur in ihren Theorien bestätigt.

Der Grad der technischen Kompetenz der Piltdown-Fälschung bleibt weiterhin ein Diskussionsthema, jedoch liegt das besondere Wesen der Fälschung darin, dass sie den damaligen Experten genau das anbot, was sie suchten: den überzeugenden Beweis, dass die menschliche Evolution vom Gehirn ausging – und in Europa stattgefunden hatte. Es wurde auch vermutet, dass Nationalismus und Rassismus ebenso eine Rolle bei der Akzeptanz des Fossils als Original spielten, da bereits zuvor die Forderung aufkam, Großbritannien bräuchte einen „Ersten Briten“, um ihn gegen die fossilen Funde von Hominiden zu stellen, die in anderen Teilen der Welt, besonders Frankreich und Deutschland, gefunden worden waren.

Die Identität des Piltdown-Fälschers bleibt ebenso unbekannt wie seine Motive. Viele Autoren vermuten, dass diese wohl berühmteste Betrugsaffäre der Naturforschung ein Streich war, der außer Kontrolle geriet. Verdachtsmomente konnten für alle an dem Fund beteiligten Forscher nachgewiesen werden. So wurden Dawson, Woodward, Teilhard de Chardin, die Anatomen Arthur Keith und Grafton Elliot Smith ebenso beschuldigt wie Arthur Conan Doyle, der damals 15 km von der Fundstelle entfernt wohnte. Ihm wurde ein Rachemotiv unterstellt, weil die etablierte Wissenschaft seine Forschungen zu Geistwesen heftig angegriffen hatte.

So gut wie jeder, der jemals mit dem Fund in Berührung kam, wurde irgendwann der Tat verdächtigt. So hat 1978 der Londoner Paläontologe Brian Gardiner einen weiteren möglichen Täter in die Diskussion eingebracht. Er hält Martin Alister Campbell Hinton (1883–1961) für den Urheber der Posse. Hinton hatte zur Zeit des Fundes als freier Mitarbeiter und bis 1945 als Kurator für Zoologie im Natural History Museum in London gearbeitet und war 1961 verstorben. Er hinterließ einen Schrankkoffer im Lager des Museums, der 1978 gefunden wurde. Der Koffer enthielt Tierknochen und Zähne, die in einer Art und Weise gefeilt und gefärbt waren, die den Piltdown-Funden ähnelten. Kurz vor seinem Tod hatte Hinton zudem einem Kollegen geschrieben, wie sehnsüchtig er als junger Student davon geträumt habe, in den Hügeln von Sussex den von Charles Darwin propagierten "missing link" zwischen Mensch und Affe zu finden. Vor allem bei „charakterlich ungefestigten Mitgliedern“ der Paläontologengemeinde, hatte er geschrieben, könne „die Versuchung, die Entdeckung eines Affenmenschen zu erfinden“, schlicht „unwiderstehlich“ gewesen sein. Unwidersprochen ist aber auch dieser Verdacht nicht geblieben.

Andere Autoren halten Charles Dawson für den wahrscheinlichsten Urheber der Fälschung, da er als einziger bei allen Funden in Piltdown I anwesend war, er allein die Fundstelle Piltdown II kannte und es seit seinem Tod 1916 zu keinen weiteren Funden mehr kam. Auch hat er der Wissenschaft nachweislich eine ganze Reihe von archäologischen Funden präsentiert, die sich später als gefälscht herausstellten, unter anderem römische Ziegelstempel und eine Figurine als angeblich frühesten Beleg für die Herstellung von Gusseisen in Europa. Auch wenn Dawson als der wahrscheinlichste Fälscher des Piltdown-Menschen gilt, so wird doch angezweifelt, dass er allein handelte, und die Frage nach möglichen Komplizen beschäftigt die Wissenschaftler noch heute.

Eine in der Reihe "Geheimakte Geschichte" am 21. Mai 2016 auf Arte ausgestrahlte Dokumentation, in der die Historikerin Natalie Akbari und der ehemalige BKA-Profiler Michael Baurmann rätselhaften Fällen auf den Grund gehen, kommt aufgrund ihrer Recherchen zu dem Schluss, dass Dawson der Fälscher war, den Ehrgeiz, Geltungsdrang und das Verlangen, einer der angesehenen wissenschaftlichen Vereinigungen anzugehören, angetrieben hatten. Letztendlich habe Dawson beim Präparieren der Schädelteile durch unsachgemäßes Hantieren mit giftigen Chemikalien seinen frühen Tod wie auch den seiner Frau verschuldet. Hinton hingegen wird entlastet: Die Recherchen hätten Unterschiede zwischen den Inhalten seines Koffers und den fraglichen Schädelfragmenten hinsichtlich der Bearbeitungsweise und der verwendeten Chemikalien ergeben. Vermutlich habe Hinton Verdacht geschöpft, sei der Fälschung auf der Spur gewesen und habe dazu eigene Experimente unternommen. Aufgrund seiner fehlenden Reputation in der Fachwelt habe er aber nicht gewagt, mit dieser Vermutung an die Öffentlichkeit zu gehen. Wahrscheinlich habe er jedoch den „Cricketschläger“-Knochen platziert, um den Archäologen einen Hinweis zu liefern, dass an den bisherigen Funden etwas nicht stimme. Wissenschaftliche Untersuchungen aus dem Jahr 2016 lassen es ebenfalls als sehr wahrscheinlich erscheinen, dass Charles Dawson die Fälschungen selbst hergestellt hat.

Bereits die 1917 entstandene frühe Kurzgeschichte "Dagon" von H. P. Lovecraft verweist auf den Piltdown-Menschen. Auch in der späteren Kurzgeschichte "Die Ratten im Gemäuer" erwähnt ihn Lovecraft.

The Piltdown Men waren eine US-amerikanische Instrumental-Rock-Band aus Los Angeles, die Anfang der 1960er-Jahre einige erfolgreiche Singles bei Capitol Records veröffentlichte.

Mike Oldfield führt auf seinem Album "Tubular Bells" von 1973 den „Piltdown man“ als eines der Instrumente an, die er spielt. Dies verweist auf das zweite Stück des Albums, das durch die frühen Hominiden inspiriert und mit einer rauen Stimme gesungen wurde. In der Neubearbeitung des Albums von 2003 heißt dieser Teil „Caveman“.

In „Der Psychiater“, einer Episode der "Fawlty Towers" von 1979, wird ein Gast aus der Unterschicht als „Piltdown-Weichei“ bezeichnet.

Im März 1994 führte Apple Computer den Power Macintosh 6100 ein, der den Codenamen „Piltdown Man“ trug. Etwas später im selben Jahr wurde das Macintosh-Computerspiel Marathon herausgebracht, in dem auf dem Bildschirm kleiner Computerterminals an den Protagonisten gerichtete Botschaften mit dem Wort „piltdown“ in der Kopfzeile zu lesen sind. Dies soll vermutlich darauf hindeuten, dass diese Botschaften Teil einer Täuschung sind und ihr vorgeblicher Absender gar nicht existiert.





</doc>
<doc id="4052" url="https://de.wikipedia.org/wiki?curid=4052" title="Portable Network Graphics">
Portable Network Graphics

Portable Network Graphics (PNG [], für "portable Netzwerkgrafik") ist das meistverwendete verlustfreie Grafikformat im Internet. Es ist ein universelles, vom World Wide Web Consortium (W3C) anerkanntes Grafikformat für Rastergrafiken mit verlustfreier Datenkompression. Da PNG-Fotos in der Regel wesentlich größer sind als JPG und PNG lange Zeit nicht ohne Weiteres in Browsern dargestellt wurden, konnte sich PNG nicht gegen GIF und JPG durchsetzen.

PNG wurde als freier Ersatz für das ältere, bis zum Jahr 2006 mit Patentforderungen belastete Graphics Interchange Format (GIF) entworfen und ist weniger komplex als das Tagged Image File Format (TIFF). PNG unterstützt neben unterschiedlichen Farbtiefen auch Transparenz per Alphakanal.

Die Entwicklung des PNG-Formats begann Ende 1994, verglichen mit anderen Grafikformaten wie TIFF, GIF und JFIF („JPEG“) also recht spät. Auslöser waren Lizenzforderungen der Softwarefirma Unisys für den von GIF verwendeten Lempel-Ziv-Welch-Algorithmus (LZW).

Am 4. Januar 1995 legte Thomas Boutell einen frühen Entwurf (PBF Draft 1) vor.
Die erste richtige PNG-Spezifikation (Version 1.0) von Thomas Boutell und Tom Lane wurde bereits am 1. Oktober 1996 offizielle Empfehlung des World Wide Web Consortium (W3C).
Am 14. Oktober 1996 erhielt PNG von der Internet Assigned Numbers Authority (IANA) den MIME-Typ "codice_1" zugewiesen.
Am 15. Januar 1997 wurde PNG von der Internet Engineering Task Force (IETF) als RFC 2083 verabschiedet.
Am 31. Dezember 1998 erschien die von Adam Costello und Glenn Randers-Pehrson überarbeitete PNG-Spezifikation Version 1.1.
Am 11. August 1999 veröffentlichte Glenn Randers-Pehrson schließlich die bisher letzte Version 1.2.
Diese wurde am 10. November 2003 zum ISO-Standard ISO/IEC 15948:2003 erhoben und gleichzeitig zur zweiten Ausgabe der W3C-Empfehlung.

Das PNG-Format ermöglicht Graustufen-, Vollfarb- und Farbpaletten-Modus, sowie einen Farbmodus mit Alpha-Kanal (Farb-Typen 0, 3, 6, 4 und 9).

Bei Graustufenbildern kann die Auflösung 1, 2, 4, 8 oder 16 Bit pro Pixel betragen, bei Farbbildern 8 (RGB8) oder 16 Bit (RGB16) pro Farbkanal. Farbbilder können alternativ mit dem Farbpalettenmodus mit bis zu 256 indizierten Farben gespeichert werden. Die indizierten Farben sind aus dem vollen RGB8-Spektrum frei wählbar.

PNG-Dateien können Transparenzinformationen enthalten, entweder in Form eines Alphakanals, als einzelne transparente Farbe oder als ergänzende Transparenzpalette zu einer vorhandenen Farbpalette, die zu jeder Palettenfarbe einen Transparenzwert enthält.

PNG unterstützt Alphakanäle mit 8 Bit oder 16 Bit Tiefe. Es handelt sich um ein Kanal mit einer zusätzliche Information für jedes Pixel. Diese gibt in unterschiedlichen Abstufungen die Deckkraft an, wie viel vom Hintergrund eines Bildes durchscheinen soll. Ist dieser Wert 0, so scheint der Hintergrund vollständig zu 100% durch. Wird bei einer Tiefe von 8 Bit der Maximalwert von 255 angeben (beziehungsweise 65535 bei 16 Bit), entspricht das eine vollständige Deckung. Der Hintergrund ist in diesen Fall nicht sichtbar.

Das PNG-Format erlaubt somit, unabhängig vom Hintergrund die Kanten von Text und Bildern zu glätten. Man kann echte Schlagschatten verwenden, die im Hintergrund ausblenden, oder Bilder erzeugen, die beliebig geformt erscheinen – wenn das Anzeigeprogramm das PNG-Format beherrscht.

PNG-Dateien sind aus verschiedenen Datenblöcken (englisch „chunks“) mit jeweils unterschiedlichen Funktionen aufgebaut, die durch eine Zeichenkette aus vier Buchstaben gekennzeichnet werden (beispielsweise "tEXt" für textuelle Informationen). Neben den Pflicht-Blocktypen IHDR, IDAT, PLTE und IEND, die jede Implementierung unterstützen muss, sind weitere optionale Datenblöcke standardisiert.
Diese können Metadaten zu den Bildinhalten und andere Zusatzinformationen enthalten, etwa zur Farbkorrektur. Diese können mit Programmen wie TweakPNG bearbeitet werden. Anwendungen können auch private Datenblöcke für eigene Zwecke definieren. Adobe Fireworks verwendet PNG als Anwendungsformat und nutzt private Datenblöcke, um darin verschiedene weitere Informationen abzulegen. Dabei besteht allerdings auch Verwechslungsgefahr mit den wesentlich kleineren, normalen PNG-Dateien.

Eine PNG-Datei beginnt immer mit der folgenden, acht Byte langen Signatur (Magische Zahl):

Die verlustfreie Datenkompression in PNG basiert auf mehreren, teils optionalen Verarbeitungsschritten. Zuerst können mit einem Vorfilter die Werte benachbarter Bildpunkte dekorreliert werden, um sie besser komprimierbar über eine Differenz zu Nachbarwerten beschreiben zu können. Dann kann mit einer Substitutionskompressionsmethode versucht werden, wiederkehrende Bildmuster zu erkennen und durch kürzere Rückverweise auf ein vorheriges Auftreten zu ersetzen.
Abschließend wird eine Entropiekodierung angewendet, die Auftrittswahrscheinlichkeiten einzelner Werte ausnutzt, indem sie die Werte nach Wahrscheinlichkeit sortiert durch Codes variabler Länge ersetzt.

In der Regel korreliert der Farbwert eines Bildpunktes mit Werten von Nachbarpunkten, das heißt es besteht eine Abhängigkeit oder Ähnlichkeit. Um diese Korrelationen auszunutzen, unterstützt PNG Vorfilter, die die Ausgangsdaten zunächst dekorrelieren. Dadurch werden Bildpunkte über die Differenz zu Nachbarpunkten beschrieben (Delta-Kodierung). Zu jeder Bildzeile kann eine von 5 Filtermöglichkeiten bestimmt werden (siehe unten). Die Auswahl erfolgt aus Geschwindigkeitsgründen oft heuristisch. Diese Filter ersetzen auf umkehrbare Weise die Farbwerte der Bildpunkte durch (ebenso viele, ebenso große) Differenzwerte. Dieses Differenzsignal hat in der Regel eine wesentlich geringere Dynamik, also Werte mit im Schnitt kleineren Beträgen. Diese sind von der abschließenden Entropiekodierung effektiver zu komprimieren. Je uniformer die Bildinhalte ausfallen, desto gewinnbringender funktioniert dieser Mechanismus.
Bei der Dekodierung werden nach der Dekomprimierung der Daten umgekehrte Versionen der Filter angewandt, um die eigentlichen Bilddaten wiederherzustellen.

Diese Möglichkeit ist einer der Gründe für die geringe Größe von PNG-Dateien.

PNG definiert in der derzeitigen Version fünf verschiedene Filtertypen:
Manche Kodierer probieren zur Verbesserung der Kompression mehrere Filter durch. Dies ist besonders bei den zahlreichen Werkzeugen zur Optimierung der PNG-Kompression eine gängige Technik.

In vielen Fällen bietet der nach seinem Erfinder Alan W. Paeth benannte Paeth-Predictor die besten Ergebnisse. Mit diesem wird versucht, aus den links, oben und linksoben benachbarten Bildpunkten automatisch den ähnlichsten für die Differenzbildung zu nutzen. Die Funktion wählt den Bildpunkt, der links+oben−linksoben am nächsten kommt.

erfolgen nach dem populären Deflate-Verfahren, da dieses ohne Belastung durch Softwarepatente frei verwendbar ist. Es umfasst Substitutionskompression nach Storer, Szymanski, Lempel und Ziv (LZSS-Algorithmus) und Entropiekodierung nach Huffman.

Viele Programme binden für die Deflate-Kodierung und -Dekodierung (Codec) die freie Deflate-Bibliothek zlib ein, welche ursprünglich extra für PNG geschaffen wurde.
Die Deflate-Komprimierung kann üblicherweise (wie auch in anderen Anwendungen – beispielsweise bei der ZIP-Kompression) im Ausgabeprogramm in 10 Stufen von 0 (keine) bis 9 (beste) eingestellt werden.

Bislang ist Deflate die einzige unterstützte Methode. Es ist aber absichtlich Raum für Erweiterungen gelassen worden, um in zukünftigen PNG-Versionen auch andere, effizientere oder schnellere Algorithmen zu unterstützen. Um Abwärtskompatibilität zu existierenden PNG-fähigen Programmen zu gewährleisten, ist derzeit eine Aufnahme anderer Verfahren in den Standard jedoch nicht geplant.

Bis auf Safari und Firefox (nach Aktivierung) unterstützt bisher kein Browser eingebettete Farbprofile (iCCP-Blöcke). Sie bieten daher kein vollständiges Farbmanagement. Dadurch, dass Safari als einziger Browser auch vollständige Farbprofile wiedergibt, ist eine einheitliche und plattformübergreifende Darstellung bei Bildern im PNG-Format mit eingebettetem Farbprofil zurzeit nicht möglich. Zumindest für den Browser Firefox ist diese Funktion aber für die Zukunft geplant. Eingebettete Gammakorrekturwerte (gAMA-Blöcke) hingegen werden von den meisten aktuellen Browsern fehlerfrei erkannt und verarbeitet.

Der Microsoft Internet Explorer hatte bis zur Version 6 Probleme mit der Darstellung von transparenten PNG-Dateien mit Alphakanal. PNG-Dateien mit binärer („ja/nein“-)Transparenz wurden jedoch fehlerfrei dargestellt. Es gibt aber Umgehungslösungen zur Nutzung des Alphakanals in älteren Versionen des Internet Explorers. Die neuesten Versionen der Browser Mozilla Firefox, Konqueror, Safari und Opera sowie der Internet Explorer ab der Version 7 unterstützen PNG jedoch weitgehend fehlerfrei.

Das PNG-Format unterliegt keiner Patentbeschränkung. Jeder Softwarehersteller kann daher ohne Zahlung von Lizenzgebühren Programme veröffentlichen, die PNG lesen und schreiben.



Mangels Animationsmöglichkeit stellt PNG für sich eine unvollständige Alternative zum GIF-Format dar.
Animationsmöglichkeiten sind bei PNG in ein eigenes Format ausgelagert. Die beiden Formate sollten zusammen GIF vollständig ersetzen; dazu kam es bis heute nicht.

Neben dem Farbpaletten-Modus unterstützt PNG zusätzlich Vollfarb- und Graustufenbilder mit einer Präzision von bis zu 16 Bit pro Farbkanal.
Bei GIF ist es nur möglich, einen einzigen der Paletteneinträge als vollständig transparent zu deklarieren.

Die Kompressionsrate von PNG ist meistens besser als bei GIF.
Die Vorfilter sind der Grund für den meist geringeren Platzbedarf von PNG-Dateien.
Dagegen ist jedoch auch die Komplexität des Formats höher.

Bei PNGs progressivem Bildaufbau mit dem Adam7-Interlacing treten deutlich weniger Bildverzerrungen auf als bei GIF.

Bis 2004 war die Nutzung des GIF durch die Verwendung des patentbelasteten LZW-Algorithmus’ beschränkt, wovon PNG von Anfang an frei war. (GIF war bis Oktober 2006 noch von weiteren Patenten beschränkt.)


Weder MNG, JNG, noch APNG oder PNG+ sind W3C-Empfehlungen.

Die offizielle Referenzimplementierung für das PNG-Format ist die Programmbibliothek "libpng". Sie unterliegt einer Freie-Software-Lizenz und findet sich daher üblicherweise auch als wichtige Systembibliothek in freien Betriebssystemen.
PNGOUT und ZopfliPNG sind bekannte auf kompakteste Komprimierung optimierte PNG-Kodierer. Es existiert eine Reihe von Hilfsprogrammen zur Dateigrößenoptimierung für PNG-Dateien, die maßgeblich wegen ihres Einsatzes für die Optimierung von Webseiten-Ladezeiten bekannt sind und oft unter anderem auf diesen beiden Spezialkodierern basieren.
Viele Programme speichern PNG-Bilder nicht optimal ab, was zu unnötig großen Dateien führt. Verschiedene Programme wie beispielsweise pngcrush, OptiPNG oder andere ermöglichen eine verlustfreie Neukomprimierung und oft wesentlich kleinere Dateien.

Mittlerweile unterstützen alle modernen Webbrowser das Format. Wichtige Bildbearbeitungsprogramme unterstützen es üblicherweise lesend wie auch schreibend.




</doc>
<doc id="4055" url="https://de.wikipedia.org/wiki?curid=4055" title="Protolyse">
Protolyse

Die Protolyse (oder auch protolytische Reaktion) ist eine chemische Reaktion, bei der ein Proton (H-Ion) "zwischen zwei Reaktionspartnern übertragen wird". Mit dem Begriff Proto"lyse" wird häufig fälschlich die Abspaltung von Protonen beschrieben. Die Protolyse ist der entscheidende Vorgang nach der wichtigen Brønstedschen Säure-Base-Theorie. Danach überträgt eine Säure ein Proton (H) an einen Reaktionspartner. Die als Säure bezeichnete Verbindung wirkt als Protonenspender ("Protonendonator"), die Base (häufig Wasser) nimmt die Protonen auf und wird daher als "Protonenakzeptor" bezeichnet. Zwischen den Reaktionspartnern stellt sich ein chemisches Gleichgewicht ein.

Wird das Gas Chlorwasserstoff (HCl) in Wasser eingebracht, bildet sich unter Protolyse die Salzsäure. In dieser Gleichgewichtsreaktion sind das Molekül HCl und das Ion HO Protonendonatoren, also nach Brønsted "Säuren". HO und Cl wirken als Protonenakzeptoren, sie sind nach Brønsted also "Basen".

Wird beispielsweise reine Essigsäure (HC–COOH) in Wasser gegeben, bilden sich unter Protolyse HO und das Acetat-Anion (HC–COO). Hier sind CHCOOH und HO Protonendonatoren, während HC–COO und HO Protonenakzeptoren sind.

Protolyse der zweiprotonigen Verbindung Schwefelsäure in Wasser:

In dieser Reaktionsgleichung sind die Moleküle HSO und das Ion HO Protonendonatoren, also nach Brønsted "Säuren". HO und SO wirken als Protonenakzeptoren, sie sind nach Brønsted also "Basen". Eine besondere Rolle spielt HSO, das je nach Reaktionsrichtung als Protonenakzeptor oder Protonendonator reagieren kann. Man bezeichnet Substanzen mit solchen Eigenschaften als Ampholyte.

Wird das Gas Ammoniak (NH) in Wasser eingeleitet, bilden sich Ammonium-Ionen (NH) und Hydroxid-Ionen (OH). Protonendonatoren sind hier NH und HO, während OH und NH Protonenakzeptoren sind.

Reines Wasser unterliegt einer sogenannten "Autoprotolyse" (auch "Autodissoziation"). Hierbei entstehen Oxoniumionen (HO) und Hydroxidionen (OH). HO kann sowohl als Protonendonator (als Säure) oder als Protonenakzeptor (als Base) reagieren. Man spricht daher auch hier von einem Ampholyten.

Das Gleichgewicht liegt sehr stark auf Seite des Wassers. Das Ionenprodukt für diese Reaktion beträgt bei 298 K (25 °C) etwa 10 mol l. Die Autoprotolyse des Wassers ist der Grund dafür, dass auch chemisch reines Wasser eine zumindest geringe elektrische Leitfähigkeit besitzt. Eine Anwendung der Autoprotolyse zur elektrischen Ladungstrennung findet sich bei dem Kelvin-Generator, wenn dieser mit chemisch reinem Wasser betrieben wird.

Die Autoprotolyse (und damit der pH-Wert) ist stark temperaturabhängig. So betragen die Ionenprodukte von Wasser (in mol l):

In Brønstedschen Säure-Base-Reaktionen können neben Wasser auch andere hinreichend polare Lösungsmittel als Reaktionspartner dienen, zum Beispiel Methanol oder Ethanol. Ein gutes Beispiel ist die Autoprotolyse des flüssigen Ammoniaks. Es bilden sich die Ionen Ammonium und Amid.

Ionenprodukt = 10

Auch in konzentrierter Schwefelsäure sind analoge Reaktionen bekannt:

Ionenprodukt = 10

Ebenso von Fluorwasserstoff:

Ionenprodukt = 10 (0 °C)



</doc>
<doc id="4057" url="https://de.wikipedia.org/wiki?curid=4057" title="Pfalz">
Pfalz

Pfalz steht für:

geografisch:

historisch:

Sonstiges:

Pfalz ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="4060" url="https://de.wikipedia.org/wiki?curid=4060" title="Phase Alternating Line">
Phase Alternating Line

Das -Verfahren [], kurz PAL, ist ein Verfahren zur Farbübertragung beim analogen Fernsehen. Es wurde mit dem Ziel entwickelt, störende Farbton-Fehler, die im NTSC-Verfahren nur manuell und unbefriedigend ausgeglichen werden können, automatisch zu kompensieren. Grundlage des Verfahrens ist der Gedanke, dass zwei aufeinander folgende Bildzeilen mehr Ähnlichkeit als Unterschied aufweisen, weil Bilder aus Flächen bestehen. Der technische Kniff, das rote Farbdifferenzsignal jeder zweiten Bildzeile zur vorhergehenden um 180° phasenverschoben (darum der Name) zu übertragen, ermöglicht es, auf der Empfängerseite durch Verrechnung der beiden Zeilen einen eventuell auftretenden Farbton-Fehler vollständig aufzuheben, lediglich ein kleiner Farbsättigungs-Fehler bleibt. Ein Fehler der Farbsättigung ist für den Menschen allerdings wesentlich schwerer wahrzunehmen als ein Farbtonfehler. Dadurch, dass jeweils 2 Bildzeilen zur Farbinformationsgewinnung herangezogen werden, reduziert sich die vertikale Farbauflösung auf die Hälfte. Da die räumliche Auflösungsfähigkeit des menschlichen Sehsinnes für Farbinformationen gegenüber derjenigen für Helligkeitsinformationen jedoch geringer ist, nimmt man diesen Nachteil in Kauf.

PAL wird vor allem in Europa benutzt, aber auch in Australien und vielen Ländern in Afrika, Asien und Südamerika. Details siehe im Abschnitt "Verbreitung."

Umgangssprachlich wird der Begriff PAL häufig für die Gesamtheit aller Parameter der Fernsehnorm verwendet.
Die Anfänge des Fernsehens waren unbunt. Es wurden nur die Helligkeitswerte des Bildes übertragen, keine Farben. Um bereits vorhandene Schwarz-Weiß-Fernsehapparate nach Einführung des Farbfernsehens weiterhin nutzen zu können, wurden die Farbfernsehsysteme abwärtskompatibel entwickelt. Mit einem Schwarz-Weiß-Fernseher konnte man bei geringfügig verschlechterter Bildqualität auch Farbausstrahlungen, auf einem Farbfernseher auch Schwarz-Weiß-Ausstrahlungen empfangen.

PAL wurde Anfang der 1960er Jahre von Walter Bruch bei der Telefunken GmbH in Hannover entwickelt, zum Patent angemeldet und am 3. Januar 1963 erstmals vor Experten der Europäischen Rundfunkunion (EBU) vorgeführt.

Auf die Frage, warum er dem unter seiner Leitung entwickelten Verfahren den Namen „PAL“ gab, antwortete er sinngemäß: „Ein Bruch-System wäre wohl schwer verkäuflich gewesen.“

Das Farbfernsehen in der Bundesrepublik Deutschland wurde auf der 25. Großen Deutschen Funkausstellung in West-Berlin durch den Vizekanzler der Bundesrepublik Deutschland Willy Brandt am 25. August 1967 mit einem Druck auf einen roten Knopf gestartet. In dieser Szene wurde die Farbe wenige Sekunden zu früh zugeschaltet, da der rote Knopf lediglich eine Attrappe war. Um 9.30 Uhr übertrugen die Fernsehsender ARD und ZDF die Begrüßungsmoderation durch Edith Grobleben vom Sender Freies Berlin (SFB) in Farbe.

Bereits am 5. August 1967 entschied sich die Schweiz für die Einführung des PAL-Farbfernsehsystems, begann aber erst später mit der Übertragung in Farbe.

Als ein möglicher, abwärtskompatibler Nachfolger und Zwischenschritt zum digitalen Fernsehen wurde PALplus in den 1990er Jahren entwickelt, hat sich jedoch nicht weit verbreitet.

PAL hat die grundlegenden Konzepte der Signalübertragung vom amerikanischen Farbübertragungssystem NTSC übernommen. Es benutzt, wie NTSC, die Quadraturamplitudenmodulation für die Farbübertragung. Als Verbesserung treten die bei NTSC-Übertragung typischen Farbartschwankungen nicht mehr auf, allerdings wird dies mit erheblichem Mehraufwand bei der Schaltung und (meist kaum) sichtbaren Schwankungen in der Farbsättigung erkauft. Es kann jedoch bei beiden Systemen zu Cross-Color- und Cross-Luminance-Störungen kommen, die sich als störende farbige Muster (Moiré-Effekt) oder als Unruhe an Farbübergängen äußern. Moiré tritt besonders bei feinen Strukturen im Bild auf, zum Beispiel bei kleinkarierten Hemden. Mit erhöhtem Schaltungsaufwand können diese Störungen reduziert werden (Kammfilter). Zusätzlich verschlechtert sich bei PAL im Vergleich zu NTSC die vertikale Farbauflösung.

Das französische Farbfernsehsystem SECAM unterscheidet sich wesentlich stärker von NTSC als PAL.

Im PAL-Farbsystem selbst ist keine Zeilen- oder Bildfrequenz definiert, stattdessen gibt es verschiedene Normen. In Deutschland wird üblicherweise ein Videoformat mit 625 Zeilen pro Bild verwendet, welches eine Bildübertragungsrate von 25 Vollbildern pro Sekunde besitzt. Diese werden halbbildweise übertragen, d. h., es wird erst ein Halbbild mit 312½ ungeraden und dann ein Halbbild mit 312½ geraden Zeilen übertragen, was eine Halbbildfrequenz von 50 Hz ergibt, das sogenannte Zeilensprungverfahren. Dadurch erhält man bei geringer Bandbreite des Fernsehsignals ein flimmerarmes Bild. Das PAL-System überträgt die Fernsehnormen B, G, H, I und N. Einige osteuropäische Staaten, die ihre Fernsehnorm von SECAM D und K auf PAL umgestellt haben, verwenden PAL D/K, wobei es einige Ausnahmen gibt, in denen die Länder komplett auf PAL B/G umgestellt haben. In Brasilien wird PAL in Verbindung mit 525 Zeilen und 29,97 Bildern pro Sekunde (System M) und einer fast identischen Farbträgerfrequenz wie NTSC benutzt. Alle anderen Länder, die das Übertragungssystem „M“ benutzen, verwenden NTSC für das Farbfernsehen. In Argentinien, Paraguay und Uruguay wird PAL mit den normalen 625 Zeilen verwendet, jedoch mit einer Farbträgerfrequenz, die fast mit der für NTSC identisch ist. Diese Abart der PAL-Norm wird PAL-N und PAL-CN genannt.

Neuere PAL-Fernsehempfänger können fast alle PAL-Varianten (außer PAL-M und PAL-N) verarbeiten und korrekt wiedergeben. Viele davon können auch fehlerfrei SECAM darstellen, das in Osteuropa und im Nahen Osten verbreitet ist. Allerdings funktionieren sie im Regelfall nicht mit der Variante des SECAM-Systems, die in Frankreich verwendet wird; davon ausgenommen sind Geräte französischer Herkunft. Viele dieser neueren Geräte kommen auch problemlos mit NTSC-M-Signalen zurecht, die von Videorekordern, DVD-Spielern oder Spielkonsolen erzeugt werden und über die Videobuchse oder die SCART-Buchse ins Fernsehgerät eingespeist werden (sogenannte Basisband-Signale). Allerdings treten häufig Probleme auf, wenn es um die Verarbeitung von NTSC-Signalen geht, die von Fernsehstationen ausgestrahlt werden oder über Kabelnetze übertragen werden und die über die Antennenbuchse ins Fernsehgerät eingespeist werden (hochfrequent aufmodulierte Signale).

Kinofilme werden traditionell mit 24 Bildern pro Sekunde gedreht, dadurch ergibt sich auf PAL-Geräten eine Laufzeitverkürzung um 4 %, da PAL 25 Bilder in der Sekunde wiedergibt. Dieser schnellere Ablauf des Filmes (Fachbegriff: PAL-Beschleunigung) wird von Menschen kaum wahrgenommen, nur die damit einhergehende, etwa einen Halbton höhere Tonwiedergabe kann auffallen, wenn man zum Beispiel darin vorkommende Musikstücke schon von anderen Quellen (CDs, etc.) kennt.

PAL baut wie NTSC und SECAM auf dem Schwarz-Weiß-Fernsehen auf. Aus Gründen der Kompatibilität müssen die Farbkomponenten „versteckt“ innerhalb des S/W-Luminanzsignals (Grauwert) mit übertragen werden. Weil dieses bereits aus allen drei Farbkomponenten zusammengesetzt ist, reicht die Übertragung von zwei Farbdifferenzsignalen für Rot (R-Y) und Blau (B-Y) aus. Diese beiden Signale werden aus der Differenz von Farb- und Luminanzsignal (schwarz-weiß-Signal) gebildet. Im Empfänger können aus den drei Signalen R-Y, B-Y und Y die drei Farbsignale R, G und B wieder erzeugt werden. (Dies beschreiben die Artikel YUV und Farbübertragung.) Durch die additive Farbmischung können mit den drei Einzelfarben Rot, Grün und Blau alle anderen Farben zusammengesetzt werden, begrenzt durch den Farbraum der Farbbildröhre.

Ebenso wie NTSC verwendet PAL für die Übertragung der beiden Farbdifferenzsignale Rot minus Helligkeit (R-Y) und Blau minus Helligkeit (B-Y) die Quadraturamplitudenmodulation (QAM). Da bei der QAM der Träger unterdrückt ist, dieser für die Demodulation aber benötigt wird, wird er im Empfänger durch einen quarzgesteuerten Hilfsträgeroszillator neu generiert. Dieser wird durch den „Burst“, einer ca. 10 Perioden langen Schwingung, die auf der hinteren Schwarzschulter des FBAS-Signals übertragen wird, mit dem Sendersignal synchronisiert.

PAL korrigiert automatisch Phasenfehler auf dem Übertragungsweg, die zu einer falschen Farbdarstellung führen. Hierzu wird der R-Y-Anteil des Farbsignals nach jeder übertragenen Zeile um 180° phasenverschoben (also einfach „umgepolt“) und tritt dann im Farbartsignal mit Phasenverschiebungen von +90°, bzw. −90° auf (siehe Falschfarben). Die Information, welche Phasenlage das R-Y-Signal gerade hat, wird im Burst mit übertragen. Bei +90° ist die Phase des Bursts +135°, bei −90° entsprechend −135°. Das B-Y-Signal hat dabei immer die Phasenlage 0°.

Darstellung des FBAS-Signals bei PAL, eine Bildzeile. Der PAL-Burst befindet sich an Punkt 5.

"Phase Alternating Line" invertiert die Phase des Rot-Differenzsignals von Zeile zu Zeile. Im Empfänger werden, im Gegensatz zu NTSC, Farbtonfehler (die in diesen Systemen den häufig auftretenden elektrischen Phasenfehlern entsprechen) durch Mittelwertbildung des Farbsignals zweier benachbarter Zeilen automatisch kompensiert, wenn die Farbe und der Farbtonfehler zwischen beiden Zeilen konstant sind, und in einen geringen Farbsättigungsfehler umgewandelt. Farbsättigungsfehler fallen dem menschlichen Auge wesentlich weniger auf als Farbtonfehler. Dies ist der entscheidende Vorteil des PAL-Verfahrens gegenüber NTSC.

Stellt man sich die analoge Quadraturamplitudenmodulation (QAM) im Zeigerdiagramm vor (siehe Abbildung), so steckt beim jeweiligen Zeiger in der Phase (Richtung) die Farbart (der Farbton), in der Länge des Zeigers der Farbkontrast (die Farbsättigung). Die beiden Farbsignale R-Y und B-Y werden dabei im Sender zueinander um 90 Grad verschoben, dann auf den Farbhilfsträger mittels QAM moduliert und als ein Signal übertragen. Treten Phasenfehler auf, würden sich diese daher bei einer einfachen Demodulation wie bei NTSC als Farbtonfehler zeigen. Jedoch wird bei PAL in jeder zweiten Zeile der Träger der Rotkomponente (R-Y) um 180 Grad gedreht, die Blaukomponente (B-Y) wird ohne laufenden Phasensprung übertragen. Von diesem Prinzip leitet sich auch der Name "PAL" ab. Bei der Demodulation wird diese Phasendrehung entsprechend kompensiert und damit ein eventuell aufgetretener Phasenfehler (Farbtonfehler) über zwei aufeinanderfolgende Zeilen weggemittelt.


Dabei geht man davon aus, dass sich von Zeile zu Zeile die Farbinformation nur wenig ändert und der zu verdeckende Farbfehler sich ebenfalls von Zeile zu Zeile wenig ändert.

Über diesen Voraussetzungen wird der Farbtonfehler 1. Ordnung in einen Farbsättigungsfehler 2. Ordnung umgewandelt, der vom Auge wesentlich schwieriger wahrzunehmen und daher vernachlässigbar ist.

Da zur Dekodierung des PAL-Signals jeweils die Information der aktuellen sowie der vorherigen Zeile benötigt werden, durchläuft das eingehende PAL-Signal im Empfänger eine Verzögerungsleitung mit einer Laufzeit knapp von der Länge einer Fernsehzeile (63,943 μs) zur Speicherung. Ausgegeben wird dann jeweils ein Mittelwert zwischen dem gerade einlaufenden und dem aus der vorigen Bildzeile gespeicherten Signal.

Nachteilig ist jedoch, dass sich dabei die Farbinformation um eine halbe Zeile nach unten verschiebt, was besonders unangenehm bei mehrfach kopierten Videokassetten auffällt, da bei jedem Kopiervorgang eine weitere Verschiebung entsteht.

Moderne (digitale) PAL-Decoder arbeiten wesentlich aufwändiger:

Zum Verständnis der PAL-Farbträgerfrequenzwahl wird zuerst die einfachere Wahl bei NTSC erklärt:

Die Farbträgerfrequenz wurde so gelegt, dass das durch sie hervorgerufene Stör-Moiré (vor allem auf den bereits existierenden Schwarz-Weiß-Empfängern) möglichst unauffällig ist und gleichzeitig feinstrukturierte Helligkeitsinformationen (feinkarierte Hemden im Bild u. ä.) möglichst wenig störende Farbbilder verursachen. Zugleich darf jedoch auch das Tonsignal nicht gestört werden.

Dazu wird:

Das ergibt dann 4,5 MHz / 286 * 227,5 Perioden = 3,57954545 MHz für den Farbträger bei der NTSC-Farbmodulation. Es werden etwa 1,3 MHz des unteren Seitenbandes und 0,4 MHz des oberen Seitenbandes davon übertragen. Durch die Natur des Farbsignals treten dabei ganz bestimmte Frequenzen in diesen Seitenbändern wesentlich stärker auf als andere; im Empfänger genügt es diese Frequenzen aus dem Schwarz-Weiß-Bild wieder „herauszufischen“, um eine möglichst saubere Trennung von Helligkeits- und Farbinformation zu erreichen.

Die Farbträgerfrequenz wurde so gelegt, dass das durch sie hervorgerufene Stör-Moiré möglichst unauffällig ist und gleichzeitig feinstrukturierte Helligkeitsinformationen möglichst wenig störende Farbbilder verursachen.

Dazu wird:


Das ergibt dann 15625 Hz * 283,75 Perioden + 25 Hz = 4,43361875 MHz für den Farbträger bei der PAL-Farbmodulation. Es werden etwa 1,3 MHz des unteren Seitenbandes und 0,65 MHz des oberen Seitenbandes davon übertragen. Die Farbträgerfrequenz wird üblicherweise im Empfangsgerät durch einen vom Fernsehsender nachsynchronisierten Quarzoszillator erzeugt. Dieser Oszillator wird durch den Burst in Frequenz und Phase an den Oszillator beim Sender angeglichen. Damit steht in jedem Fernsehgerät eine stabile, hochkonstante Referenzfrequenz zur Verfügung.

Die verwendete Frequenz wird teilweise auch zur Basisband-Übertragung von NTSC verwendet und heißt dann NTSC-4.43. Dieses Verfahren wird häufig mit PAL 60 verwechselt, unterscheidet sich jedoch darin, dass der Farbunterträger seine Phasenlage nicht ändert. Ein PAL 60 können auch die meisten neueren PAL-Fernseher problemlos anzeigen, weshalb es beispielsweise genutzt wird, um NTSC-DVDs auf einem PAL-Fernsehgerät wiederzugeben. Die Störunterdrückung des Farbträgers (Trägerfrequenz ist das 281,78-fache der Zeilenfrequenz, was nicht mehr halbzahlig ist) ist dann allerdings nicht mehr optimal.

Die Mittelung von benachbarten Zeilen bei der Dekodierung ist bei PAL, im Gegensatz zu SECAM, nicht prinzipiell notwendig. Man kann jede Zeile auch für sich unabhängig dekodieren. Die Korrektur von Farbtonfehlern funktioniert bei geringen Fehlern immer noch ordentlich, die Mittelung wird bei geringem Farbtonfehler (wie man sie heutzutage durch Kabelfernsehen und andere phasenfestere Übertragungsmethoden häufig antrifft) problemlos durch das menschliche Auge übernommen. Die vertikale Auflösung verringert sich dabei (gegenüber der Variante mit Zeilenmittlung) nicht. Die Gerätehersteller können auf diese Weise die PAL-Lizenzen umgehen. Bei der Farbübertragung von PAL via Y/C (Hosidenverbindung, S-Video), also mit getrenntem Helligkeits- und Farbsignal, ist auch eine größere Farbbandbreite möglich, da es keine Beschränkung auf 1,3 MHz Bandbreite mehr gibt. Hiervon wird jedoch kaum Gebrauch gemacht.

Die derzeit neueste Version des Standards, der das PAL-System (und auch das NTSC-System) definiert, wurde 1998 von der Internationalen Fernmeldeunion ("International Telecommunications Union" - ITU) publiziert und hat den Titel „Recommendation ITU-R BT.470-6, Conventional Television Systems“.

Beachte:
In vielen Ländern stirbt die Farbnorm aus. PAL gibt es in Deutschland nur noch in Kabelnetzen.
In den Kopfstationen wird SDTV über DVB in analoges PAL-Fernsehen umgesetzt, um Fernsehgeräten ohne DVB-C
den Empfang zu ermöglichen. Eine ähnliche Situation liegt vor, wenn DVB-C verschlüsselt übertragen wird.

In Anlehnung an die in Europa geprägten falschen Übersetzungen („Nie die gleiche Farbe“) und („Seit Christi Geburt nie getestet“) für die amerikanische Farbnorm NTSC revanchierten sich die US-Amerikaner mit den ebenso falschen, scherzhaften Übersetzungen („Bezahle den zusätzlichen Luxus“) sowie („Bezahle eine weitere Lizenz“) für die europäische Farbnorm PAL.

Diese bezogen sich auf den größeren Schaltungsaufwand und den deshalb höheren Preis der PAL-Farbfernseher zu Beginn des Farbfernsehzeitalters. Europa schlug mit den Aufschlüsselungen („Endlich Frieden“) und („Endlich Perfektion“) für PAL zurück, die wieder auf die schlechte Qualität der NTSC-Norm verwiesen.

Alles bisher Beschriebene bezieht sich auf den Begriff PAL bei der analogen Signalübertragung, also zum Beispiel beim Analogfernsehen und bei Videorekordern. Bei digitalen Formaten, etwa beim Digitalfernsehen, neueren Spielkonsolen oder auf einer DVD, wird die Farbkodierung, die mit der Analog-Eingangsbuchse des Ziel-Fernsehers kompatibel ist, erst im Abspielgerät erzeugt; sie ist nicht auf dem Medium selbst gespeichert. Auf diesem werden die Farbinformationen, unabhängig ob digitales PAL/SECAM oder digitales NTSC, stets mittels des digitalen Farbmodells YCbCr kodiert.

Zwischen PAL und SECAM besteht auf einem digitalen Medium kein Unterschied mehr – ein PAL-DVD-Spieler erzeugt aus einer „PAL-DVD“ ein analoges PAL-Videosignal, ein SECAM-DVD-Spieler aus der gleichen DVD ein analoges SECAM-Videosignal. Dieses wird auch nur zur Ansteuerung per FBAS/Composite Video/RCA oder S-Video/YC/Hosiden-Anschluss benutzt. Am RGB/SCART oder YPbPr-Component-Video-Anschluss oder über digitale Schnittstellen (DVI, HDMI) findet keine Wandlung in YUV (analoges PAL), YDbDr (analoges SECAM) oder YIQ (veraltet, früher bei analogem NTSC verwendet) mehr statt.

Wenn eine digitale Signalverarbeitung oder Speicherung des analogen Videosignals stattfindet (etwa bei moderneren analogen Fernsehern), existiert dabei allerdings schon eine digitale Repräsentation der PAL-Farbkodierung. Dabei wird das analoge Signal mit der vierfachen Farbträgerfrequenz abgetastet. Die Abtastung geschieht synchron zum Farbträger. Durch Addition und Subtraktion nahestehender Abtastwerte erhält man das Farbdifferenzsignal. Dieses Verfahren wird besonders intern in videoverarbeitenden Geräten benutzt. Digitale Fernsehgeräte arbeiten hier häufig mit einer 7- oder 8-Bit-genauen Abtastung (Analog-Digital-Wandlung), bessere Geräte verwenden bis zu 10 Bit. Frühe digitale Videorekorder (zum Beispiel D2) nutzten ebenfalls dieses Verfahren.

PAL bezeichnet im Digitalbereich, losgelöst von der Bedeutung des Akronyms, alle Bildformate mit einer Bildauflösung von 576 sichtbaren Zeilen je Vollbild (ggf. auch 288) bei 25 Vollbildern pro Sekunde; die horizontale Auflösung variiert. Heutige Bezeichnungen (nach EBU) sind bei Verwendung des Zeilensprungverfahrens 576i/25, bei Vollbildern 576p(sf)/25 (es wird in jedem Fall in „Bildern“ gezählt, nicht in „Feldern“). Technisch gesehen ist 576p immer „psf“ (progressive segmented frame), der Einfachheit spricht man von 576p (es gibt hier keine tatsächliche Progressive Kodierung, wie beispielsweise bei 720p).

Den Gegenpart zu „PAL“ bildet hier wiederum „NTSC“, das auf digitalen Medien eine Auflösung von 480 (bzw. 486) Zeilen je Vollbild bei entweder 29,97 bzw. 30, oder (für Spielfilme) 23,976 bzw. 24 Vollbildern pro Sekunde bedeutet, wobei die Farbinformationen auf dem Medium ebenfalls YCbCr-kodiert gespeichert sind. Fast alle PAL-DVD-Spieler können jedoch aus NTSC-Medien ein PAL-60 genanntes PAL-ähnliches Signal erzeugen, mit dem fast alle neueren PAL-Fernsehgeräte problemlos zurechtkommen.

Die horizontalen Auflösungen beziehen sich auf das PAL-System in seiner digitalisierten Darstellung, welche mit Pixeln arbeitet wie es z. B. im ITU-R BT 601-Standard festgelegt ist. Dort besteht eine digitale Zeile aus nicht-quadratischen Pixeln.

Digital entspricht im PAL-System ein 4:3-Bild einer Auflösung von 702×576 wobei allerdings typischerweise 720×576-Bilder gespeichert werden. (siehe Artikel CCIR 601 zur Entstehung der 702 Pixel)

Falls das gewünschte Ausgabemedium mit (idealisierten) quadratischen Pixeln arbeitet (z. B. ein an einen PC angeschlossener Monitor), muss dies entsprechend berücksichtigt werden, indem das Seitenverhältnis idealerweise umgerechnet wird.

Siehe auch Pixelseitenverhältnis.

Nach dem Umrechnen auf quadratische Pixel (z. B. am PC) ergeben sich proportional korrekt:

In vielen Medien wird dies vielfach falsch erklärt und weitergegeben, auch viele Softwarepakete rechnen hier falsch. Zum Beispiel rechnet Adobe After Effects und Photoshop erst ab der Version CS4 korrekt, in früheren Versionen wurde mit dem gängigen, aber falschen Pixelseitenverhältnis (PAR) gerechnet.

Bei nicht-quadratischen Pixeln ist das Seitenverhältnis "(Aspect Ratio "oder" AR)" des Bildes (z. B. 4:3) nicht identisch zum Verhältnis der horizontalen zur vertikalen Pixelzahl (z. B. 11:9 bei 704 × 576).
Daher muss neben der reinen Pixelzahl zusätzlich entweder das Seitenverhältnis der Pixel oder das des Gesamtbilds angegeben werden. Es muss also eindeutig sein, ob es sich um das Pixelseitenverhältnis "(PAR)" oder das Bild-Seitenverhältnis "(DAR)" handelt. Nur dann kann ein verzerrungsfreies Bild angezeigt werden.

Üblich sind:





</doc>
<doc id="4063" url="https://de.wikipedia.org/wiki?curid=4063" title="Persönliche Identifikationsnummer">
Persönliche Identifikationsnummer

Eine Persönliche Identifikationsnummer (PIN) oder Geheimzahl ist eine nur einer oder wenigen Personen bekannte Zahl, mit der diese sich gegenüber einer Maschine authentisieren können. Häufig werden auch das redundante Akronym PIN-Nummer oder die Bezeichnung PIN-Code verwendet. Im engeren Sinne sind PINs numerische Passwörter.

Bei der Karte ohne Chip erfolgt diese Verarbeitung nach dem Lesen der Daten von der Karte ausschließlich in einer geschützten Umgebung, bei Karten mit Chip leistet diese zusätzlich einen durch die Verbindung zum Leser geschützten Beitrag.

Eine häufige Anwendung für PINs ist die Authentifizierung an einem Geldautomaten. Hier ist die Eingabe einer mindestens vierstelligen Ziffernfolge nötig, um einen Kontozugriff durch unbefugte Personen zu verhindern oder zumindest zu erschweren. Auch kann man mit der Bankkarte und der zugehörigen PIN in vielen Geschäften bargeldlos zahlen.

Auch für das Internetbanking ist zumeist eine PIN nötig. Mit dieser PIN und den Kontodaten kann man sich sein Konto, den Kontostand und die letzten Buchungen ansehen. Mit einer TAN kann man dann eine Überweisung tätigen oder andere Bankgeschäfte abwickeln.

PINs werden auch zum Schutz von Mobiltelefonen vor unberechtigter Nutzung und in vielen weiteren Anwendungsgebieten der Technik verwendet, wo ein Mindestmaß an Sicherheit erforderlich ist. SIM-Karten für Mobiltelefone werden mit einer PIN, PIN2, PUK und PUK2 ausgeliefert. Alle Codes sind auf der SIM-Karte gespeichert. PINs sind veränderbar, PUKs nicht. Die PUKs dienen für die Entsperrung gesperrter PINs. Die PIN2 wird zum Ändern von speziellen oft kostenpflichtigen Diensten verwendet.

Das Bundesamt für Sicherheit in der Informationstechnik hat im Jahr 2001 ein neu eingeführtes PIN-Verfahren begleitet. Die PIN ist neben der Kontonummer, der Bankleitzahl und anderen Daten – in besonders verschlüsselter Form auf den EC-Karten enthalten. Mit der Tastatur des Geldautomaten ist ein so genannter Krypto-Prozessor verbunden, der die PIN zur sicheren Übertragung entschlüsselt. Zur vorangegangenen Verschlüsselung nutzen die Kreditinstitute das Verschlüsselungsverfahren Data Encryption Standard (DES). Bei der einfachen Variante des DES werden die zu verschlüsselnden Informationen in Textblöcke zu je 64 Bit umgewandelt. Dann werden die Zeichen innerhalb eines Blocks mehrfach vertauscht und addiert. Diese Zahlendaten werden außerdem geteilt und zur Sicherheit weitere 16 Male chiffriert. Schon die alte Variante war damals nur durch Profis zu knacken – aufgrund der rasanten Entwicklung der Computertechnik war es jedoch möglich. Seit 1997 benutzen die Banken nunmehr den Triple-DES, eine Variante mit noch längeren Verschlüsselungsketten, die jedenfalls für die Gerichte als sicher gilt. Letztendlich ist es allerdings gar nicht unbedingt nötigt, ein volles Triple-DES zu knacken. Insider scheinen Angriffstechniken gegen das Verfahren entwickelt zu haben bei denen sie z. B. durch Kompromittierung der Terminals auch ohne Triple-DES zu knacken an die PIN kommen.

Konkret passiert also Folgendes: Der Geldautomat liest die verschlüsselte PIN von der Karte. Er entschlüsselt diese mit Hilfe eines Krypto-Prozessors und des im Geldautomaten hinterlegten Institutsschlüssels. Schließlich vergleicht er das Ergebnis mit der eingegebenen Ziffernfolge. Im Falle der Übereinstimmung sind die weiteren Transaktionen freigegeben (z. B. Abhebung), andernfalls nicht.

Kartenterminals und Kassen, wie sie im Einzelhandel, der Gastronomie und in anderen Branchen vorkommen, verwenden dasselbe Prinzip, um Kartenzahlung mit PIN zu autorisieren.

Es ist deshalb auch mit größtmöglichem finanziellen Aufwand mathematisch ausgeschlossen, die PIN ohne vorherige Erlangung des Institutsschlüssels in einer Breite von 118 Bit zu ermitteln. Ein Betrüger, der eine Maestro-Karte (ehem. EC-Karte) gefunden oder gestohlen hat, wird probieren, damit an einem Automaten Geld abzuheben. Auch wenn er die PIN nicht kennt, kann er versuchen, sie zu raten. Bei der vierstelligen Maestro-Karten-PIN aus numerischen Ziffern ist die Wahrscheinlichkeit, dass der Betrüger die PIN mit einem Versuch raten kann, 1/10000 (Bei Ziffern von 0–9 ergeben sich für jede Ziffernstelle 10 Möglichkeiten – bei 4 Stellen folgt daraus: 10=10.000). Da jedoch im Allgemeinen bis zu drei Versuche erlaubt sind, hat der Betrüger eine Wahrscheinlichkeit von etwa 
die richtige PIN zu raten.
Allgemein lässt sich die Ratewahrscheinlichkeit nach folgender Formel berechnen, wobei formula_2 gleich der Anzahl der möglichen PIN-Kombinationen ist und der Betrüger bei zwei bzw. drei Versuchen die falschen PINs nicht wiederholt ausprobieren wird:

Wenn beim Online-Banking eine fünfstellige PIN verwendet wird mit ausschließlicher Nutzung von Ziffern, ergibt sich somit (unter Vernachlässigung der Beschränkung von Kombinationen) eine Ratewahrscheinlichkeit von ungefähr 1 zu 33 Tsd. (wegen formula_4). Falls jedoch auch Kleinbuchstaben verwendet werden, verringert sich die Wahrscheinlichkeit bei zehn Ziffern und 26 Buchstaben auf ungefähr 1 zu 20 Mio. 

Damit eine PIN nicht durch wiederholtes Ausprobieren zu erraten ist (sog. Enumerations-Angriff), darf ein durch PIN geschütztes System nicht beliebig viele falsche Eingaben der PIN akzeptieren. Insbesondere bei Online-Formularen könnte ein Angreifer sonst einfach automatisch alle möglichen PIN durchprobieren.

Die meisten Systeme sperren daher nach einer bestimmten Anzahl von Falscheingaben der PIN den Zugang, der dann auf anderem Wege (meist durch eine weitere PIN oder durch den Kundendienst des Anbieters) entsperrt werden muss.
Bei Geldautomaten, beim Online-Banking und bei Mobiltelefonen erfolgt die Sperre üblicherweise nach drei Falscheingaben.

Bemerkung: Die PIN auf dem Magnetstreifen erlaubt lediglich eine Übertragung zur unidirektionalen Prüfung oder zum erneuten Schreiben. Heute tragen Bankkarten meist neben dem Magnetstreifen zusätzlich einen Chip, der eine dynamische Prüfung der PIN durch eine bidirektionale Verbindung ermöglicht. Das unterstützt einen komplexeren Schutz.

PINs unterliegen bei der Eingabe dem Angriff durch Skimming, einer betrügerischen Handlung.

Viele international herausgegebene Kreditkarten sind mittlerweile mit bis zu sechsstelligen PINs gesichert, in Europa betrifft dies zum Beispiel einen großen Teil der Schweizer Kreditkarten. Häufig ergeben sich hierbei im internationalen Umgang mit Zahlungssystemen Probleme, denn die meisten Händler akzeptieren nur noch Kreditkarten-Einsätze mit PIN-Code. Sollte das Lesegerät des Händlers fest auf vierstellige PINs programmiert sein und keine Möglichkeit zur Eingabe von 6 Ziffern bieten, kann so mit der Karte nicht bezahlt werden; dies ist beispielsweise bei vielen niederländischen Fahrkartenautomaten und einem Großteil der POS-Systeme der Fall.

Auffällig ist die Häufung dieses Problems bei POS-Systemen mit zeilenbasierten LC-Displays. Systeme wie beispielsweise Geldautomaten mit großen LC-Displays schalten bei Einführung einer Kreditkarte meist in einen Modus, der die Eingabe PINs beliebiger Länge erlaubt.

Abhilfe für den Konsumenten schafft hier meist, sofern möglich, die Änderung der PIN auf vier Stellen im Ursprungsland.

Das Bundesamt für Sicherheit in der Informationstechnik (BSI) empfiehlt generell, als PIN nur zufällige Abfolgen von Zeichen aus dem zulässigen Zeichensatz zu benutzen; PINs wie z.B. „0000“ oder „1234“ sollten dringend vermieden werden. Folgende Tabelle kann als Anhaltspunkt für eine sichere PIN-Auswahl gelten:



</doc>
<doc id="4065" url="https://de.wikipedia.org/wiki?curid=4065" title="Priester">
Priester

Der Priester (lat.: ) existiert in einem Großteil der Religionen als eine aus der Allgemeinheit herausgehobene Amtsperson, die in ihrer Eigenschaft als Vorsteher kultischer Handlungen eine Mittlerrolle zwischen jeweiliger Gottheit und den Menschen einnimmt.

Das Judentum und der Islam sehen keinen Mittler zwischen dem Menschen und Gott vor, folglich gibt es bei diesen Religionen keine Priester in diesem Sinne. Die jüdischen Tempelbediensteten und die islamischen Vorbeter (Imame) sind nur theologisch gebildete Bedienstete, die bestimmte Aufgaben bei den Gottesdiensten erfüllen. Im Islam können die fünf täglichen Gottesdienste sowohl in der Moschee als auch allein und zu Hause durchgeführt werden. Ein Vorbeter ist nur dann nötig, wenn mehrere Gläubige gemeinsam beten (gewissermaßen zur Synchronisation des Rituals); er soll ein Mindestmaß an theologischen Fertigkeiten besitzen.

Das deutsche Wort Priester stammt vom griechischen πρεσβύτερος, "presbyteros" ‚Ältester‘. Davon abgeleitet sind auch die entsprechenden Wörter vieler europäischer Sprachen. Religionsphänomenologisch und soziologisch steht der Priesterbegriff jedoch im Bedeutungsfeld von griech. ἱερός, "hierós" ‚heilig, geweiht‘ und lat. "sacerdos" (von "sacer" ‚heilig, geweiht‘).

In fast allen Religionen gibt es Menschen, die durch besondere Kenntnisse, Fähigkeiten, Vollmachten und göttliche Kräfte eine Verbindung zwischen dem göttlichen Bereich und der Alltagswelt der Menschen vermitteln und dadurch als göttlicher Stellvertreter ordnen, heilend wirken oder Erkenntnisse gewinnen. Aus schamanischen Ursprüngen hat sich in den Hochkulturen in der Regel im Umfeld der Tempel ein Priesterstand mit genau geregelten Rechten und Pflichten entwickelt. Die jüdischen Tempelbediensteten, die Kohanim (Einzahl: Kohen) lassen sich nicht in den religionswissenschaftlichen Grundsatz einordnen. Sie sind und waren keine Mittler zwischen Menschen und Gott und haben entschieden keine göttlichen Kräfte. Dies im Unterschied zu anderen Religionen, die gerade priesterliche Vermittler zwischen Gott (bzw. Göttern) und den Menschen vorsehen und aus ihrer zwingenden Vermittlerrolle heraus die Existenzberechtigung herleiten.

Bei der Einordnung in das Typen-Modell religiöser Autorität ergeben sich für den Priester gewisse Überschneidungen zu anderen Typen, die, neben der allgemeinen Unschärfe des Modells, vor allem auf die Schwierigkeiten zurückzuführen sind, die sich ergeben, wenn der Begriff des Priesters aus den Mittelmeerreligionen auf vollkommen anders strukturierte Religionen beispielsweise aus Fernost oder Nordamerika übertragen wird.
In Gesellschaften, in denen es noch nicht zur Ausbildung eines Priesterstandes gekommen ist, aber auch in solchen, in denen dieser Schritt bereits vollzogen ist, gibt es gewisse Vorstufen zum Priestertum. So ist in der Regel der Hausvater oder das Oberhaupt der Sippe mit der Wahrnehmung sakraler Funktionen betraut. In archaischen Kulturen ist die Ausübung priesterlicher Aufgaben ursprünglich dem König vorbehalten, der sie aber mit der zunehmenden Weiterentwicklung und Differenzierung des religiösen Kultes an ihm untergeordnete Priester vergibt. Ein in solcher Weise ausgeprägtes Priestertum ist zuerst in Neolithikum und Bronzezeit im östlichen Mittelmeer zu beobachten. Das Gottkönigtum eines Pharao, Sohn, Abgesandter, Mittler und Nachfolger der Gottheiten, ist ein Beispiel.

Der Typ des Priesters ist in schriftlosen Kulturen zumeist von denen der Geisterbeschwörer – Medizinleute, Zauberer, Schamanen usw. – oftmals nicht klar zu trennen. Jedoch ist festzuhalten, dass Geisterbeschwörer charakteristischerweise mit unpersonalen Mächten oder Kräften zu tun haben, die sie beherrschen müssen, statt über den Kultdienst in einer personalen Beziehung zu einer Gottheit zu stehen "(siehe dazu: Abgrenzungsproblematik; Beispiel Schamane und Priester)."

Der Mönch schließlich hat ursprünglich zwar nicht die kultische Mittlerfunktion des Priesters, doch kann er, wie beispielshalber im Buddhismus, priesterliche Funktionen übernehmen und so aus seiner ursprünglichen Lebensweise in ein Priestertum hineinwachsen. Oftmals hat sich also das Priestertum erst aus dem Mönchtum entwickelt. Typologisch charakteristisch ist jedoch, dass der Mönch die göttliche Kraft oder Gnade aus seiner Lebensführung und nicht wie der Priester seines Amtes wegen erhält.

Die Aufgaben, die dem Priester zugeordnet sind, differieren je nach Religion. Grundsätzlich nimmt der Priester jedoch stets eine Mittlerfunktion zwischen dem Göttlichen und den Menschen ein. Dabei ist er wechselseitig mit Stellvertretung der Gottheit gegenüber den Menschen und der Menschen gegenüber der Gottheit betraut: Er tut den göttlichen Willen kund, bewahrt das heilige Wissen und vermittelt die göttlichen Gnadenerweise. Als Stellvertreter der Menschen handelt er in Opferungen und bei den Gebeten an die Gottheit.

Als Kultdiener vollzieht er die kultischen Handlungen zumeist in einem engen räumlichen Zusammenhang mit einem Tempel, Altar oder Naturheiligtum. Er opfert der Gottheit und leitet die Riten, verliest die heiligen Schriften und bewahrt den Kultort vor dem Eindringen religionsgesetzlich Unbefugter. Zusätzlich zu diesen beiden Bereichen treten diverse andere Aufgaben hinzu, die jedoch nicht genuin priesterlich sind. Dazu zählen psychische und medizinische Betreuung der Gläubigen, das Verkünden von Prophezeiungen oder Beschwörungen der Gottheit oder anderer Geistwesen. Darüber hinaus sind in vielen Religionen die Priester gleichzeitig Lehrer und Missionare und übernehmen administrative Aufgaben oder die Rechtsprechung.
Die Initiation der Priester erfolgt entweder über eine leibliche oder eine geistige Abfolge (Sukzession). In beiden Fällen ist wichtig, dass die Auswahl dabei nicht durch menschlichen Willen, sondern durch göttliche Kraft fällt. Bei der "leiblichen" Sukzession wird das Priesteramt innerhalb einer Familie vom Vater an den Sohn vererbt und weitergegeben. Der Vater weiht den Sohn in das priesterliche Wissen und eine eventuelle Geheimlehre ein. Die "geistige" Sukzession unterscheidet sich nur dahingehend, dass der Priester nicht durch Geburt, sondern durch eine besondere Weihe in das Priestertum aufgenommen wird und daher nicht in einer leiblichen, sondern über seinen „Weihevater“ in einer geistigen Ahnenreihe steht. Dabei werden die potentiellen Bewerber gezielt ausgewählt und im Hinblick auf ihre spätere Aufgabe erzogen, eventuell sogar in einer eigens dafür geschaffenen Institution. Die Ausbildung erstreckt sich dabei in erster Linie auf das Wissen um die korrekte Verrichtung des Kultes. Das Erlernen einer vielfach vorhandenen alten Kultsprache, des richtigen Ablaufs der verschiedenen Riten und der oft umfangreichen Gebetstexte steht im Vordergrund. Daneben ist die Priesterschaft einer Kultur aber oft auch ein Kulturträger ersten Ranges und wird in vielen anderen Bereichen zusätzlich ausgebildet. Dazu zählen bevorzugt Astronomie (Priesterastronom), Mathematik, Zeitrechnung, Medizin, Krankenpflege, Schrift, Kartographie und Geschichtsschreibung. Nicht selten waren Mönche, Äbte und Priester auch mit Erfolg technisch und naturwissenschaftlich tätig; mehrere wichtige Erfindungen und Entdeckungen gehen auf sie zurück, siehe etwa Roger Bacon, Nikolaus von Kues, Christoph Scheiner, Johann Adam Schall von Bell, Athanasius Kircher, Christophorus Clavius, Marin Mersenne, Kaspar Schott, Claude Chappe, Gregor Mendel und Sebastian Kneipp.

Der Standort des Priestertums innerhalb der Gesamtgesellschaft ist durch eine Reihe von Sonderstellungen gekennzeichnet. Auf der einen Seite können dazu Tabuvorschriften wie bestimmte Speisevorschriften, Reinheitsgebote, sexuelle Enthaltsamkeit und allgemein das Einhalten eines strengen Lebenswandels gehören. Die Vorschriften können auf einen bestimmten Zeitraum vor und während der Kulthandlung beschränkt oder aber auch dauerhaft sein. Andererseits genießen die Priester meist gewisse Vorrechte, haben oftmals auch einen rechtlichen Sonderstatus, der sich z. B. in der Steuerfreiheit, Nichtteilnahme an direkten Kriegshandlungen, oder der Immunität des Klerus äußert, und heben sich äußerlich (Amtstracht, Tonsur oder ähnliches) von den Laien ab.

Aus diesen Sonderregelungen für die Priesterschaft entwickelte sich das Priestertum in einer Gesellschaft oft zur abgeschlossenen Kaste fort, das sich streng hierarchisch geordnet nach unten abschloss: Dabei bilden sich vielfach innerhalb des Priestertums Rangklassen mit abgestuften Befugnissen oder Kenntnissen und an die Spitze des gesamten Priestertums stellte sich ein allgemeiner Oberpriester (Hohepriester) mit umfassender Leitungsgewalt. Prominenteste Beispiele hierfür sind der Papst in der römisch-katholischen Kirche oder der chinesische Kaiser.

Im antiken Römischen Reich war das Priestertum stark mit politischer Tätigkeit vermischt. Zahlreiche Politiker pflegten vor ihren Entscheidungen einen Priester zu konsultieren.

Die Priester wurden im Volksmund kurz als "sacerdotes", bei offiziellen Anlässen als "sacerdotes publici populi Romani Quiritium" („öffentliche Priester des römischen Volkes der Quiriten“) bezeichnet. Sie unterstanden dem Gewohnheits- und Sakralrechtswesen im antiken Rom. 

Zum römischen Priestertum gehörten die Sodalitäten der Fetialen, Salier, Arvalbrüder, Titii sodales und Luperci. Insbesondere die Arvalbrüder betrieben einen Kaiserkult. Die vier höchsten Priesterkollegien waren die Pontifices, die Augures, die Quindecimviri sacris faciundis und die Septemviri epulonum.

Der Titel des "Kohen" [] (hebräisch כהן) ist ein Status des Judentums. Ihr Status geht allein auf die Gebote Gottes zurück. Die Kohanim [] (hebräisch כהנים, Plural von "Kohen") sind eine Untergruppe der Leviten, des priesterlichen unter den Zwölf Stämmen Israels. Sie gelten als direkte Nachfahren des Aaron, eines Bruders des Mose. Die Kohanim übten im Jerusalemer Tempel den Tempeldienst am Altar aus. Der "HaKohen HaGadol" (Hohepriester, wörtlich „Großer Priester“) war die höchste religiöse Autorität des Judentums.

Die Kohanim sind keine Mittler zwischen jüdischen Menschen und Gott oder der Menschheit und Gott. Damit ist bis heute der Unterschied gegenüber anderen Religionen bestimmt, die Vermittler zwischen Gott (bzw. Göttern) und den Menschen vorsehen. Jeder Jude ist Gott direkt verantwortlich. Der Tempelkult hatte keine vermittelnde Funktion und – bis auf die Ausnahme der möglichen Sühne einer unbeabsichtigt begangenen Sünde – keine Sünden tilgende Funktion durch Opferung und Blut. Das Volk Israel – ein Königreich von Priestern – hat die Aufgabe, den am Sinai geschnittenen Bund oder „Vertrag“ einzuhalten. Davon hängt das Wohl jedes Juden bzw. Israeliten, des Volkes Israel, ja sogar das der Menschheit und der Erde ab. Im dritten Tempel Jerusalems wird der jüdische Mashiach das "Reinigungsopfer" oder "Sühnopfer" (Chatat) darbringen, um Sünden zu tilgen, die unabsichtlich begangen wurden.

Im Christentum ist der Priester aus der Allgemeinheit der Laien abgesondert, dauerhaft bestellt durch den Empfang der Priesterweihe. Das Dekret Presbyterorum ordinis Papst Johannes Pauls II. stellt dazu fest, dass die Priester „aus der Reihe der Menschen bei Gott bestellt“ würden, „um Gaben und Opfer für die Sünden darzubringen“. Die christlichen Konfessionen unterscheiden sich in ihrer Definition der priesterlichen Vollmacht und Aufgaben. Zwischen den römisch-katholischen, alt-katholischen und orthodoxen Traditionen stimmt das theologische Profil des Priesters in großen Teilen überein. Die protestantischen Konfessionen der Reformation haben sich vom Amtsverständnis des Priestertums distanziert und verwenden den Begriff meist nicht.

Ausgehend von der jüdischen Jerusalemer Urgemeinde Jesu Christi hatte das Judenchristentum noch keine besonderen Mittler zwischen Mensch und Gott. Nach dem Neuen Testament gilt für das daraus entstandene hellenistische Heidenchristentum (ab 49 n. Chr.) und die frühe Kirche: „Einer ist Gott, Einer auch Mittler zwischen Gott und den Menschen: der Mensch Christus Jesus“ (). Jesus Christus war zugleich Hohepriester und Knecht. Durch das Anwachsen der Heidenmission und der heidenchristlichen Gemeinden entstand die christliche Liturgie und gewannen die kirchlichen Ämter an Gewicht.

Im zweiten Jahrhundert bildete sich eine bis heute verbreitete dreigliedrige hierarchische Struktur heraus: Bischof, Ältester (presbyteros) und Diakon, wobei die heutigen Priester in die Kategorie der Presbyter einzustufen sind. Nach der konstantinischen Wende von 313 erlangten die Bischöfe eine hervorgehobene Rolle im Reich; dadurch war eine Abgrenzung von den Priestern gegeben, die aber nicht immer genau festgelegt wurde. Hinzu kam die Kategorie der Mönche, die oft, aber nicht immer, die Priesterweihe empfangen hatten. Bischöfe wurden meist aus den Reihen der Priestermönche gerufen. Im Mittelalter unterschieden sich in der Westkirche Bischofs- und Priesteramt durch ihre Kompetenzen: Nur ein Bischof kann die Weihen und im Normalfall die Firmung spenden, die übrigen Sakramente kann der Priester spenden.

"→ Hauptartikel: Mambo (Priesterin) und Houngan"

Im Voodoo werden die Priester als Houngans, die Priesterinnen als Mambos bezeichnet; beide sind gleichberechtigt. Ihr Amtssymbol ist die Asson genannte Rassel, die im Hounfour (Tempel) aufbewahrt wird. Voodoo-Priester, die sich auch oder nur mit Schadenszauber beschäftigen, werden als Bocore bezeichnet.




</doc>
<doc id="4066" url="https://de.wikipedia.org/wiki?curid=4066" title="Pfalzgraf">
Pfalzgraf

Die Pfalzgrafen (von lateinisch "palatinus" „der im Palast bzw. bei Hofe“) waren ursprünglich Amtsträger und Vertreter des Königs oder Kaisers. Sie standen dem Hofgericht vor und hatten eine leitende Funktion allgemeiner Art inne. Außerdem fungierten sie auch als Verbindungsmänner zwischen Bittstellern aus dem Reich und dem König oder Kaiser. Im Heiligen Römischen Reich gab es anfangs je einen Pfalzgrafen für jedes Herzogtum. Später wurden die meisten Pfalzgrafschaften einem mächtigeren Fürstentum inkorporiert. Der einzig übriggebliebene Pfalzgraf bei Rhein gehörte ab dem Spätmittelalter dem Reichsfürstenstand an und war den Herzögen faktisch gleichgestellt. 

Zur Bedeutung des lateinischen Wortes "palatinus" siehe unter Paladin, denn sowohl das Wort „Pfalzgraf“ als auch das Wort „Paladin“ leiten sich von lat. "palatinus" ab.

In der Merowingerzeit, genauer im Laufe des 6. Jahrhunderts, wurde erstmals der Titel eines Pfalzgrafen genannt, ihm oblag die Verwaltung des königlichen Hofes. In der Karolingerzeit stieg der Pfalzgraf am Königs- oder Kaiserhof zur höchsten Instanz für weltliche Angelegenheiten auf und erhielt insbesondere den Vorsitz im Pfalzgericht. Die Träger des Pfalzgrafenamtes waren also in ihrer Frühzeit leitende königliche Amtsträger bei Hofe mit vorwiegend administrativen und richterlichen Aufgaben. Mit ihrem Amt wurde den Pfalzgrafen oft auch die Herrschaft über eine Königs- bzw. Kaiserpfalz mit Gefolge und zugehörigen Gütern verliehen. Diese burgähnlichen Pfalzen bzw. Königshöfe lagen verstreut über das Königreich in unterschiedlichen Herzogtümern.

Im Rahmen der Entwicklung des deutschen Königreichs aus dem ostfränkischen auf der Grundlage der Stammesherzogtümer des Ostfrankenreichs erhielten die Pfalzgrafen weitgehende königliche Sonderrechte, um den Zusammenhalt des Königtums politisch zu sichern und die mächtigen Herzöge in Schach zu halten. Dabei entwickelten sich seit Ende des 10. Jahrhunderts in den Stammesherzogtümern Sachsen, Bayern, Schwaben und Lothringen Stammes-Pfalzgrafen als Vertreter und Wahrer der königlichen Rechte. Die Pfalzgrafenwürde war nun nicht mehr mit der ursprünglichen Aufgabe der Betreuung einer Königspfalz verbunden, sondern beinhaltete eine Art Kontrollfunktion und Vertretung des Königs innerhalb der Stammesherzogtümer und damit auch die zweite Position nach dem Herzog innerhalb des Herzogtums. Damit verbunden war eine Rangerhöhung gegenüber anderen Grafen des Herzogtums und das Recht, das Richteramt an Königs Statt auszuüben. Damit einher gingen Jagd-, Zoll- und Münzrecht. Der mächtigste unter den Pfalzgrafen, der Pfalzgraf bei Rhein, war Stellvertreter des Königs im Hofgericht, Reichsvikar bei Thronvakanzen, und sogar Richter über den König. 

Später wurde die Bezeichnung „Pfalzgraf“ zu einem erblichen Titel in verschiedenen deutschen Fürstenhäusern. Zum Reichsfürstenstand zählten im Heiligen Römischen Reich ab dem Spätmittelalter Herzöge, Land-, Mark- und Pfalzgrafen, hinzu kamen die kirchlichen Fürsten: Erzbischöfe, Bischöfe und die Äbte und Äbtissinnen von Reichsabteien.

Eine strikte Ämtertrennung z. B. zwischen Pfalzgrafen einerseits und anderen Fürstenämtern gab es nicht. Mächtige Pfalzgrafen waren oft in Personalunion auch Land- oder Markgrafen, Herzöge oder auch kirchliche Fürsten. Damit wuchs dem Herrschaftsbegriff „Pfalz“ im Heiligen Römischen Reich eine neue Bedeutung zu: Pfalz bezeichnete danach nicht nur befestigte Königshöfe, sondern auch von Pfalzgrafen bzw. Kurfürsten beherrschte Territorien. 
Die Rechte und Pflichten der Ämter und Würden der mittelalterlichen Gesellschaft unterlagen immer wieder Änderungen und waren zudem auch regional unterschiedlich. So beklagte Mitte des 18. Jhdts ein Geschichtsschreiber, dass in den seinerzeitigen Erläuterungen der Goldenen Bulle die Begriffe comes palatii, Seneschall, oberster Hofmeister, Truchsess, „Hausmajor und Majordom“ (Hausmeier) „ganz unrecht miteinander vermenget“ würden. Außerdem sei der comes palatii „Reichshofrichter“ und nicht etwa „Hofrichter“.<br>
Klarer ist die Unterscheidung zu den Hofpfalzgrafen: Diese waren zwar häufig zur Ausübung ihrer Privilegien bestimmten Territorien zugeteilt, hatten selbst aber aufgrund ihres Hofpfalzgrafenamts keine landesherrlichen Rechte, sondern schließlich beamtenähnliche Funktionen. 


Der Karolinger Lothar (König von Frankreich 954–986) machte Odo I., Graf von Blois, einen seiner treuesten Verbündeten im Kampf gegen die Robertiner neben den Grafen von Vermandois, zum Pfalzgrafen, ein Titel, der in seiner Familie erblich und dann auf die Champagne bezogen geführt wurde.

Das Pfalzgrafenamt hing ursprünglich mit der Pfalz in Regensburg zusammen und war in Bayern vermutlich nicht dem König, sondern dem bayerischen Herzog untergeordnet. Es verlieh dem Inhaber im Rechts- und Gerichtsbereich eine führende Stellung im Herzogtum.


1169 von Kaiser Friedrich I. aus der Freigrafschaft Burgund gebildet, siehe hier.


Ab 985 war die Pfalzgrafschaft von Lothringen im Hause der Ezzonen verankert:

Nach dem Tod von Hermann II. von Lothringen heiratete seine Witwe Adelheid den Luxemburger Heinrich II. von Laach, der zwischen 1085/1087 in der Pfalzgrafschaft nachfolgte. Die Pfalzgrafschaft von Lothringen ging damit in die Pfalzgrafschaft bei Rhein über.

Die Pfalzgrafschaft bei Rhein entwickelte sich seit 1085/1087 aus der Pfalzgrafschaft Lothringen. Die regierenden Pfalzgrafen bei Rhein sind in der Liste der Herrscher der Kurpfalz aufgeführt. Der dem Hause Wittelsbach entstammende "Pfalzgraf bei Rhein" war seit 1214, endgültig seit 1356 einer der sieben Kurfürsten des Heiligen Römischen Reiches und in Abwesenheit des Königs dessen Stellvertreter. Die wichtige Kurfürstenwürde überdeckte dabei den Pfalzgrafentitel und ließ die Bezeichnung „Pfalz“ allmählich zum Namen für die Territorien dieses „Kurfürsten von der Pfalz“ (Kurpfalz) bzw. für Länder mit ihm verwandter Nebenlinien (z. B. Oberpfalz, Pfalz-Neumarkt, Pfalz-Neuburg) werden. Als der Kurfürst von der Pfalz 1777 das Kurfürstentum Bayern erbte, entstand kurzfristig der Doppelstaat „Pfalz-Bayern“. Die linksrheinischen Teile Pfalz-Bayerns (das Herzogtum Jülich und die westliche Pfalz) gingen mit der Besetzung des Linken Rheinufers und dem Friede von Lunéville an Frankreich verloren, dessen Herrscher Napoléon Bonaparte Bayern 1806 zum Königreich erhob. Als Teile der alten Kurpfalz 1814/15 an Bayern zurückkamen, änderte dies am nunmehr rein bayerischen Landesnamen nichts mehr, und „die Pfalz“ war nun eine bayerische Provinz unter anderen. 1945 wurde sie von Bayern abgetrennt und mit dem Südteil der bisherigen preußischen Rheinprovinz und Rheinhessen zum neuen deutschen Bundesland Rheinland-Pfalz vereinigt. In diesem Namen lebt der Bedeutungswandel des Begriffes „Pfalz“ bis heute fort.
Pfalzgrafen von Sachsen waren zunächst die Grafen von Goseck, die Amt und Titel an die Grafen von Sommerschenburg, und diese sie wiederum an die Landgrafen von Thüringen vererbten:


Auf dem Reichstag zu Gelnhausen wurde Landgraf Ludwig III. von Thüringen am 13. April 1180 zum Pfalzgrafen von Sachsen ernannt.


Nach dem Tod Heinrich Raspes ging das Amt des Pfalzgrafen von Sachsen aufgrund einer Eventualbelehnung durch Kaiser Friedrich II. zunächst auf die Wettiner über.


Unter König Rudolf I. von Habsburg ging das Amt des Pfalzgrafen von Sachsen an die braunschweigischen Welfenherzöge.


Die schwäbische Pfalzgrafschaft ging 1146 an die Pfalzgrafen von Tübingen über.



</doc>
<doc id="4068" url="https://de.wikipedia.org/wiki?curid=4068" title="Partnerschaft (Rechtsform)">
Partnerschaft (Rechtsform)

Die Partnerschaft ist eine Personengesellschaft nach deutschem Recht, in der sich Angehörige freier Berufe zur Ausübung ihrer Berufe zusammenschließen können. Die Rechtsform wurde 1995 mit dem Partnerschaftsgesellschaftsgesetz (PartGG) neu geschaffen. Die Partnerschaft ist an freiberufliche Tätigkeiten gebunden und übt im Gegensatz zu den Personenhandelsgesellschaften kein Handelsgewerbe aus. Angehörige einer Partnerschaft können nur natürliche Personen sein. Bloße Kapitalbeteiligung ist nicht zulässig.

Der Partnerschaftsvertrag bedarf gemäß Abs. 1 PartGG der Schriftform. Der Partnerschaftsvertrag muss nach Abs. 2 PartGG enthalten:


Eine bestimmte Mindestausstattung mit Eigenkapital ist nicht gesetzlich vorgeschrieben.

Die Partnerschaft übt kein Handelsgewerbe aus, ist folglich keine Handelsgesellschaft und kann somit auch im eigentlichen Sinne keine Firma führen. Nach PartGG führt sie jedoch einen Namen. Dieser muss den Namen mindestens eines Partners, den Zusatz „& Partner“ oder „Partnerschaft“ sowie die Berufsbezeichnungen aller in der Partnerschaft vertretenen Berufe enthalten. Vornamen müssen nicht angegeben werden. Andere Namen als die der Partner dürfen nicht mit aufgenommen werden; laut Urteil des Bundesgerichtshofs vom 11. März 2004 (I ZR 62/01) gilt dieses Verbot aber nicht für sonstige Zusätze wie etwa Phantasienamen. Ergänzend gelten für die Namensführung der Partnerschaft bestimmte Regelungen des HGB entsprechend ( Abs. 2 PartGG).

Die Gesellschafter der Partnerschaft müssen nach Abs. 1 PartGG die Partnerschaft im Partnerschaftsregister eintragen lassen. Auch der Ein- oder Austritt eines Partners, die Änderung des Namens oder die Sitzverlegung der Partnerschaft müssen zur Eintragung ins Partnerschaftsregister angemeldet werden.

Es gilt das Partnerschaftsgesellschaftsgesetz (PartGG). Soweit dort nichts anderes bestimmt ist, finden die Vorschriften des Bürgerlichen Gesetzbuches (§§ 705–740) und des Handelsgesetzbuchs (§§ 105–160) über die Gesellschaft Anwendung.

Zur Führung der Geschäfte sind grundsätzlich alle Partner berechtigt und verpflichtet, es sei denn, im Partnerschaftsvertrag ist etwas anderes vereinbart. Einzelne Partner können im Partnerschaftsvertrag nur von der Führung der sonstigen Geschäfte ausgeschlossen werden. Im Übrigen richtet sich das Rechtsverhältnis der Partner untereinander nach dem Partnerschaftsvertrag. Die Geschäftsführergehälter der Partner sind steuerlich nicht als Betriebsausgabe abzugsfähig; sie sind bei der steuerlichen Gewinnverteilung dem jeweiligen Partner als Vorwegvergütung zuzurechnen.

Die Aufteilung von Gewinn und Verlust auf die Partner ist regelmäßig im Partnerschaftsvertrag geregelt. Trifft der Partnerschaftsvertrag keine Aussage darüber, sind die §§ 110 bis 116 Abs. 2, §§ 117 bis 119 des Handelsgesetzbuchs entsprechend anzuwenden (§ 6 Abs. 3 PartGG).

Eine Partnerschaft kann unter ihrem Namen Rechte erwerben und Verbindlichkeiten eingehen, Eigentum und andere dingliche Rechte an Grundstücken erwerben und vor Gericht klagen und verklagt werden ( Abs. 2 PartGG i.V.m HGB).

Die Partner einer Partnerschaft haften – im Unterschied zu einer bloßen Bürogemeinschaft – für die Verbindlichkeiten der Partnerschaft den Gläubigern als Gesamtschuldner persönlich. Nach Abs. 2 PartGG haften für berufliche Fehler kraft Gesetz neben dem Gesellschaftsvermögen nur diejenigen Partner, die mit der Bearbeitung eines Auftrags tatsächlich befasst waren. Scheidet ein Partner aus, haftet er für die bis dahin begründeten Verbindlichkeiten weiter. Für Verbindlichkeiten, die nicht mit der Ausführung eines Auftrages in Verbindung stehen (beispielsweise die Bestellung von Büromaterial) haften demnach die Partner wie in einer Gesellschaft bürgerlichen Rechts (GbR) immer als Gesamtschuldner.

Eine Partnerschaft wird aufgelöst


Ein Partner scheidet aus der Partnerschaft aus,


Nach BGB hat der Rechnungsabschluss bei einer Gesellschaft und die Gewinnverteilung im Zweifel am Schluss jedes Geschäftsjahrs zu erfolgen. Diese Rechnungslegungspflicht ist auch im steuerlichen Interesse zu befolgen. Partnerschaften können als Gewinn den Überschuss der Betriebseinnahmen über die Betriebsausgaben ansetzen. Die Vorschriften über die Absetzung für Abnutzung oder Substanzverringerung sind dabei zu befolgen. Die Anschaffungs- oder Herstellungskosten für nicht abnutzbare Wirtschaftsgüter des Anlagevermögens sind erst zum Zeitpunkt der Veräußerung oder Entnahme dieser Wirtschaftsgüter als Betriebsausgaben zu berücksichtigen. Die nicht abnutzbaren Wirtschaftsgüter des Anlagevermögens sind unter Angabe des Tages der Anschaffung oder Herstellung und der Anschaffungs- oder Herstellungskosten oder des an deren Stelle getretenen Werts in besondere, laufend zu führende Verzeichnisse aufzunehmen. Nicht abziehbare Aufwendungen im Sinne des Abs. 5 Satz 1 Nr. 1 bis 4, 6b und 7 EStG sind einzeln und getrennt von den sonstigen Betriebsausgaben aufzuzeichnen. Soweit diese Aufwendungen nicht vom Abzug ausgeschlossen sind, dürfen sie bei der Gewinnermittlung nur berücksichtigt werden, wenn sie besonders aufgezeichnet sind.

Wirtschaftsgüter, die ein Partner einer Partnerschaft für Zwecke der Partnerschaft nutzt, gehören zum Sonderbetriebsvermögen des betreffenden Partners. Die Partnerschaft ist dann nicht gewerbesteuerpflichtig, wenn an ihr keine berufsfremden Personen beteiligt sind. Ein Partner einer Partnerschaft erzielt aus seiner Beteiligung an der Partnerschaft Einkünfte aus selbständiger Arbeit. Einkommensteuerpflichtig ist nicht die Partnerschaft, sondern jeder einzelne Partner. Die Partnerschaft ist Unternehmer im Sinne des Umsatzsteuergesetzes. Ein Unternehmer ist verpflichtet, zur Feststellung der Umsatzsteuer und der Grundlagen ihrer Berechnung Aufzeichnungen zu machen. Bei der Übertragung einer Partnerschaftsbeteiligung im Wege der Schenkung oder Erbfolge auf einen Nachfolger wird bei der Erbschaftsteuer ein spezieller Freibetrag für das Betriebsvermögen des Partners gewährt.





</doc>
<doc id="4069" url="https://de.wikipedia.org/wiki?curid=4069" title="Prachteiderente">
Prachteiderente

Die Prachteiderente ("Somateria spectabilis"), auch Königseiderente, ist eine Vogelart aus der Gattung der Eiderenten ("Somateria") und der Familie der Entenvögel (Anatidae). Die Art, deren Männchen ein unverwechselbares Prachtkleid haben, brütet zirkumpolar an den Küsten und Inseln des nördlichen Eismeers. Wie bei vielen arktischen Vogelarten ist der Bestand der Prachteiderente starken Schwankungen unterworfen. Sie gilt jedoch insgesamt als nicht gefährdet. Während des Winterhalbjahres sind sie an den Küsten Skandinaviens und Islands zu beobachten. An den Küsten der Nord- und Ostsee sind sie nur vereinzelt als Irrgast zu sehen.

Die Prachteiderente ist etwas kleiner als die zur selben Gattung gehörende Eiderente ("Somateria mollissima"). Das Männchen, der Erpel, ist unverwechselbar mit seinem schwarz gefärbten Körper, der weißen bis lachsfarbenen Brust und dem hellblauen Oberkopf und Nacken. Die Nackenfedern sind leicht verlängert, so dass sich eine Federhaube andeutet. Auffälligstes Merkmal des Erpels ist der zu einem Stirnhöcker erweiterte rote Oberschnabel. Er ist durch einen schwarzen Federkranz vom hellblauen Oberkopf abgesetzt. Die Wangen sind meergrün. Kinn und Kehle sind weiß. Das schwarze Gefieder des hinteren Körperteils ist durch ein schmales weißes Seitenband und einen fast runden weißen Fleck an den Bürzelseiten scharf abgesetzt. Geschlechtsreife Erpel haben wie die Weibchen sichelförmig gekrümmte innere Armschwingen. Im Ruhekleid sind die weißen Federn am Oberkopf durch schwarzbraune ersetzt. Die Kopfseiten und die Vorderbrust ist hell zimtbraun. Das übrige Körpergefieder ist dunkelbraun bis schwarzbraun.

Das Weibchen hat ein braunes Gefieder. Es kann aber anhand der Größe und dem Körperbau leicht von allen Enten außer anderen Eiderenten unterschieden werden. Verglichen mit den Weibchen der Eiderente ist bei den Weibchen der Prachteiderente das Gefieder rötlicher, der Schnabel ist etwas kürzer, und das Körpergefieder ist anders als bei der Eiderente nicht gebändert, sondern wirkt mit Ausnahme des Kopfes schuppenförmig getüpfelt. Brust und die Körperunterseite sind schwarzbraun. Der Schnabel und die Füße haben eine grünbraune Farbung. Die Iris ist schmutziggelb. Das Ruhekleid des Weibchens gleicht dem Brutkleid. Allerdings sind die Farbkontraste etwas schwächer ausgeprägt, und das schuppenförmige Muster ihres Körpergefieders ist weniger auffällig.

Junge weibliche Prachteiderenten gleichen in ihrem Gefieder bereits den weiblichen Prachteiderenten sehr weitgehend. Allerdings ist die Tüpfelung noch wenig auffällig, und den Federsäumen fehlt das kräftige Zimtbraun, das bei den geschlechtsreifen Weibchen charakteristisch ist. Ähnlich wie bei den Eiderenten dauert es bei den jungen Erpeln einige Jahre, bis sie das Prachtkleid vollkommen ausgebildet haben. Im ersten Prachtkleid fehlt noch die blaue Kopfoberseite, und die weißen Federpartien sind noch von braunen Federn durchsetzt. Im zweiten Prachtkleid ist die Ähnlichkeit zum Prachtkleid adulter Erpel schon weitergegeben, erst im dritten Jahr aber gleicht es dem der geschlechtsreifen Vögel. Auch der Stirnhöcker entwickelt sich erst mit der Zeit. Er ist zwar nach der Mauser ins erste Prachtkleid schon leicht vorhanden. Voll ausgebildet ist er jedoch erst im 3. oder 4. Jahr.

Die Prachteiderente ist verglichen mit der Eiderente weniger ruffreudig. Der Balzruf des Männchens ist ein dumpfes "ruú go go." oder "gu-gruu gruúu-gruu". Er zeigt dabei die charakteristische "Verbeugung nach hinten", wie sie auch bei den Erpeln der Eiderente zu beobachten ist. Dabei legt das Männchen den Kopf weit in den Nacken und wölbt die Brust nach vorne. Die Weibchen antworten auf die Balzrufe mit einem sonoren, rauen "gok".

Die Prachteiderente brütet entlang der arktischen Küste von Nordosteuropa, Asien und Nordamerika. Das Brutgebiet ist nicht geschlossen, sondern weist besonders im atlantischen Bereich weite Verbreitungslücken auf. Die Prachteiderente fehlt zum Beispiel als Brutvogel auf Island und den Küsten Norwegens, da auf Grund des warmen Golfstromes diese Regionen für Prachteiderenten nicht als Brutstätte in Frage kommen. Sie kommt auf den nördlichen Polarmeeren in Küstennähe vor, und wandert flussaufwärts auch bis zu 100 km ins Binnenland, wo sie dann oft an Seen brütet. Sie überwintert etwas weiter im Süden in Norwegen und im östlichen Kanada, wo sie in passenden Küstengewässern auch große Gruppen bilden kann. Schwerpunkt der Verbreitung ist die Subarktis. Ihre südliche Verbreitungsgrenze ist der Beginn der Strauchtundra. Sie brütet an der westlichen Küste Spitzbergens, auf der Halbinsel Kanin, der Jenissej-Mündung und an dem südlichen und mittleren Teil von Nowaja Semljas. Sie kommt an der Küste Nordostsibiriens vor und ist dort bis zur Tschuktschen-Halbinsel verbreitet. In Ostsibirien brütet sie gelegentlich auch im Binnenland.

Prachteiderenten nutzen stärker als die Eiderenten die Gewässer der Tundra. Dies bedingt auch eine etwas andere Nahrungszusammensetzung.

Die Überwinterungsquartiere der Prachteiderente sind die Teile der arktischen Meere, die eisfrei bleiben. Zu den Überwinterungsquartieren gehört die Südwestküste Grönlands. Sie überwintert in kleinerer Zahl auch in Island und an den nördlichen schottischen Inseln. Überwinterungsgäste gibt es in geringer Zahl auch der nördlichen und mittleren Küste Norwegens. Sie erscheint in dieser Jahreszeit auch verhältnismäßig häufig im nordöstlichen Teil der Ostsee. Es handelt sich dabei um Prachteiderenten, die auf den Inseln und an der Küste des Weißen Meeres brüten. An der dänischen, der südschwedischen sowie den west- und mitteleuropäischen Küsten ist die Prachteiderente ein verhältnismäßig seltener Irrgast.

Die Prachteiderente ist eine omnivore Entenart. Allerdings spielt tierische Nahrung eine größere Rolle als pflanzliche. Hauptnahrung sind Wirbellose und zwar insbesondere Wasserinsekten wie Schnaken und Köcherfliegenlarven. Sie frisst außerdem in großen Mengen Mollusken und Stachelhäuter wie Seeigel, See- und Schlangensterne. Vermutlich nimmt sie auch Kleinnager zu sich, wenn diese sich in der Tundra stark vermehrt haben. An pflanzlicher Nahrung spielen vor allem die Samen von verschiedenen Wasserpflanzen eine Rolle.

Prachteiderenten suchen ihr Futter weiter draußen im Wasser als Eiderenten, in der Regel können sie auch größere Tauchtiefen erreichen. Im Allgemeinen gehen sie im Salzwasser in einer Tiefe von 15 Metern auf Futterjagd. Prachteiderenten jagen länger als Eiderenten und können bis zu zwei Minuten unter Wasser bleiben. Im Sommer wird die Nahrung aus den aufgetauten Tundrenseen geholt und besteht dann zum größten Teil aus Insektenlarven sowie Süßwasserkrustentieren.

Prachteiderenten kehren in ihre Brutgebiete zurück, sobald diese eisfrei sind. Dies ist häufig erst ab Mitte Mai und in strengen Wintern sogar erst Anfang Juni der Fall. Fortpflanzungsfähige Prachteiderenten halten sich häufig geraume Zeit an der Meeresküste in der Nähe ihrer Brutgebiete auf, bis die klimatischen Bedingungen es zulassen dass sie ihre Brutplätze aufsuchen. Prachteiderenten sind in der Regel schon verpaart, wenn sie in ihren Brutgebieten auftauchen. Die Balz wird aber bis zum Beginn der Brutphase fortgesetzt. Die Balzposen und -gesten gleichen weitgehend denen der Eiderente; die Prachteiderente bläht ihren weißen Hals allerdings stärker auf. Zu dem Balzrepertoire gehört auch eine nach vorne schiebende Kopfbewegung, die auf den Betrachter wirkt, als schöbe die Ente ein imaginäres Objekt weg.

Prachteiderenten brüten meist einzeln. Das unterscheidet sie unter anderem von den Eiderenten, die große Brutkolonien bilden. Wo allerdings die Lebensumstände günstig sind, stehen die Nester der Prachteiderente nahe beieinander. Prachteiderenten brüten auch in der Nähe von Wildgänsen, in Möwenkolonien und bauen ihr Nest gelegentlich auch in Eiderentenkolonien. Sie bastardisieren gelegentlich mit der Eiderente. Nach bisherigen Beobachtungen gehen aus solchen Kreuzungen allerdings nur männliche Tiere hervor.

Das Nest wird vom Weibchen gebaut und ist meist nicht mehr als eine flache Mulde. Wie die Eiderente nutzt die Prachteiderente die körpereigenen Daunen, um ihr Nest auszupolstern. Die Daunen der Prachteiderente sind gröber als die der Eiderente und das Nest wird mit deutlich weniger Daunen ausgelegt. Die Daunen spielen deshalb auch keine wirtschaftliche Rolle. Das Gelege umfasst zwischen vier und sieben Eier von grüner bis bräunlicher Farbe. Die Brutdauer, bis die Dunenjungen schlüpfen, beträgt zwischen 22 und 23 Tage.

Der Gesamtbestand an Prachteiderenten wird auf 790.000 bis 930.000 Individuen geschätzt. Der europäische Brutbestand beträgt davon nur 37.000 bis 46.000 Brutpaare, der aber als stabil und ungefährdet gilt. Der Winterbestand in Nordeuropa beträgt mehr als 350.000 Individuen.




</doc>
<doc id="4072" url="https://de.wikipedia.org/wiki?curid=4072" title="Pasteurisierung">
Pasteurisierung

Pasteurisierung [] oder Pasteurisation bezeichnet die kurzzeitige Erwärmung von flüssigen oder pastösen Lebensmitteln auf Temperaturen von mindestens 60 °C (klassisches Verfahren von Pasteur) bis maximal 100 °C ("Hochpasteurisieren") zur Abtötung der vegetativen Phasen von Mikroorganismen. Sie dient z. B. dazu, Lebensmittel, unter anderem Milch, Frucht- und Gemüsesäfte und Flüssigei, schonend haltbar (keimarm) zu machen.

Durch die kurze, 15 Sekunden bis wenige Minuten umfassende, Zeitdauer der Hitzeeinwirkung und die mäßige Temperatur werden der Nährwert, der Geschmack und die Konsistenz des Lebensmittels nur unbedeutend verändert und dennoch die meisten Lebensmittelverderber wie Milchsäurebakterien und Hefen sowie viele krankheitserregende Bakterien wie Salmonellen zuverlässig abgetötet. Hitzeresistente Bakteriensporen wie die von "Clostridium botulinum", die Erreger der Paratuberkulose sowie Sporen einiger Schimmelpilze überleben diese Behandlung zumindest teilweise. Aus diesem Grund sollte der Mikroorganismengehalt der Rohware möglichst gering gehalten werden.

Da sie nicht keimfrei sind, müssen pasteurisierte Lebensmittel im Regelfall gekühlt gelagert werden und sind dann einige Tage bis einige Wochen haltbar. Davon abzugrenzen ist die Sterilisation bei Temperaturen von über 100 °C, die auch hitzeresistente Bakteriensporen abtötet. Sterilisierte Lebensmittel können erheblich länger und ungekühlt gelagert werden als pasteurisierte Lebensmittel, verlieren jedoch durch das Erhitzen stärker an Nährwert und Geschmack.

Das Verfahren der Pasteurisierung wurde nach dem französischen Chemiker Louis Pasteur benannt und 1864 entwickelt. Pasteur hatte erkannt, dass durch kurzzeitiges Erhitzen von Lebensmitteln und anderen Stoffen die meisten der darin enthaltenen Mikroorganismen abgetötet werden. Sind so behandelte Stoffe in einem abgeschlossenen Bereich, können auch keine neuen Mikroorganismen in diese eindringen. Unter Zuhilfenahme eines speziell angefertigten Glaskolbens, des Pasteurkolbens, wurde diese Methode sehr eindrucksvoll demonstriert. Bei Lebensmitteln kann dadurch die Haltbarkeitszeit deutlich gesteigert werden. Zugleich war mit diesem Versuch auch die Urzeugungsthese widerlegt.

Bereits 1795 wendete der Koch, Konditor und Erfinder Nicolas Appert die Wärmekonservierung in Form des Einkochens an. Als Vorversuch zur Entwicklung einer systematisierten Anwendung dieses Verfahrens in den 1860er Jahren gilt Pasteurs Nachweis, dass Lebensmittelverderb eindeutig durch Lebewesen verursacht wird und kein abiotischer Prozess ist, wie viele seiner Zeitgenossen annahmen. Für diesen Nachweis füllte er frisch gekochte Bouillon in zwei gläserne Kolben. Einen davon ließ er unverschlossen stehen, auf den anderen setzte er ein S-förmig gekrümmtes Glasrohr, das zwar einen Ausgleich des Gasdrucks zwischen Gärraum und Umgebung erlaubt, das Eindringen von Partikeln jedoch verhindert (sog. Gärröhrchen). In beiden Glaskolben kochte er die Bouillon nochmals auf und beobachtete dann die Entwicklung des Inhalts beider Kolben über einen längeren Zeitraum. Der Inhalt des unverschlossenen Kolbens verdarb schneller, während die Bouillon im Kolben mit dem Gärröhrchen längere Zeit genießbar blieb.

Am bekanntesten ist die Pasteurisierung von Milch, die hierzu 15 bis 30 Sekunden auf 72 bis 75 °C erhitzt und danach sofort wieder abgekühlt wird. Pasteurisierte Milch bleibt ungeöffnet bei 6 bis 7 °C gelagert etwa 6 bis 10 Tage fast unverändert.

Bei der Hochpasteurisierung wird Milch auf 85 bis 134 °C erhitzt. Die resultierende „hoch pasteurisierte Milch“ ist nahezu keimfrei und bleibt wesentlich länger haltbar als die übliche pasteurisierte Milch (im Kühlschrank bei 5 °C etwa 2 Wochen). Sie wird in Deutschland seit 1990 auch als ESL-Milch angeboten. Eine Pasteurisierung bei Temperaturen von über 135 °C wird Ultrahocherhitzung genannt und führt zu H-Milch.

In Deutschland und der EU ist nach der europäischen Milchhygiene-Richtlinie die Pasteurisierung für alle gehandelten Milchsorten außer Roh- und Vorzugsmilch gesetzlich vorgeschrieben. Milcherzeugnisse die abgepackt und zum Verkauf an Verbraucher bestimmt sind werden als „ultrahocherhitzt“ bezeichnet, wenn sie auf Temperaturen von 135 bis 150 °C erhitzt wurden und „wärmebehandelt“, wenn sie auf Temperaturen mindestens 50 °C erhitzt wurden. Rohmilchkäse müssen als solche gekennzeichnet werden und sollten von Schwangeren und Menschen mit geschwächtem Immunsystem vorbeugend gemieden werden. In Australien und Neuseeland dürfen seit 1994, mit wenigen Ausnahmen wie Roquefort, nur pasteurisierte oder thermisierte Milchprodukte in den Handel gebracht werden.

Die Pasteurisierung von Milch und Milchprodukten geht nicht auf Louis Pasteur zurück, sondern wurde erst später von Franz von Soxhlet verwirklicht.

Aufgeschlagene Eier sind ein leicht verderbliches Lebensmittel, das durch Pasteurisierung haltbar gemacht werden kann. Wie bei Milch kann die Erhitzung im Durchflussverfahren mit Platten- oder Röhrenwärmetauschern, für kleinere Produktionsmengen aber auch chargenweise in Behältern mit beispielsweise 60 oder 120 Litern Inhalt geschehen. Ein Problem ist, dass flüssiges Ei nicht sehr hitzebeständig ist, sondern bereits bei niedrigen Temperaturen gerinnt: Eiklar bei 62 °C, Eigelb bei 68 °C, Vollei bei 66 °C. Aus diesem Grund kommt nur Dauererhitzung bei niedrigeren Temperaturen in Frage. Mögliche Verfahrensparameter für die Pasteurisierung von Flüssigei sind zum Beispiel:
(Andere Quellen geben andere Werte an, so etwa für Vollei: 65 °C, 5 min.)
Pasteurisiertes Ei ist verschlossen bei Temperaturen bis 4 °C einige Tage bis wenige Wochen haltbar. Nach dem Öffnen besteht die Gefahr, dass das Ei erneut mit Mikroorganismen in Kontakt kommt und von ihnen besiedelt wird (Rekontamination). Daher muss pasteurisiertes Flüssigei nach dem Öffnen weiterhin kühl gelagert und innerhalb eines Tages verbraucht werden.

Auch andere Lebensmittel wie Wein, Fruchtsaft oder Bier werden von der Lebensmittelindustrie häufig pasteurisiert oder aus pasteurisierten Bestandteilen erzeugt in den Handel gebracht. In diesen Bereichen wird die Behandlung meist mit dem Kürzel KZE (Kurzzeiterhitzung) bezeichnet. Saure Produkte mit einem pH-Wert kleiner als 4,5 können in Kombination mit der Pasteurisierung so haltbar gemacht werden, dass eine gekühlte Lagerung nicht erforderlich ist. Zu dieser Gruppe zählen viele Obst- und Gemüsesäfte oder -konserven und u. a. Limonaden und Energy Drinks.

Neben Lebensmitteln können auch andere Produkte pasteurisiert werden, beispielsweise Klärschlamm oder Flüssigmist.

Andere Verfahren zur Haltbarmachung sind




</doc>
<doc id="4074" url="https://de.wikipedia.org/wiki?curid=4074" title="Pulitzer-Preis">
Pulitzer-Preis

Der Pulitzer-Preis ist ein US-amerikanischer Journalisten- und Medienpreis. Er ist dort bei Journalisten ebenso berühmt und begehrt wie der Oscar in der Filmindustrie. Mit seinen Auszeichnungen für Romane und Sachbücher ist er der wichtigste US-amerikanische Literaturpreis. Es werden zudem Reportagen, Fotos, Karikaturen, Lyrik, Theaterstücke und Musikaufnahmen ausgezeichnet.

Er wurde vom Journalisten und Zeitungsverleger Joseph Pulitzer gestiftet und wird seit 1917 verliehen. Jährlich gibt die Pulitzer-Journalisten-Schule an der New Yorker Columbia Universität die Preisträger bekannt, die von einer Jury aus US-amerikanischen Journalisten und Verlegern ausgewählt werden. Das Preisgeld beträgt je Kategorie ca. 10.000 US-Dollar.

Die Preisträger werden in der Regel Ende April bekanntgegeben. Die Preisverleihung erfolgt etwa einen Monat später während eines Mittagessens in der Bibliothek der Columbia Universität.

Erstmals wurde ein Pulitzer-Preis am 4. Juni 1917 vergeben.

Im Jahr 2010 wurde der Preis erstmals an eine Online-Publikation verliehen. Die Journalistin Sheri Fink (ProPublica) gewann in der Sparte Investigativer Journalismus für einen Artikel über die Arbeit eines Krankenhauses in New Orleans nach dem Hurrikan Katrina.

Als Antwort auf die vielfach kritisierte geringe Berücksichtigung von Minderheiten bei der Vergabe des Preises initiierte der afroamerikanische Autor Ishmael Reed 1980 den American Book Award, der jährlich durch die "Before Columbus Foundation" gestiftet wird.

Im Lauf der Jahre wurde auch Deutschen die Pulitzer-Ehre zu teil. Die erste bekannte deutsche Preisträgerin war die Fotografin Anja Niedringhaus, die im Jahr 2005 die Auszeichnung in der Kategorie "Breaking News Photography" gewann. Ein weiterer deutscher Gewinner ist Daniel Etter, der 2016 ebenfalls in der Kategorie "Breaking News Photography" zu den Preisträgern gehörte. Im Jahr 2017 standen dann die beiden deutschen Investigativjournalisten Frederik Obermaier und Bastian Obermayer auf der Bühne der Preisverleihung in der Columbia-Universität und nahmen die Auszeichnung - als Mitglieder des Panama Papers-Teams - in der Kategorie "Explanatory Reporting" entgegen. Als weitere namentlich in den eingereichten ICIJ-Texten genannte Autorin gehört auch die deutsche Journalistin Petra Blum zu den direkten Gewinnern.

Die wichtigste der Preiskategorien ist die "Dienst an der Öffentlichkeit"
genannte, deren Preisträger seit 1918 neben dem Preisgeld noch die Pulitzer-Goldmedaille erhalten.

Der Pulitzer-Preis wird derzeit in den folgenden 22 Kategorien (inklusive des Sonderpreises) vergeben:

Über die Jahre hinweg wurden mehrere Kategorien eingestellt, erweitert oder umbenannt:





</doc>
<doc id="4076" url="https://de.wikipedia.org/wiki?curid=4076" title="Privatrecht">
Privatrecht

Privatrecht ist dasjenige Rechtsgebiet, das die Rechtsbeziehungen zwischen rechtlich – nicht zwingend auch wirtschaftlich – gleichgestellten Rechtssubjekten (→ natürliche Person, → juristische Person) regelt. Die Bezeichnungen Bürgerliches Recht bzw. Zivilrecht (Verdeutschungen des lat. Terminus "") werden oft synonym zu "Privatrecht" verwendet, bezeichnen genau genommen allerdings nur einen Teil desselben (nämlich das Gebiet „Allgemeines Privatrecht“; s. u.).
Das Privatrecht steht in der Rechtswissenschaft neben dem Öffentlichen Recht (einschließlich des Strafrechts); zur genaueren Abgrenzung siehe Abgrenzung zum Privatrecht. Das Privatrecht sieht – im Gegensatz zum Öffentlichen Recht – eine aus der Privatautonomie abgeleitete Freiheit des Willens vor, die es dem Einzelnen grundsätzlich gestattet, mit anderen in eine Rechtsbeziehung zu treten (oder auch darauf zu verzichten). Diese Freiheit kann allerdings durch eine Vielzahl von tatsächlichen Gegebenheiten eingeschränkt sein, etwa durch ein Monopol oder die finanzielle Leistungskraft des Einzelnen. Sie ist jedoch, davon unabhängig, für das Privatrecht prägend, weil sie eine Gestaltung des Rechts ohne staatlichen Einfluss zulässt. Eines der wichtigsten privatrechtlichen Gestaltungsmittel ist der privatrechtliche Vertrag (siehe auch: Rechtsgeschäft, Vertragsrecht).

Das Privatrecht gliedert sich zuoberst in
"Für weitere Unterteilungen siehe die entsprechenden Absätze und Grafiken unten."

Während im Bürgerlichen Recht grundlegende Regeln über Personen, Sachen und Schuldverhältnisse (in der Schweiz: Obligationen) festgelegt sind, wird das Sonderprivatrecht – gelegentlich mit dem Wirtschaftsprivatrecht zusammengefasst – besonders geregelt, etwa im Handels-, Arbeits- und Mietrecht, sowie andere, detailliert ausgearbeitete Rechtsgebiete.

Die Gliederung nach dem Pandektensystem teilt das Zivilrecht in fünf (beziehungsweise sechs, mit eigenständigem Personenrecht) Teilbereiche ein: Allgemeiner Teil (in der Regel mit Personenrecht), Schuldrecht, Sachenrecht, Erbrecht, Familienrecht. Diesem pandektistischen Schema folgt neben dem deutschen Bürgerlichen Gesetzbuch (BGB) das schweizerische Zivilgesetzbuch (ZGB).


Die Gliederung des Zivilrechtes nach dem Institutionensystem, das nach dem Hauptwerk des klassischen römischen Juristen Gaius benannt ist, ist eine Einteilung nach römischem Recht, die in der Zeit der ersten großen Kodifikationswelle – französischer "Code civil," österreichisches Allgemeines Bürgerliches Gesetzbuch (ABGB) – aufgenommen wurde.

Die Einteilung gliedert sich grundsätzlich folgendermaßen:

Das österreichische ABGB folgt diesem Schema, jedoch ohne das Prozessrecht einzubeziehen:

Für Österreich: 

Das Handelsrecht wird als „Sonderprivatrecht der Kaufleute“ bezeichnet. Es beinhaltet Rechtsnormen, die für Kaufleute gelten, mithin Bestimmungen zu Handelsgeschäften, zur Firmierung des Kaufmannes, zu kaufmännischen Hilfspersonen (Handelsmakler, Handelsvertreter, Kommissionäre, Spediteure, Lagerhalter) sowie weiter im Bereich des Gesellschaftsrechts und Regeln über Personen- und Kapitalgesellschaften.

Für all diese Rechtsgebiete gilt, dass die Normen des allgemeinen Privatrechts subsidiär gelten, sodass z. B. für Handelsgeschäfte grundsätzlich allgemeines Privatrecht gilt, jedoch modifiziert und erweitert durch die Normen des Handelsrechts.

Dieses subsidiäre Verhältnis findet in Deutschland seine Kodifizierung in Abs. 1 EGHGB. In der schweizerischen Rechtstradition wurde ein eigenständiges kaufmännisches Handelsrecht seit jeher abgelehnt, dies mit der Begründung einer demokratischen Gleichheit aller Personen, die eine besondere Behandlung der Kaufleute nicht rechtfertige. Trotzdem finden sich im OR vereinzelt Sonderregeln für den kaufmännischen Verkehr (z. B. Art. 190 OR), die sachgerechte Differenzierungen ermöglichen sollen.

Das Arbeitsrecht regelt die Rechtsbeziehungen zwischen einzelnen Arbeitnehmern und Arbeitgebern (Individualarbeitsrecht) sowie zwischen den Koalitionen der Arbeitnehmer und Arbeitgeber und zwischen Vertretungsorganen der Arbeitnehmer und dem Arbeitgeber (kollektives Arbeitsrecht).

Die Normen des Arbeitsrechtes enthalten vielfach (einseitig) zwingende Vorschriften zugunsten des Arbeitnehmers. Auch hier gelten die Normen des allgemeinen Privatrechts subsidiär.

Weitere Sonderprivatrechtsbereiche sind z. B. das Mietrecht, das Verkehrszivilrecht, das Konsumentenschutzrecht oder das Wertpapierrecht, wobei anzumerken ist, dass das Mietrecht, das Verkehrszivilrecht und das Konsumentenschutzrecht oft gemeinsam mit dem Bürgerlichen Recht (Schuldrecht/Vertragsrecht) behandelt werden und das Wertpapierrecht eine immanente Nahebeziehung zum Handelsrecht hat.

Eine Kodifikation des Zivilrechts erfolgte in Deutschland 1900 mit dem Bürgerlichen Gesetzbuch (BGB), in Österreich 1812 mit dem Allgemeinen Bürgerlichen Gesetzbuch (ABGB), in der Schweiz 1883 mit dem Schweizerischen Obligationenrecht (OR) und 1912 mit dem Zivilgesetzbuch (ZGB), in Frankreich 1804 mit dem Code civil (Code Napoléon) und in Italien mit dem Codice civile. Besonders der Code civil hatte eine starke Ausstrahlungskraft und war Vorbild für die übrigen Kodifikationen der sogenannten "Civil Law Countries".

Bevor das Deutsche Kaiserreich das Bürgerliche Gesetzbuch einführte, gab es in einigen deutschen Teilstaaten bereits ein kodifiziertes Landrecht, so den Codex Maximilianeus Bavaricus Civilis von 1756 in Bayern und das Allgemeine Landrecht für die Preußischen Staaten von 1794 (ALR). Manche Landrechte basierten auf dem Code civil, z. B. das Badische Landrecht von 1810.

Bereits im Mittelalter hatten viele Territorien des Heiligen Römischen Reiches deutscher Nation ein kodifiziertes Landrecht, das jedoch neben dem Zivilrecht auch andere Rechtsbereiche (z. B. Straf- und Verfassungsrecht) regelte.

Bei privatrechtlichen Fällen mit Auslandsbezug (z. B. bei Eheschließung von zwei Personen unterschiedlicher Staatsbürgerschaft, bei einem Schadensfall im Ausland oder bei internationalen Verträgen) bestehen besondere Kollisionsnormen, die bestimmen, welches Privatrecht anzuwenden ist. Dieser Rechtsbereich wird – etwas missverständlich – als "Internationales Privatrecht" bezeichnet.

Einzelne Rechtsmaterien haben völkerrechtliche Regelungen erhalten, die dann den nationalen Regelungen vorangehen, so insbesondere der internationale Warenkauf durch das Übereinkommen der Vereinten Nationen über Verträge über den internationalen Warenverkauf vom 11. April 1980, dem auch Deutschland, Österreich und die Schweiz beigetreten sind.

Die weltweit einzigartige Möglichkeit, privatrechtliche Ansprüche aus jedem Land der Welt vor einem US-Gericht einzuklagen, wird durch den US-amerikanischen Alien Tort Claims Act geregelt.





</doc>
<doc id="4081" url="https://de.wikipedia.org/wiki?curid=4081" title="Pazifische Eibe">
Pazifische Eibe

Die Pazifische Eibe ("Taxus brevifolia") ist eine Pflanzenart, die zur Familie der Eibengewächse (Taxaceae) gehört. 

Die Pazifische Eibe ist im pazifischen Nordamerika beheimatet. Sie kommt entlang der Küste von Alaska bis Mittelkalifornien vor. Ostwärts dringt sie bis Idaho vor und reicht bis auf Höhenlagen von 2100 Meter. Die Pazifische Eibe wächst bevorzugt in feuchten und schattigen Lagen. 

Die Pazifische Eibe ist ein immergrüner Strauch oder breitkroniger, oft krummstämmiger Baum, der Wuchshöhen von bis zu 15 (selten auch 25) Meter und Stammdurchmesser von 0,5 bis zu 1,4 Meter erreicht. Die schuppige Borke ist purpurfarben bis rötlich. Die Rinde der Zweige ist anfangs grün, später rot-braun. Die scheinbar zweireihig an den Zweigen angeordneten Nadelblätter sind 8 bis 35 mm lang und 1 bis 3 mm breit. Sie besitzen einen kurzen (5 bis 8 mm) Stiel.

Die Pazifische Eibe ist zweihäusig getrenntgeschlechtig (diözisch). Die einzeln oder in Gruppen an einjährigen Zweigen stehenden männlichen Zapfen sind zuerst kugelig, grün und weisen einen Durchmesser von 1,5 mm auf. Die weiblichen Zapfen sind eiförmig, zwei- bis vierkantig und weisen einen Durchmesser von etwa 5 bis 6,5 mm auf. Die im Spätsommer bis Herbst reifenden Samen sind von einem roten Arillus umhüllt, der einen Durchmesser von etwa 10 mm aufweist.

Seit 1980 ist die Pazifische Eibe aufgrund ihres Inhaltsstoffes Paclitaxel bekannt, einem wichtigen Medikament in der Behandlung von verschiedenen Krebserkrankungen wie Brustkrebs und Eierstockkrebs. Die Pazifische Eibe ist allerdings eine geschützte Art. Noch dazu gehört sie zu den am langsamsten wachsenden Bäumen der Welt. Eine Wirkstoffisolierung aus ihrer Rinde (wobei der Baum getötet wird) ist daher kaum in größerem Umfang möglich. So bedürfte es sechs 100 Jahre alter Bäume, um genug Paclitaxel für die Behandlung einer einzigen Krebspatientin zu gewinnen. Inzwischen kann Paclitaxel halbsynthetisch aus kultivierten Exemplaren der viel häufigeren Europäischen Eibe gewonnen werden.


</doc>
<doc id="4084" url="https://de.wikipedia.org/wiki?curid=4084" title="Pflanzenöle">
Pflanzenöle

Pflanzenöle sind aus Ölpflanzen gewonnene fette Öle (Lipide), im Gegensatz zu den ätherischen Ölen. 

Pflanzenöle sind Ester des Glycerols mit Fettsäuren, sogenannte Triglyceride. 
Pflanzliche Öle und Fette bestehen zu circa 98 % aus gemischten Triglyceriden. Diglyceride sind nur in geringen Mengen um die 3 % enthalten, Monoglyceride nur in Spuren (1 %). Die für den Aufbau wichtigsten Fettsäuren sind Ölsäure C17H33COOH, Linolsäure C17H31COOH und Linolensäure C17H29COOH.
Pflanzenöle enthalten anders als tierische Öle und Fette kein Cholesterin, stattdessen aber bis zu 65 % Linolensäure viel Vitamin E. 
Die Abgrenzung zu den Pflanzenfetten ist die "Fließfähigkeit", Pflanzenöle sind bei Raumtemperatur (ca. 20 °C) flüssig.

Pflanzenöle sind seit Jahrtausenden Teil der menschlichen Ernährung und Kultur. Mohn­samen, Raps, Leinsamen, Mandelöl, Sesam­samen, Saflor und Baumwollsamen wurden seit mindestens dem Bronzezeitalter im Mittleren Osten und Zentralasien verwendet. Die Phönizier brachten 2000 v. Chr. den Ölbaum aus dem Nahen Osten nach Griechenland. Die Griechen brachten ihn dann nach Italien, wo er auch bei den Römern eine wichtige Rolle spielte.

"Zur Herstellung von Pflanzenölen siehe Ölmühle."

Die Pflanzenöle werden durch verschiedene Verfahren gewonnen:

Pflanzenöle enthalten oft einen höheren Anteil an ungesättigten Fettsäureresten als tierische Fette, weshalb bei starker Erhitzung "trans"-Fettsäuren entstehen können.

Die Pflanzenöle unterscheiden sich aufgrund ihrer unterschiedlichen Zusammensetzung in einer Vielzahl von Eigenschaften. Diese werden mit einer Reihe von Kennwerten (Fettkennzahlen) bestimmt. Es werden auch verschiedene Kontaminationen gemessen.

Je nach dem Anteil an ungesättigten Fettsäureresten unterscheidet man zwischen "nicht trocknenden" (z. B. Olivenöl) Iodzahl < 100, "halb trocknenden" (z. B. Soja- oder Rapsöl) Iodzahl 100–170 und "trocknenden" Pflanzenölen (z. B. Lein- oder Mohnöl) Iodzahl > 170. 

Der Begriff „Trocknung“ bezeichnet hierbei nicht Verdunstung, sondern das durch Oxidation mit Sauerstoff und anschließende Polymerisation (Vernetzung) der ungesättigten Fettsäuren bedingte Verdicken („Verharzen“) des Öls.

Bei den trocknenden Ölen unterscheidet man zwischen "Isolenölen" mit isolierten Doppelbindungen und "Konjuenölen" mit konjugierten Doppelbindungen.


Der Spreitwert beschreibt das Verfließen eines Öls, er ist vor allem bei der Mischung von Ölen wichtig.

Die Qualität von Pflanzenölen ist sowohl von der Art der Herstellung des Öls als auch von Herstellung der ölhaltigen Früchte und Saaten abhängig. Die Herstellung der Früchte und Saaten geschieht entweder im konventionellen Landbau oder im biologischen bzw. ökologischen Landbau.

Während im konventionellen Landbau chemische Pflanzenschutzmittel, Mineraldünger und Grüne Gentechnik zum Einsatz kommen, wird hierauf in der ökologischen Landwirtschaft verzichtet.

Zur Kennzeichnung von Ölen, die aus biologisch angebauten Früchten oder Saaten hergestellt und denen bei der Verarbeitung keine weiteren Zusatz- und Hilfsstoffe zugesetzt wurden, bedienen sich die Hersteller verschiedener Bio-Siegel. Raffinierte Öle erfüllen diese Kriterien nicht.

Bei allen unraffinierten Ölen ist die Qualität der Rohware entscheidend für Geschmack, Geruch, Farbe und Vitamin­gehalt. Bei raffinierten Ölen werden diese Eigenschaften unabhängig von der Qualität der Rohware verringert.

Das Öl wird zunächst bei Temperaturen von über 100 °C heiß gepresst. Bei der chemischen oder physikalischen Raffination gehen wertvolle sekundäre Pflanzenstoffe, geschmackliche Eigenarten und die typische Farbe verloren. Das raffinierte Öl ist weitestgehend geschmacksneutral, von heller Farbe, lange haltbar und universell einsetzbar.

Die Rohware wird kalt gepresst, eine geringe Wärmezufuhr bei der Pressung bis ca. 60 °C ist möglich. Zur Steigerung der Haltbarkeit werden diese Öle teilweise gedämpft. Bei der Dämpfung werden wie bei der Raffination erwünschte Begleitstoffe vermindert.

Kalt gepresste Öle werden ohne Wärmezufuhr nur durch Druck oder Reibung in meist dezentralen Ölmühlen hergestellt. Auf die Pressung folgt meist eine Filtration. Die Öle enthalten alle Inhaltsstoffe. Diese haben positiven Einfluss auf die Qualitätskriterien wie Geschmack, Geruch, Farbe und Vitamingehalt.

Native Öle sind naturbelassen und kalt gepresst ohne weitere Wärmezufuhr. (bei Olivenöl auch "Virgin") Die kalte Pressung ohne Wärmezufuhr vermeidet Oxidation. Das Öl wird filtriert. Weder Öl noch Rohware werden vor- oder nachbehandelt, etwa durch Raffination, Dämpfung oder Rösten der Saat. Es bleiben alle Inhaltsstoffe erhalten. Der deutliche Frucht- oder Saatgeschmack, Geruch und intensive Farbe sind charakteristisch.

Die Saat wird in Schälmühlen von der Schale befreit. Die Kerne werden anschließend zu kalt gepresstem, nativem Öl weiterverarbeitet. Auf eine Raffination kann verzichtet werden. Das gewonnene Öl ist ein reines Kernöl. Wie bei den kalt gepressten oder nativen Ölen bleiben die Inhalts- und Geschmacksstoffe sowie die Vitamine erhalten. Durch die Schälung werden unerwünschte Geschmacksbeeinträchtigungen und Trübungen, die von den Schalen ausgehen, vermindert.

Pflanzenöle finden vielfältige Verwendung, so unter anderem als





</doc>
<doc id="4085" url="https://de.wikipedia.org/wiki?curid=4085" title="Prokura">
Prokura

Prokura (, Vollmacht, von , für etwas Sorge tragen, zu , für, und , Sorge) ist in Deutschland, Österreich und in der Schweiz bei Unternehmen eine, durch einen Kaufmann an Mitarbeiter erteilte, umfangreiche geschäftliche Vertretungsmacht. Sie stellt, wie die Handlungsvollmacht, eine gewillkürte Form der Stellvertretung dar und hat den Zweck, dem Handelsverkehr eine sichere Grundlage für das Vertretungshandeln der kaufmännischen Gehilfen zu bieten.

Die Rechtsgrundlagen der Prokura finden sich in bis HGB. Danach ermächtigt sie gemäß Abs. 1 HGB „zu allen Arten von gerichtlichen und außergerichtlichen Geschäften und Rechtshandlungen, die der Betrieb eines Handelsgewerbes mit sich bringt“. Durch Verwendung des unbestimmten Artikels „eines Handelsgewerbes“ will der Gesetzgeber zum Ausdruck bringen, dass der Prokurist auch branchenübergreifende Geschäfte abschließen darf. Genau dies ist dem Handlungsbevollmächtigten verwehrt, weil die entsprechende Bestimmung in Abs. 1 HGB die bestimmtere Formulierung „eines derartigen Handelsgewerbes“ verwendet und damit sein Geschäftsfeld auf das Handelsgewerbe beschränkt, in dem er tätig ist. Die Prokura ist ausdrücklich (mündlich oder schriftlich) zu erteilen ( Abs. 1 HGB) und vom Inhaber des Handelsgewerbes nach Abs. 1 HGB im Handelsregister einzutragen. Diese Eintragung hat lediglich deklaratorische Wirkung, da bereits die förmliche Ernennung zum Prokuristen eine handelsrechtliche Prokura begründet.

Die Vorschrift des Abs. 1 HGB legt den Umfang der Prokura zwingend fest, sodass der Prokurist in diesem Rahmen rechtsgeschäftlich im Namen und für Rechnung des Kaufmanns handeln darf und die so abgeschlossenen Geschäfte den Kaufmann verpflichten und berechtigen ( Abs. 1 BGB). Selbst wenn die Prokura im Innenverhältnis zwischen Kaufmann und Prokurist enger ausgestaltet wird als es der gesetzliche Rahmen vorsieht, darf der Prokurist im Außenverhältnis den Umfang seiner handelsrechtlichen Prokura voll ausschöpfen, denn HGB erklärt alle Beschränkungen dieser Prokura für unwirksam. Die Geschäftspartner des Prokuristen dürfen ohne weiteres auf den gesetzlich vorgesehenen Umfang der Prokura vertrauen und brauchen nicht das Risiko einer eventuell fehlenden Vertretungsmacht zu fürchten.

Nicht nur alle typischen und gewöhnlichen Geschäfte werden von einer Prokura erfasst, sondern alle Geschäfte, die sich auch nur mittelbar auf irgendein Handelsgewerbe beziehen, also mit ihm zumindest noch in einem entfernten, lockeren Zusammenhang stehen. Das Vorliegen eines Handelsgeschäfts wird nach HGB vermutet. Diese weitreichende Vollmacht versetzt den Prokuristen in die Lage, rechtswirksam Geschäfte vorzunehmen, die weit außerhalb des Tätigkeitsbereichs seines Unternehmens liegen. Erst recht umfasst die Prokura alle betrieblichen Funktionen (Beschaffung, Produktion, Vertrieb, Versicherung oder Finanzierung). Auch organisatorische oder arbeitsrechtliche Handlungen sind im Rahmen einer Prokura statthaft. 

Ausgenommen von der Prokura sind sogenannte "Grundlagen-" und "Prinzipalgeschäfte". "Grundlagengeschäfte" umfassen alle Handlungen, die das Handelsgeschäft als solches betreffen. Dazu zählen insbesondere die Belastung und Veräußerung von Grundstücken und grundstücksgleichen Rechten ( Abs. 2 HGB), sofern der Prokurist hierzu nicht ausdrücklich ermächtigt wird, aber auch die Veräußerung des Unternehmens, die Aufnahme neuer Gesellschafter, der Antrag auf Insolvenz, die Änderung der Rechtsform oder die Abänderung des Unternehmenszwecks. "Prinzipalgeschäfte" sind originäre Geschäfte des Kaufmanns, z. B. die Erstellung von Handelsbilanzen ( HGB), Anmeldungen zum Handelsregister, die nur der Kaufmann wirksam vornehmen kann (, HGB) oder die Erteilung der Prokura selbst ( Abs. 1 HGB). Dies macht eine sog. "Unterprokura" unzulässig. 

Der Prokurist unterliegt, wie auch ein Geschäftsführer, den Verboten des "Selbstkontrahierens" und der "Mehrvertretung" nach BGB, von beiden (in § 181 BGB enthaltenen) Verboten kann der Prokurist jedoch befreit werden (wie auch der Geschäftsführer), was zur Eintragung in das Handelsregister anzumelden ist (siehe Insichgeschäft). 
Da die Prokura keine Untervollmacht des Geschäftsführers darstellt, kann der Prokurist selber von den Verboten des §181 BGB befreit sein - auch wenn der Geschäftsführer selber diese Befreiung nicht hat.

Führt der Prokurist Geschäfte aus, welche durch seine Vollmacht nicht gedeckt sind, handelt er als Vertreter ohne Vertretungsmacht nach § ff. BGB.
Im Außenverhältnis haftet jedoch immer die durch den Prokuristen vertretene Gesellschaft, im Innenverhältnis kann der Prokurist von der Gesellschaft in Haftung genommen werden.
Zu berücksichtigen ist die mehrfache Bestätigung durch die Rechtsprechung, dass der fremde Dritte jederzeit den gesetzlichen Umfang der entsprechenden Prokura annehmen darf.

Beispiel:

Einzelprokura ist die einer einzelnen Person erteilte Vollmacht, wodurch sie allein vertretungsberechtigt handeln kann. Diese Prokura hat einen umfassenden Charakter.

Bei Filialprokura ist die Prokura auf eine Filiale oder Geschäftsstelle eines Unternehmens beschränkt ( Abs. 3 HGB). Eine Prokura, die sich auf alle Niederlassungen eines Kaufmanns erstreckt, bezeichnet man als Generalprokura. Die Firmen der Zweigniederlassungen müssen sich durch einen Zusatz unterscheiden, etwa „Filiale Grenzach-Wyhlen“.

Bei der echten Gesamtprokura (Kollektivprokura) sind nur zwei oder mehrere Prokuristen zum gemeinschaftlichen Handeln befugt ( Abs. 2 HGB; siehe Gesamtvertretung). Die Prokuristen müssen gemeinschaftlich handeln und unterschreiben regelmäßig gemeinsam.

Ein Prokurist darf insbesondere

Sofern er zur selbständigen Einstellung oder Entlassung von Arbeitnehmern berechtigt ist, gilt für ihn selbst das Kündigungsschutzgesetz nur eingeschränkt (grundlose Beendigung gegen Abfindung, Abs. 2, Abs. 1 S. 2 KSchG).

Er darf hingegen keine höchstpersönlichen Geschäfte des Betriebsinhabers ausführen, die kraft Gesetzes dem Kaufmann vorbehalten sind. Dazu gehören insbesondere


Im Verhältnis zwischen Kaufmann und Prokuristen ist eine Beschränkung der Prokura dem Umfang nach, zum Beispiel durch Dienst- oder Arbeitsvertrag des Prokuristen, möglich.

Beschränkungen im Innenverhältnis sind gegenüber Dritten absolut unwirksam ( Abs. 1 und 2 HGB), daher sind auch die vom Prokuristen ohne Vertretungsbefugnis abgeschlossenen Geschäfte für den Kaufmann verbindlich, sofern nicht der Geschäftsgegner positive Kenntnis von dem Fehlen der Vertretungsbefugnis hatte (siehe auch Abschnitt über den Missbrauch). Der Prokurist ist dem Kaufmann dann aber zum Schadensersatz verpflichtet.

Bei einem Missbrauch der Prokura sind die Grundsätze über den Missbrauch der Vertretungsmacht teilweise anwendbar. Folgende Konstellationen sind denkbar:


Des Weiteren sind Fälle denkbar, in denen die Prokura weder gem. HGB in das Handelsregister eingetragen wurde noch das Erlöschen der Prokura Eintrag in das Handelsregister gefunden hat, die so genannte "sekundäre Unrichtigkeit des Handelsregisters". Hier ist umstritten, ob das Rechtsgeschäft, das ein Vertreter nach dem Erlöschen dieser Prokura vorgenommen hat, trotzdem gegen den Vertretenen wirkt, also ob Absatz 1 HGB anwendbar ist. Bejaht man die Anwendbarkeit, gilt das oben Geschriebene auch in diesem Fall.

Die Prokura erlischt bei:

Hingegen erlischt die Prokura nicht beim Tod des Geschäftsinhabers ( Abs. 3 HGB). Hier wirkt sich die rechtsgeschäftliche Vollmacht aus, zu denen die Prokura gehört. Der Tod des Vollmachtgebers ändert nichts an rechtswirksam erteilten Vollmachten.

Erfolgt nach Widerruf der Prokura keine Löschung im Handelsregister, können sich Dritte auf das Fortbestehen der Prokura berufen ( Abs. 1 HGB).

Die Prokura ist nicht auf Dritte übertragbar.

Das Ausüben der Vollmacht wird durch Hinzufügen eines Hinweises auf die Prokura zum Namen des Kaufmanns (der Firma) und zum Namen des Prokuristen kenntlich gemacht (  HGB). Der Zusatz wird üblicherweise mit "ppa." ( "per procura autoritate", deutsch ‚mit der Macht einer Prokura‘) abgekürzt.

Der Begriff der Prokura ist nahezu identisch mit dem des deutschen Rechts. Die gesetzlichen Regelungen finden sich in OR Art. 458–465. Die Regelung, wie Prokura definiert ist und wie sie erteilt wird, findet sich in OR Art. 458. Auch der Umfang der Prokura ist nahezu identisch mit der im deutschen Recht geregelten Prokura. Besonderheiten ergeben sich aus OR Art. 459. Die Löschung der Prokura muss zwingend in das Handelsregister eingetragen werden. Sofern dies nicht geschieht, können sich gutgläubige Dritte weiterhin darauf berufen.




</doc>
<doc id="4088" url="https://de.wikipedia.org/wiki?curid=4088" title="Parteispende">
Parteispende

Als Parteispenden werden Spenden an politische Parteien bezeichnet. Die Einnahmen aus den Parteispenden bilden zusammen mit den Mitgliedsbeiträgen und öffentlichen Zuschüssen die Grundlage der Parteienfinanzierung.

In Deutschland dürfen sowohl natürliche als auch juristische Personen in unbegrenzter Höhe spenden. Spenden sind in bestimmtem Umfang steuerlich absetzbar, die Parteien erhalten für Spendeneinnahmen zudem einen staatlichen Zuschuss.

2007 finanzierten sich die Parteien zu etwa 15 % durch Parteispenden.

In Wahljahren fließen traditionell deutlich mehr Spenden als in anderen Jahren. 2009 (Bundestagswahl) waren es zum Beispiel etwa 6 Millionen Euro.

2011 flossen insgesamt 2,03 Millionen Euro Großspenden an die Parteien.

2012 waren es 1,3 Millionen Euro (CSU 460.000 Euro; CDU und SPD je etwa 260.000 Euro, FDP 205.000 Euro, Marxistisch-Leninistische Partei Deutschlands (MLPD) 115.000 Euro, Grüne 60.000 Euro).

Das Parteiengesetz (PartG) wurde 2002 (Kabinett Schröder I) wesentlich geändert: Seit dem 1. Juli 2002 müssen Parteispenden von über 50.000 Euro („Großspenden“) unverzüglich beim Bundestagspräsidenten angezeigt und anschließend als Drucksache und auf der Homepage des Bundestags veröffentlicht werden. Parteispenden über 10.000 Euro müssen in den jährlichen Rechenschaftsberichten der Parteien veröffentlicht werden; sie erscheinen etwa eineinhalb Jahre nach Ende des betreffenden Jahres. Spenden über 10.000 Euro (und unter 50.000 Euro) im Wahlkampfjahr 2005 wurden z. B. im Sommer 2007 bekannt.

Bundestagspräsident Norbert Lammert (CDU) wies am 28. Januar 2010 die Parlamentsverwaltung an, Großspenden von mehr als 50.000 Euro sofort schriftlich oder im Internet zu veröffentlichen. Zuvor wurden sie in Sammelübersichten im Vier-Wochen-Rhythmus publik gemacht. Parteien zeigen Großspenden .

Anlass für diese Anweisung waren drei Spenden in Höhe von insgesamt 1,1 Millionen Euro, die die FDP zwischen Oktober 2008 und Oktober 2009 von einem Hotel-Unternehmer erhalten hatte.

Nach Angaben von Lobbycontrol blieben im Berichtsjahr (2014) über drei Viertel der Spenden von Unternehmen und Wirtschaftsverbänden anonym, 7 % mehr als 2013.

Gemäß Abs. 2 Parteiengesetz dürfen die politischen Parteien keine Spenden annehmen:

Für Parteispenden wird dem Steuerpflichtigen gemäß EStG eine Ermäßigung der Einkommensteuer gewährt. 50 % des gespendeten Betrags können direkt von der Steuerschuld abgezogen werden, maximal 825 Euro (erreicht bei einer Spendensumme von 1.650 Euro). Bei Zusammenveranlagung beträgt der maximale Abzug 1.650 Euro (erreicht bei einer Spendensumme von 3.300 Euro). Aufgrund von Solidaritätszuschlag und Kirchensteuer beträgt die Steuerersparnis etwas mehr als die Hälfte der Spendensumme. So ermäßigt sich die Steuerbelastung bei einem Kirchensteuersatz von 9 % und dem Solidaritätszuschlag von 5,5 % um insgesamt 57,25 % der Parteispendensumme.

Werden pro Kalenderjahr mehr als 1.650 Euro (bzw. mehr als 3.300 Euro bei Zusammenveranlagung) an Parteien gespendet, kann der übersteigende Teil der Spendensumme gemäß Abs. 2 EStG als Sonderausgabe abgezogen werden. Für diesen Teil gilt erneut eine Grenze von 1.650 Euro (Zusammenveranlagung 3.300 Euro). Da hierdurch lediglich das zu versteuernde Einkommen gemindert wird, hängt die Steuerersparnis für diesen Teil vom persönlichen Steuersatz ab.

Werden mehr als 3.300 Euro (bei Zusammenveranlagung 6.600 Euro) jährlich an politische Parteien gespendet, ist der übersteigende Teil nicht mehr steuerlich begünstigt.

Absetzbar sind nur Parteispenden von natürlichen Personen – juristische Personen (Unternehmen) können gemäß Abs. 6 EStG Parteispenden nicht absetzen. Der Abzug nach gilt auch für Spenden an kommunale Wählervereinigungen, die jedoch nicht rechenschaftsplichtig sind.

Spenden von natürlichen Personen werden im Rahmen der „staatlichen Teilfinanzierung“ politischer Parteien mit 0,45 Euro für jeden Euro bezuschusst. Dabei werden pro Person höchstens bis 3.300 Euro berücksichtigt.

Die nachfolgenden Statistiken sollen einen Überblick über die größten Spender und Spendenempfänger verschaffen.

Für den Zeitraum 2003–2007.


<nowiki>*)</nowiki> Das Spendenvolumen der Familie Quandt und der im Familienbesitz befindlichen Unternehmen BMW und Altana sind eventuell als eine gemeinsame Spende der Familie zu werten.

Für den Zeitraum 2000-2008. Ab 2007 sind hierbei nur Großspenden ab 50.000 Euro berücksichtigt sind, da kleinere Spenden durch die Rechenschaftsberichte der Parteien noch nicht veröffentlicht wurden.




Im Jahr der Bundestagswahl 2013 gab es wesentlich mehr Großspenden an die Parteien als im Jahr zuvor.

Im Jahr der Europawahl 2014 gab es bisher folgende Großspenden an die Parteien:

Im Jahr 2015 gab es bisher folgende Großspenden an die Parteien:

Aus dem Jahr 2016 wurden bisher folgende Großspenden an die Parteien veröffentlicht:

Im Jahre 2017 wurden folgende Großspenden (größer 50.000€) an die Parteien veröffentlicht:

In Österreich sind seit 1. Juli 2012 Spenden, die im Einzelfall die Höhe von 50.000 Euro übersteigen, unverzüglich dem Rechnungshof zu melden. Dieser hat die Spenden unter Angabe des Spenders auf der Website des Rechnungshofes zu veröffentlichen. Diese Veröffentlichung wird fortlaufend aktualisiert. Eine zeitnahe Offenlegung wird von den Parteien mitunter umgangen, indem Spenden von über 50.000 Euro in mehrere Transaktionen unter 50.000 Euro gestückelt werden.

Zudem hat jede politische Partei über die Art ihrer Einnahmen und Ausgaben jährlich bis 30. September des jeweiligen Folgejahres dem Rechnungshof einen Rechenschaftsbericht zu übermitteln. In einer Anlage zum Rechenschaftsbericht sind Spenden gesondert auszuweisen. Spenden, deren Gesamtbetrag in einem Rechenschaftsjahr 3.500 Euro übersteigen, sind unter Angabe des Namens und der Anschrift des Spenders auszuweisen. Der Rechenschaftsbericht ist nach Prüfung durch den Rechnungshof auf der Webseite des Rechnungshofes als auch auf der Webseite der jeweiligen politischen Partei zu veröffentlichen. Die bislang publizierten Rechenschaftsberichte können auf der Internetseite des Rechnungshofs eingesehen werden.

Eine zivilgesellschaftliche Plattform dokumentiert offengelegte Parteispenden sowie Einnahmen der Parteien aus Sponsoring und Inseraten in Parteimedien, die in den Rechenschaftsberichten ebenfalls offengelegt werden müssen.

Die Schweiz gehört zu den wenigen europäischen Ländern, die über keine besonderen gesetzlichen Bestimmungen zu den politischen Parteien verfügen, weder zu deren Finanzierung noch zur Finanzierung von Wahlkampagnen. Parteispenden sind auf Bundesebene bis CHF 10'000 pro Jahr und auf Kantonsebene bis zu einem vom Kanton festgelegten Betrag steuerlich abziehbar.

Verstöße gegen die jeweiligen gesetzlichen Regelungen zu Parteispenden werden vielfach als "Spendenaffäre" bzw. "Parteispendenaffäre" bezeichnet, zum Beispiel:


Ab 2005 gab es einige Affären, die durch englische Medien wie die Sunday Times und Channel 4 ausgelöst wurden:




</doc>
<doc id="4089" url="https://de.wikipedia.org/wiki?curid=4089" title="Politische Partei">
Politische Partei

Eine politische Partei (, Genitiv ‚Teil‘, ‚Richtung‘) ist ein auf unterschiedliche Weise organisierter Zusammenschluss von Menschen, die innerhalb eines umfassenderen politischen Verbandes (eines Staates o. Ä.) danach streben, möglichst viel politische Mitsprache zu erringen, um ihre eigenen sachlichen oder ideellen Ziele zu verwirklichen und/oder persönliche Vorteile zu erlangen. Wesentlicher Teil des Erringens bzw. Ausübens solcher politischer Macht ist es, Führungspositionen in staatlichen und anderen Institutionen (zum Beispiel Öffentlich-rechtlicher Rundfunk) mit Parteimitgliedern oder der Partei nahestehenden Menschen zu besetzen.

Innerhalb eines Mehrparteiensystems konkurrieren politische Parteien untereinander um die Besetzung der politischen Entscheidungspositionen; sie tragen zur politischen Willensbildung bei und bilden insofern eine wichtige Säule der politischen Verfasstheit eines demokratischen Staates. In einem Einparteiensystem ändern sich notwendigerweise auch Struktur und Funktion einer Partei. Eine derartige Partei „neuen Typs“ bekommt in der Regel „totalitären“ Charakter.

In der Massendemokratie erfolgt die politische Willensbildung, indem der Wähler verschiedene Kandidaten der einzelnen Parteien wählt. Parlament und Regierung beeinflussen das Volk mittels Gesetzen und Verwaltungsakten. In diesem Prozess gegenseitiger Einwirkung kommt den Parteien die Aufgabe der Integration und Repräsentation des Wählerwillens gegenüber dem Staate zu. Parteien müssen in der demokratischen Struktur für die staatlichen Organe sowie für die Stimm- und Wahlberechtigten zeit- und kostenintensive Dienstleistungen erbringen, die von staatlichen Organen oder anderen privaten Institutionen nicht wahrgenommen werden können. Die Zeit wird von den Parteimitgliedern aufgebracht, und die Kosten werden aus den Mitgliederbeiträgen, in Deutschland inzwischen durch eine überwiegend vom Staat übernommene Parteienfinanzierung aufgebracht.

Eine besonders wichtige und wertvolle Aufgabe der Parteien ist die Evaluation (Bewertung) und Nomination (Benennung) von Kandidaten für staatliche Aufgaben. Die staatlichen Organe erwarten außerdem, dass die Parteien zu allen Sachgeschäften umgehend und professionell Stellung nehmen. Die Stimm- und Wahlberechtigten erwarten, dass sie von den Parteien informiert werden.

Es gibt Mehrparteien- und Zweiparteiensysteme, was nicht zuletzt durch das jeweils herrschende Wahlrecht bedingt ist. Das Mehrheitswahlrecht trägt zur Bildung von Zweiparteiensystemen bei, z. B. Vereinigtes Königreich, USA. Hierbei ist nur eine Regierungs- und eine Oppositionspartei im Parlament vertreten, allerdings bei vergleichsweise geringer Bindung des Abgeordneten an Vorgaben seiner Partei (v. a. USA, bei GB durch innerparlamentarische Opposition wieder stärkere Bindung an die Partei). Das Verhältniswahlrecht hingegen begünstigt die Bildung eines Parlaments mit mehreren Parteien, wobei jedoch im Allgemeinen die Fraktionsdisziplin eine größere Rolle spielt (z. B. Niederlande, Deutschland). Zweiparteiensysteme führen zu klaren Mehrheiten, die Regierungsbildung ist relativ einfach zu vollziehen. Mehrparteiensysteme führen zu Koalitionsregierungen, die schwieriger zu bilden sind und bei denen es leicht zu internen Konflikten kommt. Andererseits bildet ein Mehrparteiensystem die komplexe gesellschaftliche Wirklichkeit besser ab. In diesem Zusammenhang findet das Medianwählermodell Anwendung.

Einparteiensysteme finden sich nur in nicht-demokratischen Staaten.







Als Obrigkeitsstaat mit seiner Selbstinterpretation als „überparteiliches“ Gebilde verwehrte das Deutsche Reich den Parteien den Zugang zu staatlichen Organen, innerhalb derer sie erst zur Geltung hätten kommen können. Hegels Lehre vom Staate als dem „sittlich Ganzen“ setzt „Partei“ gleich mit der „Gewalt Weniger“, dem „besonderen, zufälligen Interesse“. Sie traf sich hierin mit Jean-Jacques Rousseaus fiktiver radikaler Demokratie. Es waren nicht die Monarchie oder der militärische und zivile Beamtenstab, die die Bildung der Parteien behinderten; denn Parteien entstehen gegen die autoritäre Herrschaft, indem ein bisher ausgeschlossener Teil an der Herrschaft zu partizipieren verlangt, gemäß dem englischen Vorbild, wo eine Partei die parlamentarische Herrschaft ausübt und die andere eine regierungsfähige Opposition bildet. Zur Zeit der Schaffung des deutschen Nationalstaates auf der Basis der Volkssouveränität sind in der Theorie Parteien ausgeschlossen, um der Einheit willen, die eine Identität von Herrschern und Beherrschten verlangt.

Auch das Staatsrecht war geteilter Meinung über den Parteienstaat. Doch die „Lebenslüge des Obrigkeitsstaates“ von der Überparteilichkeit des Staates lässt die Weimarer Reichsverfassung sie nur „mit einer negativen Gebärde sprödester Abwehr“ (Leo Wittmayer) erwähnen.

In Art. 130 Weimarer Verfassung verlautet, Und: In der politischen Praxis bedeutete dies: Dasselbe vollzog sich im Bereich der Wahlen: 

Hierbei gab es mehrere Entwicklungsstufen. Die Ansicht, dass Parteien für den Parlamentarismus unerlässlich seien, als , hatte sich noch nicht durchgesetzt.

Im Bonner Grundgesetz fand hingegen ein neues Verständnis der Parteien seinen Ausdruck. 

Die im Grundgesetz vorgesehene Diese singuläre Mittellage, die dem bisherigen Verfassungsrecht fremd war, hat das Bundesverfassungsgericht mit dem Ausdruck „Inkorporation in das Verfassungsgefüge“ umschrieben.

In Abs. 1 GG heißt es:
Zielt eine Partei auf Beseitigung oder Beeinträchtigung der freiheitlichen demokratischen Grundordnung ab, so ist sie nach Abs. 2 Satz 1 GG verfassungswidrig und kann vom Bundesverfassungsgericht verboten werden. Wichtig ist jedoch, dass hierüber gemäß Abs. 2 Satz 2 GG nur das Bundesverfassungsgericht zu entscheiden hat. Erst durch ein entsprechendes Urteil verliert eine solche Partei dann den Schutz durch die Verfassung.

Das in Abs. 3 GG vorgesehene Gesetz über die politischen Parteien kam erst 1967 zustande. Das Parteiengesetz schreibt unter anderem vor, dass eine Partei demokratisch organisiert sein und zudem offenlegen muss, woher sie ihre (finanziellen) Mittel hat. Parteien und ihre Untergliederungen (z. B. Ortsvereine) haben in Deutschland in der Regel die Rechtsform eines nicht rechtsfähigen (d. h. nicht eingetragenen) Vereins im Sinne von BGB. Ausnahmen sind die CSU und die FDP, die als eingetragene Vereine (e. V.) geführt werden. Auch deren Untergliederungen sind jedoch eigene, nicht rechtsfähige Vereine. Die Rechtsform des nicht rechtsfähigen Vereins hatte früher zur Folge, dass Parteien sich der Konstruktion eines Treuhänders bedienen mussten, wenn sie etwa Grund- und Unternehmensvermögen kaufen und halten wollten. Seit der Bundesgerichtshof im Jahr 2001 seine Rechtsprechung zur Gesamthandsdoktrin geändert hat, wird jedoch auch der nicht rechtsfähige Verein (trotz seiner Bezeichnung) als rechts- und parteifähig behandelt, sodass die Treuhänderkonstruktion unnötig wurde. Da nach der derzeitigen Rechtsprechung aber nur die höchste und zweithöchste Organisationsebene einer Partei (in der Regel der Bundesverband und die Landesverbände) als Eigentümer eines Grundstücks ins Grundbuch eingetragen werden können, kann die Treuhänderkonstruktion für das Halten von Immobiliareigentum auch weiterhin sinnvoll bleiben.

Zur Gründung einer Partei in Deutschland bedarf es zunächst einer politischen Vereinigung, beispielsweise muss diese nach PartG über eine Mindestzahl an Mitgliedern verfügen, eine Anzahl von 55 Personen wurde von einem Gericht als zu gering bewertet. Hingegen wurde die Partei „Nein!-Idee“ mit 61 Mitgliedern für die Bundestagswahl 2013 vom Bundeswahlausschuss als Partei anerkannt. Die Partei muss innerhalb von sechs Jahren an mindestens einer Bundestags- oder Landtagswahl teilnehmen, um weiterhin als Partei anerkannt zu werden. Nur natürliche Personen können Mitglied einer Partei sein.

Die Zulassung von Parteien zu Wahlen wird über Wahlgesetze geregelt – z. B. bei Bundestagswahlen über das Bundeswahlgesetz. 

In der Schweiz müssen Parteien nicht registriert werden, da sie in der Regel als Verein im Sinne des Art. 60 Zivilgesetzbuch (ZGB) organisiert sind. Insofern können sich Schweizer Parteien ihre eigenen Statuten geben und Einschränkungen wie Altersgrenzen, Einschränkungen wegen Herkunft oder Stimmberechtigtkeit usw. verordnen. Dadurch können auch juristische Personen einer Partei beitreten, sofern die Partei dies nicht einschränkt. Auf Grund der fehlenden eigenen Rechtsform für Parteien, werden sie staatlich nicht finanziert. Im parlamentarischen Tagesgeschäft sind Fraktionen maßgebend.

In Österreich ist politische Partei nach des Parteiengesetzes 2012 „eine dauernd organisierte Verbindung, die durch gemeinsame Tätigkeit auf eine umfassende Beeinflussung der staatlichen Willensbildung, insbesondere durch die Teilnahme an Wahlen zu allgemeinen Vertretungskörpern und dem Europäischen Parlament, abzielt“. Politische Parteien erlangen Rechtspersönlichkeit durch Hinterlegung ihrer Satzung beim Bundesministerium für Inneres. Da dem Bundesministerium für Inneres im Zusammenhang mit dem Gründungsvorgang keine Kontroll- oder Entscheidungsbefugnisse zukommen, kann die Existenz einer politischen Partei nur in einem anderen Verfahren als Vorfrage beurteilt werden.

Das Recht zur Teilnahme an Wahlen ist von der Gründung einer politischen Partei völlig unabhängig; im österreichischen Verfassungsrecht wird strikt zwischen den politischen Parteien einerseits und den nur zum Zweck der Kandidatur an einer einzigen Wahl gebildeten „wahlwerbenden Parteien“ (auch „Wahlparteien“ genannt) andererseits unterschieden. Rechtsgrundlagen für die wahlwerbenden Parteien sind die jeweiligen Wahlordnungen.

Das Parteiengesetz 2012 trifft nähere Vorschriften über die Prüfung der Rechnungslegung und die Offenlegungsverpflichtungen; diese gelten neben den politischen Parteien teilweise auch für die wahlwerbenden Parteien. Eine Abmeldung einer Partei ist erst seit dem Inkrafttreten des Parteiengesetzes 2012 vorgesehen; daher gibt es in Österreich auch über 710 sogenannter „Parteileichen“.

In Polen kann es zur Gründung einer Partei kommen, wenn mindestens ein tausend polnische Staatsbürger einen Gründungsantrag unter Angabe der Personenregisternummer unterzeichnen und dieser durch das Bezirksgericht Warschau als rechtsgültig anerkannt wird. Mitglieder einer Partei müssen mindestens 18 Jahre alt sein. Dem Antrag muss der Namenskürzel sowie das Logo hinzugefügt werden. Eine Teilnahme an Wahlen ist für die polnischen Parteien nicht zwingend vorgeschrieben, jedoch die "Teilnahme an der Öffentlichkeit mittels demokratischen Aktivitäten zwecks Prägung der staatlichen Politik oder Ausübung der Staatsgewalt."

Zur Entstehungszeit der Parlamente waren Parteien meist nur lockere Vereinigungen, die vor allem kurz vor Wahlen tätig wurden, um Kandidaten zu unterstützen. Die ersten Parteien in einem klar definierten Parteiensystem gab es im englischen Parlament um 1690–1695. „Whig“ und „Tory“ definierten mehr und mehr eine politische Vorliebe für die verschiedensten Politikfragen. Seit den 1830er Jahren wurden Parteien in England erstmals auch vollständig mit Regierung und Opposition in Verbindung gebracht. Die Bildung von Parteistrukturen auf lokaler, regionaler und nationaler Ebene sowie der Aufbau von Parteisekretariaten mit besoldeten Parteisekretären geht maßgeblich auf die Sozialdemokratie zurück.

Man unterscheidet heute immer noch grob zwischen „linken“ und „rechten“ Parteien. Diese Unterscheidung geht auf das französische Parlament zur Restaurationszeit zurück. Dort saßen die Gegner der Regierung vom Parlamentspräsidenten aus gesehen links. Andere und oft signifikantere „Unterscheidungen“ sind:

Ein spezieller Fall sind die sogenannten Blockparteien, wie es sie zum Beispiel in der DDR gab. Ihre Funktion bestand darin, breitere Bevölkerungskreise in das Herrschaftssystem einzubinden und so die Herrschaft der führenden SED abzusichern.

Parteien mit besonders kleinem Mitgliederkreis und niedrigen Wahlergebnissen werden auch als Kleinparteien oder Splitterparteien bezeichnet.




Die folgenden Kategorien sind vor allem in der europäischen Politik entstanden. Sie spiegeln die gesellschaftlichen Furchungen, also Konfliktlinien wider, die in verschiedenen Staaten ganz unterschiedlich ausgeprägt sein können. Grundsätzlich ist allerdings seit Mitte der 1980er Jahre eine Auflösung bzw. Schwächung der Konflikte zu beobachten.

Die „klassischen“ Furchungen der Parteienforschung sind:

Diese Konfliktlinien durchkreuzen sich teilweise bzw. überlagern sich gegenseitig, sodass in den einzelnen Parteien, v. a. den größeren mehrere Konfliktlinien zu finden sind.

Unter den Parteien, die nicht zu den Volksparteien zählen, sind mehrere Typen unterscheidbar:


Wer Mitglied einer Partei werden möchte, stellt einen Aufnahmeantrag beim betreffenden Landesverband. Über die Aufnahme des Mitgliedes entscheidet der Vorstand des Kreis- oder Unterverbandes, in dem der Antragsteller wohnt, per Abstimmung. Es kann dafür eine Anhörung anberaumt werden. Je nach parteiinterner Regelung kann auch eine Eintrittserklärung ohne Genehmigung eines Parteigremiums zum Erreichen der Mitgliedschaft ausreichen. Allerdings besteht in den betreffenden Parteien meist explizit die Möglichkeit eines Parteiausschlusses unter bestimmten Bedingungen. Parteimitglieder können sich in unterschiedlich gestufte Parteigremien wählen lassen (Stadt-/Unterverbandsvorstand, Kreis-, Bezirksvorstand, Landes- und Bundesvorstand, Landes- und Bundesfachausschüsse). Wer für eine Partei in ein Parlament gewählt werden möchte, muss sich von seinem Verband als Kandidat ernennen lassen. Während Direktkandidaten von Wahlkreiskonferenzen (Versammlung aller Mitglieder, die im Wahlkreis wahlberechtigt sind) gewählt werden, werden Listenplätze durch Wahlgänge auf einem dafür anzuberaumenden Landesparteitag (Landesdelegiertenversammlung) besetzt.

Die Kritik am Parteienstaat hat in Deutschland ansehnliche Tradition. Sie wird von der Politikwissenschaft erklärt aus der deutschen Ideologie des Obrigkeitsstaates, der angeblich überparteilich die Geschicke des Volkes verwaltet, während Parteien nichts weiter als Sonderinteressen vertreten. Begünstigt wurde diese Anschauung dadurch, dass die deutsche Revolution von 1848 gescheitert war und keine Partei von sich aus an die parlamentarische Macht gelangt war. Der Praxistest durch Regierungsverantwortung blieb demzufolge aus, und die Parteien konnten sich den Luxus weltanschaulicher Aufspalterei erlauben. Darauf folgten die politisch turbulenten Jahre nach dem Ersten Weltkrieg, die zu einer überhitzten Politisierung vormals unpolitischer Schichten führten, wobei sich eine mangelnde Integrationskraft des politischen Systems herausstellte.

Seit einigen Jahrzehnten ist auch in der Gegenwart der Bundesrepublik das Ansehen der Parteien rückläufig, da einerseits der Wohlstand das politische Interesse abnehmen lässt, andererseits regierenden Parteien häufiger Bürgerferne vorgeworfen wird, sei es wegen Kritik an bestimmten einzelnen Entscheidungen, sei es aus wirtschaftlichen Interessen oder weltanschaulichen Motiven. (siehe auch Politikverdrossenheit).

Der russische Literatur-Nobelpreisträger Alexander Issajewitsch Solschenizyn ist einer der bekanntesten Kritiker des Parteien-Parlamentarismus. In einem "Spiegel"-Interview ("DER SPIEGEL" Nr. 30/2007, S. 100) sagte er:





</doc>
<doc id="4091" url="https://de.wikipedia.org/wiki?curid=4091" title="Pfeffergewächse">
Pfeffergewächse

Die Pfeffergewächse (Piperaceae) sind eine Pflanzenfamilie aus der Ordnung der Pfefferartigen (Piperales). Die 13 Gattungen mit etwa 1920 Arten sind in den Subtropen bis Tropen fast weltweit verbreitet. Die für den Menschen wichtigste Gattung aus dieser Familie ist der Pfeffer ("Piper").

Es sind verholzende Pflanzen: kleine Bäume und aufrechte, windende oder schlingende Sträucher (Lianen) oder einjährige bis ausdauernde krautige Pflanzen. Es werden oft Rhizome ausgebildet. Einige Arten duften aromatisch. Pflanzenteile können behaart bis unbehaart oder mit Drüsen bedeckt sein. Sie wachsen terrestrisch oder epiphytisch; einige Arten sind sukkulent. Die meist wechselständig und spiralig oder seltener gegenständig oder wirtelig angeordneten, meist gestielten Laubblätter sind einfach und krautig oder fleischig. Der Blattrand ist glatt. Nebenblätter sind meist vorhanden und sind oft mit den Blattstielen verwachsen.

Die Blüten sind in endständigen, kolbenförmigen, ährigen Blütenständen angeordnet; selten sind es doldige, bei "Zippelia" traubige Blütenstände. Jede Blüte steht fast immer ohne Blütenstiel über einem kleinen Deckblatt. Die kleinen Blüten sind zwittrig oder bei "Piper" eingeschlechtig; wenn sie eingeschlechtig sind dann sind die Arten meist zweihäusig getrenntgeschlechtig (diözisch). Die Blüten besitzen keine Blütenhüllblätter. Es sind ein bis zehn meist freie Staubblätter vorhanden. Ein Teil der Staubblätter kann zu Staminodien umgewandelt sein. Die (zwei bis fünf) meist vier Fruchtblätter sind zu einem oberständigen, einfächerigen Fruchtknoten verwachsen, der mit basaler Plazentation nur eine Samenanlage enthält. Die Griffel enden meist in drei bis vier Narben. 

Es werden Beeren oder Steinfrüchte gebildet, sie enthalten nur einen Samen. Der Same besitzt nur spärlich Endosperm, viel stärkehaltiges Perisperm und einen winzigen Embryo. 

Die ätherischen Öle enthalten meist Alkaloide.

Die Gattung "Peperomia" bildete früher eine eigenständige Familie Peperomiaceae , jetzt wird sie den Piperaceae zugeordnet. Die Familie Piperaceae wurde 1792 von Paul Dietrich Giseke in "Praelectiones in ordines naturales plantarum", S. 123 und am 13. Juni 1824 von Carl Adolf Agardh in "Aphor. Bot.", S. 201 beschrieben. Typusgattung ist "Piper" 

Die Familie der Pfeffergewächse (Piperaceae) wird seit M.-S. Samain et al. 2008 in drei Unterfamilien neu aufgegliedert. Davor gab es die zwei Unterfamilien Piperoideae und Peperomioideae. Heute gehören fünf Gattungen mit etwa 3600 Arten zur Familie. Wenn aus der Gattung "Piper" verschiedene Gattungen ausgegliedert sind können es bis zu zehn Gattungen mehr sein.




Einige Arten werden als Zierpflanzen verwendet.

"Peperomia blanda", "Peperomia tetraphylla" und "Piper nigrum" werden in der chinesischen Medizin verwendet. "Piper hainanense", "Piper hancei", "Piper hongkongense", "Piper nigrum", "Piper sarmentosum", "Piper wallichii", "Piper wangii" und "Piper yunnanense" liefern Gewürze.




</doc>
<doc id="4093" url="https://de.wikipedia.org/wiki?curid=4093" title="Palmen">
Palmen

Palmen bezeichnet:
Palmen ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="4095" url="https://de.wikipedia.org/wiki?curid=4095" title="Palmengewächse">
Palmengewächse

Die Palmengewächse oder Palmen (Arecaceae oder Palmae) sind eine Familie der Ordnung der Palmenartigen (Arecales) innerhalb der Monokotyledonen (Einkeimblättrigen Pflanzen). Verwandte Arten waren schon vor etwa 70 Millionen Jahren in der Kreidezeit weit verbreitet. Die Familie enthält 183 Gattungen mit etwa 2600 rezenten Arten.

In der Familie der Palmengewächse findet sich das längste Blatt (bei Palmen der Gattung "Raphia" mit bis zu 25 Meter Länge), der größte Samen (von der Seychellenpalme "Lodoicea maldivica" mit bis zu 22 Kilogramm Gewicht), und der längste Blütenstand des Pflanzenreichs (in der Gattung "Corypha") mit einer Länge von etwa 7,5 Metern und geschätzten 10 Millionen Blüten pro Blütenstand.

Die Vertreter der Palmengewächse sind sehr vielgestaltig. Sie können klein, mittelgroß oder groß sein, stehen einzeln oder in Gruppen (Cluster), sind bewehrt oder unbewehrt. Sie können mehrmals im Leben (pleonanth) oder nur einmal im Leben blühen (hapaxanth). Sie sind zwittrig, polygam, monözisch oder diözisch.

Der Stamm ist „verholzt“. Er ist schlank bis massiv, sehr kurz bis sehr hoch, er kann kriechend, unterirdisch, kletternd oder aufrecht sein. Normalerweise ist der Stamm im oberirdischen Bereich unverzweigt, selten ist er dichotom verzweigt. Dem Stamm fehlt ein Kambium, Palmen verfügen daher über kein sekundäres Dickenwachstum, weshalb sie nicht zu den Bäumen gerechnet werden. Bei manchen Palmen tritt jedoch ein diffuses Dickenwachstum auf. Die Internodien sind kurz bis lang. Die Blattnarben sind auffällig bis unauffällig. Bei manchen Palmen treten Stelzwurzeln auf.

Die Laubblätter sind wechselständig und stehen in spiraliger Anordnung, selten zweizeilig (distich). Die Blattspreite wird anfangs immer ungeteilt gebildet, häufig spaltet sie sich später auf. Die Blätter können mit Stacheln oder Borsten bewehrt sein, sie sind kahl oder verschiedenartig mit Schuppen und/oder Haaren besetzt. Manchmal besitzen sie ein Ligula-ähnliches Anhängsel an beiden Seiten oder vor dem Blattstiel. Die Blattscheiden bilden manchmal einen Kronschaft. Der Blattstiel ist meist deutlich ausgebildet. Er ist unbewehrt oder verschieden bewehrt. Eine Hastula kann ausgebildet sein oder fehlen.

Die Form der Blattspreite ist gefächert (palmat, Fächerpalme), costapalmat, gefiedert (pinnat), doppelt gefiedert (bipinnat), zweiteilig (bifid) oder ungeteilt, dann aber mit fiederförmiger Aderung. In der Knospe ist das Blatt gefaltet. Später reißt die Spreite entweder entlang der adaxialen Faltkanten auf (induplicat) oder entlang der abaxialen Kanten (reduplicat). Nur selten reißt sie zwischen den Faltkanten auf oder überhaupt nicht. Die derart entstehenden Segmente oder Fiederchen sind lanzettlich oder linealisch, bis hin zu rhombisch oder keilförmig. Im Querschnitt sind die Fiederchen bei den induplicaten Blättern V-förmig, bei den reduplicaten A-förmig. Sie sind einfach oder mehrfach gefaltet, meist besitzen sie eine Mittelrippe und zahlreiche parallel verlaufende Nebenadern. Die Segmente reißen selten zwischen diesen Nebenadern weiter ein. Die Spreite kann unterschiedlich behaart oder beschuppt sein, auch Stacheln und Borsten kommen vor. Die proximalen Blättchen können bei kletternden Arten zu Dornen umgewandelt sein (Akanthophylle), die Rhachis kann distal zu einer Kletterhilfe (Ranke) umgebildet sein und kann dann ebenfalls Akanthophylle tragen.

Die Blütenstände befinden sich meist seitlich (oder seltener an) der Spitze. Befindet sich der Blütenstand an der Spitze der Palme (endständig = terminal), dann stirbt die Pflanze nach dem Blühen und der Samenbildung ab; diese Arten sind hapaxanth, also mehrjährig, alle anderen sind ausdauernd. Die meist stark verzweigten Blütenstände sind im knospigen Zustand von einer Spatha als Schutz umhüllt. Die Blüten können zwittrig sein, meistens sind sie aber eingeschlechtig. Es gibt einhäusige (monözische) und zweihäusige (diözische) Palmen-Arten. Die Blüten sind immer dreizählig und sind meistens radiärsymmetrisch, selten ist der Blütenaufbau schraubig. Es sind meist drei Kelch- und Kronblätter vorhanden; sie sind frei oder verwachsen. Es sind meist sechs, selten weniger Staubblätter vorhanden; davon sind oft einige zu Staminodien reduziert. Die drei Fruchtblätter können völlig frei sein oder an ihrer Basis verwachsen. Jedes Fruchtblatt enthält ein bis zwei Samenanlagen. Selten kann man einen Griffel erkennen und so sind die drei Narben je Blüte meist sitzend. Die Bestäubung erfolgt durch Insekten (Entomophilie) oder durch den Wind (Anemophilie). 

Palmen bilden Schließfrüchte, meist harte Beeren oder Steinfrüchte. Das Perikarp ist glatt, behaart, mit Stacheln oder Schuppen besetzt. Die Früchte enthalten meist einen oder zwei bis drei, nur selten bis zu zehn Samen.

Palmen sind weltweit in tropischen und subtropischen Gebieten verbreitet. Die größte Artenvielfalt findet sich in tropischen Regenwäldern; Palmen wachsen aber auch in einigen saisonalen oder semiariden Gebieten. Ein Beispiel für letzteres ist etwa der recht palmenreiche Cerrado in Zentral-Brasilien.

Im Norden reicht das Areal der Palmen bis etwa 44° nördlicher Breite in Südfrankreich ("Chamaerops humilis"), im Süden etwas über 44° auf den Chatham Islands nahe Neuseeland ("Rhopalostylis sapida"). In Nordamerika reicht "Sabal minor" bis fast 36° nördlicher Breite in North Carolina, "Washingtonia filifera" in Kalifornien bis 37°. In Südamerika reicht "Jubaea chilensis" in Chile bis 35° südlicher Breite. Auf dem asiatischen Festland reicht "Nannorrhops ritchiana" bis etwa 34° nördlicher Breite in Afghanistan und Pakistan. In Australien reicht "Livistona australis" über 37°, in Afrika "Jubaeopsis caffra" bis etwa 31°.

Der Familienname Arecaceae wurde 1820 durch Friedrich von Berchtold und Jan Svatopluk Presl in "O Prirozenosti Rostlin", S. 266, gültig veröffentlicht. Der Familienname Palmae wurde 1789 durch Antoine Laurent de Jussieu in "Genera Plantarum", S. 37, erstveröffentlicht.

Die Arecaceae bilden eine monophyletische Gruppe. Sie sind die Schwestergruppe der Dasypogonaceae. Diese Verwandtschaftsverhältnisse waren lange unklar, weshalb erst 2016 die Dasypogonaceae zu den Arecaceae in die Ordnung Arecales gestellt wurden.

Die Systematik der Arecaceae wurde durch molekulargenetische Studien stark verändert. 2005 wurde eine die verschiedenen Teilarbeiten zusammenfassende Klassifikation der Familie publiziert. Diese Klassifikation wurde 2008 in "Genera Palmarum" mit Aktualisierungen übernommen.
2016 veröffentlichten Baker und Dransfield eine weitere Aktualisierung der Klassifikation, auf der die folgende Systematik beruht::






Die Verwandtschaftsverhältnisse der fünf Unterfamilien sind mittlerweile recht gut untersucht. Sie können durch folgendes Kladogramm dargestellt werden:

Die Früchte von rund 100 Palmenarten sind essbar (Dattelpalme, Palmyrapalme), von anderen deren Samen (Kokosnuss, Betelnuss, Palmyrapalme). Den essbaren Vegetationskegel bezeichnet man als Palmherz, aus dem Mark des Stammes einiger Arten lässt sich Sago gewinnen. Im indomalayischen Raum hat die Sagopalme eine große Bedeutung als Stärkelieferant. Der Saft von Palmen – vor allem der Zuckerpalme – wird zur Herstellung von Getränken benutzt, aus ihm kann auch Zucker gewonnen werden. Durch Gärung entsteht Palmwein. Auch durch die Vergärung von Fruchtfleisch lassen sich Getränke herstellen. Wenige Palmenblüten locken Bienen an, Palmhonig wird nicht von Bienen produziert, sondern entsteht durch Einkochen des Palmensaftes, vergleichbar mit Ahornsirup. Als wirtschaftlich bedeutender Ölproduzent wird die aus Afrika stammende Ölpalme mittlerweile weltweit in den Tropen kultiviert.

In vielen Ländern sind Palmenarten das Grundmaterial für den Hausbau, wobei deren Stämme als Holz und die wasserabweisenden Blätter für die Dach-Eindeckung genutzt werden. Aus Rotangpalmen ("Calamus") werden Rattan-Möbel hergestellt.

Palmenarten werden sowohl als Stilelement zur Vermittlung eines südländischen Eindrucks in öffentlichen Grünanlagen und Gärten genutzt als auch als Zimmerpflanzen. 

In regenarmen Gebieten werden sie zur Stabilisierung des Bodens an Böschungen und (evtl. bewässerten) Grünanlagen verwendet. Sie sind auch wesentlich unempfindlicher gegen Windbruch als Laubbäume, sodass sie gut an windexponierten Stellen gedeihen können.





</doc>
<doc id="4098" url="https://de.wikipedia.org/wiki?curid=4098" title="Phraseologismus">
Phraseologismus

Unter einem Phraseologismus versteht man in der Sprachwissenschaft eine zu einer festen Form verwachsene Folge von Lexemen (Komponenten; Konstituenten), also eine bestimmte Art eines Syntagmas (= grammatische Fügung (Wortverbindung) aus i. d. R. mehreren Wörtern). Die Bedeutung eines solchen sprachlichen Fertigbausteins geht meist über die rein wörtliche Bedeutung ihrer einzelnen Bestandteile hinaus.

Der Ausdruck kommt vom neugr. ', einem Neologismus aus dem altgr. φράσεος ' (Gen. von ', (hier) ‚das Sprechen‘, ‚Ausdruck(sweise)‘ ⇒ Phrase und ', ‚Rechnen‘, ‚Berechnung‘ ⇒ Logismus: „auf die Vernunft gegründeter Schluss“).

In der Regel synonym zu „Phraseologismus“ werden die Termini Phrasem, Phraseolexem, phraseologische Wortverbindung, Wortgruppenlexem und (idiomatische) Redewendung sowie teilweise auch Idiom – hier: im Sinne einer – und Idiomatisierung verwendet (die beiden Letzteren nur zum Teil bzw. selten(er), da es sich hierbei um uneindeutige bzw. mehrdeutige Begrifflichkeiten handelt; vgl. "Artikel").

Der Gegenbegriff zu „Phraseologismus“ und Synonymen lautet „freie Wortverbindung“. Die Unterdisziplin der Sprachwissenschaft, die sich mit diesen "festen Wortverbindungen" beschäftigt, ist die Phraseologie.

Phraseologismen sind und waren immer geschichtlicher Entwicklung unterworfen. Gegenwartssprachliche Phraseologismen sind leichter verständlich, wohingegen historische schwieriger zu erschließen sind. Die Teildisziplin der Phraseologie, die sich mit solchen historischen Phraseologismen befasst, nennt sich entsprechend Historische Phraseologie.

Die drei "Hauptkriterien", die zur Beschreibung von Phraseologismen verwendet werden, sind

"Weitere nennenswerte Eigenschaften" eines Phraseologismus sind

Ein Phraseologismus muss aus mindestens zwei lexikalischen Einheiten bestehen. Eine Maximalgröße existiert nicht (gehen sie in ihrer Struktur allerdings über Satzlänge hinaus, gehören sie nicht mehr zum phraseologischen Bestand). In der Forschung ist man sich uneinig darüber, ob Phraseologismen Autosemantika (bedeutungstragende Wörter) beinhalten müssen, oder ob eine minimale feste Wortverbindung auch aus zwei Synsemantika (bedeutungslose oder -schwache Wörter) bestehen kann. Dieses Postulat, dass ein Phraseologismus aus mindestens zwei Komponenten bestehen muss, kann jedoch durch das Vorhandensein von sogenannten Einwortphraseologismen scheinbar in Frage gestellt sein: Ein "Haarspalter" ist deshalb phraseologisch, da er "Haare spaltet" und ohne diesen Phraseologismus nicht denkbar ist.

Die Festigkeit (oder Stabilität) kommt als formale, lexikalische und semantische Festigkeit vor.

Zusätzlich lassen sich weitere Arten der Festigkeit ausmachen, welche die genannten erweitern:


Die Festigkeit ist ein relatives Kriterium, das heißt, dass Phraseologismen in unterschiedlichem Maß modifiziert werden können. Dies geschieht vor allem in der mündlichen Alltagssprache, in Medientexten (z. B. in der Werbesprache) und in literarischen Texten (einschließlich Liedtexten).

Unter der Idiomatizität versteht man die semantische Umdeutung einzelner Komponenten oder des ganzen Phraseologismus. Die einzelnen Komponenten geben ihre freie Bedeutung zugunsten einer neuen Bedeutung auf.
Die Idiomatizität ist ebenfalls ein relatives Merkmal, denn sie ist einerseits abhängig von Kontext und Vorwissen (vor allem wenn unikale Komponenten auftreten, also Wörter, die in der heutigen Sprache keine freie Bedeutung mehr haben, z. B. „Maulaffen feilhalten“, „jemanden ins Bockshorn jagen“), andererseits ist sie graduell stufbar. So existieren

Phraseologismen kann man nach Burger in Basisklassifikationen einteilen, und zwar anhand ihrer Zeichenfunktion, die sie in der Kommunikation haben.

Referentielle Phraseologismen beziehen sich auf Objekte, Vorgänge und Sachverhalte der Wirklichkeit. Wenn sie diese Objekte, Vorgänge oder Sachverhalte bezeichnen (semantisches Kriterium) und satzgliedwertig (syntaktisches Kriterium) sind, lassen sich solche Phraseologismen als ‚nominative Phraseologismen‘ subklassifizieren.

Beispiele hierfür wären "Schwarzes Gold" (bezeichnet das Objekt Kohle), "jemanden übers Ohr hauen" (bezeichnet den Vorgang des Betrugs).

Nach dem graduell abgestuften Merkmal der Idiomatizität (Erklärbarkeit der Bedeutung ohne historisches Wissen) lassen sich wiederum drei Untergruppen der nominativen Phraseologismen einteilen, nämlich

Wenn referentielle Phraseologismen Aussagen über Objekte, Vorgänge und Sachverhalte machen (semantisches Kriterium) und satzwertig (syntaktisches Kriterium) sind, können sie als ‚propositionale Phraseologismen‘ subklassifiziert werden. Sind diese in einen Kontext eingebettet und nur durch diesen verständlich, bezeichnet man sie als ‚feste Phrasen‘ (z. B. die Redensart "Alles für die Katz!"). Gibt es keinen Anschluss an einen Kontext, so bezeichnet man sie als "topische Formeln". Dies sind beispielsweise Sprichwörter oder Gemeinplätze.

Die bessere Unterscheidung von Redensart und Redewendung soll die folgende Tabelle ermöglichen:
Strukturelle Phraseologismen sind Funktionswörter, die innerhalb einer Sprache grammatische Relationen herstellen. Beispiele sind "entweder … oder, in Bezug auf" oder " nicht nur … sondern auch".

Kommunikative Phraseologismen sind feste Fügungen, die in sich wiederholenden Handlungen (Routinen) meist unbewusst verwendet werden.

Beispiele für situationsgebundene Routineformeln:


Beispiele für nicht situationsgebundene Routineformeln:



Phraseologische Ausdrücke in Texten haben vor allem stilistische Funktionen. So haben sie generelle „‚höhere Expressivität‘ gegenüber nicht-phraseologischen Verbindungen, was stilistisch geeignet ist für das Hervorheben.“ Phraseologische Ausdrücke wirken deshalb viel stärker auf den Leser und intensivieren daher die Aussage des Autors.

Peter Kühn charakterisiert Phraseme als
Demnach verbergen sich also hinter Phraseologismen neben üblichen pragmatischen Funktionen noch weitere. Diese werden nach Sandig unter anderem in folgende eingeteilt:

Phraseme wurden in Deutschland in der Vergangenheit zum Beispiel als Zeichen für die Zugehörigkeit zum Bildungsbürgertum benutzt. Eine bestimmte gesellschaftliche Gruppe kann also spezielle Phraseologie benutzen, um deutlich zu machen, wo in der Gesellschaft sie steht oder selbst glaubt zu stehen. Die Phraseologismen dienen dann als Abgrenzung zu anderen gesellschaftlichen Gruppen.

Unter "Adressatenberücksichtigung" wird verstanden, sowohl den Leser des Textes zu unterhalten, als auch den Text für den Leser zu strukturieren. Phraseologismen können zu beidem beitragen. „Unterhaltsamkeit wird gefördert durch die Verwendung idiomatischer Phraseme und deren spielerische Modifizierungen in unterschiedlichen Kontexten und Intensitäten.“ Gerade die idiomatischen Phraseologismen tragen also dazu bei, den Text für den Leser interessanter zu machen. Strukturierende Funktion haben Phraseme, wenn sie an eine mit dem Hörer geteilte Wissensbasis anknüpfen. Die oftmals verschleiernde Wirkung bestimmter idiomatischer Phraseme wirkt zum Beispiel in dieser Art und Weise.





</doc>
<doc id="4099" url="https://de.wikipedia.org/wiki?curid=4099" title="Peristaltik">
Peristaltik

Der Begriff Peristaltik (von griech.: "peri" „herum“ und "stellein" „in Gang bringen“) bezeichnet die Muskeltätigkeit verschiedener Hohlorgane:

Bei der propulsiven Peristaltik handelt es sich um ringförmig einschnürende Kontraktionen der glatten Muskulatur, die sich in eine Richtung fortsetzen und auf diese Weise dem Transport des Inhalts dienen. Die Peristaltik beruht zum Teil auf einem Eigenrhythmus der Muskulatur (besonders an Magen und Harnleiter) und teilweise auf lokalen Reflexen (besonders im Darm: Peristaltischer Reflex). Zusätzlich wird die Peristaltik durch den Parasympathikus gefördert und durch den Sympathikus gehemmt.

Die nicht-propulsive Peristaltik des Darmes, auch als „Segmentationen“ bezeichnet, dient insbesondere der Durchmischung des Darminhalts.

Eine retrograde Peristaltik findet sich als Transportbewegung in die umgekehrte Richtung in der Speiseröhre bei Erbrechen sowie bei Wiederkäuern, physiologischerweise aber auch im Dickdarm, wo sie der Stuhlspeicherung dient.

Im Gegensatz zu der gleichmäßigeren Peristaltik des Dünndarmes wird der Dickdarminhalt durch periodische Massenbewegungen angetrieben. Sie treten etwa ein- bis dreimal pro Tag im Dickdarm auf und treiben den Darminhalt zum Rektum voran. Diese Massenbewegungen sind durch den gastrokolischen Reflex auslösbar.

Hormone und Neurotransmitter, die die Peristaltik beeinflussen sind:


Die Peristaltik ist reduziert:

Die Peristaltik ist vermehrt:

Antiperistaltik, Peristaltikpumpe



</doc>
<doc id="4100" url="https://de.wikipedia.org/wiki?curid=4100" title="Photovoltaik">
Photovoltaik

Unter Photovoltaik bzw. Fotovoltaik versteht man die direkte Umwandlung von Lichtenergie, meist aus Sonnenlicht, in elektrische Energie mittels Solarzellen. Seit 1958 wird sie in der Raumfahrt genutzt. Inzwischen wird sie überwiegend auf der Erde zur Stromerzeugung eingesetzt und findet unter anderem Anwendung auf Dachflächen, bei Parkscheinautomaten, in Taschenrechnern, an Schallschutzwänden, Freiflächen und auf dem Wasser.

Der Begriff leitet sich aus dem griechischen Wort für „Licht“ (φῶς, phos, im Genitiv: φωτός, photos) sowie aus der Einheit für die elektrische Spannung, dem Volt (nach Alessandro Volta) ab. Die Photovoltaik ist ein Teilbereich der Solartechnik, die weitere technische Nutzungen der Sonnenenergie einschließt.

Ende 2017 waren weltweit Photovoltaikanlagen mit einer Leistung von rund 400 GW installiert. Zwischen 1998 und 2015 stieg die weltweit installierte Photovoltaik-Leistung mit einer Wachstumsrate von durchschnittlich 38 % pro Jahr. Es wird erwartet, dass der jährliche Zubau bis 2020 auf 100 GW steigt und die installierte Leistung bis 2030 zwischen 3.000 und 10.000 GW erreicht. 2014 betrug der weltweite Marktanteil von kristallinen Siliziumzellen etwa 90 %. Prognosen gehen davon aus, dass Siliziumzellen auch langfristig die dominierende Photovoltaik-Technologie bleiben und gemeinsam mit Windkraftanlagen die „Arbeitspferde“ der Energiewende sein werden.

Die Photovoltaik galt lange als die teuerste Form der Stromerzeugung mittels erneuerbaren Energien; eine Sicht, die mittlerweile durch die starken Kostensenkungen der Anlagenkomponenten jedoch überholt ist. Mit Stand 2017 sind die Kosten der Stromerzeugung aus Photovoltaik binnen sieben Jahren um fast 75 % gefallen. In den USA sind bei Solarparks Vergütungen von unter 5 US-Cent/kWh ( Euro-Cent/kWh) üblich; ähnliche Werte sind unter günstigen Umständen auch in anderen Staaten möglich. In mehreren Staaten wurden in Ausschreibungen Rekordwerte von 3 US-Cent/kWh ( Euro-Cent/kWh) erreicht. In Deutschland liegen die Stromgestehungskosten von neu errichteten Photovoltaik-Großanlagen mit Stand 2018 inzwischen niedriger als bei allen anderen fossilen oder erneuerbaren Energien.

Bereits 2014 lagen die Stromgestehungskosten der Photovoltaik in bestimmten Regionen der Erde auf gleichem Niveau oder sogar niedriger als bei fossilen Konkurrenten. Inklusive Speicher, die bei hohem Anteil der Photovoltaik am Strommix notwendig werden, waren die Kosten zu diesen Zeitpunkt jedoch noch höher als bei fossilen Kraftwerken. Allerdings war Solarstrom auch zu diesem Zeitpunkt bereits konkurrenzfähig, wenn die externen Kosten der fossilen Stromerzeugung (d. h. Umwelt-, Klima- und Gesundheits­schäden) mit berücksichtigt wurden; selbst wenn diese nur zum Teil internalisiert wurden.

Die Photovoltaik basiert auf der Fähigkeit bestimmter Materialien, Licht direkt in Strom umzuwandeln. Der photoelektrische Effekt wurde bereits im Jahre 1839 von dem französischen Physiker Alexandre Edmond Becquerel entdeckt. Dieser wurde daraufhin weiter erforscht, wobei insbesondere Albert Einstein mit seiner 1905 erschienenen Arbeit zur "Lichtquantentheorie" großen Anteil an dieser Erforschung hatte, für die er 1921 mit dem Nobelpreis für Physik ausgezeichnet wurde. 1954 gelang es, die ersten Silizium­solarzellen mit Wirkungsgraden von bis zu 6 % zu produzieren. Die erste technische Anwendung wurde 1955 bei der Stromversorgung von Telefonverstärkern gefunden. In Belichtungsmessern für die Photographie fand Photovoltaik weite Verbreitung.

Seit Ende der 1950er Jahre werden Photovoltaikzellen in der Satellitentechnik verwendet; als erster Satellit mit Solarzellen startete Vanguard 1 am 17. März 1958 in die Erdumlaufbahn, und blieb bis 1964 in Betrieb. In den 1960er und 1970er Jahren führte die Nachfrage aus der Raumfahrt zu Fortschritten in der Entwicklung von Photovoltaikzellen, während Photovoltaikanlagen auf der Erde nur für bestimmte Inselanlagen eingesetzt wurden.

Ausgelöst durch die Ölkrise von 1973/74 sowie später verstärkt durch die Nuklearunfälle von Harrisburg und Tschernobyl setzte jedoch ein Umdenken in der Energieversorgung ein. Seit Ende der 1980er Jahre wurde die Photovoltaik in den USA, Japan und Deutschland intensiv erforscht; später kamen in vielen Staaten der Erde finanzielle Förderungen hinzu um den Markt anzukurbeln und die Technik mittels Skaleneffekten zu verbilligen. Infolge dieser Bemühungen stieg die weltweit installierte Leistung von 700 MWp im Jahr 2000 auf 177 GWp im Jahr 2014 an und wächst stetig weiter.

Üblicherweise wird die Schreibung "Photovoltaik" und die Abkürzung "PV" angewendet. Seit der deutschen Rechtschreibreform ist die Schreibweise "Fotovoltaik" die neue Hauptform und "Photovoltaik" eine weiterhin zulässige alternative Schreibung. Im deutschen Sprachraum ist die alternative Schreibweise "Photovoltaik" die gebräuchliche Variante. Auch im internationalen Sprachgebrauch ist die Schreibweise PV üblich. Für technische Fachgebiete ist die Schreibweise in der Normung (hier ebenfalls "Photovoltaik") ein wesentliches Kriterium für die anzuwendende Schreibweise.

Zur Energiewandlung wird der photoelektrische Effekt von Solarzellen genutzt, die zu so genannten Solarmodulen verbunden werden. Die erzeugte Elektrizität kann direkt genutzt, in Akkumulatoren gespeichert oder in Stromnetze eingespeist werden. Vor der Einspeisung in Wechselspannungs-Stromnetze wird die erzeugte Gleichspannung von einem Wechselrichter umgewandelt. Das System aus Solarmodulen und den anderen Bauteilen (Wechselrichter, Stromleitung) wird als Photovoltaikanlage oder Solargenerator bezeichnet.

Die Nennleistung von Photovoltaikanlagen wird häufig in der Schreibweise W (Watt Peak) oder kW angegeben und bezieht sich auf die Leistung bei Testbedingungen, die in etwa der maximalen Sonnenstrahlung in Deutschland entsprechen. Die Testbedingungen dienen zur Normierung und zum Vergleich verschiedener Solarmodule. Die elektrischen Werte der Bauteile werden in Datenblättern angegeben. Es wird bei 25 °C Modultemperatur, 1000 W/m² Bestrahlungsstärke und einer Luftmasse (abgekürzt AM) von 1,5 gemessen. Diese Standard-Testbedingungen (meist abgekürzt STC, engl. "standard test conditions") wurden als internationaler Standard festgelegt. Können diese Bedingungen beim Testen nicht eingehalten werden, so muss aus den gegebenen Testbedingungen die Nennleistung rechnerisch ermittelt werden.

Zum Vergleich: Die Strahlungsstärke der Sonne im erdnahen Weltall (Solarkonstante) beträgt im Mittel 1367 W/m². (Am Boden kommen bei klarem Wetter ca. 75 % dieser Energie an.)

Ausschlaggebend für die Dimensionierung und die Amortisation einer Photovoltaikanlage ist neben der Spitzenleistung vor allem der Jahresertrag, also die gewonnene Strommenge. Die Strahlungsenergie schwankt tages-, jahreszeitlich und wetterbedingt. So kann eine Solaranlage in Deutschland im Juli gegenüber dem Dezember einen bis zu zehnmal höheren Ertrag aufweisen. Tagesaktuelle Einspeisedaten mit hoher zeitlicher Auflösung sind für die Jahre ab 2011 im Internet frei zugänglich.

Der Ertrag pro Jahr wird in Wattstunden (Wh) oder Kilowattstunden (kWh) gemessen. Standort und Ausrichtung der Module sowie Verschattungen haben wesentlichen Einfluss auf den Ertrag, wobei in Mitteleuropa Dachneigungen von 30 - 40° und Ausrichtung nach Süden den höchsten Ertrag liefern. An der maximalen Sonnenhöhe (Mittagssonne) orientiert, sollte in Deutschland bei einer Festinstallation (ohne Nachführung) die optimale Neigung im Süden des Landes ca. 32°, im Norden ca. 37° betragen. Praktisch empfiehlt sich ein etwas höherer Neigungswinkel, da dann sowohl zweimal am Tag (am Vormittag und am Nachmittag) als auch zweimal im Jahr (im Mai und im Juli) die Anlage optimal ausgerichtet ist. Bei Freiflächenanlagen werden deshalb in aller Regel derartige Ausrichtungen gewählt. Zwar lässt sich die über das Jahr verteilte, durchschnittliche Sonnenhöhe und damit die theoretisch optimale Neigung für jeden Breitengrad exakt berechnen, jedoch ist entlang eines Breitengrades die tatsächliche Einstrahlung durch verschiedene, meist geländeabhängige Faktoren unterschiedlich (z. B. Verschattung oder besondere lokale Wetterlagen). Da auch die anlagenabhängige Effektivität bezüglich des Einstrahlungswinkels unterschiedlich ist, muss die optimale Ausrichtung im Einzelfall standort- und anlagenbezogen ermittelt werden. Bei diesen energetischen Untersuchungen wird die standortbezogene Globalstrahlung ermittelt, welche neben der direkten Sonneneinstrahlung auch die über Streuung (z. B. Wolken) oder Reflexion (z. B. in der Nähe befindliche Hauswände oder den Erdboden) einfallende Diffusstrahlung umfasst.

Der spezifische Ertrag ist als Wattstunden pro installierter Nennleistung (Wh/W bzw. kWh/kW) pro Zeitabschnitt definiert und erlaubt den einfachen Vergleich von Anlagen unterschiedlicher Größe. In Deutschland kann man bei einer einigermaßen optimal ausgerichteten fest installierten Anlage pro Modulfläche mit 1 kW mit einem Jahresertrag von ca. 1.000 kWh rechnen, wobei die Werte zwischen etwa 900 kWh in Norddeutschland und 1150 kWh in Süddeutschland schwanken.

Bei den Montagesystemen wird zwischen Aufdach-Systemen und Indach-Systemen unterschieden. Bei einem Aufdach-System für geneigte Hausdächer wird die Photovoltaik-Anlage mit Hilfe eines Montagegestells auf dem Dach befestigt. Diese Art der Montage wird am häufigsten gewählt, da sie für bestehende Dächer am einfachsten umsetzbar ist.

Bei einem Indach-System ist eine Photovoltaik-Anlage in die Dachhaut integriert und übernimmt deren Funktionen wie Dachdichtigkeit und Wetterschutz mit. Vorteilhaft bei solchen Systemen sind die optisch attraktivere Erscheinung sowie die Einsparung einer Dachdeckung, sodass der höhere Montageaufwand oftmals kompensiert werden kann.

Die Aufdach-Montage eignet sich neben Ziegeldächern auch für Blechdächer, Schieferdächer oder Wellplatten. Ist die Dachneigung zu flach, können spezielle Haken diese bis zu einem gewissen Grad ausgleichen. Die Installation eines Aufdach-Systems ist in der Regel einfacher und preisgünstiger als die eines Indach-Systems. Ein Aufdach-System sorgt zudem für eine ausreichende Hinterlüftung der Solarmodule. Die Befestigungsmaterialien müssen witterungsbeständig sein.

Das Indach-System eignet sich bei Dachsanierungen und Neubauten, ist jedoch nicht bei allen Dächern möglich. Ziegeldächer erlauben die Indach-Montage, Blechdächer oder Bitumen­dächer nicht. Auch die Form des Dachs ist maßgebend. Die Indach-Montage ist nur für ausreichend große Schrägdächer mit günstiger Ausrichtung zur Sonnenbahn geeignet. Generell setzen Indach-Systeme größere Neigungswinkel voraus als Aufdach-Systeme, um einen ausreichenden Regenwasserabfluss zu ermöglichen. Indach-Systeme bilden mit der übrigen Dacheindeckung eine geschlossene Oberfläche und sind daher aus ästhetischer Sicht attraktiver. Zudem weist ein Indach-System eine höhere mechanische Stabilität gegenüber Schnee- und Windlasten auf. Die Kühlung der Module ist jedoch weniger effizient als beim Aufdach-System, was die Leistung und den Ertrag etwas verkleinert. Eine um 1 °C höhere Temperatur reduziert die Modulleistung um ca. 0,5 %.

Bisher basiert der Großteil der Photovoltaikanlagen weltweit auf Siliziumtechnik. Daneben konnten verschiedene Dünnschichttechnologien Marktanteile gewinnen. So finden auch weitere Halbleiter Verwendung wie Cadmiumtellurid oder Galliumarsenid. Bei sogenannten Tandem-Solarzellen kommen Schichten unterschiedlicher Halbleiter zur Anwendung.

Als sehr aussichtsreich wird aufgrund der günstigen Herstellung die Entwicklung von Solarmodulen auf Perowskit-Basis beurteilt. Die Zellen können deutlich dünner als Siliziumzellen gebaut werden. Problematisch ist bisher jedoch noch die geringe Haltbarkeit.

Ein weiteres Forschungsziel ist die Entwicklung organischer Solarzellen. Dem Fraunhofer-Institut für Solare Energiesysteme ISE in Freiburg ist es zusammen mit Partnern gelungen, eine günstige organische Solarzelle auf flexibler Folie herzustellen.

Die auf die Erdatmosphäre auftreffende Sonnenenergie beträgt jährlich 1,56 · 10 kWh, was knapp dem 12.000-fachen des Primärenergieverbrauchs der Menschheit im Jahr 2005 (1,33 · 10 kWh/Jahr) entspricht. Von dieser Energie erreicht etwa die Hälfte die Erdoberfläche, womit sie potentiell für die photovoltaische Energiegewinnung nutzbar ist. Einer 2017 im Fachjournal Nature Energy erschienenen Studie zufolge kann die Photovoltaik bis zum Jahr 2050 ca. 30-50 % des weltweiten Strombedarfs technisch und wirtschaftlich decken und damit die dominierende Art der Stromerzeugung werden. Hierbei ist bereits berücksichtigt, dass zu diesem Zeitpunkt das Energiesystem stromlastiger sein wird als derzeit, sodass die Photovoltaik dann auch mittels Sektorkopplung zu einer erheblichen Dekarbonisierung weiterer Sektoren wie dem Verkehrssektor oder dem industriellen Energieverbrauch beitragen könnte.

Die Einstrahlung hängt von der geographischen Lage ab: Nahe dem Äquator, beispielsweise in Kenia, Indien, Indonesien, Australien oder Kolumbien, sind aufgrund der hohen Einstrahlungsdichte die Stromgestehungskosten niedriger als in Mitteleuropa. Zudem schwankt am Äquator der Energieertrag im Jahresverlauf viel weniger als an höheren Breitengraden (ziemlich gleichbleibende saisonale Sonnenstände und Zeiten zwischen Sonnenauf- und -untergang).

Weltweit wurden bis Ende 2017 Photovoltaikanlagen mit einer Leistung von ca. 400 GW installiert. Bis 2020 rechnet die IEA mit einem weiteren Anstieg auf ca. 400 bis 500 GWp. Bis Ende 2015 waren weltweit insgesamt 229 GW Solarleistung installiert worden. In China wurden allein im ersten Quartal 2016 mehr als 7 GW PV-Leistung neu installiert. In Europa beträgt die installierte Gesamtleistung 100 GW. Zwischen 1998 und 2015 wuchs die weltweit installierte Photovoltaik-Leistung um durchschnittlich 38 % pro Jahr. Dies war deutlich stärker als die meisten Wachstumsszenarien angenommen hatten. So sind die tatsächlichen Wachstumsraten historisch nicht nur wiederholt durch die Internationale Energieagentur, sondern auch durch den IPCC, den Wissenschaftlicher Beirat der Bundesregierung Globale Umweltveränderungen sowie Greenpeace unterschätzt worden.

Der Zubau neuer Anlagen hält aus mehreren Gründen an:

Die folgenden Tabellen geben einen Überblick über die Entwicklung der installierten Nennleistung der Photovoltaikanlagen in der Europäischen Union in den Jahren 2005 bis 2015.

Aufgrund stark gesunkener Modulpreise im Zuge billiger Importe aus China ist die deutsche Solarindustrie wie auch die europäische in eine Krise geraten. Zahlreiche Hersteller meldeten Insolvenz an. Im Mai 2013 verhängte die EU-Kommission Strafzölle gegen China, da dieses Land durch enorme staatliche Subventionen unter den Herstellungskosten verkauft (Dumping). Die Strafzölle sind in der Branche und unter Umweltverbänden umstritten. Ende Juli einigten sich China und die EU auf einen Mindestpreis von 56 ct/W und eine jährliche Höchstliefermenge von 7 GW.

Neben der Stromgewinnung zur Netz-Einspeisung wird die Photovoltaik auch für mobile Anwendungen und Anwendungen ohne Verbindung zu einem Stromnetz, so genannte Inselanlagen, eingesetzt. Hier kann der Gleichstrom auch direkt genutzt werden. Am häufigsten finden sich daher akkugepufferte Gleichstromnetze. Neben Satelliten, Solarfahrzeugen oder Solarflugzeugen, die oft ihre gesamte Energie aus Solarzellen beziehen, werden auch alltägliche Einrichtungen, wie Wochenendhäuser, Solarleuchten, elektrische Weidezäune, Parkscheinautomaten oder Taschenrechner von Solarzellen versorgt.

Inselanlagen mit Wechselrichter können auch Wechselstromverbraucher versorgen. In vielen Ländern ohne flächendeckendes Stromnetz ist die Photovoltaik eine Möglichkeit, elektrischen Strom preisgünstiger zu erzeugen als z. B. mit einem Dieselgenerator.

Auch die Einbindung von Photovoltaikanlagen und Solarbatterien in bestehende Inselnetze stellt eine Möglichkeit dar, die Kosten der Energieproduktion deutlich zu verringern.

Der Wirkungsgrad ist das Verhältnis zwischen momentan erzeugter elektrischer Leistung und eingestrahlter Lichtleistung. Je höher er ist, desto geringer kann die Fläche für die Anlage gehalten werden. Beim Wirkungsgrad ist zu beachten, welches System betrachtet wird (einzelne Solarzelle, Solarpanel bzw. -modul, die gesamte Anlage mit Wechselrichter bzw. Laderegler und Akkus und Verkabelung). Der Ertrag von Solarmodulen ist zudem auch temperaturabhängig. Bei einer Temperaturerhöhung von 25 °C nimmt die Leistung eines kristallinen Moduls um ca. 10 % ab. Eine Kombination von Solarzellen und thermischem Sonnenkollektor, sogenannte Hybridkollektoren, steigert den Gesamtwirkungsgrad durch die zusätzliche thermische Nutzung, und kann den elektrischen Wirkungsgrad aufgrund der Kühlung der Solarzellen durch die thermischen Kollektoren verbessern.

Die mit Solarzellen erzielbaren Wirkungsgrade werden unter standardisierten Bedingungen ermittelt und unterscheiden sich je nach verwendeter Zelltechnologie. Der Mittelwert des nominellen Wirkungsgrads waferbasierter PV-Module lag 2014 bei etwa 16 % (nach dem Jahr der Markteinführung), bei Dünnschicht-Modulen liegt er um 6–11 %. Eine Tabelle von Wirkungsgraden einzelner Zelltechnologien findet sich hier. Besonders hohe Wirkungsgrade werden von Mehrfachsolarzellen mit Konzentrator erreicht; hier wurden im Labor bereits Wirkungrade bis ca. 46 % erreicht. Durch die Kombination von Solarzellen unterschiedlicher spektraler Empfindlichkeit, die optisch und elektrisch hintereinander angeordnet sind, in Tandem- oder Tripelschaltung wurde der Wirkungsgrad speziell bei amorphem Silicium erhöht. Allerdings begrenzt bei einer solchen Reihenschaltung stets die Zelle mit dem geringsten Strom den Gesamtstrom der Gesamtanordnung. Alternativ wurde die Parallelschaltung der optisch hintereinander angeordneten Solarzellen in Duo-Schaltung für Dünnschichtzellen aus a-Si auf dem Frontglas und CIS auf dem Rückseitenglas demonstriert.

Ein Vorteil dieser Technik ist, dass mit einfachen und günstigen optischen Einrichtungen die Solarstrahlung auf eine kleine Solarzelle gebündelt werden kann, die der teuerste Teil einer Photovoltaikanlage ist. Nachteilig ist hingegen, dass konzentrierende Systeme wegen der Lichtbündelung zwingend auf Nachführsysteme und eine Kühleinrichtung für die Zellen angewiesen sind.

Heutige Solarmodule absorbieren einen Teil des Sonnenlichts nicht, sondern reflektieren es an ihrer Oberfläche. Daher werden sie in der Regel mit einer Antireflexionsschicht ausgestattet, die die Reflexion bereits stark vermindert. Schwarzes Silicium vermeidet diese Reflexionen fast vollständig.

Die "Performance Ratio" (PR) – häufig auch Qualitätsfaktor (Q) genannt – ist der Quotient aus dem tatsächlichen Nutzertrag einer Anlage und ihrem Sollertrag. Der „Sollertrag“ berechnet sich aus der eingestrahlten Energie auf die Modulfläche und dem nominalen Modul-Wirkungsgrad; er bezeichnet also die Energiemenge, die die Anlage bei Betrieb unter Standard-Testbedingungen (STC) und bei 100 % Wechselrichter-Wirkungsgrad ernten würde.

Real liegt der Modulwirkungsgrad auch bei unverschatteten Anlagen durch Erwärmung, niedrigere Einstrahlung etc. gegenüber den STC unter dem nominalen Wirkungsgrad; außerdem gehen vom Sollertrag noch die Leitungs- und Wechselrichterverluste ab. Der Sollertrag ist somit eine theoretische Rechengröße unter STC. Die Performance ratio ist immer ein Jahresdurchschnittswert. Beispielsweise liegt die PR an kalten Tagen über dem Durchschnitt und sinkt vor allem bei höheren Temperaturen sowie morgens und abends, wenn die Sonne in einem spitzeren Winkel auf die Module scheint.

Die Performance Ratio stieg mit der Entwicklung der Photovoltaik-Technik deutlich an: Von 50-75 % in den späten 1980er Jahren über 70-80 % in den 1990er Jahren auf mehr als 80 % um ca. 2010. Für Deutschland wurden ein Median von 84 % im Jahr 2010 ermittelt, Werte von über 90 % werden in der Zukunft für möglich gehalten. Quaschning gibt mit durchschnittlich 75 % niedrigere Werte an. Demnach können gute Anlagen Werte von über 80 % erreichen, bei sehr schlechten Anlagen sind jedoch auch Werte unter 60 % möglich, wobei dann häufig Wechselrichterausfälle oder längerfristige Abschattungen die Ursache sind.

Wie auf jeder Oberfläche im Freien (vergleichbar mit Fenstern, Wänden, Dächern, Auto usw.) können sich auch auf Photovoltaikanlagen unterschiedliche Stoffe absetzen. Dazu gehören beispielsweise Blätter und Nadeln, klebrige organische Sekrete von Läusen, Pollen und Samen, Ruß aus Heizungen und Motoren, Sand, Staub (z. B. auch Futtermittelstäube aus der Landwirtschaft), Wachstum von Pionierpflanzen wie Flechten, Algen und Moosen sowie Vogelkot.

Bei Anlagen mit Neigungswinkel um 30° ist die Verschmutzung gering; hier liegen die Verluste bei ca. 2-3 %. Stärker wirkt sich Verschmutzung hingegen bei flachen Anstellwinkeln aus, wo Verschmutzungen bis zu 10 % Verluste verursachen können. Bei Anlagen auf Tierställen von landwirtschaftlichen Betrieben sind auch höhere Verluste möglich, wenn Schmutz aus Lüftungsschächten auf der Anlage abgelagert wird. In diesen Fällen ist eine Reinigung in regelmäßigen Abständen sinnvoll.

Stand der Technik zur Reinigung ist die Verwendung von vollentsalztem Wasser (Demineralisiertes Wasser), um Kalkflecken zu vermeiden. Als weiteres Hilfsmittel kommen bei der Reinigung wasserführende Teleskopstangen zum Einsatz. Die Reinigung sollte durchgeführt werden, ohne Kratzer an der Moduloberfläche zu verursachen. Zudem sollten Module überhaupt nicht und Dächer nur mit geeigneten Sicherheitsvorkehrungen betreten werden.

Auch mit einer Wärmebildkamera kann man die Verschmutzung feststellen. Verschmutzte Stellen auf den Modulen sind bei Sonneneinstrahlung wärmer als saubere Stellen.

Die Erzeugung von Solarstrom unterliegt einem typischen Tages- und Jahresgang, überlagert durch Wettereinflüsse. Diese lassen sich durch Wetterbeobachtung einigermaßen zuverlässig vorhersagen ("siehe Meteorologie").

Insbesondere im Frühling und Sommer kann Solarstrom um die Mittagszeit zur Deckung eines Teils der Mittellast genutzt werden - aber nur, wenn es das Wetter zulässt (kein bewölkter Himmel). Im Herbst und Winter (insbesondere in den Monaten November bis Januar) erzeugen die PV-Anlagen in den Regionen von den Polen bis etwa zum jeweiligen 45. Breitengrad wegen der kurzen Sonnenscheindauer und des niedrigen Sonnenstandes nur wenig Strom. Da dann für Heizung und Beleuchtung aber besonders viel Strom gebraucht wird, müssen dann auch besonders viele Kapazitäten aus anderen Energiequellen zur Verfügung stehen. Allerdings liefern Windkraftanlagen im Winter mehr Strom als im Sommer, sodass sich Photovoltaik und Windenergie jahreszeitlich sehr gut ergänzen. Um die statistisch vorhersagbaren Tages-, Wetter- und Jahresschwankungen auszugleichen, sind aber auch Speichermöglichkeiten und schaltbare Lasten zur Verbrauchsanpassung (Smart-Switching in Verbindung mit Smart-Metering) erforderlich.

Tagesaktuelle Einspeisedaten (für Deutschland) sind für die Jahre ab 2011 im Internet frei zugänglich.

Bei einer dezentralen Stromversorgung durch viele kleine Photovoltaikanlagen (PVA) im Leistungsbereich einiger 10 kW liegen Quelle und Verbraucher nah beieinander; es gibt dann kaum Übertragungsverluste und die erzeugte Leistung verlässt den Niederspannungsbereich praktisch nicht (Stand 2009). Der PVA-Betreiber speist die nicht selbst verbrauchte Leistung in das Niederspannungsnetz ein. Bei einem weiteren erheblichen Ausbau der Photovoltaik werden regional Überschüsse entstehen, die per Stromnetz in andere Regionen transportiert oder für den nächtlichen Bedarf gespeichert werden müssen.

Bei Inselanlagen wird die gewonnene Energie in Speichern, meist Akkumulatoren, gepuffert. Die deutlich häufigeren Verbundanlagen speisen den erzeugten Strom direkt in das Verbundnetz ein, wo er sofort verbraucht wird. Photovoltaik wird so zu einem Teil des Strommixes.

Bei Inselanlagen müssen die Unterschiede zwischen Verbrauch und Leistungsangebot der Photovoltaikanlage durch Energiespeicherung ausgeglichen werden, z. B., um Verbraucher auch nachts oder bei ungenügender Sonneneinstrahlung zu betreiben. Die Speicherung erfolgt meist über einen Gleichspannungszwischenkreis mit Akkumulatoren, die Verbraucher bei Bedarf versorgen können. Neben Bleiakkumulatoren werden auch neuere Akkutechnologien mit besserem Wirkungsgrad wie Lithium-Titanat-Akkumulatoren eingesetzt. Mittels Wechselrichter kann aus der Zwischenkreis-Spannung die übliche Netzwechselspannung erzeugt werden.

Anwendung finden Inselanlagen beispielsweise an entlegenen Standorten, für die ein direkter Anschluss an das öffentliche Netz unwirtschaftlich ist. Darüber hinaus ermöglichen autonome photovoltaische Systeme auch die Elektrifizierung einzelner Gebäude (wie Schulen oder Ähnliches) oder Siedlungen in „Entwicklungsländern“, in denen kein flächendeckendes öffentliches Stromversorgungsnetz vorhanden ist. Bereits heute sind derartige Systeme in vielen nicht-elektrifizierten Regionen der Welt wirtschaftlicher als Dieselgeneratoren, wobei bisher jedoch häufig noch die Subventionierung von Diesel die Verbreitung hemmt.

Bei kleineren Anlagen wird alle verfügbare bzw. über dem Eigenverbrauch liegende Leistung in das Verbundnetz abgegeben. Fehlt sie (z. B. nachts), beziehen Verbraucher ihre Leistung von anderen Erzeugern über das Verbundnetz. Bei größeren Photovoltaikanlagen ist eine Einspeiseregelung per Fernsteuerung vorgeschrieben, mit deren Hilfe die Einspeiseleistung reduziert werden kann, wenn die Stabilität des Versorgungsnetzes das erfordert. Bei Anlagen in einem Verbundnetz kann die lokale Energiespeicherung entfallen, da der Ausgleich der unterschiedlichen Verbrauchs- und Angebotsleistungen über das Verbundnetz erfolgt, üblicherweise durch Ausregelung durch konventionelle Kraftwerke. Bei hohen Anteilen von Solarstrom, die mit konventionellen Kraftwerken nicht mehr ausgeglichen werden können, werden jedoch weitere Integrationsmaßnahmen notwendig, um die Versorgungssicherheit zu garantieren.

Hierfür kommen eine Reihe von Power-to-X-Technologien in Frage. Neben der Speicherung sind diese insbesondere Flexibilisierungsmaßnahmen wie z. B. der Einsatz von Power-to-Heat, Vehicle-to-Grid oder die Nutzung intelligenter Netze, die bestimmte Verbraucher (z. B. Kühlanlagen, Warmwasserboiler, aber auch Wasch- und Spülmaschinen) so steuern, dass sie bei Erzeugungsspitzen automatisch zugeschaltet werden. Aus Effizienzgründen sollten zunächst bevorzugt auf die Flexibilisierung gesetzt werden, bei höheren Anteilen müssen ebenfalls Speicherkraftwerke zum Einsatz kommen, wobei zunächst Kurzfristspeicher ausreichen und erst bei sehr hohen Anteilen variabler erneuerbarer Energien auf Langfristspeicher wie Power-to-Gas gesetzt werden sollte.

Trotz des schwankenden Angebots steht die Leistung aus Photovoltaik (etwa 24 Stunden im Voraus durch Wettervorhersagen prognostizierbar) deutlich zuverlässiger zur Verfügung als die eines einzelnen Großkraftwerks. Ein Ausfall oder ein geplanter Stillstand eines Großkraftwerks hat im Stromnetz eine stärkere Auswirkung als der Ausfall einer einzelnen Photovoltaikanlage. Bei einer hohen Anzahl von Photovoltaikanlagen ergibt sich eine im Vergleich zu einer einzelnen Großanlage extrem hohe Einspeise-Zuverlässigkeit.

Um einen Ausfall großer Stromerzeuger abzusichern, müssen Kraftwerksbetreiber Reserveleistung bereithalten. Dies ist bei Photovoltaik bei einer stabilen Wetterlage nicht notwendig, da nie alle PV-Anlagen gleichzeitig in Revision oder Reparatur sind. Bei einem hohen Anteil von dezentraler Photovoltaik-Kleinanlagen muss jedoch eine zentrale Steuerung der Lastverteilung durch die Netzbetreiber erfolgen.

Während der Kältewelle in Europa 2012 wirkte die Photovoltaik netzunterstützend. Im Januar/Februar 2012 speiste sie zur Mittagsspitze zwischen 1,3 und 10 GW Leistung ein. Aufgrund des winterbedingt hohen Stromverbrauchs musste Frankreich ca. 7–8 % seines Strombedarfs importieren, während Deutschland exportierte.

Solarstrom verursacht geringere Umweltschäden als Energie aus fossilen Energieträgern oder Kernkraft und senkt somit die externen Kosten der Energieerzeugung (s. a. externe Kosten bei Stromgestehungskosten).

Noch im Jahre 2011 betrugen die Kosten der Vermeidung von CO-Emissionen durch Photovoltaik 320 € je vermiedener Tonne CO und waren damit teurer als bei anderen erneuerbaren Energiequellen, der Modernisierung des konventionellen Kraftwerksparks oder Maßnahmen zur Energieeinsparung (Gebäudeisolierung), die Kosten von bis zu 45 € je Tonne CO verursachen oder sogar Kostenersparnisse erwirtschaften. Durch die starke Kostensenkung der Photovoltaik sind die Vermeidungskosten einer Hausdachanlage in Deutschland jedoch auf ca. 17-70 € je Tonne CO gefallen, womit die Solarstromerzeugung günstiger ist als die Kosten für Klimawandelfolgeschäden, die mit 80 € je Tonne CO angesetzt werden. In sonnenreicheren Gegenden der Welt werden sogar Vorteile bis ca. 380€ je Tonne vermiedener CO-Emissionen erzielt.

Wie viel CO-Emissionen durch Photovoltaik tatsächlich vermieden werden, hängt dabei auch von der Koordination des EEGs mit dem EU-Emissionshandel ab; außerdem von der für die Herstellung der Module verwendeten Energieform.

Der durchschnittliche Preis für Anlagen bis 100 kW lag im Dezember 2014 in Deutschland bei 1240 € netto je kW. Dieser Preis enthält neben den Modulen auch Wechselrichter, Montage und Netzanschluss. Eine in Deutschland installierte Anlage liefert je nach Lage und Ausrichtung einen Jahresertrag von etwa 700 bis 1100 kWh und benötigt bei Dachinstallation 6,5 bis 7,5 m² Fläche pro kW Leistung.

Die Amortisation ist abhängig vom Zeitpunkt der Inbetriebnahme, der Sonneneinstrahlung, der Ausrichtung und Neigung der Anlage sowie dem Anteil der Fremdfinanzierung. Die langjährige und zuverlässige Förderung durch die Einspeisevergütungen des deutschen EEGs war ein entscheidender Faktor für die starken Kostensenkungen der Photovoltaik.

In Österreich lag der Preis für Photovoltaikanlagen im Jahr 2013 durchschnittlich bei 1792 Euro je kW und somit deutlich über dem in Deutschland im selben Jahr durchschnittlich zu zahlenden Preis von 1510 Euro je kW.

Photovoltaik galt lange als die teuerste Form der Stromerzeugung mittels erneuerbaren Energien. Durch den starken Preisrückgang hat sich dies mittlerweile geändert, sodass Photovoltaik inzwischen konkurrenzfähig zu anderen regenerativen und konventionellen Arten der Stromerzeugung ist. In manchen Teilen der Welt werden PV-Anlagen mittlerweile ganz ohne Förderung installiert. Die konkreten Stromgestehungskosten sind abhängig von den jeweiligen Verhältnissen. In den USA sind z.B. Vergütungen von unter 5 US-Cent/kWh ( Euro-Cent/kWh) üblich. Ähnliche Werte werden auch für anderen Staaten wirtschaftlich darstellbar gehalten, wenn die Strahlungs- und Finanzierungsbedingungen günstig sind. Bei den mit Stand 2017 günstigsten Solarprojekten wurden in Ausschreibungen Stromgestehungskosten von 3 US-Cent/kWh ( Euro-Cent/kWh) erreicht bzw. diese Werte selbst ohne Subventionen noch leicht unterboten.

Durch die Massenproduktion sinken die Preise der Solarmodule, seit 1980 fielen die Modulkosten um 10 % pro Jahr; ein Trend, dessen weitere Fortsetzung wahrscheinlich ist. Mit Stand 2017 sind die Kosten der Stromerzeugung aus Photovoltaik binnen 7 Jahren um fast 75 % gefallen. Nach Swansons Law fällt der Preis der Solarmodule mit der Verdopplung der Leistung um 20 %.

Seit 2018 sind neu gebaute große Photovoltaikanlagen die günstigsten Kraftwerke in Deutschland (siehe Tabelle rechts). Bereits im dritten Quartal 2013 betrugen die Stromgestehungskosten zwischen 7,8 und 14,2 ct/kWh bzw. 0,09 und 0,14 $/kWh. Damit lagen die Stromgestehungskosten von Photovoltaikanlagen bereits zu diesem Zeitpunkt auf dem gleichen Niveau wie die Stromgestehungskosten von neuen Kernkraftwerken wie Hinkley Point C mit prognostizierten Kosten von 0,14 $/kWh im Jahr 2023. Ein direkter Vergleich ist jedoch schwierig, da eine Reihe von weiteren Faktoren wie die wetterabhängige Produktion von der Photovoltaik, die Endlagerung sowie die Versicherung der Anlagen berücksichtigt werden müssen.

Im Januar 2014 war in mindestens 19 Märkten die Netzparität erreicht; die Wirtschaftlichkeit für Endverbraucher wird von einer Vielzahl an Analysedaten gestützt. Das Deutsche Institut für Wirtschaftsforschung (DIW) stellt fest, dass die Kosten für Photovoltaik bislang weit schneller gesunken sind als noch vor kurzem erwartet. So sei in einem jüngsten Bericht der EU-Kommission noch von Kapitalkosten ausgegangen worden, die „bereits heute zum Teil unterhalb der Werte liegen, die die Kommission für das Jahr 2050 erwarte“. Als günstigster Solarpark weltweit galt bis Anfang 2016 eine Anlage in Dubai, der eine Einspeisevergütung von 6 US-Cent/kWh erhält (Stand 2014). Im August 2016 wurde dieser Rekord bei einer Ausschreibung in Chile deutlich unterboten. Dort ergaben sich für einen 120-MWp-Solarpark Stromgestehungskosten von 2,91 US-Cent/kWh ( ct/kWh), was nach Angaben von Bloomberg L.P. die niedrigsten Stromgestehungskosten sind, die jemals bei einem Kraftwerksprojekt weltweit erzielt wurden.

Die Internationale Organisation für erneuerbare Energien (IRENA) prognostizierte im Jahr 2016, dass die Kosten für Solarstrom bis 2025 um bis zu 59 Prozent fallen werden. Als Gründe nannte der Bericht eine Ausweitung der Produktion, effizientere Versorgungsketten und technische Verbesserungen.

Die Modulpreise sind in den letzten Jahren stark gesunken, getrieben durch Skaleneffekte, technologische Entwicklungen, Normalisierung des Solarsiliziumpreises und durch den Aufbau von Überkapazitäten und Konkurrenzdruck bei den Herstellern. Die durchschnittliche Preisentwicklung seit Januar 2009 nach Art und Herkunft ist in der nebenstehenden Tabelle dargestellt. 

Infolge der Marktankurbelung durch Einspeisevergütungen in Deutschland, Italien und einer Reihe weiterer Staaten sowie der damit einhergehenden sehr steilen Lernkurve kam es zu einem drastischen Kostenrückgang bei den Modulpreisen, die von 6 bis 7 US-Dollar/Watt im Jahr 2000 auf 4 $/Watt im Jahr 2006 und 0,4 $/Watt im Jahr 2016 zurückgingen. Historisch betrachtet fielen die Modulpreise über die vergangenen 40 Jahre um 22,5 % pro Verdopplung der installierten Leistung. 

Die weitere Preisentwicklung hängt von der Entwicklung der Nachfrage sowie von den technischen Entwicklungen ab. Die niedrigen Preise für Dünnschichtanlagen relativieren sich teilweise für die fertige Anlage durch den aufgrund des geringeren Wirkungsgrades höheren Installationsaufwand für Anlagen gleicher Leistung. Es handelt sich bei den angegebenen Preisen nicht um Endkundenpreise; die Kosten für die Module haben lediglich einen Anteil von 40–50 % an den Gesamtkosten (Stand 2012).

Insgesamt wächst der Photovoltaikmarkt immer noch stark (um ca. 40 % jährlich); allerdings sind andere erneuerbare Energien, insbesondere die Windkraft an Land, deutlich günstiger je kWh erzeugter Energie. Bis 2025 wird Solarstrom in sonnigen Regionen der Welt billiger als Kohle- oder Gasstrom sein, so eine Studie des Fraunhofer Instituts für Solare Energiesysteme. So werden sich bis 2025 die Erzeugungskosten in Mittel- und Südeuropa auf 4–6 Cent pro Kilowattstunde verringern, bis 2050 auf 2–4 Cent. Zugrunde liegen konservative Annahmen zur technologischen Weiterentwicklung von Solaranlagen.

Bloomberg New Energy Finance sieht einen sog. tipping point bei Wind- und Solarenergie. Die Preise für Wind- und Solarstrom seien in den letzten Jahren stark gefallen und würden heute (1/2014) in einigen Bereichen bzw. Teilen der Welt bereits unter den Preisen der konventionellen Stromerzeugung liegen. Die Preise würden weiter fallen. Die Stromnetze seien weltweit stark ausgebaut worden, so dass diese nun auch Strom aus erneuerbaren Energien aufnehmen und verteilen könnten. Zudem hätten die erneuerbaren Energien weltweit dafür gesorgt, dass die Strompreise stark unter Druck geraten seien. Zudem würden die erneuerbaren Energien enthusiastisch von den Verbrauchern aufgenommen. Bereits im Jahr 2014 soll dieser Systemwechsel für sehr viele Menschen offensichtlich werden.

Seit dem Jahr 2011 liegen die Stromgestehungskosten in Deutschland unterhalb des Haushaltsstrompreises, womit die Netzparität erreicht ist.

Ende 2016 waren 41,3 GW elektrische Nettoleistung installiert. Die Unternehmensberatung Roland Berger und die Prognos AG hielten in ihrer Publikation aus 2010 bis 2020 einen Ausbau auf 70 GW für realistisch. Unter der Annahme, dass elektrische Energie verlustfrei gespeichert werden könnte, wären bei einem durchschnittlichen jährlichen Ertrag von 900 kWh je kW für eine Energieversorgung ausschließlich mit Photovoltaik insgesamt rund 690 GW zu installieren.

Im Juni 2014 stufte Barclays Anleihen von US-Stromversorgern herunter wegen der Konkurrenz durch die Kombination aus Photovoltaik und Energiespeichern, welche zu einem verstärkten Eigenverbrauch führt. Dies könne das Geschäftsmodell der Stromversorger verändern. Barclays schrieb dazu: „Wir rechnen damit, dass in den nächsten paar Jahren sinkende Preise für dezentrale Photovoltaik-Anlagen und private Stromspeicher den Status Quo durchbrechen werden.“ Und weiter heißt es: „In der über 100-jährigen Geschichte der Stromversorger gab es bisher noch keine wettbewerbsfähige Alternative zum Netzstrom. Wir sind überzeugt, dass Photovoltaik und Speicher das System in den nächsten zehn Jahren umgestalten können.“

Im Sommer 2014 hat die Investmentbank Lazard mit Sitz in New York eine Studie zu den aktuellen Stromgestehungskosten der Photovoltaik in den USA im Vergleich zu konventionellen Stromerzeugern veröffentlicht. Die günstigsten großen Photovoltaikkraftwerke können Strom mit 60 USD pro MWh produzieren. Der Mittelwert solcher Großkraftwerke liegt aktuell bei 72 USD pro MWh und die Obergrenze bei 86 USD pro MWh. Im Vergleich dazu liegen Kohlekraftwerke zwischen 66 USD und 151 USD pro MWh, Atomkraft bei 124 USD pro MWh. Kleine Photovoltaikaufdachanlagen liegen jedoch noch bei 126 bis 265 USD pro MWh, welche jedoch auf Stromtransportkosten verzichten können. Onshore-Windkraftanlagen liegen zwischen 37 und 81 USD pro MWh. Ein Nachteil sehen die Stromversorger der Studie nach in der Volatilität von Solar- und Windstrom. Eine Lösung sieht die Studie in Batterien als Speicher (siehe Batterie-Speicherkraftwerk), die bislang jedoch noch teuer seien.

Die Umweltauswirkungen bei der Silizium-Technologie und bei der Dünnschichttechnologie sind die typischen der Halbleiterfertigung, mit den entsprechenden chemischen und energieintensiven Schritten. Die Reinstsiliziumproduktion bei der Silizium-Technologie ist aufgrund des hohen Energieaufwandes und dem Aufkommen an Nebenstoffen maßgebend. Für 1 kg Reinstsilizium entstehen bis zu 19 kg Nebenstoffe. Da Reinstsilizium meist von Zulieferfirmen produziert wird, ist die Auswahl der Lieferfirmen unter Umweltaspekten entscheidend für die Umweltbilanz eines Moduls.

Bei der Dünnschichttechnologie ist die Reinigung der Prozesskammern ein sensibler Punkt. Hier werden teilweise die klimaschädlichen Stoffe Stickstofftrifluorid und Schwefelhexafluorid verwendet. Bei der Verwendung von Schwermetallen wie der CdTe-Technologie wird mit einer kurzen Energierücklaufzeit auf der Lebenszyklus-Basis argumentiert.

2011 bestätigte das Bayerische Landesamt für Umwelt, dass CdTe-Solarmodule im Fall eines Brandes keine Gefahr für Mensch und Umwelt darstellen.

Durch die absolute Emissionsfreiheit im Betrieb weist die Photovoltaik sehr niedrige externe Kosten auf. Liegen diese bei Stromerzeugung aus Stein- und Braunkohle bei circa 6 bis 8 ct/kWh, betragen sie bei Photovoltaik nur etwa 1 ct/kWh (Jahr 2000). Zu diesem Ergebnis kommt ein Gutachten des Deutschen Zentrums für Luft- und Raumfahrt und des Fraunhofer-Instituts für System- und Innovationsforschung. Zum Vergleich sei der dort ebenfalls genannte Wert von 0,18 ct/kWh externer Kosten bei solarthermischen Kraftwerken genannt.

Auch wenn es im Betrieb selbst keine CO-Emissionen gibt, so lassen sich Photovoltaikanlagen derzeit noch nicht CO-frei herstellen, transportieren und montieren. Die rechnerischen CO-Emissionen von Photovoltaikanlagen betragen Stand 2013 je nach Technik und Standort zwischen 10,5 und 50 g CO/kWh, mit Durchschnitten im Bereich 35 bis 45 g CO/kWh. Eine neuere Studie aus dem Jahr 2015 ermittelte durchschnittliche Werte von 29,2 g/kWh. Verursacht werden diese Emissionen durch Verbrennung fossiler Energien insbesondere während der Fertigung von Solaranlagen. Mit weiterem Ausbau der erneuerbaren Energien im Zuge der weltweiten Transformation zu nachhaltigen Energieträgern wird sich die Treibhausgasbilanz damit automatisch verbessern. Ebenfalls sinkende Emissionen ergeben sich durch die technologische Lernkurve. Historisch betrachtet sanken die Emissionen um 14 % pro Verdopplung der installierten Leistung (Stand 2015).

Nach einem ganzheitlichen Vergleich der Ruhr-Universität Bochum von 2007 lag der CO-Ausstoß bei der Photovoltaik noch bei 50–100 g/kWh, wobei vor allem die verwendeten Module und der Standort entscheidend waren. Im Vergleich dazu lag er bei Kohlekraftwerken bei 750–1200 g/kWh, bei GuD-Gaskraftwerken bei 400–550 g/kWh, bei Windenergie und Wasserkraft bei 10–40 g/kWh, bei der Kernenergie bei 10–30 g/kWh (ohne Endlagerung), und bei Solarthermie in Afrika bei 10–14 g/kWh.

Die Energetische Amortisations­zeit von Photovoltaikanlagen ist der Zeitraum, in dem die Photovoltaikanlage die gleiche Energiemenge geliefert hat, die während ihres gesamten Lebenszyklus benötigt wird; für Herstellung, Transport, Errichtung, Betrieb und Rückbau bzw. Recycling.

Sie beträgt derzeit (Stand 2013) zwischen 0,75 und 3,5 Jahren, je nach Standort und verwendeter Photovoltaiktechnologie. Am besten schnitten CdTe-Module mit Werten von 0,75–2,1 Jahren ab, während Module aus amorphem Silizium mit 1,8 bis 3,5 Jahren über dem Durchschnitt lagen. Mono- und multikristalline Systeme sowie Anlagen auf CIS-Basis lagen bei etwa 1,5 bis 2,7 Jahren. Als Lebensdauer wurde in der Studie 30 Jahre für Module auf Basis kristalliner Siliziumzellen und 20 bis 25 Jahren für Dünnschichtmodule angenommen, für die Lebensdauer der Wechselrichter wurden 15 Jahre angenommen. Bis zum Jahr 2020 wird eine Energierücklaufzeit von 0,5 Jahren oder weniger für südeuropäische Anlagen auf Basis von kristallinem Silizium als erreichbar angesehen.

Bei einem Einsatz in Deutschland wird die Energie, die zur Herstellung einer Photovoltaikanlage benötigt wird, in Solarzellen in etwa zwei Jahren wieder gewonnen. Der Erntefaktor liegt unter für Deutschland typischen Einstrahlungsbedingungen bei mindestens 10, eine weitere Verbesserung ist wahrscheinlich. Die Lebensdauer wird auf 20 bis 30 Jahre geschätzt. Seitens der Hersteller werden für die Module im Regelfall Leistungs-Garantien für 25 Jahre gegeben. Der energieintensiv hergestellte Teil von Solarzellen kann 4- bis 5-mal wiederverwertet werden.

PV-Anlagen werden überwiegend auf bestehenden Dach- und über Verkehrsflächen errichtet, was zu keinem zusätzlichen Flächenbedarf führt. Freilandanlagen in Form von Solarparks nehmen demgegenüber zusätzliche Flächen in Anspruch, wobei häufig bereits vorbelastete Areale wie z. B. Konversionsflächen (aus militärischer, wirtschaftlicher, verkehrlicher oder wohnlicher Nutzung), Flächen entlang von Autobahnen und Bahnlinien (im 110 m Streifen), Flächen die als Gewerbe- oder Industriegebiet ausgewiesen sind oder versiegelte Flächen (ehem. Deponien, Parkplätze etc.) verwendet werden. Werden Photovoltaikanlagen auf landwirtschaftlicher Fläche errichtet, was in Deutschland derzeit nicht gefördert wird, kann es zu einer Nutzungskonkurrenz kommen. Hierbei muss aber berücksichtigt werden, dass Solarparks verglichen mit der Bioenergie­erzeugung auf gleicher Fläche einen um ein Vielfaches höheren Energieertrag aufweisen. So liefern Solarparks pro Flächeneinheit etwa 25 bis 65 mal so viel Strom wie Energiepflanzen.

In Deutschland können auf Dach- und Fassadenflächen mehr als 200 GW Photovoltaikleistung errichtet werden; auf brachliegenden Ackerflächen u. ä. sind über 1000 GW möglich. Damit existiert in Deutschland für die Photovoltaik ein Potential von mehr als 1000 GW, womit sich pro Jahr weit mehr als 1000 TWh elektrischer Energie produzieren ließen; deutlich mehr als der derzeitige deutsche Strombedarf. Da damit jedoch insbesondere in den Mittagsstunden sonniger Tage große Überschüsse produziert würden und enorme Speicherkapazitäten aufgebaut werden müssten, ist ein solch starker Ausbau nur einer Technologie nicht sinnvoll und die Kombination mit anderen erneuerbaren Energien erheblich zweckmäßiger. Wollte man den gesamten derzeitigen Primärenergiebedarf Deutschlands mit Photovoltaik decken, d. h. ca. 3800 TWh, würde dafür ca. 5 % der Fläche Deutschland benötigt. Problematisch ist hierbei die jahreszeitlich und im Tagesverlauf stark schwankende Erzeugung, sodass ein Energiesystem, das ausschließlich auf Solarstrom basiert, unplausibel ist. Für eine vollständig regenerative Energieversorgung ist in Deutschland vielmehr ein Mix verschiedener erneuerbarer Energien erforderlich, wobei die größten Potentiale dabei die Windenergie hat, gefolgt von der Photovoltaik.

Bisher läuft die einzige Recyclinganlage (spezialisierte Pilotanlage) für kristalline Photovoltaikmodule in Europa im sächsischen Freiberg. Die Firma Sunicon GmbH (früher Solar Material), ein Tochterunternehmen der SolarWorld, erzielte dort im Jahr 2008 eine massenbezogene Recyclingquote bei Modulen von durchschnittlich 75 % bei einer Kapazität von ca. 1200 Tonnen pro Jahr. Die Abfallmenge von PV-Modulen in der EU lag 2008 bei 3.500 Tonnen/Jahr. Geplant ist durch weitgehende Automatisierung eine Kapazität von ca. 20.000 Tonnen pro Jahr.

Zum Aufbau eines freiwilligen, EU-weiten, flächendeckenden Systems zur Wiederverwertung gründete die Solarindustrie als gemeinsame Initiative im Jahr 2007 den Verband PV CYCLE. Es werden in der EU bis 2030 ansteigend ca. 130.000 t ausgediente Module pro Jahr erwartet. Als Reaktion auf die insgesamt unbefriedigende Entwicklung fallen seit 24. Januar 2012 auch Solarmodule unter eine Novellierung der Elektroschrott-Richtlinie. Für die PV-Branche sieht die Novelle vor, dass 85 Prozent der verkauften Solarmodule gesammelt und zu 80 Prozent recycelt werden müssen. Bis 2014 sollten alle EU-27-Mitgliedsländer die Verordnung in nationales Recht umsetzen. Man will dadurch die Hersteller in die Pflicht nehmen, Strukturen für die Wiederverwertung bereitzustellen. Die Trennung der Module von anderen Elektrogeräten wird dabei bevorzugt. Bereits existierende Sammel- und Recyclingstrukturen sollen zudem ausgebaut werden.

Die Erzeugung elektrischen Stroms mittels Photovoltaik wird in vielen Staaten gefördert. Nachstehend ist eine (unvollständige) Liste von verschiedenen regulatorischen Rahmenbedingungen in einzelnen Staaten aufgeführt.

In Deutschland gibt es eine gesetzlich geregelte und über 20 Jahre gewährte Einspeisevergütung; die Höhe ist im Erneuerbare-Energien-Gesetz geregelt. Die Einspeisevergütung ist degressiv gestaltet, fällt also für neue Anlagen pro Jahr um einen gewissen Prozentsatz. Zudem gibt es zwölf weitere Programme, die die Anschaffung einer Photovoltaikanlage fördern sollen.

Auf Bundesebene kann die sogenannte Investitionszulage für Photovoltaikanlagen im produzierenden Gewerbe und im Bereich der produktionsnahen Dienstleistungen in Form von Steuergutschriften genehmigt werden.

Daneben stellt die KfW-Förderbank folgende Programme zur Verfügung:


Die Fördergelder der KfW-Förderbank werden im Gegensatz zur Investitionszulage ausschließlich als Darlehen genehmigt und über die jeweilige Hausbank zur Verfügung gestellt.

Des Weiteren haben folgende Bundesländer eigene Solarfördergesetze erlassen:


Weitere Fördermittel und Zuschüsse werden auch von zahlreichen Städten und Kommunen, lokalen Klimaschutzfonds sowie einigen privaten Anbietern angeboten. Diese können teilweise mit anderen Förderprogrammen kombiniert werden.

Ein lokales Förderprogramm bietet die oberbayerische Stadt Burghausen mit 50 € je 100 W installierte Leistung bis max. 1.000 € pro Anlage und Wohngebäude.

Bei einem Jahresumsatz bis 17.500 € gilt die Kleinunternehmerregelung nach UStG. Als Kleinunternehmer muss man keine Steuererklärung abgeben, darf dem Abnehmer aber auch keine Umsatzsteuer in Rechnung stellen. Ein umsatzsteuerpflichtiger Unternehmer (Kleinunternehmer können zur Steuerpflicht optieren) bekommt die Vorsteuer auf alle Investitionen erstattet, muss aber zusätzlich zur Einspeisevergütung dem Abnehmer die Umsatzsteuer in Rechnung stellen und an das Finanzamt abführen.

Für die Einkünfte aus der Photovoltaikanlage gilt EStG. Ein eventueller Verlust mindert die Steuerlast, wenn hierbei keine Liebhaberei vorliegt. Es wäre eine Liebhaberei, wenn sich anhand der auf die Betriebsdauer der Anlage gerichteten Berechnung von vornherein ergeben hat, dass der Betrieb der Anlage keinen Gewinn erwirtschaftet. Soweit einschlägige Renditeberechnungsprogramme einen Steuervorteil berücksichtigen, muss diese Problematik berücksichtigt werden.

Da es für die Gewerbesteuer einen Freibetrag von 24.500 € für natürliche Personen und Personengesellschaften gibt ( Abs. 1 Nr. 1 GewStG), fallen meist nur große Anlagen unter die Gewerbesteuer.

PV eignet sich als Lieferant von Spitzenlaststrom, da sie zur „Kochspitze“ am Mittag die höchsten Erträge erzielt, und verdrängt teure Gas- und Steinkohlekraftwerke aus dem Markt. Solarenergie dämpft daher die Börsenpreise für Spitzenstrom („Merit-Order-Effekt“). Die Spitzenpreise für Strom sind in den letzten Jahren parallel zum Ausbau der Solarenergie im Vergleich zum Durchschnittspreis stark zurückgegangen. Im Sommer sind die früheren Tagesspitzen weitgehend verschwunden. Dieser preissenkende Effekt kommt durch die fehlerhafte Konstruktion des EEG-Ausgleichsmechanismus jedoch nicht beim Privatkunden an, sondern verteuert paradoxerweise die Stromkosten von Privatkunden, während hingegen die Industrie von den gesunkenen Beschaffungskosten an der Strombörse profitiert.

Der Strompreis an der Strombörse war bis zum Jahr 2008 kontinuierlich gestiegen und erreichte im Jahr 2008 das Maximum von 8,279 Cent/kWh. Durch das vermehrte Auftreten der erneuerbaren Energien ist der Strompreis unter Druck geraten. Im ersten Halbjahr 2013 betrug der mittlere Strompreis an der Strombörse nur noch 3,75 Cent/kWh und für den Terminmarkt 2014 lag dieser im Juli 2013 bei 3,661 Cent/kWh.

Durch ein Bürger- bzw. Kundenbeteiligungsmodell hat der Versorger Wien Energie von Mai 2012 bis Ende 2015 Menschen zur Finanzierung von 23 PV-Anlagen im Ausmaß von in Summe 9,1 MW (Stand: 10. Mai 2016) bewegt. Das EVU zahlt den Investoren eine Miete.

Der Ausbau von Photovoltaik wird von der chinesischen Regierung stark vorangetrieben. Die chinesischen Nationalen Energieagentur hat ihre Ausbauziele zuletzt um 30 % erhöht und 2015 Deutschland als größten Installateur von Photovoltaik sowohl insgesamt (21,3 GW) als auch pro Kopf der Bevölkerung der neu installierten Leistung (16,3 W) überholt.

Ein Jahr nach der Nuklearkatastrophe von Fukushima hat die japanische Regierung ein Gesetz nach dem Vorbild des deutschen EEG beschlossen. Seit 1. Juli 2012 wird bei Photovoltaikanlagen mit einer Leistung ab zehn Kilowatt eine Einspeisevergütung von 42 Yen/kWh gezahlt (umgerechnet etwa 0,36 €/kWh). Diese Vergütung wird 20 Jahre lang gezahlt. Kleinere Anlagen bis 10 kW werden nur zehn Jahre lang gefördert.

Der rumänische Staat vergibt aufgrund eines Gesetzes vom November 2011 grüne Zertifikate, gegenwärtig sechs Zertifikate je 1000 kWh bis zum 31. Dezember 2013. Eine Reduzierung der Zahl der Zertifikate war für das Jahr 2014 geplant. Der Wert der grünen Zertifikate wird an der Börse ausgehandelt und sinkt mit der Menge des erzeugten Stroms aus erneuerbaren Energien. Im Februar 2012 belief sich der Preis für ein Zertifikat auf umgerechnet 55 €, so dass für 1 kWh 0,33 € gezahlt wurde. Allerdings kann der Preis auch auf rund die Hälfte sinken.

In der Schweiz existieren zwei Förderungsmodelle: Die Einmalvergütung (EIV) und die kostendeckende Einspeisevergütung (KEV). Solaranlagen unter 10 kWp können nur EIV beantragen, solche zwischen 10 und 30 kWp können zwischen EIV und KEV wählen, während für grössere Anlagen nur die KEV in Frage kommt.

Die EIV soll etwa 30 % der Investitionskosten rückvergüten. Dafür werden aber nicht die realen Kosten, sondern der Preis einer entsprechenden theoretischen Referenzanlage zugrunde gelegt. Entsprechend der sinkenden Kosten für Solaranlagen verringerte sich in den letzten Jahren auch die EIV. Auch bei Inanspruchnahme der EIV ist die Abnahme des Stroms durch die Netzbetreiber garantiert, aber nur zu Marktpreisen. Bei der KEV kauft der Netzbetreiber den Strom der Anlage zu investitionsdeckenden Preisen. Die dabei entstehenden Kosten werden auf alle Stromverbraucher umgelegt.

Die Koordination der Förderprogramme erfolgt durch Swissgrid.

Im westafrikanischen Staat Sierra Leone soll bis Ende 2016 etwa ein Viertel des erzeugten Stroms aus erneuerbaren Energien, vor allem aus Solarenergie, stammen. In der Nähe der Hauptstadt Freetown soll mit 6 MW Leistung Westafrikas größter Solarpark entstehen. In Koindu wird nachts das Stadtzentrum von einer solarbasierten Straßenbeleuchtung erhellt. Diese ist seit Juli 2013 in Betrieb. Außerdem werden Teile der Straße nach Yenga, einem Dorf an der Grenze zu Guinea und Liberia, ebenfalls von Photovoltaikbeleuchtungen erhellt.




</doc>
<doc id="4101" url="https://de.wikipedia.org/wiki?curid=4101" title="Persona non grata">
Persona non grata

Persona non grata (auch , für "unerwünschte Person"; ) bezeichnet den Status eines Angehörigen des diplomatischen Dienstes oder einer anderen Person, deren Aufenthalt von der Regierung des Gastlandes per Notifikation nicht mehr geduldet wird. Das Gegenteil ist die "persona grata" beziehungsweise die "persona gratissima".

Diplomaten besitzen im Gastland diplomatische Immunität, was sie vor strafrechtlicher Verfolgung schützt. Dem Gastgeberland bleibt im Falle einer Straftat nur die Möglichkeit, den Diplomaten des Landes zu verweisen, indem es ihn zur "Persona non grata" erklärt. In diesem Fall haben die Diplomaten das Gastland innerhalb einer bestimmten Frist zu verlassen. Ein besonders schwerer Grund für einen „diplomatischen Rauswurf“ ist (mutmaßliche) Spionage. Außerdem reagieren Staaten mitunter mit der Ausweisung ausländischer Diplomaten auf die Ausweisung eigener Diplomaten, auch ohne jenen tatsächliche Verstöße nachzuweisen. So achteten während des Kalten Krieges die Sowjetunion und der Westen regelmäßig auf ein Gleichgewicht der Maßnahmen: Wurde beispielsweise hier ein Kulturattaché und zwei Botschaftsmitarbeiter ausgewiesen, wurden dort genauso viele Diplomaten in vergleichbarer Stellung des Landes verwiesen. Im Zusammenhang mit dem Mordfall Litwinenko kam es 2007 zu wechselseitigen Ausweisungen russischer und britischer Diplomaten.

Die Erklärung zur "Persona non grata" betrifft mittelbar auch die Familien der ausgewiesenen Diplomaten, da deren Aufenthaltserlaubnis in der Regel vom diplomatischen Status abhängt.



</doc>
<doc id="4102" url="https://de.wikipedia.org/wiki?curid=4102" title="Partitionstabelle">
Partitionstabelle

Als Partitionstabelle (), auch Partitionsschema ( und ), bezeichnet man normierte Datenstrukturen, die Informationen über die Aufteilung eines Datenspeichers in separate Bereiche beinhalten. Diese Bereiche werden als Partitionen bezeichnet und sind mehrere voneinander unabhängig benutzbare Teile auf normalerweise einem Speichermedium. Partitionen können sich niemals überlappen, je nach Spezifikation können sie jedoch beispielsweise andere Partitionen beinhalten (umfassen) und als Kennzeichnung unbenutzter Bereiche dienen.

Der Begriff "Partitionstabelle" umfasst alle Formen der Partitionierung, wenn diese eine Tabelle verwenden; umgangssprachlich wird häufig die am weitesten verbreitete im " (MBR)" enthaltene Partitionstabelle des IBM-kompatiblen PC mit dem Begriff gleichgesetzt, obwohl dies nicht korrekt ist. Die beiden am häufigsten verwendeten und anzutreffenden Partitionstabellen sind der schon erwähnte "" sowie dessen Nachfolger " (GPT)", deutsch "GUID-Partitionstabelle".

Eingeführt wurden Partitionstabellen für Festplattenlaufwerke. Auf Disketten wurde mangels Kapazität üblicherweise keine weitere Unterteilung des Datenträgers in Partitionen vorgenommen und somit auch keine Partitionstabelle eingesetzt. Auf größeren Datenspeichern wie USB-Sticks ist hingegen fast immer eine Partitionstabelle vorhanden, die allerdings in der Norm nur eine einzige Partition über den gesamten Speicherbereich definiert.

Es gibt verschiedene Spezifikationen nach denen Partitionstabellen aufgebaut sein können. Im Falle des weit verbreiteten "" ist die primären Partitionstabelle inklusive eines Bootloaders innerhalb eines einzigen Datenblocks auf einem Datenträger gespeichert und umfasst präzise 64 Bytes, da sich die "MBR"-Partitionstabelle aus 4 Einträgen (4 primäre Partitionen) à 16 Byte zusammensetzt (4 × 16 Bytes = 64 Bytes). Der MBR muss gemäß Spezifikation im ersten Sektor eines Datenspeichers, dem Sektor mit der Nummer 0, gespeichert sein, der bei der üblichen Sektorgröße für 512 Bytes Platz bietet. Somit verbleiben 446 Bytes für einen auswechselbaren Bootloader in x86-Maschinencode für den IBM-PC und kompatible Computer und 2 Bytes für eine Magische Zahl.

Eine umfassende Aufteilung von Datenspeicher auf der Ebene eines Dateisystems (siehe "") ist nicht Teil des Begriffs, jedoch stützen sich diese meist auf eine vorhandene Partitionierung mittels Partitionstabellen. Auch RAID-Konfigurationen, die auf bestehende Partitionen aus Partitionstabellen aufbauen, sind möglich, sowie der umgekehrte Fall, dass eine Partitionstabelle innerhalb einer bestehenden RAID-Konfiguration definiert ist.

Als synonyme Bezeichnung hat sich Partitionsschema, und , etabliert. Dies ist durchaus auch auf die ursprüngliche Plattform bezogen, sodass z. B. oft vom "PC-Partitionsschema" oder vom "Mac-Partitionsschema" die Rede ist. Diese Bezeichnungen sind jedoch ungenau, weil auf diesen Plattformen verschiedene Partitionstabellen gebräuchlich sind: Am PC mit BIOS (bis ca. 2010) ist es der ', am Macintosh bis 2006 ist es die ', während beide Plattformen mit EFI bzw. UEFI die GUID-Partitionstabelle (GPT) verwenden. Zur eindeutigeren Unterscheidung ist daher auch "MBR-Partitionsschema" und "GPT-Partitionsschema" gebräuchlich. Aber es kommen auch Mischformen vor, etwa "GUID-Partitionstabellenschema" oder "-Partitionstabellentyp".

Je nach dem, auf was Bezug genommen wird, heißt die MBR-Partitionstabelle jedoch auch "MS-DOS-Partitionstabelle" oder "-Partitionsschema" (), weil der 1983 mit MS-DOS 2.0 eingeführt wurde. Aber auch "PC-Partitionierungsschema", , ist eine mögliche Bezeichnung für die Partitionstabelle im .

Da die Begriffe sehr ungenau sind, kommt es jedoch auf den Kontext an. So kann der Begriff "Partitionsschema" auch das Aufteilen der Partitionen für eine Unix-Betriebssysteminstallation bedeuten, etwa wenn einzelne Verzeichnisse gemäß FHS als separate Partitionen erstellt werden. Unter FreeBSD wird diese Aufteilung hingegen als Partitionslayout bezeichnet.

Unter den meisten Betriebssystemen auf PCs werden Partitionstabellen mit einem Programm namens codice_1 eingerichtet. Es gibt aber auch eine Vielzahl weiterer Programme, um eine Festplatte zu partitionieren.

Moderne Betriebssysteme mit grafischer Benutzeroberfläche bieten meist ein einfach zu bedienendes grafisches Dienstprogramm zum Erstellen und Verwalten von Partitionen (unterschiedlicher Partitionstabellen) an. Unter Windows ist dies die "Datenträgerverwaltung", bei Linux- und BSD-Distributionen ist meist ein entsprechendes geführtes Modul beim Installationsprogramm enthalten bzw. wird üblicherweise ein auf "GNU Parted" gestütztes Frontend mitgeliefert. In Mac OS X/​OS X/​macOS ist neben diversen Kommandozeilenprogrammen das "Festplattendienstprogramm" als grafisches Partitionierungswerkzeug enthalten.

Auf Festplatten für IBM-PC-kompatible Computer mit BIOS wird der "" (kurz: "MBR") als Partitionstabelle verwendet. Da er auf fast allen gängigen Betriebssystemen und Plattformen unterstützt wird, hat er sich als De-facto-Standard für die Partitionierung auf externen Speichermedien (z. B. Speicherkarten, USB-Sticks, Zip-Disketten) etabliert. Man findet ihn daher auf Geräten wie MP3-Playern oder Autoradios (wenn ein USB-Stick angesteckt wird) ebenso wie auf externen Speichermedien aller Art.

Weil die "MBR"-Partitionstabelle ein Größenlimit von 2,2 TB aufweist, hat sich seit ca. 2010 auf Festplatten, die größer als rund 2 TB sind, der "MBR"-Nachfolger "GUID-Partitionstabelle", kurz "GPT", als neuer De-facto-Standard etabliert. Diese Partitionstabelle wurde etwa um das Jahr 2000 von Intel mit der EFI-Spezifikation definiert und soll einige Einschränkungen des "MBR" als dessen Nachfolger aufheben. Somit ist die Chance groß, dass ein Gerät, wenn es Speichermedien mit >2,2 TB Speicherkapazität unterstützt, neben "MBR" auch "GPT" lesen kann.

Obwohl auf Speichermedien eine Art von Partitionierung immer möglich ist, ist sie keineswegs notwendig. Einige Betriebssysteme verlangen jedoch eine Partitionstabelle, um auf ein Speichermedium zugreifen zu können, könnten ein Speichermedium ohne Partitionstabelle als leer betrachten und ohne Rückfrage darauf schreiben, oder haben Einschränkungen beim Zugriff und können nur bestimmte Partitionen verwenden. Als Nachfolger des "Master Boot Record" enthält die GUID-Partitionstabelle ("GPT") zusätzlich eine "MBR"-Partitionstabelle mit nur einem Eintrag, der den gesamten belegbaren Speicherplatz umfasst, was den Datenträger für alte Betriebssysteme als belegt kennzeichnet und somit den Inhalt der GUID-Partitionstabelle schützt (siehe Schutz-MBR).

Auf verschiedenen Systemen und Plattformen sind für Startmedien meist spezifische Partitionstabellen, oft in Kombination mit definierten Partitionen und Dateisystemen, erforderlich. Startmedien sind Speichermedien, die zum Systemstart () verwendet werden können. Dies trifft für Workstations ebenso zu wie für u. a. eingebettete Systeme und mobile Geräte. Beispiele im Bereich der Personal Computer sind etwa Apple Macintosh mit "APM" bis 2005 und Amiga mit "Rigid Disk". Auf modernen IBM-PC-kompatiblen Computern mit dem BIOS-Nachfolger UEFI wie auch auf Apple-Computern seit 2006 wird " (GPT)" verwendet.




</doc>
<doc id="4103" url="https://de.wikipedia.org/wiki?curid=4103" title="Priamos">
Priamos

Priamos (, , eingedeutscht auch Priam) ist der Sohn des Laomedon und der Strymo (Plakia, Tochter des Otreus). Er war in der griechischen Mythologie der sechste und letzte König von Troja (Ilios), während des von Homer geschilderten trojanischen Krieges.

Sein Name rührt nach antiker Etymologie von dem (griech. oder lydischen) Wort πρίασθαι ("priasthai" ‚kaufen‘) her. Diesen Namen hatte er der Sage nach erhalten, weil seine Schwester Hesione ihn freikaufte, als die Stadt von Herakles erstürmt worden war, da dieser von seinem Vater und damaligen König Laomedon, betrogen worden war. Davor soll er Podarkes ( ‚der Schnellfüßige‘) genannt worden sein.

Als seine (Halb-)Brüder gelten Tithonos, Lampo, Klytios, Hiketaon, als (Halb-)Schwestern Hesione, Killa und Astyoche. 

Homer berichtete auch, dass Priamos als junger Mann Phryger, die sich am Sangarios niedergelassen hatten, im Kampf unterstützte, als diese von Amazonen angegriffen wurden.

Er nahm am Kampf um Troja seines Alters wegen nicht teil. In der "Ilias" Homers tritt er als Greis durch seinen nächtlichen Gang ins griechische Schiffslager hervor, wo er von Achilleus den geschändeten Leichnam seines Sohnes Hektor erbittet und schließlich auch erhält.

Priamos soll bei der Eroberung Trojas von Neoptolemos (bei Vergil: "Pyrrhus"), dem Sohn des Achilles, am Altar des Zeus vor seinen Angehörigen niedergemacht worden sein. Spätere Dichtungen nach Homer stellen das Geschehen anders dar: Nach Erstürmung der Stadt Troja habe sich Priamos bewaffnet und wollte sich so in die feindliche Menge stürzen. Hekabe flehte ihn aber an zu fliehen, weshalb er sich mit ihr und seinen Töchtern an den Altar des Zeus Herkeios flüchte. Hier sah er, wie sein Sohn Polites [also hier nicht Hektor] durch Pyrrhus getötet wurde, und „sendete sein Geschoß gegen diesen, findet aber hierbei seinen Tod durch denselben“.

Priamos’ erste Frau soll Arisbe, Tochter des Merops gewesen sein, mit welcher er Aisakos zeugte. Danach heiratete er Hekabe (auch "Hecuba" geschrieben; Tochter des phrygischen Königs Dymas) und zeugte mit ihr insgesamt 19 Kinder, darunter Hektor, Krëusa, Helenos, Antiphos, Polites, Laodike, Paris, Kassandra, Polydoros, Polyxena, Troilos, Agathon, Deiphobos. 

Weitere Frauen und Nachkommen des Priamos laut der überlieferten Mythen:


Er soll mit allen seinen Frauen und Nebenfrauen weit über 50 Kinder gehabt haben, die fast alle im Krieg um Troja gefallen oder umgekommen sind.

Nach Priamos wurde 1872 von Heinrich Schliemann der „Schatz des Priamos“ benannt, den dieser bei Ausgrabungen in den Ruinen von Troja entdeckt hatte.

Priamos tritt sowohl in Homers „"Ilias"“ als auch in der „"Aeneis"“ Vergils auf, sowie in zahlreichen Erzählungen und Bühnenstücken über den Trojanischen Krieg, bis hinein in die Moderne.

Priamos’ Klage bei Homer:



</doc>
